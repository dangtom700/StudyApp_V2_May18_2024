Numerical Methods for 
Scientists and Engineers
Numerical Methods for Scientists and Engineers: With Pseudocodes is designed as a primary textbook for a 
one-semester course on Numerical Methods for sophomore or junior-level students. It covers the fundamental 
numerical methods required for scientists and engineers, as well as some advanced topics which are left to the 
discretion of instructors. 
The objective of the text is to provide readers with a strong theoretical background on numerical methods en￾countered in science and engineering, and to explain how to apply these methods to practical, real-world prob￾lems. Readers will also learn how to convert numerical algorithms into running computer codes. 
Features:
• Numerous pedagogic features including exercises, “pros and cons” boxes for each method discussed, and 
rigorous highlighting of key topics and ideas 
• Suitable as a primary text for undergraduate courses in numerical methods, but also as a reference to working 
engineers 
• A Pseudocode approach that makes the book accessible to those with different (or no) coding backgrounds, 
which does not tie instructors to one particular language over another
• A dedicated website featuring additional code examples, quizzes, exercises, discussions, and more: https://
github.com/zaltac/NumMethodsWPseudoCodes
• A complete Solution Manual and PowerPoint Presentations are available (free of charge) to instructors at 
www.routledge.com/9781032754741Numerical Analysis and Scientific Computing Series
Series Editors:
Frederic Magoules, Choi-Hong Lai
About the Series
This series, comprising of a diverse collection of textbooks, references, and handbooks, brings 
together a wide range of topics across numerical analysis and scientific computing. The books 
contained in this series will appeal to an academic audience, both in mathematics and computer 
science, and naturally find applications in engineering and the physical sciences.
Modelling with Ordinary Differential Equations
A Comprehensive Approach
Alfio Borzì
Numerical Methods for Unsteady Compressible Flow Problems
Philipp Birken
A Gentle Introduction to Scientific Computing
Dan Stanescu, Long Lee
Introduction to Computational Engineering with MATLAB
Timothy Bower
An Introduction to Numerical Methods
A MATLAB• Approach, Fifth Edition
Abdelwahab Kharab, Ronald Guenther
The Sequential Quadratic Hamiltonian Method
Solving Optimal Control Problems
Alfio Borzì
Advances in Theoretical and Computational Fluid Mechanics
Existence, Blow-up, and Discrete Exterior Calculus Algorithms
Terry Moschandreou, Keith Afas, Khoa Nguyen
Stochastic Methods in Scientific Computing
From Foundations to Advanced Techniques 
Massimo D’Elia, Kurt Langfeld, Biagio Lucini
For more information about this series please visit: https://www.crcpress.com/Chapman--HallCRC￾Numerical-Analysis-and-Scientific-Computing-Series/book-series/CHNUANSCCOMNumerical Methods for 
Scientists and Engineers
With Pseudocodes
Zekeriya AltaçDesigned cover image: peterschreiber.media/Shutterstock
First edition published 2025 
by CRC Press
2385 NW Executive Center Drive, Suite 320, Boca Raton FL 33431
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC
© 2025 Zekeriya Altaç 
Reasonable efforts have been made to publish reliable data and information, but the author and publisher cannot as￾sume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have 
attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders 
if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please 
write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or 
utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including pho￾tocopying, microfilming, and recording, or in any information storage or retrieval system, without written permission 
from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.com or contact the 
Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. For works that are 
not available on CCC please contact mpkbookspermissions@tandf.co.uk
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are used only for 
identification and explanation without intent to infringe.
Library of Congress Cataloging-in-Publication Data
Names: Altaç, Zekeriya, author. 
Title: Numerical methods for scientists and engineers : with pseudocodes / 
Zekeriya Altaç. 
Description: First edition. | Boca Raton : C&H/CRC Press, 2025. | Series: 
Chapman and Hall/CRC numerical analysis and scientific computing series 
| Includes bibliographical references and index. 
Identifiers: LCCN 2024020497 (print) | LCCN 2024020498 (ebook) | ISBN 
9781032754741 (hardback) | ISBN 9781032756424 (paperback) | ISBN 
9781003474944 (ebook) 
Subjects: LCSH: Numerical analysis--Textbooks. | Numerical analysis--Data 
processing--Textbooks. 
Classification: LCC QA297 .A538 2025 (print) | LCC QA297 (ebook) | DDC 
518--dc23/eng/20240522 
LC record available at https://lccn.loc.gov/2024020497
LC ebook record available at https://lccn.loc.gov/2024020498
ISBN: 978-1-032-75474-1 (hbk)
ISBN: 978-1-032-75642-4 (pbk) 
ISBN: 978-1-003-47494-4 (ebk)
DOI: 10.1201/9781003474944
Typeset in Latin Modern font 
by KnowledgeWorks Global Ltd. 
Publisher’s note: This book has been prepared from camera-ready copy provided by the authors.
Access the Instructor Resources: www.routledge.com/9781032754741I dedicate this book to my beloved uncle Aydın BİNGÖL, whom I have always looked
up to in all my endeavors. Without his support, I would not have been able to get my
master’s and doctoral degrees in the USA and to be academically where I am today. I will
always be grateful to him.Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com Contents
List of Figures xv
List of Tables xix
Preface xxi
Author Bio xxvii
Chapter 1  Numerical Algorithms and Errors 1
1.1 NUMERICAL ALGORITHMS IN PSEUDOCODES 2
1.1.1 ELEMENTS OF A PSEUDOCODE 4
1.1.2 SEQUENTIAL CONSTRUCTS 6
1.1.3 CONDITIONAL CONSTRUCTS 6
1.1.4 LOOPS AND ACCUMULATORS 7
1.1.5 PSEUDOCODES, FUNCTION MODULES AND MODULES 10
1.1.6 TIPS FOR EFFICIENT PROGRAMMING 13
1.2 BASIC DEFINITIONS AND SOURCES OF ERROR 15
1.2.1 DEFINITION OF ERROR 17
1.2.2 MEASUREMENT ERROR, ACCURACY, AND PRECISION 19
1.3 PROPAGATION OF ERROR 20
1.4 NUMERICAL COMPUTATIONS 23
1.4.1 NUMBER SYSTEMS 23
1.4.2 FLOATING-POINT REPRESENTATION OF NUMBERS 24
1.4.3 PRECISION OF A NUMERICAL QUANTITY 26
1.4.4 ROUNDING-OFF AND CHOPPING 28
1.4.5 SIGNIFICANT DIGITS 29
1.4.6 DECIMAL-PLACE AND SIGNIFICANT-DIGIT ACCURACY 32
1.4.7 EFFECT OF ROUNDING IN ARITHMETIC OPERATIONS 32
1.4.8 LOSS OF SIGNIFICANCE 34
1.4.9 CONDITION AND STABILITY 36
1.5 APPLICATION OF TAYLOR SERIES 38
1.5.1 TAYLOR APPROXIMATION AND TRUNCATION ERROR 38
1.5.2 MEAN VALUE THEOREMS 42
viiviii  Contents
1.6 TRUNCATION ERROR OF SERIES 43
1.7 RATE AND ORDER OF CONVERGENCE 45
1.8 CLOSURE 49
1.9 EXERCISES 50
1.10 COMPUTER ASSIGNMENTS 61
Chapter 2  Linear Systems: Fundamentals and Direct Methods 64
2.1 FUNDAMENTALS OF LINEAR ALGEBRA 65
2.1.1 MATRICES 65
2.1.2 SPECIAL MATRICES 66
2.1.3 OPERATIONS WITH MATRICES 68
2.1.4 DETERMINANTS 72
2.1.5 SYSTEM OF LINEAR EQUATIONS 74
2.1.6 EIGENVALUES AND EIGENVECTORS 75
2.1.7 VECTOR AND MATRIX NORMS 79
2.2 ELEMENTARY MATRIX OPERATIONS 81
2.3 MATRIX INVERSION 83
2.3.1 ADJOINT METHOD 84
2.3.2 GAUSS-JORDAN METHOD 84
2.4 TRIANGULAR SYSTEMS OF LINEAR EQUATIONS 90
2.5 GAUSS ELIMINATION METHODS 93
2.5.1 NAIVE GAUSS ELIMINATION 93
2.5.2 GAUSS-JORDAN ELIMINATION METHOD 96
2.5.3 GAUSS-ELIMINATION WITH PIVOTING 101
2.6 COMPUTING DETERMINANTS 105
2.7 ILL-CONDITIONED MATRICES AND LINEAR SYSTEMS 105
2.8 DECOMPOSITION METHODS 109
2.8.1 DOOLITTLE LU-DECOMPOSITION 110
2.8.2 CHOLESKY DECOMPOSITION 115
2.9 TRIDIAGONAL SYSTEMS 117
2.9.1 THOMAS ALGORITHM 117
2.9.2 LU DECOMPOSITION 119
2.9.3 CHOLESKY DECOMPOSITION 120
2.10 BANDED LINEAR SYSTEMS 123
2.10.1 NAIVE GAUSS ELIMINATION 124
2.10.2 LU-DECOMPOSTION 126
2.11 CLOSURE 132
2.12 EXERCISES 133
2.13 COMPUTER ASSIGNMENTS 149Contents  ix
Chapter 3  Linear Systems: Iterative Methods 155
3.1 OVERVIEW 156
3.1.1 STEPS OF AN ITERATIVE METHOD 157
3.1.2 SOURCES OF ERROR IN ITERATIVE METHODS 159
3.2 STATIONARY ITERATIVE METHODS 159
3.2.1 JACOBI METHOD 159
3.2.2 GAUSS-SEIDEL (GS) METHOD 163
3.2.3 SOR METHOD 166
3.3 CONVERGENCE OF STATIONARY ITERATIVE METHODS 169
3.3.1 RATE OF CONVERGENCE 171
3.3.2 ESTIMATING OPTIMUM RELAXATION PARAMETER 174
3.4 KRYLOV SPACE METHODS 180
3.4.1 CONJUGATE GRADIENT METHOD (CGM) 180
3.4.2 CONVERGENCE ISSUES OF THE CGM 185
3.4.3 PRECONDITIONING 186
3.4.4 CGM FOR NONSYMMETRIC SYSTEMS 186
3.4.5 CHOICE OF PRECONDITIONERS 188
3.5 IMPROVING ACCURACY OF ILL-CONDITIONED SYSTEMS 189
3.6 CLOSURE 191
3.7 EXERCISES 193
3.8 COMPUTER ASSIGNMENTS 198
Chapter 4  Nonlinear Equations 203
4.1 BISECTION METHOD 204
4.2 METHOD OF FALSE POSITION 209
4.3 FIXED-POINT ITERATION 211
4.4 NEWTON-RAPHSON METHOD 213
4.5 MODIFIED NEWTON-RAPHSON METHOD 218
4.6 SECANT AND MODIFIED SECANT METHODS 222
4.7 ACCELERATING CONVERGENCE 226
4.8 SYSTEM OF NONLINEAR EQUATIONS 231
4.9 BAIRSTOW’S METHOD 237
4.10 POLYNOMIAL REDUCTION AND SYNTHETIC DIVISION 245
4.11 CLOSURE 252
4.12 EXERCISES 253
4.13 COMPUTER ASSIGNMENTS 266
Chapter 5  Numerical Differentiation 270
5.1 BASIC CONCEPTS 271
5.2 FIRST-ORDER FINITE DIFFERENCE FORMULAS 278
5.3 SECOND-ORDER FINITE-DIFFERENCE FORMULAS 279x  Contents
5.4 CENTRAL DIFFERENCE FORMULAS 281
5.5 FINITE DIFFERENCES AND DIRECT-FIT POLYNOMIALS 283
5.6 DIFFERENTIATING NON-UNIFORMLY SPACED DISCRETE DATA 292
5.7 RICHARDSON EXTRAPOLATION 296
5.8 ALTERNATIVE METHODS OF EVALUATING DERIVATIVES 299
5.9 CLOSURE 301
5.10 EXERCISES 302
5.11 COMPUTER ASSIGNMENTS 309
Chapter 6  Interpolation and Extrapolation 313
6.1 LAGRANGE INTERPOLATION 315
6.2 NEWTON’S DIVIDED DIFFERENCES 322
6.3 NEWTON’S FORMULAS FOR UNIFORMLY SPACED DATA 329
6.4 CUBIC SPLINE INTERPOLATION 341
6.4.1 NATURAL SPLINE END CONDITION 344
6.4.2 LINEAR EXTRAPOLATION END CONDITION 345
6.4.3 COMPUTING SPLINE COEFFICIENTS 346
6.5 ROOTFINDING BY INVERSE INTERPOLATION 350
6.6 MULTIVARIATE INTERPOLATION 352
6.6.1 BILINEAR INTERPOLATION 353
6.6.2 BIVARIATE LAGRANGE INTERPOLATION 354
6.7 EXTRAPOLATION 355
6.8 CLOSURE 360
6.9 EXERCISES 361
6.10 COMPUTER ASSIGNMENTS 368
Chapter 7  Least Squares Regression 370
7.1 POLYNOMIAL REGRESSION 371
7.1.1 DETERMINING REGRESSION MODEL 373
7.1.2 GOODNESS OF FIT 375
7.1.3 OVERFITTING 384
7.2 TRANSFORMATION OF VARIABLES 385
7.3 LINEARIZATION OF NON-LINEAR MODELS 386
7.4 MULTIVARIATE REGRESSION 391
7.5 CONTINUOUS LEAST SQUARES APPROXIMATION 394
7.6 OVER- OR UNDER-DETERMINED SYSTEMS OF EQUATIONS 397
7.7 CLOSURE 399
7.8 EXERCISES 400
7.9 COMPUTER ASSIGNMENTS 409Contents  xi
Chapter 8  Numerical Integration 413
8.1 TRAPEZOIDAL RULE 414
8.2 SIMPSON’S RULE 422
8.3 ROMBERG’S RULE 429
8.4 ADAPTIVE INTEGRATION 435
8.5 NEWTON-COTES RULES 440
8.5.1 CLOSED NEWTON-COTES RULES 440
8.5.2 OPEN NEWTON-COTES RULES 441
8.6 INTEGRATION OF NON-UNIFORM DISCRETE FUNCTIONS 443
8.7 GAUSS-LEGENDRE METHOD 445
8.8 COMPUTING IMPROPER INTEGRALS 452
8.9 GAUSS-LAGUERRE METHOD 457
8.10 GAUSS-HERMITE METHOD 461
8.11 GAUSS-CHEBYSHEV METHOD 465
8.12 COMPUTING INTEGRALS WITH VARIABLE LIMITS 470
8.13 DOUBLE INTEGRATION 471
8.14 CLOSURE 475
8.15 EXERCISES 476
8.16 COMPUTER ASSIGNMENTS 493
Chapter 9  ODEs: Initial Value Problems 498
9.1 FUNDAMENTAL PROBLEM 499
9.2 ONE-STEP METHODS 501
9.2.1 EXPLICIT EULER METHOD 501
9.2.2 METHOD OF TAYLOR POLYNOMIAL 506
9.2.3 IMPLICIT EULER METHOD 510
9.2.4 TRAPEZOIDAL RULE 512
9.2.5 MIDPOINT METHOD 514
9.2.6 MODIFIED EULER METHOD 514
9.2.7 RUNGE-KUTTA (RK) METHODS 516
9.3 NUMERICAL STABILITY 521
9.3.1 STABILITY OF EULER METHODS 522
9.3.2 STABILITY OF RUNGE-KUTTA METHODS 523
9.3.3 STIFFNESS 523
9.4 MULTISTEP METHODS 527
9.4.1 ADAMS-BASHFORTH METHOD (AB) 528
9.4.2 ADAMS-MOULTON METHOD 531
9.4.3 BACKWARD DIFFERENTIATION FORMULAS 533
9.5 ADAPTIVE STEP-SIZE CONTROL 540
9.6 PREDICTOR-CORRECTOR METHODS 542
9.6.1 HEUN’S METHOD 543xii  Contents
9.6.2 ADAMS-BASHFORTH-MOULTON METHOD (ABM4) 544
9.6.3 MILNE’S FOURTH-ORDER METHOD 546
9.7 SYSTEM OF FIRST-ORDER IVPS (ODES) 550
9.8 HIGHER-ORDER ODES 553
9.8.1 STIFF ODEs 554
9.8.2 NUMERICAL STABILITY 554
9.9 SIMULTANEOUS NONLINEAR ODES 557
9.10 CLOSURE 558
9.11 EXERCISES 559
9.12 COMPUTER ASSIGNMENTS 570
Chapter 10  ODEs: Boundary Value Problems 574
10.1 INTRODUCTION 575
10.2 TWO-POINT BOUNDARY VALUE PROBLEMS 576
10.2.1 DIRICHLET BOUNDARY CONDITION 577
10.2.2 NEUMANN BOUNDARY CONDITION 577
10.2.3 ROBIN (MIXED) BOUNDARY CONDITION 577
10.2.4 PERIODIC BOUNDARY CONDITION 578
10.3 FINITE DIFFERENCE SOLUTION OF LINEAR BVPs 578
10.3.1 IMPLEMENTING DIRICHLET BCs 581
10.3.2 IMPLEMENTING NEUMANN AND ROBIN BCs 584
10.4 NUMERICAL SOLUTIONS OF HIGH-ORDER ACCURACY 589
10.5 NON-UNIFORM GRIDS 593
10.6 FINITE VOLUME METHOD 599
10.7 FINITE DIFFERENCE SOLUTION NONLINEAR BVPS 609
10.7.1 TRIDIAGONAL ITERATION METHOD (TIM) 610
10.7.2 NEWTON’S METHOD 614
10.8 SHOOTING METHOD 619
10.8.1 LINEAR ODES 619
10.8.2 NON-LINEAR ODEs 623
10.9 FOURTH-ORDER LINEAR DIFFERENTIAL EQUATIONS 625
10.10 CLOSURE 630
10.11 EXERCISES 631
10.12 COMPUTER ASSIGNMENTS 643
Chapter 11  Eigenvalues and Eigenvalue Problems 645
11.1 EIGENVALUE PROBLEM AND PROPERTIES 646
11.1.1 LOCATION AND BOUNDS OF EIGENVALUES 647
11.1.2 EIGENPAIRS OF REAL SYMMETRIC MATRICES 648
11.2 POWER METHOD 651
11.2.1 POWER METHOD WITH SCALING 652Contents  xiii
11.2.2 POWER METHOD WITH RAYLEIGH QUOTIENT 654
11.2.3 INVERSE POWER METHOD 658
11.2.4 SHIFTED INVERSE POWER METHOD 659
11.3 SIMILARITY AND ORTHOGONAL TRANSFORMATIONS 662
11.3.1 DIFFERENTIAL EQUATIONS 664
11.4 JACOBI METHOD 666
11.5 CHOLESKY DECOMPOSITION 673
11.6 HOUSEHOLDER METHOD 677
11.7 EIGENVALUES OF TRIDIAGONAL MATRICES 683
11.7.1 THE STURM SEQUENCE 683
11.7.2 THE QR ITERATION 686
11.7.3 CLASSIC GRAM-SCHMIDT PROCESS 694
11.7.4 COMPUTING EIGENVECTORS 700
11.8 FADDEEV-LEVERRIER METHOD 701
11.9 CHARACTERISTIC VALUE PROBLEMS 703
11.9.1 NUMERICAL SOLUTION OF CVPs 705
11.10 CLOSURE 710
11.11 EXERCISES 711
11.12 COMPUTER ASSIGNMENTS 721
Appendix A  A Guide on How to Read and Write a Pseudocode 723
A.1 BASICS AND VARIABLES 723
A.1.1 VARIABLES AND CONSTANTS 723
A.1.2 DECLARATION 723
A.1.3 COMMENTING 724
A.1.4 ASSIGNMENT 724
A.1.5 SEQUENTIAL STATEMENTS 724
A.1.6 INPUT/OUTPUT STATEMENTS 725
A.2 LOGICAL VARIABLES AND LOGICAL OPERATORS 725
A.3 CONDITIONAL CONSTRUCTIONS (IF-THEN, IF-THEN-ELSE) 726
A.4 CONTROL CONSTRUCTIONS 727
A.4.1 WHILE-CONSTRUCTS 727
A.4.2 REPEAT-UNTILCONSTRUCT 728
A.4.3 FOR-CONSTRUCT 728
A.4.4 EXITING A LOOP 729
A.5 ARRAYS 730
A.6 PROGRAM, MODULE, AND FUNCTION MODULE
STRUCTURES 731
A.6.1 SUB-PROGRAM DEFINITIONS: Module 732xiv  Contents
A.6.2 FUNCTION MODULE OR RECURSIVE FUNCTION MODULE 732
Appendix B  Quadratures 734
B.1 GAUSS-LEGENDRE QUADRATURE 734
B.2 GAUSS-LAGUERRE QUADRATURE 735
B.3 GAUSS-HERMITE QUADRATURE 736
Bibliography 737
References 747
Index 751List of Figures
1.1 Process of finding a numerical solution of a problem. 3
1.2 Schematic illustration of the accumulator process for (a) sum and (b) product. 8
1.3 (a) Accuracy-low, Precision-low, Bias present (on the east of the target); (b)
Accuracy-high, precision-low (No bias), (c) Accuracy-low, Precision-high,
Bias present (on the SW of the target), (d) Accuracy-high, Precision-high
(No Bias) 20
1.4 IEEE 754 Floating-Point Standard (a) single, (b) double precision. 26
1.5 Depiction of significant/non-significant figures of a (a) small, (b) large, and
(c) exponential number. 29
2.1 Illustration of the matrix inversion procedure ‚ and b denote the non-zero
elements occupied in matrices A and I, respectively. 85
2.2 Illustration of the forward substitution procedure. Note: The boxes denote
allocated memory for xi’s and bi’s, and symbols denote non-zero values. 90
2.3 Illustration of the back substitution procedure. Note: The boxes denote al￾located memory for xi’s and bi’s, and symbols denote non-zero values. 92
2.4 (a) Row echelon (Ux “ b1
), (b) reduced row echelon forms (Ix “ b2). Sym￾bols b and b denote the modified elements of A and b, respectively, and
any two identical symbols do not imply equality of numbers. 93
2.5 Illustration of the naive Gauss elimination procedure without pivoting (‚
and b denote the modified (computed) and unmodified (to be modified)
elements of A and b, respectively). Any two identical symbols do not imply
equality of numbers. 95
2.6 Illustrating the Gauss elimination procedure with pivoting (‚ and b denote
the modified (computed) and unmodified (to be updated) elements of A and
b, respectively). 103
2.7 Depicting Doolittle LU-decomposition; (a) left to right row sweeps on U,
(b) top to bottom column sweeps on L. 111
2.8 Matrix storage for (a) a banded, (b) a compact matrix. 123
2.9 Depiction of naive Gauss Elimination procedure for a banded matrix with
pivot at (a) i ă n ´ mU , (b) i ą n ´ mU . 124
3.1 Sparcity patterns of coefficient matrices resulting from discretization of an
elliptic PDE in (a) 2D and (b) 3D-rectangular, elliptic PDE in (c) 2D- and
(d) 3D-nonrectangular domain. The non-zero elements are marked in black. 156
3.2 Variation of ρpMωq with ω and ρpMJ q. 173
xvxvi  List of Figures
4.1 Graphical depiction of a function with a root in (a, b) interval. 204
4.2 Depicting the convergence of the root with the Bisection method. 205
4.3 Depicting the root estimation with the method of false position. 209
4.4 Depiction of the fixed-point iteration sequence of possible cases: (a) mono￾tone convergence (0 ď g1
pxppq
q ă 1); (b) oscillating convergence (´1 ă
g1
pxppq
q ď 0); (c) monotonic divergence (g1
pxppq
q ą 1); (d) oscillating diver￾gence (g1
pxppq
qă´1). 211
4.5 Graphical depiction of the Newton-Raphson method. 215
4.7 Depiction of the Secant method. 222
4.8 Illustration of iteration sequence of implementing the Steffensen’s method. 227
5.1 A graphic depiction of (a) the height distribution of individuals in a popu￾lation and (b) the density-temperature relationship of a chemical solution. 271
5.2 Determining the slope of a function at point A. 272
5.3 Depicting numerical differentiation errors as a function of h. 275
5.5 Graphical depiction of data points used in (a) forward and (b) backward
differences of f 2pxiq. 280
5.6 Graphical depiction of forward, backward, and central difference approxi￾mations for f1
i . 282
5.7 A polynomial approximation to a discrete data. 284
5.9 (a) Uniform, (b) non-uniform data set near steep changes. 292
5.10 Non-uniform discrete data points for (a) forward, (b) backward, (c) central
differences. 293
6.1 Discrete functions with (a) uniform or (b) nonuniform distribution. 314
6.2 Graphical depiction of a linear interpolation. 314
6.3 The effect of interval size on linear interpolation. 315
6.4 318
6.5 The distribution of the true function and the Lagrange interpolating poly￾nomial for uniformly spaced (a) 6-interpolation points, (b) 11-interpolation
points, and (c) non-uniformly spaced 11-interpolation points. 321
6.7 An example of uniform discrete data distribution. 330
6.8 Construction of (a) forward, (b) backward, (c) central, (d) modified central
difference tables. 331
6.9 The true error distributions for the GN-FWD-IF with (a) t1 “ 0.5 and (b)
t2 “ 1 as the baseline. 339
6.10 Distribution of enptq with (a) the GN-BKWD-IF with t5 “ 2.5 as the base￾line, and (b) the Stirling’s formula with t3 “ 1.5 as the baseline. 340
6.11 Illustration of linear splines employed to a data set of five. 341
6.12 Cubic splines employed to a discrete data set. 342
6.13 Graphical depiction of rootfinding by linear interpolation. 350
6.14 Graphical depiction of bilinear interpolation. 353
6.15 Extrapolation for x ă x0 using Gregory-Newton forward difference table
depicting three different cases, (a), (b), and (c). 357List of Figures  xvii
7.1 A data set and fitted curves: (a) linear, (b) high-order polynomial. 371
7.2 Correlatability of the observed to a linear model; (a) no correlation, (b)
weak correlation, (c) fair correlation, (d) strong correlation. 373
7.3 Data sets and suitable models; (a) quadratic, (b) cubic model. 374
7.4 Measured mass-volume relationship of samples of a substance. 374
7.5 (a) Observed data along with the best fit, (b) residual plot. 376
7.6 Possible residual patterns: (a) unbiased and homoscedastic, (b), (c) biased
and homoscedastic, (d) unbiased and heteroscedastic, (e), (f) biased and
heteroscedastic. 377
7.8 Variation of 68% and 95% rmse bands (CI: confidence interval) with (a)
r2 “ 0.653, (b) r2 “ 0.815, and (c) r2 “ 0.967. 380
7.9 (a) Linear and (b) nonlinear fits with data and PO plots. 381
7.11 (a) residual plot (b) Confidence Interval (CI) plot. 383
7.12 Effects of increasing the degree of polynomial model on the fit. 385
7.13 Influence of b on (a) power, (b) exponential, (c) saturation models. 387
7.14 (a) Best-fit curve and original data, (b) PO curve. 391
7.15 (a) Best-fit curve and original data, (b) PO curve. 393
7.16 Approximation functions of fpxq by (a) linear (b) quadratic functions. 394
7.17 Linear and quadratic Taylor approximations for y “ ex. 395
8.1 The true area under the curve (definite integral). 414
8.2 Graphical depiction of the effect of approximations by (a) one, (b) two, (c)
four panels. 415
8.3 Graphical depiction of integration with multiple panels. 416
8.5 (a) Fitting a parabola to a two-panel interval, (b) fitting of two parabolas
in a four-panel case. 423
8.6 Simpson’s three-panel integration. 426
8.8 Graphical depiction of (a) two- (b) three- and (c) four-panel open Newton￾Cotes abscissas and approximating polynomials (in black). 441
8.9 A non-uniform distribution 444
8.11 Graphical depiction of integration domains for Type I integrals with infinity
at (a) the upper limit, (b) the lower limit, or (c) both limits. 453
8.12 Graphical depiction of integrands for Type II integrals with discontinuities
at the (a) upper end, (b) lower end, or (c) inside the integration interval. 454
8.13 Domains of integration (a) rectangular, (b) irregular. 471
9.1 (a) Swinging pendulum system and a typical solution; (a) Damped mass￾spring system and a typical solution. 500
9.2 Graphical depiction of forward advancing with explicit Euler’s method. 502
9.4 Graphical depiction of marching with the Explicit Euler, Implicit Euler
methods and Trapezoidal rule. 513
9.6 Marching strategy for (a) one-step, (b) multistep schemes. 528
9.7 Estimates used in 2nd, 3rd, and 4th order in explicit multistep schemes. 529xviii  List of Figures
9.8 A graphical depiction of marching in predictor-corrector methods. 543
10.1 (a) Heat conduction in a rod and a predicted solution; (b) Lateral deflection
of a beam and a predicted solution. 575
10.2 Typical grid construction on [a, b]. 578
10.3 Discretization and fictitious nodes. 580
10.4 Illustration of grids with Dirichlet BCs. 581
10.8 Grid stretching from left to right, or right to left. 594
10.9 Grid stretching using x “ a ` c ξn as stretching function examples of (a)
n ă 1, and (b) n ą 1. 595
10.11 General grid configuration and finite volume cells for one-dimensional sys￾tems (a) rod, (b) plane parallel layers, (c) cylindrical or spherical shells. 599
10.12 General grid configuration and a typical control volume surrounding a node
at xi. 600
10.13 Gridding the fuel-pin system. 606
10.16 Graphical depiction of the shooting method (a) numerical solution with
different B’s, (b) guess versus target values. 620
10.17 Evolution of numerical solution in a semi-infinite interval. 621
11.1 Depiction of Gerschgorin circles in complex plane. 648
11.3 The distribution of the eigenvalues with respect to the estimate α. 659
11.4 Depiction of the matrix operations in Householder method. 679
11.6 (a) A slender column under compressive stress, (b,c,d) buckling modes. 704
11.7 Grid structure. 705
A.1 Flowchart for a (a) While- and (b) Repeat-Until constructs. 728
A.2 Flowchart for a For-construct. 729
A.3 Depiction of elements of an array in computer memory. 730List of Tables
1.1 Error propagation rules. 21
1.2 Examples of significant digits. 30
1.5 Convergence behavior of the sequences an, bn, and cn. 46
5.2 The order of accuracy of derivatives with symmetrical (central) and non￾symmetrical (forward/backward) finite difference formulas. 285
5.3 The central difference formulas of order of accuracy Oph2q and Oph4q for
derivatives up to fourth order. 286
5.4 The forward difference formulas of order of accuracy Ophq, Oph2q, and Oph4q
for the derivatives up to fourth order. 287
5.5 The backward difference formulas of order of accuracy Ophq, Oph2q, and
Oph4q for the derivatives up to fourth order. 287
5.7 Richardson extrapolation table for f1
pxq using the CDF. 298
6.1 Construction of Newton’s divided-difference table. 323
6.2 Data updating in the Newton’s divided-difference table. 324
6.5 Error propagation in difference tables. 335
7.3 Typical transformation examples. 386
7.4 Linearization of common nonlinear models. 389
8.4 Construction of Romberg table. 432
9.7 The coefficients (βk) and the local truncation errors of n-step Adams￾Bashforth Formulas (n “ 1,..., 6). 529
9.8 The coefficients (βk) and the local truncation errors of n-step Adams￾Moulton formulas (n “ 1,..., 6). 531
9.9 The coefficients and the local truncation errors for BDFs (n “ 1,..., 6). 534
xixTaylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com Preface
This book is intended primarily as a core textbook for courses taught under the general
title of “Numerical Methods.” It is also suitable as a main or supplementary textbook for
upper-level undergraduate or graduate courses, as well as a reference book for practicing
engineers.
The objective of the text can be summarized as follows: (i) Provide a strong theoretical
background on numerical methods encountered in science and engineering education and
beyond; (ii) Understand and apply the methods to practical problems; (iii) Develop the
ability to choose a suitable method for a given problem; (iv) Gain the skill of converting
numerical algorithms into running computer codes, emphasized via pseudocodes. I explain
in detail the strategy implemented in the text to achieve the aforementioned objectives
under the following main headings:
A. LEARNING OBJECTIVES
Each chapter begins with the “Learning Objectives,” which briefly describe educational
goals and competencies to be achieved. The theoretical and applied objectives make up the
broader aims of the course. The learning objectives inform the students what they should
master by the end of each chapter.
B. CONTENT
The text content is designed for a one-semester course. It consists of 11 chapters covering
the fundamental methods as well as some advanced topics with significant reference values,
which are left to the discretion of instructors. In the arrangement of the chapters, the
contents of the subsequent chapters have been taken into account so that a student new to
a subject can digest the course material chapter by chapter. In cases where there is little
interdependence between the chapters, a sufficiently brief introduction to the pertinent topic
is provided before it is covered in greater detail in the following chapter(s).
My thoughts on how to teach and design the course and its contents have been shaped
over the past three decades while teaching engineering students. The S&E students want
to see the relevance of this math course and its material in their professional education.
Accordingly, in each chapter, the importance and application areas of the topics in S&E
are briefly stated. Every method is kept as simple as possible, and proofs have only been
included where they help students understand the underlying theory of the method better.
The S&E students learn best when they are motivated by solved examples and end￾of-chapter exercises. Besides, this is a field in which problem solving by hand is essential
to understanding the material, so a problem-solving perspective is adopted in this text.
Immediately after a method is presented, its application is reinforced with example problems
that comprehensively and thoroughly illustrate and explain the complexities and potential
difficulties of the method. Most examples and exercise problems consist of problems with
xxixxii  Preface
known exact solutions, allowing a clear and concise assessment of the performance of the
method. There are a total of 108 solved example problems, in which the solution is explained
step-by-step and through error assessments. Furthermore, maximum effort has been put into
preparing examples and end-of-chapter exercise problems from various disciplines, not only
to demonstrate the relevance of the methods in S&E but also to illustrate the strengths and
weaknesses of the presented methods. Each chapter ends with a Closure that summarizes
the key features of the numerical methods presented.
C. ATTENTION BOXES
In order to grab the attention of students, key points, good practices, pitfalls, or warnings
in connection with the application of a numerical method are highlighted throughout the
text using the “attention boxes” with a hand pointing to information provided.
D. “PROS AND CONS” BOXES
Numerical methods devised for a specific mathematical problem differ from each other in
objective, scope, degree of accuracy, amount of computation, and so on. In this regard, no
single numerical method is suitable for handling every problem in its class. Consequently,
there are usually several methods to solve a given problem numerically. For this reason, the
basics of a numerical method and associated errors are presented in the theoretical part,
and a clear and concise idea of when the method performs well or poorly is provided as
well. This kind of knowledge is critical and necessary, even if one uses modules or procedures
from available software packages. As a result, the ability to select a suitable method for a
numerical task is crucial for the accurate and successful implementation of any numerical
method. In this context, after a numerical method is presented, its advantages and disad￾vantages are presented in a “Pros and Cons Box.” With this approach, by the end of the
course, the students are expected to have gained the ability to question and understand the
purpose and limitations of the methods or modules before using them.
E. USE OF PSEUDOCODES
All numerical algorithms in this text are presented as self-contained pseudomodules that
can be converted and used in any program. The modules are written as simply as possible
(with very few commands) so that translation to other programming platforms is as easy
as possible. This approach blends the algorithms and methods with sufficient mathematical
theory and supports easy implementation of the algorithms.
Why pseudocode? Today’s engineers and scientists are expected to become proficient
in one or more programming languages (C/C++, Python, Visual Basic, Fortran 90/95,
etc.), as well as a number of software packages (MatLab, Mathematica, Maple, etc.) known
as the Computer or Symbolic Algebra System (CAS or SAS). To be more general and
inclusive, the text is not tied to any specific programming language or to any specific CAS
software. This approach aims to teach numerical methods that students will encounter in
their future courses or careers while enabling them to gain the ability to prepare and/or
use well-structured programs in a variety of languages.
There are basically three main reasons why not to adopt popular “programming lan￾guages” or CAS software:
(1) Experience has shown over the last several decades that programming languages
and/or software are highly transient in nature. They can become partially or completelyPreface  xxiii
popular in one field (or course) or unpopular in another, or they can be replaced very quickly
by other language or software in certain disciplines.
(2) Teaching numerical methods together with a particular programming language, or
CAS, runs the risk of blinding the student to the underlying principles of the methods.
Such practices inadvertently encourage students to simply copy, compile, and run the given
source codes (or simply use or type the specified built-in function, procedure, or module)
rather than attempt to understand the methods in depth, including their limitations or
advantages. Furthermore, this attitude generally leads students to believe that any method
can be applied to similar problems by utilizing readily available codes or software without
having any knowledge or sufficient understanding of the relevant details.
(3) A majority of problems that scientists and engineers face today cannot be tackled
by employing a single method (or ready-to-use modules). At some point, programming pro￾ficiency is required to implement a series of numerical methods within a broader computer
program to achieve a more global computational objective. In this respect, the best way to
learn numerical methods is to reinforce the application skills with computer programming
while taking the “Numerical Methods” course.
The motivation for adopting a pseudocode approach is that the texts providing brief
algorithms target an audience with some programming experience. A beginner-level under￾graduate student without sufficient hands-on programming skills, in general, struggles to
convert such algorithms into running computer programs. The pseudocodes adopted in the
text mimic the full features of a real program, including input/output statements, struc￾tured coding, and line-by-line explanatory comments and annotations, so that translation
to other programming languages is as simple as possible. By incorporating the pseudocode
approach adopted here as a teaching tool, a student with minimum programming skills
should be able to not only grasp the logic of the numerical method but also gain insight on
how to implement it in a programming language of his or her choice.
The text contains a total of 96 pseudocodes. Every numerical algorithm is presented as a
completely independent pseudomodule that can be converted and used with other programs.
Some pseudocodes use one or more of the pseudomodules presented in prior sections or
chapters. This text also puts forth a convention for writing pseudocodes since there is
no common standard. However, basic structured programming statements and commands
of common programming languages are adopted in this pseudocode convention, which is
presented in the Appendix section under the heading “How to Read and Write Pseudocodes.”
With this as a reference, a student with an elementary programming background can easily
follow, write, and convert the pseudocodes to any programming language s/he desires.
F. END-OF-CHAPTER “EXERCISE” PROBLEMS:
The text includes more than 1500 end-of-chapter exercise problems presented at the end
of each chapter, organized according to sections. These problems emphasize the mechanics
of the method and are designed to give practice in applying the algorithms presented. The
majority of the exercises are made up of problems with true (exact) solutions, allowing for
an accurate quantitative performance assessment of the methods. Some problems that are
purely mathematical in nature are designed not only to reinforce methods through practice
but also to demonstrate the strengths and weaknesses of a method. Additional applied
or realistic problems are chosen from S&E disciplines to help reinforce understanding of
methods and demonstrate their relevance in applied problems.
The exercises are designed to be assigned to students as self-study or homework prob￾lems. Some exercises do require computer programming to carry out calculations. Even inxxiv  Preface
such cases, it is important for the students to solve most of the exercise problems or, at
least, the first few steps by hand or by using spreadsheets to ensure full comprehension of
the details of an algorithm before attempting to program it. Such exercise problems can
also be used to validate the results of a computer program or to compare them with the
results of other available software.
G. END-OF-CHAPTER “COMPUTER ASSIGNMENTS”
It is important for students to be able to not only select and use suitable modules (methods)
from the pertinent software but also program the numerical algorithms in a language of
their (or the instructor’s) choice. To develop programming and parametric analysis skills,
additional exercise problems 115 are provided as “Computer Assignments” at the end of each
chapter, which require using and/or writing and running computer code (of the instructor’s
choice) and analyzing the results. The instructors may use or modify some exercises or
computer assignment problems to allow methodological analysis and comparisons. By the
end of the course, the student should realize and appreciate the importance of this skill in
his profession.
A NOTE TO THE INSTRUCTOR
This author’s experience and thoughts on teaching numerical methods were shaped as fol￾lows: applying the course contents effectively to practical problems requires theoretical
knowledge as well as computational experience. The knowledge of how well or poorly a
method will perform is extremely important and necessary. The student should be able to
understand the purpose and limitations of a method or module to know whether it applies
to his problem or not. Most problems that today’s professionals deal with in practice require
the use of several numerical methods and cannot be simply solved by adopting a standard
module or subprogram. For such problems, students should be able to find solutions by
putting together a number of suitable numerical methods and developing skills in adopting
numerical methods for a new problem.
The course instructor has a significant influence on the direction of the course and what
students can gain from it. In this context, this text empowers the instructors to utilize Excel
and/or MatLab, Mathematica, MathCad, and similar CAS software. Besides implementing
readily available modules or procedures specific to a particular algorithm, students can
be directed to develop simple, well-structured programs to extend the base capabilities of
such software environments. Alternatively, this strategy can be employed in programming
languages such as Python, C/C++, Visual Basic, Fortran 90, and so on. Combined with
realistic problems that require analysis, this approach allows students to be able to identify,
select, and employ algorithms (or modules) for a specific task, allowing them to reinforce
the numerical methods by experimenting with competing methods and helping them to
explore their advantages and limitations.
I will be maintaining the webpage, set up specifically for this text to aid the students
as well as instructors, which can be accessed at
https://github.com/zaltac/NumMethodsWPseudoCodes
It will be dynamic, and I will be posting modules in C/C++, Python, Visual Basic,
Fortran, Mathematica and MatLab, PowerPoint presentations, solutions to additional ex￾ercises, errata, and more. I encourage the instructors who adopt this text as a coursebookPreface  xxv
to provide me with feedback on how I can improve the materials that I will be sharing on
this website.
A NOTE TO THE STUDENT
Numerical methods is a very different area of mathematics and certainly different from your
previous math courses. In math courses, the tasks are clearly defined, with a very concrete
notion of “the right answer.” Here, we are concerned with computing approximations, and
this involves a slightly different kind of thinking. We have to understand what we are
approximating well enough to construct a reasonable approximation, and we have to be
able to think clearly and logically enough to analyze the accuracy and performance of that
approximation.
The pseudocodes are not intended to be programs with all possible routes or advanced
programming techniques. They are designed in a simple way that will help you understand
and implement the algorithms. Writing your own programs will help you understand how
the method works. In this regard, the computational effort that you put in is an important
part of learning numerical methods.
ACKNOWLEDGMENTS
I am indebted to a number of people for their encouragement and assistance during the
preparation of this and previous texts. I would like to express my gratitude to all of my
students and colleagues for their support and encouragement. Particular thanks are due to
Necati MAHİR, Hasan Hüseyin ERKAYA, Nevzat KIRAÇ, Mesut TEKKALMAZ, Nihal
UĞURLUBİLEK, and Zerrin SERT for proofreading the Turkish or English versions of the
manuscript. I would like to extend a special thanks to Gökçe Mehmet AY for reintroducing
me to the LaTeX world. I would also like to thank Callum Fraser of CNC Press for his
continued interest and advice in making this project possible.
I thank my mother Muzaffer, my siblings Mithat, Afitap, and Asuman, my daughter
Delilah, and my son-in-law Mahmut for their unconditional love and support, as well as my
grandchildren Efe and Meryem, who fill my heart with joy each and every day. Finally, I
thank my wife Leslie for her love, enduring patience, and unconditional support, without
which the book would probably not have been completed.
Zekeriya ALTAÇ
April, 2024.
Eskişehir, Turkiye
e-mail: altacz@gmail.com
https://github.com/zaltac/NumMethodsWPseudoCodesTaylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com Author Bio
Zekeriya Altaç is currently a professor of mechanical engineering at Eskişehir Osmangazi
University and he received his B.Sc. degree in 1983 from Istanbul Technical University in
Turkey. He received a scholarship from the Ministry of Education of Turkey to study nu￾clear engineering in the USA. He attended Iowa State University and earned his M.Sc.
(1986) and Ph.D. (1989) in Nuclear Engineering. Upon returning to Turkey, he worked as
an assistant professor at the Mechanical Engineering Department of Anadolu University
between 1989 and 1992 in Eskişehir, Turkey. Later, he joined the Department of Mechan￾ical Engineering at Eskişehir Osmangazi University as an associate professor in 1993 and
received a tenure-track position in 1998, where he is currently working. His areas of inter￾est and expertise are computational nuclear reactor physics, computational fluid mechanics
and heat transfer, and numerical (computational) methods in general. He is the author
of over 100 journal articles, conference and research papers, books, and book translations.
The author has books in Turkish on computer programming (BASIC, FORTRAN) and the
application of CFD software (ANSYS, FLUENT). Most of his research deals with devel￾oping efficient numerical methods and/or employing computational techniques in practical
scientific and engineering problems. He served as an advisor at the Von Karman Institute
Technical Advisory Committee (VKI-TAC) in Brussels, Belgium, between 2009 and 2011.
He has also served as Eskişehir Osmangazi University’s vice president (2007-2011), as head
of the Mechanical Engineering Department (2012-2021), and held positions on numerous
conferences, symposiums, and academic committees, such as the engineering college board
and university senate.
xxviiTaylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com CHAPTER 1
Numerical Algorithms and
Errors
LEARNING OBJECTIVES
After completing this chapter, you should be able to
‚ state the purpose and benefits of numerical methods in science and engineering;
‚ read and apply pseudocode conventions to understand and implement algo￾rithms;
‚ understand how numerical data is managed in computers;
‚ define absolute/relative errors and use them in error reporting;
‚ understand and apply the error propagation formula in error analysis;
‚ realize the computer representation of numbers in different bases;
‚ distinguish the difference between chopping and rounding;
‚ determine and apply the concept of significant digits;
‚ state the difference between accuracy and precision;
‚ discuss the effects of rounding on floating-point arithmetic operations;
‚ understand the concept of loss of significance and how to avoid it;
‚ determine the convergence rate of sequences;
‚ explain the significance of the condition of a problem and the stability of a
method;
‚ determine the truncation error of a series approximation;
‚ apply the mean value theorems to practical problems;
‚ investigate and identify the truncation error of a positive and alternating series.
N UMERICAL algorithms were first developed for hand calculations in the 19th and
early 20th centuries to tackle mathematical problems such as solving small systems of
linear equations, inverting matrices, approximating integrals, and solving ordinary differen￾tial equations. Early numerical solutions were obtained manually until the introduction of
the computer after World War II. Rapid technological advances led to the development of
the high-speed computers we use today. As a result, making use of the computer’s capability
to perform long sequences of calculations has brought about advances and further techno￾logical developments in many fields, and computers and mathematical sciences naturally
became inseparable.
DOI: 10.1201/9781003474944-1 12  Numerical Methods for Scientists and Engineers
Numerical analysis, a whole new subject born out of the advent of computers, is a
branch of mathematics that deals with the theory, development, and analysis of approxima￾tion methods for solving a wide range of problems in applied mathematics. Having said that,
numerical methods are concerned with numerical algorithms and their applications to solve
science and engineering problems that do not have analytical solutions or are very difficult
to solve. In this respect, most numerical methods only give solutions that are approxima￾tions of the true solutions. Thus, the computed results are only approximations (estimates),
inevitably containing errors that are for the most part specific to the numerical method. For
instance, many problems require an infinite sequence of operations to obtain approximate
solutions, but in practice, only a finite number of these are processed. In such cases, the
level of accuracy of a solution depends on how many sequences of operations the analyst
is willing to perform or execute. Hence, it is important for an analyst to understand and
estimate the errors incurred and, preferably, the lower and upper bounds of the numerical
method implemented.
Over the last several decades, computers have become indispensable in engineering de￾sign and scientific research due to their speed and computational power. Nowadays, comput￾ers are widely used to design programs or software to numerically simulate various physical
events or to design engineering systems that often require the use of multiple programming
languages. As a result of the developments in computer technologies in parallel with tech￾nological developments, the functions demanded from programming languages have both
increased and diversified. Over time, some programming languages can become obsolete as
they lose their ability to meet the needs and requirements of programmers, and outdated
languages are replaced by emerging, more functional languages. In practice, today, a single
programming language is often not enough to meet the needs of engineers or scientists, who
eventually have to be proficient in more than one programming language to achieve their
goals. The main motivation for presenting algorithms in the form of pseudocodes in this
book is to embrace the common programming languages of not only our time but also the
near future by not adhering to a specific programming language.
This chapter introduces the general concept of numerical algorithms as a tool for solv￾ing numerical problems. The implementation of the pseudocode convention adopted in this
text is presented with elementary examples illustrating control and conditional construc￾tions, accumulators, loops, and modular programming. After briefly covering various error
types and error sources encountered in mathematical modeling and numerical simulation
of physical problems, errors arising due to floating-point representation of numbers and
floating-point arithmetic during the storage and processing of numbers by computers are
also examined. The general concept of error propagation and its application in error anal￾ysis are also presented. Different strategies for minimizing computational errors are also
given. Finally, the concepts of truncation error, convergence, and stability are illustrated
with commonly encountered problems.
1.1 NUMERICAL ALGORITHMS IN PSEUDOCODES
A numerical algorithm is a set of instructions outlining how to carry out the numerical
solution of a certain mathematical problem. Although most algorithms for short and/or
simple calculations can also be carried out by hand, the term numerical algorithm almost
always implies a procedure for computer implementation to obtain an approximate solution
to a mathematical problem.
The task of selecting and implementing a numerical algorithm for a particular prob￾lem is not always easy. In most cases, there are numerous algorithms devised to solve aNumerical Algorithms and Errors  3
FIGURE 1.1: Process of finding a numerical solution of a problem.
class of mathematical problems. These algorithms usually differ from each other in accu￾racy, efficiency, reliability, and/or stability, which results in varying performances. Some
algorithms yield results very quickly, while others either diverge or converge slowly due
to their unfavorable convergence properties. In brief, analysts need to seek and implement
algorithms that are not only accurate and reliable but also efficient and robust, which is
easier said than done. Selecting a suitable algorithm for a problem is just the beginning of
the problem-solving process.
The numerical solution to a problem often requires putting together a collection of nu￾merical algorithms, involving, for instance, integration, interpolation, the solution of matrix
equations, and so on. An analyst has to select and implement the most suitable algorithms
in order to obtain the best possible numerical solutions for his problems. Once the overall
numerical algorithm for a problem is established, a computer program can then be devised
and converted into an executable program.
A typical process flowchart for finding the numerical solution to a problem via com￾puters is presented in Fig. 1.1. Once a suitable algorithm has been designed, the rest of the
process is carried out on the computer, a device capable of carrying out a set of instructions
communicated by a computer program. A computer program is basically a tool that directs
the computer to do the tasks that we want it to do. In this context, there are two kinds of
computer programs: (1) human-readable programs and (2) machine-readable programs. A
computer programmer uses a suitable programming language to convert the algorithm into a
human-readable (instructions) program, which is referred to as source code. No matter what
the programming language is, the source code is also an artificial language that still needs
to be translated into machine-readable (binary) instructions so that it can be executed by
computers. Translating instructions into machine language is carried out by a compiler–
software that creates a machine-readable executable program. An executable program can
then be run as many times as needed to find approximate solutions for various scenarios.
An algorithm eventually has to be programmed using a programming language to
create an executable program. However, there is not just one programming language out
there; there are many! Computers are used almost everywhere, from household appliances
to cell phones, from artificial intelligence to manufacturing, and so on. That is why many
computer programming languages have been developed to date, as each language has its own
limitations. Web developers utilize HTML, PHP, and JavaScript to create web applications,4  Numerical Methods for Scientists and Engineers
while Kotlin and Java are used for mobile applications. Scientists and engineers make use of
C, C++, Python, Visual Basic, Pascal, Fortran, and so on, along with symbolic processing
software such as MatLab, Maple, Mathematica, etc. In short, professionals in different fields
use different programming tools because different projects have different needs and goals,
and no single language usually meets the needs of the users.
A computer is a tool that provides an environment for running programs; it merely
executes the instructions prepared by its programmer. Sometimes a program may contain
error(s), flaw(s), or fault(s) that produce an incorrect or unexpected result or behave in
unexpected ways. We cannot blame computers for any one of these undesirable results
because a computer does exactly what it is instructed to do. So the question is, Who
is to blame? The blame may lie in the choice or misapplication of the algorithm(s) or
outright programmer error. If a suitable numerical algorithm is chosen and implemented
correctly with sufficient clarity, then the programmer’s errors can be justifiably questioned.
Programmers should check the coding of the algorithm, test it module by module and as
a whole to determine whether it works properly, and make necessary corrections. In this
respect, a well-prepared algorithm before writing a computer program saves the programmer
from many troubles (writing, checking, debugging, etc.). A successful algorithm is one that
can be understood by a programmer (who may not even be a numerical analyst) and can
be turned into a computer program that generates the correct results.
1.1.1 ELEMENTS OF A PSEUDOCODE
In this text, numerical algorithms are presented in pseudocodes. A pseudocode is simply
a text-based fake code that specifies a detailed description of an algorithm. Presenting an
algorithm in pseudocode has the following benefits:
‚ It mimics structured programming languages, describing the step-by-step implemen￾tation of an algorithm;
‚ It is intended for human rather than compiler interpretation, so it is a lot easier for
people to follow, relate to, and understand;
‚ It is a language-independent, efficient way of characterizing the key features of an
algorithm;
‚ Well-thought-out and well-crafted pseudocodes contribute to an easier and faster
translation into an actual programming language and save programmers a lot of trou￾ble when debugging;
‚ Pseudocodes address all programmers, regardless of their programming background
or knowledge.
For the reasons stated above, numerical algorithms in this textbook are presented in
pseudocodes. There is, however, no generally agreed-upon convention or accepted standard
norm for writing pseudocodes nor is there a standard, well-defined pseudocode syntax.
Moreover, pseudocodes are not compiled and executed by computers; they are processed by
humans. Thus, the correctness of a pseudocode cannot be verified until it is programmed,
compiled, and run. For this reason, it is important that pseudocodes are clear and concise.
In this context, the pseudocode style adopted in this textbook omits many of the details
of a real programming language yet contains roughly the same level of detail, so that the
pseudocode is complete enough to allow line-by-line translation into any source code.
In order for a pseudocode to achieve its intended objective, the following characteristics
should be adhered to when designing a pseudocode:Numerical Algorithms and Errors  5
‚ Definiteness: A pseudocode must specify every step and should follow a sequence of
steps (or instructions) in the right order. It must be clear and unambiguous, and it
should be detailed enough to include error handling;
‚ Finiteness: A pseudocode must terminate after a finite number of steps with either
an output or error message, and it should not go into an infinite loop or terminate
without any clue;
‚ Effectiveness: A pseudocode must be free of unnecessary or redundant steps or in￾structions.
Pseudocodes, like real programs, have three basic elements:
1. Input(s). A pseudocode requires at least one or more inputs, which include the form,
amount, and type of data;
2. Statements. It is the body of the algorithm; a sequence of expressions, commands, and
operations (looping, branching, conditionals, etc.) is embedded here;
3. Output(s). It should produce at least one (or more) output (form, type, amount, etc.)
that is the intended result.
A short guide on how to read and/or write pseudocodes (a summary of pseudocode
syntax, statements, structures, etc.) is presented in Appendix A. The body (statements
block) of the pseudocode must describe precisely the order of instructions, statements,
modules, and so on in the program.
A pseudocode, just as in normal programming languages, contains three basic con￾structs: sequence, selection, and iteration (or repetition).
Sequence constructs: The statements are processed and executed in the same se￾quential order they are written in the program.
Selection (branching) constructs: These constructs (If-Then and If-Then-Else) de￾cide which of the two sets of expression blocks will be processed. The selection constructs
start by testing a condition (a Boolean result: True or False). Depending on the result, the
program is directed down in one of the two available paths.
Iteration (repetition or looping) constructs: There are two kinds of repeti￾tion structures: count-controlled iteration (For-construct) and condition-controlled itera￾tion (While- and Repeat-Until). A block of instructions is repeated a number of times until
a condition is met.
Think of a pseudocode (i.e., algorithm) very much like a cake recipe. Inputs are the
ingredients (with measurements and amounts): 1 cup of butter, 3 cups of flour, 4 eggs, and
so on. Statements, the main body of the cake recipe, contain step-by-step instructions for
making a cake, e.g., (i) mix cream, butter, and sugar until fluffy; (ii) add eggs and beat
well; (iii) sift flour, baking powder, and salt into a bowl, etc., and so on, and finally (ix)
bake in a preheated oven at 190˝C for 25 minutes. When one follows these steps exactly
with the correct input (ingredients and measurements), the output (i.e., the final product)
is a baked cake. In other words, the success of an end result (the accuracy of the output)
is determined by how well and accurately a process is described in the statements section.
Notice that all instructions in this recipe are not only specified without skipping any critical
steps but also itemized in the correct order (definiteness). The process of making a cake
requires a finite number of steps (finiteness). The recipe does not contain any instructions
such as add ground beef to the pot or add a tea spoon of cumin to the sauce, which would
be irrelevant since none of these instructions are part of making a cake (effectiveness).6  Numerical Methods for Scientists and Engineers
1.1.2 SEQUENTIAL CONSTRUCTS
The instructions in sequential constructs are processed one after another in a sequence.
Even though each instruction in a code is stated in a specific order, some of the instructions
may be interchangeable. For example, consider estimating the cost of the paint for a large
room (width ˆ length ˆ height). A pseudocode segment may be given as follows:
Read: width, length, height \ Read in the room dimensions
Read: paint \ Read the cost of paint per unit area
A1 Ð 2 ˚ width ˚ height \ Find area of front & back walls
A2 Ð 2 ˚ length ˚ height \ Find area of left & right walls
Atop Ð width ˚ length \ Find area of ceiling
A Ð A1 ` A2 ` Atop \ Find total surface area
cost Ð A ˚ paint \ Calculate cost of paint
Write: “Cost of paint is”, cost \ Print out cost of paint
In the first and second lines, the room dimensions and paint cost (per unit area) are
supplied into the code, respectively, using Read statements, the order of which can be
changed. Next, the total room surface area should be determined to estimate the cost of
the paint. Following Read statements, the calculation of front-back, left-right, and ceiling
wall areas is carried out with three statements. Note that these three statements are also
interchangeable. The following (fourth) statement gives the total surface area. The fifth
statement computes the cost of paint by multiplying the total area by the cost of the paint
per unit area. Finally, the cost is printed out as output information. It should be pointed
out, however, that the last three statements are not interchangeable.
1.1.3 CONDITIONAL CONSTRUCTS
Selection of the part of a code to be executed is done with conditional (If-Then or If-Then￾Else) constructs. An If construct is a logical (boolean) expression that takes the value True
or False. Recall that logical expressions are built with relational operators (ą, ě, ă, ď,
=, ‰) and may be combined with logical operators (And, Or, Not) to form more complex
logical expressions.
Consider the following pseudocode segment:
Read: name, grade \ Read in name and grade
Write: name,“Your grade is”,grade \ Print out name and grade
If “
grade ă 50‰
Then \ If-1, Case of grade ă 50
Write: “NP” \ Print NP (Not Passed) as letter grade
Else \ Case of grade ě 50
Write: “P” \ Print P (Passed) as letter grade
End If
If “
grade “ 100‰
Then \ If-2, Case of grade “ 100
Write: “CONGRATULATIONS”, name \ Print “congratulation”
End If
First, the student’s name and grade are read into the code and printed out in the next
statement. Here, both If-Then-Else and If-Then-constructs are used in this code segment.
The If-Then-Else-construct 1, which is based on grade ă 50 (a logical expression), is used
for printing out the student’s letter grade as NP (Not Passed) or P (Passed) for grade ě 50.
Finally, in the If-2, the message “CONGRATULATIONS name” is printed for the student
who received 100.Numerical Algorithms and Errors  7
More complicated If-structures are constructed by nesting If-Then-Else-constructs. Con￾sider the following step-wise varying function:
fpxq “
$
&
%
2 ´ x, x ă ´1
x2 ` 2, ´1 ď x ď 3
3x ` 2 3 ă x ď 5
A pseudocode segment of this function is established by nesting three If-Then-Else￾constructs as follows:
Read: x \ Read x from file or console
If “
x ă ´1
‰
Then \ If-1, case of x ă ´1
f Ð 2 ´ x
Else \ case of x ě ´1
If “
x ď 3
‰
Then \ If-2, case of ´1 ď x ď 3
f Ð 2 ` x2
Else \ case of x ą 3
If “
x ď 5
‰
Then \ If-3, case of 3 ă x ď 5
f Ð 3x ` 2
Else \ case of x ą 5
Write: “x is Out-of-Range.” \ Print out a warning
End If \ End of If-3
End If \ End of If-2
End If \ End of If-1
Note that three nested If-Then-Else-constructs are used to define the function with respect
to step intervals. If x ă ´1 (If-1), then fpxq “ 2´x is processed; otherwise, x corresponds to
an x ě ´1 interval that requires further refining. In If-2, fpxq “ 2 ` x2 is executed if x ď 3,
which falls within ´1 ď x ď 3. If x is not in ´1 ď x ď 3 interval, then it must be greater
than 3. Next, If-3 narrows down the interval to 3 ă x ď 5, for which case fpxq “ 3x ` 2 is
evaluated; otherwise, to ensure correct data processing, an “x is Out-of-Range” message is
printed for x ą 5 since fpxq is not defined for this interval.
1.1.4 LOOPS AND ACCUMULATORS
Most programming languages are furnished with sequential and conditional loop constructs.
These loops are represented by the For-(definite counting loop), While-, and Repeat-Until
(indefinite counting loops) constructs.
A common task in most algorithms is to find the sum (or product) of a series of values.
This is accomplished through the use of an accumulator. An accumulator is a variable that
a program uses to perform the sum (or product) of successive values in a loop construct.
For example, the following pseudocode segment illustrates the sum of integer numbers from
1 to 10 using a For-construct (a counting loop).
S Ð 0 \ Initialize accumulator by setting S “ 0
For “
k “ 1, 10‰ \ Iterate block statement for k “ 1 to 10
S Ð S ` k \ Add present value of k to S; the accumulator
End For \ Increment counter variable, k ` 1
To visualize how this code works, the process of a series sum or a product using an
accumulator is illustrated in Fig. 1.2. In Fig. 1.2a, the accumulator is initialized by zero8  Numerical Methods for Scientists and Engineers
FIGURE 1.2: Schematic illustration of the accumulator process for (a) sum and (b) product.
(S Ð 0), which is moved into the memory location of S. The loop index (or counter) is k,
and the loop (iteration) block consists of only one expression: S Ð S ` k. Starting with
k “ 1, the rhs of this expression is processed first. The value on the rhs (0 ` 1 “ 1) is
moved to the memory location of the accumulator as denoted by the Ð symbol, where its
prior “0” value is overwritten by “1” (S Ð 1). Note that in Fig. 1.2 the memory location
of S is denoted with boxes. For k “ 2, the most recent value of the accumulator is used to
calculate the rhs (S Ð 1 ` 2), and the sum is moved to the lhs (the memory location of
the accumulator), overwriting on its previous value, i.e., S Ð 3. The process is repeated as
long as the k ď 10 condition remains True, which is executed for k “ 1 to 10 with Δk “ 1
increments (see flowchart in Appendix A). The procedure gives the sum řn
k“1 k for any n.
This construction can be easily extended to any řn
k“1 ak by simply replacing “k” with the
general term of a series, i.e., akf(k).
The procedure outlined above can also be thought of as generating a sequence using
the Sk “ Sk´1 ` ak recursion formula for k “ 1, 2,...,n with S0 “ 0. For every k, the
corresponding Sk is the partial sum from 1 up to k: i.e., Sk “ řk
i“1 ai.
The computation of 10!, which requires a series of multiplications, is illustrated using
an accumulator in Fig. 1.2b. The accumulator is initialized as S Ð 1, which is stored in
the memory location of S. The loop index remains k, and the loop block is replaced by
S Ð S ˆ k. Starting with k “ 1, the rhs becomes “1” (1 ˆ 1 “ 1), and it is carried to
the memory location of S. For k “ 2, the most recent value of the accumulator is used
to calculate the rhs as S Ð 1 ˆ 2. This product is moved to the memory location of S,
overwriting the accumulator’s previous value of “1.” This process is repeated in the same
way until the k ď 10 condition becomes False (i.e., k ą 10). This process gives śn
k“1 k for
any n. This construction can be extended to any śn
k“1 ak by replacing “k” with a general
term, ak. This process can also be represented as a recursive procedure by setting S0 “ 1.
Then, a sequence of values is generated according to the Sk “ Sk´1 ˆ k recursion formula
for k “ 1, 2, ..., n, which eventually yields Sk “ k! for an arbitrary k.
Always indent the body of the statement(s) block in If-Then-, If￾Then-Else, or loop constructs by two or more spaces to highlight
the blocks and improve the readability of the code.Numerical Algorithms and Errors  9
Using a While-construct, we consider evaluating the same summation. A pseudocode
segment may be given as
Initialize: n \ Initialize number of terms
S Ð 0 \ Initialize accumulator
k Ð 0 \ Initialize counter
While “
k ď n
‰ \ Execute block so long as k ď n
k Ð k ` 1 \ Increment counter by `1
S Ð S ` k \ Add present value of k to S; the accumulator
End While \ Go to top of the loop
First, the number of terms, n (a user-supplied value), is provided. Next, the accumulator
is initialized by assigning S Ð 0. The While-loop cycles an unknown number of times until
it stops when a criterion that does not necessarily involve the loop control variable is met.
However, in this pseudocode segment, a stopping criterion (k ď n) is tied to the iteration
counter (k). Hence, the iteration counter k is also initialized as k Ð 0. This example
requires two accumulators: one for the counter and the other for the summation. However,
the counter is placed before the accumulator so that the loop can run from k “ 1 to 10. The
loop condition (k ď n) is evaluated before the iteration block. The exchange of data in the
memory location of the accumulator and the index variable works in the same way. While
counting the number of terms (k), the value of the accumulator in the memory location is
continuously updated, leading to a recursive operation. The block statements (k Ð k ` 1
and S Ð S ` k) are processed n-times until the counter limit (k ą n) is reached.
Next, we code the same example by using a Repeat-Until construct. The pseudocode
segment is given below:
Initialize: n \ Initialize number of terms
S Ð 0 \ Initialize accumulator
k Ð 0 \ Initialize counter
Repeat \ Execute block statements
k Ð k ` 1 \ Increment counter by `1
S Ð S ` k \ Add present value of k to accumulator
Until “
k “ n
‰ \ Exit loop when k “ n, else go to top of the loop
The procedure works pretty much the same as the While-construct; however, the loop
stopping (termination) condition (k ą n) is evaluated at the end of the loop (see Appendix
A). After initializing the counter and accumulator (k Ð 0 and S Ð 0), the iteration block
is executed, resulting in k Ð 1 and S Ð 1. The logical expression becomes 1 ă n, which is
True for n “ 10. The second iteration yields k Ð 2, S Ð 3, and 2 ă n (True). The iteration
block is executed in the same way for k ą 2 until the loop condition becomes False (i.e.,
k ą n). Note that a counter variable is not required for neither While- nor Repeat-Until
construct, only the stopping criterion must be satisfied.
In programming, the letters i, j, k, m, n, etc. are generally adopted
as counter variables because they are also used as index variables
in mathematics (e.g., řn
i“1 ai or śn
j“1 bj ). Declare a counter vari￾able as Integer because it is operationally easier and faster to in￾crease the value of an integer-type variable.10  Numerical Methods for Scientists and Engineers
Modular Programming
‚ A module can be reused as many times as needed; thus, modular
programming reduces program complexity by eliminating repetition
of tasks and results in shorter codes;
‚ It makes reading and understanding a code simpler and easier;
‚ Maintaining or debugging codes is easier since errors are localized in
a module that can be independently modified and tested;
‚ Software development is easier since each team focuses on a smaller
part of the entire code;
‚ A module can also be used in other projects that require the process￾ing of the same task;
‚ Establishing a library of modules that work reduces (coding, debug￾ging, etc.) the project completion time.
‚ Modular programming may require extra time and/or budget for the
program development of a project;
‚ It is a critical, yet cumbersome, task to prepare “complete and de￾tailed documentation” for each module, which is communicated be￾tween the design teams;
‚ Preparing a collection of modules can be a challenging task.
1.1.5 PSEUDOCODES, FUNCTION MODULES AND MODULES
A program or module in any programming language is organized as follows: name, list
of variables, input statements, processing/computing blocks (involving sequential and/or
conditional constructs), and output statements. Every program or module has a name and
a list of internal (specific to the module) and/or external (common with other modules)
variables. In this respect, pseudocodes are organized in the same way.
Modular programming is the name of the general concept of dividing a complicated
computer program into smaller yet separate modules (i.e., procedures or sub-programs).
Each module (sub-program or procedure) comprises everything that is needed to carry out
only one aspect of the functionality. Modules can be utilized for a range of applications and
functions in conjunction with other modules in a general program. A module may also make
use of other module(s). Many programming languages allow two types of modules (proce￾dures, subroutines, sub-programs, etc.) and functions (built-in and user-defined). Hence, it
is wise to adopt a modular coding approach to write efficient programs. The advantages
of modular programming outweigh its disadvantages; thus, in this text, every method has
been presented with a pseudocode module.
Pseudocode 1.1 is an example of a pseudo-program containing various elements of the
pseudocode convention presented in this text. All comments and annotations are distin￾guished from the actual code statements with light fonts. The pseudo-program is delimited
by the Program-End Program statement. It contains a header block with a brief description
of the code. Also following the USES statement, any user-defined Module and/or Function
Module used in the code (i.e., program dependencies) are specified. In this example, two
user-defined modules, QUADRATIC_EQ and FUNC, are identified in the header block. The
square root function, commonly named SQRT, is available as a built-in function in most
programming languages. Throughout this text, built-in functions such as square root, abso￾lute value, and so on have been used in their symbolic forms but are explicitly declared inNumerical Algorithms and Errors  11
Pseudocode 1.1
Program EXAMPLE \ First executable statement: define program name
\ DESCRIPTION: A pseudocode illustrating general features of a pseudocode.
\ USES:
\ FUNC :: User-defined function, Pseudocode 1.2;
\ QUADRATIC_EQ :: A pseudomodule finding the roots of a quadratic equation.
Declare: an, bn, cn, re2, im2 \ Declaring array variables
Read: u,v \ Read in u and v from console or a file
Read: a, b \ Read in arrays a and b of length n from a file
u Ð u ` 2 ˚ v \ Arithmetic expression-1 is assigned to u
v Ð v ` u ˚ u \ Arithmetic expression-2 is assigned to v
w Ð FUNCpu, vq \ Set function FUNC value to w
c Ð 2 ˚ a ` 3 ˚ b \ Computes ci “ 2ai ` 3bi for all i
Write: “c=”,c \ Print ci for i “ 1, 2,...,n
c Ð a ˚ b \ Compute ci “ aibi for i “ 1, 2,...,n
Write: “c=”,c \ Print ci for i “ 1, 2,...,n
d Ð c1 ˚ cn \ Arithmetic expression-3 is assigned to d
If “
d ą 0
‰
Then \ Case of d ą 0
QUADRATIC_EQ(u, d, re, im) \ Invoke Module QUADRATIC_EQ
Else \ Case of d ď 0
QUADRATIC_EQ(u, ´d, re, im) \ Invoke Module QUADRATIC_EQ
End If
Write: “Root 1=”, re1, im1 \ Print out x1 “ re1 ` i ˚ im1
Write: “Root 2=”, re2, im2 \ Print out x2 “ re2 ` i ˚ im2
End Program EXAMPLE \ End of Program
the USES statement with their full names. The array variables (an, bn, cn) of length n and
arrays (re2, im2) of length 2 are declared with the Declare statement. If the types of the
other (non-array) variables are not obvious from the context of the pseudocode, their type
declarations (Real, Integer, Logical, etc.) should be provided to leave no room for doubt.
Reading the initial values of u and v from an input file into the program is indicated
by the “Read: u, v” statement. Similarly, the “Read: a, b” statement indicates that two
arrays (a and b) of length n are read into the code (likely from an input file) as whole￾arrays. In the next eight lines, each line is processed in the order in which it is written. The
w is computed by invoking the FUNC function. Since a and b are arrays (vectors) of the
same length, whole-array arithmetic (see Appendix A) is incorporated as c=2*a+3*b and
c=a*b. The Write statements are used anywhere in the code to report any intermediate or
final value of variable(s). The pseudomodule QUADRATIC_EQ is invoked within conditional
statements with different argument lists. The real re2 and imaginary im2 parts of the roots
are then printed out by using the Write statements.
A typical structure of a pseudo-Function Module is illustrated in Pseudocode 1.2. A
function of two independent variables is given as funcpx, yq “ MODpx, yq
ax2 ` y2. The
module makes use of two built-in functions (MOD and SQRT), which are also identified with
the USES statement. (In this text, an effort has been made to use built-in function names
common to most programming languages.) This module checks the validity of the input
data to determine a course of action to achieve effectiveness, finiteness, and definiteness
properties. The case of y ą 0 and |x| ě 10 (If-1 condition becomes True) indicates that the
arguments are within the validity range. Then the mathematical operation is carried out,12  Numerical Methods for Scientists and Engineers
Pseudocode 1.2
Function Module FUNC (x, y)
\ DESCRIPTION: A pseudo-function module defined as funcpx, yq=MODpx, yq
\ ˆ
ax2 ` y2 for y ą 0 and |x| ě 10, where x and y are integers.
\ USES:
\ MOD:: Built-in function returning the remainder of x divided by y;
\ SQRT:: A built-in function computing the square-root of a value.
Declare: x, y \ Type declarations
If “
y ą 0 And |x| ě 10 ‰
Then \ If-1, arguments within range?
F ÐMOD(x, y) \ Built-in function is invoked
FUNC Ð F ˚
ax2 ` y2 \ Set rhs as FUNC
Else \ Identify argument not in range
Write: “Error: Illegal Input” \ Print out an error message
FUNCÐ 1030 \ Set a large value for FUNC
If “
y ď 0
‰
Then \ If-2, y has the illegal input value
Write: “Input y is”, y \ Print y to indicate y ď 0
End If
If “
|x| ă 10‰
Then \ If-3, Case of x with an illegal input value
Write: “Input x is”, x \ Print x to indicate |x| ă 10
End If
End If
End Function Module FUNC
and the result is assigned to FUNC (i.e., the function name) before exiting the module. If x
or y or both fall outside the validity ranges, the If-1 condition becomes False, for which the
user is warned with the “Error: Illegal Input” message, and FUNC is a large set to value.
If-2 and -3 are used to check if x or y are out of range, respectively, and print the input
value(s).
The module QUADRATIC_EQ presented in Pseudocode 1.3 is an example to illustrate
the general structure and features of a pseudomodule. The module is intended to give the
roots of a quadratic equation of the form x2 `px`q “ 0 under all conditions. Note that the
sub-program is delimited by Module-End Module statements. The arguments of the module
are p, q, re, and im. Of these, p and q are the scalar input variables denoting the coefficients
of the quadratic equation, re and im, are the output array variables of length 2 spared for
the real and imaginary parts of the roots. Annotations can be inserted into steps (or lines)
deemed necessary to explain any intended actions. The discriminant is computed first. All
real and imaginary roots are determined using an If-Then-Else construct with d ă 0 as
the condition. Since we only need ?
d from this point forward, we may assign d Ð ?
d (or
d Ð ?´d in case of d ă 0) to save from memory space. The real and imaginary parts of
the roots are evaluated and saved on rek and imk for k “ 1, 2.
An example of a pseudo-recursive function module is presented in Pseudocode 1.4.
The module FACTORIAL with a single argument n is intended to give n! An If-Then-Else
construct (If-1) is used to determine the paths for n ă 0 and n ě 0. For n ă 0, the code
echoes the value entered with an error message and suggests a remedy. The If-2 provides
paths for n ď 1 and n ě 2 conditions. The FACTORIAL is set to “1” for n “ 0 or 1. For
n ě 2, the number is passed to the function, where n is multiplied by pn ´ 1q! This number
is passed again to the function, where it is multiplied with the factorial of n ´ 2, and so on.
This procedure is repeated until n reaches 1.Numerical Algorithms and Errors  13
Pseudocode 1.3
Module QUADRATIC_EQ (p, q, re, im) \ Name & variable list of the module
\ DESCRIPTION: A pseudo module to find the roots of a quadratic equation
\ of the form x2 ` px ` q “ 0.
\ USES:
\ SQRT:: A built-in function computing the square-root of a value.
Declare: Real p, q, re2, im2 \ Type declaration of arguments
d Ð p ˚ p ´ 4 ˚ q \ Compute discriminant, Δ
If “
d ă 0
‰
Then \ Case of imaginary roots, Δ ă 0
d Ð ?´d \ Evaluate d “ ?´Δ
re1 Ð ´p{2; re2 Ð re2 \ Real parts are the same
im1 Ð ´d{2; im2 Ð ´im1 \ Imaginary parts have opposite signs
Else \ Case of real roots, Δ ě 0
d Ð ?
d \ Evaluate d “ ?
Δ
re1 Ð p´p ´ dq{2; re2 Ð p´p ` dq{2 \ Real parts of the roots
im1 Ð 0; im2 Ð 0 \ No imaginary parts
End If
End Module QUADRATIC_EQ \ End of the Module
Pseudocode 1.4
Recursive Function Module FACTORIAL (n)
\ DESCRIPTION: A pseudo-function (recursive) module for computing n!
Declare: Integer n, FACTORIAL \ Type declarations
If “
n ă 0
‰
Then \ If-1, validity range check (n ă 0 ?)
Write: “Error: Illegal Input” \ Issue an error message
Write: “You entered”,n,“ Enter n ě 0” \ Echo input and guide user
Exit \ Exit the module
Else \ Case of n ě 0, compute n!
If “
n ď 1
‰
Then \ If-2, Case of n “ 0 and 1
FACTORIALÐ 1 \ Set FACTORIAL=1 since 0! “ 1! “ 1
Else \ Case of n ą 1
FACTORIALÐ n˚FACTORIAL(n ´ 1) \ Compute n! “ n ˚ pn ´ 1q!
End If \ End of If-2
End If \ End of If-1
End Recursive Function Module FACTORIAL
1.1.6 TIPS FOR EFFICIENT PROGRAMMING
Regardless of your computer programming background, you can improve your programming
skills, reduce programming and debugging time, and increase the efficiency of your programs
by adhering to the following recommendations:
1. Take your time. To be able to see the “big picture,” make a flow chart of the
algorithm with all possible scenarios (including bounds-checking for inputs, providing error
paths, etc.). Make a list of the variables, functions, and sub-programs that you will need.
2. Establish a “list of variables” for each program or module. First of all, use
meaningful names for the variables, constants, modules, and so on to avoid confusion. A list
of variables, containing a full list of input and output variables used in a program or module,14  Numerical Methods for Scientists and Engineers
should be included in the header of any module or program. Here, we will skip identifying
the input and output variables in the header block to save space. However, explanations
of the variables are presented in the code description paragraph. It is also a good practice
to define (by commenting) the internal variables within the code. The list should include
the description as well as any units, if applicable. Although this practice may seem like an
unnecessary burden or a waste of time, it is nonetheless invaluable when you or someone
else needs to modify the program at a later time.
3. Echo all input variables. Displaying any data immediately after it is read into
a program is called echoing. Especially when a code is being developed, it should contain
several temporary or permanent Write statements that echo input and display even some of
the intermediate results. Failure to do so may lead to erroneous results due to incorrectly
typing the input data, of which the user would be unaware. Also, echoing any input data
supplied to a code ensures that the number of data, their type, and their values are not
only correctly supplied but also correctly received by the code. This trick eliminates some of
the compiler and runtime errors such as “missing input data,” “type mismatch,” “undefined
variables,” and so on.
4. Split large programs’ tasks into smaller sub-programs. Any specific task
that is performed a number of times in a program or subprogram can be defined as a
separate subprogram (module, procedure, or function). Creating a library of modules or
procedures reduces the complexity of a program by eliminating repetitions and increasing
the readability of the code, giving it a modular structure. Apart from being independent
from the rest (and possibly undesirable elements) of the program, the procedures offer the
benefits of easy testing and reusability. Implementing any built-in functions or procedures
available in compiler software reduces coding and debugging time, thereby reducing the
overall program development time.
5. Design programs to reduce memory requirements. Avoid using arrays (or
arrays larger than actually required) unless they are absolutely needed because large arrays
use up a lot of memory, leading to a large program that is much larger than it should be.
Also, if the size of an executable file is too large, it may not run on a particular computer.
Keep in mind that the use of an array is not justified unless a problem requires a list of
input data (such as vectors or matrices) to be retained in memory at the same time.
6. Design programs to reduce cpu-time. The cost of arithmetic and logical opera￾tions depends on the central processing unit. In the programming of numerical algorithms,
a significant portion of the cpu-time is devoted to floating-point arithmetic and logic opera￾tions. By defining the cost of an addition, subtraction, or comparison operation as one-cpu
unit, the cost of other operations (albeit machine dependent) can be roughly established. To
give you an idea, the cost of various operations is given with the relative cpu cost (in paren￾thesis) as follows: absolute value („2), multiplication („4), division and modulus („10),
exponentiation, square root („50), power xy („100).
One way to reduce cpu-time is to replace an expression that is executed numerous
times with a different expression that yields the same value but is cheaper to compute. For
instance, consider calculating the real roots of a quadratic equation. A naive programmer
would most likely express the computation of the roots as
x1 Ð p´b ` ?
b2 ´ 4 ˚ a ˚ cq{p2 ˚ aq \ Compute x1
x2 Ð p´b ´ ?
b2 ´ 4 ˚ a ˚ cq{p2 ˚ aq \ Compute x2Numerical Algorithms and Errors  15
Note that the computation of each root requires 2 addition or subtraction, 3 multiplications,
1 power, 1 division, and 1 square root operations, corresponding to 348 cpu-units. However,
the expression leading the roots may be rearranged as follows:
d Ð 2 ˚ a \ Find denominator
Δ Ð ?b ˚ b ´ 4 ˚ a ˚ c \ Evaluate Δ
x1 Ð p´b ` Δq{d \ Find x1
x2 Ð p´b ´ Δq{d \ Find x2
Now, the set of expressions requires 3 additions or subtractions, 4 multiplications, 2 divi￾sions, and 1 square root, totaling 89 cpu-units. Notice that by restating the mathematical
expressions differently, a cpu-time saving of about 75% is achieved.
Some other examples of rearranging mathematical expressions are given below:
u Ð 0.5 ˚ b ´ 0.3 ˚ c \ Find u “ pb ´ 3 ˚ c{5q{2
v Ð d ˚ d \ Compute v “ d2
w Ð v ˚ v \ Compute w “ d4
y Ð x ˚ px ˚ pa ˚ x ` bq ` cq ` d \ Find y “ ax3 ` bx2 ` cx ` d
Note that in the first expression, division operations are expressed as multiplications after
removing the parentheses, i.e., 1/2=0.5 and 3/10=0.3. The second and third expressions
yield d2 and d4 with only two multiplications, respectively. In the last expression, a poly￾nomial is recast in nested form, requiring only additions and multiplications.
Another example of saving cpu-time is given by array applications. If a particular
element of an array is to be accessed numerous times, access the value once, assign its value
to a temporary scalar variable, and use the temporary variable.
For “
k “ 1, n‰
tmp Ð ak \ Get ak and set tmp “ ak
v Ð tmp ˚ p2 ˚ tmp ´ 3q \ Find v “ akp2ak ´ 3q
w Ð ak`1{tmp \ Find w “ ak`1{ak
End For
Minimize the number of conditional constructs and indefinite￾loop constructs due to the cpu-time devoted to evaluating logical
expressions, which require larger cpu-time. In this regard, For￾constructs are faster than While- or Repeat-Until constructs.
1.2 BASIC DEFINITIONS AND SOURCES OF ERROR
Sources of errors in simulating scientific and engineering problems can be divided into three
categories: modeling, experimental, and numerical errors.
Modelling Error. Scientists and engineers deal with the problems of the physical
world, and they frequently resort to computer simulation. Computer simulation is the pre￾diction of the behavior of a physical phenomenon in nature or the outcome of a physical
system under consideration. The first step in numerical simulation is to derive precise and
detailed mathematical models. Any mathematical model of a process or physical system
may consist of one or more mathematical sub-models. The complexity of a model de￾pends on the complexity of the physical phenomenon under consideration. Mathematical16  Numerical Methods for Scientists and Engineers
modeling includes the complete specification of all pertinent differential equations, bound￾ary conditions, and initial conditions for the system.
Mathematical models help scientists and engineers develop an understanding of systems
or processes using mathematical equations. Measuring exactly every variable or including
the effects of all pertinent variables in a mathematical model may be impossible and/or
impractical. Furthermore, a complete mathematical model may become way too expensive
to be used in analysis with little benefit. As a result of these factors, mathematical models
are revised and simplified after perhaps many simplifying assumptions, resulting in less
accurate models. Hence, simplified mathematical models may not be ‘exact’ as compared to
the underlying physical process. If a mathematical model is derived erroneously or with false
or gross assumptions, the numerical simulation will not produce the results corresponding to
the physical event, even if the governing mathematical equations are solved exactly by any
analytical means. In other words, the simulation of a simplified physical event will deviate
to some extent from its true behavior. This small or large deviation due to modeling error
depends on the validity of the assumptions made.
Modeling errors may be the result of simplifying geometry (reducing it to one- or
two-dimensional geometry, etc.), material description (assuming homogeneity when it is
not, inadequate representation of material properties), prescribing boundary conditions,
which may be difficult to do in some cases, etc. Oversimplifying a model surely requires
less computational effort and yields simpler solutions, but it may also result in inaccurate
solutions that cannot be accepted or tolerated. For instance, neglecting air resistance in the
motion of a pendulum or the friction between the tires of an automobile and the road, etc.,
will give results that are somewhat inaccurate. Hence, it is not surprising that analysts end
up using different mathematical models to take into account the effects of fewer or more
variables. It should be kept in mind that the accuracy of the prediction of a mathematical
model depends more on its ability to correctly identify the dominant parameters and their
effects than on its complexity. In this regard, a simple model may sometimes be more useful
than a more complex model. Therefore, there is a need to find a balance between the level
of accuracy required for simulations and the complexity level of a mathematical model.
Once the numerical solution of a physical event is obtained, additional information,
generally computed using numerical methods such as differentiation, integration, interpola￾tion, etc., may also be required. These computed quantities also approximate and contribute
to the numerical errors in the final results.
Experimental Error. When we speak of experimental errors, we do not refer to
human errors that result from mistakes, blunders, or numerical miscalculations. Although
these errors are definitely important, they can be eliminated by correctly repeating exper￾iments and/or calculations. Yet experimental errors are inherent in any measurement, and
they cannot be eliminated no matter how carefully experiments are conducted.
Experimental errors are due to observations and/or instruments or devices used in
measurements. Data or empirical errors arise when a mathematical model acquires experi￾mentally derived data as input. Such errors are mostly the result of limitations and errors in
instrumentation. The true value of any measurement is therefore unknown; that is, we are
not certain if a measured value is smaller or larger than the actual value. All experiments
include systematic and random errors to some degree. In some cases, it is possible to reduce
the magnitude of an experimental error.
Additionally, instruments may be affected by the frequency of use, humidity, tem￾perature, pressure, magnetic field of a physical environment, etc. Instruments yield devia￾tions in their measurement readings as a combined result of manufactured settings, humanNumerical Algorithms and Errors  17
handling, and environmental factors. For this reason, all instruments need to be periodically
calibrated with an instrument of known accuracy. In order to reduce modeling errors, it is
important to improve the accuracy of experimental data, probably more than the precision
of arithmetic operations.
Numerical error. It is introduced as a result of carrying out an arithmetic operation
using either a calculator or a digital computer, or numerical methods or approximations. At
this point, we exclude user errors resulting from, for instance, accidentally entering incorrect
data into a numerical process.
To understand the sources of numerical errors in computations, it is important that
we understand how numbers are manipulated in digital computers. When a floating-point
number is converted to its binary form, on which computers base their arithmetic opera￾tions, most numbers cannot be represented in their exact forms due to the finite length of
the floating-point mantissa. The digits that are not kept in the number result in chopping
or rounding errors. As a result, a digital computer retains only a limited number of digits
in a floating-point representation that is mostly inexact. These errors, which arise due to
the inability of computers to store the exact value of the numbers, are called machine er￾rors. Arithmetic operations with the stored or computed numbers yield round-off errors.
On the other hand, implementation of a numerical method that, in reality, requires ap￾proximation(s) to a mathematical procedure results in truncation errors. Thus, the overall
numerical error is a combination of truncation and round-off errors. Since these errors are
specific to digital computers and numerical methods, they are covered in the subsequent
sections.
1.2.1 DEFINITION OF ERROR
Determining and reporting the accuracy of a computed quantity is one of the most important
tasks in numerical analysis. An approximation error can be the result of computed or
measured data that is not precise or of approximations used instead of the real data. There
are basically two ways to express the magnitude of error in a computed quantity: absolute
error and relative error.
Let Q be a physical quantity, and Q and Q˜ denote its true and approximate values,
respectively. Then error is defined as the difference between the true and approximate values;
that is,
Error, ΔQ “ Q ´ Q˜
Error and absolute error are often used interchangeably; however, absolute error is merely
the absolute value of the error:
Absolute error, |ΔQ| “ |Q ´ Q˜| (1.1)
On the other hand, relative error is a measure of “computed” or “measured” quantity
relative to the magnitude of its true value:
Relative error,
ˇ
ˇ
ˇ
ˇ
ΔQ
Q
ˇ
ˇ
ˇ
ˇ “
ˇ
ˇ
ˇ
ˇ
Q ´ Q˜
Q
ˇ
ˇ
ˇ
ˇ
(1.2)
where Q ‰ 0; otherwise, the relative error is undefined.
Relative error is independent of the magnitude of the value since it is defined as a ratio
of the absolute value of error to the true value. Hence, it is also a dimensionless quantity and
more meaningful than absolute error. These errors are sometimes expressed in percentages
(%).
Absolute error, |Q ´ Q˜| ˆ 100% (1.3)18  Numerical Methods for Scientists and Engineers
RelativeError,
ˇ
ˇ
ˇ
ˇ
Q ´ Q˜
Q
ˇ
ˇ
ˇ
ˇ
ˆ 100% (1.4)
Absolute and/or relative errors are often used in iterative algorithms to determine the
convergence level of the iterated quantity, as follows:
ˇ
ˇ
ˇQpp`1q ´ Qppq
ˇ
ˇ
ˇ
ă ε1 or
ˇ
ˇ
ˇ
ˇ
Qpp`1q ´ Qppq
Qpp`1q
ˇ
ˇ
ˇ
ˇ
ă ε2 (1.5)
where ε1 and ε2 are the desired convergence tolerances, and Qppq denotes a computed
numerical quantity at any pth iteration. When the true value of a computed quantity is zero
(Q “ 0), its relative error becomes undefined since Qppq Ñ Q as p Ñ 8. For this reason, it
is customary to add a very small number to the denominator to prevent “division by zero”
or “overflow” errors:
ˇ
ˇ
ˇ
ˇ
Qpp`1q ´ Qppq
Qpp`1q ` ε
ˇ
ˇ
ˇ
ˇ
ă ε2 (1.6)
where � is a small real positive number.
EXAMPLE 1.1: Evaluating absolute and relative errors
Consider the following approximation for the inverse tangent function:
tan´1 x –
x
1.012 ` 0.267x2 for 0 ď x ď 1
Write a pseudocode segment that calculates and prints the true and approximate
values of tan´1 x along with its absolute and relative errors from x “ 0 to 1 with 0.1
increments.
SOLUTION:
A pseudocode segment to compute tan´1 x, its approximation, and the pertinent
errors are presented below:
x Ð 0; Δx Ð 0.1 \ Initialize x and Δx (increment)
While “
x ď 1
‰ \ While x ď 1 execute block statements
true Ð tan´1 x \ Find true value using built-in function
appx Ð x{p1.012 ` 0.267x2q \ Find approximate value
Ea Ð |true ´ appx| \ Calculate absolute error
Er Ð Ea{true \ Calculate relative error
Write: x, true, appx, Ea, Er \ Print out computed results for x
x Ð x ` Δx \ Find next x as x ` Δx
End While \ Return to the top of the loop
In this code segment, first x and Δx are initialized. Then a While-construct is
used to compute the desired quantities for x values from 0 to 1 with 0.1 increments.
The computed results for each x are printed out within the loop. Note that an
accumulator is used to determine the value of the next x.
Discussion: The same result can also be accomplished more efficiently by using
a For-construct. Although tan´1 x is computed using its built-in function, its true
value and resulting calculated error depend on the precision (defined as single or
double precision) adopted in the actual program.Numerical Algorithms and Errors  19
1.2.2 MEASUREMENT ERROR, ACCURACY, AND PRECISION
It is impossible to measure the true value of any physical quantity. Even if measurements
are repeated multiple times, each time a different value will be recorded, and none of these
measurements can be favored over the others. In other words, there will always be an error
in any measurement, and from this point of view, experimentally collected data are not
perfect. Therefore, measurement error (also referred to as observational error) is defined
as the difference between a measured quantity and its true value or between two measured
values of a physical quantity.
Scientists and engineers are aware of the presence of measurement errors in experi￾ments. It is thus desired to minimize experimental errors as much as possible to ensure that
measurements represent true value as accurately as possible. There are two kinds of errors:
random and systematic errors.
Random errors (also referred to as precision errors), which are caused by unpredictable
changes in the environment, can be easily detected and analyzed by statistical methods. We
know that multiple measurements of a physical quantity always exhibit fluctuations in the
measured quantity around a mean value. The statistical method for finding a quantity (Q),
its uncertainty (dQ), or standard deviation pσQq requires repeating the measurements and
computing its average, average deviation, or the standard deviation. Thus, we hope to reduce
the adverse effects of random errors by conducting a sufficient number of experiments.
Systematic errors are the result of how an experiment is conducted and can be identified
by leading to results that are too high or too low. These errors cannot be mitigated simply
by averaging the measured values. Systematic errors may be more difficult to detect, but
their effects can be reduced by changing the way experiments (using a different method
or technique) are conducted. Systematic errors are the result of a wide range of factors
that arise due to poorly designed experiments, faulty calibration of measuring instruments,
poorly maintained instruments, and/or faulty readings by the operator. These errors do
not include human errors such as blunders and arithmetic errors, which can be eliminated
once noticed. To minimize the effects of experimental errors, the analyst should know how
to measure experimental errors, analyze them, and report the measurements and their
uncertainties clearly and accurately.
Experimental error is best measured by the accuracy and precision of any measured
quantity. Accuracy is a measure of the degree of closeness of a measured (or computed)
value to its true value. However, since the true value of a physical quantity is generally
unknown, it is seldom possible to determine the accuracy of a measurement. Precision is
a measure of how closely multiple measurements agree with each other. Precision is also
referred to as reproducibility or repeatability. Bias, which is a result of systematic errors, is
a measure of how far a measured value is from its true value. In other words, the difference
between the mean value of the measurements and their true value is the bias. The source
of bias error is attributed to measuring instrument calibration errors.
Fig. 1.3 illustrates accuracy, precision, and bias by using the analogy of grouping darts
in a target. A close grouping of darts (measured values) is always considered a good thing, as
it indicates consistency in the action of throwing. In Fig. 1.3(a), the accuracy and precision
are low, and the grouping is on the right side of the target value (i.e., bias exists). In Fig.
1.3(b), the accuracy is high with no bias, but the precision is low (i.e., the darts are far apart
from each other). In Fig. 1.3(c), accuracy and bias are poor (dart group on the south-west
side of the target), but precision is good (i.e., darts are close to each other). In Fig. 1.3(d),
the accuracy and precision are high with no bias.20  Numerical Methods for Scientists and Engineers
FIGURE 1.3: (a) Accuracy-low, Precision-low, Bias present (on the east of the target);
(b) Accuracy-high, precision-low (No bias), (c) Accuracy-low, Precision-high, Bias
present (on the SW of the target), (d) Accuracy-high, Precision-high (No Bias)
1.3 PROPAGATION OF ERROR
Since no measurement is exact, we seek the best possible values for the experimentally
determined quantities. Thus, the experimental results are reported as a range of possible true
values (e.g., Q˘dQ) based on the limited number of measurements. In most cases, computer
numbers are not exact, either due to rounding or truncation errors. The quantities derived as
a result of either experiment with uncertainties or numerical computations with rounded or
chopped numbers are generally put through functional manipulations (addition, subtraction,
multiplication, division, etc.) to compute the desired quantity. The errors associated with
each independent variable propagate through the functional manipulations, which affect the
corresponding compound error in the derived quantity.
The propagation of error (or uncertainty) is defined as the effects of variable uncer￾tainties on a function. The concept of error propagation analysis is developed to determine
the effects of uncertainties (specifically random or bias errors) on a computed quantity.
Consider the calculated quantity to be F “ Fpx, yq, where x and y are independent
(measured) variables. If x and y have uncertainties dx and dy, then the measured quantities
are reported as x ˘ dx and y ˘ dy. These average errors contribute to the error of the
computed quantity F, i.e., the final result should be reported as F ˘ dF. Assuming the
average errors are small, the compound error can be estimated using the total differential
formula as follows:
dF “ BF
Bx
dx `
BF
By
dy (1.7)
where dx and dy differentials are average (or absolute) errors in the x and y variables,
respectively, and dF is the compound error.
Dividing both sides of Eq. (1.7) by F gives the relative error:
ˇ
ˇ
ˇ
ˇ
dF
F
ˇ
ˇ
ˇ
ˇ “
ˇ
ˇ
ˇ
ˇ
x
F
BF
Bx
ˆdx
x
˙ˇ
ˇ
ˇ
ˇ
`
ˇ
ˇ
ˇ
ˇ
y
F
BF
By
ˆdy
y
˙ˇ
ˇ
ˇ
ˇ
(1.8)
or multiplying both sides of Eq. (1.8) by 100, we get
F % “
ˇ
ˇ
ˇ
ˇ
x
F
BF
Bx
ˇ
ˇ
ˇ
ˇ
px%q `
ˇ
ˇ
ˇ
ˇ
y
F
BF
By
ˇ
ˇ
ˇ
ˇ
py%q `
ˇ
ˇ
ˇ
ˇ
z
F
Bf
Bz
ˇ
ˇ
ˇ
ˇ
pz%q (1.9)Numerical Algorithms and Errors  21
TABLE 1.1: Error propagation rules.
Expression Standard Deviation
F “ Cx σF “ |C| σx Ø σF
F “ σx
x , C=constant
F “ x ˘ y σF “
b
σ2
x ` σ2
y
F “ x
y
σF
F “
d´σx
x
¯2
`
´σy
y
¯2
F “ xnym σF
F “
d´n σx
x
¯2
`
´m σy
y
¯2
where F % is the percent relative error. The guiding principle in all cases is to consider the
most pessimistic situation.
Random errors in measured x and y similarly propagate to the computed quantity F
as well. In the following analysis, we assume that (a) the random errors of each measured
value are independent of each other; (b) measurements follow a Gaussian distribution; and
(c) there is negligible or no covariance between the errors. Under these assumptions, the
differentials may take (+/´) signs since these errors fluctuate about a mean value. Thus,
instead of dF, we examine pdFq2. Taking the square of Eq. (1.7) yields
pdFq
2 “
ˆBF
Bx
dx `
BF
By
dy˙2
(1.10)
“
ˆBF
Bx
˙2
pdxq
2 `
ˆBF
By
˙2
pdyq
2 ` 2
ˆBF
Bx
˙ ˆBF
By
˙
pdxqpdyq
where x and y are independent variables. Since dx and dy can have positive or negative
signs, the square terms are always positive. However, the cross terms may cancel each other
out, and as a result, the mean value of the cross terms will be very small or zero. In this
case, we may write
dF “
dˆBF
Bx
˙2
pdxq2 `
ˆBF
By
˙2
pdyq2 (1.11)
For random errors, it is customary to replace the dx and dy differentials with the
standard deviations to give
σF “
d ˆBF
Bx
˙2
σ2
x `
ˆBF
By
˙2
σ2
y (1.12)
Equation (1.11) or (1.12) is the error propagation formula for a quantity of two independent
variables. Using Eq. (1.12), the standard deviation of the computed quantity F associated
with common mathematical operations is tabulated in Table 1.1.22  Numerical Methods for Scientists and Engineers
EXAMPLE 1.2: Calculate average and standard error
The density of a solid cylindrical object is to be determined. Estimate the computed
error of the density if the object’s circumference, height, and mass are measured
with (a) the average error of ˘1% and (b) the standard deviation of ˘1%.
SOLUTION:
The density of a cylindrical object is determined by ρ “ m{π r2h, where m, h,
and r are the mass, height, and radius, respectively. However, the error for the radius
is not given, but it can be estimated by using the perimeter (S “ 2πr). Replacing
the radius with the circumference leads to ρ “ 4π m{hS2.
(a) The measured quantities have average errors (˘dm, ˘dS, ˘dh), which gives
rise to ˘dρ. Therefore, the measured quantities (m, h, and S) are taken as the
independent variables, i.e., ρ “ fpm, h, Sq.
Considering the most pessimistic scenario, the maximum possible error in each
variable will contribute to the maximum possible error in the density. In this case,
the total differential formula for the density is expressed as
dρ “
ˇ
ˇ
ˇ
ˇ
Bρ
Bm
dm
ˇ
ˇ
ˇ
ˇ
`
ˇ
ˇ
ˇ
ˇ
Bρ
Bhdh
ˇ
ˇ
ˇ
ˇ
`
ˇ
ˇ
ˇ
ˇ
Bρ
BS dS
ˇ
ˇ
ˇ
ˇ
where dm, dS, and dh are differentials that denote absolute errors. Replacing the par￾tial derivatives by Bρ{Bm “ 4π{hS2, Bρ{BS “ ´8πm{hS3, and Bρ{Bh “ ´4πm{h2S2
and dividing both sides with ρ, we get
dρ
ρ “
ˇ
ˇ
ˇ
ˇ
dm
m
ˇ
ˇ
ˇ
ˇ
`
ˇ
ˇ
ˇ
ˇ
´dh
h
ˇ
ˇ
ˇ
ˇ
` 2
ˇ
ˇ
ˇ
ˇ
´dS
S
ˇ
ˇ
ˇ
ˇ
or multiplying both sides by 100 leads to
ρ% “ m% ` h% ` 2pS%q
Since m, h, and S are measured with relative errors of ˘1%, the relative error of the
estimated density is found to be ˘4%.
(b) Replacing the differentials with the standard deviations (σm, σS, σh) implies
that the errors in m, r, and h are independent of each other. By dividing both sides
with ρ, the propagation of error formula becomes
σρ
ρ “
c´σm
m
¯2
`
´σh
h
¯2
`
´
2
σS
S
¯2
or
ρ% “
b
pm%q
2 ` ph%q
2 ` 4 pS%q
2
In this case the relative error is obtained as ˘2.45%.
Discussion: This example illustrates the two types of uncertainty estimations. The
average error estimate in the density is larger than that of the random error. It
should be noted that the standard deviation describes the spread of the data. On
the other hand, standard error is not about the spread of our data but the accuracy
of the average. Any experimental calculation should be accompanied by a proper
uncertainty analysis.Numerical Algorithms and Errors  23
1.4 NUMERICAL COMPUTATIONS
A major source of numerical errors in digital computers arises from the way numbers are
stored and managed in computers. Before diving into examining numerical errors, it is
necessary to briefly look into how numbers are handled on digital computers.
1.4.1 NUMBER SYSTEMS
In daily life, the decimal system (a 10-based numbering system) with 10 digits (from 0 to
9) is used to express integer, rational, and irrational numbers. Several numbering systems
with different bases are incorporated into digital computer hardware to interpret and com￾municate data between its components. Binary (base 2), ternary (base 3), octal (base 8),
and hexadecimal (base 16) numbering systems are the most commonly used.
The input supplied to digital computers is in decimal format, but the digital computers
process any type of numeric data, letters, and special symbols in discrete form as pulses
sent by electronic components. For instance, the state of an electrical pulse is interpreted
as 1 for a high voltage (on) or 0 for a low voltage (off ). This concept can be thought of as
switching an electric circuit on and off. Hence, digital computers are designed to read and
interpret data in binary form as a sequence of on’s (1’s) and off ’s (0’s). In other words,
devices based on binary (on/off logic) can be constructed, and all forms of data can be
expressed in a binary system.
Computers use bits to represent information. A bit is the basic unit of storage in a
computer, and it is defined as a binary, which is 0 or 1. A bayt is a group of eight bits used
to represent a character, and it is the basic unit for measuring memory size in computers.
Two or more bits are referred to as a word.
In the decimal number system, an integer is expressed as a polynomial having one of
the ten digits ranging from 0 to 9 as coefficients.
N “ panan´1an´2 ¨¨¨ a1a0q10
“ an ˆ 10n ` an´1 ˆ 10n´1 `¨¨¨` a1 ˆ 101 ` a0 ˆ 100 “ ÿn
k“0
ak ˆ 10k
For instance, 125 is expressed as 125 “ 1 ˆ 102 ` 2 ˆ 101 ` 5 ˆ 100.
A positive real number has an integer (N) and a fractional part (F). The fractional
part is written as a decimal fraction.
F “ pb1b2b3 ¨¨¨ bn ¨¨¨q10
“ b1 ˆ 10´1 ` b2 ˆ 10´2 `¨¨¨` bn ˆ 10´n `¨¨¨“ ÿ8
k“1
bk ˆ 10´k
where each bn takes one of the values from 0 to 9. For example,
p0.125q10 “ 1 ˆ 10´1 ` 2 ˆ 10´2 ` 5 ˆ 10´3
p0.111 ¨¨¨q10 “ 1 ˆ 10´1 ` 1 ˆ 10´2 ` 1 ˆ 10´3 `¨¨¨
A floating-point representation of a decimal number in base β is expressed as
pan ...a2a1a0. b1b2b3b4 ...qβ “ ÿn
k“0
akβk ` ÿ8
k“1
bkβ´k (1.13)24  Numerical Methods for Scientists and Engineers
Note that an’s and bn’s take the values of 0, 1, 2, . . . , β ´ 1.
In the binary number system (β “ 2), for instance, 125 is found as
p125q10 “ p1111101q2 “ 1 ˆ 26 ` 1 ˆ 25 ` 1 ˆ 24 ` 1 ˆ 23 ` 1 ˆ 22 ` 0 ˆ 21 ` 1 ˆ 20
The numbers with fractional parts are expressed as
p0.125q10 “ p0.001q2 “ 0 ˆ 2´1 ` 0 ˆ 2´2 ` 1 ˆ 2´3
p0.111 ¨¨¨q10 “ p0.000111q2
“ 0 ˆ 2´1 ` 0 ˆ 2´2 ` 0 ˆ 2´3 ` 1 ˆ 2´4 ` 1 ˆ 2´5 ` 1 ˆ 2´6 `¨¨¨
Expressing data in binary format uses too many bits, so the binary system can be im￾practical due to the difficulty of reading large numbers in binary form. In order to overcome
this problem, binary numbers in groups of four bits are merged to form the hexadecimal
number system. This system, also known as “hex,” has a base of 16 (β “ 16) and consists
of 16 symbols: 10 numbers of the decimal system (digits ranging from 0 to 9) and six let￾ters from A to F with letters representing 10 to 15, respectively. The hex system allows
information to be represented with fewer bits, making it more useful and suitable for digital
hardware. For example, 125 and 0.21875 in the hex system are expressed as
p125q10 “ p7Dq16 “ 7 ˆ 161 ` 13 ˆ 160
p0.21875q10 “ p0.38q16 “ 3 ˆ 16´1 ` 8 ˆ 16´2
Octal system (β “ 8) which is also used to make any binary information more compact,
is not as popular or common as hex or binary systems. An octal number is made up of
digits ranging from 0 to 7. The octal system groups binary numbers into triplets instead of
quartets. For instance, 125 in the octal system is expressed as
p125q10 “ p175q8 “ 1 ˆ 82 ` 7 ˆ 81 ` 5 ˆ 80
p0.21875q10 “ p0.16q8 “ 1 ˆ 8´1 ` 6 ˆ 8´2
1.4.2 FLOATING-POINT REPRESENTATION OF NUMBERS
In this section, computer arithmetic of floating-point numbers is presented using the stan￾dard set by the IEEE (Institute for Electrical and Electronic Engineers). In 1985, the
IEEE published its first report, IEEE 754-1985 [15], that regulated standards for binary
or decimal floating-point numbers, algorithms for rounding arithmetic operations, how to
handle exceptions, etc. The IEEE standard was updated in 2019 as IEEE 754-2019 [16].
This standardization, now followed by microprocessor manufacturers, has enhanced program
portability.
Real numbers on a computer are represented by a floating-point number system. We
normally supply data (numbers) as input to a computer using the decimal system. The
input is then converted to and stored in binary form. Computers have a finite word length,
and for this reason, only numbers with a finite number of digits (integers or numbers with
powers of 2) can be exactly represented. In other words, since they cannot be represented
exactly on computers, most numbers are rounded. This rounding process alone can lead to
errors due to the inexact storage of machine numbers. A machine number of x (i.e., a finite
floating-point approximation) is denoted by flpxq.Numerical Algorithms and Errors  25
EXAMPLE 1.3: Converting decimals to binary, octal, and hex numbers
Convert 587, 18.75, and 1.5625 (in the decimal system) to equivalent binary, octal,
and hexadecimal numbers.
SOLUTION:
Conversion of 587 (an integer) leads to
p587q10 “ 1 ˆ 29 ` 0 ˆ 28 ` 0 ˆ 27 ` 1 ˆ 26 ` 0 ˆ 25 ` 0 ˆ 24 ` 1 ˆ 23 ` 0 ˆ 22
` 1 ˆ 21 ` 1 ˆ 20 “ p1001001011q2
p587q10 “ 1 ˆ 83 ` 1 ˆ 82 ` 1 ˆ 81 ` 3 ˆ 80 “ p1113q8
p587q10 “ 2 ˆ 162 ` 4 ˆ 161 ` B ˆ 160 “ p24Bq16
where B denotes 11.
Conversion of 18.75 (a real number) gives
p18.75q10 “ 1 ˆ 24 ` 0 ˆ 23 ` 0 ˆ 22 ` 1 ˆ 21 ` 0 ˆ 20 ` 1 ˆ 2´1 ` 1 ˆ 2´2
“ p10010.11q2
p18.75q10 “ 2 ˆ 81 ` 2 ˆ 80 ` 6 ˆ 8´1 “ p22.6q8
p18.75q10 “ 1 ˆ 161 ` 2 ˆ 160 ` C ˆ 16´1 “ p12.Cq16
where C denotes 12. Finally, converting 1.5625 yields
p1.5625q10 “ 1 ˆ 20 ` 1 ˆ 2´1 ` 0 ˆ 2´2 ` 0 ˆ 2´3 ` 1 ˆ 2´4 “ p1.1001q2
p1.5625q10 “ 1 ˆ 80 ` 4 ˆ 8´1 ` 4 ˆ 8´2 “ p1.44q8
p1.5625q10 “ 1 ˆ 160 ` 9 ˆ 16´1 “ p1.9q16
Discussion: Note that the decimals in this example are made up of the sum of
positive and negative powers of 2. Hence, they are represented “exactly” in the
binary, octal, and hexadecimal number systems.
A non-negative real number is represented with an integer part, a fractional part, and a
decimal point, separating the integer and fractional parts, i.e., 123.4567. This representation
is referred to as decimal notation. Any real number in the decimal system can be expressed
in normalized scientific notation. This is accomplished by shifting the decimal point and
expressing the number with a power of 10 such that all digits on the right of the decimal point
are not zero; for example, 0.00012 (0.12ˆ10´3), 0.125 (0.125ˆ100), and 23.4 (0.234ˆ102).
Arithmetic calculations account for the major source of numerical errors. This is be￾cause computers carry out calculations in floating-point arithmetic. An n-digit floating-point
representation of a real number in base-β has the form
M ˆ βe or ˘ p0.m1m2 ¨¨¨ mnqβ ˆ βe
where β denotes the base of the number system (β “ 2 for binary, β “ 10 for decimal
systems), M is a β-fraction called mantissa (β´1 ď M ă 1 or M=0), containing the
normalized value of the number, ˘ denotes the sign of the number, m1 ‰ 0, m2, m3, ...,
mn are the decimal digits (0, 1, 2, . . . , β ´ 1), and e is an integer (positive, negative, or
zero) called the exponent, which effectively defines the offset from normalization. The total
number of bits assigned to a number is fixed by computer architecture.26  Numerical Methods for Scientists and Engineers
FIGURE 1.4: IEEE 754 Floating-Point Standard (a) single, (b) double precision.
EXAMPLE 1.4: Expressing numbers in Normal Scientific Notation
Express 123.4567, 0.01234567, and 123456.7 in normalized scientific notation.
SOLUTION:
Applying the normalized scientific notation rule results in
12.34567 “ 0.1234567 ˆ 102
0.01234567 “ 0.1234567 ˆ 10´1
123456.7 “ 0.1234567 ˆ 106
Discussion: Notice that the leading decimal in any fraction is not zero unless the
number itself is zero; for instance, expressing 0.001234567 as 0.01234567 ˆ 10´1 or
0.001234567 ˆ 100 can be viewed as a format option, but these expressions cannot
be regarded as the normalized scientific notation.
1.4.3 PRECISION OF A NUMERICAL QUANTITY
The definition of precision for a numerical quantity in computers is different from that of
an experimental value. The precision of a numerical value describes the number of digits (or
bits) that are used to represent a numerical value. The precision of floating-point numbers
on any computer is determined by the word length of the computer. However, computing
systems generally provide floating-point numbers of different lengths. A floating-point num￾ber with a short form (32 bits or 8 bytes) is called a single-precision floating-point number.
As depicted in Fig. 1.4(a), each number is represented by a 24-bit mantissa (including a one￾bit sign) and an 8-bit exponent. The exponents lie between ´126 and 127. Exponent values
of ´127 and 128 are reserved for floating-point exceptions such as “infinity” due to division
by zero or NaN (“Not a Number”). In other words, single precision allows for the represen￾tation of numbers with an accuracy of seven decimals, ranging from 5.877ˆ10´39p“ 2´127)
to 3.4ˆ1038p“ 2128).
A floating-point number represented by a long form (64 bits or 16 bytes) is called
a double-precision floating-point number. A double-precision number uses 53 bits for the
mantissa (including the sign) and 11 bits for the exponent, as shown in Fig. 1.4(b). The
range of exponents becomes ´1022 and 1023, which means that the numbers between
2.225ˆ10´308p“ 2´1022) and 8.988ˆ10308p“ 21023) can be represented with an accuracy
of sixteen decimals. Calculation in double precision doubles the memory storage require￾ments and the cpu-time as compared to single precision. Some compilers allow the use of
quadruple and octuplet precision.
Regardless of its “precision,” when a real number is too large or too small to be rep￾resented in the normalized scientific notation, we may encounter a representation problem
since the range of floating-point numbers is finite. When an arithmetic operation results in a
number larger than the maximum allowed number, it is referred to as overflow. Similarly, anNumerical Algorithms and Errors  27
Pseudocode 1.5
Function Module MACHEPS (x)
\ DESCRIPTION: A pseudo Function module to estimate machine epsilon.
While “
1 ` x{2 ą 1
‰ \ x is large enough so that 1 ` x{2 ‰ 1
x Ð x{2 \ Reduce magnitude of x by halving
End While
MACHEPSÐ x \ Set smallest value of x to MACHEPS
End Function Module MACHEPS
operation resulting in a smaller number than the minimum allowed is called an underflow.
An error message is issued when an operation yielding overflow or underflow is encountered.
Machine epsilon, εm, represents the round-off error for a floating-point number with
a given precision. It is defined as the smallest positive number (i.e., the gap between the
number 1 and the next largest floating-point number); mathematically speaking, εm‘1 ą 1,
where ‘ denotes floating-point addition. Hence, we can compute εm from
εm “
" β´t
, rounding
β1´t
, chopping (1.14)
where β is the number base and t is the number of significant digits in the mantissa.
Machine epsilon can also be defined as an upper bound on the relative error due to
rounding in floating-point arithmetic. It is in practice used to compare whether two floating￾point numbers are equivalent. Its size is determined by the precision used, type of rounding,
and computer hardware. For binary machines (β “ 2), theoretically, it yields 1.19ˆ10´7 and
2.22ˆ10´16 for single (t “ 24) and double precision (t “ 53), respectively.
A pseudofunction module, MACHEPS, that determines the machine epsilon of a com￾puter is presented in Pseudocode 1.5. The code accepts an arbitrary floating-point number
x as input. Using a While-construct, x is halved successively until 1 ‘ px{2q “ 1, where the
machine can no longer distinguish the difference between 1 ‘ px{2q and 1. When a bound
for the machine epsilon is found, the value of x is set to MACHEPS. However, it should be
noted that most compilers come with built-in functions for furnishing machine epsilon.
Utilizing the machine epsilon, �m, in computer programs makes
the programs portable, i.e., machine-independent. In other words,
the programs become independent of the computer hardware or
systems for which they were designed.
Equality of two real numbers. We have already learned that digital computers
can rarely store or manage the “true” or “exact value” of a real number due to computer
representations of numbers and round-offs. A fundamental step in many algorithms is to
decide if and when an approximation is accurate enough. So testing for equality of two real
numbers as x “ y, x´y “ 0, or x{y “ 1 must be done cautiously. (Symbolic processors that
do not use rounding easily overcome this problem.) Instead, a smart approach is to test for
near equality; that is, one must determine the interval around the value of “0” or “1” that is
sufficient to meet the criteria. The machine epsilon (or a user-prescribed tolerance) is used to
test for either equality of two real numbers (x and y) or convergence of an iterative process
(as a stopping criterion) as |x ´ y| ă εm or |x{y ´ 1| ă εm, where �m is the machine epsilon.28  Numerical Methods for Scientists and Engineers
In iterative processes, �m may be too strict, in which case it is replaced by a user-prescribed
tolerance, �.
A more prudent approach is to base an equality test on the relative
error, |x ´ y|{|y| ă εm, rather than the absolute error, especially
when dealing with very small or very large numbers.
1.4.4 ROUNDING-OFF AND CHOPPING
Most real numbers are approximated by their closest representation in the machine since
they cannot be exactly represented by the floating-point representation. There are two ways
of rounding: (i) optimal rounding and (ii) chopping.
Consider any real positive number (within the numerical range of a digital computer)
expressed in the normalized scientific form as
0 . m1m2 ¨¨¨ mnmn`1 ¨¨¨ˆ 10e
where 1 ď m1 ď 9 and 0 ď mn ď 9 for n ą 1.
In optimal rounding, the closest machine number is chosen. Rounding is done in such
a fashion as to cause the fewest possible errors. Assuming that n is the maximum number
of decimal digits to be rounded, the general rule for rounding off a number to n decimal
places is given as
flrp0 . m1m2 ¨¨¨ mnmn`1 ¨¨¨ˆ 10eq “ " 0 . m1m2 ¨¨¨ mnmn`1 ˆ 10e, 0 ď mn`1 ă 5
0 . m1m2 ¨¨¨pmn ` 1q ˆ 10e, 5 ď mn`1 ď 9
Note that if mn`1 (the digit of the number to the right of mn) is less than 5, then mn
remains unchanged and all digits after mn (i.e., mn`1mn`2 ¨¨¨) are discarded. If mn`1 ě 5,
then the value of mn is raised by 1 (mn Ð mn ` 1) and all digits after dn are discarded.
In chopping, the digits after the nth place (mn`1mn`2 ¨¨¨) are omitted regardless of
their magnitude, yielding the following approximation:
flcp0 . m1m2 ¨¨¨ mnmn`1 ¨¨¨ˆ 10eq “ 0 . m1m2 ¨¨¨ mn ˆ 10e
Chopping is easier than optimal rounding, but slightly less accurate. Notice that we
used the subscripts “r” and “c” in the floating-point representation of rounded and chopped
numbers, respectively. Many computers use “chopping” rather than “rounding” after each
arithmetic operation.
Some examples of rounding and chopping to four decimal places are illustrated below:
Number Rounding Chopping
0.123456 0.1235 0.1234
´0.123456 ´0.1235 ´0.1234
0.123450 0.1235 0.1234
0.123750 0.1238 0.1237
3.141592 3.1416 3.1415
18.127653 18.1277 18.1276Numerical Algorithms and Errors  29
FIGURE 1.5: Depiction of significant/non-significant figures of a
(a) small, (b) large, and (c) exponential number.
1.4.5 SIGNIFICANT DIGITS
The term significant digits (or significant figures) is used to indicate the accuracy of a
number. Apart from its exponential part, it contains the information (i.e., digits) that
contributes to the size or precision of a number.
Significant figures contain any one of the digits 1, 2, 3, . . . , 9, and 0. The rules to
identify significant digits are given as
‚ All non-zero digits 1, 2, ... , 9 are significant; e.g., 123 and 1.234 have, respectively,
three and four significant digits;
‚ Zeros between any two non-zero digits are significant; e.g., 102 and 1.02 have three
significant figures, while 12.003 has five significant digits;
‚ Zeros to the left of the first non-zero digit in a number are not significant; e.g.,
0.01, 0.012, 0.00123, and 0.01234 have one, two, three, and four significant digits,
respectively;
‚ Zeros to the right of the last non-zero digit in a number (i.e., trailing zeros) with a
decimal point are significant; e.g., 1.30 and 123.000 have, respectively, three, and six
significant digits;
‚ The significance of trailing zeros in an integer depends on the measurement; e.g., 12300
is ambiguous, but it becomes more meaningful when expressed as 1.23ˆ104 (three￾significant digits), 1.230ˆ104 (four significant digits), or 1.2300ˆ104 (five significant
digits).
For example, consider that 0.00600012300 has nine significant digits (i.e., 6, 0, 1, 2,
and 3). As shown in Fig. 1.5(a), zeros after the decimal point are not significant; they are
used to fix the decimal point. But the zeros between 6 and 1 and the two trailing zeros are
significant. The number 12003.006300 depicted in Fig. 1.5(b) has 11 significant digits. All
non-zero digits are significant. The zeros between 2 and 3 and between 3 and 6, along with
the two trailing zeros, are significant. The exponential number presented in Fig. 1.5(c) has
five significant figures; in other words, all digits (1, 2, 0, 6, 3) are significant.
Consider the additional numbers given in Table 1.2. All numbers with non-zero digits
(123456.7, 1.2345, and 0.123456) are significant. The numbers with zeros between non-zero
digits (1002, 12.034, and 2.00076) are significant. The leading zeros in 0.00016 and 0.004830
vanish when expressed in the normalized scientific notation as 0.16ˆ10´3 and 0.483ˆ10´2.
Similarly, when 70000 and 1200 are also expressed in normalized scientific notation (0.7ˆ105
and 0.12ˆ104), we observe one and two significant digits, respectively. Also note that the
‘trailing zeros’ in 3.000 and 0.004830 are important, as these indicate the level of accuracy
of the measurement as much as any other digits.30  Numerical Methods for Scientists and Engineers
TABLE 1.2: Examples of significant digits.
Number Significant digits No. of significant digits
123456.7 1, 2, 3, 4, 5, 6, 7 seven
70000 7 one
1200 1, 2 two
1002 0, 1, 2 four
12.034 1, 2, 0, 3, 4 five
0.00016 1, 6 two
2.00076 2, 0, 7, 6 six
3.000 3, 0 four
0.004830 0, 3, 4, 8 four
1.2345 1, 2, 3, 4, 5 five
0.123456 1, 2, 3, 4, 5, 6 six
In Pseudocode 1.6, the pseudomodule MANTISSA_EXP which retrieves the mantissa
and exponent (m, e) for an input floating-point number (fl) is presented. The module
makes use of three mathematical functions—absolute value Abspxq “ |x|, Floor(x)=txu, and
Log10(x)—that are incorporated in most programming languages as built-in functions; these
functions are specified in the module following the USES statement. While on the subject,
it should be pointed out here that, although not required, the practice of identifying module
dependencies can be useful if a source code is translated into a different language or compiled
and run on different computing platforms that may not share the same built-in functionality.
Some computer software, including CAS, has built-in functions to extract the mantissa and
exponent of a floating-point value.
The built-in function Floor(x) returns the greatest integer less than or equal to x, while
Log10(x) returns the base-10 logarithm of x. The code first determines the exponent for any
fl ‰ 0 and then the mantissa by m “ fl ˆ 10´e. Clearly, if fl “ 0, then m “ e “ 0.
Pseudocode 1.6
Module MANTISSA_EXP (fl, m, e)
\ DESCRIPTION: A pseudomodule to find mantissa (m) and exponent (e)
\ of a real number (fl).
\ USES:
\ ABS :: A built-in function computing the absolute value;
\ FLOOR:: Built-in floor function;
\ LOG10:: Built-in base-10 logarithm function.
If “
|fl| ą 0
‰
Then \ Case of |fl| ‰ 0
e Ð 1`Floor(Log10p|fl|qq \ Find exponent
m Ð fl ˚ 10´e \ Find mantissa
Else \ Case of fl “ 0
e Ð 0; m Ð 0 \ Both e and m are zero
End If
End Module MANTISSA_EXPNumerical Algorithms and Errors  31
EXAMPLE 1.5: Chopping and rounding by significant digits
Apply four-significant-digit (a) chopping, (b) rounding to the following numbers:
123456789.12, 3.7841828451, 0.3183698863, 0.0881078861
SOLUTION:
These numbers are first expressed in normalized scientific notation:
123456789.12 “ 0.12345678912 ˆ 109
3.7841828451 “ 0.37841828451 ˆ 101
0.3183698863 “ 0.3183698863 ˆ 100
0.0881078861 “ 0.8810788610 ˆ 10´1
(a) The floating-point forms using four-digit chopping are
123456789.12 “ 0.1234 ˆ 109 “ 123400000
3.7841828451 “ 0.3784 ˆ 101 “ 3.784,
0.3183698863 “ 0.3183 ˆ 100 “ 0.3183,
0.0881078861 “ 0.8810 ˆ 10´1 “ 0.08810
The absolute and relative (in parenthesis) errors are 56789.12 (0.046%),
0.000182845 (0.00483%), 0.0000698863 (0.02195%), and 0.000007886 (0.00895%)
for 123456789.12, 3.7841828451, 0.3183698863, and 0.0881078861, respectively. Note
that the absolute error is large for the largest number.
(b) Since the fifth digits of the first, second, and third numbers are greater than
or equal to 5, the fifth digits are increased by 1, while the first number remains
unchanged. So the floating-point forms using four-digit rounding become:
0.12345678912 ˆ 109 “ 0.1235 ˆ 109 “ 123500000
0.37841828451 ˆ 101 “ 0.3784 ˆ 101 “ 3.784,
0.3183698863 ˆ 100 “ 0.3183 ˆ 100 ` 0.0001 ˆ 100
“ 0.3184 ˆ 100 “ 0.3184,
0.881078861 ˆ 10´1 “ 0.881 ˆ 10´1 ` 0.0001 ˆ 10´1
“ 0.8811 ˆ 10´1 “ 0.08811
The true absolute and relative errors turn out to be 43210.88 (0.035%), 0.000182845
(0.00483%), 0.000030114 (0.00946%), and 0.000002114 (0.0024%) for 123456789.12,
3.7841828451, 0.3183698863, and 0.0881078861, respectively. However, clearly, the
round-off errors can be reduced by increasing the number of digits allowed in a
representation.
Discussion: Round-off error is caused by approximating a number, which is a quan￾tization error that results from mathematical calculations. It all boils down to is to
figure out how big is the round-off error. The absolute (round-off) error for large
numbers can be very large. The relative error indicating how good a number is
relative to its true size should be preferred in decisions.32  Numerical Methods for Scientists and Engineers
1.4.6 DECIMAL-PLACE AND SIGNIFICANT-DIGIT ACCURACY
In the preceding discussions, we have learned that the numbers processed by digital com￾puters are chopped or rounded, which eventually results in numbers that differ from their
true values. To terminate a numerical procedure, it is often necessary to determine the
significant-digit accuracy of a computed number.
Accuracy is a measure of how close a measured or computed quantity is to its true
value. A number is said to be “accurate to n decimal places” if the n-digits to the right
of the decimal place (including the nth digit) are correct. In this case, the error is at most
10´n. But the nth decimal place is where the rounding takes place, in which case the error
is at most 10´n{2. Hence, for n-decimal place accuracy, it is sufficient to show that the
following inequality holds:
ˇ
ˇQ ´ Q˜
ˇ
ˇ ď
1
2
ˆ 10´n
where Q˜ and Q denote the approximate and true values of a quantity Q.
A number is said to be “accurate to n-significant digits” if the n-digits to the right of
the first non-zero digit are correct. If |Q ´ Q˜| has a magnitude less than or equal to 5 in the
(n ` 1)th digit of Q, counting to the right starting from the first nonzero digit in Q, then
the quantity Q˜ is said to have n-significant digits with respect to its true value. In other
words, it indicates that you can trust a total of n digits.
The significant digit accuracy is a measure of relative error, and so the following in￾equality can be used to measure the number of (accurate) significant digits in Q˜:
ˇ
ˇ
ˇ
ˇ
Q ´ Q˜
Q
ˇ
ˇ
ˇ
ˇ
ď
1
2
ˆ 10´n or n –
Z
´ log10 ˆˇ
ˇ
ˇ
ˇ
Q ´ Q˜
Q
ˇ
ˇ
ˇ
ˇ
˙^
We can then say Q˜ has n´significant digits with respect to Q. Note that when defining
accurate digits, txupi.e.,Floor(x) function) is used.
Note that absolute error, |Q ´ Q˜|, gives the number of digits af￾ter the decimal point, while relative error, |Q ´ Q˜|{|Q|, gives the
number of digits regardless of the position of the decimal point.
1.4.7 EFFECT OF ROUNDING IN ARITHMETIC OPERATIONS
Round-off errors are initially encountered when saving a real number in the computer’s
memory as a floating-point number. Computer arithmetic with floating-point numbers is
predominantly inexact and thus yields computation errors. Although the error of a single
arithmetic operation is negligibly small, the accumulation of round-off errors due to numer￾ous arithmetic operations (in repetitive or iterative computations) can dominate calculations
under certain conditions.
In this section, we briefly look at how common arithmetic operations affect round￾off errors. It should be pointed out that the rounding process works basically similarly
in binary, octal, and hexadecimal number systems. Hence, the normalized decimal system
is used here to illustrate the effect of round-off errors on arithmetic operations. To make
visualization easier, consider a hypothetical decimal computer with a 3-digit mantissa and
a 1-digit exponent. When adding or subtracting two floating-point numbers, the mantissa
of the number with the smaller exponent is modified to equalize the exponents of bothNumerical Algorithms and Errors  33
EXAMPLE 1.6: Determining significant digits
Determine the significant digit accuracy of approximations to the following numbers:
True Value Approximation
1/7 0.1439
0.01234 0.012
1.23456 1.235
12.4 12.3462
SOLUTION:
For Q “ 1{7 and Q˜ “ 0.1439, we find the absolute and relative errors as |Q ´ Q˜| “
0.00104286 and |Q ´ Q˜|{Q “ 0.0073, respectively. Since the error is less than 5 in
the third digit to the right of the first non-zero digit in Q, Q˜ has two significant
digits with respect to Q. Also note that n – t´ log10p0.0073qu “ 2.
For Q “ 0.01234 and Q˜ “ 0.012, similarly, the absolute and relative errors are
found as |Q ´ Q˜| “ 0.00034 and |Q ´ Q˜|{Q “ 0.0275527. The error is less than 5
in the third digit to the right of the first non-zero digit in Q (which is 3), so Q˜
has two significant digits with respect to Q. However, the prediction yields n –
t´ log10p0.0275527qu “ 1.
For Q “ 1.23456 and Q˜ “ 1.235, the absolute and relative errors yield |Q ´ Q˜| “
0.00044 and |Q ´ Q˜|{Q “ 0.0003564. The error is less than 5 in the fourth digit to
the right of the first non-zero digit in Q so Q˜ has three significant digits with respect
to Q. We also find n – t´ log10p0.0003564qu “ 3.
We find |Q ´ Q˜| “ 0.0538 and |Q ´ Q˜|{Q “ 0.00433871 for Q “ 12.4 and Q˜ “
12.3462. The error is less than 5 in the third digit to the right of the first non￾zero digit in Q so Q˜ has two significant digits with respect to Q. Also note that
n – t´ log10p0.00433871qu “ 2.
Discussion: In this example, determining the significant-digit accuracy of approxi￾mate numbers was explained in detail, but in practice, we need to rely on quantitative
tools. The inequalities presented in this section successfully estimate the significant￾digit accuracy of the given approximations.
numbers. This action aligns the decimal points. For instance, the normalized floating-point
representations of 21 ˆ 105 and 19 ˆ 103 are 0.21 ˆ 107 and 0.19 ˆ 105, respectively. To add
or subtract these numbers, the decimal point of the small number is aligned with that of
the larger one as 0.0019ˆ107. Then the two numbers can be added or subtracted as follows:
0.2100 ˆ 107
`0.0019 ˆ 107
0.2119 ˆ 107
0.2100 ˆ 107
´0.0019 ˆ 107
0.2081 ˆ 107
The results are then chopped to give 0.211 ˆ 107 and 0.208 ˆ 107, respectively. Note that
the fourth digit of the arithmetic operations is lost in the computation.
The round-off errors get worse when a very large and a very small number (or two
numbers that are very close to each other) are added or subtracted. Now consider adding
and subtracting 2.5 ˆ 103 and 2.3.34  Numerical Methods for Scientists and Engineers
0.25000 ˆ 104
`0.00023 ˆ 104
0.25023 ˆ 104
0.25000 ˆ 104
´0.00023 ˆ 104
0.24977 ˆ 104
The addition and subtraction operations with the 3-digit mantissa result in 0.250ˆ104 and
0.249ˆ104, respectively. The absolute errors in the addition and subtraction operations are
2.3 and 7.7, respectively, compared to the true values of 2502.3 and 2497.7.
Since most real numbers cannot be represented in exact form, their floating-point values
are used in arithmetic computations. Adopting the fl(x) notation to denote the floating￾point representation of the real number x, the difference between x and flpxq is the round-off
error, which also depends on the size of x. Thus, it is measured relative to x as
flpxq “ xp1 ` εq (1.15)
with |ε| ď 2´t where t denotes the number of binary places in the mantissa.
The floating-point representation of finite-digit arithmetic for analogous elementary
operations is given as
flpx ˘ yq“px ˘ yqp1 ` εq, flpx ˆ yq “ xyp1 ` εq, flpx ˜ yq“px{yqp1 ` εq
where x and y are floating-point numbers, and ˘, ˆ and ˜ denote binary arithmetic oper￾ations.
At this stage, without going into a detailed analysis of floating-point arithmetic, we will
only mention that the computation errors propagate across multiple arithmetic operations.
The reader can find a comprehensive account of errors and error propagation associated
with the elementary operations in Ref. [34].
1.4.8 LOSS OF SIGNIFICANCE
The term “loss of significance” refers to the undesirable effects that arise when performing
calculations with floating-points. In any computation, each floating-point operation that
does not precisely represent the true arithmetic operation results in an error that may be
reduced or amplified in subsequent operations. This has to do with the finite precision
with which computers represent numbers. In general, it is desirable in any floating-point
arithmetic operation to preserve the same number of significant digits. In practice, this is
unfortunately not always possible. A common, often avoidable anomaly is observed when
an operation on two numbers increases relative error significantly more than it increases
absolute error.
Consider two nearly equal real numbers, a “ 0.123459526 and b “ 0.123459876, which
have eight significant digits. Subtracting, for instance, the two floating-point numbers yields
b ´ a “ 0.00000035 (“ 0.35 ˆ 10´6), which has two significant digits. This event is referred
to as loss of significance, which is a source of inaccuracy in most computations. In most
cases, the loss of significance can be partially remedied or entirely avoided by changing
the way arithmetic operations are performed. In this context, the use of rationalization,
identities of trigonometric or logarithmic functions, Taylor series, increasing the precision
of the computations, and so on are some of the techniques that can be exploited to guard
against degradation of precision. The analyst should be alert to the possibility of loss of
significance and take steps to avoid it, if possible.Numerical Algorithms and Errors  35
EXAMPLE 1.7: Effect of rounding/chopping in arithmetic ops
Perform the following arithmetic operations: (i) exactly, using three-digit floating￾point arithmetic; (ii) chopping; and (iii) rounding.
paq 7
3 `
3
11, pbq 7
3 ´ 3
11, pcq
ˆ7
3 `
3
11˙
`
1
7
SOLUTION:
(a) Using the exact and floating-point representations of the numbers, for (i),
(ii), and (iii), we obtain
7
3 `
3
11 “ 86
33 “ 0.260606 ˆ 101,
flc
´7
3 `
3
11
¯
“ 0.260 ˆ 101 “ 2.60
flr
´7
3 `
3
11
¯
“ 0.261 ˆ 101 “ 2.61
where subscripts c and r denote chopping and rounding operations, respectively.
(b) The operations for the subtraction case are as follows:
7
3 ´ 3
11 “ 68
33 “ 0.20606 ˆ 101,
flc
´7
3 ´ 3
11
¯
“ 0.206 ˆ 101 “ 2.06,
flr
´7
3 ´ 3
11
¯
“ 0.206 ˆ 101 “ 0.206 ˆ 101 “ 2.06
The 4th decimal place of 7/3 is 0, so it is kept as zero when rounding.
(c) The operations are as follows:
´7
3 `
3
11
¯
`
1
7 “ 635
231 “ 0.2748917 ¨¨¨ˆ 101,
flc
´1
7
¯
“ 0.142857 ¨¨¨ˆ 100,
flc
´7
3 `
3
11
¯
` flc
´1
7
¯
“ 0.260 ˆ 101 ` 0.014 ˆ 101 “ 0.274 ˆ 101 “ 2.74,
flr
´7
3 `
3
11
¯
` flr
´1
7
¯
“ 0.261 ˆ 101 ` 0.014 ˆ 101 “ 0.275 ˆ 101 “ 2.75
Discussion: This example illustrates the effect of rounding and chopping on ad￾dition and subtraction operations. In some cases, such arithmetic operations with
rounded or chopped numbers may yield the same approximate result; however, the
consequences of rounding or chopping a number that is very small or large can be
far greater and these will be explained in the following sections. In such cases, high
precision calculations should be used in arithmetic operations.36  Numerical Methods for Scientists and Engineers
EXAMPLE 1.8: Loss of Significance
Compute the roots of x2 ´ 256x ` 1 “ 0 with three significant digits.
SOLUTION:
The roots of x2 ´ px ` q “ 0 are x1,2 “ pp ˘ ap2 ´ 4qq{2, which lead to
x1 “ 1
2
p256 ´ a2562 ´ 4q and x2 “ 1
2
p256 ` a2562 ´ 4q
or in decimal form: x1 “ 0.00390631, and x2 “ 255.99609369.
Rounding ?
2562 ´ 4 to three significant digits results in 256, eventually resulting
in x1 “ 0 and x2 “ 256 due to loss of significance. But, here, the loss of significance
can be avoided by multiplying the numerator and denominator with the conjugate
of the numerator to yield
x1 “ 2q
p ` ap2 ´ 4q Ñ x1 “ 2
256 ` 256 “ 0.00390625 “ 0.391 ˆ 10´2
Discussion: Note that in this example, the way the calculation is carried out caused
the loss-of-significance, not the errors in the computed data. By using an alternative
arithmetic expression, the smallest root was recovered with four significant digits.
Though it has been easy for this case to find an alternative way of computing the
same quantity that eliminated the loss-of-significance error, this may not always be
possible for every problem.
1.4.9 CONDITION AND STABILITY
Every problem requires a set of input and output data. The condition of a problem is
concerned with the sensitivity of the problem to perturbations in the input data. If a small
change in the input data yields large changes in the output, the problem is said to be
ill-conditioned.
In modeling physical events, a model may lead to erroneous output; however, physical
events with inherent instabilities are quite rare. If ill-conditioning is observed, then the
problem should most likely lie in the assumptions made, in the data, or in the physical
and/or numerical models or techniques being used. For instance, consider the following
cubic equation, whose roots are x “ 1, 13, and 14:
x3 ´ 28x2 ` 209x ´ 182 “ 0.
Now consider the cubic equation obtained by perturbing the coefficient of x3 as
0.99x3 ´ 28x2 ` 209x ´ 182 “ 0
We find the roots of this equation to be x «1, x «12.1378, and x «15.1449. Notice that, in
comparison to a small change in the coefficient of x3, the two large roots have significantly
deviated from the true solution.
Next, consider the following perturbed cubic equation:
1.01x3 ´ 28x2 ` 209x ´ 182 “ 0Numerical Algorithms and Errors  37
which yields a pair of complex conjugate roots with a respectable imaginary part: x «
0.999877 and x « 9.8961 ˘ 1.0437 i. This leads us to conclude that the second and third
roots are ill-conditioned; however, we cannot deduce anything about the condition of the
first root.
Consider the following matrix A and a slightly perturbed matrix A1
:
A “
»
–
2 100 0
0 2 100
00 2
fi
fl A1 “
»
–
2 100 0
0 20 100
10´4 0 20
fi
fl
which leads |A| “ 8 and |A1
| “ 9. This example from matrix algebra shows that slight
perturbations in numbers can lead to serious deviations in the final results.
Now, to analyze the condition of a problem, we consider a univariant function, fpxq,
subjected to a perturbation of δx. The corresponding perturbation in f can be estimated
by Taylor’s formula as
δf “ fpx ` δxq ´ fpxq « δx f1
pxq
Then, the relative error of the output can be expressed as
δf
f « xf1
pxq
fpxq ¨
δx
x
which yields true equality as δx Ñ 0. This suggests that the condition number fpxq be
defined as
κ –
ˇ
ˇ
ˇ
ˇ
xf1
pxq
fpxq
ˇ
ˇ
ˇ
ˇ
Note that the condition number provides a measure of how large the relative perturbation in
f is compared to the relative perturbation in x. A condition number of unity (κ “ 1) implies
that the input and output perturbations are identical. The effects of perturbation diminish
for κ ă 1 but amplify for κ ą 1. Large condition numbers are indicative of “ill-condition.”
The condition of a problem is independent of floating-point number systems or numerical
methods; it indicates the magnification of initial errors through exact calculations.
The stability of a method deals with the sensitivity of the numerical method to the
propagation of round-off errors during the numerical solution. It is desirable that the cumu￾lative effect of the round-off errors in any numerical method remain bounded, i.e., stable.
The stability of a method refers to the influence of rounding errors as a result of inexact
calculations due to finite precision arithmetic. A method that ensures accurate solutions is
said to be stable; otherwise, the method is unstable.
This is the point where we will introduce the Big O notation, Opxq,
which will be extensively used throughout the text for analyzing
the limiting behavior of a polynomial when x tends to either x Ñ 0
or x Ñ 8.
For fpxq “ 2x ` 3x2 ` 4x3, the function is expressed with its
effective dominant term as fpxq “ 2x ` Opx2q, or fpxq “ 2x `
3x2 ` Opx3q as x Ñ 0.
Similarly, for large x, the dominant term of the polynomial is
stated as x Ñ 8 by fpxq “ 4x3 ` Opx2q.38  Numerical Methods for Scientists and Engineers
1.5 APPLICATION OF TAYLOR SERIES
1.5.1 TAYLOR APPROXIMATION AND TRUNCATION ERROR
Truncation errors often arise when an approximation formula is made to an infinite series
by a partial (truncated) sum. A truncation error is defined as the difference between a true
value and its truncated (approximated) value.
Consider the Taylor series of fpxq that is infinitely differentiable at x “ a:
fpxq“fpaq` f1
paq
1! px´aq` f 2paq
2! px´aq
2`¨¨¨` fpnq
paq
n! px´aq
n`¨¨¨ (1.16)
The Taylor series for a “ 0 is referred to as the Maclaurin series.
In practice, it is impossible and impractical to sum up all the terms on the rhs of Eq.
(1.16). Hence, the Taylor series is truncated and replaced with the following N-term partial
sum, while higher-order terms are discarded:
fpxq « pN pxq“fpaq` f1
paq
1! px´aq` f 2paq
2! px´aq
2`¨¨¨` fpN´1qpaq
pN ´ 1q! px´aq
N´1 (1.17)
The truncated sum (approximation) is called the Taylor polynomial, and it results in esti￾mates of some degree of truncation error.
Truncation error is commonly used to assess how many terms are required to get an
estimate sufficiently close to a true value, and it is defined as
RN pxq “ fpxq ´ pN pxq “ fpNq
pξq
N! px´aq
N , a ď ξ ď x (1.18)
which is referred to as the remainder.
If x is sufficiently close to a, then px ´ aq will be small enough so that higher-order
terms in the approximation diminish. In such cases, truncating a series into a few terms
generally results in an approximation that yields an estimate that is sufficiently close to the
true value. In other words, if a truncation error is reasonably small, then an approximating
Taylor polynomial becomes a good approximation for the function the series represents.
However, Equation (1.18) has two major pitfalls: (1) ξ, which is unknown, lies somewhere
between a and x; (2) the Nth derivative of fpxq is required. If fpxq is known, then there is
no need for the Taylor series expansion in the first place! Nevertheless, Eq. (1.18) is useful
to gain insight on the magnitude of the truncation error and the upper bound of the error.
When assessing the truncation error, fpNq
pξq is generally replaced by its maximum absolute
value in a ď ξ ď x, i.e., M “ max|fpNq
pξq|. Then the upper bound of a truncation error
can be written as
RN pxq ď M
N!
px ´ aq
N (1.19)
Now, going back to Eq. (1.19), note that the truncation error is proportional to px ´
aqN with M and N! being constants. This expression, however, implies nothing about the
magnitude of M, yet it is still useful in assessing the comparative error of a numerical
approximation based on Taylor polynomials. With h “ x ´ a, the remainder is cast as
RN ” OphN q (1.20)
which implies that the truncation error is of the order of hN , i.e., |RN | ď C ¨ hN , where CNumerical Algorithms and Errors  39
is a positive constant. In a way, the Big O notation provides a relative measure of accuracy
as it indicates how rapidly the accuracy can be improved by reducing h.
The Maclaurin series of several commonly encountered functions is given as
ex “ 1 ` x `
x2
2! `
x3
3! `
x4
4! ` ... “ ÿ8
k“0
xk
k!
, p´8 ă x ă 8q (1.21)
sin x “ x ´ x3
3! `
x5
5! ´ x7
7! ` ... “ ÿ8
k“0
x2k`1
p2k ` 1q!
, p´8 ă x ă 8q (1.22)
cos x “ 1 ´ x2
2! `
x4
4! ´ x6
6! ` ... “ ÿ8
k“0
x2k
p2kq!
, p´8 ă x ă 8q (1.23)
lnp1 ` xq “ x ´ x2
2 `
x3
3 ´ x4
4 ` ... “ ÿ8
k“1
p´1q
k`1 xk
k , p´1 ă x ď 1q (1.24)
1
1 ´ x “ 1 ` x ` x2 ` x3 ` x4 ` ... “ ÿ8
k“0
xk, p´1 ă x ă 1q (1.25)
where the specified intervals denote the convergence intervals.
Stopping (Termination) Criteria. An easy way of evaluating a continuous and differen￾tiable function at x “ x0 is to truncate its Taylor (Maclaurin) series to an N-term partial
sum. Evaluating Eq. (1.17) at x “ x0 yields
SN “ s0 ` s1 ` s2 ` ... ` sN´1 “
N
ÿ´1
k“0
sk
where sk “ ckpx0 ´ aqk is the general term. However, the question becomes: what will be
the associated error with the SN ? or is SN good enough as an approximation?
Estimating truncation error via Eq. (1.18) may be impossible when fpNq
pxq is too
difficult or cumbersome to obtain. On the other hand, sometimes, even if fpNq
pxq can be
determined, finding its absolute or local maximum values to find the upper error bound
(especially ξ and fpNq
pξq) may pose another challenging problem. That is why we need
alternative quantitative tools for deciding when to terminate (truncate) an infinite series.
In a computational process, this decision is usually made rather easily when increasing the
degree of the polynomial (i.e., adding new terms to the series). To be more specific, to obtain
an approximation that is accurate within ε tolerance, we consider in our decision-making
the following relative or absolute error criterion:
For relative error: |Sk ´ Sk´1| ă ε ¨ |Sk| or equivalently |sk| ă ε ¨ |Sk|.
For absolute error: |Sk ´ Sk´1| ă ε or equivalently |sk| ă ε.
The relative error criterion is often preferred because it takes into account the relative
size of the partial sum Sk and the new term sk to be added. The absolute error criterion,
which is simpler and easier to apply, requires less effort as long as both Sk and sk are “fairly
sized,” i.e., Sk is not very large.
The number of correct decimals gives us an idea of the magni￾tude of the absolute error. Assuming fpNq
pxq is easily obtained, if
|RN | ă 0.5 ˆ 10´b, then we say that a truncated approximation
is correct to b decimal places.40  Numerical Methods for Scientists and Engineers
EXAMPLE 1.9: Effect of number of terms on accuracy of Taylor polynomials
Find approximating polynomials to compute the number “e” by truncating the
Maclaurin series of ex into an N-term finite sum (SN ), then (a) calculate SN , trunca￾tion error RN , and the true (absolute and relative) error for approximations of N “ 4
to 10; (b) for x “ 0.01, 0.05, 0.10, 0.50, and 1, assess the accuracy of S2 “ 1 ` x and
S3 “ 1 ` x ` x2{2 approximating polynomials.
SOLUTION:
(a) Using the Maclaurin series of ex given by Eq. (1.21), an approximation is
obtained by truncating the first N-terms (SN ) as
SN “ 1 ` x `
x2
2! `
x3
3! `
x4
4! ` ... `
xN´1
pN ´ 1q!
, RN “ xN
N!
eξ p0 ď ξ ď xq
where RN is the truncation error.
TABLE 1.3
N SN RN e ´ SN pe ´ SN q{e
4 2.66666667 0.11326174 0.0516152 0.0189882
5 2.70833334 0.02265235 0.0099485 0.0036598
6 2.71666667 0.00377539 0.0016152 0.0005942
7 2.71805556 0.00053934 0.0002263 0.0000832
8 2.71825397 0.00006742 0.0000279 0.0000102
9 2.71827877 7.4909 ˆ 10´6 3.0586 ˆ 10´6 1.1252 ˆ 10´6
10 2.71828153 7.4909 ˆ 10´7 3.0289 ˆ 10´7 1.1142 ˆ 10´7
To evaluate e, we set x “ 1 in the expressions for SN and RN . The absolute
maximum of eξ on [0,1] occurs at ξ “ 1. Thus, setting M “ e, the upper bound for
the truncation error is found to be RN “ e{N!.
In Table 1.3, the approximation (SN ), truncation error (RN ), and true (absolute
and relative) errors are presented for N “ 4 to 10 to depict the effect of N on
the approximation for the number e. Notice that a four-term approximation (S4)
yields 2.66666667, and its true absolute error (0.0516152) is less than the estimated
truncation error (=0.11326174). As N increases, both the true and truncation errors
are reduced. It is also worth noting that the true error is always less than the
estimated truncation error for all N.
TABLE 1.4
x S2 R2 S3 R3
0.01 1.0100000 0.0001359 1.0100050 4.530ˆ10´7
0.05 1.0500000 0.0033978 1.0512500 0.0000566
0.1 1.1000000 0.0135914 1.0105000 0.0004530
0.2 1.2000000 0.0543656 1.2200000 0.0036244
0.5 1.5000000 0.3397852 1.6250000 0.0566309
1.0 2.0000000 1.3591409 2.5000000 0.4530470
The effect of x on the accuracy of two- and three-term approximations (S2 and
S3), as well as the upper bounds for the truncation errors (R2 and R3), are presentedNumerical Algorithms and Errors  41
Pseudocode 1.7
Function Module EXPE (x, N)
\ DESCRIPTION: A pseudo function module to compute ex using an N-term
\ Taylor polynomial approximation, i.e., řN´1
k“0 xk{k!.
term Ð 1 \ Initialize term, term “ x0{0! “ 1
sum Ð 1 \ Initialize accumulator, sum
For “
k “ 1, N ´ 1
‰ \ Loop: series sum by accumulation
term Ð term ˚ x{k \ yields px{1qpx{2q¨¨¨px{kq “ xk{k!
sum Ð sum ` term \ Add the general term to accumulator
End For
EXPEÐ sum \ Set accumulated sum to EXPE
End Function Module EXPE
in Table 1.4. Notice that RN “ M xN {N! is proportional to the power of x; thus, the
truncation error for x ă 1 decays with relatively few terms. However, for large x ě 1,
it also increases, resulting in large approximation errors for S2 and S3. Nevertheless,
the magnitude of the errors for three-term approximations (R2 ą R3) is naturally
smaller than that of two-term approximations.
Discussion: This example illustrates that the value of SN differs from the true
value by no more than RN . That is, the true error of an approximation obtained by
a truncated sum from a series is always less than or equal to the truncation error
(Etrue ď |RN |). In other words, the truncation error can be confidently used to
estimate the accuracy of an approximation in cases where the true error cannot be
determined.
A pseudofunction, EXPE, that computes ex for an arbitrary x with an n-term Taylor
approximation is presented in Pseudocode 1.7. The arguments of the function are a real
number (x) and the number of terms of the approximation (N). A For-construct is used to
perform the finite sum since N is supplied as an input. Note that the accumulator variable
term, which corresponds to xk{k! (i.e., the general term), is also evaluated recursively
(termk “ termk´1 ˚ px{kq with term0 “ 1) primarily to (1) reduce cpu-time by averting the
direct computation of k! and xk; and (2) avoid possible overflow errors when calculating the
numerator and denominator separately. On exit, the N-term approximation (accumulator
variable sum) is set to EXPE. This module, however, does not provide much information
about the accuracy of the calculated approximation.
A pseudofunction, EXPA, computing ex for an arbitrary x with the known accuracy
using the Maclaurin series is given in Pseudocode 1.8. The arguments of the module are x
(a real number) and �, a user-supplied tolerance that may be reasonably replaced with the
machine epsilon, MACHEPS (see Pseudocode 1.5). The first three lines are the initializations:
the counter with k “ 0, the term with the first term of the series term “ a0, and the
accumulator with sum “ term. A While-construct with the relative error criterion (|term| ą
ε¨|sum|) is adopted to carry out the series approximation since the number of terms required
is not known beforehand. In the block statements, the computation of pk ` 1qth term
(term “ xk{k!) is carried out in the same manner as in Pseudocode 1.7, i.e., sum ` term.
The loop is terminated when the condition becomes False. Then the value of the accumulator
is assigned to the function EXPA on exit. Notice that this code applies to a convergent series;
therefore, it does not include any safeguard against the possibility of divergence.42  Numerical Methods for Scientists and Engineers
Pseudocode 1.8
Function Module EXPA (x, ε)
\ DESCRIPTION: A pseudo-function to compute ex using the Maclaurin series
\ within ε tolerance.
\ USES:
\ ABS:: A built-in function computing the absolute value.
k Ð 0 \ Initialize counter
term Ð 1 \ Initialize term with a0 “ x0{0!
sum Ð term \ Initialize accumulator with 1st term, S0 “ a0
While “
|term| ą ε|sum|
‰ \ Keep accumulating until |ak| ă ε ¨ |Sk|
k Ð k ` 1 \ Count the term being added
term Ð term ˚ x{k \ Added term is ak “ xk{k!
sum Ð sum ` term \ Add term to accumulator, Sk “ Sk´1 ` term
End While \ Go to top of the loop
EXPAÐ sum \ Set accumulated sum to EXPA
End Function Module EXPA
1.5.2 MEAN VALUE THEOREMS
The Mean Value Theorem for the derivative is obtained from the Taylor series using a
one-term approximation of fpxq at x “ b which is expressed as
fpbq “ fpaq`pb ´ aqf1
pξq, a ă ξ ă b
which can be used to find an approximation for f1
pxq at some point ξ in (a,b); that is,
f1
pξq “ fpbq ´ fpaq
b ´ a (1.26)
For the mean value theorem for the integrals, we consider two bounded and integrable
functions (fpxq and gpxq) where gpxq does not change sign in ra, bs. There is a number ξ in
the interval [a, b] such that
ż b
a
fpxqgpxqdx “ fpξq
ż b
a
gpxqdx (1.27)
For gpxq “ 1, it yields
ż b
a
fpxqdx “ fpξqpb ´ aq or fpξq “ 1
b ´ a
ż b
a
fpxqdx
where fpξq is the mean value of fpxq. Geometrically speaking, the area of the region under
the curve from a to b is equal to that of a rectangle whose lengths are fpξq and pb ´ aq.
The second mean value theorem for the integrals is given
(i) if fpxq is bounded, decreasing, and non-negative function in [a, b], and gpxq is a bounded
integrable function, then
ż b
a
fpxqgpxqdx “ fpaq
ż ξ
a
gpxqdx (1.28)
or (ii) if fpxq is bounded, increasing, and non-negative function in [a, b], and gpxq is a
bounded integrable function, then
ż b
a
fpxqgpxqdx “ fpbq
ż b
η
gpxqdx (1.29)Numerical Algorithms and Errors  43
1.6 TRUNCATION ERROR OF SERIES
Positive Series. The truncation error, or error estimation, of a series that is not generated
from the Taylor series is slightly different. To illustrate the truncation error, consider a
convergent infinite series, ř an, where tanu is a sequence of positive terms with an “ fpnq.
Estimating the error of the N-term partial sum is equivalent to evaluating:
indexError!truncation
RN “ S ´ SN “ aN`1 ` aN`2 ` aN`3 `¨¨¨
where S “ ř8
n“0 an and SN “ řN
n“0 an. The integral test can be applied to find lower and
upper bounds for the error.
The truncation error for positive, continuous, and decreasing fpxq for all x ě N satisfies
the following inequality:
ż 8
N`1
fpxqdx `
aN`1
2 ď RN ď
ż 8
N
fpxqdx ´ aN`1
2 (1.30)
Adding SN on each side of Eq. (1.30) yields
SN `
ż 8
N`1
fpxqdx `
aN`1
2 ď S ď SN `
ż 8
N
fpxqdx ´ aN`1
2 (1.31)
which indicates that the true error cannot be larger than the length of the interval in this
inequality. In cases where the integral test is impractical to assess the convergence of a
series, the comparison test should be employed.
Now consider another convergent infinite series, ř bn, which is an ď bn for all n. The
remainders for both series can be expressed as
RN “ ÿ8
n“N`1
an and BN “ ÿ8
n“N`1
bn
Since an ď bn, this leads to RN ď BN , and generally, bn is chosen such that it is a fairly
simple expression, which permits the integral test to be employed. An upper bound for the
partial sum is given as
RN ď BN ď
ż 8
N
gpxqdx (1.32)
where gpnq “ bn. If the magnitude of TN cannot be determined, then there is no benefit in
estimating the truncation error using the comparison test.
Next, consider applying the ratio test to a convergent series (ř an) with tanu being
a positive and decreasing sequence with limnÑ8|an`1{an| Ñ ρ ă 1. An estimate for the
upper and lower bounds of the truncation error can then be found as follows:
If |an`1{an| approaches the limit ρ from above, then we have
aN
ˆ ρ
1 ´ ρ
˙
ď RN ď
aN`1
1 ´ aN`1
aN
(1.33)
If |an`1{an| approaches the limit ρ from below, then we apply
aN`1
1 ´ aN`1
aN
ď RN ď aN
ˆ ρ
1 ´ ρ
˙
(1.34)44  Numerical Methods for Scientists and Engineers
Alternating Series. Let S “ ř8
n“1p´1qn´1an and let the N-term partial sum be SN “ řN
n“1p´1qn´1an, where tanu is a positive sequence with limnÑ8 an Ñ 0 and ř8
n“1 an
converges. The standard (Leibniz’s) error bound is given by
´aN`1 ă RN ă aN`1 (1.35)
EXAMPLE 1.10: Estimating truncation error of positive series
Consider the following infinite series:
(a) ÿ8
n“1
1
n2 ` 1 “ 1
2 pπ coth π ´ 1q, (b) ÿ8
n“1
n2
4n “ 20
27
Find the 10-term approximation (partial sum), the true error, and error bounds and
discuss the true error in relation to the error bounds.
SOLUTION:
(a) The 10-term partial sum is obtained as
S10 “ ÿ
10
n“1
1
n2 ` 1 “ 1
12 ` 1 `
1
22 ` 1 `
1
32 ` 1 `¨¨¨`
1
102 ` 1 “ 0.9817928223
and the true error is Etrue “ 1
2 pπ coth π ´ 1q ´ 0.98179282 “ 0.0948812.
Using Eq. (1.31), the truncation error satisfies
ż 8
11
fpxqdx `
a11
2 ď R10 ď
ż 8
10
fpxqdx ´ a11
2
Setting fpxq “ 1{p1 ` x2q and noting ş8
N fpxqdx “ π{2 ´ tan´1pNq, we find
π
2 ´ tan´1p11q ` 1{122
2
ă R10 ă π
2 ´ tan´1p10q ´ 1{122
2
or 0.09475825 ă R10 ă 0.09557029.
(b) The first 10-term partial sum yields
S10 “ ÿ
10
n“1
n2
4n “ 12
41 `
22
42 `
32
43 `¨¨¨`
102
410 “ 0.74069977
which leads to Etrue “ 20{27 ´ 0.74069977 “ 4.0971 ˆ 10´5.
Setting an “ n2{4n and applying the ratio test, we find
ρ “ limnÑ8
an`1
an
“ limnÑ8
pn ` 1q2
4n2 Ñ 4
also we have a11{a10 “ p112{411qp410{102q“121{400. Note that |an`1{an| approaches
the limit value of ρ “ 1{4 from above as n is increased, in which case the upper and
lower bounds for the truncation error are obtained from Eq. (1.33):
a10 ˆ ρ
1 ´ ρ
˙
ď R10 ď
a11
1 ´ a11
a10Numerical Algorithms and Errors  45
Substituting the numerical values results in
102
410 ˆ 1{4
1 ´ 1{4
˙
ă R10 ă
112
411 ˆ
1 ´ 121
400˙´1
or 3.1789 ˆ 10´5 ă R10 ă 4.136 ˆ 10´5.
Discussion: Note that the true errors in both cases are within the error bounds. If
the true error happens to be near the middle of the bounding interval, adding the
arithmetic average of the upper and lower error bounds to the approximation will
bring it closer to the true value. For the series in Part (a), adding the arithmetic
average of the upper and lower error bounds (=0.095164270) to the approximation
yields 1.07695709, which reduces the true error to 2.8304 ˆ 10´4. Likewise, for the
series in Part (b), adding the average of the upper and lower bounds (“ 3.65745 ˆ
10´5) to S10 results in 0.74073634, which is in error by 4.396 ˆ 10´6.
1.7 RATE AND ORDER OF CONVERGENCE
In numerical analysis, there is often more than one method to solve a particular problem.
Many of the numerical methods covered in the subsequent chapters are iterative in nature.
These methods always generate a sequence of estimates through successive approximations
that converge toward a solution. Some sequences depict very slow and some very rapid
convergence. In such cases, when determining the most suitable method for the numeri￾cal solution of a problem, the method whose sequence converges as rapidly as possible is
preferred.
The order of convergence and the rate of convergence of a convergent sequence are
quantities that indicate how quickly a sequence approaches its limit value. One of the ways
in which numerical algorithms are compared is via their order and rates of convergence.
These are also the most important factors to consider when choosing one method over
another. The methods with a high order or rate of convergence require fewer iterations to
reach a reasonable solution.
A sequence txnu is said to converge or be convergent if
limnÑ8xn Ñ L or limnÑ8|xn ´ L| Ñ 0
where L is the limit of the sequence and denotes the value to which the sequence converges.
The sequence is said to diverge if limnÑ8 xn does not exist.
Let txnu be a sequence that converges to a number L. If there exists a sequence tqnu
that converges to zero and a constant λ ą independent of n, such that
|xn ´ L| ď λ|qn| (1.36)
for large values of n, then txnu is said to converge to L with the rate of convergence Opqnq,
which is commonly expressed in shorthand notation as xn “ L ` Opqnq, and λ is called the
asymptotic error constant.
The sequence tqnu is generally chosen as a p-sequence (i.e., řp1{npq for p ą 0) or a
geometric sequence (i.e., ř rn, r ă 1) to allow for simple comparison between different
sequences. For instance, a sequence with a convergence rate of Op1{2nq converges faster
than the one with Op1{n4q, which in turn converges faster than a sequence with a rate of46  Numerical Methods for Scientists and Engineers
TABLE 1.5: Convergence behavior of the sequences an, bn, and cn.
n an bn cn
1 1.000000 1.000000 1.000000
2 1.500000 1.750000 1.875000
3 1.666667 1.888889 1.962963
4 1.750000 1.937500 1.984375
5 1.800000 1.960000 1.992000
.
.
. ... ... ...
10 1.900000 1.990000 1.999000
11 1.909091 1.991736 1.999249
12 1.916667 1.993056 1.999421
.
.
. ... ... ...
20 1.950000 1.997500 1.999875
convergence of Op1{n2q. For instance, consider an “ 2´1{n, bn “ 2´1{n2, and cn “ 2´1{n3,
all of which converge to “2.” In Table 1.5, the terms of the sequences up to n “ 20 are
presented. From Eq. (1.36), we find that |an ´ 2| ď p1{nq (i.e., λ “ 1 and qn “ 1{n) so that
we may write an “ 2`O p1{nq. Similarly, we obtain bn “ 2`O `
1{n2
˘
and cn “ 2`O `
1{n3
˘
,
which converges fastest.
EXAMPLE 1.11: Limit and rate of convergence of sequences
Find the limit and the rate of convergence of the following sequences:
(i) an “ 3n2 ` 7
4n2 ` 3n ` 1
, (ii) bn “ 2n ` 3
?
9n2 ` 2
, (iii) cn “ n sin ˆ 2
n
˙
SOLUTION:
(i) The limit at which the sequence should converge is found as limnÑ8 an “
3{4. To find the convergence rate, we express the general term of the sequence as
a function of (1/n). Dividing both the numerator and denominator with n2 and
simplifying yields:
an “ 3n2 ` 7
4n2 ` 3n ` 1 “ 3 ` 7p1{nq2
4 ` 3p1{nq`p1{nq2
Setting x “ 1{n, the sequence becomes p3 ` 7xq{px2 ` 3x ` 4q. For n Ñ 8 (i.e.,
x Ñ 0), it can be expanded to the Maclaurin series as follows:
an “ 3
4 ´ 9
16 ˜
1
n
¸
`
127
64 ˜
1
n
¸2
` ... or “ 3
4 ´ 9
16
x `
127
64 x2 ` ...
For very large n, we arrive at |an ´ 3{4| “ p9{16qp1{nq, from which the convergence
rate and λ are found to be Op1{nq and 9/16, respectively.
(ii) The limit value of the sequence is limnÑ8 bn “ 2{3. To determine the con￾vergence rate, the general term of the sequence is likewise expressed as a function of
1{n and expanded to a Maclaurin series to giveNumerical Algorithms and Errors  47
bn “ 2n ` 3
?
9n2 ` 2 “ 2 ` 3p1{nq
a9 ` 2p1{nq2
“ 2
3 `
ˆ 1
n
˙
´ 2
27 ˆ 1
n
˙2
` ... or “ 2
3 ` x ´ 2
27
x2 ` ...
which may also be cast as |bn ´ 2{3| “ p1{nq for very large n. This expression gives
the convergence rate as Op1{nq and λ “ 1.
(iii) The limit value of this sequence is limnÑ8 cn “ 2. This general term is
already in the form of 1{n. Setting 1{n “ x and expanding it to the Maclaurin series
gives
cn “ n sin ˆ 2
n
˙
“ sinp2{nq
p1{nq
“ 2 ´ 4
3
ˆ 1
n
˙2
`
4
15 ˆ 1
n
˙4
` ... or “ 2 ´ 4
3
x2 `
4
15
x4 ´ ...
which leads to |cn ´ 2| “ p4{3qp1{nq2, yielding the rate of convergence of Op1{n2q
and λ “ 4{3.
Discussion: For a sequence whose general term is given by an “ gpnq, a function
fpxq is obtained by substituting n “ 1{x in the general term. Then fpxq is expanded
into a Maclaurin series. For very large n, the series expansion will have L ` λxm
form or |an ´ L| “ λp1{nqm, where m is the order of convergence.
Let us consider again the sequence txnu that converges to L. A stronger condition for
the convergence rate is given by
limnÑ8
|xn`1 ´ L|
|xn ´ L|
r “ limnÑ8
|en`1|
|en|
r Ñ λ (1.37)
where en “ xn ´ L is the absolute error, λ is the asymptotic error constant, and r (ě 1) is
called the order convergence. For a linearly converging sequence, the order of convergence
r is 1 while the sequence is said to have an order of convergence of r “ 2 for quadratically
converging sequences.
A practical way of estimating the rate of convergence needs to be established since
the value of L in Eq. (1.37) is also being computationally predicted. For a sufficiently large
number of iterations, we may replace Eq. (1.37) by
|en`1| « λ|en|
r or |en| « λ|en´1|
r (1.38)
The order (rate) of convergence can then be estimated by eliminating λ and solving for
r, which leads to
r « logp|en`1|{|en|q
logp|en|{|en´1|q “ logp|xn`1 ´ xn|{|xn ´ xn´1|q
logp|xn ´ xn´1|{|xn´1 ´ xn´2|q (1.39)
Note that the estimate for the order of convergence with Eq. (1.39) is good only for suffi￾ciently large n.48  Numerical Methods for Scientists and Engineers
EXAMPLE 1.12: Determining rate of convergence of a sequence
Determine the order of convergence of the following sequences:
(a) xn “ 2 ´ 1{n3, (b) yn “ 2 ´ 1{22n
SOLUTION:
(a) Several terms of the sequence, the absolute difference between two consecutive
terms, the logarithm of the ratio of two consecutive absolute differences, and the
estimate for the order of convergence by Eq. (1.39) are tabulated in Table 1.6.
TABLE 1.6
n xn |xn ´ xn´1| logp|en|{|en´1|q r
1 1.000000000000
2 1.875000000000 0.875000000
3 1.962962962963 0.087962963 ´0.99770820
4 1.984375000000 0.021412037 ´0.61364186 0.61505144
5 1.992000000000 0.007625000 ´0.44841814 0.73074893
6 1.995370370370 0.003370370 ´0.35456222 0.79069554
7 1.997084548105 0.001714178 ´0.29362178 0.82812483
8 1.998046875000 0.000962327 ´0.25073323 0.85393266
.
.
. .
.
. .
.
. .
.
.
30 1.999629629629 3.9651ˆ10´6 ´0.05993833 0.96547547
ÓÓ Ó Ó Ó
8 2 0 01
Since limnÑ8 xn “ 2, the sequence is convergent and the order of convergence is 1.
(b) In Table 1.7, the first six terms of the sequence, the absolute difference be￾tween two consecutive terms, the logarithm of the ratio of two consecutive absolute
differences, and the estimate for the rate of convergence by Eq. (1.39) are listed. It is
seen that the sequence converges much faster (i.e., the rate of convergence is larger
than in (b)).
TABLE 1.7
n yn |yn ´ yn´1| logp|en|{|en´1|q r
1 1.75000000
2 1.93750000 0.18750000
3 1.99609375 0.05859375 ´0.50514998
4 1.99998474 0.00389099 ´1.17779104 2.33156705
5 2 1.52593ˆ10´5 ´2.40654681 2.04327144
6 2 2.3283ˆ10´10 ´4.81647330 2.00140437
ÓÓ Ó Ó Ó
8 2 0 ´8 2
Discussion: Note that the sequence xn converges rather slowly. The absolute error
is reduced to the order of 10´6 for n «25-30. As r Ñ 1, the sequence depicts linearly
convergent behavior for increasing n. On the other hand, the sequence yn converges
to 2 much faster, in n «5-6. The absolute error (|yn ´ yn´1|) quickly goes to zero
and r approaches 2 even for small n, indicating a quadratic convergence rate.Numerical Algorithms and Errors  49
1.8 CLOSURE
Nowadays, science and engineering calculations have evolved to the point where they are too
complex to be carried out by hand. Numerical methods are the main source for modeling
science and engineering problems on computers. Communicating with computers is achieved
through software designed in various programming languages. In this regard, it is important
to understand and code a numerical method to be used in digital computers.
There are many programming languages out there. This is because different tasks re￾quire different tools to solve them, and each programming language has certain features that
make it suitable for a specific task. Therefore, it is inevitable that a scientist or engineer will
encounter problems that require the use of more than one programming language and/or
symbolic processing software throughout his career. Most programming languages provide
mechanisms for writing programs using the same type of constructs common to all, even
though the syntax of those languages is quite different. That is why this chapter focused on
presenting and explaining the basic elements of common algorithms (sequential constructs,
conditional constructs, loops, accumulators, etc.) using pseudocodes. While learning numer￾ical methods, a special emphasis is placed on writing codes that use modular and efficient
programming techniques.
Most people falsely believe that when a computer is used for a computational task,
the result (answer) is unquestionably correct. But in scientific computing, the computed
answer rarely represents the true answer. The inexactness of the result depends on the
mathematical model, or the use of experimentally derived data, and the truncation and/or
round-off errors acquired in computations. Roundoff and truncation errors are common
in any numerical calculation. A roundoff error occurs as the floating-point numbers are
represented with finite precision. On the other hand, truncation error occurs when we do a
discrete approximation to a continuous function. In this chapter, some examples of each of
these errors were given, and their significance was discussed. The definitions of error were
presented along with the computation and reporting of the sources of numerical errors. The
errors from several variables could propagate so much as to affect the final value. The error
propagation formula was presented in this connection to estimate the propagation of error
in computed or estimated quantities. The reason for the inexactness of digital computation
is commonly related to how numbers are stored and used in computers. The concepts of
accuracy, precision, round-off, significant digit, significant place, and loss of significance are
defined and illustrated with examples. Apart from the errors caused by computer hardware
operations, numerical analysts may encounter condition and stability issues depending on
how a numerical procedure is carried out. These problems are illustrated, as are techniques
or methods to avoid them.
Most numerical methods are almost exclusively based on approximations, especially
Taylor series approximations. In this respect, estimation of a quantity by truncating the
Taylor series is covered in detail, including efficient programming techniques for a series sum
(or product) and estimation of the truncation errors. Additionally, the basic convergence
theorems of sequences and series (including alternating series) are presented. Important
criteria for iterative methods are the order and rate of convergence, which are covered with
examples. Often, we also encounter problems that are insensitive or well-conditioned when
there are small errors in the problem parameters. In other words, a small perturbation to
a problem parameter of a well-conditioned problem yields only a small perturbation of the
true solution. On the other hand, sometimes we are faced with ill-conditioned problems
whose parameters (i.e., variables) are sensitive to small perturbations, which result in a
significant deviation from the final result.50  Numerical Methods for Scientists and Engineers
1.9 EXERCISES
Section 1.1 Numerical Algorithms in Pseudocodes
E1.1 The period of a pendulum is determined by T “ 2π
aL{g, where L is its length and g is
the acceleration of gravity. Write a pseudocode for calculating the period of a pendulum for an
arbitrary input L.
E1.2 Write a pseudocode to find the area of an arbitrary triangle with the side lengths a, b, and
c, using A “ asps ´ aqps ´ bqps ´ cq where s “ pa ` b ` cq{2.
E1.3 A pump curve is approximated by hpQq “ 31.2 ` 0.003Q ´ 4.29 ˆ 10´6Q2 ´ 8.6 ˆ 10´10Q3,
where Q is the flowrate (in liters/min) and h is the head (in meters). Write a pseudocode for
estimating the head (using conformal units) for an input flowrate.
E1.4 Write a pseudocode that calculates and prints out the slope of the line passing through two
arbitrary input points, Ppx1, y1q and Qpx2, y2q, and the distance between them.
E1.5 Write a pseudocode to calculate the dot product (u ¨ v) of the arbitrary input vectors
u “ pu1, u2, u3q and v “ pv1, v2, v3q and the angle between them (in degrees).
E1.6 When a circular cone is cut parallel to the base, a frustum shown
in Fig. E1.6 is obtained. The slant length (l), volume (V ), lateral surface
(Slat), and the total surface area (S) can be determined from the fol￾lowing expressions. Write a pseudo-program to calculate the volume and
the surface areas for arbitrary input values of R, h, and r.
l “ ah2 ` pR ´ rq2, V “ πh
3 pR2 ` r2 ` Rrq
Slat “ πlpR ` rq, S “ πpl ˚ pR ` rq ` R2 ` r2q
Fig. E1.6
E1.7 Write a pseudocode to calculate the following expression for an arbitrary x.
fpxq “ " x2 ` 3x ` 1, x ď 0
4x2 ´ 5x ` 1, x ą 0
E1.8 Write a pseudocode to convert an input temperature value from ˝F to ˝C (switch=1) or
from ˝C to ˝F (switch=2) using the conversion formula ˝F “ 32 ` 1.8˝C. Use an integer-type
switch variable to select a conversion formula.
E1.9 Repeat E1.2, extending the pseudocode to include validity checks (a ą 0, b ą 0, and c ą 0)
for the input variables. Note: When an invalid entry (a ď 0, b ď 0, or c ď 0) is encountered, an
error message should be prompted, asking the user for valid input. You may use a switch (a logical
type variable) to determine if the input is valid or not.
E1.10 Consider finding the roots of a quadratic equation of the form ax2 ` bx ` c “ 0. Write a
pseudocode that takes into account all possible outcomes (a “ 0 or b “ 0 or c “ 0, Δ ă 0, Δ ą 0,
Δ “ 0 and so on).
E1.11 Write a pseudocode to calculate the arithmetic, geometric, harmonic, and quadratic means
of a sequence (xi) of real numbers of length n, read sequentially from a file. Note: Avoid using an
array variable.
ma “ 1
n
ÿn
i“1
xi, mh “ n
˜ÿn
i“1
1
xi
¸´1
, mg “
˜źn
i“1
xi
¸1{n
, mq “
˜ 1
n
ÿn
i“1
x2
i
¸1{2Numerical Algorithms and Errors  51
E1.12 Write a pseudocode that uses (a) For- and (b) While-constructs to find the sum and average
of all odd numbers from 1 to n.
E1.13 A formula for wind chill index (WC) as a function of ambient temperature Ta (
˝C) and
wind speed V (km/h) is given as follows:
WCpTa, V q “ 13 ` 0.6T ´ 11.4V 1{6 ` 0.4T V 1{6
, Ta ă 30˝
C and V ą 6 km/h
Write a pseudocode that accepts ambient temperature and wind speed as an input, checks the
valid range of the input variables, and prompts the user to re-enter the number(s) in case “wrong
type” or “out-of-range” data is detected.
E1.14 Consider the motion of an object thrown into the air. The horizontal and vertical displace￾ments of the object relative to its initial position are described by
x “ V t cos θ, y “ V t sin θ ´ 1
2
gt2
where t, V , θ, and g denote the time, launch velocity, launch angle (in degrees), and the acceleration
of gravity, respectively. The velocity components, velocity magnitude, and direction of the object
can then be obtained from
vx “ dx
dt , vy “ dy
dt , U “
b
v2
x ` v2
y, tan α “ vy
vx
Write a pseudocode to find the horizontal and vertical positions, velocity magnitude, and direction
of the object until it hits the ground. Note: Use the Repeat-Until construct and assume time
intervals of Δt “ V ˚ sin θ{50.
E1.15 Write a pseudocode to find the largest value and its position in an arbitrary array of n
positive integers. Note: Use a For-construct.
E1.16 An integer array of length n consists of 1’s, 2’s, ..., 5’s. Write a pseudocode that reads
the array (a) and its length (n) from a file and determines the frequency of each value. Note: Use
a For-construct and assume an array of maximum length 999.
E1.17 Write a pseudo-Function Module LOG(a, x) to compute the logarithm of a real number x
to an arbitrary base a. Make use of the change of base formula.
E1.18 Write a pseudo-Module LINE for the problem given in E1.4.
E1.19 Write a pseudo-Function Module FX for the function given in E1.7.
E1.20 The double factorial function is defined as
n!! “
" p2kq!! “ 2 ¨ 4 ¨ 6 ¨¨¨p2kq, n “ 2k
p2k ` 1q!! “ 1 ¨ 3 ¨ 5 ¨¨¨p2k ` 1q, n “ 2k ` 1
where 0!!=1!!=1. Write a recursive pseudo-Function Module DBL_FACTORIAL that evaluates a dou￾ble factorial with the least computation time. Note: Include a validity check for n and suggest a
proper action with a warning message in case of invalid input.
E1.21 The binomial coefficient, Bpn, kq, can be efficiently computed as
Bpn, kq “ npn ´ 1qpn ´ 2q¨¨¨pn ´ pk ´ 1qq
kpk ´ 1qpk ´ 2q¨¨¨ 1 “ źk
j“1
n ` 1 ´ j
j
Since it is symmetrical with regard to k and n ´ k, its computation can be further simplified as
Bpn, kq “ " Bpn, kq, k ď n{2
Bpn, n ´ kq, k ą n{2
Write a pseudo-Function Module B(n,k) that calculates the binomial coefficient most efficiently
(with minimum arithmetic operations) for arbitrary input values of n and k.52  Numerical Methods for Scientists and Engineers
E1.22 The Fibonacci numbers form a sequence in which each number is the sum of the two
preceding ones. It is expressed algebraically by the following recurrence relation:
F0 “ 0, F1 “ 1, Fn “ Fn´1 ` Fn´2, for n ě 2
Write a pseudo-Function Module FIBONACCI(n) that generates the Fibonacci numbers without
using arrays.
E1.23 Write a pseudocode that constructs Pascal’s triangle from the first to the nth power for an
arbitrary input value n. The code should print all rows, beginning with the first and ending with
the last. The coefficients of the kth power are found from the following recurrence relation:
c10 “ 1, c11 “ 1, ckj “ ck´1,j´1 ` ck´1,j , k ą 1
where k is the row number. Use two one-dimensional arrays (of lengths n and n ` 1) to store two
successive (prior and current) rows of the triangle. Prepare a Module PASCALS_TRIANGLE that
finds the coefficients of Pascal’s triangle for the kth row and uses arrays of a maximum length of
99. The code should also check for the bounds and validity of the input data.
Section 1.2 Basic Definitions and Sources of Error
E1.24 Describe the sources of modeling (mathematical model) errors.
E1.25 Describe a computer simulation and its purpose.
E1.26 Briefly name the sources of numerical errors.
E1.27 In your own words, define accuracy and precision.
E1.28 In your own words, define the systematic errors and give an example of a systematic error.
E1.29 In your own words, define random errors and give an example of a random error.
E1.30 Define blunders in coding or programming. Suggest ways or approaches to avoid blunders
in programming.
E1.31 Consider the mathematical model Fnet “ mpdv{dtq“´mg for a falling object. Here g is
the acceleration of gravity, m is the mass, and v is the velocity of the object. Do you think that
this model would be sufficient to describe the motion of a parachutist?
E1.32 Consider the mathematical model dp{dt “ a pptq for the population growth of a species.
Here a is a constant and pptq denotes the population size. Do you think that this model would be
sufficient to describe the population of a species?
E1.33 Consider a cubic-shaped carbon steel specimen placed on a graphite (C) layer. The model
proposed for the diffusion of carbon atoms to the bottom surface of a specimen at high tem￾perature is given as BC{Bt “ D B2C{Bz2, where D is the diffusion constant and Cpz, tq is the
concentration of diffusing atoms in z (vertical) direction as a function of time t. Do you think that
this model adequately describes the diffusion process of carbon atoms into the specimen? Under
what conditions would this mathematical model provide reasonably accurate estimates?
E1.34 The true surface area of an object is known to be A “ 1.25 m2. A computer model numer￾ically estimates the surface area as 1.252 m2. Calculate the absolute, relative, percent absolute,
and relative errors of the estimate.
E1.35 Suppose the true speed of a flying object is V “ 150 m/s. Its speed is also estimated by
two measurements: 148.33 and 152.82 m/s. Calculate the absolute and relative errors as well as
the percent absolute and relative errors for each velocity estimate.
E1.36 Calculate the absolute and relative errors of the following quantities with respect to the
reported approximations.
(a) Qt “ 10000, Qa “ 9997, (b) Qt “ 789.6573, Qa “ 789.6643,
(c) Qt “ 1.4587, Qa “ 1.3678, (d) Qt “ 0.00089645, Qa “ 0.00089512
E1.37 Determine the absolute and relative errors resulting from computing sinpπ{4q as sinp0.785q.Numerical Algorithms and Errors  53
E1.38 Find the absolute and relative errors made by calculating π as 22{7.
E1.39 Find the absolute and relative errors made by calculating e as 19{7.
E1.40 Determine the absolute and relative errors made by calculating ?1.1 as 1.05.
E1.41 Determine the absolute and relative errors made by calculating 1/?1.4 as 0.8.
E1.42 Determine the absolute and relative errors made by computing e0.04 and e0.1 using ex «
1 ` x ` x2{2.
Section 1.3 Propagation of Error
E1.43 What would be the change in volume (in %) if the height and radius of a cone differed by
+2% and ´2%, respectively?
E1.44 The force of attraction between the charges q1 and q2 at a distance r from each other is
dictated by F “ kq1q2{r2, where k is a constant. Estimate the percent change in force if each
charge is increased by 2% and the distance is reduced by 3%.
E1.45 The pressure of an ideal gas of volume V , held at temperature T, is determined by the
ideal gas law P V “ nRT, where n is the amount of gas and R is the universal gas constant.
Estimate the percent change in pressure if the temperature of the gas is reduced by 3% and the
volume is increased by 2%.
E1.46 Find the percent error in y at x “ 0.8 if y “ 3x ` 2x4 and the error in x is 0.01.
E1.47 Consider Newton’s second law of motion, F “ ma, for a projectile with m “ p125 ˘ 2q kg
and a “ p2˘0.1q m2/s. Find the maximum possible absolute error that can be made in calculating
the force.
E1.48 The measured dimensions of a cone are reported with their mean uncertainty as D “
p6 ˘ 0.03q m and h “ p4 ˘ 0.02q m. Develop an expression for the uncertainty of volume (in
percent) and estimate the volume of the cone as well as its uncertainty.
E1.49 A wire of length L0 undergoes thermal elongation when subjected to a temperature rise
ΔT “ T ´ T0. The length of a wire as a function of temperature difference is given by L “
L0p1 ` βΔTq, where β is the linear expansion coefficient. Develop an expression to estimate the
percent uncertainty in L in terms of the uncertainties of L0, β, and ΔT.
E1.50 The volume of a parallelepiped is given with V “ abc sin θ, where a “ p8 ˘ 0.01) cm,
b “ p10 ˘ 0.05) cm, c “ p9 ˘ 0.1) cm, and θ “ pπ{6 ˘ π{60q radians. Find the upper and lower
bounds (i.e., uncertainty) for the estimated volume.
E1.51 The law of cosines, a2 “ b2 ` c2 ´ 2bc cos β, relates the lengths of adjacent sides of a
triangle to the cosine of the angle (β) between them. If the side lengths of a triangle are given as
a “ p20 ˘ 0.1) cm, b “ p30 ˘ 0.1) cm, and c “ p12 ˘ 0.1) cm, estimate β and dβ.
E1.52 A plot of land has the shape of a parallelogram. The lengths of adjacent sides and the
angle between them are reported as a “ p15 ˘ 0.2q m, b “ p12 ˘ 0.15q m, and θ “ p38 ˘ 2.5q
˝,
respectively. Estimate the area of the plot, its average, and its standard deviations, assuming the
reported errors (uncertainties) are averages or standard deviations.
E1.53 A plot of land has the shape of a trapezoid. Its bases and height are reported with their
standard deviations as a “ p120˘0.55q m, b “ p78˘0.45q m, and h “ p53˘0.38q m, respectively.
Find the area of the land and estimate its standard deviation.
E1.54 The mass of a spherical shell (tank) can be expressed in terms of its internal radius (R)
and its thickness (d) as m “ p4π{3qρ
`
pR ` dq
3 ´ R3˘
, where ρ “ 8000 kg/m3. The mean values
and standard deviations of radius and thickness measurements made at different points on the
shell are p113 ˘ 1.27q cm and p2.53 ˘ 0.13q cm, respectively. Estimate the mass of the shell and
the associated standard deviation.
E1.55 The thermal (volume) expansion of a liquid is described by ΔV “ βV0ΔT, where β is the
volumetric expansion coefficient, ΔT is the temperature rise, and V0 is the initial volume of the54  Numerical Methods for Scientists and Engineers
liquid. Use the furnished data to estimate the standard deviation of ΔV . Data: β “ p1100 ˘ 33q ˆ
10´6 K´1, V0 “ p0.5 ˘ 0.01q, m3, and ΔT “ p25 ˘ 0.5q K.
E1.56 The acceleration of gravity can be experimentally determined by using a simple pendulum
experiment. Estimate the acceleration of gravity and associated uncertainty using T “ 2π
a�{g and
the results of the measurements for a pendulum with � “ p73 ˘ 0.5q cm and T “ p1.714 ˘ 0.0096q.
E1.57 Resistance R of a wire is related to its resistivity ρ, length L, and cross-section area A by
the relationship R “ ρL{A. For a particular wire, the mean values and standard deviations of the
measured values are R “ p0.0753˘0.0005q Ω, L “ p122˘2.3q cm, and D “ p1˘0.04q mm (circular
in cross section). Estimate the wire resistivity (in Ω-cm) and associated standard deviation.
E1.58 A bar, rectangular in cross section, is fixed to a wall at one end,
as shown in Fig. E1.58. The length L, the width w, and the thickness
t are reported as p25 ˘ 0.01q mm, p5 ˘ 0.01q mm, and p1 ˘ 0.01q mm,
respectively. A relationship for the deflection of the bar upon application
of force is given by F “ Ewt3y{4L3. If the deflection y is measured as
p2.5 ˘ 0.01q mm, estimate the applied force and the resulting standard
deviation. The elasticity modulus of the bar is E “ 2 ˆ 105 MPa. Fig. E1.58
E1.59 An Atwood machine consists of two masses coupled by a massless
string over a massless pulley, as shown in Fig. E1.59. When the two weights
are unequal, the system will move so that the heavier mass is pulled down
while the lighter mass is pulled up. The acceleration of an Atwood machine
is given by the relationship:
a “ g m1 ´ m2
m1 ` m2
(a) Estimate the system acceleration and its standard deviation for the given
m1 “ p110 ˘ 1q g, m2 “ p70 ˘ 1q g, and g “ 9.81 m/s2. (b) Repeat (a) with
g “ 9.80655 m/s2 and determine the effect of rounding on the result. Fig. E1.59
E1.60 The thermal conductivity of an alloy is determined experimentally. A rectangular sample
with a cross section of A “ p10 ˘ 1q cm2 and a length of L “ p20 ˘ 1q cm was used for this
experiment. All sides of the sample were perfectly insulated. When Q “ p2400 ˘ 25q J heat was
supplied to one end of the bar for a duration of t “ p120 ˘ 2q s, the temperatures at both end
points stabilized at T1 “ p55 ˘ 1q
˝ C and T2 “ p38 ˘ 1q
˝C. Estimate the thermal conductivity (k)
of this alloy with its standard deviation, assuming that the heat transferred can be calculated by
Q{t “ kApT1 ´ T2q{L.
Section 1.4 Numerical Computations
E1.61 Convert the following integers to their binary, octal, and hexadecimal equivalents.
(a) 36, (b) 17, (c) 81 , (d) 341
E1.62 Convert the following decimal numbers to their binary, octal, and hexadecimal equivalents:
(a) 0.015625, (b) 3.4375, (c) 38.125, (d) 180.328125
E1.63 Convert the following numbers to their decimal equivalents:
(a) p1110.0011q2, p10111.11q2, p110101.0101q2
(b) p1572.432q8, p30254.027q8, p123.4567q8
(c) p2AB1.Fq16, pDE1.A0Bq16, p234.ABCq16Numerical Algorithms and Errors  55
E1.64 Express the following quantities in normalized scientific notation:
(a) 1.2345 (b) ´ 3.10203 (c) 0.00012345 (d) 123456 (e) 0.0000011, (f) 2.34 ˆ 10´12
E1.65 Express the following real numbers in normalized scientific notation:
(a) 54.3210, (b) 765.45, (c) 0.54325, (d) 0.00543,
(e) 0.5432 ˆ 102, (f) 5.432 ˆ 103, (g) 5.54 ˆ 10´3
E1.66 In a decimal floating-point system, the number 10{9 “ 1.1111111 ¨¨¨ does not have an
exact representation with finite precision. Is there a finite floating-point system where the number
can be exactly represented?
E1.67 Convert 3377{343 “ p9.845481049 ¨¨¨q10 to a base-7 number system. Is the floating-point
representation exact?
E1.68 Determine the number of significant digits in the following numbers:
(a) 54.3210, (b) 5.4003, (c) 0.54321, (d) 0.00543,
(e) 0.5432 ˆ 102, (f) 5.432 ˆ 101, (g) 0.54 ˆ 10´3
E1.69 Express the following approximate and true quantities in normalized scientific notation:
(a) Q˜ “ 123.012, Q “ 123.03, (b) Q˜ “ 12.3469, Q “ 12.3458
(c) Q˜ “ 0.012313, Q “ 0.01236, (d) Q˜ “ 999.90, Q “ 1000
E1.70 For the approximate (Q˜) and true (Q) quantities given in E1.69, determine the number of
decimal digit accuracy of Q˜.
E1.71 Determine the “most accurate” one among the sets of approximate (Q˜) and true (Q)
quantities given below.
(a) Q˜ “ 785600, Q “ 785543, (b) Q˜ “ 0.00035, Q “ 0.0003544
(c) Q˜ “ 1.89315, Q “ 1.8942120, (d) Q˜ “ 5.415 ˆ 10´3, Q “ 5.4421 ˆ 10´3
E1.72 Determine the number of significant digits of the following approximations for π.
(a) π –
22
7 , (b) π –
333
106 , (c) π –
355
113
E1.73 Determine the number of significant digits in the following approximations for e.
(a) e –
19
7 , (b) e –
87
32 , (c) e –
193
71
E1.74 Round the following numbers to two decimal places:
(a) 84.61485, (b) 5.3687, (c) 0.64557, (d) 0.02847
E1.75 Repeat E1.74 for two significant (digit) figures.
E1.76 Round the following numbers to four significant digits:
(a) 54.3210, (b) 765.45, (c) 0.54325, (d) 0.00543,
(e) 0.5432 ˆ 102, (f) 5.432 ˆ 101, (g) 0.54 ˆ 10´3
E1.77 Approximate the following numbers to six decimal places by applying rounding and chop￾ping rules.
(a) Q1 “ 1.1234469, (b) Q2 “ 1.1234569, (c) Q3 “ 1.1234669,
(d) Q4 “ 2.1234569, (e) Q5 “ 4.1234369
E1.78 Round the following numbers to four significant figures:
(a) 12.345678, (b) 4.120381, (c) 0.0012345, (d) 43.462180,
(e) 0.033678, (f) 0.2489741
E1.79 Round the numbers in E1.78 to four decimal (place) figures:56  Numerical Methods for Scientists and Engineers
E1.80 Determine the absolute and relative errors when A “ 12.4527 is (a) chopped off and (b)
rounded off to three significant places.
E1.81 Determine the absolute and relative errors when A “ 12.4527 is (a) chopped off and (b)
rounded off to three decimal places.
E1.82 Determine the absolute and relative errors when the floating-point representations of A “
2{3 and B “ 4{9 are (a) chopped off and (b) rounded off to six significant places.
E1.83 Given x “ 5.4321, y “ 0.5432, and z “ 0.5433, perform the following calculations: (i)
exactly, and (ii) using four-digit decimal arithmetic with chopping.
(a) x
z ´ x
y , (b) x ˆ
ˆ1
z ´ 1
y
˙
E1.84 For x “ 2{3, y “ 3{7, and z “ 5{9, perform the following calculations: (i) exactly; (ii)
using three-digit decimal arithmetic with chopping; (iii) using three-digit decimal arithmetic with
rounding.
(a) f lpxq ` f lpy ` zq, (b) f lpx ` yq ` f lpzq, (c) f lpx ` yq, (d) f lpx ˆ yq,
(e) f lpx{yq, (f) f lpy{zq, (g) f lpz{xq, (h) f lpz{yq
E1.85 For given x “ 1.56897463 and y “ 1.56886938, perform the x´y operation (i) exactly and
(ii) using three-, four-, five-, and six-digit decimal arithmetic with chopping.
E1.86 Repeat E1.85 by using decimal arithmetic with rounding.
E1.87 Consider p “ 2.342356 and q “ 5.134236, which are approximated as ˜p “ 2.342 and
q˜ “ 5.134. Calculate the absolute and relative error associated with the p`q and p´q operations.
E1.88 Consider p “ 3.1415927 and q “ 3.1416932, which are approximated as ˜p “ 3.141 and
q˜ “ 3.142. Calculate the absolute and relative error associated with the p`q and p´q operations.
E1.89 Perform the following sum (i) exactly and (ii) using four-digit decimal arithmetic with
rounding. Find the true error.
f l ˆ1
7
˙
` f l ˆ1
9
˙
` f l ˆ 1
11˙
` f l ˆ 1
13˙
` f l ˆ 1
15˙
E1.90 Evaluate the following sum by applying four-significant-digit rounding. What is the true
error?
f l ˆ 1
?104˙
` f l ˆ 1
?10.4
˙
` f l ´?
10.4
¯
` f l ´?
104¯
E1.91 Evaluate x3 ´ 5.3x2 ` 6.8x ` 2.71 for x “ 1.42 using three-digit decimal arithmetic with
chopping and rounding.
E1.92 Repeat E1.91 using its nested form, i.e., pxpx ´ 5.3q ` 6.8qx ` 2.71.
E1.93 Evaluate 1.328 e3x ´ 2.051 e2x ` 3ex ´ 1.7812 for x “ 0.04 using two-digit arithmetic with
rounding. Find the absolute and relative errors in percent.
E1.94 Repeat E1.93 by using the nested form of the polynomial.
E1.95 Perform the following calculations using four-significant-places.
(a) 0.8972 ˆ 10´8 ´ 0.4683 ˆ 10´8, (b) `
0.8972 ˆ 10´30˘
ˆ `
0.4683 ˆ 10´25˘
(c) `
0.8972 ˆ 1025˘
ˆ `
0.4683 ˆ 1030˘
, (d) `
0.8972 ˆ 103˘
{
`
0.4683 ˆ 103˘
E1.96 Find the roots of x2 ´ 1010 x ` 2.5 “ 0 using floating-point arithmetic with four-digit
chopping.
E1.97 Consider the mathematical equivalent expressions given below for the calculation of p
?
?
3´
2q
6. Which is a better estimate if ?6 is approximated by 2.4?
(a) p5 ´ 2
?
6q
3
, (b) 1{p5 ` 2
?
6q
3
, (c) p485 ´ 198?
6q, (d) 1{p485 ` 198?
6qNumerical Algorithms and Errors  57
E1.98 Evaluate ?16.42 ´ ?16.40 by applying three-, four-, and five-significant-digit chopping,
and also find the true errors.
E1.99 Repeat E1.98 after recasting the expression as 0.02{p?16.42 ` ?16.40q.
E1.100 Calculate the following expressions for x “ 1, 10´1, 10´2,¨¨¨ , 10´7. Do you observe any
problems? If yes, identify the problem and devise an alternative form to avoid it.
(a) sec x ´ 1
tan2 x , (b)
?x2 ` 4 ´ 2
x2 , (c) px ´ 1q
3 ` 1
x
E1.101 Compute x3{2p
ax ` 1{x ´ ax ´ 1{xq for x “ 10, 102 ¨¨¨ , 108. Do you observe any
problems? If yes, identify the problem and devise an alternative form to avoid it.
E1.102 Compute p1 ´ cos 2xq{x2 for 10´1, 10´2, ¨¨¨ , 10´9. Do you observe any problems? If yes,
identify the problem and devise an alternative form to avoid it.
E1.103 Consider computing ln x ´ ln y for x – y which leads to a loss of significance. Derive an
alternative form of computation to avoid this problem.
E1.104 Consider computing lnp
?x2 ´ 1´xq for large x. (a) Show whether ´ lnp
?x2 ´ 1`xq can
be an alternative form or not; (b) which expression is more suitable for numerical computations?
Explain why?
E1.105 Calculate ?16.07 ´ 4 by chopping and rounding, using three-digit arithmetic.
E1.106 Calculate the arithmetic operation given in E1.105 alternatively as 0.07{p?16.07 ` 4q.
Section 1.5 Application of Taylor Series
E1.107 Use Taylor polynomials of degree n “ 3, 4, ..., 7 to find approximate values of ?e, ?3 e,
and ?4 e. How many terms of the series are needed to accurately estimate the approximate values
to within 10´3.
E1.108 Using the Maclaurin series, find a fourth-degree approximation and corresponding re￾mainder for fpxq “ 1{px ` 5q.
E1.109 Consider estimating π{4 with (a) tan´1p1q and (b) 4tan´1p1{5q´tan´1p1{239q (the Mach￾nin’s formula) using the Taylor polynomial given below.
tan´1
x – SN “ x ´ x3
3 ` x5
5 ´ ... ` p´1q
N x2N`1
p2N ` 1q “ ÿN
k“0
p´1q
k x2k`1
p2k ` 1q!
Use 10-terms (S10) to calculate inverse tangents and determine the decimal-place accuracy of the
estimates obtained from both approximations. Note: Use 10-decimal place arithmetic in calcula￾tions.
E1.110 Use the Maclaurin series to show that the following two-term approximation is fourth￾order, i.e., Opx4q: a1 ` x2 “ 1 ` 1
2 x2
E1.111 Consider the Maclaurin series of fpxq“p1 ` xq
´1{3 on p´1, 1s given below: Use the three￾term approximation to estimate 1.1´1{3. What can you say about the decimal-place accuracy of
the estimated result?
p1 ` xq
´1{3 “ 1 ´ 1
3
x `
1 ¨ 3
3 ¨ 6
x2 ´ 1 ¨ 3 ¨ 5
3 ¨ 6 ¨ 9
x3 `¨¨¨
E1.112 How many terms of the series must be included to accurately predict e2.75 to five decimal
places using the series expansion of ex? Suppose the true value of e2 is given. How can you calculate
e2.75 with the same accuracy by reducing the number of terms (as well as cpu-time) in the series?
Can you suggest a measure to further reduce the cpu-time?
E1.113 Use the following series expansion to determine a four-term approximation (S4) and its58  Numerical Methods for Scientists and Engineers
upper error bound (R4) to fpxq“p2x ` 9q
´1{2 given on r´1{2, 1{2s.
1
?x ` 1 “ 1 ´ 1
2
x `
1 ¨ 3
2 ¨ 4
x2 `
1 ¨ 3 ¨ 5
2 ¨ 4 ¨ 6
x3 ´¨¨¨
E1.114 Use power series to find 2nd and 3rd-degree polynomial approximations to p3 ` xq
´1{2
for any x P p1{5, 9{5q. Hint: Determine the most suitable point “a” for the Taylor polynomials.
E1.115 Use the power series of p1 ` xq
1{3 on p´1, 1s to obtain a third-degree polynomial approx￾imation to estimate p1.92q
1{3. Is the true error less than the upper error bound? Use at least
seven-decimal-place accuracy in arithmetic operations.
p1 ` xq
1{3 “ 1 ` x
3 ´ 2
3 ¨ 6
x2 `
2 ¨ 5
3 ¨ 6 ¨ 9
x3 ´ 2 ¨ 5 ¨ 8
3 ¨ 6 ¨ 9 ¨ 12x4 `¨¨¨
E1.116 Find a 5th-degree Maclaurin polynomial for the following functions:
(a) e
sin x, (b) cospsin xq, (c) e
x cos x
E1.117 Expand fpxq “ sin x cos x to the Maclaurin series. Then, use the series to estimate
sin 36˝ cos 36˝ accurate to within 10´3?
E1.118 Find the range of x for which the truncation error of approximating ex with the 9th-degree
Maclaurin polynomial is bounded by 10´8.
E1.119 Construct a 3rd-degree Maclaurin polynomial for sin 3x and determine the largest possible
truncation error in r0, π{2s?
E1.120 Find a 3rd-degree polynomial approximation that avoids the “loss of significance” in the
following expressions:
(a) e2x ´ 1
x , (b) 1 ´ e´x{2
x , (c) px ` 4q
3{2 ´ 3x ´ 8
x2 , (d) 2x ´ sin 2x
x2
E1.121 Use the Maclaurin series of lnpx`1q to construct a polynomial approximation that yields
estimates accurate to within 10´3 in [0, 1/2].
E1.122 Use the Maclaurin series of px`1q
´2 to construct a polynomial approximation that yields
estimates accurate to within 10´3 in [0, 1/2].
E1.123 Use the Maclaurin series of tan´1x to determine a four-term approximation (S4) and its
upper error bound (R4) for any x in [´1{2, 1{2].
E1.124 For fpxq “ sin x, construct Taylor polynomial approximations (of degree N “ 2,3,4, and
5) about x “ π{4 and find the associated upper error bounds. Use the approximations to estimate
sin π{3 and calculate the maximum possible errors.
E1.125 Use the Maclaurin series to develop Nth-degree polynomial approximation and corre￾sponding truncation error for the following definite integrals:
(a) ż x
0
e
´u2
du, (b) ż x
0
sin t
2dt
t , (c) ż x
0
lnp1 ` w4
qdw
E1.126 Determine all ξ points that satisfy the mean-value theorem for fpxq “ 8x3 `9x2 ´6x`5
on p´2, 1q.
E1.127 Show that fpxq “ x5 ´ x4 ` x2 ´ 1 has only two points on p´1, 1q that satisfy the mean
value theorem.
E1.128 Determine whether the mean value theorem can be applied to fpxq “ 3`?x ` 1 on [0,3].
If so, find all possible values of ξ.
E1.129 Consider fpxq “ 2
?x on (1,9). Determine a point ξ whose existence is assured by the
mean value theorem.
E1.130 For fpxq “ x4 ´2x3 ´36x2 `3, determine a point ξ on (0,3) that satisfies the mean value
theorem for f1
pxq.Numerical Algorithms and Errors  59
E1.131 Consider fpxq that is bounded on p0, hq. If fphq “ fp0q ` 4h2, estimate the derivative of
fpxq at the midpoint of the interval, i.e., f1
ph{2q.
E1.132 Find the average value of fpxq“4´ x2 on r0, 2s.
E1.133 Find the average value of fpxq“x´ x3 on [0,1].
E1.134 Find the average value of fpxq“ 8{x3 on [1,2].
E1.135 The axial temperature distribution of a 30-cm-long circular bar is described by Tpxq “
10 ` 20 coshp2 ´ 3xq, where x is the axial direction (in m) and T is the temperature (in ˝C).
Estimate the average temperature of the bar.
E1.136 Consider the flow of a fluid between two large horizontal
parallel plates, where the bottom plate is stationary and the
top plate is moving at speed U. The fluid velocity distribution,
depicted in Fig. E1.136, is given by vpyq “ UY p1 ` Φ ´ ΦY q,
where Y “ y{b, Φ is a constant, and b is the distance between
parallel plates. Find an expression for the average fluid velocity. Fig. E1.136
E1.137 Find the average value of fpxq in the specified intervals.
(a) On r´3, 3s, fpxq “
$
&
%
1, x ă ´2
3 ´2 ď x ă 1
2 x ě 1
(b) On r´2, 3s, fpxq “
$
&
%
´3x, x ă 0
x2 0 ď x ă 1
1 x ě 1
E1.138 For the given integral, find the average value for gpxq on ra, bs.
ż b
a
fpxqgpxqdx “ 3pa ´ bq
3
where fpxq “ x ´ a and gpxq is bounded and integrable on ra, bs.
E1.139 Consider fpxq that is bounded and integrable on r0, as. Find a suitable K for some ξ that
satisfies the integral in this interval.
ż a
0
xpa ´ xqfpxqdx “ K ¨ fpξq
E1.140 Find ξ-values that satisfy the second mean-value theorem for
ż 1
0
fpxqgpxqdx
where fpxq“x2`1 and gpxq“e´2x. Hint: Swap fpxq and gpxq to find a suitable ξ for both cases.
E1.141 Consider fpxq which is bounded and integrable on r´h{2, h{2s. What would be a suitable
K for the integral to be estimated as follows?
ż h{2
´h{2
x2
fpxqdx – K ¨ fp0q
Section 1.6 Truncation Error of Series
E1.142 Find the true values of the following infinite sums: Hint: First obtain an expression for
the partial sums, SN , and then take the limit for N Ñ 8.
(a) ÿ8
k“1
1
4k2 ´ 1
, (b) ÿ8
k“1
1
4k2 ´ 9
, (c) ÿ8
k“1
2k ` 1
k2pk ` 1q
2 , (d) ÿ8
k“1
kp4k2 ` 1q
16p4k2 ´ 1q
460  Numerical Methods for Scientists and Engineers
E1.143 For the series given in E1.142, calculate five-term approximations and error bounds. How
many decimal places of accuracy are achieved in each series?
E1.144 Estimate the six-term partial sums, true errors, and error bounds of the following series.
Compare the results with the true solution. Is the true error within the error bounds?
(a) ř
8
n“1
1
n2pn2 ` 1q “ 1
6
p3 ` π2 ´ 3π coth πq, (b) ř
8
n“1
3n
n ¨ 4n “ ln 4
(c) ř
8
n“1
2n
n4 ` 1 “ 1.388346044, (d) ř
8
n“1
4n
pn!q
2 “ I0p4q ´ 1
where I0pxq is the modified Bessel function of the first kind.
E1.145 Estimate the nine-term partial sums, true errors, and the error bounds of the following
alternating series. Compare the results with the true solution.
(a) ÿ8
n“1
p´1q
n´1
n2 ` 1 “ 1
2 p1 ´ π csch πq , (b) ÿ8
n“1
p´π2q
n`1
p2nq! “ 2π2
(c) ÿ8
n“1
p´1q
n´1 n2
n4 ` 4 “ 1
4
π csch π, (d) ÿ8
n“1
p´1q
n
npn ` 1q “ 1 ´ 2 ln 2
E1.146 How many decimal places of accuracy can be expected in the worst case if the following
series are replaced by 10-term approximations?
(a) ÿ8
n“1
p´1q
n
n2pn2 ` 1q
, (b) ÿ8
n“1
p´1q
n
npn ` 1qpn ` 2q
, (c) ÿ8
n“1
p´1q
n
n3pn ` 1q
3
Section 1.7 Rate and Order of Convergence
E1.147 Find the limits and convergence rates of the following sequences:
(a) xn “ 4n ` 1
5n ` 12, (b) xn “ n ` 3
n3 ` 2
, (c) xn “ 1
n2p1 ` 21{nq
, (d) xn “ ln ˜
3n ` 1
3n ` 5
¸
E1.148 For three cases, the absolute errors of successive iterates xn (e.g., |xn ´ xn´1| for n “
1, 2,..., 7) are given in the table below. Estimate the order of convergence for each case.
n Case 1 (xn) Case 2 (yn) Case 3 (zn)
1 120.9
2 0.64 1.5 0.195
3 0.262144 0.68695298 0.04200
4 0.044000 8.65ˆ10´2 0.0091500
5 1.24ˆ10´3 1.90ˆ10´4 0.0020000
6 9.85ˆ10´7 2.03ˆ10´12 0.0004350
7 6.2ˆ10´13 2.49ˆ10´36 0.0000946
E1.149 The absolute errors of successive iterates xpnq (i.e., |xn ´ xn´1| for n “ 1, 2,..., 7) are
given below. Estimate the order of convergence.
n 12 3 4 5 6 7
xn 2 1.781 1.477 1.091 0.668 0.302 0.0836Numerical Algorithms and Errors  61
1.10 COMPUTER ASSIGNMENTS
CA1.1 Consider the conversion of an integer number (input) to an equivalent binary number
(output string). Write a pseudomodule, Module Int2Binary (integer_number, binary_number), and
test your code with a computer program in the language of your choice.
CA1.2 Consider the conversion of a decimal number (input in the range 0 ă x ă 1) to an
equivalent binary number (23-bit string output). Write a pseudomodule, Module Decimal2Binary
(x, binary_number), and test your code with a computer program in the language of your choice.
CA1.3 Write a computer program that uses n-digit floating-point arithmetic to perform the
řn
i“1 Δx operation. Perform 10 million additions with Δx “ 2´16 and 2ˆ10´5 using both rounding
and chopping to six-, seven-, and eight-digit. Compare and interpret your findings.
CA1.4 Write a pseudocode to calculate SN “ řN
k“1 ak and convert the code to a computer
program in the language of your choice. To estimate the series sums whose true values are given
below, (a) run your program for N “ 10, 20, 50, and 100; (b) calculate the true (absolute) error
for the estimates; and (c) interpret your findings.
(a) ÿ8
n“1
1
n2pn2 ` 1q
“ 1
6
p3 ` π2 ´ 3π coth πq (b) ÿ8
n“1
3n
n ¨ 4n “ln 4, (c) ÿ8
n“1
6n
n!
“e
6 ´ 1
CA1.5 Write a computer program that finds the partial sum of the p´series, SN “ řN
k“1p1{npq,
using five- and ten-digit floating-point arithmetic with both rounding and chopping. In doing so,
your program should be able to perform the additions (i) back-to-front (k “ N, pN ´ 1q, ¨¨¨ , 2, 1,
adding numbers from smallest to largest) and (ii) front-to-back (k “ 1, 2, ¨, pN ´ 1q, N, adding
numbers from largest to smallest). Then investigate the effect (if any) of the direction on the
addition process. Discuss your findings for p “ 1{2, 1 and 2 with N “ 10, 50, and 100 terms.
CA1.6 Write a program to compute π{4 using the expressions in (i), (ii), and the Machin’s
formulas given by (iii) and (iv). (a) Find π{4 using the Maclaurin series of tan´1x with N “ 10,
50, and 100 terms. How does accuracy improve? (b) Write another program to compute tan´1x
with an ε as a stopping criterion. (d) Using all three methods, how many terms of the series are
necessary to obtain π{4 with 10 decimal digits? Report and discuss your findings.
(i) π
4 “ tan´1p1q, (ii) π
4 “ 4tan´1
˜
1
5
¸
´ tan´1
˜ 1
239¸
,
(iii) π
4 “ 3
2
tan´1
˜ 1
?3
¸
(iv) π
4 “ 6tan´1
˜
1
8
¸
` 2tan´1
˜ 1
57¸
` tan´1
˜ 1
239¸
CA1.7 Devise a general program to evaluate ex for all x accurate within machine epsilon using
the Maclaurin series. (a) Use your program to obtain the approximations as well as the true
relative errors for negative (x ă 0) and positive (x ą 0) real values. (b) How does the relative
error change? (c) Does a program with double-precision arithmetic eliminate the problem you
observed? If not? What is the reason for this? (d) How can you overcome this problem?
CA1.8 The Taylor series expansion of the inverse sine function is defined as
sin´1
x “ x `
1
2
x3
3 `
1 ¨ 3
2 ¨ 4
x5
5 `
1 ¨ 3 ¨ 5
2 ¨ 4 ¨ 6
x7
7 `¨¨¨`
1 ¨ 3 ¨¨¨p2n ´ 1q
2 ¨ 4 ¨¨¨p2nq
x2n`1
2n ` 1 `¨¨¨
Write a pseudo-Function Module ARCSIN(x) that calculates the inverse sine of any x in r´1, 1s
using the series expansion accurately within ε. Note: Make sure that the code performs a minimum
number of arithmetic operations. Also, you should include warning messages, errors, and validity
control statements.62  Numerical Methods for Scientists and Engineers
CA1.9 The Bessel function of the first kind is given as follows:
Jnpxq “ ÿ8
k “0
p´1q
k px{2q
n`2k
k!pn ` kq! “ px{2q
n
n!
"
1 ´ px{2q
2
pn ` 1q ` px{2q
4
pn ` 1qpn ` 2q ´¨¨¨*
Write a pseudocode, Module BESSELJ(n, x, ε, k, BesselJn), that calculates the Bessel function
(BesselJn, real output) for any n (integer order) and x real inputs, accurate to within ε (input)
tolerance. The program should also return the number of terms added (k) as output. Apply cpu￾time minimization strategy. Convert your code to a program in the language of your choice and test
the program for various n and x values. Finally, use the program to obtain the numerical values
of J0pxq, J1pxq, and J2pxq on r0, 2s with 0.2 increments and comparatively plot these functions.
Also, you should include warning messages, errors, and validity control statements.
CA1.10 For real a and b and integer n, k, and m input variables, a computer program is to be
prepared to evaluate the following definite integral:
dm “
żb
0
xnpa ` bxkq
m
dx
The integral dm (output) can be computed using the following recursion formula:
dm “ bn`1`
a ` bk`1˘m
1 ` mk ` n `
amk
mk ` n ` 1
dm´1 and d0 “ bn`1
n ` 1
(a) Write a pseudocode, Module DITEG(a, b, m, n, k, d), that calculates the integrals up to m and
returns the results in d0, d1,...,dm. Make your code as efficient as possible by keeping the number
of arithmetic operations (power, multiplication, and division) to a minimum; and (b) convert your
code to a computer program in the language of your choice and test the program for various input
values.
CA1.11 The following expression is known as the hyperbolic sine integral function.
Shipxq “ żx
0
sinh t
t
dt
(a) Obtain a polynomial approximation for Shipxq by making use of the Maclaurin series; (b)
Use the approximation derived in Part (a) to write a pseudocode, Function Module SHI(x), that
calculates an approximate value of Shi(x) for an input x in r0, 1s within 5ˆ10´7 tolerance; and (c)
convert your code to a computer program in the language of your choice and test the program for
various input values. Make your code as efficient as possible by keeping the number of arithmetic
operations (power, multiplication, and division) to a minimum.
CA1.12 Consider the following recursive sequences for which convergence rates are to be deter￾mined:
(a) xn`1 “ 2 `
3
xn
, x0 “ 1, (b) xn`1 “ ?2xn ` 15, x0 “ 1, (c) xn`1 “ e
´xn{5
, x0 “ 1
(a) Write a computer program (or use a spreadsheet) to evaluate the terms of a sequence up to 11
and then tabulate |xn ´ xn´1|, logp|xn`1 ´ xn|{|xn ´ xn´1|q and r; (b) Determine the limit value
and the order of convergence of a sequence.
CA1.13 In the field of heat transfer, the Nusselt number is a dimensionless number that gives
the ratio of convective to conductive heat transfer at a wall in a fluid. The Nusselt number for
slug flow in annular ducts subjected to constant heat flux may be predicted from the following
correlation [33]:
Nupmq “ 8pm ´ 1qpm2 ´ 1q
2
m4p4 ln m ´ 3q ` 4m2 ´ 1
,Numerical Algorithms and Errors  63
where m “ D2{D1 denotes the diameter ratio. Find simpler linear and quadratic approximations
valid in the range 1 ď m ď 3, and investigate the accuracy of these approximations by plot￾ting comparatively the true and approximate values in 1 ă m ď 3. Clue: Expand the Nusselt
relationship to the Taylor’s series about m “ 1.CHAPTER 2
Linear Systems:
Fundamentals and Direct
Methods
LEARNING OBJECTIVES
After completing this chapter, you should be able to
‚ refresh the basic linear algebra concepts and fundamental matrix operations;
‚ describe the general structure of a system of linear equations;
‚ realize the difference between direct and iterative methods;
‚ perform elementary row operations and matrix transformations;
‚ perform matrix inversion using the Gauss-Jordan algorithm;
‚ carry out forward- and back-substitution algorithms to solve upper- and lower￾triangular systems of equations;
‚ apply Gauss elimination to solve linear systems with and without pivoting;
‚ understand and explain ill-condition and its effects on matrix algebra;
‚ apply Doolittle and Cholesky’s decomposition techniques to solve linear sys￾tems;
‚ understand and implement the Gauss elimination and LU-decomposition meth￾ods to solve tridiagonal systems of equations;
‚ implement Gauss elimination method to solve banded linear systems;
‚ explain and implement LU-decomposition for banded linear systems.
MOST problems in science and engineering are reduced to problems in the field of
linear algebra. A few of these applications are structural analysis, circuit analysis,
optimization, least squares fitting, data analysis, network flow, eigenvalue and eigenvector
problems, the solution of systems of simultaneous linear equations, the numerical solution
of differential equations using finite difference or finite volume methods, etc.
Solving a linear system, inverting a matrix, and determining the eigenvalues or eigen￾vectors of a matrix are the most common math operations in applied sciences. The solution
of a system of linear equations can be obtained by either direct or iterative methods. In the
absence of round-offs or other errors, direct methods give the exact solution of the linear
64 DOI: 10.1201/9781003474944-2Linear Systems: Fundamentals and Direct Methods  65
system of equations. On the other hand, iterative methods discussed in Chapter 3 give an
approximate solution of a linear system within a prescribed tolerance.
Linear algebra basics, definitions, notations, etc. are revisited to refresh the readers’
memory. Comprehensive pseudocodes are provided for most of the methods presented to
reinforce understanding and writing computer codes on this topic. Eigenvalues and eigen￾vectors are briefly discussed here, but this topic is covered in greater detail in Chapter
11.
2.1 FUNDAMENTALS OF LINEAR ALGEBRA
Systems of linear equations are conveniently expressed in matrix notation, and methods of
solution for such systems can be developed very compactly using matrix algebra. Conse￾quently, in this text, the elementary properties of matrices, vectors, and determinants are
presented in this section, and shorthand matrix and vector notations are extensively used
throughout the text.
2.1.1 MATRICES
A matrix is a rectangular arrangement of numbers in tabular form with m rows and n
columns. The number of rows and columns determines the size or dimension of the matrix.
An m ˆ n rectangular matrix is expressed as
A “ raij smˆn “
»
—
—
—
—
—
–
a11 a12 a13 ¨¨¨ a1n
a21 a22 a23 ¨¨¨ a2n
a31 a32 a33 ¨¨¨ a3n
.
.
. .
.
. .
.
. ... .
.
.
am1 am2 am3 ¨¨¨ amn
fi
ffi
ffi
ffi
ffi
ffi
fl
mˆn
(2.1)
where aij denotes the element of the ith row and the jth column. A comma is used to
separate two indices when necessary to avoid confusion, e.g., an,n`1 instead of ann`1. En￾closing the general element within square brackets as “raij smˆn” is an abbreviation for a
general matrix by indicating its size. Each number in a matrix, aij , is referred to as a matrix
element or matrix entry.
A matrix consisting of a single row b “ rbis1ˆn is referred to as a row matrix, while
a matrix consisting of a single column x “ rxisnˆ1 is called a column matrix or a column
vector. Following are examples of such matrices:
b “ rbis1ˆn “ r b1 b2 ¨¨¨ bn s, x “ rxisnˆ1 “
»
—
—
—
–
x1
x2
.
.
.
xn
fi
ffi
ffi
ffi
fl
Elements of a row vector that may involve long or complicated expressions could be sep￾arated from each other with commas to avoid confusion, i.e., b “ r a ` 3, a ´ b, b ´
2, ¨¨¨ , 2a ` 3b s.
A matrix is denoted with an uppercase boldface letter (A, B, C)
and a vector (row or column matrices) by a lowercase boldface
letter (a, b, c) throughout this text.66  Numerical Methods for Scientists and Engineers
A square matrix, most often encountered when solving systems of linear equations, has
the same number of rows and columns: n ˆ n. The main (or principal) diagonal elements
of a square matrix A are a11, a22, a33, ¨¨¨ , ann.
The trace of a square matrix, A “ raij snˆn, is defined as the sum of its main diagonal
elements,i.e.,
trpAq “ a11 ` a22 ` a33 `¨¨¨` ann “ ÿn
k“1
akk
2.1.2 SPECIAL MATRICES
A matrix whose elements are zero is called a zero matrix. For example,
O “ r0s2ˆ2 “
„
0 0
0 0j
, O “ r0s2ˆ4 “
„
0000
0000j
, O “ r0s3ˆ1 “
»
–
0
0
0
fi
fl
where O denotes any conformable zero matrix. In cases where necessary, the notation O “
r0smˆn may be used to refer to a zero matrix by its size.
Square matrices have additional special properties, unlike rectangular matrices. If A
is an n ˆ n square matrix and satisfies the aij “ aji condition for all i, j, then the matrix
is said to be symmetric. For instance, the following matrices are symmetrical about the
principal diagonal:
„ 1 ´4
´4 5 j
,
»
–
a ´x u
´xbe
u ef
fi
fl ,
»
—
—
–
3 21 ´3
2 42 1
1 27 3
´313 5
fi
ffi
ffi
fl
A diagonal matrix, usually denoted by D, is a square matrix whose off-diagonal elements
are all zero (dij “ 0 for all i ‰ j), while identity matrix or unit matrix, I, is a special case
of a diagonal matrix whose diagonal elements are “1.” For example,
D “
»
—
—
—
—
—
–
d11 0 0 ¨¨¨ 0
0 d22 0 ¨¨¨ 0
0 0 d33 ¨¨¨ 0
.
.
. .
.
. .
.
. ... .
.
.
000 ¨¨¨ dnn
fi
ffi
ffi
ffi
ffi
ffi
fl
, In “
»
—
—
—
—
—
–
100 ¨¨¨ 0
010 ¨¨¨ 0
001 ¨¨¨ 0
.
.
. .
.
. .
.
. ... .
.
.
000 ¨¨¨ 1
fi
ffi
ffi
ffi
ffi
ffi
fl
(2.2)
where the notation In is often used to denote the identity matrix with its size (n ˆ n). The
subscript n is generally omitted in cases where its size is obvious from the context.
A square matrix with zero elements below the main diagonal (uij “ 0, i ą j) is called an
upper triangular matrix, and the one with zero elements above the diagonal (�ij “ 0, i ă j)
is called a lower triangular matrix.
U “
»
—
—
—
—
—
–
u11 u12 u13 ¨¨¨ u1n
0 u22 u23 ¨¨¨ u2n
0 0 u33 ¨¨¨ u3n
.
.
. .
.
. .
.
. ... .
.
.
000 ¨¨¨ unn
fi
ffi
ffi
ffi
ffi
ffi
fl
, L “
»
—
—
—
—
—
–
�11 0 0 ¨¨¨ 0
�21 �22 0 ¨¨¨ 0
�31 �32 �33 ¨¨¨ 0
.
.
. .
.
. .
.
. ... .
.
.
�n1 �n2 �n3 ¨¨¨ �nn
fi
ffi
ffi
ffi
ffi
ffi
fl
(2.3)
where U and L are generally used to denote Upper and Lower triangular matrices.Linear Systems: Fundamentals and Direct Methods  67
A band matrix or banded matrix, B, is a square matrix that consists of mostly nonzero
elements in a rectangular band about the main diagonal. For example, the following is a
7 ˆ 7 band matrix:
B “
»
—
—
—
—
—
—
—
—
–
b11 b12 b13 b14
b21 b22 b23 b24 b25
b31 b32 b33 b34 b35 b3,6
b42 b43 b44 b45 b46 b47
b53 b54 b55 b56 b57
b64 b65 b66 b67
b75 b76 b77
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(2.4)
where mL and mU are the lower and upper bandwidths, which for this example are mL “ 2
and mU “ 3. Note that bij “ 0 if j ´ i ą mU or i ´ j ą mL, and bij ‰ 0 for i ´ mL ď j ď
i ` mU . The total bandwidth of a banded matrix is nb “ mL ` mU ` 1.
A tridiagonal matrix, T, is a square matrix and has nonzero elements only on the
main diagonal (di, i “ 1, 2,...,n), the first diagonal below (bi, i “ 2, 3,...,n), and the first
diagonal above it (ai, i “ 1, 2,...,n ´ 1). Although tij “ 0 for all i ´ j ą 1 and j ´ i ą 1, a
few elements of bi or ai may be zero. Note that it is also a special case of a banded matrix
with mL “ mU “ 1 and nb “ 3.
T “
»
—
—
—
—
—
—
—
–
d1 a1
b2 d2 a2
b3 d3 a3
... ... ...
bn´1 dn´1 an´1
bn dn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
In most cases, it is desirable to work with matrices that are in row-echelon or reduced row￾echelon forms. These matrices could be square or rectangular in shape. A matrix in row
echelon form illustrated below has a pivot at its non-zero rows; in other words, all the entries
to the left and below the pivot (1’s here) are equal to zero.
»
—
—
—
—
—
–
1 ˚ ˚ ¨¨¨ ˚
0 1 ˚ ¨¨¨ ˚
001 ¨¨¨ ˚
.
.
. .
.
. .
.
. ... .
.
.
000 ¨¨¨ 1
˚
˚
˚
.
.
.
˚
˚
˚
˚
.
.
.
˚
¨¨¨
fi
ffi
ffi
ffi
ffi
ffi
fl
nˆm
where * denotes a nonzero element. When the coefficient matrix of a linear system is con￾verted into a row echelon form, it is easier to obtain the solution of the system using the
so-called back substitution algorithm.
The following matrix is said to be in reduced row echelon form:
»
—
—
—
—
—
–
100 ¨¨¨ 0
010 ¨¨¨ 0
001 ¨¨¨ 0
.
.
. .
.
. .
.
. ... .
.
.
000 ¨¨¨ 1
˚
˚
˚
.
.
.
˚
˚
˚
˚
.
.
.
˚
¨¨¨
fi
ffi
ffi
ffi
ffi
ffi
fl
nˆm
Notice that the basic columns are vectors of the standard basis, i.e., vectors having all zero
elements except for one equal to 1.68  Numerical Methods for Scientists and Engineers
The rank of a matrix, denoted by rank(A), is the maximum number of linearly inde￾pendent (unique) rows (or columns), and the number of rows (or columns) that are not
obtained by a linear combination of other rows (or columns) is termed the row rank (or
column rank). Consider the following matrices:
A “
»
–
3 ´2 2
111
´2 3 ´1
fi
fl , B “
„ 2 4 ´8
´1 ´2 4 j
, C “
»
—
—
–
1 ´1
1 3
´3 3
´2 6
fi
ffi
ffi
fl
Note that rank(A)=2 since row-2 is obtained as row-1+row-3, i.e., row-1 and row-3 are
the only rows that are linearly independent. On the other hand, rank(B)=1 because row￾1“ p´2q ˆ row-2, i.e., there is only one linearly independent row. However, rank(C)=2
because row-3“ p´3q ˆrow-1 and row-4“ p´2q ˆrow-2, i.e., only row-1 and row-2 are the
linearly independent rows.
2.1.3 OPERATIONS WITH MATRICES
2.1.3.1 Transpose of a matrix
A new matrix obtained by interchanging the rows and columns of the original matrix A “
raij smˆn is called the transpose (matrix) of A, and it is expressed as
AT “
»
—
—
—
—
—
–
a11 a21 a31 ¨¨¨ an1
a12 a22 a32 ¨¨¨ an2
a13 a23 a33 ¨¨¨ an3
.
.
. .
.
. .
.
. ... .
.
.
a1,m a2m a3m ¨¨¨ anm
fi
ffi
ffi
ffi
ffi
ffi
fl
nˆm
where the superscript T denotes the transpose operation.
Let A and B be matrices of the same (conformable) size and α be a non-zero
scalar, then
‚ Symmetry property: AT “ A if A is a square symmetric matrix;
‚ Reflexive property: pAT q
T “ A;
‚ Addition property: pA ` Bq
T “ AT ` BT “ BT ` AT ;
‚ Exchange of order in multiplication: pABq
T “ BT AT ;
‚ Scalar multiple property: pαAqT “ αAT .
Also, a column vector is frequently denoted with the transpose notation to save vertical
space in equations:
x “ rx1 x2 x3 ¨¨¨ xns
T
2.1.3.2 Matrix addition/subtraction
Addition, or subtraction, of the same-size matrices A “ raij smˆn and B “ rbij smˆn is
accomplished by adding or subtracting the corresponding elements of A and B:
A ˘ B “ raij ˘ bij smˆnLinear Systems: Fundamentals and Direct Methods  69
Pseudocode 2.1
Module MAT_ADD (m, n, A, B, C)
\ DESCRIPTION: A pseudomodule for matrix addition, A ` B “ C.
Declare: amn, bmn, cmn \ Declare matrices as array variables
For “
i “ 1, m‰ \ Loop i: Sweep from top row to bottom
For “
j “ 1, n‰ \ Loop j: Sweep row from left to right
cij Ð aij ` bij \ Set aij ` bij to cij
End For
End For
End Module MAT_ADD
Let A, B, C, and O (zero) be matrices of the same size, then
‚ Commutative property: A ` B “ B ` A;
‚ Associative property: A ` pB ` Cq“pA ` Bq ` C;
‚ Additive identity property: A ` O “ O ` A “ A;
‚ Additive inverse property: A ` p´Aq “ O;
‚ Matrix subtraction is non-commutative and non-associative, that is,
A ´ B ‰ B ´ A and A ´ pB ´ Cq‰pA ´ Bq ´ C.
A pseudomodule, MAT_ADD, for adding two matrices is presented in Pseudocode 2.1.
The module requires two matrices (A and B) and their size (m ˆ n) as input. The addition
operation is carried out by summing up the corresponding elements of A and B (cij Ð
aij ` bij ) for every row and every column (i “ 1, 2,...,m, j “ 1, 2,...,n) using two For
loops. The loop over j-variable sweeps all the elements in a row from left to right. The loop
over the i variable sweeps rows from top to bottom. The same module can be adopted for
matrix subtraction simply by replacing cij with cij Ð aij ´bij . Note that the inner or outer
loops are interchangeable in this module.
2.1.3.3 Scalar multiplication
In multiplying a matrix A “ raij smˆn by a λ scalar number, the scalar multiplication λA
is obtained by multiplying each element of matrix A by λ as follows:
λA “ λraij smˆn “ rλaij smˆn “
»
—
—
—
—
—
–
λa11 λa12 λa13 ¨¨¨ λa1n
λa21 λa22 λa23 ¨¨¨ λa2n
λa31 λa32 λa33 ¨¨¨ λa3n
.
.
. .
.
. .
.
. ... .
.
.
λam1 λam2 λam3 ¨¨¨ λamn
fi
ffi
ffi
ffi
ffi
ffi
fl
Let A, B, and O (zero) be matrices of the same size and α and β be non-zero
scalar numbers, then
‚ Associative property of multiplication: αpβAq “ βpαAq“pαβqA;
‚ Distributive properties: αpA ` Bq “ αA ` αB, pα ` βqA “ αA ` βA;
‚ Multiplicative identity property: 1 ¨ A “ A;
‚ Multiplicative properties of zero: 0 ¨ A “ 0 and α ¨ O “ 0.70  Numerical Methods for Scientists and Engineers
Pseudocode 2.2
Module SCALM (m, n, λ, A, B)
\ DESCRIPTION: A pseudomodule to perform matrix addition, A ` B “ C.
Declare: amn, bmn \ Declare matrices as array variables
For “
i “ 1, m‰ \ Loop i: Sweep from top row to bottom
For “
j “ 1, n‰ \ Lop j: Sweep row left to right
bij Ð λ aij \ Assign λaij product to bij
End For
End For
End Module SCALM
A pseudomodule, SCALM, performing a scalar multiplication is given in Pseudocode
2.2. As input, the module requires a matrix (A), its size (m ˆ n), and a real scalar number
(λ). The output is the resultant matrix B, whose elements, bij , are obtained by multiplying
every element of A with the real-type input scalar λ; that is, bij “ λaij for i “ 1, 2,...,m
and j “ 1, 2,...,n for every row and column.
2.1.3.4 Inner or dot product
The inner or dot product of two vectors of the same size, x “ rxisnˆ1 and y “ ryisnˆ1, is
denoted by (x, y) or xT y and is defined as
px, yq “ xT y “ x1y1 ` x2y2 `¨¨¨` xnyn “ ÿn
k“1
xkyk (2.5)
Let x, y, and z be real vectors and α and β be non-zero scalars, then
‚ Positive-definiteness property: x ¨ x ą 0 for x ‰ 0 and x ¨ x “ 0 if and
only x “ 0;
‚ Commutative property: x ¨ y “ y ¨ x;
‚ Scalar multiplication property: pαxq¨pβyq“pαyq¨pβxq “ αβpx ¨ yq;
‚ Distributive property: x ¨ py ` zq “ x ¨ y ` x ¨ z;
‚ x¨y “ 0 when either the vectors are orthogonal or either x “ 0 or y “ 0.
Pseudocode 2.3
Function Module XdotY (n, x, y)
\ DESCRIPTION: A pseudo-function to perform dot product, x ¨ y.
Declare: xn, yn \ Declare vectors as arrays of length n
sums Ð 0 \ Initialize accumulator, sum
For “
k “ 1, n‰ \ Loop k: accumulate xk ˚ yk’s
sums Ð sums ` xk ˚ yk \ Add xk ˚ yk to accumulator
End For
XdotYÐ sums \ Set accumulator to XdotY (dot product)
End Function Module XdotYLinear Systems: Fundamentals and Direct Methods  71
A pseudofunction module, XdotY, that calculates the inner product of two input vectors
(x and y) of the same size (n) is given in Pseudocode 2.3. The module requires a single
accumulator to perform the dot product: Eq. (2.5). The accumulator variable, sum, is first
initialized (sum Ð 0), and the products of the corresponding vector elements, xkyk, are
accumulated with a For-loop. The accumulated sum is then set to the real function name
XdotY on exit.
2.1.3.5 Matrix multiplication
The matrix multiplication, AB “ C, is defined only if the number of columns in A is equal
to the number of rows in B; that is, A “ raij smˆp and B “ rbij spˆn. These matrices are
said to be “conformable” to multiplication. The product matrix of a multiplication of two
matrices is represented by C “ rcij smˆn. An element of C, cij , is found by multiplying
corresponding elements of the ith row of A with the jth column of B and summing the p
products as
cij “ ÿp
k“1
aikbkj (2.6)
cij may also be regarded as the inner product of the ith row of A and the jth column of B.
Let A, B, and C be matrices of conformable size, then
‚ Non-commutative property: AB ‰ BA even if BA is conformable;
‚ Associative property: pABqC=ApBCq;
‚ Distributive property: ApB ` Cq=AB`AC or pB ` CqA=BA`CA;
‚ Multiplicative property of zero: A ¨ 0 “ 0 ¨ A “ 0;
‚ Multiplicative identity property: AI “ IA “ A.
A pseudomodule, MAT_MUL, performing multiplication of two conformable matrices,
C “ AB, is presented in Pseudocode 2.4. The module requires two matrices A and B of
conformable sizes as input. The output is the product (resultant) matrix, C “ rcij smˆn.
The inner product of the ith row of A (ai˚) and the jth column of B (b˚j ) is calculated
by Eq. (2.6) using an accumulator, i.e., the For-loop that runs over all k. The accumulated
result for all pi, jq’s is stored on the accumulator variable cij , which represents the element
of the resultant matrix at the ith row and the jth column.
2.1.3.6 Matrix-vector multiplication
Multiplication of a matrix A “ raij smˆn by a vector x “ rxisnˆ1 gives a vector of length
m. Multiplying a matrix by a vector may also be regarded as the “dot product” of x with
every row of A. The elements of the b “ Ax vector are shown explicitly as
»
—
—
—
—
—
–
a11 a12 a13 ¨¨¨ a1n
a21 a22 a23 ¨¨¨ a2n
a31 a32 a33 ¨¨¨ a3n
.
.
. .
.
. .
.
. ... .
.
.
am1 am2 am3 ¨¨¨ amn
fi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
–
x1
x2
x3
.
.
.
xn
fi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
–
a11x1 ` a12x2 `¨¨¨` a1nxn
a21x1 ` a22x2 `¨¨¨` a2nxn
a31x1 ` a32x2 `¨¨¨` a3nxn
.
.
.
am1x1 ` am2x2 `¨¨¨` amnxn
fi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
–
b1
b2
b3
.
.
.
bm
fi
ffi
ffi
ffi
ffi
ffi
fl
The components of the product can be formulated as follows:72  Numerical Methods for Scientists and Engineers
Pseudocode 2.4
Module MAT_MUL (m, p, n,A, B,C)
\ DESCRIPTION: A pseudomodule to perform matrix multiplication, AB “ C.
Declare: amp, bpn, cmn \ Declare matrices as array variables
For “
i “ 1, m‰ \ Loop i: Sweep from top row to bottom
For “
j “ 1, n‰ \ Loop j: Sweep row from left to right
cij Ð 0 \ Initialize accumulator, cij
For “
k “ 1, p‰ \ Loop k: Accumulator for aik ˚ bkj ’s
cij Ð cij ` aik ˚ bkj \ Add aik ˚ bkj to accumulator, cij
End For
End For
End For
End Module MAT_MUL
bi “ ÿn
k“1
aikxk, i “ 1, 2,...,m
In Pseudocode 2.5, a pseudomodule, Ax, performing Ax matrix-vector product is pre￾sented. The module requires a square matrix (A), a vector (x), and the size (n also denoting
n ˆ n) as input. The output, b, is a vector of length n. For all i’s, the accumulator variable
bi is initialized and is used as an accumulator (bi Ð bi ` aikxk) to compute ř
k aikxk using
the inner loop over the j index variable.
Pseudocode 2.5
Module Ax (n,A,x,b)
\ DESCRIPTION: A pseudomodule to perform matrix-vector multiplication, Ax.
Declare: ann, xn, bn \ Declare matrix and vectors as array variables
For “
i “ 1, n‰ \ Loop i: Sweep from top row to bottom
bi Ð 0 \ Initialize the accumulator, bi
For “
k “ 1, n‰ \ Loop k: Sweep row from left to right
bi Ð bi ` aik ˚ xk \ Add aik ˚ xk to bi
End For
End For
End Module Ax
2.1.4 DETERMINANTS
A determinant, denoted by det(A) or |A|, is a unique scalar number calculated by the
elements of a square matrix A. It provides useful information about the matrix, such as
whether it is invertible or not. The determinant of a 2ˆ2 matrix is defined as follows:
ˇ
ˇ
ˇ
ˇ
ˇ
a1 a2
b1 b2
ˇ
ˇ
ˇ
“ a1b2 ´ b1a2,
ˇ
ˇ
Sarrus’s rule is commonly used as a quick way of evaluating a 3ˆ3 determinant. The first
two columns are copied to the right side of the determinant, as shown below. Then the sumLinear Systems: Fundamentals and Direct Methods  73
of the products of the three opposite diagonal elements (from south-west to north-east) is
subtracted from the product of the main diagonal elements (from north-west to south-east).
a1 a2 a3 a1 a2
b1 b2 b3 b1 b2
c1 c2 c3 c1 c2
“ a1b2c3 ` a2b3c1 ` a3b1c2 ´ pa3b2c1 ` a1b3c2 ` a2b1c3q
Sarrus’ rule is valid for matrices of size 2ˆ2 and 3ˆ3; it cannot be
generalized to the determinants of matrices of size 4 ˆ 4 or larger.
To generalize the determinant of a square matrix A “ raij snˆn, we define minor (de￾noted by Mij ) associated with aij , which is the determinant of the pn´1qˆpn´1q submatrix
obtained by deleting the ith row and the jth column of matrix A. The cofactor of aij is
related to the minor by Cij “ p´1q
i`j
Mij . For example, for matrix A “ raij s3ˆ3, the
minor and cofactor of a11 and a23 are obtained as follows:
M11 “
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
a11 a12 a13
a21 a22 a23
a31 a32 a33
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
“
ˇ
ˇ
ˇ
ˇ
a22 a23
a32 a33
ˇ
ˇ
ˇ
ˇ “ a22a33 ´ a32a23, C11 “ p´1q
1`1
M11 “ M11
M23 “
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
a11 a12 a13
a21 a22 a23
a31 a32 a33
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
“
ˇ
ˇ
ˇ
ˇ
a11 a12
a31 a32
ˇ
ˇ
ˇ
ˇ “ a11a32 ´ a31a12, C23 “ p´1q
2`3
M23 “ ´M23
Note that the 2 ˆ 2 determinant obtained after deleting the first row and first column of
the matrix gives M11, while in the latter case, the one obtained after deleting the second
row and third column gives M23.
In general, the determinant of a square matrix A is calculated by the cofactor expansion
method along any row or any column as follows:
detpAq “ ai1Ci1 ` ai2Ci2 `¨¨¨` ainCin “ ÿn
j“1
aijCij for any i (2.7)
or
detpAq “ a1jC1j ` a2jC2j `¨¨¨` anjCnj “ ÿn
i“1
aijCij for any j (2.8)
Properties of Determinants: Let A and B be n ˆ n square matrices and
λ be a non-zero scalar number, then
‚ Reflection property: detpAT q “ detpAq;
‚ Proportionality property: detpAq “ 0 if all of the elements in any row
(or column) are proportional by λ to the elements in another row (or
column);
‚ Switching property: If any two rows (or columns) of a determinant are
interchanged, then its sign changes;
‚ Scalar multiple property: If any row (or column) of a determinant is
multiplied by λ, then the value of the new determinant becomes74  Numerical Methods for Scientists and Engineers
λ. ¨ detpAq; likewise, if a matrix A is multiplied by λ, then detpλAq “
λn detpAq;
‚ Invariance property: A determinant remains unaltered if any row (or
column) is replaced with a linear combination of two or more rows (or
columns);
‚ Triangular matrix property: The determinant of an upper-(U) or lower￾triangular (L) or diagonal (D) matrix is equal to the product of the main
diagonal elements, i.e., detpUq “ u11u22 ¨¨¨ unn, detpLq “ �11�22 ¨¨¨ �nn,
and detpDq “ d11d22 ¨¨¨ dnn;
‚ Product property: detpABq “ detpAq detpBq;
‚ Inverse matrix property: detpA´1q “ 1{ detpAq.
2.1.5 SYSTEM OF LINEAR EQUATIONS
General definition. A system of linear equations, or linear system, is a collection of m
equations with n unknowns of the form:
a11x1 ` a12x2 ` a13x3 `¨¨¨` a1nxn “ b1
a21x1 ` a22x2 ` a23x3 `¨¨¨` a2nxn “ b2
a31x1 ` a32x2 ` a33x3 `¨¨¨` a3nxn “ b3
.
.
. .
.
. .
.
. ... .
.
. .
.
.
am1x1 ` am2x2 ` am3x3 `¨¨¨` amnxn “ bm
(2.9)
For convenience, the system can be expressed in matrix form as
Ax “ b (2.10)
where aij and bi are the known coefficients of matrix A and the rhs vector b, respectively,
and x is a vector of unknown coefficients expressed explicitly as
A “
»
—
—
—
—
—
–
a11 a12 a13 ¨¨¨ a1n
a21 a22 a23 ¨¨¨ a2n
a31 a32 a33 ¨¨¨ a3n
.
.
. .
.
. .
.
. ... .
.
.
am1 am2 am3 ¨¨¨ amn
fi
ffi
ffi
ffi
ffi
ffi
fl
, x “
»
—
—
—
—
—
–
x1
x2
x3
.
.
.
xn
fi
ffi
ffi
ffi
ffi
ffi
fl
, b “
»
—
—
—
—
—
–
b1
b2
b3
.
.
.
bm
fi
ffi
ffi
ffi
ffi
ffi
fl
A linear system of n equations with n unknowns (i.e., a consistent set of equations) has
a unique solution if it is nonsingular (i.e., detpAq ‰ 0). A system whose number of equations
is smaller than the number of unknowns (m ă n) is said to be an underdetermined system.
But if the number of equations is greater than that of the unknowns (m ą n), then it is
called an over-determined system. The solution of a linear system with m equations and
n unknowns is an ordered sequence (s1, s2,...,sn) in which each equation is satisfied for
x1 “ s1, x2 “ s2, ..., xn “ sn. The general solution, or the solution set, is the set of all
possible solutions. In most practical problems, however, the number of unknowns is the
same as the number of equations.
A system of linear equations is called homogeneous if the right-hand side of each and
every equation is zero, i.e., Ax “ 0. When the solution of a linear system is x “ 0, then it
is termed the trivial solution. A solution with at least one non-zero value is referred to as a
non-trivial solution.Linear Systems: Fundamentals and Direct Methods  75
Tridiagonal systems. A system of linear equations with a tridiagonal coefficient
matrix is encountered in many aspects of numerical analysis. A tridiagonal system is a
special case of a banded system (i.e., mL “ mU “ 1, nb “ 3) with nonzero elements on the
main diagonal, the first diagonal above, the first diagonal below, and the right-hand-side
vector. A typical triangular n by n system is expressed as follows:
»
—
—
—
—
—
—
—
–
d1 a1
b2 d2 a2
b3 d3 a3
... ... ...
bn´1 dn´1 an´1
bn dn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
–
x1
x2
x3
.
.
.
xn´1
xn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
–
c1
c2
c3
.
.
.
cn´1
cn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(2.11)
By taking advantage of the sparsity of the tridiagonal matrix, a significant reduction in
both memory and computational cost is achieved by defining the system with four arrays:
bi, di, ai, and ci for all i’s.
Cramer rule. One of the direct methods that may be applied to solve a system of n
linear equations with n unknowns is the Cramer’s rule. Consider the linear system given in
Eq. (2.9), where matrix A is invertible. The matrix Ai is constructed by replacing the ith
column of A with b. If the unique solution of a matrix equation is x “ rx1, x2,...,xns
T ,
then its solution for xi is found from
xi “ detpAiq
detpAq
, for i “ 1, 2,...,n
where
detpAq “
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
a11 a12 a13 ¨¨¨ a1n
a21 a22 a23 ¨¨¨ a2n
a31 a32 a33 ¨¨¨ a33
.
.
. .
.
. .
.
. ... .
.
.
an1 an2 an3 ¨¨¨ ann
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
, detpAiq “
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
a11 a12 ¨¨¨ b1 ¨¨¨ an
a21 a22 ¨¨¨ b2 ¨¨¨ a2n
a31 a32 ¨¨¨ b3 ¨¨¨ a3n
.
.
. .
.
. ... .
.
. ... .
.
.
an1 an2 ¨¨¨ bn ¨¨¨ ann
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
Cramer’s rule, based on computing multiple determinants, is a
highly inefficient method. The memory requirement for a system
of nˆn linear equations is on the order of Opn3q, and the number
of arithmetic operations is on the order of O ppn ` 1q!q. For this
reason, it is impractical to apply to large linear systems as it
requires extensive memory and cpu-time.
2.1.6 EIGENVALUES AND EIGENVECTORS
Eigenvalues, also known as characteristic values, are a set of unique scalar values associated
with a set of linear equations, i.e., a matrix equation. The formal definition of a matrix
eigenvalue problem is given as
A x “ λ x (2.12)
where A is a n ˆ n square matrix, x is a vector of length n, and λ is a scalar constant.
Solving an eigenvalue problem means finding scalar the λ values and associated x
vectors satisfying Eq. (2.12) which may also be recast as follows:
pA ´ λIq x “ 0 (2.13)76  Numerical Methods for Scientists and Engineers
In order for this homogeneous system of linear equations to have a non-trivial solution
(x ‰ 0), the system must satisfy the following condition:
detpA ´ λIq “ pnpλq “ 0 (2.14)
where pnpλq is an nth-degree polynomial referred to as a characteristic polynomial, and λ
denotes the set of unique scalars called the eigenvalues, i.e., the roots of the characteristic
polynomial, λi for i “ 1, 2,...,n. Accordingly, it can be said that λ is an eigenvalue if and
only if it satisfies pnpλq “ 0. Note that the characteristic equation yields n eigenvalues
(real or imaginary) that are unique to the matrix A. A unique solution of Eq. (2.12) for a
specified λi is denoted by xi and referred to as the corresponding or associated eigenvector.
Each eigenvalue is paired with its associated eigenvector, also frequently referred to as an
eigenpair.
EXAMPLE 2.1: Finding eigenvalues and normalized eigenvectors
Find the eigenvalues and eigenvectors of the following matrix.
A “
»
– ´9 48
´11 10 4
´9 84
fi
fl
SOLUTION:
All eigenvalues and eigenvectors satisfy Eq. (2.13), so we set
detpA ´ λIq “
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
´9 ´ λ 4 8
´11 10 ´ λ 4
´9 84 ´ λ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
“ 0
which results in the following characteristic polynomial:
p3pλq “ λ3 ´ 5λ2 ´ 2λ ` 24 “ 0
All of the roots (i.e., eigenvalues) of the characteristic polynomial are real and
distinct: λ1 “ ´2, λ2 “ 3, and λ3 “ 4. An eigenvector (xk) associated with λk is
the solution of the corresponding matrix equation pA ´λkIqxk “ 0, which takes the
following explicit form:
»
–
´9 ´ λ 4 8
´11 10 ´ λ 4
´9 84 ´ λ
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
0
0
0
fi
fl
For λ1 “ ´2, the homogeneous system becomes
»
– ´7 48
´11 12 4
´9 86
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
0
0
0
fi
fl
Depending on the choice of x1, x2, or x3, there are an infinite number of solution sets
that are multiples of the same solution. As a result, for instance, arbitrarily letting
x1 “ c1 and solving for x2 and x3 yields the following associated eigenvector:
x1 “
»
–
x1
x2
x3
fi
fl “ c1
»
–
1
3{4
1{2
fi
flLinear Systems: Fundamentals and Direct Methods  77
Next, for λ2 “ 3, the homogeneous system takes the form
»
–
´12 4 8
´11 7 4
´9 81
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
0
0
0
fi
fl
Similarly, employing the above procedure with x1 “ c2, the solution of the homoge￾neous system for x2 and x3 results in the following eigenvector:
x2 “
»
–
x1
x2
x3
fi
fl “ c2
»
–
1
1
1
fi
fl
Finally, for λ3 “ 4, we obtain
»
–
´13 4 8
´11 6 4
´9 80
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
0
0
0
fi
fl
Setting x1 “ c3 and solving the homogeneous system for x2 and x3 yields its associ￾ated eigenvector:
x3 “
»
–
x1
x2
x3
fi
fl “ c3
»
–
1
9{8
17{16
fi
fl
Eigenvectors are generally normalized to unit length (as xi{xi) and saved in
matrix form as each eigenvector is placed in a column corresponding to the eigen￾values λ1, λ2, and λ3, respectively. Consequently, the eigenvectors of this example
can be expressed as follows:
»
—
—
—
—
–
4
?29
1
?3
16
?869
3
?29
1
?3
18
?869
2
?29
1
?3
17
?869
fi
ffi
ffi
ffi
ffi
fl
where the first, second, and third columns correspond to the normalized associated
eigenvectors of λ1 “ ´2, λ2 “ 3 and λ3 “ 4, respectively.
Discussion: An n ˆ n square matrix A has n independent and unique eigenvalues
and eigenvectors, which are found by solving a homogeneous system: pA´λIqx “ 0.
Normalizing an eigenvector to have a unit length gives the same vector regardless of
the arbitrary choices of constants c1, c2, and so on.
Eigenpairs of a Tridiagonal Toeplitz matrix. In numerical analysis, the eigenvalues and
eigenvectors of a tridiagonal matrix are frequently encountered. A common special form of
a tridiagonal matrix is the diagonally-constant matrix (all the elements are constant along
each diagonal) called the Toeplitz matrix (named after the German mathematician Otto
Toeplitz). This kind of matrix is not confined to tridiagonal or symmetric matrices. A
square Toeplitz matrix has at most 2n ´ 1 unique values as opposed to n2 in a general
square matrix.78  Numerical Methods for Scientists and Engineers
A tridiagonal Toeplitz has the following form:
T “
»
—
—
—
—
—
—
—
–
d a
bd a
bd a
... ... ...
b da
b d
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
nˆn
where a, b, and c are real constants.
The exact eigenvalues of T are found by the following expression
λk “ d ` 2
?
ab cos ˆ kπ
n ` 1
˙
, k “ 1, 2,...,n (2.15)
Additionally, the jth component of the associated eigenvector of the kth eigenvalue is ob￾tained from
vpkq
j “
ˆ b
a
˙j{2
sin ˆ jkπ
n ` 1
˙
, j “ k “ 1, 2,...,n (2.16)
Equations (2.15) and (2.16) are simple and very useful when determining the exact eigen￾pairs of some of the most common eigenvalue problems.
EXAMPLE 2.2: Application of eigenvalues and eigenvectors in physics
(1,3)-Butadiene (C4H6) is a conjugated diene, structurally joined together as two
vinyl groups (CH2) with a single bond. Hückel’s approximation is applied to the
π-orbitals of butadiene. Upon employing Schöringer’s equation with simplifying as￾sumptions, it leads to the so-called secular equations (HΨ “ EΨ), where the Hamil￾tonian matrix (H) and molecular orbital (Ψ) are given as
H “
»
—
—
–
α β
βαβ
βαβ
β α
fi
ffi
ffi
fl
and Ψ “
»
—
—
–
ψ1
ψ2
ψ3
ψ4
fi
ffi
ffi
fl
with α and β being the Hückel parameters. Noting that every orbital energy (Ei,
eigenvalue) corresponds to a molecular orbital (Ψi, eigenvector), find the orbital
energies and corresponding molecular orbitals for the butadiene.
SOLUTION:
The Hamiltonian matrix is a symmetric tridiagonal Toeplitz matrix and the
eigensystem can be rearranged as a homogeneous system (HΨ ´ EΨ “ 0) as
»
—
—
–
α ´ E β
β α ´ E β
β α ´ E β
β α ´ E
fi
ffi
ffi
fl
»
—
—
–
ψ1
ψ2
ψ3
ψ4
fi
ffi
ffi
fl “
»
—
—
–
0
0
0
0
fi
ffi
ffi
fl
The parameter dependence (α and β) is eliminated by employing pα ´ Eq“´xβLinear Systems: Fundamentals and Direct Methods  79
substitution, yielding the following form:
»
—
—
–
0 ´ x 1
1 0 ´ x 1
1 0 ´ x 1
1 0 ´ x
fi
ffi
ffi
fl
»
—
—
–
ψ1
ψ2
ψ3
ψ4
fi
ffi
ffi
fl “
»
—
—
–
0
0
0
0
fi
ffi
ffi
fl
The eigenvalues of the 4ˆ4 tridiagonal matrix are determined from Eq. (2.15)
by setting a “ b “ 1 and d “ 0. The resulting eigenvalues are obtained from
xk “ 2 cospkπ{5q for k “ 1, 2, 3, and 4, which yield
x1 “ 1
2
p1 ` ?
5q, x2 “ 1
2
p´1 ` ?
5q, x3 “ 1
2
p1 ´ ?
5q, x4 “ ´1
2
p1 ` ?
5q,
Making use of the substitution pα ´ Eq“´xβ, the orbital energies are found as
E1,2 “ α ` β
2
p˘1 ` ?
5q, E3,4 “ α ˘ β
2
p1 ´ ?
5q
For the eigenvectors, Eq. (2.16) yields ψpkq
j “ sin pjkπ{5q, which for j, k “ 1, 2,
3, and 4, rounding to four digits, results in
»
—
—
–
0.5877 0.9511 0.9511 0.5877
0.9511 0.5877 ´0.5877 ´0.9511
0.9511 ´0.5877 ´0.5877 0.9511
0.5877 ´0.9511 0.9511 ´0.5877
fi
ffi
ffi
fl
Discussion: The eigensystem gave four unique solutions. Columns 1 through 4 are
the eigenvectors of Ψ1 through Ψ4, respectively.
2.1.7 VECTOR AND MATRIX NORMS
Vector norms. In any numerical computation, we are always concerned with knowing the
magnitude of an approximation error. Moreover, when two vectors are frequently compared,
it is more practical and efficient to measure the difference between the two vectors with a
single scalar value rather than comparing the vectors element-by-element. A vector norm,
denoted by �x�, is a single positive scalar value representing the length, size, or magnitude
of the vector, and it is well suited for vector comparisons.
The most commonly used vector norms belong to the family of p´norms (or �p´norms),
which depend on their physical meaning. A general mathematical definition is given as
�x�p “ pxp
1 ` xp
2 `¨¨¨` xp
nq
1{p “
´ ÿn
i“1
|xi|
p
¯1{p
where x “ rx1, x2, ¨¨¨ , xns
T . For p “ 1, the �1´norm becomes
�x�1 “ ÿn
i“1
|xi|
which is the sum of the lengths of the vectors in a space, while for p “ 2, the �2´norm (or
also called the Euclidean norm) denotes the shortest distance between two points:80  Numerical Methods for Scientists and Engineers
�x�2 “
´ ÿn
i“1
|xi|
2
¯
1{2
The �8´norm (or infinity norm), which gives the largest magnitude among each ele￾ment of a vector (also called maximum magnitude norm), is mathematically expressed as
�x�8 “ max
1ďiďn |xi|
Properties of a vector norm: Let x and y be vectors of the same length and
α be a non-zero scalar, then a vector norm satisfies the following conditions:
‚ Positivity property: �x� ě 0 and �x� “ 0 if and only if x “ 0;
‚ Homogeneity property: �αx� “ |α|�x�;
‚ Triangle inequality: �x� ` �y� ď �x ´ y� ď �x ` y�.
Matrix norms. The norms of matrix A is denoted �A� and is similar to a vector norm in
that it is a measure of the magnitude of the matrix. The two norms commonly encountered
are �1´ norm (maximum column sum) and �8´norm (maximum row sum):
}A}1 “ max
1ďjďn
ÿn
i“1
|aij | and }A}8 “ max
1ďiďn
ÿn
j“1
|aij |
The �2´norm of a matrix A (also called spectral norm) is defined as
}A}2 “ aλmax
where λmax is the largest magnitude eigenvalue of the AT A product. If A is a symmetric
matrix, �A�2 “ μmax corresponds to the largest magnitude eigenvalue of A. The �A�2
norm is always less than (or equal to) �A�1 and �A�8.
The Frobenius norm, or Euclidean norm (or �F ´norm), is defined as the square root
of the sum of the squares of the elements of a matrix. For a matrix A “ raij snˆm, it is
mathematically expressed as follows:
�A�F “
´ ÿn
i“1
ÿm
j“1
a2
ij¯1{2
Properties of matrix norms: Let A and B be matrices of the same size and
α be a non-zero scalar, then matrix norms satisfy the following properties:
‚ Positivity property: �A� ě 0 and �A� “ 0 if and only if A “ O;
‚ Homogeneity property: �αA� “ |α|�A�;
‚ Triangle inequality: �A ` B� ď �A� ` �B� or �A� ´ �B� ď �A ´ B�
In addition to the above-given “required” properties, some also satisfy the
following (additional) properties not required of all matrix norms:
‚ Subordinance property: �Ax� ď �A��x�;
‚ Submultiplicativity property: �AB� ď �A��B�.Linear Systems: Fundamentals and Direct Methods  81
EXAMPLE 2.3: Norms of a vector and a matrix
For the given matrix A and vector x, calculate all possible norms.
A “
»
–
2 1 ´2
´1 ´3 3
342
fi
fl , x “
»
–
´2
1
3
fi
fl
SOLUTION:
For x, the norms are found as
�x�1 “|´2| ` |1| ` |3|“6, �x�2 “`
p´2q
2 ` 12 ` 32˘1{2
“?
14, �x�8 “|3|“3
For the matrix A, the norms are determined as follows:
�A�1 “max
j p|2|`|´1|`|3|, |1|`|´3|`|4|, |´2|`|3|`|2|q“max p6, 8, 7q“8
�A�8 “max
i p|2|`|1|`|´2|, |´1|`|´3|`|3|, |3|`|4|`|2|q“max p5, 7, 9q“9
�A�F “`
22`p´1q
2`22`p´1q
2`p´3q
2`32`32`42`22˘1{2
“?
57
In order to find �A�2, the maximum magnitude eigenvalue of the product AT A
is required. The characteristic polynomial of the AT A is obtained as
detpAT A ´ λIq“
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
14 ´ λ 17 ´1
17 26 ´ λ ´3
´1 ´3 17 ´ λ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
“λ3´57λ2`745λ´1225“0
The roots (i.e., eigenvalues) are found as λ «1.916, 16.629, and 38.4552 yielding
λmax “38.4552 and �A�2 “?38.4552 “ 6.20122.
Discussion: A norm gives a single unique scalar value for a vector or matrix, which
is used for comparing or sorting vectors or matrices. Matrix norms are also used for
sensitivity analysis and estimating the condition number of a matrix.
2.2 ELEMENTARY MATRIX OPERATIONS
Elementary matrix operations play an important role in linear algebra when inverting ma￾trices or solving systems of linear equations. These operations are applied to augmented
matrices either as row or column operations. Upon employing row (or column) operations,
also referred to as row-reduction (or column-reduction), an augmented matrix is transformed
into an equivalent matrix.
Before we proceed any further, let us introduce column and row notations, which we
will extensively use in this text from this point on. The following 3 ˆ 2 rectangular matrix
has 3 rows and 2 columns; all rows and columns are represented as
»
–
2 1
3 ´2
4 5
fi
fl ñ
r1 “ r 2 1 s
r2 “ r 3 ´ 2 s
r3 “ r 4 5 s
, c1 “
»
–
2
3
4
fi
fl , c2 “
»
–
1
´2
5
fi
fl
where ri and cj denote the ith row and jth column, respectively. These notations are partic￾ularly useful when developing algorithms and tracking the order of elementary operations.82  Numerical Methods for Scientists and Engineers
As a reduction is applied to a specific row or column (ri or ci), it is wise to code the ele￾mentary operations with arrows (Ð, Ò, Ø) aligned with the modified row or column (r1
i or
c1
i).
There are basically three elementary operations or transformations: scaling, pivoting,
and elimination. Now, we will discuss these elementary operations in greater detail.
1) Scaling: The elements of a row (or a column) of a matrix can be multiplied by a non￾zero real number (λ ‰ 0). For example, the notation ri Ð λri (or ci Ð λci) indicates
that the multiplication of the ith row (or ith column) by a scalar λ is overwritten on
the ith row (or i’th column);
»
–
2 1
3 ´2
4 5
fi
fl „
»
–
6 3
3 ´2
4 5
fi
fl Ð 3r1
»
–
2 1
3 ´2
4 5
fi
fl „
»
–
2 ´2
3 4
4 ´10
fi
fl
Ò
´2c2
Row-1 is multiplied by 3 in place;
r1
1 Ð 3r1.
Column-2 is multiplied by p´2q in
place; c1
2 Ð p´2qc2.
2) Pivoting: Two rows (or two columns) of a matrix can be interchanged. The notation
ri Ø rj or ri Õ rj (or ci Õ cj ) indicates interchanging the ith row (or i’th column)
with the jth row (or i’th column).
»
–
2 1
3 ´2
4 5
fi
fl „
»
–
4 5
3 ´2
2 1
fi
fl
r1 Ø r3
»
–
2 1
3 ´2
4 5
fi
fl „
»
–
1 ´2
´2 3
5 ´4
fi
fl
c1
Ù
c2
Row-1 and Row-3 are interchanged Col-1 and Col-2 are interchanged
3) Elimination: A row (or column) of a matrix can be obtained by a linear combination
of two rows (or two columns). For given m and n, the notation rk Ð mri ` nrj
(or ck Ð mci ` ncj ) denotes placing a linear combination of ith and jth rows (or
columns) into the kth row (or column) or the row (or column) to which the arrow is
points (Ð mri ` nrj or Ð mci ` ncj ).
»
–
2 1
3 ´2
4 5
fi
fl „
»
–
2 1
7 0
6 0
fi
flÐ r2 ` p2qr1
Ð r3 ` p´5qr1
»
–
2 1
3 ´2
4 5
fi
fl „
»
–
0 1
7 ´2
´6 5
fi
fl
Ò
c1 ´ 2c2
Row-1 times 2 is added to Row-2 in pla￾ce, r1
2 Ð r2 `2r1; Row-1 times p´5q is
added to Row-3 in place, r1
3Ðr3`p´5qr1.
Col-2 times p´2q is added to Col-1 in
place, c1
1 Ð c1 ` p´2qc1.
When a matrix A is reduced to another matrix (say B) by a
series of row (or column) reductions, matrix A is said to be row
(or column) equivalent to B and is denoted A„B. It is incorrect
to use ““” or “ñ” instead of “„” for equivalent matrices.Linear Systems: Fundamentals and Direct Methods  83
In numerical algorithms, the elementary operations are applied to serve a specific pur￾pose. As a divisor in the elimination process, it is important that the pivot (main diagonal)
element should neither be “zero” nor have a value that is too small. A numerical procedure
that requires numerous eliminations with pivot elements whose magnitudes are smaller than
those of the other elements can result in significant rounding errors. Scaling is done mainly
to select pivot elements, which helps prevent “underflow” or “overflow” of numbers and
reduces susceptibility to round-off errors. On the other hand, even if the original matrix
has no zeros on the principal diagonal, the elimination process can produce zeros on the
diagonal. Therefore, rows (or equations) or columns (or variables in linear systems) can be
modified so that the element with the largest magnitude is placed on the diagonal. This
process is called “pivoting. Additionally, many algorithms require that the elements below
or above the diagonal of a matrix, or both, be zero. The elimination procedure can be used
to systematically eliminate elements above or below the main diagonal, either row by row,
column by column, or both.
How can a matrix be equivalent even though the original matrix
is destroyed? Recall that a linear system of equations can be put
into matrix form. (1) Scaling is simply multiplying both sides of
an equation by the same scalar; (2) Pivoting by interchanging
the two rows means changing the order of the equations; and (3)
Elimination is simply a linear combination of the two equations
that preserves the solution if the system has a common solution.
2.3 MATRIX INVERSION
We have already seen that matrix addition, subtraction, and multiplication are analogs
to the same operations for numbers, and matrix division is not defined. To introduce the
matrix analogy of division, consider the solution of ax “ b. For a ‰ 0, multiplying both
sides of ax “ b by the multiplicative inverse of a (i.e., a´1 “ 1{a), the solution is found as
x “ a´1b since a´1a “ 1. The concept of multiplicative inverse is applied to matrix algebra
using the same reasoning.
Let A be an n ˆ n square matrix. If there exists an n ˆ n matrix B such that AB “
BA “ In, where In is the identity matrix of order n, then the matrix B is the multiplicative
inverse of A. The inverse matrix is denoted by A´1, and left- or right-multiplying it with A
gives the identity matrix, i.e., AA´1 “ A´1A “ In. However, it should be noted that not
every matrix necessarily has an inverse. Firstly, a matrix to be inverted must be a square
matrix. Secondly, it must be invertible (or non-singular); that is, |A| ‰ 0.
The inverse of an invertible diagonal matrix D “ rdiisnˆn (dii ‰ 0 for all i) represents
the simplest case and corresponds to the reciprocal of the corresponding diagonal elements:
D´1 “
»
—
—
—
–
1{d11 0 ¨¨¨ 0
0 1{d22 ¨¨¨ 0
.
.
. .
.
. ... .
.
.
0 0 ¨¨¨ 1{dnn
fi
ffi
ffi
ffi
fl
There are several ways in which a matrix can be inverted: the adjoint method, Gauss￾Jordan, matrix partition, LU-decomposition, etc. In this section, much attention will be
devoted to the Gauss-Jordan Reduction method due to its computational efficiency and low
cpu cost in comparison to other existing methods.84  Numerical Methods for Scientists and Engineers
Adjoint Method
‚ Fast and easy evaluation of the inverse of 2 ˆ 2 or 3 ˆ 3 matrices;
‚ Once the inverse of a matrix with variable elements is obtained, it
can then be used repeatedly.
‚ For large n, the computational cost of matrix inversion is very expen￾sive. The number of arithmetic operations is in the order of pn ` 1q!,
and it is generally not recommended for matrices larger than 3 ˆ 3.
Properties of the inverse: Let A and B (also AT , A´1, and AB) be invertible
(non-singular) matrices and α be a non-zero scalar, then
‚ Uniqueness property: The matrix A´1 is the unique matrix satisfying
the condition AA´1 “A´1A“ In, where In is an n ˆ n identity matrix;
‚ Reflexive property: pA´1q
´1
“A;
‚ Exchange of inverse/transpose operations: pAT q
´1
“ pA´1q
T
;
‚ Scalar multiple property: pαAq
´1 “α´1pA´1q;
‚ Exchange of multiplication order: pABq
´1 “B´1A´1.
2.3.1 ADJOINT METHOD
Consider an invertible matrix A “ raij snˆn and a matrix C “ rCij snˆn, where Cij is the
cofactor of aij . The adjoint matrix of A denoted AdjpAq is simply the transpose of the
cofactor matrix C, i.e., AdjpAq “ CT . Then the inverse is determined by
A´1 “ AdjpAq
detpAq
For a 2 ˆ 2 matrix, the inverse matrix yields
A´1 “
„
a b
c dj´1
“ 1
detpAq
„
C11 C12
C21 C22 jT
“ 1
detpAq
„ d ´b
´c a j
2.3.2 GAUSS-JORDAN METHOD
Gauss-Jordan elimination is a method that is used to find the inverse of an invertible matrix
and, also, to find the solution of a linear system of equations. The method basically relies
on elementary row operations. It is applied to a larger matrix called the augmented matrix,
which (in the case of matrix inversion) is formed by adjoining matrix A and identity matrix
I, leading to a rectangular matrix rA| Is of size n ˆ 2n:
rA| Is “
»
—
—
—
–
a11 a21 ¨¨¨ a1n
a21 a22 ¨¨¨ a2n
.
.
. .
.
. ... .
.
.
an1 an2 ¨¨¨ ann
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
1 0 ¨¨¨ 0
0 1 ¨¨¨ 0
.
.
. .
.
. ... .
.
.
0 0 ¨¨¨ 1
fi
ffi
ffi
ffi
fl
nˆ2n
(2.17)
where the vertical line separates the two matrices.Linear Systems: Fundamentals and Direct Methods  85
FIGURE 2.1: Illustration of the matrix inversion procedure ‚ and b denote the non-zero
elements occupied in matrices A and I, respectively.
The objective is to shift the identity matrix to the left-hand side of Eq. (2.17) through
a sequence of elementary row operations. The row operations, however, are carried out in a
systematic manner, as illustrated in Fig. 2.1. After a series of row operations, the first pass
should yield the first column of I in place of the first column of A. Note that while working
on the first column of A, simultaneously the first column of I is destroyed. On moving to the
next column, the element on the diagonal (the pivot) must first be normalized to“1." Then
the elements above and below “1” are zeroed through proper row operations. With each
pass, a column of matrix A is replaced sequentially from left to right by the corresponding
column of the identity matrix. After n passes, A is transformed to I, and the identity matrix
on the rhs is destroyed completely, leading to rI| Bs, where B is now the inverse matrix,
i.e., B “ A´1 and rA| Is Ñ “
I| A´1
‰
.
Now we follow the step-by-step implementation of this algorithm in more detail. The
first step is to scale (or normalize) the first row by multiplying with p “ 1{a11:
»
—
—
—
–
1
a21
.
.
.
an1
a1
12
a22
.
.
.
an2
¨¨¨
¨¨¨
...
¨¨¨
a1
1n
a2n
.
.
.
ann
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
b1
11
0
.
.
.
0
0
1
.
.
.
0
¨¨¨
¨¨¨
...
¨¨¨
0
0
.
.
.
1
fi
ffi
ffi
ffi
fl
Ð p r1
(2.18)
where ri “ rai1 ... ain bi1 ... bins denotes the i
th row of the augmented matrix, b1
11 and
a1
1j denote the matrix element modified as a result of normalization, i.e., b1
11 “ p and
a1
1j “ p a1j for j “ 1, 2, 3,...,n.
Next, the elements below the first column pivot (a21, a31, ..., an1) are eliminated by
the following row operations: r1
i Ð ri ´ ai1r1
1 (i “ 2, 3,...,n). The first pass yields
»
—
—
—
–
1
0
.
.
.
0
a1
12
a1
22
.
.
.
a1
n2
¨¨¨
¨¨¨
...
¨¨¨
a1
1n
a1
2n
.
.
.
a1
nn
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
b1
11
b1
21
.
.
.
b1
n1
0
1
.
.
.
0
¨¨¨
¨¨¨
...
¨¨¨
0
0
.
.
.
1
fi
ffi
ffi
ffi
fl
Ð r2 ´ a21r1
1
.
.
.
Ð rn ´ an1r1
1
(2.19)86  Numerical Methods for Scientists and Engineers
Note that the first column of A is now identical to I. Meanwhile, the row operations naturally
modify the elements of A (for rows i ě 2) and the first column of I accordingly:
a1
ik “ aik ´ s ˚ a1
1,k and b1
i1 “ bi1 ´ s ˚ b1
11, i, k “ 2, 3,...,n
where s “ ai1, and i and k denote row and column sweeps, respectively. Also, notice that,
by iterating from i “ 2 and k “ 2, we do not actually carry out the computation that makes
zero, which would be a waste of cpu-time.
When a matrix is read into a program, make sure that the diago￾nal elements are not zero (or that the matrix is entered correctly)
or that the matrix is invertible. In Pseudocode 2.6, the input ma￾trix A is assumed to be invertible and possess non-zero diagonal
elements (ajj ‰ 0).
The second pass begins by scaling the second row so that the pivot is a1
22 “ 1. The
elimination step zeros out the elements above and below the pivot. Subsequently, with each
pass, a column of A is replaced with the corresponding column of I for the rest of the
columns (j “ 3, 4,...,n). In loop´j (column-loop), the normalizing multiplier (p “ 1{ajj )
is prepared for scaling. The jth row is scaled with p; then all the elements in the jth row of
the A and I matrices are modified using the inner loop over k variable according to
b
pjq
jk “ p ˚ b
pj´1q
jk , and apjq
jk “ p ˚ apj´1q
jk , for k “ 1, 2,...,n
where we will now abandon the “prime notation” and instead use the superscript “(j)”
to denote the number of modifications that have occurred, i.e., indicating that the matrix
element has been overwritten j times.
Row reduction is applied with the loop´i, which sweeps the row from the pivot to the
end of the row:
apjq
ik “ apj´1q
ik ´ s ˚ apjq
jk and b
pjq
ik “ b
pj´1q
ik ´ s ˚ b
pjq
jk , for i ‰ j, i, k “ 1, 2,...,n
where s “ apj´1q
ij .
Finally, once all possible elementary operations for all columns are completed, the
augmented matrix takes the form rI| Bs:
»
—
—
—
–
1
0
.
.
.
0
0
1
.
.
.
0
¨¨¨
¨¨¨
...
¨¨¨
0
0
.
.
.
1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
b11
b21
.
.
.
bn1
b12
b22
.
.
.
bn2
¨¨¨
¨¨¨
...
¨¨¨
b1n
b2n
.
.
.
bnn
fi
ffi
ffi
ffi
fl
(2.20)
where bij ’s are now elements of the inverse matrix, i.e., A´1 “ B.
In Pseudocode 2.6, setting p “ 1{ajj and s “ aij serves to speed
up the computation (reduce cpu-time) by minimizing the search
time to locate and retrieve an element of a matrix that would
otherwise have been accessed numerous times. This is one of the
important factors to take into account in programming when deal￾ing with very large matrices.Linear Systems: Fundamentals and Direct Methods  87
Pseudocode 2.6
Module INV_MAT (n, A, B)
\ DESCRIPTION: A pseudomodule to find inverse of a matrix with Gauss-
\ Jordan method.
Declare: ann, bnn \ Declare matrices as arrays
B Ð 0 \ Initialize B with zero matrix
For “
i “ 1, n‰ \ Construct identity matrix
b ii Ð 1 \ Set diagonals to “1”
End For
For “
j “ 1, n‰ \ Loop j: Sweep from top row to bottom
p Ð 1{ajj \ Set normalizing multiplier, p “ 1{ajj
For “
k “ 1, n‰ \ Loop k: Normalize pivot row (Row-j)
ajk Ð p ˚ ajk \ Normalizing jth row of A
bjk Ð p ˚ bjk \ Normalizing jth row of B
End For
For “
i “ 1, n‰ \ Loop i: Elimination steps on column j
s Ð aij \ Save aij on s
If “
i ‰ j
‰
Then \ Skip diagonal (pivot) element
For “
k “ 1, n‰ \ Loop k: Apply eliminations
aik Ð aik ´ s ˚ ajk \ Modify i
th column of A
bik Ð bik ´ s ˚ bjk \ Modify i
th column of B
End For
End If
End For
End For
\ The original matrix A is destroyed on Exit.
End Module INV_MAT
The solution of a linear system of equations, Eq. (2.9), can be obtained by left￾multiplying both sides of Eq. (2.10) with A´1 and applying A´1A “ I identity:
x “ A´1b (2.21)
In practice, finding the inverse matrix to solve a system of linear equations by Eq. (2.21)
is not recommended. As a matter of fact, computing the inverse matrix requires far more
arithmetic operations in comparison to finding the solution of the system with the Gauss
elimination method (covered in Section 2.5). However, there are some cases where evaluating
the inverse matrix offers some advantages.
A pseudomodule, INV_MAT, employing the Gauss-Jordan elimination method without
pivoting to invert a matrix is presented in Pseudocode 2.6. As input, the module requires a
matrix to be inverted (A) and its size n. On exit, the output matrix B is the inverse of A.
Firstly, the identity matrix is setup: bii “ 1 and bij “ 0 in the code. The matrices A and B
are simultaneously modified as a result of successive elementary row reductions to yield the
inverse matrix. Loop´j runs from 1 to n and sweeps the jth row from left (using the loop
over k variable) to right of the augmented matrix after scaling by p “ 1{ajj : rjk Ð p ˚ rjk.
Then the elements above and below the pivot (skipping i “ j with an If-construction) are
eliminated inside the inner loop´k. Note that setting s “ aij , which is used in eliminations,
is to access the matrix element only once. During this process, the original matrix A is
destroyed.88  Numerical Methods for Scientists and Engineers
Matrix Inversion
‚ The method permits rescaling at each stage, which makes it less sen￾sitive to round-off errors;
‚ Having an inverse matrix is useful when many sets of linear equations
need to be solved with the same matrix A but a different b (i.e.,
Axk “ bk for k “ 1, 2,...). Then, A´1 can be calculated once, and
the solutions of the linear systems are obtained simply by matrix￾vector multiplication—Eq. (2.21);
‚ Finding A´1 can offer invaluable information on the existence or
severity of ill-conditioning (covered in Section 2.7).
‚ Elementary row operations as applied to the augmented matrix dou￾ble the memory requirement and number of arithmetic operations;
‚ Arithmetic operation count is on the order of Opn3q; hence, for large
matrices, cpu-time can get unacceptably large.
EXAMPLE 2.4: Inverse matrix by Gauss-Jordan Method
Given below are three planes that intersect each other at one point.
x ´ y ´ 2z ` 2 “ 0
2x ´ 3y ´ 5z ` 5 “ 0
´x ` 3y ` 5z ´ 5 “ 0
These equations (planes) can be expressed by a system of linear equations, Ax “ b,
where
A “
»
–
1 ´1 ´2
2 ´3 ´5
´13 5
fi
fl , x “
»
–
x
y
z
fi
fl , b “
»
–
´2
´5
5
fi
fl
The intersection point can be found from x “ A´1b. First employ the Gauss-Jordan
method to obtain A´1, and then find the point of intersection, i.e., x.
SOLUTION:
The augmented matrix is constructed as follows:
rA | I s“
»
–
1
2
´1
´1
´3
3
´2
´5
5
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
1
0
0
0
1
0
0
0
1
fi
fl
In the first step, we skip the normalization procedure since a11 “ 1 and proceed
to eliminate the first column elements below a11. The second and third-row elements
in the first column are eliminated by the elementary row operations coded in line
with the second and third rows:
„
»
–
1
0
0
´1
´1
2
´2
´1
3
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
1
´2
1
0
1
0
0
0
1
fi
flÐ r2 ´ 2r1
Ð r3 ` r1
where we used “„” to indicate the matrix equivalence.Linear Systems: Fundamentals and Direct Methods  89
Next, the second row, r2, is scaled to get a22 “ 1 by multiplying r2 with (´1),
i.e., r1
2 Ð p´1qr2, where r1
2 indicates that all the elements of the second row have
been modified.
„
»
–
1
0
0
´1
1
2
´2
1
3
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
1
2
1
0
´1
0
0
0
1
fi
flÐ p´1qr2
The elements above and below the second column pivot (i.e., ´1 and 2) are
eliminated by adding p´2qr2 to r3 (r1
3 Ð r3´2r2) and adding it to r2 (r1
3 Ð r3`r2):
„
»
–
1
0
0
0
1
0
´1
1
1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
3
2
´3
´1
´1
2
0
0
1
fi
fl Ð r1 ` r2
Ð r3 ´ 2r2
In the last step, the third row need not be scaled since a33 “ 1. The third column
elements above the pivot (i.e., ´1 and 1) are eliminated by r1
1 Ð r1 ` r3 (adding r3
to r1) and r1
2 Ð r2 ´ r3 (adding p´r3q to r2 ) row operations:
„
»
–
1
0
0
0
1
0
0
0
1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
0
5
´3
1
´3
2
1
´1
1
fi
fl
Ð r1 ` r3
Ð r2 ´ r3
Notice that, at the end of the row reduction procedure, the modified matrix on
the right-hand side became the inverse of A, while the identity matrix was shifted
to the left-hand side of the augmented matrix.
A´1 “
»
–
0
5
´3
1
´3
2
1
´1
1
fi
fl
The intersection point can now be determined as
x “ A´1b “
»
–
0
5
´3
1
´3
2
1
´1
1
fi
fl
»
–
´2
´5
5
fi
fl “
»
–
0
0
1
fi
fl
which means that the intersection point is (0,0,1).
Discussion: The Gauss-Jordan elimination procedure requires a series of elementary
row operations to transform an original matrix into its inverse. The procedure can
give true solutions if exact arithmetic can be employed. In this regard, symbolic
algebra systems are able to render exact solutions. Once the inverse matrix is found,
especially in hand calculations, it is always a good idea to verify whether the equality
A´1A “ I is satisfied.
The inverse matrix method is one of the leading numerical methods used in
structural engineering applications, signal processing in electrical engineering, con￾trol systems analysis, power systems, and more. Although the method is extremely
useful and versatile, it has some limitations that are presented in the “Pros and
Cons” box. Finding the inverse of a matrix using the adjoint method is also compu￾tationally very expensive and cpu-time intensive, especially for large matrices.90  Numerical Methods for Scientists and Engineers
2.4 TRIANGULAR SYSTEMS OF LINEAR EQUATIONS
Systems of linear equations representing special cases in matrix algebra are expressed by
Lx “ b and Ux “ b, where L and U denote lower and upper triangular matrices, respec￾tively. In such systems, one of the unknowns can be explicitly and easily found from the
solution of either the first or last equation, which consists of a single unknown. The remain￾ing unknowns are then found by marching in one direction (up or down) and substituting
for the unknowns already found.
A lower triangular system of equations, Lx “ b, has a lower triangular coefficient ma￾trix:
»
—
—
—
—
—
–
�11 0 0 ¨¨¨ 0
�21 �22 0 ¨¨¨ 0
�31 �32 �33 ¨¨¨ 0
.
.
. .
.
. .
.
. ... .
.
.
�n1 �n2 �n3 ¨¨¨ �nn
fi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
–
x1
x2
x3
.
.
.
xn
fi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
–
b1
b2
b3
.
.
.
bn
fi
ffi
ffi
ffi
ffi
ffi
fl
(2.22)
or explicitly
�11x1 “ b1
�21x1 ` �22x2 “ b2
�31x1 ` �32x2 ` �33x3 “ b3
.
.
. .
.
. .
.
. ... .
.
.
�n1x1 ` �n2x2 ` �n3x3 ` ¨¨¨` �nnxn “ bn
(2.23)
The algorithm for finding the solution of a lower-triangular system of equations is
termed forward substitution. Assuming L is not singular (i.e., �ii ‰ 0 for all i), the equations
are visited in the forward direction from top to bottom, as shown in Fig. 2.2. First, x1 is
obtained from the first equation (x1 “ b1{�11), and it is placed in the memory space allocated
for x1. With x1 secured, the second equation involving only x1 and x2 is used to find x2:
x2 “ pb2 ´ �21x1q{�22. Note that the value of x1 is fetched from memory when executing
the latter equation, i.e., mathematically speaking, the available solution of x1 is substituted
into the rhs. This procedure is repeated from the top to the bottom until all equations are
reduced to and solved as a single equation with one unknown.
A general expression for xk can be written from the kth row (or equation) as
�k1x1 ` �k2x2 `¨¨¨` �k,k´1xk´1 ` �kkxk “ bk
where xi’s enclosed in the box contain all “known solutions” obtained in previous steps, and
xk, which is the only unknown, yields a generalized solution:
FIGURE 2.2: Illustration of the forward substitution procedure. Note: The boxes
denote allocated memory for xi’s and bi’s, and symbols denote non-zero values.Linear Systems: Fundamentals and Direct Methods  91
Pseudocode 2.7
Module FORW_SUBSTITUTE (n,L,b, x)
\ DESCRIPTION: A pseudomodule to solve Lx “ b by forward substitution.
Declare: �nn, xn, bn \ Declare array variables
x1 Ð b1{�11 \ Solve x1 from 1st equation
For “
k “ 2, n‰ \ Loop k: Sweep equations from top to bottom
sums Ð 0 \ Initialize accumulator variable, sum
For “
j “ 1, k ´ 1
‰ \ Loop j: Accumulate terms for j “ 1,...,pk ´ 1q
sums Ð sums ` �kj ˚ xj \ Add �kj ˚ xj to accumulator
End For
xk Ð pbk ´ sumsq{�kk \ Solve xk from kth equation
End For
End Module FORW_SUBSTITUTE
x1 “ b1
�11
, xk “ 1
�kk ˜
bk ´
k
ÿ´1
j“1
�kjxj
¸
, k “ 2, 3,...,n (2.24)
A pseudomodule, FORW_SUBSTITUTE, employing the forward substitution algorithm
to solve a lower triangular system is given in Pseudocode 2.7. The module requires the
number of equations (n), a lower triangular matrix (L), and the rhs vector (b) as input
and places the solution on x as the output. Solving for x1 is straightforward: x1 “ b1{�11.
Solving xk for k “ 2, 3,...,n is carried out with the outer For-loop over the index variable
k until all unknowns are found. The summation term in Eq. (2.24) is denoted by sum
and obtained with the inner For-loop-j. Finally, the solution for the kth unknown is set to
xk “ pbk ´ sumq{�kk.
An upper triangular system, Ux “ b, has a coefficient matrix that is an upper triangular
matrix:
»
—
—
—
—
—
–
u11 u12 u13 ¨¨¨ u1n
0 u22 u23 ¨¨¨ u2n
0 0 u33 ¨¨¨ u3n
.
.
. .
.
. .
.
. ... .
.
.
000 ¨¨¨ unn
fi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
–
x1
x2
x3
.
.
.
xn
fi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
–
b1
b2
b3
.
.
.
bn
fi
ffi
ffi
ffi
ffi
ffi
fl
(2.25)
where U denotes an upper triangular matrix throughout the text.
The system is explicitly expressed as
u11x1 ` u12x2 ` u13x3 `¨¨¨` u1nxn “ b1
u22x2 ` u23x3 `¨¨¨` u2nxn “ b2
u33x3 `¨¨¨` u3nxn “ b3
... .
.
. .
.
.
unnxn “ bn
(2.26)
If the system is non-singular (uii ‰ 0 for all i), the unknowns can be easily found
by applying the back substitution algorithm. Unlike forward substitution, the algorithm
starts from the last equation and progresses toward the first equation. Note that in this
case, xn is readily available from the nth equation: xn “ bn{unn. Once this solution of92  Numerical Methods for Scientists and Engineers
FIGURE 2.3: Illustration of the back substitution procedure. Note: The boxes
denote allocated memory for xi’s and bi’s, and symbols denote non-zero values.
xn is secured, it is substituted in the above equation to obtain xn´1, yielding xn´1 “
pbn´1´un´1,nxnq{un´1,n´1. The rest of the unknowns are obtained sequentially by sweeping
the equations from the bottom row (equation) to the top and solving one equation with one
unknown at each step, as illustrated in Fig. 2.3. The equation for the kth row is generalized
as
ukkxk ` uk,k`1xk`1 ` uk,k`2xk`2 `¨¨¨` uknxn “ bk
where the term enclosed in the box is “known” due to the solutions found in previous steps,
and therefore the only unknown is xk. Solving this for xk, the generalized solution for the
back substitution algorithm is obtained as
xn “ bn
unn
, xk “ 1
ukk
´
bk ´ ÿn
j“k`1
ukjxj
¯
, k “ pn ´ 1q,pn ´ 2q,..., 2, 1 (2.27)
A pseudomodule, BACK_SUBSTITUTE, employing the back substitution algorithm to
obtain the solution of an upper triangular system of linear equations is presented in Pseu￾docode 2.8. The module requires the number of equations (n), an upper triangular matrix
(U), and the rhs vector (b) as input, and on exit, the solution is saved on x. xn is easily
found as xn “ bn{unn. We obtain xn´1 after substituting xn into the pn´1qth equation and
solving for it. The xk is obtained by back substituting all available xi’s (i “ n,pn´1q,...,k)
into the kth equation. Then the summation term in Eq. (2.27), denoted by sum, is calcu￾lated using an accumulator (For-loop over j). Finally, the solution for the kth unknown is
obtained as xk “ pbk ´ sumq{�kk. This procedure is carried out sequentially using a For
loop over k from k “ pn ´ 1q until the first equation (k “ 1) is reached.
Pseudocode 2.8
Module BACK_SUBSTITUTE (n,U,b, x)
\ DESCRIPTION: A pseudomodule to solve Ux “ b by back substitution.
Declare: unn, xn, bn \ Declare array variables
xn Ð bn{unn \ Solve xn from last equation
For “
k “ pn ´ 1q, 1,p´1q
‰ \ Loop k: Sweep equations from bottom to top
sums Ð 0 \ Initialize accumulator variable, sums
For “
j “ pk ` 1q, n‰ \ Loop j: Accumulation terms for j “ pk ` 1q,...,n
sums Ð sums ` ukj ˚ xj \ Adde ukj ˚ xj to accumulator
End For
xk Ð pbk ´ sumsq{ukk \ Solve xk from kth equation
End For
End Module BACK_SUBSTITUTELinear Systems: Fundamentals and Direct Methods  93
FIGURE 2.4: (a) Row echelon (Ux “ b1
), (b) reduced row echelon forms (Ix “ b2). Symbols b and
b denote the modified elements of A and b, respectively, and any two identical symbols do not
imply equality of numbers.
Any linear system that has a unique solution can be transformed
to an equivalent upper triangular system (Ux “ b) through a set
of row reductions. The purpose of such transformations is to make
use of the simplicity and speed that the back substitution algo￾rithm provides.
2.5 GAUSS ELIMINATION METHODS
Gauss elimination and its variants are the direct methods of solving a system of linear
equations. The method is considered one of the most efficient and practical in this field. It
requires a row reduction algorithm that consists of a sequence of elementary row operations
applied to the augmented matrix formed by adjoining A “ raij snˆn and b “ rbisn as
rA |bsnˆpn`1q “
»
—
—
—
—
—
–
a11 a12 a13 ¨¨¨ a1n
a21 a22 a23 ¨¨¨ a2n
a31 a32 a33 ¨¨¨ a3n
.
.
. .
.
. .
.
. ... .
.
.
an1 an2 an3 ¨¨¨ ann
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
b1
b2
b3
.
.
.
bn
fi
ffi
ffi
ffi
ffi
ffi
fl
nˆpn`1q
(2.28)
where the augmented matrix has n rows (each corresponding to an equation) and pn ` 1q
columns. A vertical line separates the coefficient matrix from the rhs.
2.5.1 NAIVE GAUSS ELIMINATION
This technique consists of two phases: (1) forward elimination, (2) back substitution. The
aim of the forward elimination step is to transform matrix A through a series of elementary
row operations into an upper triangular matrix (U). The rhs vector b is also simultaneously
modified. This process results in an upper triangular system of equations: Ux “ b1 (see Fig.
2.4a). Finally, the upper triangular system is solved using the back substitution algorithm
(Section 2.4).
The method is identical to that of the Gauss-Jordan row reduction used to invert
a matrix. The row reduction procedure is employed to create a system of equations in
either Ux “ b1 or Ix “ b2 form (see Fig. 2.4). Each elementary row operation modifies the
elements of both A and b as the algorithm is implemented. So, from this point forward, we
will use appq superscript notation to indicate that an element has been modified p times.
The procedure begins with scaling the first row with a11 (r
p1q
1 Ð r1{a11), so all the
elements in the first row are modified as ap1q
1,m Ð val ˚ a1,m and b
p1q
1 Ð val ˚ b1 for m “
1, 2,...,n, where val “ 1{a11.94  Numerical Methods for Scientists and Engineers
»
—
—
—
—
—
–
1 ap1q
12 ap1q
13 ¨¨¨ ap1q
1n
a21 a22 a23 ¨¨¨ a2n
a31 a32 a33 ¨¨¨ a3n
.
.
. .
.
. .
.
. ... .
.
.
an1 an2 an3 ¨¨¨ ann
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
b
p1q
1
b2
b3
.
.
.
bn
fi
ffi
ffi
ffi
ffi
ffi
fl
r
p1q
1 Ð val ˚ r1
Using the first row as a pivot, the elements in the first column below a11 (ai1’s for
i “ 2, 3,...n) are eliminated by r
p1q
i Ð ri ´ s ˚ r
p1q
1 operations that yield:
ap1q
ik “ aik ´ s ˚ ap1q
1k and b
p1q
i “ bi ´ s ˚ b
p1q
1 , i, k “ 2, 3,...,n
where s “ ai1 serves to reduce cpu-time. After the first pass, the augment matrix becomes:
»
—
—
—
—
—
—
–
1 ap1q
12 ap1q
13 ¨¨¨ ap1q
1n
0 ap1q
22 ap1q
23 ¨¨¨ ap1q
2n
0 ap1q
32 ap1q
33 ¨¨¨ ap1q
3n
.
.
. .
.
. .
.
. ... .
.
.
0 ap1q
n2 ap1q
n3 ¨¨¨ ap1q nn
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
b
p1q
1
b
p1q
2
b
p1q
3
.
.
.
b
p1q n
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
r
p1q
2 Ð r2 ´ a21r
p1q
1
r
p1q
3 Ð r3 ´ a31r
p1q
1
.
.
.
r
p1q n Ð rn ´ an1r
p1q
1
Note that row operations are encoded to the right of the augmented matrix, in the row
where the elementary row operation is carried out.
The scaling (normalization) procedure is not a requirement in
Gauss elimination or Gauss-Jordan elimination methods; however,
it does help minimize round-off errors.
The second pass also begins with scaling the second row: r
p2q
2 Ð val ˚ r
p1q
2 , where
val “ 1{ap1q
22 (i.e., ap2q
2k Ð val ˚ ap1q
2k and b
p2q
2 Ð val ˚ b
p1q
2 for k “ 2, 3,...,n). Then, using
the second row as the pivot, all elements in the second column below ap1q
22 are eliminated by
r
p2q
i Ð r
p1q
i ´ s ˚ r
p2q
2 for i “ 3, 4,...,n, which yield
ap2q
ik “ ap1q
ik ´ s ˚ ap2q
2k and b
p2q
i “ b
p1q
i ´ s ˚ b
p2q
2 , for i, k “ 3, 4,...,n
where s “ ap1q
i2 .
Upon completing the second pass, the augmented matrix becomes
»
—
—
—
—
—
—
–
1 ap1q
12 ap1q
13 ¨¨¨ ap1q
1n
0 1 ap2q
23 ¨¨¨ ap2q
2n
0 0 ap2q
33 ¨¨¨ ap2q
3n
.
.
. .
.
. .
.
. ... .
.
.
0 0 ap2q
n3 ¨¨¨ ap2q nn
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
b
p1q
1
b
p2q
2
b
p2q
3
.
.
.
b
p2q n
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
r
p2q
2 Ð val ˚ r
p1q
2
r
p2q
3 Ð r
p1q
3 ´ ap1q
32 r
p2q
2
.
.
.
r
p2q n Ð r
p1q n ´ ap1q
n2 r
p2q
2
The third, fourth, and so on passes are similarly carried out first by applying the
normalization (scaling) and then elimination procedures, as illustrated earlier, which can
be generalized as r
pjq
i “ r
pj´1q
i ´ r
pjq
k apjq
ik for an ith row, leading to
apjq
j,m “val ˚ apj´1q
j,m , bpjq
j “val ˚ b
pj´1q
j , val“1{apj´1q
jj , j, m“1, 2,...,n
apjq
ik “apj´1q
ik ´ s ˚ apjq
jk and b
pjq
i “b
pj´1q
i ´ s ˚ b
pjq
j , s“apj´1q
ij , i, k“ pj ` 1q,...,nLinear Systems: Fundamentals and Direct Methods  95
FIGURE 2.5: Illustration of the naive Gauss elimination procedure without pivoting (‚
and b denote the modified (computed) and unmodified (to be modified) elements of A
and b, respectively). Any two identical symbols do not imply equality of numbers.
The forward elimination procedure is schematically illustrated in Fig. 2.5. Due to scal￾ing, the principal diagonal elements of the processed rows consist of “1”s. The elements
below “1”s are zeroed during the forward elimination procedure, but in practice, these com￾putations that should yield zero are not performed, which would otherwise be a waste of
cpu-time.
After n passes, the coefficient matrix is reduced to an upper triangular matrix:
»
—
—
—
—
—
—
–
1 ap1q
12 ap1q
13 ¨¨¨ ap1q
1n
0 1 ap2q
23 ¨¨¨ ap2q
2n
00 1 ¨¨¨ ap2q
3n
.
.
. .
.
. .
.
. ... .
.
.
00 0 ¨¨¨ 1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
b
p1q
1
b
p2q
2
b
p3q
3
.
.
.
b
pnq n
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl r
pnq n Ð val ˚ r
pnq n
The procedure is completed when the back substitution algorithm is successfully applied
to the upper triangular matrix, as outlined in Section 2.4. Nevertheless, notice that this
algorithm assumed |api´1q
ii | ą 0, which does not give “division by zero” error, which is why
it is referred to as “naive Gauss Elimination.” The method will clearly fail if, at least, one
of the diagonal values is zero (|api´1q
ii | “ 0). Having said that, it is possible to avoid this
problem by changing the order of the equations using a technique called pivoting, which is
also covered in Section 2.5.3.
The memory requirement and the operation count for a full system Gauss elimination
method are on the order of Opn2q and Opn3q, respectively.
2.5.1.1 Application to underdetermined systems
A linear system is said to be underdetermined when A “ raij smˆn is a rectangular ma￾trix, i.e., the number of equations is less than the number of unknowns (m ă n). An
underdetermined linear system can have either no solution, one solution, or infinitely many
solutions. Consider, for example, a linear system consisting of one equation and two un￾knowns: a`2b “ 8, i.e., m “ 1 and n “ 2. This simple system has infinitely many solutions,96  Numerical Methods for Scientists and Engineers
Gauss elimination Method
‚ It is a reliable method that gives the true solution if no round-off
errors are made;
‚ It requires much less computation in comparison to finding A´1;
‚ The coefficient matrix need not be diagonally dominant;
‚ Pivoting is not needed if A is symmetric, positive-definite, or diago￾nally dominant;
‚ It is also applied to ill-conditioned systems;
‚ It may be used to compute the rank of a matrix;
‚ The determinant of a matrix can be found simultaneously while per￾forming forward elimination step.
‚ This method is impractical for sparse or very large systems due to
the propagation of rounding errors and excessive cpu-time;
‚ When no pivoting strategy is employed and aii becomes zero in at
least one row, a “division by zero” will be encountered during the
forward elimination step;
‚ When no pivoting strategy is employed and aii becomes very small
(aii « 0) in at least one row, a “loss of significance” may be encoun￾tered;
‚ Every pass (solution step) depends on the previous pass, which makes
the method is highly susceptible to round-off errors;
‚ The accuracy of the solution improves as the number of significant
digits increases, but this does not completely eliminate the round-off
errors.
which can be obtained by recasting it as a “ 8 ´ 2b. We can then find a solution for a
(i.e., basic variable) in terms of an arbitrary b (i.e., free or independent variable). In an
underdetermined system, any m of the n unknowns can be selected as basic variables, but
the choice of the basic variables is not usually evident. The Gauss elimination method (or
preferably the Gauss-Jordan method, in Section 2.5.2) may be utilized to pick out the basic
variables.
2.5.1.2 Determing the rank of a matrix.
If a linear system of equations is homogeneous (i.e., Ax “ 0) and also det(A)=0 (if the
rank of A is less than n), the system Ax “ 0 will have r basic and pn ´ rq free variables.
In such a case, a non-trivial (non-zero) solution can be found. In other words, any set of r
variables could then be expressed in terms of free variables. Like underdetermined systems,
the Gauss-Jordan elimination method may be employed to obtain the solution in terms of
free variables.
2.5.2 GAUSS-JORDAN ELIMINATION METHOD
The Gauss-Jordan Elimination method, which is similar to the Gaussian elimination, trans￾forms the augmented matrix into the reduced row-echelon form (Fig. 2.4b). It requires, in
addition to the Gauss elimination procedure, the elimination of the elements above the pivot
(i.e., diagonal). The normalization of the pivot row is a requirement in this method, while
in the naive Gauss elimination method it is not.Linear Systems: Fundamentals and Direct Methods  97
Gauss-Jordan Elimination Method
‚ Gauss-Jordan elimination is very stable when a pivoting technique is
also employed;
‚ The algorithm can be applied to an augmented matrix ([A||I]) to find
the inverse matrix of a matrix, A.
‚ If a pivot element is very small (aii «0), a “loss of significance” and
“stability” issues associated with roundoffs may be encountered;
‚ A “division by zero” may be encountered, even if the system has a
unique solution;
‚ The method requires more arithmetic operations (on the order of
Opn3q about 50% more) than Gauss elimination or LU decomposition
methods.
In the Naive Gauss elimination algorithm presented in Section 2.5.1, the normalization
of pivot elements was carried out as a standard part of the procedure. Therefore, it will
be sufficient to zero out the elements above diagonal through elementary row operations.
Nonetheless, there is no need to carry out the arithmetic operations with the elements
above the diagonal to save cpu-time, as these will eventually become zero. The elimination
procedure is only applied to the right-hand side vector, leading to
b
pkq
i “b
pk´1q
i ´apk´1q
ij b
pkq
j , j “ n,pn´2q,..., 2, i“ pj´1qpj´2q,..., 1 (2.29)
where k denotes the elimination step (k ą n).
The final form of the system becomes
»
—
—
—
—
—
—
–
10 0 ¨¨¨ 0
01 0 ¨¨¨ 0
.
.
. .
.
. ... ... .
.
.
0 0 ¨¨¨ 1 0
0 0 ¨¨¨ 0 1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
b
pn`k`1q
1
b
pn`kq
2
b
p3q
3
.
.
.
b
pnq n
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(2.30)
where the last column of the augmented matrix is the solution vector, xi “ bi for all i.
A pseudomodule, LINEAR_SOLVE, for solving a system of linear equations with the
naive Gauss elimination (opt “ 0) or Gauss-Jordan elimination (opt ‰ 0) is presented in
Pseudocode 2.9. The number of equations (n), coefficient matrix (A), rhs vector (b), and
method flag (opt) are required as input. On exit, solution is stored on x. The elimination
procedure is carried out so long as |aij | ą 0; otherwise, the procedure is terminated with
a warning message. In this module, the normalization of the pivot row and elimination of
elements below the pivot are carried out inside the loop´j (outermost loop) in column-by￾column order from left to right. The module requires a small number (�), either internally
defined or set to machine epsilon, to check whether the diagonal values of the matrix are
zero. As a cpu saving measure, before scaling the jth row, the normalization factor is set
to val Ð 1{ajj once, and it is applied to scale the rhs (bj Ð val ˚ bj ) and non-zero elements
(ajm Ð val ˚ ajm for m “ n,pn ` 1q,...,n) of the pivot row. The loop´i ranges down
the jth column just below the diagonal element (from i “ j ` 1) all the way to the last
row (i “ n), where the pivot is assigned s “ aij and the elements are set to zero without
performing the actual computations (aij “ 0) to reduce cpu-time.98  Numerical Methods for Scientists and Engineers
Pseudocode 2.9
Module LINEAR_SOLVE (n,A, b, x,opt)
\ DESCRIPTION: A pseudomodule to solve Ax “ b by Naive Gauss
\ elimination or Gauss-Jordan method without pivoting.
\ USES:
\ BACK_SUBSTITUTE :: Module for back substitution, see Pseudocode 2.8;
\ ABS :: A built-in function computing the absolute value.
Declare: ann, xn, bn \ Declare array variables
ε Ð 10´12 \ Define a small number or get Machine epsilon
For “
j “ 1, n‰ \ Loop j: Sweep row from left to right
ajj Ð ajj \ Isolate the pivot element
If “
|ajj| ă ε
‰
Then \ Check for possible “division by zero”
Write: “Pivot row is zero ”,j \ Warning! Matrix may be singular
Exit \ Exit the loop
Else \ Case of non-zero pivot, aij ‰ 0
val Ð 1{ajj \ Set normalization (scaling) factor
bj Ð val ˚ bj \ Scale rhs of the pivot row
For “
m “ j, n‰ \ Loop m: Skip zeros on the left, m ă j
ajm Ð val ˚ ajm \ Scaling pivot row of m ě j
End For
For “
i “ pj ` 1q, n‰ \ Loop i: Eliminate elements below ith row
s Ð aij \ Save pivot on s
aij Ð 0 \ Set 0 for zeroed element, no need to compute
For “
k “ pj ` 1q, n‰ \ Loop k: Elimination for the ith row
aik Ð aik ´ s ˚ ajk \ Modify elements of kth row
End For
bi Ð bi ´ s ˚ bj \ Modify the rhs of ith row
End For
End If
End For
If “
opt=0‰
Then \ opt “ 0, Naive Gauss elimination solution
BACK_SUBSTITUTE(n,A, b, x) \ Goto back substitution module
Else \ opt ‰ 0, Eliminate above diagonal elements
For “
j “ n, 2,p´1q
‰ \ Loop j: Sweep rows right to left
For “
i “ pj ´ 1q, 1,p´1q
‰ \ Loop i: Sweep from bottom to top
bi Ð bi ´ aij ˚ bj \ Perform operations only on rhs
End For
xj Ð bj \ x “ b since A is now I
End For
x1 Ð b1 \ Since a11 “ 1 and a1j “ 0
End If
End Module LINEAR_SOLVE
The inner loop over k-index variable ranges across the ith row, starting after the pivot
element all the way to the last element in the row, modifying each element appropriately
(aik Ð aik ´ s ˚ ajk) and the row reductions are completed with (bi Ð bi ´ s ˚ bj ). At the
end, after n steps, the matrix A is transformed to the equivalent matrix Apnq
, which is in
the row-echelon form:Linear Systems: Fundamentals and Direct Methods  99
»
—
—
—
—
—
—
–
1 ap1q
12 ap1q
13 ¨¨¨ ap1q
1n
0 1 ap2q
23 ¨¨¨ ap2q
2n
00 1 ¨¨¨ ap3q
3n
.
.
. .
.
. .
.
. ... .
.
.
00 0 ¨¨¨ 1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
b
p1q
1
b
p2q
2
b
p3q
3
.
.
.
b
pnq n
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
In the case of the Gauss elimination method (opt “ 0), the solution set is obtained by
the BACK_SUBSTITUTE module, which implements the back substitution algorithm (see
Section 2.5) since the system is in the upper triangular form.
In the case of the Gauss-Jordan elimination method (opt ‰ 0), there is practically no
need to employ row reductions to the elements above the diagonals (aij ’s for j ą i) since
they are eventually eliminated. Thus, it is sufficient to carry out the required eliminations
only on the rhs vector (bi Ð bi ´ aij bj ) to save cpu-time. The row reductions are applied
from the bottom row (i “ j ´1 above diagonal) to the top (j “ 1) and from the last (j “ n)
to the second column (j “ 2). The first column is already x1 “ b1.
EXAMPLE 2.5: Application of Gauss elimination Method
Ethane (C2H6) burns in oxygen to form carbon dioxide (CO2) and water (H2O)
according to the equation:
x1C2H6 ` x2O2 Ñ x3CO2 ` x4H2O
Use the row reductions to balance the chemical reaction in question and demonstrate
that the resulting is an underdetermined homogeneous system that has a nontrivial
solution.
SOLUTION:
First, we balance the number of Carbon (C), Hydrogen (H), and Oxygen (O)
atoms on each side.
C ::
H ::
O ::
x1
»
–
2
6
0
fi
fl ` x2
»
–
0
0
2
fi
fl “ x3
»
–
1
0
2
fi
fl ` x4
»
–
0
2
1
fi
fl
Our task is to find positive integers (x1, x2, x3, x4) that balance the reaction. Ex￾pressing all equations in matrix equation form (Ax “ 0), we obtain
»
–
2 0 ´1 0
60 0 ´2
0 2 ´2 ´1
fi
fl
»
—
—
–
x1
x2
x3
x4
fi
ffi
ffi
fl “
»
—
—
–
0
0
0
0
fi
ffi
ffi
fl
This system is underdetermined and homogeneous as well. To reduce it into row￾echelon form, the augmented matrix is established as
rA | 0 s3ˆ5 “
»
–
2 0 ´1 0
60 0 ´2
0 2 ´2 ´1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
0
0
0
fi
fl100  Numerical Methods for Scientists and Engineers
Now we can begin the elimination procedure for the augmented matrix. As a first
step, the first and second rows are normalized as follows:
„
»
—
–
1 0 ´1
2 0
10 0 ´1
3
0 2 ´2 ´1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
0
0
0
fi
ffi
fl
Ð 1
2 r1
Ð 1
6 r2
Next, the second element of the first column is eliminated by adding (´r1) to r2 in
place of Row-2:
„
»
—
–
1 0 ´1
2 0
0 0 1
2 ´1
3
0 2 ´2 ´1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
0
0
0
fi
ffi
flÐ r2 ´ r1
Interchanging Row-2 with (1/2)ˆRow-3 results in
„
»
—
—
–
1 0 ´1
2 0
0 1 ´1 ´1
2
0 0 1
2 ´1
3
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
0
0
0
fi
ffi
ffi
fl
r2 Õ 1
2 r3
The third row is multiplied with “2” to obtain a33 “ 1:
„
»
—
—
–
1 0 ´1
2 0
0 1 ´1 ´1
2
00 1 ´2
3
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
0
0
0
fi
ffi
ffi
fl
Ð 2r3
Finally, eliminating a13 and a23 yields
„
»
—
–
100 ´1
3
010 ´7
6
001 ´2
3
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
0
0
0
fi
ffi
fl
Ð r1 ` 1
2 r3
Ð r2 ` r3
which is equivalent to x1 ´ x4{3 “ 0, x2 ´ 7x4{6 “ 0, x3 ´ 2x4{3 “ 0. The basic
(leading) variables are x1, x2 and x3, and x4 is the free (independent) variable.
Setting x4 “ u, we obtain the general solution as x1 “ u{3, x2 “ 7u{6, x3 “ 2u{3.
Hence, a nontrivial solution can be found for any real u. For u “ 6, we obtain x1 “ 2,
x2 “ 7, and x3 “ 4.
Discussion: Homogeneous underdetermined linear systems always have an infinite
number of nontrivial solutions, in addition to the trivial solution (x “ 0). It is pos￾sible to obtain expressions for all solutions to an underdetermined system. Which
one is “better” is a value judgment. In this problem, we have chosen u such that the
solution is in integer form. However, it should be emphasized that even though the
column vector has been augmented in this example, it is not necessary to explicitly
augment the coefficient matrix with the column vector b “ 0 since no basic row
operations will affect the zeros on the rhs.
When dealing with underdetermined systems, it may be very difficult to identify
at a glance the leading and free variables of an underdetermined system; however,
as in this case, the Gauss-Jordan elimination procedure is an important tool to
determine the free and basic variables.Linear Systems: Fundamentals and Direct Methods  101
2.5.3 GAUSS-ELIMINATION WITH PIVOTING
Diagonal elements of a coefficient matrix play a vital role in implementing the Gauss elim￾ination algorithm. The elements on the principle diagonal are called the pivots or pivot
elements because of their significance in this regard. As we have discussed earlier, in order
to avoid “division by zero,” it is critical to know if a pivot element is or becomes zero at any
step of the elimination procedure. Pivoting is the process of interchanging columns (order
of unknowns) or rows (order of equations) to avoid having zero as the pivot element.
We have learned that the naive Gauss elimination does not employ pivoting. If any one
of the pivots turns out to be zero during the row reduction process, a “division by zero”
is encountered. In this case, the method is doomed to fail even if the coefficient matrix is
non-singular or the linear system has a unique solution. Consider, for instance, the following
coefficient matrix:
„
0 3
2 ´2
j
Note that the matrix is non-singular; nonetheless, we cannot initiate the elimination pro￾cedure because val “ 1{0 is encountered in the first pass. But this problem can be avoided
simply by changing the first and second lines, as illustrated below:
„
2 ´2
0 3 j r1 Õ r2
Now the pivots of the equivalent matrix are no longer zero (a11 ‰ 0 or a22 ‰ 0), which
allows the elimination procedure to be implemented successfully.
There are several pivoting strategies. In complete pivoting, both rows and columns are
interchanged, whereas only the rows are interchanged in the partial pivoting process. The
partial pivoting covered in this section is faster and least expensive. This technique requires
only row interchanges to push the zero element off the main diagonal. Even though partial
pivoting is more efficient and easier, it may still yield some inaccuracies in the final solution.
Overall, the pivoting strategy is based on finding the largest element in the same column
below the current diagonal element, which is considered the pivot, and swapping it with the
current pivot row. This strategy helps prevent a division by zero or a very small number,
which could lead to numerical instability. Although not required, scaling can be applied to
the pivot rows selected, which leads to an algorithm that is less susceptible to round-off
errors.
Gauss (-Jordan) elimination with Pivoting
‚ Gauss-Jordan or Gauss elimination methods with pivoting avoid pos￾sible divisions by zero;
‚ It minimizes round-off errors by using the largest elements as pivots;
‚ It does offer, in some cases, a remedy for solving ill-conditioned sys￾tems of linear equations.
‚ Full or partial pivoting leads to an increase in cpu-time due to search￾ing for the largest element in a row or column, but partial pivoting
requires less cpu as it needs fewer interchanges;
‚ The symmetry or regularity properties of the original matrix may be
lost;
‚ It is computationally expensive for very large linear systems.102  Numerical Methods for Scientists and Engineers
Pseudocode 2.10
Module GAUSS_ELIMINATION_P (n,A, b, x)
\ DESCRIPTION: A pseudomodule to solve a system of linear equations
\ (Ax “ b) by naive Gauss elimination with pivoting.
\ USES:
\ BACK_SUBSTITUTE :: Module back substitution (Pseudocode 2.8);
\ ABS :: A built-in function computing the absolute value.
Declare: ann, xn, bn \ Declare matrices as arrays
For “
j “ 1,pn ´ 1q
‰ \ Loop j: Sweep row from left to right
valmax Ð |ajj | \ Set pivot as absolute max value
pos Ð j \ Find position of pivot
For “
i “ pj ` 1q, n‰ \ Loop i: Search for a greater element below ajj
If “
|aij | ą valmax‰
Then \ If a greater value found
valmax Ð |aij | \ Set ajj as max value
pos Ð i \ Update position of pivot
End If
End For
If “
pos ą j
‰
Then \ A row of greater magnitude found
For “
k “ j, n‰ \ Loop k: Swap rows rowpos Ø rowj
temp Ð ajk \ Use temp for swaping rows
ajk Ð apos,k
apos,k Ð temp
End For
temp Ð bj \ Use a temp for swaping rhs
bj Ð bpos
bpos Ð temp
End If \ Pivoting for jth column is complete
For “
i “ pj ` 1q, n‰ \ Loop i: Perform forward elimination
r Ð aij {ajj \ Set normalization factor
aij Ð 0 \ Set 0 for zeroed element, no need to compute
For “
k “ pj ` 1q, n‰ \ Loop k: Perform forward elimination
aik Ð aik ´ r ˚ ajk \ Row op. for kth row
End For
bi Ð bi ´ r ˚ bj \ Row op. for rhs of ith row
End For
End For
BACK_SUBSTITUTE(n,A,b,x) \ Apply back substitution to get solution
End Module GAUSS_ELIMINATION_P
A pseudomodule, GAUSS_ELIMINATION_P, for applying Gauss elimination with partial
pivoting to solve a system of linear equations is provided in Pseudocode 2.10. The number
of equations (n), the coefficient matrix (A), and the rhs (b) are required as input. On exit,
the solution is stored on x. The basic structure of the program is the same as that of the
module LINEAR_SOLVE with one fundamental difference, which requires interchanging the
row with a small (or zero) pivot with the largest one of the rows below it. Afterwards, the
usual forward elimination phase is performed.
The pivoting strategy is illustrated in Fig. 2.6. At the beginning of the loop over row-j,
the default pivot and its position are initialized with these of the principal diagonal element
and its corresponding row number (valmax “ |ajj | and pos “ j). With the For-loop over i,Linear Systems: Fundamentals and Direct Methods  103
FIGURE 2.6: Illustrating the Gauss elimination procedure with pivoting (‚ and b denote the mod￾ified (computed) and unmodified (to be updated) elements of A and b, respectively).
the rows below the pivot (from i “ pj ` 1q to n) are visited to find a row with the largest
magnitude. When such a row is found, its magnitude and row number are set as current
ones (valmax “ aij and pos “ i), respectively. Whenever a different row with a larger
magnitude than the current pivot is found, the current row (rj ) is swapped with the row
with the largest magnitude (rpos) using an If-construction with a pos ą j condition. Note
that the contents of the jth row are rj “ tajj , aj,j`1,...,ajn, bj u.
The pivoting procedure is carried out column by column from left (j “ 1) to right
j “ n inside the outermost loop´j, in each row from left (i “ j ` 1) to right (n), and from
top (k “ j ` 1) to bottom (n). The scaling factor is set to r Ð aij {ajj once and is used in
the forward elimination of the rows in the loop´k. The loop´i ranges just below the pivot
element (from i “ j ` 1) all the way to the last row (i “ n), and the elements are zeroed
without actually performing the calculations (aij “ 0) to reduce the cpu-time. The inner
loop over k variable ranges across the ith row, starting after the pivot element all the way
to the last element in the row, modifying each element appropriately (aik Ð aik ´ r ˚ ajk
for i, k “ pj ` 1q,...,n), and the row reductions are completed with (bi Ð bi ´ r ˚ bj ). At
the end, the matrix A is transformed to an equivalent matrix Apnq
, which is in row-echelon
form. Once the forward elimination procedure is complete, the back substitution procedure
is applied to obtain the solution.
EXAMPLE 2.6: Application of Elimination Methods
The given circuit consists of three loops and
loop currents denoted by i1, i2 and i3. The
Kirchoff’s current law for the circuit loops
yields
i1 ` 4i2 ´ 6i3 “ 0
´i1 ` 6i2 ´ 4i3 “ 60
4i1 ´ i2 ´ i3 “ 0
Solve the given system for the loop currents with the Gauss elimination with pivoting.104  Numerical Methods for Scientists and Engineers
SOLUTION:
The linear system is first expressed in Ax “ b form and construct the augmented
matrix:
»
–
1 4 ´6
´1 6 ´4
4 ´1 ´1
fi
fl
»
–
i1
i2
i3
fi
fl“
»
–
0
60
0
fi
fl , rA | bs“
»
–
1 4 ´6
´1 6 ´4
4 ´1 ´1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
0
60
0
fi
fl
We start with the first column. The first row is interchanged with the third row,
which has the largest magnitude element (a31 “ 4) in this column, and then the first
row is normalized as follows:
»
–
4 ´1 ´1
´1 6 ´4
1 4 ´6
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
0
60
0
fi
fl
r3 Ø r1
„
»
–
1 ´1{4 ´1{4
´1 6 ´4
1 4 ´6
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
0
60
0
fi
fl
Ð p1{4qr1
Next, we proceed to eliminate the elements below a11 as follows:
„
»
–
1 ´1{4 ´1{4
0 23{4 ´17{4
0 17{4 ´23{4
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
0
60
0
fi
flÐ r2 ` r1
Ð r3 ´ r1
Note that the elementary row operations are encoded on the rhs of the augmented
matrix, aligned with the modified row.
We observe that a22 is the largest element in the second column, so we will keep
it as the pivot. Next, we normalize the second row (as r2 Ð 4r2{23) and go on to
eliminate the element below the pivot (i.e., a23).
„
»
–
1 ´1{4 ´1{4
0 1 ´17{23
0 0 ´60{23
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
0
240{23
´1020{23
fi
fl
Ð r3 ´ p17{4qr2
To complete the elimination process, the third row is scaled with a33.
„
»
–
1 ´1{4 ´1{4
0 1 ´17{23
00 1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
0
240{23
17
fi
fl ” rA | bs
Finally, the coefficient matrix has now been transformed into an upper triangular
matrix. The equivalent system of linear equations is
i1 ` 4i2 ´ 6i3 “ 0, i2 ´ i3 “ 6, i3 “ 17
Applying the back substitution algorithm to find the currents, we note that i3 is
readily available from the last equation: i3 “ 17 A. Substituting this into the second
equation and solving for i2 gives i2 “ 23 A. Finally, substituting both i2 and i3 into
the first equation and solving for i1 yields i1 “ 10 A.
Discussion: The Gauss elimination method with exact arithmetic gives the true
solution. The Gauss-Jordan method additionally requires the elimination of the el￾ements above the diagonals, which increases the cpu-time. The advantage of the
Gauss elimination method over the Gauss-Jordan method is more apparent in large
systems of equations.Linear Systems: Fundamentals and Direct Methods  105
2.6 COMPUTING DETERMINANTS
Using the cofactor expansion method to evaluate the determinant of an n ˆ n matrix re￾quires about n ˆ n! arithmetic operations. For this reason, computing the determinant of a
large matrix via cofactor expansion is impractical due to its immense cpu-time demand. A
determinant, however, can be computed efficiently as the product of the diagonal elements
once it is transformed into an upper triangular or row-echelon form.
In order to compute the determinant of a matrix quickly and efficiently, a variant
of the Gauss elimination algorithm can be devised to transform the matrix to an upper
triangular form. However, the elementary row operations must be used with caution. This
Gauss elimination procedure works exactly as indicated above if neither pivoting nor scaling
are used. But recall that both scaling and pivoting modify the original determinant in a
predictable manner. In this regard, these row operations can still be applied, provided that
suitable corrections are made. For instance, if a row (or column) is multiplied by a scalar
λ, the determinant variable must be divided by λ as well. On the other hand, when two
rows (ri and rj ) are interchanged, the sign of the determinant is changed if |i ´ j| is odd;
however, if |i ´ j| is even, no sign change is needed. This procedure requires the order of
Opn3q arithmetic operations to compute the determinant of an nˆn square matrix, which is
considerably less than the number of operations required by the cofactor expansion process.
The direct solution methods are not employed for solving systems
of linear equations with more than a few hundred unknowns. For
a system with fewer than about 100 unknowns, it is possible to
obtain accurate results using single-precision arithmetic. However,
in order to obtain reasonable results in systems with more than
about a hundred unknowns, it is recommended to use “double
precision” in arithmetic operations.
2.7 ILL-CONDITIONED MATRICES AND LINEAR SYSTEMS
A “well-conditioned” problem is one in which a small change in any of its numerical data
causes only a small change in the solution of the problem. On the other hand, an ill￾conditioned problem is one in which a small change in any of its numerical values causes
a large change in the solution of the problem. In other words, ill-conditioned systems are
extremely sensitive to round-off errors. But ill-conditioning is not a problem with the use
of fractions or, in theory, infinite precision arithmetic.
Ill-condition is observed when inverting matrices and solving a system of linear equa￾tions using calculators or digital computers. In the literature, there is no clear-cut definition
for “ill-condition.” It is a phenomenon that manifests itself as a result of round-off errors (or
small perturbations in the matrix elements) during arithmetic operations. An ill-conditioned
system of linear equations converges to a solution that deviates from the true solution. The
magnitudes of the deviations depend on how badly the system of equations or a matrix is
conditioned. In other words, the more severe the ill-condition, the greater the magnitude of
the deviations (from the true answer) will be.
When dealing with matrices or systems of linear equations, we cannot visually de￾termine if a matrix or a linear system is ill-conditioned. However, it is important for the
numerical analyst to determine whether or not an ill condition exists and how severe it is.106  Numerical Methods for Scientists and Engineers
The condition number, κpAq, is used as a measure of sensitivity of a matrix or system
to small changes (errors or perturbations) in any one of its elements, and it is defined as
κpAq “ AA´1 (2.31)
where ¨ denotes any one of the norms of a matrix. In other words, the numerical value of
κpAq depends on the type of norm (indicated by a corresponding subscript such as 1, 2, F,
or 8) from which it is calculated.
The numerical accuracy of the condition number computed by Eq. (2.31) also depends
on how accurately A´1 is computed. If A is severely ill-conditioned, estimating the condition
number by evaluating A´1 not only increases the computational cost significantly but also
raises questions regarding the accuracy of the result. Consequently, we should not use the
numerical value of the condition number that we cannot trust.
The condition number is easiest to calculate when the matrix A is a diagonal matrix
(A “ D), and it is found from
κpAq “ κpDq “
max
1ďiďn dii
min
1ďiďn dii
(2.32)
However, determining the condition number of a matrix directly from the definition requires
considerably more work than solving the actual linear system whose accuracy is to be
determined. That is why, in practice, the condition number is estimated to be within an
order of magnitude. A rough estimate for the condition number in terms of calculable
quantities is given as κpAq « AF {detpAq [84]. A large value in the condition number
is an indicative ill-conditioning.
Another similar but more accurate estimate for the spectral condition number was
proposed by Guggenheimmer et. al. [10]
κpAq « 2
|detpAq|˜
1
n
ÿn
i“1
ÿn
j“1
a2
ij¸n{2
" 1 (2.33)
where n is the order of matrix A, and det(A) can be computed easily and quickly during
the forward elimination procedure (see Section 2.6). The product of the diagonal elements
resulting in an upper triangular matrix gives the determinant of A, but the product should
be calculated at every pivot row before the row is normalized (if a normalization strategy
is employed).
Equation (2.33) is an upper bound for κpAq, and it may result in estimates much larger
than the actual condition number. Therefore, in order to minimize the round-offs, the linear
system of equations is normalized. This is achieved by either dividing each row by the largest
magnitude row element (i.e., ri8) or by transforming each row of the matrix into a unit
vector in Euclidean norm (i.e., ri2 “ 1). With this procedure, matrix A is transformed
into a matrix M, said to be equilibrated or row-balanced. Then Eq. (2.33) for M becomes
κpMq « 2
|detpMq| " 1 (2.34)
We now have p
ř ř m2
ij {nq
n{2 “ 1 for the equilibrated matrix. Ill-condition can then be
based on the magnitude of κpMq. For relatively small κpMq values, A is considered well￾conditioned, whereas very large values indicate severe ill-condition.Linear Systems: Fundamentals and Direct Methods  107
The numerical estimate can be improved while solving the system of equations. Let us
consider Ax “ b system whose solution is z [31]. Owing to properties of the norms, we may
write
z “ A´1b ď A´1b
which provides us a lower bound for A´1 ď z{b. If this bound is chosen to be as large
as possible, then A´1 can be reasonably estimated. However, finding an optimum value
for b could also be prohibitively expensive. Instead, we will adopt a heuristic approach that
requires solving the AT b “ u system, where u is a vector promoting growth in the solution,
and it is chosen such that each element is made up of (˘1)’s, i.e., ui “ ˘1, i “ 1, 2,...,n.
As a result, in order to estimate the condition number, the following linear systems are
solved [126]:
AT b “ u and Az “ b (2.35)
and use z{b as an estimate for A´1. We then obtain
κpAq«}A} }z}
}b} (2.36)
Clearly, the condition number depends on the type of norm. Note that it is easy to compute
A1 and A8, and A2 can be estimated using the Power algorithm (see Chapter 11).
Now, we turn our attention to the effects of ill-conditioning. To detect or assess the
severity of an ill-condition, there are two simple and effective tests that require the com￾putation of either AA´1 “ I or pA´1q´1 “ A. Any substantial deviation from the original
or expected outcome is considered evidence of an ill-conditioning. pA´1q´1 “ A is particu￾larly important to observe since matrix inversion is performed twice, leading to noteworthy
round-off accumulation. To answer the question of how badly the expected results will be
distorted, we use the following “heuristic rule” for solving Ax “ b: expect to lose at least
k-digits of precision, where k “ log κpAq.
The numerical treatment of ill-conditioned systems is an iterative procedure; for this
reason, a remedy for an ill-conditioned system is covered in Section 3.5.
It should be noted that the systems of linear equations obtained
as a result of numerical treatments of ordinary differential equa￾tions (ODEs) or partial differential equations (PDEs) are not ill￾conditioned. The ill-conditioning is most severe in linear systems
arising from least squares regressions.
EXAMPLE 2.7: Estimating Condition Number
Consider the following matrix:
A “
»
–
1 3 ´2
2.99 ´16.11 4
´3.01 1 2.03
fi
fl
(a) Use 1-, 2-, F-, and 8-norm to estimate the norm-based condition numbers of
matrix A and its equilibrated form M; (b) find the spectral condition number of
A using Eqs. (2.33) and (2.34); (c) compute AA´1 “ I and pA´1q ´1 “ A and
comment on the severity of ill-condition.108  Numerical Methods for Scientists and Engineers
SOLUTION:
(a) We select the starting vector u as u “ r1, ´1, 1s
T , and solve AT b “ u
»
–
1 2.99 ´3.01
3 ´16.11 1
´24 2.03
fi
fl
»
–
b1
b2
b3
fi
fl “
»
–
1
´1
1
fi
fl
which, with single precision, yields tb1, b2, b3u“t2122.34, 467.881, 1169.54u. Then
we solve Az “ b which gives tz1, z2, z3u“t3.488 ˆ 106, 1.721 ˆ 106, 4.324 ˆ 106u.
We can then estimate the condition number with compatible norms using Eq. (2.36).
TABLE 2.1
Norm True, κpAq }A} }z} }b} Estimated, κpAq
1 66798.6 20.110 9.533ˆ106 3759.76 50989.6
2 40510.4 17.191 5.816ˆ106 2468.01 40511.5
F 41668.6 17.682 5.816ˆ106 2468.01 41668.6
8 61659.5 23.101 4.324ˆ106 2122.34 47063.3
Norm True, κpMq }M} }z} }b} Estimated, κpMq
1 25136.6 2.0225 2.265ˆ108 20236.7 22636.7
2 15604.2 1.3585 1.382ˆ108 12030.5 15604.2
F 19895.0 1.7321 1.382ˆ108 12030.5 15604.2
8 23042.5 1.6039 1.027ˆ108 7941.07 20752.3
The condition numbers, κpAq and κpMq, based on 1-, 2-, F-, and 8-norms are
computed using the definition, i.e., Eq. (2.31), and these results along with the
estimated values from Eq. (2.36) are tabulated in Table 2.1. We notice that the
estimates for both κpMq and κpAq are smaller than the “true” condition number
values. However, a caution is in order here: if A were badly ill-conditioned, the
condition numbers computed by Eq. (2.31) would be questionable.
When we analyze the condition numbers of the equilibrated matrix, it is observed
that the orders of magnitude of the κpMq are about 40% of the κpAq. The fact that
the estimates for κpMq are also large verifies ill-conditioning. Evaluating log10 κpAq
suggests at least 4-5 digits of precision will be lost; on the other hand, log10 κpMq
predicts a loss of 3-4 digits of precision.
(b) For matrix A, we find |detpAq| “ 0.0302, and ř ř a2
ij “ 312.653. An estimate
for the spectral condition number is obtained from Eq. (2.33) as
κpAq « 2
0.0302ˆ312.653
3
˙3{2
“ 70458.8 " 1
Notice that this estimate, as an upper bound for the condition number, is larger
than κpAq “ 40510 value (see Table 2.1).
Next, we determine M by making each row of the matrix a unit vector in the
Euclidean norm as follows:
M“
»
–
1 3 ´2
2.99 ´16.11 4
´3.01 1 2.03
fi
fl
r1{�r1�2
r2{�r2�2
r3{�r3�2
“
»
–
0.267261 0.801784 ´0.534522
0.177276 ´0.955159 0.237159
´0.799306 0.265550 0.539067
fi
flLinear Systems: Fundamentals and Direct Methods  109
Similarly, for matrix M we find |detpMq| “ 0.00012688 and an estimate for the
condition number using Eq. (2.34) leads to
κpMq « 2
|detpMq| “ 2
0.00012688 “ 15763 " 1
Note that this estimate is slightly higher than the computed true value, κpMq “
15604.2 given in Table 2.1.
(c) In order to determine the severity of “ill-conditioning,” the following matrix
identities are computed using double precision arithmetic: The acquired errors from
the computation of AA´1 and pA´1q´1 are
AA´1 ´ I “
»
–
000
0.00195 0.00024 0.00049
0 0.00006 0.00024
fi
fl
pA´1q
´1 ´ A “
»
–
0.00109 ´0.00632 0.00163
0.00522 0.00155 ´0.00483
´0.00407 0.01084 ´0.00102
fi
fl
where the results above are reported correct to seven decimal places.
Discussion: This example demonstrates that estimating the condition number using
the equilibrated matrix with Eq. (2.34) is closer to the true theoretical value of κpMq.
Since the true and estimated values of κpMq are very large, we can surely expect
to observe the ill-effects of ill-condition when computing the inverse of A or solving
linear systems with A as the coefficient matrix.
In general, the order of the average absolute error in the computation of AA´1 “
I is 10´5, which is about the same as the magnitude of the reciprocal of the condition
number. On the other hand, the maximum errors in the computation of A´1 and
pA´1q´1 are 0.00195 and 0.01084, respectively. The average and maximum errors
of the latter are larger because of the accumulation of round-off errors owing to
inverting the matrix twice. It is clear that when an ill-conditioned matrix is inverted
or an ill-conditioned linear system is solved, the accumulation of round-offs will be
a major problem.
2.8 DECOMPOSITION METHODS
We often encounter Ax “ b systems, where A remains fixed while b changes (e.g., Ax “ b1,
Ay “ b2, etc.). For instance, in dynamics, matrix A is generally termed the “structure
matrix,” which depends on the structure of a mechanical or dynamic system, and vector b
denotes the “load.” Especially in structural engineering, the load denotes the applied forces,
and the solution vector x gives the stresses at various points on the structure. Hereby, a
structure is tested for various applied load scenarios.
Matrix decomposition (or factorization) methods have been developed as alternatives
to Gauss elimination to reduce the computation time and effort when solving multiple linear
systems consisting of the same A. A matrix decomposition (or matrix factorization) is the
process of factorizing a matrix into a product of matrices as A “ LU, where L and U are
generally lower and upper triangular matrices, respectively. Utilizing triangular matrices in110  Numerical Methods for Scientists and Engineers
matrix computations (on the order of Opn2q) makes calculations easier in the process and
provides advantages and efficiency in programming. However, when solving linear systems
with varying rhs vectors (Ax “ bm) by Gauss elimination, the row reductions are applied
to the augmented matrix (including the rhs vector). Therefore, the computational effort
(Opn3q) and cpu-time are the same whether A is the same or not.
Many different matrix decomposition methods have been developed over the years, e.g.,
LU, Cholesky, QR, SVD, and so on. Each method is used for a particular class of problems.
In other words, some of these methods are more useful than others, depending on the field
of application. In this section, we will present the two more common methods, Doolittle￾LU and Cholesky decomposition, that work with only positive definite and nonsingular
diagonally dominant square matrices.
1) Doolittle LU-Decomposition method is particularly useful when a matrix has
the form A “ LU, where L and U are lower and upper triangular matrices, respec￾tively. The Doolittle decomposition requires that the diagonal elements of the lower
triangular matrix L be “1” and an upper triangular matrix U. An alternative method,
Crout LU-Decomposition, factors the matrix into a lower triangular matrix L and an
upper triangular matrix U whose diagonal entries are required to be “1.” In Crout’s
decomposition, however, the errors may grow very rapidly; therefore, it is generally
not recommended unless the system of linear equations is relatively small. For this
reason, the Doolittle LU-decomposition is presented in this text.
2) Cholesky Decomposition Method is applied to matrices that are positive definite,
non-singular, and symmetric. Matrix A can be decomposed as A “ LLT , where L is
a lower triangular matrix and LT is its transpose. The method is computationally
faster and more efficient since only one triangular matrix is determined.
An advantage of decomposing matrix A as LU is that the system, Ax “ b, can be
rewritten as LUx “ b. Defining Ux “ y leads to an upper triangular system, and Ax “ b
eventually results in a lower triangular system, as illustrated below:
LUx “ b tUx “ yu Ñ Ly “ b (2.37)
First, Ly “ b is solved to find an intermediate solution y, using the forward substitution
(Pseudocode 2.7); then Ux “ y is solved to obtain x by applying the back substitution
(Pseudocode 2.8).
2.8.1 DOOLITTLE LU-DECOMPOSITION
The objective of this method is to find two matrices, L and U, that satisfy A “ LU, where
L is a lower triangular matrix with diagonal elements 1, and U is an upper triangular
matrix.
Consider a square matrix A, which has an LU-decomposition in the following forms:
»
—
—
—
—
—
–
a11 a12 a13 ¨¨¨ a1n
a21 a22 a23 ¨¨¨ a2n
a31 a32 a33 ¨¨¨ a3n
.
.
. .
.
. .
.
. ... .
.
.
an1 an2 an3 ¨¨¨ ann
fi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
–
100 ¨¨¨ 0
�21 1 0 ¨¨¨ 0
�31 �32 1 ¨¨¨ 0
.
.
. .
.
. .
.
. ... .
.
.
�n1 �n2 �n3 ¨¨¨ 1
fi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
–
u11 u12 u13 ¨¨¨ u1n
0 u22 u23 ¨¨¨ u2n
0 0 u33 ¨¨¨ u3n
.
.
. .
.
. .
.
. ... .
.
.
000 ¨¨¨ unn
fi
ffi
ffi
ffi
ffi
ffi
fl
(2.38)
Notice that the diagonal elements of L are set to 1, i.e., �ii “ 1 for i “ 1, 2,...,n.Linear Systems: Fundamentals and Direct Methods  111
FIGURE 2.7: Depicting Doolittle LU-decomposition; (a) left to right row sweeps on U, (b) top to
bottom column sweeps on L.
The LU product can be explicitly written as
A“
»
—
—
—
—
—
—
—
–
u11 u12 u13 ¨¨¨ u1n
�21u11 �21u12`u22 �21u13`u23 ¨¨¨ �21u1n`u2n
�31u11 �31u12`�32u22 �31u13`�32u23`u33 ¨¨¨ �31u1n`�32u2n`u3n
.
.
. .
.
. .
.
. ... .
.
.
�n1u11 �n1u12`�n2u22 �n1u13`�n2u23 ` u33 ¨¨¨ nř´1
k“1
�nkukn`unn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(2.39)
Comparing and matching the first rows of A and LU in Eq. (2.39), we find
u1j “ a1j , j “ 1, 2,...,n (2.40)
which takes care of the first row of U.
In Fig. 2.7, the steps of Doolittle decomposition on U and L matrices are schematically
illustrated. Finding the first row of U by Eq. (2.40) does not require any calculations, so
the first row of A is placed exactly on the first row of U (Fig. 2.7a). Next, comparing and
matching the first columns of A and LU, we obtain ai1 “ �i1u11 for i “ 2, 3,...,n. Since
u11 “ a11, for the first column (Fig. 2.7b), we deduce
�i1 “ ai1{u11, i “ 2, 3,...,n (2.41)
Similarly, we compare and match the diagonal and subsequent elements of the second row,
which yields a2j “ �21u1j ` u2j . Solving for u2j gives
u2j “ a2j ´ �21u1j , j “ 2, 3,...,n (2.42)
which takes care of the second row U (see Fig. 2.7a). The corresponding second column of
L is likewise obtained by matching and comparing the elements below the diagonal (i.e.,
�22 “ 1) as ai2 “ �i1u12 ` �i2u22. Solving for �i2, we obtain
�i2 “ ai2 ´ �i1u12
u22
, i “ 3, 4,...,n (2.43)112  Numerical Methods for Scientists and Engineers
LU-Decomposition
‚ The method is relatively easy to understand and implement;
‚ It is numerically stable;
‚ It does not need the information on b;
‚ It is cpu-time efficient when solving Axm “ bm for m “ 1, 2, ...;
‚ Having L with �ii “ 1 for all i guarantees the uniqueness of A “ LU.
‚ Employing it to solve a single linear system (Ax “ b) is inefficient
due to excessive calculations (decomposition, forward and backward
substitutions);
‚ Memory requirement increases by twofold if L and U are kept (stored)
as two separate matrices;
‚ Every square matrix is not LU-decomposable (if A is reducible to the
row echelon form with Gaussian elimination without ever interchang￾ing two rows, the matrix A has an LU-form);
‚ For large matrices, the round-off error emerges as the main drawback
of the decomposition methods.
which are placed in L from top to bottom (see Fig. 2.7b).
Notice that the procedure is sequenced to compute one row of U and its corresponding
column of L until all rows and columns of A are exhausted. Hence, we systematically find
the elements in L and U with the help of equations obtained from matching and comparing
the elements of A “ LU.
The elements of the ith row of U and the jth row of L can be generalized as
uij “ aij ´
i
ÿ´1
k“1
�ikukj , for i ď j ď n p1 ď i ď nq
�ij “ 1
ujj ˜
aij ´
j
ÿ´1
k“1
�ikukj¸
, for i ` 1 ď j ď n
(2.44)
Using two matrices (L and U) as given in the algorithm dou￾bles the memory requirement. As depicted below, the memory
requirement can be minimized by overwriting A with L and U
and omitting the storage of 1’s and 0’s of L or U, as they are the
givens of the method.
»
—
—
—
–
a11 a12 ¨¨¨ a1n
a21 a22 ¨¨¨ a2n
.
.
. .
.
. ... .
.
.
an1 an2 ¨¨¨ ann
fi
ffi
ffi
ffi
fl Ð
»
—
—
—
–
u11 u12 ¨¨¨ u1n
�21 u22 ¨¨¨ u2n
.
.
. .
.
. ... .
.
.
�n1 �n2 ¨¨¨ unn
fi
ffi
ffi
ffi
fl
A pseudomodule, LU_DECOMP, implementing the Doolittle LU-decomposition is pre￾sented in Pseudocode 2.11. The module requires the matrix A and its order n as input and
returns L and U as output. Both triangular matrices are first initialized: �ii “ 1 for all i
and �ij “ 0 for i ‰ j, and uij “ 0 for all i, j.Linear Systems: Fundamentals and Direct Methods  113
Pseudocode 2.11
Module LU_DECOMP (n, A, L, U)
\ DESCRIPTION: A pseudomodule to find LU-decomposition of a matrix.
Declare: ann, unn, �nn \ Declare matrices as arrays
L Ð 0; U Ð 0 \ Initialize L and U with zero matrices
For “
i “ 1, n‰
�ii Ð 1 \ Set �ii “ 1
End For
For “
i “ 1, n‰
For “
j “ i, n‰ \ Loop j: Find row elements of U
uij Ð aij \ Initialize accumulator, uij “ aij
For “
k “ 1,pi ´ 1q
‰ \ Loop k: Accumulating loop
uij Ð uij ´ �ik ˚ ukj \ Add ´�ik ˚ ukj to uij
End For
End For
For “
j “ pi ` 1q, n‰
�ji Ð aji \ Initialize accumulator, �ji “ aji
For “
k “ 1,pi ´ 1q
‰ \ Loop k: Accumulating loop
�ji Ð �ji ´ �jk ˚ uki \ Add ´�jk ˚ uki to �ji
End For
�ji Ð �ji{uii \ Find uii
End For
End For
End Module LU_DECOMP
The algorithm constructs U and L by sweeping through the matrix by rows and
columns, as illustrated in Fig. 2.7. The inner loop´j is first used to determine the ele￾ments of U along the rows. uij is used as an accumulator with an initial value assigned
to aij . When for i ď j, the accumulation carried out with the loop´k is completed, the
accumulated result is uij . The elements of L are computed across columns with the second
loop´j. Note that the matrix indices i and j in Eq. (2.44) are reversed here, since the
index of the loop´i also runs for the column index. A column of L is constructed with the
second loop´j from j “ i to n. Similarly, �ji is used as the accumulator, with an initial
value assigned to aij . Once the accumulation is completed for j ą i using the loop´k and
dividing it by uii gives �ij . Note that this module requires a memory allocation of 2n2 for
L and U. Removing L and U from the Module’s variable list, Module LU_DECOMP(n,A),
and replacing �ij and uij ’s simply with aij ’s in both the module and Eq. (2.44) reduce the
memory requirement to a single matrix storage.
EXAMPLE 2.8: LU-Decomposition to solve linear systems
The smallest eigenvalue and associated eigenvector of a matrix satisfy Ax “ λ´1x,
where λ “ max|x|. A procedure for determining the smallest eigenvalue begins with
a guess xp0q vector, followed by a series of successive solutions of the matrix equation:
Axpp`1q “ 1
λppq xppq
, λppq “ max
ˇ
ˇ
ˇxppq
ˇ
ˇ
ˇ , p “ 0, 1, 2,....
where p denotes an estimate at the pth step. Since matrix A is fixed while only
rhs varies, the LU-decomposion is suitable for this example. Thus, the matrix A114  Numerical Methods for Scientists and Engineers
is decomposed once, and then two linear systems are solved: (1) a lower-triangular
Ly “ p1{λppq
qxppq and (2) an upper-triangular system, Uxpp`1q “ y. Apply the
Doolittle algorithm to the following matrix to find suitable L and U.
A “
»
—
—
–
´1 ´2 ´2 ´1
´2 ´6 ´7 ´4
´12 5 6
1 ´4 ´3 3
fi
ffi
ffi
fl
SOLUTION:
Starting with u1j “ a1j for j “ 1, 2, 3, and 4, using Eq. (2.40) we readily obtain
u11 “ ´1, u12 “ ´2, u13 “ ´2, u14 “ ´1
From �i1 “ ai1{u11, Eq. (2.41), for i “ 2, 3, and 4 we find
�21 “ a21{u11 “ ´2{p´1q “ 2,
�31 “ a31{u11 “ p´1q{p´1q “ 1,
�4,1 “ a4,1{u11 “ 1{p´1q“´1
Using Eq. (2.42) for j “ 2, 3, and 4 results in
u22 “ a22 ´ �21u12 “ p´6q´p2qp´2q“´2,
u23 “ a23 ´ �21u13 “ p´7q´p2qp´2q“´3,
u24 “ a24 ´ �21u14 “ p´4q´p2qp´1q“´2
Using Eq. (2.43) for i “ 3 and 4, we obtain
�32 “ a32 ´ �31u12
u22
“ 2 ´ p1qp´2q
p´2q “ ´2,
�4,2 “ a4,2 ´ �4,1u12
u22
“ ´4 ´ p´1qp´2q
p´2q “ 3
The third row of U leads to u3i “ a3i ´ �31u1i ´ �32u2i which for i “ 3 and 4 gives
u33 “ a33 ´ �31u13 ´ �32u23 “ 5 ´ p1qp´2q ´ p´2qp´3q “ 1
u34 “ a34 ´ �31u14 ´ �32u24 “ 6 ´ p1qp´1q ´ p´2qp´2q “ 3
Equation (2.44) gives the third column of L:
�43 “ 1
u33
pa43 ´ �41u13 ´ �42u23q “ 1
p1q
pp´3q ´ p´1qp´2q´p3qp´3qq “ 4
Lastly, from Eq. (2.44), we obtain
u44 “ a44 ´ �41u14 ´ �42u24 ´ �43u34 “ 3 ´ p´1qp´1q´p3qp´2q´p4qp3q“´4
Finally, collecting the elements of the lower and upper triangular matrices, we find
L “
»
—
—
–
1 0 00
2 1 00
1 ´210
´1 3 41
fi
ffi
ffi
fl , U “
»
—
—
–
´1 ´2 ´2 ´1
0 ´2 ´3 ´2
0013
000 ´4
fi
ffi
ffi
flLinear Systems: Fundamentals and Direct Methods  115
Cholesky Decomposition
‚ The method requires fewer arithmetic operations;
‚ It outperforms the LU-decomposition by a factor of two;
‚ It is stable without pivoting since A is positive definite;
‚ The memory requirement is cut in half since U “ LT .
‚ The method can be employed to symmetric matrices only;
‚ Matrix A must be positive definite to avoid square root of negative
numbers when calculating �ii’s.
2.8.2 CHOLESKY DECOMPOSITION
If A “ raij snˆn is a positive definite and symmetric matrix, the matrix A can be decomposed
into a product of a unique lower triangular matrix L and its transpose: A “ LLT . The
decomposition matrix can be written explicitly as
»
—
—
—
—
—
–
a11 a12 a13 ¨¨¨ a1n
a21 a22 a23 ¨¨¨ a2n
a31 a32 a33 ¨¨¨ a3n
.
.
. .
.
. .
.
. ... .
.
.
an1 an2 an3 ¨¨¨ ann
fi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
–
�11 0 0 ¨¨¨ 0
�21 �22 0 ¨¨¨ 0
�31 �32 �33 ¨¨¨ 0
.
.
. .
.
. .
.
. ... .
.
.
�n1 �n2 �n3 ¨¨¨ �nn
fi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
–
�11 �21 �31 ¨¨¨ �n1
0 �22 �23 ¨¨¨ �2n
0 0 �33 ¨¨¨ �3n
.
.
. .
.
. .
.
. ... .
.
.
000 ¨¨¨ �nn
fi
ffi
ffi
ffi
ffi
ffi
fl
(2.45)
Upon performing LLT , we obtain
A“
»
—
—
—
—
—
—
—
–
�2
11 �11�21 �11�31 ¨¨¨ �11�n1
�11�21 �2
21`�2
22 �21�31`�22�32 ¨¨¨ �21�n1`�22�n2
�11�31 �21�31`�22�32 �2
31`�2
32 ` �2
33 ¨¨¨ �31�n1`�32�n2`�33�n3
.
.
. .
.
. .
.
. ... .
.
.
�11�n1 �21�n1`�22�n2 �31�n1`�32�n2`�33�n3 ¨¨¨ řn
k“1
�2
nk
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(2.46)
Inspecting and matching the first row and first column entries of A and LLT , we find
�11 “ ?a11. The Cholesky decomposition is a special case of the LU-decomposition where
U “ LT . So we could skip the derivation of the relations between the elements of A and L
and simply replace uij ’s with �ij ’s in Eq. (2.44) to get
�ij “
$
’’’&
’’’%
d
aii ´ i
ř´1
k“1
�2
ik, j “ i and i “ 1, 2,...,n
1
�jj ˆ
aij ´ j
ř´1
k“1
�ik�jk˙
, i ą j and j “ 1, 2,...,pi ´ 1q
(2.47)
A pseudomodule, CHOLESKY_DECOMP, for decomposing a positive definite symmetric
matrix using the Cholesky’s method is given in Pseudocode 2.12. The module requires an
input matrix A of size n and returns a lower triangular matrix L as output. The outer
loop´j is used to compute �ij ’s along a column using Eq. (2.47). After setting sum1 “ ajj
(initializing the accumulator variable), the accumulator loop over the k index variable, ´�2
ik116  Numerical Methods for Scientists and Engineers
Pseudocode 2.12
Module CHOLESKY_DECOMP (n,A, L)
\ DESCRIPTION: A pseudomodule to decompose a symmetric matrix A as
\ LLT by Cholesky Decomposition.
\ USES:
\ SQRT:: A built-in function computing the square-root of a value.
Declare: ann, �nn \ Declare matrices as arrays
L Ð 0 \ Initialize L with zero matrix
For “
j “ 1, n‰ \ Loop j: Perform column sweep
sum1 Ð ajj \ Initialize sum1
For “
k “ 1,pj ´ 1q
‰ \ Loop k: Accumulating loop
sum1 Ð sum1 ´ �2
jk \ Add ´�2
ik to accumulator
End For
�jj Ð ?sum1 \ Set diagonal element
For “
i “ pj ` 1q, n‰ \ Loop i: Find elements below diagonal
sum2 Ð aij \ Initialize sum of off-diagonal elements
For “
k “ 1,pj ´ 1q
‰ \ Loop k: Accumulating loop
sum2 Ð sum2 ´ �ik ˚ �jk \ Add ´�ik�jk to accumulator
End For
�ij Ð sum2{�jj \ Save result on L
End For
End For
End Module CHOLESKY_DECOMP
terms up to k “ pj´1q are accumulated. The diagonal element for the jth row then becomes
�jj “ ?sum1. The elements below the diagonals (j ă i) also require an accumulator,
sum2, using the inner loop-i. This accumulator is first initialized as sum2 “ aij . With the
accumulator loop over index variable k, the ´�ik�jk terms are added to sum2. Lastly, an
�ij is found as sum2{�jj .
EXAMPLE 2.9: Cholesky Decomposition
In electronic structure calculations, Cholesky’s algorithm can be efficiently utilized to
remove linear dependencies in a given matrix. Decompose matrix A into A “ LLT .
A “
»
—
—
–
4 ´24 2
´2 10 ´5 ´4
4 ´59 1
2 ´41 7
fi
ffi
ffi
fl
SOLUTION:
Recall that A must be symmetric and positive-definite in order to apply the
Cholesky decomposition. The given matrix is symmetric, and since all its eigenvalues
are positive («17.5673, 7.05325, 4.29779, and 1.08164), the matrix meets the positive
definiteness criteria. Therefore, A is suitable for the Cholesky decomposition.
The first row and first column elements are obtained as
�11 “ ?a11 “ ?
4 “ 2Linear Systems: Fundamentals and Direct Methods  117
For i “ 2, the second-row elements are computed from Eq. (2.47) as follows:
�21 “ a21
�11
“ p´2q
2 “ ´1, �22 “
b
a22 ´ �2
21 “ a10 ´ p´1q2 “ 3
For i “ 3, similarly, we obtain
�31 “ a31
�11
“ 4
2 “ 2,
�32 “ 1
�22
pa32 ´ �21�31q “ 1
p3q
p´5 ´ p´1qp2qq “ ´1
�33 “
b
a33 ´ �2
31 ´ �2
32 “
b
9 ´ p2q
2 ´ p´1q
2 “ 2
The elements for the last row (i “ 4) lead to
�4,1 “ a4,1
�11
“ 2
2 “ 1,
�4,2 “ 1
�22
pa4,2 ´ �4,1�21q “ 1
p3q
p´4 ´ p1qp´1qq “ ´1
�4,3 “ 1
�33
pa4,3 ´ �31�4,1 ´ �32�4,2q “ 1
p2q
p1 ´ p2qp1q ´ p´1qp´1qq “ ´1
�44 “
b
a44 ´ �2
4,1 ´ �2
4,2 ´ �2
4,3 “
b
7 ´ p1q
2 ´ p´1q
2 ´ p´1q
2 “ 2
Finally, collecting all elements in L, we obtain
L “
»
—
—
–
2 0 00
´13 00
2 ´120
1 ´1 ´1 2
fi
ffi
ffi
fl
Discussion: Finding the L matrix, if it exists, requires two recursive relationships:
one for establishing the diagonal elements (i “ j), and the other for obtaining
the elements below diagonals (i ą j). The procedure is easy and straightforward;
however, one must make sure that A is positive symmetric before attempting to
solve the problem.
2.9 TRIDIAGONAL SYSTEMS
2.9.1 THOMAS ALGORITHM
Tridiagonal systems of linear equations are the most common type of linear system in prac￾tice. They are encountered in many science and engineering problems, such as interpolation
problems, the numerical solution of ordinary and partial differential equations, and so on.
The tridiagonal matrix algorithm, also frequently referred to as Thomas algorithm,
is a special and simplified case of the Gauss elimination algorithm. A general form of
the tridiagonal system of linear equations is given in Eq. (2.11), where di, bi, ai are one￾dimensional arrays of length n containing diagonal, lower- and upper-diagonal elements of118  Numerical Methods for Scientists and Engineers
Thomas Algorithm
‚ The algorithm is simple and easy to implement;
‚ It is very fast owing to requiring minimum arithmetic operations;
‚ It requires minimum memory allocation (max. 5n ´ 2 for five arrays
of length n) in comparison to full matrix storage (n2 ` 2n).
‚ It can only be applied to diagonally dominant tridiagonal systems;
that is, |di|ą|bi|`|ai| for all i;
‚ For large or ill-conditioned systems, round-offs can be a problem.
the matrix, respectively. Since a tridiagonal system has only one lower diagonal of non-zero
elements, it is sufficient to employ the forward elimination procedure once for each row of
the augmented matrix as follows:
»
—
—
—
—
—
—
—
–
d1 a1
0 d1
2 a2
0 d1
3 a3
... ... ...
0 d1
n´1 an´1
0 d1
n
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
c1
c1
2
c1
3
.
.
.
c1
n´1
c1
n
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
r1
2 Ð r2 ´ r1b 2{d1
r1
3 Ð r3 ´ r1
2b3{d1
2
.
.
.
r1
n´1 Ð rn´1 ´ r1
n´2bn´1{d1
n´2
r1
n Ð rn ´ r1
n´1bn{d1
n´1
where the primes denote elements that have been modified from their original values. Note
that only the diagonal and the rhs elements are modified in the forward elimination step.
Hence, the forward elimination procedure leads to the following equations:
c1
i “ ci ´ bi
di´1
ci´1, d1
i “ di ´ bi
di´1
ai´1, i “ 2, 3,...,n (2.48)
The arithmetic operations on bi’s are not explicitly shown since the lower diagonal elements
are zeroed out. In other words, the actual arithmetic operations on the lower diagonal
elements that should result in b
p1q
i “ 0 are not performed, as this would be a waste of time.
The resulting system is an upper-bidiagonal matrix, which is solved using the back
substitution algorithm. Noting that d1
ixi ` aixi`1 “ c1
i for i “ pn´1q,pn´2q,..., 2, 1 we
obtain:
xn “ c1
n
d1
n
, xi “ c1
i ´ aixi`1
d1
i
, i“ pn´1q,pn´2q,..., 2, 1 (2.49)
A pseudomodule, TRIDIAGONAL, solving a tridiagonal system of equations using the
Thomas algorithm is presented in Pseudocode 2.13. The module inputs are s1 and sn (sub￾scripts of the first and last equations), di, bi, ai, and ci (diagonal, lower-, upper-diagonal,
and rhs) elements. On exit, the solution is saved on x. The forward elimination loop modifies
di and ci, according to Eq. (2.48). By the end of the forward elimination, the last unknown
becomes available, xsn “ csn{dsn, which is used as a starting value for the back substitu￾tion step. The rest of the solution is obtained backward from Eq. (2.49) using the (back
substitution) loop. Note that the memory requirement for the arrays used in this module is
5n´2. However, we can reduce the memory requirement to 4n´2 by eliminating the array
x from the module and overwriting the solution on c as follows:
c1
n Ð c1
n
d1
n
, c1
i Ð c1
i ´ aic1
i`1
d1
i
, i“ pn´1q,pn´2q,..., 2, 1 (2.50)Linear Systems: Fundamentals and Direct Methods  119
Pseudocode 2.13
Module TRIDIAGONAL (s1, sn, b, d, a, c, x)
\ DESCRIPTION: A pseudomodule employing the Thomas algorithm to a tri-
\ diagonal system. Caution! Arrays c and d are destroyed.
Declare: bs1:sn, ds1,sn, as1:sn, cs1:sn, xs1:sn \ Declare arrays from s1 to sn
For “
i “ s1 ` 1, sn‰ \ Loop i: Forward elimination step
ratio Ð bi{di´1 \ Set normalization factor
di Ð di ´ ratio ˚ ai´1 \ Modify diagonal element
ci Ð ci ´ ratio ˚ ci´1 \ Modify rhs element
End For
xsn Ð csn{dsn \ Get xsn from last equation
For “
i “ psn ´ 1q, s1,p´1q
‰ \ Loop i: Back substitution step
xi Ð pci ´ ai ˚ xi`1q{di \ Find unknowns from psn ´ 1q to s1
End For
End Module TRIDIAGONAL
2.9.2 LU DECOMPOSITION
Crout’s LU-decomposition (i.e., placing 1’s to the diagonal of U instead of L) of a tridiagonal
matrix can be useful in solving Tx “ cm, where T is a tridiagonal matrix for m “ 1, 2,...,
and so on. Consider a tridiagonal system in the form given by Eq. (2.11). We assume
LU-decomposition in the following form:
T “ LU “
»
—
—
—
—
—
—
—
–
�1
b2 �2
b3 �3
... ...
bn´1 �n´1
bn �n
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
–
1 u1
1 u2
1 u3
... ...
1 un´1
1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(2.51)
where the matrices T (with bi, di, ai, and ci’s), L (with �i and bi’s), and U (with ui’s) are
stored as one-dimensional arrays to minimize the memory requirement.
Comparing and matching the elements of the T and LU matrices one-to-one using the
same procedure applied to full matrices, the elements of the bidiagonal matrix are found as
follows:
�i “ di ´ biui´1, for i “ 2,...,n with �1 “ d1,
ui “ ai{�i, for i “ 1, 2,...,pn ´ 1q. (2.52)
To further minimize memory allocation, if destroying T is unimportant, matrices L and U
can be placed in T by overwriting �i Ñ di and ui Ñ ai. Then the symmetric tridiagonal
system can be solved by employing a forward substitution procedure to solve Ly “ b) and
a back substitution procedure to solve Ux “ y) yielding
Forward substitution: x1 “ c1{d1, xi “ pci ´ bixi´1q{di, for i “ 2, . . . , n,
Back substitution: xj “ xj ´ ajxj`1, j “ pn ´ 1q,..., 2, 1
where the array y is (eliminated) replaced by x to reduce memory space.
The LU-decomposition requires about 3n arithmetic operations, while the forward￾and back-substitutions altogether demand about 5n operations. Thus, the solution of a120  Numerical Methods for Scientists and Engineers
large tridiagonal system of equations by LU decomposition requires about 8n operations in
total.
The Doolittle LU decomposition for the tridiagonal systems is not given here; however,
an algorithm can easily be developed in a similar way. The advantages and disadvantages of
Doolittle or Crout’s decomposition methods are the same as those of the Thomas algorithm.
2.9.3 CHOLESKY DECOMPOSITION
Consider a symmetric tridiagonal matrix in the following:
T “
»
—
—
—
—
—
—
—
–
d1 b2
b2 d2 b3
b3 d3 b4
... ... ...
bn´1 dn´1 bn
bn dn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
, L “
»
—
—
—
—
—
—
—
–
�1
e2 �2
e3 �3
... ...
en´1 �n´1
en �n
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(2.53)
where T has a Cholesky decomposition as LLT with the given lower bidiagonal matrix L.
Then, the matrix product LLT is obtained as
LLT “
»
—
—
—
—
—
—
—
–
�2
1 e2�1
e2�1 e2
2 ` �2
2 e3�2
e3�2 e2
3 ` �2
3 e4�3
... ... ...
en´1�n´2 e2
n´1 ` �2
n´1 en�n´1
en�n´1 e2
n ` �2
n
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“ T (2.54)
Inspecting and matching the non-zero elements of Eqs. (2.53) and (2.54), we arrive at the
following relationships:
�1 “ ad1, ek “ bk
�k´1
, �k “
b
dk ´ e2
k, k“2, 3,...,n (2.55)
Having decomposed the matrix T as LLT , the linear system becomes LLT x “ b.
Setting LT x “ y, the system can be written as Ly “ b. Employing the forward substitution
to Ly “ b and back-substitution to LT x “ y leads to
Forward substitution: y1 “ c1{d1, yi “ pci ´ biyi´1q{di, for i “ 2, 3, .., n
Back-substitution: xn “ yn{dn, xi “ pyi ´ bi`1xi`1q{di, for i “ pn ´ 1q,..., 2, 1
Original principal diagonal and lower diagonal elements can be used to minimize the
memory requirement by overwriting arrays as follows:
d1 Ð ad1, bk Ð bk
dk´1
, dk Ð
b
dk ´ b2
k, k“2, 3,...,n
Writing y on c, then x on c (i.e., the solution is saved on the right-hand side vector), two
arrays (x and y) are eliminated, which makes the algorithm more memory efficient:
Forward substitution: c1 Ð c1{d1, ci Ð pci ´ bici´1q{di, for i “ 2, 3, .., n
Back substitution: cn Ð cn{dn, ci Ð pci ´ bi`1ci`1q{di for i “ pn ´ 1q,..., 2, 1Linear Systems: Fundamentals and Direct Methods  121
In Pseudocode 2.13, arrays d and b are destroyed (i.e., overwrit￾ten with the new values). When solving a tridiagonal system of
the form Axm “ bm, where bm changes, the original arrays d and
b must be stored with different names (say d0 and b0) and re￾trieved (d Ð d0 and b Ð b0) each time before calling the module
TRIDIAGONAL.
EXAMPLE 2.10: Application of Tridiagonal System of Equations
Five objects (W1, W2, W3, W4, and W5) are connected in series with
five springs (with spring constants of k1, k2, k3, k4, and k5) that
are suspended, as illustrated in the figure. We wish to determine
the displacement of each object from its unstretched position. The
equations for the steady-state system are expressed as follows:
W1 ` k2px2 ´ x1q ´ k1x1 “ 0
W2 ` k3px3 ´ x2q ´ k2px2 ´ x1q “ 0
W3 ` k4px4 ´ x3q ´ k3px3 ´ x2q “ 0
W4 ` k5px5 ´ x4q ´ k4px4 ´ x3q “ 0
W5 ´ k5px5 ´ x4q “ 0
Apply (a) Thomas algorithm to solve the resulting tridiagonal system of equations
obtained for the case data given as W1 “ W3 “ W4 “ W5 “ 10 N, W2 “ 15 N, k1 “
100, k2 “ 300, k3 “ 180, k4 “ 200, and k5 “ 300 N/m, and (b) Crout’s algorithm to
decompose the coefficient matrix into lower and upper tridiagonal matrices.
SOLUTION:
Substituting the numerical values of W’s and k’s into the steady-state equations,
the augmented matrix for the linear system becomes
»
—
—
—
—
–
40 ´30
´60 96 ´36
´18 38 ´20
´20 50 ´30
´30 30
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
1
3
1
1
1
fi
ffi
ffi
ffi
ffi
fl
The first step is to employ the forward substitution procedure. Performing the el￾ementary row operations on the augmented matrix as indicated for each row, we
find
„
»
—
—
—
—
—
—
—
—
—
—
—
–
40 ´30
0 51 ´36
0 430
17 ´20
0 1470
43 ´30
0 180
49
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
1
9
2
44
17
131
43
180
49
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
r1
2 Ð r2 `
3
2
r1
r1
3 Ð r3 `
6
17 r1
2
r1
4 Ð r4 `
34
43 r1
3
r1
5 Ð r5 `
43
49 r1
4
where r1
i’s denote the altered rows due to prior row operations.122  Numerical Methods for Scientists and Engineers
Now going back to the equation form, we have
40x1 ´ 30x2 “ 1, 51x2 ´ 36x3 “ 9
2
, 430
17 x3 ´ 20x4 “ 44
17
1470
43 x4 ´ 30x5 “ 131
43 , 180
49 x5 “ 180
49
Next, the back substitution step is carried out in order to find the unknowns one by
one from the last equation to the first one as follows:
180
49 x5 “ 180
49 Ñ x5 “ 1
1470
43 x4 ´ 30x5 “ 131
43 Ñ x4 “ 29
30
430
17 x3 ´ 20x4 “ 44
17 Ñ x3 “ 13
15
51x2 ´ 36x3 “ 9
2 Ñ x2 “ 7
10
40x1 ´ 30x2 “ 1 Ñ x1 “ 11
20
(b) Notice that the tridiagonal matrix is not symmetrical, so Crout’s algorithm is
suitable for this example. To start decomposition, the first row leads to �1 “ d1 “ 40
and u1 “ a1{�1 “ ´3{4. Employing Eqs. (2.52) to the remaining rows and columns,
we obtain
�2 “d2 ´ b2u1 “96 ´ p´60q
´
´3
4
¯
“51 and u2 “ a2
�2
“ p´36q
51 “ ´12
17
�3 “d3 ´ b3u2 “38 ´ p´18q
´
´12
17¯
“ 430
17 and u3 “ a3
�3
“ ´20
430{17 “ ´34
43
�4 “d4 ´ b4u3 “50 ´ p´20q
´
´34
43¯
“ 1470
43 and u4 “ a4
�4
“ ´30
1470{43 “´43
49
�5 “d5 ´ b5u4 “30 ´ p´30q
´
´43
49¯
“ 180
49
Finally, by substituting the computed values into the L and U definitions, Eq.
(2.51), the matrices are obtained in the desired forms.
L “
»
—
—
—
—
—
—
–
40
´60 51
´18 430
17
´20 1470
43
´30 180
49
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
, U “
»
—
—
—
—
—
—
—
–
1 ´3
4
1 ´12
17
1 ´34
43
1 ´43
49
1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Discussion: The number of arithmetic operations in employing the Thomas algo￾rithm given in Pseudocode 2.13 are p3n ´ 3q additions/subtractions and p5n ´ 4q
multiplications/divisions, yielding a total operation count of 8n ´ 7. This is a signif￾icant reduction from the roughly 2n3{3 operations needed to solve a linear system
with full matrix storage. The memory requirement can be reduced to 4n ´ 2 from
5n ´ 2 by eliminating intermediate arrays and writing the solution on the rhs vec￾tor. The LU-decomposition is preferred when multiple linear systems of the form
Tx “ bm are to be solved.Linear Systems: Fundamentals and Direct Methods  123
2.10 BANDED LINEAR SYSTEMS
The numerical solution of high-order ODEs or partial differential equations in two or three
variables leads to band diagonal (or banded) linear systems of equations. A matrix with zero
elements everywhere except on a narrow band about the main diagonal is referred to as
a banded matrix. Clearly, if a large banded system of equations is solved with full n ˆ n
matrix storage, not only will the memory requirement increase but also the cpu-time due
to excessive arithmetic operations with zeros. For this reason, solving a banded system or
decomposing a banded matrix based on full matrix storage is inefficient and impractical.
An efficient way of solving banded linear systems involves storing and manipulating
only the non-zero elements of the matrix. The Gauss elimination and/or LU-decomposition
algorithms are specifically tailored for banded systems to significantly reduce memory allo￾cation and cpu-time.
The definition of a banded matrix with elements aij is mathematically expressed as
aij “ 0, j ă i ´ mL or j ą i ` mU
where mL and mU (mL, mU ą 0) are the lower and upper bandwidths, respectively.
The bandwidth of a banded matrix is defined as nb “ mL ` mU ` 1. In Fig. 2.8a, an
8 ˆ 8 banded matrix is illustrated. The matrix has a lower and an upper bandwidth of
3 and 2, respectively, leading to nb “ 6. The compact form of the banded (rectangular)
matrix, shown in Fig. 2.8b, excludes off-the-band zeros. It is evident that, if the total band￾width is sufficiently small, considerable savings in memory requirements will be achieved
by implementing the compact matrix storage scheme.
Several banded matrix indexing schemes are encountered in the literature. The compact
matrix storage scheme adopted here is based on nonzero diagonals. Rotating the band 45˝
clockwise results in a compact matrix of size n ˆ nb. Extra spaces created are padded with
zeros, i.e., the zeros in bold font in the upper left and lower right corners of Fig. 2.8b.
This allows the nonzero diagonals to be aligned as columns. In the compact matrix form
denoted by B =[aik] (nˆnb), the new indices represent row (i) and column (k, i.e., diagonal)
numbers, respectively. In matrix B, the indices of the main and uppermost diagonals become
d “ mL `1 and nb “ mL `mU `1, respectively, as moving from left to right along columns.
The relationship between the elements of the full banded and compact matrices can be
expressed as bik “ aij where k “ d ` j ´ i for i, j “ 1, 2, 3,...,n.
FIGURE 2.8: Matrix storage for (a) a banded, (b) a compact matrix.124  Numerical Methods for Scientists and Engineers
FIGURE 2.9: Depiction of naive Gauss Elimination procedure for a banded matrix with pivot at (a)
i ă n ´ mU , (b) i ą n ´ mU .
2.10.1 NAIVE GAUSS ELIMINATION
We will employ the naive Gauss elimination procedure presented in Section 2.5 to a com￾pact banded linear system. Consider a banded coefficient matrix with lower and upper
bandwidths mL and mU , shown in Fig. 2.9. We will denote the maximum number of el￾ements under a pivot by imax, which corresponds to mL for most rows (i ď n ´ mL
in Fig. 2.9a), while it is n ´ i for rows with i ą n ´ mL (Fig. 2.9b). Thus, we may set
imax “MIN(mL, n ´ i). Similarly, the column number of the rightmost non-zero element in
row´i is jmax “ nb for rows with i ď n ´ mU (Fig. 2.9a) or jmax “ n ` d ´ i otherwise
(Fig. 2.9b), which can be generalized as jmax “MAX(nb, n ` d ´ i).
In this procedure, the column elements below the diagonal (lower band) are to be zeroed
out. Note that the indices k and j refer to the row and column indices of the pivot in the
original matrix, respectively (see Fig. 2.9). In the forward elimination step, the elements
in the pivot row (aij , jmin ď j ď jmax) are used in row reductions while clearing the
elements below the pivot ak`i,j´k (in compact matrix notation) for 1 ď k ď imax. The
matrix coefficients and the right-hand side are modified as follows:
ai`k,j´k “ ai`k,j´k ´ aij ˆai`k, d´k
aid ˙
, j “ jmin, . . . , jmax (2.56)
bi`k “ bi`k ´ bi
ˆai`k, d´k
aid ˙
, k “ imin, . . . , imax (2.57)
where imin “ 1, imax “ MINpmL, n ´ kq, jmin “ d ` 1 and jmax “ MINpnb, n ` d ´ iq.
The back substitution procedure is employed as follows:
xn “ bn
and
, xi “ 1
aid ˜
bi ´
jmax
ÿ
j“jmin
aij xi`j´d
¸
, i “ pn ´ 1q,pn ´ 2q,..., 2, 1 (2.58)
where jmin and jmax have the same definitions as before.Linear Systems: Fundamentals and Direct Methods  125
Pseudocode 2.14
Module BAND_SOLVE (n, mL, mU , A, b)
\ DESCRIPTION: A pseudomodule to solve a banded linear system by naive
\ Gauss elimination without pivoting.
\ USES:
\ BAND_BACK_SUBSTITUTE:: Back-substitution module (Pseudocodes 2.15);
\ MIN :: Built-in function to find the minimum of a set of integers.
Declare: an,mL`mU `1, bn \ Declare array variables
d Ð mL ` 1 \ Find position of diagonal in compact matrix
nb Ð d ` mU \ Find total bandwidth
For “
i “ 1, n‰ \ Loop i: Forward elimination loop
imin Ð 1 \ First row below the diagonal
imax Ð MINpmL, n ´ iq \ Find number of elements below diagonal
For “
k “ imin, imax‰ \ Loop k: Sweep all elements below diagonal
ratio Ð ai`k, d´k{ai,d \ Set normalization factor p
jmin Ð d ` 1 \ Set index of pivot element in row
jmax Ð MINpnb, n ` d ´ iq \ Set index of last non-zero element
For “
j “ jmin, jmax‰ \ Loop j: Perform elimination on row
ai`k,j´k Ð ai`k,j´k ´ ai,j ˚ ratio \ Original matrix A is modified
End For
bi`k Ð bi`k ´ bi ˚ ratio \ Carry out elimination steps on rhs
End For
End For
BAND_BACK_SUBSTITUTEpn, mL, mU , A, bq \ Solution is overwritten on b
End Module BAND_SOLVE
A pair of pseudocodes that implement forward elimination and back substitution proce￾dures of the Gauss elimination method on a band diagonal matrix are given in Pseudocodes
2.14 and 2.15. The module BAND_SOLVE requires a number of equations (n), the lower and
upper bandwidths (mL, mU ), the coefficient matrix in compact form (A), and the right￾hand side vector (b) as input. On exit, the matrix A is an upper-banded matrix, and b is
the altered rhs vector.
The forward elimination procedure for all rows is carried out in the outer loop over
the i variable. Using the ith row as the “pivot row,” the elimination of the elements below
the pivot is accomplished in the inner loop-j that runs from j “ jmin to the last non￾zero element to the right of the pivot at jmax. However, the arithmetic operations for the
elements that give zero are not actually performed to save cpu-time. While the non-zero
elements below the pivot are zeroed out row-by-row using the inner loop-k for 1 ď i ď
imax, the rest of the elements in row-k are also modified according to Eqs. (2.56) and
(2.57). When the row reductions are completed for all i, j, and k’s, the coefficient matrix
becomes an upper banded matrix, which is now suitable for obtaining the solution vector
with BAND_BACK_SUBSTITUTE. The module calls BAND_BACK_SUBSTITUTE to obtain
the solution vector and returns the solution saved in b.
The module BAND_BACK_SUBSTITUTE has the same argument list as BAND_SOLVE.
However, the matrix A is an upper band matrix in compact form, which permits the back
substitution algorithm to be employed. This module is not suitable to be used stand-alone
unless matrix A is an upper diagonal band matrix. The unknown xn is available from the
last (nth) equation: xn “ bn{an,d. Using the outer loop over i that signifies the rows and126  Numerical Methods for Scientists and Engineers
Pseudocode 2.15
Module BAND_BACK_SUBSTITUTE (n, mL, mU , A, b)
\ DESCRIPTION: A pseudomodule to solve an upper banded linear system.
\ USES:
\ MIN :: Built-in function to find the minimum of a set of integers.
Declare: an,mL`mU `1, bn \ Declare matrices as arrays
d Ð mL ` 1 \ Find position of diagonal in compact matrix
nb Ð d ` mU \ Find total bandwidth
bn “ bn{and \ Find solution from last equation, bn Ð xn
For “
i “ pn ´ 1q, 1,p´1q
‰ \ Loop i: Back substitution steps
jmin Ð d ` 1 \ Set index of element next to diagonal
jmax Ð MINpnb, n ` d ´ iq \ Set rightmost column index
For “
j “ jmin, jmax‰ \ Loop j: Accumulating loop
bi Ð bi ´ aij ˚ b i`j´d \ Add ´aij ˚ b i`j´d to accumulator
End For
bi Ð bi{aid \ Solution is written on b
End For
End Module BAND_BACK_SUBSTITUTE
Eq. (2.58), the rest of the unknowns are obtained in the order from the last row (i “ n) to
the first row (i “ 1). Note that at any ith row, the unknowns to the right of the pivot are
known except for xi, so the inner loop j runs from jmin to jmax to sum up the known
products of ‘aij ˚ xi`j´d’ to find the solution to an equation with one unknown. To reduce
memory requirements, the solution vector x is written onto the rhs vector b without the
need to use an extra array.
2.10.2 LU-DECOMPOSTION
In this section, it is assumed that a banded linear system results from the discretiza￾tion of a partial differential equation and that it is diagonally dominant, which does not
require pivoting. The naive Gauss elimination algorithm presented with Pseudocodes 2.14
and 2.15 does not implement any pivoting strategy. Hence, the LU-decomposition algorithm
presented here also does not use pivoting.
Let A be an n ˆ nb real banded matrix defined as
A “
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
0 ¨¨¨ 0 a1,d a1,d`1 ¨¨¨ a1,nb
.
.
. a2,d´1 a2,d a2,d`1 ¨¨¨ a2,nb
0 .
.
. .
.
. .
.
. .
.
.
ad,1 ¨¨¨ ad,d´1 ad,d ad,d`1 ¨¨¨ ad,nb
.
.
. .
.
. .
.
. .
.
. .
.
.
ak,1 ¨¨¨ ak,d´1 ak,d ak,d`1 ¨¨¨ ak,nb
ak`1,1 ¨¨¨ ak`1,d´1 ak`1,d ak,d`1 0
.
.
. .
.
. .
.
. .
.
. .
.
.
an,1 ¨¨¨ an,d´1 an,d 0 ¨¨¨ 0
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
, (2.59)
where d “ mL ` 1 and k “ nb ´ mU .Linear Systems: Fundamentals and Direct Methods  127
The lower and upper band matrices have a banded form similar to that of A, and the
decomposition is unique if L has unit diagonal elements. The compact forms of L and U
can be written as
L “
»
—
—
—
—
—
—
—
—
—
—
—
—
—
–
00 0 �1,d
0 0 �2,d´1 �2,d
0 .
.
. .
.
.
�d,1 ¨¨¨ �2,d´1 �d,d
.
.
. .
.
. .
.
.
�i,1 ¨¨¨ �i,d´1 �i,d
.
.
. .
.
. .
.
.
�n,1 ¨¨¨ �n,d´1 �n,d
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
, U “
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
u1,d u1,d`1 ¨¨¨ u1,nb
.
.
. .
.
. .
.
.
ui,d ui,d`1 ¨¨¨ ui,nb
.
.
. .
.
. .
.
.
uk,d uk,d`1 ¨¨¨ uk,nb
.
.
. .
.
. 0
un´1,d un´1,d`1
.
.
.
un,d 0 ¨¨¨ 0
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(2.60)
where �i,d “ 1 for i “ 1, 2,...,n.
The elements of A “ LU can be generalized as
ai,d`j´i “
kmax
ÿ
k“kmin
�i,d`k´i uk,d`j´k,
" 1 ď i ď n
maxp1, i ´ mLq ď j ď minpi ` mU , nq
*
(2.61)
where kmin “ maxp1, i´mL, j ´mU q, kmax “ minpi, jq´1, and pi, jq’s denote the original
matrix notations.
For i ď j ď minpi ` mU , nq, the row sweep leads to
ui,d`j´i “ ai,d`j´i ´
kmax
ÿ
k“kmin
�i,d`k´i uk,d`j´k, (2.62)
On the other hand, for j ` 1 ď i ď minpj ` mL, nq column sweep yields
�i,d`j´i “ 1
uj,d ˜
ai,d`j´i ´
kmax
ÿ
k“kmin
�i,d`k´i uk,d`j´k
¸
, (2.63)
Since the diagonals of L are �i,d “ 1, this property allows L and U to be stored exactly in
place of A, as shown below:
A “
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
0 ¨¨¨ 0 u1,d u1,d`1 ¨¨¨ u1,nb
.
.
. �2,d´1 u2,d u2,d`1 ¨¨¨ u2,nb
0 .
.
. .
.
. .
.
. .
.
.
�d,1 ¨¨¨ �d,d´1 ud,d ud,d`1 ¨¨¨ ud,nb
.
.
. .
.
. .
.
. .
.
. .
.
.
�k1 ¨¨¨ �k,d´1 uk,d uk,d`1 ¨¨¨ uk,nb
�k`1,1 ¨¨¨ �k`1,d´1 uk`1,d uk,d`1 0
.
.
. .
.
. .
.
. .
.
. .
.
.
�n1 ¨¨¨ �n,d´1 un,d 0 ¨¨¨ 0
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
, (2.64)
The strategy of storing the computed �ij and uij ’s on aij reduces the memory requirement.
A pair of pseudomodules, one for LU-decomposition (BANDED_LU_ DECOMPOSE)
and another for the solution (BANDED_LU_SOLVE) of a real banded linear system, are128  Numerical Methods for Scientists and Engineers
presented in Pseudocodes 2.16 and 2.17. The module BANDED_LU_DECOMPOSE requires
the number of equations (n), a banded matrix in compact form (A), and the lower and upper
bandwidths (mL, mU ) as input. On exit, the triangular matrices L and U are written on
the matrix A.
For the first row, we obtain u1,d`j “ a1,d`j for j “ 1,...,mU , while the second column
below a1,d`1 leads to �i,d`1´i “ ai,d`1´i{u1,d for i “ 2 to i “ mL ` 1. A row sweep from
the diagonal (jmax “ i) to the rightmost non-zero element (jmax) gives a row of U, while
the inner loop over the k variable in Eq. (2.62) gives the summation terms. To obtain a
column of L, the columns are swept starting from the element below diagonal (jmin “ i`1)
and running down to the last non-zero element (jmax) with the summation in Eq.(2.63)
obtained using the inner loop over k. This procedure is successively repeated until the last
row.
Pseudocode 2.16
Module BANDED_LU_DECOMPOSE (n, mL, mU , A)
\ DESCRIPTION: A pseudomodule to find A “ LU decomposition of
\ a banded matrix.
\ USES:
\ MAX/MIN:: Built-in functions to find the max/min of a set of integers.
Declare: an,mL`mU `1, bn
d Ð mL ` 1 \ Find position of diagonal in compact matrix
nb Ð d ` mU \ Find total bandwidth
For “
i “ 1, n‰ \ Loop i: Sweep from to to bottom
jmin Ð i \ Find minimum column index in ith
jmax Ð MINpi ` mU , nq \ Find maximum column index in ith
For “
j “ jmin, jmax‰ \ Loop j: Sweep through row-i left to right
p Ð d ` j ´ i
kmin Ð MAXp1, i ´ mL, j ´ mU q \ Find kmin
kmax Ð MINp i, j q ´ 1 \ Find kmax
For “
k “ kmin, kmax‰ \ Loop k: Accumulating loop
s Ð d ` k ´ i; r Ð d ` j ´ k
aip Ð aip ´ ais ˚ akr \ Save uip on A, uip Ñ aip
End For
End For
jmin Ð i ` 1 \ First row below diagonal
jmax Ð MINpi ` mL, nq \ Last row below diagonal
For “
j “ jmin, jmax‰ \ Loop j: Sweep through column-i below diagonal
p Ð d ` i ´ j
kmin Ð MAXp1, j ´ mL, i ´ mU q \ Find kmin
kmax Ð MINp i, j q ´ 1 \ Find kmax
For “
k “ kmin, kmax‰ \ Loop k: Accumulating loop
s Ð d ` k ´ j
r Ð d ` i ´ k
ajp Ð ajp ´ ajs ˚ akr \ Add ´ajs ˚ akr to ajp
End For
ajp Ð ajp{aid \ Save �jp on A, �jp Ñ ajp
End For
End For
End Module BANDED_LU_DECOMPOSELinear Systems: Fundamentals and Direct Methods  129
Pseudocode 2.17
Module BANDED_LU_SOLVE (n, mL, mU , A, b)
\ DESCRIPTION: A pseudomodule to solve Ax “ b system by LU-decomposition.
\ USES:
\ MAX/MIN:: Built-in functions to find the max/min of a set of integers.
Declare: an,mL`mU `1, bn
d Ð mL ` 1 \ Find position of diagonal in compact matrix
nb Ð d ` mU \ Find total bandwidth length
For “
i “ 1, n‰ \ Forward elimination step to solve Ly “ b
kmin Ð MAXp1, i ´ mLq
kmax Ð i ´ 1
For “
k “ kmin, kmax‰ \ Applying row reduction to below diagonals
s Ð d ` k ´ i
bi Ð bi ´ ais ˚ bk \ Apply Eq. (2.65)
End For
End For
bn Ð bn{and \ Save solution from the last equation on bn
For “
i “ pn ´ 1q, 1,p´1q
‰ \ Backward elimination, Ux “ y
kmin Ð 1
kmax Ð MINpmU , n ´ iq
For “
k “ kmin, kmax‰
s Ð k ` d; r “ k ` i
bi Ð bi ´ ais ˚ br \ Apply Eq. (2.65)
End For
bi Ð bi{aid \ Save the solution on bi
End For
End Module BANDED_LU_SOLVE
The input argument list of BAND_LU _SOLVE includes the right-hand side vector b in
addition to those of BANDED_LU_ DECOMPOSE. The task of finding the solution is broken
into two parts: first, y is found such that Ux “ y; next, x is found from Ly “ b.
The forward substitution can be applied to Ly “ b to obtain y as follows:
yi “bi´
kmax
ÿ
k“kmin
�i,d`k´i yk, i“1, 2,...,n (2.65)
where kmin “ MAXp1, i ´ mLq and kmax “ i ´ 1. Note that the fact that �i,d “ 1 for
all i’s has been taken into account in Eq. (2.65), i.e., �i,d’s have already been used in the
formulation.
The back substitution procedure applied to Ux “ y yields
xn “ yn
un,d
, xi “ 1
ui,d
´
yi ´
kmax
ÿ
k“kmin
ui,k`d yk`i
¯
, i“ pn´1q,..., 3, 2, 1 (2.66)
where kmin “ 1 and kmax “ minpmU , n ´ iq.130  Numerical Methods for Scientists and Engineers
As a measure to reduce the memory requirement, L and U can be
saved on matrix A by setting �ij “ aij and uij “ aij in Eqs. (2.65)
and (2.66). Similarly, setting bi “ yi in the forward elimination
and bi “ xi in the back substitution algorithm eliminates the need
for y and x vectors.
EXAMPLE 2.11: Gauss elimination of Banded Systems
Consider a cantilever beam of constant cross-section of length L that is fixed at one
end and free at the other end, as shown in the figure below. A distributed load of
qpxq “ q0 px{Lq is applied to the beam.
The model for the downward deflection, y, of the beam is given as
EI d4y
dx4 “ qpxq, yp0q “ y1
p0q “ 0, y2pLq “ y2pLq “ 0
where E is the modulus of elasticity and I is the cross-sectional area moment of
inertia. For EI “ 500 MPa, L “ 6 m, q0 “ 1 MN/m and 6 intervals (h “ L{6),
solving the model differential equation using a second-order finite-difference scheme
(see Section 10.9) yields the following linear system:
»
—
—
—
—
—
—
–
7 ´4 1
´4 6 ´4 1
1 ´4 6 ´4 1
1 ´4 6 ´4 1
1 ´4 5 ´2
2 ´4 2
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
–
y1
y2
y3
y4
y5
y6
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“ h4
EI ¨
q0
L
»
—
—
—
—
—
—
–
x1
x2
x3
x4
x5
x6
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Here xi “ ih denotes the position of the nodal points in i “ 1, 2,..., 6 for which the
deflection is computed, i.e., yi “ ypxiq. (a) First apply the banded LU-decomposition
algorithm to find L and U matrices, and (b) use L and U with the forward and
backward substitution algorithms to solve the banded system of equations.
SOLUTION:
The L and U matrices are assumed to have the following compact forms:
L “
»
—
—
—
—
—
—
–
0 01
0 �22 1
�31 �32 1
�4,1 �4,2 1
�5,1 �5,2 1
�6,1 �6,2 1
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
, U “
»
—
—
—
—
—
—
–
u13 u14 u15
u23 u24 u25
u33 u34 u35
u4,3 u44 u45
u53 u54 0
u6,3 0 0
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
,Linear Systems: Fundamentals and Direct Methods  131
Employing Eq. (2.62) to the first row, we find
u13 “ 7, u14 “ ´4, u15 “ 1
Next, we apply Eq. (2.63) to the first column to get
�22u13 “ ´4 ñ �22 “ ´4
7
, �31u13 “ 1 ñ �31 “ 1
7
Revisiting Eq. (2.62), we find
�22u14 ` u23 “ 6 ñ u23 “ 26
7 , �22u15 ` u24 “ ´4 ñ u24 “ ´24
7
Then, by applying Eq. (2.63), we obtain
u14�31 ` u23�32 “ ´4 ñ �32 “ ´12
13
u23�4,1 “ 1 ñ �4,1 “ 1
7
In this manner, going back and forth between rows and columns (i.e., between Eqs.
(2.62) and (2.63)), the following equations are obtained.
u25 “ 1, u35 “ 1,
u45 “ 1, �6,1u4,3 “ 2,
�4,1u23 “ 1, �5,1u33 “ 1,
�32u25 ` u34 “ ´4, �5,2u45 ` u54 “ ´2,
�4,2u35 ` u44 “ ´4, �31u14 ` �32u23 “ ´4,
�4,1u24 ` �4,2u33 “ ´4, �5,1u34 ` �5,2u4,3 “ ´4,
�6,1u44 ` �6,2u53 “ ´4, �31u15 ` �32u24 ` u33 “ 6,
�4,1u25 ` �4,2u34 ` u4,3 “ 6, �5,1u35 ` �5,2u44 ` u53 “ 5,
�6,1u45 ` �6,2u54 ` u6,3 “ 2
Each time, solving one equation for one unknown leads to the following solution:
L “
»
—
—
—
—
—
—
—
—
—
–
0 01
0 ´4{7 1
1{7 ´12{13 1
7{26 ´8{7 1
13{35 ´40{31 1
28{31 ´110{73 1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
, U “
»
—
—
—
—
—
—
—
—
—
–
7 ´4 1
26{7 ´24{7 1
35{13 ´40{13 1
31{14 ´20{7 1
146{155 ´22{31 0
2{73 0 0
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(b) Since Ax “ b can now be expressed as Lz “ b where Uy “ z. Substituting
the numerical values given in the problem, the matrix equation Lz “ b becomes
»
—
—
—
—
—
—
—
—
—
–
0 01
0 ´4{7 1
1{7 ´12{13 1
7{26 ´8{7 1
13{35 ´40{31 1
28{31 ´110{73 1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
—
–
z1
z2
z3
z4
z5
z6
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“ 1
3000
»
—
—
—
—
—
—
—
—
—
–
1
2
3
4
5
6
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
,132  Numerical Methods for Scientists and Engineers
This system is solved using forward substitution, where the unknowns (the elements
of vector z) are calculated one by one, starting from the first to the last equation,
which leads to
z “ 1
3000„
1, 18
7 , 68
13, 65
7 , 2331
155 , 1470
73 jT
Substituting z into Uy “ z, we find
»
—
—
—
—
—
—
—
—
—
–
7 ´4 1
26{7 ´24{7 1
35{13 ´40{13 1
31{14 ´20{7 1
146{155 ´22{31 0
2{73 0 0
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
—
–
y1
y2
y3
y4
y5
y6
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“ 1
3000
»
—
—
—
—
—
—
—
—
—
–
1
18{7
68{13
65{7
2331{155
1470{73
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
This linear system is solved using the back substitution algorithm, which results in
y “ 1
6000
»
—
—
—
—
—
—
–
73
256
515
820
1147
1480
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
–
0.012167
0.042667
0.085833
0.136667
0.191167
0.246667
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Discussion: The solution of a single Ax “ b using LU-decomposition, whether A is
a banded matrix or not, is not economical. The banded linear system avoids dealing
with zeros, which can lead to significant savings in computation time. The elements
of L and U matrices are easily obtained by successive row-column sweeps.
2.11 CLOSURE
Computer applications of linear algebra and matrices are encountered in many fields of
science and engineering. Most applied problems are reduced to a set of simultaneous linear
equations or a linear system. That is why manipulation of matrices and solving systems of
linear algebraic equations have been the core of many methods. In this respect, the most
commonly used direct methods have been covered in this chapter.
As the most popular methods, Gauss elimination and its variants with and without piv￾oting and Gauss-Jordan elimination algorithms are very effective and suitable for moderately
sized simultaneous systems of linear equations. The direct elimination methods reduce equa￾tions to an upper triangular or diagonal matrix through a series of row reductions. Then,
the unknowns are easily obtained by employing the back-substitution procedure. Gauss
elimination is the method of choice for small systems (about a few hundred unknowns) with
few zeros in the coefficient matrices. Specially tailored algorithms, such as symmetric or
tridiagonal systems, and so on, are not only fast but also memory efficient.
The Gauss-Jordan can be economically used to obtain the inverse of a relatively small
matrix as well. Though similar to Gaussian elimination, it zeros out the elements both aboveLinear Systems: Fundamentals and Direct Methods  133
and below the principle diagonal of the augmented matrix. The Gauss-Jordan method can
be successfully employed for overdetermined systems as well.
Decomposition methods factor a coefficient matrix into products of two (or three)
matrices. It is more convenient to implement the LU decomposition methods when the
right-hand side of a linear system changes while the coefficient matrix remains unchanged.
Once the matrix is decomposed, the solution is obtained by first solving Ly “ b (forward
substitution) for y and then solving Ux “ y (back substitution) for x.
For large matrices or systems that are not diagonally dominant, round-off errors can
be a serious problem. Errors in a computed solution are either due to existing errors in
A and b or rounding errors made during the execution of the algorithm or both. The
quality of matrix A strongly affects the error behavior. The condition of a matrix can be
quantitatively determined by using the concept of norms. A large condition number implies
ill-conditioning, which requires further treatment.
The numerical solution of high-order ODEs and elliptic PDEs in regular or irregular
domains results in a banded linear system of equations. Finding numerical solutions to these
kinds of problems is fast and very efficient for moderate-sized problems.
2.12 EXERCISES
Section 2.1 Fundamentals of Linear Algebra
E2.1 For given A and B, find A ` B and A ´ B.
A “
„
3 1 ´2
3 ´4 5 j
2ˆ3
B “
„ 2 ´2 ´4
´1 3 ´1
j
2ˆ3
E2.2 For given A, B, C, and D, find (a) 2A ´ 3B ` C, and (b) determine x and y values that
satisfy D “ xB ` yC.
A “
„
3 1 ´2
1 ´2 ´1
j
, B “
„
1 2 ´1
43 1 j
, C “
„
123
2 ´3 5j
, D “
„
6 12 10
16 ´6 22j
E2.3 For given A and B, find AB and BA.
(a) A “
»
–
2 3
1 ´1
0 4
fi
fl , B “
„ 5 ´247
´6 1 ´3 0j
, (b) A “
„
1 2
3 4j
, B “
„
1 1
4 1j
E2.4 For given A and B, find AB.
(a) A “
„
1 1
2 2j
, B “
„
´1 1
1 ´1
j
, (b) A “
»
–
10 4
20 0
00 0
fi
fl , B “
»
–
000
035
000
fi
fl
E2.5 For given A, B, and C, find (a) pAT CqB, (b) pAT ` BT qC, and (c) pBT CqAT .
A “
»
–
2 1 ´1 1
30 2 ´3
1 ´23 0
fi
fl , B “
»
–
11 0 3
0 1 ´2 1
31 2 1
fi
fl , C “
»
–
´20 1
1 1 ´3
´21 1
fi
fl
E2.6 For given A, B, and C, verify that (a) AB ‰ BA; (b) ApBCq“pABqC; and (c) AT BT “
pBAq
T .
A “
»
–
2 1 ´2
31 2
1 4 ´3
fi
fl , B “
»
–
1 3 ´3
1 4 ´2
2 ´2 5
fi
fl , C “
»
–
21 0
2 1 ´1
10 2
fi
fl134  Numerical Methods for Scientists and Engineers
E2.7 For given A and B, determine a and b that satisfy the following matrix equation:
A “
„
a b
c dj
, B “
„ 4 2
´1 ´4
j
,
´
3A ` BT ¯T
´ 2AT “
„ 7 1
´3 0j
E2.8 Find A such that the following matrix operations are satisfied:
pAT ´ 2Iq
´1 “
„
´3 8
´1 3j
and detpAT ´ 2Iq“´1
E2.9 Determine all the values a, b, c, and d that satisfy the following matrix equation:
„
a b
c dj „1 2
3 1j
´
„
1 2
3 1j „a b
c dj
“
„ 1 0
´1 ´1
j
E2.10 For given A and B, is there a pair of x and y that satisfy A2 ´ B2 “ pA ´ BqpA ` Bq?
A “
»
–
1 2 ´1
x 1 ´2
´2 ´3 2
fi
fl , B “
»
–
413
201
7 y 5
fi
fl
E2.11 Use Sarrus’s rule to calculate the determinants:
(a)
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
1 2 ´1
1 1 ´2
´1 ´1 1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
, (b)
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
1 1 ´1
´1 1 ´1
´1 ´1 1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
, (c)
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
32 1
4 1 ´3
1 ´5 ´2
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
, (d)
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
1 ´5 ´1
6 3 ´1
´4 ´1 3
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
E2.12 Compute the determinants using the cofactor expansion across the first row:
(a)
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
1 ´1 ´1 ´1
1 1 ´1 1
11 1 ´1
1 ´11 1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
, (b)
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
2 ´1 ´3 1
4 1 ´3 1
32 4 ´2
2 ´13 1
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
, (c)
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
1 3 ´2 2
´2 ´3 ´1 4
1 1 21
1 ´345
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
E2.13 Find the unknowns that satisfy the following determinants:
(a)
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
1 2 x
´323
4 12
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
“ 4, (b)
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
5 ´3 1
u ´2 1
´232
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
“ 3, (c)
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
3 ´23 1
5 1 ´3 2
12 3 1
2 1 z ´2
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
“ 0
E2.14 Express the given system of linear equations in matrix equation form, Ax “ b.
(a) "
3x ` 2y “ 1
x ´ 5y “ 7 , (b) "
4y ´ 3x “ 9
5x ` 6y “ 3 (c)
$
&
%
a ` 2b ´ c “ 1
a ´ 3c ´ 2b “ 2
c ´ 3a ` 4b “ 1
(d)
$
&
%
3u ` 2v ´ w “ 0
w ` 2u ´ 2v “ 0
5v ´ w ` 3u “ 1
(e)
$
&
%
3x ` y ` 2z “ 6
´2y ` 4z “ 8
7z “ 7
(f)
$
&
%
5a “ 15
2a ´ 5b “ ´14
4a ´ 3b ´ 2c “ 4
E2.15 Express the given system of linear equations in matrix equation form, Ax “ b.
(a)
$
’’’’’’&
’’’’’’%
5x1 ´ 4x2 “ 9
´2x1 ` 4x2 ´ x3 “ ´8
´2x2 ` 6x3 ´ 3x4 “ 20
´2x3 ` 4x4 ´ x5 “ ´15
´2x4 ` 6x5 ´ 2x6 “ 28
´2x5 ` 4x6 “ ´18
, (b)
$
’’’’’’&
’’’’’’%
2x1 ` 3x2 “ 7,
x1 ` 2x2 ` x3 “ 7,
´x1 ´ x2 ` 2x3 ` 4x4 “ 19,
x2 ` x3 ` 3x4 ´ x5 “ 17,
´x3 ´ x4 ` 3x5 ` 2x6 “ ´14,
x4 ` x5 ` 4x6 “ ´5
E2.16 Apply Cramer’s rule to find solutions for the linear systems given below.
(a) "
2x ´ 3y “ ´15
3x ´ 5y “ ´26 , (b)
$
&
%
4x ` 3y ` 2z “ 4
3x ´ 2y ` 5z “ 27
´2x ` 3y ` z “ ´16
, (c)
$
&
%
x ` y ´ 2z “ ´5
2x ` y ` 3z “ 3
x ` 2y ´ 3z “ ´6Linear Systems: Fundamentals and Direct Methods  135
E2.17 Use Cramer’s rule to find solutions for the linear systems given below.
(a) "
3
?x ` 2{y “ 11
?x ` 5{y “ 8 , (b)
$
&
%
?a ` 2
?
b ´ c2 “ 4
?a ´ 3c2 ` 2
?
b “ 2
c2 ´ 3
?a ` 4
?
b “ 6
, (c)
$
&
%
3xy ` y2 ` 2ez “ 29
2y2 ´ 4ez “ 14
3xy ´ ez “ 17
E2.18 Find all the eigenvalues and eigenvectors of the following matrices.
(a) „
2 ´3
3 ´8
j
, (b) „
3 5
16 1j
, (c)
»
–
12 4 ´18
13 6 ´21
11 4 ´17
fi
fl , (d)
»
–
3 4 ´8
2 7 ´10
2 4 ´7
fi
fl
E2.19 Find all the eigenvalues and eigenvectors of the tridiagonal Toeplitz matrices given below.
(a)
»
—
—
–
´4 1
1 ´4 1
1 ´4 1
1 ´4
fi
ffi
ffi
fl , (b)
»
—
—
–
10 ´9
´1 10 ´9
´1 10 ´9
´1 10
fi
ffi
ffi
fl , (c)
»
—
—
–
3 ´2
´2 3 ´2
´2 3 ´2
´2 3
fi
ffi
ffi
fl
E2.20 Calculate the 1-, 2-, and 8-norms of the following vectors:
(a) x “ r 3 1 ´ 2 1 01 s
T (b) y “ r1 1 ´ 111 s
T , (c) u “ r´1 10 2 1 ´ 2 s
T
E2.21 Calculate the 1-, 2-, and F-norms of the following matrices:
(a) A “
„
1 2
3 4j
, (b) A “
»
–
1 0 ´2
2 ´3 1
4 1 ´1
fi
fl , (c) A “
»
—
—
–
2 1 ´1 0
4 ´32 2
10 2 ´3
1 1 ´2 1
fi
ffi
ffi
fl
E2.22 Compare 1-, 2-, and F-norms of A2 and A2 for the given matrices in E2.21.
E2.23 Use the adjoint method to find the inverse of the matrices given below:
(a) „
2 ´3
3 ´5
j
, (b)
»
–
3 ´6 2
3 1 ´1
4 3 ´2
fi
fl , (c)
»
–
1 ´1 2
3 ´2 1
´231
fi
fl , (d)
»
–
1 1 ´2
2 ´4 3
1 2 ´3
fi
fl
E2.24 For given A, B, and C, what should x and y be for these matrices to be singular?
(a) A “
»
–
x ´1 1
y 1 ´1
4 2 ´1
fi
fl , (b) B “
»
–
2 x ´9
0 y x
1 3 ´4
fi
fl , (c) C “
»
–
223
´2 ´1 x
4y y ´y
fi
fl
E2.25 For given A, verify detpA´1q “ 1{detpAq.
A “
»
–
2 ´2 ´3
´22 2
4 ´5 2
fi
fl
E2.26 Find the condition(s) under which the following matrices are not invertible.
A“
»
–
1 2 ´1
24 2
3 x 2
fi
fl , B“
»
–
3 x ` y ´2
2 x ´ y ´1
´31 5
fi
fl , C“
»
–
3 ´8 2
´2 ´y 1
1 1 y
fi
fl , D“
»
–
x ´2 2
´2 x 2
2 2 x
fi
fl
E2.27 For given B and P, find PBP´1 and P´1BP.
P “
»
–
1 2 ´1
1 1 ´2
´1 ´1 1
fi
fl , B “
»
–
1 1 ´1
´1 1 ´1
´1 ´1 1
fi
fl136  Numerical Methods for Scientists and Engineers
Section 2.2 Elementary Matrix Operations
E2.28 Propose a sequence of row operations on the given matrices to obtain the desired form.
(a)
»
–
357
x ` 2 y ` 3 z ` 4
x ` 1 y ` 2 z ` 3
fi
fl Ñ
»
–
xyz
123
234
fi
fl , (b)
»
—
—
–
x ´y ´x y
y x ´y ´x
x ´y x ´y
yx y x
fi
ffi
ffi
fl Ñ
»
—
—
–
x ´y 0 0
y x 0 0
0 0 x ´y
0 0 y x
fi
ffi
ffi
fl
E2.29 Apply elementary row operations to the homogeneous linear systems given below to find
their reduced-row echelon forms.
(a) " x1 ´ 3x2 ´ 11x3 “ 0
4x1 ` 5 x2 ` 7x3 “ 0 (b)
$
&
%
x1 ` 3x2 ´ 2x3 ` 2x4 “ 0
2x1 ´ x2 ` x3 ` 3x4 “ 0
7x1 ` 7x2 ´ 4x3 ` 12x4 “ 0
(c)
$
’’&
’’%
2x1 ` x2 ´ 3 x3 ` 2x4 ´ 7x5 “ 0
3x1 ´ x2 ` 3x3 ` 2 x4 ` 9x5 “ 0
2x1 ` 2x2 ´ x3 ` 3 x4 ´ 6x5 “ 0
´x1 ` 4 x2 ` 5x3 ` x4 ` 2x5 “ 0
(d)
$
’’&
’’%
x1 ` 2x2 ´ x3 ` x4 ` x5 “ 0
4x1 ´ 2x2 ` 3x3 ` x4 ` 5x5 “ 0
2x1 ` 2x2 ´ 4x3 ` x4 ` 3x5 “ 0
´7x1 ` 8x2 ´ 5x3 ´ 10x5 “ 0
E2.30 Find the augmented matrix and then reduce it to reduced-row echelon form to balance
the following chemical equations:
(a) x1pC2H5OHq ` x2pO2q Ñ x3pCO2q ` x4pH2O)
(b) x1pH2(SO4qq ` x2pFe(OH3q Ñ x3pFe2pSO4q3 ` x4pH2O)
(c) x1pFe2pSO4q3q ` x2(KOH) Ñ x3pK2SO4q ` x4pFe(OH)3q
(d) x1(HIO3q ` x2(FeI2q ` x3(HCl) Ñ x4pFeCl3q ` x5(ICl) ` x6pH2O)
(e) x1p(KMnO4q ` x2(HCl) Ñ x3(KCl) ` x4(MnCl2q ` x5pH2O) ` x6pClq2q
(f) x1pC2H2Cl4q ` x2pCa(OHq2q Ñ x3pC2HCl3q ` x4(CaCl2q ` x5pH2Oq
E2.31 The traffic flow in a district with several one-way
streets is depicted in Fig. E2.31. Obtain the general flow
pattern for the network, assuming that the total flow
into the network (or junction) is equal to the total flow
out of the network (or junction). Apply row reductions
to find the minimum number of vehicles for each road.
Fig. E2.31
E2.32 The traffic pattern for a roundabout is shown in
Fig. E2.32. What is the smallest possible value for x1?
Note that the net flow rate in and out of the roundabout
is 700 vph (vehicles per hour).
Fig. E2.32Linear Systems: Fundamentals and Direct Methods  137
E2.33 Find all solutions to the simultaneous equations by reducing the augmented matrix into
reduced-row echelon form.
2x1 ´ 2x2 ` 4x3 ` 3x4 ` x5 “ 5
5x1 ` x2 ´ x4 ´ 3x5 “ ´7
´x1 ` 4x2 ´ 2x3 ` 3x4 ` x5 “ 8
´3x1 ´ 4x2 ` 5x3 ` 7x4 ` 10x5 “ 35
E2.34 Raw materials with different compositions from three sources are processed in an enrich￾ment plant to obtain two highly concentrated products containing either B or C. The process
flowchart with compositions in w% is given in Fig. E2.34.
Fig. E2.34
The mass and species balance are given as
Overall balance: m9 1 ` m9 2 ` m9 3 “ m9 4 ` m9 5
For A: 23m9 1 ` 55m9 2 ` p0%qm9 3 “ p100 ´ xq59 ` 21.72p91q
For B: 40m9 1 ` 45m9 2 ` 32m9 3 “ 59x ` 91p78.28 ´ yq
For C: 37m9 1 ` p0qm9 2 ` 68m9 3 “ 91y
and at the enrichment stage:
For A: 17.9pm9 1 ` m9 2 ` m9 3q“m9 4p100´ xq ` 21.72m9 5
For B: 37pm9 1 ` m9 2 ` m9 3q “ xm9 4 ` p78.28 ´ yqm9 5
For C: 45.1pm9 1 ` m9 2 ` m9 3q “ xm9 4 ` p78.28 ´ yqm9 5
(a) Write the augmented matrix for the linear (over-determined) system; (b) use row reduction
to (if it is possible) obtain the steady flow rates of each feed and the composition of A and B at
m9 4 and B and C at m9 5.
Section 2.3 Matrix Inversion
E2.35 Use the adjoint method to invert the following matrices:
(a) „
5 ´3
3 ´2
j
, (b) „
4 1
3 2j
, (c)
»
–
12 1
1 1 ´2
2 2 ´3
fi
fl , (d)
»
–
32 2
1 2 ´3
2 2 ´1
fi
fl , (e)
»
–
312
141
232
fi
fl
E2.36 Use the Gauss-Jordan method to invert the following matrices:
(a)
»
–
2 ´5 ´1
´1 ´4 2
´21 2
fi
fl (b)
»
–
1 0 ´3
2 2 ´3
49 1
fi
fl (c)
»
–
1 44
2 23
´352
fi
fl , (d)
»
–
12 1
1 1 ´2
2 2 ´3
fi
fl
(e)
»
–
32 2
1 ´2 ´3
10 1 ´1
fi
fl , (f)
»
–
312
241
231
fi
fl , (g)
»
–
1 1 ´2
2 2 ´5
1 2 ´3
fi
fl , (h)
»
–
122
111
323
fi
fl
E2.37 Use the Gauss-Jordan method to invert the following matrices:
(a)
»
—
—
–
48 1 4
1 4 ´2 2
36 4 ´3
13 1 ´2
fi
ffi
ffi
fl , (b)
»
—
—
–
1 3 ´1 2
2 2 ´2 ´3
´2 ´5 3 ´1
1122
fi
ffi
ffi
fl , (c)
»
—
—
–
2 ´2 ´1 ´2
´32 1 2
1023
2312
fi
ffi
ffi
fl138  Numerical Methods for Scientists and Engineers
E2.38 Invert the triangle matrix A using the Gauss-Jordan method (a) without rounding frac￾tional numbers, (b) using the approximate matrix found by rounding fractional numbers, (c) Verify
AA´1 “ I for both cases.
A “
»
—
—
–
1 0 00
1{21 00
1{3 1{410
1{4 1{5 1{6 1
fi
ffi
ffi
fl , A –
»
—
—
–
1 0 00
0.5 1 00
0.333 0.25 1 0
0.250 0.20 0.167 1
fi
ffi
ffi
fl
Section 2.4 Triangular Systems of Linear Equations
E2.39 Apply the forward or backward substitution algorithms to find the solution to the following
linear systems:
(a)
»
–
400
140
364
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
12
´5
13
fi
fl , (b)
»
–
217
043
003
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
15
´6
6
fi
fl
(c)
»
—
—
–
2 ´22 4
054 ´1
0033
000 ´3
fi
ffi
ffi
fl
»
—
—
–
x1
x2
x3
x4
fi
ffi
ffi
fl “
»
—
—
–
16
2
9
´3
fi
ffi
ffi
fl , (d)
»
—
—
–
3 0 00
2 ´300
3 7 40
6 ´535
fi
ffi
ffi
fl
»
—
—
–
x1
x2
x3
x4
fi
ffi
ffi
fl “
»
—
—
–
6
10
8
19
fi
ffi
ffi
fl
E2.40 Consider the truss system shown in Fig. E2.40. The equilibrium of the horizontal and
vertical forces yields the following (9 ˆ 9) linear system:
»
—
—
—
—
—
—
—
—
—
—
—
—
–
1 c 000 0 0 00
0 s 000 0 0 00
´1001 c 0 0 00
0 010 s 0 0 00
0 c 000 ´1 0 00
0 s 100 0 0 00
0 0010 0 ´100
0 0000 0 0 10
0 000 s 0 01 s
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
—
—
—
—
–
F1
F2
F3
F4
F5
F6
F7
F8
F9
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
—
—
—
—
—
–
0
r
0
0
0
0
0
0
0
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
where c “ 0.6, s “ 0.8, and r “ ´750. Apply elementary row operations to convert the augmented
matrix into a lower triangular matrix, and then solve the system of linear equations by employing
the forward substitution algorithm.
Fig. E2.40
Section 2.5 Gauss Elimination Methods
E2.41 The following linear system is from the discretization of an ordinary differential equation.
Find the solution of the linear system using the Gauss elimination method.
y1 ` 3y2 “ 10, y1 ` 4y2 ` 2y3 “ 23, y2 ` 3y3 ` 5y4 “ 53, y3 ` 7y4 “ 54Linear Systems: Fundamentals and Direct Methods  139
E2.42 Obtain the solution of the following systems of linear equations using the Gauss elimination
method without pivoting.
(a)
$
&
%
x ` 3y ´ z “ 8
2x ´ 2y ` z “ ´11
3x ` 4y ´ 5z “ 11
, (b)
$
&
%
x ` 2y ´ z “ ´5
4x ` y ` z “ 6
2x ` 2y ´ 3z “ ´5
, (c)
$
&
%
x ` 3y ´ 2z “ ´6
2x ´ z ` 3y “ ´3
x ` 2z ` y “ 2
(d)
$
&
%
3x ` y ` 2z “ 7
x ` 3y ` z “ ´6
4x ` 3y ´ 2z “ 9
, (e)
$
&
%
4x ` 3y ` 2z “ 4
3x ´ 2y ` 5z “ 27
2x ´ 3y ´ z “ 16
, (f)
$
&
%
x ` y ´ 2z “ ´5
2x ` y ` 3z “ 3
x ` 2y ´ 3z “ ´6
E2.43 Consider the following partial fraction decomposition:
fpxq “ 5x3 ´ x2 ` 14x ` 4
px2 ` 2qpx2 ` 4q “ Ax ` B
x2 ` 2 `
Cx ` D
x2 ` 4
Clear the denominator and determine the set of four equations with four undetermined coefficients.
Then, apply the Gauss elimination method to find the unknowns.
E2.44 Applying Kirchoff’s current law to the currents flowing in each branch of the circuit given
in Fig. E2.44 yields
i1R1 ` R3 pi1 ´ i2q ` R4 pi1 ´ i3q “ 15 V
R3 pi2 ´ i1q ` R2i2 ` R5 pi2 ´ i3q “ 0
R4 pi3 ´ i1q ` R5 pi3 ´ i2q ` R6i3 “ 0
Use the Gauss elimination method to solve the resulting system of equations for the currents.
Given: R1 “ R2 “ 1 Ω, R3 “ 6 Ω, R4 “ 3 Ω, R5 “ 4 Ω, R6 “ 10 Ω.
Fig. E2.44 Fig. E2.45
E2.45 Consider the circuit shown in Fig. E2.45. Kirchoff’s voltage law for the loop currents yields
the following system of linear equations:
Loop 1: R1i1 ` R3pi1 ´ i2q ` R2pi1 ´ i3q “ 0
Loop 2: R3pi2 ´ i1q ` R4i2 ` R5pi2 ´ i4q “ 0
Loop 3: R2pi3 ´ i1q ` R6pi3 ´ i4q “ V1
Loop 4: R6pi4 ´ i3q ` R5pi4 ´ i2q “ V2
Find the solution to the linear system using the Gauss elimination method. Given: R1 “ 3 Ω,
R2 “ 5 Ω, R3 “ R4 “ 8 Ω, R5 “ R6 “ 2 Ω, V1 “ 14 V , V2 “ 22 V .140  Numerical Methods for Scientists and Engineers
E2.46 Apply the Gauss-Jordan method to obtain the solution to the following matrix equations:
(a)
»
—
—
–
1 ´2 ´2 ´5
3 1 ´1 4
´62 7 1
2.5 1 ´1.5 3
fi
ffi
ffi
fl
»
—
—
–
x1
x2
x3
x4
fi
ffi
ffi
fl“
»
—
—
–
´20
18
´6
15
fi
ffi
ffi
fl , (b)
»
—
—
—
—
–
112 ´1 3
´1 4 ´32 2
11 ´1 5 ´2 3
8 3 ´9 7 ´3
4 1 ´26 1
fi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
–
x1
x2
x3
x4
x5
fi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
–
12
22
14
26
30
fi
ffi
ffi
ffi
ffi
fl
E2.47 A third-degree polynomial passes through the points p´2, ´35q, p´1, ´8q, p1, 10q, and
p3, 20q. Forcing the polynomial to satisfy all four points leads to a linear system of four equations
and four unknowns. Establish the linear system and solve it using the Gauss-Jordan elimination
method.
E2.48 Two weights are suspended with ropes, as shown in Fig. E2.48. The static equilibrium
equations can be expressed as
´
?3
2
T1 ` T2 “ 0, 1
2
T1 ´ 25 “ 0, ´T2 `
1
2
T3 “ 0,
?3
2
T3 ´ 75 “ 0
Apply elementary row operations to obtain the reduced-row echelon form of the augmented matrix
to find the solution to the linear system. Does this system have a solution?
Fig. E2.48
E2.49 The three trolleys interconnected by springs are subjected to P1, P2, and P3 loads, as
shown in Fig. E2.49.
Fig. E2.49
The displacements of the trolleys from their equilibrium positions are governed by
P1 ` k1p0 ´ x1q ` k4px2 ´ x1q ` k3px3 ´ x1q “ 0
P2 ` k5p0 ´ x2q ` k4px1 ´ x2q ` k6px3 ´ x2q “ 0
P3 ` k2p0 ´ x3q ` k3px1 ´ x3q ` k6px2 ´ x3q ` k7p0 ´ x3q “ 0
where x’s are displacements and k’s are the spring constants. (a) Express the equilibrium equations
in Ax “ b form; (b) find the displacements using the Gauss-Jordan method using the furnished
data. Given: k1 “ 750 N/m, k2 “ 5250 N/m, k3 “ 3000 N/m, k4 “ 1000 N/m, k5 “ 1670 N/m,
k6 “ 3500 N/m, k7 “ 1500 N/m, P1 “ 825 N, P2 “ 3895 N, and P3 “ 1025 N.
E2.50 Apply the Gauss elimination method with partial pivoting to solve the following linear
systems:
(a)
»
–
134
5 1 ´2
´28 2
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
9
´1
´20
fi
fl , (b)
»
–
3 ´3 1
3 2 ´2
41 2
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
´7
´8
´3
fi
flLinear Systems: Fundamentals and Direct Methods  141
E2.51 Consider the separation process shown in Fig. E2.51. The inlet mass flow rate is m9 1 “ 18
kg/h, and the mass fractions of each species at the inlet and outlet streams are given in the figure.
Find the mass flow rate of each stream using the Gauss elimination method with partial pivoting.
The mass conservation for the stream and the species is given below:
m9 2 ` m9 3 ` m9 4 “ m9 1 “ 18
0.09m9 2 ` 0.48m9 3 ` 0.18m9 4 “ 0.18p18q
0.82m9 2 ` 0.16m9 3 ` 0.10m9 4 “ 0.51p18q
Fig. E2.51
E2.52 (a) Use Kirchhoff’s voltage law to obtain a
linear system for the loop currents of the circuit in
Fig. E2.52, (b) use the given data to solve the sys￾tem using the Gauss elimination with partial piv￾oting.Given: R1 “ 1 Ω, R2 “ R3 “ R10 “ 2 Ω,
R6 “ 5 Ω, R4 “ R7 “ R8 “ 3 Ω, R5 “ R9 “ 4 Ω,
V1 “ 23 V , V2 “ 18 V , V3 “ 24 V , V4 “ 48 V .
Fig. E2.52
E2.53 Apply Kirchhoff’s current law to the
electric circuit is given in Fig. E2.53 to obtain
a system of linear equations for the currents in
each branch. Then solve the system of equations
using a suitable direct method. Given: R1 “ 5 Ω,
R2 “ 2 Ω, R3 “ 4 Ω, R4 “ 3 Ω, R5 “ 2 Ω,
R6 “ 5 Ω.
Fig. E2.53
Section 2.6 Computing Determinants
E2.54 Find the determinants of the matrices in E2.11 by constructing the upper triangular matrix
through elementary row operations.
E2.55 Find the determinants of the matrices in E2.12 by constructing the upper triangular matrix
through elementary row operations.
Section 2.7 Ill-conditioned Matrices and Linear Systems
E2.56 Find the �1, �2, and Frobenius norm-based condition numbers of the following matrices:
(a) „
1 0.3
3 1 j
, (b) „
99 98
98 97j
, (c)
»
–
2 ´0.019 4
2 4.99 3
151
fi
fl , (d)
»
–
2.9 1.9 3.89
1.07 4.05 1.9
´2.1 4.9 ´1.95
fi
fl142  Numerical Methods for Scientists and Engineers
E2.57 Use the definition, Eq. (2.31), to calculate the spectral (�2´norm-based) condition number
of the matrices in E2.56.
E2.58 Use the expression proposed by Guggenheimmer et al. [10], Eq. (2.32), to estimate the
spectral condition numbers of the matrices in E2.56. Compare your estimates with those obtained
in E2.57.
E2.59 Estimate the spectral (�2´norm-based) condition number of the matrices in E2.56 using
the two-step method, Eqs. (2.35) and (2.36), and compare your estimates with those obtained in
E2.58. Hint: Choose vector elements of A as ui “ p´1q
i`1
, i “ 1, 2,...,n.
E2.60 Consider the following tridiagonal Toeplitz matrix:
A “
»
—
—
–
3.1 1.9
1.9 3.1 1.9
1.9 3.1 2
2 3.24
fi
ffi
ffi
fl
Calculate the �1´norm-based condition number by using (a) the definition—Eq. (2.31), (b) the
two-step method, i.e., Eqs. (2.35) and (2.36), and (c) Eq. (2.33).
E2.61 Consider the matrix A in E2.60. (a) Apply row equilibration such that each row is a unit
vector; (b) calculate the �1´norm-based condition number of the equilibrated matrix M using the
definition; (c) estimate the same condition number using the two-step method; and (d) estimate
the spectral condition number using Eq. (2.34).
E2.62 For given A, (a) calculate the Frobenius´ and spectral (�2´ norm-based) condition num￾bers; (b) estimate the spectral condition number using Eq. (2.33); (c) is the matrix ill-conditioned?
A “
»
—
—
—
—
–
1 0 00
1{22 1 0 0
1{4 1{410
1{8 1{8 1{8 1
fi
ffi
ffi
ffi
ffi
fl
E2.63 Investigate the condition of Ax “ b and the corresponding equilibrated system Mx “ d
for the following coefficient matrices:
(a) A“
»
–
11 22 ´6
´125 350 55
0.03 ´0.1 0.05
fi
fl, (b) A“
»
–
´187 66 99
´0.05 0.1 0.05
6 19 ´3
fi
fl, (c) A“
»
–
0.1 66 99
0.02 0.3 1.2
0.1 190 ´2
fi
fl
E2.64 To find the solution for the given linear system of equations, (a) use the simplified estimate
for the spectral condition number of the coefficient matrix (A) to determine whether the linear
system is ill-conditioned or not; (b) repeat Part (a) for the matrix generated by equilibrating the
rows to a unit vector; (c) solve the system with the Gauss-Jordan elimination method, assuming
exact arithmetic (whole fractions without rounding); (d) solve the system by rounding the fractions
numbers and carrying out the arithmetic operations to four decimal places; (e) compare the
solutions obtained in Parts (c) and (d) and interpret the results.
»
—
—
—
—
—
—
—
–
1 1{2 1{3 1{4 1{5
1{2 1{3 1{4 1{5 1{6
1{3 1{4 1{5 1{6 1{7
1{4 1{5 1{6 1{7 1{8
1{5 1{6 1{7 1{8 1{9
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
–
x1
x2
x3
x4
x5
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
–
59{60
11{20
169{420
34{105
689{2520
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
E2.65 (a) Solve the given linear system by the Gauss-Jordan elimination using the exact arith￾metic (whole fractions without rounding); (b) repeat Part (a) after rounding the fractions toLinear Systems: Fundamentals and Direct Methods  143
three-decimal places; (c) compare the solutions obtained in Parts (a) and (b) and interpret the
results; (d) estimate the spectral condition number of the rounded matrix and the matrix whose
rows were equilibrated by unit vector.
«
1{3 4{9
1{7 3{16ff «x1
x2
ff
“
« 1{9
3{56ff
E2.66 Consider the linear system Ax “ b, where
A “
»
–
1 1{3 1{5
1{3 1{5 1{7
1{5 1{7 1{9
fi
fl , b “
»
–
23{15
71{105
143{315
fi
fl
(a) Investigate if the coefficient matrix is ill-conditioned or not using the simple criterion for the
spectral condition number; (b) solve the system of equations for three- and five-significant digits
(chopped) numbers of A and b; and (c) compare and explain the deviation of the results from
the true solution of x “ r 111 s
T .
E2.67 Consider the linear system Ax “ b, where
A “
»
—
—
–
1 1{3 1{5 1{7
1{3 1{5 1{7 1{9
1{5 1{7 1{9 1{11
1{7 1{9 1{11 1{13
fi
ffi
ffi
fl , b “
»
—
—
–
176{105
248{315
1888{3465
3800{9009
fi
ffi
ffi
fl
(a) Investigate if matrix A is ill-conditioned or not using the simple criterion for the spectral
condition number; (b) solve the system of equations for three- and five-significant digits (chopped)
numbers of A and b; and (c) compare and explain the deviation of the results from the true solution
of x “ r 1111 s
T .
Section 2.8 Decomposition Methods
E2.68 Use the Doolittle method to find the LU-decomposition of the following matrices:
(a)
»
–
2 ´2 4
188
´3 9 ´1
fi
fl , (b)
»
–
´3 2 ´1
6 ´6 ´2
´99 7
fi
fl , (c)
»
–
´2 ´2 ´3
8 6 14
4 10 ´6
fi
fl ,
(d)
»
—
—
–
8 4 ´8 12
2 13 ´10 11
2 16 0 17
2 10 ´8 13
fi
ffi
ffi
fl , (e)
»
—
—
–
´44 8 8
´2 14 ´2 10
4 8 ´18 ´4
´2 10 6 14
fi
ffi
ffi
fl
E2.69 Use the Doolittle method to find the LU-decomposition of matrix A and then find the
matrix product UL.
A “
»
—
—
–
2 ´1 ´2 2
6 ´2 ´9 7
´2 3 ´2 ´2
2 3 ´16 12
fi
ffi
ffi
fl
E2.70 Use the Doolittle method to find the LU-decomposition of the following matrix, and then
find the matrix B that satisfies BA “ U.
»
—
—
–
a 0 s 0
01 0 s
ax 1 xp1 ` sq s
as b s2 bp1 ` sq
fi
ffi
ffi
fl
E2.71 The inverse of a matrix can also be obtained by A´1 “ U´1L´1, where L and U are
factored matrices from LU-decomposition. Having decomposed A, apply the Gauss-Jordan method144  Numerical Methods for Scientists and Engineers
to find the inverses of U and L, and consequently calculate U´1L´1.
A “
»
—
—
–
1 3 ´2 2
´2 ´5 ´1 ´1
1121
1 ´35 3
fi
ffi
ffi
fl
E2.72 The following systems of linear equations, in matrix form, yield the coefficient matrices in
E2.68. To solve these systems, apply forward and backward substitution operations using L and
U matrices found in E2.68.
(a)
$
&
%
2x1 ´ 2x2 ` 4x3 “ 28
x1 ` 8x2 ` 8x3 “ 26
´3x1 ` 9x2 ´ x3 “ ´29
,
.
- , (b)
$
&
%
´2x1 ´ 2x2 ´ 3x3 “ ´15
8x1 ` 6x2 ` 14x3 “ 58
4x1 ` 10x2 ´ 6x3 “ 18
,
.
- ,
(c)
$
’’&
’’%
8x1 ` 4x2 ´ 8x3 ` 12x4 “ ´72
2x1 ` 13x2 ´ 10x3 ` 11x4 “ ´70
2x1 ` 16x2 ` 17x4 “ 1
2x1 ` 10x2 ´ 8x3 ` 13x4 “ ´69
,
//.
//-
, (d)
$
’’&
’’%
8x1 ` 4x2 ´ 8x3 ` 12x4 “ 0
2x1 ` 13x2 ´ 10x3 ` 11x4 “ ´78
2x1 ` 16x2 ` 0 ` 17x4 “ ´94
2x1 ` 10x2 ´ 8x3 ` 13x4 “ ´34
,
//.
//-
E2.73 Decompose the following symmetric matrices using Cholesky’s method.
(a)
»
–
46 4
6 10 4
4 4 12
fi
fl , (b)
»
–
1 2 ´3
2 13 6
´3 6 29
fi
fl , (c)
»
–
4 2 ´4
2 10 ´11
´4 ´11 14
fi
fl ,
(d)
»
—
—
–
4 2 ´6 4
2 10 ´9 ´1
´6 ´9 14 ´6
4 ´1 ´6 25
fi
ffi
ffi
fl , (e)
»
—
—
–
12 0 1
28 6 6
0 6 13 4
16 4 7
fi
ffi
ffi
fl , (f)
»
—
—
–
1 1 ´1 ´3
1 5 ´7 1
´1 ´7 14 ´5
´3 1 ´5 18
fi
ffi
ffi
fl
E2.74 Use A “ LLT decomposition to solve the given system of linear equations. Note that the
coefficient matrices are given in E2.73(a) and (f).
(a)
$
&
%
4x1 ` 6x2 ` 4x3 “ ´9
6x1 ` 10x2 ` 4x3 “ ´73
5
4x1 ` 4x2 ` 12x3 “ ´38
5
,
.
- , (b)
$
’’&
’’%
x1 ` x2 ´ x3 ´ 3x4 “ ´49
x1 ` 5x2 ´ 7x3 ` x4 “ ´69
´x1 ´ 7x2 ` 14x3 ´ 5x4 “ 81
´3x1 ` x2 ´ 5x3 ` 18x4 “ 170
,
//.
//-
Fig. E2.75 Fig. E2.76
E2.75 Applying Kirchhoff’s law of voltage (KLV) to the circuit in Fig. E2.75 leads to the following
system of equations for the loop currents:
»
—
—
–
18 ´2 ´6 ´4
´2 9 ´4 0
´6 ´4 15 ´5
´4 0 ´5 12
fi
ffi
ffi
fl
»
—
—
–
i1
i2
i3
i4
fi
ffi
ffi
fl “
»
—
—
–
V1
0
0
V2
fi
ffi
ffi
fl
where V1 and V2 voltages can be changed. Use Cholesky’s method to decompose the coefficientLinear Systems: Fundamentals and Direct Methods  145
matrix and solve the linear system using a forward and a backward elimination procedure for
V1 “ 16.8 and V2 “ 18.6 V.
E2.76 Applying Kirchhoff’s law of voltage (KLV) to the circuit shown in Fig. E2.76 leads to the
following system of equations for the loop currents:
»
—
—
—
—
–
17 ´5 ´20 0
´5 13 ´2 ´1 ´2
´2 ´27 0 ´3
0 ´10 5 ´2
0 ´2 ´3 ´2 9
fi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
–
i1
i2
i3
i4
i5
fi
ffi
ffi
ffi
ffi
fl “
»
—
—
—
—
–
V1
0
V2
´V3
V4
fi
ffi
ffi
ffi
ffi
fl
where the voltages may be subject to change. Apply Cholesky’s method to decompose the coeffi￾cient matrix, and then solve the linear system with forward and back substitution procedures for
the case with V1 “ 6, V2 “ 11, V3 “ 15, and V4 “ 11 V.
E2.77 The truss shown in Fig. E2.77 is applied vertically and horizontally loads for X and Y
at node (2). The system members have the same cross section and elastic modulus, yielding the
value of EA “ 2.5 ˆ 106 kN. The nodal displacements of the system can be obtained by solving
the equilibrium equations are given in matrix form as
»
—
—
–
2.14443 ´0.44721 ´0.89443 0.44721
´0.44721 0.22361 0.44721 ´0.22361
´0.89443 0.44721 2.68330 ´0.44721
0.44721 ´0.22361 ´0.44721 3.17082
fi
ffi
ffi
fl
»
—
—
–
d1x
d1y
d2x
d2y
fi
ffi
ffi
fl “
»
—
—
–
0
0
´X
´Y
fi
ffi
ffi
fl ˆ 10´6
where dmx and dmy denote the x and y displacements of node m. Noting that the distances in
the figure are given in meters, apply the Doolittle-LU decomposition once and determine the
nodal displacements for the following loads: (a) pX, Y q“p50, 50q kN ; (b) pX, Y q“p75, 25q kN ;
(c) pX, Y q“p25, 75q kN. Fig. E2.77
E2.78 Find the inverse of the given matrices by A´1 “ U´1L´1.
(a)
»
–
10 ´9 14
2 ´2 3
9 ´8 13
fi
fl , (b)
»
–
´1 ´8 5
1 5 ´3
0 ´2 1
fi
fl , (c)
»
—
—
–
´1 ´10 0
5 4 01
41 32 2 6
´27 ´21 ´1 ´4
fi
ffi
ffi
fl
Section 2.9 Tridiagonal Systems
E2.79 Apply the Thomas algorithm to solve the following tridiagonal system of equations:
(a)
$
’’&
’’%
x1 ` 3x2 “ 10
x1 ` 4x2 ` 2x3 “ 23
x2 ` 3x3 ` 5x4 “ 53
x3 ` 7x4 “ 54
,
//.
//-
, (b)
$
’’’’&
’’’’%
2x1 ` x2 “ 1
2x1 ` 3x2 ´ x3 “ ´1
x2 ` 4x3 ` 3x4 “ 5
´ 2x3 ` 2x4 ` x5 “ 2
´x4 ` 2x5 “ ´6
,
////.
////-146  Numerical Methods for Scientists and Engineers
E2.80 Apply the Thomas algorithm to solve the following tridiagonal system of equations:
»
—
—
—
—
—
—
–
1 2
2 3 ´2
2 3 ´2
2 3 ´2
2 3 ´2
2 3
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
–
x1
x2
x3
x4
x5
x6
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
–
´1
´5
8
´8
11
´3
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
E2.81 Find the Crout’s LU-decomposition of the coefficient matrix of the linear system (by
Ax “ b) presented in E2.80 and then solve the tridiagonal systems (Ly “ b and Ux “ y) to
obtain the solution.
E2.82 Use Crout’s algorithm to decompose the following tridiagonal matrices as LU.
(a)
»
—
—
–
26 0 0
1 6 ´6 0
0 ´2 5 ´1
00 3 ´1
fi
ffi
ffi
fl , (b)
»
—
—
–
1 ´200
2 ´1 ´9 0
0 2 ´8 8
00 10
fi
ffi
ffi
fl , (c)
»
—
—
–
3 12 0 0
1790
0288
0 0 4 21
fi
ffi
ffi
fl
E2.83 Use Crout’s algorithm to decompose the following tridiagonal matrices as LU.
(a)
»
—
—
—
—
–
360 00
´1 0 ´200
022 40
001 58
000 ´2 1
fi
ffi
ffi
ffi
ffi
fl
, (b)
»
—
—
—
—
–
12 0 00
1 6 12 0 0
01 6 90
00 1 78
00 0 29
fi
ffi
ffi
ffi
ffi
fl
, (c)
»
—
—
—
—
–
2 8 00 0
2 11 6 0 0
0 1 32 0
0 0 14 8
0 0 0 2 11
fi
ffi
ffi
ffi
ffi
fl
E2.84 Use the following matrices (and notations) to develop the Doolittle algorithm to decompose
a tridiagonal matrix as A “ LU. Make the algorithm as cpu-time and memory-efficient as possible.
»
—
—
—
—
—
–
d1 a1
b2 d2 a2
... ... ...
bn´1 dn´1 an´1
bn dn
fi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
–
1
�2 1
... ...
�n´1 1
�n 1
fi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
–
u1 a1
u2 a2
... ...
un´1 an´1
un
fi
ffi
ffi
ffi
ffi
ffi
fl
E2.85 Apply the Doolittle algorithm to decompose the following tridiagonal matrices as A “ LU.
(a)
»
—
—
–
41 0 0
43 1 0
0 8 11 1
0 0 21 5
fi
ffi
ffi
fl , (b)
»
—
—
–
3200
6820
0262
0058
fi
ffi
ffi
fl , (c)
»
—
—
–
3 ´20 0
6 5 ´1 0
0 18 1 ´2
0 0 15 2
fi
ffi
ffi
fl
E2.86 Apply the Doolittle algorithm to decompose the following tridiagonal matrices as A “ LU.
(a)
»
—
—
—
—
–
33 0 0 0
68 2 0 0
0 6 10 1 0
0 0 16 8 2
0 0 0 12 11
fi
ffi
ffi
ffi
ffi
fl
, (b)
»
—
—
—
—
–
420 0 0
441 0 0
045 2 0
003 7 4
0 0 0 10 9
fi
ffi
ffi
ffi
ffi
fl
, (c)
»
—
—
—
—
–
4100 0
8310 0
0482 0
0086 4
0 0 0 4 10
fi
ffi
ffi
ffi
ffi
fl
E2.87 Consider the linear system Ax “ b, where b “ r1, 1, 1, 1, 1s
T and A is given in E2.86.
Apply the LU decomposition to solve the linear system of equations.Linear Systems: Fundamentals and Direct Methods  147
E2.88 Use the Doolittle algorithm to decompose the following tridiagonal matrices as A “ LU.
(a)
»
—
—
–
96 0 0
65 3 0
0 3 13 8
0 0 8 41
fi
ffi
ffi
fl , (b)
»
—
—
–
4600
6 18 3 0
0323
0 0 3 25
fi
ffi
ffi
fl , (c)
»
—
—
–
96 0 0
6 13 9 0
0 9 25 12
0 0 12 25
fi
ffi
ffi
fl
(d)
»
—
—
—
—
–
42 000
2 10 12 0 0
0 12 17 1 0
00 154
0 0 0 4 13
fi
ffi
ffi
ffi
ffi
fl
, (e)
»
—
—
—
—
–
12 000
2 20 4 0 0
0 4 10 6 0
00 651
0 0 0 1 26
fi
ffi
ffi
ffi
ffi
fl
, (f)
»
—
—
—
—
–
93 0 0 0
32 4 0 0
0 4 17 4 0
0 0 4 25 3
0 0 0 3 26
fi
ffi
ffi
ffi
ffi
fl
,
E2.89 Consider a symmetric, positive-definite linear system Ax “ b, where b “ r1, 1, 1, 1, 1s
T and
A is given in E2.88(d)-(f). Use the lower triangular matrix L obtained by applying the Cholesky
decomposition to solve the linear system using forward and backward substitutions.
E2.90 Consider the five-stage separation process depicted in Fig. E2.90. Stream-1 V (kmole/s) is
separated at a mole fraction of y6. Stream-2 is L (kmole/s) at a concentration of x0. The state
variables are identified in the figure. The mass balance on each stage (i) leads to V pyi`1 ´ yiq “
Kpxi ´ xi´1q.
Fig. E2.90
Assuming the streams leaving the stage are in thermodynamic equilibrium, we may write yi “ Kxi,
where K is called the equilibrium ratio. The mass balances for each stage can be expressed in
matrix form as
»
—
—
—
—
–
1`β ´1
´1 1`β ´1
´1 1`β ´1
´1 1`β ´1
´1 1`β
fi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
–
x1
x2
x3
x4
x5
fi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
–
βx0
0
0
0
y6{K
fi
ffi
ffi
ffi
ffi
fl
where β “ L{KV . To solve the system for an arbitrary set of x0, y6, K, and β values, first find
L by Cholesky decomposition of the coefficient matrix and then obtain the numerical solution for
the case with x0 “ 0.8, y6 “ 0.3, β “ 1.25, and K “ 1.2.
Section 2.10 Banded Linear Systems
E2.91 A weight of 10 kN is suspended with ropes, as shown in Fig. E2.91. The system is in a
state of static equilibrium. (a) derive the equilibrium equations and express them in matrix form;
(b) solve the resulting system of banded linear equations by Gauss elimination.
E2.92 A weight of 50 N is suspended with ropes, as illustrated in Fig. E2.92. Assuming the system
is in a state of static equilibrium, (a) derive all possible equilibrium equations, (b) express them
in matrix equation form, and (c) solve the linear system with naïve Gauss elimination.148  Numerical Methods for Scientists and Engineers
Fig. E2.91 Fig. E2.92
E2.93 Consider the three-bar truss system shown in Fig. E2.93, subjected to internal and external
forces. The truss is subjected to a vertical load of 5.78 kN at joint (1). In static equilibrium, the
sum of the Fx- and Fy-components of the forces at each joint should be zero, so the equilibrium
equations can be written as
At joint (1): ´ cos α F12 ` cos β F13 “ 0 At joint (2): cos α F12 ` F23 ` R2x “ 0
´ sin α F12 ´ sin β F13 ´ F “ 0 sin α F12 ` R2y “ 0
At joint (3): ´F23 ´ cos β F13 “ 0
sin β F13 ` R3y “ 0.
(a) Express the given six equations and six unknowns
(the internal bar forces, F12, F13, and F23, and the
reaction forces, R2x, R2y, and R3y) in matrix form,
and (b) use the Gauss elimination method to find the
direct solution of the resulting banded linear system.
Given: �1 “ 8-m, �2 “ 15-m, and �3 “ 17-m.
Fig. E2.93
E2.94 The matrix on the rhs is the compact form of the
banded matrix A. Find a lower L and an upper U matrices
such that A “ LU:
»
—
—
—
—
—
—
–
0 0 04 ´1
0 0 ´1 4 ´1
0 ´1 ´1 4 ´1
´1 ´1 ´1 4 ´1
´1 ´1 ´1 4 ´1
´1 ´1 ´14 0
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
E2.95 Find the solution of a tridiagonal system using the compact banded system.
»
—
—
—
—
—
—
–
4 ´10 0 0 0
´1 4 ´10 0 0
0 ´1 4 ´10 0
0 0 ´1 4 ´1 0
000 ´1 4 ´1
0000 ´2 4
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
–
x1
x2
x3
x4
x5
x6
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
–
´7
9
5
18
20
28
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
E2.96 For the given linear system, (a) use the Doolittle method to decompose the coefficient
matrix as A “ LU, and (b) apply forward (Ly “ b) and backward (Ux “ y) substitutions toLinear Systems: Fundamentals and Direct Methods  149
find the solution of the linear system.
x1 ` 2x2 ` x3 “ 0
´ 2x1 ´ 2x2 ´ x3 ´ 2x4 “ 0
´ x1 ` 2x2 ` 3x3 ´ 2x4 ` x5 “ 0
2x2 ´ 3x3 ´ 7x4 ´ 4x5 ´ 2x6 “ ´1
2
2x3 ` 4x4 ` 3x5 ` 2x6 “ 0
´ x4 ´ 4x5 ´ 3x6 “ 4
E2.97 Consider the compact banded systems below: (a) Use the Doolittle method to decom￾pose the coefficient matrix as A “ LU, and (b) apply forward (Ly “ b) and backward (Ux “ y)
substitutions to find the solution of the linear system.
(a)
»
—
—
—
—
—
—
—
—
–
0012
011 ´2
´1 ´4 ´2 2
´1031
4 2 ´3 ´2
1362
´2 ´8 ´3 0
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
x“
»
—
—
—
—
—
—
—
—
–
0
´5
´2
0
10
13
´20
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
, (b)
»
—
—
—
—
—
—
—
—
–
0 0 0 1 2 12
0 0 1 1 2 01
0 ´1 ´4 3 ´4 31
2311 ´1 ´3 1
04234 ´2 0
2 3 ´1 ´1 0 00
3 ´5 ´8 3 0 00
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
x“
»
—
—
—
—
—
—
—
—
–
0
2
1
0
4
´4
´13
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
2.13 COMPUTER ASSIGNMENTS
CA2.1 Write a pseudocode, Module Function VECNORM(n, p, x), to find }x}p; i.e., pth norm of a
vector of length n. The integer input variable p should return �1, �2, or �8-norm for the values 1,
2, or ą 2, respectively.
CA2.2 Develop an algorithm to find the inverses of lower and upper triangular matrices using
the Gauss-Jordan method, and write a computer program in a language of your choice to test
your algorithm.
»
—
—
—
—
–
�11 0 ¨¨¨ 0
�21 �22
.
.
.
.
.
. ... 0
�n1 �n2 ¨¨¨ �nn
fi
ffi
ffi
ffi
ffi
fl
´1
,
»
—
—
—
—
—
–
u11 u12 ¨¨¨ u1n
0 u22 ¨¨¨ u2n
.
.
. ... .
.
.
0 0 ¨¨¨ unn
fi
ffi
ffi
ffi
ffi
ffi
fl
´1
CA2.3 A general matrix form of a backward tridiagonal system is given below. (a) Develop a
simple memory-efficient algorithm to solve this system using the Gauss elimination method with
no pivoting. (b) Write a computer program in a language of your choice to test your code for
n “ 10 with ai “ bi “ ´1 and di “ 2 with c1 “ c10 “ 1, ci “ 0 for i “ 2, 3,..., 9. Hint: What
would the coefficient matrix and the vector of unknowns (rhs) look like if you rearranged the
equations backward from the bottom row to the top?
»
—
—
—
—
—
—
—
—
—
–
a1 d1
a2 d2 b2
a3 d3 b3
... ... ...
an´2 dn´2 bn´2
an´1 dn´1 bn´1
dn bn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
—
–
x1
x2
x3
.
.
.
xn´2
xn´1
xn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
—
—
–
c1
c2
c3
.
.
.
cn´2
cn´1
cn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl150  Numerical Methods for Scientists and Engineers
CA2.4 The general matrix form of a penta-diagonal system of linear equations is given below.
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
d1 c1 f1
a1 d2 c2 f2
e1 a2 d3 c3 f3
e2 a3 d4 c4 f4
... ... ... ... ...
em´2 am´1 dm cm fm
... ... ... ... ...
en´4 an´3 dn´2 cn´2 fn´2
en´3 an´2 dn´1 cn´1
en´2 an´1 dn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
x1
x2
x3
x4
.
.
.
xm
.
.
.
xn´2
xn´1
xn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
b1
b2
b3
b4
.
.
.
bm
.
.
.
bn´2
bn´1
bn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(a) Develop a simple and memory-efficient algorithm to solve this system using the Gauss elimi￾nation method with no pivoting, and (b) write a computer program in a language of your choice
to test your algorithm for n “ 10 with ai “ ei “ ci “ fi “ ´1, di “ 4 with the rhs vector set as
b1 “ b10 “ 2, b2 “ b9 “ 1, bi “ 0 for i “ 3, 4, ..., 8. The true solution is x “ 1.
CA2.5 A general matrix form of a bordered tridiagonal system of linear equations is given below.
»
—
—
—
—
—
—
—
—
—
–
d1 a1 f1
b2 d2 a2 f2
b3 d3 a3 f3
... ... ... .
.
.
bn´2 dn´2 an´2 fn´2
bn´1 dn´1 an´1
e1 e2 ¨¨¨ en´1 en´2 bn dn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
—
–
x1
x2
x3
.
.
.
xn´2
xn´1
xn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
—
—
–
c1
c2
c3
.
.
.
cn´2
cn´1
cn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Develop an algorithm to (a) perform LU decomposition to find matrices L and U that have the
general forms given below; (b) solve the linear system using forward and backward substitutions
(Ly “ c and Ux “ y). (c) Write a computer program in a language of your choice to test the
algorithms you developed in Parts (a) and (b). Make your programs as simple and memory￾efficient as possible. Use the following as the test problem: k “ 5, d “ 4, a “ b “ f “ e “ 1, and
c “{6,7,7,7,7,6,10}, and the true solution is x “ 1.
L “
»
—
—
—
—
—
—
—
—
—
–
�1
b2 �2
b3 �3
... ...
bn´2 �n´2
bn´1 �n´1
t1 t2 ¨¨¨ tn´1 tn´2 tn´1 �n
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
, U “
»
—
—
—
—
—
—
—
—
—
–
1 u1 w1
1 u2 w2
1 u3 w3
... ... .
.
.
1 un´2 wn´2
1 wn´1
1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
CA2.6 Use Gauss-Jordan elimination with no pivoting to calculate the current in each branch of
the circuit given in Fig. CA2.6. Note: The resistances are given in Ω.Linear Systems: Fundamentals and Direct Methods  151
Fig. CA2.6 Fig. CA2.7
CA2.7 In order to determine the force in each member of the truss in Fig. CA2.7, (a) derive the
equilibrium equations and express them in Ax “ b form; (b) apply elementary row operations to
convert matrix A into lower triangular L to solve the system of linear equations using forward
substitution. (c) State if the members are in tension (T) or compression (c).
CA2.8 Olive oil (O) is produced by an industrial extraction process that involves the preparation
of a homogeneous olive paste (skin, pulp, and stone) by crushing and malaxing processes. Olive
paste is made up primarily of olive oil, water, and carbohydrates, and olive oil is extracted from
this paste by a series of centrifugation processes (primary CF and second-stage CF-1 and CF-2).
The contents (in kg per kg slurry) of W (water), O (olive oil), and C (carbohydrates) at each
stage of a production facility are depicted in the simplified flowchart in Fig. CA2.8. The facility
will produce 304 kg of olive paste per hour. About half of the processed olive paste is converted
into olive cake. Determine the steady-state flow rates at each branch of the flowchart in order to
attain the specified contents (mass ratios). Use the furnished data, as well as the conservation of
overall mass, CF-1 and CF-2, and the conservation of mass for olive oil at each stage, to construct
an 8ˆ8 system of linear equations for the flow rates. Then solve the system of linear equations
using the Gauss-Jordan elimination algorithm.
Fig. CA2.8
CA2.9 Consider balancing the following chemical reactions: Express all equations for the un￾knowns (chemical elements present in the equations) in matrix form. Then use the Gauss-Jordan
elimination algorithm to solve the resulting system.
(a) x1K4Fe(SCN)6 ` x2K2Cr2O7 ` x3H2SO4 Ñ x4Fe2(SO4)3 ` x5Cr2(SO4)3 ` x6 CO2
` x7H2O ` x8 K2SO4 ` x9KNO3
(b) x1K4Fe(CN)6 ` x2 KMnO4 ` x3H2SO4 Ñ x4 KHSO4 ` x5 Fe2pSO4q3 `
x6 MnSO4 ` x7 HNO3 ` x8CO2 ` x9 H2O152  Numerical Methods for Scientists and Engineers
CA2.10 Modify the pseudomodule BAND_SOLVE (Pseudocodes 2.14) to carry out partial pivot￾ing. Program this module in a language of your choice to solve the following system of equations
given in compact form:
»
—
—
—
—
—
—
—
—
–
0 4 ´1 ´1
´2 4 ´1 ´1
´2 4 ´1 ´1
´2 4 ´1 ´1
´2 4 ´1 ´1
´2 4 ´1 0
´24 0 0
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
–
x1
x2
x3
x4
x5
x6
x7
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
—
–
1
´1
´1
8
19
´17
´8
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Fig. CA2.11
CA2.11 A simply supported beam of length L is resting on an elastic foundation of stiffness K
(N/m2), as shown in Fig. CA2.11. The displacement of a beam y (m) caused by a nonuniform
(parabolic) load, wpxq “ w0xpL ´ xq (N/m), is described by the following fourth-order ODE:
d4y
dx4 ` β y “ wpxq “ α x
L
´
1 ´ x
L
¯
, yp0q “ y2
p0q “ ypLq “ y2
pLq “ 0
where α “ w0L2{EI and β “ K{EI.
The finite-difference method solution (see Chapter 10) corresponding to α “ 50, β “ 10, and
h “ Δx “ L{8 leads to the following penta-diagonal system of linear equations, given below.
»
—
—
—
—
—
—
—
—
–
5 ` h4β ´4 1
´4 6 ` h4β ´4 1
1 ´4 6 ` h4β ´4 1
1 ´4 6 ` h4β ´4 1
1 ´4 6 ` h4β ´4 1
1 ´4 6 ` h4β ´4
1 ´4 5 ` h4β
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
–
y1
y2
y3
y4
y5
y6
y7
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“ αh4
64
»
—
—
—
—
—
—
—
—
–
7
12
15
16
15
12
7
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(a) Use the program written in Fig. CA2.7 to solve the linear system; (b) treat it as a banded
system with bandwidths of 2 and solve it using Gauss elimination with no pivoting (Pseudocodes
2.14) and LU-decomposition for banded systems (Pseudocodes 2.16 and 2.17).
CA2.12 The steady-state heat conduction in the square geometry shown in Fig. CA2.12 is solved
using the finite-difference method. Three sides are kept at the same temperature, while one side is
insulated, i.e., pBT{Bxqx“0 “ 0. The geometry is uniformly meshed, and the difference equations
corresponding to the nodes are given as
For p “ 1 : ´4T1 ` 2T2 ` T5 ` 400 “ 0,
For p “ 5 : T1 ´ 4T5 ` 2T6 ` T9 “ 0,
For p “ 9 : T5 ´ 4T9 ` 2T10 ` 500 “ 0,
For p ‰ 1, 5, 9 : Ts ` Tw ´ 4Tp ` Te ` Tn “ 0
where s, w, e, and n denote the compass directions. Constructing the system of linear equations
yields a banded system. Use compact banded forms to solve the system with LU-decomposition,
etc.Linear Systems: Fundamentals and Direct Methods  153
Fig. CA2.12
CA2.13 A special case of sparse matrices, as illustrated in Fig. CA2.13 where the dots denote
non-zero elements, are called the skyline matrices. The sparsity pattern of L or U is inherited
from A, so it is easier to develop an algorithm.
Fig. CA2.13
Consider the system of linear equations Ax “ b with a symmetric skyline matrix whose general
form is given as below. Develop an algorithm to: (a) perform LU decomposition to give the L
and U matrices that have the general forms given below; (b) solve the system using forward and
backward substitutions (Ly “ b and Ux “ y); and (c) solve the linear system using the Gauss
elimination method with no pivoting. (d) Write a computer program in a language of your choice
to test the algorithms you developed in Parts (a), (b), and (c). Use the following test problem:
i “ 6, d “{1,2,3,2,4,1,1,2,5}, p “{3,1,3,2,2}, q “{2,1,1,3,1,2,1,1}, and b “{2, 0, 2, 5, 11, 17, 9,
-3, 19}. The true solution is x “{1, -1, 2, -2, 3, -3, 4, -4, 5}. Make your algorithms as simple and
memory-efficient as possible.
A “
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
d1 0 ¨¨¨ 0 p1 0 ¨¨¨ 0 q1
0 d2 0 .
.
. p2 0 ¨¨¨ 0 q2
.
.
. ... ... 0 .
.
. .
.
. ... .
.
. .
.
.
0 ¨¨¨ 0 di´1 pi´1 0 ¨¨¨ 0 qi´1
p1 p2 ¨¨¨ pi´1 di 0 ¨¨¨ 0 qi
0 ¨¨¨ 00 0 di`1 0 .
.
. qi`1
.
.
. 0 .
.
. ¨¨¨ ... 0 ... 0 .
.
.
000 ¨¨¨ 0 ¨¨¨ 0 dn´1 qn´1
q1 q2 ¨¨¨ qi´1 qi qi`1 ¨¨¨ qn´1 dn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“ LU, b “
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
b1
b2
.
.
.
bi´1
bi
bi`1
.
.
.
bn´1
bn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl154  Numerical Methods for Scientists and Engineers
L “
»
—
—
—
—
—
—
—
—
—
—
—
–
10 0 ¨¨¨ 000
01 0 ¨¨¨ 000
.
.
. 0 ... ... .
.
. .
.
. .
.
.
�p1 ¨¨¨ �p,i´1 1 0 ¨¨¨ 0
.
.
. ¨¨¨ 0 0 ... ... .
.
.
0 0 ¨¨¨ 00 1 0
�q1 �q1 ¨¨¨ �qi ¨¨¨ �q,n´1 dn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
,
U “
»
—
—
—
—
—
—
—
—
—
—
—
—
—
–
u1 0 ¨¨¨ up1 0 ¨¨¨ uq1
0 u2 0 .
.
. 00 uq2
.
.
. 0 ... up,i´1
.
.
. .
.
. .
.
.
0 ¨¨¨ 0 ui 0 ¨¨¨ uqi
.
.
. ¨¨¨ 0 ... ... 0 .
.
.
0 0 ¨¨¨ .
.
. 0 un´1 uq,n´1
0 0 ¨¨¨ 0 ¨¨¨ 0 un
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
flCHAPTER 3
Linear Systems: Iterative
Methods
LEARNING OBJECTIVES
After completing this chapter, you should be able to
‚ explain the main concept of iterative methods;
‚ realize the importance and application areas of sparse matrices;
‚ understand and implement the basic stationary methods (Jacobi, Gauss-Seidel,
and SOR methods);
‚ define and discuss the concepts of convergence and convergence rate;
‚ identify the requirements for convergence of iterative methods;
‚ explain the significance of diagonal dominance in iterative methods;
‚ understand the importance of using the optimum relaxation parameter;
‚ understand and implement the algorithms to estimate the optimum relaxation
parameter;
‚ describe the conjugate gradient method (CGM) for symmetric linear systems;
‚ understand the motivation of Krylov subspace methods;
‚ apply the CGM to solve a system of linear equations;
‚ discuss the preconditioning of linear systems, as well as the choice and selection
of preconditioners;
‚ implement the variants of the conjugate gradient method to solve symmetric
or non-symmetric preconditioned or non-conditioned linear systems;
‚ describe the effects of ill-conditioning;
‚ improve the accuracy of an ill-conditioned system of equations.
T HE direct methods for solving systems of linear equations in various forms were dis￾cussed in Chapter 2. The direct methods require a finite number of arithmetic op￾erations, regardless of the number of “zero” elements the matrix has. The direct methods
also give the true solution in the absence of round-offs or other errors. Nonetheless, solving
large linear systems by direct methods requires very large memory storage space and leads
to excessive computation time. For this reason, direct methods are generally preferred in
cases where a matrix is a dense (matrix) of a moderate size, i.e., a matrix with few zero
elements.
DOI: 10.1201/9781003474944-3 155156  Numerical Methods for Scientists and Engineers
FIGURE 3.1: Sparcity patterns of coefficient matrices resulting from discretization of an elliptic
PDE in (a) 2D and (b) 3D-rectangular, elliptic PDE in (c) 2D- and (d) 3D-nonrectangular domain.
The non-zero elements are marked in black.
This chapter is mainly devoted to basic stationary iterative methods (Jacobi, Gauss￾Seidel, and SOR) for solving large simultaneous systems of linear equations that cannot
be effectively handled by direct methods. A substantial discussion on Conjugate Gradient
Methods (CGMs) for symmetric and non-symmetric systems is also introduced. The im￾plementations of stationary and CGM methods are presented with algorithms (also pseu￾docodes) and solved examples. The performance and convergence rates of the iterative
methods covered in the text are also discussed.
3.1 OVERVIEW
The majority of large systems of linear equations are encountered in practice in the nu￾merical solution of ordinary differential equations (ODEs) or partial differential equations
(PDEs). The size of the matrices can be in the order of hundreds, thousands, or even mil￾lions. Furthermore, most of the non-zero elements are concentrated on or near the principal
diagonal, while the remaining elements consist of zeros.
Numerical solutions of partial differential equations (PDEs) in two- and three￾dimensional rectangular domains result in coefficient matrices with 5- and 7-diagonals, re￾spectively, consisting of mostly non-zero elements (see Fig. 3.1a and b). Such linear systems
with narrow bandwidths may be treated as banded systems, which do reduce the memory
requirement and the computational efforts to some degree. Yet arithmetic operations have
to be carried out with numerous zeros within the band. We also encounter resulting ma￾trices with non-zero elements dispersed regularly or irregularly around the main diagonal,
as depicted in Fig. 3.1c and d. These matrices generally appear in the numerical solution
of linear or non-linear PDEs in non-rectangular domains. Such matrices with relatively few
non-zero elements are called sparse matrices.
Digital computers and general multi-purpose algorithms are not designed to distinguish
between zero and non-zero elements. In other words, even though humans can immediately
spot zeros and skip arithmetic operations with them, digital computers do carry out the
arithmetic operations regardless of the values of the numbers. When a linear system is
treated as non-banded or non-sparse, the number of arithmetic operations required to obtain
the solution is on the order of Opn3q, and the computational process can require excessive
cpu-time, which can be prohibitive even for reasonably large matrices. Yet another vital
constraint in dealing with large, dense matrices is the excessive memory requirement for
storing the matrix elements, which can be extremely restrictive. For the above reasons, it
should not be surprising that direct solution methods are not preferred when working with
sparse systems.Linear Systems: Iterative Methods  157
Iterative methods are techniques that, unlike direct methods, begin with an initial guess
for a system of linear (or nonlinear) equations and refine the solution by successively apply￾ing a suitably chosen estimation algorithm until a desired level of accuracy or convergence is
achieved. Iterative methods require less memory allocation because only non-zero elements
that are used in the computation are stored. This means that they are computationally
cheaper as arithmetic calculations are carried out only with non-zero elements, reducing
the cpu-time significantly.
To begin, consider a system of linear equations (Ax “ b) given as
a11x1 ` a12x2 ` a13x3 `¨¨¨` a1nxn “ b1
a21x1 ` a22x2 ` a23x3 `¨¨¨` a2nxn “ b2
a31x1 ` a32x2 ` a33x3 `¨¨¨` a3nxn “ b3
.
.
. .
.
. .
.
. ... .
.
.
an1x1 ` an2x2 ` an3x3 `¨¨¨` annxn “ bn
(3.1)
where xi’s are the unknowns, aij ’s and bi’s are the elements of the coefficient matrix (A)
and the right-hand side vector (b), respectively.
This system can be solved iteratively after expressing Eq. (3.1) in the following general
mathematical form:
xpp`1q “ Fpxppq
q, p ě 0 (3.2)
where the superscript notation “ppq” denotes the iteration level, x is the vector of unknowns,
F “ rF1, F2,...,Fns
T is the fixed-point form of the linear equations that depends on the
numerical algorithm, and xpp`1q and xppq denote the vector of computed estimates at the
pp ` 1qth and pth iteration steps, which will also be referred to throughout this text as
current (new iterate) and prior estimates (previous iterate), respectively.
An iterative method is a procedure based on generating a sequence of improved approx￾imations using an initial guess, xp0q, which should be supplied to start the iterative process.
By setting p “ 0 in Eq. (3.2) and using the initial guess on the rhs, a new estimate, xp1q
,
is obtained. Then, using xp1q, another “hopefully better” approximation, xp2q, is found, and
subsequent approximations or estimates are found in this manner. In convergent systems,
this process yields a sequence of estimates xp1q, xp2q,..., xppq that improve with each iter￾ation step. An iterative method is said to be convergent when the difference between the
true solution x and the sequence xppq generated from an initial xp0q tends to zero as the
number of iterations increases, i.e., x ´ xppq Ñ 0 as p Ñ 8.
3.1.1 STEPS OF AN ITERATIVE METHOD
All iterative algorithms consist of three basic steps: initialization, computation, and termi￾nation.
(i) Initialization: This is the first step in every iteration process where a suitable
initial guess is made. In most problems, an arbitrary initial guess (i.e., xp0q=constant) is
often set for the values of unknowns due to the simplicity and ease of implementation.
Moreover, the linear systems of equations are not very sensitive to the initial guess, so
there is no point in trying to choose a perfect initial guess for the unknowns. However,
when dealing with the linearized form of a set of non-linear equations, the coefficients and
the rhs of the linear systems consist of the prior iterates. In such cases, an initial guess in
close proximity to the solution (also referred to as a “smart guess”) speeds up the iteration
process considerably.158  Numerical Methods for Scientists and Engineers
(ii) Computation: This step involves the way current (new) estimates are computed,
i.e., how xpp`1q is computed. In this step, an iterative scheme which is expressed mathemat￾ically by the general equation Eq. (3.2) is implemented. This step is the core of an iterative
algorithm and is different for each iterative method, as discussed in detail in the subsequent
sections.
(iii) Termination: Any iterative solution procedure is terminated for basically two
reasons:
(1) When a current estimate converges to a solution within a predetermined tolerance. This
step is crucial to finding a quick solution while avoiding oversolving. The successive esti￾mates of a converging iterative system will approach closer to the true solution with each
subsequent iteration. Since the true solution x is unknown, it is impossible to verify how
close xppq is to x.
An iteration process is terminated when the current and prior estimates are sufficiently
close to meet a preset convergence tolerance (or stopping criterion). The stopping criterion,
ε, determines the level of accuracy sought in the solution and also affects the computation
time. For general programming practices, analysts often use both absolute and relative
error criteria for terminating an iteration process. The number of arithmetic operations
depends not only on the accuracy required in the solution but also on the iteration scheme
adopted. A modest number of iterations will be sufficient to obtain an approximate solution
with acceptable accuracy when the accuracy requirement is not too strict. However, if the
convergence tolerance is too small, then the total number of iterations and consequently
the cpu-time will be substantially large.
Commonly used stopping criteria are based on the absolute error of the displacement
vector at the pth iteration step, i.e., �1, �2, or �8-norms of dppq
.
�dppq
�1 “ ÿn
i“1
|dppq
i |ăε1, �dppq
�2 “
gffeÿn
i“1
´
dppq
i
¯2
ăε2, �dppq
�8 “ max
1ďiďn |dppq
i |ăε3 (3.3)
where dppq “ xpp`1q ´ xppq is so-called the displacement vector. Another set of stopping
criteria, based on the norms of dppq
{xpp`1q
, provide a measure for relative errors:
›
›
›
›
›
dppq
xpp`1q
›
›
›
›
›
1
“ řn
i“1
ˇ
ˇ
ˇ
ˇ
ˇ
dppq
i
xpp`1q
i
ˇ
ˇ
ˇ
ˇ
ˇ
ă ε1,
›
›
›
›
›
dppq
xpp`1q
›
›
›
›
›
2
“
gffe řn
i“1
˜ dppq
i
xpp`1q
i
¸2
ă ε2,
›
›
›
›
›
dppq
xpp`1q
›
›
›
›
›
8
“ max
1ďiďn
ˇ
ˇ
ˇ
ˇ
ˇ
dppq
i
xpp`1q
i
ˇ
ˇ
ˇ
ˇ
ˇ
ă ε3
(3.4)
where ε1, ε2, and ε3 are convergence tolerances that may be chosen differently.
(2) When the number of iterations reaches a predetermined upper bound (MAXIT). This
criterion is very important in that we do not know in advance whether convergence with
the desired tolerance can be achieved within a predetermined number of iterations. In the
event that a system converges slowly or does not converge at all, the iteration process will
go into an infinite loop unless the loop is terminated. Therefore, it is a good programming
practice to set an upper bound (denoted by maxit throughout the text) for the maximum
number of iterations permitted.Linear Systems: Iterative Methods  159
3.1.2 SOURCES OF ERROR IN ITERATIVE METHODS
There are basically two sources of error in iterative methods as applied to systems of linear
equations: truncation errors and round-off errors.
All iterative methods are subjected to “truncation” because the iteration process even￾tually needs to be terminated after a finite number of iterations. In reality, it is impractical
to perform an infinite number of iterations in an attempt to find the true solution, which
is unknown. For this reason, the iterative solution of a linear system is “approximate.” Nev￾ertheless, the truncation (or approximation) error can be reduced by applying a smaller
convergence tolerance, i.e., increasing the number of iterations.
The second source of error is round-off error, which is introduced when computing
iterates over many iteration steps. In direct methods, a round-off error in the value of
the last unknown propagates at every step of the solution, corrupting the values of the
remaining unknowns, while a round-off error in an iterative method is an error that occurs
only in the last iteration step. The reason for this is that the coefficient matrix is always
intact throughout the iteration process, and the current estimates are always computed
from scratch at each iteration using prior estimates as the initial guess vector.
In an iterative process, the magnitude of round-off error is usually not considered serious
(except for ill-conditioned systems) because the truncation error introduced by terminating
the iteration is generally larger. In this context, it is only natural to expect round-off errors
to be a much less serious problem in iterative methods than in direct methods. Furthermore,
there are two more factors that make iterative methods less sensitive to round-off errors.
(i) The iterative methods are employed for diagonally dominant systems, which reduces the
influence of round-off errors. The more dominant the system, the less the round-off error.
(ii) The systems of equations dealt with are generally sparse, and the presence of the zeroes
may remove the influence of some of the previous components.
3.2 STATIONARY ITERATIVE METHODS
3.2.1 JACOBI METHOD
Consider the system of linear equations given by Eq. (3.1). This system can be expressed
in fixed-point form, i.e., x “ Fpxq, and it has a unique solution provided that aii ‰ 0 for
all i. However, there are several ways to express the system in a fixed-point form. The most
common approach is to solve each equation for its diagonal component; that is, solve x1
from the first equation, x2 from the second equation, and so on. Employing this idea in Eq.
(3.1) yields
x1 “ F1px1, x2,...,xnq “ 1
a11
pb1 ´ a12x2 ´ a13x3 ´¨¨¨´ a1nxnq
x2 “ F2px1, x2,...,xnq “ 1
a22
pb2 ´ a21x1 ´ a23x3 ´¨¨¨´ a2nxnq
x3 “ F3px1, x2,...,xnq “ 1
a33
pb3 ´ a31x1 ´ a32x3 ´¨¨¨´ a3nxnq
.
.
. .
.
.
xn “ Fnpx1, x2,...,xnq “ 1
ann
pbn ´ an1x1 ´ an2x2 ´¨¨¨´ an,n´1xn´1q160  Numerical Methods for Scientists and Engineers
Next, we introduce the superscript notation “(p)” to denote the estimates (or approx￾imations) at the pth iteration step. With this notation, Jacobi iteration equations that are
used to generate successive approximations are set up as
xpp`1q
1 “ 1
a11
´
b1 ´ a12xppq
2 ´ a13xppq
3 ´ a14xppq
4 ´¨¨¨´ a1nxppq n
¯
xpp`1q
2 “ 1
a22
´
b2 ´ a21xppq
1 ´ a23xppq
3 ´ a24xppq
4 ´¨¨¨´ a2nxppq n
¯
xpp`1q
3 “ 1
a33
´
b3 ´ a31xppq
1 ´ a32xppq
2 ´ a34xppq
4 ´¨¨¨´ a3nxppq n
¯
.
.
. .
.
.
xpp`1q n “ 1
ann
´
bn ´ an1xppq
1 ´ an2xppq
2 ´ an3xppq
3 ´¨¨¨´ an,n´1xppq
n´1
¯
(3.5)
where xppq
i and xpp`1q
i denote the estimates of the ith unknown at the pth (prior) and
(p ` 1)th (current) iteration steps, respectively.
Jacobi method is also called the “method of simultaneous displacements” because each
equation is simultaneously changed with prior estimates; in other words, all current esti￾mates are calculated using the prior estimates. The Jacobi iteration equations, Eq. (3.5),
can be expressed in the most general and most suitable form for programming as
xpp`1q
i “ 1
aii #
bi ´
i
ÿ´1
j“1
aijxppq
j ´ ÿn
j“i`1
aijxppq
j
+
(3.6)
where aii ‰ 0 for all i is a mandatory requirement. It should, however, be pointed out that
the Jacobi method, whose convergence issues are covered in greater detail in Section 3.3
does not converge for every linear system.
A set of pseudomodules for solving a linear system of equations with the Jacobi method
is presented in Pseudocode 3.1. As input, the main module JACOBI requires the number of
equations (n), the coefficient matrix (A), the rhs vector (b), an initial guess vector (xo), a
convergence tolerance (ε), and an upper bound for the number of iterations (maxit). The
outputs of the module are the solution vector (x), the total number of iterations performed
(iter), and the �2´norm of the displacement vector (error).
The JACOBI_DRV driver module performs a one-step Jacobi iteration and provides the
current estimate (xpp`1q
) as well as the �2´norm of the displacement vector (δ “ dppq
2)
utilized in JACOBI to terminate the iteration process. On the first visit to JACOBI_DRV,
xo is set to the initial guess (xo “ xp0q). In the subsequent visits, xo contains the prior
estimates, xo “ xppq
. An accumulator, sum, is used to sum up all aijxppq
i products (except
i “ j) in the ith row, and the current estimate x is found from Eq. (3.6). The number
of iterations in JACOBI is counted with the counter variable p. After setting the current
estimates as prior (xo Ð x), the iteration process is continued until the convergence criterion
(δ ă ε) is satisfied or the maximum number of iterations (maxit) is reached. On exit from
JACOBI, the most recent displacement norm is set to error.
The �2´norm of the displacement vector, dppq
2, is evaluated using Function Module
ENORM, which has two arguments: a vector x and its length n. An accumulator loop and
an accumulator variable δ are used to sum up all x2
i . The square root of the accumulator is
set to function ENORM. Note that, in the accumulator, x2
i is carried out as xi ˚ xi, which
is used to save cpu-time because the latter is about ten times faster.Linear Systems: Iterative Methods  161
Pseudocode 3.1
Module JACOBI (n, ε, A, b, xo, x, maxit, iter, error)
\ DESCRIPTION: A pseudomodule to solve Ax “ b using the Jacobi method.
\ USES:
\ JACOBI_DRV:: Driver module performing one step Jacobi iteration.
Declare: ann, bn, xn, xon \ Declare array variables
p Ð 0; δ0 Ð 1 \ Initialize iteration counter and �2´norm
Repeat \ Iterate until convergence is achieved
p Ð p ` 1 \ Count iterations
JACOBI_DRV(n, A, b, xo, x, δ) \ Perform one-step Jacobi iteration
Write: “For p=”, p,“ Error=”,δ,“ Rate=”, δ{δ0 \ Echo iteration progress
xo Ð x \ Set current estimates as prior; xppq Ð xpp`1q
δ0 Ð δ \ Set current error as prior; δppq Ð δpp`1q
Until “
δ ă ε Or p “ maxit‰ \ Terminate iteration if δ ă ε and p ď maxit
error Ð δ \ Set current �2´norm as error
iter Ð p \ Set current p as iter
\ If iteration limit maxit is reached with no convergence
If “
p “ maxit‰
Then \ Issue info and a warning
Write: “Error=”,error,“Jacobi failed to converge after”,maxit,“iterations”
End If
End Module JACOBI
Module JACOBI_DRV (n, A, b, xo, x, δ)
\ DESCRIPTION: A pseudomodule to perform one step Jacobi iteration.
\ USES:
\ ENORM:: Function Module calculating Euclidean (�2´)norm of a vector.
Declare: ann, bn, xn, xon, dn \ Declare array variables
For “
i “ 1, n‰ \ Loop i: Sweep equations from top to bottom
sums Ð 0 \ Initialize accumulator, sums of off-diagonals
For “
j “ 1, n‰ \ Loop j: Sweep terms from left to right
If “
i ‰ j
‰
Then \ If i “ j, skip diagonal term
sums Ð sums ` aij ˚ xoj \ Accumulate aij ˚ xoj products
End If
End For
xi Ð pbi ´ sumsq{aii \ Estimate xi from Eq. (3.6)
di Ð xi ´ xoi \ Find displacement vector, di Ð xpp`1q
i ´ xppq
i
End For
δ Ð ENORMpn, dq \ Set �2´norm of dppq to δ
End Module JACOBI_DRV
Function Module ENORM (n, x)
\ DESCRIPTION: A function to calculate �2´norm, x2, of a vector of length n.
Declare: xn
delta Ð 0 \ Initialize �2´norm
For “
i “ 1, n‰ \ Loop to find sums of squares of elements of x
delta Ð delta ` xi ˚ xi \ Accumulate x2
i ’s
End For
ENORMÐ ?
delta \ Assign ař x2
i to ENORM
End Function Module ENORM162  Numerical Methods for Scientists and Engineers
Jacobi Method
‚ The method is easy to understand and implement;
‚ Numerically it is robust;
‚ Iteration procedure can be carried out in a parallel environment.
‚ The method in practice is not preferred for large systems;
‚ It converges under certain conditions (it is generally used to solve
linear systems whose coefficient matrix is “diagonally dominant”);
‚ The rate of convergence, if it converges at all, is quite slow, making
it unsuitable and unacceptable for large linear (or sparse) systems.
EXAMPLE 3.1: Application of the Jacobi method
Consider the circuit in the figure below.
Kirchoff’s voltage law for each loop yields a linear system for the loop currents.
Apply the Jacobi method to estimate the loop currents. Use i
p0q “ 1 as the initial
guess and d
ppq
2 ď 10´3 as the stopping criterion.
»
—
—
–
3 ´10 0
´1 3 ´1 0
0 ´1 3 ´1
0 0 ´1 3
fi
ffi
ffi
fl
»
—
—
–
i1
i2
i3
i4
fi
ffi
ffi
fl “
»
—
—
–
1.5
1.5
2
5.5
fi
ffi
ffi
fl
SOLUTION:
After expressing the system in explicit form and solving i1 from the first equation,
i2 from the second equation, and so on, the Jacobi iteration equations are expressed
as follows:
i
pp`1q
1 “ 1
3
pi
ppq
2 ` 1.5q
i
pp`1q
2 “ 1
3
pi
ppq
1 ` i
ppq
3 ` 1.5q,
i
pp`1q
3 “ 1
3
pi
ppq
2 ` i
ppq
4 ` 2q,
i
pp`1q
4 “ 1
3
pi
ppq
3 ` 5.5q.
Setting p “ 0 in the above equations and making use of the initial guess (i
p0q
k “ 1,
k “ 1, 2, 3, 4), the first step estimates are obtained as
i
p1q
1 “ 1
3
`
1 ` 1.5
˘
“ 0.833333,
i
p1q
2 “ 1
3
`
1 ` 1 ` 1.5
˘
“ 1.166666,
i
p1q
3 “ 1
3
`
1 ` 1 ` 2
˘
“ 1.333333,Linear Systems: Iterative Methods  163
i
p1q
4 “ 1
3
`
1 ` 5.5
˘
“ 2.166666.
Now, setting p “ 1 in the iteration equations and making use of i
p1q
k , the current
step estimates are found as follows:
i
p2q
1 “ 1
3
`
1.166666 ` 1.5
˘
“ 0.888889,
i
p2q
2 “ 1
3
`
0.833333 ` 1.333333 ` 1.5
˘
“ 1.222222,
i
p2q
3 “ 1
3
`
1.166666 ` 2.166666 ` 2
˘
“ 1.777778,
i
p2q
4 “ 1
3
`
1.333333 ` 5.5
˘
“ 2.277778
The Jacobi iteration process is continued in this way to obtain new and better
estimates. At the end of each iteration, the ratio of the �2´norms of the displacement
vectors obtained in two consecutive iterations (i.e., r “ dppq
2{dpp´1q
2) is also
calculated.
The estimates i
ppq
, displacements dppq
2, and the ratio r are presented in Table
3.1. The method converged to an approximate set of solutions that met the prescribed
error tolerance after 12 iterations. Notice that as the number of iterations increases,
the estimated solution with each subsequent iteration approaches (i.e., converges)
toward the true solution of the linear system: ti1, i2, i3, i4u “ {1, 1.5, 2, 2.5}.
TABLE 3.1
p ippq
1 i
ppq
2 i
ppq
3 i
ppq
4
›
›dppq
›
›
2 r
01 1 1 1
1 0.833333 1.166667 1.333333 2.166667 1.236030
2 0.888889 1.222222 1.777778 2.277778 0.464811 0.376051
3 0.907407 1.388889 1.833333 2.425926 0.230554 0.496016
4 0.962963 1.41358 1.938272 2.444444 0.122683 0.532122
5 0.971193 1.467078 1.952675 2.479424 0.066036 0.538268
6 0.989026 1.474623 1.982167 2.484225 0.035606 0.539187
7 0.991541 1.490398 1.986283 2.494056 0.019203 0.539322
8 0.996799 1.492608 1.994818 2.495428 0.010357 0.539341
9 0.997536 1.497206 1.996012 2.498273 0.005586 0.539344
10 0.999069 1.497849 1.998493 2.498671 0.003013 0.539345
11 0.999283 1.499187 1.99884 2.499498 0.001625 0.539345
12 0.999729 1.499374 1.999562 2.499613 0.000876 0.539345
Discussion: Note that the ratio of the norms also converges to the value of 0.539345,
while dppq
2 is steadily decreasing toward zero. This ratio represents the conver￾gence rate, and its significance is covered in detail in Section 3.3.
3.2.2 GAUSS-SEIDEL (GS) METHOD
Gauss-Seidel method is probably one of the most common iterative methods used to solve a
system of linear equations. Although it is similar to the Jacobi method, it utilizes the most
current estimates in the subsequent equations as soon as they become available.164  Numerical Methods for Scientists and Engineers
Gauss-Seidel Method
‚ The Gauss-Seidel method is easy to understand and program;
‚ It is a special case of the SOR method with ω “ 1;
‚ It is neither computationally complicated nor involved;
‚ It generally converges twice as fast as the Jacobi method;
‚ It converges to the correct solution regardless of the initial guess;
‚ It requires less memory compared to faster iterative methods.
‚ The method does not converge for every system of linear equations
(applicable to strictly diagonally dominant, or symmetric positive
definite matrices);
‚ It is, in general, not competitive with the non-stationary methods;
‚ Unlike the Jacobi method, computations generally cannot be carried
out in parallel; parallelization depends on the properties of the coef￾ficient matrix.
The Gauss-Seidel iteration equations are mathematically expressed as
xpp`1q
1 “ 1
a11
´
b1 ´ a12 xppq
2 ´ a13 xppq
3 ´ a14 xppq
4 ´ ¨¨¨´ a1n xppq n
¯
xpp`1q
2 “ 1
a22 ˆ
b2 ´ a21 xpp`1q
1 ´ a23xppq
3 ´ a24xppq
4 ´¨¨¨´ a2nxppq n
˙
xpp`1q
3 “ 1
a33 ˆ
b3 ´ a31 xpp`1q
1 ´ a32 xpp`1q
2 ´ a34xppq
4 ´ ¨¨¨´ a3nxppq n
˙
.
.
. .
.
. .
.
.
xpp`1q n “ 1
ann ˆ
bn ´ an1 xpp`1q
1 ´ an2 xpp`1q
2 ´¨¨¨´ an,n´1 xpp`1q
n´1
˙
(3.7)
where the current estimates xpp`1q
i (for i “ 1, 2, ..., n ´ 1) on the rhs are boxed for visual
clarity.
The generalized form of the iteration equations is expressed as
xpp`1q
i “ 1
aii #
bi ´
i
ÿ´1
j“1
aij xpp`1q
j ´ ÿn
j“i`1
aijxppq
j
+
for i “ 1, 2,...,n (3.8)
provided that aii ‰ 0. Note that the presumably more accurate estimates (boxed compo￾nents) from previous equations are available at the ith equation and can be readily incorpo￾rated in the first sum of Eq. (3.8). This procedure is convenient for computer calculations
because a current estimate can be immediately stored in the location of the prior estimate,
minimizing the number of storage locations.
A drawback of most iterative methods is that they may not converge for every linear
system. In this regard, the applicability of the Gauss-Seidel method is limited. The terms and
conditions required for the convergence of the iterative method are discussed in Section 3.3.
For now, assuming that a linear system is convergent, the Gauss-Seidel method converges
faster (i.e., reaches an approximate solution with fewer iterations) than the Jacobi method
because the unknown components for j ă i are updated with the improved current estimates
as soon as they become available.Linear Systems: Iterative Methods  165
EXAMPLE 3.2: Application of Gauss-Seidel method
Repeat Example 3.1 using the Gauss-Seidel method.
SOLUTION:
Using Eq. (3.8), the Gauss-Seidel iteration equations can be cast as follows:
i
pp`1q
1 “ 1
3
`
i
ppq
2 ` 1.5
˘
i
pp`1q
2 “ 1
3
` i
pp`1q
1 ` i
ppq
3 ` 1.5
˘
i
pp`1q
3 “ 1
3
` i
pp`1q
2 ` i
ppq
4 ` 2
˘
i
pp`1q
4 “ 1
3
` i
pp`1q
3 ` 5.5
˘
Note that every current estimate from a prior unknown component (as shown in
boxes) is used in the subsequent iteration equations.
Substituting the initial guess, i
p0q
k “ 1, into the Gauss-Seidel iteration equations,
the current step estimates are obtained as
i
p1q
1 “ 1
3
`
1 ` 1.5
˘
“ 0.833333
i
p1q
2 “ 1
3
` 0.833333 ` 1 ` 1.5
˘
“ 1.111111
i
p1q
3 “ 1
3
` 1.111111 ` 1 ` 2
˘
“ 1.370370
i
p1q
4 “ 1
3
` 1.370370 ` 5.5
˘
“ 2.290123
where the boxed numbers are the most current estimates available.
TABLE 3.2
p ippq
1 i
ppq
2 i
ppq
3 i
ppq
4
›
›dppq
›
›
2 r
01 1 1 1
1 0.833333 1.111111 1.37037 2.290123 1.357098
2 0.870370 1.246914 1.845679 2.448560 0.520418 0.383478
3 0.915638 1.420439 1.956333 2.485444 0.213927 0.411069
4 0.973480 1.476604 1.987349 2.495783 0.087001 0.406683
5 0.992201 1.493184 1.996322 2.498774 0.026736 0.307313
6 0.997728 1.498017 1.998930 2.499643 0.007840 0.293215
7 0.999339 1.499423 1.999689 2.499896 0.002283 0.291230
8 0.999729 1.499832 1.999909 2.499970 0.000664 0.290942
When this iteration procedure is continued in this manner, the Gauss-Seidel
method converges to a set of estimates, meeting the prescribed error tolerance after
8 iterations. To depict the convergence history of the computed estimates, dppq
2
and r (dppq
2{dpp´1q
2) are presented in Table 3.2. Using the same set of uniform
initial guesses and the same stopping criterion, the Gauss-Seidel iteration procedure
converged to an approximate solution in 8 iterations, while the Jacobi method did
so in 12 iterations (see Example 3.1).166  Numerical Methods for Scientists and Engineers
Discussion: Savings of 4 iterations indicate 25% fewer calculations. Note that the
dppq
2 norm in Table 3.2 steadily decreases while the ratio of the norm in the last
column converges to about 0.29, which is referred to as the convergence rate and is
covered in detail in Section 3.3.
3.2.3 SOR METHOD
The SOR method, a faster variant of the Gauss-Seidel method, is widely employed to obtain
the numerical solution of sparse linear systems. It is termed Successive Over Relaxation
(SOR) because the current estimate is improved by a weighted average of the “prior” and
“Gauss-Seidel” estimates as follows:
xpp`1q “ p1 ´ ωqxppq ` ω xpp`1q
GS ,
where ω is referred to as the relaxation (or acceleration) parameter, and xpp`1q
GS denotes the
Gauss-Seidel estimate obtained by Eq. (3.8).
Substituting xpp`1q
GS from Eq. (3.8) into the weighted-average, the SOR iteration equa￾tions yields
xpp`1q
1 “ p1 ´ ωqxppq
1 `
ω
a11
´
b1 ´ a12xppq
2 ´ a13xppq
3 ´¨¨¨´ a1nxppq n
¯
xpp`1q
2 “ p1 ´ ωqxppq
2 `
ω
a22
´
b2 ´ a21xpp`1q
1 ´ a23xppq
3 ´¨¨¨´ a2nxppq n
¯
xpp`1q
3 “ p1 ´ ωqxppq
3 `
ω
a33
´
b3 ´ a31xpp`1q
1 ´ a32xpp`1q
2 ´¨¨¨´ a3nxppq n
¯
.
.
. .
.
.
xpp`1q n “ p1 ´ ωqxppq n `
ω
ann
´
bn ´ an1xpp`1q
1 ´ an2xpp`1q
2 ´¨¨¨´ an,n´1xpp`1q
n´1
¯
(3.9)
A more generalized form of Eq. (3.9) may be written in a compact form as
xpp`1q
i “ p1 ´ ωqxppq
i `
ω
aii #
bi ´
i
ÿ´1
j“1
aijxpp`1q
j ´ ÿn
j“i`1
aijxppq
j
+
, i“1, 2,...,n (3.10)
The relaxation parameter typically varies in the range 0 ă ω ă 2. For ω “ 1, Eq. (3.10)
is reduced to the Gauss-Seidel method. The iteration process is termed over-relaxation for
1 ă ω ă 2 and successive under-relaxation (SUR) for 0 ă ω ă 1, which is usually applied
when solving systems of non-linear equations. If the system is convergent, the method
converges to an approximate solution regardless of the choice of ω; however, there is an
optimum value, ωopt, for which the system yields an approximate solution with the least
number of iterations and faster convergence rate.
Factors like the number of equations, the strength of the diagonal dominance (covered
in Section 3.3.1), the pattern of the coefficient (full, banded, sparse, etc.) matrix, and so
on affect the optimum relaxation factor. Hence, the SOR method is generally applied to
large systems with a “smart guess” for the ωopt. If a system of linear equations is to be
solved numerous times in an iterative algorithm, it is extremely important to estimate and
use a ωopt value close to the true optimum value (see Section 3.3.2 on estimating ωopt). In
near-optimal conditions, the problem-solving time can be significantly reduced.Linear Systems: Iterative Methods  167
Pseudocode 3.2
Module SOR (n, ε, ω, A, b, xo, x, maxit, iter, error)
\ DESCRIPTION: A pseudomodule to solve Ax “ b using the SOR method.
\ USES:
\ SOR_DRV:: Driver module performing one step SOR iteration.
Declare: ann, bn, xn, xon \ Declare array variables
p Ð 0 \ Initialize iteration counter
δ0 Ð 1 \ Initialize �2´norm, δp0q “ 1
Repeat \ Iterate until convergence is achieved
p Ð p ` 1 \ Iteration counter
SOR_DRV(n, ω, A, b, xo, x, δ) \ Perform one-step SOR iteration
Write: “For p=”, p,“ Error=”,δ,“ Rate=”, δ{δ0 \ Echo iteration progress
δ0 Ð δ \ Set current error as prior, δppq Ð δpp`1q
xo Ð x \ Set current estimates as prior; xppq Ð xpp`1q
Until “
δ ă ε Or p ă maxit‰ \ Terminate iterations if δ ă ε and p ă maxit
error Ð δ \ Set current �2´norm as error
iter Ð p \ Set current p as iter
\ If iteration limit maxit is reached with no convergence
If “
p “ maxit‰
Then \ Issue info and a warning
Write: “SOR method failed to converge after”, maxit,“iterations”
Write: “Error level readed is”,error
End If
End Module SOR
Module SOR_DRV (n, ω, A, b, xo, x, δ)
\ DESCRIPTION: A pseudomodule to perform one step SOR iteration.
\ USES:
\ ENORM:: Function Module calculating Euclidean (�2´)norm of a vector (see
Pseudocode 3.1).
Declare: ann, bn, xn, xon, dn \ Declare array variables
ω1 Ð 1 ´ ω \ Set ω1 “ 1 ´ ω to reduce arithmetic ops
For “
i “ 1, n‰ \ Loop i: Sweep equations from top to bottom
sums Ð 0 \ Initialize accumulator
For “
j “ 1, n‰ \ Loop j: Accumulate off-diagonal terms of ith row
If “
j ą i
‰
Then \ For terms to the right of aii ˚ xi
sums Ð sums ` aij ˚ xoj \ Accumulate aij ˚ xppq
j products
Else \ Alternative condition is i ě j
If “
i ą j
‰
Then \ For terms to the left of aii ˚ xi
sums Ð sums ` aij ˚ xj \ Accumulate aij ˚ xpp`1q
j products
End If
End If \ Note that case of i “ j is skipped
End For
xi Ð ω1 ˚ xoi ` ω ˚ pbi ´ sumsq{aii \ Compute xpp`1q
i by Eq. (3.10)
di Ð xi ´ xoi \ Find displacement vector, di Ð xpp`1q
i ´ xppq
i
End For
δ Ð ENORMpn, dq \ Set �2´norm of dppq to δ
End Module SOR_DRV168  Numerical Methods for Scientists and Engineers
SOR Method
‚ The basic SOR algorithm is easy to understand and implement;
‚ It usually converges faster than the Jacobi or Gauss-Seidel methods;
‚ For linear systems, the iteration process is accelerated for 1 ă ω ă 2;
however, the linear systems that diverge with SOR may converge for
0 ă ω ă 1;
‚ There is an optimum acceleration value (ωopt) that results in the
fewest iterations (or least cpu-time) and the highest convergence rate;
‚ Adaptive SOR algorithms that estimate ωopt are also available;
‚ Parallelization properties depend on the structure of the coefficient
matrix.
‚ The method, like Gauss-Seidel, may not converge for every linear
system; the coefficient matrix must satisfy certain properties (covered
in Section 3.3);
‚ Its performance can be sensitive to the choice of ωopt, which is difficult
and costly to estimate beforehand;
‚ Solving a linear system without a “smart estimate” for ωopt can lead
to a large number of iterations, which may be worse than that of the
Gauss-Seidel method.
A pseudomodule, SOR, for solving a system of linear equations using the SOR method
is presented in Pseudocode 3.2. In addition to JACOBI’s argument list, it requires a guess
(preferably a smart guess) for the relaxation parameter ω. The structure of the SOR module
is basically the same as that of JACOBI; however, the only difference is in the computation
of the current estimates in SOR_DRV, carried out by Eq. (3.10). Note that a separate code
for the Gauss-Seidel method is not given here as the SOR method becomes the GS method
for ω “ 1.
The module SOR_DRV performs one-step SOR iteration and, as input, it requires the
number of unknowns (n), the linear system (A and b), a near-optimal guess for ω, and an
initial guess (xo). The outputs are the current estimates (x) and �2-norm of the displacement
vector (δ). Setting ω1 “ 1 ´ ω before moving on to calculate the current estimates helps
save cpu-time, especially for large numbers of iterations and very large matrices. The outer
loop i sweeps the equations from top to bottom, while the inner loop j sweeps the terms of
the ith equation from left to right. If the current estimate of the jth unknown is available,
aij ˚ xj (i ą j) or, if not, aij ˚ xoj (i ă j) terms are added to the accumulator (the sum
of off-diagonal terms) using a couple of nested If-structures. The displacement vector d is
calculated and used to estimate the error at the current iteration level by calculating the
Euclidean norm, δ “ d2, obtained with the module ENORM and passed to SOR to aid in
the convergence decision. The iteration procedure in SOR is terminated in the same manner
as described for the Jacobi method.
From this point on, the L and U matrix notations will be used
to denote the lower and upper triangular matrices, whose primary
diagonal elements are zero. They should not be confused with those
matrix definitions in the LU decomposition methods where �ii ‰ 0
and uii ‰ 0 for all i.Linear Systems: Iterative Methods  169
3.3 CONVERGENCE OF STATIONARY ITERATIVE METHODS
For the formal theory of the stationary iterative methods, we will decompose the coefficient
matrix A as A “ D ´ L ´ U, where D, U, and L are diagonal, upper, and lower triangular
matrices, respectively.
A system of linear equations may now be written in matrix form as
Dx “ pL ` Uqx ` b (3.11)
Using superscript notation to incorporate the iteration steps, Eq. (3.11) can then be ex￾pressed as
Dxpp`1q “ pL ` Uqxppq ` b (3.12)
where all diagonal elements are kept on the left and off-diagonal elements on the rhs, which
is equivalent to the matrix form of the Jacobi method. Note that if aii ‰ 0 for all i, then
the inverse of a diagonal matrix D exits, and it is simply a diagonal matrix consisting of
the reciprocals of the elements on the diagonal, i.e., dii “ 1{aii.
Solving Eq. (3.12) for xpp`1q yields
xpp`1q “ MJ xppq ` D´1b (3.13)
where MJ “ D´1pL ` Uq is called the Jacobi iteration matrix.
The Gauss-Seidel iteration equations can be similarly expressed in matrix form as
Dxpp`1q “ Lxpp`1q ` Uxppq ` b (3.14)
where Lxpp`1q denotes the terms in which are updated with the current estimates. Now,
solving Eq. (3.14) for xpp`1q gives
xpp`1q “ MGSxppq ` pD ´ Lq
´1
b (3.15)
where D ´ L is a lower triangular matrix, and MGS “ pD ´ Lq
´1
U is referred to as the
Gauss-Seidel iteration matrix.
Applying the same rationale to the SOR iteration equations, we obtain
xpp`1q “ pD ´ ωLq
´1`
p1 ´ ωqD ` ωU˘
xppq ` ωpD ´ ωLq
´1 b (3.16)
where the SOR iteration matrix, Mω, takes the following form:
Mω “ pD ´ ωLq
´1`
p1 ´ ωqD ` ωU˘ (3.17)
In order to establish a general framework for the convergence properties of iterative
methods, consider the following iteration equations in fixed-point matrix form:
x “ Mx ` c (3.18)
where M is the iteration matrix and c is a vector. Then any iteration scheme may be
expressed as follows:
xpp`1q “ Mxppq ` c (3.19)
where xpp`1q and xppq have the usual meanings.
Subtracting Eq. (3.19) from Eq. (3.18) gives
epp`1q “ Meppq
, p “ 0, 1, 2,... (3.20)
where eppq “ x ´ xppq is the true error at the pth iteration step.170  Numerical Methods for Scientists and Engineers
Successively employing Eq. (3.20), the error at the pth iteration step becomes
eppq “ Mpep0q
, p “ 0, 1, 2,... (3.21)
For an arbitrary initial guess xp0q (or ep0q), the sequence of estimates should converge to x
as p Ñ 8 if limpÑ8eppq “ 0, where 0 is the zero vector. Hence, in order for an iterative
algorithm to converge (i.e., for errors to decay) for any arbitrary initial guess vector, the
necessary and sufficient condition is
limpÑ8Mp Ñ O
where O is the zero matrix.
Now suppose M has n real eigenvalues λk (k “ 1, 2,...,n) with |λ1| ą |λ2| ě |λ3| ě
... ě |λn| and corresponding linearly independent eigenvectors, vk. (Recall that the eigen￾values and eigenvectors of a square matrix are presented in Section 2.1.6, and they are
covered in greater detail in Chapter 11.) The eigenvectors can then be used as the basis of
n-dimensional vector space, and the arbitrary error vector ep0q with its n components can
also be uniquely expressed as a linear combination of them; specifically,
ep0q “ ÿn
i“1
civi (3.22)
where ci’s are scalar constants. Hence, setting p “ 1 in Eqs. (3.21) and (3.22) and using
Mvi “ λivi, we may write
ep1q “ Mep0q “ ÿn
i“1
ciMvi “ ÿn
i“1
ciλivi (3.23)
Multiplying Eq. (3.23) by M successively and similarly using the generalized identity
Mpvi “ λp
i vi, we obtain
eppq “Mpep0q “ ÿn
i“1
ciMpvi “ ÿn
i“1
ciλp
i vi “λp
1
!
c1v1`
´λ2
λ1
¯p
c2v2`...`
´λn
λ1
¯p
cnvn
)
(3.24)
This expression indicates that eppq will tend to zero vector as p tends to infinity for any
arbitrary ep0q if and only if |λi| ă 1 for all i. Also note that, for sufficiently large p, pλk{λ1qp
terms for k “ 2, 3,...,n go to zero, yielding eppq – λp
1c1v1 or likewise epp`1q – λp`1
1 c1v1.
Thus, we can deduce epp`1q – λ1eppq
, which by taking the �2´norm of both sides and
rearranging, we get
›
›epp`1q
›
›
2
›
›eppq
›
› 2
– |λ1| “ ρpMq (3.25)
where ρpMq denotes the spectral radius of the iteration matrix M, corresponding to the
dominant (or largest magnitude) eigenvalue. In cases where a matrix has complex eigenval￾ues, the spectral radius can be interpreted as the radius of the smallest circle centered at
the origin in the complex plane that contains all the eigenvalues of the matrix, either on
the circle or inside.
Up until now, not much has been said about “under what conditions” iterative methods
converge. However, it is also clear from Eq. (3.25) that the spectral radius of iteration matrix
M must be less than “1” (ρpMq ă 1) so that the errors decay as p increases; otherwise,
the iterative method will diverge. In this respect, the necessary and sufficient condition forLinear Systems: Iterative Methods  171
the convergence of the Jacobi, Gauss-Seidel, or SOR methods can be stated as ρpMJ q ă 1,
ρpMGSq ă 1, or ρpMωq ă 1. Having said that, it should be pointed out that the computation
of the spectral radius can be very expensive and time-consuming. Instead, an estimate for
the spectral radius of a matrix A can be obtained with the aid of the Gerschgorin Theorem,
which follows:
ρpAq ď min ˜
max
i
ÿ
j
|aij |, max
j
ÿ
i
|aij |
¸
(3.26)
This expression indicates that the dominant eigenvalue is less than or equal to the minimum
of the maximum �1´norms of the row or column vectors of matrix A.
In general, the diagonal dominance of a coefficient matrix A is a sufficient condition
for convergence; that is, a matrix A is said to be strictly diagonally dominant if
|aii| ą ÿn
j“1,j‰i
|aij | for all i (3.27)
for every row with strict inequality for all i. On the other hand, the matrix A is said to be
weakly diagonally dominant, with strict inequality at least for one i.
|aii| ě ÿn
j“1,j‰i
|aij | for all i
In the literature, the term “diagonally dominant” is often used rather than “weakly
diagonally dominant.” Note that the diagonal dominance is a sufficient condition, not a
necessary condition for the convergence of the stationary iterative methods. Usually, the
more dominant the diagonal is, the more rapidly the system will converge. It should also be
pointed out that even though this may seem like a very restrictive condition, the system of
linear equations resulting from the numerical treatment of applied boundary value problems
does satisfy the diagonal dominance condition.
If A is diagonally dominant coefficient matrix, then the Jacobi
or Gauss-Seidel methods do converge for any initial guess vector,
xp0q. However, there are cases where Jacobi, Gauss-Seidel, or both
methods may converge to an approximate solution even if A does
not meet the sufficient condition.
3.3.1 RATE OF CONVERGENCE
When solving large linear systems with iterative methods, it is important that the methods
be used under “conditions” that result in rapid convergence. The speed at which a sequence
of estimates approaches its limit is called the rate of convergence. A faster rate indicates
that fewer iterations are required to obtain a reasonably accurate estimate.
The dominant eigenvalue of an iteration matrix provides critical information on the
rate of convergence. To quantify how fast an iterative algorithm will converge in terms of
ρpMq, we ask, “how many iterations does it take to reduce the error by a factor of 10?” For
simplicity, suppose one-decimal-place accuracy is gained by a single iteration; for instance,
accordingly and arbitrarily, we set epp`1q
2 “ 10´3 and eppq
2 “ 10´2. Using Eq. (3.25)
and taking the base-10 logarithm of eppq
2{epp`1q
2 gives
log10
eppq
2
epp`1q2
“ log10p10q “ log10 ˆ 1
|λ1|
˙
“ log10 ˆ 1
ρpMq
˙
“ ´ log10pρpMqq “ 1172  Numerical Methods for Scientists and Engineers
EXAMPLE 3.3: Predicting if a linear system converges or not
Determine whether or not the following linear system converges with the Jacobi
method.
»
–
4 ´1 2
´157
2 ´4 2
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
8
30
´2
fi
fl
SOLUTION:
Checking for diagonal dominance for each row, we find:
r1 : |4| ą |´1|`|2| , r2 : |5| ă |´1|`|7| , r3 : |2| ă |´4|`|2|
Note that “as is” the matrix A is not diagonally dominant; however, by interchanging
the second and third equations (rows), we get
»
–
4 ´1 2
2 ´4 2
´157
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
8
´2
30
fi
fl r2 Ø r3
Now checking for diagonal dominance of the rows, we find
r1 : |4| ą |´1|`|2| , r2 : |´4|“|2|`|2| , r3 : |7| ą |´1|`|5| ,
The first and third rows are strictly diagonally dominant. However, the sum of the
absolute values of the off-diagonal elements of the second row is equal to the absolute
value of the diagonal element. This makes the system (weakly) diagonally dominant.
Discussion: A system of linear equations can be reordered (rows are interchanged)
such that each diagonal element of the coefficient matrix is larger in magnitude than
the sum of the magnitudes of the other coefficients in that row. In this example, we
were able to interchange two rows, which illustrates that the system can be made
diagonally dominant. However, the determination of diagonal dominance by visual
inspection may not be so obvious for even fairly small matrices. Hence, it is better
to resort to the Gerschgorin Theorem when in doubt.
which is the number of decimal digits by which the error is reduced after one iteration. In
other words, mathematically speaking, the rate of convergence of the iterative scheme given
by Eq. (3.19) is defined as
R “ ´log10 pρpMqq (3.28)
Since, for convergence, 0 ă ρpMq ă 1, the number of decimal digits of accuracy gained per
iteration increases as ρpMq decreases. Clearly, the smaller the optical radius, the higher the
rate of convergence will be. Alternatively, for large p, using Eq. (3.25), we obtain
eppq
2 « λ1epp´1q
2 «¨¨¨« λp
1ep0q
2 “ rρpMqsp ep0q
2
The rρpMqsp term in this expression provides a measure of how the error norm decreases
after p iterations. Now, we find the minimum value of p from ρppMq ď ε as
rρpMqsp ď 10´m or pmin “
R
´ m
log10ρpMq
V
“
Qm
R
U
(3.29)Linear Systems: Iterative Methods  173
FIGURE 3.2: Variation of ρpMωq with ω and ρpMJ q.
where r¨s denotes the ceiling function. Thus, it can be seen that the number of iteration steps
required by a convergent iterative method to reduce the initial error vector by the factor
ε “ 10´m (m ą 0) is inversely proportional to the rate of convergence, R. Consequently,
the number of iterations to reduce the error by more than an order of magnitude is found
by the inverse of the rate, i.e., 1{R.
For a sufficiently large number of iterations and asymptotic con￾vergence rate R, the 1{R ratio gives approximately the number of
iterations needed to reduce the error by a factor of 10. The smaller
the spectral radius is, the larger the convergence rate becomes.
We now return to the SOR method, whose convergence rate is affected by the choice of
ω. Recall that there exists an optimum ω for which the convergence of the SOR method is
achieved with the minimum number of iterations. However, determining the best value for
ω is the major difficulty of the SOR method. It turns out that there is a direct relationship
between the spectral radii of the Jacobi and SOR iteration matrices resulting from the
discretization of elliptic PDEs. The detailed analysis of this topic is beyond this book’s scope,
and at this stage, we will emphasize this here without going into much detail. However, the
reader should consult Refs. [3, 12, 17, 29] for additional information on this topic.
The relationship between the spectral radii of the Jacobi and the SOR iteration matrices
is
pρpMωq ` ω ´ 1q
2 “ ω2ρ2pMJ qρpMωq (3.30)
where ρpMJ q and ρpMωq are the spectral radii of the Jacobi and the SOR iteration matrices,
respectively.
Solving Eq. (3.30), a quadratic equation, for ρpMωq yields
ρpMωq“#
1 ´ ω` 1
2ω2ρ2pMJ q ` ωρpMJ q
b
1´ω` 1
4ω2ρ2pMJ q, 0 ă ω ď ωopt
ω ´ 1, ωopt ď ω ă 2 (3.31)
where ωopt is the optimum relaxation parameter estimated by
ωopt “ 2
1 ` a1 ´ ρ2pMJ q (3.32)
The variation of the spectral radius of the SOR iteration matrix, ρpMωq, as a function174  Numerical Methods for Scientists and Engineers
of ω and ρpMJ q is illustrated in Fig. 3.2. Notice that the slope of the curve approaches
infinity as we approach ωopt from the left side, whereas it is linear on the right side. This
means that underestimating ωopt by a small margin can result in a larger increase in ρpMωq
than a small overestimation of equal magnitude. The convergence rate for the optimum is
ρpMωq “ ωopt ´ 1 for ωopt ď ω ă 2
The ωopt value depends on the size of the coefficient matrix, the strength of the diagonal
dominance, and the structure of the linear system. In general, a larger linear system resulting
from grid refining of elliptic PDEs yields a larger value of ωopt. A best estimate for the
optimum relaxation parameter is given by Eq. (3.32).
3.3.2 ESTIMATING OPTIMUM RELAXATION PARAMETER
There is not an easy and cheap way of determining ωopt in advance; however, a smart
estimate can be computed simultaneously while performing the SOR iterations. A relatively
simple algorithm for estimating the optimum relaxation parameter given in Ref. [19] is
described below.
When A is symmetric and positive definite, ρpMωq is minimum for ω “ ωopt. It can
be shown that ρpMGSq “ ρ2pMJ q from examining the eigenvalues of Mω at ω “ 1. Note
that we can replace ρ2pMJ q with ρpMGSq in Eq. (3.32) as the maximum eigenvalue of
MGS can be easily computed from Eq. (3.25) during the SOR iterations simply by setting
ω “ 1. But since the true error is not known, an error estimate at the mth iteration step is
approximated by δpm`1q – ˇ
ˇλpmq
ˇ
ˇ δpmq
, where δpmq “ �dpmq
�2, dpmq “ xpmq ´ xpm´1q
, and
λpmq Ñ λ1 for m Ñ 8 (or sufficiently large m). Based on these assumptions, the algorithm
is given as:
1. Perform m iterations (m «10-15 is sufficient in most cases) with ω =1, or iterate until
λ convergences to λ1 within a preset tolerance value, i.e., |1 ´ λpm´1q
{λpmq
| ă ε;
2. Compute �2´norm of the displacement vector at the end of the mth iteration step,
i.e., δpmq “ �dpmq
�2;
3. Perform additional k iterations with ω =1 (k “ 1 is sufficient in most cases) and
compute the �2´norm of the displacement vector, dpm`kq
, i.e., δpm`kq “ �dpm`kq
�2;
4. Compute an estimate for the maximum eigenvalue of the Gauss-Seidel iteration matrix
from λ1 – `
δpm`kq
{δpmq
˘1{k
. Note that the �2´norms of d at (m`k)th and mth steps
are now related to each other by δpm`kq – λk
1δpmq
;
5. Estimate ωopt after (m ` k)th iterations by
ωopt « 2
1 ` ?1 ´ λ1
6. Perform the subsequent iterations with ω “ ωopt.
EXAMPLE 3.4: Application of the SOR method
Repeat Example 3.1 using the SOR method with ω “ 1.1 and estimate the optimum
relaxation parameter.Linear Systems: Iterative Methods  175
SOLUTION:
The SOR iteration equations with ω “ 1.1 take the following form:
i
pp`1q
1 “ ´0.10i
ppq
1 `
1.1
3
`
i
ppq
2 ` 1.5
˘
i
pp`1q
2 “ ´0.10i
ppq
2 `
1.1
3
`
i
pp`1q
1 ` i
ppq
3 ` 1.5
˘
i
pp`1q
3 “ ´0.10i
ppq
3 `
1.1
3
`
i
pp`1q
2 ` i
ppq
4 ` 2
˘
i
pp`1q
4 “ ´0.10i
ppq
4 `
1.1
3
`
i
pp`1q
3 ` 5.5
˘
With i
p0q “ 1, the current estimates are found as
i
p1q
1 “ ´0.1p1q ` 1.1
3
`
1 ` 1.5
˘
“ 0.816667
i
p1q
2 “ ´0.1p1q ` 1.1
3
` 0.816667 ` 1 ` 1.5
˘
“ 1.116111
i
p1q
3 “ ´0.1p1q ` 1.1
3
` 1.116112 ` 1 ` 2
˘
“ 1.409241
i
p1q
4 “ ´0.1p1q ` 1.1
3
` 1.409241 ` 5.5
˘
“ 2.433388
where the boxed values are current estimates that became available from prior equa￾tions.
The iteration procedure is continued in the same manner until the stopping cri￾terion based on |dppq
|2 is met in five iterations. The SOR iterations clearly converged
to an approximate solution faster than the Jacobi and the Gauss-Seidel methods.
The history of computed estimates and the �2´norm of the displacement vector are
presented in Table 3.3. The fact that the SOR method converges to a solution in 5
iterations in comparison to 8 iterations in the Gauss-Seidel method (see Example
3.2) and 12 iterations in the Jacobi method (see Example 3.1) indicates that the
ω “ 1.1 value is close to the optimum relaxation parameter.
TABLE 3.3
p ippq
1 i
ppq
2 i
ppq
3 i
ppq
4 �dppq
�2
01 1 1 1
1 0.816667 1.116111 1.409241 2.433388 1.506377
2 0.877574 1.276888 1.952844 2.489371 0.572885
3 0.930435 1.479513 1.993306 2.498609 0.213481
4 0.999445 1.499391 1.999936 2.500116 0.072137
5 0.999832 1.499976 2.00004 2.500003 0.000718
To calculate a theoretical estimate for ωopt, the spectral radius of the Jacobi
iteration matrix, MJ , is required:176  Numerical Methods for Scientists and Engineers
MJ “ D´1pL+Uq “
»
—
—
–
3
3
3
3
fi
ffi
ffi
fl
´1 »
—
—
–
0100
1010
0101
0010
fi
ffi
ffi
fl “
»
—
—
–
0 1
3 0 0
1
3 0 1
3 0
0 1
3 0 1
3
0 0 1
3 0
fi
ffi
ffi
fl
The Jacobi iteration matrix is a tridiagonal Toeplitz matrix whose eigenvalues
can easily be obtained as λ “ ˘p1˘
?5q{6 using Eq. (2.15). The maximum magnitude
eigenvalue turns out to be |λ|max “ p1 ` ?5q{6 « 0.539; thus, using Eq. (3.25), the
spectral radius is found as ρpMJq “ |λmax| “ 0.539.
Discussion: Recall that the spectral radius of an iteration matrix is estimated by
Eq. (3.25) with �epp`1q
�2{�eppq
�2 where eppq “ x´xppq is the true error, which cannot
be determined exactly unless the true solution is known. Since limpÑ8 dppq “ eppq
,
the next best alternative for the error estimation is the displacement vector dppq “
xppq ´ xpp´1q
, which can be easily calculated at the end of every iteration.
Also recall that the �dpp`1q
�2{�dppq
�2 ratios in Examples 3.1 and 3.2 had been
computed and presented in Tables 3.1 and 3.2. As it should be clear from the above
discussions, this ratio should converge to the spectral radius (or dominant eigenvalue)
as p increases. The spectral radii for the Jacobi and Gauss-Seidel iteration matrices
for Examples 3.1 and 3.2 have been found to be λJ,max – 0.539 and λGS,max –
0.291, respectively. These findings also verify the ρpMGSq “ ρ2pMJ q relation, i.e.,
λGS,max “ λ2
J,max.
Now that ρpMJ q is available, the optimum relaxation parameter can be estimated
from Eq. (3.32) to find ωopt, which yields 1.086 or rounded to 1.1. Hence, it should
not be surprising that the SOR method converged to a solution with relatively fewer
iterations than Gauss-Seidel since ω “ 1.1 « ωopt. Nonetheless, it is clear that
extending this analysis (of determining ρpMJ q and ωopt) to very large matrices is a
very laborious and time-consuming task, even for computers. In practice, relatively
rough estimates of the spectral radius can be given with minimal computation, using
algorithms similar to those given in Section 3.3.2.
EXAMPLE 3.5: Estimating optimum relaxation parameter
Eleven springs with the same spring constants (k) and same unstretched lengths
(� “ 15 cm) are attached to each other in series, as shown in the figure below.Linear Systems: Iterative Methods  177
While fixed on the left-hand side, this system is stretched from the right side out
to Point A, yielding a total length of L “ 2.2 m. The positions x1, x2,...,x10 denote
the distances with respect to Point O. The equilibrium equations can be written as
kpx1 ´ �q “ k ppx2 ´ x1q ´ �q,
k ppxi ´ xi´1q ´ �q “ k ppxi`1 ´ xiq ´ �q, i “ 2, 3,..., 9
k ppx10 ´ x9q ´ �q “ k ppL ´ x10q ´ �q
(a) Apply the Jacobi and Gauss-Seidel methods to solve the resulting system of
equations for the positions and compute the convergence rate for each case; (b)
estimate the theoretical value of ωopt; and (c) solve the linear system using the SOR
method, starting with ω “ 1 and increasing in increments Δω “ 0.05 up to ω “ 1.95
and then plot the relaxation parameter versus the total number of iterations, i.e.,
ω ´ N. Use xp0q “ 0 and |dppq
|2 ă 10´5. The true solution of the linear system is
given as xi “ i{5 for i “ 1, 2,..., 10.
SOLUTION:
(a) Simplifying and rearranging the equilibrium equations, following tridiagonal
(Toeplitz) system is obtained:
»
—
—
—
—
—
—
—
—
—
—
–
2 ´1
´1 2 ´1
´1 2 ´1
... ... ...
´1 2 ´1
´1 2
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
—
—
–
x1
x2
x3
.
.
.
x9
x10
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
—
—
—
–
0
0
0
.
.
.
0
L
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
To estimate the convergence rate of the method, the maximum magnitude eigenvalue
of the Jacobi iteration matrix is constructed as
MJ “ D´1pL+Uq“´1
2
»
—
—
—
—
—
—
—
—
—
—
–
0 1
10 1
10 1
... ... ...
1 01
1 0
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
The eigenvalues of MJ are found from Eq. (2.15) as λi “ cospiπ{11q for i “
1, 2,..., 10, and the spectral radius is found as ρpMJ q “ λmax “ 0.95949. Using
Eq. (3.28), the convergence rate of the Jacobi method is found from R “ ´log
(0.95949)=0.017958. The number of iteration steps required by the Jacobi method
to reduce the maximum error by a factor of 10 is p ą 1{R « 56. A total of 227
iterations are required for the linear system to converge to an approximate solution
within the given tolerance.
The Gauss-Seidel method, on the other hand, converges to an approximate solu￾tion with 117 iterations. The Gauss-Seidel iteration matrix (not in tridiagonal form)
yields178  Numerical Methods for Scientists and Engineers
MGS “ pD ´ Lq
´1
U “
»
—
—
—
—
—
—
—
—
—
—
—
—
—
–
0 1
2 0 ¨¨¨ 000
0 1
22
1
2 ¨¨¨ 000
.
.
. .
.
. .
.
. ... .
.
. 1
2 0
0 1
29
1
28 ¨¨¨ 1
23
1
22
1
2
0 1
210
1
29 ¨¨¨ 1
24
1
23
1
22
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Finding the eigenvalues of this matrix, covered in Chapter 11, is not an easy
task. At this point, the computation of the eigenvalues is left to the reader, and
only the results are presented here. The maximum magnitude eigenvalue is found as
λmax “ 0.062063, i.e., ρpMGSq “ 0.92063. The convergence rate of the Gauss-Seidel
method is similarly derived from Eq. (3.28) as R “ ´ logp0.92063q “ 0.035925,
and we find p ą 1{R « 28, which is the number of iteration steps required by the
Gauss-Seidel method to reduce the maximum error by a factor 10.
(b) Since ρpMJ q “ 0.95949, the optimum relation parameter can be estimated
by Eq. (3.32) as
ωopt “ 2
1 ` a1 ´ ρ2pMJ q
“ 2
1 ` a1 ´ 0.959492 “ 1.56
It should also be pointed out that ρpMGSq “ ρ2pMJq relationship is also verified.
(c) The SOR solutions for ω’s from 1 to 1.9 with 0.1 intervals were obtained,
and the total number of iterations was plotted against the relaxation parameter ω
and presented in Fig. 3.3. It is observed that the optimum value of the relaxation
parameter occurred at about ω « 1.6 (with 26 iterations). When we repeat the SOR
solutions for ω “ 1.56, 1.57, 1.58, and 1.59 to narrow down the optimum value, we
achieve convergence with 28, 24, 27, and 27 iterations, respectively. This indicates
that the optimum relaxation parameter is realized at ω “ 1.57 with 24 iterations,
which is consistent with the theoretical prediction.
FIGURE 3.3
The SOR iteration matrix, Mω, is also not in tridiagonal form, and it is too largeLinear Systems: Iterative Methods  179
and complicated; thus, it will not be reported here to save space. However, we will
leave this task as an exercise for the reader to perform using the symbolic processor.
The dominant (maximum magnitude) eigenvalue can be determined by a suitable
method presented in Chapter 11 as well. Nonetheless, we already know by Eq. (3.31)
that the spectral radius of the SOR method is given as ρpMωq “ ω ´ 1 for ωopt ď
ω ď 2. So we may use this expression to find ρpMωq “ ω ´ 1 “ 0.56 for ωopt “ 1.56.
The convergence rate of the SOR method with the optimum relaxation parameter
is then found as R “ ´log10ρpMωq“´log10p0.56q “ 0.57982. In other words, the
SOR with ωopt “ 1.56 requires 1/R «2 steps to reduce the maximum error by a
factor 10.
Discussion: Note that the Gauss-Seidel method in this example also converges
twice as fast as the Jacobi method, and the number of iterations is half that of the
Jacobi method, as the convergence rates of the Jacobi and Gauss-Seidel methods are
found to be 0.017958 and 0.035925, respectively. The Gauss-Seidel and SOR iteration
matrices are very complicated, even for simple tridiagonal (coefficient) matrices, and
in practice, theoretical estimation of the ωopt is computationally not feasible. It is
clear that estimating ωopt without the need for too many calculations and using the
SOR method with ωopt could provide a computational advantage. However, there
are algorithms to estimate ωopt, and an effective algorithm has been provided in this
section.
FIGURE 3.4
The Euclidean (�2) norm of the displacement vector is computed and compara￾tively depicted for the Jacobi, Gauss-Seidel, and SOR (with ωopt) methods in Fig.
3.4. It is observed that the steady convergence rates are established very quickly (in
«15 to 30 iterations). Fig. 3.4 clearly shows that the steady convergence rates for
all three methods are linear, and the slopes of the straight lines also correspond to
the theoretical convergence rates.180  Numerical Methods for Scientists and Engineers
3.4 KRYLOV SPACE METHODS
3.4.1 CONJUGATE GRADIENT METHOD (CGM)
The conjugate gradient method (CGM) developed by Hestenes and Stiefel [14] is an effective
iterative method used to solve symmetric positive-definite systems of linear equations. The
method is usually applied to sparse systems that are impractical to handle by direct solution
methods, such as Gauss elimination or Cholesky decomposition.
The method is based on minimizing the quadratic scalar function, defined as
fpxq “ 1
2
xT Ax ´ bT x (3.33)
where A is a square symmetric positive definite matrix, and the gradient of the scalar func￾tion fpxq is gradf “ Ax ´ b. Minimizing the gradient of fpxq requires setting Ax ´ b “ 0.
In other words, minimizing fpxq is equivalent to obtaining the solution of a system of linear
equations given by Ax “ b.
The gradient minimization is an iterative procedure that requires an initial guess, xp0q.
The current step estimate is improved as follows:
xpp`1q “ xppq ` αppq
dppq (3.34)
where xppq is the estimate at the pth iteration level, αppq is a scalar called the line search
parameter which is chosen at the pth iteration step so that fpxppq
q is minimized in the
vector of conjugate search direction, dppq
. This step is called the line search.
Substituting the current (improved) estimate, Eq. (3.34), into Axpp`1q “ b yields
Axppq ` αppq
Adppq “ b (3.35)
Noting that b ´ Axppq is the residual vector at the pth iteration step (denoted by rppq
), Eq.
(3.35) can be rearranged as
αppq
Adppq “ rppq (3.36)
Once the residual vector is known, the orthogonal (conjugate) search directions can be
obtained by multiplying both sides of Eq. (3.36) with pdppq
q
T and solving for αppq
:
αppq “ pdppq
q
T
rppq
pdppqq
T Adppq “ pdppq
, rppq
q
pdppq, Adppqq (3.37)
where we have now used the inner product notation, i.e., px, yq “ xT y.
By left-multiplying Eq. (3.34) with A and subtracting b from both sides, we obtain
b ´ Axpp`1q “ b ´ Axppq ´ αppq
Adppq
After simplifications, this expression gives a recursive relationship involving the residual
vectors of two successive iterations:
rpp`1q “ rppq ´ αppq
Adppq (3.38)
It should be noted that the residuals are also orthogonal, i.e.,prppq
, rpp`1q
q“prpp`1q
, rppq
q “
0. The next search direction is found from
dpp`1q “ rpp`1q ` βppq
dppq (3.39)Linear Systems: Iterative Methods  181
where the conjugation parameter βppq is also a scalar quantity determined such that
pdpp`1q
, Adppq
q “ 0, i.e., the two successive search directions are conjugate.
Substituting Eq. (3.39) into this inner product yields
pdpp`1q
, Adppq
q “ 0 “ prpp`1q
, Adppq
q ` βppq
pdppq
, Adppq
q (3.40)
and solving Eq. (3.40) for βppq leads to
βppq “ ´prpp`1q
, Adppq
q
pdppq, Adppqq (3.41)
It turns out that the previous search direction and the current residual vector are also
orthogonal, i.e., prpp`1q
, dppq
q “ 0. Equation (3.41) can be further simplified by also recalling
that prpp`1q
, rppq
q “ 0. For the numerator of Eq. (3.41), the inner product of Eq. (3.38) with
rpp`1q
, leads to
prpp`1q
, Adppq
q“´ 1
αppq prpp`1q
, rpp`1q
q (3.42)
From Eq. (3.38), we may write Adppq “ prpp`1q´rppq
q{αppq
. Also recalling pdppq
, rpp`1q
q “ 0,
the denominator of Eq. (3.41) results in
pdppq
, Adppq
q “ 1
αppq
´
pdppq
, rpp`1q
q´pdppq
, rppq
q
¯
“ ´ 1
αppq pdppq
, rppq
q (3.43)
Finally, using Eq. (3.39) to get dppq and substituting it into Eq. (3.43) gives
pdppq
, Adppq
q“´ 1
αppq
´
prppq
, rppq
q ` βpp´1q
pdpp´1q
, rppq
q
¯
“ ´ 1
αppq prppq
, rppq
q (3.44)
Now combining Eqs. (3.42) and (3.44) in Eq. (3.41), we arrive at
βppq “ prpp`1q
, rpp`1q
q
prppq, rppqq (3.45)
Initial Guess. If a rough estimate for the solution vector x is available, it can be used
as the initial value. Otherwise, setting xp0q “ 0 is sufficient for most cases since the CGM
eventually converges in at most n iterations.
Stopping Criterion. In the CGM algorithm, residuals at two iteration levels (rppq and
rpp`1q
) are available, so there is no need to specify absolute or relative errors in terms of the
displacement vector. The simplest and most common absolute and relative stopping criteria
are based on the �2 norms of the available residuals at the end of each iteration step:
rppq
2 “
´
rppq
, rppq
¯1{2
ă ε1 or rppq
2
b2
ă ε2 (3.46)
where ε1 and ε2 are the preset tolerances for the absolute and relative errors, respectively.
One or both criteria can be employed at no additional cost, with the first criterion being
cast as rppq
2 ă ε1 ` ε2b2. Note that setting ε1 “ 0 and ε2 ą 0 and ε2 “ 0 and ε1 ą 0
give relative and absolute errors, respectively.
Providing an initial guess is critical for nonlinear optimization
problems, which may possess several local extrema. In such cases,
the choice of the initial guess affects whether the method will
converge or not, or to which solution set it will converge.182  Numerical Methods for Scientists and Engineers
Conjugate Gradient Method
‚ The CGM is a simple method to program and easy to parallelize;
‚ Theoretically, in the absence of round-off errors, it yields the true
solution after a finite number of iterations (as many or less than the
number of equations);
‚ It converges, if it does so, with a linear convergence rate dictated by
its condition number, i.e., the smaller the condition number is, the
faster the convergence will be;
‚ It is particularly suitable and useful for large sparse systems with
regular patterns.
‚ The method is applicable to linear systems with positive definite and
symmetric coefficient matrices;
‚ It may exhibit instabilities even for small perturbations;
‚ It is computationally more expensive in comparison to stationary
iterative methods per iteration step and requires more memory allo￾cation;
‚ In large sparse systems, the rate of convergence may become quite
slow unless an effective preconditioning is employed;
‚ In practice, the true solution may never be obtained as most direc￾tions are not conjugate, or round-off errors due to finite-precision
arithmetic may result in a loss of orthogonality in the residuals.
A pseudomodule, CGM, employing the CGM on a linear system is presented in Pseu￾docode 3.3. As input, the module requires the number of unknowns (n), the linear system
(A and b), a vector of initial guess (xp0q
), an upper bound for the number of iterations
(maxit), and a convergence tolerance (ε). The approximate solution (x) and the number of
iterations performed (iter) are the outputs of the module. The module uses Pseudocodes 2.3
and 2.5 to compute the x ¨ y and Ax products, respectively. The residual vector computed
by rp0q “ b ´ Axp0q is set as the starting direction, dp0q “ rp0q. The For-loop over p is the
iteration loop. The �2-norm of the residual vector is found from ρppq “ aprppq, rppqq and is
used to terminate the iteration procedure when the convergence is achieved. For ρppq ą ε,
αppq is computed by Eq. (3.37) to obtain xpp`1q and corresponding residual rpp`1q using
Eqs. (3.34) and (3.38), respectively. The iteration process is repeated with a new direction
determined by Eq. (3.39) along with Eq. (3.45) until the stopping criterion is satisfied.
In applying iterative methods to sparse linear systems, it is easier
to perform computations with the non-zero matrix elements only.
A programming strategy avoiding arithmetic operations with ze￾ros results in a significant reduction in cpu-time.
As far as the memory requirement is concerned, the module needs additional memory
space to be allocated for c, d, and r, and they can be used for prior and current step
quantities and may be overwritten to save the current step values. If A is a sparse orLinear Systems: Iterative Methods  183
banded matrix, the memory requirement can be further reduced by employing banded or
sparse matrix storage solutions.
Pseudocode 3.3
Module CGM (n, ε, A, b, x, maxit, iter, error)
\ DESCRIPTION: A pseudomodule to solve Ax “ b using the CGM method.
\ USES:
\ Ax :: Module to perform matrix-vector multiplication (Pseudocode 2.5);
\ XdotY:: Function giving the dot product of two vectors (Pseudocode 2.3).
Declare: ann, bn, xn, rn, cn, dn \ Declare array variables
Ax(n, A, x, r) \ Visit Ax to compute Axp0q “ rp0q
r Ð b ´ r \ Find residual rp0q “ b ´ Axp0q
d Ð r \ Initialize dp0q with rp0q
ρ0 Ð XdotY(n, r, r) \ Compute ρ0 “ prp0q, rp0qq
For “
p “ 0, maxit‰ \ Begin iteration loop
R Ð ?ρ0 \ Find rppq
2
Write: “Iteration=”, p,“rppq
2=”,R \ Printout iteration progress
If “
R ă ε
‰
Then \ Check for convergence, rppq
2 ă ε?
Exit \ Converged, EXIT loop
Else \ Not converged, continue iterations
Ax(n, A, d, c) \ Find Ax to compute Adppq “ cppq
ρ ÐXdotY(n, d, c) \ Compute ρ “ pdppq
, cppq
q
α Ð ρ0{ρ \ Find αppq
x Ð x ` α ˚ d \ Find current estimate, xpp`1q
r Ð r ´ α ˚ c \ Find current residual, rpp`1q
ρ ÐXdotY(n, r, r) \ Find ρpp`1q “ prpp`1q
, rpp`1q
q
β Ð ρ{ρ0 \ Find βppq “ ρpp`1q
{ρppq
d Ð r ` β ˚ d \ Find new direction dpp`1q
ρ0 Ð ρ \ Set ρpp´1q Ð ρppq
End If
End For \ End of iteration loop
error Ð R \ Set current �2´norm as error
iter Ð p \ Set current p ´ 1 as iter on exit
\ If iteration limit maxit is reached with no convergence
If “
iter “ maxit‰
Then \ Issue info and a warning
Write: “CGM failed to converge after”,maxit,“iterations”
Write: “The error at the last step =”,error
End If
End Module CGM
EXAMPLE 3.6: Application of Conjugate Gradient Method
Apply the conjugate gradient method to estimate the solution of the following linear
system. Use xp0q “ 1 and r2 ă 10´5.
»
—
—
–
8121
1612
2171
1218
fi
ffi
ffi
fl
»
—
—
–
x1
x2
x3
x4
fi
ffi
ffi
fl “
»
—
—
–
´5
3
11
´13
fi
ffi
ffi
fl184  Numerical Methods for Scientists and Engineers
SOLUTION:
Notice that the coefficient matrix is symmetric and positive-definite. Starting
with xp0q “ r 1111 s
T , the initial residual is found as
rp0q “ b ´ Axp0q “ r´17 ´ 7 0 ´ 25 s
T
Setting dp0q “ rp0q, we find
ρp0q “ prp0q
, rp0q
q “ p´17q
2 ` p´7q
2 ` 02 ` p´25q
2 “ 963
cp0q “ Adp0q “ r´168 ´ 109 ´ 66 ´ 231s
T
pdp0q
, cp0q
q “ p´17qp´168q ` p´7qp´109q ` 0p´66q ` p´25qp´231q “ 9394
Next, we calculate the step length and the current estimate as
αp0q “ ρp0q
pdp0q, cp0qq “ 963
9394 “ 0.102512
xp1q “ xp0q ` αp0q
dp0q “ r´0.742708 0.282414 1 ´ 1.562806 s
T
The residual associated with the current estimate is obtained as follows:
rp1q “ rp0q ´ αp0q
cp0q “ r 0.222057 4.173834 6.765808 ´ 1.319672 s T
with ρp1q “ prp1q, rp1q
q “ 64.9879 and rp1q
2 “ aρp1q “ 8.06151 ą ε. Then the
conjugation parameter is obtained as
βp0q “ ρp1q
ρp0q “ 64.9879
963 “ 0.0674848
To carry out the second iteration, we need to determine, dp1q and cp1q:
dp1q “ rp1q ` βp0q
dp0q “r´0.925185 3.701440 6.765808 ´ 3.006792 s T
cp1q “ Adp1q “ r 6.824784 22.035679 46.204934 ´ 10.810833 s T
which yields pdp1q
, cp1q
q “ 420.369. The step length is then calculated as
αp1q “ ρp1q
pdp1q, cp1qq “ 64.9879
420.369 “ 0.154597
The second estimate and its associated residuals are found as
xp2q “ xp1q ` αp1q
dp1q “ r 0.885739 0.854647 2.045975 ´ 2.02765 s T
rp2q “ rp1q ´ αp1q
cp1q “ r´0.833035 0.767180 ´ 0.377345 0.351652 s T
ρp2q “ prp2q
, rp2q
q “ 1.54856 and rp2q
2 “
b
ρp2q “ 1.24442 ą ε
Next, the selection of the new, improved direction is determined as follows:
βp1q “ ρp2q
ρp1q “ 1.54856
64.9879 “ 0.023828
dp2q “ rp2q ` βp1q
dp1q “r´0.855081 0.855379 ´ 0.216127 0.280005 s TLinear Systems: Iterative Methods  185
The third estimate, in similar fashion, yields
cp2q “ Adp2q “ r´6.137518, 4.621078, ´2.087665, 2.879593 s
T
pdp2q
, cp2q
q “ 10.4584, αp2q “ ρp2q
pdp2q, cp2qq “ 1.54856
10.4584 “ 0.1480685
xp3q “ xp2q ` αp2q
dp2q “ r´1.01235, 0.981302, 2.013973, ´1.98619s
T
rp3q “ rp2q ´ αp2q
cp2q “ r 0.075738, 0.082943, ´0.068228, ´0.074725s
T
ρp3q “ prp3q
, rp3q
q “ 0.0228547 and rp3q
2 “
b
ρp3q “ 0.15118 ą ε
βp2q “ ρp3q
ρp2q “ 1.54856
64.9879 “ 0.014759
dp3q “ rp3q ` βp2q
dp2q “ r 0.063118 0.095568 ´ 0.071418 ´ 0.070592 s T
Finally, at the fourth iteration, we obtain
cp3q “ r 0.387085 0.423922 ´ 0.348712 ´ 0.381902 s T ,
pdp3q
, cp3q
q “ 0.11681, αp3q “ 0.195659
xp4q “ xp3q ` αp3q
dp3q “ r´112 ´ 2 s
T and rp4q “ r 0000 s
T
Discussion: The CGM is guaranteed to converge in at most n steps, where n is
the number of equations. Using double precision computation, the CGM yielded the
true solution accurate to 16 decimal places in four iterations (i.e., n “ 4 steps). (The
final estimates were rounded to the nearest value.) When the same system is solved
with the Jacobi (Example 3.1) and Gauss-Seidel methods (Example 3.2), the results
were obtained with 22 and 8 iterations, respectively. On the other hand, the SOR
method with ωopt “ 1.092 converged to the solution in 4 iterations.
3.4.2 CONVERGENCE ISSUES OF THE CGM
For a symmetric positive definite Ax “ b, an upper error bound for the pth estimate is
given as
eppq
A 
ep0qA
ď 2
˜aκpAq ´ 1
aκpAq ` 1
¸p
, p ě 0, κpAq ą 1 (3.47)
where eA “ ape, Aeq, eppq “ x ´ xppq denotes the true error vector, and κpAq “
A2A´12 denotes the spectral condition number of A. In order to simplify this ex￾pression, we make use of px ´ 1q{px ` 1q ă expp´2{xq in Eq. (3.47) as follows:
›
›eppq
›
›
›
A
›ep0q
›
›
A
ă 2 exp ˜
´ 2p
aκpAq
¸
, p ě 0, (3.48)
For a positive definite and symmetric A, the condition number can be expressed as
κpAq “ λmax{λmin, where λ denotes the eigenvalues of A. It should be noted that for
κpAq « 1, the convergence is faster than the theoretical estimate as the eigenvalues are
clustered closely together. The number of iterations required to reduce the error norm by a186  Numerical Methods for Scientists and Engineers
fixed amount is in the order of Op
aκpAqq. The systems with κpAq " 1 are the cases where
the eigenvalues of A spread over a wide range, which leads to extremely slow convergence
rates. The convergence of such systems can be accelerated by applying the procedure to an
equivalent preconditioned system.
3.4.3 PRECONDITIONING
The performance of the CGM is very good when applied to well-conditioned systems, i.e.,
linear systems having κpAq « 1. If a linear system is ill-conditioned (κpAq " 1), then the
numerical solution is plagued with round-off errors, affecting the convergence rate adversely.
In fact, the error bound, Eq. (3.47), indicates that the CGM may converge very slowly even
for a system that has a moderate condition number. In cases where the CGM and its
variants are expected to converge slowly, the linear system is transformed into a problem
with a smaller condition number so that rapid convergence is assured.
The technique of accelerating the convergence of a linear system by reducing its con￾dition number is called preconditioning. A preconditioner, a matrix, is used to modify the
original linear system so that it is easier and faster to solve iteratively.
The preconditioned form of Ax “ b is written as
P´1Ax “ P´1b (3.49)
where A is a symmetric positive-definite matrix and P is an invertible preconditioner matrix.
The condition number of the modified matrix is reduced by selecting a reasonable P, i.e.,
κpP´1Aq ă κpAq. The effectiveness of a preconditioner depends on κpP´1Aq.
The residual form of the preconditioned system is written as Pz “ r. This residual
vector r is used to compute the scalar α and β parameters. The preconditioned linear
system is solved at each iteration step; thus, the preconditioner should be selected such
that the cost of solving Pzppq “ rppq is minimum. Preconditioning increases the memory
requirement and computational effort to some degree. However, the additional cost incurred
is compensated by the reduction in the number of iterations required to reach an acceptable
solution.
A basic preconditioned conjugate gradient (PCGM) method algorithm is illustrated in
Algorithm 3.1.
3.4.4 CGM FOR NONSYMMETRIC SYSTEMS
The CGM presented in the previous section cannot be applied to non-symmetric linear
systems because the residual vectors will not be orthogonal with short recurrences. If A is
a non-symmetric matrix, then an equivalent symmetric system is solved. For example, left
multiplying the linear system with AT results in a symmetric positive definite system:
AT Ax “ AT b (3.50)
and �2´norm of the residual is minimized, i.e., �b ´ Ax�2. The algorithm derived from this
formulation is often called the conjugate gradient normal residual (CGNR). An alternative
approach also makes use of AT and requires setting x “ AT y. The resulting linear system
is now symmetric:
AAT y “ b (3.51)
The method based on this formulation is referred to as the conjugate gradient normal error
(CGNE). Once Eq. (3.51) is solved, the unknown vector x is simply obtained from x “ AT y.Linear Systems: Iterative Methods  187
\ ALGORITHM 3.1: Preconditioned Conjugate Gradient Method (PCGM)
rp0q Ð b ´ Axp0q \ Compute the initial residual
zp0q Ð P´1rp0q \ Solve linear system: Pzp0q “ rp0q
dp0q Ð zp0q \ Initialize dp0q “ zp0q
For “
p “ 0, maxit‰ \ Iteration loop p
\ Find current estimate and its residual
αpp`1q Ð prppq
, zppq
q{pdppq
, Adppq
q \ Find current αppq
xpp`1q Ð xppq ` αppq
dppq \ Find current solution, xpp`1q
rpp`1q Ð rppq ´ αppq
Adppq \ Find current residuals, rpp`1q
If “
rpp`1q
 ă ε
‰
Then \ Check for convergence
Exit \ Exit the iteration loop
Else
\ Not converged, compute new direction at pth step
zpp`1q Ð P´1rpp`1q \ Solve Pzpp`1q “ rpp`1q
βppq Ð prpp`1q
, zpp`1q
q{prppq
, zppq
q \ Find new β
dpp`1q Ð zpp`1q ` βppq
dppq \ Find new direction, d
End If
End For \ End iteration loop p
The CGNR and CGNE algorithms are presented in Algorithms 3.2 and 3.3, respec￾tively. Both of these methods can be employed (or coded) easily by modifying the CGM
algorithms. The scope of this topic is wide; Generalized Minimal Residual (GMRES), Bicon￾jugate Gradient (BiCGM), Quasi-Minimal Residual (QMR), Conjugate Gradient Squared
(CGS), and Biconjugate Gradient Stabilized (BiCGstab) are commonly encountered meth￾ods in the literature applicable to non-symmetric systems.
\ ALGORITHM 3.2: Conjugate Gradient Normal Residual method (CGNR)
rp0q Ð b ´ Axp0q \ Find initial residual
zp0q Ð AT rp0q \ Find zp0q “ AT r
p0q
dp0q Ð zp0q \ Set dp0q “ zp0q
For “
p “ 0, maxit‰ \ Iteration loop
ICommentCompute current estimate and its residual at (p ` 1)th step
αpp`1q Ð pzppq
, zppq
q{pAdppq
, Adppq
q
xpp`1q Ð xppq ` αppq
dppq
rpp`1q Ð rppq ´ αppq
Adppq
If “
rpp`1q
 ă ε
‰
Then \ Convergence achieved
Exit \ Exit the iteration loop
Else
ICommentNot convergenced, compute new direction at pth step
zpp`1q Ð AT rpp`1q \ Solve Pzpp`1q “ rpp`1q
βppq Ð pzpp`1q
, zpp`1q
q{pzppq
, zppq
q \ Find new β
dpp`1q Ð zpp`1q ` βppq
dppq \ Find new direction, d
End If
End For \ End the iteration loop-p
GMRES yields the smallest residual for a fixed number of iteration steps; however, the
memory requirements and computational costs are large. The BiCGM method requires ATx
product besides Ax, which increases the computational cost. Furthermore, minimization of
the residuals is not assured, and extremely erratic convergence behavior may be observed.188  Numerical Methods for Scientists and Engineers
Parallelization properties are similar to those for CGM. QMR was designed to overcome the
irregular convergence behavior of BiCGM. The method requires ATx product as well; hence,
the computational cost is slightly higher than BiCGM. The CGS converges about twice as
fast as the BiCGM, but its convergence behavior in some cases may be quite irregular,
and it might diverge if the initial guess is close to the solution. The computational cost is
similar to that of BiCGM. The BiCGstab method is designed as an alternative to that of
CGS because it avoids the erratic convergence patterns of CGS. The speed of convergence is
similar to that of CGS, and the computational cost is also similar to that of CGS or BiCGM.
More information on the use of iterative methods for solving linear systems, particularly
CGM and its derivatives, can be found in Refs. [4, 5, 11, 8, 18, 25, 29, 35, 37, 38].
Note that the condition number of AAT is the square the condi￾tion number of A, i.e., κpAT Aq “ κ2pAq. Slow convergence rates
can be observed, especially in the case of ill-conditioned systems.
\ ALGORITHM 3.3: Conjugate Gradient Normal Error method (CGNE)
rp0q Ð b ´ Axp0q \ Find initial residual
zp0q Ð AT rp0q \ Find zp0q “ AT r
p0q
For “
p “ 0, maxit‰ \ Iteration loop
\ Find current estimate and its residual at (p ` 1)th step
αppq Ð prppq
, rppq
q{pdppq
, dppq
q
xpp`1q Ð xppq ` αppq
dppq
rpp`1q Ð rppq ´ αppq
Adppq
If “
rpp`1q
 ă ε
‰
Then \ Convergence achieved
Exit \ Exit the iteration loop
Else
\ Not convergenced, compute new direction at pth step
βppq Ð prpp`1q
, rpp`1q
q{prppq
, rppq
q \ Find new β
dpp`1q Ð AT rpp`1q ` βppq
dppq \ Find new direction, d
End If
End For \ End the iteration loop-p
3.4.5 CHOICE OF PRECONDITIONERS
In many applications, such as the numerical solution of Poisson’s equation, the condition
number of the coefficient matrix could become quite high. Hence, it is important to modify
the system by using a suitable preconditioner to reduce the condition number. The pre￾conditioner P should be chosen such that the cost of solving the preconditioned (modified)
system is not too expensive in comparison to the original system.
The ideal preconditioner is the matrix itself (P “ A) which would reduce the condition
number to 1. However, inverting the matrix, P´1, would be cpu- and memory-intensive. On
the other hand, the choice of the simplest preconditioner is a diagonal matrix (P “ D)
because it is easily constructed by the reciprocals of the diagonal elements. The Jacobi (or
diagonal) preconditioner is a diagonal matrix having the diagonal entries of matrix A; that
is,
P “ D “
"
aii, i “ j
0, i ‰ j
where aii ‰ 0 for all i. The Jacobi preconditioner can be very efficient if A is symmetricLinear Systems: Iterative Methods  189
(positive definite) and strictly diagonally dominant. Additionally, a diagonal preconditioner
may be damped as P “ D{ω, where ω is a damping parameter (ω ‰ 0). For non-symmetric
matrices, the common choice of D is
dii “
gffeÿn
j“1
a2
ij for i “ 1, 2,...,n
The Gauss-Seidel and the SOR preconditioners are, respectively, defined as P “ D ` L
and P “ D ` ωL) with ω ‰ 0. For a symmetric matrix A, it can be partitioned as
A “ L ` D ` LT, where D and L are the diagonal and lower-triangular matrices of A,
respectively. The Symmetric Successive Over Relaxation (SSOR) preconditioner is defined
as
P “ pD ` ωLq D´1 `
D ` ωLT ˘
, 0 ă ω ă 2
where ω is a relaxation parameter. An effective preconditioned system can be expressed as
Ar xr “ br
where Ar “ D´1{2A D´T{2, xr “ DT{2x, br “ D´1{2b, and
D1{2 “
»
—
—
—
–
?a11
?a22
... ?ann
fi
ffi
ffi
ffi
fl , D´1{2 “
»
—
—
—
–
1{
?a11
1{
?a22
...
1{
?ann
fi
ffi
ffi
ffi
fl
Since D is a diagonal matrix, its transpose is also equal to itself. The resulting precondi￾tioned system is also symmetric. Note that matrix Ar is constructed once at the beginning,
and the true solution is obtained as x “ D´T{2xr “ D´1{2xr.
The scope of this topic is restricted by the preceding discussions; however, there are a
number of more sophisticated ways of preconditioning in the literature. For more informa￾tion, the readers are referred to advanced texts on the topic for detailed discussions [29].
An effective preconditioner needs to be cheap to construct and implement (not too hard on
the cpu or memory requirements), and the preconditioned system should be easy to solve,
i.e., converge rapidly.
3.5 IMPROVING ACCURACY OF ILL-CONDITIONED SYSTEMS
In general, increasing computation precision significantly improves most ill-conditioned lin￾ear systems. In the event that this strategy fails, a corrective iterative scheme may be devised
for an ill-conditioned system of linear equations to achieve considerable improvements.
Consider an ill-conditioned linear system: Ax “ b. Suppose its first solution (x1
) is ob￾tained with a direct or iterative method. The numerical solution is approximate (not exact),
at least due to round-off errors, even if a direct method is applied. However, depending on
the severity of ill-conditioning, we can expect the final estimate to deviate significantly from
the true solution. To determine the effect on the solution of the system, we multiply A with
x1 to get
Ax1 “ b1 (3.52)
where b1 will naturally be different than b. Subtracting Eq. (3.52) from Ax “ b side-by-side
gives
Apx ´ x1
q “ b ´ b1 (3.53)190  Numerical Methods for Scientists and Engineers
The difference (or deviation) between the “estimated” and “correct” solutions is Δx “ x´x1
.
Also, defining Δb “ b ´ b1
, Eq. (3.53) takes the form
A Δx “ Δb (3.54)
If we can find the exact values of Δb and Δx of Eq. (3.54), the system accuracy will
surely improve in one step. Nevertheless, the round-off errors may inevitably prevent rapid
convergence. Thus, upon numerically solving Eq. (3.54) with a suitable method, a correction
to the approximate solution can be found from
x “ x1 ` Δx (3.55)
If the condition number is not too large, the improved solution can be obtained in a few
steps; otherwise, the procedure is repeated a few more times until a convergence criterion,
such as �Δx�8 ă ε, is satisfied.
Another technique to improve the solution of a linear system is preconditioning, which
was covered in Section 3.4.3. In which case, PAx “ Pb is solved instead of Ax “ b, provided
that the precondition matrix P is chosen such that the condition number of PA is smaller
than that of A; that is, κpPAq ă κpAq.
EXAMPLE 3.7: Improving solution of ill-Conditioned linear systems
Consider the following system of linear equations:
6x1 ` x2 “ 10
2x1 ` 0.333x2 “ 3.332
(a) Calculate the spectral condition number to determine the condition of the given
system; (b) Solve the system for b2 “ 3.33 and 3.3333 to determine the effect of
rounding on the solution; (c) apply the corrective iterative technique to improve the
solution accuracy for the b2 “ 3.33 case.
SOLUTION:
(a) Since the coefficient matrix is 2ˆ2, the true spectral condition number can be
calculated from κpAq “ �A�2 �A´1�2 without much difficulty. The inverse matrix
and pertinent spectral norms are found as
�A�F “�A�2 “6.41178, A´1 “
„
´166.5 500
1000 ´3000j
, �A´1�2 “3205.89
which leads to κpAq “ 20555 " 1. This magnitude of the condition number indicates
that the system is ill-conditioned; in other words, we can expect the solution to be
highly susceptible to rounding.
Next, we find |detpAq| “ 0.0020 and estimate the condition number from Eq.
(2.33):
κpAq « 2 �A�n
?
F
nn |det(Aq| “ 2 p6.41178q
2
?
22 p0.002q “ 20555
which also gives the true value of the condition number spot on.
(b) The solution of the original system (without any rounding) is found as
px1, x2q“p1, 4q. For b2 “ 3.33 and b2 “ 3.3333, the numerical solutions are ob￾tained as px1, x2q“p0, 10q and (1.65, 0.1), respectively. We would normally expectLinear Systems: Iterative Methods  191
a small change in the final solution as a result of a small change in one of the coef￾ficients of either A or b. However, this example reveals that the solution of a linear
system with a single perturbed coefficient may be substantially different from the
true solution. It is clear that the given linear system is very sensitive to fluctuations
in coefficient values, and the system is ill-conditioned.
(c) To correct the ill-effects of rounding in the case b2 “ 3.33 leading to x1 “
px1, x2q“p0, 10q, we find the new rhs vector:
b1 “ Ax1 “
„
6 1
2 0.333j „ 0
10j
“
„ 10
3.33j
The difference (or deviation) in the rhs is obtained as
Δb “ b ´ b1 “
„ 10
3.332j
´
„ 10
3.33j
“
„ 0
0.002j
Next, the deviations in the solution, Δx, are found by solving the following system
of equations:
„
6 1
2 0.333j „Δx1
Δx2
j
“
„ 0
0.002j
The exact solution of this system is obtained likewise by the Cramer’s rule with no
round-off errors: pΔx1, Δx2q“p1, ´6q. The improved solution is then obtained as
x “ x1 ` Δx “
„
0
10j
`
„ 1
´6
j
“
„
1
4
j
Discussion: With a single corrective iteration, we achieved a significantly improved
solution. However, it should be noted that no rounding errors were made when
solving A Δx “ Δb. In the case of rounding on one or more of the elements of
A and b (or in extremely ill-conditioned cases), this procedure may require several
corrective steps. This example illustrates the ill effects of small errors or round-offs
that easily arise from the way digital computers store and process real numbers. If
not careful, a digital error or a user-introduced round-off error has the potential to
lead to a completely different solution.
3.6 CLOSURE
Direct solution methods impose serious restrictions on memory requirements and cpu-time.
However, as an alternative, iterative methods only require storing non-zero matrix elements
and performing computations with only non-zero elements, which not only greatly resolves
the memory requirement issue to a large extent but also significantly reduces the cpu-time.
The basic stationary iterative methods are discussed, and the pseudocodes are presented
for each method. The performance of SOR depends on how close the relaxation parameter
is to the optimum value. In this regard, an algorithm to estimate the optimum relaxation
parameter for the SOR method is given. The SOR method with a smart estimate for the
ωopt parameter is the method of choice. The conditions for convergence and convergence
rates are discussed.192  Numerical Methods for Scientists and Engineers
The classic CGM and its convergence properties for positive symmetric systems are cov￾ered in-depth due to being a fundamental Krylov space method. The fact that acceleration
parameters are calculated as part of the CGM and CGM-like algorithms eliminates the need
to estimate the optimum relaxation parameter as in the SOR. The method, in the absence
of round-offs, results in the true solution of a system of n ˆ n linear equations in at most
n iterations. The CGM performs very well for well-conditioned linear systems (κpAq « 1).
Nonetheless, as a result of using finite precision arithmetic, the method may depict slow
convergence, depending on the accumulation of the round-offs that plague orthogonality
relations.
The advantages of the iterative methods can be summarized as follows:
‚ Iterative methods are suitable for solving large linear systems requiring extremely
large memory allocation and cpu-time;
‚ Iterative methods are very efficient when applied to diagonally dominant sparse ma￾trices, so they are relatively less sensitive to the growth of round-off errors;
‚ An iterative procedure can be terminated once a reasonably approximate solution is
reached;
‚ Some algorithms only prefer approximate solutions with a low degree of accuracy. For
instance, it is often sufficient to solve a linear system with inner iterations within a
nonlinear outer iteration with a low degree of accuracy;
‚ In some cases, a “rough” or “sufficiently close” estimate for the solution may be
available, which, when used, can dramatically accelerate the iteration process. For
example, in transient problems, almost all algorithms require the solution of a linear
system at every time step. In general, the solutions at the current time level (t ` dt)
and the prior time level (t) are very close to each other, so a solution from the prior
time level could be used as the initial guess for the current time level. This technique
of initializing an algorithm with a solution from a prior iteration rather than starting
from scratch is called a “warm start” and serves to obtain the current solution much
more quickly.
The disadvantages of the iterative methods are summarized as follows:
‚ A major handicap of iterative methods is that not all linear systems can be solved
with iterative methods;
‚ The error in the final solution depends on the method, the linear system, and the
number of iterations performed;
‚ The effectiveness and speed of an iterative method depend on the numerical algorithm
and its convergence rate;
‚ An iterative process, if it converges, gives an approximate solution, not the true solu￾tion. In general, some iterative algorithms may diverge, while others might converge
so slowly that they are practically useless.
A relatively small change in the elements of the coefficient matrix of an ill-conditioned
system results in unacceptably large deviations in the solution. Hence, the computations
need to be carried out using high-precision computing. Central to the error analysis of a
linear system, the condition number is a tool for measuring the relative magnitude of the
ill-conditioning, and it provides a quantitative measure for the number of significant decimal
digits that are likely to be lost when solving a system of equations. Additionally, a corrective
iterative technique is presented to improve the accuracy of the solution for ill-conditioned
systems.Linear Systems: Iterative Methods  193
3.7 EXERCISES
Section 3.1 Overview
E3.1 An iterative algorithm leads to the following prior and current estimates at the pth iteration
level: Compute the dppq
1, dppq
2, and dppq
8 norms of the displacement vector, which is
defined as dppq “ xpp`1q ´ xppq
.
paq xppq “t0.0015, 0.0072, 0.0044, 0.0092u, pbq xppq “t0.351, 0.217, 0.347, 0.697u,
xpp`1q “t0.0019, 0.0063, 0.0048, 0.0081u xpp`1q “t0.359, 0.233, 0.336, 0.722u
pcq xppq “ t35.12, 43.77, 62.73, 55.79u,
xpp`1q “ t34.62, 44.13, 63.35, 53.98u
E3.2 Use the given two iteration steps given in E3.1 to compute dppq
{xpp`1q
1, dppq
{xpp`1q
2,
and dppq
{xpp`1q
8 norms.
E3.3 Consider the following iteration sequences: How many iterations will be required to get an
estimate that satisfies the dppq
8 ă 0.01 criterion?
paq xppq “
# 1
40p ` 19e´p, 7p2 ´ 1
28p2 ` 99, ln p
155p
, 3e´p
+
, pbq xppq “
# 3
p2 ` 1
,
5 ´ p2
p2 ` 1
, 3 ´ p ` 1
p3
+
,
pcq xppq “
#
p sin p
p ` 9 , cos p
p2 ` 9
,
?p ` 1 ´ ?p,
1
a4p2 ` 1
+
E3.4 Repeat E3.3 if the convergence criterion is replaced by dppq
{xpp`1q
8 ă 0.01.
E3.5 Put the following system of equations into the fixed-point iteration form.
paq
$
&
%
2x ´ y ´ z “ 8
x ` 4y ´ 2z “ 5
3x ´ 2y ` 5z “ 2
,
.
- , pbq
$
&
%
4x ´ 3y ` 2z “ 1
´x ` 4y ´ 3z “ 5
2x ´ 2y ´ 7z “ 4
,
.
- , pcq
$
&
%
5x ´ 3y ´ 2z “ 9
2x ` 5y ´ 3z “ 12
3x ´ 3y ` 10z “ 30
,
.
-
Section 3.2 Stationary Iterative Methods
E3.6 For the given linear systems: (i) perform the first two Jacobi iterations by hand using xp0q “
0, (ii) use a spreadsheet program to find an approximate solution estimated with a maximum
absolute error less than ε “ 10´2.
paq
$
&
%
2x ` y ` z “ ´0.08
x ` 3y ` 2z “ 0.11
x ` y ` 3z “ ´0.23
,
.
- , pbq
$
&
%
5x ` y ´ 2z “ 1.1
x ` 3y ´ z “ 1.8
x ´ y ` 3z “ 3
,
.
-
E3.7 Find the approximate solution of the systems of equations in E3.6 using the Jacobi method.
How many iterations were required for convergence? Use xp0q “ 0 and dppq
2 ă 0.5 ˆ 10´5 as
the stopping criterion.
E3.8 Consider the given linear system of equations. (a) Find the true solution of the system using
a direct method; (b) Perform the first three Jacobi iterations by hand with xp0q “1 as the initial
guess. Does it look like it is converging? (c) Reorder the equations as {eq2, eq3, eq1} and use a
spreadsheet program to find a set of estimates with dppq
8 ă 10´2 as the convergence criterion.
x ´ 3y ´ 4z “ 4, 4x ` 2y ´ z “ 3, x ` 4y ´ 2z “ ´8.
E3.9 For the given linear system, (a) find the solution of the system using a direct method; (b)
carry out the first four iterations of the Jacobi method by hand using xp0q “0; (c) interchange
the first and third equations (r1 Ø r3) and repeat Part (b). Comment on the iteration progress.
2x ´ y ` 6z “ 1.6, x ` 6y ` z “ ´0.8, 5x ´ 2y ` 2z “ 1.95194  Numerical Methods for Scientists and Engineers
E3.10 For the given linear system, (a) carry out the first four iterations of the given linear system
with the Jacobi method using xp0q “ 0 and a spreadsheet program; (b) how many iterations are
required to obtain the solution with dppq
8 ă 5 ˆ 10´6 as the convergence criterion?
»
—
—
—
—
—
—
–
4 001 2 0
1 510 0 3
´2040 0 ´3
1 028 0 ´1
0 1 0 3 10 4
2 030 ´1 6
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
–
x1
x2
x3
x4
x5
x6
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
–
12.5
21
´4.5
21.5
53
26
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
E3.11 For the given linear system: (a) find an approximate solution of the system using the
Jacobi method with xp0q “ 0 and dppq
8 ă 5 ˆ 10´6 as the convergence criterion; (b) compute
dppq
8{dpp´1q
8 after the first iteration and comment on the convergence of this ratio; (c) how
many iterations were required to obtain the converged estimate?
»
—
—
—
—
—
—
–
62 1
´26 2 1
´26 2 1
´1 ´26 2
´1 ´262
´1 ´2 6
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
–
x1
x2
x3
x4
x5
x6
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
–
9
7
7
5
5
3
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
E3.12 Repeat E 3.6 using the Gauss-Seidel method.
E3.13 Repeat E 3.7 using the Gauss-Seidel method.
E3.14 Repeat E 3.8 using the Gauss-Seidel method.
E3.15 Repeat E 3.9 using the Gauss-Seidel method.
E3.16 Repeat E 3.10 using the Gauss-Seidel method.
E3.17 Repeat E 3.11 using the Gauss-Seidel method.
E3.18 For the given linear system: (a) find the true solution using a direct method; (b) carry out
(by hand) the first three iterations of the Jacobi method using xp0q “0; (c) carry out (by hand)
the first three iterations of the Gauss-Seidel method using the same initial guess and compare the
dppq
2 norms for both methods.
»
—
—
–
4 ´2 0 ´1
´2 4 ´2 0
0 ´2 4 ´2
´1 0 ´2 4
fi
ffi
ffi
fl
»
—
—
–
x1
x2
x3
x4
fi
ffi
ffi
fl “
»
—
—
–
´1.44
0.58
0.06
0.90
fi
ffi
ffi
fl
E3.19 For the systems of linear equations given in E 3.6 (a) write out the iteration equations
for the SOR method; (b) obtain approximate solutions with ω “ 1, 1.1, 1.2, 1.3, 1.4, and 1.5 to
determine the best choice for the acceleration parameter. Start with xp0q “0 and use dppq
8 ă
5 ˆ 10´4 as the convergence criterion.
E3.20 For the systems of linear equations given in Part (c) of E 3.8 (a) write out the iteration
equations for the SOR method; (b) obtain approximate solutions with ω “ 1, 1.1, 1.2, 1.3, and
1.4 to determine the best choice for the acceleration parameter. Start with xp0q “0 and use
dppq
8 ă 5 ˆ 10´4 as the convergence criterion.
E3.21 For the system of linear equations given E 3.9 (a) write out the iteration equations for the
SOR method; (b) obtain approximate solutions with ω “ 1, 1.1, 1.2, 1.3, and 1.4 to determine the
best choice for the acceleration parameter. Start with xp0q “0 and use dppq
8 ă 5 ˆ 10´4 as the
convergence criterion.
E3.22 For the systems of linear equations given in E 3.10 (a) write out the iteration equations
for the SOR method; (b) obtain approximate solutions with ω “ 1, 1.1, 1.2, and 1.3 to determine
the best choice for the acceleration parameter. Start with xp0q “0 and use dppq
8 ă 5 ˆ 10´4 as
the convergence criterion.Linear Systems: Iterative Methods  195
E3.23 For the systems of linear equations given in E 3.11 (a) write out the iteration equations
for the SOR method; (b) obtain approximate solutions with ω “ 0.7, 0.8, 0.9, 1, 1.1, 1.2, and
1.3 to determine the best choice for the acceleration parameter. Start with xp0q “0 and use
dppq
8 ă 5 ˆ 10´4 as the convergence criterion.
Section 3.3 Convergence of Stationary Iterative Methods
E3.24 Determine if the following linear systems meet the “diagonal dominance” condition for the
Jacobi and Gauss-Seidel methods.
paq
»
–
4 23
´344
2 45
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
9
5
11
fi
fl , pbq
»
–
3 21
´321
2 73
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
6
0
12
fi
fl ,
pcq
»
—
—
–
32 1 ´1
0 ´32 1
12 2 ´1
1 ´1 ´1 3
fi
ffi
ffi
fl
»
—
—
–
x1
x2
x3
x4
fi
ffi
ffi
fl “
»
—
—
–
5
0
4
2
fi
ffi
ffi
fl , pdq
»
—
—
–
1 1 ´3 1
13 1 ´2
2 ´14 2
2 ´23 5
fi
ffi
ffi
fl
»
—
—
–
x1
x2
x3
x4
fi
ffi
ffi
fl “
»
—
—
–
0
3
7
8
fi
ffi
ffi
fl
E3.25 Find the Jacobi iteration matrices of the linear systems in E 3.24 and verify whether or
not the Jacobi method will converge with an arbitrary initial guess.
E3.26 For the converging linear systems in E 3.25, find approximate solutions using the Jacobi
method. Start with xp0q “0 and use dppq
8 ă 5 ˆ 10´3 as the convergence criterion.
E3.27 Find the Gauss-Seidel iteration matrices for the linear systems in E 3.24 and verify whether
the Gauss-Seidel method will converge for arbitrary initial guesses.
E3.28 For the converging linear systems in E 3.27, find approximate solutions using the Gauss￾Seidel method. Start with xp0q “0 and use dppq
8 ă 5 ˆ 10´3.
E3.29 For given matrix A, find the spectral radius of A and A´1:
A “
»
—
—
–
0 1{20 0
1{201{2 0
0 1{201{2
0 01{2 0
fi
ffi
ffi
fl
E3.30 The following linear systems are not diagonally dominant. For given linear systems, find
the spectral radii of the Jacobi and Gauss-Seidel iteration matrices and determine whether or not
the methods will converge with an arbitrary initial guess.
paq
»
–
1 2 ´2
111
´22 1
fi
fl
»
–
x
y
z
fi
fl“
»
–
3
3
3
fi
fl, pbq
»
–
534
364
435
fi
fl
»
–
x
y
z
fi
fl“
»
–
12
13
12
fi
fl, pcq
»
–
2 24
1 33
´325
fi
fl
»
–
x
y
z
fi
fl“
»
–
2
´1
3
fi
fl
E3.31 For the given matrix, find the values of β for which the Jacobi method converges.
»
—
—
–
4 β β 0
β 4 β β
β β 4 β
0 β β 4
fi
ffi
ffi
fl
E3.32 For a given linear system, (a) find the solution of the system using the Gauss-Seidel and
SOR (for ω “ 1.3) methods with dppq
8 ă 5ˆ10´3 as the convergence criterion, and (b) estimate
the optimum relaxation parameter for the SOR method. Use xp0q “1 for the initial guess.
»
–
416
162
´2 2 ´4
fi
fl
»
–
x
y
z
fi
fl “
»
–
22
´16
´24
fi
fl196  Numerical Methods for Scientists and Engineers
E3.33 Consider the linear system in E 3.18; (a) estimate the theoretical value of ωopt; (b) having
rounded ωopt to two-decimal places, carry out the first three SOR iterations (by hand) starting
with xp0q “0.
E3.34 Find the solution of the system of linear equations resulting from E2.45 using the Jacobi
and Gauss-Seidel methods with dppq
8 ă 5 ˆ 10´4 as the convergence criterion, and estimate
the optimum relaxation parameter for the SOR method. Start with xp0q “0.
E3.35 (a) Find the solution of the system of linear equations resulting from E2.76 using the
Jacobi and Gauss-Seidel methods with dppq
8 ă 10´5 as the convergence criterion and estimate
the rate of convergence and the minimum number of iterations to reduce the error by a factor
of 10. (b) Estimate the optimum relaxation parameter and the rate of convergence for the SOR
method in the range 1 ď ω ď 1.5. Start with xp0q “0.
E3.36 (a) Find the solution of the given system of linear equations using the Jacobi and Gauss￾Seidel methods with dppq
8 ă 10´5 as the convergence criterion; (b) estimate the rate of con￾vergence and the minimum number of iterations to reduce the error by a factor of 10; and (c)
to estimate the optimum relaxation parameter, obtain the SOR solutions for ω values with 0.1
increments in the range 0.9 ď ω ď 1.5 and estimate the convergence rate. Start iterations with
xp0q “ 0.
»
—
—
—
—
–
6 ´1 ´30 0
´1 4 ´1 ´2 0
´3 ´16 2 0
0 ´2 ´2 5 ´1
000 ´1 1
fi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
–
x1
x2
x3
x4
x5
fi
ffi
ffi
ffi
ffi
fl “ 1
15
»
—
—
—
—
–
1
1
1
0
1
fi
ffi
ffi
ffi
ffi
fl
E3.37 The linear system given below is not diagonally dominant. (a) Investigate whether
the Jacobi and Gauss-Seidel methods will converge for this linear system; (b) for the con￾verging method(s), determine the number of iterations required to achieve convergence with
dppq
8 ă 10´4; (c) Verify whether the ρ2pMJ q “ ρpMGSq relation holds; (d) Is the SOR method
applicable for solving the system? If yes, estimate the optimum relaxation parameter and the rate
of convergence by varying ω value from 1 to 1.5 in increments of 0.1. Use xp0q “ 0 as the initial
guess.
»
—
—
—
—
–
20 8 0 3 5
8 16 2 5 6
2 3 18 7 8
4 5 6 20 2
6 5 4 2 19
fi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
–
x1
x2
x3
x4
x5
fi
ffi
ffi
ffi
ffi
fl “
»
—
—
—
—
–
36
37
38
37
36
fi
ffi
ffi
ffi
ffi
fl
E3.38 Ten objects of equal weight (W) are connected in series with ten springs (k) and suspended
as shown in Fig. E3.38. The steady-state equilibrium equations yield
2x1 ´ x2 “W{k,
´xi´1 ` 2xi ´ xi`1 “W{k, i “ 2, 3,..., 9
´x9 ` x10 “W{k
where xi denotes the displacement of the object i from its unstretched po￾sition. (a) For W{k “ 1 cm, apply the Jacobi and Gauss-Seidel methods to
solve the tridiagonal system of equations for the displacements, (b) com￾pute the convergence rate for both methods, (c) estimate the theoretical
optimum relaxation parameter, and (d) apply the SOR method to solve
the linear system for values starting at ω “ 1 in Δω “ 0.05 increments
to ω “ 1.95 and estimate the optimum relaxation parameter. Start with
xp0q “0 and use dppq
8 ă 10´5 for convergence tolerance. Fig. E3.38Linear Systems: Iterative Methods  197
Section 3.4 Krylov Space Methods
E3.39 Find approximate solutions to the given linear systems using the conjugate gradient
method. Use xp0q “0 as the initial guess and dppq
8 ă 5 ˆ 10´4 as the convergence criterion.
paq
4x1 ` x2 ` 2x3 “ 17
x1 ` 3x2 ` x3 “ ´2
2x1 ` x2 ` 4x3 “ 19
pbq
9x1 ` x2 ` x3 “ 21
x1 ` 8x2 ` 2x3 “ ´7
x1 ` 2x2 ` 12x3 “ 63
pcq
9x1 ` 4x2 ` 3x3 “ 32.75
4x1 ` 5x2 ` x3 “ 22.25
3x1 ` x2 ` 4x3 “ 20.5
E3.40 Find approximate solutions to the given linear systems using the conjugate gradient
method. Use xp0q “0 as the initial guess and dppq
8 ă 5 ˆ 10´6 as the convergence criterion.
paq pbq pcq
6x1 ` 2x2 ` x3 ` 3x4 “15.60
2x1 ` 5x2 ` 2x3 ` 4x4 “14.85
x1 ` 2x2 ` 8x3 ` 5x4 “19
3x1 ` 4x2 ` 5x3 ` x4 “13.15
4x1 ` 3x2 ` 4x3 ` 3x4 “10.95
3x1 ` 9x2 ` 5x3 ` 2x4 “13.90
4x1 ` 5x2 ` 11x3 ` 4x4 “22.25
3x1 ` 2x2 ` 4x3 ` x4 “8.75
10x1 ` 2x2 ` x3 ` 6x4 “15.15
2x1 ` 8x2 ` 4x3 ` 5x4 “27.35
x1 ` 4x2 ` 6x3 ` 4x4 “19.25
6x1 ` 5x2 ` 4x3 ` 3x4 “18.9
E3.41 For the given linear systems: (i) Obtain the preconditioned system of equations, Ar xr “
br, where Ar “ D´1{2A D´T {2, xr “ DT {2x, and br “ D´1{2b; (ii) Compute the infinity-based
condition numbers of A and Ar ; (iii) solve the preconditioned system using the CGM. Start with
xp0q “0 and use dppq
8 ă 5 ˆ 10´6.
paq pbq pcq
x1 ` 3x2 ` 2x3 “ 7.2
3x1 ` 4x2 ` 10x3 “ 17
2x1 ` 10x2 ` 196x3 “ 37.6
0.25x1 ` x2 ` 3x3 “ 2
x1 ` 25x2 ` 16x3 “ 46
3x1 ` 16x2 ` 81x3 “ ´13
4x1 ` x2 ` 6x3 “ 87
x1 ` 0.04x2 ´ 0.2x3 “ 8
6x1 ´ 0.2x3 ` 400x3 “ 2043
E3.42 Assume the linear systems in E3.41 are conditioned by employing the Jacobi preconditioner
(P “ D). (i) What would be the infinity-based condition numbers of the preconditioned systems?
(ii) Employ the preconditioned conjugate gradient method (PCGM) algorithm presented in Section
3.4 to estimate the solution of the system.
E3.43 Consider solving the following linear system using a preconditioner: (a) compute the
infinity-based condition number to determine whether the system is ill-conditioned or not; (b)
apply the Jacobi preconditioner (P “ D) to obtain an approximate solution; (c) apply the SSOR
preconditioner with ω “ 1 (P “ pD ` LqD´1pD ` LT q); (d) compute the infinity-based condition
numbers of the preconditioned matrices in Parts (b) and (c); (e) vary ω between 0.4 ď ω ď 1.9 to
determine ωopt. »
—
—
—
—
–
1 1{3 1{5 1{7
1{3 1{5 1{7 1{9
1{5 1{7 1{9 1{11
1{7 1{9 1{11 1{13
fi
ffi
ffi
ffi
ffi
fl
»
—
—
—
–
x1
x2
x3
x4
fi
ffi
ffi
ffi
fl “
»
—
—
—
—
–
2{5
´2{45
´302{3465
´278{3003
fi
ffi
ffi
ffi
ffi
fl
E3.44 Estimate the solution of the following linear systems using the CGNR (Conjugate Gradient
Normal Residual) method. Use dppq
8 ă 5 ˆ 10´6 as the convergence criterion and xp0q “ 0 as
the initial guess vector.
paq
2x1 ` x2 ´ 4x3 “ 6
x1 ` 3x2 ` x3 “ 7
3x1 ` x2 ` 2x3 “ ´9
pbq
3x1 ` 4x2 ` 2x3 “ 10
4x1 ` 3x2 ` 3x3 “ 7
5x1 ` 4x2 ` x3 “ 4
pcq
5x1 ´ 2x2 ` 4x3 “ 13
2x1 ´ 3x2 ´ 2x3 “ 24
4x1 ` 3x2 ´ 2x3 “ 22
E3.45 Estimate the solution of the following linear systems using the CGNR (Conjugate Gradient
Normal Residual) method. Use dppq
8 ă 5 ˆ 10´6 as the convergence criterion and xp0q “ 0 as
the initial guess vector.198  Numerical Methods for Scientists and Engineers
paq
x1 ` 3x2 ` 2x3 ` x4 “ 3
3x1 ` 2x2 ´ 2x3 ´ x4 “ ´1
2x1 ` x2 ´ x3 ` 4x4 “ 19
´x1 ` 3x2 ´ 3x3 ` x4 “ ´8
pbq
4x1 ` x2 ´ x3 ` 3x4 “ 12
2x1 ´ 4x2 ´ 3x3 ` x4 “ 5
x1 ´ 3x2 ` x3 ` 3x4 “ ´12
2x1 ` 3x2 ` 2x3 ´ x4 “ 11
E3.46 Repeat E3.44 with CGNE (Conjugate Gradient Normal Error) method.
E3.47 Repeat E3.45 with CGNE (Conjugate Gradient Normal Error) method.
Section 3.5: Improving Accuracy of Ill-Conditioned Systems
E3.48 Find the approximate solution of the given linear system using the Gauss elimination
method (a) without rounding the numbers; (b) with rounding the numbers and using three decimal
places in the arithmetic. Compare your estimates with those obtained in Part (a). Is the system
ill-conditioned? (Estimate the spectral condition number); (c) Apply the corrective algorithm to
find an improved solution for the system in Part (b). Use high-precision arithmetic operations.
x1
3 `
4x2
9 “ 7
9
, x1
7 `
3x2
17 “ 38
119
E3.49 For the given linear system below, (a) determine whether the coefficient matrix (A) is
ill-conditioned or not by estimating κpAq, (b) estimate the solution of the system using Gauss
elimination by rounding the numbers to three decimal places; (c) repeat Part (b) using four
decimal place arithmetic; (d) explain your findings in comparison to the true solution of x “1,
(e) apply the one-step corrective algorithm to improve the solution in Part (b). Use high-precision
arithmetic operations.
x1 ` x2
3 ` x3
5 “ 23
15 , x1
3 ` x2
5 ` x3
7 “ 71
105 , x1
5 ` x2
7 ` x3
9 “ 143
315
3.8 COMPUTER ASSIGNMENTS
CA3.1 The following matrices are not diagonally dominant. For linear systems with given coeffi￾cient matrices, (a) find the spectral radius of the Jacobi and Gauss-Seidel iteration matrices (i.e.,
ρpMJ q and ρpMGSq) and determine whether each method will converge with an arbitrary initial
guess; (b) calculate ρpMωq for various acceleration parameters between 0.1 and 1.9 to determine
for which values of ω the SOR or SUR scheme will converge. Is there an optimum value for ω?
paq
»
–
´4 4 ´7
´3 6 ´9
5 1 ´9
fi
fl , pbq
»
–
3 45
6 31
´112
fi
fl , pcq
»
–
2 4 ´1
1 3 ´2
32 4
fi
fl ,
pdq
»
—
—
–
41 3 1
11 1 ´1
22 5 ´4
1 1 ´4 ´3
fi
ffi
ffi
fl , peq
»
—
—
–
8 ´3 ´9 ´2
´45 2 ´2
´2 4 10 6
´5 ´4 9 ´9
fi
ffi
ffi
fl , pfq
»
—
—
–
3 21 3
´121 ´2
2 13 2
4 12 4
fi
ffi
ffi
fl
CA3.2 Consider the following linear system:
»
—
—
—
—
—
–
31 2
13 1
... ... ...
1 31
2 13
fi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
–
x1
x2
.
.
.
x19
x20
fi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
–
9{2
c2
.
.
.
c19
81{10
fi
ffi
ffi
ffi
ffi
ffi
fl
where ci “ p´1q
i ` pi{2q for i “ 2,..., 19. (a) Estimate theoretical ωopt, (b) find approximateLinear Systems: Iterative Methods  199
solutions of the system using the SOR method in increments of Δω “ 0.1 in the range of 0.8 ď
ω ď 1.7. For each ω value, record the number of iterations required for convergence. Is there an
ωopt value? If yes, does it agree with the theoretical value? (d) Calculate the convergence rates for
the numerical estimates obtained in Part (b). The true solution is given as xi “ p´1q
i
`pi{10q, i “
1, 2,..., 20. Use xp0q “ 0 as the initial guess and dppq
8 ă 5 ˆ 10´4 for the stopping criterion.
CA3.3 Consider the following set of nonlinear equations:
2
?x ` ay ` 1 ` ?z ` 4 “ 8, ?x ` z ` 2y “ 3, ?x ` y ` 2z “ 12
(a) Obtain the SOR iteration equations by isolating x, y, and z from the first, second, and third
equations and applying the relaxation parameter, just as in the case of linear systems. (b) Solve
the iteration equations for ω “ 0.8, 0.9, 1, and 1.1. How many iterations are required to satisfy the
convergence criterion dppq
8 ă 5 ˆ 10´4, (c) compare and interpret your numerical estimates.
Use xp0q “ 0 for the starting guess.
CA3.4 Develop an algorithm (a pseudomodule) specifically to estimate the solution of the fol￾lowing penta-diagonal linear system, using the SOR method.
j “ 1 j “ 2 ¨¨¨ j “ m ¨¨¨ j “ k ¨¨¨ j “ n
i “ 1
i “ 2
.
.
.
i “ m
.
.
.
i “ k
.
.
.
.
.
.
i “ n ´ 1
i “ n
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
e1 f1 h1
d2 e2 f2 h2
... ... ... ...
bm dm em fm hm
... ... ... ... ...
bk dk ek fk hk
... ... ... ...
... ... ... ...
bn´1 dn´1 en´1 fn´1
bn dn en
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
x1
x2
.
.
.
xm
.
.
.
xk
.
.
.
.
.
.
xn´1
xn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
q1
q2
.
.
.
qm
.
.
.
qk
.
.
.
.
.
.
qn´1
qn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
where k “ n ´ m ` 1 and the module should have the following argument list.
Module SOR5D(n, m, ω, ε, maxit, b, d, e,f, h, q, xo, x, iter, errorq
INPUT VARIABLES
n : Number of (unknowns) equations;
m : Position of the 4th (below) and 5th (above) diagonals;
ω : A relaxation parameter (1 ď ω ă 2);
ε : Convergence tolerance;
maxit : Upper bound for the number of iterations;
b : Array of below below diagonal elements, bi, i “ m, pm ` 1q,...,n;
d : Array of below diagonal elements, di, i “ 2, 3,...,n;
e : Array of diagonal elements, ei, i “ 1, 2,...,n;
f : Array of above diagonal elements, fi, i “ 1, 2,..., pn ´ 1q;
h : Array of above above diagonal elements, hi, i “ 1, 2,...,k;
q : Array of right-hand-side, qi, i “ 1, 2,...,n;
xo : Array of initial guess, xoi, i “ 1, 2,...,n;
OUTPUT VARIABLES
x : Array containing the solution, xi, i “ 1, 2,...,n;
iter : Number of iterations performed;
error : Maximum error (x ´ xo8 or x ´ xo2)200  Numerical Methods for Scientists and Engineers
CA3.5 The following linear system has a true solution, given by xi “ p1`i{4q{5 for i “ 1,..., 20.
Find the solution of the system by converting the module developed in CA 3.4 into a real program of
your choice. Run the program and record the number of iterations for the relaxation parameters
in the range 1 ď ω ă 1.9 in increments of Δω “ 0.05. Use xp0q “ 0 as the initial guess and
dppq
8 ă 10´6 for the stopping criterion.
j “ 1 j “ 2 ¨¨¨ j “ 7 ¨¨¨ j “ 14 ¨¨¨ j “ 20
i “ 1
i “ 2
.
.
.
i “ 7
.
.
.
i “ 14
.
.
.
.
.
.
i “ 19
i “ 20
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
´6 2 1
2 ´6 2 1
... ... ... ...
1 2 ´6 2 1
... ... ... ... ...
1 2 ´6 2 1
... ... ... ...
... ... ... ...
1 2 ´6 2
1 2 ´6
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
x1
x2
.
.
.
x7
.
.
.
x14
.
.
.
.
.
.
x19
x20
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
q1
q2
.
.
.
q7
.
.
.
q14
.
.
.
.
.
.
q19
q20
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
where q “ p1{20q r´7, 0, ´1, ´2, ´3, ´4, 0, 0, 0, 0, 0, 0, 0, 0, ´25, ´26, ´27, ´28, ´29, ´80s
T .
CA3.6 Consider the following iteration scheme for the 20ˆ20 penta-diagonal linear system given
in CA 3.4:
Txpp`1q “ q ´ Mxppq
where q “ rq1, q2, ¨¨¨ , qm, ¨¨¨ , qn´1, qns
T , x “ rx1, x2, ¨¨¨ , xm, ¨¨¨ , xn´1, xns
T , A “ T ` M and
T “
»
—
—
—
—
—
—
—
—
—
—
—
–
e1 f1
d2 e2 f2
... ... ...
dm em fm
... ...
dn´1 en´1 fn´1
dn en
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
, M “
»
—
—
—
—
—
—
—
—
—
—
—
—
–
h1
...
bm hm
... ...
bk hk
...
bn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
This scheme requires solving a tridiagonal system of equations at every iteration step. (a) Develop
a pseudocode and then convert it into a running computer program; (b) solve the system given
in CA 3.4 and compare the number of iterations with those obtained with the SOR method. Use
the initial guess xp0q “ 0 and dppq
8 ă 10´6 for the stopping criterion.
CA3.7 Consider the following linear system:
»
—
—
—
—
—
—
—
—
—
–
´41 1
1 ´41 1
1 1 ´41 1
... ... ... ... ...
1 1 ´41 1
1 1 ´4 1
1 1 ´4
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
—
–
x1
x2
x3
.
.
.
x9
x10
x11
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“ 1
10
»
—
—
—
—
—
—
—
—
—
–
´17
´18
b3
.
.
.
b9
´62
25
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
flLinear Systems: Iterative Methods  201
where bi “ 4 i p´1q
i`1 for i “ 3,..., 9, and the true solution is given by xi “ 1 ` p´1q
i
pi{10q
for i “ 1,..., 11. (a) Develop a pseudocode and convert it into a running computer program to
find an approximate solution of the system with the SOR method; (b) determine the optimum
relaxation parameter by varying ω in Δω “ 0.05 increments between 1 and 2 and making the plot
of ω versus the number of iterations. Use xp0q “ 0 as the initial guess and dppq
8 ă 10´6.
CA3.8 The Jacobi or Gauss-Seidel method can also be employed for some nonlinear systems of
equations. Consider the following set of nonlinear equations:
15x ` y2 ` z2 “ 11, x2 ` 28y ´ 3z “ 24, 3x ` y2 ` 20z “ 11
(a) To find a set of (possible) suitable iteration equations, isolate x, y, and z from the first,
second, and third equations, respectively; (b) apply the Jacobi and Gauss-Seidel methods to
obtain approximate solutions with xp0q “ 0 as the initial guess. The system has another solution
near px, y, zq “ p´21, ´16, ´8.75q. Can you find the other solution set by changing the initial
guess? Use dppq
8 ă 5 ˆ 10´5 for the stopping criterion.
CA3.9 Consider the following system of linear equations obtained for the loop currents of a
circuit:
»
—
—
—
—
–
20 0 ´12 0 0
0 14 0 ´4 0
´12 0 42 0 ´29
0 ´40 8 0
0 0 ´29 0 31
fi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
–
i1
i2
i3
i4
i5
fi
ffi
ffi
ffi
ffi
fl “
»
—
—
—
—
–
0
48
0
0
82
fi
ffi
ffi
ffi
ffi
fl
(a) Solve the system iteratively using the Jacobi and Gauss-Seidel methods, (b) calculate the
optimum relaxation parameter theoretically and also estimate it numerically by applying the SOR
method for ω values from 1 in increments of 0.1 up to 1.9, (c) estimate the condition number of the
coefficient matrix, and (e) solve the linear system using the Conjugate Gradient Method (CGM)
with rp0q
 ă 10´4. Use xp0q “ 0 as the initial guess and dppq
8 ă 10´6 for the convergence
tolerance of the stationary methods.
CA3.10 Consider E2.52, for which the system of linear equations for the loop currents are
»
—
—
—
—
–
5 ´1 0 ´2 ´2
´1 10 ´3 0 ´3
0 ´3 12 ´4 ´5
´2 0 ´4 12 ´4
´2 ´3 ´5 ´4 14
fi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
–
i1
i2
i3
i4
i5
fi
ffi
ffi
ffi
ffi
fl “
»
—
—
—
—
–
23
´18
´24
48
0
fi
ffi
ffi
ffi
ffi
fl
(a) Solve the system iteratively using the Jacobi and Gauss-Seidel methods, (b) calculate the
optimum relaxation parameter theoretically and also estimate it numerically by applying the SOR
method for ω values from 1 in increments of 0.1 up to 1.9, (c) estimate the condition number of the
coefficient matrix, and (e) solve the linear system using the Conjugate Gradient Method (CGM)
with rp0q
 ă 10´4. Use xp0q “ 0 as the initial guess and dppq
8 ă 10´6 for the convergence
tolerance of the stationary methods.
CA3.11 A system of linear equations, Ax “ b, can be converted to an equivalent system, Aˆ ˆx “ bˆ,
where Aˆ “ P´1A, P is a preconditioner, and Aˆ is a symmetric positive-definite matrix. To ensure
that Aˆ is positive definite and symmetric, we take P´1 “ LLT where L is a nonsingular lower
triangular matrix. Then P´1Ax “ P´1b can be modified as follows:
LT Ax “ LT b or LT ALL´1
x “ LT b or Aˆ ˆx “ bˆ
where Aˆ “ LT AL, ˆx “ L´1x and bˆ “ LT b. The CGM algorithm, of course, can be given in terms
of “hatted” quantities; however, preconditioning can be directly incorporated into the iterations.
The reader may find the details of this procedure in Ref. [29]. An algorithm to solve Ax “ b
linear system using the Preconditioned Conjugate Gradient Method (PCGM) with the Jacobi
preconditioner is given below. Write a computer program that incorporates the given algorithm.202  Numerical Methods for Scientists and Engineers
\ Algorithm PCGM: Preconditioned Conjugate Gradient method
rp0q Ð b ´ Axp0q \ Compute the initial residual
sp0q Ð P´1rp0q \ Solve Psp0q “ rp0q system
dp0q Ð sp0q \ Initialize search direction with s
ρp0q Ð prp0q
, sp0q
q \ Compute ρp0q “ prp0q
, sp0q
q
For “
p “ 1, maxit‰ \ Begin the iteration loop p
\ Compute current estimate and its residual at pth step
αppq Ð ρpp´1q
{pdpp´1q
, Adpp´1q
q Compute αppq
xppq Ð xpp´1q ` αppq
dpp´1q \ Compute current solution, xppq
rppq Ð rpp´1q ´ αppq
Adpp´1q \ Compute current residuals, rppq
sppq Ð P´1rppq \ Solve Psppq “ rppq system
If “
rppq
 ă ε
‰
Then \ Convergence achieved
Exit \ Exit the iteration loop
Else \ Linear system NOT converged yet!
\ Not converged, compute new direction at pth step
ρppq Ð prppq
, sppq
q
βppq Ð ρppq
{ρpp´1q
dppq Ð sppq ` βppq
dppq \ Compute current direction
End If
End For \ End the iteration loop p
CA3.12 Bi-Conjugate Gradient Method (BiCGM) is applied to any Ax “ b system. The method
uses two residual vectors (ˆr and r) and directions (dˆ and d). The scalar α is chosen to force the
bi-orthogonality condition, prppq
,ˆrppq
q“pˆrppq
, rppq
q “ 0, and β is chosen to force the biconjugacy
condition. There is also mutual orthogonality, i.e., pdppq
,ˆrppq
q“pˆrppq
, dppq
q “ 0. The reader may
find the details of this procedure at Ref. [29]. The algorithm for the Bi-Conjugate Gradient Method
(BiCGM) is given below. Write a computer program that incorporates the given algorithm.
\ Algorithm BiCGM: Bi-Conjugate Gradient method
rp0q Ð b ´ Axp0q \ Compute the initial residual
ˆrp0q Ð rp0q \ or any ˆrp0q
, prˆp0q
, rˆp0q
q ‰ 0,
dp0q Ð rp0q \ Initialize search direction with r
dˆp0q Ð ˆrp0q \ Initialize search direction with ˆr
ρp0q Ð prp0q
,ˆrp0q
q \ Compute ρp0q “ prp0q
,ˆrp0q
q
For “
p “ 1, maxit‰ \ Begin the iteration loop p
\ Compute current estimate and its residual at pth step
αppq Ð ρpp´1q
{pdˆpp´1q
, Adˆpp´1q
q Compute αppq
xppq Ð xpp´1q ` αppq
dpp´1q \ Compute current estimate, xppq
rppq Ð rpp´1q ´ αppq
Adpp´1q \ Compute current residual, rppq
ˆrppq Ð ˆrpp´1q ´ αppq
AT dˆpp´1q
If “
rppq
 ă ε
‰
Then \ Convergence achieved
Exit \ Exit the iteration loop
Else \ Linear system NOT converged yet!
\ Not converged, compute new direction at pth step
ρppq Ð prppq
,ˆrppq
q \ Compute current ρ
βppq Ð ρppq
{ρpp´1q
dppq Ð rppq ` βppq
dppq
dˆppq Ð ˆrppq ` βppq
dˆppq
End If
End For \ End the iteration loop pCHAPTER 4
Nonlinear Equations
LEARNING OBJECTIVES
After completing this chapter, you should be able to
‚ explain and apply the bisection method;
‚ describe and implement the false-position method;
‚ explain and implement the fixed-point iteration method;
‚ apply the Newton-Raphson and modified Newton-Raphson methods;
‚ apply the Secant and modified Secant methods,
‚ understand and assess the error estimation of the iterative procedures;
‚ employ Aitken and Steffensen’s acceleration techniques;
‚ list the pros and cons of the root-finding methods;
‚ describe and apply Newton’s method to the system of nonlinear equations;
‚ explain Bairstow’s algorithm and apply it to find the roots of a polynomial;
‚ understand and implement “Synthetic Division” and “Polynomial Reduction”
as means for finding the real roots of polynomials.
MANY problems in science and engineering involve the solution of nonlinear algebraic
equations, that is, finding the roots or zeros of a function. Functions whose roots or
zeros are sought may be polynomials or transcendental functions containing trigonometric,
logarithmic, exponential, or other functions or their combinations, and so on. Equations of
this kind are referred to as nonlinear algebraic equations.
Finding the zeros of a univariant function, y “ fpxq, is equivalent to determining its
x-intercepts, i.e., solving fpxq “ 0. The task of finding the intersections of two curves,
gpxq “ hpxq, also results in nonlinear algebraic equations. The latter can be expressed as a
problem of finding the zeros of the function fpxq “ gpxq´hpxq. Following are some examples
of nonlinear algebraic equations with a single unknown:
tan cx “ x or x2 sinhpxq “ 5
Collecting the expressions on one side of the equality sign gives
fpxq “ tan cx ´ x “ 0 or gpxq “ x2 sinhpxq ´ 5 “ 0
A function may have a single, several, or no zeros at all. For example, in the examples
above, fpxq has an infinite number of zeros, whereas gpxq has only one.
DOI: 10.1201/9781003474944-4 203204  Numerical Methods for Scientists and Engineers
FIGURE 4.1: Graphical depiction of a function with a root in (a, b) interval.
Methods for explicitly determining the true roots of nonlinear equations are not gen￾erally available, except in a few special cases. For this reason, analysts have to resort to
numerical methods that are usually iterative in nature.
In general, an iteration equation for a nonlinear equation is constructed as
xpp`1q “ Fpxppq
q, p “ 0, 1, 2,...
where Fpxq is referred to as the iteration function, p and xppq denote the iteration step and
the estimate (approximate value) for the root at the pth step, respectively.
Every iterative algorithm, including the ones for finding the roots of a nonlinear equa￾tion, requires an initial guess. Using the iteration function and the initial guess denoted by
xp0q, the first estimate is calculated as xp1q “ Fpxp0q
q. Next, using the latter estimate, a
second improved second is found as xp2q “ Fpxp1q
q. A sequence of successive estimates, xp3q,
xp4q
, and so on, can be generated in the same manner by repeating this recursive procedure.
After a sufficient number of iterations, we expect to obtain a “converged solution.” In other
words, if the iteration function is convergent, the subsequent estimates will approach the
true root α; that is, limpÑ8xppq “ α.
There may be several ways to define an iteration function. In such
cases, each iteration equation affects the convergence of the iter￾ation process and the convergence rate differently.
This chapter is mainly devoted to methods for finding the roots of nonlinear equations
with a single variable. A brief introduction to the numerical solution of a system of nonlin￾ear equations is also presented. Additionally, methods for finding the roots of polynomials
are also covered. The most common numerical methods, along with their advantages and
disadvantages, are presented.
4.1 BISECTION METHOD
The bisection method, also known as interval halving, is a numerical method for finding a
root of a nonlinear equation within a specified interval. The nonlinear equation is first cast
as fpxq “ 0. Then, a suitable search interval pa, bq containing a unique root is determined.
If fpxq is continuous on pa, bq and fpaqfpbq ă 0, then fpaq and fpbq have opposite signs (see
Fig. 4.1). So fpaqfpbq ă 0, in a way, provides a criterion for isolating or bracketing a root
in pa, bq. Henceforward, the search interval is halved successively until the interval within
which the estimated root lies becomes sufficiently narrow.Nonlinear Equations  205
FIGURE 4.2: Depicting the convergence of the root with the Bisection method.
The interval-halving procedure is applied as follows: The function is evaluated at the
midpoint of the interval fpxp1qq, where xp1q “ pa ` bq{2 is also an intermediate estimate for
the root. The midpoint divides the interval into two subintervals of equal width: pa, xp1qq
and pxp1q
, bq. If fpxp1qq “ 0, the procedure is terminated because xp1q is the root. Otherwise,
we need to determine and discard the interval not containing the root. To accomplish this,
the sign of fpxp1qq is checked in relation to fpaq or fpbq. For example, if fpaqfpxp1qq ă 0,
the root is in pa, xp1q
q, and the upper end of the search interval is updated to bp1q Ð xp1q
. If
fpaqfpxp1q
q ą 0, the root is bracketed as pxp1q, bq, and the lower end of the search interval
is updated to ap1q Ð xp1q (see Fig. 4.2). From this point onward, we will use appq and bppq
notations for the updated lower and upper ends of the search interval at the pth bisection
step. Setting pap0q, bp0q
q“pa, bq, the midpoint becomes xppq “ papp´1q ` bpp´1q
q{2. Each
time, the search interval is bracketed further down, and the endpoints are updated as
bppq Ð xppq and appq Ð app´1q if fpapp´1q
qfpxppq
q ă 0 or appq Ð xppq and bppq Ð bpp´1q if
fpapp´1q
qfpxppq
q ą 0. After a sufficient number of successive estimates, the search interval
becomes small enough (|bpq ´ appq
| ă ε) that the best estimate for the root becomes xroot “
pappq ` bppq
q{2.
Alternatively, at the end of the foregoing algorithm, the estimate for the root can be
further improved by employing a linear interpolation as follows:
xroot –
appq
fpbppq
q ´ bppq
fpappq
q
fpbppqq ´ fpappqq (4.1)
Successive approximations, in the absence of truncation errors, will eventually converge
to the true value after a sufficient number of iterations, i.e.,limpÑ8xppq “ α. However, it is
impossible to find the true root due to round-off errors that are unavoidable in digital com￾putations. Hence, this iterative procedure is terminated when, at least, one of the following
stopping criteria is met:
ˇ
ˇ
ˇ
xppq ´ xpp´1q
ˇ
ˇ
ˇ
ă ε1 or
ˇ
ˇ
ˇ
ˇ
xppq ´ xpp´1q
xppq ` δ
ˇ
ˇ
ˇ
ˇ
ă ε2 or
ˇ
ˇ
ˇfpxppq
q
ˇ
ˇ
ˇ
ă ε3
where ε1, ε2, and ε3 are the prescribed tolerances, xppq denotes the root estimate at the pth
bisection step, and δ is a real small number (δ ă ε2) added to the denominator to prevent
“division by zero” in case xppq becomes zero at some pth step.
The interval size can be determined in advance from bppq ´appq “ pb´aq{2p`1, which is
a useful relationship that may also be used to determine the accuracy to be achieved after
p bisections.
ˇ
ˇ
ˇ
xppq ´ α
ˇ
ˇ
ˇ « bppq ´ appq “ b ´ a
2p`1 ă ε (4.2)206  Numerical Methods for Scientists and Engineers
where ε is the tolerance and α is the best estimate for the root. This expression gives
us a tool for predicting the number of bisections required to obtain the root within ε. If
the interval estimate is quite good (sufficiently small), then the root is found within the
prescribed tolerance very quickly.
A solution is said to be correct within n decimal places if the
error is less than ε “ 0.5 ˆ 10´n. Substituting this value into Eq.
(4.2) and solving for p, we obtain an expression for the minimum
number of iterations as p ą log2r10npb ´ aqs.
Pseudocode 4.1
Module BISECTION (a, b, maxit, ε, root, halves)
\ DESCRIPTION: A pseudo module to estimate a real root of a nonlinear
\ equation in pa, bq using the Bisection method.
\ USES:
\ FUNC:: A user defined function providing the nonlinear equation, fpxq;
\ ABS:: A built-in function computing the absolute value.
p Ð 0 \ Initialize counter
interval Ð b ´ a \ Find initial interval size
f a Ð FUNC paq \ Evaluate fpxq at endpoints
f b Ð FUNC pbq
Repeat \ Bisection loop
p Ð p ` 1 \ Count bisections
xm Ð pa ` bq{2 \ Find current midpoint
fxm Ð FUNCpxmq \ Find fpxq at midpoint
Write: p, a, b, xm, f a, f b, fxm \ Print computation progress
\ Is the root on either left or right interval?
If “
f a ˚ fxm ą 0
‰
Then \ Root is in the right interval
a Ð xm; f a Ð fxm \ Update a and f a
Else \ Root is in the left interval
b Ð xm; f b Ð fxm \ Update b and f b
End If
interval Ð interval{2 \ Halve current interval
Until “
(|fxm| ă ε And interval ă ε) Or p “ maxit‰ \ End of loop
root Ð xm \ Current midpoint is the root estimate
halves Ð p \ Number of halves performed
\ If the iteration limit maxit is reached with no convergence
If “
p “ maxit‰
Then \ Issue info and a warning
Write: “Failed to converge after”,maxit,“iterations”
Write: “Current estimate is accurate within”, interval
End If
End Module BISECTION
Function Module FUNC (x)
\ DESCRIPTION: Non-linear equation suppled in fpxq “ 0 form.
FUNC Ð x ` 0.165 ´ 0.7 e´0.1x \ Set function to FUNC
End Function Module FUNCNonlinear Equations  207
Bisection Method
‚ The Bisection method is simple and reliable;
‚ It converges to a solution under any circumstances, as long as there
is a root within the starting search interval;
‚ The estimates for the root improve as the number of bisections in￾creases (the error bound decreases by half with each bisection).
‚ Determining the starting search interval pa, bq requires preliminary
work to ensure that fpaq and fpbq have opposite signs;
‚ A repeated (double, triple, etc.) root cannot be determined because
the search interval will yield fpaqfpbq ą 0;
‚ It converges linearly with an average convergence rate of 0.5, which
is considerably slow;
‚ It requires a large number of bisections to obtain an estimate with
a reasonable degree of accuracy if the starting search interval is too
large;
‚ Starting with an endpoint very close to the true root (a « root or
b « root) does not ensure fast convergence;
‚ Imaginary roots cannot be determined;
‚ It always converges to a root if fpaqfpbq ă 0; however, this criterion
implies the existence of a root rather than its uniqueness. If there is
more than one root in the search interval, it will converge to any one
of the roots.
A pseudomodule, BISECTION, implementing the bisection algorithm is presented in
Pseudocode 4.1. The code requires a starting search interval pa, bq, an upper bound on
the number of iterations (maxit), and a convergence tolerance (ε) as input. The nonlinear
equation whose root is sought is supplied as a function module, FUNC(x). The outputs of
the module are the estimate for the root (root) and the number of bisections performed
(halves). The bisection algorithm is implemented in a For-loop with index variable p, which
terminates when either p “ maxit or both |fpxppq
q| ă ε and interval ă ε stopping criteria
are met. During the bisections, the midpoint may happen to land exactly on the root or
at a point very close to the root, |fpxppq
q| ă ε; in this case, the loop must be terminated.
This likelihood, though, has not been addressed in the present pseudocode and is left as
an exercise for the reader. At every step, the midpoint of the current interval, xppq “
papp´1q`bpp´1q
q{2, is evaluated as the current root estimate. If fpapp´1q
qfpxppq
q ă 0, then the
root lies in the left subinterval: papp´1q
, xppq
q. The search interval is bracketed by updating
the right endpoint as bppq Ð xppq and fpbppq
q Ð fpxppq
q. Otherwise, fpapp´1q
qfpxppq
q ą 0,
the root lies in the right subinterval, pxppq
, bpp´1q
q, in which case the left endpoint values
are updated as app´1q Ð xppq and fpapp´1q
q Ð fpxppq
q. If the convergence criteria are not
met, the interval size is halved before moving on to the next bisection procedure. So the
final estimate is an approximate value within the tolerance prescribed.
Recall that checking for equality of two floating-point numbers as
a “ b or a ´ b “ 0 in programs is not a good practice! Hence, the
root, in Pseudocode 4.1, is determined by satisfying the |fpxppq
q| ă
ε condition instead of |fpxppq
q| “ 0 or fpxppq
q “ 0.208  Numerical Methods for Scientists and Engineers
EXAMPLE 4.1: Application of Bisection method
Signal delays are inherent in numerous engineering systems (control, electric trans￾mission, remote control, etc.), for which delay differential equations (DDEs) consti￾tute the basic mathematical models for real phenomena. A homogeneous first-order
DDE is represented by
dy
dt ` αypt ´ Tq ` βyptq “ 0, 0 ď t ď T
where α and β are constants, and T is the delay time.
Assuming a homogeneous solution in the form of yptq “ cert and substituting it
into the preceding DDE, the following nonlinear transcendental characteristic equa￾tion is obtained:
fprq “ r ` β ` αe´rT “ 0
This characteristic equation has a root in (0.2, 0.6) for α “ ´0.7, β “ 0.165, and
T “ 0.1. Apply the bisection method to estimate the root accurately to within four
decimal places.
SOLUTION:
The search interval is already given in the question as (0.2, 0.6). In practice,
one should have a rough idea of where the root may be so that the search interval
can be estimated as accurately as possible. The fact that fp0.2q“´0.3211391
and fp0.6q “ 0.1057648 have opposite signs confirms that there is a root in (0.2,
0.6). Since a four-decimal place accurate solution is desired, we set the convergence
tolerance to ε “ 0.5 ˆ 10´4.
TABLE 4.1
p appq bppq fpappq
q fpbppq
q rpp`1q fprpp`1q
q
0 0.2 0.6 ´0.3211391 0.1057648 0.4 ´0.1075526
1 0.4 0.6 ´0.1075526 0.1057648 0.5 ´0.0008606
2 0.5 0.6 ´0.0008606 0.1057648 0.55 0.0524604
3 0.5 0.55 ´0.0008606 0.0524604 0.525 0.0258020
4 0.5 0.525 ´0.0008606 0.0258020 0.5125 0.0124712
5 0.5 0.5125 ´0.0008606 0.0124712 0.50625 0.0058054
6 0.5 0.50625 ´0.0008606 0.0058054 0.503125 0.0024725
7 0.5 0.503125 ´0.0008606 0.0024725 0.5015625 0.0008059
8 0.5 0.5015625 ´0.0008606 0.0008059 0.5007813 ´0.0000273
9 0.5007813 0.5015625 ´0.0000273 0.0008059 0.5011719 0.0003893
10 0.5007813 0.5011719 ´0.0000273 0.0003893 0.5009766 0.0001810
11 0.5007813 0.5009766 ´0.0000273 0.0001810 0.5008789 0.0000768
12 0.5007813 0.5008789 ´0.0000273 0.0000768 0.5008301 0.0000248
We begin by computing the midpoint rp1q “ p0.2 ` 0.6q{2 “ 0.4 and splitting
the initial search interval into two subintervals: (0.2, 0.4) and (0.4, 0.6). The func￾tion at the midpoint yields fprp1qq“´0.1075526. Since fp0.2qfprp1qq ą 0, the root
lies in the right subinterval; thus, we can discard the left subinterval. This proce￾dure is repeated for the new search interval, giving rp2q “ p0.4 ` 0.6q{2 “ 0.5 and
fprp2q
q“´0.0008606. From fp0.5qfprp2qq ą 0, the root is determined to be in theNonlinear Equations  209
FIGURE 4.3: Depicting the root estimation with the method of false position.
right subinterval: (0.5, 0.6). Repeating the bisection procedure in this manner nar￾rows down the search interval containing the root. Eventually, the search interval,
as well as fprpnq
q, will approach zero with each bisection.
The results of the computations are presented in Table 4.1. It is worth noting
that the values in the fprppq
q column approach zero with each iteration, and the
stopping criteria are met at the 12th step, i.e., |rp12q ´ rp11q
| “ 4.88 ˆ 10´5 ă ε and
|fprp12q
q| “ 2.48 ˆ 10´5 ă ε.
Discussion: The minimum number of bisections required to compute the root within
four decimal places can be estimated by Eq. (4.2), which gives n ą log2p0.4ˆ104q “
11.966 « 12. The computational effort can be reduced if the starting interval pb ´ aq
is selected as narrowly as possible.
4.2 METHOD OF FALSE POSITION
Method of False Position, or Regula Falsi Method, is also based on bracketing a root in a
tight interval. It is similar to the bisection method; however, it differs only in computing
the new estimate. The root is estimated by the x-intercept of a straight line passing through
the endpoints of the current interval, i.e., pappq
, fpappq
qq and pbppq
, fpbppq
qq.
Assuming that fpxq is a continuous function with a root in pa, bq, the equation of a
straight line (�1) passing through the points pa, fpaqq and pb, fpbqq is
y “ bfpaq ´ afpbq
b ´ a
` fpbq ´ fpaq
b ´ a
x (4.3)
An approximation to the root can then be obtained from the x-intercept of �1, i.e., xp1q as
illustrated in Fig. 4.3.
Bracketing the root is done in the same way as described for the bisection method. A
second line (�2) for the new interval leads to a second x-intercept (xp2q
), which is closer to
the root, and so on. This procedure is repeated until the interval size is sufficiently small,
i.e., ˇ
ˇ bppq ´ appq
ˇ
ˇ ă ε or ˇ
ˇ fpxppq
q
ˇ
ˇ ă ε.
An expression for estimating the root (i.e., the intercept) in the pappq
, bppq
q interval can
be obtained by setting y “ 0 in Eq. (4.3) and solving for xpp`1q
:
xpp`1q “ appq
fpbppq
q ´ bppq
fpappq
q
fpbppqq ´ fpappqq (4.4)210  Numerical Methods for Scientists and Engineers
Figure 4.3 depicts how successive estimates approach the root. Note that in the course
of bracketing, if fpxq is concave up, the right end of the search interval is anchored while
the left end moves toward the root. If fpxq is concave down, this time the left end remains
anchored while the right end moves toward the root.
The algorithm for the method is basically the same as the bisection method. The only
difference is that the new estimate is replaced by Eq. (4.4), whereby the algorithm and the
pseudocode presented for the bisection method can be used with a minor modification. For
this reason, no pseudocode for this method has been given.
The advantages and disadvantages of the method of false position are pretty much the
same as those of the bisection method. Nevertheless, since the method is based on finding
the point that intersects the x-axis by connecting the end points of the interval containing
the root with a straight line, it can converge faster than the bisection method. However, if
the function is not suitable to be approximated with a line, the method may fail or stagnate.
EXAMPLE 4.2: Method of False Position
Repeat Example 4.1 using the method of False Position.
SOLUTION:
The first estimate for the root is calculated from Eq. (4.4)
rp1q “ p0.2qfp0.6q´p0.6qfp0.2q
fp0.6q ´ fp0.2q “ 0.213836
0.426904 “ 0.500901
which leads to fprp1qq “ 9.826 ˆ 10´5 ą ε.
We observe that the root is on the left subinterval, (0.2, 0.500901), since
fpaqfprp1q
q ă 0. The second estimate is similarly obtained as
rp2q “ p0.2qfp0.500901q´p0.500901qfp0.2q
fp0.500901q ´ fp0.2q “ 0.160878
0.321237 “ 0.500807
with fprp2qq “ 1.356 ˆ 10´7 ă ε.
The quantitative results of the first three iterations are summarized in Table
4.2. The method converged much faster (with only 2 iterations) than the bisection
method (12 iterations in Example 4.1.) Due to introducing linear interpolation into
this scheme, it should, and usually does, give better estimates of the root, especially
when the approximation of the nonlinear function by a linear function is valid.
TABLE 4.2
p appq bppq fpappq
q fpbppq
q rpp`1q fprpp`1q
q
0 0.2 0.6 ´0.3211391 0.1057648 0.500901 9.83 ˆ 10´5
1 0.2 0.500901 ´0.3211391 0.0000999 0.500807 1.36 ˆ 10´7
2 0.2 0.500807 ´0.3211391 9.47 ˆ 10´8 0.500807 8.97 ˆ 10´11
Discussion: In this example, not only was the root estimate correct to within six
decimal places due to |fprp2qq| ă 0.5ˆ10´6, but it was also achieved in two iterations.
Also note that the root, in this case, was bracketed from the left side, whereas
bracketing in the bisection method alternates between both sides as it closes in on
the root.Nonlinear Equations  211
FIGURE 4.4: Depiction of the fixed-point iteration sequence of possible cases: (a) monotone
convergence (0 ď g1
pxppq
q ă 1); (b) oscillating convergence (´1 ă g1
pxppq
q ď 0); (c)
monotonic divergence (g1
pxppq
q ą 1); (d) oscillating divergence (g1
pxppq
qă´1).
4.3 FIXED-POINT ITERATION
A nonlinear equation can be rearranged into an equivalent x “ gpxq form, which may be
accomplished in a variety of ways. Here, x is referred to as the “fixed-point” for the iteration
function gpxq. Then the iteration equation can be expressed as
xpp`1q “ gpxppq
q, p ě 0 (4.5)
The fixed-point iteration, as a technique, is equivalent to finding the geometric inter￾section of the line y “ x and the curve y “ gpxq. The numerical procedure is illustrated in
Fig. 4.4a. This iterative procedure begins with an initial guess, xp0q
. Then, the point on the
curve’s vertical projection is found: yp0q “ gpxp0q
q. This value is then projected horizontally
on the y “ x line to find an improved estimate, xp1q “ yp0q. Note that the arrows in the
figure indicate the direction of the projections. The procedure, which involves a vertical
projection onto the curve followed by a horizontal projection onto y “ x, is repeated until
the points on the curve approach a fixed-point.
Several alternative forms of the iteration function gpxq, generally non-unique, can be
obtained for a given nonlinear equation. For example, x2 ` x “ e´x can be rewritten as
x “ e´x ´ x2 or x “ ?
e´x ´ x or x “ ´ lnpx2 ` xq or x “ e´xpx ` 1q
Some of these may converge rapidly or slowly, or some may not converge at all. In general,
the convergence behavior depends on the iteration function and the initial guess.
To assess the convergence behavior of an iteration equation, we assume that gpxq and
g1
pxq are continuous functions on pa, bq in which a root lies. The sufficient (but not nec￾essary) condition is |g1
pxppq
q| ď λ ă 1 for all x on pa, bq, where λ is an upper bound
provided that the initial estimate is close enough to the true root. After a sufficient number
of iterations, λ gives the convergence rate.212  Numerical Methods for Scientists and Engineers
Pseudocode 4.2
Module FIXED_POINT (maxit, ε, root, iter)
\ DESCRIPTION: A pseudomodule to find a real root of nonlinear equation
\ using the Fixed-Point Iteration method.
\ USES:
\ FUNC:: A user-defined function supplying nonlinear equation fpxq;
\ ABS:: A built-in function computing the absolute value.
x0 Ð root \ Initialize initial root
e0 Ð 1 \ Initialize prior and current errors
p Ð 0 \ Initialize iteration counter
Repeat \ Iterate until convergence conditions are met
p Ð p ` 1 \ Count iterations
x1 Ð FUNCpx0q \ Find current estimate xppq
e1 Ð |x1 ´ x0| \ Find current error, |xppq ´ xpp´1q
|
λ Ð e1{e0 \ Estimate convergence rate
Write: p, x0, x1, e1, λ \ Print iteration progress
x0 Ð x1 \ Set current estimate as prior
e0 Ð e1 \ Set current error as prior
e2 Ð |FUNCpx1q ´ x1| \ Find error-2, |gpxppq
q ´ xppq
|
Until “
(e1 ă ε And e2 ă ε) Or p “ maxit ‰ \ Test for convergence
root Ð x1 \ Root is found if p ă maxit
iter Ð p \ Number of iterations performed
\ If the iteration limit maxit is reached with no convergence
If “
p “ maxit‰
Then \ Issue info and a warning
Write: “Maximum number of iterations reached, root=”, root
Write: “Estimated root has NOT converged! errors=”, e1, e2
End If
End Module FIXED_POINT
The iteration sequence for four possible cases is graphically depicted in Fig. 4.4. Figure
4.4a and b show that convergence is guaranteed for |g1
pxppq
q| ă 1 as the iteration sequence
approaches the fixed-point of y “ x and y “ gpxq. In other words, if |g1
pxppq
q| ă 1, the
xpp`1q “ gpxppq
q is said to be locally convergent, meaning that there is an interval containing
xp0q such that the method converges for any starting value xp0q within the interval. On the
other hand, in Fig. 4.4c and d for |g1
pxppq
q| ą 1, we observe that the iteration sequence leads
away from the fixed (intersection) point. We can then say that the method diverges for any
starting xp0q value other than the true root itself. Defining the error as eppq “ xppq ´ xpp´1q
,
it can be shown that the error is linearly proportional to the previous error by eppq
{epp´1q Ñ
|g1
pxppq
q| ă λ as p Ñ 8. Hence, the method of the fixed-point iterations is said to be linearly
convergent. Even when gpxq is convergent (λ ă 1), its rate of convergence may turn out to
be very slow in some cases. On the other hand, g1
pxppq
q “ 1 could yield the root estimate
of either a slow convergence rate or divergences.
A pseudomodule, FIXED_POINT, implementing the fixed-point method is given in Pseu￾docode 4.2. The code requires an initial estimate (root), an upper bound for the number of
iterations (maxit), and a convergence tolerance (ε) as input. The iteration function gpxq is
provided separately as an external function module, FUNC(x). On exit, the variables root
and iter are set to the estimated root and the total number of iterations performed, respec￾tively. The current root estimate is computed from x1 “ xppq “ gpxpp´1q
q in a Repeat-UntilNonlinear Equations  213
Fixed-Point Iteration Method
‚ The method is simple and easy to apply;
‚ Its cost per iteration is low;
‚ If it does converge, it may converge very slowly;
‚ Its convergence rate can be estimated simultaneously with the root.
‚ The method requires an initial guess that is sufficiently close to the
root; even then, the convergence may not always be guaranteed;
‚ The convergence property depends on the choice of iteration function
gpxq, i.e., finding a suitable iteration function can be a major problem.
loop as long as the number of iterations does not exceed maxit (p “ maxit) or stopping cri￾teria, eppq “ e1 “ |xppq ´ xpp´1q
| ă ε and e2 “ |gpxppq
q ´ xppq
| ă ε, are not met concurrently.
Absolute errors from prior (e0 “ |xppq ´ xpp´1q
|) and current iterations (e1 “ |xpp`1q ´ xppq
|)
are tracked and used to estimate the rate of convergence, λppq “ e1{e0, at every iteration.
Note that the error from the prior iteration level is initialized to e0 “ 1 before the loop,
and the current estimate along with the current error (x1 and e1) is assigned as the prior
estimate and error (x0 Ð x1 and e0 Ð e1) once they have been used. Additionally, the
agreement of the fixed-point form (e2 “ |gpxpp`1q
q ´ xpp`1q
|) is verified in the convergence
decision. When the number of iterations reaches maxit, it means that the estimated root
has yet to converge. In this case, a warning message is printed out along with the current
estimate and current error.
4.4 NEWTON-RAPHSON METHOD
The Newton-Raphson method is one of the most widely used methods for finding the roots of
a nonlinear equation. To illustrate how the method works, we consider a nonlinear equation
set up as fpxq “ 0. The Taylor series of a real function fpxq about a point x0 is given by
fpxq “ fpx0q`px ´ x0qf1
px0q ` px ´ x0q
2
2! f 2px0q ` ... (4.6)
Equation (4.6) will yield zero for the true root of fpxq. Thus, setting fpxq “ 0, the rhs
of Eq. (4.6) becomes a polynomial of infinitieth degree, one of whose roots is also the root of
fpxq. Finding the root of this equation is very difficult. However, to find an approximation
for this root, for convenience, we will replace this equation with a finite polynomial. The
simplest approximation can be found by truncating the Taylor series into a two-term sum
by neglecting the higher-order terms:
0 – fpx0q`px ´ x0qf1
px0q ` ... (4.7)
Solving Eq. (4.7) for x yields
x “ x0 ´ fpx0q
f1
px0q (4.8)
which provides an expression for estimating the root. Computing a new estimate x1 from
Eq. (4.8) will not satisfy fpx1q “ 0 unless it happens to be the root. This estimate is used
on the rhs of Eq. (4.8) to obtain a new and hopefully improved estimate. If the new estimate
is not sufficiently close to the root, this procedure is repeated until successive estimates are
close enough to the root with reasonable accuracy.214  Numerical Methods for Scientists and Engineers
EXAMPLE 4.3: Application of Fixed-Point Iterations
Repeat Example 4.1 using the fixed-point iteration with rp0q “ 1 as the initial guess.
SOLUTION:
We define the iteration function as gprq “ r “ 0.7e´0.1r ´ 0.165 since it is easy
to solve for r. Starting with the given initial guess, rp0q “ 1, and setting p “ 0, 1
and 2 in rpp`1q “ gprppq
q, the first three iterations are obtained as follows:
rp1q “ gprp0q
q “ 0.7e´0.1p1q ´ 0.165 “ 0.4683862,
rp2q “ gprp1q
q “ 0.7e´0.1p0.4683862q ´ 0.165 “ 0.5029690
rp3q “ gprp2q
q “ 0.7e´0.1p0.5029690q ´ 0.165 “ 0.5006629
The iterative estimation is repeated until the stopping criterion, |eppq
| ă 0.5 ˆ
10´4, is met. Also, by computing rpp`1q “ gprppq
q, g1
prppq
q, and eppq “ rpp`1q´rppq in
the same manner, the resulting estimated values are presented in Table 4.3. Notice
that the absolute error is obtained as |ep4q
| “ 1.022 ˆ 10´5 ă 0.5 ˆ 10´4 after 4
iterations. Also note that for p ą 2, |g1
prppq
q| approaches ´0.0666, which satisfies
the sufficient condition for convergence.
Similarly, the ratio of the absolute errors of two successive iterations yields
ˇ
ˇep4q
ˇ
ˇ
ˇ
ˇep3q
ˇ
ˇ “
ˇ
ˇ
ˇg1
prppq
q
ˇ
ˇ
ˇ “ 0.06658 “ λ ă 1
This ratio, which is directly proportional to ˇ
ˇg1
prppq
q
ˇ
ˇ, depicts a linear convergence
rate.
TABLE 4.3
p rppq gprppq
q ˇ
ˇeppq
ˇ
ˇ rpp`1q g1
prppq
q
0 1 0.4683862 0.5316138 0.4683862 ´0.063339
1 0.4683862 0.5029690 0.0345828 0.5029690 ´0.066797
2 0.5029690 0.5006629 0.0023060 0.5006629 ´0.066566
3 0.5006629 0.5008165 0.0001535 0.5008165 ´0.066582
4 0.5008165 0.5008062 1.02ˆ10´5 0.5008062 ´0.066581
Discussion: In this example, there are two other candidates for the iteration func￾tion: (1) gprq “ r “ 0.7e´0.1r ´ 0.165 and (2) gprq“´10 lnppr ` 0.165q{0.7q.
Investigating the convergence condition for the first iteration function, we obtain
g1
prq“´0.07e´0.1r. It is worth noting that g1
prq is defined for all positive r and is
valid within the range 0 ă |g1
prq| ă 1. This means that the first function satisfies
the convergence criterion for all positive r. On the other hand, the second iteration
function with g1
prq“´10{pr ` 0.165q yields |g1
prq| ą 1 for any initial guess value
falling in the range 0 ă r ă 9.835. It is clear that the second function will diverge
with the initial value of rp0q “ 1.
It may be difficult to find the convergence rate or convergence interval when more
than one fixed-point iteration equation exists. However, evaluating the convergence
rate during the iteration process does not impose an extra computational cost, as
done in Pseudocode 4.2.Nonlinear Equations  215
FIGURE 4.5: Graphical depiction of the Newton-Raphson method.
Introducing the superscript (iteration) notation, the Newton-Raphson iteration equa￾tion can be expressed as
xpp`1q “ xppq ´ fpxppq
q
f1
pxppqq
, p ě 0 (4.9)
where xppq is the estimate for the root at the pth iteration step.
Geometrical Interpretation: A root of y “ fpxq is also its x´intercept, as illustrated
in Fig. 4.5. The slope of the tangent line to fpxq at point P0 is f1
pxp0qq. The x´intercept
of the tangent line, xp1q, can be calculated from the slope, i.e., m1 “ tan θ1 “ f1
pxp0qq “
fpxp0qq{pxp0q ´ xp1qq. Similarly, a tangent line drawn to fpxq at point P1 intersects the
x´axis at xp2q. Likewise, the intercept of the tangent is determined from the slope of fpxq
at xp1q
; m2 “ tan θ2 “ f1
pxp1qq “ fpxp1q
q{pxp1q´xp2q
q. Repeating this procedure successively
yields a sequence of estimates (xp3q, xp4q
,. . . ) that approach the true root as p Ñ 8. The
generalized expression for the intercept at the pth iteration step can be written as
xpp`1q “ xppq ´ fpxppq
q
f1
pxppqq
, p ě 0 (4.10)
It becomes clear that both Eq. (4.9) and Eq. (4.10) are identical. In practice, the
Newton-Raphson method is based on finding the tangent lines, followed by calculating the
intercepts that should eventually approach the true root.
Convergence issues: The method needs an initial guess, xp0q, to begin the iteration
process. However, it requires two function evaluations per iteration step, namely fpxppq
q and
f1
pxppq
q, in comparison to bisection or fixed-point iteration methods. Noting that fpxpp`1q
q
will tend to zero, Eq. (4.6) for any pth iteration step can be written as
fpxppq
q`pα ´ xppq
qf1
pxppq
q ` pα ´ xppq
q
2
2! f 2pξq “ 0 (4.11)
where α is the true root, x0 is replaced by xppq
, and the third term is the Lagrange form of
the remainder, where α ă ξ ă xppq
. Making use of Eq. (4.10) in the first term of Eq. (4.11)
and simplifying, we obtain
xpp`1q ´ α “ f 2pξq
2f1
pxppqq
pxppq ´ αq
2
or
ˇ
ˇ
ˇ
epp`1q
ˇ
ˇ
ˇ ď K
ˇ
ˇ
ˇ
eppq
ˇ
ˇ
ˇ
2
(4.12)
where eppq “ xppq´α is the true error at the pth iteration step, and K is the asymptotic error
constant defined as K “ |f 2pαq|{2|f1
pαq|. This result implies that if fpxq is continuously216  Numerical Methods for Scientists and Engineers
Newton-Raphson Method
‚ The method is simple, reliable, and easy to implement;
‚ Its convergence is quadratic when the initial guess is in close proxim￾ity to the root;
‚ Most divergence problems can be overcome by simply changing the
initial guess.
‚ The method requires an analytical evaluation of the first derivative;
‚ It may not be suitable for functions whose derivatives are difficult to
obtain and/or require too many arithmetic operations;
‚ Convergence cannot be guaranteed for any given function or any ini￾tial guess;
‚ A Division By Zero problem may arise in case the of f1
pxppq
q Ñ 0;
‚ If a nonlinear equation has a root of multiplicity m (m ą 1), it may
diverge or converge very slowly with a linear convergence rate;
‚ Some functions, like periodic functions, may exhibit unexpected be￾havior for a range of estimates.
differentiable with f1
pαq ‰ 0 and f 2pαq exists, then convergence of the Newton-Raphson
method is quadratic or faster if f 2pαq “ 0. For p " 1, we can roughly estimate K from
δpp`1q – K pδppq
q
2
, where δpnq “ xpn`1q ´ xpnq
.
There are cases when Newton-Raphson fails to converge. The convergence property
of the method depends on the nature of the given function and the choice of the initial
guess. Most of the divergence problems can be attributed to poor choice of an initial guess,
which could simply be resolved by changing the initial guess. For instance, an initial guess
or a new estimate may correspond to a local extremum point where the derivative is very
small or zero, then the δppq term will tend to be very large, which will result in throwing the
current estimate away from the true root. Or, if a root is near an inflection point, the current
estimate also tends to diverge from the true root. Also, if a function is not continuously
differentiable in the vicinity of a root or has a vertical tangent line, the method will fail or
diverge.
EXAMPLE 4.4: Application of Newton-Raphson method
Repeat Example 4.1 using the Newton-Raphson method with rp0q “ 1 as an initial
guess.
SOLUTION:
In this method, we cast the nonlinear equation as fprq “ 0. The method, besides
fprq, requires the first derivative, which is given below:
fprq “ r ` 0.165 ´ 0.7e´0.1r and f1
prq “ 1 ` 0.07e´0.1r
For p “ 0, using the iteration equation, Eq. (4.10), the first estimate leads to
rp1q “ rp0q ´ fprp0qq
f1
prp0qq “ 1 ´ 0.5316138
1.063339 “ 0.5000522Nonlinear Equations  217
Likewise, for p “ 1, we obtain
rp2q “ rp1q ´ fprp1qq
f1
prp1qq “ 0.5000522 ´ ´0.000805
1.066586 “ 0.50080687
A sequence of estimates (rp2q, rp3q, and so on) is found by carrying out succes￾sive iterations in the same way. The results are summarized in Table 4.4. It is ob￾served that with each subsequent iteration, the root estimates approach the value of
r “ 0.5008069. As we have seen, the Newton-Raphson method not only estimated
the root with fewer iterations but also yielded an eight-decimal place-accurate an￾swer. Also, note that the last column converges to 0.00312, which corresponds to
the asymptotic error constant whose true value is found as K “ |f 2prq|{2|f1
prq| “
´0.003121.
TABLE 4.4
p rppq fprppq
q f1
prppq
q δppq ˇ
ˇδppq
ˇ
ˇ {
ˇ
ˇδpp´1q
ˇ
ˇ
2
0 1 0.53161381 1.0633390 ´0.4999478
1 0.50005220 ´0.00080493 1.0665857 0.0007547 0.003019
2 0.50080687 ´1.9 ˆ 10´9 1.0665807 1.78ˆ10´9 0.003121
3 0.50080687 Op10´16q 1.0665810 Op10´16q
Discussion: Note that, after two iterations, we achieve |δp2q| “ 0.178 ˆ 10´8 and
|fprp2qq| “ 0.19ˆ10´8 ă 0.5ˆ10´8, i.e., the root estimate is accurate to at least eight
digits. In the third iteration, the order of magnitude of the error becomes Op10´16q.
This is not surprising because the convergence rate of the method is quadratic, i.e.,
much more rapid than previously covered methods. Actually, the error after the
first iteration is about three-thousandths of (corresponding to the asymptotic error
constant) the square of the previous error.
A pseudomodule, NEWTON_RAPHSON, implementing the Newton-Raphson algorithm
is presented in Pseudocode 4.3. The module requires an initial estimate (root), an upper
bound for iterations (maxit), and a convergence tolerance (ε) as input. Two user-defined
external functions, FUNC(x) and FUNCP(x), are used to supply fpxq and f1
pxq, respectively.
The estimated root (root) and the total number of iterations performed (iter) are the module
outputs. After initializing the root (x0 Ð root), the iteration procedure is carried out in a
Repeat-Until loop until the convergence criteria (|δ| ă ε and |fn| ă ε) are met.
The iteration procedure begins by evaluating fpx0q and f1
px0q, and then the current
estimate x is computed from Eq. (4.9). At every iteration step, the asymptotic convergence
constant is also computed by K – δ{δ02, where δ0 “ x0 ´ xpp´1q and δ “ x ´ x0. The
stopping criteria, |δ| ă ε and |fpxq| ă ε, must be satisfied concurrently. If the conver￾gence criteria are not met while within the preset maximum number of iterations (i.e., for
p ď maxit), then, once computed and used, the current estimate and the associated error
are set as prior before the next iteration, i.e., x0 Ð x and δ0 Ð δ. On exit from the module,
the current root estimate and the total iteration number are set to root and iter variables,
respectively. If the upper bound maxit is not sufficiently large enough, due to slow conver￾gence or for some other reason, the iteration loop terminates before the convergence criteria
are met. In which case, the user is issued a warning message along with the latest estimate
and error.218  Numerical Methods for Scientists and Engineers
Pseudocode 4.3
Module NEWTON_RAPHSON (root, maxit, ε, iter)
\ DESCRIPTION: A pseudomodule to find a root of nonlinear equation,
\ fpxq “ 0, using Newton-Raphson method.
\ USES:
\ FUNC and FUNCP :: User defined functions for fpxq and f1
pxq.
x0 Ð root \ Initialize root with input root
δ0 Ð 1 \ Initialize (prior) error
p Ð 0 \ Initialize iteration counter
Repeat \ Iterate until convergence conditions are met
fn Ð FUNCpx0q \ Compute fpxppq
q
f pn Ð FUNCPpx0q \ Compute f1
pxppq
q
δ Ð ´fn{f pn \ Finc current displacement, δppq
K Ð |δ|{δ02 \ Estimate convergence constant
Write: p, x0, fn, f pn, |δ|, K \ Print iteration progress
x Ð x0 ` δ \ Find current estimate, Eq. (4.10)
x0 Ð x \ Set current estimate as prior
δ0 Ð |δ| \ Set current displacement as prior
p Ð p ` 1 \ Count iterations
Until “
(|δ| ă ε And |fn| ă ε ) Or p “ maxit ‰ \ Converged?
root Ð x \ Root is found, if p ă maxit
iter Ð p \ Number of iterations = counter
\ If the iteration limit maxit is reached with no convergence
If “
p “ maxit‰
Then \ Issue info and a warning
Write: “Maximum number of iterations reached”
Write: “Estimated root has NOT converged! |δ|, |fpxq|=”, δ0,fn
End If
End Module NEWTON_RAPHSON
A root of multiplicity m is defined as the number of repeating
(multiple) roots of a nonlinear equation. When fpxq has a root of
multiplicity m ą 1 at x “ α, fpαq “ f1
pαq “ ... “ fpm´1q
pαq “ 0,
but fpmq
pαq ‰ 0. Hence, the Newton-Raphson method leads to a
linear convergence rate for a root of multiplicity.
4.5 MODIFIED NEWTON-RAPHSON METHOD
The Newton-Raphson method fails when f1
pxppq
q “ 0. Likewise, when fpxq has a multiple
root, say in x “ α, the function as well as its first derivative becomes zero, fpαq“f1
pαq“0.
In the latter case, as the successive estimates approach the true root, the first derivative
in Eq. (4.10) will tend to zero, i.e., f1
pxppq
q Ñ 0. Depending on fpxq, this may result in
divergence or oscillations around the true value. Consequently, such cases require a different
approach.Nonlinear Equations  219
Let fpxq be a continuous and differentiable function (up to at least second order) with
at least one repeating root. We define a new function upxq as follows:
upxq “ fpxq
f1
pxq (4.13)
where upxq and fpxq share the same root. Furthermore, suppose fpxq has a root of multi￾plicity m. This can occur if fpxq contains a factor px ´ αq such as
fpxq“px ´ αq
mgpxq, m ą 1 (4.14)
where now g2pαq ‰ 0.
Upon substituting Eq. (4.14) into Eq. (4.13) and arranging, we obtain
upxq “ px ´ αqgpxq
m gpxq`px ´ αqg1
pxq (4.15)
which is a function with a simple root at x “ α. It can be shown that u1
pαq “ 1{m ‰ 0,
where m indicates the multiplicity of the root. Since it is effective for simple roots, the
Newton-Raphson method can also be employed for upxq instead of fpxq by simply replacing
fpxq in Eq. (4.10) with upxq:
xpp`1q “ xppq ´ upxppq
q
u1
pxppqq (4.16)
where u1
pxq can now be expressed in terms of fpxq and f1
pxq as follows:
u1
pxq “ 1 ´ fpxq f 2pxq
pf1
pxqq2
Substituting u1
pxq and Eq. (4.13) into Eq. (4.16) and simplifying results in
xpp`1q “ xppq ´ fpxppq
qf1
pxppq
q
f1 2pxppqq ´ fpxppqqf 2pxppqq (4.17)
Normally, we do not know in advance that a nonlinear equation has a root of multiplicity.
However, if we knew that fpxq had a multiplicity of m, we could speed up the Newton￾Raphson method by replacing Eq. (4.10) with the following one:
xpp`1q “ xppq ´ m
fpxppq
q
f1
pxppqq (4.18)
which gives the conventional Newton-Raphson for m “ 1.
The algorithm, as well as terminating the iteration process, is similar to that of the
Newton-Raphson method. For this reason, a pseudocode for the modified Newton-Raphson
method has not been given, as Pseudocode 4.3 could be used by simply replacing functions
fpxq and f1
pxq in FUNC(x) and FUNCP(x) with upxq and u1
pxq in the user-defined functions.
EXAMPLE 4.5: Finding a repeating root of a function
The function fpxq “ ex´2x´2`ln 4 has a root in p´2, 2q. Estimate the root correct
to four decimal places, using xp0q “ ´1 as the initial guess.220  Numerical Methods for Scientists and Engineers
Modified Newton-Raphson Method
‚ The method is simple and easy to implement;
‚ The modified scheme given by Eq. (4.18) is computationally effective
and fast if the multiplicity is known in advance;
‚ Both schemes of the modified method, like Newton-Raphson, con￾verge quadratically.
‚ The variant of the modified Newton-Raphson method given by Eq.
(4.17) requires deriving and calculating explicit expressions for f1
pxq
and f 2pxq, besides fpxq, which can be computationally expensive for
complicated functions.
SOLUTION:
The graph of fpxq in ´2 ă x ă 2 is presented in Fig. 4.6, which reveals that
fpxq has a repeating (multiple) root about x « 0.7. Therefore, it is appropriate and
necessary to apply modified Newton-Raphson to achieve stable and fast convergence.
FIGURE 4.6
To apply Eq. (4.17), the first and second derivatives are obtained:
fpxq “ ex ´ 2x ´ 2 ` ln 4 “ 0, f1
pxq “ ex ´ 2, f 2pxq “ ex
Starting with the initial guess of xp0q “ ´1, we find
fpxp0q
q “ 1.754174, f1
pxp0q
q“´1.632120, f 2pxp0q
q “ 0.367879
which lead to
δp1q “ fpxp0q
qf1
pxp0q
q
f1 2pxp0qq ´ fpxp0qqf 2pxp0qq
“ ´1.418396
Upon substituting this result into Eq. (4.16), we find
xp1q “ ´1 ´ p´1.418396q “ 0.418396
When the iteration process is carried out in the same manner, it is observed
that the estimates converge to the answer only after three iterations. The iteration
progress is presented in Table 4.5. The rate of convergence is quadratic, i.e., K Ñ
δppq
{pδpp´1q
q2 Ñ 1{6. Since |δp3q| “ 0.219 ˆ 10´4 ă ε “ 0.5 ˆ 10´4, we may predict
at least four-decimal place accuracy in xp3q even though |fpxp3qq| “ 0.48 ˆ 10´9.Nonlinear Equations  221
TABLE 4.5
p xppq fpxppq
q f1
pxppq
q f 2pxppq
q δppq δppq
{pδpp´1q
q2
0 ´1 1.754174 ´1.632120 0.36788 1.418396 0.130858
1 0.418396 0.069025 ´0.480478 1.51952 0.263266 0.130858
2 0.681662 0.000131 ´0.022839 1.97716 0.011465 0.165388
3 0.693125 0.5ˆ10´9 ´0.4 ˆ 10´4 1.99996 0.22 ˆ 10´4 0.166664
The given nonlinear equation has a root of multiplicity 2. Using this information,
we can also use Eq. (4.18) with m “ 2 to solve the problem. The iteration history is
summarized in Table 4.6. This scheme also converges to an estimate accurate to at
least seven decimal places: |δp4q| ă 0.5 ˆ 10´7 and |fpxp4qq| ă 0.5 ˆ 10´7. The rate
of convergence is also quadratic with a bounded K.
TABLE 4.6
p xppq fpxppq
q f1
pxppq
q δppq δppq
{pδpp´1q
q2
0 ´1 1.7541740 ´1.632120 2.1495640
1 1.149564 0.2439826 1.156816 0.4218174 0.0912901
2 0.7277466 0.0012110 0.070410 0.0344000 0.1933341
3 0.6933467 0.398ˆ10´7 0.399 ˆ 10´3 0.199 ˆ 10´3 0.1685966
4 0.6931472 ´0.22ˆ10´15 0.133 ˆ 10´7 0.335 ˆ 10´7 0.8408510
When the problem is also solved using the conventional Newton-Raphson method,
it yields the approximate solution after 14 iterations: |δp14q
| “ 0.309 ˆ 10´4 ă ε and
|fpxp14q
q| “ 0.39 ˆ 10´8 ă ε. The iteration progress, along with the δppq
{δpp´1q
ratio, is tabulated in Table 4.7. Note that |fpxppq
q| approaches zero faster than
|f1
pxppq
q|, which is why the Newton-Raphson method remains stable and converges
to an approximate value. The convergence of the ratio δppq
{δpp´1q to 0.5 verifies that
the convergence rate is linear.
TABLE 4.7
p xppq fpxppq
q f1
pxppq
q δppq δppq
{δpp´1q
0 ´1 1.754174 ´1.6321210 1.074782
1 0.0747820 0.3143796 ´0.9223510 0.3408460 0.3171303
2 0.4156280 0.0703604 ´0.4846780 0.1451695 0.4259093
3 0.5607974 0.0167686 ´0.2479309 0.0676341 0.4658980
4 0.6284316 0.0040992 ´0.1253320 0.0327067 0.4835838
5 0.6611384 0.0010137 ´0.0630039 0.0160897 0.4919403
.
.
. .
.
. .
.
. .
.
. .
.
. .
.
.
12 0.6928997 0.61 ˆ 10´7 ´0.495 ˆ 10´3 0.124 ˆ 10´3 0.4999381
13 0.6930235 ´0.15 ˆ 10´7 ´0.247 ˆ 10´3 0.618 ˆ 10´4 0.4999691
14 0.6930853 0.39 ˆ 10´8 ´0.123 ˆ 10´3 0.309 ˆ 10´4 0.4999845
Discussion: After three iterations, the estimated root with the modified Newton￾Raphson method, Eq. (4.17), is accurate to at least four decimal places. Since the
true value of the root is α “ ln 2, we can determine the true error by α ´ xp3q “
0.2218ˆ10´4 ă 0.5ˆ10´4, which also verifies that this estimate is accurate to four￾decimal places. Note that the magnitudes of |fpxppq
|q and |δppq
| can be significantly
different; that is why it is important to ensure that both criteria are met.222  Numerical Methods for Scientists and Engineers
FIGURE 4.7: Depiction of the Secant method.
Since the multiplicity of the root is known, we are able to use Eq. (4.18) to find
an estimate for the root. The method converged to an approximate value with a
quadratic convergence rate after 4 iterations, with fewer arithmetic operations and
function evaluations compared to using Eq. (4.17).
The conventional Newton-Raphson method with the same initial guess converged
after 14 iterations with a linear convergence rate rather than quadratic, near a root of
multiplicity. Although the modified Newton-Raphson method is given by Eq. (4.17)
is preferred for cases with the roots of multiplicity, it is less efficient and requires
more computational effort, largely due to f 2pxq evaluations, than the conventional
Newton-Raphson method for simple roots. If the multiplicity of the root is known,
Eq. (4.18) should be utilized instead of Eq. (4.17) because of its computational
efficiency.
4.6 SECANT AND MODIFIED SECANT METHODS
A major handicap of the Newton-Raphson or modified Newton-Raphson method is that both
methods require derivatives of fpxq. On the other hand, deriving analytical expressions for
the derivatives of some functions can be difficult or may result in complicated expressions. In
such cases, numerical methods that do not require analytical derivatives are more attractive.
One of the methods that suits this purpose is the Secant method. The algorithm is
similar to that of the Newton-Raphson method. Basically, the same iteration equation, Eq.
(4.10), can be used with the exception that f1
pxppq
q is replaced with a suitable approxima￾tion.
In Fig. 4.7, the two initial guess values and a line that passes through P0 and P1 are
depicted. The slope of the secant line at P1 is set equal to that of the tangent (dashed) line;
that is,
f1
pxp1q
q « fpxp0q
q ´ fpxp1q
q
xp0q ´ xp1q (4.19)
Substituting Eq. (4.19) into Eq. (4.10), a new estimate xp2q is found as
xp2q “ xp1q ´ fpxp1qq
f1
pxp1qq – xp1q ´ fpxp1qqpxp0q ´ xp1qq
fpxp0qq ´ fpxp1qq (4.20)
Equation (4.20) involves two abscissas (xp0q, xp1q
) and two ordinates (fpxp0qq andNonlinear Equations  223
Secant Method
‚ The Secant method is simple and easy to apply;
‚ It requires only one function evaluation as opposed to two evaluations
in the Newton-Raphson method;
‚ It does not require derivatives;
‚ Starting with a set of initial guesses in close proximity to the root,
the method converges very rapidly (faster than that of bisection
and Regula-falsi methods) with an order of convergence of 1.62 (i.e.,
super-linear convergence).
‚ It requires two starting initial guesses;
‚ It suffers from many of the disadvantages of the Newton-Raphson
method;
‚ Its convergence behavior depends on the function and the proximity
of the initial guesses to the true value;
‚ The current estimates have no assured error boundaries;
‚ It may not always converge to a solution for the same reasons also
pointed out for the Newton-Raphson method.
fpxp1q
q) to compute xp2q. That is why a couple of initial guesses must be supplied to start
the iteration process. However, unlike the bisection method, the signs of fpxp0qq and fpxp1qq
need not be opposite.
To generalize the iteration equation, f1
pxppq
q in Eq. (4.10) is replaced by
f1
pxppq
q – fpxpp´1q
q ´ fpxppq
q
xpp´1q ´ xppq (4.21)
After simplifications, the iteration equation for the Secant method becomes
xpp`1q “ xppq
fpxpp´1q
q ´ xpp´1q
fpxppq
q
fpxpp´1qq ´ fpxppqq , p ě 1 (4.22)
The xpp´1q and fpxpp´1q
q values must always be saved for the next iteration.
The modified Secant method is “modified” in the sense that it requires only one initial
guess instead of two. To implement this method, f1
pxppq
q in Eq. (4.10) is replaced by the
following approximation obtained from the definition of limit:
f1
pxppq
q – fpxppq ` hq ´ fpxppq
q
h (4.23)
where h should be chosen “small enough” so that the proposed approximation yields accu￾rate estimates and “large enough” so that the current estimates are not contaminated by
round-off errors.
The iteration equation for the modified Secant method can be expressed as
xpp`1q “ xppq ´ h fpxppq
q
fpxppq ` hq ´ fpxppqq
, p ě 0 (4.24)224  Numerical Methods for Scientists and Engineers
In Pseudocode 4.4, a pseudomodule implementing the secant method is presented. The
SECANT_METHOD requires two initial guesses (x0 and x1), an upper bound for the number
of iterations (maxit), and a convergence tolerance (ε) as input. The nonlinear equation fpxq
is provided to the module by a user-defined external function, FUNC(x). The outputs of
the module are the root and the total number of iterations performed, iter. As an iteration
loop, a Repeat-Until-construct is used. First, the fpxq is calculated for the initial guesses,
fpxp0q
q and fpxp1qq. At the top of every iteration step, the estimate closest to the root
is determined. Of these two, the xppq assumed to be is closer to the root, which means
|fpxppq
q| ă |fpxpp´1q
q|. The estimates are swapped (xppq Õ xpp´1q
, fpxppq
q Õ fpxpp´1q
q)
if |fpxppq
q| ą |fpxpp´1q
q| occurs at any time before a new estimate, xpp`1q
, is computed
with Eq. (4.22). The convergence test is based on both δppq “ |xpp`1q ´ xppq
| ă ε and
|fpxpp`1q
q| ă ε. If no convergence is achieved, then the two most recent estimates are kept
for the next iteration step by setting xpp´1q Ð xppq
, xppq Ð xpp`1q and fpxpp´1q
q Ð fpxppq
q,
fpxppq
q Ð fpxpp`1q
q. To prevent runaway looping, the iteration count is monitored to ensure
it does not exceed maxit. And if maxit is reached with no convergence, the most current
values are printed out with a warning message to the user.
EXAMPLE 4.6: Application of Secant method
Repeat Example 4.1 using (a) the Secant method with rp0q “ 0 and rp1q “ 0.1 and
(b) the modified Secant method with h “ 0.05 and rp0q “ 0.
SOLUTION:
(a) For the two initial guesses, we obtain
fprp0q
q“´0.535, fprp1q
q “ 0.1 ` 0.165 ´ 0.7e´0.12
“ ´0.428035
Using Eq. (4.22), the first estimate and associated ordinates are found as
rp2q “ rp1qfprp0q
q ´ rp0qfprp1q
q
fprp0qq ´ fprp1qq “ 0.1p´0.535q´p0qp´0.428035q
p´0.535q ´ p´0.428035q “ 0.5001636
fprp2q
q“´0.000686
The next estimate obtained as
rp3q “ rp2qfprp1qq ´ rp1qfprp2qq
fprp1qq ´ fprp2qq “ 0.5001636p´0.428035q ´ 0.1p´0.000686q
p´0.428035q ´ p´0.000686q
“ 0.500806
which yields |fprp3qq| “ 0.869 ˆ 10´6 ă ε but δp3q “ rp3q ´ rp2q ą ε. The iterative
procedure is continued in this manner until the convergence criteria are met, i.e.,
the procedure is terminated after 4 iterations when |fprp4qq| ă ε and |δp4q| ă ε. The
iteration history is summarized in Table 4.8.
TABLE 4.8
p rppq fprppq
q |δppq
|
0 0 ´0.535000
1 0.1 ´0.428035 0.1
2 0.5001636 ´6.867 ˆ 10´4 0.4001631
3 0.5008061 ´8.695 ˆ 10´7 6.430 ˆ 10´4
4 0.5008069 ´1.747 ˆ 10´12 8.153 ˆ 10´7Nonlinear Equations  225
Pseudocode 4.4
Module SECANT_METHOD (x0, x1, maxit, ε, root, iter)
\ DESCRIPTION: A pseudomodule to compute a root of nonlinear equation
\ using the Secant method.
\ USES:
\ ABS:: A built-in function computing the absolute value;
\ FUNC:: A user defined function giving nonlinear equation fpxq.
fx0 Ð FUNCpx0q \ Find fpxp0q
q
fx1 Ð FUNCpx1q \ Find fpxp1q
q
p Ð 1 \ Initialize iteration counter
Repeat \ Iterate until convergence conditions are met
p Ð p ` 1 \ Count iterations
If “
|fx1| ą |fx0|
‰
Then \ Swap to order as |fpxpp´1q
q| ą |fpxppq
q|
xt Ð x1; fxt Ð fx1 \ Store x1 and fx1 on temporary variables
x1 Ð x0 \ Swap x’s, xppq Õ xpp´1q
fx1 Ð fx0 \ Swap f’s, fpxppq
q Õ fpxpp´1q
q
x0 Ð xt; fx0 Ð fxt \ Set xt to xpp´1q and fxt to fpxpp´1q
q
End If
x2 Ð px1 ˚ fx0 ´ x0 ˚ fx1q{pfx0 ´ fx1q \ Find current estimate
fx2 Ð FUNCpx2q \ Find fpxpp`1q
q
δ Ð |x2 ´ x1| \ Find displacement δppq
Write: p, x2, fx2, δ \ Print iteration progress
\ Keep the last two estimates and corresponding ordinates
x0 Ð x1; fx0 Ð fx1 \ Set xpp´1q “ xppq and fpxpp´1q
q “ fpxppq
q
x1 Ð x2; fx1 Ð fx2 \ Set xppq “ xpp`1q and fpxppq
q “ fpxpp`1q
q
Until “
(|fx2| ă ε And δ ă ε) Or p “ maxit ‰ \ Check for convergence
root Ð x2 \ Root is found, if p ă maxit
iter Ð p \ Number of iterations = Counter
If “
p “ maxit‰
Then \ Issue info and a warning
Write: “Maximum number of iterations reached”
Write: “Estimated root has NOT converged,! |δ|, |fpxq|=”, δ, fx2
End If
End Module SECANT_METHOD
(b) For rp0q “ 0 and h “ 0.05, we obtain fprp0q
q“´0.535 and fprp0q ` hq “
´0.481509. Using Eq. (4.24) gives
rp1q “ rp0q ´ h fprp0qq
fprp0q ` hq ´ fprp0qq “ 0 ´ 0.05p´0.535q
´0.481509 ´ p´0.535q “ 0.5000817
The iteration process converges to an approximate solution after three iterations
when both convergence criteria are met, that is, |fprp4q
q| “ 1.851 ˆ 10´11 ă ε and
|rp4q ´ rp3q| “ 1.114 ˆ 10´7 ă ε. The iteration history is provided in Table 4.9.226  Numerical Methods for Scientists and Engineers
TABLE 4.9
p rppq fprppq
q fprppq ` hq ´ fprppq
q
h |δppq
|
0 0 ´0.535 1.0698250
1 0.5000817 ´7.735 ˆ 10´4 1.0664193 0.5000817
2 0.5008070 1.188 ˆ 10´7 1.0664145 7.253 ˆ 10´4
3 0.5008069 ´1.851 ˆ 10´11 1.0664145 1.114 ˆ 10´7
Discussion: Comparing to the iteration history of the Newton-Raphson method in
Table 4.4, the Secant method (in Table 4.9) converges at rp4q, whereas the Newton￾Raphson method reached this accuracy by rp3q
. This is because the Secant method
converges at a rate of 1.62, which is considerably faster than the linear convergence
rate of the fixed-point iteration method but slightly slower than the quadratic conver￾gence rate of the Newton-Raphson method. As a rule of thumb, if the computational
effort required to evaluate f1
pxq is less than roughly half of the effort required to
evaluate fpxq, the Newton-Raphson method is more efficient; otherwise, the secant
method is.
The convergence of the modified secant method depends on the selection of the
initial guess and the selection of h. The smaller the value of h, the more accurate
f1
pxq. In this example, h “ 0.05 produced very good estimates for the derivatives.
4.7 ACCELERATING CONVERGENCE
From a given convergent sequence, one can build several iterative schemes that converge
faster toward the same point. This process, known as acceleration has found an application in
the root-finding algorithms. In this context, an iteration method with a linear convergence
rate can be accelerated. Aitken’s acceleration technique is one of several that suit this
purpose. This technique transforms an original sequence txppq
u into another sequence that
converges with fewer iterations (faster) to the answer than the original sequence.
To illustrate the method, consider the sequence txppq
u that converges to α, i.e.,
limpÑ8xppq Ñ α. Recalling the proportionality of the error for sufficiently large p, we
may write
epp`1q
eppq “ eppq
epp´1q “ λ (4.25)
where eppq “ xppq ´ α is the true error at the pth iteration step, α is the true root, and λ is
the convergence rate (|λ| ă 1 for converging series).
Using Eq. (4.25), for large p, we may write
xpp`1q ´ α
xppq ´ α “ xppq ´ α
xpp´1q ´ α
(4.26)
Solving Eq. (4.26) for α yields
α “ xpp`1q
xpp´1q ´ pxppq
q
2
xpp`1q ´ 2xppq ` xpp´1q (4.27)
or alternatively
αppq “ xpp`1q ´ pxpp`1q ´ xppq
q
2
xpp`1q ´ 2xppq ` xpp´1q (4.28)Nonlinear Equations  227
Aitken’s Acceleration Technique
‚ The method does not depend on the kind of root-finding method used
to generate a sequence;
‚ It is not confined to sequences obtained by root-finding methods but
can also be applied to convergent series.
‚ Its convergence is rather slow;
‚ Computation of the differences (Δxppq and Δ2xppq
) can lead to “loss
of significance”;
‚ An original sequence should have a sufficient number of terms; other￾wise, the acceleration procedure may lead to divergence or, at worst,
give a solution that is grossly in error.
αp0q Initial guess
ÝÝÝÝÝÝÝÑ &
%
xp0q “ αp0q
xp1q “ gpxp0q
q
xp2q “ gpxp1q
q
.
-
Ó
compute αp1q
Exit yes ÐÝÝÝ
#|αp1q ´ αp0q
| ă ε or
|αp1q ´ fpαp1q
q| ă ε
+
no, αp1q
ÝÝÝÝÝÝÑ
$
&
%
xp1q “ αp1q
xp2q “ gpxp1q
q
xp3q “ gpxp2q
q
,
.
-
Ó
compute αp2q
Exit yes ÐÝÝÝ
#|αp2q ´ αp1q
| ă ε or
αp2q f αp2q ε
+
no, αp2q
ÝÝÝÝÝÝÑ
$ ,
| ´ p q| ă
FIGURE 4.8: Illustration of iteration sequence of implementing the Steffensen’s method.
Defining Δxppq “ xppq ´ xpp´1q and Δ2xppq “ Δxppq ´ Δxpp´1q
, Eq. (4.28) can then be
recast as
αppq “ xpp`1q ´ pΔxpp`1q
q
2
Δ2xpp`1q , p ě 1 (4.29)
which is used to generate a new and improved sequence of estimates, αppq
, with the aid of
xpp´1q
, xppq and xpp`1q
.
Steffensen’s Acceleration is another acceleration method that is frequently used. It is
an acceleration scheme that offers accelerated convergence for sequences that converge to
a solution linearly. It is a combination of fixed-point iteration and Aitken’s acceleration
scheme and has a quadratic convergence property. The idea behind this technique is that
αppq is thought to be a better approximation to the fixed-point estimate xpp`1q
, so it should
be used as the next iterate for the fixed-point iteration process. In brief, the improved
estimates are used in the fixed-point iteration equations as soon as they become available.
A pseudomodule, STEFFENSEN, for accelerating the convergence of a sequence with
Steffensen’s acceleration scheme is given in Pseudocode 4.5. An initial estimate (x0), an
upper bound for iterations (maxit), and a convergence tolerance (ε) are supplied as input.
As usual, the iteration function gpxq is supplied to the module by FUNC(x), a user-defined
external function. The iteration sequence is illustrated in the flowchart presented in Fig. 4.8.228  Numerical Methods for Scientists and Engineers
Steffensen’s Method
‚ The method has a quadratic convergence rate;
‚ It does convert diverging iterations into a converging sequence;
‚ Unlike Newton’s methods, it does not require the first or second
derivatives, which provide a considerable advantage in cases where
a derivative is not easy to evaluate.
‚ The cost of faster convergence comes as a result of two function eval￾uations, namely gpxppq
q and gpxpp`1q
q per iteration step, which in￾creases the cpu time;
‚ The convergence behavior depends on the iteration function and the
proximity of the initial guess to the true value;
‚ The method may fail to converge, or the sequence of estimates may
either oscillate about the true value or diverge if the initial guess is
not sufficiently proximate to the true root;
‚ It is difficult to extend and apply problems to multivariable systems.
The improved sequence of estimates is denoted by αppq
. At the start, the initial guess is set
to αp0q “ xp0q. The intermediate values, xp1q “ gpxp0qq and xp2q “ gpxp1qq, are successively
computed within the Repeat-Until loop. An improved estimate, αp1q
, is found from Eq.
(4.29), and the convergence check is performed. The root estimation procedure is repeated
until both |αppq ´ αpp´1q
| ă ε and |αppq ´ fpαppq
q| ă ε are simultaneously satisfied. So long
as p ď maxit, the improved estimate is set as the initial value (xppq Ð αppq
) to compute the
next set of subsequent intermediate values: xpp`1q “ gpxppq
q and xpp`2q “ gpxpp`1q
q. When
the number of iterations reaches p “ maxit with no convergence, the most recent values
are printed out along with a warning message to the user.
Pseudocode 4.5
Module STEFFENSEN (x0, maxit, ε, root, iter)
\ DESCRIPTION: A pseudomodule to compute a root of nonlinear equation
\ using the Steffensen’s acceleration with fixed-point iteration method.
\ USES:
\ FUNC:: A user-defined function supplying gpxq as FUNC(x)=gpxq;
\ ABS :: A built-in function computing the absolute value.
α0 Ð x0 \ Initialize root with initial guess
p Ð 0 \ Initialize iteration counter
Repeat \ Iterate until convergence conditions are met
p Ð p ` 1 \ Count iterations
x1 Ð FUNCpx0q \ Find estimate xppq
Δx1 Ð x1 ´ x0 \ Find displacement Δx1
x2 Ð FUNCpx1q \ Find another estimate xpp`1q
Δx2 Ð x2 ´ x1 \ Find displacement Δx2
α Ð x2 ´ pdx2q
2
{pdx2 ´ dx1q \ Find improved estimate, αpnq
dα Ð α ´ α0 \ Find dα
dg Ð α ´ FUNCpαq \ Find dg
Write: p, x0, x1, x2, α, dα, dg \ Print iteration progressNonlinear Equations  229
α0 Ð α; x0 Ð α0 \ Set current values as prior
Until “
(|dα| ă ε And |dg| ă εq Or p “ maxit‰ \ Check for convergence
root Ð α \ Root is found, if p ă maxit
iter Ð p \ Counter gives total number of iterations
If “
p “ maxit‰
Then \ Issue info and a warning
Write: “Maximum number of iterations reached”
Write: “Estimated root has NOT converged,! α, gpαq=”, α, dg
End If
End Module STEFFENSEN
EXAMPLE 4.7: Acceleration techniques
Consider the iteration sequence trppq
u generated in Example 4.3. (a) Apply Aitken’s
acceleration to the sequence to improve its convergence and calculate the convergence
rates for both trppq
u and tαppq
u sequences; (b) Apply Steffensen’s acceleration to
estimate the root to four decimal places.
SOLUTION:
(a) The fixed-point iterates are generated using gprq “ 0.7e´0.1r ´ 0.165 as the
iteration function. The first five iterates, trppq
u, are read from Table 4.3. The ap￾proximate convergence rate of the sequence is computed from λppq « Δrpp`1q
{Δrppq
.
Starting with p “ 1, we find
Δrp1q “ rp1q ´ rp0q “ 0.44683862 ´ 1 “ ´0.5316138
Δrp2q “ rp2q ´ rp1q “ 0.5029690 ´ 0.4683862 “ 0.0345828
Δ2rp2q “ Δrp2q ´ Δrp1q “ 0.0345828 ´ p´0.5316138q “ 0.5661966
Then, for the current step, we find
αp1q “ rp2q ´ pΔrp2qq
2
Δ2rp2q “ 0.5029690 ´ p0.0345828q
2
0.566197 “ 0.5008567
λp1q «
ˇ
ˇ
ˇ
ˇ
Δrp2q
Δrp1q
ˇ
ˇ
ˇ
ˇ “
ˇ
ˇ
ˇ
ˇ
0.0345828
0.5316138
ˇ
ˇ
ˇ
ˇ “ 0.0650524
For p “ 2, we obtain
Δrp3q “ rp3q ´ rp2q “ 0.5006629 ´ 0.5029690 “ ´0.0023060
Δ2rp3q “ Δrp3q ´ Δrp2q “ ´0.0023061 ´ p0.0345828q“´0.0368889
αp2q “ rp3q ´ pΔrp3qq
2
Δ2rp3q “ 0.5006629 ´ p´0.0023060q
2
p´0.0368889q “ 0.5008071
λp2q «
ˇ
ˇ
ˇ
ˇ
Δrp3q
Δrp2q
ˇ
ˇ
ˇ
ˇ “
ˇ
ˇ
ˇ
ˇ
´0.0023060
0.0345828
ˇ
ˇ
ˇ
ˇ “ 0.0666815230  Numerical Methods for Scientists and Engineers
TABLE 4.10
p rppq Δrppq
q |λr
ppq
| αppq Δαppq |λα
ppq
|
0 1
1 0.4683862 ´0.5316138 0.0650524 0.5008567
2 0.5029690 0.0345828 0.0666815 0.5008071 ´4.97 ˆ 10´5 0.0043963
3 0.5006629 ´0.002306 0.0665740 0.5008069 ´2.18 ˆ 10´7 0.0044340
4 0.5008165 1.54 ˆ 10´4 0.0665811 0.5008069 ´9.7 ˆ 10´10
5 0.5008062 ´1.02 ˆ 10´5
Repeating this procedure leads to a new sequence tαppq
u with four values. Five
iteration steps of Aitken’s acceleration parameters are presented in Table 4.10. The
actual computations were performed to 15 decimal places but rounded to 7 decimal
places to fit in the tables. Note that the fixed-point iteration sequence trppq
u depicts
a convergence rate that is slightly oscillating as λr Ñ 0.06658. However, as the new
sequence tαppq
u decreases toward the true root with the convergence rate λα Ñ
0.04434, we observe λα « p2{3qλr, which is evidence of faster convergence than
fixed-point iteration.
(b) To start Steffensen’s acceleration procedure, we begin with the first three iter￾ates of fixed-point iteration (rp0q, rp1q, rp2q). Using Eq. (4.29), the first approximation
with Aitken’s acceleration, αp1q, is found as
αp1q “ rp2q ´ pΔrp2qq
2
Δ2rp2q “ 0.502969 ´ p0.0345828q
2
0.566197 “ 0.5008567
with Δαp1q “ αp1q ´ rp0q “ ´0.4991433.
We use this value to restart fixed-point iterations, rp1q “ αp1q, and calculate the
next two iterates as
rp2q “ gprp1q
q “ gp0.5008567q “ 0.5008036,
rp3q “ gprp2q
q “ gp0.5008036q “ 0.5008071
Next, we compute
Δrp2q “ rp2q ´ rp1q “ 0.5008036 ´ 0.5008567 “ ´5.31 ˆ 10´5
Δrp3q “ rp3q ´ rp2q “ 0.5008067 ´ 0.5008036 “ 3.1 ˆ 10´6
Δ2rp3q “ Δrp3q ´ Δrp2q “ 3.1 ˆ 10´6 ´ p´5.31 ˆ 10´5q “ 5.62 ˆ 10´5
Applying Aitken’s acceleration formula, Eq. (4.29), to rp1q, rp2q, and rp3q we find
αp2q “ rp3q ´ pΔrp3qq
2
Δ2rp3q “ 0.5008071 ´ p3.1 ˆ 10´6q
2
5.62 ˆ 10´5 “ 0.5008069
The iteration history is summarized in Table 4.11. In this exercise, as shown,
the Steffensen’s acceleration required only two iterations and converged with
|αp2q ´ αp1q| “ 0.498 ˆ 10´4 ă ε and |αp2q ´ gpαp1q
q| “ 0.549 ˆ 10´12 ă ε. TheNonlinear Equations  231
ratio Δαp2q{Δαp1q at the end of the second iteration is about 10´4, which indicates
a convergence faster than the linear convergence rate.
TABLE 4.11
p rpp´1q rppq
q rpp`1q |αppq
| Δαppq |αppq ´ gpαppq
q|
1 1 0.4683862 0.5029690 0.5008567 0.4991433 5.31 ˆ 10´5
2 0.5008567 0.5008036 0.5008071 0.5008069 5 ˆ 10´5 5.49 ˆ 10´13
Discussion: The fixed iterative point method was modified such that Steffensen’s
method converges quadratically while still keeping the cost low and the method
simple.
In this example, we clearly observe that Steffensen’s method converges faster
than Aitken’s method. Though a word of caution is in order, due to the subtractions
of very close iterates, the “loss of significance” can occur, and it may ultimately
corrupt the results. Hence, the Aitken acceleration process should be terminated im￾mediately after the iterates become apparently stationary. But Steffensen’s method
combines the fixed-point iteration with Aitken’s method by using the new iterates,
αppq
, immediately since they are better estimates of the root than rppq
. The method
converges with a quadratic convergence rate but does not require the derivative of
the nonlinear equation.
4.8 SYSTEM OF NONLINEAR EQUATIONS
Systems of nonlinear equations are frequently encountered in many branches of science and
engineering since most physical systems occurring in nature are inherently nonlinear. Hence,
the mathematical modeling of such systems also leads to nonlinear simultaneous equations.
Nonlinear systems of equations could be very complicated and difficult to solve if the
system variables are highly interdependent, in which case the solution of such a system may
depict abrupt changes with slight perturbations in the input variables. For this reason, the
numerical solution of nonlinear equations is one of the major challenges facing a numerical
analyst today.
Hence, the search for the numerical solution of nonlinear systems of equations has
been an old and difficult problem. The bracketing methods are practically very difficult to
implement in systems of nonlinear equations. The best alternative, for being very simple
and effective, is Newton’s method. That is why the numerical algorithm that we will be
discussing in this section is an extension of the Newton-Raphson Method to multivariable
functions.
For simplicity, we consider the following system of two nonlinear equations with two
unknowns:
f1px, yq “ 0, f2px, yq “ 0 (4.30)
where x and y are independent variables (unknowns), and f1 and f2 are two coupled nonlin￾ear functions. In some cases, a 2 ˆ 2 non-linear system can be reduced to a single nonlinear
equation without much difficulty. Nevertheless, in this section, we will assume that the
coupled nonlinear systems are irreducible to a single nonlinear equation.232  Numerical Methods for Scientists and Engineers
Consider the Taylor series expansion of fpx, yq about px0, y0q:
fpx, yq“fpx0, y0q` 1
1!
´
px´x0q B
Bx `py´y0q B
By
¯
fpx0, y0q`O`
pδxq
2`pδyq
2˘
`... (4.31)
Now, consider the truncated Taylor series approximation of f1px, yq and f2px, yq with
the first two terms in the vicinity of px0, y0q:
f1px, yq – f1px0, y0q`px ´ x0q
Bf1
Bx px0, y0q`py ´ y0q
Bf1
By px0, y0q
f2px, yq – f2px0, y0q`px ´ x0q
Bf2
Bx px0, y0q`py ´ y0q
Bf2
By px0, y0q
(4.32)
When px, yq is the true solution, both equations will be satisfied. Thus, setting f1px, yq“0
and f2px, yq“0 in Eq. (4.32) and rearranging yields the following matrix equation:
»
—
—
—
–
Bf1
Bx px0, y0q Bf1
By px0, y0q
Bf2
Bx px0, y0q Bf2
By px0, y0q
fi
ffi
ffi
ffi
fl
«
δx
δyff
“ ´ «
f1px0, y0q
f2px0, y0q
ff
(4.33)
where δx “ x ´ x0 and δy “ y ´ y0, and the matrix evaluated at px0, y0q is called the
Jacobian matrix, which is usually denoted by J. In order for Eq. (4.33) to have a unique set
of solutions, the Jacobian matrix needs to be non-singular, i.e., detpJq “ J ‰ 0.
The solution of Eq. (4.33) provides a set of improved estimates. Adopting the super￾script notation, the sequence of estimates and the absolute errors at any pth iteration step
are denoted by (xppq
, yppq
) and (δxppq “ xpp`1q ´ xppq
, δyppq “ ypp`1q ´ yppq
), respectively.
The iteration equations from the solution of Eq. (4.33) can be written explicitly as
xpp`1q “xppq ´ 1
J
!
f1pxppq
, yppq
q
Bf2
By pxppq
, yppq
q´f2pxppq
, yppq
q
Bf1
By pxppq
, yppq
q
)
ypp`1q “yppq ´ 1
J
!
´f1pxppq
, yppq
q
Bf2
Bx pxppq
, yppq
q`f2pxppq
, yppq
q
Bf1
Bx pxppq
, yppq
q
)
(4.34)
where
J “ Jpxppq
, yppq
q “ ˆBf1
Bx
Bf2
By ´ Bf1
By
Bf2
Bx
˙
pxppq,yppqq
(4.35)
Note that the algorithm requires the partial derivatives of f1px, yq and f2px, yq with respect
to x and y variables. Hence, it is necessary to evaluate f1 and f2 as well as their partial
derivatives, preferably analytically. The convergence criterion can be based on �8´norm
max `
δxppq
, δyppq
˘
ă ε or �2´norm ppδxppq
q2 ` pδyppq
q2q1{2 ă ε.
Partial derivatives of a Jacobian matrix may be computed using
finite difference formulas if nonlinear equations contain very long
and/or complex mathematical expressions. However, this alter￾native may increase cpu-time and can have an adverse effect on
convergence if the system is sensitive to small perturbations or
the linear system is ill-conditioned.Nonlinear Equations  233
Now, we extend the Newton’s method to n simultaneous nonlinear equations with n
unknowns. Let the system of nonlinear equations be given as
fpxq “ 0 (4.36)
where
x “
»
—
—
—
–
x1
x2
.
.
.
xn
fi
ffi
ffi
ffi
fl , fpxq “
»
—
—
—
–
f1pxq
f2pxq
.
.
.
fnpxq
fi
ffi
ffi
ffi
fl “
»
—
—
—
–
f1px1, x2,...,xnq
f2px1, x2,...,xnq
.
.
.
fnpx1, x2,...,xnq
fi
ffi
ffi
ffi
fl
In matrix-vector notation, the generalized Taylor series expansion of fpxq about xppq
in general space can be written as
fpxq « fpxppq
q ` d
dxfpxppq
q px ´ xppq
q`¨¨¨
Higher-order terms of the Taylor series can be neglected if x “ xpp`1q is sufficiently close
to xppq
. Then, a two-term Taylor approximation becomes
fpxpp`1q
q « fpxppq
q ` J pxppq
q δppq ` Opδppq
q
2
(4.37)
where δppq “ xpp`1q ´ xppq is the displacement (or error) vector, xpp`1q and xppq are vectors
containing the current and prior estimates at the pth iteration step, and J is an n ˆ n
Jacobian matrix expressed as
J pxq “ d
dxfpxq “
»
—
—
—
—
—
—
—
—
—
–
Bf1
Bx1
pxq Bf1
Bx2
pxq ¨¨¨ Bf1
Bxn
pxq
Bf2
Bx1
pxq Bf2
Bx2
pxq ¨¨¨ Bf2
Bxn
pxq
.
.
. .
.
. ... .
.
.
Bfn
Bx1
pxq Bfn
Bx2
pxq ¨¨¨ Bfn
Bxn
pxq
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(4.38)
with the matrix elements Jij “ Bfi{Bxj evaluated at xppq
.
Assuming xpp`1q is sufficiently close enough to the true solution, thus setting
fpxpp`1q
q “ 0 in Eq. (4.37) and rearranging, the following system of linear equations for
δppq is obtained:
Jpxppq
q δppq “ ´fpxppq
q (4.39)
which should be solved with a direct method at every iteration step. Nevertheless, the
characteristic of the Jacobian matrix is important in determining the numerical method, as
it may be ill-conditioned. The Jacobian matrix is a dense matrix, and for a small number
of nonlinear equations, it can be inverted using a symbolic processor to obtain the solution
as δppq “ ´J´1pxppq
qfpxppq
q. The current estimates, xpp`1q
, are then updated as
xpp`1q “ xppq ` δppq (4.40)
The Jacobian is a dense matrix and is usually not diagonally dom￾inant. Hence, the iterative methods for solving Eq. (4.39) in non￾linear problems cannot be considered an alternative.234  Numerical Methods for Scientists and Engineers
Newton’s Method
‚ Newton’s method is easy to understand and implement;
‚ It can be applied to solve a variety of problems;
‚ If a solution exists, it converges at a quadratic convergence rate with
a good set of initial guesses, i.e., the error is squared (or the number
of accurate-digit doubles) at each iteration step.
‚ It may fail to converge, or convergence difficulties may be observed
if the system is very sensitive to initial values;
‚ The Jacobian should be constructed, preferably analytically, which
can be tedious for large or complex systems;
‚ The Jacobian and its inverse have to be calculated at each iteration
step, which can be quite time-consuming depending on how large the
system is.
‚ The resulting linear systems are generally not suitable to be solved by
iterative methods; thus, applying a direct method at each iteration
step can also be cpu-time intensive;
‚ For ill-conditioned systems, κpJq " 1, a proper correction algorithm
should also be employed.
A good set of initial guesses, xp0q
, is required to start the iteration process. Nonetheless,
the choice of a set of initial values is a determining factor for the convergence of the method.
If the initial values are not in close proximity to the true solution, then the procedure may
diverge or converge. In this context, finding a set of good initial guesses is important and
a bit tricky as well, because it often requires some knowledge of the variables of the actual
physical problem being undertaken. The current estimates are computed from Eqs. (4.39)
and (4.40), preferably employing the Gauss elimination method with partial pivoting, which
is the most common and generally the most robust method for this purpose. Note that the
Jacobian matrix requires prior estimates (Jppq “ Jpxppq
q), and thus it needs to be computed
at every iteration step. The process of solving the system of linear equations at every
iteration step is repeated until convergence is achieved.
Pseudocode 4.6
Module NEWTON_SYSTEM (n, x0, x, maxit, ε, Norm, iter)
\ DESCRIPTION: A pseudomodule to find common solution of a system of
\ a coupled nonlinear equations using Newton’s method.
\ USES:
\ FCN:: User-defined module supplying the nonlinear eqs and Jacobian;
\ GAUSS_ELIMINATION_P:: Gauss-Elimination module (Pseudocode 2.10;
\ ENORM:: Function Module calculating Euclidean (�2´)norm of a vector
(Pseudocode 3.1)
Declare: xn, x0n, δn, fn, jn,n \ Declare array variables
p Ð 0 \ Initialize iteration counter
Repeat \ Iterate until convergence conditions are met
p Ð p ` 1 \ Count iterations
FCN(n, x0, f, J) \ Compute fpxppq
q and Jpxppq
q
GAUSS_ELIMINATION_P(n, J, f, δ) \ Solve Jpxppq
q δppq “ fpxppq
qNonlinear Equations  235
Norm Ð ENORMpn, δq \ Set �2´norm of δppq to Norm
Write: “Iteration=”, p, “�2 norm=”,Norm \ Print iteration progress
For “
k “ 1, n‰ \ Loop k: Find current estimates
xk Ð x0k ´ δk \ Compute xpp`1q
x0k Ð xk \ Set current estimates as prior
End For
Write: x, f
Until “
Norm ă ε Or p “ maxit ‰ \ Check for convergence
iter Ð p \ Total number of iterations performed
\ If the iteration limit maxit is reached with no convergence
If “
p “ maxit‰
Then \ Print most current estimates and issue a warning
Write: “Maximum number of iterations reached, xi’s=”, x
Write: “Estimated root has NOT converged! Errors δi’s=”, δ
End If
End Module NEWTON_SYSTEM
Module FCN (n, x, f, J)
\ DESCRIPTION: A user-defined module to evaluate nonlinear Eqs & Jacobian.
Declare: xn, fn, jn,n \ Declare array variables
\ Supply n-nonlinear eqs as f1px1,...,xnq “ 0, ..., fnpx1,...,xnq “ 0
f1 Ð x2
1 ` 2x2
2 ´ 10 \ Equation 1: f1px1, x2q “ x2
1 ` 2x2
2 ´ 10 “ 0
f2 Ð x2
1x2 ` x3
2 ` 3 \ Equation 2: f2px1, x2q “ x2
1x2 ` x3
2 ` 3 “ 0
\ Construct the Jacobian, Jij “ Bfi{xj
j1,1 Ð 2x1; j1,2 Ð 4x2; \ Row-1: Bf1
Bx1 “ 2x1, Bf1
Bx2 “ 4x2
j2,1 Ð 2x1x2; j2,2 Ð x2
1 ` 3x2
2 \ Row-2: Bf2
Bx1 “ 2x1x2, Bf2
Bx2 “ x2
1 ` 3x2
2
End Module FCN
A sufficient condition for the convergence at every iteration step is detpJ´1q ă 1.
A convergence criterion is generally based on either �8 or �2´norms or both. The most
common criterion is based on �2´norm; that is,
›
›
›
δppq
›
›
›
2
“
›
›
›
›xpp`1q ´ xppq
›
›
›
›
2
ă ε
A pseudo-module, NEWTON_SYSTEM, implementing Newton’s algorithm for a system
of nonlinear equations is presented in Pseudocode 4.6. The code requires the number of
equations (n), an initial guess vector (x0), an upper bound for iterations (maxit), and a
convergence tolerance (ε) as input. The system of nonlinear equations fpxq and the Jacobian
Jpxq are supplied by FCN, a user-defined module. The estimated solution (x), Euclidean
norm of the displacement vector (Norm), and the total number of iterations performed
(iter) are the outputs. As a standard procedure, a Repeat-Until (iteration) loop is checked
against maxit to avoid computations going into an infinite loop. By visiting the module FCN
at the top of every iteration step, the nonlinear equations fpxppq
q and the Jacobian Jpxppq
q
are updated. Then, Jpxppq
qδppq “ ´Fpxppq
q is solved using the Gauss elimination procedure
that implements a pivoting technique. The current estimates are found by Eqs. (4.40). The
iteration procedure is terminated when the δppq
2 ă ε condition is met; otherwise, the
iteration procedure is repeated by setting the current estimates as prior xppq Ð xpp`1q
.236  Numerical Methods for Scientists and Engineers
EXAMPLE 4.8: Implementing Newton’s method to two equations with two unknowns
A pair of reversible chemical reactions that occur simultaneously are given as follows:
A ` 2B Õ D, A ` C Õ D
where A, B, C, and D denote chemical compounds. A chemical reactor is fed with
15 moles of A, 9 moles of B, and 5 moles of C. At equilibrium, the first and second
reactions produce x and y moles of D, respectively. The system leads to the following
equilibrium equations:
K1 “ x ` y
p15 ´ x ´ yqp9 ´ 2xq
2 , K2 “ x ` y
p15 ´ x ´ yqp5 ´ yq
where K1 “ 1 and K2 “ 50 are the equilibrium constants. Determine x and y
accurately to four decimal places using Newton’s method. Use xp0q “ yp0q “ 2.
SOLUTION:
Rearranging the equilibrium equations yields the following nonlinear system of
equations:
f1px, yq “ x ` y ´ p15 ´ x ´ yqp9 ´ 2xq
2 “ 0,
f2px, yq “ x ` y ´ 50p15 ´ x ´ yqp5 ´ yq “ 0
To construct the Jacobian matrix, the Newton’s method requires partial deriva￾tives of f1 and f2 wrt x and y, which are obtained as
Bf1
Bx “ 1 ` p9 ´ 2xqp69 ´ 6x ´ 4yq, Bf1
By “ 1 ` p9 ´ 2xq
2
,
Bf2
Bx “ 1 ` 50p50 ´ yq, Bf2
By “ 1 ` 50p20 ´ x ´ 2yq.
For given initial guess set, we obtain fpx, yq, Jpx, yq, and Jpx, yq as
f p0q
p2, 2q “ „
´271
´1646j
, Jp0q
p2, 2q “ „
246 26
151 701j
, Jp0q
p2, 2q “ 168520
where Jpx, yq is the determinant of the Jacobian matrix. Note that, with detpJp0q
q ą
1, the system is not ill-conditioned.
Now we proceed to compute the current displacement vector as
δp0q “ ´ ”
Jp0q
p2, 2q
ı´1
f p0q
p2, 2q “ „
0.873338
2.159951j
where δppq “ rδ
ppq x δ
ppq y s
T is the displacement vector with δ
ppq x “ xpp`1q ´ xppq and
δ
ppq y “ ypp`1q ´ yppq
.
Next, the current estimates are updated by xp1q “ xp0q ` δp0q as follows:
xp1q “
„
2
2
j
`
„
0.873338
2.159951j
“
„
2.873338
4.159951jNonlinear Equations  237
Note that δppq
2 is an indicator of how close the overall estimate is to the true
solution. Thus, to obtain estimates accurate to four decimal places, we will seek
δppq
2 ă ε, where the tolerance is set to ε “ 0.5 ˆ 10´4.
TABLE 4.12
p xppq yppq
q δ
ppq x δ
ppq y δppq
2
0 2 2 0.873338 2.159950 2.329829
1 2.873338 4.159951 0.601855 0.683618 0.910804
2 3.475193 4.843570 0.318869 0.119941 0.340681
3 3.794063 4.963511 0.095786 0.007497 0.096079
4 3.889849 4.971007 0.008659 0.000057 8.66 ˆ 10´3
5 3.898508 4.971064 6.91 ˆ 10´5 4.68 ˆ 10´7 6.91 ˆ 10´5
6 3.898577 4.971063
The iterative procedure was continued in the same manner until δp0q
2 ă ε was
achieved in six iterations. The summary of the iteration history is presented in Table
4.12. It should be pointed out that the converged solution is in fact one of the two
solutions of the nonlinear system. The other set is found as (5.227185, 4.957696) by
changing the initial guess.
Discussion: Nonlinear systems of equations usually do not have closed-form so￾lutions. Consequently, various numerical methods have been developed for solving
such systems of equations. In this section, we introduced Newton’s method, which is
a generalization of the Newton-Raphson method that has a quadratic convergence
rate. In this respect, it is superior to other methods such as bisection, fixed-point,
or secant methods, and so on.
With a “smart initial guess,” we hope to have the system converge quite rapidly.
In this problem, initially, the error norm is reduced by approximately one-third in the
first few iterations, but then the convergence accelerates as the estimates approach
the true solution. The advantage of Newton’s method is that it may require fewer
iterations to converge an approximate solution, compared to other methods with
a lower rate of convergence. A nonlinear system that does not converge may be
indicative of a non-existing solution or ill-condition.
4.9 BAIRSTOW’S METHOD
Bairstow’s method is one of the earliest methods for finding all (real and imaginary) roots
of a polynomial with real coefficients. It is an iterative method for determining a quadratic
factor of a polynomial of nth degree Pnpxq, based on the idea of synthetic division of a
polynomial by a quadratic polynomial.
Consider the following nth-degree (n ą 2) polynomial with real coefficients:
Pnpxq “ xn ` a1xn´1 ` a2xn´2 ` ... ` an´1x ` an “ 0 (4.41)
where a1, a2,...,an are the coefficients. A polynomial may have n simple, multiple (re￾peated), or complex roots coexisting in conjugate pairs.238  Numerical Methods for Scientists and Engineers
This method, in essence, is based on the synthetic division of a polynomial by a
quadratic factor (x2 ` px ` q), which is then used to compute its real or imaginary roots.
Any polynomial can be expressed in terms of one of its quadratic factors as
Pnpxq“px2 ` px ` qqQn´2pxq ` Rx ` S (4.42)
where Qn´2pxq “ xn´2 ` b1xn´3 ` ... ` bn´3x ` bn´2 is the reduced polynomial, p and
q (initially unknown) are the coefficients of the quadratic polynomial, and Rx ` S is the
remainder term. Once a quadratic factor is obtained (i.e., p and q are found), the remainder
should yield zero (R “ S “ 0).
We assume that the coefficients of the remainder depend on p and q, i.e., R “ Rpp, qq
and S “ Spp, qq, which will vanish when p and q satisfy the system of nonlinear equations
defined as Rpp, qq “ 0 and Spp, qq “ 0. Thus, in a way, Bairstow’s method is reduced to
simply finding the solution of a 2ˆ2 nonlinear system, whose numerical solution is obtained
iteratively applying Newton’s method (see Section 4.8). The remainder for a set of initial
guesses will most likely not result in non-zero R and S; thus, we require the solution of the
following system of nonlinear equations:
Rppp0q ` δp, qp0q ` δqq “ 0, Sppp0q ` δp, qp0q ` δqq “ 0 (4.43)
where δp and δq are the improvements of p and q, respectively.
The next step is to determine a pair of δp and δq that ensures the elimination of the
remainder. Using the Newton’s method presented in Section 4.8 and replacing pf1, f2q Ñ
pR, Sq, px, yqÑpp, qq, px0, y0qÑppp0q, qp0q
q and pδx, δyqÑpδp, δqq in Eqs. (4.32) and
(4.33), the solution of the system of linear equations for δp “ p ´ pp0q and δq “ q ´ qp0q can
be expressed as
»
—
—
—
–
BR
Bp
ppp0q, qp0qq BR
Bq
ppp0q, qp0qq
BS
Bp
ppp0q
, qp0q
q BS
Bq
ppp0q, qp0qq
fi
ffi
ffi
ffi
fl
»
–
δp
δq
fi
fl “ ´
»
–
Rppp0q, qp0q
q
Sppp0q, qp0qq
fi
fl (4.44)
Upon solving Eq. (4.44), the improvements become available, and then the next values
(current estimates) of p and q can be computed. From this point on, we will use ppkq
and qpkq notations to denote p and q values at the kth iteration step. Also, we define the
improvements at the kth iteration step as δppkq “ ppk`1q ´ ppkq and δqpkq “ qpk`1q ´ qpkq
.
The current solution is then obtained as ppk`1q “ ppkq ` δppkq and qpk`1q “ qpkq ` δqpkq
. In
subsequent iterations, the improvements should yield zero if the initial guess is good.
In order to solve Eq. (4.44), we note that R, S, and the pertinent partial derivatives
are unknown at this stage. The coefficients of the polynomials Pnpxq and Qn´2pxq must
also satisfy Eq. (4.42). So multiplying things out, collecting similar x-terms and constants,Nonlinear Equations  239
and equating the coefficients yields
a1 “ b1 ` p,
a2 “ b2 ` pb1 ` q,
a3 “ b3 ` pb2 ` qb1,
.
.
.
ak “ bk ` pbk´1 ` qbk´2,
.
.
.
an´2 “ bn´2 ` pbn´3 ` qbn´4
an´1 “ R ` pbn´2 ` qbn´3
an “ S ` qbn´2
(4.45)
where b1, b2, ..., bn´2, R, S can be determined step by step.
Equation (4.45) can be expressed as a single recurrence relationship by setting b´1 “ 0
and b0 “ 1. This leads to the following single-recurrence relationship:
bi “ ai ´ pbi´1 ´ qbi´2, i “ 1, 2,...,n (4.46)
which should also yield bn´1 “ an´1 ´ pbn´2 ´ qbn´3 “ R and bn “ an ´ pbn´1 ´ qbn´2 “
S ´ pbn´1. Thus, we may write
R “ bn´1 and S “ bn ` pbn´1, (4.47)
Having derived the above expressions, the partial derivatives can now be found from
BR
Bp “ Bbn´1
Bp , BR
Bq “ Bbn´1
Bq
BS
Bp “ B
Bp pbn ` pbn´1q “ Bbn
Bp
` p
Bbn´1
Bp
` bn´1
BS
Bq “ B
Bq pbn ` pbn´1q “ Bbn
Bq
` p
Bbn´1
Bq
(4.48)
Substituting Eqs. (4.47) and (4.48) into Eq. (4.43) and simplifying, we get
´Bbn´1
Bp
¯
δp `
´Bbn´1
Bq
¯
δq “ ´bn´1 (4.49)
´Bbn
Bp
` p
Bbn´1
Bp
` bn´1
¯
δp `
´Bbn
Bq
` p
Bbn´1
Bq
¯
δq “ ´bn ´ pbn´1 (4.50)
Multiplying Eq. (4.49) with p and subtracting it from Eq. (4.50) yields
bn `
´Bbn
Bp
` bn´1
¯
δp `
´Bbn
Bq
¯
δq “ 0 (4.51)
This last expression may be used in place of Eq. (4.50). Furthermore, Eq. (4.46) is used
to evaluate the partial derivatives of bn and bn´1 in Eqs. (4.49) and (4.50) and to develop
recurrence relationships for Bbi{Bp and Bbi{Bq. Since the coefficients (ai’s) of the polynomial
are constants, we may set Bai{Bp “ Bai{Bq “ 0. The partial derivatives in Eq. (4.46) become
Bbi
Bp “ ´p
Bbi´1
Bp ´ bi´1 ´ q
Bbi´2
Bp (4.52)240  Numerical Methods for Scientists and Engineers
Bairstow’s Method
‚ The method gives all real and imaginary roots of a polynomial;
‚ It does not require arithmetic operations with imaginary numbers;
‚ It does not require partial derivatives;
‚ It converges, when it does, with a quadratic rate of convergence.
‚ The method is hard to understand and implement;
‚ It provides the roots of only “polynomials;”
‚ It requires a good set of initial guesses; the closer the starting
quadratic equation is to an actual one (pp0q « p, qp0q « q), the faster
it converges;
‚ It is not suitable for every polynomial and has difficulty handling
large polynomials;
‚ It can be sensitive to the initial guess and/or the coefficients of the
polynomial.
Bbi
Bq “ ´p
Bbi´1
Bq ´ q
Bbi´2
Bq (4.53)
Recalling that b´1 “ 0 and b0 “ 1, we conclude the following:
Bb´1
Bp “ Bb0
Bp “ 0, Bb´1
Bq “ Bb0
Bq “ 0 (4.54)
Finally, the partial derivatives are computed from Eqs. (4.52), (4.53), and (4.54). How￾ever, to obtain simpler expressions, we adapt Bbi{Bp “ ´ ci´1 pi “ 1, 2,...,nq definition, in
which case we may write
Bbi´1
Bp “ Bbi
Bq “ ´ci´2 (4.55)
Using these, the recurrence relationship can be further simplified as follows:
c´1 “ 0, c0 “ 1, ci “ bi ´ pci´1 ´ qci´2, i “ 1, 2,...,pn ´ 1q (4.56)
where bi’s are calculated from ai’s, and ci’s are calculated from bi’s.
Using Eq. (4.49), (4.51), (4.55), and (4.56), we obtain
cn´2δp ` cn´3δq “ bn´1, pcn´1 ´ bn´1qδp ` cn´2δq “ bn (4.57)
Defining cn´1 ´ bn´1 “ c¯n´1, the system of equations for the correction terms becomes
»
–
δp
δq
fi
fl “
»
–
cn´2 cn´3
c¯n´1 cn´2
fi
fl
´1 »
–
bn´1
bn
fi
fl (4.58)
Pseudocode 4.7
Module BAIRSTOW (n, p0, q0, a, ε, maxit, xre, xim)
\ DESCRIPTION: A pseudomodule to compute all (real and imaginary) roots
\ of an nth degree polynomial given by Eq.(4.41) with Bairstow’s method.Nonlinear Equations  241
\ USES:
\ QUADRATIC:: Module for finding all roots of a quadratic equation;
\ ABS:: A built-in function computing the absolute value.
Declare: a0:n, b0:n, c0:n, xren, ximn, xr2, xi2 \ Declare array variables
For “
k “ n, 0,p´1q
‰ \ The method works if a0 “ 1
ak Ð ak{a0 \ Normalize coefficients of the polynomial with a0
End For
m Ð n \ Save the degree of polynomial for later use
kount Ð 0 \ Counter for the number of roots “found”
While “
n ą 1
‰ \ Loop to find all quadratic factors until n “ 1
p Ð p0; q Ð q0 \ Set initial guesses to p, q
k Ð 0 \ Initialize iteration counter
Δ Ð 1 \ Initialize overall error estimate
\ b is an array of length n containing coefficients of quotient polynomial
\ c is an array of length n containing coefficients of partial derivatives
While “
Δ ą ε And k ď maxit ‰ \ Loop to find a quadratic factor
k Ð k ` 1 \ Count iterations
b0 Ð 1; c0 Ð 1 \ Initialize b0 and c0
b1 Ð a1 ´ p; c1 Ð b1 ´ p \ Find b1 and c1
For “
i “ 2, n‰ \ Loop i: compute bi’s and ci’s
bi Ð ai ´ p ˚ bi´1 ´ q ˚ bi´2 \ Use Eq. (4.46)
ci Ð bi ´ p ˚ ci´1 ´ q ˚ ci´2 \ Use Eq. (4.56)
End For
\ Construct & solve 2 ˆ 2 system to find improved p & q
c¯ Ð cn´1 ´ bn´1; δ Ð cn´2 ˚ cn´2 ´ c¯ ˚ cn´3
δp Ð pbn´1 ˚ cn´2 ´ bn ˚ cn´3q{δ \ Find δp
δq Ð pbn ˚ cn´2 ´ bn´1 ˚ c¯q{δ \ Find δq
p Ð p ` δp; q Ð q ` δq \ Find current estimates p, q
Δ Ð |δp|`|δq| \ Find current error by δppq
1
End While
If “
k “ maxit And Δ ą ε
‰
Then \ Check for convergence
Write: “Quadratic factor did not converge after”, maxit,“iterations”
Write: “Recent values of p, q and Δ are”, p, q, Δ
Exit
Else \ Find all roots of a quadratic factor
QUADRATIC(p, q, xr, xi) \ Find roots of x2 ` px ` q “ 0
\ Save real and imaginary parts on arrays xr, xi
kount Ð kount ` 1 \ Root 1 is xr1 ` i ˚ xim1
xre kount Ð xr 1
ximkount Ð xi 1
kount Ð kount ` 1 \ Root 2 is xr2 ` i ˚ xim2
xre kount Ð xr 2
ximkount Ð xi 2
n Ð n ´ 2 \ Find the degree of quotient
End If
p0 Ð p; q0 Ð q \ Set current pp, qq as “prior” for next iterations
For “
i “ 0, n‰ \ Loop to reset coefficients of the new polynomial
ai Ð bi \ Set bi’s as ai’s
End For242  Numerical Methods for Scientists and Engineers
If “
n “ 1
‰
Then \ Case of a simple root, x ` a1 “ 0
kount Ð kount ` 1 \ Count last root
xre kount Ð ´a1 \ Set ´a1 as the final root
ximkount Ð 0
End If
End While
n Ð m \ Restore degree of polynomial for output
End Module BAIRSTOW
Module QUADRATIC (p, q, xr, xi)
\ DESCRIPTION: A pseudomodule to find the real and imaginary roots of
\ a quadratic equation of the form x2 ` px ` q “ 0.
\ USES:
\ SQRT:: A built-in function computing the square-root of a value.
Declare: xr2, xi2 \ Declare array variables
Δ Ð p2 ´ 4 ˚ q \ Compute discriminant
If “
Δ ă 0
‰
Then \ Case of Imaginary roots
Δ Ð ?´Δ \ Set Δ Ð a4q ´ p2
xr1 Ð ´p{2; xi1 Ð Δ{2 \ Root 1: p´p ` i
a4q ´ p2q{2
xr2 Ð xr1; xi2 Ð ´xi1 \ Root 2: p´p ´ i
a4q ´ p2q{2
Else \ Case of Real roots (Δ ě 0)
Δ Ð ?
Δ \ Set Δ Ð ap2 ´ 4q
xr1 Ð p´p ` Δq{2; xi1 Ð 0 \ Root 1: p´p ` ap2 ´ 4qq{2
xr2 Ð p´p ´ Δq{2; xi2 Ð 0 \ Root 2: p´p ´ ap2 ´ 4qq{2
End If
End Module QUADRATIC
which gives
δp “ bn´1cn´2 ´ bncn´3
c2
n´2 ´ c¯n´1cn´3
, δq “ bncn´2 ´ bn´1c¯n´1
c2
n´2 ´ c¯n´1cn´3
(4.59)
Since this solution is obtained from a truncated two-term Taylor series approximation, the
leading error is Orpδpq
2
s ` Orpδqq
2
s. If the initial guess is in close proximity to the true
solution, the method converges quadratically.
The pseudomodule, BAIRSTOW, designed to compute all roots of a polynomial, is
presented in Pseudocode 4.7. The degree of the polynomial (n), a set of initial guesses
ppp0q
, qp0q
q, an array containing the coefficients of the polynomial pai, i “ 1, 2,...,nq, an
upper bound for the maximum iterations (maxit), and a convergence tolerance (ε) are
supplied as input. Two internal arrays (b and c) are used to keep the coefficients of the
quotient polynomial (b1
k’s) and the partial derivatives (c1
k’s).
Since this algorithm is based on a polynomial in the form of Eq. (4.41), the coefficients
of the input polynomial are normalized by a0 so that a0 “ 1, i.e., ai Ð ai{a0 for i “ n,pn´
1q,..., 1, 0. The iteration loop (innermost While-loop) is used to determine the coefficients
of a quadratic factor (p and q). First, the coefficients bi’s and ci’s are calculated from Eqs.
(4.46) and (4.56). Then, the improvements δp and δq are found from Eq. (4.59), and the
current step estimates (p and q) are obtained by ppk`1q “ ppkq ` δp and qpk`1q “ qpkq ` δq.
The convergence criterion based on the �1-norm of δppq yields Δ “ |δp| ` |δq| ă ε. When
p and q converge to a set of solutions, a quadratic factor (i.e., x2 ` px ` q) is found. TheNonlinear Equations  243
roots of the quadratic equation are obtained by the module QUADRATIC giving the real
and imaginary roots on two separate arrays xre and xim. The outermost While-loop is for
repeating the procedure for another quadratic factor from the deflated polynomial, Qn´2pxq,
after setting ai Ð bi, i “ 0, 1, 2,..., i.e., Pn´2pxq Ð Qn´2pxq. Note that if n is odd, then
the last factor will be a linear factor, which yields x Ð ´a1.
EXAMPLE 4.9: Finding all roots of a polynomial
The poles of a transfer function are the roots of the following 7th-degree polynomial:
s7 ` 1.5s6 ´ 11s5 ` 15s4 ´ 76s3 ` 43.5s2 ´ 64s ` 30 “ 0
Find all real and imaginary poles of this transfer function using the Bairstow’s
method, starting with the initial values pp0q “ qp0q “ 0. For the convergence criterion,
use |δp|`|δq| ă 10´4.
SOLUTION:
The coefficients of the 7th-degree polynomial are set as a0 “ 1, a1 “ 1.5, a2 “
´11, a3 “ 15, a4 “ ´76, a5 “ 43.5, a6 “ ´64, and a7 “ 30.
1st iteration: We start with pp0q “ qp0q “ 0. The bk’s and ck’s are computed from
Eq. (4.46) and Eq. (4.56), respectively.
k ak bk ck
0 1 1 1¯c “ c6 ´ b6 “ 0
1 1.5 1.5 1.5
2 ´11 ´11 ´11 δp “ ´0.26635
3 15 15 15 δq “ 0.689655
4 ´76 ´76 ´76
5 43.5 43.5 43.5 pp1q “ pp0q ` δp “ ´0.26635
6 ´64 ´64 ´64 qp1q “ qp0q ` δq “ ´0.26635
7 30 30
2nd iteration: Continuing with pp1q “ ´0.26635, qp1q “ 0.689655 we find
k ak bk ck
0 1 1 1¯c “ c6 ´ b6 “ 38.353
1 1.5 1.76635 2.0327
2 ´11 ´11.2192 ´11.367 δp “ 0.35422
3 15 10.7936 6.364 δq “ 0.24562
4 ´76 ´65.3878 ´55.853
5 43.5 18.6401 ´0.6253 pp2q “ pp1q ` δp “ 0.08787
6 ´64 ´13.9402 24.4126 qp2q “ qp1q ` δq “ 0.93528
7 30 13.4318
3rd iteration: Restart with pp2q “ 0.08787, qp2q “ 0.93528:244  Numerical Methods for Scientists and Engineers
k ak bk ck
0 1 1 1¯c “ c6 ´ b6 “ 49.1373
1 1.5 1.41213 1.3242
2 ´11 ´12.0594 ´13.111 δp “ ´0.0862
3 15 14.7389 14.652 δq “ 0.05596
4 ´76 ´66.016 ´55.041
5 43.5 35.5159 26.648 pp3q “ pp2q ` δp “ 0.00167
6 ´64 ´5.3773 43.76 qp3q “ qp2q ` δq “ 0.99124
7 30 ´2.7447
4th iteration: Continue with pp3q “ 0.0016, qp3q “ 0.99124.
k ak bk ck
0 1 1 1¯c “ c6 ´ b6 “ 50.8005
1 1.5 1.4983 1.4967
2 ´11 ´11.994 ´12.987 δp “ ´0.00168
3 15 13.5348 12.073 δq “ 0.00873
4 ´76 ´64.134 ´51.28
5 43.5 30.1906 18.309 pp4q “ pp3q ` δp “ ´10´5
6 ´64 ´0.4784 50.322 qp4q “ qp3q ` δq “ 0.99997
7 30 0.07476
5th iteration: Continue with pp4q “ 0, qp4q “ 0.99997.
k ak bk ck
0 1 1 1¯c “ c6 ´ b6 “ 50.8005
1 1.5 1.5 1.5
2 ´11 ´12 ´13 δp “ ´1.04 ˆ 10´5
3 15 13.5 12 δq “ 3.27 ˆ 10´5
4 ´76 ´64 ´51
5 43.5 30 18 pp5q “ pp4q ` δp “ 0
6 ´64 0 51 qp5q “ qp4q ` δq “ 1
7 30 0
After five iterations, the convergence criterion yields |δp| ` |δq| “ 4.31 ˆ 10´5 ă
10´4; hence, the iterations are terminated. This yields the coefficients of the
quadratic equation with p “ 0 and q “ 1, i.e., the quadratic factor is (s2 ` 1).
The coefficients of the deflated polynomial (bk’s) are
b0 “ 1, b1 “ 1.5, b2 “ ´12, b3 “ 13.5, b4 “ ´64, b5 “ 30
Note that the degree of this polynomial is 5, and b6 and b7 representing the remainder
terms (R and S) are zero. Then, the given polynomial can be factored as
ps2 ` 1qps5 ` 1.5s4 ´ 12s3 ` 13.5s2 ´ 64s ` 30q
Next, setting ai Ð bi’s for i “ 1,2,..,5 and applying Bairstow’s algorithm once more
with pp0q “ qp0q “ 0 yieldsNonlinear Equations  245
s5 ` 1.5s4 ´ 12s3 ` 13.5s2 ´ 64s ` 30 “ ps2 ` 4.5s ´ 2.5qps3 ´ 3s2 ` 4s ´ 12q
The method is applied for the last time by setting ai Ð bi’s for i “ 1, 2, 3 and using
pp0q “ qp0q “ 0 as initial values. The procedure yields
s3 ´ 3s2 ` 4s ´ 12 “ ps2 ` 4qps ´ 3q
Finally, by putting all the quadratic factors together, we obtain
s7`1.5s6´11s5`15s4´76s3`43.5s2´64s`30 “ ps2`1qps2`4qps`5qps´3qps´0.5q
The final step is to find the roots of all quadratic or linear factors, which leads to
s1,2 “ ˘i, s3,4 “ ˘2i, s5 “ 3, s6 “ 0.5, s7 “ ´5
The closer the initial values are to the actual p and q of a quadratic factor, the faster
convergence is achieved.
Discussion: The method is straightforward; however, calculating the roots of even
low-degree polynomials by hand is quite complicated. The method works very well, as
in this example, especially when the real and imaginary roots are of the same order.
But polynomials tend to be ill-conditioned when they have a few roots that are too
large or too small. In most real-world applications where polynomials are also used
to model data, the coefficients of the polynomial are not exact because of imperfect
primary data. As a result, a small perturbation in the coefficients of the polynomial
results in large changes in the roots. It is also worth noting that this problem is not
limited to the Bairstow method alone. The ill-condition problem can be remedied
by increasing the computation precision, providing a degree of improvement, but it
becomes impossible to avoid round-off errors in high-order polynomials.
4.10 POLYNOMIAL REDUCTION AND SYNTHETIC DIVISION
So far in this chapter, we have covered common numerical methods for finding the root of a
nonlinear equation, which can be used to find the real root of a polynomial as well. However,
while finding a single root of a polynomial with a suitable method is simple, determining
the second, third, and remaining roots with different starting guesses can be somewhat
problematic as the method may yield a previously determined root. Hence, once a root is
found, we need to eliminate the possibility of ending up with the same root again.
Polynomial reduction or polynomial deflation is a technique to remove a root (say,
x “ α) from the original polynomial. To achieve this, the polynomial Pnpxq is divided by
px´αq which leads to a new polynomial Qn´1pxq of degree n´1: Pnpxq“px´αqQn´1pxq.
The new polynomial no longer has x “ α as its root unless Pnpxq has multiple roots at
x “ α.
Horner’s rule, also called synthetic division, has a variety of uses and saves work and
time when evaluating a polynomial at a certain value (x “ α).
Consider the following nth-degree polynomial:
Pnpxq “ anxn ` an´1xn´1 ` an´2xn´2 ` ... ` a1x ` a0 (4.60)246  Numerical Methods for Scientists and Engineers
Pnpxq is divided by a simple linear factor px ´ αq to give the polynomial Qn´1pxq:
Pnpxq
x ´ α “ Qn´1pxq ` R0
x ´ α (4.61)
where Qn´1pxq “ bn´1xn´1 ` bn´2xn´2 ` ... ` b1x ` b0 and R0 is the remainder. If x “ α
is a root of Pnpxq, then R0 “ 0.
Multiplying both sides of Eq. (4.61) with px ´ αq gets rid of the denominators:
Pnpxq“px ´ αq Qn´1pxq ` R0 (4.62)
Expanding the rhs of Eq. (4.62) and grouping the x-terms as well as the constants gives
anxn ` an´1xn´1 ` an´2xn´2 ` ... ` a1x ` a0 “ bn´1xn ` pbn´2 ´ cbn´1qxn´1
`pbn´3 ´ cbn´2qxn´2 ` ... ` pb0 ´ cb1qx ` R0 ´ cb0
(4.63)
For the two sides to be equal, the coefficients of the two polynomials should be equal.
Hence, by matching both sides of Eq. (4.63), the following sequence of constants is obtained:
an “ bn´1
an´1 “ bn´2 ´ α bn´1 Ñ bn´2 “ an´1 ` α bn´1
an´2 “ bn´3 ´ α bn´2 Ñ bn´3 “ an´2 ` α bn´2
.
.
. .
.
.
a1 “ b0 ´ α b1 Ñ b0 “ a1 ` α b1
R0pαq “ a0 ` α b0
(4.64)
The coefficients of Qn´1pxq can be obtained from Eq. (4.64) and the following recurrence
relationship:
bn “ 0, bk “ ak`1 ` α bk`1 for k“ pn´1q,pn´2q,..., 1, 0 (4.65)
Reducing Qn´1pxq by x ´ α, following (n ´ 2)th-degree polynomial is obtained:
Qn´1pxq
x ´ α “ Sn´2pxq ` R1
x ´ α (4.66)
where Sn´2pxq “ cn´2xn´2 ` cn´3xn´3 ` ... ` c1x ` c0 and R1 is also the remainder.
Combining Eqs. (4.62) and (4.66) yields
Pnpxq“px ´ αq
2
Sn´2pxq`px ´ αqR1 ` R0 (4.67)
The derivative of Eq. (4.67) with respect to x becomes
dPn
dx “ px ´ αq
2 dSn´2
dx ` 2px ´ αqSn´2pxq ` R1 (4.68)
Adopting the Newton-Raphson method to find a root of Pnpxq, Pnpxppq
q and P1
npxppq
q
are required to compute the successive estimates. Note that Eqs. (4.67) and (4.68) are
reduced to R0 “ Pnpαq and R1 “ P1
npαq for the true root, i.e., xppq Ñ α. Then the
Newton-Raphson iteration equation can be expressed as
xpp`1q “ xppq ´ R0pxppq
q
R1pxppqq
, p “ 0, 1, 2,... (4.69)
where as usual, subscript p denotes the iteration step, R0pxppq
q and R1pxppq
q are the re￾mainders for an estimate xppq
. Note that xppq Ñ α and R0pxppq
q Ñ 0 for p Ñ 8.Nonlinear Equations  247
Polynomial Reduction/Synthetic Division
‚ Evaluating polynomials using synthetic division is computationally
cheaper and easier since it does not require powers of x;
‚ There is no risk of getting the same root (except for a root of multi￾plicity m ě 2) since the root is factored out once it is found;
‚ The algorithm produces the coefficients of the reduced polynomial,
Qn´1pxq, as well as the remainder, R0, which provides information
on whether the root is found or not, i.e., R0 “ 0?
‚ A random, unintelligent first guess could result in a large number of
iterations;
‚ Every new root is obtained with finite accuracy, and the numerical
errors in the coefficients of the reduced polynomials could build up
so that the computed roots thereafter could become more and more
inaccurate;
‚ The order in which the roots are found can also affect the coefficients
of reduced polynomials;
‚ The method may suffer from instabilities when the roots are sensitive
to small changes in the coefficients of the polynomials;
‚ Polynomials with roots of multiplicity are also sensitive to small
changes in their coefficients.
When a polynomial is ill-conditioned, polynomial reduction can
lead to a degradation of accuracy in the coefficients of the reduced
polynomial and, subsequently, in estimated roots. To reduce the
effects of such errors, consider using the “approximate root” as
the initial guess to find another root of the original polynomial.
As we have learned, an initial guess is critical to accelerating convergence. As a strategy
to find a good estimate, it is desirable to isolate each root or determine the bounds of all
real roots, i.e.,
L ď any real root of Pnpxq ď U
The subject of root isolation is rich in studies and diversity. In this context, as a simple
approach, determining the upper or lower bounds of the roots of a polynomial will be
briefly presented here. This alternative is helpful in narrowing down the search intervals.
Estimating the bounds of a root (L,U) is followed by employing a rootfinding technique to
determine the root. Once a root is found with acceptable accuracy, it is factored out, and
the procedure is repeated for the reduced polynomial. With this approach, searching for the
roots of a polynomial is reduced in a way to finding the bounds of the roots.
Lagrange’s and Cauchy’s estimates for the upper bounds of all roots are given by
U� “ max #
1,
n
ÿ´1
k“1
ˇ
ˇ
ˇ
ˇ
ak
an
ˇ
ˇ
ˇ
ˇ
+
and Uc “ 1 ` max
0ďkďn´1
"ˇ
ˇ
ˇ
ˇ
ak
an
ˇ
ˇ
ˇ
ˇ
*
The upper bound may be taken as the smaller of the two: U “ min tU�, Ucu. The commonly
used upper and lower bounds (L ď |x| ď U) are given as follows:248  Numerical Methods for Scientists and Engineers
Pseudocode 4.8
Module POLY_ROOT (n, a, maxit, ε, nroot, roots)
\ DESCRIPTION: A pseudomodule to compute all real roots of a polynomial
\ given by Eq. (4.60) using the Horner’s rule and Newton-Raphson method.
\ USES:
\ ONE_ROOT :: Module for determining a real root;
\ BOUNDS :: Module for determining the bounds of roots of a polynomial.
Declare: a0:n, b0:n, rootsn \ Declare array variables
Declare: Logical: found \ Declare logical variable
For “
k “ 0, n‰ \ Loop to normalize the coefficients of Pn with an
ak Ð ak{an \ Algorithm works if an “ 1
End For
m Ð n \ Save the degree of polynomial for output
found Ð True \ Locate a root
r Ð 0 \ Initialize number of roots (found)
While “
found And r ă n
‰ \ Main loop to find one root
BOUNDS(m, a, Lbound, U bound) \ Find pL, Uq bounds of Pmpxq
x0 Ð ´Lbound \ Initialize lower bound
ONE_ROOT(m, x0, a, b, maxit, ε, found) \ A root exists if found=True
If “
found‰
Then \ If a root is found
r Ð r ` 1 \ Count roots found
rootsr Ð x0 \ Save root on roots
End If
m Ð m ´ 1 \ The degree of quotient
For “
k “ 0, m‰ \ Find coefficients of the reduced polynomial
ak Ð bk \ Assing coefficients of Qm´1 to a
End For
End While
nroot Ð r \ Total number of roots = found
End Module POLY_ROOT
Module ONE_ROOT (n, x0, a, b, maxit, ε, found)
\ DESCRIPTION: A pseudomodule to compute a real roots of an nth degree
\ polynomial applying synthetic division to Pnpxq twice in a row and finds
\ the coefficients of the reduced polynomials and remainders.
\ USES:
\ ABS:: A built-in function computing the absolute value.
Declare: a0:n, b0:n \ Declare array variables
Declare: Logical: found \ Declare logical variable
p Ð 0; δ0 Ð 1 \ Initialize iteration counter and absolute error
found Ð True \ Initialize logical variable
kount Ð 0 \ Initialize counter for divergence decision
While “
δ0 ą ε And p ď maxit‰
p Ð p ` 1 \ Count iterations
bn Ð 0 \ Synthetic division of Pnpxq{px ´ xppq
q
For “
k “ pn ´ 1q, 0,p´1q
‰ \ Loop k to sweep index k backwards
bk Ð ak`1 ` x0 ˚ bk`1 \ Find coefficients of Qn´1pxq
End For
R0 Ð a0 ` x0 ˚ b0 \ Find remainderNonlinear Equations  249
m Ð n ´ 1 \ Degree of reduced polynomial, Qn´1pxq
cm Ð 0 \ Apply synthetic division to Qmpxq
For “
k “ pm ´ 1q, 0,p´1q
‰ \ Loop k to sweep index k backwards
ck Ð bk`1 ` x0 ˚ ck`1 \ Find coefficients of Sn´2pxq
End For
R1 Ð b0 ` x0 ˚ c0 \ Remainder Qmpxq“px ´ xppq
qSm´1pxq ` R1
δ Ð R0{R1 \ Compute δppq Ð Pnpxppq
q{P1
npxppq
q
x0 Ð x0 ´ δ \ Use Newton-Raphson, xpp`1q Ð xppq ´ δppq
\ Check to see if subsequent displacements are increasing (diverging)
If “
|δ| ą δ0
‰
Then \ Current error is larger than prior
kount Ð kount ` 1; δ0 Ð |δ| \ Count occurences
End If
If “
kount “ 10‰
Then \ if |δ| ą δ0 occurence is repeated 10 times
Write: “Diverging sequence is obtained” \ Issue a warning
found Ð False \ Root not found, perhaps imaginary pairs
Exit \ Exit the module
End If
δ0 Ð |δ| \ Set current displacement as prior, δppq Ð ˇ
ˇδpp`1q
ˇ
ˇ
End While
End Module ONE_ROOT
Module BOUNDS (n, a, Lbound, U bound)
\ DESCRIPTION: A pseudomodule to estimate lower and upper bounds of the
\ roots of an nth degree polynomial: Lbound ă |roots of Pnpxq| ă U bound.
\ USES:
\ MAX:: A built-in function returning the maximum of the agrument list.
Declare: a0:n
Lbound Ð 0; U bound Ð 0 \ Initialize Lbound and U bound
For “
k “ 1, n‰
Lbound Ð MAXpLbound, |ak|
1{k
q \ L Ð maxpL, |ak|
1{k
q
U bound Ð MAXpU bound, |an´k|
1{k
q \ U Ð maxpU, |an´k|
1{k
q
End For
Lbound Ð 0.5{Lbound; U bound Ð 2 ˚ U bound \ Apply Eq. (4.70)
End Module BOUNDS
L “
˜
2 max
1ďkďn
# ˇ
ˇ
ˇ
ˇ
ak
a0
ˇ
ˇ
ˇ
ˇ
1{k
+¸´1
, U “ 2 max
1ďkďn
# ˇ
ˇ
ˇ
ˇ
an´k
an
ˇ
ˇ
ˇ
ˇ
1{k
+
(4.70)
More elaborate approximations for the upper/lower bounds exist and can be found in the
literature, some of which have been summarized in Refs. [110, 111].
Pseudocode 4.8 is designed to find all real roots of an nth-degree polynomial us￾ing the synthetic division coupled with the Newton-Raphson method. The main module,
POLY_ROOT, requires the coefficients of the nth-degree polynomial Pnpxq (ai, i “ 0, 1, 2,
..., n), an upper bound for the maximum number of iterations (maxit), and a convergence
tolerance (ε) as input. The outputs are the total number of roots found (nroot) and an
array containing the roots (rootsi, i “ 0, 1, 2, . . . , nroot). The coefficients of the reduced
polynomial, Qn´1pxq, are stored on an internal array, bi for i “ 0, 1, 2, ..., n ´ 1. The
algorithm is based on the normalized polynomial, i.e., ai Ð ai{an for i “ 0, 1, ..., n. The250  Numerical Methods for Scientists and Engineers
module used the module BOUNDS to determine the global bounds that contain all roots of
Pnpxq. The initial estimate is set to xp0q “ L as priority is given to finding the smallest roots
as they are most affected by rounding. The module calls ONE_ROOT to obtain a root and
the coefficients of the reduced polynomial. If a (real) root exists, then module ONE_ROOT
will return an estimate for the root (x0), and the logical variable found is set to True. Once
a root is found, the counter (r) is incremented by one, and the root found (x0) is written
to the solution array, roots. The reduced polynomial is then set as Pn´1pxq, i.e., ai Ð bi,
i “ 0, 1, 2, ... ,(n ´ 1), and the above procedure is repeated for the reduced polynomial. If
no root is found, then found is set to f alse, and the While-loop is terminated.
The pseudomodule, ONE_ROOT, is designed to find a real root of an nth-degree poly￾nomial using the Newton-Raphson method. The degree (n) and the coefficients (ai, i “ 0,
1, 2, ..., n) of the polynomial, the initial guess for the root (x0), an upper bound for the
maximum number of iterations (maxit), and a convergence tolerance (ε) are the inputs. If
a root exists, found is set to true and the estimated root (x0) is returned as output along
with the array b containing the coefficients of the reduced polynomial. With x0 and ak’s,
the first synthetic division yields bk’s and Pnpx0q “ R0. Applied to Qn´1pxq, the second
synthetic division with x0 gives Sn´2pxq (i.e., ck’s) and R1 corresponding to P1
npx0q “ R1.
Using δ “ R0px0q{R1px0q, the current (improved) estimate is computed from Eq. (4.69).
The While-loop, in which improved estimates are computed, is terminated when |δ| ă ε is
met. If Pnpxq or any one of its subsequent reduced polynomials does not have a real root,
then |δppq
| will either diverge or oscillate; hence, |δpp`1q
| ą |δppq
| occurrences are counted by
kount to determine if the iterations stabilize or diverge. If kount “ 10, then found is set
to f alse and the procedure is terminated.
The pseudomodule, BOUNDS, is used to estimate the lower and upper bounds of an
nth-degree polynomial, Pnpxq. The degree (n) and the coefficients (ai, i “ 0, 1, 2, ..., n) of
the polynomial are the inputs of the module. On exit, the module provides the lower bound
(Lbound) and upper bound (U bound) as output. The module determines the bounds using
the criteria provided in Eq. (4.70).
EXAMPLE 4.10: Application of polynomial reduction technique
Find all the real zeros of P4pxq “ x4`1.5x3`3x2`6x´4 using the Newton-Raphson
method coupled with polynomial reduction. Use ε “ 10´6 as convergence tolerance
and xp0q “ 0 as the initial guess.
SOLUTION:
The coefficients of the polynomial are: a4 “ 1, a3 “ 1.5, a2 “ 3, a1 “ 6, and
a0 “ ´4. Noting that xp0q “ 0 and applying the sequence given by Eq. (4.65) to
P4pxq and Q3pxq we get:
b4 “ 0, c3 “ 0,
b3 “ a4 “ 1 c2 “ b3 “ 1,
b2 “ a3 ` p0qb3 “ 1.5 ` 0p1q “ 1.5, c1 “ b2 ` p0qc2 “ 1.5 ` 0p1q “ 1.5,
b1 “ a2 ` p0q b2 “ 3.0 ` 0p1.5q “ 3, c0 “ b1 ` p0q c1 “ 3.0 ` 0p1.5q “ 1,
b0 “ a1 ` p0q b1 “ 6.0 ` 0p1q “ 6, c0 “ b1 ` p0q c1 “ 3.0 ` 0p1.5q “ 1,
We can determine the remainders from P4pxq and Q3pxq as follows:Nonlinear Equations  251
R0 “ a0 ` p0q b0 “ ´4 ` 0p6q“´4, and
R1 “ b0 ` p0q c0 “ 6 ` 0p1q “ 6.
At the end of the first iteration, we obtain
xp1q “ xp0q ´ R0pxp0qq
R1pxp0qq “ 0 ´ p´4q
6 “ 2
3
When this procedure is repeated for the new estimate xp1q and check for convergence
(i.e., |xppq ´ xpp´1q
| ă ε) each time a new estimate is found. The first root (x “ 0.5)
converges after 4 iterations. The iteration details are:
Iteration # 1, xp1q “ 2{3 Iteration # 2, xp2q “ 0.516854
k ak bk ck k ak bk ck
4 100 4 1 00
3 1.5 1 0 3 1.5 1 0
2 3 1.5 1 2 3 2.1667 1
1 6 3 1.5 1 6 4.4444 2.83333
0 -4 6 3 0 -4 8.963 6.33333
R0 “ ´4 R1 “ 6 R0 “ 1.9753 R1 “ 13.1852
Iteration # 3, xp1q “ 0.5001797 Iteration # 4, xp2q “ 0.5
k ak bk ck k ak bk ck
4 1 00 4 1 00
3 1.5 1 0 3 1.5 1 0
2 3 2.0169 1 2 3 2.0002 1
1 6 4.0424 2.5337 1 6 4.0004 2.5004
0 ´4 8.0893 5.352 0 ´4 8.0009 5.2511
R0 “ 0.181 R1 “ 10.8555 R0 “ 0.0019 R1 “ 10.6274
The first step yields a third-degree polynomial: Q3pxq “ x3 ` 1.5x2 ` 3x ` 6. In
the second step we proceed to find the root of Q3pxq with the initial guess, xp0q “ 0.
The second root converges to the zero of x “ 2 after six iterations.
Iteration # 1, xp1q “ 2{3 Iteration # 2, xp2q “ 0.516854
k ak bk ck k ak bk ck
3 1 00 3 1 00
2 2 10 2 2 10
1 4 2.5 1 1 4 0.9259 1
0 8 5.25 3 0 8 3.0055 ´0.1481
R0 “ 10.625 R1 “ 6.75 R0 “ 4.7719 R1 “ 3.1646
Iteration # 3, xp3q “ ´2.127923 Iteration # 4, xp4q “ ´2.007676
k ak bk ck k ak bk ck
3 1 00 3 1 00
2 2 10 2 2 10
1 4 ´0.582 1 1 4 ´0.1279 1
0 8 5.5026 ´3.1639 0 8 4.2722 ´2.2558
R0 “ ´6.2076 R1 “ 13.6718 R0 “ ´1.0909 R1 “ 9.0725252  Numerical Methods for Scientists and Engineers
Iteration # 5, xp3q “ ´2.000029 Iteration # 6, xp4q “ ´2
k ak bk ck k ak bk ck
3 1 00 3 100
2 2 10 2 210
1 4 ´0.0077 1 1 4 ´0.1279 1
0 8 4.0154 ´2.0154 0 8 4.2722 ´2.2558
R0 “ ´0.0616 R1 “ 8.0616 R0 “ 0 R1 “ 8
Finally the polynomial is factored as
x4 ` 1.5x3 ` 3x2 ` 6x ´ 4 “ px ´ 0.5qpx ` 2qpx2 ` 4q “ 0
Note that the quadratic multiplier has imaginary roots.
Discussion: The synthetic division (or deflation) algorithm is a simple process ap￾plied to an array of polynomial coefficients. With this method, there is no need
to determine the upper and lower bounds of the roots. The computational effort
that goes into searching for all or several roots of a polynomial can be significantly
reduced. More importantly, we can avoid encountering the problem of the method
converging to a non-repeating root multiple times.
4.11 CLOSURE
A root of a nonlinear equation can be obtained using the methods or techniques presented in
this chapter. All root-finding algorithms require iterative procedures. Fewer iterations imply
fewer floating-point operations and faster convergence. The choice of a method depends on
the nonlinear equation whose root is to be determined.
Bracketing methods such as bisection or false-position methods are suitable for search￾ing the roots of a nonlinear equation within a specified interval. These methods converge
rather slowly and require a clever estimation of the lower and upper bounds of the initial
interval in which a root lies. These methods have the lowest convergence rate, and it may be
difficult to find the desired root if the interval contains several roots. With the bracketing
methods, convergence is guaranteed since a root is confined within a closed interval that
is constantly narrowed down until its width is less than a predefined tolerance value. The
false position method usually converges more rapidly than the bisection method.
Open-domain methods such as Newton-Raphson, Secant, and so on are normally pre￾ferred because they converge to a solution much more rapidly. Newton-Raphson or modified
Newton methods with a quadratic convergence rate require an initial estimate (instead of
two) and converge to a root faster than other bracketing methods. The initial guess can be
chosen randomly; generally, the closer the initial guess is to the root, the fewer iterations are
required for the accuracy desired in the estimates. If a non-linear equation has more than
one root, all roots can be found with the Newton-Raphson method by varying the initial
guess over a wide range. If the derivatives of fpxq are long, complex, or require too many
arithmetic operations, the Newton-Raphson methods may not be computationally favorable.
In such cases, the Secant or modified Secant methods (having a convergence rate of 1.62),
which do not require the derivative of the nonlinear equation, could be a suitable alternative
that leads to an estimate as fast and effective as next to the Newton-Raphson methods.
But the open-domain methods may diverge since the new estimates are not bracketed.Nonlinear Equations  253
Aitken’s and Steffensen’s methods serve to accelerate the speed of the convergence of
slowly converging sequences. Steffensen’s method provides quadratic convergence, but the
function fpxq must be three times continuously differentiable in order to ensure quadratic
convergence.
Many failures that may be encountered when using the root-finding algorithms can be
traced back to a poor initial guess. Instead of making random estimates, it is possible in
engineering applications to make a smart estimate by simply interjecting the physics of the
problem. This reduces not only the computational effort but also the potential for round-off
accumulation due to possibly numerous iterations.
The real roots of a polynomial can be determined with either bisection, Newton￾Raphson, secant, or other methods by specifying the initial guess or interval. But these
methods are clearly not equipped to handle finding the imaginary roots. The Bairstow
method with a quadratic convergence rate is fast and very effective for finding all real and
imaginary roots of a polynomial, and it is suitable for most applications. Polynomial reduc￾tion, coupled with bisection, Newton-Raphson, etc. methods, can also be effectively used
to obtain all the real roots of a polynomial. However, polynomials that are ill-conditioned
require high-precision computation, so avoid the accumulation of round-off errors that may
lead to many inaccurate estimates.
Newton’s method is extended to solve a system of nonlinear equations. The method
converges quickly with a good set of initial guesses. The computation of the Jacobian that is
done at each iteration can be done numerically or analytically; however, it may be difficult
to find analytical expressions of the Jacobian for some problems.
4.12 EXERCISES
Section 4.1 Bisection Method
E4.1 Apply the bisection method for the given search intervals to estimate the following roots
accurate to two decimal places. Hint: Assume fpxq “ xn ´ a “ 0.
paq
?
15.1492 r3, 4s, pbq
?3 15.1492, r2, 3s, pcq
?4 15.1492 r1.5, 2.5s
E4.2 For each of the following nonlinear equations, (a) verify that the nonlinear equation has a
root in the specified interval, and (b) estimate the number of bisections that would be required
to locate the root to an accuracy of ε “ 10´3.
(a) p3x ´ 2q ln x “ x ` 1, (i) (0.1, 1), (ii) (1, 3)
(b) x2 ´ 3x ln x “ sin πx, (i) (1, 3), (ii) (3, 6)
(c) x cos x ` x2 sin x ` 3 “ ?x ` 1, (i) (2.6, 3.6), (ii) (11, 14)
(d) cos 2x cosh x “ 2, (i) (2, 3), (ii) (3, 4)
E4.3 Estimate the roots of the nonlinear equations in E4.2 using the bisection method to an
accuracy of ε “ 10´3.
E4.4 Given fpxq “ x4 ´ 3.7x3 ´ 1.5x2 ` 14.4x ´ 10.8 “ 0 has four real roots in (´3,4). Propose
search intervals enclosing each root.
E4.5 Apply five bisections to E4.4 to estimate a root in (´1, 2). How accurate is your estimate?
E4.6 Do the curves y1 “ cospx{3q and y2 “ sinp4x{5q intersect in (0, π{2)? If yes, find the
intersection point accurate to two decimal places (ε “ 0.5 ˆ 10´2).
E4.7 Given fpxq “ x sinpπxq´1 has two roots near x « 2.5, use the bisection method with initial
intervals of (1.5, 2.5) and (2.5, 3.5) to estimate the roots, accurate to three decimal places.254  Numerical Methods for Scientists and Engineers
E4.8 The nonlinear equation e´ax “ sinpωxq is encountered in the design of a planetary gear
system in an automatic transmission. Consider the case of a “ 1 and ω “ π{4, which has a root in
(0,1). Apply the bisection method to estimate the root accurately to at least four decimal places.
E4.9 Given fpxq “ 6x´2x3 ´3 sinp2xq´1 has a local maximum point in (1, 2). Use the bisection
method to estimate the maximum point, accurate to two decimal places.
Section 4.2 Method of False Position
E4.10 Repeat E4.1 with the method of false position.
E4.11 Repeat E4.2 with the method of false position.
E4.12 Repeat E4.5 with the method of false position.
E4.13 Repeat E4.6 with the method of false position.
E4.14 Repeat E4.7 with the method of false position.
E4.15 Repeat E4.8 with the method of false position.
E4.16 Repeat E4.9 with the method of false position.
E4.17 The following polynomial has a root in (0,1): fpxq “ x3 ´x2{2`x{12´1{2216 “ 0. (a) use
the method of false position to estimate the root, accurate to three decimal places; (b) compare
your estimate with the true root, given that the polynomial is fpxq“px ´ 1{6q
3; (c) How would
you explain the difference between the estimated and the true root?
E4.18 Use the method of false position to estimate the root of the following equation in (0, π{3)
to an accuracy of ε “ 5 ˆ 10´4.
tan ´π
5 ´ x
¯
` tan ´x
5
¯
tan ´
x ` π
10
¯
´ x “ 0
E4.19 Use the method of False Position to estimate the root of the following equation in (0, π{3)
to an accuracy of ε “ 10´4.
2x ` sinp2xq ´ cosp3xq “ 3
E4.20 The maximum solubility of graphite in liquid Fe (in wt% C) as a function of temperature
(in ˝C) is correlated with Tpxq “ 2420?x ´ 80.1x4{3 ´ 3258.35. Using this correlation, estimate
the maximum graphite solubility at 1600˝C and 2500˝C accurately to one decimal place, using
the method of false position. Note: The solubility is expected to be within p4, 10q wt%.
E4.21 The following nonlinear equation is encountered in the design of helical gears:
tan θ ´ θ “ K
where θ denotes pressure angle and K is a constant. For K “ 0.005, use the method of false
position to estimate θ within p0.2, 0.5q accurately to four decimal places.
Section 4.3 Fixed-Point Iteration
E4.22 Repeat E4.8 using the fixed-point iteration with xp0q “ 1 as the initial guess.
E4.23 Repeat E4.18 with the method of fixed-point iteration method using xp0q “ 0.5.
E4.24 Estimate the root of e´3x “ 2x ´ 1, using the fixed-point iteration method with xp0q “ 0
and ε “ 5 ˆ 10´4.
E4.25 The point of intersection of y “ ?6x ` 5 and y “ x2 ´ 4x is to be determined. The two
possible fixed-point iteration functions are:
piq gpxq “ 1
4
px2 ´ ?5 ` 6xq, piiq gpxq “ b
4x ` ?5 ` 6x
(a) Find the intersection point using the fixed-point iteration method with xp0q “ 0 and ε “
5ˆ10´4 (also compute g1
pxq at each iteration step); (b) which iteration function converged faster?
why?Nonlinear Equations  255
E4.26 There is a root of the equation x7´2x´1 “ 0 in the interval (1,2). Can we apply the fixed￾point iteration method to estimate its root using the following two iteration functions? Explain
why.
piq gpxq “ 1
2
px7 ´ 1q, piiq gpxq “ ?7 1 ` 2x
E4.27 For the iteration functions given below, (a) determine which ones will converge to a fixed￾point, and (b) find the convergence rate for the convergent cases.
(a) gpxq “ 3x
4 `
1
8x3, (b) gpxq “
1
3x ` 4
, (c) gpxq “
1
3
sin x `
1
4x
E4.28 Repeat E4.19 with the method of fixed-point iteration. An iteration function is given as
xpn`1q “ 1
2
´
3 ´ sinp2xpnq
q ` cosp3xpnq
q
¯
(a) Compute the first five iterations starting with xp0q “ 0.7 and explain the convergence trend;
(b) Repeat Part (a) by modifying the iteration equation by adding x to both sides of the iteration
function to obtain:
xpn`1q “ 1
2
xpnq `
1
4
´
3 ´ sinp2xpnq
q ` cosp3xpnq
q
¯
E4.29 Given that fpxq “ 12x3 ´ 23x2 ´ 37x ` 70 has a root in the neighborhood of 1.5, (a)
which one of the following iteration functions will converge to the root? (b) Using the convergent
iteration functions, estimate the root to an accuracy of ε “ 5 ˆ 10´4 and calculate the linear
convergence rate.
(a) gpxq“ 70
37 ´ 23x2
37 `
12x3
37 , (b) gpxq“
?70´37x ` 12x3
?23 , (c) gpxq“ 23
12 ´ 35
6x2 `
37
12x
E4.30 Repeat E4.21 using the fixed-point iteration method with θp0q “ 0.3.
E4.31 A relationship between voltage and current flowing through a solar cell under the maximum
power condition is given by the following equation:
Vm
Vt
“ ln „ˆ1 `
Iph
Is
˙
{
ˆ
1 `
Vm
Vt
˙j
where Is is the saturation current of the diode (=2.5 ˆ10´12 A), Iph is the photo current (=25
mA), Vt is the thermal voltage (=28 mV), and Vm is the voltage corresponding to the maximum
power. Apply the fixed-point iteration method to find the maximum voltage, accurate to two
decimal places. Use pVm{Vtq
p0q “ 10 V for the initial guess.
E4.32 The stiffness (k) of two plates fastened by a bolted joint is defined by
k “ pπ{2qEd tan α{ln ˜
p� tan α ` dw ´ dqpdw ` dq
p� tan α ` dw ` dqpdw ´ dq
¸
where E is the Young’s modulus, d is the diameter of the bolt, dw is the washer face diameter, and
� is the plate thickness. Assuming dw “ 1.5d, E “ 180 GPa, � “ 0.15 m, and tan α “ 0.6, apply
the fixed-point iteration method to estimate to three decimal place accuracy the bolt diameter
(d) corresponding to k “ 8.623 ˆ 109 N/m. Use dp0q “ 0.02 m.
Section 4.4 Newton-Raphson Method
E4.33 Repeat E4.1 using the Newton-Raphson method with xp0q “ 1.
E4.34 Repeat E4.3 using the Newton-Raphson method. Use the midpoint as an initial guess.
E4.35 Repeat E4.8 using the Newton-Raphson method. Use xp0q “ 0 as the initial guess.256  Numerical Methods for Scientists and Engineers
E4.36 For each of the following nonlinear equations with specified initial values, apply the
Newton-Raphson method to estimate the root, accurate to three decimal places.
(a) x ` 1 “ 2e´x, xp0q “ 0 (b) p3 ´ xqe´x “ 1, xp0q “ 0
(c) 2x ´ lnpx ` 2q “ e´x, xp0q “ 0.2 (d) x lnp1 ` xq “ sin 2x, xp0q “ 0.8
(e) x2 ´ 3x ` 1 “ x sin x, xp0q “ 1 (f) x ln x p2 ` ln xq ´ 3 ` x “ 0, xp0q “1
(g) x ¨ 4x ´ 80 “ 0, xp0q “ 2 (h) x ¨ 4x ´ 4x2 “ 3 , xp0q “ 1.3
(i) 2x
`
1 ´ 3x2 ´ 2x
˘
ln x ` 3x “ x2 ´ 5, xp0q “1.2
E4.37 Repeat E4.20 with the Newton-Raphson method for xp0q “ 4 and ε “ 10´2.
E4.38 Repeat E4.21 with the Newton-Raphson method for xp0q “ 0.2 and ε “ 5 ˆ 10´4.
E4.39 The nonlinear equation sin θ “ α θ is encountered in physics when studying Fraunhofer
diffraction. Estimate the root of the nonlinear equation for α “ a2{3, accurate to four decimal
places, using the Newton-Raphson method with θp0q “ 1.
E4.40 The following equation describes the transient current of a circuit:
iptq “ 2e
´2t sin ˆπt
3
˙
` 3 sin ´
2t ´ π
3
¯
Use the Newton-Raphson method to estimate (a) the moment when the current becomes zero
and (b) the moment in the neighborhood of t
p0q “ 1.6, when the current is at its maximum or
minimum. The convergence tolerance is given as ε “ 5 ˆ 10´4.
E4.41 The spectral emissive power of a black body is given by
Epλ, Tq “ C1
λ5pexppC2{λTq ´ 1q
where λ is the wavelength (μm), T is the temperature of the blackbody (K), C1 and C2 are
constants defined respectively as 3.742 ˆ 108 W.μ m4/m2 and 1.439ˆ104 μm.K. Use the Newton￾Raphson method to estimate the wavelength corresponding to the maximum emissive power within
an absolute error tolerance of ε “ 10´3. Hint: Defining x “ C2{λT, the spectral emissive power
can be written as Epxq “ C x5{pex ´1q, where C “ C1T 5{C5
2 , which is also constant. Use xp0q “ 5
for the initial guess.
E4.42 Water flows in an open, rectangular channel of constant width (w), as shown in Fig. E4.42.
There is a small ramp (height δ) downstream. The Bernoulli equation, which relates the pressure,
velocity, and height of fluid at two points (before and after the ramp), and conservation of mass
(Q “ V0h0w “ V1h1w) yield the following equation:
Q2
2gw ˆ 1
h2
0
´ 1
h2
˙
´ ph ´ h0q ´ δ “ 0
where w is the width of the channel and δ is the height of the
ramp, g is the acceleration of gravity, h0 and h are the water
levels, and V0 and V1 are the velocities at the elevations z0
and z1. For an open channel with Q “ 0.4 m3/s, w “ 0.8 m,
h0 “ 0.5 m, δ “ 0.07 m, estimate the elevation of the water
surface downstream of the ramp, i.e., h`δ “? Use ε “ 10´3
and hp0q “ h0 for the initial guess. Fig. E4.42
E4.43 The pressure (P), volume (V ), and temperature (T) of a real gas are related to each other
through the van der Waals equation:
ˆ
P ` n2a
V 2
˙
pV ´ nbq “ nRT
where R “ 0.08206 atm-L/mol-K, a and b are constants, and n is the molarity of the gas. TheNonlinear Equations  257
pressure of 3 mol of BCl3 (boron trichloride) gas (a “ 15.39 atm.L2/mol2, b “ 0.1222 L/mol)
is 1.15 atm at 310 K. Using the absolute error tolerance of ε “ 10´3 and the initial guess of
V p0q “ 20 L, estimate the volume of BCl3 by applying the Newton-Raphson method. Compare
your estimate with that obtained from the ideal gas law.
E4.44 The kinematic equations of a crank mechanism il￾lustrated in Fig. E4.44 are given as
�1 cos θ ` �2 cos α ´ L “ 0, �1 sin θ ` �2 sin α “ 0
For L “ 0.625 m, �1 “ 18 cm, �2 “ 54 cm, estimate α
and θ accurately to two decimal places by applying the
Newton-Raphson method. Use θp0q “ 60˝. Hint: Reduce
the equations into a nonlinear equation. Fig. E4.44
E4.45 The current-voltage relationship of a semiconductor diode is
given by Shockley’s diode equation: I “ IspexppV {nVT q ´1q, where
n is called the ideality factor, which can be assumed to be unity,
Is is the saturation current (A), V is the applied voltage (V), VT
is the thermal voltage. Consider a diode connected in series with a
resistor (R “ 1.5 kΩ) and a battery with E “ 9 V as shown in Fig.
E4.45. The applied voltage for the closed circuit can be written as Fig. E4.45
E “ IR ` V “ IsR
´
e
V {VT ´ 1
¯
` V
Using Is “ 10´8 A and VT “ 0.026 V, estimate the applied voltage across the diode to three
decimal places by applying the Newton-Raphson method. Use V p0q “ 0.4 as the initial guess.
E4.46 The Gibbs free energy of one mole of a gas mixture in the temperature range above 200
K is given as
GpTq“´RT "
ln ˆ A T 7{2
1 ´ e´B{T
˙
`
C
T
*
where R “ 8.314 J/K, A “ 4.3 ˆ 10´4K´7{2, B “ 6000 K and C “ 52000 K are the gas constants
for H2. Apply the Newton-Raphson method to estimate the temperature (accurate to one decimal
place) that yields GpTq“´5.19 ˆ 105 J. Use Tp0q “ 260 for the initial guess.
E4.47 The internal effectiveness factor for a first-order reaction (AÑB) in a spherical catalyst
pellet is given by
η “ 3
φ2 pφ coth φ ´ 1q
where φ is the Thiele modulus of the first-order reaction defined as φ2 “ k1R2{De, where R is the
pellet radius, k1 is the rate constant, and De is the diffusion coefficient. The magnitude of the
effectiveness factor (0 ď η ď 1) indicates the relative importance of diffusion and reaction. Apply
the Newton-Raphson method to estimate the Thiele modulus of a first-order reaction, accurate
to four decimal places, that will achieve the effectiveness factor η “ 0.875. Use φp0q “ 2 for the
initial guess.
E4.48 The head loss in pipes may be described by H “ KQ1.75, where Q is the flow rate (m3/s)
and K is constant depending on the pipe properties. A water distribution system with two inlets
and two outlets, as well as the flow rates and pipe constants, are given in Fig. E4.48. We obtain
four equations from the continuity of flow at junctions; ➊, ➋, ➌, and ➍; that is,
At junction ➊, 0.20 “ Q1 ` Q2 At junction ➋, Q2 “ 0.10 ` Q4
At junction ➌, Q1 ` 0.05 “ Q3 At junction ➍, Q3 ` Q4 “ 0.15258  Numerical Methods for Scientists and Engineers
We have an additional equation from the head losses, which must be zero for the loop:
K2Q1.75 2 ` K4Q1.75 4 ´ K3Q1.75 3 ´ K1Q1.75 1 “ 0
The above equations constitute a system of nonlinear equations with five equations and four
unknowns. Using the available equations, reduce this system to a single nonlinear equation with
one variable. Then solve the resulting equation with the Newton-Raphson method to estimate the
flow rates (accurate to four decimal places) at each pipe section. Use Qp0q
k “ 0 for the initial guess.
Fig. E4.48
E4.49 A typical normalized voltage waveform is depicted in Fig. E4.49. Such pulses are used in
electrical engineering to characterize voltage surges on power lines and on circuits. For pulses, a
series of definitions (t10, t90, tm) are specified by IEEE Standards C62.41-2002. t50 is a parameter
for measuring the width of the pulse. The rise time of a waveform is specified as the time taken
for the waveform to go from 10% to 90% of its maximum value. The time for the peak is not
specified, but it is clearly the point where the derivative of V ptq is zero. A waveform is given as
V ptq “ 1.44576 e
´0.01t2
p1 ´ e
´0.415t
q
Use the Newton-Raphson method to determine t10, t90, tm and t50, accurate to four decimal
places.
Fig. E4.49
Section 4.5 Modified Newton-Raphson Method
E4.50 Use the Newton-Raphson and modified Newton-Raphson methods with xp0q “ 1 to esti￾mate the root of the following function with an absolute error less than ε “ 10´5. Comment on
the convergence of both methods.
fpxq “ x2 ` e
´2x ´ 2xe´x
E4.51 Use the Newton-Raphson and modified Newton-Raphson methods with xp0q “ 3 to esti￾mate the root of the following function with an absolute error less than ε “ 10´5. Comment onNonlinear Equations  259
the convergence of both methods.
fpxq “ 6 ` x2 ´ 4x sin ´πx
4
¯
´ 2 cospπxq
E4.52 Use the modified Newton-Raphson method to estimate the roots of the following polyno￾mials with the given initial guesses within the absolute error tolerance of ε “ 10´5.
paq 25x5 ` 5x4 ´ 111x3 ` 157x2 ´ 322x ` 294 “ 0, xp0q “ 1
pbq x5 ´ 8.1x4 ` 22.87x3 ´ 27.783x2 ` 21.87x ´ 19.683 “ 0, xp0q “ 1.5
pcq x5 ´ 1.6x4 ´ 1.88x3 ` 3.312x2 ` 0.864x ´ 1.728 “ 0, xp0q “ 0.25 and xp0q “ ´0.5
E4.53 Apply the modified Newton-Raphson method with xp0q “ 1 to estimate the root of the
following equation to an accuracy of ε “ 10´5. Is it a root with multiplicity? If yes, determine the
degree of multiplicity.
fpxq“´1 ` 4x ´ 4x2 ´ 2 ln x ` 8x2 ln x ` p2x ´ 1q ln 16
E4.54 Apply the modified Newton-Raphson method with xp0q “ 1.5 to estimate the root of
4x ´ 4x3{2 ` x2 “ 0 to an accuracy of ε “ 10´5. Is it a root with multiplicity? If yes, determine
the degree of multiplicity.
E4.55 Apply the modified Newton-Raphson method with xp0q “ 3 to estimate the root of 2 `
cos 2x ´ sin x “ 0 to an accuracy of ε “ 10´5. Is it a root with multiplicity? If yes, determine the
degree of multiplicity.
E4.56 The dynamic properties of a machine may be obtained from the natural frequency ω0 (in
an undamped state) and damping ratio D. A factor α, which constitutes the cutting parameters,
is proportional to cutting depth. When modeling the radial motion between a workpiece and a
tool, the following nonlinear equation is encountered:
λ2 ` 2D λ ` 1 ` α
´
1 ´ e
´ω0T0λ
¯
“ 0
Estimate the root of this equation for the cases given below by applying the modified Newton￾Raphson method with λp0q “ 1 and ε “ 10´4.
(a) D “ 0.08, α “ 0.26, ω0T0 “ ´1.90, (b) D “ 0.10, α “ 1.52, ω0T0 “ ´1.44
Section 4.6 Secant and Modified Secant Methods
E4.57 Repeat E4.8 using (a) the Secant method with xp0q “ 0 and xp1q “ 0.1 as the starting
values, and (b) the modified Secant method with h “ 0.05 and xp0q “ 0.
E4.58 The quality of the initial guess (or starting values) could result in divergence, The quality
of the initial guess (or starting values) could result in divergence, slow convergence, or fast con￾vergence. To investigate the effect of the starting values on the Secant method, estimate the root
of fpxq “ 6x ´ 2x3 ´ 3 sinp2xq ´ 1 within ε “ 10´6 using the following starting values:
paq xp0q “ ´1, xp2q “ 2, pbq xp0q “ 0, xp2q “ 2, pcq xp0q “ 0, xp2q “ 1,
pdq xp0q “ 1, xp2q “ 1.1, peq xp0q “ 2, xp2q “ 3, pfq xp0q “ 1, xp2q “ 2
E4.59 Use the Secant method with t
p0q “ 0.5 and t
p1q “ 1 to estimate the maximum of the given
function with an absolute error tolerance less than ε “ 10´5.
fptq “ 3e
´1.5t sin 3t ` 0.3 sin 4t
E4.60 Repeat E4.59 using the Secant method with t
p0q “ 0.1, t
p1q “ 0.2 and ε “ 10´5.
E4.61 Repeat E4.21 using the Secant method with θp0q “ 0.3, θp1q “ 0.4 and ε “ 10´5.260  Numerical Methods for Scientists and Engineers
E4.62 The amount of money required to pay off a mortgage over a fixed period of time is calcu￾lated from the ordinary annuity equation:
P “ A ip1 ` iq
n
p1 ` iq
n ´ 1
where A is the amount of the mortgage, P is the amount of each payment, and i is the interest
rate per period for the n payment periods. Suppose that a 5-year home mortgage in the amount
of $100,000 is needed and the borrower can afford house payments of at most $1750 per month.
Use the secant method with i
p0q “ 0.005, i
p0q “ 0.01 and ε “ 10´4 to estimate the maximum
interest rate that the borrower can afford to pay.
E4.63 The revised van der Waals equation of state, also known as the Redlich–Kwong equation of
state, is an empirical algebraic equation that relates the volume (V ), pressure (P), and temperature
(T) of gases. Generally, it is more accurate than the van der Waals equation and the ideal gas
equation at temperatures above the critical temperature. Redlich–Kwong proposed the following:
ˆ
P ` a ?T V pV ` bq
˙
pV ´ bq “ RT
where R “ 0.08206 atm-L/mol-K is the universal gas constant, a and b are related to the critical
properties (Tc, Pc) of the substance, and they are defined as
a “ 0.42747R2
T 2.5 c {Pc and b “ 0.08664RTc{Pc
Estimate the volume of 1 mole of boron trichloride gas at 310 K and 1.15 atm with the Secant
method. Use Tc “ 455 K, Pc “ 39.46 atm, V p0q “ 10 L, V p1q “ 15 L and ε “ 10´4.
E4.64 Apply the Secant method with xp0q “ 2 and xp1q “ 3 to estimate the intersection point of
y “ ?3x ` 2 ` 3
?x ` 1 and y “ 2
?1 ` x2 with an absolute error tolerance of ε “ 10´3.
E4.65 Repeat E4.64 using (a) the modified secant method with h “ 0.01, (b) the Newton-Raphson
method with xp0q “ 2, (c) compare both methods in terms of the total iteration numbers required
for convergence.
E4.66 Estimate the roots of the following nonlinear equations using the Secant method with an
absolute error tolerance of ε “ 10´4.
(a) 2?x ` x ´ 3 ln x “ 3, xp0q “ 1.8, xp1q “ 2 (b) xe´2x ` x “ 2, xp0q “ 0.7, xp1q “ 1.1
(c) tan´1
´ 2x
1`x
¯
lnp1 ` xq “ 2, xp0q “ 1, xp1q “ 1.5
E4.67 Repeat E4.66 with the modified secant method using h “ 0.1, 0.05, and 0.01 and xp0q “ 1.8.
How does the value of h affect convergence?
E4.68 Water is pumped from a reservoir at elevation h1 “ 400 m to another reservoir at elevation
h2 “ 430 m through a pipeline with a diameter of D “ 27.5 cm and a length of L “ 100 m, as
illustrated in Fig. E4.68. Neglecting local losses, the energy equation between the two reservoir
surfaces can be written as
hp “ f L
D
V 2
2g
` h2 ´ h1
where hp is the pump head (m), h1 and h2 are the elevations of the reservoirs, f is the friction
factor, V is the mean velocity of water (m/s), and g is the acceleration of gravity (m/s2). The
velocity and the friction factor can be expressed in terms of the discharge Q as
V “ Q{pπD2
{4q, f “ 0.01pD{Qq
1{4
If the pump head-Q characteristic curve is given by hp “ 33 ´ 78.6 Q ´ 9060 Q2 where Q and hp
are in units m3/s and m, respectively, (a) derive an expression for the discharge Q in the pipeline;
(b) find the discharge, accurate to four decimal places, using the modified secant method with
Qp0q “ 0.01 and h “ 0.01.Nonlinear Equations  261
Fig. E4.68
E4.69 A thermistor is a resistor whose resistance is dependent on temperature. Thermistors are
used as temperature sensors due to their high sensitivity. A change in the temperature of the ther￾mistor results in a change in electrical resistance. By measuring the resistance of the thermistor,
one can determine the temperature. The Steinhart–Hart equation, a third-order approximation,
relates the temperature and the resistance as
T ´1 “ a ` b ln R ` cpln Rq
2
where R is the resistance (Ω), T is the absolute temperature (K), and the coefficients of the
Steinhart–Hart equation of a thermistor are a “ 1.32 ˆ 10´3, b “ 2.34 ˆ 10´4, and c “ 8.8 ˆ 10´8.
Apply the modified secant method to estimate the resistance of the thermistor at 40˝C. Use
h “ 0.01, ε “ 10´5 and Rp0q “ 100 Ω.
E4.70 In modeling an AC-DC converter, the following nonlinear equation is encountered:
sinpβ ´ φq ´ e
´β{ tan φ sin φ “ 0
where β is the dead-lock angle (π ď β ď 2π) and φ is the load angle (0 ď φ ď π{2). Estimate two
roots, φ1 and φ2, for β “ 11π{9 using the modified secant method with h “ 0.01, φp0q “ 0.5 and
φp0q “ 1.3. Use ε “ 10´3.
E4.71 In the study of like-charge conducting same-size spheres, identifying the regions of attrac￾tion or repulsion leads to the following nonlinear equation:
ˆ
ln 4a
s ` 2γ
˙ cs
a “ δ0
where a is the capacitance coefficient, s is the surface-to-surface distance of the spheres, δ0 is a
parameter varying as a function of charge ratio, and γ “ 0.5772 is Euler’s constant. This nonlinear
equation yields two s{a values (roots) for every δ0. Estimate the two roots for δ0 “ 0.5, 1.5, and
2.5 using the modified secant method with h “ 0.001 and ε “ 10´3.
E4.72 In an isentropic turbine, a flammable gas expands at atmospheric pressure. If the exhaust
temperature satisfies the following equation, find the flame temperature using the modified secant
method with h “ 0.01, Tp0q “ 400 K, and ε “ 10´3.
3.45 ˆ 10´2 pT ´ 597q ´ 3.98 ˆ 10´6 `
T 2 ´ 5972˘
` 0.195 ln ˆ T
597˙
` ln „
93.3T ´ 157.2
486 j
“ 0
Section 4.7 Accelerating Convergence
E4.73 Consider the fixed-point iteration with gpxq “ 1 ` x{
?x ` 2 and xp0q “ 1. (a) Carry
out the first 10 steps to compute the linear convergence rate from λpnq « |g1
pxpnq
q| and the
approximate convergence rate from λpnq « |Δxpn`1q
{Δxpnq
|, (b) apply the Aitken acceleration to
the sequence of approximations in Part (a) and evaluate the approximate convergence rate from
λpnq « |Δαpn`1q
{Δαpnq
|. Use ε “ 0.5 ˆ 10´4.
E4.74 Repeat E4.73 for gpxq “ lnp2 ` e´xq with xp0q “ 1{3.262  Numerical Methods for Scientists and Engineers
E4.75 Consider the following finite series:
Spnq “ 1
1 ¨ 3 `
1
3 ¨ 5 `¨¨¨`
1
p2n´1qp2n`1q or Spnq “Spn´1q
`
1
p4n2´1q
, Sp0q “0, n ě 1
which converges to 1/2 for large n. (a) Sum up the sequence up to Sp10q
, the true absolute error by
|1{2 ´ Spnq
|, and the convergence rate from λpnq « |Spn`1q ´ Spnq
|{|Spnq ´ Spn´1q
|, and (b) apply
Aitken’s acceleration to the sequence generated in Part (a) and compare your results.
E4.76 Successive approximations produced by a linear iterative process yield the results presented
below. Use the Aitken acceleration to compute tαpnq
u and estimate the convergence rate.
n xpnq n xpnq
0 0.84089 64153 5 0.74517 97287
1 0.78323 89193 6 0.74324 85821
2 0.75966 08667 7 0.74239 71531
3 0.74957 52538 8 0.74202 11823
4 0.84089 64153 9 0.74185 50490
E4.77 Estimate the root of x “ ?x ´ x5 with xp0q “ 1{4, (a) using the fixed-point iteration
method, accurate to six decimal places. How many iterations are required to reach the answer?
(b) How many iterations would be required if Aitken acceleration were used?
E4.78 For the following sequences, generate the first five terms of the sequence tαpnq
u using the
Aitken acceleration and determine the decimal-place accuracy of αp4q
.
piq xpn`1q “ 3 ` 2e´xpnq
, xp0q “ 0, piiq xpn`1q “ ?1 ´ sin xpnq, xp0q “ 0,
piiiq xpn`1q “
a
2 ` ?3 xpnq, xp0q “ 1, pivq xpn`1q “ exppxpnq
{3q, xp0q “ 2
E4.79 The isentropic condensation temperature is the temperature at which saturation is reached
when a parcel of moist air is lifted adiabatically with its vapor content (w) held constant. It can
be estimated from the solution of the following nonlinear equation:
Tic “ C1
lnpC2ε{wpiq`p1{κq lnpTi{Ticq
where C1 “ 5400 K, C2 “ 2.5 ˆ 109 kPa, κ “ 0.286, ε “ 0.622, w is the water vapor mixing
ratio, and Ti and pi are the initial temperature (K) and pressure (kPa), respectively. Using the
fixed-point iteration with Aitken’s acceleration, estimate the isentropic condensation temperature
for the case of w “ 0.04 kg vapor/kg dry air, Ti “ 265 K and pi “ 100 kPa. Use Tp0q “ Ti and
ε “ 10´3.
E4.80 For the fixed-point iteration equation given by xpn`1q “ sinpxpnq ` sin xpnq
q with xp0q “ 1,
apply Steffensen’s acceleration to estimate αp1q and |αp1q ´ gpαp1q
q|.
E4.81 For the fixed-point iteration equation given by xpn`1q “ cospxpnq
{2q coshpxpnq
{3q with
xp0q “ 1{2, apply Steffensen’s acceleration to estimate αp1q and
ˇ
ˇ
ˇαp1q ´ gpαp1q
q
ˇ
ˇ
ˇ.
E4.82 For the fixed-point iteration equation given by xpn`1q “ xpnq
{p5 ` pxpnq
q
2
q ` 3xpnq ´ 1 and
xp0q “ 0, apply Steffensen’s acceleration to estimate αp2q and
ˇ
ˇ
ˇαp2q ´ gpαp2q
q
ˇ
ˇ
ˇ.
E4.83 Apply Steffensen’s acceleration to the iteration equation: gpxq “ 5´2x lnp1`0.5x2q using
xp0q “ 1. (a) Perform 5 iterations to estimate αp5q and
ˇ
ˇ
ˇαp5q ´ gpαp5q
q
ˇ
ˇ
ˇ; and (b) what can you say
about the accuracy of αp5q
? (c) Show that Steffensen’s method converges quadratically.
E4.84 Apply Steffensen’s acceleration to estimate the solution of 2x “ 5´x to within ε “ 10´6.
Use xp0q “ 1 as the starting value.
E4.85 Repeat E4.78 with Steffensen’s acceleration for four iterations.
E4.86 Repeat E4.31 with Steffensen’s acceleration for four iterations.Nonlinear Equations  263
E4.87 A vertical metal panel (A “ 0.05 m2) suspended in a room at T8 “ 15˝C is subjected
to electric current, which causes the panel to produce Q “ 24 W heat energy. The rate of heat
transfer from the plate to the room is calculated by
Q “ 0.0475ApTs ´ T8q
´
0.825 ` 14.15´Ts ´ T8
Ts ` T8
¯1{6¯2
where Ts is the mean surface temperature (K) and A is the surface area of the plate (2A, losses
from both sides). Apply Steffensen’s acceleration to estimate the mean surface temperature of the
panel, accurate to two decimal places. Use Tp0q s “ 300K for the initial value.
E4.88 Consider the iteration sequence txppq
u generated in Example 4.3. (a) Apply Aitken’s ac￾celeration to the sequence to improve convergence and calculate the convergence rates for both
txppq
u and tαppq
u sequences; (b) Starting with xp0q “ 1, apply Steffensen’s acceleration to estimate
the root to four decimal places.
Section 4.8 System of Nonlinear Equations
E4.89 Using the given initial guesses, estimate the common solution of the following systems of
nonlinear equations accurate within ε “ 10´5.
(a) x2 ` y2 “ 10, x2y ´ xy3 ´ xy ` 3 “ 0, pxp0q
, yp0q
q“p1, 1q
(b) x2 ` 2xy “ 1.35, x ` 3y ` y2 ´ x2 “ 4.76, pxp0q
, yp0q
q“p1, 1q
(c) 2x2 ` x ` 3y “ 7, 3x ` 4y ` xy “ 3, pxp0q
, yp0q
q“p0, 0q
(d) ex´y “ 7, xey ` yex “ 9, pxp0q
, yp0q
q“p2, 1q
(e) cospx ´ yq ` x “ π, cospx ` yq ` 2y “ π, pxp0q
, yp0q
q“p3, 1.5q
(f) 2y{x ` 2x2{y “ 17, 4y2{x ` 2x{y3 “ 5, pxp0q
, yp0q
q“p1, 1q
(g) y
?x ` ?y “ 21, xy ` x ` y “ 49, pxp0q
, yp0q
q“p5, 5q
E4.90 The steady-state temperature (in ˝C) distribution of a large metal plate (0 ď x ď 6 m,
0 ď y ď 4 m) is given by the following equation:
Tpx, yq “ x3 ` 4y3 ´ 9x2 ´ 24y2 ` 2xy ` 15x ` 36y ` 65
Use Newton’s method to estimate the critical points of the temperature distribution, accurate to
two decimal places, and identify them as the local maximum, local minimum, or saddle point. Use
(1,1), (1,3), (4,1), and (4,3) for the initial guesses.
E4.91 Find the intersection points of the ellipse 2px ´ 1q
2`py ` 1q
2 “ 2 and the circle x2`y2 “ 1.
Use (0,1) and (1,1) as the initial guesses and ε “ 0.5 ˆ 10´2 as convergence tolerance.
E4.92 Apply Newton’s method to find all intersection points of the parabola y “ 6´2x´x2 and
the ellipse px ` 1q
2
{9 ` py ´ 3q
2
{4 “ 1. Use (´2,5), (0,5), (´3,2), and (1,2) as the initial guesses
and ε “ 0.5 ˆ 10´3 as convergence tolerance.
Section 4.9 Bairstow’s Method
E4.93 Apply Bairstow’s method to estimate all real and imaginary roots of the polynomials given
below with a tolerance of |δp| ` |δq| ă 0.001. Use pp0q “ qp0q “ 0 as the starting values for all
solutions.
paq fpxq “ x3 ` 0.92x2 ´ 0.674x ´ 3.45
pbq fpxq “ x4 ` 2x3 ` 3.31x2 ` 2.32x ` 1.375
pcq fpxq “ x5 ´ 0.58x4 ´ 6.0845 x3 ´ 11.0546 x2 ´ 3.3959x ` 1.63625
pdq fpxq “ x6 ` 6.95x5 ` 20.42x4 ` 42.11x3 ` 67.48x2 ` 57.24x ` 7.2
peq fpxq “ x7 ` 2.85 x6 ` 3.4x5 ´ 6.2 x4 ´ 31.45 x3 ´ 43.8 x2 ` 7.4x ` 92264  Numerical Methods for Scientists and Engineers
E4.94 A mixture of 1.5 mol of N2 and 3 mol of H2 is introduced into a 10 L reaction vessel at
500 K to make NH3. At this temperature, the reaction equilibrium constant for N2(g)+3H2(g)Õ
2NH3(g) is given as K “ 306. Using the equilibrium equation below, estimate the equilibrium
concentrations by applying Bairstow’s method with pp0q “ qp0q “ ´1 as the initial guess. Use
|δp| ` |δq| ă 0.001 for the stopping criterion.
K “ rNH3s
2
rN2srH2s
3 “ p2xq
2
p0.15 ´ xqp0.3 ´ xq
2
E4.95 The chemical equilibrium of the ammonia synthesis reaction is given by
K “ 8x2p4 ´ xq
2
p6 ´ 3xq
2
p2 ´ xq “ 0.19
where x is the ammonia concentration. Estimate the equilibrium concentration of ammonia using
Bairstow’s method with pp0q “ qp0q “ 0 as the initial guess. Use |δp|`|δq| ă 0.001 for the stopping
criterion.
E4.96 The azeotropic point of a binary solution is described by
0.6 p1 ´ xq
2 ´ 0.4 x3
p0.6 ´ 0.2 xq
2 ` 0.625 “ 0
where x denotes the composition of the dominant component. Find the azeotropic point compo￾sition using Bairstow’s method, starting with pp0q “ ´1, qp0q “ 2. Use |δp| ` |δq| ă 0.001 for the
stopping criterion.
E4.97 The combustion reaction of methane is given by CH4+2O2 Ñ CO2+2H2O(g). Both
methane and air enter the burner at 25˝C. The fundamental equation relating reaction heat
to temperature is given by
ΔHo “ ΔHo
298 `
ż T
298
C0
pdT
where ΔHo is the standard heat of reaction at temperature T, and the standard heat of reaction
at 298 K is given as ΔHo
298 “ ´802 kJ. The mean heat capacity of the products Co
p is given as
Co
ppTq “ 361.4 ` 0.079T ´ 536000
T 2
ˆ J
kg ¨ K
˙
Assume the combustion reaction goes to completion adiabatically and the overall energy balance
for the process reduces to ΔH “ 0; that is,
ΔHo
298 `
ż T
298
C0
pdT “ 0
Under these conditions, estimate the maximum temperature (flame temperature) that can be
reached due to the combustion of methane by using Bairstow’s method (use pp0q “ ´2000, qp0q “
1000). For the stopping criterion, use |δp| ` |δq| ă 0.001.
E4.98 0.55 mol N2, 0.8 mol H2, and 0.35 mol NH3 gas are mixed in a container at 298 K. The
pressure-scale equilibrium constant is given by Kp “ 6.0406 ˆ 105 atm´2. The pressure of the
mixture is maintained at P “ 0.0025 atm throughout the course of the chemical reaction stated
by N2(g)+3H2(g)Õ 2NH3(g). The equilibrium equation leads to
´85.935125x4 ` 116.01241875x3 ´ 64.067615x2 ` 17.10638x ´ 0.70912 “ 0
Estimate the amount of N2, H2, and NH3 at equilibrium using Bairstow’s method with pp0q “ ´1,
qp0q “ 0 as the initial guess. Use |δp| ` |δq| ă 0.1 for the stopping criterion.Nonlinear Equations  265
E4.99 The characteristic equation of matrix A is given
as
λ8 ´ 64λ7 ` 1351λ6 ´ 10420λ5 ` 21310λ4 ` 56λ3
` 1621λ2 ` 44000λ ` 106 “ 0
Find all eigenvalues (roots) using Bairstow’s method.
Start with pp0q “ ´2000, qp0q “ 1000, and use |δp|`|δq| ă
0.1 for the stopping criterion.
A “
»
—
—
—
—
—
—
—
—
—
—
–
87654321
98765432
9876543
987654
98765
9876
987
9 8
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Section 4.10 Polynomial Reduction and Synthetic Division
E4.100 Use Synthetic division to reduce the given polynomial by the specified root.
(a) P3pxq “ x3 ´ 2.9x2 ´ 11.32x ` 26.24 px “ 2q
(b) P3pxq “ x3 ´ 2x2 ´ 9.39x ` 16.83 px “ ´3q
(c) P4pxq “ 2x4 ` 5x3 ´ 15x2 ´ 10x ` 8 px “ 1{2q
(d) P4pxq “ x4 ´ 4 x3 ´ 7 x2 ` 22x ` 24 px “ ´1q
(e) P5pxq “ x5 ´ x3 ´ 2 x2 ` 32 px “ ´2q
E4.101 Estimate all real roots of the following polynomials, accurate to six decimal places. Reduce
the polynomials down to quadratic equations and solve the quadratic equation to find the last
two roots. Start with xp0q “ 0 to estimate a root, and then use this estimate as the initial guess
to find the next root, and so on.
(a) x4 ´ 4.8x3 ´ 9.49x2 ` 35.892x ` 46.368 “ 0
(b) x4 ´ 3x3 ´ 9.75x2 ` 15.5x ` 30 “ 0
(c) 50x4 ` 180.5 x3 ` 136.02 x2 ´ 40.788x ` 2.6136 “ 0
(d) x5 ` 3.32x4 ` 0.72 x3 ´ 0.75 x2 ´ 0.025x ` 0.01 “ 0
E4.102 The composition-temperature (xB´T) phase
diagram of a binary system of A and B is illus￾trated in Fig. E4.102. The temperatures, describing
the solidus and liquidus curves, are given with the
following third-degree polynomials:
TspxBq “ 33 ` 8.39xB ` 34 x2
B ´ 5.39x3
B
TlpxBq “ 33 ` 80.14xB ´ 53x2
B ` 9.86x3
B
Estimate the liquidus and solidus compositions (xa
and xb) corresponding to 50˝C with two decimal
places of accuracy. Use xp0q “ 0.5 and polynomial
reduction for both cases. Fig. E4.102266  Numerical Methods for Scientists and Engineers
E4.103 A uniform beam subjected to a varying dis￾tributed load is depicted in Fig. E4.103. The equation
for the deflection is given as
y “ Kξp0.2 ´ 0.4ξ ` 2.8ξ2 ´ 4.42ξ3 ` 1.83ξ4
q
where ξ “ x{L, L is the length of the beam, and K is
a constant. Use the method of polynomial reduction
to estimate the point of maximum deflection as well
as the deflection. Start with ξp0q “ 0.55.
Fig. E4.103
E4.104 The system characteristic curve is the response of the pump head of the installation to
a given liquid flow rate. The head hp (m) is given below as a fourth-degree polynomial of flow
rate Q (m3/h). Use the method of polynomial reduction to estimate the flow rate corresponding
to hppQq “ 9 m, accurate to two decimal places. Use Qp0q “ 50 m3/h for the initial guess.
hppQq “ 10.89 ` 0.00475 Q ´ 0.000762 Q2 ` 7.36 ˆ 10´6
Q3 ´ 2.96 ˆ 10´8
Q4
E4.105 A seven-component flash process is modeled by the following equation:
ÿ7
i“1
p1 ´ kiq Ci
1 ` ψ pki ´ 1q “ 0
where Ci and ki are the feed composition and the equilibrium ratio provided in the table below,
and ψ is the fraction of the feed that goes into the vapor phase. This equation leads to a polynomial
of 6th degree for ψ. Apply Bairstow’s method to estimate ψ in the range (0,1). Use ε “ 10´5 and
(p, q)=(0,0) for the starting guess.
i 1 234 5 6 7
ki 1.71 3.19 0.74 0.41 0.22 0.18 0.067
Ci 0.005 0.835 0.04 0.016 0.006 0.008 0.09
4.13 COMPUTER ASSIGNMENTS
CA4.1 In fluid mechanics, the two correlations that are commonly used for estimating the friction
factor in smooth pipe flows are given as
Haaland Formula: 1
?f “ ´1.8 log10 ˆ6.9
Re ˙
, 4 ˆ 103 ă Re ă 108
Colebrook Formula: 1
?f “ ´2 log10 ˆ 2.51
Re?f
˙
, 4 ˆ 103 ă Re ă 108
where f and Re denote the friction factor and the Reynolds number, respectively. The Haaland
formula is an explicit equation that allows the friction factor to be computed for a specified
Reynolds number. On the other hand, the Colebrook formula is an implicit equation that requires
solving a nonlinear equation. Apply a suitable numerical method to estimate the friction factor,
accurate to at least six decimal places for Re=104, 105, 106, 107, 108, and 109 using Colebrook’s
equation. Compare your estimates to those computed using Haaland’s formula.
CA4.2 In quantum physics, the solution of the Schrödinger equation for a finite square well with
potential V0 leads to the following nonlinear equation:
tan z “
c ´z0
z
¯2
´ 1, z “ a

a2mpE ` V0q and z0 “ a

c2m
V0Nonlinear Equations  267
where E is the energy, m is the mass of the particle, and  is the Plank’s constant. Using a suitable
numerical method, estimate the quantity z, accurate to five decimal places for z0 “ 0.05, 0.1, 0.5,
1, 5, 10, and 20 and obtain a plot of z0 versus z.
CA4.3 The transient one-dimensional heat conduction from a plane wall, subjected to convection
on both sides, requires the solution of the following nonlinear equation:
βn sin βn ´ Bi cos βn “ 0, n “ 0, 1, 2, 3, ..
where Bi= hL{k is referred to as the Biot number, h is the convection transfer coefficient, k is the
conductivity of the plate, L is the thickness of the wall. This nonlinear equation has an infinite
number of roots called eigenvalues. Use an appropriate numerical root finding method to estimate
the first five eigenvalues (βn, n “ 0,1,2,3,4) correct to six-decimal places for Bi=0, 0.1, 0.5, 1, 2
and 10.
CA4.4 When a mass m2 is attached to the tip of a vertical rod, having a length L and a mass m1,
longitudinal vibrations in the rod are observed. The analytical solution for the vibration frequency
leads to the following nonlinear equation:
m1
m2
“ ωn
cm1
k tan ˆ
ωn
cm1
k
˙
where k is a constant and ωn is the vibration frequency. Letting αn “ ωn
am1{k and R “ m1{m2
(the mass ratio), the above equation is expressed as R “ αn tan αn. Use a suitable numerical
root-finding method to estimate the first five eigenvalues (αn, n “ 0,1,2,3, and 4) accurate to six
decimal places for R “ 0.01, 0.1, 0.25, 0.5, 1, 2, 5, and 10.
CA4.5 The criticality equation for a spherical nuclear reactor, surrounded by an infinite (very
thick) reflector, is given as
BR cot BR ´ 1 “ ´Dr
Dc
ˆ BR
BLr
` 1
˙
where R is the reactor radius, Dc and Dr are the diffusion coefficients of the core and reflector,
Lr is the reflector diffusion length, and B is a parameter, referred to as buckling. In order for
a reactor to be able to achieve self-sustained nuclear reactions, the foregoing nonlinear equation
needs to be satisfied. This equation, which also has an infinite number of solutions (eigenvalues)
can be rewritten in simplified dimensionless form as
ψ cot ψ “ 1 ´ η
` ψ
BLr
` 1
˘
where ψ “ BR and η “ Dr{Dc is the diffusion coefficient ratio. Using a suitable numerical root￾finding method, estimate the smallest positive root (ψmin), accurate to four decimal places for
BLr “ 0.6 and η “ 0.6, 0.7, 0.8, 0.9, 1, 1.2, 1.4, and 1.6.
CA4.6 The following nonlinear equation has two positive roots for any c ą 0. Apply a suitable
numerical root-finding method to estimate both of the roots, accurate to five decimal places for
c “ 0.3, 0.5, 1, 2, 5, and 25.
e
´3x ` 1.26 e
x{3 ` 1.2e
´3x{2 “ c x2
CA4.7 The following nonlinear equation is encountered in the study of two-phase diffusion prob￾lems. Apply a suitable root-finding method to estimate the smallest four positive roots (βn, n “ 1,
2, 3, and 4), accurate to five decimal places for λ “ 0.1, 0.5, π{4, 1 and π{2.
sinpλβq cospβq ` cospλβq sin β ´ β sin β sinpλβq “ 0268  Numerical Methods for Scientists and Engineers
CA4.8 In structural engineering, slender members experience a
mode of failure called buckling. When designing a slender mem￾ber, the critical buckling load is calculated for safety purposes. If
a load P acts through the centroid of the cross section, the critical
buckling load is calculated from Pcrit “ π2EI{L2, where E is the
modulus of elasticity (Pa), L is the length (m), and I is the moment
of inertia (m4). However, in reality, the load P might be applied at
an offset or the slender member may not be completely straight (see
Fig. CA4.8). In such cases, the maximum stress is calculated with
the secant formula, which is given as
σmax “ P
A
´
1 ` ec
R2 sec ´L
2
c P
EI
¯¯ Fig. CA4.8
where e is the eccentricity (m), A is the cross-section area (m2), c is the half depth of the column
(m), r is the radius of gyration of the cross section (m), and σ is the stress (Pa). The secant formula
can be rearranged to give
Smax “ pc
´
1 ` φ secp
π
2
?pcq
¯
where three dimensionless quantities appear: Smax “ σmax{pPcrit{Aq, φ “ ec{R2, and pc “ P{Pcrit.
Use a suitable numerical method to estimate the buckling load ratio for Smax “ 1, 3, 5, 7, 9, and
φ “ 0.5, 1, and 2, accurate to three decimal places.
CA4.9 The Peng-Robinson equation of state is used to estimate the volume of pure propane gas
as a function of pressure and temperature.
P “ RT
V ´ b ´ aα
V 2 ` 2bV ´ b2
where T is the temperature in Kelvin, V is the volume in cm3/gmol, P is the pressure MPa,
R “ 8.3145 J/mol-K, a “ 1.14 ˆ 106 MPa.cm6/gmol2, b “ 56.29 cm3/gmol, and α is given by
α “
´
1 ` 0.6027´
1 ´
c T
369.8
¯¯2
Write a computer program to estimate the volume (cm3/gmol) of propane at a specified temper￾ature (K) and pressure (MPa). Using this program, estimate the volume for T “ 300, 320, 340,
and 360 K and P “ 0.1, 0.5, and 1 MPa.
CA4.10 Gibbs energy of mixing the two liquid phases A and B is given as
ΔGmix “ nRT txA ln xA ` xB ln xB ` βxAxBu
where R “ 8.314 J/mol-K is the universal gas constant, T is the mixing temperature (K), x
denotes the mole fraction of a phase, and β is a dimensionless parameter that is a measure of
chemical interactions (if β ă 0, mixing is exothermic or endothermic otherwise). For β ą 2, the
Gibbs energy of mixing has a double minimum, and we can expect phase separation to take place.
Noting that xA `xB “ 1, use the fixed-point iteration to estimate the compositions corresponding
to the minimum points with three decimal place accuracy for β “ 2, 2.25, 2.5, and 3.
CA4.11 The natural frequencies (ωn) of radial vibrations of an elastic sphere are given as
tanpξnq “ Bγ2ξ2
n ´ 4 pAξn ` Bq
Aγ2ξ2
n ` 4 pB ξn ´ Aq
where γ2 “ G{pλ`2Gq, A and B are material dependent constants, R is the radius of the sphere,
ξn “ knR, and kn is defined as k2
n “ ω2
n ρ{pλ ` 2Gq, ρ is the density, G “ E{2p1 ` νq is the shear
modulus, E is the Young’s modulus, ν is the Poisson ratio, and λ “ Eν{p1 ` νqp1 ´ 2νq. ForNonlinear Equations  269
a spheroidal rubber gum for which E “ 9 MPa, ν “ 0.45, R “ 0.10 m, ρ “ 1100 kg/m3, and
A{B “ 10, the frequency equation is reduced to
tanpξnq “ ξ2
n ´ 440ξn ´ 44
2p5ξ2
n ` 22ξn ´ 220q
Using a suitable root-finding method of your choice, estimate the natural frequency of the vibration
(ω1), accurate to four decimal places.
CA4.12 Bridgman analysis is used to estimate the stresses that would develop in the necking re￾gion of a cylindrical tension specimen. The analysis makes use of the following Bridgman correction
factor (B):
B “ σT B
σT
“
˜´
1 `
2R
a
¯
ln ´
1 ` a
2R
¯
¸´1
where σT is the tensile strength, a is the radius of the tensile specimen at the thinnest section of
the neck, and R is the radius of curvature of the neck profile as illustrated in Fig. CA4.12. Estimate
a{R ratios that correspond to correction factors of B “ 0.776, 0.822, 0.9, and 0.95.
Fig. CA4.12
CA4.13 A reduction reaction of M “ 1 mol of an oxide at T “ 300 K is investigated. The
electrode potential difference, which is described by the Nernst equation, takes the following form:
ΔE “ RT
nF ln ˆnMF ´ Q
Q
˙
where R “ 8.3145 J/K.mol is the universal gas constant, T is the temperature (K), F “ 96480
C/mol is the Faraday’s constant, Q is the discharge (C), and n is the number of electrons trans￾ferred. Using the Newton-Raphson method, estimate the number of electrons transferred when
the discharge is at its maximum, which is accurate to two decimal places. The electrode potential
difference is given as 1.2 mV.
CA4.14 When studying heat transfer in a plane parallel medium with anisotropic scattering, a
transcendental equation similar to the one given below is encountered:
x arccoth x “ c x3 ´ 2x ´ 1
0.1 x3 ` c x ` 1
Use a numerical root-finding method to solve the given nonlinear equation, accurate to four decimal
places for c “0.2, 0.4, 0.6, 0.8, and 1.
CA4.15 The zeroth- and first-degree Legendre polynomials are defined as P0pxq“1 and P1pxq“x.
The nth-degree Legendre polynomial can be generated using the following recurrence relationship:
n Pnpxq“p2n ´ 1qxPn´1pxq´pn ´ 1qPn´2pxq, for n ě 2
On the other hand, a recursive expression for the derivative of the nth-degree Legendre polynomial
is given by
dPnpxq
dx “ nPn´1pxq ´ nxPnpxq
1 ´ x2 , n ě 1
Using the Newton-Raphson method as well as the above relationships, write a computer program
that estimates the positive roots of the Nth-degree Legendre polynomial up to N “10, accurate
to within specified ε. The number of positive roots is N{2 for which you can use the following
expression as your initial guess:
xp0q
k “ cos ˆp4k ´ 1qπ
4N ` 2
˙
, k “ 1, 2, .., N
2CHAPTER 5
Numerical Differentiation
LEARNING OBJECTIVES
After completing this chapter, you should be able to
‚ distinguish the difference between analytic and discrete functions;
‚ explain the procedure for developing difference formulas using Taylor series;
‚ develop a difference formula for any derivative using the Taylor series;
‚ apply forward, backward, and central difference formulas to evaluate (first,
second, etc.) derivatives of a uniform discrete function;
‚ use the difference operators to abbreviate difference formulas or expressions;
‚ find the truncation error of a difference formula using the Taylor series;
‚ identify and explain the order of error;
‚ understand the truncation error and its role in error estimation;
‚ derive forward, backward, and central difference formulas for non-uniformly
spaced discrete functions;
‚ derive forward, backward, and central finite-difference formulas for any deriva￾tives using direct fit polynomials;
‚ apply Richardson extrapolation to estimate the derivatives of analytic func￾tions.
A UNIVARIANT function, y “ fpxq, defines a relationship between two (x-independent
and y-dependent) variables. In the physical world, each variable represents a physical
quantity, such as density, distance, mass, time, temperature, etc. Functional relationships
between any two quantities are defined as either continuous functions or discrete functions.
An explicitly defined (continuous) function y “ fpxq contains an infinite amount of
information through all x-values in its domain. The numerical value of a function for an
input value, say x “ c within its domain, can be obtained simply by direct substitution:
y “ fpcq.
Differentiation operation is encountered in all fields of science and engineering. Recall
that freshman calculus largely deals with the differentiation and integration of continuous
functions and their applications. However, in practice, most (even univariant) functions that
scientists and engineers deal with are not always known explicitly. Functional relationships
between some physical variables are obtained as a result of either sampling or experimen￾tation. Hence, a set of data establishes a relationship, which may have a form or a shape,
and the data points are not continuous or connected. A data set that describes a functional
270 DOI: 10.1201/9781003474944-5Numerical Differentiation  271
FIGURE 5.1: A graphic depiction of (a) the height distribution of individuals in a population and
(b) the density-temperature relationship of a chemical solution.
relationship is referred to as a discrete function. In other words, the values of a discrete
function are represented solely by a number of distinct and separate data points that are
unconnected to each other and whose values are expressed by a table or list of ordered pairs
(points). For instance, the height distribution of individuals in a large population or the
variation of the experimentally measured density of a chemical solution with temperature,
depicted in Fig. 5.1a and b, are two examples of discrete functions. Most physical prop￾erties (density, viscosity, heat capacity, enthalpy, saturation pressure, etc.) are assumed to
be smooth functions of space and time whose explicit mathematical relationships are not
known. The physical properties of solids, liquids, or gases are generally available as discrete
functions of temperature, pressure, etc.
Discrete functions are also encountered in the numerical solution of the differential
equation. An ordinary differential equation contains an infinite amount of information (ypxq
for any x), while a computer can handle only a finite amount of data (y1, y2, ..., yn).
For this reason, the numerical solution of a differential equation is carried out following a
discretization procedure in which a finite set of points is created for which the solution is
obtained.
A discrete function may be either uniformly or non-uniformly spaced. Data sampling
with digital instruments or data collected periodically is generally uniformly spaced, which
is computationally favorable. The mathematical operations with uniformly spaced discrete
data sets are simpler and require less computation and cpu-time. The memory requirement
is also halved since the abscissas do not need to be stored. Therefore, numerical analysts
prefer dealing with uniformly spaced data whenever possible due to the computational
advantages mentioned above.
In general, an explicitly (or implicitly) defined function can be differentiated exactly.
Nevertheless, the differentiation of discrete functions requires a suitable approximation. This
chapter is devoted to the numerical methods and techniques for approximately differentiat￾ing discrete and/or continuous functions.
5.1 BASIC CONCEPTS
Numerical differentiation is the process of evaluating the derivative of a function. It provides
ways and means of estimating the derivative of any continuous or discrete function. The
derivative of a smooth differentiable function y “ fpxq is the instantaneous rate of change
of fpxq, or, geometrically speaking, it is the slope of the tangent line drawn to y “ fpxq at
any point x. In order to approximate the slope of a tangent line at x “ a, we attempt to272  Numerical Methods for Scientists and Engineers
FIGURE 5.2: Determining the slope of a function at point A.
evaluate the slope of the secant line (see Fig. 5.2). To do just that, consider Apa, fpaqq a
fixed point and Bpa ` h, fpa ` hqq a nearby mobile point on y “ fpxq. Next, we consider
the secant line through points A and B. The slope of the secant line passing through points
A and B can be written as
m|AB| “ fpa ` hq ´ fpaq
h
This is clearly not exactly equal to the slope of the tangent line to fpxq at the point
Apa, fpaqq. When we move point B toward A along y “ fpxq, we observe that the slope of
the secant line approaches the slope of the new tangent line. Mathematically, we achieve
this by taking the limit of the slope as h Ñ 0. In Fig. 5.2, the point B moves along the curve
toward B1 as h decreases, ending up close enough to A. In other words, the slope of the
secant line approaches the slope of the tangent line as h is reduced to below a sufficiently
small value.
Derivative (Definition). Mathematically speaking, the first derivative (or true slope) of
y “ fpxq at x “ a is the limit of the value of the difference quotient as the secant lines get
closer and closer to being a tangent line. The derivative of a smooth continuous univariant
function, y “ fpxq, at x “ a is expressed as
f1
paq “ lim
hÑ0
fpa ` hq ´ fpaq
h (5.1)
The limit operation in Eq. (5.1) can be performed if fpxq is explicitly known. However,
in practice, some functions that we deal with do not have explicit forms. For instance, data
collected periodically are usually uniformly spaced with a fixed sampling interval size, h.
Every data collection instrument has a sampling rate that is limited by default manufactur￾ing settings, which cannot be changed for the most part. In other words, since the sampling
interval is fixed, it is practically impossible to truly perform the limit operation as h Ñ 0.
Therefore, the analyst has no choice but to use available data with the present interval
size to approximately estimate the derivative of a given discrete function. In other words,
since the numerical differentiation is approximate, the result naturally contains some error.
Therefore, before using any approximation, it is important to be aware of the magnitude of
the error in the differentiation process.
Truncation and Leading Error. If fpxq is a continuous and infinitely differentiable func￾tion, it can be expanded into the Taylor series in the neighborhood of point a:
fpa ` hq “ fpaq ` h f1
paq ` h2
2! f 2paq ` h3
3! f 3paq`¨¨¨` hn
n!
fpnq
paq ` Rnphq (5.2)
where Rnphq is the remainder defined asNumerical Differentiation  273
Rnphq “ hn`1
pn ` 1q!
fpn`1q
pξq, a ď ξ ď a ` h
Solving Eq. (5.2) for f1
paq yields
f1
paq “ fpa ` hq ´ fpaq
h ´ h
2!f 2paq ´ h2
3! f 3paq´¨¨¨´ hn
pn ` 1q!
fpn`1q
pξq (5.3)
All the terms on the rhs, except the first one, have a factor that includes the power of h.
Hence, as h Ñ 0, the limit of Eq. (5.3) does give Eq. (5.1) due to the vanishing terms with
the factor of hk for k ě 1. But, since the interval size of a discrete function is fixed, the
limit operation cannot, in reality, be achieved for h Ñ 0. For this reason, the derivatives of
discrete functions can only be determined approximately.
Neglecting the remaining terms that contain multiples of hk, Eq. (5.3) yields
f1
approxpaq « fpa ` hq ´ fpaq
h (5.4)
Besides approximating the derivatives of functions, it is equally important to predict the
magnitude of the resulting approximation errors. Equation (5.4) contains a certain amount
of error whose magnitude we cannot quantify at this stage. To estimate the approximation
error, we first define the differentiation error as the difference between the true and the
approximate values:
f1
truepaq ´ f1
approxpaq“´ h
2!f 2paq ´ h2
3! f 2paq ´ h3
4! fp4q
paq´¨¨¨ (5.5)
This expression, which includes higher derivatives of fpxq, accounts for the total error
(or truncation error) made when f1
paq is approximated by Eq. (5.4). Since higher deriva￾tives are also unknown, it is impossible to estimate the true truncation error. However,
generally, we are interested in knowing the relative magnitude of the error rather than its
true value. Note that the terms containing hk (k ě 2) vanish quickly with increasing k, and
an approximate f1
paq can be expressed as
ˇ
ˇ
ˇ f1
truepaq ´ fpa ` hq ´ fpaq
h
ˇ
ˇ
ˇ « h
2
ˇ
ˇf 2paq
ˇ
ˇ (5.6)
This term, referred to as the leading error, is the first term on the rhs of Eq. (5.5). In other
words, the rest of the terms in Eq. (5.5) are much smaller than the leading term, which
makes up the bulk of the true error. Hence, whenever we speak of the “truncation error” to
assess the magnitude of the error of a finite difference approximation, we will be referring
to the “leading error.”
In practice, Equation (5.5) is sufficient to predict the magnitude of the error, but f 2paq
is unknown when fpxq is a discrete function. In this regard, an upper bound on truncation
error can be obtained by replacing f 2paq with f 2pξq:
ˇ
ˇ
ˇ
ˇ
f1
truepaq ´ fpa ` hq ´ fpaq
h
ˇ
ˇ
ˇ
ˇ “ h
2
ˇ
ˇf 2pξq
ˇ
ˇ (5.7)
where a ď ξ ď a ` h and |f 2pξq| denotes its maximum value on ra, a ` hs. Then, the
truncation error (leading error) of the approximation for the first derivative, Eq. (5.4),
becomes ´hf 2pξq{2.274  Numerical Methods for Scientists and Engineers
Order of Accuracy. At this point, we make use of the big-O notation, Op¨q, to symbolize
the order of error. Recall that Ophq implies that the error is directly proportional to h; that
is, Ophq ” C ¨ h, where C is a constant. The exponent of h is referred to as the order of
accuracy, which is also indicative of how rapidly the truncation error vanishes as the interval
is refined. Ophnq denotes nth-order approximation. The higher the order, the faster the
truncation error vanishes. Note that the f 2pξq{2 as a multiplier in Eq. (5.7) is independent
of h regardless of ξ, and it can be assumed to be a constant. The truncation error can then
be expressed as hf 2pξq{2 ” C ¨ h ” Ophq. Making use of the big-O notation, we express Eq.
(5.3) as
f1
paq “ fpa ` hq ´ fpaq
h ` Ophq (5.8)
which is referred to as the first-order accurate finite-difference formula.
Round-off Errors. Apart from truncation error, another type of error that plays an un￾favorable role in numerical differentiation is the round-off error. The problem arises because
the derivative of a function cannot be computed with the same precision as the function
itself. To illustrate this point, consider an explicitly defined continuous function, fpxq. The￾oretically, the true value of f1
pxq is obtained by making h in Eq. (5.4) as small as possible
(h Ñ 0). The numerators of the finite difference formulas typically consist of fpxq, fpx˘hq,
fpx ˘ 2hq, and so on. If we make the value of h too small, the numerator will be approx￾imately equal to zero due to the restrictive nature of machine epsilon. This would be, of
course, inaccurate! The sum of the coefficients of the terms in the numerator of any differ￾ence formula, without exception, is always zero, so when evaluating the numerator, several
significant figures may be lost while performing multiplication, subtraction, and addition
operations with coefficients. Nevertheless, we cannot make h too large either because the
truncation error would dominate the results. Unfortunately, this problem cannot be fully
rectified, but its adverse effects may be mitigated to some extent by either using high￾precision arithmetic or adopting finite difference formulas of accuracy greater than or equal
to 2.
In order to analyze the loss of accuracy during numerical differentiation, we use ˆfpxq
to denote the floating point representation of the function. The difference between the true
and machine-computed value, ˆf1
comppxq, from the approximate formula is
f1
truepxq ´ ˆf1
comppxq “ f1
truepxq ´ ˆfpx ` hq ´ ˆfpxq
h
Next, we assume ˆfpxq differs from the true value, fpxq, in relative error sense, by a small
amount on the order of machine epsilon, i.e., ˆfpxq “ fpxqp1 ` �q and � « Op�machq. Substi￾tuting the floating point values of the function into the approximation, we find
f1
truepxq ´ fpx ` hq ` �1fpx ` hq ´ fpxq ´ �2fpxq
h
“
´
f1
truepxq ´ f1
approxpxq
¯
` �1fpx ` hq ´ �2fpxq
h
(5.9)
where f1
approxpxq is evaluated by Eq. (5.4). Note that the first and second terms in Eq. (5.9)
account for the truncation and round-off errors, respectively. Then, the total error can be
expressed as
Etotalphq “ Etruncphq ` Erndoffphq (5.10)
where Etruncphq “ f1
truepxq ´ f1
approxpxq, and Erndoffphq“p�1fpx ` hq ´ �2fpxqq{h.Numerical Differentiation  275
FIGURE 5.3: Depicting numerical differentiation errors as a function of h.
An upper bound for the truncation error, using Eq. (5.7), is found as
Etruncphq “ h
2
M (5.11)
where M “ max|f 2pξq| on x ď ξ ď x ` h. On the other hand, for very small h, the upper
bound for the round-off error is reduced to
Erndoffphq “
ˇ
ˇ
ˇ
ˇ
�1fpx ` hq ´ �2fpxq
h
ˇ
ˇ
ˇ
ˇ
ď |�1 ´ �2|
h N ď
2ε˚
h N
where ε˚ is the upper bound for the relative error in which no underflow or overflow occurs,
N denotes N “ |fpxq|, assuming |fpx ` hq| « |fpxq|. In the above expression, however, we
do not know the signs of �1 and �2; thus, we cannot accurately estimate |�1 ´ �2|. But if �1
and �2 have opposite signs, the magnitude of the difference could be doubled, so we replaced
this difference with |�1 ´ �2| – 2 ε˚.
The round-off and truncation errors are graphically depicted in Fig. 5.3, where we
immediately observe two distinct regions that are dominated by either truncation or round￾off errors. The truncation error, Eq. (5.11), is proportional to h, and it decreases linearly
with decreasing h. On the other hand, the region dominated by round-off error is inversely
proportional to h, as given by 2ε˚N{h. For large h, it is initially very small (on the order of
machine epsilon) but increases with decreasing h. The total error decreases as h is reduced,
but the approximations worsen with further reduction in h since the round-off errors become
more dominant than the truncation errors. This means that there is an optimum value where
the total error is at a minimum, and we cannot reduce h below that to improve the difference
approximation.
In order to find the minimum of the total error, we set dEtotal{dh “ 0 giving
dEtotal
dh “ 1
2
M ´ 2ε˚
h2 N “ 0 Ñ h˚ “ 2
cε˚N
M (5.12)
where h˚ is the optimum value of h.276  Numerical Methods for Scientists and Engineers
EXAMPLE 5.1: Effect of reducing h on approximate derivative.
Given fpxq “ x5, we wish to estimate f1
p0.4q using Eq. (5.4). (a) Investigate the
effect of reducing h on the accuracy of f1
p0.4q, and (b) find the optimum h˚.
SOLUTION:
(a) Since fpxq is explicitly given, the true f1
p0.4q and total error can be calculated.
For a specified h, the true error due to the use of the difference approximation is
determined by
Etotalphq “ ˇ
ˇf1
truep0.4q ´ f1
approxp0.4q
ˇ
ˇ
where f1
compp0.4q“p ˆfp0.4 ` hq ´ ˆfp0.4qq{h and since the machine-computed values
are inexact (i.e., floating point values), f’s in Eq. (5.4) are replaced with ˆf’s to
emphasize this distinction.
TABLE 5.1
h f1
compp0.4q Etruephq
10´1 0.2101 8.21000ˆ10´2
10´2 0.134562010 6.56201ˆ10´3
10´3 0.128641602 6.41602ˆ10´4
10´4 0.128064016 6.40160ˆ10´5
10´5 0.128006400 6.40016ˆ10´6
10´6 0.128000640 6.39997ˆ10´7
10´7 0.128000064 6.39856ˆ10´8
10´8 0.128000006 6.08049ˆ10´9
10´9 0.128000003 2.95798ˆ10´9
10´10 0.128000006 6.42743ˆ10´9
10´11 0.127999868 1.32350ˆ10´7
10´12 0.127996572 3.42832ˆ10´6
10´13 0.127935856 6.41436ˆ10´5
10´14 0.128022593 2.25925ˆ10´5
10´15 0.126634814 1.36519ˆ10´3
10´16 0.138777878 1.07779ˆ10´2
10´16.55 0.123100623 4.89938ˆ10´3
10´18 0 0.128
In Table 5.1, the computed derivative and true error are presented for a wide
range of increment size h. As h decreases down to h « 10´9, the true error also
decreases in parallel with the improvement in the computed approximation. Recall
that real numbers in digital computers are stored as rounded or truncated to the
nearest floating-point numbers, and they cannot be precisely represented on com￾puters. Hence, it is observed that the computed approximations lose accuracy for
very small h as the true differentiation error for h ă 10´9 begins to increase. The
reason for this is primarily the loss of significance. As a result of rounding due to
subtracting two nearly equal quantities, namely fp0.4 ` hq ´ fp0.4q, the computed
value loses significant digits. And then, to make matters worse, the inaccuracies are
magnified when this difference is divided by a very small number such as h. The
computation breaks down slightly below h « 10´16.55 “ 2.818 ˆ 10´17, so we mayNumerical Differentiation  277
take this value as the upper bound (ε˚ « 2.818 ˆ 10´17) for which no underflow
occurs.
In Fig. 5.4, a log-log plot of the total approximation error based on 200 values is
plotted as a function of h. The solid line represents the error estimate Etotalphq given
by Eq. (5.10). On the right side of the figure, where the truncation error dominates
the error, the theoretical straight line (=Mh{2) and the computed errors are in
complete agreement. However, on the left side of the figure, where the round-off errors
dominate, the computed values depict a jagged distribution just below the solid line,
and the data points are clearly not in line with the theoretical expectations. The
floating point arithmetic for the differences df (i.e., fp0.4`hq ´fp0.4q) breaks down
below ε˚. Because the small values df are set to “zero,” the error of the computed
data is horizontally aligned at log10p0.128q«´0.893 which is the logarithm of the
true value of the derivative.
FIGURE 5.4
(b) Finally, in order to compute the optimum value of the increment, we note
that M “ 20p0.4q
3 “ 1.28 and N “ p0.4q
5 “ 0.01024. Then, using Eq. (5.12), we
find
h˚ “ 2
cε˚N
M “ 2
c2.818 ˆ 10´17p0.01024q
1.28 “ 9.496 ˆ 10´10
which is consistent with the h˚ « 10´9 value in Table 5.1.
Discussion: The numerical differentiation process is highly sensitive to round-off
errors. Decreasing the interval size reduces the impact of the truncation error while
limiting the magnitude of the total error by the round-off error. That is why, as h is
reduced down to h˚, the finite difference approximation improves in accuracy. But
as h is reduced further below h˚, the finite difference approximations lose signifi￾cant digits due to the “loss of significance” phenomenon observed in such numerical
subtraction operations. Then the round-off errors begin to dominate the arithmetic
calculations, resulting in an increasing trend in the total error.
This analysis applies only to Eq. (5.4). You can investigate the effects of the
round-off errors by performing similar analyses for other difference formulas that
you would like to make use of in your work.278  Numerical Methods for Scientists and Engineers
5.2 FIRST-ORDER FINITE DIFFERENCE FORMULAS
Consider a uniformly-spaced discrete function fpxq on [x0, xn]: fpx0q, fpx1q,...,fpxnq with
xi “ ih for i “ 0, 1,...,n. Before going any further, we will adopt the following short-hand
notations:
fpxiq “ fi, fpxi ˘ hq “ fpxi˘1q “ fi˘1, fpxi ˘ 2hq “ fpxi˘2q “ fi˘2,
f1
pxiq “ f1
i , f 2pxiq “ f 2
i , fp4qpxiq “ fp4q
i , and so on
This notation not only helps abbreviate the finite difference expressions but also has a
convenient form that can be used to easily code in any computer language.
The finite difference formula for f1
pxiq, obtained by substituting xi for a in Eq. (5.8),
is referred to as the Forward Difference Formula (FDF) and is expressed in short-hand
notation as:
f1
i “ fi`1 ´ fi
h ` Ophq (5.13)
Finite-difference expressions can be further abbreviated and simplified with the use of dif￾ference operators, e.g., employing the forward difference operator, Eq. (5.13) becomes
f1
i “ Δfi
h ` Ophq (5.14)
where Δfi “ fi`1 ´ fi involves values of the function at xi and xi`1.
Now, consider the Taylor series of fpxi ´ hq about xi:
fpxi´hq “ fpxiq´h f1
pxiq` h2
2! f 2pxiq´ h3
3! f 3pxiq`...`p´1q
n hn
n!
fpnq
pxiq`¨¨¨ (5.15)
Solving Eq. (5.15) for f1
pxiq yields
f1
pxiq “ fpxiq ´ fpxi ´ hq
h `
h
2!f 2pxiq ´ h2
3! f 3pxiq ` h3
4! fp4q
pxiq´¨¨¨ (5.16)
Equation (5.16) can be expressed as follows:
f1
pxiq “ fpxiq ´ fpxi ´ hq
h ` Ophq (5.17)
Noting that xi´1 “ xi ´ h and using the subscript notation leads to
f1
i “ fi ´ fi´1
h ` Ophq (5.18)
which is called the Backward Difference Formula (BDF) due to the use of the discrete point
fi´1 behind fi. The truncation error of this alternative formula becomes Ephq “ hf 2pξq{2
for xi´1 ď ξ ď xi.
The backward difference operator ∇ operating as ∇fi “ fi ´ fi´1 abbreviates Eq.
(5.18) as
f1
i “ ∇fi
h ` Ophq (5.19)
So far, only the finite difference formulas for the first derivative have been derived. Similarly,
the difference formulas for f 2pxiq or higher derivatives could be developed through the use
of Taylor series.Numerical Differentiation  279
To derive a difference formula for f 2pxq, consider the Taylor series of fpxi ` 2hq about
xi:
fpxi`2hq“fpxiq`2hf1
pxiq` p2hq
2
2! f 2pxiq` p2hq
3
3! f 3pxiq`¨¨¨` p2hqn
n! fpnq
pxiq`¨¨¨ (5.20)
Setting a “ xi in Eq. (5.2) and using Eq. (5.20) to eliminate f1
pxiq leads to
f 2pxiq “ fpxi ` 2hq ´ 2fpxi ` hq ` fpxiq
h2 ´ hf 3pxiq ` ... (5.21)
or, in shorthand notation,
f 2
i “ fi`2 ´ 2fi`1 ` fi
h2 ` Ophq (5.22)
which is first-order accurate with the truncation error Ephq“´hf 3pξq for xi ď ξ ď xi`2h.
The numerator may be abbreviated as follows:
fi`2 ´ 2fi`1 ` fi “ pfi`2 ´ fi`1q´pfi`1 ´ fiq “ Δfi`1 ´ Δfi “ ΔpΔfiq “ Δ2fi
Implementing the short-hand notation in Eq. (5.22) results in
f 2
i “ Δ2fi
h2 ` Ophq (5.23)
or f 2
i leads to f 2
i “ ∇2fi{h2 ` Ophq for the backward difference formula.
Finite difference expressions for higher-order derivatives can be similarly developed.
For instance, the forward and backward difference formulas of the nth derivative of fpxq at
x “ xi with the order of accuracy of Ophq are expressed as
dnfi
dxn “ Δnfi
hn ` Ophq (5.24)
dnfi
dxn “ ∇nfi
hn ` Ophq (5.25)
5.3 SECOND-ORDER FINITE-DIFFERENCE FORMULAS
So far, we have discussed the derivation of the first-order accurate (backward and forward)
finite-difference formulas. It is possible to develop higher-order (Oph2q, Oph3q, etc.), more
accurate finite difference formulas by simply including more terms (leading term and so on)
from the Taylor series expansion.
Equation (5.3) is written for any discrete point, x “ xi, as
f1
pxiq “ fpxi ` hq ´ fpxiq
h ´ h
2!f 2pxiq ´ h2
3! f 3pxiq ´ h3
4! fp4q
pxiq´¨¨¨ (5.26)
Replacing f 2pxiq by Eq. (5.21) leads to
f1
pxiq “ fpxi`hq´fpxiq
h ´ h
2
˜
fpxi`2hq´2fpxi`hq`fpxiq
h2 ´hf 3pxiq`...¸
´h2
6 f 3pxiq ` ...
(5.27)280  Numerical Methods for Scientists and Engineers
FIGURE 5.5: Graphical depiction of data points used in (a) forward
and (b) backward differences of f 2pxiq.
Collecting the common terms and simplifying yields
f1
pxiq “ ´fpxi ` 2hq ` 4fpxi ` hq ´ 3fpxiq
2h ´ h2
3 f 3pxiq ` ... (5.28)
where the new leading error term is clearly ´h2f 3pxiq{3; in other words, this formulation
is second-order accurate, i.e., Oph2q. But this formula requires two additional data points
besides fpxiq, namely fpxi ` hq and fpxi ` 2hq (see Fig. 5.5a). The truncation error is cast
as Ephq“´h2f 3pξq{3, where xi ď ξ ď xi ` 2h.
Finally, the second-order forward difference formula, in shorthand notation, for f1
i can
be expressed as
f1
i “ ´fi`2 ` 4fi`1 ´ 3fi
2h ` Oph2q (5.29)
A second-order backward difference formula for f1
i requires fpxi ´ hq and fpxi ´ 2hq
points on the left of xi (see Fig. 5.5b). Similarly, we expand fpxi ´ hq and fpxi ´ 2hq into
Taylor series about xi to find a backward differentiation formula for f 2pxiq. Substituting
the BDF for f 2pxiq into Eq. (5.16) leads to
f1
i “ 3fi ´ 4fi´1 ` fi´2
2h ` Oph2q (5.30)
where the truncation error is Ephq “ h2f 3pξq{3 for xi´2 ď ξ ď xi. Note that when the
interval size is reduced by a factor of 2, the truncation error is reduced by a factor of 4.
First-order accurate difference formulas for f1
pxiq involve a data point either on the
right (xi`1, fi`1) or left side (xi´1, fi´1) of the point pxi, fiq, where the derivative is evalu￾ated. As the order of accuracy of a finite difference formula increases, the number of data
points required in the formula also increases. For example, three data points about the
point pxi, fiq are needed to develop a finite difference formula of order Oph3q. These points
in forward or backward difference formulas are distributed on the right or left sides of xi,
respectively.
When dealing with a set of discrete data, we do not know whether
the data is generated from a continuous or piecewise continuous
function. The function, in fact, may not be differentiable at some
points in its domain. Hence, numerical differentiation must always
be handled with caution!Numerical Differentiation  281
5.4 CENTRAL DIFFERENCE FORMULAS
In central difference formulas, the points are distributed about xi in symmetric pairs. Con￾sider the Taylor series of fpxi ` hq and fpxi ´ hq about xi given by Eqs. (5.2) and (5.15),
respectively. Subtracting Eq. (5.15) from Eq. (5.2) and solving for f1
pxiq yields
f1
pxiq “ fpxi ` hq ´ fpxi ´ hq
2h ´ h2
6 f 3pxiq ´ h4
120fp5q
pxiq... (5.31)
or
f1
i “ fi`1 ´ fi´1
2h ` Oph2q (5.32)
where the truncation error is Ephq“´h2f 3pξq{6 for xi´1 ď ξ ď xi`1.
Now, adding Eqs. (5.2) and (5.15) side by side leads to
fpxi ` hq ` fpxi ´ hq “ 2fpxiq ` h2 f 2pxiq ` h4
12 fp4q
pxiq ` ... (5.33)
Solving Eq. (5.33) for f 2pxiq gives
f 2pxiq “ fpxi ` hq ´ 2fpxiq ` fpxi ´ hq
h2 ´ h2
12 fp4q
pxiq ´ ... (5.34)
or
f 2
i “ fi`1 ´ 2fi ` fi´1
h2 ` Oph2q (5.35)
where the truncation error is Ephq“´h2fp4qpξq{12 for xi´1 ď ξ ď xi`1.
Equations (5.32) and (5.35), which are second-order accurate finite difference formulas,
are referred to as the Central Difference Formulas (CDFs). More compact expressions,
similar to forward and backward differences, can be obtained by incorporating the central
difference operator δ, which is defined as
δfi “ fi`1{2 ´ fi´1{2
It is possible to show that δ2fi “ δfi`1{2 ´ δfi´1{2 “ fi`1 ´ 2fi ` fi´1. Then, Eq. (5.35)
can be expressed as
f 2
i “ δ2fi
h2 ` Oph2q (5.36)
A graphical depiction of the data points used, along with the predicted slopes with
the forward, backward, and central difference formulas for f1
pxiq, is presented in Fig. 5.6.
The slopes of the secant lines, estimated by the forward and backward differences, provide
a visual depiction of the significant deviations from the true slope. But the slope estimated
with the central difference formula (i.e., the slope of the line passing through pxi´1, fi´1q
and pxi`1, fi`1q points) clearly yields much better agreement with the slope of the true
tangent line. Thus, so long as the number of data points is sufficient, the central difference
formulas should be preferred due to their second-order accuracy. Note that the errors in the
derivatives (i.e., slopes) can be spotted visually, but their magnitudes cannot.
For high-order derivatives, the central difference formulas can be generalized in terms
of the forward and/or backward difference operators as
dnfi
dxn “
$
’&
’%
1
2hn
`
∇nfi`n{2 ` Δnfi´n{2
˘
, if n is even
1
2hn
`
∇nfi`pn´1q{2 ` Δnfi´pn´1q{2
˘
, if n is odd
,
/.
/-
` Oph2q (5.37)282  Numerical Methods for Scientists and Engineers
FIGURE 5.6: Graphical depiction of forward, backward, and central
difference approximations for f1
i .
EXAMPLE 5.2: Order of accuracy and truncation error
Use Taylor series expansion to determine the derivative represented by the following
approximation and find the truncation error and order of accuracy.
´3fi´2 ´ 25fi ` 30fi`1 ´ 2fi`3
30h “? Oph?q
SOLUTION:
(a) The Taylor series of fpxi`khq about xi can be generalized as
fi`k “ fi ` kh f1
i ` pkhq2
2! f 2
i ` pkhq3
3! f 3
i ` pkhq
4
4! fp4q
i ` ...
where k is an integer denoting the number of increments (k ą 0) or decrements
(k ă 0). Then the Taylor series of fpxi´2q, fpxi`1q, and fpxi`3q about xi are found
by setting k “ 1, 3, and ´2 to give
fi`1 “ fi ` h f1
i `
h2
2 f 2
i `
h3
6 f 3
i `
h4
24 fp4q
i `
h5
120fp5q
i ` ...
fi`3 “ fi ` 3h f1
i `
9h2
2! f 2
i `
9h3
2 f 3
i `
27h4
8 fp4q
i `
81h5
40 fp5q
i ` ...
fi´2 “ fi ´ 2h f1
i ` 2h2f 2
i ´ 4h3
3 f 3
i `
2h4
3 fp4q
i ´ 4h5
15 fp5q
i ` ...
Substituting the above expressions into the numerator and collecting the common
terms leads to
´3fi´2 ´ 25fi ` 30fi`1 ´ 2fi`3 “ 30h f1
i ´ 15h4
2 fp4q
i ´ 3h5fp5q
i ` ...
where the lowest order derivative on the rhs is f1
i . Dividing it by 30h gives
f1
i “ ´3fi´2 ´ 25fi ` 30fi`1 ´ 2fi`3
30h `
h4
4 fp4q
i `
h5
10 fp5q
i ` ...Numerical Differentiation  283
The first term of this expression is identical to the finite difference formula given in
the problem statement; thus, we conclude that the difference formula is that of the
first derivative (f1
i ).
The remaining terms shed light on the truncation error. The leading (truncation)
error is Ephq “ h4fp4qpξq{4 for xi´2 ď ξ ď xi`3, and the difference formula is fourth￾order accurate due to having h4 factor, i.e., Oph4q.
Discussion: The Taylor series can be effectively used to derive approximations
for any derivative and help determine the truncation error term and the order of
accuracy. The finite difference formulas are generally referred to by their order of
accuracy. A high value indicates faster convergence to the true value with decreasing
h.
Higher-order approximations for the higher derivatives could
plague the solution with noise since higher-order derivatives tend
to be larger, especially near sharp gradients.
5.5 FINITE DIFFERENCES AND DIRECT-FIT POLYNOMIALS
Finite difference formulas can alternatively be derived with the aid of polynomials. This
approach requires fitting a set of function values to obtain an approximating polynomial
and then differentiating the fitted polynomial.
A function to be fitted (or approximated) by a polynomial is assumed to be continuous,
differentiable, and have exact values at the furnished discrete points. A polynomial fit can
then be achieved in a variety of ways, such as by either directly fitting a set of discrete
data to a polynomial, by using Lagrange polynomials (see Chapter 6), or by approximating
with least squares polynomial fit (see Chapter 7). In this section, we will rather focus on
applying the direct-fit polynomials to obtain finite difference formulas.
To generate numerical differentiation formulas by direct-fit polynomials, follow the
three-step procedure given below:
1. Establish an nth-degree polynomial, pnpxq “ anxn ` an´1xn´1 ... ` a1x ` a0;
2. Force pnpxq to be equal to fpxq at n ` 1 data points or nodes, i.e., setting fpxiq “
pnpxiq “ fi for i “ 0, 1, 2,...,n gives a linear system of n ` 1 equations for the
undetermined coefficients (a0, a1,...,an);
3. Find an approximation to fpkq
pxiq (k ď n) by setting it to the kth derivative of pnpxiq,
i.e., fpkq
pxiq “ p
pkq n pxiq;
Here, the distribution of xi’s depends on the type of difference formulas desired to be
derived, i.e., forward, backward, or central.
Once the type of difference formula is decided, the other parameter that we can con￾trol is the degree of the approximating polynomial. Deciding on the degree of an approx￾imating polynomial is an important step in this process. In the preceding sections, it was
shown that f1
pxiq computed by the two-point difference formulas (FDF or BDF) led to
first-order accurate approximations, i.e., Ophq. On the other hand, difference formulas for
f 2pxiq, even as Ophq, required three points. This means that the second derivative cannot be284  Numerical Methods for Scientists and Engineers
FIGURE 5.7: A polynomial approximation to a discrete data.
approximated with linear relationships like the first derivatives, and an approximating poly￾nomial of the lowest degree representing fpxq should be a parabola.
There is a relationship between the desired order of accuracy in the finite difference
formulas and the degree of interpolating polynomials. For example, consider the nth-degree
accurate forward difference approximation for f1
paq, Eq. (5.3), whose truncation error is
expressed as
Ephq “ hn
pn ` 1q!
fpn`1q
pξq, a ď ξ ď a ` h
which implies that, if fpxq is a polynomial of nth degree, the finite difference approximation
to any kth derivative fpkq
pxiq (for k ď n) will be exact since dn`1f{dxn`1 “ 0. This
deduction can be used to determine finite difference formulas of the desired order of accuracy,
i.e., the degree of approximating polynomial.
The order of truncation error of a function approximated by an nth-degree polyno￾mial (fpxq – pnpxq) is Ophn`1q. The order of accuracy of f1
pxiq becomes Ophnq, which
also requires (n ` 1) data points in the difference formula to attain this accuracy. Differ￾ent relationships for higher-order derivatives apply; for instance, the degree of a direct fit
approximating polynomial should be n ` k ´ 1 in order to find a finite difference formula
for the kth derivative with an accuracy order of Ophnq. Once the degree (n) of an approxi￾mating polynomial is decided, pnpxq should satisfy (n ` 1) data points. This step leads to
a system of simultaneous linear equations with (n ` 1) unknowns for the coefficients of the
polynomial. Finally, the fpkq
pxq derivative of fpxq is approximated by the p
pkq n pxq derivative.
Consider deriving a second-order accurate forward difference formula, Oph2q, for f1
pxiq
using a direct-fit polynomial. For this case, an approximating polynomial is a second-degree
polynomial (i.e., a parabola).
fpxq – p2pxq “ ax2 ` bx ` c, xi ď x ď xi`2 (5.38)
In Figure 5.7, an arbitrary function fpxq and an approximating polynomial p2pxq are
depicted. The parabola is forced to pass through the uniformly spaced three data points:
pxi, fiq, pxi`1, fi`1q, and pxi`2, fi`2q. We apply a shifted coordinate system by setting
xi “ 0, xi`1 “ h, and xi`2 “ 2h to simplify the derivation process while maintaining
generality. Matching the function and the parabola for all three data points leads to the
following system of linear equations with three unknowns (a, b, c):
fi “ fpxiq “ p2p0q “ ap0q
2 ` bp0q ` c
fi`1 “ fpxi`1q “ p2phq “ aphq
2 ` bphq ` c
fi`2 “ fpxi`2q “ p2p2hq “ ap2hq
2 ` bp2hq ` c
(5.39)Numerical Differentiation  285
TABLE 5.2: The order of accuracy of derivatives with symmetrical (central) and non￾symmetrical (forward/backward) finite difference formulas.
Non-symmetric Differences Symmetric Differences
fi “ pnpxiq fi “ pnpxiq
f1
i – p1
npxiq Ophnq f1
i – p1
npxiq Ophnq
f 2
i – p2
npxiq Ophn´1q f 2
i – p2
npxiq Ophnq
f 3
i – p3
n pxiq Ophn´2q f 3
i – p3
n pxiq Ophn´2q
fp4q
i – p
p4q n pxiq Ophn´3q fp4q
i – p
p4q n pxiq Ophn´2q
.
.
. .
.
. .
.
. .
.
. .
.
. .
.
.
fpn´1q
i – p
pn´1q n pxiq Oph2q fpn´1q
i – p
pn´1q n pxiq Oph2q
fpnq
i – p
pnq n pxiq Ophq fpnq
i – p
pnq n pxiq Oph2q
Solving Eq. (5.39) for a, b, and c yields
a “ fi ´ 2fi`1 ` fi`2
2h2 , b “ ´3fi ` 4fi`1 ´ fi`2
2h , c “ fi (5.40)
Having found the direct-fit polynomial, we now turn our attention to finding the dif￾ference formulas for f1
pxiq and f 2pxiq. We achieve this by matching the first and sec￾ond derivatives of fpxq and p2pxq at xi “ 0; that is, f1
pxiq “ f1
p0q – p1
2p0q “ b and
f 2pxiq “ f 2p0q – p2
2 p0q “ 2a. Finally, using the coefficients from Eq. (5.40), we obtain
f1
i – b “ ´fi`2 ` 4fi`1 ´ 3fi
2h (5.41)
f 2
i – 2a “ fi ´ 2fi`1 ` fi`2
h2 (5.42)
Note that Eqs. (5.41) and (5.42) are the same finite difference formulas derived from the
Taylor series—Eqs. (5.29) and (5.22). Also note that the order of accuracy of f1
i and f 2
i
is Oph2q and Ophq, respectively. Each successive differentiation (of a direct-fit polynomial)
reduces the order of the associated truncation error. The drop in the order of accuracy
of difference formulas also depends on how the data points are distributed around the
point where the numerical derivative is sought. Using the pnpxq to evaluate higher-order
derivatives, one order of accuracy is lost for each increasing order for derivatives of non￾symmetrical (forward or backward) difference formulas. In symmetrical (central) difference
formulas, two orders of accuracy are lost for derivatives increasing by multiples of two due
to points being added in symmetric pairs. The order of accuracy for the derivatives for
symmetrical or non-symmetrical finite difference formulas is illustrated in Table 5.2.
Table 5.3 summarizes the central difference formulas of order of accuracy Oph2q and
Oph4q for derivatives up to 4th order. The CDFs should be used whenever there is a sufficient
number of (also symmetric) data points on the left and right sides of a point pxi, fiq. The
forward and backward difference formulas of order of accuracy Ophq, Oph2q, and Oph4q are
presented for derivatives up to the 4th order in Tables 5.4 and 5.5. The FDFs are used when
there are no data points on the left of pxi, fiq, while the BDFs are used when data points
are stacked on the left of pxi, fiq.286  Numerical Methods for Scientists and Engineers
TABLE 5.3: The central difference formulas of order of accuracy Oph2q and Oph4q for derivatives up
to fourth order.
Difference Formula Ephq
f1
i –
1
2h pfi`1´fi´1q ´1
6
f 3pξqh2
–
1
12h p´fi`2`8fi`1´8fi´1`fi´2q 1
30 fp5qpξqh4
f 2
i –
1
h2 pfi`1´2fi`fi´1q ´ 1
12 fp4qpξqh2
–
1
12h2 p´fi`2`16fi`1´30fi`16fi´1´fi´2q 1
90 fp6qpξqh4
f 3
i –
1
2h3 pfi`2´2fi`1`2fi´1´fi´2q ´1
4
fp5qpξqh2
–
1
8h3 p´fi`3`8fi`2´13fi`1`13fi´1´8fi´2`fi´3q 7
120 fp7qpξqh4
fp4q
i –
1
h4 pfi`2´4fi`1`6fi´4fi´1`fi´2q ´1
6
fp6qpξqh2
–
1
6h4 p´fi`3`12fi`2´39fi`1`56fi´39fi´1`12fi´2´fi´3q
7
240 fp8qpξqh4
EXAMPLE 5.3: Estimate first derivative with difference formulas
Given fpxq “ ex, for decreasing values of h, estimate f1
p1q and the associated trun￾cation error with the first-order FDF, second-order CDF, and fourth-order CDF
formulas. How do truncation errors compare to true errors?
SOLUTION:
First-, second-, and fourth-order difference formulas, as well as their truncation
errors, for f1
i are given in Tables 5.3 and 5.4. To estimate the truncation error, f 2pξq,
f 3pξq, and fp5qpξq derivatives are required. Note that eξ is an increasing function for
all ξ, and all high-order derivatives give fpxq “ ex. The absolute maximum of the
derivatives occurs at the rightmost end of the pertinent intervals, i.e., ξ “ 1 ` h for
FDF and 2th-order CDF and ξ “ 1`2h for the 4th-order CDF. Then, the difference
formulas and the corresponding truncation errors can be expressed as
f1
FDFp1q “ e1`h ´ e
h , Ephq“´h
2
e1`h
f1
CDFp1q “ e1`h ´ e1´h
2h , Ephq“´h2
6
e1`h
f1
CDFp1q “ ´e1`2h ` 8e1`h ´ 8e1´h ` e1´2h
12h , Ephq “ h4
30
e1`2h
Since, in this example, fpxq is explicitly given, the true value and the associated
true error can be calculated as f1
truep1q “ e and Etruephq “ |f1
truephq ´ f1
approxphq|,
respectively. In Table 5.6, the numerical estimates of f1
p1q obtained by the first￾order FDF, second-order CDF, and fourth-order CDF, along with the associatedNumerical Differentiation  287
TABLE 5.4: The forward difference formulas of order of accuracy Ophq, Oph2q, and Oph4q for the
derivatives up to fourth order.
Difference Formula Ephq
f1
i –
1
h pfi`1 ´ fiq ´1
2
f 2pξqh
–
1
2h p´fi`2`4fi`1´3fiq 1
3
f 3pξqh2
–
1
12h p´3fi`4`16fi`3´36fi`2`48fi`1´25fiq 1
5 fp5q
pξqh4
f 2
i –
1
h2 pfi`2´2fi`1`fiq ´f 3pξq h
–
1
h2 p´fi`3`4fi`2´5fi`1`2fiq 11
12 fp4q
pξqh2
f 3
i –
1
h3 pfi`3´3fi`2`3fi`1´fiq ´3
2
fp4qpξqh
–
1
2h3 p´3fi`4`14fi`3´24fi`2`18fi`1´5fiq 7
4
fp5qpξqh2
fp4q
i –
1
h4 pfi`4´4fi`3`6fi`2´4fi`1`fiq ´2fp5qpξq h
–
1
h4 p´2fi`5`11fi`4´24fi`3`26fi`2´14fi`1`3fiq
17
6 fp6qpξq h2
TABLE 5.5: The backward difference formulas of order of accuracy Ophq, Oph2q, and Oph4q for the
derivatives up to fourth order.
Difference Formula Ephq
f1
i –
1
h pfi´fi´1q 1
2
f 2pξqh
1
2h p3fi´4fi´1`fi´2q 1
3
f 3pξqh2
–
1
12h p25fi´48fi´1`36fi´2´16fi´3`3fi´4q 1
5
fp5q
pξqh4
f 2
i –
1
h2 pfi´2fi´1`fi´2q f 3pξq h
–
1
h2 p2fi´5fi´1`4fi´2´fi´3q 11
12 fp4qpξqh2
f 3
i –
1
h3 pfi´3fi´1`3fi´2´fi´3q 3
2
fp4qpξqh
–
1
2h3 p5fi´18fi´1`24fi´2´14fi´3`3fi´4q 7
4
fp5qpξqh2
fp4q
i –
1
h4 pfi´4fi´1`6fi´2´4fi´3`fi´4q 2fp5qpξq h
–
1
h4 p3fi´14fi´1`26fi´2´24fi´3`11fi´4´2fi´5q
17
6 fp6qpξq h2288  Numerical Methods for Scientists and Engineers
truncation errors and the true (absolute) errors, are tabulated for decreasing values
of h. Notice that the truncation errors are always greater than the true errors, as
the interval size h is kept in the region dominated by truncation errors (h ă h˚).
TABLE 5.6
h f1
approxp1q |Ephq| True error
First-order FDF
10´1 2.85884195 0.1502108 0.1405601
10´2 2.73191866 0.0137280 0.0136368
10´3 2.71964142 0.0013605 0.0013596
10´4 2.71841775 0.0001359 0.0001359
Second-order CDF
10´1 2.72281456 5.0069ˆ10´3 4.5327ˆ10´3
10´2 2.71832713 4.5760ˆ10´5 4.5305ˆ10´5
10´3 2.71828228 4.5350ˆ10´7 4.5304ˆ10´7
Fourth-order CDF
10´1 2.71827276 1.1067ˆ10´5 9.0717ˆ10´6
10´2 2.71828183 9.2440ˆ10´10 9.0609ˆ10´10
A log-log graph of the true absolute error as a function of h is presented in Fig. 5.8.
Since the truncation error is proportional to hm, a curve representing the true error
should be a straight line with a slope giving the order of the difference formula, i.e., m.
For instance, for h “ 0.1, the 10-based logarithm of true absolute errors with the 1st,
2nd, and 4th-order difference formulas is ´0.852, ´2.344, and ´5.042, respectively.
These values, respectively, are realized as ´1.855, ´4.344, and ´9.042 for h “ 0.01.
The slopes of the lines are found from m “ log10Etruep10´1q ´ log10Etruep10´2q
which verifies the order of each finite difference equation as 1, 2, and 4.
FIGURE 5.8Numerical Differentiation  289
Highly accurate approximations for the derivatives can be achieved when using
high-order difference formulas (order of accuracy ą 2). As h is decreased (moving
from right to left in Fig. 5.8) in the region dominated by truncation error, the log￾errors decrease linearly, most rapidly for the fourth-order formula. But the effect of
round-off errors becomes evident as h is further decreased. After passing a minimum
point, the total error begins to rise in a jagged fashion and eventually dominates the
total errors. As the order of accuracy of the approximation increases, the crossover
point moves to the right (greater efficiency) and down (greater accuracy).
Discussion: The truncation error, if calculated fairly accurately, provides a con￾servative means of estimating the computational errors resulting from finite differ￾ence formulas. In practice, the true distribution of a discrete function is not known.
Therefore, although it is not possible to calculate the true error, the magnitude of
the truncation error can be roughly estimated.
In this example, the effects of rounding on the difference formulas are observed
in Fig. 5.8. We note that the use of the second-order finite difference formula for
h À 10´5 and the fourth-order finite difference formula for h À 10´3 is contaminated
with round-off errors, which lead to an increase in the total error. Hence, overall,
h should be chosen “small enough” so that the finite difference formulas produce
accurate approximations and “large enough” so that the approximations are not too
contaminated by round-off errors.
EXAMPLE 5.4: Calculate speed and acceleration from a dataset
The Vehicle Acceleration Test (VAT) result of a minivan is given below as distance￾time data recorded at 5-second intervals. The van, initially at rest, reaches its max￾imum speed in 25 seconds. Estimate the speed (m/s) and acceleration (m/s2) for
each available data point using second-order accurate finite-difference formulas.
t (s) 0 5 10 15 20 25
s (m) 0.0 5.45 21.3 82.84 212.86 473.6
SOLUTION:
The distance, sptq, is provided by a set of uniformly spaced (Δt “ 5 s) discrete
data points. The speed and acceleration can only be estimated for t “ 0, 5, 10, 15,
20, and 25 seconds since no other data is available in the set. We first express the
available in an array form (ti, si). For 1 ď i ď 6, we may write pt1, s1q“p0, 0q,
pt2, s2q“p5, 5.45q, and so on. Note that since Δt is uniform, there is no need to
establish the array ti for the time variable.
By definition, the speed is the derivative of distance with respect to time, vptq “
ds{dt, and the acceleration is the second derivative of distance with respect to time,
aptq “ d2s{dt2. To estimate the speed and acceleration with an accuracy of order
Oph2q, the central difference formulas will be applied for the interior data points
(2 ď i ď 5):
vi “ dsi
dt “ si`1 ´ si´1
2 Δt , ai “ d2si
dt2 “ si`1 ´ 2si ` si´1
pΔtq2290  Numerical Methods for Scientists and Engineers
Ar t “ 0 (or i “ 1), when the minivan is at rest, we will apply the forward
difference formula of order Oph2q since no data prior to t “ 0 (i ă 1) is available.
Thus, the speed and acceleration at t “ 0 (i “ 1) will be calculated as
v1 “ dsi
dt “ ´s3 ` 4s2 ´ 3s1
2Δt , a1 “ d2si
dt2 “ ´s4 ` 4s3 ´ 5s2 ` 2s1
pΔtq
2
When the minivan reaches its maximum speed at t “ 25 s (or i “ 6), we will use
the backward difference formula of order Oph2q since no other data is available for
t ą 25 s. Hence, the speed and acceleration at t “ 25 s will be calculated from
v6 “ ds6
dt “ s4 ´ 4s5 ` 3s6
2Δt , a6 “ d2s6
dt2 “ ´s3 ` 4s4 ´ 5s5 ` 2s6
pΔtq
2
Substituting the numerical values into the difference formulas, the speeds at t “ 0
and t “ 25 s are found as
v1 “ ´s3 ` 4s2 ´ 3s1
2Δt “ ´21.3 ` 4p5.45q ´ 3p0q
2p5q “ 0.05 m
s
v6 “ s4 ´ 4s5 ` 3s6
2Δt “ 82.84 ´ 4p212.86q ` 3p473.6q
2p5q “ 65.22 m
s
For the interior data points (2 ď i ď 5), the speed values are found as
v2 “ s3 ´ s1
2Δt “ 21.3 ´ 0
2p5q “ 2.13 m
s
v3 “ s4 ´ s2
2Δt “ 82.84 ´ 5.45
2p5q “ 7.739 m
s
v4 “ s5 ´ s3
2Δt “ 212.86 ´ 21.3
2p5q “ 19.156 m
s
v5 “ s6 ´ s4
2Δt “ 473.6 ´ 82.84
2p5q “ 39.076 m
s
The accelerations at i “ 1 and i “ 6 are computed with the 2nd-order FDF and
BDF, respectively. Using the pertinent difference formulas from Tables 5.3 and 5.5
we find
a1 “ ´s4`4s3´5s2`2s1
pΔtq
2 “ ´82.84 ` 4p21.3q ´ 5p5.45q ` 2p0q
52 “´0.9956 m
s2
a6 “ ´s3`4s4´5s5`2s6
pΔtq
2 “ ´21.3 ` 4p82.84q ´ 5p212.86q ` 2p473.6q
52 “7.7184 m
s2
For the interior data points (2 ď i ď 5), using the 2nd-order CDF, the accelera￾tion values yieldNumerical Differentiation  291
a2 “ s3´2s2`s1
pΔtq
2 “ 21.3 ´ 2p5.45q ` 0
52 “ 0.416 m
s2
a3 “ s4 ´ 2s3 ` s2
pΔtq
2 “ 82.84 ´ 2p21.3q ` 5.45
52 “ 1.8276 m
s2
a4 “ s5 ´ 2s4 ` s3
pΔtq
2 “ 212.86 ´ 2p82.84q ` 21.3
52 “ 2.7392 m
s2
a5 “ s6 ´ 2s5 ` s4
pΔtq
2 “ 473.6 ´ 2p212.86q ` 82.84
52 “ 5.2288 m
s2
The estimated maximum speed and maximum acceleration are 65.22 m/s (or 234.8
km/h) and 7.6184 m/s, respectively. A sample pseudo-program to calculate the speed
and acceleration for this example is provided in Pseudocode 5.1.
Discussion: When the vehicle is at rest, the speed and acceleration must be zero.
However, at t “ 0, we could not obtain the velocity and acceleration values, which
should have been “zero,” with the finite difference formulas. One of the reasons for
this result is due to truncation errors stemming from a large data sampling interval
(Δt “ 5 s). A smaller data sampling interval (Δt ă 5) would improve the accuracy
of calculations.
Finite-Difference Formulas
‚ Finite-difference formulas are simple and easy to use;
‚ Higher-order finite difference formulas can be easily generated using
difference operators;
‚ For a given h, 2nd- or 4th-order difference formulas offer a descent
trade-off between accuracy and the number of function evaluations;
‚ Derivatives of explicit functions can be accurately computed by
increasing the order of accuracy (i.e., data points).
‚ The truncation error is a function of interval size, Ophnq, and it has
a strong influence on the accuracy of differentiation;
‚ Difference formulas are plagued with round-off and loss of signifi￾cance errors when h « 0;
‚ When high-order difference formulas are used, there is no need to
use a very small h to get a very good estimate; however, such for￾mulas avoid floating-point problems at the expense of the increased
number of function evaluations.
Higher derivatives can also be numerically calculated by successive
differentiations: f 2 “ pf1
q1
, f 3 “ pf 2q1
, etc. This approach causes
the uncertainties already present in the original data, as well
as the total errors in the computed quantities, to be propagated
to the next level. In this regard, a prudent approach would be to
use the original discrete data set with the suitable finite difference
formulas of the desired order of accuracy.292  Numerical Methods for Scientists and Engineers
Pseudocode 5.1
Program EXAMPLE_5.4
\ DESCRIPTION: A pseudocode to calculate the speed and acceleration using
\ a set of distance data given in Example 5.4.
\ VARIABLES:
\ n :: Number of data in the set;
\ Δt :: Time interval (s);
\ S :: Array of length n containing distance (m);
\ v :: Array of length n containing speed (m/s)
\ a :: Array of length n containing acceleration (m/s2).
Declare: Sn, vn, an \ Declare array variables
Read: n, Δt, p Si, i “ 1, nq \ Read data into program
\ FD formulas of order Oph2q are used for all data points.
v1 Ð p´S3 ` 4S2 ´ 3S1q{p2Δtq \ Use FDF for v at i “ 1
a1 Ð p2S1 ´ 5S2 ` 4S3 ´ S4q{pΔtq
2 \ Use FDF for a at i “ 1
For “
i “ 2,pn ´ 1q
‰ \ Loop for finding v and a at interior points
vi Ð pSi`1 ´ Si´1q{p2Δtq \ Use CDF for v
ai Ð pSi`1 ´ 2Si ` Si´1q{pΔtq
2 \ Use CDF fora
End For
vn Ð p3Sn ´ 4Sn´1 ` Sn´2q{p2Δtq \ Use BDF for v at i “ n
an Ð p2Sn ´ 5Sn´1 ` 4Sn´2 ´ Sn´3q{pΔtq
2 \ Use BDF for a at i “ n
Write: pti, Si, vi, ai, i “ 1, nq \ Print out the results
End Program EXAMPLE_5.4
5.6 DIFFERENTIATING NON-UNIFORMLY SPACED DISCRETE DATA
In practice, a set of discrete data collected or generated may not always be uniformly spaced,
in which case the finite-difference formulas given in Tables 5.3´5.5 cannot be used.
In general, the approximations for differentiating non-uniformly spaced data sets are
derived using polynomials: direct-fit, Lagrange, or divided differences. In this section, we
will illustrate the use of direct-fit polynomials, which is just as in uniform cases, based on
fitting the data directly with a polynomial of nth degree, pnpxq, and differentiating the
polynomial as many times as necessary.
There are situations, such as in the numerical solution of differential equations, where an
analyst does not wish the discrete data to be uniform. It is efficient to work with uniformly
FIGURE 5.9: (a) Uniform, (b) non-uniform data set near steep changes.Numerical Differentiation  293
FIGURE 5.10: Non-uniform discrete data points for (a) forward, (b) backward, (c) central differences.
spaced discrete data when the rate of change in the solution of the differential equation
is small over a wide range, in which case the change in a physical quantity can be well
represented by low-order polynomials. A continuous function fpxq containing a sharp rise
near x1 and discrete data sets generated from fpxq with uniform and non-uniform intervals
are shown in Fig. 5.9. Notice that fpxq is almost linear for x3 ă x ă x6; hence, the rate
of change can be estimated fairly accurately with low-order polynomials. However, the
derivatives of functions that exhibit steep or sharp changes (e.g., for x ă x3), calculated
based on uniform distributions, cannot be estimated very accurately by low-order finite
difference formulas.
To adequately capture or represent important features of any discrete distribution
(velocity, temperature, concentration gradients, etc.), analysts deliberately stack the data
points in regions where steep or sharp changes are expected (see Fig. 5.9b). Hence, suitable
difference formulas need to be generated to accommodate the use of non-uniformly spaced
data sets.
Consider the non-uniformly spaced discrete data points shown in Fig. 5.10. To derive
second-order finite difference approximations for the case depicted in Fig. 5.10a, we adopt
a quadratic approximating polynomial: p2pxq “ ax2 ` bx ` c. Using the shifted coordinate
system and setting xi “ 0, xi`1 “ h1, and xi`2 “ h1 ` h2, the data points must satisfy the
approximating polynomial:
fpxiq “ fi – p2p0q “ c
fpxi`1q “ fi`1 – p2ph1q “ ah2
1 ` bh1 ` c
fpxi`2q “ fi`2 – p2ph1 ` h2q “ aph1 ` h2q
2 ` bph1 ` h2q ` c
(5.43)
which constitute a system of three equations with three unknowns: a, b, and c. Since f1
i –
p1
2pxiq “ p1
2p0q “ b, the forward-difference formula for the first derivative is obtained as
f1
i “ ´ 2h1 ` h2
h1ph1 ` h2q
fi `
h1 ` h2
h1h2
fi`1 ´ h1
h2ph1 ` h2q
fi`2 (5.44)
Similarly, the second derivative is found by matching p2
2p0q “ f 2
i “ 2a, leading to
f 2
i “ 2
h1ph1 ` h2q
fi ´ 2
h1h2
fi`1 `
2
h2ph1 ` h2q
fi`2 (5.45)
Note that setting h1 “ h2 “ h yields the expressions for the uniformly spaced discrete data.
Equations (5.44) and (5.45) do not provide much information on the truncation error
or the order of accuracy. This issue can be resolved by making use of the Taylor series, as294  Numerical Methods for Scientists and Engineers
in the case of uniform discrete functions. For this case, the truncation error and order of
accuracy with the forward difference formulations of the first and second derivatives are
given below:
For f1
i : Eph1, h2q “ f 3pξq
6
h1ph1 ` h2q ” Oph2
1 ` h1h2q, xi ď ξ ď xi`2
For f 2
i : Eph1, h2q“´f 3pξq
3 p2h1 ` h2q ” Op2h1 ` h2q
The central and backward-difference formulas for f1
i and f 2
i are similarly obtained by
using the shifted coordinates as xi´1 “ ´h1, xi “ 0, xi`1 “ h2 (see Fig. 5.10b) and xi “ 0,
xi´1 “ ´h1, xi´2 “ ´ph1 ` h2q (see Fig. 5.10c), respectively. Matching the derivatives of
the approximating polynomial, f1
i – p1
2p0q “ b and f 2
i “ p2
2p0q “ 2a, we find for the central
difference formulas:
f1
i “ ´ h2
h1ph1 ` h2q
fi´1 `
ˆ 1
h1
´ 1
h2
˙
fi `
h1
h2ph1 ` h2q
fi`1 (5.46)
f 2
i “ 2
h1ph1 ` h2q
fi´1 ´ 2
h1h2
fi `
2
h2ph1 ` h2q
fi`1 (5.47)
The truncation errors for the central difference formulas, Eqs. (5.46) and (5.47), which
are derived using the Taylor series in a similar fashion, yield
For f1
i : Eph1, h2q“´f 3pξq
6
h1h2 ” Oph1h2q pxi´1 ď ξ ď xi`1q
For f 2
i : Eph1, h2q “ f 3pξq
3 ph1 ´ h2q ´ fp4qpξq
12 ph2
1 ´ h1h2 ` h2
2q ` ...
” Oph1 ´ h2q ` Oph2
1 ´ h1h2 ` h2
2q
Note that the order of accuracy of f 2
i is Oph1 ´h2q for h2 ‰ h1 and Oph2q for h2 “ h1 “ h.
An alternative central difference formula can be obtained by expanding fi`1 and fi´1
into Taylor series about xi and eliminating fi’s. Again, using the shifted coordinate system
and denoting fpxiq “ fi “ fp0q, fi`1 “ fph2q, and fi´1 “ fp´h1q, we arrive at
f1
i “ fi`1 ´ fi´1
h1 ` h2
`
1
2
ph1 ´ h2qf 2
i `
1
6
O `
h2
1 ´ h1h2 ` h2
2
˘
which behaves nearly quadratic when h1 « h2.
The backward difference formulas are obtained as
f1
i “ h1 ` 2h2
h2ph1 ` h2q
fi ´ h1 ` h2
h1h2
fi´1 `
h2
h1ph1 ` h2q
fi´2 (5.48)
f 2
i “ 2
h2ph1 ` h2q
fi ´ 2
h1h2
fi´1 `
2
h1ph1 ` h2q
fi´2 (5.49)
Employing the Taylor series analysis to obtain the truncation errors for the backward￾difference formulas, Eqs. (5.48) and (5.49), we find
For f1
i : Eph1, h2q “ f 3pξq
6
h2ph1 ` h2q ” Oph1h2 ` h2
2q pxi´2 ď ξ ď xiq
For f 2
i : Eph1, h2q “ f 3pξq
3 ph1 ` 2h2q ” Oph1 ` 2h2q
Here, the order of accuracy of f 2
i , is first order, whether the data set is uniform or not.Numerical Differentiation  295
EXAMPLE 5.5: Deriving difference formulas with direct-fit polynomials
Consider the non-uniform discrete distri￾bution depicted in Fig. 5.11. Using a third￾order approximating polynomial, derive a
forward difference formula for f 2pxiq and
f 3pxiq.
FIGURE 5.11
SOLUTION:
A 3rd-degree direct-fit polynomial is chosen as p3pxq “ ax3 ` bx2 ` cx ` d. Using
a shifted coordinate system, the direct-fit polynomial must satisfy every data point,
i.e., fpxi`kq – ppxi`kq for k “ 0, 1, 2, and 3:
For xi “ 0, fi “ d
For xi`1 “ h1, fi`1 “ ah3
1 ` bh2
1 ` ch1 ` d
For xi`2 “ h1 ` h2, fi`2 “ aph1 ` h2q
3 ` bph1 ` h2q
2 ` cph1 ` h2q ` d
For xi`3 “ h1 ` h2 ` h3, fi`3 “ aph1 ` h2 ` h3q
3 ` bph1 ` h2 ` h3q
2
`cph1 ` h2 ` h3q ` d
which is a system of four linear equations with four undetermined coefficients: a, b,
c, and d.
The second derivative at xi is set to the second and third derivatives of the fitted
polynomial: f 2
i – p2
3p0q “ 2b and f 3
i – p3
3 p0q “ 6a, which give
f 2
i “ 2p3h1`2h2`h3qfi
h1ph1`h2qph1`h2`h3q
´ 2p2h1`2h2`h3q
h1h2ph2`h3q fi`1 `
2p2h1`h2`h3q
h2h3ph1` h2q fi`2
´ 2p2h1` h2qfi`3
h3ph2`h3qph1`h2`h3q ` O `
3h2
1 ` 4h2h1 ` 2h3h1 ` h2
2 ` h2h3
˘
f 3
i “ ´ 6fi
h1 ph1`h2q ph1`h2`h3q `
6fi`1
h1h2ph2`h3q ´ 6fi`2
h2h3ph1`h2q
`
6fi`3
h3 ph2`h3q ph1`h2`h3q ` O p3h1 ` 2h2 ` h3q
The truncation errors in the above finite difference approximations were obtained
from the Taylor series and presented with each formula.
Discussion: Note that when h1 « h2 « h3, the finite difference formula for f 2
i is
nearly second-order accurate, even though the derivation is carried out with a third￾degree approximating polynomial. For notably different interval sizes, the second￾order accuracy will be lost. On the other hand, the order of accuracy of the ap￾proximation for f 3
i is first order in that the error is proportional to the sum of the
interval sizes from xi to xi`1, xi`2, and xi`3. The only approximation formula that
has third-order accuracy is the formula for f1
i .296  Numerical Methods for Scientists and Engineers
The finite difference formulas derived for non-uniformly spaced
discrete functions have a lower order of accuracy compared to
those derived for uniform discrete functions with the same stencil.
5.7 RICHARDSON EXTRAPOLATION
Frequently, numerical differentiation is applied to explicitly defined functions involving
mathematical expressions that are either too long or too complicated. The first-, second-, or
higher-order derivatives of such functions will likely be either too long or too complicated
as well. In such cases, it may be preferable to compute derivatives numerically rather than
dealing with long and complex expressions.
So far, we have used Taylor series or direct-fit polynomials to derive finite difference
formulas to estimate any order derivative of a function (f1
pxiq, f 2pxiq, and so on) to any
order of accuracy (Ophq, Oph2q,. . . ). Also, we have shown that, in most cases, the numerical
differentiation is not “exact” and provides only an approximate answer. Nevertheless, the
accuracy of a differentiation with finite differences can be improved by either decreasing
the step size (i.e., h Ñ 0) or increasing the order of accuracy of the difference formula (i.e.,
Ophmq, m ą 2, which requires more data points).
An alternative technique, referred to as Richardson extrapolation, can only be applied
to explicitly defined, continuous, and sufficiently differentiable functions. It consists of using
two derivative estimates obtained with different step sizes using a low-order finite difference
formula to calculate a higher order, more accurate estimate.
To illustrate, consider a sufficiently differentiable continuous function fpxq. The central
difference approximation of the first derivative, Eq. (5.31), can be expressed as
f1
pxq “ fpx ` hq ´ fpx ´ hq
2h ` K1h2 ` K2h4 ` K3h6 ` ... (5.50)
where K1, K2, K3, and so on are the coefficients containing higher-order derivatives of fpxq.
An approximation for the first derivative (as a function of h) is defined as
D0phq “ fpx ` hq ´ fpx ´ hq
2h (5.51)
which is second-order accurate, Oph2q. Next, we rewrite Eq. (5.50) as follows:
D0phq “ f1
pxq ´ K1h2 ´ K2h4 ´ K3h6 ´ ... (5.52)
To increase the order of accuracy of Eq. (5.50), we shall eliminate the K1h2 term. To do that,
a second estimate for the first derivative will be needed. Equation (5.52) for h{2 (halving
the interval size) leads to
D0
ˆh
2
˙
“ f1
pxq ´ K1
h2
4 ´ K2
h4
16 ´ K3
h6
64 ´ ... (5.53)
Now, using Eq. (5.52) and Eq. (5.53), Oph2q terms are eliminated to give
D1phq “ 4D0ph{2q ´ D0phq
3 “ f1
pxq ` K2
h4
4 ` K3
5h6
16 ` ... (5.54)
which is Oph4q due to being the term with the lowest degree on the rhs.Numerical Differentiation  297
We can similarly eliminate the term with h4 in Eq. (5.54) to find another estimate with
the order of accuracy Oph6q. The interval size of Eq. (5.54) is likewise halved to give
D1
ˆh
2
˙
“ f1
pxq ` K2
h4
64 ` K3
5h6
1024 ` ... (5.55)
Next, eliminating K2h4 between Eq. (5.54) and (5.55) yields
D2phq “ 42D1ph{2q ´ D1phq
42 ´ 1 “ f1
pxq ` K3
h6
64 ` ... (5.56)
which is Oph6q.
Repeating the elimination procedure in the same manner, successive approximations
to the first derivative can be generalized by the following expression:
f1
pxq – Dmphq “ 4mDm´1ph{2q ´ Dm´1phq
4m ´ 1 ` Oph2m`2q, m “ 1, 2,... (5.57)
where Dmphq denotes the extrapolation value at the mth step.
Equation (5.57) can also be used to generate finite difference formulas of high accu￾racy. However, instead, we can apply a tabular approach using Richardson extrapolation.
The extrapolation process can be repeated indefinitely, with each step of the extrapolation
leading to a new estimate that is more accurate than the previous one. We start by using
a difference formula, such as the centered difference formula in this section, where h is de￾creased by a factor of 2. So we employ Eq. (5.51) for h “ 1{2k where k “ 0, 1, 2, and so
on. Then we employ the Richardson extrapolation formula, Eq. (5.57), to these values for
m ě 1.
To obtain an expression for the convergence criterion, we add and subtract Dm´1ph{2q
in the numerator of Eq. (5.57) to find
Dmphq “ Dm´1ph{2q ` Dm´1ph{2q ´ Dm´1phq
4m ´ 1 ` Oph2m`2q, (5.58)
Note that the fractional term on the rhs of Eq. (5.58) is the error estimate for two successive
approximations: |Dmphq ´ Dm´1ph{2q| ă ε. Yet this expression does not provide much
information on the levels of h. To overcome this problem, Eq. (5.57) is expressed and coded
as a double subscript array as follows:
Dk,m “ Dk,m´1 `
Dk,m´1 ´ Dk´1,m´1
4m ´ 1 , m ą 0 (5.59)
where Dk,0 is the difference formula for the pertinent derivative (in this section evaluated
solely by Eq. (5.51)), k denotes the row index (or exponent in h “ 1{2k), and m denotes
the extrapolation step (column index).
Besides Eq. (5.58), a more conservative convergence criterion can be applied to diagonal
elements at row k as follows:
|Dk,k ´ Dk´1,k´1| ă ε (5.60)
The method can be applied not only to f1
pxq but also to for￾ward, backward, and central differences of higher derivatives. In
such cases, the suitably generated Richardson interpolation for￾mula D0phq must be used.298  Numerical Methods for Scientists and Engineers
TABLE 5.7: Richardson extrapolation table for f1
pxq using the CDF.
m “ 0 m “ 1 m “ 2 m “ 3
k Step size Oph2q Oph4q Oph6q Oph8q
0 h D0,0 “ D0phq
1
h
2
D1,0 “ D0
´h
2
¯
D1,1 “ D1phq
2
h
22 D2,0 “ D0
´ h
22
¯
D2,1 “ D1
´h
2
¯
D2,2 “ D2phq
3
h
23 D3,0 “ D0
´ h
22
¯
D3,1 “ D1
´ h
22
¯
D3,2 “ D2
´h
2
¯
D3,3 “ D3phq
.
.
. .
.
. .
.
. .
.
. .
.
. .
.
.
A few steps of successive Richardson extrapolation of the numerical differentiation are
illustrated in Table 5.7. The numerical estimates improve as we move down any column,
mainly due to a reduction in the step size. The estimates also improve as we move to the
right along the same row as a result of increasing order of accuracy. In the Richardson
extrapolation, each improvement made for the forward (or backward) difference formula
increases the order of solutions by one, whereas for the central difference formula, each
improvement increases the order by two.
Pseudocode 5.2
Module RICHARDSON (x0, h, ε, n, D, deriv)
\ DESCRIPTION: A pseudomodule to compute the first derivative of an
\ explicitly defined function using the Richardson’s extrapolation technique.
\ USES:
\ FUNC:: A user-defined function supplying fpxq.
\ ABS:: A built-in function computing the absolute value.
Declare: D0:10,0:10 \ Declare maximum table size
m Ð 0 \ Initialize extrapolation step, m
k Ð 0 \ Initialize interval size exponent, h{2k
err Ð 1 \ Initialize error
While “
err ą ε
‰ \ Start of the loop
Dk,0 Ð pFUNCpx0 ` hq ´ FUNCpx0 ´ hqq {p2hq \ Use CDF for f1
px0q
For “
m “ 1, k‰ \ Loop k: Compute estimates for kth row
Dk,m Ð Dk,m´1 ` pDk,m´1 ´ Dk´1,m´1q{p4m ´ 1q \ Apply Eq. (5.59)
End For
If “
k ě 1
‰
Then \ Check for convergence
err Ð |Dk,k ´ Dk´1,k´1| \ Find abs. error for Eq. (5.60)
End If
h Ð h{2 \ Find next step size, h{2
k Ð k ` 1 \ Find next row number, k
End While
n Ð k ´ 1 \ Final size of the table
deriv Ð Dn,n \ Set final estimate to f1
px0q
End Module RICHARDSONNumerical Differentiation  299
Richardson Extrapolation Method
‚ Richardson’s extrapolation is used to generate high-accuracy results
while using low-order formulas;
‚ Successive computation of Dk,m is faster and cheaper (due to fewer
function evaluations) than using equivalent higher-order finite dif￾ference equations;
‚ It is suitable for adaptive computations, and any derivative (assum￾ing it exists) can be computed within a specified tolerance;
‚ Round-off errors are less of a concern since relatively large h can be
employed.
‚ The method can be applied only to explicit functions;
‚ It uses more points involving a wider range than perhaps necessary;
‚ It is based on the assumption that higher and bounded derivatives
of fpxq exist, which introduces vulnerability when this assumption
is violated;
‚ It can be employed for uniform discrete functions only if the number
of data points in a uniform set is multiples of 2.
In Pseudocode 5.2, the pseudomodule RICHARDSON is given for computing the first
derivative of an explicitly defined continuous function using Richardson’s extrapolation
technique. The module requires the point at which the derivative will be calculated (x0),
an initial interval size (h), and a tolerance (ε) as input. The output is the size of the
Richardson extrapolation table (n), the table of the extrapolations D, and the first derivative
(deriv) that meets the convergence criterion. The function fpxq is supplied to the module by
FUNC(x), a user-defined external function module. Because the row variable is incremented
before the End While statement, its value becomes the table size plus one. Thus, the table
size is set to n “ k ´ 1.
Extrapolation step m, row index k, and the differentiation error (err) are initialized.
Richardson’s table, Table 5.7, is constructed using a While-loop. The first row consists of only
D0,0. Before going to the top of the loop, the interval size is halved (h{2) and the row index
is increased by one, k Ð k `1. For the second row, after computing D1,0, the extrapolation
formula is applied using For-loop for as many columns as possible. A conservative criterion
for the differentiation error, |Dk,k ´ Dk´1,k´1| ă ε, is monitored at the end of each row.
5.8 ALTERNATIVE METHODS OF EVALUATING DERIVATIVES
A series of experimental measurements is one of the sources for generating discrete data.
Recall that all experimental data also contains measurement errors or noise to a certain
degree. The derivatives of such data are a bit tricky since a small error or noise in the
measurements may lead to amplified errors or large non-physical fluctuations. These effects
become increasingly significant as the order of the derivative increases.
Differentiation of discrete functions with finite differences that are based on polynomial
approximations is unsatisfactory because the interval size cannot be reduced arbitrarily. Fi￾nite difference formulas of high order of accuracy tend to oscillate and/or amplify the errors
(see Chapter 6). The greater the degree of the polynomial, the larger the amplifications.300  Numerical Methods for Scientists and Engineers
EXAMPLE 5.6: Differentiation with Richardson extrapolation
Under specific conditions, the Van der Waals equation of state for a real gas takes
the following form:
PpV q “ 25000
V ´ 57 ´ 5.2 ˆ 106
V 2
where V is the volume (cm3), and P is the pressure (bar). Starting with the CDF
and h “ 5 cm3, use Richardson’s extrapolation to estimate dP{dV for V “ 150 cm3
within ε “ 10´3 tolerance.
SOLUTION:
To be able to start the extrapolation procedure, we need two estimates of dP{dV
with the CDF using h “ 5 and h “ 2.5 cm3:
D0,0 “ Pp150 ` 5q ´ Pp150 ´ 5q
2p5q “ 38.6608 ´ 36.7663
2p5q “ 0.189454
D1,0 “ Pp150 ` 2.5q ´ Pp150 ´ 2.5q
2p2.5q “ 38.1843 ´ 37.2313
5 “ 0.190596
Using the estimates, we obtain an improved estimate by applying the (first-step)
Richardson extrapolation as
D1,1 “ 4D1,0 ´ D0,0
3 “ 4p0.190596q ´ 0.189454
3 “ 0.190977
The estimated error at the end of this step is |D1,1 ´ D0,0| “ 0.01523 ą 10´3. To
obtain an improved second step estimate, dP{dV is calculated with h “ 1.25 cm3:
D2,0 “ Pp150 ` 1.25q ´ Pp150 ´ 1.25q
2p1.25q “ 37.9451 ´ 37.4679
2.5 “ 0.190880
Using D0,1 and D0,2, the first step extrapolation value D1,2 is obtained as follows:
D2,1 “ 4D2,0 ´ D1,0
3 “ 4p0.190880q ´ 0.190596
3 “ 0.190975
Now that D1,1 and D1,2 are available, the second step Richardson extrapolation
value can be calculated as
D2,2 “ 16D2,1 ´ D1,1
15 “ 16p0.190975q ´ 0.190977
15 “ 0.190974 bar
cm3
The error estimate at the end of the second step is |D2,2 ´ D1,1| “ 3.2ˆ10´6 ă 10´3,
which is sufficient to terminate the computation at this stage. The true derivative is
found to be 0.19097391.
Discussion: The true error for the computed derivative is 9ˆ10´8, which is in fact
much less than 10´3 or 3.2 ˆ 10´6 ă 10´3.
The technique is a convenient and effective way to improve accuracy without
much computational effort. Similar hierarchies can be constructed for forward, back￾ward, and central differences for the first-, second-, and higher-order derivatives. It
should also be pointed out that Richardson’s extrapolation process can be used not
only to improve the order of a given numerical differentiation formula but also to
find the given basic numerical differentiation formulas.Numerical Differentiation  301
An alternative to differentiation with finite differences is to fit the data to an ap￾propriate mathematical model and then differentiate it analytically. Curve fitting and its
applications, covered in Chapter 7, smooth out the noise or measurement errors to some
extent. Curve fitting the data to a polynomial is equally not recommended unless the finite
difference equations are equivalent to polynomial interpolation. The coefficients of finite
difference formulas (e.g., Eqs. (5.44)-(5.49)) for non-uniform data are functions of interval
size (h1, h2, ...), and hence the finite differences of such functions are computationally even
more involved. That is why differentiation after curve fitting is very useful, especially for
non-uniform data, as it provides a great deal of savings in cpu-time as well.
Cubic spline interpolation (covered in Chapter 6) makes use of nearby data points. One
of the most important features of cubic spline interpolation is that it provides a smoothly
varying interpolant as well as its first and second derivatives (see Section 6.4). That is why
splines are better models, and the two-step spline approach requires (1) interpolation of the
data by a spline and (2) differentiation of the splines. Unlike other methods, this approach
allows computation of the first and second derivatives at locations other than the original
data points.
5.9 CLOSURE
The goal of numerical differentiation is to develop formulas to estimate the derivatives
of functions about a base point. The numerical differentiation formulas are based on ap￾proximating polynomials. Taylor polynomials, as well as direct-fit, Lagrange, and divided
difference polynomials, work well for both uniformly and non-uniformly spaced data.
The order of accuracy of a particular derivative depends on the number of function
points used. Using the polynomials, it is possible to derive difference formulas with varying
orders of accuracy, i.e., Ophq with two points, Oph2q with three points, and so on.
The accuracy of any numerical differentiation scheme can be increased by either (i)
reducing the interval size (h or hi’s) as much as possible or (ii) using finite difference
formulas with an order of accuracy of Oph2q, Oph4q, or higher if the number of data points
in a distribution is sufficient.
(1) When dealing with uniformly or non-uniformly spaced discrete functions in [a, b],
the numerical differentiation can be carried out by forward, backward, or central difference
formulas. Deriving difference formulas with direct-fit polynomials is the easiest avenue, given
the functional points you want to use and the degree of accuracy you desire. In this case, we
do not have the freedom to reduce the interval size because either experimentally collected
or computed discrete data sets and their interval sizes are invariant. The best option we
have is to use different formulas with a higher order of accuracy.
(2) An explicitly defined function can be differentiated exactly. However, when these
functions consist of very long or complex expressions, higher derivatives also yield very
long and complex expressions. In such cases, numerical differentiation may be preferred due
to its computational speed and simplicity. The value of h can be varied by the analyst,
and there is an optimum value for which one can reduce h without increasing the effect
of round-off errors. In this respect, the Richardson extrapolation technique can also be
applied to explicitly defined functions. It is a powerful tool for improving the accuracy of a
finite-difference formula. This technique allows us to achieve greater precision with minimum
effort (fewer function evaluations) in comparison to other methods. The underlying idea is of
critical importance in numerical analysis, and it is often used in integrations, extrapolation,
and so on.302  Numerical Methods for Scientists and Engineers
5.10 EXERCISES
Section 5.1 Basic Concepts
E5.1 Use the definition of limit to find f1
pxq for the functions given below.
(a) fpxq “ x3
, (b) fpxq “ sin x, (c) fpxq “ e
2x
E5.2 For functions in E5.1, find approximations for f1
pxq for an arbitrary x using Eq. (5.4).
E5.3 For approximations in E5.2, identify the leading error term.
E5.4 For approximations in E5.2, identify the truncation error as well as its upper/lower bounds.
E5.5 Apply the first-order forward difference formula to the functions given below to (i) estimate
f1
p1q and true errors by using h “ 10´m where m “ 1, 2,..., 20, and (ii) interpret your results.
(a) fpxq “ x3
1 ` x2 , (b) fpxq “ ln x, (c) fpxq “ e
x´1
E5.6 Derive an expression for the optimum interval size for the following difference formulas:
(a) f1
pxq – fpx ` hq ´ fpx ´ hq
2h , (b) f 2
pxq – fpx ` hq ´ 2fpxq ` fpx ´ hq
h2
Section 5.2 First-Order Finite Difference Formulas
E5.7 Derive a general difference formula for Δnfi.
E5.8 Derive a first-order forward difference formula for d5fi{dx5.
E5.9 Using the first-order forward and backward difference formulas, (a) estimate the first deriva￾tive for fpxq “ x3 at x0 “ 2 with h “ 0.25, 0.1, 0.025, and 0.01, (b) compare the estimates and
truncation errors with those of the true derivatives and errors, and discuss your results.
E5.10 Repeat E5.9 for f 2px0q.
E5.11 Using the first-order forward difference formulas, (a) estimate f1
p1q and f 2p1q for fpxq “
ln2px ` 1q with h “ 0.1, 0.05, 0.025, and 0.01, (b) compare your estimates with those of the true
values, and discuss your results.
E5.12 Repeat E5.5 for f 2p1q with h “ 10´1, 10´2, ..., 10´8, and interpret your results.
Section 5.3 Second-Order Finite Difference Formulas
E5.13 (a) Use h “ 0.1 and the second-order accurate forward and backward difference formulas
to estimate f1
p1q, where fpxq “ e2x; (b) compare your estimates and truncation errors with those
of the true values and discuss your results.
E5.14 Using the second-order accurate finite difference formulas, investigate the existence of the
first derivative of the given discrete function at x “ 1. Hint: The first derivative of a continuous
function at a point exists if its left- and right-hand derivatives are equal.
xi 0 0.5 1 1.5 2
fpxiq ´2 ´1.25 3 13.75 34
E5.15 Using the second-order accurate finite difference formulas, investigate the existence of the
first derivative of the given discrete function at x “ 2.
xi 1.5 1.75 2 2.25 2.5
fpxiq 0.879 1.352 2 1.414 0
E5.16 Using the Taylor series expansion, determine (a) which derivatives the following difference
formulas correspond to; (b) the truncation error term; and (c) the order of accuracy, i.e., Oph?q.
Hint: Expand fi˘k in the difference formulas into a Taylor series around xi.Numerical Differentiation  303
(i) fi`3 ´ fi´2
5h , (ii) fi´2 ´ 4fi´1 ` 6fi ´ 4fi`1 ` fi`2
h4 ,
(iii) fi´3 ´ 6fi´2 ` 12fi´1 ´ 10fi ` 3fi`1
2h3 , (iv) ´ 3fi´2 ` 20fi ´ 30fi`1 ` 15fi`2 ´ 2fi`3
10h3
(v) fi´2 ´ 8fi ` 15fi`1 ´ 11fi`2 ` 3fi`3
h2
E5.17 Even-order derivatives of a function fpxq are zero for all x, i.e., f 2pxq “ fp4q
pxq “
fp6q
pxq “ ... “ 0. Using the Taylor series expansion, determine (a) which derivative the following
difference formula corresponds to, (b) the truncation error term, and (c) the order of accuracy.
16fi`3 ´ 60fi`2 ` 72fi`1 ´ 28fi
4h3 “? Oph?
q
E5.18 Use the Taylor series expansions to derive the truncation error for the following difference
formulas and list them from the highest order of accuracy to the lowest.
(a)
5fi´2 ´ 3fi´1 ´ 10fi ` 3fi`1 ` 5fi`2
6h , (c) ´ fi´3 ` 3fi´2 ´ 5fi´1 ` 5fi`1 ´ 3fi`2 ` fi`3
4h ,
(b) fi´3 ´ 3fi´2 ` 5fi´1 ´ 10fi ` 7fi`1
5h (d) ´ fi´2 ` 2fi´1 ´ 2fi`1 ` fi`2
2h3
E5.19 Using the Taylor series, determine the truncation error and order of accuracy of the fol￾lowing finite difference formulas for f1
i .
(a) f1
i “ 2fi`3 ´ 9fi`2 ` 18fi`1 ´ 11fi
6h , (b) f1
i “ ´3fi`4 ` 16fi`3 ´ 36fi`2 ` 48fi`1 ´ 25fi
12h
E5.20 Using the Taylor series, determine the truncation error and order of accuracy of the fol￾lowing finite difference formulas:
(a) f 2
i “ 1
12h2
#
11fi`4 ´ 56fi`3 ` 114fi`2
´104fi`1 ` 35fi
+
, (b) f 3
i “ 1
4h3
# 7fi`5 ´ 41fi`4 ` 98fi`3
´118fi`2 ` 71fi`1 ´ 17fi
+
E5.21 The left- and right-biased difference formulas for the first derivative are defined as p2pf1
i q
´`
pf1
i q
`q{3 and ppf1
i q
´ ` 2pf1
i q
`q{3, respectively. For the given discrete data below, estimate f1
p1.5q
using (a) the first- and second-order accurate forward and backward difference formulas, (b) the
left- and right-biased formulas, and (c) compare the accuracy of the estimates in Parts (a) and
(b) if the true derivative is given as 1.4375.
xi 1.3 1.4 1.5 1.6 1.7 1.8
fpxiq 2.53 2.35 2.41 2.61 2.9 3.24
Section 5.4 Central Difference Formulas
E5.22 The sum of the coefficients in any difference formula is zero. Explain why.
E5.23 Expand the following finite-difference operations to obtain explicit forms:
(a) Δ∇fi´1, (b) Δδ2
fi`1, (c) Δδ∇f3{2
E5.24 Use Eq. (5.37) to find a second-order central difference formula for d5fi{dx5.
E5.25 For an arbitrary x and h and fpxq “ x2, obtain the central difference formulas for f1
pxq
and f 2pxq. Comment on your findings.
E5.26 Repeat E5.25 for fpxq “ x3, and discuss the implications of your findings.
E5.27 Find the truncation error for E5.25.
E5.28 Find the truncation error for E5.26.
E5.29 Repeat E5.5 using the CDFs with h “ 10´1, 10´2,..., 10´12.
E5.30 Repeat E5.12 using the CDFs with h “ 10´1, 10´2,..., 10´8.304  Numerical Methods for Scientists and Engineers
E5.31 Derive a third-order accurate finite difference formula for f1
pxq that uses the following
points: fpx ´ 2hq, fpx ´ hq, fpxq, and fpx ` hq.
E5.32 Given x0 and h, estimate f1
px0q for the following functions using the central difference
formulas of order of accuracy of Oph2q and Oph4q.
(a) fpxq “ 2x3 ´ x2 ` 4x ´ 3, x0 “ 1, h “ 0.1 (b) fpxq “ 3x4 ` x3 ` x, x0 “ 2, h “ 0.05
(c) fpxq “ x2e´2x, x0 “ 0.1, h “ 0.01 (d) fpxq “ x tan 2x, x0 “ π, h “ 0.02
E5.33 For given fpxq “ xe´x, (a) estimate f1
p0.7q using the second-order accurate forward,
backward, and central difference formulas with h “ 0.2, 0.1, 0.05, and 0.025; (b) compare your
estimates with the true value.
E5.34 For given fpxq “ x3, (a) estimate f1
p2q using the central difference formula with h “ 0.25,
0.1, 0.025, and 0.010; (b) compare your estimates with the true value.
E5.35 Repeat E5.34 for f 2p2q.
E5.36 For given fpxq “ e2x (a) estimate f1
p1q using three-point forward, backward, and central
difference formulas, i.e., Op0.12q; (b) compare your results with the true value.
E5.37 For the given discrete function, estimate f1
p0q, f1
p2q, f1
p4q, f 2p0q, and f 2p4q using suitable
second-order accurate difference formulas.
xi 0123 4
yi 30 36 30 12 ´18
E5.38 A 20-second recording of a vehicle’s motion is given below with 2.5-second intervals. Using
the finite difference formulas of order of accuracy Oph2q, estimate the acceleration for each data
point listed.
xi 0 2.5 5 7.5 10 12.5 15 17.5 20
fpxiq 0 11 19 28 37 40 42 35 16
E5.39 Shear stress, τ (N/m2), due to the flow of a fluid over a solid surface, is defined by
τ “ μpdu{dyqy“0, where μ is the dynamic viscosity (N-s/m2), u is the velocity component (m/s)
parallel to the surface, and y (mm) is the distance normal to the surface (see Fig. E5.39). Velocity
has been measured with an LDV (Laser Doppler velocimetry), and the results are tabulated below.
Use the finite difference formulas of order of accuracy Ophq, Oph2q, Oph3q, and Oph4q to estimate
the shear stress for the given case and comment on the accuracy of your findings. The dynamic
viscosity of the fluid is given as μ “ 7.5 ˆ 10´4 N-s/m2.
xi (mm) 0 1.5 3 4.5 6 7.5
yi (mm) 0 3.6 6.4 8.76 10.56 12.2
Fig. E5.39 Fig. E5.40
E5.40 Heat applied to one side of a plate with a conductivity of k “ 50 W/m-K results in a steady
temperature distribution. The temperature along the wall is measured by a series of thermocouples,
as shown in Fig. E5.40. The thermocouple position x (mm) and measured temperature T (
˝C) are
tabulated below. The heat flux in the plate, q{A (W/m2), is related to the plate conductivity and
the derivative of temperature as q{A “ ´k dT{dx. Estimate the heat flux at the thermocouple
locations using the second-order accurate finite difference formulas.
x (mm) 0 5 10 15 20 25
T (
˝C) 90 86 82 78 74 70Numerical Differentiation  305
E5.41 Kinematic viscosity versus temperature data for a biodiesel is tabulated below. Compute
dν{dT at T “ 30˝C as accurately as possible using the second-order accurate (a) central difference;
(b) forward difference; (c) backward difference formulas; (d) are all estimates obtained for the
derivative consistent? Explain why.
T (
˝C) 10 20 30 40 50 60 70
ν (m2/s) 5.4 4.2 3.3 2.7 2.3 1.9 1.6
E5.42 The data below depicts transient core temperature readings of a solidifying melt. (a) Is
the number of data points sufficient to determine a steady cooling rate in ˝C per minute? (b) If
yes, estimate the solidification rate.
t (min) 0 2 4 5 8 10 12
T (
˝C) 1575 1217 1019 903 880 860 840
E5.43 Newton’s law of cooling is given as
dT
dt “ ´ 1
τt
pTptq ´ T8q
where Tptq is the temperature of the object, T8 is the ambient temperature, and τt is the thermal
time constant (in seconds). If the ambient temperature is 10˝C, estimate the thermal time constant
for the solidification of the melt given in E5.42.
E5.44 The monthly average of daylight hours (L) for a town is tabulated below. Use the second￾order accurate finite difference formulas to determine the months at which the rate of change in
daylight hours is at its maximum or minimum.
Month Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
L (hr) 9.35 10.42 12.07 13.19 14.32 15.05 14.47 13.46 12.24 11.02 9.50 9.17
E5.45 In an isobaric process, the heat capacity of gases is defined as the rate of change of enthalpy
h (J/kg) with respect to temperature T (K), i.e., cp “ pBh{BTqp. An experimental work yields the
h ´ T data furnished below. For the tabulated temperature range, estimate the heat capacity of
the gas at every available temperature point with an order of accuracy of O `
pΔTq
2˘
.
T (K) 250 300 350 400 450 500
h (J/kg) 19.75 28.61 39.11 51.25 65.04 80.45
E5.46 A uniform 1 m-long beam, simply supported at both ends, is subjected to a bending
moment given by Mpxq “ EId2y{dx2, where ypxq is the beam deflection (m), Mpxq is the bending
moment (N¨m), E is the elasticity modulus (Pa), and I is the inertia moment (m4). Use second￾order-accurate finite difference formulas to estimate the bending moment at each available data
point for a beam whose deflection is tabulated below. Given: EI “ 200 N-m2.
xi (m) 0 0.125 0.250 0.375 0.500 0.625 0.750 0.875 1
yi (m) 0 0.117 0.206 0.263 0.285 0.271 0.220 0.130 0
E5.47 A numerical simulation for a thick-walled cylindrical pressure vessel yields the tabulated
data for the radial displacement. The relationships between the radial strain, radial and tangential
stresses are given by:
εr “ du
dr, σr “ E
1 ´ ν2
˜
du
dr ` ν
u
r
¸
, σθ “ E
1 ´ ν2
˜
ν
du
dr ` u
r
¸
where E “ 200 GPa is the modulus of elasticity and ν “ 0.3 is the Poisson ratio. Using second￾order accurate finite difference formulas, estimate the radial strain, radial, and tangential stresses
at each available radial location. Make sure you are using the correct units in your calculations.
xi (m) 0.600 0.625 0.650 0.675 0.700 0.725 0.750 0.775 0.800
u (mm) 1.22 1.33 1.46 1.60 1.75 1.9 2.06 2.22 2.30306  Numerical Methods for Scientists and Engineers
E5.48 In an isentropic process, where there is no entropy change, the following relationship be￾tween T (in K) and υ (in m3/kg) holds: pBT{BPq“pT{CpqpBυ{BTqP . Using the given ideal gas
data below and the second-order difference formulas, estimate BT{BP at each available tempera￾ture point.
T p
˝
Cq 10 15 20 25 30 35 40 45 50
υ (m3/kg) 1 0.966 0.934 0.904 0.876 0.85 0.825 0.802 0.78
Cp(kJ/kg.K) 1.7 1.73 1.76 1.8 1.83 1.87 1.9 1.96 1.98
E5.49 The load deviation of a disc spring is estimated both theoretically and experimentally.
The variation of the measured and computed load with the deviation is tabulated below. Use the
second-order accurate finite difference equations, Oph2q, to estimate the rate of change in the load
(dF{ds) for each theoretical and experimental data point listed.
s (mm) 0 0.2 0.4 0.6 0.8 1 1.2 1.4
Ftheo (N) 50 1500 2400 3200 4000 4700 5350 6000
Ftheo (N) 150 1000 2250 3175 3950 4850 6030 8000
E5.50 The volumetric expansion coefficient is defined by β “ ´p1{ρqpdρ{dTq. The density of a
water-based solution as a function of temperature is available in 5 K increments from 290 to 335
K. Estimate the volumetric expansion coefficient for every available temperature data using finite
difference equations of order of accuracy of Oph2q.
T (K) 290 295 300 305 310 315 320 325 330 335
ρ (kg/m3) 999 998 997 995 993 991 989 987 984 981
E5.51 Experimental data to determine the variation of soil temperature (T) with depth (z) is
collected at the same time of day from three locations considered dry, medium, and wet. The
conductivities of the soil are k “ 0.4, 1.1, and 1.6 W/m-K, respectively, at dry, medium, and wet
soil locations. The heat loss from the soil to the surrounding air is governed by Fourier’s law,
defined as q “ ´k dT{dz, where q is the heat loss per unit area, k is the conductivity of soil,
and dT{dz is the rate of change of soil temperature with depth. Use the furnished data below to
estimate the rate of heat loss per square meter from each location, ´kpdT{dzqz“0, using the finite
difference formulas of order of accuracy Ophq, Oph2q, Oph3q, and Oph4q.
T (
˝C)
z (m) Dry Medium Wet
0 5.8 5.5 4.9
1 8.7 7.6 7.1
2 10.3 9.2 9.5
3 11 10.4 12
4 11.3 10.9 14.5
5 11.4 11.3 14.6
E5.52 Spectral emissive power (Eλ, W/m2μm) is defined as the amount of radiation energy
emitted by a blackbody at an absolute temperature T per unit time, per unit surface area, and
per unit wavelength about wavelength λ. The change in the spectral emissive power of a black body
at 500 K with the wavelength (λ, μm) is tabulated below. Calculate the dEλ{dλ and d2Eλ{dλ2
derivatives for every available data points using finite difference equations of order of accuracy
Oph2q.
λ (μm) 2 10 18 26 34 42 50
Eλ (W/m2μm) 6.6 223 50 15.6 6.2 2.9 1.5Numerical Differentiation  307
E5.53 The change in the spectral emissive power Eλ of a black body of 10 μm wavelength with
the temperature (T, K) is tabulated below. Estimate dEλ{dT and d2Eλ{dT 2 derivatives for each
temperature value listed, using the finite differences of order of accuracy Oph2q.
T (K) 300 350 400 450 500 550 600
Eλ (W/m2μm) 31 63 105 160 223 295 374
E5.54 The performance of an engine at 30% acceleration level is tested. The torque (T) is deter￾mined as a function of angular speed (N), and the results of the test runs are presented below.
Estimate the dT{dN and d2T{dN2 derivatives for each angular speed value listed using the finite
difference equations of order of accuracy Oph2q.
N (rpm) 1300 1800 2300 2800 3300 3800 4300
T (N¨m) 133 141 147 153 152 150 155
E5.55 A circular disc is rotating about its axis. The angular position of a point on the outer ring
of a disc, θptq, is tabulated below as a function of time. For each data value listed, estimate the
disk’s angular speed, ωptq “ dθ{dt, and the angular acceleration, αptq “ dω{dt, using the finite
difference equations of order of accuracy Oph2q.
t (s) 0 0.5 1 1.5 2 2.5 3
θ (rad) ´1 ´1.036 ´0.572 0.392 1.855 3.819 6.283
E5.56 In a circuit, voltage (V ptq) is defined by Kirchoff’s first law as V ptq “ Lpdi{dtq ` R iptq,
where L is the inductance, R is the resistance, and iptq is the current varying with time. The
resistance and inductance are fixed at R “ 0.15 Ω and L “ 0.95 H. When the voltage is applied,
the current is measured at 0.2-second intervals. Use the reported data below to estimate the voltage
corresponding to every current listed, using the finite differences of order of accuracy Oph2q.
t (s) 0 0.2 0.4 0.6 0.8 1
i (A) 3 3.18 3.32 3.44 3.52 3.58
Section 5.5 Finite Differences and Direct-Fit Polynomials
E5.57 (a) Use the direct-fit polynomials technique to validate the following forward difference
equation; (b) apply the Taylor series to show that the order of the difference equation is Oph3q.
f1
i – ´11fi ` 18fi`1 ´ 9fi`2 ` 2fi`3
6h
E5.58 Find a finite difference formula for f1
pxq that has a truncation error of order Oph3q and
uses the points fpx ´ 2hq, fpx ´ hq, fpxq, and fpx ` hq.
E5.59 The function fpxq is defined at points fi, fi`1, and fi`2, where xi`1 ´ xi “ h and xi`2 ´
xi`1 “ 3h. For f1
pxiq and f 2pxiq, develop forward difference formulas of order of accuracy Oph2q.
E5.60 The function fpxq is given as a non-uniform discrete function with a constant stretching
ratio r “ hi`1{hi (r ą 1), as shown in Fig. E5.60. For f1
pxiq and f 2pxiq, develop forward difference
formulas of order of accuracy Oph2q.
Fig. E5.60
Section 5.6 Differentiating Non-uniformly Spaced Discrete Functions
E5.61 Consider a non-uniformly spaced discrete function, fpxq. Employ the direct-fit polynomials
to derive second-order accurate backward and central difference formulas for f1
pxiq and f 2pxiq.
Given: xi ´ xi´1 “ h1, xi`1 ´ xi “ h2, and xi´1 ´ xi´2 “ h2.308  Numerical Methods for Scientists and Engineers
E5.62 A non-uniform discrete function is tabulated below. Estimate f1
pxiq and f 2pxiq for each
data point listed using second-order accurate finite difference formulas.
xi 0 0.5 0.9 1.1 1.5 1.8
fpxiq 1.12 0.69 0.37 0.26 0.25 0.47
E5.63 A non-uniform discrete function is tabulated below. Estimate f1
pxiq and f 2pxiq for each
data point listed using the second-order accurate finite difference formulas.
xi 1.2 1.5 1.9 2.3 2.4 2.8
fi 2.577 2.597 2.625 2.653 2.66 2.69
E5.64 The process of “icing water” (ice thickness) in a very large container is studied. The
container is kept in a freezer maintained at a steady ´12˝C, and ice formation (thickness, h in
mm) is recorded for over a period of 10 hours at different time intervals. Use the given data below
to estimate the rate of icing (dh{dt, mm/min) by using the second-order accurate finite difference
formulas.
t (min) 0 30 180 220 420 540 600
h (mm) 0 2 5 5.6 7.7 8.8 10.1
E5.65 The effect of plastic pellet additives on the strength of concrete is investigated. The change
in the compression strength is measured as a function of the volumetric ratio of the pellets, x (%).
For the given data below, estimate the rate of change in the compression strength with pellet ratio
(dP{dx) for every data point listed. Use second-order-accurate finite difference formulas.
x (%) 0 3 7 10 15 25 35
P (kPa) 88 73 58 45 29 22 18
E5.66 The data on the annual steel production (in tons) of a plant since 2008 is presented
below. Estimate the rate of steel production (dm{dt) using second-order-accurate finite difference
formulas.
t 2008 2010 2011 2014 2016 2018
m 2280 2560 2660 2990 3110 3150
E5.67 For a reaction whose rate is rapid enough to achieve dynamic equilibrium, the van’t Hoff
equation is expressed as
dpln Keqq
dp1{Tq “ ´ΔH0
R
where R “ 8.314 J/K.mol is the ideal gas constant, ΔH0 is the reaction standard enthalpy (J),
Keq is the reaction equilibrium constant, and T is the reaction temperature (in K). Experimental
data on the temperature versus equilibrium constant of a chemical reaction is tabulated below.
Using the finite differences, estimate an approximate value for ΔH0. Hint: The van’t Hoff relation
gives the slope of a straight line when it is plotted as 1{T versus lnpKeqq.
T (
˝C) 39.9 40.2 52.9 64 77.6 79.5
Keq (ˆ105) 0.2721 0.279 0.750 1.675 4.186 4.7265
Section 5.7 Richardson Extrapolation
E5.68 Apply the Richardson extrapolation to estimate f1
px0q accurately to five decimal places
for the given functions below. Use the CDF with a starting value of h “ 0.1. How accurate is the
estimated value?
(a) fpxq “ ?x, x0 “ 1, (b) fpxq“px2 ` 1q sin x, x0 “ 0
(c) fpxq “ 1{p5 ´ x2q, x0 “ 1, (d) fpxq “ lnp1 ` ?x ` 1q, x0 “ 0
E5.69 Repeat E5.68 to estimate f 2px0q.Numerical Differentiation  309
E5.70 Develop Richardson extrapolation steps, starting with the first-order-accurate forward
difference formula, and write a program to adaptively evaluate the derivative of a function with
an accuracy of |Dk,k ´ Dk´1,k´1| ă ε.
E5.71 Consider the problems given in E5.68. Use the Richardson extrapolation with the for￾ward difference formula as the starter and apply the algorithm developed in E5.70) to estimate
the derivatives at the specified points. How accurate will the derivative be after three steps of
Richardson extrapolation (i.e., how accurate is D3,3?).
E5.72 Can you apply Richardson’s extrapolation to the following discrete function to estimate
f1
p3q using central difference formula? If yes, find the derivative as accurately as possible.
xi 2.8 2.9 3 3.1 3.2
fpxiq 4.883 4.941 5 5.061 5.124
5.11 COMPUTER ASSIGNMENTS
CA5.1 Consider the following central-difference formula:
f1
i – ´fi`2 ` 8fi`1 ´ 8fi´1 ` fi´2
12h
Using the Taylor series, (a) find the truncation error for the difference formula, (b) obtain an
expression for the optimum spacing for which the total error is minimum, and (c) write a program
to compute f1
p0q, its truncation, and true errors for fpxq “ ex with h “ 10´1, 10´2,..., 10´10.
CA5.2 Transient data for the concentration of the product of a chemical reaction is presented
below. An nth-order law for a single reactant is described by the following differential equation:
dC{dt “ ´kCnptq, where k “ 0.54 M1´n{ min is the reaction rate, and n is the order of reaction.
Use second-order accurate differences to determine the order of the reaction.
t (min) 0 20 40 60 80 100 120
C (M) 0.55 0.067 0.041 0.029 0.022 0.018 0.015
CA5.3 A tube 0.2-m in diameter with a conductivity of k “ 10 W/m-˝C
is used to transport an exothermic solution (see Fig. CA5.3). A steady￾state temperature reading along the radial position is in the table below.
The heat conduction equation for the tube can be written as
1
r
d
dr ˆ
r
dT
dr ˙
` q90
k “ 0, 0 ď r ď R
where q90 is the constant heat generation (W/m3), and R is the radius
of the tube (m). Use second-order-accurate finite difference formulas to
estimate the amount of heat generation.
Fig. CA5.3
r (m) 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1
T (
˝C) 215 214 211 206 199 190 179 166 151 134 115
CA5.4 Consider fpxq “ cos 20πx. (a) Estimate f1
p0.025q using first- and second-order accurate,
Ophq and Oph2q, forward-difference formulas with h “ 0.4, 0.2, 0.1, and 0.05; (b) Compare your
estimates with the true answer and discuss the accuracy; (c) Plot the function for 0 ď x ď 0.5, and
using this plot, discuss the implications of the results obtained in Part (a); (d) Reduce the values
of h to determine an optimum value h; (e) Using this optimum value and second-order-accurate
difference formulas, generate a table of the first and second derivatives of the given function for a
0 ď x ď 0.5 interval with increments of 0.025.310  Numerical Methods for Scientists and Engineers
CA5.5 Consider a non-uniform discrete function containing a set of n data points. (a) Write a
general computer program to compute f1
pxiq and f 2pxiq derivatives at every available data point;
(b) using this program, evaluate the derivatives of the tabulated discrete function.
xi 0 0.2 0.5 0.6 0.7 0.9 1 1.05 1.08 1.1
fpxiq 1 1.004 1.25 1.7465 2.8824 9.503 17 22.44 26.39 29.345
CA5.6 Consider the following table of data: (a) Use the computer program prepared in CA 5.5 to
compute f1
i for every data point listed; (b) estimate dfi{dx using dfi{dy, which may be estimated
using the difference formulas of uniform distributions. Note that the data is uniformly distributed
with respect to the y´variable, where xi “ y2
i and y “ 1, 1.1, 1.2, ..., 2, 2.1.
xi fpxiq xi fpxiq xi fpxiq
1.0 1 1.96 14.75789 3.24 110.19961
1.21 2.143589 2.25 25.62891 3.61 169.8363
1.44 4.299817 2.56 42.949673 4.0 256
1.69 8.157307 2.89 69.757574 4.41 378.22859
CA5.7 A digitized outline of an axisymmetric jug in xy´plane as y “ fpxq, is presented in the
table below. Write a computer program (or use a spreadsheet) that uses any arbitrary data to
calculate the curvature κpxiq “ ˇ
ˇf 2pxiq
ˇ
ˇ {p1 ` rf1
pxiqs2
q
3{2
of a discrete function at every listed
data point xi.
xi fpxiq xi fpxiq xi fpxiq xi fpxiq
0 0 0.07 0.1225 0.14 0.0857 0.21 0.0450
0.01 0.043 0.08 0.1205 0.15 0.079 0.22 0.0406
0.02 0.0741 0.09 0.1167 0.16 0.0726 0.23 0.0365
0.03 0.0956 0.1 0.1116 0.17 0.0664 0.24 0.0328
0.04 0.1098 0.11 0.1056 0.18 0.0605 0.25 0.0294
0.05 0.1181 0.12 0.0992 0.19 0.0550
0.06 0.1220 0.13 0.0925 0.2 0.0498
CA5.8 The surface tension of a water-based solution, σ (mN/m), as a function of normalized
temperature θ “ p373 ´ Tq{100 (T in˝C) is tabulated below. Write a program to calculate the
dσ{dT and d2σ{dT 2 at every available data point using the finite differences.
θ, ˝C σpθq, mN/m θ, ˝C σpθq, mN/m θ, ˝C σpθq, mN/m
0 10.5 0.3 37.5 0.65 51.3
0.1 14.7 0.45 40.5 0.7 54.6
0.15 22.8 0.5 41.1 0.85 64.4
0.25 26.6 0.55 44.5 1 73.9Numerical Differentiation  311
CA5.9 The voltage across a capacitor is given as a function of time as follows:
V ptq “ 1
C
ż t
0
iptqdt
For a capacitor of C “ 5 ˆ 10´5 F, use the following data to find the evolution of the transient
current across the capacitor, i.e., current for every time value available.
t (s) V (mV) t (s) V (mV) t (s) V (mV)
0 0 0.6 14.85 1.2 23.69
0.1 2.33 0.7 16.98 1.3 24.25
0.2 4.84 0.8 18.87 1.4 24.62
0.3 7.44 0.9 20.5 1.5 24.84
0.4 10 1 21.83
0.5 12.51 1.1 22.89
CA5.10 In order to determine the convection heat transfer coefficient (h) of an indoor ice-skating
ring that is kept at T8 “ 280 K at all times, a small spherical metal ball (mcp “ 250 J/kg)
embedded with a thermocouple is heated to 500 K. It is then left to cool in a suspended position.
The temperature of the ball is read at 2-minute intervals and reported in the table below. A simple
model for cooling the ball can be given as
´mcp
dT
dt “ hpT ´ T8q ` εσpT 4 ´ T 4
8q
where σ “ 5.67 ˆ 10´8 W/m2¨K4 is the Stefan-Boltzmann constant, h is the convection heat
transfer coefficient (W/m2K), and ε “ 1 is the emissivity of the ball. Using the data given,
estimate the indoor convection heat transfer coefficient.
t (min) T (K) t (min) T (K) t (min) T (K)
0 500 8 352.2 16 306.5
2 442.7 10 335.9 18 300.7
4 402.6 12 323.5 20 296.2
6 373.8 14 313.9
CA5.11 Consider a two-dimensional array (fi,j ) of a scalar physical quantity fpx, yq (temperature,
pressure, etc.) defined on a rectangular domain. The rate of change of fpx, yq along the path s is
referred to as the directional derivative, and it is determined as
df
ds “ n ¨ grad f “ cos θ
Bf
Bx ` sin θ Bf
By
where n “ cos θ i ` sin θ j is the unit vector along the direction-s, Bf{Bx, and Bf{By are obviously
the partial derivatives (see Fig. CA5.11).
Fig. CA5.11312  Numerical Methods for Scientists and Engineers
Write a computer program that reads the direction (θ) and a two-dimensional array (fi,j for
i “ 0, 1,...,n, and j “ 0, 1, 2,...,m) uniform in both directions-order, i.e., Δx and Δy are
constant. The program should compute df{ds and d2f{ds2 derivatives for every available data point
using second-order finite-difference formulas. Given data is the temperature distribution Tpx, yq on
[0,0.5]ˆ[0,0.5]. Use the given true data below to validate your computer program. Given: Data for
program verification: for θ “ π{6, Tp0.3, 0q “ 3.2063, Tp0.3, 0.1q “ 0.367, Tp0.3, 0.5q“´35.979,
Tp0, 0.1q “ 23.634, and Tp0.5, 0.1q“´7.1586.
y (m)
x (m) 0 0.1 0.2 0.3 0.4 0.5
0 58.2 64.9 71.1 75 71.8 65.6
0.1 56.2 63.1 70.1 76.9 75.9 69.1
0.2 53.5 60.3 67.1 73.6 76.4 71.0
0.3 50.2 56.8 63.2 69 72.5 70.6
0.4 46.5 52.9 58.9 64.3 67.8 67.9
0.5 42.6 48.7 54.5 59.5 63.1 64.3CHAPTER 6
Interpolation and
Extrapolation
LEARNING OBJECTIVES
After completing this chapter, you should be able to
‚ understand and explain the concept underlying Lagrange interpolations;
‚ approximate a set of discrete data using Lagrange polynomials;
‚ construct a divided difference table;
‚ understand and implement Newton’s divided-difference interpolation;
‚ construct forward, backward, and central difference tables for a set of uniformly
spaced discrete data points;
‚ apply forward, backward, and central difference interpolation formulas;
‚ explain the adverse effects of round-off on a difference table;
‚ discuss the Runge phenomenon and its consequences;
‚ understand and apply inverse interpolation;
‚ describe the concepts underlying cubic splines and apply them to discrete data;
‚ discuss the advantages and disadvantages of interpolation methods;
‚ explain bi-linear and bi-Lagrange interpolation and implement them on a set
of bivariate data;
‚ determine and use suitable interpolation formulas for extrapolations.
DISCRETE functions, also discussed in Chapter 5, are the result of either sampling or
experimentation. Most physical properties (density, viscosity, heat capacity, enthalpy,
pressure, etc.) used in computations are assumed to be smooth functions whose explicit
mathematical relationships are unknown. The physical properties of solids, liquids, or gases
are usually available as discrete (tabular) data as a function of temperature, pressure, etc.
In other words, the properties are only known at discrete points. On the other hand, even
though an ordinary differential equation in ypxq contains an infinite amount of information
on [a, b], its numerical solution is obtained only for a set of discrete data points because a
computer can handle only a finite amount of data.
A set of discrete data may be either uniformly or non-uniformly spaced (see Fig.
6.1a or 6.1b). Periodically sampled or collected data with digital instruments is uniformly
spaced. Differentiation, integration, or interpolation of a uniformly spaced set of discrete
DOI: 10.1201/9781003474944-6 313314  Numerical Methods for Scientists and Engineers
FIGURE 6.1: Discrete functions with (a) uniform or (b) nonuniform distribution.
data points is easier and requires less memory because abscissas need not be stored. That
is why numerical analysts prefer dealing with uniformly spaced discrete functions due to
their computational advantages.
Consider a smooth discrete function fpxq, which is defined by a set of discrete values
pxi, fpxiqq for i “ 1, 2,..., 8 (see Fig. 6.1). We frequently need to estimate the value of the
function for a point x “ c that lies in px1, x8q. The process of estimating fpcq, where c falls
between x1 ă c ă x8 is called interpolation. The estimation of the function values lying
outside the observed range, c ă x1 or c ą x8, is referred to as extrapolation.
When interpolation is mentioned, the first procedure that comes to mind is linear
interpolation. Consider two points, Apa, fpaqq and Bpb, fpbqq, as illustrated in Fig. 6.2. We
wish to estimate fpcq, where a ă c ă b. Linear interpolation is achieved by geometrically
rendering a straight line between A and B. All estimates for a ă c ă b lie on the straight
line connecting the points A and B, i.e., the interpolates can be obtained from
ˆfpxq “ fpaq`px ´ aq
fpbq ´ fpaq
b ´ a (6.1)
where ˆfpxq denotes all possible interpolates.
Linear interpolation yields satisfactory estimates in cases where the distribution of data
points does not exhibit a strong curvature. As shown in Fig. 6.2, y “ fpxq has a strong
curvature on [a, b]. The interpolation error, fpcq ´ ˆfpcq, is quite large for an interior c value,
but for values of c near the end points, x “ a or x “ b, the error decreases. In other words,
linear interpolation is insufficient to estimate a wide range of values for a function with
strong curvature and large interval sizes. A continuous function with strong curvature can
FIGURE 6.2: Graphical depiction of a linear interpolation.Interpolation and Extrapolation  315
FIGURE 6.3: The effect of interval size on linear interpolation.
be better estimated by straight lines only if the interval size is small, as shown in Fig. 6.3. A
remedy to improve the accuracy of interpolates is to decrease the interval size, i.e., increase
the number of data points for the same range. However, this is not possible, especially
when dealing with experimentally collected data. An alternative avenue is to represent the
data with a polynomial, which has the potential to account for the actual curvature of the
discrete function. Therefore, in most cases, it is necessary to use interpolating polynomials
of n ě 2 to obtain an improved estimate (interpolate) and to ensure better approximation.
In this chapter, numerical interpolation and extrapolation methods for uniformly and
non-uniformly spaced discrete functions are discussed. As interpolation functions, in this
text only polynomials and piecewise-defined polynomials (splines) are considered.
6.1 LAGRANGE INTERPOLATION
An alternative way of connecting a set of discrete data, aside from using straight lines, is
to use a polynomial that passes through all data points. In this context, Lagrange polyno￾mials play an important role in describing a set of discrete data points by a representative
polynomial.
Consider a set of discrete data points pxk, fkq for k “ 0, 1, 2,...,n, called interpolation
points. The abscissas xk are assumed to be distinct and may not necessarily be uniformly
spaced. A nth-degree polynomial can be used to represent the data set:
pnpxq “ c0 ` c1x ` c2x2 ` ... ` cnxn (6.2)
which passes through all (n ` 1) distinct data points should satisfy all interpolation points;
that is,
pnpxkq “ yk, k “ 0, 1, 2,...,n (6.3)
This condition is enforced to determine the unknown coefficients, c0, c1, c2,..., cn. To do
so, we seek a set of polynomials of degree n (Lkpxq for k “ 0, 1, 2,...,n) that satisfy the
following property:
Lkpxiq “ " 0,
1,
if k ‰ i
if k “ i (6.4)
where xk denotes n ` 1 distinct abscissas for k “ 0, 1, 2,...,n, and Lkpxq is so called the
Lagrange polynomials. Then a Lagrange interpolating polynomial pnpxq, passing through
n ` 1 points, can be constructed as follows:
fpxq – pnpxq “ L0pxqf0 ` L1pxqf1 ` ... ` Lnpxqfn “ ÿn
k“0
Lkpxqfk (6.5)
Note that, owing to Eq. (6.4), Eq. (6.5) satisfies Eq. (6.3) for every xk.316  Numerical Methods for Scientists and Engineers
Lagrange Interpolation Method
‚ The method provides an easy and simple way of constructing an
interpolating polynomial;
‚ It is very easy to adapt to computer programming;
‚ It is applicable to both uniform and non-uniform discrete functions;
‚ Once Lagrange polynomials are constructed, they can be used with
different ordinate sets (fk’s) as long as the number of data points and
the abscissas (xk’s) in the set are fixed.
‚ It is cpu-time intensive and unpractical for a large data set;
‚ Adding new data points to an existing data set increases the degree
of the interpolating polynomial, which has to be reconstructed from
scratch;
‚ Estimating the error bounds of an approximation is not easy;
‚ Errors outside the data range increase uncontrollably, which makes
the method too risky to use for extrapolations.
Then, for every xk, an nth-degree polynomial is defined as
Lkpxq“ckpx´x1qpx´x2q¨¨¨px´xk´1qpx´xk`1q¨¨¨px ´ xnq“ck
źn
i‰k
px´xiq (6.6)
where ck is a constant, and the factor px ´ xkq is skipped. Evaluating Eq. (6.6) at x “ xk
and applying the stipulated condition, Eq. (6.4), yields
1“ckpxk´x1qpxk´x2q¨¨¨pxk´xk´1qpxk´xk`1q¨¨¨pxk´xnq“ck
źn
i‰k
pxk´xiq (6.7)
Solving ck from Eq. (6.7) and substituting it into Eq. (6.6), we find
Lkpxq “ źn
i“0
i‰k
ˆ x ´ xi
xk ´ xi
˙
, k “ 0, 1, 2,...,n (6.8)
Note that Lkpxq “ 0 for all x “ xi’s, except x “ xk, resulting in Lkpxkq “ 1.
ERROR ASSESSMENT. First, we assume the interpolation points pxk, fkq do in fact represent
the true values of function fpxq at the given abscissas, and fpxq has (n ` 1) continuous
derivatives for all x. Letting pnpxq to be a unique Lagrange interpolating polynomial of at
most nth degree, passing the interpolating data points pxk, fkq for k “ 0, 1, 2,...,n, there
exists a point ξpxq for any x in rx0, xns such that
enpxq“fpxq´pnpxq“ px´x0qpx´x1q¨¨¨px´xnq
pn ` 1q! fpn`1q
pξpxqq, x0 ď ξpxq ď xn (6.9)
where enpxq is the error, defined as the difference between the true and interpolated values.
Equation (6.9) does not seem to be very useful since the true function that gives pxk, fkq
is unknown. fpn`1qpξq and ξpxq are also the other two unknowns. However, Eq. (6.9) would
provide bounds for the interpolation error if we can estimate the bounds of fpn`1qpξq. ForInterpolation and Extrapolation  317
a smooth function fpxq, we can anticipate its higher derivatives to be smooth as well. But
keep in mind that the higher derivatives of a function depicting sharp changes depict even
sharper changes, which in turn lead to large interpolation errors.
One of the significant results of Eq. (6.9) is that if a discrete function fpxq is truly
a polynomial of nth degree or lower, i.e., fpxq “ pmpxq, pm ď nq, then the interpolating
polynomial, Eq. (6.5), reproduces the true polynomial, in which case the absolute error is
zero since fpn`1qpξq will be zero for all x. Otherwise, the value estimated using a Lagrange
interpolating polynomial will contain a certain amount of error. The magnitude of this error
is influenced by a number of factors, such as the length of the interval, rx0, xns; the interval
size of the subintervals, Δxk; the position of x in the interval; the degree of the interpolating
polynomial; and the number of discrete data points in the set.
EXAMPLE 6.1: Lagrange Interpolation in Action
A set of data describing the efficiency (η) of the cross-flow turbine as a function of
rotor inlet relative velocity (V ) is tabulated below:
V 0.08 0.25 0.50 0.90
ηpV q 0.25 0.625 0.81 0.43
(a) For efficiency, find a Lagrange interpolating polynomial that uses all data points;
(b) Plot the interpolating polynomial and the true error for values in the range
0 ď V ď 1 and discuss the accuracy of the polynomial approximation; (c) Use the
interpolating polynomial in Part (a) to estimate η(0.4), η(0.7), and η(0.85). The true
theoretical efficiency is defined as
ηpV q “ 2V
´
κ b ´ V ` κ b aκ2 ` V 2 ´ 2κbV ¯
where κ “ 0.93 and b “ cospπ{10q.
SOLUTION:
(a) Employing Eq. (6.8) to the relative velocity data leads to
L0pV q “ pV ´ 0.25qpV ´ 0.50qpV ´ 0.9q
p0.08 ´ 0.25qp0.08 ´ 0.50qp0.08 ´ 0.9q
,
L1pV q “ pV ´ 0.08qpV ´ 0.50qpV ´ 0.9q
p0.25 ´ 0.08qp0.25 ´ 0.50qp0.25 ´ 0.9q
,
L2pV q “ pV ´ 0.08qpV ´ 0.25qpV ´ 0.9q
p0.50 ´ 0.08qp0.50 ´ 0.25qp0.50 ´ 0.9q
,
L3pV q “ pV ´ 0.08qpV ´ 0.25qpV ´ 0.50q
p0.90 ´ 0.08qp0.90 ´ 0.25qp0.90 ´ 0.50q
Using Eq. (6.5), the interpolating polynomial is found as
ηpV q – p3pV q“p0.25qL0pV q`p0.625qL1pV q`p0.81qL2pV q`p0.43qL3pV q
which is a cubic polynomial that can be used to estimate the efficiency for any value
of V in the range 0 ď V ď 1.
(b) A comparative plot of the true efficiency, ηpV q, with the third-degree inter￾polating polynomial and the distribution of the true error, e3pV q, in the 0 ď V ď 1318  Numerical Methods for Scientists and Engineers
range are presented in Fig. 6.4a and b, respectively. For V ă 0.6, the approximat￾ing polynomial agrees very well with the true efficiency. Only slight deviations are
observed for V ą 0.6. However, when the true error distribution is examined, some
discrepancies between the true and interpolating polynomials do not go unnoticed.
The true error e3pV q depends on the value of V , which depicts a maximum and
two minimums on [0,1]. The errors are constrained near the interpolation points
rVk, Vk`1s which anchor the approximation (the true error is zero at the interpola￾tion points due to enforcing Eq. (6.3)).
FIGURE 6.4
(c) The estimates with the interpolating polynomial are found as η(0.4)–
p3p0.4q “ 0.78314, η(0.7)– p3p0.7q “ 0.70446, and η(0.85) – p3p0.85q “ 0.5116.
Note that in Fig. 6.4b, the deviation from horizontal “zero line” in the range
0 ă V ă 0.55 is very small, resulting in a small interpolation error for V “ 0.4
(=0.78617´0.78313=0.00304). However, the approximations for η(0.7) and η(0.85)
yield relatively larger errors (0.02331 and 0.01776, respectively) since the interpola￾tion points are close to V « 0.77, where the error is largest.
Discussion: An interpolating polynomial that passes all four data points is a cubic
equation. Expanding and simplifying all-like expressions yield
ηpV q – 1.0856 V 3 ´ 4.39125 V 2 ` 3.55848 V ´ 0.00713
which, using this form, will significantly reduce the computation time if it is to be
used numerous times in a computation.
The true errors are small near interpolation (data) points but almost always
depict an increase toward the midpoint of any two interpolation points. The absolute
deviations in this example are «0.002 in [0.08,0.25], 0.0031 in [0.25,0.5], and ´0.03
in [0.5,0.9], which indicates relatively good agreement. Note that the interpolating
polynomial provides very good estimates when the distance between two data points
is small; however, the errors tend to increase as the distance between data points
increases.
If an interpolating polynomial is to be used numerous times, in￾stead of applying Eq. (6.5), it is more convenient and practical to
express it in simplified power form, pnpxq “ anxn `¨¨¨`a1x`a0,
or preferably in the nested form as
pnpxq “ pppanx ` an´1qx `¨¨¨qx ` a1qx ` a0Interpolation and Extrapolation  319
Pseudocode 6.1
Function Module LAGRANGE_EVAL (n, xval, x,f)
\ DESCRIPTION: A pseudo-function module for evaluating the interpolant
\ yval “ ypxvalq using Lagrange interpolation.
\ USES:
\ LAGRANGE_P:: A module to compute Lagrange polynomials (given below).
Declare: x0:n, f0:n, L0:n \ Declare array variables
LAGRANGE_P(n, xval, x,L) \ Go to module to find Lkpxvalq’s
fval Ð 0 \ Initialize accumulator
For “
k “ 0, n‰ \ Loop k: Accumulator for interpolant
fval Ð fval ` Lk ˚ fk \ Accumulate Lkpxvalq ˚ fk products
End For
LAGRANGE_EVALÐ fval \ Set fval to LAGRANGE_EVAL
End Function Module LAGRANGE_EVAL
Module LAGRANGE_P (n, xval, x,L)
\ DESCRIPTION: A pseudomodule evaluating the Lagrange polynomials at xval.
Declare: x0:n, L0:n \ Declare array variables
For “
k “ 0, n‰ \ Loop k: Accumulator to find Lk
Lk Ð 1 \ Initialize accumulating variable Lk
For “
i “ 0, n‰ \ Loop i: Loop to compute products
If “
i ‰ k
‰
Then \ Skip block for k “ i
Lk Ð Lk ˚ pxval ´ xiq{pxk ´ xiq \ Product accumulation
End If
End For
End For
End Module LAGRANGE_P
A pseudomodule, LAGRANGE_P, evaluating the Lagrange polynomials, as well as a
pseudomodule, LAGRANGE_EVAL, computing an approximation using Lagrange’s method,
are presented in Pseudocode 6.1. LAGRANGE_P evaluates all possible Lagrange polynomials
for x “ xval. (It is important to note that in this method the subscripts of the n`1 discrete
data points range from 0 to n, which leads to Lagrange polynomials of nth degree.) As input,
it requires the number of discrete data points n (i.e., the number of data points minus 1),
the data points pxk, fkq for all k, and the abscissa (xval) at which interpolation is sought.
The module LAGRANGE_P computes Lkpxvalq for all k. The For-loop over k computes Lk’s
using Eq. (6.8). A product accumulator Lk is initialized: Lk Ð 1. The For loop over i is
used to accumulate pxval ´ xiq{pxk ´ xiq products, except i “ k, which is skipped with an
If-construct. Once both loops are completed, the Lagrange polynomials calculated for xval
become available for interpolation and passed to the calling module.
The module LAGRANGE_EVAL evaluates an interpolant yval “ fpxvalq using Lagrange
polynomials. The module requires an input value of n, xval for which interpolant is sought,
and the discrete data points (xk and yk for k “ 0, 1,...,n). Although not included here to
save space, it is always a good idea to do a data check at the beginning of every module
to ensure the input (like xval) is within the valid data range. The actual interpolation
procedure for x “ xval is carried out in this module. An accumulator fval is used to
accumulate Lkpxvalq ˚ fk products for all k, i.e., perform the summation in Eq. (6.5). Once
the accumulation is completed, the result is set to LAGRANGE_EVAL.320  Numerical Methods for Scientists and Engineers
EXAMPLE 6.2: Runge’s phenomenon with a remedy
The energy dependence of neutron resonance crosssection, depicted in the figure
below, is described by the Breit-Wigner formula,
σpEq “ σ0
1 ` 4pE ´ E0q2{Γ2
where E0 is the resonance energy (MeV), Γ is the width half maximum (MeV), σ0 is
the peak resonance cross section (barn). By defining y “ σpEq{σ0 and x “ E ´ E0,
the normalized cross section for Γ “ 2{3 MeV can be expressed as y “ 1{p1 ` 9x2q.
Establish a set of data with (i) x“´2, ´1.6, ´1.2, 1.2, 1.6, 2, (ii) x“´2, ´1.8, ´1.6,
´1.4, ´1.2, 0, 1.2, 1.4, 1.6, 1.8, 2, and (ii) x“´2, ´1.9, ´1.6, ´1.2, ´0.6, 0, 0.6, 1.2,
1.6, 1.9, 2. For each case, (a) construct the Lagrange interpolating polynomial that
passes through all data points; (b) compare the distributions of true and approximate
values and discuss the results.
SOLUTION:
Using the abscissas provided in (i), (ii), and (iii), three sets of interpolation data
(not listed here) are established by yk “ ypxkq for all k.
(i) The corresponding interpolating polynomials, respectively, are found as
p4pxq “ 0.06427x4 ´ 0.367053x2 ` 0.46692
p10pxq“´0.2883x10 ` 2.569131x8 ´ 7.83586x6 ` 9.898657x4 ´ 5.082046x2 ` 1
q10pxq“´0.032702x10 ` 0.39718x8 ´ 1.78882x6 ` 3.63707x4 ´ 3.21813x2 ` 1
Note that the interpolating polynomials resulted in 4th and 10th-degree polyno￾mials instead of the expected 5th and 11th degree polynomials. This is a natural
consequence of working with a discrete function that is symmetric about x “ 0.
The true function, interpolating polynomials, and interpolation points are com￾paratively presented for all three cases in Fig. 6.5. In Fig. 6.5a, the interpolating
polynomial p4pxq yields a significant underestimation at x “ 0 (the midpoint of the
interval) and moderate deviations elsewhere. In Fig. 6.5b, on the other hand, the
p10pxq (obtained with uniformly spaced 11 data points) clearly exhibits very poor
performance in between each pair of interpolation points, especially near the end￾points of the data range. Also notice that increasing the number of interpolation
points worsened the accuracy of the approximations rather than improving them.Interpolation and Extrapolation  321
FIGURE 6.5: The distribution of the true function and the Lagrange interpolating poly￾nomial for uniformly spaced (a) 6-interpolation points, (b) 11-interpolation points,
and (c) non-uniformly spaced 11-interpolation points.
In Fig. 6.5c, the polynomial q10pxq obtained non-uniformly spaced interpolation
points with finer spacings near the ends of the interval largely mitigates the devia￾tions near the edges of the data interval.
Discussion: Generally, a sufficient number of data points is necessary in order for
an interpolating polynomial to closely represent the true distribution. As a solution
for improving the accuracy of an interpolating polynomial, beginners to numerical
interpolation tend to increase the number of interpolation points, i.e., the degree
of approximating polynomial. Nevertheless, high-degree polynomials are usually not
suitable for interpolations because they lead to non-physical oscillations that cannot
be eliminated by further increasing the degree of the polynomial. This observation is
known as the Runge’s phenomenon and it is observed when using a large number of
“uniformly spaced data points,” as in this example. It exhibits itself as oscillations
at the endpoints of the uniformly spaced data set when a high-degree interpolating
polynomial is employed.
One of the following remedies may be used to mitigate the adverse effects of
Runge’s phenomenon:
(i) use piecewise interpolating polynomials (i.e., instead of using a single interpo￾lating polynomial for all, the data interval can be divided into several data sets
and employ lower degree interpolation polynomials on each),
(ii) use non-uniformly spaced interpolation points that are more densely stacked
by the endpoints of the interval (as in q10pxq),
(iii) use spline curves that are piecewise polynomials (see Section 6.3), or
(iv) fit the data to a low-degree polynomial (or a suitable non-polynomial model)
using the method of least squares (see Chapter 7).322  Numerical Methods for Scientists and Engineers
6.2 NEWTON’S DIVIDED DIFFERENCES
Consider a set of discrete data points: pxk, fkq for k “ 0, 1, 2,...,n and an interpolating
polynomial of the form:
fpxq – pnpxq“a0 ` a1px´x0q ` a2px´x0qpx´x1q ` a3px´x0qpx´x1qpx´x2q
` ... ` anpx´x0qpx´x1q...px´xn´1q “ ÿn
k“0
ak
˜k
ź´1
i“0
px´xiq
¸ (6.10)
where an’s are the unknown coefficients.
The polynomial satisfies the interpolation points, so imposing fpxkq “ pnpxkq “ fk
for k “ 0, 1, 2,...,n leads to a system of linear equations with (n ` 1) unknowns. The
solution to this system is obtained easily by applying the forward substitution procedure.
For instance, when x “ x0, all but the first term of Eq. (6.10) vanishes, yielding f0 “ a0.
For x “ x1, all the terms in the polynomial except for the first two will vanish. Solving for
a1, after making use of a0 “ f0, gives
a1 “ f1 ´ f0
x1 ´ x0
(6.11)
Setting x “ x2 in Eq. (6.10), utilizing a0 and a1, and solving for a2 yields
a2 “ 1
x2 ´ x0
ˆ f2 ´ f1
x2 ´ x1
´ f1 ´ f0
x1 ´ x0
˙
(6.12)
Continuing in this fashion, the rest of the coefficients are obtained as complicated expressions
of xk and fk’s. Nonetheless, there is an easy way to reproduce the coefficients by employing
the concept of divided differences.
The divided differences are defined as differences starting from the 0th to nth divided
difference. The zeroth divided-difference is the discrete function value at x “ xk; that is,
frxks “ fpxkq “ fk
Substituting this value into Eq. (6.11) gives the first divided-difference of f:
frxk, xk`1s “ frxk`1s ´ frxks
xk`1 ´ xk
(6.13)
Equation (6.13), which is also symmetrical, can be written as
frxk, xk`1s “ frxk`1, xks “ frxk`1s
xk`1 ´ xk
` frxks
xk ´ xk`1
(6.14)
The second divided-difference is obtained from Eq. (6.12) as
frxk, xk`1, xk`2s “ frxk`1, xk`2s ´ frxk, xk`1s
xk`2 ´ xk
(6.15)
This process is continued in a recursive manner, yielding a general recurrence expression
for the nth divided difference as follows:
frx0, x1,...,xks “ frx1, x2,...,xks ´ frx0, x1,...,xk´1s
xk ´ x0
(6.16)Interpolation and Extrapolation  323
TABLE 6.1: Construction of Newton’s divided-difference table.
Upon inspecting Eqs. (6.11) and (6.12), the first two coefficients of Newton’s interpo￾lating polynomial are found to be
a1 “ frx0, x1s, a2 “ frx0, x1, x2s “ frx1, x2s ´ frx0, x1s
x2 ´ x0
(6.17)
Finally, the nth coefficient results in
an “ frx0, x1,...,xns (6.18)
A simple way to construct the divided differences for Newton’s interpolation formula is to use
the recursive relationships to generate the divided differences through the Newton’s Divided
Difference Table. This table is constructed by filling in n`1 entries of the first column with
the abscissas, xi’s, and the second column with the zeroth divided difference (fi’s). Equation
(6.16), is then used to systematically fill in the entries of the subsequent columns. That is,
the divided-difference in the current column is calculated using the divided-differences from
the previous column directed by arrows, as shown in Table 6.1. Once the table is filled out,
the coefficients of the Newton interpolating polynomial (an for i “ 0, 1,...,n) are available
in the first row. In practice, there is no need to store the entire table since only the first
row entries are required in the interpolation formula.
The final form of the Newton’s interpolating polynomial is written as
fpxq “ frx0s ` frx0, x1spx ´ x0q ` frx0, x1, x2spx ´ x0qpx ´ x1q
` frx0, x1, x2, x3spx ´ x0qpx ´ x1qpx ´ x2q ` ...
` frx0, x1,...,xnspx ´ x0qpx ´ x1q...px ´ xn´1q
(6.19)
Equation (6.19) can also be expressed as a sequence of nested multiplications and additions,
which is suitable for computer programming because it reduces computational efforts.
fpxq “ a0 ` px ´ x0q pa1 ` px ´ x1q t a2 ` px ´ x2qp... pan´1 ` anpx ´ xnqquq (6.20)
For a set of interpolation points, Newton’s divided difference table depends on the
ordering of the abscissas. The interpolating polynomial, unlike Lagrange interpolation, can
be easily updated as interpolation points are removed or new points are added. The divided￾difference table can be extended to include the new additional differences without requiring
reconstructing the table from scratch. Strictly speaking, when the interpolation formula
is expanded and simplified, all orderings yield the same unique interpolating polynomial.324  Numerical Methods for Scientists and Engineers
TABLE 6.2: Data updating in the Newton’s divided-difference table.
However, for a large data set, the best results are obtained with xn ą xn´1 ą ... ą x2 ą x1.
The reconstruction of the divided difference table upon adding new data, px4, f4q, to an
existing set is depicted in Table 6.2. This process leads to computing the divided-differences
and filling in the last spot of each column in the table.
Equation (6.19) may be used to cover all or partial interpolation points. This property
of Newton’s interpolating polynomials can be used to obtain all available interpolating
polynomials. Starting with the first-degree (linear) interpolating polynomial for any x, the
degree of the interpolating polynomial can be increased recursively by adding new terms.
In this case, a series of low-degree interpolating polynomials can be expressed as
p1pxq “ frx0s ` frx0, x1spx ´ x0q,
p2pxq “ p1pxq ` frx0, x1, x2spx ´ x0qpx ´ x1q,
p3pxq “ p2pxq ` frx0,...,x3spx ´ x0qpx ´ x1qpx ´ x2q,
¨¨¨ ¨¨¨
pnpxq “ pn´1pxq ` frx0,...,xns
n
ź´1
k“0
px ´ xkq,
(6.21)
where p1pxq, p2pxq, and p3pxq are linear, quadratic, and cubic interpolating polynomials.
ERROR ASSESSMENT. A polynomial passing through n`1 interpolation points has a degree
of n or less. As a matter of fact, when expanded into traditional polynomial form, Newton’s
interpolating polynomial exactly reproduces Lagrange’s interpolating polynomial. Thus,
the error term of Newton’s interpolating polynomial is identical to that of the equivalent
Lagrangian polynomial, and so the error term associated with the nth degree interpolating
polynomial can also be explained by Eq. (6.9). This means that if a set of discrete data is
truly represented by a polynomial of nth degree or less, the error will be zero regardless
of x. This event can be visually identified from the divided difference table, where all
entries in a column become the same. As with the Lagrangian interpolation polynomials,
the interpolation errors are smaller near the endpoints of an interval, pxk, xk`1q, between
two consecutive interpolation points and larger roughly toward the middle.Interpolation and Extrapolation  325
Newton’s Divided-Difference Method
‚ The method can be used to construct an interpolating polynomial of
any degree, so long as there are sufficient data points;
‚ The interpolating polynomial resulting from the method provides
greater advantage because it lends itself to nested multiplications;
‚ It is efficient and stable, and it allows adding more data points to
obtain a higher-degree interpolating polynomial without discarding
the existing data;
‚ A low-degree interpolating polynomial can yield very accurate results
if the data is smooth.
‚ The method is a bit difficult to understand and implement;
‚ It is expensive and unpractical for interpolating large data sets;
‚ It is sensitive to the errors in the data set;
‚ It may lead to inaccurate results when the data points are closely
spaced or when the discrete function is highly oscillatory;
‚ Estimating the error bounds is not easy;
‚ The errors outside the data interval increase uncontrollably, and it is
too risky to use it for extrapolation purposes.
For smooth functions, the difference between two subsequent interpolating polynomials
is expected to decrease, i.e., |pnpxq ´ pn´1pxq| Ñ 0. This difference, Rnpx0,...,xnq, is the
error of adding or “not” adding new terms to the interpolating polynomial.
pnpxq ´ pn´1pxq “ Rnpx0,...,xnq “ frx0,...,xns
n
ź´1
k“0
px ´ xkq (6.22)
In general, any interpolation process should be carried out with an interpolating poly￾nomial of the smallest degree possible. A linear (p1pxq) or quadratic (p2pxq) interpolation in
the neighborhood of the interpolating points may be sufficient. In most cases, interpolating
polynomials of at most 3rd or 4th degrees, constructed from the nearest data points, will
yield very good results. Any high-degree interpolating polynomials derived from a large set
of data should be treated with caution due to the implications that may also give rise to
Runge’s phenomenon.
Pseudocode 6.2
Module NEWTONS_TABLE (n, x,f, A)
\ DESCRIPTION: A pseudomodule to generate the Divided-Difference table.
Declare: x0:n, f0:n, a0:n,0:n \ Declare array variables
A Ð 0 \ Initialize table entries
For “
k “ 0, n‰ \ Place fk’s into the 1st column
ak0 Ð fk \ Set ak0 “ fk for all k
End For
For “
j “ 1, n‰ \ Loop j: Sweep column from top to bottom
For “
k “ 0, n ´ j
‰ \ Loop k: Sweep
akj Ð pak`1,j´1 ´ ak,j´1q{pxk`j ´ xkq \ Compute coefficients
End For
End For
End Module NEWTONS_TABLE326  Numerical Methods for Scientists and Engineers
A pseudomodule, NEWTONS_TABLE, generating Newton’s divided difference table is
presented in Pseudocode 6.2. The module requires a set of n`1 discrete data points (xi, fi)
as input. All divided differences, aij ’s, are computed and stored on matrix A, which is also
the output of the module. The first column of the table is the zeroth divided difference
(ak0) and the ordinates are placed into the first column simply by substitution (ak0 “ fk).
Starting with the second column (j “ 1), the differences are computed from top to bottom
using the entries of the prior column. The table is constructed column by column, and with
each column, the number of successive differences decreases by one. Thus, the row sweep
for the jth column covers from k “ 0 to pn´jq. When the table is complete, the coefficients
of the interpolating polynomial are available on the first row of the table. If the divided
differences are to be computed only for constructing Newton’s interpolating polynomials,
then there is no need to store the entire table. This proposition has been incorporated in
Pseudocode 6.3.
EXAMPLE 6.3: Interpolation with non-uniform data sets
Consider the following tabular data for the specific heat of a gas mixture.
T (K) 282 400 500 620 848
cppTq (kJ/kg.K) 10.3 9.14 8.74 8.499 8.3
(a) Use Newton’s divided differences to obtain an interpolating polynomial for cppTq
that passes through all data points; (b) given cppTq “ 8.14 ´ 100{T ` 200000{T2
as the true specific heat, plot the interpolation error e4pTq for the interpolating
polynomial in the range [270, 870] K and interpret your results.
SOLUTION:
(a) We note that the discrete data is non-uniformly spaced, and the interpolating
polynomial passing all data points will be a 4th-degree polynomial (n “ 5 ´ 1 “ 4).
In other words, Newton’s interpolating polynomial will have the following form:
p4pTq “cprT0s ` cprT0, T1s pT ´ T0q ` cprT0, T1, T2s pT ´ T0q pT ´ T1q
`cprT0, T1, T2, T3s pT ´ T0q pT ´ T1qpT ´ T2q
`cprT0, T1, T2, T3, T4s pT ´ T0q pT ´ T1qpT ´ T2qpT ´ T3q
The coefficients of the polynomial are easily determined once the divided dif￾ference table is constructed (see Table 6.3). The second and third columns of the
table have been reserved for the original data set. The entries of subsequent columns
are computed using the entries of the current column along with the recurrence
formula—Eq. (6.16).
TABLE 6.3
i Ti cp,i cprTi, Ti`1s cprTi, Ti`1, Ti`2s cprTi,...,Ti`3s cprTi,...,Ti`4s
0 282 10.3 ´0.009831 2.6745ˆ10´5 ´5.2344 ˆ 10´8 6.9647ˆ10´11
1 400 9.14 ´0.004000 9.0530ˆ10´6 ´1.2924 ˆ 10´8
2 500 8.74 ´0.002008 3.2630ˆ10´6
3 620 8.499 ´0.000873
4 848 8.30
Once the table is completed, the entries in the first row are the coefficients of theInterpolation and Extrapolation  327
interpolating polynomial. Upon substituting the coefficients, we obtain a 4th-degree
interpolating polynomial as
cppTq – p4pTq “ 10.3 ´ 0.00983051pT ´ 282q
` 2.67455 ˆ 10´5pT ´ 282qpT ´ 400q
´ 5.23444 ˆ 10´8pT ´ 282qpT ´ 400qpT ´ 500q
` 6.96471 ˆ 10´11pT ´ 282qpT ´ 400qpT ´ 500qpT ´ 620q
(b) The distribution of the true error, e4pTq “ cppTq ´ p4pTq, is illustrated in
Fig. 6.6. The magnitude of the interpolation error increases toward the middle of
the subintervals, which are bounded on each side by an interpolation point that acts
as an anchor. However, the error outside the original temperature range (T ă 282
or T ą 848) does grow uncontrollably.
FIGURE 6.6
Discussion: The coefficients of Newton’s divided difference polynomial are placed in
the first row of the divided difference table. The interpolating polynomial produces
exactly the data set for the interpolation points, which is why the errors at the
original interpolation points are zero, just as in the Lagrange interpolation. Note
that when dealing with experimental data, the true distribution (and the true error)
is almost always unknown.
In Pseudocode 6.3, the pseudomodule, NEWTONS_DD_COEFFS, calculating the co￾efficients of Newton’s divided difference interpolating polynomial and a function module,
NEWTONS_DD_EVAL, evaluating the interpolant are presented. The divided differences in
NEWTONS_DD_COEFFS are obtained in the same manner as in Pseudocode 6.2 after ini￾tializing the zeroth difference, ak “ fk for k “ 0, 1, 2,...,n. However, the coefficients are
computed from the bottom to the top of each column using Eq. (6.16). In the mean time,
the divided differences are overwritten each time from the last storage location backward
so that, at the end, only the desired coefficients remain. This process is repeated column
by column until all the data is used. The function module NEWTONS_DD_EVAL simply
applies Eq. (6.10) to compute fval. Besides xval, this module requires the coefficients for
the interpolating polynomial (aij ) and the data set (xi, fi) as input. The program computes
the difference from Eq. (6.22) to determine whether the interpolant has converged or not. If
the discrete function is smooth and the number of data points is sufficient, the interpolant
could converge within a preset tolerance level (i.e., |Rnpx0,...,xnq| ă ε) in which case the
interpolating polynomial is truncated and the rest of the terms are discarded.328  Numerical Methods for Scientists and Engineers
Pseudocode 6.3
Function Module NEWTONS_DD_EVAL (n, xval, x,f, a)
\ DESCRIPTION: A pseudofunction to find an interpolation fval “ fpxvalq, using
\ the Newton’s Divided-Difference (NDD).
\ USES:
\ NEWTONS_DD_COEFFS:: Module computing NDD coefficients.
\ CAUTION: For ε, use MACH_EPS or set a value internally.
Declare: x0:n, f0:n, a0:n \ Declare array variables
NEWTONS_DD_COEFFS(n, x,f, a) \ Get Newton’s Divided Diff. coefficients
fval Ð a0 \ Initialize interpolant
prod Ð 1; k Ð 0 \ Initialize accumulator and counter
Repeat \ Conditional accumulator
k Ð k ` 1 \ Count terms
prod Ð prod ˚ pxval ´ xk´1q \ Accumulate, prod “ śk´1
i“0 px ´ xiq
Rn Ð prod ˚ ak \ Evaluating a term, Eq. (6.22)
fval Ð fval ` Rn \ Add term to interpolant
Until “
|Rn| ă ε Or k “ n
‰ \ Truncate the sum if |Rn| ă ε or k “ n
NEWTONS_DD_EVALÐ fval \ Set interpolant to function
End Function Module NEWTONS_DD_EVAL
Module NEWTONS_DD_COEFFS (n, x,f, a)
\ DESCRIPTION: A pseudomodule generating the NDD interpolation coefficients.
Declare: x0:n, f0:n, a0:n \ Declare array variables
a Ð f \ Initialize the coefficients, ai Ð fi for all i
For “
j “ 1, n‰ \ Loop j: Sweep columns from left to right
For “
k “ n, j,p´1q
‰ \ Loop k: Differences from last to 1st row
ak Ð pak ´ ak´1q{pxk ´ xk´j q \ Divided differences
End For
End For
End Module NEWTONS_DD_COEFFS
EXAMPLE 6.4: A unique feature of divided-difference table
The following data was obtained from a study investigating the effect of reaction
time t (in hours) on the yield of a chemical product y (mol %). Use the given data
to construct an interpolating polynomial for the yield that passes through all data
points.
t (h) 1 2 4 8 11
yptq (mol%) 24.7 44 70 88.4 91.7
SOLUTION:
The number of data points in this data set is 5, so the interpolating polynomial
passing through all points will be at most a 4th-degree polynomial (n “ 5 ´ 1 “ 4).
Using the data furnished, the Newton’s divided differences are computed, and the
resulting table is presented in Table 6.4. Notice that the third divided-differences
(entries of the sixth column) are 0.1, and the fourth divided-difference (i.e., lastInterpolation and Extrapolation  329
column) is “zero.” This indicates that the set of data can be exactly represented by
a third-degree polynomial because the fourth divided difference is zero.
TABLE 6.4
i ti yi yrti, ti`1s yrti, ti`1, ti`2s yrti,...,ti`3s yrti,...,ti`4s
0 1 24.7 19.3 ´2.1 0.1 0
1 2 44.0 13.0 ´1.4 0.1
2 4 70.0 4.6 ´0.5
3 8 88.4 1.1
4 11 91.7
Finally, using the NDD coefficients in the first row of the table, the NDD inter￾polating polynomial yields
yptq “ 24.7 ` pt ´ 1q p19.3 ` pt ´ 2q p´2.1 ` pt ´ 4q p0.1qqq
“ 0.1 t
3 ´ 2.8 t
2 ` 27 t ` 0.4
Discussion: This resulting interpolating polynomial is, in fact, the true polynomial
from which the data was generated. Also notice that the interpolating polynomial
turned out to be a 3rd-degree polynomial, even though we initially predicted that
the interpolating polynomial would be at most 4th-degree.
If a discrete data set contains noise or experimental errors, the
interpolating polynomial that passes through every available data
point will be tainted due to the inclusion of the errors in the
interpolating polynomial.
6.3 NEWTON’S FORMULAS FOR UNIFORMLY SPACED DATA
Newton’s divided-difference or Lagrange methods can certainly be applied to uniformly
spaced data. However, using interpolating polynomials or formulas especially suited for
uniform data not only simplifies formulations but also reduces the computation time con￾siderably. In this section, the derivation and use of interpolating polynomials are emphasized
and customized for uniformly spaced data.
Consider a set of uniformly-spaced data: (xk, fk) for k “ 0, 1,...,n, where xk “ x0`kh
and fk “ fpxkq. We make use of the (forward, backward, and central) difference operators
(see Chapter 5) to obtain a more compact form for the interpolation formulas:
Δfk “ fk`1 ´ fk, ∇fk “ fk ´ fk´1, δfk “ f
k` 1
2
´ f
k´ 1
2
(6.23)
Also recall that the nth-order difference operators are related to lower orders with the
following recurrence relationships:
Δnfk “ ΔpΔn´1fkq, ∇nfk “ ∇p∇n´1fkq, δnfk “ δpδn´1fkq (6.24)
Newton’s interpolation formula can be used to obtain more simplified interpolating
polynomials for uniformly spaced data points. Several versions of the interpolation formula
are known as Gregory-Newton interpolation formulas. Now consider a set of uniform data330  Numerical Methods for Scientists and Engineers
FIGURE 6.7: An example of uniform discrete data distribution.
on [x0, xn] with n intervals. We seek a high-order interpolation formula for any x in xk ď
x ď xk`1 as shown in Fig. 6.7. Noting that a0 “ frxks “ fk and making use of the forward
difference operator, Eqs. (6.11) and (6.12) can be rewritten as
a1 “ fk`1 ´ fk
h “ Δfk
h (6.25)
a2 “ 1
2h
ˆΔfk`1
h ´ Δfk
h
˙
“ Δ2fk
2h2 (6.26)
The remaining coefficients are obtained in the same way, leading to a generalized expression:
an “ Δnfk
n! hn (6.27)
where Δnfk is the nth forward-difference evaluated at the base point xk, and Δ0fk “ fk.
Substituting Eqs. (6.25), (6.26), and (6.27) (i.e., the modified coefficients) into Eq.
(6.10) yields
fpxq – pnpxq “ fk ` px ´ xkq
h Δfk ` px ´ xkqpx ´ xk ´ hq
2!h2 Δ2fk
` px ´ xkqpx ´ xk ´ hqpx ´ xk ´ 2hq
3!h3 Δ3fk ` ...
` px ´ xkqpx ´ xk ´ hq...px ´ xk ´ pn ´ 1qhq
n!hn Δnfk
(6.28)
which can be expressed in a more compact form as
fpxq – pnpsq “ fk ` s Δfk `
sps ´ 1q
2! Δ2fk `
sps ´ 1qps ´ 2q
3! Δ3fk ` ...
`
sps ´ 1q...ps ´ n ` 1q
n! Δnfk “ ÿn
m“0
ˆ s
m
˙
Δmfk
(6.29)
where s “ px ´ xkq{h is referred to as the interpolating variable and
ˆ s
m
˙
“ sps ´ 1q¨¨¨ps ´ m ` 1q
m!
Equation (6.29) is referred to as the Gregory-Newton Forward Interpolation formula (GN￾FWD-IF). Note that the forward differences Δfk, Δ2fk, Δ3fk, ..., etc., correspond to theInterpolation and Extrapolation  331
FIGURE 6.8: Construction of (a) forward, (b) backward, (c) central, (d) modified
central difference tables.
values in row xk. Likewise, an alternative interpolation formula, the Gregory-Newton Back￾ward Interpolation formula (GN-BKWD-IF), can also be constructed by using backward
differences as
fpxq – pnpsq “ fk ` s∇fk `
sps ` 1q
2! ∇2fk `
sps ` 1qps ` 2q
3! ∇3fk ` ...
`
sps ` 1q...ps ` n ´ 1q
n! ∇nfk “ ÿn
m“0
p´1q
m
ˆ
´s
m
˙
∇mfk
(6.30)
where s has the usual definition. Both interpolation formulas, Eq. (6.29) and Eq. (6.30),
satisfy pn ` 1q data points.
It may be tempting to add as many terms as are available in the Gregory-Newton in￾terpolation polynomials to obtain the most accurate approximation; however, if the discrete
function is not smooth, adding up all or most terms does not guarantee improved accuracy.
Adding each term in the interpolation polynomial increases the degree of the interpolating
polynomial by one; that is, for the backward interpolation formula, we write
Linear : p1psq “ fk ` s ∇fk,
Quadratic : p2psq “ p1psq ` 1
2!sps ` 1q ∇2fk,
Cubic : p3psq “ p2psq ` 1
3!sps ` 1qps ` 2q ∇3fk , ...
The forward and backward differences (Δnfk and ∇nfk) appearing in Eqs. (6.29) and
(6.30) can be easily obtained by constructing difference tables similar to Newton’s divided
difference table. A forward difference table, in Fig. 6.8a, is constructed in ascending order
with additional columns consisting of the differences between the values in the preceding
column, i.e., Δnfk “ Δn´1fk`1 ´ Δn´1fk (or ∇nfk “ ∇n´1fk ´ ∇n´1fk´1 for backward
differences). It should be pointed out that the numerical values of the differences in the332  Numerical Methods for Scientists and Engineers
forward and backward difference tables are the same, but their notations (i.e., locations in
the table) are different. In other words, ∇f2 “ f2 ´ f1 and Δf1 “ f2 ´ f1 represent the
same numerical value, but Δf1 is placed into the first row while ∇f2 is placed into the
second row, as shown in Figs. 6.8a and 6.8b. On the other hand, the lower diagonal in the
forward difference table or the upper diagonal in the backward difference table cannot be
filled because an entry is lost for every new column for which a new set of differences is
computed. The table size depends on the number of data points in the data set.
The GN-FWD or GN-BKWD interpolation formulas are used almost exclusively for
interpolations near the beginning or end of a table, respectively. Thus, these formulas may
not be appropriate when interpolating for x values near the middle of the tables. For such
cases, interpolating formulas that make use of central differences are preferred. A central
difference table is similarly constructed by using the central differences. In Fig. 6.8c, a new
line (in plain background) between each line of data (with highlighted background) is in￾serted as shown. At this point, we define the lines containing the original data as “full lines,”
and distinguish the new lines from the full lines as “half lines.” The first central difference,
δfi`1{2 “ fi`1 ´ fi, is computed using the values in the first column and written in the
second column, exactly halfway between the two lines of values involved. This differencing
procedure is repeated for subsequent columns by using the values of the previous column,
i.e., δnfi`1{2 “ δn´1fi`1 ´ δn´1fi. The final form of the table contains blank spaces on
both half and full lines. These spaces are filled out by simply taking the arithmetic average
of the numerical values above and below each blank space, as illustrated in Fig. 6.8d. The
new table, the modified central difference table, which provides additional entries, is used to
construct the central difference interpolating polynomials.
A number of interpolating formulas requiring central differences have been described
in the literature with slightly different properties, two of the most important of which are
presented here. The Stirling formula is used along with the full lines, while the Bessel
formula uses the central differences along the half lines.
Stirling’s Formula (full lines as the base line):
fpxq “fk`s δfk`
s2
2! δ2fk`
sps2´1q
3! δ3fk`
s2ps2´1q
4! δ4fk`
sps2´1qps2 ´ 4q
5! δ5fk
`
s2ps2 ´ 1qps2 ´ 4q
6! δ6fk ` ...
(6.31)
Bessel’s Formula (half lines as the base line):
fpxq “fk`spδfkq` 1
2!
`
s2´ 1
4
˘
δ2fk`
s
3!
`
s2´ 1
4
˘
δ3fk`
1
4!
`
s2´ 1
4
˘`s2´ 9
4
˘
δ4fk
`
s
5!
`
s2´ 1
4
˘`s2´ 9
4
˘
δ5fk ` ...
(6.32)
where s “ px ´ xkq{h and xk is the corresponding abscissa of the base line (either a full or
a half line).
Once forward, backward, or central difference tables are constructed, the GN-FWD or
GN-BKWD and Bessel or Stirling interpolation formulas can be used to find interpolates for
a discrete data set. The best estimates are obtained if the base line is chosen as the line in the
table that has a large number of entries. This strategy allows for the construction of a high￾degree interpolating polynomial. For a smooth function, successively higher differences along
the chosen base line become smaller in magnitude. This implies that increasing the degree of
the interpolating polynomial improves the accuracy of the interpolate. However, increasingInterpolation and Extrapolation  333
the degree of the interpolation polynomial is not advised if higher successive differences
depict an increase in magnitude or oscillate about the base line. A simple guideline for
selecting the optimal base line is to choose the line in the table closest to the interpolation
point so that the resulting value of s is the smallest, provided that the higher differences are
decreasing in magnitude. In order to guarantee a fairly large number of entries in the base
line, we generally prefer the GN-FWD-IF if the value to be interpolated is near the top of
the table and the -BKWD-IF if it is near the bottom of the table. For the interpolations
near the center of the data set, central difference interpolation formulas (Stirling or Bessel)
are usually used.
Pseudocode 6.4
Module GN_FWD_EVAL (n, xval, x,f, a, fval)
\ DESCRIPTION: A pseudomodule to interpolate uniformly spaced discrete data
\ using the Gregory-Newton Forward interpolation.
Declare: xn, fn, an,n`1 \ Declare array variables
BINARY_SEARCH(n, xval, x, k) \ Find first index k of rxk, ă xk`1s
h Ð x2 ´ x1 \ Find interval size
s Ð pxval ´ xkq{h \ Find interpolation variable
coef Ð 1 \ Initialize numerator
f actor Ð 1 \ Initialize denominator
fval Ð ak2 \ Initialize the interpolant
For “
i “ 3, n ` 1
‰ \ Loop i: Evaluate interpolant
coef Ð coef ˚ ps ` 3 ´ iq \ Find sps ` 1q...ps ` nq
f actor Ð f actor ˚ pi ´ 2q \ Find pi ´ 2q!
Rn Ð coef ˚ aki{f actor \ Find a next term
fval Ð fval ` Rn \ Apply Eq. (6.29)
End For
End Module GN_FWD_EVAL
Module GN_FWD_TABLE (n, x,f, a)
\ DESCRIPTION: A pseudomodule generate GN Forward interpolation table.
Declare: xn, fn, an,n`1 \ Declare array variables
For “
i “ 1, n‰ \ Loop i: Placa data into the table
ai1 Ð xi \ Place abscissas in 1st column
ai2 Ð fi \ Place ordinates in 2nd column
End For
For “
j “ 3, n ` 1
‰ \ Generate jth column
For “
i “ 1, n ´ j ` 2 ‰ \ Sweep column from top to bottom
aij Ð ai`1,j´1 ´ ai,j´1 \ Find forward differences for jth column
End For
End For
End Module GN_FWD_TABLE
Module BINARY_SEARCH (n, xval, x, k)
\ DESCRIPTION: A pseudomodule to find index k the interval xk ď xval ă xk`1
\ using the Binary Search algorithm.
Declare: xn \ Declare array variable
F irst Ð 1; Last Ð n \ Set 1st & last index of array
While “
Last ´ f irst ą 1
‰ \ while two elements are apart from each other334  Numerical Methods for Scientists and Engineers
M iddle Ð pLast ` F irstq{2 \ Find index of mid point
If “
xval ą xM iddle‰
Then \ xval is on the rhs (M iddle, Last)
F irst Ð M iddle \ Narrow down from lhs, F irst “ M iddle
Else
If “
xval ă xM iddle‰
Then \ xval is on the lhs (F irst, M iddle)
Last Ð M iddle \ Narrow down from rhs, Last “ M iddle
Else \ Case of xM iddle “ xval
F irst Ð M iddle; Last Ð M iddle
Exit
End If
End If
End While
k Ð F irst \ Set first index k of interval to F irst
End Module BINARY_SEARCH
A pseudomodule, GN_FWD_TABLE, generating the forward difference table of a uni￾formly spaced discrete data set is presented in Pseudocode 6.4. As input, the module requires
the number of data points (n) and the data set pxi, fiq for i “ 1, 2,...,n. First, the input
data are placed on the 1st and 2nd columns of the generating table, i.e., pxi, fiqÑpai1, ai2q.
Starting from the third column, the first forward difference and subsequent higher forward
differences are computed using the prior column entries with aij Ð ai`1,j´1 ´ ai,j´1 and
placed in the same column in the table. The module output is the generated forward dif￾ference table (a). It should be noted that this module should be called only once in the
main program, and then numerous interpolations can simply be performed by using the
evaluation module, GN_FWD_EVAL.
The pseudomodule, GN_FWD_EVAL, uses the forward difference table (a) in addition
to n and x. The module for an input value of xval computes the interpolant fval. The first
subscript of the interval (k) containing xval is determined (i.e., xk ď xval ă xk`1) using
the module BINARY_SEARCH, implementing the binary search algorithm. Then, using the
interpolation variable s Ð pxval ´ xkq{h, the forward interpolation formula, Eq. (6.29) is
used to find the interpolate. This module computes the interpolant with a polynomial of the
highest possible degree, i.e., all available differences along the nearest baseline are incorpo￾rated into the interpolation formula. Nevertheless, it is prudent to check for the smoothness
of the discrete data before indulging in the computation of an estimate with an interpola￾tion polynomial of the highest degree. The module can be easily modified by adapting the
strategy implemented in the NEWTONS_DD_EVAL module presented in Pseudocode 6.3.
The Pseudocode 6.4 can be used for Gregory-Newton backward interpolation as well as
central difference interpolation formulas with minor modifications.
We can determine whether a set of discrete data is smooth or
not by inspecting the entry values of a difference table. If the
discrete data is smooth, the successively higher differences along
the chosen base line will become smaller in magnitude. In such
cases, a high-degree interpolating polynomial will produce very
accurate interpolates.
ERROR ASSESSMENT. As we have seen, the GN-FWD or GN-BKWD interpolation formu￾las are essentially generated from Newton’s divided differences; thus, the error bound forInterpolation and Extrapolation  335
TABLE 6.5: Error propagation in difference tables.
δ4fk´2 ` ε
δ3f
k´ 3
2
` ε
fk´1 δ2fk´1 ` ε δ4fk´1 ´ 4ε
δfk´ 1
2
` ε δ3f
k´ 1
2
´ 3ε
fk ` ε δ2fk ´ 2ε δ4fk ` 6ε
δfk` 1
2
´ ε δ3f
k` 1
2
` 3ε
fk`1 δ2fk`1 ` ε δ4fk`1 ´ 4ε
δfk` 3
2
´ ε
δ4fk`2 ` ε
an nth degree interpolating polynomial, pnpxq, can be obtained by
|enpxq| “ |fpxq ´ pnpxq| “
ˇ
ˇ
ˇ
ˇ
sps ˘ 1qps ˘ 2q¨¨¨ps ˘ nq
hn`1fpn`1qpξq
pn ` 1q!
ˇ
ˇ
ˇ
ˇ
, x0 ď ξ ď xn
(6.33)
The order of magnitude of the approximation error, |enpxq|, is about the next difference
not used in |pnpxq|. The multiplying factor |sps ˘ 1qps ˘ 2q¨¨¨ps ˘ nq| is an nth-degree
polynomial with decreasing magnitude near the data end points (s “ 0 and s “ ˘n). Since
enpxq is proportional to hn`1, the magnitude of the error decreases for h ă 1 with increasing
n. On the other hand, if fpn`1qpξq derivative grows larger with n, the magnitude of the error
decreases only if h is sufficiently small. Neglecting high-order differences could also give rise
to truncation errors.
The accuracy of the data in the set itself is also crucial in terms of the overall accuracy
of the interpolation. Sometimes the data contains errors that the interpolation formulas
cannot be blamed for. In this context, the difference tables can serve as an important tool
for checking the smoothness of the discrete data set or detecting isolated errors. To illustrate
this, consider an error of “ε” in a certain tabular value; that is, we assume that all data in
the set are correct except for one that is in `ε error. As illustrated in Table 6.5, the isolated
error is magnified and spread out in a triangular pattern as higher differences are generated.
While the largest error occurs in the same row as the erroneous value, the algebraic sum of
the errors in any column is zero. A single-entry error yields the binomial coefficient pattern
for any column, and high-order differences greatly magnify the errors. For this reason, high￾degree interpolating polynomials are very sensitive to errors in the data. As we have seen,
a high-degree interpolating polynomial may yield estimates that significantly diverge from
Gregory-Newton Interpolation Formulas
‚ The method has the same advantages as Newton’s divided differences;
‚ They require fewer arithmetic operations and less cpu-time since they
are derived specifically for uniformly spaced data;
‚ For smooth distributions, the convergence (or desired accuracy) can
be monitored by |pnpsq ´ pn´1psq| ă ε.
‚ As a method, it is no different from Newton’s divided difference;
‚ They can only be applied to uniformly spaced discrete functions.336  Numerical Methods for Scientists and Engineers
the true value, just as the Lagrange interpolating polynomial does. It is therefore advised
to construct error propagation tables to carry out an exploratory data analysis. Also note
that even if an original data (error-free) set is used, errors can still be introduced during
the construction of the difference tables. For a large data set, the arithmetic operations may
inevitably yield notable round-off errors.
EXAMPLE 6.5: Interpolation with uniform data sets
The following data are read from a bacterial growth curve:
t (h) 0.5 1.0 1.5 2.0 2.5
fptq 0.75 2 3.375 4.8 6.25
where t is the time in hours, fptq “ log10nptq, and nptq is the bacterial population.
(a) Use the data to generate the difference tables and develop 2nd, 3rd, and 4th￾degree GN-FWD, GN-BKWD, and central interpolating polynomials; (b) use the
polynomials derived in Part (a) to compute f(0.55), f(0.7), f(1.76), and f(2.3). The
true solution (growth curve) is given as log10 nptq “ 6t
2{p1 ` 2tq.
SOLUTION:
(a) In this exercise, a set of discrete data generated from a known function is
used to illustrate the implementation and performance of the interpolating formulas.
In reality, the true solution is unknown; thus, the true interpolation errors cannot
be determined with absolute certainty. In general, the best we can do is estimate
the trend of the true function based on our expertise in the physical or numerical
problem.
Considering the given data set, at most a 4th-degree (n “ 5´1 “ 4) interpolating
polynomial can be generated from the set. As a first step, pertinent difference ta￾bles should be constructed. The generated forward, backward, and modified central
difference tables are presented, respectively, in Tables 6.6, 6.7, and 6.8.
TABLE 6.6
i ti fi Δfi Δ2fi Δ3fi Δ4fi
1 0.5 0.75 1.25 0.125 ´0.075 0.05
2 1 2 1.375 0.05 ´0.025
3 1.5 3.375 1.425 0.025
4 2 4.8 1.45
5 2.5 6.25
TABLE 6.7
i ti fi ∇fi ∇2fi ∇3fi ∇4fi
1 0.5 0.75
2 1 2 1.25
3 1.5 3.375 1.375 0.125
4 2 4.8 1.425 0.05 ´0.075
5 2.5 6.25 1.45 0.025 ´0.025 0.05Interpolation and Extrapolation  337
TABLE 6.8
i ti fi δfi δ2fi δ3fi δ4fi
1 0.5 0.75
3/2 0.75 1.3750 1.25
2 12 1.3125 0.125
5/2 1.25 2.6875 1.375 0.0875 ´0.075
3 1.5 3.375 1.4 0.05 ´0.050 0.05
7/2 1.75 4.0875 1.425 0.0375 ´0.025
4 2 4.8 1.4375 0.025
9/2 2.25 5.525 1.45
5 2.5 6.25
The numerical values in the tables are the same, but their positions in the columns
change. It should also be noted that the successively higher differences along any line
(from left to right) are getting smaller in magnitude, which implies that the discrete
data set is smooth. As a result, we can expect to make very accurate estimates with
high-degree interpolating polynomials.
To develop interpolation polynomials, we first need to decide which interpolation
formula to use. For t “ 0.55 and 0.7, we will prefer the GN-FWD-IF since there
are more entries in the baseline x “ 0.5 in Table 6.6; these entries are f1 “ 0.75,
Δf1 “ 1.25, Δ2f1 “ 0.125, Δ2f1 “ ´0.075, and Δ4f1 “ 0.05. For this case, Eq.
(6.29) becomes
fptq – p4ptq “ 0.75 ` sp1.25q ` sps ´ 1q
2! p0.125q ` sps ´ 1qps ´ 2q
3! p´0.075q
`
sps ´ 1qps ´ 2qps ´ 3q
4! p0.05q
where s “ pt ´ t1q{h “ 2t ´ 1.
A general interpolating polynomial in terms of t can be found by substituting
s “ 2t´1 into the GN-FWD interpolation formula. Simplifying leads to a 4th-degree
interpolating polynomial:
p4ptq “ 1
30
t
4 ´ 4
15
t
3 `
101
120
t
2 `
197
120
t ´ 1
4
, 0.5 ď t ď 2.5
Likewise, for t1 “ 0.5 as the baseline, linear, quadratic, and cubic interpolating
polynomials are also obtained as follows:
p1ptq “ f1 ` s Δf1 “ 1
2
p5t ´ 1q,
p2ptq “ p1ptq ` sps ´ 1q
2! Δ2f1 “ 1
8
p2t
2 ` 17t ´ 3q,
p3ptq “ p2ptq ` sps ´ 1qps ´ 2q
3! Δ3f1 “ 1
20p´2t
3 ` 11t
2 ` 37t ´ 6q
It is also possible to generate GN-FWD (or GN-BKWD or central) interpolating
polynomials using any one of the other rows (tk, k “ 2, 3, 4, or 5) as the baseline.
For instance, the entries for baseline t2 “ 1 (s “ pt ´ t2q{h “ 2t ´ 2) are read from338  Numerical Methods for Scientists and Engineers
Table 6.6 as f2 “ 2, Δf2 “ 1.375, Δ2f2 “ 0.05, and Δ3f2 “ ´0.025. Then, Eq.
(6.29) for n “ 1, 2, and 3 yields
p1ptq “ 1
4
p11t ´ 3q,
p2ptq “ 1
10 pt
2 ` 25t ´ 6q,
p3ptq “ 1
60 p´2t
3 ` 15t
2 ` 137t ´ 30q
Note that a 4th-degree interpolating polynomial cannot be constructed with t2 “ 1
as a baseline due to having fewer entries.
In Table 6.6, contrary to Table 6.8, there are fewer entries corresponding to
lines (rows) of t “ 1.7 and 2.5. Thus, it is suitable to use GN-BKWD-IF since
there are more entries in the t “ 2 and 2.5 rows. Using t5 “ 2.5 as the baseline
(s “ pt ´ t5q{h “ 2t ´ 5), the baseline entries are read from Table 6.8 as f5 “ 6.25,
∇f5 “ 1.45, ∇2f5 “ 0.0255, ∇3f5 “ ´0.025, and ∇4f5 “ 0.05. The GN-BKWD
interpolating polynomial that passes from all five data points, Eq. (6.30), leads to
fptq – p4ptq “ 1
30
t
4 ´ 4
15
t
3 `
101
120
t
3 `
197
120
t ´ 1
4
, 0.5 ď t ď 2.5
Notice that p4ptq is the same as the polynomial obtained from the GN-FWD-IF. The
other lower-degree backward interpolating polynomials can also be constructed as
follows:
p1ptq “ f5 ` s ∇f5 “ 1
10p29t ´ 10q,
p2ptq “ p1ptq ` sps ` 1q
2! ∇2f5 “ 1
40p2t
2 ` 107t ´ 30q,
p3ptq “ p2ptq ` sps ` 1qps ` 2q
3! ∇3f5 “ 1
60p´2t
3 ` 15t
2 ` 137t ´ 30q
For interpolations (t values) near the middle of the dataset, the number of row￾wise data in the near middle of Tables 6.6 and 6.7 is fewer than that of Table 6.8. Since
we prefer to include as many terms as possible for smooth distributions, the central
difference (Stirling or Bessel) interpolation formulas can be used to interpolate points
near the middle of a dataset. In this text, only Stirling and Bessel’s interpolation
formulas were introduced. Which one to use is best determined by how close the
interpolation point is to a full or a half line, the number of entries in the baseline,
whether a distribution is smoot or not, and so on.
The row with the most number of entries in Table 6.8 is observed to be for t3 “ 1.5
(a full line), whose entries from left to right are also decreasing in magnitude. The
interpolation formula best suited for this case is Stirling’s formula. Using t3 “ 1.5 as
the base line (s “ pt ´ t3q{h “ 2t ´ 3), the entries along the baseline are determined
to be f3 “ 3.375, δf3 “ 1.4, δ2f3 “ 0.05, δ3f3 “ ´0.05, and δ4f3 “ 0.05. However,
as expected, the interpolating polynomials (with degrees lower than four) presented
below are different from those obtained with the forward and backward interpolating
formulas.Interpolation and Extrapolation  339
p1ptq “ f3 ` s δf3 “ 1
40p112t ´ 33q,
p2ptq “ p1ptq ` s2
2! δ2f3 “ 1
10pt
2 ` 25t ´ 6q,
p3ptq “ p2ptq ` sps2 ´ 1q
3! δ3f3 “ 1
15p´t
3 ` 6t
2 ` 31t ´ 6q
Stirling’s formula that passes all data points yields the same 4th-degree polynomial
obtained by the GN-FWD and -BKWD-IFs.
(b) The 1st to 4th-degree interpolating polynomials generated by taking t1 “ 0.5
as the baseline and the estimates computed for t “ 0.55, 0.7, 1.76, and 2.30 are
comparatively presented along with the true values in Table 6.9. It is observed that
as the degree of the interpolating polynomial increases, the estimates approach the
true values. Furthermore, the estimates calculated by p4pxq or p3pxq yield fairly good
agreements due to the smoothness of the discrete distribution, i.e., |Δf0| ą |Δ2f0| ą
|Δ3f0|, and so on.
TABLE 6.9
t fptq p1ptq p2ptq p3ptq p4ptq
0.55 0.864286 0.875 0.869375 0.8672375 0.8642044
0.70 1.225000 1.250 1.2350 1.2302000 1.2281200
1.76 4.111858 3.900 4.1394 4.1145024 4.1125106
2.3 5.667857 5.250 5.8350 5.647800 5.666520
To determine how well the interpolating polynomials behave overall in the range
0.5 ď t ď 2.5, the true error distribution, enptq “ fptq ´ pnptq, is also presented
graphically in Fig. 6.9a. The maximum error in p4pxq is about 0.003, so e4pxq is
practically flat. Since it is constructed from the first three data points, the p3pxq
does not pass through the rightmost data point, t “ 2.5. Therefore, the p3pxq depicts
relatively larger deviations for 2 ă t ď 2.5. The p2ptq corresponds to the case where
the last two data points are removed from the generated polynomial, which explains
why the estimates for t “ 2 and 2.5 deviate from the true values as much as they
do.
FIGURE 6.9: The true error distributions for the GN-FWD-IF with (a) t1 “ 0.5
and (b) t2 “ 1 as the baseline.
The GN-FWD interpolation polynomial that can be constructed with the num￾ber of data points available in row t2 “ 1 (as the baseline) can be at most a third340  Numerical Methods for Scientists and Engineers
degree. Hence, the true error distributions of the 1st, 2nd, and 3rd degree inter￾polating polynomials are depicted in Fig. 6.9b. The agreement of the interpolating
polynomials with the true distribution is observed to be better about the baseline,
but this agreement deteriorates as t is moved further away from the baseline. On the
other hand, the overall agreement improves with an increasing degree of interpolating
polynomials.
The GN-BKWD interpolation polynomials are preferred when interpolations near
the end of the data sets are desired. The 2nd, 3rd, and 4th-degree interpolating poly￾nomials with t5 “ 2.5 as the baseline are different from those obtained by the GN￾FWD-IFs. The true error distributions for p2pxq, p3pxq, and p4pxq are comparatively
illustrated in Fig. 6.10a. In constructing p3pxq and p2pxq, the data points t1 “ 0.5,
and t1 “ 0.5 and t2 “ 1 have been removed, respectively. For this reason, the errors
increase toward the lower end of the data (t1 “ 0.5).
The agreement of the interpolating polynomials obtained by Stirling’s formula
with the true distribution is illustrated in Fig. 6.10b. The polynomials depict good
agreement for estimates in the neighborhood of the baseline (t3 “ 1.5) since the lower
degree polynomials also use the interpolation point near the center, which serves as
anchors.
FIGURE 6.10: Distribution of enptq with (a) the GN-BKWD-IF with t5 “ 2.5 as
the baseline, and (b) the Stirling’s formula with t3 “ 1.5 as the baseline.
The estimated interpolants computed by the 1st to 4th-degree interpolating poly￾nomials generated with t5 “ 2.5 as the baseline, along with the true values, are
comparatively presented for t “ 0.55, 0.7, 1.76, and 2.30 in Table 6.10. For p3ptq and
p4ptq, the estimates depict agreements up to two-decimal places for values close to
the baseline (t ą 2). Also note that the estimates with p4ptq are very good through￾out the data interval, while p2ptq and p3ptq depict substantial deviations from the
true values for x ă 1.5 and x ă 1, respectively.
TABLE 6.10
t fptq p1ptq p2ptq p3ptq p4ptq
0.55 0.864286 0.595 0.73638 0.825913 0.866204
0.70 1.225000 1.030 1.14700 1.209004 1.228120
1.76 4.111858 4.104 4.11288 4.111341 4.112511
2.3 5.667857 5.670 5.66700 5.668600 5.66652
Discussion: An interpolating polynomial generated by including all data points
is the same regardless of the central, forward, or backward interpolation formulaInterpolation and Extrapolation  341
FIGURE 6.11: Illustration of linear splines employed to a data set of five.
used. That being the case, one might ask, “Should we construct an interpolation
polynomial of the highest degree?” To answer this question, it is necessary to examine
difference tables and determine whether the discrete distribution is smooth or not.
For a smooth distribution, the magnitudes of the entries (higher differences) in any
row should be decreasing. If the magnitudes of the higher differences oscillate or
increase, use low-degree interpolating polynomials.
Generating an interpolation polynomial for a large dataset (even if it is smooth)
can lead to Runge’s phenomenon. Besides, using fewer data points in GN-FWD,
GN-BKWD, or central interpolation formulas results in lower-order interpolation
polynomials that are different (perhaps better) from each other depending on the
baseline selected. In any case, a lower-degree interpolating polynomial should only
be used in the vicinity of the preferred baseline. The best strategy for interpolation
of large data sets is to divide the data set into smaller sets and generate a piecewise
distribution by constructing low-order interpolation polynomials (at most 4th or 5th
degrees) for each set.
6.4 CUBIC SPLINE INTERPOLATION
A polynomial interpolation interpolates between n ` 1 points. As we have learned, a major
problem with polynomial interpolation is that it often leads to unacceptably inaccurate
estimates. The problem gets even worse when dealing with a large number of data points,
which is, in practice, commonly encountered as Runge’s phenomenon. Hence, interpolation
with a single polynomial covering the whole data range is generally not advised. Alter￾natively, low-order piecewise polynomials can be employed throughout the data range to
avoid Runge’s phenomenon. Since only low-degree polynomials are used, the problem of
over-fitting or excessive oscillations is eliminated.
A function composed of low-order piecewise polynomials is frequently referred to as a
spline. A data point where two splines are joined is called a knot. Conventionally, a data
interval is divided into more than one interval, where a linear, quadratic, or cubic polynomial
is employed for each interval. In this respect, spline methods present a different approach
than the polynomial interpolations covered so far.
The simplest spline method is a linear spline, where two data points are joined by a
line. As shown in Fig. 6.11, a linear spline with a data set of five (n-points) consists of four
lines (y “ mkx ` bk, k “ 1,...,n ´ 1). While y “ fpxq is continuous at the knots, its first342  Numerical Methods for Scientists and Engineers
FIGURE 6.12: Cubic splines employed to a discrete data set.
derivative, y1
pxq, is undefined. In other words, the linear splines lack smoothness, i.e., the
left and right derivatives (slopes of the lines on each side of the knot) are not equal. In many
cases, however, physical considerations require interpolating functions to be continuous and
differentiable.
An alternative approach is to use quadratic (quadratic spline) or cubic polynomials
(cubic spline) to achieve smoothness and greater accuracy. While only a couple of knots are
sufficient to define a unique line, an infinite number of quadratic or cubic polynomials pass
between any two knots. So to be able to define a unique high-degree spline, the continuity
of the second derivative, y2pxq, in addition to ypxq and y1
pxq, must be satisfied at every
knot.
The use of cubic polynomials is the most common choice in the literature. The reason for
this is that cubic splines can be joined in different ways to produce an overall interpolating
curve. At this stage, we shall consider non-uniformly spaced data and will employ a cubic
polynomial ykpxq, which has a different set of coefficients for each interval, xk ď x ď xk`1.
Each cubic polynomial is then joined to its neighboring cubic polynomials (yk´1pxq, ykpxq,
and yk`1pxq) at the knots by matching the slopes and curvatures, i.e., y1
pxq and y2pxq. The
treatments for the knots at both ends of the data interval will be covered in the subsequent
discussions. Once the coefficients of the cubic polynomials are obtained, these piecewise
polynomials are valid in the interval that they are specified.
Consider a nonuniform discrete data set consisting of px1, f1q, px2, f2q, ..., pxn, fnq
data points (knots) as illustrated in Fig. 6.12. A cubic polynomial that will be imposed on
(n ´ 1) intervals is expressed for the kth interval, rxk, xk`1s, as
ykpxq “ akpx ´ xkq
3 ` bkpx ´ xkq
2 ` ckpx ´ xkq ` dk rxk, xk`1s (6.34)
In other words, the cubic polynomials valid in each interval are written as
y1pxq “ a1px ´ x1q
3 ` b1px ´ x1q
2 ` c1px ´ x1q ` d1, rx1, x2s
y2pxq “ a2px ´ x2q
3 ` b2px ´ x2q
2 ` c2px ´ x2q ` d2, rx2, x3s
.
.
. .
.
.
yn´1pxq“an´1px ´ xn´1q
3`bn´1px ´ xn´1q
2`cn´1px ´ xn´1q`dn´1, rxn´1, xns
This system consists of (n ´ 1) cubic polynomials (as the number of intervals) and
4ˆ pn´1q unknowns (namely the coefficients of the cubic polynomials a, b, c, and d’s). The
following conditions will then be imposed to determine the unknowns:Interpolation and Extrapolation  343
(1) Continuity of ypxq: The cubic splines are continuous at interior knots; that is,
interpolating polynomials must satisfy the data points:
ykpxkq “ fk, k “ 1, 2,...,pn´1q, and yn´1pxnq “ fn; (6.35)
ykpxk`1q “ yk`1pxk`1q “ fk`1, k“1, 2,...,pn ´ 2q (6.36)
From the continuity condition, we find (n ´ 1) equations (or dk’s) as
ykpxkq “ dk “ fk, k“1, 2,...,pn´1q (6.37)
Employing Eq. (6.37), the continuity of splines leads to
yk`1pxk`1q “ ykpxk`1q “ ak Δx3
k ` bk Δx2
k ` ck Δxk ` fk “ fk`1 (6.38)
where Δxk “ xk`1 ´ xk.
(2) Continuity of y1
pxq: The first derivatives of cubic splines are continuous at interior
knots; that is,
y1
kpxk`1q “ y1
k`1pxk`1q, k“1,...,pn´2q (6.39)
where
y1
kpxq “ 3akpx ´ xkq
2 ` 2bkpx ´ xkq ` ck (6.40)
By investigating the slopes at the knots from the right- and left-hand sides, we get
y1
k`1pxk`1q “ 3ak`1pxk`1 ´ xk`1q
2 ` 2bk`1pxk`1 ´ xk`1q ` ck`1 “ ck`1 (6.41)
y1
kpxk`1q “ 3ak pΔxkq
2 ` 2bkΔxk ` ck (6.42)
The equality of Eq. (6.40) and (6.42) leads to
3ak´1 pΔxk´1q
2 ` 2bk´1Δxk´1 ` ck´1 “ ck (6.43)
Equation (6.43) is made up of 3ˆ pn´1q equations, i.e.,ak, bk, and ck for k “ 1, 2,...,n´2.
(3) Continuity of y2pxq: The second derivatives (curvatures) of cubic splines are
continuous at interior knots, that is,
y2
kpxk`1q “ y2
k`1pxk`1q, k“1, 2,...,pn´2q (6.44)
The second derivative of Eq. (6.34) yields
y2
kpxq “ 6akpx ´ xkq ` 2bk, (6.45)
Since y2pxq of a cubic polynomial gives a linear relationship, its distribution on rxk, xk`1s
must also be linear. Defining Sk “ y2pxkq for k “ 1, 2,...,pn ´ 1q and using this notation
in subsequent derivations, we may write
y2pxk`1q “ Sk`1 “ 6akpxk`1 ´ xkq ` 2bk (6.46)
y2pxkq “ Sk “ 6akpxk ´ xkq ` 2bk (6.47)
Using Eqs. (6.46) and (6.47) to solve for ak and bk yields
bk “ Sk
2 , ak “ Sk`1 ´ Sk
6 Δxk
(6.48)344  Numerical Methods for Scientists and Engineers
Substituting ak and bk into Eq. (6.38) and solving for ck results in
ck “ fk`1 ´ fk
Δxk
´ Δxk
´Sk`1 ` 2Sk
6
¯
(6.49)
Notice that we have introduced additional unknowns, namely the curvatures (i.e., Sk’s).
Next, to determine the curvatures at the knots, we substitute the coefficients of the poly￾nomial, Eqs. (6.48) and (6.49), into Eq. (6.43)
y1
k “ck “ fk`1´fk
Δxk
´Δxk
´Sk`1 ` 2Sk
6
¯
“ Sk´Sk´1
2
Δxk´1`Sk´1Δxk´1`
"fk´fk´1
Δxk´1
´Δxk´1
´Sk ` 2Sk´1
6
¯* (6.50)
Rearranging Eq. (6.50) gives
Δxk´1Sk´1 ` 2pΔxk ` Δxk´1qSk ` ΔxkSk`1 “ 6
´ Δfk
Δxk
´ Δfk´1
Δxk´1
¯
(6.51)
The above stipulations provide a set of (3n´5) equations: Eq. (6.34) gives (n´1) equations,
Eq. (6.35) and (6.36) yield 2 ˆ pn ´ 2q equations. However, the number of equations is still
less than the number of unknowns. It is clear that two additional equations are required to
complete a system of linear equations.
(4) End Conditions: Equation (6.51), which is valid for k“2, 3,...,pn´1q, provides
n´2 equations. Additionally, we need two equations to establish n equations with Sk as
unknowns. These equations are supplied by estimating the curvature at the end points of the
data range. Several spline formulations can be developed through the curvature estimations
referred to as “end conditions.” In this section, we consider only two cases: natural splines
and linear extrapolation end conditions.
6.4.1 NATURAL SPLINE END CONDITION
The natural spline condition imposes the inflection point conditions at the end points. In
other words, the discrete distribution is assumed to have “inflection points” at x “ x1 and
x “ xn, where y2px1q “ 0 and y2pxnq “ 0. Recalling the definition Spxq “ y2pxq in the
preceding section, we may write
S1 “ 0 and Sn “ 0 (6.52)
Now that we have two more equations, we can construct the system of linear equations by
using Eqs. (6.51) and (6.52) for k“2, 3,...,pn´1q to give
Δx1p0q ` 2pΔx2 ` Δx1qS2 ` Δx2S3 “ 6
´ Δf2
Δx2
´ Δf1
Δx1
¯
Δx2S2 ` 2pΔx3 ` Δx2qS3 ` Δx3S4 “ 6
´ Δf3
Δx3
´ Δf2
Δx2
¯
.
.
. .
.
.
Δxn´3Sn´3 ` 2pΔxn´2 ` Δxn´3qSn´2 ` Δxn´2Sn´1 “ 6
´ Δfn´2
Δxn´2
´ Δfn´3
Δxn´3
¯
Δxn´2Sn´2 ` 2pΔxn´1 ` Δxn´2qSn´1 ` Δxn´1p0q “ 6
´ Δfn´1
Δxn´1
´ Δfn´2
Δxn´2
¯
(6.53)Interpolation and Extrapolation  345
Solving the system of linear equations gives Sk’s. When Eqs. (6.52) and (6.53) are combined,
we obtain a tridiagonal system of linear equations given below:
»
—
—
—
—
—
—
—
–
¯
d2 a¯2
¯b3 ¯
d3 a¯3
¯b4 ¯
d4 a¯4
... ... ...
¯bn´2 ¯
dn´2 a¯n´2
¯bn´1 ¯
dn´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
–
S2
S3
S4
.
.
.
Sn´2
Sn´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
–
c2
c3
c4
.
.
.
cn´2
cn´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(6.54)
where ¯ak “ Δxk, ¯bk “ Δxk´1, ¯
dk “ 2pa¯k `¯bkq and ck “ 6pΔfk{Δxk ´Δfk´1{Δxk´1q which
can also be expressed in terms of divided differences as ck “ 6 pfrxk, xk`1s ´ frxk´1, xksq.
6.4.2 LINEAR EXTRAPOLATION END CONDITION
This is one of the most frequently used end-conditions. Instead of setting S1 “ 0 and
Sn “ 0 (natural splines), taking into account the fact that the second derivatives of cubic
splines are linear, the end point values are estimated by linear extrapolation. Hence, linear
extrapolation for the knot-2 yields
S2 ´ S1
Δx1
“ S3 ´ S2
Δx2
and S1 “
´
1 `
Δx1
Δx2
¯
S2 ´ Δx1
Δx2
S3 (6.55)
The linear extrapolation for knot number (n ´ 1) gives
Sn ´ Sn´1
Δxn´1
“ Sn´1 ´ Sn´2
Δxn´2
and Sn “
´
1 `
Δxn´1
Δxn´2
¯
Sn´1 ´ Δxn´1
Δxn´2
Sn´2 (6.56)
Substituting k “ 2 in Eq. (6.51), we find
Δx1S1 ` 2pΔx2 ` Δx1qS2 ` Δx2S3 “ 6
ˆ Δf2
Δx2
´ Δf1
Δx1
˙
(6.57)
The extrapolation value of S1, Eq. (6.55), is substituted into Eq. (6.57) to get
d2S2 ` a2S3 “ 6
´ Δf2
Δx2
´ Δf1
Δx1
¯
(6.58)
Similarly, substituting Sn from Eq. (6.56) into Eq. (6.51) for k “ n ´ 1, followed by simpli￾fications, we arrive at
bn´1Sn´2 ` dn´1Sn´1 “ 6
´ Δfn´1
Δxn´1
´ Δfn´2
Δxn´2
¯
(6.59)
The coefficients for k “ 3, 4, .., n´2 equations and the system of linear equations are the
same except for the first and last rows, which are modified with the following quantities:
a2 “ pΔx2q2 ´ pΔx1q2
Δx2
, d2 “ pΔx1 ` Δx2qpΔx1 ` 2Δx2q
Δx2
,
bn´1 “ pΔxn´2q2 ´ pΔxn´1q2
Δxn´2
, dn´1 “ pΔxn´1 ` Δxn´2qpΔxn´1 ` 2Δxn´2q
Δxn´2
(6.60)346  Numerical Methods for Scientists and Engineers
6.4.3 COMPUTING SPLINE COEFFICIENTS
Upon employing an end-condition, S2, S3, ..., Sn´1 are found from the solution of the system
of linear equations. Before implementing the cubic spline interpolation, the coefficients (a,
b, c, and d’s) are computed for each rxk, xk`1s interval (for k “ 1, 2,...,pn´1q), as follows:
ak “ Sk`1 ´ Sk
6 Δxk
, bk “ Sk
2 , ck “ fk`1 ´ fk
Δxk
´ Δxk
´Sk`1 ` 2Sk
6
¯
, dk “ fk (6.61)
The arithmetic operations are significantly reduced when the data set is uniform. For ex￾ample, the tridiagonal system of linear equations for cubic spline interpolation with linear
extrapolation end conditions takes the form:
»
—
—
—
—
—
—
—
–
60 0 0 ¨¨¨ 0
14 1 0 ¨¨¨ 0
01 4 1 0
.
.
. ... ... ... .
.
.
0 ¨ 0 1 41
0 ¨¨¨ 0 0 06
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
–
S2
S3
S4
.
.
.
Sn´2
Sn´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“ 6
»
—
—
—
—
—
—
—
–
f 2
2
f 2
3
f 2
4
.
.
.
f 2
n´2
f 2
n´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(6.62)
where f 2
k “ δ2fk{h2 “ pfk`1 ´ 2fk ` fk´1q{h2, and the linear extrapolation end conditions
become S1 “ 2S2 ´ S3 and Sn “ 2Sn´1 ´ Sn´2.
EXAMPLE 6.6: Application of cubic splines with two end conditions
The plastic viscosity (μ) versus yield stress (σ) relationship for a concrete recipe with
a special ingredient is investigated. The experimental results are presented below:
μ (Pa¨s) 5 9 11 15 19 24
σ (Pa) 126 196 294 590 980 1550
Find interpolates for plastic viscosities of μ “ 7, 10, 12, 18, and 23 Pa¨ s using the
cubic splines with (a) natural spline and (b) linear extrapolation end conditions.
SOLUTION:
(a) For the natural spline end condition (setting S1 “ S6 “ 0), Eq. (6.62) takes
the form
»
—
—
–
12 2
2 12 4
4 16 4
4 18
fi
ffi
ffi
fl
»
—
—
–
S2
S3
S4
S5
fi
ffi
ffi
fl “
»
—
—
–
189
150
141
99
fi
ffi
ffi
fl
The solution of the tridiagonal system of equations yields S2 “14.3831, S3 “8.20148,
S4 “ 5.70402, and S5 “ 4.23244. The cubic splines (i.e., coefficients) corresponding
each interval are calculated using Eq. (6.61) as follows:
σ1pμq“0.599295pμ´5q
3 ` 0 ˆ pμ´5q
2 ` 7.911275pμ´5q ` 126, r5, 9s
σ2pμq“´0.515134pμ´9q
3 ` 7.191543pμ´ 9q
2 ` 36.677449pμ´9q ` 196, r9, 11s
σ3pμq“´0.104061pμ´11q
3 ` 4.100739pμ´11q
2 ` 59.262015pμ´11q `294 r11, 15s
σ4pμq“´0.061316pμ´15q
3 ` 2.852010pμ´15q
2 ` 87.073013pμ´15q `590, r15, 19s
σ5pμq“´0.141081pμ´19q
3 ` 2.116220pμ´19q
2 ` 106.94593pμ´19q `980, r19, 24sInterpolation and Extrapolation  347
(b) For the linear extrapolation end-condition requirement Eq. (6.60), the system of
linear equations yields
»
—
—
–
24 ´6
2 12 4
4 16 4
´2.25 29.25
fi
ffi
ffi
fl
»
—
—
–
S2
S3
S4
S5
fi
ffi
ffi
fl “
»
—
—
–
189
150
141
99
fi
ffi
ffi
fl
The solution of the system of equations gives S2 “ 10.11016, S3 “ 8.940622, S4 “
5.623055, and S5 “ 3.817158.
Next, linear extrapolations are carried out as follows: S1 “ 2S2 ´S3 “ 12.449222
and S6 “ 2S5 ´ S4 “ 1.559787. Having calculated the coefficients using Eq. (6.61),
the cubic splines corresponding to each interval become
σ1pμq“´0.097461pμ´5q
3 ` 6.224611pμ´5q
2 ´ 5.839066pμ´5q ` 126, r5, 9s
σ2pμq“´0.097461pμ´9q
3 ` 5.055078pμ´9q
2 ` 39.27969pμ´9q ` 196, r9, 11s
σ3pμq“´0.138232pμ´11q
3 ` 4.470311pμ´11q
2 ` 58.33047pμ´11q ` 294, r11, 15s
σ4pμq“´0.075246pμ´15q
3 ` 2.811527pμ´15q
2 ` 87.45782pμ´15q ` 590, r15, 19s
σ5pμq“´0.075246pμ´19q
3 ` 1.908579pμ´19q
2 ` 106.33825pμ´19q ` 980, r19, 24s
The cubic spline coefficients corresponding to the corresponding μ value are used as
interpolating polynomials. For example, μ “ 7 falls into the first interval. Thus, we
calculate the interpolation from σp7q “ σ1p7q and similarly the other interpolates as
σp10q “ σ2p10q, σp12q “ σ3p12q, σp18q “ σ4p18q, and σp23q “ σ5p23q.
TABLE 6.11
μ (Pa¨s ) Natural Spline Linear Extrapolation Difference
7 146.617 138.441 8.176
10 239.354 240.237 ´0.883
12 357.259 356.662 0.597
18 875.232 875.646 ´0.414
23 1432.614 1431.075 1.539
Interpolates computed in both cases, along with the difference between the two
estimates, are comparatively presented in Table 6.11. For the μ values away from
the end points, the cubic spline estimates with both end conditions are very good,
yielding absolute errors less than 1%. It turns out that the difference between the
estimates obtained with both end conditions for μ values near the lower end is the
largest. When the data is visually inspected, the distribution has a large curvature
at the left end (μ2p5q ą 0), and the inflection point condition imposed (μ2p5q “ 0)
is naturally not suitable for the data.
Discussion: The overall performance of the spline interpolations with the linear
extrapolation end condition is better than that obtained with the natural spline
end conditions. This is because calculating the curvature at each end by linear ex￾trapolation from the curvature of the neighboring interval leads to better curvature
estimates for uniform distributions.348  Numerical Methods for Scientists and Engineers
Pseudocode 6.5
Module SPLINE (opt, n, x,f, a, b, c)
\ DESCRIPTION: A pseudomodule to compute the cubic spline coefficients (an,
\ bn, cn) for “natural spline” or “linear extrapolation” end conditions. The
\ cubic splines have the following form
\ ykpxq “ axpx ´ xkq3 ` bkpx ´ xkq2 ` ckpx ´ xkq ` fk, xk ď x ď xk`1
\ USES:
\ TRIDIAGONAL:: Module to solve tridiagonal system (Pseudocode 2.13)
Declare: xn, fn, an, bn, cn, dxn, dfn, a¯n, ¯bn, c¯n, ¯
dn, Sn \ Declare array variables
For “
k “ 1, n ´ 1
‰ \ Loop k: Calculate the differences
dxk Ð xk`1 ´ xk \ Finde Δxk’s
dfk Ð pfk`1 ´ fkq{dxk \ Find frxk, xk`1s’s
End For
For “
k “ 2, n ´ 1
‰ \ Loop k: Construct tridiagonal matrix
¯bk Ð dxk´1 \ Find lower diagonals
a¯k Ð dxk \ Find upper diagonals ¯
dk Ð 2 ˚ pa¯k ` ¯bkq \ Find diagonals
c¯k Ð 6 ˚ pdfk ´ dfk´1q \ Construct rhs
End For
If “
opt “ 0
‰
Then \ opt “ 0: Case of natural spline end conditions
¯
d1 Ð 1; ¯a1 Ð 0; ¯c1 Ð 0 \ Set coeffients of 1st row ¯
dn Ð 1; ¯bn Ð 0; ¯cn Ð 0 \ Set coeffients of last row
s1 Ð 1; sn Ð n \ Indices of the 1st & last rows
Else \ opt ‰ 1: Case of linear extrapolation end conditions
cf Ð 1 ` dx1{dx2
a¯2 Ð pdx2 ´ dx1q ˚ cf \ Modify coeffients of 1st row ¯
d2 Ð p2 ˚ dx2 ` dx1q ˚ cf
cf Ð 1 ` dxn´1{dxn´2
¯bn´1 Ð pdxn´2 ´ dxn´1q ˚ cf \ Modify coeffients of lastst row ¯
dn´1 Ð p2 ˚ dxn´2 ` dxn´1q ˚ cf
s1 Ð 2; sn Ð n ´ 1 \ Indices of the 1st & last rows
End If
TRIDIAGONAL(s1, s2, b¯, d¯, ¯a, ¯c, S) \ Use Pseudocode 2.13 to solve system
If “
opt ‰ 0
‰
Then \ Modify 1st and last rows by Eq. (6.60)
S1 Ð S2 ˚ p1 ` dx1{dx2q ´ S3 ˚ dx1{dx2
Sn Ð Sn´1 ˚ p1 ` dxn´1{dxn´2q ´ Sn´2 ˚ dxn´1{dxn´2
End If
For “
k “ 1, n ´ 1
‰ \ Compute Spline coefficients from Eq. (6.61)
ak Ð pSk`1 ´ Skq{p6 ˚ dxkq
bk Ð Sk{2
ck Ð dfk ´ dxk ˚ pSk`1 ` 2 ˚ Skq{6
End For
End Module SPLINE
Module SPLINE_EVAL (n, xval, x,f, a, b, c,fval)
\ DESCRIPTION: A pseudomodule to evaluate fpxvalq, f1
pxvalq, f 2pxvalq
\ USES:
\ BINARY_SEARCH:: Module performing binary search, Pseudocode 6.4.
Declare: xn, fn, an, bn, cn, fval0:2 \ Declare array variablesInterpolation and Extrapolation  349
Cubic Spline Interpolation
‚ The method provides stable and smooth interpolating polynomials;
‚ Once the cubic spline coefficients are determined, not only ypxq but
also y1
pxq and y2pxq derivatives can be easily estimated at low cost;
‚ Cubic spline algorithm with linear extrapolation end-condition gen￾erally gives better results;
‚ Nonphysical oscillations associated with Runge’s phenomenon are not
observed.
‚ Since the curvature of the distribution at the endpoints is critical,
the method may not produce a proper interpolation curve if the end
conditions do not suit the true curvature;
‚ Splines are not suitable for extrapolation operations;
‚ The method is a bit more computationally involved in comparison to
linear splines.
If “
xval ă x1 Or xval ą xn
‰
Then \ Check if xval is within data range
Write: “xval outside data range!”
Exit \ Issue a warning and exit the module
End If
BINARY_SEARCH(n, xval, x, k) \ Find first index k of xk ď xval ă xk`1interval
dx Ð xval ´ xk
fval0 Ð ppak ˚ dx ` bkq ˚ dx ` ckq ˚ dx ` fk \ interpolant at xval
fval1 Ð p3 ˚ ak ˚ dx ` 2 ˚ bkq ˚ dx ` ck \ first derivative at xval
fval2 Ð 6 ˚ ak ˚ dx ` 2 ˚ bk \ second derivative at xval
End Module SPLINE_EVAL
A pseudomodule, SPLINE, computing the cubic spline coefficients for a set of discrete
data is presented in Pseudocode 6.5. The module requires the number of data points (n), the
data set pxi, fiq, and an option key for the cubic-spline end-condition type (opt) as input.
The module computes and returns the cubic spline coefficients ai, bi, and ci for i “ 1 to
n as output. Arrays dxi and dfi are used to compute the interval spacing and first divided
differences, and Si denotes the second derivative. Since the procedure requires the solution of
a tridiagonal system given by Eq. (6.54), the module TRIDIAGONAL (see Pseudocode 2.13)
is utilized here. The first and last row of the tridiagonal system are specified according to
the natural spline (Eq. (6.52) or linear extrapolation end conditions, Eq. (6.60), respectively.
The intermediate arrays ¯ai, ¯bi, ¯ci, and ¯
di are used for constructing the tridiagonal system.
This module should be called once in the related module, so long as f is not altered.
The accompanying module, SPLINE_EVAL, is for evaluating the interpolant as well as
its first and second derivatives. The module requires the number of discrete data points (n),
an interpolation point (xval), a set of discrete data pxi, fiq, and the cubic spline coefficients
(ai, bi, and ci generated by SPLINE) as input. An input validity check for xval is carried
out at the top of the module. Then the first subscript k of the interval containing xval is
determined via BINARY_SEARCH (see Pseudomodule 6.4). The module can be called as
many times as necessary to calculate fpxvalq, or f1
pxvalq and f 2pxvalq using Eqs. (6.34),
(6.40), and (6.45), which are stored on variables fval0, fval1, and fval2, respectively.350  Numerical Methods for Scientists and Engineers
FIGURE 6.13: Graphical depiction of rootfinding by linear interpolation.
ERROR ASSESSMENT. A cubic-spline interpolant constructed by implementing any end￾condition is unique. The uniqueness of the interpolants allows us to estimate the approxima￾tion errors (error bounds) associated with cubic splines by the polynomial approximations,
i.e., the absolute error bounds for the cubic splines are given by
max xPrxk,xk`1s
|ypxq ´ ykpxq| ď h4
4! max xPrxk,xk`1s
ˇ
ˇ
ˇyp4q
pxq
ˇ
ˇ
ˇ
where h “ max|xk`1 ´ xk|. This expression indicates that if h is sufficiently small, the order
of the interpolation error will be Oph4q. The order of error for the cubic splines method with
linear extrapolation is Oph4q as h Ñ 0, whereas the maximum error for the natural spline
end condition is Oph2q.
6.5 ROOTFINDING BY INVERSE INTERPOLATION
The most frequently encountered problem in science and engineering is to find the value of x
for a given y in a set of data´pxi, yiq. This reverse process is known as inverse interpolation.
In other words, inverse interpolation is the process of finding the value of an argument that
corresponds to a given value of a function within a tabulated data set.
The simplest and easiest inverse interpolation method is linear interpolation, which
requires a single interval and two data points. In this case, the interval of the root, rx1, x2s,
is first bracketed, just as in the bisection method. As in Fig. 6.13, the endpoint values of
the interval must have opposite signs (y1 ą 0, y2 ă 0 or y2 ą 0, y1 ă 0). The equation of a
straight line connecting the end points is y “ y1 ` py2 ´y1qpx´x1q{px2 ´x1q. This line cuts
the x´axis at x “ c2, which is obtained by simply setting y “ 0 in the equation of straight
line and solving for x:
c2 « x “ x2y1 ´ x1y2
y1 ´ y2
This approximation may be used as an estimate for the root (c2 « c1). If the curvature of
the curve near the root is relatively small (i.e., the curve is near linear), then the estimate
for the root will be close to the true value (c1 « c2). However, estimating a root with a linear
interpolation could be off by a large margin for a discrete function with a large curvature, as
seen in Fig. 6.13. In such cases, a higher-degree interpolating polynomial may be needed to
obtain a better estimate. Unlike linear interpolation, a high-degree interpolating polynomial
requires more than one interval and more data points to accurately mimic the curvature
near the root.
Now consider y “ fpxq on [x0, xn], which satisfies the inverse-function theorem; that
is, y “ fpxq is differentiable and f1
pxq ‰ 0. Inverse interpolation, x “ f ´1pyq, requires
interchanging the data so that the discrete function is cast as pyi, xiq for i “ 0, 1, 2,...,n.Interpolation and Extrapolation  351
Inverse Polynomial Interpolation
‚ The method is simple and straightforward;
‚ It requires neither derivatives nor an iterative procedure;
‚ A quadratic or cubic polynomial is sufficient in most cases;
‚ It can also be used to find the roots of an explicit function in cases
where the root-finding methods exhibit convergence difficulties or re￾quire a large number of iterations for convergence.
‚ The function, y “ fpxq, must be an invertible function that allows
inverse interpolation;
‚ The quality of the estimate depends on how well the inverse function
is approximated by a polynomial.
Finding the value of x “ f ´1p0q is then equivalent to determining the root of y “ fpxq.
Even if the original data is uniformly spaced, the set obtained by interchanging x and
y’s will not be uniformly spaced. A second-, third-, or higher-order polynomial can be
used for interpolation as long as a sufficient number of data points are available. Even if
there is sufficient data, it is crucial to determine which points (among many) to use in the
interpolation to estimate the root. Selecting four or five points from several intervals in the
neighborhood of the root would be adequate in most cases. In this regard, the Lagrange
interpolating polynomials can best serve this purpose. An approximating polynomial is
obtained as xpyq – Σn
k“0xkLkpyq, where n is the degree of polynomial. The root is then
estimated by setting y “ 0 in the interpolating function: xroot « xp0q.
The inverse interpolation can also be used to estimate the value of x that corresponds
to a particular value of ypxq “ yx. Instead of using all available data, the distribution of the
data should be examined visually, and lower-order polynomials using localized data should
be constructed for inverse interpolation.
ERROR ASSESSMENT. Inverse interpolation error depends basically on the numerical in￾terpolation method used. The error of inverse Lagrangian interpolation is
enpxq“px ´ x0qpx ´ x1q¨¨¨px ´ xnq
f ´1pn`1q
pξq
pn ` 1q! , x0 ď ξ ď xn
The derivatives of f ´1pxq can be calculated in terms of the derivatives of fpxq; for
instance,
d
dx
`
f ´1pxq
˘
“ 1
f1
pxq
, d2
dx2
`
f ´1pxq
˘
“ ´ f 2pxq
pf1
pxqq3 , ...
which reveals the limitations of the inverse interpolation. That is, the error is clearly un￾bounded for f1
pxq Ñ 0. Nonetheless, we can still estimate an interpolant through inverse
interpolation, although the inverse function does not exist in [x0, xn]. But in this case, we
should expect the accuracy of the estimate to be very poor.
When dealing with experimental data, knowing the accuracy of
the data is important. If the accuracy of the data is unknown,
then the true solution may be off by a large margin even if the
root is estimated with high-degree interpolating polynomials.352  Numerical Methods for Scientists and Engineers
EXAMPLE 6.7: Uses of inverse interpolation in practice
The density of plastic-based composite materials increases in proportion to the
amount of fiber additive used. A correlation for the flexural modulus E (in GPa) of
a composite material as a function of density ρ (in g/cm3) is given as
Epρq “ 3967 ρ4 ´ 15200 ρ3 ` 16420 ρ2 ´ 5584, 1.3 ď ρ ď 1.8
Estimate what the density of the material should be so that the modulus of elasticity
is at least 250 GPa.
SOLUTION:
The problem is finding the root of Epρq ´ 250 “ 0, which can be obtained by
employing a root-finding method (see Chapter 4). First, a set of four data points is
generated, and the abscissas and ordinates are interchanged, as shown in Table 6.12.
The problem is then reduced to finding ρp250q.
TABLE 6.12
i 0123
Ei 143.938 190.131 324.981 614.379
ρi 1.5 1.6 1.7 1.8
A Lagrange interpolating polynomial using all points is a cubic polynomial:
p3pEq“p143.938qL0pEq`p190.131qL1pEq`p324.981qL2pEq`p614.379qL3pEq
Expanding and simplifying yields the approximation, which we find
ρpEq “ 1.47266 ˆ 10´8E3 ´ 1.75671 ˆ 10´5E2 ` 0.00679294 E ` 0.842278
which for E “ 250 GPa leads to ρ “ 1.67268 g/cm3.
Discussion: We can also construct a quadratic interpolation polynomial, for which
we need three data points. If we choose the data points corresponding to i “ 0, 1,
and 2, the interpolating polynomial becomes
ρpEq“´7.86148 ˆ 10´6E2 ` 0.0047911 E ` 0.973253
which yields 1.67969 g/cm3 for E “ 250 GPa. This estimate also agrees with the cu￾bic approximation. However, the linear interpolation between two bracketing points,
(190.131,1.6) and (324.981,1.7), yields 1.6444 g/cm3, which is considerably different
from the estimates found by quadratic and cubic polynomials. When the data is
plotted in the range 140 ď E ď 325, the density depicts a strong curvature in this
interval, which explains the large deviation from the other two estimates.
6.6 MULTIVARIATE INTERPOLATION
All interpolation methods discussed so far involved single-variable (univariate) polynomials,
meaning that the dependent variable is a function of a single independent variable: y “ fpxq.
However, in many problems arising in science and engineering applications, the dependent
variable is a function of two or more independent variables, e.g., z “ fpx, yq. Such functions
are usually referred to as multivariate functions. When multivariate discrete functions areInterpolation and Extrapolation  353
FIGURE 6.14: Graphical depiction of bilinear interpolation.
available as tabular data, multivariate interpolations are inevitable to perform calculus
operations such as differentiation, integration, and so on.
Much of the theory for univariate interpolation can be generalized to multivariable in￾terpolation problems. Polynomial interpolation of multivariate discrete functions is usually
more difficult than that of univariate ones. Because there is a lack of uniqueness, particu￾larly, it may not suffice to require that the interpolation points be distinct. In this section,
for simplicity, we consider a discrete function of two independent variables, z “ fpx, yq,
containing known values on a regular grid (not necessarily uniformly spaced) to illustrate
the implementation of multivariate interpolation.
6.6.1 BILINEAR INTERPOLATION
The method is generally applied to discrete functions sampled on two-dimensional rectangu￾lar grids. Consider a discrete function with two variables defined for a rectangular mesh, i.e.,
a set of nonuniform discrete data zij “ fpxi, yj q “ fij for i “ 1, 2,...,n and j “ 1, 2,...,m
representing the known discrete values corresponding to pxi, yj q in a rectangular (a ď x ď b
and c ď y ď d) domain.
Bilinear interpolation is based on the assumption that the variation between the neigh￾boring four data points is linear, i.e., z “ fpx, yq “ a ` bx ` cy. The closer the points are
to each other (Δx “ xb ´ xa Ñ 0 and Δy “ yb ´ ya Ñ 0), the better the accuracy of the
interpolation is. In Fig. 6.14, the interpolant, fval, and its neighboring data points (fA, fB,
fC , and fD) are illustrated. The linear interpolation in terms of neighboring nodes can be
written as
fval “ 1
A pfAAa ` fBAb ` fC Ac ` fDAdq (6.63)
where Aa “ pxb´xqpy´yaq, Ab “ px´xaqpy´yaq, Ac “ pxb´xqpyb´yq, Ad “ px´xaqpyb´yq
and A “ pxb ´ xaqpyb ´ yaq are the areas of rectangles.
An alternative bi-linear interpolation equation has the form: fpx, yq “ a0 `a1x`a2y `
a3xy, where the coefficients (a0, a1, a2, and a3) are obtained by solving a system of four
equations from direct substitution, i.e., fpxa, ybq “ fA, fpxb, ybq “ fB, fpxa, yaq “ fC , and
fpxb, yaq “ fD.
A pseudomodule, BILINEAR_INTERP, performing bilinear interpolation for the input
of a non-uniformly spaced set of data, pxi, yj , fij q for i “ 1, 2,...,m and j “ 1, 2,...,n,
is presented in Pseudocode 6.6. The module requires a vector x of length m, containing
the x-coordinates, and a vector y of length n, containing the y-coordinates of the data
points, a two-dimensional array f, containing the fij data at pxi, yj q for i “ 1, 2,...,m and
j “ 1, 2,...,n, and (xval, yval) is the coordinate point for which the interpolation is to be
performed. The output fval is the interpolated value, fval “ fpxval, yvalq. Though not354  Numerical Methods for Scientists and Engineers
Pseudocode 6.6
Module BILINEAR_INTERP (m, n, xval, yval, x, y,f, fval)
\ DESCRIPTION: A pseudomodule to compute fpxval, yvalq of a bivariate
\ nonuniform discrete data set using bilinear interpolation.
\ USES:
\ BINARY_SEARCH:: A binary search module given in Pseudocode 6.4.
Declare: xm, yn, fmn \ Declare array variables
\ Input validity check for xval and yval should be inserted here
BINARY_SEARCH(m, xval, x, i) \ Find first index i of xi ď xval ă xi`1
BINARY_SEARCH(n, yval, y, j) \ Find first index j of yj ď yval ă yj`1
Area Ð pxi`1 ´ xiq˚pyj`1 ´ yj q
f a Ð fi,j`1; Aa Ð pxi`1 ´ xvalq˚pyval ´ yj q
f b Ð fi`1,j`1; Ab Ð pxval ´ xiq˚pyval ´ yj q
fc Ð fij ; Ac Ð pxi`1 ´ xvalq˚pyj`1 ´ yvalq
f d Ð fi`1,j ; Ad Ð pxval ´ xiq˚pyj`1 ´ yvalq
fval Ð pf a ˚ Aa ` f b ˚ Ab ` fc ˚ Ac ` f d ˚ Adq {Area \ Employ Eq. (6.63)
End Module BILINEAR_INTERP
given here to save space, the module should include an input validity check to determine
whether (xval, yval) is within the domain of the discrete function. Then, the first indices
of the rectangular cell (i, j) where the point (xval, yval) is located are determined through
the binary search algorithm. Finally, the interpolate fval is obtained upon employing Eq.
(6.63) and returned as output.
6.6.2 BIVARIATE LAGRANGE INTERPOLATION
A second method involves the use of Lagrange polynomials, which also have the capability
of representing nonuniform discrete data sets. The application of this technique requires
defining the data on square or rectangular grids, typically on all four corners of an element.
Consider a rectangular, non-uniform grid in the x´y-plane, and data is defined at every
intersecting grid line, as depicted in Fig. 6.14. Using the notations described in Section 6.1,
the bivariate Lagrange interpolating polynomial can be written (in so-called tensor product)
as
z “ fpx, yq – pm,npx, yq “ ÿm
i“1
Lipyq
˜ÿn
j“1
Lj pxqfij¸
(6.64)
where m and n are the number of data points in the x and y-directions, pxm, yn, fmnq
denotes a bivariate set of discrete data with i “ 1, 2,...,m and j “ 1, 2,...,n defined in a
rectangular domain, and Lmpxq and Lnpyq are the Lagrange polynomials (basis functions)
derived using Eq. (6.8) for x and y data, respectively.
In this approach, the multidimensional interpolation problem is decoupled into a prod￾uct of one-dimensional ones. In fact, extending this bivariate approach to more dimensions is
also, in principle, quite simple. The advantages and disadvantages of the Lagrange method
pretty much apply here, but other interpolation methods should be considered if the dis￾crete function is irregularly dispersed throughout the domain or defined for non-rectangular
domains.Interpolation and Extrapolation  355
Pseudocode 6.7
Module BIVARIATE_LAGRANGE (m, n, xval, yval, x, y,f, fval)
\ DESCRIPTION: A pseudomodule to compute fpxval, yvalq of a bivariate
\ nonuniform discrete data set using Lagrange interpolation.
\ USES:
\ LAGRANGE_P:: Module generating Lagrange polynomials, Pseudocode 6.1
Declare: xm, yn, fmn, Lxm, Lyn \ Declare arrays
\ Input validity checks for xval and yval should be inserted here
LAGRANGE_P(m, xval, x,Lxq \ Compute Lxipxq’s i “ 1,...,m
LAGRANGE_P(n, yval, y,Lyq \ Compute Lyipxq’s i “ 1,...,n
fval Ð 0 \ Initialize Interpolate
For “
i “ 1, m‰ \ Accumulator loops
For “
j “ 1, n‰
fval Ð fval ` Lxi ˚ Lyj ˚ fij \ Accumulate Lxi ˚ Lyj ˚ fij ’s
End For
End For
End Module BIVARIATE_LAGRANGE
A pseudomodule, BIVARIATE_LAGRANGE, performing bivariate Lagrange interpola￾tion using a non-uniformly spaced data set, pxi, yj , fij q for i “ 1, 2,...,m and j “ 1, 2,...,n
is presented in Pseudocode 6.7. The input and output variables are the same as those of the
Pseudocode 6.6. The Lagrange polynomials (Lxmpxvalq and Lynpyvalq) are evaluated for
the interpolation point pxval, yvalq using the set of x and y arrays. Then, the interpolant
fval is obtained from Eq. (6.64) and returned as output.
6.7 EXTRAPOLATION
So far, we have discussed the methods of estimating the values of a discrete function within
its original observation range, x0 ď x ď xn, called “interpolation.” Estimating a discrete
function outside of its original observation range (x ă x0 or x ą xn) is referred to as
extrapolation.
An extrapolation procedure, in essence, is simply implementing a generated interpolat￾ing polynomial outside its range. In interpolation, the distribution is anchored at both ends
of a subinterval where the interpolation point lies, whereas in extrapolation, the distribution
is anchored only on one side and is loose on the other side. That is why the uncertainty
in extrapolation is much greater, mainly due to the assumption that the existing trends
will continue outside the data range. If a discrete function is uniformly spaced, then the
Gregory-Newton forward or backward interpolation formulas can be employed for extrap￾olations. The choice of formula depends on the position of x in relation to the data range.
To extrapolate for x ă x0, the first row corresponding to x “ x0 is used as the baseline
together with the forward interpolation formula. To extrapolate for x ą xn, the backward
interpolation formula with the last row (x “ xn) as the baseline is adopted.
ERROR ASSESSMENT. In Section 6.3, it was shown that the most accurate estimates were
obtained when the interpolant was in the neighborhood of the original data points. In
extrapolation, however, the first and last data points of the set are the outermost anchors.
Since the extrapolating polynomial extends far out of its original interval, the extrapolation356  Numerical Methods for Scientists and Engineers
error can become very large. Hence, extreme care should be exercised when performing
extrapolation.
EXAMPLE 6.8: Bivariate Lagrange Interpolation
Consider a ternary system (A, B, and C liquids) in equilibrium. Estimating the
logarithm of the activity coefficient of C (log γC ) with the mole fractions of the
liquids A and B (xA and xB) is needed. Experimentally generated data for z “
fpxA, xBq “ log `
γC pxA, xBq
˘ are presented in the table below. Use bi-Lagrange
interpolation to estimate f(0.22, 0.10) and f(0.13, 0.27).
xB
xA 0.05 0.15 0.35
0 0.231 0.316 0.484
0.10 0.207 0.293 0.464
0.25 0.161 0.246 0.414
0.35 0.124 0.208 0.373
SOLUTION:
By inspection of tabular data, we determine that m “ 3 and n “ 2. The Lagrange
polynomials for xA’s, L0pxAq, L1pxAq, L2pxAq and L3pxAq, and xB’s L0pxBq, L1pxBq,
L2pxBq are constructed as follows:
L0pxAq“´800
7 pxA ´ 0.1qpxA ´ 0.25qpxA ´ 0.35q,
L1pxAq “ 800
3 pxA ´ 0qpxA ´ 0.25qpxA ´ 0.35q,
L2pxAq“´800
3 pxA ´ 0qpxA ´ 0.10qpxA ´ 0.35q,
L3pxAq “ 800
7 pxA ´ 0qpxA ´ 0.10qpxA ´ 0.25q,
L0pxBq “ 100
3 pxB ´ 0.15qpxB ´ 0.35q,
L1pxBq“´50pxB ´ 0.05qpxB ´ 0.35q,
L2pxBq “ 100
3 pxB ´ 0.05qpxB ´ 0.15q
Using fpxA,i, xB,j q “ fij notation, the data is denoted by an array f with its elements
defined as f00 “ 0.231, f01 “ 0.316, f10 “ 0.207, f20 “ 0.161, and so on. Then the
interpolating polynomial using Eq. (6.64) can be expressed as
fpxA, xBq – pm,npxA, xBq “ ÿn
j“0
Lj pxBq
´ ÿm
i“0
LipxAqfij¯
Using this interpolating polynomial we get f(0.22,0.10) =0.21386 and f(0.13,0.27)=
0.38742. The true values are 0.213418 and 0.386813, respectively.
Discussion: Interpolation by univariate polynomials is a very classical and well￾studied topic. However, interpolation by polynomials of several variables is much
more intricate and is a subject that is currently an active area of research.
Interpolation to a point within the domain, where the data is defined at the nodes
placed at each intersection of a rectangular grid, is the simplest case in this regard.
The bilinear and bivariate Lagrange interpolation methods presented in this sectionInterpolation and Extrapolation  357
FIGURE 6.15: Extrapolation for x ă x0 using Gregory-Newton for￾ward difference table depicting three different cases, (a), (b), and (c).
can give satisfactory estimates for smooth functions. When deciding which method
to use among the ones presented here, it is best to pick the method that matches
the nature and behavior of the data while avoiding overfitting or underfitting.
In order to obtain a reasonable extrapolant, the discrete function should be smooth
and suitable for a polynomial extrapolation. In this respect, the backward or forward inter￾polation tables offer a clue as to how good an extrapolation may turn out to be. In other
words, for smooth functions, the magnitudes of successive higher-order differences along the
chosen baseline should decrease, i.e.,
|Δfk| ą |Δ2fk| ą |Δ3fk| ą¨¨¨ or |∇fk| ą |∇2fk| ą |∇3fk| ą ¨¨¨
Otherwise, any extrapolation operation can be extremely risky in that the estimates may
drastically deviate from the true trend.
For extrapolation of x values (x ă x0) with GN-FWD-IF, consider the snapshots from
the baselines of three different tables, as given in Fig. 6.15. In Fig. 6.15a, the magnitude (in
absolute value) of the successive differences decreases for smooth distributions as the order
of differences increases. For this case, the highest degree extrapolating polynomial would
be fpxq – p4pxq for extrapolations of x ă x0. However, as x shifts further away from x0,
predictions will tend to diverge from true expectations. On the other hand, in Fig. 6.15b,
the successive differences depict a constantly increasing trend in magnitude, which indicates
that the extrapolations will diverge when using high-order extrapolating polynomials. In
such cases, the linear extrapolating polynomial, fpxq – p1pxq, can be used as the best
alternative. In Fig. 6.15c, the magnitude of the forward differences after the |Δ2fk| term
depicts a continuously increasing trend. It is tempting to use the second order fpxq – p2pxq
polynomial for extrapolation, but it will be a prudent practice to check the |pnpxq ´ pn´1pxq|
difference before deciding whether to add an extra term to the interpolating polynomial.
The extrapolation process is much more delicate than the interpolation process. The
function may have a discontinuity that is not obvious in the data but still has a fatal effect358  Numerical Methods for Scientists and Engineers
Extrapolation
‚ A univariant extrapolation is the simplest way of data estimation;
‚ An extrapolation method allows estimating the quantities that are
not directly measurable or observable, or that are too costly or cpu￾time intensive;
‚ It helps filling in the gaps outside data range, making the data more
complete and consistent.
‚ Quality of the extrapolation depends on the assumptions made (con￾tinous, smooth, etc.) and/or approximations that may not always
hold true or represent the reality of the data;
‚ It is significantly affected by the fluctuations in the existing data sets;
‚ It can introduce errors and uncertainties in the estimated values,
especially when the data is noisy or extrapolated far beyond the data
range.
on the extrapolation process. As a final word, in situations where extrapolation cannot be
avoided, exercise the following precautionary actions: (1) use low-order (linear or quadratic)
polynomials; (2) restrict the value of x as close as possible to the tabulated region, even if a
low-order polynomial approximation is used; and (3) plot the data and verify whether the
extrapolated value is meaningful or not.
EXAMPLE 6.9: Extrapolation of oscillating functions
Generate uniformly spaced discrete data points for fpxq “ sin x with x “ 0.1, 0.5,
0.9, 1.3, and 1.7. Using the data, estimate f(2.1), f(2.5), f(2.9), f(3.3), and f(3.7)
and discuss the accuracy of the extrapolants.
SOLUTION:
Since estimates for values greater than 1.7 are sought, the baseline for the extrap￾olations is chosen as x “ 1.7. Also, note that the number of entries in the backward
difference table is the largest in this row (baseline). Hence, for uniformly spaced data
with h “ 0.4, the backward difference table is generated and presented in Table 6.13.
TABLE 6.13
i xi fi ∇fi ∇2fi ∇3fi ∇4fi
1 0.1 0.099833
2 0.5 0.479426 0.379592
3 0.9 0.783327 0.303901 ´0.075691
4 1.3 0.963558 0.180231 ´0.123670 ´0.047979
5 1.7 0.991665 0.028107 ´0.152125 ´0.028455 0.019525
Setting x5 “1.7 as the baseline, the entries in this row are f5 “ 0.991665, ∇f5 “
0.028107, ∇2f5 “ ´0.152125, ∇3f5 “ ´0.028455, and ∇4f5 “ 0.019525. Notice that
the magnitude of the second difference jumps to «0.152, but then the magnitudes
of the higher (3rd and 4th) differences depict a steady decline. The extrapolationInterpolation and Extrapolation  359
variable becomes s “ px ´ x5q{h “ px ´ 1.7q{0.4. The abscissas of the points to be
extrapolated are given in increments of h; hence, the extrapolation variable becomes
s “ 1, 2, 3, 4, and 5 for x “ 2.1, 2.5, 2.9, 3.3, and 3.7, respectively.
The GN-BKWD interpolation formula will be used to generate extrapolation
polynomials passing through all data points:
fpxq – p4psq “ f5 ` s ∇f5 `
sps ` 1q
2! ∇2f5 `
sps ` 1qps ` 2q
3! ∇3f5
`
sps ` 1qps ` 2qps ` 3q
4! ∇4f5
The 4th-degree extrapolation polynomials for x “ 2.1, 2.5, and 2.9 are explicitly
given as
fp2.1q –p4ps “ 1q “ f5 ` ∇f5 ` ∇2f5 ` ∇3f5 ` ∇4f5,
fp2.5q –p4ps “ 2q “ f5 ` 2 ∇f5 ` 3 ∇2f5 ` 4 ∇3f5 ` 5∇4f5,
fp2.9q –p4ps “ 3q “ f5 ` 3 ∇f5 ` 6 ∇2f5 ` 10 ∇3f5 ` 15∇4f5,...
Using the above extrapolation expressions, any low-degree extrapolation polyno￾mial can also be deduced. The first two terms correspond to linear, the three terms
to quadratic, and the four terms to cubic extrapolating polynomials.
TABLE 6.14
x sin x p2pxq p3pxq p4pxq
2.1 0.863209 0.86765 (0.44%) 0.83919 (2.40%) 0.858709 (0.45%)
2.5 0.598472 0.59151 (0.70%) 0.47769 (12.1%) 0.575284 (2.32%)
2.9 0.239249 0.16324 (7.60%) ´0.12131 (36.1%) 0.171499 (6.77%)
3.3 ´0.157746 ´0.41715 (25.9%) ´0.98625 (82.9%) ´0.303010 (14.5%)
3.7 ´0.529836 ´1.14966 (62.0%) ´2.14560 (162%) ´0.779090 (24.9%)
A summary of extrapolated estimates using 2nd, 3rd, and 4th-degree GN-BKWD
interpolating polynomials along with the true errors is presented in Table 6.14. It
is observed that the third-degree extrapolant, p3pxq, produces worse estimates than
that of the second-degree extrapolant p2pxq.
FIGURE 6.16360  Numerical Methods for Scientists and Engineers
A graphical depiction of the absolute errors, enpxq “ |sin x ´ pnpxq|, on the
1.5 ď x ď 4 interval is presented in Fig. 6.16. The results of the linear extrapolations
are not presented here because they gave the worst results. We note that the second
and fourth-degree polynomials extrapolate fairly well up to x « 3, while the third
degree polynomial deviates from the true distribution much earlier. The sine function
has a maximum at π{2p« 1.57q, so a quadratic formula obtained from the last three
interpolation points seems to better represent the curvature of the data beyond
x “ 1.7. While none of the extrapolated values could be called accurate by any
means, the performance of the second-degree approximation is the “best.”
Discussion: The further away the extrapolation point is from the baseline, the larger
the extrapolation error will inevitably be. If a polynomial extrapolation must be done
with non-smooth or poorly behaved functions, then a very low degree extrapolation
is usually the safest but even this should be performed only for values of x very close
to the baselines.
6.8 CLOSURE
Scientific experimentation, data collection, or numerical computation usually result in values
for a function only at discrete points that are either uniformly or non-uniformly spaced.
We often want to find the value of the function fpxq at points that do not correspond to
discrete data points. A number of numerical methods have been developed over the years
for interpolation (estimating the unknown function within the data range) or extrapolation
(estimating the unknown function outside the data range) purposes.
Lagrange interpolation makes use of Lagrangian polynomials, and it can be applied to
uniformly or non-uniformly spaced discrete functions. A Lagrange interpolating polynomial
always includes the original data points. The interpolant is anchored on both sides of an
interval, but the error tends to be largest toward the middle of any subinterval. If the
subinterval size is very large, the magnitude of the interpolation errors increases. Also, for a
large data set, the number of floating-point arithmetic operations and rounding errors will
increase considerably, yielding Runge’s effect. Lagrange interpolating polynomials are not
suitable for extrapolation as well.
Newton’s divided difference can be applied to both uniformly and non-uniformly spaced
discrete functions, while the Gregory-Newton interpolation polynomials are specifically
suited for uniformly spaced discrete functions. Similar to the Lagrange interpolation, the
interpolating polynomials are anchored at the interpolation points but yield the largest
errors toward the middle of each interval. Unlike the Lagrange method, lower-order inter￾polating polynomials can be constructed by using fewer data points, which reduces some of
the handicaps of high-order approximating polynomials. The tabulated differences (in abso￾lute value) depicting a decreasing trend along a chosen baseline imply a “smooth function,”
which likely leads to convergence with fewer terms.
The cubic spline interpolation method is based on constructing a third-order polynomial
for each interval instead of using a single interpolation polynomial for the whole range of
data. The accuracy of cubic spline interpolation depends on the distribution of data and the
suitability of the end condition. Calculating the coefficients of the cubic polynomials takes
time and effort, but once they are evaluated, only a small number of arithmetic operations
are needed to obtain the interpolate as well as its derivatives.Interpolation and Extrapolation  361
Multivariate interpolation is much more complex than univariant interpolations. Bi￾linear and bi-Lagrange interpolations for smooth functions and data defined on a regular
rectangular grids are presented.
6.9 EXERCISES
Section 6.1 Lagrange Interpolation
E6.1 Find the Lagrange interpolating polynomial that passes through px0, y0q and px1, y1q.
E6.2 Find the Lagrange interpolating polynomials of the highest degree for the given data sets.
(a) x 1 7
y 7 1
(b) x 0 3 11
y 6 0 12
(c) x 12 4 5
y 8 19 71 124
E6.3 Use the following data to (a) construct a Lagrange interpolating polynomial and (b) show
that it satisfies the interpolation points.
x ´10 2 3
fpxq 1 ´1 19 77
E6.4 (a) Use Lagrange’s method to construct a quadratic polynomial passing through Ap2, ´4q,
Bp5, ´1q, and Cp8, 20q; (b) find a quadratic polynomial by setting up a system of equations as
ax2
i ` bxi ` c ´ yi “ 0 for i “ 1, 2, and 3; and (c) compare the polynomials obtained in Parts (a)
and (b), and (d) plot the Lagrange polynomials on [2,8].
E6.5 (a) Construct a Lagrange interpolating polynomial for ln x on [1,4] using ln(1)=0,
ln(2)=0.693147, and ln(4)=1.38629; (b) estimate ln(3) as well as its interpolation error in percent.
E6.6 Find the Lagrange interpolating polynomial passing through Ap´1, ´9q, Bp1, ´1q, Cp2, 3q,
and Dp3, 23q.
E6.7 Determine the interval width (h) of a set of uniformly spaced discrete data generated from
fpxq “ 1{x2 in [2,3] so that a quadratic interpolating polynomial interpolates to five-decimal
places accuracy.
E6.8 Use Lagrange’s method to estimate fp1.5q and the error bound using the following data
generated from fpxq “ 1{
?x ` 1.
x 0 0.5 1 2
fxq 1 0.8165 0.7071 0.57735
E6.9 Given pxi´1, fi´1q, pxi, fiq, and pxi´2, fi´2q are points uniformly spaced by h. (a) Find an
approximation for fpxq using the Lagrange interpolating polynomial passing through all three
points; (b) use this approximation also to develop approximations for the first- and second￾derivatives at x “ xi, i.e., f1
i “ f1
pxiq, and f 2
i “ f 2pxiq.
E6.10 A pump characteristic curve can be expressed for the pump head, hp, with a second-degree
polynomial: hp “ aQ2 ` bQ ` c. The set of data for the expected range of operation of a certain
brand of pump is given below. Use the given data and Lagrange’s method to find the coefficients
of the pump characteristic curve.
Q (m3/h) 5 65 140
hp (m) 10.91 9.56 5.43
E6.11 The thermal conductivity of an alloy as a function of temperature is tabulated below.
(a) Find the Lagrange interpolating polynomial for the thermal conductivity that passes through
all data points; (b) plot the data and the interpolating polynomial together on the same graph362  Numerical Methods for Scientists and Engineers
and comment on how well the polynomial fits the data; (c) use the interpolating polynomial to
estimate the thermal conductivity of the alloy for T “ 0, 17, 100, and 450˝C.
T (
˝C) ´23 77 327 727
k pW/m ¨ ˝Cq 406 396 379 352
E6.12 The given is a set of data from the lift coefficient measurements for a wing as a function
of angle of attack (α). (a) Find the Lagrange interpolating polynomial for the lift coefficient that
passes through all data points; (b) plot the data and the interpolating polynomial together on the
same graph and discuss how well the polynomial fits the data; (c) use the interpolating polynomial
to estimate the lift coefficient for α “ 0, 10, and 21.5˝.
α (
˝) ´5 5 16 20 23
CL 0.23 1.51 2.9 3 1
E6.13 The light transmittance of a polymer material was tested using polymer sheets in various
thicknesses. Experimental results for the transmissivity (τ ) as a function of sheet thickness (t) are
presented below. (a) Find a Lagrange interpolating polynomial for the transmissivity that passes
through all data points; (b) plot the data and the interpolating polynomial together on the same
graph and comment on how well the polynomial fits the data; (c) use the interpolating polynomial
to estimate the transmissivity for t “ 2, 4, and 6 mm.
t (mm) 0 3 5 7
τ (%) 100 71 56 45
E6.14 The dissolution rate of a chemical substance in water was experimentally measured as
a function of temperature. Using the tabulated results below, (a) find a Lagrange interpolating
polynomial for the dissolution rate that passes through all data points; (b) plot the data and the
interpolating polynomial together on the same graph and comment on how well the polynomial
fits the data; (c) use the derived interpolating polynomial to estimate the dissolution rates for
T “ 285, 295, and 303 K.
T (K) 275 300 305
R (M/s) 0.35 2 2.75
E6.15 The Brinell hardness test is used to determine the hardness of metals and alloys. The test
is conducted by applying a predetermined test load to a carbide ball of fixed diameter, which is
held for a period of time and then removed. The resulting impression diameter, d, and hardness are
related to each other. A specimen is subjected to a series of tests with various loads. The impression
diameter and Brinell Hardness Number (BHN) of a series of tests are presented below. (a) Find
the Lagrange interpolating polynomial for the Brinell hardness number that passes through all
data points; (b) plot the data and the interpolating polynomial together on the same graph and
comment on how well the polynomial fits the data; (c) use the interpolating polynomial to estimate
the Brinell hardness numbers for the specimen with d “ 3.5, 4.5, and 5.5 mm.
d (mm) 2.5 3 4 5 6
BHN 600 415 229 145 95
E6.16 The specific volume (m3/kg) of the vapor phase of a liquid at various temperatures is
tabulated below. (a) Find the Lagrange interpolating polynomial for the specific volume that
passes through all data points; (b) plot the data and the interpolating polynomial together on
the same graph and comment on how well the polynomial fits the data; (c) estimate the specific
volume for T “ 60, 100, 135, and 175˝C using the derived interpolating polynomial.Interpolation and Extrapolation  363
T (
˝C) 50 72 83 200
ν (m3/kg) 12 5 3 0.125
Section 6.2 Newton’s Divided Differences
E6.17 Complete the following divided difference tables, where fij is the shorthand notation for
frxi, .., xj s.
(a) (b)
i xi fi fi,i`1 fi,i`2 fi,i`3 fi,i`4 i xi fi fi,i`1 fi,i`2 fi,i`3
0 1 9 16 14 0 ´0.7089 0 0.25 ´5.25 7.8 4 0
1 1.5 17 ? ? ? 1 1.2 ? ? ?
22 ? ? ? 21 ? ?
3 2.5 ? ? 3 2.2 ?
43 ?
E6.18 Construct Newton’s divided difference table for the following data set and find a polyno￾mial approximation that passes through all data points.
xi 12 4 5
fpxiq 0 7 39 64
E6.19 Construct Newton’s divided difference table for the following data set and find a polyno￾mial approximation that passes through all data points.
xi ´1 1 23
fpxiq ´9 ´1 3 23
E6.20 Construct Newton’s divided difference table for the following data set and find a polyno￾mial approximation that passes through all data points.
xi 0.2 0.5 1 1.5 1.7
fpxiq 0 0 0 0 0.14
E6.21 For the given data set below, (a) obtain the coefficients of Newton’s divided difference
interpolation formula, which passes through all data points; (b) obtain the coefficients for the
data reordered as {(0,1), (3,52), (2,9), (4,145)} and {(4,145), (0,1), (2,9), (3,52)} and comment
on your observations.
xi 02 3 4
fpxiq 1 9 52 145
E6.22 Using the following data, (a) construct Newton’s divided difference table; (b) generate first￾to fourth-degree interpolating polynomials; (c) plot the interpolating polynomials along with the
data points; (d) estimate fp0.5q, fp1.5q, and fp2.5q using the approximating polynomials obtained
in Part (b). Given the true values of fp0.5q “ 1.040625, fp1.5q “ 3.27812, and fp2.5q “ 10.76562,
which interpolating polynomial gave better estimates? Explain why.
xi 0 1 1.3 2 3
fpxiq 1 1.55 2.4 6.6 13.15
E6.23 Using the lift coefficient-angle of attack data in E6.12, (a) generate first- to fourth-degree
interpolating polynomials; (b) plot the interpolating polynomials along with the data points; (c)
estimate CL’s for α “ 0, 10, and 21.5˝ using the interpolating polynomials found in Part (a).
Which interpolating polynomial gave better estimates? Explain why.
E6.24 Using the dissolution rate-temperature data in E6.14, (a) derive first- and second-degree
interpolating polynomials; (b) plot the interpolating polynomials along with the data points; (c)364  Numerical Methods for Scientists and Engineers
estimate the dissolution rates for T “ 285, 295, and 303 K using the interpolating polynomials
found in Part (a). Which interpolating polynomial gave better estimates? Explain why.
E6.25 Using the specific volume of the saturated vapor-temperature data in E6.16, (a) gener￾ate first- to third-degree interpolating polynomials; (b) plot the interpolating polynomials along
with the data points; (c) estimate the specific volume for T “ 60, 100, 135, and 175˝C using
the interpolating polynomials found in Part (a); (d) which interpolating polynomial results in
better estimates? (e) Reorder the data backward to obtain first- and second-degree interpolating
polynomials. Which interpolating polynomial gave better estimates? Explain why.
E6.26 The compressibility factor of a gas mixture at 50˝C is given below as a function of pressure.
Using the furnished data, (a) derive first- to third-degree Newton’s interpolating polynomials; (b)
plot the interpolating polynomials along with the data points; (c) estimate the compressibility
factors at p “ 5, 20, 30, 50, 75, and 90 bar using the interpolating polynomials derived in Part
(a). Which interpolating polynomial gives better estimates? Explain why.
p (bar) 1 10 40 100
Z 0.996 0.96 0.83 0.6
E6.27 Repeat E6.26 by reordering the data backward.
Section 6.3 Newton’s Formulas for Uniformly Spaced Data
E6.28 For the given data below, (a) derive a Gregory-Newton forward interpolating polynomial
that passes through all data points, and (b) estimate fp0.23q and fp1.37q.
x 0.2 0.6 1 1.4 1.8 2.2
fpxq 0.008 1.512 13 52.136 145.8 330.088
E6.29 Repeat E6.28 using the Gregory-Newton backward interpolation formula.
E6.30 Experimental data below depicts the solubility of a certain rock salt (x in g/100 g water) in
water as a function of water temperature (T in ˝C). Using the data, (a) derive first- through fifth￾degree Gregory-Newton forward interpolating polynomials; (b) plot the interpolating polynomials
along with the data points and interpret the agreement; (c) using the interpolating polynomials
in Part (a), estimate the solubility at T “ 15, 30, 40, 60, and 75˝C. Which interpolating poly￾nomial gave better estimates for the given data range? Explain why. Given: The true values are
xp15˝C)=217, xp30˝C)=250.26, xp40˝C)=264.07, xp60˝C)=283.53, and xp75˝C)=294.24.
T (
˝C) 5 20 35 50 65 80
x (g) 164 231 258 275 287 297
E6.31 The dynamic viscosity of a water-based solution is given below as a function of tempera￾ture. Using this data, (a) derive first- through fourth-degree Gregory-Newton forward interpolating
polynomials; (b) plot the interpolating polynomials along with the data points and interpret the
results; (c) estimate the dynamic viscosity for T “ 15, 45, 58, 85, and 95˝C using the interpolating
polynomials. Which interpolating polynomial gave estimates for the data range given? Explain
why. Given: The true values are μ(15˝C)=1.1404, μ(45˝C) =0.5955, μ(58˝C)=0.4788, μ(85˝C)=
0.3307, μ(95˝C)= 0.2948 cP.
T (
˝C) 10 30 50 70 90
μ (cP) 1.305 0.8 0.545 0.4013 0.312
E6.32 The experimental data used to determine the drag coefficient (CD) of an airfoil as a func￾tion of angle of attack (α) are given below. Using the furnished data, (a) derive first- through
fourth-degree Gregory-Newton forward and backward interpolating polynomials; (b) plot the in￾terpolating polynomials along with the data points and interpret the results; (c) estimate the
drag coefficient for α “ ´2, 3, and 10˝ using the interpolating polynomials. Which interpolating
polynomial gives better estimates for the data range given? Explain why.Interpolation and Extrapolation  365
α (deg) -8˝ -4˝ 0˝ 4˝ 8˝ 12˝
CD 0.0435 0.0302 0.0448 0.0707 0.1122 0.1508
E6.33 The Rockwell hardness (R) of a steel specimen (rods) heat-treated from one end was
measured at a distance of x (in mm) from the treated end. Using the distance-hardness data
presented below, (a) construct forward, backward, and central difference tables; (b) derive the
first- through sixth-degree Gregory-Newton forward and backward interpolating polynomials; and
(c) estimate the Rockwell hardness at x “ 3, 8, 14, and 27 mm from the treated end using the
interpolating polynomials. Which interpolating polynomials give better estimates for the data
range given? Explain why.
x 0 5 10 15 20 25 30
R 60 35.3 26.1 23.8 23.3 22 20
E6.34 Saturation pressure (p) versus saturation temperature (T) data for a water-based solution
is presented below. Using the data, (a) construct forward, backward, and central difference tables;
(b) derive the first- through sixth-degree Gregory-Newton forward and backward interpolating
polynomials; (c) estimate the saturation pressure for T “ 15, 45, 90, 100, and 145˝C using the
interpolating polynomials. Which interpolating polynomials gave better estimates for the data
range given? Explain why?
Tp
˝Cq 5 30 55 80 105 130 155
p (kPa) 0.87 4.25 15.76 47.42 120.9 270.3 543.5
E6.35 The compressive strength (P in MPa) of concrete is improved by the process called “aging”
(days). A new concrete recipe that includes an additive (labeled as XZ12) has been put through
aging tests. Using the tabulated data, (a) construct forward, backward, and central difference
tables; (b) derive the Gregory-Newton forward and backward interpolating polynomials from the
first to fifth degree; and (c) estimate the compressive strength for t “ 7, 14, and 45 days using
the interpolating polynomials.
Aging 1 11 21 31 41 51
P 11.15 32.76 36.76 38.8 40.1 41
Section 6.4 Cubic-Spline Interpolation
E6.36 Using the data in E6.12, (a) employ the natural spline end condition to find cubic spline
interpolating polynomials for the lift coefficient; (b) estimate the lift coefficients at α “ 0, 10, and
21.5˝; (c) repeat Parts (a) and (b) with the linear extrapolation end condition.
E6.37 Using the data in E6.15, (a) employ the natural spline end condition to find cubic spline
interpolating polynomials for the Brinell Hardness number (BHN); (b) estimate the BHN for
d “ 3.5, 4.5, and 5.5-mm; and (c) repeat Parts (a) and (b) with a linear extrapolation end
condition.
E6.38 Using the data in E6.16, (a) employ the natural spline end condition to find cubic spline
interpolating polynomials for the specific volume; (b) estimate the specific volume at T “ 60, 100,
135, and 175˝C; and (c) repeat Parts (a) and (b) with a linear extrapolation end condition.
E6.39 Using the data in E6.26, (a) employ the natural spline end condition to find cubic spline
interpolating polynomials for the compressibility factor of CO; (b) estimate the compressibility
factors at p “ 5, 20, 30, 50, 75, and 90 bar; (c) repeat Parts (a) and (b) with a linear extrapolation
end condition.
E6.40 The length parameter (ξ) of a triangular fin and its thermal efficiency (η) are presented
below. Using cubic splines with a linear extrapolation end condition, estimate the fin thermal
efficiency at ξ “ 0.25, 0.5, 0.6, 0.7, 0.8, 1, 1.25, 1.5, 1.75, and 2.366  Numerical Methods for Scientists and Engineers
ξ (-) 0 0.41 0.77 1.2 1.8 2.5
η (%) 100 90 78 62 49 38
E6.41 Using the data in E6.40 and the cubic splines with a linear extrapolation end condition,
estimate the length parameter at η “ 55%, 65%, 75%, 85%, and 95%.
Section 6.5 Rootfinding by Inverse Interpolation
E6.42 For the given tabulated discrete function fpxq, estimate (a) the root in the given interval,
and (b) find the x value corresponding to fpxq “ 0.075.
x 3.6 3.7 3.8 3.9 4 4.1
fpxq 0.09546 0.05383 0.01281 ´0.02724 ´0.06604 ´0.10327
E6.43 For the given tabulated discrete function fpxq, estimate (a) the root in the given interval,
and (b) find the x value corresponding to fpxq “ 0.10 and 0.35.
x 1.3 1.5 1.7 1.9 2.1
fpxq ´0.65893 ´0.3918 ´0.09793 0.21952 0.55807
E6.44 For the given tabulated discrete function fpxq, estimate its root in the given interval.
x 1.5 1.75 2 2.25 2.5
fpxq ´0.42324 ´0.18777 ´0.02456 0.05625 0.4962
E6.45 For the given tabulated discrete function fpxq, estimate its root in the given interval.
x 2 2.5 3 3.5
fpxq 8.1948 3.0164 ´5.0755 ´18.052
Section 6.6 Multivariate Interpolation
E6.46 The thermal conductivity, k (W/m¨
˝C), of an alloy is tabulated below as a function of an
impurity x (%) and temperature T (
˝C). Use Lagrange’s interpolation to obtain an interpolating
polynomial (passing through all data points) to estimate the thermal conductivity of the alloy
with impurity contents of 3.75%, 7.5%, 18.5%, and 27.5% at 150, 425, and 875˝C.
T (
˝C)
x (%) 0 200 600 1000
1 73 62 40 35
5 54 45 33 31
10 40 36 29 29
30 22 22 24 29
E6.47 The compressibility factor, ZpT,pq, of a water-based solution at various temperatures and
pressures is tabulated below. To estimate the compressibility factor at 10 and 40 bar and 550,
875, 1250, and 1750 K, derive Lagrange’s interpolation to obtain an interpolating polynomial that
passes through all data points.
T (K)
p (bar) 400 700 1000 2000
5 0.003 0.994 0.999 1.002
15 0.009 0.984 0.995 1.002
50 0.029 0.941 0.987 1.002
250 0.143 0.618 0.935 1.008Interpolation and Extrapolation  367
E6.48 The density of an acidic solution ρ (g/m3) as a function of acid content x (%) and tempera￾ture (˝C) is tabulated below. Use Lagrange’s interpolation to generate an interpolating polynomial
that uses all data points to estimate the compressibility factor for 15%, 50%, and 85% acid solu￾tions at 5, 25, and 55˝C.
Acid T (
˝C)
x (%) 0 15 40 60 100
1 10.1 10.1 9.98 9.89 9.65
34 12.7 12.5 12.4 12.2 11.9
79 17.4 17.2 16.9 16.8 16.4
100 18.5 18.3 18.1 17.9 17.5
E6.49 In the grain drying process, the moisture loss (M) is determined by the initial (%w)
and final moisture contents (%w). Use Lagrange’s method to obtain an interpolating polynomial
passing through all data points to estimate the moisture loss for initial moisture contents of 21,
25, and 28% that result in a final moisture content of 12, 14, 16, and 18%.
Initial Final moisture content %w
moisture 19 17 15 13 11
w% Moisture loss (kg/ton)
30 136 157 176 195 213
27 111 120 153 172 180
24 62 84 106 126 146
20 12 36 59 80 101
Section 6.7 Extrapolation
E6.50 Using E6.14 data and Newton’s divided differences, derive first- and second-degree extrap￾olation polynomials to estimate the dissolution rates at T “ 310 and 315 K.
E6.51 Using E6.16 data and Newton’s divided differences, derive first-, second-, and third-degree
extrapolation polynomials to estimate the specific volume at T “ 20, 40, 225, and 250˝C.
E6.52 Using E6.26 data and Newton’s divided differences, derive first-, second-, and third-degree
extrapolation polynomials to estimate the compressibility factor at 50˝C and 110, 130, and 200
bar.
E6.53 Using E6.28 data (excluding x “ 2.2) and Newton’s divided differences, (a) derive first-,
second-, third-, and fourth-degree extrapolation polynomials to estimate f(0), f(2.2), f(2.6), and
f(3); and (b) discuss the accuracy of the extrapolated values if fpxq “ 15x4 ´ 2x3 is the true
function.
E6.54 Using E6.30 data and Newton’s divided differences, derive first- to fifth-degree extrapola￾tion polynomials to estimate the dissolution rate of the salt at T “ 85 and 90˝C. What can you
say about the accuracy of the estimates?
E6.55 The laptop sales of a computer company between 2018 and 2023 are presented in the table
below. Assuming the trend will continue, forecast laptop sales in 2025, 2027, and 2030.
y (year) 2018 2019 2020 2021 2022 2023
S (thousand) 219.9 230.1 208 174.9 163.7 152
E6.56 Consider the saturation pressure versus saturation temperature data of a water-based
solution given in E6.34. Estimate the saturation pressure of the solution at T “ 160 and 175˝C
using linear, quadratic, and cubic approximations. Given that the true values are 609.2 and 849
kPa, respectively, which approximation produces better estimates? Explain.368  Numerical Methods for Scientists and Engineers
E6.57 Consider the concrete aging data in E6.35. Estimate the compressive strength at t “ 90
days. Given that the true value is 42.5, which approximation produces better estimates? Explain.
6.10 COMPUTER ASSIGNMENTS
CA6.1 Use the dynamic viscosity data of a water-based solution given in E6.31 to obtain in￾terpolating polynomials with the GN-FWD/-BKWD IFs. Calculate the absolute error using the
following analytical expression for viscosity and discuss the results.
μpTq “ 0.02414 ˆ 10247.8{pT `133q
CA6.2 The Gamma function, Γpxq, is defined by the integral given below.
Γpxq “ ż 8
0
ux´1
e
´udu
The Gamma function for some x in [0.1,1] is tabulated below. (a) Find the interpolating polyno￾mials using Newton’s divided difference method; (b) For each interpolating polynomial, plot the
absolute error, ei “ |Γi ´ Pnpxiq|.
x 0.1 0.2 0.3 0.45 0.75 1
Γpxq 9.51351 4.59084 2.99157 1.96814 1.22542 1
CA6.3 The following discrete data on r´3, 3s is derived from y “ 3x{p1 ` x2q. Apply cubic spline
interpolation to the data set for both the natural and linear extrapolation end conditions to
calculate the values of the function as well as its first- and second-order derivatives for values in
the given range with Δx “ 0.5, 0.25, and 0.125 intervals. Discuss the accuracy of the results.
i xi yi i xi yi i xi yi
1 ´3 ´0.9 6 ´0.5 ´1.2 11 2 1.2
2 ´2.5 ´1.0345 7 0 0 12 2.5 1.0345
3 ´2 ´1.2 8 0.5 1.2 13 3 0.9
4 ´1.5 ´1.3846 9 1 1.5
5 ´1 ´1.5 10 1.5 1.3846
CA6.4 The enthalpy of a flue gas is calculated based on experiments at temperatures between
400 and 1600 K. According to thermodynamics, the heat capacity at constant pressure is related
to cp “ Bh{BT. Use the tabulated flue gas data to calculate the enthalpy, heat capacity, and its
derivative for temperatures from 500 K to 1500 K in 50 K increments by cubic spline (linear
extrapolation end condition) interpolation.
T (K) 400 600 800 1000 1200 1400 1600
h (kJ/kg) 413 627 851 1086 1331 1586 1853
CA6.5 Consider a uniformly spaced bivariate discrete function denoted by fpq “ fpxp, yqq. Gen￾erate a bi-quadratic interpolating polynomial using central differences OrpΔxq
3
, pΔyq
3
s and write
a computer code to carry out bivariate interpolation. Use a nine-point grid, i.e., xp`1 ´ xp “ Δx,
yq`1 ´ yq “ Δy, xval ´ xp “ hx and yval ´ yq “ hy. Employ a binary search algorithm to find
pp, qq and construct the computational grid.Interpolation and Extrapolation  369
Fig. CA6.10
Hint: The Taylor series of fpx, yq aboutpxp, yqq is given as
fpxval, yvalq “ ÿ8
k“0
1
k!
„
pxval ´ xpq B
Bx ` pyval ´ yqq B
By
jk
fpxp, yqq
For partial derivatives, use the central difference formulas. Here, hx and hy denote spacings with
respect to the x´ and y´ directions, respectively.
CA6.6 The partial pressure p (mmHg) of a water-sodium hydroxide solution is tabulated below
as a function of temperature T (
˝ C) and sodium hydroxide content x (g NaOH/100g water).
Using the formulation developed in CA 6.5, write a program to compute the partial pressure and
estimate p(10˝ C,5), p(10˝ C,25), p(10˝ C,35), p(45˝ C,5), p(45˝ C,25), p(45˝ C,35), p(75˝ C,5),
p(75˝C,25), p(75˝ C,35), p(115˝ C,5), p(115˝ C,25), p(115˝C,35).
g NaOH per Temperature (˝ C)
100 g water 0 20 40 60 80 100 120
0 4.6 17.5 55.3 149.5 355.5 760 1489
10 4.2 16 50.6 137 325.5 697 1365
20 3.6 13.9 44.2 120.5 288.5 621 1225
30 2.9 11.3 36.6 101 246 537 1070
40 2.2 8.7 28.7 81 202 450 920
CA6.7 Using the furnished data for Γpxq in CA 6.2, estimate the Gamma function Γpxq and its
derivative Γ1
pxq with increments of 0.05 from x “ 0.1 to 1 using cubic spline interpolation with
(a) the natural cubic spline and (b) linear extrapolation end conditions.
CA6.8 Consider the population of a small town between 1960 and 2020.
Year 1960 1970 1980 1990 2000 2010 2020
Population (ˆ103) 13.1 17.3 21.4 23.3 25.1 26.2 27.3
Use the population data to find a sixth-degree interpolating polynomial to extrapolate the popu￾lation. Write a program to extrapolate the population for a 10-year increment and estimate the
population for the years 2030, 2040, and 2050.CHAPTER 7
Least Squares Regression
LEARNING OBJECTIVES
After completing this chapter, you should be able to
‚ understand and discuss the concept of least squares regression;
‚ apply least squares regression to polynomial models to approximate a set of
discrete data;
‚ list the advantages and disadvantages of least squares regression;
‚ explain the goodness of fit analysis and apply it to fitted models;
‚ understand and apply the linearization procedure to nonlinear models;
‚ apply multivariate least squares regression to multivariate data;
‚ employ the least squares procedure under the integral sign to obtain simple
approximate functions;
‚ utilize least squares regression to obtain an approximate solution to an over￾or under-determined system.
R EGRESSION is a method of finding the best-fit for a given set of discrete data. Least
squares is one of the most common methods used for fitting data to straight lines or
arbitrary curves. Recall that the data representing the discrete values (distribution) of a
continuum may be collected digitally with digital instruments or obtained from experimental
measurements or calculations based on experiments, as discussed in Chapters 5 and 6. The
numerical values of a continuum between any two discrete values of a uniform or non-uniform
discrete function are unknown. Moreover, experimentally derived data are not perfect, and
even when perfectly calibrated instruments are used, the measured quantities may contain
errors to some extent. Furthermore, keep in mind that instruments are not always in perfect
condition because the accuracy of all measuring instruments degrades over time due to
many factors such as wear and tear, electric or mechanical shock, or a non-ideal working
environment (temperature, humidity, pressure, magnetism, fumes, etc.). All these factors
contribute to the quality of the data.
A set of experimentally derived data for a continuum that is supposed to be linear is
depicted in Fig. 7.1a. The data points, which should ideally correspond to points on the
straight line, exhibit deviations from a linear relationship. These deviations, or scatter, are
called noise. A prediction curve can be constructed with, for instance, a Lagrange interpo￾lation polynomial to represent the data with an interpolating polynomial. Nevertheless, a
high-order Lagrange interpolating polynomial will pass through all data points, including
370 DOI: 10.1201/9781003474944-7Least Squares Regression  371
FIGURE 7.1: A data set and fitted curves: (a) linear, (b) high-order polynomial.
the data tainted by the noise, as shown in Fig. 7.1b. As a result, the prediction curve will
also contain noise, and the reliability of the predictions will be questionable. For this reason,
a method called the least squares fit is generally used to generate a function (a curve-fit)
that is free from existing noise in the raw data. This method, in addition to devising an
approximating function for interpolation purposes, is also used to find some experimentally
determined physical quantities for materials or various physical processes.
In this chapter, least squares regression and applications in science and engineering
fields are introduced. Overfitting issues, as well as the quality of fitting in terms of goodness
of fit, are discussed. The linearization procedures of nonlinear models commonly used in
practice are discussed. The method is extended to multivariate curve fits, over- or underde￾termined linear systems, as well as minimization problems under the integral sign.
7.1 POLYNOMIAL REGRESSION
One of the most commonly used models in curve fitting is the polynomial model, where a
set of discrete data is approximated by an nth-degree polynomial. In this class, the linear
regression (i.e., fitting data to a straight line) is a commonly encountered procedure in
almost every field of science and engineering.
Let px1, y1q, px2, y2q,...,pxn, ynq be a set of n discrete data points, perhaps, contam￾inated with noise. The principle of least squares regression is to minimize the difference
between the predicted and measured (or observed) values, and so we define the error as
epxq “ Y pxq ´ ypxq (7.1)
where Y pxq and ypxq, respectively, denote the predicted and observed values, and epxq is
the error referred to as the residual.
In order to obtain a model (or a prediction curve) free of noise, minimizing the sum of
the squares of residuals is considered an effective criterion; that is, the sum of the squares
of residuals is the objective function defined as
E “ ÿn
i“1
e2pxiq “ ÿn
i“1
`
Y pxiq ´ ypxiq
˘2 (7.2)
where n is the number of measurements, observations, or sample size. Notice that the
objective function amplifies the effects of large residuals while ignoring the effects of small
residuals.372  Numerical Methods for Scientists and Engineers
Now consider a best-fit model to be an mth-degree polynomial:
Y pxq “ a0 ` a1x ` a2x2 ` ... ` amxm (7.3)
Substituting Eq. (7.3) into Eq. (7.2), the objective function becomes
Epa0, a1,...,amq “ ÿn
i“1
`
a0 ` a1xi ` a2x2
i ` ... ` amxm
i ´ yi
˘2 (7.4)
where pxi, yiq denotes the set of measured or observed data, a0, a1,...,am are the coefficients
of the polynomial, also called model parameters and can be varied in order to minimize E.
The objective function is a single equation with m`1 unknowns: a0, a1, ..., am. We also
require m`1 equations to determine the unknowns. We can supply the extra equations in a
variety of ways, but keep in mind that we also wish to minimize E. Recall from the calculus
lectures that the partial derivatives of a function with respect to its independent variables
vanish at a local minimum point. Hence, we will derive m ` 1 equations by differentiating
E with respect to every unknown and then setting each equation to zero as follows:
BE
Ba0
“ 0, BE
Ba1
“ 0, BE
Ba2
“ 0, ..., BE
Bam
“ 0 (7.5)
For instance, differentiating with respect to a0 by applying the chain rule gives
BE
Ba0
“ ÿn
i“1
B
Ba0
`
a0`a1xi`a2x2
i `...`amxm
i ´ yi
˘2
“ ÿn
i“1
2 p¨ ¨ ¨ q B
Ba0
p¨ ¨ ¨ q “ 2
ÿn
i“1
`
a0`a1xi`a2x2
i `...`amxm
i ´yi
˘
“0
(7.6)
where Bp...q{Ba0 “ 1. Since the rhs of Eq. (7.6) is zero, the multiplicative factor “2” in front
of the summation sign may be dropped. So removing the parenthesis and rearranging yields
n a0 `
´ ÿn
i“1
xi
¯
a1 `
´ ÿn
i“1
x2
i
¯
a2 ` ... `
´ ÿn
i“1
xm
i
¯
am “ ÿn
i“1
yi (7.7)
Similarly, differentiation with respect to a1 leads to
BE
Ba1
“ ÿn
i“1
2 p¨ ¨ ¨ q B
Ba1
p¨ ¨ ¨ q“2
ÿn
i“1
`
a0`a1xi`a2x2
i `...`amxm
i ´yi
˘
xi “ 0 (7.8)
where Bp...q{Ba1 “ xi. After algebraic manipulations, Eq. (7.8) leads to
´ ÿn
i“1
xi
¯
a0`
´ ÿn
i“1
x2
i
¯
a1`
´ ÿn
i“1
x3
i
¯
a2`...`
´ ÿn
i“1
xm`1
i
¯
am “ ÿn
i“1
xiyi (7.9)
Likewise, evaluating the partial derivatives of E with respect to the other unknown coef￾ficients and setting them to zero completes the m ` 1 equations. The collection of these
equations altogether forms a system of linear equations.
The system of linear equations for the unknown coefficients is expressed as
»
—
—
—
—
—
–
n ř xi
ř x2
i ... ř xm
ř
i
xi
ř x2
i
ř x3
i ... ř xm`1
ř
i
x2
i
ř x3
i
ř x4
i ... ř xm`2
i
.
.
. .
.
. .
.
. ... .
.
. ř xm
i
ř xm`1
i
ř xm`2
i ... ř x2m
i
fi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
–
a0
a1
a2
.
.
.
am
fi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
–
ř
ř
yi
ř
xiyi
x2
i yi
.
.
. ř xm
i yi
fi
ffi
ffi
ffi
ffi
ffi
fl
(7.10)Least Squares Regression  373
FIGURE 7.2: Correlatability of the observed to a linear model; (a) no correlation, (b)
weak correlation, (c) fair correlation, (d) strong correlation.
where the summation signs encompass all n.
The resulting system of linear equations for m “ 1, 2, or 3 may be solved by
the Cramer’s rule. For high-degree polynomial models, the linear system tends to be ill￾conditioned. In many cases, the degree of a polynomial is severely restricted due to round￾ing errors. For example, polynomial models of m ą 7 generally produce absurd results
using single-precision arithmetic. It is therefore recommended to use direct solution al￾gorithms with pivoting and high-precision arithmetic to overcome the adverse effects of
ill-conditioning (see Chapter 2).
7.1.1 DETERMINING REGRESSION MODEL
Consider a set of discrete data where xi and yi are the independent and dependent (ob￾served) variables, respectively. In a set of data, a correlation between the independent
variable and the observed variables may or may not exist. If a functional relationship can
be established between two or more variables, it can be said that there is a correlation
between the dependent and independent variables.
The correlatability of a linear model to four different sets of observed data is depicted
in Fig. 7.2. In Fig. 7.2a, the observed data spreads out over a wide range, indicating “no
correlation” between x and y. In Fig. 7.2b, the data is gathered around a straight line with
large residuals, indicating “weak correlation.” As depicted in Fig. 7.2c and d, the tighter the
observed data points are around the model, the stronger the correlation.
A regression model (or best-fit curve) is best determined after inspecting the observed
data distribution. If the observed data is suitable to be fitted into a polynomial, then
we must address the following question: what should the degree of the polynomial be? To
answer this question, a logical approach is to plot the observed data and determine the
best-fit model (or the degree of the polynomial) by visual inspection. The two observed
data sets are illustrated in Fig. 7.3. The visual inspection reveals that the data set in Fig.374  Numerical Methods for Scientists and Engineers
FIGURE 7.3: Data sets and suitable models; (a) quadratic, (b) cubic model.
7.3a is suitable for a quadratic model (or a higher-degree polynomial). Likewise, the set of
data in Fig. 7.3b is a candidate for a cubic polynomial (or a sinusoidal) model. One should
experiment with applicable models that give optimal goodness of fit.
The most important and critical factor in adopting a best-fit model should include
consideration of the physical event and the relationship suggested by the theory for which
the data was collected. For example, consider the experimental determination of the den￾sity of a substance using a batch of samples of different shapes and sizes. The density of
a substance may be estimated from a single measurement using a single sample. While
measuring the mass of a sample is fairly straightforward, measuring the volume in this ex￾ample essentially conceals a major challenge. To determine the volume, we can make use of
Archimedes’ principle, which states that “the volume of displaced fluid is equivalent to the
volume of an object fully immersed in a fluid.” Nonetheless, the volume of the substance
cannot be precisely measured due to the presence of inhomogeneities, such as pores and
cracks or trapped air bubbles within, etc., which introduce uncertainties. For this reason,
multiple measurements (using samples of different sizes) should be performed to minimize
measurement errors. Considering the physics of this phenomenon, the theory suggests a
linear relationship between the mass and the volume of a substance, m “ ρV , where the
density is the slope of the straight line. Hence, when the mass and volume data of the sam￾ples are plotted, the data should ideally be aligned exactly on a straight line. In reality, the
measurements will most likely depict deviations that may give a discrete mass-volume rela￾tionship that does not look linear, as seen in Fig. 7.4. The deviations of the observed values
from linearity are due to the so-called “noise” caused by uncertainties (pores, cracks, etc.)
in the measurements. Hence, even though the data set appears to conform to a quadratic
or cubic relationship (depicted with a dashed curve), this observation is inconsistent with
the theory, and one should not be tempted to use a model other than a linear model.
FIGURE 7.4: Measured mass-volume relationship of samples of a substance.Least Squares Regression  375
Zero-Intercept Model. In cases where the data is theoretically
related by the y“mx relationship, the regression line is forced to
intercept the origin. Hence, the objective function is chosen as
Epmq “ ÿn
i“1
pyi ´ mxiq
2
with m as the only (unknown) model parameter. Setting BE{Bm “
0 yields
m “ ÿn
i“1
xiyi
O ÿn
i“1
x2
i
7.1.2 GOODNESS OF FIT
After deciding on a suitable model, the model constants are determined by applying the
least squares method. Next, we seek answers to the following questions: How well does this
best model represent the data? How good will the estimates from this model be? Can the
model be further improved? and so on.
A plot of the model along with the data, as shown in Fig. 7.3, provides a visual aid re￾garding the suitability of the model to the data. However, a visual aid alone is not sufficient
and provides a qualitative assessment only; a measurable quantitative criterion is also re￾quired to measure the goodness of fit. We know that there is a residual (i.e., ei “ Yi´yi ‰ 0)
for every point that does not correspond exactly to the best curve. The sum of the squares
of the residuals (SSR) of the observed data will be minimal owing to the minimization re￾quirement—Eq. (7.5). Under ideal conditions, Eq. (7.4) should be zero for any data that is
completely free of noise.
The sum of the squares of the mean deviation (SSMD), which is a measure of how
much the data deviates from the mean value, is computed from
S “ ÿn
i“1
pyi ´ y¯q
2 (7.11)
where ¯y is the arithmetic mean of the data set. Unless all data are distributed along a
horizontal line (yi “constant), S will not be zero even if an ideal curve fit is found.
Residual Plots: An assessment of the usefulness of a curve fit should start by exam￾ining the residuals via the residual plots that reveal the effectiveness and reliability of the
model adopted. Residuals, ei “ Yi ´ yi, provide qualitative information about the goodness
of the fit. A residual plot is obtained simply by placing the abscissas (xi) on the horizontal
axis and the residuals (ei’s) on the vertical axis. As an example, a data set along with its
best fit and the residual plot corresponding to the best-fit line are respectively shown in
Fig. 7.5a and b. Note that the observed data points are gathered along the best-fit line.
If the residuals are small and unbiased, we can claim that the model is suitable for this
data set. In this context, what we mean by “unbiased” is that the predicted values are not
systematically too high or too low within the regression range.
Before examining quantitative metrics of the goodness-of-fit, one should qualitatively
assess the residual plots because they can reveal a biased model far more effectively. A
biased model cannot be trusted, and a new model should be proposed.376  Numerical Methods for Scientists and Engineers
FIGURE 7.5: (a) Observed data along with the best fit, (b) residual plot.
A residual plot of a linear regression depicts certain features that hint at the suitability
or appropriateness of the regression. Some examples of residual plots that might be encoun￾tered as a result of a linear regression procedure are depicted in Fig. 7.6. The residual data
is termed homoscedastic if the variance is constant or depicts homogeneous scatter. For
unbiased and homoscedastic data (seen in Fig. 7.6a), the residuals are randomly scattered
around the e “ 0 line, and the average of the residuals is zero in each thin vertical band,
xi ´ δ ă x ă xi ` δ where δ is a small number. The residuals form the same size horizontal
bands below and above the e “ 0 line, indicating that the variances (or standard deviations)
of the error are the same all across the x-axis. Also note that there are no outliers in the
residual distribution; in other words, no one residual “stands out” from the random pattern
of the residuals. The residuals in the biased and homoscedastic cases are illustrated in Fig.
7.6b yield a linear pattern, which is most likely due to an independent variable that was
overlooked or not taken into account in the experiment. The residuals in Fig. 7.6c forming
a quadratic pattern are most likely indicative of a nonlinear relationship; in other words,
a linear relationship is forced onto nonlinear data. A linearization (i.e., transformation) of
the data covered in Section 7.3 might help mitigate this type of problem. In heteroscedastic
data, the variance of the residuals increases or decreases along the horizontal axis. As illus￾trated in Fig. 7.6d, in an unbiased heteroscedastic case, while the average of the residuals
yields zero for every thin vertical band, the variance increases from the left of the plot to
the right. Biased and heteroscedastic linear and quadratic patterns are shown in Fig. 7.6e
and f are also signs of trouble owing to using an unsuitable regression model.
In multi-regressions, plotting the residuals with respect to each
independent variable may help determine the kind of relationship
between the independent and dependent variables. However, in
general, the knowledge, expertise, and experience of the analyst
in the field are the most valuable assets that contribute to finding
relationships between variables and developing a suitable model.
In multi-regressions, sometimes the bias and/or heteroscedasticity of the residuals can￾not be mitigated. This is largely due to implementing an unsuitable regression model that
does not take into account one or more of the “critical” parameters or variables. For in￾stance, modeling the development of a storm as a function of pressure and wind speed may
reveal that the temperature, humidity, or both (additional variables) need to be taken into
account as well.
R-Squared: R-Squared is one of the quantitative metrics for assessing the goodness
of a regression. The difference between E (the sum of the squares of the residuals of the
observed data) and S (the sum of the squares of the mean deviation) is accepted as aLeast Squares Regression  377
FIGURE 7.6: Possible residual patterns: (a) unbiased and homoscedastic, (b), (c) biased and ho￾moscedastic, (d) unbiased and heteroscedastic, (e), (f) biased and heteroscedastic.
measure of the accuracy of the curve fit. Because this difference is scale-dependent, most
commonly a normalized form of the difference, obtained by dividing by S, is used:
r2 “ pS ´ Eq{S “ 1 ´ E{S (7.12)
where r2 is referred to as the r-squared or coefficient of determination, while r is termed
the correlation coefficient. In practice, r2 is more commonly reported instead of r.
Ideally, for any regression, r-squared (or r) should be calculated and interpreted because
it provides a quantitative measure for the strength of the relationship between the dependent
and independent variable(s). Notice that r-squared approaches “1” when E is small (or S
is large), hinting that the model explains a large fraction of the variation in the data. In
other words, when the observed and predicted values are in perfect agreement, the sum of
the residuals becomes zero, yielding r2 “ 1, meaning that the regression model explains
100% of the variability of the data. When the data, as shown in Fig. 7.2a, is dispersed, the
r-squared approaches zero because E Ñ S, in which case the variability of the data cannot
be explained by the model. The variation of r-squared in connection with the distribution
of data is illustrated in Fig. 7.2b, c, and d. Notice that as r-squared approaches one, the
data cluster more tightly around the best-fit. Also, r-squared represents the percentage of
the data close to the fit; for instance, r2 “ 0.81 means 81% of the data are represented by
the best fit and 19% are not.
EXAMPLE 7.1: Application of Linear Regression
The effect of salinity of water x (parts per thousand) in an agricultural land on
the electrical conductivity y (miliSiemens per cm) of the soil was investigated. The
results of the salinity and conductivity measurements of samples taken from five
locations in the field are tabulated below. Assuming that the data can be described378  Numerical Methods for Scientists and Engineers
by a linear regression model, apply least squares regression to determine the best-fit
parameters and discuss the goodness of the fit.
xi (ppt) 2 6 7 10 14
yi (mS/cm) 3 4 6 7 8
SOLUTION:
The model is Y pxq “ a0 ` a1x, and the model parameters are a0 and a1, respec￾tively. The objective function is defined as the sum of the squares of the residuals
as
Epa0, a1q “ ÿ
5
i“1
pa0 ` a1xi ´ yiq
2
Differentiating the objective function with respect to the model parameters a0 and
a1 gives a 2 ˆ 2 system of linear equations. However, essentially having to perform
these operations, we will make use of Eq. (7.10) by including the first two rows and
two columns, leading to
« n ř xi
ř xi
ř x2
i
ff «a0
a1
ff
“
« ř yi
ř xiyi
ff
The coefficient matrix and the rhs of this system require Σxi, Σx2
i , Σyi, and
Σxiyi quantities. The original data and pertinent calculational details are presented
in Table 7.1.
TABLE 7.1
i xi yi x2
i xiyi pyi ´ y¯q
2 pyi ´ Yiq
2
1 2 3 4 6 6.76 0.00198
2 6 4 36 24 2.56 0.65110
3 7 6 49 42 0.16 0.56626
4 10 7 100 70 1.96 0.18553
5 14 8 196 112 5.76 0.10998
Σ 39 28 385 254 17.2 1.51485
The values typed in bold numbers in the last row of the table are the sum of the values
in that column. Substituting the computed quantities into the matrix equation, we
obtain
« 5 39
39 385ff «a0
a1
ff
“
« 28
254ff
which yields pa0, a1q“p2.16337, 0.44059q. The squares of both the deviations from
the mean value and the residues are also presented on the 6th and 7th columns of
Table 7.1, respectively. Using the tabulated data, we find
S “ ÿ
5
i“1
pyi ´ 5.6q
2 “ 17.20, E “ ÿ
5
i“1
pY pxiq ´ yiq
2 “ 1.51485
Finally, the r-squared value is obtained as r2 “ pS ´ Eq{S “ 0.912.Least Squares Regression  379
FIGURE 7.7
Discussion: It is important to know quantitative measurements of model coefficients
and model accuracy, and we must also understand visual approaches to evaluate our
model. In this regard, the distribution of the measured data as well as the best￾fit model are comparatively depicted in Fig. 7.7. It is observed that the measured
data spreads in close proximity to the (linear) model, and 91.2% of the data is well
represented by the linear model. The model parameters, which are the slope and
intercept, are found to be 0.44059 and 2.16337, respectively.
The correlation coefficient for a linear model, Y pxq “ a0 ` a1x, is reduced to
r “ n ř xiyi ´ př xiq př yiq
b
n ř x2
i ´ př xiq
2
b
n ř y2
i ´ př yiq
2 (7.13)
Adjusted R-Squared: A danger of relying solely on r-squared is that it can lead
an analyst to misconclusions, especially when assessing a set with very few data points.
Similarly, using a large data set or adding extra variables to the data set may also lead
to illusive conclusions. In other words, when more data is added, r2 does not decrease
but rather increases, giving a false sense of confidence. Similar arguments can be made
for multivariable regression models as well. For instance, when an independent variable
is added to the model, r2 increases, and an increase in r2 approaching unity can also be
misleading for the analyst. In this context, variables having very little or no effect on the
main variable(s) should be excluded from the model to reduce model complexity and to
build understandable and interpretable models. In this regard, to penalize adding extra
variables, the adjusted r2 is defined as
r2
adj “ 1 ´ p1 ´ r2q
ˆ n ´ 1
n ´ k ´ 1
˙
(7.14)
where n is the number of data points, k is the number of variables, including the model
constants. When p1 ´ r2q, i.e., E{S, is multiplied by pn ´ 1q{pn ´ 1 ´ kq , the new factor
will always be smaller than r2 since 1 ` k{pn ´ 1 ´ kq ą 1. This way, adding extra data and
variables is penalized, and a new index of compliance is established.
RMSE: Another quantitative metric for measuring the quality of the model fit is
the rmse (abbreviation for root mean square error). A best-fit curve, Y pxq, estimates a
prediction for any x, while the rmse measures the spread of y’s. It is defined as the square380  Numerical Methods for Scientists and Engineers
FIGURE 7.8: Variation of 68% and 95% rmse bands (CI: confidence interval) with (a) r2 “ 0.653,
(b) r2 “ 0.815, and (c) r2 “ 0.967.
root of the arithmetic mean of the sum of the squares of the residuals and is given as
rmse =
gffe
1
n
ÿn
i“1
pYi ´ yiq
2 “
cE
n (7.15)
where n is the number of measurements (or observations).
An alternative definition of rmse is the standard deviation of residuals. Using Eq. (7.11),
the standard deviation of the measured (or observed) data set is computed as
σ “
gffe
1
n
ÿn
i“1
pyi ´ y¯q
2 “
cS
n (7.16)
and the variance, denoted by s, is obtained from s “ σ2.
The standard deviation, as a measure of dispersion, is a quantitative metric indicating
how spread out the measured (or observed) points are. Using Eq. (7.12), the rmse can be
written as
E
n “ p1 ´ r2q
S
n
or rmse “
cE
n “ a1 ´ r2 σ (7.17)
which can be obtained in terms of r2 and σ. For example, rmse “ 0 (i.e., r “ 1) indicates
that the measured data are 100% represented by the fitted (predicted) model. Since rmse
is measured on the same scale and in the same units as y, 68% of y-values should be within
˘rmse. In Fig. 7.8, the data set is represented by confidence intervals of 68% and 95%
(˘2 ˆ rmse) around the best-fit. Notice that the band shrinks as r-squared increases.
Predicted-Observed (PO) Data Plots. Predicted versus observed (PO) plots do
provide qualitative and quantitative information about the regression models. When assess￾ing the predictions of multi-regression models, the use of residual plots is not that practical.
So instead, the predicted-observed values can be plotted by placing the predicted values
(Y ’s) on the horizontal axis and the observed values (y’s) on the vertical axis. The slope
(m) and intercept (a) of the PO line, Y “ a ` my, describe the consistency and the model
bias, respectively. In Fig. 7.9, the results of a linear and a nonlinear regression model, as well
as their PO plots, are depicted. It is evident that for identical observations or predictions,
the resulting PO plot is perfectly aligned on the diagonal (i.e., y “ Y line and rmse “ 0).
Likewise, having data points close to the diagonal line (rmse «0) indicates a reasonably
good fit for the overall data. The data points closely spread about the diagonal line, as seen
in Fig. 7.9a, are an indication that the predicted and observed data are correlated. Any
nonlinear trends in the plots indicate no correlation. The unit slope validates the linear re￾lationship between y and Y , i.e., the linear regression model is suitable for the data. On theLeast Squares Regression  381
FIGURE 7.9: (a) Linear and (b) nonlinear fits with data and PO plots.
other hand, the deviation of the slope from unity reflects a relative scaling factor between
the two sets. A non-zero intercept indicates that some data are shifted relative to others by
a constant offset. The model bias (shift) is determined (or quantified) by regressing the PO
values onto a straight line and comparing the slope and intercept of the best-fit line against
the diagonal line. This way, any bias in the data can be identified. Notice that, in Fig. 7.9b,
a nonlinear fit leads to a relatively small shift («0.727) to the right.
When linearly correlated data are fitted to a line, the r-squared value of the x ´ Y
and Y ´ y regressions are the same (r “ 0.9115) for Fig. 7.9a. However, this is not the case
for nonlinear regression models, as noted in Fig. 7.9b (r2 “ 0.95 for x ´ Y and 0.8965 for
Y ´y). The r-squared, the slope and intercept of the PO line, and so on provide parameters
for judging and building confidence in the model’s performance.
EXAMPLE 7.2: Quadratic model and goodness of fit
Experimental results for the efficiency η (%) of a turbine-type centrifugal pump
versus the flowrate Q (m3/min) are presented below:
Q pm3{ minq 1.1 2.3 4.3 6.6 9.1
η p%q 26 57 78 90 42
Assuming the data is suitable for a quadratic regression model, apply least squares
regression to determine the best-fit parameters and discuss the goodness of the fit.
SOLUTION:
This regression model, ηpQq “ a0 ` a1Q ` a2Q2, has three model parameters,
which are obtained by solving the system of linear equations.382  Numerical Methods for Scientists and Engineers
TABLE 7.2
i Qi ηi Q2
i Q3
i Q4
i Qiηi Q2
i ηi
1 1.1 26 1.21 1.331 1.4641 28.6 31.46
2 2.3 57 5.29 12.167 27.984 131.1 301.53
3 4.3 78 18.49 79.507 341.88 335.4 1442.22
4 6.6 90 43.56 287.496 1897.5 594 3920.4
5 9.1 42 82.81 753.571 6857.5 382.2 3478.02
Σ 23.4 293 151.36 1134.07 9126.3 1471.3 9173.63
The linear system is simply constructed by replacing (x,y) with (Q, η) in the
first three rows and three columns of Eq. (7.10) yielding
»
–
n ΣQi ΣQ2
i
ΣQi ΣQ2
i ΣQ3
i
ΣQ2
i ΣQ3
i ΣQ4
i
fi
fl
»
–
a0
a1
a2
fi
fl “
»
–
Σηi
ΣQiηi
ΣQ2
i ηi
fi
fl
The computational details of the terms required to construct the coefficient ma￾trix are summarized in Table 7.2. Substituting the pertinent numerical sums in the
last row of the table, the linear system becomes
»
–
5 23.4 151.36
23.4 151.36 1134.072
151.36 1134.072 9126.298
fi
fl
»
–
a0
a1
a2
fi
fl “
»
–
293
1471.3
9173.63
fi
fl
which, for the model parameters, yields a0 “ ´10.4986, a1 “ 36.3711, and a2 “
´3.34032. And computing E and S results in
E “ ÿ
5
i“1
pηp Qiq ´ ηiq
2 “ 79.1432, S “ ÿ
5
i“1
pηi ´ 58.6q
2 “ 2703.2
A graphical depiction of the observed data as well as the best fit (prediction)
curve is presented in Fig. 7.10. In Figure 7.11, the residual and rmse confidence
interval plots are presented. Using E and S, we obtain r2 “ 0.97072, which indicates
that 97.07% of the observed data are explained by the best-fit curve. This result
supports the conclusion that the quadratic equation exhibits very good agreement
between the observed and predicted data, as is also evident from Fig. 7.10. On the
other hand, the residual plot (Fig. 7.11a) has two points with large residuals (« ˘6)
while the rmse plot (rmse “ ap79.1432{5q « 4) reveals that all these points lie
within 68% CI, i.e., Y pxiq ˘ rmse.
FIGURE 7.10Least Squares Regression  383
least squares Regression
‚ least squares regression is an effective tool for numerical analysts;
‚ Estimated model parameters are the optimal values;
‚ When a model compatible with the theory is used, a very good result
can be obtained even with a relatively small amount of data;
‚ Regression allows important physical parameters (slope, intercept,
etc.) to be determined;
‚ The theory of computing confidence intervals, predictions, goodness
of fit, and so on is well established;
‚ Numerous software programs are available, allowing the use of a broad
range of single- or multi-regression models.
‚ Regression models can justifiably be used within the range of data;
hence, they have poor extrapolation properties;
‚ The procedure can be sensitive to outliers or some data anomalies;
‚ The data is assumed to be suitable for the adopted regression model;
‚ Overfitting can be a major issue when using polynomial models;
‚ Nonlinear models may require iterative techniques to solve the system
of nonlinear equations.
FIGURE 7.11: (a) residual plot (b) Confidence Interval (CI) plot.
Discussion: A regression model is trained to find model parameters that minimize
the difference between predicted values and observed (actual) values in its data. The
qualitative and quantitative tools should be exploited to the fullest to make sure the
model can be safely used to make predictions on new, unseen data.
A pseudomodule, POLYNOMIAL_FIT, calculating the model parameters and r-squared
of an mth-degree polynomial model by least squares regression is presented in Pseudocode
7.1. The module requires the number of data points (n), the degree of the polynomial model
(m), and the set of data points (xi, yi, i “ 1, 2,...,n) as input. The module outputs are the
model parameters a, the sum of the squares of the residuals (E), the sum of the squares of
the mean deviation (S), and the r-squared. In the first block of loops (i, j “ 1,2,...,m`1),
the elements of the coefficient matrix cij are constructed. In the second block of loops
(i “ 1, 2,...,m ` 1), the rhs elements, bi, are obtained using Eq. (7.10). As a precaution
against ill-conditioning, the resulting system of linear equations is solved using the Gauss
elimination with pivoting algorithm, Pseudocode 2.10, to minimize the adverse effects of
the roundoff errors.384  Numerical Methods for Scientists and Engineers
Pseudocode 7.1
Module POLYNOMIAL_FIT (n, m, x, y, a, E, S, r2)
\ DESCRIPTION: A pseudomodule fitting data to an mth degree polynomial.
\ USES:
\ GAUSS_ELIMINATION_P:: Gauss elimination module, Pseudocode 2.10
Declare: xn, yn, cm`1,m`1, bm`1, am`1 \ Declare array variables
For “
i “ 1, m ` 1
‰ \ Loop i: Sweep rows from top to bottom
For “
j “ 1, m ` 1
‰ \ Loop j: Sweep a row from left to right
cij Ð 0 \ Initialize accumulator
For “
k “ 1, n‰ \ Loop k: Accumulator loop
p Ð i ` j ´ 2 \ Find exponent p
cij Ð cij ` xp
k \ Add xp
k to accumulator
End For
End For
End For
For “
i “ 1, m ` 1
‰ \ Loop i: Construct the rhs
bi Ð 0 \ Initialize accumulator
For “
k “ 1, n‰ \ Loop k: Accumlator loop
p Ð i ´ 1 \ Find exponent p
bi Ð bi ` yk ˚ xp
k \ Add yk ˚ xp
k to accumulator
End For
End For
\ Solve Cr “ b using Gauss elimination with pivoting
GAUSS_ELIMINATION_Ppm ` 1, C, b, aq \ System is solved by Pseudocode 2.10
yavg Ð 0 \ Initialize accumulator, yavg
For “
k “ 1, n‰
yavg Ð yavg ` yk \ Accumulate yk’s
End For
yavg Ð yavg{n \ Find average y
E Ð 0; S Ð 0 \ Initialize E and S
For “
k “ 1, n‰ \ Loop k: Loop to find E and S
yk Ð a1 \ Initialize yk as yk Ð a1
For “
j “ 1, m‰ \ Loop j: Compute predictons, Yk’s
yk Ð yk ` aj`1 ˚ xj
k \ Add aj`1 ˚ xj
k’s to accumulator
End For
S Ð S ` pyk ´ yavgq2 \ Accumulate squares of deviations from yavg
E Ð E ` pyk ´ ykq2 \ Accumulate squares of residues
End For
r2 Ð 1 ´ E{S \ Find r-squared
End Module POLYNOMIAL_FIT
7.1.3 OVERFITTING
When a polynomial is chosen as a regression model, it is tempting to increase the degree of
the polynomial to improve the model and its prediction ability. As the degree polynomial
(assuming the number of data points is sufficient) is gradually increased, the residuals
decrease and r-squared improves until the observed and predicted data become identical,
i.e., r2 Ñ 1. One can increase the degree of a polynomial as long as there is a statistically
significant reduction in the variance computed by σ2 “ Σe2
i {pn ´ m ´ 1q, where ei is theLeast Squares Regression  385
FIGURE 7.12: Effects of increasing the degree of polynomial model on the fit.
individual residual, n and m are the number of data points and the degree of the polynomial,
respectively. Note that if the data is corrupted with significant noise, the best-fit polynomials
tend to contain the noise as well.
The effect of increasing the degree of the polynomial on the least squares fit is illustrated
in Fig. 7.12. For linear and quadratic models, as shown in Fig. 7.12a and b, the residuals
are noticeably large. For n “ 3, 4, and 5, the polynomials approach closer to the observed
data points, except for a few, and the residuals decrease further, as seen in Fig. 7.12c, d,
and e. However, the 6th-degree polynomial model, shown in Fig. 7.12f, perfectly satisfies the
observed data points but depicts large oscillations leading to abnormal deviations. It should
be recalled from Chapter 6 that oscillations cannot be mitigated simply by increasing the
degree of the approximating polynomial. Doing so, on the contrary, increases and spreads
the magnitude of the oscillations over the whole data range. Thus, one should not be fooled
by the improvements attained with r2 (i.e., r2 Ñ 1). To avoid falling into this pitfall, the
visual inspection tools should also be utilized to the fullest.
The linear systems resulting from implementing polynomial re￾gression models can be ill-conditioned, and the round-off errors
also tend to distort the solution. Polynomials up to the third or
fourth degree can be managed relatively without a problem; how￾ever, polynomial models of degrees higher than four are rarely
used.
7.2 TRANSFORMATION OF VARIABLES
In data analysis, a transformation is frequently applied to a set of data to obtain a linear or
polynomial model. Transforming variables in regression is a technique applied to improve the
linear model (and the data) and to meet the underlying assumptions of regression analysis.
A successful transformation model can better satisfy the assumptions of normality, linearity,
and homoscedasticity.386  Numerical Methods for Scientists and Engineers
TABLE 7.3: Typical transformation examples.
Model Transformation
?y “ A ` B x Y “ ?y, X “ x,
y “ A ` B ?x Y “ y, X “ ?x,
log y “ A ` B x Y “ log y, X “ x
log y “ A ` B log x Y “ log y, X “ log x
log y “ A ` B{x Y “ log y, X “ 1{x,
A transformation is applied to either dependent or independent variables, or both.
This process involves changing the variable(s) of the data with a suitable mathematical
function of an independent or dependent variable so that the final relationship between the
transformed variables is linear. For instance, we may change the measurement scales of a
set of data by replacing variable x with ex or ln x, or similarly replacing variable y with
log y or 1{y, and so on.
A transformation changes the shape of a distribution or relationship. In this process,
the model parameters are exactly conserved; hence, the regression between the dependent
and independent variables remains the same. For example, consider a model given by ln y “
A ` B?x. Redefining a set of new variables as Y “ ln y and X “ ?x, we transform the
model to a linear relationship: Y “ A ` BX. Note that the model parameters (A and B)
have been preserved in the new model. Some typical transformation examples are presented
in Table 7.3.
In univariate distributions, transformations are basically applied for the following rea￾sons: (i) obtain linear relationships, (ii) reduce skewness, and (iii) achieve convenience. We
have already covered ways and means of achieving linear regression models. In general, we
also seek to reduce the skewness of the distribution because it is easier to treat and interpret
a non-skewed distribution. For this purpose, skewness reduction can be achieved by taking
the square, cubic, etc., roots, or powers, or logarithms, or reciprocals, and so on. A transfor￾mation of data in percentages to ratios, or in radians to degree scales, or in non-dimensional
quantities (T{Tr, U{Ur where quantities with the r subscript denote a reference value), and
so on, may be more convenient. This type of transformation does not affect the shape of a
distribution.
Confidence intervals computed on transformed variables should be
recomputed by transforming back to the original units of interest.
Models (goodness of fit, etc.) can only be compared to the original
unit of the dependent variable.
7.3 LINEARIZATION OF NON-LINEAR MODELS
Nonlinear regression is a very powerful alternative to linear regression in that it provides
greater flexibility in fitting a curve (i.e., a nonlinear function). Besides, most physical events
in science and engineering are nonlinear in nature, and the observed nonlinear data should
satisfy pertinent theoretical relationships. Nonetheless, applying the least squares regression
procedure to a nonlinear model leads to a nonlinear system of equations (the nonlinear least
squares problem) that can be solved by iterative methods (see Chapter 3 and Chapter 4).Least Squares Regression  387
FIGURE 7.13: Influence of b on (a) power, (b) exponential, (c) saturation models.
However, the nature of equations and the number of variables can make the numerical
solution very difficult and/or dependent on the initial guess.
A scatter plot of a set of data may reveal a nonlinear distribution, in which case
transforming the variable(s) may become necessary to facilitate an easy solution while
avoiding the solution of a nonlinear system of equations. Thankfully, some of the most
common nonlinear models can be easily linearized or transformed; in other words, it is
possible to “transform” the original data to make it preferably “linear,” which then allows
the linear regression model to be employed. These models will be referred to as transformable
or linearizable models. The linearizable models are classified into three categories.
Power Model: Some physical quantities encountered in a wide range of science and
engineering fields are related by y “ a xb. The magnitude of b determines the rate of change.
The model results in growth of the dependent variable y (b ą 0) or decay (b ă 0) (see Fig.
7.13a). This model is also suitable for cases where the underlying model is unknown.
Exponential Growth/Exponential Decay Model: A significant number of phys￾ical laws are also represented by the exponential form y “ a ebx (b ‰ 0). The exponential
model, like the power model, yields exponential growth (b ą 0) or exponential decay (b ă 0)
in the independent variable (see Fig. 7.13b).
Saturated Growth/Saturated Decay Rate Model: This model can be expressed
as either y “ ax{px`bq or y “ a{px`bq. Both models generally observed in chemical reaction
kinetics depict growth or decay rate under limiting conditions in which the dependent
variable levels off (or saturates) as x Ñ 8 (ax{px`bq Ñ a, a{px`bq Ñ 0) (see Fig. 7.13c).388  Numerical Methods for Scientists and Engineers
Pseudocode 7.2
Module LINEARIZE_REGRESS (n, x, y, model, a, b, E, S, r2)
\ DESCRIPTION: A pseudomodule employing least squares regression to obtain
\ the nonlinear best fit parameters for the model y “ axb.
Declare: xn, yn, c22, b2, Xn, Yn
X Ð lnpxq; Y Ð lnpyq \ Linearize data set for model “ 1
For “
i “ 1, 2
‰ \ Loop i: Construct 2ˆ2 linear system
For “
j “ 1, 2
‰ \ Loop j : Sweep row from left to right
c i,j Ð 0 \ Initialize accumulator cij
For “
k “ 1, n‰ \ Accumulato loop k
c ij Ð c ij ` Xi`j´2
k \ Accumulate Xp
k ’s
End For
End For
b i Ð 0 \ Initialize accumulator for rhs
For “
k “ 1, n‰ \ Loop k: Construct the rhs
bi Ð bi ` Xi´1
k ˚ Yk \ Accumulate Xp ˚ Y ’s
End For
End For
dd Ð c11 ˚ c22 ´ c21 ˚ c12 \ Use Cramer’s rule to solve 2ˆ2 system
d1 Ð b1 ˚ c22 ´ b2 ˚ c12
d2 Ð b2 ˚ c11 ´ b1 ˚ c21
A Ð d1{dd; m Ð d2{dd \ Find A and m, linearized case.
a Ð eA; b Ð m \ Find a and b, nonlinear case.
yavg Ð 0 \ Initialize average Y
For “
k “ 1, n‰ \ Loop k: Calculate average Y
yavg Ð yavg ` yk \ Accumulate yk’s
End For
yavg Ð yavg{n \ Find yavg
E Ð 0; S Ð 0 \ Initialize E and S
For “
k “ 1, n‰ \ Loop k: Accumulator for E and S
S Ð S ` pyk ´ yavgq
2 \ Accumulate mean deviations
E Ð E ` pa ˚ xb
k ´ ykq
2 \ Accumulate residuals
End For
r2 Ð 1 ´ E{S \ Calculate r-squared
End Module LINEARIZE_REGRESS
Linearization of the “power” and “exponential” models is carried out by taking the
natural logarithm of the model, resulting in linear relationships in terms of ln y versus ln x,
and ln y versus x, respectively. Defining new variables as X “ x or X “ ln x and Y “ ln y
(and model parameters as A “ ln a and m “ b), the nonlinear model is transformed to
Y “ A ` mX.
On the other hand, when saturated growth or saturated decay rate models are linearized
by inverting the original data and defining X “ x or 1{x and Y “ 1{y, these models are
reduced to the same linear relationship. The model parameters are then found once the linear
least squares procedure is employed on the transformed data set, pXi, Yiq. Henceforth, the
original model parameters can be computed from the transformed model parameters. The
linearization procedure of common nonlinear models is presented in Table 7.4.Least Squares Regression  389
TABLE 7.4: Linearization of common nonlinear models.
Model (Curve) Linearization, Y “ A ` mX
y “ a xb ln y “ ln a ` b ln x, Y “ ln y, X “ ln x, A “ ln a, m “ b
y “ a ebx ln y “ ln a ` b x, Y “ ln y, X “ x, A “ ln a, m “ b
y “ ax
b ` x
1
y “ b ` x
ax
, Y “ 1
y
, X “ 1
x
, A “ 1
a
, m “ b
a
y “ a
b ` x
1
y “ b ` x
a
, Y “ 1
y
, X “ x, A “ b
a
, m “ 1
a
In most cases, the original nonlinear model parameters (a and b) have physical signifi￾cance. The objective of linear regression is sometimes to determine these physical param￾eters. For instance, Nptq “ N0 e´λt represents the decay of a radioactive isotope, where
λ “ lnp2q{t1{2 is the decay constant and t1{2 is the half-life of the isotope. The decay con￾stant appears in the transformed linear model parameter as m “ ´λ. To determine the
half-life of any radioactive isotope, the radioactivity measurements of the isotope (decay
counts per unit time) are carried out, and time versus decay rate data is obtained pti, Niq.
When the data is fitted to the linearized (exponential growth) model, the half-life of the
isotope is obtained from t1{2 “ lnp2q{λ “ ´ lnp2q{m.
A pseudomodule, LINEARIZE_REGRESS, performing the nonlinear fit to the model
y “ axb is given in Pseudocode 7.2. As input, the module requires the number of data
points (n), the data (xi, yi, i “ 1, 2,...,n), and the model flag (model) for selecting the
nonlinear model. The module outputs are the model parameters (a0 and b0), the sum of
the squares of residuals (E), the sum of the squares of mean deviation (S), and r-squared
(r2). The model flag is used for the model (indicating the type of linearization procedure)
to be applied, as given in Table 7.4. To save space, however, the implementation of only
y “ axb (model “ 1 case) is shown here, but the module can be generalized to include the
other linearizable models without much difficulty. First, the data is linearized as Xk “ ln xk
and Yk “ ln yk for k “ 1, 2,...,n. The model parameters (A and m for model “ 1) are
obtained by solving a 2ˆ2 system of linear equations (cij , i, j “ 1, 2 and bi, i “ 1, 2). Since
they involve the sums of Xp
k or YkXp
k , the elements of the coefficient matrix and the rhs
are set as accumulating variables, and the resulting system is solved by Cramer’s rule. The
model parameters are determined: a “ eA and b “ m, and using the original data, the sums
of the squares of residuals and the sum of the squares of mean deviation (S) are computed
by Eqs. (7.4) and (7.11), respectively.
EXAMPLE 7.3: Linearization of power model
The following strain-stress data was obtained from the tensile testing of an alloy steel.
The theoretical relationship between strain and stress is dictated by the Hollomon
equation, stated as σ “ K ¨ εn, where σ is the stress, ε is the strain, K is the
strength coefficient, and n is a material-dependent constant. Apply the method of
least squares to determine the model parameters (K and n) of the alloy steel.
ε 0.002 0.003 0.004 0.006 0.01 0.015 0.02 0.024
σ (MPa) 535 550 565 585 610 630 645 650390  Numerical Methods for Scientists and Engineers
SOLUTION:
TABLE 7.5
i Xi Yi X2
i XiYi pYi ´ Y¯iq
2
e2
i
1 ´6.21461 6.28227 38.6213 ´39.0418 0.01124 1.45ˆ10´7
2 ´5.80914 6.30992 33.7461 ´36.6552 0.00614 1.53ˆ10´5
3 ´5.52146 6.33683 30.4865 ´34.9885 0.00265 6.76ˆ10´6
4 ´5.11599 6.37161 26.1734 ´32.5971 0.00028 3.05ˆ10´7
5 ´4.60517 6.41346 21.2076 ´29.5351 0.00063 5.31ˆ10´6
6 ´4.19970 6.44572 17.6375 ´27.0701 0.00330 3.20ˆ10´6
7 ´3.91202 6.46925 15.3039 ´25.3078 0.00655 4.27ˆ10´6
8 ´3.72970 6.47697 13.9107 ´24.1572 0.00786 2.45ˆ10´5
Σ ´39.10781 51.106 197.0872 ´249.353 0.03866 5.72ˆ10´5
The Hollomon equation is a nonlinear equation suitable for the power model.
Taking the natural logarithm of both sides, we find
ln σ “ ln K ` n ln ε
Upon setting Y “ ln σ, X “ ln ε, and A “ ln K, it is transformed to Y “ A `
nX. Next, the strain-stress data are transformed according to the transformation
variables: pεi, σiqÑpXi, Yiq. Using Eq. (7.10) and the transformed data, the linear
least squares leads to 2ˆ2 system of linear equations given as
„ n ř
ř
Xi
Xi
ř X2
i
j „A
n
j
“
„ ř
ř
Yi
XiYi
j
The numerical sums for ΣXi, ΣX2
i , ΣYi, and ΣpYiXiq are required to construct
and solve the linear system. The details for the construction of the required sums are
tabulated in Table 7.5. Substituting these numerical values into the matrix equation
yields
„ 8 ´39.1078075
´39.1078075 197.0871520 j „A
n
j
“
„ 51.106024
´249.35294j
The determinant of the coefficient matrix is 47.276, which implies that ill￾conditioning is not going to be a problem (see Section 2.7). The solution of the
linear system is found to be A “ 6.78335387 and n “ 0.080822913. Then, we evalu￾ate K “ eA – 883 and take n » 0.081, yielding the Hollomon equation:
σ “ 883 ε0.081, 0.002 ď ε ď 0.024
To assess the goodness of the fit, we calculate the mean value ¯σ “ 596.25, the sum
of the squares of the mean deviations, S “ Σpσk ´ σ¯q2 “ 13587.5, and the sum of
the squares of the residuals E “ Σpσpεkq ´ σkq2 “ 23.8716. The r-squared is then
determined as r2 “ 1 ´ E{S “ 0.99825, which implies that 99.824% of the measured
or observed data are explained by the best-fit model.
Discussion: The least square regression of the data to the Hollomon equation is
simple and straight forward. The conformity of the measured experimental and pre￾dicted data from the best-fit curve is depicted in Fig. 7.14a. The visual inspection of
the figure reveals an “excellent agreement” between the data and the best-fit curve.Least Squares Regression  391
FIGURE 7.14: (a) Best-fit curve and original data, (b) PO curve.
The predicted and measured (observed) data are depicted in the PO plot shown
in Fig. 7.14b. Applying the linear least squares fit to the PO data leads to
σobs. “ 2.11231 ` 0.99722σpred., r2 “ 0.9984
Note that the slope of the line is 0.99722 (or 44.92˝ is the angle of slope, i.e., almost
45˝), and the data has a bias of 2.11, which is small in comparison to the stress
scale ranging from 500 to 700 MPa. Hence, we conclude that the curve fit can be
confidently used within the 0.002 ď ε ď 0.024 interval.
7.4 MULTIVARIATE REGRESSION
In most practical problems, the independent variables that determine or affect the dependent
variable (outcome) are often not limited to a single variable. A multivariate regression model
must be adopted in a process where the outcome is affected by more than one independent
variable. The least squares regression can very well be applied to linear and nonlinear
regression models, referred to as multivariate regression or multivariate least squares.
Linear model: It is the most common multivariate (with m independent variables)
linear regression model and results in
Y “ a0 ` a1X1 ` a2X2 `¨¨¨` amXm (7.18)
The sum of the squares of the residuals, Eq. (7.2), for this model yields
Epa0, a1,...,amq “ ÿn
i“1
pa0 ` a1X1i ` a2X2i `¨¨¨` amXmi ´ Yiq
2 (7.19)
where a0, a1, ...,am are the model parameters.
In order to minimize it, the partial derivatives of Eq. (7.19) with respect to model
parameters are set to zero, i.e., BE{Ba0 “ BE{Ba1 “ ... “ BE{Bam “ 0. After algebraic
manipulations and simplification, a system of m equations with m unknowns is obtained as
»
—
—
—
—
—
—
—
–
n ř X1i
ř X2i ¨¨¨ ř Xmi
ř X1i
ř X2
1i
ř X2iX1i ¨¨¨ ř X1iXmi
ř X2i
ř X1iX2i
ř X2
2i ¨¨¨ ř X2iXmi
.
.
. .
.
. .
.
. ... .
.
. ř Xmi ř X1iXmi ř X2iXmi ¨¨¨ ř X2
mi
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
–
a0
a1
a2
.
.
.
am
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
–
ř Yi
ř X1iYi
ř X2iYi
.
.
. ř XmiYi
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(7.20)392  Numerical Methods for Scientists and Engineers
Multivariate Regression
‚ Multivariate regression provides a tool to determine relationships or
the relative influence of the predictor variables on the outcome;
‚ It allows better prediction capability due to having multiple variables
(predictors), i.e., it is not dependent on a single predictor;
‚ It provides the capability to identify outliers or anomalies.
‚ Multivariate (non-linear) regression can be a bit complicated and may
require high-level mathematics and programming techniques;
‚ The output of multivariate regression models may be a bit more dif￾ficult to interpret;
‚ It does not have much scope for small data sets.
where the coefficient matrix and the rhs can be readily found using the observed data.
However, it should be kept in mind that such linear systems tend to be ill-conditioned.
The non-linear model: Another frequently encountered nonlinear model involving
multiple variables has the following power form:
ypx1, x2,...,xmq “ A xa1
1 xa2
2 ¨¨¨ xam
m (7.21)
This model can be linearized similarly to the univariate power model. Taking the natural
logarithm of Eq. (7.21) yields
ln y “ ln A ` a1 ln x1 ` a2 ln x2 `¨¨¨` am ln xm (7.22)
which gives Eq. (7.18) form after setting a0 “ln A, pXik, Yiq“pln xik, ln yiq for k“1, 2,...,n
and i“1, 2,...,m in Eq. (7.22).
EXAMPLE 7.4: Applying Bivariate regression
The specific heat of a chemical compound has been experimentally determined as a
function of temperature. The experimental results are presented in the table below:
T (˝C) 5 25 100 185 260 320
cp (kJ/kgoC) 5 22 25 30 34 41
An algebraic model for the specific heat is suggested as cppTq “ A ` BT ` C{T2,
where T is in ˝C units. Apply a bivariate linear regression model to determine model
parameters and discuss the goodness of the fit.
SOLUTION:
Instead of a single variable T, two independent variables may be introduced by
defining x1 “ T and x2 “ 1{T2. The bivariate linear model now has the form:
cppx1, x2q “ A ` Bx1 ` Cx2,
Employing the least squares procedure leads to a 3ˆ3 system of linear equations
(first three rows and three columns of Eq. (7.20)):Least Squares Regression  393
TABLE 7.6
i x1i x2i cpi x2
1i x1ix2i x2
2i x1icpi x2icpi
1 5 0.04 5 25 0.2 1.60ˆ10´3 25 0.200
2 25 0.0016 22 625 0.04 2.56ˆ10´6 550 0.0352
3 100 1ˆ10´4 25 10000 0.01 1.00ˆ10´8 2500 0.0025
4 185 2.9ˆ10´5 30 34225 0.00541 9ˆ10´10 5550 9ˆ10´4
5 260 1.5ˆ10´5 34 67600 0.00385 2ˆ10´10 8840 5.0ˆ10´4
6 320 9.8ˆ10´6 41 102400 0.00313 1ˆ10´10 13120 4.0ˆ10´4
Σ 895 0.041754 157 214875 0.262377 0.0016026 30585 0.23948
»
—
–
n ř x1i
ř x2i
ř x1i
ř x2
1i
ř x1ix2i
ř x2i
ř x1ix2i
ř x2
2i
fi
ffi
fl
»
—
–
A
B
C
fi
ffi
fl “
»
—
–
ř cpi
ř x1icpi
ř x2icpi
fi
ffi
fl
Using the observed data, Table 7.6 is prepared to determine the coefficient matrix
and the rhs. Substituting the pertinent numerical values into the matrix equation
results in
»
–
6 895 0.041754
895 214875 0.262377
0.041754 0.262377 0.0016026
fi
fl
»
–
A
B
C
fi
fl “
»
–
157
30585
0.23948
fi
fl
The solution of the linear system yields the following best-fit model:
cppTq “ 19.7356 ` 0.0606 T ´ 374.68
T2
with E “ 9, S “ 762.834, and r2 “ 0.9882, obtained from the original regression
model.
FIGURE 7.15: (a) Best-fit curve and original data, (b) PO curve.
Discussion: The presence of very large and very small values in the coefficient ma￾trix should lead us to question the existence of ill-conditioning. In this context, the
Frobenius-based condition number is found to be 2.458ˆ108 implying the require￾ment of high precision arithmetic to avoid the adverse effects of ill-conditioning.
The measured data and the prediction model (best-fit curve) are comparatively
depicted in Fig. 7.15a. The model and the measured data are observed to be con￾sistent, with no significant residuals. Besides a visual inspection, a high coefficient394  Numerical Methods for Scientists and Engineers
of determination (r2 “ 0.9882) validates the very good agreement between the data
and the model. In Fig. 7.15b, the predicted-measured cp data are clearly tightly
aligned along the diagonal. Fitting the predicted-measured data to a straight line
gives a slope of 1 and negligible bias. Hence, we conclude that the model can be
confidently used in 5 ď T ď 320 ˝C.
7.5 CONTINUOUS LEAST SQUARES APPROXIMATION
In science and engineering, one often has to deal with complicated functions. Sometimes a
simpler approximation is used to replace a function to simplify calculations or to more clearly
understand or explain the relationships between dependent and independent variables. Of
course, in the meantime, the approximation should yield results of reasonable and acceptable
accuracy.
The “best” approximations for continuous functions should minimize the maximum￾minimum error on the interval they are prescribed. But this is oftentimes very difficult
to achieve, so we must settle for a “better” approximation. A better approximation of a
continuous function oscillates about the function (under- and over-estimates) in the interval
of interest.
In Fig. 7.16, linear and quadratic approximations replacing a continuous function are
depicted. The areas between the true and approximate functions corresponding to the parts
above and below the true function should be approximately equal. Ideally, it is desirable to
have the deviations on both sides be in the same order of magnitude as well. Recall that
we used the Taylor polynomials to approximate continuous functions. However, a Taylor
polynomial is good only in the vicinity of x “ a, and the degree of the polynomial needs
to be increased to improve its accuracy. For instance, consider the linear and quadratic
Taylor polynomial approximations of y “ ex. Note that in Fig. 7.17, both polynomial
approximations are in very good agreement in the neighborhood of x “ 0. Even though
high-degree polynomials yield better approximations, eventually all approximations will
deteriorate with increasing x. Hence, the Taylor polynomials are not suitable if we are
looking for a simple approximation for relatively large intervals.
The least squares method, serving the same purpose, can be alternatively applied to
continuous functions, y “ fpxq, say on ra, bs, to obtain a simpler approximating function
Y pxq with a small average error. The average error of an approximation is defined by the
FIGURE 7.16: Approximation functions of fpxq by (a) linear (b) quadratic functions.Least Squares Regression  395
FIGURE 7.17: Linear and quadratic Taylor approximations for y “ ex.
root-mean-square error (rmse) expressed as
RmsepY ; fq “ ” 1
b ´ a
ż b
a
pY pxq ´ fpxqq2
dxı1{2
(7.23)
where RmsepY ; fq denotes the rmse error of approximating fpxq by Y pxq. Note that min￾imizing Eq. (7.23) is equivalent to minimizing the sum of the squares of the continuous
residual, Δy “ Y pxq ´ fpxq; in other words, our objective function will be
E “
ż b
a
`
Y pxq ´ fpxq
˘2
dx (7.24)
Equation (7.24) is referred to as the least squares approximation problem.
Now, consider an mth-degree approximating polynomial, Y pxq. The objective function,
Eq. (7.24), is written as
Epa0, a1,...,amq “ ż b
a
`
a0 ` a1x ` a2x2 ` ... ` amxm ´ fpxq
˘2
dx (7.25)
To obtain m`1 equations, the partial derivatives of the objective function are evaluated
with respect to ak for k “ 0, 1, 2,...,m and set to zero to determine the solution set that
makes Eq. (7.25) minimum.
BE
Ba0
“ 2
ż b
a
`
a0 ` a1x ` a2x2 ` ... ` amxm ´ fpxq
˘
p1qdx “ 0
BE
Ba1
“ 2
ż b
a
`
a0 ` a1x ` a2x2 ` ... ` amxm ´ fpxq
˘
pxqdx “ 0
.
.
. .
.
.
BE
Bam
“ 2
ż b
a
`
a0 ` a1x ` a2x2 ` ... ` amxm ´ fpxq
˘
pxmqdx “ 0
(7.26)
After simplifications, Eq. (7.26) becomes
a0
şb
a dx ` a1
şb
a xdx ` a2
şb
a x2dx ` ... ` am
şb
a xmdx “ şb
a fpxqdx
a0
şb
a xdx ` a1
şb
a x2dx ` a2
şb
a x3dx ` ... ` am
şb
a xm`1dx “ şb
a xfpxqdx
.
.
.
a0
şb
a xmdx ` a1
b
ş
a
xm`1dx ` a2
şb
a xm`2dx ` ... ` am
şb
a x2mdx “ şb
a xmfpxqdx
(7.27)396  Numerical Methods for Scientists and Engineers
The definite integrals in Eq. (7.27) can be evaluated analytically (or numerically if neces￾sary), and then the resulting system of equations is solved for the coefficients.
The resulting system of linear equations, Eq. (7.27), is ill￾conditioned for m ą 5. Therefore, approximating functions with
high-degree polynomials is not recommended.
EXAMPLE 7.5: Regression under the integral sign
Find a continuous linear approximation to y “ 3x2 on [0,2] using the least squares
approximation.
SOLUTION:
We are seeking a linear approximation, Y pxq “ a`bx, to y “ 3x2 whose residual
can be written as Δy “ a`bx´3x2. The objective function to be minimized becomes
Epa, bq “ ż 2
0
`
a ` bx ´ 3x2˘2
dx
where a and b are the unknown parameters.
Taking the partial derivatives of E with respect to a and b and using the chain
rule, we obtain
BE
Ba “ 2
ż 2
0
`
a ` bx ´ 3x2˘
p1qdx “ 0 ñ
ż 2
0
pa ` bxqdx “
ż
2
0
3x2dx
BE
Bb “ 2
ż 2
0
`
a ` bx ´ 3x2˘
pxqdx “ 0 ñ
ż 2
0
pax ` bx2qdx “
ż
2
0
3x3dx
Definite integrals in the above expressions yield
ż 2
0
dx “ 2,
ż 2
0
xdx “ 2,
ż 2
0
x2dx “ 8
3
,
ż
2
0
x3dx “ 4
Upon substituting the numerical values into the preceding equations, we obtain a
2ˆ2 system of linear equations: a ` b “ 4, a ` 4b{3 “ 6. Having solved this system
of equations, we find the linear approximation as Y pxq “ 6x ´ 2.
FIGURE 7.18Least Squares Regression  397
Discussion: The true function and its linear approximation are comparatively plot￾ted in Fig. 7.18. We can see that the residual, Δ, is not uniform throughout [0,2].
The linear approximation at x “ 0 and 2 underestimates ex by Δy “ 2, which is
also the point where the absolute (maximum) errors occur. Notice that the integrals
of the true and approximate functions on [0,2] are 8 (i.e., the same), while the error
(i.e., the objective function) corresponds to E “ 1.6.
7.6 OVER- OR UNDER-DETERMINED SYSTEMS OF EQUATIONS
In analyzing experimentally collected data, the number of equations is sometimes deliber￾ately set higher than the number of unknowns in order to minimize the experimental errors.
A linear system with more equations than unknowns is called an overdetermined system. In
reality, such systems do not have exact solutions, but their solutions are obtained such that
the approximate solution error is minimal.
Another type of system of linear equations encountered in the fields of electronics,
statistics, geology, mechanics, and so on is called an underdetermined system. In these
systems, the number of unknowns is greater than the number of equations. The solution of
an underdetermined system can be obtained by employing the method of least squares if it
has a common solution.
Consider a system of linear equations with m unknowns and n equations (n ą m or
n ă m):
ÿm
j“1
aijxj “ bi, i “ 1, 2,...,n (7.28)
where aij ’s and bi’s are known.
The residual for the ith equation can be expressed as
ri “ ai1x1 ` ai2x2 ` ai3x3 `¨¨¨` aimxm ´ bi, i “ 1, 2,...,n (7.29)
Since the number of equations and the number of unknowns are not the same, an exact
solution will not be achieved; in other words, the goal of ’zero residuals’ will not be realized.
Hence, we force the sum of the squares of the residuals (the objective function) to be a
minimum as follows:
Epx1, x2,...,xmq “ ÿn
i“1
pai1x1 ` ai2x2 ` ai3x3 `¨¨¨` aimxm ´ biq
2 (7.30)
Next, to find the minimum of the objective function, the partial derivatives of the sum of
the squares of the residuals, Eq. (7.30), with respect to model parameters will be set to zero
as follows:
BE
Bxj
“2
ÿn
i“1
pai1x1`ai2x2`ai3x3`¨¨¨`aimxm´biqaij “ 0, j “ 1, 2,...,m (7.31)
This procedure yields a system of m linear equations with m unknowns, defined in this case
as xi’s for i “ 1, 2, ...,m.398  Numerical Methods for Scientists and Engineers
EXAMPLE 7.6: Solving overdetermined system by least-square regression
The management of a spark-plug manufacturer believes that the sales figures (Rk
piece) can be correlated by the state’s population (Ak1) and gross domestic product
per capita (GDP) (Ak2). The pertinent data for five states is summarized in the
accompanying table. The manufacturer, based on past experience, believes that the
sales income is related to the population and GDP per capita as
Rk “ x1 ` Ak1x2 ` Ak2x3
where x1, x2, and x3 are correlation parameters. Use the method of least squares to
determine the model parameters.
k State Sales, Rk Population, GDP per capita,
(million $) Ak1 (million) Ak2 (1000 $)
1 California 33.3 39.5 67.9
2 Texas 18.2 28.3 58.4
3 Utah 2.1 3.1 49.7
4 Arizona 5.4 7.1 43.1
5 Idaho 1.3 1.7 39.8
SOLUTION:
The available data can be used to construct the following equations:
33.3 “ x1 ` 39.5 x2 ` 67.9 x3, 5.4 “ x1 ` 7.1 x2 ` 43.1 x3
18.2 “ x1 ` 28.3 x2 ` 58.4 x3, 1.3 “ x1 ` 1.7 x2 ` 39.8 x3
2.1 “ x1 ` 3.1 x2 ` 49.7 x3
These five equations with three unknowns constitute an overdetermined system,
which can be rearranged in the form of residuals as follows:
r1 “ x1 ` 39.5 x2 ` 67.9 x3 ´ 33.3 “ 0,
r2 “ x1 ` 28.3 x2 ` 58.4 x3 ´ 18.2 “ 0,
r3 “ x1 ` 3.1 x2 ` 49.7 x3 ´ 2.1 “ 0,
r4 “ x1 ` 7.1 x2 ` 43.1 x3 ´ 5.4 “ 0,
r5 “ x1 ` 1.7 x2 ` 39.8 x3 ´ 1.3 “ 0
The objective is to minimize the sums of the squares of the residuals, so the objective
function is constructed as follows:
Epx1, x2, x3q “ ÿ
5
i“1
r2
i
Now, taking the partial derivatives of E with respect to x1, x2, and x3 and setting
the resulting equations to zero results in
BE
Bx1
“ 10 x1 ` 159.4 x2 ` 517.8 x3 ´ 120.6 “ 0
BE
Bx2
“ 159.4 x1 ` 4848.1 x2 ` 9725.02 x3 ´ 3754.94 “ 0
BE
Bx3
“ 517.8 x1 ` 9725.02 x2 ` 27865.4 x3 ´ 7425.6 “ 0Least Squares Regression  399
Discussion: Solving the preceding 3 ˆ 3 linear system yields
x1 “ ´5.04608, x2 “ 0.726145, x3 “ 0.106824
and
E “ ÿ
5
i“1
r2
i “ 19.73854, S “ ÿ
5
i“1
pri ´ r¯q
2 “ 748.174
Finally, the coefficient of determination is found to be r2 “ 0.97362.
Discussion: Least squares regression is an alternative technique for solving an
overdetermined system of equations based on the principle of least squares minimiza￾tion of observation residuals. It is used extensively in the disciplines of engineering
to minimize experimental measurement errors.
7.7 CLOSURE
In this chapter, the least squares technique for developing regression curves for discrete
functions is presented. The method is one of the most common methods used to approximate
a set of observations, i.e., discrete data sets. With this method, the best curve fit to a given
discrete data set is achieved by minimizing the sum of the squares of the residuals (i.e.,
deviations) of the data points from the curve. The method is successfully applied to data
sets that are not very accurate or precise. Often, the data collected from the instruments or
devices through experiments may have a significant amount of noise, and this method helps
smooth out the noise present in the data sets. As a result, unlike interpolating polynomials,
the best-fit curve may not pass through every individual data point.
The method requires the selection of a regression model, Y pxq, and the process, termed
“regression analysis,” quantifies the relationship between independent and dependent vari￾ables. The linear problems, Y pxq “ a0 ` a1x, are often encountered in regression analysis
in statistics and engineering applications, while nonlinear problems are also commonly en￾countered in science and engineering problems. On the other hand, approximation functions
in polynomial form, Y pxq “ pmpxq, can be generated for most experimental observations.
By increasing the degree of the polynomial, the sum of the squares of the residuals decreases
and r-squared improves; nevertheless, increasing the degree of an approximating polynomial
yields Runge’s phenomenon.
An analyst has several tools to assess the goodness of a regression. The so-called “best￾fit” curve should not be assumed to be the “best-fit curve” without confirmation. Plotting
and visually comparing the data and the selected model is one of the most important and
obvious tools to verify the suitability of a model. On the other hand, quantitative tools
such as r-squared, adjusted r-squared, rmse, PO-plots, and so on provide the analysts with
ways and means of assessment. The goodness or fit of a model should be assessed using
both numerical and visual tools.
When selecting a best-fit model, the underlying theory of the physical event should
be taken into account. The physical properties (such as density, viscosity, heat capacity,
conductivity, etc.) of a substance that vary with temperature can be described by poly￾nomials. The discrete data for these properties (tabulated in reference books) are free of
experimental uncertainties, making them less likely to contain interference. Some of the
most common nonlinear relationships observed in science and engineering, such as power400  Numerical Methods for Scientists and Engineers
(axb), exponential (aebx), saturation growth, and so on, can be easily linearized by applying
a suitable transformation. The model parameters of the nonlinear regressions generally have
physical meaning. The regression analysis, which can be very sensitive to outliers, might be
unreliable if the discrete data is small in number or is not uniformly distributed. When the
dependent variable is a function of two or more independent variables, multivariate linear or
non-linear regression models can be employed in the same way. Multivariate power models
are also easily linearized, just as univariate power models are.
The least squares method can be applied to replace complicated functions over a given
range with continuous but simpler mathematical expressions by minimizing the squares of
the difference between the original function and the approximation under the integral sign.
Also, in applications of linear systems of equations, sometimes a linear system of equations
has fewer equations (underdetermined systems) or more equations (overdetermined systems)
than the unknowns. In such a case, the system may have no solution or an infinite number
of solutions. The least squares method can often be applied to establish a problem that has
a unique solution in the least squares sense.
7.8 EXERCISES
Section 7.1 Polynomial Regression
E7.1 The thermal conductivity k (in W/cm.K) of a substance is measured as a function of
temperature T (in K), and the results are presented below. Assuming that the conductivity varies
linearly with temperature (k “ a0 `a1T), apply least squares to determine the best-fit parameters
and calculate r2.
T 226 285 335 362 400 470
k 0.88 1 1.1 1.15 1.22 1.35
E7.2 A series of experiments are conducted to determine the torque required to turn a spring
mechanism with an applied angle. The proposed relationship between the torque and applied
angle is Tpθq “ a0 ` a1 θ. Considering the experimental results furnished below, (a) determine
the least squares best-fit parameters; (b) calculate the coefficient of determination (r2); and (c)
comparatively plot the data and the best fit and interpret the results.
θ (rad) 0 0.39 0.78 1.18 1.57 1.96 2.35
T (N¨m) 0.243 0.29 0.35 0.34 0.44 0.49 0.51
E7.3 The table below contains the results of five experiments conducted to determine the specific
heat of an alloy, cp (kJ/kg¨K), as a function of temperature, T (K). The specific heat is assumed
to be linear in the temperature range tested. Use the following experimental data to determine
the least squares best-fit parameters and calculate r2.
T 275 350 500 750 1250
cp 0.38 0.39 0.41 0.43 0.48
E7.4 The wet-bulb Tw (
˝C) and cold-water temperature Tc (
˝C) data tabulated below will be
used to develop a cooling-tower performance curve. Assuming that the relationship between the
temperatures is linear, apply the linear regression to find the model parameters and comment on
the goodness of the fit.
Tw (
˝C) 15.5 17.8 20.1 22.8 25.1 27.3 29.5
Tc (
˝C) 24.5 25.4 27.3 28.8 30.7 31.7 33.8
E7.5 The relative humidity φ (%) versus temperature T (
˝C) relationship of a dairy powder is
tabulated below. Assuming that the relative humidity varies linearly with the temperature, (a)Least Squares Regression  401
apply the linear least squares to find the best-fit parameters; (b) plot the residuals; (c) compute
r2 and comment on the goodness of the fit.
φ 15 22 29 33 36 45 52
T (
˝C) 94 80 73 68 60 55 45
E7.6 The following log-log data is to be used to establish the power curve (i.e., the relationship of
the Reynolds number, Re, versus the power number, Po) of a rotationally reciprocating impeller
under assumed operating conditions. It is known that the power curve on the log-log scale behaves
linearly for Re ă 103. In other words, the power curve can be expressed as y “ A ` B x, where
x “ log10 Re and y “ log10 Po. Use the linear regression model to find the best-fit parameters and
comment on the goodness of the fit.
log10Re ´0.39 0 0.7 1.4 2 2.5 3
log10Po 2.7 2.34 1.63 1.02 0.36 0.028 ´0.35
E7.7 The braking distance of a vehicle (S) is investigated as a function of its pre-braking speed
(v). The experimental data is known to be well described with a quadratic model, Spvq “ av2 `
bv ` c. Apply least squares regression to determine the model parameters and calculate r2.
v (km/h) 10 40 85 115 145
S (m) 2.1 5.6 13.8 21 31
E7.8 A pump characteristic curve for the head (hp in m) can be expressed by an nth-degree
polynomial, i.e., hp “ PnpQq, where Q is the volumetric flowrate of the fluid in m3/h. Consider
the pump whose head-versus-flowrate test results are given below. Use the data and the method
of least squares to obtain the pump characteristic curve with second-, third-, and fourth-degree
polynomial approximations. Which approximation would you recommend? and why?
Q 5 25 65 90 120 140
hp 10.91 10.60 9.56 8.47 7.13 5.43
E7.9 The lift coefficient (CL) is a dimensionless parameter that relates the lift generated by a
lifting body. A wind tunnel is used for determining a relationship between the lift coefficient and
the angle of attack (the angle between the horizontal line and the line of motion). The wind
tunnel test results for an airfoil are presented below. Use the method of least squares regression
to develop linear and quadratic models. Which model is better? and Why?
CL ´5 0 5 10 15 20
θ (deg) 0.02 0.56 1.06 1.50 1.71 1.65
E7.10 Consider two elements, A and B, which are completely soluble in each other in solid
and liquid states. The following solidus (Ts in ˝C) and liquidus temperature (T� in ˝C) data are
obtained from the binary A-B phase diagram as a function of B content (xB, %). Use least squares
regression to obtain the best-fit polynomials for the solidus and liquidus curves with r2 ą 0.99.
xB 0 20 40 60 80 100
Ts 1110 1138 1186 1256 1353 1500
T� 1110 1291 1390 1452 1488 1500
E7.11 Hook’s law states that the force applied (F) is directly proportional to the length (x) of
a spring, F “ kx, where k is the spring constant. Experimental data conducted to determine the
spring constant of a new product is presented below. Apply the least squares to determine the
spring constant.
x (cm) 1 3 4 5 8 10
F (N) 175 550 700 900 1450 1830402  Numerical Methods for Scientists and Engineers
Section 7.2 Transformation of Variables
E7.12 The specific heat of a substance as a function of temperature is to be determined using the
experimental results presented below. The temperature dependence of the specific heat is given
as cppTq “ A ` B{T. Apply a linear regression model to determine A and B and calculate r2.
T (K) 300 400 500 600
cp (Kcal/kg˝C) 1.267 1.351 1.402 1.434
E7.13 Experiments were carried out to determine the relationship between sugar solubility (x in
g sugar/100g solution) in a water-based solution and solution temperature (T in ˝C). The data
is assumed to be well-described by x “ a ` b ln T. Using a linear regression model with the data
furnished below, determine the best-fit parameters and the coefficient of determination.
T 5 10 30 50 75 90
x 160 200 260 275 295 298
E7.14 Grain-boundary strengthening is a method of strengthening materials that involves chang￾ing the average grain (crystallite) size. The relation between yield stress and grain size is described
by the Hall-Petch equation given as σy “ σ0 ` k{
?
d, where σy is the yield stress (MPa), σ0 is
a material constant (MPa), k is the strengthening coefficient (?μm), and d is the average grain
diameter (μm). Experimental results for the yield stress and the grain size of a heat-treated alloy
are tabulated below. (a) Apply least squares regression to the observed data to find the model
parameters; (b) make a residual plot and calculate r2. What can you say about the goodness of
the fit?
d (μm) 2 5 10 16 20 30
σy (MPa) 156.6 145.2 135.4 134.5 129.1 126.9
E7.15 When a particular bacterial culture is introduced into milk, its population remains more
or less the same for a short time (time lag), but then begins to increase exponentially. A suitable
mathematical model is proposed as log10nptq “ a ` b et
. The observed bacterial population with
time is tabulated below. (a) Use least squares regression to determine the model parameters; (b)
calculate the coefficient of determination; and (c) plot residuals and discuss the goodness of the
fit.
t (min) 0.2 0.5 0.9 1.2 1.8 2.5 3.3
log10 n 4.7 4.8 5.1 5.4 6.1 10.8 13.9
E7.16 The variation of the aerodynamic drag coefficient (CD) with speed (V ) of a high-speed
train model was determined as a result of a series of wind tunnel tests. The test results are
tabulated below. The data is known to obey the CDpV q “ a ` b lnpV q relationship. (a) Use least
squares regression to find the model parameters; (b) calculate the coefficient of determination;
and (c) plot residuals and discuss the goodness of the fit.
V (km/h) 50 90 110 140 190 270 440
CD 0.37 0.36 0.35 0.344 0.335 0.32 0.31
E7.17 The data collected to determine the solubility of glycerin (y in g glycerin/100 g acid) in
an acid solution (x in mole) is tabulated below. The relationship between the solubility and acid
solution is given as y “ a ` b
?x. Compute the best-fit parameters using the method of least
squares and discuss the goodness of the fit.
x (mole) 1 4 9 16 25
y (g/100 g acid) 4.6 6.5 8.8 10.1 13.2
E7.18 The tabulated data is taken from an analog input signal. A suitable mathematical model
for the data is given by Y ptq “ a ` b cosp2πtq. (a) Use the data and least squares regression to
estimate the model parameters, and (b) discuss the goodness of the fit.Least Squares Regression  403
t 0 0.1 0.2 0.3 0.4 0.5 0.6
Y 1.67 1.62 1.50 1.36 1.23 1.19 1.24
E7.19 A thermistor is an electrical resistor whose resistance changes in response to temperature,
and it is used as temperature sensor due to its high sensitivity. By measuring the resistance of the
thermistor, one can determine the temperature of the medium. A relation between the temperature
and the resistance of a thermistor is given by the Steinhart–Hart equation, defined as
1
T “ a ` b ln R ` cpln Rq
3
where R is the resistance (Ω) and T is the absolute temperature (K). Apply least squares regression
to the furnished data to determine the coefficients of the Steinhart–Hart equation (a, b, and c)
and the goodness of the fit.
T (K) 275 300 320 350 380
R (kΩ) 80.4 38.6 21.2 9.1 4.4
E7.20 The data for the solubility of graphite (x) in liquid iron as a function of temperature (T)
is given below. A mathematical model relating the temperature of the iron to its graphite content
(%) is given by Tpxq “ a ` b
?x ` c x. (a) Apply least squares regression to determine the model
parameters and r-squared; (b) plot the residuals and measured-predicted values; and (c) apply
linear regression to the predicted-observed data to find the slope and bias of the PO-line. What
can you say about the trustworthiness of the model?
x 4.3 5 6.5 8 10 11 12
T (
˝C) 1020 1710 1860 2590 2480 2690 3340
E7.21 The solubility of an unknown gas in an aqueous solution is studied experimentally. A
correlation (model) for the Henry’s law constant is given by
ln KH “ ÿn
k“0
akT k
(a) Use least squares regression with the tabulated data to determine linear, quadratic, and cubic
regression model parameters and find r2 for each case; (b) plot the residuals and PO values; and
(c) apply linear regression to the PO data to find the slope and bias of the PO-line. What can
you say about the reliability of the model?
T (
˝C) 80 100 150 195 260 350
KH 5.94 6.17 6.22 6.18 5.99 4.82
E7.22 The effect of an additive on the drying time of an oil-based paint is investigated. The
additive concentration (m in g/100 mL) and drying time (t in hours) are assumed to be described
by the following model:
?
t “ a ` bm ` cm2
(a) Use the data presented below and the least squares regression to find model parameters and
calculate r2; (b) plot the residuals and PO values; and (c) do you think that the model is suitable
for the furnished data?
m (g/100 mL) 0 0.5 1 2 4 6
t (h) 4 3.5 3.1 2.6 2.4 3
Section 7.3 Linearization of Nonlinear Models
E7.23 Using the given data, apply least squares regression to the Y pxq “ axbx model. (a) Plot
the data and the best-fit curve together for a visual inspection; (b) plot the residuals and calculate404  Numerical Methods for Scientists and Engineers
r-squared; and (c) apply linear regression to the PO data to find the slope and bias of the PO
line and the goodness of the fit.
x 1.2 2.8 4.3 5.4 6.8 7.9
Y pxq 2.1 11.5 28.1 41.9 72.3 91.4
E7.24 Repeat E7.23 for Y pxq “ ax ebx.
E7.25 Electrical resistivity (ρ) and salt content (C) of soil at several locations in an agricultural
region are measured. The relationship between the resistivity and soil content is assumed to obey
the power law, i.e., ρ “ mCn. Using the tabulated data and least squares regression, determine
the model parameters and comment on the goodness of the fit.
C (g/L) 0.3 1 2 4.7 7.2 10 12
ρ (Ω¨m) 144.1 89.8 19.5 23.2 5.3 10.5 6.8
E7.26 The discharge-time relationship for a capacitor obeys the exponential decay model: Qptq “
Q0 e´λt, where R is the resistance, C is the capacitance, Q0 is the initial charge, and λ “ 1{RC.
Use the tabulated experimental data to (a) estimate Q0 and λ using least squares regression; (b)
compute the coefficient of determination. (c) Discuss the goodness of the fit.
t (s) 10 20 30 40 50 75 130
Q (C) 14 13 11 10.3 7 4 1
E7.27 The variation of the viscosity of liquids with temperature is modeled by the so-called
the Andrade equation: μpTq “ aeb{T , where T is the temperature (K). The results of the viscosity
measurements of an acidic solution at different temperatures are presented below. Use the method
of least squares to estimate model parameters and discuss the goodness of the fit.
T (K) 280 300 310 330 360
μ (N¨s/m2) 0.1 0.082 0.075 0.063 0.05
E7.28 The solubility of oxygen (m, mol/ton water) is a function of temperature (T, K) and
is described by the Setschenow equation: m “ aeb T . The solubility of oxygen in a solution at
different temperatures is tabulated below. Using least squares regression, (a) estimate the best-fit
parameters; (b) compute r2; (c) plot the residuals and PO plot; and (d) discuss the goodness of
the fit.
T (K) 272 276 285 294 306 318 345
m (mol/ton) 2.17 1.97 1.65 1.37 1.14 0.99 0.83
E7.29 Steel-bearing balls removed from an annealing furnace are left to cool in the air. The
temperature readings taken from the balls are tabulated below. Assuming that the conditions for
the lumped capacitance apply, the cooling temperature obeys
Tptq ´ T8 “ pTi ´ T8q expp´t{τ q
where T8 is the ambient temperature (=10˝C), Ti is the initial temperature of the bearings, and
τ is a time constant. Use the method of least squares to estimate the time constant τ and initial
temperature Ti. What can you say about the goodness of the fit?
t (min) 1 2 4 6 8 10
T (
˝C) 330 230 115 50 26 10
E7.30 A concrete recipe has been developed for use in nuclear reactor containment structures for
protection from radioactivity. The relationship between the radiation intensity and the concrete
thickness is given by the following relationship: Ipxq “ I0e´μx, where I0 is the incident intensity
(cps, counts per second), x is the thickness (cm), and μ is the mass attenuation coefficient (cm´1).
Concrete slabs of varying thickness are irradiated from one end, and the intensity of the penetratingLeast Squares Regression  405
radiation is measured from the other end. Applying least squares regression to the tabulated data,
(a) estimate the mass attenuation coefficient and incident intensity, and (b) determine the goodness
of the fit.
x (cm) 1 16 25 30 50
I (ˆ10´6 cps) 1.86 0.33 0.023 0.01 0.0003
E7.31 In an isentropic process, the relationship between the pressure (P) and volume (V ) of
an ideal gas is known to obey P V γ “ C=constant, where γ is the heat capacity ratio. Apply
least squares regression to the furnished experimental data to estimate γ and C and discuss the
goodness of the fit.
V (m3) 0.1 0.22 0.31 0.44 0.7
P (Pa) 75 25 16 10 5
E7.32 In an optimization project, the cost of a horizontal centrifugal pump as a function of
power is required. The cost (C in $) of the pump is related to its power (P in kW) by the
C “ aP n relationship. Market research yields the tabulated data presented below. Use least
squares regression to estimate the best-fit parameters and discuss the goodness of the fit.
P (kW) 0.2 2.5 5.0 12 16
C ($) 725 1400 1600 2000 2300
E7.33 Atmospheric pressure (P in mmHg) varies with altitude (z in km), which may be described
by an exponential model P “ aebz. Apply least squares regression to the tabulated data to estimate
the best-fit parameters and discuss the goodness of the fit.
z (km) 0 0.15 0.46 1.07 2.45 6.1 10.7
P (mmHg) 760 746 720 668 565 350 180
E7.34 To determine the relationship between the volumetric flow rate (Q in m3/h) and pressure
drop (ΔP in kPa) in a hydraulic circuit, a series of measurements were performed. The pressure
drop and flow rate are known to obey a power law model: ΔP “ aQn. Apply the method of least
squares to the collected data tabulated below to estimate the best-fit parameters. What can you
say about the goodness of the fit?
Q (m3/h) 5 10 15 20 25
ΔP (kPa) 500 750 1100 1800 2750
E7.35 The tabulated data presented below was obtained from experimental work to determine
the variation of the dynamic viscosity (μ) of an engine oil with temperature. The relationship of
viscosity with temperature is known to obey μ “ a{pb ` Tq. Apply least squares regression to the
data to estimate the model parameters and discuss the goodness of the fit.
t(K) 275 290 325 350 400
μ (N.s/m) 0.42 0.07 0.025 0.015 0.01
E7.36 The model for the dynamic viscosity of gases as a function of temperature is given by the
Sutherland equation: μ “ aT 3{2{pT ` bq (in N¨s/m). Apply the method of least squares fit to the
furnished data to estimate the model parameters and discuss the goodness of the fit.
T (in K) 270 300 320 350 375
μ ˆ 106 16.7 18.6 19.2 20.7 21.8
E7.37 A type of bacteria called Acidithio-bacillus is used for the processing of low-grade copper
ores. A series of experiments with copper solutions containing the bacteria resulted in the tabulated
results for the copper precipitation with residence time. The amount of precipitate as a function
of time is thought to be governed by the relationship: rcs% “ a t{pb ` tq. Use the method of least
squares to estimate the best fit parameters and discuss the goodness of the fit.406  Numerical Methods for Scientists and Engineers
t (hrs) 18 66 120 190 250 320 390 475 570
[c] (Cu%) 4 14 25 36 45 55 65 72 80
E7.38 Impurities in a pure metal lower its compressive strength. A relationship between the
impurity content (x in %) and the compressive strength (σ in MPa) is suggested as σ “ a{pb `
xq. Apply least squares regression to the furnished experimental data to estimate the best-fit
parameters and discuss the goodness of the fit.
x (%) 0.01 0.02 0.04 0.045 0.055
σ (MPa) 150 130 100 95 88
E7.39 The following data are obtained from the molecular weight of a gas mixture (M) versus
the entrainment ratio (weight of gas to weight of air, ω) curve. Assuming that the data is well
represented by ωpMq “ M{paM ` bq, apply the least squares method to determine the best-fit
parameters and the coefficient of correlation.
M 1.5 5 15 25 35 60 95 120 140
ω 0.12 0.42 0.75 0.92 1.1 1.33 1.47 1.53 1.6
E7.40 The half-life of an irradiated radioactive specimen is to be determined. The number of
radioactive nuclides (cph, counts per hour) obeys the exponential decay law: Nptq “ N0e´λt,
where N0 is the initial radioactivity (cph) and λ “ lnp2q{t1{2, where t1{2 is the half-life of the
specimen. Apply least squares regression to estimate the half-life of this radioactive specimen and
discuss the goodness of the fit.
t (h) 1 4 7 11 15 20 30
N (cph) 960 430 190 55 20 5 0.25
E7.41 The vapor pressure of an aqueous solution at a given temperature is determined exper￾imentally. The vapor pressure (p in mmHg) is related to the temperature (T in ˝C) with the
following relationship: log10p “ aT{pb ` Tq, where a and b are solution-dependent constants. Use
least squares regression to estimate model parameters and discuss the goodness of the fit.
T (
˝C) 1 3 4 6 8 9.9
p (mmHg) 1.3 2.1 2.6 4.1 6.1 9.0
Section 7.4 Multivariate Regression
E7.42 The solubility of N2 in an aqueous solution (x in g gas/kg aq) is determined as a function
of temperature (T in K) by a series of experiments. The x ´ T relationship obeys the ln x “
a`bT `c ln T model. Apply least squares to the reported data to estimate the best-fit parameters
and discuss the suitability of the model.
T 275 290 310 330 345
x 0.029 0.021 0.016 0.014 0.0135
E7.43 An interpolating function that relates the vapor pressure (p in Pa) of a liquid melt with
temperature (T in K) is to be determined using the tabulated data. Use the following regression
model: ln ppTq “ A ` B ln T ` C{T, apply the method of least squares to determine the best-fit
parameters, and discuss the goodness of the fit.
T (K) 350 400 450 500 550
p (Pa) 9 110 760 3900 14300
E7.44 As liquids A and B dissolve in water, the thermal conductivity of the resulting aque￾ous solution increases. The experimental results for the thermal conductivity measurements with
differing amounts of A and B (vol% xA and xB) are tabulated below. Using the given data, ap-Least Squares Regression  407
ply least squares regression to the proposed model, kpxA, xBq “ a ` bxA ` cxB (in W/m¨K), to
determine the best-fit parameters and discuss the goodness of the fit.
xA 3 7 15 4 3 23 9
xB 0 1 4 8 16 12 27
k 11.8 12 12.6 14.5 17.2 14.9 20.6
E7.45 CO2 levels (C in ppm) at a rural location are collected periodically. The set of data since
the year 2000 (t “ year ´ 2000) is tabulated below. The CO2 concentration data with time is
suitable for the following mathematical model:
Cptq “ A ` B t ` C sinp2πtq
Use the least squares method to estimate the model parameters and comment on the goodness of
the fit.
tC tC tC tC
0.25 372.935 1.75 368.434 3.25 380.769 4.75 374.970
0.5 368.519 2 373.378 3.5 378.607 5 381.065
0.75 363.326 2.25 380.523 3.75 372.36 5.25 387.712
1 369.530 2.5 375.467 4 380.817 5.5 385.426
1.25 376.010 2.75 368.530 4.25 383.846 5.75 377.855
1.5 370.302 3 375.424 4.5 379.400 6 383.766
E7.46 In an open channel flow, the volumetric flow rate (Q in m3/h), channel inclination (m
in radian), and hydraulic radius (R in m) are assumed to be related by Q “ a mb
Rc. Use the
furnished experimental data and the method of least squares to estimate the best-fit parameters
and discuss the goodness of the fit.
m R (m)
(radian) 0.25 0.5 1.2 2
0.5 4.9 7.6 12.8 18.3
1.0 6.6 10.5 17.3 24.0
1.5 7.1 12.2 21.4 30.9
E7.47 The chemical affinity fpx, yq of a mixture of Salt-1 and Salt-2 varies with the mole ratio
of the salts in the mixture, [x] and [y]. The chemical affinity of the salt solutions is anticipated
to be well represented by fprxs,rysq “ arxs
b
rys
c. Apply linear regression to the experimental data
below to estimate the best-fit parameters and discuss the goodness of the fit.
[x] (mol/L) 0.1 0.5 0.9 0.5 1.1 1.1
[y] (mol/L) 0.3 0.4 0.6 0.7 0.5 1.4
fpx, yq 0.009 0.021 0.032 0.027 0.031 0.047
E7.48 The light reflection coefficient (ρ in m´1) of an optically anisotropic composite glassy
material is to be determined as a function of the angle of incidence (θ in rad). The model proposed
for the reflection coefficient is ρpθq “ a ` b cos θ ` c sin θ. Use the results of the light reflection
experiments given below and the method of least squares to estimate the best-fit parameters and
discuss the goodness of the fit.
θ (rad) 10 20 30 50 60 80
ρ (m´1) 4.05 3.80 3.40 2.09 1.42 0.12408  Numerical Methods for Scientists and Engineers
E7.49 The rate equation for a chemical reaction taking place while being continuously stirred is
assumed to have the form
r “ exp `
A ´ B
t
˘
Cnptq
where A, B, and n are the model parameters. Use the multivariate linear regression model to
determine the model parameters using the experimental data given below.
t 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
C 2.9 4.6 5.7 6.5 7.1 7.6 8 8.3 8.6 8.8
r 0.1 9 68 191 362 560 770 980 1180 1380
Section 7.5 Continious Least Squares Approximation
In Exercises E7.50´E7.57, (a) Use the method of least squares to approximate the given functions
in the specified intervals with a continuous polynomial of a specified degree. (b) Calculate the
residual error for the approximations.
E7.50 y “ cos x, quadratic on [´π/2,π/2].
E7.51 y “ 1{p2 ` x2q, quadratic on [´2,2].
E7.52 y “ |sin πx|, quadratic on [´1,1].
E7.53 fpxq “ ?2x, linear on [1/2,2].
E7.54 fpxq “ ?x, piecewise linear on intervals [0,1/4] and [1/4,1].
E7.55 fpxq “ lnp1 ` xq, quadratic on [0,2].
E7.56 y “ ex, quadratic on [´1,1].
E7.57 y “ x4, quadratic approximation of the form Y pxq “ a ` b x2 on [´1,1].
E7.58 Find an approximation in the form of y “ ax ` b{px ` 1q to a cubic polynomial defined
as fpxq “ 3 ´ x ` 2x2 ´ 3x3{4 on the interval [0,1] using the least squares method. Calculate the
residual error for the approximation.
E7.59 Using the least squares method under the integral sign, fit the given data to a continuous
fpxq “ ax2 ` b sin x function. Note: Apply the trapezoidal rule for numerical integrations.
xi 0.5 0.65 0.8 0.95 1.1 1.25 1.4
fpxiq 1.6487 1.9155 2.2255 2.5857 3.0041 3.4903 4.0552
E7.60 For a solution, the relationship of vapor pressure with temperature, in the 250ď T ď900
K range, is given below:
log10 Pv “ 32 ´ 1600
T ´ 6log10T ` 15 ˆ 10´6
T 2
Find the linear approximation log10 Pv “ A ` B T for the vapor pressure in the interval 250 ď
T ď 300 K. Estimate the relative magnitude of the error for the interval.Least Squares Regression  409
E7.61 The Nusselt number, which is a dimensionless heat transfer parameter in pipes, is given
by the Dittus-Boelter correlation as
NupRe,Prq “ 0.023Re0.8
Prn, Re ě 10000
where Re and Pr are also dimensionless parameters called the Reynolds and Prandtl numbers,
respectively. For a flow optimization problem in pipes, a linear relationship (Nu= A ` B Re) in
the range 104 ď Re ď 2 ˆ 104 is required. Find a linear approximation for any Pr number in the
indicated Re number range, i.e., find A and B as functions of Pr.
Section 7.6 Over and Underdetermined Systems
E7.62 Use the method of least squares to find the common solution of the following over- or
under-determined system of linear equations.
(a) x1 ´ x2 “ 5, 2x1 ´ 3x2 “ 2, x1 ` x2 “ 8, 2x1 ` x2 “ 12
(b) 2x1 ` x2 ` 3x3 “ 24, 3x1 ` 4x2 ´ 2x3 “ 14
(c) 3x1 ` 2x2 ` x3 ` 3x4 “ 10, x1 ` 4x2 ` 2x3 ´ x4 “ 6
(d) x1 ` 2x2 ` x3 “ 1, 3x1 ´ x2 ` 4x3 “ ´1,
´3x1 ` 5x2 ´ 3x3 “ 2, 2x1 ` 3x2 ` 5x3 “ 4
(e) 2x1 ` x2 ´ x3 ` 3x4 “ 2.5, 5x1 ` 3x2 ` 4x3 ´ 2x4 “ 7,
5x1 ` 3x2 ` 4x3 ´ 2x4 “ 1.5
E7.63 A ball is projected into the air with an angle of θ0 and an initial velocity of V0, leading to
a parabolic trajectory: y “ ax ´ bx2. During the ball’s flight, at different instances, we obtain
2.7 ´ 4a ` 16b “ 0, 4.8 ´ 8a ` 64b “ 0, 6.9 ´ 14a ` 196b “ 0,
6 ´ 27a ` 729b “ 0, 3.9 ´ 33a ` 1089b “ 0, 1.4 ´ 37a ` 1369b “ 0
Recalling that a “ tan θ0 and b “ pg{2q{pV0 cos θ0q
2, estimate the initial velocity and the projection
angle by solving the overdetermined system.
7.9 COMPUTER ASSIGNMENTS
CA7.1 The Nusselt number for fully developed laminar flow in a circular tube annulus with
the outer surface insulated and the inner surface at constant temperature is given below (see
Ref. [27]). A model for the Nusselt number as a function of diameter ratio has the following
form: Nu “ a ` b pDo{Diq ` c pDo{Diq
2, where Di and Do are the inner and outer diameters,
respectively. Apply the method of least squares to the available data to determine the proposed
model parameters.
Di{Do 0.02 0.05 0.1 0.25 0.5 1
Nui 32.337 17.46 11.56 7.3708 5.7382 4.8608
CA7.2 Experimental results for the reduced pressure with the compressibility factor of a real gas
at TR “ 1.30 are tabulated below. To determine a suitable relationship between Z and PR, use
polynomial models (n “ 2, 3,..., 7) to relate Z to PR as accurately as possible. For polynomial
models, plot the data and the polynomial approximations on the same graph to make both a visual
and a numerical determination by calculating the r2 value. Do you observe oscillations associated
with overfitting?
PR 0.05 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5 7
Z 0.98 0.92 0.84 0.76 0.7 0.65 0.64 0.65 0.68 0.7 0.73 0.78 0.82 0.85 0.89410  Numerical Methods for Scientists and Engineers
CA7.3 Friction factor (f) in pipes is known to vary with the relative roughness of the pipe wall
(ε{D). The flow measurements in pipes for different diameter ratios yield the tabulated friction
factors given below. The relationship of the friction factor with the relative roughness is known
to obey 1{
?f “ ´1.80log10pA ` B pε{Dq
0.36q. Use the method of least squares to determine the
best-fit parameters.
ε{D 10´2 10´3 5 ˆ 10´3 10´4 10´6
f 0.0072 0.0037 0.0057 0.0022 0.0012
CA7.4 A representative plot of the frequency (f) and amplitude (I) of a signal is presented below.
The amplitude versus frequency model is known to fit the Gauss (bell-shaped) distribution:
Ipfq “ I0 exp „
´pf ´ f0q
2
2σ2
j
where I0 and f0 are, respectively, the maximum amplitude and corresponding frequency, and σ
represents the width of the signal. Apply the method of least squares to estimate I0, f0, and σ.
f 0.1 0.45 0.8 1.15 1.5 1.85 2.2 2.55 2.9
Ipfq 18 40 65 86 98 95 80 55 30
Hint: Linearize the model similarly to the exponential models; however, the number of equations
is not sufficient to determine the parameters. Defining a new quantity Q as the ratio of the
amplitudes between two data points results in the elimination of I0 as follows:
Qpfiq “ Ipfi´1q
Ipfi`1q “ exp „
2pfi ´ f0q
σ2
j
Next, the above expression is linearized to estimate f0 and σ. The maximum amplitude is computed
from
ln I0 “ 1
n
ÿn
k“1
"
ln Ik ` pfk ´ f0q
2
2σ2
*
CA7.5 The emissivity of a metal oxide (ελ) at 1400 K is experimentally determined as a function
of wavelength. The wavelength vs. emissivity data is predicted to well fit the Gaussian distribution,
i.e.,
ελ “ ε0 exp „
´pλ ´ λ0q
2
2σ2
j
where ε0 is the maximum emissivity, λ0 is the frequency corresponding to the maximum emissivity,
and σ is the deviation width of the frequency. Use the method of least squares to determine the
best-fit parameters.
λ (μm) 1 1.5 3.25 4.9 6.5 11 13.5 14.9 18 23
ελ 0.17 0.175 0.3 0.63 0.9 0.94 0.7 0.56 0.43 0.34
CA7.6 Apply the method of least squares to the tabulated data below to obtain the best-fit
approximating functions (N “ 2, 3, and 4), which are defined in terms of Legendre polynomials.
x 0 0.8 1.6 2.4 3.2 4
y 0.1 1.9 2.1 3.4 4.3 4.9
The general form of an Nth-degree approximating polynomial is given as
Y pxq “ ÿN
n“1
cnPnpxq
where Pnpxq denotes the nth-degree Legendre polynomial, and the first four polynomials are givenLeast Squares Regression  411
as
P1pxq “ x, P2pxq “ 1
2
p3x2 ´ 1q, P3pxq “ x
2 p5x2 ´ 3q, P4pxq “ 1
8
p35x4 ´ 30x2 ` 3q
CA7.7 The Nusselt number (a dimensionless heat transfer rate) is considered to be the function
of the Reynolds (a dimensionless number that describes the flow characteristics) and Prandtl
(describing the thermal behavior of a fluid) numbers. For a pipe with specially designed turbulator
inserts, the experimental measurements lead to the following tabulated results: Employ the method
of least squares to the data to determine the model parameters for Nu “ aRenPrm and discuss
the goodness of the fit.
Re Pr Nu Re Pr Nu Re Pr Nu
100 0.14 10 100 0.59 13 100 1.12 13.8
500 0.13 31 500 0.62 39 500 1.24 42.1
1220 0.14 57 1220 0.66 72 1220 1.16 77
2400 0.26 98 2400 0.71 115 2400 1.48 127
3500 0.28 128 3500 0.68 148 3500 1.33 162
7000 0.32 207 7000 0.69 234 7000 1.19 256
11000 0.27 284 11000 0.73 326 11000 1.42 263
CA7.8 Monthly average temperature T (
˝C) data collected from a local weather station is tabu￾lated below.
Month Temperature Month Temperature
Jan 11.1 Jul 29.4
Feb 12.8 Aug 30
Mar 16.4 Sep 26.7
Apr 20.8 Oct 21.9
May 25 Nov 16.1
Jun 27.8 Dec 11.4
Three mathematical models (A, B, and C) are considered for modeling the monthly average
temperatures.
Model A Tpmq “ A ` B sin2 rpm ´ 1qπ{12s
Model B Tpmq “ A ` B exp “
´Cpm ´ 6q
2‰
Model C Tpmq “ A exp “
´Bpm ´ Cq
2‰
Use least squares regression to determine the model parameters and the most suitable model.
Note: Models B and C are nonlinear equations that cannot be easily linearized. Use a trial-and￾error approach where you make a guess for C to later obtain the estimates for A and B. Calculate
r-squared each time until you find the optimum value.
CA7.9 The reaction of methanation on a catalyst is given as
CO ` 3H2 Ñ CH4 ` H2O
Initial rates obtained at constant temperature at a variety of partial pressures (in atm) of reactants
and products are tabulated below:
PCO PH2 rA PCO PH2 rA PCO PH2 rA
1 1 0.1219 1 2 0.1056 2 2 0.1056
1 1 0.0944 1 4 0.1203 2 4 0.1552
1 1 0.0943 1 8 0.1189 4 1 0.0533
1 1 0.0753 2 1 0.0782 4 2 0.0911
1 1 0.0753 2 2 0.1204 8 1 0.0317
1 1 0.0512 2 2 0.1057 8 8 0.1476
1 2 0.1274 2 2 0.1056
Linearize the following models and apply multivariate least squares regression to find the model412  Numerical Methods for Scientists and Engineers
parameters and comment on the suitability of the models. Can you perform nonlinear regression?
Does it affect model parameters?
paq rA “ kP n
COP m
H2, pbq rA “ kPCOPH2
p1 ` KCOPCO ` KH2PH2q
2CHAPTER 8
Numerical Integration
LEARNING OBJECTIVES
After completing this chapter, you should be able to
‚ describe the general features of numerical integration;
‚ derive the formulas of the trapezoidal and Simpson’s rules;
‚ apply composite trapezoidal and Simpson’s rules to continuous functions with
and without end corrections and to uniformly spaced discrete data sets;
‚ implement Romberg’s rule to estimate the integrals of continuous functions;
‚ explain and discuss the open Newton-Cotes formulas;
‚ describe the objective of adaptive numerical integration algorithms;
‚ apply Trapezoidal and Simpson’s rules to nonuniformly spaced data points;
‚ explain the derivation and implementation of the Gauss-Legendre, Gauss￾Laguerre, Gauss-Hermite, and Gauss-Chebyshev quadrature methods;
‚ understand and analyze the accuracy of quadrature methods;
‚ identify singular and improper integrals and select suitable numerical methods
to evaluate them;
‚ calculate double integrals in rectangular or nonrectangular domains.
I
NTEGRATION is one of the most important mathematical operations encountered
in many fields of science and engineering. Definite integration is used to calculate arc
length, work, area, volume, centroid, and numerous other physical quantities.
Some functions can be integrated using the exact methods and techniques covered
in calculus lectures. Furthermore, indefinite integrals of common functions and/or combi￾nations of functions are available in the form of tables in mathematical handbooks as a
reference. On the other hand, today there are computer software systems that can perform
a wide range of symbolic computations on functions. When the definite integral of a func￾tion is evaluated numerous times in a computation, the true solution (if it exists) can be
coded in the most general form (i.e., şx
0 eaudu “ peax ´ 1q{a) to save time and avoid the
accumulation of round-off errors.
Definite integrals of some functions are either impossible or very difficult to obtain by
analytical means. This is where numerical integration enters the picture as an “alternative”
for estimating a definite integral. A numerical integration does not give the “true value”
of the integral; it is essentially an approximation to the integral itself. In other words, an
analyst can only estimate an integral within a desired tolerance.
DOI: 10.1201/9781003474944-8 413414  Numerical Methods for Scientists and Engineers
FIGURE 8.1: The true area under the curve (definite integral).
The nature of an integrand (the function you wish to integrate) may necessitate al￾ternative numerical treatments. For instance, integrands may have singularities at their
endpoints or within the integration domain, or integrals may cover a semi-infinite domain,
ra, 8q or p´8, bs, or an infinite domain, p´8, 8q, etc. Numerical integrations in such cases
require suitable numerical techniques or treatments.
In practice, not all integrands are defined as explicit continuous functions. Some in￾tegrands are uniformly or nonuniformly spaced discrete functions, such as data digitally
sampled or collected from experiments or obtained numerically from solutions of differen￾tial equations (see Chapters 9 and 10), and so on. In cases where the integrand is a discrete
function, it is not possible to calculate the true integral using the tools and techniques
available in calculus. Such integrals can only be calculated approximately using a suitable
numerical method.
This chapter presents the most common numerical integration methods applied to both
explicitly given continuous and discrete functions. The methods and techniques for dealing
with singular and improper integrals are also covered. Extending and applying the numerical
methods to estimate double integrals in rectangular and non-rectangular domains have also
been presented.
8.1 TRAPEZOIDAL RULE
Let fpxq be a continuous and integrable function defined on the closed interval [a, b]. The
definite integral of fpxq from a to b is expressed as
I “
ż b
a
fpxq dx (8.1)
which corresponds basically to the area of the region bounded by the function (curve)
y “ fpxq, x´axis, and x “ a and x “ b lines (see Fig. 8.1). When the true value of
a definite integral cannot be determined, it is instead approximated by the area of the
bounded region under the curve, which is the basis of numerical integration.
The trapezoidal rule, as a numerical method, is based on estimating the area under
a curve by approximating the bounded domain with the areas of trapezoids. When imple￾menting this idea, the curve defined by y “ fpxq on [a, b] is approximated by a straight line
connecting its end points, as shown in Fig. 8.2a. Then, the area under the curve is estimated
by the area of the resulting trapezoid (i.e., the average of the long and short bases times
the altitude, that is,
ż x1
x0
fpxqdx « pb ´ aq
´f0 ` f1
2
¯
(8.2)Numerical Integration  415
FIGURE 8.2: Graphical depiction of the effect of approximations by (a) one, (b) two, (c) four panels.
where the ordinates are denoted in shorthand notation as fpxiq “ fi. This expression, which
is an approximation to Eq. (8.1), is referred to as the Trapezoidal Rule.
When a single trapezoid is used for the entire integration interval of a function with
strong curvature, the resulting error from this approximation can be very large since, geo￾metrically speaking, the unshaded area between the curve and the trapezoid will be large,
as depicted in Fig. 8.2a. It is evident that an approximation by a single trapezoid is not
going to be a good representation of the area of the bounded region. To overcome this prob￾lem, the integration interval, [a, b], is divided into N-subregions (also referred to as panels)
of uniform size, h “ Δx “ pb´aq{N. Then, the endpoints of the panels are connected by
straight-line segments, yielding trapezoids. The area of a trapezoid is an approximation to
the area of a panel or subregion. A definite integral is then approximated by summing up
the areas of the trapezoids corresponding to the panels. For example, when the interval [a, b]
is divided into two panels, the sum of the unshaded areas (i.e., the approximation error)
is substantially reduced (Fig. 8.2b). The approximation error with four panels is further
reduced (Fig. 8.2c). It is clear that as the number of panels increases, the arc segments
bounded by the corresponding panels will approach a straight line, leading to an improved
numerical estimate for the area. Hence, a numerical integration will theoretically tend to
approach the true value (area) for N Ñ 8.
To formulate this idea, we first consider the two-panel numerical integral:
ż x2
x0
fpxqdx “
ż x1
x0
fpxqdx `
ż x2
x1
fpxqdx (8.3)
The integrals on the rhs of Eq. (8.3) are approximated by the areas of the trapezoids
(shown in Fig. 8.2b as
ż x1
x0
fpxqdx « A1 “ h
´f0 ` f1
2
¯
(8.4)
and
ż x2
x1
fpxqdx « A2 “ h
´f1 ` f2
2
¯
(8.5)
Substituting these approximations, Eqs. (8.4) and (8.5), into Eq. (8.3) yields
ż x2
x0
fpxqdx « h
´1
2
f0 ` f1 `
1
2
f2
¯
(8.6)
In order to find a more general result, consider the interval [a, b] divided into uniform
N panels. The abscissas and ordinates are obtained by xi “ a ` ih and fi “ fpxiq for
i “ 0, 1,...,N, respectively. The segment of the curve in each panel is approximated with
a straight line that connects the endpoints, resulting in a trapezoid (Fig. 8.3). The lines are416  Numerical Methods for Scientists and Engineers
FIGURE 8.3: Graphical depiction of integration with multiple panels.
essentially simple linear interpolations for the curve on rxi´1, xis. The area under the curve
can then be estimated by finding the areas of the trapezoids. Finally, summing up the areas
of the trapezoids in all panels yields
ż b
a
fpxqdx « ÿ
N
i“1
Ai “ h
´1
2
f0 ` f1 ` f2 ` ... ` fN´1 `
1
2
fN
¯
(8.7)
Equation (8.7) is referred to as the composite (or N´panel) Trapezoidal Rule, and it
can be generalized as
ż b
a
fpxqdx « TN “ h
´f0 ` fN
2 `
N
ÿ´1
i“1
fi
¯
(8.8)
where TN denotes the approximation or estimate obtained by the composite (or N´panel)
Trapezoidal rule. Note that Eq. (8.8) can also be expressed as a weighted sum as
TN « ÿ
N
i“0
wifpxiq (8.9)
where wi’s are the weights, defined as w0 “wN “h{2 and wi “h for i“1, 2,...,N ´1.
It is evident that as the number of panels is increased, under the limiting condition
(N Ñ8 or h Ñ 0), the numerical integral approaches the true value:
ż b
a
fpxqdx “ lim
NÑ8 ´ ÿ
N
i“1
Ai
¯
However, it is neither possible nor practical to use an infinitely large number of panels
to estimate the definite integral of a function due to requiring excessive cpu-time (not to
mention the round-off errors acquired). For this reason, the composite rule is limited to a
finite number of panels that provide an approximate value. The approximation error can
be reduced by increasing the number of panels, as illustrated in Fig. 8.2. Contrary to the
integration of explicit functions, the panel width of a discrete function is fixed and cannot be
arbitrarily adjusted. A larger data set with a smaller panel size means reproducing a discrete
data set, which may not always be technically possible. Thus, the numerical analysts have
to deal with the consequences of using the available discrete data set and then figure out
the associated numerical errors.
Equation (8.8) does not provide any quantitative information about the accuracy of the
values estimated by the composite trapezoidal rule or the magnitude of the errors made.Numerical Integration  417
However, it is important that the resulting error be associated with a measurable and
comparable numerical value. To be able to do that, we resort to the Taylor series, where we
define the integral of a continuous and integrable function fpxq as
Ipxq “ ż x
a
fpuqdu (8.10)
Assuming it is analytic, Ipxq is expanded to the Taylor series about xi as follows:
Ipxi ` hq “ Ipxiq ` h I1
pxiq ` h2
2! I2pxiq ` h3
3! I3pxiq ` Oph4q (8.11)
Employing the Leibniz rule of differentiation to Eq. (8.10) and using a shorthand notation,
we find
Ipxi ` hq“Ii`1, I1
pxiq“fpxiq“fi, I2pxiq“f1
pxiq“f1
i , I3pxiq“f 2pxiq“f 2
i ,
Equation (8.11) can now be expressed in terms of fpxq as
Ii`1 “ Ii ` h fi `
h2
2! f1
i `
h3
3! f 2
i ` Oph2q (8.12)
Then, recalling the following forward difference formula for f1
i (see Chapter 5)
f1
i – fi`1 ´ fi
h ´ h
2
f 2
i ` Oph2q
Substituting it into Eq. (8.12) and simplifying yields
Ii`1 “ Ii `
h
2
pfi`1 ` fiq ´ h3
12 f 2
i ` Oph4q (8.13)
Using Eq. (8.10), the definite integral of fpxq on [xi, xi`1] can now be expressed as
ż xi`1
xi
fpxqdx “
ż xi`1
a
fpxqdx ´
ż xi
a
fpxqdx “ Ii`1 ´ Ii
Letting Ai`1 “ Ii`1 ´ Ii and substituting it into Eq. (8.13) results in
Ai`1 “ h
2
pfi`1 ` fiq ´ h3
12 f 2
i ` Oph4q (8.14)
Notice that hpfi`1`fiq{2 is the area of fpxq estimated by the trapezoidal rule in the interval
[xi, xi`1]. The other term in Eq. (8.14) provides a concrete measure for the magnitude of
the incurred error. Applying this result to all panels on ra, bs, we get
TN “ ÿ
N
i“1
Ai “ h
´f0 ` fN
2 `
N
ÿ´1
i“1
fi
¯
´ h3
12
N
ÿ´1
i“0
f 2
i ` Oph4q (8.15)
where the first term of Eq. (8.15) corresponds to the composite trapezoidal rule, and the
second term represents the leading error.
To develop a simpler and more manageable expression, the mean value theorem is
applied to the error term as follows:
N
ÿ´1
i“0
f 2pxiq “ N f 2pξq ” b ´ a
h f 2pξq, a ď ξ ď b (8.16)
where N “ pb´aq{h.418  Numerical Methods for Scientists and Engineers
Making use of Eq. (8.16) in Eq. (8.15) gives rise to
RN ” ´h3
12
N
ÿ´1
i“0
f 2
i “ ´h2
12 pb ´ aqf 2pξq, a ď ξ ď b (8.17)
where RN will be referred to as the dominant or global error of the N-panel Trapezoidal
rule. Notice that the magnitude of the error is second order, i.e., RN ” Oph2q.
Finally, the N´panel trapezoidal rule with the order of global error is expressed as
TN “ h
ˆf0 ` fN
2 `
N
ÿ´1
i“1
fi
˙
` Oph2q (8.18)
It is clear that the order of global error will be Oph4q if the dominant error, Eq. (8.17), is
somehow incorporated into the composite rule:
TN “ h
ˆfpaq ` fpbq
2 `
N
ÿ´1
i“1
fpxiq
˙
` RN ` Oph4q (8.19)
Trapezoidal Rule with End Correction: To include the dominant error in Eq. (8.18),
we note that the second derivative in Eq. (8.17) is unknown; however, applying the mean
value theorem to f1
pξq gives
f 2pξq “ f1
pbq ´ f1
paq
b ´ a (8.20)
Substituting Eq. (8.20) first into Eq. (8.17) and then into Eq. (8.19) leads to
CTN “ h
2
´
fpaq ` fpbq ` 2
N
ÿ´1
i“1
fpxiq
¯
´ h2
12
`
f1
pbq ´ f1
paq
˘
` Oph4q (8.21)
which is a fourth-order accurate approximation referred to as the N-panel Trapezoidal rule
with end correction. This name derives from the inclusion of f1
paq and f1
pbq in the approx￾imation. Equation (8.21) can be applied without much additional computation, provided
that the true derivatives of the integrand at the end points are known. Clearly, CTN can
be used for estimating the integral of functions explicitly defined so that the true values of
the first derivatives at the end points can be evaluated exactly. The first derivatives can be
approximated with the finite differences, provided that at least fourth-order accurate finite
difference formulas are used. However, fourth order accuracy cannot be assured.
In numerical integration of smooth functions, the composite trapezoidal rule may be
sufficient to achieve the desired level of accuracy in most engineering applications. If highly
accurate results are sought, the composite trapezoidal rule TN with a large N can demand
a great deal of computational effort. If f1
paq and f1
pbq can be easily found, the composite
rule with end correction CTN may offer a remedy. To estimate an integral only once, the
computational cost of using a very large N may be bearable and justified. However, when
integrating many integrals or a single integral numerous times or integrands that are time￾consuming to evaluate, we need to implement efficient numerical methods of high-order
accuracy, i.e., Oph4q, Oph6q, and so on.Numerical Integration  419
Trapezoidal Rule
‚ The trapezoidal rule is simple and easy to apply;
‚ It is a weighted sum of the integrand for a specified number of inte￾gration points;
‚ Cpu-time is the minimum for uniformly spaced discrete functions;
‚ There is no restriction on the number of panels to be used;
‚ As the number of panels increases, the numerical estimates of a con￾tinuous function always approach the true value;
‚ Since they are 2nd and 4th order accurate methods, TN and CTN give
the true integral value for polynomials of degree m ď 1 and m ď 3,
respectively.
‚ Error analysis is straightforward.
‚ The integrands must be continuous;
‚ The rate of convergence of TN is rather slow;
‚ Integrands depicting oscillations and/or sharp changes require a large
number of panels to yield sufficiently accurate estimates.
EXAMPLE 8.1: Application of Trapezoidal Rule in Thermodynamics
The tendency of a gas to escape or expand is explained by the fugacity property of
the gas. For ideal gases, fugacity f is equal to its pressure, but in real gases, it is
computed by the following integral:
ln ˆ f
P
˙
“
ż P
0
Zpxq ´ 1
x
dx
where P is the pressure, Z is the compressibility factor, and f{P is referred to as the
fugacity coefficient. The data on the compressibility factor of a real gas at a constant
temperature are fitted to the curve given below:
Zppq “ 1 ´ 5 ˆ 10´4 p e´p{50, 0 ă p ă 400 atm
Estimate lnpf{Pq for P “ 400 atm using the 8-panel Trapezoidal rule with and
without end correction. Calculate the true error and the global error bounds for
both cases.
SOLUTION:
Substituting Zppq into the integral and simplifying gives
ż P
0
Zpxq ´ 1
x
dx “ ´5 ˆ 10´4
ż P
0
x e´x{50 dx
The true value of the above integral is 0.025pP ` 50q e´P {50 ´ 1.25 for P “ 400
atm, which results in 2492.4521. Dividing the interval (0,400) into 8 panels yields
h“ pP ´0q{N “400{8“50 atm.
To estimate the integral with the N-panel trapezoidal rules, a set of discrete
data is generated from Fpxq “xe´x{50. To use Eq. (8.9), the weights are defined as
w0 “w8 “25 and wi “50 for i“1 to 7.420  Numerical Methods for Scientists and Engineers
TABLE 8.1
i xi Fi wi wiFi
0 0 0 25 0
1 50 18.394 50 919.6986
2 100 13.5335 50 676.6764
3 150 7.46806 50 373.4030
4 200 3.66313 50 183.1564
5 250 1.68449 50 84.2243
6 300 0.74363 50 37.1813
7 350 0.31916 50 15.9579
8 400 0.13418 25 3.3546
ΣwiFi “ 2293.6526
The abscissas (xi “50 i), the integrand for the integration points (Fi “Fpxiq), and
the wiFi products are calculated and tabulated in Table 8.1. The sum of the last
column is the approximation obtained with the 8-panel trapezoidal rule:
T8 “ ÿ
8
i“0
wiFi “ 2293.6526
To estimate the global error with Eq. (8.17), we require F2pξq. However, the F2pξq
cannot be truly determined because the specific value of ξ is unknown. Nonetheless,
we can anticipate that it will be somewhere between its maximum and minimum
values within the interval of integration.
FIGURE 8.4
The distribution of F2pξq on the interval 0 ď x ď 400 is plotted in Fig. 8.4. It is
observed that F2pξq has an absolute minimum value of ´1{25 at x “ 0 and a local
maximum value of 1{50e3 at x“150. The upper and lower error bounds for h“50
and N “8 are found from Eq. (8.17) as follows:
R5,max “ ´p50q
2
12 p400 ´ 0q
´
´ 1
25
¯
“ 3333.33
R5,min ” ´p50q
2
12 p400 ´ 0q
´ 1
50e3
¯
“ ´82.98
which indicates that the true error (2492.4521´2293.6526=198.7996) is within the
bounds.Numerical Integration  421
Now we apply the end-correction formula. Substituting F1
p0q “1 and F1
p400q “
´7{e3 into the correction term and then into Eq. (8.21), we find
correction“´h2
12
`
F1
p400q ´ F1
p0q
˘
“´p50q
2
12
´
´ 7
e8 ´ 1
¯
“ 208.823
and finally
CT8 “ 2293.6526 ` 208.823 “ 2502.475
which yields the true error of ´10.02291.
Substituting these estimates into the integral in question, we obtain
ż P
0
Zpxq ´ 1
x
dx “ ´5 ˆ 10´4
ż P
0
x e´x{50 dx “
$
&
%
´1.2462260, True integral
´1.1468125, Ð T8
´1.2512375, Ð CT8
Discussion: We find that the true absolute errors for the trapezoidal rule without
and with the end correction are 0.0994135 and 0.005011, respectively. The true error
has been reduced about 20-fold with the use of the end correction formula. It is
also seen that the true error of the estimate with the trapezoidal rule is within the
theoretical error bounds given by Eq. (8.17). However, determining the error bound
is often not an easy task because higher-order derivatives of many functions can be
difficult to derive or impossible to analyze, even when derived to determine the local
extremums.
Pseudocode 8.1
Module TRAPEZOIDAL_RULE_RF (a, b, n, intg, intgc)
\ DESCRIPTION: A pseudo-module to estimate the definite integral of y “ fpxq
\ on [a, b] using composite Trapezoidal rule with/without end correction.
\ USES:
\ FX and FU :: User-defined functions providing fpxq and f1
pxq, respectively.
h Ð pb ´ aq{n \ Find panel width
intg Ð 0.5 ˚ pFXpaq ` FXpbqq \ Initialize intg with avg of endpoints
xi Ð a \ Initialize abscissa, x0 “ a
For “
i “ 1, n ´ 1
‰ \ Accumulator loop
xi Ð xi ` h \ Accumulate h to get xi
intg Ð intg ` FXpxiq \ Accumulate Fpxiq’s to get intg
End For
intg Ð h ˚ intg \ Estimate integral by Eq. (8.18)
corr Ð ´h ˚ h ˚ pFUpbq ´ FUpaqq{12 \ Compute the correction term
intgc Ð intg ` corr \ Estimate integral by Eq. (8.21)
End Module TRAPEZOIDAL_RULE_RF
Function Module FX (x) \ User-defined Function module giving fpxq
FXÐ x ˚ EXPp´x{50q \ Built-in function EXP(x) is used
End Function Module FX
Function Module FU (x) \ User-defined Function module giving f1
pxq
FUÐ p50 ´ xq ˚ EXPp´x{50q{50
End Function Module FU422  Numerical Methods for Scientists and Engineers
In Pseudocode 8.1, a pseudomodule, TRAPEZOIDAL_RULE_RF, is provided for esti￾mating the definite integral of an explicitly given continuous function using the composite
Trapezoidal rule with and without end correction. The module requires the number of panels
(n) and the integration interval (a, b) as input. The integrand, as well as its derivative, which
is required in the end correction formula, are supplied by a couple of user-defined functions,
FX and FU, which are exemplified here for Example 8.1. The module generates uniformly
spaced n panels and calculates the abscissas and the ordinates for n ` 1 integration points.
The algorithm is straightforward in that the integral of FX(x) on (a, b) is estimated with
the trapezoidal rule without and with the end correction, Eqs. (8.19) and Eq. (8.21). Note
that no array variables are used for abscissas and the integrand in order to reduce memory
allocation.
A separate module is not given here for uniformly spaced discrete functions as inte￾grands. A typical argument list requires the number of integration points n (note that n is
the number of integration points, not the number of panels), the panel width h, and the dis￾crete function f (fi for i “ 1, 2,...,n) as input. In the module, the abscissas are no longer
required because the discrete function is already provided as an array (i.e., fpxiq “ fi).
Thus, it is sufficient to specify the panel width to carry out the numerical integration.
Likewise, for the same reasons, the function modules FX and FU are not required. The
integral (intg) of f on rx1, xns is the only output of the module, and it is computed by
Eq. (8.18). The trapezoidal rule with end correction requires accurate computation of the
first derivatives at end points, which is not recommended for the integrands described by
discrete functions unless the derivatives are evaluated with a high order (higher than 4th
order) finite-difference approximation.
8.2 SIMPSON’S RULE
An analyst can arbitrarily determine the number of panels (or panel width) when numer￾ically integrating a continuous explicit function. In other words, it is up to the analyst to
use 100, 1000, or millions of panels so that an integral is approximated to a specified ac￾curacy level. However, the number of panels (or the interval size) in a discrete function is
fixed; that is, the number of data points can neither be arbitrarily increased nor the panel
width reduced. Hence, to obtain estimates with improved accuracy, alternative integration
formulas using the available data sets are required.
From this point on, we will seek higher-order integration schemes yielding smaller global
errors for a fixed number of panels. We have so far learned that connecting the endpoints
of a panel with a straight line and calculating the area of the resulting trapezoid led to the
trapezoidal rule. By any means, a line is not well suited to approximate a curve in a large
interval; therefore, the approximation error (the gap between the curve and the line) can be
large. As an interpolating function, a parabola may be used to reduce this gap, but fitting
a curve with a unique parabola requires three data points (or two panels, see Fig. 8.5a). As
depicted qualitatively in Fig. 8.2b, the integration error is markedly reduced in comparison
to the trapezoidal rule.
The fitting of three points to a parabola has been covered in Section 5.5. In Fig. 8.5a,
the equation of a parabola centered about x1 can be expressed as
fpxq – f 2
1
2
x2 ` f1
1 x ` f1, (8.22)
where f1
1 and f 2
1 denote central finite difference approximations.Numerical Integration  423
FIGURE 8.5: (a) Fitting a parabola to a two-panel interval, (b) fitting of two parabolas in a four￾panel case.
Now, integrating the parabola on rx0, x2s yields the following approximation for the
area under the curve shown in Fig. 8.5a:
ż x2
x0
fpxqdx –
h3
3 f 2
1 ` 2hf1 “ h
3 pf0 ` 4f1 ` f2q (8.23)
where f 2
1 « pf2 ´ 2f1 ` f0q{h2 is incorporated in Eq. (8.23). This approximation is referred
to as the Simpson’s Rule or Simpson’s 1/3 rule due to the “1/3” factor.
To improve the accuracy of the numerical integration, we double the number of panels,
i.e., halve the panel width. This time, two interpolating parabolas are required to integrate
over rx0, x4s. As shown in Fig. 8.5b, the polynomials p2,1pxq and p2,2pxq are defined on
the intervals rx0, x2s and rx2, x4s, respectively. The area on [x0, x4] is then obtained by
employing Simpson’s rule on the areas A1 and A3 as follows:
A1 ` A3 “
ż x2
x0
p2,1pxqdx `
ż x4
x2
p2,2pxqdx –
h
3 pf0 ` 4f1 ` 2f2 ` 4f3 ` f4q (8.24)
where A1 “ ph{3qpf0 ` 4f1 ` f2q and A3 “ ph{3qpf2 ` 4f3 ` f4q.
In order to extend Simpson’s rule to many panels, the number of panels obviously needs
to be “even.” Also, note that the integrands with odd and even subscripts are multiplied by
4 and 2, respectively. With this observation, a generalized approximation is obtained as
SN “ h
3
N
ÿ
{2
i“1
´
f2i´2 ` 4f2i´1 ` f2i
¯
“ h
3
˜
f0 ` fN ` 4
N
ÿ´1
i“1
odd
fi ` 2
N
ÿ´2
i“2 even
fi
¸
(8.25)
where fi “ fpxiq and xi “ a ` ih for i “ 0, 1, 2,...,N. This equation, Eq. (8.25), is called
N-panel (or Composite) Simpson’s (1/3-)Rule.
We anticipate the numerical integration with Simpson’s rule to provide a marked im￾provement over the trapezoidal rule. Nonetheless, at this stage, we are not yet able to
quantify either the approximation error or its magnitude. In order to assess the numerical
error incurred, we resort to the Taylor series again. We expand Ipxq, defined by Eq. (8.10),
to Taylor series at xi about xi´1 and xi`1:
Ipxi`1q “ Ii`1 “ Ii ` hfi `
h2
2! f1
i `
h3
3! f 2
i `
h4
4! f 3
i `
h5
5! fp4q
i `
h6
6! fp5q
i ` Oph7q (8.26)
Ipxi´1q “ Ii´1 “ Ii ´ hfi `
h2
2! f1
i ´ h3
3! f 2
i `
h4
4! f 3
i ´ h5
5! fp4q
i `
h6
6! fp5q
i ` Oph7q (8.27)
where we made use of I1
pxq“fpxq, I2pxq“f1
pxq, and so on.424  Numerical Methods for Scientists and Engineers
Subtracting Eq. (8.27) from Eq. (8.26), we get
Ai “
ż xi`1
xi´1
fpxqdx “ Ii`1 ´ Ii´1 “ 2hfi `
h3
3 f 2
i `
h5
60 fp4q
i ` Oph7q (8.28)
Next, recalling the 4th-order-accurate CDF for f 2
i is given by
f 2
i “ fi`1 ´ 2fi ` fi´1
h2 ´ h2
12 fp4q
pηiq ` Oph4q, xi´1 ď ηi ď xi`1 (8.29)
We substitute Eq. (8.29) into Eq. (8.28) and simplify to obtain
Ai “ h
3 pfi`1 ` 4fi ` fi´1q ´ h5
90 fp4q
pηiq ` Oph7q (8.30)
Note that the first term of Eq. (8.30) corresponds to the numerical integration of fpxq on
rxi´1, xi`1s with the Simpson’s rule; that is, Ai accounts for the area of the subinterval
with two panels. Thus, to find the approximation of the integral in rx0, xN s, all we need to
do is sum up the odd Ai’s over N/2 subintervals.
A general expression for N-panels (provided that N is even) on [a, b] yields
SN “
N
ÿ
{2
i“1
A2i´1 “ h
3
´
f0`fN `4
N
ÿ
{2
i“1
f2i´1`2
N
ÿ
{2´1
i“1
f2i
¯
´ h5
90
N
ÿ
{2
i“1
fp4q
pη2i´1q`N
2 Oph7q (8.31)
The dominant error term in Eq. (8.31) can be treated in the same way as the dominant
error term in the trapezoidal rule. Thus, the dominant error term yields
RN “ ´h5
90
N
ÿ
{2
i“1
fp4q
pη2i´1q “´N
2
´h5
90 fp4q
pξq
¯
“´ pb ´ aq
180
h4fp4q
pξq (8.32)
This last expression is the global error, which covers the entire range [a, b]. Also note that
η2i´1 px2i´2 ď η2i´1 ď x2iq refers to an abscissa shared by two panels; therefore, the
number of abscissas (terms) in the sum is N/2. As a result, the sum has been replaced by
pN{2qfp4qpξq where a ď ξ ď b. By inspection of Eq. (8.32), the order of global error is said
to be fourth order, Oph4q.
Finally, incorporating Eq. (8.32) in Eq. (8.31), an approximation for the definite integral
of fpxq on [a, b] with the N-panel Simpson’s rule can be written as
SN “ h
3
`
f0 ` fN
˘
`
4h
3
N
ÿ´1
i“1
odd
fi `
2h
3
N
ÿ´2
i“2 even
fi ` Oph4q (8.33)
which has the appropriate form to be applied to discrete functions. Equation (8.33) can also
be cast for explicit continuous functions as
SN “ h
3
`
fpaq ` fpbq
˘
`
4h
3
N
ÿ
{2
i“1
fpx2i´1q ` 2h
3
N
ÿ
{2´1
i“1
fpx2iq ` Oph4q (8.34)
Simpson’s 1/3-rule can be expressed as a weighted-sum by definings w0 “ wN “ h{3,
wi “4h{3 for odd i’s, and wi! “2h{3 for even i’s.Numerical Integration  425
Simpson’s Rule with End Correction: Since the dominant error term in Eq. (8.31)
involves the fourth derivative, attempting to find an end correction by approximating this
error term is impractical for several reasons. Nevertheless, an improved Simpson’s formula
can be found by constructing a fourth-degree polynomial approximation p4pxq, which passes
through the points px0, f0q, px1, f1q, and px2, f2q (see Fig. 8.5a). To be able to define a unique
polynomial, two additional constraints not requiring extra integration points are needed.
To derive new constraints, we note that the trapezoidal rule with the end correction, Eq.
(8.21), involves the first derivatives at the end points. Thus, we may also require that the
first derivatives of the integrand at the end points of each double panel (f1
0 and f1
2) be known.
Finally, the constraints sufficient to obtain a fourth-degree polynomial approximation can
be summarized as follows:
f0 “p4px0q, f1 “p4px1q, f2 “p4px2q, f1
0 “p1
4px0q, f1
2 “p1
4px2q,
The system of linear equations for five unknowns (i.e., the coefficients of p4pxq) is solved
by satisfying the five constraint equations above. Integrating the approximating polynomial,
p4pxq, over the two panels, rx0, x2s, yields
ż x2
x0
fpxqdx –
ż x2
x0
p4pxqdx “ h
15
´
7f0 ` 16f1 ` 7f2 ` h pf1
0 ´ f1
2q
¯
`
h7fp6q
pηq
4725 (8.35)
where x0 ď η ď x2. However, the error term in this equation has been derived from the
Taylor series analysis. Note that this formula is analogous to the trapezoidal rule with end
correction, which is derived by including the error term of the trapezoidal rule, whereas Eq.
(8.35) is not based on an estimation of the dominant error term [21].
To obtain the composite rule, we construct N/2 continuous subintervals, each contain￾ing two panels. When the integrals over rx2i´2, x2is subintervals are summed up, the first
derivatives pairwise cancel out at all interior points with the exception of the first and the
last one. Using the shorthand notation, we finally obtain
CSN “ h
15
´
7 pf0 ` fN q ` 16
N
ÿ´1
i“1
odd
fi ` 14
N
ÿ´2
i“2 even
fi ´ h
`
f1
pbq ´ f1
paq
˘¯
` Oph6q (8.36)
which is referred to as N-Panel (or Composite) Simpson’s rule with end correction. Note
that N-panel Simpson’s rule with end correction, which also requires an even number of
panels, is sixth-order accurate, and it can be readily applied without much additional work.
A note on Simpson’s Second Rule (3/8 rule): Another version of Simpson’s rule is
developed with three-panels (or equally spaced four integration points), as shown in Fig.
8.6. A cubic interpolating polynomial passing through all four points is constructed and
then integrated over rx0, x3s to yield
"
S3 “ 3h
8
´
f0 ` 3f1 ` 3f2 ` f3
¯
´ 3h5
80 fp4q
pξq, x0 ă ξ ă x3 (8.37)
where h “ pb ´ aq{3 and "
S3 is an approximation referred to as Simpson’s 3/8 rule, named
after the “3/8” factor.
This approximation can be extended to N/3 intervals (each containing 3 panels), which
leads to N-panel (or composite) Simpson’s 3/8 rule:
"
SN “ 3h
8
˜
f0 ` fN ` 3
N
ÿ´2
i“1,4,7,.
fi ` 3
N
ÿ´1
i“2,5,8,.
fi ` 2
N
ÿ´3
i“3,6,9,.
fi
¸
` Oph4q (8.38)426  Numerical Methods for Scientists and Engineers
Simpson’s 1/3 Rule
‚ Simpson’s rule is relatively simple and easy to employ;
‚ Due to being 4th and 6th-order accurate methods, the SN and CSN
yield the true integral values for polynomials of degree m ď 3 and
m ď 5, respectively;
‚ Error analysis is straightforward.
‚ Integrands must be continuous;
‚ The number of panels is restricted to even numbers for the 1/3 rule
and multiples of 3 for the 3/8 rule;
‚ The formula with the end correction cannot be applied to discrete
functions unless the first derivatives are computed using a high-order
finite difference formula (ě6).
FIGURE 8.6: Simpson’s three-panel integration.
which is a fourth-order formula with a global error of RN “ ´pb´aqh4fp4qpξq{80 that is
larger than that of N-panel Simpson’s 1/3 rule, RN “´pb´aqh4fp4qpξq{180. For this reason,
whenever possible, Simpson’s 1/3 rule should be preferred, instead of the 3/8 rule using
four integral points, as it provides better accuracy with three integral points.
The total number of panels in the 3/8 rule should be in multiples
of “3.” In practice, when N is even, the 1/3 rule is preferred due
to its smaller global error. When N is odd, the Simpson’s 3/8
rule is only applied to the last three panels, while the 1/3 rule is
employed for the rest.
A pseudomodule, SIMPSONS_RULE_DF, to estimate the integral of a uniformly spaced
discrete function using composite Simpson’s rule is given in Pseudocode 8.2. The module
requires the number of panels (n), the panel width (h), and the discrete function fiq for
i“0, 1, 2,...,n as input. The Simpson’s 1/3 rule requires the number of panels to be “even.”
As an input validity check, it is prudent to test whether n is even or not right at the top
of the module. (In cases where the number of panels is odd, the 1/3 rule is applied to all
panels except the last three panels, and the 3/8 rule is adapted to the last three panels.
Implementing this strategy in the module is left as an exercise for the reader.) The integral
of the discrete function over px0, xnq with the 1/3 rule (intg) is calculated by Eq. (8.33)
using two accumulator loops (odd and even as the accumulator variables) for the odd- and
even-subscripted integrands.Numerical Integration  427
Pseudocode 8.2
Module SIMPSONS_RULE_DF (n, h,f, intg)
\ DESCRIPTION: A pseudocode to estimate the integral of a uniformly spaced
\ discrete function fpxq on [x1, xn] using the composite Simpson’s Rule.
Declare: fn \ Declare array variables
odd Ð 0 \ Initialize accumulator variable, odd
For “
i “ 1, n ´ 1, 2
‰ \ Accumulator loop to accumulate fi’s for i “ 1, 3, 5, ...
odd Ð odd ` fi \ Accumulate fi’s with odd
End For
even Ð 0 \ Initialize accumulator variable, even
For “
i “ 2, n ´ 2, 2
‰ \ Accumulator loop to accumulate fi’s for i “ 2, 4, 6, ...
even Ð even ` fi \ Accumulate fi’s with, even
End For
intg Ð f0 ` fn ` 4 ˚ odd ` 2 ˚ even \ Apply Eq.(8.33)
intg Ð intg ˚ h{3 \ Estimated integral value
End Module SIMPSONS_RULE_DF
A separete module is not presented here for the numerical integration of an explicitly
defined continuous function. Such a module will require the number of panels (n) and the
integration interval [a, b] as input. Similar to Pseudocode 8.1, the integrand and its first
derivative should be supplied to the module by a couple of user-defined functions. The
abscissas and a set of discrete data points (i.e., integrand) should be prepared before the
even and odd-subscripted fi’s are accumulated in accordance with Eq. (8.34), using the odd
and even accumulator variables. If desired, the end correction formualtion, Eq. (8.36), can
be implemented here.
EXAMPLE 8.2: Trapezoidal/Simpson’s 1/3 rules with/without end correction
The surface area of the three-dimensional body generated by rotating the y“2
?sin x
curve about the x-axis over [0,π] is to be estimated. The problem setup leads to the
following integral:
I “ 4π
ż π
0
asin x ` cos2 xdx
Excluding the 4π factor, estimate the definite integral using uniform 4-, 6-, and 10-
panel Trapezoidal and Simpson’s rules with and without end correction. Discuss the
accuracy of T10 and S10? The true integral is 3.346868487781.
SOLUTION:
For N “ 10, we have 11 integration points, whose abscissas are xi “ ih, where
h “ pb´aq{N “ π{10. Since the number of panels is even, the Simpson’s 1/3 rule is
suitable for this exercise.
The T10 and S10 can be explicitly written as
T10 “ h
´f0 ` f10
2 ` f1 ` f2 ` ... ` f8 ` f9
¯
S10 “ h
3
`
f0 ` f10˘
`
4h
3
´
f1 ` f3 ` f5 ` f7 ` f9
¯
`
2h
3
´
f2 ` f4 ` f6 ` f8
¯
Both formulas can be expressed as weighted-sums, i.e., I « Σwifi.428  Numerical Methods for Scientists and Engineers
TABLE 8.2
Trapezoidal Rule Simpson’s Rule
i xi fi wi wifi wi wifi
00 1 h/2 0.157079 h/3 0.104720
1 π{10 1.101601 h 0.346078 4h/3 0.461438
2 2π{10 1.114582 h 0.350156 2h/3 0.233438
3 3π{10 1.074481 h 0.337558 4h/3 0.450077
4 4π{10 1.023009 h 0.321388 2h/3 0.214259
5 5π{10 1 h 0.314159 4h/3 0.418879
6 6π{10 1.023009 h 0.321388 2h/3 0.214259
7 7π{10 1.074481 h 0.337558 4h/3 0.450077
8 8π{10 1.114582 h 0.350156 2h/3 0.233438
9 9π{10 1.101601 h 0.346078 4h/3 0.461438
10 π 1 h/2 0.157079 h/3 0.104720
Σwifi “ 3.33867954 Σwifi “ 3.34674092
For N “10, the abscissas xi, corresponding integrands fi “fpxiq, and wifi prod￾ucts are calculated and tabulated in Table 8.2. The approximations for each method,
expressed as weighted sums Σiwifi, are available in the last row as 3.33867954 and
3.34674092 for the trapezoidal and Simpson’s 1/3 rules, respectively. The correspond￾ing true errors are 0.0081889 and 0.0001276 for T10 and S10, respectively.
TABLE 8.3
Method N Estimate Absolute Error
Trapezoidal Rule, TN 4 3.29660530 0.0502632
6 3.32428153 0.0225870
10 3.33867954 0.0081889
Trapezoidal Rule, CTN 4 3.34800949 0.0011410
(with end correction) 6 3.34712783 0.0002594
10 3.34690421 0.0000357
Simpson’s 1/3 Rule, SN 4 3.34827618 0.0014077
6 3.34578851 0.0010800
10 3.34674092 0.0001276
Simpson’s 1/3 rule, CSN 4 3.34806282 0.0011943
(with end correction) 6 3.34685997 8.52ˆ10´6
10 3.34687155 3.06ˆ10´6
The first derivative of the integrand at the end points, required for Eqs. (8.21)
and (8.36), are respectively obtained as f1
p0q“1{2, f1
pπq“´1{2. Using Eqs. (8.18),
(8.21), (8.33), and (8.36), the TN , CTN , SN , and CSN are computed and presented in
Table 8.3 for N “4, 6, and 10. It is observed that as the number of panels is increased
(or panel width is decreased), the estimates approach the true value regardless of
the method used.Numerical Integration  429
Discussion: As long as the number of panels is even (as in this case), the perfor￾mance of Simpson’s 1/3 rule for the same number of panels is better than that of
the trapezoidal rule due to its fourth-order accuracy. On the other hand, the CTN
or CSN approximations (with the end corrections) depict a significant improvement
over the ordinary TN or SN approximations. The Simpson’s 1/3 rule with end cor￾rection, having an order of error of Oph6q, yields superior estimates.
In Example 8.1, it was relatively easy to obtain F2pxq to calculate the dominant
error term. In this example, we have not attempted to determine the error bounds
because the high-order derivatives of the integrand here are difficult to obtain and
analyze. In reality, the typical error estimation of an approximation is determined
by using two or more estimates with different numbers of panels, i.e., |T6 ´ T4|,
|T10 ´ T6|, or |T10 ´ T4|, and so on. We could also use, say, T10, S10, or CT10 to
compare with a better solution such as CS10 and use the difference to estimate
the error, i.e., |CT10 ´ T10| or |CT10 ´ S10|, or |CT10 ´ CS10|, where theoretically
CT10 is the most accurate one. If the error predicted in this manner satisfies a
predetermined convergence criterion, then the integration procedure is considered
complete.
Even though Simpson’s 1/3 rule is Oph4q, it may not always
be superior to the trapezoidal rule. Recall that the global error
of the Trapezoidal and Simpson’s rules depends on the 2nd and
4th derivatives, respectively. In cases where the integrand is not
smooth, higher derivatives and thus the truncation error may be￾come very large. That is why, before applying end correction for￾mulas, make sure that the integrand is sufficiently smooth; other￾wise, there may be little or no advantage at all in using a so-called
the “better rule.”
8.3 ROMBERG’S RULE
Romberg’s rule is a powerful numerical technique to approximate a definite integral. It is a
recursive procedure based on the trapezoidal rule that improves the numerical approxima￾tions by employing the Richardson extrapolation technique.
Before introducing the method, it is important to analyze the truncation error of the
trapezoidal rule. For this purpose, consider a sufficiently differentiable continuous function,
fpxq. The N-panel trapezoidal rule, Eq. (8.19), can be rewritten as
I “ h
2
ˆ
fpaq ` fpbq ` 2
N
ÿ´1
i“1
fpa ` ihq
˙
` K1h2 ` K2h4 ` K3h6 ` ... (8.39)
where I is the true integral value, K1, K2, K3,... are the coefficients containing higher
order derivatives of fpxq, and the first term on the rhs is the N-panel Trapezoidal rule,
which is a function of panel width, i.e., TN phq.
Equation (8.39) can then be rewritten as
TN phq “ I ´ K1h2 ´ K2h4 ´ K3h6 ` ... (8.40)
Now consider two numerical estimates, TN ph1q and TMph2q, with panel widths of h1 and430  Numerical Methods for Scientists and Engineers
h2:
TN ph1q “ I ´ K1 h2
1 ´ K2 h4
1 ´ K3 h6
1 ` ... (8.41)
TMph2q “ I ´ K1 h2
2 ´ K2 h4
2 ´ K3 h6
2 ` ... (8.42)
Next, using Eq. (8.41), we estimate the integral with twice the panel width (i.e., h1 “ 2h2):
TN p2h2q “ I ´ 4K1 h2
2 ´ 16K2 h4
2 ´ 64K3 h6
2 ` ... (8.43)
For h2 case, setting M “ 2N, we can then use a linear combination of Eq. (8.42) and Eq.
(8.43) to eliminate Oph2q terms as follows:
4T2N ph2q ´ TN p2h2q
3 “ I ` 4K2h4
2 ` 20K3 h6
2 ` ... “ S2N ph2q (8.44)
which becomes Oph4q. It should be noted that this approximation corresponds to the com￾posite Simpson’s 1/3 rule, which is S2N ph2q. The technique of improving the order of ac￾curacy using the two estimates of I, is known as the Richardson Extrapolation (see Section
5.7).
This integral can also be extrapolated by setting h3 “ h2{2:
4T4N ph3q ´ T2N p2h3q
3 “ I ` 4K2h4
3 ` 20K3 h6
3 ` ... “ S4N ph3q (8.45)
which is also fourth-order accurate.
The Richardson extrapolation technique can be applied to Simpson’s rule at any stage
to obtain an approximation of higher accuracy. For instance, in Eq. (8.45), setting h3 “ h2{2
and using Eq. (8.44), the elimination of Oph4q terms yields
16S4N ph2{2q ´ S2N ph2q
15 “ I ´ K3 h6
2 ` ...
which is sixth-order accurate, i.e., Oph6q.
The Richardson extrapolation technique can be indefinitely repeated, with each step of
the extrapolation yielding a new numerical estimate that is more accurate than the previous
one, i.e., Oph8q, Oph10q, etc. The successive evaluation of a numerical integral in this manner
is known as Romberg’s Rule. Now, we seek to express the preceding formulation in a suitable
form that can be incorporated into a numerical algorithm. Since the first step of Romberg’s
rule is the trapezoidal rule, the first step will be expressed as
Rk,1 “ h
2
ˆ
fpaq ` fpbq ` 2
ÿ
M
i“1
fpa ` ihq
˙
(8.46)
where h“ pb´aq{M is the panel width, M denotes the number of panels, which is defined
as M “ 2k´1, and the subscript “k” controls the panel width. Note that for increasing k,
the number of panels increases by two folds (or the panel width is cut by half), which is
necessary to employ the Richardson extrapolation. For k “1, 2 and 3 (or M “1, 2 and 4),
Eq. (8.46) can be expressed as
R1,1 “T1 “ b ´ a
2
´
fpaq`fpbq
¯
R2,1 “T2 “ b ´ a
4
´
fpaq`fpbq`2f
`
a`
b ´ a
2
˘¯
R3,1 “T4 “ b ´ a
8
´
fpaq`fpbq`2f
`
a`
b ´ a
4
˘
`2f
`
a`
b ´ a
2
˘
`2f
`
a`
3pb ´ aq
4
˘¯
(8.47)Numerical Integration  431
It is clear that some of the function evaluations in Rk,1 overlap with those of Rk´1,1. To
reduce the computational effort, for instance, Equation (8.47) can be rearranged to allow
recursive representation as
R2,1 “ R1,1
2 `
b ´ a
2 f
´
a `
b ´ a
2
¯
(8.48)
R3,1 “ R2,1
2 `
b ´ a
4
ˆ
f
´
a `
b ´ a
4
¯
` f
ˆ
a `
3pb ´ aq
4
˙˙ (8.49)
Comparing Eqs. (8.48) and Eq. (8.49), a general recurrence relation for the trapezoidal
rule is found as
Rk,1 “ Rk´1,1
2 ` h
M
ÿ
{2
i“1
f
`
a ` p2i ´ 1qh
˘
, k “ 2, 3, 4 ... (8.50)
which provides an efficient way of incrementally computing the trapezoidal rule.
The Richardson extrapolation formula can now be generalized as follows:
Rk,n “ 4n´1Rk,n´1 ´ Rk´1,n´1
4n´1 ´ 1 “ Rk,n´1 `
Rk,n´1 ´ Rk´1,n´1
4n´1 ´ 1 , n ě 2 (8.51)
where index n signifies the level of extrapolation, i.e., n“1 is linear (Trapezoidal), n“2 is
quadratic (Simpson), and so on.
For n “ 2 and 3, the Richardson extrapolations yield
R2,2 “ 4R2,1 ´ R1,1
3 ` Oph4q, R3,2 “ 4R3,1 ´ R2,1
3 ` Oph4q, ...
R3,3 “ 16R3,2 ´ R2,2
15 ` Oph6q, R4,3 “ 16R4,2 ´ R3,2
15 ` Oph6q, ¨¨¨
The Romberg estimates, as depicted in Table 8.4, can be arranged in a tabular form,
referred to as the Romberg Table. A Romberg table is a lower triangular form that ex￾tends down and across. It can be built either column-by-column or row-by-row. First, the
estimates from the trapezoidal rule (Rk,1) are placed in the first column in the order of
increasing k. The number of rows placed in the first column determines how large the table
will be. The subsequent columns are obtained from Eq. (8.51) with only a few arithmetic
operations. In fact, the majority of the computations (i.e., cpu-time) in Romberg’s rule are
devoted to construct the first column. The panel width for the first row is h “ b´a, and
in the subsequent rows, the panel width is halved. Because the first column corresponds to
the composite trapezoidal rule, the order of accuracy of the first column is Oph2q. Then,
Rk,2’s corresponding to the composite Simpson’s rule with Oph4q are placed into the second
column. With each subsequent column added, the order of error increases by 2. Notice that
the order of accuracy of each column has been identified at the top of the Romberg table
in Table 8.4. Arrows indicate the direction of improvements in the estimates. The accuracy
of the estimates increases as we go down each column due to decreasing h. As we move
to the right along each row (or diagonally), the accuracy increases due to the increasing
order of accuracy. Hence, a criterion can be established by taking advantage of the fact that
the order of accuracy increases in a specific row to estimate an integral within a certain
tolerance (ε) according to Romberg’s rule. Using Eq. (8.51), the absolute difference between
two subsequent estimates in a row can be written as
|Rn,n ´ Rn,n´1| “ |Rn,n´1 ´ Rn´1,n´1|
4n´1 ´ 1
ă ε432  Numerical Methods for Scientists and Engineers
Romberg’s Rule
‚ Romberg’s rule is simple and easy to use;
‚ It is applied to explicitly given continuous functions;
‚ It is ideal for adaptive integration;
‚ It converges quickly for smooth functions;
‚ The number of function evaluations is very small for the level of
accuracy achieved;
‚ The table can be constructed one row at a time, so the analyst does
not have to determine the order of accuracy in advance.
‚ It is customarily not employed for uniformly spaced discrete functions
unless the number of panels is multiples of two (M “ 2n) in which
case the largest size of a Romberg table is limited by the available
data;
‚ It cannot handle non-uniformly spaced discrete functions;
‚ For a large integration interval (h “ b´a), the table size can become
very large, making it susceptible to the ill-effects of round-off errors;
‚ It is not suitable for improper integrals (integrands with discontinu￾ities or integrals with infinite or semi-infinite domains).
TABLE 8.4: Construction of Romberg table.
h Oph2q Oph4q Oph6q Oph8q ¨¨¨ Oph2n´2q Oph2nq
pb ´ aq{20 R1,1 .
pb ´ aq{21 R2,1 R2,2
pb ´ aq{22 R3,1 R3,2 R3,3 Œ
pb ´ aq{23 R4,1 R4,2 R4,3 R4,4
.
.
. Ó .
.
. .
.
. .
.
. ...
.
.
. .
.
. .
.
. Ñ .
.
. ¨¨¨ Rn´1,n´1
pb ´ aq{2n´1 Rn,1 Rn,1 Rn,3 Rn,4 ¨¨¨ Rn,n´1 Rn,n
The extrapolated estimates along the diagonal approach the true answer much more
rapidly. As a more conservative criterion, the difference of the diagonal values is computed;
that is,
|Rn,n ´ Rn´1,n´1|
2n´1 ă ε
The size of the resulting table for a particular integral depends on the size of the integration
interval [a, b] and the tolerance desired (ε).
A pseudomodule, ROMBERGS_RULE, estimating the definite integral of fpxq on [a, b]
using Romberg’s rule is presented in Pseudocode 8.3. As input, this module requires the
integration interval [a, b] and a tolerance (ε). The integrand, fpxq, is supplied as a user￾defined function, FX(x). The module output, intg, is the final estimate for the definite
integral. First, a one-panel Trapezoidal estimate, R1,1, is found, and the rest of the Romberg
table is constructed on a row-by-row basis using a While-loop (k-loop, k “ 2, 3,...). The
Rk,1 is evaluated by Eq. (8.50), and the next level estimates corresponding to the kth
row are computed by the Richardson extrapolation formula, Eq. (8.51). The While-loopNumerical Integration  433
Pseudocode 8.3
Module ROMBERGS_RULE (a, b, ε, intg)
\ DESCRIPTION: A pseudocode to estimate the integral of fpxq on [a, b] within
\ ε tolerance using Romberg’s rule.
\ USES:
\ ABS :: A built-in function computing the absolute value;
\ FX:: A user-defined function module supplying fpxq.
Declare: R10,10 \ Declare max. table size as 10 ˆ 10
\ R is an internal double-subscripted array (table) containing the Romberg table
k Ð 1 \ Initialize row number
h Ð b ´ a \ Initialize panel width
R1,1 Ð pFXpaq ` FXpbqq ˚ h{2 \ Apply one-panel Trapezoidal rule
err Ð 1 \ Initialize error
While “
err ą ε
‰ \ Execute loop until err ă ε
k Ð k ` 1 \ Find current row number
m Ð 2k´2 \ Number of panels for kth row
h Ð h{2 \ Halve the panel width
sums Ð 0 \ Initialize accumulator, sums
For “
i “ 1, m‰ \ Loop i: Apply Trapezoidal rule to 1st column
xi Ð a ` p2i ´ 1q ˚ h \ Evaluate abscissas
sums Ð sums ` FXpxiq \ Accumulate non-overlapping terms
End For
Rk,1 Ð Rk´1,1{2 ` h ˚ sums \ Apply Eq. (8.50)
For “
n “ 2, k‰ \ Apply Richardson extrapolation
Rk,n Ð Rk,n´1 ` pRk,n´1 ´ Rk´1,n´1q{p4n´1 ´ 1q
End For
err Ð |Rk,k ´ Rk´1,k´1|{2k´1 \ Find estimated error
End While
intg Ð Rk,k \ Set Rk,k as the final estimate
End Module ROMBERGS_RULE
operates until the error term is reduced below the user-specified tolerance value, err “
|Rk,k´Rk´1,k´1|{2k´1 ă ε.
EXAMPLE 8.3: Employing the Romgerg’s rule to estimate area
Consider a lamina that is bounded by Ar￾cihmedes’ spiral (r “ θ, y ě 0q and y “ 0
line corresponding to Region (D) in the xy￾coordinate system, as depicted in the figure.
The polar inertia moment for the lamina leads
to the following integral:
ż π
0
θ2
a1 ` θ2dθ
For the given integral, construct Romberg’s table up to Oph10q.434  Numerical Methods for Scientists and Engineers
SOLUTION:
Recall that, in the Romberg table, the order of global error increases by 2 as we
move from one column to another.
We roughly estimate that the table should be 5ˆ5 by observing Oph2nq ” Oph10q.
Setting fpθq “ θ2
?
1 ` θ2, one-panel trapezoidal rule gives
R1,1 “ π´ 0
2
`
fpπq`fp0q
˘
“ π
2
p32.53918 ` 0q“51.112425
Then, using Eq. (8.50), the first column of the Romberg table is constructed as
follows:
R2,1 “ R1,1
2 `
b ´ a
2 f
´
a `
b ´ a
2
¯
“ 51.112425
2 ` π ´ 0
2 f
´π
2
¯
“ 25.556212 ` 7.217083 “ 32.773295
R3,1 “ R2,1
2 `
b ´ a
4
”
f
´
a `
b ´ a
4
¯
` f
´
a `
3pb ´ aq
4
¯ı
“ 32.773295
2 ` π ´ 0
4
”
f
´π
4
¯
` f
ˆ3π
4
˙ ı
“ 28.163282
Likewise, R4,1 and R5,1 are also evaluated from Eq. (8.50).
The second column (n“2) corresponding to Simpson’s rule is obtained using Eq.
(8.51) as follows:
R2,2 “ 4R2,1 ´ R1,1
3 “ 4p32.773295q ´ 51.112426
3 “ 26.660252
R3,2 “ 4R3,1 ´ R2,1
3 “ 4p28.163282q ´ 32.773295
3 “ 26.626611
and so on.
The third column entries are similarly calculated using Eq. (8.51) with n“3 as
R3,3 “ 42R3,2 ´ R2,2
42 ´ 1 “ 16p26.626611q ´ 26.660252
15 “ 26.624369
R4,3 “ 42R4,2 ´ R3,2
42 ´ 1 “ 16p26.618893q ´ 26.626611
15 “ 26.618379
TABLE 8.5
h Oph2q Oph4q Oph6q Oph8q Oph10q
π 51.112426
π{2 32.773295 26.660252
π{22 28.163282 26.626611 26.624369
π{23 27.004991 26.618893 26.618379 26.618284
π{24 26.714874 26.618168 26.618120 26.618116 26.618115
Finally, the fourth and fifth column entries are filled out using Eq. (8.51), with
n“4 and n“5:
R4,4 “ 43R4,2 ´ R3,3
43 ´ 1 “ 64p26.618379q ´ 26.624369
63 “ 26.618284Numerical Integration  435
All entries in the Romberg table are calculated and presented in Table 8.5. The
numerical values, computed with high precision, are reported for convenience only
in six-decimal places. The best estimate is 26.618115.
Discussion: The order of error for R5,5 is Oph10q ” Op8.517 ˆ 10´8q=K ˆ 10´8,
where K is a constant involving fp20q
pξq. If K ă 1{2, then R5,5 will be correct to
8-decimal places but the magnitude of error for R5,5 increases for K ą 1{2. We
also note that fpθq is an increasing function on [0,π], 0 ď fpθq ă 32.54. High-order
derivatives of fpθq will tend to be large as well, making it more likely that K will
be much larger than 1/2.
To assess the accuracy of the best estimate, we compute the absolute differ￾ences between estimates along the last row and diagonal, which give |R5,5 ´ R4,4| “
0.1685 ˆ 10´3 ă 0.5 ˆ 10´3 and |R5,5 ´ R5,4| “ 0.658 ˆ 10´6 ă 0.5 ˆ 10´5, respec￾tively. These error estimates suggest that R5,5 is accurate by at least three decimal
places in the worst scenario. The true solution is
Itrue “ 1
8
´
πp1 ` 2π2q
a1 ` π2 ´ sinh´1π
¯
“ 26.6181187059562
Using the true value, we can now find the true error of the best estimate as E “
|Itrue ´ R5,5| “ 0.352 ˆ 10´5 ă 0.5 ˆ 10´5, which indicates that the best estimate
is in fact five-decimal place accurate. This example illustrates that it is not possible
to predict in advance the size of the Romberg table corresponding to a certain level
of accuracy.
8.4 ADAPTIVE INTEGRATION
The majority of the computation time of any numerical integration is devoted to function
evaluations, which are associated with the number of panels or integration points used. By
now, we have learned that the numerical estimate of an integral improves as the number
of panels is increased, i.e., panel width is reduced. Moreover, when an integrand is smooth
(varies slowly) over its integration range, highly accurate estimates can be achieved with a
relatively small number of panels. But an integrand with sharp changes in short subinter￾vals requires more panels to accurately approximate rapid or sharp variations. High-order
derivatives of such integrands depict rapid changes as well. For this reason, it is difficult
to determine the adequate number of panels (or optimum panel width) needed to ensure
the desired accuracy. Consequently, using a small and uniform panel width everywhere may
result in a very large number of panels over the entire integration domain, including the
parts of the domain where the integrand varies slowly or mildly.
Adaptive integration technique is a powerful tool to estimate a definite integral within
a desired tolerance with minimum computation time. The numerical technique discussed in
this section is based on Simpson’s 1/3 rule. However, this procedure can be easily extended
to the trapezoidal rule or other suitable numerical integration methods.
Consider the numerical integration of fpxq over the entire interval [a, b] using the two￾panel Simpson’s 1/3 rule (Fig. 8.5a):
Ipa, bq “ ż b
a
fpxqdx “ Spa, bq ´ h5
90 fp4q
pξ0q ` ... pa ă ξ0 ă bq (8.52)436  Numerical Methods for Scientists and Engineers
and
Spa, bq “ b ´ a
6
`
fpaq ` 4fpcq ` fpbq
˘ (8.53)
where h“ pb´aq{2 and c“ pa`bq{2 is the midpoint. Notice that the truncation error in Eq.
(8.52) involves the fourth derivative of the integrand. Evaluating the pertinent derivatives
is not an efficient way of estimating the integration error. That is why a simple criterion is
required to determine if the panel width is suitable for a particular subpanel by estimating
the integration error without requiring fp4qpxq.
By halving the integral and approximating each half with Simpson’s rule (see Fig.
8.5b), we find
ż c
a
fpxqdx `
ż b
c
fpxqdx “Spa, cq´ ph{2q
5
90 fp4q
pξ1q`Spc, bq´ ph{2q
5
90 fp4q
pξ2q` ...
“Spa, cq`Spc, bq´ h5
16
fp4qpξ3q
90 `..., a ă ξ3 ă b
(8.54)
where the generalized intermediate value theorem has been applied in the second line to
merge the two error terms.
The difference between the errors of the single-interval (two-panel) estimate Spa, bq
and the two-subinterval (four-panel) estimate (Spa, cq `Spc, bq) is found by subtracting Eq.
(8.54) from Eq. (8.52):
Spa, bq´Spa, cq´Spc, bq“ h5
90 fp4q
pξ0q´ h5
16
fp4q
pξ3q
90 ` ...«15 ˆh5
16
fp4qpξ0q
90 ˙
` ... (8.55)
where we assumed fp4qpξ0q «fp4qpξ3q. The error term in Eq. (8.55) is 15 times that of Eq.
(8.54). This observation provides a suitable stopping criterion that does not require fp4q
pxq.
By incorporating a user-supplied tolerance (ε), a criterion for terminating the numerical
integration for ra, bs can be expressed as
1
15
ˇ
ˇSpa, cq ` Spc, bq ´ Spa, bq
ˇ
ˇ ă ε (8.56)
In general, the 1/15-factor in Eq. (8.56) is replaced with a
more conservative 1/10-factor to compensate for the error in the
fp4qpξ0q«fp4q
pξ3q assumption.
A pseudo-recursive function module, ADAPTIVE_SIMPSON, for recursively evaluating
the definite integral of an explicitly defined function within a tolerance value is presented in
Pseudocode 8.4. The module arguments are the integration interval (a, b) and the accuracy
tolerance desired (ε). The module uses the accompanying function module SIMPSON for
estimating the definite integral of fpxq on (a, b) using a two-panel Simpson’s 1/3 rule, i.e.,
Eq. (8.53). The integration procedure starts with an estimate for the entire (initial) interval:
Spa, bq. Then the integral is split into two subintegrals of equal width (Spa, cq and Spc, bq),
each computed within ε/2 tolerance. When the stopping criterion, Eq. (8.56), is satisfied for
ra, bs, the integral has been estimated within the prescribed tolerance. We can then pass on
to the next subinterval. If the stopping criterion is not met, then the integral, as before, is
split into two subintegrals of equal width until convergence is achieved for each subintegral.
This procedure is repeated for the remaining subintegrals.Numerical Integration  437
Adaptive Integration Methods
‚ Adaptive integration is suitable for recursive programming;
‚ Trapezoidal or Simpson’s rules can be efficiently implemented into
adaptive integration algorithms;
‚ Functions depicting sharp changes can be effectively integrated;
‚ The accuracy of the final estimate can be predetermined by imposing
a tolerance value.
‚ It is applied to explicitly defined continuous functions;
‚ It is not suitable for highly oscillating integrands, improper integrals
over infinite-, or semi-infinite intervals, or discrete functions.
Pseudocode 8.4
Recursive Function Module ADAPTIVE_SIMPSON (a, b, ε)
\ DESCRIPTION: A recursive pseudo-function module to estimate the integral
\ of fpxq on [a, b] using adaptive Simpson algorithm.
\ USES:
\ SIMPSON:: A function module applying a two-panel Simpson’s rule.
c Ð pa ` bq{2 \ Find midpoint of [a, b]
Sab Ð SIMPSONpa, bq \ Two-panel estimate on [a, b]
Sac Ð SIMPSONpa, cq \ Two-panel estimate on [a, c]
Scb Ð SIMPSONpc, bq \ Two-panel estimate on [c, b]
S2ab Ð Sac ` Scb \ Four-panel estimate on [a, b]
error Ð |S2ab ´ Sab|{15 \ Find error estimate
If “
error ă ε
‰
Then \ Converged; Terminate integration
ADAPTIVE_SIMPSONÐ S2ab
Else \ Not converged, halve interval & continue with ε/2 in each half
ADAPTIVE_SIMPSONÐ ADAPTIVE_SIMPSON(a, c, ε{2)
+ADAPTIVE_SIMPSON(c, b, ε{2)
End If
End Recursive Function Module ADAPTIVE_SIMPSON
Function Module SIMPSON (a, b)
\ DESCRIPTION: A pseudo-function to estimate the integral of fpxq on [a, b]
\ using a two-panel Simpson’s 1/3 rule.
\ USES:
\ FX:: A user-defined function supplying fpxq.
c Ð pa ` bq{2 \ Find midpoint of [a,b]
SIMPSONÐ pFXpaq ` 4 ˚ FXpcq ` FXpbqq ˚ pb ´ aq{6
End Function Module SIMPSON
EXAMPLE 8.4: Adaptive integration of Maxwell-Boltzmann distribution
The normalized Maxwell-Boltzmann distribution, for energy, is given by
fpEq “ 2π
?
E
pπkTq
3{2 e´E{kT , 0 ď E ă 8438  Numerical Methods for Scientists and Engineers
where k is Boltzmann’s constant, T is the temperature, and E is the energy of a
molecule. Setting x “ E{kT, the fraction of the molecules having energies between
0 and 4kT can be expressed by the following integral:
2
?π
ż 4
x“0
?x e´xdx
Use adaptive Simpson’s rule to estimate the integral within ε “ 5 ˆ 10´3.
SOLUTION:
The starting interval is [0,4]. Defining fpxq “ ?x e´x, we first estimate the
integral with two-panel and four-panel Simpson’s rule (Eqs. (8.53) and (8.54)):
Sp0, 4q “ 4 ´ 0
6
`
fp0q ` 4fp2q ` fp4q
˘
“ 0.5348022
Sp0, 2q ` Sp2, 4q “ 2 ´ 0
6
`
fp0q ` 4fp1q ` fp2q
˘
`
4 ´ 2
6
`
fp2q ` 4fp3q ` fp4q
˘
“ 0.5543036 ` 0.1909864 “ 0.745290
Then, the first approximation error is found as
1
15
ˇ
ˇSp0, 4q´Sp0, 2q´Sp2, 4q
ˇ
ˇ “ 1
15
ˇ
ˇ0.5348022´0.745290ˇ
ˇ“0.0140325 ą ε
The first approximation fails to be within the prescribed tolerance, so we divide [0,2]
into two subintervals, [0,1] and [1,2]. Since Sp0, 2q was previously computed, we need
to evaluate only Sp0, 1q ` Sp1, 2q, which gives
Sp0, 1q ` Sp1, 2q “ 1 ´ 0
6
`
fp0q ` 4fp0.5q ` fp1q
˘
`
2 ´ 1
6
`
fp1q ` 4fp1.5q ` fp2q
˘
“ 0.3472345 ` 0.2753971 “ 0.6226316
The computed approximation error for Sp0, 2q is
1
15
ˇ
ˇSp0, 2q´Sp0, 1q´Sp1, 2q
ˇ
ˇ “ 1
15
ˇ
ˇ0.5543036´0.6226316ˇ
ˇ“0.0045552 ą
ε
2
which also fails to be within the desired tolerance.
Next, dividing the interval [0,1] into two subintervals, [0,0.5] and [0.5,1], and
estimating the integral from Sp0, 0.5q ` Sp0.5, 1q yields
S p0, 0.5q`S p0.5, 1q“0.1655403`0.2027573 “ 0.3682976
1
15
ˇ
ˇSp0, 1q´Sp0, 0.5q´Sp0.5, 1q
ˇ
ˇ “ 1
15
ˇ
ˇ0.3472345´0.3682976ˇ
ˇ“1.404ˆ10´3 ą
ε
4
Now, dividing the [0, 0.5] interval into [0, 0.25] and [0.25, 0.5] intervals, and
estimating the integral from the results in
S p0, 0.25q`S p0.25, 0.50q“0.0682266`0.1042413“0.1724679
1
15
ˇ
ˇSp0, 0.5q´Sp0, 0.25q´Sp0.25, 0.5q
ˇ
ˇ“ 1
15
ˇ
ˇ0.1655403´0.1724679ˇ
ˇ“4.62ˆ10´4 ă
ε
8Numerical Integration  439
which satisfies the stopping criterion. This means that we have achieved the inte￾gration on [0,0.5] with sufficient accuracy, and there is no need to further subdivide
this interval.
Now, we proceed to compute the integral on [0.5,1], which is obtained by
Sp0.5, 0.75q ` Sp0.75, 1q as
1
15
ˇ
ˇSp0.5, 1q´Sp0.5, 0.75q´Sp0.75, 1q
ˇ
ˇ“ 1
15
ˇ
ˇ0.2027573´0.2028052ˇ
ˇ“3.19 ˆ 10´6 ă
ε
8
Since it meets the stopping criterion, we proceed to compute the integral on [1,2]
by Sp1, 1.5q ` Sp1.5, 2q as
S p1, 1.5q`S p1.5, 2q“0.1602038`0.1153496“0.2755534
1
15
ˇ
ˇSp1, 2q´Sp1, 1.5q´Sp1.5, 2q
ˇ
ˇ“ 1
15
ˇ
ˇ0.2753971´0.2755534ˇ
ˇ“1.04 ˆ10´5 ă
ε
2
The numerical integration on [1,2] also satisfies the tolerance, so we can advance
to the integration on [2,4]. The two-panel estimate for [2,4] was computed before;
thus, the four-panel estimate and the error of approximation are obtained by
Sp2, 3q`Sp3, 4q“0.0794647`0.0533392“0.1328039
1
15
ˇ
ˇSp2, 4q´Sp2, 3q´Sp3, 4q
ˇ
ˇ“ 1
15
ˇ
ˇ0.1327963´0.1328039ˇ
ˇ“3.32 ˆ10´6 ă
ε
2
Because the approximation error is well below the prescribed tolerance, the numerical
integration procedure is terminated.
Finally, the estimate for the integral is obtained by summing up the more accurate
estimates obtained for sub-integrals, leading to
Sp0, 0.5q ` Sp0.5, 1q ` Sp1, 2q ` Sp2, 4q “ 0.8417632
The true integral is ?π{2 erfp2q ´ 2e´4=0.845450112984953, where erf denotes the
error function. The true error is then obtained as
ˇ
ˇI ´ Sp0, 0.5q ´ Sp0.5, 1q ´ Sp1, 2q ´ Sp2, 4q
ˇ
ˇ “ 3.687 ˆ 10´3 ă ε
which is more accurate than the tolerance desired.
FIGURE 8.7
Discussion: In this example, the two-panel Simpson’s rule, Spa, bq, was applied 15
times, which resulted in 45 function evaluations. As marked on the curve in Fig.
8.7, this procedure places extra integration points as needed in the intervals of the
domain where the integrand exhibits sharp variation, as in 0.5 ă x ă 2.440  Numerical Methods for Scientists and Engineers
Note that the order of error for Sab “ Spa, bq is Oph4q, while for
S2ab “ Spa, cq ` Spc, bq it is Oph4{16q. Accordingly, using Sab
and S2ab, the Richardson extrapolation may also be employed to
further improve the accuracy of the integration, as follows:
Ipa, bq – 16 ˆ S2ab ´ Sab
15 ` Oph6q
In Pseudocode 8.4, the term S2ab in the ADAPTIVE_SIMPSONÐ
S2ab statement should be replaced with the above expression.
8.5 NEWTON-COTES RULES
The Newton-Cotes rules or formulas have an important place among numerical integration
methods. There are basically two kinds of rules (open or closed) that make up a family of
formulas for the numerical integration of uniformly spaced integration points. This distinc￾tion is based on the choice of abscissas used in constructing the approximation polynomial
for the interval [x0, xn].
8.5.1 CLOSED NEWTON-COTES RULES
Any closed formula can be derived by constructing a Lagrange interpolating polynomial of
nth degree, from a set of n`1 uniformly spaced discrete integration points, i.e., (x0, f0),
(x1, f1), ..., (xn, fn), where xk “ x0 ` kh for k “ 0, 1,...,n with h “ pxn ´ x0q{n. An
approximation formula is then obtained by integrating the interpolating polynomial over
[x0, xn]. For example, the procedure for a set of two points, (x0, f0) and (x1, f1), leads to
ż x1
x0
fpxqdx “
ż x1
x0
p1pxqdx “
ż x1
x0
´ x ´ x1
x0 ´ x1
f0 `
x ´ x0
x1 ´ x0
f1
¯
dx “ h
2
pf0 ` f1q
which is the trapezoidal rule with h “ x1 ´ x0.
A general family of resulting numerical approximations is called closed Newton-Cotes
rules because they include the integrands at both ends, i.e., f0 and fn. A few of the closed
Newton-Cotes rules, along with the local error terms, are as follows:
ż x2
x0
fpxqdx “ h
3
`
f0`4f1`f2
˘
´ h5
90 fp4q
pξq Simpson’s 1/3 pn “ 2q
ż x3
x0
fpxqdx “ 3h
8
`
f0`3f1`3f2`f3
˘
´ 3h5
80 fp4q
pξq Simpson’s 3/8 pn “ 3q
ż x4
x0
fpxqdx “ 2h
45
`
7f0`32f1`12f2`32f3`7f4
˘
´ 8h7
945 fp6q
pξq Boole’s rule pn “ 4q
where x0 ă ξ ă xn. Higher-order formulas can be found in Ref. [1].
We notice that the trapezoidal, Simpson’s-1/3, and -3/8 rules are indeed closed Newton￾Cotes formulas, derived by interpolating polynomials of degree n “ 1, 2, and 3, respectively.
To obtain Boole’s rule, a 4th-degree interpolating polynomial is used. The order of accuracy
for even n is n`3, whereas for odd n is n`2. Upon employing the composite rule for integrals
of uniformly spaced discrete functions over a general interval [a, b], the order of accuracy
decreases by one.Numerical Integration  441
FIGURE 8.8: Graphical depiction of (a) two- (b) three- and (c) four-panel open Newton-Cotes
abscissas and approximating polynomials (in black).
8.5.2 OPEN NEWTON-COTES RULES
Some integrands may be discontinuous at the lower, upper, or both ends of the integration
interval. These integrals cannot be estimated with the “closed” Newton-Cotes formulas since
they require the integrand at both ends, i.e., f0 and fn. Alternative formulas that avoid the
integrands at end points are referred to as the open Newton-Cotes rules [1].
An open rule is derived by constructing a Lagrange interpolating polynomial that passes
through a set of n´1 uniformly spaced discrete integration points, avoiding the end points,
i.e., (x1, f1), (x2, f2), ..., (xn´1, fn´1). As illustrated earlier, the interpolating polynomial
is integrated over [x0, xn] to find an approximation formula.
The simplest case in this family, also known as the midpoint rule, requires two rectan￾gular panels of width h. The approximate integral of fpxq over x0 ď x ď x2 is set to the
mid-point value p0pxq “ f1 (i.e., height) times the interval width, 2h (see Fig. 8.8a):
ż x2
x0
fpxqdx “ 2hf1 `
h3
3 f 2pξq, x0 ă ξ ă x2 (8.57)
where h3f 2pξq{3 is the leading truncation error. Notice that the integration error would be
zero if f 2pξq “ 0 implying that the numerical estimate is exact if the integrand is constant
or a straight line.
Another rule can also be obtained by dividing the integration interval into three panels
of uniform width (see Fig. 8.8b). Excluding the end points, the interpolating polynomial
passing through px1, f1q and px2, f2q is a straight line: p1pxq“pf2px ´ x1q ` f1px2 ´ xqq{h.
Integrating p1pxq over rx0, x3s gives a second-order formula (i.e., two-point rule):
ż x3
x0
fpxqdx “ 3h
2 pf1 ` f2q ` 3h3
4 f 2pξq, x0 ă ξ ă x3 (8.58)
where 3h3f 2pξq{4 is the leading error term. Note that even though the three-panel approx-442  Numerical Methods for Scientists and Engineers
imates the integrand with a straight line, the order of error is the same as the two-panel
rule (i.e., Eq. (8.58)).
As the number of panels increases, the degree of the Lagrange interpolating polynomials
and, consequently, the order of accuracy increases. Integrating a second-degree interpolating
polynomial over the prescribed interval (Fig. 8.8c) yields a fourth-order formula (i.e., three￾point rule):
ż x4
x0
fpxqdx“ 4h
3 p2f1´f2`2f3q` 14h5
45 fp4q
pξq, x0 ă ξ ă x4 (8.59)
Higher-order formulas can be similarly developed; four-point (4th order) and five-point (6th
order) rules are given as
ż x5
x0
fpxqdx“ 5h
24 p11f1`f2`f3`11f4q` 95h5
144 fp4q
pξq, x0 ă ξ ă x5 (8.60)
ż x6
x0
fpxqdx“ 3h
10 p11f1´14f2`26f3´14f4`11f5q` 41h7
140 fp6q
pξq, x0 ă ξ ă x6 (8.61)
The dominant error in “open” or “closed” rules can be expressed as K hn`1fpnq
pξq, where K
is a constant and h“ pxn´x0q{n. For formulas having the same hn as a factor, h, the error
term clearly becomes smaller for hă1. But it is the coefficient K that provides a guide to
the accuracy of the formulas, since the accuracy with a smaller K is better. Although Eqs.
(8.59) and (8.60) have the same order of accuracy, Eq. (8.59) is more accurate because of
a smaller coefficient K, i.e., K “ 14{45 ă 95{144. On the other hand, to prevent the error
term from growing, attention must also be paid to fpxq to ensure that its higher derivatives,
fpnq
pξq, do not become excessively large within the integration interval.
Many versions of the Newton-Cotes formulas that use a large number of panels have
been developed. For larger n, one might expect the numerical estimates with the Newton￾Cotes formulas to improve, however, but that is not the case. Employing an open or closed
Newton-Cotes formula derived from a single high-degree interpolating polynomial over the
entire [a, b] range does not assure good results unless the integration domain is quite small,
i.e., small b ´ a. On the other hand, to estimate the definite integral of a function over
a large interval with reasonable accuracy, a very large number of integration points (or a
high-order interpolation polynomial) spanning the entire range is required.
For any n, only the weights of the Newton-Cotes formula are determined because
the abscissas are uniformly spaced. The weights turn out to be symmetrical with respect
to the midpoint of the integration interval, and the sum of the weights is equal to the
integration interval. In closed formulas (for n ě 8) and in open formulas (for n ě 2),
the weights turn out to be of mixed sign and, moreover, grow without bound. Besides
being laborious and having poor round-off error properties, these undesirable features lead
to Runge’s phenomenon, which causes the error to grow. (Recall that fitting uniformly
spaced discrete points to a high-degree interpolating polynomial tends to exhibit Runge’s
phenomenon; see Section 6.1). Hence, the Newton-Cotes formulas for large n are rarely used
in numerical computations.
An alternative approach to using high-order formulas in order to increase the accuracy
of a numerical integration process is to take advantage of the composite rule, as in the cases
of the N-panel composite closed Newton rules. In practice, it is preferable to use composite
(open or closed) Newton-Cotes formulas rather than applying high-order formulas that cover
larger intervals. To accomplish this, the original interval [a, b] is divided into M subregions,Numerical Integration  443
Open Newton-Cotes Formulas
‚ Open Newton-Cotes formulas are simple and easy to program;
‚ Formulae of high order of accuracy are available;
‚ They are effective in treating the integrands that have singularities
at the end point(s);
‚ They can be easily adapted to composite rules;
‚ A Newton-Cotes rule based on n integration points yields the exact
value of the integral for polynomials of degree at most n ´ 1;
‚ They are used in multistep integration procedures in the numerical
treatment of ordinary differential equations (see Section 9.4).
‚ An integrand must be smooth to ensure the specified order of accu￾racy;
‚ Very high-order-accurate integration formulas are rarely used in prac￾tice because they also suffer from Runge’s phenomenon;
‚ Errors can be very large for large integration intervals, which neces￾sitates the use of a composite rule in most cases;
‚ The convergence of any open formula can be very slow for integrands
with singular points;
‚ Weights can take large values and alternate in sign for n ě 2, which
can be a source of instability, in particular, due to the propagation
of rounding errors.
and then the integral is piecewise estimated by applying a low-order Newton-Cotes formula
to each subregion. Numerical integration carried out in this manner is called composite
Newton-Cotes rules. The global error terms for the open Newton-Cotes composite rules are
given as pb´aqh2f 2pξq{6, pb´aqh2f 2pξq{4, 7pb´aqh4fp4qpξq{90, 19pb´aqh5fp4qpξq{144, and
41pb´aqh6fp6q
pξq{840 for Eqs. (8.57)-(8.61), respectively.
Open Newton-Cotes rules are generally not applied to regular
smooth functions. They are useful only when estimating certain
improper integrals where the integrand is discontinuous at the
lower, upper, or both ends of the integration interval.
8.6 INTEGRATION OF NON-UNIFORM DISCRETE FUNCTIONS
The numerical methods, such as trapezoidal, Simpson’s, and Romberg’s rules, are generally
formulated for uniformly spaced discrete functions. In cases where the integrand is explicitly
known, the analyst has the freedom to choose the numerical method or number of panels,
and so on, in which case uniformly spaced discrete data points are generated to simplify
the computation procedure and reduce the cpu-time.
When dealing with non-uniformly spaced discrete functions that are encountered as
a result of random data sampling or experimental observations, the analyst has no choice
but to use the available data “as is.” In this section, numerical integration formulations
for the composite trapezoidal or Simpson’s rules that allow non-uniform panel widths are
presented.444  Numerical Methods for Scientists and Engineers
FIGURE 8.9: A non-uniform distribution
Consider a non-uniformly spaced discrete function on [a, b] (see Fig. 8.9) whose abscissas
are given as a“x0 ă x1 ă ... ă xN´1 ă xN “b. Let fpxiq“fi and hi “xi´xi´1 denote the
integrand at xi and preceding interval size, respectively. The area of the trapezoid bounded
by panel rxi´1, xis is Ai “ pfi´1 ` fiqhi{2. Then the N-panel trapezoidal rule can be cast as
the sum of the areas of all trapezoids:
ż b
a
fpxqdx – TN “ 1
2
ÿ
N
n“1
hipfi´1 ` fiq
For Simpson’s rule, a second-degree Lagrange interpolating polynomial passing through
three data points, pxi´1, fi´1q, pxi, fiq, pxi`1, fi`1q, is constructed (see Section 6.1) as
follows:
fpxq – xpx ´ hi`1q
hiphi ` hi`1q
fi´1 ´ px ` hiqpx ´ hi`1q
hihi`1
fi `
xpx ` hiq
hi`1 phi ` hi`1q
fi`1 (8.62)
Integrating the approximation for fpxq, Eq. (8.62), on rxi´1, xi`1s yields
Ai “
ż xi`1
xi´1
fpxqdx – phi`1`hiq
6
#
p2hi´hi`1q
hi
fi´1`phi`1`hiq
2
hi`1hi
fi`p2hi`1´hiq
hi`1
fi`1
+
(8.63)
Employing Eq. (8.63) to an even number of panels results in
ż b
a
fpxqdx – A1 ` A3 ` ... ` AN´1 “
rN
ÿ
{2s
i“1
A2i´1
“
rN
ÿ
{2s
i“1
ph2i`h2i´1q
6
#
p2h2i´1´h2iq
h2i´1
f2i´2` ph2i ` h2i´1q
2
h2i´1h2i
f2i´1 ` p2h2i´h2i´1q
h2i
f2i
+
which is the composite rule for non-uniform discrete functions.
For an “odd” number of panels, the integral of the interpolating polynomial, Eq. (8.62),
over the last panel rxn´1, xns is obtained as follows and added to Eq. (8.63).
ż xn
xn´1
fpxqdx –
hn´1p2hn´1 ` 3hn´2q
6phn´2 ` hn´1q fn `
hn´1phn´1 ` 3hn´2q
6hn´2
fn´1
´ h3
n´1
6hn´2phn´2 ` hn´1q
fn´2
(8.64)Numerical Integration  445
Pseudocode 8.5
Module NONUNIFORM_SIMPSON (panel, x, f, intg)
\ DESCRIPTION: A pseudocode to estimate the integral of a non-uniformly
\ spaced discrete function using Simpson’s rule.
\ USES:
\ MOD:: A built-in function returning the remainder of x divided by y.
Declare: x0:panel, f0:panel, h0:panel \ Declare array variables
For “
i “ 0, panel‰ \ Loop: Find panel widths
hi Ð xi`1 ´ xi
End For
m Ð MODpn ´ 1, 2q \ Find remainder of n ´ 1 divided by 2
If “
m “ 0
‰
Then \ Determine if number of panels is even or not
np Ð panel ` 1 \ if EVEN, apply Eq. (8.63) to all panels
Else
np Ð panel \ if ODD, apply Eq. (8.63) to all but last panel
End If
intg Ð 0 \ Initialize integration accumulator variable
For “
i “ 0,pnp ´ 2q, 2
‰ \ Loop: Compute and accumulate Eq. (8.63)
dx Ð phi`1 ` hiq{6
aw Ð p2hi ´ hi`1q{hi
ap Ð phi`1 ` hiq
2
{phi`1 ˚ hiq
ae Ð p2hi`1 ´ hiq{hi`1
intg Ð intg ` dx ˚ paw ˚ fi ` ap ˚ fi`1 ` ae ˚ fi`2q
End For
n “ panel
If “
m “ 1
‰
Then \ For odd number panels, apply Eq. (8.64)
intg Ð intg ` fnhn´1p2hn´1 ` 3hn´2q{p6phn´2 ` hn´1qq
intg Ð intg ` fn´1hn´1phn´1 ` 3hn´2q{p6hn´2q
intg Ð intg ´ fn´2h3
n´1{p6hn´2phn´2 ` hn´1qq
End If
End Module NONUNIFORM_SIMPSON
A pseudomodule, NONUNIFORM_SIMPSON, estimating the integral of a non-uniformly
spaced discrete function is given in Pseudocode 8.5. The module requires the number of data
points (n) and the non-uniform discrete data set, pxi, fiq for i“1, 2,...,n, as input. Using
the array of abscissas, the panel widths are computed within a For-loop as hi “xi`1´xi for
i “ 1, 2,...,n´1. The module first checks whether the number of panels is even using the
built-in MOD (modulo) function. In the case of m“0, the n´1 is evenly divided by 2 with
no remainder; that is, the number of panels is even, so the number of panels is set to np“n.
For m“1, the number of panels is odd, and np“n´1 becomes even, for which Eq. (8.63)
is applied. On the other hand, Eq. (8.64), which is the integration over the last panel, is
added to the previous sum. The result is saved to the output (accumulator) variable integ.
8.7 GAUSS-LEGENDRE METHOD
The Gauss-Legendre method is a powerful numerical tool to compute the definite integral
of an explicitly defined function. The method requires the evaluation of the integrand at
non-uniformly spaced integration points to achieve the highest accuracy, which is much
higher than Newton-Cotes formulas.446  Numerical Methods for Scientists and Engineers
Integration of Non-Uniformly Spaced Discrete Functions
‚ It is essential for estimating the numerical integral of non-uniform
functions;
‚ Numerical integration procedures based on open- or closed- Newton￾Cotes formulas can be easily employed for non-uniformly spaced dis￾crete functions;
‚ Estimating the error and its bounds is not an easy task;
‚ The order of accuracy is dominated by the largest h values;
‚ Cpu-time increases due to computing the coefficients.
The objective of the method is to determine a set of abscissas (integration points) and
corresponding weights such that the numerical approximation of an integral on [´1, 1] can
be expressed as a weighted sum:
ż `1
´1
fpxq dx – GN “ ÿ
N
i“1
wifpxiq (8.65)
where N is the number of quadrature points, xi and wi’s are the abscissas and corresponding
weights, respectively, together referred to as Gauss-Legendre quadrature.
Since the numerical estimate is a weighted sum, it is easy to program it once the
quadrature for a specified N is known. The main difficulty in this method, however, is
obtaining highly accurate quadrature. To illustrate the determination of the quadrature, we
set fpxq “ xk in Eq. (8.65) and integrate the left-hand side exactly. This process yields the
following set of 2N nonlinear equations with 2N unknowns:
ÿ
N
i“1
wixk
i “
ż `1
´1
xk dx “ 1 ` p´1q
k
k ` 1 , k “ 0, 1, 2,...,p2N ´1q (8.66)
or explicitly
w1 ` w2 `¨¨¨` wN “ 2
x1w1 ` x2w2 `¨¨¨` xN wN “ 0
x2
1w1 ` x2
2w2 `¨¨¨` x2
N wN “ 2
3
.
.
. .
.
.
x2N´1
1 w1 ` x2N´1
2 w2 `¨¨¨` x2N´1
N wN “ 0
(8.67)
The solution of Eq. (8.67) for specified N gives the Gauss-Legendre quadrature [1].
Although it would be theoretically possible to solve Eq. (8.67), in practice, it is extremely
difficult to find a numerical solution to this set of nonlinear equations, especially when N
is fairly large. To get around this problem, alternatively, a set of abscissas is proposed. The
problem is then reduced to determining only the weights from Eq. (8.67), which is now a
set of linear equations with weights as unknowns. It turns out that the abscissas are the
roots of the Nth-degree Legendre polynomial: PN pxq “ 0. Evaluating the corresponding
weights is a bit involved, so before we do that, we will briefly visit the so-called Legendre
polynomials and some of their properties [1].Numerical Integration  447
The nth-degree Legendre polynomial is defined as
Pnpxq “ 1
2nn!
dn
dxn px2 ´ 1q
n (8.68)
For n “ 0 and n “ 1, Equation (8.68) yields P0pxq “ 1 and P1pxq “ x. An easy way to
generate higher-degree Legendre polynomials is to make use of the recurrence relationship
that relates Legendre polynomials of three subsequent degrees:
n Pnpxq“p2n ´ 1qxPn´1pxq´pn ´ 1qPn´2pxq, for n ě 2 (8.69)
which leads up to 4th degree are as follows:
P2pxq “ 1
2
p3x2 ´ 1q, P3pxq “ x
2
p5x2 ´ 3q, P4pxq “ 1
8
p35x4 ´ 30x2 ` 3q
The roots of the Legendre polynomials for a small N can be easily calculated by
hand. However, for a large N, the roots in general are calculated with the Newton-Raphson
method, leading to the following iteration equation:
xpp`1q – xppq ´ PN pxppq
q
P1
N pxppqq (8.70)
Here, PN pxq is evaluated by Eq. (8.69), and P1
N pxq, which is the first derivative of PN pxq,
is evaluated by applying the following recursive relationship:
p1 ´ x2q
dPnpxq
dx “ nPn´1pxq ´ nxPnpxq, (8.71)
It turns out that the weights can be computed from
wi “ 2
p1 ´ x2
i qrP1
N pxiqs2 (8.72)
The proof of Eq. (8.72) will be skipped here as it is out of the scope of this text.
To compute the abscissas, it is critical to start the iteration process with an initial
guess sufficiently close to a zero of PN pxq. An initial guess for an abscissa is obtained by
xp0q
i “ cospp4i ´ 1qπ{p4N ` 2qq [22]. Once an abscissa (i.e., a zero) is iteratively found using
Eq. (8.70), the corresponding weight is calculated by Eq. (8.72). It should be noted that
the distribution of abscissas is symmetrical with respect to x “ 0, and the weights are all
positive. The abscissas are non-uniformly spaced and stacked closer toward the end points
as N increases. This feature, unlike Newton-Cotes formulas, allows the use of more points
in a single interval by avoiding Runge’s phenomenon. The complete quadrature sets for
various N are tabulated in Appendix B.1.
The global error for the Gauss-Legendre quadrature is given by
RN “ 22N`1pN!q
4
p2N ` 1qrp2Nq!s
3 fp2Nq
pξq, ´1 ă ξ ă 1 (8.73)
Note that Eq. (8.73), which involves fp2Nq
pξq derivative, will be zero for polynomials of up
to the (2N ´1)th degree since fp2Nq
pξq “ 0. This result indicates that the Gauss-Legendre
method yields the true values of the integrands of polynomials of degree up to 2N ´1 if no
round-off errors were made. This property makes the method very powerful since, even for448  Numerical Methods for Scientists and Engineers
small N, numerical approximations with higher accuracy can be achieved. Nevertheless, an
integrand sometimes may not be smooth and might behave differently at different intervals.
In such cases, the most rational approach is to divide the integral into several subintervals,
then integrate each region separately with a low-order quadrature rule, and finally add all
estimates together.
GENERALIZATION: The Gauss-Legendre quadrature method can be extended to an
arbitrary definite integral. To do this, a definite integral is transformed into an integral on
r´1, 1s. Upon invoking x “ pb ´ aqu{2 ` pb ` aq{2 substitution with u P r´1, 1s, we get
ż b
x“a
fpxqdx “ b ´ a
2
ż `1
u“´1
f
´b ´ a
2
u `
b ` a
2
¯
du (8.74)
Employing the Gauss-Legendre sum to the right-hand side of Eq. (8.74) yields
ż b
a
fpxqdx “ ÿ
N
i“1
wifpxiq ` RN (8.75)
where
xi “ b ´ a
2
ui `
b ` a
2 , wi “ b ´ a
2
wui (8.76)
and pui, wuiq is the quadrature set on r´1, 1s (Appendix B.1), and RN is the global error
given by
RN “ pb ´ aq
2N`1
pN!q
4
p2N ` 1qrp2Nq!s
3 fp2Nq
pξq, a ă ξ ă b (8.77)
whose upper and lower bounds can be estimated using the maximum and minimums of
fp2Nq
pξq.
The global error clearly involves the derivative fp2Nq
pxq. Equations (8.73) and Eq.
(8.77) are generally not very useful unless the integrand is a polynomial, a simple expo￾nential or trigonometric function, and so on. Otherwise, they are impractical for assessing
the accuracy of an estimated value. The best way to assess the accuracy is to compare
the estimates for several different values of N, such as GN1 and GN2 . In certain cases,
these comparisons may yield substantially different results, which may be indicative of an
integrand with singular point(s) or a highly oscillatory character.
The foregoing comments regarding accuracy apply equally to any numerical integration
method. The Gauss-Legendre method is unique in that it yields very accurate estimates for a
variety of integrands with very small N. The accuracy of the method may inspire confidence
in an analyst, who might attempt to do one evaluation with a single (perhaps large) value
of N and accept the estimate blindly.
The true error of an integral can be determined only when its
true solution is known. The reader may wonder why the global
error bounds have been computed in some examples presented
here even when the true error can be readily calculated. It should
be kept in mind that we resort to numerical integration in cases
where the true solution cannot be found. In this text, examples
with known true solutions are used to illustrate the importance
and effectiveness of the error estimation process.Numerical Integration  449
Pseudocode 8.6
Module GAUSS_LEGENDRE_QUAD (N, ε, x, w)
\ DESCRIPTION: A pseudomodule generating N-point Gauss-Legendre quad-
\ rature for N ě 2; abscissas and weights are on [´1, 1].
\ USES:
\ ABS :: A built-in function computing the absolute value.
\ COS :: Trigonometric cosine function.
Declare: xN , wN \ Declare array variables
m Ð pN ` 1q{2 \ Find number of abscissas in symmetric half
For “
i “ 1, m‰ \ Loop i: find xi and wi
u Ð cospπ p4i ´ 1q{p4N ` 2qq \ Apply initial guess estimator xi
δ Ð 1 \ Initialize absolute error
While “
|δ| ą ε
‰ \ Newton-Raphson iteration loop to find xi
P0 Ð 1 \ Initialize P0
P1 Ð u \ Initialize P1
For “
k “ 2, N‰ \ Loop to find PN
P2 Ð pp2k ´ 1quP1 ´ pk ´ 1qP0q{k \ Apply Eq. (8.69)
P0 Ð P1 \ Assign Pk´2 Ð Pk´1
P1 Ð P2 \ Assign Pk´1 Ð Pk
End For
dP Ð npuP2 ´ P0q{pu2 ´ 1q \ Compute P1
N puq by Eq. (8.71)
δ Ð P2{dP \ Difference between two successive iterates
u Ð u ´ δ \ Apply Newton-Raphson, Eq. (8.70)
End While \ Exits loop with converged xi
xi Ð ´u \ Save root as the negative abscissa too
wi Ð 2{p1 ´ u2q{dP2 \ Find corresponding weight by Eq. (8.72)
xN`1´i Ð u \ Save the symmetric abscissa
wN`1´i Ð wi \ Save the symmetric weight
End For
End Module GAUSS_LEGENDRE_QUAD
A pseudomodule, GAUSS_LEGENDRE_QUAD, generating the N-point Gauss-Legendre
quadrature on r´1, 1s is presented in Pseudocode 8.6. As input, the module requires the
number of integration points (N) and a tolerance value (ε) for the desired accuracy in the
quadrature. The output is the quadrature set: pxi, wiq for i“1, 2, .., N.
A For-loop over i-variable covers the computation of positive abscissas only, i.e., xi for
i“1, 2, ..,pN`1q{2. In other words, the zeros with the opposite sign (as well as their weights)
are not computed due to symmetry. In order to speed up and to ensure convergence, the
initial guess for the ith zero is obtained by pxiqp0q “cospπ p4i´1q{p4N `2qq. Then, using a
While-loop, the Newton-Raphson iteration procedure is employed until the zero (i.e., xi) is
found within the preset tolerance (ă ε). To find an improved estimate of xi using Eq. (8.70),
we need to evaluate the Nth-degree Legendre polynomial and its derivative (PN pxq and
P1
N pxq). This is achieved with a For-loop over k-variables from k “ 2 to N, which applies the
recurrence relations to find PN pxq using Eq. (8.69). Once PN pxq is obtained, P1
N pxq is found
by using (8.71). The iteration procedure is terminated when the absolute error between two
consecutive iterates is less than the prescribed tolerance (δ “ |upp`1q ´ uppq
| ă ε). Having
found xi, the corresponding weight is determined from Eq. (8.72). The negative abscissas450  Numerical Methods for Scientists and Engineers
of the Gauss-Legendre quadrature are produced from the known positive zeros, and they
are assigned the same corresponding weights.
EXAMPLE 8.5: Deriving and employing 3-point Gauss-Legendre quadrature
The solar heat flux (kW/m2) incident on an absorber tube wall is approximated by
qpxq“p22x3 ´ 5.5x2 ´ 2x ` 30qex3{40 for ´ 1 ď x ď `1
where x “ cos θ, and θ corresponds to the radial angle along the symmetric half￾perimeter of the tube, i.e., 0 ď θ ď π. The mean solar heat flux for the tube may be
calculated using the following integral:
qm “ 1
2
ż 1
´1
qpxqdx
(a) Generate the 3-point Gauss-Legendre quadrature (abscissas and weights), and
(b) find G3 as well as the corresponding error bounds and the true error.
SOLUTION:
(a) For N “3, the abscissas (xi’s) are determined by setting P3pxq“0; that is,
P3pxq “ 1
2
xp5x2 ´ 3q “ 0
which yields x1 “ 0 and x2,3 “ ˘a3{5.
Noting P1
3pxq“p15x2´3q{2, the quadrature weights are obtained from Eq. (8.72)
as follows:
w1 “ 2
p1 ´ 02qpP1
3p0qq2 “ 8
9
w2,3 “ 2
p1 ´ p˘a3{5q
2
qpP1
3p˘a3{5qq2
“ 5
9
(b) Using Eq. (8.65), the numerical estimate is obtained as
ż `1
´1
qpxqdx « G3 “ ÿ
3
i“1
wiqpxiq “ 8
9
qp0q ` 5
9
q
´c3
5
¯
`
5
9
q
´
´
c3
5
¯
“ 8
9 p30q ` 5
9 p35.788906q ` 5
9 p17.816303q
“ 56.44733833
Finally, the mean solar heat flux is found as qm “ 56.44733{2 “ 28.2364 kW/m2.
Discussion: To assess the global error, we make use of Eq. (8.73):
R3 “ 27p3!q
4
7p6!q
3 qp6q
pξq “ 6.3492 ˆ 10´5qp6q
pξq, ´1 ă ξ ă 1Numerical Integration  451
For determining the upper and lower bounds of the error, we require the maximum
and minimum values of qp6q
pξq on [´1, 1].
FIGURE 8.10
The graph of qp6qpxq illustrated in Fig. 8.10 depicts an absolute minimum and
an absolute maximum value of qp6qp´1q«´8.3 and qp6qp1q « 819.5), respectively,
at the endpoints of the integration interval. Substituting these values into the global
error term yields
6.3492 ˆ 10´5p´8.3q ă R3 ă 6.3492 ˆ 10´5p819.5q
or
´0.000527 ă True Error ă 0.052032
Discussion: It is possible to obtain a highly accurate value for the integral by
increasing the number of integration points, i.e., G5, G9, and so on. The true
integral of this example exits and is found as Itrue “ 56.47277954, which is re￾ported correct to eight decimal places. The absolute error can now be found to be
56.472779538´56.447338333“0.025441205. Notice that the true error is within the
lower and upper bounds of the global error.
EXAMPLE 8.6: Applying Gauss-Legendre method to an arbitrary definite integral
Repeat Example 8.2 using 4-point Gauss-Legendre quadrature and determine the
true error.
SOLUTION:
For now, if we exclude the factor 4π, we will transform the integral into an integral
over [´1, 1] so that the Gauss-Legendre quadrature can be applied.
With x“πpu`1q{2 substitution and dx“ pπ{2qdu, the integral becomes
ż π
0
asin x ` cos2xdx “ π
2
ż 1
u“´1
asin x ` cos2x du – G4 “ π
2
ÿ
N
i“1
wifpxiq
where xi “ πpui ` 1q{2 and pui, wiq are the 4-point Gauss-Legendre quadrature on
[´1, 1] (see Appendix B.1).452  Numerical Methods for Scientists and Engineers
Gauss-Legendre Quadrature Method
‚ The method is easy to implement as it yields a simple weighted sum;
‚ The degree of precision of GN is 2N ´1, and it gives the exact value
for the integral of a polynomial of degree 2N ´1 or less;
‚ It is superior to Newton-Cotes rules because very high levels of ac￾curacy can be achieved with a small number of function evaluations
(even for N ă10);
‚ It offers, in some cases, a remedy for computing improper integrals
since abscissas avoid the end points.
‚ It is limited to explicit functions, i.e., unsuitable for discrete func￾tions;
‚ Abscissas and weights are not static; They change as N is increased,
which makes the method unsuitable for recursive algorithms;
‚ To obtain highly accurate estimates, high-precision quadrature sets
must be generated or should be available in tabular form;
‚ Numerical approximations with N ą 20 may be justified if the inte￾grand (having singularities, rapid oscillations, etc.) behaves badly;
‚ Theoretical error estimation is not an easy task.
TABLE 8.6
i ui wi xi fpxiq wifpxiq
1 ´0.861136 0.347855 0.218127 1.081467 0.376194
2 ´0.339981 0.652145 1.036755 1.058232 0.690121
3 0.339981 0.652145 2.104837 1.058232 0.690121
4 0.861136 0.347855 2.923466 1.081467 0.376194
ř wifi = 2.132630
The Gauss quadrature pxi, wiq, integrands fpxiq, and weighted products wifpxiq
are calculated and presented in Table 8.6. The sum of the last column gives the
weighted sum, Eq. (8.65). Substituting this result into the above expression yields
G4 “ π
2
p2.132630q “ 3.349925
Discussion: The true integral, correct to eight decimal places, is 3.34686849. The
true (absolute) error is determined to be 0.0030567, which is accurate to two deci￾mal places. For G6, G8, and G10, the true errors are calculated to be 1.1555ˆ10´4,
5.6852ˆ10´6, and 3.1621ˆ10´7, respectively. This example illustrates how fast the
Gauss-Legendre method converges. Especially note that the G10 yielded a six￾decimal place accurate estimate with only ten function evaluations.
8.8 COMPUTING IMPROPER INTEGRALS
So far, we have dealt with the numerical computation of definite integrals with a finite
domain of integration and a finite range on this domain. However, in practice, we encounterNumerical Integration  453
two types of integrals that do not meet the foregoing conditions. In this regard, we refer to
an integral as “improper integral” when the interval of integration or the integrand itself is
unbounded. Improper integrals can present challenges to numerical computation.
Type I (improper) integrals are definite integrals that are infinite at one or both end￾points, that is, the integration interval is infinite or semi-infinite (see Fig. 8.11). Another
class of improper integrals (Type II) have integrands with a singularity within or on one
or both end points of the integration interval, i.e., the integration interval is finite, but the
integrand is unbounded (see Fig. 8.12).
Type I improper integrals have the following general forms:
ż 8
a
fpxqdx or ż ´b
´8
fpxqdx or ż 8
´8
fpxqdx
where a ě 0 and b ě 0.
An improper integral can be estimated numerically, provided that it is convergent. If
we recall the physical interpretation of a definite integral, Type I integrals can similarly be
interpreted as the area bounded by the x-axis, the integral interval, and the curve, provided
that fpxq ą 0. It is important to determine whether a definite integral is convergent or not
before attempting to estimate it. If an improper integral is divergent, the true integral (area
under the curve) will be infinite; in other words, the numerical estimate will differ as the
number of integration points (or panels) continuously increases.
Coordinate transformation: Before moving on to discuss the numerical integration of
Type I and Type II integrals, we should point out that, in some cases, it is possible to avoid
integrals with discontinuities by applying a suitable coordinate transformation.
In order to avoid Type I integrals over the semi-infinite interval, the two integrals
presented above can be transformed into the following integrals upon x “ 1{u (or x “ ´1{u)
substitution:
ż 1{a
0
g
ˆ 1
u
˙ du
u2 or ż 1{b
0
g
ˆ
´ 1
u
˙ du
u2
If gp˘1{uq{u2 is not singular (or has a removable singularity) at u “ 0, the numerical
integration can be safely and efficiently carried out with one of the previously covered
numerical integration methods.
One may devise various u-substitutions to transform such an improper integral into
one with a finite interval. For instance, r0, 8q can be transformed to r0, Ls by x“Lu{pu`cq
(where c is a real constant), or to [0,1] by x “ tanh u, and so on. Alternatively, when the
FIGURE 8.11: Graphical depiction of integration domains for Type I integrals with infinity at (a)
the upper limit, (b) the lower limit, or (c) both limits.454  Numerical Methods for Scientists and Engineers
FIGURE 8.12: Graphical depiction of integrands for Type II integrals with discontinuities at the (a)
upper end, (b) lower end, or (c) inside the integration interval.
transformations do not work, an integral over p0, 8q can be split into two integrals as
ż 8
0
fpxqdx “
ż a
0
fpxqdx `
ż 8
a
fpxqdx
Typically, fpxq is finite in a bounded interval, r0, as, while fpxq « 0 everywhere else. Taking
this into account, the first integral can be estimated by any of the methods discussed earlier.
Then, a coordinate transformation such as x “ 1{u or x “ a{p1 ´ uq can be employed for
the second integral so that the integration interval is finite.
The integration over p´8, ´bq (b ě 0) can be handled in the same manner as the ones
over pa, 8q (a ě 0). Depending on the numerical method to use, the integration of functions
over p´8, 8q is carried out by first splitting the integral into either two or three convenient
parts, i.e., p´8, 0q and p0, 8q or p´8, ´bq, p´b, aq, and pa, 8q. Any suitable method can
then be applied to each integral separately.
Avoid calculating Type I integrals directly if possible. First, in￾vestigate whether the integral can be transformed into a defi￾nite integral (having a continuous integrand) with a suitable u￾substitution. Whenever possible, prefer approximating definite in￾tegrals to improper integrals.
Improper integrals of Type II have a discontinuity at the lower, upper, or both end
points, or at an interior point, as illustrated in Fig. 8.12. Recall that the definite integral of a
function with closed Newton-Cotes formulas is based on the assumption that the integrand
is continuous and defined on [a, b]. There is, however, an exception where the integrand has
a multiplying factor, the so-called weight function. The integral can be handled numerically
if the weight function is singular.
Some improper integrals of Type II can be transformed to standard definite integrals
in a similar manner as for Type I, by using a suitable change of variable that completely
eliminates the discontinuity of the integral. For example, the following integrands having a
discontinuous point at either lower or upper end points are transformed into standard form
with the indicated u-substitutions:
ż 1
0
fpxq
?4 x
dx `
x “ u4˘ Ñ 4
ż 1
0
u2fpu4qdu
ż 1
0
fpxq
?1 ´ x
dx `
1 ´ x “ u2˘ Ñ 2
ż 1
0
fp1 ´ u2qduNumerical Integration  455
ż 1
´1
fpxq
?
1 ´ x2
dx px “ sin uq Ñ ż π{2
´π{2
fpsin uqdu
where fpxq is a bounded function over the entire integration range.
For a bounded fpxq on [0,1], a discontinuity at x “ 0 of the following integrals is
eliminated with the indicated change of variable, while the transformed integrals are now
the ones over a semi-infinite interval, i.e., p0, 8q or p´8, 0q. These integrals can then be
calculated using a suitable quadrature method.
ż 1
0
fpxqln xdx pln x “ ´uqÑ´ ż 8
0
ue´ufpe´uqdu
ż 1
0
fpln xqdx pln x “ uq Ñ ż 0
´8
eufpuqdu
Subtraction of singularity: In cases where integral transformations cannot offer a
remedy, a singularity can be eliminated by a technique referred to as subtraction of singu￾larity. The discontinuity is removed by adding and subtracting an integrable function from
the integrand, thereby forming two integrals, one with a removable singularity and the other
that can be evaluated analytically or numerically. For example, in the definite integral of
ecx{
?x on [0, 1], the integrand has a non-removable singular point at x “ 0 that behaves
as 1{
?x. By subtracting and adding 1{
?x to the integrand, the result can be expressed in
two parts as follows:
ż 1
0
ecx
?x
dx “
ż 1
0
ecx ´ 1
?x
dx `
ż 1
0
dx
?x
The second integral is easily integrated analytically, which gives “2.” On the other hand, the
first integral can be evaluated numerically using N-panel composite trapezoidal or Simpson’s
rules, expressed as
ż 1
0
ecx ´ 1
?x
dx – w0
ˆecx0 ´ 1
?x0
˙
` ÿ
N
i“1
wi
ˆecxi ´ 1
?xi
˙
where xi “ih for i“0, 1, 2,...,N, and the wi’s are the weights whose values depend on the
method selected. For x0 “0, the first term of this approximation results in 0/0-indeterminate
form. Applying L’Hopital’s rule to remove the indeterminate form yields pecx0 ´1q{?x0 Ñ 0
as x0 Ñ 0. Using this result, the numerical estimate (i.e., weighted sum) is reduced to
ż 1
0
ecx ´ 1
?x
dx – ÿ
N
i“1
wi
ˆecxi ´ 1
?xi
˙
which is now finite.
Next, consider the following example with a singularity at x “ a:
ż a
0
fpxqdx
?
a2 ´ x2
where fpxq is bounded on [0, a], and the singularity arises only from the 1{
?
a2 ´ x2 term.
After adding and subtracting the fpaq{?
a2 ´ x2 term from the integrand, the integral can
be rearranged as a sum of two integrals as follows:
ż a
0
fpxq
?
a2 ´ x2
dx “
ż a
0
fpaq
?
a2 ´ x2
dx `
ż a
0
fpxq ´ fpaq
?
a2 ´ x2
dx456  Numerical Methods for Scientists and Engineers
The first integral is convergent despite its singularity at x “ a, and it can be easily integrated
analytically to give πfpaq{2. The second integral has the 0/0-indeterminate form at x “ a.
Employing then N-panel composite trapezoidal or Simpson’s rules to the integral results in
ż a
0
fpxq ´ fpaq
?
a2 ´ x2
dx – w0Fpx0q ` w1Fpx1q ` ... ` wN´1FpxN´1q ` wN FpxN q
where Fpxq “ `
fpxq ´ fpaq
˘
{
?
a2 ´ x2. Note that FpxN q leads to 0/0-indeterminate form
for x “ a, which can be resolved by applying L’Hopital’s rule: FpxN q“limxÑaFpxq.
Here we use the term “removable singularity” to refer to an in￾tegrand fpxq that has a discontinuity at x “ a but limxÑafpxq
exists. Whenever an integrand has a removable singularity, the
best way to approach it is to remove it analytically and specify
the limit value in the function module accordingly.
Ignoring singularity: This approach is often applied to Type II integrals with a
singularity at the lower, upper, or both endpoints. It is a relatively primitive way of treating
singular integrals, as we circumvent the singular point by applying a numerical integration
rule while disregarding or ignoring the singularity. Inasmuch as the convergence of the
numerical integration rule is ordinarily established only for bounded integrable functions,
the analyst, in a way, blindly applies a numerical procedure and hopes for the best. The
best results are obtained when the Gauss-Legendre quadrature or open Newton-Cotes rules
are applied because the methods inherently avoid endpoints. However, this approach does
not work well for the integrands depicting oscillatory behaviors.
Improper integrals of Type II having a singular point at an interior point x “ c (a ă
c ă b) are handled by simply splitting the integral into two integrals (each integral having
a discontinuity at either endpoint) as shown below, and then a suitable method is applied
to each integral separately.
ż b
a
fpxqdx “
ż c´
a
fpxqdx `
ż b
c`
fpxqdx (8.78)
For Type II integrals, the foregoing techniques discussed are applicable, provided that the
integrals are convergent.
Truncating interval: This technique is usually employed for estimating Type I inte￾grals. Let fpxq be an integrable function on any finite interval [a, b]. An improper integral
of fpxq on ra, 8q can be broken up into two parts:
ż 8
a
fpxqdx “
ż b
a
fpxqdx `
ż 8
b
fpxqdx
where b is a sufficiently large number. Then the first integral is approximated using a suitable
numerical method. The second integral corresponds to the approximation error resulting
from truncating the integral to [a, b]; consequently, an upper bound for this error, Rpbq,
can be estimated by evaluating the integral of fpxq over rb, 8q. To illustrate this approach,
consider the following Type I integral:
ż 8
1
dx
px4 ` 1q
?x “
ż b
1
dx
px4 ` 1q
?x
`
ż 8
b
dx
px4 ` 1q
?xNumerical Integration  457
Here, however, the second integral is not integrable analytically. Hence, to estimate the
error bound, the integrand is approximated to be simple and easily integrable. Note that
for all large real x, we may reduce the integrand as follows:
1
px4 ` 1q
?x
ă
1
x9{2
Then an expression for the upper bound of the approximation error is found as
ż 8
b
dx
px4 ` 1q
?x
ă
ż 8
b
dx
x9{2 “ Rpbq “ 2
7b7{2
Now it is possible to determine a suitable value of b that minimizes Rpbq, while maintaining
the desired accuracy. The error estimates for b “ 5, 10, and 20 are obtained as 1.0222ˆ10´3,
9.04ˆ10´5, and 7.98ˆ10´6, respectively. The error bound of Rp20q “ 7.98 ˆ 10´6 ă 0.5 ˆ
10´5 implies that five decimal place accuracy can be achieved if the first integral is also
calculated better than five decimal places.
8.9 GAUSS-LAGUERRE METHOD
The Gauss-Laguerre method is another powerful quadrature method that serves specifically
to handle Type I improper integrals of the following form:
ż 8
0
e´xfpxqdx (8.79)
Just as in the Gauss-Legendre method, Eq. (8.79) is also expressed as a weighted sum:
ż 8
0
e´xfpxqdx – GLN “ ÿ
N
i“1
wifpxiq (8.80)
where N is the number of quadrature points, and pxi, wiq are the so-called Gauss-Laguerre
quadrature [1]. Here, fpxq is assumed to be bounded in the prescribed semi-infinite domain,
in which case the Gauss-Laguerre sum will converge to an approximate value for increasing
N.
The Laguerre polynomials are defined as
Lnpxq “ ex dn
dxn pxne´xq, n ě 0 (8.81)
With L0pxq “ 1 and L1pxq “ 1 ´ x as starters, any high-degree Laguerre polynomial can
be found by the following recurrence relationship [1]:
Lnpxq“p2n ´ 1 ´ xqLn´1pxq´pn ´ 1q
2
Ln´2pxq, n ě 2 (8.82)
The N-point Gauss-Laguerre quadrature points (or abscissas) are chosen as the zeros of the
N th-degree Laguerre polynomial; that is, LN pxq “ 0. Then, the Gauss-Laguerre quadrature
weights (wi’s) are computed from
wi “ pN!q
2
xirL1
N pxiqs2 (8.83)
where L1
N pxiq is the first derivative of the Nth-degree Laguerre polynomial, and it can be
calculated from the following relationship:458  Numerical Methods for Scientists and Engineers
dLnpxq
dx “ n
x
`
Lnpxq ´ nLn´1pxq
˘
, n ě 1 (8.84)
The Gauss-Laguerre quadrature sets for selected N values are tabulated in Appendix B.2.
The global error is given by
RN “ pN!q
2
p2Nq!
fp2Nq
pξq, 0 ă ξ ă 8 (8.85)
where the upper and lower bounds for this can be determined from the maximum and
minimum values of fp2Nq
pξq on p0, 8q.
The improper integrals that fall into this category do not always come in standard
form (Eq. (8.79)), but they can be expressed in the standard form with a suitable change
of variable followed by a bit of algebra. Some common cases are presented below:
(1) ż 8
0
fpxqdx integrals are transformed to the standard form as follows:
ż 8
0
fpxqdx ”
ż 8
0
e´xFpxqdx (8.86)
where Fpxq “ exfpxq. Equation (8.86) can then be expressed as a weighted sum:
ż 8
0
e´xFpxqdx – ÿ
N
i“1
wiFpxiq “ ÿ
N
i“1
Wifpxiq (8.87)
where Wi denotes the modified weights defined as Wi “ wiexi (see Appendix B.2).
(2) ż 8
a
fpxqdx integral is transformed to the standard form by introducing the u “ x´a
substitution, leading to
ż 8
0
fpu ` aqdu or ż 8
0
e´xFpxqdu
where Fpxq “ exfpx ` aq. Note that ş´b
´8 fpxqdx can be treated in the same manner.
The advantages and disadvantages of the Gauss-Laguerre method are pretty much the
same as those of the Gauss-Legendre method. The fact that the integration interval of
the Gauss-Laguerre quadrature is semi-infinite makes it a very powerful method that the
Newton-Cotes methods cannot match.
A pseudomodule, GAUSS_LAGUERRE_QUAD, for generating N-point Gauss-Laguerre
quadrature is provided in Pseudocode 8.7. The module requires the number of quadrature
points (N) and a convergence tolerance (ε) as input, and the quadrature set pxi, wiq is the
output. This module computes the abscissas by finding the zeros of the Nth-degree Laguerre
polynomial (Lnpxq “ 0) that are non-uniformly distributed on p0, 8q. All roots (abscissas)
are covered sequentially from the smallest to the largest within the outermost For-loop over
index variable k. Each root (denoted by the variable u) is found by the Newton-Raphson
iterations, performed within the While-loop as
xpp`1q “ xppq ´ LN pxppq
q
L1
N pxppqq
, for p “ 0, 1, 2,...Numerical Integration  459
Pseudocode 8.7
Module GAUSS_LAGUERRE_QUAD (N, ε, x, w)
\ DESCRIPTION: A pseudomodule to generate N-point Gauss-Laguerre
\ quadrature abscissas and weights for any N ě 2.
\ USES:
\ ABS :: A built-in function computing the absolute value;
\ FACTORIAL :: A function computing N!, Pseudocode 1.4.
Declare: xN , wN \ Declare array variables
f actor Ð N! \ Calculate N! only once
For “
k “ 1, N‰ \ Loop k: Find kth zero, xk
If “
k ď 3
‰
Then \ Apply initial guess estimator for first three zeros
u Ð π2pk ´ 0.25q2{p4N ` 1q
Else \ Case of k ą 3, extrapolates using most recent three zeros
u Ð 3xk´1 ´ 3xk´2 ` xk´3
End If \ u denotes xppq
k
δ Ð 1 \ Initialize error for xk
While “
|δ| ą ε
‰ \ Newton-Raphson iteration loop
L0 Ð 1; L1 Ð 1 ´ u \ Initialize Lagrange polynomials, L0 and L1
For “
m “ 2, N‰ \ Apply Eq. (8.82) to find LN
L2 Ð p2m ´ 1 ´ uq ˚ L1 ´ pm ´ 1q2 ˚ L0
L0 Ð L1; L1 Ð L2 \ Keep two most recent polynomials
End For
dL Ð n ˚ pL1 ´ n ˚ L0q{u \ Compute L1
N puq by Eq. (8.84)
δ Ð L2{dL \ Find difference between two-successive iterates
u Ð u ´ δ \ Finc current estimate upp`1q
End While
xk Ð u \ Save converged root as xk
wk Ð pf actor{dLq2{u \ Compute weight from Eq. (8.83)
End For
End Module GAUSS_LAGUERRE_QUAD
where LN pxq and L1
N pxq are computed recursively using Eq. (8.82) and (8.84). In this
algorithm, however, it is crucial to start the iteration process with an initial guess close
enough to each zero of LN pxq to avoid finding the same zero over and over again. To serve
this purpose, an initial guess value in close proximity to the true abscissa is calculated with
the following asymptotic approximation:
xp0q
i “ pi ´ 0.25q2π2
4N ` 1
which works well for iă4. Nonetheless, the initial guesses become worse for large x. Hence,
the initial guess for i ě 4 is estimated by using the following extrapolation formula by
Takemasa [30]:
xp0q
i “ 3xp0q
i´1 ´ 3xp0q
i´2 ` xp0q
i´3
The iteration process is terminated when the stopping criterion (ˇ
ˇδppq
ˇ
ˇ ă ε) is met. Following
the initial guess estimation, the Newton-Raphson algorithm converges to a root very quickly.
However, the number of maximum iterations (maxit) may be set to prevent an infinite loop
for large N. The quadrature weights are then obtained from Eq. (8.83).460  Numerical Methods for Scientists and Engineers
EXAMPLE 8.7: Computing improper integrals of Type I
The Debye’s model for the heat capacity, CV , for the low-temperature limit (T ! TD)
leads to the following expression:
CV
kNa
– 9
ˆ T
TD
˙3 ż 8
0
x4exdx
pex ´ 1q
2
where k is the Boltzmann’s constant, Na is the number of atoms in the solid, and T
and TD are the absolute and Debye temperatures, respectively. Apply N “ 2, 4, 6,
and 8-point Gauss-Laguerre quadrature to estimate the above integral over p0, 8q
and compare the estimates with the true value of 4π4{15.
SOLUTION:
The integral in question is an improper integral of Type I that is integrated over
a semi-infinite interval, so the Gauss-Laguerre method can be applied to calculate
its approximate value. However, the integral is not in the standard Gauss-Laguerre
form but can be easily converted to standard form as follows:
ż 8
0
x4exdx
pex ´ 1q
2 “
ż 8
0
e´xfpxqdx – GLN “ ÿ
N
i“1
wifpxiq
where pxi, wiq are the Gauss-Laguerre quadrature abscissas and weights, and the
integrand takes the form fpxq “ x4e2x{pex ´ 1q
2
.
TABLE 8.7
N 2468
GLN 21.7878911 25.9906374 25.9746594 25.9761192
True Error, eN 4.1878665 0.01487975 0.00109822 0.00036155
The numerical estimates of the transformed integral with N “2, 4, 6, and 8-point
Gauss-Laguerre quadrature sets are computed, and the estimated results along with
the true (approximation) errors are presented in Table 8.7. It is observed that the
sequence of the numerical estimates is approaching the true value of 25.9757576
very fast. The ratio of the subsequent true absolute errors (indicative of convergence
speed) for the estimated sequence, eN1 {eN2 , is e2{e4 « 281.4, e4{e6 « 13.55, and
e6{e8 « 3.04. While the convergence speed is very high at small N, it decreases with
increasing N.
Discussion: Note that the original integrand x4ex{pex ´ 1q2 is indeterminate at
x “ 0, and the integrand is bounded on p0, 8q. However, the singularity at x “ 0
does not pose a problem in this example because none of the Laguerre abscissas
coincide with x “ 0. In other words, no special treatment is required for x “ 0.
Nonetheless, the integrand for the transformed case, fpxq, is unbounded on p0, 8q
as it is likewise indeterminate at x “ 0. While the numerical integration using
the Gauss-Laguerre quadrature works for the unbounded fpxq from an analytical
perspective, the integration in the transformed form is not always numerically stable.
In this example, even though fpxq is unbounded, we observe that the convergence
rate slows down with increasing N as the largest abscissa, xN , grows larger.Numerical Integration  461
8.10 GAUSS-HERMITE METHOD
The Gauss-Hermite method is another powerful quadrature technique used in the numerical
approximation of the following types of improper integrals:
ż 8
´8
e´x2
fpxqdx (8.88)
Similar to the other quadrature methods, the integral is replaced with a numerical
approximation expressed as a weighted sum:
ż 8
´8
e´x2
fpxqdx – GHN “ ÿ
N
i“1
wifpxiq (8.89)
where N is the number of quadrature points and pxi, wiq is a set of parameters referred to
as Gauss-Hermite quadrature [1]. The quadrature abscissas are the zeros of the nth-degree
Hermite polynomial, which is defined as
Hnpxq “ p´1q
nex2 dn
dxn pe´x2
q, n ě 0 (8.90)
where H0pxq “ 1 and H1pxq “ 2x. Higher-degree Hermite polynomials can also be con￾structed with the aid of the following recurrence relationship:
Hnpxq “ 2xHn´1pxq ´ 2pn ´ 1qHn´2pxq, n ě 2 (8.91)
The first derivative of the nth-degree Hermite polynomial satisfies the following rela￾tionship:
dHnpxq
dx “ 2nHn´1pxq, n ě 1 (8.92)
The computation of the Gauss-Hermite quadrature set requires the solution of a system
of 2N nonlinear equations, as in the case of the Gauss-Legendre quadrature (see Section
8.7). The zeros of an Nth-degree Hermite polynomial are set as the abscissas, i.e., the
solution of Hnpxq “ 0 gives xi’s. Accordingly, the non-linear system with 2N unknowns
is reduced to a linear system with the weights (wi’s) as unknowns. It turns out that the
weights also satisfy the following expression:
wi “
?π N! 2N`1
p H1
N pxiqq2 (8.93)
Gauss-Hermite quadrature sets are tabulated for selected N values in Appendix B.3. The
global error for the Gauss-Hermite method is given as
RN “
?πpN!q
2N p2Nq!
fp2Nq
pξq, ´8 ă ξ ă 8 (8.94)
A pseudomodule, GAUSS_HERMITE_QUAD, generating N-point Gauss-Hermite
quadrature is presented in Pseudocode 8.8. The module requires the number of points (N)
and convergence tolerance (ε) as input. The module output is the quadrature set pxi, wiq for
i “ 1, 2,...,N. The zeros of HN pxq are symmetrically distributed on p´8, 8q. Hence, the
duplicate zeros with the opposite sign are not computed. However, the Newton-Raphson
procedure may end up with a previously detected zero if the initial guess is not sufficiently
close to the root.462  Numerical Methods for Scientists and Engineers
Pseudocode 8.8
Module GAUSS_HERMITE_QUAD (N, ε, x, w)
\ DESCRIPTION: A pseudomodule to generate N-point Gauss-Hermite quadrature
\ abscissas and weights for N ě 2.
\ USES:
\ MOD :: A built-in function returning the remainder of x divided by y;
\ ABS :: A built-in function computing the absolute value;
\ SQRT:: A built-in function computing the square-root of a value;
\ FACTORIAL:: Function Module computing factorial, Pseudocode 1.4.
Declare: xN , wN \ Declare array variables
io Ð MODpN, 2q \ Find remainder of N/2.
If “
io “ 0
‰
Then \ Number of zeros is even
m “ N{2 \ Find half of the zeros
Else \ io “ 1, Number of zeros is odd
m “ pN ` 1q{2 \ Find half of the zeros (includes x “ 0)
End If
f actor Ð 2N`1N!
?π \ Precalculate the factor for weights
For “
k “ 1, m‰ \ Loop k: Find the abscissa xk
If “
k ď 4
‰
Then \ Apply initial guess estimator for first four zeros
If “
io “ 0
‰
Then \ Case of even N
u Ð pk ´ 0.5qπ{
?2N ` 1
Else \ Case of odd N
u Ð pk ´ 1qπ{
?2N ` 1
End If
Else \ Apply extrapolation formula for k ą 4
j Ð m ` k ´ io \ Find location of new zero
u Ð 3xj´1 ´ 3xj´2 ` xj´3 \ Extrapolate using most recent 3-zeros
End If
δ Ð 1 \ Initialize successive difference
While “
|δ| ą ε
‰ \ Newton-Raphson iteration loop for xk
H0 Ð 1 \ Initialize H0
H1 Ð 2u \ Initialize H1
For “
i “ 2, N‰ \ Apply Eq. (8.91) to find HN
H2 Ð 2 ˚ pu ˚ H1 ´ pi ´ 1q ˚ H0q
H0 Ð H1 \ Assign Hi´2 Ð Hi´1
H1 Ð H2 \ Assign Hi´1 Ð Hi
End For
dH Ð 2 ˚ N ˚ H0 \ Compute H1
N puq by Eq. (8.92)
δ Ð H2{dH \ Difference between two successive iterates
u Ð u ´ δ \ Find current estimate upp`1q
End While
j Ð m ` k ´ io \ Find location of positive root
xj Ð u \ Save converged root as xk
wj Ð f actor{dH2 \ Compute weight by Eq. (8.93)
xN`1´j Ð ´u \ Find (negative) abscissas
wN`1´j Ð wj \ Find corresponding weights
End For
End Module GAUSS_HERMITE_QUADNumerical Integration  463
Thus, to get around this problem, the initial guess that is sufficiently close to the true root
is estimated for i ď 4 as:
xp0q
i “
$
’’’&
’’’%
p2i ´ 1qπ
2
?2N ` 1
, N “ even,
pi ´ 1qπ ?2N ` 1
, N “ odd
(8.95)
which are predictions sufficiently close to true ones. For i ě 5, an initial guess is computed
from the extrapolation formula xp0q
i “ 3xp0q
i´1 ´ 3xp0q
i´2 ` xp0q
i´3, introduced by Takemasa [31].
For odd N, one abscissa (zero= of Hnpxq is x “ 0. For even N, the positive abscissas
(m “ pn`1q{2) are computed in the outermost For-loop (xi, k “ 1, 2,...,m). The Newton￾Raphson iteration loop to find xk is carried out by the While-loop):
upp`1q “ uppq ´ HN puppq
q
H1
N puppqq
where HN pxq and H1
N pxq are evaluated recursively using Eqs. (8.91) and (8.92). The it￾eration process is terminated when the stopping criterion, |upp`1q ´ uppq
| ă ε, is met, i.e.,
While-loop is terminated. Once an abscissa (zero) is found, its corresponding weight is com￾puted from Eq. (8.95) [28]. Negative abscissas are determined from known positive abscissas,
while the corresponding weights are the same as the weights of positive abscissas.
The reader is warned that computing the weights for cases with a
large N (N ą 20) will result in “arithmetic overflow.” As a remedy
to avoid arithmetic overflow, an orthonormal Hermite polynomial
set is used (see Ref. [31]).
Any improper integral of fpxq bounded on p´8, 8q can be evaluated once it is trans￾formed to the standard form, Eq. (8.88), as follows:
ż 8
´8
fpxqdx “
ż 8
´8
e´x2
Fpxqdx (8.96)
where Fpxq “ ex2
fpxq. Then Eq. (8.96) can be expressed as a weighted sum as follows:
ż 8
´8
e´x2
Fpxqdx – ÿ
N
i“1
wiex2
i fpxiq “ ÿ
N
i“1
Wifpxiq (8.97)
where Wi “ wiex2
i are the modified weights, also presented in Appendix B.3.
EXAMPLE 8.8: Computing improper integrals of Type I
The standard normal distribution has the following probability density:
Fpxq “ 1
?2π
e´x2{2, ´8 ă x ă 8
(a) Use 2, 4, 6, 8, and 10-point Gauss-Hermite quadrature to estimate the integral
of Fpxq over the entire domain, and (b) can you estimate the upper bound of the464  Numerical Methods for Scientists and Engineers
global error for any N? Recall that the true value of the integral of Fpxq over the
entire domain is “1.”
SOLUTION:
(a) This integral is not in standard Gauss-Hermite form, but it can be trans￾formed into standard form as follows:
ż 8
´8
Fpxq dx “
ż 8
´8
e´x2
fpxq dx – GHN “ ÿ
N
i“1
wifpxiq
where fpxq “ ex2{2{
?2π. Note that fpxq, an even function, has a local minimum
at x “ 0, and it is unbounded as x Ñ ˘8. Nevertheless, Fpxq itself is a bounded
function on (´8, 8), so the desired integral is convergent. The unboundedness of the
integrand of the transformed integral, fpxq, however, can lead to slow convergence.
TABLE 8.8
N 246810
GHN 0.90794308 0.97038621 0.99896336 0.99988715 0.99998764
eN 0.09205692 0.02961379 0.00103664 0.00011285 1.236ˆ10´5
Using the Gauss-Hermite quadrature sets given in Appendix B.3, the numeri￾cal estimates GHN , along with the true approximation errors eN “ 1´GHN , are
presented in Table 8.8 for N “ 2, 4, 6, 8, and 10. It is observed that the absolute
errors are about 9.2% and 2.96%, for GH2 and GH4, respectively. But the numerical
estimates improve as N is increased, leading to absolute errors less than 0.1% for
N ě 6.
(b) In order to estimate the global error bounds, e.g., for N “ 2 and 4, the fourth
and eighth derivatives of the integrand are required:
fp4q
pxq“p3 ´ 24x2 ` 8x4qex2{2,
fp8q
pxq“p105 ` 420x2 ` 210x4 ` 28x6 ` x8qex2{2
We observe that both fp4q
pxq and fp8q
pxq are unbounded as x Ñ ˘8 and have
a local minimum at x “ 0. Hence, it is not possible to find an upper bound for the
numerical estimates since the higher derivatives are unbounded. However, we can
find the lower bounds for the estimates since fp4q
p0q “ 3, and fp8qp0q “ 105; in other
words, fp4qpξq ě 3 and fp8qpξq ě 105. Then the lower bounds of the global error are
found by R2 ą 2!?π{224! “ 0.1108 and R4 ą 4!?π{248! “ 0.00692.
Discussion: The original integrand Fpxq is bounded, and its integral is conver￾gent over the entire integration domain because Fpxq decays very fast as x Ñ ˘8.
However, when the integral is transformed to suit the general Gauss-Hermit form,
the transformed form of the integral can become unstable or cause slow convergence
problems due to the unboundedness of the new integrand, fpxq. Yet the integrals are
likely to converge for sufficiently large N. Nonetheless, given that GH10 is computed
with only 10 function evaluations with an absolute error of 1.236ˆ10´5, the method
is superior to other methods such as trapezoidal or Simpson’s rules, which can only
be applied after truncating the integral over a bounded range.Numerical Integration  465
8.11 GAUSS-CHEBYSHEV METHOD
The Gauss-Chebyshev method is also a powerful quadrature method that can handle the
numerical integration of integrands that have discontinuities in both the lower and upper
integral limits. The standard form of the integral is
ż 1
´1
fpxqdx
?
1 ´ x2 (8.98)
where fpxq is bounded and p1 ´ x2q´1{2 term is singular at x “ ˘1.
The integral is approximated by a weighted sum as follows:
ż 1
´1
fpxqdx
?
1 ´ x2
– CN “ ÿ
N
i“1
wifpxiq (8.99)
where CN denotes the N-point Gauss-Chebyshev approximation and pxi, wiq are so-called
the Gauss-Chebyshev quadrature.
The abscissas for an N-point quadrature are given by the zeros of the Chebyshev
polynomial of the first kind, Tnpxq, which are symmetrical about x “ 0. Fortunately, in this
method, the abscissas (xi) and the weights (wi) can be calculated explicitly as follows:
xi “ cos p2i ´ 1qπ
2N and wi “ π
N , i“1, 2,...,N (8.100)
The global error term for the method is given as
RN “ π
p2Nq!22N´1 fp2Nq
pξq, ´1 ă ξ ă 1 (8.101)
A pseudocode for generating the Gauss-Chebyshev quadrature has not been given here
since the quadrature abscissas and weights are explicitly available, Eq. (8.100), which can
be easily calculated practically at no cost.
GENERALIZATION: This method, similar to Gauss-Legendre, can also be extended to
estimate a definite integral over an arbitrary interval after transforming it to the standard
Gauss-Chebyshev form. Consider the following general integral with discontinuities at both
ends:
ż b
a
fpxqdx
apx ´ aqpb ´ xq (8.102)
Upon applying the x “ pb ´ aqu{2 ` pb ` aq{2 substitution, the integral is transformed to
standard Gauss-Chebyshev form, which can then be approximated by the weighted sum as
follows:
ż 1
u“´1
f
´b ´ a
2
u `
b ` a
2
¯ du
?
1 ´ u2
– ÿ
N
i“1
wifpxiq (8.103)
where pui, wiq are the Gauss-Chebyshev quadrature defined by Eq. (8.100) and xi “ ppb ´
aqui ` pa ` bqq{2 are the abscissas. The global error term, RN , takes the following form:
RN “ πpb ´ aq
2N`1
p2Nq!24N fp2Nq
pξq, a ă ξ ă b (8.104)466  Numerical Methods for Scientists and Engineers
EXAMPLE 8.9: Application of Gauss-Chebyshev method to Type II integrals
Estimate the following integral using the three-point Gauss-Chebyshev quadrature
and compare the result with the true value of 3π.
ż 3
´1
x2dx
apx ` 1qp3 ´ xq
SOLUTION:
The integral is already in the standard form, Eq. (8.102), with fpxq “ x2. For
N “ 3, the abscissas and weights on p´1, 1q are found from Eq. (8.100) as
u1 “cos π
6 “
?3
2 , u2 “cos π
2 “0, u3 “cos
5π
6 “´
?3
2 , wi “ π
3 pfor all iq
The abscissas in the [´1, 3] interval are calculated by xi “ 2ui ` 1 as
x1 “?
3 ` 1, x2 “1, x3 “´?
3 ` 1
The estimate for the given integral is obtained by Eq. (8.103) as
C3 “ π
3
`
x2
1 ` x2
2 ` x2
3
˘
“ π
3
”
p
?
3 ` 1q
2
` p1q
2 ` p´?
3 ` 1q
2ı
“ 3π
which happens to give the true integral value. Note that the global error term R3,
Eq. (8.104), is
R3 “ π
180fp6q
pξq, ´1 ă ξ ă 3
For fpxq “ x2, the Gauss-Chebyshev sum, C3, should theoretically yield the true
value because fp6qpxq “ 0 and R3 “ 0. Moreover, it is also clear that the global error
R3 will be zero for fpxq being a polynomial of degree m ď 5, i.e., fpxq “ pmpxq,
provided that the numbers are not rounded or truncated.
Discussion: As in all Gaussian quadrature methods, the Gauss-Chebyshev approx￾imation obtained by a weighted sum involves a fixed number of integration points
and provides the true value for polynomials of degree 2N ´1, where N is the number
of integration points. In this example, the quadrature abscissas and weights were de￾termined in their exact forms and used in the arithmetic computations. This process
completely eliminated the possibility of any truncation and/or round-off errors.
Some integrals may partially have Gauss-Chebyshev form; that is, the integrand can
have a discontinuity only at x“a or x“b. For example, consider the integration of gpxq“
fpxq{?b ´ x and gpxq “ fpxq{?x ´ a on [a, b]. These can be expressed in the standard
Gauss-Chebyshev form as follows:
ż b
a
gpxqdx “
ż b
a
fpxqdx
?b ´ x “
ż b
a
fpxq
?x ´ adx
apx ´ aqpb ´ xq
“
ż b
a
Fpxqdx
apx ´ aqpb ´ xq (8.105)
where Fpxq “ fpxq
?x ´ a.Numerical Integration  467
Gauss-Laguerre and Gauss-Hermite Quadrature Methods
‚ High-order accuracy can be achieved using GLN and GHN ;
‚ They converge very quickly for relatively small N if the integrals are
in standard improper forms.
‚ The theoretical error estimation procedure is not an easy task;
‚ They are not suitable for discrete functions;
‚ They are very poor at integrating non-exponentially decreasing inte￾grands;
‚ Highly accurate quadrature sets are required to compute integrals
with high accuracy.
EXAMPLE 8.10: Alternative ways to calculate Type II integrals
Estimate the following integral using (a) 2-, 3-, 4-, and 6-panel open Newton-Cotes
formulas; (b) 5-, 8-, 16-, 32-, and 64-point Gauss-Chebyshev quadrature; (c) 4-, 8-,
16-, 32-, and 64-point Gauss-Legendre quadrature; (d) 4-, 8-, 16-, 32-, and 64-point
Gauss-Laguerre quadrature after transforming the integral into the one on p0, 8q;
and (e) after truncating it to an improper integral of Type I.
ż 1
0
dx
?x
SOLUTION:
This integral is one of the easiest integrals in calculus, whose true value is “2.”
However, we encounter a problem in its numerical integration because the integrand
is discontinuous at x “ 0, which makes it a Type II improper integral.
(a) The closed Newton-Cotes formulas are not suitable for estimating this integral
because the integrand is not defined at x “ 0, i.e., fpxqÑ8 for x Ñ 0`, which is
why we need to resort to open-type formulas. Applying the 2-, 3-, 4-, and 6-panel
composite open Newton-Cotes rules, Eqs. (8.57), (8.58), (8.59), (8.60), and (8.61)
avoid the singularity at x “ 0. The orders of error for the composite rules are Oph2q,
Oph2q, Oph3q, and Oph4q for the 2-, 3-, 4-, and 6-panel schemes, respectively. The
numerical results, keeping the total number of panels (N) the same in each method,
are presented in Table 8.9.
TABLE 8.9
N Eq. (8.57) Eq. (8.58) Eq. (8.59) Eq. (8.61)
24 1.825525 1.811233 1.849491 1.859210
48 1.876562 1.866423 1.924745 1.900447
96 1.912699 1.905522 1.924745 1.929605
192 1.938265 1.933187 1.946787 1.950223
6912 1.989710 1.988864 1.991131 1.991704
We observe that the three-panel formula Eq. (8.58) with K “ 3f 2pξq{4 exhibits
worse performance than that of the two-panel formula Eq. (8.57) with K “ f 2pξq{3,468  Numerical Methods for Scientists and Engineers
even though they both are second-order accurate. The explanation for this situation
lies in the magnitude of the truncation error constants.
(b) The Gauss-Chebyshev method, which inherently avoids the end points, is a
potential remedy for such an integral with an endpoint singularity. However, the
integral is not in the standard Gauss-Chebyshev form. Nevertheless, it could be
put in the standard form by multiplying both the numerator and denominator with
?1 ´ x, giving:
ż 1
0
dx
?x “
ż 1
0
?1 ´ x
axp1 ´ xq
dx
Now, defining fpxq “ ?1 ´ x and using the N-point Gauss-Chebyshev quadrature,
the integral can be approximated as
CN “ ÿ
N
i“1
wi
?1 ´ xi
where xi “ pui ` 1q{2 and pui, wiq are the Gauss-Chebyshev quadratures on (´1, 1).
The numerical estimates for N “ 5, 8, 16, 32, and 64 are presented in Table 8.10.
It is observed that the CN estimates converge very quickly toward the true value of
“2” for increasing N since the discontinuity at the end points is avoided by fpxq.
TABLE 8.10
N 5 8 16 32 64
CN 2.008248 2.003216 2.000803 2.0002 2.00005
(c) The Gauss-Legendre method can be employed for Type II integrals since the
abscissas never coincide with the upper or lower endpoints. The numerical approxi￾mation using the Gauss-Legendre quadrature can be expressed as
ż 1
0
dx
?x « GN “ ÿ
N
i“1
wi
?xi
where pui, wiq is the quadrature set on [´1, 1] and xi “ pui ` 1q{2. The numerical
estimates with N “ 4, 8, 16, 32, and 64 are computed and tabulated in Table 8.11.
It is observed that the Gauss-Legendre estimates slowly converge toward the true
value of 2 for increasing N with a small convergence rate.
TABLE 8.11
N 4 8 16 32 64
GN 1.806342 1.89754 1.947225 1.973201 1.986421
(d) To avoid the singular point at x “ 0, we may employ the x “ 1{y2 substitu￾tion, which yields the following Type I improper integral:
ż 1
0
dx
?x “ 2
ż 8
1
dy
y2
The new integral has a discontinuity at y “ 0, outside the integration domain r1, 8q,
and moreover, it is not in the standard Gauss-Laguerre form either. Next, the integralNumerical Integration  469
is transformed to the standard form by multiplying the integrand with e´ueu under
the integral sign and applying the u “ y ´ 1 substitution as follows:
2
ż 8
1
dy
y2 “ 2
ż 8
0
e´u eudu
pu ` 1q
2
Having defined fpxq “ ex{px ` 1q2, the 4-, 8-, 16-, 32-, and 64-point Gauss-Laguerre
quadratures are used to estimate the transformed integral. The tabulated results in
Table 8.12 indicate that this alternative integration technique also converges slowly.
TABLE 8.12
N 4 8 16 32 64
GLN 1.852473 1.930778 1.96633 1.983509 1.991889
(e) The integrand of the improper integral over the 1 ď y ă 8 interval obtained
in Part (d) is continuous in this interval since y “ 0 lies outside the integration
range. Hence, this integral can be truncated into a proper definite integral, which
can then be approximated by either the trapezoidal or, preferably, Simpson’s rules.
To determine an interval that minimizes the truncation error, the integral is split
into two integrals:
ż 8
1
dy
y2 “
ż b
1
dy
y2 `
ż 8
b
dy
y2
where the first integral is the truncated integral and the second one, denoted by
Rpbq, is the approximation error due to truncation. In this example, the analytical
integration of Rpbq is quite simple and yields Rpbq “ 1{b. It is clear that to get an
estimate that is accurate to four significant digits (i.e., Rpbq “ 1{b ă 10´4), the
value of b should be greater than 104. Hence, we can conclude that this alternative
technique is computationally impractical.
Discussion: Even though the numerical approximations with the open Newton￾Cotes rules and Gauss-Legendre method improve as the number of integration points
increases, the convergence rate is slow. However, the integrand in the Chebyshev sum
avoids the endpoint discontinuities. The approximation C64 (estimate with N “ 64)
yields an answer correct to four decimal places.
Another technique that can be used to avoid singularity in the integrals is to
transform an integral into a new one whose integrand is not singular in the new
range. In this regard, even though the integrand 1{x2 is bounded on [1, 8), the
integrand in the transformed space fpxq “ ex{px ` 1q2 is not bounded. This feature
accounts for the slow convergence with increasing N.
Finally, resorting to blindly truncating improper integrals is not recommended
unless the truncation error is estimated, as demonstrated in this example. In cases
where Rpbq is difficult to obtain analytically, find an upper bound by approximating
the integrand to a form that can be easily integrated (see Section 8.8). Another
alternative is to obtain the numerical estimates for several b values (b1 ă b2 ă
b3). Then, for instance, using Simpson’s rule for a fixed h, calculate the difference
(|SN pb1q ´ SN pb2q|, |SN pb2q ´ SN pb3q|, and so on) and determine the value of b,
which will be acceptable.470  Numerical Methods for Scientists and Engineers
8.12 COMPUTING INTEGRALS WITH VARIABLE LIMITS
Definite integrals with variable upper or lower integration limits are frequently encountered
in science and engineering. Such integrals may be generalized as follows:
Ipxq “ ż vpxq
y“upxq
fpx, yq dy (8.106)
where upxq and vpxq are functions of x. Hence, in such cases, it may be difficult to determine
in advance the number of integration points required to ensure that the computed integral
is estimated with the desired accuracy. To overcome this problem, the definite integral is
preferably transformed into an integral with fixed (constant) upper and/or lower limits. For
instance, upon applying the following substitution to Eq. (8.106)
ypx, tq “ upxq`pvpxq ´ upxqq t ´ c
d ´ c (8.107)
the integral is transformed into an integral over an arbitrary interval [c, d]:
Ipxq “ ż d
t“c
Fpx, tq dt with Fpx, tq “ vpxq ´ upxq
d ´ c
f
`
x, ypx, tq
˘ (8.108)
Once the transformation is completed, a suitable numerical method is applied to esti￾mate Eq. (8.108) for an arbitrary x. The Gauss-Legendre or Gauss-Chebyshev quadrature
methods, for instance, can be applied due to the considerable computational savings they
offer. It is often more convenient to transform Eq. (8.106) to an integral on [´1, 1] to take
advantage of tabulated quadrature in this range so that no further processing is necessary.
EXAMPLE 8.11: Computing integrals with variable limits
Propose a suitable numerical method to estimate the following integral, and trans￾form the integral to one with constant upper and lower endpoints.
gpxq “ ż x
´x
Fpx, tq with Fpx, tq “ sin2ptq
?
t2 ´ x2
dt
SOLUTION:
?
(a) The integrand Fpx, tq is discontinuous at the endpoints, x “ ˘t. Having
t2 ´ x2 in the denominator makes it a suitable candidate for the Gauss-Chebyshev
method. Therefore, it is more convenient to transform the integral to the one on
[´1, 1].
(b) To obtain the standard Gauss-Chebyshev form, we apply Eq. (8.107) to obtain
the following transformation:
t “ ´x ` xpy ` 1q “ x y Ñ dt “ x dy
Substituting these into the integral and simplifying leads to
gpxq “ ż x
´x
sin2ptq
?
x2 ´ t2
dt “ x
|x|
ż `1
y“´1
sin2pxyq
a1 ´ y2 dyNumerical Integration  471
Defining fpyq “ x sin2pxyq{ |x|, the integral is in standard Gauss-Chebyshev
form, which may be replaced by an N-point Gauss-Chebyshev sum:
ż `1
y“´1
fpyq
a1 ´ y2 dy – π
N
ÿ
N
n“1
fp ynq
Finally, substitution of the quadrature sum into the transformed integral yields
gpxq “ ż x
´x
sin2ptq
?
x2 ´ t2
dt – CN “ π
N
x
|x|
ÿ
N
i“1
sin2px yiq
where yi’s are the abscissas of the Gauss-Chebyshev quadrature on [´1, 1].
Discussion: It can be difficult to determine and/or apply a suitable method or
treatment (number of panels or quadrature points, error estimation, etc.) for the
integrals with variable limits. However, it becomes easier once such an integral for
an arbitraty x is transformed into one with constant integration limits.
8.13 DOUBLE INTEGRATION
Numerical approximation of double integrals is relatively easy when the integration domain
is rectangular (Domain D: a ď x ď b, c ď y ď d, see Fig. 8.13a). To illustrate the integration
of fpx, yq over a rectangular domain, consider the following double integration:
I “
ż b
x“a
ż d
y“c
fpx, yqdydx (8.109)
where fpx, yq is assumed to be an integrable function that cannot be expressed as a product
of two single-variable functions, i.e., fpx, yq ‰ XpxqY pyq. In the case of fpx, yq “ XpxqY pyq,
the double integral is reduced to the product of two single integrals, p
şb
a Xpxqdxqpşd
c Y pyqdyq,
for which any suitable method or technique applicable to single integrals can be used.
Fubini’s theorem states that the double integral of any continuous function fpx, yq over
a rectangular domain (D as shown in Fig. 8.13a) can be calculated as an iterated integral
FIGURE 8.13: Domains of integration (a) rectangular, (b) irregular.472  Numerical Methods for Scientists and Engineers
in either order of integration:
ĳ
D
fpx, yqdA “
ż b
a
ż d
c
fpx, yqdydx “
ż d
c
ż b
a
fpx, yqdxdy
In this regard, we choose to compute the integral with respect to y first:
Fpxq “ ż d
y“c
fpx, yqdy (8.110)
Equation (8.109) can then be cast as a single definite integral as
I “
ż b
x“a
Fpxqdx (8.111)
Any suitable numerical method can be employed for Eq. (8.111), assuming it is not
singular in or on the boundaries of domain D. However, recall that numerical integration
of some integrands with removable singularities can be obtained using a suitable technique.
In generalizing the numerical integration procedure, we exploit the idea that all nu￾merical estimates can be expressed as a weighted sum. So we approximate Eq. (8.111) as
ż b
a
Fpxq dx – ÿ
N
i“1
wxiFpxiq (8.112)
where N is the number of integration points and pxi, wxiq for i “ 1, 2,...,N is a set of
abscissas and weights depending on the chosen method.
To complete the numerical calculation of Eq. (8.109), Equation (8.110) is also expressed
as a weighted sum for any x in [a, b] as follows:
Fpxq « ÿ
M
j“1
wyjfpx, yj q (8.113)
where M is the number of integration points, and pyi, wyiq for i “ 1, 2,...,M is a set of
abscissas and weights depending on the method selected. It should be pointed out that the
number of integration points (or panels) or the numerical method for the integration over
each independent variable need not be the same. In other words, the analyst is free to select
the most suitable numerical method or the number of integration points for each integration
variable. It should be kept in mind, however, that the accuracy of the numerical estimates
will be affected by the number of integration points and the applied methods, which will
contribute to the global error.
Combining with Eqs. (8.112) and (8.113) for xi’s, we arrive at
ż b
x“a
ż d
y“c
fpx, yqdydx « ÿ
N
i“1
wxi ÿ
M
j“1
wyjfpxi, yj q (8.114)
In the case of double integration in an irregular domain, as depicted in Fig. 8.13b, the
range of integration over the y variable is upxq ď y ď vpxq, which is transformed to an
integral with a fixed integration interval, as discussed in Section 8.13. Then, the double
integral can be expressed by the following approximation:
ż b
x“a
ż vpxq
y“upxq
fpx, yq dydx–
1
d ´ c
ÿ
N
i“1
wxipvi´uiq
ÿ
M
j“1
wtj f
´
xi, ui``
tj´c
˘vi´ui
d´c
¯
(8.115)Numerical Integration  473
where the integral over the variable y is transformed to an integral over the variable t in an
arbitrary interval [c, d] using Eq. (8.107). The lower and upper endpoints are abbreviated as
ui “ upxiq, vi “ vpxiq, and pti, wtiq is the set of abscissas and weights on [c, d] depending
on the chosen numerical method.
EXAMPLE 8.12: Computing hydrostatic pressure force by double integration
A shipwreck is lying upside down on a flat seafloor at a depth of H m, as illustrated
in the figure below.
FIGURE 8.14
The ship, a three-dimensional space region (V ), is bounded by the z “ 0 plane
(the seafloor), a sphere (the top: z “ a42 ´ x2 ´ y2) and on the sides by parabolic
planes (the hulls: y “ ˘3px2{16 ´ 1q). The ship is subjected to hydrostatic pressure
force, dF “ ρghpzq per differential area dA. The total force is found by integrating
over the bottom surface area of the ship, leading to the following double integral:
F “
ĳ
ρghpzqdA “ ρg ż 4
x“´4
3p1´x2{16q
ż
y“3px2{16´1q
´
H ´ a16 ´ x2 ´ y2
¯
dy dx
First transform both integrals to the ones on [´1, 1], and then, for H “ 100 m,
estimate the double integral using 2-, 3-, and 4-point Gauss-Legendre quadrature on
both the x- and y-integrals and assess the accuracy of the integrals. The lengths in
the figure are given in meters.
SOLUTION:
By making the x “ 4x1 substitution (dx “ 4dx1
), the integral over x is easily
transformed to an integral on ´1 ď x1 ď 1. On the other hand, to transform the
integral over the y-variable to the one on ´1 ď y1 ď 1, we employ Eq. (8.108) to
convert y “ 3px2{16 ´ 1q to y1 “ ´1 and y “ 3p1 ´ x2{16q to y1 “ 1 as
y “ 3
´x2
16 ´ 1
¯
`
3
2
´
1 ´ x2
16
¯
py1 ` 1q
Also, substituting x1 “ x{4 leads to y “ 3p1 ´ x12
qy1 and dy “ 3p1 ´ x12
qdy1 for
´1 ď y1 ď 1. We can now express the integration over the y-variable as
I1px1
q “ 3p1 ´ x12
q
ż `1
y1“´1
´
H ´
b
16 ´ 16x12 ´ 9p1 ´ x12q
2
y12
¯
dy1474  Numerical Methods for Scientists and Engineers
Next, this integral is replaced with the N-point Gauss-Legendre sum:
I1px1
q – 3p1 ´ x12
q
ÿ
N
i“1
wi
´
H ´
b
16 ´ 16x12 ´ 9p1 ´ x12q
2
y2
i
¯
where pyi, wiq are the N-point Gauss-Legendre quadrature set on [´1, 1] (see Ap￾pendix B.1).
TABLE 8.13
ij x1
i w1
i y1
j w1
j w1
iw1
jFi,j
1 1 ´0.57735 1 ´0.57735 1 775.5596
1 2 ´0.57735 1 0.57735 1 775.5596
2 1 0.57735 1 ´0.57735 1 775.5596
2 2 0.57735 1 0.57735 1 775.5596
3102.2384
The x1
-integration is also replaced by the N-point Gauss-Legendre sum. Using
the same quadrature set, pxi, wiq”pyi, wiq, we write
I2 “
ż 4
x“´4
I1pxqdx “ 4
ż 1
x1“´1
I1px1
qdx1 – 4
ÿ
N
j“1
wj I1px1
j q
Finally, substituting I1px1
q into the above expression yields
I2 “ ÿ
N
j“1
wj
ÿ
N
i“1
wiFpx1
j , y1
iq
where
Fpx1
, y1
q “ 12p1 ´ x12
q
´
H ´
b
16 ´ 16x12 ´ 9p1 ´ x12q
2
y12
¯
The numerical details for only the N “M “2 case are tabulated in Table 8.13. For
N “M “2, 3, and 4, the numerical estimates are found to be 3102.2384, 3097.5511,
and 3096.7293, respectively.
Discussion: The error estimation for the Gauss-Legendre method in practice can
be very difficult, even for the integration of single-variable functions, since it requires
the 2Nth derivative. Yet, the true error may be much less than a bound established
by the RN . Another approach to estimating the error in numerical approximation
is to use two Gaussian-Legendre quadrature sets of different orders and to obtain
the error as the difference between the two estimates. In this example, the absolute
error between the estimates with N “ 2 and N “ 3 is 4.6873 (or 0.15%), while the
estimates with N “ 3 and N “ 4 are 0.8218 (or 0.0265%), which indicates that the
integral computed even with N “ M “ 2 is a good approximation for the double
integral.Numerical Integration  475
8.14 CLOSURE
In science and engineering, we basically encounter the definite integration of two kinds of
functions: (a) explicit or (b) discrete.
(a) Numerical estimation of a definite integral of an explicitly stated continuous func￾tion fpxq should be carried out if the analytical integration is either impossible or too
difficult to obtain. In this case, the user has the freedom to choose (i) a numerical method
and (ii) a suitable h.
Low-order closed Newton-Cotes formulas such as Trapezoid and Simpson rules, usually
applied in composite formulations, are preferred to higher-order methods due to their ability
to provide satisfactory solutions to many problems. In this regard, the computation time
and accuracy of the Trapezoidal and Simpson’s rules can be further improved with a little
computational effort by employing the end corrected formulas, leading to the orders of error
of Oph4q and Oph6q, respectively. The choice of the interval size h in the compound rules is
another challenge of the numerical integration process. In an effort to reduce the truncation
error that is a function of h, the interval size should be chosen as “small,” but not so small
that it requires unacceptably excessive cpu-time. The explicit expressions derived for the
truncation errors can guide the analyst in selecting a suitable method and an optimal h.
Romberg’s rule allows the estimation of an integral to a desired degree of accuracy,
which is why it is used for the adaptive integration of well-behaved functions. Most of the
cpu-time is devoted to generating the first column (the trapezoidal rule with minimum
functional evaluations) of Romberg’s table, and the rest of the table entries are obtained by
Richardson’s extrapolation.
Quadrature methods are quite powerful and effective, as they provide highly accu￾rate estimates of the integrals of smooth functions with much fewer integration points.
Gauss-Legendre quadrature can be applied to the integration of any explicit function (ex￾cept for those depicting highly oscillatory behavior), including those with discontinuities
at endpoints. There are several efficient quadrature methods, each of which corresponds
to a special case: Type II integrals with singular points on both ends (Gauss-Chebyshev),
Type I improper integrals over a semi-infinite (Gauss-Laguerre) or infinite domain (Gauss￾Hermite), and so on. However, very accurate computations of abscissas and weights are
required.
The open Newton-Cotes rules avoid the integrand at end points and thus should be
preferred only for Type II integrals with discontinuities at one or both endpoints. Even
then, low-order open Newton-Cotes formulas implemented with the composite rule should
be preferred to a high-order formula that leads to Runge’s phenomenon. In many cases,
improper integrals can be transformed into ones in more convenient finite ranges where
the integrals can be computed with a standard numerical method of choice. If an improper
integral can be transformed into a proper integral, then this avenue should be taken first.
(b) Discrete functions arise from experiments and data collection instruments, where
the data (integration) points and the interval size are fixed. Estimating the definite integral
of a discrete function is carried out using the composite trapezoidal rule or Simpson’s rules.
In special cases, Romberg’s rule may be used only if the number of panels is a multiple of
two (2m) and the number of data points is sufficient. Since the analyst does not have the
freedom to modify the interval size h, a high-order numerical method should be exploited.
Finally, the numerical methods or techniques applied to the integration of univariant,
explicit continuous, or discrete functions can easily be extended to multiple integrals.476  Numerical Methods for Scientists and Engineers
8.15 EXERCISES
Section 8.1 Trapezoidal Rule
E8.1 Find the theoretical global error bounds if the N-panel trapezoidal rule is to be applied to
the following integrals:
(a) ż 1
0
x4
dx, (b) ż 1
´1
e
2xdx, (c) ż 1{2
0
dx
x ` 1
, (d) ż π{2
0
sinpe
xqdx
E8.2 Find the minimum number of panels required to evaluate the following integrals using the
N-panel trapezoidal rule so that the approximation error is less than 10´4.
(a) ż 1
´1
x5
dx, (b) ż 2
1
lnpxqdx, (c) ż 3
0
dx
x2 ` 9
, (d) ż π{2
0
cos x
2
dx
E8.3 Estimate the following integral: (i) analytically; (ii) numerically using the trapezoidal rule
with 1-, 2-, 4-, and 8-panels; (iii) determine the theoretical global error bounds; (iv) compare the
true error with the approximation errors; and (v) discuss your findings.
(a) ż 3
0
p9x ´ 3x2
qdx, (b) ż 1
0
xe´x2
dx
E8.4 Estimate the following integral: (a) using the 6- and 10-panel trapezoidal rule; (b) deter￾mining the global error bounds; and (c) comparing the approximation errors with the maximum
global error bound. The integral value, correct to nine decimal places, is given as 5.8091946665521.
ż π
0
?1 ` 4 sin x dx
E8.5 Consider the uniformly spaced discrete function fpxq tabulated below. Use the trapezoidal
rule with (a) h “ 0.4; (b) h “ 0.2; and (c) h “ 0.1 to estimate the integral of fpxq between x “ 0.4
and 1.2.
x 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2
fpxq 15 12 10 8.6 7.5 6.7 6 5.5 5
E8.6 Consider the uniformly spaced discrete function fpxq tabulated below. Use the trapezoidal
rule to (a) estimate the integral below and (b) compare your numerical estimate with the true
value of 9.542592.
ż 1.4
0.5
p1 ` x2
qfpxqdx
x 0.5 0.65 0.8 0.95 1.1 1.25 1.4
fpxq 2 2.69 3.56 4.61 5.84 7.25 8.84
E8.7 The heat capacity of gases in an isobaric process is defined as the rate of enthalpy change
with temperature as cp “ pBh{BTqp. The experimental data for the heat capacity (in kJ/kg¨K) of
a gas mixture as a function of temperature (in K) is tabulated below.
T 250 300 350 400 450 500
cp 0.161 0.194 0.226 0.259 0.292 0.324
The enthalpy of the gas mixture at a given temperature T is determined as follows:
hpTq ´ hp250 Kq “ ż T
T “250
cppyqdy
Given h(250K)=19.8 kJ/kg, apply the trapezoidal rule to estimate the enthalpy of the gas for
every temperature value available in the table.Numerical Integration  477
E8.8 Estimate the following integrals (i) analytically and (ii) numerically with the specified
number of panels using the trapezoidal rule with and without end-correction. (iii) Determine the
errors for both numerical estimates and discuss your findings.
(a) ż 1
0
x3 dx pN “ 5q, (b) ż 2
0
x7 dx pN “ 5, 10, 20q, (c) ż 1
0
xpx2 ` 3q
3
dx pN “ 10q
E8.9 Estimate the given integrals (a) analytically and (b) numerically by using the 10-panel
trapezoidal rule. (c) Determine the true error and the theoretical global error bounds to discuss
the accuracy of your estimate. (d) Can you apply the trapezoidal rule with end-correction to this
integral?
ż 0
´2
a4 ´ x2 dx
E8.10 A discrete function, fpxq, is tabulated below. (a) Estimate the definite integral of fpxq
between x “ 1 and 2.6 using the trapezoidal rule; (b) compare your estimate with the true value of
45.943466667. (c) Can you apply the trapezoidal rule with end-correction to this integral? Explain.
x 1 1.2 1.4 1.6 1.8 2 2.2 2.4 2.6
fpxq 0 2.016 4.288 8.352 15.744 28 46.656 73.248 109.312
Section 8.2 Simpson’s Rule
E8.11 Repeat E8.1 using Simpson’s 1/3 rule.
E8.12 Repeat E8.2 using Simpson’s 1/3 rule.
E8.13 Repeat E8.3 using Simpson’s 1/3 rule.
E8.14 Repeat E8.4 using Simpson’s 1/3 rule.
E8.15 Repeat E8.5 using Simpson’s 1/3 rule.
E8.16 Repeat E8.6 using Simpson’s 1/3 rule.
E8.17 Repeat E8.8 using Simpson’s 1/3 rule.
E8.18 Repeat E8.9 using Simpson’s 1/3 rule.
E8.19 Repeat E8.10 using Simpson’s 1/3 rule.
E8.20 Estimate the following integrals: (a) using the 8-panel Simpson’s 1/3 rule with and without
the end-correction; (b) What can you say about the accuracy of your estimates? Use a symbolic
processor for plotting and evaluating the required derivatives.
(a) ż π{2
0
u cos udu
u4 ` u3 ` 1
, (b) ż π
0
c
cos x
4 dx, (c) ż 1
0
e
tan´1p2xq
dx, (d) ż 1
0
e
px{2qx
dx.
E8.21 Estimate the following integral using N=4, 6, and 10-panel Trapezoidal and Simpson’s
1/3 rules with and without end-correction, and compare your estimates with the true value of
0.5048545941137.
ż 1
0
sinpπx2
qdx
E8.22 Estimate the following definite integrals as well as the global error for the specified N
using (i) the trapezoidal rule, (ii) Simpson’s 1/3 rule, and (iii) Simpson’s 3/8 rule. (iv) Determine
which method yields a more accurate estimate. Why?
(a) ż π{2
0
lnp3 ` 2 sin xqdx pN “ 11q Itrue “ 2.263925463697169
(b) ż 1
0
e
x3
dx pN “ 10q, Itrue “ 1.3419044179774196478  Numerical Methods for Scientists and Engineers
E8.23 Estimate the following definite integral using the 4- and 10-panel trapezoidal and Simpson’s
1/3 rules. If the true value of the integral is 1.0000211, which method is a more accurate estimate?
ż 2
0
lnp2.41 x ` 1q
x2 ` 1
dx
E8.24 The current passing through a circuit is described by the following equation:
iptq “ 1
Le
´tR{L
ż t
u“0
V puqe
uR{Ldu
where iptq is the current in amperes, V ptq is the applied voltage in volts, L is the inductance
(L “ 5 Henry), and R is the resistance (R “ 10 Ω). The transient voltage applied is given in the
table below. Use the trapezoidal rule to estimate the current for each value of time provided in
the data set.
t (s) 0 0.2 0.4 0.6 0.8 1 1.2 1.4
V ptq 0 89 105 34 ´65 ´110 ´65 33
t (s) 1.6 1.8 2 2.2 2.4 2.6 2.8 3
V ptq 104 88 0 ´89 ´105 ´34 66 110
E8.25 The tidal height recorded at a tidehouse for a 24-hour period is tabulated below. We wish
to determine the root-mean-square (rms) value of the tidal height, which denotes the potential of
tidal power. The rms value is computed as
Hrms “
d
1
T
ż T
t“0
H2ptqdt
where T is the recording duration (in hours) and H is the instantaneous tidal height in meters.
Use the given data to estimate the rms value with the trapezoidal and Simpson’s 1/3 rules.
t H t H t H t H tH
0 0.30 5 ´1.24 10 2.94 15 ´2.70 20 2.81
1 ´1.20 6 0.25 11 1.88 16 ´2.36 21 3.30
2 ´2.29 7 1.75 12 0.40 17 ´1.32 22 2.98
3 ´2.70 8 2.86 13 ´1.11 18 0.16 23 1.96
4 ´2.31 9 3.30 14 ´2.24 19 1.67 24 0.49
E8.26 A digital recording of the speed (V, km/min) of a rocket is presented below. (a) Determine
a numerical method to estimate the distance that the rocket traveled with the least possible error,
and (b) estimate the distance the rocket traveled during its 22-minute flight.
tV tV t V t V t V
0 0 5 6.09 10 2.46 15 0.62 20 0.07
1 4.09 6 5.36 11 1.93 16 0.44 21 0.03
2 6.07 7 4.56 12 1.49 17 0.3 22 0
3 6.73 8 3.79 13 1.13 18 0.2
4 6.62 9 3.08 14 0.85 19 0.12
E8.27 (a) Estimate the following integral involving the tabulated discrete function fpxq using
the trapezoidal and Simpson’s 1/3 rules. (b) Which method gave a better estimate? why? (c) Can
you estimate the theoretical global error for this integral?
ż 2
x“1
fpxq
x dx
x 1 1.125 1.25 1.375 1.5 1.625 1.75 1.875 2
fpxq 5 5.75 6.65 7.84 9.25 10.95 12.97 15.4 18Numerical Integration  479
E8.28 (a) Estimate the following integral involving the tabulated discrete function fpxq using
the trapezoidal and Simpson’s 1/3 rules. (b) Which method gave a better estimate? why? (c) Can
you apply the end-point corrected formulas for this integral?
ż 1.2
x“0
fpxqdx
1 ` 0.3 x2
x 0 0.15 0.3 0.45 0.6 0.75 1.05 1.2
fpxq 3 3.1 3.2 3.4 3.7 4.2 5.2 5.8
E8.29 The velocity vpxq and temperature Tpxq distributions of water fluid flowing between two
large heated plates, depicted in Fig. E8.29, are obtained from the numerical solution of pertinent
governing equations. The numerical solution for selected points is presented in the table below.
Use a suitable numerical method to estimate the mean fluid velocity and mean temperature.
Fig. E8.29
Compute the mean vel
vm “ 1
W
where W is the distan
rticle is moving from point A(1,0
en is by Fpx, y, zq“px2 ` y2qi `
te data set presented in the table
x (m) 0 0.2 0.4 0.6 0.8 1
v (m/s) 0 0.236 0.351 0.358 0.241 0
T (
˝C) 60 57 46 51 55 62
ocity and mean temperature from
ż W
x“0
vpxqdx, Tm “ 1
vm
ż W
x“0
vpxqTpxqdx
ce between the plates.
E8.30 A pa ) to point B(6.485,0.866) under the influence of a
force field giv px2 ´ y2qj along the path defined by the uniformly
spaced discre below.
t xptq yptq t xptq yptq t xptq yptq
0 0 0 2.8 0.794 0.208 5.6 3.177 ´0.407
0.4 0.016 0.407 3.2 1.038 ´0.208 6 3.648 0
0.8 0.065 0.743 3.6 1.313 ´0.588 6.4 4.150 0.407
1.2 0.146 0.951 4 1.621 ´0.866 6.8 4.685 0.743
1.6 0.259 0.995 4.4 1.962 ´0.995 7.2 5.252 0.951
2 0.405 0.866 4.8 2.334 ´0.951 7.6 5.852 0.995
2.4 0.584 0.588 5.2 2.74 ´0.743 8 6.485 0.866
Estimate the work done by the particle using the trapezoidal and Simpson’s 1/3 rules. Clue: Apply
second-order difference formulas for the derivatives and compute the work by
W “
ż B
A
F ¨ dr “
ż 8
t“0
! `
x2 ` y2˘ dx
dt ` `
x2 ´ y2˘ dy
dt
)
dt
E8.31 Estimate the following integral using (a) a 6-panel Simpson’s 1/3 and 3/8 rules; (b) com￾pare the theoretical maximum and the true errors for both methods; and (c) explain the difference
in the numerical estimates. The true integral value is 1.054593421075752.
ż 1
0
p5x ` 1qe
´4x2
dx480  Numerical Methods for Scientists and Engineers
E8.32 Consider E8.31: what is the minimum number of panels that should be used with the
Simpson’s 1/3 rule so that the order of approximation error is less than 10´4?
Section 8.3 Romberg’s Rule
E8.33 Consider the first column of Romberg’s table for the definite
integral, presented below. Complete the table and compute R4,4 ´ R4,3,
R4,4 ´ R3,3, and the absolute error in the final estimate.
ż 2
0
xe´xdx “ 1 ´ 3
e2
0.2706705665
0.5032147244
0.5705876472
0.5880964505
E8.34 Consider the first column of Romberg’s table for the definite
integral presented below. Complete the table and compute R4,4 ´ R4,3,
R4,4 ´ R3,3, and the absolute error in the final estimate.
ż ?3
0
xp1 ` x2
q
3{2
dx “ 31
5
12.0
7.7362742979
6.5890218645
6.2975565330
E8.35 Consider the first column of Romberg’s table for the definite
integral presented below. Complete the table and compute R5,5 ´ R5,4,
R5,5 ´ R4,4, and the absolute error in the final estimate.
ż π
0
p2 ´ xq cos x dx “ 2
4.9348022005
2.4674011003
2.1060585751
2.0259014934
2.0064379289
E8.36 Consider the first column of Romberg’s table for the definite
integral presented below. Complete the table and compute R5,5 ´ R5,4,
R5,5 ´ R4,4, and the absolute error in the final estimate.
ż π
0
xdx
sin x “ 2G
where G “0.915965594177219 is the Catalan’s constant.
2.0190987135
1.8819073817
1.8446859979
1.8351378037
1.8327339925
E8.37 Use Romberg’s rule to evaluate the following integrals within ε “ 10´6.
(a) ż 0.8
0
e
´1.5625 x2
dx, (b) ż π{4
0
lnpsec xq dx, (c) ż 1
0
a3 x3 ` 1 dx
E8.38 Repeat E8.22 with Romberg’s rule to find R2,2 and R3,3. Comment on the accuracy of the
integrations.
E8.39 Repeat E8.10, by constructing the largest Romberg’s table as possible. How many decimal
places of accuracy are attained in the worst scenario?
E8.40 Repeat E8.27, by constructing the largest Romberg’s table as possible. How many decimal
places of accuracy are attained in the worst scenario?
E8.41 Repeat E8.28, by constructing the largest Romberg’s table as possible. How many decimal
places of accuracy are attained in the worst scenario?
E8.42 Estimate the following integrals using the Romberg’s rule by constructing tables of a
specified size. How many decimal places of accuracy are attained in the worst scenario?
(a) ż 2
1
e´x
x dx p4 ˆ 4q, (b) ż π
0
sin x
x dx p3 ˆ 3q, (c) ż 2π
0
e´xdx
1 ` sin2x p5 ˆ 5qNumerical Integration  481
E8.43 Consider the shaded region (D) that lies between
y “ x2 and y “ 2 cos x curves on 0 ď x ď α as shown in
Fig. E8.43. The area of a region R can be determined by
the following definite integral:
Area= ż α
0
p2 cos x ´ x2
q dx
where α is the solution of the 2 cos x “ x2 non-linear equa￾tion on x ą 0. Estimate the area with at least three deci￾mal places of accuracy. Fig. E8.43
E8.44 A bar of variable circular cross section, as illus￾trated in Fig. E8.44, is given. The axial displacement (u)
of the bar under a load P can be computed from
u “
ż L
x“0
P
EAdx
Fig. E8.44
where L is the length (m), E is the Young’s modulus (Pa), and A is the cross-sectional area (m2).
Assuming P “ 2000 kg and using the tabulated radius and Young’s modulus data below, use
Romberg’s method to determine the axial displacement of the bar with the least error.
x (cm) 0 4 8 12 16 20 24 28 32
r (cm) 11 7 4.5 3 2.2 2 2 2 2
E (MPa) 35 33 31 30 29 27 24 21 18
E8.45 A weather station records precipitation (in cm/m2) in 15-minute intervals. A data set
collected over a 4-hour period is tabulated below. Using the furnished data, (a) estimate the
total precipitation, H “ ş
hdt, during a 4-hour rainfall using the Simpson’s 1/3 rule. (b) Can you
estimate the total precipitation using Romberg’s rule? (c) How many decimal places of accuracy
will be attained in the worst scenario?
t (h) h (cm/m2) t (h) h (cm/m2) t (h) h (cm/m2)
0 0 1.50 0.25 3.00 0.44
0.25 0.65 1.75 0.12 3.25 0.52
0.50 0.51 2.00 0.35 3.50 0.28
0.45 0.38 2.25 0.32 3.75 0.18
1.00 0.55 2.50 0.35 4.00 0
1.25 0.12 2.75 0.36
Section 8.4 Adaptive Integration
E8.46 Develop an algorithm for the adaptive trapezoidal rule.
E8.47 Estimate the integral below (a) analytically and (b) numerically using adaptive Simpson’s
1/3 rule to within 10´3. ż 3
0
dx
?x ` 1
E8.48 Repeat E8.47 with the improved extrapolation formula.
E8.49 Repeat E8.37, using adaptive Simpson’s 1/3 rule to within 10´3.
E8.50 Repeat E8.49 with the improved extrapolation formula.482  Numerical Methods for Scientists and Engineers
E8.51 Estimate the integral below (a) numerically using adaptive Simpson’s 1/3 rule to within
10´3, and (b) determine the accuracy if the true value of the integral is 0.82811632884.
ż π{2
0
sinpx2
q
Section 8.5 Newton-Cotes Rules
E8.52 Estimate the following integral: (a) analytically; (b) numerically, by applying the composite
trapezoidal rule with 4- and 8-panels over the entire range; (c) repeat Part (b) with the composite
midpoint rule; and (d) compare the error estimates for Parts (b) and (c) and discuss the accuracy
of both methods.
ż π{2
´π{2
|sin x|dx
E8.53 For the following piecewise functions, estimate the integral of fpxq on [0,1], (i) analytically;
(ii) numerically, by applying the composite trapezoidal and midpoint rules using 4-panels over the
entire range; and (iii) is fpxq and f1
pxq continuous at x “ 1{2? Comment on the accuracy of both
methods.
(a) fpxq “ " x2, x ď 1
2
x ´ x2, x ą 1
2
(b) fpxq “ " x2, x ď 1
2
x ´ x2, x ą 1
2
E8.54 Estimate the following integral: (a) analytically; (b) numerically by employing the com￾posite midpoint rule with 4-, 8-, 16-, 32-, 64-, and 128-panels over the entire range. (c) Discuss
the convergence of the midpoint rule. (d) Can you apply the trapezoidal or Simpson’s 1/3 rules
to this integral?
ż 1
´1
dx
?1 ´ x2
E8.55 Estimate the following integral: (a) analytically and (b) numerically by applying the com￾posite midpoint, trapezoidal, and Simpson’s 1/3 rules with 4-, 8-, 16-, 32-, and 64-panels over the
entire range; (c) compare and discuss your estimates.
ż 1{2
0
lnp1 ` 4x2
qdx
E8.56 To estimate the following integral, apply the composite midpoint, 2-point, 3-point, and
5-point open Newton-Cotes formulas with 24-, 48-, and 96-panels over the entire range and discuss
your estimates.
ż 1
0
ex
?1 ´ x2 dx “ 3.10437901785555
E8.57 Consider the following integrals, whose true solutions are provided: (a) Estimate the in￾tegrals by applying the composite midpoint, 2-point, 3-point, and 5-point open Newton-Cotes
formulas with 24-, 48-, and 96-panels over the entire range. (b) Compare and discuss the numeri￾cal estimates.
(a) ż π{4
0
x
sin 2x
dx “ 0.4579827971, (b) ż 2
0
tan´1px{2q
x dx “ 0.9159655942,
(c) ż π{2
0
e
´2 sec x sec x dx “ 0.1138938727, (d) ż 1
0
dx
2e
?x ` e´?x ´ 3 “ 1.18340407703954
Section 8.6 Integration of Non-uniform Discrete Functions
E8.58 The boron concentration in a water stream is measured over a 10-hour period. Use theNumerical Integration  483
trapezoidal rule and the tabulated data below to calculate the average Boron concentration (ppm)
and the average time, which are determined from:
C “ 1
24 ż 24
t“0
Cptqdt, t “ 1
C
ż 24
t“0
t Cptqdt
t (h) 0 1 3 4 6 7 10 11 14 16 19 20 22 24
C (ppm) 15 21 29 32 39 41 45 46 41 35 24 20 15 13
E8.59 The current applied across an electric circuit is tabulated below. Using the trapezoidal
rule, estimate the average current and the rms current, which are calculated as follows:
I “ 1
T
ż T
t“0
Iptqdt, Irms “
d
1
T
ż T
t“0
I2ptqdt
where T is the time interval.
t (msec) I t (msec) I t (msec) I
0 0 3.5 26.7 7.0 24.3
0.5 4.7 4.0 28.5 7.5 21.2
1.0 9.3 5.0 30.0 8.0 17.6
2.0 17.6 5.5 29.6 9.0 9.3
2.5 21.2 6.5 26.7 10 0
E8.60 The stress-strain data obtained from a tensile test of a specimen is tabulated below. The
total energy dissipation is calculated from
Upεq “ ż ε
0
σpuqdu
(a) Estimate the energy dissipation for every available ε-value using the trapezoidal rule. (b)
Which method (Trapezoidal or Simpson’s 1/3 rule) is more suitable for this problem? (c) How
can we improve the estimate over the trapezoidal rule?
ε (mm) σ (MPa) ε (mm) σ (MPa) ε (mm) σ (MPa)
0 0 0.1869 32.976 0.3907 43.098
0.0237 3.256 0.2072 36.573 0.4553 43.621
0.0543 8.982 0.2243 39.183 0.5130 43.991
0.0883 15.342 0.2615 40.488 0.6116 44.085
0.1223 21.055 0.3058 41.628
0.1495 26.933 0.3432 42.616
E8.61 The thermal expansion coefficient of a steel specimen as a function of temperature is
tabulated below. The contraction of a specimen is evaluated from
ΔDpTq “ D0
ż T
Ta
αpT¯q dT¯
where ΔDpTq is the contraction, D0 is the initial diameter, α is the thermal expansion coefficient,
and Ta is the ambient temperature. If a specimen 10 cm in diameter is immersed in dry ice at484  Numerical Methods for Scientists and Engineers
temperature T (in ˝C), estimate the contraction in the specimen for every available temperature
listed in the table.
Tp
˝
Cq α pμm/m/˝
Cq Tp
˝
Cq α pμm/m/˝
Cq Tp
˝
Cq α pμm/m/˝
Cq
´240 2.80 ´160 6.50 ´50 10.15
´220 3.81 ´130 7.66 ´20 10.85
´200 4.76 ´100 8.70 ´15 10.96
´180 5.66 ´80 9.32 40 11.90
E8.62 Enthalpy and entropy are two important thermodynamic properties of a substance that
are calculated from
HpTq “ H0 `
ż T
275
Cppxqdx, SpTq “ S0 `
ż T
275
Cppxq
x dx
where Cp is the specific heat, H0 =47.37 kJ/kg, and S0 “ 0 J/kg.K are the enthalpy and entropy
of the substance at 275 K. The specific heat vs. temperature data is presented below. Use the
data to estimate the enthalpy and entropy of the substance for every available temperature listed
in the table.
T pKq 275 285 300 325 330 350 370
Cp pJ/kg ¨ Kq 208.66 211.38 215.45 222.24 223.60 229.03 234.46
E8.63 Fugacity is a thermodynamic property of a real gas, and it is defined as
ln f
P “
ż P
0
Zppq ´ 1
p
dp
where f is the fugacity, P is the pressure, and Z is the compressibility factor. The compressibility
factor of an ideal gas is tabulated below for various pressures. Estimate the fugacity of the gas for
each available pressure value in the data.
P (atm) Z P (atm) Z P (atm) Z
0 1 20 0.906 250 0.327
0.1 0.993 50 0.675 300 0.382
0.5 0.985 100 0.15 400 0.491
1 0.982 150 0.21 500 0.595
10 0.952 200 0.265
E8.64 The cylinder (in diameter Dp “ 20 cm) pressure (P) of an internal combustion engine is
tabulated below in relation to the piston position (x) from the top dead center. The work done
can be calculated from
W “
ż
F ¨ dr “ Ap
ż
P dx
where Ap is the piston area (m2), P is the pressure (Pa), and W is the work. Estimate the work
done by the piston using the trapezoidal and Simpson’s 1/3 rules.
x (cm) P (MPa) x (cm) P (MPa) x (cm) P (MPa)
0 130 2.5 163 6 127
0.5 150 3 156 7 120
1 160 3.5 150 8 113
1.5 163 4 140 9 105
2 166 4.5 135
E8.65 The diameter (d) of a circular shaft varies with axial position (x). The digitally sampled
diameter data is tabulated below. The axial load of P “ 50, kN is applied to one end of the shaft,Numerical Integration  485
whose modulus of elasticity is E “ 200 GPa. The axial elongation of the shaft is computed by
Δz “ P
E
ż
L
0
dz
Apzq
where Apzq “ πd2pzq{4 is the cross-section area of the shaft. Estimate the axial elongation of the
piston using the trapezoidal and Simpson’s 1/3 rules.
z (cm) d (cm) z (cm) d (cm) z (cm) d (cm) z (cm) d (cm)
0 2 60 2.30 110 2.90 150 2.10
10 2.10 65 2.44 120 2.66 180 2.10
20 2.10 80 2.66 135 2.44 190 2.10
50 2.10 90 2.90 140 2.30 200 2
55 2.12 100 3 145 2.12
Section 8.7 Gauss-Legendre Method
E8.66 Estimate the following integral: (a) analytically and (b) numerically, using the 2-point
Gauss-Legendre quadrature (x1,2 “ ˘1{
?3, w1,2 “ 1, do not approximate the quadrature points
and weights). (c) Calculate the global error and discuss the accuracy of the numerical estimates.
ż 1
´1
px3 ` 3x2 ` x ` 1qdx
E8.67 Estimate the following integral: (a) analytically and (b) numerically, using the 2-, 3-, and
4-point Gauss-Legendre quadrature. (c) Calculate the global error and discuss the accuracy of the
numerical estimates.
ż 1
´1
p7x2 ` 5q
3
dx
E8.68 Estimate the following integral: (a) Using the 3-, 4-, and 5-point Gauss-Legendre quadra￾ture. (b) Calculate and discuss the accuracy of the numerical estimates. The integral, correct to
ten decimal places, is given as 1.3658660636140654.
ż 1
´1
cos x
1 ` x2 dx
E8.69 Estimate the following integral using the 2-, 3-, 4-, and 5-point Gauss-Legendre quadrature
and discuss the accuracy of the estimates.
ż π
0
`
sin x
˘1{3 dx
E8.70 To estimate the following integrals: (a) use 5-point Gauss-Legendre quadrature; and (b)
calculate the global error and discuss the accuracy of your estimates.
(a) ż π{4
0
cospe
x{2
q dx, (b) ż 1
0
sinhpx3
qdx, (c) ż π{2
0
cosp
a1 ` x2qdx,
(d) ż 3
1
exdx
1 ` x5, (e) ż 2
1
e
´x2
dx, (f) ż 1
0
xx2
dx
E8.71 Estimate the following integral: (a) analytically, and (b) numerically, using the 5-point
Gauss-Legendre quadrature over the whole range. (c) Transform the integral by u “ cos x substi￾tution and then use the 5-point Gaussian quadrature to estimate the resulting integral. (d) Which
integral yields a better estimate? why?
ż π
0
sin 2x
4 ´ 3 cos xdx486  Numerical Methods for Scientists and Engineers
E8.72 To estimate the following integral with a removable singularity at x “ 0 (a) use 3-, 4-, and
5-point Gauss-Legendre quadrature. (b) Calculate the global errors and discuss the accuracy of
your estimates.
ż 1
0
ex2
´ 1
x dx
E8.73 Consider the following integral, which has non-removable singularities at both endpoints.
(a) Estimate the integral using the 2-, 3-, 4-, and 5-point Gauss-Legendre quadrature, and (b)
discuss the accuracy of your estimates.
ż 1
´1
ex
?1 ´ x2 dx
E8.74 Consider the integral in E8.57c, which has a non-removable singularity at x “ π{2. (a)
Estimate the integral using the 2-, 3-, 4-, and 5-point Gauss-Legendre quadrature, and (b) discuss
the accuracy of your numerical estimates.
E8.75 Consider the integral in E8.57d, which has a non-removable singularity at x “ 0. (a)
Estimate the integral using the 2-, 3-, 4-, and 5-point Gauss-Legendre quadrature, and (b) discuss
the accuracy of your numerical estimates.
E8.76 Estimate the following integral: (a) using a 2-point Gauss-Legendre quadrature over the
whole range; (b) applying a two-point Gaussian quadrature to each integral (i.e., composite rule)
after splitting it into three integrals on [0,1], [1,2], and [2,3]. (c) Calculate the true errors for Parts
(a) and (b), discuss the accuracy of your estimates. The true integral is given as 5.251805081.
ż 3
0
a
ex{3 ` x dx
E8.77 Estimate the following integral: (a) analytically; (b) numerically, using 2-point Gauss￾Legendre quadrature over the whole range; (c) by applying the 2-point Gaussian quadrature to
each interval and having the integral split into four parts, [´2, ´1], [´1.0], [0,1], and [1,2]. (d)
Calculate the true errors for Parts (a) and (b) and discuss the accuracy of your estimates.
ż 2
´2
dx
x2 ` 4
E8.78 Estimate the following integral: (a) analytically; (b) numerically, using 2-, 3-, 4-, and
5-point Gauss-Legendre quadrature. (c) Discuss the accuracy of your estimates.
ż π
0
sinp10xq cos x dx
Section 8.8 Computing Improper Integrals
E8.79 Estimate the following integrals using the subtraction of singularity technique with the
10-panel Simpson’s 1/3 rule and discuss the accuracy of your estimate.
(a) ż π{2
0
sin xdx
?π ´ 2x “ 1.38232508, (b) ż 1
0
e´xdx
?1 ´ x “ 1.076159014,
(c) ż 1
0
cosh xdx
?x ` x2 “ 1.92353087, (d) ż 1
0
x sinpπx{2q
?1 ´ x2 dx “ 0.89036520,
(e) ż 1
0
lnp1{xq
1 ` x dx “ π2
12 , (f) ż 1
0
lnpcsc xqdx “ 1.05672021.
E8.80 Using 5-, 10-, 15-, and 20-point Gauss-Legendre quadrature, estimate E8.79 integrals by
ignoring the singularity and discuss the accuracy of your estimates.Numerical Integration  487
E8.81 Apply the truncating the intervals technique to estimate the following integrals using the
Simpson’s 1/3 rule with h “ 0.1 and b “ 5, 10, and 15 as infinity, and discuss the accuracy of
your estimates.
(a) ż 8
1
e´x
x2 dx “ 0.14849551, (b) ż 8
1
sin x ?1 ` x2 dx “ 0.49037744,
(c) ż 8
0
dx
p1 ` ex{5q
2 “ 0.96573590, (d) ż 8
0
e
´2x lnp1 ` xq “ 0.18066431.
E8.82 In each of the following integrals, apply a suitable substitution to first transform the
improper integral into a proper integral. Then, apply the 100-panel Simpson’s 1/3 rule to estimate
the resulting integrals and their true errors.
(a) ż 8
2{π
1
x4 sin ˆ 1
x
˙
dx “ π ´ 2, (b) ż 1
0
sin´1?x ?1 ´ x2 dx “ 1.51219929379,
(c) ż 1
´1
cos x ?1 ´ x2 dx “ 2.403939430634, (d) ż π{2
0
cos 2x ?x dx “ 0.9374359444665,
(e) ż 1
0
?1 ´ x2
?x dx “ 1.748038369528, (f) ż 1
0
c1 ` cos´1x
1 ´ x2 dx “ 2.08129232270.
E8.83 Estimate the following improper integrals using open Newton-Cotes formulas (midpoint
rule, two-point rule, and three-point rule). Apply 10-, 50-, and 100-panels for each formula and
discuss the accuracy of your estimates.
(a) ż 1
0
dx
p1 ´ xq
1{4 “ 4
3
, (b) ż 1
0
lnp
1
x
qe
´2xdx “ 0.65963168,
(c) ż ln 2
0
cex ` 1
ex ´ 1
dx “ π
3 ` lnp2 ` ?
3q, (d) ż π{2
0
csec x
x dx “ 3.42312699
(e) ż 1
´1
dx
?1 ´ x2 “ π, (f) ż π{2
0
lnp1 ` tan xqdx “ 1.46036212,
(g) ż 1
0
lnpxqdx
x2 ´ 1
dx “ π2
8 , (h) ż π{3
0
cos xdx
?4cos2x ´ 1 “ π
4 .
E8.84 Estimate the following improper integrals using open Newton-Cotes formulas. (a) Divide
the integration interval into 200-, 400-, and 800 subregions and apply the three-point rule for each
subregion, and (b) discuss the accuracy of your estimates.
(a) ż 2
0
dx
px ´ 1q
2{3 “ 6, (b) ż 1
´1
ln |x| dx “ ´2,
(c) ż 1
´1
1 ´ x2{3 ` x2
x2{3p1 ` x2q dx “ 6 ´ π
2 , (d) ż 8
0
dx
3
b
p
?3 x ´ 1q
2
“ 144
7 ,
Section 8.9 Gauss-Laguerre Method
E8.85 Estimate the following integral using the 2-point Gauss-Laguerre quadrature.
ż 8
0
e
´xdx488  Numerical Methods for Scientists and Engineers
E8.86 Estimate the following integrals: (i) analytically and (ii) numerically using the Gauss￾Laguerre quadrature with the specified number of points. (iii) Also calculate the upper- and
lower-error bounds and discuss the accuracy of your estimates.
(a) ż 8
0
e
´x cos x dx, pN “ 4q, (b) ż 8
0
e
´3xdx, pN “ 2q,
(c) ż 8
0
e
´x sinh x
3 dx, pN “ 3q, (d) ż 8
0
e
´x lnpx ` 1qdx. pN “ 4q,
E8.87 Estimate the following integrals using the 5-point Gauss-Laguerre quadrature.
(a) ż 8
0
e´2xdx
x2 ` 9 , (b) ż 8
0
e
´x´2x2
dx, (c) ż 8
0
e
´3x
b
2 ` sinp πx
2 qdx
(d) ż 8
0
e´x?x
x ` 3 dx, (e) ż 8
0
e
´px2`3x`4q
dx.
E8.88 Consider the following improper integrals: (a) Apply a suitable substitution to transform
the integrals into Gauss-Laguerre type integrals, and (b) estimate the integrals using the 5-point
Gauss-Laguerre quadrature.
(a) ż 8
1
e´x{2
x ` 3
dx, (b) ż 8
π
e
´xsin2
xdx, (c) ż 8
´1
e
´x lnp1 ` x2
qdx
E8.89 Consider the following improper integrals: (a) Apply a suitable substitution to transform
the integrals into Gauss-Laguerre type integrals, and (b) estimate the integrals using the 5-point
Gauss-Laguerre quadrature.
(a) ż 8
0
dx
cosh x, (b) ż 8
1
sin x
x dx, (c) ż 8
2
xdx
x3 ` 3x2 ´ 1
, (d) ż 8
0
e´x2{5
1 ` x3 dx
E8.90 Consider the following improper integral: Apply the 2-, 4-, 6-, and 8-point Gauss￾Laguerre quadrature to estimate the integral and compare your estimates with the true value
of 1.198705652879.
ż 8
0
dx
1 ` ex2{5
E8.91 Consider the following improper integral: (a) Apply the 2-, 4-, 6-, and 8-point Gauss￾Laguerre quadrature to estimate the integral, and compare your estimates with the true value of
1/2. (b) Explain how the estimates converge on the solution.
ż 8
0
e
´xx2 sin x dx
E8.92 Consider the following improper integral with a removable discontinuity at x “ 0: (a)
Estimate the integral using the 2-, 3-, 4-, and 5-point Gauss-Laguerre quadrature, and (b) comment
on the accuracy of the estimates.
ż 8
0
e´x ´ e´2x
x dx “ ln 3
E8.93 Consider the following improper integrals with a removable discontinuity at x “ 0: (i)
Estimate the integrals using the 2- through 8-point Gauss-Laguerre quadrature, and (ii) comment
on the accuracy of the estimates.
(a) ż 8
0
1 ´ cos x
x2 dx “ π
2 , (b) ż 8
0
sin3pxq
x3 dx “ 3π
8 , (c) ż 8
0
x2dx
px2 ` 1q
3
px2{9 ` 1q “ 27π
512Numerical Integration  489
E8.94 A power surge in an electric circuit across a resistor results in a change in the current,
according to
Iptq “ I0 exp ˆ
´pt ´ t0q
2
2σ2
˙
where I0 “ 50 A is the peak current, t0 “ 0.05 s is the instance of surge, and σ “ 0.008 s is a
parameter giving the peak width. The dissipated energy by the resistor is computed as
E “
ż 8
t“0
RI2
ptqdt
Estimate the energy dissipation using the 3-, 4-, and 5-point Gauss-Laguerre quadrature if R “ 20
Ω. Are you able to obtain a converged solution?
E8.95 The spectral blackbody emissive power (Eb,λ) is defined by Planck’s law.
Ebλpλ, Tq “ C1
λ5 rexppC2{λTq ´ 1s
ˆ W
m2 ¨ μm
˙
where C1 “ 3.742 ˆ 108, W¨μm4/m2, C2 “ 1.439 ˆ 104, μm4¨K, T is the absolute temperature of
the solid surface, and λ is the wavelength of the radiation emitted by the solid (μm). The radiation
emitted, including all wavelengths, is obtained from
ż 8
λ“0
Ebλpλ, Tqdλ “ σT 4
Using 3-, 4-, and 5-point Gauss-Laguerre quadrature, estimate the Stefan-Boltzmann constant,
which is 5.670374ˆ10´8 W/m2K4. Hint: Apply the x “ 1{λT substitution to the integral.
Section 8.10 Gauss-Hermite Method
E8.96 Estimate the following integral using the two-point Gauss-Hermite quadrature.
ż 8
´8
e
´x2
dx
E8.97 (a) Derive the 3-point Gauss-Hermite quadratures; (b) Estimate the following integral with
the Gauss-Hermite quadrature (without rounding) from Part (a); and (c) Discuss the accuracy of
your estimates.
ż `8
´8
x4
e
´x2
dx “ 3
?π
4
E8.98 (i) Use the Gauss-Hermite quadrature with a specified N to estimate the following inte￾grals; and (ii) calculate the global error bounds and discuss the accuracy of your estimates. Note:
K0pxq is the modified Bessel function of the second kind, and Γpxq is the gamma function.
(a) ż 8
´8
e
´x2´2xdx “ e
?π, pN “ 4q, (b) ż 8
´8
e
´x2
cos x dx “
?π
e1{4 , pN “ 3q,
(c) ż 8
´8
e
´x2a|x|dx “ Γ
ˆ3
4
˙
, pN “ 3q, (d) ż 8
´8
e
´x2
sin x dx “ 0, pN “ 4q,
E8.99 (a) Use the 2-, 3-, 4-, and 5-point Gauss-Hermite quadrature to estimate the following
integral, and (b) comment on the accuracy of the estimates. Note: erfc(x) is the complementary
error function.
(a) ż 8
´8
e
´x2
cos 2x dx “
?π
e , (b) ż 8
´8
e´x2
dx
x2 ` 4 “ π
2 e
4
erfc(2),
(c) ż 8
´8
e
´x2
sin2
xdx “
?π
2
ˆ
1 ´ 1
e
˙
, (d) ż 8
´8
e
2x´x2
dx “ e
?π490  Numerical Methods for Scientists and Engineers
E8.100 Consider the following improper integral: (i) Estimate the integral using the 2-, 4-, 6-,
8-, and 10-point Gauss-Hermite quadrature and compare your estimates with the true value; (ii)
Find the upper and lower bounds of the global error for N “ 2 and 4.
(a) ż 8
´8
e´x2
dx
?1 ` x2 “ ?e K0
ˆ1
2
˙
, (b) ż 8
´8
dx
4x4 ` 1 “ π
2
where K0pxq is the modified Bessel function of the second kind.
E8.101 (a) Evaluate the following integral using the 2-, 3-, 4-, and 5-point Gauss-Hermite quadra￾ture; (b) Using a suitable substitution, transform the integral to the one on r´π{2, π{2s and apply
the 2-, 3-, 4-, and 5-point Gauss-Legendre quadrature; and (c) Discuss the accuracy of your esti￾mates in Parts (a) and (b).
ż `8
´8
dx
px2 ` 1q
?3 ` 4x2 “ ln 3
E8.102 Consider estimating the following improper integral: (a) Use the 2-, 3-, 4-, and 5-point
Gauss-Legendre quadrature; (b) Apply the 2-, 3-, 4-, and 5-point Gauss-Hermite quadrature after
transforming the integral to the one on p´8, 8q; and (c) Discuss the accuracy of your estimates
in Parts (a) and (b).
ż π{2
´π{2
sinp2 tan θq tan θdθ “ π
e2
E8.103 Consider estimating the following improper integral: (a) use the 2-, 3-, 4-, and 5-point
Gauss-Hermite quadrature; (b) apply the 2-, 3-, 4-, and 5-point Gauss-Laguerre quadrature after
transforming the integral to the one on p0, 8q; and (c) discuss the accuracy of your estimates.
ż `8
´8
xdx
pe´x ` 1qpex ` 2q “ pln 2q
2
2
Section 8.11 Gauss-Chebyshev Method
E8.104 Estimate the following improper integral: (a) analytically and (b) numerically using the
2-point Gauss-Chebyshev quadrature. (c) Also calculate the global error bounds and discuss the
accuracy of your estimate.
ż 1
´1
dx
?1 ´ x2
E8.105 Estimate the following integrals: (a) Use the 5-point Gauss-Chebyshev quadrature; and
(b) Calculate the global error bounds and discuss the accuracy of your estimates.
(a) ż 1
´1
x6dx
?1 ´ x2 “ 5π
16 , (b) ż 1
´1
e´xdx
?1 ´ x2 “ πI0p1q, (c) ż 1
´1
cos 2xdx
?1 ´ x2 “ πJ0p2q.
where J0pxq and K0pxq are the Bessel functions.
E8.106 Consider estimating the following integrals: (a) use the 5-point Gauss-Chebyshev quadra￾ture; and (b) calculate the global error bounds and discuss the accuracy of your estimates. Note:
I0pxq is the modified Bessel function of the first kind.
(a) ż 2
0
sinhpx{2qdx
axp2 ´ xq “ π sinh ˆ1
2
˙
I0
ˆ1
2
˙
, (b) ż 1
0
e1´2xdx
axp1 ´ xq
“ πI0p1q,
(c) ż 2
1
ln x dx
apx ´ 1qp2 ´ xq
“ 2π ln ˆ1 ` ?2
2
˙Numerical Integration  491
E8.107 Consider estimating the following integrals: (a) Use the 5-point Gauss-Chebyshev quadra￾ture; and (b) Calculate the global error bounds and discuss the accuracy of your estimates.
(a) ż π
0
sin 2x ?x dx “ 0.608688442, (b) ż 1
0
ex
?xdx “ 2.925303492,
(c) ż 2
0
ex
?2 ´ x
dx “ 12.500854858, (d) ż 3
´1
lnp1 ` x2qdx
?3 ´ x “ 5.472304123,
(e) ż 1
0
cospe´xq
?1 ´ x dx “ 1.692305349, (f) ż 1
0
?3 xdx
p1 ` x2q
?1 ´ x “ 1.104688454
E8.108 Estimate the following integral using the 2-, 3-, 4-, 5-, 8-, and 10-point Gauss-Chebyshev
quadrature and discuss the accuracy of your estimates.
(a) ż 1
´1
dx
?1 ´ x4 “ 2
?π Γp5{4q
Γp3{4q “ 2.6220575543, (b) ż 1
0
tan´1x
x
?1 ´ x2 dx “ π
2
lnp1 ` ?
2q
where Γpxq is the Gamma function.
Section 8.12 Computing Integrals with Variable Limits
E8.109 Transform the following to integrals on the [0,1] interval.
(a) ż x
0
sin t
t dt, (b) ż 8
x
e´t
t2 dt, (c) ż 8
?x
cos t
t dt, (d) ż 8
x
pt ´ xq
3{2
e
´t
dt,
(e) ż sin x
0
e
´t2
dt, (f) ż x
0
sin t dt
?x2 ´ t2 , (g) ż 8
x
dt
?1 ` t4 , (h) ż ´x
´8
dt
?1 ´ t3 px ą 0q
(i) ż 1
x
et
dt
?t ´ x
, (j) ż x`1
x´1
et
dt
t ` 1
, (k) ż x
x2
sinpt ´ x2
qdt, (l) ż 1{x
x
tan´1?t ´ x dt
Section 8.13 Double Integration
E8.110 Consider estimating the following double integrals: Use (i) the trapezoidal rule, (ii) Simp￾son’s 1/3 rule with 10-, 20-, 40-, and 80-panels for x- and y-intervals, and (c) discuss the accuracy
achieved in both methods.
(a) ż 1
x“´1
ż 2
y“´2
dydx
x2 ` y2 ` 1 “ 3.72286450603, (b) ż 1
x“0
ż 1
y“0
xy
1 ` xy
dydx “ 1 ´ π2
12
(c) ż 2
x“0
ż 2
y“0
ydydx
ax2 ` y2 ` 1 “ 2.05391167545, (d) ż 6
x“3
ż 5
y“2
px2 ` y2
qdydx “ 306
(e) ż 1
x“0
ż 1
y“0
ax2 ` y2dydx “ 1
3
p
?
2 ` lnp1 ` ?
2qq
E8.111 Repeat E8.110 with 3-, 4-, and 5-point Gauss-Legendre quadratures for both x- and
y-variables.
E8.112 Consider the following double integrals: (a) Transform the integrals with variable intervals
to the specified (fixed) intervals, and estimate the integrals (b) using (i) the trapezoidal and
Simpson’s 1/3 rule with 10- and 20-panels for x- and y-intervals and (ii) 3- and 5-point Gauss￾Legendre quadrature for x- and y-intervals, and (c) compare your solutions with those given true492  Numerical Methods for Scientists and Engineers
values.
(a) ż 2
0
ż ?x
0
px2
`xy ` y2
qdydx “ 5.32006868, �
0 ď y ď ?x Ñ 0 ď u ď 1
(
(b) ż π{2
0
ż cos2x
sin2x
p1 ´ 6x2
y2
qdydx “ 1.48352986,
!
sin2
x ď y ď cos2
x Ñ 0 ď u ď π
2
)
(c) ż 1
0
ż x
0
xye´xydydx “ 0.08223952, t0 ď y ď x Ñ 0 ď u ď 1u
(d) ż 1
´1
ż x2
x
ax2 ` y2e
x2`x´2ydydx “ 0.86697785, �
x ď y ď x2 Ñ ´1 ď u ď 1
(
E8.113 Estimate the following integrals using a suitable 5-point Gauss quadrature on both in￾tervals and discuss the accuracy of your estimates.
(a) ż 8
x“0
ż 1
y“0
dydx
p1 ` x ` yq
3 “ 1
4
,
(b) ż 8
x“0
ż π
y“0
sinpx ` yqe
´px`yq
dydx “ 1 ` e´π
2 ,
(c) ż 8
x“0
ż 1
y“0
exp p´x ´ x2y{8qdydx
ay ´ y2 “e
?
2π K0p1q,
(d) ż 8
x“0
ż 1
y“0
tan´1
ˆ y
x ` 1
˙ ydydx
px ` 1q2 “ p1 ´ ln 2q
2 ,
(e) ż 8
x“´8
ż 8
y“0
exp t´px2 ` y2qudydx
ax2 ` y2 “ π
?π
2 ,
(f) ż 1
x“0
ż 1
y“0
e´xydydx
?y “ 2
ˆ?π erfp1q ` 1 ´ e
e
˙
,
(g) ż 8
x“´8
ż 1
y“0
px ` yqe
y2´x2
dydx “
?πpe ´ 1q
2 ,
(h) ż π{2
x“0
ż x
y“´x
y cospx ´ yq
ax2 ´ y2 dydx “ π3
12 J1p
π
2 q,
where erfpxq is the error function, J1pxq and K0pxq are the Bessel and Modified Bessel functions,
respectively.
E8.114 The polar inertia moment of a homogeneous rectangular plate is computed by the fol￾lowing double integral:
ĳ
R
px2 ` y2
qdydx
where R : 3 ď x ď 6, 2 ď y ď 5. Estimate the polar inertia moment of the plate using (a) the
trapezoidal rule and (b) the Simpson’s 1/3 rule with 10-panels in x- and y-intervals for both cases.
E8.115 (i) Transform the following double integrals into a polar coordinate system; (ii) Estimate
the integrals with the 5-point Gauss quadrature method and discuss the accuracy of your estimates.
(a) ż 2
0
ż ?4´x2
0
x2
ye´x
?x2`y2
dydx “ 3
?π
4 erf(2) `
2
e4 ´ 1Numerical Integration  493
(b) ż 1
´1
ż ?1´x2
0
x2e´xdydx
px2 ` y2q
3{2 “ πI1p1q
(c) ż 1
0
ż ?1´x2
0
ye´px`y2{xq
dydx
ax2 ` y2 “ 1
3
ˆ
1 ´ 1
e ´ E1p1q
˙
,
(d) ż 4
0
ż ?4´x2
0
tan´1py{xqdydx
ax2 ` y2 “ π2
2
where erfpxq is the error function, I1pxq is the modified Bessel function of the first kind, and E1pxq
is the exponential integral function.
E8.116 Consider a unit circular disk, whose density is given as ρpx, yq “ exptx´y´x2´y2u. Recall
that the mass of a planar object is calculated with the following double integral: (a) Transform
the double integral into the polar coordinate system; (b) estimate the mass of the disk using
Trapezoidal and Simpson’s 1/3 rules with 10-panels for radial integration and 20-panels for angular
integration.
m “
ĳ
x2`y2ď1
ρpx, yqdxdy
E8.117 A three-dimensional solid object is bound by the z “ 0 plane, the z “ expp´x2 ´ y2q
surface, and the x2 ` y2 “ 1 cylinder. The polar inertia moment of the object is reduced to the
following double integral in the polar coordinate system:
Iz “
¡
V
px2 ` y2
qdV “
ż 2π
θ“0
ż 1
r“0
r3
e
´r2
drdθ
Use the trapezoidal and Simpson’s -1/3 rules with N “M “10, 20, 40, and 100 to estimate Iz.
E8.118 (a) Transform the following into double integrals with constant limits, and (b) then
estimate the double integrals numerically using the trapezoidal and Simpson’s 1/3 rules with
N “M “10, 20, 40, and 100 panels.
(a) ż 1
0
ż x
0
xyex2´y2
dydx “ e ´ 2
4 , (b) ż π
0
ż 2x
x
px ` yq
2
sinpx ` yqdydx “ 8π,
(c) ż 2
0
ż ?8x
´x
xydydx
ax2 ` y2 “ 8
´
1 ´
?2
3
¯
, (d) ż 1
0
ż ex
0
lnpe
x ` yqdydx “ 2 ´ e ` pe ´ 1q ln 4
8.16 COMPUTER ASSIGNMENTS
CA8.1 The digitized two-dimensional outline of a jug px, fpxqq (measurements in meters), shown
in CA8.1, is given in the table below:
Fig. CA8.1494  Numerical Methods for Scientists and Engineers
The surface area of the surface generated by revolving the curve y “ fpxq around the x´axis is
calculated with
S “
ż b
a
2πfpxq
b
1 ` pf1
pxqq2
dx
Write a computer program (or use a spreadsheet) to estimate the surface area of the jug with the
trapezoidal and Simpson’s 1/3 rules. Compute the surface area with both rules and discuss the
accuracy of your estimates. Hint: Estimate f1
pxq using second-order finite difference formulas.
x fpxq x fpxq x fpxq x fpxq
0 0 0.07 0.1225 0.14 0.0857 0.21 0.0450
0.01 0.0430 0.08 0.1205 0.15 0.0790 0.22 0.0406
0.02 0.0741 0.09 0.1167 0.16 0.0726 0.23 0.0365
0.03 0.0956 0.10 0.1116 0.17 0.0664 0.24 0.0328
0.04 0.1098 0.11 0.1056 0.18 0.0605 0.25 0.0294
0.05 0.1181 0.12 0.0992 0.19 0.0550
0.06 0.1220 0.13 0.0925 0.20 0.0498
CA8.2 The volume of a three-dimensional body, obtained by revolving y “ fpxq about the x´axis,
is found by
V “ π
ż b
a
`
fpxq
˘2 dx
Write a computer program (or use a spreadsheet) to estimate the volume of the jug (presented in
CA 8.1) by revolving the discrete data about the x-axis. Apply the trapezoidal and Simpson’s 1/3
rules to estimate the volume of the jug and discuss the accuracy of your solutions.
CA8.3 The position of a flying object is available as a set of discrete data with uniform time
steps, txptq, yptq, zptqu. (a) Write a computer program that reads the set of data and estimates
the cumulative distance traveled (by the trapezoidal rule) corresponding to every available time
value by
sptq “ ż t
t“0
dˆdx
dt ˙2
`
ˆdy
dt ˙2
`
ˆdz
dt ˙2
dt
(b) Use xptq “ 200t
2, yptq “ 350t
2, zptq “ 55tet{4 as the equations of motion to test your program.
Generate time history data with Δt “ 0.1 s up to 10 seconds and compare your answer with the
true answer of 40.942 km. Hint: Evaluate the time derivatives using second-order finite-difference
formulas.
CA8.4 Write a computer program for estimating the integral of a continuous function fpxq on
r0, as using the trapezoidal and Simpson’s 1/3 rules and Gauss-Legendre quadrature (quadrature
read from an input file). Use your program to estimate the following integral:
ż 1
0
dx
1 ` ex
(a) with the 6-, 12-, 24-, and 48-panel trapezoidal rule; (b) with the 5-, 10-, 20-, and 50-panel
Simpson’s 1/3 rule; (c) with the 3-, 5-, 7-, and 9-point Gauss-Legendre quadrature; and (d) discuss
the accuracy of your estimates. The true integral value is 1 ` lnp2{p1 ` eqq
CA8.5 The Arias intensity, used to measure the magnitude of earthquake shaking in earthquake
engineering, is defined as the total energy stored per unit weight by a series of undamped simple
oscillators at the end of the earthquake and is calculated with
Ih “ π
2g
ż T
t“0
`
a2
xptq ` a2
yptq
˘
dt
where g is the acceleration of gravity (in m/s2), T is the duration of ground shaking (in seconds),
and axptq and ayptq are the acceleration time histories in EW and NS directions (in m/s2), re￾spectively. (a) Write a computer program that reads strong-motion data (i.e., displacements xptqNumerical Integration  495
and yptq in EW and NS directions) to calculate the velocity and acceleration, as well as the Arias
intensity, with the trapezoidal rule; (b) Test your program using the data below. Hint: Note that
the strong motion data is not smooth; thus, you may use first-order finite-difference formulas to
compute the velocity and acceleration components (vxptq “ dx{dt, axptq “ dvx{dt, and so on).
t (s) xptq (cm) yptq (cm) t (s) xptq (cm) yptq (cm)
0 0 0 0.06 0.32 -1.3
0.01 -0.24 -1.45 0.07 -0.26 -0.63
0.02 0.45 1.26 0.08 -1.38 1.1
0.03 -1.09 0.53 0.09 0.12 -1.06
0.04 1.14 1.12 0.1 0.75 -0.66
CA8.6 A wire extends from x “ ´a to x “ b. The charge contained within a wire of length dx
is dq “ λpxqdx, where λpxq denotes the charge density at position x. The electric potential at
distance r between points px, 0q on the wire and px0, y0q is given by
V “
ż dq
r “
żb
x“´a
λpxqdx
b
px ´ x0q
2 ` y2
0
where λpxq “ λ0p1 ` c sin2xq with λ0 “ 1 and a “ b “ 1, px0, y0q“p0, dq. Estimate the electric
potential, correct to four decimal places, for c “ 0.1k (k “ 0, 1, 2, ..., 10) and d “ 0.25, 0.5, 1,
and 2 using Simpson’s 1/3 rule.
CA8.7 A speed boat leaves the harbor and follows the
route (shown in CA8.7), which is well represented by
xptq “ 3αt{p1 ` t
3q, yptq “ 3βt2{p1 ` t
3q where t denotes
time (in minutes), α “ 26000 m¨min2 and β “ 26000
m¨min. The force realized as a result of the propulsion of
the engine, seawater, wind drags, etc. is assumed to be
represented by
Fpx, yq “ m
ˆx3 ` y3
α3 i `
2x2y2
α2β2 j
˙
Fig. CA8.7
where m is the mass of the boat. Estimate the work done (per kg) for the first 5 minutes of travel;
that is, compute the given integral correct to four-significant-place using Romberg’s rule.
W
m “
ż
C
Fpx, yq
m ¨ dr
CA8.8 Wind turbine towers are subjected to strong winds. The wind pressures are measured
from various heights on the tower, as shown in CA8.8. The data is reported in the table below.
Location h (m) p (kPa)
0 0 288
1 5 460
2 15 530
3 20 544
4 27 556
5 40 567
6 45 570
7 50 572
8 55 574 Fig. CA8.8
Using the trapezoidal rule and the following expression to estimate the center of pressure (cop) of496  Numerical Methods for Scientists and Engineers
the tower:
Hcop “
ż H
x“0
hpphqdh{
ż H
x“0
pphqdh
CA8.9 The error function, encountered in many fields of science and engineering, is defined as
erfpxq “ 2
?π
ż x
0
e
´t2
dt
(a) Transform the integral to one on [´1, 1]; (b) write a computer program (or use a spreadsheet)
to estimate the error function for any x using 2-, 3-, 4-, and 5-point Gauss-Legendre quadrature;
(c) compare your estimates for x “ 0.1, 0.5, 1, and 2 with the true values of 0.1124629160182849,
0.5204998778130466, 0.8427007929497149, and 0.9953222650189527, respectively.
CA8.10 The distribution of molecular speed is described by the well-known Maxwell-Boltzmann
distribution, which is a function of the mass and temperature of molecules.
fpvq “ 4πv2
´ m
2πkT
¯3{2
exp „
´mv2
2kT j
where m is the mass of the molecule, k is the Boltzmann constant, v is the speed of the molecule,
and T is the absolute temperature. The average and root-mean-square (rms) speeds and the
average kinetic energy of the molecules are computed from
vavg “
ż 8
v“0
vfpvqdv vrms “
dż 8
v“0
v2fpvqdv, Eavg “
ż 8
v“0
mv2
2 fpvqdv
Write a computer program (or use a spreadsheet) to estimate the average and rms speeds of gas
molecules using a suitable quadrature method with N “ 2, 3, 4, 5, 10, 20, 30, and 40 points.
CA8.11 In radiation heat transfer, the fraction of blackbody emissive power contained between
0 and λT(wavelength ˆ temperature) leads to the following integral:
fpλTq “ 15
π4
ż 8
14390{λT
x3dx
ex ´ 1
Noting that 0 ď λT ď 105, how would you estimate the improper integral using available quadra￾ture methods? Compare your estimates with the true value of 6.493 for λT “ 105.
CA8.12 The exponential integral function of first order is defined as
E1pxq “ ż 8
x
e´t
t
dt
(a) Transform the integral to an improper integral on p0, 8q and estimate E1pxq for x “ 0.5, 1,
and 2 with N “ 2-, 4-, 6-, and 10-point Gauss-Laguerre quadrature; (b) Transform the integral
to a definite integral on [0,1] and estimate E1pxq for x “ 0.5, 1, and 2 with N “ 3-, 5-, 7-, and
9-point Gauss-Legendre quadrature.
CA8.13 (a) Write a computer program to estimate the following double integral with the trape￾zoidal and Simpson’s 1/3 rules by applying N and M panels for x- and y-intervals; (b) Estimate
the double integral for N “ M “ 8, 16, and 32 and compare your numerical estimates with those
of the true value, which is p1 ´ 4
?2 ` 3
?3q{3.
ż 1
x“0
ż 1
y“0
xy dydx
ax2 ` y2 ` 1
CA8.14 Repeat CA 8.13 using the Gauss-Legendre quadrature with N “ 3, 5, 7, and 10 points
on both x- and y-intervals.
CA8.15 The temperature distribution of a 1mˆ1m plate exposed to a heat source is obtained by
thermal imaging, and the collected data (in C˝) is presented below.Numerical Integration  497
y (m)
x (m) 0 0.2 0.4 0.6 0.8 1
0 120.0 120.20 120.80 121.80 123.20 125.0
0.2 120.4 120.76 121.52 122.68 124.24 126.2
0.4 121.6 122.12 123.04 124.36 126.08 128.2
0.6 123.6 124.28 125.36 126.84 128.72 131.0
0.8 126.4 127.24 128.48 130.12 132.16 134.6
1 130.0 131.00 132.40 134.20 136.40 139.0
An average temperature value for any plate is computed by
Tavg “ 1
Aplate ĳ
Aplate
Tpx, yqdA
Estimate the average temperature of the plate (a) using the trapezoidal rule; (b) repeat Part (a)
by employing the midpoint rule; and (c) comment on how you can apply Simpson’s 1/3 rule.CHAPTER 9
ODEs: Initial Value
Problems
LEARNING OBJECTIVES
After completing this chapter, you should be able to
‚ identify linear or nonlinear, first- or high-order initial value problems (IVPs)
and describe their general features;
‚ employ one-step explicit methods to solve IVPs;
‚ explain and implement Taylor’s method to obtain high-order solutions;
‚ formulate and employ implicit methods and linearization for first-order IVPs;
‚ explain the concepts underlying the Runge-Kutta methods and apply the 2nd,
3rd, and 4th-order schemes to first-order IVPs;
‚ define and discuss the concepts of order of accuracy, consistency, stability, and
convergence;
‚ understand the concepts of unconditional and conditional stability;
‚ explain the concept of a stiff ODE and discuss the problems and solutions
arising in solving stiff IVPs;
‚ explain and employ multi-step methods and backward differentiation formulas;
‚ understand and apply adaptive step-size control techniques;
‚ employ predictor-corrector methods to solve IVPs;
‚ explain the relative advantages and disadvantages of the one-step, multi-step,
and predictor-corrector methods;
‚ apply a suitable method to solve simultaneous first-order IVPs;
‚ reduce any high-order ODEs to a set of simultaneous first-order ODEs;
‚ apply numerical techniques to solve a set of simultaneous nonlinear IVPs.
A MATHEMATICAL model gives a theoretical prediction of a real-world physical
event in mathematical terms. A set of governing differential equations describing a
physical phenomenon or a system in the fields of science and engineering is an inevitable
part of a mathematical model. Besides differential equations, every mathematical model
consists of additional constraints such as initial or boundary conditions. In this regard,
scientists and engineers perform simulations of a physical problem or a design by solving
the pertinent mathematical model to understand the problem or predict its behavior under
certain circumstances. Some ODEs are simple enough to have true solutions, while the
498 DOI: 10.1201/9781003474944-9ODEs: Initial Value Problems  499
majority of ODEs either do not have true solutions or are difficult or too complicated to
obtain. In the latter case, numerical solutions to ODEs are sought almost exclusively as a
tool to find approximate solutions.
Differential equations encountered in science and engineering can be classified as (1)
Initial Value Problems (IVPs), (2) Boundary Value Problems (BVPs), and (3) Characteristic
or Eigenvalue Problems (EVPs). This chapter is dedicated solely to the numerical solution
of initial value problems. Boundary and Eigenvalue value problems are covered in Chapters
10 and 11, respectively.
Initial Value Problems (IVPs) describe a transient or unsteady behavior of a physical
event for t ą t0, provided that the solution is known at t “ t0 (i.e., the initial value). In
other words, the solution interval of the IVPs is semi-infinite. But numerical solutions are
typically sought for a finite interval t0 ă t ď T or until a physical event reaches the steady￾state. Some of the most common numerical methods for approximating linear or nonlinear
IVPs are presented and discussed in this chapter.
9.1 FUNDAMENTAL PROBLEM
Consider a first-order ODE (the fundamental IVP) expressed as
dy
dx “ fpx, yq, ypx0q “ α0, x ą x0 (9.1)
where x is the independent, y is the dependent variable, fpx, yq is a function describing the
differential equation, and ypx0q “ α0 is a starting value called the initial condition (IC).
Initial value problems are not confined to first-order ODEs. A second-order IVP is a
second-order ODE with two initial conditions at x “ x0:
a y2 ` b y1 ` c y “ gpxq, ypx0q “ α0, y1
px0q “ α1, x ą x0 (9.2)
High-order IVPs require as many initial conditions as the order of the ODE. For in￾stance, the following IVP requires three initial conditions to find its unique solution.
ay3 ` by2 ` cy1 ` d ypxq “ hpxq, ypx0q “ α0, y1
px0q “ α1, y2px0q “ α2 (9.3)
where α0, α1, and α2 are constants describing some physical quantities at the initial state.
The coefficients of an IVP may consist of constants (a, b, c, and d) as in the example
above, or they may be functions of the independent variable (apxq, bpxq, cpxq, and so on),
as in the example below:
px2 ` 3qy2 ` 2xy1 ` 5ypxq “ gpxq, yp0q “ 0, y1
p0q “ 3
The differential equations presented above are termed linear ordinary differential equa￾tions. If at least one of the coefficients of the ODE involves a dependent variable and/or its
derivatives (i.e., y, y1
, y2, ...), the ODE is referred to as a non-linear ordinary differential
equation. The following is an example of a non-linear ODE (or IVP):
2
d2y
dx2 ` ex`y dy
dx `
b
y ` y12 “ 0, yp0q “ 1, y1
p0q“´1
where a “ 2, b “ ex`y, and c “ ay ` y12. Note that in this IVP, b is a function of y in
addition to x, and c is a function of both y and y1
.500  Numerical Methods for Scientists and Engineers
FIGURE 9.1: (a) Swinging pendulum system and a typical solution; (a) Damped
mass-spring system and a typical solution.
In Fig. 9.1a, a damped motion of a mass m attached to a spring of length L is depicted.
The mathematical model of this event is expressed with the following second-order ODE:
d2y
dt2 `
b
m
dy
dt `
k
m
yptq “ 0
where k is the spring constant, b is the damping coefficient, and y is the vertical displacement
relative to the equilibrium position. Before the motion is set (t ă 0), the spring-mass system
is in equilibrium, i.e., yptq “ 0, which is referred to as the trivial case. This IVP has a
unique solution that depends on the imposed initial conditions. A motion is set when the
attached mass is pulled down by an amount � and then released at t“0. Hence, the vertical
displacement and velocity at rest, before the mass is released, are set as the initial condition
values, i.e., yp0q“´� and vp0q “dy{dtqt“0 “0. The mass-spring system leads to a damped
oscillatory motion about the equilibrium position. The mass eventually comes to rest at the
equilibrium position after a sufficient amount of time (as t Ñ 8, yptq Ñ 0).
In Fig. 9.1b, a pendulum with mass m is displaced from its equilibrium position by
θ0 “π{4. When the pendulum is released, a periodic (swinging) motion is set. Assuming the
motion is undamped, a second-order ODE describing the angular position of the pendulum
as a function of time, θptq, is expressed as
d2θ
dt2 ` g
L sin θ “ 0
where t is time, g is the acceleration of gravity, L is the length of the pendulum, and θ is the
angular displacement. This ODE, as is, has an infinite number of solutions. Since the IVP is
a second-order ODE, the mathematical description of the model will be complete when two
initial conditions (IC) specific to this motion are specified. Note that since the pendulum is
initially at rest (t “ 0), the angular displacement and angular speed at the onset are known,
i.e., θp0q “ π{4 and ωp0q “ dθ{dtqt“0 “ 0. By initializing the simulation time at t “ 0, theODEs: Initial Value Problems  501
rest of the motion will be relative to the initial time. The angular displacement varies in
the range ´π{4 ď θptq ď π{4 during the swinging motion, as shown in Fig. 9.1b.
As we have seen in the preceding examples, the physical quantities attached to y and
y1 are defined at the initial time; that is, ypt0q “ y0 and y1
pt0q “ y1
0 must be given or known
in order to find the solution of the IVPs for t ą 0. The solution interval of an IVP is rt0, 8q;
however, the numerical solution is frequently sought for a finite interval (0 ă t ď T or until
the steady-state is reached within a tolerance value).
9.2 ONE-STEP METHODS
One-step methods are numerical schemes in which numerical estimates or approximations
of a first-order ODE are obtained by advancing one step at a time. In other words, ypxi `hq
denoting the (i ` 1)th step estimate (also referred to as current step estimate) depends on
ypxiq denoting the ith step estimate (referred to as prior step estimate).
9.2.1 EXPLICIT EULER METHOD
Consider the following first-order IVP:
dy
dx “ fpx, yq, ypx0q “ α0, x ą x0 (9.4)
The objective is to find an approximate solution of this ODE on rx0, 8q. To obtain the
approximate value, y1 in Eq. (9.4) is replaced by the forward difference formula. Adapting
a uniform step size throughout (i.e., xi`1 ´ xi “ h for all i), this step, referred to as the
discretization procedure, results in
yi`1 ´ yi
h ` Ophq “ fpxi, yiq (9.5)
Solving Eq. (9.5) for the current step estimate, yi`1, yields
yi`1 “ yi ` hfpxi, yiq ` Oph2q, i “ 0, 1, 2,... (9.6)
where xi “ x0 ` ih and yi “ ypxiq denote the abscissa and the prior step estimate, respec￾tively. This numerical scheme is called the Explicit Euler or Forward Euler Method because
the current estimate yi`1 is explicitly obtained using the known prior estimates. Inspecting
Eqs. (9.5) and (9.6), it is clear that the method has a local truncation error order of Oph2q
and a global error order of Ophq.
The solution strategy is graphically depicted in Fig. 9.2. Since y1 is replaced by the
first-order forward difference formula (FDF), the current projection is determined along the
tangent line based on the derivative at the prior point. For any x on [xi, xi`1], Eq. (9.6)
can be rewritten as
ypxq “ yi ` px ´ xiqfpxi, yiq
which corresponds to the equation of a straight line with slope fpxi, yiq “ y1
i. The first esti￾mate, y1, is obtained after moving a small step, h. At x1, a new tangent line is constructed,
and a second estimate, y2, is found along the new tangent line. This procedure, in which
estimates are obtained step by step, is also referred to as marching or forward advancing.
By setting i “ 0 in Eq. (9.6), the first step estimate is found as
y1 “ ypx0 ` hq “ y0 ` h fpx0, y0q (9.7)502  Numerical Methods for Scientists and Engineers
FIGURE 9.2: Graphical depiction of forward advancing with explicit Euler’s method.
where x0 and y0 are prior values already known due to the initial condition, and y1 is the
current step estimate.
Now that y1 is available, setting i “ 1 in Eq. (9.6) yields
y2 “ ypx1 ` hq “ y1 ` h fpx1, y1q (9.8)
where y1 and y2 denote the prior and current step estimates, respectively. As the solution
is advanced one step at a time, the quantities on the rhs of Eq. (9.6), consisting of prior
estimates, are always known. Hence, this procedure leads to a simple and straightforward
explicit algorithm. The recursive procedure for finding the approximate solution at any xi
is obtained as follows:
ypx2 ` hq “ y3 “ y2 ` h fpx2, y2q
ypx3 ` hq “ y4 “ y3 ` h fpx3, y3q
¨¨¨ ¨¨¨
ypxi ` hq “ yi`1 “ yi ` h fpxi, yiq
Before proceeding any further, it should be noted that, from now on, the numerical
solution of an IVP will be referred to as an “estimate” (or “approximation”) since the
solution is not going to be exact.
Essentially, two types of errors are inevitably encountered in the numerical solution of
an IVP: (i) round-off error, which is due to the finite precision of floating-point arithmetic,
and (ii) truncation error, which is due to the discretization of the ODE.
Round-off errors can be remedied to some extent by increasing the arithmetic precision.
Since the explicit Euler method is first-order accurate, Ophq, a very small step size is required
to achieve very high accuracy. Nonetheless, a digital computer can, of course, handle very
small intervals in the order of billions, but then we would have to face the consequences of
the round-off errors (see Section 1.2). Even if an extremely small step size is used, the round￾off errors will accumulate over a large number of steps, and the computed estimates will
tend to diverge from the true value. In other words, round-off errors do impose a limitation
on the number of intervals that one can efficiently deal with.
Truncation error is inherited from the method, i.e., the way an ODE is discretized.
Truncation error cannot be completely eliminated in the numerical solution of any ODE,
but its magnitude can be reduced only by adopting a different numerical method. Truncation
error can basically be viewed in two parts: (i) global error, which is the difference between
the true and estimated value, ei “ yi ´ yˆpxiq, and (ii) local truncation error, which is theODEs: Initial Value Problems  503
Pseudocode 9.1
Module EXPLICIT_EULER (h, x0, y0, xlast)
\ DESCRIPTION: A pseudomodule to solve y1 “ fpx, yq on rx0, xlasts using the
\ Explicit Euler method.
\ USES:
\ FCN:: User-defined function supplying fpx, yq “ y1
.
Write: “x, y “”, x0, y0 \ Print initial value
x Ð x0 \ Initialize x
While “
x ă xlast‰ \ Marching loop: Advance one step if x ă xlast
x Ð x0 ` h \ Find current step abscissa, xi`1
y Ð y0 ` h ˚ FCNpx0, y0q \ Compute current step estimate, yi`1
Write: “x, y “”, x, y \ Print xi`1, yi`1
x0 Ð x; y0 Ð y \ Set current values as prior
End While
End Module EXPLICIT_EULER
error resulting from a one-step computation, εi “ yi ´ yi´1, where yi´1 is the true solution
of the prior point. The accumulation of all local errors with each step determines the global
error observed after numerous steps. Note that global errors are not necessarily the sum
of local errors. Global error generally becomes greater than the sum of the local errors
if a numerical scheme is unstable (see Section 9.3). However, if the numerical scheme is
stable, then the global error becomes less than the sum of the local errors. The goal of any
numerical scheme is to minimize the global error; however, keep in mind that a numerical
analyst can control only the local error.
A pseudomodule, EXPLICIT_EULER, implementing the explicit Euler scheme for the
solution of the first-order IVP is presented in Pseudocode 9.1. The module requires a step
size (h), a solution interval [x0, xlast], and the initial value y0 as input. The module is
general in that it can be applied to any first-order IVP. In other words, the solver module is
separate, and the user only needs to supply the first derivative, y1 “ fpx, yq, as a user-defined
function, FCN.
The numerical estimates are obtained step by step in the While-loop using Eq. (9.6),
Explicit Euler Method
‚ Explicit Euler method is the simplest and easiest algorithm;
‚ Memory requirement is minimal, as there is no need for arrays;
‚ It can be employed for linear and nonlinear IVPs;
‚ It requires one function evaluation per step.
‚ It is less accurate because it has a global error of Ophq;
‚ The step size should be chosen sufficiently small in order to obtain
accurate and meaningful solutions;
‚ It is conditionally stable (see Section 9.3), i.e., it is unstable under
certain conditions, giving rise to a restriction on the size of h;
‚ Using a very small step size leads to excessive cpu-time as well as the
accumulation of round-off errors.504  Numerical Methods for Scientists and Engineers
until xlast is reached. Note that the subscripted variables pxi, yiq in Eq. (9.6) are not
translated to one-dimensional array variables. If array variables were used for x and y
variables, the memory requirement would increase substantially in problems that require a
large number of steps. Besides, the estimates calculated at the end of each step need not
be kept in memory once they are printed out or saved to an output file. As a strategy to
minimize memory usage, the current estimates px, yq are printed out at the end of each
step, and then the current estimates are assigned as prior estimates, px0, y0qÐpx, yq, in
preparation for the next step calculations. Thus, only the prior and current step estimates
are known at any time step.
EXAMPLE 9.1: Implementing explicit Euler method
For t ă 0, the RL circuit shown in the figure is in steady-state, and the current
source E is off. When the switch is opened at t “ 0, the source generating current
at an exponential rate is turned on. The transient current response is subjected to
the following initial value problem:
2
dI
dt ` 5Iptq “ Eptq, ip0q “ 2A, Eptq “ " 0,
16e´t{2
t ă 0
t ě 0
Solve the resulting IVP on [0,2] with the explicit Euler method using Δt “ 0.2,
0.1, and 0.01 s. Compare your estimates with the true solution given as Iptq “
4e´t{2 ´ 2e´5t{2.
SOLUTION:
For t ě 0, rearranging the IVP as dI{dt “ fpt, Iq leads to fpt, Iq “ 8e´t{2 ´2.5I.
The initial condition at t0 “ 0 is given in the problem statement as Ip0q “ I0 “ 2A.
The computational details of the first three steps with h “ Δt “ 0.2 s are
explicitly presented here. Starting with i “ 0 (t1 “ t0 ` h “ 0.2) in Eq. (9.6), the
first step estimate, I1 “ Ipt1q “ Ip0.2q, is found as
Ip0.2q “ I1 “ I0 ` hfp0, I0q “ 2 ` p0.2qfp0, 2q “ 2 ` p0.2qp3q “ 2.6
Now, for i “ 1 (t2 “ t1 ` h “ 0.4), the second step estimate, I2 “ Ip2hq “ Ip0.4q, is
obtained as
I2 “ I1 ` hfp0.2, I1q “ 2.6 ` p0.2qp7.38699q “ 2.747740ODEs: Initial Value Problems  505
FIGURE 9.3
Next, setting i “ 2 (t3 “ 0.6), the third step estimate I3 “ Ip0.6q leads to
I3 “ I2 ` hfpt2, I2q “ 2.747740 ` p0.2qp´0.319504q “ 2.683839
The estimates for the subsequent steps, like Ip0.8q “ I4, Ip1q “ I5, and so on,
are obtained in a recursive manner. The true solution as well as the estimates Iptiq
obtained with the explicit Euler for h“0.1 and h“0.01, are comparatively depicted
in Fig. 9.3. Note that the deviations of the estimates from the true current can be
clearly distinguished between t“0.2 and t“1.5, and the maximum deviations appear
to occur around t“0.5. For t ą 1.5, as the estimates with both h“0.2 and h“0.1
recover and approach the true solution, the global errors (deviations) also approach
zero. However, the magnitude of the deviations is smaller for the h“0.1 case since
a smaller value of h naturally leads to a smaller local truncation error.
TABLE 9.1
True h “ 0.2 h “ 0.1 h “ 0.01
t Solution Ii ei Ii ei Ii ei
0 22 2 2
0.2 2.406288 2.60000 1.94E-01 2.485984 7.97E-02 2.413221 6.93E-03
0.4 2.539164 2.74774 2.09E-01 2.629835 9.07E-02 2.547322 8.16E-03
0.6 2.517013 2.683839 1.67E-01 2.593561 7.66E-02 2.52414 7.13E-03
0.8 2.410610 2.527229 1.17E-01 2.46712 5.65E-02 2.416062 5.45E-03
1 2.261953 2.336126 7.42E-02 2.300049 3.81E-02 2.265774 3.82E-03
1.2 2.095672 2.138512 4.28E-02 2.119256 2.36E-02 2.098149 2.48E-03
1.4 1.925946 1.947355 2.14E-02 1.939005 1.31E-02 1.927408 1.46E-03
1.6 1.760685 1.768214 7.53E-03 1.766535 5.85E-03 1.761423 7.39E-04
1.8 1.604061 1.603033 1.03E-03 1.605205 1.14E-03 1.604308 2.47E-04
2 1.458042 1.452028 6.01E-03 1.456263 1.78E-03 1.45797 7.19E-05
The numerical estimates of the IVP for h “ 0.2, 0.1, and 0.01, as well as the
true errors (ei “ |Itrueptiq ´ Ii|) at each time step, are comparatively tabulated in
Table 9.1. It is clear that the absolute errors are reduced by half on average as the
time step size is reduced from h “ 0.2 to h “ 0.1. Similarly, when the step size is
reduced from h “ 0.1 to 0.01 (i.e., h is reduced by a factor of ten), the errors also
decrease by a factor of ten on average, confirming that the global error is directly
proportional to h. Note that the absolute error, while smaller in magnitude at first,
steadily increases up to t «0.5, where it goes through a local maximum and then
decreases as t increases.506  Numerical Methods for Scientists and Engineers
Discussion: In this example, we can rule out round-off errors as a source of error
accumulation since moderate step sizes (h « 0.01´0.2) and high precision arithmetic
were used to obtain the estimates in Table 9.1. The main source of error here is
primarily due to discretization (global) error, which is directly proportional to h, i.e.,
Ophq. Additionally, for any h, the global errors depict an increase at the beginning
of the solution (0.2 ă t ă 1.5), but then the errors steadily decrease. This behavior
indicates that the estimates with this method are stable for the h values at which
the solution is obtained.
Each time step contributes a new error to the current estimate,
which accumulates over a large number of steps. The rate of ac￾cumulation depends on the ODE, i.e., y1 “ fpx, yq.
9.2.2 METHOD OF TAYLOR POLYNOMIAL
In calculus courses, we have learned that every univariate function can be expanded into a
power series about x “ x0. In other words, a function fpxq can be expressed in the form of
a power series if fpx0q and f1
px0q, f 2px0q, and so on derivatives can be evaluated. In this
respect, the Taylor series is a robust tool for obtaining a power series approximation of a
continuous and differentiable function.
Consider the first-order model IVP, Eq. (9.1). The first derivative of the function, y1 “
fpx, yq, and its initial value, ypx0q“y0, are already given in the problem. This information
is sufficient to construct an nth-degree Taylor polynomial, Tnpxq, of ypxq in the vicinity of
x “ x0; that is,
Tnpxq“ypx0q`px´x0qy1
px0q` px´x0q
2
2! y2px0q`...` px´x0q
n
n! ypnq
px0q`Rnpxq (9.9)
where Rnpxq denotes the remainder, which is given as
Rnpxq “ px´x0q
n`1
pn ` 1q! ypn`1q
pξq, x0 ă ξ ă x
For any x “ xi, Tnpxq can be generalized as
Tnpxiq “ yi`1 “ yi`hfpxi, yiq` h2
2! f1
pxi, yiq`...`
hn
n!
fpn´1q
pxi, yiq`Rn (9.10)
Note that for order n “ 1, Taylor’s method gives the explicit Euler method. An upper
bound can be estimated from
|Rn| ă
hn`1
pn ` 1q!
M, xi ă ξ ă xi`1
with
M ě
ˇ
ˇ
ˇypn`1q
pξq
ˇ
ˇ
ˇ
max
“
ˇ
ˇ
ˇfpnq
pξ,ypξqq
ˇ
ˇ
ˇ
max
, xi ă ξ ă xi`1
Even though the algorithm is straightforward, it does require the user to derive and
supply an appropriate number of derivatives of y or f, which can become cumbersome very
quickly. The above error estimation strategy can be useful when high-order derivatives are
simple and easy to derive.ODEs: Initial Value Problems  507
Pseudocode 9.2
Module TAYLOR_POLY (degree, h, x0, y0, xlast)
\ DESCRIPTION: A pseudomodule to solve y1 “ fpx, yq on rx0, xlasts using the
\ Taylor polynomials of degree ě 2.
\ USES:
\ FCNS:: A user-defined function supplying y1
, y2, y3 and so on.
h2 Ð h ˚ h{2; h3 Ð h2 ˚ h{3 \ Pre-set h2{2!, h3{3!
Write: “x, y=”, x0, y0 \ Print initial value
x Ð x0 \ Initialize x
While “
x ă xlast‰ \ Marching Loop : Advance one step if x ă xlast
x Ð x0 ` h \ Find current step abscissa, xi`1
yp1 Ð FCNSp1, x0, y0q \ Compute y1
px0q using FCNS
yp2 Ð FCNSp2, x0, y0q \ Compute y2px0q using FCNS
y Ð y0 ` h ˚ yp1 ` h2 ˚ yp2 \ 2nd degree approximation, yi`1 “ T2pxi`1q
If “
degree “ 3
‰
Then
yp3 Ð FCNSp3, x0, y0q \ Compute y3px0q using FCNS
y Ð y ` h3 ˚ yp3 \ 3nd degree approximation, yi`1 “ T3pxi`1q
Else
\ You can nest higher-order terms of the approximation here
End If
Write: “x, y=”, x, y \ Print current estimates
x0 Ð x; y0 Ð y \ Set current step values as prior
End While \ End the integration loop
End Module TAYLOR_POLY
Function Module FCNS (n, x0, y0)
\ DESCRIPTION: A pseudo Function for computing the derivatives up to the 4th
\ degree. All pertinent derivatives must be supplied here.
If “
n “ 1
‰
Then
FCNSÐ fpx0, y0q \ Define y1
px0q “ fpx0, y0q here
Else
If “
n “ 2
‰
Then
FCNSÐ y2 \ Define y2px0q at px0, y0q here
Else
\ You can nest all ypnq
’s at px0, y0q here
End If
End If
End Function Module FCNS
Subsequently, higher-order derivatives of ypxq can be generated by simply differenti￾ating y1 “ fpx, yq as many times as necessary. Using the chain rule, the second and third
derivatives are obtained as follows:
y2
i “ f1
pxi, ypxiqq “ ˆBf
Bx
dx
dx `
Bf
By
dy
dx˙
x“xi
“
ˆBf
Bx
`
Bf
By
fpx, yq
˙
x“xi
y3
i “ f 2pxi, ypxiqq “ ˆB2f
Bx2 ` 2y1 B2f
BxBy ` py1
q
2 B2f
By2 ` y2 Bf
By
˙
x“xi
“
ˆB2f
Bx2 ` 2f B2f
BxBy
` f 2 B2f
By2 `
ˆBf
Bx
` f Bf
By
˙ Bf
By
˙
x“xi
,508  Numerical Methods for Scientists and Engineers
Method of Taylor Polynomials
‚ It is a semi-analytical method for solving IVPs;
‚ It is an explicit method that only requires knowledge of the prior step
to estimate the current step;
‚ It is capable of providing highly accurate solutions if a high-degree
polynomial approximation can be derived, i.e., the order of accuracy
is Ophnq where n is the degree of polynomial approximation;
‚ An optimal degree of a polynomial can be determined by ensuring
that the remainder satisfies the desired accuracy (|Rn| ă ε) in the
solution range.
‚ It may diverge when the ODE is “stiff” or fpx, yq is not analytic near
x0 or has discontinuous points, and so on;
‚ It requires explicit expressions for high-order derivatives of fpx, yq,
which may be cumbersome or very difficult to find since the coeffi￾cients of the polynomials depend on fpx, yq;
‚ Computationally, it can be expensive per step due to the larger num￾ber of function and high-order derivative evaluations.
Once the required derivatives are substituted into Eq. (9.10), the construction of the
polynomial approximation of the nth degree is complete.
The module, TAYLOR_POLY, introduced in Pseudocode 9.2 computes the approximate
solutions of the first-order IVP using Taylor polynomials up to 3rd degree. The module
requires the degree of polynomial approximation (degree), the initial value (y0), as well
as the solution interval [x0, xlast] as input. Before starting the marching procedure, the
multipliers h2{2! and h3{3! are computed and saved on the variables h2 and h3 as a cpu-time￾saving strategy. Since Eq. (9.10) involves high-order derivatives, the IVP-specific derivatives
should be derived by the analyst and provided to the main module via the user-defined
external function module, FCNS. If-Then-Else structures are used to add new terms to
Taylor polynomials. Using Eq. (9.10), the forward marching procedure is executed step by
step in the While-loop until xlast is reached. As with Pseudocode 9.1, the use of array
variables is avoided to keep the memory requirement at a minimum. Hence, the current
estimates are assigned as prior estimates, px0, y0qÐpx, yq, before returning to the top of
the loop. If the user wishes, for instance, to obtain only, say, a third-degree polynomial
approximation to solve the IVP, then If-Then-Else statements within the While-loop can be
removed (in Pseudocode 9.2) to make the program more cpu-time efficient. The module
does not have an output argument as EXPLICIT_EULER since the solutions at intermediate
steps are printed out or saved on an external file.
The arguments of FCNS are the order of the derivative (n) and the point (x0, y0)
where the nth derivative is evaluated. For n “ 1, 2, 3, and so on, the derivatives y1
px0q,
y2px0q, y3px0q, and so on are respectively defined using conditional statements, as shown
in the pseudocode presented in Pseudocode 9.2.
The order of a numerical method is determined by its order of
global error. An nth-order method has a global error of Ophnq.ODEs: Initial Value Problems  509
EXAMPLE 9.2: Applying Taylor polynomial method
Using h “ 0.1, obtain 2nd, 3rd, and 4th-degree Taylor polynomial approximations
for the IVP given in Example 9.1.
SOLUTION:
The initial condition and the first derivative are pt0, I0q“p0, 2q and fpt, Iq “
8e´t{2 ´ 2.5I, respectively. Our next step is to find analytical expressions for higher
derivatives. It turns out that the derivatives up to the nth order can be easily ob￾tained by sequential differentiation of dI{dt, as shown below:
d2I
dt2 “ d
dt ˆdI
dt ˙
“ df
dt “ ´4e´t{2 ´ 5
2
dI
dt “ ´24e´t{2 `
25
4
Iptq
d3I
dt3 “ d
dt ˆd2I
dt2
˙
“ d
dt ˆ
´24e´t{2 `
25
4
I
˙
“ 62e´t{2 ´ 125
8
Iptq
d4I
dt4 “ d
dt ˆd3I
dt3
˙
“ d
dt ˆ
62e´t{2 ´ 125
8
I
˙
“ ´156e´t{2 `
625
16
Iptq
Substituting the above expressions into Eq. (9.10) leads to the nth-degree Taylor
polynomial:
Ii`1 “ Tnptiq “ Ii ` ÿn
k“1
hk
k!
dk
dtk Iptiq ` Ophn`1q
The order of approximation depends on the level of accuracy desired, which can
be determined by ensuring that the truncation error is less than a preset tolerance.
For n ě 2, noting that M “ max|I pn`1q
pξq|, we may write
|Rn| “ hn`1
pn ` 1q!
M ă 0.5 ˆ 10´d, ti ă ξ ă ti`1
where d is the decimal-place accuracy desired.
Even though the high-degree derivatives are obtained easily and simplified to
I
pn`1q
i “ C1e´ti{2 ` C2Iptiq, determining the value of M, for instance, for n “ 3,
requires the investigation of the d4Ii{dt4 distribution over the solution range, which
is not an easy task.
TABLE 9.2
ti True T2 T3 T4
Solution Ii ei Ii ei Ii ei
02 2 2 2
0.2 2.406288 2.37 3.63E-02 2.411000 4.71E-03 2.405808 4.80E-04
0.4 2.539164 2.494668 4.45E-02 2.544864 5.70E-03 2.538582 5.82E-04
0.6 2.517013 2.476146 4.09E-02 2.522182 5.17E-03 2.516483 5.30E-04
0.8 2.410610 2.377308 3.33E-02 2.414776 4.17E-03 2.410181 4.28E-04
1 2.261953 2.236576 2.54E-02 2.265099 3.15E-03 2.261628 3.25E-04
1.2 2.095672 2.077174 1.85E-02 2.097951 2.28E-03 2.095436 2.36E-04
1.4 1.925946 1.912903 1.30E-02 1.927550 1.60E-03 1.925779 1.67E-04
1.6 1.760685 1.751740 8.94E-03 1.761788 1.10E-03 1.760569 1.16E-04
1.8 1.604061 1.598086 5.97E-03 1.604807 7.46E-04 1.603982 7.89E-05
2 1.458042 1.454162 3.88E-03 1.458538 4.96E-04 1.457989 5.31E-05510  Numerical Methods for Scientists and Engineers
For h “ 0.1, the numerical estimates with the 2nd, 3rd, and 4th-degree Taylor
polynomial approximations, along with the absolute errors (ei “ |Ii,true ´ Ii|), are
comparatively tabulated in Table 9.2. It is clear that as the degree of the polynomial
(or order of the approximation Ophnq) increases, the numerical estimates approach
the true solution. In other words, the numerical (truncation) errors are reduced with
increasing the degree of the Taylor polynomial.
Discussion: Table 9.2 shows that the errors in the Taylor approximations obtained
with T2ptq, T3ptq, and T4ptq increase up to x « 0.5 (just as in Example 9.1), and
then the errors tend to decrease with increasing x. It should also be noted that the
order of global errors is consistent with Ophnq, i.e., for h “ 0.1, the order of errors
are Op10´2q, Op10´3q, and Op10´4q for T2ptq, T3ptq, and T4ptq, respectively.
9.2.3 IMPLICIT EULER METHOD
As an alternative to the explicit Euler method, which uses the slope at point xi, y1
i “
fpxi, yiq, the slope in the implicit Euler method is estimated along the tangent line at point
xi`1: y1
i`1 “ fpxi`1, yi`1q. The first derivative is discretized at x “ xi`1 using the first￾order backward difference formula, while the right-hand side of Eq. (9.5) is evaluated at
xi`1 as follows:
yi`1 ´ yi
h ` Ophq “ fpxi`1, yi`1q (9.11)
This method is also referred to as backward Euler method in the literature.
By solving Eq. (9.11) for yi`1, a general recursive relation marching in the x-direction
is obtained, as in the explicit method. Having said that, note that when fpx, yq is nonlinear,
which is generally the case, this scheme may not permit us to explicitly isolate yi`1. Thus,
in cases where the IVP is nonlinear, the yi`1 has to be essentially solved iteratively using
a root-finding method such as the fixed-point or Newton-Raphson method at every step.
For instance, employing the fixed-point iteration method, the iteration equation can be
expressed as
zpp`1q “ yi ` hfpxi`1, zppq
q (9.12)
where z “ yi`1 and superscript p denotes the iteration step.
To employ the Newton-Raphson method, we express Eq. (9.11) as
Gpzq “ z ´ yi ´ hfpxi`1, zq “ 0 (9.13)
then the Newton-Raphson iteration equation becomes
zpp`1q “ zppq ´ Gpzppq
q
dG
dz pzppqq
“ zppq ´ Gpzppq
q
1 ´ h df
dz pzppqq
(9.14)
where zp0q “ yi may be taken as a suitable initial estimate.
Clearly, if f is nonlinear, then the implicit Euler method is not only computationally
more involved but also time-consuming, in contrast to the explicit Euler method. Never￾theless, a linearization technique may be applied to avoid solving nonlinear equations. To
demonstrate this technique, consider the Taylor series of fpxi`1, yi`1q about pxi`1, yiq:
fpxi`1, yi`1q“fpxi`1, yiq`pyi`1´yiq
´Bf
By
¯
pxi`1,yiq
` pyi`1´yiq
2
2!
´B2f
By2
¯
pxi`1,yiq
`... (9.15)ODEs: Initial Value Problems  511
Pseudocode 9.3
Module IMPLICIT_EULER (h, ε, maxit, x0, y0, xlast)
\ DESCRIPTION: A pseudomodule to solve y1 “ fpx, yq on rx0, xlasts using the
\ Implicit Euler method. Fixed-Point iteration is used to solve the resulting
\ nonlinear equations.
\ USES:
\ ABS:: A built-in function computing the absolute value;
\ FCN:: A User-defined function supplying fpx, yq “ y1
.
Write: “x, y=”, x0, y0 \ Print initial value
x Ð x0 \ Initialize x
While “
x ă xlast‰ \ Marching loop, march until x ă“ xlast
x Ð x0 ` h \ Advance one step, xi`1
p Ð 0 \ Initialize counter for FP iterations
z Ð y0 \ Set initial estimate, zp0q “ yi
y Ð z \ Set current estimate to zp0q
Repeat \ FP iteration loop to solve nonlinear eqs.
p Ð p ` 1 \ Count iterations
z Ð y0 ` h ˚ FCNpx, zq \ Find current estimate
aerr Ð |z ´ y| \ Find absolute error
y Ð z \ Set current step estimate as prior
If “
p “ maxit‰
Then \ Check no. iterations to prevent infinite loop
Write: “Max Iterations reached. Computation is terminated.”
Stop \ FPI did not converge with current maxit and h
End If
Until “
aerr ă ε
‰ \ When converged yi`1 “ zpp`1q
Write: “x, y=”, x, y \ Print current estimates
x0 Ð x; y0 Ð y \ Set current step values as prior
End While \ End marching loop
End Module IMPLICIT_EULER
Replacing fpxi`1, yi`1q in Eq. (9.11) with the two-term approximation from Eq. (9.15)
yields
yi`1 ´ yi
h ` Ophq “ fpxi`1, yiq`pyi`1 ´ yiq
´Bf
By
¯
pxi`1,yiq
` ... (9.16)
Note that since yi`1 ´ yi ” Ophq, the order of accuracy in this scheme remains the same as
in the explicit method. Rearranging and solving Eq. (9.16) for yi`1 results in
yi`1 “ yi ` hfpxi`1, yiq
1 ´ hBf
By pxi`1, yiq
` Oph2q (9.17)
Equation (9.17) now permits us to march forward step by step without the need for
any iteration.
A pseudomodule IMPLICIT_EULER, implementing the implicit Euler scheme on a first￾order IVP, is presented in Pseudocode 9.3. The module requires a step size (h), an upper
bound for the maximum number of iterations (maxit), a convergence tolerance (ε) for
the solution of the nonlinear equations, the initial value (y0), and the solution interval
[x0, xlast] as input. The module is basically the same as EXPLICIT_EULER, with the ex￾ception that the current estimate (z) is obtained by solving Eq. (9.12) iteratively using512  Numerical Methods for Scientists and Engineers
Implicit Euler Method
‚ The implicit Euler method is a stable scheme that allows the use of
large step sizes;
‚ It can be applied to both linear and nonlinear IVPs;
‚ The linearized scheme may offer some advantages if sufficiently large
step sizes can be used.
‚ The method has a first-order global error, Ophq;
‚ For nonlinear IVPs, the algorithm is a bit involved, and the cpu-time
can be significantly higher due to the implementation of a root-finding
scheme at each step.
the fixed-point iteration method within the Repeat-Until loop. In this loop, z and y denote
the current (zpp`1q
) and prior (zppq
) estimates, respectively. For sufficiently small h, the
fixed-point iteration (FPI) generally converges very quickly; however, it should be noted
that the Newton-Raphson method in general exhibits better convergence properties. An
absolute error criterion, |z ´ y| ă ε, is used to terminate the FPI procedure. Once the cur￾rent step estimate yi`1 is obtained and printed out, it is assigned as the prior step estimate
px0, y0qÐpx, yq before proceeding with the next step. The computations are repeated until
xlast is reached.
9.2.4 TRAPEZOIDAL RULE
The explicit Euler scheme linearizes IVP at xi with the slope fpxi, yiq, while the implicit
Euler method does that at xi`1 with the slope fpxi`1, yi`1q. If the step size is not sufficiently
small, the estimates with either method over many steps will lead to substantial deviation
from the true solution, as illustrated in Fig. 9.4.
An alternative scheme can be established by replacing the slope with the arithmetic
average of fpxi, yiq and fpxi`1, yi`1q, that is,
yi`1 ´ yi
h ` Oph2q “ 1
2
`
fpxi, yiq ` fpxi`1, yi`1q
˘ (9.18)
which is an implicit scheme, and it yields a nonlinear equation when the IVP is nonlinear.
Note that y1
i in Eq. (9.18) is discretized by the central difference approximation at xi`1{2,
i.e., the mid-interval.
To find an expression for the truncation error of the trapezoidal rule, Eq. (9.4) is
integrated from xi to xi`1:
yi`1 “ yi `
ż xi`1
xi
f
`
u, ypuq
˘
du (9.19)
Replacing the integral with the trapezoidal rule yields
yi`1 “ yi `
h
2
`
fpxi, yiq ` fpxi`1, yi`1q
˘
´ h3
12 y3pξq, xi ă ξ ă xi`1 (9.20)
Comparing Eqs. (9.20) and (9.18), the two equations are clearly the same. However, Equa￾tion (9.20) reveals an expression for the local truncation error, E “ ´h3y3pξq{12. Moreover,
Eq. (9.20) indicates that the trapezoidal rule should give more accurate estimates than theODEs: Initial Value Problems  513
FIGURE 9.4: Graphical depiction of marching with the Explicit Euler,
Implicit Euler methods and Trapezoidal rule.
Trapezoidal Rule
‚ The trapezoidal rule is a stable implicit method;
‚ It has a second-order global error, Oph2q;
‚ A linearized scheme with a suitable h can be successfully employed
to solve nonlinear IVPs.
‚ The method suffers from the same drawbacks as the implicit Euler’s
method, i.e., since it is implicit, obtaining the current step estimate
for nonlinear IVPs requires the solution of nonlinear algebraic equa￾tions, which increases the cpu-time.
explicit or implicit Euler methods because its global error is Oph2q. Also, note that the
slope at each step is the average of those of explicit and implicit Euler schemes, which
follows more closely to the true slope, as seen in Fig. 9.4. This method is referred to as the
Trapezoidal rule since the trapezoidal rule is used in the numerical integration.
For nonlinear IVPs, the fixed-point iteration or Newton-Raphson methods can be ap￾plied (see Section 9.2.3). Equation (9.20) can likewise be linearized using the same procedure
illustrated for the implicit Euler method. The linearization procedure for the trapezoidal
scheme yields
yi`1 “ yi `
h
2
fpxi, yiq ` fpxi`1, yiq
1 ´ h
2
Bf
By pxi`1, yiq
` Oph3q (9.21)
Note that this expression retains the second-order global accuracy and also allows the
numerical estimates to be obtained recursively and explicitly.
A pseudocode for the trapezoidal rule is not given here simply because replacing Eq.
(9.12) in IMPLICIT_EULER with Eq. (9.18) is sufficient to modify the code for this purpose.
The linearized implicit Euler and linearized trapezoidal methods are easier to implement
since these schemes are explicit in nature, so the EXPLICIT_EULER module can be eas￾ily modified to accommodate the linearized methods. However, in this case, another user￾defined function will be necessary to evaluate the partial derivative Bf{Bypxi`1, yiq, which
needs to be explicitly derived and supplied to the module.514  Numerical Methods for Scientists and Engineers
Midpoint Method
‚ It is simple and easy to program;
‚ It can be employed for both linear and nonlinear IVPs;
‚ The global error for the midpoint method is second order, Oph2q;
‚ It is comparable to the trapezoidal rule in terms of cost.
‚ It requires two function evaluations per step, i.e., its computational
cost is about twice as much as that of the explicit Euler method.
9.2.5 MIDPOINT METHOD
In this method, the first-order IVP, Equation (9.4), is evaluated at the midpoint of the next
step, y1
pxi`1{2q “ fpxi`1{2, yi`1{2q. Then, using the central difference approximation for the
first derivative results in
yi`1 ´ yi
h ` Oph2q “ f
`
xi`1{2, yi`1{2
˘
Solving for yi`1 gives
yi`1 “ yi ` hf`
xi`1{2, yi`1{2
˘
` Oph3q (9.22)
On the other hand, by replacing the integral in Eq. (9.19) with the midpoint rule, we also
arrive at Eq. (9.22), which is why it is referred to as the midpoint method. In practice, Eq.
(9.22) is applied as follows:
yi`1 “ yi ` hf ˆ
xi `
h
2
, yi `
h
2
fpxi, yiq
˙
` Oph3q (9.23)
where xi`1{2 “ xi ` h{2 and yi`1{2 “ yi ` ph{2qfpxi, yiq.
9.2.6 MODIFIED EULER METHOD
The modified Euler’s method is based on the trapezoidal rule. To make this an explicit
method, the current estimate yi`1 on the rhs of the trapezoidal rule is replaced by the
explicit Euler approximation (yi`1 Ð yi ` hfpxi, yiq), as follows:
yi`1 ´ yi
h ` Oph2q “ 1
2
´
fpxi, yiq ` f
`
xi`1, yi ` hfpxi, yiq
˘¯
Hence, the modified Euler’s scheme can be expressed as
yi`1 “ yi `
h
2
`
fi,j ` fpxi`1, yi ` hfi,j q
˘ or yi`1 “ yi `
1
2
pk1 ` k2q (9.24)
where fi,j “ fpxi, yiq, k1 “ hfpxi, yiq, and k2 “ hfpxi ` h, yi ` k1q.
The advantages and disadvantages of the method are practically the same as those of
the midpoint method. In this section, pseudocodes are not given for the midpoint rule and
modified Euler because they are explicit methods. The difference equation for the explicit
Euler needs to be simply replaced by either Eq. (9.23) or (9.24).ODEs: Initial Value Problems  515
EXAMPLE 9.3: Comparing implicit Euler, trapezoidal, midpoint, modified Euler methods
Solve Example 9.1 using the implicit Euler, trapezoidal, midpoint, and modified Eu￾ler methods with h “ 0.1. Compare your numerical estimates with the true solution.
Use ε “ 0.5 ˆ 10´6.
SOLUTION:
The difference equation for the implicit Euler method, Eq. (9.11), in this example,
takes the following form:
Ii`1 “ Ii ` hfpti`1, Ii`1q “ Ii ` h
`
8e´ti`1{2 ´ 2.5Ii`1
˘
Since fpt, Iq “ 8e´t{2 ´ 2.5I, the IVP is a linear IVP. Hence, a root-finding method
is not required to find the current estimate, Ii`1.
Next, solving for Ii`1 yields the following explicit equation:
Ii`1 “ Ii ` 8he´ti`1{2
1 ` 2.5h , i “ 0, 1, 2,...
Similarly, using Eqs. (9.18) and solving for Ii`1, the trapezoidal rule gives
Ii`1 “ p1 ` 1.25hq
´1
!
p1 ´ 1.25hq Ii ` 4h
´
e´ti{2 ` e´ti`1{2
¯) , i “ 0, 1, 2,...
where ti`1 “ ti ` h.
TABLE 9.3
t True Implicit Trap. Midpoint Modified
Solution Euler Rule Method Euler
02 0 0
0.2 2.406288 6.02E-02 3.12E-03 7.07E-03 6.65E-03
0.4 2.539164 7.30E-02 3.76E-03 8.43E-03 7.79E-03
0.6 2.517013 6.58E-02 3.40E-03 7.49E-03 6.75E-03
0.8 2.410610 5.20E-02 2.72E-03 5.85E-03 5.09E-03
1 2.261953 3.78E-02 2.03E-03 4.22E-03 3.47E-03
1.2 2.095672 2.56E-02 1.45E-03 2.86E-03 2.15E-03
1.4 1.925946 1.60E-02 1.00E-03 1.81E-03 1.14E-03
1.6 1.760685 8.89E-03 6.73E-04 1.05E-03 4.35E-04
1.8 1.604061 3.87E-03 4.37E-04 5.25E-04 4.15E-05
2 1.458042 4.66E-04 2.74E-04 1.73E-04 3.45E-04
Discussion: Applying the midpoint and modified Euler schemes, Eqs. (9.23) and
(9.24), is straightforward because they are explicit methods. Starting with the initial
condition pt0, I0q“p0, 2q, the numerical estimates for i “ 1, 2, 3,... (i.e., ti ą 0) are
obtained recursively using the pertinent difference equations. In Table 9.3, the true
errors by the method for selected points are presented comparatively. It is observed
that the implicit Euler method yields the largest errors among all four methods.
This result is not surprising since the global error of the implicit Euler is first order,
i.e., Op10´2q.
The trapezoidal, the midpoint, and the modified Euler methods, on the other516  Numerical Methods for Scientists and Engineers
hand, have second-order global errors, i.e., Op10´4q. For this reason, the trapezoidal
rule results in better estimates than those of the midpoint and modified Euler meth￾ods. The average error for the midpoint rule and the modified Euler method is about
twice as large as that of the trapezoidal rule.
9.2.7 RUNGE-KUTTA (RK) METHODS
Using Taylor’s polynomials to obtain the numerical solution y1 “ fpx, yq in order to get a
high-order approximation is not very practical. As we have seen earlier, obtaining explicit
expressions for high-order derivatives of ypxq depends on the function fpx, yq, and in most
cases, the derivatives can become very complicated very quickly.
A class of Runge-Kutta methods are one-step explicit schemes that only involve fpx, yq
evaluations. Hence, it is easier to generate second-, third-, or fourth-order-accurate schemes.
All Runge-Kutta schemes have the following general form:
yi`1 “ yi ` φph, xi, yiq (9.25)
where φ is referred to as the increment function, which is simply chosen to represent a
suitable approximation for the slope on rxi, xi`1s.
The increment function may be expressed in terms of a weighted average as
φph, xi, yiq “ w1k1 ` w2k2 `¨¨¨` wnkn (9.26)
where n denotes the order of the Runge-Kutta method, w1, w2,...,wn are the weights, and
k1, k2,...,kn are relationships defined as
k1 “ hfpxi, yiq
k2 “ hfpxi ` α2h, yi ` β21k1q
k3 “ hfpxi ` α3h, yi ` β31k1 ` β32k2q
.
.
.
kn “ hfpxi ` αnh, yi `
n
ÿ´1
m“1
βnmkmq
(9.27)
Equations (9.25) through (9.27) can be expressed in a compact form as
yi`1 “ yi ` ÿn
m“1
wmkm (9.28)
To determine the aforementioned weights and coefficients, consider the Taylor series expan￾sion of ypxi ` hq about xi:
yi`1 “ yi ` hy1
i `
h2
2! y2
i `
h3
3! y3
i `¨¨¨ (9.29)
The goal is to obtain the higher derivatives in Eq. (9.29) in terms of fpx, yq by successive
differentiations as follows:ODEs: Initial Value Problems  517
y1
i “ fpxi, yiq
y2
i “ df
dx
ˇ
ˇ
ˇ
ˇ
i
“ `
fx ` fyy1
˘
i “ pfx ` fyfqi
y3
i “ d2f
dx2
ˇ
ˇ
ˇ
ˇ
i
“
´
fxx ` 2ffxy ` f 2fyy ` fxfy ` fpfyq
2
¯
i
...
(9.30)
where subscripts x and y denote partial differentiations.
Substituting Eq. (9.30) into Eq. (9.29) yields
yi`1 “yi`hfpxi, yiq` h2
2!
`
fx`fyf
˘
i
`
h3
3!
`
fxx`2ffxy`f 2fyy`fxfy`ff 2
y
˘
i
`¨¨¨ (9.31)
which also involves many partial derivatives. To avoid the partial derivatives and make it
comparable to Eq. (9.28), the function evaluations in Eq. (9.27) are replaced with their
Taylor series approximations.
Recall that the Taylor series of fpx, yq about the point pa, bq is
fpa`h, b`kq“fpa, bq`´
h B
Bx
`k B
By
¯
fpa, bq` 1
2!
´
h B
Bx
`k B
By
¯2
fpa, bq`¨¨¨ (9.32)
Now the function evaluations in Eq. (9.27) are approximated by the Taylor series as follows:
fpxi ` α2h, yi ` β21k1q – fpxi, yiq ` α2hfxpxi, yiq ` β21k1fypxi, yiq ` ...
fpxi ` α3h, yi ` β31k1 ` β32k2q – fpxi, yiq ` α3hfxpxi, yiq`pβ31k1 ` β32k2q
ˆ fypxi, yiq ` ...
¨¨¨ ¨¨¨
(9.33)
Next, substituting Eq. (9.33) into Eq. (9.27) and comparing with Eq. (9.31), a set of
linear equations is obtained where wn, αn and βnm’s are unknowns. In this section, only the
derivation of the first- and second-order Runge-Kutta methods is presented to save space.
The first-order Runge-Kutta method yields
yi`1 “ yi ` k1 ` Oph2q “ yi ` hfpxi, yiq ` Oph2q (9.34)
where k1 “ hfpxi, yiq, and it corresponds to the explicit Euler method.
The increment function for quadratic Runge-Kutta is expressed as:
yi`1 “ yi ` w1k1 ` w2k2 ` Oph3q (9.35)
where
k1 “ hfpxi, yiq and k2 “ hfpxi ` αh, yi ` βk1q (9.36)
The parameters α, β, w1, and w2 are determined in such a way that Eq. (9.35) satisfies the
following 3rd-degree Taylor polynomial approximation of ypxq:
yi`1 “ yi ` hfpxi, yiq ` h2
2! pfx ` fyfqxi ` Oph3q (9.37)518  Numerical Methods for Scientists and Engineers
Runge-Kutta Method
‚ Runge-Kutta schemes are explicit and relatively easy to apply;
‚ They are self-starting, i.e., only an initial condition is required to
start the forward marching procedure;
‚ They have better stability characteristics (see Section 9.2);
‚ Unlike the method of Taylor polynomials, partial derivatives of
fpx, yq are not needed;
‚ Fourth- and fifth-order RK-schemes are very popular and widely used.
‚ RK methods do not provide any means of error estimation based on
the interval size h;
‚ They require a relatively large amount of cpu-time due to multiple
function evaluations;
‚ They are insufficient and ineffective in cases where the IVP is “stiff.”
where k1 “ hfpxi, yiq. On the other hand, k2 is approximated by the following Taylor series
approximation:
k2 “ hfpxi ` αh, yi ` βk1q
“ hfpxi, yiq ` h2α
Bf
Bxpxi, yiq ` h2β fpxi, yiq
Bf
By
pxi, yiq ` Oph3q (9.38)
Then substituting Eq. (9.38) into Eq. (9.35) yields
yi`1 “ yi ` w1hfpxi, yiq ` w2
`
hf ` h2α fx ` h2βffy
˘
pxi, yiq ` Oph3q
“ yi ` pw1 ` w2qhfpxi, yiq ` h2 pw2α fx ` w2βffyq pxi, yiq ` Oph3q (9.39)
Now equating the coefficients of like terms in Eq. (9.37) and Eq. (9.39) leads to:
hfpxi, yiq : w1 ` w2 “ 1,
h2fxpxi, yiq : αw2 “ 1
2
,
h2fyfpxi, yiq : βw2 “ 1
2
,
(9.40)
Equation (9.40) represents an underdetermined system–four unknowns and three equa￾tions. One of the parameters can be chosen arbitrarily; for instance, the system can be
solved in terms of w2. With the choice of w2 “ 1{2, Eq. (9.40) yields w1 “ 1{2 and
α “ β “ 1, the most common second-order RK-scheme. For w2 “ 3{4, the solution is
w1 “ 1{4, α “ β “ 2{3, which is known as the method of Ralston.
Higher-order Runge-Kutta schemes are derived in the same manner; however, the
derivation can get extremely complicated. Among the Runge-Kutta methods, the fourth￾order scheme (RK4) is the one that is most commonly used in the literature. The derivation
of RK4 uses the fourth-order Taylor series expansion, which results in 11 equations and 13
unknowns. The system of linear equations is solved by arbitrarily choosing two unknowns.
The most popular 2nd, 3rd, and 4th order Runge-Kutta schemes are presented below:
The second-order Runge-Kutta scheme
k1 “ h fpxi, yiq
k2 “ h f pxi`1, yi ` k1q
+
yi`1 “ yi `
1
2
`
k1 ` k2
˘
` Oph3q (9.41)
which is also known as the Improved Euler’s Method.ODEs: Initial Value Problems  519
The third-order Runge-Kutta scheme
k1 “ h fpxi, yiq
k2 “ h f´
xi`1{2, yi `
k1
2
¯
k3 “ h fpxi`1, yi ´ k1 ` 2k2q
,
//.
//-
yi`1 “ yi `
1
6
`
k1 ` 4k2 ` k3
˘
` Oph4q (9.42)
The fourth-order Runge-Kutta scheme
k1 “ h fpxi, yiq
k2 “ h f´
xi`1{2, yi `
k1
2
¯
k3 “ h f´
xi`1{2, yi `
k2
2
¯
k4 “ h fpxi`1, yi ` k3q
,
///////.
///////-
yi`1 “ yi `
1
6
`
k1 ` 2k2 ` 2k3 ` k4
˘
` Oph5q (9.43)
where xi`1{2 “ xi ` h{2, xi`1 “ xi ` h, and yi`1 is the current step estimate. Note that
the order of the local error is one degree higher than the global error.
Pseudocode 9.4
Module RUNGE_KUTTA (n, h, x0, y0, xlast)
\ DESCRIPTION: A pseudomodule to solve a first-order IVP on rx0, xlasts
\ with 2nd, 3rd, or 4th order Runge-Kutta method.
\ USES:
\ DRV_RK:: Module performing one-step Runge-Kutta scheme.
Write: “x, y=”,x0, y0 \ Print initial value
x Ð x0 \ Initialize x
While “
x ă xlast‰ \ Marching loop
DRV_RK(n, h, x0, y0, x, y) \ Apply one-step RK scheme
Write: “x, y=”,x, y \ Print current step
x0 Ð x; y0 Ð y \ Set current values as prior
End While
End Module RUNGE_KUTTA
Module DRV_RK (n, h, x0, y0, x, y)
\ DESCRIPTION: A driver Module employing one-step RK2, RK3, and RK4 for
\ n “ 2, 3, and 4, respectively. Implementation of RK4 only is presented.
\ USES:
\ FCN:: A user-defined function module supplying fpx, yq.
Declare: k4
xh Ð x0 ` h{2 \ Find xi`1{2
x1 Ð x0 ` h \ Find xi`1
If “
n “ 4
‰
Then \ Apply RK4 scheme
k1 Ð h ˚ FCNpx0, y0q
k2 Ð h ˚ FCNpxh, y0 ` k1{2q
k3 Ð h ˚ FCNpxh, y0 ` k2{2q
k4 Ð h ˚ FCNpx1, y0 ` k3q
ym Ð pk1 ` 2k2 ` 2k3 ` k4q{6
y Ð y0 ` ym \ Find current step estimate520  Numerical Methods for Scientists and Engineers
x Ð x1
Else
If “
n “ 3
‰
Then \ Apply RK3 scheme
\ You can nest other lower-order schemes
End If
End If
End Module DRV_RK
A pseudomodule, RUNGE_KUTTA, solving a first-order IVP with a nth-order Runge￾Kutta method is presented in Pseudocode 9.4. The module requires the order of the method
(n), a step size (h), an initial value (y0), and a solution interval [x0, xlast] as input. The
ODE (y1
) is supplied to the module with the user-defined external function, FCN. The
marching procedure is implemented within the While-construct until xlast is reached. The
driver module, DRV_RK, accompanying the RUNGE_KUTTA (main module), computes a
single-step RK estimate based on the order of scheme selected (i.e., n). The DRV_RK input
arguments are the same as those of RUNGE_KUTTA. The abscissa x (“ xi`1) and the
estimate for the current step y (“ yi`1) computed by Eq. (9.43) are the outputs of the
DRV_RK. In Pseudocode 9.4, however, the module is presented only for RK4 to save space
since RK2 and RK3 (cases of n “ 2 or 3) can be easily integrated into the module using
nested If-constructs. As in previously given codes, no array variables for x and y have been
used here, which is why, after each step, the current values are printed out and assigned as
the prior, px0, y0qÐpx, yq.
EXAMPLE 9.4: On the application of Runge-Kutta Methods
Solve Example 9.1 using 2nd, 3rd, and 4th order Runge-Kutta schemes with h “ 0.1,
and compare your estimates with the true solution.
SOLUTION:
For step size h “ 0.1, the first three steps of the fourth-order Runge-Kutta scheme
are given below.
First, using pt0, I0q“p0, 2q and Eq. (9.43), the ki’s of the 4th order Runge-Kutta
method is calculated as follows:
k1 “ hfpt0, I0q“p0.1qfp0, 2q“p0.1qp3q “ 0.3,
k2 “ hfpt1{2, I0 `
k1
2 q “ hfp0.05, 2.15q“p0.1qp2.42748q “ 0.242748,
k3 “ hfpt1{2, I0 `
k2
2 q “ hfp0.05, 2.12137q“p0.1qp2.499044q “ 0.249904,
k4 “ hfpt1, I0 ` k3q “ hfp0.10, 2.249904q“p0.1qp1.985074q “ 0.198507
Substituting the ki’s into Eq. (9.13) yields
Ip0.10q “ I1 “ I0 `
1
6 pk1 ` 2k2 ` 2k3 ` k4q
“ 2 `
1
6 p0.3 ` 2p0.242748q ` 2p0.249904q ` 0.198507q “ 2.247302
For the steps i “ 2, 3, and 4, the corresponding estimates for t2 “ 0.2, t3 “ 0.3, andODEs: Initial Value Problems  521
t4 “ 0.4 are found as
k1 “0.199158, k2 “0.155475, k3 “0.160935, k4 “0.121811, and I2 “2.406267
k1 “0.122303, k2 “0.089143, k3 “0.093288, k4 “0.063678, and I3 “2.498074
k1 “0.064048, k2 “0.039041, k3 “0.042167, k4 “0.019924, and I4 “2.539139
The estimates for the subsequent steps are easily and recursively computed.
TABLE 9.4
True Runge-Kutta Method
t Solution RK2 RK3 RK4
02 0 0
0.2 2.406288 6.65E-03 4.38E-04 2.17E-05
0.4 2.539164 7.79E-03 5.18E-04 2.56E-05
0.6 2.517013 6.75E-03 4.55E-04 2.23E-05
0.8 2.410610 5.09E-03 3.51E-04 1.70E-05
1 2.261953 3.47E-03 2.48E-04 1.19E-05
1.2 2.095672 2.15E-03 1.63E-04 7.59E-06
1.4 1.925946 1.14E-03 9.82E-05 4.38E-06
1.6 1.760685 4.35E-04 5.20E-05 2.09E-06
1.8 1.604061 4.15E-05 2.03E-05 5.32E-07
2 1.458042 3.45E-04 5.03E-07 4.75E-07
Discussion: In Table 9.4, the numerical estimates obtained by the 2nd, 3rd, and
4th-order Runge-Kutta methods, the true errors along with the true solution are
comparatively presented for selected points. It is observed that as the order of the
method increased, the numerical estimates in the solution interval improved. For
example, the average error of the solutions obtained with the RK3 scheme is about
15 times less than that of the RK2. Similarly, comparing the true errors incurred
with RK3 and RK4, the average error due to RK3 is about 20 times that of RK4.
Also notice that the fourth-order Runge-Kutta method with a relatively large step
size (h “ 0.1) yielded numerical estimates correct to at least four significant places.
9.3 NUMERICAL STABILITY
Up until now, we focused on increasing the accuracy of the numerical estimates by either
decreasing the step size or increasing the order of the numerical scheme. To solve an IVP, it
may be tempting to apply any of the numerical methods discussed so far with an arbitrary
step size without much thought. However, it should be noted that some numerical methods
may not always behave as they should. This is where the concept of numerical stability, the
most critical property of a numerical method, enters the picture.
In a numerical method, it is desirable to produce estimates within the range of the true
solution when the initial value of an IVP is perturbed. Even in the absence of round-off or
truncation errors, a numerical method rather than dampening the numerical errors could
magnify them to a point where the errors continue to grow unboundedly. Of course, there522  Numerical Methods for Scientists and Engineers
are many cases where the true solution grows unboundedly, but for now, we shall restrict
our discussion to the cases having bounded true solutions.
The numerical solution of an IVP with an unstable numerical method leads to un￾bounded growth with any choice of step size, whereas a stable numerical method does not
cause numerical instabilities or unbounded growth. On the other hand, the stability of a
method may depend on the choice of step size; such methods are referred to as conditionally
stable methods. In some cases, the step size required for stability may be too small, and
thus the cost of the calculations may become prohibitively large. It is therefore important
to determine the stability properties of a numerical method using stability analysis.
9.3.1 STABILITY OF EULER METHODS
Consider y1 “ λy with yp0q “ 1 (a real λ), for which the solution is ypxq “ eλx. It is clear
that the true solution is stable and bounded for λ ă 0, i.e., y Ñ 0 as x Ñ 8. Applying the
explicit Euler scheme to the IVP leads to
yi`1 “ yi ` h pλyiq“p1 ` λhq yi, i “ 0, 1, 2,... (9.44)
The numerical estimates for successive steps are found as follows:
yi “ p1 ` λhq yi´1 “ p1 ` λhq
2
yi´2 “ ... “ p1 ` λhq
i
y0 (9.45)
Here, the term 1 ` λh is called the amplification factor, and it will be denoted by ξ.
Equation (9.45) implies that the amplification factor must be bounded (|ξ| ă 1) in
order to prevent errors from growing. In other words, for |ξ| ě 1, numerical instabilities
will be observed beyond a certain (critical) step size, that is, solving |1 ` hλ| ď 1 gives
h ď ´2{λ, which is the upper bound for a stable solution. In the explicit Euler method, the
step size must be less than a critical step size to ensure “numerical stability,” and hence
the method is said to be conditionally stable. Besides being a first-order accurate method,
the conditional stability of the explicit method is its major handicap.
Employing the implicit Euler method to y1 “ λy yields
yi`1 “ yi ` λh yi`1 or yi`1 “ ξ yi or yi`1 “ ξi y0, i “ 0, 1, 2,...
where ξ “ 1{p1 ´ λhq. The condition of numerical stability (|ξ| ď 1) results in |1 ´ λh| ě 1,
which is satisfied for all λ ă 0.
Similarly, applying the trapezoidal rule to y1 “ λy, the stability condition for the am￾plification factor becomes ξ “ p2 ` λhq{p2 ´ λhq. Letting λ “ ´a ă 0 with a ą 0, the
stability analysis for the implicit Euler and Trapezoidal rules yields ξ “ 1{p1 ` ahq ă 1
and ξ “ p2 ´ ahq{p2 ` ahq ă 1, respectively. Consequently, the implicit Euler and Trape￾zoidal rules are unconditionally stable for λ ă 0. Moreover, linearized implicit Euler and
trapezoidal methods are also unconditionally stable. In summary, all implicit methods are
unconditionally stable, but the main drawback of these methods is their high computational
cost per step.
The selection of the step size is important because the numerical
method should be chosen such that it is not only accurate but
also stable. Even though linearized schemes are unconditionally
stable, in practice they may lead to some loss of total stability for
nonlinear IVPs.ODEs: Initial Value Problems  523
9.3.2 STABILITY OF RUNGE-KUTTA METHODS
Runge-Kutta methods are inherently conditionally stable because they are explicit schemes.
First, we investigate the stability range of the second-order Runge-Kutta method (RK2) by
revisiting the model IVP, y1 “ λy (for λ ă 0).
The difference equation of the model with RK2 can be expressed as
k1 “ λhyi, k2 “ λhpyi ` k1q “ λhp1 ` λhqyi
substituting into Eq. (9.41) and rearranging yields
yi`1 “ `
1 ` λh ` 1
2λ2h2˘
yi ` Oph3q
where 1 ` λh ` λ2h2{2 “ ξ is the amplification factor.
The numerical stability requires |ξ| ď 1, yielding the same (restriction) stability interval
as the explicit Euler method, i.e., λh ď ´2. For higher-order RK schemes, the stability
interval improves slightly, yielding λh ď ´2.513 and λh ď ´2.785 for the RK3 and RK4,
respectively. However, it should be noted that “numerical stability” does not necessarily
translate to “numerical accuracy!” A numerical method can be stable yet inaccurate. We
strive to obtain numerical estimates of an IVP as accurately as possible by reducing the
step size and/or using a high-order numerical scheme. But it should be kept in mind that
the computational cost of a numerical solution is dominated by the number of function
evaluations, which is directly proportional to the cpu-time. An analyst’s goal is to apply
the maximum allowable step size h to reach the final destination (x “ xlast), as a large
step size leads to a fewer function evaluations and lower cpu-time. Thus, the optimum step
size and cost-effectiveness of a numerical scheme depend not only on its accuracy but also
on stability considerations.
Recall that the RK4 requires four function evaluations per step, while the explicit
Euler method requires only one. Nevertheless, it provides better stability (λh ď 2.785) and
accuracy characteristics over the explicit Euler, RK2, and RK3 methods. These stability
and accuracy characteristics of the RK4 method make it one of the most popular schemes
for the numerical solution of IVPs.
9.3.3 STIFFNESS
In Section 9.3.2, the explicit Euler method was demonstrated to be conditionally stable
with a stability condition of h ď ´2{λ for the model IVP. This requirement results in an
extremely small step size for a large λ (|λ| " 1). This phenomenon observed in IVPs is
referred to as stiffness.
Stiffness depends basically on the IVP itself, the accuracy criterion, the length of the
solution interval, and the absolute stability interval of the applied method. There is no
precise definition of stiffness in the literature. All we know is that differential equations
containing widely varying time scales (i.e., eigenvalues) exhibit the stiffness phenomenon.
Some components of the solution decay much more rapidly than others. The accuracy
restriction depends mainly on the slowly varying component of the solution; however, the
rapidly varying component dictates the use of very small step sizes over the entire solution
interval to maintain stability. Even if we apply an adaptive step size procedure to overcome
this problem (i.e., by using a small step size for the interval where the solution changes
rapidly and larger steps elsewhere), the stability requirement may still force us to use a
small step size across the entire solution domain.524  Numerical Methods for Scientists and Engineers
EXAMPLE 9.5: Stability of the explicit Euler method
Under certain conditions, heat transfer analysis is idealized as a lumped system, in
which case the temperature of an object can be assumed to vary with time only. The
energy balance in a convection-dominated heat transfer process yields the following
IVP:
dΘ
dt “ ´a Θ, Θp0q “ 1
where Θptq“pTptq´T8q{pT0´T8q is the dimensionless temperature, T8 is the ambient
temperature, T0 and Tptq are respectively the initial and instantaneous temperatures
of the object, and a is a constant depending on the material and geometry of the
object as well as environmental conditions. Apply the explicit Euler method with
the step sizes h“0.40, and 0.45 to obtain numerical estimates up to t“8. Compare
your results with the true solution, given as
Θptq“e´at
SOLUTION:
This is the model IVP with λ “ ´a, whose true solution decays fairly quickly.
The numerical estimates on [0,8] with h “ 0.40, and h “ 0.45, as well as the true
solution, are comparatively depicted in Fig. 9.5. For h ą 0.5, the numerical estimates
(not shown here) oscillate widely at startup and quickly diverge with increasing t.
Recall that the stability analysis for the explicit Euler method dictates the choice of
step size to be h ď 0.5. Nevertheless, the numerical estimates for values of h “ 0.40,
and h “ 0.45 (i.e., for h Ñ 0.5) not only oscillated about the true solution at the
start but also the amplitude of the oscillations increased as h approached 0.5.
FIGURE 9.5
Discussions: Although a numerical solution, depending on the proximity of h to
0.5, initially oscillates more violently, it recovers and decays toward the true solution
as t increases. Nevertheless, the recovery period can become very large as h Ñ 0.5.
This example illustrates that to achieve the desired accuracy, the step size h may
need to be further restricted by staying far enough away from the boundary of the
stability region (i.e., in explicit Euler h – 0.5) so that the numerical solution does
not oscillate in cases where the method is stable for a specified h.ODEs: Initial Value Problems  525
EXAMPLE 9.6: Nonlinear problems with and without linearization
Consider the following third-order chemical reaction rate equation:
dC
dt “ ´5C3ptq, Cp0q “ 1
where Cptq is the concentration of the reactant as a function of time t. Solve the
reaction rate equation with and without linearization for t ď 1 with the explicit Euler,
implicit Euler, and trapezoidal rule using h “ Δt “ 0.1. Compare your numerical
estimates with the true solution given as Cptq“1{
?10t ` 1. Use ε “ 0.5 ˆ 10´6 for
convergence tolerance.
SOLUTION:
The problem is a first-order nonlinear IVP with Cp0q “ C0 “ 1 as the initial
condition and fpt, Cq“´5C3.
Using Eq. (9.6), the explicit Euler method leads to the following equation:
Ci`1 “ Ci ´ 5hC3
i “ p1 ´ 5hC2
i qCi, i “ 0, 1, 2,...
where ξ “ 1 ´ 5h C2
i is the amplification factor.
Since the stability requirement for the explicit method dictates that the step size
should satisfy |ξ| ď 1, we deduce h ď 2{5C2
i (or C2
i ď 2{5h). For h “ 0.1, the stability
restriction leads to a limitation on the numerical estimates at any given step, i.e.,
the stability condition is C2
i ď 4. Noting that Cptiq ď 1 for the true solution, it is
clear that the step size of h “ 0.1 will not violate the stability condition. Then the
current step estimates can be computed recursively since the rhs of the difference
equation contains the prior step estimates.
For the linearized difference equations, we need Bf{BC “ ´15C2. Using Eqs.
(9.17) and (9.21), the linearized implicit Euler and trapezoidal schemes, respectively,
yield the following difference equations:
Ci`1 “ Ci ´ 5hC3
i
1 ` 15hC2
i
“
ˆ1 ` 10hC2
i
1 ` 15hC2
i
˙
Ci
Ci`1 “ Ci ´ 10hC3
i
2 ` 15hC2
i
“
ˆ 2 ` 5hC2
i
2 ` 15hC2
i
˙
Ci
where the expressions in brackets on the rhs are the amplification factors. It is clear
that in both cases, the amplification factors are restrained against an increase in
hC2
i . Hence, both schemes are unconditionally stable.
The true solution, as well as the true (absolute) errors obtained from the solutions
using explicit Euler, linearized implicit Euler, and linearized trapezoidal schemes
for h “ 0.1, are comparatively presented in Table 9.5. In all three methods, the
errors are considerably large in the initial steps but tend to decrease steadily as t
increases. The explicit Euler and linearized implicit Euler methods yield errors of the
same magnitude since both methods are first-order accurate, i.e., Op0.1q. Notice that
the errors from the linearized trapezoidal method are smaller than those obtained
with the linearized implicit Euler method, which is an expected outcome since the
trapezoidal scheme is a second-order accurate method, i.e., Op0.12q.526  Numerical Methods for Scientists and Engineers
TABLE 9.5
Absolute error, |ei|
t True Explicit Linearized Linearized
Solution Euler Implicit Euler Trapezoidal
01 0 0 0
0.1 0.707107 0.20711 0.09289 0.00718
0.2 0.577350 0.13985 0.09204 0.00515
0.3 0.500000 0.10437 0.07970 0.00372
0.4 0.447214 0.08255 0.06773 0.00282
0.5 0.408248 0.06783 0.05785 0.00222
0.6 0.377964 0.05727 0.04995 0.00181
0.7 0.353553 0.04935 0.04362 0.00150
0.8 0.333333 0.04321 0.03851 0.00128
0.9 0.316228 0.03831 0.03432 0.00110
1 0.301511 0.03433 0.03085 0.00096
Employing the implicit Euler method (without linearization) leads to
Ci`1 “ Ci ` hfpti`1, Ci`1q “ Ci ´ 5hC3
i`1
which is a cubic equation for which Ci`1 is the unknown, and it needs to be solved at
every time step. Defining z “ Ci`1, the difference equation becomes z “ Ci ´ 5hz3.
Adapting the fixed-point iteration method to solve this, the FPI equation becomes
zpp`1q “ Ci ´ 5h
`
zppq
˘3
, p “ 0, 1, 2,...
where, as usual, the superscript “(p)” denotes the iteration step. To speed up the
convergence, the numerical estimate from the prior time step is simply chosen as the
initial guess for the current state, i.e., zp0q “ Ci.
TABLE 9.6
True Absolute error |ei|
t Solution Implicit Euler Trap. Rule
01 0 0
0.1 0.707107 0.06381 0.03351
0.2 0.577350 0.06255 0.02280
0.3 0.500000 0.05461 0.01636
0.4 0.447214 0.04703 0.01241
0.5 0.408248 0.04074 0.00981
0.6 0.377964 0.03564 0.00800
0.7 0.353553 0.03151 0.00668
0.8 0.333333 0.02812 0.00569
0.9 0.316228 0.02530 0.00492
1 0.301511 0.02294 0.00431ODEs: Initial Value Problems  527
The nonlinear algebraic equation resulting from applying the trapezoidal rule is
similarly obtained, yielding
zpp`1q ´ Ci `
5h
2
´
C3
i ` `
zpp`1q
˘3
¯
“ 0, p “ 0, 1, 2, ..
In Table 9.6, the true solution as well as the true (absolute) errors obtained with
the implicit Euler method and the trapezoidal rule are comparatively tabulated for
h “ 0.1. The errors due to the trapezoidal rule are about half of the implicit Euler
method in the initial steps of the marching. However, as t increases, the implicit
method, due to being a first-order scheme, yields errors about 4-5 times larger than
those obtained from the trapezoidal rule.
Discussion: The fact that the true solution satisfies the stability condition for all
t made it possible to obtain approximate solutions with the explicit method. How￾ever, in most nonlinear problems, the approximate solution with explicit schemes
may reach magnitudes that violate the stability condition for t ě tc. In cases where
we do not have a general idea about the solution trend, the most reliable way is to
apply an implicit scheme via a root-finding method. Nonetheless, solving a nonlin￾ear equation in this manner aside from being restrictive in terms of cpu-time, the
linearized schemes offer an alternative economical solution.
9.4 MULTISTEP METHODS
One-step methods covered in Section 9.2 are also referred to as self-starting methods in
that the marching process can start with the initial value (Fig. 9.6a). In the steps that
follow, only the prior step estimate yi is used to compute the current step estimate yi`1.
However, after a few steps, several prior estimates, yi, yi´1, yi´2, ..., and their derivatives,
y1
i, y1
i´1, y1
i´2, ... (i.e., fi, fi´1, fi´2, ...), become available. As depicted in Fig. 9.6b, the
prior estimates can provide considerable information regarding the curvature of the true
solution, which can then be utilized to extrapolate the estimate for the current step.
Methods making use of more than one prior step estimates are referred to as multistep
methods. The main idea behind the multistep methods is to make use of y and y1 from the
prior steps to construct a high-degree interpolating polynomial to estimate the current step.
This means that multistep methods are not self-starting; they require one or more prior step
estimates of y and y1 besides the initial value. The higher the degree of the interpolating
polynomial, the more points with known prior step estimates are required.
A linear n-step scheme is generalized as
yi`1 “ α1yi ` α2yi´1 ` ... ` αnyi´n`1 ` h tβ0fi`1 ` β1fi ` ... ` βnfi´n`1u (9.46)
where fi “ fpxi, yiq denotes the first derivative at pxi, yiq, and αk and βk are the coefficients
of the interpolating polynomial. The scheme is explicit for β0 “ 0, implicit otherwise. The
implicit formulas are generally more accurate and stable than the explicit ones. However,
as we have seen in one-step implicit methods, they require a root-finding algorithm to
obtain the current step estimate. Adams-Bashforth (explicit) and Adams-Moulton (implicit)
formulas are two of the most popular families of multistep methods.528  Numerical Methods for Scientists and Engineers
FIGURE 9.6: Marching strategy for (a) one-step, (b) multistep schemes.
9.4.1 ADAMS-BASHFORTH METHOD (AB)
Consider the first-order model IVP, y1 “ fpx, yq, with n known prior estimates, pxi, yiq,
pxi´1, yi´1q,...,pxi´n`1, yi´n`1q, computed by a one-step method of the same order or
better. Integrating Eq. (9.4) between xi and xi`1 points gives
xż
i`1
xi
dy “ yi`1 ´ yi “
xż
i`1
xi
f
`
u, ypuq
˘
du (9.47)
To begin, the integrand is approximated by an pn´1qth-degree polynomial passing through
n uniformly spaced points: pxi, yiq, pxi´1, yi´1q, ..., pxi´n`1, yi´n`1q. A Lagrange interpo￾lating polynomial for the integrand is constructed as follows:
fpx, ypxqq “
n
ÿ´1
k“0
Lkpxqf
`
xi´k, ypxi´kq
˘
` enpxq (9.48)
where Lkpxq denotes the Lagrange polynomials, defined as
Lkpxq “
n
ź´1
j“0
j‰k
ˆ x ´ xi´j
xi´k ´ xi´j
˙
, k“0, 1, 2,...,pn´1q
and enpxq is the interpolation error given by
enpxq “ fpnq
px, ¯ξq
n!
n
ź´1
j“0
px ´ xi´j q, xi ă ¯ξ ă xi`1
Substituting Eq. (9.48) into Eq. (9.47) and integrating yields so-called Adams-Bashforth
Formulas. Marching with uniformly spaced steps is preferred mainly due to the ease with
which the interpolating polynomials can be constructed, resulting in less computational
effort and cpu-time.
Adams-Bashforth formulas, using Eq. (9.46), are expressed as
yi`1 “ yi ` h ÿn
k“1
βkfi´k`1 ` Ophn`1q (9.49)
where βk’s are the coefficients for Adams-Bashforth formulas, which are tabulated in Table
9.7 along with the local truncation error terms for orders n “ 1 through 6.ODEs: Initial Value Problems  529
TABLE 9.7: The coefficients (βk) and the local truncation errors of n-step
Adams-Bashforth Formulas (n “ 1,..., 6).
n β1 β2 β3 β4 β5 β6 Ophn`1q
1 1 1
2
h2f1
pξq
2 3
2 ´1
2
5
12
h3f 2pξq
3 23
12 ´16
12
5
12
9
24h4f 3pξq
4 55
24 ´59
24
37
24 ´ 9
24
251
720h5fp4qpξq
5 1901
720 ´2774
720
2616
720 ´1274
720
251
720
475
1440h6fp5qpξq
6 4277
1440 ´7923
1440
9982
1440 ´7298
1440
2877
1440 ´ 475
1440
19087
60480h7fp6qpξq
Figure 9.7 depicts the use of prior points in n “ 2-, 3-, and 4-step schemes using
the Adams-Bashforth Method (ABn). Each step requires the computation of the current
estimate from the known prior estimates of yi and fi, fi´1, ..., fi´n`1.
The pseudomodule, ADAMS_BASHFORTH, solving a first-order IVP with the AB4
scheme is presented in Pseudocode 9.5. In the generalized form, the module requires the
order of method (n), step size (h), initial value (y0), and the solution interval [x0, xlast] as
input. Even though the input argument list includes n, only AB4 is incorporated into the
module to save space, but it can be easily extended to include AB2 and AB3. As usual, the
derivative should be supplied with a user-defined function module, FCN.
Using Table 9.8, the coefficients of the interpolation polynomial are assigned according
to the selected n. (Should the module implement only AB4, the argument n can be omitted
from the input list and, correspondingly, the If-construct.) The initial derivative is y1
n “
fpx0, y0q, and the starting values y1
n´1, y1
n´2, ..., y1
1 are obtained using DRV_RK (i.e., the
4th-order Runge-Kutta scheme; see Pseudocode 9.4). Using Eq. (9.49) in a marching loop
(While-loop), the current step estimates are obtained step by step until xlast is reached.
Once a current estimate is found, the starting values (i.e., prior estimates) are shifted one
FIGURE 9.7: Estimates used in 2nd, 3rd, and 4th order in explicit multistep schemes.530  Numerical Methods for Scientists and Engineers
Adams-Bashforth Method
‚ ABs require only one derivative evaluation per step, as opposed to
four evaluations per step in RK methods, making AB schemes con￾siderably faster;
‚ ABn gives the true solution for the IVPs, whose (true) solution is at
most an pn ` 1qth-degree polynomial.
‚ The AB schemes require multiple prior step estimates (starting val￾ues) in addition to the initial value to advance forward;
‚ Starting values are obtained with a one-step scheme, which should
have the same or better (higher) order of error as the AB scheme
used;
‚ With an increasing order of AB, accuracy increases while the stability
region shrinks;
‚ AB formulas based on non-uniformly spaced interpolation points are
much more complicated and cpu-time demanding.
step up to accommodate the marching step: fk Ð fk´1 for k“n, n´1, ..., 2. This way, at any
step, only the required number of prior estimates are preserved in memory. Before advancing
to the next step, the current step abscissa and corresponding estimate are assigned as prior
values (y0 Ð y and x0 Ð x) before returning to the beginning of the While-loop.
Pseudocode 9.5
Module ADAMS_BASHFORTH (n, h, x0, y0, xlast)
\ DESCRIPTION: A pseudomodule program to solve first-order IVP on rx0, xlasts
\ using AB4 formulas. Starting values are computed using RK4.
\ USES:
\ FCN:: User-Defined Function Module supplying fpx, yq “ y1
;
\ DRV_RK:: Module performing one-step RK scheme (Pseudocode 9.4).
Declare: f4, β4
\ The coefficients of AB2, AB3, etc., can be nested below:
If “
n “ 4
‰
Then \ Set coefficients of AB4
β1 Ð 55{24; β2 Ð ´59{24; β3 Ð 37{24; β4 Ð ´9{24
End If
fn Ð FCNpx0, y0q \ Set initial derivative, fn “ fpx0, y0q
Write: “x, y=”,x0, y0 \ Print initial values
For “
k “ n ´ 1, 1,p´1q
‰ \ Find starting values
DRV_RK(4, h, x0, y0, x, y) \ Apply RK4 to find a starting value
Write: “x, y=”,x, y \ Print starting values
fk Ð FCNpx, yq \ Find derivative from FCNpx, yq
x0 Ð x; y0 Ð y \ Assign current step values as prior
End For
While “
x ă xlast‰ \ Marching loop
sums Ð 0 \ Initialize accumulator sums
For “
k “ 1, n‰ \ Apply nth-order AB polynomial
sums Ð sums ` βk ˚ fkODEs: Initial Value Problems  531
TABLE 9.8: The coefficients (βk) and the local truncation errors of n-step
Adams-Moulton formulas (n “ 1,..., 6).
n β0 β1 β2 β3 β4 β5 Ophn`1q
1 1 ´1
2
h2f1
pξq
2 1
2
1
2 ´ 1
12
h3f 2pξq
3 5
12
8
12 ´ 1
12 ´ 1
24h4f 3pξq
4 9
24
19
24 ´ 5
24
1
24 ´ 19
720h5fp4qpξq
5 251
720
646
720 ´264
720
106
720 ´ 19
720 ´ 27
1440h6fp5qpξq
6 475
1440
1427
1440 ´ 798
1440
482
1440 ´ 173
1440
27
1440 ´ 863
60480h7fp6qpξq
End For
y Ð y0 ` h ˚ sums \ Find current step estimate
x Ð x0 ` h \ Find current step abscissa
For “
k “ n, 2,p´1q
‰ \ Shift derivatives up one step
fk Ð fk´1
End For
f1 Ð FCNpx, yq \ Find current step derivative
Write: “x, y=”,x, y \ Print current step estimate
x0 Ð x; y0 Ð y \ Set current step values as prior
End While
End Module ADAMS_BASHFORTH
9.4.2 ADAMS-MOULTON METHOD
In this method, the Adams-Moulton Formulas (AMs) are derived in the same manner as the
AB, with only one exception. The current step estimate yi`1 is also used in constructing the
interpolating polynomial in addition to the prior step estimates generated from uniformly
spaced points pxi, yiq, pxi´1, yi´1q, ..., pxi´n`2, yi´n`2q. Hence, the AMn formulas are
implicit by nature.
Lagrange interpolating polynomials in this case take the following form:
fpx, ypxqq “
n
ÿ´1
k“0
Lkpxqf
`
xi´k`1, ypxi´k`1q
˘
` enpxq (9.50)
where Lkpxq and enpxq, in this case, are defined as
Lkpxq “
n
ź´1
j“0
j‰k
´ x ´ xi´j`1
xi´k`1 ´ xi´j`1
¯
, k “ 0, 1, 2,...,pn ´ 1q
enpxq “ fpnq
px, ¯ξq
n!
n
ź´1
j“0
px ´ xi´j`1q, xi ă ¯ξ ă xi`1532  Numerical Methods for Scientists and Engineers
Substituting Eq. (9.50) into Eq. (9.47) and integrating yields Adam-Moulton Formulas,
which are generalized as
yi`1 “ yi ` h
n
ÿ´1
k“0
βkfi´k`1 ` Ophn`1q, (9.51)
where the coefficients (βk’s) and the local truncation errors are presented for n “ 1 through
6 in Table 9.8. Note that for n “ 1 the AM1 corresponds to the implicit Euler scheme. The
AMs are also not self-starting; that is, a high-order one-step method is used to generate
the required number of initial values to proceed forward marching. An AM can only be
implemented once a sufficient number of starting values become available. Since AM is
implicit, an iterative procedure similar to that described for the implicit Euler method can
be applied (see Example 9.3).
Adams-Moulton Method
‚ AMs are implicit schemes;
‚ High-order AM formulas are available (see Table 9.8);
‚ AM schemes are more accurate than AB schemes of the same order
due to smaller truncation error constants (see Tables 9.7 and 9.8);
‚ AMs have larger stability regions than ABs of the same order;
‚ To minimize the cpu-time, function evaluations can be reduced to a
single function evaluation per step, as implemented in Pseudocode
9.6, by shifting (copying) the estimates from prior solutions.
‚ AMs require a high-order single-step scheme (such as RK4) to gen￾erate a sufficient number of starting values to start AM marching;
‚ AMs require larger memory allocation due to the need to save the
required number of prior estimates;
‚ Cpu time increases due to iterative algorithms applied in cases where
the use of a root-finding algorithm is required to find the root (current
step estimate) of the nonlinear equation;
‚ The convergence of the root-finding methods depends on fpx, yq.
A pseudomodule, ADAMS_MOULTON, solving a first-order IVP with an AM2, AM3, or
AM4 is presented in Pseudocode 9.6. The module requires the order of the method (n), a step
size (h), a convergence tolerance (ε), an initial value (y0), and a solution interval [x0, xlast]
as input. The derivative, fpx, yq, is supplied by the user-defined function, FCN(x, y). The
required number of starting values is generated using DRV_RK (4th-order RK driver module,
see Pseudocode 9.4). The accuracy of the starting values is critical to the accuracy of the
estimates obtained in subsequent marching steps. Thus, it is recommended to find the
starting values with RK4, even if the AM2 or AM3 schemes are used. Before applying
the procedure, the coefficients of the interpolating polynomial given in Table 9.8 are set
according to the order of the AM scheme. In the module, the scheme is fixed by AM4;
however, it can be easily modified or extended to include AM2 and AM3. The marching
procedure is carried out within the While-loop until xlast is reached. The current step
estimates of AM4 are obtained step by step using Eq. (9.51) along with the fixed point
iteration method performed in the Repeat-Until-loop. Once the current step estimate y (i.e.,
yi`1) is found, the starting values are shifted one step up in preparation for the next step.
To advance to the next step, the current step estimates are also assigned to the prior step
estimates, x0 Ð x; y0 Ð y, before returning to the top of the While-loop.ODEs: Initial Value Problems  533
Pseudocode 9.6
Module ADAMS_MOULTON (n, h, ε, x0, y0, xlast)
\ DESCRIPTION: A pseudomodule to solve a first-order IVP on rx0, xlasts with
\ AM4. FPI method is used to solve the resulting nonlinear equations.
\ USES:
\ FCN:: A user-defined function module supplying fpx, yq “ y1
;
\ DRV_RK:: Module performing one-step RK (Pseudocode 9.4);
\ ABS :: A built-in function computing the absolute value.
Declare: f4, β4 \ β1 Ð β0 beta’s are shifted
\ The coefficients of AB2, AB3, etc., can be nested below:
If “
n “ 4
‰
Then \ Set coefficients of AB4
β1 Ð 9{24; β2 Ð 19{24; β3 Ð ´5{24; β4 Ð 1{24
End If
fn Ð FCNpx0, y0q \ Find initial derivative
Write: “x, y “”,x0, y0 \ Print initial values
For “
k “ n ´ 1, 2,p´1q
‰ \ Find starting values
DRV_RK(4, h, x0, y0, x, y) \ Apply RK4 to find a starting value
Write: “x, y=”,x, y \ Print starting values
fk Ð FCNpx, yq \ Find derivative from FCNpx, yq
x0 Ð x; y0 Ð y \ Set current values as prior
End For
While “
x ă xlast‰ \ Marching Loop
x Ð x0 ` h \ Find current step abscissa
z Ð y0 \ Set initial guess for FPI
y Ð z \ Assign current estimate as prior
Repeat \ Solve nonlinear equation for yi`1
f1 Ð FCNpx, zq \ Find y1
pxq
sums Ð 0 \ Initialize sums
For “
k “ 1, n‰ \ Apply nth-order AB polynomial
sums Ð sums ` βk ˚ fk
End For
y Ð y0 ` h ˚ sums \ Find current step estimate
errz Ð |y ´ z| \ Find absolute error
z Ð y \ Assign current step estimate as prior
Until “
errz ă eps‰ \ Converged if |y ´ zppq
| ă ε
For “
k “ n, 2, ´1
‰ \ Shift derivatives up one step
fk Ð fk´1
End For
f1 Ð FCNpx, yq \ Find y1
pxq for current step
Write: “x, y=”,x, y \ Print current estimate
x0 Ð x; y0 Ð y \ Assign current values as prior
End While
End Module ADAMS_MOULTON
9.4.3 BACKWARD DIFFERENTIATION FORMULAS
Backward differentiation formulas (BDFs), introduced by Curtiss and Hirschfelder [6], are
a class of linear multistep (implicit) formulas for numerically solving IVPs. The BDFs have
become very popular in the last few decades, primarily due to their ability to handle the534  Numerical Methods for Scientists and Engineers
TABLE 9.9: The coefficients and the local truncation errors for BDFs (n “ 1,..., 6).
n β0 α1 α2 α3 α4 α5 α6 Ophn`1q
11 1 ´1
2
h2f1
pξq
2 2
3
4
3 ´1
3 ´1
3
h3f 2pξq
3 6
11
18
11 ´ 9
11
2
11 ´1
4
h4f 3pξq
4 12
25
48
25 ´36
25
16
25 ´ 3
25 ´1
5
h5fp4qpξq
5 60
137
300
137 ´300
137
200
137 ´ 75
137
12
137 ´1
6
h6fp5qpξq
6 60
147
360
147 ´450
147
400
147 ´225
147
72
147 ´ 10
147 ´1
7
h7fp6qpξq
numerical solution of stiff IVPs. The BDFs also have better stability properties than those
of the ABs and AMs.
Constructing a BDF is quite simple and straightforward. The model first-order IVP,
Eq. (9.1), is discretized at x “ xi`1 as follows:
y1
pxi`1q “ fpxi`1, yi`1q (9.52)
Afterward, y1
pxi`1q is replaced with the backward difference formula of the desired order.
Recalling the differentiations with finite differences, the backward difference formulas, along
with the truncation errors, from the first derivative up to the fourth-order (see Chapter 5)
are presented below:
y1
pxi`1q “
$
’’’’’’’’’’&
’’’’’’’’’’%
yi`1 ´ yi
h `
h
2
y2pξq
3yi`1 ´ 4yi ` yi´1
2h `
h2
3 y3pξq
11yi`1 ´ 18yi ` 9yi´1 ´ 2yi´2
6h `
h3
4 yp4q
pξq
25yi`1 ´ 48yi ` 36yi´1 ´ 16yi´2 ` 3yi´3
12h `
h4
5 yp5q
pξq
(9.53)
The BDF1 gives the implicit Euler method. Substituting Eq. (9.53) into Eq. (9.52) and
solving for yi`1, the second-, third-, and fourth-order (BDF2, BDF3, and BDF4) formulas
are obtained as follows:
yi`1 “ 1
3 p´yi´1 ` 4yiq ` 2h
3 fpxi`1, yi`1q ` Oph3q
yi`1 “ 1
11 p2yi´2 ´ 9yi´1 ` 18yiq ` 6h
11 fpxi`1, yi`1q ` Oph4q
yi`1 “ 1
25 p´3yi´3 ` 16yi´2 ´ 36yi´1 ` 48yiq ` 12h
25 fpxi`1, yi`1q ` Oph5q
(9.54)
In general, the BDFn’s can be expressed as follows:
yi`1 “ ÿn
k“1
αkyi`k´1 ` β0hfpxi`1, yi`1q ` Ophn`1q (9.55)ODEs: Initial Value Problems  535
Backward Differentiation Formulas
‚ A BDF scheme of any order can be constructed very easily;
‚ BDFs of orders 2 to 6 are conditionally stable;
‚ BDFs are suitable for solving stiff IVPs;
‚ BDFs require the use of a root-finding algorithm to find the current
estimate when the IVP is a nonlinear equation, which leads to in￾creased cpu-time;
‚ BDFs need starting values (yi´1, yi´2, yi´3, ...), which should be
computed by another method with the same or better order of error
before the marching procedure can be started;
‚ BDFs for n ě 7 are not convergent due to their rigidity, resulting
from overfitting of the data;
‚ Difficulties may arise if a fixed step size is adopted throughout the
solution interval; thus, the step size may need to be refined.
where β0 and αk’s are the corresponding coefficients presented in Table 9.9. Note that
among the starting values under the summation sign in Eq. (9.55), only y0 is known. As
with the ABs and AMs, the rest of the starting values need to be generated with a one-step
scheme before applying the BDFs. Since the BDFs are implicit, the current step estimate
yi`1 is obtained by applying a suitable root-finding scheme when the IVP is nonlinear.
In Pseudocode 9.7, the pseudomodule, BDFS, is presented for solving a first-order IVP
with an nth-order BDF. The module requires the order of the method (n), step size (h),
initial value (y0), the solution interval [x0, xlast], a convergence tolerance (ε) for the solution
of non-linear equations, and the maximum number of FPIs allowed (maxit) as input. The
IVP y1 “ fpx, yq is supplied to the module with the user-defined external function FCN.
Although the order of BDF (n) is present in the argument list, this module involves the
solution only for BDF3 to save space, but it can be easily modified and extended to include
other BDFn formulas.
The starting values for high-order BDFs are generated using the DRV_RK (RK4 of the
module given in Pseudocode 9.4). The starting values as well as the numerical estimates
are saved on a temporary array: ˆyn Ð y0, ˆyn´1 Ð ypx0 ` hq, ..., ˆy1 Ð ypx0 ` nhq. The
more accurate the starting values, the better the accuracy of the current step estimates.
Thus, the RK4 should be employed here regardless of the order of the BDF. For x ă xlast,
a While-loop is used to perform forward marching with uniform steps. The current estimate
at each step, Eq. (9.55), is obtained with a Repeat-Until-loop (a root-finding loop) using
the fixed-point iteration method. Once the summation term of Eq. (9.55) is evaluated with
a While-loop, the procedure is carried out within a Repeat-Until construct. If the iteration
procedure reaches the maximum number of iterations allowed, the procedure is terminated
with a warning. When the root of the nonlinear equation (current estimate) is successfully
obtained, then, as in the cases of AM and AB, the starting values are shifted one step up
in preparation for the next step. To advance up another step, the current step estimates
are assigned as prior step estimates, px0, y0qÐpx, yq, before returning to the top of the
marching loop.536  Numerical Methods for Scientists and Engineers
Pseudocode 9.7
Module BDFS (n, h, ε, x0, y0, xlast, maxit)
\ DESCRIPTION: A pseudomodule to solve y1 “ fpx, yq on [x0, xlast] with
\ the BDF3. Fixed-Point iteration is used for solving nonlinear equation.
\ USES:
\ FCN:: A user-defined function module supplying fpx, yq “ y1
.
\ DRV_RK:: Module performing one-step Runge-Kutta scheme;
\ ABS :: A built-in function computing the absolute value.
Declare: yˆn`1, αn \ Declare array variables
If “
n “ 3
‰
Then \ Set coefficients of BDF3
α1 Ð 18{11; α2 Ð ´9{11; α3 Ð 2{11; β Ð 6{11
End If
Write: “x, y=”,x0, y0 \ Print a starting value
yˆn Ð y0 \ Set initial value
For “
k “ n ´ 1, 1,p´1q
‰ \ Find starting values
DRV_RK(4, h, x0, y0, x, y) \ Apply RK4 to find a starting value
Write: “x, y=”,x, y \ Print starting value
yˆk Ð y \ Add the estimate as starting value
x0 Ð x; y0 Ð y \ Assign current step values as prior
End For
While “
x ă xlast‰ \ Marching loop
sums Ð 0 \ Initialize accumulator sums
For “
k “ 1, n‰ \ Apply BDFn
sums Ð sums ` αk ˚ yˆk
End For
x Ð x0 ` h \ Find current step abscissa
z Ð y0 \ Set prior step estimate as initial guess for FPI
p Ð 0 \ Initialize iteration counter
Repeat \ Solve nonlinear eqs using FPIs
p Ð p ` 1 \ Count iterations
y Ð sums ` β ˚ h ˚ FCNpx, zq \ Find current iteration step estimate
Err Ð |y ´ z| \ Find absolute error
z Ð y \ Assign current estimate to prior
Until “
Err ă ε Or p “ maxit‰ \ Check for convergence
If “
p “ maxit‰
Then \ Print a warning message and error
Write: “Max. no of iteration reached for x=”,x
Write: “Absolute Error is =”,Err
Exit
End If
For “
k “ n, 2,p´1q
‰ \ Shift the estimates up one step
yˆk “ yˆk´1
End For
yˆ1 “ y
Write: “x, y=”,x, y \ Print current estimate
x0 Ð x; y0 Ð y \ Assign current step values as prior
End While
End Module BDFSODEs: Initial Value Problems  537
EXAMPLE 9.7: Comparison of AB, AM, and BDF methods
Repeat Example 9.1 using the 2nd, 3rd, and 4th-order Adams-Bashforth (AB),
Adams-Moulton (AM), and backward differentiation formulas (BDF) with h “ 0.1.
Compare your estimates with the true solution.
SOLUTION:
In this example, only the numerical solutions for a few steps of AB2 and AM2 are
illustrated explicitly, but the numerical results of higher-order schemes are presented
as tabular data.
The initial condition and the corresponding derivative are Ip0q “ I0 “ 2 and I1
0 “
f0 “ fp0, 2q “ 3. Using Eq. (9.49), together with the coefficients of the interpolating
polynomial from Table 9.7, the difference equation for the AB2 is expressed as
Ii`1 “ Ii `
h
2 p3fi ´ fi´1q, for i ě 1
Note that I1 and f1 “ I1
1 “ fpt1, I1q are unknown, which should be generated in
order to be able to start the AB marching process. Normally, a suitable RK method
(of the same or higher order of error) is used to obtain the starting value; however,
since it is available, we will use the true solution here. This way, errors incurred
during the computation of the starting values with the one-step method will not be
carried to the AB, AM, or BDF marching steps, which will allow these methods to
be compared based on the true errors.
For t1 “ 0.1, we find
I1 “ Ip0.1q “ 4e´0.05 ´ 2e´0.25 “ 2.247316
f1 “ fpt1, I1q “ fp0.1, 2.247316) “ 1.991545
Now that the starting values (f0 and f1) are available, we can move on to imple￾menting the AB2 scheme. The current estimate for the first step (i “ 1) becomes:
I2 “ I1 `
h
2 p3f1 ´ f0q“2.247316 `
0.1
2
`
3p1.991545q´p3q
˘
“2.396048
Before advancing to the next step, the starting value is computed as
f2 “ fpt2, I2q“fp0.2, 2.396048q“1.248579
Then the current estimate (i “ 2) is obtained from
I3 “I2`
h
2
p3f2´f1q“2.396048`
0.1
2
`
3p1.248579q´1.991545˘
“2.483758
Similarly, for i “ 3, we find f3 “ fpt3, I3q “ fp0.3, 2.483758q “ 0.676269 and
I4 “I3`
h
2 p3f3´f2q“2.483758`
0.1
2
`
3p0.676269q ´ 1.248579˘
“ 2.522769
This recursive procedure is repeated up to t “ 2. The absolute errors of the
estimates obtained with the 2nd, 3rd, and 4th-order ABs for the selected points
are comparatively presented in Table 9.10 together with the true solutions. As the538  Numerical Methods for Scientists and Engineers
circuit current rises to a local maximum of 2.545 A at t « 0.46 s, all AM’s exhibit an
increasing trend up to t « 0.5 s, though the true errors are initially small. However,
as t continues to increase after the circuit current reaches its maximum, the errors
in the estimates decrease with the circuit current.
Using the set of coefficients from Table 9.8, the AM2 scheme can be written as
Ii`1 “ Ii `
h
2 pfi`1 ` fiq, i ě 0
TABLE 9.10
True True absolute error, ei
ti Solution AM2 AM3 AM4
0 2
0.2 2.406288 1.02E-02 0 0
0.4 2.539164 1.64E-02 2.67E-03 4.22E-04
0.6 2.517013 1.62E-02 3.37E-03 7.33E-04
0.8 2.410610 1.36E-02 3.10E-03 7.32E-04
1 2.261953 1.05E-02 2.51E-03 6.16E-04
1.2 2.095672 7.72E-03 1.90E-03 4.79E-04
1.4 1.925946 5.44E-03 1.38E-03 3.56E-04
1.6 1.760685 3.72E-03 9.70E-04 2.56E-04
1.8 1.604061 2.47E-03 6.67E-04 1.80E-04
2 1.458042 1.58E-03 4.50E-04 1.25E-04
For i “ 0, we already have I0 “ 2 and f0 “ 3 from the initial value. The second
starting value, f1 “fpt1, I1q, comprises the unknown current step estimate, I1. In this
example, y“fpt, Iq is linear, which eliminates the need for a root-finding algorithm
to find Ii`1. Hence, an explicit expression for Ii`1 can easily be found as
Ii`1 “ 4 ´ 5h
4 ` 5hIi `
16h
4 ` 5h p1 ` e´h{2qe´ti{2, i ě 0
TABLE 9.11
True True absolute error, ei
ti Solution AM2 AM3 AM4
0 2
0.2 2.406288 3.12E-03 2.22E-04 0
0.4 2.539164 3.76E-03 3.95E-04 4.67E-05
0.6 2.517013 3.40E-03 3.98E-04 5.64E-05
0.8 2.410610 2.72E-03 3.37E-04 5.13E-05
1 2.261953 2.03E-03 2.62E-04 4.14E-05
1.2 2.095672 1.45E-03 1.93E-04 3.14E-05
1.4 1.925946 1.00E-03 1.38E-04 2.28E-05
1.6 1.760685 6.73E-04 9.60E-05 1.61E-05
1.8 1.604061 4.37E-04 6.54E-05 1.12E-05
2 1.458042 2.74E-04 4.39E-05 7.60E-06ODEs: Initial Value Problems  539
The absolute true errors resulting from using the 2nd, 3rd, and 4th-order AMs
and the true solutions for selected points are presented comparatively in Table 9.11.
For increasing t, the numerical estimates with the AM show significant improvement
as the errors decay rapidly. Similar to the AMs, the numerical estimates initially
have small absolute errors, but as a result of the accumulation of global errors, the
errors increase up to t «0.46, where they reach their maximum and then decrease
with increasing t.
TABLE 9.12
True True absolute error, ei
ti Solution BDF2 BDF3 BDF4
0 2
0.2 2.406288 4.28E-03 0 0
0.4 2.539164 1.07E-02 1.33E-03 9.43E-05
0.6 2.517013 1.21E-02 2.08E-03 3.39E-04
0.8 2.410610 1.07E-02 1.99E-03 3.70E-04
1 2.261953 8.47E-03 1.62E-03 3.15E-04
1.2 2.095672 6.26E-03 1.24E-03 2.53E-04
1.4 1.925946 4.43E-03 9.05E-04 1.89E-04
1.6 1.760685 3.02E-03 6.41E-04 1.36E-04
1.8 1.604061 1.99E-03 4.43E-04 9.60E-05
2 1.458042 1.27E-03 3.00E-04 6.61E-05
In this example, the BDFs also yield explicit difference equations for Ii`1. Using
the coefficients from Table 9.9, the difference equation for the BDF2 is expressed as
Ii`1 “ 1
3 ` 5h
´
4Ii ´ Ii´1 ` 16he´ti`1{2
¯
, i ě 1
This recursive procedure is repeated until t “ 2 is reached. In Table 9.12, the true
solution, along with the true errors, are comparatively depicted for 2nd, 3rd, and
4th-order BDFs for selected points. In this method, the distribution of the errors with
each method follows a similar trend as in AB and AM. That is, the errors increase up
until the circuit current reaches its maximum at t “ 0.46 and then decrease rapidly
with increasing t.
Discussion: The AMs, ABs, and BDFs are considerably faster than the RK meth￾ods of the same order due to requiring only one function evaluation once past the
computation of the starting values. Notice that, for n ą 2, the numerical solutions
with AMs are significantly better than those of the same order as ABs and BDFs.
The errors for the BDFs are smaller than those for the ABs and slightly worse than
those for the AMs. This outcome is not surprising when the local truncation errors
of the ABs, AMs, and BDFs are examined (see Tables 9.7, 9.8, and 9.9).
The main shortcoming of the AMs and BDFs is the necessity of solving the result￾ing nonlinear algebraic equations at each step when the IVP is nonlinear. The BDFs
also suffer from the accumulation of global errors, which may grow with increasing
t or die out with converging IVPs.540  Numerical Methods for Scientists and Engineers
9.5 ADAPTIVE STEP-SIZE CONTROL
In all the methods discussed so far, the step size has been kept uniform throughout the
solution range. Also, we have already learned that the choice of the step size strongly
affects the accuracy of any numerical solution. For a large step size, the numerical solution
tends to deviate from the true solution as x increases, while a step size that is too small
leads not only to an increase in cpu-time but also to the accumulation of round-off errors.
Furthermore, when the numerical simulation interval rx0, x0 ` Ts is very large, it is not
uncommon to have a true solution that varies slowly in some subintervals and rapidly in
others. Large step sizes can be used where the solution varies slowly, primarily to reduce
cpu-time without sacrificing accuracy.
A variable step size becomes necessary when the numerical simulation range is very
large or when the true solution has very sharp and/or slow changes. But applying a variable
step size brings about fluctuations in the magnitude of truncation and global errors, as they
are proportional to the step size. As a result, the desired level of accuracy may not be
achieved if one is not careful. Therefore, the truncation error is also estimated at each
step to ensure that the numerical solution meets a predetermined accuracy criterion and to
adjust the step size if necessary. If the truncation error is extremely small, the step size may
be increased. Conversely, if the truncation error is large, the step size is reduced to prevent
the errors from accumulating.
The fourth-order Runge-Kutta (RK4) is a very reliable and popular method in many
fields because it is not only a higher-order method but also has a wider stability range.
Most adaptive ODE solvers are based on the RK4 and fourth-order Adam’s methods.
An adaptive numerical scheme presented here is based on the Runge-Kutta-Fehlberg
method, which provides an efficient procedure for solving IVPs. In this method, a suitable
step size is determined at each step. The current step estimates using the 4th and 5th order
Runge-Kutta schemes (denoted by RKF45) are computed and compared with each other.
The following six parameters are computed:
k1 “ hfpxi, yiq
k2 “ hf ˆ
xi `
1
4
h, yi `
1
4
k1
˙
k3 “ hf ˆ
xi `
3
8
h, yi `
3
32
k1 `
9
32
k2
˙
k4 “ hf ˆ
xi `
12
13h, yi `
1932
2197
k1 ´ 7200
2197
k2 `
7296
2197
k3
˙
k5 “ hf ˆ
xi ` h, yi `
439
216
k1 ´ 8k2 `
3680
513
k3 ´ 845
4104k4
˙
k6 “ hf ˆ
xi `
1
2
h, yi ´ 8
27
k1 ` 2k2 ´ 3544
2565
k3 `
1859
4104k4 ´ 11
40k5
˙
(9.56)
Next, a current step estimate is computed using the following fourth-order Runge-Kutta
scheme:
y˜i`1 “ yi `
25
216
k1 `
1408
2565
k3 `
2197
4104k4 ´ 1
5
k5 ` Oph5q (9.57)
Note that k2 and k6 have not been used here.
An improved estimate is obtained with the 5th-order Runge-Kutta scheme as follows:
yi`1 “ yi `
16
135
k1 `
6656
12825
k3 `
28561
56430k4 ´ 9
50
k5 `
2
55
k6 ` Oph6q (9.58)ODEs: Initial Value Problems  541
Pseudocode 9.8
Module ADAPTIVE_RKF45 (h, x0, y0, xlast, ε, hmin, hmax)
\ DESCRIPTION: A pseudomodule to solve y1 “ fpx, yq on [x0, xlast] with
\ adaptive Runge-Kutta-Fehlberg 45 scheme.
\ USES:
\ DRV_RK45:: Module performing one-step Runge-Kutta 45 scheme;
\ MIN :: Built-in function to find the minimum of a set of integers;
\ MAX:: A built-in function returning the maximum of the agrument list;
\ ABS :: A built-in function computing the absolute value.
x Ð x0; y Ð y0 \ Initialize solution
Write: “x, y=”,x0, y0 \ Print initial solution
While “
x ď xlast‰ \ Marching loop
h Ð MINph, xlast ´ xq \ Close to the end of solution range?
DRV_RKF45(h, x0, y0, x, y, R) \ Apply one-step RKF45
If “
R ď ε
‰
Then \ Accept solution if error ď ε
x Ð x0 ` h \ Find current step abscissa
Write: “x, y=”,x, y \ Print current step values
x0 Ð x; y0 Ð y \ Assignt current step values as prior
End If
δ Ð 0.84pε{Rq
1{4 \ Find step size adjustment factor
ha Ð δ ˚ h \ Adjust step size
h1 Ð MAXpha, hminq \ If ha ă hmin, set h Ð hmin
h Ð MINph1, hmaxq \ If ha ą hmax, set h Ð hmax
End While
End Module ADAPTIVE_RKF4
Module DRV_RKF45 (h, x0, y0, x, y, R)
\ DESCRIPTION: A pseudomodule applying a single step the Runge-Kutta-
\ Fehlberg 45 scheme with an error estimate R.
\ USES:
\ FCN:: A user-defined function module supplying fpx, yq “ y1
;
\ ABS :: A built-in function computing the absolute value.
Declare: k6
k1 Ð h ˚ FCNpx0, y0q
k2 Ð h ˚ FCN px0 ` h{4, y0 ` k1{4q
k3 Ð h ˚ FCN px0 ` 3h{8, y0 ` 3k1{32 ` 9k2{32q
k4 Ð h ˚ FCN px0 ` 12h{13, y0 ` 1932k1{2197 ´ 7200k2{2197 ` 7296k3{2197q
k5 Ð h ˚ FCN px0 ` h, y0 ` 439k1{216 ´ 8k2 ` 3680k3{513 ´ 845k4{4104q
yn Ð y0 ´ 8k1{27 ` 2k2 ´ 3544k3{2565 ` 1859k4{4104 ´ 11k5{40
k6 Ð h ˚ FCNpx0 ` h{2, ynq
\ Calculate yi`1 and the error estimate R
R Ð |k1{360 ´ 128k3{4275 ´ 2197k4{75240 ` k5{50 ` 2k6{55|{h
y Ð y0 ` 25k1{216 ` 1408k3{2565 ` 2197k4{4104 ´ k5{5
End Module DRV_RKF45
If the ˜yi`1 and yi`1 are in agreement within a preset accuracy criterion, then the
computed estimate ˜yi`1 is accepted; otherwise, the step size is reduced. If the estimates agree
to more significant digits than desired, then the step size is increased. An error estimate to542  Numerical Methods for Scientists and Engineers
adjust the step size is obtained by computing the difference between Eqs. (9.58) and (9.57)
as
R “ |yi`1 ´ y˜i`1|
h “ 1
h
ˇ
ˇ
ˇ
ˇ
1
360
k1 ´ 128
4275k3 ´ 2197
75240k4 `
1
50
k5 `
2
55
k6
ˇ
ˇ
ˇ
ˇ
(9.59)
The user only specifies the step size (h) and a tolerance (ε) for the error control. A strategy
is then implemented to determine how to modify h so that the local truncation error is ap￾proximately equal to a preset tolerance. One of the following techniques is usually employed
to ensure preset accuracy in an IVP solution while changing the step size:
(a) If R is within an acceptable range (say, ε{4 ď R ď ε), then the current estimate ˜yi`1
is accepted. However, if R ą ε, then the step size is halved ph Ð h{2q because it is
large, and the current estimate ˜yi`1 is recomputed. If R ă ε{4, then ˜yi`1 is accepted,
and the step size is doubled ph Ð 2hq because it is small.
(b) This second procedure is similar to the one presented above. The only difference is in
the selection of the step size. When the current step size is to be modified, the new
optimal step size is determined as h Ð δ ˚ h, where δ “ 0.84pε{Rq
1{4 is the step size
adjustment factor. Thus, in this procedure, the step size is automatically reduced (if
ε{R ă 1) or increased (if ε{R ą 1) depending on the size of ε{R.
As a prudent practice, usually acceptable upper and lower bounds are assigned to prevent
the computed step size from being too large or too small.
A pseudomodule, ADAPTIVE_RKF45, solving a first-order IVP with the Runge-Kutta￾Fehlberg 4th/5th-order scheme is presented in Pseudocode 9.8. As input, the module re￾quires a solution interval (x0, xlast), an initial step size (h), along with the upper and lower
bounds for the step size (hmin, hmax), an initial value (y0), and a tolerance (ε). The external
user-defined function module FCN supplies the first derivative. The module makes use of a
driver module, DRV_RK45, to compute the current step estimate y, as well as an estimate
for the error, R. If R ď ε, then the current step estimate y is accepted as the solution.
Otherwise, the step size is adjusted by ha “ δ ˚ h to prevent it from remaining small or
becoming even smaller. Before proceeding to the next step, the current values are assigned
as prior (x0 Ð x and y0 Ð y). If R ą ε, the step size is reduced by δ ˚ h since ε{R ă 1.
The step size is assured to remain within the predefined lower and upper limits by the
h Ð MINph, hmaxq and h Ð MAXpδ ˚ h, hminq statements. At the very last step, the step
size is found from h Ð MINph, xlast ´ xq.
9.6 PREDICTOR-CORRECTOR METHODS
In previous sections, explicit and implicit one-step and multi-step methods have been dis￾cussed. These methods can also be used in combination as a pair of formulas known as
predictor-corrector methods. Such methods involve two computational steps: (i) the predic￾tor step employs an explicit method to obtain a reasonable estimate (prediction) for the
current step yi`1 using an unknown function and its derivatives at some prior points; (ii)
the corrector (next) step improves the estimate and its accuracy using an implicit formula.
In this section, some of the most common predictor-corrector methods are discussed.
A graphical illustration of marching with a typical predictor-corrector method is pre￾sented in Fig. 9.8. The arrows depict the order of numerical computation for the predicted
and corrected estimates, denoted by solid and hollow circles, respectively. The predictions
yˆ’s (at points A, B, C) generally deviate from the true solution by a small margin. After
the correction step, the estimates (at points a, b, c) approach the true distribution.ODEs: Initial Value Problems  543
Heun’s Method
‚ Heun’s method is simple and easy to employ;
‚ It is computationally less involved;
‚ It is self-starting, i.e., the initial condition is sufficient to start the
forward-marching procedure.
‚ Its global error is Oph2q, so the step size needs to be chosen sufficiently
small to achieve estimates of reasonable accuracy;
‚ It has the same stability properties as the Euler method;
‚ The cpu-time increases compared to the explicit Euler method be￾cause it requires two function evaluations instead of one.
9.6.1 HEUN’S METHOD
Heun’s method applies the explicit Euler method for the predictor and the trapezoidal rule
for the corrector steps.
Predictor: ˆyi`1 “ yi ` hfi ` Oph2q,
Corrector: yi`1 “ yi `
h
2
pfi ` ˆfi`1q ` Oph3q (9.60)
where fi “ fpxi, yiq, ˆfi`1 “ fpxi`1, yˆi`1q, ˆyi`1, and yi`1 denote, respectively, predicted
and corrected estimates for the current step. Note that by replacing the implicit term,
fpxi`1, yi`1q, in the corrector step with ˆfi`1 “ fpxi`1, yˆi`1q, the corrected step also becomes
an explicit scheme. In fact, the method can be recast as a one-step scheme by substituting
yˆi`1 in the corrector equation.
In Pseudocode 9.9, a pseudomodule, PC_HEUN, is presented for solving the first-order
IVP with the Heun’s (Predictor-Corrector) method. The module requires a step size (h),
a solution interval (x0, xlast), and an initial value (y0) as input. The derivative fpx, yq
is supplied with the user-defined function module, FCN. The marching procedure for x ă
xlast is carried out within a While-construct. In the predictor yi`1 and corrector ˆyi`1 step
estimates for the current step are computed by Eq. (9.60) using the prior estimate. Finally,
the current values are set as prior (x0 Ð x; y0 Ð y) before returning to the top of the
loop.
FIGURE 9.8: A graphical depiction of marching in predictor-corrector methods.544  Numerical Methods for Scientists and Engineers
Pseudocode 9.9
Module PC_HEUN (h, x0, y0, xlast)
\ DESCRIPTION: A pseudomodule program to solve first-order IVP on rx0, xlasts
\ with the Heun’s (Predictor-Corrector) method.
\ USES:
\ FCN:: A user-defined function module supplying fpx, yq “ y1
.
Write: “x, y=”, x0, y0 \ Print initial values
x Ð x0 \ Initialize abscissa
While “
x ă xlast‰ \ Marching loop
k1 Ð h ˚ FCNpx0, y0q \ Find k1 “ h ˚ fpxi, yiq for predictor
y Ð y0 ` k1 \ Find “Predicted’ estimate, ˆyi`1q
x Ð x0 ` h \ Find next abscissa
k2 Ð h ˚ FCNpx, yq \ Find k2 “ h ˚ fpxi`1, yˆi`1q for corrector
y Ð y0 ` pk1 ` k2q{2 \ Find current (corrected) step estimate
Write: “x, y=”, x, y \ Print current step values
x0 Ð x; y0 Ð y \ Assign current step values as prior
End While
End Module PC_HEUN
9.6.2 ADAMS-BASHFORTH-MOULTON METHOD (ABM4)
The Adams-Bashforth-Moulton fourth-order method employs the fourth-order Adams-Bash￾forth formula for the predictor step and the fourth-order Adams-Moulton formula for the
corrector step:
Predictor step: ˆyi`1 “ yi `
h
24 p55fi ´ 59fi´1 ` 37fi´2 ´ 9fi´3q ` 251
720
h5yp5q
pξq (9.61)
Corrector step: yi`1 “ yi `
h
24
´
9 ˆfi`1 ` 19fi ´ 5fi´1 ` fi´2
¯
´ 19
720
h5yp5q
p¯ξq (9.62)
where fi “ fpxi, yiq, ˆfi`1 “ fpxi`1, yˆi`1q, xi´3 ă ξ ă xi`1 and xi´2 ă ¯ξ ă xi`1.
Note that the corrector step is made explicit by replacing fi`1 with ˆfi`1 in Eq. (9.62).
For the predictor and corrector steps, we require f0, f1, f2, and f3 to initiate the forward￾marching scheme. Since px0, y0q is available as the initial condition, f0 is computed with
no trouble. However, the other starting values, y1, y2, and y3, are generally obtained by a
high-order one-step method such as RK4. Once f1, f2, and f3 are computed, the forward￾marching can begin.
Adams-Bashforth-Moulton Fourth-Order Method
‚ ABM4 requires fewer function evaluations, i.e., one function for the
predictor fi and one for the corrector ˆfi`1 step;
‚ There is no need to evaluate derivatives for all prior points (quantities
no longer needed can be shifted out);
‚ Its global error is Oph4q.
‚ It can lead to stability problems since it is an explicit method;
‚ An implicit scheme can be applied in the corrective step, but then
cpu-time can increase significantly.ODEs: Initial Value Problems  545
Pseudocode 9.10
Module PC_ABM4 (h, x0, y0, xlast)
\ DESCRIPTION: A pseudomodule program to solve a first-order IVP on
\ rx0, xlasts with the ABM4 predictor-corrector method.
\ USES:
\ DRV_RK:: Module performing one-step RK4 scheme (see Pseudocode 9.4);
\ FCN:: A user-defined function module supplying fpx, yq “ y1
.
Declare: f4 \ Declare array variables
Write: “x, y=”, x0, y0 \ Print initial values
f4 Ð FCNpx0, y0q \ Find derivative at x0
For “
k “ 3, 1,p´1q
‰ \ Compute other starting values
DRV_RK(4, h, x0, y0, x, y) \ Find estimate with one-step RK4 scheme
Write: “x, y=”,x, y \ Print starting estimates
fk Ð FCNpx, yq \ Compute derivative at (x, y)
x0 Ð x; y0 Ð y \ Assign current step values as prior
End For
While “
x ă xlast‰ \ Marching loop
yh Ð y0 ` h ˚ p55f1 ´ 59f2 ` 37f3 ´ 9f4q{24 \ Find prediction
x Ð x0 ` h \ Find abscissa
fh Ð FCNpx, yhq \ Find ˆf “ fpx, yhq
y Ð y0 ` h ˚ p9fh ` 19f1 ´ 5f2 ` f3q{24 \ Find correction
Write: “x, y=”,x, y \ Print current step estimate
f4 Ð f3; f3 Ð f2; f2 Ð f1; \ Shift starting values up
f1 Ð FCNpx, yq \ Update nearest starting value
x0 Ð x; y0 Ð y \ Assign current step values as prior
End While
End Module PC_ABM4
A pseudomodule, PC_ABM4, presented in Pseudocode 9.10 is designed to solve a
first-order IVP using the ABM4. The module requires a step size (h), a solution inter￾val (x0, xlast), and the initial value (y0) as input. The derivative y1 “ fpx, yq is supplied
with the user-defined function module, FCN. In preparation to apply the ABM4 method,
the starting values for y1, y2, and y3 (likewise, f1, f2, and f3) are obtained with the DRV_RK
module (in Pseudocode 9.4). For x ă xlast, all predictions and corrections are respectively
obtained by Eqs. (9.61) and (9.62) in the While-loop. Only four initial values (f0, f1, f2,
and f3) are needed to start and maintain the forward march. As an efficient programming
trick, we only keep the last four derivatives (f0, f1, f2, and f3) as well as prior and current
step estimates (y0 and y) to reduce memory allocation. Hence, we shift the derivatives one
step back sequentially as follows: f0 Ð f1, f1 Ð f2, and f2 Ð f3, and the current estimate
is assigned to the prior estimate, px0, y0qÐpx, yq, before returning to the top of the loop.
It is possible to use adaptive step size control in the ABM4 method. Consequently,
we need an approximation for the local truncation error for the ABM4, which is given as
follows:
Et “ 19
270
|yi`1 ´ yˆi`1|
h
Then, a procedure with a variable step size strategy may be applied, provided that the size546  Numerical Methods for Scientists and Engineers
Milne’s Method
‚ Milne’s method is simple and easy to implement;
‚ It requires fewer function evaluations, i.e., only fi and ˆfi`1;
‚ It has a fourth-order global error, Oph4q, and is more accurate than
the ABM4 scheme.
‚ Being an explicit method, it suffers from stability issues under certain
conditions, stemming essentially from the corrector step. For this
reason, the ABM4 method is often favored over Milne’s method.
adjustment factor is calculated from
δ “ 0.84ˆ ε
Et
˙1{4
If the estimated truncation error is less than the desired tolerance (Et ď ε), the current
estimate is accepted. The step size is also adjusted as h Ð δ ˚h to prevent it from becoming
either too small or too large. If Et ą ε, the step size is reduced according to h Ð δ ˚ h.
However, varying the step size in multistep methods is much more expensive than in one￾step methods because it requires establishing starting values that are compatible with the
adapted step sizes. In the latter case, the prior step estimates (or shifted starting values)
cannot be used, and a new set of starting values must be computed at every step, making
it a much more expensive method than one-step methods.
9.6.3 MILNE’S FOURTH-ORDER METHOD
Milne’s predictor-corrector method employs three-point open Newton-Cotes formulas for
the first derivative in the predictor and a three-point closed Newton-Cotes formula in the
corrector step.
Predictor step: ˆyi`1 “yi´3`
4h
3 p2fi´fi´1`2fi´2q`28
90
h5yp5q
pξq xi´3 ă ξ ă xi`1 (9.63)
Corrector step: yi`1 “yi´1`
h
3
´
ˆfi`1`4fi`fi´1
¯
´ 1
90
h5yp5q
p¯ξq xi´1 ă ¯ξ ă xi`1 (9.64)
where fi “ fpxi, yiq and ˆfi`1 “fpxi`1, yˆi`1q.
In Eq. (9.64), the implicit term fi`1 is replaced with ˆfi`1, making the corrector step
also explicit. Note that the truncation error constants for this method are smaller than those
of ABM4 (Eqs. (9.61) and (9.62)), so theoretically, we can expect more accurate numerical
estimates from this scheme. The numerical algorithm is also identical to that of the ABM4.
The starting values are obtained with a high-order one-step method, and then f1, f2, and
f3 are evaluated to start the forward march. Like the ABM4, there is no need to store
all derivatives during the forward-marching step; only the required derivatives are kept by
shifting the current derivatives one step down.
In Pseudocode 9.11, a pseudomodule, PC_MILNE, is presented for solving a first-order
model IVP using Milne’s method. This module is basically identical to PC_ABM4. A major
difference is in the predictor and corrector equations, which can be easily replaced with
those of ABM4. Milne’s predictor-corrector equations involve yi´3 and yi´1 rather than yi
as in the case of ABM4; hence, only the last four estimates (yi´3, yi´2, yi´1, yi) need toODEs: Initial Value Problems  547
be stored at each step as well before returning to the top of the While-loop. The last four
estimates of y are stored on a temporary storage array g of length 4.
Pseudocode 9.11
Module PC_MILNE (h, x0, y0, xlast)
\ DESCRIPTION: A pseudomodule to solve y1 “ fpx, yq on rx0, xlasts with the
\ ABM4 (predictor-corrector) method.
\ USES:
\ DRV_RK:: Module performing one-step RK4 (see Pseudocode 9.4);
\ FCN:: A user-defined function module supplying fpx, yq “ y1
.
Declare: f4, g4 \ Declare array variables
Write: “x, y=”, x0, y0 \ Print initial values
g4 Ð y0; f4 Ð FCNpx0, y0q \ Set initial derivative as starter
For “
k “ 3, 1,p´1q
‰ \ Find starting values
DRV_RK(4, h, x0, y0, x, y) \ Apply one-step RK4
Write: “x, y=”,x, y \ Print starting value
fk Ð FCNpx, yq \ Find derivative at (x, y)
gk Ð y \ Assign solution to array g
x0 Ð x; y0 Ð y \ Set current step values as prior
End For
While “
x ă xlast‰ \ Marching loop
yh Ð g4 ` 4h ˚ p2f1 ´ f2 ` 2f3q{3 \ Find prediction
x Ð x0 ` h \ Find abscissa
fh Ð FCNpx, yhq \ Corrector step
y Ð g2 ` h ˚ pfh ` 4f1 ` f2q{3 \ Find correction (current estimate)
Write: “x, y=”,x, y \ Print current estimate
f4 Ð f3; f3 Ð f2; f2 Ð f1; \ Shift starting values
f1 Ð FCNpx, yq \ Update nearest derivative
g4 Ð g3; g3 Ð g2; g2 Ð g1;
g1 Ð y; x0 Ð x; y0 Ð y \ Assign current values as prior
End While
End Module PC_MILNE
EXAMPLE 9.8: Comparison of Heun, ABM4, and Milne’s methods
Repeat Example 9.1 using (a) Heun’s, (b) the ABM4, and (c) Milne’s (predictor￾corrector) methods with h “ 0.1. Compare the estimates with the true solution.
SOLUTION:
The initial condition and corresponding derivative are I0 “ 2 and f0 “ 3.
(a) We start with Heun’s method by applying Eqs. (9.60) for i “ 0, which yields
the following predictor (ˆI) and corrector (I) values:
ˆI1 “ I0 ` hf0 “ 2 ` 0.1p3q “ 2.3
I1 “ I0 `
h
2
´
f0 ` fpt1, ˆI1q
¯
“ 2 ` 0.05`
3 ` fp0.1, 2.3q
˘
“ 2.242992
Applying Eqs. (9.60) for i “ 1, the second-step prediction and correction values are548  Numerical Methods for Scientists and Engineers
found as
ˆI2 “ I1 ` hf1 “ 2.242993 ` 0.1f
`
0.1, 2.242993˘
“ 2.443227
I2 “ I1 `
h
2
´
f1 ` fpx2, ˆI2q
¯
“ 2.242993 ` 0.05`
2.002355 ` fp0.2, 2.443227q
˘
“ 2.399641
In the third, fourth, and subsequent steps, the current prediction and correction
values are obtained using the recursive difference equations, Eqs. (9.63) and (9.64).
The true solution as well as the absolute (true) errors are presented in Table 9.13.
The variation of errors across the solution range is similar to that obtained by the
methods applied in the previous examples; that is, the error increases up to t « 0.46,
where the true solution also has a local maximum and decays with increasing t.
(b) To apply the ABM4 method, starting values are also generated from the true
solution. The fact that the starting values do not initially contain any errors allows
for an ideal comparison with Milne’s method. From the initial condition, we have
f0 “ 3, and the rest of the starting values are obtained as follows:
I1 “ Itruep0.1q “ 2.247316, f1 “ fpt1, I1q “ fp0.1, 2.247316q “ 1.991545
I2 “ Itruep0.2q “ 2.406288, f2 “ fpt2, I2q “ fp0.2, 2.406288q “ 1.222979
I3 “ Itruep0.3q “ 2.498099, f3 “ fpt3, I3q “ fp0.3, 2.498099q “ 0.640417
For the predictor step, applying Eq. (9.61) yields
ˆI4 “ I3 `
h
24
`
55f3 ´ 59f2 ` 37f1 ´ 9f0
˘
“ 2.498099 `
0.1
24
`
55p0.640416q ´ 59p1.222979q ` 37p1.991545q ´ 9p3q
˘
“ 2.538742
Then the intermediate value fpt4, ˆI4q becomes
fpt4, ˆI4q “ fp1.4, 2.538742q “ 0.202991
Using Eq. (9.62), the corrector step yields
I4 “ I3 `
h
24
´
9fpt4, ˆI4q ` 19f3 ´ 5f2 ` f1
¯
“ 2.498099 `
0.1
24 p9p0.202991q ` 19p0.640416q ´ 5p1.222979q`p1.991545qq
“ 2.539230
Before moving on to the next step, calculating the derivative of the current step
is necessary because it will be added to the list of starting values. So the required
quantities are found as follows:
t4 “ 0.4, I4 “ 2.539230, f4 “ fpt4, I4q “ 0.201771
The fifth-step predictor yields
ˆI5 “ I4 `
h
24
`
55f4 ´ 59f3 ` 37f2 ´ 9f1
˘
“ 2.539230 `
0.1
24 p55p0.201771q ´ 59p0.640416q ` 37p1.222979q ´ 9p1.991545qq
“ 2.541893ODEs: Initial Value Problems  549
and fpt5, ˆI5q “ fp0.5, 2.541893q“´0.124326.
Finally, employing the corrector step results in
I5 “ I4 `
h
24
´
9fpt5, ˆI5q ` 19f4 ´ 5f3 ` f2
¯
“ 2.539230 `
0.1
24
`
9p´0.124326q ` 19p0.201771 q ´ 5p0.640416q`p1.222979q
˘
“ 2.542295
By applying the foregoing recursive procedure in the same manner, the ABM4 esti￾mates were obtained and presented comparatively in Table 9.13. Since the starting
values (for the first four steps) are obtained from the true solution, the accumulation
of truncation error is due only to the ABM4 method. For this reason, it is observed
that the instant when the error reaches a maximum shifts to t « 0.8 due to the
accumulation of errors.
TABLE 9.13
True True absolute error, ei
ti Solution Heun’s Method ABM4 Milne’s Method
0 2
0.2 2.406288 6.65E-03
0.4 2.539164 7.79E-03 6.59E-05 4.12E-05
0.6 2.517013 6.75E-03 1.17E-04 5.59E-05
0.8 2.410610 5.09E-03 1.18E-04 5.57E-05
1 2.261953 3.47E-03 1.00E-04 5.00E-05
1.2 2.095672 2.15E-03 7.80E-05 4.32E-05
1.4 1.925946 1.14E-03 5.78E-05 3.72E-05
1.6 1.760685 4.35E-04 4.14E-05 3.25E-05
1.8 1.604061 4.15E-05 2.89E-05 2.91E-05
2 1.458042 3.45E-04 1.98E-05 2.69E-05
(c) Applying Milne’s method (Eqs. (9.63) and (9.64)) for the first step yields
ˆI4 “ I0 `
4h
3 p2f3 ´ f2 ` 2f1q
“ 2 `
4p0.1q
3
`
2p0.640416q´p1.222979q ` 2p1.991545q
˘
“ 2.538793
I4 “ I2 `
h
3
´
fpt4, ˆI4q ` 4f3 ` f2
¯
“ 2.406288 `
0.1
3
`
0.202864 ` 4p0.640416q`p1.222979q
˘
“ 2.539205
where the intermediate value is found as fpt4, ˆI4q “ 0.202864.
Evaluating the derivative for the current step gives
f4 “ fpt4, I4q “ fp0.4, 2.539205q “ 0.201834
Proceeding with the second-step calculations results in
ˆI5 “ I1 `
4h
3 p2f4 ´ f3 ` 2f2q
“ 2.247316 `
4p0.1q
3
`
2p0.201834q ´ 0.640416 ` 2p1.222979q
˘
“ 2.541877550  Numerical Methods for Scientists and Engineers
I5 “ I3 `
h
3
´
fpt5, ˆI5q ` 4f4 ` f3
¯
“ 2.498099 `
0.1
3
`
´ 0.124286 ` 4p0.201834q ` 0.640416˘
“ 2.542214
where fpt5, ˆI5q“´0.124286.
The estimates computed with Milne’s method have been repeated in the same
manner for the subsequent steps and are presented in Table 9.13. In this example,
the first four steps are derived from the true solution, just as in Part (b). For t « 0.6,
a slight increase in errors is observed.
Discussion: Examining the true errors incurred by the three methods, the Heun’s
method has yielded the largest errors among all three methods. This result is un￾derstandable since this method is second-order accurate. The ABM4 and Milne’s
methods are both fourth-order methods, but Milne’s method gives more accurate
estimates. In this example, the errors are on average about 18 times smaller. The
reason for this is that the truncation error constants in Milne’s method, Eqs. (9.63)
and (9.64), are smaller in comparison to those of the ABM4; Eqs. (9.61) and (9.62).
9.7 SYSTEM OF FIRST-ORDER IVPS (ODES)
Mathematical models of real-life problems generally involve two or more first- or higher￾order coupled ODEs. So far, we have discussed the numerical methods for the first-order
model IVP given in the form y1 “ fpx, yq. In this section, we discuss extending these
numerical methods to first-order simultaneous linear and non-linear IVP-ODEs.
Consider a system of n first-order linear IVPs (ODEs) with n initial conditions:
dY1
dx “ f1px, Y1, Y2,...,Ynq, Y1px0q “ Y1,0,
dY2
dx “ f2px, Y1, Y2,...,Ynq, Y2px0q “ Y2,0,
.
.
. .
.
. .
.
.
dYn
dx “ fnpx, Y1, Y2,...,Ynq, Ynpx0q “ Yn,0
(9.65)
where x is the independent variable, and Y1pxq, Y2pxq, ... , Ynpxq are the dependent vari￾ables.
This system of ODEs can be expressed in vector notation as
dy
dx “ Fpx, yq, ypx0q “ y0 (9.66)
where
Fpx, yq “
»
—
—
—
–
f1px, yq
f2px, yq
.
.
.
fnpx, yq
fi
ffi
ffi
ffi
fl , y “
»
—
—
—
–
Y1
Y2
.
.
.
Yn
fi
ffi
ffi
ffi
fl
and ypx0q “ y0 “
»
—
—
—
–
Y1,0
Y2,0
.
.
.
Yn,0
fi
ffi
ffi
ffi
flODEs: Initial Value Problems  551
An explicit or implicit numerical method similar to a single IVP can be readily gener￾alized by replacing the scalar variable y and the scalar function f by vector functions y and
F, respectively. For instance, the explicit Euler scheme can be expressed in vector notation
as:
yi`1 “ yi ` h Fpxi, yiq, y0 “ ypx0q (9.67)
Similarly, the RK4 scheme for a system of first-order coupled ODEs is cast as
yi`1 “ yi `
h
6 pk1 ` 2k2 ` 2k3 ` k4q, y0 “ ypx0q (9.68)
and the RK parameters are also vectorized as follows:
k1 “ h F pxi, yiq,
k2 “ h F `
xi`1{2, yi ` k1{2
˘
,
k3 “ h F `
xi`1{2, yi ` k2{2
˘
,
k4 “ h F pxi`1, yi ` k3q
(9.69)
where kj,i’s for i “ 1, 2, ...,n are the elements of kj for j “ 1, 2, 3, and 4.
Starting with the initial conditions, all dependent variables for the current step, yi`1,
can be explicitly computed using the prior step estimates—yi. However, the computation of
k’s is carried out sequentially because every km requires the information from the previous
k (i.e., km´1).
Special case: Consider a system of linear IVPs that can be expressed as follows:
dy
dx “ Apxqy ` gpxq, ypx0q “ y0 (9.70)
where A and g can be functions of the independent variable x as
Apxq “
»
—
—
—
–
a11pxq a12pxq ¨¨¨ a1npxq
a21pxq a22pxq ¨¨¨ a2npxq
.
.
. .
.
. ... .
.
.
a21pxq an2pxq ¨¨¨ annpxq
fi
ffi
ffi
ffi
fl
and gpxq “
»
—
—
—
–
g1pxq
g2pxq
.
.
.
gnpxq
fi
ffi
ffi
ffi
fl
A linear IVP in the given form allows one to obtain the numerical solution with implicit
schemes without having to resort to iterations. For instance, the implicit Euler method can
be written as
yi`1 “ yi ` h pApxi`1qyi`1 ` gpxi`1qq (9.71)
Solving Eq. (9.71) for yi`1 yields
yi`1 “ pI ´ hAi`1q
´1 pyi ` h gi`1q (9.72)
where Ai`1 “ Apxi`1q and gi`1 “ gpxi`1q.
When simulating unsteady physical phenomena (IVPs), it is not unusual to deal with
systems of hundreds or possibly thousands of first-order ODE-IVPs. Hence, it is crucial to
make sure that any numerical method employed is both accurate and efficient.552  Numerical Methods for Scientists and Engineers
Pseudocode 9.12
Module BDF2_SE (n, h, x0, y0, xlast)
\ DESCRIPTION: A pseudomodule to solve a first-order coupled linear ODEs
\ of the form; dy{dx “ Apxqypxq ` gpxq using the BDF2 method.
\ USES:
\ EQNS:: A user-defined module supplying coefficients of A and g;
\ Ax:: Module to perform matrix-vector multiplication (Pseudocode 2.5);
\ INV_MAT:: Module to invert a matrix (Pseudocode 2.6);
\ DRV_RK:: Module performing one-step RK scheme; a vectorized version of
\ Pseudocode 9.9, which is not provided here.
Declare: y0n, y1n, yn, an,n, gn, bn,n, rhsn, c3 \ Declare array variables
c1 Ð ´1{3; c2 Ð 4{3; c3 Ð 2h{3 \ Set BDF2 coefficients
Write: “x,y=”, x0,y0 \ Print initial values
DRV_RK(4, h, x0, y0, x, y) \ Find a starting value using RK4
y1 Ð y \ Assign estimates to y1 Ð ypx0 ` hq
Write: “x,y=”, x,y1 \ Print starting values
x0 Ð x \ Initialize abscissa
While “
x ă xlast‰ \ Marching loop
x Ð x0 ` h \ Find current step abscissa
EQNS(n, x, A, g) \ Construct A and g at x
For “
i “ 1, n‰ \ Construct B “ I ´ p2h{3qA
For “
j “ 1, n‰
If “
i “ j
‰
Then \ bii “ 1 ´ p2h{3q ˚ aii
bij Ð 1 ´ c3 ˚ aij
Else \ bij “ ´p2h{3q ˚ aij
bij Ð ´c3 ˚ aij
End If
End For
End For
rhs Ð c1 ˚ y0 ` c2 ˚ y1 ` c3 ˚ g \ Compute rhs Ð c1y0 ` c2y1 ` c3g
INV_MAT(n, B, B) \ Invert matrix B, B Ð B´1
Ax(n, B, rhs, y) \ Find current estimate, yi`1 Ð B´1 ¨ rhs
Write: “x,y=”, x,y \ Print current estimates
x0 Ð x; y0 Ð y1; y1 Ð y \ Set current estimates as prior
End While
End Module BDF2_SE
Module EQNS (n, x A, g) \ Declare array variables
\ DESCRIPTION: User-defined module supplying 1st -order linear ODEs.
Declare: ann, gn
aij Ð ... pi, j “ 1, 2,...,nq \ Define elements of A
gi Ð ... pi “ 1, 2,...,nq \ Define elements of g
End Module EQNS
In Pseudocode 9.12, the module BDF2_SE solving a first-order coupled linear IVPs
with BDF2 is presented. The module requires the number of ODEs (n), a step size (h), a
solution interval (x0, xlast), and the initial value (y0) as input. The set of ODEs is supplied
to the module with the user-defined function module EQNS that returns the coefficients ofODEs: Initial Value Problems  553
A and g that may be x-dependent. The main module also requires matrix multiplication
and inversion modules. To begin forward marching, the starting estimate y1 is obtained by
the RK4 scheme using DRV_RK. For x ă xlast, the current estimates are obtained by using
Eq. (9.74) within a While-loop. The current step estimates are assigned as prior estimates
(x0 Ð x; y0 Ð y1; y1 Ð y) before returning to the top of the loop.
In Pseudocode 9.12, the pseudomodule, BDF2_SE, is given for solving n simultaneous
first-order linear IVP equations of the form of Eq. (9.70) using the BDF2 scheme.
dyi
dx “ Apxi`1qyi`1 ` gpxi`1q, ypx0q “ y0 (9.73)
Replacing the derivative with the second-order BDF yields
yi`1 “ 1
3
ˆ
I ´ 2h
3
Ai`1
˙´1
p4yi ´ yi´1 ` 2h gi`1q, (9.74)
where Ai`1 “ Apxi`1q and gi`1 “ gpxi`1q.
9.8 HIGHER-ORDER ODES
Initial value problems are not solely confined to first-order ODEs. Second- or higher-order
linear or nonlinear IVPs are commonly encountered. These problems have no true solutions
except in very special cases and are predominantly solved numerically. A common approach
to the numerical solution of higher-order IVPs is to reduce them into an equivalent system
of first-order simultaneous ODEs.
Consider the following nth-order ODE-IVP:
ypnq
pxq “ F
´
x, y1
, y2,...,ypn´1q
¯
(9.75)
subjected to the following n initial conditions:
ypx0q “ α0, y1
px0q “ α1, y2px0q “ α2,..., ypn´1q
px0q “ αn´1.
Equation (9.75) can be reduced into a system of first-order IVP by defining a set of new
independent variables:
Y1pxq “ ypxq, Y2pxq “ y1
pxq, Y3pxq “ y2pxq, ... Ynpxq “ ypn´1q
pxq
As a result, the original IVP becomes equivalent to coupled n-first-order ODEs
dY1
dx “ Y2, Y1px0q “ α0,
dY2
dx “ Y3, Y2px0q “ α1,
.
.
. .
.
.
dYn´1
dx “ Yn, Yn´1px0q “ αn´2
dYn
dx “ Fpx, Y1, Y2,...,Ynq, Ynpx0q “ αn´1
(9.76)554  Numerical Methods for Scientists and Engineers
9.8.1 STIFF ODEs
In the numerical solution of coupled ODE-IVPs, just as in the case of a single ODE, a well￾known problem called stiffness may be encountered. Also recall that this problem arises
from the true solution of a system consisting of ODEs with components that grow and/or
decay at different rates. Hence, the systems of ODEs that depict stiffness are often referred
to as stiff ODEs or stiff systems.
These stiff ODEs are encountered in chemical reaction kinetics, control systems, elec￾trical networks, and so on, and can be very difficult or impossible to solve numerically using
the explicit methods discussed previously. This difficulty stems mainly from the small sta￾bility regions of explicit methods, which forces the analyst to use a very small step size to
achieve a stable solution. On the other hand, using an extremely small step size (smaller
than the desired accuracy, Δx ď ε) can yield serious problems due to round-off errors.
Stiffness is a relative concept that depends on certain features of an IVP, such as the
stability domain, integration interval, and desired accuracy. Then the question arises: How
do we know that a system of coupled IVP is stiff? Mathematically speaking, if the Jacobian
matrix has eigenvalues that differ in magnitude, then it can be said that the stiffness problem
exists.
As a measure of the stiffness of a given problem, the stiffness ratio is generally defined
in terms of the largest and smallest eigenvalues as
R “
max
i |Repλiq|
min
i |Repλiq|
where λi are the eigenvalues of the Jacobian of the system, R is the stiffness ratio, and
Re denotes the real part of an imaginary number. This definition of stiffness is invalid for
nonlinear systems. Therefore, a verbal definition of stiffness that can be applied to both
linear and nonlinear problems is given as follows:
Definition (Stiffness): If a numerical method with a finite region of absolute stability,
applied to a system with any initial conditions, is forced to use in a certain interval of
integration a step length that is extremely small in comparison to the smoothness of the true
solution in that interval, then the system is said to be stiff in that interval [20].
If a numerical method or scheme with a larger stability region can be devised, then one
can expect the numerical method to perform better in solving stiff ODEs.
The stiffness ratio may not always be a good measure of stiffness,
even in linear systems, because the problem has infinite stiffness
if the minimum eigenvalue is zero. Thus, whenever a set of simul￾taneous linear first-order ODEs is solved, beware of the stiffness
problem.
9.8.2 NUMERICAL STABILITY
Consider the following system of linear ODEs:
dY1
dx “ f1px, Y1, Y2q “ a11Y1pxq ` a12Y2pxq
dY2
dx “ f2px, Y1, Y2q “ a21Y1pxq ` a22Y2pxq
(9.77)ODEs: Initial Value Problems  555
or, in matrix notation, the system can be expressed as
dy
dx “ Ay (9.78)
where
y “
„
Y1
Y2
j
, A “
„
a11 a12
a21 a22j
, and y0 “
„
Y1,0
Y2,0
j
Applying the explicit Euler scheme to Eq. (9.78) yields
yi`1 ´ yi
h ` Ophq“pAyqi (9.79)
which leads to
„
Y1
Y2
j
i`1
“
„
1 ` ha11 ha12
ha21 1 ` ha22j „Y1
Y2
j
i
` Oph2q (9.80)
or
yi`1 “ Gexpyi (9.81)
where Gexp “ I`hA matrix denotes the amplification matrix of the explicit method, and I
is the identity matrix. Consequently, ignoring the global error term, the numerical estimate
for any ith step can be written as
yi “ Gi
expy0 (9.82)
In order to avoid instabilities and have a bounded numerical solution (if it exits), the
amplification matrix should approach the zero matrix for increasing powers of i. In other
words, a suitable step size should be chosen to ensure a bounded numerical solution as
i Ñ 8, i.e., limiÑ8 }yi} Ñ 0. It turns out that to obtain a bounded solution, the eigenvalues
of the amplification matrix should satisfy the |λ|k ă 1 condition for all k.
Now, we apply the stability analysis to the implicit Euler scheme, which leads to
yi`1 “ Gimpyi (9.83)
where Gimp “ pI ´ hAq
´1 is the amplification matrix for the implicit scheme. The eigen￾values of Gimp can be obtained as μk “ 1{p1 ´ hλkq, where λk’s denote the eigenvalues of
A (see Chapter 11). Thus, it becomes evident that all λk’s must have negative real parts
(Repλkq ă 0 for all k) to ensure the stability of this model IVP problem (Eq. 9.78). This
constraint also ensures |μk| ă 1 for all k and consequently satisfies limiÑ8�yi� Ñ 0.
Numerical methods devised for a single IVP, such as AB, AM, BDF, and predictor￾corrector, can be generalized in the same way, and stability analysis can be performed,
although it may be relatively difficult. The global error is calculated from the expression
obtained by replacing the term |ypxiq ´ yi| in univariate IVPs with �ypxiq ´ yi�8, where
ypxiq is the true solution at the ith step. Backward differentiation formulas are among the
most widely used numerical schemes for stiff problems.
A system of coupled ODEs is said to be stable if its numerical solu￾tion is bounded for all x. Moreover, the stability of the numerical
method is assured if the real parts of the Jacobian matrix are all
negative. Note that even if the real part of a single eigenvalue were
positive, the numerical solution would grow unboundedly.556  Numerical Methods for Scientists and Engineers
EXAMPLE 9.9: Solving coupled system of first-order ODEs
Consider the following coupled-system of IVP:
dY1
dx “ 396Y1pxq ` 796 Y2pxq, Y1p0q “ 1
dY2
dx “ ´398Y1pxq ´ 798 Y2pxq, Y2p0q “ 0
Obtain the numerical solution on [0,1] with (a) explicit, (b) implicit Euler, and (c)
BDF2 schemes using h “ 0.1. Compare your estimates with the true solutions given
below:
Y1pxq “ 2 e´2x ´ e´400x and Y2pxq “ e´400x ´ e´2x
SOLUTION:
(a) For the given IVP, we can write
y “
„
Y1
Y2
j
, A “
„ 396 796
´398 ´798j
and y0 “
„
1
0
j
and employing the explicit Euler scheme, Eq. (9.80), leads to
„
Y1
Y2
j
i`1
“
„
1 ` 396h 796h
´398h 1 ´ 798h
j „Y1
Y2
j
i
For h “ 0.1, the first two-step estimates yield
Y1,1 “ Y1p0.1q “ 40.6, Y2,1 “ Y2p0.1q“´39.8,
Y1,2 “ Y1p0.2q“´1519.72, Y2,2 “ Y2p0.2q “ 1520.36
In subsequent steps, the computed estimates quickly get out of control and diverge
from the true solution.
(b) The implicit Euler scheme leads to
„
Y1
Y2
j
i`1
“
„
1 ´ 396h ´796h
398h 1 ` 798h
j´1„
Y1
Y2
j
i
Similarly, the first two steps with h “ 0.1 yield
Y1,1 “ Y1p0.1q “ 1.6422764, Y2,1 “ Y2p0.1q“´0.8089431
Y1,2 “ Y1p0.2q “ 1.3882940, Y2,2 “ Y2p0.2q“´0.6938495
These estimates are close enough, given the magnitude of the truncation errors.
(c) In matrix form, the BDF2 can be written as
yi`1 “
ˆ
I ´ 2h
3
A
˙´1 1
3
p4yi ´ yi´1q
or
„
Y1
Y2
j
i`1
“ 1
3
»
–
1 ´ 264h ´1592
3
h
796
3
h 1 ` 532h
fi
fl
´1
ˆ
4
„
Y1
Y2
j
i
´
„
Y1
Y2
j
i´1
˙ODEs: Initial Value Problems  557
In this example, the starting value y1 is obtained from the true solution. Then, the
first two steps with h “ 0.1 result in
Y1,1 “ Y1p0.1q “ 1.3502382, Y2,1 “ Y2p0.1q“´0.6811432
Y2,1 “ Y1p0.2q “ 1.0933155, Y2,2 “ Y2p0.2q“´0.5469481
The true errors due to implementing the implicit Euler (IE) and 2nd-order Back￾ward Difference Formula (BDF2) and the true solutions are comparatively presented
in Table 9.14. Note that the true solutions have two exponential components that
decay at different rates (λ “ 2 and λ “ 400); hence, the stiffness ratio is 200. This
implies that the system is fairly stiff. When the tabulated errors are examined, it
is observed that the implicit Euler method yields, as expected, larger errors than
the second-order BDF2 since it is a first-order method. The errors for both meth￾ods initially depict an increase, but then they recover and continue to decrease as x
increases.
TABLE 9.14
True, Abs. error, |ei| True, Abs. Error, |ei|
x Y1,i IE BDF2 Y2,i IE BDF2
0 1 0
0.1 1.637462 0.00481 -0.818731 0.00979
0.2 1.340640 0.04765 0.00960 -0.670320 0.02353 0.01082
0.3 1.097623 0.05977 0.00431 -0.548812 0.02988 0.00186
0.4 0.898658 0.06585 0.00679 -0.449329 0.03292 0.00345
0.5 0.735759 0.06799 0.00777 -0.367879 0.03399 0.00389
0.6 0.602388 0.06741 0.00826 -0.301194 0.03370 0.00413
0.7 0.493194 0.06497 0.00834 -0.246597 0.03248 0.00417
0.8 0.403793 0.06134 0.00812 -0.201897 0.03067 0.00406
0.9 0.330598 0.05702 0.00771 -0.165299 0.02851 0.00385
1 0.270671 0.05234 0.00717 -0.135335 0.02617 0.00359
Discussion: It is possible to uncouple some ODEs, in which case each ODE can be
solved by a method suitable to its own characteristics. However, when a system of
ODEs cannot be uncoupled, the stiffness of the system becomes a critical issue. The
implicit methods, particularly BDFs, are useful. However, when the IVP is nonlinear,
the system of nonlinear implicit BDFs can be solved by Newton’s method.
9.9 SIMULTANEOUS NONLINEAR ODES
As we have learned so far, explicit methods can be employed on any IVP to obtain the
numerical solution, whether the IVP is linear or nonlinear. However, the stability criterion
of the explicit schemes may be more restrictive in nonlinear and stiff IVPs.
In nonlinear IVPs, the implicit methods require the implementation of a root-finding
algorithm. At each step, a system of nonlinear algebraic equations must be solved iteratively
using a suitable method discussed in Chapter 4. Among them, Newton’s method is a popular
and efficient way to solve most nonlinear algebraic equations.
Consider Eq. (9.66), employing the implicit Euler scheme gives
yi`1 “ yi ` h Fpxi`1, yi`1q (9.84)558  Numerical Methods for Scientists and Engineers
for which the residual vector is defined as
Rpxi`1, yi`1q “ yi`1 ´ yi ´ h Fpxi`1, yi`1q (9.85)
which constitutes a system of nonlinear algebraic equations. Expanding the residual into a
two-term Taylor series about yppq
i`1 leads to
Rpxi`1, ypp`1q
i`1 q–Rpxi`1, yppq
i`1q`ˆ
I´h BF
Byi`1
pxi`1, yppq
i`1q
˙
pypp`1q
i`1 ´yppq
i`1q (9.86)
where BF{Byi`1 is the Jacobian matrix and the superscript “(p)” denotes the iteration step.
We set Rpxi`1, ypp`1q
i`1 q “ 0, which will eventually be satisfied within a preset tolerance
value when the system of equations converges. Then, using Eq. (9.85) and solving Eq. (9.86)
for ypp`1q
i`1 , we get
ypp`1q
i`1 “ yppq
i`1 ´
´
I ´ h Jpxi`1, yppq
i`1q
¯´1 ´
yppq
i`1 ´ yi ´ h Fpxi`1, yppq
i`1q
¯
(9.87)
where yi`1 and yi, respectively, denote the current and prior step estimates.
Equation (9.87) is iterated to compute the current step estimate, yi`1. However, as
a word of caution, it should be noted that the Jacobian matrix must be updated at each
iteration if it is not constant (i.e., elements being functions of x and/or yi’s). Once the
convergence criterion, ypp`1q
i`1 ´ yppq
i`1 ă ε, is met, the numerical solution is advanced to
the next step.
9.10 CLOSURE
This chapter is largely devoted to numerical methods for solving the model IVP: y1 “ fpx, yq
with ypx0q “ y0. The explicit and implicit Euler methods, requiring the solution along the
tangent lines, have first-order global errors and are the simplest of all numerical methods
in this class of IVPs. However, these most elementary numerical methods are seldom used
in practice.
Methods of practical importance are the high-order Runge-Kutta methods, the multi￾step Adams-Bashforth method, and the Adams-Moulton method. The popularity of the
fourth-order Runge Kutta method is due to its accuracy, stability, and ease of program￾ming, besides being an explicit method. The Adams-Bashforth methods, which are a bit
more complicated to integrate into programming, are also explicit methods that provide
an alternative to the Runge-Kutta methods. On the other hand, Adams-Moulton methods,
which are implicit, have smaller truncation errors (in comparison to Adams-Bashford meth￾ods) and require integration of root-finding algorithms into programming, which in general
yields larger cpu-time.
The two-step schemes, predictor-corrector methods, require a suitable combination of
a predictor (explicit) and a corrector (implicit) scheme to establish a method with better
convergence characteristics. They also provide appealing algorithms for the numerical so￾lution of IVPs due to the fact that they require relatively few function evaluations. The
fourth-order predictor-corrector methods require two function evaluations per step, while
the RK4 requires four function evaluations per step. However, in addition to the initial
value, the starting values need to be supplied by another method to apply these methods.ODEs: Initial Value Problems  559
When selecting and using a numerical method, “stability” is a major issue that needs
to be addressed. A numerical method is either unstable, conditionally stable, or uncondition￾ally stable. An unstable method tends to diverge, while stable methods, involving implicit
schemes, always converge to an approximate solution. On the other hand, explicit methods
are conditionally stable and may require a very small step size to satisfy the “convergence
condition.” Another problem that arises in the numerical solution of IVPs is the stiffness
phenomenon. Stiff differential equations also require small step sizes, often smaller than
what is required for accuracy. In this respect, the backward difference formulas (BDFs) are
very efficient at handling stiff IVPs.
Almost all methods presented in this chapter involve a uniform step size throughout the
solution interval. Adaptive methods allow the step size to be varied (as large as possible) in
order to accommodate sharp or slow changes in the solution while retaining errors below a
specified tolerance. Among all adaptive methods, the Runge-Kutta-Fehlberg method, which
uses a fourth- and fifth-order explicit RK method (RKF45), is the most popular due to the
ease of its use.
Many science and engineering problems involve two or more variables, which lead to
a set of coupled ODEs. Also, high-order ODEs need to be reduced to a set of first-order
simultaneous ODEs. Any one of the one-step or multi-step methods presented in this chapter
can be applied to numerically solve a system of simultaneous first- or higher-order ODEs.
The scalar quantities are replaced with vector notations.
9.11 EXERCISES
Section 9.1 Fundamental Problem
E9.1 For Exercises (a)-(g), determine (i) the dependent and independent variables, (ii) the order
of the ODE, and (iii) the problem type as first- or second-order linear or nonlinear IVP.
(a) dy
dx “ sinpxyq, yp0q “ 1 (b) d2r
dt2 “ t
2 dr
dt ´ p3t ` 1qr ` 2, rp0q “ 0, dr
dt p0q “ 0
(c) dx
dt “ t x ´ cos t, yp1q “ 0 (d) d3u
dx3 “ xudu
dx ´ 1, up0q“´1, u1
p0q “ u2
p0q “ 0
(e) y2 d2x
dy2 ` 2y
dx
dy ` x “ 0, xp0q “ 0, dx
dy p0q “ 1
(f) d4y
dx4 “ dy
dxe
´xy, yp0q “ 1
2 , y1
p0q “ y2
p0q “ y3p0q “ 0
(g) d2p
dz2 ` p
ˆdp
dz ˙3
´ z p2 “ 0, pp1q “ 0, dp
dz p1q “ 0
Section 9.2 One-Step Methods
E9.2 Solve the following IVPs for the given interval and initial conditions using the explicit Euler
method, and compare your results with the true solutions.
(a) y1 “ xp1 ` yq, x0 “ 0, y0 “ 0, h “ 0.1, r0, 1s; ytruepxq“´1 ` e
x2{2
(b) y1 “ 3x2
y2
, x0 “ 0, y0 “ 1{2, h “ 0.1, r0, 1s; ytruepxq “ 1{p2 ´ x3
q
(c) y1 ` y2 cos x “ y, x0 “0, y0 “2{3, h“π{20, r0, π{4s; ytruepxq“2{p2e
´x`sin x`cos xq
(d) y1 “ y2
e
´x, x0 “ 0, y0 “ 1{2, h “ 0.2, r0, 2s; ytruepxq “ e
x{p1 ` e
xq
(e) y1 “ x{p1 ` yq, x0 “ 0, y0 “ 2, h “ 0.05, r0, 0.5s; ytruepxq“´1 ` a9 ` x2560  Numerical Methods for Scientists and Engineers
E9.3 Solve the following IVPs for h “ 0.1 and 0.01 using the explicit Euler method on [0,1] and
compare your results with the true solution for both cases.
(a) y1 ` 2y “ x2
y3
, yp0q “ 1, ytruepxq “ 4{
a8x2 ` 4x ` 1 ` 15e4x
(b) y1 ` y “ 2{y3
, yp0q “ 1, ytruepxq“p2 ´ e
´4xq
1{4
(c) y1 ` 2xy “ x
?y, yp0q “ 1, ytruepxq“p1 ` e
´x2
` 2e
´x2{2
q{4
(d) y1 ´ y{px ` 1q “ y2
, yp0q“´1, ytruepxq“´2px ` 1q{px2 ` 2x ` 2q
(e) y1 ` 3 e
y´x “ 0, yp0q “ 0, ytruepxq“´ lnp4 ´ 3e
´xq
(f) y1
cos y “ xe´ sin y, yp0q “ 0, ytruepxq “ sin´1
plnp1 ` x2
{2qq
(g) p1 ` x2
qy1 “ 2x cos2
y, yp0q “ 0, ytruepxq “ tan´1 `
lnp1 ` x2
q
˘
E9.4 Apply Taylor’s method to the following first-order ODEs to obtain a third-degree polyno￾mial approximation.
(a) y1
“x`y, (b) y1
“x{y, (c) y1
“´xy2
, (d) y1
“x3´y, (e) y1
“e
´xy, (f) y1
“e
sin y
E9.5 Repeat E9.2 using Taylor’s method: (i) obtain 2nd- and 3rd-degree polynomial approxima￾tions; (ii) estimate the solution of the IVPs; and (iii) compare your estimates with those of the
true solutions.
E9.6 Apply Taylor’s method to obtain a fourth-degree polynomial approximation to the following
first-order ODEs:
(a) y1 “ 5x5
e
´y, (b) y1 “ 2xcos2 y{p1 ` x2
q, (c) y1 “ e
´ cos y
E9.7 Find expressions for the upper error bound of the second-degree Taylor polynomial approx￾imations of E9.5.
E9.8 The motion of a damped spring-mass system is described with the following second-order
ODE:
md2y
dt2 ` c
dy
dt ` kyptq “ 0
where c is the damping coefficient, k is the spring constant, and m is the mass. The spring is
released from a point d cm above its equilibrium position, i.e., yp0q “ d and y1
p0q “ 0. Apply
Taylor’s method to find a fourth-degree polynomial approximation.
E9.9 Apply the implicit Euler method to E9.2. (a) Find an explicit expression for yi`1; (b) obtain
numerical estimates with h “ 0.1 and 0.01; and (c) compare these estimates with the true solution.
E9.10 Solve the IVPs in E9.2 using the trapezoidal rule and compare your estimates with the
true solution. Use ε “ 10´6 as a tolerance value for convergence.
E9.11 Solve the IVPs in E9.2 using the linearized implicit Euler method and compare your
estimates with the true solution.
E9.12 Solve the IVPs in E9.2 using the linearized trapezoidal rule and compare your estimates
with the true solution.
E9.13 Solve the IVPs in E9.2 using the midpoint rule and compare your estimates with the true
solution.
E9.14 Solve the IVPs in E9.2 using the modified Euler method and compare your estimates with
the true solution.
E9.15 Solve the IVPs in E9.3 using the implicit Euler method and compare your estimates with
the true solution. Use ε “ 10´6.ODEs: Initial Value Problems  561
E9.16 Solve the IVPs in E9.3 using the trapezoidal rule and compare your estimates with the
true solution. Use ε “ 10´6.
E9.17 Solve the IVPs given in E9.3 using the midpoint rule and compare your estimates with the
true solution.
E9.18 Solve the IVPs given in E9.3 using the midpoint rule and compare your estimates with the
true solution.
E9.19 Solve the IVPs in E9.2 using the 3rd- and 4th-order Runge-Kutta schemes and compare
your estimates with the true solution.
E9.20 Solve the IVPs in E9.3 using the 3rd-order Runge-Kutta scheme and compare your esti￾mates with the true solution.
E9.21 Water flows out of a discharge tube (d “ 5 cm) located at
the bottom of a square-section tank with a side a “ 2 m; see Fig.
E9.21. The change of water depth in the tank with time, hptq, is
described with the following first-order IVP:
´dh
dt “ C π
4
ˆd
a
˙2a2gh, Hp0q “ 3.5 m
where C is the discharge coefficient (C “ 0.58), g is the acceleration
of gravity (9.81 m/s2), and d is the diameter of the discharge tube.
Fig. E9.21
Solve the given IVP with the 4th-order Runge-Kutta method using Δt “ 5 seconds until the tank
is completely empty. Plot the h ´ t distribution. How long does it take to empty the tank?
E9.22 The following first-order ODE describes the current Iptq passing an RL circuit:
dI
dt “ E
L sin ωt ´ R
L Iptq, ip0q “ 0
Note that, initially, no current flows through the circuit when it is off. For a circuit with E “ 110
V, L “ 1 H, ω “ 250, and R “ 50 Ω, use the 4th-order Runge-Kutta method to estimate the
circuit current at intervals of Δt “ 0.002 up to t “ 0.2 seconds.
E9.23 Water will be transported from a reservoir (D " d) to a destination much further away
(L " D). When the valve at the end of the pipe is brought to a fully open position, the transient
behavior of the water exit velocity V ptq is described with the following IVP:
dV ptq
dt `
1
2LV 2
ptq “ g
h
L, V p0q “ 0
where g is the acceleration of gravity, h is the water elevation in the reservoir, and L is the length of
the pipe. For L “ 10 m and h “ 1.5 m, use the 4th-order Runge-Kutta method and an increment
of Δt “ 0.01 second to estimate the time required for water exit velocity to reach its steady-state
value of V “ ?2gh.
E9.24 A sack of a salt compound is poured into a large tank containing 200 liters of water. The
salt concentration in the tank, Cptq in percent as a function of time, is described by the following
differential equation:
p120 ´ 5.55 Cq
dC
dt “ 0.235 p48 ´ 4.95Cqp23 ´ 1.2Cq, Cp0q “ 0
Initially, no salt is present Cp0q “ 0; i.e., the tank is filled with water only. Using the RK4 method
with Δt “ 1 s, estimate the transient concentration of the solution until the solution is 99%
saturated. Plot the C ´ t distribution.
E9.25 The rate equation for a reaction involving two gases (A and B) is expressed by the following562  Numerical Methods for Scientists and Engineers
ODE:
dx
dt “ kpPA ´ 2xq
2
pPB ´ xq, xp0q “ 0
where x is the partial pressure of the product, k is a constant given as 1.12ˆ10´7 mm´2s
´2, and
PA and PB are the partial pressures of A and B, respectively. For PA “ 359 mmHg and PB “ 400
mmHg, estimate the partial pressure up to t “ 250 s using the RK4 with Δt “ 1 s.
E9.26 Corn from a storage tank is pneumatically fed into a cylindrical spray dryer with a radius
of R “ 0.5 m. A grain of corn enters the dryer through a tube at r “ 0 (the center of the dryer)
with a speed of 50 m/s, and it travels toward the outer wall of the dryer under the influence of
the centrifugal forces. The governing differential equation for the speed V of a single grain of corn
as a function of the radial position r is dictated by the following first-order non-linear differential
equation:
V prq
dV
dr ` A V 3
prq “ B r, V p0q “ 50
where A “ 0.000285, B “ 5.96 ˆ 105, and 0 ď r ď R. Estimate the speed of the grain of corn as it
travels from the center point r “ 0 where it enters the dryer, until it hits the outer wall (r “ R),
using the RK4 scheme with Δr “ 0.025 m.
E9.27 A highly conducting plate (As “ 1 m2, ρ “ 800 kg/m3) is initially at ambient temperature
T8 “ 20˝C. At an instant t “ 0, a constant heat flux (q2
0 “ 8000 W/m2) is applied to one side
of the plate, while heat loss takes place from the other side through convection. The first-order
differential equation describing the mean transient temperature of the plate of thickness L is given
by:
dT
dt `
hAs
mc pTptq ´ T8q “ q2
0As
mc
where h “ 40 W/m2K is the convection transfer coefficient, c “ 500 J/kg¨K is the specific heat, and
L “ 1 cm is the thickness of the plate. (a) Use the RK4 scheme to estimate the mean temperature
of the plate in intervals of Δt “ 5 s for up to 6 minutes. (b) How long does it take for the plate
to reach 100˝C?
E9.28 An aluminum ball (ρ “ 2700 kg/m3, c “ 1035 J/kg¨K, ε “ 0.75) with a diameter of 5 cm
is maintained in the annealing furnace at a uniform temperature of 800 K. At t “ 0, it is removed
from the furnace and placed in a room (T8 “ 300 K, h “ 10 W/m2K) to cool off. The heat
balance, including the convection and radiation heat losses, yields the following nonlinear ODE:
mc dT
dt “ ´hAspT ´ T8q ´ εσAspT 4 ´ T 4
8q
where σ “ 5.67 ˆ 10´8 W/m2K4 is the Stefan-Boltzmann constant, As is the surface area, m is
the mass of the ball, c is the specific heat, and ε is the emissivity of aluminum. Use the RK4 with
Δt “ 60 s to estimate (a) the temperature variation of the ball for a period of 1 hour and (b) the
time required for the sphere to reach 400 K.
E9.29 A parachutist (m “ 66 kg) deploys his parachute as soon as he jumps from a helicopter
at 3500 meters altitude. The net equilibrium force acting on the parachutist gives the following
differential equation:
Fnet “ mdv
dt “ mg ´ C v
where v is the free fall speed, g “ 9.81 m/s2 is the acceleration of gravity, and C “ 13.2 kg/s is the
average drag coefficient experienced during the fall. Assuming that the speed of the parachutist
just before he jumps is v “ 0 (at t “ 0), estimate the parachutist’s terminal speed using the RK4
scheme with Δt “ 0.1 s.
Section 9.3 Numerical Stability
E9.30 Consider the following difference equations for the model IVP: y1 “ fpx, yq “ λy. Find
the order of the truncation error and determine the stability condition for each equation.ODEs: Initial Value Problems  563
(a) yi`2 “ 4yi`1 ´ 3yi ´ 2hfpxi, yiq, (b) yi`1 “ 1
3 p4yi ´ yi´1q ` 2h
3 fpxi`1, yi`1q
(c) yi`2 “ 2yi ´ yi`1 ` 3hfpxi, yiq, (d) yi`1 “ yi ` hfpxi`1{2, yi`1{2q pmidpoint methodq
(e) yi`1 “ yi `
h
2 tfpxi, yiq ` f pxi, yi ` hfpxi, yiqqu pHeun’s methodq
E9.31 Consider employing the RK4 method to get y1 “ fpx, yq “ λy. Find the amplification
factor for the RK4 and investigate its stability.
E9.32 Find the analytical solution for each of the IVPs given below and discuss whether they
are stiff or not.
(a) y1 “ λpy ´ sin 2xq ` 2 cos 2x, yp0q “ 1, (b) y1 “ y ´ y9, yp0q “ 1{10,
(c) y1 “ λpy ´ x2q ` 2x, yp0q “ 1, (d) y1 “ 1 ´ 104y2, yp0q “ 0,
(e) y1 “ ´λpy ´ e´xq ´ e´x, yp0q “ 0
Section 9.4 Multistep Methods
E9.33 Repeat E9.2 with the Adams-Bashforth methods (AB2 and AB3) using h “ 0.1. Calculate
the starting values using the true solution, and compare your estimates with the true solution.
E9.34 Repeat E9.3 with the Adams-Bashforth methods (AB2 and AB3) using h “ 0.1. Calculate
the starting values using the true solution, and compare your estimates with the true solution.
E9.35 A simplified heat transfer model for an isothermal cylinder leads to the following first-order
nonlinear IVP:
dΘ
dt ` λΘ4{3 “ 0, Θp0q “ Θ0 “ T0 ´ T8
where Θptq “ Tptq ´ T8 and Tptq, T8, and T0 are respectively the cylinder, ambient, and initial
temperatures, and λ is a constant. (a) Find the analytical solution of the IVP; (b) For T0 “ 100˝C,
T8 “ 15˝C, and λ “ 10´4 K´1{3{s, estimate the temperature over a 1-hour period with Δt “ 100
s intervals using the AB2, AB3, and AB4 schemes; (c) Estimate the time required for the cylinder
to cool to 30˝C. Note: Use the equivalent order Runge-Kutta method to obtain the starting values.
E9.36 Repeat E9.35(b,c) with the AM2, AM3, and AM4 schemes.
E9.37 When a spherically shaped glass particle is dropped into a non-Newtonian fluid, the equa￾tion of motion of the particle results in the following IVP:
du
dt ` apnqun ´ b “ 0, up0q “ u0
where u is the vertical velocity of the particle (m/s), u0 is the initial velocity, n is the flow behavior
index, and a and b are fluid and particle-dependent constants. For a fluid with n “ 0.65 (a “ 74.08
and b “ 10.979), the glass particle is released into the fluid with u0 “ 10´3 m/s. Estimate its
terminal velocity using the AB2, AB3, and AB4 schemes with Δt “ 2.5 ˆ 10´3s. Note: Use the
equivalent order Runge-Kutta method to obtain the starting values.
E9.38 Repeat E9.37 with the AM2, AM3, and AM4 schemes.
Section 9.5 Adaptive Step-Size Control564  Numerical Methods for Scientists and Engineers
E9.39 Perform five steps of the adaptive RK45 method on the IVPs. Use hmin “ 0.01, hmax “ 0.5,
ε “ 10´5, and h “ 0.2 as the initial step size.
(a) y1 “ y ´ xy2, yp0q “ 1, ytruepxq“p2e´x ` x ´ 1q
´1
(b) y1 “ p8 ´ 40xqy, yp0q “ 1, ytruepxq “ e8x´20x2
E9.40 Given below is a simple neutron population model (kinetic equation) that takes into ac￾count delayed neutrons during a nuclear reactor start-up:
dn
dt “ ρptq ´ β
Λ nptq ` λ Cptq, np0q “ 1 and Cptq “ 4pe
0.24t ´ e
´35t
q
where Λ, ρptq, and β denote mean generation time, reactivity, and delayed neutron fraction, and
Cptq is delayed neutron population. For the case of β “ 0.007, Λ “ 5 ˆ 10´5s, λ “ 0.08 s´1, the
kinetics equations are stiff. Use the adaptive RK45 scheme with h “ 0.1 as the starting time step
to obtain the first four steps of the numerical solution for ρ “ 0.75β. The true solution is given as
nptq “ 0.992188 e´35t ` 0.008513 e0.24t
. Note: Impose hmin “ 0.001, hmax “ 0.1, ε “ 10´5.
E9.41 An unsteady model for the mean temperature of a chemical reactor with reaction heat
generation and convective cooling leads to
dθ
dτ “ α exp ˆ
´βτ ´ γ
θpτ q
˙
´ λ θpτ q, θp0q “ 1
where α “ 5, γ “ 0.075, and λ “ 2 are the model constants, and θpτ q and τ denote dimensionless
temperature and time, respectively. Use the adaptive RK45 scheme with h “ 0.05 as the starting
time step to obtain the first four steps of the numerical solution for (a) β “ 0.05 and (b) β “ 0.5.
Note: Impose hmin “ 0.01, hmax “ 0.25, and ε “ 10´5.
E9.42 A mathematical model based on the net forces (gravity and drag) acting on a falling
spherical raindrop can be expressed as
du
dt “ g ´ κ u2
ptq, up0q “ 0, κ “ 3
4d
ρa
ρw
C
where u is the velocity (m/s), ρa “ 1.2 and ρw “ 1000 kg/m3 are respectively the densities
of air and water, d is the raindrop diameter (m), g “ 9.81 m/s2 is the acceleration of gravity,
and C “ 0.55 is the drag coefficient. Use the adaptive RK45 scheme with the starting time step
h “ 0.25 to estimate the falling speed of a raindrop (d “ 5 mm) for the first four time steps. The
true solution is given as uptq “ ag{κ tanh `
t
?κg˘
. Note: Impose hmin “ 0.01, hmax “ 0.25, and
ε “ 10´5.
E9.43 The probability of failure of a machine part subjected to a cyclic load is given by
dp
dt “ ´apptq ln ˆpptq
K
˙
, pp0q “ p0
where t is the operation time (hours), p is the probability (%), a and K are constants. For
p0 “ 10´5, a “ 1.25 ˆ 10´3, K “ 100, apply the adaptive RK45 scheme with the starting time
step h “ 200 hours to estimate the first four steps. Impose hmin “ 1, hmax “ 500, and ε “ 10´5.
The true solution is given as pptq “ Kpp0{Kq
expp´atq
Section 9.6 Predictor-Corrector Methods
E9.44 Repeat E9.2 using (i) Heun’s, (ii) AMB4, and (iii) Milne’s predictor-corrector methods
and compare your estimates with the true solutions.
E9.45 Repeat E9.3 using (i) Heun’s, (ii) AMB4, and (iii) Milne’s method predictor-corrector
methods and compare your estimates with the true solutions.ODEs: Initial Value Problems  565
E9.46 A simple RC circuit consists of a capacitor (C), a resistor (R), and a voltage source. Eptq
denotes the input voltage across the voltage source as a function of time. The voltage across the
capacitor satisfies the following differential equation:
RC dVc
dt ` Vcptq “ Eptq, Vcp0q “ Vc0
The voltage is constant Eptq “ E0 for 0 ď t ď t0, but then the voltage source is turned off
Eptq “ 0 (t ě t0). For t0 “ 2 s, E0 “ 8 V, R “ 2.5 Ω, and C “ 0.125 F, use (a) the Heun’s and
(b) the AMBF predictor-corrector methods with Δt “ 0.25 s to estimate the transient capacitor
voltage over a period of 5 s for Vc0 “ 0 and Vc0 “ 10 V cases. Compare your solutions with the
true solutions given below and assess the accuracy of the methods.
For Vc0 “ 0 V For Vc0 “ 10 V
Vcptq “ "8p1 ´ e´3.2t
q, 0 ď t ď 2
4816.76e´3.2t
, t ě 2 Vcptq “ " 8 ` 2e´3.2t
, 0 ď t ď 2
4806.76e´3.2t
, t ě 2
E9.47 A simplified mathematical model of the spread of an epidemic, also known as the Kermack￾McKendrick model, is given as follows:
dR
dt “ α tP ´ Rptq ´ Hptqu , Hptq “ H0 exp ˆ
´β
αRptq
˙
,
where α is the recovery rate of infected individuals, β is the rate of infection in healthy individuals,
and H, R, I, and P denote the healthy, recovered, infected, and total number of individuals in
the population, respectively. During the COVID-19 pandemic, in a country, no definite deaths
are attributed to the pandemic at a time t “ 0 and 143 per 10000 are infected, i.e., Rp0q “ 0
and Ip0q “ 143. For given P “ 10000, α “ 0.185/day, β “ 3.25 ˆ 10´5 /(day.person), use (a)
Heun’s, (b) Adams’ 4th, and (c) Milne’s 4th-order methods to estimate the number of infected and
recovered individuals at 1-week intervals (Δt “ 7 days) for a 3-month period. Use the RK4 scheme
to obtain the starting values. Note: At any time, the total number of healthy, recovered/deceased,
and infected individuals is constant over the simulation period, i.e., Iptq ` Hptq ` Rptq “ P.
E9.48 Air quenching is the process of attaining the desired properties in a product by cooling
metal objects in an air or inert gas environment. After reaching a uniform temperature of 1000K
in an annealing furnace, a steel ball (D “ 6 cm, m “ 0.89 kg) is removed from the furnace (t “ 0)
and left to cool in the ambient air. Assuming that the heat loss is primarily due to the convection
process, the conservation of energy yields
m d
dt pcppTqTptqq “ ´UA pTptq ´ T8q , Tp0q “ 1000 K
where T, m, and A are respectively the temperature (K), mass (kg), and surface area (m2) of
the ball, cppTq is the temperature-dependent specific heat (J/kg.K), T8 “ 290 K is the ambient
temperature, and U “ 90 W/m2¨K is the overall heat transfer coefficient. Use Heun’s, Adam’s,
and Milne’s methods with Δt “ 1 minute to estimate the transient temperature change of the
steel ball during the cooling. How long does it take for the temperature of the ball to reach 310
K? Obtain the starting values using the RK4 method. Given: cppTq “ 80.7 ` 0.945 Tptq J/kg¨K.
E9.49 Repeat E9.41 up to τ “ 2.5 using Heun’s, AB4, and Milne’s 4th-order methods.566  Numerical Methods for Scientists and Engineers
E9.50 A spherical tank of radius R has a drainage tap of radius r
at its bottom (see Fig. E9.50). The tank has an inlet tap at the top,
which maintains atmospheric pressure in the tank at all times. The
mathematical model that gives the liquid level in a spherical tank
that discharges its liquid is defined by the following first-order IVP:
dh
dt “ ´ A?2gh
b
π2h2ph ´ 2Rq
2 ´ A2
, hp0q “ h0
Fig. E9.50
where A is the cross-sectional area of the drainage tap. The drainage velocity can be computed
from
v “
b
2gh ` pdh{dtq
2
The tank (R “ 3 m) is filled up to 99% of its height (i.e., h0 “ 2Rp0.99q), which will be fully drained
by opening the discharge valve. Estimate (a) the liquid level and the exit (drainage) velocity until
the tank is fully drained; (b) the elapsed time to drain the tank using the Heun’s predictor-corrector
method with Δt “ 1, 5, and 10 s; (c) discuss the effects of changing the time steps.
Section 9.7 System of First-Order IVPs (ODEs)
E9.51 Consider the RLC circuit shown in Fig. E9.51. The current I
through the inductor and the voltage V across the capacitor C satisfy
the following coupled first-order ODEs:
C dV
dt “ Iptq ´ V ptq
R1
, V p0q “ 10 V
LdI
dt “ ´R2Iptq ´ V ptq, Ip0q “ 2 A Fig. E9.51
(a) Express the IVP in matrix form; (b) use the explicit Euler method with Δt “ 1 and 0.5 s to
estimate the voltage and current for a period of 4 s; (c) compare and discuss the accuracy of the
numerical estimates. The true solution of the system is given as Iptq“p2 ` α1q e´λ1t ´ α1 e´λ2t
,
V ptq“´α2 e´λ1t ` p10`α2q e´λ2t
. Given: R1 “ 10Ω, R2 “ 5 Ω,L “ 2 H, C “ 50 F, λ1 “ 2.49599,
λ2 “ 0.00601, α1 “ 2.0112683, and α2 “ 0.032168.
E9.52 Consider the following consecutive reaction, in which the first step is reversible.
X ÐÑ
k1
k´1
Y k2 ÝÑ Z
The reaction rates constitute a coupled first-order IVP, which can be expressed as follows:
$
’&
’%
dX
dt “ ´k1X ` k´1Y
dY
dt “ k1X ´ pk´1 ` k2qY
,
/.
/- , Xp0q “ 50 M,
Y p0q “ 0 M.
where k1 “ 1, k´1 “ 0.5, and k2 “ 0.1 s´1. (a) Express the IVP in matrix form; (b) Use the
explicit Euler method with Δt “ 1 and 0.5 s to estimate X and Y concentrations for a period of 4
s; and (c) compare and discuss the accuracy of your numerical estimates. Given: The true solution
is Xptq “ 31.805 e´α1t ` 18.195 e´α2t
, Y ptq “ 34.02 `
´e´α1t ` e´α2t
˘
, where α1 “ 1.53485 and
α2 “ 0.065153.
E9.53 Two bodies (m1 and m2) of equal size, with heat capacities c1 and c2, are brought into
thermal contact and suspended in a room with a convection transfer coefficient h and ambient
temperature T8 “ 5˝ C. The area and conductivity of the contact surface are denoted by Ac andODEs: Initial Value Problems  567
k. The conservation of energy leads to the following first-order coupled ODEs (IVP).
m1c1
dT1
dt “ ´kAc
L pT1 ´ T2q ´ hApT1 ´ T8q, T1p0q “ 100o
C
m2c2
dT2
dt “ ´kAc
L pT2 ´ T1q ´ hApT2 ´ T8q, T2p0q“´10o
C
where L is the thickness of the contact material, and T1 and T2 are the bulk temperatures of the
bodies. (a) Express the IVP in matrix form; (b) use the explicit Euler method Δt “ 1 and 0.5 s
to estimate the temperatures T1 and T2 for a period of 4 seconds if m1c1 “ 100 J/K, m2c2 “ 120
J/K, kAc{L “ 5 W/K, and hA “ 30 W/K; (c) Compare and discuss the accuracy of the numerical
estimates. The set of true solutions is given as
T1ptq “ 5 ` 80 e
´α1t ` 15 e
´α2t and T2ptq “ 5 ´ 40 e
´α1t ` 25 e
´α2t
where α1 “ 3{8, and α2 “ 4{15.
E9.54 Repeat E9.51; (a) obtain 2nd and 3rd-degree Taylor polynomial approximations, and (b)
estimate X and Y up to t “ 4 s using Δt “ 1 and 0.5 s.
E9.55 Repeat E9.52; (a) obtain 2nd and 3rd-degree Taylor polynomial approximations; (b) esti￾mate V and I up to t “ 4 s using Δt “ 1 and 0.5 s.
E9.56 Repeat E9.53; (a) obtain 2nd and 3rd-degree Taylor polynomial approximations; (b) esti￾mate T1 and T2 temperatures up to t “ 4 s using Δt “ 1 and 0.5 s.
E9.57 Use a Taylor polynomial approximation to derive a third-order explicit scheme to solve
the given coupled first-order ODEs.
dY1
dt “ ´2Y1 ´ Y2 ` x ` 1, dY2
dt “ Y1 ` 2Y2 ` 3 ´ x
E9.58 Below are coupled first-order linear IVPs with constant coefficients. (a) Express the system
in matrix form; (b) use the RK4 and BDF4 schemes to estimate the solution with the specified
step size and specified range; (c) compare your estimates with the true solutions and discuss the
accuracy of the methods. Note: Use the true solution to find the required starting values.
(a) Y 1
1 “ ´2Y1 ´ 3Y2 ` 9ex, Y1p0q“´10 Y1,truepxq “ 2ex ´ 12e´x, h “ 0.2 r0, 2s
Y 1
2 “ ´Y1 ` 3Y2 ´ 28e´x, Y2p0q “ 5, Y2,truepxq “ ex ` 4e´x
(b) Y 1
1 “ Y1 ´ 2Y2 ` 31x2 ´ 18x, Y1p0q“´2, h “ 0.1 r0, 1s
Y 1
2 “ ´3Y1 ` 6Y2 ` 28x3 ` 62x ´ 10, Y2p0q“´1,
Y1,truepxq “ 2x4 ` 10x3 ` x2 ´ 2, Y2,truepxq “ x4 ` x3 ` x2 ´ 10x ´ 1
(c) Y 1
1 “ ´2Y1 ` 2Y2 ´ 8x, Y1p0q “ 2, h “ 0.1 r0, 1s, Y1,truepxq “ 3 ´ 2x ´ 2e´3x ` e2x,
Y 1
2 “ 2Y1 ` Y2 ` 2x ´ 6, Y2p0q “ 5, Y2,truepxq “ 2 ` 2x ` e´3x ` 2e2x
(d) Y 1
1 “ ´Y1 ´ Y2 ` px2 ` 1qe´x, Y1p0q “ 1, h “ 0.1 r0, 1s, Y1,truepxq “ xe´x ` e´2x
Y 1
2 “ ´Y1 ´ Y2 ` 3xe´x Y2p0q “ 1, Y2,truepxq “ x2e´x ` e´2x
(e) Y 1
1 “ ´Y1 ` Y2 ` `
1 ´ 1
2 x2˘
e´x, Y1p0q “ 1
4 , h “ 0.1 r0, 1s, Y1,truepxq “ e´xx ` 1
4 ex
Y 1
2 “ Y1 ´ Y2 ` 3
4 ex, Y2p0q “ 1
2 , Y2,truepxq “ 1
2 px2e´x ` exq
(f) Y 1
1 “ ´Y1pxq ` 8 ´ e´x, Y1p0q “ 2, h “ 0.1 r0, 1s, Y1,truepxq “ 8 ´ px ` 6qe´x
Y 1
2 “ ´Y1ptq ´ Y2ptq ` e´x ´ 2, Y2p0q “ 0, Y2,truepxq “ 1
2 px2 ` 14x ` 20qe´x ´ 10568  Numerical Methods for Scientists and Engineers
E9.59 Below are coupled first-order linear IVPs with variable coefficients. (a) Express the system
in matrix form; (b) use the RK4 and BDF4 schemes to estimate the solution with the specified
step size and specified range; (c) compare your estimates with the true solutions and discuss the
accuracy of the methods. Note: Use the true solution to find the required starting values.
(a) Y 1
1 “ xY1 ´ Y2 ´ x3, Y1p0q “ 3, h “ 0.1 r0, 1s, Y1,truepxq “ x3 ` x2 ` 3x ` 3
Y 1
2 “ 4Y1 ´ 4xY2 ` 4x5 ´ 24x ´ 11, Y2p0q“´3, Y2,truepxq “ x4 ` x ´ 3
(b) Y 1
1 “ ´xY1 ` Y2 ` 3, Y1p0q “ 2, h “ 0.2 r0, 2s, Y1,truepxq “ 2 ` 3x
Y 1
2 “ px2 ´ 2qY1 ´ xY2 ` 12x ` 6, Y2p0q “ 0, Y2,truepxq “ 2x ` 3x2
(c) Y 1
1 “ p1 ` e´xq Y1 ´ exY2 ` ex, Y1p0q “ 0, h “ 0.1 r0, 1s, Y1,truepxq “ xex
Y 1
2 “ e´xY1 ´ Y2 ` e´x ´ x, Y2p0q “ 0, Y2,truepxq “ xe´x
(d) Y 1
1 “ ´5 Y1 ´ 2xY2 ` 8e´x, Y1p0q “ 2, h “ 0.1 r0, 1s, Y1,truepxq“p2 ´ x2qe´x
Y 1
2 “ ´4Y1 ´ 2xY2 ` 9e´x Y2p0q “ 1, Y2,truepxq“p2x ` 1qe´x
E9.60 One-delayed group kinetics equations for a nuclear reactor are given as
dn
dt “ ρptq ´ β
Λ nptq ` λCptq,
dC
dt “ β
Λnptq ´ λCptq.
,
np0q “n0
Cp0q “C0 “ β
λΛn0
where nptq is the neutron density, Cptq is the delayed neutron concentration, Λ is the mean
generation time, ρptq is called the reactivity, and β is the delayed neutron fraction. The reactor
initially operates with a steady neutron population of n0, and a step reactivity change of ρptq “
ρ0 “constant is introduced at t “ 0. Having normalized the neutron density (n0 “ 1), solve the
kinetics equations to estimate n and C for a period of 1 second using the RK2 and RK4 with
intervals Δt “ 0.1, 0.05, and 0.025 s for the input of ρ0 “ 0.75β. The U-235 data are β “ 0.007,
Λ “ 5 ˆ 10´5 s, and λ “ 0.08 s´1. The true solutions are given as
nptq “ 3.946425 e
α1t ´ 2.946425 e
´α2t Cptq “ 1738.294 e
α1t ` 11.70615 e
´α2t
where α1 “ 0.23784 and α2 “ 35.31784.
E9.61 Repeat E9.60 using the same data using the AB4 with Δt “ 0.025 s for (a) step insertions
of ρptq “ 0.25β, 0.50β, 0.75β, and (b) ramp insertions of ρptq “ 0.25βt, 0.50βt, and 0.75βt. Make
xy-plots for each case and determine the time for neutron density to reach nptq{n0 “ 103.
E9.62 A simple mathematical (linear) model for simulating a car crush (see Fig. E9.62) is given
below:
dv
dt “ ´ k
mxptq ´ b
mvptq
dx
dt “vptq
xp0q “ 0
vp0q “ v0
Fig. E9.62
where x denotes the displacement of the car and the deformation of the spring, v is the velocity
of the car and the relative velocity across the damper, b is the viscous damping coefficient, and
k is the stiffness given as k “ 1206 N/m. Initially, the car (m “ 890 kg) is moving at a constant
speed of v0 “ 50 km/h. (a) Obtain the analytical solution of the model. (b) For over a period
of 0.2 s, simulate the car crash for (i) undamped (b “ 0) and (ii) damped (b “ 25000 kg/s)
cases using Heun’s predictor-corrector method; plot the displacement (xptq) and deceleration (a “ODEs: Initial Value Problems  569
pkx`bvq{m) of the car and the total force (F “ ma “ kx`bv) acting on the barrier as a function
of time; and (c) compare your estimates with the true solution you found in Part (a).
E9.63 Use the RK4 with Δt “ 0.2 to estimate (a) the altitude of the parachutist in E9.29 during
the first 27 seconds of descent and (b) the time elapsed to land on the ground.
Section 9.8 High-Order ODEs
E9.64 The ODEs given below are second-order IVPs. (a) Convert the IVPs into a set of coupled
first-order IVPs, and (b) use the RK4 method with h “ 0.1 to estimate the solution.
(a) y2 ` 25y1 ` 25y “ 0, yp0q “ 0, y1
p0q “ 1 r0, 3s, ytruepxq “ 1
24 pe
´x ´ e
´25xq
(b) y2 ` xy1 ´ 2y “ 0, yp0q “ 1, y1
p0q “ 0, r0, 1s, ytruepxq “ 1 ` x2
(c) yy2 ` py1
q
2 “ 3x, yp1q “ 1, y1
p1q “ 3{2, r1, 2s, ytruepxq “ x3{2
(d) py1
q
2 ´ y2 ´ 4y ` 1 “ 0, yp0q “ 0, y1
p0q“´1, r0, 1s, ytruepxq “ x2 ´ x
(e) x cos2
p1{xqy2 ` 2cos2
pyq y1 “ 0, yp1q “ 1, y1
p1q“´1 r1, 2s, ytruepxq “ 1{x
(f) y2 ´ 2xyy1 ´ y2 ` 1 “ 0, yp0q “ 0, y1
p0q “ 0, r0, 1s, ytruepxq“p1 ´ e
x2
qp1 ` e
x2
q
E9.65 Consider a mass m attached to the end of a pendulum (see
Fig. E9.65). Neglecting the air resistance, the motion of the pendu￾lum is governed by the following second-order nonlinear differential
equation:
d2θ
dt2 “ ´ g
L sin θ, θp0q “ θ0, dθp0q
dt “ ω0
where L, ω0, and θptq denote the length, initial angular velocity,
and angular displacement, respectively, and g is the acceleration
of gravity. Derive an explicit third-order scheme using the Taylor
polynomial approximation.
Fig. E9.65
E9.66 Consider the motion of the pendulum in E9.65, where L “ 0.25 m, θp0q “ π{4, and
ωp0q “ 0. Using the RK4 method with Δt “ 0.1 s increments, (a) estimate the angular velocity
and displacement of the pendulum for a period of 2 seconds (tabulate the results as θ ´ t and
θ1 ´ t), and (b) find the period of the pendulum using the results found in Part (a) and compare
it with the theoretical value found from T “ 2π
aL{g.
E9.67 The charge, q, in a simple RLC circuit is governed by the following differential equation:
d2q
dt2 `
R
L
dq
dt `
1
LCq “ 1
LEptq, qp0q “ 2, ip0q “ C dq
dt “ 1
3
where L “ 1 is the inductor, C is the capacitor, R is the resistor, and E is the potential. For a
circuit with L “ 1 H, C “ 1{9 F, R “ 10 Ω, and Eptq “ 2 sinpt{2q V, estimate the charge and the
current using the RK4 method with Δt “ 0.25 s for a period of 9 s after the circuit is turned on.
Tabulate the results as q ´ t and dq{dt ´ t.
E9.68 The motion of a nonlinear spring is governed by the so-called Duffing equation.
d2x
dt2 ` α dx
dt ` β xptq ` γ x3
ptq “ F0 cos ωt, xp0q “ A, x1
p0q “ B
where x is the displacement. For α “ 1, β “ γ “ 20, F0 “ 0, A “ 0.1, and B “ 0, use the RK4
method with Δt “ 0.1 s to solve the Duffing equation up to t “ 9 s and tabulate the results as
x ´ t and V ´ t.570  Numerical Methods for Scientists and Engineers
E9.69 The motion of a nonlinear, damped Van der Pol oscillator is given by the following second￾order nonlinear differential equation:
d2x
dt2 ´ μp1 ´ x2
q
dx
dt ` x “ 0, xp0q “ 0, x1
p0q “ V p0q “ υ
where x is the displacement, μ is a damping coefficient, and V is the velocity. For μ “ 2 and
υ “ 0.5, solve the Van der Pol equation using the RK4 method with Δt “ 0.2 s for a period of 30
s and tabulate the estimates as x ´ t and V ´ t.
Section 9.9 Simultaneous Nonlinear ODEs
E9.70 Below are the coupled first-order nonlinear ODEs with variable coefficients. Use the RK4
scheme to estimate the solution for the specified interval and step size.
(a) Y 1
1 “ x ` aY 2
1 ` Y 2
2 , Y 1
2 “ Y1 p1 ` Y1q , Y1p0q “ 0, Y2p0q “ 0, h “ 0.2 r0, 2s
(b) Y 1
1 “ Y1e´xY2 ` 1, Y 1
2 “ Y1 sin x ´ Y2 cos x, Y1p0q “ 0, Y1p0q “ 1, h “ 0.2 r0, 2s
(c) Y 1
1 “Y1 sinp2x`Y1Y2q, Y 1
2 “π´Y2 cospx´Y1Y2q, Y1p1q“1, Y2p1q“´1, h“0.1 r1, 2s
(d) Y 1
1 “ sinpY1 ` xq ` sinpY2 ` xq, Y 1
2 “ cospY1 ` xq ` cospY2 ` xq
Y1p0.5q “ 0.25, Y2p0.5q “ 0.5, h “ 0.2 r0.5, 2.5s
E9.71 The following simple two-variable (Healthy and Infected) population model has been pro￾posed to simulate the COVID-19 outbreak in a small country:
$
’’’&
’’’%
dH
dt “ ´ 0.009HptqIptq,
dI
dt “0.009HptqIptq ´ 0.22Iptq
,
///.
///-
Hp0q “ 99.9 p99.9%q
Ip0q “ 0.1 p0.10%q
where H and I denote the percentage of the healthy and infected populations, and t denotes time
(in days). Assuming that only one in a thousand is infected at the start of the simulation (t “ 0),
forecast the healthy and infected populations over a 30-day period. Use the Euler, RK2, and RK4
methods with time steps of Δt “ 1 day.
E9.72 Solve the given second-order initial value problem numerically on 0 ă x ď 3 using the
fourth-order Runge-Kutta scheme with a step size of h “ 0.2.
y2 ´ y y1 “ 0.1 x2
, yp0q “ 1, y1
p0q “ 0
E9.73 Solve the given second-order IVP on 0 ă x ď 2 using the fourth-order Runge-Kutta scheme
with a step size of h “ 0.2.
Y 1
1 “ 1 ` Y1p1 ´ Y1Y2q, Y 1
2 “ e
´x ´ Y2pY1 ` 2q, Y1p0q “ 1, Y2p0q “ 1
9.12 COMPUTER ASSIGNMENTS
CA9.1 Write a computer program to solve the IVP given in E9.57, subject to Y1p0q “ 0 and
Y2p0q “ 0 conditions using the 2nd and 3rd-degree Taylor polynomial approximations with h “ 0.1ODEs: Initial Value Problems  571
increments up to x “ 2, and compare your estimates with the true solution given below.
Y1,truepxq “ 1
9
p2
?
3 sinhp
?
3xq ´ 12 coshp
?
3xq ` 3x ` 12q,
Y2,truepxq “ 1
9
p8
?
3 sinhp
?
3xq ` 18 coshp
?
3xq ` 3x ´ 18q
CA9.2 Consider a cylindrical tank (H “ D “ 4 m) with a discharge
pipe of diameter d “ 5 cm, as shown in Fig. CA9.2. The tank is
initially empty and is filled with water at a rate of 1.8 m3/min. The
water level in the tank, h, is governed by the following first-order
nonlinear differential equation:
dh
dt “ Q
At
´
ˆ d
D
˙2a2g hptq, hp0q “ 0 Fig. CA9.2
where At “ πD2{4 is the tank cross section, Q is the rate of inflow water (m3/s), and g is the
acceleration of gravity. Write a program using the RK4 method (Δt “ 0.5 min) (a) to estimate
the filling time, instantaneous water level, and discharge rate until the water level reaches 95% H.
Once 95% H is reached, turn off the filling tab and let the water drain from the tank. How long
does it take to discharge water completely from the tank?
CA9.3 The air-quenching problem presented in E9.48 is extended to include radiation heat losses.
At t “ 0, a steel ball (D “ 6 cm, m “ 0.89 kg) with a uniform temperature of 1000 K is removed
from the furnace and left to cool in the ambient air. The specific heat and convection heat transfer
coefficients are given as quadratic functions of temperature. The conservation of energy, including
radiation and convection heat losses, yields
m d
dt pcppTqTptqq “ ´UpTf qA pTptq ´ T8q ´ σA `
T 4
ptq ´ T 4
8
˘
, Tp0q “ 1000 K
where T, m, and A are respectively the temperature (K), mass (kg), and surface area (m2) of
the ball, cppTq is the temperature-dependent specific heat (J/kg.K), Tf is the film temperature
computed as pTptq ` T8q{2, T8 is the ambient temperature (290K), and U is the overall heat
transfer coefficient (W/m2¨K). Use the 4th-order Adam’s method with Δt “ 60 s (1 min) to
simulate the cooling of the steel ball. Determine the following: (a) How long does it take for the
ball to reach 310 K? Plot the cooling curve (temperature vs. time), (b) compute and plot the
instantaneous convection and radiation heat transfer rates (qconvptq “ UpTf qA pTptq ´ T8q and
qradptq “ σA `
T 4ptq ´ T 4
8
˘
), (c) obtain the cooling curve for the constant property assumption
(with mean values of cp “ 644 J/kg¨K and U “ 202 W/m2¨K) and compare the results with those
in Part (a).
Given: cppTq“769´1.589T `0.00196T 2 J/kg¨K and UpTq“80`0.2226T ´4.9ˆ10´5T 2, W/m2¨K.
CA9.4 Consider the following coupled first-order system of nonlinear equations:
Y 1
1 “ cY2pxq, Y 1
2 “ c p1 ´ Y1pxqq , Y1p0q “ 0, Y2p0q “ 0
where c is a constant. Use the trapezoidal rule, RK4, BDF2, BDF4, and AM4 methods to estimate
the solutions for c “ 10, 102, 103, and 103 on [0,0.1] and discuss the accuracy of the solutions.
The true solutions are Y1pxq “ 1 ´ cospcxq and Y2pxq “ sinpcxq.
CA9.5 A simple three-variable (healthy, infected, and recovered) mathematical model for the
COVID-19 epidemic can be expressed as
dH{dt “ ´0.0061HptqIptq ` 0.0014Rptq, Hp0q “ 99.9, p99.9%q
dI{dt “ 0.0061HptqIptq ´ 0.44Hptq Ip0q “ 0.1 p0.10%q
dR{dt “ 0.44Iptq ´ 0.0014Rptq Rp0q “ 0.572  Numerical Methods for Scientists and Engineers
where t is the time (days), H, I, and R denote the percentage of the Healthy, Infected & deceased,
and infected & Recovered population, respectively. Assuming that only one in a thousand is
infected, use the implicit Euler’s, RK2, and RK4 methods with time steps of Δt “ 1 day to
forecast the healthy, infected-deceased, and infected-recovered populations over a period of 120
days.
CA9.6 Consider a bidirectional 2X ` Y2 Õ 2Z reaction, which may be represented by a couple
of uni-directional (forward and backward) reactions:
2X ` Y2
k1 ÝÑ 2Z 2Z k2 ÝÑ 2X ` Y2
where k1 “ 0.5 and k2 “ 0.1 s´1 are the reaction rate constants. This leads to the following set of
non-linear ordinary differential equations:
dCx{dt “ 2k2C2
z ptq ´ 2k1C2
xptqCyptq, Cxp0q “ 1
dCy{dt “ k2C2
z ptq ´ k1C2
xptqCyptq, Cyp0q “ 1
dCz{dt “ 2k1C2
xptqCyptq ´ 2k2C2
z ptq, Cxyp0q “ 0
where Cx, Cy, and Cz are the concentrations of X, Y , and Z, respectively. Perform a numerical
simulation of the chemical process for a period of 15 seconds. (a) Use a suitable numerical method
to solve the pertinent equations and obtain the Cx, Cy, and Cz´t distributions. (b) Investigate the
effect of k1 using k1 “ 0.001, 0.5, and 250 1/M.s. (c) Investigate the effect of k2 using (k2 “ 0.001,
0.1, and 1000).
CA9.7 The mass of an engine block (see Fig. CA9.7), whose stiffness
and damping coefficients are given respectively as k “ 2.714 ˆ 106
N/m and b “ 325 N/m¨s, is 110 kg. Upon ignition, the block is
subjected to an input force of Fptq “ F0 sin ωt with F0 “ 3π N and
ω “ ω0 “ 50π rad/s. The displacement yptq of the engine block
relative to its equilibrium position is expressed by the following
second-order differential equation:
md2y
dt2 ` b
dy
dt ` kyptq “ Fptq Fig. CA9.7
Using the given data and the AB4 predictor-corrector method to estimate the displacement y,
vertical velocity of the block v “ dy{dt, power input Pinput “ Fptqvptq, power dissipated Pdiss “
bv2ptq, and power stored Pstored “ Pinput´Pdiss in the range [0,4] seconds. (a) First, find a suitable
Δt that gives a consistent solution for the [0,4] interval. Plot t´y, t´Pinput, t´Pdiss, and t´Pstored.
Repeat the solution for ω “ ω0{3 rad/s.
CA9.8 Consider the following chemical reactions and their kinetic behavior:
A ÐÑ
k1
k´1
B, B k2 ÝÑ C
The reaction rate equations and the initial conditions are given by
dCA{dt “ ´k1CA ` k´1CB, CAp0q “ 0.10,
dCB{dt “ k1CA ´ pk´1 ` k2qCB, CBp0q “ 0,
dCC {dt “ k2CB, CC p0q “ 0.
where k1 “ 0.02, k´1 “ 0.01, and k2 “ 0.15 s´1. (a) Express the pertinent equations of motion
as a first-order coupled ODE-IVP. (b) Use a suitable method to perform a numerical simulation
to determine the effects of (i) k1 (i.e., simulate k1 “ 0.005, 0.02, and 0.10); (ii) k´1 (i.e., simulate
k´1 “ 0.01, 0.1, and 0.5); (iii) k2 (i.e., simulate k2 “ 0.15, 0.60, and 1.8); Plot θ ´ t, ω ´ t, v ´ t
and � ´ t distributions for each case.ODEs: Initial Value Problems  573
CA9.9 A suspension system of a wheel of an automobile, depicted in Fig. CA9.9, can be expressed
by the following coupled second-order differential equations:
d2x1
dt2 `
d1
m1
˜
dx1
dt ´ dx2
dt ¸
`
k1
m1
px1 ´ x2q “ 0
d2x2
dt2 `
d1
m2
˜
dx2
dt ´ dx1
dt ¸
`
k1
m2
px2 ´ x1q `
k2
m2
px2 ´ x3q “ 0
where m1 is the quarter of the mass of the automobile (m1 “ 245 kg), m2 is the mass of the wheel￾axle (m2 “ 25 kg), k1 “ 15000 N/m is the main spring constant of the automobile, k2 “ 55000
N/m is the spring constant of the wheel, d1 “ 500 N.s/m is the shock absorber constant, x1 and
x2 are respectively the vertical displacements of the automobile body and the wheel, and x3 is
the input road disturbance. Initial conditions are given as
x1p0q “ x2p0q “ dx1
dt p0q “ dx2
dt p0q “ 0, x3ptq “
$
’’’’’&
’’’’’%
h1
2 p1 ´ cos 2πtq , 1 ď t ď 2
h2
2 p1 ´ cos πtq , 6 ď t ď 8
0, otherwise
where h1 “ 0.125 m and h2 “ 0.10 m denote the amplitudes of the bump disturbances.
Fig. CA9.9
Perform a numerical simulation of this problem for a period of 20 seconds. (a) Use a suitable
numerical method to solve the pertinent equations and obtain x1, x2´t and x9 1, x9 2´t distributions.
How does the system react to the road disturbance? (b) Investigate the effect of the spring constant
of the wheel by simulating with k2 “ 40000, 55000, and 70000 N/m; (c) Investigate the effect of
the spring constant of the automobile by simulating with k1 “ 7000, 15000, and 22500 N/m; (d)
Investigate the effect of the spring constant of the automobile by simulating with d1=250, 500,
and 1000 N.s/m.CHAPTER 10
ODEs: Boundary Value
Problems
LEARNING OBJECTIVES
After completing this chapter, you should be able to
‚ identify two-point boundary value problems (BVPs) as linear or nonlinear;
‚ recognize Dirichlet, Neumann, Robin, and periodic boundary conditions;
‚ explain the objective of the finite-difference method to solve linear BVPs;
‚ understand the concept of order of error and its implications;
‚ describe the steps for numerically solving the linear BVPs;
‚ apply the finite-difference method to solve a two-point linear BVP;
‚ discuss and implement non-uniform grids for solving linear BVPs;
‚ understand and implement the Finite Volume Method (FVM) for linear BVPs;
‚ implement the shooting method for linear or nonlinear two-point BVPs;
‚ apply the finite difference method to fourth-order linear BVPs.
B OUNDARY Value Problems (BVP) arise generally when attempting to find steady￾state solutions to physical events. Mathematical models of events in steady-state de￾pend only on spatial variables (x, y, z). Since steady-state events do not change with time,
the time derivatives of the dependent variables are zero. In broader terms, boundary value
problems consist of partial differential equations (PDEs). However, in this chapter, the dis￾cussion on the BVPs will be restricted only to one dimension that leads to an ordinary
differential equation (ODE). Problems, in which the solution, its derivatives, or a combi￾nation of both are specified at only two points (i.e., boundary points) are called two-point
boundary value problems.
In this chapter, the finite difference method for the solution of two-point boundary value
problems (BVPs) will be discussed. Using the finite difference and finite volume methods,
the numerical solution of the second-order linear ordinary differential equations subjected
to Dirichlet, Neumann, and/or Robin boundary conditions is discussed in detail. The im￾plications of uniform and non-uniform gridding and the solution of the two-point BVP
problem with non-uniform grids are examined. The solution of nonlinear two-point BVPs
with a finite difference and shooting methods are discussed in detail. Finally, the solution
of two-point fourth-order linear BVP is covered.
574 DOI: 10.1201/9781003474944-10ODEs: Boundary Value Problems  575
FIGURE 10.1: (a) Heat conduction in a rod and a predicted solution; (b) Lateral deflection
of a beam and a predicted solution.
10.1 INTRODUCTION
A two-point boundary value problem in one dimension is an ODE with two boundary
conditions. Consider the following second-order ordinary differential equation (ODE):
ppxqy2pxq ` qpxqy1
pxq ` rpxqypxq “ gpxq, a ď x ď b
where x and y denote the physical quantities (independent and dependent variables, respec￾tively). It should be noted throughout this chapter that the term boundary value problem
is used to refer to two-point boundary value problem.
Having an ODE for a physical event alone is not sufficient to obtain a unique solution
to the problem it represents. Boundary value problems may have no solution, only one
solution, or an infinite number of solutions. In the case of having many solutions, additional
equations are required to be able to find its unique and complete solution. These equations,
referred to as boundary conditions (BCs), arise as a result of the system requirements or
constraints, which specify the behavior of the physical phenomenon at the boundary of
a system. In fact, boundary conditions represent the influence of the environment on the
isolated model.
In two-point BVPs, the independent variable is the space variable, commonly denoted
by x. The number of BCs required to obtain a unique solution to a two-point BVP is equal
to the order of the ODE. Boundary conditions for system boundaries are constructed by
specifying the dependent variable and/or a linear combination with its derivative. In a two￾point BVP, the BCs are imposed at the end points of the solution interval, and the solution is
acquired only in the interval restricted with these two end points. In practice, most physical
phenomena occurring on boundaries of the system can be expressed mathematically with
the following generalized boundary conditions:
α1 y1
paq ` β1 ypaq “ γ1, α2y1
pbq ` β2ypbq “ γ2,
where α1, α2, β1, β2, γ1 and γ2 are constants.
To illustrate how BCs are determined, consider the example of a heat-conducting rod
extending along a wall (0 ď x ď L), as shown in Fig. 10.1a. It is assumed that the side576  Numerical Methods for Scientists and Engineers
surface of the rod is insulated, and the heat is transmitted longitudinally along the side
surface of the rod without any loss to the surrounding wall. The temperature distribution
Tpxq for this event is governed by the following ordinary differential equation:
d2T
dx2 “ 0, 0 ď x ď L
where the independent variable x and the dependent variable T denote the axial (longitudi￾nal) coordinate and longitudinal temperature, respectively. The analytical solution for this
ODE is Tpxq “ c1 `c2x, where c1 and c2 are arbitrary constants. This solution implies that
the temperature variation depicted in Fig. 10.1a should be linear, which is. Obviously, the
temperature distribution represents a family of solutions for all possible c1 and c2 pairs. To
obtain a unique solution for this specific case, we need to impose conditions that represent
the physics of the event as realistically as possible. For this example, we can set the temper￾atures at the boundaries to Tp0q “ 20˝C (left BC) and TpLq “ 200˝C (right BC), as shown
in Fig. 10.1a. Then, with the aid of the BCs, we can proceed to obtain a unique solution
(i.e., arbitrary constants, c1 and c2) for axial rod temperature.
Consider a beam with constant cross section and length L as an example of two-point
BVP, shown in Fig. 10.1b. It is fixed at one end and subjected to a singular lateral load
P at the other (free) end. The differential equation describing the lateral deflection of the
beam is described with
EI d2y
dx2 “ Mpxq, 0 ď x ď L
where y is the lateral deflection, x is the axial coordinate, EI is referred to as flexural
rigidity, and M is the bending moment.
The general solution of this ODE also results in an infinite number of solutions due to
the arbitrary constants in its homogeneous solution. To determine the lateral deflection of
this problem under consideration (i.e., a unique solution), the BCs describing the physical
phenomenon need to be specified. Notice that there is no deflection at the left end where the
beam is fixed to the wall, so we can set yp0q “ 0. On the other hand, the amount of downward
deflection on the free end of the beam is specified as δ, so we can set ypLq“´δ. (Pay
attention to how the coordinate system is defined.) Now that we have obtained the boundary
conditions for both ends, after defining problem-specific EI and Mpxq, it is possible to find
the solution for the deflection, ypxq, throughout 0 ď x ď L.
10.2 TWO-POINT BOUNDARY VALUE PROBLEMS
An ordinary differential equation with BCs defined at each endpoint of the solution interval
is referred to as a two-point BVP. There are numerous methods for solving such problems;
however, this section covers the most widely and commonly used finite-difference and finite￾volume methods.
The Finite Difference Method (FDM) is one of the oldest and most widely used nu￾merical methods for solving ODE-BVPs. In this section, the implementation of the method
on two-point linear BVPs subjected to various BCs will be illustrated step by step.
Consider the following two-point BVP:
ppxqy2pxq ` qpxqy1
pxq ` rpxqypxq “ gpxq, a ă x ă b
α1y1
paq ` β1ypaq “ γ1, α2y1
pbq ` β2ypbq “ γ2
(10.1)
where α1, α2, β1, β2, γ1, and γ2 are real constants, and ppxq, qpxq, rpxq, and gpxq may beODEs: Boundary Value Problems  577
continuous functions of x on [a, b]. Also, we assume that rpxq{ppxq ď 0 on [a, b], which is a
necessary condition for the existence of a unique solution. Note that Eq. (10.1) is defined
in a closed interval, a ă x ă b, where the BCs are defined only for both endpoints.
A general solution of an ODE has an infinite number of solutions. A general solution can
be found if an ODE can be solved analytically, but numerical solutions can only be obtained
when all the required BCs are stated. In any case, boundary conditions are necessary to
find a unique solution to an ODE. In practice, one of the four types of boundary conditions
is applied to a boundary: Dirichlet, Neumann, Robin, and periodic BCs. It is the physics of
a physical phenomenon that dictates the type of boundary condition.
10.2.1 DIRICHLET BOUNDARY CONDITION
A boundary condition is referred to as Dirichlet BC when the solution of a differential
equation, ypxq, is specified at the boundary. If the solution is known at one of the boundary
points, say x “ a, the BC is expressed as ypaq “ δ1, where δ1 is the known solution at the
left boundary. Using the general form of the BCs given in Eq. (10.1), the Dirichlet BC in
computer programs can be expressed by setting δ1 “ γ1{β1 and α1 “ 0. In the example
depicted in Fig. 10.1a, the temperatures at the boundary points are known. When the
coordinate system is specified as the direction from x “ 0 to x “ L, the temperatures on
both sides of the wall (boundaries) can be expressed as Tp0q “ 20˝C and TpLq “ 200˝C. In
the beam example in Fig. 10.1b, the beam does not deflect at the fixed end, i.e., left BC is
ypaq “ 0 at x “ a. On the right end, the deflection is specified as ypbq“´δ, which is the
right BC.
10.2.2 NEUMANN BOUNDARY CONDITION
A boundary condition is referred to as a Neumann BC when only the derivative of ypxq is
specified at a boundary, that is, y1
paq “ λ1 or y1
pbq “ λ2. Neumann BC can be expressed in
terms of the general BC form by setting λ1 “ γ1{α1 or λ2 “ γ2{α2 with the corresponding
β1 “ 0 or β2 “ 0.
Numerous BCs with physical events fall into this category. For instance, y1
paq “ 0 is
also called the symmetry BC, which implies the physical property ypxq is symmetrical with
respect to x “ a. In other words, the physical property ypxq has a local maximum or local
minimum at x “ a. If a physical model and its BCs are symmetric, the numerical solution
is sought for only half of the system by applying the y1 “ 0 condition to the symmetry axis
or symmetry plane. In many cases, the derivative of the physical quantity, also called the
gradient, is known. For example, in heat transfer analysis, the flow of heat in the x-direction
is governed by Fourier’s law (q “ ´k dT{dx), where k is the conductivity of the medium, T
is the temperature, and q is the heat flux. Consider the rod depicted in Fig. 10.1a. If a heat
flux is specified (q0) at the right end of the rod (x “ L ), then the constant temperature
gradient condition is known at the right end, dTpLq{dx “ ´q0{k, but the temperature TpLq
is unknown.
10.2.3 ROBIN (MIXED) BOUNDARY CONDITION
A boundary condition is termed Mixed BC or Robin BC when a linear combination of ypxq
and y1
pxq is specified at a boundary, e.g., α1y1
paq`β1ypaq “ γ1 or α2y1
pbq`β2ypbq “ γ2. This
type of BC is also referred to as convection BC, where neither ypxq nor y1
pxq is known at the
boundary. In heat transfer, a convection BC at x “ a is imposed as ´kT1
paq “ hpTpaq´T8q,
which can be expressed as kT1
paq ` hTpaq “ hT8, i.e., α2 “ k, β2 “ h, γ2 “ hT8.578  Numerical Methods for Scientists and Engineers
FIGURE 10.2: Typical grid construction on [a, b].
10.2.4 PERIODIC BOUNDARY CONDITION
Up to this point, we have addressed the types of BCs that are expressed in terms of ypxq
and/or y1
pxq at a boundary. Another important type of BC encountered in practice is called
a periodic boundary condition. This BC type does not require additional equations for the
physical boundaries. A periodic boundary condition implies that the ODE’s solution on ra, bs
is periodic with a period T “ b ´ a. That is, the BCs can be simply stated as ypbq “ ypaq,
y1
pbq “ y1
paq, and so on.
In the subsequent illustrations, we will denote the symbol  for
a node whose solution is known (i.e., case of Dirichlet BC) and
the symbol b for a node whose solution is unknown (i.e., cases
of Neumann or Robin BC).
10.3 FINITE DIFFERENCE SOLUTION OF LINEAR BVPs
The finite difference method (FDM) is one of the most common numerical methods used
to solve two-point BVPs. The objective of the FDM is to find an approximate solution to
ypxq that satisfies the ODE and its BCs through a discrete approximation. The numerical
solution of Eq. (10.1) using the finite difference method is explained step by step below:
Step 1. Gridding: Gridding (or meshing) is a process of constructing discrete points for
which approximate solutions are sought. In this procedure, the solution interval (a ď x ď b)
is first divided into M subintervals. In most cases, a uniform interval size is applied, i.e.,
h “ xi`1 ´ xi “ pb ´ aq{M. Next, grid points (or nodes) are placed at the end points of
every subinterval. The nodes are then enumerated, starting from either 0 or 1, as depicted
in Fig. 10.2. The abscissas of the nodes are calculated from xi “ a ` ih (i “ 0, 1, 2,...,M),
but the corresponding ordinates, yi “ ypxiq’s, are the unknowns to be determined as the
objective of FDM.
If a physical property, ypxq, is expected to depict sharp changes
at either one or both end points of the solution interval, then
gridding should be carried out such that the nodal points are
clustered where sharp changes are expected.
Step 2. Discretizing: Discretization is the process of transforming a continuous function
satisfying an ODE, as well as its BCs, into a discrete function. An ODE is discretized at a
node xi, and the derivatives appearing in the ODE (y1
i, y2
i , and so on) are replaced by finite
difference approximations.
Consider the two-point BVP given with Eq. (10.1), which is satisfied for every x on
[a, b]. The ODE for the ith nodal point (x “ xi) is expressed as
ppxiqy2pxiq ` qpxiqy1
pxiq ` rpxiqypxiq “ gpxiqODEs: Boundary Value Problems  579
or, in shorthand notation,
piy2
i ` qiy1
i ` riyi “ gi, i“0, 1, 2,...,M (10.2)
where y2pxiq“y2
i , y1
pxiq“y1
i, ppxiq“pi,, etc.
To ensure that the order of global error is second order, we must use second-order finite
difference formulas for the first-, second-, or other derivatives, that is,
y1
i “ yi`1 ´ yi´1
2h ` Oph2q, y2
i “ yi`1 ´ 2yi ` yi´1
h2 ` Oph2q (10.3)
Substituting the CDFs in Eq. (10.3) into Eq. (10.2) yields
pi
´yi`1 ´ 2yi ` yi´1
h2
¯
` qi
´yi`1 ´ yi´1
2h
¯
` riyi “ gi, i “ 0, 1,...,M
and rearranging similar terms, we get
´ pi
h2 ´ qi
2h
¯
yi´1 `
´
ri ´ 2 pi
h2
¯
yi `
´ pi
h2 ` qi
2h
¯
yi`1 “ gi, i “ 0, 1,...,M (10.4)
Equation (10.4), referred to as the general difference equation, gives a relationship between
the discrete values of three consecutive nodes.
In the discretization of an ODE, all derivatives approximated by
the corresponding (forward, backward, and/or central) finite dif￾ference formulas should have the same order of truncation error. If
approximations with different orders are used (i.e., Ophq, Oph2q,
Oph3q, and so on), the smallest order of truncation error domi￾nates the order of global error of the ODE.
Step 3. Implementing BCs: A general difference equation is satisfied for every nodal
point whose ordinate (solution) is unknown. Such a node will hereinafter be referred to as
an unknown node. Difference equations for the unknown nodes are obtained by applying
Eq. (10.4) node as follows:
x “ a pfor i “ 0q b0 y´1 ` d0y0 ` a0y1 “ g0,
x “ x1 pfor i “ 1q b1y0 ` d1y1 ` a1y2 “ g1,
x “ x2 pfor i “ 2q b2y1 ` d2y2 ` a2y3 “ g2,
... ... ...
x “ xM´1 pfor i “ M ´ 1q bM´1yM´2 ` dM´1yM´1 ` aM´1yM “ gM´1,
x “ b pfor i “ Mq bMyM´1 ` dMyM ` aM yM`1 “ gM.
(10.5)
where bi “ ppi{h ´ qi{2q{h, ai “ ppi{h ` qi{2q{h, and di “ ri ´ 2pi{h2.
Equation (10.5) consists of M ` 1 equations and M ` 3 unknowns (y´1, y0, y1, ...,
yM´1, yM, yM`1). An unknown node at a specified point (xi) corresponds to a numerical
approximation of the true solution ypxiq. We note that the discretization procedure yields
two additional nodes, px´1, y´1q and pxM`1, yM`1q, marked by b in Fig. 10.3, which fall
outside the solution interval. These nodes are referred to as fictitious nodes and are elimi￾nated using the BCs. The interior nodes (marked by “o”) are the unknowns whose values
are to be determined.580  Numerical Methods for Scientists and Engineers
FIGURE 10.3: Discretization and fictitious nodes.
In this step, the number of equations and the number of unknowns are balanced, thus
creating a set of algebraic equations with a unique solution. In this regard, two equations
can be derived with the help of the BCs. For instance, if Dirichlet BCs are imposed on
both boundary points (ypaq “ A and ypbq “ B), then y0 and yM are known. In this case,
the nodal values from i “ 1 to (M ´ 1) are the unknowns, and the values of the fictitious
nodes (y´1 and yM`1) do not enter this picture. However, if one or both BCs are specified
as either Neumann or Robin BC, then the solutions at the boundary nodes (either y0 or
yM or both) are unknowns, and the discretization of the BCs provides additional difference
equations to eliminate the fictitious, y´1 and yM`1, nodes.
Step 4. Constructing a linear system: After incorporating the boundary conditions, a
unique system of linear equations can be constructed. The size of the linear system (M ´1,
M, or M ` 1) depends on the number of subintervals and the type of BCs applied to both
boundaries. Assuming a Neumann or Robin BC is applied to either boundary, the difference
equations corresponding to unknown nodes, including the boundary ones, can be expressed
in matrix equation form as
Ax “ r (10.6)
where x and r are column vectors defined as
x “ r y0 y1 y2 ¨¨¨ yM´1 yM s
T , r “ r g0 g1 g2 ¨¨¨ gM´1 gM s
T
and A is a matrix in tridiagonal form expressed as follows:
y´1 y0 y1 y2 ¨¨¨ yM´1 yM yM`1
i “ 0
i “ 1
i “ 2
.
.
.
i “ M ´ 1
i “ M
b0
A “
»
—
—
—
—
—
—
—
–
d0 a0
b1 d1 a1
b2 d2 a2
... ... ...
bM´1 dM´1 aM´1
bM dM
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
aM
Note that eliminating fictitious nodes modifies only the first and last rows of A and r.
Step 5. Solving the linear system: Once a system of linear equations is constructed (the
coefficient matrix and the rhs vector are properly defined), the numerical approximations
corresponding to the nodal points established in Step 1 are obtained by solving the system of
linear equations with a suitable direct or iterative method (see Chapters 2 and 3). A second￾order linear two-point BVP always results in a tridiagonal system, which is effectively solved
by Thomas’ algorithm (see Section 2.10); however, non-linear two-point BVPs also require
iterative algorithms (see Section 10.7).ODEs: Boundary Value Problems  581
FIGURE 10.4: Illustration of grids with Dirichlet BCs.
10.3.1 IMPLEMENTING DIRICHLET BCs
Consider applying Dirichlet BCs on both boundaries of Eq. (10.1):
ypaq “ A and ypbq “ B (10.7)
where A and B are real constants.
A discrete set (xi and yi, i “ 0, 1, 2,...,M) corresponding to a uniformly spaced grid
structure containing M subintervals in the interval [a, b] is illustrated in Fig. 10.4. In this
conjectural distribution, the boundary nodes have been marked with the symbol  because
they are specified with the Dirichlet BCs, i.e., ypaq “y0 “A and ypbq “yM “B. Thus, the
difference equations corresponding to the boundary nodes (i“0 and i“M) can be discarded,
leading to M ´1 equations with M ´ 1 unknowns (yi, i“1, 2,...,M ´1).
Even though the difference equations for i “ 0 and i “ M have been discarded, the
boundary nodal values (y0 and yM) are present (boxed) in the i “ 1 and i “ M ´1th
equations:
For i “ 1, b1 y0 ` d1y1 ` a1y2 “ g1
For i “ M ´ 1, bM´1yM´2 ` dM´1yM´1 ` aM´1 yM “ gM´1
Substituting y0 “A and yM “B into the difference equations and carrying them to the rhs,
only the unknowns remain on the left. Consequently, we obtain
d1y1 ` a1y2 “ g1 ´ b1A
b2y1 ` d2y2 ` a2y3 “ g2
.
.
.
bM´2yM´3 ` dM´2yM´2 ` aM´3yM´1 “ gM´2
bM´1yM´2 ` dM´1yM´1 “ gM´1 ´ aM´1B
(10.8)
Finally, the linear system, Equation (10.8), can be expressed in matrix form as follows:
y1 y2 y3 ¨¨¨ yM´2 yM´1
i “ 1
i “ 2
i “ 3
.
.
.
i “ M ´ 2
i “ M ´ 1
»
—
—
—
—
—
—
—
–
d1 a1
b2 d2 a2
b3 d3 a3
... ... ...
bM´2 dM´2 aM´2
bM´1 dM´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
–
y1
y2
y3
.
.
.
yM´2
yM´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
–
g1 ´ b1A
g2
g3
.
.
.
gM´2
gM´1 ´ aM´1B
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(10.9)
where gpxiq “ gi. The size of the resulting system of linear equations is pM ´ 1qˆpM ´ 1q,
and it is efficiently solved using Thomas’ algorithm.582  Numerical Methods for Scientists and Engineers
EXAMPLE 10.1: Boundary value problem with Dirichlet BCs
A beam of length L “ 2 m with a non-uniform cross section is supported at both
ends and carries a uniformly distributed load of w0 “ 4000 N/m, as shown in the
figure below. The displacement, ypxq, in the beam is governed by the following BVP:
EIpxq
d2y
dx2 “ w0
x
2
pL ´ xq, yp0q “ ypLq “ 0
where E “ 50 GPa is the elasticity modulus, and Ipxq is the inertia moment given as
Ipxq “ I0 e´0.2x, where I0 “ 10´6 m4. Obtain the numerical approximations using
the FDM with uniform spacings of h “ 0.4, 0.2, and 0.1. Also, find the true solution
to compare with your estimates.
SOLUTION:
Step 1: First, the solution domain, (0,2), is divided into uniformly spaced M
subintervals (h “ p2 ´ 0q{M), as shown in Fig. 10.5. The nodes are placed on both
sides of the subintervals and enumerated. The nodal solutions are denoted with
ypxiq “ yi for i “ 0, 1, 2,...,M, where xi “ ih’s are the abscissas. Since the BCs
are of Dirichlet type, the nodal solutions at the boundaries are already known, i.e.,
yp0q “ y0 “ 0 and yp1q “ yM “ 0. Thus, the total number of unknowns becomes
M ´ 1.
FIGURE 10.5
Step 2: The ODE can be written for an ith node within the solution range (1 ď
i ď M ´ 1) as
EI0e´0.2xi y2
i “ w0
xi
2 pL ´ xiq, 0 ă xi ă 2 (10.10)
Substituting the CDF of y2
i in Eq. (10.10) gives
EI0e´0.2xi
ˆyi`1 ´ 2yi ` yi´1
h2
˙
“ w0
xi
2 pL ´ xiq, (10.11)
Rearranging Eq. (10.11) and collecting similar terms, the general difference equation
is obtained:
yi`1 ´ 2yi ` yi´1 “ w0h2
2EI0
xipL ´ xiqe0.2xi , i “ 1, 2,...,pM ´ 1q (10.12)ODEs: Boundary Value Problems  583
Step 3: Making use of Eq. (10.12) for i “ 1, 2,...,pM ´ 1q, we get
For i “ 1 y0 ´ 2y1 ` y2 “ g1
For i “ 2 y1 ´ 2y2 ` y3 “ g2
.
.
. .
.
.
For i “ M ´ 2 yM´3 ´ 2yM´2 ` yM´1 “ gM´2
For i “ M ´ 1 yM´2 ´ 2yM´1 ` yM “ gM´1
(10.13)
where bi “ai “1, di “´2, and gi “w0h2xipL´xiqe0.2xi {2EI0. Notice that the boxed
quantities are the nodal boundary values whose solutions are known, i.e., y0 “0 and
yM “0.
Step 4: Upon substituting y0 “ 0 and yM “ 0 into Eq. (10.13) and collecting the
known quantities on the rhs, we obtain an pM ´ 1qˆpM ´ 1q linear system, which
can be expressed in matrix form as follows:
»
—
—
—
—
—
—
—
–
´2 1
1 ´2 1
1 ´2 1
... ... ...
1 ´2 1
1 ´2
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
–
y1
y2
y3
.
.
.
yM´2
yM´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
–
g1
g2
g3
.
.
.
gM´2
gM´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(10.14)
Step 5: The resulting system is tridiagonal, which can be easily solved using
Thomas’ algorithm. The solution of the linear system for h “ 0.4, 0.2, and 0.1
(i.e., for M “ 5, 10, and 20 intervals) yields 4, 9, and 19 unknowns. On the other
hand, the BVP is a second-order linear ODE whose true solution is obtained as
ypxq “ 170 ´ px2 ´ 22x ` 170qe0.2x ` 11.96860535x.
In Table 10.1, the true solution, as well as the true errors, are comparatively
tabulated for M “ 5, 10, and 20. Notice that the estimates turned out to be negative,
indicating that displacement is directed downward. Since the beam has a non-uniform
cross section, the solution is not exactly symmetrical with respect to x “ L{2, which
is not the result of round-off and/or truncation errors.
TABLE 10.1
True True Absolute Error
x Solution h “ 0.4 h “ 0.2 h “ 0.1
0 0
0.2 ´0.00617557 4.99E-05 1.25E-05
0.4 ´0.01175910 3.69E-04 9.24E-05 2.31E-05
0.6 ´0.01624199 1.26E-04 3.16E-05
0.8 ´0.01921942 6.00E-04 1.50E-04 3.76E-05
1 ´0.02040562 1.63E-04 4.07E-05
1.2 ´0.01965035 6.50E-04 1.63E-04 4.07E-05
1.4 ´0.01695682 1.48E-04 3.71E-05
1.6 ´0.01250115 4.69E-04 1.17E-04 2.93E-05
1.8 ´0.00665334 6.88E-05 1.72E-05
2 0584  Numerical Methods for Scientists and Engineers
Discussion: The numerical estimates improve as the number of subintervals is in￾creased. For instance, as M is doubled, the global error decreased by about four
folds, which should not come as a surprise since the order of error in the general
difference equation is Oph2q. The average errors are 5.22 ˆ 10´4, 1.2 ˆ 10´4, and
3 ˆ 10´5 for M “ 5, 10, and 20, respectively. It is also worth noting that the numer￾ical solution of an ODE is not known for any x other than those discrete values (yi)
corresponding to xi’s. To find solutions for any x other than xi’s, the procedure to
be applied is interpolation.
10.3.2 IMPLEMENTING NEUMANN AND ROBIN BCs
The implementation of Neumann and Robin BCs is essentially the same, in that Neumann
BC can be easily obtained by setting β “ 0 in Robin BC. Therefore, the implementation
of Robin BC should suffice for applying both BCs. Consider imposing Robin BC on both
boundaries in Eq. (10.5). In this case, the nodal values at the boundaries (y0 and yM) are
unknown. Thus, we end up with M ` 1 equations and M ` 3 unknowns, namely y´1, y0,
... yM, yM`1. Also, the difference equations for boundary nodes include fictitious nodes,
i.e., y´1 and yM`1. So to complete the linear system, we make use of the BCs for the two
additional equations.
Our next step is to eliminate the fictitious nodes with the aid of the BCs. First, let
us consider the left BC, α1y1
paq ` β1ypaq “ γ1, which is discretized at x0 “ a (or i “ 0).
Next, expressing ypaq “ y0 and y1
paq “ y1
0 « py1 ´ y´1q{p2hq and substituting in the left
BC yields
α1
ˆy1 ´ y´1
2h
˙
` β1 y0 “ γ1 (10.15)
where y´1 “ ypx´1q.
Solving Eq. (10.15) for the fictitious node gives y´1 “ y1 ´ 2hpγ1 ´ β1y0q{α1. Substi￾tuting this expression into the difference equation obtained by setting i “ 0 in Eq. (10.5)
and collecting the unknowns on the left and knowns on the right-hand side, the difference
equation for the left boundary node takes the following form:
ˆ
d0 ` 2h β1
α1
b0
˙
y0 ` pa0 ` b0qy1 “ g0 ` 2h γ1
α1
b0 (10.16)
The right BC, α2y1
pbq ` β2ypbq “ γ2, is likewise discretized at xM “ b (i “ M), replacing
ypbq“yM and y1
pbq“y1
M « pyM`1 ´ yM´1q{p2hq leads to
α2
ˆyM`1 ´ yM´1
2h
˙
` β2yM “ γ2
Solving for yM`1 yields
yM`1 “ yM´1 `
2h
α2
pγ2 ´ β2yMq (10.17)
where yM`1 denotes an approximation for the fictitious node in terms of yM´1 and yM.
Substituting Eq. (10.17) into the difference equation obtained by setting i “ M in Eq.
(10.5) and collecting the unknowns on the left and knowns on the right-hand side, we arrive
at the following difference equation:
paM ` bMqyM´1 `
ˆ
dM ´ 2h β2
α2
aM
˙
yM “ gM ´ 2h γ2
α2
aM (10.18)ODEs: Boundary Value Problems  585
The difference equations of the interior nodes are not affected by the implementation of the
BCs.
Finally, combining Eqs. (10.5), (10.16), and (10.18), a system of pM ` 1qˆpM ` 1q
equations is obtained as
For i “ 0 d0y0 ` a0y1 “ g0
For i “ 1 b1y0 ` d1y1 ` a1y2 “ g1,
For i “ 2 b2y1 ` d2y2 ` a2y3 “ g2, (10.19)
.
.
. .
.
.
For i “ M ´ 1 bM´2yM´2 ` dM´1yM´1 ` aM´1yM “ gM´1,
For i “ M bMyM´1 ` dMyM “ gM
where
d0 “ d0 ` 2h β1
α1
b0, a0 “ a0 ` b0, g0 “ g0 ` 2h γ1
α1
b0,
dM “ dM ´ 2h β2
α2
aM, bM “ aM ` bM, gM “ gM ´ 2h γ2
α2
aM, (10.20)
EXAMPLE 10.2: Applying FDM to heat transfer from trapezoidal fin
In the study of heat transfer, “fins” made of conducting metals or alloys are attached
to a metallic object to increase the heat transfer rate by increasing its total surface
area. Consider the trapezoidal fin shown in the figure.
The governing ODE for the temperature excess, Θpxq “ Tpxq ´ Tb, is given as
px ` δq
d2Θ
dx2 `
dΘ
dx ´ m2L Θpxq “ 0, Θ1
p0q “ 0, ΘpLq “ Θb
where Θb “ Tb ´ T8, δ “ aL{pb ´ aq, and m2 “ 2hc{kpb ´ aq are constants, T8
and Tb denote the ambient and base temperatures, hc is the convection heat transfer
coefficient, k is the conductivity of the fin, L and w are respectively the fin length
and width, and b and a denote the dimensions of the fin as shown in the figure.
A fin configuration is given as a “ 5 mm, b “ 2 mm, L “ 15 cm, Θb “ 100˝C
and m2 “ 250 m´2. Use the FDM to obtain the numerical solution by dividing the
fin length into M “ 5, 10, 20, and 40 uniform intervals. Compare the estimates with
the true solution is given with
Θpxq “ Θb
I1puqK0puξxq ` K1puqI0puξxq
I1puqK0puξLq ` K1puqI0puξLq586  Numerical Methods for Scientists and Engineers
where u “ 2m?
δL and ξx “ a1 ` x{δ, I0pxq, I1pxq, K0pxq, and K1pxq are the
zeroth and first-order modified Bessel functions of the first and second kinds.
SOLUTION:
Step 1: A one-dimensional uniform grid, depicted in Fig. 10.6, is obtained by
dividing the solution interval (0 ď x ă L) into uniformly spaced M subintervals
(h “ L{M). Note that, for convenience, the coordinate system in the figure is chosen
in the direction from the tip toward the base, thereby reversing the positions of the
tip (x “ 0) and base (x “ L). Since the nodal value at the right boundary is known
(ΘpLq “ Θb), for the right (base) node, we may set ΘM “ Θb. However, the nodal
value of the left (fin tip) boundary is unknown due to the Neumann BC. The number
of unknowns for this problem is determined to be M.
The numerical approximations for the excess temperature corresponding to the
nodal points are labeled as Θpxiq “ Θi for i “ 0, 1, 2,...,pM ´ 1q, where xi “ ih.
FIGURE 10.6
Step 2: The differential equation is satisfied for all nodes on [0,L); hence, for any xi,
we may write
pxi ` δq
d2Θi
dx2 `
dΘi
dx ´ m2L Θpxiq “ 0, xi P r0, Lq (10.21)
Substituting the CDFs for Θ1
i and Θ2
i leads to
pxi`δq
Θi`1´2Θi`Θi´1
h2 `
Θi`1´Θi´1
2h ´m2L Θi “0, i“0, 1,...,M ´1 (10.22)
Rearranging Eq. (10.22), the general difference equation is found as
biΘi´1 ` diΘi ` aiΘi`1 “ ci, i“0, 1,...,M ´1 (10.23)
where
bi “ xi`δ
h2 ´ 1
2h, ai “ xi`δ
h2 `
1
2h, di “ ´´
m2L`
2pxi`δq
h2
¯
, ci “0
Step 3: The difference equations for all unknown nodes are derived from Eq.
(10.23) as follows:
For i “ 0 b0 Θ´1 ` d0Θ0 ` a0Θ1 “ 0
For i “ 1 b1Θ0 ` d1Θ1 ` a1Θ2 “ 0
.
.
. .
.
.
For i “ M ´ 2 bM´2ΘM´3 ` dM´2ΘM´2 ` aM´2ΘM´1 “ 0
For i “ M ´ 1 bM´1ΘM´2 ` dM´1ΘM´1 ` aM´1 ΘM “ 0ODEs: Boundary Value Problems  587
The fictitious node value, Θ´1, at the left boundary is eliminated by discretizing
the left BC with the CDF as Θ1
0 – pΘ1 ´ Θ´1q{2h “ 0, which gives Θ´1 “ Θ1.
Substituting this into the difference equation with i “ 0 (after collecting the unknown
nodes on the left) leads to
d0Θ0 ` pa0 ` b0qΘ1 “ 0
At the right boundary, the nodal value of ΘM “ Θb is plugged into the difference
equation obtained with i “ M ´ 1. Likewise, rearranging this difference equation
gives
bM´1ΘM´2 ` dM´1ΘM´1 “ ´aM´1Θb
Step 4: The total of M equations and M unknowns can be expressed in matrix
form as
»
—
—
—
—
—
—
—
–
d0 a0 ` b0
b1 d1 a1
b2 d2 a2
... ... ...
bM´2 dM´2 aM´2
bM´1 dM´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
–
Θ0
Θ1
Θ2
.
.
.
ΘM´2
ΘM´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
–
0
0
0
.
.
.
0
´aM´1Θb
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(10.24)
which can be easily programmed for any value of M.
Step 5: The resulting linear system of equations is tridiagonal in form and can be
solved with Thomas’ algorithm. The true errors of the numerical estimates obtained
with M “ 5, 10, 20, and 40 and the true solution are comparatively tabulated in
Table 10.2. The average of the errors is about 0.2, 0.043, 0.011, and 0.03˝C for
M “ 5, 10, 20, and 40, respectively.
TABLE 10.2
True True Absolute Error
x Solution M “ 5 M “ 10 M “ 20 M “ 40
0 23.5472 0.6588 0.1796 0.0460 0.0116
0.015 24.4572 0.1038 0.0266 0.0067
0.030 26.9522 0.2014 0.0563 0.0145 0.0037
0.045 30.8371 0.0256 0.0067 0.0017
0.060 36.0522 0.0116 0.0056 0.0016 0.0004
0.075 42.6217 0.0072 0.0017 0.0004
0.090 50.6264 0.0622 0.0144 0.0035 0.0009
0.105 60.1908 0.0170 0.0042 0.0010
0.120 71.4745 0.0636 0.0155 0.0038 0.0010
0.135 84.6699 0.0099 0.0025 0.0006
0.150 100
Discussion: Results of very good quality were obtained even with a very small
number of nodes. In this example, the maximum true error in the estimated solution
was below 1˝C even for M “ 5. When M was doubled, the global error decreased
by about fourfold since Opph{2q2q « Oph2{4q.
Also, since no approximation is introduced for the right BC, the least error is
observed at the base (x “ L), while the largest error is observed at the tip (x “ 0)588  Numerical Methods for Scientists and Engineers
due to approximating the right BC, Θ1
0 “ 0, with CDF. However, in problems where
more accurate solutions are sought, the density of nodes should be increased at the
boundary where large gradients in the solution are expected.
Pseudocode 10.1
Module LINEAR_BVP (M, x, α, β, γ, y)
\ DESCRIPTION: A pseudomodule to solve a two-point linear BVP given as:
\ ppxqy2 ` qpxqy1 ` rpxqypxq “ gpxq, a ď x ď b subject to following BCs:
\ α1y1
paq ` β1ypaq “ γ1, α2y1
pbq ` β2ypbq “ γ2 with the FDM.
\ USES:
\ COEFFS:: Module supplying ppxq, qpxq, rpxq, and gpxq;
\ TRIDIAGONAL:: Tridiagonal system solver, Pseudocode 2.13)
Declare: α2, β2, γ2, aM, bM, cM, dM, xM, yM \ Declare array variables
h Ð x2 ´ x1 \ Find interval size
For “
k “ 1, M‰ \ Setup the tridiagonal matrix row-wise
COEFFS(xk, px, qx, rx, gx) \ Get coefficients of ODE at x “ xk
dk Ð rx ´ 2 ˚ px{h2 \ Setup diagonal element
ak Ð ppx{h ` qx{2q{h \ Setup above-diagonal element
bk Ð ppx{h ´ qx{2q{h \ Setup below-diagonal element
ck Ð gx \ Setup rhs element
End For
\ Implement the left BC
If “
α1 “ 0
‰
Then \ Case of Dirichlet BC on x “ a
d1 Ð 1; c1 Ð γ1{β1;
a1 Ð 0; b1 Ð 0
Else \ Case of Robin BC on x “ a
a1 Ð a1 ` b1
d1 Ð d1 ` b1 ˚ 2hβ1{α1
c1 Ð c1 ` b1 ˚ 2hγ1{α1
End If
\ Implement the right BC
If “
α2 “ 0
‰
Then \ Case of Dirichlet BC on x “ b
dM Ð 1; cM Ð γ2{β2;
aM Ð 0; bM Ð 0
Else \ Case of Robin BC on x “ b
bM Ð aM ` bM
dM Ð dM ´ aM ˚ 2hβ2{α2
cM Ð cM ´ aM ˚ 2hγ2{α2
End If
TRIDIAGONAL p1,M, b, d, a, c, yq \ Solve tridiagonal system to find y
End Module LINEAR_BVP
Module COEFFS (x, p, q, r, g)
\ DESCRIPTION: A user-defined module supplying p, q, r, and g for any x.
p Ð ppxq; q Ð qpxq; ... \ Define coefficients of linear ODE here
End Module COEFFS
A general pseudomodule, LINEAR_BVP, for numerically solving a two-point linear BVPODEs: Boundary Value Problems  589
Finite Difference Method
‚ It is very easy to discretize ODEs using the finite difference method;
‚ For a linear second-order BVP, the finite difference procedure with
the global error of order Oph2q results in a tridiagonal system of
equations whose direct solution with the Thomas algorithm is fast
and accurate;
‚ Accuracy of the numerical estimates improves as the interval size is
refined, i.e., h Ñ 0.
‚ Finite difference schemes of order of accuracy Oph3q, Oph4q, and so
on are unreliable for the BVPs of conservative forms;
‚ The method is quite laborious to apply to non-uniform grids;
‚ When applied to 2D and 3D BVPs, it is generally limited to struc￾tured grids.
given with Eq. (10.1) is presented in Pseudocode 10.1). As input, the module requires the
number of nodal points (M), the coefficients of the BCs as αi, βi, and γi for i “ 1,2,
and the abscissas (xi, i “ 1, 2,...,M) of the uniformly spaced nodal points established
in the interval [a, b]. On output, the numerical solution is saved on the nodal values, yi,
i “ 1, 2,...,M. The LINEAR_BVP also makes use of two additional modules: (1) COEFFS,
a user-defined module, supplying ppxq, qpxq, rpxq, and gpxq for any x; (2) TRIDIAGONAL
solving a tridiagonal system of equations, Pseudocode 2.13.
The interval size is set to h “ x2 ´ x1 since all nodal points are uniformly spaced. The
coefficients pk, qk, rk, and gk for a specified xk (row) are computed in COEFFS at the top
of the For-loop. Then the diagonal arrays of the tridiagonal matrix (bk, dk, and ak) and the
rhs (ck) are computed in accordance with Eq. (10.4). However, the first (k “ 1) and last
(k “ M) rows of the tridiagonal system are modified to implemenent the Dirichlet or Robin
the BCs using If-Then-Else constructs. If the Dirichlet BC is prescribed on x “ a (α1 “ 0),
then the first row entries of the arrays are set to d1 “1, a1 “b1 “0, and c1 “γ1{β1. (Recall
that it is more convenient to test the equality of floating point numbers as |α1| ă �.) If the
Neumann or Robin BC is prescribed on x“a (α1 ‰ 0), then the first entries of the arrays are
modified according to Eq. (10.16). Similarly for x “ b, when α2 “ 0, the Dirichlet condition
to the last row is applied as dM “ 1, aM “ bM “ 0, and cM “ γM{βM. The Robin BC on
x“b (α2 ‰ 0) modifies the last row entries according to Eq. (10.18). Once the setting up of
the tridiagonal system is complete, it is solved using Thomas’ algorithm with the module
TRIDIAGONAL and saved on array y.
10.4 NUMERICAL SOLUTIONS OF HIGH-ORDER ACCURACY
In this section, we focus on reducing the discretization error in the numerical solution by
increasing the order of accuracy. Since the numerical solution of a BVP requires solving
a linear system of equations, a direct numerical solution (yi) inevitably contains rounding
errors due to numerous arithmetic operations or insufficient computational precision. Nev￾ertheless, assuming that the computations are carried out using high-precision arithmetic
with negligible round-off errors, we let y0, y1, y2, ..., yM represent the true solutions of the
system of linear equations.
Let ypxiq be the true solution of the BVP at node xi. Then the global discretization590  Numerical Methods for Scientists and Engineers
error is defined as
max
all i |ypxiq ´ yi|
The local discretization error is just the truncation error in approximating y1
i and y2
i deriva￾tives. Since the central differences of order Oph2q were used in the preceding analysis, the
local discretization error is also Oph2q.
As far as relating the local discretization errors to global errors, so long as second-order
finite difference formulas are used for all derivatives, the global discretization error will also
be second order. It can be shown that the resulting system of linear equations from the
discretization of second-order BVPs is diagonally dominant, which is a necessary condition
for the convergence of a tridiagonal system.
Higher-order ODEs result in a banded system of linear equations that can be solved
using an iterative method or Gauss elimination (see Section 2.5). An advantage of using
second-order central difference formulas is that discretizing a two-point BVP always results
in a tridiagonal system from which a direct solution can be obtained. Nevertheless, dealing
with difference equations with global error Oph2q may result in a very large number of
unknowns to attain the desired accuracy. Hence, it is sometimes tempting to use higher￾order finite difference approximations for the derivatives to avoid dealing with large linear
systems. In practice, a major difficulty in applying this strategy to a two-point BVP is
encountered when implementing the BCs. For instance, the central difference formulas for
y1
i and y2
i of order Oph4q are:
y1
i –´yi`2 ` 8yi`1 ´ 8yi´1 ` yi´2
12h ,
y2
i –´yi`2 ` 16yi`1 ´ 30yi ` 16yi´1 ´ yi´2
12h2
Note that when implementing the approximations, for example, by setting i “ 0 for the
left boundary node, two fictitious nodes, namely y´1 and y´2, will appear in the difference
equation. To eliminate the fictitious nodes, two BCs need to be specified.
Another approach to obtaining high order, more accurate numerical solutions is to
utilize the Richardson extrapolation technique, which was used to increase the order of
accuracy of numerical differentiations (see Section 5.7) or integrations (see Section 8.3).
Applying this technique to the numerical solution of the ODEs does not require discretiza￾tion. For instance, to obtain fourth-order accurate solutions, two sets of numerical solutions
with subinterval sizes h and h{2 are required. For a sixth-order accurate approximation,
three sets of numerical approximations of order Oph2q with interval sizes h, h{2, and h{4
are obtained. The extrapolation procedure is then employed, as follows:
1st extrapolation ˜y1,i “ 1
3
´
4yi
´h
2
¯
´ yiphq
¯
` Oph4q
2nd extrapolation ˜y2,i “ 1
3
´
4yi
´h
4
¯
´ yi
´h
2
¯ ¯ ` Oph4q
3rd extrapolation ˜yi “ 1
15
´
16˜y2,i ´ y˜1,i¯
` Oph6q
where yiphq, yiph{2q, and yiph{4q are the numerical approximations obtained with the step
sizes of h, h{2, and h{4, respectively, and ˜y1,i, ˜y2,i, and ˜yi are the numerical approximations
obtained with the Richardson extrapolation.ODEs: Boundary Value Problems  591
EXAMPLE 10.3: Applying Richardson extrapolation to BVPs
A rocket body (t“1 cm in thickness, Do “20 cm in outer diameter) is made of an
aluminum alloy (E “ 64 GPa, ν “ 0.33). For cylinders, the radial stress is given
in terms of the displacement by σr “ E{p1 ´ ν2qpdu{dr ` ν u{rq, where E is the
elasticity modulus, ν is the Poisson ratio, r is the radial coordinate, and u “ uprq is
the radial displacement satisfying the following second-order ODE:
r2 d2u
dr2 ` r
du
dr ´ u “ 0, Ri ď r ď Ro
Assuming negligible external pressure on the body of the rocket (Po “ 0) in flight
conditions, the internal pressure reaches pi “ 25.6 kPa (see figure below). The BCs
for the event can be expressed as
r “ Ri, σrpRiq “ E
1 ´ ν2
´du
dr ` ν
u
r
¯
r“Ri
“ ´Pi
r “ Ro, σrpRoq “ E
1 ´ ν2
´du
dr ` ν
u
r
Bigq
r“Ro
“ 0
Apply FDM with uniform M “5 and 10 intervals to (a) obtain 2nd-order numerical
estimates; (b) apply Richardson’s extrapolation to find numerical estimates of order
Oph4q; (c) compare the results with the true solution given by uprq “ 1.14253r`
0.02268{r (in mm).
SOLUTION:
(a) The underlying assumption in this analysis is that the rocket is long enough
to permit one-dimensional analysis, L{Ro " 1. To be able to employ the Richardson
extrapolation, we need two discrete sets of numerical estimates of order Oph2q. The
FDM steps are presented below:
Step 1: Uniform grids are established by dividing the Ri ď r ď Ro interval into M
subintervals, h “ pRo´Riq{M “ t{M, as shown in Fig. 10.7. Nodal points are placed
at the end points of each subinterval and are enumerated from 0 to M. The nodal
values, unknowns, are labeled as upriq “ ui for i “ 0, 1, 2,...,M, where ri “ Ri`ih.
FIGURE 10.7
Step 2: The differential equation is satisfied for every nodal point ri P rRi, Ros. For
an arbitrary ri, the BVP is written as
r2
i
d2ri
dr2 ` ri
dui
dr ´ upriq “ 0, Ri ď ri ď Ro
or substituting the CDFs for dui{dr and d2ui{dr2 yields
r2
i
´ui`1 ´ 2ui ` ui´1
h2
¯
´ ri
´ui`1 ´ ui´1
2h
¯
´ ui “ 0 (10.25)592  Numerical Methods for Scientists and Engineers
Rearranging Eq. (10.25), the general difference equation is obtained as
biui´1 ` diui ` aiui`1 “ 0, i “ 0, 1, 2,...,M (10.26)
where
bi “ ri
h
´ri
h `
1
2
¯
, di “ ´´
1 ` 2
r2
i
h2
¯
, ai “ ri
h
´ri
h ´ 1
2
¯
, ci “ 0
Step 3: To implement BCs, we start by setting i “ 0 in Eq. (10.26), giving
b0u´1 ` d0u0 ` a0u1 “ 0 (10.27)
Discretizing the left BC at r0 (i “ 0) with the CDF yields
u1 ´ u´1
2h ` ν u0
r0
“ ´p1 ´ ν2q
Pi
E
and isolating u´1 gives
u´1 “ u1 ` 2hν u0
r0
` 2hp1 ´ ν2q
Pi
E
Substituting this result into Eq. (10.27) and collecting similar terms results in
´
d0 `
2hν
r0
b0
¯
u0 ` `
a0 ` b0
˘
u1 “ ´2hb0p1 ´ ν2q
Pi
E ,
For the right BC, setting i “ M in Eq. (10.26) gives
bMuM´1 ` dMuM ` aMuM`1 “ 0 (10.28)
where uM`1 corresponds to a fictitious nodal value. To eliminate this node, the right
BC is discretized using the central difference formula as
σrprMq “ E
1 ´ ν2
´uM`1 ´ uM´1
2h ` ν uM
rM
¯
“ 0
Solving for uM`1 results in
uM`1 “ uM´1 ´ 2hν uM
rM
(10.29)
Substituting this into Eq. (10.28) and collecting similar terms yields
`
aM ` bM
˘
uM´1 `
´
dM ´ 2hν aM
rM
¯
uM “ 0 (10.30)
Step 4: Combining the difference equations in matrix form, a linear system of pM `
1qˆpM ` 1q equations is obtained as
»
—
—
—
—
—
—
—
–
d0 a0
b1 d1 a1
b2 d2 a2
... ... ...
bM´1 dM´1 aM´1
bM dM
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
–
u0
u1
u2
.
.
.
uM´1
uM
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
–
c1
0
0
.
.
.
0
0
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(10.31)ODEs: Boundary Value Problems  593
where d0 “d0`2hνb0{r0, a0 “a0`b0, dM “dM ´2hνaM{rM, and ¯bM “aM `bM.
Step 5: The tridiagonal system, Eq. (10.31), is solved for M “ 5 (h “ 0.002) and
10 (h “ 0.001) as a first step for obtaining the numerical estimates of order Oph4q.
Then, the Richardson extrapolation procedure is carried out as follows:
u˜i “ 1
3
`
4 u2i ´ u1i
˘
` Oph4q for i “ 0, .., 5
where u2i and u1i are the numerical estimates corresponding to the ith node for
M “ 10 and 5, respectively.
TABLE 10.3
True Solution Estimates, Oph2q Estimate, Oph4q
i xi ui h “ 0.002 h “ 0.001 u˜i
0 0.090 0.354828 0.354538 0.354755 0.354827
1 0.092 0.351634 0.351349 0.351563 0.351634
2 0.094 0.348674 0.348393 0.348604 0.348674
3 0.096 0.345933 0.345656 0.345863 0.345932
4 0.098 0.343397 0.343124 0.343328 0.343396
5 0.100 0.341053 0.340784 0.340985 0.341052
The true solution (in mm), second-order numerical estimates with h “ 0.002
and 0.001, and the extrapolations are comparatively tabulated in Table 10.3. The
average displacement in the rocket body is 0.35 mm, and the maximum displacement
is on the inner surface. The average error for estimates with h “ 0.002, 0.001, and
Richardson extrapolation is about 2.8 ˆ 10´4, 7 ˆ 10´5, and 5 ˆ 10´7, respectively.
Notice that the error of the estimates with the h “ 0.001 case is reduced by p1{2q2
compared to that of the h “ 0.002 case. The fourth-order estimate obtained by the
Richardson extrapolation yields five- to six-decimal-place accurate estimates with
very few grid points.
Discussion: In this example, it is shown that Richardson extrapolation is a method
that can be used to improve the order of accuracy of numerical solutions of ODEs
uniformly discretized by the finite difference method. By combining the results of
two sets of numerical solutions obtained by discretizing with different interval sizes
(h and h{2), the leading truncation error terms can be eliminated, resulting in highly
accurate results.
10.5 NON-UNIFORM GRIDS
As we have already seen, the first step in obtaining a numerical solution to an ODE is
the gridding of the physical domain. A suitably designed grid results in good-quality nu￾merical estimates; conversely, a poorly constructed grid yields poor numerical estimates.
In many instances, the difficulties with numerical solutions may be attributed to poorly
constructed grids. For this reason, gridding is a very important first step that should not
be underestimated to obtain quality solutions.
In general, the finite difference method with uniform grids is simple and very accurate,
and the numerical solution of most ODEs can be carried out using uniform grids. On the
other hand, using a uniform grid does not always yield satisfactory estimates for cases with
boundary layers. For example, if the solution to an ODE describing the behavior of a physical594  Numerical Methods for Scientists and Engineers
FIGURE 10.8: Grid stretching from left to right, or right to left.
property (temperature, velocity, concentration, etc.) depicts a sharp gradient at one or both
ends of the solution interval, then these changes cannot be adequately resolved with uniform
grids. In order to accurately estimate the gradients within a boundary layer, a very small or
fine grid should be adopted. If a uniform fine grid is applied throughout the domain, then the
number of nodes (i.e., the size of the resulting matrix) increases undesirably. The purpose
of using non-uniform grids is, in essence, to increase the grid density near the boundaries,
where sharp changes are expected. With the computational power of present-day computers,
a one-time solution of a very large system of linear equations may not be viewed as a big
problem, but in cases where a system of linear equations is solved many times in an iterative
procedure, the cpu time can be seriously reduced by adopting non-uniform grids, leading
to smaller linear systems.
Grid generation for ODEs (one-dimensional cases) is one of the simplest tasks of grid￾ding. Depending on the physics of the problem, grids can be concentrated at one or both
ends of the interval or at an interior point. There are many techniques for grid genera￾tion, covering a considerable portion of computational work. This section provides a brief
introduction to analytical means of grid generation.
Generating arbitrary grids can be done manually by the user or, generally, using a grid
generation algorithm. For instance, stretched grids can be obtained by specifying the first
interval size and a growth factor. Letting h be the first interval size, then the size of the
subsequent intervals is determined as hr, hr2, hr3, and so on, where r is the growth factor.
If r ą 1, then grid points are clustered near x “ a or near x “ b for r ă 1 (see Fig. 10.8).
To generate a grid with M subintervals, only a first step size (h) and a growth factor (r)
are sufficient. The first step size for an arbitrary interval ra, bs can be determined from
h “ 1 ´ r
1 ´ rM pb ´ aq
and the abscissas are determined from xi “ a`hp1´ri
q{p1´rq for i “ 1, 2,...,M. The ODE
is then discretized by replacing the derivatives with the finite difference formulas derived
for non-uniform grids. This avenue increases the computational burden of constructing the
coefficient matrix (i.e., due to the coefficients of a generalized difference equation containing
complicated expressions).
An alternative technique requires the coordinate transformation of the physical domain
(x-space) into a computational domain (ξ-space), to which uniform grids can be applied.
First, the ODE is transformed from x-space to an equivalent ODE in ξ-space. Then, the
transformed ODE is solved with uniform grids (Δξ). In this technique, the grid stretching
function x “ xpξq dictates the nodal distribution in x-space. For instance, a simple stretching
function involving power functions such as x “ a` pb´aq ξn, where 0 ď ξ ď 1, may be used
to construct non-uniform grids on [a, b]. The nodes are clustered near the left boundary for
n ą 1 and near the right boundary for n ă 1 (see Fig. 10.9). More elaborate and specialized
grid stretching functions involving logarithmic, exponential, hyperbolic, or trigonometric
functions are available [7, 32, 36].ODEs: Boundary Value Problems  595
FIGURE 10.9: Grid stretching using x “ a`c ξn as stretching function
examples of (a) n ă 1, and (b) n ą 1.
Use of Non-Uniform Grids
‚ Numerical treatment of a BVP using a suitable grid stretching func￾tion can lead to very satisfactory estimates;
‚ Properly designed non-uniform grids may result in a significant re￾duction in the number of nodes.
‚ Numerical treatment of the original BVP with non-uniform grids may
not yield second-order global errors (see Section 5.6),
‚ A BVP in a transformed space can be very complicated.
Consider transforming the two-point BVP given by Eq. (10.1) into a BVP in ξ-space
using the grid stretching function x “ a`c ξn. The y1 and y2 are transformed into derivatives
with respect to the variable ξ. Applying the chain rule leads to
dy
dx “ dy
dξ
dξ
dx “ dy
dξ
´ 1
cn ξn´1
¯
(10.32)
Next, applying the chain rule one more time and making use of Eq. (10.32) gives
d2y
dx2 “ d
dξ
´dy
dx
¯ dξ
dx “ d
dξ
´dy
dξ
1
cnξn´1
¯´ 1
cnξn´1
¯
(10.33)
Applying the product rule to differentiation and simplifying leads to
d2y
dx2 “ 1
c2n2ξ2n´2
d2y
dξ2 `
1 ´ n
c2n2ξ2n´1
dy
dξ (10.34)
Substituting x “ a ` c ξn and Eqs. (10.32) and (10.34) into Eq. (10.1) gives the BVP in the
transformed coordinate:
ppξq
c2n2ξ2n´2
d2y
dξ2 `
ˆppξqp1 ´ nq
c2n2ξ2n´1 ` qpξq
cn ξn´1
˙ dy
dξ ` rpξqypξq “ fpξq,
α¯1
dy
dξ p0q ` β¯1yp0q “ γ¯1, α¯2
dy
dξ p1q ` β¯2yp1q “ γ¯2 p0 ď ξ ď 1q
(10.35)
where ¯α1, β¯1, ¯γ1, ¯α2, β¯2, and ¯γ2 denote the transformed coefficients. However, a word of
caution is in order: the stretching function should be chosen such that the ODE and its
BCs are defined in the transformed space.596  Numerical Methods for Scientists and Engineers
EXAMPLE 10.4: Coordinate transformation with a stretching function
Consider the following two-point BVP:
2xy2 ` p20x ` 1qy1 ` 10y “ 0, yp0q “ 1, y1
p1q ` 10yp1q “ 0
(a) Use x “ ξ2 as a stretching function to transform the BVP in x-space to a BVP
in ξ-space; (b) solve the transformed BVP using the FDM with M “ 10, 20, and 40
uniform intervals; (c) solve the original BVP using the FDM with the same number
of uniform intervals. The true solution is given as ypxq “ e´10x.
SOLUTION:
(a) Employing the proposed stretching function, x “ ξ2, with uniform Δξ (see
Fig. 10.10) leads to the concentration of nodes at the left boundary as illustrated in
the figure below:
To recast the BVP in ξ-space, the derivatives are also transformed into ξ-space.
Applying the chain rule and noting that dx “ 2ξdξ, the first derivative is easily
obtained as
dy
dx “ dy
dξ
dξ
dx “ 1
2ξ
dy
dξ
Next, applying the chain rule to dy{dx yields
d2y
dx2 “ d
dx ˆdy
dx˙
“ d
dξ ˆdy
dx˙ dξ
dx “ d
dξ ˆ 1
2ξ
dy
dξ ˙ 1
2ξ “ 1
4ξ2
ˆd2y
dξ2 ´ 1
ξ
dy
dξ ˙
Now, substituting y1 and y2 into the ODE and BCs and simplifying the terms gives
d2y
dξ2 ` 20ξ
dy
dξ ` 20ypξq “ 0
yp0q “ 1, dy
dξ p1q ` 20yp1q “ 0
Note that the original domain is also transformed to 0 ď ξ ď 1 in ξ-space.
(b) The step-by-step solution with the FDM is detailed below:
Step 1: First, the ξ-domain is divided into M uniform subintervals, and nodes are
placed at the end of every subinterval. The nodes are enumerated, and the abscissas
are calculated as ξi “ iΔξ (i “ 0, 1, 2,...,M), as depicted in Fig. 10.10.
FIGURE 10.10ODEs: Boundary Value Problems  597
Step 2: The transformed ODE is satisfied for every ξi:
ˆd2y
dξ2
˙
ξ“ξi
` 20ξi
ˆdy
dξ ˙
ξ“ξi
` 20yi “ 0, ξi P p0, 1s
Replacing dy{dξ and d2y{dξ2 with their CDFs approximations yields
yi`1 ´ 2yi ` yi´1
pΔξq
2 ` 20ξi
yi`1 ´ yi´1
2 Δξ ` 20yi “ 0, i “ 1, 2,...,M
Rearranging and collecting similar terms, the general difference equation becomes
1
Δξ
´ 1
Δξ ´ 10ξi
¯
yi´1 ` 2
´
10 ´ 1
pΔξq
2
¯
yi `
1
Δξ
´ 1
Δξ ` 10ξi
¯
yi`1 “ 0
Step 3: Notice that the value of the left boundary node is known, i.e., y0 “ 1.
Substituting i “ 1 and y0 into the general difference equation and carrying the
known quantities to the rhs gives
2
´
10 ´ 1
pΔξq
2
¯
y1 `
1
Δξ
´ 1
Δξ ` 10ξ1
¯
y2 “ 1
Δξ
´ 1
Δξ ´ 10ξ1
¯
Likewise, the difference equation for the right boundary node is obtained by setting
i “ M in the general difference equation:
1
Δξ
´ 1
Δξ ´ 10ξM
¯
yM´1 ` 2
´
10 ´ 1
pΔξq
2
¯
yM `
1
Δξ
´ 1
Δξ ` 10ξM
¯
yM`1 “ 0
which contains the fictitious node yM`1.
Discretizing the right BC with the CDFs, we find
yM`1 ´ yM´1
2 Δξ ` 20yM “ 0 or yM`1 “ yM´1 ´ 40Δξ yM
Substituting yM`1 into the difference equation for i “ M and simplifying the terms
leads to
2
pΔξq
2 yM´1 `
´
20 ´ 2
pΔξq
2 ´ 40
Δξ ´ 400ξM
¯
yM “ 0,
The difference equations for 2 ď i ď pM ´ 1q (interior nodes) are easily obtained
from the general difference equation without the need for further action.
Step 4: By putting all the difference equations together, a matrix equation with a
tridiagonal coefficient matrix is obtained.
Step 5: The resulting tridiagonal system is solved using Thomas’ algorithm.
The true solution, as well as the true (absolute) errors arising from the FDM
solution obtained with uniform M “ 10, 20, and 40 grids, are tabulated in Table
10.4. This is the case when the nodes are concentrated at the left boundary. The
solutions agree well with those of the case with M “ 10 yielding an average error
of 0.0107. As the number of grids increases by two folds, the errors also decrease by
four folds. For instance, the estimates depict noteworthy improvements for M “ 20
and 40, yielding average errors of 2.62ˆ10´3 and 6.52ˆ10´4, respectively. The true
errors are also observed to be small at the left boundary.598  Numerical Methods for Scientists and Engineers
TABLE 10.4
True Absolute true error
x ξ Solution M “ 10 M “ 20 M “ 40
00 1 0 0 0
0.01 0.1 0.904837 4.83E-03 1.16E-03 2.88E-04
0.04 0.2 0.670320 1.58E-02 3.81E-03 9.44E-04
0.09 0.3 0.406570 2.47E-02 5.98E-03 1.48E-03
0.16 0.4 0.201897 2.57E-02 6.22E-03 1.54E-03
0.25 0.5 0.082085 1.91E-02 4.70E-03 1.17E-03
0.36 0.6 0.027324 1.05E-02 2.67E-03 6.71E-04
0.49 0.7 0.007447 4.29E-03 1.16E-03 2.95E-04
0.64 0.8 0.001662 1.29E-03 3.84E-04 1.00E-04
0.81 0.9 0.000304 2.77E-04 9.40E-05 2.50E-05
1 1 0.000045 3.90E-05 1.20E-05 3.00E-06
(c) The steps of the FDM solution of the original BVP with uniform Δx are left
as an exercise for the reader. The true solution, as well as the true errors arising
from the FDM estimates obtained with uniform M “ 10, 20, and 40 subintervals,
are tabulated in Table 10.5. The numerical estimates with M “ 10 depict large
deviations (average absolute error 2.11ˆ10´1) near the left boundary. Even though
the numerical solution improves with increasing M, the resulting average errors are
still larger than those of the non-uniform case.
TABLE 10.5
True Absolute true error
ξ x Solution M “ 10 M “ 20 M “ 40
00 1 0 0 0
0.316 0.1 0.367879 1.10E-01 2.65E-02 6.97E-03
0.447 0.2 0.135335 5.75E-02 1.47E-02 3.84E-03
0.548 0.3 0.049787 2.54E-02 6.86E-03 1.80E-03
0.632 0.4 0.018316 1.05E-02 2.99E-03 7.90E-04
0.707 0.5 0.006738 4.23E-03 1.26E-03 3.34E-04
0.775 0.6 0.002479 1.66E-03 5.15E-04 1.38E-04
0.837 0.7 0.000912 6.44E-04 2.07E-04 5.60E-05
0.894 0.8 0.000336 2.46E-04 8.10E-05 2.30E-05
0.949 0.9 0.000123 9.10E-05 3.00E-05 8.00E-06
1 1 0.000045 3.20E-05 1.00E-05 3.80E-04
Discussion: While the true distribution depicts a very rapid change up to x « 0.5,
the rate of change slows down for x ą 0.5. In Part (b), we obtain better numerical
estimates by solving the BVP in ξ-space because more nodes are staggered at the left
boundary, where the rapid change is observed. On the other hand, in Part (c), the
uniform grids in the original BVP lead to fewer nodes near the left boundary, where
a sharp change occurs. It is clear that wisely placed nodes make a big difference in
obtaining very good estimates.
It should be kept in mind that the numerical solution of an ODE, regardless
of the true distribution of the ODE, will approach the true solution provided that
fine enough grids are used. In practice, when a numerical solution with a moderate
number of subintervals is sought, non-uniform gridding could be most helpful.ODEs: Boundary Value Problems  599
10.6 FINITE VOLUME METHOD
Finite Volume Method (FVM) is also a widely used numerical method to discretize BVPs.
The basis of this method is the integral conservation law.
Consider the following two-point BVP, subjected to the following mixed BCs:
d
dx ˆ
ppxq
dy
dx˙
` qpxq
dy
dx ` rpxqypxq “ gpxq, a ă x ă b
α1y1
paq ` β1ypaq “ γ1, α2y1
pbq ` β2ypbq “ γ2
(10.36)
where ppxq, qpxq, rpxq, and gpxq may be piecewise continuous functions.
The BVPs in the form of Eq. (10.36) are also encountered in steady-state diffusion
problems in layered media, where each layer consists of a different material of homogeneous
composition. As a result, p, q, r, and g are typically material-dependent (step-wise varying)
quantities, which may depict jump discontinuities at the interfaces. In such cases, the phys￾ical property ypxq as well as the flux ppxqdy{dx must satisfy what are called the interface
conditions. Mathematically speaking, the continuity of the physical property and flux at an
interface point x “ xc is expressed as
Continuity of physical property: ypx´
c q “ ypx`
c q
Continuity of physical flux : ppx´
c q
dy
dx
ˇ
ˇ
ˇ
ˇ
x´
c
“ ppx`
c q
dy
dx
ˇ
ˇ
ˇ
ˇ
x`
c
(10.37)
It is noted that the FVM is especially ideal for the numerical treatment of problems with
jump discontinuities. The steps for the numerical solution of FVM are given as follows:
Step 1. Gridding: The gridding of the computational interval is similar to that of FDM. First,
the solution interval is divided into M subintervals called cells, which are fine enough to
ensure the resulting difference equations adequately approximate the ODE (see Fig. 10.11).
Then, physical nodes are placed at the endpoints of each cell. A set of abscissas txiu denoting
these nodes is generated with the cell size Δxi “ xi`1 ´xi that is allowed to be nonuniform.
In the subsequent derivations, for generality, the cell size is assumed to be nonuniform.
FIGURE 10.11: General grid configuration and finite volume cells for one-dimensional
systems (a) rod, (b) plane parallel layers, (c) cylindrical or spherical shells.600  Numerical Methods for Scientists and Engineers
FIGURE 10.12: General grid configuration and a typical control volume surrounding a node at xi.
A control volume (CV) is designated as xi´1{2 ď x ď xi`1{2 by placing nodal (bound￾ary) points into the middle of the cells (see Fig. 10.12). The abscissas of boundary nodes on
either side of xi are obtained with xi´1{2 “ xi ´Δxi´1{2 and xi`1{2 “ xi `Δxi{2. Near the
physical boundaries (x “ a and x “ b), the CVs are created in such a way that the physical
boundaries coincide with the CV boundaries, i.e., x0 ď x ď x1{2 and xM´1{2 ď x ď xM.
The volume element for the cylindrical and spherical geometries
with radial symmetry is dV “ 2πrdr or dV “ 4πr2dr, respectively.
In the discretization of ODEs using the FVM, both sides of the
ODE are multiplied by dV . Therefore, the constant multipliers in
dV are usually dropped, leaving only dV “ rdr or dV “ r2dr.
Step 2. Discretizing: The essence of the FVM is to integrate the governing ODE over each
control volume, i.e., xi´1{2 ď x ď xi`1{2. The quantities p, q, r, and g are assumed to be
constant (uniform) in each CV, allowing stepwise variations. In subsequent discussions, we
also assume Eqs. (10.36) and (10.37) make up a mathematical model (BVP) of a physical
event in a rod or a plane parallel geometry, where a finite cell volume is proportional to the
cross-section area, i.e., dV “ A dx.
Equation (10.36) is integrated over ith cell, xi´1{2 ď x ď xi`1{2, that is,
ż xi`1{2
xi´1{2
d
dx
´
ppxq
dy
dx
¯
dx`
ż xi`1{2
xi´1{2
qpxq
dy
dxdx`
ż xi`1{2
xi´1{2
rpxqypxqdx“
ż xi`1{2
xi´1{2
gpxqdx (10.38)
where ppxqdy{dx is continuous. Each term in Eq. (10.38) is then replaced with a suitable
approximation.
The first integral is split into two parts:
ż xi`1{2
xi´1{2
d
dx
´
ppxq
dy
dx
¯
dx“
ż x´
i
xi´1{2
d
´
ppxq
dy
dx
¯
`
ż xi`1{2
x`
i
d
´
ppxq
dy
dx
¯
“ppx´
i q
dy
dx˙
x´
i
´ppxi´1{2q
dy
dx˙
xi´1{2
`ppxi`1{2q
dy
dx˙
xi`1{2
´ppx`
i q
dy
dx˙
x`
i
Note that this equation holds whether xi is an interface node or not.
Next, the continuity of flux at the ith node is invoked, and the derivatives at cell
midpoints (xi´1{2 and xi`1{2) are replaced by their central difference approximations asODEs: Boundary Value Problems  601
follows:
ż xi`1{2
xi´1{2
d
dx ˆ
ppxq
dy
dx˙
dx – pi`1{2
ˆyi`1 ´ yi
Δxi
˙
´ pi´1{2
ˆyi ´ yi´1
Δxi´1
˙
(10.39)
where ppxi`1{2q “ pi`1{2 and ppxi´1{2q “ pi´1{2.
The second term of Eq. (10.38) is also split into two parts as
ż xi`1{2
xi´1{2
qpxq
dy
dxdx “
ż xi
xi´1{2
qpxq
dy
dxdx `
ż xi`1{2
xi
qpxq
dy
dxdx
For sufficiently small cells, the first derivatives are replaced by the CDFs, and qpxq is cal￾culated at the cell boundaries (qi`1{2 and qi´1{2) as
ż xi`1{2
xi´1{2
qpxq
dy
dxdx – qi`1{2
´yi`1 ´ yi
Δxi
¯Δxi
2 ` qi´1{2
´yi ´ yi´1
Δxi´1
¯Δxi´1
2 (10.40)
This approximation also allows for a piecewise variation of qpxq to be included.
The integrals of the third and fourth terms are simply expressed as follows:
ż xi`1{2
xi´1{2
rpxqypxqdx“
ż xi
xi´1{2
rpxqypxqdx `
ż xi`1{2
xi
rpxqypxqdx –
1
2
`
ri`1{2Δxi`ri´1{2Δxi´1
˘
yi
(10.41)
ż xi`1{2
xi´1{2
gpxqdx“
ż xi
xi´1{2
gpxqdx`
ż xi`1{2
xi
gpxqdx –
1
2
`
gi`1{2Δxi`gi´1{2Δxi´1
˘ (10.42)
Substituting the approximations Eqs. (10.39)-(10.42) into Eq. (10.38), for an interior ith
cell, gives
pi`1{2
´yi`1 ´ yi
Δxi
¯
´ pi´1{2
´yi ´ yi´1
Δxi´1
¯
` qi`1{2
´yi`1 ´ yi
2
¯
` qi´1{2
´yi ´ yi´1
2
¯
`
1
2
`
ri`1{2Δxi ` ri´1{2Δxi´1
˘
yi “ 1
2
`
gi`1{2Δxi ` gi´1{2Δxi´1
˘
(10.43)
or in a compact form as
biyi´1 ` diyi ` aiyi`1 “ ci, (10.44)
where
bi “ pi´1{2
Δxi´1
´ qi´1{2
2 , di “ ´ai ´ bi `
1
2
`
ri`1{2Δxi ` ri´1{2Δxi´1
˘
,
ai “ pi`1{2
Δxi
` qi`1{2
2 , ci “ 1
2
`
gi`1{2Δxi ` gi´1{2Δxi´1
˘
Step 3. Implementing BCs: Implementing Dirichlet BC at node i “ 0 or i “ M is straightfor￾ward. We consider the case of left BC (α1 “ 0). This procedure can likewise be extended to
the right BC (α2 “ 0). The nodal value y0, just as in FDM, is substituted into the difference
equation obtained by setting i “ 1 into Eq. (10.44) and carried to the rhs. Alternatively,
since the solution at the boundary node is known, y0 “ γ1 can be treated as a difference
equation. Then the coefficients of the discretized equation are determined simply by setting
β1 “ 1, d0 “ 1, c0 “ γ1, and a0 “ 0, i.e., y0 is incorporated into the linear system as if it
were an unknown.602  Numerical Methods for Scientists and Engineers
In the case of the Neumann or Robin BCs (α1y1
0 ` β1y0 “ γ1, α1 ‰ 0), Eq. (10.36) is
integrated over a control volume extending from x0 to x1{2, i.e., over a half-cell.
ż x1{2
x0
d
ˆ
ppxq
dy
dx˙
`
ż x1{2
x0
qpxq
dy
dxdx `
ż x1{2
x0
rpxqypxqdx “
ż x1{2
x0
gpxqdx, (10.45)
The first term of Eq. (10.45) gives
ż x1{2
x0
d
ˆ
ppxq
dy
dx˙
“ ppx1{2q
dy
dx˙
x1{2
´ ppx0q
dy
dx˙
x0
Replacing pdy{dxqx1{2 with the central difference approximation and evaluating pdy{dxqx0
from the left BC as y1
0 “ pγ1 ´ β1y0q{α1 yields
ż x1{2
x0
d
´
ppxq
dy
dx
¯
– p1{2
´y1 ´ y0
Δx1
¯
´ p0
´γ1 ´ β1y0
α1
¯
(10.46)
Note that, unlike in the FDM, no fictitious nodes arise in the FVM discretization of Neu￾mann or Robin BCs.
The second, third, and fourth terms of Eq. (10.45) are approximated by
ż x1{2
x0
qpxq
dy
dxdx – q1{2
´y1 ´ y0
Δx0
¯Δx0
2 (10.47)
ż x1{2
x0
rpxqypxqdx –
Δx0
2
r0y0 and ż x1{2
x0
gpxqdx –
Δx0
2 g0 (10.48)
where pdy{dxqx1{2 is replaced by its central difference approximation.
Using approximations by Eq. (10.46), (10.47), and (10.48) yields
d0y0 ` a0y1 “ c0, (10.49)
where
a0 “ p1{2
Δx0
` q1{2
2 , d0 “ ´a0 ` β1
α1
p0 `
Δx0
2
r0, c0 “ Δx0
2 g0 ´ γ1
α1
p0
The foregoing derivation is extended to the right boundary node over the control volume
xM´1{2 ď x ď xM. For the right boundary node, the difference equation results in
bMyM´1 ` dMyM “ cM, (10.50)
where
bM “ pM´1{2
ΔxM´1
´ qM´1{2
2 , dM “´bM ´ β2
α2
pM `
ΔxM´1
2
rM, cM “ ΔxM´1
2 gM ´ γ2
α2
pM
Step 4. Constructing a linear system: The FVM equations for all interior nodes, Eq. (10.44),
and those equations derived for the boundary nodes, Eqs. (10.49) and (10.50), constitute a
tridiagonal system of linear equations that can be expressed as
Ay “ bODEs: Boundary Value Problems  603
Pseudocode 10.2
Module LBVP_FVM (M, x, α, β, γ, y)
\ DESCRIPTION: A pseudomodule to solve a two-point linear BVP, defined in
\ conservation form: pppxqy1
q1 ` qpxqy1 ` rpxqy “ gpxq, a ď x ď b
\ subject to BCs: α1y1
paq`β1ypaq “ γ1, α2y1
pbq`β2ypbq “ γ2 using the FVM.
\ USES:
\ COEFFS:: Module supplying ppxq, qpxq, rpxq, gpxq;
\ TRIDIAGONAL:: Tridiagonal system solver, Pseudocode 2.13
Declare: α2, β2, γ2, aM, bM, cM, dM, xM, yM dxM \ Declare array variables
For “
k “ 1, M ´ 1
‰
dxk Ð xk`1 ´ xk \ Find cell sizes, Δxk “ xk`1 ´ xk
End For
For “
k “ 1, M‰ \ Setup of tridiagonal matrix row-wise
xk Ð xk; hp Ð dxk; xp Ð xk ` hp{2 \ Scalarize xk, dxk, xk`1{2
COEFFSpxp, pp, qp, rp, gpq \ Find pk`1{2, qk`1{2, rk`1{2, gk`1{2
If “
k “ 1
‰
Then \ Modify coefficients for left boundary
If “
α1 “ 0
‰
Then \ Set up Dirichlet BC
dk Ð 1; ck Ð γ1{βk; ak Ð 0; bk Ð 0
Else \ Set up Neumann or Robin BC
ak Ð pp{hp ` qp{2
dk Ð ´ak ` rp ˚ hp{2 ` pp ˚ β1{α1 \ Apply Eq. (10.49)
bk Ð 0; ck Ð gp ˚ hp{2 ´ pp ˚ γ1{α1
End If
Else
If “
k “ M‰
Then \ Modify coefficients for right boundary
If “
α2 “ 0
‰
Then \ Set up Dirichlet BC
dk Ð 1; ck Ð γ2{β2; ak Ð 0; bk Ð 0
Else \ Set up Neumann or Robin BC
bk Ð pm{hm ´ qm{2;
dk Ð ´bk ` rm ˚ hm{2 ´ pm ˚ β2{α2 \ Apply Eq. (10.50)
ak Ð 0; ck Ð gm ˚ hm{2 ´ pm ˚ γ2{α2
End If
Else
\ Set up coefficients for interior nodes, 2 ď k ď m ´ 1
bk Ð pm{hm ´ qm{2 \ Apply Eq. (10.44)
ak Ð pp{hp ` qp{2
dk Ð ´ak ´ bk ` prp ˚ hp ` rm ˚ hmq{2
ck Ð pgp ˚ hp ` gm ˚ hmq{2
End If
End If
hm Ð hp \ Set hm “ hp for the next cell (Δxk “ Δxk`1)
pm Ð pp; qm Ð qp \ Set coefficients, pk´1{2 “ pk`1{2, qk´1{2 “ qk`1{2)
rm Ð rp; gm Ð gp \ rk´1{2 “ rk`1{2, gk´1{2 “ gk`1{2 for the next cell
End For
TRIDIAGONAL p1,M, b, d, a, c, yq \ Solve the tridiagonal system of equations
End Module LBVP_FVM604  Numerical Methods for Scientists and Engineers
Finite Volume Method
‚ Discretization procedure requires integration of the ODE over a finite
control volume, allowing the interface conditions of the discontinuous
source terms as well as the Neumann or Robin BCs to be implemented
naturally;
‚ Unlike finite difference and finite element methods, the FVM does
not require coordinate transformation to ODEs;
‚ It is suitable for both uniform and non-uniform grids;
‚ The discretized equations are conservative, i.e., mass, momentum,
and energy are conserved for each finite (cell) volume.
‚ Order of accuracy of FVM is Oph2q, and it is very difficult to achieve
higher orders of accuracy;
‚ Computational cost of BVPs with variable coefficients increases.
where A is the coefficient matrix, r is the rhs vector, and y is the numerical solution defined
as
A “
»
—
—
—
—
—
—
—
–
d0 a0
b1 d1 a1
b2 d2 a2
... ... ...
bM´1 dM´1 aM´1
bM dM
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
, y “
»
—
—
—
—
—
—
—
–
y0
y1
y2
.
.
.
yM´1
yM
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
, b “
»
—
—
—
—
—
—
—
–
c0
c1
c2
.
.
.
cM´1
cM
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Step 5. Solving the linear system: The resulting system of linear algebraic equations is solved
by using Thomas’ algorithm. Recall that the necessary condition for the convergence of a
tridiagonal system (Section 2.10) is that the coefficient matrix must be diagonally dominant.
The matrix equations resulting from FVM discretization are diagonally dominant; thus,
convergence problems should not be expected.
A pseudomodule, LBVP_FVM, solving a linear BVP in conservation form using the
FVM is presented in Pseudocode 10.2. The module requires the number of grid points (M),
coefficients of the BCs (αi, βi, γi for i “ 1,2), and abscissas of the nodal points (xi, i“1, 2,
..., M) as input. The numerical solution is saved in the output array y. This module uses
two additional modules: (1) COEFFS, a user-defined module supplying ppxq, qpxq, rpxq, and
gpxq for any x; and (2) TRIDIAGONAL, a module solving a tridiagonal system of equations
in Pseudocode 2.13. This module allows the use of non-uniform cell sizes with ease. But,
the array containing the abscissas (x) needs to be prepared as input to the module before
invoking it.
After computing Δxk’s, the tridiagonal system elements are set up row by row using
the For-construct from k “ 1 to M. Using the COEFFS, the coefficients of the BVP (pp,
qp, rp, and gp) are computed at xk`1{2. For 1 ă k ă M, the coefficients of the difference
equation (row entries) are calculated from Eq. (10.44). The computation of coefficients of
difference equations (row elements) according to node number (left, right, or interior) and
BC type (Dirichlet, Neumann, or Robin) is provided by a series of If-Then-Else constructs.
The Dirichlet BCs are incorporated into the linear system by treating the boundary nodes
as unknowns, as done in Pseudocode 10.2. The Neumann or Robin boundary conditions areODEs: Boundary Value Problems  605
implemented with Eqs. (10.49) and (10.50) for the left (k “ 1) and right (k “ M) boundary
nodes, respectively. Once all elements of a row are calculated, the coefficients of the ODE
and cell size at xk`1{2 are no longer needed. So these are assigned as the quantities of the
cell on the left, i.e., pm Ð pp, qm Ð qp, rm Ð rp, gm Ð gp, and hm Ð hp correspond to
the values at xk´1{2. The solution of the tridiagonal system is carried out in TRIDIAGONAL
using the Thomas algorithm, and the solution is returned in y.
EXAMPLE 10.5: Application of FVM to heat transfer from a fuel rod
Consider a cylindrical nuclear fuel pin (kf “ 16 W/m˝C) with radius R “ 5 cm,
enclosed by a 2 cm thick steel cladding (kc “ 50 W/m˝C, outer radius Ro “ 7 cm),
as shown in the figure. During steady operation, heat is generated in the nuclear
fuel at a uniform rate of q3
0 “ 106 W/m3. The fuel assembly in the pressurized
reactor is cooled by water at T8 “ 132˝C, which adjoins the outer surface, and it is
characterized by a convection coefficient of h “ 38000 W/m2˝C.
Assuming a one-dimensional conduction heat transfer condition exists, the temper￾ature distribution for this system is governed by the following ODE:
1
r
d
dr
´
rkprq
dT
dr
¯
` q3prq “ 0, 0 ď r ď Ro
where r is the radial coordinate (m), k is the thermal conductivity (W/m¨
˝C), q3prq
is the internal heat generation (W/m3), and T is the temperature (˝C). The con￾ductivity and heat generation depict a jump discontinuity at the fuel-clad interface,
i.e.,
kprq “ "
kf , r ă R
kc, r ą R , q3prq “ "
q3
0 , r ă R
0, r ą R
The corresponding boundary and interface conditions are given as follows:
Radial symmetry at r “ 0 pdT{drqr“0 “ 0
Continuity of Tprq at r “ R TpR´q “ TpR`q
Continuity of qprq at r “ R ´kf pdT{drqr“R´ “ ´kcpdT{drqr“R`
Convection transfer at r “ Ro ´kcpdT{drqr“Ro “ h pTpRoq ´ T8q
Apply the FVM to obtain the temperature distribution within the fuel and clad using
uniform M “ 7, 14, 28, and 56 cells. Comment on the accuracy of the distribution.
SOLUTION:
An ODE of this type is very common in fluid flow and heat transfer problems.
The steady-state diffusion equation (neglecting the transient and convective terms)
yields
divpD grad φq ` Sφ “ 0 (10.51)606  Numerical Methods for Scientists and Engineers
where D is the diffusion coefficient, Sφ is called the source, and φ is a transport
property.
Equation (10.51) for one-dimensional geometries can be expressed as
1
rg
d
dr
´
rgDprq
dφ
dr
¯
` Sφprq “ 0 (10.52)
where g “ 0, 1, and 2 denote plane parallel, cylinder, and sphere geometry, re￾spectively, and r denotes the axial coordinate for the plane parallel coordinate or
the radial coordinate for the cylinder or sphere. In this example, the property φ
represents the temperature.
Step 1. Gridding: Gridding is carried out by dividing the fuel with cladding regions
into M uniform cells of size Δr “ Ro{M. Nodal points are placed on cell boundaries
at ri, as shown in Fig. 10.13. The positions of the cell boundaries and midpoints
are obtained by ri “ i Δr and ri`1{2 “ ri ` Δr{2, respectively, and M is chosen
such that rN “ NΔr. Note that the cell boundary nodes coincide with the physical
boundaries.
FIGURE 10.13: Gridding the fuel-pin system.
Step 2. Discretizing: To discretize, first the governing ODE is multiplied by the cylin￾drical volume element dV “ rdr (omitting 2π) and integrated over the finite cell
volume, ri´1{2 ď r ď ri`1{2:
ż ri`1{2
ri´1{2
d
dr
´
rkprq
dT
dr
¯
dr “ ´ż ri`1{2
ri´1{2
rq3prqdr
The material conductivity varies step-wise at r “ rN ; thus, the integral is split
into two parts, where the heat flux ´kprqdT{dr is continuous on both sides of the
interface (i.e., interface condition). This equality holds for any node placed at ri,
whether it is an interface point or not. Keeping this in mind, the term on the left of
the equality sign can be integrated as follows:
ż ri`1{2
ri´1{2
d
dr
´
rkprq
dT
dr
¯
dr “
ż ri
ri´1{2
d
dr
´
rkprq
dT
dr
¯
dr `
ż ri`1{2
ri
d
dr
´
rkprq
dT
dr
¯
dr
“ri
´
k
dT
dr
¯
r´
i
´ri´1{2ki´1{2
dT
dr ˙
ri´1{2
`ri`1{2ki`1{2
dT
dr ˙
ri`1{2
´ ri
´
k
dT
dr
¯
r`
iODEs: Boundary Value Problems  607
where ki˘1{2 “ kpri˘1{2q denote the right- and left-hand cell conductivities, respec￾tively.
Invoking the continuity of heat flux at r “ ri, pkdT{drqr´
i
“ pkdT{drqr`
i
, and
replacing the derivatives at ri´1{2 and ri`1{2 with the CDFs yields
ż ri`1{2
ri´1{2
d
dr
´
kr dT
dr
¯
dr – ri`1{2ki`1{2
Ti`1 ´ Ti
Δr ´ ri´1{2ki´1{2
Ti ´ Ti´1
Δr
Making use of Eq. (10.42), the term on the right-hand side becomes:
ż ri`1{2
ri´1{2
rq3prqdr“
ż ri
ri´1{2
rq3prqdr`
ż ri`1{2
ri
rq3prqdr–
Δr
2
`
ri´1{2q3
i´1{2 ` ri`1{2q3
i`1{2
˘
Finally, the general form of the discretized equations for interior nodes can be written
as
biTi´1 ` diTi ` aiTi`1 “ ci, pi “ 1, 2,...,M ´ 1q
where
ai “
ˆ ri
Δr
`
1
2
˙
ki`1{2, ci “ ´ ´
ri´1{2q3
i´1{2 ` ri`1{2q3
i`1{2
¯ Δr
2
bi “
ˆ ri
Δr ´ 1
2
˙
ki´1{2, di “ ´ai ´ bi
Step 3. Implementing BCs: The left and right BCs are Neumann and Robin types, re￾spectively. To obtain the finite volume equations for the boundary cells, we integrate
the cells over the half-control volumes.
Noting that the conductivity and heat generation are k “ kf and q3 “ q3
0 at the
left boundary node, integration of Eq. (10.52) over [r0, r1{2] yields
ż r1{2
r0
d
dr
´
rkprq
dT
dr
¯
dr `
ż r1{2
r0
rq3prqdr“r1{2kf
dT
dr ˙
r1{2
´ r0kf
dT
dr ˙
r0
` r1{2q3
0
Δr
2 “0
In this expression, the second term on the rhs is zero since r0 “ 0 and also
pdT{drqr“0 “ 0. The integral over the source term is approximated with the mid￾point rule. The temperature gradient at r1{2 is replaced with the central difference
approximation as pdT{drqr1{2 – pT1 ´ T0q{Δr.
Substituting the foregoing approximations into the above expression and simpli￾fying leads to
d0T0 ` a0T1 “ c0
where a0 “ kf {2, d0 “ ´a0, and c0 “ ´r1{2 q3
0 Δr{2.
For the right boundary cell (with k “ kc and q3 “ 0), integration of Eq. (10.52)
over [rM´1{2, rM] gives
ż rM
rM´1{2
d
dr
´
rkprq
dT
dr
¯
dr “ rMkc
dT
dr ˙
rM
´ rM´1{2kc
dT
dr ˙
rM´1{2
“ 0
Note that kcpdT{drqrM can be obtained from the convection transfer BC, which can608  Numerical Methods for Scientists and Engineers
be expressed as kcdTM{dr “ hpT8 ´ TMq. The temperature gradient at rM´1{2 is
approximated with the CDF as pdT{drqrM´1{2 – pTM ´ TM´1q{Δr. Substituting
these approximations into the above equation yields
´hrMpTM ´ T8q ´ rM´1{2kc
TM ´ TM´1
Δr “ 0
or rearranging and simplifying similar terms leads to
bMTM´1 ` dMTM “ cM
where bM “ kcrM´1{2{Δr, dM “ ´aM ´ hrM, and cM “ ´hrMT8.
Step 4. Constructing a linear system: We find M ´ 1 equations for the interior nodes
(0 ă i ă M), an equation for the left node (i “ 0), and one for the right node
(i “ M), which together constitute a linear system of M `1 equations. They can be
expressed in matrix form as
»
—
—
—
—
—
—
—
—
—
—
–
d0 a0
b1 d1 a1
... ... ...
bN dN aN
... ... ...
bM´1 dM´1 aM´1
bM dM
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
—
—
–
T0
T1
.
.
.
TN
.
.
.
TM´1
TM
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
—
—
—
–
c0
c1
.
.
.
cN
.
.
.
cM´1
cM
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Step 5. Solving the linear system: The solution of this tridiagonal system of equations
for M “ 7, 14, 28, and 56 gives numerical estimates for the ODE. (Finding the true
solution of this ODE is quite an easy task and is left to the reader as an exercise.)
TABLE 10.6
True True Error, absolute
x Solution M “ 7 M “ 14 M “ 28 M “ 56
0 320.944 2.773 0.828 0.241 0.069
0.01 319.382 1.210 0.307 0.077 0.019
0.02 314.694 0.689 0.174 0.044 0.011
0.03 306.882 0.376 0.094 0.023 0.006
0.04 295.944 0.154 0.039 0.010 0.003
0.05 281.881 0.019 0.004 0.001 0
0.06 277.323 0.007 0.001 0 0.001
0.07 273.469 0.001 0.001 0.001 0.001
Discussion: The true solution, as well as the true errors, are comparatively pre￾sented in Table 10.6. The maximum absolute errors are obtained at the center of the
fuel pin as 2.773, 0.828, 0.241, and 0.069˝C for M “ 7, 14, 28, and 56, respectively.
The FVM is a second-order accurate method; thus, when doubling the grid size, the
global errors are reduced by a factor of four on average. The numerical estimates
improve with an increasing number of cells due to the reduction in the truncation
error as the cell size decreases.ODEs: Boundary Value Problems  609
10.7 FINITE DIFFERENCE SOLUTION NONLINEAR BVPS
Consider the following nonlinear two-point BVP subject to Dirichlet BCs:
y2 “ fpx, y, y1
q, a ď x ď b, ypaq “ A, ypbq “ B (10.53)
where A and B are known real constants. Furthermore, assume that f is continuous on
the domain D “ tpx, y, y1
qPra, bs, y, y1 P Ru, Bf{By, and Bf{By1 exist and are continuous
functions on D, Bf{By ą 0, and Bf{By1 is bounded on D. The preceding assumptions
guarantee that the BVP defined by Eq. (10.53) has a unique solution.
The finite difference method introduced in this section can be used to numerically solve
nonlinear two-point BVPs. However, the resulting system of discretized equations yields
a simultaneous system of nonlinear equations. In this section, we will present in detail
the steps for solving nonlinear two-point BVPs subjected to Dirichlet BCs. The numerical
procedure can be similarly and easily extended to nonlinear two-point BVPs with mixed
BCs. The numerical solution procedure follows the same basic steps: gridding, discretizing,
implementing BCs, constructing, and solving the system of equations.
Step 1. Gridding. The computational domain (a ď x ď b) is divided into M-subintervals to
create uniformly or non-uniformly spaced grid nodes. Nevertheless, for the sake of simplicity,
we adopt uniform subintervals: h “ pb ´ aq{M. The nodes are enumerated starting from
0 to M. For given grids similar to those shown in Fig. 10.2, the abscissas are calculated:
xi “ a ` ih, where i “ 0, 1, 2,...,M.
Step 2. Discretizing. Equation (10.53) is satisfied for an arbitrary node xi in (a, b): y2
i “
fpxi, yi, y1
iq. The discretization is carried out by approximating the derivatives of y with the
CDFs:
yi`1 ´ 2yi ` yi´1
h2 “ f
´
xi, yi,
yi`1 ´ yi´1
2h
¯
, (10.54)
which satisfies both true and approximate solutions of the nonlinear ODE. In this context,
from now on, we will use yi to denote a numerical (estimate) solution.
Step 3. Implementing the BCs. Implementing the Dirichlet BCs is as simple as expressing
them as difference equations such that y0 ´ A “ 0 and yM ´ B “ 0.
Step 4. Constructing a nonlinear system. Rearranging Eq. (10.54) yields the following simul￾taneous system of non-linear equations:
gpyq “ 0 (10.55)
where y “ ry0, y1, ¨¨¨ , yM´1, yMs
T and g “ rg0, g1, ¨¨¨ , gM´1, gMs
T , and
g0 “ y0´A
gi “ yi`1´2yi`yi´1 ´ h2f
´
xi, yi,
yi`1´yi´1
2h
¯
, i“1, 2,...,M ´1
gM “ yM ´B
Step 5. Solving the nonlinear system. This nonlinear system is solved iteratively using a
method such as fixed-point iteration, Newton’s method, etc. Here, we will describe the
implementation of two techniques: (1) Tridiagonal Iteration and (2) Newton’s methods.610  Numerical Methods for Scientists and Engineers
In nonlinear problems, regardless of the numerical method employed, the initial guess is
crucial for the convergence of the BVPs. Fortunately, a nonlinear two-point BVP subjected
to Dirichlet BCs is unique in that the distribution of the numerical solution can be roughly
estimated with the aid of the boundary values. For instance, in Eq. (10.53), the numerical
solution varies between ypaq “ A and ypbq “ B. Hence, a smart initial guess for each nodal
point can be obtained through linear interpolation between pa, Aq and pb, Bq as
yi “ A `
B ´ A
b ´ a pxi ´ aq, i “ 1, 2,...,pM ´ 1q (10.56)
10.7.1 TRIDIAGONAL ITERATION METHOD (TIM)
The left-hand side of Eq. (10.54) yields a tridiagonal matrix whose diagonal elements are
constant. Hence, an iterative scheme is established by casting Eq. (10.55) as
Typp`1q “ rpyppq
q (10.57)
where y denotes the estimates of the nodal points, T and rpyq are defined as
T “
»
—
—
—
—
—
–
1 0
1 ´2 1
... ... ...
1 ´2 1
0 1
fi
ffi
ffi
ffi
ffi
ffi
fl
, rpyq “
»
—
—
—
—
—
—
—
—
–
A
h2f
´
x1, yppq
1 ,
yppq
2 ´ yppq
0
2h
¯
.
.
.
h2f
´
xM´1, yppq
M´1,
yppq
M ´ yppq
M´2
2h
¯
B
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Note that when the Robin BC is imposed to the left, right, or both boundaries, then only
the first, last, or both rows will need to be modified.
Pseudocode 10.3
Module NONLINEAR_TIM (M, ε, x, yo, y, α, β, γ, maxit)
\ DESCRIPTION: A pseudomodule to solve a nonlinear two-point BVP of the
\ form: y2 “ fpx, y, y1
q, a ď x ď b subject to BCs:
\ α1y1
paq ` β1ypaq “ γ1, α2y1
pbq ` β2ypbq “ γ2, using TIM method.
\ USES:
\ FUNC:: A user supplied Function Module defining fpx, y, y1
q;
\ ENORM:: A function calculating �2´norm of a vector, Pseudocode 3.1;
\ TRIDIAGONAL:: Tridiagonal system solver, Pseudocode 2.13.
Declare: α2, β2, γ2, aM, bM, cM, dM, xM, yM yoM \ Declare array variables
h Ð x2 ´ x1 \ Set interval size
h2 Ð 2 ˚ h; hsqr Ð h ˚ h \ Save 2h & h2 on scalar variables
p Ð 0 \ Initialize iteration counter
Repeat \ Iteration loop
For “
k “ 1, M‰ \ Construct coefficient (tridiagonal) matrix
If “
k “ 1
‰
Then \ Set up coefficients for left boundary
If “
α1 “ 0
‰
Then \ Set up Dirichlet BC
dk Ð 1; ck Ð γ1{β1
ak Ð 0; bk Ð 0
Else \ Set up Neumann or Robin BC
yp Ð pγ1 ´ β1 ˚ yokq{α1 \ Compute y1
1ODEs: Boundary Value Problems  611
ak Ð 2; bk Ð 0; dk Ð ´2 ` h2 ˚ β1{α1
ck Ð hsqr ˚ FUNCpxk, yok, ypq ` h2 ˚ γ1{α1
End If
Else
If “
k “ M‰
Then \ Set up coefficients for right boundary
If “
α2 “ 0
‰
Then \ Set up Dirichlet BC
dk Ð 1; ck Ð γ2{β2; ak Ð 0; bk Ð 0
Else \ Set up Neumann or Robin BC
yp Ð pγ2 ´ β2 ˚ yokq{α2 \ Evaluate y1
n
bk Ð 2; ak Ð 0; dk Ð ´2 ´ h2 ˚ β2{α2
ck Ð hsqr ˚ FUNCpxk, yok, ypq ´ h2 ˚ γ2{α2
End If
Else \ Set up coefficients for interior nodes by Eq. (10.57)
yp Ð pyok`1 ´ yok´1q{h2 \ Evaluate y1
k
ck Ð hsqr ˚ FUNCpxk, yok, ypq \ Find rhs
ak Ð 1; dk Ð ´2; bk Ð 1 \ Set up lhs–coefficients of TDM
End If
End If
End For
TRIDIAGONALp1,M, b, d, a, c, yq \ Solve tridiagonal system
error Ð ENORMpM, y ´ yoq \ Find Euclidean norm, |yppq ´ ypp´1q
|2
p Ð p ` 1 \ Count iterations
Write: “p, error=”, p, error \ Print out iteration progress
yo Ð y \ Set current estimates as prior
Until “
error ă ε Or p “ maxit‰
If “
p “ maxit‰
Then \ Issue info and a warning
Write: “Failed to converge after”,maxit,“iterations”
Write: “Current error is ”, error
End If
End Module NONLINEAR_TIM
A pseudo-module, NONLINEAR_TIM, solving a non-linear BVP given with Eq. (10.53)
using the TIM is presented in Pseudocode 10.3. As input, the module requires the number of
grid points (M), an array of abscissas (xi), an initial guess (yoi, i “ 1, 2,...,M), an upper
bound for the maximum number of iterations allowed (maxit), a convergence tolerance (ε),
and the BCs (αi, βi, γi, i “ 1,2). On exit, the numerical solution is saved in the array y.
The module also makes use of two external modules: (1) FUNC, supplying y2 “ fpxi, yi, y1
iq;
and (2) TRIDIAGONAL, a module for solving a tridiagonal system of equations (Pseudocode
2.14). Only the first and last rows of the difference equations are modified to incorporate
the BCs. A Dirichlet BC (e.g., yi “ value) can be easily incorporated into the system by
simply setting di “ 1, ai “ bi “ 0, and ci “ value, where i “ 1 or M. In the case of
Neumann or Robin BC (α1 ‰ 0 or α2 ‰ 0), the fictitious node will appear in the difference
equations. A fictitious node is eliminated by applying the same usual procedure as for the
linear BVPs. The nonlinear terms are kept on the rhs of Eq. (10.57). The construction of T
is easy since all rows consist of 1’s and ´2’s. The tridiagonal system of equations is solved
at each iteration using the Thomas algorithm. It is important to start with a “smart guess,”
yp0q in order for the system to converge or make the system converge faster. After updating
the rhs rpyq with the updated estimates, the procedure by which Eq. (10.57) is solved is
repeated until the current and prior estimates converge within a predetermined tolerance
value, i.e., �ypp`1q ´ yppq
�2 ă ε.612  Numerical Methods for Scientists and Engineers
Finite Difference Methods for Nonlinear ODEs
‚ Finite difference methods are ideal for treating unstable nonlinear
problems;
‚ The resulting tridiagonal system of equations can be efficiently solved
using Thomas’s algorithm;
‚ If a BVP converges, it converges quadratically.
‚ FDM may lead to a large number of simultaneous nonlinear equa￾tions, requiring iterative schemes to achieve an approximate solution
with the desired accuracy;
‚ A smart initial guess (or a rough prediction) may be necessary to
ensure convergence.
EXAMPLE 10.6: Solving a nonlinear BVP
A gas, labeled as X, in a sealed container diffuses into
the liquid Y (shown in the figure) that undergoes the
second-order reaction X ` Y Ñ XY . Assuming the
amount of the reaction product XY is negligible, a
simplified mathematical model for the concentration
of diffusing X into Y can be expressed by the following
ODE:
d
dz ˆ
Dpcq
dc
dz ˙
´ kc2 “ 0
where c is the concentration of X, k is the rate constant of the reaction, D is
the diffusion coefficient, which is linearly proportional to the concentration (Dpcq “
D0 cpzq), and z is the coordinate axis. Introducing dimensionless quantities, the BVP
and the BCs are rewritten as
d
dZ ˆ
C dC
dZ ˙
´ φ2C2 “ 0, Cp0q “ 1, dC
dZ p1q “ 0
where C “ cpzq{c0, Z “ z{H, and φ2 “ h2k{D0, c0 is the gas concentration at z “ 0
(liquid surface), H is the depth of the liquid, and φ is a parameter called the Thiele
modulus. For φ2 “ 1.25, use the TIM iteration scheme to obtain the numerical
solution for uniformly spaced M “ 10, 20, 40, and 80 grids. The true solution is
given as
CpZq “ b
coshpφ
?
2p1 ´ Zqq{ coshpφ
?
2q
SOLUTION:
To facilitate understanding of the application of the theoretical formulation pre￾sented, in this example we replace the dependent variable C with y (i.e., CpZq Ñ
ypxq) and the independent variable Z with x (i.e., Z Ñ x). Then the BVP can be
expressed in the standard nonlinear form as
y2 “ fpx, y, y1
q “ φ2y ´ py1
q
2
{y, yp0q “ 1, y1
p1q “ 0ODEs: Boundary Value Problems  613
Step 1: Gridding. The interval [0,1] is divided into M subintervals (h “ 1{M), with
nodes placed on both sides of the subintervals. The nodes are enumerated from 0 to
M, and the abscissas are determined by xi “ ih for all i.
FIGURE 10.14
Step 2. Discretizing. The ODE, which is valid for any arbitrary x “ xi, is discretized
using the CDFs as follows:
yi`1 ´ 2yi ` yi´1
h2 “ f
ˆ
xi, yi,
yi`1´yi´1
2h
˙
, i“1, 2,...,M
Step 3. Implementing BCs. For the left and right boundary nodes, the BCs are ex￾pressed as y0 “1 and y1
M “0, respectively. Applying the CDF to y1
M “0, we obtain
yM`1 “yM´1.
Step 4. Constructing a nonlinear system. Substituting the known values into the first
and last equations yields a system of pM´1qˆpM´1q equations, expressed in matrix
form as
Typp`1q “ rpyppq
q
where x “ ry1, y2, ¨¨¨ , yM´1, yMs
T , and
T “
»
—
—
—
—
—
—
—
—
—
–
´2 1
1 ´2 1
... ... ...
1 ´2 1
2 ´2
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
, r “
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
´ 1 ` h2f
´
x1, yppq
1 ,
yppq
2 ´ 1
2h
¯
h2f
´
x2, yppq
2 ,
yppq
3 ´ yppq
1
2h
¯
.
.
.
h2f
´
xM´1, yppq
M´1,
yppq
M ´ yppq
M´2
2h
¯
h2f
´
xM, yppq
M , 0
¯
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
where the boxed values are the BCs, i.e., y0 “ 1 and y1
M “ 0.
Step 5. Solving the system. An initial guess of a fixed value between 0 and 1 is
assigned. Then the current estimate yp1q is found from the solution of the tridiagonal
system. In the first iteration, the difference between the prior and current estimates
will likely be large. The current estimates are set as prior estimates before the next
iteration: yppq Ð ypp`1q
. The iteration procedure is repeated until the current and
prior estimates converge within ε ă 10´6.614  Numerical Methods for Scientists and Engineers
TABLE 10.7
True True (absolute) error
x Solution M “ 10 M “ 20 M “ 40 M “ 80
0 1
0.1 0.930939 1.58E-05 3.92E-06 1.01E-06 2.32E-07
0.2 0.868912 2.24E-05 5.57E-06 1.45E-06 3.23E-07
0.3 0.813807 2.10E-05 5.18E-06 1.37E-06 2.90E-07
0.4 0.765603 1.26E-05 3.09E-06 8.65E-07 1.51E-07
0.5 0.724360 9.85E-07 3.08E-07 3.04E-08 6.67E-08
0.6 0.690212 1.77E-05 4.48E-06 1.00E-06 3.31E-07
0.7 0.663341 3.49E-05 8.78E-06 2.07E-06 6.03E-07
0.8 0.643952 5.00E-05 1.25E-05 3.00E-06 8.40E-07
0.9 0.632231 6.03E-05 1.51E-05 3.64E-06 1.00E-06
1 0.628308 6.39E-05 1.60E-05 3.87E-06 1.06E-06
The true solution, along with the true absolute errors of the numerical estimates
obtained using the tridiagonal iteration technique with uniform intervals of M “
10, 20, 40, and 80, are tabulated in Table 10.7. The average errors are 3 ˆ 10´5,
7.5 ˆ 10´6, 1.83 ˆ 10´6, and 4.9 ˆ 10´7, respectively, for M “ 10, 20, 40, and 80.
When the intervals are doubled, the global errors decrease by about fourfold due to
the difference scheme and second-order global error.
Discussion: The convergence of most nonlinear two-point BVPs depends on rpyq.
Convergence can be achieved fairly quickly as long as the initial guess is in the
vicinity of the true solution. However, if there are rapidly growing terms on the rhs,
such as ey, ym (large m), and so on, convergence issues may be experienced.
10.7.2 NEWTON’S METHOD
Consider the nonlinear system given by Eq. (10.55). This system can be solved with New￾ton’s method, which was also applied to solve nonlinear systems of equations (see Section
4.8). The method leads to an improved solution of the following form:
ypp`1q “ yppq ` δppq (10.58)
where δppq is the displacement vector, which is obtained from solving
Jpyppq
q δppq “ ´gpyppq
q (10.59)
where Jpyppq
q is the Jacobian matrix for this system, leading to a tridiagonal matrix given
as
J pyppq
q “
»
—
—
—
—
—
—
—
—
—
—
–
1 0
b1 d1 a1
... ... ...
bi di ai
... ... ...
bM´1 dM´1 aM´1
0 1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
and for i“1, 2,...,pM ´1q the diagonal arrays are obtained fromODEs: Boundary Value Problems  615
bi “ Bgi
Byi´1
“ 1 `
h
2
Bf
By1
´
xi, yppq
i ,
yppq
i`1 ´ yppq
i´1
2h
¯
di “ Bgi
Byi
“ ´2 ´ h2 Bf
By
´
xi, yppq
i ,
yppq
i`1 ´ yppq
i´1
2h
¯
ai “ Bgi
Byi`1
“ 1 ´ h
2
Bf
By1
´
xi, yppq
i ,
yppq
i`1 ´ yppq
i´1
2h
¯
(10.60)
For a simple function f, the Jacobian matrix can be easily constructed analytically and
solved efficiently. For functions containing more complex expressions, numerical differen￾tiation is applied to construct the Jacobian, which also contributes to numerical errors.
Pseudocode 10.4
Module NONLINEAR_NEWTON (M, ε, x, yo, y, α, β, γ, maxit)
\ DESCRIPTION: A pseudomodule to solve a nonlinear two-point BVP of the
\ form: y2 “ fpx, y, y1
q, a ď x ď b subject to BCs:
\ α1y1
paq ` β1ypaq “ γ1, α2y1
pbq ` β2ypbq “ γ2 with Newton’s method.
\ USES:
\ FUNCS:: A user-defined module supplying fpx, y, y1
q, Bf{By, Bf{By1
;
\ ENORM:: A function calculating �2´norm of a vector, Pseudocode 3.1;
\ TRIDIAGONAL:: Tridiagonal system solver, Pseudocode 2.13
Declare: α2, β2, γ2, aM, bM, cM, dM, xM, yM, yoM \ Declare array variables
h Ð x2 ´ x1 \ Set interval size
h2 Ð 2 ˚ h; hsqr Ð h ˚ h \ Precompute multipliers to save time
C1d Ð h2 ˚ β1{α1; C1r Ð h2 ˚ γ1{α1
C2d Ð h2 ˚ β2{α2; C2r Ð h2 ˚ γ2{α2
p Ð 0 \ Initialize iteration counter
Repeat \ Iterate until convergence is achieved
For “
k “ 1, M‰ \ Construct tridiagonal system
If “
k “ 1
‰
Then \ Set up coefficients for left boundary
If “
α1 “ 0
‰
Then \ Set up Dirichlet BC
ak Ð 0; bk Ð 0; ck Ð 0; dk Ð 1; yok Ð γ1{β1
Else \ Set up Neumann or Robin BC
yx Ð pγ1 ´ β1 ˚ yokq{α1 \ Evaluate y1
1
FUNCS(xk, yok, yx, f, fy, f p) \ Find f, fy, fy1 at x1
ak Ð 2; bk Ð 0
dk Ð ´2 ` C1d ´ hsqr ˚ pfy ´ f p ˚ β1 ˚ yok{α1q
ck Ð ´C1r ` pC1d ´ 2q ˚ yok ` 2 ˚ yok`1 ´ hsqr ˚ f
End If
Else
If “
k “ M‰
Then \ Set up coefficients for right boundary
If “
α2 “ 0
‰
Then \ Set up Dirichlet BC
ak Ð 0; bk Ð 0; ck Ð 0; dk Ð 1; yok Ð γ2{β2
Else \ Set up Robin BC
yx Ð pγ2 ´ β2 ˚ yokq{α2 \ Evaluate y1
n
FUNCS(xk, yok, yx, f, fy, f p) \ Find f, fy, fy1 at xM
ak Ð 0; bk Ð 2 \ Modify coefficients of the last row
dk Ð ´2 ´ C2d ´ hsqr ˚ pfy ´ f p ˚ β2 ˚ yok{α2q616  Numerical Methods for Scientists and Engineers
ck Ð C2r ´ pC2d ` 2q ˚ yok ` 2 ˚ yok´1 ´ hsqr ˚ f
End If
Else \ Set up coefficients for interior nodes
yx Ð pyok`1 ´ yok´1q{h2 \ Evaluate y1
k
yxx Ð yok`1 ´ 2 ˚ yok ` yok´1 \ Find lhs
FUNCS(xk, yok, yx, f, fy, f p) \ Find fk, pfyqk, pfy1qk
ak Ð 1 ` f p{h2; dk Ð ´2 ´ hsqr ˚ fy
bk Ð 1 ´ f p{h2; ck Ð yxx ´ hsqr ˚ f
End If
End If
End For
TRIDIAGONAL(1,M, b, d, a, c, y) \ Solve tridiagonal system for correction
error Ð ENORMpM, cq \ Find Euclidean norm
y Ð yo ´ c \ Find current estimates
p Ð p ` 1 \ Count iterations
Write: “p, error=”, p, error \ Print out iteration progress
yo Ð y \ Set current estimates as prior
Until “
error ă ε Or p “ maxit‰
If “
p “ maxit‰
Then \ Issue info and a warning
Write: “Failed to converge after”,maxit,“iterations”
End If
End Module NONLINEAR_NEWTON
Module FUNCS (x, y, yx, fun, df dy, df dp)
\ DESCRIPTION: A user-defined module to evaluate f, Bf{By, Bf{By1 at px, y, y1
q.
fun Ð fpx, y, yxq \ Define ODE here
df dy Ð Bf{By \ Define partial derivatives here
df dp Ð Bf{By1
End Module FUNCS
A pseudo-module, NONLINEAR_NEWTON, solving a nonlinear two-point BVP given
with Newton’s method is presented in Pseudocode 10.4. This module also allows Robin BCs
to be imposed on both boundaries. The argument list is the same as that in Pseudocode
10.3. It requires (1) FUNCS, which provides fpxi, yi, y1
iq, as well as Bf{By and Bf{By1 partial
derivatives at pxi, yi, y1
iq, and (2) TRIDIAGONAL, a tridiagonal linear system solver applying
the Thomas’ algorithm (see Pseudocode 2.13). The coefficients of the Jacobian are computed
from Eq. (10.60). Only the first and last rows of J are modified to incorporate the left and
right BCs. The Dirichlet BC (yppq
i “ γi{βi) is imposed by simply setting the coefficients as
di “ 1, ai “ bi “ ci “ 0, where i “ 1 and 2 for the left and right boundaries, respectively.
In Neumann or Robin BCs (α1 ‰ 0 or α2 ‰ 0), the fictitious nodes are eliminated by
applying the same procedure employed as in the cases of two-point BVPs; hence, only the
first or last row (i “ 1 or M) coefficients (ai, bi, ci, and di’s) are modified accordingly. A
smart initial estimate is helpful for speedy convergence. When the Dirichlet BC is applied
on both boundaries, a smart initial guess can be found from Eq. (10.56). The initial guess
is improved by solving Eqs. (10.59) using Thomas’ algorithm, and the current estimates are
updated by Eq. (10.58). This procedure is successively repeated to obtain better estimates.
The convergence (or termination) criterion is based on the displacement; that is, when
δppq
2 “ ypp`1q ´ yppq
2 ă ε.ODEs: Boundary Value Problems  617
EXAMPLE 10.7: Solving a nonlinear BVP with Newton’s method
Repeat Example 10.6 for φ2 “ 16 using the Newton’s method.
SOLUTION:
The gridding and discretization steps are the same as those presented in Example
10.6. The resulting nonlinear difference equations are expressed as gpyq “ 0, which
is explicitly given as
g1 “´2y1 ` y2 ` 1 ´ h2f
´
x1, y1,
y2 ´ 1
2h
¯
“ 0
gk “yk´1 ´ 2yk ` yk`1 ´ h2f
´
xk, yk,
yk`1 ´ yk´1
2h
¯
“ 0, k“2,...,M ´1
gM “yM´1 ´ 2yM ` yM´1 ´ h2f
`
xM, yppq
M , 0 ˘
“ 0
where fpx, y, y1
q “ φ2y ´ py1
q
2
{y. Note that the known BCs (y0 “ 1 and y1
M “ 0)
are boxed. Since y1
M “ 0, we set yM`1 “ yM´1 in the final difference equation and
highlighted it by boxing it too.
As in Example 10.6, an initial guess, yp0q, is obtained by a linear interpolation.
Using Newton’s method, the numerical approximations are estimated from
ypp`1q “ yppq ` δppq
where δppq is found from the solution of a tridiagonal system of linear equations.
Noting that Bf{By1 “ f1py, y1
q“´2y1
{y and Bf{By “ f2py, y1
q “ φ2 ` py1
{yq2, the
displacement vector, is obtained by solving the following system of linear equations:
Jpyppq
q δppq “ ´gpyppq
q
where J is the Jacobian, and using the tridiagonal matrix notations, the matrix
elements are
bi “ Bgi
Byi´1
“ 1 `
h
2
f1
´
yppq
i ,
yppq
i`1 ´ yppq
i´1
2h
¯
di “ Bgi
Byi
“ ´2 ´ h2f2
´
yppq
i ,
yppq
i`1 ´ yppq
i´1
2h
¯
, i“1, 2,...,M
ai “ Bgi
Byi`1
“ 1 ´ h
2
f1
´
yppq
i ,
yppq
i`1 ´ yppq
i´1
2h
¯
For i “ 1, we find
d1 “ ´2 ´ h2f2
´
yppq
1 ,
yppq
2 ´ 1
2h
¯
, a1 “ 1 ´ h
2
f1
´
yppq
1 ,
yppq
2 ´ 1
2h
¯
For i “ M, having eliminated the resulting fictitious node in the usual manner, we
obtain
bM “ 2, dM “ BgM
ByM
“ ´2 ´ h2f2
´
yppq
M , 0
¯
Starting with a uniform initial guess of yp0q
i “ 0.3, this system converges in five
iterations. At the end of each iteration, the �2´norm is found as618  Numerical Methods for Scientists and Engineers
�δp1q
�2 “ 1.112720 �δp2q
�2 “ 0.215433 �δp3q
�2 “ 0.025728
�δp4q
�2 “ 2.1727 ˆ 10´4 �δp5q
�2 “ 1.3790 ˆ 10´8
The true solution and the absolute errors resulting from numerical estimates for
uniformly spaced M “ 10, 20, 40, and 80 intervals are presented in Table 10.8. The
average errors are 2.2 ˆ 10´3, 5.5 ˆ 10´4, 1.4 ˆ 10´4, and 3.5 ˆ 10´5 for M “ 10, 20,
40, and 80, respectively.
TABLE 10.8
True Absolute true error
x Solution M “ 10 M “ 20 M “ 40 M “ 80
0 1
0.1 0.753648 1.74E-03 4.42E-04 1.11E-04 2.77E-05
0.2 0.568001 2.63E-03 6.65E-04 1.67E-04 4.18E-05
0.3 0.428120 2.97E-03 7.51E-04 1.88E-04 4.71E-05
0.4 0.322771 2.98E-03 7.53E-04 1.89E-04 4.72E-05
0.5 0.243539 2.79E-03 7.04E-04 1.76E-04 4.41E-05
0.6 0.184210 2.48E-03 6.26E-04 1.57E-04 3.93E-05
0.7 0.140381 2.11E-03 5.32E-04 1.33E-04 3.34E-05
0.8 0.109345 1.70E-03 4.29E-04 1.08E-04 2.69E-05
0.9 0.090194 1.32E-03 3.35E-04 8.42E-05 2.11E-05
1 0.083588 1.15E-03 2.93E-04 7.37E-05 1.85E-05
FIGURE 10.15
Discussion: Notice that the absolute errors are much larger in φ2 “ 16 in com￾parison to the φ2 “ 1.25 case presented in Example 10.6. To understand the reason
for this, we refer to the concentration plot for φ2 “ 0.1, 1, and 10 presented in Fig.
10.15. It is clear that CpZq Ñ 1 as φ Ñ 0. In other words, the gas penetration into
the liquid by diffusion is faster and easier; that is why the CpZq variation tends to be
linear or almost flat. As φ increases, gas diffusion becomes more difficult, leading to
sharp concentration gradients and concave distribution. Hence, using second-order
accurate, Oph2q, finite difference formulation, more accurate estimates for small φ
can be obtained with a smaller or moderate number of nodes, while for a large φ, a
large number of grid points should be utilized to accurately represent sharp changes
in the concentration.ODEs: Boundary Value Problems  619
10.8 SHOOTING METHOD
There are many circumstances where a linear or nonlinear two-point BVP is too complex or
impractical to be solved by conventional numerical methods. It can be extremely difficult to
find numerical solutions to such problems with conventional FDM or FVM methods. Also,
BVPs involving a semi-infinite (semi-bounded) interval pose another numerical difficulty
since one boundary is at infinity. In this regard, shooting methods for complex or nonlinear
two-point BVPs offer great convenience and advantages to the numerical analyst.
Consider the following two-point BVPs:
p1q y2 ´ xyy1 “ exy ypaq “ A, ypbq “ B
p2q y2 ´ 4xy “ x2 ` 1 ypaq “ A, yp8q “ B
The first BVP is a nonlinear ODE due to the yy1 and exy terms. The FDM discretization
leads to a set of nonlinear equations, which is not desirable, especially when the number of
unknowns is large. The numerical solution of nonlinear two-point BVPs requires an iterative
algorithm followed by a linearization procedure, which is generally very sensitive to an initial
guess. This algorithm may diverge primarily because the iteration procedure is started with
an unsuitable initial guess.
The second BVP is a linear ODE; however, the solution interval is semi infinite, and the
Dirichlet BC is applied at x “ 8. In order to obtain the numerical solution, it is necessary
to make a numerical prediction for “x “ 8.” In some cases, the value of infinity may be
in the order of x « 1, x « 100s, or larger. Hence, even though a BVP may be linear, it
could be difficult to find a reliable solution or assess the accuracy of the solution because
we cannot truly define 8.
For the reasons mentioned, a BVP of this type is generally treated and solved as an
IVP, which makes nonlinear ODEs easier to handle.
10.8.1 LINEAR ODES
Consider the following two-point BVP, subjected to Dirichlet BCs:
ppxqy2 ` qpxqy1 ` rpxqy “ fpxq, ypaq “ A, ypbq “ B (10.61)
where the coefficients ppxq, qpxq, rpxq, and fpxq are continuous and defined on [a, b] and
rpxq{ppxq ď 0.
Rearranging Eq. (10.61), the BVP may also be cast as
y2 “ Fpx, y, y1
q, ypaq “ A, ypbq “ B (10.62)
Equations (10.61) or (10.62) can be reduced to a set of coupled first-order IVPs. Defining
Y1pxq “ ypxq and Y2pxq “ y1
pxq, Eq. (10.62) leads to the following IVPs:
dY1
dx “ Y2, dY2
dx “ Fpx, Y1, Y2q,
Y1 paq “ A, Y1pbq “ B,
(10.63)
To solve Eq. (10.63) as an IVP, Y1paq and Y2paq (ypaq and y1
paq) need to be known
beforehand. However, the initial value y1
paq in Eqs. (10.61) and (10.62) is undefined. Since
initial conditions are required in a typical IVP problem, a suitable guess for y1
paq (i.e.,620  Numerical Methods for Scientists and Engineers
FIGURE 10.16: Graphical depiction of the shooting method (a) numerical solution with
different B’s, (b) guess versus target values.
Y2paq) should be made to start the forward marching process. A reasonable “smart” guess,
which may be deduced from the physics of the problem or an arbitrary guess, is assigned
to Y2paq to start the process. For example, setting Y2paq “ c1 (a first guess), the IVP Eq.
(10.63) is solved up to, say, x “ b using a numerical method to solve the IVPs. If the initial
guess (y1
paq, which is also the initial slope) is much different than the true value, the target
estimate, ypbq, will deviate from the true value by a large margin. After the first trial, the
target value of ypbq “ Y1pbq “ B1 is recorded. Then the IVP is resolved with a second
(smaller or larger) guess, e.g., Y2paq “ c2. We let the second target value be Y1pbq “ B2,
which is also recorded (see Fig. 10.16a). This technique is called the shooting method because
of its resemblance to trying to hit a distant target. Normally, it is very unlikely that the
target is hit with the first or second trials. Therefore, we continue shooting by adjusting the
guess until the boundary value Y1pbq “ B is satisfied.
In linear ODEs, the target values vary linearly with the guesses (see Fig. 10.16b). Using
the results of the two trials, a straight line that passes through points (c1, B1) and (c2, B2)
can be found to estimate the initial slope, Y2paq. A linear interpolation procedure leads to
c “ c2 ` pc1 ´ c2q
B ´ B2
b1 ´ B2
(10.64)
where c is the true initial slope.
Of course, the true values of B and c depend on the accuracy (truncation and round-off
errors) of the numerical procedure. In order to obtain highly accurate numerical estimates,
the step size must be sufficiently small to ensure negligible truncation and round-off errors.
Once B and c are obtained with sufficient accuracy, the numerical solution of the IVP is
carried out one last time for y1
paq “ c.
Boundary at infinity. A linear or non-linear two-point BVP may be given on a semi￾infinite interval. Consider imposing the following BCs on the BVP given by Eq. (10.62):
ypx “ aq “ A and ypx Ñ 8q “ B, a ă x ă 8 (10.65)
where the boundary at “8” is, numerically speaking, only achievable asymptotically. In
order to tackle it as a two-point BVP, it is essential to predict and assign a suitable value
to “8,” which is not an easy task. But such problems are also more convenient to treat as
IVPs using the shooting method.
The numerical solution of an ODE eventually yields y Ñ B as x Ñ 8. In Fig. 10.17,
the numerical solutions of three BVPs in a semi-infinite interval are shown graphically. It
is observed that the solutions approach their boundary (asymptotic) values at different xODEs: Boundary Value Problems  621
FIGURE 10.17: Evolution of numerical solution in a semi-infinite interval.
values (x1 ă x2 ă x3). When the shooting method is used, an estimate for x8 value may
be obtained based on the order of magnitude of the physical phenomenon under study. If
an estimate is not possible, two trial solutions (with c1 and c2) must be obtained for a
sufficiently large x value, denoted by x8. The solution should become asymptotic within a
preset ε value, which depends on the physical problem and the choice of the analyst. The
initial condition y1
paq “ Y2paq “ c is found using Eq. (10.64) and the most recent two target
values.
Shooting Method
‚ The shooting method can be easily implemented for both linear and
nonlinear two-point BVPs;
‚ Numerical solutions of ODEs with an accuracy higher than Oph2q
can be obtained easily with either RK4, ABF4, or AMF4.
‚ A good estimate is required for the missing initial condition so that
the iterative procedure converges quickly;
‚ High-order numerical schemes are susceptible to round-off errors;
‚ Nonlinear BVPs may lead to stability problems, especially over long
intervals;
‚ In nonlinear two-point BVPs, the c´B relationship is also nonlinear,
and the secant method or similar root-finding methods may be used
to estimate the initial conditions.
EXAMPLE 10.8: Solving BVP in a semi-infinite medium
A viscoelastic fluid is a non-Newtonian fluid that combines a viscous and an elastic
component. The flow of linear viscoelastic fluid about a large horizontal plate leads
to the following boundary value problem [26]:
ν d2Fβ
dη2 ´ p1 ` λβqβFβpηq “ 0, Fβp0q “ U0, Fβp8q “ 0
where Fβ “ u{eβt is the normalized velocity, u is the velocity, ν is the kinematic
viscosity, λ is the relaxation time, and β is a real constant. Fluid attached to the
plate moves quickly, whereas fluid particles sufficiently away from the plate remain
stationary, i.e., u Ñ 0 as η Ñ 8.622  Numerical Methods for Scientists and Engineers
Apply the shooting method to the given two-point linear BVP to obtain a nu￾merical solution and solve the resulting system of IVPs using the RK4 with h “ 0.1.
SOLUTION:
Before attempting to apply the RK4 method, let us introduce the following trans￾formation:
x “ η
ap1 ` λβqβ{ν, ypxq ” ypηq “ Fβpηq{U0
Then, the BVP can be expressed as
y2 ´ y “ 0, yp0q “ 1, yp8q “ 0
which has the true solution ypxq “ e´x.
Next, the problem is converted to a coupled first-order IVP by setting Y1pxq “
ypxq and Y2pxq “ y1
pxq, which results in
Y 1
1 “ Y2, Y1(0) “ 1, Y 1
2 “ Y1, Y2(0) “ c
As the unknown initial slope y1
p0q is unknown, we set Y2p0q “ c. We also assume
a value for x8, which will be x8 “ 8. Using the RK4 method for the step size of
h “ 0.1, the numerical solution is obtained sequentially from the initial value up
to x8 “ 8. The upper bound can be determined by achieving the asymptotic value
within a tolerance. Noting that yp0q “ 1 ą yp8q “ 0, it is clear that the solution
overall is a decreasing function (i.e., y1
pxq ă 0), though it may depict some local
extremums of the solution interval. Hence, the guess for the initial slope is chosen
to satisfy y1
p0q ă 0.
TABLE 10.9
x Y1pxq “ ypxq Y2pxq “ y1
pxq
0 1 ´1
0.1 0.904838 ´0.904838
0.2 0.818731 ´0.818731
0.3 0.740818 ´0.740818
0.4 0.670320 ´0.670320
0.5 0.606531 ´0.606531
1 0.406570 ´0.406570
2 0.367880 ´0.367880
3 0.135336 ´0.135336
4 0.049787 ´0.049787
5 0.018316 ´0.018316
6 0.006738 ´0.006738
7 0.002479 ´0.002479
8 0.000912 ´0.000912ODEs: Boundary Value Problems  623
Numerical solutions with c “ ´0.1 and c “ ´1 yield the following target values:
For c1 “ ´0.1, B1 “ Y1p8q “ 1341.423
For c2 “ ´0.5, B2 “ Y1p8q “ 745.2352
A new estimate for the initial slope is then calculated from Eq. (10.64) as
c“c2`pc1´c2q
B ´ B2
b1 ´ B2
“´0.5`p´0.1`0.5q 0 ´ 745.2352
1341.423´745.2352 “´1
In other words, the true initial slope is y1
p0q“´1. Since this problem is a linear BVP,
it is solved as an IVP one last time for y1
p0q“´1 to obtain its numerical solution,
tabulated in Table 10.9. We observe that both y and y1 decrease as x increases and
asymptotically approach zero as x is further increased.
Discussion: We note that the numerical solutions for y and y1 approach zero within
� ă 10´3 as x Ñ 8, and our initial estimate of x8 “ 8 is sufficiently large enough to
yield fairly good estimates. If it turns out that the predicted x8 value is not large
enough, the final solution is taken as the initial value and the IVP is solved up to a
larger x8 value. Of course, we expect a numerical solution with a smaller h to yield
better estimates. When tackling BVPs in a semi-infinite interval, the analyst should
continue marching until a suitable x8 is found.
10.8.2 NON-LINEAR ODEs
Consider the second-order nonlinear two-point BVP expressed by Eq. (10.62). Nonlinear
BVPs can also be converted into a set of coupled first-order IVPs, just like linear BVPs.
Hence, finding the numerical solution for an initial slope y1
pBiq “ ci is similar, but the c´B
relationship is no longer linear.
We start out with two independent guesses for the initial slope: Y2paq “ c1 and Y2paq “
c2. The IVP is then solved by a suitable numerical method, and the achieved target values
are recorded, e.g., Y1pbq “ B1 and Y1pbq “ B2. Although there is no linear relationship
between the target values (B1, B2, ..., Bn) and the initial slopes of nonlinear two-point
BVPs, the slope estimate can be improved with Eq. (10.64), which will unlikely satisfy
Y1pbq “ B in the first attempt. For this reason, to determine improved estimates for c,
Equation (10.64) is recast as follows:
cp`1 – cp ` pcp´1 ´ cpq B ´ Bp
Bp´1 ´ Bp
, p ě 2 (10.66)
where p denotes the iteration number, cp and Bp are the initial slope and corresponding
target estimate at the pth iteration, respectively.
EXAMPLE 10.9: Solving a BVP as an IVP
The steady-state energy balance for an absorbing planar medium is depicted be￾low (As"L) with no internal heat generation and specified surface temperatures is
reduced to the following second-order nonlinear two-point BVP:
k
d2T
dx2 “ κ
`
4σT4pxq ´ Gpxq
˘
, Tp0q “ T1, TpLq “ T2624  Numerical Methods for Scientists and Engineers
where κ is the absorption coefficient (m´1), k is the thermal conductivity (W/m¨K),
σ is the Stefan-Boltzmann constant (W/m2¨K4), G is the incident energy (W/m2),
L is the plate thickness (m), and T1 and T2 are the specified surface temperatures
(K).
Introducing the following parameters and quantities
Θ “ T{T1, g “ G{4σT4
1 , N “ κk{σT3
1 , τ “ κx, τL “ κL
where Θ and g are dimensionless temperature and incident energy, N is called the
radiation-conduction parameter, and τ and τL are the optical distance and optical
thickness, respectively. Then the dimensionless form of the BVP becomes
N d2Θ
dτ 2 “ Θ4pτ q ´ gpτ q, Θp0q “ 1, ΘpLq “ Θ2 “ T2{T1
For Θ2 “ 0.5, N “ 0.5, and gpτ q“pπ2{18q cospπτ {3q ` cos4pπτ {3q, find the dimen￾sionless temperature distribution in the medium. Apply the shooting method using
the RK4 in increments of Δτ “ 0.1 to obtain a numerical solution. Compare your
estimates with the true solution given with Θpτ q “ cospπτ {3q.
SOLUTION:
Setting Y1pτ q “ Θpτ q and Y2pτ q “ dΘ{dτ , the two-point BVP is reduced to the
following set of coupled first-order IVP:
dY1
dτ “ Y2, dY2
dτ “ 1
N pY 4
1 ´ gpτ qq, Y1p0q “ 1, Y2p1q “ 0.5
Since Θ1
p0q is unknown, we set Y2p0q “ c to find the numerical solution for an
abitrary c. The coupled system of nonlinear IVP is solved using the RK4 method
with Δτ “ 0.1. Numerical solutions for c “ 0.5 and c “ ´0.5 (two independent
initial guesses) yield the following target values:
For c1 “ 0.5, B1 “ Y1p1q “ 2.523334
For c2 “ ´0.5, B2 “ Y1p1q“ ´0.317103
Equation (10.66) is used to obtain a new prediction (cp) and its corresponding
numerical solution (Bp) is found by applying the RK4 method. The subsequent
predictions and the target estimates are obtained iteratively until convergence is
achieved, i.e., |cp ´ cp´1| ă ε. The iteration history of the initial slope and the
target estimates is summarized in Table 10.10. The converged value of Θ1
p0q was
found to be zero after six iterations.
Next, we find the numerical solution for the BVP on [0,1] using the RK4 scheme
with the initial slope Θ1
p0q “ 0. The true solution and the true (absolute) errors
are tabulated in Table 10.11. The numerical estimates Θ and Θ1 are found to be
accurate to at least four decimal places.ODEs: Boundary Value Problems  625
TABLE 10.10
p cp “ Θ1
p0q Bp “ Θp1q
1 0.5 2.523334
2 ´0.5 ´0.317103
3 ´0.212332 0.111556
4 0.048348 0.603751
5 ´0.006601 0.486432
6 ´2.46ˆ10´5 0.499986
TABLE 10.11
True Solution Absolute error
τ Y1pτ q Y2pτ q ˇ
ˇY1 ´ Y˜1
ˇ
ˇ ˇ
ˇY2 ´ Y˜2
ˇ
ˇ
01 0
0.1 0.994522 ´0.109462 1.05E-07 8.95E-06
0.2 0.978148 ´0.217725 1.40E-06 1.76E-05
0.3 0.951057 ´0.323602 2.48E-06 2.58E-05
0.4 0.913545 ´0.425934 5.54E-06 3.36E-05
0.5 0.866025 ´0.523599 8.60E-06 4.18E-05
0.6 0.809017 ´0.615527 1.30E-05 4.93E-05
0.7 0.743145 ´0.700712 1.82E-05 5.59E-05
0.8 0.669131 ´0.778219 2.34E-05 6.24E-05
0.9 0.587785 ´0.847201 2.98E-05 6.76E-05
1 0.5 ´0.906899 3.70E-05 7.17E-05
Discussion: In this example, the numerical solution of the given BVP was obtained
with increments of Δτ “ 0.1, i.e., equivalent to M “ 10 uniform subintervals. While
non-linear FDM results in a quadratic global error, the global error of the solution
with RK4 as IVP is Op0.14q.
10.9 FOURTH-ORDER LINEAR DIFFERENTIAL EQUATIONS
Fourth-order ODEs are generally encountered in the areas of applied mathematics, beam
theory, viscoelastic and inelastic flows, and electric circuits. The common fourth-order linear
differential equation has the following general form:
ppxqypivq ` qpxqy2 ` rpxqypxq “ gpxq, a ď x ď b (10.67)
where p, q, r, and g are continuous functions of x. Four BCs (two for the left and two for
the right boundary) are required to find its unique solution. The boundary conditions may
be in the form of Dirichlet, Neumann, or Robin BC, or may include linear combinations
including second or third derivatives as well.
In this section, we will discuss the numerical solution of such BVPs with the finite
difference method. The solution steps of Eq. (10.67) are similar to the two-point BVPs,
which are detailed below:626  Numerical Methods for Scientists and Engineers
Step 1. Gridding: A one-dimensional grid structure is created by dividing the solution range
into M uniform (or nonuniform) subintervals. The abscissas are found from xi “ a ` ih for
i “ 0, 1, 2,...,M if uniform grids (h “ xi`1 ´xi “ pb´aq{M) are adopted. As usual, nodal
points are placed at both end points of each subinterval and are enumerated from 0 to M.
Step 2. Discretizing: For an arbitrary xi on [a, b], the ODE can be written as
piypivq
i ` qiy2
i ` riyi “ gi, i “ 0, 1,...,M (10.68)
where short-hand notations (y2pxiq “ y2
i , ppxiq “ pi, etc.) have been adopted.
All derivatives in Eq. (10.68) are approximated by CDFs, resulting in
pi
´yi`2 ´ 4yi`1 ` 6yi ´ 4yi´1 ` yi´2
h4
¯
`qi
´yi`1 ´ 2yi ` yi´1
h2
¯
`riyi “gi (10.69)
Rearranging and collecting similar terms of Eq. (10.69) gives
pi
h4 yi´2´
´4pi
h4 ´ qi
h2
¯
yi´1`
´
ri`
6pi
h4 ´ 2qi
h2
¯
yi ´
´4pi
h4 ´ qi
h2
¯
yi`1` pi
h4 yi`2 “gi (10.70)
which, for i“0, 1,...,M, leads to an pM ` 1qˆpM ` 1q system.
Step 3. Implementing BCs: Implementing the Dirichlet BC is simple and straightforward.
A Dirichlet BC (y0 ´ A “ 0 or yM ´ B “ 0) can be most easily implemented by setting
the diagonals to “1,” the off-diagonal coefficients to zero, and the rhs’s to the BC values
(g0 “ A, gM “ B). Using the compact banded matrix notation, the Dirichlet BCs are
incorporated into the pentadiagonal linear system as a03 “ aM3 “ 1, a04 “ a05 “ 0, and
aM1 “ aM2 “ 0. In the case of a Neumann or Robin BC (or other BCs involving either
y2paq or y2pbq, or both), the discretization of both the ODE and the BCs at x0 and xM will
result in fictitious node(s), i.e., y´1 and/or yM`1. If a BC involves y3paq or y3pbq or in a
linear combination with y, y1
, or y2, then two fictitious nodes will appear on the opposite
side (outside of the solution range) of the boundary node. These fictitious nodes are, in the
same manner, eliminated with the aid of the BCs.
Step 4. Constructing a linear system: The difference equations can now be expressed in matrix
form:
Ay “ g (10.71)
where A is a banded matrix (or a pentadiagonal system) with a band size of 5. The general
difference equation is adapted to compact band matrix form as follows:
ai1yi´2 ` ai2yi´1 ` ai3yi ` ai4yi`1 ` ai5yi`2 “ gi, i“0, 1,...,M
where ai1, ai2, ai3, ai4, and ai4 are sub-lower-diagonal, sub-diagonal, diagonal, upper￾diagonal, and super-upper diagonal elements arranged as columns, respectively. Finally,
the system can be expressed in compact banded matrix form as follows:
»
—
—
—
—
—
—
—
—
—
–
0 0 a03 a04 a05
0 a12 a13 a14 a15
a21 a22 a23 a24 a25
.
.
. .
.
. .
.
. .
.
. .
.
.
apM´2q1 apM´2q2 apM´2q3 apM´2q4 apM´2q5
apM´1q1 apM´1q2 apM´1q3 apM´1q4 0
aM1 aM2 aM3 0 0
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
—
–
y0
y1
y2
.
.
.
yM´2
yM´1
yM
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
—
—
–
g1
g2
g3
.
.
.
gM´2
gM´1
gM
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(10.72)ODEs: Boundary Value Problems  627
Step 5. Solving the linear system: The system of banded (specifically penta-diagonal) equa￾tions can be solved with the Gauss elimination method (see Section 2.1.1, Pseudocodes 2.14
and 2.15). In this case, the memory storage requirement for the coefficient matrix in either
banded or pentadiagonal form is the same.
Pseudocode 10.5
Module LBVP4 (M, x, AB, α, β, γ, y)
\ DESCRIPTION: A pseudomodule to solve a 4th-order linear BVP of the form:
\ ppxqypivq`qpxqy2`rpxqypxq“gpxq, a ďxď b, subjected to BCs:
\ ypaq “ A, α1y1
paq ` β1ypaq “ γ1, ypbq “ B, α2y1
pbq ` β2ypbq “ γ2,
\ USES:
\ COEFFS:: A user-defined module supplying ppxq, qpxq, rpxq, gpxq
\ BAND_SOLVE:: A banded linear system solver, Pseudocode 2.14
Declare: xM, ABM,5, yM
h Ð x2 ´ x1; h2 Ð h ˚ h; h4 Ð h2 ˚ h2 \ Set interval size & h2, h4
mL Ð 2; mU Ð 2 \ Set the upper & lower bandwidths
For “
k “ 1, M]
‰ \ Construct banded linear system
COEFFS(xk, px, qx, rx, gx) \ Find coefficients for x “ xk
ABk,1 Ð px; ABk,5 Ð ABk,1 \ Set up matrix elements and rhs
ABk,2 Ð qx ˚ h2 ´ 4 ˚ px; ABk,4 Ð ABk,2
ABk,3 Ð 6 ˚ px ´ 2 ˚ qx ˚ h2 ` rx ˚ h4; yi Ð gx ˚ h4
If “
k “ 1
‰
Then \ Set up Dirichlet BC
ABk,1 Ð 0; ABk,2 Ð 0; \ Modified coefficients of row k “ 1
ABk,4 Ð 0; ABk,5 Ð 0; ABk,3 Ð 1; yk Ð A
Else
If “
k “ 2
‰
Then \ Set up Robin BC
ABk,3 Ð ABk,3 ` ABk,1 \ Modify coefficients of row k “ 2
ABk,2 Ð ABk,2 ` 2h ˚ β1 ˚ ABk,1{α1
yk Ð yk ` 2h ˚ γ1 ˚ ABk,1{α1; ABk,1 Ð 0
Else
If “
k “ M ´ 1
‰
Then \ Set up Robin BC
ABk,3 Ð ABk,3 ` ABk,5 \ Modify coefficients of row k“M ´1
ABk,4 Ð ABk,4 ´ 2h ˚ β2 ˚ ABk,5{α2
yk Ð yk ´ 2h ˚ γ2 ˚ ABk,5{α2; ABk,5 Ð 0
Else
If “
k “ M‰
Then \ Set up Dirichlet BC
ABk,1 Ð 0; ABk,2 Ð 0; \ Modified coefficients of row k “ M
ABk,4 Ð 0; ABk,5 Ð 0; ABk,3 Ð 1; yk Ð B
End If
End If
End If
End If
End For
BAND_SOLVE(M,mL, mU , AB, y) \ Use Pseudocode 2.14
End Module LBVP4
Module COEFFS (x, p, q, r, g)
\ DESCRIPTION: A user-defined module supplying ppxq, qpxq, rpxq, and gpxq.
P Ð ppxq; Q Ð qpxq; ... \ Define coefficients here
End Function Module COEFFS628  Numerical Methods for Scientists and Engineers
A pseudomodule, LBVP4, numerically solving a fourth-order linear BVP defined by Eq.
(10.67) (with Dirichlet or Robin BCs) is presented in Pseudocode 10.5. The module requires
the number of nodes (M), the Dirichlet BCs (A and B), the Robin BCs (αi, βi, γi, i “ 1
and 2), and an array containing the abscissas of the nodes (xi, i “ 1, 2,...,M) as input.
Note that in the Robin BC, αi ‰ 0. On exit, the numerical estimate y is stored on an array
of length M. The module requires two additional modules: (1) COEFFS, a user-defined
module supplying the coefficients of the 4th-order linear ODE for an arbitraty input x, and
(2) BAND_SOLVE, a banded linear system solver, given in Pseudocode 2.14. The structure
of the code is similar to Pseudocode 10.1, i.e., the coefficients of the difference equation and
the rhs, Eq. (10.70) are computed. To complete the construction of the banded matrix, the
first two and last two rows of the matrix and the corresponding rhs entries are modified.
(For Example 10.10, the pertinent difference equations are Eqs. (10.74), (10.75), (10.77),
and (10.78).) Finally, the banded system is solved, and the solution estimate is returned in
the array variable y.
EXAMPLE 10.10: Solution of fourth-order BVP
Consider a beam of length L “ 1 m with the flexural rigidity EI “ 25 kN¨m2, resting
on an elastic foundation with stiffness K “ 150 kN/m2. As shown in the figure, both
free ends are embedded. The beam is subjected to a uniform loading of w0 “ 2500
kN/m.
The deflection (y) of the beam is expressed with the following BVP:
EI d4y
dx4 ` Ky “ w0, yp0q “ y1
p0q “ ypLq “ y1
pLq “ 0
Use the finite difference method to obtain the numerical solution of the BVP with
uniform grid intervals of M “ 5, 10, 20, and 40, and discuss the results.
SOLUTION:
The problem is a fourth-order linear BVP with four BCs: two Dirichlet and two
Neumann BCs. Substituting the numerical values into the differential equation and
simplifying yields
ypivq ` 6y “ 100, yp0q “ y1
p0q “ yp1q “ y1
p1q “ 0
We follow the numerical solution procedure for the fourth-order linear BVPs:
FIGURE 10.18ODEs: Boundary Value Problems  629
Step 1. Gridding: A set of uniformly spaced grid points is established by dividing the
beam into M uniform subintervals, giving h “ 1{M. The abscissas for the resulting
nodal points are xi “ ih for i “ 0, 1, 2,...,M (see Fig. 10.18).
Step 2. Discretization: The fourth-order ODE is satisfied for all xi P p0, 1q, thus we
may write
ypivq
i ` 6yi “ 100
Next, approximating ypivq by the CDF results in
yi`2 ´ 4yi`1 ` 6yi ´ 4yi´1 ` yi´2
h4 ` 6yi “ 100
Rearranging and simplifying gives
yi´2´4yi´1`6p1 ` h4qyi´4yi`1`yi`2 “ 100h4, i“1, 2,...,M ´1 (10.73)
where the difference equations for i “ 0 and i “ M are excluded since the Dirichlet
BCs are imposed on the corresponding boundary nodes.
Step 3. Implementing BCs: Although the boundary nodes are known (y0 “yM “0), the
fictitious nodes (y´1 and yM`1) also appear in the difference equations for i“1 and
i“M´1 due to the Neumann BCs (see Fig. 10.18). The fictitious nodes are eliminated
by applying the same procedure employed as in the cases of two-point BVP. The
left boundary BC is discretized using the CDF as y1
p0q–py1 ´ y´1q{2h “ 0, which
leads to y´1 “y1. Similarly, for the right boundary, we obtain yM`1 “yM´1.
Setting i “ 1 in Eq. (10.73), the difference equation for the left node becomes
y´1 ´ 4y0 ` 6p1 ` h4qy1 ´ 4y2 ` y3 “ 100h4
In light of our findings from the treatment of left BCs (y0 “ 0 and y´1 “ y1 ), the
difference equation takes the form
p7 ` 6h4qy1 ´ 4y2 ` y3 “ 100h4 (10.74)
Now, substituting y0 “0 in the difference equation for node i“2 gives
´4y1 ` 6p1 ` h4qy2 ´ 4y3 ` y4 “ 100h4 (10.75)
Thus, y0 and y´1 are eliminated from the difference equations for i“1 and i“2.
Next, we move on to the right boundary. Setting i“M ´1 in Eq. (10.73) yields
yM´3 ´ 4yM´2 ` 6p1 ` h4qyM´1 ´ 4yM ` yM`1 “ 100h4 (10.76)
Substituting yM “0 and yM`1 “yM´1 into Eq. (10.76) gives
yM´3 ´ 4yM´2 ` p7 ` 6h4qyM´1 “ 100h4 (10.77)
Finally, imposing yM “0 in the difference equation for node i“M ´2 yields
yM´4 ´ 4yM´3 ` 6p1 ` h4qyM´2 ´ 4yM´1 “ 100h4 (10.78)
Step 4. Constructing a linear system: The general difference equation, Eq. (10.73), for
interior nodes (i “ 3, 4, ..., M ´4, and M ´3) is unaffected by the BCs. However,630  Numerical Methods for Scientists and Engineers
the finite difference equations corresponding to the first two and last two nodes
(i “ 1, 2, M ´1, and M ´2) have been modified as a result of implementing the
BCs. Combining all difference equations for all unknown nodes, Eqs. (10.73)-(10.75),
(10.77), and (10.78), the system of linear equations is expressed in matrix form as
»
—
—
—
—
—
—
—
—
—
–
E ` 1 ´4 1
´4 E ´4 1
1 ´4 E ´4 1
... ... ... ... ...
1 ´4 E ´4 1
1 ´4 E ´4
1 ´4 E ` 1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
—
—
–
y1
y2
y2
.
.
.
yM´3
yM´2
yM´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“ 100h4
»
—
—
—
—
—
—
—
—
—
–
1
1
1
.
.
.
1
1
1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
where E “ 6p1 ` h4q.
Step 5. Solving the linear system: This system is a banded matrix (penta-diagonal)
with upper and lower bandwidths of 2, which is solved using the Gauss elimination
method designed specifically for banded systems. The BVP has an analytical so￾lution, but it is not given here explicitly because it contains long and complicated
expressions. Finding the true solution is left to the readers as a practice problem.
TABLE 10.12
True Numerical estimates, M
x Solution 5 10 20 40
00 0 0 0 0
0.1 0.033377 0.04077 0.03523 0.03384
0.2 0.105447 0.15758 0.11852 0.10872 0.10627
0.3 0.181594 0.19870 0.18587 0.18266
0.4 0.237139 0.31486 0.25664 0.24202 0.23836
0.5 0.257296 0.27759 0.26237 0.25857
The solution of the BVP is symmetrical with respect to x “ 0.5; hence, the
numerical solutions for M “ 5, 10, 20, and 40 are comparatively tabulated in Table
10.12 for the first half only. The average error is in the order of 0.065, 0.016, 0.004,
and 0.001 for M “ 5, 10, 20, and 40, respectively.
Discussion: Notice that the numerical error for M “ 5 is quite high, but the esti￾mates depict improvement as M increases. From the numerical results, we observe
that the error decreased by a factor of four as the number of intervals was halved.
10.10 CLOSURE
Many steady-state problems in science and engineering lead to linear or nonlinear two-point
ODEs (two-point BVPs). The most common two-point BVPs consist of a second-order linear
ODE and two BCs, which may be of the Dirichlet, Neumann, or Robin type.
The Finite Difference Method is the easiest and most common technique to find the
approximate solution of a linear BVP. The method is based on converting the ODE￾BVP into a system of algebraic equations by gridding and discretizing procedures. When
the derivatives are approximated with finite difference formulas, the method leads to aODEs: Boundary Value Problems  631
tridiagonal system of linear equations, traditionally solved with the Thomas algorithm. The
global error in this case is Oph2q. However, apart from reducing the interval size, high￾accuracy numerical solutions can be achieved with Richardson’s extrapolation technique,
which allows numerical estimates of the Oph4q or higher order to be obtained. For this, we
need two sets of solutions with grid intervals of h and h{2.
Higher-order two-point ODEs lead to a banded system of linear equations, which may
also be solved using a band matrix solver to obtain an approximate solution. However, the
numerical solution of a higher-order ODE also requires as many BCs as the order of the
ODE, i.e., a fourth-order ODE requires four BCs, and so on.
The Finite Difference Method using uniform grids is the simplest and cheapest, but it
is not satisfactory for problems with boundary layers. If the number of nodal points is not
sufficient to resolve the boundary layer (at least several nodes within it), then the numerical
solution can result in gross errors even for the interior nodes. The use of large enough grid
points to resolve the boundary layer may yield an unacceptably large cpu-time. The problem
can be solved by employing non-uniform grids with smaller spacing near the boundary or
by transforming the ODE with a suitable stretching function.
The Finite Volume Method is another discretization method suitable for those that
especially arise from physical conservation laws. In other words, it is particularly useful for
BVPs that are cast in conservation forms because boundary and interface conditions can
naturally and easily be implemented without requiring fictitious nodes. The two-point BVP
is then converted into a system of algebraic equations by integrating it over a finite control
volume, centering a nodal point. The method is also second-order accurate.
An ODE or a system of coupled ODEs as BVPs can be treated as an IVP, which can
then be solved by employing the shooting method, which uses the methods developed for
IVPs, by guessing the initial condition(s) and iterating until the BCs are satisfied. Solving
a linear or nonlinear two-point BVP with a BC at infinity is very difficult with the finite
difference method since the magnitude of infinity for any particular problem is unknown. In
this regard, the shooting method is suitable for these problems, where the BVP is treated
as an initial value problem by guessing and interpolating the “initial slope.” High-order
accurate numerical solutions (3rd or 4th-order RK methods, etc.) of two-point BVPs can
be achieved with the shooting method, while FDM leads to at most second-order accurate
solutions.
The numerical solution of nonlinear two-point BVPs is often a very difficult task. The
finite difference method is generally employed when the shooting method fails. The methods
presented here (Newton’s and tridiagonal iteration methods) require an iterative procedure
to obtain an approximate solution. Nonetheless, these methods can be very sensitive to
initial guesses and can lead to convergence problems.
10.11 EXERCISES
Section 10.1 Introduction
E10.1 Identify the following ODEs as either linear or nonlinear.
(a) x2 u2 ` x u1 ´ x2u “ ex, (b) y2 ` x y1 ` p1 ´ sin xqy “ 0, (c) z2 “ azez1
,
(d) Z2 “ p1 ´ ex2
qZZ1 ` 1, (e) p1 ` cos yq y2 ´ p2 ´ sin yq y “ 0, (f) u2 ´ p1 ` x2qu “ 0632  Numerical Methods for Scientists and Engineers
Section 10.2 Two-Point Boundary Value Problems
E10.2 Define the following BCs subject to the ODE y2 ` y “ gpxq, as Dirichlet, Neumann, or
Robin.
(a) yp´1q “ 3, yp1q“´2, (b) y1
p0q “ 0, yp5q “ 1,
(c) y1
p´1q ` 2yp´1q “ 0, y1
p1q ´ 2yp0q “ 0, (d) yp0q “ yp1q, y1
p0q “ y1
p1q,
(e) yp´2q “ 1, 2y1
p1q ` 3yp1q “ 4
E10.3 Verify that ypxq “ c1 sin x`c2 cos x satisfies y2 `y “ 0 and determines the solution of the
BVP (i.e., c1 and c2) that satisfy the following BCs:
(a) yp0q “ 0, ypπq “ 0, (b) y1
p0q “ 0, ypπq “ 1,
(c) y1
p0q “ 1, y1
pπ{2q “ 2, (d) 2y1
p0q ` yp0q “ 1, 2y1
pπ{2q ´ ypπ{2q “ 1,
(e) yp´πq “ 1, y1
pπ{4q ` ypπ{4q “ 1
E10.4 Verify that ypxq “ c1
?x`c2
?
x3 satisfies 4x2y2´4xy`3y “ 0 and determines the solution
of the BVP (i.e., c1 and c2) that satisfies the following BCs:
(a) yp1q “ 3, yp4q “ 6, (b) y1
p0q “ 0, yp4q “ 1,
(c) y1
p1q ´ yp1q “ 1, y1
p4q ` yp4q “ 1, (d) y1
p1q “ 1, y1
p4q “ 2
Section 10.3 Finite Difference Solution Linear BVPs
E10.5 Consider the following BVPs with Dirichlet BCs applied to both sides. (i) Divide the
solution interval into M equally spaced subintervals and use the FDM to derive the general
difference equation with a second-order global error; (ii) implement the BCs and obtain a linear
system of equations in matrix form; (iii) solve the resulting linear system for M “ 5.
(a) x2
y2 ` xy1 ´ x2
y “ 0, yp1q “ 1, yp2q “ 0
(b) px2 ` 2qy2 ` p3x ´ 1qy1 ´ 2y “ e
3x, yp´1q “ 0, yp1q “ 2
(c) p1 ` x2
qy2 ` xy1 ` 3y “ pln xq
2
, yp1q “ 2, yp2q“´1
E10.6 Consider the given BVPs with Neumann or Robin BC on the left and Dirichlet BC on the
right boundary. (i) Divide the solution interval into M equally spaced subintervals and use the
FDM to derive the general difference equation with a second-order global error; (ii) implement
the BCs and obtain a linear system of equations in matrix form; (iii) solve the resulting linear
system for M “ 5.
(a) y2 ` p1 ` xqy1 ` 5e
´xy “ e
x, y1
p0q “ 1, yp5{2q “ 0
(b) p1 ` 2xqy2 ` 2y1 ´ 2y “ 2x, y1
p1q “ 4, yp3q “ 2
(c) p1 ` sin xq y2 ` cos x y1 ` y “ x, y1
p0q ´ 2yp0q “ 0, yp1q “ 3
(d) y2 ` p2 ´ x2
qy “ 2x2 ` 1, y1
p0q ´ yp0q “ 2, yp2q “ 5
E10.7 Consider the given BVPs with Dirichlet BC on the left and Neumann or Robin BC on the
right boundary. (i) Divide the solution interval into M equally spaced subintervals and use the
FDM to derive the general difference equation with a second-order global error; (ii) implement
the BCs and obtain a linear system of equations in matrix form; (iii) solve the resulting linear
system for M “ 5.
(a) y2 ´ 2xy1 ` y “ lnp1 ` xq, yp0q “ 1, y1
p1q “ 0
(b) p2x2 ` 5qy2 ` 2xy1 ´ 3y “ 0, yp1q “ 3, y1
p6q “ yp6q
(c) y2 ` 8y1 ` xy “ 14x ` 2, yp2q “ 1, y1
p3q “ 0
(d) ´ px2 ` 4qy2 ` xy1 ` p1 ` 4xqy “ ?x ` 0.8, yp0.2q “ 0, y1
p1.2q ´ yp1.2q “ 0,
(e) e
xy2 ` y1 ´ e
´xy “ sin x, yp´π{4q “ 1{5, 2y1
pπ{4q ` ypπ{4q “ 1ODEs: Boundary Value Problems  633
E10.8 Consider the following BVPs with Robin BCs on both boundaries: (i) Divide the solution
interval into M equally spaced subintervals and use the FDM to derive the general difference
equation with a second-order global error; (ii) implement the BCs and obtain a linear system of
equations in matrix form; (iii) solve the resulting linear system for M “ 5.
(a) p1 ` 2xqy2 ´ 2y1 ` p2e
x ´ xqy “ sec x, y1
p0q ` yp0q “ 0, y1
p1q ´ yp1q “ 0,
(b) px2 ` 1qy2 ` px ´ 2qy1 ` p1 ` sin2
xqy “ 0, y1
p´1q ` 3yp´1q “ 2, y1
p1q ´ 3yp1q“´2,
(c) 3y2 ` p3x ` 2qy1 ` 3y “ e
´x2
, 2y1
p1q ´ yp1q “ 4, y1
p2q ` 3yp2q “ 6
E10.9 Use the FDM to solve the following two-point linear BVPs with M “ 5, 10, 20, and 40
equispaced subintervals. Compare your numerical estimates with the given true solution.
(a) x2
y2 ´ 2y “ 3x2 ´ 1, yp0q “ 1{2, y1
p1q “ 1, ytruepxq “ 1{2 ` x2 ln x
(b) x2
y2 ´ 5xy1 ` 8y “ 0, y1
p1q ` 2yp1q “ 4, y1
p2q ´ 2yp2q “ 2, ytruepxq“x4´x2
{2
(c) x2
y2 ` 4xy1 ` 2y“2 ln x, yp1{22q“´3{2´ln 2, yp1q“12, ytruepxq“4{x´2{x2´3{2 ` ln x
(d) y2 ´ px2 ` 1q y “ 0, y1
p0q “ 0, yp1q “ ?e, ytruepxq “ e
x2{2
E10.10 Use the FDM to solve the following two-point linear BVPs with M “ 5, 10, 20, and 40
equispaced subintervals. Compare your numerical estimates with the given true solution.
(a) p1 ` x2
qy2 ` xy1 ´ y “ 3x2
, yp0q “ 0, yp1q “ 1, ytruepxq“2`2p
?
2 ´ 1qx`x2 ´ 2
a1 ` x2
(b) 2xy2 ´ p4x ` 3qy1 ` p3 ´ 8x3
qy “ 0, yp0q “ 1, yp1q “ 1, ytruepxq“e
xp1´xq
E10.11 A rectangular fin attached to a hot wall is shown in Fig.
E10.11. The temperature distribution as a function of x (distance
from the base) is governed by the ODE given as
d2T
dx2 ´ hP
kA pTpxq ´ T8q “ 0, p0 ď x ď Lq
where h is the convection heat transfer coefficient (W/m2K), k is
the conductivity (W/m¨K), P is the perimeter (m), and A is the
cross-section area of the fin (m2). The base temperature is Tp0q “
Tb “ 420 K, and the fin tip is insulated (i.e., T1
pLq “ 0). (a) Discre- Fig. E10.11
tize the governing ODE with FDM by dividing the fin length into M equal subintervals; (b) for
given hP{kA “ 22 and 70, L “ 0.1 m, and T8 “ 290 K, obtain the numerical solution for M “ 5,
10, and 20 intervals; (c) compare and comment on the convergence of the numerical solutions.
E10.12 When two chemicals in liquid form are mixed (kmix “ 1
W/m¨K) in a cylindrical reactor (L{R " 1), exothermic reactions
take place, resulting in a heat generation of 1500 W/m3 (see Fig.
E10.12). The reactor diameter, ambient temperature, and convec￾tion heat transfer coefficient are given as 1.2 m, T8 “ 10˝ C, and
h “ 20 W/m2K, respectively. Using the governing ODE and BCs
given below, (a) discretize the ODE using the FDM by dividing the
radius into M equal subintervals; (b) implement the BCs and ex￾press the general difference equations in matrix equation form; and
(c) solve the resulting linear system for M “ 5. Fig. E10.12
1
r
d
dr ˆ
r k dT
dr ˙
` q3prq “ 0,
ˆdT
dr ˙
r“0
“ 0,
ˆdT
dr ˙
r“R
“ h
k pT8 ´ TpRqq634  Numerical Methods for Scientists and Engineers
E10.13 Fluid flow in an annular cylindrical duct (R1 ď r ď R2), extending in the z-direction, is
shown in Fig. E10.13. The governing ODE for the axial velocity is given by
1
r
d
dr ˆ
r
du
dr ˙
“ 1
μ
dP
dz “ C
Fig. E10.13
where u is the axial velocity component, r is the radial coordinate, dP{dx is the pressure drop
across the duct, and μ is the kinematic viscosity of the fluid. The cylinder walls are impermeable,
i.e., upR1q “ 0 and upR2q “ 0. To solve the ODE using FDM with C “ ´0.01 and C “ ´10,
(a) obtain the general difference equation by dividing the R1 ď r ď R2 interval into M equal
subintervals; (b) implement the BCs and express the difference equations in matrix form; (c) solve
the linear system of equations for R1 “ 0.1, R2 “ 0.35, and M “ 5; and (d) find the true solution
of the BVP and compare it with your estimates.
E10.14 The governing differential equation of radial stresses of a
thick rotating annular disc made of aluminum (see Fig. E10.14) is
given as
r
d2σ
dr2 ` 3dσ
dr ` p3 ` νq ρω2
r “ 0, R1 ď r ď R2
where r is the radial coordinate, σ is the radial stress, ω is the
angular speed, ρ “ 2700 kg/m3, and ν “ 0.35 are the density and
Poisson ratio of aluminum, respectively. Assuming that there are no
radial stresses at the inner and outer surfaces (i.e., σpR1q “ σpR2q “
0), (a) obtain a general difference equation by dividing R1 ď r ď R2
Fig. E10.14
into M equal subintervals; (b) implement the BCs and express the difference equations in matrix
form; (c) solve the linear system of equations for R1 “ 0.1, R2 “ 0.35, ω “ 2 rad/s, M “ 5 and
10, and (d) compare your estimates with the true solution given as
σprq “ ω2
ρp3 ` νq
pR2
1 ´ r2qpr2 ´ R2
2q
8r2
E10.15 Consider the fluid flow between two large paral￾lel plates, as shown in Fig. E10.15. The gap between the
plates is filled with grease (μ “ 1.8 N¨/m2). The plates
are moving in opposite directions at velocities V1 and V2.
The governing ODE and the BCs are given as
d2u
dy2 “ 1
μ
dp
dx “ C “ constant, p´ b
2 ď y ď
b
2
q
upb{2q“´V1, up´b{2q “ V2 Fig. E10.15
where u is the y-velocity component, b is the gap thickness, dp{dx is the pressure drop, and μ is the
kinematic viscosity of grease. To solve the governing ODE with the FDM, divide ´b{2 ď y ď b{2
into M equispaced subintervals and (a) derive the general difference equation; (b) implement the
BCs and express the difference equations in matrix equation form; (c) solve the linear system of
equations for dp{dx “ ´7.2 Pa/m, b “ 5 mm, V1 “ 0.0075 m/s, V2 “ 0.005 m/s, and M “ 5; (d)
obtain the true solution of the BVP and compare your estimates with that of the true solution.
E10.16 A mathematical model for steady-state axial diffusion in a tubular reactor is described
with the following BVP:
D d2c
dz2 ´ u dc
dz ` r “ 0ODEs: Boundary Value Problems  635
where c is the gas concentration, D is the diffusion coefficient, u is the axial velocity, z is the axial
coordinate, and r is the reaction rate. The reaction rate equation is given with r “ ´kc, where k
is the reaction rate coefficient. In order to obtain a more general solution to such problems, it is
customary to express differential equations in terms of dimensionless quantities. Introducing the
following dimensionless quantities: C “ c{c0, Z “ z{L, Pe“ uL{D, and Da“ kL{u, where Pe and
Da are referred to as Peclet and Damköhler numbers. The BVP can then be expressed with the
dimensionless quantities as
1
Pe
d2C
dZ2 ´ dC
dZ ´ Da C “ 0, C “ CpZq, 0 ď Z ď 1
subjected to dispersion (at the entry) and continuity (at the exit) BCs, which, in dimensionless
forms, are expressed as
ˆ
´ 1
Pe
dC
dZ ` C
˙
Z“0
“ 1 and ˆdC
dZ ˙
Z“1
“ 0
Solve the pertinent BVP using the FDM for Pe=10 and Da=1, dividing the tube length into
M “ 10, 20, and 40 intervals, and compare your estimates with the true solution given by
CpZq “ pb ´ Peqem1Z ` pb ` Peqeb`m2Z
b ´ a ` pa ` bqeb
where a “ Pe ` 2Da, b “ aPe2 ` 4PeDa, m1 “ pPe ` bq{2, and m2 “ pPe ´ bq{2.
E10.17 Consider an infinitely long plane pad, having a slight inclination, and a plate moving
with a velocity V , as depicted in Fig. E10.17.
Fig. E10.17
The thickness of an oil film between the two surfaces varies linearly as hpxq “ h1 ´ ph1 ´ h2qx{L,
where L is the length of the pad in the direction of the plane motion. The pressure in the lubricant,
due to the pad, is governed by the following BVP:
d
dξ ˆ
H3
pξq
dP
dξ ˙
“ 1 ´ m, Pp0q “ Pp1q “ 0
where the following dimensionless quantities have been introduced: ξ “ x{L, P “ ph2
2{6μV L,
m “ h1{h2, Hpξq “ h{h2 “ m ´ pm ´ 1qξ. Use the FDM to solve the given BVP to obtain the
pressure distribution: (a) for m “ 2 by dividing 0 ď ξ ď 1 into M “ 5 equispaced subintervals;
(b) repeat Part (a) for m “ 3; and (c) repeat Parts (a) and (b) with M “ 10. Compare your
numerical estimates with the true solution given by
Ppξq “ pm ´ 1qpξ ´ ξ2q
pm ` 1qpm ` p1 ´ mqξq
2
E10.18 A mathematical model for steady-state radial diffusion or reaction in a spherical catalyst
is described by the following BVP:
D
ˆd2C
dr2 `
2
r
dC
dr ˙
´ kC “ 0
where k is the reaction rate constant, D is the diffusion coefficient, C is the concentration, and r is636  Numerical Methods for Scientists and Engineers
the radial coordinate. Introducing the dimensionless quantities C “ c{c0, η “ r{R, φ2 “ R2k{D,
where c0 is the maximum concentration, R is the radius of the catalyst, and φ is so-called the
Thiele modulus. The BVP and the BCs in dimensionless quantities are expressed as
d2C
dη2 `
2
η
dC
dη ´ φ2
C “ 0,
ˆdC
dη ˙
η“0
“ 0, Cp1q “ 1
Use the FDM to solve the BVP for φ “ 1 and 3 by dividing 0 ď η ď 1 into M “ 5 and 10 intervals,
and compare your numerical solutions with the true solution given by Cpηq “ sinh φη{pη sinh φq.
Section 10.4 Numerical Solutions of High-Order Accuracy
E10.19 Repeat E10.9 to find numerical estimates of order Oph6q. Use FDM estimates for M “ 10,
20, and 40 subintervals and compare your estimates with those of the true solution.
E10.20 Repeat E10.10 to find numerical estimates of order Oph6q. Use FDM estimates for M “ 10,
20, and 40 subintervals, and compare your estimates to the true solution.
Section 10.5 Non-Uniform Grids
E10.21 Consider the following coordinate transformation:
x “ α ` p1 ´ αq ln ˆpβ ´ 1qh ` 2ξ
pβ ` 1qh ´ 2ξ
˙
{ ln ˆβ ` 1
β ´ 1
˙
which maps 0 ď ξ ď h onto 0 ď x ď 1. (a) Plot the node distribution for β “ 1, α “ 0.1, and 0.5
using 20 equispaced subintervals in the ξ-domain; (b) obtain expressions for y1 and y2 derivatives
as a function of ξ.
E10.22 Consider the following BVP with Dirichlet BCs on both sides. (a) Convert the problem to
the ξ-domain by clustering the grid points near x “ 0 with the x “ peaξ´1q{pea´1q transformation;
(b) Plot the distribution of nodal points for a “ 0.01, 0.5, 1, and 2 by using 20 equispaced
subintervals in the ξ-domain.
y2 ` y1 ` y “ ´x2
, yp0q “ 1, yp1q “ 0
E10.23 Consider the following BVP with Dirichlet BCs on both sides. Using M “ 4 and h1 “
8Δx, h2 “ 4Δx, h3 “ 2Δx, and h4 “ Δx (nodes clustered by x “ 1), (a) discretize the BVP
with the FDM to obtain the difference equations; (b) express the resulting difference equations in
matrix form; (c) solve and compare your estimates to the true solution given as ypxq “ 2x4.
xy2 ` px ´ 3q y1 ´ 4y “ 0, yp0q “ 0, yp1q “ 2
E10.24 Consider the following BVP with Dirichlet BCs on both sides. Using M “ 4 and h1 “ Δx,
h2 “ 2Δx, h3 “ 4Δx, and h4 “ 8Δx (nodes clustered by x “ 0), (a) discretize the BVP with the
FDM to obtain the difference equations; (b) express the resulting difference equations in matrix
form; (c) solve and compare your estimates to the true solution given as ypxq “ 2x{px ` 1q.
2px ` 1q
2
y2 ` 5px ` 1qy1 ` y “ 2, yp0q “ 0, yp9q “ 9{5
E10.25 Consider the following BVP: Using M “8 and h1 “Δx, h2 “rΔx, h3 “r2Δx, h4 “r3Δx,
h5 “r3Δx, h6 “r2Δx, h7 “rΔx, and h8 “ Δx (nodes clustered on both sides), (a) discretize the
BVP with the FDM and implement the BCs; (b) write out and solve the resulting finite-difference
equations; (c) compare the estimates with the true solution given as ypxq “ 4xp1 ´ xq.
y2 ` 2y1 ` 4y “ ´16x2
, yp0q “ 0, yp1q “ 0
E10.26 Consider E10.23. (a) Transform the BVP to the ξ-computational domain using the trans￾formation equation x “ ?ξ; (b) solve the transformed BVP using the FDM with M “ 4 equispaced
subintervals; (c) comment on the accuracy of your numerical estimates.ODEs: Boundary Value Problems  637
E10.27 Consider E10.24 (a) Transform the BVP to the ξ-computational domain using the trans￾formation equation x “ ξ2; (b) solve the transformed BVP using the FDM with M “ 4 equispaced
subintervals; (c) comment on the accuracy of your numerical estimates.
E10.28 Consider E10.25 (a) Transform the BVP to the ξ-computational domain using the trans￾formation equation given below; (b) solve the transformed BVP using the FDM with M “ 8
equispaced subintervals; (c) comment on the accuracy of your numerical estimates.
x “ 1
2
`
1 ` β tanh “
pξ ´ 1
2 q lnrpβ ` 1q{pβ ´ 1qs‰˘
Section 10.6 Finite Volume Method
E10.29 Solve the following BVPs using FVM with M “ 10 uniformly sized cells and compare
your estimates with the true solution.
(a) ´ `
px2 ` 1qy1
˘1
` 12px ` 1qy “ 24x4 ` 12, y1
p0q “ 0, yp1q “ 3, ytruepxq “ 2x3 ` 1
(b) ´ `
4px ` 1qy1
˘1
` px ´ 1qy “ 0, yp0q “ 1, 2y1
p2q ` yp2q “ 0, ytruepxq “ e
´x{2
(c) `
xy1
˘1
` xy “ 0, y1
p0q “ 0, yp4q “ 2, ytruepxq “ 2J0pxq{J0p4q
(d) e
x `
e
x y1
˘1
` y1 ` 2y “ 6, yp0q “ 3, y1
p1q ` 2yp1q “ 0, ytruepxq “ 3e
´2x
(e) px2
y1
q
1 ` px2 ´ xqy1 ´ y “ 0, yp0q “ 0, y1
p1q ` 2yp1q “ 1, ytruepxq“px ´ 1 ` e
´xq{x
where J0pxq is the Bessel function.
E10.30 Repeat E10.17 using the FVM with uniformly sized cells.
E10.31 Repeat E10.18 using the FVM with uniformly sized cells.
E10.32 A research nuclear reactor operates with plate-type
UO2 fuel elements are enveloped by a steel cladding. Making
use of the physical symmetry, the half-fuel and clad thicknesses
are given as a “ 0.6 cm and b “ 0.3 cm, respectively (see
Fig. E10.32). Heat is generated in the fuel region (q3
0 “ 75
W/cm3) as a result of nuclear reactions. The conductivities
of fuel and cladding are kf “ 0.6 W/cm¨K and kc “ 0.15
W/cm¨K, respectively. At steady-state operating conditions,
the temperature distribution in the fuel and cladding can be
modeled with the following set of ODEs:
Fig. E10.32
kf
d2Tf
dx2 ` q3
0 “ 0 p0 ď x ď aq, kc
d2Tc
dx2 “ 0, pa ď x ď a ` bq
subjected to the following boundary and interface conditions:
Symmetry at the center pdTf {dxqx“0 “ 0
Convection from cladding kcpdTc{dxqx“a`b “ h pT8 ´ Tcpa ` bqq
Continuity of temperature at interface Tf paq “ Tcpaq
Continuity of heat flux at interface kf pdTf {dxqx“a “ kcpdTc{dxqx“a
where h “ 1 W/cm2¨K is the convection transfer coefficient, and T8 is the fluid bulk temperature.
Use the FVM to solve the given problem using M “ 9 uniformly sized cells and compare your
estimates with those of the true solution, which is given as
Tpxq “ "
277.5 ´ 62.5x2, 0 ď x ď 0.6 cm
435 ´ 300x, 0.6 ď x ď 0.9 cm638  Numerical Methods for Scientists and Engineers
E10.33 Consider a thick-walled spherical pressure vessel,
shown in Fig. E10.33. A strain gauge bonded tangentially
at the inner wall to measure the normal tangential strain
gives a reading of εθpRiq “ 0.00065 at maximum pressure.
The strain at the outer wall is zero, i.e., εθpRoq “ 0. Since
the radial displacement and tangential strain are related
with εθ “ u{r, these conditions lead to the BCs in terms
of the displacement. The governing ODE for the radial
displacement is given by Fig. E10.33
d2u
dr2 `
2
r
du
dr ´ 2 u
r2 “ 0, Ri ď r ď Ro upRiq “ RiεθpRiq and upRoq “ 0
whereRi “ 50 cm and Ro “ 66 cm are the inner and outer radii of the vessel under internal
pressure. To solve the BVP for radial displacement using the FVM, (a) divide the wall thickness
into 8 cells and obtain the displacement at the surfaces of each cell; (c) calculate the maximum
normal stress at the inner pressure vessel wall, which is given by
σmax “ E
p1 ` νqp1 ´ 2νq
„
2ν u
r ` p1 ´ νq
du
dr j
r“R0
where E “ 200 GPa is the Young’s modulus of steel and ν “ 0.3 is the Poisson’s ratio.
E10.34 Consider the two-layer composite spherical pressure
vessel (Ri “ 0.4m, RI “ 0.6, Ro “ 0.75) shown in Fig. E10.34.
The differential equation for the radial displacement in the ves￾sel subjected only to internal pressure is governed by the fol￾lowing ODE:
d2u
dr2 `
2
r
du
dr ´ 2 u
r2 “ 0, Ri ď r ď Ro Fig. E10.34
where r is the radial coordinate and u is the radial displacement. The radial stress-displacement
relationship is given by
σr “ E
p1 ` νqp1 ´ 2νq
ˆ
2ν u
r ` p1 ´ νq
du
dr ˙
where σr is the radial stress and E is the modulus of elasticity. The material properties of the
inner and outer shells are E1 “ 200 GPa, ν1 “ 0.3, E2 “ 80 GPa, and ν2 “ 0.36, respectively. The
corresponding boundary conditions are given as
At r “ Ri, σrpRiq“´P (internal wall pressure)
At r “ RI , upR´
I q “ upR`
I q and
σrpR´
I q “ σrpR`
I q (interface continuity)
At r “ Ro, σrpRiq “ 0 (external wall pressure)
where P “ 10 MPa is the applied internal pressure. Apply the FVM to solve the BVP for the
radial displacement subjected to the prescribed BCs. Obtain numerical estimates for Δr “ 0.05,
0.025, 0.0125, and 0.00625 m. (b) Solve the ODE using second-order difference formulas. The
difference equation for the continuity of radial stress will contain five nodal points; thus, the
resulting system of equations will no longer be a tridiagonal system. Use the Gauss-Seidel method
to solve the system of equations.ODEs: Boundary Value Problems  639
E10.35 A steel pipe shown in Fig. E10.35, carrying steam at T8,1 “ 260˝C, has an internal
diameter of Di “ 100 mm and a thickness of 5 mm. The conductivity of the pipe is kpipe “ 15
W/m˝C, and the convection heat transfer coefficient for the internal flow is hin “ 32 W/m2 ˝C.
The pipe is insulated with a 30-mm-thick insulation material having a conductivity of kins “0.5
W/m˝C. The pipe is exposed to air at T8,2 “20˝C with hout “80 W/m2˝C.
The governing ODE for this system is given as
1
r
d
dr ˆ
kprq r
dT
dr ˙
“ 0, Ri ď r ď Ro
where r is the radial coordinate (m), k is the thermal con￾ductivity (W/m˝C), and T is the temperature (˝C). Fig. E10.35
Note that the conductivity has a jump discontinuity at the pipe-insulation interface, which is
expressed as kprq “ kpipe for r ă RI and kprq “ kins for r ą RI . The corresponding boundary
and interface conditions are given as follows:
At r “ Ri, ksteel
dT
dr pRiq “ hin pTpRiq ´ T81q
At r “ RI , TpR´
I q “ TpR`
I q and ksteel
dT
dr pR´
I q“´kins
dT
dr pR`
I q
At r “ Ro, ´kins
dT
dr pR0q “ hout pTpRoq ´ T82q
Apply the FVM to numerically estimate the temperature under the prescribed boundary condi￾tions using uniform M “ 7, 14, and 28 cells, and discuss the accuracy of your numerical estimates.
Section 10.7 Finite Difference Solution of Nonlinear BVPs
E10.36 Use the FDM to solve the following nonlinear two-point BVPs by employing tridiagonal
iteration with M “ 10, 20, 40, and 80 equispaced subintervals. Compare your numerical estimates
with the true solution. Use ε “ 10´6 for tolerance for convergence.
(a) y2 “ e
y, yp0q “ ln 2, ypπ{4q “ ln 4, ytruepxq “ lnp2sec2
xq
(b) yy2 ` 3py1
q
2 “ 0, yp0q “ 0, yp4q “ 1, ytruepxq“px{4q
1{4
(c) y2 “ py1 ` ypy1
q
2
q{p1 ` y2
q, yp0q “ 1, yp2q “ e
2
, ytruepxq “ e
x
(d) y2 “ 2ypy1
q
2
{p1 ` y2
q, yp0q “ 0, yp2q “ 1, ytruepxq “ tanpπx{8q
E10.37 Use the FDM to solve the following nonlinear two-point BVPs by employing Newton’s
method with M “ 10, 20, 40, and 80 equispaced subintervals. Compare your numerical estimates
with the given true solution. Use ε “ 10´6 for tolerance for convergence.
(a) y2 “ 2e
2y ´ py1
q
2
, yp0q “ 0, yp1q“´ ln 2, ytruepxq“´ lnp1 ` xq
(b) y2 “ 12y3 ` 2yy1
, yp0q “ 1, yp1{2q “ 1{2, ytruepxq “ 1{p2x ` 1q
(c) y2 “ 2y5 ` py1
q
2
{y, yp0q “ 1, yp4q “ 1{3, ytruepxq “ 1{
?2x ` 1
(d) y2 “ 6e
´x ´ yexpy1 ` yq{3, yp0q “ 1, yp1{3q “ 0, ytruepxq“p1 ´ 3xqe
´x
E10.38 Use the FDM to solve the nonlinear two-point BVPs given below using Newton’s method
with M “ 10, 20, 40, and 80 equispaced subintervals. Compare your numerical estimates with the
given true solution. Take yp0q
i “ 1 for the initial guess and ε “ 10´6 for convergence tolerance.
(a) 2xy2 ` 2y1 ` 3xey “ 0, y1
p0q “ 0, yp1q “ 0, ytruepxq “ 2 lnp4{p3 ` x2
qq
(b) 4y2 ` py1
q
2 ` 32y “ 16px6 ` 1q, y1
p0q “ 0, yp1q “ 0, ytruepxq“px2 ´ 1q
2
(c) y2 ` py1 ` yq
2 ´ y “ 6, y1
p0q “ 2, yp1q “ 3 ´ 2{e, ytruepxq “ 3 ´ 2e
´x640  Numerical Methods for Scientists and Engineers
(d) y2 “ 2y3 ´ 3yy1
, y1
p0q“´2, yp2q “ 1{5, ytruepxq “ 1{p2x ` 1q
E10.39 An exothermic reaction takes place in a non-isothermal batch reactor. The conservation
of material and energy yields the following second-order nonlinear two-point BVP:
y2 `
2y1
x ` 1 ´
´
1 ´ y
2
¯
exp ˆ y
y ´ 2
˙
“ ´ e´x
x ` 1
, yp0q “ 0, yp1q “ 1
(a) Use the TIM with M “ 5 and 10 equispaced subintervals to solve the BVP; (b) compare your
numerical estimates with the true solution given with ypxq “ 2x{px`1q. Clue: Obtain initial guess
values by linear interpolation using the two BC values and use ε “ 10´6 for convergence.
E10.40 Consider the thin-long cantilever beam shown in
Fig. E10.40. The curvature of the beam is dictated by the
following BVP for the deflection angle φ:
d2φ
ds2 “ ´P
B cos φ, φp0q “ 0, dφ
ds p1q “ 0
where s is the arc length measured from the tip, P is the
concentrated vertical load at the free end, B is the flexural
rigidity,L is the beam length, Δ is the horizontal compo- Fig. E10.40
nent of the displacement at the loaded end, and δ is the corresponding vertical displacement. Use
Newton’s method with M “ 5 and 10 subintervals to estimate the deflection angle throughout the
beam for P{B “ 0.025 and 0.25.
E10.41 A reaction-diffusion process leads to the following nonlinear two-point BVP:
d2C
dx2 “ 5Cpxq
10 ` 2Cpxq
, Cp0q “ 1, dC
dx p1q ´ 1
2
Cp1q “ 0
Use the tridiagonal iteration method with M “ 5 and 10 uniform intervals to estimate the solution
of the BVP. Note: For the initial guess and convergence tolerance, use Cp0q
i “ 1 and ε “ 10´6,
respectively.
E10.42 Consider a uniform cable hanging between two un￾equal poles, as shown in Fig. E10.42. As a function of the hor￾izontal tension (T) in the cable and its weight (W) per unit
length, the differential equation describing the shape of a hang￾ing cable is given by
y2 “ W
T
b
1 ` py1
q
2
, yp´5q “ h1, yp4q “ h2 Fig. E10.42
Use the tridiagonal iteration method with M “ 5 and 10 subintervals, estimate the shape of the
cable for a case with W{T “ 0.3, h1 “ 15 m and h2 “ 9 m, and compare the estimates with the
true shape is given by ypxq “ 4.3282723´1.12513 sinh 0.3x`3.51810 cosh 0.3x For an initial guess
and convergence tolerance, use yp0q
i “ 1 and ε “ 10´6, respectively.
E10.43 Consider heat conduction in a cylindrical nuclear fuel rod, with heat generation pro￾portional to an exponential function of temperature. The dimensionless conduction equation is
expressed as
d2Θ
dη2 `
1
η
dΘ
dη ` aebΘ “ 0, dΘ
dη p0q “ 0, Θp1q “ 1
where η “ r{R, Θ “ T{Tr, R is the rod radius, and Tr is a reference temperature. Use the finite
differences with the TIM method and M “5 and M “10 subintervals to estimate the dimensionless
temperature of the rod. Given: Use a“1.30, b“0.72, Θp0q “1, and ε“10´6.ODEs: Boundary Value Problems  641
Section 10.8 Shooting Method
E10.44 Repeat E10.36, using the shooting method along with the RK4 and a step size of h “ 0.1.
E10.45 Repeat E10.37, using the shooting method along with the RK4 and a step size of h “ 0.1.
E10.46 Repeat E10.38(a)-(c), using the shooting method along with the RK4 and a step size of
h “ 0.1.
E10.47 Solve each of the following two-point linear BVPs using the shooting method. Having
found the equivalent IVPs, apply the RK4 scheme with step size h “ 0.1 to obtain the numerical
solutions.
(a) y2 ` 2y1 ` y “ 2e
´x, yp0q “ 2, yp1q “ 0 puse c1 “ ´1, c2 “ 2q
(b) y2 ` p3 ´ xqy1 ` xy “ 0, yp1q “ 3, yp4q “ 2, puse c1 “ ´2, c2 “ 1q
(c) x4
y2 ` x3
y1 ´ 3xy “ ´9, yp1q “ 4, yp3q “ 2, puse c1 “ ´2, c2 “ 1q
E10.48 Solve each of the following nonlinear two-point BVPs using the shooting method. Having
found the equivalent IVPs, apply the RK4 scheme with step size h “ 0.1 to obtain the numerical
solutions.
(a) y2 “ 2y3 ´ 3yy1
, yp0q “ 1, yp2q “ 1{5, ytruepxq “ 1{p2x ` 1q, pc0 “ ´2.5, c1 “ ´1q
(b) y2 “ 4 ´ 2x3 ` yy1
, yp1q“´1, yp2q “ 3, ytruepxq “ x2 ´ 2{x, pc0 “ 1, c1 “ 3q
(c) y2 ´ 2y1
{x ` e
y{py´x4q “ e
1´x ` 4x2
, yp1q “ 0, yp2q “ 8,
ytruepxq “ x4 ´ x3 pc0 “ ´1, c1 “ 2q
(d) y2
y2 ` xy1 ´ y “ 0, yp0q “ 1, yp2q “ ?
5, ytruepxq “ a1 ` x2 pc0 “ ´1, c1 “ 1q
(e) y2 ` e
´x2
y1 ´ 2xe´y “ 2, yp0q “ 0, yp1q “ 1, ytruepxq “ x2 pc0 “ ´1, c1 “ 1q
(f) py ´ 1q
2
y2 ` px ` 1{22qy1 ´ y ` 1 “ 0, yp0q “ 2, yp2q “ 1 ` ?
7,
ytruepxq “ 1 ` a1 ` x ` x2 pc0 “ 0, c1 “ 3q
E10.49 Solve each of the following two-point linear BVPs defined in a semi-infinite domain using
the shooting method. Apply the RK4 scheme with step size h “ 0.1 to obtain the numerical
solutions.
(a) p1 ` xqy2 ` xy1 ` y “ 0, yp1q “ 4, yp8q “ 0
(b) p2x3 ` x2 ` 1qy2 ` 4xpx2 ´ 1qy1 ´ 8xy “ 0, y1
p0q“´2, yp8q “ 0
E10.50 Solve each of the following nonlinear two-point BVPs defined in a semi-infinite domain
using the shooting method. Apply the RK4 scheme with step size h “ 0.1 to obtain the numerical
solutions.
(a) y2 ` yy1
{2 “ 0, yp0q “ 0, yp8q “ 2, puse c1 “ 0.1, c2 “ 1.5q
(b) y2 ` 5xy1 ´ e
´xy “ 0, yp0q “ 1, yp8q “ 3{2, puse c1 “ 0.1, c2 “ 0.25q
(c) y2 ` 2xy1
{
a1 ´ y{4 “ 0, yp0q “ 1{2, yp8q “ ´1{2, puse c1 “ ´0.1, c2 “ ´0.5q
(d) xp1 ` x2
qy2 ` p1 ` x2
qyy1 ` 2y1 “ 0, yp1q “ 0, yp8q “ 3, puse c1 “ 1, c2 “ 2q
E10.51 The concentration of species due to dif￾fusion in a planar material, shown in Fig. E10.51,
is described by the so-called diffusion equation. In
one-dimensional space, it is expressed as
Bc
Bt “ B
Bx
ˆ
Dpcq
Bc
Bx
˙
, cp0, tq “ 1,
cpx, 0q “ 0, cp8, tq “ 0
Fig. E10.51642  Numerical Methods for Scientists and Engineers
A method called similarity solution is used to reduce a partial differential equation into an ODE.
Introducing the η “ x{
?t variable, concentration can be expressed as a function of η, i.e., cpx, tq “
fpηq, where fpηq is unknown. Upon substituting the similarity variable, the diffusion equation and
its BCs result in the following BVP:
D d2f
dη2 ` η
2
df
dη “ 0, fp0q “ 1, fp8q “ 0
Apply the shooting method along with the RK4 scheme to estimate the numerical solution to this
problem. Assume a constant diffusion coefficient (D “ 1) and a step size of Δη “ 0.025.
Section 10.9 Fourth-order Linear Differential Equations
E10.52 Consider the following fourth-order linear ODEs with constant coefficients:
(a) u(iv) ´ u “ ´ cos x, up0q “ upπq “ 0, u1
p0q “ 1{4, u1
pπq “ ´p1 ` πq{4,
utruepxq“p1 ` xq sinpxq{4
(b) u(iv) ` 4u “ 0, up0q “ upπq “ 0, u1
p0q “ 1, u1
pπq“´e
π, utruepxq “ e
x sin x
(c) u(iv) ` u2 ` 4u “ 2e
x cos x, up0q “ upπq “ 0, u1
p0q “ 1, u1
pπq“´e
π,
utruepxq “ e
x sin x
(d) u(iv) ` 3u2 ` 9u “ ´9x4
, up0q “ up2q “ 0, u1
p0q “ 0, u1
p2q“´16,
utruepxq “ 4x2 ´ x4
(e) u(iv) ´ 4u2 ´ 45u “ ´48 cosh x, up0q “ 3, upln 2q “ 3{2, u1
p0q“´6, u1
pln 2q “ 0,
utruepxq “ cosh x ` 2e
´3x
To apply the FDM method, (a) divide the solution interval into uniform M subintervals to obtain
the general difference equation; (b) implement the BCs and express the difference equations in
matrix form; and (c) find the numerical solutions for M “ 8, 16, and 32 and compare your solutions
with the true solution.
E10.53 Consider the following fourth-order linear ODEs with variable coefficients:
(a) u(iv) ´ xu2 ` x upxq “ 3x ´ 4, up0q “ 0, up1q “ e
´1
, u1
p0q “ 1, u1
p1q “ 0,
utruepxq “ xe´x
(b) u(iv) ` px ´ 1qupxq “ x5 ` 23, up0q “ u1
p0q “ 1, up1q “ 5, u1
p1q “ 10,
utruepxq “ 1 ` x ` x2 ` x3 ` x4
(c) 8px ´ 2qu(iv) ` 2p4 ´ xqu2 ` upxq “ 0, up0q “ 0, u1
p0q “ 2, up1q “ 2{
?e,
u1
p1q “ 1{
?e, utruepxq “ 2xe´x{2
(d) px ´ 3qu(iv) ` p6 ´ xqu2 ´ 5u “ 0, up0q “ 0, up2q “ 2, u1
p0q “ 1{e
2
, u1
p2q “ 3,
utruepxq “ xex´2
(e) u(iv) ` px ` π2
qu2 ` xπ2
u “ 4π2
x2
, up0q “ 0, up1q “ 4, u1
p0q “ 4 ´ π,
u1
p1q “ 4 ` π, utruepxq “ 4x ´ sin πx
To employ the FDM method, (a) divide the solution interval into uniform M subintervals to obtain
the general difference equation; (b) implement the BCs and express the difference equations in
matrix form; and (c) find the numerical solutions for M “ 8, 16, and 32 and compare your solutions
with the true solution.ODEs: Boundary Value Problems  643
E10.54 A beam undergoes deflection when it is subjected to an applied load. The deflection for
a beam of length L embedded at both ends with a variable load satisfies the following BVP:
d4y
dx4 “ ´ W
EI sin ´πx
L
¯
, yp0q “ y1
p0q “ ypLq “ y1
pLq “ 0
where W{EI “ π4 and L “ 1 m. To find a numerical solution to this problem, (a) divide
0 ď x ď L into uniform M subintervals and derive the general difference equation; (b) implement
the BCs and express the difference equations in matrix form; and (c) solve the resulting linear
system for M “ 10, 20, and 40 and compare your estimates with the true solution given by
ytruepxq “ πx ´ πx2 ´ sin πx
10.12 COMPUTER ASSIGNMENTS
CA10.1 In an isothermal packed-bed reactor, chemical reactions of the form A ` A Ñ B take
place. The governing ODE for the fraction of A remaining, y, can be expressed in dimensionless
form as
1
Pe
d2y
dx2 ` dy
dx ´ α y “ 0
where x is the axial coordinate, α is a constant, and Pe is a dimensionless number referred to as
the Peclet number. The differential equation is subjected to the following BCs:
At x “ 0 y1
p0q “ 0, At x “ 1 1
Pe y1
p1q ` yp1q “ 1,
Find the numerical solution for α “ 5 and Pe=0.1, 1, and 10 using the FDM by dividing the
solution interval into uniform M “ 10 intervals.
CA10.2 As shown in Fig. CA10.2, a conical-frustum fin is used to dissipate the heat conducted
from a base surface through convection across all its surfaces. The dimensionless form of the
governing energy equation is given as
p1`pη ´1qξq d2Θ
dξ2 ` 2pη ´ 1q
dΘ
dξ ´ 2Bib
a2`pη´1q
2
Θpξq“0
At ξ “ 0, Θp1q “ 1 pKnown base temperatureq,
At ξ “ 1, Θ1
p0q ´ Bi Θp0q “ 0, pConvection at the tipq
Fig. CA10.2
where ξ “ x{L (0 ď ξ ď 1) is the dimensionless axial coordinate, L is the fin length, and Θpξq “
pTpxq ´ T8q{pTb ´ T8q is the dimensionless temperature, Tb and T8 are respectively the base and
surrounding temperatures, k is the conductivity, h is the convection heat transfer coefficient, η is
the ratio of the base (R) to tip (r) radius (“ R{r), a is defined as (“ r{L), and Bi is called the
Biot number, which is a dimensionless parameter defined as Bi“ hL{k. Write a computer program
to solve the given BVP and (a) solve this BVP for Bi=0.5, η “ 2, and a “ 5 using uniform M “ 5,
10, 20, and 40 intervals, and comment on the accuracy of your estimates. (b) Investigate the effect
of the Biot number (Bi=0.02, 0.2, and 2) on the dimensionless temperature distribution for η “ 1.5644  Numerical Methods for Scientists and Engineers
and a “ 10. (c) Investigate the effect of η (η “ 1, 1.25, 1.5, 1.75, and 2) on the dimensionless
temperature distribution for Bi=0.5 and a “ 5. (d) Investigate the effect of the length-to-tip radius
ratio (a “ 2, 5, and 8) on the dimensionless temperature distribution for Bi=0.2 and η “ 1.5. For
(b)-(d), generate comparative plots of Θ.
CA10.3 The steady-state conservation equation for a chemical species subject to reaction and
diffusion within a radially symmetric sphere is
d2C
dr2 `
2
r
dC
dr ` β Cprq “ 0, 0 ď r ď 1
where c is the concentration, r is the normalized radial coordinate, β “ kR{D, D is the diffusion
coefficient, R is the radius of the sphere, and k is the chemical reaction rate constant. The boundary
conditions are given as
ˆdC
dr ˙
r“0
“ 0 and Cp1q “ 1
Use the FDM to find the numerical solution by first obtaining the difference equations in matrix
form. For β “ 0.09, 1, and 4, obtain the numerical estimates for uniform M “ 5 subintervals, and
compare your estimates with those of the true solution given with Cprq “ sinpβrq{pr sin βq.
CA10.4 The steady-state temperature distribution between two annular cylinders filled with in￾compressible Newtonian fluid can be expressed as follows:
1
r
d
dr
´
r
dθ
dr
¯
` 4 a
r4 “ 0, κ ď r ď 1
where κ “ Ro{Ri, r is the dimensionless radial coordinate, θ is the dimensionless temperature, a is
a dimensionless constant, and Ri and Ro are the radii of the inner and outer cylinders, respectively.
The boundary conditions are given as
θpκq “ 0 and θp1q “ 1
We seek the numerical solution for a “ 0.5 and κ “ 0.25, 0.5, and 0.75. Obtain solutions with
FDM and FVM using M “ 5, 10, 20, and 40 cells or uniform subintervals. Compare your estimates
with those of the true solution given as
θprq “ ´
a ` 1 ´ a
r2
¯
´
´
a ` 1 ´ a
κ2
¯ ln r
ln κCHAPTER 11
Eigenvalues and Eigenvalue
Problems
LEARNING OBJECTIVES
After completing this chapter, you should be able to
‚ explain the significance of an eigenvalue problem;
‚ describe the mathematical properties of eigenvalue problems;
‚ explain the basis of the Power method and implement it to find the largest
eigenvalue;
‚ understand and implement the inverse Power method to find the smallest (ab￾solute) eigenvalue;
‚ apply the inverse Power method to find the smallest or an intermediate eigen￾value;
‚ apply similarity or orthogonal transformations;
‚ find the eigenvalues and the eigenvectors of symmetric matrices using the Ja￾cobi method;
‚ diagonalize square matrices;
‚ understand and implement Householder transformation;
‚ find the characteristic polynomial of a matrix;
‚ compute the eigenvalues and eigenvectors of tridiagonal matrices;
‚ apply the QR- or Gram-Schmidt QR-method to decompose suitable matrices;
‚ apply the finite-difference method to solve characteristic value problems.
I
N GERMAN, the word “eigen” means “own” or “one’s own.” In this sense, the eigen￾value and its associated eigenvector (also referred to as the eigenpair) are commonly
termed the characteristic value and characteristic vector of a matrix.
Eigenvalue problems are commonly encountered when solving mathematical models
of various physical systems. For example, eigenvalues have applications in physics when
studying vibrating strings or the dynamic analysis of structures such as bridges, buildings,
or mass-spring systems; in studying elasticity; in electrical networks when studying the
behavior of complex electrical circuits; in control systems when determining the response of
the system; etc. A physical system can be expressed in terms of normal modes, which are the
eigenvectors. Each normal mode oscillates at some frequency, which is basically an eigenvalue
DOI: 10.1201/9781003474944-11 645646  Numerical Methods for Scientists and Engineers
associated with the corresponding eigenvector. The eigenvalues and the eigenvectors have
a number of useful properties that make them an invaluable tool, and they also yield a
number of valuable relationships with the matrices from which they are derived.
Scientists and engineers frequently resort to the numerical computation of the eigen￾values and eigenvectors of matrix equations, especially those resulting from the discretiza￾tion of differential equations. Differential equations, whose eigenvalues and/or eigenvectors
are sought, are called the Eigenvalue Problems (EVPs) or Characteristic Value Problems
(CVPs). In some cases, only the largest (dominant) or the smallest eigenvalue (in absolute
value) and its associated eigenvector may be of primary interest to an analyst.
In this chapter, the methods of computing the eigenvalues and/or associated eigenvec￾tors of a real matrix A are discussed. A general overview of the numerical methods that
are either fundamental or the basis of more elaborate methods is presented. In practice,
the matrices encountered in most eigenvalue problems are symmetric; hence, many of the
numerical methods presented here are for symmetric matrices. As in previous chapters,
algorithms for the eigenvalues are presented in the form of pseudocodes.
11.1 EIGENVALUE PROBLEM AND PROPERTIES
A matrix-eigenvalue problem is defined as
A x “ λ x (11.1)
where A is a n ˆ n square matrix, x is a non-zero vector of length n, and λ is a scalar.
Our aim is basically to solve Eq. (11.1), which has a trivial solution of x “ 0. However,
we are interested in only the non-trivial (x ‰ 0) solutions, which may exist for some special
λ values of A, called eigenvalues. The solution of an eigenvalue problem involves finding
scalar-vector pairs, i.e., a scalar λ and a vector x. Note that Eq. (11.1) can also be written
as
pA ´ λIq x “ 0 (11.2)
where I is the identity matrix. This equation poses a homogeneous system of equations, and
in order for a system to have a non-trivial solution, it must satisfy the following condition:
detpA ´ λIq “ pnpλq “ 0 (11.3)
where pnpλq is an nth-degree polynomial called characteristic polynomial or characteristic
equation and λ denotes the eigenvalues (or roots of the characteristic polynomial). For every
real λ, a corresponding real eigenvector x unique to A can be obtained from Eq. (11.2).
Basic Properties: Before moving on to numerical methods, it will be beneficial to refresh
our memories of some basic definitions and properties of eigenvalues and eigenvectors. Let
A be a real n ˆ n square matrix, then
Property 1. The sum of all eigenvalues of A (including the repeated eigenvalues) is
equal to the sum of the diagonal elements (trace) of A, i.e.,
ÿn
i“1
λi=trpAqEigenvalues and Eigenvalue Problems  647
Property 2. The product of all eigenvalues of matrix A is equal to the determinant
of matrix A, i.e.,
źn
i“1
λi=detpAq
Property 3. If A is a singular matrix, then at least one eigenvalue is zero.
Property 4. If A has imaginary eigenvalues, they exist as complex conjugate pairs.
Property 5. If A is an upper or lower triangular matrix, then the eigenvalues of A
are equal to the diagonal elements (i.e., λi “ aii).
Property 6. For β ‰ 0, if λ and x are eigenpairs of a matrix A, then the eigenpairs
of βA are βλ and x.
Property 7. If {λ, x} is an eigenpair of a matrix A and m is a natural number, then
the eigenpair of Am is {λm, x}.
Property 8. If {λ, x} is an eigenpair of an invertible matrix A, then the eigenpair of
the inverse matrix, A´1, is {λ´1, x}.
Property 9. If A is a diagonal matrix, then its eigenvalues are equal to the diagonal
elements (i.e., λi “ aii for all i), and its eigenvectors are the unit basis vectors for the
corresponding ith column.
Property 10. Eigenvectors of a matrix are linearly independent; that is, an eigenvector
cannot be expressed as a linear sum of other eigenvectors.
Property 11. If a matrix A has n independent eigenvectors, there exists a nonsingular
matrix P, where P´1AP “ D is diagonal, and the matrix A here is called a diagonalizable
matrix. Furthermore, the columns of P are the eigenvectors, and the diagonal elements are
the eigenvalues of A, i.e., λi “ dii.
11.1.1 LOCATION AND BOUNDS OF EIGENVALUES
Some iterative methods for finding the eigenvalues require an estimate of the lower and upper
bounds for the interval in which the eigenvalues lie. For this reason, it is important to become
familiar with the Gerschgorin Circle Theorem before diving into the numerical methods for
computing eigenpairs. This theorem, dealing with the location of the eigenvalues, provides
a useful tool for estimating the eigenvalues of a matrix solely based on the matrix elements.
In the complex plane, a Gerschgorin circle with the center at aii and radius equal to
ri “ Σi‰j |aij | can be defined for each row of A (i “ 1, 2,...,n). The theorem states that
every eigenvalue of A is enclosed by one Gerschgorin circle:
Ci “ tz P C : |z ´ aii| ď riu where ri “ ÿ
i‰j
|aij | for i “ 1, 2,...,n (11.4)
where C denotes the complex plane. If m of these circles form a union set D “ Ci Y
Cj ¨¨¨Y Cn that is disjoint from the remaining (n ´ m) circles, then D contains exactly m
of A’s eigenvalues. Since the eigenvalues of AT and A are the same, we may also construct
Gerschgorin circles by columns; that is,
Cpi : tz P Cp : |λ ´ ajj | “ rj u where rj “ ÿ
i‰j
|aij | for j “ 1, 2,...,n (11.5)648  Numerical Methods for Scientists and Engineers
FIGURE 11.1: Depiction of Gerschgorin circles in complex plane.
In Fig. 11.1, the Gerschgorin circles of a real 6ˆ6 square matrix in the complex plane
are illustrated: Ci : |λ ´ aii| “ ri for i “ 1, 2,..., 6. Since we are dealing with real matrices
in this chapter, the centroids of the circles always lie on the real axis, even if some eigenvalues
exist in complex conjugate pairs. Circle C4 encloses exactly one eigenvalue, which has to be
real. The union of D1 “ C5 Y C6 contains two eigenvalues, which may be real eigenvalues
or a complex conjugate pair. The union of D2 “ C1 Y C2 Y C3, on the other hand, contains
exactly three eigenvalues, which are either all real or one real and a complex conjugate pair.
Combining Eqs. (11.4) and Eq. (11.5) provides a way to possibly
narrow down the intervals for the eigenvalues. Thus, we may de￾duce that an eigenvalue of A is contained in the intersection of Cpi
and Ci, i.e., Cpi X Ci.
11.1.2 EIGENPAIRS OF REAL SYMMETRIC MATRICES
Many eigenvalue problems resulting from numerical solutions to physical problems generally
involve real and symmetric matrices; for this reason, the eigenpairs of symmetric matrices
have deserved special attention. Hence, a considerable portion of this chapter is devoted to
numerical methods applied to symmetric matrices and their applications.
Let A be a symmetric matrix (i.e., A “ AT ) whose eigenvalues are all real. There also
exists an orthogonal QT AQ “ D matrix such that D is a diagonal matrix with eigenvalues
λi “ dii, which correspond to diagonal elements of D.
For an arbitrary column vector x, we may write
Ax “ w (11.6)
and
xT A “ wT (11.7)
where w is a column vector of length n, as well. The jth elements of Ax and xT A are
obtained as
pAxqj “ ÿn
k“1
ajkxk (11.8)
pxT Aqj “ ÿn
k“1
xkakj (11.9)
However, since akj “ ajk, we can write
Ax “ pxT Aq
T (11.10)Eigenvalues and Eigenvalue Problems  649
EXAMPLE 11.1: Determining the location of eigenvalues
Plot the Gerschgorin circles for matrix A, and determine the approximate locations
of the eigenvalues.
A “
»
—
—
—
—
–
´8 ´1101
1 ´1011
´1 1 ´6 1 ´2
0 1 ´14 0
´1 0 1 1 10
fi
ffi
ffi
ffi
ffi
fl
SOLUTION:
In Section 3.3, an estimate for the spectral radius (i.e., the largest eigenvalue in
magnitude) was given as |λmax| ď �A�8 according to the Gerschgorin theorem. This
result also gives us a rough estimate that, in this case, all eigenvalues are bounded
by |λ| ď 13.
For a more refined determination, we may use a row by row approach. The radii
of the circles are determined from Eq. (11.4) as
r1 “ |´1| ` 1 ` 0 ` 1 “ 3,
r2 “ 1 ` 0 ` 1 ` 1 “ 3,
r3 “ |´1| ` 1 ` 1 ` |´2| “ 5,
r4 “ 0 ` 1 ` |´1| “ 2,
r5 “ |´1| ` 0 ` 1 ` 1 “ 3
The circles are then obtained as
C1 : tz P C : |z ` 8| ď 3u , C2 : tz P C : |z ` 1| ď 3u , C3 : tz P C : |z ` 6| ď 5u
C4 : tz P C : |z ´ 4| ď 2u , C5 : tz P C : |z ´ 10| ď 3u
FIGURE 11.2
The circles, along with the true eigenvalues marked by solid dots, are illustrated
in Fig. 11.2a. Note that C5 is disjoint from the others, indicating that one eigenvalue
is contained in C5. Moreover, since it is a real matrix, the eigenvalue contained in this650  Numerical Methods for Scientists and Engineers
circle must lie in (7,13). The circle C4 is tangent to C2, and an eigenvalue contained
in C4 is also real and lies on (2,6), even if it were exactly at the intersection point
of the circles C2 and C4. On the other hand, the union of the three circles (C1, C2,
and C3) contains three eigenvalues, one of which must be real, and the other two
eigenvalues may be either real or a complex conjugate pair.
A column-wise approach leads to
Cp1 :
�
z PCp : |z ` 8|ď 3
(
, Cp2 :
�
z P Cp : |z ` 1|ď 3
(
, Cp3 :
�
z P Cp : |z ` 6|ď 3
(
Cp4 :
�
z PCp : |z ´ 4|ď 3
(
, Cp5 :
�
z P Cp : |z ´ 10|ď 4
(
where the radii are determined by Eq. (11.5). An optimized case is found by replacing
C3 with Cp3, as depicted in Fig. 11.2b. The true eigenvalues of A are ´1.285, 4.0011,
9.8736, and ´6.795 ˘ 0.4944i, and they are located approximately near the centroid
of the disks.
Discussion: The Gerschgorin circles in rows or columns may result in different
bounds. An optimal solution can be deduced by analyzing both approaches.
Consider another arbitrary vector y and note that
py, wq“pw, yq (11.11)
satisfies the commutative property, and pw, yq denotes the inner product defined as
pw, yq “ w1y1 ` w2y2 ` ... ` wnyn
Since w “ Ax, Eq. (11.11) can be expressed as
py, Axq“px, Ayq (11.12)
Now, consider two different eigenvectors of A to be y “ x1 and y “ x2, substituting these
into Eq. (11.12) gives
px1, Ax2q“px2, Ax1q (11.13)
But using
Ax1 “ λ1x1 and Ax2 “ λ2x2 (11.14)
Equation (11.13) may be written as
λ1px2, x1q “ λ2px1, x2q (11.15)
Due to the commutative property of the inner product, Eq. (11.15) implies λ1 “ λ2. How￾ever, the eigenvalues, in general, are unequal. Therefore, the following conditions must be
met to ensure equality: px1, x2q “ 0.
Vectors satisfying this condition are called orthogonal vectors. Consequently, the eigen￾vectors of a symmetric matrix are said to be orthogonal. Hence, an ordinary vector may be
written as a linear combination of eigenvectors; that is,
x “ c1x1 ` c2x2 ` c3x3 ` ... ` cnxn (11.16)
From this point on, we will discuss the methods used to determine the eigenvalues, or
eigenpairs.Eigenvalues and Eigenvalue Problems  651
11.2 POWER METHOD
In many physical problems, computing the dominant eigenvalue is of primary interest. The
power method is a widely used iterative method for finding solely the dominant eigenvalue
of a matrix, i.e., the largest eigenvalue in absolute value. The method, which can be applied
to any matrix, whether symmetrical or not, simultaneously gives the associated eigenvector
as well.
Consider a nonsingular nˆn matrix A with n eigenvalues, |λ1| ą |λ2| ą ... ą |λn|, and
linearly independent associated eigenvectors, i.e., Axk “ λkxk for 1 ď k ď n. Using Eq.
(11.16), an arbitrary vector xp0q can be constructed as a linear combination of n eigenvectors.
Left-multiplying xp0q by A yields
Axp0q “ c1Ax1 ` c2Ax2 ` c3Ax3 ` ... ` cnAxn “ xp1q (11.17)
Invoking Axk “ λkxk, Eq. (11.17) can be expressed as
xp1q “ c1λ1x1 ` c2λ2x2 ` c3λ3x3 ` ... ` cnλnxn (11.18)
Left-multiplying both sides of Eq. (11.18) with A and invoking again the Axk “ λkxk
relationship yields
A2xp0q “ Axp1q “ xp2q “ c1λ2
1x1 ` c2λ2
2x2 ` c3λ2
3x3 ` ... ` cnλ2
nxn (11.19)
Each time multiplying by A and invoking Axk “ λkxk, the power of the eigenvalues is
raised by one. Thus, the sequence of successive multiplications can be expressed as
xppq “ Apxp0q “ Axpp´1q “ c1λp
1x1 ` c2λp
2x2 ` c3λp
3x3 ` ... ` cnλp
nxn (11.20)
which means that Apxp0q is an eigenvector of A.
Factoring out λp
1 from the rhs of Eq. (11.20) yields
xppq “ λp
1
"
c1x1 ` c2
ˆλ2
λ1
˙p
x2 ` c3
ˆλ3
λ1
˙p
x3 ` ... ` cn
ˆλn
λ1
˙p
xn
*
(11.21)
Given that |λ1| ą |λk| for k ą 1, Eq. (11.21) converges to c1λp
1x1 as p Ñ 8 . Thus, it
follows that
limpÑ8
xppq
λp
1
“ c1x1 (11.22)
If c1 “ 0, then the above expression would yield the next eigenvector with a nonzero
coefficient. Since any nonzero multiple of an eigenvector is also an associated eigenvector of
λ1, the scaled sequence, xppq
{λp
1, converges to the eigenvector of the dominant eigenvalue,
if c1 ‰ 0.
In practice, the dominant eigenvalue can be obtained by comparing the corresponding
elements of two consecutive vectors. For a large enough p, we have the following:
limpÑ8
pApxqi
pAp´1xqi
“ limpÑ8
pxp`1qi
pxpqi
“ limpÑ8
xpp`1q
i
xppq
i
“ λ1 ` O
ˆ´λ2
λ1
¯p˙
(11.23)
where xppq
i denotes the ith component of the associate vector. The growth (or decay) of λp
1
does not lead to any difficulty, and the convergence is observed to be at least of first order
with the ratio at most pλ2{λ1q
p
. Then the convergence rate may be defined as ln|λ2{λ1|.652  Numerical Methods for Scientists and Engineers
11.2.1 POWER METHOD WITH SCALING
The power method is based on the results of Eqs. (11.22) and (11.23). The procedure, which
relies on comparing the corresponding elements of consecutive vector components, results
in some problems, especially when one or more of the components of xppq is zero. For this
reason, making comparisons between corresponding vector components is undesirable. Yet
another issue arises as xppq grows larger with each iteration. an In order to avoid the overflow
problem, xppq is scaled (i.e., normalized) at each iteration. Accordingly, this procedure is
referred to as the Power method with scaling. The scaling, or normalizing, of xppq is based
on the largest component of the estimated eigenvector (i.e., the infinity norm) as follows:
xpp`1q “ 1
�xppq�8
Axppq
Next, comparing the above equation to Eq. (11.1), the largest component of xppq converges
to the dominant eigenvalue for p Ñ 8; that is,
Axppq “ �xppq
�8xpp`1q
, λ “ limpÑ8 �xppq
�8
A convergence criterion can be based on either the relative or absolute error of the
largest eigenvalue or the maximum absolute difference of the two consecutive eigenvectors,
i.e.,
|1 ´ λpp´1q
{λppq
| ă ε or |λppq ´ λpp´1q
| ă ε or �xppq ´ xpp´1q
�8 ă ε
A termination criterion based on the dominant eigenvalue is com￾putationally cheap. However, when A is ill-conditioned, the eigen￾vector has yet to converge, even though the dominant eigenvalue
generally does. Hence, it is prudent to use the maximum norm or
�2-norm of the difference vector as a termination criterion, that
is, �xppq ´ xpp´1q
�2.
A pseudomodule, POWER_METHOD_S, presented in Pseudocode 11.1 finds the domi￾nant eigenvalue and its associated eigenvector by applying the Power method with scaling.
As input, the module requires a matrix (A) and its size (n), an initial guess for the eigen￾value (λ “ λp0q
) and its associated eigenvector (x “ xp0q
), a convergence tolerance (ε), and
an upper bound for the number of iterations (maxit) as input. The module also requires
two additional modules: (1) Module Ax for carrying out matrix-vector multiplications, and
(2) Module MAX_SIZE, for finding the element with the largest magnitude (in absolute
value sense) among the elements of a vector (i.e., infinity norm, maxi|xi|). On exit from the
module, the estimated eigenvalue (λ), corresponding eigenvector (x), and maximum error
(error) are returned.
This process is carried out within a While-construct, and the iterative procedure is
stopped when the eigenpairs converge within a preset tolerance level or the number of iter￾ations reaches maxit. The procedure involves computing xppq “ Axpp´1q at each iteration
step. An estimate for the dominant eigenvalue is obtained from λppq “ max|xppq
i |, i.e., the in￾finity norm of |xppq
|. Next, the estimated eigenvector is scaled with the estimated eigenvalue
xppq Ð xppq
{λppq
. The convergence test is based on the maximum of the �xppq ´ xpp´1q
�8
and |1 ´ λpp´1q
{λppq
|. If convergence is not achieved with the maximum number of iterations
allowed (maxit), the user is issued a warning along with the error value reached.Eigenvalues and Eigenvalue Problems  653
Pseudocode 11.1
Module POWER_METHOD_S ( n, A, x, λ, ε, error, maxit)
\ DESCRIPTION: A pseudomodule to compute the dominant eigenvalue (largest in
\ absolute value) using the Power Method with scaling.
\ USES:
\ Ax:: Module computing Ax product, Pseudocode 2.5;
\ ABS:: Built-in function giving absolute value a real number;
\ MAX:: Built-in function giving largest of the argument list;
\ MAX_SIZE:: Module returning the largest magnitude element in an array.
Declare: ann, xn, xnn \ Declare array variables
error Ð MAX_SIZEpxq \ Initialize error with �xp0q�8
p Ð 0 \ Initialize iteration counter
λo Ð λ \ Initialize eigenvalue
\ Iterate until maxit is reached or convergence is achieved
While “
error ą ε And p ă maxit‰ \ Iteration loop
p Ð p ` 1 \ Count iterations
Ax(n, A, x, xn) \ Perform xppq Ð Axpp´1q
λ Ð MAX_SIZEpxnq \ Estimate largest eigenvalue
xn Ð xn{λ \ Scale eigenvector estimate with λ
err1 Ð MAX_SIZEp|x ´ xn|) \ Find infinity norm of |xppq ´ xpp´1q
|
err2 Ð |1 ´ λo{λ| \ Find rel. error for eigenvalue estimate
error Ð MAXperr1, err2q \ Find max. error of eigenpair
Write: p, λ, error \ Print iteration progress
x Ð xn \ Assign current eigenvector estimate as prior
λo Ð λ \ Assign current eigenvalue estimates as prior
End While
\ Issue a warning and error level achieved if maxit is reached with no convergence.
If “
error ą ε And p “ maxit ‰
Then
Write: “Not converged with maxit, current error is”,error
End If
End Module POWER_METHOD_S
Function Module MAX_SIZE (n, x)
\ DESCRIPTION: A pseudo Function module finding the largest element
\ (in absolute value) of an array.
\ USES:
\ ABS:: Built-in function giving absolute value a real number.
Declare: xn \ Declare array variables
xmax Ð x1 \ Set xmax to |x1|
For “
i “ 2, n‰
If “
|xi| ą |xmax|
‰
Then \ If |xi| ą xmax, update xmax.
xmax Ð xi \ Updated max is xmax “ |xi|
End If
End For
MAX_SIZEÐ xmax \ Set xmax to MAX_SIZE
End Function Module MAX_SIZE654  Numerical Methods for Scientists and Engineers
Power Method
‚ The method is simple and easy to implement;
‚ Each iteration requires only a single matrix-vector product, Ax,
which greatly reduces the computational effort, especially when A
is very large;
‚ It gives, when it converges, the largest magnitude (dominant) eigen￾value and its associated eigenvector simultaneously;
‚ Rayleigh quotient version, which generally converges faster, is effec￾tive for symmetric matrices.
‚ It only gives the dominant eigenvalue and its associated eigenvector;
‚ Its convergence rate depends on the dominance ratio |λ2{λ1|; its con￾vergence is very slow when λ2 « λ1;
‚ It is sensitive to initial guess and matrix condition number; it con￾verges to the wrong eigenpair if an initial guess coincides with an
eigenvector other than its dominant eigenvalue of A.
11.2.2 POWER METHOD WITH RAYLEIGH QUOTIENT
Another common approach requires a vector comparison based on a single criterion called
the Rayleigh Quotient (RQ). This iterative algorithm, originally developed for real sym￾metric matrices, is based on minimizing the �2-norm of pAx ´ λxq at every iteration.
The procedure starts with an initial arbitrary nonzero vector, xp0q, and then the dom￾inant eigenvalue is estimated in each iteration by
λppq “ pxpp´1q
, Axpp´1q
q
pxpp´1q, xpp´1qq “ pxpp´1q
, xppq
q
pxpp´1q, xpp´1qq
, p ě 1 (11.24)
where p denotes the iteration step and “(u,v)” the inner product.
The convergence status is checked at the end of each iteration, and the iteration pro￾cedure is terminated when the stopping criterion is met.
Convergence Issues: The number of iterations required to converge the dominant
eigenvalue, λ1, within a predetermined tolerance depends on the initial guess xp0q
. But
note that the power method may not always give the dominant eigenvalue for every matrix.
The convergence of the method also depends strongly on how well the two largest eigen￾values (λ1 and λ2) are separated; that is, |λ1| " |λ2|. In this respect, the dominance ratio
|λ2|{|λ1| is of fundamental importance in determining how quickly λ1 overwhelms other
terms.
‚ If |λ2|{|λ1| ă 1, then λ1 dominates other terms very quickly, so the convergence rate
is fast; otherwise, the convergence rate is slow.
‚ If the largest two eigenvalues coincide (λ1 “ λ2), the method may or may not converge
after a large number of iterations.
If the two eigenvalues are equal but opposite (λ1 “ ´λ2), the dominant eigenvalue can
be estimated by applying the Power method to A2, where the eigenvalues satisfy λkpA2q “
λ2
kpAq for k “ 1, 2,...,n. However, if the initial guess corresponds to any one of the non￾dominant eigenvalues of A, then the method fails due to converging to the wrong eigenvalue.Eigenvalues and Eigenvalue Problems  655
Pseudocode 11.2
Module POWER_METHOD_RQ ( n, A, x, λ, ε, error, maxit)
\ DESCRIPTION: A pseudomodule to compute the dominant eigenvalue (largest
\ in absolute value) using the Power Method with Rayleigh quotient.
\ USES:
\ Ax:: Module computing Ax product, Pseudocode 2.5;
\ ABS:: A built-in function computing the absolute value;
\ MAX:: Built-in function giving largest of the argument list;
\ XdotY:: Module computing the inner product of two vectors, Pseudocode 2.3;
\ MAX_SIZE:: Module returning the largest magnitude element in an array.
Declare: ann, xn, xnn, wn
error Ð MAX_SIZEpxq \ Initialize error with �xp0q�8
λo Ð λ \ Initialize eigenvalue estimate
p Ð 0 \ Initialize iteration counter
\ Issue a warning and error level achieved if maxit is reached with no convergence.
While “
error ą ε And p ă maxit‰ \ Iteration loop
p Ð p ` 1 \ Count iterations
Ax(n, A, x, w) \ Perform w Ð Axpp´1q
λ Ð XdotYpn, w, xq \ Find dot product, pw, xq
λ Ð λ{XdotYpn, x, xq \ Find λ Ð pw, xq{px, xq
wmax Ð MAX_SIZEpn, wq \ Find infinity norm of w
xn Ð w{wmax \ Scale w with its max norm
err1 Ð MAX_SIZEp|x ´ xn|) \ Find infinity norm of |xppq ´ xpp´1q
|
err2 Ð |1 ´ λo{λ| \ Find relative error of eigenvalue estimate
error Ð MAXperr1, err2q \ Find max. error of eigenpair
Write: p, λ, error \ Print iteration progress
x Ð xn \ Assign current eigenvector estimate as prior
λo Ð λ \ Assign current eigenvalue estimates as prior
End While
\ Issue a warning if maxit is reached with no convergence.
If “
error ą ε And p “ maxit ‰
Then
Write: “Not converged with maxit, current error is”,error
End If
End Module POWER_METHOD_RQ
A pseudomodule, POWER_METHOD_RQ, for finding the dominant eigenvalue and its
associated eigenvector by the Power method using the Rayleigh quotient technique is pre￾sented in Pseudocode 11.2. The module’s input and output lists are the same as those of
the one presented in Pseudocode 11.1. This module, besides Ax and MAX_SIZE, requires
module XdotY to compute the inner product of vectors, namely x and w. The iterative pro￾cedure requires the computation of a matrix-vector product wppq “ Axpp´1q
, and the inner
products. pxpp´1q
, wppq
q and pxpp´1q
, xpp´1q
q to estimate the eigenvalue from Eq. (11.24).
The eigenvector is normalized with the maximum norm of w, i.e., xppq “ wppq
{wmax. This
normalization procedure prevents arithmetic overflow (in the case of |λ1| ą 1) or underflow
(in the case of |λ1| ă 1) during the iterations. The termination procedure is the same as de￾scribed in Pseudocode 11.1. The user is issued a warning along with the error value reached
if convergence is not achieved with the maximum number of iterations allowed (maxit).656  Numerical Methods for Scientists and Engineers
EXAMPLE 11.2: Finding the dominant eigenvalue with the Power method
Consider the following undamped mass-spring system:
Assuming solutions of the form xi “ Ai sinpωt`φq, where Ai is the amplitude of xi;
ω and φ are the frequency and the phase shift, respectively. The equations of motion
for m1 “ m2 “ m3 “ m and k3{3 “ k1{2 “ k4{2 “ k2 “ k5 “ k are reduced to the
following eigenvalue problem:
»
–
6 ´1 ´3
´1 2 ´1
´3 ´1 6
fi
fl
»
–
x1
x2
x3
fi
fl “ m
k ω2
»
–
x1
x2
x3
fi
fl
Find the dominant frequency (largest magnitude eigenvalue) of the system, correct
to six decimal places, using the Power method with (i) scaling and (ii) the Rayleigh
quotient.
SOLUTION:
(i) A nonzero initial guess for the eigenvector is needed to apply the Power
method. Let the initial guess be x “ r 110 s
T . Setting λ “ ω2m{k and using the
“scaling technique,” we find
»
–
6 ´1 ´3
´1 2 ´1
´3 ´1 6
fi
fl
»
–
1
1
0
fi
fl “
»
–
5
1
´4
fi
fl “ 5
»
–
1
1{5
´4{5
fi
fl , λp1q “ 5
Note that Ax is normalized by its largest value (i.e., 5), which is also the first
estimate for the dominant eigenvalue.
Using the normalized vector, the second iteration yields
»
–
6 ´1 ´3
´1 2 ´1
´3 ´1 6
fi
fl
»
–
1
1{5
´4{5
fi
fl “
»
–
41{5
1{5
´8
fi
fl “ 41
5
»
–
1
1{41
´40{41
fi
fl , λp2q “ 41
5 “ 8.2
and repeating this procedure successively results in
λp3q “ 8.9024390, xp3q “ r 1 0.002740 ´ 0.997260 s
T
λp4q “ 8.9890411, xp4q “ r 1 0.000305 ´ 0.999695 s
T
λp5q “ 8.9987809, xp5q “ r 1 0.000034 ´ 0.999966 s
T
λp6q “ 8.9998645, xp6q “ r 1 3.76 ˆ 10´6 ´ 0.999996 s
T
λp7q “ 8.9999849, xp7q “ r 1 4.18 ˆ 10´7 ´ 1 s
T
λp8q “ 8.9999983, xp8q “ r 1 4.65 ˆ 10´8 ´ 1 s
T
λp9q “ 8.9999998, xp9q “ r 1 5.16 ˆ 10´9 ´ 1 s
TEigenvalues and Eigenvalue Problems  657
(ii) Using xp0q “ r 110 s
T with the Rayleigh quotient also yields
w “ Axp0q “
»
–
6 ´1 ´3
´1 2 ´1
´3 ´1 6
fi
fl
»
–
1
1
0
fi
fl “
»
–
5
1
´4
fi
fl
For the first iteration step for the inner products, we obtain pw, xp0qq “ 6 and
pxp0q, xp0qq “ 2, which leads to the first estimate for the eigenvalue:
λp1q “ pxp0q, wq
pxp0q, xp0qq “ 6
2 “ 3
Having normalized the eigenvector estimate to xp1q “ w{�w�8 “ r 1 1{5 ´ 4{5 s
T ,
the vector w at the second iteration becomes
w “ Axp1q “
»
–
6 ´1 ´3
´1 2 ´1
´3 ´1 6
fi
fl
»
–
1
1{5
´4{5
fi
fl “
»
–
41{5
1{5
´8
fi
fl
which leads to pxp1q
, wq “ 366{25 and pxp1q, xp1qq “ 42{25. Then the second estimate
for the dominant eigenvalue becomes
λp2q “ pxp1q, wq
pxp1q, xp1qq “ 61
5 “ 8.7142857
Likewise, the dominant eigenvalue estimates of the subsequent iterations, summa￾rized below, are obtained in the same manner.
λp3q “ 8.9963437, xp3q “ r 1 0.002740 ´ 0.997260 s
T
λp4q “ 8.9999548, xp4q “ r 1 0.000305 ´ 0.999695 s
T
λp5q “ 8.9999994, xp5q “ r 1 0.000034 ´ 0.999966 s
T
λp6q “ 8.9999999, xp6q “ r 1 3.76 ˆ 10´6 ´ 0.999996 s
T
λp7q “ 9, xp7q “ r 1 4.18 ˆ 10´7 ´ 1 s
T
With both methods, the dominant eigenvalue and its associated eigenvector con￾verge to λ “ 9 and x “ r1 0 ´ 1s. The dominant frequency can then be obtained
from λ “ ω2m{k as ω “ 3
ak{m.
Discussion: The Power method is quite simple and requires very little storage.
Normalization of the vector is necessary because the eigenvector iterates will grow
exponentially (or decay) if scaling is not applied.
As far as the speed of convergence is concerned, the dominance ratio plays an
important role. The dominance ratio |λ2{λ1|, where λ1 and λ2 denote the largest and
next largest eigenvalues (in magnitude), is of fundamental importance in determining
how fast the method will converge. In this example, the matrix is symmetrical, and
its eigenvalues are 9, 4, and 1. We note that the second-largest eigenvalue (λ2 “
4) is considerably smaller than the largest one (λ1 “ 9). That is why we obtain
convergence in both methods very quickly. Since the rate of convergence is pλ2{λ1q2 “
p4{9q2, the procedure with the Rayleigh quotient converged faster.658  Numerical Methods for Scientists and Engineers
11.2.3 INVERSE POWER METHOD
Inverse Power Method is designed to determine the smallest eigenvalue of a matrix and
its associated eigenvector. To achieve this, both sides of Eq. (11.1) are multiplied with the
inverse of A and rearranged as follows:
1
λx “ A´1x or A´1x “ μx (11.25)
where μ “ 1{λ and A is a nonsingular matrix.
Once the problem is cast as A´1x “ μx, the Power method can be applied to find
the dominant eigenvalue (μ) corresponding to the smallest value of 1/λ. In practice, of
course, the computation of the inverse matrix is avoided by noting that the Power iteration
(xpp`1q “ A´1xppq
) is equivalent to
Axpp`1q “ xppq (11.26)
The above linear system is then solved for the current eigenvector estimates, xpp`1q
. A
judicious strategy to tackle this linear system is to employ LU-decomposition on A. Once
we have L and U, we can start with an initial guess xp0q such that xp0q
8 “ 1 and xp0q ‰ 0.
The system is solved by a forward elimination, Lz “ xppq
, followed by a back substitution,
Uxpp`1q “ z algorithm. A new estimate for the minimum eigenvalue (in absolute value) is
computed from
μpp`1q “ pxppq
, xpp`1q
q
pxppq, xppqq (11.27)
where the prior step eigenvector estimate is obtained as xpp´1q Ð xppq
{xppq
. If μpp`1q ă
0, the eigenvector is updated as xpp´1q Ð p´1qxppq
{xppq
]. The iteration procedure is
continued until the stopping criterion is met.
When using the Gauss elimination algorithm to solve Eq. (11.26),
the matrix A is destroyed during the forward elimination step.
Hence, a copy of A should also be created before the Power itera￾tion procedure is applied, and this copy should be retrieved each
time A is needed (A Ð B). On the other hand, LU-decomposition
of A is a memory-efficient approach that does not require a copy
of A (L and U can also be saved on A). Then, LUxpp`1q “ xppq
is solved at each iteration.
EXAMPLE 11.3: Finding the smallest eigenvalue
Find the smallest frequency (eigenvalue) of the spring-mass system given in Example
11.2 (correct to six decimal places) using the power method with Rayleigh quotient.
SOLUTION:
Starting with xp0q “ r 110 s
T , we begin with the following linear system:
Axp1q “
»
–
6 ´1 ´3
´1 2 ´1
´3 ´1 6
fi
fl
»
–
x1
x2
x3
fi
fl
p1q
“
»
–
1
1
0
fi
fl
which leads to xp1q “ r5{914{9s
T .Eigenvalues and Eigenvalue Problems  659
FIGURE 11.3: The distribution of the eigenvalues with respect to the estimate α.
The first estimate for the dominant eigenvalue of A´1 becomes
μp1q “ pxp1q
, xp0q
q
pxp0q, xp0qq “ 14{9
2 “ 7
9
The second iteration leads to
Axp2q “
»
–
5 3 ´1
´2 5 ´3
´1 ´3 5
fi
fl
»
–
x1
x2
x3
fi
fl
p2q
“
»
–
5{9
1
4{9
fi
fl ñ
»
–
x1
x2
x3
fi
fl
p2q
“
»
–
41{81
1
40{81
fi
fl
which, for the eigenvalue estimate, gives
μp2q “ pxp2q
, xp1q
q
pxp1q, xp1qq “ 1094{729
122{81 “ 547
549
The subsequent iteration steps result in
μp3q “ 0.9999548, xp3q “ r 0.500686 1 0.499314 s
T
μp4q “ 0.9999994, xp4q “ r 0.500076 1 0.499924 s
T
μp5q “ 1, xp5q “ r 0.500008 1 0.499992 s
T
μp6q “ 1, xp6q “ r 0.500001 1 0.499999 s
T
The smallest eigenvalue is then found from λ “ 1{μ “ 1. Using λ “ ω2m{k, the
smallest frequency is obtained as ω “ ak{m.
Discussion: The smallest eigenvalue is obtained very quickly. The 3ˆ3 system in
this example was solved by applying the Gauss elimination method. However, for
large matrices, the computational effort with Gaussian elimination will increase for a
large number of iterations, so the implementation of the LU-decomposition method
should be considered in such cases.
11.2.4 SHIFTED INVERSE POWER METHOD
This method is based on shifting and inverting the eigenvalues. For i“1, 2,...,n, suppose
λi and xi are the eigenpairs of a nonsingular matrix A. For a constant α, the eigenpair of
A´αI (shifting) is λi´α and xi. On the other hand, the eigenpair of A´1 (inverting) is 1{λi
and xi for all i. Hence, employing shifting by α (α ‰ λ) and inverting, we find pλi´αq
´1 and
xi as the eigenpair of pA ´ αIq
´1
. Further, we will assume that A has distinct eigenvalues:
|λ1| ą |λ2| ą |λ3| ą ¨¨¨ ą |λn|. If α is chosen for an arbitrary λk such that α « λk (see
Fig. 11.3), then the eigenvalue of pA ´ αIq
´1 closest to α will be the largest one; in other
words, μk Ñ 8 as α Ñ λk while the remaining eigenvalues, μi “ pλi´λkq
´1
, are finite.
Now, starting with xp0q ‰ 0 provided that �xp0q� “ 1, a sequence of eigenpairs can be
obtained as follows:
pA ´ αIqxpp`1q “ xppq (11.28)660  Numerical Methods for Scientists and Engineers
Inverse Power Method
‚ The power method with inverse or shift is easy to implement;
‚ It can provide very fast convergence in cases of symmetric matrices;
‚ It is also suitable to find complex eigenvectors.
‚ Only one eigenpair can be determined;
‚ The approximate value of the desired eigenvalue needs to be known
in advance;
‚ The rate of convergence may be slow if there is a second eigenvalue
close to the one sought.
μpp`1q “ pxppq
, xpp`1q
q
pxppq, xppqq (11.29)
where, before the next iteration, the current eigenvector estimate is normalized as the prior
eigenvector estimate, i.e., xpp´1q Ð xppq
{�xppq
�. If μpp`1q ă 0, the eigenvector is also up￾dated as xpp´1q Ð p´1qxppq
{�xppq
�. This sequence converges to the dominant eigenvalue and
its associated eigenvector of the matrix pA ´ αIq
´1
. Finally, the corresponding eigenvalue
of A is obtained from
λk “ α `
1
μ1
(11.30)
where μ1 is the dominant eigenvalue of pA ´ αIq
´1
.
The convergence rate of the shifted Inverse Power method depends on how fast the
power of the dominance ratio, |λ2{λ1|
p, yields zero as p Ñ 8. The asymptotic error constant,
Op|μ2{μ1|q, for pA ´ αIq
´1
in the vicinity of α « λk can be written as
ˇ
ˇ
ˇ
ˇ
μk`1
μk
ˇ
ˇ
ˇ
ˇ “
ˇ
ˇ
ˇ
ˇ
1{pλk`1 ´ αq
1{pλk ´ αq
ˇ
ˇ
ˇ
ˇ “
ˇ
ˇ
ˇ
ˇ
λk ´ α
λk`1 ´ α
ˇ
ˇ
ˇ
ˇ
Despite its linear convergence rate, this approach improves convergence with a judicious
choice of α (i.e., λk « α) because the asymptotic error constant becomes smaller (λk ´
α « 0). From the preceding arguments, it is clear that λk does not necessarily need to
be the largest or smallest one; it can be used to determine any desired eigenpair of A.
Having cast the eigenvalue problem as pA ´ αIq
´1
x “ μx, the inverse Power method is
generally employed to determine the dominant eigenvalue and its associated eigenvector.
After replacing A with pA ´ αIq, the Power method can be employed.
To achieve fast convergence, a ’smart’ prediction sufficiently close
to the α value is required. Otherwise, the benefit of shifting cannot
materialize if the prediction is far from the desired eigenvalue.
EXAMPLE 11.4: Finding a specific eigenvalue
Consider the spring-mass system given in Example 11.2. Find the frequency closest
to α “ 4.1 to an accuracy of six decimal places using the shifted inverse power
method with the Rayleigh quotient.Eigenvalues and Eigenvalue Problems  661
SOLUTION:
First, we construct the pA´αIq matrix, which is symmetrical. To solve the linear
system, Eq. (11.28), we may employ Cholesky decomposition, giving
A ´ αI “
»
–
1.9 ´1 ´3
´1 2.1 ´1
´3 ´1 1.9
fi
fl , L “
»
—
—
—
—
–
?60 0
´ 1
?6
c11
6 0
´
c3
2 ´3
c 3
22
6
?11
fi
ffi
ffi
ffi
ffi
fl
The starting guess vector, as in previous examples, is set to x “ r 110 s
T . The first
iteration requires the solution of pA ´ αIqxp1q “ xp0q, which yields
xp1q “r ´0.0592495 ´ 0.3225807 ´ 0.2633311 s
T
Next, the largest eigenvalue is estimated by
μp1q “ pxp1q
, xp0qq
pxp0q, xp0qq “ ´0.3818302
2 “ ´0.190915
Since the eigenvalue is negative, the solution vector is normalized before the second
iteration as
xp1q Ð ´xp1q
{xp1q
8 “ r 0.1836735 1 0.8163266 s
T
Then solving pA ´ αIqxp2q “ xp1q yields
xp2q “r ´0.2258468 ´ 0.3225806 ´ 0.0967339 s
T
The second estimate for the dominant eigenvalue yields
μp2q “ pxp2q, xp1qq
pxp1q, xp1qq “ ´0.4430291
1.7001249 “ ´0.2605862
and we find the normalized solutions as
xp2q Ð ´xp2q
{xp2q
8 “ r 0.700125 1 0.299875 s
T
In the subsequent iteration steps, the estimates for the dominant eigenvalue and
corresponding eigenvector are found as follows:
μp3q “ ´0.295883, xp3q “ r 0.373390 1 0.626610 s
T
μp4q “ ´0.311560, xp4q “ r 0.580100 1 0.419900 s
T
μp5q “ ´0.318113, xp5q “ r 0.449324 1 0.550676 s
T
.
.
. .
.
.
μp15q “ ´10, xp15q “ r 1 ´ 1 1 s
T
The largest eigenvalue of pA ´ αIq
´1 (in magnitude) converges to ´10. Using this
value in Eq. (11.30), the eigenvalue near α “ 4.1 is obtained as
λ “ α ` 1{μ1 “ 4.1 ` p´1{10q “ 4
Discussion: From λ “ ω2m{k, the corresponding frequency is obtained as ω “
2
ak{m, and its associated eigenvector is r1 ´ 1 1s
T . For large A, the solution
of the linear system pA ´ αIqxpp`1q “ xppq should be obtained with the Cholesky
decomposition, which can result in a significant reduction in the cpu-time.662  Numerical Methods for Scientists and Engineers
11.3 SIMILARITY AND ORTHOGONAL TRANSFORMATIONS
The calculation of eigenvalues of diagonal or triangular matrices is much easier. In this
regard, it is desirable to transform a dense matrix into a similar matrix with a diagonal or
tridiagonal form, as this reduces the complexity of calculating the eigenvalues.
To begin, let A and P be square and nonsingular matrices. The matrix A and P´1AP
are said to be similar matrices, and the conversion from matrix A to matrix P´1AP is
called similarity transformation. The similarity transformation of a matrix A to a matrix
C is defined as
C “ P´1AP (11.31)
Note that this relationship is also symmetric since A “ PCP´1, and Eq. (11.31) can be
expressed as PC“AP.
A similarity relation can be interpreted to say that A and C are matrix representations
of the same linear transformation with respect to a different basis. Matrix P may be referred
to as the change of basis matrix. Now, let us begin with the standard eigenvalue problem:
Ax “ λx (11.32)
Left-multiplying Eq. (11.32) with B´1 gives
B´1A x “ λ B´1x (11.33)
Setting z “ B´1x (also x “ Bz) in Eq. (11.33) yields
Cz “ λ z (11.34)
where C “ B´1A B. Note that Eq. (11.34) is similar to Eq. (11.32), and λ remains unaf￾fected by the preceding matrix algebra. In other words, if A and C are similar, then the
characteristic polynomials and eigenvalues of A and C are the same, i.e., pApλq “ pCpλq
and λipAq “ λipCq. But the eigenvectors are different.
A matrix A is said to be diagonalizable if matrix D is diagonal and similar to A. Any
diagonalizable matrix A has n distinct eigenvalues, λi, and linearly independent associated
eigenvectors, xi. This condition is sufficient (but not necessary) for diagonalization.
An invertible matrix P is needed to diagonalize a square matrix. If a suitable matrix
can be constructed from its eigenvectors, such as P “ rx1 x2 ¨¨¨ xns, then D “ P´1A P is
a diagonal matrix whose diagonal elements are eigenvalues:
D “
»
—
—
—
–
λ1
λ2
...
λn
fi
ffi
ffi
ffi
fl
Note that changing the order of eigenvalues or eigenvectors (by exchanging the column
vectors of P) produces a different diagonalization of the same matrix.
In symmetric matrices, the matrix P can be chosen to have the special property of
P´1 “ PT . A matrix P is called an orthogonal matrix if P´1 “ PT , provided that the
matrix P is invertible. A matrix A is orthogonally diagonalizable if an orthogonal matrix P
exists, so thatEigenvalues and Eigenvalue Problems  663
EXAMPLE 11.5: Application of diagonalization procedure
Determine if matrix A is diagonalizable or not. If so, find P and P´1A P.
A “
»
–
3 11
1 31
´1 ´1 1
fi
fl
SOLUTION:
The characteristic equation of matrix A is found to be λ3 ´ 7λ2 ` 16λ ´ 12 “ 0
or pλ ´ 2q
2
pλ ´ 3q “ 0. Note that λ “ 2 is an eigenvalue of the multiplicity of “two.”
Thus, the diagonalizability of A depends on having two independent eigenvectors
for λ “ 2.
By definition, we seek a nontrivial solution, that is,
pA ´ λIqx “
»
–
3 ´ λ 1 1
1 3 ´ λ 1
´1 ´1 1 ´ λ
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
0
0
0
fi
fl
For λ1 “ 3, the linear system becomes
»
–
011
101
´1 ´1 ´2
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
0
0
0
fi
fl
and the solution of this homogeneous system can be written x1 “ r 1 1 ´ 1 s
T .
For λ2,3 “ 2, we arrive at
»
–
111
111
´1 ´1 ´1
fi
fl
»
–
x1
x2
x3
fi
fl “
»
–
0
0
0
fi
fl
which gives x1 ` x2 ` x3 “ 0. With one leading and two free variables, s and t, the
general solution is found as x1 “ s ` t, x2 “ ´s, and x3 “ ´t. Two independent
eigenvectors can then be obtained by arbitrary choices of ps, tq“p1, 0q and ps, tq “
p0, 1q, as follows:
x2 “
»
–
1
´1
0
fi
fl and x3 “
»
–
1
0
´1
fi
fl
Now, constructing matrix P and calculating P´1 gives
P “
»
–
111
1 ´1 0
´1 0 ´1
fi
fl and P´1 “
»
–
111
101
´1 ´1 ´2
fi
fl
Finally, we obtain
P´1A P “
»
–
111
101
´1 ´1 ´2
fi
fl
»
–
3 11
1 31
´1 ´1 1
fi
fl
»
–
111
1 ´1 0
´1 0 ´1
fi
fl “
»
–
300
020
002
fi
fl
Discussion: Notice that the triple matrix product gives a diagonal matrix whose
diagonal elements are the eigenvalues of A.664  Numerical Methods for Scientists and Engineers
D “ QT AQ (11.35)
which represents an orthogonal transformation. Here, the letter Q is denoted instead of P
to distinguish this special case. Note that the QAQT matrix product is also an orthogonal
transformation, which preserves not only the eigenvalues but also the symmetry property
of the matrix. To diagonalize an orthogonal matrix,
i. find all eigenvalues of A and determine the multiplicity of each eigenvalue;
ii. choose a unit eigenvector for every eigenvalue of multiplicity of m “ 1, i.e., normalize
eigenvectors;
iii. find a set of m eigenvectors for each eigenvalue of multiplicity m ě 2.
If an orthonormal set cannot be found, then the Gram-Schmidt orthonormalization process
can be applied. Each eigenvector is a column in matrix Q.
11.3.1 DIFFERENTIAL EQUATIONS
The diagonalization property is especially useful to uncouple a linear system of ODEs. Let
a set of first-order coupled ODEs be given as
dx
dt “ Ax, xp0q “ x0 (11.36)
where x “ rx1ptq, x2ptq, ¨¨¨ , xnptqsT and A is a diagonalizable matrix. Substituting
x “ Pz, into Eq. (11.36) and left-multiplying by P´1 on both sides, the system is an un￾coupled form:
P´1Pdz
dt “ P´1APz Ñ dz
dt “ Dz (11.37)
where D is a diagonal matrix whose diagonal elements are the eigenvalues of A.
EXAMPLE 11.6: Solving coupled ODEs using the diagonalization procedure
Consider the following reversible chemical reactions:
X ÐÑ
k1
k2
Y ÐÑ
k3
k4
Z
The kinetic behavior of the reactions is described by the rate equations and the
initial conditions and is given by:
dCX
dt “ ´k1CX ` k2CY
dCY
dt “ k1CX ´ pk2 ` k3qCY ` k4CZ
dCZ
dt “ k3CY ´ k4CZ
,
////////.
////////-
CXp0q “ 1
CY p0q “ 0
CZp0q “ 0
where k1 “ 0.5, k2 “ 0.75, k3 “ 0.25, and k4 “ 1.5 min´1 are the reaction rate
constants. (a) Express the given system of coupled ODEs in matrix form, and (b)
apply the diagonalization procedure to decouple the system into first-order ODEs.Eigenvalues and Eigenvalue Problems  665
SOLUTION:
(a) The system can be expressed in matrix form as dx{dt “ Ax, where
A “ 1
4
»
–
´23 0
2 ´4 6
0 1 ´6
fi
fl , x “
»
–
CX
CY
CZ
fi
fl , and xp0q “
»
–
1
0
0
fi
fl
The matrix has three distinct and real eigenvalues: λ “ ´2, ´1, and 0; hence, the
given system is diagonalizable.
Using a suitable method, the eigenpairs of A are determined to construct D, P,
and P´1, yielding
D “
»
–
´2
´1
0
fi
fl , P “
»
–
1 ´3 9
´226
1 11
fi
fl , P´1 “ 1
16
»
–
1 ´3 9
´226
1 11
fi
fl
As illustrated in Eq. (11.37), by introducing x “ Pz substitution, followed by left
multiplication of both sides with P´1, the system is decoupled as dz{dt “ Dz,
leading to
dz1
dt “ ´2z1
dz2
dt “ ´z2
dz3
dt “ 0
,
/////.
/////-
Ñ zp0q “ P´1xp0q “ 1
16
»
–
1
´2
1
fi
fl
The analytical solutions of the uncoupled ODEs can then be obtained as
z1ptq “ e´2t
{16, z2ptq“´e´t
{8 z3ptq “ 1{16
Finally, using x “ Pz, the solution for the original coupled ODEs is found as
x1ptq “ z1ptq ´ 3z2ptq ` 9z3ptq “ 1
16
e´2t
p3et ` 1q
2
,
x2ptq“´2z1ptq ` 2z2ptq ` 6z3ptq “ 1
8
e´2t
p3et ` 1qpet ´ 1q,
x3ptq “ z1ptq ` z2ptq ` z3ptq “ 1
16
e´2t
pet ´ 1q
2
Discussion: Note that the diagonalization procedure through the decoupling of an
ODE system greatly simplifies the mathematical procedure and leads to the easy
finding of analytical solutions.
One of the most important consequences of diagonalization is that
the very large power of a matrix, A, can easily be evaluated by
Ak “ PDkP´1.666  Numerical Methods for Scientists and Engineers
11.4 JACOBI METHOD
The Jacobi method, based on orthogonal transformations, is a numerical technique fre￾quently used to evaluate the eigenvalues and eigenvectors of a symmetric matrix. Nonethe￾less, this limitation is relatively insignificant since many problems of practical importance
in science and engineering involve symmetric matrices [9].
The Jacobi method transforms a symmetric matrix into a diagonal matrix through a
series of orthogonal transformations. As indicated earlier, the eigenvalues are unaltered dur￾ing these transformations. With each transformation (or plane rotation), symmetric pairs of
the largest off-diagonal element are zeroed out. However, the zeros established by previous
plane rotations are not necessarily preserved in subsequent transformations; that is, new
rotations generally undo elements previously zeroed out. Yet, with each transformation,
the non-zero off-diagonal elements become smaller and smaller until the matrix is diago￾nal to machine precision. The diagonal elements of the final transformed matrix are the
eigenvalues.
The basic Jacobi orthogonal rotation matrix U has the following form:
U “
p ... q
»
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
–
1
...
1
c s
1
...
1
´s c
1
...
1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
p
.
.
.
q
(11.38)
where (p, q) denotes the plane of rotation, uij “ 1 for i ‰ p, i ‰ q, upp “ uqq “ c and
upq “´uqp “s, where c and s denote the cosine and sine of the rotation angle θ, respectively,
so c2`s2 “1. All off-diagonal elements are zero except s and ´s.
A plane rotation, using matrix U, modifies only the pth and qth rows and columns
of matrix A. The objective is to zero the off-diagonal elements (apq “ aqp “ 0) by a series
of UT AU orthogonal transformations. As a result, with each successive transformation,
matrix A is modified until all off-diagonal elements are zero. Accordingly, this similarity
transformation procedure is expressed as
Ak`1 “ UT AkU (11.39)
where Ak`1 and Ak denote the two successive transformation matrices, and A0 is the
original matrix A.
The matrix U is orthogonal since U and UT are inverses of each other (UTU“I). In
the triple product UTAU, only the pth row and the qth column elements of A are altered,
so we may express this operation as
„
c ´s
s c j „app apq
aqp aqqj „ c s
´s cj
“
„
a1
pp a1
pq
a1
qp a1
qqj
(11.40)Eigenvalues and Eigenvalue Problems  667
Equation (11.40) gives
a1
pp “ c2app ` s2aqq ´ 2cs apq
a1
qq “ c2aqq ` s2app ` 2csapq
a1
pq “ a1
qp “ pc2 ´ s2qapq ` cspapp ´ aqqq
(11.41)
The next step involves determining c and s. We already have c2` s
2 “ 1, and we need
an additional relationship to find c and s. We enforce a1
pq “a1
qp “0. Setting a1
pq “0 in Eq.
(11.41) and noting that c2´s2 “cos 2θ and cs“ p1{2qsin 2θ, we obtain
cos 2θ apq `
1
2
sin 2θ papp ´ aqqq “ 0 (11.42)
or rearranging Eq. (11.42) gives
tan 2θ “ 2apq
app ´ aqq
(11.43)
For rotations, only sin θ and cos θ are required. Rather than finding θ from Eq. (11.42) and
then computing sin θ and cos θ, it is more convenient and also more accurate to compute c
and s directly using trigonometric relationships. After some manipulation, we write
c “ cos θ “
d
1
2 ` |α|
2β (11.44)
s “ sin θ “ α p´apqq
2β|α| cos θ (11.45)
where α “ papp ´ aqqq{2 and β “
b
a2
pq ` α2.
The pth row and the qth column elements of A are modified as a result of an orthogonal
transformation as follows:
For pth and qth rows (j ‰ p or q)
a1
pj “ c apj ´ s aqj and a1
qj “ s apj ` c aqj (11.46)
For pth and qth columns (i ‰ p or q)
a1
ip “ c aip ´ s aiq and a1
iq “ s aip ` c aiq (11.47)
All other elements of A remain unchanged.
The procedure now requires choosing the element apq, which we desire to make zero.
Then, having calculated c and s from Eqs. (11.44) and (11.45), the new values of a1
pp and a1
qq
can be determined from Eq. (11.41), and apq and aqp are set to zero. Finally, Eqs. (11.41)
provide the remaining modifications to the matrix A.
The eigenvectors are also computed in the same manner by applying the same orthog￾onal transformations (Xk`1 “ XkU) to the identity matrix (where the identity matrix and
its elements are taken as X0 “ I), so the transformation yields
pth column : xip “ c xip ´ s xiq
qth column : xiq “ s xip ` c xiq
(11.48)668  Numerical Methods for Scientists and Engineers
All other columns remain unchanged. When the eigenvalue problem converges, the columns
of X become the eigenvectors of the original matrix.
The eigenvalues of matrix A are the diagonal elements of the modified matrix for
sufficiently large k, i.e., A8 “ limkÑ8Ak. The jth column of the converged matrix, X8,
is the eigenvector corresponding to λj , i.e., Xpjq
8 “ r x1j , x2j , x3j , ..., xnj s
T .
The algorithm presented in this section is referred to as the classical Jacobi method.
It is based on searching for the location of the largest off-diagonal element (p and q) to
be zeroed out. Nevertheless, a drawback of this algorithm is that searching for the largest
element in very large matrices can be time-consuming due to the number of comparisons on
the order of Opn2q. While a1
pq and a1
qp are zeroed out, the matrix elements are modified with
each plane rotation, which also modifies previously annihilated elements, often resulting in
nonzero values. But the magnitude of the non-zero off-diagonal elements becomes smaller
with each subsequent rotation, and the matrix eventually tends toward diagonal form. The
method converges quite quickly once the off-diagonal elements become small.
An alternative, the cyclic Jacobi method, has been developed to eliminate excessive
(search and compare) operations devoted to finding the location of the largest off-diagonal
element. In this variant, the off-diagonal elements are zeroed out in a predetermined order
regardless of their magnitudes, i.e., ordering can be devised on a row-by-row or column-by￾column basis. Each element of a matrix is zeroed out only once in a sequence of n rotations
called a sweep. This method also converges quite quickly once off-diagonal elements become
small.
The accuracy of the Jacobi method and its variants depends on the criteria adopted for
terminating iterations. The simplest criterion that can be used to terminate the iteration
process is to check whether the maximum absolute value of the off-diagonal elements is
smaller than a preset convergence tolerance; that is, max|apq| ă ε for all p and q (p ‰ q).
However, at this stage, the reader must be cautioned to take into account the relative
magnitudes of app and aqq. However, another criterion involves checking the sum of the
squares of the off-diagonal elements at the end of each sweep.
T “ ÿn
i“1
ÿn
j“i`1
a2
ij
This procedure is called the Threshold Method. The threshold value is found by μ “ ?
T{n
which is computed in the subsequent iterations as μpkq “ ?
Tpkq{n, where Tpkq is the sum
of the squares of the off-diagonal elements of Ak. This process is terminated when μk ď ε.
Jacobi Method
‚ Jacobi method is simple, reliable, and easy to program;
‚ It works well for well-conditioned matrices;
‚ It gives all eigenvalues and eigenvectors of a real symmetric matrix.
‚ It is limited to symmetric matrices;
‚ It converges rather slowly and may require a very large number of
iterations (the minimum number of rotations is npn ´ 1q{2);
‚ The zeros created in previous rotations can be undone in subsequent
rotations.Eigenvalues and Eigenvalue Problems  669
Pseudocode 11.3
Module JACOBI ( n, A, X, ε, maxit)
\ DESCRIPTION: A pseudomodule to find the eigenvalues and eigenvectors
\ using the Jacobi method.
\ USES:
\ ABS:: A built-in function computing the absolute value;
\ SQRT:: A built-in function computing the square-root of a value;
\ ROTATE:: Module performing plane rotations.
Declare: ann, xnn \ Declare array variables
X Ð 0 \ Initialize eigenmatrix
For “
i “ 1, n‰ \ Set up Identity matrix
xii Ð 1 \ Set up unit diagonals, xii Ð 1
End For
T hreshold Ð 1 \ Initialize the threshold value
k Ð 0 \ Initialize iteration count
Repeat \ Iteration Loop
amax Ð 0 \ Initialize max value
For “
i “ 1, n‰ \ Search for (p, q) of element with max. magnitude
For “
j “ i ` 1, n‰ \ Search upper diagonal elements
If “
|aij | ą amax‰
Then
amax Ð |aij |
p Ð i; q Ð j \ Save pi, jq on pp, qq
End If
End For
End For
α Ð papp ´ aqqq{2 \ Compute rotation angles
β Ð
b
α2 ` a2
pq
c Ð ap1 ` |α|{βq{2 \ Find rotation cosines
s Ð αp´apqq{p2 ˚ |α| c ˚ βq \ Find rotation sine
ROTATEpn, p, q, c, s, A, Xq \ Get rotation done!
sums Ð 0 \ Initialize accumulator, sums
For “
i “ 1, n‰ \ Compute sums of squares of off-diagonals
For “
j “ i ` 1, n‰
sums Ð sums ` a2
ij \ Accumulate a2
ij
End For
End For
T hreshold Ð ?sums{n \ Compute threshold value
k Ð k ` 1 \ Count iterations
Write: k, T hreshold \ Print out the iteration progress
Until “
T hreshold ă ε Or k “ maxit‰
If “
k “ maxit‰
Then \ Issue info and a warning
Write: “The maxit reached with no convergence”
Write: “The threshold value is =”, T hreshold
End If
End Module JACOBI
Module ROTATE (n, p, q, c, s, A, X)
\ DESCRIPTION: A pseudomodule to perform a plane rotation670  Numerical Methods for Scientists and Engineers
Declare: ann, xnn
For “
i “ 1, n‰
g1 “ api
api “ c ˚ g1 ´ s ˚ aqi
aqi “ s ˚ g1 ` c ˚ aqi
End For
For “
i “ 1, n‰
g1 Ð aip; g2 Ð xip
aip Ð c ˚ g1 ´ s ˚ aiq
aiq Ð s ˚ g1 ` c ˚ aiq
xip Ð c ˚ g2 ´ s ˚ xiq
xiq Ð s ˚ g2 ` c ˚ xiq
End For
End Module ROTATE
A pseudomodule, JACOBI, computing all eigenvalues and eigenvectors of a real sym￾metric matrix is presented in Pseudocode 11.3. The module requires a symmetric matrix
(A), the matrix size (n), a convergence tolerance (ε), and an upper bound for the maximum
number of iterations (maxit) as input. On exit, the matrix A is a diagonal matrix whose
elements are the eigenvalues (λi “ aii), and the eigenvectors are stored on the square matrix
X. At the beginning, X is initialized with the zero matrix, i.e., X “ 0. Then, the diagonals
are set as xii “ 1 for all i. The location of the matrix element corresponding to the largest
off-diagonal value, (p, q), is found with a couple of If-constructs. The rotation angles are
determined by Eqs. (11.44) and (11.45). Then the matrix A is modified according to Eqs.
(11.46) and (11.47) and applied to the matrix X by Eq. (11.48), which zeros the matrix
element at (p, q). The procedure is repeated until A is transformed into a diagonal matrix.
The state of the diagonalization process is verified by checking whether the threshold crite￾rion is satisfied or not, i.e., μpkq “ ?
Tpkq{n ă ε. Once the diagonalization is achieved, the
diagonal elements of A are the eigenvalues, and the corresponding columns of X are the
associated eigenvectors pλj , xj q for j “ 1, 2,...,n.
EXAMPLE 11.7: Application of the Jacobi method
A machine component is subjected to the following stress conditions:
»
–
σxx σxy σxz
σyx σyy σyz
σzx σzy σzz
fi
fl “
»
–
17 ´5 41
´5 53 ´5
41 ´5 17
fi
fl
where the stresses are in MPa. The principal stresses are obtained from the solution
of the following eigenvalue problem:
»
–
σxx σxy σxz
σyx σyy σyz
σzx σzy σzz
fi
fl
»
–
�1
�2
�3
fi
fl “ λ
»
–
�1
�2
�3
fi
fl
where λ denotes the principal stresses (eigenvalues) and �1, �2, and �3 denote the di￾rection cosines defining the principal plane on which σ acts. Use the Jacobi method to
find the principal stresses and associated principal planes in the machine component
for the given stress condition.Eigenvalues and Eigenvalue Problems  671
SOLUTION:
Since the stress matrix is symmetric, the Jacobi method can be employed to find
its eigenpairs. Setting A0 “ A, we start searching for the location of the absolute
maximum of the off-diagonal elements.
There is only one element with the largest magnitude, namely, |a13| “ 41. We
start the plane rotations with pp, qq“p3, 1q, for which we have a33 “ a11 “ 17 and
a13 “41.
Next, α and β are determined as follows:
α “ a33 ´ a11
2 “ 17 ´ 17
2 “ 0
β “
b
a2
13 ` α2 “
a
412 ` 02 “ 41
The rotation sine and cosine are obtained from Eq. (11.44) and (11.45) as
c “
d
1
2 ` |α|
2β “
d
1
2 `
0
2p12.5q “ 1
?2
s “ ´ a13
2 β c “ ´ 41
2p41qp1{
?2q “ ´ 1
?2
Then, the rotation matrix U is constructed as
U “
»
—
—
–
1
?2
0 1
?2
0 10
´ 1
?2
0 1
?2
fi
ffi
ffi
fl
Subsequently, A1 is found as follows:
A1 “ UT A0U “
»
—
—
–
1
?2
0 ´ 1
?2
01 0
1
?2
0 1
?2
fi
ffi
ffi
fl
»
–
17 ´5 41
´5 53 ´5
41 ´5 17
fi
fl
»
—
—
–
1
?2
0 1
?2
0 10
´ 1
?2
0 1
?2
fi
ffi
ffi
fl
which leads to
A1 “
»
–
´24 0 0
0 53 ´5
?2
0 ´5
?2 58
fi
fl
Notice that a31 and a13 in A1 are zero. This rotation leads coincidentally to a21 “
a12 “ 0. Now, setting X0 “ I and employing the same rotation to X yields
X1 “ X0U “
»
—
—
–
1
?2
0 1
?2
0 10
´ 1
?2
0 1
?2
fi
ffi
ffi
fl672  Numerical Methods for Scientists and Engineers
Having completed the first sweep, we search for the location of the absolute
maximum of the off-diagonal element of A1, which is |a23| “ 5
?2; that is, p “ 2
and q “ 3. For this case, the required matrix elements are a22 “ 53, a33 “ 58, and
a23 “ ´5
?2. Then the plane rotation cosine, sine, rotation matrix, and A2 matrix
are obtained as
α “ a22 ´ a33
2 “ 53 ´ 58
2 “ ´5
2
β “
b
a2
23 ` α2 “
d
p´5
?
2q
2
`
ˆ
´5
2
˙2
“ 15
2
c “
d
1
2 ` |α|
2β “
d
1
2 `
5{2
2p15{2q “
c2
3
s “ ´ a23
2 β c “ ´ p´5
?2q
2p15{2qpa2{3q
“ 1
?3
Substituting these into U yields
U “
»
—
—
—
—
–
10 0
0
c2
3 ´ 1
?3
0 1
?3
c2
3
fi
ffi
ffi
ffi
ffi
fl
Then we find
A2 “ UT A1U “
»
–
´24 0 0
0 48 0
0 0 63
fi
fl
Employing the second rotation to X1 yields
X2 “ X1U “
»
—
—
—
—
–
1
?2
1
?6
1
?3
0
c2
3 ´ 1
?3
´ 1
?2
1
?6
1
?3
fi
ffi
ffi
ffi
ffi
fl
Discussion: In this example problem, we observed that after 2 plane rotations, the
matrix A becomes diagonal, containing the eigenvalues of ´24, 48, and 63. On the
other hand, the matrix X contains the associated normalized eigenvectors, each in
a separate column for λ “ ´24, 48, and 63, respectively.
It should be pointed out that non-iterative or direct methods require Opn3q oper￾ations for square matrices of order n. The direct methods for large n are practically
useless. On the other hand, the convergence rate of the Jacobi method is poor. The
number of rotations is typically in the order of Opn2q and each rotation requires
4n operations. The overall number of mathematical operations to zero out the non￾diagonal matrix elements is in the order of Opn3q. In general, other methods are
preferred due to the relatively slow convergence properties of the Jacobi method.Eigenvalues and Eigenvalue Problems  673
11.5 CHOLESKY DECOMPOSITION
The generalized eigenvalue problem of two symmetric A and B matrices has the following
form:
Ax “ λBx (11.49)
where A and B are of the same size, and for convenience, we will assume B is a positive
definite matrix, which can be defined mathematically in several ways, but, at this stage,
it is sufficient to note that all eigenvalues of a positive definite matrix are positive. It is
also worth noting that a diagonally dominant matrix with positive diagonal elements is also
positive-definite.
We may attempt to transform Eq. (11.49) simply into the standard eigenvalue problem.
For a nonsingular matrix B, obviously the transformation is
B´1Ax “ λx (11.50)
Yet even when both A and B matrices are symmetric, the product C “ B´1A will not
necessarily be symmetric. However, C is often desired to be symmetrical. For this reason,
we adopt a different approach to reach the standard eigenvalue problem.
We propose to decompose matrix B as follows:
B “ LLT (11.51)
where L is a lower triangular matrix and LT is its transpose.
Now, left-multiplying the rhs of Eq. (11.49) with L´1 and substituting Eq. (11.51) gives
λL´1pLLT qx “ λLT x (11.52)
Similarly, left-multiplying the lhs of Eq. (11.49) with L´1 and making use of the pL´1q
T “
pLT q
´1 identity yields
L´1ApL´TLT qx (11.53)
where pL´1q
T is denoted by L´T and L´TLT “ I.
Combining Eq. (11.52) and Eq. (11.53) results in
pL´1AL´T qpLT xq “ λLT x (11.54)
The matrix product L´1AL´T is now a symmetric matrix with the same eigenvalues as the
original eigenvalue problem. By setting C “ L´1AL´T and z “ LT x in Eq. (11.54), the
standard eigenvalue problem is obtained, i.e., Cz “ λz. Subsequently, the eigenvectors of
the original problem can be determined from x “ L´T z.
The procedure for determining a lower triangular matrix L is referred to as Cholesky
Decomposition and is covered in Chapter 2 (see Section 2.8). On the other hand, its inverse,
L´1, can be found as follows:
�
´1
ii “ 1
�ii
, �´1
ij “´ 1
�jj
ÿ
i
k“j`1
�
´1
ik �kj , for j “ pi´1q,..., 3, 2, 1, i“2, 3,...,n (11.55)
where �
´1
11 “ 1{�11 and �
´1
ij denotes the ith row and jth column element of L´1, i.e.,
�
´1
ij ‰ 1{�ij for i ‰ j.674  Numerical Methods for Scientists and Engineers
EXAMPLE 11.8: Application of Cholesky decomposition
Consider the following electric circuit:
where 2C2{3 “ C1 “ C3 “ C4 “ C5 “ C and L1 “ 2L2 “ L3 “ L4 “ L5 “ 2L6 “ L.
The transient current response of the circuit is described by the following coupled
second-order ODEs:
2
d2I1
dt2 ´ d2I2
dt2 ´ 1
2
d2I3
dt2 `
5
3LC I1 ´ 1
LC I2 ´ 2
3LC I3 “ 0
´ d2I1
dt2 ` 3
d2I2
dt2 ´ d2I3
dt2 ´ 1
LC I1 `
3
LC I2 ´ 1
LC I3 “ 0
´ 1
2
d2I1
dt2 ´ d2I2
dt2 `
5
2
d2I3
dt2 ´ 2
3LC I1 ´ 1
LC I2 `
8
3LC I3 “ 0
Assuming that the currents have the form Ii “ Ai sinpωt ` φq, where Ai is the
amplitude, ω is the resonance frequency, and φ is the phase shift, the set of ODEs
can be reduced to the following eigenvalue problem:
»
—
—
—
–
5
3 ´1 ´2
3
´1 3 ´1
´2
3 ´1 8
3
fi
ffi
ffi
ffi
fl
»
—
—
–
I1
I2
I3
fi
ffi
ffi
fl “ λ
»
—
—
—
–
2 ´1 ´1
2
´1 3 ´1
´1
2 ´1 5
2
fi
ffi
ffi
ffi
fl
»
—
—
–
I1
I2
I3
fi
ffi
ffi
fl
where λ “ ω2LC denotes the eigenvalues. Apply the Cholesky decomposition to the
problem to find the resonance frequencies and associated currents.
SOLUTION:
The problem is set up as a general eigenvalue problem. The first step is to trans￾form it into a standard eigenvalue problem: Cz “ λz. To achieve this, B is decom￾posed by using Cholesky’s method.
Starting with �11 “ ?b11 “ ?2 and following the decomposition steps leads to
�21 “ b21
�11
“ ´ 1
?2
�22 “ ab22 ´ �2
21 “
b
3 ´ p´1{
?2q
2 “
c5
2
�31 “ b31
�11
“ ´ 1
2
?2
, �32 “ 1
�22
pb32 ´ �31�21q“´1
2
c5
2
�33 “ ab33 ´ �2
31 ´ �2
32 “
?7
2Eigenvalues and Eigenvalue Problems  675
So, the lower triangular matrix L is found as
L “
»
—
—
—
—
–
?2 00
´ 1
?2
c5
2 0
´ 1
2
?2 ´1
2
c5
2
?7
2
fi
ffi
ffi
ffi
ffi
fl
Now, we employ the algorithm for inverting triangular matrices to find L´1:
�
´1
11 “ 1
�11
“ 1
?2
, �´1
22 “ 1
�22
“
c2
5
, �´1
33 “ 1
�33
“ 2
?7
�
´1
21 “ ´�
´1
22 �21
�11
“ ´p
a2{5qp´1{
?2q
?2 “ 1
?10
�
´1
32 “ ´�
´1
33 �32
�22
“ ´ 1
a5{2
´ 2
?7
¯´ ´ 1
2
c5
2
¯
“ 1
?7
�
´1
31 “ ´�
´1
32 �21 ` �
´1
33 �31
�11
“ ´ 1
?2
! 1
?7
´
´ 1
?2
¯
`
2
?7
´ 1
2
?2
)
“ 1
?7
In matrix form, we get
L´1 “
»
—
—
—
—
—
—
—
–
1
?2
0 0
1
?10 c2
5 0
1
?7
1
?7
2
?7
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Then, C is constructed as follows:
C “ L´1AL´T “
»
—
—
—
—
—
—
–
1
?2
0 0
1
?10 c2
5 0
1
?7
1
?7
2
?7
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
–
5
3 ´1 ´2
3
´1 3 ´1
´2
3 ´1 8
3
fi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
–
1
?2
1
?10
1
?7
0
c2
5
1
?7
0 0 2
?7
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
—
—
–
5
6 ´ 1
6
?5 ´1
3
c2
7
´ 1
6
?5
29
30 ´1
3
c 2
35
´1
3
c2
7 ´1
3
c 2
35
20
21
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Notice that matrix C is symmetric, which now allows us to apply the Jacobi method
to the standard Cz “ λz eigenvalue problem to compute its eigenpairs. (The imple￾mentation part of the Jacobi method is left as an exercise for the reader.)676  Numerical Methods for Scientists and Engineers
As a result, the eigenvalues of C are found to be λ1 “ 38{35, λ2 “ 1, and
λ3 “ 2{3. For resonance frequencies, this leads to ω “ a38{35LC, ω “ 1{
?
LC, and
ω “ a2{3LC. The associated eigenvectors are found as
z1 “
»
—
—
—
—
—
–
´1
3
c7
2
´1
3
c7
2
1
fi
ffi
ffi
ffi
ffi
ffi
fl
, z2 “
»
—
—
—
–
´ 1
?5
1
0
fi
ffi
ffi
ffi
fl , z3 “
»
—
—
—
—
–
5
?14
c 5
14
1
fi
ffi
ffi
ffi
ffi
fl
The eigenvectors of the original system are determined by xi “ L´T zi.
x1 “
»
—
—
—
—
—
–
1
?2
1
?10
1
?7
0
c2
5
1
?7
0 0 2
?7
fi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
–
´1
3
c7
2
´1
3
c7
2
1
fi
ffi
ffi
ffi
ffi
ffi
fl
“
»
—
—
—
—
—
–
´ 2
5
?7
8
15?7
2
?7
fi
ffi
ffi
ffi
ffi
ffi
fl
x2 “
»
—
—
—
—
—
–
1
?2
1
?10
1
?7
0
c2
5
1
?7
0 0 2
?7
fi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
–
´ 1
?5
1
0
fi
ffi
ffi
ffi
fl “
»
—
—
—
—
–
0
c2
5
0
fi
ffi
ffi
ffi
ffi
fl
x3 “
»
—
—
—
—
—
–
1
?2
1
?10
1
?7
0
c2
5
1
?7
0 0 2
?7
fi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
–
5
?14
c 5
14
1
fi
ffi
ffi
ffi
ffi
fl “
»
—
—
—
—
—
–
4
?7
2
?7
2
?7
fi
ffi
ffi
ffi
ffi
ffi
fl
Note that the computed eigenvalues (x’s) are not normalized. The eigenvectors are
normalized as vi “ xi{xi if needed.
Discussion: The transformation of a generalized eigenvalue problem results in a
lower triangular matrix, which is easy to obtain and invert. The procedure is also
computationally inexpensive.
A pseudomodule, INVERSE_L, computing the inverse of any lower triangular matrix is
presented in Pseudocode 11.4. The module requires a lower triangular matrix (L) and its
size (n) as input, and returns the inverse matrix, IL (its elements denoted by i�ij ), on exit.
The inversion algorithm presented by Eq. (11.55) is simple and easy to program. Having
evaluated i�11, the rest of the elements are obtained using the two For-loops: (1) the i-loop
sweeps rows from top (i “ 2) all the way down to the last row (i “ n), while the diagonal
elements are inverted according to i�ii “ 1{�ii; (2) the loop-j finds the elements of the ith
row from right to left using Eq. (11.55). An accumulator, For-loop over index k, is used to
perform the summation term that runs from k “ j ` 1 to k “ i.Eigenvalues and Eigenvalue Problems  677
Pseudocode 11.4
Module INVERSE_L ( n, L, IL)
\ DESCRIPTION: A pseudomodule to find the inverse of a lower
\ triangular matrix.
Declare: �nn, i�nn \ Declare array variables
i�11 “ 1{�11 \ Invert diagonal element
For “
i “ 2, n‰
i�ii “ 1{�ii \ Invert diagonal element
For “
j “ pi ´ 1q, 1,p´1q
‰
sums Ð 0 \ Initialize accumulator
For “
k “ pj ` 1q, i‰
sums Ð sums ` i�ik ˚ �kj \ Accumulate �
´1
ik �kj
End For
i�ij “ ´sums{�jj \ Find ři
k“j`1 �
´1
ik �kj {�jj
End For
End For
End Module INVERSE_L
Cholesky Decomposition
‚ Cholesky decomposition is a numerically stable and computationally
efficient method;
‚ Its memory requirements are considerably less;
‚ It is highly adaptive to parallel architectures; therefore, pivoting is
not required;
‚ It is faster than the LU decomposition as it exploits the symmetry of
the matrix;
‚ It can be used for solving linear equations with symmetric positive
and definite matrices; it requires half of the number of operations
required by Gaussian elimination.
‚ The method does not work for any matrix, except for symmetric
positive definite matrices.
11.6 HOUSEHOLDER METHOD
Finding the eigenvalues of especially dense matrices can be computationally very expensive.
Alternatively, the eigenvalues of a dense matrix are determined much more efficiently by
reducing it to a tridiagonal matrix through a series of similarity transformations to overcome
the large cpu requirements.
Suppose A is a well-conditioned symmetric matrix and has n eigenvalues with n linearly
independent eigenvectors. The main objective of the Householder method is to transform
a symmetric matrix into a symmetric tridiagonal matrix having identical eigenpairs. Note
that the Householder method does not give the eigenpairs directly; the analyst needs to
apply another method for computing the eigenpairs of a symmetric tridiagonal matrix as
well.678  Numerical Methods for Scientists and Engineers
In this method, orthogonal transformations are sequentially applied to zero out the per￾tinent row and column elements all at once, except for the tridiagonal elements. Moreover,
the elements that are zeroed out in prior steps are not altered in the subsequent operations,
unlike the Jacobi method. This algorithm, which is not an iterative one, requires n ´ 2
orthogonal transformations.
The transformations are expressed as
A1 “ A
A2 “ P1A1P1
.
.
.
An´1 “ Pn´2An´2Pn´2
where n is the size of A, and Pk is the transformation matrix at the kth step, defined as
P “ I ´ 2 vvT
where the matrix P, which is symmetric and orthogonal, is called the Householder matrix,
and v is a unit vector. Recalling that any vector u can be normalized to give a unit vector
(v “ u{|u|) in the direction of u, the Householder matrix can be constructed as
Pm “ I ´ umuT
m
pum, umq{2 (11.56)
where pu, uq “ |u|
2.
We construct the vector u, whose first (m ´ 1) elements as zero, that is,
um “
”
0, 0, ..., 0, upmq m , upm`1q m ,...,upnq m
ıT
or, in short,
u “ x ¯ |x| e1 (11.57)
where x is the lower m ´1 elements of the first column of the Householder matrix and e1 is
defined as the unit vector (i.e., the first column of the identity matrix of size m ´ 1). Then,
with Eq. (11.56), we define
A1 “ A, Am “ PT
mAm´1Pm, m “ 2, 3,...,n ´ 1 (11.58)
where Am is a partially tridiagonal up to row m:
Am “
»
—
—
—
—
—
—
—
—
—
–
a11 a1
12
a1
12 a1
22 a1
23
... ... ...
a1
mpm´1q a1
mm
a1
mpm`1q
a1
mpm`1q
An´m
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
The matrix Pm has the form
Pm “ I ´ 2vmvT
m “
»
—
—
—
–
Im 0 ... 0
0
.
.
. Pn´m
0
fi
ffi
ffi
ffi
flEigenvalues and Eigenvalue Problems  679
FIGURE 11.4: Depiction of the matrix operations in Householder method.
The objective of this transformation is to obtain u such that it satisfies |v| “ 1. Setting
u “ x ` S e1, where S “˘|x|, we find
S “ sgnpam`1,mq
gffe ÿn
i“m`1
a2
im (11.59)
and
d “ 1
2
pum, umq “ 1
2 px ` S e1, x ` S e1q “ SpS ` am`1,mq (11.60)
where sgn is the sign function.
We can now construct the elements of the vector um as follows:
uim “
$
&
%
0, if i “ 1, 2,...,m
aim ` S, if i “ m ` 1
aim, if i “ pm ` 2q,...,n
(11.61)
The orthogonal matrix P (for i, j “ pk ` 1q,pk ` 2q,...,pn ´ 1qn, it is embedded in matrix
T).
pij “ δij ´ uiuj
d , i, j “ pm ` 1q,pm ` 2q,...,pn ´ 1qn (11.62)
where δij is Kronecker delta, i.e., δii “ 1 and δij “ 0 if i ‰ j. It should be noted that the first
m rows and m columns remain unaffected by the similarity transformations. Additionally,
we can restrict the calculations to the elements in the upper triangle of Am since the
transformation preserves the symmetry.
Householder Method
‚ Householder method is fast, accurate, and cpu-time efficient;
‚ It always works for any nonsingular symmetric matrix;
‚ It is stable and does not require pivoting.
‚ Procedure is bandwidth-intensive heavy (diagonally dense) and not
parallelizable;
‚ Every reflection modifies the matrices P and A.680  Numerical Methods for Scientists and Engineers
Pseudocode 11.5
Module HOUSEHOLDER (n, A)
\ DESCRIPTION: A pseudomodule transforming a positive definite and symmetric
\ matrix into a symmetric tridiagonal matrix.
\ USES:
\ SIGN:: A built-in Function returning the sign of a real number;
\ MAT_MUL:: Module for computing matrix multiplication, Pseudocode 2.4.
Declare: ann, un, pnn, cnn
For “
k “ 1,pn ´ 2q
‰ \ Triangularize the first n ´ 2 rows
m Ð k ` 1 \ Find index of below diagonal element
S Ð 0 \ Initialize accumulator
For “
i “ m, n‰
S Ð S ` a2
ik \ Accumulate a2
ik for lower elements
End For
S Ð SIGNpamkq
?
S \ Find S, Eq. (11.51)
d Ð S ˚ pS ` amkq \ Find d, Eq. (11.52) )
u=0
um Ð amk ` S \ Set the first element of vector u
For “
i “ pm ` 1q, n ‰
ui Ð aik \ Set rest of the elements of u
End For
For “
i “ m, n‰
pki Ð 0; pik Ð 0 \ Construct transformation matrix T
End For
pkk Ð 1
For “
i “ m, n‰
For “
j “ m, n‰ \ Construct Householder matrix P
pij Ð 0
If “
i “ j
‰
Then
pij Ð 1 ´ ui ˚ uj {d \ Construct diagonal elements
Else
pij Ð ´ui ˚ uj {d \ Construct off-diagonal elements
End If
End For
End For
MAT_MUL(n, n, n, P, A, C) \ Utilize Pseudocode 2.4 to find C “ PA
MAT_MUL(n, n, n, C, P, A) \ Utilize Pseudocode 2.4 to find A “ CP
End For
End Module HOUSEHOLDER
A pseudomodule, HOUSEHOLDER, transforming a positive definite symmetric matrix
into a symmetric tridiagonal matrix is presented in Pseudocode 11.5. This module requires
a square matrix (A) of order n as input. On exit, the transformed (triangularized) matrix
is stored on A. The Householder process is depicted in Fig. 11.4. Using Eqs. (11.62), the
orthogonal matrix P1, a full matrix of order pn´1qˆpn´1q, is constructed. Then, another
orthogonal transformation is applied: A2 “ P1A1P1. This leads to the tridiagonalization
of the first row while modifying the rest of the elements in A. Next, an orthogonal matrix
P2, pn ´ 2qˆpn ´ 2q, is constructed, and then the transformation matrix A3 is obtained.
This procedure is repeated successively for n ´ 2 rows of A. The final transformed matrix
Am is a tridiagonal matrix. The original matrix A is destroyed during the process.Eigenvalues and Eigenvalue Problems  681
EXAMPLE 11.9: Application of Householder method
Use the Householder method to transform A into a tridiagonal matrix.
A “
»
—
—
–
61 2 2
12 3 1
2 3 11 2
21 2 3
fi
ffi
ffi
fl
SOLUTION:
First, we need to compute S to construct P; thus, we find
S “ sgnpa21q
b
a2
21 ` a2
31 ` a2
41 “ `a12 ` 22 ` 22 “ 3
The vector u is then constructed. Since a21 “ 1, we write
u “
»
–
1 ` 3
2
2
fi
fl
Then, uuT is found as
uuT “
»
–
4
2
2
fi
fl r422s “
»
–
16 8 8
8 44
8 44
fi
fl
Evaluating d “ SpS ` a21q “ 3p3 ` 1q “ 12 and substituting uuT into Eq. (11.56)
we get
I ´ 1
d
uuT “
»
–
100
010
001
fi
fl ´ 1
12
»
–
16 8 8
8 44
8 44
fi
fl “ ´1
3
»
–
12 2
2 ´2 1
2 1 ´2
fi
fl
Now, the first transformation matrix is found as
P1 “
»
—
—
—
—
—
—
—
–
10 0 0
0 ´1
3 ´2
3 ´2
3
0 ´2
3
2
3 ´1
3
0 ´2
3 ´1
3
2
3
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Then the orthogonal transformation (i.e., P1AP1 product) yields
A2 “ P1A1P1 “
»
—
—
–
6 ´30 0
´3 10 ´3 3
0 ´3 3 ´2
0 3 ´2 3
fi
ffi
ffi
fl
The transformation matrix P2 is constructed by repeating the procedure above.682  Numerical Methods for Scientists and Engineers
Noting that the first element of x (the first column of A2, which is a 3ˆ3 matrix) is
now a32, S and d are computed as
S “ sgnpa32q
b
a2
32 ` a2
42 “ p´1q
a32 ` 32 “ ´3
?
2
d “ SpS ` a32q “ 3
?
2p3
?
2 ` 3q “ 9p2 ` ?
2q
Next, u and uuT yield
u “
«
´3 ˘ 3
?2
3
ff
“
«
´3 ´ 3
?2
3
ff
uuT “
«
´3 ´ 3
?2
3
ff «´3 ´ 3
?2
3
ffT
“ 9
« 3 ` 2
?2 ´p1 ` ?2q
´p1 ` ?2q 1
ff
And lastly, matrix P is formed.
I ´ 1
d
uuT “
«
1 0
0 1ff
´ 9
18 ` 9
?2
« 3 ` 2
?2 ´p1 ` ?2q
´p1 ` ?2q 1
ff
“ 1
?2
«
´1 1
1 1 ff
Embedding this into matrix P2 yields
P2 “
»
—
—
—
—
—
—
—
–
10 0 0
01 0 0
0 0 ´ 1
?2
1
?2
0 0 1
?2
1
?2
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Finally, the triple product matrix P2A2P2 is found as
A3 “ P2A2P2 “
»
—
—
—
—
–
6 ´3 00
´3 10 3?2 0
0 3?250
0 0 01
fi
ffi
ffi
ffi
ffi
fl
In this example, the tridiagonal matrix coincidentally yielded a43 “ a34 “ 0.
Discussion: The Householder method reduces a full symmetric matrix into a sym￾metric tridiagonal matrix to be processed later on. This procedure is numerically
stable and does not require pivoting, which also makes it a candidate for solving
systems of linear equations. In this regard, the method is much more stable than the
Gauss elimination method; however, computationally, it is more expensive than the
Gaussian elimination method.Eigenvalues and Eigenvalue Problems  683
11.7 EIGENVALUES OF TRIDIAGONAL MATRICES
Computing the eigenvalues of a large symmetric matrix by conventional methods is not
desired due to the large memory requirement and the accumulation of round-off errors as a
result of numerous transformations, resulting in a consequently large cpu-time. In practice,
to circumvent these adverse effects, the original matrix is first reduced to a symmetric
tridiagonal form with Givens or Householder’s methods. The eigenvalues and/or eigenvectors
of a symmetric tridiagonal matrix are then determined using a suitable method, some of
which will be discussed in this section.
11.7.1 THE STURM SEQUENCE
Consider a tridiagonal symmetric matrix A, defined as
A “
»
—
—
—
—
—
–
d1 e1
e1 d2 e2
... ... ...
en´2 dn´1 en´1
en´1 dn
fi
ffi
ffi
ffi
ffi
ffi
fl
(11.63)
Furthermore, suppose ek ‰ 0 for k “ 1, 2,...,n, which ensures no repeating eigenvalues.
The eigenvalues of A are determined by finding the roots of the characteristic polyno￾mial:
detpA ´ λIq “ pnpλq “
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
d1 ´ λ e1
e1 d2 ´ λ e2
... ... ...
en´2 dn´1 ´ λ en´1
en´1 dn ´ λ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
“ 0 (11.64)
where pnpλq is a polynomial of degree n.
First, we consider a 2ˆ2 matrix whose characteristic polynomial is
p2pλq“pd1 ´ λqpd2 ´ λq ´ e2
1 “ 0 (11.65)
Defining p0pλq “ 1 and p1pλq “ d1 ´ λ, Eq. (11.65) can also be rewritten as
p2pλq“pd2 ´ λqp1pλq ´ e2
1p0pλq “ 0 (11.66)
For a 3ˆ3 matrix, we find
p3pλq “
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
d1 ´ λ e1 0
e1 d2 ´ λ e2
0 e2 d3 ´ λ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
“ pd3 ´ λqp2pλq ´ e2
3p1pλq “ 0 (11.67)
For increasing orders of a matrix, a recurrence relationship between the determinants
for 2 ď k ď n can be established as follows:
pkpλq“pdk ´ λqpk´1pλq ´ e2
k´1pk´2pλq “ 0 (11.68)
which is the characteristic polynomial of a symmetric tridiagonal matrix. It can be shown684  Numerical Methods for Scientists and Engineers
Sturm Sequence Method
‚ The method is simple enough to not require elaborate matrix algebra;
‚ It is relatively easy to implement;
‚ It can be used to estimate a specified eigenvalue (smallest, largest, or
λ “ α in λa ă α ă λb);
‚ Once an eigenvalue (i.e., root) is bracketed, the bisection method is
certain to converge to an approximate value.
‚ The method is not suitable for very large matrices;
‚ It is restricted to symmetric tridiagonal matrices only;
‚ It provides the eigenvalues only (not the eigenvectors);
‚ It is susceptible to round-off errors even for fairly large matrices;
‚ Narrowing down the roots with bisections can be expensive for a
widely or tightly dispersed roots due to its linear convergence rate.
that the sequence, p0pλq, p1pλq, p2pλq, ...,pnpλq, also referred to as the Sturm sequence
(provided that ek ‰ 0), can be used to determine the root of pnpλq in any given interval.
Sturm’s method is applied to a symmetric tridiagonal matrix to obtain its eigenvalues
from its characteristic equation. The procedure involves selecting trial λ values and deter￾mining the number of sign changes to narrow down or update the upper and lower bounds
for the roots. For example, let the Sturm-sequence and the corresponding sequence of signs
for λ “ α be given as follows:
tp0pαq, p1pαq, p2pαq, ¨¨¨ , pnpαqu Ñ t`, ´, `, ¨¨¨ , `u
We will denote the number of sign changes up to λ “ α by Npαq; for instance, Npαq “ M
indicates that pnpλq has M roots on ´8 ă λ ă α. This gives the number of distinct
eigenvalues of a tridiagonal matrix lying between any two numbers. If we let the number
of sign changes corresponding to λ1 and λ2 be Npλ1q “ N1 and Npλ2q “ N2, then the
number of roots within pλ1, λ2q can be found as Npλ2q ´ Npλ1q “ N2 ´ N1. Recalling the
Gerschgorin theorem, the lower and upper bounds of the roots of pnpλq may be found from
λmin “ min
1ďkďn pdk ´ |ek´1|´|ek|q, λmax “ max
1ďkďn pdk ` |ek´1|`|ek|q (11.69)
where e0 “ en “ 0.
First, the upper and lower bounds covering all eigenvalues are determined, i.e.,
pλmin, λmaxq. Then, individual roots are isolated by further dividing the pλmin, λmaxq in￾terval into smaller pλk`1, λkq subintervals (panels) such that each root is contained in a
panel, i.e., Npλk`1q ´ Npλkq “ 1. Then, the bisection method is employed to narrow down
the root to the desired accuracy.
As a word of caution, locating the roots (even if they are isolated)
with the Newton-Raphson method is too risky in that a root es￾timate may fall outside of the isolated interval and converge to a
root in another interval. However, with the bisection method, the
successive estimates are guaranteed to remain within the brack￾eted interval.Eigenvalues and Eigenvalue Problems  685
EXAMPLE 11.10: Isolating the eigenvalues using the Sturm sequence
Isolate the eigenvalues of the following matrix:
A “
»
—
—
–
2 ´100
´1 1 ´1 0
0 ´112
0 0 22
fi
ffi
ffi
fl
SOLUTION:
Let us generate the Sturm sequence of matrix A using Eq. (11.65)–(11.68). Start￾ing with p0pλq “ 1, the Sturm sequence yields
p1pλq“p2 ´ λq
p2pλq“p1 ´ λqp1pλq ´ p0pλq “ λ2 ´ 3λ ` 1
p3pλq“p1 ´ λqp2pλq ´ 4p1pλq“´λ3 ` 4λ2 ´ 3λ ´ 1
p4pλq“p2 ´ λqp3pλq ´ p2pλq “ λ4 ´ 6λ3 ` 7λ2 ` 7λ ´ 6
Using Eq. (11.69), the minimum and maximum values of each row are determined
to be (1,3), (´1,3), (´2,4), and (0,4), respectively. Hence, we find out that all roots
of the characteristic polynomial are contained within (´2,4).
The next step is to isolate each root in this interval. For this purpose, we will
count the sign changes in the Sturm sequences to isolate individual roots. Table 11.1
depicts the Sturm sequence for various λ’s within ´2 ď λ ď 4. Note that for λ “ ´2,
the sequence does not depict any sign change, i.e., Np´2q “ 0 indicates no root in
(´8, ´2]. For λ “ 0, the sequence changes sign at p3, resulting in Np0q “ 1, which
implies the presence of a single root in (´2,0) (Np0q ´ Np´2q “ 1). For λ “ 1,
which yields Np1q “ 2, the Sturm sequence exhibits two sign changes, one at p2 and
the other at p4. The number of sign changes between (´2,1) and (0,1) intervals is
Np1q ´ Np´2q “ 2 and Np1q ´ Np0q “ 1, respectively. For λ “ 3, we observe three
sign changes on (´8,3], i.e., Np3q “ 3. The number of sign changes in (´2,3) and
(1,3) is found by Np3q ´ Np´2q “ 3 and Np3q ´ Np1q “ 1, respectively. Lastly, for
λ “ 4, we obtain Np4q “ 4, indicating that the last root is in (3,4).
TABLE 11.1
λ p0pλq p1pλq p2pλq p3pλq p4pλq Explanation
´2 1 4 11 29 72 No sign change; No eigenvalue
smaller than ´2
01 2 1 ´1 ´6 1 sign change in (´2,0); 1 eigen￾value in (´2,0) interval
11 1 ´1 ´1 3 2 sign changes in (´2,0); 2 eigen￾values. Second one is in (0,1).
3 1 ´1 1 ´1 ´3 3 sign changes in (´2,0); 3 eigen￾values. Third one is in (1,3).
4 1 ´2 5 ´13 6 4 sign changes in (´2,0); the last
one is in (3,4).
Discussion: This technique can be applied not only to find the roots (eigenvalues)
of the characteristic polynomial but also to isolate and find the real roots of any real
polynomial.686  Numerical Methods for Scientists and Engineers
11.7.2 THE QR ITERATION
Finding the eigenvalues via the Sturm sequence can be impractical when a tridiagonal
matrix is very large. Hence, a tridiagonal symmetric matrix is usually reduced to a diagonal
matrix by employing the method of QL or QR decomposition. The goal is to decompose the
matrix into the product of an orthogonal matrix (denoted by Q) and a lower (L) or upper
(R) triangular matrix.
Consider the following symmetric tridiagonal matrix:
A “
»
—
—
—
—
—
—
—
–
d1 e1
e1 d2 e2
e2 d3 e3
...
en´2 dn´1 en´1
en´1 dn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(11.70)
If e1 or en´1 is zero, then one of the eigenvalues of A is d1 or dn, respectively. The QR
iteration method exploits this property of tridiagonal matrices by successively eliminating
the sub-diagonal elements using the following two steps:
Decomposition of A as Ap “ QpRp
Construction of Ap`1 as Ap`1 “ RpQp
*
for p “ 0, 1, 2,...
where A0 “ A is the original symmetric tridiagonal matrix, Qp is an orthogonal matrix,
and Rp is an upper triangular matrix.
The objective of this numerical method is to apply a series of orthogonal transforma￾tions to obtain a sequence of similar matrices Ap (p “ 1, 2,...) that, for sufficiently large p
(p Ñ 8), eventually converge to a diagonal matrix (A8) whose diagonal elements are the
eigenvalues of A. In this process, after each similarity transformation, not only the tridiag￾onal structure and the symmetry property but also the eigenvalues of A are preserved.
A decomposed matrix is expressed by
Ap “
»
—
—
—
—
—
—
–
dppq
1 e
ppq
1
e
ppq
1 dppq
2 e
ppq
2
... ... ...
e
ppq
n´2 dppq
n´1 e
ppq
n´1
e
ppq
n´1 dppq n
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(11.71)
where A0 is the original matrix and the matrices with p ą 1 denote degenerated or modified
matrices, dppq
k and e
ppq
k denote the modified matrix elements at the pth step.
We begin by setting A1 “ A as the product of an orthogonal (Q1) and an upper
triangular matrix (R1): Q1R1. Then A2 “ R1Q1 is constructed. To generalize, we define
two matrix products: Ap “ QpRp and Ap`1 “ RpQp. Since Qp is orthogonal (i.e., Q´1 p “
QT
p and Q´1 p Q “ QT
p Q “ I) and making use of Rp “ QT
p Ap, we find
Ap`1 “ RpQp “ QT
p ApQp (11.72)
This relationship clearly demonstrates that Ap`1 is symmetric and shares the same eigen￾values as Ap and A. By defining proper Rp and Qp, Ap`1 is assured to be tridiagonal.Eigenvalues and Eigenvalue Problems  687
The essence of the QR iteration method is the computation of the QR decomposition
of Ap. Due to the symmetry, we deal with the diagonal (dk) and sub-diagonal (ek) elements
only. The decomposition is carried out by a so-called rotation matrix. For this purpose, we
define an orthogonal matrix, Ppk,k`1q, which is basically an identity matrix whose pk ´1qth
and kth rows and columns are modified for an arbitrary rotation angle θ as follows:
Ppk,k`1q “
»
ck´1 ck
—
—
—
—
—
—
—
—
—
—
—
—
–
1
...
1
cos θ sin θ
´ sin θ cos θ
1
...
1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
rk´1
rk
(11.73)
This matrix represents the rotation of the pk ´ 1qth and kth axes about the origin of
the coordinate system by the angle θ. The right-multiplication of Ppk,k`1q with an arbitrary
M (i.e., Ppk,k`1qM) affects only the pk ´1q and kth rows and columns. From this point on,
for notational convenience, as we have done so in the Householder method, we will denote
the rotation sine and cosines with s and c, respectively.
The decomposition of matrix Ak is handled in the usual manner. For a tridiagonal
matrix of order n, the sub-diagonal elements are zeroed with n ´ 1 rotations. In order
to illustrate the QR iteration steps and develop a general algorithm, a 4 ˆ 4 symmetric
tridiagonal matrix A is considered:
A “
»
—
—
–
d1 e1
e1 d2 e2
e2 d3 e3
e3 d4
fi
ffi
ffi
fl
For the first rotation, Pp1,2q is chosen such that Pp1,2qAp yields zero at the second row, first
column. The product Pp1,2qA0 yields
Pp1,2qA0 “
»
—
—
–
c1 s1
´s1 c1
1
1
fi
ffi
ffi
fl
»
—
—
–
d1 e1
e1 d2 e2
e2 d3 e4
e3 d4
fi
ffi
ffi
fl
“
»
—
—
–
c1d1 ` e1s1 c1e1 ` d2s1 e2s1
c1e1 ´ d1s1 c1d2 ´ e1s1 c1e2
e2 d3 e4
e3 d4
fi
ffi
ffi
fl
(11.74)
The second row first column element is set to zero, i.e., c1e1 ´ d1s1 “ 0. Provided that
c2
1 ` s2
1 “ 1, the rotation cosine and sine are easily obtained (by solving the two equations
with two unknowns) as:
ρp1q “
b
e2
1 ` d2
1, c1 “ d1{ρp1q
, s1 “ e1{ρp1q (11.75)688  Numerical Methods for Scientists and Engineers
Equation (11.74) now takes the following form:
Pp1,2qA0 “
»
—
—
–
ρp1q pd1`d2qe1{ρp1q e1e2{ρp1q
0 pd1d2´e2
1q{ρp1q d1e2{ρp1q
e2 d3 e3
e3 d4
fi
ffi
ffi
fl“
»
—
—
–
dp1q
1 e
p1q
1 f1
0 dp1q
2 e
p1q
2
e2 d3 e3
e3 d4
fi
ffi
ffi
fl
(11.76)
where dp1q
1 “ ρp1q, dp1q
2 “ pd1d2 ´ e2
1q{ρp1q
, f1 “ e1e2{ρp1q, and e
p1q
2 “ d1e2{ρp1q
, where the
superscripts denote degenerated elements.
The second rotation Pp2,3q is chosen such that Pp2,3qPp1,2qAp gives zero at the third
row, second column. The matrix product Pp2,3qPp1,2qAp can be explicitly written as
Pp2,3qPp1,2qAp “
»
—
—
–
1
c2 s2
´s2 c2
1
fi
ffi
ffi
fl
Pp1,2qAp
“
»
—
—
—
–
dp1q
1 e
p1q
1 f1
0 e2s2 ` c2dp1q
2 d3s2 ` c2e
p1q
2 s2e3
e2c2 ´ s2dp1q
2 d3c2 ´ s2e
p1q
2 c2e3
e3 d4
fi
ffi
ffi
ffi
fl
(11.77)
By setting c2e2 ´ s2dp1q
2 “ 0 and making use of c2
2 ` s2
2 “ 1, the cosine and sines for the
second rotation can be found as
ρp2q “
b
e2
2 ` pdp1q
2 q
2
, c2 “ dp1q
2 {ρp2q
, s2 “ e2{ρp2q (11.78)
Equation 11.77 now becomes
Pp2,3qPp1,2qAp “
»
—
—
—
–
dp1q
1 e
p1q
1 f1
0 ρp2q pd3e2 ` dp1q
2 e
p1q
2 q{ρp2q e3e2{ρp2q
0 pd3dp1q
2 ´ e2e
p1q
2 q{ρp2q e3dp1q
2 {ρp2q
e3 d4
fi
ffi
ffi
ffi
fl
“
»
—
—
—
–
dp1q
1 e
p1q
1 f1
0 dp2q
2 e
p2q
2 f2
0 dp1q
3 e
p1q
3
e3 d4
fi
ffi
ffi
ffi
fl
(11.79)
where dp2q
1 , e
p2q
2 , and f2 have conformable definitions.
The third rotation, Pp3,4q, is chosen as Pp3,4qPp2,3qPp1,2qAp so that the fourth row, the
third column, is zero. The Pp3,4qPp2,3qPp1,2qAp is obtained as
Pp3,4qPp2,3qPp1,2qAp “
»
—
—
–
1
1
c3 s3
´s3 c3
fi
ffi
ffi
fl
Pp2,3qPp1,2qAp
“
»
—
—
—
–
dp1q
1 e
p1q
1 f1
0 dp2q
2 e
p2q
2 f2
0 e3s3 ` c3dp1q
3 d3s3 ` c3e
p1q
3
e3c3 ´ s3dp1q
3 d3c3 ´ s3e
p1q
3
fi
ffi
ffi
ffi
fl
(11.80)Eigenvalues and Eigenvalue Problems  689
Similarly, setting c3e3 ´ s3dp1q
3 “ 0 and using c2
3 ` s2
3 “ 1, the cosine and sines for the third
rotation are obtained as
ρp3q “
b
e2
3 ` pdp1q
3 q
2
, c3 “ dp1q
3 {ρp3q
, s3 “ e3{ρp3q (11.81)
Finally, the first sweep yields
Pp3,4qPp2,3qPp1,2qAp “
»
—
—
—
–
dp1q
1 e
p1q
1 f1
0 dp2q
2 e
p2q
2 f2
0 ρp3q pd3e3 ` dp1q
3 e
p1q
3 q{ρp3q
0 pd3dp1q
3 ´ e3e
p1q
3 q{ρp3q
fi
ffi
ffi
ffi
fl
“
»
—
—
—
–
dp1q
1 e
p1q
1 f1
0 dp3q
2 e
p2q
2 f2
0 dp2q
3 e
p2q
3
0 dp1q
4
fi
ffi
ffi
ffi
fl
(11.82)
where dp3q
2 , e
p2q
2 , and f2 are conformably defined.
Note that in the preceding rotations, the matrix elements labeled as f1, f2, ..., and so
on were not used in decomposition; hence, these quantities do not need to be evaluated or
saved. Moreover, they are not required for the product Ap`1 “ RpQp. In fact, by ignoring
fk’s, we do not obtain a full QR decomposition of A but instead retain sufficient information
needed to compute RpQp.
The QR decomposition step of this method can be generalized as follows:
t Ð e1,
ρ Ð at2 ` d2
k,
ck Ð dk{ρk, sk Ð t{ρk,
dk Ð ρ, t Ð ek,
ek Ð tck ` dk`1sk,
dk`1 Ð ´tsk ` dk`1ck
,
//////////.
//////////-
k“1, 2,...,pn´1q (11.83)
and
t Ð ek, ek`1 Ð tck
)
k ‰ pn ´ 1q (11.84)
where the similarity transformation steps are ordered in such a way that memory allocation
is kept to a minimum.
Equations (11.83) and (11.84) generate the upper triangular matrix R of the QR￾decomposition of A, which can be defined for a symmetric triangular matrix of order n
as
Rp “ Ppn´1,nq ¨¨¨ Pp2,3qPp1,2qAp
Noting that Rp “ QT
p Ap, we find QT
p “ Ppn´1,nq ¨¨¨ Pp2,3qPp1,2q. Recalling pABq
T “
BT AT properties of the transpose, this result can be extended by induction to a general
case of multiple matrices:
Qp “ PT
p1,2qPT
p2,3q ¨¨¨ PT
pn´2,n´1qPT
pn´1,nq (11.85)690  Numerical Methods for Scientists and Engineers
At this point, it is necessary to emphasize that there is no need to compute Qp explicitly
to construct RpQp. After each rotation, ck and sk’s can be multiplied.
»
—
—
–
d1 e1 f1
0 d2 e2 f2
0 d3 e3
0 d4
fi
ffi
ffi
fl
»
—
—
–
c1 ´s1
s1 c1
1
1
fi
ffi
ffi
fl “
»
—
—
–
dp1q
1 eˆ1 f1
e
p1q
1 dp1q
2 e2 f2
0 d3 e3
0 d4
fi
ffi
ffi
fl
(11.86)
where dp1q
1 Ð c1d1 ` e1s1, e
p1q
1 Ð d2s1, e
p1q
1 Ð c1e1 ´ d1s1 and dp1q
2 Ð dp1q
2 c1.
The next step gives
»
—
—
—
–
dp1q
1 eˆ1 ˆf1
e
p1q
1 dp1q
2 e2 f2
0 d3 e3
d4
fi
ffi
ffi
ffi
fl
»
—
—
–
1
c2 ´s2
s2 c2
1
fi
ffi
ffi
fl “
»
—
—
—
–
dp1q
1 c2eˆ1 ` f1s2 c2f1 ´ eˆ1s2
e
p1q
1 dp2q
2 eˆ2 f2
e
p1q
2 dp1q
3 e3
d4
fi
ffi
ffi
ffi
fl
(11.87)
where similarly dp2q
2 Ð c2dp1q
2 ` e2s2, e
p1q
2 Ð d3s2 and dp1q
3 Ð c2d3, and so on.
Now that the tridiagonal matrix is symmetric and Qp is orthogonal, RpQp is also
symmetric. As a result, the elements of the product matrix corresponding to super-diagonal
positions do not need to be computed. Finally, we deduce the following steps to construct
the Ap`1 matrix:
dk Ð dkck ` eksk, ek Ð dk`1sk, dk`1 Ð dk`1ck, k “ 1, 2,...,pn´1q (11.88)
With this procedure, the eigenvectors can also be simultaneously obtained. Let V be a
matrix of eigenvectors, with the jth column being the associated eigenvector of λj . To
compute V, it is initialized to the identity matrix (V0 “ I) before the QR iterations. Once
Ap is decomposed, the matrix V is improved upon by
Vp`1 “ VpPT
p1,2qPT
p2,3q ¨¨¨ PT
pn´2,n´1qPT
pn´1,nq (11.89)
The eigenvector computations are generalized as
q Ð vik,
r Ð vi,k`1,
vik Ð ckq ` skr,
vi,k`1 Ð ´skq ` ckr
,
////.
////-
, for k“1, 2,...,pn´1q and i“1, 2,...,n (11.90)
where vik denotes the elements of the matrix of eigenvectors, and q and r are temporary
variables used to overwrite the updates on vik and vi,k`1, respectively.
The QR iteration procedure is repeated until the sub-diagonal elements of Ap (i.e.,
e
ppq
k ) are sufficiently small. A criterion for convergence is based on either the relative or
absolute error of the eigenvalues or �2-norm of the sub-diagonal elements:
L2 “
gffen
ÿ´1
k“1
e2
k ă ε (11.91)
Consider a real symmetric tridiagonal matrix A satisfying |λ1| ą |λ2| ą |λ3| ą ¨¨¨ ąEigenvalues and Eigenvalue Problems  691
QR Iteration Method
‚ The method presents a simple, elegant algorithm for finding the eigen￾vectors and eigenvalues of a real, symmetric, full-rank matrix;
‚ It gives the complete set of all eigenpairs (i.e., the orthogonal matrix
Qp, whose columns approach eigenvectors of A, and a diagonal ma￾trix Ap), whose diagonal elements approach the eigenvalues of A);
‚ It can be used as a direct solver for the system of linear equations;
‚ It has been improved by variants having quadratic convergence rates;
‚ The basic QR method has a linear convergence rate;
‚ It may fail when no real eigenvalues exist or when the eigenvectors
do not form an orthogonal basis;
‚ It requires excessive memory for large sparse matrices;
‚ Its convergence can be slow if the eigenvalues are close to each other.
|λn| ą 0. The sequence of iterates tApu will converge to a diagonal matrix that contains
the eigenvalues tλku in the diagonal position, ordered in descending order. During the
QR iterations on matrix Ap, the diagonal elements dppq
k approach the eigenvalues. The
convergence criterion given by Eq. (11.91) is monitored at each step, i.e., e
ppq
k Ñ 0 for
p Ñ 8. The rate of convergence is linear, with a convergence ratio of |λk{λk´1|. The basic
QR algorithm presented here converges slowly enough to be competitive, especially when
the eigenvalues are clustered together. To accelerate the convergence of the QR method,
several variants with a second-order convergence rate have been developed. In this section,
we will suffice to explain the basic QR algorithm in detail.
The QR algorithm consists of two separate stages. First, the original matrix is trans￾formed through similarity transformations into a tridiagonal form, which is the preparation
stage for the second stage. The QR iterations are, in fact, applied to a tridiagonal matrix.
The number of floating points in the method is Opn3q.
EXAMPLE 11.11: Application of QR iteration method
Compute the eigenvalues of the following matrix using the QR iteration method.
A “
»
—
—
–
2 ´100
´1 1 ´1 0
0 ´112
0 0 22
fi
ffi
ffi
fl
SOLUTION:
The diagonal and super-diagonal elements are d1 “ d4 “ 2, d2 “ d3 “ 1, e1 “
e2 “ ´1, and e3 “ 2. The sub-diagonal elements will be zeroed out by row rotations.
Note that the �2-norm of the sub-diagonal elements is initially aΣe2
k “ ?6.
We apply Eqs. (11.83) and (11.84) for the row rotations. Setting t “ ´1, the first692  Numerical Methods for Scientists and Engineers
rotation (k “ 1) leads to
ρp1q “
b
t2 ` d2
1 “ ?
5, c1 “ d1
ρp1q “ 2
?5
s1 “ t
ρp1q “ ´ 1
?5
and we set d1 “ ρp1q “ ?5.
Next, assigning t “ e1 “ ´1 we calculate
e1 “ tc1 ` d2s1 “ ´ 3
?5
, d2 “ ´ts1 ` d2c1 “ 1
?5
Since k ‰ 3, we set t “ e2 “ ´1 to find e2 “ tc1 “ ´2{
?5.
The second rotation (k “ 2) with t “ ´1 results in
ρp2q “
b
t2 ` d2
2 “
c6
5
, c2 “ d2
ρp2q “ 1
?6
, s2 “ t
ρp2q “ ´c5
6
and we set d2 “ ρp2q “ a6{5.
Setting t “ e2 “ ´2{
?5, we find
e2 “ tc2 ` d3s2 “ ´ 7
?30, d3 “ ´ts2 ` d3c2 “ ´ 1
?6
Again, since k ‰ 3, setting t “ e3 “ 2, we can then compute e3 “ tc2 “ a2{3. In
the third rotation (k “ 3) and t “ 2, we find
ρp3q “
b
t2 ` d2
3 “ 5
?6
, c3 “ d3
ρp3q “ ´1
5
, s3 “ t
ρp3q “ 2
?6
5
and we set d3 “ ρp3q “ 5{
?6. By t “ e3 “ a2{3, we find
e3 “ tc3 ` d4s3 “ 11
5
c2
3
, d4 “ ´ts3 ` d4c3 “ ´6
5
We use Eq. (11.88), for k “ 1
d1 “ d1c1 ` e1s1 “ 13
5 , e1 “ d2s1 “ 2
?6
5
For k “ 2, we find
d2 “ d2c2 ` e2s2 “ 47
30, e2 “ d3s2 “ ´5
?6
6 , d3 “ d3c2 “ 5
6
Finally, for k “ 3, we have
d3 “ d3c3 ` e3s3 “ 239
150, e3 “ d4s3 “ 6
25
Using ε “ 10´5, the QR method converges to the following eigenvalues after 36
iterations:
d1 “ λ1 “ 3.747964, d2 “ λ2 “ 2.570181,
d3 “ λ3 “ ´0.964160, d4 “ λ4 “ 0.646016
Discussion: The method converges to the approximate eigenvalues in 36 iterations.
The �2-norm of A2 for the 1st to 6th iterations are 2.257127, 1.298705, 0.794264,
0.700586, and 0.625785, respectively.Eigenvalues and Eigenvalue Problems  693
FIGURE 11.5
The 10-based logarithm of the �2-norm, log10pLppq
2 q, is plotted as a function of the
number of iterations in Fig. 11.5. After several iteration steps, the variation of the
log-�2-norm settles into a linear form, from which the convergence rate is determined
to be 0.164.
Pseudocode 11.6
Module BASIC_QR (n, d, e, ε, maxit, V)
\ DESCRIPTION: A module to find eigenvalues of a symmetric tridiagonal matrix.
\ USES:
\ SQRT:: A built-in function computing the square-root of a value.
Declare: dn, en, cn, sn, vn,n
err Ð 1 \ Initialize error
p Ð 0 \ Initialize iteration counter
V Ð 0 \ Initialize matrix of eigenvectors
For “
i “ 1, n‰
vii Ð 1 \ Initialize diagonals, vii Ð 1
End For
While “
err ą ε And p ă maxit‰ \ Iteration loop
t Ð e1 \ Start decomposition steps
For “
k “ 1, n ´ 1
‰ \ Decomposition steps
ρ Ð ad2
k ` t2
ck Ð dk{ρ
sk Ð t{ρ
dk Ð ρ
t Ð ek
ek Ð t ˚ ck ` dk`1 ˚ sk
dk`1 Ð ´t ˚ sk ` dk`1 ˚ ck
If “
k ‰ n ´ 1
‰
Then \ The last step does not contain en
t Ð ek`1
ek`1 Ð t ˚ ck694  Numerical Methods for Scientists and Engineers
End If
For “
i “ 1, n‰
q Ð vik
r Ð vi,k`1
vik Ð ck ˚ q ` sk ˚ r
vi,k`1 Ð ´sk ˚ q ` ck ˚ r
End For
End For \ End of QR-decomposition loop
For “
k “ 1, n ´ 1
‰ \ Obtain RkQk product
dk Ð dk ˚ ck ` ek ˚ sk
t Ð dk`1
ek Ð t ˚ sk
dk`1 Ð t ˚ ck
End For
err Ð 0 \ Initialize accumulator for error
For “
k “ 1, n ´ 1
‰
err Ð err ` e2
k \ Accumulate e2
k
End For
err Ð ?err \ Error computed as err “
b
Σn´1
k“1 e2
k
p Ð p ` 1 \ Count iterations
Write: “Iteration=”, p,“Error=”,err \ Print out the iteration progress
End While
If “
p “ maxit‰
Then \ Upper bound reached with no convergence
Write: “Iteration did not converge, error is”, err \ Print a warning
End If
End Module BASIC_QR
A pseudomodule, BASIC_QR, finding all eigenvalues of a symmetric tridiagonal matrix
is presented in Pseudocode 11.6. As input, the module requires the order of the tridiagonal
matrix (n), its diagonal (d) and sub-diagonal elements (e), a convergence tolerance (ε), and
an upper bound for the number of iterations (maxit). On exit, the estimated eigenvalues
are stored on the diagonal array d. In this procedure, the convergence decision is based on
�2 norm of the sub-diagonal elements (err), which should approach zero after a sufficient
number of transformations. The module processes n´1 plane rotations to zero out the sub￾diagonal elements. The last rotation, Eq. (11.84), is given with a conditional statement since
there is no en or en`1 to overwrite. If the eigenvector is to be computed, the construction
of matrix Vp is realized by updating the matrix after each rotation using Eq. (11.90). The
second loop involves the RQ product. Finally, the iterative process is terminated when the
convergence criterion is satisfied or if the iterations reach the maxit.
11.7.3 CLASSIC GRAM-SCHMIDT PROCESS
Gram-Schmidt QR decomposition is used to find an orthogonal basis from a non-orthogonal
basis. Recall that an orthogonal matrix has row and column vectors of unit length, and the
orthonormal basis simplifies the computations by having vi ¨ vi “ 1 and vi ¨ vj “ 0. In this
regard, an orthogonal basis has many advantageous properties that are desirable for the
QR decomposition.Eigenvalues and Eigenvalue Problems  695
We begin with n independent vectors t1, t2, ..., tn, i.e., the column vectors of a matrix
as
T “ rt1 t2 t3 ¨¨¨ tns “
»
—
—
—
—
—
—
—
—
—
—
—
—
—
–
d1 e1 0 00
e1 d2 e2 0 0
0 e2 d3
.
.
. .
.
.
0 0 e3 ¨¨¨ 0 0
.
.
. .
.
. 0 en´2 0
0 0 .
.
. dn´1 en´1
000 en´1 dn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(11.92)
The goal of this process is to produce n orthonormal vectors q1, q2, ..., qn (i.e., the
column vectors of an orthogonal matrix Q). However, matrix Q has the upper Hessenberg
matrix form (i.e., consists of an nonzero upper triangular matrix and the first sub-diagonal):
Q “ rq1 q2 q3 ¨¨¨ qns “
»
—
—
—
—
—
—
—
—
—
—
—
—
—
–
q11 q12 q13 q1pn´1q q1n
q21 q22 q23 q2pn´1q q2n
0 q23 q33 q3pn´1q q3n
0 0 q43 ¨¨¨ q4pn´1q q4n
.
.
. .
.
. 0 .
.
. .
.
.
0 0 .
.
. qpn´1qpn´1q qpn´1qn
000 qnpn´1q qnn
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(11.93)
In the classic Gram-Schmidt algorithm, we start with ti and subtract off its projections
onto the previous q’s to find the orthonormal qi, and then divide by the length of the vector
u to obtain a unit vector as follows:
u1 “ t1, q1 “ u1
u1
u2 “ t2 ´ r12 q1, q2 “ u2
u1
u3 “ t3 ´ r13 q1 ´ r23 q2, q3 “ u3
u1
... ...
uk “ tk ´ k
ř´1
j“1
rjkqj , qk “ uk
u1
(11.94)
where rjk denotes the inner product defined as
rjk “ ptk, qj q “
»
—
—
—
—
—
—
—
—
—
—
–
.
.
.
0
ek´1
dk
ek
0
.
.
.
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
T
.
»
—
—
—
—
—
—
—
—
—
–
q1j
q2j
.
.
.
qjj
qpj`1qj
0
0
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
and rkk “ ptk, qkq “ uk “
gffeÿn
i“1
u2
ik (11.95)696  Numerical Methods for Scientists and Engineers
Gram-Schmid Method
‚ The method is simple and easy to implement;
‚ It is not an iterative method, i.e., Q and R are found in n steps;
‚ Unlike the QR Iteration method, a complete Q and R decomposition
is performed; hence, the method can be employed to solve a linear
system as well.
‚ The process is inherently numerically unstable;
‚ It is sensitive to round-off errors; as a result, for very large matrices,
the orthogonality can be partially or completely lost, requiring the
computation of qi’s to the machine epsilon level;
‚ It gives only the eigenvalues; another method should be utilized if
eigenvectors are also needed.
The inner product rjk “ ptk, qj q is used in constructing the upper triangular matrix
R (since ptk, qj q “ 0 for j ą k) as follows:
R “
»
—
—
—
—
—
—
–
u1 pt2, q1q pt3, q1q ¨¨¨ ptn, q1q
0 u2 pt3, q2q ¨¨¨ ptn, q2q
.
.
. 0 u3 ¨¨¨ ptn, q3q
.
.
. .
.
. .
.
. ... .
.
.
0 0 ¨¨¨ 0 un
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(11.96)
A pseudomodule, GRAM_SCHMIDT, decomposing a symmetric tridiagonal matrix using
the classical Gram-Schmidt process is presented in Pseudocode 11.7. The module requires
the order of the matrix (n), a tridiagonal matrix, by its diagonal (d) and sub-diagonal (e)
elements as input. On exit, the module returns an orthogonal (Q) and upper triangular
matrix (R). The tridiagonal matrix T is overwritten on Q.
The following algorithm is employed:
(i) Find the unit vector
rkk Ð qk “
´
k
ÿ
`1
i“1
q2
ik¯
1{2
, qk Ð qk{rkk for k “ 1, 2,...,n
(ii) Find projections
rkj Ð pqi, qkq “ ÿn
i“1
qij qik,
qij Ð qij ´ rkj ˚ qik, i “ 1, 2,...,n
,
/.
/-
j “ k ` 1,...,n
In the presented Gram-Schmidt algorithm, R is constructed row by row, whereas in the
modified Gram-Schmidt algorithm, this is done column by column.Eigenvalues and Eigenvalue Problems  697
Pseudocode 11.7
Module GRAM_SCHMIDT (n, d, e, Q, R)
\ DESCRIPTION: A pseudomodule to apply classic Gram-Schmidt orthogonali-
\ zation process to a symmetric tridiagonal matrix.
\ USES:
\ MAX :: Built-in function to find the max of a set of numbers;
\ SQRT:: A built-in function computing the square-root of a value.
Declare: dn, en, rnn, qnn
Q Ð 0; R Ð 0 \ Initialize matrices, Q and R
For “
i “ 1, n‰ \ Write tridiagonal matrix on Q
If “
i “ 1
‰
Then \ Write the first row
q1i Ð d1
q2i Ð e1
Else
If “
i “ n
‰
Then \ Write the last row
qpn´1qi Ð en´1
qni Ð dn
Else \ Write 1<row<n
qpi´1qi Ð ei´1
qii Ð di
qpi`1qi Ð ei
End If
End If
End For
For “
k “ 1, n‰ \ Begin GS process
imax Ð MINpk ` 1, nq \ Row no. of sub-diagonal element
sums Ð 0
For “
i “ 1, imax‰ \ Find rkk
sums Ð sums ` q2
ik
End For
rkk Ð ?sums \ Compute rkk Ð aΣq2
ik
For “
i “ 1, imax‰ \ Normalize kth column
qik Ð qik{rkk
End For
For “
j “ k ` 1, n‰
rkj Ð 0
For “
i “ 1, n‰ \ Compute rkj projection
rkj Ð rkj ` qij ˚ qik
End For
rkj Ð rkj
For “
i “ 1, n‰ \ Compute qij
qij Ð qij ´ rkj ˚ qik
End For
End For
End For
End Module GRAM_SCHMIDT698  Numerical Methods for Scientists and Engineers
EXAMPLE 11.12: Application of Gram-Schmid method
Apply the Gram-Schmidt method to find the QR-decomposition of the following
matrix:
T “
»
—
—
–
2 ´100
´1 1 ´1 0
0 ´112
0 0 22
fi
ffi
ffi
fl
SOLUTION:
Using Eqs. (11.94) and (11.95) and setting u1 “ t1 yields
q1 “ u1
u1 “ 1
?5
»
—
—
–
2
´1
0
0
fi
ffi
ffi
fl ,
pt2, q1q“ r´1 1 ´ 1 0s ¨ 1
?5
»
—
—
–
2
´1
0
0
fi
ffi
ffi
fl“´ 3
?5
For the second column, we obtain
u2 “t2 ´ pt2, q1qq1 “ 1
5
»
—
—
–
1
2
´5
0
fi
ffi
ffi
fl ,
q2 “ u2
u2 “ 1
?30
»
—
—
–
1
2
´5
0
fi
ffi
ffi
fl
r22 “u2 “
c6
5
, r13 “ rt3, q1s “ 1
?5
, r23 “ pt3, q2q “ ´7
?30
For the third column, the Gram-Schmidt process yields
u3 “t3´pt3, q1qq1´pt3, q2qq2 “ 1
6
»
—
—
–
´1
´2
´1
12
fi
ffi
ffi
fl ,
q3 “ u3
u3 “ 1
5
?6
»
—
—
–
´1
´2
´1
12
fi
ffi
ffi
fl
r33 “u3“ 5
?6
, r14 “ pt4, q1q“0,
r24 “ pt4, q2q“´c10
3 , r34 “ pt4, q3q“ 11
5
c2
3Eigenvalues and Eigenvalue Problems  699
The last step leads to
u4 “t4´pt4, q1qq1´pt4, q2qq2´pt4, q3qq3 “ 6
25
»
—
—
–
2
4
2
1
fi
ffi
ffi
fl ,
q4 “ u4
u4 “ 1
5
»
—
—
–
2
4
2
1
fi
ffi
ffi
fl
and r44 “u4“6{5.
Finally, Q and R matrices are found as
Q “
»
—
—
—
—
—
—
—
—
—
—
–
2
?5
1
?30 ´ 1
5
?6
2
5
´ 1
?5
2
?30 ´ 2
5
?6
4
5
0 ´ 5
?30 ´ 1
5
?6
2
5
0 0 12
5
?6
1
5
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
R “
»
—
—
—
—
—
—
—
—
—
—
–
?5 ´ 3
?5
1
?5
0
0
c6
5 ´ 7
?30 ´
c10
3
0 0 5
?6
11
5
c2
3
00 0 6
5
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Discussion: The process, which requires n steps, leads to a complete QR￾decomposition that can then be used for any relevant matrix procedures.
EXAMPLE 11.13: Application of QR method to find eigenvalues
Find the eigenvalues of T given in Example 11.12 using the QR-method.
SOLUTION:
We will make use of Q and R matrices obtained in Example 11.12. We start
with
A1 “ R0Q0 “
»
—
—
–
2.6 ´0.4899 0 0
´0.4899 1.56667 ´1.86339 0
0 ´1.86339 1.59334 1.17575
0 0. 1.17575 0.24
fi
ffi
ffi
fl
Next, employing QR decomposition to A1 yields
Q1 “
»
—
—
–
0.9827 0.1137 ´0.0540 0.1358
´0.1852 0.6032 ´0.2867 0.7209
0 ´0.7894 ´0.2268 0.5704
0 00.9292 0.3695
fi
ffi
ffi
fl
R1 “
»
—
—
–
2.6457 ´0.7715 0.3450 0
0 2.3604 ´2.3819 ´0.9282
0 01.2653 ´0.0437
0 0 00.7593
fi
ffi
ffi
fl700  Numerical Methods for Scientists and Engineers
A2 “ R1Q1 “
»
—
—
–
2.7429 ´0.4371 0 0
´0.4371 3.3042 ´0.9989 0
0 ´0.9989 ´0.3276 0.7056
0 00.7056 0.2806
fi
ffi
ffi
fl
Then, the QR decomposition of A2 gives
Q2 “
»
—
—
–
0.9875 0.1502 ´0.0306 0.0356
´0.1576 0.9425 ´0.1922 0.2234
0 ´0.2985 ´0.6225 0.7235
0 00.7580 0.6522
fi
ffi
ffi
fl
R2 “
»
—
—
–
2.7775 ´0.9515 0.1572 0
0 3.3467 ´0.8437 ´0.2106
0 00.9308 ´0.2265
0 0 00.6935
fi
ffi
ffi
fl
A3 “ R2Q2 “
»
—
—
–
2.8926 ´0.5266 0 0
´0.5266 3.4062 ´0.2778 0
0 ´0.2778 ´0.7511 0.5257
0 00.5257 0.4523
fi
ffi
ffi
fl
This procedure is repeated until the norm of the sub-diagonal elements is sufficiently
close to zero. In the 14th and 38th iterations, the iteration matrices are found to be
A14 “
»
—
—
–
3.7472 ´0.0302 0 0
´0.0302 2.5709 0 0
0 0 ´0.9641 0.0073
0 00.0073 0.6460
fi
ffi
ffi
fl
A38 “
»
—
—
–
3.74796 0 0 0
0 2.57018 0 0
0 0 ´0.96416 0
0 0 00.64602
fi
ffi
ffi
fl
Finally, the eigenvalues are obtained as ´0.96416, 0.64602, 2.57018, and 3.74796.
Discussion: The process of finding eigenvalues using the QR-decomposition can
then be used for any relevant matrix procedures.
11.7.4 COMPUTING EIGENVECTORS
So far, we have seen that some numerical methods are designed to give only certain or all
eigenvalues. However, when associated eigenvectors are also needed, the analyst must resort
to another method for computing the eigenvectors. In this section, a method to compute
the eigenvectors of a symmetric tridiagonal matrix is presented.
Recalling that the eigenvectors of A satisfy
pA ´ λIqx “ 0 (11.97)
Let λk and xk be respectively known eigenvalues and their associated eigenvectors. EquationEigenvalues and Eigenvalue Problems  701
(11.97) leads to
pd1 ´ λkqx1 ` e1x2 “ 0
e1x1 ` pd2 ´ λkqx2 ` e2x3 “ 0
e2x2 ` pd3 ´ λkqx3 ` e3x4 “ 0
.
.
.
en´1xn´1 ` pdn ´ λkqxn “ 0
(11.98)
where x1, x2,...,xn are the components of xk, i.e., the kth associated eigenvector λk.
Setting x1 “ 1, solving for x2 from the first equation of Eq. (11.98), we obtain
x2 “ ´pd1 ´ λkq
e1
“ ´p1pλkq
e1
(11.99)
Similarly, substituting Eq. (11.99) into the second equation of Eq. (11.98) and solving for
x3, we get
x3 “ p2pλkq
e1e2
(11.100)
Applying the forward elimination procedure in this manner, the remaining components of
the associated eigenvector can be obtained from the following relationship:
xm “ p´1q
m´1
pm´1pλkq
e1e2 ¨¨¨ em´1
, m “ 2, 3,...,n (11.101)
where pm´1pλkq is the Sturm-sequence for the eigenvalue λk.
11.8 FADDEEV-LEVERRIER METHOD
So far, numerical methods for determining the eigenpairs of non-symmetric matrices have
not yet been discussed. The methods discussed covered the computation of the eigenvalues
of symmetric matrices, which cannot be extended to non-symmetric matrices. Nonetheless,
some eigenvalue problems can be reduced to symmetric matrices.
The Faddeev-Leverrier method is a method that can be applied to relatively small
matrices. It relies on finding the coefficients of the characteristic polynomial, and it can be
employed to solve both symmetric and non-symmetric eigenvalue problems.
Suppose the characteristic polynomial of an nth-order square matrix is given as
λn ` c1λn´1 ` c2λn´2 ` ... ` cn´1λ ` cn “ 0 (11.102)
The trace of matrix A (i.e., the sum of the diagonal elements) is computed as
TrrAs “ a11 ` a22 ` a33 `¨¨¨` ann “ ÿn
i“1
aii (11.103)
The Faddeev-Leverrier algorithm forms a series of matrices, Bk (k “ 1, 2, ..., n), and the
trace of these matrices is used in the computation of ck’s as follows:
B1 “ A and d1 “ TrrB1s
B2 “ ApB1 ´ d1Iq and d2 “ 1
2TrrB2s
.
.
. .
.
.
Bn “ ApBn´1 ´ dn´1Iq and dn “ 1
nTrrBns
(11.104)702  Numerical Methods for Scientists and Engineers
Faddeev-Leverrier Method
‚ The method is simple, efficient, and easy to implement;
‚ It can be applied to symmetric and non-symmetric matrices;
‚ Since the eigenvalues are obtained from the roots of the characteristic
polynomial, it is possible to determine imaginary eigenvalues as well;
‚ It does not involve any division by matrix elements, except division
by integers at the very end;
‚ The determinant and/or inverse of a matrix can also be computed
simultaneously with no extra computational effort.
‚ The method is practical only for small matrices; for large matrices,
it should be used with caution as it leads to the amplification of
round-off errors.
Next, the characteristic polynomial and its coefficients are obtained as
λn ´ d1λn´1 ´ d2λn´2 ´ ... ´ dn´1λ ´ pn “ 0 (11.105)
where ck “ ´dk pk “ 1, 2, 3,...,nq.
This method also allows the determinant and/or the inverse of the matrix to be com￾puted as follows:
A´1 “ 1
dn
pBn´1 ´ dn´1Iq and detpAq “ p´1q
ndn (11.106)
EXAMPLE 11.14: Finding characteristic polynomial and inverse of a matrix
Find the characteristic polynomial, the inverse, and the determinant of matrix A.
A “
»
–
3 32
4 21
´522
fi
fl
SOLUTION:
Setting B1 “ A, the first coefficient is obtained as
d1 “ TrrB1s “ ÿ
3
i“1
bii “ 3 ` 2 ` 2 “ 7
The construction of B2 yields,
B1 ´ d1I “
»
–
3 32
4 21
´522
fi
fl ´ p7q
»
–
100
010
001
fi
fl “
»
–
´43 2
4 ´5 1
´5 2 ´5
fi
fl
B2 “ ApB1 ´ d1Iq “
»
–
3 32
4 21
´522
fi
fl
»
–
´43 2
4 ´5 1
´5 2 ´5
fi
fl “
»
–
´10 ´2 ´1
´13 4 5
18 ´21 ´18
fi
flEigenvalues and Eigenvalue Problems  703
From the trace of B2, we find d2 as
d2 “ 1
2
TrrB2s “ 1
2
ÿ
3
i“1
bii “ p´10q ` 4 ` p´18q
2 “ ´12
Next, computing B3 gives
B2 ´ p2I “
»
–
´10 ´2 ´1
´13 4 5
18 ´21 ´18
fi
fl ´ p´12q
»
–
100
010
001
fi
fl “
»
–
2 ´2 ´1
´13 16 5
18 ´21 ´6
fi
fl
B3 “ ApB2 ´ d2Iq “
»
–
3 32
4 21
´522
fi
fl
»
–
2 ´2 ´1
´13 16 5
18 ´21 ´6
fi
fl “
»
–
300
030
003
fi
fl
and finally, we obtain
d3 “ 1
3
TrrB3s “ 1
3
ÿ
3
i“1
bii “ 3 ` 3 ` 3
3 “ 3
Now, changing the signs of dk’s, the coefficients of the characteristic polynomial are
found as d1 “ ´7, d2 “ 12 and d3 “ ´3, i.e., the characteristic polynomial is found
as
Ppλq “ λ3 ´ 7λ2 ` 12λ ´ 3 “ 0
Since the roots of the characteristic polynomial are the eigenvalues, the roots in this
case are found as λ1 “ 0.300372, λ2 “ 2.23912 and λ3 “ 4.4605. The determinant
and the inverse of A are obtained as
A´1 “ 1
d3
pB2 ´ d2I q “ 1
3
»
–
2 ´2 ´1
´13 16 5
18 ´21 ´6
fi
fl and detpAq “ p´1q
3
d3 “ ´3
Discussions: The Faddeev-Leverrier process generates all the coefficients, ci’s, for
the characteristic polynomial, pnpλq. We can then use a root-finding method to
obtain all the eigenvalues from pnpλq “ 0. Meanwhile, the process also allows the
determinant and the inverse to be computed.
11.9 CHARACTERISTIC VALUE PROBLEMS
Characteristic value problems (CVPs) or Eigenvalue problems (EVPs) involve the solution of
a differential equation with an undetermined parameter and associated boundary conditions.
The solution of the differential equation exists only for certain values of a parameter (i.e.,
the eigenvalue).
To illustrate how eigenvalue problems arise, consider a constant vertical compressive
force or a load of P applied to a thin column of uniform cross section (see Fig. 11.6). We
assume that the column is pinned at both ends. The column deflection, ypxq, satisfies the704  Numerical Methods for Scientists and Engineers
FIGURE 11.6: (a) A slender column under compressive stress, (b,c,d) buckling modes.
following two-point boundary value problem:
EI d2y
dx2 ` P ypxq “ 0, yp0q “ ypLq “ 0 (11.107)
Rearranging and defining k2 “ P{EI, Eq. (11.107) becomes y2 ` k2y “ 0. One of the
solutions of this BVP is so-called the trivial solution (ypxq “ 0) and is not of practical
interest due to representing the neutral equilibrium state. However, this BVP has unique
(i.e., non-trivial) solutions depending on the value of k denoting an eigenvalue, and the
associated eigenvector is the solution of the BVP.
The column deflects or buckles only when the compressive force reaches a critical load
(Pn). To determine the critical load, the BVP, a homogeneous second-order ODE, is solved,
yielding
ypxq “ c1 cos kx ` c2 sin kx
where c1 and c2 are arbitrary constants.
Upon employing the BCs, the left BC yields
yp0q “ c1 cos 0 ` c2 sin 0 “ c1 “ 0
Likewise, the right BC results in
ypLq “ c2 sin kL “ 0 (11.108)
Setting c2 “ 0 leads to the trivial solution; however, recall that we are seeking the non￾trivial solutions, i.e., y ‰ 0 and c2 ‰ 0. Nonetheless, for kL “ nπ, note that the BVP is
satisfied for all c2 ‰ 0 and n “ 1, 2, 3, and so on. In other words, this problem has infinitely
many but discrete solutions (called eigenfunctions). For a specified value of k, we find
ypxq “ c2 sinp
nπx
L q
where k now is replaced with kn “ nπ{L, which leads to Pcr,n “ n2π2EI{L2. But c2 is
indeterminate, and that is why the column displacement cannot be determined. This is to
be expected since the column is in equilibrium or a neutral state. The smallest buckling
corresponds to the n “ 1 case; that is, Pcr,1 “ π2EI{L2. The column buckles into a
longitudinal half-sine wave seen in Fig. 11.6a. For n “ 2, 3, and so on, the buckling loadsEigenvalues and Eigenvalue Problems  705
FIGURE 11.7: Grid structure.
correspond to Pcr,2 “ 4π2EI{L2, Pcr,3 “ 9π2EI{L2, and so on, respectively. The larger
values of buckling loads (Pcr,n, n ą 1) correspond to more complex buckling loads, as
depicted in Fig. 11.6b, 11.6c, and 11.6d. Theoretically, these buckling modes could occur;
nevertheless, in reality, the lowest buckling load is never exceeded because the column breaks
down due to high stresses.
11.9.1 NUMERICAL SOLUTION OF CVPs
Now let us examine step-by-step how to solve characteristic value problems using the finite
difference method.
The procedure for solving CVPs is, for the most part, similar to that for solving BVPs.
A CVP differs from the BVPs in that the final matrix equation is in the form of a standard
or general eigenvalue problem, i.e., Ax “ λx or Ax “ λBx. The numerical solution steps
are given as follows:
Step 1. Gridding. The first step is gridding of the physical domain [0,L], which is divided
into M uniformly spaced (h “ L{M) subintervals (see Fig. 11.7). The nodes placed at both
end points of each subinterval are enumerated starting from 0 to M, as depicted in Fig.
11.7. The corresponding abscissas are determined (xi “ ih for i “ 0, 1, 2, ..., M), and the
ordinates are coded with short-hand notation, ypxiq “ yi.
Step 2. Discretizing. The CVP is discretized for any nodal point xi in [0, L]:
y2
i ` k2yi “ 0
The derivatives appearing in the differential equation are approximated by the CDFs. In
this case, replacing y2
i with the CDF gives
yi`1 ´ 2yi ` yi´1
h2 ` k2yi “ 0,
or rearranging results in the general difference equation:
yi`1 ´ 2yi ` yi´1 ` pkhq
2
yi “ 0, i “ 1, 2, ..,pM ´ 1q (11.109)
Step 3. Implementing BCs. The implementation of the BCs is carried out in the same manner
as in the case of BVPs. The solutions at the boundary nodes are known (y0 “ yp0q “ 0 and
yM “ ypLq “ 0) due to the prescribed Dirichlet BCs. Hence, the difference equations for
i “ 0 and i “ M can be omitted, which leads to M ´ 1 unknown nodal points.
To implement the BCs, we concentrate on the first (i“1) and last (i“M´1) difference
equations. Substituting i“1 and y0 “0 in Eq. (11.109) gives
´2y1 ` y2 “ ´pkhq
2
y1 (11.110)706  Numerical Methods for Scientists and Engineers
For i“M ´1 and yM “0, similarly, Eq. (11.109) is reduced to
yM´2 ´ 2yM´1 “ ´pkhq
2
yM´1 (11.111)
For the interior nodes, i“2, 3,...,M ´2, the general difference equation is valid.
Step 4. Setting up an Eigenvalue Problem. In the Dirichlet BC case, the term containing
the known boundary node is moved to the rhs of the difference equation. In the Neumann
or Robin BC cases, the resulting fictitious node is eliminated in the same way as in the
treatment of the BVPs. Then, putting together all the difference equations (Eqs. (11.109),
(11.110), and (11.111)) gives the following matrix form:
»
—
—
—
—
—
—
—
–
´2 1
1 ´2 1
1 ´2 1
... ... ...
1 ´2 1
1 ´2
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
–
y1
y2
y3
.
.
.
yM´2
yM´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“ ´pkhq
2
»
—
—
—
—
—
—
—
–
y1
y2
y3
.
.
.
yM´2
yM´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
(11.112)
By setting λ “ ´pkhq
2 and denoting the coefficient matrix and column vector as A and y,
respectively, Eq. (11.112) becomes Ay “ λy, i.e., the standard eigenvalue problem.
Step 5. Solving the Eigenvalue Problem. The solution to an eigenvalue problem can be found
with a suitable analytical or numerical method, as discussed earlier in this chapter. Note that
the set xk is the solution (i.e., eigenvector) of the ODE corresponding to the predetermined
discrete nodal points for the corresponding λk (i.e., eigenvalue).
A standard eigenvalue problem having Toeptliz tridiagonal form, as presented in the
critical buckling load problem, has an analytical solution (see Section 2.1) and need not be
solved numerically. We resort only to numerical methods in cases where analytical solutions
are very difficult or impossible to obtain. Returning back to the buckling problem, the
eigenpairs of the CVP can easily be found analytically since the coefficient matrix A is a
symmetric Toeplitz tridiagonal matrix (i.e., a “ ´2 and b “ c “ 1). Using the analytical
expression, the eigenvalues are found from
λi “ ´phkiq
2 “ ´2 ` 2 cospiπ{Mq, i “ 1, 2,...,pM ´ 1q
For M “ 10, the first two eigenvalues (λ1 and λ2) are obtained as ´0.0978869 and
´0.381966, respectively. Then evaluating k2 yields
k2
1 “ ´λ1
h2 “ 9.78869 and k2
2 “ ´λ2
h2 “ 38.1966
On the other hand, recall that the analytical solution gave k2
1 “ π2 “ 9.8696 and k2
2 “
p2πq
2 “ 39.47864. These values of k2 are very close to those of the analytical solution. In
other words, as the number of intervals is increased (M Ñ 8), not only the eigenvalues but
also the eigenvectors, approach the true values.Eigenvalues and Eigenvalue Problems  707
EXAMPLE 11.15: Finite difference solution of CVPs
Apply the finite difference method to solve the following CVP:
d
dx „
p1 ` xq
2 dy
dxj
` λy “ 0, y1
p0q “ yp1q “ 0
SOLUTION:
We implement the numerical solution step by step as follows:
Step 1. Gridding: The solution domain is divided into uniform M subintervals
(h “ 1{M), and the nodal points are indexed from 0 to M (see Fig. 11.8). The
corresponding estimates (numerical solutions) are expressed as ypxiq “ yi where
xi “ ih.
FIGURE 11.8
Step 2. Discretizing: The CVP is valid for an arbitrary xi in (0,1); thus, we may write
p1 ` xiq
2
y2
i ` 2p1 ` xiqy1
i ` λyi “ 0, xi P r0, 1q
Approximating y1
i and y2
i with the CDFs yields
´p1 ` xiq
2
ˆyi`1 ´ 2yi ` yi´1
h2
˙
´ 2p1 ` xiq
ˆyi`1 ´ yi´1
2h
˙
“ λyi (11.113)
For simplicity, letting ri “ p1`xiq{h, the general difference equation is expressed as
rip1 ´ riqyi´1 ` 2r2
i yi ´ rip1 ` riqyi`1 “ λyi (11.114)
for i “ 1, 2,...,pM ´1q.
Step 3. Implementing the BCs: The Neumann and Dirichlet BCs are imposed on the
left and right boundary nodes, respectively, i.e., y1
p0q “ y1
0 “ 0 and yp1q “ yM “ 0.
By dropping the right boundary node, the total number of unknowns becomes M.
For the left boundary, setting i “ 1 in Eq. (11.114) leads to
r0p1 ´ r0qy´1 ` 2r2
0y0 ´ r0p1 ` r0qy1 “ λy0 (11.115)
The fictitious node y´1 in Eq. (11.115) is eliminated by discretizing the left BC as
y1
0 « py1 ´ y´1q{2h “ 0, giving y´1 “ y1. Substituting this into Eq. (11.115) yields
2r2
0y0 ´ 2r2
0y1 “ λy0 (11.116)
For the right boundary, setting i“M ´1 and noting yM “ 0 results in
rM´1p1 ´ rM´1qyM´2 ` 2r2
M´1yM´1 “ λyM´1 (11.117)708  Numerical Methods for Scientists and Engineers
For the interior nodes (i “ 1,...,M ´ 2), Eq. (11.114) is applied.
Step 4. Setting up the CVP: The CVP can be expressed in matrix equation form as
»
—
—
—
—
—
—
—
–
d0 a0
b1 d1 a1
b2 d2 a2
... ... ...
bM´2 dM´2 aM´2
bM´1 aM´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
»
—
—
—
—
—
—
—
–
y0
y1
y2
.
.
.
yM´2
yM´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
“ λ
»
—
—
—
—
—
—
—
–
y0
y1
y2
.
.
.
yM´2
yM´1
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
where bi “ ri p1´riq, di “ 2r2
i , and ai “ ´rip1`riq for i “ 0, 1,...,M ´1, except
a0 “´2r2
0. The resulting tridiagonal matrix, however, is not symmetrical.
Step 5. Solving the EVP: For M “ 5, we determine the eigenvalues from the roots of
the characteristic polynomial:
λ5 ` 510λ4 ´ 88508λ3 ` 6.213 ˆ 106λ2 ´ 1.567 ˆ 108λ ` 7.62 ˆ 108 “ 0
which gives λ1 “6.301, λ2 “40.799, λ3 “90.620, λ4 “142.140, and λ5 “230.137.
Discussion: Problems for large number of nodes lead to large matrices. Determining
the eigenvalues by the characteristic polynomial is not be attempted as the polyno￾mial will become ill-conditioned.
EXAMPLE 11.16: Numerical solution of a CVP
Apply the finite difference method with five uniform intervals to solve the following
characteristic value problem:
y2 ` k2x2y “ 0, yp0q “ 0, 4y1
p1q ´ yp1q “ 0
SOLUTION:
Step 1. Gridding: The solution interval is (0,1), and the BCs are given as Dirichlet (left
BC) and Robin BC (right BC). The interval is divided into uniform M subintervals
(h “ p1 ´ 0q{M) to create one-dimensional grids. The nodal points placed on both
sides of the intervals are numbered from 0 to M, and the corresponding abscissas
are found from xi “ ih, and the ordinates are set to ypxiq “ yi (see Fig. 11.9). The
total number of unknowns (nodes) is M.
FIGURE 11.9
Step 2. Discretizing: The differential equation for any node within (0,1) is
y2
i ` k2x2
i yi “ 0, xi P p0, 1qEigenvalues and Eigenvalue Problems  709
Substituting the CDF for y2
i leads to
yi`1 ´ 2yi ` yi´1
h2 ` k2x2
i yi “ 0
Rearranging this equation yields the general difference equation:
´yi´1 ` 2yi ´ yi`1 “ pkhq
2
x2
i yi (11.118)
Step 3. Implementing the BCs: The left boundary nodal value is known (yp0q “ y0 “
0), and the right boundary nodal value is unknown (4y1
M ´ yM “ 0). Setting i “ 1
and substituting y0 “ 0 (left BC) into Eq. (11.118) gives
2y1 ´ y2 “ pkhq
2
x2
1y1
Setting i “ M in Eq. (11.118) results in
´yM´1 ` 2yM ´ yM`1 “ pkhq
2
yM
which creates a fictitious node (yM`1). This fictitious node, as usual, is eliminated
by discretizing the right BC with the CDF as follows:
4
yM`1 ´ yM´1
2h ´ yM “ 0 Ñ yM`1 “ yM´1 `
h
2
yM
Substituting yM`1, the difference equation for i “ M takes the following form:
´2yM´1 `
ˆ
2 ´ h
2
˙
yM “ pkhq
2
x2
i yi
For the interior nodes (i “ 1, 2,...,M ´ 1), Eq. (11.118) is valid.
Step 4. Setting up the Eigenvalue Problem: Finally, setting λ “ pkhq
2 and combining
all difference equations, a generalized eigenvalue problem is obtained, Ay “ λBy, or
explicitly
A “
»
—
—
—
—
—
—
—
–
2 ´1
´1 2 ´1
´1 2 ´1
... ... ...
´1 2 ´1
´1 1.9
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
, B “
»
—
—
—
—
—
—
—
–
x2
1
x2
2
x2
3
...
x2
M´1
x2
M
fi
ffi
ffi
ffi
ffi
ffi
ffi
ffi
fl
Step 5. Solving the EVP: The Cholesky decomposition method is applied to solve
the resulting CVP by transforming it to a standard CZ “ λZ form, where
C “ L´1AL´T . For M “ 5, the Cholesky decomposition yields
L “
»
—
—
—
—
–
0.20 0 00
0 0.40 00
0 00.600
0 0 00.8 0
0 0 0 01
fi
ffi
ffi
ffi
ffi
fl , L´1 “
»
—
—
—
—
–
50 0 00
0 5{20 00
005{300
00 05{4 0
00 0 01
fi
ffi
ffi
ffi
ffi
fl710  Numerical Methods for Scientists and Engineers
C “
»
—
—
—
—
–
50 ´12.50 0 0
´12.5 12.5 ´25{60 0
0 ´25{6 50{9 ´25{12 0
0 0 ´25{12 3.125 ´1.25
00 0 ´1.25 1.9
fi
ffi
ffi
ffi
ffi
fl
Discussion: Notice that C is a symmetric tridiagonal matrix. By implementing the
QR algorithm, the eigenvalues are obtained as 53.8152, 11.5853, 4.8848, 2.2513, and
0.5440. Then. k2 values are computed from k2 “ λ{h2, yielding 36.6794, 17.0186,
11.051, 7.5022, and 3.6878 for k “ 1, 2,..., 5, respectively.
11.10 CLOSURE
Eigenvalues and eigenvectors have a wide and diverse range of applications, some requiring
the calculation of a single eigenvalue, others requiring the computation of all eigenvalues.
The computation of the associated eigenvector may or may not be required. In this regard,
there are a variety of methods in the literature on this subject that serve different purposes.
Many physical problems require either the largest or smallest (in magnitude) eigenvalue
and its associated eigenvector. The power method and the inverse power method provide
the numerical means to compute the largest or smallest eigenvalue as well as associated
eigenvectors, respectively. The convergence rate of the power method is slow when the
dominance ratio approaches 1, i.e., λ1 « λ1. The shift technique, on the other hand, also
allows the calculation of an eigenvalue close to a specified value.
The Jacobi method, designed for symmetric matrices, uses similarity transformations
based on plane rotations to reduce a matrix into a diagonal form. This method is iterative
in nature, and it may require many rotations in order to obtain the diagonal matrix.
Computing all eigenvalues of a symmetric matrix is handled in two steps: (1) transform￾ing the matrix into a symmetric tridiagonal form, and (2) applying an iterative procedure
to generate a sequence of matrices that eventually converge to a diagonal matrix whose
diagonal elements are the eigenvalues. The Householder method yields the desired sym￾metrical tridiagonal form by applying a series of similarity transformations to a symmetric
matrix. A common method to numerically obtain all eigenvalues of a symmetric tridiagonal
matrix is the QR algorithm, which is an iterative method. A sequence of matrices generated
as a result of this iterative procedure converges to a diagonal matrix whose elements are
the eigenvalues of the original matrix. The Gram-Schmid algorithm, which does not require
iterations, performs QR decomposition in n-steps. The method for large matrices is plagued
with round-off errors, leading to loss of orthogonality.
The Sturm sequence method is employed on a symmetric tridiagonal matrix (of order
n ă 10) to determine its characteristic polynomial. Another method is needed to find the
roots of the characteristic polynomial (eigenvalues). Moreover, a separate method is also
required to compute the eigenvectors, if it is needed. The Faddeev-Leverrier method is
a general method that gives the characteristic equation, which should be solved by any
method calculating the roots of a polynomial.
Since eigenvalue problems can yield very large matrices, the numerical computation of
eigenpairs may lead to an accumulation of round-off errors. Thus, numerical computations
with high precision should be carried out.Eigenvalues and Eigenvalue Problems  711
11.11 EXERCISES
Section 11.1 Eigenvalue Problem and Properties
E11.1 Find the characteristic polynomial, eigenvalues, and associated eigenvectors (normalized
by �8´norm) of the following matrices:
(a) „ 3 2
´3 ´4
j
, (b) „ 11 4
´15 ´5
j
, (c) „
´5 ´3
9 7 j
,
(d)
»
–
1 ´1 3
112
314
fi
fl , (e)
»
–
´5 ´2 ´2
441
6 ´3 4
fi
fl , (f)
»
–
´31 9
´10 8 1
´3 3 ´3
fi
fl
E11.2 For matrices given in E11.1, show that the sum of the eigenvalues of a matrix is equal to
its trace.
E11.3 For matrices given in E11.1, show that the product of the eigenvalues of a matrix is equal
to its determinant.
E11.4 For matrices given in E11.1 and β “ 5, show that the eigenpair of βA is pβλ, xq.
E11.5 For matrices given in E11.1, show that the eigenpair of A2 is pλ2, xq.
E11.6 For matrices given in E11.1, show that the eigenpair of A´1 is pλ´1, xq.
E11.7 Find the eigenpairs of the following triangular matrices:
(a) „
3 4
0 ´3
j
(b) „
4 0
5 ´5
j
(c)
»
–
2 3 ´3
0 ´5 ´4
00 4
fi
fl (d)
»
–
600
2 ´2 0
321
fi
fl
E11.8 Find the eigenpairs of the following diagonal matrices.
(a) „
3 0
0 ´5
j
(b) „
7 0
0 3j
(c)
»
–
´40 0
0 20
0 0 10
fi
fl (d)
»
–
60 0
03 0
0 0 ´9
fi
fl
E11.9 For the given matrices, plot the Gerschgorin disks in the complex plane and mark the
location of the eigenvalues.
(a)
»
–
2 50
´200
0 14
fi
fl (b)
»
–
´3 ´5 0
4 ´7 1
0 07
fi
fl (c)
»
–
´4 ´1 1
2 ´2 ´1
´1 1 ´6
fi
fl
(d)
»
—
—
–
´8 ´24 0
4 ´40 2
0 2 2 21
0 2 ´2 12
fi
ffi
ffi
fl
(e)
»
—
—
–
´5 ´90 1
1 ´41 1
´10 5 ´1
1 2 ´1 4
fi
ffi
ffi
fl
(f)
»
—
—
–
´10 1 0 1
´1 ´4 ´1 ´1
1 ´2 3 ´2
1 1 ´1 9
fi
ffi
ffi
fl
E11.10 Consider the symmetric tridiagonal matrix with aii “ 2 and ai`1,i “ ai,i`1 “ 1 for
i “ 1, 2,...,n. Use the Gerschgorin theorem to determine the effect of the matrix size (n “ 3, 5,
10) on the location of the eigenvalues.
E11.11 (a) Find the eigenpairs of the following symmetric matrix given below; (b) Determine712  Numerical Methods for Scientists and Engineers
whether the eigenvectors are orthogonal or not.
»
—
—
—
—
–
8 ´2 4 ´1 ´3
´2 10 0 2 ´2
4 0 11 4 0
´12 4 8 3
´3 ´2 0 3 12
fi
ffi
ffi
ffi
ffi
fl
Section 11.2 Power Method
E11.12 Apply the power method with scaling to determine the dominant eigenvalue of the fol￾lowing matrices. For a starting guess, use (i) xp0q “ r 1, 1, 1 s
T , (ii) xp0q “ r 1, 1, 0 s
T and (iii)
xp0q “ r 1, 0, 0 s
T . Use ε “ 10´6.
(a) A “
»
–
3 ´8 1
´8 ´4 ´8
1 ´8 3
fi
fl (b) B “
»
–
2 ´1 2
1 0 ´2
2 ´2 3
fi
fl (c) C “
»
–
5 4 ´2
´11 1
1 2 ´1
fi
fl
E11.13 Apply the power method with scaling to find the largest eigenvalue of the following
non-symmetric matrices, correct to within ε “ 10´6. Use xp0q “ 1.
(a)
»
–
84 4
6 ´2 2
1 2 12
fi
fl (b)
»
–
´13 ´15 ´15
´1 17 ´1
31 15 33
fi
fl (c)
»
–
5 15 ´1
1 ´9 1
´16 ´15 ´10
fi
fl
(d)
»
–
´21 9
181
2 3 ´3
fi
fl (e)
»
–
238
328
´9 4 ´9
fi
fl (f)
»
–
1 ´2 ´6
224
´21 0
fi
fl
E11.14 Apply the Power method with scaling to find the largest eigenvalue of the following
non-symmetric 4ˆ4 matrices, correct to within ε “ 10´6. Use xp0q “ 1.
(a)
»
—
—
–
6232
´6 0 ´5 ´4
´4 ´2 ´1 ´2
4135
fi
ffi
ffi
fl
(b)
»
—
—
–
12 5 9 5
´3 0 ´5 ´1
´10 ´5 ´7 ´5
´5 ´2 ´3 ´1
fi
ffi
ffi
fl
(c)
»
—
—
–
´11 0 1
´6 ´4 ´4 ´5
´2 ´1 ´3 ´1
8263
fi
ffi
ffi
fl
(d)
»
—
—
–
4 ´5 ´2 ´7
´1 ´1 ´3 ´2
´21 0 3
3 ´2 1 ´3
fi
ffi
ffi
fl
(e)
»
—
—
–
8 9 10 9
´13 ´12 ´12 ´11
´18 ´9 ´20 ´9
11 1 10 0
fi
ffi
ffi
fl
(f)
»
—
—
–
´48 ´19 ´41 ´19
2 ´1 15 ´7
38 19 31 19
42 13 29 19
fi
ffi
ffi
fl
E11.15 Repeat E11.13, using the power method with the Rayleigh quotient.
E11.16 Repeat E11.14, using the power method with the Rayleigh quotient.
E11.17 Apply the power method with the Rayleigh quotient to find the dominant eigenvalue of
the following symmetric matrices, correct to within ε “ 10´6. Use xp0q “ 1. How many iterations
are required to find the dominant eigenvalue?
A1 “
»
—
—
—
—
–
21000
12100
01210
00121
00012
fi
ffi
ffi
ffi
ffi
fl
, A2 “
»
—
—
—
—
–
41000
14100
01410
00141
00014
fi
ffi
ffi
ffi
ffi
fl
, A3 “
»
—
—
—
—
–
81000
18100
01810
00181
00018
fi
ffi
ffi
ffi
ffi
fl
E11.18 To find the minimum eigenvalue of the following matrices, first find the inverse matrices,Eigenvalues and Eigenvalue Problems  713
and then apply the power method with scaling. Use xp0q “ 1 and ε “ 10´6.
(a)
»
–
238
328
´9 4 ´9
fi
fl (b)
»
–
1 ´2 ´6
224
´21 0
fi
fl (c)
»
–
´211
3 32
´144
fi
fl
(d)
»
–
1 ´1 3
112
314
fi
fl (e)
»
—
—
–
´41 1 1
171 ´1
2 ´1 ´3 2
1124
fi
ffi
ffi
fl
(f)
»
—
—
–
901 ´2
6310
1 ´25 1
0 ´11 2
fi
ffi
ffi
fl
E11.19 Apply the power method with scaling to find the smallest eigenvalue of the matrices in
E11.13, correct to within ε “ 10´6. Start iterations with xp0q “ 1.
E11.20 Apply the power method with scaling to find the smallest eigenvalue of the matrices in
E11.14, correct to within ε “ 10´6. Start iterations with xp0q “ 1.
E11.21 Repeat E11.17 to find the smallest eigenvalue.
E11.22 For a given matrix A, (a) find its eigenpairs; (b) perform two iterations using the Power
method with scaling starting and x “ 1; (c) does the method appear to converge to the dominant
eigenvector?; and (d) repeat Part (b) with four iterations using the Rayleigh Quotient method.
What do you observe?
A “
»
–
3 ´1 1
´1 5 ´1
1 ´1 3
fi
fl
E11.23 Find the dominant eigenvalue of the matrix below using the Power method with (a)
scaling and (b) Rayleigh quotient, and determine how many iterations are required for convergence
with both methods. Use x “ 1 as the starting guess. Apply both convergence criteria: �x�8 ă 10´6
and |1 ´ λpp´1q
{λppq
| ă 10´6. »
—
—
—
—
–
1 2 ´1 1 ´1
´28 1 3 ´2
1 ´1 4 ´2 2
3 2 ´2 ´9 3
1 2 ´2 ´1 ´3
fi
ffi
ffi
ffi
ffi
fl
E11.24 Apply the shifted-inverse power algorithm to find the dominant eigenvalue of the matrix
in E11.23 for α “ ´11, ´10, ´9, ´8.5, ´8, and ´7. Use ε “ 10´6 and x “ 1 as the starting guess.
E11.25 Apply the shifted-inverse power algorithm to find the eigenvalues of the matrix given in
E11.23 in the neighborhood of α “ ´3 and +3. How many iterations did it require to reach a
converged solution? Use ε “ 10´6 and x “ 1 as the starting guess.
E11.26 Determine all eigenvalues of the following matrices by applying the shifted inverse power
method with the Rayleigh quotient and Gerschgorin theorem. Use ε “ 10´6 and x “ 1 as the
starting guess.
(a)
»
—
—
–
´48 ´19 ´41 ´19
2 ´1 15 ´7
38 19 31 19
42 13 29 19
fi
ffi
ffi
fl
(b)
»
—
—
–
´11 ´5 ´8 ´5
´13 ´5 ´6 ´9
10 5 7 5
19 7 12 11
fi
ffi
ffi
fl
(c)
»
—
—
–
10 3 ´1 1
3 5 ´1 ´1
´1 ´19 1
1 ´11 6
fi
ffi
ffi
fl
E11.27 Determine the two smallest (in magnitude) eigenvalues of the following matrix by apply￾ing the shifted inverse power method with the Rayleigh quotient. Make use of the Gerschgorin
theorem to find a good initial guess for the eigenvalue. Use ε “ 10´6 and x “ 1 as the starting714  Numerical Methods for Scientists and Engineers
guess.
»
—
—
—
—
–
30 ´11 2 ´1
´1 ´16 ´3 ´1 0
1 ´3 26 2 1
2 ´12 8 ´1
´10 1 ´1 5
fi
ffi
ffi
ffi
ffi
fl
Section 11.3 Similarity and Orthogonal Transformation
E11.28 Are the following 2 ˆ 2 matrices similar?
(a) A “
„
1 0
0 1j
B “
„ 1 0
´1 1j
, (b) A “
„
1 2
3 ´4
j
B “
„ 7 15
´4 ´10j
,
(c) A “
„
0 2
0 0j
B “
„
0 0
2 0j
E11.29 Are the following 3 ˆ 3 matrices similar?
A “
»
–
3 ´6 2
2 ´3 ´2
1 ´1 ´2
fi
fl , B “
»
–
13 ´32 3
6 ´7 3
´22 40 ´8
fi
fl
E11.30 For given matrices, obtain the modal matrix P and find P´1AP.
(a) A “
„
1 3
3 1j
(b) A “
„
5 1
2 4j
(c) A “
»
–
4 ´1 2
´1 5 ´1
2 ´1 4
fi
fl (d) A “
»
–
3 2 ´1
140
´22 3
fi
fl
E11.31 Use the given matrices to evaluate P´1
1 AP1, P´1
2 AP2, and P´1
3 AP3.
A P1 P2 P3
(a) „
4 1
3 2j „1 ´1
1 3 j „´1 1
3 1j „2 ´2
2 6 j
(b) „
5 3
6 ´2
j „3 ´1
2 3 j „´1 3
3 2j „9 ´3
6 9 j
(c)
»
–
23 ´1 0
´1 22 ´1
0 ´1 23
fi
fl
»
–
1 ´1 1
´102
1 11
fi
fl
»
–
´111
0 ´1 2
1 11
fi
fl
»
–
1 ´1 1
´101
1 21
fi
fl
(d)
»
–
1 6 ´8
´85 8
´33 2
fi
fl
»
–
185
284
153
fi
fl
»
–
121
885
543
fi
fl
»
–
185
´1 ´4 ´2
153
fi
fl
E11.32 Apply the similarity identity to evaluate A10.
(a) A “
„
1 1
2 2j
(b) A “
»
–
111
110
001
fi
fl (c) A “
»
–
103
114
301
fi
fl
E11.33 (a) Find the eigenpairs of A; (b) determine the matrix inverse by A´1 “ PD´1P´1.
A “
»
—
—
–
11 ´15 1
´1 11 1 5
5 1 11 ´1
1 5 ´1 11
fi
ffi
ffi
flEigenvalues and Eigenvalue Problems  715
E11.34 If possible, diagonalize the following matrices:
(a)
»
–
3 12 ´16
12 2
12 1
fi
fl (b)
»
–
3 ´1 1
´1 3 ´1
1 ´1 5
fi
fl (c)
»
–
2 1 ´1
´33 2
110
fi
fl
(d)
»
—
—
–
6232
´6 0 ´5 ´4
´4 ´2 ´1 ´2
4135
fi
ffi
ffi
fl
(e)
»
—
—
–
13 1 1 1
1 13 1 1
1 1 13 1
1 1 1 13
fi
ffi
ffi
fl
(f)
»
—
—
–
4 13 9 8
7 ´29 ´33 3
1 29 29 ´1
2 4 0 12
fi
ffi
ffi
fl
E11.35 The net equilibrium force acting on a free-falling body yields the following first-order
coupled ODEs:
mdv
dt “ mg ´ C v, dh
dt “ ´v with hp0q “ h0, vp0q “ 0
where h is altitude, v is free-fall speed, g is acceleration of gravity, and C is the drag coefficient ex￾perienced during the fall. (a) Express the ODEs in matrix form, and (b) apply the diagonalization
procedure to be able to find an analytical solution for the altitude and velocity.
E11.36 Consider the circuit given in Fig. E11.36. Current Iptq
through the inductor and voltage V ptq across the capacitor satisfy
the following coupled first-order ODEs:
dV
dt “ ´ 1
CR1
V ptq ` 1
C Iptq, dI
dt “ ´ 1
LV ptq ´ R2
L Iptq
with V p0q “ 10 V and Ip0q “ 2 A. Apply the diagonalization pro￾cedure to uncouple the system of ODEs and find the analytical
solution for the current and voltage if R1 “ 10Ω, R2 “ 5 Ω, L “ 2
H, and C “ 50 F. Fig. E11.36
E11.37 Consider the radioactive decay chain, A λ1 ÝÑ B λ2 ÝÑ C λ3 ÝÑ D, which satisfies the following
coupled first-order linear ODEs:
dNA
dt “ ´λ1NA, dNB
dt “ λ1NA ´ λ2NB, dNC
dt “ λ2NB ´ λ3NC
with NAp0q “ 1, NBp0q “ 0, and NC p0q “ 0. (a) Express the coupled ODEs in matrix form;
(b) apply a diagonalization procedure to find an analytical solution for NA, NB, and NC . Given:
λ1 “ 1, λ2 “ 0.75, and λ3 “ 0.20 s´1.
E11.38 Two objects (A and B) with initial temperatures
TAp0q “ 500˝C and TBp0q “ 300˝C are placed in a room
at Trp0q “ 10˝C, as shown in Fig. E11.38. Newton’s law of
cooling yields the following first-order coupled ODEs for
the temperatures:
dTr
dt “ 3
2000 pTr ´ TAq ` 5
2000 pTr ´ TBq
dTA
dt “ ´ 2
15 pTA ´ Trq , dTB
dt “ ´ 1
16 pTB ´ Trq
(a) Express the set of coupled ODEs in matrix form; (b)
apply the diagonalization procedure to find an analytical
solution for the temperatures.
Fig. E11.38716  Numerical Methods for Scientists and Engineers
Fig. E11.39
E11.39 Three tanks with volumes V1, V2, and V3 contain brine, as shown in Fig. E11.39. Salt
concentrations in the tanks are denoted by C1, C2, and C3. The first tank is fed water at a rate
of 15 m3/min. The feed rates between the tanks are r1 “ 2.5 m3/h, r2 “ 1.8 m3/h, and r3 “ 2
m3/h. A mathematical model for the salt content as a function of time is given as
dC1
dt “ ´r1
C1
V1
, dC2
dt “ r1
C1
V1
` r3
C3
V3
´ r2
C2
V2
, dC3
dt “ r2
C2
V2
´ r3
C3
V3
with the ICs: C1p0q “ 10, C2p0q “ 2, and C3p0q “ 1 kg. (a) express the ODE set in matrix
form; (b) apply the diagonalization procedure to find the analytical solutions for the salt contents.
Given: V1 “ 120, V2 “ 50, and V3 “ 120 m3.
Section 11.4 Jacobi Method
E11.40 Use the Jacobi method to find the eigenvalues and associated eigenvectors of the following
matrices (ε “ 10´6).
(a)
»
–
21 2
1 4 ´1
2 ´1 5
fi
fl (b)
»
–
42 1
2 5 ´1
1 ´1 8
fi
fl (c)
»
–
3 ´2 3
´254
3 41
fi
fl
(d)
»
—
—
–
3 ´2 1 ´1
´27 3 5
136 ´3
´1 5 ´3 8
fi
ffi
ffi
fl
(e)
»
—
—
–
1 52 ´3
5 32 ´1
2 271
´3 ´11 4
fi
ffi
ffi
fl
(f)
»
—
—
–
411 ´2
1 4 ´2 ´1
1 ´25 3
´2 ´13 8
fi
ffi
ffi
fl
E11.41 The stresses in a three-dimensional body are described by the following stress tensor: Find
the principle stresses (eigenvalues) and associated direction cosines (eigenvectors). Use ε “ 10´6
for convergence tolerance.
»
–
σx τxy τxz
τxy σy τyz
τxz τyz σz
fi
fl “
»
–
50 15 0
15 40 5?30
0 5?30 30
fi
fl
E11.42 A stress tensor at a point in a three-dimensional body is given below. Find the stress
invariants before and after transformation (i.e., T “ UT SU) of the axes at 45˝ about the z-axis.
Recall that the stress invariants are defined as I1 “ TrpSq, I2 “ p1{2qpTrpSq
2 ´ TrpS2q, and
I3 “ |S|. Hint: After transformation, find the rotation matrix U first and then compute I1, I2,
and I3 for T.
S “
»
–
10 8 5
8 16 0
5 0 18
fi
flEigenvalues and Eigenvalue Problems  717
Section 11.5 Cholesky Decomposition
E11.43 Use the algorithm given in Section 11.5 to invert the following lower triangular matrices.
(a)
»
–
400
310
231
fi
fl (b)
»
–
2 00
´130
3 ´3 2
fi
fl (c)
»
—
—
–
50 00
2 ´100
1 ´430
1 0 ´2 2
fi
ffi
ffi
fl
(d)
»
—
—
–
20 0 0
34 0 0
3 0 ´1 0
2 1 ´1 1
fi
ffi
ffi
fl
E11.44 Given A and B, transform Ax “ λBx to the standard eigenvalue problem Cx “ λx
using Cholesky decomposition, and then find all eigenpairs.
(a) A“
»
–
211
163
139
fi
fl B“
»
–
321
284
147
fi
fl (b) A“
»
–
31 2
1 10 4
2 4 12
fi
fl B“
»
–
523
242
328
fi
fl
(c) A“
»
–
2 2 ´1
231
´11 6
fi
fl B“
»
–
4 3 ´1
352
´12 4
fi
fl (d) A“
»
–
8 ´2 3
´254
3 43
fi
fl B“
»
–
2 1 ´2
163
´23 5
fi
fl
E11.45 Given A and B, transform Ax “ λBx to the standard eigenvalue problem Cx “ λx
using Cholesky decomposition, and then find all eigenpairs.
(a) A“
»
—
—
–
3211
2514
1171
1418
fi
ffi
ffi
fl
B“
»
—
—
–
4226
2 10 7 6
2766
6 6 6 47
fi
ffi
ffi
fl
(b) A“
»
—
—
–
2 1 21
1 10 1 2
2 1 81
1 2 14
fi
ffi
ffi
fl
B“
»
—
—
–
11 2 1
15 6 3
2 6 24 16
1 3 16 12
fi
ffi
ffi
fl
E11.46 Consider the following coupled second-order differential equations:
„
4 1
1 3j d2
dt2
„
y1
y2
j
“
„ 2 ´3
´3 5 j „y1
y2
j
Assuming a harmonic solution in the form of yi “ Ki sin ωt), (a) show that the system yields an
Ak “ λBk eigenvalue problem, where λ “ ω2 and k “ rK1, K2s
T ; (b) transform the problem to
a standard Mz “ λz eigenvalue problem, where z “ LT k; (c) obtain the eigenpairs of the given
system.
E11.47 Consider the spring-mass-pendulum system depicted in
Fig. E11.47. For small angular displacements (i.e., sin θ « θ and
cos θ « 1), the equations of motion can be expressed as
m1
d2x
dt2 ` m2
ˆd2x
dt2 ` �
d2θ
dt2
˙
“ ´kx,
m2�
ˆd2x
dt2 ` �
d2θ
dt2
˙
“ ´m2�g θ
Fig. E11.47
where x and θ are respectively horizontal and angular displacements, � is pendulum length, g is
gravitational acceleration, and m1 and m2 are the masses of the cart and the pendulum. Letting
x “ rx, θs
T , (a) cast the equations of motion in matrix form; (b) assuming a harmonic solution
(x “ K1 sin ωt, θ “ K2 sin ωt), express the given system as an eigenvalue problem Ak “ λBk,
where λ “ ω2 and k “ rK1, K2s
T ; and (c) transform the problem to a standard Mz “ λz
eigenvalue problem.718  Numerical Methods for Scientists and Engineers
E11.48 The two simple pendulums, shown in Fig. E11.48, are
connected with a spring (k) with negligible mass. The spring is
undeformed when the two pendulums are in the vertical posi￾tion. Assuming small angular displacements (i.e., sin θ « θ and
cos θ « 1), the equations of motion can be expressed as
m1�
d2θ1
dt2 “ k�pθ2 ´ θ1q ´ m1gθ1,
m2�
d2θ2
dt2 “ k�pθ1 ´ θ2q ´ m2gθ2, Fig. E11.48
where θ, �, and m denote the angular displacement, length, and mass of the pendulums, and g
is gravitational acceleration. Letting x “ rθ1, θ2s
T , (a) put the equations of motion in matrix
form; (b) assuming a harmonic solution of the form θ1 “ c1 sin ωt and θ2 “ c2 sin ωt, express the
system as Ac “ λBc, where λ “ ω2 and c “ rc1, c2s
T ; (c) transform this problem into a standard
eigenvalue problem, Mz “ λz, and then obtain the frequencies of vibrations (eigenvalues) along
with the relative amplitudes (eigenvectors) for the case of m1 “ 0.5 kg, m2 “ 1 kg, k “ 25 N/m,
� “ 0.5 m, and g “ 9.81 m/s2.
E11.49 Consider the horizontal motion of the spring-mass system, depicted in Fig. E11.49. The
equations of motion are expressed as
m1
d2x1
dt2 “ ´k1x1 ` k2 px2 ´ x1q , m2
d2x2
dt2 “ ´k2 px2 ´ x1q ` k3 px3 ´ x2q
m3
d2x3
dt2 “ ´k3 px3 ´ x2q ´ k4x3
where ki is spring stiffness, xi is the horizontal deflection of mi relative to static equilibrium
positions.
Fig. E11.49
Letting x “ rx1, x2, x3s
T , (a) put the equations of motion in matrix form; (b) show that the
general solution has xi “ ci sin ωt form, where ω is the frequency and ci’s amplitudes, leads
to Ac “ λBc eigenvalue problem, where λ “ ω2; (c) transform this problem to the standard
Mc “ λc eigenvalue problem; and (d) for m1 “ m2 “ m3 “ m and k1 “ k2 “ k3 “ k, find the
eigenpairs of M and associated natural frequencies.
E11.50 Consider the LC-circuit given in Fig. E11.50.
Fig. E11.50
The unsteady behavior of the circuit is described with the following coupled second-order ODEs:
L1
d2I1
dt2 `
I1
C1
`
I1´I2
C2
“ 0, L2
d2I2
dt2 `
I2´I1
C2
`
I2´I3
C3
“ 0, L3
d2I3
dt2 `
I3´I2
C3
`
I3
C4
“ 0
where L, I, and C denote the inductance, current, and capacitance. Letting i “ rI1, I2, I3s
T , (a)
put the system in matrix form; (b) assume the general solution is In “ cn sin ωt form, which leads
to the Ai “ λBi eigenvalue problem, where λ “ ω2; (c) transform the problem to the standardEigenvalues and Eigenvalue Problems  719
Mc “ λc eigenvalue problem; and (d) for L1 “ 2L2 “ 4L3 “ 1 and 4C1 “ 2C2 “ C3 “ 1, find the
eigenpairs of M.
E11.51 Consider a simplified representation of the three rotating masses, shown in Fig. E11.51.
A mathematical model, consisting of coupled second-order ODEs, is given as
I1
d2θ1
dt2 “k1pθ2´θ1q, I2
d2θ2
dt2 “k1pθ1´θ2q`k2pθ3´θ2q, I3
d2θ3
dt2 “k2pθ2´θ3q
where I, k, and θ denote the moment of inertia, shaft stiffness, and angular displacement, respec￾tively.
Fig. E11.51
Letting t “ rθ1, θ2, θ3s
T , (a) put the system in matrix form; (b) assuming the general solution is
in θn “ cn sin ωt form, which leads to Ac “ λBc eigenvalue problem, where λ “ ω2; (c) transform
the problem to the standard Mz “ λz eigenvalue problem; and (d) for 4I1 “ 10I2 “ 5I3 “ 50
kg¨m2 and 5k1{4 “ k2 “ 105 N/m, find the eigenpairs of M.
Section 11.6 Householder Method
E11.52 Use the Householder method to transform the following matrices into tridiagonal matri￾ces.
(a)
»
–
234
311
412
fi
fl (b)
»
–
12 1
2 ?5 ?5
1 ?5 2?5
fi
fl (c)
»
–
1 1 ?3
1 33?3
?3 3?3 1
fi
fl
E11.53 Use the Householder method to transform the following matrices into tridiagonal matri￾ces.
(a)
»
—
—
–
1 ´21 2
´2 3 ´2 1
1 ´2 4 11
2 1 11 3
fi
ffi
ffi
fl
(b)
»
—
—
–
1403
4354
0531
3418
fi
ffi
ffi
fl
(c)
»
—
—
–
4 1 ´2 ´2
134 5
´2 4 ´3 11
´2 5 11 3
fi
ffi
ffi
fl
Section 11.7 Eigenvalues of Tridiagonal Matrices
E11.54 Use the Strum sequence to find the coefficients of the characteristic polynomials.
(a)
»
–
210
123
034
fi
fl (b)
»
–
1 ´2 0
´2 4 ´3
0 ´3 2
fi
fl (c)
»
–
3 ´1 0
´132
0 26
fi
fl
E11.55 Use the Strum sequences to find the coefficients of the characteristic polynomials.
(a)
»
—
—
–
14 00
4 3 ´1 0
0 ´131
00 14
fi
ffi
ffi
fl
(b)
»
—
—
–
42 0 0
21 1 0
0 1 ´1 ´2
0 0 ´2 3
fi
ffi
ffi
fl
(c)
»
—
—
–
3 ´200
´2 ´3 ´1 0
0 ´141
0 0 15
fi
ffi
ffi
fl
E11.56 Use the Gram-Schmidt QR decomposition to compute the eigenvalues and the eigenvec￾tors of the following matrices.
(a)
»
–
2 ´
?5 0
´
?5 1 ´2
0 ´2 1
fi
fl (b)
»
–
2 ´2 0
´24 4
0 4 10
fi
fl (c)
»
–
12 ´5 0
´5 1 ´1
0 ´1 3
fi
fl720  Numerical Methods for Scientists and Engineers
E11.57 Use the Gram-Schmidt QR decomposition to compute the eigenvalues and the eigenvec￾tors of the following matrices.
(a)
»
—
—
–
1 ´300
´3 2 30
0 3 42
0 0 21
fi
ffi
ffi
fl
(b)
»
—
—
–
3 ´40 0
´45 1 0
014 ´2
0 0 ´2 3
fi
ffi
ffi
fl
(c)
»
—
—
–
21 00
1 4 ´1 0
0 ´142
00 28
fi
ffi
ffi
fl
Section 11.8 Leverrier-Faddeev Method
E11.58 Use the Leverrier-Faddeev method to find the characteristic polynomials and the inverse
matrices.
(a)
»
–
2 ´2 3
´24 4
3 4 10
fi
fl (b)
»
–
1 2 ´2
3 1 ´1
2 ´3 ´2
fi
fl (c)
»
–
532
1 4 ´2
´1 ´3 1
fi
fl
E11.59 Use the Leverrier-Faddeev method to find the characteristic polynomials and the inverse
matrices.
(a)
»
—
—
–
2312
1 1 ´3 ´1
´12 4 1
1 ´22 2
fi
ffi
ffi
fl
(b)
»
—
—
–
3 11 ´2
1 ´40 1
2 63 ´3
´1 ´2 2 ´3
fi
ffi
ffi
fl
(c)
»
—
—
–
2121
0 4 ´2 ´1
´2 ´23 2
´3 ´21 4
fi
ffi
ffi
fl
E11.60 Find the eigenpairs of the following tridiagonal matrices using the analytical expressions.
(a)
»
–
4 ´1 0
´2 4 ´1
0 ´2 4
fi
fl (b)
»
–
120
312
031
fi
fl (c)
»
–
530
153
015
fi
fl
E11.61 Find the eigenpairs of the following tridiagonal matrices using the analytical expressions.
(a)
»
—
—
–
3200
1320
0132
0013
fi
ffi
ffi
fl
(b)
»
—
—
–
3 ´20 0
´2 3 ´2 0
0 ´2 3 ´2
0 0 ´2 3
fi
ffi
ffi
fl
(c)
»
—
—
–
4900
1490
0149
0014
fi
ffi
ffi
fl
Section 11.8 Characteristic Value Problems
E11.62 Solve the following characteristic value problems numerically using the finite difference
method: (a) develop the difference equations and express them in matrix form; (b) obtain the nu￾merical solution using uniformly spaced M “ 5 intervals. Use ε “ 10´5 for convergence tolerance.
(a) y2 ` λy “ 0, yp0q “ yp2πq “ 0, (b) x2y2 ` 2xy1 ` λy “ 0, yp1q “ yp2q “ 0
(c) y2 ` 2y1 ` k2e´xy “ 0, yp0q “ yp1q “ 0, (d) p1 ` x2qy2 ` λy “ 0, yp´1q “ yp1q “ 0
E11.63 Solve the following characteristic value problems numerically using the finite difference
method: (a) develop the difference equations and express them in matrix form; (b) obtain the nu￾merical solution using uniformly spaced M “ 5 intervals. Use ε “ 10´5 for convergence tolerance.
(a) y2 ` λy “ 0, yp0q “ 2y1
p2q ´ yp2q “ 0, (b) px2y1
q
1 ´ λx2y “ 0, y1
p0q “ yp2q “ 0,
(c) y2 ` λy “ 0, y1
p0q “ y1
p1q ` yp1q “ 0, (d) y2 ` 6y1 ` p9 ` λqy “ 0, y1
p0q “ yp1q “ 0,
(e) xy2 ` p1 ´ xqy1 ` λy “ 0, y1
p0.5q “ 5y1
p1q ´ yp1q “ 0Eigenvalues and Eigenvalue Problems  721
11.12 COMPUTER ASSIGNMENTS
CA11.1 Applying Hückel theory to the Schörinder equation for cyclobutadiene (a) and benzene
(b) leads to the following Hamiltonian matrix:
(a) H “
»
—
—
–
αβ β
βαβ
βαβ
β βα
fi
ffi
ffi
fl , (b) H “
»
—
—
—
—
—
—
–
α β β
βαβ
βαβ
βαβ
βαβ
β β α
fi
ffi
ffi
ffi
ffi
ffi
ffi
fl
The secular equation is the standard eigenvalue problem (HΨ “ EΨ), where E (eigenvalues)
denote the orbital energies and Ψ denotes corresponding wave functions. Employing the problem
simplifications illustrated in Chapter 2 (Example 2.2), find the molecular orbitals and correspond￾ing wave functions using a suitable numerical method.
CA11.2 Write a pseudomodule to find the smallest (in magnitude) eigenvalue using the inverse
power method.
CA11.3 Write a pseudomodule to find any one of the eigenvalues of a matrix using the inverse
shift power method.
CA11.4 A four-story building frame, shown in Fig. CA11.4, can be modeled as a four-degree￾of-freedom mass-spring system. The mass and stiffness of the floors are denoted by mi and ki,
respectively. The natural frequencies and mode shapes of the frame are given with
m1x:1 “ ´k1x1 ` k2 px2 ´ x1q
m2x:2 “ ´k2 px2 ´ x1q ` k3 px3 ´ x2q
m3x:3 “ ´k3 px3 ´ x2q ` k4 px4 ´ x3q
m4x:4 “ ´k4 px4 ´ x3q
where xi’s are the horizontal displacements of the floors, ki “ 24EI{�i’s are stiffness constants
for columns subjected to shear only, EI is the bending stiffness of the column, and �i’s are
the column lengths. Find the natural frequencies and mode shapes of the building frame for
m1 “ m2 “ m3 “ m4 “ m and k1 “ k2 “ k3 “ k4 “ k. What would the dominant natural
frequency be if the number of stories were increased?
Fig. CA11.4722  Numerical Methods for Scientists and Engineers
CA11.5 Find the circular frequencies of the oscillations of the circuit in Fig. CA11.5. The Kirchoff
equations are:
Ld2i1
dt2 ` L
ˆd2i1
dt2 ´ d2i2
dt2
˙
`
1
C1
i1 “ 0,
L
ˆd2i2
dt2 ´ d2i1
dt2
˙
` L
ˆd2i2
dt2 ´ d2i3
dt2
˙
`
1
C2
i2 “ 0,
L
ˆd2i3
dt2 ´ d2i2
dt2
˙
` L
ˆd2i3
dt2 ´ d2i4
dt2
˙
`
1
C3
i3 “ 0,
L
ˆd2i4
dt2 ´ d2i3
dt2
˙
` L
ˆd2i4
dt2 ´ d2i5
dt2
˙
`
1
C4
i4 “ 0,
L
ˆd2i5
dt2 ´ d2i4
dt2
˙
` Ld2i5
dt2 `
1
C5
i5 “ 0,
Fig. CA11.5
CA11.6 A mathematical model for the motion of an airfoil (see Fig. CA11.6) being tested in a
wind-tunnel is given by the following set of coupled second-order differential equations:
md2y
dt2 ` m� d2θ
dt2 ` k1y “ 0,
m� d2y
dt2 ` J d2θ
dt2 ` k2θ “ 0
where y and θ are horizontal and angular displacement of the airfoil from the horizontal position,
m, J, and k1 are mass, inertia moment about center of gravity (cog) and spring constant of the
airfoil, k “ 2 is the spring constant of the spiral spring. Letting x “ ry, θs
T , (a) cast the equations
of motion in matrix form, (b) assuming a harmonic solution (y “ K1 sinpωtq, θ “ K2 sinpωtq),
express the given system as an eigenvalue problem Ac“ λ Bc where, λ “ ω2 and c “ rC1, C2s
T ;
(c) Transform the problem to standard Mz“ λz eigenvalue problem; (d) for k1 “ 1, k2 “ 4,
� “ 0.20, J “ 1, m “ 1, obtain the natural frequency and modes.
Fig. CA11.6APPENDIX A
A Guide on How to Read
and Write a Pseudocode
T HE algorithms presented in this book are presented in a form that requires very little
time and effort to digest. The motivations for using pseudocodes have been stated in
the preface and Chapter 1. The aim of this appendix is to present the basic syntax and
constructions with simple and concise examples.
A.1 BASICS AND VARIABLES
A.1.1 VARIABLES AND CONSTANTS
A variable, in programming languages, is usually represented symbolically with letters or
combinations of letters and numbers (A, B, AX, xy, a1, tol, . . . ). Greek letters (α, β, . . . ) can
also be used as variables in pseudocodes. A variable can be viewed as a named placeholder
to which a value (compatible with its type) is assigned and read as needed. By defining,
declaring, or initializing, a storage area is allocated for every constant or variable in the
computer’s memory. The value of a variable changes during the execution of a computer
program, while the value of a constant does not.
A.1.2 DECLARATION
The Declare statement is used to indicate the use of arrays (see Section A.5 for details) by
assigning subscripts. The extent of an array (the range of its index) is denoted by m : n
where m and n are integers (n ą m). If m is omitted, the default value is 1. Arrays a1, a2,
...,an, or b0, b1, b2, ..., bn or e11, e12, e13, and so on are declared as follows:
Declare: an, b0:n, en,n \ Array variables an, b0:n, en,n
\ For ak, k“1, 2, ..., n and bk, k“0, 1, 2, ..., n
\ For eij , i“j “1, 2, ..., n
Note that array B is declared as b0:n indicating the index runs from 0 to n. The memory
spaces allocated for A, B, and E are n, n ` 1, and n2. Arrays declared within a Module are
not accessible from outside unless they are arguments of the module.
DOI: 10.1201/9781003474944-A 723724  Numerical Methods for Scientists and Engineers
A.1.3 COMMENTING
Commenting is a common practice in actual programs. It is done to allow “human-readable”
descriptions detailing the purpose of some of the statement(s) and/or to create in situ
documentation. Everything from the “\” symbol, where it is introduced, to the end of the
line is reserved for comments. Some commenting examples are
\ Pseudocode for finding the roots of a quadratic equation.
Δ Ð b2 ´ 4ac \ Computing the discriminant
A single-line comment is applied to describe an expression (or statement) on the cor￾responding line. A block of comments generally at the beginning of each module (Header
Comments) is used to describe the purpose of the module, its variables, exceptions, other
modules used, etc.
A.1.4 ASSIGNMENT
In actual programming languages, the expression on the right-hand side of the “=” sign is
evaluated first, and its value is placed at the allocated memory location of the variable on
the left-hand side. Any recently computed value of variable replaces its previous value. As a
depiction of this process, an assignment is denoted by Ð (a left-arrow). It has the following
general form
variable Ð Expression
As an example, consider the pseudocode segment below. By invoking X Ð 0, “zero”
is placed in the memory location of X. Next, the expression on the right-hand side of the
second line is processed first (0+4), and the result is placed in the memory location of the
variable X on the left-hand side (i.e., X Ð 4). The third expression is processed in the same
way (X Ð 4 ` 6), which updates the memory location of X with 10.
X Ð 0 \ 0 “zero” is assigned to X-variable
X Ð X ` 4 \ Increment X by 4
X Ð X ` 6 \ Increment X by 6
In this text, the multiplication operator “*” has sometimes been
omitted in algebraic expressions where the multiplication is im￾plied. For instance, in the Δ Ð ?
b2 ´ 4ac expression, 4bc is clearly
the product of “4”, a, and b.
A.1.5 SEQUENTIAL STATEMENTS
Frequently, a sequence of statements (with no imposed conditions) is used to perform a
specific task. These statements will be executed in the order they are specified in the pro￾gram. By using a semicolon (;) as a separator, multiple expressions are placed in a line. For
instance, the following pseudocode segment illustrates how the first and second statements
are presented on a single line with the use of a semicolon.A Guide on How to Read and Write a Pseudocode  725
a Ð b2 ` c2; d Ð ?a \ Statement-1 and Statement-2
x1 Ð d{pb ` cq \ Statement-3
x2 Ð d{pb ´ cq \ Statement-4
y Ð x1 ` x2 \ Statement-5
A.1.6 INPUT/OUTPUT STATEMENTS
An inevitable element in any program is the communication of the input and output data
with the main or sub-programs. This is done through Read/Write statements, which are used
for reading initial values of variables into the program from a file (or console) or writing
out intermediate or final values of variables to a file (or screen).
For instance, the following code segment illustrates supplying numerical values of the
variables a, b, and c into the program. After evaluating Δ, its value is printed to the screen
or written to a file.
Read: a, b, c \ Read a, b, c variables from console or file
Δ Ð ?
b2 ´ 4ac \ Compute the discriminant.
Write: Δ \ Write discriminant to screen or file
A.2 LOGICAL VARIABLES AND LOGICAL OPERATORS
Branching in a computer program causes a computer to execute a different block of instruc￾tions, deviating from its default behavior of executing instructions sequentially. Branching
structures are controlled by logical variables and logical operations.
Logical calculations are carried out with an assignment statement:
Logical_variable Ð Logical_expression
Logical_expression can be a combination of logical constants, logical variables, and
logical operators. A logical operator is defined as an operator on numeric, character, or
logical data that yields a logical result. There are two basic types of logical operators:
relational operators and combinational operators.
The relational operators can be expressed in a variety of ways. This pseudocode con￾vention uses the following corresponding formalism:
B “ C \ Equal
B ‰ C \ Not equal
B ă C (B ď C) \ Less than or (less than and equal to)
B ą C (B ě C) \ Greater than or (greater than and equal to)
Logical statements or variables take either True or False values, and they can be con￾structed with the aid of logic operators (Or , And, and Not ).
Or \ OR operator
And \ AND operator
Not \ NOT operator
Logical operators are used in a program together with relational operators to control726  Numerical Methods for Scientists and Engineers
the flow of the program. If L1 and L2 are two logical prepositions, in order for L1 And L2
to be True, both L1 and L2 must be True. In order for L1 Or L2 to be True, it is sufficient to
have either L1 or L2 to be True. When using Not in any logical statement, the logic value
is changed to True when it is False or changed to False when it is True.
A.3 CONDITIONAL CONSTRUCTIONS (IF-THEN, IF-THEN-ELSE)
The direction of execution flow cannot be changed with sequential statements since they
are executed only once. Whereas, there is a need to execute part of a program multiple
times or, under some conditions, to change the direction of the execution flow. In high-level
programming languages, branching control and conditional structures allow the execution
flow to jump to a different part of the program.
Conditional constructions create branches in the execution path based on the evaluation
of a condition. When a control statement is reached, the condition is evaluated, and a path
is selected according to the result of the condition.
Most programming languages use If-Then and If-Then-Else constructions that behave
in the same way. They allow one to check a condition and execute certain parts of the code
if condition is True or False. The most common and simplified form (If-Then construct)
executes the block of statements if and only if a logical expression (i.e., condition) is True.
The If-Then construct has the form
If “
condition‰
Then \ Check for the condition
STATEMENT(s) if \ Execute STATEMENT(s)
the condition=True \ in block if condition is True
End If
Note that condition is enclosed with square brackets. When the condition is tied to only
one statement, the If-Then construct is further simplified as follows:
If [condition] Then STATEMENT
In the above, the STATEMENT reserved to a single statement (algebraic expression,
input or output statement, etc.), and it is executed only if condition is True.
A more general If-Then-Else construct allows the use of another block of statements
when the condition is False. The If-Then-Else construct has the following form:
If “
condition‰
Then
STATEMENT(s)-1 if \ Execute STATEMENT(s)
the condition=True \ in this block if condition is True
Else
STATEMENT(s)-2 if \ Execute STATEMENT(s)
the condition=False \ in this block if condition is False
End If
When condition is True, STATEMENT(S)-1 is executed; otherwise (condition=False),
STATEMENT(S)-2 is executed.A Guide on How to Read and Write a Pseudocode  727
A more complicated If-Then-Else construct can be devised as follows:
If “
condition1
‰
Then
STATEMENT(s)-1 if \ Execute STATEMENT(s)-1
the condition1=True \ in this block if condition1 is True
Else
If “
condition2
‰
Then
STATEMENT(s)-2 if \ Execute STATEMENT(s)-2
the condition2=False \ in this block if condition1 is False
Else.
.
.
If “
conditionn
‰
Then
STATEMENT(s)-n if \ Execute STATEMENT(s)-n
the conditionn=True \ in this block if conditionn is True
Else
STATEMENT(s)) if \ Execute STATEMENT(s)
the conditionn=False \ in this block if conditionn is False
End If
End If
End If
A.4 CONTROL CONSTRUCTIONS
Control (loop) constructions are used when a program needs to execute a block of instruc￾tions repeatedly until a condition is met, at which time the loop is terminated. There are
three control statements in most programming languages that behave in the same way:
While-, Repeat-Until, and For-constructs.
A.4.1 WHILE- CONSTRUCTS
A While-construct has the form
While “
condition‰
STATEMENT(s)) if \ Execute STATEMENT(s) in
condition=True \ this block so long as condition is True
End While
The flowchart for a While-construct is presented in Fig. A.1a. In this construct, the
condition (enclosed by square brackets) is analyzed first. If the condition is True, then the
block of statement(s) is executed. Then, the condition is analyzed again, and the loop
is iterated until the condition becomes False; i.e., the only way out of the loop is when
condition=False. If the condition=False at the start, the block of statements will not be
executed.
While- construct is used in cases where the number of iterations
cannot be determined beforehand. It does not require an index
variable, so it is used to execute the STATEMENT(s) until a de￾sired condition is met.728  Numerical Methods for Scientists and Engineers
FIGURE A.1: Flowchart for a (a) While- and (b) Repeat-Until constructs.
A.4.2 REPEAT-UNTIL CONSTRUCT
A Repeat-Until is a loop construct similar to While-construct in that a block of STATE￾MENT(S) is executed until the condition (enclosed with square brackets) becomes False.
The Repeat-Until loop construct has the form
Repeat
STATEMENT(s)) if \ Execute STATEMENT(s) in this
condition=False \ block so long as condition is False
Until “
condition‰
The flowchart for the Repeat-Until construct is illustrated in Fig. A.1b. In this construct,
however, condition is analyzed after the STATEMENT(S) is executed. The Repeat-Until
construct is convenient in cases where the condition is unknown or undefined until the
STATEMENT(S) block is executed at least once. The loop is terminated when the condition
becomes True.
The condition prior to the Repeat-Until construct is important
for skipping or executing the entire loop. Also, it is important to
make sure that the condition is always met to prevent an infinite
loop.
A.4.3 FOR-CONSTRUCT
For-construct is probably the most common type of loop construct in any programming
language. It is used when a block of STATEMENT(S) is to be executed a specified number
of times. A For-construct has the form
For “
index variable=i1, i2 (,Δi)
‰
STATEMENT(s) \ Execute STATEMENT(s) in this block
End For
where index variable is referred to as the loop index or loop counter, i1 and i2 are the
initial and terminal values of the index, and an optional parameter Δi denotes increments
pΔi ą 0q or decrements pΔi ă 0q; if it is omitted, it is 1 by default. The total number ofA Guide on How to Read and Write a Pseudocode  729
FIGURE A.2: Flowchart for a For-construct.
iterations is pi2 ´ i1 ` Δiq{Δi. To break out of a For-loop (Exit) before it reaches n2, a
condition within the STATEMENT(S) block needs to be specified.
A flowchart for a For-construct is illustrated in Fig. A.2. The initial step is executed
first and only once; the next i ď i2 condition is evaluated. If this condition is True, then
the STATEMENT(S) in the iteration loop are executed. Otherwise, (if i ď i2 is False) the
STATEMENT(S) are not executed, and the flow of control jumps to the next statement
just after End For.
A For-construct is used when the number of iterations to be per￾formed is known beforehand. It is easy to use in nested loop set￾tings (with arrays) due to having clearly identified loop indexes.
A.4.4 EXITING A LOOP
The Exit statement is used anywhere in the block statements of While-, Repeat-Until, or
For- constructs to terminate the loop. When the Exit statement is executed, the remaining
block of statement(s) in the current iteration loop is skipped.
Consider the pseudocode segment using the Exit statement in a For-construct. The
loop is executed for k=1, 2, . . . , 12, but when condition (k ą12) becomes True for k=13,
the loop is terminated before k reaches 30.
For “
k “ 1,30‰
¨¨¨
If [k ą 12] Then Exit
¨¨¨
Write: k
¨¨¨
End For
Note that any condition for exit need not be tied to the index variable.730  Numerical Methods for Scientists and Engineers
FIGURE A.3: Depiction of elements of an array in computer memory.
When using If-Then, If-Then-Else, While-, Repeat-Until, or For￾constructs, always indent the block of statements by several spaces
to improve the readability of your pseudocode.
A.5 ARRAYS
An array is a special case of a variable representing a set of data (variables) under one group
name. In general, for the most part, the data structures only need arrays and placeholders
that hold elements of the same type, i.e., a1, a2, ..., an (or d´2, d´1, ..., dm, or g0, g1,
..., g9). The values in an array occupy consecutive locations in the computer’s memory
(see Fig. A.3). The subscript of an array is always an integer.
Arrays can have two or more dimensions as well. Arrays should be explicitly declared
at the beginning of each pseudocode because the range and length of an array are critical in
coding. Vectors and matrices are declared as arrays of one and two dimensions, respectively.
This pseudocode convention also adopts whole array arithmetic. Under certain circum￾stances, whole array arithmetic is used in order to keep the pseudocodes as short as possible
without sacrificing the intended operation. If two arrays have the same size, then they are
used in arithmetic operations where the operation is carried out on an element-by-element
basis. The whole array arithmetic expressions do not require For-loops and operate with
the array names as if they were scalars.
A brief summary of whole array operations (using conformable arrays) is presented in
the pseudocode segment below:
Declare: an, bn, cn, e10,10, f10,10, g10,10 \ Declaring array variables
cÐa+b \ Performs c=a+b
cÐ 4a+(´3)b \ Performs ci=4*ai+(´3)*bi for all i
cÐ a*b \ Performs ci “ ai ˚ bi for all i
¨¨¨
gÐ e+5*f \ Performs gij=eij+5*fij for all i, j
Write: c \ Prints ci sequentially for all i
¨¨¨A Guide on How to Read and Write a Pseudocode  731
In the pseudocodes in this text, vectors (i.e., one-dimensional
arrays) and matrices (i.e., two- or multi-dimensional arrays)
are denoted in lowercase and uppercase bold typeface, respec￾tively. For instance, a denotes a1, a2, ¨¨¨ , an and E denotes
e11, e12, ¨¨¨ , enm and they should be declared as
Declare: an, enm.
A.6 PROGRAM, MODULE, AND FUNCTION MODULE STRUCTURES
A pseudo-program is a collection of instructions designed to perform a task. A pseudo￾program is given a name, and it has the form
Program EXAMPLE
Declare: an, en,m
¨¨¨
$
&
%
¨¨¨
STATEMENT(S)
¨¨¨
,
.
-
¨¨¨
Write: a, e
End Program EXAMPLE
Note that it is delimited by Program-End Program statements to indicate where the program
begins and where it ends.
In general, it is impractical to write a complete program from A to Z that includes
everything. Such programs would not only be too long but also too complicated to be
practical. When writing a computer program, it is often necessary to repeat the same set
of instructions (a task) multiple times within the same program. To avoid this unwanted
repetition, we often utilize a sub-program, which is a collection of statements for the purpose
of achieving some specific tasks, such as inverting a matrix, finding the real roots of a
polynomial, and so on. All high-level languages allow modular programming, which is a
programming technique that separates the functionality of a program into independent
modules containing only the required aspect of the desired functionality. Then these modules
are accessed and executed from the main or sub-programs whenever needed.
Two types of modules are adopted in this pseudocode convention: Function Module
for functions and Module for procedures or sub-programs. Each module, having its own
input/output variables, is a separate program completely isolated from the rest of the
program.
An efficient program generally consists of one or more independent
modules. The modular programming approach is also easier to
conceptualize and write a program as a whole.732  Numerical Methods for Scientists and Engineers
A.6.1 SUB-PROGRAM DEFINITIONS: Module
A Module as a procedure or sub-program requires a module name as well as an argument
list (input/output variables). A module is delimited with Module-End Module statements.
The general structure of a Module is given as
Module NAME(in1, in2, ..., out1, out2, ...)
$
&
%
¨¨¨
STATEMENT(S)
¨¨¨
,
.
-
Body of the module
End Module NAME
where in1, in2, ... and out1, out2, ... are respectively input and output variable lists, i.e.,
the arguments of the module. Information in and out of a module is communicated by the
argument list. It is invoked by naming it (with its full argument lists) in any pseudocode
or pseudomodule. Any array variable is declared in the module by the Declare statement.
A module usually has its own internal (local) variables that are accessible only inter￾nally. Once a specific task is completed, at least one output value is returned to the calling
module or program. No restriction is imposed on the number of modules. A module code
can be used with any other relevant programs as well. A module can also be accessed many
times in the same program. The variables declared in a module argument list are global to
the module.
A sample pseudomodule finding the magnitude of the vector V “ v1 i ` v2 j ` v3 k is
given below.
Module VECTOR_MAGNITUDE(v1, v2, v3, mag)
\ DESCRIPTION: A pseudomodule finding the magnitude of a vector.
mag Ð av2
1 ` v2
2 ` v2
3
End Module VECTOR_MAGNITUDE
A.6.2 FUNCTION MODULE OR RECURSIVE FUNCTION MODULE
A Function Module may be considered a smaller version of a Module discussed earlier. A
module may have more than one output, while Function Module is restricted to single output
value, which can also be used to evaluate an expression. Functions may consist of a single
expression or be computed as a result of multiple statements.
A function is delimited by Function Module-End Function Module statements. The
general structure of a Function Module is given as
Function Module
$
FUN_NAME (v1, v2, v3, ..., vn)
&
%
¨¨¨
STATEMENT(S)
¨¨¨
,
.
-
FUN_NAME Ð Expression
End Function Module FUN_NAME
where v1, v2, v3, ..., vn are the input variables (arguments) of the function, and the result
(output) is written on the function name (FUN_NAME) on return.A Guide on How to Read and Write a Pseudocode  733
Most common mathematical functions (?x, |x|, ex, lnpxq, logpxq, trigonometric, hyper￾bolic functions and their inverses, remainder or modulo function MOD(a, b), etc.) are built
into most programming languages. Array variables may be used in the argument list of
functions; however, they need to be stated in the Declare statement of the function module.
A Function Module is invoked by naming it in an expression with its full argument
list. For instance, a user-defined function fpx, y, zq “ ax2 ` y2 ` z2 is coded as shown
in Pseudocode A.1. Note that the function fpx, y, zq named FUNC is used in arithmetic
expressions just like any other built-in function.
Pseudocode A.1
Program EXAMPLE
\ DESCRIPTION: Purpose of this code is to illustrate the use a function module.
\ USES:
\ FUNC:: A function of three variables.
...
S ÐFUNC(2, 1, 3)*FUNC(3, ´2, 4)
...
End Program EXAMPLE
Function Module FUNC (x, y, z)
FUNCÐ ax2 ` y2 ` z2
End Function Module FUNC
A program, module, or function module is rarely independent; it frequently requires
other built-in or user-defined functions or modules. The modules called by a program or
other modules are referred to as program dependencies, which are specified in the header
section beneath as USES statements. That means the program or module will not function
without the specified modules. Note that in the pseudocode above, Function Module FUNC
is identified in the program header. Make sure that the built-in modules given in the pseu￾docodes are used with compatible names specified in the programming language you are
programming in.
If a function is to call itself again, directly or indirectly (fnpxq Ð n ` x ˚ fn´1pxq with
f0pxq Ð x), it should be explicitly delimited by Recursive Function Module-End Recursive
Function Module statements. For instance,
Recursive Function Module FX`
n, x˘
If “
n “ 0
‰
Then
FXÐ x
Else
FXÐ n ` x˚FX(n ´ 1, x)
End If
FUN_NAME Ð Expression
End Recursive Function Module FXAPPENDIX B
Quadratures
B.1 GAUSS-LEGENDRE QUADRATURE
N ˘xk wk N ˘xk wk
2 0.57735 02691 1.00000 00000 0.90411 72564 0.10693 93260
3 0.00000 00000 0.88888 88888 0.98156 06342 0.04717 53364
0.77459 66692 0.55555 55556 14 0.10805 49487 0.21526 38535
4 0.33998 10435 0.65214 51548 0.31911 23689 0.20519 84637
0.86113 63115 0.34785 48451 0.51524 86364 0.18553 83975
5 0.00000 00000 0.56888 88889 0.68729 29048 0.15720 31672
0.53846 93101 0.47862 86704 0.82720 13151 0.12151 85707
0.90617 98459 0.23692 68850 0.92843 48837 0.08015 80872
6 0.23861 91860 0.46791 39345 0.98628 38087 0.03511 94603
0.66120 93864 0.36076 15730 16 0.09501 25098 0.18945 06104
0.93246 95142 0.17132 44923 0.28160 35507 0.18260 34150
7 0.00000 00000 0.41795 91837 0.45801 67776 0.16915 65193
0.40584 51514 0.38183 00505 0.61787 62444 0.14959 59888
0.74153 11856 0.27970 53915 0.75540 44083 0.12462 89712
0.94910 79123 0.12948 49662 0.86563 12023 0.09515 85116
8 0.18343 46424 0.36268 37833 0.94457 50230 0.06225 35239
0.52553 24099 0.31370 66458 0.98940 09349 0.02715 24594
0.79666 64774 0.22238 10344 20 0.07652 65211 0.15275 33871
0.96028 98564 0.10122 85362 0.22778 58511 0.14917 29864
10 0.14887 43398 0.29552 42247 0.37370 60887 0.14209 61093
0.43339 53941 0.26926 67193 0.51086 70019 0.13168 86384
0.67940 95682 0.21908 63625 0.63605 36807 0.11819 45319
0.86506 33666 0.14945 13491 0.74633 19064 0.10193 01198
0.97390 65286 0.06667 13443 0.83911 69718 0.08327 67415
12 0.12523 34085 0.24914 70458 0.91223 44282 0.06267 20483
0.36783 14990 0.23349 25365 0.96397 19272 0.04060 14298
0.58731 79543 0.20316 74267 0.99312 85991 0.01761 40071
0.76990 26742 0.16007 83285
734 DOI: 10.1201/9781003474944-BQuadratures  735
B.2 GAUSS-LAGUERRE QUADRATURE
N xk wk Wk “ wkexk
2 0.58578 64376 8.53553 390593 E-01 1.53332 603312
3.41421 35623 1.46446 609407 E-01 4.45095 733505
3 0.41577 45567 7.11093 009929 E-01 1.07769 285927
2.29428 03602 2.78517 733569 E-01 2.76214 296190
6.28994 50829 1.03892 565016 E-02 5.60109 462543
4 0.32254 76896 6.03154 104342 E-01 0.83273 912384
1.74576 11011 3.57418 692438 E-01 2.04810 243845
4.53662 02969 3.88879 085150 E-02 3.63114 630582
9.39507 09123 5.39294 705561 E-04 6.48714 508441
5 0.26356 03197 5.21755 610583 E-01 0.67909 404221
1.41340 30591 3.98666 811083 E-01 1.63848 787360
3.59642 57710 7.59424 496817 E-02 2.76944 324237
7.08581 00058 3.61175 867992 E-03 4.31565 690092
12.64080 08442 2.33699 723858 E-05 7.21918 635435
6 0.22284 66042 4.58964 673950 E-01 0.57353 550742
1.18893 21016 4.17000 830772 E-01 1.36925 259071
2.99273 63261 1.13373 382074 E-01 2.26068 459338
5.77514 35691 1.03991 974531 E-02 3.35052 458235
9.83746 74184 2.61017 202815 E-04 4.88682 680021
15.9828 73981 8.98547 906430 E-07 7.84901 594559
8 0.17027 96323 3.69188 589342 E-01 0.43772 341049
0.90370 17767 4.18786 780814 E-01 1.03386 934767
2.25108 66298 1.75794 986637 E-01 1.66970 976566
4.26670 01702 3.33434 922612 E-02 2.37692 470176
7.04590 54023 2.79453 623523 E-03 3.20854 091335
10.75851 60101 9.07650 877336 E-05 4.26857 551083
15.74067 86412 8.48574 671627 E-07 5.81808 336867
22.86313 17368 1.04800 117487 E-09 8.90622 621529
12 0.11572 21173 2.64731 371055 E-01 0.29720 963605
0.61175 74845 3.77759 275873 E-01 0.69646 298043
1.51261 02697 2.44082 011320 E-01 1.10778 139462
2.83375 13377 9.04492 222117 E-02 1.53846 423904
4.59922 76394 2.01023 811546 E-02 1.99832 760627
6.84452 54531 2.66397 354187 E-03 2.50074 576910
9.62131 68424 2.03231 592663 E-04 3.06532 151828
13.00605 49933 8.36505 585682 E-06 3.72328 911078
17.11685 51874 1.66849 387654 E-07 4.52981 402998
22.15109 03793 1.34239 103052 E-09 5.59725 846184
28.48796 72509 3.06160 163504 E-12 7.21299 546093
37.09912 10444 8.14807 746743 E-16 10.54383 74619736  Numerical Methods for Scientists and Engineers
B.3 GAUSS-HERMITE QUADRATURE
N ˘xk wk Wk “ wkex2
k
2 0.70710 67811 8.86226 92545 E-01 1.46114 11826
3 0.00000 00000 1.18163 59006 E+00 1.18163 59006
1.22474 48713 2.95408 97515 E-01 1.32393 11752
4 0.52464 76232 8.04914 09000 E-01 1.05996 44828
1.65068 01238 8.13128 35447 E-02 1.24022 58176
5 0.00000 00000 9.45308 72048 E-01 0.94530 87204
0.95857 24646 3.93619 32315 E-01 0.98658 09967
2.02018 28704 1.99532 42059 E-02 1.18148 86255
6 0.43607 74119 7.24629 59522 E-01 0.87640 13344
1.33584 90740 1.57067 32032 E-01 0.93558 05576
2.35060 49736 4.53000 99055 E-03 1.13690 83326
8 0.38118 69902 6.61147 01255 E-01 0.81026 46175
1.15719 37124 2.07802 32581 E-01 0.82868 73032
1.98165 67566 1.70779 83007 E-02 0.89718 46002
2.93063 74202 1.99604 07221 E-04 1.10133 07296
10 0.34290 13272 6.10862 63373 E-01 0.68708 18539
1.03661 08297 2.40138 61108 E-01 0.70329 63231
1.75668 36492 3.38743 94455 E-02 0.74144 19319
2.53273 16742 1.34364 57467 E-03 0.82066 61264
3.43615 91188 7.64043 28552 E-06 1.02545 16913
16 0.27348 10461 5.07929 47901 E-01 0.54737 52050
0.82295 14491 2.80647 45852 E-01 0.55244 19573
1.38025 85391 8.38100 41398 E-02 0.56321 78290
1.95178 79909 1.28803 11535 E-02 0.58124 72754
2.54620 21578 9.32284 00862 E-04 0.60973 69582
3.17699 91619 2.71186 00925 E-05 0.65575 56728
3.86944 79048 2.32098 08448 E-07 0.73824 56222
4.68873 89393 2.65480 74740 E-10 0.93687 44928
20 0.24534 07083 4.62243 66960E-01 0.49092 15006
0.73747 37285 2.86675 50536E-01 0.49384 33852
1.23407 62153 1.09017 20602E-01 0.49992 08713
1.73853 77121 2.48105 20887E-02 0.50967 90271
2.25497 40020 3.24377 33422E-03 0.52408 03509
2.78880 60584 2.28338 63601E-04 0.54485 17423
3.34785 45673 7.80255 64785E-06 0.57526 24428
3.94476 40401 1.08606 93707E-07 0.62227 86961
4.60368 24495 4.39934 09922E-10 0.70433 29611
5.38748 08900 2.22939 36455E-13 0.89859 19614Bibliography
[1] ABERTH, O., Introduction to Precise Numerical Methods. Academic Press, 2007.
[2] ABRAMOWITZ, M., STEGUN, I. A., Handbook of Mathematical Functions. U.S Gov￾ernment Printing Office, Washington, D.C., 1964.
[3] ACTON, S. F., Numerical Methods That Work. Harper & Row Publishers, New York,
1970.
[4] AKAI, T. J., Applied Numerical Methods for Engineers. John Wiley & Sons, Inc.,
New York, 1994.
[5] ALLEN, M. P., Understanding Regression Analysis. Plenum Press, 1997.
[6] AMES, F. W., Numerical Methods for Partial Differential Equations. Academic Press,
1977.
[7] ANDERSON, A. D., TANNEHILL, C. J., PLETCHER, H. R., Computational Fluid
Mechanics and Heat Transfer. Hemisphere Publishing Corporation, New Work, 1984.
[8] ANTON, H., RORRES, C., KAUL, A., Elementary Linear Algebra: Applications Ver￾sion. John Wiley & Sons, 2019.
[9] ASCHER, U. M., GRIEF, C., A First Course in Numerical Methods (Computational
Science and Engineering Edition). Society for Industrial and Applied Mathematics
(SIAM), 2011.
[10] ATKINSON, K. E., An Introduction to Numerical Analysis. John Wiley & Sons, 1989.
[11] ATKINSON, K. E., HAN, W., STEWART, D. E., Numerical Solution of Ordinary
Differential Equations. John Wiley & Sons, Hoboken, New Jersey, 2009.
[12] AYYUB, B. M., MCCUEN, R. H., Numerical Analysis for Engineers: Methods and
Applications, Second Edition (Textbooks in Mathematics). Chapman and Hall/CRC,
2015.
[13] AXELSSON, O., Iterative Solution Methods. Cambridge University Press, 1996.
[14] BAILY, P. B., SHAMPİNE, L. F., WALTMAN, P. E., Nonlinear Two Point Boundary
Value Problems (Mathematics in Science and Engineering). Academic Press, 1968.
[15] BARRETT, R., BERRY, M., CHAN, T. F., DEMMEL, J., DONATO, J., DON￾GARRA, J., EIJKHOUT, V., POZO, R., ROMINE, C., VAN DER VORST, H.,
Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods
(Miscellaneous Titles in Applied Mathematics Series No 43). Society for Industrial
and Applied Mathematics (SIAM), 1987.
[16] BEERS, K. J., Numerical Methods for Chemical Engineering: Applications in MAT￾LAB. Cambridge University Press, 2006.
737738  Bibliography
[17] BOBER, W., Introduction to Numerical and Analytical Methods with MATLAB for
Engineers and Scientists, CRC Press, 2013.
[18] BORZI, A., Modelling with Ordinary Differential Equations: A Comprehensive Ap￾proach. Chapman & Hall/CRC Numerical Analysis and Scientific Computing Series,
CRC Press, 2020.
[19] BOSE, S. K., Numerical Methods of Mathematics Implemented in Fortran. Springer,
Singapore, 2019.
[20] BOYCE, W. E., DIPRIMA, R. C., MEADE, D. B., Elementary Differential Equations
and Boundary Value Problems. Wiley, 2017.
[21] BRADIE, B., A Friendly Introduction to Numerical Analysis. Pearson Prentice Hall,
2006.
[22] BRONSON, R., COSTA, G. B., Matrix Methods: Applied Linear Algebra and Saber￾metrics. Academic Press, 2020.
[23] BROOKS, D. R., Problem Solving with Fortran 90: For Scientists and Engineers.
Springer, New York, 2012.
[24] BURDEN, R. L., FAIRES, J. D., BURDEN, A. M., Numerical Analysis, 10th ed.
Cengage Learning, 2015.
[25] BUTCHER, J. C., Numerical Methods for Ordinary Differential Equations. John Wi￾ley, 2016.
[26] BUTENKO, S., PARDALOS, P.M., Numerical Methods and Optimization: An Intro￾duction. Chapman & Hall/CRC Numerical Analysis and Scientific Computing Series,
Taylor & Francis, 2014,
[27] CARNAHAN, B., LUTHER, H. A., WILKES, J. O., Applied Numerical Methods.
John Wiley, 1969.
[28] CHAPRA, S., CANALE, R., Numerical Methods For Engineers. 7th ed. McGraw-Hill
Education, Boston, 2020.
[29] CHENEY, E. W., KINCAID, D. R., Numerical Mathematics and Computing. 7th ed.
Cengage Learning, 2012.
[30] CLARC, M. Jr., HANSEN, F. K., Numerical Methods of Reactor Analysis. Academic
Press, New York, 1964.
[31] CLINE, A. K., MOLER, C. B., STEWART, G. W., WILKINSON, J. H., An Estimate
for the Condition Number of a Matrix. SIAM Journal on Numerical Analysis, 16(2),
368-375. http://www.jstor.org/stable/2156842
[32] COKER, A. K., Fortran Programs for Chemical Process Design, Analysis, and Sim￾ulation. Gulf Professional Publishing, 1995.
[33] CONTE, S. D., BOOR, C. D., Elementary Numerical Analysis: An Algorithmic Ap￾proach. Society for Industrial and Applied Mathematics (SIAM), 2017.
[34] CRANK, J., MARTIN H. G., MELLUIS, D. M., Numerical Solution of Partial Dif￾ferential Equations. Oxford University Press, 1978.
[35] CURTISS, C. F., HIRSCHFELDER, J. O. (1952). Integration of Stiff Equations,
Proceedings of the National Academy of Sciences of the United States of America,
38(3), 235-243. http://www.jstor.org/stable/88864Bibliography  739
[36] ÇENGEL, Y. A., Heat Transfer: A Practical Approach. McGraw-Hill Higher Educa￾tion, 1998.
[37] ÇENGEL, Y. A., Thermodynamics: An Engineering Approach. McGraw-Hill Higher
Education, 2008.
[38] DAHLQUIST, G., BJÖRCK, A., Numerical Methods in Scientific Computing: Volume
II. Society for Industrial and Applied Mathematics (SIAM), 2008.
[39] DAVIS, H. T., THOMSON, K. T., Linear Algebra and Linear Operators in Engineer￾ing: With Applications in Mathematica. Academic Press, 2000.
[40] DAVIS, P. J., RABINOWITZ, P., Methods of Numerical Integration, 2nd ed. Dover
Publications, 2007.
[41] DAVIS, G. V., Numerical Methods in Engineering and Science. Springer, 1986.
[42] DAVIS, M. E., Numerical Methods and Modeling for Chemical Engineers. John Wiley
& Sons, 1984.
[43] DAVIS, M. E., DAVIS, R. J., Fundamentals of Chemical Reaction Engineering. Mac￾Graw Hill Inc., 2003.
[44] DE COSTER, C., HABETS, P., Two-Point Boundary Value Problems: Lower and
Upper Solutions. Elsevier Science, 2006.
[45] DEMMEL, J. W., Applied Numerical Linear Algebra. Society for Industrial and Ap￾plied Mathematics (SIAM), 1997.
[46] DONGARRA, J. J., DUFF, I. S., SORENSEN, D. C., VAN DER VORST, H. A.,
Numerical Linear Algebra on High-Performance Computers (Software, Environments
and Tools, Series Number 7). Society for Industrial and Applied Mathematics (SIAM),
1998.
[47] DORFMAN, K. D., DAOUTIDIS, P., Numerical Methods with Chemical Engineer￾ing Applications (Cambridge Series in Chemical Engineering). Cambridge University
Press, 2017.
[48] DRISCOLL, T. A., BRAUN, R. J., Fundamentals of Numerical Computation. Society
for Industrial & Applied Mathematics (SIAM), 2017.
[49] DUBIN, D., Numerical and Analytical Methods for Scientists and Engineers, Using
Mathematica. Wiley-Interscience, 2003.
[50] EPPERSON, J. F., An Introduction to Numerical Methods and Analysis. Wiley, 2013.
[51] ESFANDIARI, R. S., Numerical Methods for Engineers and Scientists Using MAT￾LAB. CRC Press, 2017.
[52] FARRASHKHALVAT, M., MILES, J. P., Basic Structured Grid Generation: With an
Introduction to Unstructured Grid Generation. Butterworth-Heinemann, 2003.
[53] FAUL, A. C., A Concise Introduction to Numerical Analysis. CRC Press, 2018.
[54] FORD, W., Numerical Linear Algebra with Applications: Using MATLAB. Academic
Press, 2014.
[55] FORSYTHE, G. E., MALCOLM, M. A., MOLER, C. B., Computer Methods for
Mathematical Computations (Prentice-Hall series in automatic computation). Pren￾tice Hall, 1977.740  Bibliography
[56] FROST, J., Regression Analysis: An Intuitive Guide for Using and Interpreting Linear
Models. Statistics By Jim Publishing, 2020.
[57] FRÖBERG, C., Introduction to Numerical Analysis. Second Edition, Addison-Wesley
Publishing Company, Massachusetts, 1974.
[58] GANDER, W., GANDER, M. J., KWOK, F., Scientific Computing—An Introduction
using Maple and MATLAB (Texts in Computational Science and Engineering, 11).
Springer, 2014.
[59] GAUTSCHI, W., Numerical Analysis. Birkhäuser, 2011.
[60] GEAR, C. W., Numerical Initial Value Problems in Ordinary Differential Equations
(Automatic Computation). Prentice Hall, 1971.
[61] GERALD, C. F., WHEATLEY, P. O., Applied Numerical Analysis, 6th Ed. Addison￾Wesley Publishing Co., 1998.
[62] GEZERLIS, A., Numerical Methods in Physics with Python. Cambridge University
Press, 2020.
[63] GIESELA, E. M., UHLIG, F., Numerical Algorithms with C. Springer, 2014.
[64] GILAT, A., SUBRAMANIAM, V., Numerical Methods for Engineers and Scientists:
An Introduction with Applications Using MATLAB. Wiley, 2013.
[65] GOCKENBACH, M. S., Partial Differential Equations: Analytical and Numerical
Methods. 2nd ed. Society for Industrial and Applied Mathematics (SIAM), 2010.
[66] GOLUB, G. H., ORTEGA, J. M., Scientific Computing and Differential Equations:
An Introduction to Numerical Methods. Academic Press, San Diego, 1992.
[67] Golub, G. H., Van Loan, C. F., Matrix Computations. (Johns Hopkins Studies in the
Mathematical Sciences, 3), Johns Hopkins University Press, 2013.
[68] GREENBAUM, A., CHARTIER, T. P., Numerical Methods: Design, Analysis, and
Computer Implementation of Algorithms. Princeton University Press, 2012.
[69] GREENBAUM, A., Iterative Methods for Solving Linear Systems. Society for Indus￾trial and Applied Mathematics (SIAM), 1997.
[70] GREENSPAN, D., Discrete Numerical Methods in Physics and Engineering. Aca￾demic Press, 1974.
[71] GREWAL, B. S., Numerical Methods in Engineering and Science: with Programs in
C and C++. Khanna Publishers, 2010.
[72] GRIFFITHS, D. V., SMITH, I. M., Numerical Methods for Engineers. Chapman and
Hall/CRC, 2006.
[73] HACKBUSCH, W., Iterative Solution of Large Sparse Systems of Equations. Applied
mathematical sciences, Springer-Verlag, 1994.
[74] HAGEMAN, L. A., YOUNG, D. M., Applied Iterative Methods. Dover Publications,
2004.
[75] HAMMING, R. W., Introduction to Applied Numerical Analysis (Dover Books on
Mathematics). Dover Publications, 2012.
[76] HEARN, E. J., Mechanics of Materials Volume 1: An Introduction to the Mechanics
of Elastic and Plastic Deformation of Solids and Structural Materials. Butterworth￾Heinemann, 2000.Bibliography  741
[77] HEARN, E. J., Mechanics of Materials Volume 2: The Mechanics of Elastic and Plas￾tic Deformation of Solids and Structural Materials. Butterworth-Heinemann, 2000.
[78] HAMMING, R. W., Numerical Methods for Scientists and Engineers. Dover Publica￾tions, 1987.
[79] HENRICI, P., Elements of Numerical Analysis. Wiley, 1964.
[80] HESTENES, M. R., STIEFEL, E., Methods of conjugate gradients for solving linear
systems. Journal of Research of the National Bureau of Standards, 49, 409-435 (1952).
[81] HIGHAM, N. J., Accuracy and Stability of Numerical Algorithms. Society for Indus￾trial and Applied Mathematics (SIAM), 2002.
[82] HILDEBRAND, F. B., Introduction to Numerical Analysis. 2nd ed. Dover Books on
Mathematics), Dover Publications, 1987.
[83] HOFFMAN, J. D., Numerical Methods for Engineers and Scientists. 2nd ed., McGraw￾Hill Inc., New York, 1992.
[84] HORNBECK, W. R., Numerical Methods (With Numerous Examples and Solved Il￾lustrative Problems). Quantum Publishers, New York, 1975.
[85] HOROWITZ, E., SAHNI, S., RAJASEKARAN, S., Computer Algorithms. Computer
Science Press, 1997.
[86] ISAACSON, E., KELLER, H. B., Analysis of Numerical Methods. Dover Publishing
Co., 1994.
[87] ISERLES, A., A First Course in the Numerical Analysis of Differential Equations.
Cambridge University Press, 1996.
[88] JANA, A. K., Chemical Process Modelling and Computer Simulation. PHI Learning
Ltd., 2018.
[89] JOHNSON, L., RIESS, D., ARNOLD, J., Introduction to Linear Algebra (Classic
Version) (Pearson Modern Classics for Advanced Mathematics Series). Pearson, 2017.
[90] JOHNSTON, R. L., Numerical Methods: A Software Approach. John Wiley & Sons,
1982.
[91] KAHANER, David., MOLER, C., NASH, S. Numerical Methods and Software.
Prentice-Hall, 1989.
[92] KELLY, C. T. Iterative Methods for Linear and Nonlinear Equations. (Frontiers in
Applied Mathematics, Series Number 18), Society for Industrial and Applied Mathe￾matics (SIAM), 1995.
[93] KHARAB, A., GUENTHER, R., An Introduction to Numerical Methods. CRC Nu￾merical Analysis and Scientific Computing Series. CRC Press, 2021.
[94] KHOURY, R., HARDER, D. W., Numerical Methods and Modelling for Engineering.
Springer Int. Pub., 2016.
[95] KIUSALAAS, J., Numerical Methods in Engineering with Python 3. Cambridge Uni￾versity Press, 2013.
[96] KNUTH, D.E., The Art of Computer Programming: Fundamental algorithms.
Addison-Wesley Publishing Company, 1973.
[97] KOMZSIK, L., Approximation Techniques for Engineers. CRC Press, Taylor & Francis
Group, 2020.742  Bibliography
[98] KUO, S. S., Computer Applications of Numerical Methods. Addison-Wesley Publishing
Company, Massachusetts, 1972.
[99] LAMARSH, J. R., Introduction to Nuclear Engineering. Addison-Wesley, 1983.
[100] LAMBERT, J. D., Numerical Methods for Ordinary Differential Systems: The Initial
Value Problem. Wiley, 1991.
[101] LANDAU, R. H., MEJI, M. J. P., Computational Physics: Problem Solving with Com￾puters. Wiley, 2007.
[102] LARSON, R., FALVO, D. C., Elementary Linear Algebra. Brooks Cole Learning, 2008.
[103] LEVEQUE, R. J., Finite Difference Methods for Ordinary and Partial Differential
Equations: Steady-State and Time-dependent Problems. Society for Industrial and Ap￾plied Mathematics (SIAM), 2007.
[104] LINDFIELD, G., PENNY, J., Numerical Methods: Using MATLAB. Academic Press,
2018.
[105] LUYBEN W.L., Process Modeling, Simulation And Control For Chemical Engineers.
2nd ed. McGraw Hill Exclusive (Cbs), 2014.
[106] MARDEN, M., Geometry of Polynomials. American Mathematical Society, 1970,
[107] MARON, M. J., LOPEZ, R. J., Numerical Analysis: A Practical Approach. Wadsworth
Pub. Co., 1991,
[108] MATTHEIJ, R. M. M., RIENSTRA, S. W., TEN THIJE BOONKKAMP, J. H. M.,
Partial Differential Equations: Modeling, Analysis, Computation. (Monographs on
Mathematical Modeling and Computation, Series Number 10). Society for Industrial
and Applied Mathematics (SIAM), 1987.
[109] MEURANT, G., Computer Solution of Large Linear Systems. Elsevier Science, 1999.
[110] MCNAMEE, J. M., Numerical Methods for Roots of Polynomials—Part I. Elsevier
Science, 2013,
[111] MCNAMEE, J. M., PAN, V., Numerical Methods for Roots of Polynomials - Part II.
(Studies in Computational Mathematics, Volume 16), Elsevier Science, 2018,
[112] MESEGUER, A., Fundamentals of Numerical Mathematics for Physicists and Engi￾neers. Wiley, 2020,
[113] MIDDLETON, J.A., Experimental Statistics and Data Analysis for Mechanical and
Aerospace Engineers. Advances in Applied Mathematics, CRC Press, 2021.
[114] MILLER, G., Numerical Analysis for Engineers and Scientists. Cambridge University
Press, 2014.
[115] MOIN, P., Fundamentals of Engineering Numerical Analysis. Cambridge University
Press, 2010.
[116] MULLER, J. M., BRUNIE, N., DE DINECHIN, F., JEANNEROD, C. P., JOLDES,
M., LEFÈVRE, V., MELQUIOND, G., REVOL, N., TORRES, S., Handbook of
Floating-Point Arithmetic. Springer International Publishing, 2018.
[117] MYLER, H. R., Fundamentals of Engineering Programming with C and Fortran. Cam￾bridge University Press, 1998.
[118] NA, T. Y., Computational Methods in Engineering Boundary Value Problems. Aca￾demic Press, 2012.Bibliography  743
[119] NA, T. Y., SIDHOM, M. M. On Stokes’ Problems for Linear Viscoelastic Fluids.
ASME. J. Appl. Mech. 34(4), 1040–1042 (1967). https://doi.org/10.1115/1.3607818
[120] NASH, J. C., Compact Numerical Methods for Computers: Linear Algebra and Func￾tion Minimisation. CRC Press, 1990.
[121] NERI, F., Linear Algebra for Computational Sciences and Engineering. Springer,
2019.
[122] NEUMAIER, A., Introduction to Numerical Analysis. Cambridge University Press,
2001.
[123] NILSSON, J. W., RIEDEL, S. A., Electric Circuits. Prentice Hall, 2011.
[124] NORMAN, D., WOLCZUK, D., Introduction to Linear Algebra for Science and En￾gineering. 2nd ed. Pearson Education Canada, 2011.
[125] NYHOFF, L.R., LEESTMA, S., FORTRAN 77 for Engineers and Scientists: With an
Introduction to FORTRAN 90. Prentice Hall, 1996.
[126] O’LEARY, D. P. Estimating Matrix Condition Numbers. SIAM Journal on Scientific
and Statistical Computing, 1(2), 205–209 (1980). DOI: 10.1137/0901014
[127] ORTEGA, J. M., Numerical Analysis: A Second Course. Society for Industrial and
Applied Mathematics, 1990.
[128] PARLETT, B. N., The Symmetric Eigenvalue Problem. (Prentice-Hall series in Com￾putational Mathematics), Prentice-Hall, Englewood Cliffs, NJ, 1980.
[129] PHILLIPS, G. M., TAYLOR, P. J., Theory and Applications of Numerical Analysis.
Academic Press, 1973.
[130] PINDER, G. F., Numerical Methods for Solving Partial Differential Equations: A
Comprehensive Introduction for Scientists and Engineers. Wiley, 2018.
[131] POWERS, D. L., Boundary Value Problems: and Partial Differential Equations. Aca￾demic Press, 2009.
[132] PRESS, W. H., TEUKOLSKY, S. A., VETTERLING, W. T., FLANNERY, B. P.,
Numerical Recipes in Fortran 90: Volume 2, Volume 2 of Fortran Numerical Recipes:
The Art of Parallel Scientific Computing. Cambridge University Press, 1996.
[133] QUARTERONI, A., SACCO, R., SALERI, F., Numerical Mathematics. Texts in Ap￾plied Mathematics, Springer Berlin Heidelberg, 2006.
[134] RALSTON, A., RABINOWITZ, P., A First Course in Numerical Analysis. 2nd ed.
(Dover Books on Mathematics), Dover Publications, 2001.
[135] RAO, S., Applied Numerical Methods for Engineers and Scientists. Pearson, 2001.
[136] RAO, S. G., Numerical Analysis. New Age International, 2018.
[137] RASMUSON, A., ANDERSSON, B., OLSSON, L., ANDERSSON, R., Mathematical
Modeling in Chemical Engineering. Cambridge University Press, 2014.
[138] RAY, S. S., Numerical Analysis with Algorithms and Programming. Chapman and
Hall/CRC, 2016.
[139] REID, J. K., A Method for Finding the Optimum Successive Over￾Relaxation Parameter, The Computer Journal, 9(2), 200–204 (1966).
https://doi.org/10.1093/comjnl/9.2.200744  Bibliography
[140] RHEINBOLDT, W. C., Methods for Solving Systems of Nonlinear Equations. (CBMS￾NSF Regional Conference Series in Applied Mathematics, Series Number 70), Society
for Industrial and Applied Mathematics (SIAM), 1987.
[141] RICE, R. G., DO, D. D., Applied Mathematics And Modeling For Chemical Engineers.
Wiley-AIChE, 2012.
[142] RICE, R. J., Numerical Methods, Software, and Analysis. McGraw-Hill Book Com￾pany, New York, 1983.
[143] RIDGWAY, S. L., Numerical Analysis. Princeton University Press, 2011.
[144] RIVLIN, T.J., An Introduction to the Approximation of Functions. Dover Publica￾tions, 1981.
[145] SAAD, Y., Iterative Methods for Sparse Linear Systems. (The Pws Series in Computer
Science), Pws Pub Co, 1996.
[146] SALVADORI, G. M., BARON, L. M., Numerical Methods in Engineering. Prentice￾Hall of India LTD., New Delhi, 1966.
[147] SAUER, T., Numerical Analysis. Pearson, 2017.
[148] SCHEID, F., Numerical Analysis. McGraw-Hill Book Company, New York, 1989.
[149] SCHROEDER, L.D., SJOQUIST, D.L., STEPHAN, P.E., Understanding Regression
Analysis: An Introductory Guide. Quantitative Applications in the Social Sciences,
SAGE Publications, 1986.
[150] SHAMPINE, L. F., ALLEN, R. C., PRUESS, S., Fundamentals of Numerical Com￾puting. Wiley, 1997.
[151] SIAUW, T., BAYEN, A., An Introduction to MATLAB Programming and Numerical
Methods for Engineers. Academic Press, 2014.
[152] SMITH, G. D., Numerical Solution of Partial Differential Equations: Finite Difference
Methods (Oxford Applied Mathematics and Computing Science Series). Clarendon
Press, 1986.
[153] SPIEGEL, M., Schaum’s Outline of Calculus of Finite Differences and Difference
Equations. McGraw Hill, 1971.
[154] STEPHENS, M. A., D’AGOSTINO, R. B., Goodness-of-Fit-Techniques. Statistics: A
Series of Textbooks and Monographs, Taylor & Francis, 1986.
[155] STERLING, M. J., Linear Algebra For Dummies. Wiley, 2009.
[156] STEWART, G. W., Matrix Algorithms: Volume 1, Basic Decompositions. SIAM: So￾ciety for Industrial and Applied Mathematics, 1998.
[157] STEWART, G. W., Matrix Algorithms: Volume 2, Eigensystems. SIAM: Society for
Industrial and Applied Mathematics, 2001.
[158] STOER, J., BULIRSCH, R., Introduction to Numerical Analysis., 2nd ed. Springer
Verlag, 1993.
[159] STEWART, G. W., Matrix Algorithms: Volume 1, Basic Decompositions. Society for
Industrial and Applied Mathematics (SIAM), 1998.
[160] STEWART, G. W., Matrix Algorithms: Volume 2, Eigensystems. Society for Industrial
and Applied Mathematics (SIAM), 2001.Bibliography  745
[161] TOBIA, M.J., Matrices in Engineering Problems. Synthesis digital library of engi￾neering and computer science, Morgan & Claypool Publishers, 2011.
[162] THOMPSON, J. F., SONI, B. K., WEATHERILL, N.P. (Eds.) Handbook of Grid
Generation. 1st ed. CRC Press, 1998.
[163] TREFETHEN, L. N., BAU, D. III., Numerical Linear Algebra. Society for Industrial
and Applied Mathematics (SIAM), 1997.
[164] TURNER, P. R., Guide to Numerical Analysis (CRC Mathematical Guides). CRC
Press, 1990.
[165] YANG, Y. W., CAO, W., KIM, J., PARK, K. W., PARK, H., JOUNG, J., Applied
Numerical Methods Using Matlab. 2nd Ed., Wiley, 2020.
[166] VAN LOAN, C., COLEMAN, T. F., Handbook for Matrix Computations (Frontiers
in Applied Mathematics). Society for Industrial and Applied Mathematics (SIAM),
1987.
[167] VARGA, R. S., Matrix Iterative Analysis. Prentice Hall Inc., 1962.
[168] VINOKUR, M., On one-dimensional stretching functions for finite-difference calcula￾tions. Journal of Computational Physics, 50(2), 215–234 (1980). DOI: 10.1016/0021-
9991(83)90065-7
[169] VORST, H. A. van der. Iterative Krylov Methods for Large Linear Systems. Cambridge
University Press, 2003.
[170] XIE, W.C., Differential Equations for Engineers. Cambridge University Press, 2010.
[171] WAIT, R., The Numerical Solution of Algebraic Equations. John Wiley & Sons, 1979.
[172] WATKINS, D. S., The Matrix Eigenvalue Problem: GR and Krylov Subspace Methods.
Society for Industrial and Applied Mathematics (SIAM), 2008.
[173] WATKINS, D. S., Fundamentals of Matrix Computations. Wiley, 2010.
[174] WILKINSON, J. H., The Algebraic Eigenvalue Problem. Numerical Mathematics and
Scientific Computation, Clarendon Press, 1988.
[175] WILKINSON, J. H., REINSCH, C., Handbook for Automatic Computation, Vol. 2:
Linear Algebra (Grundlehren Der Mathematischen Wissenschaften, Vol. 186). Edi￾tor:Bauer, F. L., Springer Verlag, 1971.
[176] WILKINSON, J. H., Rounding Errors in Algebraic Processes. Hassell Street Press,
2021.
[177] WILKINSON, J. H, REINSCH, C., BAUER, F. L., Handbook for Automatic Compu￾tation: Linear Algebra (Grundlehren Der Mathematischen Wissen-schaften, Vol 186)
Springer Verlag, 1986.
[178] YOUNG, D. F., MUNSON, B. R., OKIISHI, T. H., A Brief Introduction to Fluid
Mechanics. 5th ed. Wiley, 2011.
[179] YOUNG., D. M., Iterative Solution of Large Linear Systems. Academic Press, New
York, 1971.
[180] ZALIZNIAK, V., Essentials of Scientific Computing: Numerical Methods for Science
and Engineering. Woodhead Publishing, 2008.
[181] ZAROWSKI, C. J., An Introduction to Numerical Analysis for Electrical and Com￾puter Engineers. Wiley-Interscience, 2004.746  Bibliography
[182] ZILL, D. G., CULLEN, M. R., Differential Equations with Boundary-Value Problems.
7th ed. Cengage Learning, 2009.
[183] ZILL, D. G., A First Course in Differential Equations with Applications. Brooks/Cole,
Cengage Learning, 2012.References
[1] ABRAMOWITZ, M., STEGUN, I.A., Handbook of Mathematical Functions. U.S Gov￾ernment Printing Office, Washington, D.C., 1964.
[2] AKAI, T. J., Applied Numerical Methods for Engineers. John Wiley & Sons, Inc., New
York, 1994.
[3] AMES, F. W., Numerical Methods for Partial Differential Equations. Academic Press,
1977.
[4] AXELSSON, O., Iterative Solution Methods. Cambridge University Press, 1996.
[5] BARRETT, R., BERRY, M., CHAN, T. F., DEMMEL, J., DONATO, J., DON￾GARRA, J., EIJKHOUT, V., POZO, R., ROMINE, C., VAN DER VORST, H., Tem￾plates for the Solution of Linear Systems: Building Blocks for Iterative Methods (Mis￾cellaneous Titles in Applied Mathematics Series No 43). Society for Industrial and
Applied Mathematics (SIAM), 1987.
[6] CURTISS, C. F., HIRSCHFELDER, J. O., Integration of Stiff Equations. Proceedings
of the National Academy of Sciences of the United States of America, 38(3), 235–243
(1952). http://www.jstor.org/stable/88864
[7] FARRASHKHALVAT, M., MILES, J. P., Basic Structured Grid Generation: With an
Introduction to Unstructured Grid Generation. Butterworth-Heinemann, 2003.
[8] GREENBAUM, A., Iterative Methods for Solving Linear Systems. Society for Industrial
and Applied Mathematics (SIAM), 1997.
[9] GOLUB, G. H., Van der VORST, H. A., Eigenvalue computation in the 20th century.
Journal of Computational and Applied Mathematics, 123, 35–65 (2000).
[10] GUGGENHEIMER, H. W., EDELMAN, A. S., JOHNSON, C. R., A Simple Estimate
of the Condition Number of a Linear System. The College Mathematics Journal, 26(1),
2–5 (1995). DOI: 10.1080/07468342.1995.11973657
[11] HACKBUSCH, W., Iterative Solution of Large Sparse Systems of Equations. Applied
mathematical sciences, Springer-Verlag, 1994.
[12] HAGEMAN, L. A., YOUNG, D. M., Applied Iterative Methods. Dover Publications,
2004.
[13] HART, J. J., A Correction for the Trapezoidal Rule. The American Mathematical
Monthly, 59(1), 33–37 (1952). https://doi.org/10.2307/2307187
[14] HESTENES, M. R., STIEFEL, E., Methods of conjugate gradients for solving linear
systems. Journal of Research of the National Bureau of Standards, 49, 409–435 (1952).
[15] IEEE Standard for Binary Floating-Point Arithmetic. in ANSI/IEEE Std 754-1985,
Oct. (1985). DOI: 10.1109/IEEESTD.1985.82928.
747748  References
[16] IEEE Standard for Binary Floating-Point Arithmetic (Revision of IEEE 754-2008).
IEEE Std 754 (2019). DOI: 10.1109/IEEESTD.2019.8766229
[17] ISAACSON, E., KELLER, H. B., Analysis of Numerical Methods. Dover Publishing
Co., 1994.
[18] KELLY, C. T. Iterative Methods for Linear and Nonlinear Equations. (Frontiers in Ap￾plied Mathematics, Series Number 18), Society for Industrial and Applied Mathematics
(SIAM), 1995.
[19] KIUSALAAS, J., Numerical Methods in Engineering with Python 3. Cambridge Uni￾versity Press, 2013.
[20] LAMBERT, J. D., Numerical Methods for Ordinary Differential Systems: The Initial
Value Problem. Wiley, 1991.
[21] LANCZOS, C.,Applied Analysis, Dover Books on Mathematics, Dover Publications,
1988.
[22] LETHER, F. G., WENSTON, P. R., Minimax approximations to the zeros of Pnpxq
and Gauss-Legendre quadrature. Journal of Computational and Applied Mathematics,
59(2) 245–252 (1995).
[23] MCNAMEE, J. M., Numerical Methods for Roots of Polynomials—Part I. Elsevier
Science, 2013.
[24] MCNAMEE, J. M., PAN, V., Numerical Methods for Roots of Polynomials—Part II
(Volume 16) (Studies in Computational Mathematics, Volume 16). Elsevier Science,
2018.
[25] MEURANT, G., Computer Solution of Large Linear Systems. Elsevier Science, 1999.
[26] NA, T. Y., SIDHOM, M. M., On Stokes’ Problems for Linear Viscoelas￾tic Fluids. ASME Journal of Applied Mechanics 34(4), 1040–1042 (1967).
https://doi.org/10.1115/1.3607818
[27] ROHSENOW, W. M., HARTNETT, J. P., YOUNG, I. C, eds. Handbook of Heat Trans￾fer. 3rd ed. McGraw-Hill Education, New York: 1998.
[28] PRESS, W. H., TEUKOLSKY, S. A., VETTERLING, W. T., FLANNERY, B. P.,
Numerical Recipes in Fortran 90: Volume 2, Volume 2 of Fortran Numerical Recipes:
The Art of Parallel Scientific Computing. Cambridge University Press, 1996.
[29] SAAD, Y., Iterative Methods for Sparse Linear Systems (The Pws Series in Computer
Science). Pws Pub Co, 1996.
[30] TAKEMASA, T., Abscissae and weights for the Gauss-Laguerre quadrature formula.
Computer Physics Communications, 52(1), 133–140 (1988).
[31] TAKEMASA, T., Abscissae and weights for the Gauss-Hermite quadrature formula.
Computer Physics Communications, 48, 265–270 (1988).
[32] THOMPSON, J. F., SONI, B. K., WEATHERILL, N. P. (Eds.) Handbook of Grid
Generation. 1st ed. CRC Press. 1998.
[33] TREFETHEN, L. M., Liquid metal heat transfer in circular tubes and annuli. General
discussion on heat transfer, Journal of IMechE, 436–438 (1951).
[34] WILKINSON, J. H., Rounding Errors in Algebraic Processes. Hassell Street Press,
2021.
[35] VARGA, R. S., Matrix Iterative Analysis. Prentice Hall, Inc., 1962.References  749
[36] VINOKUR, M., On one-dimensional stretching functions for finite-difference calcula￾tions. Journal of Computational Physics, 50(2), 215–234 (1980). DOI: 10.1016/0021-
9991(83)90065-7
[37] VORST, H. A. van der. Iterative Krylov Methods for Large Linear Systems. Cambridge
University Press, 2003.
[38] YOUNG., D. M., Iterative Solution of Large Linear Systems. Academic Press, New
York, 1971.Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com Index
Absolute error, see Error
Accumulator, 7, 41
Accuracy, 19, 32
decimal-place, 32
singificant-digit, 32
Adams-Bashforth
Formulas, 528, 529
Method, 528–530
Adams-Bashforth-Moulton Method, 544
Adams-Moulton
Formulas, 531
Method, 532
Adaptive integration, 435–439
Simpson, 435, 436, 440
trapezoidal, 435
Adaptive size control, 540
Adjoint method, 84
Aitken’s acceleration, 226, 227, 229, 230
Algorithm, 2, 4, 5
numerical, 4
Augmented matrix, 81, 84–88, 93, 94, 96,
97, 104, 110, 118
Back substitution, 67, 91–93, 95, 99,
118–120, 124, 125, 129
Backward difference
formulas, 278, 279, 294
operator, 329, 358
second-order, 280
table, 332
truncation error, 294
Backward Differentiation
formulas, 533–535
truncation error, 534
Bairstow’s method, 237–240, 242, 244
Banded
back substitution, 125
band width, 123
compact matrix, 123
forward elimination, 125
Gauss-elimination, 124
linear system, 626
LU-decomposition, 126, 129
matrix, 156, 183
matrix indexing, 123
systems, 123, 156
Bayt, 23
Bessel’s formula, 332, see Interpolation
Bilinear interpolation, 353
Bisection method, 204–208, 210, 684
Bit, 23
Bivariate
interpolation, 353
Lagrange interpolation, 354, 355
Boundary conditions, 575, 580, 625
Dirichlet, 577, 588, 589, 601, 603, 604,
609–611, 615, 616, 619, 625–627
Mixed, 577, 588
Neumann, 577, 586, 604, 625, 628, 629
Periodic, 578
Robin, 577, 578, 580, 584, 588, 589,
602–604, 610, 625
Boundary value problems, 574
two-point, 574, 576
Cauchy’s bounds, 247
Central difference
approximation, 296
formuals, 294
formulas, 281, 289, 294, 301
higher-derivatives, 297
operator, 281, 329
order of accuracy, 285
table, 332, 333
truncation error, 294
Characteristic Value Problems, 703, 704,
706
discretizing, 705
gridding, 705
implementing BCs, 705
numerical solution, 705, 706
Cholesky decomposition, 673, 677
Cofactor, 73
Column
matrix, 65
vector, 65
Condition, 36
necessary, 170, 171
751752  Index
number, 37, 81, 106–108, 182, 185, 186,
188, 190, 192
spectral number, 185
sufficient, 170, 171
Conditionally stable, 522, 523, 535, 559
Conjugate Gradient Method, 180, 181, 183
convergence, 185
initial guess, 181
normal error, 186
normal residual, 186
preconditioning, 186
stopping criteria, 181
Conjugation parameter, 181
Constructs, 5
For, 5, 8
If-Then-Else, 5–7
Repeat-Until, 5, 7, 9
While, 5, 7, 9
branching, 5
conditional, 6
iteration, 5
repetition, 5
selection, 5
sequence, 5
sequential, 6
Continious least squares, 394–397
Convergent
sequence, 157
series, 41
Cramer’s Rule, 75
Crout LU-Decomposition, 110
Cubic spline, 341–343, 346, 348, 349
coefficients, 344, 346, 349
errors, 350
linear extrapolation end condition, 345,
346, 348, 349
natural spline end condition, 344, 346,
348, 349
Decimal
notation, 25
place, 32
Decomposition, 109
banded LU, 126
banded matrix, 127
Cholesky, 110, 115
Doolittle LU-, 110–113
matrix, 109
tridiagonal LU, 119
Determinant, 72, 73, 75, 105, 106
computing, 105
properties, 73
Diagonally dominant, 159, 162, 164, 171,
172, 189, 192
strictly, 171
weakly, 171
Differential equations
coupled first-order, 664
Direct-fit polynomials, 283, 285, 292, 295,
301
Discrete function, 270–273, 292, 299, 301,
313, 331, 350, 353, 355
non-uniformly spaced, 271, 292, 296,
313, 360
two variables, 353
uniformly spaced, 271, 278, 313, 329,
360
Division by zero, 95
Doolittle LU-Decomposition, 110
Dot product, 70, 71, 180
Double integration, 471, 472
Eigenvalue, 75
bounds, 647
largest, 80
location, 647
problem, 75
properties, 646, 647
tridiagonal matrix, 77
Eigenvector, 75, 700
properties, 646, 647
tridiagonal matrix, 77
Error, 15, 17
absolute, 17, 18, 22, 33, 39, 40, 158
average, 394
chopping, 28, 32, 35
estimation, 43
experimental, 16, 19
leading, 272, 273
measurement, 19
modeling, 15
numerical, 17
propagation formula, 20, 21
random, 19, 21
relative, 17, 18, 33, 39–41, 158
round-off, 274, 275, 277, 289, 291, 299,
301, 502
rounding, 17, 24, 28, 32, 33, 35, 159
truncation, 17, 22, 38, 43, 44, 159,
272–275, 277–286, 288, 289, 291,
294, 295
Explicit EulerIndex  753
method, 501–505
stability, 522, 523
Extrapolation, 314, 355, 358, 359
error, 356
linear end condition, 346
Faddeev-Leverrier method, 701, 702
Finite Difference Method, 576–580
constructing linear system, 580
discretization error, 589
discretizing, 578
gridding, 578
high-order solutions, 589
implementing BCs, 579
implementing Dirichlet BCs, 581
implementing Neumann BCs, 584
implementing Robin BCs, 584
Newton’s method, 614, 616
non-uniform grids, 593–595
Richardson extrapolation, 590
TIM, 610–612
Finite Volume Method, 599
constructing linear system, 602
discretizing, 600
gridding, 599
implementing BCs, 601
non-linear BVPs, 609
Fixed Point Iteration, 211, 214, 215, 226,
227, 230, 231
convergence, 212
Floating-point, 24
equality of numbers, 207
number, 23, 24
representation, 23, 25, 34
Forward
elimination, 93, 95, 125
substitution, 90, 91, 110, 119–121, 129,
132
Forward difference
formulas, 278–280, 293
operator, 329, 330
order of accuracy, 279
table, 334
Forward Euler method, 501
Fourth-order ODEs, 625
banded system, 626
constructing linear system, 626
discretizing, 626
gridding, 626
implementing BCs, 626
solving linear system, 627
Gauss-Chebyshev
method, 465, 466
polynomial, 465
quadrature, 465
Gauss-elimination, 93–95
banded systems, 124
naive, 93
pivoting, 101, 102
procedure, 83
Gauss-Hermite
global error, 461
method, 461, 465, 467
quadrature, 461
quadrature , 461
Gauss-Jordan
elimination, 84, 96
method, 83, 84, 96
reduction, 83
Gauss-Laguerre
method, 457, 458
quadrature, 457–459
Gauss-Legendre, 445
method, 445, 447, 448, 452, 468
polynomials, 446, 447
quadrature, 446–449, 456, 468
Gauss-Seidel method, 163
iteration matrix, 169
Gerschgorin
circle, 647–649
theorem, 171, 172, 647, 684
Global error
Gauss-Chebyshev method, 465
Gauss-Hermite method, 461
Gauss-Laguerre method, 458
Gauss-Legendre, 448
Gauss-Legendre method, 447, 448
open Newton-Cotes rules, 443
Romberg’s rule, 434
Simpson’s 3/8 rule, 426
Simpson’s rule, 424, 429
trapezoidal rule, 418
Goodness of fit, 374–376, 381, 383, 386,
390, 399
Gram-Schmidt Process, 694–696
Heteroscedastic, 376
Heun’s Method, 543
Higher-order ODEs, 553
Homoscedastic, 376
Horner’s rule, 245
Householder method, 677–680754  Index
IEEE standards, 24
Ignoring singularity, 456
Ill-condition, 36, 37, 105–108
Ill-conditioned, 159, 186, 189, 232, 247, 392,
393, 396
Implicit Euler
linearized, 513, 522
method, 510–513, 522, 526, 534, 557
stability, 522, 555
Improper integral, 453
Type I, 453, 454, 456, 457, 461
Type II, 454, 456
Improved Euler Method, 518
Initial Value Problem, 499
first-order, 501, 503, 508, 511, 514, 520,
529, 532, 534, 535, 542, 543, 545
higher-order, 553
Inner product, see Dot product
Integration
non-uniform functions, 443–446
variable limits, 470
Interpolation, 314
Bessel’s formula, 332, 333, 338
cubic spline, see Cubic spline
GN backward, 331, 332
GN forward, 330–334, 355, 357
linear, 314, 350, 352
Stirling’s formula, 332, 333, 338
Inverse
matrix, 676
interpolation, 350–352
matrix, see Matrix inversion
Power method, 658, 660
Iteration
equation, 204
function, 204
Iteration function, 204, 211–214, 227, 228
Jacobi method, 159, 666, 668, 670
iteration equations, 160
iteration matrix, 169
Jacobian, 232–235, 554, 555, 558, 614, 615,
617
Lagrange
bivariate interpolation, 354
interpolating polynomial, 315, 317
interpolation, 315–317, 319
interpolation error, 316
polynomials, 315
Lagrange’s bounds, 247
Least squares, see Regression
Linear ODE, 619
Linear system, 74, 87, 93, 97, 102
homogeneous, 74, 76
ill-conditioned, 105
trivial solution, 74
upper triangular, 92
Linearization
nonlinear models, 386, 388–390
Loops, 7
conditional, 7
sequential, 7
Loss of significance, 34, 36
Machine epsilon, 27
Maclaurin series, 38–41
Mantissa, 25
Matrix, 65
addition, 68
adjoint, 84
banded, 67, 156, 183
conformable, 66
decomposition, 109
dense, 155
diagonal, 66
diagonalizable, 662
Euclidean norm, 80
Frobenius norm, 80
identity, 66
infinity norm, 80
inversion, 83, 86, 87
invertible, 83, 662
lower-triangular, 66, 90, 91, 110, 115
multiplication, 71
non-singular, 83
norms, 81
operations, 81
orhogonal, 662
positive definite, 110, 180
rank, 68, 96
rectangular, 65
scalar multiplication, 69
size, 65
spectral norm, 80
square, 66
subtraction, 68
symmetric, 66
Topelitz, 78
trace, 66
transpose, 68, 84, 110, 115
tridiagonal, 67, 75, 77, 117, 119, 120Index  755
unit, 66
upper-triangular, 66, 90–93, 95, 104,
106, 109, 110
vector product, 71
zero, 66
Matrix norms, 80
L1 norm, 171
spectral radius, 170
Mean Value Theorem, 42
Method of False Position, 209, 210
Midpoint Rule, 514, 515
Milne’s Method, 546
Minor, 73
Model
exponential, 387, 388
multivariate expoential, 392
multivariate linear, 391
nonlinear, 386
parameters, 372, 375, 383, 389, 391
power, 387
saturated, 387
Modified Euler
method, 514, 515
Modified Secant method, 223
Multistep Methods, 527
Newton’s divided difference, 322, 327, 360
coefficients, 323, 327
interpolation error, 324
interpolation formula, 323
table, 323, 326
Newton-Cotes rules, 440, 442, 446
closed, 440, 442
composite, 443
open, 441, 442
Newton-Raphson method, 215, 217, 218,
246, 684
convergence, 215, 216
modified, 218, 220–222
Newton’s method, 231, 233, 236–238, 253
Nonlinear ODE, 623
Objective function, 371, 372, 375, 378,
395–397
Order of accuracy, 274, 279, 280, 282–285,
294, 296–298
Order of convergence, see Series
Orthogonal transformations, 662
Overdetermined, see System
Overfitting, 383, 385
Overflow, 83, 275
Pivoting, 83, 101
complete, 101
partial, 101
PO plot, 380, 381, 391
Polynomial reduction, 213, 245, 247,
250–252
Polynomials
direct-fit, 283
Positive definite, 182
matrix, 180, 185, 186, 189
Power method, 651, 654
convergence, 654
Inverse, 658, 660
shifted inverse, 659
with Rayleigh quotient, 654, 655
with scaling, 652
Precision, 19, 26
double, 26
single, 26
Precondition, 186, 188
Preconditioner, 186, 188
Gauss-Seidel, 189
Jacobi, 188
SOR, 189
SSOR, 189
Predictor-corrector methods, 542–544, 546
stability, 555
Pseudocode, 4, 5, 10
Function Module, 10, 11
Module, 10, 12
Recursive Function Module, 12
elements, 5
QR Iteration, 686–691, 694
R-Squared, 376, 377, 389, 399
adjusted, 379
Rate of convergence, 162, 171–173, 182
Rayleigh quotient, 654
Regression, 370
bivariate linear model, 392
linear model, 371, 375, 387, 389, 391
model, 373, 374, 376, 377, 379, 380
model selection, 373
multivariate, 391–393
non-linear, 380, 381
polynomial model, 371–373, 385
Regula Falsi Method, see Method of False
Position
Relative error, see Error
Removable singularity, 456756  Index
Residual
plot, 375, 376, 380
vector, 180
Richardson extrapolation, 296–300, 590
convergence criterion, 297
table, 299
Rmse, 379, 380, 382, 395, 399
Romberg’s
convergence criteria, 432
rule, 429–433, 435, 475
table, 431, 432, 434
Row
echelon, 67
elimination, 82, 86, 89, 94
normalize, 85
operations, 83, 85, 89, 97
pivoting, 82
reduction, 81, 86, 93, 97
scaling, 82, 85, 86, 93, 94
Runge’s phenomenon, 321, 325, 341, 349,
360, 399, 442, 443, 475
Runge-Kutta Method, 516–521, 551
first-order, 517
fourth-order, 519, 523, 540
increment function, 516
RKF45, 540
second-order, 518, 523
stability, 523
third-order, 519
Runge-Kutta-Fehlberg method, 540, 542
algorithm, 542
Sarrus’s Rule, 72
Scientific notation
normalized, 25, 31
Secant method, 222–224, 226
convergence, 226
modified, 223, 226
Series, 43
converge, 45
convergence, 43
convergence rate, 47
convergent, 43, 45
order of convergence, 45
positive, 43
rate of convergence, 45, 48
Shifted Inverse Power Method, 659
Shooting method, 619–621
Linear ODEs, 619
non-linear ODEs, 623
Significant digit, 29, 31–33
Significant figure, see Significant digit
Similarity transformations, 662
Simpson’s rule, 422, 425, 429, 437, 445, 456,
464, 469, 475
3/8-rule, 425
composite, 425, 455
end correction, 425
truncation error, 424
Simultaneous Nonlinear ODEs, 557
SOR method, 166
convergence rate, 173
iteration matrix, 169
optimum parameter, 166, 174
Stability, 36, 37, 554
Standard deviation, 380
Statements
Declare, 11
Function Module, 10
Module, 10
Program-End Program, 10
Read, 6
USES, 10, 11
Write, 14
Steffensen’s Acceleration, 227–229, 231, 253
Stiffness, 523, 554, 557
definition, 554
ratio, 554, 557
Stirling’s formula, see Interpolation
Stopping criterion, 9, 39, 158, 181, 205, 207,
213, 217, 652
CGM, 181
Sturm sequence, 683, 684
Subtraction of singularity, 455
Sufficient condition, 235
Sum of the squares
mean deviation, 375, 377, 383, 390
residuals, 375, 376, 380, 383, 390, 391,
397
Synthetic division, see Polynomial reduction
System, 23
Banded, see Banded
binary, 23, 24
decimal, 23
floating-point number, 24
hexadecimal, 23, 24
octal, 23, 24
overdetermined, 133, 397–399
tridiagonal, 117
underdetermined, 95, 100, 397
System of First-order IVPs, 550, 551Index
 757
Taylor
approximation, 38
, 41
, 233
, 242
formula, 37
polynomial, 38
remainder, 38
series, 38
, 213
, 232
, 233
, 517
, 518
, 558
Taylor Polynomial method, 506–508
Termination criteria, see Stopping criterion
Thomas algorithm, see Tridiagonal systems
Transformation of variables, 385
, 386
Trapezoidal rule, 414–422
, 429–431
, 440
,
475
, 512–514
, 522
, 543
composite, 416
, 418
end correction, 418–422
, 425
linearized, 513
stability, 522
truncation error, 418
truncaton error, 512
Triangular Systems, 90
, 91
lower, 90
upper, 91
, 92
Tridiagonal matrix
eigenvalues, 683
Tridiagonal Systems, 75
, 117
Cholesky decomposition, 120
LU-decomposition, 119
Thomas algorithm, 117
, 118
Truncation error, 38
, 272
, 502
, 509
, 528
,
532
, 534
, 540
, 546
local, 502
, 512
, 545
trapezoidal rule, 512
Uncertainity, 20
Unconditionally stable, 522
, 525
, 559
Underdetermined, see System
Underflow, 83
, 275
Unit matrix, see Matrix
Variance, 376
, 380
, 384
Vector norm, 79
, 81
Euclidean, 79
infinity, 80
L2, 79
Well-conditioned system, 186
