RANDOM PROCESS ANALYSIS WITH RRandom Process Analysis with R
Marco Bittelli
University of Bologna
Roberto Olmi
National Research Council, Italy
Rodolfo Rosa
National Research Council, ItalyGreat Clarendon Street, Oxford, OX2 6DP,
United Kingdom
Oxford University Press is a department of the University of Oxford.
It furthers the University’s objective of excellence in research, scholarship,
and education by publishing worldwide. Oxford is a registered trade mark of
Oxford University Press in the UK and in certain other countries
© Marco Bittelli, Roberto Olmi, and Rodolfo Rosa 2022
The moral rights of the authors have been asserted
Impression: 1
All rights reserved. No part of this publication may be reproduced, stored in
a retrieval system, or transmitted, in any form or by any means, without the
prior permission in writing of Oxford University Press, or as expressly permitted
by law, by licence or under terms agreed with the appropriate reprographics
rights organization. Enquiries concerning reproduction outside the scope of the
above should be sent to the Rights Department, Oxford University Press, at the
address above
You must not circulate this work in any other form
and you must impose this same condition on any acquirer
Published in the United States of America by Oxford University Press
198 Madison Avenue, New York, NY 10016, United States of America
British Library Cataloguing in Publication Data
Data available
Library of Congress Control Number: 2022935991
ISBN 978–0–19–886251–2 (hbk)
ISBN 978–0–19–886252–9 (pbk)
DOI: 10.1093/oso/9780198862512.001.0001
Printed and bound by
CPI Group (UK) Ltd, Croydon, CR0 4YY
Links to third party websites are provided by Oxford in good faith and
for information only. Oxford disclaims any responsibility for the materials
contained in any third party website referenced in this work.What men really want is not knowledge but certainty
Bertrand RussellPreface
A random or stochastic process is a process that can be defined by random variables.
In other words, it is a process involving observations whose outcome at every time
instant is not certain. Mathematically, this reflects in working with functions whose
arguments are characterized by a probability distribution instead of having a certain
value.
The choice of describing a phenomenon by deterministic or probabilistic laws may
depend upon several reasons. The phenomenon appears to be conceptualized only as a
random process. On the other hand, we know that it can be described by deterministic
laws, but due to lack of knowledge about system parameters or high complexity, we
decide to model it as a random process.
Overall, what is a random process? Is randomness an inherent feature of nature
or simply our inability to describe it with deterministic laws? Therefore, is there an
inherent randomness in nature or is it our lack of knowledge that bring us to describe
it as a random process?
These differences can be referred to the ‘objective’ and ‘subjective’ viewpoints of
randomness. The first viewpoint considers randomness an inherent feature of nature,
while the second conceptualizes it as an ‘anthropomorphic’–‘subjective’ interpretation
of nature due to lack of knowledge. Is there a true distinction between these two
viewpoints? These ideas have been the subject of scientific and philosophical discussion
for centuries and a brief discussion is presented in the final chapter of this book.
However, when the phenomena we are studying appear as stochastic processes
and have been subjected to rigorous mathematical tests and do not reveal a fully
deterministic framework, probabilistic tools must be employed. In this book we present
concepts, theory and computer code written in R for random process analysis.Acknowledgements
We are grateful to Dr. Sonke Adlung for being most cooperative, considerate and
helpful during the publication process.Contents
1 Introduction 1
1.1 Introduction 1
2 Historical Background 4
2.1 The Philosopher and the Gambler 4
2.2 Comments 8
2.3 Exercises 11
3 Introduction to Stochastic Processes 12
3.1 Basic notion 12
3.2 Markov processes 25
3.3 Predicting the future 31
3.4 Continuous-time Markov chain 60
3.5 Ehrenfest urn model 86
3.6 Exercises 105
4 Poisson Processes 107
4.1 Counting process 108
4.2 Poisson process from counting process 110
4.3 Poisson process from Bernoulli process 111
4.4 Poisson process through the inter-arrival time 120
4.5 Poisson processes simulations 121
4.6 Nonhomogeneous Poisson process 135
4.7 Exercises 139
5 Random Walk 140
5.1 Definitions and examples 140
5.2 Some topics on Brownian motion 173
5.3 Exercises 184
6 ARMA Processes 187
6.1 White noise and other useful definitions 187
6.2 Moving-average processes 191
6.3 Autoregressive processes 196
6.4 Autoregressive moving-average processes (ARMA) 204
6.5 An introduction to non stationary and seasonal time series 208
6.6 A physical application 217
6.7 Exercises 228
7 Spectrum Analysis 232
7.1 Spectrum of stochastic signals 237x Contents
7.2 Noise spectrum 246
7.3 Applications of spectrum analysis 249
7.4 Singular Spectrum Analysis 253
7.5 Exercises 265
8 Markov Chain Monte Carlo 269
8.1 Mother Nature’s minimization algorithm 269
8.2 From physical birth to statistical development 271
8.3 The travelling salesman problem 284
8.4 Exercises 291
9 Bayesian Inference and Stochastic Processes 294
9.1 Application of MCMC in a regression problem with auto-correlated
errors 297
9.2 Bayesian spectral analysis applied to RADAR target detection 309
9.3 Bayesian analysis of a Poisson process: the waiting-time paradox 321
9.4 Bayesian analysis applied to a lighthouse 326
9.5 Exercises 332
10 Genetic algorithms: an evolutionary-based global random search 335
10.1 Introduction 335
10.2 Terminology and basics of GA 336
10.3 Simple genetic algorithm 339
10.4 A simple application: non linear fitting 349
10.5 Advanced genetic algorithms 357
10.6 Parameter estimation of ARMA models 361
10.7 Solving the travelling salesman problem 367
10.8 Concluding remarks 373
10.9 Exercises 373
11 The Problem of Accuracy 375
11.1 Estimating accuracy 375
11.2 Averaging time series 376
11.3 The batch means method 381
11.4 The moving block bootstrap method 387
11.5 Convergence diagnostic with the MBB method 396
11.6 Exercises 400
12 Spatial Analysis 402
12.1 Geostatistical perspective 402
12.2 Correlation coefficient and correlogram 406
12.3 Semivariogram 413
12.4 Spacetime analysis 430
12.5 On the optimization of the spatio-temporal variogram 439
12.6 Exercises 444
13 How Random is a Random Process? 446
13.1 Random hints about randomness 446Contents xi
13.2 Characterizing mathematical randomness 448
13.3 Entropy 454
13.4 A final note 465
Appendix A Bootstrap 467
A.1 Bootstrap standard error 469
A.2 Parametric bootstrap 472
Appendix B JAGS 476
List of Symbols 483
List of R Codes 485
References 488
Index 4981
Introduction
1.1 Introduction
The subject of random or stochastic process analysis is a very important part of
scientific inquiry. The terms stochastic and random process are used interchangeably.
Random processes are used as mathematical models for a large number of phenom￾ena in physics, chemistry, biology, computer science, information theory, economics,
environmental science and others. Many books about random processes have been
published over the years. Over time, it become more and more important to provide
not only the theory and examples regarding a specific processes, but also the com￾puter code and example data. Therefore, this book is intended to present concepts,
theory and computer code written in R, that helps readers with limited initial knowl￾edge of random processes to become operational with the material. Each subject is
described and problems are implemented in R code, with real data collected in exper￾iments performed by the authors or taken from the literature. With this intent, the
reader can promptly apply the analysis to her or his own data, making the subject
operational. Consistent with modern trends in university instruction, this book make
readers active learners, with hands-on computer experiments directing readers through
applications of random process analysis (RPA). Each chapter is also introduced with
a brief historical background, with specific references, for further readings about each
subject.
Chapter 2 provides a brief historical background about the origin of random pro￾cesses theory. In Chapter 3, the reader will find an in-depth description of the fun￾damental theory of stochastic processes. The chapter introduces concepts of station￾arity, ergodicity, Markov processes and Markov chains. Examples from mathematics
and physics are presented to exemplify random processes such as the Buffon’s nee￾dle and the Ehrenfest Urn Model. In Chapter 4, Poisson’s processes are presented.
Derivation of the well-known distribution is presented as well as homogeneous and
non–homogeneous Poisson processes are discussed. One the cornerstones of random
processes, random walk, is described in Chapter 5. The concepts of absorbing and
reflecting barriers are presented along with the gambler’s ruin example, as well as a
two-dimensional random walk code and discussion about random walk applied to the
process of Brownian motion.
Chapter 6 enters into stochastic time series analysis with the description of moving
average, autoregressive and autoregressive moving average processes. Seasonal time
series analysis is introduced with examples applied to measures of temperature and2 Introduction
water budget. The analysis of random processes requires a thorough understanding
of spectrum and noise analysis. In Chapter 7 Fourier transforms for deterministic
and stochastic time series are presented, with application to spectrum analysis. The
singular spectrum analysis technique is also presented, for analysis and removal of
trends.
Chapter 8 presents the Markov Chain Monte Carlo method with a description of
probably the most famous algorithm in stochastic theory, the Metropolis algorithm.
After a through description of the theory and code, the travelling salesman problem is
presented, with the simulated annealing approach. The concept of Bayesian analysis
is here briefly introduced and leading into the next chapter. Chapter 9 focus on a
cornerstone of modern statistics, Bayesian inference, which is applied to a description
of autoregressive processes. After introducing the main concepts, examples applied
to real data of temperature and CO2 concentration in Antarctica, as well as radar
detection, are presented. Bayesian analysis of the Poisson process is presented with the
waiting-time paradox. The chapter ends with an application to lighthouse detection
as a remarkable example of Bayesian inference.
Random processes are used as tools for random search in minimization algorithms,
as an alternative to gradient-based search algorithms used for instance in least square
optimization. Genetic algorithms are presented in Chapter 10, with application to
non–linear fitting, autoregressive moving average models. As an example of improved
optimization with respect to other approaches, the travelling salesman problem is here
solved with genetic algorithms. The modelling of stochastic processes depends on the
accuracy of the estimators derived in the process analysis. The problem of accuracy is
discussed in Chapter 11, with examples on averaging of time series, batch means meth￾ods, moving bootstrap and other techniques to improve accuracy in random processes
modelling.
Chapter 12 addresses a topic that is not traditionally described in books about
random processes: spatial analysis. It is nevertheless an important subject dealing
with the application of statistical concepts to properties varying in space. The chapter
provides an introduction to geostatistical concepts and then present a novel approach,
where spatial and temporal analysis are combined into a stochastic analysis of spatio￾temporal processes. At the end of the chapter, the optimization procedure for spatial
parameters is computed also with genetic algorithm, showing the possibility of con￾necting and applying various techniques presented in the book.
The book ends with Chapter 13, which discusses the very definition of a random
process, the mathematical definition of randomness and a discussion of the definition of
entropies. This discussion is developed into a general framework and its implications
for scientific inquiry. The book also has two appendices providing additional tools
presented in the main part of the book.
The codes presented in this book are written using the RStudio integrated de￾velopment environment (IDE). RStudio includes a console, an editor that supports
direct code execution, as well as tools for plotting, debugging and workspace man￾agement. There are many books about programming in R that can be used as ref￾erence and in particular publications and links presented in the official Comprehen￾sive R Archive Network (CRAN) available at: https://cran.r-project.org/. TheIntroduction 3
codes and example data written in this book can be dowloaded from the website
http://www.marcobittelli.it under the section Computer codes for books. Exer￾cises are presented at the end of each chapter and solutions are downloadable on the
book’s website.
Open source languages and related libraries are subject to changes, updates and
modifications, therefore the packages presented here may undergo changes in the fu￾ture. To obtain specific information and documentation about a library, the following
instruction should be used: library(help=GA), where for example the library (GA)
for genetic algorithms can be explored. Here we list the libraries necessary to run the
examples in different chapters:
Chapter 7 requires the library lubridate; Chapter 9 the library rjags, described in
detail in the Appendix B; Chapter 12 requires ggplot2, gstat, lattice, mapview, GA,
quantmod, reshape, sf, sp, stars, tidyverse, xts and zoo and Chapter 13 requires
entropy, tseriesEntropy.2
Historical Background
It is not certain that everything is uncertain.
Blaise Pascal
2.1 The Philosopher and the Gambler
To introduce the role of computer studies in stochastic processes analysis, we will
go back a few centuries to the invention of probability theory. It is the year 1654,
according to a familiar story (Hacking, 1975). Antoine Gombaud Chevalier de M´er´e,
Sieur de Baussay (1607−1684), asks some questions on a game of chance to Blaise
Pascal (1623−1662). Later, Sim´eon−Denis Poisson (1781−1840), calls Antoine Gom￾baud ‘man of the world’ and Blaise Pascal ‘austere Jansenist’: Un probl`eme relatif
aux jeux de hasard, propos´e `a un aust`ere jans´eniste par un homme du monde, a ´et´e
l’origine du calcul des probabilit´es (A problem about games of chance proposed to an
austere Jansenist by a man of the world was the origin of the calculus of probabilities)
(Poisson, 1837). We know that Pascal was not only a philosopher, but also a physi￾cist, a mathematician, a writer, a theologian. Antoine Gambaud was a writer and a
philosopher, not only a gambler.
We now discuss one of the questions that our Chevalier asked of Pascal concerning
the throws of two dice. We throw two dice and bet on the double six. How many throws
do we need to have a change of winning?
Antoine Gombaud said that a gambling rule, based on the mathematical analogy
between the probabilities of obtaining six with a single die or double six with a couple
of dice, indicates that you need at least 24 throws, but from his personal gambling
experiences the throws must be at least 25. Pascal, after discussing the problem with
Pierre de Fermat (1601−1665), answered that mathematics is not contrary to experi￾ence. Let us briefly discuss the topic.
Let A1 be the event {6, 6} at the first throw, so P {A1} = 1/36 (the symbol P {.}
means ‘probability’). Then, the probability of not obtaining two six is that of the
complementary event A1: P

A1
	
= 1 − 1/36 = 35/36. At the second throw, the event
A: none double six at the first throw and none double six at the second throw has
probability P

A
	
= P

A1
	
P

A2
	
= (35/36)2
, and so on. The probability of not
winning in 24 throws is:
P

A
	[24]
= P

A1
	
P

A2
	
· · · P

A24	
= (35/36)24 = 0.5086The Philosopher and the Gambler 5
so the probability of winning is P {A}
[24] = 1 − 0.5086 = 0.4914. While in 25 throws it
is P

A
	[25] = 0.4945 and P {A}
[25] = 0.5055. Notice that the difference is very small,
and this honours the power of observation of our Chevalier. But there are doubts about
the truth of this story (Ore, 1960).
Let us imagine being the Chevalier de M´er´e who, for 30 nights, goes to the game
table to throw two dice. Every night, we play 20 games, with 25 and 24 throws each.
If in a game two sixes appears, we win the game. At the end of the night, that is after
20 games, if the victories are more than 10, we had a lucky night.
We can describe the throw of a die as a stochastic process. In the next chapter
we will rigorously define ‘stochastic process’, but here we simply say that stochastic
processes are mathematical models of dynamical systems that evolve over time or
space in a probabilistic manner.
In our case, the dynamical system is the die, that at each throw shows a face with
probability 1/6. The code below is the ‘transcription’ in R of the dice game above.
## Code_2_1.R Throw of two dice
# 25 throws
# p_25<- 0.5055: probability of getting two sixes in 25 throws
n.nights <- 30 # number of nights
n.games <- 20 # number of games
n.throws <- 25 # number of throws
spot <- c(1:6) # spots of a 6-sided die
p_fair <- rep(1/6,6) # probabilities of a "fair" die
d6 <- numeric()
d6T <- numeric()
nseed <- 50
for (j in 1:n.nights)
{ # loop on the nights
nseed <- nseed+1
set.seed(nseed)
for (l in 1:n.games)
{ # loop on the games
d6[l] <- 0
for (i in 1:n.throws)
{ # loop on the throws
die.1 <- sample(spot,1,p_fair,replace=T) # i-th throw with the die 1
die.2 <- sample(spot,1,p_fair,replace=T) # i-th throw with the die 2
s.points <- die.1+die.2
if (s.points == 12) d6[l] <- 1
} # end loop on the throws
} # end loop on the games
d6T[j] <- sum(d6)
} # end loop on the nights
d6T
### 24 throws
## p_24<- 0.4914 # probability of getting two sixes in 24 throws
n.nights <- 30 # number of nights
n.games <- 20 # number of games
n.throws <- 24 # number of throws
spot <- c(1:6) # spots of a 6-sided die6 Historical Background
p_fair <- rep(1/6,6) # probabilities of a "fair" die
d6 <- numeric()
d6T <- numeric()
nseed <- 500
for (j in 1:n.nights)
{ # loop on the nights
nseed <- nseed+1
set.seed(nseed)
for (l in 1:n.games)
{ # loop on the games
d6[l] <- 0
for (i in 1:n.throws)
{ # loop on the throws
die.1 <- sample(spot,1,p_fair,replace=T) # i-th throw with the die 1
die.2 <- sample(spot,1,p_fair,replace=T) # i-th throw with the die 2
s.points <- die.1+die.2
if (s.points == 12) d6[l] <- 1
} # end loop on the throws
} # end loop on the games
d6T[j] <- sum(d6)
} # end loop on the nights
d6T
For the sake of clarity, we repeat for 24 throws the instructions for 25 throws,
changing only the lines n.throws <- 24 and nseed <- 500. The vector d6 at the
beginning of each game is 0, if a double six is obtained its value becomes 1, by summing
the n.nights components of d6, we know if we won or lost. Notice the R function
set.seed(.) is used to set the initial seed of the (pseudo) random number generator
(RNG). RNGs are in fact fully deterministic algorithms, so the same seed generates
the same sequences, changing the seed, we get different sequences. The result of the
code above for 25 throws is:
d6T:
10 10 12 9 11 13 12 10 12 10 13 9 9 8 13 9 12 11 9 11 11 11 15 12 12
11 10 8 11 13
We see that the two first games were tied, we won the third and lost the fourth, and
so on. We won 18 games out of 30, lost 7 games and tied 5 games. The result of the
code above for 24 throws is:
d6T:
11 7 6 11 8 17 11 9 12 10 13 10 7 7 11 16 6 12 9 7 9 13 12 11 13
13 9 7 14 11
In this case, we won 16 games, so that after 30 nights we are again win-making. These
results appear not to support the Chevalier’s claim that 24 throws are not enough to
hope to win. However, such a statement is not correct. For instance, if we put nseed
<- 100 with n.throws <- 25 we have:
d6T:
12 13 10 10 10 12 9 10 8 12 12 9 9 10 11 9 12 10 9 9 8 9 6 9 12
13 9 9 11 10
We won only 10 games out of 30. The reason for this variability is that the sample size
is too small, that is the number of games is not enough to give reliable results. Let
us increase the number of games. For each night we play 100 games and the nightsThe Philosopher and the Gambler 7
are 180. In the Code_2_1.R, it is now: n.nights <- 180 and n.games <- 100. The
result is for 25 throws:
d6T:
[1] 59 44 57 53 54 49 51 54 50 45 43 50 50 45 41 ...
....................................................
[176] 59 55 50 58 45
that is, the first night we won 59 games out of 100, the second only 44, and so on.
The result is for 24 throws:
d6T:
[1] 46 49 52 48 52 41 47 53 40 46 55 50 51 42 55 ...
....................................................
[176] 53 48 52 49 54
We can show the results both with 25 and 24 throws as in Fig. 2.1. The figure is
obtained adding the following lines after d6T, both for 25 and 24 throws.
N{6–6}
counts
35 40 45 50 55 60 65
0 1 0 2 0 3 0 4 0
Fig. 2.1 Number of times for obtaining double six has the value in the abscissa. Solid line:
25 throws, dashed line: 24 throws.
For n.throws <- 25 and nseed <- 100:
lbin <- 2
par(lwd=3)
hist(d6T,main=" ",freq=T,xlab="N{6-6}",ylab="counts",cex.lab=1.3,lty=1,
border="black",ylim=c(0,40),br=seq(36,64,by=lbin),font.lab=3)
For n.throws <- 24 and nseed <- 301:
hist(d6T,lty=2,br=seq(36,64,by=lbin),add=T)
Note that freq=T means that the histogram reports the counts component of the
result, if freq=F the histogram reports probability densities, in this case the total area
of the plot is 1.
In the abscissa it reports the number of times the double six was obtained in 100
games. The symbol N{6-6} indicates the double six. In the ordinate it reports the8 Historical Background
number of times this event occurred in 180 nights. For instance, the bin (50, 52] is
38 for the games with 25 throws, meaning that in 38 times out of 180 the double six
occurred 51 or 52 times in 100 games.
In the histogram each bin is closed on the right and open on the left, therefore
the 50 occurrences of the double six are not counted in the (50, 52] bin, but rather in
the (48, 50] bin. The results show that in the total number of games, which is 180 ×
100 = 18000, the double six occurred in 9204 games, then the probability of a double
six estimated in 18000 games is Pb{A}
[25] = 9204/18000 ≈ 0.5113 (the hat stands
for estimate), very close to the ‘theoretical’ probability. Here ‘theoretical’ means the
probability of perfect dice, that is that expected assuming equiprobability for each face.
Rigorously speaking we should not define probability by counting on ‘equiprobable’
events, because that makes the definition recursive. However, putting aside philosophy,
if the die is fair, its faces are ‘equally’ likely to occur and the probability of outcomes
can be computed as we did.
For games with 24 throws, a double six occurred in 8797 games, then the estimated
probability of the double six is Pb{A}
[24] ≈ 0.4887, in agreement with the ‘theoretical’
one P {A}
[24] = 0.4914.
Let us consider the 18000 games as N independent trials, each with probability p
of success, and let n be the number of successes. The standard error of the estimate
of the proportion p of 1’s in the N long sequence is ˆσ =
p
pˆ(1 − pˆ)/N, where ˆp is an
estimate of p, denoted above as Pb{A}
[25]. In our case ˆσ
[25] = 0.0037, practically equal
to the theoretical one. Obviously also ˆσ
[24] results in the same.
We have seen that Pb{A}
[25] = 0.5113 and Pb{A}
[24] = 0.4887, we could ask ourselves
if the difference between the two means ˆd = 0.5113 − 0.4887 = 0.0226 is significant.
We can test for the significance of the difference between two population means using
the Student’s t, which can be done in R by the line:
t.test(z,y,alt="greater",var.equal=TRUE)
where z are the winnings in the 180 nights with 25 throws in each game (59, 54,
. . . , 58, 45) and y with 24 throws in each game (46, 49, . . . , 49, 54). The option
alt="greater" is to specify a one–tailed test and var.equal=TRUE to specify equal
variances. The significance level (p value) is p-value = 5.159e-06, that is highly sta￾tistically significant. In passing, 20 throws for 30 nights yield no significant difference,
confirming what we said above about the small number of games.
We can test the difference of the means also by the bootstrap method. We will
discuss this method in Appendix A, here we limit ourselves to show the result in
Fig. 2.2, which is also presented as Exercise 2.1. As can be seen in Fig. 2.2, the
difference ˆd is significant, since out of B = 1000 replications ˆd
∗
, none of them is less
than 0.
2.2 Comments
It is difficult to believe that a real gentleman such as Antoine Gombaud went to
play with dice for about three months playing 100 games each night. Regardless of
whether the story is true or false, it teaches us something. In the doubt expressed
by the Chevalier, different concepts of probability are involved. Doubtless, the termComments 9
d
^
*
counts
0.01 0.02 0.03 0.04
0 100 200 300 400
Fig. 2.2 Distribution of 1000 bootstrap replications dˆ∗
of the difference of the means of the
two samples obtained with 25 and 24 throws (sample size = 180). The dotted line locates the
observed difference dˆ= 0.0226. There are no replications less than 0.
‘probability’ had not yet appeared, but from one side he speaks about a theoretical
mathematical argument to calculate the number of chances to get two sixes. From the
other side he relies on his experience as a gambler to evaluate the frequencies of the
results. It is already recognizable the tensions between theory and experience, between
probability as subject of study of a purely mathematical discipline and probability as
a property of a real physical random process evolving over time.
We could ask ourselves why the Chevalier affirmed that mathematics was wrong
in alleging 24 throws. According to some historians, perhaps he believed that. If the
probability of success in one throw is 1/n, in m throws it is m/n, that is 24/36 = 0.667.
Of course, this reasoning is wrong: probabilities have to be multiplied, not summed.
Historians say that similar problems about games of chance were present before
Pascal and Fermat. Gerolamo Cardano, for instance, wrote Liber de Ludo Aleae (‘The
Book on Games of Chance’), written about in 1560 and published posthumously in
1663, in which many results in various games with dice are discussed. In particular, the
chance of various combinations of points in games with three dice are presented. The
same problems about three dice were studied also by Galileo Galilei (Todhunter, 1865),
in about 1610−1620 in his Sopra le scoperte de i dadi, translated in different ways, for
instance, ‘Analysis of Dice Games’, ‘On a Discovery Concerning Dice’, ‘Concerning an
Investigation on Dice’, and so on. Actually the word ‘scoperte’ means the faces of the
dice that appear, so we could translate it simply as ‘On the Outcomes of Dice’.
Galileo was asked why playing with three dice the sum of points 10 or 11 are
observed more frequently than the sum of points 9 or 12. Galileo’s answer was:
Che nel giuoco de dadi alcuni punti sieno pi`u vantaggiosi di altri, vi ha la sua ragione assai
manifesta, la quale `e il poter quelli pi`u facilmente e pi`u frequentemente scoprirsi che questi
(The fact that in dice games certain outcomes are more advantageous than others has a very
clear reason, which is that certain outcomes can appear more easily and more frequently than10 Historical Background
others.)
For instance, the sum 9 is obtained with the following six triple number (triplicit`a),
that is the scoperte of the three dice:
1.2.6.; 1.3.5.; 1.4.4.; 2.2.5.; 2.3.4.; 3.3.3.
Six triple number are also necessary to get the sum 10:
1.3.6.; 1.4.5.; 2.2.6.; 2.3.5.; 2.4.4.; 3.3.4.
However the sum 3.3.3, for instance, can be produced by only one throw, while the
sum 3.3.4. by three throws: 3.3.4., 3.4.3., 4.3.3. In conclusion, the sum of
points 10 can be produced by 27 different throws, while the sum of points 9 by 25
only.
Let us do a forwards time warp and read the following quotation:
From an urn, in which many black and an equal number of white but otherwise identical
spheres are placed, let 20 purely random drawings be made. The case that only black balls
are drawn is not a hair less probable than the case that on the first draw one gets a black
sphere, on the second a white, on the third a black, etc. The fact that one is more likely to get
10 black spheres and 10 white spheres in 20 drawings than one is to get 20 black spheres is
due to the fact that the former event can come about in many more ways than the latter. The
relative probability of the former event as compared to the latter is the number 20!/10!10!,
which indicates how many permutations one can make of the terms in the series of 10 white
and 10 black spheres [. . . ]. Each one of these permutations represents an event that has the
same probability as the event of all black spheres.
That is what Ludwig Boltzmann wrote in 1896 in his Vorlesungen uber Gastheorie ¨
translated by Brush (1964). It is not impossible to draw 20 black balls, since this balls
extraction has the same probability as any other one, but the number of ways to draw
10 black balls and 10 white balls is far greater than that of all black balls.
In his Foundations of Statistical Mechanics, Boltzmann explicitly introduces the
postulate of equal a priori probability of microstates being compatible with a given
macroscopic state. On this Ansatz, Boltzmann explains why the ‘arrow of time’ points
to the more probable macrostate. We will – so to speak – hold in our hand these
concepts by studying the stochastic process ‘Ehrenfest’s urn model’ in the next chapter.
Both Cardano and Galileo found the solution of the problems of three dice, assum￾ing that the possible outcomes are equally possible and counts the chance of compound
events. In statistical mechanics, each microstate describes the position and velocity of
each molecule. A macrostate is a state description of the macroscopic properties of
the system: for instance its pressure, volume and such. Each macrostate is made up
of many microstates. To have an idea of microstates and macrostates, let us think
of macrostates as the sum of the points of the three dice and of microstates as the
number of favourable outcomes. So we say that the ‘system’ (the system is formed by
the three dice) is in the macrostate 10 (that is, the sum of the points is 10) which is
realized by 27 microstates. In the Boltzmann’s example of the 20 balls, each extracted
sequence is a microstate, while the number of white (or black) balls is a macrostate.
The hypothesis of equal a priori probability (explicitly stated or implicitly as￾sumed), in its turn, rests on the principle of indifference: equal probabilities have to
be assigned to each occurrence if there is no reason to think otherwise. With a small
time leap, we learn from Einstein (1925) that, in what will be called ‘Bose-Einstein
Statistics’, the microstates are not equally possible, even though there is no reason toExercises 11
consider any one of these microstates either more or less likely to occur than any other
(on this subject, see Rosa (1993)).
To conclude this introductory chapter, we notice that in the computer experiments
performed with two dice (Code_2_1.R) (the term most used is simulation and more
exactly Monte Carlo simulation), it is possible to experience the notion of probability.
In other words, the computer is regarded as something like a ‘statistical laboratory’
with which probabilistic experiments can be performed, experiments not quite fea￾sible in practice. We will encounter expressions like ‘measurements’, ‘statistical and
systematic errors’, ‘error propagation’, and so on, just as in a laboratory experiment.
2.3 Exercises
Exercise 2.1 We have seen in Code_2_1.R, relative to the throw of two dice, that the es￾timated probability of the double six in 18000 games with 24 throws is Pb{A}
[24] ≈ 0.4887,
while with 25 throws it is Pb{A}
[25] = 0.5113, result practically equal to the theoretical one.
The difference dˆ= 0.5113 − 0.4887 = 0.0226 resulted significant. Write a code to obtain Fig.
2.2, showing if the difference of the means is significant with the bootstrap method. Before
you have to read Appendix A if you are not familiar with the bootstrap method.
Exercise 2.2 In the dice game Unders and Overs (U&O), two dice are rolled. Players bet
on one of the following alternatives: (1) The result (sum of the dice faces) is below 7, (2) the
result is 7, (3) the result is above 7.
In cases (1) and (3) the pay off odds are 1:1, i.e. if you bet £1 the house gives you back
your money plus an additional £1. In case (2) the odds are 4:1, i.e. betting £1 you gain £4
(you get £5).
Suppose you bet at £1 on the outcome (1). What is your expected average win/loss (i.e.
in an infinite number of throws)?
Exercise 2.3 Referring to the previous exercise, write a code to simulate a finite game
consisting of 10, 100 or 1000 throws. Discuss the result of the simulations, compared to the
theoretical win/loss expectation.
Hint: Use the R function sample for sampling a die face, i.e. an integer number from 1:6.
Exercise 2.4 A variant of the U&O game allows the player to bet up to two alternatives
(placing £1 over each one). How does the win/loss expectation change?
Exercise 2.5 Justify the following assertion: ‘the house always wins’. Hint: If you can bet
£1 on each of the three alternatives, what is the expected outcome?
Exercise 2.6 With reference to exercise 2.5, compare the theoretical result for an infinite
number of throws with those obtained in a small number of them, e.g. 10. Simulating the
problem in R, in 100 repetitions how many times do you win and how many do you lose your
money?3
Introduction to Stochastic Processes
Noi corriamo sempre in una direzione,
ma qual sia e che senso abbia chi lo sa...
We are all headed in one direction,
but which it is and what sense it makes, who knows...
Francesco Guccini, Incontro
Probability theory is essential to the understanding of many processes (physical, chem￾ical, biological, economic, etc.). By means of random variables, we build models of such
processes, that is of systems that evolve over time. We are interested in what happens
in the future. If we know the probability distribution until now, how will it be modified
if carried forwards, through time? The answer is a matter of stochastic processes. For
further reading many books are available on the subject (Feller, 1970; Lawler, 2006;
Yates and Goodman, 2015; Jones and Smith, 2018; Grimmett, 2018).
3.1 Basic notion
In dictionaries of classical Greek, the word στoχαζεσϑαι ´ (stochazesthai) means ‘to
aim at something’, ‘to aim at a target, at a goal’, at a στoκoς´ (st´ochos). Later, figura￾tively, ‘to aim at something’ becomes ‘to have something in view’, or ‘to conjecture’,
from which στoχαστ ικ´oς (stochastik´os), ‘skilful in aiming at’, ‘able to conjecture’.
So the ‘target’ becomes the ‘conjecture’. Conjecture of what? Of something below
the apparent chance? Of undisclosed causes? Is there a hidden ‘determinism’ even in
(seemingly) random phenomena? That is the question. The interested reader can refer
to Chapter 2, where some historical answers we recalled succinctly.
A stochastic (or random) process is defined as a family of random variables:
X1, X2, . . . , Xt, . . . ,
indexed by a parameter t, and defined on the same probability space (Ω, F, P) formally
defined as follows: Ω is the sample space, i.e. the space of all possible outcomes, F is a
family of subsets of Ω, mathematically defined a σ-algebra, with particular properties
(for example, that of including the whole sample space and all possible unions of
subsets) that make Ω a measurable space. P is a probability measure function operating
on F, such that P(Ω) = 1 and P(Φ) = 0, Φ being the empty set. The index t often,
but not always, stands for a time (days, years, seconds, nanoseconds, etc.). The ‘time’
can also be a non-physical time, as for instance ‘Monte Carlo steps’.Basic notion 13
A stochastic process is written as {Xt;t ∈ T}. The set T is the parametric space,
it can be a subset of natural numbers or integers, that is T = {0, 1, 2, . . . }, or T =
{. . . , −2, −1, 0, −1, −2, . . . }, or T = {0, 1, 2, . . . , n}. In these cases {Xt} is said to be
a discrete-time stochastic process. If T is the real line R or its subset, for instance
T = (−∞, ∞), or T = [0, ∞), or T = [a, b), or T = [a, b], {Xt} is said to be a
continuous-time stochastic process.
Discrete-time and continuous-time processes essentially differ in the time scale: in
the former case events occur in a predetermined succession of time points t1, t2, . . . ,
in the latter events can occur at each time point t of a continuous range of possible
values.
The name stochastic process refers therefore to two inherent aspects: the term
‘process’ refers to a time function; the adjective ‘stochastic’ refers to randomness, in
the sense that a random variable is associated to each event in the time scale. In some
cases the stochastic process can also be associated to space and not just time.
Time has an arrow. The process has therefore a before and an after, a past and a
future. The realization xt at time t of the random variable Xt is supposed to be closer
to observations xt−1 and xt+1, rather than to those farther in time. This means that
the chronological order of observations plays an essential role.
We said that the Xt’s are random variables. They are defined on the same proba￾bility space (Ω, F, P). They take values in a measurable space, whose values are called
states. We say that ‘the process at time t is in the state xi
’, or more simply, ‘the
process at t is in i’, to mean that the random variable Xt has taken the value xi
. The
set of all values taken by the variables of the process is called the state space and it
will be denoted as S. We can say that ‘the system at time t is in the state i’, or that
it ‘occupies’ or ‘visits’ the state i, if Xt = xi for xi ∈ S.
A process is discrete, or is in discrete values, if S is discrete, that is if S is countable
(finite or infinite): S ⊆ N or S ⊆ Z. The process is continuous, or is in continuous
values, if S ⊆ R. So that, stochastic processes may be classified into four types:
1. discrete-time and discrete state space,
2. discrete-time and continuous state space,
3. continuous-time and discrete state space,
4. continuous-time and continuous state space.
In other words, discrete or continuous time concerns the domain of the time variable
t, while discrete or continuous state concerns the domain of Xt for a given t.
Let us take a practical example. We are interested in continuously recording the
temporal variations of the temperature of a device. For technical reasons, the tem￾perature does not remain constant, but floats within a certain range. Suppose the
measurements are read on an analogue scale and, to be specific, suppose also that
measurements are down to thousandths of a degree, with precision of the order of 1%.
We can consider the measurements expressed in real numbers, even though, obviously,
any measurement has a finite number of digits. With such premises, the sequence
in time of the random variable ‘temperature’ can be represented as a continuous-time
stochastic process in continuous values. If we decide to group the measurements within
a range, say, of tenths of a degree, the process is still a continuous-type process, but in14 Introduction to Stochastic Processes
discrete values. If we group the data readings within a predefined range, for instance
every 20 or 60 seconds, the process will be a discrete-time process, in (approximately)
continuous values (thousandths of a degree) or discrete values (tenth of a degree)
Further classification concerns the dimension of Ω and T. If the space Ω has di￾mension greater than 1, we refor to a multivariate stochastic process, as they are, for
instance, space-time processes. All the variables Xt are defined in the same space Ω,
then each Xt is a random function of two arguments of different nature: the variable
of probabilistic nature ω ∈ Ω indicates the event, the variable of mathematical na￾ture t ∈ T creates an order in the random variables family. The stochastic process
{Xt;t ∈ T}, in more complete manner, should be written as:
{X(ω, t); ω ∈ Ω, t ∈ T}
to highlight that the particular realization of the stochastic process at time t depends
on the particular event ω ∈ Ω.
Let us fix t, t = t. Then Xt
(ω) ≡ X(ω,t) is a random variable and, if the possible
outcomes of the ‘trial’ are ω1, ω2, . . . , the possible realizations of Xt
(ω) are given by:
Xt
(ω1) = x1, Xt¯ (ω2) = x2, . . . ,
where the subscript i (i = 1, 2, . . .) of the xi
’s numbers are the different possible
realizations of the same random variable Xt(ω) at time t = t.
It is possible to regard X(t, ω) as a function of t, fixed ω = ω in Ω, so we have
Xt (ω). In this case, we consider a particular outcome at times t = t1, t = t2, . . . , that
is:
Xt1
(ω1) = x1, Xt2
(¯ω2) = x2, . . . , (3.1)
where the subscripts i (i = 1, 2, . . .) of the xi
’s number are the time points t1 at which
the event ω1 has occurred, t2 at which the event ω2, has occurred, etc. In this case, for
each fixed ω, the sequence (x1, x2, . . .) is called realization or history or sample path or
trajectory of the process. All the possible sample paths resulting from an experiment
constitute an ensemble. Therefore a stochastic process can be regarded as formed by
the wholeness of all its possible realisations. A finite portion of a realization is called
time series:
(. . . , xk+1, xk+2, . . . , xk+t, . . . , xk+n
| {z }
time series
, . . .) (3.2)
So, to recap, Xt (ω) means, depending on the context:
1. Xt(ω) (t and ω variables): a family of time dependent real-valued functions, that
is a stochastic process.
2. Xt
(ω) (t constant and ω variable): a random variable, that is, for definition, a
measurable function defined on a probability space.
3. Xt (ω) (t variable and ω constant): a single mathematical function depending on
time.
4. Xt
(ω) (t constant and ω constant): a real number.Basic notion 15
Coming back to the example of the temperature measurement of a device, suppose
that the measurements are taken every minute and we round up the records to tenths
of a degree. The stochastic process, modelling the time variation of the temperature,
is therefore a discrete-valued process in discrete time. Suppose we have carried out the
measurements for one hour on 30 similar devices (k = 1, . . . , 30).
The records of the measurements at every minute will not be the same for the 30
devices, therefore the records will be displayed as in Table 3.1.
Table 3.1 Temperature vs time dependence of 30 devices.
❍
k
❍❍❍
t
1 2 3 4 . . . 59 60
1 17.4 21.1 13.9 19.4 . . . 18.7 19.6
2 18.4 16.3 17.1 17.8 . . . 14.4 15.0
3 20.7 16.0 19.5 12.1 . . . 15.6 19.3
· · · · · · · · · · · · · · · · · · · · · · · ·
29 16.3 14.8 11.5 17.2 . . . 19.7 19.4
30 21.1 18.1 20.1 26.7 . . . 14.7 15.1
If t is fixed, for instance, t = t = 3, the corresponding column shows the tempera￾ture of the 30 devices at time t. The values x1 = 13.9, x2 = 17.1, . . . , x30 = 20.1 are
then 30 realizations of the random variable Xt
(ω). If ω is fixed, it means picking a
device and following its change in temperature over time. For instance, the ensemble
{18.4, 16.3, 17.1, 17.8 . . . , 14.4, 15.0} shows a possible sample path corresponding to the
device k = 2. Figure 3.1 shows the temperature as a function of time for the first four
devices and it represents four realizations, or time series, of a discrete-valued stochastic
process in discrete time.
0
10
20
10
20
10
20
10
20
30
5 10 15 20 25 30
time [minutes]
temperature [°C]
35 40 45 50 55 60
Fig. 3.1 Realizations of a discrete-valued stochastic process in discrete time. Dashed line:
devices temperature at time t = 3. The lines between points are to guide the eye.16 Introduction to Stochastic Processes
Let us consider some well-known stochastic processes, based on the Bernoulli pro￾cess, i.e. on a sequence of independent and identically distributed, i.i.d., random vari￾ables:
X1, X2, . . . , Xi
, . . . , with Xi ∼ Bern(p)
This sequences is called a Bernoulli process {Xt;t ∈ N} if the Xi
’s are independent of
each other and ∀i ∈ N, P {Xi = 1} = p and P {Xi = 0} = 1 − p. It is a discrete-valued
stochastic process in discrete time. The index set is the set of natural numbers T = N,
or T = N r 0), and the state space S is the set {0, 1}.
Define the random variable sequence:
St =
Xt
i=1
Xi
where {Xt;t ∈ [1, ∞)} is the Bernoulli process. The sequence {St;t ∈ [1, ∞)} is also a
discrete-valued stochastic process in discrete time, with S = N.
Consider the random variable sequences:
Yt =
St
t
=
1
t
Xt
i=1
Xi
The sequence {Yt;t ∈ [1, ∞)} is a discrete-time stochastic process, but with continu￾ous values, that is S = R. We also know, from the law of large numbers for binomial
random variables, that the sequence {Yt} converges in probability to E [Xi
] = p.
Some general considerations about stochastic processes are now defined. A first
question is: what does it mean to have a complete knowledge of a process {Xt;t ∈
T}? We have a complete knowledge of a random variable X, and we say that X is
known when we know its repartition (or cumulative distribution) function F(x) =
P {X 6 x} , ∀x ∈ R.
As a consequence, a stochastic process is known when the repartition function of
each variable of the family is known, that is all Ft(x) = P {Xt 6 x} , ∀x ∈ R and
∀t ∈ T. But this is not enough. We have to know all the joint probability distributions
of the variables Xt, that is all the double cumulative functions:
Ft1,t2
(x1, x2) = P {Xt1 6 x1 ∩ Xt2 6 x2} , ∀x1, x2 ∈ R and ∀t1, t2 ∈ T
and, in general, all the n−tuple:
Ft1,t2,...,tn
(x1, x2, . . . , xn) =
P {Xt1 6 x1 ∩ Xt2 6 x2, . . . , ∩Xtn 6 xn} ,
∀x1, x2, . . . , xn ∈ R and ∀t1, t2, . . . , ∈ T (3.3)
The family of functions Ft1,t2,...,tn
(x1, x2, . . . , xn) is called the temporal law of the pro￾cess. For a finite dimension family and under certain precise conditions the probabilis￾tic structure of the complete process may be specified. Such a formidable theoreticalBasic notion 17
problem was faced by Kolmogorov in the first half of the 20th century (Kolmogorov,
1950).
For practical purposes, it is advantageous to search for values summarizing the main
properties of the process. Such summaries are the finite–order moments, in particular
the first- and second-order moments. Suppose that such moments exist for each Xt.
The first-order moment, the expected value, is the mean of each Xt, defined as:
µt = E [Xt]
which, in general, is different for each t. The second central moment, or autocovariance
function, at the lag k, is defined as:
γt,t−k = Cov [Xt, Xt−k] = E [(Xt − µt)(Xt−k − µt−k)] (3.4)
The autocovariance function represents the covariance between the random variable
Xt and Xt−k, k = 0, 1, 2, . . . , that is the covariance of the process with itself at pairs of
time points. This function measures the joint variation of Xt and Xt−k, either in the
same direction (positive values of γt,t−k) or in the opposite direction (negative values
of γt,t−k), at the time points k = 0, 1, 2, . . . .
Remark 3.1 In bivariate, or generally multivariate processes, covariances are named
‘cross-covariances’, when the dependence of one process over another (or more than
one) is investigated. For instance, for two processes {Xt} and {Yt}, the cross-covariance
is given by:
γxy(t, t − k) = E￾Xt − µx(t)
 ￾Yt−k − µy(t − k)
 (3.5)
with a slight change of symbols to indicate a bivariate process.
If k = 0 , from eqn (3.4):
γt = Var [Xt] = E
(Xt − µt)
2

(3.6)
is the variance of the process, also denoted by σ
2
t
.
The (linear) autocorrelation function ρt,t−k at the lag k, is given by normalizing
the autocovariance γt,t−k , eqn (3.4), to the variance (3.6):
ρt,t−k =
Cov [Xt, Xt−k]
p
Var [Xt] Var [Xt−k]
=
γt,t−k
√
γt γt−k
(3.7)
=
E [(Xt − µt) (Xt−k − µt−k)]
q
E [(Xt − µt)]2 E [(Xt−k − µt−k)]2
(3.8)
The function ρt,t−k is dimensionless and does not vary by interchanging Xt and
Xt−k. It reaches its maximum when k = 0. If Xt and Xt−k are linearly indepen￾dent ρt,t−k is equal to 0, while it is +1 or −1, in presence of a perfect correlation
(there is an exact overlapping when time is shifted by k) or a perfect anti-correlation,
respectively. The quantities ρt,t−k and γt,t−k are essential to characterize stochastic
processes; indeed, they measure the internal structure of processes and their memor18 Introduction to Stochastic Processes
3.1.1 Stationary processes
It was shown that moments depend on time and on the joint distribution of Xt and
Xt−k. This makes inferential applications very limited because, usually, only a time
series is available, that is a single and limited realization of the process: at each t, only
one realization of Xt is available. At every t, Xt varies, causing µt and the correlation
between Xt and Xt−k to vary as well. One question is whether the available series
properties persist through time, or they belong only to the time interval where the
series has been observed. Furthermore, are the observed characteristics specific to a
particular realization or do they belong also to other realizations of the same process?
If we have to make valid inferences on the properties of the Xt’s, for instance on
their moments and their transformations, but with only one available observation, we
have, so to speak, to ‘approach’ the most usual statistical situation: the analysis of a
sequence of i.i.d. random variables.
The requirement that the Xt distributions have to be ‘not too much’ different in
time, leads to specify the conditions to define a stationary process. This is a clear
restriction on the heterogeneity of series and conforms to the condition of identically
distributed random variables. The requirement that the Xt have to be ‘not too much’
dependent on one another, when they are quite far in time, leads to the notion of
ergodicity. In this case, the restriction is on the memory of the series and conforms to
the condition of independent random variables. Figure 3.2 schematically summarizes
the above reasoning.
stationary process
ergodic process
restriction on heterogeneity
of the series
restriction on the memory
of the series
identically distributed r.v.
independent r.v. i.i.d.
r.v.
Fig. 3.2 From i.i.d. random variables (r.v.) to stationarity and ergodicity properties.
We can write schematically:
identical distributed r.v. restriction on the heterogeneity
−−−−−−−−−−−−−−−−−−−−→ stationarity
and
independent r.v. restriction on the memory
−−−−−−−−−−−−−−−−→ ergodicity
Stationarity is defined at different degrees. The process {Xt} is strictly stationary,
strongly stationary or strict-sense stationary when the joint distribution ofBasic notion 19
Xt, Xt+1, . . . , Xt+h (3.9)
is the same for every t ∈ T and for every time shift h = 0, 1, 2, . . . . That is:
P {Xt1 6 x1, Xt2 6 x2, . . . , Xtn 6 xn} =
P {Xt1+h 6 x1, Xt2+h 6 x2, . . . , Xtn+h 6 xn}
(3.10)
or in different notation:
Ft1,t2,...,tn
(x1, x2, . . . , xn) = Ft1+h,t2+h,...,tn+h(x1, x2, . . . , xn)
for every t1, t2, . . . , tn, for every x1, x2, . . . , xn, ∀n and ∀h.
Intuitively, if we imagine dividing the process into time chunks, all the pieces are
‘statistically similar’, in the sense that statistical properties do not vary over time,
and there is no preferred choice of start time. Under such conditions, all the moments,
note: if they exist, do not vary over time. This condition, as we said above, reflects
that of identical distributions. Indeed, if n = 1 in eqn (3.10), all the one–dimensional
repartition functions become identical to each other: Ft(x) = P {Xt 6 x} = Ft+h(x) =
P {Xt+h 6 x}, for all t and h. If n = 2, all the two-dimensional repartition functions
Ft1,t2
(x1, x2) = P {Xt1 6 x1, Xt2 6 x2} are dependent only on (t2−t1), but not specif￾ically on t1 and t2. Stationarity up to order m is also defined, if the joint probability
distribution of the Xt, Xt+1, . . . , Xt+h has finite and equal moments up to order m.
Consider m = 2. In that case the stationarity is named weak stationarity, or
covariance-stationarity, or wide-sense stationary, or simply stationary. If a stochastic
process is weakly stationary, the first and second moments exist and do not vary
throughout the time. The second cross moment E [Xt · Xt+k], ∀k may depend on k.
The expected value is finite and constant, at all time points:
µt = E [Xt] = µ < ∞, ∀t
Variance and autocovariance are finite and constant, at all time points:
Var [(Xt)] = σ
2
t ≡ γt = σ
2 ≡ γ < ∞, ∀t
and
γt,t−k = E [(Xt − µ)(Xt−k − µ)] = γk < ∞, ∀t
Note that γk does not depend on t, but only on k, i.e. γ(k) is a function of one variable,
the time difference k.
To give greater emphasis to this property, the autocovariance may also be written:
γ(|t − s|) = E [(Xt − µ)(Xs − µ)]
k being the time lag between t and s. If k = 0, γ0 = σ
2
. Note that γk = γ−k.
The autocorrelation function becomes:
ρk =
γk
γ0
=
γk
σ
2
clearly ρ0 = 1.20 Introduction to Stochastic Processes
Strong stationarity does not necessarily imply weak stationarity, since the defini￾tion of strong stationarity does not assume the existence of finite second moments.
However, if the process has finite second moments, then the implication is valid. On
the contrary, the time constancy up to the second moment, does not imply that the
distribution of marginal random variables Xt is the same for all t. In one case the two
definitions, strong and weak, overlap: when the the process is normal (Gaussian), that
is when for every n-tuple (t1, t2, . . . , tn) the joint distribution of (Xt1
, Xt2
, . . . , Xtn
)
is n-tuple variate, for every n. Not-stationary processes (in any sense) are not ‘less
important’ than the stationary ones. The Poisson process, for instance (see next sec￾tion) continuous time process in discrete values, is not-stationary in any sense, since
µ(t) = λt, (λ is a constant).
3.1.2 Ergodic processes
We have that stationarity conditions contribute to reducing the heterogeneity degree
of the process, but are they enough to estimate moments of interest? Can we estimate
µ, σ, γk and ρk from the available observations, i.e. from time series? We are interested,
for example, in obtaining an estimate ˆµ of the mean value µ of the stochastic process
{Xt;t ∈ T}.
Suppose we have a time−discrete process in discrete values that is also weak sta￾tionary. One could reasonably assume that, as µ is the same for every Xt, the average
can be computed at time t
0 by considering m realizations of Xt
0 . Unfortunately, only
one realization of Xt
0 is available at t
0
, the observation xt
0 . But in return, n observa￾tions taken in a time window of the process are available. Call t = 1 the time point at
the starting time, so we have the time series x1, x2, . . . , xn, where x1 is the realization
of the variable X1, at time point t = 1, x2 is the realization of the variable X2, at
time point t = 2, . . . , xn is the realization of the variable Xn, at time point t = n.
Compute the arithmetic mean ¯x, namely the average of the sample of n observations
x1, x2, . . . , xn of the same trajectory:
x¯ =
1
n
Xn
i=1
xi
What can we say about ¯x? This value is the realization of the random variable Xn,
named time average, defined as:
Xn =
1
n
Xn
i=1
Xi ← time
Can we use the random variable Xn, time average, to estimate the process mean µ? It
depends. First of all, we require that such random variable Xn must be an unbiased (or
correct) estimator of µ, that is E
Xn

= µ. Remember that an estimator is unbiased
if its expected value is equal to the value of the quantity (parameter) to be estimated.
The answer is affirmative Indeed, from stationarity:
E

Xn

= E"
1
n
Xn
i=1
Xi
#
=
1
n
Xn
i=1
E [Xi
] = µBasic notion 21
Unbiased is a required property of estimators, as is consistency.
Recall that an estimator Θn (the subscript n to indicate its dependence on sample
size n) is consistent of the parameter θ if, as the sample size n increases, it converges
in probability to the value of the parameter to be estimated, that is:
Θn
p
−−−−→ n→∞
θ
More precisely, the above is the definition of weak consistency, the strong con￾sistency requiring the almost sure convergence to the parameter θ. If the estimator
is unbiased, namely E
Θn

= θ, it is more convenient to study its convergence in
quadratic mean (qm). We have:
E

(Θn − θ)
2

= E
(Θn − E [Θn])2

= Var [Θn]
If Var [Θn] → 0, it follows:
Θn
qm
−−−−→ n→∞
θ and then also Θn
p
−−−−→ n→∞
θ
that is the estimator Θn is consistent.
Remembering the asymptotic unbiased of estimates:
E [Θn]
p
−−−−→ n→∞
θ
an estimator is weakly consistent if it is asymptotic unbiased, namely when it converges
in distribution to the parameter to be estimated. If it convergences to a constant, as
in the present case, the two notions of ‘convergence in probability’ and ‘convergence
in distribution’ are equivalent.
In our case, it must be:
Xn =
1
n
Xn
i=1
Xi
p
−−−−→ n→∞
µ (3.11)
We have to ensure if:
Var h
Xn
i
= Eh
( Xn − µ)
2
i
−−−−→ n→∞
0
Now:
Var h
Xn
i
= E"
1
n
Xn
i=1
Xi − µ
2
#
=
σ
2
n
+
2
n2
nX−1
k=1
(n − k) γk (3.12)
Note that Var h
Xn
i
is enhanced by the second term. From eqn (3.12), then also:
1
n2
nX−1
k=1
(n − k) =
nX−1
k=1
n − k
n2
=
1
2
−
1
2n
For the last sum in eqn (3.12) to converge to 0, with increasing n, it is necessary that
autocovariances have to converge to 0, with increasing time interval between the Xi
’s.22 Introduction to Stochastic Processes
Autocovariance, we said, is a measure of the interdependence between time points of
elements of the process. Therefore, if such a sum does not converge to 0, it means
that memory persists for very long times. As a consequence, even though the process
is stationary, the mean estimator computed by observed trajectories is not consistent,
since its mean square error does not converge to 0 with increasing sample size. This
behaviour, in the context of the Monte Carlo method is what once was called ‘statistical
inefficiency’ (Friedberg and Cameron, 1970), namely the variance inflation due to time
correlations. It will be discussed again in Chapter 8 for the Monte Carlo method.
On the contrary, as time grows − i.e. with increasing the number of observations
− we require the time average to converge to the so-called ensemble average. In other
words, it is as if we were observing a large number of trajectories at a given time
instant, although we actually have only a single trajectory at hand. In the following a
step-by-step derivation of eqn (3.12) is reported.
Proof Consider in general the random variable X, linear combination of n random variables:
X = a1X1 + a2X2 + · · · + anXn. The variance of X is given by:
Var [ X] = Var "Xn
i=1
aiXi
#
=
Xn
i=1
a
2
i Var [ Xi] + 2 Xn
i=1
Xn
j=i+1
aiajCov [Xi, Xj ] (3.13)
By putting:
σij = Cov [Xi, Xj ] , (for i = j it is σii = Var [ Xi])
it results in:
Var [ X] = Var "Xn
i=1
aiXi
#
=
Xn
i=1
a
2
i σii + 2 Xn
i=1
Xn
j=i+1
aiajσij (3.14)
Let us consider Var h
Xn
i
and, to follow more easily the reasoning, let us take n = 3. So we have
(dropping the subscript n for convenience):
X =
1
3
X3
i=1
Xi written as =
X1
3
+
X2
3
+
X3
3
so that Var h
X
i
becomes for eqn (3.13):
Var h
X
i
=
Var [ X1]
3
2 +
Var [ X2]
3
2 +
Var [ X3]
3
2 +
+ 2 
1
3
1
3
Cov [X1, X2] + 1
3
1
3
Cov [X1, X3] + 1
3
1
3
Cov [X2, X3]

Since it is σij = Cov [Xi, Xj ] and σii = Var [ Xi], we can write:
Var h
X
i
=
1
3
2
σ11 +
1
3
2
σ22 +
1
3
2
σ33+
+
1
3
2
σ12 +
1
3
2
σ12 +
1
3
2
σ13 +
1
3
2
σ13 +
1
3
2
σ23 +
1
3
2
σ23Basic notion 23
We have written in this way to allow us to understand the term ‘2’ in eqn (3.14). In a compact
form, we write:
Var h
X
i
=
1
3
2
X3
i=1
X3
j=1
σij
Indeed:
for i = 1 we have σ11, σ12, σ13
for i = 2 we have σ21, σ22, σ23, but σ21 = σ12
for i = 3 we have σ31, σ32, σ33, but σ31 = σ13, σ32 = σ23
Then, in general:
Var h
X
i
=
1
n2
Xn
i=1
σii +
2
n2
Xn
i=1
Xn
j=i+1
σij =
1
n2
Xn
i=1
Xn
j=1
σij (3.15)
Since the process is stationary, we have Var [Xt] = σ
2
, ∀t, that is to say σii = σ
2
, ∀i (σ11 =
σ22 = · · · = σnn = σ
2
). So the part with σii becomes:
1
n2
Xn
i=1
σii =
1
n2
nσ
2 =
σ
2
n
Let us look at the covariance term σij . The autocorrelation function in the stationary case is ρk =
γk/σ2
. For a continuous process, formally we can write ρ(k) = γ(k)/σ2
or also γ(k) = σ
2
ρ(k).
Now γ(k) is the autocovariance at lag k, that is the covariance between times i and j, before
written as σij . The autocorrelation function ρ(k) can be written as ρ(|i − j|). Still remaining in
the example with n = 3, if t = 1, 2, 3, it is k = 0, 1, 2. Consider the terms with σij :
2
n2
σ12 +
2
n2
σ13 +
2
n2
σ23 =
2
n2
ρ(2 − 1) + 2
n2
ρ(3 − 1) + 2
n2
ρ(3 − 2)
which can be summarized as follows:
X3
j=2
X
j−1
i=1
ρ(j − i) and in general Xn
j=2
X
j−1
i=1
ρ(j − i), j > i
From eqn (3.15), we can write Var h
X
i
as:
Var h
X
i
=
1
n2
Xn
i=1
Xn
j=1
σij =
σ
2
n2
Xn
i=1
Xn
j=1
ρ(j − i)
which always holds, even though observations are taken at not-regular time intervals. Rewrite the
summations with n = 3. Let j − i = k be, it is:
j i k
2 1 1 −→ ρ(1)
3 1 2 −→ ρ(2)
3 2 1 −→ ρ(1)24 Introduction to Stochastic Processes
The sum of the ρ’s can be written as 2 ρ(1) + 1 ρ(2). In general:
Xn
i=1
Xn
j=1
ρ(j − i) =
nX−1
k=1
(n − k)ρ(k)
Eventually Var h
Xn
i
(retaining the n index) is the sum of two terms: the first is σ
2
/n, the second
is 2σ
2
/n2
, multiplied by the sums on the ρ(k)’s:
Var h
Xn
i
=
σ
2
n
"
1 + 2
n
nX−1
k=1
(n − k)ρ(k)
| {z }
inflation term
#
This equation is the same as eqn (3.12), in which γk = ρkγ0, with γ0 = σ
2
, and, for a continuous￾time process, we have written ρ(k) instead of ρk. ✷
We have to take into consideration the notion of ergodicity, whose discussion can
be introduced from various points of view.
To maintain the exposition at a rather intuitive level, let us return to Table 3.1. If
we consider the arithmetic average of the values in the k = 2 raw, that is (18.4+ 16.3+
17.1 + 17.8 + · · · + 14.4 + 15.0)/60 we get the time average ¯x = 17.19. If we consider
the arithmetic average of the values in the t = 3 column (dashed line in Fig. 3.1),
that is 13.9 + 17.1 + 19.5 + · · · + 11.5 + 20.1)/30 we get an estimate ˆµ = 16.96 of the
expected value µ of the process. Note we say ‘estimate’ since the 30 considered values
are supposed to be a sample extracted from all the possible realizations of Xt=3. To
ascertain if the two averages are not significantly different, it is necessary to know their
variability – not a simple task.
Usually, in real situations, only the time average ¯x is available, but we expect to
estimate µ from it, without computing ensemble averages. If, for a long enough time,
the process takes a large number of possible values, or, in other words, it visits a large
number of possible states, a ‘time sample’ of these observed values is equivalent to a
sample extracted from all the possible available values.
So far we have considered the mean, but analogous reasoning holds also for the
second moments. In the following, we report some results. In general, a process is
ergodic with respect to the mean value E [f(Xt)] of a function f(Xt) (also mean￾ergodic), if the time average of the f(Xt)’s converges in probability to the mean value
of the function, namely:
1
n
Xn
i=1
f(Xi)
| {z }
time average
p
−−−−→ n→∞
E [f(Xt)]
| {z }
ensemble average
The left-hand side term is a time average, while the right-hand side is an ensemble
average, or ‘spatial’ average, with reference to the state space.Markov processes 25
In particular, for the mean, eqn (3.11) holds:
Xn =
1
n
Xn
i=1
Xi
p
−−−−→ n→∞
µ
while for the variance, it must be:
1
n
Xn
i=1
(Xi − µ)
2 p
−−−−→ n→∞
σ
2
and for the covariance:
1
n
Xn
i=j+1
(Xi − µ)(Xi−j − µ)
p
−−−−→ n→∞
γj
Note that we have to specify which particular moment (mean, variance, etc.) ergodicity
refers to. Clearly, if a process is ergodic, it is also stationary, but not vice versa.
The Slutski ergodic theorem (Grimmett and Stirzaker, 2001) states a necessary and
sufficient condition for the mean-ergodicity, namely the validity of eqn (3.11). The
theorem says that a stationary stochastic process {Xt} is ergodic with respect to the
mean if and only if:
limn→∞
Pn
i=1 γ(t, t − k)
n
= 0
which holds (sufficient, but not necessary, condition) if γ(t, t−k) → 0, for k → ∞. The
underlying idea is that, with the time interval increasing, the information contained
in xt is not connected to that contained in xt+k.
A brief account of ergodic theory is reported in Appendix A of Huffaker et al.
(2017), here we recall that the word ‘ergodic’ derives from the Greek ργoν, that is
ergon, work, energy, and oδoσ´ , that is odon, route, path. The term was introduced
by Boltzmann in statistical mechanics to describe the ‘work trajectory’ covered in the
phase space by the representative point of the system. The system is ergodic if its work
trajectory visits, sooner or later, all the points of the phase space allowed by energy
constraints. This hypothesis was later called the ergodic hypothesis.
3.2 Markov processes
A Markov process can be defined as a stochastic process whose state space has
the property of being ‘past-forgetting’, one of the possible practical definitions of the
‘markovian property’. Conversationally, we say that the process is a Markov process
if the future does not depend on the past, but only on the present instant. The verb
‘to depend’ has to be interpreted in probabilistic terms.
Let us consider the process {X(t)}. We know that in this instant t the process is in
the state e, that is we know the event ‘Present’: {e} = {X(t) = e}. In the past, namely26 Introduction to Stochastic Processes
at times s1, s2, . . . , sn, the process was in states d1, d2, . . . , dn. Let us call ‘Past’ the
set of events P, defined as:
P = {X(s1) = d1, X(s2) = d2, . . . X(sn) = dn}
As we said, it doesn’t matter if the ‘Past’ is known or not. In a similar way, we call
‘Future’ the set of events F:
F = {X(u1) = f1, X(u2) = f2, . . . X(um) = fm}
then, in the future, that is at times u1, u2, . . . , um, the process is in states f1, f2, . . . , fm.
The Markovian condition states that the probability that the process, at a certain time
ui > t, ui ∈ T (T is the parametric space), will be in a state fi (with i = 1, . . . , m)
– and such an event is the realization of an event belonging to F – does not depend
on (it is not conditioned by) where the process was in the past, namely it does not
depend on the events {X(si) = di} ∈ P (con i = 1, . . . , n) which occurred before t.
This probability depends only on (is conditioned by) the state in which the process is
at the instant t, that is on the precise event {e} that occurs hic et nunc, at the time
t. In short:
P

F

e,P
	
= P

F

e
	
(3.16)
Since there is no possibility of confusion, the braces in {e} have been removed. The
above relation defines the Markov process. If a stochastic process meets this relation,
then the process is a Markov process. If a stochastic process is a Markov process, then
it must meet the above relation.
Markov processes are memoryless, or better: they have no ancestral memory, but
preserve a memory only of the state where they are at the present time t. It is worth
noting that eqn (3.16) is the probabilistic analogue of a deterministic law of mechanics.
For instance, the future space coordinates of a satellite are completely determined by
the equations of classical physics. If I know its state at time t, I can predict exactly
(better: to an appropriate accuracy) its time evolution, no matter what its state was
before t. Analogously, for a Markovian process, if we know the state of the system at
time t, we can predict, not with certainty, but with perfectly computable probability,
the future visited states, no matter of its past history.
In the following, as is often used in Markov chains literature, we write P {A, B},
with the comma in place of the symbol ∩. Recall that, given the events A, B, C, if
P {B, C} > 0, we have:
P

A

B, C	
=
P {A, B, C}
P {B, C}
=
P {A, B, C}
P

B

C
	
P {C}
=
P

A, B

C
	
P

B

C
	
We aim to show that:
P

P

F, e	
= P

P

e
	
(3.17)
Starting with:
P{F
A
,P
B

e
C
} = P

F

e,P
	
P

P

e
	
and for eqn (3.16):Markov processes 27
P

F,P

e
	
= P

F

e
	
P

P

e
	
now:
P

P

F, e	
=
P

F,P

e
	
P

F

e
	 = P

P

e
	
That is eqn (3.17). Equation (3.17), symmetric with respect to eqn (3.16), says
that we can know the past history of the system, regardless of how it will evolve
(probabilistically) in the future, provided the present state is known. It is obvious
enough that not all stochastic processes have the Markovian property; let us think of
a system that cannot return to states previously visited. In this case the process must
remember all the visited states, since the probability that the system may return in
the future, to an already visited state, must be zero.
State spaces of Markov processes can be discrete or continuous. First we will study
the discrete–time Markov chains, for which the state space S is discrete. In this case
the parametric space T is a subset of natural or integer numbers, for example T =
{0, 1, 2, . . . , n, . . . }. More specifically, the Markovian property can be written as follows.
For every state i0, i1, . . . , in−1, i, j ∈ S and for each n ∈ T, it is:
P

Xn+1 = j

Xn = i, Xn−1 = in−1, . . . , X0 = i0
	
= P

Xn+1 = j

Xn = i
	
(3.18)
which can be read: the probability that the process at time n + 1 is in the state j is
conditioned only by the knowledge of the state i in which it stays at the present time
n, it being not crucial to know the state visited at past times in−1, . . . , i0.
The probability P

Xn+1 = j

Xn = i
	
is called transition probability from the
state i to the state j. We write pij (n) the transition probability, at the time n, from
the state i to the state j in one step, that is from the instant n to the following instant
n + 1.
pij (n) = P

Xn+1
time
= j
state

 Xn
time
= i
state
	
Such probability pij (n) can be different if one considers a transition at a time m 6= n,
pij (n) 6= pij (m). The probability that, after a rainy night, the following day is a sunny
day depends on the season. Weather forecasts are obviously not the same in summer
or winter. We have assumed that there exists some model based on Markov chain to
describe changes in weather, as it occurs in the Land of Oz, as we will see below.
There are Markov chains, called time homogeneous or having stationary transition
probabilities. For these chains the transition probability from i to j is always the same,
at any instant, independent of the time t, namely such probability does not depend
on n, ∀n ∈ T. In the following we will only deal with homogeneous chains, so we will
write simply pij , without (n). Symbolically, a Markov chain is homogeneous if:
pij = P

Xn+1 = j

Xn = i
	
, ∀n ∈ T, ∀i, j ∈ S
and it is also:
pij = P

X1 = j

X0 = i
	
It is important not to confuse stationary, or homogeneous, transition probability
with stationary (or invariant or equilibrium) probability distribution.28 Introduction to Stochastic Processes
This distribution plays an essential role in the theory of Markov chains. To say
that a chain is in (or has achieved) a stationary distribution means that the stochastic
process is stationary as discussed in Section 3.1.1. In short, if a chain has a stationary
distribution, then X0 ∼ π, and also Xn ∼ π, ∀n ∈ T.
Let us first consider the case of a finite number of states, equal to N. In this case
S can be the set of integers {1, 2, . . . , N}, or {0, 1, . . . , N − 1}. In the following, both
numerations will be adopted. Transition probabilities pij are collected in a matrix
N × N, called transition probability matrix, or the state transition matrix or simply
transition matrix in one step:
P =


p11 p12 . . . p1N
p21 p22 . . . p2N
. . . . . . . . . . . . . . . .
pN1 pN2 . . . pNN


(3.19)
with the conditions:
pij > 0, ∀i, j ∈ S and X
j∈S
pij (t) = 1, ∀i ∈ S (3.20)
The first condition simply ensures that pij are probabilities, so they cannot be negative.
The second condition means that the rows sum of the matrix must be all 1; indeed, if
the system is in the state i, either it stays in i, or it goes to some other state j. Note:
the columns of the matrix do not in general sum to 1.
The sum P
j∈S means the sum of states in the space S, either as a finite number
N set, or if it is countably infinite.
If eqns (3.20) hold for a matrix, then such a matrix is called a stochastic matrix.
For convention we can write:
p
(0)
ij = δij =
(
1, if i = j
0, if i 6= j
(3.21)
and in matrix form P(0) = I, where I is the identity matrix. The term (0) on the
exponent will be clear in the following.
We ask ourselves: knowing the transition matrix, can we claim to know the whole
process? The answer is negative. Indeed, if the transition matrix is known, we can say
what is the probability that the system will be in the state j, at time n+ 1, if at time n
it is in i. But I have to know what is this state i at the time n, otherwise I have to know
where the system was at the time n − 1, and so on at the time n − 2 etc. In short, I
have to know the state from which the process starts, that is the initial state. Starting
from an initial state, I am able to trace the whole process step by step, by means of
the transition matrix. Therefore transitions probabilities define the probabilistic law
of the chain, conditioned on the initial state. However, the initial states are usually
not known with certainty, but only probabilistically: for instance, we could know that
all the states are equally probable. To sum up, Markov chains are defined by:
1. the transition matrixMarkov processes 29
2. the initial state distribution
Consider a system with four states, say 1, 2, 3, 4. At each time instant t = τ ,
t = τ + 1, t = τ + 2, etc., the system goes to another state, or stays in the same
state, with a particular transition probability. State transition diagrams can be useful
to describe Markov chains, as the example in Fig. 3.3 shows. In the diagram, nodes
represent states, the arrows from state i to state j represent the transition probabilities
pij . If the system stays in the state i, there is a cycle around the state, this means that
pij = 0.
1/2 1/2
1/2
1/2 1/2
1/2
1/4 1/4
1/4
1/3
1/3
2/3 2/3
0 1/4
4
3
1
2
0 0
0 1 0 0
1
0
a)
b)
0
Fig. 3.3 Transition matrix (a) and states transition diagram (b) for the system with four
states.
In the example of Fig. 3.3, if the system is in the state 2, it cannot jump either
to 1, or to 4; it either stays in 2 with probability 1/3, or moves to 3, with probability
2/3. Clearly, the time evolution of the system is not ‘deterministic’: if the system is in
2, it may stay there or not, but this option does not depend on the past visited state.
If the system obeyed a deterministic law, we could state with certainty that either it
goes to 3, or it stays in 2. But also in this case, the option does not depend on its past
place.
The transition matrix of the example, also reported in Fig. 3.3, is:


1 2 3 4
1 1/2 1/4 0 1/4
2 0 1/3 2/3 0
3 0 1 0 0
4 1/2 0 1/2 0


(3.22)
Notice that if the system visits (or starts from) the states 2 or 3, it ‘gets trapped’,
namely it cannot reach any other state. We will look at this in more detail. Now we
would wonder: is it possible to model the time evolution of the system, and in this case
why do we do it? The reason is that by means of simulations we are able to answer
questions such as, for instance, how many steps are necessary to reach the state 3,
starting from 1 ? Actually, this problem is also solvable analytically, but there are
situations which can be dealt with only by means of simulations.30 Introduction to Stochastic Processes
To model the motion of the system, let us consider the transition matrix eqn (3.19),
and suppose that the system starts from the state 1. The vector components, call it,
row1 are the transition probabilities p11, p12, . . . , p1N , according to which the system
moves to states 1, 2, . . . N. The states are chosen through random numbers and, one
after the other, the trajectory is constructed. In R this procedure can be written as
the function below:
markov<- function(x0,n,x,P) { # starting function
s<- numeric()
s[1]<- x0
row<- which(x==x0)
for(i in 2:n) {
s[i]<- sample(x,1,P[row,],replace=T)
row<- which(x==s[i])
}
return(s)
} # ending function
The initial state is x0, and n is the number of transitions executed by the process.
States can be enumerated as 1, 2, . . . , N, or named, for instance, ‘north’, ‘northeast’,
‘east’, and so on. x is the vector containing the state names. The transition matrix is
P[.]. The vector s[.] collects the states visited at each step. Code running transla￾tions between states, as the above function, is called an update function. The whole
code to perform the system trajectory is of the type reported in the following:
## Code_3_1.R
#Example of code to simulate the system motion
set.seed(2)
n<- 20 # number of steps
P<- matrix(c(1/2,1/4,0,1/4,0,1/3,2/3,0,0,1,0,0,1/2,0,1/2,0)
,nrow=4,ncol=4,byrow=T) # transition matrix
P
x<- c(1,2,3,4)
x0<- 1 # initial condition
s<- markov(x0,n,x,P) # call function Markov
t<- c(0:(n-1))
t
s
In R a matrix is constructed using the function matrix(.). To directly create a
matrix as in the above code, the matrix entries are placed along the columns by default.
If byrow=T, the matrix is filled by rows. Matrices are written in R as below:
> P
[,1] [,2] [,3] [,4]
[1,] 0.5 0.2500000 0.0000000 0.25
[2,] 0.0 0.3333333 0.6666667 0.00
[3,] 0.0 1.0000000 0.0000000 0.00
[4,] 0.5 0.0000000 0.5000000 0.00
where t is the vector of steps. The final result is reported below
> t
[1] 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
> s
[1] 1 1 2 3 2 2 2 3 2 3 2 3 2 2 3 2 2 2 3 2 3
> tPredicting the future 31
[1] 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
> s
[1] 1 1 4 1 1 2 3 2 3 2 3 2 3 2 3 2 2 3 2 2 3
The two trajectories are reported in Fig. 3.4. Dashed and dotted lines between states
are to guide the eye.
4
3
2
1
0 5 10
steps
states
15 20
Fig. 3.4 Two trajectories with the same initial state 1 of the process with transition matrix
eqn (3.22).
We see, for instance, that the first trajectory (dashed line) visits the state 3 for
the first time after thee steps, the second trajectory (dotted line), after six steps. It
could happen that the initial state x0 is not known with certainty, but only the initial
distribution of states is available. In this case, an initiation function chooses the initial
state by a random number. So the line x0<- 1 becomes:
v0<-c(0.2,0.1,0.4,0.3) # initial distribution
x0<-sample(x,1,v0,replace=T) # initiation function
In this example, the system starts from state 1, with probability 0.2, from state 2,
with probability 0.1, etc.
Sketching something like a philosophical background of computer experiments on
Markov chains, the basic idea is the following. If the estimate of mean values of pa￾rameters (the number of visits, the first visit to a state, etc.) are required, and those
quantities are representable as random variables, a suitable number of trajectories are
simulated and averages of interested quantities are computed. The term average means
ensemble average, but we must be aware that this kind of averages is not always com￾putable. This is a delicate matter: there are situations in which only one trajectory
can be computed. In this case the ‘average’ is a time average, with all problems of
stationarity, ergodicity, and so on.
3.3 Predicting the future
Through a simple example, we will introduce basic concepts, such as visit probability
after n steps, n−step transition matrix, Chapman−Kolmogorov equation, and more.32 Introduction to Stochastic Processes
The system is the weather over multiple days. Suppose that the weather each day
can be rainy or sunny. Suppose also that the event {tomorrow is rainy} depends only
on the weather today, not on how it was in past days. In the literature, similar models
are usually defined as ‘describing the weather in the Land of Oz’. The random variables
Xn, therefore, can assume only two values, say 0 and 1. If at time n, Xn is 0, the system
is in the state ‘rainy’, and if Xn is 1, the system is in the state ‘sunny’. Index n marks
the days, so we have:
X0
|{z}
0 1
today
X1
|{z}
0 1
tomorrow
X2
|{z}
0 1
after 2 days
X3
|{z}
0 1
after 3 days
X4
|{z}
0 1
after 4 days
. . . Xn
|{z}
0 1
after n days
. . .
Suppose the transition probabilities are known. If today is raining, tomorrow will be
raining with probability 0.7, and then it will be sunny with probability 1 − 0.7 = 0.3.
A similar reasoning applies if today is sunny. The transition matrix is then:


0 1
0 0.7
today rain, tomorrow rain
0.3
today rain, tomorrow sun
1 0.4
today sun, tomorrow rain
0.6
today sun, tomorrow sun

 (3.23)
In terms of random variables, if I know that today is rainy, that is P {X0 = 0} = 1,
the probability that tomorrow is rainy, that is P

X1 = 0

X0 = 0	
is just p00 = 0.7.
If I know that today is sunny, then P

X1 = 0

X0 = 1	
is p10 = 0.4. Suppose now we
know only the probability that today is rainy equal to 0.2, then P {X0 = 1} = 0.8.
The probability that tomorrow is rainy P {X1 = 0}, is given by the Total Probability
Theorem, recalled here.
Let B1, B2, . . . , Bn be mutually exclusive (also called disjoint) events, forming a
partition of the sample space Ω, all with a non-zero probability of occurrence. Let A
be an event ⊂ Ω, we can write A =
Sn
i=1{A, Bi}, then:
P {A} =
Xn
i=1
P {A, Bi} =
Xn
i=1
P

A

Bi
	
P {Bi}
Turning to our problem, we have:
P {X1 = 0} = P

X1 = 0

X0 = 0	
| {z }
p00
P {X0 = 0}
| {z }
v
(0)
0
+ P

X1 = 0

X0 = 1	
| {z }
p10
P {X0 = 1}
| {z }
v
(0)
1
(3.24)
where v
(0)
0
and v
(0)
1
are unconditional probabilities P {X0 = 0} and P {X1 = 0}, re￾spectively. In general, P {Xn = i} or also v
(n)
i
, means the probability that the system
at time n, after n steps, is in the state i:Predicting the future 33
P {Xn = i} ≡ v
(n) −→ number of steps
i −→ state after n steps
Let N be the number of states, then after n steps the system can be in the states
1, 2, . . . , N, with probability v
(n)
1
, v
(n)
2
, . . . , v
(n)
N . The vector
v
(n) = (v
(n)
1
, v
(n)
2
, . . . , v
(n)
N ) (3.25)
is called state vector at n-th step or stochastic vector, or probability vector, being a
vector with non–negative entries which add up to 1:
v
(n)
i > 0 ∀i ∈ S, ∀n ∈ T and X
i∈S
v
(n)
i = 1
In the example, the vector v
(0) = (v
(0)
0
, v
(0)
1
) is the initial probabilities vector (or
initial state vector). Note that in the following, the same symbol v will be used both
to denote a row vector, as in vP, or a column vector, as in Pv.
Turning to eqn (3.24), it is written in general as:
P {X1 = j} =
X
i∈S
P

X1 = j

X0 = i
	
P {X0 = i} , j ∈ S
or in different notation:
v
(1)
j =
X
i∈S
pijv
(0)
i
, j ∈ S
and in matrix notation:
v
(1) = v
(0) P
We ask for the probability that the day after tomorrow is raining, knowing that
today is raining. We have to compute the two-step transition probability. Tomorrow
the state can be either rainy or sunny, therefore the requested probability is the sum
of the probability that tomorrow is rainy again with the probability that tomorrow is
sunny and after tomorrow is rainy:
P

X2 = 0

X0 = 0	
= P{X2 = 0, X1 = 0
tomorrow rain

X0 = 0}
+ P{X2 = 0, X1 = 1
tomorrow sun

X0 = 0}
Recall that P

A, B

C
	
= P {A, B, C} /P {C}, consider the first addend of the
right-hand side:34 Introduction to Stochastic Processes
P

X2 = 0, X1 = 0

X0 = 0	
=
P {X2 = 0, X1 = 0, X0 = 0}
P {X0 = 0}
=
multiplying numerator and denominator for P {X1 = 0, X0 = 0} and recalling that
P

B

A
	
= P {B, A} /P {A}:
=
P {X2 = 0, X1 = 0, X0 = 0}
P {X1 = 0, X0 = 0}
×
P {X1 = 0, X0 = 0}
P {X0 = 0}
= P{X2 = 0

X1 = 0, ✘X✘✘0 = 0
Markov property
} × P

X1 = 0

X0 = 0	
=
for the Markov property the term X0 = 0 in the first brackets is unnecessary, then:
= P

X2 = 0

X1 = 0	
× P

X1 = 0

X0 = 0	
The same procedure is applied to the second addend, so the final result is:
P

X2 = 0

X0 = 0	
= P

X2 = 0

X1 = 0	
| {z }
p00
× P

X1 = 0

X0 = 0	
| {z }
p00
+ P

X2 = 0

X1 = 1	
| {z }
p10
× P

X1 = 1

X0 = 0	
| {z }
p01
Shortly, putting P

X2 = 0

X0 = 0	
as p
(2)
00 , we have:
p
(2)
00 = p00p00 + p01p10 =
X
1
k=0
p0kpk0
We now introduce a third state in the Land of Oz: a snowy day. Let the transition
matrix be:
P =


0 1 2
0 0.6 0.1 0.3
1 0.3 0.5 0.2
2 0.5 0.4 0.1

 (3.26)
To compute the probability that past tomorrow is sunny, if today is snowy, we have
to take into account that tomorrow possible states are rain, sun and snow, denoted as
k = 0, 1, 2, so we have:
P

X2 = 1

X0 = 2	
= p
(2)
21 =
X
2
k=0
p2kpk1
thus p
(2)
21 = 0.5 × 0.1 + 0.4 × 0.5 + 0.1 × 0.4 = 0.29.Predicting the future 35
In general, the two-step transition probability can be written as:
P

Xn+2 = j

Xn = i
	
= p
(2)
ij =
X
k∈S
pikpkj (3.27)
The quantities p
(2)
ij can be considered as entries in a matrix P(2), named a two-step
transition matrix, that is the one-step transition matrix multiplied by itself:
P
(2) = P · P = P
2
note the difference between P(2) and P2
.
Considering only two states 0 and 1, the state vector after two steps is:
v
(2) = (v
(2)
0
, v
(2)
1
)
with:
v
(2)
0 = P {X2 = 0} = p
(2)
00 v
(0)
0 + p
(2)
10 v
(0)
1
v
(2)
1 = P {X2 = 1} = p
(2)
01 v
(0)
0 + p
(2)
11 v
(0)
1
with N states:
v
(2)
j =
X
i∈S
p
(2)
ij v
(0)
i
and in matrix notation:
v
(2) = v
(0) P
(2) = v
(0) P
2
From the Chapman−Kolmogorov equation discussed below, the n-step transition
matrix P(n)
can be obtained by multiplying n times the one-step transition matrix:
P
(n) = P · P · · · P | {z }
n times
= P
n
(3.28)
and the state vector at n-th step:
v
(n) = v
(0) P
(n) = v
(0) P
n
(3.29)
So in conclusion, by knowing the initial probabilities vector v
(0) and the one-step
transition matrix P, we are able through eqn (3.29) to predict the future of the system
after n steps, for all large n, namely we achieve a complete statistical description of
the process, as we stated before: Markov chains are defined by a transition matrix and
an initial distribution over states.
We will prove an equation relating the transition probabilities at different steps,
known as the Chapman−Kolmogorov equation, which is written as:
p
(m+n)
ij =
X
k∈S
p
(m)
ik p
(n)
kj (3.30)
Before proving eqn (3.30), note than eqn (3.27) is a special case of eqn (3.30), with
n = m = 1 (of course, p
(1)
ij is pij ). In matrix form eqn (3.30) is:36 Introduction to Stochastic Processes
P
(n+m) = P
(n)
· P
(m)
and from eqn (3.30), it follows eqn (3.28). Indeed, if m = 1 and n = k − 1, for any
k > 1, it is: P(k) = P(k−1)
· P, and by induction:
P
(n) = P
(n−1)
· P = P
(n−2)
· P
2 = · · · = P · P
n−1 = P
n
Equation (3.30) says that the m+n-step transition probabilities can be expressed as
the sum of the products between the m-step and the n−step transition probability. In
eqn (3.30), the factor p
(m)
ik represents the probability that the process, starting from i,
visits an intermediate state k in m steps, while the factor p
(n)
kj represents the probability
that the process, from k arrives in j in n steps, as shown in Fig. 3.5, as an example.
By adding all intermediate states k, we obtain all possible different trajectories from
i to j in m + n steps. The Markov property ensures that the probability of going from
i to j in m + n steps is independent of the steps the process visited k.
i
m n
j
k
steps
Fig. 3.5 Example of application of the Chapman−Kolmogorov eqn (3.30). Three trajectories,
starting from the state i, end in the state j, in m+n steps, each visiting a different intermediate
state k.
To prove eqn (3.30), the Markov property is exploited. Transition probability
p
(m+n)
ij is given by:
p
(m+n)
ij =
the process
P{Xm+n = j

 X0 = i
starts from i
} =
X
k∈S
P{Xm+n = j, Xm = k
visits k

X0 = i}
Recalling that P

A, B

C
	
= P {A, B, C} /P {C}, within the summation, we have:Predicting the future 37
P

Xm+n = j, Xm = k

X0 = i
	
=
P {Xm+n = j, Xm = k, X0 = i}
P {X0 = i}
=
multiplying numerator and denominator for P {Xm = k, X0 = i}:
=
P {Xm+n = j, Xm = k, X0 = i}
P {Xm = k, X0 = 1}
×
P {Xm = k, X0 = i}
P {X0 = i}
= P{Xm+n = j

Xm = k, ✘X✘✘0 = i
Markov property
} × P

Xm = k

X0 = i
	
= P

Xm+n = j

Xm = k
	
| {z }
p
n
kj
× P

Xm = k

X0 = i
	
| {z }
p
m
ik
In passing, notice that if a process is a Markov process, Chapman−Kolmogorov
eqn (3.30) holds, as we have seen; nevertheless there are discrete-time processes, that
are not Markov processes, for which eqn (3.30) still holds.
Let us turn to the example of the weather in the Land of Oz. The knowledge of the
transition matrix eqn (3.23) and the initial state distribution enable to forecast the
weather in two, four or eight days. Very naively, in R we have to multiply the initial
probability vector v0 by the power of the transition matrix P, as in the short code
below.
##
mult <- function(A,n){
M <- A
if(n>1){
for(i in 2:n) {M <- M %*% A} }
M
}
nm<-2 # for instance
P<-matrix(c(0.7,0.3,0.4,0.6),2,2,byrow=T)
# P # comment if you do not wish the initial matrix printed
for(i in 1:nm){
Pm<-mult(P,i) }
# Pm # reported below
v0<-c(0.2,0.8)
vm<-v0%*%Pm
# vm # reported below
The output is:
> P2
[,1] [,2]
[1,] 0.61 0.39
[2,] 0.52 0.48
> v2
[,1] [,2]
[1,] 0.538 0.462
Four and eight days forecasts are reported below:38 Introduction to Stochastic Processes
> P4 | > P8
[,1] [,2] | [,1] [,2]
[1,] 0.5749 0.4251 | [1,] 0.5714567 0.4285433
[2,] 0.5668 0.4332 | [2,] 0.5713911 0.4286089
> v4 | > v8
[,1] [,2] | [,1] [,2]
[1,] 0.56842 0.43158 | [1,] 0.5714042 0.4285958
We see that:
v
(8)
0 ≈ p
(8)
00 ≈ p
(8)
10
v
(8)
1 ≈ p
(8)
01 ≈ p
(8)
11
That is, as steps increase, transition matrix rows tend to become the same. In fact,
there exists a limit behaviour of the transition matrix, such that it does not change any
further, even if time increases. This limiting distribution does not depend on the initial
probabilities; the system has completely forgotten the initial state and has stabilized
in a stationary condition. Such asymptotic transition matrices P(∞)
, are defined as:
P
(∞) = limn→∞
P
n
3.3.1 Stationarity
We have introduced the asymptotic transition matrix P(∞)
, given by:
P
(∞) =


v
(∞)
1
v
(∞)
2
. . . v
(∞)
N
v
(∞)
1
v
(∞)
2
. . . v
(∞)
N
. . . . . . . . . . . . . . . . . .
v
(∞)
1
v
(∞)
2
. . . v
(∞)
N


=


v
(∞)
v
(∞)
· · ·
v
(∞)


where the row is the convergence limit of the state vector v
(n)
for n → ∞. Then:
v
(∞)
j = limn→∞
p
(n)
ij
is the limiting probability, for n → ∞, that the system is in the state j.
Let N be the number of states. The vector, called the vector of asymptotic proba￾bilities,
v
(∞) = (v
(∞)
1
, v
(∞)
2
, . . . , v
(∞)
N ), with X
i∈S
v
(∞)
i = 1
represents the limiting probability that the system is in the states 1, 2, . . . , N.
Let us introduce another probability vector. Let π = (π1, π2, . . . , πN ) be a prob￾ability vector. We say that the probability vector π is a stationary distribution, or
invariant distribution, or equilibrium distribution for the transition matrix P, if:
1) πi > 0, ∀i ∈ S and X
i∈S
i = 1
2) π = π P that is X
i∈S
πipij = πj , ∀j ∈ S
(3.31)Predicting the future 39
The Property 1) simply says that π describes the probability distribution over
states 1, 2, . . . , N. Property 2) entails the following. Suppose that the initial distribu￾tion v
(0) is equal to π: v
(0) = π. Then:
v
(1) = v
(0)P = πP = π
v
(2) = v
(1)P = πP = π
. . . . . . . . . . . . . . . . . . . . . . .
and by iteration ∀n:
v
(n) = π
which means that the distribution over states remains the same, the initial one. That
is the reason for the term ‘invariant’ distribution.
Let us now compare stationary distributions and asymptotic distributions given by
the asymptotic probability vector. In the example with two state, rain and sun, the
stationary distribution is:
π = (0.57143, 0.42857)
which is also the asymptotic distribution. In this case, we can write:
limn→∞
P
n =

0.7 0.3
0.4 0.6
n
=

v
(∞)
v
(∞)

=

π
π

=

0.57143 0.42857
0.57143 0.42857
It is also:
πi = limn→∞
v
(n)
i
which we found is not always true: the stationary distribution might not be the asymp￾totic one.
Further comments about stationary distributions are now in order. In our example,
we know that, if today is rainy, the probability that tomorrow is rainy is 0.7, or is 0.4
if today is sunny. At stationarity, practically after eight days, the probability that
it is rainy is 0.57, no matter what the weather is today, and this probability always
remains the same. Then, after eight days, choosing any day, we are able to say that the
probability that it will be a rainy day is 0.57, not depending on initial probabilities.
In essence: at stationarity, the probability of observing the system in a certain
state, does not depend on the chosen time instant. From another point of view, we can
say that 57 days out of 100 will be rainy, and 43 will be sunny. Such a reasoning rests
on frequency interpretation (better: conception) of probability. To say that πi
is the
probability that the system at stationarity is in the state i means to say that πi
is the
limiting frequency distribution of the occupation time of the state i at equilibrium.
The occupation time is the time spent by the process in a state, or in a given
subset, of the state space S. Let us see a couple of examples of a chain with only two
sates 0 and 1. Let the transition matrix be:
P =

1 − a a
b 1 − b

(3.32)40 Introduction to Stochastic Processes
suppose 0 < a, b < 1. Equations (3.31) are written as:
π0 = (1 − a)π0 + bπ1
π1 = aπ0 + (1 − b)π1
π0 + π1 = 1
(3.33)
The solution is immediate:
π0 =
b
a + b
e π1 =
a
a + b
(3.34)
In conclusion, for 0 < a, b < 1, stationary and asymptotic distributions exist and
coincide. Indeed, with a = 0.3 and b = 0.4 of the above example, we find just π =
(0.5714285714, 0.4285714286), (see also Fig. 3.6a). With a = b = 1, we have:
a)
b)
c)
0.7 0.3
0.6 0
0.7
0.3
0.4
1
0.6
0.4
0. 0
0
1 1
1
1
1
1
1. 0.
1. 0.
1.
0. 1.
Fig. 3.6 Examples of systems with two states and different transition matrices.
P =

0 1
1 0
, P
2 =

1 0
0 1
, P
3 =

0 1
1 0
, P
4 =

1 0
0 1
, . . .
It is clear that here no asymptotic matrix exists, since there is no limn→∞ p
(n)
ij . Indeed,
p
(n)
00 = 1, for n > 1 even, and = 0, for n > 1 odd. That is to say, the states 0 and 1 are
periodic, 0 is visited at steps 2, 4, 6, . . . , and 1 at steps 1, 3, 5, . . . , (see also Fig. 3.6b).
Nevertheless the stationary distribution exists and is, by eqn (3.34), π = (0.5, 0.5).
This means that the system spends half the time in state 0, and half the time in state
1.
Lastly, take a = b = 0 (see Fig. 3.6c). The system remains forever in the initial
state, and obviously it is limn→∞ P(n) = Pn = P, ∀n. Stationary distributions are
infinite, being always π = π P, for any component π0 and π1 of π. In similar cases,
the transition matrix is the identity matrix.Predicting the future 41
The question is: under which conditions is the stationary distribution unique and
equal to the asymptotic distribution? Or in other words, when can we write, for any
initial distribution v
(0) ?
limn→∞
v
(0)P
n = π, ∀v
(0)
A general approach to deriving the stationary distribution (if it exists) is to view
equation π = π P as an eigenvalue equation. By means of its eigenvalues and eigen￾vectors, the matrix P can be diagonalized, that is D = A−1PA, where A columns are
the right eigenvectors of P, and A−1
rows are the left eigenvectors. Then the diagonal
matrix is:
D =


λ1 0 · · · 0
0 λ2 · · · 0
.
.
.
.
.
. · · ·
.
.
.
0 0 · · · λN


(3.35)
Given the diagonal matrix, it is not difficult, in general, to raise P to n, and to take
the limit n → ∞. The method based on eigenvalue analysis, while certainly elegant, is
not applicable if the number of states is infinite. In the following part of this section,
examples in R will be provided.
To wonder if a stationary distribution exists is to wonder if a non-negative left
eigenvector of P exists, corresponding to an eigenvalue equal to 1, all others have
modulus 6 1. P is a stochastic matrix (rows add up to 1), it always has the eigenvector
u = (1, . . . , 1) corresponding to the eigenvalue 1. In conclusion, for a finite state chain,
there exists always at least one stationary distribution. To wonder if the stationary
distribution is unique, is to wonder if the chain is ergodic.
P is ergodic if ∃m such that for ∀n > m, p
(n)
ij > 0, ∀i, j ∈ S
This means that, starting from a certain n, matrices Pn (n−step transition matrix)
have all elements strictly greater than zero, no matter if P (1-step transition matrix)
has some entry equal to zero. It seems that, at least prima facie, if the chain is periodic,
then it cannot converge to equilibrium, but, if after a certain time n, all the states
communicate to each other, there cannot be periodicity, so the chain converges to
equilibrium.
If P is ergodic, then:
a) 1 is an eigenvalue of P. It is single and all the other eigenvalues are less than 1 in
absolute value.
b) a state vector π exists and is unique, which is also an eigenvector of P corresponding
to eigenvalue 1
c) let v be any state vector, even of the initial state, then vPn → π, for n → ∞.
So to recap: stationary distribution exists, it is unique and coincides with the asymp￾totic distribution if the transition matrix is ergodic. In this case, the chain is said to
be asymptotically stationary.
An ergodic Markov chain is sometimes called ‘regular’ in the literature. Ergodic
chains can be regular or periodic, as we will see in the next section.42 Introduction to Stochastic Processes
3.3.2 Classification of states
States are named according to some specific features. We have seen, for instance, an
example of periodic states, eqn (3.32) with a = b = 1, while with a = b = 0, it is
p00 = p11 = 1. In general, if pii = 1, the state i is called absorbing, and the system
stays at the state forever, once it enters the state. This does not mean that the process
has stopped: time keeps running and the system continues to remain in the same state.
We say that a state j is accessible from state i if the system starting from i has the
possibility of reaching j in some number n > 0 of steps. Note n > 0, since for n = 0 it
results p
(0)
ij = δij (see eqn (3.21)). In symbols:
j ← i : ∃n > 0 such that p
(n)
ij > 0
Obviously, j ← i and i → j are equivalent. The accessibility relation has the following
properties (∀i, j, k ∈ S):
reflexive: any state is accessible from itself, in fact p
(0)
ii = 1
transitive: if i → j and j → k, then i → k
Indeed, the relation k ← i means ∃r > 0 such that p
(r)
ik > 0. If r = n + m from
Chapman−Kolmogorov equation (3.30), since p
(n)
ij and p
(m)
jk > 0, it results in:
p
(n+m)
ik =
X
q∈S
p
(n)
iq p
(m)
qk > p
(n)
ij p
(m)
jk > 0
Note that the property of accessibility is not symmetric. For instance, in the matrix
eqn (3.22) (see also Fig. 3.3) it is 2 ← 1, but 1✟← 2.
Two states accessible from each other (i → j and j → i) are said to communicate.
j ↔ i : ∃n, m > 0 such that p
(n)
ij > 0 and p
(m)
ji > 0
Communicability is an equivalence relation (∀i, j, k ∈ S), i.e. it has the following
properties:
reflexive: any state communicates with itself, indeed p
(0)
ii = 1.
symmetric: if i ↔ j then j ↔ i, by definition.
transitive: if i ↔ j and j ↔ k then i ↔ k. The above is a proof for k ← i, for i ← k.
The communicability relation divides the state space S into disjoint communicating
classes (a class can include only one state), such that all states belonging to the same
class communicate with each other, but not if they are in different classes. A chain is
irreducible if there is only one class, that is all states communicate with each other.
As a slogan, we could say: from anywhere to anywhere, and more formally a chain is
irreducible if:
∀i, j, ∃n, m > 0 such that p
(n)
ij > 0 and p
(m)
ji > 0
Example 3.1 A chain having the transition matrix:Predicting the future 43
P =


0 1 2
0 1/2 1/2 0
1 1/2 1/4 1/4
2 0 1/3 2/3

 (3.36)
is irreducible.
Note that the state 0 and the state 2 communicate through a two-step transition:
0
1/2
−−→ 1
1/4
−−→ 2 and 2
1/3
−−→ 1
1/2
−−→ 0.
Example 3.2 The chain with transition matrix:
P =


0 1 2 3
0 1/3 2/3 0 0
1 2/3 1/3 0 0
2 1/4 1/4 1/4 1/4
3 0 0 0 1


(3.37)
it is not irreducible, that is reducible (see Fig. 3.7).
1/3
2/3
1/3
1/4
1/4 1/4
1/4
1/3
1/3
2/3
2/3
1/4 1/4 1/4 1/4
0
2
0
0
0 0 1 0
0
1
3
2/3
1
0
Fig. 3.7 States transition diagram of the reducible chain eqn (3.37).
We have three classes: {0, 1}, {2}, {3}. Observe that if the states 0 or 1 are accessible
from 2, this state is accessible from no other states. Then, the system may be in 2,
only if it starts from this state; once it get out, it never comes back. The state 3 is
absorbing, that is pii = 1; an irreducible chain can have no absorbing state.44 Introduction to Stochastic Processes
Irreducibility can be defined via the notion of closed class. A set of states C ⊂ S
is closed if it is impossible to exit, that is if i ∈ C and j /∈ C, then p
(n)
ij = 0, ∀i ∈ C,
∀j /∈ C and ∀n. A chain is irreducible if there exists only one closed set, namely the
state space S. In the above example (matrix eqn (3.37)), the subset C = {0, 1} is a
closed irreducible class. Also the subset {0, 1, 3} is a closed class, but not irreducible.
Consider the transition matrix:
P =


0 1 2 3 4
0 0.7 0.3 0 0 0
1 0.4 0.6 0 0 0
2 0 0 0.6 0.1 0.3
3 0 0 0.3 0.5 0.2
4 0 0 0.5 0.4 0.1


If the process starts from states 0 or 1 belonging to the subset C1 = {0, 1} of S,
never leaves these states, and similarly if it starts from one of the states belonging to
the subset C2 = {2, 3, 4}. The sets C1 and C2 are closed and irreducible classes.
If the process starts from C1, it can be described by the transition matrix with only
states 0 and 1 (we have chosen the matrix eqn (3.23)), with its asymptotic stationary
distribution, is, as we know, π = (0.5714, 0.4286). It is the same reasoning, if the
process starts from C2 (the matrix is eqn (3.26). In this case, its asymptotic stationary
distribution is (to four decimal places) π = (0.4933, 0.2800, 0.2267). In this sense, we
speak of an ‘reducible’ matrix, since the state space S can be ‘reduced’ to one or more
subsets of S.
Clearly, if the chain is reducible, there cannot be a unique asymptotic distribution,
as it depends on the initial state. But for the existence of the asymptotic distribution,
the chain needs to be aperiodic, not just irreducible.
The occupation times of the states at stationarity can be obtained with the R code
Code_3_2.R below. We will see later how to derive it analytically in a simple way.
## Code_3_2.R
# Weather in the Land of Oz
# States: rain (0), sun (1), snow (2)
markov<- function(x0,n,x,P) { # starting function
s<- numeric()
s[1]<- x0 # initial state
raw<- which(x==x0)
for(i in 2:n) {
s[i]<- sample(x,1,P[raw,],replace=T)
raw<- which(x==s[i])
}
return(s)
} # ending function
set.seed(5) # reset random numbers
n<- 1000 # number of steps
# transition matrix:
P<-matrix(c(0.6,0.1,0.3,0.3,0.5,0.2,0.5,0.4,0.1),3,3,byrow=T)
P
x<-c(0,1,2) # x is the state space
x0<-0
s<-markov(x0,n,x,P) # visited statesPredicting the future 45
t<-c(0:(n-1))
# comment if you do not wish t and s values printed
# t
# s
# for clarity, stop the plot at npl
npl<-50
par(mai=c(1.02,1.,0.82,0.42)+0.1) # to control the margin size
plot(t[0:npl],s[0:npl],type="s",col="black",xlim=c(0,npl),
ylim=c(0,2),xlab="days",ylab="states",
cex.lab=1.3,font.lab=3,lwd=1.7,yaxt="none")
axis(2,seq(0,2,1))
# alternative representation: plot(t,s,type="p", ...
# ............................................................
# number of visits to states during the time-interval [0,n-1],
# given that the initial state is 0 (rain).
# number of raining, sunny and snowing days
n_rain<- numeric()
n_sun<- numeric()
n_snow<- numeric()
# visiting probability
P_rain<- numeric()
P_sun<- numeric()
P_snow<- numeric()
# The initial state (x0<- 0) is known
n_rain[1]<- 1
n_sun[1]<- 0
n_snow[1]<- 0
P_rain[1]<- 1
P_sun[1]<- 0
P_snow[1]<- 0
# stationary probability distribution
for(i in 2:n) { # starting loop on steps
n_rain[i]<- n_rain[i-1]
n_sun[i]<- n_sun[i-1]
n_snow[i]<- n_snow[i-1]
if(s[i]==0){n_rain[i]<- n_rain[i-1]+1 }
if(s[i]==1){n_sun[i]<- n_sun[i-1]+1 }
if(s[i]==2){n_snow[i]<- n_snow[i-1]+1 }
P_rain[i]<- n_rain[i]/i
P_sun[i]<- n_sun[i]/i
P_snow[i]<- n_snow[i]/i
} # ending loop on steps
# number of total visits
n_rain[n]
n_sun[n]
n_snow[n]
# stationary probability vector
par(mai=c(1.02,1.,0.82,0.42)+0.1) # to control the margin size
plot(P_rain,type="s",col="black",xlim=c(0,n),
ylim=c(0,1),xlab="days",ylab="prob. vector",
cex.lab=1.3,font.lab=3,lwd=1.7)
lines(P_sun,type="s",lty=2,lwd=1.7)
lines(P_snow,type="s",lty=3,lwd=2)
# ........... time average
ta<- 600
tb<- 1000
l_Int<- length(ta:(tb-1))
timeAv_rain<- mean(P_rain[ta:(tb-1)] )46 Introduction to Stochastic Processes
timeAv_rain
se.rain<- sd(P_rain[ta:(tb-1)])/sqrt(l_Int)
se.rain
timeAv_sun<- mean(P_sun[ta:(tb-1)] )
timeAv_sun
se.sun<- sd(P_sun[ta:tb])/sqrt(l_Int)
se.sun
timeAv_snow<- mean(P_snow[ta:(tb-1)] )
timeAv_snow
se.snow<- sd(P_snow[ta:tb])/sqrt(l_Int)
se.snow
The first ten steps are:
> t
[1] 0 1 2 3 4 5 6 7 8 9 10 ...
> s
[1] 0 0 2 2 0 0 2 1 2 2 ...
As is also shown in Fig. 3.8, we have ‘rain’ for the first two days and the third day it
is snowy (the initial time, that is first day, is t = 0). The first sunny day is the eighth
day.
0 1 0 2 0 3 0 4 0 5 0
days
states
0 1 2
Fig. 3.8 First 50 steps of the trajectory of the process with transition matrix eqn (3.26).
The numbers identifying the rainy days are 1 (initial day), 2 (1 + 1), up to the
rainy fifth day. The first numbers of sunny and snowy days are also reported.
> n_rain
[1] 1 2 2 2 3 4 4 4 4 4 ...
> n_sun
[1] 0 0 0 0 0 0 0 1 1 1 ...
> n_snow
[1] 0 0 1 2 2 2 3 3 4 5 ...Predicting the future 47
The components of the probability vector π describing the probability distribution
over states, at stationarity, are computed:
> P_rain
[1] 1.0000000 1.0000000 0.6666667 0.5000000 0.6000000 0.6666667 0.5714286
[8] 0.5000000 0.4444444 0.4000000 ...
> P_sun
[1] 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000
[7] 0.00000000 0.12500000 0.11111111 0.10000000 ...
> P_snow
[1] 0.0000000 0.0000000 0.3333333 0.5000000 0.4000000 0.3333333 0.4285714
[8] 0.3750000 0.4444444 0.5000000 ...
For instance, for the rainy day the probability of the system of staying in the state
‘rain’ is 1 for the first two days, since we know that ‘rain’ is the starting state. The
third day, this probability becomes 2/3 ∼ 0.6666667, and the day after, it is 2/4 = 0.5,
and so on, and the same for the other states. However, we have to keep in mind that
the probability distribution refers to stationarity, and stationarity is reached after a
certain number of steps. Figure 3.9 shows the time evolution of the components of the
probability vector π.
0 200 400 600 800 1000
0.0 0.2 0.4 0.6 0.8 1.0
days
prob. vector
Fig. 3.9 Time evolution of the components of the probability vector π of the process with
transition matrix eqn (3.26). Rain: continuous line, sun: dashed line, snow: dotted line.
We see that the stationarity is reached after, more or less, 600 steps. We can perform
the time average in the interval [600, 1000], obtaining:
timeAv_rain: 0.4820, timeAv_sun: 0.2837, timeAv_snow: 0.2343
in good agreement with the results from the short code presented above.
Adding to the code Code_3_2.R the following instructions, we can estimate the
ensemble average. For the sake of brevity only the state ‘rain’ is considered.48 Introduction to Stochastic Processes
## Code_3_2.R continues
# ........... ensemble average
set.seed(5) # reset random numbers if wished
n<- 1000 # number of steps
nhist<- 100 # number of histories
mstep<- 900 # compute the ensemble average at step = mstep
nen<-numeric()
PM_rain<-matrix(,nhist,n) # to save all histories
par(mai=c(1.02,1.,0.82,0.42)+0.1) # to control the margin size
plot(P_rain,type="n",col="black",xlim=c(0,n),
ylim=c(0,1),xlab="days",ylab="P_rain",
cex.lab=1.2,font.lab=3,lwd=1.7)
for(l in 1:nhist){ # starting loop on histories
x0<-0
s<-markov(x0,n,x,P)
n_rain[1]<- 1
P_rain[1]<- 1
PM_rain[l,1]<- 1
for(i in 2:n) {
n_rain[i]<- n_rain[i-1]
P_rain[i]<- P_rain[i]/i
PM_rain[l,i]<- P_rain[i]
if(s[i]==0){n_rain[i]<- n_rain[i]+1 }
P_rain[i]<- n_rain[i]/i
PM_rain[l,i]<- P_rain[i]
}
# for clarity, stop the histories plot at hpl
hpl<-20
if(l<=hpl)lines(PM_rain[l, ],type="s",lty=3,lwd=1)
} # ending loop on histories
for(i in 1:n){
nen[i]<- mean(PM_rain[,i]) } # "mean history"
lines(nen,lwd=2,type="s",col="black",lty=1) # better: col="red"
abline(v=mstep,lty=4,lwd=2,col="black") # here ensemble average
m_ens<- mean(PM_rain[,mstep])
m_ens
se.ens<- sd(PM_rain[,mstep])/sqrt(nhist)
se.ens
In the code above, nhist<- 100 different trajectories are computed, and the ensem￾ble averages are performed at step mstep<- 900. Figure 3.10 shows the time evolution
of 20 trajectories out of 100 of the component of the probability vector π referring
to the state ‘rain’. The continuous line depicts the ‘mean trajectory’ on nhist<- 100
trajectories. The step at which the ensemble average is computed mstep<- 900 is also
shown (dot-dashed line).
The two averages, time and ensemble, are quite similar: time average = 0.4820
and ensemble average = 0.4928. The differences is the standard errors of the mean:
they are 0.0003218 and 0.002196, respectively. The reason is that the time series of
the components of the probability vector π are highly correlated.Predicting the future 49
0 200 400 600 800 1000
0.0 0.2 0.4 0.6 0.8 1.0
days
P_rain
Fig. 3.10 Time evolution of 20 trajectories out of 100 of the component of the probability
vector π referring to the state ‘rain’ of the process with transition matrix eqn (3.26). Contin￾uous line: ‘mean trajectory’ on nhist<- 100 trajectories. Vertical dot-dashed line: time (days
= 900) at which the average is performed.
3.3.3 Periodic and aperiodic states
Consider the following transition matrix:
P =


0 1 2 3
0 0 0 0.6 0.4
1 1 0 0 0
2 0 1 0 0
3 0 1 0 0


(3.38)
and follow the trajectory of the system, assuming that it starts from state 2. Possible
trajectories are:
2 → 1 → 0 → 2 → 1 → 0 → 2 → 1 → 0 → 2 → 1 · · ·
2 → 1 → 0 → 3 → 1 → 0 → 2 → 1 → 0 → 2 → 1 · · ·
2 → 1 → 0 → 2 → 1 → 0 → 3 → 1 → 0 → 3 → 1 · · ·
We immediately see that the chain is irreducible, and also that states 0 and 1
occur every three steps. State 2 does not occur every three steps, but when it occurs,
it is every three steps. The same holds for state 3. The states of this chain are called
periodic with period 3. A state i is periodic with period d
(i)
if the chain starting from
i can return to i only at a multiple of some n steps, where n = d
(i)
, 2d
(i)
, 3d
(i)
, . . .,
and d
(i)
is the greatest integer with this property. More formally, a state i is periodic
with period d
(i)
if p
(n)
ii = 0, except for the n values that are multiples of some integer50 Introduction to Stochastic Processes
d
(i)
, and if there exist more integers with this property, d
(i)
1
, d(i)
2
, . . . , d(i)
m , then d
(i) =
max(d
(i)
1
, d(i)
2
, . . . , d(i)
m ).
Formally:
d
(i) = gcd {n such that p
(n)
ii > 0}
where ‘gcd’ is the greatest common denominator of all integer n > 0, for which p
(n)
ii > 0.
Periodicity is a class property. If for the state i, d
(i) = 1, the state i is called aperiodic.
A Markov chain is aperiodic if every state is aperiodic, otherwise it is periodic.
If the chain is periodic, an asymptotic distribution v
(∞) does not exist (as we have
seen for matrix eqn (3.32) with a = b = 1). For the matrix eqn (3.38), it is indeed:
P2 =


0 1 0 0
0 0 0.6 0.4
1 0 0 0
1 0 0 0

 , P3 =


1 0 0 0
0 1 0 0
0 0 0.6 0.4
0 0 0.6 0.4

 ,
P4 ≡ P =


0 0 0.6 0.4
1 0 0 0
0 1 0 0
0 1 0 0

 , . . .
However, periodic chains are allowed to possess an invariant distribution π, inde￾pendent of the initial distribution. In the example above, we see that the system stays
for a third of the time in the states 0 and 1, then π0 = π1 = 1/3, in the remaining
third in the states {2, 3}, then π2 = 1/3 × 0.6 = 0.2 and π3 = 1/3 × 0.4 = 0.133.
In general, we can say that the succession of transition matrices P(n) of an ir￾reducible and periodic chain does not converge to the asymptotic matrix P(∞)
. If,
however, the sequence of arithmetic means:
1
n
h
P + P
(2) + · · · + P
(n)
i
is considered, then for n → ∞, such a sequence converges to a limiting matrix P∗ with
all positive entries and all equal rows. In the above example, the rows are the vector
π = (0.333, 0.333, 0.2, 0.133). When the convergence of arithmetic mean succession is
taken into account, it is referred as ‘convergence in the Cesaro sense’.
Some theorems exist characterizing irreducible chains. One of them states that all
states in an irreducible chain are of the same type. This means that, for instance, states
are all aperiodic or all periodic, and in this case they all have the same period. Then,
in an irreducible chain, if ∃i such that pii > 0, we can state that ∀i ∈ S it is pii > 0
and, therefore, that the chain is aperiodic. Sufficient, but not necessary, condition.
A theorem states that: for irreducible and aperiodic chain, there exists a number
m such that for ∀n > m, all entries of the n-step transition matrix Pn are strictly
positive, namely, as previously stated, the chain is ergodic. It follows that there exists
a unique stationary distribution coinciding with the asymptotic distribution. It should
be noted, however, that if the chain has a countably infinite set of states, for ergodicity
it is not enough that the chain is irreducible and aperiodic, as we will see later.
The proof is rather simple. If i and j are any two states in S, since the chain is
irreducible, there exists q such that p
(q)
ij > 0. The chain is also aperiodic, thereforePredicting the future 51
there exists r such that from a certain n > r onwards it gives p
(r)
ii > 0. If m is the
greatest value between q and r, for all pair i and j, the theorem holds.
As an example, turn to the irreducible chain with transition matrix eqn (3.36). It
is p02 = p20 = 0. Moreover, matrix P(2) has all entries strictly positive:
P
(2) =


0 1 2
0 0.500 0.375 0.125
1 0.375 0.396 0.229
2 0.167 0.306 0.528

 (3.39)
The chain is ergodic and there exists a unique stationary distribution coinciding with
the asymptotic distribution. It is:
π = (0.364, 0.364, 0.273)
3.3.4 Stopping time and other relevant random times
Turn again to the matrix eqn (3.22). We see that if the system starts from state
1 or 4, it can visit these states a certain number of times, until it enters state 2 or 3,
after which these states are visited infinite times. There are states, called recurrent,
that are revisited infinite times, and fleeting states, called transient, which are visited
only a finite number of times.
To characterize the two types of state, let us introduce new random variables, the
first of which is the random time. A random time T is a discrete random variable which
takes values in the parameter space T = {0, 1, 2, . . . }. Then, the event {XT = i} means
that the process at the random time T is in the state i. A particular random time is
the stopping time, a name which may be a bit misleading, since it does not mean that
the process stops for certain values of T.
We say that T is a stopping time with respect to the stochastic process {Xt} if the
occurrence, or the non-occurrence, of event {T = n} can be exhaustively determined
only by observing the values assumed by the variables {X0, X1, . . . Xn}. It is ‘stopping
time’ since if we want to stop at time T, we know when to do it by looking at the
process until T, with no need to know the future. In the heads and tails game, a
stopping time could be the moment to stop gambling after, for instance, ten heads:
T = min{n > 0 : X0 + · · · + Xn = 10}
(if Xn = 0 : tail, if Xn = 1 : head)
A time which is not a stopping time is the last exit time, the time of the last visit
of a certain state. To say that the process never comes back to this state, you have
to know not only the time at which the system leaves the state, but also its future
evolution.
Another relevant random time is the time of the first visit of the process (or of the
first passage) into the state i, also known as the hitting time to state i. The random
variable Ti
is a first visit time to state i if:52 Introduction to Stochastic Processes
Ti = min{n > 1 : Xn = i}
that is Ti takes the value n when no variable X1, X2, . . . , Xn−1 takes the value i, but
i is assumed by Xn. Therefore, the event:
the process visits i for the first time at time n}
can be expressed as:
{Ti = n} = {X1 6= i, X2 6= i, . . . , Xn−1 6= i, Xn = i}
Note in Ti
, the subscript i refers to that particular state i, while n is a time, that is
the value taken by the random variable Ti
.
A couple of observations. If the initial value is n = 1, in 0 step the process does not
enter i, unless it is not already there. The random variable Ti can assume the value
+∞ with probability > 0, therefore it is a special random variable. Lastly, note that
in the Ti definition the initial state is not mentioned. More generally, we can speak of
the time of the first visit into the set of states {A}, for instance {A} = {0, 2, 4}, and
write
T{A} = min 
n > 1 : Xn ∈ {A}
	
A gambler decides to stop playing when his pockets (the system) are in the state 0
(he lost all) or when the state is 1000 (he won 1000 euro). In this case {A} = {0, 1000}.
3.3.5 Strong Markov property
We have defined a Markov process as one having the property of being memory￾less, i.e. if the system at time n (present) is in the state i, {Xn = i}, the future
{Xn+1, Xn+2, . . . } does not depend on the past {Xn−1, . . . , X0}. If, instead of the ‘de￾terministic’ time n, we consider the ‘random’ time T, stopping time, we obtain the
definition of a strong Markov property.
We can rewrite the above definition of a Markov process with the stopping time T
in place of n to define the strong Markov property: if the system at time T (present)
is in the state i, {XT = i}, the future {XT +1, XT +2, . . . } does not depend on the
past {XT −1, . . . , X0}. The meaning of this definition is the following. I know that T
is a stopping time, and that {XT = i}. Then any other information is irrelevant to
the process {XT +k}, k > 0. In other words, I have the Markov chain starting from a
certain t = 0. Let T be a stopping time with {XT = i}. It follows that the process
{XT +k}, k > 0 is the same original Markov chain {Xt}, but with the initial state i. We
want to stress here that in the definition of the strong Markov property, the essential
point is that T is a stopping time.
3.3.6 Recurrent and transient states
Turn to the first visit time in terms of which accessibility can be defined. Consider a
pair of states i and j, ∀i, j ∈ S and i 6= j. We say that j is accessible from i if:
fij = P

Tj < ∞

X0 = i
	
> 0 note: it is not = 0Predicting the future 53
We have introduced the probability fij , not null, that the system starting from i hits
for the first time j in a finite time. To be more specific, define the probability f
(n)
ij
that the system starting from i, takes the time n for visiting j for the first time:
f
(n)
ij = P

Tj = n

X0 = i
	
{X1 6= j, . . . , Xn−1 6= j, Xn = j

X0 = i} (3.40)
Note that f
(0)
ij = 0, since the process cannot go from i to j in zero steps, if it is not
i = j, but in this case f
(0)
ii = 1. We can write fij as a function of f
(n)
ij , by summing
on n:
fij =
X∞
n=1
P

Tj = n

X0 = i
	
=
X∞
n=1
f
(n)
ij
Let us introduce the probability fii that the system starting from i returns in i:
fii = P

Ti < ∞

X0 = i
	
= Pi

Ti < ∞
	
(3.41)
where the probability of event {A} with the process starting from i is written in short
as:
P

A

X0 = i
	
= Pi

A
	
By analogy with eqn (3.40), define the probability f
(n)
ii that the process, starting
from i, returns there after n steps:
f
(n)
ii = P

Ti = n

X0 = i
	
(3.42)
From this:
fii =
X∞
n=1
f
(n)
ii
On the basis of eqn (3.41), the notion of recurrence can be introduced. The state i is
recurrent if:
P

Ti < ∞

X0 = i
	
= Pi

Ti < ∞
	
= 1 (3.43)
A recurrent state is often called persistent, because a system starting from such a state
returns to it with certainty. By contrast, the state i is transient if:
P

Ti < ∞

X0 = i
	
= Pi

Ti < ∞
	
< 1
In general, f
(n)
ii is the probability that the process, being in i at any time m, returns
to i after n steps. Recurrence and transience are written:
P

Xn+m = i

Xm = i
	
−−−−−−→ n→∞ 
1 i is recurrent
0 i is transient
With fii given by
fii =



1 or f
(n)
ii −−−−→ n→∞
1 i is recurrent
< 1 or f
(n)
ii −−−−→ n→∞
0 i is transient
If fii = 1 the process will return to state i any time it has visited that state. But thanks
to the strong Markov property, as it is recurrent, it will be visited an infinite number54 Introduction to Stochastic Processes
of times. Clearly, the absorbing state, pii = 1, is also recurrent, since the process stays
forever in i. If i is recurrent, the transition probability p
(n) has to be greater than zero
for some n value.
If state i is not recurrent, but transient, then the process can return to i only with
a certain probability, so that there is a non-zero probability that the process will never
return to i after leaving it. This means that a transient state after a certain time will
never occur in the chain.
Generalize Ti
, first visit time to i, in T
k
i
, time of the k-th visit to i:
T
k
i = min{n > T k−1
i

Xn = i}
We have defined fij , the probability that the system starting from i hits for the
first time j in a finite time, similarly fji for j to i. Then, we start from j and go to
i for the first time, then we will visit it again k − 1 times. This is expressed by the
following events:
{j −→ i first time} and {i −→ i k − 1 times}
For the strong Markov property the two events are independent, therefore:
P

T
k
i < ∞

X0 = j
	
= fji f
k−1
ii
An important notion is the mean recurrence time µi
, defined as the expected value
of the random variable Ti
, conditioned on the event that the process starts from i,
that is:
µi = E [Ti

X0 = i] ≡ Ei
[Ti
] (3.44)
Let i be recurrent, then if the states of the chain are infinite, µi can be finite or infinite,
If µi
is finite the state i is said to be positive recurrent, while if µi
is infinite the state
i is said to be null recurrent. Clearly, if the states of the chain are finite, the state i is
either recurrent or transient, without further specification.
If i is recurrent, f
(n)
ii is a probability distribution; indeed, in this case P∞
n=1 f
(n)
ii =
1. As further consequence, we can define the mean recurrence time through f
(n)
ii in
eqn (3.42):
µi = E [Ti

X0 = i] = X∞
n=1
n f(n)
ii if 
< ∞ i is positive recurrent
+∞ i is null recurrent
Recurrence has been characterized through the random variable Ti(and its general￾izations, as T
k
i
) which informs about the time taken by the process to reach a certain
state i. Now, recurrence is characterized through the number of times that a state i is
visited. Let Vi be the random variable counting the number of times that the process
visits i, defined as:
Vi =
X∞
n=0
1(Xn=i)
where 1(·)
is the indicator function taking value 1, if the condition (·) is verified, and
0 otherwise. In our case, 1(Xn=i) = 1, if Xn = i, while 1(Xn=i) = 0, if Xn 6= i.Predicting the future 55
The expected value of Vi
is:
Ei
[Vi
] ≡ E

Vi

X0 = i

=
X∞
n=0
p
(n)
ii
Indeed:
Ei
[Vi
] = E"X∞
n=0
1(Xn=i)

X0 = i
#
=
X∞
n=0
E
"
1(Xn=i)

X0 = i
#
(3.45)
since the expected value of a sum of random variables is equal to the sum of their
expected values. We have:
E

1(Xn=i)

= P {Xn = i}
because by definition the indicator function of an event takes the value 1 when the
event occurs and 0 when the event does not occur. Then eqn (3.45) is equal to:
X∞
n=0
P

Xn = i

X0 = i
	
=
X∞
n=0
p
(n)
ii
A theorem can be proved, which says:
state i is recurrent if and only if Ei
[Vi
] = ∞
If i is recurrent, the number of visits in i is infinite. If i is transient the number of
visits in i is finite and the expected value of Vi
is finite:
E

Vi

X0 = i

=
fii
1 − fii
Also P∞
n=0 p
(n)
ii is finite.
We said that the states of an irreducible chain are all of the same type. Therefore
if a state is recurrent and the chain is irreducible, all the state are recurrent. But in
a finite-state Markov chain, not all states can be transient, since after a certain time
there are no more states to be reached. Only if the process has infinite states, can they
be all transient states.
In summary, for a finite-state chain, to say ‘irreducible’ or to say that ‘all states are
recurrent’ is the same thing. A finite-chain with all states recurrent (that is irreducible)
and aperiodic is ergodic. If the number of states is infinite, the chain is ergodic only if
it is aperiodic and it has positive recurrent states.
If both types of state belong to the chain, the state space can be partitioned in a
disjoint set of states: T , R1, R2, . . . , such that:
1. T it is the set of all transient states.
2. if i ∈ Rl
, then: 
fik = 1, ∀k ∈ Rl
fik = 0, ∀k /∈ Rl
Therefore Rl
is a irreducible set of states with only recurrent states.56 Introduction to Stochastic Processes
Example 3.3 Let us consider the following transition matrix:
P =


0 1 2 3 4
0 1/2 1/2 0 0 0
1 1/2 1/2 0 0 0
2 0 0 1/2 1/2 0
3 0 0 1/2 1/2 0
4 1/4 1/2 0 0 1/2


States belonging to the set R1 = {0, 1} communicate with others allowing to visit again
the states in R1 and are recurrent, and so are those belonging to the set R2 = {2, 3}.
The set T = {4} is composed of only one transient state. In fact, if the process starts
from the state 4, it cannot return there, once it entered the closed set R1 = {0, 1}.
Figure 3.11 schematically shows the states classification discussed so far.
state
transient
µi infinite
positive
recurrent
recurrent periodic
f
ii < 1
f
ii = 1
null
recurrent aperiodic
d
(i) = 1
µi finite
Fig. 3.11 Scheme of the state classification in Markov chain.
3.3.7 Mean recurrence time and stationary distribution
We are interested in estimating the mean recurrence time µi of the state i defined in
eqn (3.44). To do so we have to count the number of steps to return to i after it was
left. Consider the following example. Let i be the starting state, for short we write T
k
instead of T
k
i
to denote the time of the k-th visit to i. Suppose the random variable
T
1 assumes, for instance, the value τ
1 = 5. This means that the process starts from i
and after five steps comes back to i for the first time. The process goes on as follows:Predicting the future 57
The proess starts from i, then:
It comes back to i after 5 steps, then: τ
1 = 5 first return
It comes back to i after 2 steps, then: τ
2 = 2 second return
It comes back to i after 3 steps, then: τ
3 = 3 third return
It comes back to i after 2 steps, then: τ
4 = 2 fourth return
The time spent by the process until the fourth return to i is 5 + 2 + 3 + 2 = 12 steps.
The mean is (5 + 2 + 3 + 2)/4 = 3, therefore there are on average 3 steps between one
visit and the next, which is an estimate of the mean recurrence time of the state i.
In general, the time spent by the process until the k-th return to i is given by the
sum of the random variables T
i
, all with the same distribution:
T
1 + T
2 + · · · + T
j + · · · + T
k
where T
j
is the time between the (j − 1)-th and the j-th return. In the example, T
3
has the value 3.
The weak law of large numbers states that the average of the sequence converges
in probability to the ensemble average of the variables T
i
, ∀i ∈ S:
1
k
(T
1 + T
2 + · · · + T
k
)
p
−−−−→
k→∞
E [Ti

X0 = i] ≡ Ei
[Ti
]
(omitting the subscript i) ≡ E [T]
This means that there are about k visits in k E [T] steps (with k large, of course).
We said that πi
is the mean time spent in i, if the state i is visited nπi times in n
steps. Then we can write (omitting the subscript i):
k visits in k E [T] steps, and also:
nπ visits in n steps
if n = k E [T], it follows:
k E [T] π visits in k E [T] steps
From which:
π =
1
E [T]
Now E [T] is the average recurrence time, usually written for the state i as µi
, then:
µi =
1
πi
Remembering the example relative to the weather of the Land of Oz, notably eqn (3.23),
the recurrence time µ0, average number of days between two rainy days, is 1/0.57 =
1.75, while µ1, average number of days between two sunny days, is 1/0.43 = 2.33.58 Introduction to Stochastic Processes
Turn to the example involving three states: rain, sun and snow, whose transition
matrix is eqn (3.26). The stationary distribution is given by the system:
π0 = 0.6π0 + 0.3π1 + 0.5π2
π1 = 0.1π0 + 0.5π1 + 0.4π2
π1 = 0.5π0 + 0.4π1 + 0.1π2
π0 + π1 + π2 = 1
The solution is the stationary distribution π = (0.4933, 0.2800, 0.2267). The recurrence
time of a rainy day (between two rainy days, there can be sunny or snowy days) is
1/0.4933 = 2.03.
We could be interested in the average number vij of visits to the state i between
two visits to the states j. This is given by:
vij =
πi
πj
=
µj
µi
In the example above, the average number of rainy days between two snowy days is
given by:
v02 =
π0
π2
=
µ2
µ0
=
0.4933
0.2267
= 2.18
With a small code Code_3_3.R, we can estimate via Monte Carlo the recurrence
time in Markov chain. The example is the weather in the Land of Oz, and part of the
code is the same as in Code_3_2.R, repeated here for convenience. The new instruction
is the diff function which computes the difference between pairs of successive elements
of a vector.
## Code_3_3.R
# mean recurrence time
# stationary distribution = (0.4933,0.2800,0.2267)
# Weather in the Land of Oz
# States: rain (0), sun (1), snow (2)
markov<- function(x0,n,x,P) { # starting function
s<- numeric()
s[1]<- x0 # initial state
raw<- which(x==x0)
for(i in 2:n) {
s[i]<- sample(x,1,P[raw,],replace=T)
raw<- which(x==s[i])
}
return(s)
} # ending function
set.seed(5) # reset random numbers
n<- 200 # number of steps
# transition matrix:
P<-matrix(c(0.6,0.1,0.3,0.3,0.5,0.2,0.5,0.4,0.1),3,3,byrow=T)
x<-c(1,2,3)
x0<- 1
s<-markov(x0,n,x,P)
t<-c(0:(n-1))
r<-which(s==1)
d<-diff(r) # difference between successive values
# for instance: r = 3,7,12,13, diff(r) = 4,5,1Predicting the future 59
m<-mean(d)
m # theory 1/0.4933 = 2.03
r<-which(s==2)
d<-diff(r)
m<-mean(d)
m # theory 1/0.2800 = 3.57
r<-which(s==3)
d<-diff(r)
m<-mean(d)
m # theory 1/0.2267 = 4.41
With the stationary distribution π = (0.4933, 0.2800, 0.2267), the recurrence times
of rainy, sunny and snowy days are 2.03, 3.57 and 4.41, respectively. The results of
the code (number of steps = 200) are 1.895, 3.957 and 4.355, respectively, in good
agreement, as expected.
3.3.8 Sojourn time
Today is raining, so how many days (excluding today) will be rain days? To answer
this question, let us introduce a new random variable Si
, called sojourn time in the
state i.
Suppose that at initial time t = 0, the process is in the state i. It may remain in i
with probability pii for a further step (t = 1), or may leave i with probability (1−pii).
If it remained in i, suppose it leaves i at the next step (t = 2), that is the events are:
{the process stays in i still one step (t = 1)}
{the process leaves i at the next step (t = 2)}
Thanks to the strong Markov property, the two events are independent, therefore:
P

X2 = j

X1 = i, X0 = i
	
= pii (1 − pii), (j 6= i)
is the probability that the process at t = 2 is in the state j, given at t = 0 was in i,
and at t = 1 was still in i. Instead, if at t = 1, it still stays in i, the probability that
it leaves at t = 3 is given by:
P

X3 = j

X2 = i, X1 = i, X0 = i
	
= pii pii (1 − pii) (j 6= i)
We introduce the random variable Si as the time during which the system stays in
i, once inside. For instance, the process enters i at step 4, and stays there at steps
5, 6, 7, 8, 9, and at the step 10 is no longer in i, but it is in j, which means that the
process remained in i for five steps. So in general:
P {Si = k} = p
k−1
ii (1 − pii), k = 1, 2, . . . and ∀i ∈ S
where Si
is the random variable sojourn time, with geometric distribution, and repre￾sents the probability that the process stays in i for further k − 1 steps, once entered
there.60 Introduction to Stochastic Processes
To estimate the average number of steps spent in i, once the process is entered and
before leaving it, we have to compute the mean value of the variable Si
, given by:
E [Si
] = X∞
k=1
kpk−1
ii (1 − pii) = 1
1 − pii
, ∀i ∈ S
Therefore, the answer to the initial question is: if today it is raining, we have to wait
on average for
E [Si
] = 1
1 − π0
=
1
π1
=
1
0.429
= 2.33
days, before the weather changes.
3.3.9 Summing up
There are chains with a finite number of states (finite-chains), and chains with a
infinite, but numerable, number of states (infinite-chains)
chains 
finite
infinite
Finite-chains can be irreducible or not irreducible (namely reducible).
If a chain is irreducible, all states are recurrent, i.e. to say irreducible or recurrent is
the same thing. If it is reducible there can be recurrent and transient states, but they
cannot to be all transient.
finite-chains



irreducible: all recurrent states ⇒ irreducible ≡ recurrent
reducible (
recurrent states
transient states, but not all
A chain can possess the following combinations of attributes:
• An irreducible and aperiodic finite-chain (≡ all states recurrent) is called ergodic.
• An irreducible, aperiodic and positive recurrent infinite-chain is called ergodic.
• An irreducible, aperiodic and positive recurrent infinite-chain is not ergodic if
recurrent states are null recurrent.
• An irreducible and aperiodic infinite-chain is not ergodic if states are transient.
3.4 Continuous-time Markov chain
In a continuous-time stochastic process, events can occur at any moment t of a continu￾ous set of values, not at fixed times as in the time-discrete case. In the Poisson process,
described in the next chapter, we will see that events can happen at any point of the
time axis, unlike what happens, for example, in a Bernoulli process where events can
only occur at certain times. Of course, also for continuous-time processes there exists
a timescale, but the time units, that is the time interval in which the system changes
state (or stays in the same state), is not ‘implicit’, as in discrete-time processes. The
concept of ‘step’ has no more reason to exist, just as it makes no sense to speak ofContinuous-time Markov chain 61
‘one-step transition’, or of ‘n steps’. If time is continuous, the ‘step’ can be as small
as we want. In any case, the transition probability in ‘time period t’ remains an es￾sential concept. A further important difference is that continuous-time Markov chains
are aperiodic by definition. We will see, indeed, that the random variable representing
state changes has an exponential distribution, so states cannot follow one another at
regular intervals.
3.4.1 Matrix transition probability function
Let us study continuous-time Markov chains in discrete state space, that is t ∈ T ⊆
R[0, ∞), but the values of random variables Xt are discrete. For discrete-time chains,
we wrote the Markovian property:
P

Xn+1 = j

Xn = i, Xn−1 = in−1, . . . , X0 = i0
	
= P

Xn+1 = j

Xn = i
	
Now we write:
P

X(s + t) = j

X(s) = i, X(u) = x(u), 0 6 u < s	
= P

X(s + t) = j

X(s) = i
	 (3.46)
where x(u), 0 6 u < s are the states visited in the past, before time s. The chain is
time homogeneous if the probability:
P

X(s + t) = j

X(s) = i
	
∀s 6 t, ∀t > 0, ∀i, j ∈ S
depends only on the interval amplitude [t−s], not on the particular values of extremes
at s and t. In other words, whatever moment the system enters the state i, its proba￾bilistic time evolution is the same, as the system should start (t = 0) from the state i.
In the following we will deal with homogeneous and regular chains. A continuous-time
Markov chain is called regular if the number of transitions in a finite time interval
is finite. Define the transition probability function, or simply the transition function,
from state i to state j in the time interval [s, t] (not in n steps as in the discrete case):
pij (s, t) = P

X(s + t) = j

X(s) = i
	
, s 6 t
and for homogeneous chains:
pij (s, s + t) = pij (0, t) ≡ pij (t)
Notice that for each state i and j, the transition function pij (s, t) is a continuous func￾tion of t. Also for continuous-time Markov chains, we introduce a matrix P(t), called
the matrix transition probability function, or simply the matrix transition function,
which can be regarded as the continuous time counterpart of the transition matrix in
n steps at discrete time. With finite number N of states, it is given by:
P(t) =


p11(t) p12(t) . . . p1N (t)
p21(t) p22(t) . . . p2N (t)
. . . . . . . . . . . . . . . . . . . . . . . .
pN1(t) pN2(t) . . . pNN (t)


(3.47)
where pij (t) = P

X(t) = j

X(0) = i
	
, transition probability from state i to state j
in time t. ‘In time t’ does not mean, of course, in t steps, but it means in a time62 Introduction to Stochastic Processes
interval ∆t = [0, t], with amplitude t. Transition probabilities pij (t) exhibit the same
properties of pij in the discrete case, that is ∀t:
pij (t) ∈ [0, 1], ∀i, j ∈ S and X
j∈S
pij (t) = 1, ∀i ∈ S (3.48)
As in the discrete case, P
j∈S means summation on all the states belonging to the
space S, both with finite number N, and in the case of numerable infinity. The matrix
P(t), for eqns (3.48), is therefore a stochastic matrix. Similarly to the discrete case,
we have:
pij (0) = δij =
(
1, if i = j
0, if i 6= j
and in matrix form P(0) = I, where I is the identity matrix.
The Chapman−Kolmogorov equation is now written:
pij (s + t) = X
k∈S
pik(s) pkj (t), 0 6 s 6 k 6 t (3.49)
and in matrix form:
P(s + t) = P(s) P(t), s, t > 0
Due to this last property, together with eqns (3.48) and because P(0) = I, the family
{P(t)} is defined as a stochastic semigroup.
The process is completely described by the matrix transition function P(t), and as
in the discrete case, by the initial distribution over the N states, that is by the probabil￾ity vector at time 0: v(0) = ￾
v1(0), v2(0), . . . , vN (0)
, with vi(0) = P {X(0) = i} , i =
1, 2, . . . , N.
Supposing that pij (t) are derivable functions ∀t > 0, define (∀i, j ∈ S, i 6= j) and
∆t > 0:
qij =
pij (t)
t




t=0
= + p
0
ij (0) = lim
∆t→0
pij (∆t)
∆t
(3.50)
and for i = j:
qii =
pij (t)
t




t=0
= −p
0
ii(0) = − lim
∆t→0
1 − pii(∆t)
∆t
(3.51)
Note that it is qii < 0.
Consider eqn (3.50). The quantity qij is defined as the prime derivative with respect
to time of the transition function pij (t) at t = 0. Therefore, qij is not a probability; at
the most we might say that it is a ‘probability’ less than infinitesimal of order higher
than the first with respect to ∆t, a ‘probability per time unit’. In fact, qij represents
the ‘instantaneous transition speed’ of the system of moving from state i to state j;
qij is called also the transition intensity
Let us think of a procedure of this type, to understand the reason that ∆t must
tend to 0. We begin the experiment by resetting the clock, namely t = 0. At t = 0,
the system is in state i. We wait for a time ∆t. If at this time, the system is in state
j, we say we have achieved success. Of course, the system might not reach state j, iContinuous-time Markov chain 63
which case there is no success. If the system went to j, we would bring back it again
in i, and would repeat the experiment. Let us give some numbers. We take a time
period of 1 minute, and execute six experiments, that is one every 10 s, which means
∆t = 10 s. We say that the average number of successes in 1 minute is given by the
executed number of experiment in 1 minute, times the success probability pij (∆t) in
each experiment. Then, if pij (∆t) = 0.5, the average number of successes in 1 minute
is 6 × 0.5 = 3. Quite correct! But if ∆t is finite, we cannot affirm with certainty that
the system moved directly from i to j. In 10 s, it could have arrived at j, starting
from i and visiting states k1, k2, . . . . However, if ∆t → 0, practically if ∆t is small
enough for the realization of a unique experiment (in the example, one second), the
success guarantees exactly a direct transition i → j, therefore qij can be legitimately
regarded as the average number of transitions i → j per time unit, namely as transition
frequency, and this is just its physical meaning. Turning to eqn (3.51), the quantity
qii, always 6 0, represents the frequency with which the system in state i, leaving it
to reach another state.
Suppose that the system at time t is in state i. In the interval ∆t, the system leaves
i, namely it is no more in i at the instant t+∆t. If accessible states are h, k1, k2, . . . , kh
(see Fig. 3.12), the frequency qii has to be distributed over such states, that is, the
following relation holds:
qii = −
X
k6=i
qik, (3.52)
where k stays for states k1, k2, . . . , kh.
i
kh
k
j
k2
k1
Fig. 3.12 Transition from state i to another possible state.
For instance, suppose there are four states and that q33 = −2, that is the system
left two times (in suitable time units) the state 3. A possible scenario is that the system
once went from 3 to 2 and once from 3 to 4. Since it is q31 = 0, q32 = 1 and q34 = 1,
it results in −2 = −(0 + 1 + 1).
To derive the relation eqn (3.52), the starting point is the certain event:64 Introduction to Stochastic Processes
1 = X
k∈S
pik(∆t), ∀i ∈ S
and separating the term pii(∆t) from the others with k 6= i, it is:
1 = X
k∈S
pik(∆t) = pii(∆t) + X
k6=i∈S
pik(∆t) (3.53)
From eqn (3.51), it is: −qii(∆t) = 1 − pii(∆t), so that, for the first term of the right￾hand side, it is pii(∆t) = 1 + qii(∆t). Using eqn (3.50), qik(∆t) = pik(∆t) is replaced
in the summation in eqn (3.53), giving:
1 = X
k∈S
pik(∆t) = 1 + qii(∆t) + X
k6=i∈S
qik(∆t)
from which eqn (3.52). It may also be qii = 0, meaning that i is an absorbing state
(in discrete-time case it is pii = 1).
The quantities qij are important, since they represent observed data. We will see
now how, by knowing qij and the initial distribution p(0), the matrix transition func￾tion P(t) and consequently the probability distribution over states as a time function
can be obtained.
3.4.2 Transition intensity matrix
Turning to the Chapman−Kolmogorov equation, eqn (3.49) is written a bit differently,
substituting (t + ∆t) for (s + t). Therefore eqn (3.49) becomes:
pij (t + ∆t) = X
k∈S
pik(t) pkj (∆t)
Summing only the terms k 6= j we obtain:
pij (t + ∆t) = X
k6=j∈S
pik(t) pkj (∆t) + pij (t) pjj (∆t)
Subtracting pij (t) from both members:
pij (t + ∆t) − pij (t) = pij (t) pjj (∆t) − pij (t) + X
k6=j∈S
pik(t) pkj (∆t)
= pij (t)[pjj (∆t) − 1] + X
k6=j∈S
pik(t) pkj (∆t)
Dividing both members by ∆t:
pij (t + ∆t) − pij (t)
∆t
= pij (t)
pjj (∆t) − 1
∆t
+
X
k6=j∈S
pik(t)
pkj (∆t)
∆t
If ∆t tends to 0, the ratio at the first member is the time derivative p
0
ij (t), and by
eqn (3.51), the first term of the second member is equal to qjjpij (t). So we can write:Continuous-time Markov chain 65
p
0
ij (t) = qjjpij (t) + X
k6=j∈S
pik(t)qkj (3.54)
Assigning the initial conditions pij (0), the system eqn (3.54) can be resolved, then
functions pij (t) are determined. In matrix notation, it can be written:
Q =


q11 q12 . . . q1N
q21 q22 . . . p2N
. . . . . . . . . . . . . . . .
qN1 qN2 . . . qNN


or defining qii = −qi
=


−q1 q12 . . . q1N
q21 −q2 . . . p2N
. . . . . . . . . . . . . . . . .
qN1 qN2 . . . −qN


(3.55)
The matrix Q is called the intensity matrix or infinitesimal generator matrix or rate
matrix. Notice that:
a) non-diagonal elements are non-negative: qij > 0, ∀i 6= j.
b) diagonal elements are non-positive: 0 6 −qii < ∞, ∀i. Note: (−qii) > 0.
c) the sum of each row is = 0: P
j∈S qij = 0, ∀i.
For example, matrix Q can be as (S = {1, 2, 3, 4}):
Q =


−1 1 0 0
1 −3 1 1
0 1 −2 1
0 1 1 −2


where: −1 is q11 or −q1, −3 is q22 or −q2, and so on. As we said, qij are not probabilities,
so they can assume values greater than one. Matrix entries are to be read adding the
time unit, for instance: “the system left three times the state 2 in 1 hour, (or in one
minute, one second, etc.) and visited once the state 1, once the state 3, and once the
state 4”.
The system eqn (3.54) can be written in terms of transition functions:
P
0
(t) = d P(t)
dt = P(t) Q (forward equation)
or:
P
0
(t) = d P(t)
dt = Q P(t) (backward equation)66 Introduction to Stochastic Processes
with initial conditions: P(0) = I. The unique solution is given by:
P(t) = e
Qt
(3.56)
where the exponential can be written as:
e
Qt =
X∞
j=0
Qj
t
j
j!
= I + Qt +
Q2
t
2
2
+
Q3
t
3
3
+ · · ·
and eqn (3.56) can be also written:
P(t) = I +
X∞
j=1
Qj
t
j
j!
(3.57)
We said that if time is discrete, the probability vector (or state vector) at the n-th
step is (N number of states) (see eqn (3.25)):
v
(n) = (v
(n)
1
, v
(n)
2
, . . . , v
(n)
N ),
with ∀i ∈ S, ∀n ∈ T, v
(n)
i > 0 and X
i∈S
v
(n)
i = 1
In this case, v
(n)
i = P {Xn = i} is the probability that the system is in state i after n
steps.
With continuous time, we have the probability vector:
v(t) = ￾
v0(t), v1(t), . . . , vN (t)

,
with ∀i ∈ S, vi(t) > 0 and X
i∈S
vi(t) = 1
Now vi(t) = P {X(t) = i} is the probability that the system is in state i at time t.
In general, v(t) says that the system at time t is in state 0 with probability v0(t), in
state 1 with probability v1(t), etc.
Let v(0) be the probability vector at time 0. It is:
v(t) = v(0) P(t)
by deriving both members with respect to time:
v
0
(t) = v(0) P
0
(t) = v(0) P(t) Q
from which the equation describing the variation over time of the states probability,
given the initial conditions v(0), is:
v
0
(t) = v(t) Q (3.58)
or alsoContinuous-time Markov chain 67
dvi(t)
dt =
X
j∈S
qjivj (t), ∀i ∈ S (3.59)
and in explicit form:
v
0
0
(t) = q00v0(t) + q10v1(t) + · · · + qN0vN (t)
v
0
1
(t) = q01v0(t) + q11v1(t) + · · · + qN1vN (t)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
v
0
N (t) = q0N v1(t) + q1N v1(t) + · · · + qNN vN (t)
(3.60)
and also for eqn (3.56):
v(t) = v(0) e
Qt
3.4.3 Embedded matrix
We have seen that a continuous-time Markov process is defined if, given the in￾tensity matrix Q (whose entries are observable quantities) and the initial conditions
v(0), we are able to determine the matrix transition function P(t). Either we resolve
the differential equation system eqn (3.58), or we diagonalize Q and compute P(t).
We will now describe a further way to describe the process, by means of the random
variable Si sojourn time. The sojourn time was defined in Section 3.3.8 for discrete￾time Markov chains. The sojourn time in state i is the time spent by the system in
state i before leaving it, or in other words the time instant at which the system in i
changes states. It is defined as the distribution of the sojourn time in i since it is the
same every time the system enters state i (recall the assumption of homogeneity of
the chain). Supposing that the system is in state i at time 0, the the sojourn time in
i is given by:
Si = min{t > 0, X(t) 6= i

X(0) = i} (3.61)
Therefore the value j, given by X(Si) = j, denotes the state visited the first time by
the process when it leaves the initial state i.
A simple example can illustrate the meaning of Si
, in practice. Consider a process
with only two states 0 and 1, and suppose that only one transition from 0 to 1 occurs.
Let λ be the transition rate (or transition intensity) q01. From the above assumptions,
it follows: q00 = −λ and q10 = q11 = 0. The intensity matrix results:
Q =

−λ λ
0 0 
= −λ

1 −1
0 0 
= −λ Q
Since Qk = (−λ)
k Q, k = 1, 2, . . . , from eqn (3.57) it is:
P(t) = I +
X
k>1
Qk
t
k
k!
= I + Q
X
k>1
(−λ)
k
t
k
k!
The series above is equal to:
X
k>1
(−λ)
k
t
k
k!
= e
−λt − 168 Introduction to Stochastic Processes
Therefore:
P(t) = I + Q
￾
e
−λt − 1

More explicitly:
P(t) = 
1 0
0 1
+

1 −1
0 0 
￾
e
−λt − 1

that is:
P(t) = 
1 0
0 1
+

e
−λt − 1 −e
−λt + 1
0 0 
=

e
−λt 1 − e
−λt
0 1 
(3.62)
Therefore, the transition probability function p00(t), namely the probability that the
system still stays in state 0 at time t, has an exponential form:
P

X(t) = 0

X(0) = 0	
= p00(t) = e
−λt = e
q00t = e
−q0t
At time t, the process goes into state 1 with transition probability function p01(t):
P

X(t) = 1

X(0) = 0	
= p01(t) = 1 − e
−λt = 1 − e
−q0t
which can be regarded as the distribution of the random variable S0, “sojourn time in
state 0”, that is P {S0 6 t}. Take, for instance, λ = 3. As saying:
q01 = 3, q00 = −3, −q0 = −3
Then:
p01(t) = 1 − e
−3t
p00(t) = e
−3t
At time t = 0, we know P {X(0) = 0} = 1, namely: v0(0) = 1 and v1(0) = 0. At time
t = 1, e.g. after one second, we have:
t = 1 : (
p01(t) = 0.9502
p00(t) = 0.0498
We see that the probability P

X(t) = 1

X(0) = 0	
that the system at time t = 1
enters the state 1 is given by p01(t) = 0.9502. Similarly, 0.0498 is the probability that
the system still stays in state 0 after one second. After a further second:
t = 2 : (
p01(t) = 0.9975
p00(t) = 0.0025
the probability to remain in state 0 diminishes, and as a consequence the probability
of leaving it increases.
The reasoning holds for any time t. Therefore, if the system is in i at time t = 0 it
stays there for a time given by the exponential random variable with intensity qi
:
P {Si > t} = e
−qit
, t > 0 (3.63)
The system enters state j with probability given by:
P {X(Si) = j} =
qij
qi
as we see lateContinuous-time Markov chain 69
The same results can also be obtained without resorting to the random variable
Si
. Consider a generic state i, to which there are no transitions, that is transitions
towards i are excluded. Equation (3.60) reduces to:
vi(t)
t
= −qivi(t), con vi(0) = 1
The solution is:
vi(t) = 1 − e
−qit
(3.64)
which says that the probability that the system at time t is in state i decreases expo￾nentially with time, the frequency being equal to the number of times that the system,
in time units, left state i.
This process is memoryless. We have already seen what this means: the transition
probability at a time after a certain instant s is the same regardless of the time spent
before s, that is of the time considered as an initial instant, which may be 0, or any t.
Suppose that the system has been in state i for 60 minutes. We want the probability
that it will stay there for another 30 minutes, that is P

Si < 90

Si > 60	
. If the
process is memoryless, the system behaves as if it had left i, that is:
P

Si 6 90

Si > 60	
= P

Si 6 (60 + 30)

Si > 60	
= P {Si 6 30}
We have seen that the process stays in state i for some time, described by an
exponential random variable, before changing state. Figure 3.12 suggests that the
system will visit states k1, k2, . . . , kh with probability proportional to the transition
probabilities qik. By normalizing with respect to the transition intensity qi = −qii with
which the system leaves i, we can define a new type of transition probability ˜pij :
p˜ij = X
qij
k6=i
qik
=
qij
qi
, i 6= j
Such probabilities are not a transition function, and they do not depend on time.
Of course, ˜pii = 0, since the process cannot stay in i when it leaves it. That is also
justified by an empirical point of view, because what is observable is a state change.
It must be P
j6=i
p˜ij = 1, since the system must reach some state in any case. For the
sake of completeness, we have to take into account that if i is absorbing, then qi = 0.
In this case ˜pii = 1 and ˜pij = 0, for i 6= j.
In conclusion, we know when the system moves and where it goes, that is we know
the process. Then, if the process at time 0 is in state i0, it will stay in such a state for
a time described by an exponential distribution with intensity qi0
, after it jumps in
state i1 with probability ˜pi0i1
, and it stays there for a time qi1
, after which it jumps
to state i2 with probability ˜pi1i2
, and so on.
Turning to eqn (3.63), representing the distribution of the sojourn time Si
, it gives
the time spent in i by the system, but it does not give any information about where
it goes after leaving i. To know the probability that the sojourn time in i ends with a
transition to state j, we have to know the transition function pij (t), given by:
pij (t) = qij
qi
e
−qit = ˜pij e
−qit
, i 6= j70 Introduction to Stochastic Processes
The quantities ˜pij form a stochastic matrix Pe called the embedded transition ma￾trix. This matrix completely describes the behaviour of a discrete-time Markov chain.
To every continuous-time Markov chain {X(t)} it is possible to associate a discrete￾time embedded Markov Chain {Xe(t)}. The embedded chain is also called the ‘skeleton’
of the continuous-time chain. The matrix Pe only accounts for transition probabilities
from state to state, but not for transition rates, sojourn times, etc. We can form a
continuous-time chain from a discrete-time embedded chain, but we need qi
in addi￾tion to ˜pij .
Example 3.4 Let us write an intensity matrix Q, and the corresponding embedded
matrix Pe, with S = {1, 2, 3}.
Q =


−1 1 0
1 −2 1
0 1 −1

 Pe =


0 1 0
0.5 0 0.5
0 1 0


For example:
p˜12 =
q12
q12 + q13
=
1
1 + 0
, . . . p˜32 =
q32
q31 + q32
=
1
0 + 1
Figure 3.13 provides a visual synthesis of the meaning of the embedded chain of
a Markov process. Let {X(t)} be a continuous-time Markov chain and let {Xe(t)} be
the corresponding embedded chain. The process at time t = 0 is in state 0 = 1, where
it stays for a time [0, t1), determined by the exponential distribution with intensity
q0. At t1, with probability ˜p01 = q01/q0, it jumps into state 1 = 5, and so on. The
realization of Xe(0) is denoted as ˜x0 = 1.
3.4.4 Poisson retrouv´e
The Poisson process is described in the next chapter, here we describe the Poisson
process from the point of view of a Markov process. Let’s go back to reconsider the
process involving two states 0 and 1, and a unique transition from 0 to 1 with fre￾quency λ = q01, q00 = −λ and q10 = q11 = 0. Consider now a countable infinity of
states: 0, 1, 2, . . . , as sketched in Fig. 3.14. In this case, the transition function matrix
eqn (3.47) has infinite rows and infinite columns. At the beginning, the process is in
state 0, and at a certain instant it jumps to state 1, leaves 1 and enters 2, forgetting
what it did before, and so on. Let us suppose that all the random variables Si
, sojourn
time in states i, i = 0, 1, . . . , have the same exponential distribution with intensity λ.
Therefore:
qij =
(
λ, if j = i + 1
0, if j 6= i + 1, i = 0, 1, . . .
with qii = −λ and initial condition P {X0 = 0} = 1. The intensity matrix is:Continuous-time Markov chain 71
5
4
3
2
1
0
t1
exp(- q0 t)
q01 / q
X(t)
0
q12 / q1
exp(- q1
 t)
˜x1
˜x2
˜x3 ˜x0 exp(- q2 t)
time
t2
t3
Fig. 3.13 Scheme of realizations of embedded chain {Xe(t)} of a continuous-time Markov
chain {X(t)}.
0
λ λ λ λ
1 2 i - 1 i i + 1
Fig. 3.14 Sketch of the Poisson process with a countable infinity of states, and with a
transition frequency from state to state equal to λ.
Q =


−λ
q00
λ
q01
0
q02
0
q03
. . .
0
q10
−λ
q11
λ
q12
0
q13
. . .
0
q20
0
q21
−λ
q22
λ
q23
. . .
. . . . . . . . . . . . . . . . . . . . .


(3.65)
The corresponding embedded matrix (‘skeleton’) is:
Pe =


0 1 0 0 . . .
0 0 1 0 . . .
0 0 0 1 . . .
. . . . . . . . . . . . . . . . .


Let us turn to eqn (3.58), namely:
v
0
(t) = v(t) Q
With the aim to determine how the probability vector v changes with time, consider
that we now have a countable infinity of states, so we write:72 Introduction to Stochastic Processes
v(t) = ￾
v0(t), v1(t), . . . , vj (t), . . . 
recalling that vi(t) = P {X(t) = i} is the probability that the process is in state i at
time t.
The above equation is written in explicit form (see also eqn (3.60)):
v
0
0(t) = q00v0(t) + q10v1(t) + q20v2(t) + · · · + qj0vj (t) + · · ·
v
0
1(t) = q01v0(t) + q11v1(t) + q21v2(t) + · · · + qj1vj (t) + · · ·
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
v
0
i (t) = q0iv0(t) + q1iv1(t) + q2iv2(t) + · · · + qjivj (t) + · · ·
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
(3.66)
Equation (3.66) greatly simplifies considering that the only non-zero entries qij are
those with j = i + 1, equal to λ, and the entries qii are equal to −λ. For instance, for
v
0
0
(t), v 0
1
(t) and v
0
2
(t), it is:
v
0
0
(t) = (−λ)
q00
v0(t) ( q10, q20, etc. are all = 0)
v
0
1
(t) = λ
q01
v0(t) + (−λ)
q11
v1(t) (q21, etc. are all = 0)
v
0
2
(t) = ✘q ✘✘✘ 02 v0(t) + λ
q12
v1(t) + (−λ)
q22
v2(t) + ✘q ✘✘✘ 32 v3(t), etc.
= λ v1(t) − λ v2(t)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
In general:
v
0
j
(t) = λ vj−1(t) − λ vj (t) (3.67)
The integral of v
0
0
(t) = −λv0(t) is:
v0(t) = e
−λt
Equation (3.67) is integrated by multiplying both members by exp(λt), obtaining:
e
λt v
0
j
(t) = e
λt λ vj−1(t) − λ vj (t)
or also:
d
dt

e
λt vj (t)

= e
λt vj−1(t)
By integrating both members:
e
λt vj (t) = Z t
0
λ eλy vj−1(y) dy
from which:
vj (t) = e
λt λ
Z t
0
e
λy vj−1(y) dContinuous-time Markov chain 73
Let us compute v1(t). In the place of v0, that is vj−1, we take its value v0(t) = exp(−λt):
v1(t) = λ eλt Z t
0
e
λy e
−λy
v0(y)
dy = λ eλt Z t
0
dy = λ t e−λt
By solving recursively the remaining equations, that is: v2(t) = . . . , v3(t) = . . . , etc.,
the general solution is obtained:
vi(t) = P {X(t) = i} =
(λt)
i
i!
e
−λt, t > 0, i ∈ S = {0, 1, 2 . . . }
which is just the Poisson distribution with parameter λ. From the point of view of
Markov processes, the Poisson process can be defined as a continuous-time Markov
process, in discrete state space, S = {0, 1, 2 . . . }. Every transition of the system cor￾responds to an arrival at time t. We know that the system stays in a state during a
time given by the exponential random variable, therefore we find again that in the
Poisson process, waiting times between arrivals are described by independent random
variables, all with the same exponential distribution.
3.4.5 Birth-death process
We can look at Fig. 3.14 as a depiction of a pure birth process. Let us imagine observing
an animal population which reproduces according to the following rule. When a new
individual comes to light, we say that a transition occurs and the state of the system
(number of animals) is increased by one. In theory, all animal species are good, except
rabbits, since we know that from early 1200 that Leonardo da Pisa, who in the 18th
century was called Fibonacci, precisely modelled the growth of a rabbit population by
a nonlinear time series, later known as the Fibonacci’s.
We wish now to make the model more ‘realistic’, so first the intensities λ can change
from state to state, so they become λi
. Moreover, we admit also transitions from i to
i − 1, not only from i to i + 1, that is transitions can only be to a neighbouring state.
Intensities given by sojourn times from i to i − 1 are denoted as µi
. In this model,
transitions from state i to state i + 1 are called as births, and those from i to i − 1 as
deaths.
The matrix eqn (3.65) becomes:


0 1 2 3 ... i−1 i i+1 ...
0 −λ0 λ0
1 µ1 −(λ1 + µ1) λ1
2 0 µ2 −(λ2 + µ2) λ2
... . . . . . . . . . . . . . . . . . . . . . . . . . . .
i−1
i µi −(λi + µi) λi
i+1
... . . . . . . . . . . . . . . . . . . . . . . . . . . .


(3.68)
where λi > 0, while µi > 0, with µ0 = 0 and qi = −qii = λi + µi
.
Figure 3.14 is generalized as Fig. 3.15.74 Introduction to Stochastic Processes
0
λ0 λ1 λ2 λi-1 λi+1 λi
µ1 µ2 µi-1 µi µi+1
1 2 i - 1 i i + 1
Fig. 3.15 Sketch of the birth-death process with a countable infinity of states, and with a
transition frequency from state to state equal to λi (birth) or µi (death).
To summarize:
qij =



λi
, if j = i + 1 (rate of birth)
µi
, if j = i − 1 (rate of death)
0, if j 6= i + 1, i − 1, i = 0, 1, . . .
qi = −qii = λi + µi
, ∀i
Stationary distribution does not depend on time, as if to say that at stationarity the
probability vector v(t) does not depend on time, as if to say that its derivative with
respect to time must be zero:
limt→∞
dvi(t)
dt = 0, i = 0, 1, . . .
The system of equations eqns (3.60) becomes in the present case:
v
0
0
(t) = q00v0(t) + q10v1(t) + · · · + qN0vN (t)
v
0
1
(t) = q01v0(t) + q11v1(t) + · · · + qN1vN (t)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
v
0
N (t) = q0N v1(t) + q1N v1(t) + · · · + qNN vN (t)
In the present case:
v
0
0
(t) = −λ0v0(t) + µ1v1(t)
and in general:
v
0
k
(t) = −λk−1vk−1(t) + µk+1vk+1(t) − (λk + µk)vk(t)
The solution is obtained by knowing the initial probability distribution v
(0). Another
way to derive the stationary distribution is through the condition detailed balance for
continuous-time processes. The global detailed balance is written as:
πi
X
j6=i
qij =
X
j6=i
πj qij
where the left-hand side represents the total flow outgoing from state i into states
j 6= i, while the right-hand side represents the total flow entering state i outgoing
from all states j 6= i.Continuous-time Markov chain 75
The local detailed balance holds if the probability fluxes from i to j and vice versa
are equal:
πiqij = πj qji, ∀i, j
In discrete time, the detailed balance is (π equilibrium distribution):
πkpij = πjpji, ∀i, j
In our case, at stationary the probability vector v(t) = ￾
v0(t), v1(t), . . . , vj (t), . . . 
,
can be derived by writing the balance equations:
µ1v1 = λ0v0
λ0v0 + µ2v2 = (λ1 + µ1)v1
λ1v1 + µ3v3 = (λ2 + µ2)v2
. . . . . . . . . . . . . . . . . . . . . . . . . .
from which:
v1 = v0
λ0
µ1
v2 = v1
λ1
µ2
= v0
λ0λ1
µ1µ2
v3 = v2
λ2
µ3
= v0
λ0λ1λ2
µ1µ2µ3
. . . . . . . . . . . . . . . . . . . . . . .
In general:
vn = v0

λ0λ1 . . . λn−1
µ1µ2 . . . µn

, n > 1
From the normalization condition P∞
i=0 vi = 1, the existence of stationary probability
implies for all n > 0 the following ergodicity condition:
X∞
n=1

λ0λ1 . . . λn−1
µ1µ2 . . . µn

< ∞
Then all vn can be determined:
vn =
nY−1
i=0

λi
µi+1 
1 + X∞
n=1
nY−1
i=0

λi
µi+1 
, n > 1 (3.69)
where v0 is:
v0 =
 
1 + X∞
n=1
nY−1
i=0
λi
µi+1 !−1
(3.7076 Introduction to Stochastic Processes
or more explicitly:
v0 =
 
1 + X∞
n=1
λ0λ1 . . . λn−1
µ1µ2 . . . µn
!−1
The following code can be seen somewhat as a generalization of the Code_3_2 in
which exponential generation of arrival times are simulated. Now we have to take into
account transitions from state i either to state i+1, or to state i−1. The probability of
moving to state i+ 1 is λi/(λi +µi), while that of moving to state i−1 is µi/(λi +µi).
The process spends a length of time in the new state according to an exponential
distribution with parameter (λi + µi), and the process continues with choosing the
subsequent state to be visited. The binomial random variable decides whether there
is a birth (bin==1) or a death (bin==0). The process is followed until a fixed value
Deltat, in some time units, unless the population becomes extinct before.
## Code_3_4.R
# Birth-death process simulation
set.seed(2) # reset random numbers
nhistories<- 4 # number of simulated histories
Deltat<- 3 # the process is followed no more than Deltat
lambda<- 0.5 # rate of birth
mu<- 0.9 # rate of death
n.init <- 20 # initial population size
time<-matrix(,nhistories,200)
population<-matrix(,nhistories,200)
ls<- numeric()
lp<- numeric()
### 2 loops: most external loop on histories,
# inner loop generates births/deaths
for(l in 1:nhistories) { # starting loop on histories
ev<- 0
np<- n.init
n<- np
t<- 0
while(t<Deltat & np>0) { # population may become extinct
t<- t + rexp(1,(lambda+mu)*n)
# if 1: birth, if 0: death
bin<-rbinom(1,1,(lambda/(lambda+mu)))
np<-ifelse (bin==1, np<- np+1, np<- np-1)
ev<- c(ev,t)
n <- c(n,np)
} # ending while loop
ls[l]<-length(ev)
time[l,1:ls[l]] <- ev
lp[l]<-length(n)
population[l,1:ls[l]] <-n
} # ending loop on histories
## # uncomment the following lines to print the transition instant
# and the number of individuals over time for each history
# time[1,1:ls[1]]
# time[2,1:ls[2]]
# time[3,1:ls[3]]
# time[nhistories,1:ls[nhistories]]
# population[1,1:ls[1]]
# population[2,1:ls[2]]
# population[3,1:ls[3]]
# population[nhistories,1:ls[nhistories]]
par(mfrow=c(2,2))Continuous-time Markov chain 77
plot(time[1,1:ls[1]],population[1,1:ls[1]],type="s"
,xlab="time",ylab="population")
abline(v=Deltat,lty=3)
plot(time[2,1:ls[2]],population[2,1:ls[2]],type="s"
,xlab="time",ylab="population")
abline(v=Deltat,lty=3)
plot(time[3,1:ls[3]],population[3,1:ls[3]],type="s",
xlab="time",ylab="population")
abline(v=Deltat,lty=3)
plot(time[nhistories,1:ls[nhistories]],
population[nhistories,1:ls[nhistories]],type="s",
xlab="time",ylab="population")
abline(v=Deltat,lty=3)
0.0 1.0 2.0 3.0
10 16 22
time
population
0.0 1.0 2.0
0 1 0 2 0
time
population
0.0 1.0 2.0 3.0
5 1 5
time
population
0.0 0.4 0.8 1.2
0 1 0 2 0
time
population
Fig. 3.16 Number of individuals as a function of time for four histories. Transition frequency
from state to state are equal to λi (birth) or µi (death). The dotted line is at ∆t = 3.
Figure 3.16 shows four time histories of the birth-death process. In the histories 1
and 3, the process is stopped after 71 and 90 steps, respectively, while in the histories
2 and 4, the process becomes extinct after 69 and 41 steps, respectively, as reported
below:
> population[1,1:ls[1]]
[1] 20 21 20 21 20 19 20 21 22 21 ...
[51] 14 15 16 15 14 13 12 11 10 9 10 11 12 13 12 13 14 13 12 13 14
> population[2,1:ls[2]]
[1] 20 19 18 17 16 17 16 15 14 15 ...
[51] 10 9 8 7 6 5 4 5 4 5 6 5 4 3 2 3 2 1 078 Introduction to Stochastic Processes
> population[3,1:ls[3]]
[1] 20 21 20 21 22 23 22 21 20 19 ...
[76] 11 12 11 10 11 10 9 10 9 8 7 6 5 4 5
> population[4,1:ls[4]]
[1] 20 19 20 21 20 19 18 19 18 17 ...
[26] 11 10 11 10 11 10 9 8 7 6 5 4 3 2 1 0
In any case, if the rate of death µ is greater than the rate of birth λ, the population
sooner or later becomes extinct; on the contrary, if λ > µ, the population increases
without bounds. When all individuals have disappeared, the size of the population
remains zero from there on; that is 0 is an absorbing state. By increasing the number
of histories and following each of them up to extinction, we can estimate the average
survival time of the population, about 75 steps.
The birth-death process has wide application in queueing models, in which ‘animals’
are now customers in a queue. In a simple queue, the process is in state i if there are
i costumers in the queue. The costumers are served in order of arrival, the first is
receiving the service and the others (if any) are waiting in the queue. In this model,
the arrivals (births) occur in a Poisson process with rate λ, then here λi = λ, ∀i. The
service rate (or processing time) is µ (µi = µ, ∀i), so the mean processing time is 1/µ.
Once the customer has been served, they leave the queue and the number of customers
decreases by one (death). For instance, the queue is empty, so the waiting time for
the first customer is zero. Suppose that the waiting time for the second customer is 20
minutes, and the waiting time for the third customers is 20 + 25 = 45 minutes. Then
the average waiting time is (0 + 20 + 25)/3 = 15 minutes.
We have described a type of queue named M/M/1. The two Ms stand for ‘Markov’,
since both the arrival and the service rate are exponential, therefore they have Marko￾vian property. The ‘1’ indicates that there is one server, so customers are served once
at a time. Equation (3.69) becomes with λi = λ and µi = µ:
vn =

λ
µ
n
1 + X∞
n=1

λ
µ
n =

λ
µ
n 
1 −
λ
µ

, n > 0
The quantity ρ = λ/µ is named the ‘load of the queue’, or ‘traffic intensity’, the mean
number of arrivals per unit of time. Clearly, ρ must be less than 1, if the customers
arrive with a rate λ greater than the service rate µ, the queue tends to become infinitely
long. In other words, stationary probability exists if and only if:
X∞
n=1
λn
µn
=
X∞
n=1
ρ
n < ∞
Then, eqn (3.70) becomes:
v0 =
 
1 + X∞
n=1
λn
µn
!−1
=
 X∞
n=0
ρ
n
!−1
=

1
1 − ρ
−1
= (1 − ρ)
So, if µ < ρ, the process is positive recurrent and the stationarity distribution is:Continuous-time Markov chain 79
vn =

λn
µn

v0 = (1 − ρ) ρ
n
Notice that v0 represents the probability that the queue has no customers, that is the
system is in the state 0, so the next lucky customer is promptly served.
Some formulas characterizing the queueing process are reported below. We said
that vn(t) = P {X(t) = n} is the probability that, at time t, the process is in the
state n , that is the probability that the system contains n customers. Therefore, the
average number of customers in the system N, that is the average length of the queue,
at stationarity is (Ross, 2014):
N =
X∞
n=0
nvn = (1 − ρ)
X∞
n=0
nρn =
ρ
1 − ρ
=
λ
µ − ρ
and the variance of the number of customers is:
Var [N] = X∞
n=1
n
2
vn − N
2 =
ρ
(1 − ρ)
2
The average amount of time spent by a customer in the system (note: in the system)
is:
T =
N
λ
=
1
µ − λ
=
1/µ
1 − λ/µ
The formula N = λT holds and is known as Little’s law, in words: the average number
of customers present in the queue is equal to the arrival rate of customers times the
average amount of time spent by customers in the queue.
Notice that T includes the queueing delay plus the mean service time TS = 1/µ.
So, to be precise, N refers to the number of customers either in the queue or in service.
The average amount of time that a customer spends in the queue is given by:
W = T − TS = T −
1
µ
=
1
µ − λ
−
1
µ
However, T and W are not always separately examined, but only a unique average
time spent in the system (in queue or process) is considered.
Turning to the example of customers going to a pub, modelled by the Code_3_1.R.
Suppose now that the average number of arrivals is 50 per hour. The time between
successive arrivals (inter-arrival time) is exponentially distributed with mean 1/λ:
1
λ
= (in seconds) 60 × 60
50
= 72 s = 1.2 minutes
and the average frequency is then λ = 0.0139 s−1
. The waiting time W of a costumer
in line depends on the length of the queue, that is the number of customers before
him in the line, at time t. The bartender takes 1 minute to serve a costumer, then the80 Introduction to Stochastic Processes
service rate is µ = 60 customers per hour. From Little’s Law, the average amount of
time spent by a customer in the system is
T =
1
µ − λ
=
1
60 − 50
=
1
10
hrs = 6.0 minutes
The waiting time W of a costumer in line is:
W = T −
1
µ
= 6 − 1 = 5 minutes
and there are on average:
N = λT = 0.8333 × 6.0 = 5
customers in the pub. The load of the queue is then:
ρ = λ/µ = 50/60
The paper The Theory of Probabilities and Telephone Conversations published in
1909 by A. K. Erlang is referred to as the first paper on queuing theory. Many excellent
books have been written on this subject, including (Medhi, 2003; Ross, 2014).
3.4.6 Probability and determinism: the Buffon’s needle
Before introducing other techniques based on the Monte Carlo method, let us discuss,
perhaps, the first Monte Carlo application. This example is extensively covered in the
literature on Monte Carlo and has a certain charm, probably because it shows how it
is possible to estimate a universal constant, π, probably the most universally known
constant, with a very simple game. Also it is an interesting example of a problem that
can be solved with either a deterministic approach or a probabilistic one. Buffon’s
needle was one the earliest problems in geometric probability to be solved.
In 1777 Georges−Louis Leclerc, Count of Buffon, an illustrious naturalist, described
in his treatise Essai d’Arithm´etique morale (Buffon, 1777) a method of estimating π
using a random process. Obviously he did not use computers nor did it have sequences
of random numbers. The fact remains, however, that the basic idea was precisely that
of evaluating an unknown quantity by assuming it as an average value of a suitable
random variable and then trying to estimate the latter by examining an appropriate
sample.
Suppose we ‘randomly’ throw a needle of length a on a plane intersected by parallel
lines, each d apart on the other and let d > a. With N the total number of tosses and
n the number of times the needle crosses a line, Buffon showed that:
π ≈
2aN
dn (3.71)
Let’s see the reason for the formula (3.71). The first step is to formulate the problem
in a probabilistic context. In order for the problem to be determined, it must explicitly
state what is meant by the expression ‘random throw’. In reality, a good ‘pitcher’ could
certainly make the needle intersect all, or almost, the times she or he wants, but hereContinuous-time Markov chain 81
d
C
a
X
(a/2) sin(f)
φ
Fig. 3.17 Buffon’s needle for the calculation of π. Geometry scheme: a is the length of the
needle, C its centre point and d the distance between the lines.
d/2
x
(a/2) sin (f)
f [rad] 0
Ω
!
A
0
Fig. 3.18 Buffon’s needle for the calculation of π. Graphical representation of the change of
x (distance of C from the nearest row) as a function of φ (angle between the needle and the
lines).
we have specified that the launches must be at random. It is about imposing certain
conditions − entirely intuitive − on the position of the needle centre and direction.
We introduce two random variables: X and Φ such that: (see Fig. 3.17 and 3.18):
• (A) X represents the values that at the distance can take centre C of the needle
from the nearest row. X is uniformly distributed in [0, d/2], so that the probability
density function is fX(x) = 2/d, for 0 6 x 6 d/2. This is equivalent to assigning
equal probability to the ordinate of C.
• (B) The variable Φ represents the values that the angle can take formed by the
needle and parallel lines. Φ is uniformly distributed in [0, π], so that the probability
density function is fΦ(φ) = 1/π, for 0 6 φ 6 π.82 Introduction to Stochastic Processes
This means assigning equal probability to the direction of the needle. In the simple
fact that we have defined the range [0, π], it is implied that we know how much
is π and, with the procedure that we are going to explain, we also suppose it
unknown. The purpose of all this is obviously didactic.
• (C) X and Φ are independent, so f(x, φ) = fX(x) fΦ(φ), that is, the joint density
function is equal to the product of marginal densities.
The density function f(x, φ) of the double random variables (X, Φ) is given by:
f(x, φ) = 2
d
×
1
π
, if 0 6 x 6 d/2, 0 6 φ 6 π
Geometrically, therefore, the needle positions are defined by the points of a rectan￾gle of sides d/2 and π (see Fig. 3.18). From Fig. 3.18 it can be deduced that, in order
for the needle to cross one of the lines, the realizations of the random variables (X, Φ)
must belong to A, where A is given by:
A =
n
(x, φ) : x 6
a
2
sin φ
o
(3.72)
Recall now the so-called ‘geometrical definition’ of probability. R is the region of the
plane (or of a space or of an hyperspace) and we consider another region r contained
in R, as shown in Fig. 3.19.
r
R
Fig. 3.19 Geometric definition of probability.
The probability P that a randomly chosen point in R falls into r is proportional
to the measure (area, volume, hypervolume) of r and it does not depend on either its
shape or position within R. We will therefore write:
P = mis r / mis R
In agreement with the above definition, in our case, the probability that the needle
intersects a line, that is the probability P {intersection} of the ’intersection’ event, is
given by the ratio between the area dashed A and the area of the whole rectangle
Ω = [0, d/2] × [0, π]
P {intersection} =
A
Ω
=
Z π
0
(a/2) sin φ dφ ×
2
dπ (3.73)Continuous-time Markov chain 83
In particular, if you choose a needle that is half the length of the width between
the parallel lines, i.e. with a = d/2, we obtain:
P {intersection} = 1/π (3.74)
Let us dwell on the equation written above. On the one hand we have a probability,
and on the other the unknown number. Probability is also a number and it is also
unknown, but we can estimate it. Each throw of the needle, in fact, can be regarded
as a test with probability p of success (intersection with the line) and 1 − p of failure,
that it can be described by the Bernoulli (Bern) random variable; in N repeated and
independent tests
S =
X
N
i=1
Bern(i)
where S is the binomial r.v. Bin of parameters N and p, or by writing it more ex￾tensively Bin(n

p, N), where n represents the number of successes. The first moment
(average) E [Bin] and the second moment (variance) Var [Bin] are:
E [Bin] = N p Var [Bin] = N p (1 − p)
What is observed are the f = n/N of the r.v. with frequency Fr = Bin/N, which
represents the ratio of the number n of successes and the number N of trials, and has
average:
E [Fr ] = E [Bin]
N
=
N p
N
= p
and variance:
Var [Fr ] = Var [Bin]
N2
=
N p(1 − p)
N2
=
p(1 − p)
N
fr is a unbiased estimator of p. In other words, in a series of tests, frequencies have
mean p, hence eqn (3.71). The standard deviation σf of fn − the so-called ‘standard
error of the average’ − is given, from:
σf =
r
p (1 − p)
N
(3.75)
where, commonly, in the calculation of p its estimate fn is used.
The procedure suggested by Buffon can be reproduced by computers, once a suit￾able program has been developed. This essentially consists of determining N values of
x and φ, in the appropriate ranges of variation, through sequences of random numbers
provided by the computer itself. The calculation program will contain one statement
to perform the eqn (3.72) inequality, and another to count the times it is verified.
Finally, eqn (3.71) ratio is calculated.
The code to simulate needle throws is Code_3_5.R, where pi is the ‘known’ π,
pig is the ‘unknown’ one. The meaning of n and N is that described in the text and
the runif (k, a, b) function generates k random numbers uniformly distributed
between a and b.84 Introduction to Stochastic Processes
Table 3.2 shows the results of calculations with a = d/2 and in which the number
of ‘throws’ has changed, that is to say the number N of the assumed values randomly
from x and from φ. The first column shows N, the second the number of intersections
obtained, i.e. how many times the inequality eqn (3.72) was found to be valid, and in
the third the value of π given by eqn (3.74) and in the fourth the percentage error
 compared to the true value of π (3.141592654 ...), showing that as N increases the
dispersion decreases, as eqn (3.75) also indicates.
Table 3.2 Buffon needle from computations.
throws intersections π computed (%)
100 40 2.50000 20
1000 286 3.49650 11
10000 3086 3.24044 3.1
100000 31652 3.15936 0.57
1000000 318319 3,14150 0.17
In this example, the value of π is known, but of course the Monte Carlo method is
used when the value to be estimated is unknown; in this case, the error will have to be
evaluated by running simulations with increasing values of N. If we didn’t know the
value of π, all that we could get from the calculator’s answers would be a confidence
interval which the value sought belongs. If, for example, we had chosen N = 1000 and
a level equal to 95% for the confidence interval, we would have final results:
1/π = f ± 1, 96r
f(1 − f)
N
= 0.286 ± 0, 028
where f is the proportion of successes in 1000 launches (see Table 3.2 second line).
Results of the computation are shown in Fig. 3.20
0 1000 3000 5000
2.5 3.0 3.5 4.0
number of throws
!
Fig. 3.20 Buffon needle: computation of π as function of the number of throws N.Continuous-time Markov chain 85
The above procedure for calculating π can also be regarded as a general method of
calculating through random numbers a definite integral. Consider, in fact, Fig. 3.18.
Suppose we want to evaluate the dotted area A. It holds for eqn (3.73) written differ￾ently:
A =
number
P {intersection}
probability
Ω ≈ (n/N)
estimate
Ω
Therefore, the computation of a definite integral is reduced to the computation
of how many times by ‘randomly pulling’ on the rectangle of area Ω, the area A is
hit. Such a method for calculating integrals defined is called Monte Carlo ‘win or lose’
(hit-or-miss). The Code_3_5.R computes the solution as presented below.
#Code_3_5.R
## Buffon needle problem
# needle length a = half the distance d between the lines.
# d = 1
# pig is the known mathematical constant pi
set.seed(2)
pigl<- numeric()
Nin<- 100
Nfin<- 5000
N<- seq(Nin,Nfin,by=100)
Nl<-length(N)
for(l in 1:Nl){ # external loop: changes number of throws
n<-0
for(i in 1:N[l]){ # internal loop on throws
x<-runif(1,0,1/2)
phi<-runif(1,0,pi)
if( x <=(1/4)*sin(phi))n<- n+1 # formula: x <= a/2 sin(phi) a=d/2 con d=1
p<- n/N[l]
pig<- 1/p
}
pigl[l]<- pig
}
N
pigl
par(mai=c(1.02,1.,0.82,0.42)+0.1)
plot(N,pigl,type="b",xlab="number of throws",
ylab=expression(pi),cex.lab=1.3,cex.axis=1.2,
lwd=2,lty=3,col="black",font.lab=3)
abline(h=pi,lty=1,lwd=2,col="black")86 Introduction to Stochastic Processes
3.5 Ehrenfest urn model
We conclude this chapter describing a famous stochastic problem whose roots deeply
penetrate into the ground of physics. There are two urns, A and B, and 100 balls
enumerated from 1 to 100. We put, for instance, 80 ball in A and 20 in B. It does not
matter how we choose them. We can put in A the balls 1, 2, . . . , 80 and the remaining
ones in B, or we can put them at random. We also have a third urn with 100 cards,
each with a number from 1 to 100. We draw a card and pick up the ball with this
number. If it is in A, we move it in B, if it is in B, it goes in A. The drawn card is put
again in the card-urn. We continue the drawing, and follow for some time the balls
going from one urn to the other one. This ‘game’ was conceived in 1907 by Paul and
Tatiana Ehrenfest (Ehrenfest and Ehrenfest, 1907), and it is known as the ‘Ehrenfest
urn model’, but also ‘Ehrenfest dog-flea model’. Costantini and Garibaldi (2004) quote
the 1907 article in which the Ehrenfest say:
Jedesmal, wenn eine Nummer gezogen wird, huepft die Kugel mit dieser Nummer aus der
Urne [. . . ] in die andere Urne
[Every time a number is drawn, the ball with that number bounces out of it]
Perhaps the Ehrenfest had in mind just fleas, or anyway jumping bugs, since they
used the verb hupfen ¨ , which means ‘to jump’, more suitable to fleas rather than balls.
It was not an easy life, that of Tatiana Alexeyevna Afanassjewa and Paul Ehrenfest.
She was an Orthodox Russian, while he was Jewish. They both had to declare that
they were without religion to get married. Tatiana was born in 1876 in Kiev in the
Ukraine, Paul in 1880 in Vienna. The interested reader can find many articles about
the Ehrenfest, for instance in Klein (1985).
In the years 1899−1900, in Vienna, Paul followed Boltzmann’s lectures on thermo￾dynamics. In 1902 he went to G¨ottingen and in 1904 he received his doctorate with the
dissertation (English translation) On the extension of Hertz’s mechanics to problems
in hydrodynamics, with Boltzmann’s supervision. In 1906, he came back to G¨ottingen.
Ehrenfest and Boltzmann would never see each other again. Boltzmann killed himself
in that year in Duino near Trieste and Ehrenfest did the same in Leiden in 1933.
Tatiana and Paul got married in Vienna on December 21, 1904, so Tatiana became
Tatiana Ehrenfest.
Difficult years followed, without any regular employment, but they continued study￾ing and writing. The ‘Urn model’ was published in 1907 and in 1906 they had begun
to write the famous monograph (English translation) The Conceptual Foundations
of the Statistical Approach in Mechanics for the Encyklopaedie der mathematischen
Wissenschriften (published in 1912). For five years (1907−1912) they lived in St. Pe￾tersburg and eventually, in October 1912, they moved to Leiden, where Paul became
professor of theoretical physics, succeeding H.A. Lorentz. But depression dug deep into
Paul’s soul. Their third son Wassik was born with Down’s syndrome. On September
25, 1933 Paul committed suicide by shooting himself.
Tatiana continued her life in Leiden, taking care of teaching mathematics up to
her death in 1964. One of her final writings, published in the American Journal of
Physics in 1958, is On the use of the notion of ’probability’ in physics. We quote here
the abstract.Ehrenfest urn model 87
An analysis of the meaning of such terms as ‘at random’, ‘equal chances’, ‘probability cal￾culus’, ‘laws of probability’ as used in physics. The author treated this subject for the first
time in a lecture at St. Petersburg in 1911, after which it was published in Russian in the
Journal of the Physical-Chemical Association of St. Petersburg. Later, Paul Ehrenfest made
it the topic of several lectures.
To get an idea of the underlying problems addressed by the Ehrenfest we have to go
back in time, some years before the ‘Urn model’, to 1872 when Boltzmann pursued the
intent of deducing from the laws of Newtonian mechanics the thermodynamic proper￾ties of a system formed by an extremely large number of particles (atoms or molecules),
given constraints and initial conditions. The development of such a programme culmi￾nated in the famous ‘H -theorem’. In this theorem Boltzmann aimed to prove that for
a perfect (or ideal) gas with no external forces acting on the particles, and under the
assumption of absence of correlation in the initial state between the positions and the
velocities of colliding particles, there is a function, called the H function, that may
never grow over time and tends to decrease as equilibrium is approached up to a lim￾iting value corresponding to Maxwell’s distribution of molecule velocities. Moreover,
the H function is linked to the thermodynamic entropy S through the relation:
S = −kBH
with kB the Boltzmann’s constant. This equation is, in the intentions of Boltzmann, the
proof of the second law of thermodynamics from principles of mechanics and it shows
that the time evolution of the macroscopic state of a gas is irreversible. Note that the
assumption of absence of correlation (Stosszahlansatz, or molecular chaos assumption)
is purely statistical in character. That introduces an element of statistical nature in
Boltzmann’s ‘deduction’.
The theorem was strongly criticized. In 1876, Loschmidt maintained that if at
equilibrium the direction of all particles is reversed, since the microscopic laws are re￾versible, the system should return to its initial state with a decrease of H (reversibility
objection). Later, Poincar´e (1890) and Zermelo (1896) argued that, waiting for a long
enough time, the distribution of particles in the state space becomes indistinguishable
from the original distribution (recurrent objection). Therefore the H function should
be periodic, contrary to H -theorem.
These and similar objections led Boltzmann to revise his theory published in 1877.
There are no more references to molecules collisions, and the key point is the explicit
introduction of the statistical hypothesis about microstate equiprobability. For the
first time a statistical hypothesis enters, so to speak, in a pair with deterministic laws,
those of Newtonian mechanics. Boltzmann relates the entropy S to the logarithm of
the probability W, that is to the number of microstates corresponding to the same
macrostate, through the well-known formula:
S = kB log W
We return to the Ehrenfest model to read how Paul and Tatiana attempted to
explain as clearly as possible Boltzmann’s use of probability inside the conceptual
framework of physics. We will describe the model by means of a discrete-time Markov
chain in a discrete state space. The time steps are the draws of the cards and the88 Introduction to Stochastic Processes
consequent transfers of the balls from one urn to the other. The state of the system at
time i, that is after the i−th draw of the card, is defined by the number of balls in the
urns. If N is the number of balls (100 in our example) the possible states are 0 balls
in urn A and 100 in urn B, 1 ball in A and 99 in B, . . . 100 in A and 0 in B. Obviously
if there are j balls in A, there will be N − j in B.
As before in this chapter, Xi = j means that there are j balls in A at time i. At
the next time i + 1, either a ball has been drawn in A, so that Xi+1 = j − 1, or in B,
then Xi+1 = j + 1. It is clear that this chain is a Markov chain, Xi+1 = j ± 1 depends
only on how many balls there were in the urns at the previous time. If j balls are in
A at time i, the probability of drawing a ball from A is j/N, therefore the transition
probability from state j to state j − 1 is given by:
pj,j−1 =
j
N
and of course:
pj,j+1 =
N − j
N
= 1 −
j
N
Let us remark on a point, that might appear obvious but is very important. The motion
of the single ball is time symmetric, meaning that each ball at each time instant (i.e.
at each card draw) has the same probability equal to 1/N of moving from A to B
or from B to A. The ball number 4, for instance, always has the same probability of
being raffled off, either it is in A or in B. Here there is an underlying hypothesis of
equiprobability: the cards continue to have the same odds of being drawn. This is not
so obvious as perhaps it might seems.
Other notations can be used to define the state of the system. For example, instead
of Xi = j, we can write (NA)i = nA or (NB)i = nB, to indicate the number of balls
in A or in B at time i. We write also N = nT for the total number of balls, and by
putting ne = nT /2, we can consider the variable ∆n = NA − ne. The state of the
system at time i is defined by (∆n)i = (NA − ne)i = δ. In any case, regardless the
chosen notation, the state of the system at time i is defined by means of only one
variable: NA or ∆n or |NA − NB|, and so on.
We now study the Markov chain. The transition matrix E is given by:
E =


0 1 2 3 . . . nT −1 nT
0 0 1 0 0 . . . 0 0
1 n
−1
T
0 1 − n
−1
T
0 . . . 0 0
2 0 2n
−1
T
0 1 − 2n
−1
T
. . . 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nT −1 0 0 0 0 . . . 0 n
−1
T
nT 0 0 0 0 . . . 1 0


(3.76)
For instance, with only 4 balls (nT = 4) the states are 0, 1, 2, 3, 4 and the transition
matrix is (the state of the system is defined by the variable NA):Ehrenfest urn model 89
E =


0 1 0 0 0
1/4 0 3/4 0 0
0 1/2 0 1/2 0
0 0 3/4 0 1/4
0 0 0 1 0


(3.77)
Notice that there are no absorbing states, that is pjj = 0, ∀j, j = 0, . . . , nT , even
though at a certain time there may be zero balls in an urn, the process continues
since, at the next time, there will be certainly one ball in the same urn.
Let us note also that the chain eqn (3.76) is irreducible and periodic with period
2, i.e. every state has period 2. Recall that a state j is periodic with period d
(j)
if starting from j it is possible (even if it is not sure) to return to j only after n
steps, with n = d
(j)
, 2d
(j)
, 3d
(j)
, . . ., and d
(j)
is the largest of such integer values. With
nT = 4, if the system is, for example, in the state nA = 1, two possible trajectories
are:
1 → 0 → 1 → 0 → 1 → 0 → 1 · · ·
1 → 2 → 1 → 2 → 3 → 2 → 1 · · ·
We can transform the chain from periodic to aperiodic, if the flea has the chance to
repent. The flea, while it is in flight, can stop on a shrub and toss a coin. If it gets heads
it continues the hop, if tails it comes back on the starting dog. Both chains (periodic
and aperiodic) have the same stationary (or invariant) distribution. For an aperiodic
chain, remember, the stationary distribution is also the asymptotic distribution.
The stationary distribution is given by:
πk =
1
2
nT
nT !
k!(nT − k)!, k = 0, 1, . . . , nT (3.78)
The above eqn (3.78) is nothing different from a binomial distribution Bin(p), giving
the probability of observing k successes in n trials, when the probability of success in
a single trial is p:
Bin(p) = 
n
k

p
k
(1 − p)
n−k
, k = 0, 1, . . . , n
If we put n = nT and p =
1
2
, we find again eqn (3.78). The stationary distribution πk
is the probability of the process being in state k in the long run, and such a probability
does not change anymore. Therefore we can write:
πk = P {Xi→∞ = k} , k = 0, 1, . . . , nT (3.79)
or with another notation, by referring to the number of balls in the urns A and B,
eqn (3.78) can be written as:
πnA =
1
2
nT
nT !
nA!nB!
, nA = 0, 1, . . . , nT
and so eqn (3.79) becomes:90 Introduction to Stochastic Processes
πnA = P {(NA)i→∞ = nA} , nA = 0, 1, . . . , nT
The result is the same, whatever the notation is: at the starting time we can have
any number of balls in A, also nA = nT or nA = 0, but for i → ∞, that is after
many draws of the numbered cards, the probability πk of k balls in A does not change
anymore. The situation is similar to the previously discussed example of the Land of
Oz. After a certain number of days, the probability of raining is equal to 57%, whatever
the previous time state has been. Saying that the system tends to a stationary state,
regardless of the starting state, implicitly indicates a clear and precise time direction.
Therefore, while the move of a single ball from one urn to another determines no time
direction, a time evolution emerges from the initial to the stationary state, by the
way the states of all balls following each other, that is the system as a whole. Once
stationarity is reached, the time direction disappears.
Recall that the succession of transition matrices P(n) of an irreducible and periodic
chain does not converge to the asymptotic matrix P(∞)
. If however the sequence of
arithmetic means:
1
n
h
P + P
(2) + · · · + P
(n)
i
(3.80)
is considered, then for n → ∞, such a sequence converges (‘in the Cesaro sense’) to a
limiting matrix P∗ with all positive entries and all rows identical. In the example of
matrix eqn (3.77) and with n = 1000 in eqn (3.80), the limiting matrix E∗
is obtained:
E
∗ =


0.06266667 0.2506667 0.375 0.2493333 0.06233333
0.06266667 0.2501667 0.375 0.2498333 0.06233333
0.06250000 0.2500000 0.375 0.2500000 0.06250000
0.06233333 0.2498333 0.375 0.2501667 0.06266667
0.06233333 0.2493333 0.375 0.2506667 0.06266667


Through eqn (3.78),the stationary probabilities vector is:
π = (0.0625, 0.25, 0.375, 0.25, 0.0625) (3.81)
So with n = 1000, a good approximation to the exact result is already obtained.
Let us still remain on eqn (3.78). We can look at the ratio:
nT !
k!(nT − k)! (3.82)
as giving all the possible ways of distributing k balls in A and nT − k in B, regardless
of the number they have written on. Let us suppose to have six balls 1, 2, 3, 4, 5, 6.
If we have k = 4 balls in A (2 in B) at time i, we may have the balls 2, 4, 5, 6 are in
A and the 1, 3 in B, or balls 1, 2, 3, 4 in A and 5, 6 in B, and so on. There are 15
ways of placing four balls in A and 2 in B, that is there are 15 possible combinations.
In formulae, the number of combinations is given by:
6!
4!(6 − 4)! = 15
Let us use, from now on, the language of statistical mechanics. To say that there
are four balls in A means to specify the macrostate, or ‘macroscopic state’, of theEhrenfest urn model 91
system. To say that the ball number 1 is in A, the number 2 is in B, and so on, means
to specify the microstate, or ‘microscopic state’, of the system. In this terminology
eqn (3.82) gives the number of microstates corresponding to the same macrostate with
k balls in A and πk in eqn (3.78) gives the probability that the system will be in the
macrostate with k balls in A in the long run.
Look at Table 3.3. It reports all the possible microstates compatible with the
macroscopic constraints. The only macroscopic constraint is the number of balls nT =
6. Denote by µl the microstates. In total there are 26 = 64 microstates, so µ1 =
(1, 2, 3, 4, 5, 6; −), µ2 = (1, 2, 3, 4, 5; 6), µ3 = (1, 2, 3, 4, 6; 5), . . . µ64 = (−; 1, 2, 3, 4, 5, 6),
the semicolon divides the balls in A from those in B. All the microstates have the same
occurrence probability, i.e. we make the hypothesis that each ball has probability
1/nT of being drawn, as a consequence of the assumption of equiprobability of the
card draws. The equiprobability of microstates is the basic hypothesis of Boltzmann’s
foundation of statistical mechanics.
If the microstates have the same probability, this is not true for the macrostates.
The macrostate composed of 20 microstates is more probable than that composed by
six microstates. For instance, the probability of four balls in A is 15/64 ≈ 0.23, that
of three balls is 20/64 ≈ 0.31, that of five is 6/64 ≈ 0.094, that of six is 1/64 ≈ 0.016.
When the system is in the stationary state it is said that it is ‘at equilibrium’, a
property of the macrostate. Suppose now we have 1000 pairs of urns and 1000 persons
drawing the balls at the same time. After stationarity is reached, we take a picture
of the 1000 pairs of urns. It results that about 230 urns have four balls in A, about
310 have three balls, . . . , about 16 have six balls. Turning to the statistical mechanics
language, we can state that the evolution of a system at the microscopic level is
symmetric with respect to time, but a precise temporal direction appears if the system
is considered at the macroscopic level. Past and future only exist at the macroscopic
level. It is like saying that we can talk about a ‘before’ and an ‘after’, only if the system
is dealt with ‘statistically’.
The stationary probability vector comes from eqn (3.78):
π = (0.016, 0.094, 0.23, 0.31, 0.23, 0.094, 0.016) (3.83)
We will shortly deal with the random variable ∆n = NA − ne where ne = nT /2.
∆n has values:
δ1 = ne, δ2 = ne + 1, . . . , δne+1 = 0, . . . ,
δnT = −ne − 1, δnT +1 = −ne
Of course, the enumeration can be reversed by putting δ1 = −ne and δnT +1 = ne.
To summarize, ∆n is the random variable, and the δn’s are its realizations. The
expression:
πδn = P {(∆n)i→∞ = δn} , δn = ne, ne + 1, . . . , −ne − 1, −ne
is the analogue of:
πnA = P {(NA)i→∞ = nA} , nA = 0, 1, . . . , nT
For instance, with nT = 6, the values assumed by the random variable ∆n are reported
in Table 3.4.92 Introduction to Stochastic Processes
Table 3.3 Macrostates and corresponding number of microstates with six balls.
macrostate balls in A balls in B # microstates
6 − 0 1, 2, 3, 4, 5, 6 − 1
1, 2, 3, 4, 5 6
1, 2, 3, 4, 6 5
5 − 1 . . . . . . 6
2, 3, 4, 5, 6 1
. . . . . .
2, 3, 4, 5 1, 6
1, 2, 5, 6 3, 4
4 − 2 . . . . . . 15
2, 4, 5, 6 1, 3
. . . . . .
2, 3, 4 1, 5, 6
2, 5, 6 1, 3, 4
3 − 3 . . . . . . 20
1, 4, 6 2, 3, 5
. . . . . .
2, 5 1, 3, 4, 6
1, 2 3, 4, 5, 6
2 − 4 . . . . . . 15
1, 6 2, 3, 4, 5
. . . . . .
2 1, 3, 4, 5, 6
1 2, 3, 4, 5, 6
1 − 5 . . . . . . 6
5 1, 2, 3, 4, 6
. . . . . .
0 − 6 − 1, 2, 3, 4, 5, 6 1
Table 3.4 Macrostates, number of corresponding microstates and values of ∆n = NA −nT /2
with nT = 6.
macrostate # microstates δn
6 − 0 1 3
5 − 1 6 2
4 − 2 15 1
3 − 3 20 0
2 − 4 15 −1
1 − 5 6 −2
0 − 6 1 −3
In a further notation the invariant distribution πk in eqn (3.78) is written as:
γg(k) or also γg(nA) or also γg(δn)
where γ = 1/2
nT and g(·), called multiplicity function, is the ratio eqn (3.82), that
written as a function of δn is:Ehrenfest urn model 93
g(δn) = nT !
(ne + δn)!(ne − δn)!, δn = ne, . . . , −ne
Note that γ, or more precisely γ(nT ), is the probability of realization of any microstate
l of the set of all the 2nT possible microstates µl
, l = 1, . . . , 2
nT :
γ(nT ) ≡ P {µl} =
1
2
nT
(3.84)
As said before, g(·) represents the number of microstates corresponding to the same
macrostate with nA − ne balls in A. The total number of microstates 2nT can be
written as a function of g(·):
2
nT =
nXT +1
r=1
g(δr) (3.85)
For instance, with nT = 6, it is from Table 3.4: g(δ1) = 1, g(δ2) = 6, . . . , therefore:
2
nT = 1 + 6 + 15 + 20 + 15 + 6 + 1 = 64
Let us slightly increase the number of balls, by putting nT = 10. Table 3.4 becomes
Table 3.5. The stationary probabilities vector is now given by:
Table 3.5 Macrostate, number of corresponding microstates and values of ∆n = NA −nT /2
with nT = 10.
macrostate # microstates δn
10 − 0 1 5
9 − 1 10 4
8 − 2 45 3
7 − 3 120 2
6 − 4 210 1
5 − 5 252 0
4 − 6 210 −1
3 − 7 120 −2
2 − 8 45 −3
1 − 9 10 −4
0 − 10 1 −5
π = (0.00098, 0.0098, 0.044, 0.12, 0.21, 0.25,
0.21, 0.12, 0.044, 0.0098, 0.00098) (3.86)
and the number of microstates is now 210 = 1024.
The macrostate with nA = nB = 5 is the realization of 252 microstates, while it
is 20 with nT = 6. Therefore if the number of balls increases by a factor of 2, the
probability of realization of the macrostate nA = nB, with respect to the probability
of a macrostate with no ball in A (or in B), increases by one order of magnitude.
Therefore, the equilibrium situation having nA = nB is more and more probable with
respect to the other ones, with the increase of the number of balls. In terms of fleas94 Introduction to Stochastic Processes
and famous cartoon dogs, even though all the fleas are on the Tramp at the beginning,
after a little time about one half of them have hopped onto the Lady.
If nT → ∞, the binomial distribution tends to a sharply peaked normal distribu￾tion. How much peaked? The density function of the binomial random variable
Bin(p) = N!
k!(N − k)! p
k
(1 − p)
N−k
, k = 0, 1, . . . , N
for very large N tends to the density function of the normal random variable
N (µ, σ) = 1
√
2πσ
exp 
−
1
2
(x − µ)
2
σ
2

where µ = N p and σ =
p
N p(1 − p).
We can see that:
g(δn) ≈ g(0) × exp 
−
2 δ
2
n
nT

(3.87)
where g(0) is:
g(0) = [2/(πnT )]1/2
2
nT
g(0) is g(δn), when δn = 0, that is in the ‘continuous’ approximation of eqn (3.82)
with k = (nT − k), that is to say nA = nB.
Proof Compute g(nA) and make nT tend to infinity. Compute first the logarithm of g(nA).
log g(nA) = log nT !
nA!nB!
= log nT ! − log nA! − log nB!
and by Stirling’s approximation:
n! ≈ (2πn)
1/2
n
n
exp 
−n +
1
12n
+ . . . 
n→∞ −−−−→ (2πn)
1/2
n
n
e
−n
Then:
log nT ! ≈
1
2
log 2π +
1
2
log nT + nT log nT − nT
=
1
2
log 2π +

nT +
1
2

log nT − nT
Similarly for nA and nB:
log nA! ≈
1
2
log 2π +

nA +
1
2

log nA − nA
log nB! ≈
1
2
log 2π +

nB +
1
2

log nB − nB
Turning to log nT !, we put nA +nB instead of nT in some place and add and remove 1
2
log nT .
So we have:Ehrenfest urn model 95
log nT ! ≈
1
2
log 2π
nT
+

nA +
1
2
+ nB +
1
2

log nT − (nA + nB)
and subtracting log nA! and log nB!, we have:
log g(nA) = log nT !
nA!nB!
≈
1
2
log 1
2πnT
−

nA +
1
2

log nA
nT
−

nB +
1
2

log nB
nT
Consider that:
log(1 + x) ≈ x −
1
2
x
2 + . . . , se x  1
Now:
nA
nT
=
δn + ne
nT
=
δn
nT
+
nT /2
nT
=
1
2
+
δn
nT
=
1
2

1 + 2δn
nT

Similarly:
nB
nT
=
1
2

1 −
2δn
nT

And in logarithm form :
log nA
nT
= log 
1
2

1 + 2δn
nT
 = − log 2 + log 
1 + 2δn
nT

≈ − log 2 + 2δn
nT
−

2δn
nT
2
and
log nB
nT
≈ − log 2 −
2δn
nT
+

2δn
nT
2
Neglecting the terms (2δn/nT )
2
:
log g(δn) ≈
1
2
log 2
πnT
+ nT log 2 −
2 δ
2
n
nT
Finally the logarithms are removed.
✷
So we find that g(δn) for nT → ∞ is normal, practically a Dirac delta function.
At stationarity the peak is on the maximum value corresponding to δn = 0. If we put
δn = (nT /2)1/2
, we have:
δn
nT
=
1
nT
nT
2
1/2
=

1
2nT
1/2
and from eqn (3.87), it results g(δn) ≈ g(0) × e
−1
, that is g(δn) has decreased by a
factor 1/e ≈ 0.37 with respect to its maximum value. Therefore the quantity 1/(2n
1/2
T
)
can be assumed to be a measure of the relative width, that is in relation to nT , in
g(δn). Figure 3.21 shows the above quantities with nT = 100. With this value, we
have δn = (100/2)1/2 ≈ 7.071 and since g(0) ≈ 1 × 1029
, g(δn) ≈ 1 × 1029 exp[−2 ×
7.0712/100] ≈ 3.7 × 1028, that is g(δn) ≈ g(0) × e
−1
.96 Introduction to Stochastic Processes
To give an idea of how narrow the peak is, let nT be equal to one million. The
probability of finding 505000 balls in the urn A, with respect to that of finding 500000,
that is a fluctuation about 1% with respect to ne, is given from the ratio g(5000)/g(0)
that is of the order of 10−22. If the balls are 1024, the peak width is of the order of
10−12, almost 0. Such extreme numbers are those of statistical mechanics, the number
of molecules of a gas, for instance. So the lesson is: the process is a Markov chain
x1028
10
8
6
4
2
0
-50 -40 -30 -20 -10 0
δ
n
g(δ
n
)
(- nT
/2)1/2 (+ nT
/2)1/2
nT = 100
10 20 30 40 50
Fig. 3.21 Gaussian approximation of g(δn) with nT = 100. The dashed lines, starting from
the marked points, cross the curve at 1/e of its maximum value.
converging to an invariant distribution π. When nT is large, the probabilities:
πδn = P {(∆n)
i→∞ = δn} , δ1 = ne, . . . , δnT +1 = −ne
are all ≈ 0, with the exception of δn ≈ 0, that is when n ≈ ne + 1, for which:
πne+1 = P {(∆n)
i→∞ = δne+1 = 0} ≈ 1
Remember the definition of expected value. Let X be a discrete or continuous
random variable and Y a random variable function of X, that is Y = f(X). The
expected value E [Y ] of Y is given by:
E [Y ] = E [f(X)] =



Z +∞
−∞
f(x)p(x) dx continuous case
X
r
f(xr)p(xr) discrete case
where, if X is discrete p(xr) = P {X = xr} is the probability mass function and r
covers the support of X, finite or numerable subset of R.
For example, let us take nT = 6 and X = ∆n, and choose f(∆n) = (∆n)
2
. In the
above sum, xr is δr and f(xr) = δ
2
r
. We are interested in evaluating the expected valueEhrenfest urn model 97
of Y , that is E [Y ] = E [f(X)] = E 
(∆n)
2

. The total number of microstates is given
by eqn (3.85) for which, if all the microstates are equiprobable, p(xr) is:
p(δr) = 1
2
nT
g(δr) = 1
nXT +1
r=1
g(δr)
g(δr) (3.88)
We have seen in Table 3.4 that:
δr (r = 1, . . . , 7) = 3, 2, 1, 0, −1, −2, −3
then the values assumed by the random variable (∆n)
2 are:
δ
2
r
(r = 1, . . . , 7) = 9, 4, 1, 0, 1, 4, 9
Again from the Table 3.4:
g(δr) (r = 1, . . . , 7) = 1, 6, 15, 20, 15, 6, 1
and:
X
7
r=1
g(δr) = 64
The general formula:
E [Y ] = E
(∆n)
2

=
nXT +1
r=1
δ
2
r p(δr) (3.89)
becomes in the present example:
E

(∆n)
2

=
X
7
r=1
δ
2
r p(δr) = 9 ×
1
64
+ 4 ×
6
64
+ 1 ×
15
64
+ 0 ×
20
64
+ 1 ×
15
64
+ 4 ×
6
64
+ 9 ×
1
64
= 96/64 = 1.5 = nT /4
(3.90)
We have reported all the steps for the sake of clarity, but recall that the probabilities
p(δr) are already known, indeed they are the components of the probability vector π
in the eqn (3.83).
When nT is large enough we can exploit the approximation eqn (3.87) and compute
the expected value by replacing the sum with an integral between −∞ and +∞. Then:
E

(∆n)
2

=
1
2
nT
Z +∞
−∞
δ
2
n
g(δn) dδn
=
1
2
nT
[2/(πnT )]1/2
2
nT
Z +∞
−∞
δ
2
n
exp(−2 δ
2
n/nT ) dδn
(3.91)98 Introduction to Stochastic Processes
Changing variables, by putting δ
2
n = x
2
, we obtain:
E

(∆n)
2

= [2/(πnT )]1/2
(nT /2)3/2
Z +∞
−∞
x
2
e
−x
2
dx
= [2/(πnT )]1/2
(nT /2)3/2
(π/4)1/2 = nT /4
Here we have computed exactly E [Y ]. We ask ourselves: is there a way to estimate this
expected value, if we are not able to solve the integral eqn (3.91) or compute the sum
eqn (3.89)? One way is to use the Markov Chain Monte Carlo method, by constructing
a Markov chain such that its invariant distribution π is the interested distribution.
In the example above with nT = 6, if we do not know the probabilities p(δr) in
eqn (3.88), we estimate them through the frequencies with which the system occupies
the states corresponding to δr = 3, . . . , −3 at stationarity. In the following we say only
‘state’ instead of ‘microstate’, if there is no possibility of misunderstanding.
The R code Ehrenfest_model simulates the urns model. Given an initial configu￾ration, chosen at random or fixed, the code computes the number of balls in the urns
at each card draw.
### Code_3_6.R
## Ehrenfest_model
## Ehrenfest urn model 6 balls 200 steps
## expected value of (Delta n)^2
set.seed(1)
diffe<-numeric()
f<-numeric()
nt<- 6
ne<-nt/2;
nsteps<- 200
numberA <- 4
ta<- 100
tb<- 200
nA<- numberA
diffe0<- nA-ne # starting state
i<-0
while(i<nsteps){ # starting loop on steps
i<- i+1
u<- sample(nt,1,1/nt)
if(u<=nA) nA<-nA-1 else nA<-nA+1
diffe[i]<-nA-ne
f[i]<- diffe[i]^2 # function of (Delta n), now = (Delta n)^2
} # ending loop on steps
t<-c(0:nsteps) # adding t=0
diffe<- c(diffe0,diffe) # adding starting diffe0
# t # uncomment to print t and/or diffe
# diffe
plot(t,diffe, type="l",ylim=c(-3,3),xlim=c(0,nsteps),main="",
ylab=expression(italic(Delta*n)),xlab="time",cex.lab=1.3,font.lab=3)
abline(h=0,lty=3,col="black",lwd=2)
lt<- length(diffe[(ta+1):tb] )
mtemp<- mean(diffe[(ta+1):tb] )
mtemp # time average
std.mtemp<- sqrt(var(diffe[(ta+1):tb])/lt) # std of the mean
std.mtemp
mtempf<- mean(f[(ta+1):tb] )Ehrenfest urn model 99
mtempf # time average
std.mtempf<- sqrt(var(f[(ta+1):tb])/lt) # std of the mean
std.mtempf
Figure 3.22 shows the time evolution of the values of the variable ∆n, with nT = 6.
The initial state is fixed with four balls in A, so (∆n)0 = 1. In words: at time 0 the
random variable ∆n assumes with probability 1 the value δ3 = 1. We performed the
0 5 0 100 150 200
−3 −2 −1 0 1 2 3
time
∆n
Fig. 3.22 Time evolution of the values assumed by the random variable ∆n with nT = 6.
time average of ∆n in the interval [101, 200]. The time average is given by:
1
100
X
200
i=101
(∆n)i = - time
[1 + 0 + (−1) + (−2) + . . . , +1 + 0]/100 = −0.12 (3.92)
Therefore, at the 101-th step it is: (∆n)101 = δ3 = 1, at the 102-th step it is:
(∆n)102 = δ4 = 0, etc. By saving the number of visits to states, we have the vec￾tor (2, 11, 25, 30, 23, 9, 0) to be compared with the second column, ‘# microstates’, of
Table 3.4 (the symbol ‘#’ means ‘number’). The estimate πb of the stationary proba￾bilities vector π, and π, are then:
πb = (0.020, 0.110, 0.25, 0.30, 0.23, 0.090, 0.00)
π = (0.016, 0.094, 0.23, 0.31, 0.23, 0.094, 0.016)
The estimate (∆\n)
2 of E
(∆n)
2

is obtained by inserting the components
(ˆp(δ1), . . . , pˆ(δ1)) = (0.020, . . . , 0.00)100 Introduction to Stochastic Processes
of the vector πb in eqn (3.90), obtaining:
Yb = (∆\n)
2 =
X
7
r=1
δ
2
r pˆ(δr) = 1.46
The exact value is E 
(∆n)
2

= nT /4 = 1.5.
More simply, we compute the time average of the values of f(∆n) = (∆n)
2
, that
is we put in eqn (3.92) (∆n)
2
i
in the place of (∆n)i
:
(∆n)
2 =
1
100
X
200
i=101
(∆n)
2
i = (1 + 0 + 1 + 4 + . . . , +1 + 0)/100 = 1.46
In this way we avoid doing the sum from r = 1 to r = nT + 1 in eqn (3.89), but if
nT were of the order of 1020 it would be prohibitive. Moreover, this way of proceeding
considers only those states that contribute to the average, that is only those states
visited with high probability.
The code computes directly the time averages and their standard error:
mtemp time average of diffe (Delta n = nA - ne)
-0.12
std.mtemp standard error of the mtemp
0.1208388
mtempf time average of f[i]<- diffe[i]^2 (Delta n)^2
1.46
std.mtempf standard error of the mtempf
0.1788967
Let us run again the above code Ehrenfest_model with nT = 1000. The input
data are now:
set.seed(2)
nt<- 1000
ne<-nt/2;
nsteps<- 5000
numberA <- 600
ta<- 3000
tb<- 5000
in this case, the results are:
mtemp time average of diffe (Delta n = nA - ne)
11.187
std.mtemp standard error of the mtemp
0.2511813
mtempf time average of f[i]<- diffe[i]^2 (Delta n)^2
251.054
std.mtempf standard error of the mtempf
5.731442
The time average of the (∆n)
2
i
is given by:
(∆n)
2 =
1
5000
5000
X
i=3000
(∆n)
2
i = - time
245 (3.93)
The exact value is 250. Figure 3.23 shows the time evolution of the values of the
variable ∆n, with nT = 1000. The initial state is fixed with 600 balls in A.Ehrenfest urn model 101
0 1000 2000 3000 4000 5000
−40 0 2 0 4 0 6 0 8 0 100
time
∆n
Fig. 3.23 Time evolution of the values assumed by the random variable ∆n wit nT = 1000.
From that figure, we notice that about 2000 steps are necessary to reach the sta￾tionarity, but even once it is reached, the system shows a poor mixing around the value
∆n = 0. The positive values assumed by the random variable ∆n are more than the
negative ones.
Figure 3.24 shows the number of states visited by the system. It is obtained by the
commands:
y<- diffe[(ta+1):tb]
hist(y,main="",xlab=expression(delta[n]),ylab="# visits",
cex.lab=1.5,font.lab=3)
Figure 3.24 shows two things. Bad news, it is not symmetric. As we said, the states
corresponding to positive values assumed by ∆n are visited more frequently than the
others. This means that the chain needs more steps to eliminate such a distortion.
But there is also good news. The code considers only those states that contribute
substantially to the time average. With nT = 1000, the range of possible states (mi￾crostates) is from δ1 = 500 (all the balls in A) to δ1001 = −500 (all the balls in B).
Figure 3.24 includes only states from δn = −30 to δn = 40, because the probability
of visiting other states is quite negligible. Just out of curiosity, if the number of steps
is increased, for instance, up to 40,000 and the average is taken between 20,000 and
40,000, the time average of ∆n is now −1.329.
We can rephrase what we said above in terms of entropy. In Chapter 13 we write
Boltzmann’s formula SB = k log W, where W represents the number of real microstates
corresponding to the same macrostate, under the hypothesis of equiprobability of the
microstates. In the present urns case, W is the number of all the possible ways of
distributing nA balls in A and nB balls in B:
W =
nT !
nA!nB!
So the Boltzmann’s equation can be written:102 Introduction to Stochastic Processes
δn
# visits
−20 −10 0 10 20 30 40
0 100 200 300 400
Fig. 3.24 Number of visits in the states δn, n = −nT /2, · · · + nT /2, with nT = 1000.
S = kB log 
nT !
nA!nB!

In this equation S represents the entropy of the system. One sees clearly that S in￾creases as W increases and reaches its maximum when the balls are equally distributed
between the two urns. If the system is far from the equilibrium state, it will evolve
in one precise direction, the one leading to the maximum S. Thus it is easy to see,
if nA > nB it is more probable the transition A → B and the opposite if nB > nA.
This can be seen as a model of the spontaneous expansion of gas particles. It is not
impossible de iure that S decreases, but de facto it is extremely improbable.
Are the Ehrenfests urns the answer to the Wiederkehreinwand (recurrent objection)
and to the Umkehreinwand (reversibility objection)? The answer is affirmative, surely
in their intention, but also for other readers. In any case, Tatiana and Paul have left
a fine lesson to make us understand clearly how statistics enters in physics. Mark Kac
refers to the Ehrenfest’s model as ‘probably one of the most instructive models in the
whole of physics.’ (Kac, 1959). About lecturer qualities, Delft (2014) writes: ‘[Paul]
Ehrenfest [. . . ] was a gifted lecturer who seemed to do magic tricks at the blackboard.’
Albert Einstein once called him ‘the best teacher in our profession whom I have ever
known’.
Let us recall the concept of ergodicity, by modifying the previous example. Instead
of the time average, we estimate the ensemble average, executing a number of different
trajectories (histories) and performing the average at a certain time. In the R code
Code_3_7.R below, the number of steps is 5000 and the number of trajectories is 1000.
The initial number of balls in the urns is chosen at random, that is at starting time
either all the 1000 balls or no balls can be in the urn A.
### Code_3_7.REhrenfest urn model 103
## Ehrenfest_many_hist
## Ehrenfest urn model 1000 trajectories, 5000 steps
# ensemble average of (Delta n)^2
set.seed(1)
nt<- 1000
ne<-nt/2
numberA <- seq(0,nt)
nsteps<- 5000 # number of steps
nhist<- 1000 # number of histories
splot<- 20 # to plot only "splot" histories
diffe0<- numeric() # see comments below
diffe<-matrix(,nhist,nsteps) # to save each history
f<- matrix(,nhist,nsteps) # to save (Delta n)^2
mstep<- 4000 # compute the ensemble average at step = mstep
numberA <- seq(0,nt) # starting with any number of balls in A
# (0<= numberA <= nt)
plot(t, type="n",ylim=c(-500,500),xlim=c(0,nsteps),main="",
ylab=expression(Delta*n),xlab="time",cex.lab=1.3,font.lab=3)
for(l in 1:nhist) { # starting loop on histories
i<-0
nA<- sample(numberA,1) # nA different for each history
diffe0[l]<- nA-ne
while(i<nsteps) { # starting loop on steps
i<-i+1
u<- sample(nt,1,1/nt)
if(u<=nA) nA<- nA-1 else nA<- nA+1
diffe[l,i]<- nA-ne
f[l,i]<- diffe[l,i]^2 # function of (Delta n), now = (Delta n)^2
} # ending loop on steps
} # ending loop on histories
t<-c(0:nsteps) # adding t=0
# adding to the matrix diffe a first column with the starting diffe
diffe<- cbind(diffe0,diffe)
for(l in 1:nhist) {
# to plot the histories from 1 to splot
if(l<=splot)lines(t,diffe[l,],lty=3)
}
segments(mstep,-500,mstep,+500,lty=4,lwd=2,col="black")
mdi<- mean(diffe[,mstep]) # ensemble average
mdi
se.mdi<- sqrt(var(diffe[,mstep]))/sqrt(nhist)
se.mdi
mdif<- mean(f[,mstep]) # ensemble average
mdif
se.mdif<- sqrt(var(f[,mstep]))/sqrt(nhist)
se.mdif
The result is shown in Fig. 3.25. At the time 4000 the values of (∆n)4000 and (∆n)
2
4000
are saved for all the 1000 histories. The ensemble average of the (∆n)
2
i
(here i numbers
the histories: 1 6 i 6 1000) gives the result:
(∆\n)
2 =
1
1000
1000
X
i=1
(∆n)
2
i = - history
245.566 (3.94)
Time average and ensemble average are in good agreement: the system is ergodic.
A theorem due to Birkhoff, known as ergodic theorem, states (in a discursive way)104 Introduction to Stochastic Processes
0 1000 2000 3000 4000 5000
−400 −200 0 200 400
time
∆n
Fig. 3.25 Time evolution of the values assumed by the random variable ∆n with nT = 1000
of 20 histories out of 1000. The trajectories length is 5000 steps. The vertical dot-dashed line
indicates the time point (t = 4000) at which the average is performed.
that because in the long run the system visits a large number of possible states,
a time sample of these values is equivalent to a sample extracted from all possible
system values. In other words, a single trajectory is representative of all the possible
trajectories allowed to the system.
Note the conceptual difference between eqns (3.93) and (3.94). The former is a time
average: only one trajectory and average on 2000 values (from 3000 to 5000 steps)
assumed by the variable (∆n)
2
. The latter is an ensemble average: 1000 trajectories
and the average is on values of (∆n)
2
that each of them assumes at a precise time
instant.
Figure 3.25 shows that all the trajectories, although starting from different initial
values, once they have reached stationarity (at about 2000 steps) assume values in a
narrow interval around ∆n = 0, as expected. Things are different for the standard error
of ∆n = 0, the ensemble average (∆\n)
2 and the time average (∆n)
2, computed through
the sample variance sqrt(var(f[,mstep]))/sqrt(nhist), in the code Code_3_7.R.
For (∆n)
2, std.mtempf = 5.731442, while for (∆\n)
2, se.mdif = 10.98741, a little bit
more than 50% with respect to the standard error of the time average. Here we are in a
situation in which the values assumed by the variable (∆n)
2
cannot be considered the
realization of mutually independent random variables with the same distribution. If
only one trajectory is available, we have to address, for instance, to the moving block
bootstrap, discussed in Chapter 11 to derive a correct estimate of the standard error
of the time average.Exercises 105
3.6 Exercises
Exercise 3.1 Consider a simple game. In a track like the following.
1 2 3
You start on position 1, the goal is to reach position 3. You flip a coin. If you get heads,
stay where you are. If you get tails, move forward in the next position. Each player counts
the number of coin flips. The winner is who reaches 3 in the lowest number of moves.
Write the transition probability matrix and the state transition diagram for this game.
Exercise 3.2 Write an R code to simulate the game of exercise 3.1.
Exercise 3.3 Consider a simplified version of the Game of The Goose, a very popular old
game attributed to Francesco de Medici (1541−1587). The simplified version consists of a
path having only 10 positions (instead of 64), starting from 0 and ending at 9, with a single
player and one die only (instead of two dice).
The path is like the following:
0 1 2 3 4 5 6 7 8 9
The rules of the game are the following:
• The player starts on position 0. He rolls a dye and moves forwards the number of steps shown
by the die face.
• Position 6 is a goose. If the player lands on it he moves forwards the same number of positions.
For example, if he was in 5 and the dye shows 1, he goes in 6 and repeats the move, i.e. he
goes in position 7.
• Position 4 is the death. The player falling there comes back to position 0.
• The goal is to reach position 9, exactly. If the number of moves is such as to exceed that
position, the surplus is counted backwards from position 9.
Represent the problem in terms of its transition matrix. In particular, while in general the
transition probabilities are 1/6 (due to the die), rows 4, 7 and 9 of the matrix has peculiar
characteristics. Which ones?
Exercise 3.4 Justify the following assertion: an independent and identically distributed se￾quence of random variables is a Markov chain.
Hint: write the transition matrix.
Exercise 3.5 Assume the following transition probability matrix for a Markov process de￾scribing weather, with possible states:
1 = clear sky
2 = cloud covered sky
3 = rain
Let P be:
P =


0.4 0.3 0.3
0.2 0.4 0.4
0.1 0.6 0.3


A meteorologist predicts for tomorrow, Friday, 40% rain probability and 60% clear sky. What
is the probability of rain on Saturday?106 Introduction to Stochastic Processes
Hint: consider the initial vector probability and refer to Section 3.3.
Exercise 3.6 Compute the eigenvectors and eigenvalues of the matrix P given by eqn (3.32).4
Poisson Processes
But if we want to know what to think about such irregular and extraordinary actions,
we might consider the view that is commonly taken of irregular events
that appear in the course of nature and in the operations of external objects.
All causes are not conjoined to their usual effects with the same uniformity.
David Hume, Enquiry Concerning Human Understanding
It was Sim´eon−Denis Poisson himself who introduced in the work Recherches sur
la probabilit´e des jugements en mati`ere criminelle et en mati`ere civile, pr´ec´ed´ees des
r`egles g´en´erales du calcul des probabilit´es, published in 1837, the distribution that later
will bear his name. His intent was to introduce the judiciary statistics into juridical
questions, so as to estimate the probability of an incorrect judgement, problems already
faced by Nicolas de Condorcet in 1785 and by Pierre Simon Laplace in 1825 (Laplace,
1825).
On pages 205-206 of his Recherches one can find the emergence of that probability
distribution, which is now called Poisson distribution. He also proved - as he called it
− the ‘loi g´en´erale des grands nombres’, extending the well-known theorem of Jacques
Bernoulli, and applied this law in a variety of examples ‘pris dans l’ordre physique et
dans l’ordre moral’.
As we shall see below, the Poisson distribution is used to compute how many times
a random event will occur in a time or space interval, when the probability of its hap￾pening is very small but the number of trials is very large. Historically, one of the most
famous applications of the Poisson distribution was made by Ladislaus Bortkewitsch
in Das Gesetz der Kleinen Zahlen (1898) (Law of Small Numbers), who estimated the
probability of a Prussian cavalryman being killed by a horse-kick (Preece et al., 1988).
Such accidents were very rare, but the number of ‘trials’ (observations) was very large,
recorded over 20 years (1875−1894), from 14 military corps: a paradigmatic scenario
for applying Poisson distribution.
Poisson processes are generally associated with the occurrence of some type of
events within a given period of time: customers arrivals, particle emissions, telephone
calls, bus arrival times, etc., but they also find essential applications in spatial statis￾tics, to analyse the distribution of points in plane or space: locations of beeches in a
certain area of a forest, galaxy distribution, heavy metal concentrations, etc. In the
Encyclopædia Britannica it is reported that, during World War II, British statistician
R. D. Clarke analysed the distribution of hits of V-1 and V-2 flying bombs in London,
finding a Poisson pattern.
In the next section the Poisson process will be regarded as a continuous-time
Markov chain on the discrete state space of non-negative integers and here in this108 Poisson Processes
section, the aim is to introduce some basic techniques to perform computer experi￾ments on stochastic processes. The Poisson process is chosen as a worked example.
After reviewing fundamental concepts, some strategies to build up Monte Carlo sim￾ulations are illustrated. Parameters estimates are ‘ensemble estimates’, since different
realizations of the same process are performed, through different sequences of random
numbers.
4.1 Counting process
Consider the stochastic process {X(t), t > 0}. The process is called a counting process
if X(t) stands for the total number of ‘events’ occurred (or ‘counted’) up to a particular
time t. By ‘up to time t’ we mean ‘before and at time t’. If the event occurs exactly at
time t, it is counted. This process is a continuous-time process in discrete state space
S = N. The number of customers entering a store from opening hours 8.00 up to 10.00
may be described by a counting process, but not the number of customers inside the
store at 10 o’clock. A counting process might model the number of births of an animal
species up to a certain day, or the number of pieces of music written by a composer
up to a certain year.
The counting process {X(t), t > 0} is defined on the basis of the following proper￾ties:
1. {X(t)} > 0.
2. {X(t)} is integer-valued for all t > 0.
3. if s < t then X(s) 6 X(t).
4. if s < t then the difference X(t)−X(s) counts the number of events that occurred
in the time interval (s, t].
Therefore, the differences X(t) − X(s) takes only non-negative integer values. In par￾ticular, if X(0) = 0, {X(t)} denotes the number of events counted in the time interval
[0, t]. Such processes are also called purely discontinuous processes. The occurrence of
each event is referred to also as an ‘arrival’, or an ‘emission’, for instance, of radioactive
particles from a substance.
The random variables X(t) − X(s) ∀t, s positive are named increments of the
process and are of major importance when they are independent and stationary. A
counting process has independent increments if the numbers of events in disjoint in￾tervals are independent. This means that the number of events, the realization of the
random variable X(t) − X(s), that occurred in any interval (s, t], is independent of
the number of events, realizations of the random variable X(v) − X(u), occurred in
any other disjoint interval (u, v], for instance in [0, s]. With this definition, the ran￾dom variables, describing the number of events in disjoint intervals, are independent
of each other. That is the probability of n events in (s, t] does not change even though
it is conditioned on information on events in other intervals different from (s, t]. This
independence property of increments has its counterpart in the independence of trials
in the discrete-time: the discrete values Bernoulli process.
Examples of independent increment processes are those describing the number of
customers entering a store, or the number of pieces written by a composer. Although
that assumption could appear reasonable also for the number of births, actually it isCounting process 109
not. Indeed, if the number of births is large enough, it is not plausible that the number
of future births does not depend on births in the past times.
A counting process has stationary increments (or homogeneous increments) if the
number of events in any interval [t, t + ∆t] only depends on the length ∆t of that
interval, and not on t, that is on its particular position in time. In other words, the
random variable ‘number of events’ in ∆t interval has the same distribution of the
random variable ‘number of events’ in t + ∆t interval. So, the stationarity property is
written as:
X(z + t + ∆t) − X(z + t) ∼ X(t + ∆t) − X(t)
or, with ∆t = t − s, t > s:
X(t + z) − X(s + z) ∼ X(t) − X(s), ∀z > 0, ∀t > s (4.1)
This means that events are ‘equally probable’ for any t.
The stationary increments property has its counterpart in the constancy of proba￾bility of success in the Bernoulli process. In the store example, the stationarity assump￾tion is not reasonable, since almost certainly there are periods of time with greater
access of customers. Also in the other examples, the increments are not stationary.
For the births, the stationary increments assumption would be reasonable if the pop￾ulation was constant. For the composer, it is probable that inspiration changes over
time. Lastly, note that if a process has stationary increments, that does not imply its
stationarity. An example is the Poisson process.
Further random variables are associated to X(t), the number of counted arrivals
in [0, t]: the instant of the n-th arrival and the time between each arrival and the next.
In the following, we usually refer to the occurrence of each event also as an ‘arrival’.
Let Tn be the random variable describing the n-th arrival, or also the time to wait the
n-th arrival, or the waiting time, so defined:
Tn = inf{t > 0, X(t) = n} (4.2)
where inf is the mathematical infimum. At time t at least n arrivals are counted if and
only if Tn 6 t. This means that the following events are equivalent:
{X(t) > n} and {Tn 6 t} (4.3)
It follows that a counting process may be defined on the basis of the waiting time:
{X(t)} = max{n : Tn 6 t}
For instance, in a store the event {at 11 o’clock there were at least 6 arrivals} (they
might have been also 7, 8, etc.), that is X(11) > 6, is equivalent to the event {the
waiting time of the 6-th arrival does not exceed 11 o’clock}, that is T6 6 11. The
following events are equivalent:
{X(t) > n} = {Tn+1 6 t} and {X(t) = n} = {Tn 6 t < Tn+1}
The time between any two consecutive arrivals, that is between the event (n − 1)-
th and the event n-th, (inter-arrival time), is described by the random variable An =110 Poisson Processes
5
4
3
Xt
2
1
0
0
a
1 a
2 a
3 a
4 a
5 a
6
10 20 30 t1
t2
t3
t4
t t 5
Fig. 4.1 Sample path of the counting process {X(t)}.
Tn − Tn−1. Figure 4.1 shows a sample path of the counting process {X(t), t > 0}. Let
us suppose that the first arrival occurs at the instant t1 = 6 (in some unit of time,
for instance seconds). There is no arrival up to the time t2 = 14. The third arrival
occurs after 10 further seconds, at t3 = 24, and so on. We have denoted with t1 a
realization of the variable T1, with t2 a realization of the variable T2, etc. Therefore,
the realizations of the random variable An are a1 = 6, a2 = 8, a3 = 10, etc. Figure 4.1
clearly shows the equivalence of the events reported in eqn (4.3). For instance, the
third arrival occurs at a time t 6 30 (before the time 30 or at the time 30) if and only
if the number of arrivals in [0, 30] is > 3. It may even be 4, but in any case > 3. A
small note: in this case the time has been discretized, supposing for recording arrivals,
for instance, with a timer accurate to one second.
4.2 Poisson process from counting process
The Poisson process is a counting process with stationary and independent increments,
defined in the following. The counting process {X(t)} is a Poisson process if:
a) X(0) = 0. The counting process begins at time t = 0.
b) The process has independent increments.
c) The probability of n arrivals in any interval of finite length ∆t is given by the
random variable, X(∆t) with distribution, named Poisson distribution:
P {X(∆t) = n} =
(λ∆t)
n
n!
e
−λ∆t
, n = 0, 1, . . . (4.4)Poisson process from Bernoulli process 111
The interval ∆t may be written also as (t − s), s < t or, for the independence of
increments, (t − 0) = t. So the above equation may be rewritten as:
P {X(t + s) − X(s) = n}
= P {X(t) = n} =
(λt)
n
n!
e
−λt, n = 0, 1, . . .
The state space is clearly S = {0, 1, 2, . . . }. In that counting process, X(t) − X(s) ≥
0, ∀t > s, all the trajectories of the Poisson process are non-decreasing in t. The
average number of arrivals in [0, t] is given by:
E [(X(t)] = X∞
1
n
(λt)
n
n!
e
−λt
(sum from 1, with 0 the term cancels out)
=
X∞
1
(λt)
n
(n − 1)! e
−λt = λt
It is clear that the Poisson process is not stationary in any sense.
The parameter λ, also called the intensity or rate of the process, represents the
number of arrivals per unit of time, that is the frequency, so it has the dimensions
of inverse time, i.e. s−1
. If λ is the frequency, then 1/λ represents the average time
between arrivals. Then, the larger λ the smaller the average waiting time is.
Condition c) also implies that the process has stationary increments. From eqn (4.1),
we have to prove that:
P {X(t + z) − X(s + z) = n} = P {X(t) − X(s) = n}
Now:
P {X(t + z) − X(s + z) = n} =
[λ(t − s)]n
n!
exp[−λ(t − s)]
= P {X(t) − X(s) = n}
Therefore the process is not stationary, but has stationary increments.
4.3 Poisson process from Bernoulli process
Let us start from different conditions and derive that the number of arrivals in a time
interval ∆t has a Poisson distribution.
The counting process {X(t), t > 0} is a Poisson process with intensity λ (λ > 0) if:
a) X(0) = 0.
b) The following conditions hold:
P {X(t + dt) − X(t) = 0} = 1 − λdt + o(dt)
P {X(t + dt) − X(t) = 1} = λdt + o(dt)
P {X(t + dt) − X(t) > 2} = o(dt)112 Poisson Processes
c) The process has independent and stationary increments.
The notation o(dt) (the Landau symbol “little-o”) means limdt→0 o(dt) = 0, that is,
per dt → 0, o(dt) goes to zero much faster than dt.
The first and second conditions in b) mean that in any infinitesimal interval dt
either no event or only one event occurs. The probability of occurrence of such a
unique event is given by the frequency multiplied by the interval length. The third
condition in b) means that the probability of occurrence of more than one event in
any infinitesimal interval dt is negligible with respect P {X(dt) = 1}.
The random variable X(∆t), the number of arrivals in an interval ∆t of finite
length, can be demonstrated to follow the Poisson distribution by showing that it is
an approximation to the binomial distribution:
Bin(n, N, p) = N!
n!(N − n)! p
n
(1 − p)
N−n
in the limits N → ∞, p → 0, and if E [Bin] tends to a finite constant N p. In the above
equation, n is the number of successes n in N independent trials, each of which has
the same success probability p. Imagine dividing the interval ∆t in N infinitesimal
subintervals of length dt, such that N = ∆t/dt  1, with ∆t a finite quantity (see
Fig. 4.2). Furthermore, by virtue of the above conditions in a tiny interval dt at most
N = ∆t / dt
∆t
dt infinitesimal
arrivals
finite
Fig. 4.2 Poisson distribution derived from the binomial distribution. The finite interval ∆t
is divided in N infinitesimal subintervals of length dt.
one arrival may occur. That is like saying that in the time interval ∆t, N trials are
executed (arrivals observation) and in each subinterval dt the event either happens
(arrival) or it does not (no arrival). Let p = λdt be the success probability, the average
number of successes is given by:
N p = Nλdt, but N =
∆t
dt ,
then N p =
∆tλdt
dt = λ∆t finite quantity
Since dt is an infinitesimal interval, if λ is a finite quantity p is very small. Therefore
very few successes may occur in ∆t, that is n  N. In such conditions, we can write:Poisson process from Bernoulli process 113
N!
(N − n)! = N(N − 1)(N − 2). . . ,(N − n + 1) nN
≈ N
n
Consider, for example, N = 9 and n = 4:
9!
(9 − 4)! =
1 · 2 · 3 · 4 · 5 · 6 · 7 · 8 · 9
1 · 2 · 3 · 4 · 5
= 9 · 8 · 7 · 6 = 3024
which is not close to 94 = 6561, because n = 4 in not much smaller than N = 9. But
if we take N = 90 and still n = 4, we have:
90!
(90 − 4)! = 61 324 560 . . . , while 904 = 65 610 000
the two quantities differing by about 6%. Increasing N by a factor of 10 the discrepancy
becomes as small as 0.6%.
If we put:
y = (1 − p)
N−n
we have:
log y = (N − n) log(1 − p)
p1
−−−→ −p(N − n)
nN
−−−→ −N p
for instance, if p = 0.01, log(1 − 0.01) = −0.01005. With these approximations:
y = (1 − p)
N−n −→ e
−Np
then:
Bin(n, N, p) −−−−→ N→∞
p→0
Nn
n!
p
n
e
−Np =
(N p)
n
n!
e
−Np
and being N p = λ∆t:
=
(λ∆t)
n
n!
e
−λ∆t
Notice the substantial difference between a Bernoulli process and a Poisson process:
in the former, events are bound to occur in fixed time intervals, while in the latter
they occur in continuous time.
Up to now we have been faced with the number of events occurring in a finite time
interval ∆t. Let us now consider two more random variables: the waiting time Tn of
the n-th arrival eqn (4.2), and An = Tn − Tn−1, the time between the (n − 1)-th and
n-th events.
Let us focus on the latter. Take a certain instant t0 as the origin of the time scale,
by putting t0 = 0 and consider A1 = T1 − 0, elapsed time up to the first arrival. We114 Poisson Processes
ask ourselves: for every t > 0, what is the probability that the random variable T1
takes a value t1 > t, that is:
P {T1 > t} ≡ P {A1 > t}
Such a probability is the same as:
P {0 arrivals in [0, t]}
Then, for every t > 0 the event T1 > t is realized if and only if there are no events in
the interval [0, t] (including t). By putting n = 0 in eqn (4.4) and since ∆t = [0, t] = t,
it results in:
P {X(t) = 0} = P {A1 > t} = e
−λt (since A1 = T1)
The functional form of A1 is therefore a negative exponential. The repartition function
of A1 is given by:
FA1 = P {A1 ≤ t} = 1 − P {A1 > t} = 1 − e
−λt
which is the repartition function of the exponential random variable with parameter
λ, denoted as Exp(λ).
Remark 4.1 Recall that the continuous random variable X has exponential distribution Exp(λ)
if its probability density function is:
f(x) = (
λ e−λx
, 0 6 x < ∞
0 , else (4.5)
Its repartition function is:
F = P {Exp ≤ t} =
Z t
0
λ e−λy dy = −λ
1
λ
e
−λt



t
0
= 1 − e
−λt
To derive the expected value E [Exp(λ)] = 1/λ and the variance Var [Exp(λ)] = 1/λ2
, we shall
make use of the ‘moment-generating function’ of the random variable X given by:
MX(y) = Eh
e
yXi
=



X
x∈SX
e
yx
p(x) X discrete
Z ∞
−∞
e
yxf(x) dx X continuous
Note that MX(y) is a function of the numerical variable y ∈ R, since the random variable X is
fixed and it is defined for the values of the y’s for which the expected value of exp(yX) exists. If
MX(y) is differentiated n times in the origin, the n-th moment of X is found, which is:
M
(n)
X (0) = E [X
n
] , n = 1, 2, . . .
Then:
M0
X(y) = d
dy E
h
e
yXi
= E
d
dy e
yX
= Eh
X eyXi
and for y = 0 it is M0
X(0) = E [X].Poisson process from Bernoulli process 115
Similarly:
M00
X(0) = d
2
dy2
E
h
e
yXi
= E
d
2
dy2
e
yX
= Eh
X
2
e
yXi
and for y = 0 it is M00
X(y) = E
X
2

. Turning to the random variable Exp, it results in:
MExp(y) = Eh
e
yExp(λ)
i
=
Z ∞
0
e
yx λ e−λxdx
=
Z ∞
0
e
−(λ−y)x
dx =
λ
λ − y
, y < λ
from which:
E [Exp(λ)] = M0
Exp(0) = d
dy
λ
λ − y



y=0
=
λ
(λ − y)
2



y=0
=
1
λ
and:
Var [Exp(λ)] = E
Exp(λ)
2

−
￾
E [Exp(λ)]2 =
1
λ2
since it is the second moment:
E

Exp(λ)
2

= M00
Exp(0) = 2λ (λ − y)
(λ − y)
4



y=0
=
2
λ2
.
We ask now the probability that the random variable A takes a certain value
t2 > t1, given that A1 = t1, that is:
P

0 arrivals in (t1, t1 + t]

A1 = t1
	
To get the functional form of A2 = T2 − T1, the elapsed time between the event 1 and
the event 2, the conditional probability given A1 = t1 is required:
P

A2 > t

 A1 = t1
	
= P

0 arrivals in (t1, t1 + t]

 A1 = t1
	
Now the events {A1 = t1} and {0 arrivals in (t1, t1 + t]} are independent for the inde￾pendence property of increments, hence:
P

A2 > t

 A1 = t1
	
= P {0 arrivals in (t1, t1 + t]}
= e
−λt
, for the stationary property of increments
By induction, we can conclude that the random variables A1, A2, . . . are independent
and identically distributed, according to an exponential function, with mean
1
λ
average time between the (n−1)-th and the n-th event. The independence property of
increments ensures that what takes place from some time onwards is independent o116 Poisson Processes
what happened before. Moreover, due to the stationarity property of increments, any
time instant t = t
∗ behaves as t = 0, i.e. the initial time does not affect the statistical
properties of the process. In other words, the process is memoryless, as we shall see in
detail in the following.
Let us turn to the random variable Tn, waiting time of the n-th arrival, that can
be also written as:
Tn =
Xn
i=1
Ai
, n > 1 (4.6)
In Fig. 4.1, the realizations t1, t2, t3, . . . of the random variables T1, T2, T3, . . . are
given by 6 (a1 = 6), 14 (a1 + a2 = 6 + 8), 24 (a1 + a2 + a3 = 6 + 8 + 10), . . . . So the
random variable Tn appears as the sum of the independent exponential function, all
with mean 1/λ, and it has a gamma distribution with parameters (n, λ), n, λ > 0,
denoted as Gam(n, λ). Its probability density function is given by:
fGam(t) = λ e−λt (λt)
n−1
(n − 1)!, t > 1 (4.7)
Remark 4.2 The distribution (4.7) is actually a special case of the gamma distribution, at￾tributed to A.K. Erlang and consequently often denoted as an ‘Erlang’ distribution. The generalized
gamma distribution involves a real parameter α, instead of the integer n, and its probability density
is:
fGamma(t) = λ
α
Γ(α)
t
α−1
e
−λt
where Γ(x) is the Euler gamma function, generalizing the factorial to real values:
Γ(x) = Z ∞
0
ξ
x−1
e
−ξdξ
If x = k, integer: Γ(k) = (k − 1)!
To derive such a result, note that if n = 1, the first arrival takes place at the time
t if and only if there are no events in [0, t) (excluding t). Since for n = 1 is T1 = A1
(elapsed time up to the first arrival), for t > 0 we have:
P {T1 6 t} = 1 − P {A1 > t} = 1 − e
−λt
= F(t) = Z t
0
λ e−λy dy
(4.8)
Therefore the random variable T1 is the exponential random variable Exp(λ). We
already know it, since T1 = A1, which here represents the waiting time before the oc￾currence of the first arrival, where λ, it was said, is the average frequency of occurrence
and 1/λ the average time between one event and the next one.Poisson process from Bernoulli process 117
To recover eqn (4.7), the result obtained for the waiting time of the first arrival is
extended to the waiting time of the n-th arrival. The n-th arrival occurs at an instant
6 t, if and only if the number of arrivals in the interval [0, t] is > n.
As previously described, see eqn (4.3), the events: {n-th arrival before of, or at,
time t} and {number of arrivals up to t > n} are equivalent. Therefore the repartition
function of the random variable Tn is given by:
FTn
(t) = P {Tn 6 t} = P {Poiss(t) > n} =
X∞
i=n
(λt)
i
i!
e
−λt (4.9)
where Poiss denotes the random variable with Poisson distribution.
Given the repartition function of the random variable Tn, the probability density
function fTn
(t) is obtained by differentiating with respect to t:
fTn
(t) = −
X∞
i=n
λ
(λt)
i
i!
e
−λt +
X∞
i=n
λ
(λt)
i−1
(i − 1)! e
−λt
= λ e−λt "
−
X∞
i=n
(λt)
i
i!
+
X∞
i=n
(λt)
i−1
(i − 1)!#
= λ e−λt "X∞
i=n
(λt)
i−1
(i − λt)
i!
#
= λ e−λt (λt)
n−1
(n − 1)! = e
−λt λ
n
(n − 1)! t
n−1
that is the density fGam of Gam(n, λ) in eqn (4.7).
Remark 4.3 A small clarification. We said that the random variable Tn, waiting time, has
a gamma distribution with probability density function given by eqn (4.7). This equation is a
particular case, of a more general gamma distribution function:
f(x) = λ
rx
r−1
e
−λx
Γ(x)
, x > 1, λ, r > 0 (4.10)
where λ is the scale parameter and r the shape parameter, and:
Γ(x) = Z ∞
0
x
r−1
e
−x
dx, n > 0
The following inequalities are notable:
Γ(1) = 0! = 1
Γ(r + 1) = rΓ(r)
Γ(r + 1) = r!
Γ(1/2) = √
π
The mean is µ = r/λ and the variance is σ
2 = r/λ2
. If r is a positive integer = 1, 2, . . . , then
eqn (4.10) becomes eqn (4.7) that, in the case, is also named the Erlang distribution.118 Poisson Processes
We claimed before that the Poisson process is memoryless. Let us clarify such a
concept. First of all, let us also see how, and in what sense, the exponential random
variable Exp(λ) is memoryless. Consider a random variable X and let {X > t} be the
event {an arrival event occurs at a time greater than t}, equivalent to {no arrivals
occurred up to the time t}, and let {X > t + ∆t} be the event {an arrival occurs at a
time greater than ∆t, starting from t}, equivalent to {no arrivals occurred up to the
time t + ∆t}. We say that the random variable X is memoryless if:
P {X > t + ∆t|X > t} = P {X > ∆t} (4.11)
Described differently, the probability of an arrival at a time t + ∆t is always the
same, regardless of t, i.e. regardless of the time considered as origin. Let us consider the
negative exponential random variable. Recall the definition of conditional probability:
given events A and B, it holds P {A
T
B} = P {A|B} P {B}. Hence, it is:
P {X > t + ∆t|X > t} =
P

{X > t + ∆t}
T
{X > t}
	
P {X > t}
=
P {X > t + ∆t}
P {X > t}
since {X > t + ∆t} ⊂ {X > t}. If X ∼ Exp(λ) it results in:
=
exp[−λ(t + ∆t)]
exp[−λt]
= exp[−λ∆t] = P {X > ∆t}
It can be proved that the negative exponential random variable is the only continuous
random variable with such lack-of-memory property.
Let us look at the condition {X > t+∆t} ⊂ {X > t} by means of an example. Let
E1 be the event {a customer enters after 12 o’clock}, which means that the customer
entered from 12 to 19 o’clock (closing time). Let E2 be the event {a customer enters
after 12.30}. It is clear that if the event E2 occurred, also the event E1 previously
occurred. Obviously the converse is not true: just think of a customer entered at
12.10. Then, since E2 ⊂ E1, it is P {E1
T
E2} = P {E2}.
A very recurrent example concerns the life time of a device before failing. The event
‘failure’ is what we named ‘arrival’, so eqn (4.11) says that the probability that the
device fails after a certain time is the same as it is new, just out of the factory, even
though it has worked for some time t. The device, indeed, ‘does not remember’ that
it was working for some time.
This does not mean that the exponential function is always a good choice for
modelling systems, electronic, mechanical, biological, etc., affected by aging. In such
situations the Weibull (Weib) random variable is exploited. Its probability density
function is:
fWeib (t) = atb−1
exp(−atb
/b), t > 0, aandb parameters > 0
and its cumulative distribution function is:Poisson process from Bernoulli process 119
FWeib (t) = Z t
0
fWeib (t
0
) dt0 = 1 − e
−(a/b)t
b
If a = b = 1, we come back to the exponential random function, while if, for instance,
a = b = 2, it is:
fWeib (t) = 2t e−t
2
hence:
P {X > t + ∆t|X > t} = exp[−(∆t)
2 − 2t∆t]
that is the probability of no events (failures) up to t + ∆t (life expectancy) decreases
with increasing t.
Resorting to the Poisson process, a further important property concerns the precise
instant at which an event occurs. We know that in the interval (0, t] an arrival took
place. We ask ourselves: what is the probability that the arrival occurred at the instant
s ∈ (0, t]? The probability of the first arrival, given an arrival in (0, t], is:
P {A1 6 s|X(t) = 1} =
P

{A1 6 s}
T
{X(t) = 1}
	
P {X(t) = 1}
=
P

{X(t) − X(s) = 0}
T
{X(s) = 1}
	
P {X(t) = 1}
Because of the independence property of increments, the events in the intersection are
independent. But if A and B are stochastically independent, we have P {A
T
B} =
P {A} P {B}. Now the probability of the first event in the intersection is given by:
P {X(t) − X(s) = 0} =
[λ(t − s)]0
0! e
−[λ(t−s)] = exp[−λ(t − s)]
while for the second event in the intersection, we have:
P {X(s) = 1} =
(λs)
1
1! e
−λs = λs exp(−λs)
and similarly for P {X(t) = 1} at the denominator. Therefore:
P {A1 6 s|X(t) = 1} =
exp[−λ(t − s)] λs exp(−λs)
λt exp(−λt)
=
s
t
that is the repartition function of the random variable U (0, t), uniformly distributed
in the interval [0, t].
Indeed, if U (a, b) is the uniform random variable defined in the interval [a, b], the
probability that a point x is inside the interval [a1, b1] ⊂ [a, b], is proportional to its
width:
P {a1 6 X 6 b1} =
Z a1
b1
p(x)dx =
Z a1
b1
1
b − a
dx =
b1 − a1
b − a
in our case, [a1, b1] = [0, s] and [a, b] = [0, t].120 Poisson Processes
So, we have seen that, given an arrival in the [0, t], this arrival may occur with
uniform probability at any point of the interval. It follows, if we know that n arrivals
occurred in a certain interval [0, t], the arrival times T1, . . . , Tn form an ordered n size
sample, extracted from the random variable U (0, t).
We said that, if the random variables An, describing the time between two consec￾utive arrivals, have exponential distribution, then there may not be memory between
such events. However, stationarity and independence properties entail the memory￾lessness of the process and, therefore, random variables modelling inter-arrival times
can only be of the exponential type. These considerations allow for definition of the
Poisson process in a further way, through the random variables An.
4.4 Poisson process through the inter-arrival time
The counting process {X(t), t > 0} is a Poisson process with intensity λ if:
a) X(0) = 0.
b) The random variables An describing the time between two consecutive arrivals are
independent and identically distributed exponential random variables with mean
1/λ.
The reasoning is the following. Since An i.i.d ∼ Exp(λ), we have Tn =
Pn
i=1 Ai ∼
Gam(n, λ) and its probability density function is given by eqn (4.7). The repartition
function of the random variable Tn, that is P {Tn 6 t} which can be written as:
FTn
(t) = P {Tn 6 t} = 1 −
nX−1
i=0
(λt)
i
i!
e
−λt
Now, for n = 1, 2, . . . , the event {X(t) = n} occurs if the process at time t is in state
n, that is the following events are equivalent (counting from n = 1):
{X(t) = n} = {A1 + · · · + An 6 t < A1 + · · · + An + An+1}
= {Tn 6 t < Tn+1}
For instance, with reference to Fig. 4.1, the number of arrivals at time 30 is equal to
4, if the fourth arrival occurs before or at time 30, and the fifth arrival occurs after
time 30. Indeed, T4 = 29 and T5 = 33. Similarly, by summing the Ai
: 6 + 8 + 10 + 5 ≤
30 < 6 + 8 + 10 + 5 + 4. Therefore:
P {X(t) = n} = P {Tn 6 t} − P {Tn+1 6 t} =
(λt)
n
n!
e
−λt
which is a Poisson distribution with intensity λt. For n = 0:
P {X(t) = 0} = P {T1 > t} = e
−λt
i.e. the probability of not having any arrival exponentially decreases with time. To
summarize, we have seen three random variables playing a major role in the Poisson
process:Poisson processes simulations 121
• Poisson random variable: it describes the number of events in a certain interval.
• exponential random variable: it describes the time between two consecutive ar￾rivals.
• random variable gamma (Erlang): it describes the waiting time of the n-th event.
4.5 Poisson processes simulations
Let us begin with a simple example. Suppose that, in the morning, usually 10 customers
per hour arrive on average in a pub. The average time between two arrivals is given
by
1
λ
=
3600
10
= 360 s = 6 minutes
and the average frequency is λ = 2.8 × 10−3
s
−1
. This does not mean that a customer
arrives every about 6 minutes (i.e. the first customer after 6 minutes, the following after
12 minutes, etc.) We could think that customers arrive at times ‘centered’ on multiples
of 6 minutes, like 6 ± 1, 12 ± 1, . . . , or 6.0 ± 1.5, 12.0 ± 1.5, . . . or, in other words, we
could think to Gaussian functions centred at 6, 12, 18, . . . . But this is not the correct
picture: the shape of the density function eqn (4.5) of the exponential random variable
Exp(λ) says that events occurring closer in time are more likely than relatively long
waits between successive events. Don’t forget also that the random variable Exp(λ) is
memoryless, therefore there is no correlation between arrivals. Indeed, we derived it
under this hypothesis of no correlation. Perhaps the barman believes that those few
customers agreed with each other to have a drink together, but we know he is wrong.
It is just the absence of correlation between arrivals that could lead to the existence
of temporal correlation.
Someone might recognize from the above the ‘scientific demonstration’ of the pop￾ular saying (vox populi, vox Dei) ‘bad luck never comes alone’. Misfortunes, as strokes
of luck, are rare events and, often, are not related to each other. We expect from theory
that occurrences of rare events in close times are more likely than relative long waits.
Remark 4.4 The demonstration of the last statement is not simple, but an elementary simu￾lation of a Poisson process should convince the reader about its truth. Consider the following R
code, simulating a Poisson process with λ = 1/10.
t <- 300
lambda<-1/10
set.seed(2)
N<-rpois(1,lambda*t)
set.seed(4)
unifs<-runif(N,0,t)
arrivals<-sort(unifs)
plot(arrivals,rep(1,length(arrivals)),yaxt="n",ylab="",xlab="arrival times")
The result is shown in Figure 4.3. We observe that inter-arrival times have an exponential distri￾bution e
−λt, whose maximum is at t = 0. Therefore, short inter-arrival times are rather frequent
and, as a consequence, arrival times tend to cluster. This phenomenon is also known as Poisson
clumping or Poisson burst.
The quantification of the Poisson burst phenomenon requires us to compute the probability
that a large time interval of length T contains a smaller interval (a ‘window’) Wn of length w in
which n or more arrivals occur. This probability can be computed by means of the so-called Alm’s
approximation (Glaz and Balakrishnan, 1999)122 Poisson Processes
0 50 100 150 200 250 300
arrival times
Fig. 4.3 Arrival times in a Poisson process.
P(Wn ⊂ T) = 1 − Pn−1(λw)exp 
−(1 −
λw
n
)λ(T − w)pn−1(λw)

(4.12)
where
pk(λw) = e
−λw(λw)
k
k!
is the Poisson probability of having k arrivals in time w, if λ is the arrival rate, and
Pn(λw) = Xn
k=0
pk(λw)
An example, borrowed from (Tijms, 2003), will help in verifying the above formula. The
average annual number of people hit and killed by a tram in Amsterdam is about 3.7. If you
read in a newspaper that seven people have been killed by a tram in the first five months this
year, you may ask yourself if such a “cluster” of events is exceptional and in contrast with the
known average. While it is clearly exceptional that seven people will be killed exactly in those five
months, it is not exceptional at all to find a cluster of seven (or even more) fatal accidents in a
window of five months in a long period. Indeed, the probability of having seven or more accidents
in t = 5 months, given a monthly rate λ = 3.7/12 is:
P r(k ≥ 7) = 1 − P r(k < 7) = 1 −
X6
k=0
e
−λt(λt)
k
k!
which gives P r(k ≥ 7) = 0.001. Instead, the probability of seven or more accidents in any
period of five months in ten years, computed by means of the Alm’s formula (4.12) is 0.105, i.e.
100 times greater! A rare event, like seven accidents in five months, becomes increasingly more
probable when increasing the total observation time. With the above numbers, that event would
become almost certain in a period of 300 years (that is of course unrealistic in a rapidly changing
world). The following code implements the above computations.
# rate
lam <- 3.7/12
# window
w <- 5
# number of accidentsPoisson processes simulations 123
n <- 7
# Probability of k<=6
P <- 0
for (i in 0:(n-1)){
P <- P + exp(-lam*w)*(lam*w)^i/factorial(i)
}
# Probability of 7 or more accidents
1-P
# total time 10 years
T <- 120
p <- exp(-lam*w)*(lam*w)^(n-1)/factorial(n-1)
prob <- 1 - P*exp(-(1-lam*w/n)*lam*(T-w)*p)
prob
Some examples follow of problems concerning the Poisson process.
Example 4.1 What is the probability of an arrival in the first minute, that is the
probability of 1 event in [0, 60] s?
It is given by:
P {0 6 T1 6 60} =
Z 60
0
λe−λt dt = 1 − e
−λt
=1 − e
−0.002860 = 0.1546 ≈ 15%
Example 4.2 What is the probability of an arrival after 20 minutes?
With time in seconds, it is:
P {T1 > 1200} =1 − P {0 6 T1 6 1200} = 1 − (1 − e
−λt)
= exp[−2.8 × 10−3 × 1200] = 0.0347 ≈ 3.5%
Example 4.3 How much time has the barman to wait on average for the arrival of
the third customer?
Compute the value t3 assumed by the random variable T3 distributed as Gam(3, 2.8×
10−3
). Hence E [T3] = 3/2.8 × 10−3 = 1071 s, which is about 18 minutes.
Not always can an analytical answer be given as in the examples above, for instance
if the Poisson process is non-homogeneous (see Section 4.6). The way forwards is
computer simulation.
The Monte Carlo method lends itself to computer experiments on stochastic pro￾cesses. Turning to the example of customers going to a pub, a code modelling their
random arrivals is discussed below.124 Poisson Processes
## Code_4_1.R
# distribution of: number of arrivals, time of arrivals (waiting time),
# inter arrival time
# the average time between two arrivals is 1/lambda = Deltat/events
# lambda is the events average frequency.
# The process is discretized. The time unit is the second (infinitesimal dt).
Deltat<-3600 # the process is followed for 1 hour (3600 sec)
nevents<-10 # average number of arrivals in 1 hour
nhistories<-1000 # number of simulated histories (sample size)
lambda<-nevents/Deltat
lambda
ev<-numeric() # vector of the number of arrivals in the l-th history
at<- numeric() # vector of the inter arrival times in the l-th history
intert<- numeric() # vector of the all inter-arrival times
t.arr<- numeric() # vector of the arrival times in the l-th history
# to save each history (arrival times):
matr.history.t<-matrix(,nhistories,Deltat)
# to save each history (inter arrival times):
matr.history<-matrix(,nhistories,Deltat)
lst<- numeric() # vector of the number arrival times in the l-th history
l_int<- numeric() # vector of the number inter arrival times in the l-th history
### 2 loops: most external loop on histories, inner loop on steps
set.seed(2) # reset random numbers
for(l in 1:nhistories) { # starting loop on histories
# at the start of each history:
t.arr<- 0
at<- 0
t1<- 0
i<- 0 # 'i' counts the seconds
ev[l]<-0
while(i <= Deltat) { # starting loop on steps
i<-i+1 # the following step
u<- runif(1) # random number in [0, 1]
# if u<=lambda, an arrival occurs
if(u<=lambda) { # starting if
# when an event occurs, the event and the occurrence time are saved
ev[l]<-ev[l]+1 # events in the l-th history are counted
t2<- i # the occurrence time is saved in t2
t.arr <- c(t.arr,t2) # all the arrivals time in the l-th history are saved
# in the vector inter-arrival time the time interval between
# the two occurrences is saved
at[ev[l]]<- t2-t1
t1<- t2 # t2 becomes t1
} # # ending 'if(u<=lambda)'
} # ending loop on steps
lst[l]<-length(t.arr) # No. arrival times in this l-th history
# save the number of arrivals in this l-th history:
matr.history.t[l,1:lst[l]] <- t.arr
l_int[l]<-length(at) # No. of inter-arrival times in this l-th history
# save the number of inter arrival times in this l-th history:
matr.history[l,1:l_int[l]] <- at
} ################ # ending loop on histories
# =============================== analysis of results =======================
#................. number of arrivals .............
# ev # uncomment to print all the arrivalsPoisson processes simulations 125
lbin<- 1
hist(ev,br=seq(0.5,24.5,by=lbin),main=" ",
freq=T,xlab="No. arrivals",ylab="counts",cex.lab=1.1,font.lab=3)
quant<-quantile(ev, c(0.025, 0.1587, 0.5, 0.8413, 0.975))
quant
abline(v=quant[2],lty=2,col="black",lwd=1)
abline(v=quant[4],lty=2,col="black",lwd=1)
mev<- mean(ev)
sdev<- sd(ev)
mev
sdev
minev<- min(ev)
maxev<- max(ev)
minev
maxev
#........................ arrival time ............................
# matr.history.t[l,1:lst[l]] to print the arrival times of l-th history.
# examples: histories 1, 2, 3 and last history
matr.history.t[1,1:lst[1]]
matr.history.t[2,1:lst[2]]
matr.history.t[3,1:lst[3]]
matr.history.t[nhistories,1:lst[nhistories]]
# in the plots: lst[l]-1, to ignore the initial 0
# type="s" for stair step look
par(mfrow=c(2,2))
plot(matr.history.t[1,1:lst[1]],0:(lst[1]-1),type="s",xlab="time (s)",
ylab="No. arrivals",cex.lab=1.,font.lab=3)
plot(matr.history.t[2,1:lst[2]],0:(lst[2]-1),type="s",xlab="time (s)",
ylab="No. arrivals",cex.lab=1.,font.lab=3)
plot(matr.history.t[3,1:lst[3]],0:(lst[3]-1),type="s",xlab="time (s)",
ylab="No. arrivals",cex.lab=1.,font.lab=3)
plot(matr.history.t[nhistories,1:lst[nhistories]],0:(lst[nhistories]-1),
type="s",xlab="time (s)",ylab="No. arrivals",cex.lab=1.,font.lab=3)
# the following lines for further analysis of arrival times
# all the arrival times in all the histories are saved in the vector t.arrT
t.arrT<- numeric()
# first history, after summing on the remaining histories:
t.arrT<- matr.history.t[1,1:(lst[1]-1) ]
for (l in 2:nhistories) {
t.arrT<- c(t.arrT,matr.history.t[l,1:(lst[l]-1) ])
}
# t.arrT # uncomment to print all the arrival times
t.arrT_c<- numeric()
t.arr_c<- t.arrT[t.arrT>1200] # arrival times > 20 minutes ...
# t.arr_c # uncomment to print
nt.arr_c<- length(t.arr_c)
nt.arr_c
t.arr_c<- t.arr_c[t.arr_c<1500] # ... but < 30 minutes
# t.arr_c # uncomment to print
nt.arr_c<- length(t.arr_c)
nt.arr_c
#........................ inter arrival time .................................
# matr.history[l,1:l_int[l]] to print the inter arrival times of l-th history.
# for instance l=3 it is:
matr.history[3,1:l_int[3]]126 Poisson Processes
# summing the inter arrival times of the l-th history (example l=3):
sum_int_sl<- sum(matr.history[3,1:l_int[3]])
sum_int_sl
# examples: histories 1, 2, 3 and last history
xmax<- 2000
ymax<- 6
lbin<-120
nclass<- seq(0,xmax,by=lbin) #vector of breakpoints
par(mfrow=c(2,2))
hist(matr.history[1,],breaks=nclass,ylim=c(0,ymax),xlab="int_arr (s)",
ylab="counts",main=" ",font.lab=3,cex.lab=1.)
hist(matr.history[2,],breaks=nclass,ylim=c(0,ymax),xlab="int_arr (s)",
ylab="counts",main=" ",font.lab=3,cex.lab=1.)
hist(matr.history[3,],breaks=nclass,ylim=c(0,ymax),xlab="int_arr (s)",
ylab="counts",main=" ",font.lab=3,cex.lab=1.)
hist(matr.history[nhistories,],breaks=nclass,ylim=c(0,ymax),xlab="int_arr (s)",
ylab="counts",main=" ",font.lab=3,cex.lab=1.)
# plot=F to have a list of breaks and counts (example l=3):
hist(matr.history[3,],breaks=nclass,plot=F)
# all the inter arrival times in all the histories are saved
# first history, after summing on the remaining histories
int_arr<- matr.history[1,1:l_int[1] ]
for (l in 2:nhistories) {
int_arr<- c(int_arr,matr.history[l,1:l_int[l] ])
}
min.int<- min(int_arr)
min.int
max.int<- max(int_arr)
max.int
max.int.minutes<- max.int/60
max.int.minutes
mean.int = Deltat/nevents # (=1/lambda)
mean.int
n.int_arr<- length(int_arr)
n.int_arr
lbin.int<-120
hist(int_arr,freq = T,main=" ",xlab="int_arr (s)")
# to compare with analytical density
hist(int_arr,freq=F,main=" ",xlab="int_arr(s)",ylab="density",
cex.lab=1.2,br=seq(0,2640,by=lbin.int),font.lab=3,axes=F)
axis(2)
x.minutes<- seq(0,2400,by=480)
axis(1,int_arr,at=x.minutes,lab=x.minutes,srt=0)
x<-rexp(n.int_arr,lambda)
curve(dexp(x,lambda),add=T,col="black",0,2640,lty=2,lwd=2)
histog<-hist(int_arr,plot=F,br=seq(0,2640,by=lbin.int))
histog
lb<-lbin.int
y.rescale <- pretty(range(histog$density*n.int_arr*lb))
axis(4, at = y.rescale / n.int_arr/lb, lab = y.rescale, srt = 90)
mtext("No. arrivals", side=4, line=-2,cex=1.2,font=3)
The most intuitive idea is to choose a tiny interval dt as time unit. According to the
theory of Poisson stochastic processes, in dt only one event may occur with probability
λdt  1. In our case, we choose ∆t = 3600 s, that is 60 minutes, dt = 1 s, and suppose
we know the average number of arrivals to be equal to 10. Therefore, as seen above,
the events average frequency is λ = 2.8×10−3
s
−1
. It should be obvious that we speakPoisson processes simulations 127
of ‘second’, ‘minutes’, etc., but they are not real physical time, they are – as they are
called – ‘computer steps’ or, more specifically, ‘Monte Carlo steps’.
A loop from 1 to 3600 (∆t) is executed; at each step (in each dt) an arrival may or
may not occur; such an event is decided by a random number u uniformly distributed
in [0, 1]. If u < λ, a customer has arrived: the time of occurrence and the elapsed time
from the previous arrival are recorded. In any case, the loop goes one step further.
At the end of the loop, a computed time series (or history) is obtained. A cer￾tain number, for instance 1000, of such histories, all different from each other, are
constructed by means of different sequences of random numbers. In this manner, the
parameters of interest can be estimated. The code is divided into two parts. In the first
part, the two loops, on the time steps and on the histories, respectively, are performed.
In the second part, results produced by simulations are statistically analysed.
Figure 4.4 shows the distribution of the number of arrivals in an hour computed
on all the 1000 histories. Note freq=T in the command hist(ev, ..., to plot the
bin counts, or frequencies, of resulted values. With freq=F, probability densities are
plotted, so that the total area of the histogram is equal to 1. As we shall see in
the following, the density scale is used for comparison with a mathematical density
model. The dashed lines give the limits of the confidence level with coverage probabil￾ity = 68%. The command quant<-quantile(ev, c(0.025, 0.1587, 0.5, 0.8413,
0.975),type=2) estimates sample quantiles corresponding to chosen percentiles. In
the R help (?quantile) page, we can find nine methods for computing sample quantiles
from Hyndman and Fan (1996). Types 1, 2 and 3 refer to sample quantiles of discrete
distributions. The computed quantiles are: 4, 7, 10, 13, 17. The mean, mean(ev), and
the standard deviation, sd(ev), result in being, as expected, 10 and 3.25, respectively.
No. arrivals
counts
0 5 10 15 20 25
0 20 40 60 80 100 120
Fig. 4.4 Number of arrivals computed on 1000 histories. The dashed lines give the limits of
the confidence level with coverage probability = 68%.128 Poisson Processes
The printed counts and the figure describe, for instance, that in 122 out of 1000
histories, 10 arrivals occurred, but also that there are 5 histories in which only one
event took place, while the maximum number of events in a history is 23.
Figure 4.5 shows the number of arrivals as a function of time for four different
histories, the first, second, third histories and the last one.
0 1500 3500
0 2 4 6 8
time (s)
No. arrivals
No. arrivals
No. arrivals
No. arrivals
0 1000 2500
0 4 8 12
time (s)
0 1500 3500
0 2 4 6 8
time (s)
0 1500 3500
0 2 4 6 8
time (s)
Fig. 4.5 Number of arrivals as a function of time for four different histories.
The histograms report that the events up to the last one occurred just before the
end time (1 hour). The histograms are of the type seen before in Fig. 4.1, where the
realizations of the random variable X(t) are the ‘arrivals’.
The command matr.history.t[l,1:lst[l]] prints the arrival times of the l-th
history. For instance, with l = 3, the nine arrival times are:
matr.history.t[3,1:lst[3]]
0 136 1687 2239 2568 2589 2812 3000 3374 3447
The last arrival is at 3447 s, just before ∆t = 3600 s, and the next arrival would
be at 3704 s, if ∆t were increased to 3800 s. The following lines can be executed if
the number of arrivals within a certain time interval is required. The total number of
arrival times is saved in t.arrT, and, for instance, those in (1200, 1500) are selected.Poisson processes simulations 129
int_arr (s)
counts counts
counts counts
0 500 1500
0 2 4 6
int_arr (s)
0 500 1500
0 2 4 6
int_arr (s)
0 500 1500
0 2 4 6
int_arr (s)
0 500 1500
0 2 4 6
Fig. 4.6 Number of the time intervals between two consecutive arrivals as a function of their
length, for four different histories.
Figure 4.6 shows the number of the time intervals between two consecutive arrivals
as a function of their length. Four different histories, first, second, third and last
one, are plotted. The command matr.history[l,1:l_int[l]] prints the inter-arrival
times of the l-th history. For instance, with l = 3, the inter-arrival times are:
matr.history.t[3,1:l_int[3]]
136 1551 552 329 21 223 188 374 73
As an example, in the first bin between 0 and 120 (see the list $‘breaks‘ by
hist(matr.history[3,],breaks=nclass,plot=F) there are two inter-arrival times,
21 and 73. The inter-arrival time equal to 1551 is the time between the two arrivals
at 136 and 1687 (see above matr.history.t[3,1:lst[3]]).
In the vector int_arr, all the inter-arrival times are saved. The shortest time
(min.int) results to be 1 s, while the longest (max.int) is 2539 s, a little bit more
than 42 minutes. Further information is found in Fig. 4.7, comparing the normalized
histogram with an exponential fitting. The printed counts and the figure show, for
instance, that in the first two minutes (120 s) there are 3140 inter arrival times, between
2 and 4 minutes they are 2079, etc.
As we discussed, simulations involve discretized events, while the exponential ran￾dom variable, describing the waiting times, is continuous. Figure 4.7 shows how well
an exponential model fits data derived by a discrete approximation. The reason for
such an excellent agreement is that in simulations the arrival time is ruled by the130 Poisson Processes
int_arr (s)
density
0.0000 0.0010 0.0020
0 480 960 1440 1920 2400
0 500 1500 2500
counts
Fig. 4.7 Number of inter-arrival times computed on 1000 histories. The y-axis on the left
refers to the normalized histogram for comparison with the probability density function of
the exponential random variable (dashed line); the y-axis on the right refers to the number
of inter-arrival times in each interval of 120 s, that is two minutes.
geometric discrete random variable (Geom), whose probability distribution function
(better: probability mass function) is analogous, both in its form and properties, to
the exponential random variable.
The probability mass function of the geometric random variable is given by:
p(k) = p (1 − p)
k−1
, k = 1, 2, . . . (4.13)
sometimes written as:
p(k) = p (1 − p)
k
, k = 0, 1, 2, . . .
which is essentially the same. In fact, in eqn (4.13), the random variable Geom
describes the ‘number k of independent trials needed to get one success’, each with
success probability p. In the second definition, the random variable, which we call
Geom0
, describes the number of independent trials before the first success, that is the
‘number of failures until the first occurrence of success’, therefore Geom0 = Geom − 1.
In the code Code_4_1.R above, the process evolves second after second and, at
each instant, an arrival may or may not take place. In the following code Code_4_2.R,
the step length is decided by the exponential distribution, and at each step an arrival
occurs.
## Code_4_2.R
# Poisson process simulation by exponential generation of arrival times
# lambda is known
Deltat<-3600 # the process is followed up to 1 hour (3600 sec)
lambda<-0.003
set.seed(2) # reset random numbersPoisson processes simulations 131
nhistories<-4 # number of simulated histories
# to save arrival times in each history:
matr.history.t<-matrix(,nhistories,Deltat)
lst<- numeric()
### 2 loops: most external loop on histories,
# inner loop generates arrival times
set.seed(2) # reset random numbers
for(l in 1:nhistories) { # starting loop on histories
# at the start of each history:
t.arr<- 0 # arrival times
t.exp<- 0 # exponential generated times
while(t.exp<Deltat) { # arrival times generation
# 1 random deviate from an exponential distribution is generated:
t.exp<- t.exp + rexp(1,lambda)
# the arrivals time in the l-th history are saved:
t.arr<- c(t.arr,t.exp)
# print(t.arr) # uncomment to see how events occur over time
} # ending while loop
lst[l]<-length(t.arr) # No. arrivals in this l-th history
# save the number of arrivals in this l-th history:
matr.history.t[l,1:lst[l]] <- t.arr
} # ending loop on histories
# matr.history.t[l,1:lst[l]] to print the arrival times of l-th history.
# the four histories
matr.history.t[1,1:lst[1]]
matr.history.t[2,1:lst[2]]
matr.history.t[3,1:lst[3]]
matr.history.t[nhistories,1:lst[nhistories]]
# in the plots: lst[l]-1, to ignore the initial 0
# type="s" for stair step look
ymax<- 12
par(mfrow=c(2,2))
plot(matr.history.t[1,1:lst[1]],0:(lst[1]-1),type="s",font.lab=3,
xlab="time (s)",ylab="No. arrivals",ylim=c(0,ymax),cex.lab=1.)
abline(v=Deltat,lty=3)
plot(matr.history.t[2,1:lst[2]],0:(lst[2]-1),type="s",font.lab=3,
xlab="time (s)",ylab="No. arrivals",ylim=c(0,ymax),cex.lab=1.)
abline(v=Deltat,lty=3)
plot(matr.history.t[3,1:lst[3]],0:(lst[3]-1),type="s",font.lab=3,
xlab="time (s)",ylab="No. arrivals",ylim=c(0,ymax),cex.lab=1.)
abline(v=Deltat,lty=3)
plot(matr.history.t[nhistories,1:lst[nhistories]],0:(lst[nhistories]-1),
type="s",xlab="time (s)",ylab="No. arrivals",ylim=c(0,ymax),
font.lab=3,cex.lab=1.)
abline(v=Deltat,lty=3)
The code simulates only four histories to stress the difference in their construction
with respect to the code Code_4_1.R. The R function rexp(n,lambda) generates n
random deviates from an exponential distribution with rate parameter λ. As in the code
Code_4_1.R, the command matr.history.t[l,1:lst[l]] prints the arrival times of
history l-th. Arrival times are also called above waiting times. For instance, with l = 2,
the arrival times are (with two decimal places):
matr.history.t[2,1:lst[2]]
0.0000 224.59 754.69 1115.44 1380.97 1862.73 3360.04 3927.85
Note that now the last arrival is after ∆t = 3600, since the loop stops when the
condition t.exp<Deltat results FALSE.132 Poisson Processes
Figure 4.8 shows the number of arrivals as a function of time for the four simulated
histories, as in Fig. 4.5. Once the trajectories generation is executed, the number of
histories can be increased and results produced by simulations can be statistically
analysed with the instructions of code Code_4_1.R.
0 1000 3000
0 4 8 12
time (s)
No. arrivals No. arrivals
No. arrivals No. arrivals
0 1000 3000
0 4 8 12
time (s)
0 1000 3000
0 4 8 12
time (s)
0 1000 3000
0 4 8 12
time (s)
Fig. 4.8 Number of arrivals as a function of time for four histories. The dotted line is at
∆t = 3600 s
We recall that random deviates can also be generated from an exponential dis￾tribution by means of a well-known algorithm that exploits random deviates from a
uniform distribution. The R function runif(n,a,b) generates n random deviates from
a uniform distribution in [a, b], if a and b are omitted the default is [0, 1]. So, the com￾mand t.exp<- t.exp + rexp(1,lambda) may be replaced by the command t.exp<-
t.exp-1/lambda*log(runif(1)).
There exists a more elegant (efficient) way to generate the arrival times, by using
vectorized operations of R. We have said that the random variable Tn, waiting time of
the n-th arrival, may be written as the sum of independent exponential function, all
with mean 1/λ: Tn = A1, A2, . . . , An. The code Code_4_3.R exploits this property.Poisson processes simulations 133
## Code_4_3.R
# Poisson process simulation by exponential vectorized generation of arrival times
# lambda is known
Deltat<-3600 # the process is followed up to 1 hour (3600 sec)
lambda<-0.003
set.seed(2) # reset random numbers
nhistories<-4 # number of simulated histories
# to save arrival times in each history
matr.history.t<-matrix(,nhistories,Deltat)
t.max<- 50 # alleged max number of arrivals
lst<- numeric()
t.exp<- numeric()
t.arr<- numeric()
### 1 loop on histories
set.seed(2) # reset random numbers
for(l in 1:nhistories) { # starting loop on histories
# at the start of each history:
t.exp<- rexp(t.max,lambda) # generation of t.max arrival times
# print(t.exp) # uncomment to print the generated arrival times
t.arr<-c(0,cumsum(t.exp)) # since T1=A1, T2=A1+A2,... Tk=A1+A2+...+Ak
length(t.arr)<- max(which(t.arr<Deltat)) + 1
# +1 if desired the arrival immediately after Deltat
lst[l]<- length(t.arr) # No. arrivals in this history l-th
# save the number of arrivals in this l-th history
matr.history.t[l,1:lst[l]] <- t.arr
} # ending loop on histories
# matr.history.t[l,1:lst[l]] to print the arrival times of l-th history.
# examples: histories 1, 2, 3 and last history
matr.history.t[1,1:lst[1]]
matr.history.t[2,1:lst[2]]
matr.history.t[3,1:lst[3]]
matr.history.t[nhistories,1:lst[nhistories]]
# in the plots: lst[l]-1, to ignore the initial 0
# type="s" for stair step look
ymax<- 12
xmax<- 4000
par(mfrow=c(2,2))
plot(matr.history.t[1,1:lst[1]],0:(lst[1]-1),type="s",font.lab=3,
xlab="time (s)",ylab="No. arrivals",ylim=c(0,ymax),
xlim=c(0,xmax),cex.lab=1.)
abline(v=Deltat,lty=3)
plot(matr.history.t[2,1:lst[2]],0:(lst[2]-1),type="s",font.lab=3,
xlab="time (s)",ylab="No. arrivals",ylim=c(0,ymax),
xlim=c(0,xmax),cex.lab=1.)
abline(v=Deltat,lty=3)
plot(matr.history.t[3,1:lst[3]],0:(lst[3]-1),type="s",font.lab=3,
xlab="time (s)",ylab="No. arrivals",ylim=c(0,ymax),
xlim=c(0,xmax),cex.lab=1.)
abline(v=Deltat,lty=3)
plot(matr.history.t[nhistories,1:lst[nhistories]],0:(lst[nhistories]-1),
type="s",xlab="time (s)",ylab="No. arrivals",ylim=c(0,ymax),font.lab=3,
cex.lab=1.,xlim=c(0,xmax))
abline(v=Deltat,lty=3)
The loop while(t.exp<Deltat) is replaced by the unique command t.exp<-
rexp(t.max,lambda) to generate t.max deviates from an exponential distribution
with parameter λ. The number of deviates t.max is quite arbitrary, but it must be not
less than the presumed number of events in the considered ∆t interval. It is advisable134 Poisson Processes
to choose it with a bit of generosity, but taking into account that the average number
of arrivals in [0, ∆t] is λ∆t. The cumsum function returns the cumulative sum, for in￾stance if x<- c(1,2,3,4), the command cumsum(x) returns 1 3 6 10. So in the code,
the first t.exp generated values are (with two decimal places): 621.78 134.91 48.88
576.90 . . . , the first waiting times are: 0.00 621.78 756.70 805.58 1382.48 . . . .
Further on the code is the same as Code_4_1.R and Code_4_2.R.
We have seen that, given an arrival in the [0, t], this arrival may occur with uniform
probability in any point of the interval. If I know that n arrivals occurred in a certain
interval [0, t], it follows that the arrival times T1, . . . , Tn form an ordered n size sample,
extracted from the random variable U (0, t). The code Code_4_4.R (only the initial part
is reported below) generates arrival times by implementing the above Poisson process
property.
## Code_4_4.R
# If I know that N arrivals occurred in a certain interval
# [0,Deltat], the arrival times T_1, ..., T_N
# form an ordered N size sample, extract from
# the random variable U[0,Deltat].
# rpois(n,lambda) returns n random numbers from the Poisson distribution
# the number N of points on an interval [0,Deltat] is Poisson distributed
# with mean lambda*Deltat
Deltat<-3600 # the process is followed for 1 hour (3600 sec)
nevents<-10 # average number of arrivals in 1 hour
nhistories<-1000 # number of simulated histories (sample size)
lambda<-nevents/Deltat
lambda
ev<- numeric() # vector of the number of arrivals in the l-th history
lst<-numeric() # vector of the number of arrival times in the l-th history
# to save arrival times in each history:
matr.history.t<-matrix(,nhistories,Deltat)
### 1 loop on histories
set.seed(2) # reset random numbers
for(l in 1:nhistories) { # starting loop on histories
# at the start of each history:
# N: a random number generated from the Poisson distribution with mean nevents
N<- rpois(1,nevents)
# print(N) # uncomment to print N
Un<- runif(N,0,Deltat) # NOT ORDERED arrival times from U[0,3600]
# print(Un) # uncomment to see N not ordered arrivals
t.arr<- sort(Un) # arrival times (Un ordered) in the l-th history
# print(t.arr) # uncomment to see how events occur over time
ev[l]<- max(which(t.arr<= Deltat)) # to check if arrivals are in [0,Deltat]
lst[l]<-length(t.arr) # No. arrival times in this l-th history
# save the number of arrival times in this l-th history:
matr.history.t[l,1:lst[l]] <- t.arr
} # ending loop on histories
#................. number of arrivals .............
############ to follow as Code_4_1:R ############
The R function rpois(n,lambda) is used to generate n numbers from the Poisson
distribution with mean λ. In the code Code_4_4.R, the command N<- rpois(1,nevents)
gives how many Poisson observations, with mean nevents equal to 10, are generated
in the [0, ∆t] interval, for instance, for the first history, N = 7. The command Un<-Nonhomogeneous Poisson process 135
runif(N,0,Deltat) generates the following seven uniform random numbers in [0, ∆t]
(with two decimal places):
604.98 3397.82 3396.50 464.97 3000.41 1684.86 1979.94
and the command sort(Un) orders them, giving the t_arr arrival times in the first
history:
464.97 604.98 1684.86 1979.94 3000.41 3396.50 3397.82
Once the trajectories generation is executed, the code follows the instructions of the
code Code_4_1.R.
4.5.1 Merging of independent Poisson processes
Let {N1(t), t ≥ 0} and {N2(t), t ≥ 0} be two independent Poisson processes having
rates λ1 and λ2, respectively. Suppose N1 = (5, 12, 25, ...) and N2 = (7, 26, 30, ...) are
“arrivals”. The process defined by
N(t) = N1(t) + N2(t) = (5, 7, 12, 25, 26, 30, ...)
obtained combining the arrival times in N1 and N2 is a Poisson process too, with
a rate λ = λ1 + λ2. The demonstration is left as an exercise.
Passing from the arrival times N1(t) and N2(t) to the inter-arrival times X(t) of the
merged process (that is exponentially distributed), the inter-arrival time Xk between
the (k − 1)-th and k-th arrival is such that the probabilities of occurrence of an event
of type “1” (i.e. coming from N1) or “2” are:
P(“1”|Xk = t) = λ1
λ1 + λ2
P(“2”|Xk = t) = λ1
λ1 + λ2
and they are independent of the value t.
This result can be generalized to n independent processes. In particular, for the
minimum M of a set of independent exponential processes:
M = min(X1, X2, .....)
having rates λ1, λ2, ..., the following relations can be demonstrated (Dobrow, 2016) for
any time t > 0:
P(M > t) = e
−(λ1+λ2+...)t
i.e., the combined process has an exponential distribution, and for any k > 1:
P(M = Xk) = P
λk
i
λi
4.6 Nonhomogeneous Poisson process
We said that a counting process has stationary, or homogeneous increments, if the
number of arrivals in any interval [t, t + ∆t] depends only on the interval length ∆t,136 Poisson Processes
but not on t. In the non-homogeneous Poisson process, increments are not stationary,
without prejudice to the validity of their independence. This means that the arrivals
frequency varies along the time axis, so that λ is no more constant, but a function of
time λ(t).
The definition of a Poisson process from a counting process is rewritten as follows.
The counting process {X(t)} is a non-homogeneous Poisson process if:
a) X(0) = 0. The counting process begins at time t = 0.
b) The process has independent increments.
c) The probability of n arrivals in any interval of finite length [s, t] is given by:
P {X(t) − X(s) = n} =
1
n!
Z t
s
λ(y)dyn
exp 
−
Z t
s
λ(y)dy
Therefore, we see that the distribution of the number of arrivals explicitly depends on
both s and t, and not simply on the length of the interval (s, t]. Define:
m(s, t) = Z t
s
λ(y)dy
If s = 0, m(t) is called the mean value function of the process. Then the random
variable X(t) − X(s) has Poisson distribution with mean m(s, t), ∀0 6 s < t.
The definition of the Poisson process from the Bernoulli process is modified in a
quite analogous way. The counting process {X(t), t > 0} is a non-homogeneous Poisson
process with intensity λ(t) if:
a) X(0) = 0.
b) The following conditions hold:
P {X(t + dt) − X(t) = 0} = 1 − λ(t)dt + o(dt)
P {X(t + dt) − X(t) = 1} = λ(t)dt + o(dt)
P {X(t + dt) − X(t) > 2} = o(dt)
c) The process has independent increments.
We have seen that, for a homogeneous process, the waiting time of the first arrival
T1 ≡ A1 has distribution exp(−λt), that is:
P {X(t) = 0} = P {T1 > t} = e
−λt
If the process is non-homogeneous the above equation becomes:
P {X(t) = 0} = P {T1 > t} = exp 
−
Z t
0
λ(y)dy
The derivative with respect to t is the probability density function as a generalization
of eqn (4.7):
fT1
(t) = λ(t) exp 
−
Z t
0
λ(y)dy
= λ(t) e
−m(t)
It should be clear that the waiting times Tn are not independent random variables.Nonhomogeneous Poisson process 137
To simulate a non-homogeneous Poisson process, a technique often adopted is what
is called thinning. The idea is to start from a homogeneous Poisson process with
constant frequency λ. Suppose that an event occurs at time s. Such an event is not
always counted, but only with a certain probability p(s). Counted events form a Poisson
process with not constant frequency, but with frequency λ(s) = λp(s), where s =
1, 2, . . . are the counted events.
Basically, to simulate a non-homogeneous process, a command has to be added,
which allows to count or not the event. Let us suppose that events occur according to
a function λ(t) of t, such that λ(t) ≤ λ, ∀t ≥ 0. We want to deal with a homogeneous
process with intensity λ, and to generate the arrival times. If s is one of these instants,
we ask whether it has to be counted, even for the non-homogeneous process. The
answer is given by the usual random number u between 0 and 1: the event occurred
at time s for the Poisson homogeneous process with constant frequency λ is also an
event for the Poisson process with time dependent λ(t), if:
u ≤
λ(s)
λ
Naturally, it is necessary to know (or to conjecture) the functional form of λ(t).
As an example, let us consider the non-homogeneous Poisson process with λ(t) =
sin(t) (code Code 4_5, below). We choose λ = 1, so that the condition λ(t) ≤ λ always
holds. The process is followed for, say, 60 minutes, now the time unit is the minute.
## Code_4_5.R
# Non homogeneous Poisson process is a process with intensity lambda not constant,
# but depending on time: lambda(t).
# It is a not-stationary increments process,
# not depending only on the length interval, but also on its time position.
# The simulation is performed by sampling from a homogeneous process,
# arrivals occur according the time function. Here: lambda(t) = sin(t)
# lambda1 (homogeneous process) is known and lambda(t) <= lambda1, all t
Deltat<- 60 # minutes
lambda1<- 1.
set.seed(1) # reset random numbers
# vector of the number of accepted arrivals:
ev<- 0
# vector of the number of arrivals if it were a homogeneous process:
evt<- 0
# vector of values lambda(t)/lambda1:
evs<- 0
t<- 0
while (t<Deltat) { # arrival times generation
t<- t + rexp(1,lambda1)
evt<- c(evt,t) # arrival times for a homogeneous process
fl=sin(t)/lambda1 # lambda(t)/lambda1
evs<- c(evs,fl) # fl values, but not all are accepted
if (runif(1) < fl) ev<- c(ev,t) # <----- crucial point
} # ending loop while
length(ev)
length(evt)
plot(evt,0:(length(evt)-1),type="s",ylim=c(-4,length(evt)),xlab="time (min)",
ylab="No. arrivals",lty=3,col="black",cex.lab=1.1,font.lab=3,lwd=2)
curve(sin(x),0,Deltat,add=T,lty=2,lwd=1.6)
lines(ev,0:(length(ev)-1),type="s",lwd=2.2,col="black")138 Poisson Processes
The result is displayed in Fig. 4.9, showing the histories of both non-homogeneous
(continuous line) and homogeneous (dotted line) process. At the bottom, the function
sin(t) (dashed line) is also reported.
0 10 20 30 40 50 60
0 10 20 30 40 50 60
time (min)
No. arrivals
Fig. 4.9 Number of arrivals as a function of time for a non-homogeneous Poisson process
(continuous line), with intensity λ(t) = sin(t) (dashed line, at the bottom). The dotted line
refers to the homogeneous Poisson process, with λ = 1.
It is clear how the ‘thinning’ operation works. In the homogeneous process, there
are 67 arrivals (length(evt)) in 60 minutes, while they decrease to 24 (length(ev))
in the non-homogeneous process. Notice that the arrivals occur in correspondence of
sin(t) values close to near 1, for which the inequality u ≤ λ(t)/λ holds almost always.
On the contrary, the process languishes for small or negative values of sin(t).
Some comments are in order. In experimental practice, not always are all the oc￾curred arrivals counted arrivals. Suppose we have a source emitting charged particles
and a counter to record the events. There exists for the instrument a threshold under
which it is not able to register all the events anymore: such a threshold is the temporal
resolution of the instrument. For instance, if the temporal resolution is equal to a tenth
of a second, and two events occur at a distance of a hundredth of a second from one
another, only one event is registered by the instrument.
There is a further factor which might alter experimental observations, the so-called
dead time. It is the time interval during which the instrument remains inactive after
the registration of an event. If we wish to count by eye the flashes of light arriving
from a source and these flashes are extremely intense, the eye may remain dazzled for
a moment after a flash, so that further flashes in rapid succession cannot be seen. The
same might happen with an instrument. We wish to stress, in short, that there might
be events following a Poisson distribution, but that their registration might not look
like it, unless the temporal resolution and the dead time are negligible with respect to
the arrival time.Exercises 139
4.7 Exercises
Exercise 4.1 The following is a fun example extracted from the R. McElreath book Statis￾tical Rethinking (CRC Press, 2020). You are in the business with a monastery that you own.
The monastery employs 1000 monks, independently working from one another. They copy
by hand ancient manuscripts of varying length, and produce every day a variable number of
manuscripts, some day more than three, other day none. You know that on the average one
out of 1000 monks finishes a manuscript.
That is clearly a binomial process, so you can compute the mean µ and variance v by the
well-known formulae:
µ = N p
v = N p(1 − p)
where p = 1/1000 and N = 1000.
Simulate a true manuscript production in 10 years (3650 days) by an R code, to verify how
close the ‘true’ average production and variance are to the theoretical quantities. Why are
you allowed to affirm that the number of finished manuscripts follows a Poisson distribution?
Hint: use the binomial random number generator rbinom
Exercise 4.2 With reference to exercise 4.1, suppose that you want to monopolize the mar￾ket for manuscript production. You are offered to buy a new monastery, but you don’t know
how many monks work there. The only information you have is that it produces, on the av￾erage, two manuscripts per day. Use this information to infer the distribution of the number
of manuscripts completed each day.
Exercise 4.3 Your sister constantly watches her smartphone, waiting for messages from her
friends. Starting from 8 a.m. she gets messages at the rate of five per hour. Assuming that
the messages arrival is a Poisson process, find:
(1) The probability that she receives exactly 20 messages before eleven o’clock.
(2) The probability of receiving at least 20 messages by the same hour.
(3) The probability that she receives exactly 20 messages by 11 a.m. and exactly 40 by 4 p.m.
Exercise 4.4 Use the R function dpois to compute the probabilities of exercise 4.3.
Exercise 4.5 A motorway rescue service receives calls according to a Poisson process at the
rate of six calls per hour.
(1) Find the probability that they do not receive calls over a period of two hours.
(2) Find the probability of having exactly six calls in the next hour and a half.
Exercise 4.6 Write an R code to simulate the Poisson process of exercise 4.5 and numerically
compute what is required in that exercise.
Hint: remember that inter-arrival times are exponentially distributed.5
Random Walk
A man starts from a point O and walks L yards in a straight line; he then
turns through any angle whatever and walks another L yards in a straight line.
He repeats this process N times. I require the probability that after these N
stretches he is at a distance between R and R+δR from his starting point O.
Karl Pearson, The Problem of the Random Walk
5.1 Definitions and examples
This epigraph is well known, since the term ‘random walk’ was introduced for the
first time by K. Pearson in a letter to Nature (Pearson, 1905). He asked if there
was any reader able to solve the above problem. The answer came from Lord Rayleigh
(1842−1919) only a week later published in the same volume of Nature, (Lord Rayleigh,
1905, 318). He wrote that he had faced similar problems 25 years earlier, in his studies
concerning the superposition of sound waves of equal frequency and amplitude but
with random phases (Lord Rayleigh, 1880; Lord Rayleigh, 1899); the complete citation
should be John William Strutt, third Baron Rayleigh, but we simply cite him as Lord
Rayleigh.
Pearson, a real gentleman, in a subsequent page (Nature p. 342), recognized that:
‘I ought to have known [Lord Rayleigh’s solution], but my reading of late years has
drifted into other channels, and one does not expect to find the first stage in a biometric
problem provided in a memoir on sound.’
Let us start with the simple random walk. This random walk is described through
a discrete-time Markov chain in the discrete state space. The walker flips a coin (even
not fair). If it lands on heads he moves one unit step to the left, and if the outcome
is tails the step is to the right. If we call 0 the starting point, after, say, 8 steps the
walker could be on 0, −1, −2, −1, 0, +1, +2, +3, +2. The metaphor of a coin toss
will follow us for the whole chapter.
More formally, let X1, X2, X3, . . . , Xn, . . . be i.i.d. random variables, where Xi
is
the walk distance on the ith step. Each step i has a probability P {Xi} such that:
(
P(Xi = +1) = p
P(Xi = −1) = q = 1 − p
with 0 < p < 1. If p = q = 1/2, the walk is called symmetric. The motion is schematized
in Fig. 5.1. Let us see if the walk is irreducible, that is whether all states are recurrent.
In that case all states communicate with each other, then we have to estimate the
probability p
m
00 of a path of the type: 0, −1, −2, −3, −2, −1, 0. But starting at 0, itDefinitions and examples 141
q
i - 1 i i + 1
p
Fig. 5.1 Random walk on a line. Transitions from state i to state i + 1 with probability p
or to state i − 1 with probability q = 1 − p.
is not possible to return to 0 in an odd number of steps, therefore m must be equal to
2n. We can think again of the walker flipping a coin, winning or losing £1 at each flip.
Starting at 0, he returns to 0 after 2n steps, if he won £n and lost £n. The probability
of such an event is that of n successes in 2n independent Bernoulli trials. Recall that
the probability of x successes in n independent trials is:
p(x) = 
N
x

p
x
(1 − p)
N−x =
N!
x!(N − x)! p
x
(1 − p)
N−x
in the present case x = n, N = 2n and (N − x) = n. So, we write:
p
(2n)
00 =
(2n!)
(n!n!) p
n
(1 − p)
n =
(2n!)
(n!)2
p
n
(1 − p)
n
For large n, we can exploit the Stirling’s approximation:
n! ≈
√
2πn (n/e)
n
, n → ∞
then:
(2n!)
(n!)2
≈
√
2π · 2n

2n
e
2n
√
2πn 
n
e
n 2 =
2
(2n)
√
πn
therefore, we have:
p
(2n)
00 ≈
(4p(p − 1))n
√
πn
, n → ∞
If p = q = 1/2, 4pq = 1, and:
p
(2n)
00 =
1
√
nπ
so it is:
p
(2n)
00 ≥
1
2
√
nπ
then:
X
n>1
p
(2n)
00 ≥
1
2
√
π
X
n>1
1
√
n
= ∞142 Random Walk
which means that the walk is recurrent. On the contrary, if p 6= q, 4pq = r < 1, so now
we have:
X
n>1
p
(2n)
00 ≤
1
π
X
n>1
r
n < ∞
then the walk is transient. Also in two dimensions, all states are recurrent, but in three
dimensions no state is recurrent; they are all transient.
Let Sn = X1 + X2 + X3 · · · + Xn be the position of the walker at time n, or
after n steps, since the time unit is the step of size one. For convenience, the starting
point is S0 = 0. If x1, x2, . . . are the realization of X1, X2, . . . , we have, for instance:
S8 = 0 + (−1) + (−2) + (−1) + 0 + (+1) + (+2) + (+3) = 2. Note that the steps
x1, x2, x3, . . . , xn are realizations of independent random variables, either +1 or −1,
while the positions at Sn is correlated with the position at Sn−1.
In Chapter 3, we introduced the number of visits Vi to the state i and its expected
value:
E

Vi

X0 = i

=
X∞
n=0
p
n
ii
We saw that if i is recurrent, the number of visits to i is infinite, so the expected value
of Vi
is also infinite, while if i is transient Vi
is finite and the expected value of Vi
is
finite too. We can estimate the position of the particle after n steps, by evaluating the
expectation of the walk given by:
E [Sn] = E"Xn
i=1
Xi
#
=
Xn
i=1
E

Xi

=
Xn
i=1
p − (1 − p) = n(2p − 1)
equal to 0, if the walk is symmetric. The uncertainty in this estimate is given by the
variance:
Var [Sn] = Xn
i=1

E

X2
i

−

E(Xi)
2

= p+(1−p)−(2p−1)2 = 1−4p
2+4p−1 = 4np(1−p)
equal to n, if p = 1 − p. Then, if the walk is symmetric, the standard deviation after
n steps is √
n, perhaps a bit of a counter intuitive result.
A viewable approach, which we will also follow later on for two-dimensional walks,
consists in considering again the total displacement after n steps, each of length li (not
necessarily of unit length):
Sn =
Xn
i=1
li
whose mean is:
hSni =
Xn
i=1
hlii = 0
since, if n is large enough, there are as many positive as negative li values. Consider
now S
2
n
, which is:Definitions and examples 143
S
2
n = (l1, l2, . . . , ln)(l1, l2, . . . , ln) = Xn
i=1
l
2
i +
Xn
i6=j
li
lj
Suppose now the steps are all equal to l, the first sum is nl2
, while the second sum
is zero, since hli
lj i = 0, there being no correlation between steps. Then we have the
previous derived result for unit step:
hS
2
n
i = n
The random walk can be graphically represented as an XY plot with the number
of steps on the x-axis there are the number of steps and the value of Sn on the y-axis.
The points (n, Sn) and (n + 1, Sn+1) are joined with a straight line segment. An R
code with somewhat na¨ıve instructions could be of the type:
x <- numeric()
x[1] <- 0
n <- 20
for(i in 2:n)
{
u<-runif(1,0,1)
if (u<=p) z<- +1
if (u>p) z<- -1
x[i]<-x[i-1]+z
}
so the Sn path is constructed. However, this way is not the most efficient one. A better
version is the Code_5_1.R below, in which one path for a few steps is shown. The R
code is based on the Bernoulli distribution given by rbinom(n,size,prob) where n is
the number of independent random variables to be generated, size is the number of
trials, and prob the probability of success.
## Code_5_1.R
# simple symmetric random walk, one path
set.seed(1)
nsteps<- 20 # total number of steps
p<-0.5 # transition probability
x<-numeric()
n<- numeric() # time after n steps
x0<- 0 # starting point
x<-rbinom(nsteps,1,p)
x[which(x==0)]<- -1 # also x<- 2*x-1
# Sn: position of the walker after n steps
Sn<-c(x0,cumsum(x)) # adding the starting point
n<- 0:nsteps # time axis
par(mai=c(1.02,1.,0.82,0.42)+0.1)
plot (n,Sn,type="o",xlab="number of steps",ylim=c(-4,4),lty=1,
ylab=expression(italic("S"[n] )~~ italic((position)) ),
cex.lab=1.2,font.lab=3,pch=19,xaxt="none" )
axis(1, at=seq(0,nsteps,1))
abline(h=0,lty=2)
Ten realizations from the Bernoulli random variable are (added x1 = 0):
0 -1 -1 1 1 -1 1 1 1 1 -1
from which the Sn values are:144 Random Walk
0 -1 -2 -1 0 -1 0 1 2 3 2
Note the ‘time axis’ command n <- 0:nsteps creates a timescale from 0 to nsteps,
one step at a time, with S0 = 0.
Figure 5.2 shows the position of the walker Sn as a function of the number of steps.
We see that the first return to the origin occurs at time n = 4, that is to say after four
−4 −2 0 2 4
number of steps
Sn 
(position
)
0 2 4 6 8 10 13 16 19
Fig. 5.2 Symmetric random walk on a line. Position Sn of the walker after n steps. The dots
are the reached points.
steps. The following Code_5_2.R extends the previous Code_5_1.R to simulate several
different random paths. The estimate Sˆ
n of the expectation of the walk E [Sn] and the
estimate σˆ
2
n of the variance Var [Sn] are also computed.
## Code_5_2.R
# simple symmetric random walk, nhists paths
set.seed(1)
p<- 0.5 # transition probability
nhists<- 100 # number of histories
nsteps<- 200 # total numero of steps
x0<-0 # starting point
splot<- 30 # plotted only splot histories out of all nhists
# starting vector: each history starts from x0
# column vector with all x0 repeated nhists times
start_matr<-matrix(x0,nhists,1)
Sn<-matrix(,nhists,nsteps+1)
y<-matrix(,nhists,nsteps)
for (i in 1:nhists) { # starting loop on histories
x<-rbinom(nsteps,1,p)
x[which(x==0)]<- -1 # also x<- 2*x-1
y[i,]<-cumsum(x) # Sn without the starting point
} # ending loop on historiesDefinitions and examples 145
Sn <- matrix(c(start_matr,y+x0),nhists,nsteps+1) # adding the starting vector
n<- seq(0, nsteps) # time axis
par(mai=c(1.02,1.,0.82,0.42)+0.1)
plot (n,Sn[1,],xlab="number of steps",ylim=c(-40,40),type="l",lty=1,
ylab=expression(italic("S"[n] )~~ italic((position)), ),
cex.lab=1.2,font.lab=3,lwd=0.5,xaxt="none" )
axis(1, at=seq(0,nsteps,20))
# Sn[1,] # uncomment to print the first path
for (i in 2:splot)
{
lines(Sn[i,],type="l",lty=1,lwd=0.5)
}
par(mai=c(1.02,1.,0.82,0.42)+0.1)
hist(Sn[,nsteps+1],freq=F,main="",
xlim=c(-50,50),font.lab=3,
xlab=expression(italic("S"[n] )~~ italic((position)) ),
cex.lab=1.2,font.lab=3)
# freq=F to compare with analytical density by 'curve(.)'
mSn<-mean(Sn[,nsteps+1])
mSn
VarSn<- var(Sn[,nsteps+1])
VarSn
se.Sn<- sqrt(var(Sn[,nsteps+1]))
se.Sn
curve (dnorm(x, mean=mSn, sd=se.Sn),lty=2,lwd=2,add=T)
shapiro.test(Sn[,nsteps+1])
mSn.an<- nsteps*(2*p-1) # .an stays for 'analytical'
mSn.an
VarSn.an<- 4*nsteps*p*(1-p)
VarSn.an
seSn.an<- sqrt(VarSn.an)
seSn.an
Figure 5.3 shows the time evolution of 30 paths out of 100 simple symmetric random
walks. The number of different symmetric walks of n steps is 2n, and each of these
walks is equally likely. From the simulation, we can estimate where on average the
walker is after n steps. It results in Sˆ
n = −0.82 and ˆσ
2
n = 211.40, in agreement with
the theory E [Sn] = 0 and Var [Sn] = 200, respectively.
In the space (n, Sn) of possible paths, the ensemble of paths covers a bounded
‘triangular’ area. If we plot a histogram of the endpoints, Fig. 5.4 is obtained. The
y-axis refers to the normalized histogram for comparison with the probability density
function of the normal random variable (dashed line). One sees that the histogram
is approximately normal in shape, suggesting that the distribution of the end points
might tend, in probability, to a normal one with mean Sˆ p n and standard deviation
σˆ
2
n
, as the central limit theorem states. This claim is evidenced by superimposing a
normal probability density function (dashed lines in Fig. 5.4) with mean = −0.82 and
standard deviation = √
211.40: the agreement appears to be very good.
To further assess the normality of the histogram data, the Shapiro-Wilk normality
test is exploited with the R command shapiro.test(Sn[,nsteps+1]). For this test,
the null hypothesis is that the data come from a normal distribution, and commonly it
is accepted (better: it cannot be rejected) if the p-value is greater or equal than 0.05.
In our case, the p-value is equal to 0.1487, so we cannot reject the hypothesis that the
distribution is normal.146 Random Walk
−40 −20 0 20 40
number of steps
Sn 
(position
)
0 20 60 100 140 180
Fig. 5.3 Symmetric random walk on a line. Position Sn of the the walker after n steps of 30
paths out of 100.
density
−40 −20 0 20 40
0.000 0.010 0.020 0.030
Sn (position)
Fig. 5.4 Symmetric random walk on a line. Histogram of the endpoints of 100 simulated
paths. Comparison with the probability density function of the normal random variable
N

Sˆn,
√
σˆ
2
n

(dashed line) is also shown.
Let us compare the above results with those of an asymmetric simple random walk.
In the Code_5_2.R, we set the transition probability to p <- 0.7. The result is shownDefinitions and examples 147
in Fig. 5.5. The shape of the ‘triangular’ area of possible paths does not change, it
0 20 40 60 80 100
number of steps
Sn 
(position
)
0 20 60 100 140 180
Fig. 5.5 As in Fig. 5.3, but with p = 0.7.
looks a bit narrower and points upwards. Moreover, the estimates result Sˆ
n = 79.58
and ˆσ
2
n = 156.95, in agreement with the theory E [Sn] = 80 and Var [Sn] = 168,
respectively.
5.1.1 Barriers
Barriers are states with characteristic properties. If a state is absorbing, we say that
it is an absorbing barrier. If the walk reaches an absorbing barrier, the walk ends and
the walker stays there forever. The matrix of transition probabilities is:


1 0 0 0 · · · 0 0 0
q 0 p 0 · · · 0 0 0
0 q 0 p · · · 0 0 0
. . . . . . . . . . . . . . .
0 0 0 0 · · · q 0 p
0 0 0 0 · · · 0 0 1


If the states are numbered 0, 1, 2, . . . , n, the state space is partitioned into three classes:
{0}
absorbing state
{1, 2, . . . , n − 1}
transient states
{n}
absorbing state
The walk is restricted to the internal states {1, 2, . . . , n − 1} where transitions are
allowed either to the right pi,i+1 = p or to the left pi,i−1 = q = 1 − p. If the walker
ends up in 0 or n, then p00 = pnn = 1. If there is no barrier, the walk is called
unrestricted.148 Random Walk
The following code Code_5_3.R simulates several restricted random paths with two
absorbing barriers. The code can plot on the same graph, trajectories of unrestricted
and restricted walks. The first part of the code is a copy of Code_5_2.R, but the plot
is limited to only one unrestricted trajectory (plot (n,Sn[1,] ...)), for clarity of
plots. The barriers b1 and b2 are in the positions S10 and S−10, respectively. If the
walker at a certain step reaches one of the barriers, he gets absorbed and the position
of such event is recorded. The code gives the estimates of the expected duration of the
walk and other quantities.
## Code_5_3.R
# restricted random walk with absorbing barriers
# beginning with walk without barriers
set.seed(3)
p<-0.5
x0<-0
nsteps<- 200
nhists<- 100
# starting vector: each history starts from x0
# column vector with all x0 repeated nhists times
start_matr<-matrix(x0,nhists,1)
y<-matrix(,nhists,nsteps)
for (i in 1:nhists) { # starting loop on histories
x<-rbinom(nsteps,1,p)
x[which(x==0)]<- -1 # also x<- 2*x-1
y[i,]<-cumsum(x) # Sn without the starting point
} # ending loop on histories
Sn <- matrix(c(start_matr,y+x0),nhists,nsteps+1) # adding the starting vector
n<- seq(0, nsteps) # time axis
par(mai=c(1.02,1.,0.82,0.42)+0.1)
plot (n,Sn[1,],xlab="number of steps",ylim=c(-15,15),type="l",lty=2,
ylab=expression(italic("S"[n] )~~ italic((position)), ),
cex.lab=1.2,font.lab=3,lwd=1,xaxt="none" )
axis(1, at=seq(0,nsteps,20))
#-----------------------------------------
# continue to plot the walks on a single graph
set.seed(3) # reset random numbers if wanted
t1<-numeric()
t2<-numeric()
t1b1<-numeric()
t2b2<-numeric()
no.abs<-numeric()
# the following parameters can be redefined
x0<- 0
nsteps<- 200
nhists<- 100
x<-numeric()
p<-0.5
y<-matrix(,nhists,nsteps)
b1<- 10
b2<- -10
for(i in 1:nhists) { # starting loop on histories
x[1]<-x0
for(j in 2:nsteps) { # starting loop on steps
u<-runif(1,0,1)
if(u<p)z<- +1Definitions and examples 149
else z<- -1
x[j]<-x[j-1]+z
if(x[j-1] == b1) x[j]<- b1
if(x[j-1] == b2) x[j]<- b2
} # ending loop on steps
y[i, ]<- x
t1[i]<- min(which(x==b1))
t2[i]<- min(which(x==b2))
} # ending loop on histories
Sn <- matrix(c(start_matr,y+x0),nhists,nsteps+1) # adding the starting vector
#y[i, ] # if wanted to print entire i-th selected history
t1[!is.finite(t1)] <- 0
t1
t1b1 <- t1[t1!=0]
t1b1
t2[!is.finite(t2)] <- 0
t2
t2b2 <- t2[t2!=0]
t2b2
t12<- 0
for(i in 1:nhists)
if(t1[i]==0 & t2[i]==0) {t12<-t12+1
no.abs[t12]<- i }
no.abs
par(lwd=1.7,font=3)
lines(y[17, ], type="l",lty=1)
text(180,5,"c")
lines(y[60, ], type="l",lty=1)
text(115,12,"a")
lines(Sn[2, ], type="l",lty=1)
text(135,-12,"b")
lines(y[60, ], type="l",lty=1)
abline(h=b1,lty=4,col="black",lwd=2)
abline(h=b2,lty=4,col="black",lwd=2)
l_t1b1<- length(t1b1)
l_t1b1
l_t2b2<- length(t2b2)
l_t2b2
l_no.abs<-length(no.abs)
l_no.abs
m.t1b1<- mean(t1b1)
m.t1b1
quant<-quantile(t1b1)
quant
se.t1b1<- sqrt(var(t1b1))
se.t1b1
m.t2b2<- mean(t2b2)
m.t2b2
quant<-quantile(t2b2)
quant
se.t2b2<- sqrt(var(t2b2))
se.t2b2
hist(t1b1,freq=F,main=" ",xlab="number of steps",
col=rgb(0.8275, 0.8275, 0.8275,1),
cex.lab=1.2,font.lab=3,ylab="absorbed states",xaxt="none",
ylim=c(0,0.015),right = TRUE )
axis(1, at=seq(0,nsteps,20))
lines(density(t1b1),lwd=2,lty=1)
op <- par(cex = 1.3,font=3)150 Random Walk
text(160,0.010,"b1")
histb1<- hist(t1b1,plot=F)
hist(t2b2,freq=F,main=" ",xlab="number of steps",
col=rgb(0.8275, 0.8275, 0.8275,1),
cex.lab=1.2,font.lab=3,ylab="absorbed states",xaxt="none",
ylim=c(0,0.015),right = TRUE )
axis(1, at=seq(0,nsteps,20))
lines(density(t2b2),lwd=2,lty=1)
op <- par(cex = 1.3,font=3)
text(160,0.010,"b2")
histb2<- hist(t2b2,plot=F)
histb1
histb2
The command y[i, ] prints the entire i-th path, for instance y[60, ] prints:
[1] 0 1 0 1 2 3 2 3 2 1
..............................
[76] 9 8 7 6 5 6 5 6 7 8 7 6 7 6 7 6 7 8 9 8 9 8 9 10 10
.............................
It shows that at step 99, the walker encounters the higher barrier and thereafter the
path does not change anymore.
In the vector t1 the absorbing time for each history is recorded, where 0 means
that for this history, the walker does not reach the barrier b1. For example, for the
first five histories no absorbtion in b1 occurs. The 6-th path is absorbed at the 193-th
step. Note that the history 60 is absorbed at the step 99.
[1] 0 0 0 0 0 193 0 53 ......
..............................
[55] 0 37 51 0 91 99 63 0 0 ......
..............................
In the vector t1b1, the 0 values are removed to print only the absorbing time on the
higher barrier b1 for each absorbed path.
[1] 193 53 95 103 61 125 59 101 109 29 27 109 75 123 33 25 123 27 87
[20] 51 65 37 51 91 99 63 117 77 193 137 63 23 33 43 93 89 61 33
[39] 113 51 17 85 151 33
The same procedure is applied for the lower barrier b2=-10.
There are paths that do not end on one barrier, but continue up to the given
number of steps. The vector no.abs reports such not absorbed histories:
[1] 1 17 18 24 31 38 54 62 77 94
Figure 5.6 shows one unrestricted path (dashed line) and three restricted paths (con￾tinuous line), out of 100. The walk (a) is absorbed on b1 barrier at step 99 (history 60).
The walk (b) reaches the b2 barrier at the step 113 (history 2). The walk (c) (history
17) is never absorbed during 200 steps.
The number of absorbing paths on the b1 barrier is l_t1b1 = 44, and on the b2
barrier is l_t2b2 = 46, then no.abs = 10 paths do not fall on a barrier. The mean of
the number of steps at which the path is absorbed is m.t1b1 = 77.86, and m.t2b2 =Definitions and examples 151
−15 −5 0 5 10 15
number of steps
Sn 
(position
)
0 20 60 100 140 180
c
a
b
Fig. 5.6 One unrestricted path (dashed line) and three restricted paths (continuous line),
out of 100, of a random walk with barriers. Dot-dashed lines: barriers positions S10 and S−10,
respectively.
84.74, for the higher and lower barrier, respectively. The command quantile(t1b1)
gives the quantile estimates corresponding to chosen percentiles, for the b1 barrier:
> quant
0% 25% 50% 75% 100%
17.0 41.5 70.0 104.5 193.0
and for the b2 barrier:
> quant
2.5% 15.87% 50% 84.13% 97.5%
22.000 47.283 72.000 140.151 181.000
Figure 5.7 shows the number of absorbing states as a function of the number of steps.
The figure refers to the barriers b1 and b2 respectively. Note that with freq=F, proba￾bility densities are plotted instead of frequencies, so the histogram can be overlaid with
a kernel density plot. Use the command plot=F to have a list of breaks and counts, as
reported below:
breaks
histogram
[1] 0 20 40 60 80 100 120 140 160 180 200
counts
[1] 1 10 6 7 7 6 4 1 0 2 b1 barrier
counts
[1] 1 5 12 10 4 3 3 4 1 3 b2 barrier152 Random Walk
For instance, in histogram b1, in the steps interval (20, 40] (left-hand endpoints are
not included) there are 10 absorbing states, that is 10 paths out of 100 end on the
higher b1 barrier between 21 and 40 steps.
number of steps
absorbed states
0.000 0.005 0.010 0.015
0 20 60 100 140 180
b1
number of steps
0.000 0.005 0.010 0.015
0 20 60 100 140 180
b2
Fig. 5.7 Histograms of the absorbing states on b1 and b2 barriers of 100 simulated paths.
A kernel density curve is also overlaid.
Since the walk is symmetric, the two barriers are equiprobable, indeed the values
of the means m.t1b1 = 77.86, and m.t2b2 = 84.74 are close to each other. On the
contrary, for instance, with p = 0.7, no history reaches the b2 barrier, but all end on
b1 (for the p 6= q case, see later).
We now run the Code_5_3.R, with nsteps = 400, and barriers at 0 and 40. The
starting point is x0 = 20. Before discussing the results, let us imagine the walker not as
a pilgrim carrying out el camino step by step. He (or she) is actually a gambler, tossing
a coin at every step. We ask ourselves whether he is winning or he has lost everything.
The answer is dealt with in the next section by facing the well-known gambler’s ruin
problem.
5.1.2 Gambler’s ruin
Figure 5.8 is obtained with Code_5_3.R. We see again three different behaviours:
absorption on b1 = 40, absorption on b2 = 0, and no absorption. However, Fig. 5.8
can now be interpreted as the result of 400 tosses of a fair coin, that is the probability
of heads and the probability of tails are p = 1/2. The gambler plays against the
house (a casino) and he wins £1, if heads occurs, or loses £1, in the case of tails. His
initial amount of money is £20, that is the initial value (x0, in the code) and it has
to be imagined as the coins in his pockets before starting the game, rather than the
beginning of the path. Similarly, the quantity Sn is no more the position of the walker
after n steps, but the capital of the gambler after n tosses. The game ends when:
i ) either the gambler has finished all the money; he is ruined,
ii ) or the gambler has decided to retire after a certain winning, for instance when his
initial capital is doubled.
In real life, we know that losing is more likely than winning, and the ruin is the
ultimate fate of all gamblers, as, for instance, William Makepeace Thackeray writes inDefinitions and examples 153
0 10 20 30 40
number of tosses
Sn 
(gambler’s capital
)
0 50 150 250 350
Fig. 5.8 Three restricted paths, out of 100, of a random walk with barriers at 0 and 40.
The Luck of Barry Lyndon (first original edition 1844):
Barry was born clever enough at gaining a fortune, but incapable of keeping one. For the
qualities and energies which lead a man to triumph in the former case are often the very
cause of his undoing in the latter.
Later, Stanley Kubrick, in 1975, directed and produced the famous and Oscar￾winning film Barry Lyndon, inspired by the novel by Thackeray.
In our simulation, the walker (sorry, the gambler) returns home, winning 31 (l_t1b1),
evenings out of 100, he is ruined 27 (l_t1b1) times, but there are also 42 (l_no.abs)
games in which the gambler is neither winner nor loser, and the number of tosses is
not enough to go to one end. Let us look into the matter by applying the method of
the difference equation.
The Gambler’s ruin problem has a long history tracing it back to Pascal who pro￾posed this problem to Fermat in 1656. Many mathematicians dealt with this subject:
Huygens, James Bernoulli, De Moivre, up to Amp`ere in 1802. How the problem was
formulated over time and how solutions were proposed are discussed in Song and Song
(2013). Here we follow the exposition which is found in the ‘immortal’ An Introduction
to Probability Theory and Its Applications by Feller (1970), adopting also the same
symbology.
Let z be the initial capital of the gambler and let qz be probability of his ruin and
pz of his winning, that is when his capital becomes a, for instance 2z. We are at the
first toss. After that, the capital becomes either z + 1 or z − 1. We have seen that the
state space of the process is partitioned into three classes: recurrent states {0} and
{n}, transient states: {1, 2, . . . , n − 1}. In the gambler’s case the transient states are:
z = 1, 2, . . . , a − 1154 Random Walk
Transient states are visited a finite number of times, therefore after a certain number
of steps 0 or a is reached. Then it is:
qz = p qz+1 + q qz−1, 1 < z < a − 1
Considering also z = 1 and z = a − 1, we have the additional equations:
q1 = pq2 + q, z = 1
qa−1 = qqa−2, z = a − 1
All these equations can be incorporated into the difference equation:
qz = p qz+1 + q qz−1, 1 6 z 6 a − 1 (5.1)
Equation (5.1) does not hold for z = 0 or z = a, since the game terminates for these
values of z. We have the boundary conditions:
q0 = 1 qa = 0
clearly, if q0 = 1, the gambler has no money to begin with (he is certain to lose), i.e.
the game is already over; similarly if qa = 0 (he already has £a) the probability of
ruin is zero, so there is no reason to play again. Equation (5.1) is linear, so if qz and q
0
z
are solutions, also Aqz + Bq0
z
(A, B ∈ R) is a solution. The above difference equation
can be derived by the partition theorem. This says that the probability that an event
A occurs, considering the fact that another event B may or may not have occurred, is
given by:
P {A} = P

A

B
	
P {B} + P

A

B
c
	
P {B
c
}
Here A is the event ‘gambler’s ruin’ and B the event ‘gambler wins’. It is P {B} =
p, P {Bc} = 1 − p = q. If the first game is won, the capital becomes z + 1 and the
game goes on:
P

A

B
	
= qz+1
and if the first game is lost:
P

A

B
c
	
= qz−1
Therefore, for the partition theorem:
qz = qz+1p + qz−1q, 1 6 z 6 a − 1
with the above boundary conditions.
Equation 5.1 can be solved by introducing an auxiliary equation of the type:
pα2 − α + q = 0
where α
z = qz. The above equation has the roots α1 = 1 and α2 = q/p. If p 6= 1/2, α1
and α2 are different, the general solution of eqn (5.1) can be written as:
qz = c1(1)z + c2

q
p
zDefinitions and examples 155
with the condition p 6= q. The boundary conditions are satisfied if c1 and c2 are the
solutions of the equations:
1 = c1 + c2 0 = c1 + c2

q
p
z
The solutions are:
c1 =

q
p
a

q
p
a
− 1
c2 = −
1

q
p
a
− 1
Finally, the solution for qz is given by:
qz =

q
p
a
−

q
p
z

q
p
a
− 1
, p 6= q (5.2)
Then we have found the probability of the gambler’s ruin, when he begins to play with
an initial capital z. We have presented a scenario in which a gambler plays against
the house; historically and in modern literature, the problem is seen also as a game
between two players G1 ad G2. The initial capital of G1 is z, the initial capital of G2
is a − z. If G1 wins, his capital becomes z + 1 and G2’s capital a − z − 1. Similarly, if
G1 loses, his capital decreases to z − 1, while the G2’s capital increases to a − z + 1.
In any case the total capital a stays constant. The game continues as we have already
seen, with £1 going from G1 to G2 or vice versa. The game finishes when either G1 or
G2 has no more money. In conclusion: either G1 has won £a and G2 is ruined, or G1
is eventually ruined and G2 has gained £a.
Turning to eqn (5.2), we have found the probability of the gambler’s ruin, when
he began to play with £a, we can ask ourselves what is the probability of winning the
desired amount of money a. It is the probability that his adversary is ruined, given by
interchanging in eqn (5.2) p and q, and putting a − z in the place of z, obtaining:
pz =

q
p
z
− 1

q
p
a
− 1
(5.3)
From eqn (5.2) and eqn (5.3) it follows that:
qz + pz = 1
which means that formally the game cannot continue indefinitely, but ends with prob￾ability 1. We have seen in Fig. 5.8 the results of fair games, that is the toss of a fair
coin, for which the probabilities of making heads or tails are equal to 1/2. In this case,156 Random Walk
eqn (5.2) is no longer valid, since the particular solutions qz = 1 and qz = (q/p)
z are
identical. However, also qz = z is a solution of eqn (5.2), therefore also qz = c1 + c2z
is a solution of eqn (5.2). The boundary conditions are satisfied if:
1 = c1 0 = c1 + c2 a
Then:
qz = 1 −
z
a

, p = q (5.4)
Another simple way suggested by Feller (1970, p. 345), to obtain the above result,
consists of computing the limit p → 1
2
through the L’Hospital rule. Putting u = q/p
in eqn (5.2), gives:
qz = limu→1
u
a − u
z
u
a − 1
= limu→1
aua−1 − zuz−1
aua−1
= 1 −
zuz−1
aua−1
= 1 −
z
a

Figure 5.8 shows three trajectories of fair games in which, with the present symbol￾ogy, z = 20 and a = 40. Then z =
1
2
a, and qz =
1
2
. This is sensible, since, in the random
walk language, the walker starts half-way between the boundaries and he has an equal
chance of reaching either the boundary 0 or the boundary a. In the Code_5_3.R, the
number of steps is now 1000 and, to increase the sample size, the number of histories
is put equal to 1000. As expected, the number of ruins (l_t1b2= 481) is very close to
the number of wins (l_t1b1= 451), also in agreement with eqn (5.4). The number of
unending paths, up to 1000 steps, is 68. Unending paths are present also in a run up
to 5000 steps. We will return later to this topic.
Feller (1970, p. 346) notes that from eqn (5.4), when p = q, if the gambler has an
initial capital z = £999, has a probability 0.999 to win £1 before losing all. If q = 0.6
and p = 0.4 the game is unfair, nevertheless the gambler has a probability about 2/3
to win £1 before being ruined. If the initial capital z is conspicuous, the gambler can
hope for a small quantity a − z before losing his wealth.
Suppose now that there is no limiting value a, since the gambler wants to win
an ‘infinite’ fortune and continues to play stubbornly until he is ruined. This case is
known as a game against an infinitely rich adversary, for instance a casino. Such a
fanciful probability of success can be obtained by a → ∞ in eqn (5.2) and in eqn (5.4).
In eqn (5.2), it gives:
lima→∞

q
p
a
−

q
p
z

q
p
a
− 1
= lima→∞
1 −

p
q
a−z
1 −

p
q
a = 1
In conclusion:
(
qz = 1 if p 6 q
(q/p)
z
if p > q
Therefore:Definitions and examples 157
p 6 q. The ruin has a probability equal to one to occur, even though p is slightly less
than 1/2. Clearly, the probability of ruin decreases as the gambler’s initial capital
increases, that is to say that the starting point of the walk is nearer to the barrier
b1 = 40.
p > q. There exists a non-zero probability that the gambler becomes ‘indefinitely’ rich.
However, even though the bias is in favour of the gambler, there still exists the
possibility of ruin.
In the random walk language, a walker starting at the origin z = 0 has probability 1
of reaching the position z > 0, if p > q, and equal to (p/q)
z
if p > q.
Let us address the problem by simulating with the Code_5_3.R, in which now
p = 0.45 and the number of histories is 1000. The results show that the ruin happens
976 (l_t1b2) times out of 1000, while 22 (l_t1b1) histories reach the barrier b1 =
40, and two do not terminate on one of the barriers. By the way, note that the above
number can change slightly, with different sequences of random numbers.
Suppose now that the game is biased in favour of the gambler, with p = 0.55.
In this case, the gambler wins 987 (l_t1b1) times and loses 11 games, two histories
have not ended on one of the barriers. The peculiar implementation of the computer
simulations leads to a symmetry of the set-ups: the number of games ending on the
barrier at 0 when p = 0.45 is about the number of games reaching the barrier at a = 40
when p = 0.55.
A further important quantity to characterize the gambler’s ruin problem is the
expected duration of the game, which is the number of tosses until one of the barriers
is reached. Let dz be the final expectation of the game starting at z. The first toss
can be either a success (occurring with probability p) or a failure (occurring with
probability q). In the first case the conditional expectation of the duration is dz+1 + 1,
analogously in the second case it is dz−1 + 1. Therefore the expected duration is given
by the following difference equation:
dz = p(dz+1 + 1) + q(dz−1 + 1)
= 1 + pdz+1 + qdz−1, 1 6 z 6 a − 1
(5.5)
with the boundary conditions:
d0 = 0 and da = 0
Clearly, no further trials are to be expected if a barrier has been reached. Equation (5.5)
is non-homogeneous, for the presence of the term +1. The general solution is:
dz = A + B

q
p
z
+
z
q − p
, p 6= q
and from the boundary conditions:
A + B = 0 and A + B

q
p
a
+
a
q − p
= 0
Then the expected duration of the game, when p 6= q is:158 Random Walk
dz =
z
q − p
−
a
q − p
×
1 −

q
p
z
1 −

q
p
a
When p = q =
1
2
, the particular solution dz = z/(q − p) is invalid, so it is replaced by
dz = −z
2
, another particular solution of eqn (5.5). Then:
A + Bz − z
2
and using again the boundary conditions, we obtain:
dz = z(a − z)
As previously, consider the game against an infinitely rich adversary, that is when
a → ∞. When p > q, the game may continue forever and the quantity ‘expected
duration’ has no more sense. If p < q, the expected duration is:
dz = z
1
q − p
, p < q (5.6)
. Lastly, if p = q, the expected duration becomes infinite.
By simulation, it is possible to estimate the distribution of the expected duration, as
it has already appeared in Fig. 5.7 showing results concerning rather short path lengths
and a reduced number of histories. With reference to Code_5_3.R with increasing
numbers of steps and histories, for p = q we have found that a considerable number of
games do not end on a barrier, even after 5000 tosses.
number of tosses
losses
0 200 400 600 800 1000
0 100 200 300 400
p = 0.45
barr ier at z = 0
Fig. 5.9 Distribution of the expected duration of 976 games out of 1000 ending on the barrier
at z = 0.Definitions and examples 159
The histogram in Figure 5.9 is obtained by Code_5_3.R with p = 0.4. We see, for
instance, that 413 games out of 1000 reach the barrier z = 0 after an expected duration
in the interval (101, 200]. The duration of the games is between 29 and 925 tosses, and
the average duration is 184.37 in agreement with the value 200 from eqn (5.6).
The gambler might be so lucky to generously receive £1 from the casino when he
has lost all his money. Similarly, the gambler could decide to bet £1 more, even if he
has already reached the prefixed winning. Clearly, with such terms the game could
continue indefinitely: we are in the presence of reflecting barriers situations.
5.1.3 Reflecting barriers
Consider a perfectly reflecting barrier. If the random process is in the state 0, it goes
to state 1 with probability 1: p01 = 1. Just like that, pn,n−1 = 1, if n is the number of
states of the process. Note that p00 = 0, i.e. the walker cannot stay in 0. The matrix
of transition probabilities is:


0 1 0 0 · · · 0 0 0
q 0 p 0 · · · 0 0 0
0 q 0 p · · · 0 0 0
. . . . . . . . . . . . . . .
0 0 0 0 · · · q 0 p
0 0 0 0 · · · 0 1 0


At a non-perfectly reflecting barrier the walker can stay at 0, not with certainty, but
with probability p00 = q. So it can go to 1 with probability p01 = p. Similarly, pnn = p
and pn,−1 = q. The code to simulate reflecting barriers derives from the Code_5_3.R.
The Code_5_4.R is adapted to deal with the gambler’s ruin scenario, when the game
might go even further. As before, the barriers are at 0 and £40, respectively, with an
initial capital of £20. If desired, we can obtain estimates of quantities about reflecting
barriers, as was done by Code_5_3.R about absorbing barriers.
## Code_5_4.R
# restricted random walk with reflecting barriers
# beginning with walk without barriers
set.seed(3)
p<-0.5
x0<-20
nsteps<- 400
nhists<- 100
# starting vector: each history starts from x0
# column vector with all x0 repeated nhists times
start_matr<-matrix(x0,nhists,1)
y<-matrix(,nhists,nsteps)
for (i in 1:nhists) { # starting loop on histories
x<-rbinom(nsteps,1,p)
x[which(x==0)]<- -1 # also x<- 2*x-1
y[i,]<-cumsum(x) # Sn without the starting point
} # ending loop on histories
Sn <- matrix(c(start_matr,y+x0),nhists,nsteps+1) # adding the starting vector
# Sn: position of the walker after n steps
n<- seq(0, nsteps) # time axis
par(mai=c(1.02,1.,0.82,0.42)+0.1)
plot (n,Sn[1,],xlab="number of tosses",ylim=c(-2,42),type="n",lty=2,160 Random Walk
ylab=expression(italic("S"[n] )~~
italic(("gambler's"~~capital)), ),
cex.lab=1.2,font.lab=3,lwd=1,xaxt="none" )
axis(1, at=seq(0,nsteps,50))
#-----------------------------------------
# continue to plot the walks on a single graph
set.seed(3) # reset random numbers if wanted
r1<-numeric()
r2<-numeric()
r1b1<-numeric()
r2b2<-numeric()
no.refl<-numeric()
# the following parameters can be redefined
x0<- 20
nsteps<- 400
nhists<- 100
x<-numeric()
p<-0.5
y<-matrix(,nhists,nsteps)
b1<- 40
b2<- 0
for(i in 1:nhists) { # starting loop on histories
x[1]<- x0
for(j in 2:nsteps) { # starting loop on steps
u<-runif(1,0,1)
if(u<p)z<- +1
else z<- -1
x[j]<-x[j-1]+z
if(x[j-1] == b1) x[j]<- b1-1
if(x[j-1] == b2) x[j]<- b2+1
} # ending loop on steps
y[i, ]<- x
r1[i]<- min(which(x==b1))
r2[i]<- min(which(x==b2))
} # ending loop on histories
#y[i, ] # if wanted to print entire i-th selected history
r1[!is.finite(r1)] <- 0
r1
r1b1 <- r1[r1!=0]
r1b1
r2[!is.finite(r2)] <- 0
r2
r2b2 <- r2[r2!=0]
r2b2
r12<- 0
for(i in 1:nhists)
if(r1[i]==0 & r2[i]==0) {r12<-r12+1
no.refl[r12]<- i }
no.refl
Sn <- matrix(c(start_matr,y+x0),nhists,nsteps+1) # adding the starting vector
par(lwd=1.7,font=3)
lines(y[1, ], type="l",lty=1)
lines(y[60, ], type="l",lty=1)
lines(y[5, ], type="l",lty=1)
abline(h=b1,lty=4,col="black",lwd=2)
abline(h=b2,lty=4,col="black",lwd=2)
l_r1b1<- length(r1b1)
l_r1b1
l_r2b2<- length(r2b2)Definitions and examples 161
l_r2b2
l_no.refl<-length(no.refl)
l_no.refl
m.r1b1<- mean(r1b1)
m.r1b1
se.r1b1<- sqrt(var(r1b1))
se.r1b1
m.r2b2<- mean(r2b2)
m.r2b2
se.r2b2<- sqrt(var(r2b2))
se.r2b2
Figure 5.10 shows two trajectories hitting the barriers. In the lower one, we see that
after 197 tosses the gambler should be ruined, but thanks to of gift of £1 he goes
on, beginning a series of bounces up to 339 tosses. Let us see the higher trajectory:
the absorbed trajectory in Fig. 5.8 is reported in Fig. 5.10, to compare it with the
reflected trajectory. Both trajectories are the results of the same history, i.e. the same
initial values and the same random numbers are employed. The absorbed trajectory is
overlapped on the reflected one for the first 259 tosses, after that it changes no more,
while the reflected trajectory continues its race, reaching the barrier again after 273
tosses and one more time after 395 tosses. Barriers may also be partially reflecting, that
0 10 20 30 40
number of tosses
Sn 
(gambler’s capital
)
0 50 150 250 350
Fig. 5.10 Three restricted paths out of 100, of a random walk with barriers at 0 and 40.
Reflected trajectories: thinner lines; absorbed trajectory: thicker line.
is the reflection is not certain, but only probable. The matrix of transition probabilities
is:162 Random Walk


q p 0 0 · · · 0 0 0
q 0 p 0 · · · 0 0 0
0 q 0 p · · · 0 0 0
. . . . . . . . . . . . . . .
0 0 0 0 · · · q 0 p
0 0 0 0 · · · 0 q p


In gambling language this situation may be interpreted as mentioned at the end of the
previous section. The house, or the adversary, gives or does not give £1 to the ruined
gambler allowing him to play again. If the gambler receives no money, he continues
nonetheless to toss a coin, waiting for £1. Similar observations can be made when the
gambler has won £a.
In the Code_5_4.R, reflection is implemented by the lines:
if(x[j-1] == b1) x[j]<- b1-1
if(x[j-1] == b2) x[j]<- b2+1
become:
if(x[j-1] == b1) {u<-runif(1,0,1)
if(u>c_refl){x[j]<- b1-1}
else{x[j]<- b1} }
if(x[j-1] == b2) {u<-runif(1,0,1)
if(u>c_refl){x[j]<- b2+1}
else{x[j]<- b2} }
where we have introduced the coefficient of reflection c_refl, that is the probability
that the trajectory reaching the barrier will be reflected (the game goes on). Clearly, it
is 0 6 c_refl 6 1. If c_refl = 0, the barrier is perfectly reflecting, if c_refl = 1, the
barrier is absorbing. Figure 5.11 shows four examples of histories with c_refl = 0.2
(dotted line) and c_refl = 0.8 (solid line). At the barrier 0, we see that the trajectory
with c_refl = 0.8 (solid line) has a series of bounces from about 131 to 258 tosses,
while the trajectory with c_refl = 0.2, relative small probability of reflection (dotted
line), reaches the barrier after 355 tosses and remains on the barrier up to 400 tosses,
except some isolated small reflections. At the barrier a = 40, the trajectory with
c_refl = 0.8 (solid line) has bounces from about 227 to 360 tosses, while the trajectory
with c_refl = 0.2 (dotted line) stays on the barrier from about 303 to 326 tosses.
Let us find now the invariant distribution of a Markov chain with partially reflecting
barrier. Consider, for example, a chain with four states:
P =


0 1 2 3
0 q p 0 0
1 q 0 p 0
2 0 q 0 p
3 0 0 q p


where:
p01 = p12 = p23 = p33 = p
p00 = p10 = p21 = p32 = 1 − p = q
then P can be written:Definitions and examples 163
0 10 20 30 40
number of tosses
Sn 
(gambler’s capital
)
0 50 150 250 350
Fig. 5.11 Four restricted paths out of 100, of a random walk with partially reflected barriers
at 0 and 40. Dotted line: c_refl = 0.2; solid line: c_refl = 0.8
P =


0 1 2 3
0 p00 p01 p02 p03
1 p10 p11 p12 p13
2 p20 p21 p22 p23
3 p30 p31 p32 p33


There are entries equal to 0, for instance:
π2 p20 = 0, π3 p30 = 0, etc.
So we have:
π0 = π0 p00 + π1 p10
π1 = π0 p01 + π2 p21
π2 = π1 p12 + π3 p32
π3 = π2 p23 + π3 p33
With some simplification, we have:164 Random Walk
π1 =
(1 − p00) π0
p10
=
p01
p10
π0 =
p
q
π2 =
p01 p12
p10 p21
π0 =
p · p
q · q
=
p
2
q
2
π3 =
p01 p12 p23
p10 p21 p32
π0 =
p · p · p
q · q · q
=
p
3
q
3
with: π0 + π1 + π2 + π3 = 1
In general, when the number of states is finite, it gives:
πk ∝

p
q
k
(5.7)
If we have a countably infinite number of states, we may speak again of a ‘transition
matrix’, but it is an infinite matrix. For instance, the transition matrix of the chain
with partially reflecting barrier, if the states are numbered 0, 1, 2, . . . , is:
P =


q p 0 0 0 . . . 0 0 0 . . .
q 0 p 0 0 . . . 0 0 0 . . .
0 q 0 p 0 . . . 0 0 0 . . .
0 0 q 0 p . . . 0 0 0 . . .
. . . . . . . . . . . . . . . . . . . .
0 0 0 0 0 . . . q 0 p . . .
. . . . . . . . . . . . . . . . . . . .


If the number of states is infinite, the above equation eqn (5.7) holds again, but for
k → ∞, there are three possible cases.
• p > 1/2. In that case:
πk ∝

p
1 − p
k
and since p/(1 − p) > 1:
πk ∝

p
1 − p
k
−−−−→ n→∞
∞
then there is no invariant distribution. All the states are transient, at that situation is
possible since the chain is not finite.
• p < 1/2. In this case, the states are positive recurrent. Let c be the constant of
proportionality, which is:
πk = c

p
1 − p
k
Let:
θ =

p
1 − p
Definitions and examples 165
it becomes (geometric series):
X∞
k=0
θ
k =
1
1 − θ
, since θ < 1
If it has to be P∞
k=0 πk = 1, c has to be:
X∞
k=0
c θk = 1, that is c
1
1 − θ
= 1
from which:
c = 1 − θ =
1 − 2p
1 − p
Then, in conclusion:
πk = (1 − θ) θ
k =
1 − 2p
1 − p

p
1 − p
k
• p = 1/2. In this case, the states are recurrent, but null recurrent. It gives:
πk = c

1/2
1/2
k
= c
But it must be:
X∞
k=0
πk = 1, that is X∞
k=0
c = 1
which is not possible.
5.1.4 Two-dimensional random walk
The walker, whatever direction of origin is, can go towards north (N), south (S), east
(E) or west (W). Assuming a symmetric random walk, the four directions have the
same probability, to be chosen equal to 1/4, as sketched in Fig. 5.12. The transition
probabilities are:
(
pij = 1/4 if |i − j| = 1
0 otherwise
Each position has four neighbours: (+1, +1),(+1, −1),(−1, +1),(−1, −1). At each
time point, the walker takes a step in one of the above positions, with the same prob￾ability equal to 1/4. We can ask ourselves firstly whether the process is irreducible,
that is if all states are recurrent, or more simply if the walk is recurrent. The answer
goes back to a theorem proved by G. P´olya (1921) which says that a simple random
walk in one and two dimensions returns to its starting position with probability 1, but
if the dimensions are greater than or equal to three, then with positive probability, the
walk will never return where it started from. We can say equivalently that a walker
may start from any point, but certainly he will reach the origin, or pass through some166 Random Walk
N
S
Y
W
X
E
1/4
1/4
1/4
1/4
Fig. 5.12 Symmetric random walk in the plane. All the four directions have the same prob￾ability 1/4.
other possible points infinitely often. Feller (1970) cites the old saying ‘all roads lead
to Rome’. A further equivalent formulation of the theorem is that two walkers (in one
and two dimensions) are certain to meet infinitely often.
To show that in two dimension the walk is recurrent, it is useful to look at the
process as two one-dimensional processes, as sketched in Fig. 5.13. The trajectory of
y'
x'
X'
Y'
(x
n
, yn) X
Fig. 5.13 Orthogonal projection on the X
0
and Y
0
axes of some steps of a trajectory taking
place on the (X, Y ) plane.
the walk described in the (X, Y ) plane is projected onto orthogonal axes X0 and Y
0
rotated by π/4. The unit steps in the new coordinates become:
(X, Y ) axes, (X0
, Y 0
) axes
± (1, 0) ± (1/
√
2) (1, 1)
± (0, 1) ± (1/
√
2) (−1, 1)
For instance, after six steps the trajectory is in the point (x6 = 3, y6 = −1), whichDefinitions and examples 167
is projected in the points x
0 and y
0
. In this way, the two-dimensional random walk
is reconstructed in a pair of one-dimensional independent symmetric simple random
walks. Now if the walk returns to the origin (0, 0) in the (X, Y ) plane, this means that
it returns to 0 in the X0 and Y
0 axes. Then:
p
(2n)
00 =

(2n)!
n!n!
1
2
(2n)
2
≈
1
πn
then:
X
n>1
p
(2n)
00 = ∞
and the walk is recurrent. With a similar approach it is proved that in three dimensions
(or more) the walk is transient.
A further result obtained by considering the projection of the walk along the X- and
Y -axes concerns the estimate of the final distance reached by the walker (see Peterson
and Noble (1972) and also for extended applications Bertozzi (2008)). Let us look at
Fig. 5.14. The walker starts at the origin and takes a step length l1 up to the point 1.
final dist.
3
2
x
2
y
1
y
2
y
3
Y
x
1 x
3
l1
l2
l3
l
X
Fig. 5.14 Three step trajectory with the arrival points projected on the X- and Y -axes, the
final distance df is also indicated.
The projections of this point on the X- and Y -axes are x1 and y1. The walker takes a
second random step length l2 (not necessary equal to l1) and the new coordinates are
X = x1 + x2, where x2 is the projection of l2 on the X-axis. Similarly, Y = y1 + y2.
The point 3 with coordinates (x3, y3) determines the final distance d
2
f = x
2
3 + y
2
3 of
this three step trajectory, given by:
d
2
f =(x
2
1 + 2x1x2 + 2x1x3 + · · · + x
2
2 + 2x2x3 + · · · + x
2
3
) +
(y
2
1 + 2y1y2 + 2y1x3 + · · · + y
2
2 + 2y2y3 + · · · + y
2
3
)
The final distance after n steps is then d
2
f = X2 + Y
2
, where:
X2 = (x1 + x2 + x3 · · · + xn) and Y
2 = (y1 + y2 + y3 · · · + yn)168 Random Walk
By performing the calculations and considering that for n large positive x and y values
balance the negative ones, it gives:
d
2
f = x
2
1 + y
2
1 + x
2
2 + y
2
2 + x
2
3 + y
2
3 + · · · + x
2
n + y
2
n
so that for each li
, it is l
2
i = x
2
i + y
2
i
. In this way d
2
f = l
2
1 + l
2
2 + l
2
3 + · · · + l
2
n or:
df =
vuutXn
i=1
l
2
i
If the step length is the same, it is:
df =
vuutXn
i=1
l
2 = l
√
n and for unit step df =
√
n
the same as for the one-dimensional case.
The following Code_5_5.R simulate 5000 histories with 1000 steps each, for a two￾dimensional random walk. Note firstly that each neighbour xdir and ydir, in the four
direction, is chosen by a random number u.
## Code_5_5.R
# Two-dimensional random walk
set.seed(1)
nsteps<- 1000
nhists<- 5000
distf<-matrix(,nhists,nsteps)
dist<-matrix(,nhists,nsteps)
dist_max<-matrix(,nhists,nsteps)
dist_min<-matrix(,nhists,nsteps)
dist0<-matrix(,nhists,nsteps)
rmax<- 10 # max dist about 100
xf<-matrix(,nhists,nsteps)
yf<-matrix(,nhists,nsteps)
for (i in 1:nhists) { # starting loop on histories
xdir<-0
ydir<-0
x<-vector()
x[1]<-xdir
y<-vector()
y[1]<-ydir
khist<- vector()
for (j in 1:(nsteps-1)) { # starting loop on steps
u<-runif(1,0,1)
if(u<=0.25) {xdir<-xdir+1}
if(u>0.25 & u<=0.5) {xdir<-xdir-1}
if(u>0.5 & u<=0.75) {ydir<-ydir +1}
if(u>0.75) {ydir<-ydir-1}
x[j+1]<-xdir
y[j+1]<-ydir
} # ending loop on steps
if(i==3){ # to plot this history as example
xmin<- -45
xmax<- 45Definitions and examples 169
ymin<- -45
ymax<- 45
plot(x,y,type="l",xlab="x",ylab="y",main=" ",lwd=1,
col="black",xlim=range(xmin:xmax),ylim=range(ymin:ymax),
font.lab=3,cex.lab=1.4)
}
if(i==15){lines(x,y,col="black",lty=1,lwd=2)} # to plot this history as example
dist[i, ]<- sqrt(x**2 + y**2)
#print(dist[i, ]) # uncomment to print dist
for (j in 1:nsteps) { # instructions for the minimum distances
if(dist[i,j]>rmax) {
break }
khist[i]<- j
}
dist0[i,khist[i]:nsteps] <- dist[i,khist[i]:nsteps]
dist_min[i, ]<- min(dist0[i,khist[i]:nsteps])
dist_max[i, ]<- max(dist[i, ])
distf[i, ]<- sqrt(x[nsteps]**2 + y[nsteps]**2)
xf[i, ]<- x[nsteps]
yf[i, ]<- y[nsteps]
} # ending loop on histories
xfmin<- -80
xfmax<- 80
yfmin<- -80
yfmax<- 80
nfpoint<- 500
plot(xf[1:nfpoint],yf[1:nfpoint],type="p",xlab="x",ylab="y",
main=" ",lwd=1, col="black",xlim=range(xfmin:xfmax),
ylim=range(yfmin:yfmax), font.lab=3,cex.lab=1.4,pch=19,cex = .7)
abline(v=0,lwd=2,lty=4)
abline(h=0,lwd=2,lty=4)
text(70,-70,"IV",cex=1.1,font=2)
text(-70,-70,"III",cex=1.1,font=2)
text(-70,70,"II",cex=1.1,font=2)
text(70,70,"I",cex=1.1,font=2)
dist_f<-distf[1:nhists]
#dist_f # uncomment to print dist_f
dist_max<- dist_max[1:nhists]
#dist_max # uncomment to print dist_max
dist_min<- dist_min[1:nhists]
#dist_min # uncomment to print dist_min
xf_f<-xf[1:nhists]
#xf_f # uncomment to print x_f
yf_f<-yf[1:nhists]
#yf_f # uncomment to print y_f
xf_fm<- max(abs(xf_f))
xf_fm
yf_fm<- max(abs(yf_f))
yf_fm
nq1<- sum(xf_f >= 0 & yf_f >= 0)
nq1
nq2<- sum(xf_f >= 0 & yf_f < 0)
nq2
nq3<- sum(xf_f < 0 & yf_f < 0)
nq3
nq4<- sum(xf_f < 0 & yf_f >= 0)
nq4
opar <- par(lwd=2)170 Random Walk
hist(dist_f,freq=F,main="",xaxt="none",
xlim=range(0:100), ylim=c(0,0.035),right = TRUE,
xlab="distance",ylab="density",font.lab=3,cex=1.1,
cex.lab=1.2,lwd=1.5 )
axis(1, at=seq(-10,90,20))
m_distf<-mean(dist_f)
v_distf<-var(dist_f)
m_distf
v_distf
sd_distf=sqrt(v_distf)
sd_distf
curve (dnorm(x, mean=m_distf, sd=sd_distf),
lwd=1, add=TRUE, yaxt="n")
opar <- par(lwd=2,lty=2,font.lab=3)
hist(dist_max,freq=F,xaxt="none",
right = TRUE,add=TRUE)
text(63,0.03,"max distance")
text(10,0.03,"final dist.")
m_distm<-mean(dist_max)
v_distm<-var(dist_max)
m_distm
v_distm
sd_distm=sqrt(v_distm)
sd_distm
opar <- par(lwd=2)
hist(dist_min,freq=T,main="",xaxt="none",
xlim=range(0:10), ylim=c(0,1500),right = TRUE,
xlab="min distance",ylab="counts",font.lab=3,cex=1.1,
cex.lab=1.2,lwd=1.5 )
axis(1, at=seq(0,10,2))
# plot=F to print a list of breaks and counts
hist_f<- hist(dist_f,plot=F)
hist_f
hist_max<- hist(dist_max,plot=F)
hist_max
hist_min<- hist(dist_min,plot=F)
hist_min
−40 −20 0 2 0 4 0
−40 −20 0 2 0 4 0
x
y
Fig. 5.15 Two trajectories 1000 steps long of a two-dimensional random walk.Definitions and examples 171
Figure 5.15 shows, as examples, two trajectories in the (X, Y ) plane, both starting at
(0, 0). The plane is divided in four quadrants and in each of them the final points of
the trajectories are recorded. The results confirm that the trajectories arrive on the
plane with a constant probability equal to 1/4. The code gives the following number
of the final points: 1287, 1280, 1201, 1232. Figure 5.16 shows the final points of 500 out
of 5000 trajectories.
−50 0 5 0
−50 0 5 0
x
y
III IV
II I
Fig. 5.16 Final points of 500 out of 5000 trajectories 1000 steps long of a two-dimensional
random walk.
For each walk, the maximum distance from the origin (dist_max) and the final dis￾tance (distf) are computed. By ‘final distance’ we mean the distance from the origin
to the last step n (nsteps) of the trajectory. Also the minimum distance (dist_min)
is computed, but requires a small discussion later. The estimated mean of the 5000
final distances is m_distf= 28.00, in agreement with √
n (n = 1000), and the stan￾dard error sd_distf = 14.75. For the maximum distance, it is m_distm = 36.80, and
sd_distf = 12.39. Figure 5.17 shows the distributions of the final and the maximum
distances. The histograms are normalized for comparison with the probability density
function of the normal random variable, (dnorm(x, mean=m_distf, sd=sd_distf)).
To estimate the minimum distance (dist_min) consider Fig. 5.18 where a trajectory
1110 steps long is plotted.
The trajectory starts at the point (0, 0) and it remains very close to the origin for a
few steps. At step = 36, for instance, the trajectory returns to (0, 0), so that dist_min
= 0. But can we really say that the walk returns where it started from? We ask the
walker to move far enough and only after we may say that he returned actually to
the origin. So we have introduced a circle of radius rmax equal to 10 and consider as
‘minimum distance’ only the distances of the trajectories that have gone outside that
circle. Of course a trajectory, after moving outside the circle, may return inside. The
value 10 is somewhat arbitrary, but checks performed with reasonably different values
do not invalidate such an approach. From the output of the code, it results that at step
36 the final distance is zero, but only after 230 steps do the distances become stably172 Random Walk
distance
density
0.000 0.010 0.020 0.030
10 30 50 70 90
final dist. max distance
Fig. 5.17 Distributions of the final (continuous line) and the maximum (dotted line) dis￾tances of 5000 trajectories 1000 steps long of a two-dimensional random walk. Comparison
with the probability density function of the normal random variable is also shown.
−30 −20 −10 0 1 0 2 0 3 0
−30 −20 −10 0 1 0 2 0
x
y
max dist.
final dist.
Fig. 5.18 Trajectory 1110 steps long of a two-dimensional random walk. The final distance
= 23.19 and the maximum distance = 32.65 are marked. For the circle see text.
greater than rmax. At step = 988 an actual first return to the origin takes place. The
distribution of the minimum distances are reported in Fig. 5.19. Note that the 30%
of dist_min is in the interval (0, 10], and by increasing the number of steps to 5000,
the percentage becomes 43%, as the walker has more possibilities to find the road toSome topics on Brownian motion 173
min distance
counts
0 500 1000 1500
0 2 4 6 8 10
Fig. 5.19 Distributions of the minimum distances of 5000 trajectories 1000 steps long of a
two-dimensional random walk.
home.
5.2 Some topics on Brownian motion
The term ‘Brownian motion’ has physical origins: it refers to the disordered movement
of small particles as a consequence of collisions. Let X(t) be the position of a particle
at time t. Now t ∈ [0, ∞) and X(t) takes values on the real line. It is assumed that:
1. X(0) = 0, i.e. at time t = 0 the particle is conventionally in 0. Otherwise, if
X(0) = x0 6= 0, x0 is taken as initial point.
2. The process is ‘completely random’.
The last item is quoted from Lawler (1995). Let us take two times s and t, with s < t.
Here ‘random’ means that the positions xs and xt are independent of each other and,
furthermore, that the motion of the particle after time s, X(t) − X(s), is independent
of the ‘past’ xi
, i < s, and also of the ‘present’ xs. The particle is at X(s) = xs, it
moves to X(t) = xt, then the step xt − xs is independent of trajectories inside the
interval [0, s], s included, provided that X(s) = xs. This assumption must hold for
every si and ti
, i = 1, . . . , n. If:
0 6 s1 < t1 6 s2 < t2 6 · · · 6 sn < tn < ∞
then the random variables:
X(t1) − X(s1), X(t2) − X(s2), . . . , X(tn) − X(sn)
are jointly independent. These independent increments are also stationary increments.
This means that, for any 0 < s, t < ∞, the distribution of the increment X(t+s)−X(s)
has the same distribution as X(t) − X(0) = X(t).
Recall also that the Poisson process has independent and stationary increments,
but their distribution is quite different. For the Poisson process it is required that the
sample path is a stepped motion. For the Brownian motion, we require a continuous
path, that is X(t) must be a continuous function of t.174 Random Walk
Formally, a stochastic process X(t) is a Brownian motion or a Wiener process with
variance σ
2
if:
1. X(0) = 0
2. The process X(t) has stationary increments. The distribution of any increment
X(t) − X(s) only depends on the length of the time interval (t − s).
3. The process X(t) has independent increments. For any non-overlapping time in￾tervals (t1, t2] and (t3, t4], the random variables X(t2) − X(t1) and X(t4) − X(t3)
are independent.
4. ∀s < t, the increment X(t) − X(s) has a normal distribution with mean 0 and
variance (t − s)σ
2
.
5. The trajectories of a Brownian motion are continuous.
Some comments are in order. If σ
2 = 1 the motion is called standard Brownian
motion.
Point 1 is conventional. X(0) can assume any other value.
Point 2 is a consequence of time homogeneity. The dynamics of a particle in the
time interval (t − s) depends only on the length of the interval, not where it is on the
time axis.
Point 3 represents a physical assumption. It means that the time intervals are much
larger than the ‘infinitesimal’ interval between the collisions of the particles.
Point 4 is a redundant condition. It can be proved that a stochastic process with
stationary and independent increments, having also a continuous sample path is a
Brownian motion with normally distributed increments. Similarly in discrete state
space, the Poisson process is the unique counting process that has stationary and
independent increments.
Point 5 is essential, since we have to describe the trajectory of a physical particle
as a function of time.
Remark 5.1 To say ‘Brownian motion’ is to say ‘history of Physics’, ’philosophy of probability’,
‘mathematics achievements’, and more. Galavotti (2005) writes:
The process by which probability gradually entered physical science, not only in connection
with errors of measurement, but more penetratingly as a component of physical theory, can be
traced back to the work of the Scottish traveller and botanist Robert Brown (1773-1858).
Our botanist (Brown, 1828) in 1827 demonstrated that any small particle suspended in a fluid,
even an inorganic grain, is subject to an incessant and irregular motion (Brown, 1828). Actually,
explanations of such motion did not enter the field of biology, rather that of physics, or even
economics. In physics, theories were advanced by Einstein in 1905 and Marian von Smoluchowski
in 1906. These theories were confirmed by a series of experiments on fluctuation phenomena. It
is interesting from an historical and philosophical perspective that these theories are founded on
different bases (see the comprehensive von Plato (1994), also Brush (1968)). Einstein writes at
the beginning of his (Einstein, 1905):
In this paper it will be shown that according to the molecular-kinetic theory of heat, bodies of
microscopically-visible size suspended in a liquid will perform movements of such magnitude
that they can be easily observed in a microscope, on account of the molecular motions of
heat. It is possible that the movements to be discussed here are identical with the so-called
‘Brownian molecular motion’
and says: ‘an exact determination of actual atomic dimensions is then possible’.Some topics on Brownian motion 175
In his doctoral dissertation written before the paper quoted above, he determines the diffusion
coefficient D as:
D =
kBT
6πkr
where kB is the Boltzmann’s constant, T the absolute temperature, k the viscosity coefficient,
and r the radius of Brownian particles (that is the diffusing molecules).
Reasoning on the basis of statistical mechanics, with some probabilistic assumptions, (for
instance, independency of the movements of particles), he gives the probability density function
of a Brownian particle satisfying the following diffusion equation:
∂f(x, t)
∂t = D
∂
2
f(x, t)
∂x2
whose solution is:
f(x, t) = 1
√
4πDt
exp(−x
2
/4Dt)
Einstein suggests that the mean square displacements of the particles, h∆x
2
(t)i = 2Dt being
related to their diffusion coefficient D, rather their velocities, are observable and measurable
quantities.
A probabilistic account of the Brownian motion was put forward by Smoluchowski (1906)
based on the following assumptions, which are the same as for random walk, using the current
terminology. The particle velocity remains constant, and very small changes in direction occur at
each collision. Successive collisions are independent of each other, the length of the steps are all
the same. von Plato (1994, 129-130) writes:
The individual motions are described as consisting of a succession of nearly linear parts with
randomly changing direction at points of collision. The random changes occur at discrete
intervals. Being based on assumptions about collisions, it is an approach in the style of kinetic
theory, whereas Einstein’s theory has the abstract character of statistical mechanics.
In 1900 Louis Bachelier, independently of Brown’s work, developed a mathematical
theory of Brownian motion, anticipating the theories by Einstein (1905) and Smolu￾chowski (1906). Bachelier submitted his work in his PhD dissertation, with H. Poincar´e
as examiner (Bachelier, 1900). One of his achievement was to grasp the Markovian
character of the Browning motion: to give an idea, the future prices of assets are in￾dependent from information of past prices. His analysis of markets allowed him to
advance a theory for continuous time random process with independent increment.
5.2.1 Brownian motion as limit of random walks
Consider the symmetric random walk:
Sn = X1 + X2 + · · · + Xn
The Xi
’s are independent random variables such that:
P {Xi} = 1 = P {Xi} = −1 = 1/2
so Sn is the position of the particle after n steps. Recall that:
E [Sn] = Xn
k=1
E [Xk] = 0 Var [Sn] = Xn
k=1
E

X2
k

= n
Divide the half-line into intervals of length δ (see Figure 5.20):176 Random Walk
0 δ 2δ 3δ 4δ 5δ 6δ … t = nδ
Fig. 5.20 The half-line [0, ∞) is divided into intervals of length δ.
The time unit for each step is δt, that is in each interval [(j − 1)δ, jδ], the particle
moves forwards or backwards. Suppose δx to be the length of the step to the left or
to the right. After n × δt time, St will be the position of the particle at time t. Then:
St = δx (Xδt + X2δt + · · · + Xnδt)
To sum up: t is the time elapsed, δt is the time unit, then t/δt is the number of the
taken steps. It gives:
E [St] = 0 Var [St] = t
δt (δx)
2
But if δx = δt → 0, St → 0 and the motion of the particle will become meaningless.
The idea is to choose δx proportional to √
δt, that is δx = c
√
δt. Define Xi as (see
Figure 5.21):
Xi =
( √
δt with P = 1/2 to the right
−
√
δt with P = 1/2 to the left
The motion begins at (0, 0), and after one time step δt, the particle is in the point
−
√
δt; after two steps 2δt, it is at 0, and so on. With this choice, the size of an increment
is:
|Xt+δt − Xt| ≈ √
δt
and, if δt → 0, √
δt → 0, thus fulfilling the request of the continuity of the trajectories.
If c = 1 (standard motion), we have:
E [St] = 0 Var [St] = t
Applying the central limit theorem to a sequence of i.i.d. random variablesSome topics on Brownian motion 177
δt
-3 √δt
2δt 3δt
time
position
4δt 5δt 6δt
-2 √δt
- √δt
√δt
2√δt
3√δt
Fig. 5.21 Sketch of the construction of a Brownian motion from a limiting case of a random
walk.
X1, X2, . . . , Xn
with finite mean µ, finite non-zero variance σ
2
, and Sn =
Pn
i=1 Xi
, we have:
Zn =
Sn − E [Sn]
p
Var [Sn]
=
Sn − nµ
σ
√
n
For n → ∞, this converges in distribution to a standard normal random variable
N (0, 1). In our case, µ = E [Sn] = 0 and σ
2 = Var [Sn] = 1 and we can write:
St − E [St]
d
−−−−→ n→∞
N (0, t)
Then ∀s > 0 and ∀t > 0:
Xs+t − Xs ∼ N (0, t)
with ∀t > 0, Xt ∼ N (0, t).
The following short code, Code_5_6.R, illustrates how to think intuitively of the
Brownian motion as a limiting case of a random walk as its time increment approaches
zero. The first part of the code is the as same already presented in Code_5_1.R.
## Code_5_6.R
set.seed(3)
x0<- 0
nsteps<-100
p<-0.5
x<-rbinom(nsteps,1,p)
x<- 2*x-1
y<- cumsum(x)
Sn<- c(x0,y+x0) # adding the starting point x0178 Random Walk
n<- 0:nsteps # time axis
par(mai=c(1.02,1.,0.82,0.42)+0.1) # to control the margin size
plot (n,Sn,type="l",xlab="time",ylim=c(-15,7),lty=3,
ylab=expression(italic("S"[t] )~~ italic((position)) ),lwd=4,
cex.lab=1.2,font.lab=3,xaxt="none" )
axis(1, at=seq(0,nsteps,20))
set.seed(3)
dt<- 0.01
x0<- 0
y<-numeric()
y[1]<- x0
n<- 10000
t<- 100
for(i in 2:(n+1)) {
y[i]<- y[i-1] + rnorm(1,mean=0,sd=sqrt(dt))
}
lines(seq(0,t,dt),y,type="l",lty=1,lwd=1)
−15 −10 −5 0 5
time
St (position
)
0 20 40 60 80 100
Fig. 5.22 Brownian motion from limiting case of random walk. Dotted lines: sample path
of a random walk. Continuous line: sample path of a Brownian motion.
With δt → 0 the segmented line approximates to a continuous curve, that is; the
process becomes a continuous-time Markov process in a continuous state space. In
Fig. 5.22 the x-axis label is denoted as ‘time’, not ‘number of steps’, in view of the
limit continuous approximation.
The trajectories of a Brownian motion {X(t)} possess some peculiarities:
i ) They have infinite length in any finite time interval.Some topics on Brownian motion 179
ii ) They are not differentiable anywhere. We imagine that the particle moves more
and more in different directions in each infinitesimal interval.
The proof of the above statements requires advanced mathematics, but an intuitive
idea might be the following. For item i), we have seen that in the interval [0, t], t/δt
is the number of the steps taken, each of length √
δ, so the total path length λ is
λ = t/√
δ, which is:
lim
δ→0
t
√
δ
= +∞
For item ii), We should compute the derivative dXt/dt, that is:
dXt
dt = lim
∆t→0
dXt+∆t − Xt
∆t
This limit does not exist, since we have already seen that the numerator |Xt+δt − Xt|
is on the order of √
∆t,  ∆t.
The standard Brownian motion is a time-homogeneous Markov process, since, as
said above, for the independent increments property, the increment X(s + t) − X(s)
does not depend on the past time before time s. So, the future time X(s+t), given the
present state X(s) only depends on X(s+t)−X(s). Moreover, since the increments are
also stationary, the process is a time-homogeneous Markov process. Being a Markov
process, the Brownian process also has some properties that hold in discrete time.
The random variable T with values in [0, ∞) is a stopping time for Brownian
motion X(t) if the event {T 6 t} only depends on {X(s), 0 6 s 6 t}, but not on
{X(s), s > t}.
Let Y (t) be the process after the stopping time T, that is: Y (t) = X(t+T)−X(T),
then Y (t) is also a standard Brownian motion. The strong Markov property says that
Y (t) is a Brownian motion that does not depend on T, that is:
∀t > 0, {Y (s), 0 6 s 6 t} is independent of {X(u), 0 6 u 6 T}
Now we are interested in the first passage times. Following Ross (2019), let Ta be
the first time the standard Brownian motion hits a, a > 0. We compute the probability
of the event {Ta 6 t}, that is P {Ta 6 t}, and the probability P {X(t) > a}, conditioned
on the occurrence or not of the event {Ta 6 t}. We have:
P {X(t) > a} = P

X(t) > a

Ta 6 t
	
P {Ta 6 t}
+ P

X(t) > a

Ta > t	
| {z }
=0
P {Ta > t}
The second right-hand term is equal to 0, since the term between curly braces is
zero. Indeed, the path of the motion is continuous, so the process cannot go beyond a
without having yet hit a. For the first addendum, if Ta 6 t, the process has hit a at
some instant in [0, t] and, by symmetry, it is just as likely to be either above or below
a at time t, that is:
P

X(t) > a

Ta 6 t
	
=
1
2180 Random Walk
Then the partition function of the random variable Ta is given by:
P {Ta 6 t} = 2 Z ∞
a
1
2πt
exp −
x
2
2t
dx and putting x/√
t = u:
=
2
√
2π
Z ∞
a/√
t
exp −
u
2
2
du
It follows:
P {Ta < ∞} = limt→∞
P {Ta 6 t} = 2 Z ∞
0
N (0, 1) du = 1
therefore the path hits the point a with probability 1. From the above equation, we can
derive the expected value of the first passage time E [Ta] which results to be infinite.
Then, it is true that the process will hit any point a with certainty, but it is equally
true it needs an infinite time to arrive to a, however close it is.
From the notion above, we introduce the reflection principle, as follows. Let Ta
be the first passage time to the value a of the Brownian process X(t), t > 0. Let us
consider the new process Y (t):
(
Y (t) = X(t), t < Ta
Y (t) = 2a − X(t), t > Ta
From the strong Markov property, it can be proved that Y (t) is also a standard Brown￾ian motion. In other words, the reflection principle states that if the path of a Brownian
process X(t) reaches the value a at time t = Ta, then the path after Ta has the same
distribution as the path reflected from the horizontal line X(t) = a. Figure 5.23 shows
the underlying idea of the reflection principle. Note that the paths are segmented as
for a discrete time stochastic process. position
a
0
0
time
T
a
X(t)
Y(t)
Fig. 5.23 Sketch showing the basic of the reflection principle. Continuous line: sample path
of a Brownian motion. Dashed line: portion of the reflected Brownian motion after Ta.Some topics on Brownian motion 181
From the reflection principle, the probability that the process hits the origin, that
is it crosses the x-axis at some time s, with 1 6 s 6 t, is (see Lawler 1995):
P {X(s) = 0, for some 1 6 s 6 t} = 1 −
2
π
arctan
1
√
t − 1
(5.8)
For t → ∞ such quantity → 1, so that the process returns to the origin infinite times
(strong Markov property), and the motion assumes positive and negative values for
large t, Let us see what happens near t = 0. Consider the process Y (t) = t X(1/t).
Also, Y (t) is a standard Brownian motion, therefore when t → ∞ in the process
X(t), we have t → 0 in the process Y (t). Since the process X(t), for arbitrarily large
t, assumes positive and negative values, Y (t) assumes arbitrarily small positive and
negative values. Then in any interval near the origin, the Brownian motion assumes
positive and negative values and, for continuity also 0.
Consider the ‘random set’ Z defined as:
Z = {t : X(t) = 0}
and also Z1 = Z
T
[0, 1]. We wish to cover Z1 by intervals of length  = 1/n of the
type:
h
k − 1
n
,
k
n
i
, k = 1, 2, . . . , n
How many n intervals do we need to cover Z1? Before answering, a brief digression on
the concept of ‘fractal dimension’ seems appropriate. For a more extended discussion
of such an argument, see Huffaker et al. (2017) and references therein.
A line segment of length L can be covered with another segment of the same length,
or with two segments of length L/2, or with four segments of length L/4, etc. If N()
is the number of segments of length  covering the segment of length L, it is:
N() = L
1

Analogously when we wish to cover a square of side L, it needs:
N() = L
2
1

2
squares of side . In d dimension we have:
N() = L
d
1

d
where  is the side of the d-dimensional hypercube. Taking the logarithms, we have:
d =
log N()
log L + log(1/)
If  → 0, we can formulate the definition of the Hausdorff-Besicovitch dimension
(H–B) as:
DHB = lim→0
log N()
log(1/)
(5.9)
For a line segment, for a surface, and analogously for a cube and a hypercube, the
H–B dimension coincides with the usual Euclidean dimension, but let us see when it182 Random Walk
is applied to the fractal subset of [0, 1], that is to the Cantor set, or more exactly the
middle third Cantor set.
The rules of the game to construct the Cantor set are the following. At the begin￾ning the unit interval, say I0, is divided into three equal parts and the middle part is
deleted, that is the open interval (1/3, 2/3) is cancelled, so remains the set of points
I1 = [0, 1/3] S
[2/3, 1]. The same procedure is executed in the successive iterations.
Then, I2 consists of four segments, each of length 1/9, and in general In consists of 2n
segments, each of length (1/3)n. As the process moves forwards the number of small
segments increases, but their length decreases. The geometrical figure so created, the
middle third Cantor set, is formally defined as:
C = limn→∞
In =
\∞
n=0
In
The set C is the set of points that belong to all In, ∀n. Without going into more detail,
let us compute the H–B dimension on C. Clearly, we can cover the initial interval
I0 = [0, 1] by N() = 1 segment of length  = 1 and the interval I1 = [0, 1/3] ∪ [2/3, 1]
by N() = 2 segments of length  = 1/3. In this manner, we arrive at:
DHB = lim→0
log N()
log(1/)
= lim→0
log 2n
log 3n
=
log 2
log 3 ≈
0.6931
1.0986
≈ 0.63
which is not an integer number, that is to say DHB is a fractional dimension, and we
say that the Cantor set is a fractal. It is a geometric figure consisting of infinite points,
but its dimension is less than 1, so to speak, ‘a little lesser’ than a segment (dimension
1), and ‘a little greater’ than a point (dimension 0).
Turning to the set Z1 defined before, to cover it we must have:
Z1
\ h
k − 1
n
,
k
n
i
6= ∅
or, in other words, searching for:
P(k, n) = P
h
k − 1
n
,
k
n
i
6= 0
If k = 0, the probability is 1, since 0 ∈ Z, then we assume k > 1. If X(t) is a standard
Browning motion, then:
Y (t) = 
(k − 1)/n−1/2
X

nt/(k − 1)
is a standard Brownian motion. We have:
P(k, n) = P

X(s) = 0, for some 1 6 s 6 t
	
Now it is:
P
n
Y (t) = 0, for some 1 6 t 6
k
k − 1
oSome topics on Brownian motion 183
therefore from eqn (5.8):
P(k, n) = 1 −
2
π
arctan
1
r
k − (k − 1)
k − 1
= 1 −
2
π
arctan √
k − 1
Then the number of intervals to cover Z1 is of the type:
Xn
k=1
P(k, n) = Xn
k=1

1 −
2
π
arctan √
k − 1

By developing arctan(.) into Taylor series, the final result is:
Xn
k=1
P(k, n) = 4
π
√
n
therefore, √
n intervals of length 1/n are necessary to cover Z1, and
DBH =
log √
n
log n
=
1
2
log n
log n
Conclusion: the dimension of the set Z1 is 1/2.
Until now we have dealt with the standard Brownian motion, characterized by zero
mean µ and variance σ
2 = 1, that is ∀s < t, and the random variable X(t) − X(s) has
distribution N (0, 1). Consider now a non-zero mean process: W(t) = X(t) +µt, where
X(t) a Brownian motion and µ is the drift parameter (a stochastic process is said to
be ‘with drift’, if its mean value is not zero). With respect to a standard motion, the
difference is that the distribution of the increments is:
W(t) − W(s) ∼ N
￾
µ(t − s), σ2
(t − s)

To account for stochastic processes we have used a mathematical formulation de￾scribing the random state of the system in different times of type P {X(t) = i}. Now
we write the probabilistic law describing the motion of the system, that is the steps
between states. The process is given in differential formulation and to obtain W(t) we
have to integrate:
dW(t) = µ
￾
W(t), t
dt + σ
￾
W(t), t
dX(t)
with X(t) a standard Brownian motion. The first term on the right is a deterministic
term that stand for the expected move of the system, while the second is a stochastic
term that represents the variability of this expected value. The above equation is a
stochastic differential equation, whose solution is the ‘path’:
W(T) = Z T
0
µ
￾
W(t)

dt +
Z T
0
σ
￾
W(t)

dX(t)
For a rigorous introduction to stochastic differential equation, see Øksendal (2184 Random Walk
We simulate the process on the basis of random walk. We choose a small dt and
simulate a random walk with time increment dt and spatial increment √
dt. Then we
can write:
dW(t) = Z(t)
√
dt
If Z(t) ∼ N (0, 1), we have Z(t)
√
dt ∼ N (0, dt). The following Code_5_7.R is an ex￾ample of simulation of a stochastic differential equation (see also Iacus and Masarotto
2003).
## Code_5_7.R
set.seed(2)
n<- 1000
T<- 1
dt<- T/n
x0<- 1 # starting point
mu <-function(x,t) {-x*t}
sigma <-function(x,t) {+x*t}
w<-numeric()
w[1]<- x0
for(i in 2:(n+1)) {
t<- dt*(i-1)
w[i]<- w[i-1] + mu(w[i-1],t) * dt + sigma(w[i-1],t)*rnorm(1,sd=sqrt(dt))
plot(seq(0,T,dt),w,type="l",ylim=c(0.,2),font.lab=3,lty=1,lwd=2,
ylab="W(t)",cex.lab=1.2)
# second trajectory
set.seed(4)
w[1]<- x0
for(i in 2:(n+1)) {
t<- dt*(i-1)
w[i]<- w[i-1] + mu(w[i-1],t)*dt + sigma(w[i-1],t)*rnorm(1,sd=sqrt(dt))
lines(seq(0,T,dt),w,type="l",lty=2,lwd=2 )
We have chosen µ(x, t) = −xt and σ(x, t) = xt. Figure 5.24 shows two trajectories of
a Brownian motion with drift, obtained as a random walk with time increment dt and
spatial increment √
dt. Note that the walk begins at x0 = 1.
Finally, the motion they call Brownian has never stopped over the years. In its journey
it visited a variety of lands, statistical physics, interpretation of probability, stochastic
differential equations, entropic forces, and more. There have been several attempts
to involve Brownian motion in alternative conceptions of quantum mechanics. Also
the Feynman path integral formulation has been compared to stochastic approaches.
Robert Brown wrote (1828): ‘These motions were such as to satisfy me . . . that they
arose neither from currents in the fluid, nor from its gradual evaporation, but belonged
to the particle itself’, and Richard Feynman (1948): ‘The paths involved are, there￾fore, continuous but possess no derivative. They are of a type familiar from study of
Brownian motion.
5.3 Exercises
Exercise 5.1 A flea lives on a finite segment of a line, of length D. It is able to do jumps
towards left or right, except in the starting position x = 0 and in the ending position x = D,
i.e. the two barriers are not surmountable with a jump. The insect jumps to the left withExercises 185
probability pL, to the right with probability pR or it decides to rest where it is, with prob￾ability pS. The jump length is such that D is covered with 10 jumps in the same direction
(starting from 0). A rather limited world, poor little bugs!
(1) How can you write the transition matrix of this random walk? And, (2) What kind of
random walk is it?
Exercise 5.2 If, in the previous exercise, you take pR = 1 − pL and, consequently, pS = 0,
what does this new random walk represent? Write its transition matrix.
Exercise 5.3 What kind of random walk is represented by the following transition matrix
(q = 1 − p)?
P =


q p 0 . . . 0 0 0
q 0 p . . . 0 0 0
.
.
.
.
.
.
0 0 0 . . . q 0 p
0 0 0 . . . 0 q p


Exercise 5.4 Draw a graph like that in Fig. 5.1 to represent a transition matrix of exercise
5.3 for five states.
Exercise 5.5 We borrow from H. Tijms (Understanding Probability, Cambridge University
Press, 2007), the depiction of the random walk as the walk of a drunk man. A drunkard
moves on a line, at each step going to the left or to the right with equal probability. Call Sn
the distance covered by a drunkard on a line, starting from the origin (the pub exit) after n
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.5 1.0 1.5 2.0
seq(0, T, dt)
W(t)
Fig. 5.24 Two trajectories of Brownian motion with drift, obtained as a finite time approx￾imation of a random walk.186 Random Walk
steps. We have seen in Section 5.1 that for n large the average of Sn is zero, because there are
as many positive as negative steps. But nevertheless, with the passing of time, the drunkard
gets tired of walking because he actually ‘travelled’ for a distance that is
Dn = |X1 + . . . + Cn|
where X1 . . . Xn are the n steps taken by the drunkard.
(1) Numerically compute E [Dn] and E [Sn] with increasing n, for a random walk simulated
with R. (2) For n large: demonstrate that E [Dn] ≈
q2n
π
Hint: apply the central limit theorem to X1 . . . Xn
Exercise 5.6 A common approach to simulating spatial dispersal of insects with time is
to use the random walk. A two-dimensional random walk is used to simulate the degree of
insect diffusion over an area with time. Write a code to simulate a two-dimensional movement
of a female insect that starts with 100 eggs and deposits one egg at each step. Assume an
equiprobability for the movement in four directions: left, right, top, bottom. The program
ends when the female has finished the eggs. Then assume there are N females and modify the
program by setting a variable steps to oviposit that is the number of steps the insect must
take before depositing one egg. Play with the variables: number of females, number of eggs
per female and steps to ovideposit, to see how the distribution of eggs changes.
Exercise 5.7 With reference to exercise 5.6, suppose there are two species of ants: black and
red. Suppose the queens of both species have 100 eggs, and assign a different number Ns of
steps to oviposit to the species, for example Ns = 2 for black ants, Ns = 5 for red ants. On
a graph, represent black and red eggs with a coloured dot and see the behaviour of the two
distributions of eggs.
Exercise 5.8 Simulate a trajectory of a three-dimensional random walk.6
ARMA Processes
Statistics are the only tools by which an opening can be cut
through the formidable thicket of difficulties that bars
the path of those who pursue the Science of man.
Sir Francis Galton, Natural Inheritance
In this chapter we will introduce a class of stochastic processes which are of great
importance in the study of random phenomena, e.g. for time-series forecasting. It is
the subject of auto-regressive processes, moving-average processes and, more generally,
of auto-regressive moving-average (ARMA) ones. ARMA models essentially describe
a time series in terms of a combination of polynomials. The first thorough description
of such models was probably given in 1970 in the first edition of the book (Box et al.,
1994), although they are a generalization of auto-regressive models, described more
than forty years earlier by Yule (1927). As we shall see, ARMA processes are based
on white noise, which therefore deserves a short introduction.
6.1 White noise and other useful definitions
Some of the concepts introduced in this chapter will be clearer after reading Chapter
7, where the spectral analysis of stochastic processes is treated in more detail. We
anticipate here, without any proof, that a time series (as a time-dependent signal)
can also be analysed in the frequency domain, exactly as an electrical circuit or a
mechanical system can be studied in physics in terms of its time response or of its
frequency behaviour.
White noise is a simple stationary process {t}, defined as follows
E[t] = 0, ∀t ∈ N
var[t] = σ
2
, ∀t ∈ N (6.1)
corr[t, s] = 0, ∀t, s ∈ N
where the stochastic process has been assumed discrete. In other words, t is an i.i.d.
sequence of zero-mean, constant variance, uncorrelated random variables. For example,
if every t is normally distributed t ∼ N (0, σ2
), the sequence {t} is white noise.
Anyway, t is not necessarily Gaussian.
The following R example code shows how to simulate white noise.
## Code_6_1.R
#White noise188 ARMA Processes
# for reproducibility: change seed to change noise
set.seed(3)
# create noise time series
w <- rnorm(1000)
plot(as.ts(w),ylab="White noise")
# verify w is zero mean
mean(w)
var(w)
hist(w,main="",col="white")
# verify w is uncorrelated
acf(w,main="",lag=20,ylim=c(-0.2,1))
# verify w is really white
w.spec <- spectrum(w,log="no",span=200,plot=FALSE)
plot(w.spec$freq,w.spec$spec,t="l",ylim = c(0,1.2),
xlab="frequency (cycles/sample interval)",ylab="spectral density")
Figure 6.1 shows the time series generated by the R code above.
time
white noise
0 200 400 600 800 1000
−3 −2 −1 0 1 2 3
Fig. 6.1 White noise time series.
The mean and variance of the generated series are very close to 0 and 1 respectively,
as the histogram in Fig. 6.2 also shows. The acf R function allows us to verify that
data are uncorrelated (see Figure 6.3).
The reason for the ‘white’ attribute comes from the analogy with light signals. A
white light is composed of all colours or, which is the same, its frequency spectrum
contains all optical frequencies with approximately the same intensities. Indeed, by
performing a spectral analysis on the time series (see Chapter 7) we obtain what
figure 6.4 shows.White noise and other useful definitions 189
w
frequency
−2 0 2 4
0 5 0 100 150 200
Fig. 6.2 White noise histogram.
0 5 10 15 20
−0.2 0.2 0.6 1.0
lag
ACF
Fig. 6.3 White noise auto-correlation function.190 ARMA Processes
All frequencies (note, by default the frequency axis of the spectrum function is
expressed in cycles per sample interval) of the spectrum are present with comparable
intensities in the periodogram, or spectral density plot.
0.0 0.1 0.2 0.3 0.4 0.5
0.0 0.4 0.8 1.2
frequency (cycles/sample interval)
spectral density
Fig. 6.4 White noise spectral analysis.
White noise, as we will see shortly, plays a fundamental role in auto-regressive
moving-average processes.
6.1.1 The lag operator
Given a discrete stochastic process {Xt} we formally define a ‘lag’ operator L estab￾lishing a correspondence between the random variable Xt at time t and the variable Xs
at time s, a step backwards or forwards with respect to t, i.e. s = t−1 or s = t+1. The
lag operator is often named ‘backwards shift’ or ‘backshift’ operator1
, usually denoted
by B and defined by BXt = Xt−1. Choosing the backwards time, for the moment, L
is defined by:
LXt = Xt−1 (6.2)
where the ‘standard’ notation LX is not a product but actually means L(X), i.e. L
operates on X. L is linear, because:
1Lag is more general than backshift. The lag operator allows relating Xt with X values at previous
or successive times, while the backshift only considers previous times.Moving-average processes 191
L(βXt) = βXt−1
L(Xt + Xs) = Xt−1 + Xs−1
As a consequence, L can be recursively applied:
L
2Xt = L(LXt) = Xt−2
...
L
nXt = Xt−n
and it also admit an inverse operator L
−1
such that L
−1Xt = Xt+1. The above
relationships allows us to formally treat expressions involving L as algebraic ones. L
also allows us to express more complex relations between the terms of time series. For
example, the difference operator, defined as the difference between random variables
at adjacent times ∆Xt = Xt − Xt−1, is simply given by:
∆Xt = Xt − Xt−1 = (1 − L)Xt (6.3)
Complex relationships can be easily written by symbolic manipulation, for example
one involving Xt, Xt−1 and Xt−2 in terms of two constants φ1 and φ2:
(φ1 + φ2L)LXt = φ1Xt−1 + φ2Xt−2
The algebraic representation also allows us to define a polynomial of degree n in
L, in terms of n + 1 coefficients θ0, θ1, ...θn as in (6.4). We will make extensive use of
the lag operator and lag polynomials in the succeeding sections.
Θ(L) = Xn
k=0
θkL
k
(6.4)
6.2 Moving-average processes
By using the lag polynomial (6.4) it is possible to construct a weighted moving average
of white noise processes {t}, as follows:
Xt = Θ(L)t = t + θ1t−1 + θ2t−2 + ... + θqt−q (6.5)
where θk, for k = 1..q are the weights of the average. Equation (6.5) formally defines
a moving-average process of q-th order, or shortly an MA(q) process. To understand
what MA(q) processes are, let us start with q=1.
An MA(1) process is defined by:
Xt = t + θt−1 = (1 + θL)t (6.6)
which is a zero mean process, remembering that t is white noise for any t. The
definition (6.6) can be made more general by including the possibility of a non-zero192 ARMA Processes
mean: Xt = µ+t+θt−1, but this does not add anything as we are allowed to consider
Xt − µ as a random variable (µ does not depend on t if the process is stationary in
the weak sense).
Before briefly analysing the peculiar characteristics of MA(1) and, subsequently of
MA(q) processes, a simple fictional example can help in understanding what an MA(1)
process is, and how a process at time t can depend on noise at the same time and at
a previous time t − 1. The MA(1) process has three ingredients: a random variable Xt
at time t and two white noise variables t and t−1.
Suppose Xt is the difference with respect to the average value, say in one year
and in a great number of cities in a given country, of the number of umbrellas bought
by the country’s inhabitants. Qualitatively, the number of sold umbrellas in a locality
increases when it rains or when the weather forecasts predict rain with a high prob￾ability. Now, forcing somewhat, suppose the rain ‘expectation’ to be a white noise
process. That is not actually true, but what is certainly true is that the daily amount
of precipitation in a place at a given time is described by a probability distribution (Ye
et al., 2018), and that the so-called ‘probability of precipitation’ (PoP) we find in the
‘meteo’, is based on it (and, of course, on climatological models!). Therefore, although
it is not scientifically based, we can give to t a meaning related to the PoP in a certain
area at a certain time. Essentially, suppose that a high value of PoP pushes people to
get an umbrella, or to buy it if you do not possess one. Continuing with the artificial
example, we can suppose that the expected number of bought umbrellas Yt is given
by an expression like:
Yt = µ + β(t + θt−1)
telling us that the number of sold umbrellas is the average number plus (or minus; it
depends on the sign of β) something proportional to the PoP. By normalizing, to the
β coefficient, the difference with respect to the annual average value, Xt = (Yt −µ)/β:
Xt = t + θt−1
Let us arbitrarily suppose that θ = −0.5. If the rain probability/amount at time
t − 1 was 0 (dry weather), at time t we have Xt = t. In other words, the increment of
the number of bought umbrellas with respect to the mean depends on t, i.e. on the
weather forecast at a given place on day t, (remember: Xt and t are random variables,
not deterministic!). If the PoP is very high somewhere, e.g.  ' 1, we have Xt ' 1, a
high number of sold umbrellas.
Now, if the bad weather persists at time t + 1, at that time we have:
Xt+1 = t+1 − 0.5t = 0.5
that is, the increment in the number of sold umbrellas decreases because most people
bought it the day before.
The example is, of course, fictitious and cannot be pushed too far. What we want to
point out from this is that random phenomena exist that can be described by models
accounting for a relationship between a random variable and noise at different times. A
moving-average process has some kind of ‘memory’, but at time t it does not rememberMoving-average processes 193
where it was at t − 1; it only remembers what the random noise component was at
that previous time. And, because of the limited memory (limited to two time steps in
the MA(1) case) an MA process can produce ‘pairs’ of observations that are likely to
be both high or low. After two time steps (or q + 1 in the MA(q) case) the process is
again random.
Let us briefly look at the main characteristics of an MA(1) process, defined by
equation (6.6). Taking into account the characteristics of white noise (eqn 6.1), the
autocovariances of different orders are easy to compute:
γ(0) = E[XtXt] = E[(t + θt−1)
2
] = σ
2
(1 + θ
2
)
γ(1) = E[XtXt−1] = E[XtLXt] = E[(t + θt−1)(t−1 + θt−2)] = σ
2
θ (6.7)
γ(k) = E[XtXt−k] = 0, for k > 1
As a consequence, the autocorrelation coefficient ρ(1) is given by:
ρ(1) = γ(1)
γ(0) =
θ
1 + θ
2
while ρ(k) = 0 for any k greater than one. This means that MA(1) processes are 1-
correlated, while white noise is 0-correlated. Note that by substituting the θ coefficient
with its reciprocal 1/θ, the first-order autocorrelation coefficient does not change.
The following code shows how to simulate an MA(1) process.
## Code_6_2.R
# Moving-average process
# MA(1) positive coefficient
# for reproducibility
set.seed(13)
# generate white (Gaussian) noise
w <- rnorm(100)
x <- rep(0,100)
# generate MA(1)
x[1] <- w[1]
for (t in 2:100) x[t] <- w[t] + 0.5*w[t-1]
# plot time series
plot(as.ts(x),ylab="MA(1) process")
# plot autocorrelation function
acf(x,main="")
# plot spectral density
x.spec <- spectrum(x,log="no",span=30,plot=FALSE)
plot(x.spec$freq,x.spec$spec,t="l",ylim = c(0,3),
xlab="frequency (cycles/sample interval)", ylab="spectral density")
Figure 6.5 shows the MA(1) process generated in the example above. Exactly the
same data can be generated with a single R function (arima.sim), present in the
standard package stats, which we will find useful in the following:
set.seed(13)
x <- arima.sim(model = list(ma = 0.5), n = 100)
At first sight, the process appears similar to white noise. But, looking more thor￾oughly, some kind of correlation seems to exist between data at consecutive times; in194 ARMA Processes
time
MA(1) process
0 20 40 60 80 100
−2 −1 0 1 2
Fig. 6.5 MA(1) process
particular it appears that every ‘up’ in the plot is followed by a ‘down’. Indeed, that
is the case: Fig. 6.6 shows that the first autocorrelation is not zero, as it comes from
eqn 6.7, and also that the spectrum of the process is not flat.
0.0 0.1 0.2 0.3 0.4 0.5
0.0 0.5 1.0 1.5
frequency (cycles/sample interval)
spectral density
0 5 10 15 20
−0.2 0.2 0.6 1.0
lag
ACF
Fig. 6.6 ACF (left) and spectral density (right) of an MA(1) process with θ = 0.5.
Changing the sign of the θ coefficient a very similar process is obtained, the main
difference being the reversed sign of the first-order autocorrelation coefficient ρ(1) (see
Fig. 6.7). Moreover, MA(1) processes with negative coefficient tend to be less ‘smooth’Moving-average processes 195
than the positive counterpart, by virtue of the negative correlation between consecutive
times. The spectrum is not flat, as before, and the sign of the θ coefficient affects the
frequency distribution. We will come back to this example in the next chapter.
0 5 10 15 20
−0.2 0.2 0.6 1.0
lag
ACF
0.0 0.1 0.2 0.3 0.4 0.5
0.0 0.5 1.0 1.5
frequency (cycles/sample interval)
spectral density
Fig. 6.7 ACF (left) and spectral density (right) of an MA(1) process with θ = −0.5.
6.2.1 Moving-average processes of higher order
Equation (6.5) generalizes the MA(1) case to an order q. By computing the autoco￾variance and autocorrelation coefficients, it is easy to verify that an MA(q) process is
q-correlated:
γ(0) = σ
2
(1 + θ
2
1 + ... + θ
2
q
)
γ(k) = σ
2
qX−k
j=0
θj θj+k, for k ≤ q (6.8)
γ(k) = 0, for k > q
The k-order autocorrelation for k ≤ q is expressed by:
ρ(k) =
Pq−k
j=0 θj θj+k
1 + Pq
j=1 θ
2
j
and it is zero for k > q. Without going too deep inside the connection between MA(q)
processes and general stationary processes, we mention the fact that the opposite is
also true: any stationary q-correlated process can be represented by an MA(q) model.
Remark 6.1 The observed quantity in a time series is Xt, and the value at time t, while the
noise t is unknown. It appears from (6.5) that if the lag polynomial Θ(L) is invertible, we can
formally obtain the noise:
t = Θ(L)
−1Xt
An in-depth discussion of the invertibility property of an MA process is beyond the purpose of this
book. The reader can find it in any specific textbook dealing with time series, e.g. (Box et al.,196 ARMA Processes
1994), (Shumway and Stoffer, 2006). Inverting the lag polynomial is in principle rather easy. It
means finding a series:
Ψ(L) ≡ Θ(L)
−1 = ψ0 + ψ1L + ψ2L
2 + ....
such that Ψ(L)Θ(L) = 1 which, by matching the coefficients of the product, brings:
ψ0 = 1
ψ1 + ψ0θ1 = 0
ψ2 + ψ1θ1 + ψ0θ2 = 0
...
Aa MA(q) process is invertible if the roots of the lag polynomial, i.e. the solutions
of a q-order equation 1 + θ1z + ... + θqz
q = 0 lie all outside the unit circle. In the
MA(1) case, for example, this means |θ| < 1. We have seen that θ and 1/θ give the
same autocorrelation coefficient, e.g. θ = 0.5 and θ = 2 give ρ(1) = 0.4. But the
former is invertible, while the latter is not. A true comprehension of what invertibility
implies requires a knowledge of autoregressive processes, and their relationship with
moving-average ones.
6.3 Autoregressive processes
The time series {Xk} is an autoregressive process of order p, denoted by AR(p), if X
at time k = t depends only on the previous p times:
Xt = φ1Xt−1 + φ2Xt−2 + ... + φpXt−p + t =
Xp
k=1
φkXt−k + t (6.9)
where {t} is white noise as defined in Section 6.1. For example, {t} can be a Gaussian
error with zero average and variance σ
2
, with zero autocovariance (cov(k, j ) = 0 for
any k and j) and uncorrelated with the signal X: cov(Xt, k) = 0, for any k < t. The
process defined by (6.9) is defined ‘autoregressive’ because it represents X at time t
as a linear regression on past terms of the same series.
Equation (6.9) can be expressed in terms of the lag operator L introduced earlier.
Recursively applying it to the terms of the time series (6.9) we obtain:
 
1 −
Xp
k=1
φkL
k
!
Xt = t (6.10)
In terms of a lag polynomial Φ(L), equation (6.9) can be re-written as follows:
Φ(L)Xt = t
from which we note the formal analogy between Φ(L) and Θ(L)
−1
introduced for
MA(q) processes.
Let us start with the simplest AR process, that is for p = 1:Autoregressive processes 197
(1 − φL)Xt = t
and invert the lag polynomial:
Xt = (1 − φL)
−1
t =
X∞
i=0
(φL)
i
t =
X∞
i=0
φ
i
t−i
which formally identifies the AR(1) as an MA(∞) process. We can easily check that in
the AR(1) case the condition for stationarity is |φ| < 1, which implies that the root of
the characteristic equation Φ(z) = 1 − φz = 0, associated to the lag polynomial Φ(L),
must be greater than 1.
In the general case of p order the stationarity of the process depends on the roots
of the polynomial Φ(L). Indeed, the AR process is stationary if and only if all the
solutions of Φ(L) = 0, in general complex, are outside the unit circle in the complex
plane (Box et al., 1994). Φ(L) is the lag polynomial of the process, and Φ(z) = 1 −
φ1z − φ2z
2 − ....φpz
p = 0 is the characteristic equation.
6.3.1 Low-order autoregressive processes
We will analyse the main properties of AR(p) process in the p = 1 and p = 2 cases.
If p = 1 the process is autoregressive of order 1, AR(1), i.e.:
Xt = φXt−1 + t (6.11)
In this case, the lag polynomial is simply Φ(L) = 1 − φL and, explicitly:
(1 − φL)Xt = t (6.12)
The AR(1) process is stationary if |φ| < 1. This condition implies, as discussed
above, that the absolute value of the root of the characteristic equation must be greater
than 1.
Proof Given (6.11), the characteristic equation is simply:2
1 − φz = 0
whose root is z1 = 1/φ. |z1| > 1 only if |φ| < 1.
✷
In the above, the (wide-sense) stationary AR(1) process was supposed to have zero
mean or the Xt are differences with respect to the mean value. The expected value µX
and the variance σ
2
X are readily computed:
2We use z instead of L to avoid confusion between the operator and the polynomial variable198 ARMA Processes
µX ≡ E[Xt] = φE[Xt−1] + E[t] = 0
σ
2
X ≡ var[Xt] = E[φ
2X2
t−1 + 
2
t + 2φXt−1t] = φ
2E[X2
t−1
] + σ
2
therefore:
σ
2
X =
σ
2
1 − φ2
In a similar way, it is rather simple to compute the k-th autocovariance γk:
γk = E[Xt, Xt−k] = φ
k σ
2
1 − φ2
If p = 2, we have an AR(2) process:
Xt = φ1Xt−1 + φ2Xt−2 + t (6.13)
In this case the lag polynomial is Φ(L) = 1 − φ1L − φ2L
2
. The characteristic equation
can be written in terms of its two (possibly complex or coincident) roots z1 = λ
−1
1
and
z2 = λ
−1
2
:
(1 − λ1z)(1 − λ2z) = 0 (6.14)
The AR(2) process is stationary if both roots are outside the unit circle: |z1| > 1
and |z2| > 1, which is true if and only if the parameters φ1 and φ2 satisfy the following
conditions:
φ1 + φ2 < 1 φ2 − φ1 < 1 |φ2| < 1 (6.15)
Expected value µX and variance σ
2
X of AR(2) process are listed below. The demon￾stration is left to the reader as an exercise.
µX ≡ E[Xt] = 0
σ
2
X ≡ E[X2
t
] = (1 − φ2)σ
2
(1 + φ2)(1 − φ1 − φ2)(1 + φ1 − φ2)
The autocovariance γk can be computed by means a recursive relationship:
γk ≡ E[XtXt−k] = φ1γk−1 + φ2γk−2
which, knowing γ0 = σ
2 allows to compute any order.
Let us look at a couple of simple R examples, to gain familiarity with the above
concepts. The first shows the dependence of an AR(1) process (6.11) on the parameter
φ. The following code shows how a time series with φ = 0.3 (stationary) and one with
φ = 1.01 (not stationary) will differ. Here, the white noise is Gaussian with unitary
variance.Autoregressive processes 199
## Code_6_3.R
#Autoregressive model (1), stationary and not stationary
# Set seed for random number generation, for reproducibility of results
set.seed(45)
# Common parameters
N <- 101
t <- 0:(N-1)
# white (gaussian) noise with variance = 1
epsilon <- rnorm(N)
# AR(1) stationary
phi <- 0.3
Xs <- rep(0,N)
for (i in 2:N) {
Xs[i] <- phi*Xs[i-1] + epsilon[i]
}
# AR(1) not stationary
phi <- 1.01
X <- rep(0,N)
for (i in 2:N) {
X[i] <- phi*X[i-1] + epsilon[i]
}
# Comparison
plot(t,X,t="l")
lines(t,Xs,col="red")
Figure 6.8 compares the time series with φ = 0.3 (dashed line) and φ = 1.01 (solid
line).
The second example shows, through computation in R, how the parameters of
a stationary AR(2) process are related to the roots of the characteristic equation.
Suppose that φ1 = 0.5 and φ2 = 0.2 in (6.13), which fulfils the stationarity conditions
(6.15). The following code computes such a time series.
## Code_6_4.R
#Autoregressive model (2) parameters
# AR(2)
phi1 <- 0.5
phi2 <- 0.2
X <- rep(0,N)
for (i in 3:N) {
X[i] <- phi1*X[i-1] + phi2*X[i-2] + epsilon[i]
}
plot(t,X,t="l")
The series is stationary (evident also by eye), as the computation of (6.15) confirms.
But, what about the roots of the lag polynomial?
In the present case, the second-order characteristic equation 1 − 0.5z − 0.2z
2 = 0
is (easily) analytically solvable, giving the following roots z1 and z2:
z1 =
−2.5 + √
26.25
2
= 1.31
z2 =
−2.5 −
√
26.25
2
= −3.81200 ARMA Processes
0 2 0 4 0 6 0 8 0 100
0 5 10
t
X
Fig. 6.8 Comparison of AR(1) time series with φ = 0.3 (dashed line) and φ = 1.01 (solid
line).
Therefore, as it should be, both |z1| and |z1| are greater than one. Figure 6.9 shows
the stationarity of the time series.
0 20 4 0 6 0 80 100
−2 0 1 2 3 4
t
X
Fig. 6.9 AR(2) time series with φ1 = 0.5 and φ2 = 0.2Autoregressive processes 201
6.3.2 Autocorrelation structure and model analysis
When faced with a time series, suppose a stationary one, we might be interested in
determining which model best represents the data. Is the process MA(q)? Or is it
better described by an AR(p)? With which order? A first useful step in this direction
is the analysis of the autocorrelation and partial autocorrelation at different time lags.
The latter, ak, is defined in terms of the autocorrelation between two times t and t−k,
conditional on all other times:
ak ≡ Corr(Xt, Xt−k|Xt−1...Xt−k+1)
with this definition, a1 = Corr(Xt, Xt−1) = ρ(1)
Intuitively, while the autocorrelation among Xt and Xt−k, i.e. the correlation of
the random variable X at time t with the same variable at a time distance k, depends
on X at all other intermediate times t−1, t−2, ... t−k+ 1, the partial autocorrelation
only depends on the “direct” effect of time t − k on time t.
The meaning of partial autocorrelation can be better understood considering the
partial correlation among three correlated random variables, say X, Y and Z. The
partial correlation between X and Y given Z is obtained regressing X on Z (obtaining
Xˆ), regressing Y on Z (obtaining Yˆ ) and eventually computing the correlation among
X − Xˆ and Y − Yˆ :
Corr(X, Y |Z) = Corr(X − X, Y ˆ − Yˆ )
In other words, partial correlation is the correlation between the prediction errors of the
regressions of X and Y on Z. Now, if X, Y, Z are respectively substituted by Xt, Xt−k
and Xt−1 . . . Xt−k+1, we obtain an operative definition of the partial autocorrelation
for any k is readily:
ak = Corr(Xt − Xˆ
t, Xt−k − Xˆ
t−k)
i.e. ak is the autocorrelation between Xt and Xt−k with the linear dependence on
Xt−1 . . . Xt−k+1 removed. In the above formula, Xˆ
t is the linear regression estimate
of Xt over Xt−1 . . . Xt−k+1, and Xˆ
t−k is the best prediction at time t − k based
on the same Xt−1 . . . Xt−k+1. To be explicit, thanks to the assumed stationarity, if
Xˆ
t = α1Xt−1 + α2Xt−2 + . . . + αk−1Xt−k+1, reverting the time direction: Xˆ
t−k =
α1Xt−k+1 + α2Xt−k+2 + . . . + αk−1Xt−1.
Remark 6.2 The particular case of AR(1) can help to clarify the issue. At any time t the
regression of Xt+2 on Xt+1 is:
Xˆt+2 = βXt+1
with no constant term, because E[Xt] = 0. We choose β so as to minimize the variance of
Xt+2 − Xˆt+2:
E[Xt+2 − βXt+1]
2 = γ(0) − 2βγ(1) − β
2
γ(0)
Differencing with respect to β and equating to zero, we obtain:202 ARMA Processes
β =
γ(1)
γ(0) ≡ ρ(1) = φ
We obtain the same result for the regression of Xt on Xt+1, hence:
Corr(Xt+2 − Xˆt+2, Xt − Xˆt) = Corr(Xt+2 − φXt+1, Xt − φXt+1) = Corr(t+2, Xt − φXt+1)
the last is equal to zero because Xt −φXt+1 is, by definition, not correlated with t+2. The same
is true for any k, so we conclude that for an AR(1) process the partial autocorrelations are a1 = φ
and ak = 0 for any k > 1.
The computation of partial autocorrelations for an AR(p) process is similar to
that developed above but, of course, a bit more complex. Referring to classical texts
on time series analysis, e.g. (Box et al., 1994), the partial autocorrelations ak are all
zeros for k > p, while they can be non-zero for k ≤ p. Remember that, instead, the
autocorrelations are exponentially decreasing with k: ρ(k) = φ
k
. The duality between
AR and MA processes suggests that, because a finite-order stationary AR corresponds
to an infinite-order invertible MA process, and a finite-order invertible MA process can
be expressed as an infinite-order AR, an invertible MA(q) process has exponentially
decreasing partial autocorrelation, while we have seen (6.8) that its autocorrelations
are zero for lags k > q.
In conclusion, the autocorrelation function (ACF) and the partial autocorrelation
function (PACF) play dual roles in AR and MA processes. The analysis of the PACF
of an AR(p) process helps in determining the order p, as well as the ACF of an
MA(q) allows to estimate q. Furthermore, looking at the autocorrelation and partial
autocorrelation structure of a process can shed light on its nature. Simplifying things
(a process can contain both an AR and an MA structure, as we will see in Section 6.4),
if the ACF dies out slowly and PACF is non zero only for a few lags, the process is
probably described by an MA model, while in the opposite case the model is probably
an AR. A few lines of code follow to show the different autocorrelation structure of
AR(2) and MA(2) example processes.
## Code_6_5.R
#Autocorrelation structure of AR(2) and MA(2)
# stationary AR(2)
set.seed(4321)
x.AR2 <- arima.sim(model = list(order=c(2,0,0), ar = c(0.5,-0.7)),
n = 200)
plot(x.AR2,main="")
acf(x.AR2,main="")
pacf(x.AR2,main="")
# invertible MA(2)
set.seed(4321)
x.MA2 <- arima.sim(model = list(order=c(0,0,2), ma = c(0.2,-0.5)),
n = 200)
plot(x.MA2,main="")
acf(x.MA2,main="")
pacf(x.MA2,main="")
Figure 6.10 compares the plot (a) of a time series generated from an AR(2) process
with coefficients 0.5 and −0.7, therefore stationary, with an invertible MA(2) timeAutoregressive processes 203
series (b) with coefficients 0.2 and −0.5. Plots (d) and (e), respectively showing the
ACF of the MA process and the PACF of the AR process, inform us about the order of
the two processes. Plots (c) and (f) show an oscillatory exponential decay, confirming
that generated data are AR and MA, respectively.
−0.4 −0.3 −0.1 0.0 0.1
0 5 10 15 20
lag
0 5 10 15 20
−0.5 0.0 0.5 1.0
lag
ACF
ACF
0 5 10 15 20
−0.4 0.0 0.4 0.8
lag
time
x.AR2
0 50 100 150 200
−3 −2 −1 0 1 2 3
time
x.MA2
0 50 100 150 200
−2 −1 0 1 2
(a) (b)
(c) (d)
0 0 15 20 5 1
−0.6 −0.2 0.0 0.2
lag
Partial ACF
Partial ACF
(e) (f)
Fig. 6.10 Comparison between stationary AR(2) (a) and invertible MA(2) (b) processes;
ACF of AR(2) (c) and MA(2) (d); PACF of AR(2) (e) and MA(2) (f)204 ARMA Processes
6.4 Autoregressive moving-average processes (ARMA)
The autocorrelation structures we have learned in the previous sections are of two
types: (1) slowly decaying ACF and sharp cut-off of PACF (autoregressive process),
(2) slowly decaying PACF and sharp cut-off of ACF (moving-average process). Of
course, not all stationary processes can be represented by AR(p) or MA(q) models.
For example, a time series like that shown in Fig. 6.11 has the autocorrelation
structure shown in Fig. 6.12, clearly pointing out that such a process is neither AR
nor MA because no evident cut-off is present there.
The data in Fig. 6.11 has been generated by the following chunk of code, using the
function arima.sim introduced in Section 6.2.
## Code_6_6.R
# Mixed models
# for reproducibility
set.seed(5678)
# ARMA(2,2) process with
# AR parameters: phi_1 = 0.5, phi_2 = -0.7
# MA parameters: theta_1 = 0.2, theta_2 = -0.5
x.ARMA <- arima.sim(model = list(order=c(2,0,2), ar = c(0.5,-0.7),
ma = c(0.2,-0.5)), n = 200)
plot(x.ARMA,main="",ylab="time series")
acf(x.ARMA,main="")
pacf(x.ARMA,main="")
time
time series
0 50 100 150 200
−4 −2 0 2 4
Fig. 6.11 Time series: ARMA(2,2) process.
A generalization of pure autoregressive or pure moving-average processes is a com￾bination of them. Writing AR(p) and MA(q) in terms of the relevant lag polynomials
Φ(L) and Θ(L), and solving for Xt − t we obtain:Autoregressive moving-average processes (ARMA) 205
0 5 10 15 20
−0.5 0.0 0.5 1.0
lag
ACF
5 10 15 20
−0.8 −0.4 0.0 0.2
lag
Partial ACF
Fig. 6.12 Autocorrelation structure of the time series of Fig. 6.11.
Φ(L)Xt = Θ(L)t (6.16)
Such a process is named auto-regressive-moving-average (ARMA). Explicitly, an ARMA(p, q)
process is:
Xt = φ1Xt−1 + ... + φpXt−p + t − θ1t−1 − ... − θqt−q (6.17)
where the choice of the sign of θk is of course arbitrary.
From the previous discussion it should not be a surprise that a stationary and
invertible ARMA process has both the infinite moving-average representation and the
infinite autoregressive one. We will give the main properties of ARMA models without
mathematical proof (the interested reader can refer to books like (Box et al., 1994)).
Essentially, the properties of ARMA(p, q) models derive from those of the AR(p)
and MA(q) of which ARMA is a mixture. In particular:
• The condition of stationarity, i.e. of stability, is that of the AR(p) process. Given
the characteristic equation associated with the lag polynomial
1 − φ1z − φ2z
2 − ... − φpz
p = 0
the process is stationary if all roots lie outside the circle |z| = 1, i.e. when all
roots exceed unity in absolute value.
• The condition of invertibility is ‘inherited’ from the MA(q) process. Given the
characteristic equation associated with the lag polynomial:
1 + θ1z + θ2z
2 + ... + θqz
q = 0
the process is stationary if all the roots lie outside the circle |z| = 1.
• An ARMA(p, q) model possessing both the above properties is called causal.
• The mean of an ARMA process also derives from those of the constituting AR
and MA. Having subtracted the possible non-zero mean µ: E[Xt] = 0.
• The ACF of an ARMA(p, q) process dies out as that of its AR(p) component.
• The PACF of an ARMA(p, q) process dies out as that of its MA(q) component.206 ARMA Processes
Note that, from (6.16), the properties of an ARMA process are related to the roots
of the ratio Θ(z)/Φ(z).
The computation of autocorrelations and partial autocorrelations for a generic￾order ARMA model is rather cumbersome. The R functions acf and pacf do the job
for us. In the first-order case, ARMA(1,1) the autocovariances are given by:
γ0 ≡ Var[Xt] = 1 + 2φθ + θ
2
1 − φ2
σ
2
γ1 ≡ Cov[Xt, Xt−1] = (φ + θ)(1 + φθ)
1 − φ2
σ
2
(6.18)
γk ≡ Cov[Xt, Xt−k] = φγk−1, for k > 1
where σ
2
is the white noise variance.
A few words are in order about redundancy. If you try to fit data coming from an
ARMA(p, q) model, e.g. those in Fig. 6.11, by means of an ARMA(P,Q) model, with
P ≥ p and Q ≥ q you can always find several couples (P,Q) fitting well the data.
A nice example from (Shumway and Stoffer, 2006) clears the concept of parameter
redundancy.
Suppose you have a purely white noise process, i.e. Xt = t. Multiplying both
members at a previous time by the same coefficient (e.g. 0.5) and subtracting them
from the left- and right-hand side we obtain a ‘new’ process:
Xt − 0.5Xt−1 = t − 0.5t−1
which can be re-written:
Xt = 0.5Xt−1 + t − 0.5t−1
Now, Xt is still white noise, but it appears as an ARMA(1,1) process. Writing it in
terms of the lag polynomials:
(1 − 0.5L)Xt = (1 − 0.5L)t
i.e. we observe that the ratio between the lag polynomials in (6.16) is 1 or, said differ￾ently, Θ(z) and Φ(z) have a common factor 1−0.5z. The common factor identifies the
redundancy of the parameters: once removed, both lag polynomials reduce to Θ(z) = 1
and Φ(z) = 1. We conclude that, when trying to fit an ARMA(p, q) model to data,
the common roots of the AR(p) and MA(q) polynomials constituting it must be ruled
out, decreasing the order or both the AR and MA components of the ARMA model.
The R package stats includes a function for fitting an ARMA model to an univari￾ate time series: arima. As we will briefly introduce in the next section, that function
actually deals with ‘integrated’ ARMA models, or ARIMA, but it can also be used for
ARMA by taking as zero the integration order. As a conclusive example of this sec￾tion we will show how to fit an ARMA(p, q) model to data, taking as an example the
ARMA(2,2) data of Fig. 6.11, having parameters φ1 = 0.5, φ2 = −0.7, θ1 = 0.2, θ2 =
−0.5. The following code tests an ARMA(2,2) model with the data.Autoregressive moving-average processes (ARMA) 207
fit <- arima(x.ARMA,order = c(2,0,2))
fit
The output shows that the fitted parameters are rather close to the actual ones.
Call:
arima(x = x.ARMA, order = c(2, 0, 2))
Coefficients:
ar1 ar2 ma1 ma2 intercept
0.4548 -0.6676 0.3076 -0.5485 -0.0380
s.e. 0.0625 0.0557 0.0732 0.0736 0.0433
sigma^2 estimated as 0.9385: log likelihood = -279.38, aic = 570.76
Determining the ‘true’ values of the order p and q is not a straightforward task. One
possibility is that of fitting ARMA models of increasing AR and MA orders, and to
look at some ‘performance’ parameter of the fitting. The most intuitive approach could
be to choose the fitting giving the maximum likelihood value, i.e. that minimizing the
residual error. But the error has been proved to decrease with increasing order of the
ARMA model. A solution to that problem has been proposed by Akaike (Akaike, 1974)
whose idea was to penalize the error variance by a term proportional to the number of
parameters. The so-called AIC (Akaike’s information criterion) assumes that the best
model is that having an order k = p + q such as to minimize:
AIC = 2(k + 1) − 2log(L) (6.19)
where L is the likelihood of the model, and k + 1 is the number of parameters in the
model (AR and MA orders, plus the residual variance). Just to show an application of
the AIC, we can try to compute a sequence of ARMA(p, q) fittings with p and q both
in the range [1, 4]. The following listing shows how to do it in R.
aic.fit <- c()
for (p in 1:4){
for (q in 1:4) {
fit <- arima(x.ARMA,order = c(p,0,q))
aic.fit <- c(aic.fit,fit$aic)
}
}
AIC.fit <- matrix(data=aic.fit,nrow=4,ncol=4)
AIC.fit
The result f it of an arima fitting is a list, one of whose components is the computed
AIC. The code above collects the AIC values in a list and then converts it to a p × q
matrix. The result:
[,1] [,2] [,3] [,4]
[1,] 700.7636 600.6213 578.7266 575.0947
[2,] 658.2359 570.7618 572.6152 574.3528
[3,] 613.6939 572.6444 574.5500 576.3364
[4,] 605.5990 574.2344 576.1547 574.4908
indicates that the minimum AIC value is 570.76, and that it occurs for p = 2 and
q = 2 as it does.208 ARMA Processes
6.5 An introduction to non stationary and seasonal time series
All the processes considered so far were stationary. Consider random walk (see Chapter
5 for a description of random walk), which is an AR(1) process with unitary parameter,
thus not fulfilling the stationarity requirements:
Xt = Xt−1 + t (6.20)
By differencing (6.20) we obtain a stationary time series:
∇Xt ≡ Xt − Xt−1 = t (6.21)
i.e. a stationary white noise. The following code generates a random walk Xt and
computes its first difference ∇Xt.
## Code_6_7.R
# Random walk as autoregressive process
# for repeatability
set.seed(123456)
# gaussian white noise
w <- rnorm(200)
x <- rep(0,200)
# Random walk
x[1] <- w[1]
for (t in 2:200) x[t] <- w[t] + x[t-1]
# Plot time series
plot(as.ts(x),ylab="Random walk")
# Compute and plot the ACF
acf(x,main="")
# Compute and plot the spectrum
x.spec <- spectrum(x,log="no",span=30,plot=FALSE)
plot(x.spec$freq,x.spec$spec,t="l",
xlab="frequency (cycles/sample interval)", ylab="spectral density")
# difference
d <- rep(0,199)
d[1] <- x[2]-x[1]
for (t in 3:200) d[t-1] <- x[t] - x[t-1]
# Plot time series
plot(as.ts(d),ylab="Difference(Random walk)")
# Compute and plot the spectrum
d.spec <- spectrum(d,log="no",span=30,plot=FALSE)
plot(d.spec$freq,d.spec$spec,t="l",
xlab="frequency (cycles/sample interval)", ylab="spectral density",
ylim = c(0,1.2))
acf(d,main="")
The results of the above computation are shown in Fig. 6.13. Figures (a) and (b)
compare the random walk Xt to ∇Xt. Figures (c) and (d) compare the relative ACFs
and, eventually, figures (d) and (e) compare the spectra, clearly showing that ∇Xt is
white noise.
The ACF behaviour of Fig. 6.13 (c), very slowly going to zero, is typical of non￾stationary processes. Actually, the analysis of the autocorrelation structure of a time
series is very helpful in determining if it is stationary or not (of course, specific statis￾tical tests also exist for detecting non-stationarity).An introduction to non stationary and seasonal time series 209
time
random w alk
0 50 100 150 200
−5 0 5 10
time
difference (random w alk)
0 50 100 150 200
−2 −1 0 1 2
(a) (b)
0 5 10 15 20
0.0 0.2 0.4 0.6 0.8 1.0
lag
ACF
0 5 10 15 20
0.0 0.2 0.4 0.6 0.8 1.0
lag
ACF
(c) (d)
0.0 0.1 0.2 0.3 0.4 0.5
0 10 20 30 40 50 60
frequency (cycles/sample inter val)
spectral density
0.0 0.1 0.2 0.3 0.4 0.5
0.0 0.2 0.4 0.6 0.8 1.0 1.2
frequency (cycles/sample inter val)
spectral density
(e) (f)
Fig. 6.13 Comparison between random walk (a) and its first difference (b); ACF of random
walk (c) and first difference (d); spectrum of random walk (e) and first difference (f).210 ARMA Processes
6.5.1 Integrated ARMA models
The difference operator introduced in eqn 6.21 can be recursively applied to any time
series. Given a time series Xt, we say it is ‘integrated of order d’ if the d-th difference
of Xt is white noise:
∇dXt = t
In terms of the lag operator L we have: ∇ ≡ 1 − L and ∇d ≡ (1 − L)
d
. Therefore,
Xt is integrated of order d if:
(1 − L)
dXt = t
More generally, Xt is an integrated autoregressive moving-average process, and we
denote it by ARIMA(p, d, q), if ∇dXt is ARMA(p, q). Using lag polynomials, if we set:
Yt = (1 − L)
dXt we have:
Φp(L)Yt = Θqt
where we have explicitly declared the order of the AR and MA polynomials. Or, which
is the same, the lag-polynomial representation of an ARIMA(p, d, q) model is:
Φp(L)(1 − L)
dXt = Θq(L)t
The following simple example shows how the difference operator, simply imple￾mented in R by means of the diff function, transforms a non stationary ARIMA(2,1,1)
series into a stationary ARMA(2,1).
## Code_6_8.R
# Difference operator
# for repeatability
set.seed(1234)
# generate data from an ARIMA(2,1,1) model
# AR parameters: 0.5, -0.7
# MA parameter: 0.6
x.ARIMA <- arima.sim(model = list(order=c(2,1,1), ar = c(0.5,-0.7), ma = 0.6), n = 200)
plot(x.ARIMA,main="",ylab="time series")
acf(x.ARIMA,main="")
The non-stationary character is evident from the plot, Fig. 6.14 (left), and con￾firmed from the behaviour of the ACF (right). Applying the difference operator we
obtain the time series shown in Fig. 6.15.
x.ARMA = diff(x.ARIMA)
plot(x.ARMA,main="",ylab="time series")
acf(x.ARMA,main="")
In order to verify that the difference series is actually ARMA(2,1) we proceed as
before, by trying to apply ARMA(p, q) models of increasing orders.
aic.fit <- c()
for (p in 1:3){
for (q in 1:3) {
fit <- arima(x.ARMA,order = c(p,0,q))
aic.fit <- c(aic.fit,fit$aic)An introduction to non stationary and seasonal time series 211
time
time series
0 50 100 150 200
−10 0 10 20
0 5 10 15 20
0.0 0.2 0.4 0.6 0.8 1.0
lag
ACF
Fig. 6.14 Plot of an ARIMA(2,1,1) time series (left), and of its ACF (right).
}
}
AIC.fit <- matrix(data=aic.fit,nrow=3,ncol=3)
AIC.fit
The matrix AIC.f it actually shows that the (2,1) case is that with the lowest AIC.
[,1] [,2] [,3]
[1,] 681.1871 590.3703 592.3307
[2,] 686.1645 592.3207 594.2920
[3,] 629.1372 594.1055 595.9257
time
time series
0 50 100 150 200
−4 −2 0 2 4
0 5 10 15 20
−0.5 0.0 0.5 1.0
lag
ACF
Fig. 6.15 Plot of the ARMA(2,1,1) time series (left) obtained from ARIMA(2,1,1), and of
its ACF (right).
The arima function fits an ARIMA(p, d, q) model to the data. The resulting AR
and MA parameters, and the error variance, are rather close to those used for the
simulation.
arima(x.ARMA,order=c(2,0,1))212 ARMA Processes
# RESULT:
Call:
arima(x = x.ARMA, order = c(2, 0, 1))
Coefficients:
ar1 ar2 ma1 intercept
0.5653 -0.6772 0.6669 0.0627
s.e. 0.0585 0.0565 0.0622 0.1088
sigma^2 estimated as 1.05: log likelihood = -290.19, aic = 590.37
6.5.2 Seasonal ARIMA models
We conclude this chapter with a brief introduction to seasonal ARIMA (or SARIMA)
models.
A seasonal model is something of a generalization of the models we encountered in
the preceding sections, where in a time series {Xt} the random variable Xt was related
to Xs and/or to white noise s at previous times s = t − 1, t − 2, .... For example, in
an MA(1) process, as discussed in Section 6.2:
Xt = t + θt−1
which has the property that the autocorrelation ρk is zero for k > 1. It could happen
that the non-zero autocorrelation occurs between Xt and Xt−k with k 6= 1. That is the
case of a seasonal MA(1) where, for example, we observe in the data a ‘periodicity’ of
seven time intervals (e.g. if the time scale is the day, and data has a weekly periodicity):
Xt = t + θt−7
The same can be done for an AR(1) process. The seasonality can be extended to any
order MA(q) processes, AR(p), ARMA(p, q) and also to non-stationary ARIMA(p, d, q)
processes. Using the lag operator notation (we substitute φk with ψk and θk with ωk
to avoid confusion between non-seasonal and seasonal lag polynomials), we define:
Definition 6.1 Seasonal MA(q) with period s
Xt = (1 + ω1L
s + ω2L
2s + ... + ωqL
qs)t
Definition 6.2 Seasonal AR(p) with period s
(1 − ψ1L
s − ψ2L
2s − ... − ψpL
ps)Xt = t
With these definitions, a seasonal ARMA(P,Q) model with period s is defined in
terms of lag polynomials in L
s
instead of L. Generalizing (6.16) we have:
ΨP (L
s
)Xt = ΩQ(L
s
)t (6.22)
where the subscripts explicitly declare the order of the lag polynomials.An introduction to non stationary and seasonal time series 213
Finally, we observe that ‘conventional’ ARMA models contain autocorrelation at
time lags 1, 2, ....k, while seasonal ARMA models incorporate autocorrelations only at
seasonal lags s, 2s, ....ks. A more general model should include both correlations, taking
into account that a seasonal effect can be superimposed on another autocorrelation
effect acting on a lower time lag. One of the most used seasonal models doing that
is the multiplicative seasonal ARMA (Box, Jenkins and Reinsel, 1994), which can be
concisely defined by:
ΨP (L
s
)Φp(L)Xt = ΩQ(L
s
)Θq(L)t (6.23)
where all orders have been explicitly given3
.
An example should help to better understand what (6.23) means. Suppose you
have monthly data showing an annual seasonality and that would be AR(1) if the
seasonality could be removed. Such data could be represented, for example, by:
(1 − φL)Xt = (1 − ωL12)t
i.e. by a multiplicative AR(1) × seasonal MA(1)12 model, or: ARMA(1,0) × ARMA(0,1)12.
In the notation of (6.23):
Ψ0(L
12)Φ1(L)Xt = Ω1(L
12)Θ0(L)t
with:
Ψ0(L) = 1
Φ1(L) = 1 − φL
Ω1(L
12) = 1 + ωL12
Θ0(L) = 1
A last consideration about the power of the lag operator formalism: suppose the
above mixed model consisting of a non-seasonal AR(1) and a seasonal MA(1)12 also
has a seasonal AR(1)12, i.e. it is represented by the equation:
(1 − φL)(1 − ψL12)Xt = (1 + ωL12)t
Expanding the above we obtain:
(1 − φL − ψL12 + φψL13)Xt = (1 + ωL12)t
or, solving for Xt:
Xt = φXt−1 + ψXt−12 − φψXt−13 + t + ωt−12
which states the dependence of Xt on the variable at lags 1, 12 and 12 + 1, and on
white noise at the current time and at lag 12. The multiplicative nature of the mixed
3
If Xt is replaced by Yt = (1 − L)
d(1 − Ls
)D, with d and D respectively integration orders of non￾seasonal and seasonal components, (6.23) transforms into a multiplicative ARIMA(p, d, q)×(P, D, Q)s214 ARMA Processes
model manifests in the presence of the product φψ as a coefficient for Xt−13, instead
of having a free parameter there.
Remark 6.3 The multiplicative form (6.23) is actually a consequence of the conceptual con￾struction of the seasonal ARMA model. For example, if we are analysing monthly observations
of some meteorological quantity (temperature, pressure, wind speed, precipitation amount) we
often find that a relation exists between observations for the same month in different years, i.e. 12
months apart. Therefore, observations of Xt one year apart can be related by a seasonal ARMA
model like (6.22):
ΨP (L
12)Xt = ΩQ(L
12)αt (6.24)
where, this time, αt is not white noise, but a residual ‘error’ component that is not explained by
the model. The errors αt will usually be correlated, as a consequence of a correlation between
observations in successive months. A non-seasonal ARMA model can be suitable to explain that
relationship:
Φ(L)αt = Θ(L)t (6.25)
with t white noise. By ‘multiplying’ the left- and right-hand sides of (6.24) by Φ(L) and using
(6.25), we obtain (6.23) with s=12.
6.5.3 An example
We will apply a SARIMA model to the analysis of the maximum temperatures reg￾istered in Bologna, Italy, from 1961 to 1991. Figure 6.16 (left) clearly suggests the
presence of a seasonal behaviour, as confirmed by the ACF of the time series, shown
in Fig. 6.16 (right).
The R code for obtaining Fig. 6.16 and 6.17 is the following.
## Code_6_9.R
# Seasonal ARIMA
setwd("C:/RPA/code/ARMA")
# Maximum temperature in Bologna (Period: 1961-1991)
Tmax.data <- read.table("data/Bologna_Tmax_1961_1991.txt",h=FALSE)
Tmax <- Tmax.data$V1
# Compute lag-12 differences
Tmax.diff12 <- diff(Tmax,12)
# Plots
ts.plot(Tmax,xlab="Months from January 1961")
ts.plot(Tmax[1:60],xlab="Years 1961-1991",ylab="Tmax")
# ACF
acf(Tmax,lag.max = 36,main="")
acf(Tmax.diff12,lag.max = 36,main="")
pacf(Tmax.diff12,lag.max = 36,main="")
The correlation structure shown in Fig. 6.16 (right) clearly suggests a seasonal
ARIMA model, which means that data must be seasonally-differenced, i.e. transformed
by taking differences at lag 12: ∇12Xt ≡ (1 − L
12)Xt.
Figure 6.17 shows the ACF and PACF of the lag-12 differences. Note that the
lagged-difference ∇s can be combined with the difference operator ∇D with integration
order D defined in 6.5.1, to give:An introduction to non stationary and seasonal time series 215
months from January 1961
Tmax
0 100 200 300
0 5 10 15 20 25 30 35
0 5 10 15 20 25 30 35
−0.5 0.0 0.5 1.0
lag
ACF
Fig. 6.16 Temperature in Bologna from 1961 to 1991 (left) and ACF of the time series
(right).
0 5 10 15 20 25 30 35
−0.4 0.0 0.4 0.8
lag
ACF
0 5 10 15 20 25 30 35
−0.4 −0.2 0.0 0.1 0.2
lag
Partial ACF
Fig. 6.17 ACF (left) and PACF (right) of the yearly differences of the time series of figure
6.16.
∇D
s ≡ (1 − L
s
)(1 − L)
D
which, when applied to a time series Xt gives (1 − L)
D(Xt − Xt−s).
The ACF of ∇12Xt has a large autocorrelation at lag 12, suggesting the presence of
a moving-average MA(1) term in the seasonal component, while the shape of the PACF
suggests the presence of both AR and MA terms. By a procedure like that implemented
in Subsection 6.5.1, it is simple to find the set of numbers (p, q, P, Q) giving rise to the
minimum AIC value for a mixed (SARIMA) model (p.0, q) × (P, 1, Q)12.
The R code that follows computes the parameters of an ARIMA (1, 0, 0)×(1, 1, 1)12
model.
## Code_6_10.R
# best fit SARIMA orders
p = 1
q = 0
P = 1216 ARMA Processes
Q = 1
# SARIMA fit
(arima(Tmax,order=c(p,0,q),list(order=c(P,1,Q),period=12)) -> fit)
acf(residuals(fit),48,main="")
hist(residuals(fit),main="")
The residuals of the SARIMA fitting appear to have the characteristics of white
noise, as Fig. 6.18 shows, confirming the goodness of the model.
0 10 20 30 40
0.0 0.2 0.4 0.6 0.8 1.0
lag
ACF
residuals(fit)
frequency
−6 −4 −2 0 2 4
0 20 40 60 80
Fig. 6.18 ACF (left) and histogram (right) of the ARIMA (1, 0, 0) × (1, 1, 1)12 residuals.
The fitted model can be ‘tested’ by using the generic R function predict, which is
an overloaded one, i.e. depending on the context. When the fitted model is ARIMA, R
actually invokes predict.Arima. Suppose we are at the beginning of the year 1990,
and the maximum temperatures from January 1961 to December 1989 are available to
us. The following code fits a multiplicative ARIMA (1, 0, 0) × (1, 1, 1)12 to the above
time series.
## Code_6_11.R
# convert Tmax into a time series
# X.true is the full time series
X.true <- ts(Tmax)
# X is the time series of the period 1961-1989
X <- ts(Tmax[1:(29*12)],start=1,end=(1989-1961+1)*12)
# seasonal ARIMA model
fit <- arima(X,order=c(1,0,0),list(order=c(1,1,1),period=12))
# make prediction for next 2 years
X.pred <- predict(fit, 24)
# Plot predicted values
plot(X[313:348],t="l",xlim=c(1,60),xlab="January 1987 to December 1991",ylab="Tmax")
lines(37:60,X.pred$pred,col="red")
lines(36:60,X.true[(313+35):372],lty=2)
Figure 6.19 shows the temperatures in the period 1987:1989 (solid line), and the
known values between 1990 and 1991 (dashed line). The prediction based on the sea￾sonal ARIMA model (red line) is rather good.A physical application 217
0 10 20 30 40 50 60
5 10 15 20 25 30
January 1987 to December 1991
Tmax
Fig. 6.19 True data (solid line) vs fitting results (dashed line).
6.6 A physical application
Physical quantities related to non-stationary phenomena, when repeatedly measured
over time, often appear as random. Of course, randomness usually is not a true physical
characteristics but, rather, it reflects our incomplete knowledge of the phenomenon at
hand. For example, referring to the same data to which Fig. 6.16 refers, the maximum
daily temperatures in Bologna in December 2018 (see Fig. 6.20) do not appear to have
any evident relation with the day number or, in other words, they look random.
River streamflow time series exhibit a stochastic nature too. The flow is certainly
deterministic, but the multiplicity and complexity of factors influencing it - rainfall,
evaporation, ice melting and human activities - are such as to make it a random quan￾tity or better as employing random variables to describe it. As an example, Fig. 6.21
(left) plots the time series of the average monthly water flow of the Tiber river, in
Italy, from January 1921 to December 1979 (data are extracted from the database
Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC)
for Biogeochemical Dynamics, NASA (Vorosmarty et al., 1998)).
Figure 6.21 (right) plots a time series relative to the Dow Jones index in a 25-year
period. Do you see any real difference among the properties of the two series? Both
appear as random, in spite of their totally different origin. By looking at the two time
series could you guess which one refers to a river flow and which to a stock exchange
index?
Without entering into the realms of philosophy (what is actually random?), we
have no doubt about the apparent randomness of time series like those in Fig. 6.21.218 ARMA Processes
0 5 10 15 20 25 30
2 4 6 8 10 12 14
day
Tmax
Fig. 6.20 Maximum daily temperatures measured in Bologna (Italy) in December 2018
Month # from Jan 1921 to Dec 1979
flow (m3 s)
0 100 200 300 400 500 600 700
200 400 600 800 1000
0 100 200 300 400
0 1000 2000 3000 4000
month # from Jan 1921 to Dec 1979 month # from Jan 1986 to Dec 2019 Dow Jones index: high−low
Fig. 6.21 Average monthly water flow of Tiber (Italy) (left), and High-Low Dow Jones index
(right).
The question is: can we establish any kind of relationship among physical laws and
the flow behaviour of Fig. 6.21(left) (or, in perfect analogy between economic laws and
Fig. 6.21(right))?
Concerning the river flow, and its relation to rainfall, the answer is yes, actuallyA physical application 219
that is the subject of stochastic hydrology (Hipel and McLeod, 1994). Indeed, stochas￾tic hydrology, a statistical branch of hydrology dealing with probabilistic models of
hydrological processes, has been shown to be physically based several years ago (Salas
and Smith, 1981). What follows is known to be an over-simplification of a very complex
problem, but it is nevertheless instructive.
Precipitation
Pt
St
Dt
Gt
Rt
Land
surface
Infiltration superf.
deep
Groundwater
deep storage
l
t
River
runoff
Et
Evaporation
Fig. 6.22 Hydrological cycles in a watershed.
Figure 6.22 shows a simplified schematic of the hydrological cycle in a watershed.
The precipitation Pt is the rainfall quantity affecting a river watershed during the
year t. Rain hitting the land surface is partially absorbed, partially evaporates and a
fraction of water flows into the river (this is called runoff ). Let Et = e Pt represent
the fraction of water that evaporates, with e being an evaporation coefficient. It = iPt
represents the water amount that infiltrates through the soil.
The surface runoff St is given by Pt − Et − It or, in other words, it consists of the
water amount coming from rain precipitation that is neither absorbed by the land,
nor evaporated. We assume that Gt−1 = It represents the ground water storage at the
beginning of year t. A fraction r of that storage contributes to the total river runoff
in the year t, as a deep runoff term Dt. As a consequence of the above mechanisms,
the total runoff Rt during year t is:
Rt = St + Dt = (1 − e − i)Pt + rGt−1 (6.26)220 ARMA Processes
The mass balance requires:
Gt = (1 − r)Gt−1 + iPt (6.27)
which we can read as ‘water stored at the end of year t is given by the initial storage
at t − 1, minus the deep runoff rGt−1, plus the i fraction of precipitation during year
t.’
Combining eqns (6.26) and (6.27), we obtain:
Rt = (1 − r)Rt−1 + sPt − (s(1 − r) − ir) (6.28)
where s = 1 − e − i is the superficial water fraction coefficient, i.e. such that sPt is the
surface contribution to the total river runoff.
Equations (6.26), (6.27) and (6.28) can be rewritten in terms of differences with
respect to the mean values of Rt, Gt an Pt, respectively µR, µG, µP :
Rˆ
t ≡ Rt − µR = r(Gt−1 − µG) + s(Pt − µP ) (6.29)
Gˆ
t ≡ Gt − µG = (1 − r)(Gt−1 − µG) + i(Pt − µP ) (6.30)
Rˆ
t ≡ Rt − µR = (1 − r)(Rt−1 − µR) + s(Pt − µP ) − (s(1 − r) − ir) (Pt−1 − µP ) (6.31)
Suppose, as in Section 6.2, that the probability of precipitation can be approxi￾mated by a normal distribution or, in other words, that rainfall is a Gaussian white
noise. In practice, this means that rain quantity at time τ1 is assumed independent
from rain quantity at time τ2 6= τ1. Under such hypothesis Pˆ
t is i.i.d. (0, σ2
P
) and
(6.30), rewritten as follows:
Gˆ
t = (1 − r)Gˆ
t−1 + iPˆ
t (6.32)
very closely resembles an AR(1) model, because if Pˆ
t is white noise and its fraction
iPˆ
t is.
With a pretty similar reasoning, eqn (6.31) can be written:
Rˆ
t = (1 − r)Rˆ
t−1 + sPˆ
t − (s(1 − r) − ir) Pˆ
t−1 (6.33)
which looks like an ARMA(1,1).
To summarize, a normally distributed rainfall gives rise to a groundwater storage
that is described by an AR(1) model. In the same conditions, the river runoff is de￾scribed by an ARMA(1,1) model. More generally, it can be expected that the ARMA
model for runoff-rainfall, i.e. relating the river flow Rˆ
t to the rainfall Pˆ
t (both difference
with respect to their average values), is something like:
Rˆ
t = α1Rˆ
t−1 + α2Rˆ
t−2 + ... + β0Pˆ
t + β1Pt−1 + ... + vt
where vt is a ‘residual’ term at time t and the α and β play the role of AR an
MA coefficients, respectively, allowing us to interpret the runoff-rainfall stochastic
behaviour of a river basin in terms of an ARMA(p, q) model.A physical application 221
What happens if rainfall is not independent? A plausible assumption, particularly
on short timescales (days, weeks) is that precipitations are autocorrelated. Therefore,
suppose that Pt is given by:
(Pt − µP ) = φ1(P − t − 1 − µP ) + t (6.34)
with t white noise. By substituting (6.34) in eqn (6.30) we get the model for ground￾water storage:
(Gt − µG) = (1 − r + φ1)(Gt−1 − µG) + (1 − r)φ1(Gt−2 − µG) + it
which appears to be an AR(2). Similarly, by substituting (6.34) in eqn (6.31), the river
runoff coming from an AR(1) rainfall is:
Rt − µR = (1 − r + φ1)(Rt−1 − µR) − (1 − r)φ1(Rt−2 − µR) + st − (s(1 − r) − ir) t−1
The above equations say that an AR(1) precipitation, causing the groundwater
storage to be AR(2), makes the river flow ARMA(2,1).
The procedure can be repeated for any kind of rainfall statistics. For example,
assuming an ARMA(1,1) model for precipitation Pt, the groundwater storage can be
demonstrated (Hipel and McLeod, 1994) to be described by an ARMA(2,1), while the
river runoff is ARMA(2,2). The demonstration is left as an exercise.
6.6.1 Runoff-rainfall relationship in a real case: the Loire river
Loire is a very long river (about 1000 km), crossing France from south-east to north￾west, with a basin extending for about one fifth of the country area. We will make
use of flow and precipitation data coming from the Oak Ridge National Labora￾tory Distributed Active Archive Center (ORNL DAAC) for Biogeochemical Dynamics
https://daac.ornl.gov/, a NASA Earth Observing System Data and Information Sys￾tem (EOSDIS) data centre. This open archive contains a huge number of geophysical
data, including the Global Monthly River Discharge Data Set (Vorosmarty, Fekete and
Tucker, 1998) and the Global Monthly Precipitation (Hulme, 1999). The former con￾tains monthly averaged discharge measurements (years 1807:1991) for 1018 stations
located throughout the world, and the latter is a historical monthly precipitation data
set for global land areas (years 1900:1999).
We choose a river station located at Blois (47o34048.000N latitude, 1o19048.000E
longitude). Due to the length of the river, the flow at Blois clearly depends on the
precipitation in the whole upstream area. Nevertheless, it appears reasonable to relate
the annual flow to the annual precipitation amount at a couple of locations close to the
discharge station, located upstream. The closest grid-points for rainfall data appear to
be at coordinates (47o30000.000N, 2o30000.000E) and (47o30000.000N 3o45000.000E). The
precipitation stations are distant from Blois about 90 km and 180 km, respectively.
Figure 6.23 shows the time series of the annual rainfall at the chosen locations,
that appear to be strongly correlated, as expected.
It is easy to show that both the time series shown in Fig. 6.23 have neither AR nor
MA behaviour Also the annual rainfall values averaged between the two locations are
normally distributed, as Fig. 6.24 shows.222 ARMA Processes
1900 1920 1940 1960 1980 2000
5000 7000 9000
year
precipitation (mm)
Fig. 6.23 Annual rainfall recorded at 47o
300
00.000N latitude, 2o
300
00.000E longitude (solid
line) and at 47o
300
00.000N 3o
450
00.000E (dashed line).
total rain (average between locations)
density
4000 5000 6000 7000 8000 9000
0e+00 2e−04 4e−04
−2 −1 0 1 2
4000 6000 8000
theoretical quantiles
sample quantiles
Fig. 6.24 Histogram of total precipitation (left) averaged between the two locations, and
normality test (right).
The above analysis is conducted by the following R code (see Code 6 12.R). Data
files Loire rainfall1.txt and Loire rainfall2.txt have been extracted from the Global
Monthly Precipitation data cited above (ID numbers 1981 and 5234), and they can beA physical application 223
downloaded from the book website.
## Code_6_12.R
# Loire river analysis
setwd("C:/RPA/code/ARMA")
read.table("data/Loire_rainfall1.txt",h=T) -> rain1
read.table("data/Loire_rainfall2.txt",h=T) -> rain2
# annual data
plot(rain1$Y,rain1$TOT,t="l",xlab="Year",ylab="Precipitation (mm)")
lines(rain2$Y,rain2$TOT,col="red")
# average between locations
rain.AVG <- (rain1$TOT+rain2$TOT)/2
hist(rain.AVG,freq=FALSE,main="",xlab="Total rain (average between locations)")
x <- rain.AVG
curve(dnorm(x, mean=mean(rain.AVG), sd=sd(rain.AVG)), add=TRUE, col="red")
qqnorm(x,main="")
qqline(x,col="red")
A normally-distributed precipitation should imply an ARMA(1,1) river runoff,
based on the stochastic hydrological model introduced before. The monthly discharge
data for the Loire river at the Blois station are extracted from the cited Global Monthly
River Discharge Data Set (ID number 179), and are reported in the file Loire runoff.txt.
Before analysing annual data, let us take a look at the monthly flow time series, shown
in Fig. 6.25.
month # from January 1863
monthly runoff (m3/s)
0
200
400
600
800
1000
1200
1400
0 500 1000 1500
Fig. 6.25 Monthly runoff recorded at Blois (47o
340
48.000N latitude, 1o
190
48.000E longitude)224 ARMA Processes
Figures 6.26 (left) and (right) respectively show the sample ACF and PACF of
the flow data. The oscillating patterns clearly suggest a seasonal component in a
multiplicative ARIMA, with period 12.
0 10 20 30 40
−0.2 0.2 0.6 1.0
ACF
lag lag
0 10 20 30 40
−0.2 0.0 0.2 0.4 0.6
Partial ACF
Fig. 6.26 ACF (left) and PACF (right) of the time series of Figure 6.25.
A cut-and-try procedure allows us to establish a reasonable ARIMA model:
(1,0,2)×(2,1,1)12
i.e. an ARIMA(2,1,1) (AR2, MA1, integration order 1) for the seasonal part, and an
ARMA(1,2) for the residual stationary part. Once the ARIMA model is computed
we observe from a plot of the residuals versus fitted values that the residual vari￾ance is rather uneven (see Fig. 6.27). An increasing variance is never a good thing in
parametric statistical analysis, and it usually calls for some kind of transformation.
Indeed, by analysing the logarithm of the river flow the situation gets better. Figure
6.28 plots the residuals versus fitted values for a seasonal ARIMA model, having the
same orders (1,0,2)(2,1,1), applied to the logarithm of the runoff time series. Figure
6.29 compares the log of the monthly runoff (black line) to the values expected from
the seasonal ARIMA fitting (red line).
The code for analysing monthly runoff data, and producing Figs 6.25 to 6.29, is
the following.
## Code_6_13.R
# Analyse monthly runoff
setwd("C:/RPA/code/ARMA")
# read data
dati <- read.table("data/loire_runoff.txt",h=TRUE)
# convert discharge data to time series
X <- ts(dati$DISCHRG)
plot(X,ylab=expression(paste("Monthly runoff (m"^"3","/s)")),
xlab="Month # from January 1863",las=3)
# seasonal ARIMA analysis on monthly data
arima(X,order=c(1,0,2),seasonal=list(order=c(2,1,1),period=12)) -> fit
X.fitted <- X-fit$residuals
acf(X,na.action=na.pass,44,main="")
pacf(X,na.action=na.pass,44,main="")A physical application 225
200 400 600 800 1000
−500 0 500 1000
fitted
residuals
Fig. 6.27 Residuals of the ARIMA (1,0,2)×(2,1,1)12 model versus fitted values.
4.0 4.5 5.0 5.5 6.0 6.5 7.0
−1 0 1 2
fitted
residuals
Fig. 6.28 Residuals of the ARIMA (1,0,2)×(2,1,1)12 model versus fitted values, applied to
log(runoff).226 ARMA Processes
month # from January 1863
log(monthly runoff) (m3/s)
0
200
400
600
800
1000
1200
1400
4 5 6 7
Fig. 6.29 Logarithm of monthly runoff recorded at Blois (black line) and fitted values from
seasonal ARIMA (red line).
plot(X.fitted,fit$residuals,xlab="Fitted",ylab="Residuals")
# seasonal ARIMA analysis on log of monthly data
plot(log(X),ylab=expression(paste("log(Monthly runoff) (m"^"3","/s)")),
xlab="Month # from January 1863",las=3)
arima(log(X),order=c(1,0,2),seasonal=list(order=c(2,1,1),period=12)) -> fit.log
lines(fitted(fit.log),col="red")
plot(fitted(fit.log),fit.log$residuals,xlab="Fitted",ylab="Residuals")
We can now focus on annual runoff, to verify if a relation like (6.33) holds. Annual
runoff, computed from January to December4
can be straightforwardly obtained by
summing up monthly values. Figure 6.30 plots the standardized annual runoff time
series.
## Code_6_14.R
# Compute annual average
Y <- dati$YEAR
N <- length(dati$YEAR)
years <- Y[1]:Y[N]
n <- Y[N]-Y[1]+1
x <- rep(0,n)
for (i in 1:n){
x[i] <- sum(X[(12*(i-1)+1):(12*(i-1)+12)])
}
# recover NA value
x[113] <- 0.5*(x[112]+x[114])
4That is an arbitrary choice, of course. Annual runoff can be computed from any day, the only
constraint being the total period of 365 days, and annual values can slightly depend on such a choice.A physical application 227
# Standardized runoff x
x.m <- mean(x)
x.sd <- sd(x)
x <- ts((x-x.m)/x.sd,start=1863)
plot(x,xlab="Year",ylab=expression(paste("Annual runoff (m"^"3","/s)")),mgp=c(2,0.7,0))
year
annual runoff (m3/s)
1860 1880 1900 1920 1940 1960 1980
−2 −1 0 1 2 3
Fig. 6.30 Standardized annual runoff at Blois computed from monthly time series.
In order to obtain the proper ARMA model parameters, we can adopt the approach
introduced in Section 6.5.1, computing the AIC (6.5.1) for various AR and MA orders,
as follows.
## Code_6_15.R
# Manual search for best fitting:
# AR1, MA1, ARMA11, AR2, MA2, ARMA12, ARMA21, ARMA22
fit_AR1 <- arima(x,order = c(1,0,0))
fit_MA1 <- arima(x,order = c(0,0,1))
fit_ARMA11 <- arima(x,order = c(1,0,1))
fit_AR2 <- arima(x,order = c(2,0,0))
fit_MA2 <- arima(x,order = c(0,0,2))
fit_ARMA12 <- arima(x,order = c(1,0,2))
fit_ARMA21 <- arima(x,order = c(1,0,2))
fit_ARMA22 <- arima(x,order = c(2,0,2))
fits <- c(fit_AR1$aic,fit_MA1$aic,fit_ARMA11$aic,fit_AR2$aic,fit_MA2$aic,
fit_ARMA12$aic,fit_ARMA21$aic,fit_ARMA22$aic)
plot(fits, xlab="Model" ,ylab="aic",xaxt="n")
xtick = 1:8
# grids
for (ii in xtick){
abline(v=ii,lty=1,lwd=0.5)
}228 ARMA Processes
axTicks(2) -> ygrid
for (jj in ygrid){
abline(h=jj,lty=1,lwd=0.5)
}
#
axis(side=1, at=xtick, labels = FALSE)
text(x=xtick, par("usr")[3], labels =c("AR1","MA1","ARMA11","AR2","MA2","ARMA12","ARMA21",
"ARMA22"),srt = 90, pos = 2, xpd = TRUE,cex=0.85)
The result of the computation, graphically shown in Fig. 6.31, confirms that the
most suitable model is ARMA(1,1).
331.0 332.0 333.0 334.0
model
aic
AR1
MA1
ARMA11
AR2
MA2
ARMA12
ARMA21
ARMA22
Fig. 6.31 ARMA model choice for annual runoff time series.
6.7 Exercises
Exercise 6.1 A non-stationary process, consisting in a monotonic trend and a white noise,
exhibits a slowly decreasing linear ACF. For example, the process defined by:
Xt = βt + t
with β constant, has an ACF like that in Fig. 6.32. Remembering the definition of autoco￾variance at lag k:
γk = E [(Xt − µ)(Xt+k − µ)]
justify the linear behaviour in that particular case.
Hint: take β = 1 and consider the difference between subsequent lags, γk − γk+1Exercises 229
0 2 0 4 0 6 0 80 100
0.0 0.4 0.8
lag
ACF
Fig. 6.32 ACF of a linear trend
Exercise 6.2 With reference to Exercise 6.1, write an R code for computing the autocorre￾lation function by implementing:
γk =
1
N − k
NX−k
i=1
(Xi − µ)(Xi+k − µ)
and remembering that ρk = γk/γ0. Use the code to show that a process involving a quadratic
trend:
Xt = βt2 + t
also has a linear slowly decreasing ACF. Using the following time series data:
ti <- 0:1000
beta <- 1e-4
err <- rnorm(1001,sd=2)
x <- beta*ti^2+err
show that the 100 lag ACF is very similar to that of the linear trend process.
Exercise 6.3 The following code:
n <- 10
x_n <- runif(n)
Nr <- 100
X <- rep(x_n,Nr)
acf(X)
# series length
N <- Nr*n
shows that the ACF of a repeated pattern of period n has almost unitary values at lags n
and multiples of n. How can you explain it?
Hint: use the definition of autocorrelation, considering that N − n ≈ N.
Exercise 6.4 An invertible MA(1) process with parameter θ can be expressed as an infinite
AR. In fact, expressing the former in terms of the lag polynomial Θ(L) = 1 + θL, if Ψ(L)
denotes the inverse of Θ, we have:230 ARMA Processes
Ψ(L)Xt = Ψ(L) (Θ(L)wt) = wt
because ΨΘ = 1, with Ψ polynomial of infinite order:
Ψ(L) = 1 + ψ1L + ψ2L
2 + ψ3L
3 + ...
(i) Obtain the expression of the coefficients ψk.
(ii) How many of them are necessary to regain the noise wt?
(iii) Write an R program for computing the noise, if the MA(1) is given by the following code:
N <- 200
theta <- 0.5
w <- rnorm(N)
X <- rep(0,N)
# generate MA(1)
X[1] <- w[1]
for (t in 2:N) X[t] <- w[t] + theta*w[t-1]
and verify that the input and output noise are practically identical with just 5-6 terms.
Exercise 6.5 Repeat Exercise 6.4, items (i) and (iii), for an invertible MA(2) process with
MA coefficients θ1 = 0.8 and θ2 = 0.5. In this case, the process can be generated by:
N <- 200
theta1 <- 0.8
theta2 <- 0.5
w <- rnorm(N)
X <- rep(0,N)
# generate MA(2)
X[1] <- w[1]
X[2] <- w[2]
for (t in 3:N) X[t] <- w[t] + theta1*w[t-1] + theta2*w[t-2]
Exercise 6.6 A theorem coming from general linear process theory asserts that an AR(p)
process is stationary if and only if the modulus of all the roots of the characteristic polynomial
(associated to the lag polynomial) is greater than one. Demonstrate that, for an AR(2) process
with coefficients φ1 and φ2 and roots z1 and z2:
|z1| > 1 and |z2| > 1
implies the following relations:
φ1 + φ2 < 1
φ2 − φ1 < 1
|φ2| < 1
Hint: consider the roots λ1 and λ2 of the ‘reciprocal’ characteristic equation λ
2−φ1λ−φ2 = 0
instead of those of −φ1z−φ2z
2
, and determine conditions on the solutions satisfying |λ1| < 1
and |λ2| < 1.
Exercise 6.7 Write an R code graphically showing that an AR(1) process Xt = φXt−1 + t
gradually changes from random noise (φ = 0) to a random walk (φ = 1), passing through an
infinite number of stationary processes (for 0 < φ < 1). Which parameter can you compute
to distinguish between different AR(1) processes?Exercises 231
Exercise 6.8 Which model does the following expression represent?
Φ1(L)(1 − L)Xt = Θ2(L)t
with:
Φ1(L) = 1 − φL
and:
Θ2(L) = 1 + θ1L + θ
2
2L
Explicitly write down the model represented by it. Is that a stationary process? If not, how
can you get a stationary process from it?
Exercise 6.9 Simulate in R a series of 100 values extracted from an ARMA(1,2) model with
autoregressive coefficient φ = 0.5 and moving average coefficients θ1 = 0.4 and θ2 = −0.3,
using the function arima.sim. Refer to help(arima.sim) for the proper syntax.
Analyse its plot, and investigate its autocorrelation structure by computing and plotting
the ACF and PACF.
Exercise 6.10 With reference to Exercise 6.9, compare the ACF and PACF plots of the
ARMA(1,2) model with those of AR(1) and MA(2) processes with the same parameters. Try
to generate data several times and see how the autocorrelation functions change.
Hint: generate AR(1) and MA(2) data with arima.sim, and superimpose ACF and PACF of
the three models.7
Spectrum Analysis
Fourier's Theorem ...
is not only one of the most beautiful results of modern analysis,
but it may be said to furnish an indispensable instrument
in the treatment of nearly every recondite question in modern physics.
Lord William Thomson Kelvin, Treatise on Natural Philosophy
Fourier’s Theorem, as Lord Kelvin wrote, was actually a turning point in the history
of physics and applied mathematics. Essentially, that theorem asserts that any periodic
function f(x) of a variable x (not necessarily time), continuous and limited on a given
domain, can be represented as a sum of harmonic functions (sines and cosines or
complex exponentials) of suitable frequencies and amplitudes, the so-called Fourier
series. Spectrum analysis concerns the detection and computation of such frequencies.
The Fourier Theorem has a mathematical realization in the Fourier transform,
which essentially extends the Fourier series representation to aperiodic functions. Given
a time signal s(t) describing some physical process, the same information about the
process is contained in a function S(f) of the frequency, referred to as the Fourier
transform of s(t) and defined by:
S(f) = Z ∞
−∞
s(t)e
−2πif tdt (7.1)
A similar relation exists between s(t) and S(f):
s(t) = Z ∞
−∞
S(f)e
2πif tdf (7.2)
which defines the ‘inverse’ Fourier transform. If s(t) is a discrete time sequence:
{s(t);t = 0, ±1, ±2, ...} the integral (7.1) becomes a summation:
S(f) = X∞
t=−∞
s(t)e
−2πif t (7.3)
which is the definition of the discrete Fourier transform (DFT). Note also that if the
angular frequency ω = 2πf is used in place of f, and if ω is measured in radians
per sampling interval, i.e. it is assumed to belong to the interval [−π, π], the inverse
transform (7.2) becomes:
s(t) = 1
2π
Z π
−π
S(ω)e
iωtdω (7.4)Spectrum Analysis 233
Among the mathematical requisites for a function to posses a Fourier transform,
the most stringent is that of being square-summable:
X∞
t=−∞
|s(t)|
2 < ∞
a characteristic that, for an actual physical signal, means it has finite energy (Stoica
and Moses, 2005).
Before tackling the spectral analysis of stochastic processes, consider a simple de￾terministic signal s(t) consisting of a sinusoidal function of time:
s(t) = Asin(2πf t)
A is the signal amplitude and f its frequency, i.e. the number of times the signal
repeats itself in one second or, in other terms, the inverse of the period of the sinusoid.
Figure 7.1 plots such a signal for A = 1 and f = 0.05 Hz, corresponding to a period
T = 20 s. In this simple case, f is the only frequency, and we do not need to invoke
Fourier to see that. But what can we say of the signal in Figure 7.2?
0 20 40 60 80 100
−1.0 −0.5 0.0 0.5 1.0
t (s)
sin(2"ft)
Fig. 7.1 Time plot of a sinusoidal signal.
Looking at the time behaviour of that signal, it appears that more than one repe￾tition frequency is present there. Indeed, we can count five repetitions in 100 s of the
whole signal s(t) (having several positive and negative peaks of different amplitudes)
but s(t) is anything but sinusoidal. In terms of the angular frequency ω = 2πf, the
spectrum S(ω) of s(t) is that shown in Figure 7.3(right), compared to the spectrum
of a single sinusoid (left). Indeed, the signal in Fig. 7.2 is:
s(t) = A1sin(2πf1t) + A2sin(2πf2t) + A3sin(2πf3t)234 Spectrum Analysis
0 20 40 60 80 100
−2 −1 0 1 2
t (s)
s(t)
Fig. 7.2 Time plot of a complex deterministic signal.
with A1 = A2 = A3 = 1, f1 = 0.05 Hz, f2 = 0.1 Hz, f3 = 0.2 Hz.
The spectrum is computed through the FFT (fast Fourier transform) procedure,
a fast numerical implementation of the discrete Fourier transform, or DFT, which is
the Fourier transform (7.1) applied to discrete data. 1 The R code generating Figures
7.1, 7.2 and 7.3 is shown below.
0.00 0.02 0.04 0.06 0.08 0.10
0 1000 3000 5000
frequency (Hz)
|S(
w)|
0.0 0.1 0.2 0.3 0.4 0.5
0 10000 30000 50000
frequency (Hz)
|S(
w)|
Fig. 7.3 (Left) Spectrum of a sinusoid (with reference to Fig. 7.1) and (right) spectrum of
a composition of sinusoids (with reference to Fig. 7.2).
## Code_7_1.R
# Single sinusoid
1We will not go into the depths of FFT, which is exhaustively explained in hundreds of texts. We
only warn the reader that the FFT algorithm requires the sample length to be a power of 2. Such a
limitation can be circumvented by the zero-padding procedure (Stoica and Moses, 2005).Spectrum Analysis 235
deltaT <- 1/10
T <- 1000
N <- T/deltaT
t <- seq(0,T,deltaT)
A1 <- 1
f1 <- 0.05
s <- A1*sin(2*pi*f1*t)
# plot sample of 100 seconds
plot(t[1:1000],s[1:1000],t="l",xlab="t (s)",ylab=expression(paste("sin(2",pi,"f t)")))
abline(v=c(0,T/50,2*T/50,3*T/50,4*T/50,T/10),lty=2,col="red")
abline(h=c(-1,-0.5,0,0.5,1),lty=2,col="red")
fs <- 1/deltaT
deltaf <- fs/N
S <- fft(s);
Sm <- abs(S)
fr <- deltaf*(0:(N/2))
# plot frequencies up to 0.1 Hz
plot(fr[1:100],Sm[1:100], type="h",xlab="frequency (Hz)",
ylab=expression(paste("|S(",omega,")|")))
## Code_7_2.R
# Sum of three sinusoidal signals
deltaT <- 1/10
T <- 10000
N <- T/deltaT
t <- seq(0,T,deltaT)
A1 <- 1
f1 <- 0.05
A2 <- 1
f2 <- 0.1
A3 <- 1
f3 <- 0.2
s <- A1*sin(2*pi*f1*t)+A2*sin(2*pi*f2*t)+A3*sin(2*pi*f3*t)
# plot sample of 100 seconds
plot(t[1:1000],s[1:1000],t="l",xlab="t (s)",ylab="s(t)")
abline(v=c(0,T/500,2*T/500,3*T/500,4*T/500,T/100),lty=2,col="red")
abline(h=c(-2,-1,0,1,2),lty=2,col="red")
fs <- 1/deltaT
deltaf <- fs/N
S <- fft(s);
Sm <- abs(S)
fr <- deltaf*(0:(N/2))
# plot frequencies up to 0.5 Hz
plot(fr[1:5000],Sm[1:5000], type="h",xlab="frequency (Hz)",
ylab=expression(paste("|S(",omega,")|")))
Figure 7.4 summarizes the transformation between time and frequency domains.
Note that N time samples (input) produce N frequency samples (output), but only half
of them are useful for the spectrum analysis because the Fourier transform produces
a symmetric frequency range between −fs/2 and fs/2. Given the sampling time ∆t,
which is the total time duration divided by the number of samples, the sampling
frequency fs is the reciprocal of ∆t. fc ≡ fs/2 is the critical frequency, also known as
the Nyquist frequency, the highest frequency that can be detected from data having236 Spectrum Analysis
a time spacing ∆t. Any frequency component outside the range (−fc, fc) is wrongly
translated into that interval, the phenomenon known as ‘aliasing’.
TIME
1.0 0.5 0.0
sin(2f t)
–0.5 –1.0
0 20 40
t (s)
60 80 100
0
0.0 0.2 0.4 0.6
frequency (Hz)
0.8 1.0
100 200
|S(ω)|
300 400 500
N samples
n = 0,1.... N-1
∆t = T/N
t: 0, ∆t, ... (N-1)∆t f: 0, ∆f, ... (N-1)∆f
Actual frequency range:
0...f
c
f
c
 = f
s
/2 critical (Nyquist) frequency
∆f = f
s
/N
f
s
 = 1/∆t
k = 0,1,...N-1
N samples
band B = f
s duration: T (s) (Hz)
FREQUENCY
Fig. 7.4 Transformation between time and frequency domain.
We briefly come back to the definitions (7.3) and (7.4). If S(ω) is a physical signal,
its squared absolute value:
ESD(ω) = S(ω)
2
represents its energy spectral density.
Remark 7.1 The above definition has a physical origin. If v(t) is the voltage across a resistor
of unitary resistance
Z ∞
−∞
v(t)
2
dt
represents the total energy dissipated in the resistor (v(t) is a real function). By the Parseval’s
theorem, if the complex function V (ω) denotes the Fourier transform of v(t), we have:
Z ∞
−∞
v(t)
2
dt =
1
2π
Z ∞
−∞
|V (ω)|
2
dω
and V (ω)
2
is an ‘energy spectral density’.Spectrum of stochastic signals 237
If we compute the total ‘energy’, substituting (7.3) in the above definition we
obtain:
1
2π
Z π
−π
ESD(ω)dω =
X∞
t=−∞
|s(t)|
2
(7.5)
where the angular frequency is in radians per cycle, as in (7.4).
The energy spectral density is related to the autocorrelation function, which is
analogous to the statistical autocovariance defined in eqn (3.4). We are dealing with a
deterministic signal, so that definition is modified to express the autocorrelation R(k)
of a deterministic, finite-energy signal:
R(k) = X∞
t=−∞
s(t)s(t − k)
As for random time series, the autocorrelation of a deterministic signal measures the
similarity of it with delayed versions of the signal itself, the first-order R(0) expressing
the signal energy. It is easy to verify that the following relationship holds:
X∞
k=−∞
R(k)e
−iωk = ESD(ω)
i.e. the energy spectral density is the DFT of the autocorrelation function. In other
words, the energy spectral density and the autocorrelation function are Fourier trans￾form pairs.
7.1 Spectrum of stochastic signals
The application of the Fourier analysis to a stochastic process is pretty similar to
that seen for deterministic signals. Indeed, a time series – a particular realization
of a stochastic process (see Chapter 3) – is nothing different from a function s(t).
Therefore, the spectrum (Fourier transform) S(ω) would appear to be a well-defined
quantity. Actually it is not, for several reasons.
First of all, there is a conceptual problem. A finite time series is only a particular
‘realization’ of a stochastic process, therefore its Fourier transform does not necessarily
shed light on the underlying process itself. In other words, only probabilistic statements
can be made on a random phenomenon. A key assumption in the spectral analysis of
stochastic processes is ergodicity: if the equivalence between ensemble averages and
time averages would not hold, we should not be allowed to infer process characteristics
from a single realization of it.
Moreover, a discrete-time sequence {s(t);t = 0, ±1, ±2, ...} does not have finite
energy, if it is supposed to be extracted from a ‘virtually’ infinite time series including
all possible realizations of the process. Thus, the finite-energy requirement cannot be
assumed with certainty, and s(t) does not rigorously2 possess a DFT.
2Making the DFT of a finite discrete time series is of course possible, but it corresponds to take
s(t) = 0 outside the sampling time interval.238 Spectrum Analysis
In the following, the process s(t) will be assumed to be wide-sense stationary. We
can furthermore assume the time series to have zero average, E[s(t)] = 0, without
any undesired counter-effect. By analogy with the definition of the energy spectral
density of a deterministic signal, we can define the power spectral density (Jenkins
and Watts, 1968) PSD of a stochastic process as the (discrete) Fourier transform of
the autocovariance function γk defined in (3.4):
PSD(ω) = X∞
k=−∞
γke
−iωk (7.6)
whose inverse transform gives the autocovariance function:
γ(k) ≡ γk =
1
2π
Z π
−π
PSD(ω)e
iωkdω
If s(t) is real-valued, the power spectral density (7.6) can be expressed in terms of
the Fourier cosine transform:
PSD(ω) = γ0 + 2X∞
k=1
γkcos(ωk) (7.7)
7.1.1 Periodogram and power spectral density (PSD) estimation
Both previous definitions, eqns (7.6) and (7.7), involve an infinite sample. We might
be tempted to modify the PSD definition in terms of a finite length N of the time
series:
φP (ω) = X
N
k=−N
γke
−iωk (7.8)
or
φP (ω) = γ0 + 2X
N
k=1
γkcos(ωk) (7.9)
for a real-valued series.
Equations (7.8) and (7.9) define the periodogram for ω ∈ [−π, π]. In terms of the
time series s(t) it is obviously defined as:
φP (ω) = 1
N
X
N
k=1
s(k)e
−iωk
2
(7.10)
The periodogram has a very simple interpretation when s(t) is periodic, and thus
can be represented on a basis of orthogonal harmonic functions:Spectrum of stochastic signals 239
s(t) = a0 +
X
N
j=1
[aj cos(2πfj t) + bj sin(2πfj t)]
and the fj = j/N are Fourier frequencies, i.e. integer multiples of f1 = 1/N. The above
relation, the Fourier series, appears as a regression with coefficients aj , bj , which can
be estimated, for example by means of a least-squares procedure (Box, Jenkins and
Reinsel, 1994), as:
a0 = ¯s
aj =
2
N
X
N
t=1
stcos 
2π
j
N
t

bj =
2
N
X
N
t=1
stsin 
2π
j
N
t

for j = 1...n if N = 2n + 1 is odd, and as:
aj =
1
N
X
N
t=1
(−1)t
st
bj = 0
for j = 1...n − 1, if N = 2n is even.
In those cases the periodogram consists of n values:
φP (fj ) = N
2
(a
2
j + b
2
j
) j = 1..(n-1)
φP (fn) = N
2
(a
2
n + b
2
n
) if N is odd (7.11)
φP (fn) = N a2
n
if N is even
Note that with the adopted units the highest frequency is 1/2 cycles per sampling inter￾val. Limiting ourselves to the case N odd, there are n = (N −1)/2 pairs of coefficients.
The j-th component of the periodogram defined by (7.11), φP (j) = N/2(a
2
j + b
2
j
),
is associated with the variance explained by the j−th couple of coefficients in the
regression, because:
X
N
t=1
(st − s¯)
2 =
Xn
k=1
φP (fk)
In the most general case, s(t) not periodic and frequencies not Fourier harmonics
(i.e. not integer multiples of 1/N), the periodogram still maintains a relationship with
the ‘variance’ of the process, being related to its autocovariance function by (7.9).240 Spectrum Analysis
As mentioned before, a single finite time succession is not sufficient for inferring the
properties of the underlying stochastic process. If we could repeat the observations
we would obtain a population of pairs of coefficients (af ,bf ) related to any frequency
value f, due to the randomness of the process. Therefore, we could think that a better
estimate of the periodogram is its expected value. For example, from (7.8), we obtain:
E[φP (ω)] = X
N
k=−N
E[ ˆγk]e
(−iωk)
(7.12)
where we have used the ˆ symbol to specify that ˆγj are sample quantities.
In the limit N → ∞, the expected value of ˆγj of the sample autocovariances tends
to the theoretical γj , so the sample autocovariances can be substituted for the the￾oretical, unknown values. But in general the sample spectrum fluctuates about the
theoretical spectrum, and its variance does not vanish with increasing length of the
time series (Jenkins and Watts, 1968) or, in other words, E[φP (ω)] is an inconsistent
estimator of the theoretical periodogram.3
If you find that result strange, you should
reflect about this: the fact that a quantity y is a consistent estimate of a statistics Y
does not imply at all that the Fourier transform F(y) consistently estimates F(Y ).
Let us summarize what we learnt and what we still need to define.
• The knowledge of the power spectrum density and of the autocovariance structure
are perfectly equivalent, by virtue of (7.7).
• The periodogram φP (ω) is an estimate of the power spectral density PSD(ω) which
consists of ‘truncating’ the infinite time series to the available sample, which has
a finite length.
• The expected value E[φP ] is not consistent (we will see some numerical demon￾strations in the following).
The final item needs an in-depth analysis. First of all, suppose you have an estimator
ˆθ of a quantity θ. The ‘correctness’ of the estimator is usually determined in terms of
unbiasedness and consistency. Consider how close ˆθ is to θ, for example by computing
the mean squared error of the estimator:
MSE(ˆθ) ≡ E[(ˆθ − θ)
2
]
which, once developed, gives:
MSE(ˆθ) = E[(ˆθ − E[
ˆθ])2
] + (E[
ˆθ] − θ)
2
The first term is the variance, the second is the square of the bias:
var(ˆθ) = E[(ˆθ − E[
ˆθ])2
] (7.13)
bias(ˆθ) = (E[
ˆθ] − θ) (7.14)
3Anyway, the limit operation cannot be actually performed: in the end, we have the data we have!Spectrum of stochastic signals 241
The computation of the bias and variance of the periodogram as an estimator
of the power spectral density is not easy; in particular the latter is not possible in
the general case while it is affordable for a Gaussian white noise process (Stoica and
Moses, 2005). The main result is that the periodogram φP (ω) is an unbiased estimator
for the power spectral density PSD(ω) of a Gaussian noise, while it is asymptotically
unbiased for other stationary processes. But, var(φP (ω)) does not vanish in any case
with increasing sample size.
In order to compute the bias of the periodogram:
bias(φP (ω)) = E[φP (ω)] − PSD(ω)
of course, we need to know the theoretical PSD but, anyway, the quantity depending
on the characteristics of the sample (in particular its length N) is E[φP (ω)]. From
(7.12) we need to compute the expected values of the autocovariance functions. The
sample autocovariances, computed from a sample {s(t) : t = 1...N}, are defined by:
γˆk =
1
N
N−X
1−|k|
t=0
(s(t) − s¯) (s(t − k) − s¯)
for k = −(N − 1)...(N − 1). As a consequence:
E[ ˆγk] = N − |k|
N
γk (7.15)
and we are able to compute the expected value of the periodogram:
E[φP (ω)] =
N
X−1
k=−(N−1)

1 −
|k|
N

γke
−iωk (7.16)
The quantity in parenthesis inside the summation is the so-called ‘Bartlett’ or
triangular window:
wB(k) = 
1 − |k|/N , if k ∈ (−N, N)
0 , otherwise
which allows us to rewrite (7.16) between infinite limits:
E[φP (ω)] = X∞
k=−∞
wB(k)γ(k)e
−iωk (7.17)
Note that wB(k) has the shape shown in Fig. 7.5.
Equation (7.17) is the DFT of a product, therefore it is the convolution between
the DFT of the autocovariance, i.e. the PSD (7.6), and that of the Bartlett window,
WB(ω):
E[φP (ω)] = 1
2π
Z π
−π
PSD(ξ)WB(ω − ξ)dξ (7.18)242 Spectrum Analysis
−10 −5 0 5 10
0.2 0.4 0.6 0.8 1.0
k
WB
Fig. 7.5 Bartlett window.
The Fourier transform WB(ω) can be easily demonstrated to be:
WB(ω) = 1
N

sin(ωN/2)
sin(ω/2) 2
(7.19)
which exhibits an oscillatory behaviour depending on N as shown in Fig. 7.6 for N = 11
(black line) and N = 110 (red line):
Figure 7.6 shows that increasing N by an order of magnitude, the width of the
main ‘lobe’ of (7.18) rapidly decreases, WB approaches a Dirac delta function and,
finally, φP (ω) tends to PSD(ω). In other words, the periodogram is an asymptotically
unbiased estimator of the power spectral density.
Computing the variance of the periodogram, in order to complete the evaluation
of its goodness as an estimator of the spectral power, is not a simple task, as we
mentioned before. Instead of replicating a result that the interested reader can find
in the literature (Stoica and Moses, 2005), simply we will show the behaviour of the
variance of the periodogram in a specific case. Suppose the process, whose the series
st : t = 1..N is one realization, is a Gaussian noise with unitary standard deviation.
The variance of the periodogram, from (7.13), is:
var(φP (ω)) = E
h
(φP (ω) − E[φP (ω)])2
i
(7.20)
in particular, we are interested to the asymptotic value of variance. We have seen that
φP asymptotically tends to PSD and we also asserted (the demonstration is left to
the reader) that it is unbiased for a white noise process. White noise was introduced
in Section 6.1 and we will see just ahead that a Gaussian noise is characterized bySpectrum of stochastic signals 243
−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5
0.0 0.2 0.4 0.6 0.8 1.0
ꞷ
normalized W
B(ꞷ) 
Fig. 7.6 DFT of the Bartlett window.
constant variance (as a function of frequency), equal to the noise variance, unitary in
the case at hand. For a Gaussian process ∼ N (0, 1), therefore, (7.20) reduces to:
var(φP (ω)) = E[φP (ω)
2
] − E[PSD(ω)
2
] = E[φP (ω)
2
] − 1 (7.21)
Thus, E[φP (ω)
2
] is the only quantity we need to compute. The code below computes
that quantity by running 100 simulations for N = 10 (thin line), N = 100 (dashed
line) and N = 10000 (thick line) to investigate the asymptotic behaviour of the peri￾odogram variance. Figure 7.7 shows that var(φP (ω)) tends to a constant value (one,
as the process variance) with increasing N. Thus, the periodogram is an inconsistent
estimator of power spectral density.
## Code_7_3.R
# Gaussian noise
set.seed(246)
phi2.10 <- rep(0,100)
phi2.100 <- rep(0,100)
phi2.10000 <- rep(0,100)
for (i in 1:100)
{
w <- rnorm(10)
w.spec <- spectrum(w,log="no",plot=FALSE)
phi <- w.spec$spec
phi2.10[i] <- mean(phi^2)-1
w <- rnorm(100)
w.spec <- spectrum(w,log="no",plot=FALSE)
phi <- w.spec$spec
phi2.100[i] <- mean(phi^2)-1
w <- rnorm(10000)
w.spec <- spectrum(w,log="no",plot=FALSE)
phi <- w.spec$spec
phi2.10000[i] <- mean(phi^2)-1244 Spectrum Analysis
0 2 0 4 0 6 0 80 100
0 2 4 6 8
run
E[
f2
P]
Fig. 7.7 Variance of the periodogram of Gaussian white noise with σ
2 = 1: N = 10 (thin
line), N = 100 (dashed line), N = 10000 (thick line).
}
plot(phi2.10,t="l",xlab="run",ylab=expression(paste("E[",phi[P]^2,"]")))
lines(phi2.100,col="red")
lines(phi2.10000,col="blue")
7.1.2 Consistent estimation of power spectral density
It should be clear from the previous discussion that the ‘raw’ periodogram is not
very suitable for representing the power spectrum of a time series. As a numerical
confirmation, Fig. 7.8 shows the raw periodogram of a time series extracted from a
Gaussian white noise process ∼ N (0, 4) with variance σ
2 = 4 (dashed line). Could you
infer that we are dealing with a frequency-independent noise from that figure?
It appears reasonable to apply some kind of smoothing procedure to the peri￾odogram in order to obtain a less variable spectrum. The shape of equation (7.17)
suggests how to do it: the Bartlett window wB can be substituted with a more suit￾able window function w, whose Fourier transform - or, practically, whose DFT −
has lower side lobes compared to WB. In other words, the smoothed periodogram is
computed by:
φˆ
P (ω) = X
M
k=−M
w(k)ˆγ(k)e
−iωk (7.22)
w is sometimes called ‘lag’ window, because it operates on the lag k.
The raw periodogram corresponds to M = N and w(k) = 1 − |k|/N, the Bartlett
window. Windowing is a well-established technique, commonly used in signal pro￾cessing (Poularikas, 1999). A variety of smoothing windows can be applied for ourSpectrum of stochastic signals 245
0.0 0.1 0.2 0.3 0.4 0.5
0 5 10 15 20 25
ω/2!
fP (ꞷ)
Fig. 7.8 Periodogram of Gaussian white noise with σ
2 = 4 (solid line), compared to the
theoretical PSD (dashed line).
purpose, for example a ‘cosine-taper’ such as the Hann window which, for a sample
length N = 2M + 1, is defined by:
w(k) = 1
2

1 + cos 
πk
M

Its DFT is:
W(ω) = 1
4
h
2D(ω) + D(ω −
π
M
) + D(ω +
π
M
)
i
where D is the Dirichlet kernel:
D(ω) = sin(Mω)
sin(ω/2)
The effect of that smoothing window on the spectrum estimation should be clear
from Fig. 7.9(left) and (right), respectively comparing the Hann (solid line) and Bartlett
(dots) window and their DFTs, with N = 51, M = 25.
The purpose of the window is to provide a smooth decay to zero for the extreme
values of k. That reflects in lower side lobes in the Fourier transform of the window,
producing a smoothing in the power spectral density estimation. Several other windows
give lower side lobes than Hann’s, for example the Blackman-Tukey, which is a slight
modification of the former:
w(k) = 0.42 + 0.5cos 
πk
M

+ 0.08cos 
2πk
M

The choice of the window is related to the amount of smoothing we can tolerate
(in order to reduce variance) without reducing too much the resolution of the spectral
analysis. The R function spectrum, in the base stats package allowing statistical246 Spectrum Analysis
−20 −10 0 10 20
0.0 0.2 0.4 0.6 0.8 1.0
k
w
−3 −2 −1 0 1 2 3
1e−05 1e−03 1e−01
ω
Normalized W(ω)
Fig. 7.9 (Left) Comparison between Hann (solid line) and Bartlett window (dots); (right)
Comparison between their DFTs (same line specifications).
calculations, invokes the spec.pgram which allows various kinds of smoothing. By
defining the ‘span’ argument, the Daniell moving-average window is used. This last
kind of smoothing is a ‘local’ one where, based on the spans value m, the value of
φˆ
P (ω) at ω = ωk is computed by averaging on the values at ωk−m...ωk+m:
φˆ
P (ωk) = 1
2m + 1
Xm
i=−m
φP

ωk +
i
m

We will see application examples and operative details of the spectrum function in
the next sections.
7.2 Noise spectrum
We have encountered white noise several times throughout the book. It’s time to give
it a more deep look.
A seemingly good definition of noise is ‘spurious’ or ‘unwanted’ signal. When the
meaning of signal is well-defined, think for example to a piece of music or to a pho￾tographic image, such a definition works: noise is everything confusing the signal con￾taining information, i.e. acoustic or visual disturbances. Anyway, the physical nature
of noise and signal is the same: acoustics waves in music, electromagnetic waves in
image visualization.
It therefore appears that what is noise and what is signal depends on the ‘receiver’.
The distinction between wanted and unwanted signals is a matter of information con￾tent. Thus, a better definition is that noise is the part of data we choose not to explain.
In Chapter 6 we saw that white noise, i.e. noise having a flat frequency spectrum,
plays a fundamental role in ARMA models. We go back to the definition (6.1) of white
noise of variance σ
2
. Substituting in (7.7) the zero-th order autocovariance γ0 = σ
2
,
all other γk being zero for k > 0, we obtain the constant power spectral density that
motivates the ‘white’ attribute. In order to estimate the power spectrum, we apply
some smoothing procedure to the time series giving rise to Fig. 7.8.Noise spectrum 247
In function spectrum the argument kernel can be used in place of spans to use a
different Daniell kernel. the R function kernel (also belonging to the stats package)
can be used inside plot to show the kernel shape. Figure 7.10 compares a simple
Daniell moving average window, based on five points, with a more complex window
with different weights. The two figures are generated by the following code:
# Daniell kernels
plot(kernel("daniell",5),main="")
plot(kernel("daniell",c(3,3)),main="")
−4 −2 0 2 4
0.06 0.08 0.10 0.12
k
W[k]
−6 −4 −2 0 2 4 6
0.02 0.06 0.10 0.14
k
W[k]
Fig. 7.10 Daniell kernel with moving average over five points (left) and modified Daniell
kernel with smooth behaviour (right).
Kernels such those shown in Fig. 7.10 (a,b) are employed inside the spectrum com￾mand as shown below. Figure 7.11 shows the spectrum of a Gaussian white noise series
of length 1000 and variance 4, highlighting the effect of the smoothing window on the
power spectral density estimation.
0.0 0.1 0.2 0.3 0.4 0.5
2 3 4 5 6
ω/2!
fP(ω)
^248 Spectrum Analysis
Fig. 7.11 Smoothed periodogram of Gaussian white noise with σ
2 = 4 (Daniell kernel: dotted
line, modified Daniell kernel: solid line), compared to the theoretical PSD (thick dashed line).
See the R code producing this figures.
## Code_7_4.R
# Gaussian white noise smoothed periodogram
set.seed(123)
w <- rnorm(1000,mean=0,sd=2)
w.spec <- spectrum(w,log="no",kernel=kernel("daniell",51),plot=FALSE)
plot(w.spec$freq,w.spec$spec,t="l",ylab=expression(paste(hat(phi)[P],"(",omega,")")),
xlab=expression(paste(omega,"/2",pi)),ylim=c(2,6),mgp=c(2.2,1,0),col="blue",lty=3)
w.spec <- spectrum(w,log="no",kernel=kernel("daniell",c(51,51)),plot=FALSE)
lines(w.spec$freq,w.spec$spec,col="black")
grid(lty=1)
abline(h=4,col="red",lwd=2)
7.2.1 Red and blue spectrum
The definition of white noise as a constant-spectrum signal has been extended to
coloured noise, meaning with that term ‘noise with a non constant spectral density’.
We look at two examples, treated in Section 6.2, dealing with two MA(1) processes
with θ = 0.5 and θ = −0.5 respectively, and σ
2 = 1. Equation (6.7) allows computing
the two non-zero autocovariances for such processes, and the theoretical power spectral
density by means of (7.7):
PSD(ω) = σ
2
￾
1 + θ
2 + 2θcos(ω)

, ω ∈ [0, π]
The spectrum relative to an MA(1) process with positive coefficient concentrates its
power density at low frequency, thus resembling a red noise (remember that the visible
spectrum of electromagnetic energy goes from red to blue, with increasing frequency).
Conversely, a negative coefficient brings a power density increasing with frequency, i.e.
shifted towards blue.4 The above two spectra are shown in Fig. 7.12, comparing the
theoretical spectra with the smoothed periodograms, as detailed in the R code that
follows.
## Code_7_5.R
## red and blue noise
##MA(1) positive coefficient
set.seed(246)
# generate white (Gaussian) noise
w <- rnorm(100)
x1 <- rep(0,100)
x2 <- rep(0,100)
theta1 <- 0.5
theta2 <- -0.5
# generate MA(1)
x1[1] <- w[1]
x2[1] <- w[1]
for (t in 2:100) {
4The above depends on the choice of the sign in the definition (6.6) of the MA(1) process. If the
process were defined by Xt = t − θt−1, as some authors do, red and blue spectra would correspond
to negative and positive θ coefficients, respectivelyApplications of spectrum analysis 249
0.0 0.1 0.2 0.3 0.4 0.5
0.0 0.5 1.0 1.5 2.0 2.5 3.0
frequency (cycles/sample interval)
spectral density
0.0 0.1 0.2 0.3 0.4 0.5
0.0 0.5 1.0 1.5 2.0 2.5 3.0
frequency (cycles/sample interval)
spectral density
Fig. 7.12 Estimated (solid line) and theoretical (dashed line) spectrum for MA(1) processes
representing red noise (left) and blue noise (right).
x1[t] <- w[t] + theta1*w[t-1]
x2[t] <- w[t] + theta2*w[t-1]
}
# plot spectral densities
omega <- seq(0,pi,pi/100)
x1.spec <- spectrum(x1,log="no",kernel=kernel("daniell",c(11,11)),plot=FALSE)
plot(x1.spec$freq,x1.spec$spec,t="l",ylim = c(0,3),
xlab="frequency (cycles/sample interval)",ylab="spectral density")
grid(lty=1)
lines(omega/(2*pi),1+theta1^2+2*theta1*cos(omega),col="red")
x2.spec <- spectrum(x2,log="no",kernel=kernel("daniell",c(11,11)),plot=FALSE)
plot(x2.spec$freq,x2.spec$spec,t="l",ylim = c(0,3),
xlab="frequency (cycles/sample interval)",ylab="spectral density")
grid(lty=1)
lines(omega/(2*pi),1+theta2^2+2*theta2*cos(omega),col="red")
7.3 Applications of spectrum analysis
As a simple example of application we will conduct an exploratory analysis on a
time series containing the maximum temperature values measured monthly in Bologna
(Italy) from 1961 to 1991 (see the example in Section 6.5.3). Figure 6.16 shows the
time series and the ACF. Note that this time the frequency peak (see Fig. 7.13) is
so sharp and well defined that a smoothing window would introduce an artefact The
code below shows the R code for computing the spectrum; the last two lines find the
index of the peak of the spectral density and use this index to compute the period,
i.e. the reciprocal of the frequency, which is nearly 12 months as expected.
## Code_7_6.R
## Temperature analysis
# Maximum (monthly averages) temperatures in Bologna: years 1961-1991
# change directory to that containing the data
setwd("C:\RPA\code\spectrum_analysis\data")
maxTempData <- read.table("Bologna_Tmax_1961_1991.txt")
Tmax <-maxTempData$V1
T.spec <- spectrum(Tmax,log="no",plot=FALSE)250 Spectrum Analysis
plot(T.spec$freq,T.spec$spec,t="l",ylab=expression(paste(phi[P],"(",omega,")")),
xlab=expression(paste(omega,"/2",pi)),mgp=c(2.2,1,0))
grid(lty=1)
# Find index of the spectrum peak
max_index <- which(T.spec$spec==max(T.spec$spec))
period <- 1/T.spec$freq[max_index]
That’s exactly the same result we found from analysing the autocorrelation struc￾ture (Fig. 6.16). Indeed, spectral density and autocorrelation exactly convey the same
information, as the relationships (7.6)-(7.7) indicate.
0.0 0.1 0.2 0.3 0.4 0.5
0 2000 6000 10000
ω/2!
fP(ω)
Fig. 7.13 Raw periodogram of the maximum monthly temperatures in Bologna (Italy) from
1961 to 1991.
7.3.1 Searching for hidden periodicity
The search of hidden periodicity is one of the objectives of spectral analysis. In this last
example we will analyse a time series of rainfall data, extracted from the UK National
River Flow Archive (NRFA).5 NRFA is the primary archive of daily and peak river
flows for the UK: it incorporates daily, monthly and flood peak data from over 1500
gauging stations, and it also provides catchment rainfall time series consisting of the
total rainfall averaged over the catchment in millimetres per day. We will consider a
catchment area of 122 km2
in the south of UK (Hampshire) where the river Dever
flows. The time series (Fig. 7.14) covers 57 years (684 months) from January 1961 to
December 2017.
Monthly and annual time series are computed from the above data, and a spectral
analysis is conducted on both series. Figure 7.15 shows the smoothed periodogram of
5Website: https://nrfa.ceh.ac.uk/Applications of spectrum analysis 251
1960 1970 1980 1990 2000 2010 2020
0 10 20 30 40 50
days from 1/1/1961 to 31/12/2017
rainfall (mm)
Fig. 7.14 Daily rainfall at Bransbury (UK) from 1961 to 2017.
the annual time series. If we compute the frequency position of the first two maxima,
with reference to the R code shown below, we obtain periods of 6.67 and 2.30 years.
Figure 7.16 shows the power spectral density estimated for the monthly time series.
The solid line confirms the existence of an expected periodicity with annual frequency.
The dashed lines correspond to the first two maxima found in the annual series (see
Fig. 7.15). The dotted lines suggest the presence of other characteristic frequencies
corresponding to periods of about 6 and about 4 months.
## Code_7_7.R
# Searching hidden periodicity
library(lubridate)
# change directory to that containing the data
setwd("C:\RPA\code\spectrum_analysis\data")
# read data
cdr <- read.table("42027_cdr.txt")
as.Date(cdr$V1) -> cdr.date
rain.daily <- cdr$V2
# plot daily time series
plot(cdr.date,rain.daily,t="l",ylab="Rainfall (mm)",
xlab="Days from 1/1/1961 to 31/12/2017")
# Compute monthly series
rain.monthly <- rep(0,(2017-1961)*12)
i <- 1
for (Y in 1961:2017) {
for (j in 1:12){
tmp <- rain.daily[year(cdr.date)==Y & month(cdr.date)==j]
rain.monthly[i] <- sum(as.numeric(as.character(tmp)))
i <- i+1252 Spectrum Analysis
0.0 0.1 0.2 0.3 0.4 0.5
5000 15000 25000 35000
w/2p
PSD
(w)
^
Fig. 7.15 Smoothed periodogram of annual rainfall at Bransbury (UK) from 1961 to 2017.
0.0 0.1 0.2 0.3 0.4 0.5
2000 4000 6000 8000
ω/2!
PSD(ω)
^
1/12
Fig. 7.16 Smoothed periodogram of monthly rainfall at Bransbury (UK) from 1961 to 2017.Singular Spectrum Analysis 253
}
}
rain.annual <- rep(0,57)
for (i in 1:57){
rain.annual[i] <- sum(rain.monthly[(1+(i-1)*12):(12+(i-1)*12)])
}
# spectrum of annual series
spectrum(rain.annual,log="no",kernel("daniell",c(1,1)),plot=FALSE) -> annual
max_index <- which(annual$spec==max(annual$spec))
annual.period <- 1/annual$freq[max_index]
plot(annual$freq,annual$spec,xlab=expression(paste(omega,"/2",pi)),
ylab=expression(paste(hat(P)[SD],"(",omega,")")),t="l",mgp=c(2.2,1,0))
grid(lty=2)
abline(v=annual$freq[max_index],col="red")
abline(v=0.434,col="red",lty=2)
# spectrum of monthly series
spectrum(rain.monthly,log="no",kernel("daniell",c(3,3)),plot=FALSE) -> monthly
plot(monthly$freq,monthly$spec,xlab=expression(paste(omega,"/2",pi)),
ylab=expression(paste(hat(P)[SD],"(",omega,")")),t="l",mgp=c(2.2,1,0))
grid(lty=1)
abline(v=1/12,col="green")
text(0.12,8000,"1/12")
abline(v=1/(12*annual.period),col="red",lty=2)
abline(v=0.434/12,col="red",lty=2)
abline(v=0.165,col="blue",lty=3)
abline(v=0.265,col="blue",lty=3)
7.4 Singular Spectrum Analysis
In several processes wide-sense stationarity cannot be assumed valid. In those cases the
Wiener-Khinchin theorem, expressing the power spectral density (PSD) as the Fourier
transform (FT) of the autocorrelation function, does not hold. Actually, if the FT of
the ACF exists, a version of that theorem can be demonstrated also in the non-WSS
case, but that is not true in general.
As a consequence, when the process at hand is non-stationary (in particular when
it has a trend) the PSD estimation does not give meaningful results. Consider, for
example, a random walk process (see the example in Section 6.5). Figure 6.13 shows
that the spectral density tends to become virtually infinite at zero frequency and shows
a steep decrease to zero with increasing frequency.
A similar situation occurs with random signals having a trend. The following artifi￾cial example shows the spectral analysis of a quasi-periodic time series having a trend
(see Fig. 7.17).
## Code_7_8.R
# Signal with trend
set.seed(2)
ti <- seq(0:0.1:100)
A <- 5+rnorm(length(ti))
omega <- 2*pi*(0.05+0.01*rnorm(length(ti)))
X <- A*sin(omega*ti)+10*ti/100
X <- ts(X)
plot(X,mgp=c(2.2,1,0))
PSD <- spectrum(X,spans=5,log="no",plot=FALSE)254 Spectrum Analysis
plot(PSD$freq,PSD$spec,xlab=expression(paste(omega,"/2",pi)),
ylab=expression(paste(hat(P)[SD],"(",omega,")")),t="l",mgp=c(2.2,1,0))
time
X
0 2 0 4 0 6 0 80 100
0 5 10 15
Fig. 7.17 Time series.
Figure 7.18 ‘correctly’ shows the main frequency, which is a random value because
of the definition (see R listing). We would like to separate the clear trend from the
quasi-periodic oscillation. Here singular spectrum analysis (SSA) comes into play. We
can think of SSA as a sort of principal component analysis: it operates by decomposing
a matrix identifying the process (the trajectory matrix) by means of the singular value
decomposition (SVD) (see Huffaker et al. (2017) and Golyandina et al. (2018) for more
details about the method). The trajectory matrix M is built from the original time
series {xt;t : 1..T} by choosing a window length L and rewriting xt in a matrix format
(by column) as follows:
M =





x1 x2 . . . xN
x2 x3 . . . xN+1
.
.
.
.
.
.
.
.
.
.
.
.
xL xL+1 . . . xT





where N = T − L + 1. M is written as a ‘sequence’ of N column vectors X1 =
(x1, x2, .., xL)
0
, X2 = (x2, x3, .., xL+1)
0
, ..., XN = (xN , xN+1, .., xT )
0
, where the single
prime denotes transposition.
M = (X1X2..XN) (7.23)Singular Spectrum Analysis 255
0.0 0.1 0.2 0.3 0.4 0.5
5 1 0 1 5 2 0 2 5 30
ω/2!
PSD(ω)
^
Fig. 7.18 Smoothed spectrum.
The SVD decomposition consists of expressing M as a product:
M = UD1/2V0
between the matrices:
• U: consisting of the eigenvectors of MM0
• D1/2
: a diagonal matrix containing the square root of the eigenvalues of MM0
• V0
: consisting in the eigenvectors of M0M associated with non-zero eigenvalues
In practice the above matrices are the result of the command svd in R. An exam￾ple of a step-by-step application of the construction of the trajectory matrix and its
decomposition is reported in Huffaker et al. (2017). The explorative part of SSA is
conducted in R by the function ssa, included in the Rssa package. With reference to
the artificial data example before, we can do it with very few lines of code:
library(Rssa)
s <- ssa(X)
plot(s,numvalues = 10,main="",ylab="Eigenvalue norms")
plot(s,type="vectors",idx=1:10,main="")
Figures 7.19 and 7.20 respectively show the first ten eigenvalues and the relative
eigenvectors.
The SSA allows us to partition the trajectory matrix by grouping the Xj vectors in
(7.23) in groups of vectors corresponding to eigenvalues λj of similar value, for example
(λ1, λ2 − λ3, λ4 − λ10) with reference to Fig. 7.19. We see from Fig. 7.20 that the first
two groups (principal components...), corresponding to λ1 and (λ2, λ3), explain 71%256 Spectrum Analysis
index
eigenvalue norms
10^1.6
10^1.8
10^2.0
10^2.2
10^2.4
2 4 6 8 10
Fig. 7.19 Eigenvalues of the trajectory matrix SVD decomposition.
1 (76.1%) 2 (2.09%) 3 (1.89%) 4 (1.45%)
5 (1.41%) 6 (1.05%) 7 (0.98%) 8 (0.86%)
9 (0.8%) 10 (0.8%) Fig. 7.20 Eigenvectors of the trajectory matrix SVD decomposition.Singular Spectrum Analysis 257
of the variance. Therefore, the subsequent step (grouping plus series reconstruction)
is accomplished as follows:
r <- reconstruct(s, groups = list(1, c(2,3)))
plot(r,main="")
with the result shown in Fig. 7.21. From top to bottom: the original time series, the
first two ‘principal components’ and the residual term (bottom line).
−5 5 15
original
2 6 10
F1
−2 2 4
F2
−6 0 4
0 2 0 4 0 6 0 80 100
residuals
time
Fig. 7.21 Reconstruction phase of SSA.
The series F1 and F2 correspond to the trend and oscillatory part of the original
one. Figure 7.22 superimposes the trend on the original time series.
The oscillatory part is shown in Fig. 7.23 (left) together with its smoothed spectrum
(right).
Compare Fig. 7.23 (b) with Fig. 7.18 to understand the power of SSA: we have
obtained the trend and a more reliable estimation of the spectrum of the quasi-periodic
part of the series in one shot. The code for producing the above plots is the following:
# trend part
plot(X); lines(r$F1,col="red")
# oscillatory part
plot(r$F2,t="l")
PSD <- spectrum(r$F2,spans=5,log="no",plot=FALSE)
plot(PSD$freq,PSD$spec,xlab=expression(paste(omega,"/2",pi)),
ylab=expression(paste(hat(P)[SD],"(",omega,")")),t="l",mgp=c(2.2,1,0))258 Spectrum Analysis
time
X
0 2 0 4 0 6 0 80 100
−5 0 5 10 15
Fig. 7.22 Trend computed by means of the SSA.
time
r$F2
0 20 40 60 80 100
−3 −2 −1 0 1 2 3
0.0 0.1 0.2 0.3 0.4 0.5
0 5 10 15 20
ꞷ/2"
PS D (ꞷ)
^
Fig. 7.23 Time plot (left) and spectrum (right) of the oscillatory part of the SSA recon￾struction.
In many cases the time series can contain gaps and missing data. The analysis
performed with the Rssa package supports a solution to perform the analysis when
the original time series has missing data. Golyandina et al. (2018) presented two ap-Singular Spectrum Analysis 259
proaches, the subspace-based approach and the iterative one. For details and applica￾tion with R, see Golyandina et al. (2018).
7.4.1 Application to real data: the average temperatures in Switzerland
in one century
The dataset concerns the historical monthly temperatures in Switzerland from 1901
to 2015, downloaded from climateknowledgeportal.worldbank.org. The SSA analysis is
performed as follows:
## Code_7_9.R
# Application to real data (temperature)
# Read data
setwd("C:\RPA\code\spectrum_analysis\data")
read.csv("Temp_Switzerland.csv",h=TRUE) -> dati
X <- ts(dati$Temperature)
# First stage of SSA: Embedding and Decomposition
s <- ssa(X)
plot(s,numvalues = 10,ylab="Eigenvalue norms",main="")
plot(s,type="vectors",idx=1:10,main="")
# Second stage of SSA: Grouping and Reconstruction
r <- reconstruct(s, groups = list(1, c(2,3)))
plot(r,main="")
signal <- r$F1+r$F2
noise <- X-signal
plot(X); lines(r$F1,col="red")
PSD <- spectrum(r$F2,spans=5,log="no",plot=FALSE)
plot(PSD$freq,PSD$spec,xlab=expression(paste(omega,"/2",pi)),
ylab=expression(paste(hat(P)[SD],"(",omega,")")),t="l",mgp=c(2.2,1,0))
plot(noise)
PSD <- spectrum(noise,spans=20,log="no")
plot(PSD$freq,PSD$spec,xlab=expression(paste(omega,"/2",pi)),
ylab=expression(paste("noise ",hat(P)[SD],"(",omega,")")),t="l",mgp=c(2.2,1,0))
index
eigenvalue norms
10^2.0
10^2.5
10^3.0
10^3.5
2 4 6 8 10
Fig. 7.24 Eigenvalues.
Figures 7.24 and 7.25 show the results of the first stage of the SSA analysis.260 Spectrum Analysis
1 (46.68%) 2 (24.97%) 3 (24.91%) 4 (0.07%)
5 (0.07%) 6 (0.04%) 7 (0.03%) 8 (0.03%)
9 (0.03%) 10 (0.03%)
Fig. 7.25 Eigenvectors.
From Figs 7.24 and 7.25 we can decide how to partition the trajectory matrix.
Figure 7.26 shows the reconstruction based on the first three eigenvalues, grouped as
λ1 and (λ2, λ3).
−5 5 15
original
5.4 6.0 6.6
F1
−5 0 5
F2
−6 −2 2
0 200 400 600 800 1000 1200 1400
residuals
time
Fig. 7.26 SSA reconstruction of the original time series.
Figure 7.27 show how well the trend fits the original time series. The following
figures show the spectrum of the periodic component and, finally, time series andSingular Spectrum Analysis 261
time
X
0 200 400 600 800 1000 1400
−5 0 5 10 15
Fig. 7.27 Original time series with SSA trend.
smoothed spectrum of the residual noise.
0.0 0.1 0.2 0.3 0.4 0.5
0 1000 3000 5000
ω/2!
PSD(ω)
^
Fig. 7.28 Spectrum of the periodic component.262 Spectrum Analysis
SSA analysis showed a clear trend in the time series that was successfully identified
and removed from the original time series. The pairs of eigenvectors indicated a clear
quasi-periodicity (F2) explaining a large part of the signal. The residuals (noise) did not
display clear frequency spectra indicating either blue or red coloured noise, therefore
additional information cannot be extracted from noise.
time
noise
0 200 400 600 800 1000 1400
−6 −4 −2 0 2 4
0.0 0.1 0.2 0.3 0.4 0.5
1 2 3 4 5 6
frequency
spectrum
Series: x
Smoothed Periodogram
bandwidth = 0.00402
Fig. 7.29 Time plot (left) and spectrum (right) of the residual noise.
7.4.2 An SSA application for beer lovers
This final dataset concerns the monthly beer production in Australia from 1956 to
1995. Data come from the Australian Bureau of Statistics. Figure 7.30 shows the time
series.
months from January 1956
X
0 100 200 300 400
100 150 200
Fig. 7.30 Monthly beer production in Australia in millions of litres, from 1956 to 1995.Singular Spectrum Analysis 263
Note, in particular, that here we do not use the ‘rule of thumb’ used before for
grouping: although the first eigenvalue is higher that the second, third and fourth
(that are comparable to one another), we group as (λ1, λ2) and (λ3, λ4) because that
grouping scheme appears to give a better trend line. Moreover, looking at the eigen￾vectors (Fig. 7.32) this choice appears reasonable, as the ‘oscillating’ eigenvectors are
the third and fourth. Figure 7.33 superimposes the trend on the beer time series.
The SSA analysis is performed in two stages, as in the preceding section.
index
eigenvalue norms
10^3.0
10^3.5
10^4.0
10^4.5
2 4 6 8 10
Fig. 7.31 Eigenvalues.
## Code_7_10.R
# Application to real data (beer)
setwd("C:\RPA\code\spectrum_analysis\data")
# Read data
dati <- read.csv("monthly-beer-production-in-austr.csv",h=TRUE)
X <- ts(dati$Monthly.beer.production)
plot(X,xlab="Months from January 1956")
# First stage of SSA: Embedding and Decomposition
s <- ssa(X)
plot(s,numvalues = 10,ylab="Eigenvalue norms",main="")
plot(s,type="vectors",idx=1:10,main="")
# Second stage of SSA: Grouping and Reconstruction
r <- reconstruct(s, groups = list(c(1,2), c(3,4)))
plot(r,main="")
signal <- r$F1+r$F2
noise <- X-signal
# time series and trend
plot(X,xlab="Months from January 1956"); lines(r$F1,col="red")
PSD <- spectrum(r$F2,log="no",spans=5,plot=FALSE)
plot(PSD$freq,PSD$spec,xlab=expression(paste(omega,"/2",pi)),
ylab=expression(paste(hat(P)[SD],"(",omega,")")),t="l",
mgp=c(2.2,1,0))
abline(v=1/12,col="red")
# residual noise
plot(noise)264 Spectrum Analysis
PSD <- spectrum(noise,spans=5,log="no",plot=FALSE)
plot(PSD$freq,PSD$spec,xlab=expression(paste(omega,"/2",pi)),
ylab=expression(paste("noise ",hat(P)[SD],"(",omega,")")),
t="l",mgp=c(2.2,1,0))
1 (97.5%) 2 (0.69%) 3 (0.48%) 4 (0.47%)
5 (0.08%) 6 (0.07%) 7 (0.07%) 8 (0.05%)
9 (0.05%) 10 (0.04%)
Fig. 7.32 Eigenvectors.
original
−5 5 15 −2 2 6 10
F1
−4 0 4
F2
−6 −2 2
0 200 400 600 800 1000 1200 1400
residuals
time
Fig. 7.33 Original time series with SSA trend.
Finally, the spectrum of the periodic component is shown in Fig. 7.34, with the
dotted line denoting the annual frequency, and the remaining figures concern the resid-Exercises 265
ual noise. Note that Fig. 7.35 clearly shows that the residual is far from being white,
or also coloured, noise. Going back to the definition of ‘noise’, here it represents the
part of the signal that we are not interested - or capable - to explain.
PSD(ω)
^
0.0 0.1 0.2 0.3 0.4 0.5
0 2000 6000 10000
ω/2 
Fig. 7.34 Spectrum of the periodic component.
time
noise
0 100 200 300 400
−40 −20 0 20
0.0 0.1 0.2 0.3 0.4 0.5
0 500 1000 1500
ꞷ/2p
noise PS D (w) 
^
Fig. 7.35 Time plot (left) and spectrum (right) of the residual noise.
7.5 Exercises
Exercise 7.1 Given a zero-mean, stationary real-valued time series Xt = {xt}, t = 1..N,
the power spectral density is defined as:
PSD(ω) = 1
N





XN
k=0
xke
−iωk





Prove that:266 Spectrum Analysis
PSD =
1
N
XN
k=−N
γˆ(k)e
−iωk
where ˆγ(k) is the sample autocovariance:
γˆ(k) = 1
N
NX−k
j=0
xjxj+k
for k ≥ 0, and:
γˆ(k) = ˆγ(−k)
for k < 0.
Hint: use the notable relation:
XN
t=1
XN
s=1
f(t − s) =
NX−1
τ=−N+1
(N − |τ |)f(τ )
Exercise 7.2 The autocovariance function is defined in terms of the periodogram φ(ω), as:
γ(k) = 1
2π
Z π
−π
φ(ω)e
iωkdω
Prove that |γ(k)| ≤ γ(0) for any k > 0.
Exercise 7.3 Prove that for a causal AR(1) process Xt = φXt−1 + t, with noise variance
σ
2
:
PSD(ω) = σ
2
1 + φ2 − 2φcosω
Exercise 7.4 If Xt an MA(1) process Xt = t + θt−1, its lag-polynomial is defined by:
Θ(L) = 1 + θL
Prove that, given the noise variance σ
2
:
PSD(ω) = σ
2



Θ

e
−iω


2
that is, the power spectral density of an MA(1) process is the squared modulus of the lag￾polynomial computed at e
−iω
.
Exercise 7.5 With reference to Exercise 7.4, it can be demonstrated that the relationship
between the lag-polynomial and the power spectral density PSD holds for any MA(n) process.
(1) Justify the following assertion:
For a causal AR(1) process, PSD can be expressed as the modulus of a polynomial Ψ(e
−iω):
PSD(ω) = σ
2



Ψ

e
−iω


2Exercises 267
(2) What is the Ψ polynomial?
Exercise 7.6 The relationship of Exercise 7.4 can be further generalized for any causal
ARMA process. For such a process Φ(L)Xt = Θ(L)t, with noise variance σ
2
, we can write
the power spectral density PSD in rational form:
PSD(ω) = σ
2

Θ
￾
e
−iω

2
|Φ (e−iω)|
2
Using the above rational formula, write the explicit expression of PSD(ω) for an ARMA(1,1)
process of coefficients φ and θ.
Exercise 7.7 Using the result of Exercise 7.6 write an R code to compute the PSD(ω) of an
ARMA(1,1) process with φ = 0.5, θ = 0.5 and σ
2 = 1. Plot it for 0 ≤ ω ≤ π.
Exercise 7.8 Following Section 7.4 and the example codes developed there, perform a sin￾gular spectrum analysis on the R dataset airmiles. Write an R code and obtain a plot like
Fig. 7.26.
What can we say about the trend of that time series?
Exercise 7.9 Do the same analysis of Exercise 7.8 to the closing prices of the S&P 500 stock
index recorded between 1950 and 2019. The datafile, available on the website of the book,
can be read with the following code.
# change directory to that containing the data
setwd("C:/RPA/code/spectrum_analysis/data")
# read data
dat <- read.csv("snp500.csv")
dat.date <- as.POSIXct(dat$Date, format = "%Y-%m-%d", tz = "GMT")
dat.close <- dat$Close
# plot it
plot(dat.date,dat.close,t="l")
Exercise 7.10 The file bike_rent_daily.csv on the book website contains the daily num￾ber of bikes rent in Washington D.C. from January 1st 2011 to December 31st 2012 (Fanaee-T
and Gama, 2013). Figure 7.36 shows the result of the SSA analysis.
Write an R code to obtain that figure, and discuss the meaning of the three plots below that
of the original time series268 Spectrum Analysis
original
0 6000 3000 6000
F1
0
F2
−6000 0−2000
0 200 400 600
residuals
time
Fig. 7.36 SSA reconstruction of the bike rent data series8
Markov Chain Monte Carlo
In a Monte Carlo problem the experimenter has complete control of his sampling procedure.
If for example he wanted a green-eyed pig with curly hair and six toes and this event had
a non zero probability, then the Monte Carlo experimenter, unlike the agriculturist,
could immediately produce the animal.
Herman Kahn, Use of Different Monte Carlo Sampling Techniques
8.1 Mother Nature’s minimization algorithm
During the night of October 31 to November 1, 1952, on the Pacific Atoll Eniwetok,
the first thermonuclear reaction took place. On March 1, 1954, on the Bikini Atoll in
the Marshall Islands, the first H-bomb was detonated. Among the numerous scientists
who took part in the construction of the bomb, were Nicholas Constantine Metropolis,
Arianna Wright Rosembluth, Marshall Nicholas Rosembluth, Augusta Maria Harkanyi
Teller and Edward Teller. On March 6, 1953, the journal The Journal of Chemical
Physics received from these scientists the article ‘Equation of State Calculations by
Fast Computing Machines’, in which they put forward an algorithm, almost certainly
the most important algorithm of the last century (Metropolis et al., 1953). This algo￾rithm is now called the Metropolis algorithm, or the M(RT)2 algorithm with reference
to the initial letters of the authors’ names.
The article begins with these words: ‘The purpose of this paper is to describe a gen￾eral method, suitable for fast computing machines, of calculating the properties of any
substance which may be considered as composed of interacting individual molecules.’
By ‘fast computing machines’ they refer to MANIAC (Mathematical Analyser Nu￾merical Integrator and Computer ), as it is written in the Abstract: ‘Results for the
two-dimensional rigid-sphere system have been obtained on the Los Alamos MANIAC
and are presented here.’ The ‘general method’ is described in their words:
So the method we employ is actually a modified Monte Carlo scheme, where instead of choos￾ing configurations randomly, then weighting them with exp(−E/kT), we choose configurations
with a probability exp(−E/kT) and weight them evenly.
where T is the absolute temperature and k is Boltzmann’s constant, relating temper￾ature to energy. To realize this intent, that is to weight configurations according to
Boltzmann’s probability, we have to rely on Mother Nature, to encode the minimiza￾tion algorithm used by nature itself. A system may (albeit, with small probability)
exit from a local energy minimum in favour of reaching a better one.
Suppose the system is in the state xi
, with energy Ei
. It may try to reach the state
xj having energy Ej . If Ej 6 Ei
, the state xj is certainly the new state of the system,
while if Ej > Ei
, the system will be in the state xj with probability exp(−∆E/kT),270 Markov Chain Monte Carlo
where ∆E = Ej −Ei
. Encoding such a principle, our scientists achieve a Markov chain,
which they prove to be irreducible and ergodic. As the length of the sequence tends to
infinity, that is at stationarity, it visits the most probable states, the states having the
lowest energy. The time average on those states is an estimate of the expected value of
the process, in other words, one estimates an integral by an arithmetic mean computed
only on the states whose contribution to the integrand is high, or ‘important’. Let us
see in more detail how the algorithm works.
1. Choose a starting microstate.
2. Choose the ‘move’. The move provides the change of the system, that is it deter￾mines how to go from state xi with energy Ei to state xj with energy Ej .
3. Compute the difference between the energies ∆E = Ej − Ei
.
4. If ∆E 6 0, state xj is accepted and go to item 8. Indeed, in this case, the move
led the system to a state with lower energy, which means an energy gain.
5. If ∆E > 0, compute the transition probability:
pij = exp(−∆E/kT).
6. Generate a random number u uniformly distributed in [0, 1].
7. If u 6 pij , the state xj is accepted, otherwise the system remains still in state xi
.
This means that the system goes in xj with probability pij . If u > pij the state
xi must be considered anyhow as a ‘new’ state, that is xj ≡ xi
.
8. Compute the interested physical quantity A(xj ).
9. Repeat steps 1 to 8, in order to construct a chain of states
x1, x2, . . . , x, . . . , xM
from which the values below are derived:
A(x1), A(x2), . . . , A(xm), . . . , A(xM).
10. After a suitable ‘burn-in’, time necessary to reach the invariant distribution, com￾pute the time average on Am, m = 1, . . . , M .
It is apparent that the random numbers are employed to choose between different
alternatives, which means that the procedure is based on the Monte Carlo method. It
is likewise apparent that the procedure is devised to construct a Markov chain:
Monte Carlo + Markov Chain = MCMC
Some comments are in order. Curiously, Metropolis et al., the scientists who conceived
and realized the algorithm, did not actually apply this ‘game of chance’ – as they
called it – in their experiments on two-dimensional rigid-sphere system, since, in that
case, the difference Ej − Ei was either zero or infinity.From physical birth to statistical development 271
In the present book, the reader has already encountered, and will encounter in
the following, the term ‘Monte Carlo’. In a broad sense, this method is to try the
solution of a problem, representing it as a parameter of a hypothetical population and
estimate such a parameter through a sample of the population obtained by random
number sequences. For instance, suppose I is a definite integral that we cannot compute
analytically, but we can regard it as the expected value of a random variable X. In
this case, the Monte Carlo method consists of estimating I by generating N value of
X with the use of random numbers. The mean of this obtained sample is an estimate
of I. Note that with the Monte Carlo method we ‘estimate I’, not numerically or
approximately compute I, but we make inference from a sample to a population to
estimate the parameter of interest.
Traditionally, the solution of a ‘probabilistic’ problem may be found through differ￾ential or integro-differential equations, with the Monte Carlo method; ‘deterministic’
problems are faced by finding a probabilistic analogue and acquiring an estimated
solution through sampling procedures.
An example of Monte Carlo estimation of a numerical constant is the well-known
‘Buffon’s needle’ problem, discussed in Chapter 3. Some consider Buffon’s needle the
first example of a Monte Carlo application, needing neither computer codes nor pencil
and paper calculations. Georges Louis LeClerc, Comte de Buffon (1707−1788) used
repeated needle tosses onto a lined background to estimate π.
The Monte Carlo method, as we know it today, was first applied during the Man￾hattan Project to construct atomic weapons, in particular to solve problems related to
neutron travelling through fissile material. Those scientists, knowing the probabilities
of the microscopic events occurring to a neutron (absorption, fission, etc.) devised a
stochastic process following the ‘story’ (trajectory) of a single neutron. Constructing
many neutron trajectories different from one another, that is a sample of trajecto￾ries, they were able to estimate the quantities of interest: how many neutrons were
absorbed, how many neutrons were generated, etc.
The name ‘Monte Carlo’ is attributed to Metropolis, with a clear reference to the
Monte Carlo Casino. Metropolis himself wrote (Metropolis, 1987):
It was at that time [in 1947] that I suggested an obvious name for the statistical method –
a suggestion not unrelated to the fact that Stan [Stanislaw Ulam] had an uncle who would
borrow money from relatives because he ‘just had to go to Monte Carlo.’ The name seems to
have endured.
It is evocative that with the name ‘Monte Carlo’, the concept of probability, that is at
foundations of the method, returns to gaming houses, which is where it was born.
8.2 From physical birth to statistical development
The history of science teaches us that great ideas go far beyond the immediate intents
of those who had them. Now the applications of Monte Carlo methods cover a variety
of areas, besides physics, ranging from medicine to economics, from archaeology to
genetics, etc. In 1970, Keith Hastings, more in statistical than in physical style, wrote
the paper ‘Monte Carlo Sampling Methods Using Markov Chains and Their Applica￾tions’, in which ‘. . . potential applications of the [Monte Carlo] methods in numerical
problems arising in statistics, are discussed’ (Hastings, 1970).272 Markov Chain Monte Carlo
The essential idea of Markov Chain Monte Carlo (MCMC) is to generate realiza￾tions of a random variable X from an irreducible Markov chain converging to (i.e.
having as its stationary distribution) the desired distribution of X. It is important to
keep in mind that MCMC generated values are not independent, as in the ‘ordinary’
Monte Carlo method, rather they are often strongly correlated.
Let us enter a Bayesian environment, anticipating concepts that will be treated in
the following chapter. In the Bayesian framework, inference is performed by calculating
the posterior probability density function. For continuous distributions, we have for
the final distribution:
p(θ|D) = p(θ) p(D|θ)
Z
p(θ) p(D|θ) dθ
where D denotes the observed data, θ model parameters and missing data, p(θ) the
prior distribution, and p(D|θ) the likelihood. The prior distribution expresses our
knowledge (or uncertainty) about θ before seeing the data. The posterior distribution
expresses our knowledge about θ after seeing the data. When the posterior expectation
of a function of θ is the quantity of interest, we have
E [f(θ)|D] = Z
f(θ) p(θ|D) dθ
The basic idea is to draw samples from the posterior distribution and use those samples
to estimate the properties of the posterior distribution (mean, variance, quantiles, etc.).
For this aim we have to construct a Markov chain such that its equilibrium probability
distribution is the required posterior density. The chain has to be stationary and
ergodic, that is the stationarity can be reached, whatever the starting point at which
the chain begins. This implies that the chain is irreducible and aperiodic.
As least in principle, with this method one is able to generate observations from
any distribution. As already said above by Metropolis et al. (1953), MCMC was in￾troduced as ‘. . . a general method, suitable for fast computing machines, of calculating
the properties of any substance which may be considered as composed of interacting
individual molecules’. Now this method has become ‘a miraculous tool of Bayesian
analysis’ (Geyer, 1996) and the flag of what has been called, with a touch of irony,
the ‘model liberation movement’ (Smith, 1992). On these topics, see the fine review
by Robert and Casella (2011), which includes a rich list of references.
The Comprehensive R Archive Network (CRAN), the public clearing house for R
packages, contains a considerable number of packages for the application of MCMC to
various problems. Packages are: MCMCpack (Martin et al., 2018), MCMCglmm (Hadfield,
2019), mcmc (Geyer, 2019). Here we will present a very simple code, showing step
by step how to construct a Markov chain such that its invariant distribution is the
standard normal.
In statistics, the terminology is a little different from physics. Dealing with MCMC,
we talk about the ‘Metropolis-Hastings (or Hastings-Metropolis) algorithm’, which is
more general than the original one. The basic formula is:
α(x, y) = min 
1 +
π(y)q(y, x)
π(x)q(x, y)

(8.1)From physical birth to statistical development 273
where:
• y is the candidate state. That is, if at time t the system (process) is in the state
Xt = x, y is the value assumed by Xt+1, which may be accepted or not. The value
y is generated by the density q(x, y).
• q(x, y) is the proposal distribution, since it is the distribution that proposes the
candidate. The proposal distribution is also written as q(·|Xt).
• π(·) is the invariant distribution, called the target density, that is the distribution
of interest, from which a certain number of realizations are generated and their
time average is estimated.
• α(x, y) is the probability of move (or probability of acceptance), since if α(x, y) 6
u, then Xt+1 = y, otherwise Xt+1 = x. As usual, u is a random number, uniformly
distributed in [0, 1].
With this symbology, the transition probability of the chain p(x, y) is:
p(x, y) = q(x, y) α(x, y), x 6= y (8.2)
however, it is not complete at all, since it does not consider when the transition to y
does not occur, that is when Xt+1 = x. Let r(x) be the probability that the system
remains in x, that is:
r(x) = P {Xt+1 = x|Xt = x}
we have:
p(x, y) = q(x, y) α(x, y) + r(x)1x(y)
where 1x(y) is equal to 0 or 1 depending on whether x 6= y or x = y.
Note that the distribution of interest appears in the definition of p(x, y) (8.2)
through α(x, y), that is as the target density ratio π(y)/π(x). This means that it is
sufficient to know π(x) up to a constant. In Bayesian statistics, we do not need to actu￾ally compute the normalization constant, and write simply the posterior distribution
as the product between the prior distribution and the likelihood.
A possible choice of the proposal density is when it is symmetric q(x, y) = q(y, x),
as in the original Metropolis algorithm. More specifically it is:
q(x, y) = q(|x − y|)
q(y, x) = q(|y − x|)
This choice is equivalent to a random walk, since the new value y equals the old x
+ ‘something’, for instance a ‘noise’. So we can write y = x + z, with z a random
perturbation independently realized from x.
A further choice is q(x, y) = q(y), that is the candidate y is independent from x. In
other words, the value assumed by Xt+1 does not depend on the current state Xt = x,
that is to say that the candidates do not depend on the time instant at which the
process is. Note, however, that the realizations, also in this case, are not independent,
since the probability of accepting y depends on x. In this case, eqn (8.1) becomes:
α(x, y) = min 
1 +
π(y)q(x)
π(x)q(y)

= min 
1 +
w(y)
w(x)

with w(x) = π(x)/q(x).274 Markov Chain Monte Carlo
There are several choices for q(x, y). It is desirable that:
1. q(x, y) generates a chain rapidly converging to the invariant distribution.
2. q(x, y) generates a chain that, after the burn-in period, has a ‘good mixing’. The
property of mixing refers to how fast the states of the chain acquire new values.
Poor mixing means that the candidate state Xt+1 is still
= Xt = Xt−1 = . . .
3. q(x, y) generates the candidates in an easy way.
Let us now see how to construct a Markov chain having the standard normal
N (0, 1) as invariant distribution. Of course, in R we can generate n realizations from
a standard normal using the instruction rnorm(n). We employ here the Metropolis￾Hastings algorithm, following it step by step. The target density π(x) is:
π(x) = 1
√
2π
exp(−x
2
/2)
As proposal distribution q(x, y) we choose:
q(x, y) = q(y, x) = q(|x − y|)
this means that the candidate y derives from x through y = x + z, where z is a
realization of the random variable Z with a certain distribution. We choose for Z the
uniform distribution in the interval [−δ, +δ] with δ = 0.75. Realizations of Z are then
given by:
z = −0.75 + [0.75 − (−0.75)] u = −0.75 + 1.5u
where u is a uniform random number in (0, 1). In R the instruction is simply:
z<-runif(1,-0.75,0.75).
In the code below only five steps are considered to show in detail how the code
works.
## Code_8_1.R
## M-H algorithm
## target density : Standard Normal
## only 5 steps
mh <- function(nsteps,x0,d,pi){
x<- numeric()
x[1]<- x0 # initial state of the chain. Note: i=1 corresponds to the time t=0
for(i in 2:nsteps) {
# extraction of the candidate y as possible state at the generic time i
z<- runif(1,min=-d,max=d)
print(i) # some printed values to follow the progress of the chain
print(x[i-1])
print(z)
y<-x[i-1]+z # actually this y is y[i-1]
print(y)
# will y be the new x[i]?
alpha<- min(pi(y)/pi(x[i-1]),1) # alpha: probability of move
u<- runif(1) # uniform random number in (0, 1) to check whether u <= alphaFrom physical birth to statistical development 275
print(u)
print(pi(y))
print(pi(x[i-1]))
print(alpha)
if(u <= alpha) x[i]<- y else x[i]<- x[i-1]
} # ending loop on the iterations
return(x)
} # end function
# target density : standard normal
mu<-0
sigma<-1
target<-function(x){
dnorm(x,mean=mu,sd=sigma)
}
set.seed(2)
nsteps<- 5
x0<- -10
delta<- 0.75
x<- mh(nsteps,x0,delta,target)
As we said, in principle any choice of the initial value x(1) is correct, since at sta￾tionarity the process does non depend on the starting state. In practice, however, it
is advisable to run the code with different initial values (better: over-dispersed initial
values), and to compare the plots by eye. Such a somewhat na¨ıve method is also used
to assess convergence (see next section).
We enumerate the states starting from i = 1 and set x(1) = −10, i.e. the random
variable X at t = 0, that is X1, takes the value −10. Now we need z. Let for instance
z = −0.4727 (we limit ourselves to writing four decimal significant digits), then y(1) =
−10 + (−0.4727) = −10.4727. We say that y(1) is the candidate at the iteration 1
(t = 0) proposed by q(x, y). Will y(1) be the new x(2)? That is, will it be x(2) = y(1)?
Such a decision is up to the probability of move α(x, y). We have to compute:
α[x(1), y(1)] = π[y(1)]
π[x(1)]
This is:
π[y(1)] = 1
√
2π
exp[−(−10.47272
)/2]= 6.0934 × 10−25
π[x(1)] = 1
√
2π
exp[−(−102
)/2] = 7.6946 × 10−23
then α[x(1), y(1)] = 0.007919. At this point a further random number u is required,
for instance u = 0.7024. Is u < α[x(1), y(1)] = 0.007919? No. So the process remains
in the old state x(1), that is x(2) = x(1) = −10. The value −10 has to be regarded as
the realization of the random variable X2.
The code goes ahead with a further value of Z and, for instance, let z(2) = 0.1100,
from which y(2) = −9.8900. Repeating the above calculations results in:
α[x(2), y(2)] = exp[−(−9.89002
)/2]
exp[−(−102)/2] = 2.9857276 Markov Chain Monte Carlo
The new random number u will be in any case < 2.9857. Therefore, the new state at
i = 2 is surely accepted, that is x(2) = y(1) = −9.8900. The process continues up to
the fixed number of iterations.
The next code is almost the same as Code_8_1.R, but without the printed values,
and δ is varied to show the effect of the q(x, y) on mixing. For completeness the whole
code is reported.
## Code_8_2.R
## M-H algorithm
## target density : Standard Normal
## varying delta
mh <- function(nsteps,x0,d,pi){
x<- numeric()
x[1]<- x0 # initial state of the chain. Note: i=1 corresponds to the time t=0
for(i in 2:nsteps) {
# extraction of the candidate y as possible state at the generic time i
z<- runif(1,min=-d,max=d)
y<-x[i-1]+z # actually this y is y[i-1]
# will y be the new x[i]?
alpha<- min(pi(y)/pi(x[i-1]),1) # alpha: probability of move
u<- runif(1) # uniform random number in (0, 1) to check whether u <= alpha
if(u <= alpha) x[i]<- y else x[i]<- x[i-1]
} # ending loop on the iterations
return(x)
} # end function
# target density : standard normal
mu<-0
sigma<-1
target<-function(x){
dnorm(x,mean=mu,sd=sigma)
}
set.seed(2)
nsteps<- 1000
x0<- -10
delta<- 0.75
x<- mh(nsteps,x0,delta,target)
par(mai=c(1.02,1.,0.82,0.42)+0.1) # to control the margin size
plot(x[1:length(x)],type="l",main=paste("delta =",delta),
ylim=c(-10,5),xlab="iterations",ylab="x(i)",cex.lab=1.5,cex.main=1.1)
abline(h=0,lty=3,col="black",lwd=2)
## 2 chains with different initial states
nsteps<- 1000
delta<- 0.75
x0<- -10
x<- mh(nsteps,x0,delta,target)
plot(x[1:length(x)],type="l",main=paste("delta =",delta),
ylim=c(-10,+10),xlab="iterations",ylab="x(i)",cex.lab=1.7)
abline(h=0,lty=3,col="black",lwd=2)
x0<- +10
x<- mh(nsteps,x0,delta,target)
lines(x[1:length(x)],type="l",lty=2,col="black")
## delta=0.1
set.seed(2) # the seed is the same as at the beginning
nsteps<- 1000
delta<- 0.1
x0<- -10From physical birth to statistical development 277
x<- mh(nsteps,x0,delta,target)
plot(x[1:length(x)],type="l",main=paste("delta =",delta),
ylim=c(-10,5),xlab="iterations",ylab="x(i)",cex.lab=1.7)
abline(h=0,lty=3,col="black",lwd=2)
## delta=30
set.seed(2) # the seed is the same as at the beginning
nsteps<- 1000
delta<- 30
x0<- -10
x<- mh(nsteps,x0,delta,target)
plot(x[1:length(x)],type="l",main=paste("delta =",delta),
ylim=c(-10,5),xlab="iterations",ylab="x(i)",cex.lab=1.7)
abline(h=0,lty=3,col="black",lwd=2)
In Fig. 8.1, it appears that the chain converges after about 200 steps and that there
is a fast mixing around the line x(i) = 0. To sum up: we have generated 1000 values
x(i), which after i & 200 are to be regarded as realizations of N (0, 1).
0 200 400 600 800 1000
−10 −5 0 5
delta = 0.75
iterations
x(i)
Fig. 8.1 Realizations of the Metropolis-Hastings algorithm with target density the standard
normal N (0, 1), and δ = 0.75.
Figure 8.2, with the time series of the above figure, shows also the series with
x(0) = +10, and the same convergence is apparent. Figures 8.3 and 8.4 show the effect
of the choice of q(x, y), with the same x(0) = −10. In Fig. 8.3, Z varies in the interval
[−0.1, 0.1], and it appears that after 1000 iterations, stationarity is not still reached,
since variations induced in each state are too ‘small’ to produce substantial changes
in the states within a reasonable period of time.
The opposite effect is shown in Fig. 8.4, in which Z varies in [−30, 30]. Even though
stationarity is reached almost immediately, such a choice of δ is not a good choice,
since the chain remains over or under the line x(i) = 0 for relative long period of time;
in other word the mixing is very poor.278 Markov Chain Monte Carlo
0 200 400 600 800 1000
−10 −5 0 5 10
delta = 0.75
iterations
x(i)
Fig. 8.2 As Figure 8.1, but with two different initial states
0 200 400 600 800 1000
−10 −5 0 5
delta = 0.1
iterations
x(i)
Fig. 8.3 As Figure 8.1, but δ = 0.1
The estimate of the expected value E[X] of the random variable X ∼ N (0, 1) is
given by:
x¯ =
1
n
Xn
i=1
xi
which is a time average, where x1 is the first value after the chain has converged.
In Code_8_2.R, we increase the number of iterations nsteps<- 11000 and intro￾duce a burn-in period burn.in<- 1000. We add also the following lines to the section
of Code_8_2.R relative to δ = 0.75:From physical birth to statistical development 279
0 200 400 600 800 1000
−10 −5 0 5
delta = 30
iterations
x(i)
Fig. 8.4 As Figure 8.1, but δ = 30
## CONTINUE Code_8_2.R
set.seed(11)
nsteps<- 11000
burn.in<- 1000
delta<- 0.75
x0<- -10
x<- mh(nsteps,x0,delta,target)
plot(x[1:length(x)],type="l",main=paste("delta =",delta),
ylim=c(-10,5),xlab="iterations",ylab="x(i)",
cex.lab=1.2,cex.main=1.1,
font.lab=3,lty=1,lwd=1.5)
abline(v=burn.in,lty=3,col="black",lwd=2)
ta<- burn.in
tb<- length(x)
lt<- length(x[ta:tb])
temp.av<- mean(x[ta:(tb-1)] )
temp.av
se.temp<- sqrt(var(x[ta:tb])/lt)
se.temp
# correlation
par(col="black",lwd=2,font.lab=3)
acf_x<-acf(x[ta:(tb-1)],lag.max=60)
#acf_x # comment if you do not wish the acf values printed
Figure 8.5 reports the same time series of Fig. 8.1, but with 11000 iterations.
The code gives ¯x = 0.059 and the standard error of the mean = 0.00979, but this
value makes no sense, since the realizations are strongly correlated as shown in Fig. 8.6.
The ACF already encountered in Chapters 6 and 7 measures the linear correlation
at different time lags. The R function acf computes the ACF, and also automatically
adds the horizontal dashed lines, corresponding to the 95% confidence intervals as￾suming a white noise input. This routine requires as input the value of the series x280 Markov Chain Monte Carlo
0 2000 4000 6000 8000 10000
−10 −5 0 5
delta = 0.75
iterations
x(i)
Fig. 8.5 As Fig. 8.1, but with 11000 iterations. The burn-in is located at the iteration =
1000.
0 1 0 2 0 3 0 4 0 5 0 60
0.0 0.2 0.4 0.6 0.8 1.0
lag
ACF
Series x[ta:(tb − 1)]
Fig. 8.6 Autocorrelation plot for the realizations of the Metropolis-Hastings algorithm with
target density the standard normal N (0, 1), δ = 0.75.
after the burn-in and a maximum delay, which we set at lag.max=60. If requested, the
output acf.x can report numerical ACF values. Recall that the lag 0 autocorrelation
is fixed at 1 by convention.
To end this section, we show as a further instance how to generate vectors: x1, x2, . . . , xn.
Let us consider vectors with two components, obtained by the bivariate normal distri￾bution:
N2(µ, Σ)From physical birth to statistical development 281
where the mean vector µ is:
µ =

1
2

and the covariance matrix Σ is:
Σ = 
1.0 0.9
0.9 1.0

This distribution is ‘cigar-shaped’, because of very high correlation. Recall that if the
random variables X1 and X2 have bivariate normal distributions, the joint density
function is given by:
f(x1, x2) = 1
2πσ1σ2
p
1 − ρ
2
×
× exp (
−
1
2(1 − ρ
2)
"
x1 − µ1
σ1
2
−
2ρ(x1 − µ1)(x2 − µ2)
σ1σ2
+

x2 − µ2
σ2
2
#)
(8.3)
where −∞ < µ1, µ2 < +∞ and σ
2
1
, σ2
2 > 0 are the expected values and the variances
of X1 and X2, respectively, while −1 6 ρ 6 +1 is the correlation coefficient:
ρ =
Cov [X1, X2]
σ1σ2
Turning to the example in which: σ1 = σ2 =
√
1 = 1 and ρ = 0.9/(σ1σ2) = 0.9,
the MCMC code to work on the bivariate normal distribution is reported below.
## Code_8_3.R
## M-H algorithm
## target density : Bivariate Normal
mh <- function(nsteps,x01,x02,d1,d2,g2){
x1<- numeric()
x1[1]<- x01 # initial state of the chain. Note: i=1 corresponds to the time t=0
x2<- numeric()
x2[1]<- x02 # initial state of the chain. Note: i=1 corresponds to the time t=0
for(i in 2:nsteps) { # starting loop on the iterations
# extraction of the candidates y1 and y2
# as possible state at the generic time i
z1<- runif(1,min=-d1,max=d1)
z2<- runif(1,min=-d2,max=d2)
y1<- x1[i-1]+z1
y2<- x2[i-1]+z2
g2num<- g2(y1,y2,mu1,mu2,sig1,sig2,ro)
g2den<- g2(x1[i-1],x2[i-1],mu1,mu2,sig1,sig2,ro)
alpha<- min(g2num/g2den,1) #alpha: probability of move
u<- runif(1) # uniform random number in (0, 1) to check whether u <= alpha
if(u <= alpha) {
x1[i]<- y1
x2[i]<- y2 }
else {
x1[i]<- x1[i-1]
x2[i]<- x2[i-1]282 Markov Chain Monte Carlo
}
} # ending loop on the iterations
return(list(x1,x2))
} # end function
# target density:
mu1<- 1
mu2<- 2
ssd11<- 1
ssd22<- 1
ssd12<- 0.9
ssd21<- 0.9
sig1<- sqrt(ssd11)
sig2<- sqrt(ssd22)
ro<- ssd12/(sig1*sig2)
g2<-function(x1,x2,mu1,mu2,sig1,sig2,ro){
exp(-(1./(2.*(1.-ro^2))) *
(((x1-mu1)/sig1)^2 - (2.*ro*(x1-mu1)*(x2-mu2))/(sig1*sig2) + ((x2-mu2)/sig2)^2))
}
set.seed(2)
nsteps<- 10000
delta1<- 0.75
delta2<- 0.75
x01<- -4
x02<- +4
xx<- mh(nsteps,x01,x02,delta1,delta2,g2)
xx1<-as.numeric(unlist(xx[1]))
xx2<-as.numeric(unlist(xx[2]))
nplot<- 500 # to plot only nplot iterations
# xx1[1:nplot] # uncomment to see x1(i) and x2(i)
# xx2[1:nplot]
par(mai=c(1.02,1.,0.82,0.42)+0.1) # to control the margin size
plot(xx1[1:nplot],type="l",font.lab=3,
ylim=c(-5,5),xlab="iterations",ylab="x1(i) and x2(i)",
lwd=2,cex.lab=1.2)
lines(xx2[1:nplot],type="l",lty=2,col="black",lwd=2)
abline(h=mu1,lty=3,col="black",lwd=2)
abline(h=mu2,lty=3,col="black",lwd=2)
# to plot a different representation
plot(xx1[1:nplot],xx2[1:nplot],type="l",font.lab=3,
ylim=c(-2,5),xlab="x1(i)",ylab="x2(i)",
lwd=2,cex.lab=1.2)
# final check if desired:
burn.in<- 2000
x1m<- mean(xx1[burn.in:nsteps])
x1m
x2m<- mean(xx2[burn.in:nsteps])
x2m
The code is an extension of Code_8_2.R, where now the realizations x1, x2, . . . , xn
have to be generated. The candidate vector y is still chosen through:
y = x + z
that is:
y1 = x1 + z1
y2 = x2 + z2From physical birth to statistical development 283
where z1 and z2 are the realizations of the uniform random variables Z1 and Z2,
respectively, with Z1 and Z1 defined in [−0.75, +0.75]. The initial values are x1(0) = −4
and x2(0) = +4. Let it be, for instance, that z1 = −0.4727 and z2 = 0.3036, so
y1 = −4.4727 and y2 = 4.3036, and α[x(0)), y(0)] = 3.0700 × 10−13, computed by
eqn (8.3). A random number u decides the change of state of x(0). A random number
could be u = 0.5733, which is not less than α[x(0)), y(0)], therefore x(1) is still equal
to x(0).
The statement return(list(x1,x2)) of the function mh returns a list of multiple
values, and the command unlist, repeated for xx[1] and xx[2], converts the list into
a single vector.
Figure 8.7 reports the values of x1 and x2 for the first 500 iterations (nplot). A
0 100 200 300 400 500
−4 −2 0 2 4
iterations
x1(i) and x2(i)
Fig. 8.7 Realizations of the Metropolis-Hastings algorithm with target density the bivariate
normal distribution. The horizontal dotted lines mark the components µ1 = 1 and µ2 = 2 of
the mean vector µ.
different representation is shown in Figure 8.8, plotting x1 vs x2. The last code lines
are aimed to check if the computed time averages are approximately equal to µ1 = 1
and µ2 = 2, and indeed x1m and x2m result equal to 1.0155 and 1.99458, respectively,
with 10000 iterations (nsteps) and a generous burn-in (burn.in) = 2000.284 Markov Chain Monte Carlo
−4 −2 0 2
−2 −1 0 1 2 3 4 5
x1(i)
x2(i)
Fig. 8.8 As in in Fig. 8.7, but with a different representation.
8.3 The travelling salesman problem
In 1983, (Kirkpatrick et al., 1983) proposed a computational method, that they called
simulated annealing, to address the travelling salesman problem (TSP). The same
method was proposed independently by Cern`y (1985 ˇ ). The name ‘annealing’ was in￾spired by the metallurgical process consisting of heating a solid material and allowing
it to cool down very slowly, for the purpose of bringing the material closer to its
equilibrium state. In this way, structural imperfections can be removed.
Suppose we have n cities in a certain geographical region, for instance in a whole
nation. A salesman has to travel through all the cities and return to his original one.
The problem is to find the shortest road route for his journey, taking into account that
no city has to be visited twice. This is an NP-hard (non-deterministic polynomial￾time hard) problem. A problem is NP-hard if it is as hard as or more than the hardest
problem in NP, where in turn solutions of NP problems can be verified in polynomial
time. For the salesman problem, we cannot be sure to find the shortest route within
a reasonable computer time. If n are the cities including his own one, the salesman
can choice the second city in (n − 1) ways, the third one in (n − 2) ways, and so on.
Multiplying all these ways together gives:
(n − 1) · (n − 2) · (n − 3) · · · · · 3 · 2 · 1 = (n − 1)!
therefore there are (n − 1)! possible routes, for instance, if n = 10, there are 362, 880.
But if n is 20, the number of routes is ≈ 2
18, using the Stirling approximation:
n! ≈
√
2πn (n/e)
n
, n → ∞
The problem is ‘translated’ into the Metropolis algorithm, discussed in the previ￾ous sections. In fact, there is an ‘energy’, there is a ‘move’, there is an ‘acceptance
probability’, and there is a ‘temperature’. But the ‘energy’ is a length, the length of
the salesman’s journey, the ‘move’ is how to change the order in which the cities areThe travelling salesman problem 285
visited, the ‘acceptance probability’ is the probability of accepting a new route, the
‘temperature’ is also a length, measured, for instance, in kilometres, not of course with
kelvin degrees. To explain, if the energy is the difference between the lengths associ￾ated with two consecutive ‘moves’, the initial temperature of the system is a value
large compared to the largest energy we expect to encounter from move to move. We
suggest (Applegate et al., 2006), for the applications and theory of TSP and also for
the history of the problem.
Code_8_4.R searches for a solution of the TSP with twenty cities, the main Italian
provinces. Distances between cities, in kilometres, are read from a file (available on the
book website). Figure 8.9 shows the distance table. The first city, where the salesman
starts his journey, is L’Aquila, the wonderful town severely damaged by a terrible
earthquake in 2009.
AQ
AQ
0
412
629
629 244
159
403
658
958
573
954 368
612
227
376
673 
1258
873
303
873 303
277 376 673
573
394 690 137 546 606 189 190 725 424 710 940 319 620 173 790 542
244
394
690
137
546
606
189
190
725
424
710
940
319
620
173
790
542
412
0
0
320
403
958
1258
612
1085
1155
774
480
1274
354
844
381
858
1196
756
1339
700
770
418
155
889
261
806
714
473
784
370
954
293
213
227
528
332
670
763
1272
121
227
254
397
548
413
522
823
551
965
1060
1569
418
360
551
592
501
571
304
229
690
431
595
923
274
585
171
755
144
523
703
171
905
760
1396
245
350
378
248
443
744
144
886
885
1470
319
224
452
185
324
558
466
808
1126
280
453
131
623
853
224
809
793
476
754
373
924
1001
930
1586
434
357
567
114
1009
664
676
893
561
1063
815
465
677
971
686
1006
912
1170
1481
1067
1651
1421
328
151
498
269
464
464
400
400
214
214
0
0
0
0
841
1085
700
293
548 
501
0
871
1155
770
213
413 
571
144
454
774
418
227
522
304 
523
0 443
0
199
480
155
528
823
229 
703
744
324
0
989
1274
889
332
551
690 
171
144
558
859
0
166
354
261
670
965
431 
905
885
466
224
1001
0
948
844
806
763
1060
595 
760
885
808
809
930
1009
0
631
381
714
1272
1569
923 
1396
1470
1126
793
1586
664
465
1170
614
858
473
121
418
274 
245
319
280
476
434
676
677
0
0
884
1169
784
227
360
585 
350
224
453
754
357
893
971
1481
328
512
756
370
254
551
171 
378
452
131
373
567
561
686
1067
151
1054
1339
954
397
592
755 
248
185
623
924
114
1063
1006
1651
498
633
633
446
446
403
403
806
1110
725
155
161
526
396
270
375
676
402
815
912
1421
269
0
0
0
0
320
159
658
954
368
841
871
454
199
989
166
948
631
614
884
512
1054
806 1110 725 155 161 526 396 270 375 675 402
PZ
PZ
CZ
NAP
BO
TS
RM
GE
MI
AN
CB
TO
BA
CA
PA
FI
TN
PG
AO
VE
CZ NAP BO TS RM GE MI AN CB TO BA CA PA FI TN PG AO VE
Fig. 8.9 Distances between the main 20 provinces in Italy.
The initial visit order is chosen at random, for example by adopting the random
city sequence corresponding to the city order of the table shown in Fig. 8.9.
# Code_8_4.R
# Read the province coordinates and mutual distances
setwd("C:/RPA/code/markov_chain_montecarlo")
city.coords <- read.table("data/provinces.txt",h=T)
cx <- as.matrix(city.coords[,3:4])
labs <- city.coords$Code
distances <- read.table("data/distances.txt",h=TRUE)
M_Cities <- data.matrix(distances)
# distance function
dista <- function(x) {
ret <- M_Cities[x[1],x[2]]
for (i in 2:20)
ret <- ret + M_Cities[x[i],x[i+1]]
as.numeric(ret)}
# Simulated Annealing (SA) code
r<-nrow(M_Cities) # number of raws of M_Cities
# route starting from L'Aquila
s<-1:r
route <- c(s,1)286 Markov Chain Monte Carlo
# route coming from SA
best.route <- route
# route corresponding to best minimum
very.best.route <- route
# Initial route length
L0 <- dista(route)
# Other parameters
# initial "temperature"
T0<-1000
# number of cooling steps
nT<-100
# number of tentative routes, for every T value
nL<-20
# slow-cooling parameter
a<-0.9
# route vector
L<-vector()
sL<-vector()
# best routes found during cooling
minL<-vector()
# Comment the following for an actual SA algorithm
set.seed(4321)
for (t in 1:nT)
{ # starting cooling loop
T<-a^t*T0
L[1]<-L0
r_sample<-sample(s[2:20],replace = FALSE)
sL<-r_sample
route<- c(s[1],r_sample,s[1])
for (l in 2:nL)
{ # starting tour length loop
route0<- route[2:20]
L[l] <- dista(route)
if(L[l] > L[l-1]) # worse tour
{
# energy
DL <- L[l] - L[l-1]
# transition probability
P_tr <- exp(-DL/T)
u<- runif(1)
if(u <= P_tr)
{
# Metropolis: accepted
# generate new solution
r_sample<- sample(sL,replace = FALSE)
route<- c(s[1],r_sample,s[1])
}
else
{
# Metropolis: not accepted
L[l] <- L[l-1]
# generate new solution
r_sample<- sample(route0,replace = FALSE)
route<- c(s[1],r_sample,s[1])
}
} # ending worse tour
else
{
# better tourThe travelling salesman problem 287
best.route <- route
if (dista(best.route)<dista(very.best.route)) very.best.route <- best.route
# generate new solution
r_sample<- sample(sL,replace = FALSE) # better tour
route<- c(s[1],r_sample,s[1])
if (dista(route)<dista(very.best.route)) very.best.route <- route
}
} # ending tour length loop
minL[t]<- min(L)
} # ending cooling loop
# Graphically show the route
# swap Long ang Lat
tmp <- cx
coords <- cx
coords[,1] <- tmp[,2]
coords[,2] <- tmp[,1]
xmin <- 38
xmax <- 46.5
ymin <- 7
ymax <- 17
# plot TSP
plot(seq(ymin,ymax,(ymax-ymin)/10),seq(xmin,xmax,(xmax-xmin)/10),
type="n",xlab="Longitude",ylab="Latitude")
text(coords[s[1],2],coords[s[1],1],labels=labs[s[1]],col="black",cex=0.8)
text(coords[s[1],2],coords[s[1],1],labels="O",col="black",cex=2.5)
for (i in (2:20))
text(coords[s[i],2],coords[s[i],1],labels=labs[s[i]],col="red",cex=0.8)
# best route found by simulated annealing
for (i in 1:19)
arrows(coords[very.best.route[s[i]],2],coords[very.best.route[s[i]],1],
coords[very.best.route[s[i+1]],2],
coords[very.best.route[s[i+1]],1],col="blue",angle=20,length=0.1)
arrows(coords[very.best.route[s[20]],2],coords[very.best.route[s[20]],1],
coords[very.best.route[s[1]],2],
coords[very.best.route[s[1]],1],col="blue",angle=20,length=0.1)
# Show solution and route length
print(very.best.route)
print(dista(very.best.route))
Figure 8.10 shows the flow-chart of the simulated annealing (SA) procedure of
Code_8_4.R.
The simulated annealing (SA) code consists of two loops, one nested into the other.
The outer loop, defined ‘cooling loop’ in the above code, simulates the annealing
process. Starting with a temperature T0, a slow cooling process is simulated, decreasing
the temperature down to a preset chosen limit. A cooling schedule, commonly used in
SA problems, prescribes a temperature decrease of the type Tt = α
t T0, with α = 0.8−
0.9. To propose a ‘candidate’ we need a ‘move’. As we have said above, the move can be
a permutation of the cities configuration. Further strategies are proposed, for instance
a simple swap between two cities chosen at random. These type of permutations are
also called ‘nearest permutations’.
Lin (1965) suggests the following procedure, known as 2-OPT. Let Sa be the con￾figuration in which the salesman visits the cities in this order:288 Markov Chain Monte Carlo
Set initial T0
Generate initial
route r0
Generate new
route ri
(initial i = 1)
Store best
route:
r

 = ri
Iterative Loop
Route
length
N Y
N
N
i = i + 1
i = iMAX
?
T < Tmin
?
Accept ri
Accept or not ri
based on the 
Metropolis algorithm
Decrease T
r

 is the solution
L(i) > L (i-1)
?
Fig. 8.10 Flow-chart of the Simulated Annealing solution to the TSP
Cj−1, Cj , . . . , Cl
, Cl+1
The new configuration Sb is:
Cj−1, Cl
, . . . , Cj , Cl+1
That is Sb is equal to Sa, except a stretch of road is travelled in the opposite direction.
Let ∆L = Ll − Ll−1 be the change of mileage, where Ll and Ll−1 are the lengths of
the existing and of the candidate configuration, respectively. If ∆L 6 0, that is Ll
is
less than Ll
, we accept the candidate configuration as the new tour. If ∆L > 0, that is
Ll
is greater than the existing tour, the proposed configuration still may be accepted,
but with probability:
P(∆L) = exp 
−
∆L
T

Then, when T is large, several configurations are accepted (to avoid getting trapped
in a local minimum), while as T decreases fewer ‘worse’ candidates are accepted. ThatThe travelling salesman problem 289
is the essence of the Metropolis algorithm, described by the flow-chart of Fig. 8.11,
constituting the iterative loop nested in the cooling one.
Concerning the annealing loop, two problems deserve attention: the initial temper￾ature and the stopping criterion. Various suggestions can be found in the literature
concerning the choice of T0, see for example Ben-Ameur (2004) and references therein
for a review of existing methods and for their own proposal. In the Code_8_4.R we
assume for T0 a value higher than the absolute value |Ll − Ll−1|, computed on some
hundred permutations, namely T0 = 1000. The algorithm of Code_8_4 simply com￾pletes the cooling loop, saving at every step the best solution encountered.
Figure 8.12 shows a ‘best’ route resulting from the SA algorithm. That route (re￾member: it is not the absolute best!) corresponds to the sequence [1 10 18 20 19 5
8 6 2 13 11 4 16 17 12 9 7 14 15 3 1] and it has a length of 7849 km, as the
last two lines of the code show.
Anticipating what will be presented in Chapter 12 in the framework of spatial anal￾ysis, we want to show the power of the R environment, Fig. 8.13 shows the same TSP
route reported in Fig. 8.12, on a real geographical context. We will not go inside the
mapview function. We only note, in passing, how a few lines of code, see Code_8_5.R,
can accomplish a complex task!
The plot of Fig. 8.13 shows the first route-step, starting from L’Aquila, with a
thick black line, while a thick grey line shows the last route-step coming back to that
town. The remaining thin black lines are all other steps, corresponding to the arrows
in Fig. 8.12.
## Code_8_5.R
# Load required library
library(mapview)
# Coordinate projection on geo map
x = city.coords$Long
y = city.coords$Lat
xx = x[2:20]
yy = y[2:20]
pts = matrix(0, 19, 2)
pts[, 1] = xx
pts[, 2] = yy
ls = st_sfc(st_linestring(pts), crs = 4326)
x1= c(x[1],x[2])
y1 = c(y[1],y[2])
pts1 = matrix(0, 2, 2)
pts1[, 1] = x1
pts1[, 2] = y1
ls1 = st_sfc(st_linestring(pts1), crs = 4326)
x2 = c(x[20],x[1])
y2 = c(y[20],y[1])
pts2 = matrix(0, 2, 2)
pts2[, 1] = x2
pts2[, 2] = y2
ls2 = st_sfc(st_linestring(pts2), crs = 4326)
mapview(ls1,color="black",lwd=4) + mapview(ls,color="black",lwd=0.5) +
mapview(ls2,color="gray",lwd=4)290 Markov Chain Monte Carlo
Given routes
r
k-1 and rk
Compute route
length Lk-1 and Lk
(Worse route)
N
Lk
 < Lk-1
?
Compute ‘energy’
∆L = (Lk
 – Lk-1)
And evaluate
Ptr = exp (-∆L/T)
Sample u Î[0,1]
From uniform
Probability density
Reject rk
Assign rk
 = rk-1
RETURN
RETURN
Accept rk
Accept rk
RETURN
u ≤ Ptr
?
Y N
(Better route)
Y
Fig. 8.11 Flow-chart of the Metropolis algorithm.Exercises 291
8 1 0 1 2 1 4 1 6
38 40 42 44 46
longitude
latitude
OAQ
PZ
CZ
NAP
BO
TS
RM
GE
MI
AN
CB
TO
BA
CA
PA
FI
TN
PG
AO
VE
Fig. 8.12 TSP solution obtained by the simulated annealing algorithm.
8.4 Exercises
Exercise 8.1 Familiarize yourself with the Monte Carlo method in R. By means of rnorm
sample N numbers (with N = 100, 1000, 10000) from a normal distribution with mean µ = 1.5
and standard deviation σ = 0.5, compare the computed average and standard deviation with
µ and σ. Use the density ‘counterpart’ dnorm of the random number generator to compute
and plot the theoretical density, and superimpose it on the sample density (plotted by the
hist command).
Exercise 8.2 The numerical computation of multidimensional integrals can be a quite de￾manding task. In addition to the computation time, which can become prohibitive, the ge￾ometrical definition of the boundaries can be extremely complicated if the domain of inte￾gration is not of simple geometry, but rather a complicated or irregular domain. A Monte
Carlo approach greatly simplifies the task. Random numbers are generated within an area
of simple boundaries (for instance a square in a two-dimensional domain, or a cube in a
three-dimensional one) that contains the area with complicated boundaries. Then, a method
is implemented to discriminate whether a generated random point of given coordinates is
inside or outside the complicated region.
Write an R code using the Monte Carlo method to compute the volume of the sphere x
2+y
2+
z
2 = R
2
. Try to change the order of the discretization to see how that affects the accuracy.292 Markov Chain Monte Carlo
Fig. 8.13 TSP solution mapped on a true geographical context.
Exercise 8.3 Modify the last section of Code_8_1.R to sample 5000 values from a gamma
distribution with parameters: shape = 2, scale = 0.5.
(1) Try to reproduce the plot in Fig. 8.14, where the histogram of data coming from the
MCMC simulation (excluding a burn-in of 100 values) is compared to the curve obtained by:
curve(dgamma(x,shape=2,scale=0.5),add=T)
(2) Generate 5000 samples using the R function rgamma and compare the results.
(3) How does the number of discharged values (burn-in) affect the result?
Warning: pay attention to the starting value x(1), and remember the peculiar characteristics
of the gamma distribution!
Exercise 8.4 Sampling by the MCMC algorithm is also allowed from a discrete distribution,
with very simple changes to Code_8_1.R concerning the target distribution and the proposed
one. Suppose, for example, you have to sample from a distribution of six integers (from 1 to
6) given by a table like the following:
1 2 3 4 5 6
0.15 0.30 0.20 0.10 0.17 0.08
Obtain 5000 samples for such a distribution and verify that the frequency of the obtained
values is consistent with those of the above table.
Hint: change the target distribution to one based on the list c(0.15,0.30,0.20,0.10,0.17,0.08)
and use a ‘fair’ die roll for the proposed distribution.Exercises 293
density
0 1 2 3 4 5
0.0 0.2 0.4 0.6 0.8
Fig. 8.14 MCMC simulation of a gamma distribution.
Exercise 8.5 Run the R code written to solve Exercise 8.4 with different numbers of steps:
100, 1000, 10000. Compute the relative frequencies of the six possible outcomes and compare
them in a table with the values describing the discrete target distribution. How do the initial
value and the burn-in period affect the results?9
Bayesian Inference and Stochastic
Processes
Every logical system must start somewhere,
and the question simply amounts to: where do we start?
Sir Harold Jeffreys, Scientific Inference
Statistical techniques based on the Bayesian paradigm can be applied in any flavour
of data analysis (Gelman et al., 2004) including the spectral analysis of time series
(Broemeling, 2018) and for the study of stochastic processes (Insua et al., 2012). On
the other hand, stochastic processes (namely Markov chains) constitute an essential
tool for Bayesian analysis.
We assume that the reader is familiar with the Bayes theorem, also known as
the ‘rule’ of conditional probabilities, that is the theoretical foundation of Bayesian
statistical methods (Bernardo and Smith, 1994). Essentially the Bayes theorem mathe￾matically expresses the process of ‘learning from observations’, as it allows to quantify
our state-of-knowledge about a hypothesis (a theory, a model) through the ‘posterior’
probability, computed in terms of the data (the likelihood) and of the previous state
of knowledge, measured by the ‘prior’ probability (Robert, 1994).
Bayesian statistics has experienced a great development and diffusion in the sci￾entific world mainly thanks to Sir Harold Jeffreys (Jeffreys, 1939) who re-discovered
the Bayesian approach at the beginning of the past century, in a text developing a
fundamental theory of scientific inference based on the Bayesian paradigm. Jeffreys
‘who saw the truth and preserved it’, using the words of Jaynes (2003) in his book on
the foundations of the theory of probability.
The Bayes theorem (for probability density) is formulated as follows. The (poste￾rior) probability distribution for a parameter θ, given the observed dataset y is:
p(θ|y) = p(y|θ)p(θ)
R
Θ
p(y|θ)p(θ)dθ (9.1)
The terms on the right-hand side are: the prior probability density p(θ) and the like￾lihood p(y|θ) (sometimes called ‘sampling distribution’), i.e. the probability of the
observed data y given the parameter θ. The denominator is essentially a normalizing
constant, conceptually given by the integral of the numerator over all possible values
of θ, which is the definition of the marginal probability p(y) of the data. Following
(Smith, 1991), the Bayesian inference scheme is formally quite simple: the uncertainty
about a parameter θ after data y have been observed, and it is computed simply byBayesian Inference and Stochastic Processes 295
specifying p(y|θ) (usually referred to as the likelihood l(θ, y) when viewed as a function
of θ) and p(θ), and normalizing their product to make it a probability distribution.
The presence of the prior probability, sometimes criticized by ‘classical’ statisti￾cians for its apparent subjectivity, is one of the strengths of the Bayesian approach.
Indeed, the prior probability represents our current state of knowledge before a bunch
of data is taken into consideration. Prior can be chosen so as to express ‘ignorance’
about the parameters of interest, or it can take into account what we really know
about them (for example, as a consequence of theoretical considerations or in virtue
of previous data).
Using a very simple probabilistic example, it can be easily demonstrated that data
always wins over the prior knowledge, but that the greater is our confidence in a
hypothesis, the greater must be the evidence in the data to change our mind. The
example comes from (Sivia, 1996), and concerns the verification of the hypothesis
of unfairness of a coin in coin-tossing game. Let H be a number between 0 and 1
representing the bias-weighting of the coin, with H = 0.5 indicating a fair coin, H = 0
and H = 1 representing a coin which always gives tail or head, respectively. Our
inference about the fairness of the coin is given by the conditional probability p(H|D),
where D represents the results of coin-tossing. Bayes theorem tells us that such a
posterior probability is given by:
p(H|D) = p(D|H)p(H)
p(D)
∝ p(D|H)p(H) (9.2)
In (9.2) the denominator can be omitted, because it is only a normalizing constant.
The likelihood p(D|H) for D = ‘k heads in N tosses’ is given by the binomial
distribution, therefore:
p(D|H) ∝ Hk
(1 − H)
N−k
Suppose the true value of H is 0.25, i.e. the coin is biased with ‘head’. We can
compare how different priors influence the posterior p(H|D) by computing:
p(H)Hk
(1 − H)
N−k
for different choices of p(H). Figure 9.1 (top-left) compares three priors: (1) a uniform
one (ignorance) p(H) = 1; (2) an assumption of probable fairness (usually coins are
fair...) expressed by a distribution centred at H = 0.5, e.g. a normal one: p(H) ∼
N(0.5, 5 × 10−4
); (3) an assumption of quasi-certain unfairness, e.g. expressed by a
beta distribution p(H) ∼ Be(0.5, 0.5).
The effect of the above priors on the posterior probability p(H|D) after 10, 100 and
1000 coin tosses is shown in Fig. 9.1. What it clearly shows is that a wrong hypothesis
(fair coin) is rejected by a sufficiently large amount of data but it also tells us what we
intuitively know: the more we are convinced of the correctness of a hypothesis, the more
data confuting it we would need to definitely reject it. If from a single measurement
we obtain that the gravitational acceleration on the Earth surface is different from
9.81 m/s
2 we do not reject Newton’s law, but if we were to obtain 9.7 in one million
independent measurements, made by different people with well-calibrated instruments,
we should conclude that such a law is no longer valid!
The following R code has been used to generate Fig. 9.1.296 Bayesian Inference and Stochastic Processes
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
H
prior probability
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
H
posterior probability
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
H
posterior probability
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
H
posterior probability
Fig. 9.1 Top-left: prior probabilities for H: uniform (solid line), fair-coin (dashed line), un￾fair-coin (dotted line). Top-right: posterior probabilities for H, after 10 coin tosses. Bottom-left
and bottom right: posterior probabilities for H, after 100 and 1000 coin tosses, respectively
## Code_9_1.R
# Unfair coin tossing example
# PRIOR distributions
# true H value
h <- 0.25
H <- seq(0,1,0.001)
# uniform prior
prior1 <- rep(1,1001);
plot(H,prior1,t="l",lty=1,ylim=c(0,1),ylab="Prior probability")
# fair coin
prior2 <- exp(-(H-0.5)^2/5e-3)
lines(H,prior2,lty=2)
# unfair coin
prior3 <- dbeta(H,0.5,0.5)/10
# delete Inf values in H=0 and H=1
prior3[1] <- prior3[2]
prior3[1001] <- prior3[1000]
lines(H,prior3,lty=3)
# POSTERIOR distributions
# posterior after 10 coin tosses
k <- rbinom(1,10,0.25)
likelihood <- dbinom(k,10,H)
post1 <- prior1*likelihood/max(prior1*likelihood)
plot(H,post1,t="l",lty=1,ylim=c(0,1),ylab="Posterior probability")
post2 <- prior2*likelihood/max(prior2*likelihood)
post3 <- prior3*likelihood/max(prior3*likelihood)
lines(H,post2,lty=2)
lines(H,post3,lty=3)
# posterior after 100 coin tossesApplication of MCMC in a regression problem with auto-correlated errors 297
k <- rbinom(1,100,0.25)
likelihood <- dbinom(k,100,H)
post1 <- prior1*likelihood/max(prior1*likelihood)
plot(H,post1,t="l",lty=1,ylim=c(0,1),ylab="Posterior probability")
post2 <- prior2*likelihood/max(prior2*likelihood)
post3 <- prior3*likelihood/max(prior3*likelihood)
lines(H,post2,lty=2)
lines(H,post3,lty=3)
# posterior after 1000 coin tosses
k <- rbinom(1,1000,0.25)
likelihood <- dbinom(k,1000,H)
post1 <- prior1*likelihood/max(prior1*likelihood)
plot(H,post1,t="l",lty=1,ylim=c(0,1),ylab="Posterior probability")
post2 <- prior2*likelihood/max(prior2*likelihood)
post3 <- prior3*likelihood/max(prior3*likelihood)
lines(H,post2,lty=2)
lines(H,post3,lty=3)
The power of the Bayesian approach is that of producing the posterior probabil￾ity distribution, which in turn allows us to compute any needed statistics and confi￾dence interval for them. Moreover, it can be easily demonstrated that well-established
estimates, like maximum-likelihood and least-squares, are easily justified under the
Bayesian paradigm (Sivia, 1996): maximizing the posterior probability with a uniform
prior coincides with maximizing the likelihood function, while a posterior analysis
using a uniform prior and a Gaussian likelihood with a least-squares estimate.
Bayesian analysis is increasingly used for studying random processes (Insua et al.,
2012; Broemeling, 2018), but also the reverse is true: stochastic process like MCMC
(in particular, the Metropolis-Hasting and Gibbs algorithms) are largely employed
for sampling from posterior probability distributions, which is the basis of the more
widespread approach to numerical Bayes computation.
9.1 Application of MCMC in a regression problem with
auto-correlated errors
Research activities in Antarctica, the greatest untouched ‘natural laboratory’ of the
world, are of paramount importance for several sectors of science, from biology to
geophysics. As an example, the study of the gas content trapped in ice cores drilled
in the Antarctic pack, has allowed to prove the existence of climate cycles (first hy￾pothesized in 1920 by the geophysicist Milutin Milankovitch).1 Such a result, which is
tightly connected with global climate change, suggests a causal relationship between
temperature and carbon dioxide (CO2) concentration in Antarctic ice as a function of
age (Luethi et al., 2008), which is directly correlated to the depth along the ice core.
Figure 9.2 compares the temperature anomaly (difference with respect to mean
temperature of the last millennium, in oC) with the CO2 content, in parts per million,
relative to the last 800,000 years, obtained from ice cores drilled in the framework
of the European project EPICA (European Project for Ice Coring in Antarctica).
The drilling were conducted at Kohnen Station (75o009006000S;00o0490 04000E) and at
1A review of the Milankovitch theory and its experimental confirmations was published in (Berger
et al., 2015)298 Bayesian Inference and Stochastic Processes
Concordia Station (Dome C; 75o069004000S;123o 209052000E), at a depth of up to 2,774
m and 3,270 m, respectively.
temperature anomaly (°C)
−10 −5 0 5
−800 −600 −400 −200 0
180 220 260 300
age (kyr)
CO2 (ppm)
Fig. 9.2 Temperature anomaly (top) and CO2 content (bottom) as a function of the age in
thousands of years.
The relationship between two historical time series, one concerning the temperature
and the other one the CO2 concentration from about 800,000 years ago to the present
time, appears to be approximately linear. Indeed, an ordinary linear regression analysis
applied to the EPICA data obtained from http://www.climatedata.info, thinned out
so as to have a time step of 2500 years (the original time step was 100 years), shows
that such a linear relation exists, as Fig. 9.3 shows.
Figure 9.3 has been obtained by the following bunch of code, which uses the R
function lm to perform the linear fitting.2
## Code_9_2.R
# Bayesian analysis of temperature and CO2
setwd("C:/RPA/code/bayesian")
ice.data <- read.table("data/Epica.txt",h=TRUE)
# assign column data to Year, T, CO2
Year <- ice.data$Year
T <- ice.data$T
CO2 <- ice.data$CO2
2The data file ‘epica.txt’ is available on the book websiteApplication of MCMC in a regression problem with auto-correlated errors 299
180 200 220 240 260 280
−10 −6 −2 0 2 4
CO2
T
Fig. 9.3 Relationship between CO2 content and temperature.
# Linear regression
Lfit <- lm(T~CO2)
plot(T~CO2)
abline(Lfit,col="red")
summary(Lfit)
# Linear regression
lm(formula = T ~ CO2)
The output produced from the last line in the code above shows that the linear
regression coefficients obtained by the ordinary least squares (OLS) analysis appear
to be statistically significant. The problem is that OLS, to be performed, requires the
residual errors to be independent and identically distributed. Instead, if we compute
the autocorrelation of the residuals, by means of the acf function in R, we verify that
errors are correlated, as Fig. 9.4 shows.
lm(formula = T ~ CO2)
Residuals:
Min 1Q Median 3Q Max
-3.7783 -0.7896 0.0081 0.8350 3.7894
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) -27.54583 0.68712 -40.09 <2e-16 ***
CO2 0.09942 0.00305 32.60 <2e-16 ***
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 1.394 on 318 degrees of freedom
Multiple R-squared: 0.7697,Adjusted R-squared: 0.769300 Bayesian Inference and Stochastic Processes
F-statistic: 1063 on 1 and 318 DF, p-value: < 2.2e-16
Figure 9.4 shows the presence of autocorrelation between data at consecutive times
in the time series.3 That is more clearly shown by computing the partial autocorrelation
of the residuals, using pacf(Lfit$residuals), which gives an autocorrelation of about
0.68 between consecutive data points (see Fig. 9.5). Such a correlation affects the
reliability and robustness of the OLS regression analysis. When the Gauss-Markov
assumptions (homoscedasticity and independence of errors) are violated, the variance
of the regression parameters obtained by means of the OLS procedure is wrong and,
as a consequence, also the confidence intervals of the parameters are incorrect.
Figure 9.5, showing a single dominant peak in the partial autocorrelation function,
suggests a first-order autoregressive structure AR(1) for the residual error (see Chapter
6).
0 5 10 15 20 25
−0.2 0.2 0.6 1.0
lag
ACF
Fig. 9.4 Autocorrelation of residual errors versus time lag.
A robust Bayesian regression analysis on data with AR(1) errors can be performed
based on Gibbs sampling (Chib, 1993) or, more generally, on MCMC sampling. We
will briefly describe the rationale of the Bayesian analysis, schematically describing
how the regression can be implemented in R, but instead of implementing it ‘from
scratch’, we will use the R package rjags which works as an interface with the JAGS
(Just Another Gibbs Sampler) program (Plummer, 2003). JAGS, which allows rather
simple but efficient implementations of the Bayesian procedure to obtain a posterior
probability distribution given a prior distribution and a likelihood function, is concisely
described in Appendix B.
3A similar analysis has been conducted in (Hoff, 2009) for data coming from ice cores drilled at
the Russian Vostok station in East Antarctica (78oS, 106oE) (Petit et al., 1999).Application of MCMC in a regression problem with auto-correlated errors 301
5 10 15 20 25
0.0 0.2 0.4 0.6
lag
partial ACF
Fig. 9.5 Partial autocorrelation of residuals versus time lag.
We firstly need to understand how the presence of autocorrelation influences the
regression analysis. Coming back to the uncorrelated-error case, the OLS procedure
can be summarized in matrix form. Suppose we have k = 1...n observations at k
times of the variable y = (yk)
T
(the CO2 concentration) corresponding to the variable
x = (xk)
T
, where the superscript ‘T’ denotes the transpose operation. Denoting by
X = (x 1) the matrix of size n × 2 having a column vector of ones as the rightmost
column, by β = (a b)
T
the vector of the (unknown) linear regression coefficients, and
by  = (1...n)
T a vector of independent normally-distributed errors with standard
deviation σ, the linear model can be written:
y = Xβ +  (9.3)
In other words, each yk = axk + b has a normal distribution:
yk ∼ N (axk + b, σ2
) (9.4)
and therefore y has a multivariate normal distribution:
y ∼ MN (Xβ, σ2
I) (9.5)
where I is the identity matrix of rank n, and MN (µ, σ2
I) is the product of n identical
normal distributions.
Remark 9.1 We use the notation Z ∼ MN (µ, Σ) to mean that Z = (Z1...Zn)
T
has a
multivariate normal distribution, while N (µ, σ2
) denotes an univariate normal distribution, when
the variable Z is a vector. The general definition of a multivariate normal probability distribution
is:302 Bayesian Inference and Stochastic Processes
p(Z) = 1
p
(2π)
ndet(Σ)
exp 
−
1
2
(Z − µ)
T Σ
−1
(Z − µ)

which reduces to (9.5), with Σ ≡ σ
2
I, if the Zk are independent normally distributed variables
with identical variance σ
2
.
The OLS procedure consists of estimating a vector βˆ minimizing the sum of squared
residuals (SSR):
βˆ = min
β
SSR(β) (9.6)
It is easy to show that such a minimum corresponds to the solution of the system:
βˆ = (XTX)
−1XTy (9.7)
provided XTX is not singular.
Proof Solving (9.3) with respect to , with βˆ in place of β, and multiplying to the
left by its transpose, we obtain the sum of squared residuals:
SSR = 
T
 = (y − Xβˆ)
T
(y − Xβˆ) (9.8)
Developing the product:
SSR = y
T y − 2βˆT XT y − βˆT XT Xβˆ (9.9)
and taking the derivative of SSR with respect to βˆ we obtain:
−2XT y + 2XT Xβˆ = 0 (9.10)
which eventually gives (9.7). ✷
If, instead of being independent and identically distributed, errors are correlated,
the matrix σ
2
I must be replaced by a covariance matrix Σ (see Remark 9.1). In the
AR(1) case, errors are first-order autoregressive, i.e. error ut at time t only depends
upon the error at the previous time t − 1:
ut = ρut−1 + t (9.11)
with t ∼ N (0, σ2
), i.e. are normal independent (uncorrelated) errors, identically dis￾tributed as required by the Gauss-Markov conditions necessary for the application of
the ordinary least square procedure. In (9.11) ρ is the correlation among errors, in ab￾solute value a number less than one. Modifying (9.3),the linear model with the serially
correlated error u is:
y = Xβ + u (9.12)
The variance σ
2
u of the true, serially correlated, error at time t is E[u
2
t
] = σ
2/(1−ρ
2
)
while the covariance between times t and t − k is E[utut−k] = ρ
kσ
2/(1 − ρ
2
).Application of MCMC in a regression problem with auto-correlated errors 303
Proof Suppose there are N temporal steps. By applying (9.11) recursively, we obtain:
ut = ρ
N tt−N
N
X−1
i=0
ρ
i
t−i =
X∞
i=0
ρ
i
t−i (9.13)
where the last passage holds for very large N (virtually infinite). The covariance γk
for a time lag k is therefore:
γk = E[utut−k] = X∞
i=0
X∞
j=0
ρ
i
ρ
jE[t−it−k−j ] (9.14)
Remembering the assumptions, E[
2
i
] = σ
2 and E[ij ] = 0 for i 6= j, we obtain:
γk = σ
2
ρ
k
1 − ρ
2
(9.15)
✷
Therefore, in presence of AR(1) error, the covariance matrix can be computed in
terms of the correlation coefficient ρ:
Σ = σ
2Ω = σ
2
1 − ρ
2





1 ρ ρ2
. . . ρn−1
ρ 1 ρ . . . ρn−2
.
.
.
.
.
.
.
.
.
.
.
.
ρ
n−1 ρ
n−2 ρ
n−3
. . . 1





(9.16)
If Ω is known, the OLS approach can be generalized (Sen and Srivastava, 1990) to
give the required estimation of β:
βˆ = (XT Ω−1X)
−1XT Ω−1y (9.17)
All the above treatise requires is that we know the correlation coefficient ρ, but we
don’t!
9.1.1 MCMC implementation of Bayesian regression
The unknown parameters of the regression problem are four: the vector of regression
coefficients β = (β0, β1), the error variance σ
2 and the correlation coefficient ρ. From
the Bayesian viewpoint we are not trying to obtain single estimates (and, possibly, vari￾ances or confidence intervals) for those parameters, but we are interested in evaluating
their posterior probability distribution.
The procedure, as described by (9.2), specialized in (9.18) for the regression prob￾lem, requires us to write down a likelihood function and to assign a prior probability
distribution to each parameter of interest:
p(β, σ2
, ρ|y, X) ∝ p(y|β, σ2
, ρ, X)p(β, σ2
, ρ|X) (9.18)
The likelihood is no different from the joint probability distribution for the ob￾servation, supposing the regression parameters and the error correlation are known.304 Bayesian Inference and Stochastic Processes
Therefore, using the AR(1) definition (9.11) in (9.12) it can be expressed (Chib, 1993)
as a modified multivariate normal, given the first observation y1. It comes out that
the probability distribution of yt depends on yt−1 as:
yt|t−1 ∼ N ￾
ρyt−1 + (β0xt + β1) − ρ(β0xt−1 + β1), σ2

(9.19)
because, by subtracting ρyt−1 from yt we obtain a linear model with normal, indepen￾dent errors. As a consequence, the desired likelihood is given by:
p(y2...yn|y1, β, ρ, X) ∝
1
σ
n−1
exp 
−
1
2σ
2
Xn
t=2
(yt − yt|t−1)
!
(9.20)
The choice of the prior distribution is important, because the prior includes the
information available (if any), but, as discussed at the beginning of this chapter, data
always win. Therefore, a proper choice of the prior helps a faster convergence, but if no
information is available, a ‘diffuse’ non-informative prior, assigning probability evenly
over large regions of the parameter space, is a suitable choice. In the current regression
problem, for example, we can assume that the prior distribution of (β, σ2
, ρ) is the
product:
p(β, σ2
, ρ) = p(β|σ
2
)p(σ
2
)p(ρ) (9.21)
meaning that (β, σ2
) is independent of ρ. A possible strategy is to choose for p(β|σ
2
)
and p(σ
2
) a pair of ‘conjugate’ priors.4
.
A possible choice for our problem (Chib, 1993; Hoff, 2009) is:
β|σ
2 ∼ MN (β0, Σ0)
ρ ∼ N (ρ0, σ2
ρ
) (9.22)
σ
2 ∼ IG(
ν0
2
,
δ0
2
)
Different choices of the parameters β0, Σ0, ρ0, σ2
ρ
, ν0, δ0 in (9.23) allow us to express
the desired degree of knowledge about the regression parameters of interest. A diffuse
prior is obtained posing β0 = 0, ρ0 = 0, ν0 = 1, and by assuming very large variances.
The following R code implements the Bayesian estimation of the regression problem
using JAGS for the MCMC sampling. Note that JAGS uses precisions τ = 1/σ2
instead of variances in the specification of the probability distributions. Therefore, a
diffuse prior involves rather small precision, because of the inverse relationship between
precision and variance.
## Code_9_3.R
# Linear regression using JAGS
library(rjags)
# Initialization
n <- length(T)
4A prior is conjugate to the likelihood function when it gives rise to a posterior belonging to the
same distribution family (Gelman et al., 2004Application of MCMC in a regression problem with auto-correlated errors 305
jags.data = list("Y"=T,"N"=n,"X"=CO2)
jags.params=c("sigma","alpha","beta","rho")
jags.inits <- list("tau" = 1, "alpha" = 1, "rho" = 0)
# JAGS code
jags.model <- textConnection("model {
alpha ~ dnorm(0, 0.01);
tau ~ dgamma(1,0.001);
sigma <- 1/sqrt(tau);
beta ~ dnorm(0,0.001);
rho ~ dnorm(0, 0.1);
predY[1] <- Y[1];
for(i in 2:N) {
predY[i] <- alpha + beta*X[i] + rho * (Y[i-1] - alpha - beta*X[i-1]);
Y[i] ~ dnorm(predY[i], tau);
}
}")
# Evaluation
model <- jags.model(jags.model, data=jags.data, inits=jags.inits, n.chains=1)
# Burning in
update(model, n.iter=10000)
# Sampling
samples <- coda.samples(model, jags.params,30000)
# Transform into dataframe
out <- do.call(rbind.data.frame, samples)
# Plot histograms
hist(out$rho)
hist(out$beta)
hist(out$alpha)
hist(out$sigma)
The above code assigns suitable probability distributions to the parameters (α,
β, ρ, τ ) and defines the standard deviation σ as the reciprocal of the precision τ .
Then it computes the likelihood function, or predictive distribution, based on the
experimental data. JAGS evaluation consists of defining the model and running it for
a suitable number of times. More details are given in Appendix B.
Figure 9.6 shows the posterior probability distributions of the four parameters.
By means of the above analysis, we eventually obtain the desired regression model.
Figure 9.7 compares the OLS linear fitting (dashed line) with the regression corrected
for error autocorrelation (solid line).
## Code_9_4.R
# Plot results
plot(CO2,T)
abline(Lfit,col="red",lty=2)
interc <- mean(out$alpha)
slope <- mean(out$beta)
rho <- mean(out$rho)
abline(interc,slope,col="blue")
Let us take a look at the meaning of what we have found. Coming back to (9.19),
that equation is simply an OLS regression involving ‘lagged’ variables. With reference306 Bayesian Inference and Stochastic Processes
a
density
−30 −28 −26 −24 −22 −20
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35
b
density
0.07 0.08 0.09 0.10
0 20 40 60 80
f
density
0.55 0.60 0.65 0.70 0.75 0.80 0.85
0 2 4 6 8
s
density
0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20
0 2 4 6 8
Fig. 9.6 From top-left to bottom-right, posterior probability distributions of α, β, ρ and σ.
180 200 220 240 260 280
−10 −8 −6 −4 −2 0 2 4
CO2
T
Fig. 9.7 Result of the Bayesian regression analysis.Application of MCMC in a regression problem with auto-correlated errors 307
to Chapter 6, the presence of an AR(1) error ut means that the relationship among
the dependent variable y = T and the independent variable x = CO2 is:
yt = β + αxt + ut (9.23)
By applying the lag operator L to the error term in (9.23), in terms of the computed
correlation coefficient ρ, we transform it to:
yt = β + αxt + (1 − ρL)
−1
t
where t is white noise. Multiplying both members by (1 − ρL) gives:
(1 − ρL)yt = (1 − ρL)β + α(1 − ρL)xt + t
which defining ˜yt = (1−ρL)yt, ˜xt = (1−ρL)xt and β
∗ = (1−ρL)β, gives an ordinary
OLS equation:
y˜t = β
∗ + αx˜t + t (9.24)
i.e. the equation describing a correct linear regression (because t is uncorrelated white
noise) with the slope computed by the Bayesian algorithm and an intercept β
∗
that
is scaled by a factor (1 − ρ). The above procedure was firstly proposed by (Cochrane
and Orcutt, 1949) and is known as Cochrane-Orcutt estimation.
Now, if we transform the original temperature and carbon dioxide data using the
lag operator L and the correlation ρ, using the code below:
## Code_9_5.R
# define x and y as above
y <- T
x <- CO2
# redefine xx = x_tilde and yy = y_tilde
yy <- rep(0,n-1)
xx <- rep(0,n-1)
for (i in 2:n){
yy[i-1] <- y[i] - rho*y[i-1]
xx[i-1] <- x[i] - rho*x[i-1]
}
fit2 <- lm(yy~xx)
plot(xx,yy,
xlab=expression(paste("(1-",rho,"L)CO"[2])),
ylab=expression(paste("(1-",rho,"L)T")))
abline(fit2,col="red")
abline(interc*(1-rho),slope,col="green",lty=2)
# ACF
acf(fit2$residuals)
we verify that the OLS regression conducted on the lagged variables practically co￾incides with the line having the slope computed by the Bayesian procedure and the
intercept transformed according to the correlation coefficient (Fig. 9.8).308 Bayesian Inference and Stochastic Processes
40 60 80 100
−4 −2 0 2 4
(1−ρL)CO2
(1−ρL)T
Fig. 9.8 OLS regression on the lagged variables. The solid line refers to the linear fitting,
the dotted line to the parameters computed by the Bayesian procedure.
Finally, we can convince ourselves that the errors involved in the lagged regression
are practically uncorrelated by looking at the autocorrelation function, computed in
the last line of the above code, as Fig. 9.9 shows.
0 5 10 15 20 25
0.0 0.4 0.8
lag
ACF
Fig. 9.9 OLS regression on the lagged variables: autocorrelation function.
Another powerful application of Bayesian analysis is the computation of the spec￾trum of a time series. In the following section we will show how Bayesian spectralBayesian spectral analysis applied to RADAR target detection 309
analysis can help in the problem of target detection by a stepped frequency continu￾ous wave (SFCW) RADAR.
9.2 Bayesian spectral analysis applied to RADAR target detection
Sometimes random phenomena obscure coherent signals. That happens when a deter￾ministic signal is hidden by the presence of a stochastic signal (noise). That is exactly
what happens when a RADAR operates in presence of intense rain, at wavelengths
comparable to the size of the rain drops. Rain backscattering produces a stochastic
signal having the characteristics of noise (Richards, 2014). Indeed, it has been demon￾strated (Richards, 2014; Eaves and Reedy, 1987) that the motion of the clutter (rain
droplets, in the present case) induces decorrelation in time and, therefore, also in space,
which makes the clutter signal similar to white noise.
We will apply Bayesian analysis to the detection of an obstacle by means of a
stepped-frequency continuous-wave RADAR in the presence of rain. The situation is
depicted in Fig. 9.10.
RADAR
ANTENNA
OBSTACLE
Fig. 9.10 Radar detection of an object through a rain curtain.
The SFCW radar is a practical implementation of the frequency modulated CW
radar, where the modulating ramp actually consists of a stepwise digital signal making
the radar output a succession of short CW pulses of increasing frequency. In the mono￾static configuration, the transmitted radar signal is mixed with that received by the
same antenna, reflected from an obstacle, giving rise to an oscillating low-frequency
signal. In the case of a single reflector, this last signal depends harmonically on the
path length, allowing the detection of the radar-to-target distance.
In general, if K reflectors are present and detectable (i.e. in the range of the radar,
and having a radar cross section sufficient to produce an echo above the clutter/noise),
the radar signal s(f) consists of a sum of harmonics:310 Bayesian Inference and Stochastic Processes
s(f) = X
K
k=1
[Akcos(wdk) + Bksin(wdk)] (9.25)
where w is a function of the signal frequency f. In particular, considering that the
transmitted CW signal ‘contains’ N discrete frequencies:
fi = f0 + i
BW
N
, i = 0...N (9.26)
the received signal consists of the cosine of the phase difference between the echoed
signal and the transmitted one:
si = cos(φi)
with:
φi = 2π
2d
c

f0 + i
i
N
BW
(9.27)
In other words, the measured data belong to the ‘frequency domain’, and the elab￾oration (a discrete Fourier transform, in the simplest case) transforms the frequency
domain into the ‘distance domain’. A target at distance dj gives rise to a signal:
sj = cos(φj ) = cos 
4πf
c
dj

(9.28)
The distance domain is scanned by varying the index j in a suitable range. The
probabilistic approach essentially consists of evaluating the posterior probability of the
parameters of the problem p(δ|D, I) (δ denotes the set {dk} of the target distances)
given the measured data D and all the available information I, starting from a prior
probability p(δ|I) of those parameters and computing the likelihood p(D|δ, I) of the
data given the assumed model. The Bayes theorem gives the following relation:
p(δ|D, I) ∝ p(δ|I)p(D|δ, I) (9.29)
Assuming a Gaussian noise, the posterior probability can be demonstrated (Bret￾thorst, 1988) to have the form of a Student t-distribution:
p({dk}|D, I) ∝
"
1 −
mh
2
ND2
# M−n
2
(9.30)
where: m = 2K is the number of model functions (two for a single reflection, four for
two reflections, and so on), N is the number of data points, D2 is the squared mean
of the data points, and h
2 is the squared mean of the projections of the data on a setBayesian spectral analysis applied to RADAR target detection 311
of orthogonal model functions derived from those in (9.25). This last function is given
by:
h
2 =
1
m
Xm
j=1
h
2
j
(9.31)
In terms of a set of orthogonal functions Hk eqn (9.25) is written as:
s(f) = Xm
k=1
CkHk(w, {dk}) (9.32)
the orthogonality property allows us to obtain a diagonal ‘interaction matrix’ H · HT
which simplifies the computation. Incidentally, h
2 happens to be a generalization of the
periodogram (Bretthorst, 1988) to arbitrary model functions, and thus it is a sufficient
statistics for the set of investigated parameters δ = {dk}.
Referring to (Bretthorst, 1988) the computation procedure can be summarized as
follows:
1. Assumed a ‘model’ for the main reflections (m = 2, in the usual case of ‘separable’
targets), for each possible value of the parameter of interest dk in a guessed
interval (dmin, dmax) derive the orthogonal model functions Hk, with a simple
eigen-analysis of the initial harmonic functions (9.25).
2. For each distance value compute the statistics (9.31) and the relative probability
value (9.30).
The mode (maximum value) of the posterior probability distribution gives the
most probable reflector distance (or distances). The orthogonal model functions allow
us to estimate the amplitude of the main reflection from the data and to evaluate the
signal-to-noise ratio (SNR). The noise variance is easily computed from the data:
σ
2 =
N
N − m − 2
h
d
2 −
m
N
h
2
i
(9.33)
The SNR is eventually computed by means of (9.31) and (9.33):
SNR =
mh
2
Nσ
2
(9.34)
In (9.34), the average signal power and the estimated noise variance are computed
for the distance dj giving the maximum probability value. In practice (as m = 2) the
SNR is computed according to (9.35):
SNR =
2 max(h
2)
Nmin(σ
2)
(9.35)
The posterior probability (9.30) would assume a simpler shape if the noise variance
were known. In that case it would correspond to an exponential of the periodogram:312 Bayesian Inference and Stochastic Processes
p({dk}|D, I) ∝ exp "
mh
2
2σ
2
#
(9.36)
Figure 9.11 shows the detection of a metallic cube placed at a distance of 7.6 m
from the radar. The weather conditions were good (no rain) in that case. Figure 9.11
shows the periodogram h
2 as a function of the distance from the radar.
2 3 4 5 6 7 8 9
0.00 0.05 0.10 0.15
distance (m)
<h
2>
Fig. 9.11 Radar detection of an obstacle in dry air
Figure 9.12 shows the posterior probability computed from (9.36) by approximating
the true variance with that computed from the data. Based on the posterior probability
the distance detection is a sharp peak, as a consequence of the exponentiation of h
2.
The problem with posterior probability is that a very intense echo makes lower
reflections undetectable. In Fig. 9.12 the posterior probability related to Fig. 9.13 for
a test reflector (a metallic target placed at a known distance as a reference) at a
distance of 13.5 m from the radar appears as a sharp vertical line. No other obstacles
are detected.
A plot of h
2 as a function of distance, instead, suggests the presence of at least two
small-reflections at about 10 and 11.8 m from the antenna, as Fig. 9.14 shows.
Now, let us see what happens during rain. The radar made a frequency sweep be￾tween 76 and 77 GHz, which means an average wavelength of 3.9 mm. Rain drops have
a size of the same order of magnitude as the radar wavelength, being approximately
distributed as a truncated gamma function with a mode around 0.5 mm (Fiser, 2010).
Moreover, their size is very small compared to the radar resolution, and of course
they are (rapidly) moving objects, so that rain backscattering is a very dynamic process
involving a decorrelation time (decorrelation means, in the present context, a non-Bayesian spectral analysis applied to RADAR target detection 313
2 3 4 5 6 7 8 9
0.4 0.6 0.8 1.0
distance (m)
posterior probability
Fig. 9.12 Posterior probability for the detection of an obstacle in dry air.
10 12 14 16 18
0.0 0.2 0.4 0.6 0.8 1.0
distance (m)
posterior probability
Fig. 9.13 Radar detection of a test reflector.314 Bayesian Inference and Stochastic Processes
9.0 9.5 10.0 10.5 11.0 11.5 12.0
0.00 0.10 0.20
distance (m)
<h
2>
Fig. 9.14 Radar detection of obstacles close to a test reflector
constant relationship between the phase of the transmitted signal and that of the
echo) that decrease with increasing rain velocity. This is a very complex problem that
can be summarized by asserting that rain is something like as Gaussian noise.
Figure 9.15 shows the disastrous tentative attempt at detection of the same obstacle
as Fig. 9.11 in presence of rain. The metallic cube at 7.6 m is not detected at all!
It appears reasonable that averaging over successive measurements can bring a
substantial gain in the ‘signal-to-clutter’ ratio, as successive measurements on immobile
or very slow targets are very correlated whilst rain clutter is not. By averaging over
five measurements, which span a time of the order of a couple of seconds (irrelevant for
the purpose of detecting a still or slowly moving object, but long relative to the rain
drops movement), the situation completely changes, allowing us to correctly detect the
obstacle (Fig. 9.16).
Apart from the qualitative reasoning suggesting the averaging procedure to solve
the detection problem in the presence of intense noise, that approach has an obvious
statistical ground. Resorting to the approximate expression for posterior probability
(9.36), observing that in N repeated measurements the posterior probability of the
distance δ becomes:
p(δ|σ, D, I) ∝ exp "X
N
k=1
C(δ)k
σ
2
#
(9.37)
where C(δ)k is the k-th periodogram. The standard deviation is assumed known, as
explicitly indicated in (9.37). It can be demonstrated that, if the spectrum amplitudeBayesian spectral analysis applied to RADAR target detection 315
2 3 4 5 6 7 8 9
0.00 0.05 0.10 0.15 0.20
distance (m)
<h
2>
Fig. 9.15 Failure in detecting an obstacle in presence of rain.
2 3 4 5 6 7 8 9
0.00 0.05 0.10 0.15 0.20
distance (m)
<h
2>
Fig. 9.16 Radar detection of an obstacle in presence of rain.316 Bayesian Inference and Stochastic Processes
remains constant during the repetition, the estimation of δ becomes better by a factor
N1/2
, as expected for the mean of a quantity (whose standard error is σ/N1/2
).
The following R codes can be used to generate the figures of this section. In par￾ticular: Code_9_6.R refers to a single measurement in dry weather, Code_9_7.R to a
single measurement extracted from a set of multiple measurements (the number 40)
and Code_9_8.R to the average of five measurements extracted from the same set. The
data files are available on the book website.
## Code_9_6.R
# Radar Analysis
setwd("C:/RPA/code/bayesian")
# speed of EM signal in air
c <- 2.998e8
# read data: single measurement
data.dry <- readLines("data/single_dry.txt")
strsplit(data.dry,"SIGNAL,") -> tmp
header <- tmp[[1]][[1]]
dat <- tmp[[1]][[2]]
dat <- as.numeric(unlist(strsplit(dat, ",")))
# Elaboration
f1 <- 76e9
f2 <- 77e9
N <- length(dat)
deltaf <- (f2-f1)/(N-1)
f <- f1 + (0:(N-1))*deltaf
BW <- f2-f1
# normalized signal
dat1 <- dat-mean(dat)
si <- dat1/max(dat1)
# Bayesian spectrum analysis
d2m <- mean(si^2)
T <- round((N-1)/2)
T1 <- -T
T2 <- T-1
# choose distance interval
dd <- seq(2,9,0.01)
# uncomment to zoom a distance portion: Figure 9_14
# dd = seq(9,12,0.01)
# uncomment to see test reflector: Figure 9_13
# dd = seq(10,18,0.01)
Np <- length(dd)
# Needed matrices
Hj1 <- matrix(0,N,Np)
Hj2 <- matrix(0,N,Np)
G = matrix(0,2,N)
hj1 <- matrix(0,Np)
hj2 <- matrix(0,Np)
hh <- matrix(0,Np)
PP <- matrix(0,Np)
ss <- matrix(0,Np)
# Decompose in the orthogonal base sin/cos
k <- 1
m <- 2
for (d in dd)
{
w <- 4*pi*d*BW/(N*c)
C <- N/2 + sin(N*w)/(2*sin(w))
S <- N/2 - sin(N*w)/(2*sin(w))Bayesian spectral analysis applied to RADAR target detection 317
# Gjk matrix
for (i in 1:N)
{
l <- -T + 2*T*(i-1)/(N-1)
G[1,i] <- cos(w*l)
G[2,i] <- sin(w*l)
}
# orthogonal functions
Hj1[,k] <- G[1,]/sqrt(C)
Hj2[,k] <- G[2,]/sqrt(S)
# project data on the orthogonal base
h1 <- sum(si*Hj1[,k]);
h2 <- sum(si*Hj2[,k]);
hj1[k] <- h1;
hj2[k] <- h2;
h2m = 0.5*(h1^2+h2^2)
hh[k] = h2m;
# Periodogram: equation (9.30)
P <- (1-m*h2m/(N*d2m))^((m-N)/2)
PP[k] <- P
# Periodogram: equation (9.36)
sigma2 <- N/(N-m-2)*(d2m - m*h2m/N)
#P <- exp(m*h2m/sigma2)
ss[k] <- sigma2
#PP[k] <- P
k <- k+1
}
HHMAX <- max(hh)
SSMIN <- min(ss)
# Posterior probability: equation (9.30)
p <- PP/max(PP)
# Plots
plot(dd,p,t="l",xlab="Distance (m)",
ylab="Posterior Probability",mgp=c(2,0.8,0))
plot(dd,abs(hh),t="l",xlab="Distance(m)",
ylab=expression(paste("<",h^2,">")),mgp=c(2,0.8,0))
# Signal to Noise Ratio
snr <- (2*HHMAX/N)/SSMIN
## Code_9_7.R
setwd("C:/RPA/code/bayesian")
# speed of EM signal in air
c <- 2.998e8
# read data: multiple measurements
M <- readLines("data/multiple_rain.txt")
temp <- list()
vec <- c(list())
for (i in 1:length(M))
{318 Bayesian Inference and Stochastic Processes
one.line <- M[i]
strsplit(one.line,"SIGNAL,") -> tmp
header <- tmp[[1]][[1]]
M.textdata <- paste(header,"SIGNAL,")
dat <- tmp[[1]][[2]]
dat <- as.numeric(unlist(strsplit(dat, ",")))
M.data <- dat
temp$data <- M.data
temp$textdata <- M.textdata
vec[[i]] <- temp
}
# vec[[1]][[1]], vec[[2]][[1]] contain data
# vec[[1]][[2]], vec[[2]][[2]] contain headers
choice <- 40
meas <- vec[[as.integer(choice)]][[1]];
# Elaboration
f1 <- 76e9
f2 <- 77e9
N <- length(meas)
deltaf <- (f2-f1)/(N-1)
f <- f1 + (0:(N-1))*deltaf
BW <- f2-f1
# normalized signal
meas <- meas-mean(meas)
si <- meas/max(meas)
# Bayesian spectrum analysis
d2m <- mean(si^2)
T <- round((N-1)/2)
T1 <- -T
T2 <- T-1
# choose distance interval
dd <- seq(2,9,0.01)
Np <- length(dd)
# Needed matrices
Hj1 <- matrix(0,N,Np)
Hj2 <- matrix(0,N,Np)
G = matrix(0,2,N)
hj1 <- matrix(0,Np)
hj2 <- matrix(0,Np)
hh <- matrix(0,Np)
PP <- matrix(0,Np)
ss <- matrix(0,Np)
# Decompose in the orthogonal base sin/cos
k <- 1
m <- 2
for (d in dd)
{
w <- 4*pi*d*BW/(N*c)
C <- N/2 + sin(N*w)/(2*sin(w))
S <- N/2 - sin(N*w)/(2*sin(w))
# Gjk matrix
for (i in 1:N)
{
l <- -T + 2*T*(i-1)/(N-1)
G[1,i] <- cos(w*l)
G[2,i] <- sin(w*l)
}Bayesian spectral analysis applied to RADAR target detection 319
# orthogonal functions
Hj1[,k] <- G[1,]/sqrt(C)
Hj2[,k] <- G[2,]/sqrt(S)
# project data on the orthogonal base
h1 <- sum(si*Hj1[,k]);
h2 <- sum(si*Hj2[,k]);
hj1[k] <- h1;
hj2[k] <- h2;
h2m = 0.5*(h1^2+h2^2)
hh[k] = h2m;
# Periodogram: equation (9.30)
P <- (1-m*h2m/(N*d2m))^((m-N)/2)
PP[k] <- P
# Periodogram: equation (9.36)
sigma2 <- N/(N-m-2)*(d2m - m*h2m/N)
#P <- exp(m*h2m/sigma2)
ss[k] <- sigma2
#PP[k] <- P
k <- k+1
}
HHMAX <- max(hh)
SSMIN <- min(ss)
# Posterior probability: equation (9.30)
p <- PP/max(PP)
# Plot periodogram
plot(dd,abs(hh),t="l",xlab="Distance
(m)",ylab=expression(paste("<",h^2,">")),mgp=c(2,0.8,0))
# Signal to Noise Ratio
snr <- (2*HHMAX/N)/SSMIN
## Code_9_8.R
setwd("C:/RPA/code/bayesian")
# speed of EM signal in air
c <- 2.998e8
# read data: multiple measurements
M <- readLines("data/multiple_rain.txt")
temp <- list()
vec <- c(list())
for (i in 1:length(M))
{
one.line <- M[i]
strsplit(one.line,"SIGNAL,") -> tmp
header <- tmp[[1]][[1]]
M.textdata <- paste(header,"SIGNAL,")
dat <- tmp[[1]][[2]]
dat <- as.numeric(unlist(strsplit(dat, ",")))
M.data <- dat
temp$data <- M.data
temp$textdata <- M.textdata
vec[[i]] <- temp
}
# vec[[1]][[1]], vec[[2]][[1]]320 Bayesian Inference and Stochastic Processes
# vec[[1]][[2]], vec[[2]][[2]]
# average measurements
Nstart <- 40
Nave <- 5
meas = 0;
for (i in 1:Nave)
meas <- meas + vec[[Nstart+i-1]][[1]]
meas <- meas/Nave
# Elaboration
f1 <- 76e9
f2 <- 77e9
N <- length(meas)
deltaf <- (f2-f1)/(N-1)
f <- f1 + (0:(N-1))*deltaf
BW <- f2-f1
# normalized signal
meas <- meas-mean(meas)
si <- meas/max(meas)
# Bayesian spectrum analysis
d2m <- mean(si^2)
T <- round((N-1)/2)
T1 <- -T
T2 <- T-1
# choose distance interval
dd <- seq(2,9,0.01)
Np <- length(dd)
# Needed matrices
Hj1 <- matrix(0,N,Np)
Hj2 <- matrix(0,N,Np)
G = matrix(0,2,N)
hj1 <- matrix(0,Np)
hj2 <- matrix(0,Np)
hh <- matrix(0,Np)
PP <- matrix(0,Np)
ss <- matrix(0,Np)
# Decompose in the orthogonal base sin/cos
k <- 1
m <- 2
for (d in dd)
{
w <- 4*pi*d*BW/(N*c)
C <- N/2 + sin(N*w)/(2*sin(w))
S <- N/2 - sin(N*w)/(2*sin(w))
# Gjk matrix
for (i in 1:N)
{
l <- -T + 2*T*(i-1)/(N-1)
G[1,i] <- cos(w*l)
G[2,i] <- sin(w*l)
}
# orthogonal functions
Hj1[,k] <- G[1,]/sqrt(C)
Hj2[,k] <- G[2,]/sqrt(S)
# project data on the orthogonal base
h1 <- sum(si*Hj1[,k]);Bayesian analysis of a Poisson process: the waiting-time paradox 321
h2 <- sum(si*Hj2[,k]);
hj1[k] <- h1;
hj2[k] <- h2;
h2m = 0.5*(h1^2+h2^2)
hh[k] = h2m;
# Periodogram: equation (9.30)
P <- (1-m*h2m/(N*d2m))^((m-N)/2)
PP[k] <- P
# Periodogram: equation (9.36)
sigma2 <- N/(N-m-2)*(d2m - m*h2m/N)
P <- exp(m*h2m/sigma2)
ss[k] <- sigma2
PP[k] <- P
k <- k+1
}
HHMAX <- max(hh)
SSMIN <- min(ss)
# Posterior probability: equation (9.30)
p <- PP/max(PP)
# Plot periodogram
plot(dd,abs(hh),t="l",xlab="Distance
(m)",ylab=expression(paste("<",h^2,">")),mgp=c(2,0.8,0))
# Signal to Noise Ratio
snr <- (2*HHMAX/N)/SSMIN
9.3 Bayesian analysis of a Poisson process: the waiting-time
paradox
The waiting-time paradox, also known as the inspection paradox, can be summarized
with a question everyone has at some time asked: why is my bus always late? Apart
from psychological considerations (waiting a time longer than expected is annoying,
and our mind tends to record late bus arrivals more than early bus arrivals) the
question is true, but bad luck should not be invoked to explain it. Actually, Mr Poisson
is responsible of it.
Bus arrival is a Poisson process. Suppose three buses stop at a given station, iden￾tified by letters ‘A’, ‘B’ and ‘C’. Suppose the average inter-arrival times, tA, tB and
tC , or their frequencies (the reciprocals) are known. You arrive at the bus stop at time
T0 to catch bus ‘A’ and, luckily, you don’t miss it. Your chain of reasoning could be as
follows: I did not miss it, therefore I should wait a time greater than zero minutes and
lower than ten; in other words I expect it will arrive in five minutes, on the average.
Nothing more incorrect. On the average you will wait for ten minutes. This apparent
paradox, see for example (Dobrow, 2016), was dealt with in the scientific literature
about one century ago (Masuda and Hiraoka, 2020).
Suppose the average inter-arrival times are: tA = 10 min, tB = 15 min, tC = 20
min, or which is the same, the bus frequencies are λA = 1/10, λB = 1/15, λC = 1/20.
Nice! Your bus is the more frequent. But, a few minutes after arriving at the bus322 Bayesian Inference and Stochastic Processes
stop, you are disappointed to see that the first bus approaching the station is ‘B’.
You ask yourself: why is my bus always late? In the mathematical language: what is
the probability that the first bus arriving at the bus stop is ‘B’, when I am actually
waiting for bus ‘A’?
If we know the values of λA, λB and λC there is no problem at all in computing
the requested probability. With reference to Section 4.5.1, denoting by TX the time
before bus ‘X’ arrives (with X = A, B, C) the probability that the first bus to arrive
is ‘B’ is simply given by:
P(min(TA, TB, TC ) = TB) = λB
λA + λB + λC
which, with the given numbers, is 31%. Not so improbable, don’t you think?
Moreover, the higher frequency of bus ‘A’ does not guarantee at all its early arrival.
The following R code shows that it is perfectly plausible that bus ‘B’ and ‘C’ arrive
before your bus, in spite of their lower frequencies.
## Code_9_9.R
# Arrival times
# Total observation time
t<-120
# Arrival times for bus A
# Frequency of bus A
lambda.A<-1/10
set.seed(456)
# Number of arrivals of bus A
N.A<-rpois(1,lambda.A*t)
unifs<-runif(N.A,0,t)
arrivals.A<-sort(unifs)
plot(arrivals.A,rep(1,length(arrivals.A)),yaxt="n",ylab="",
xlab="Bus arrival times",ylim=c(0,4),xlim=c(0,120),pch="A",cex=0.9)
# Arrival times for bus B
# Frequency of bus B
lambda.B<-1/15
set.seed(789)
# Number of arrivals of bus B
N.B<-rpois(1,lambda.B*t)
unifs<-runif(N.B,0,t)
arrivals.B<-sort(unifs)
points(arrivals.B,rep(2,length(arrivals.B)),pch="B",cex=0.9)
# Arrival times for bus C
# Frequency of bus C
lambda.C<-1/20
set.seed(123)
# Number of arrivals of bus C
N.C<-rpois(1,lambda.C*t)
unifs<-runif(N.C,0,t)
arrivals.C<-sort(unifs)
points(arrivals.C,rep(3,length(arrivals.C)),pch="C",cex=0.9)
Every Poisson process (A, B, C) is simulated by generating a number of arrivals
(N.A, N.B, N.C) by the function rpois, then generating N.A, N.B and N.C arrivals
having uniform distribution. The Poisson process results in sorting the arrivals in
ascending order.Bayesian analysis of a Poisson process: the waiting-time paradox 323
Figure 9.17 shows a possible distribution of arrivals. As you see, bad luck is not
responsible of the two ‘B’ arrivals followed by a ‘C’ and, eventually, by an ‘A’ bus. It’s
randomness!
Of course, the explanation of the paradox is that the average arrival frequency is,
indeed, the average of a random quantity. If bus ‘A’ arrives exactly every ten minutes,
your average waiting time is five minutes. To say that a bus arrives ‘on the average’
every 10 minutes means that you will wait for it 10 minutes, on the average.
A A A A A A A
0 20 40 60 80 100 120
bus arrival times
BB B B BB B BB
C C C C C
Fig. 9.17 Simulation of the arrival times of three buses with frequencies λA = 1/10,
λB = 1/15, λC = 1/20.
Now, what can I say if I don’t know the bus frequencies? Well, I can spend a couple
of hours recording the arrival times and I could be interested in estimating either (λA,
λB and λC ) and the probability that bus ‘B’ is the first to arrive.
Consider first a single bus line, for instance line ‘A’. From Chapter 4, we know
that the inter-arrival times {t1, .., tn} are exponentially distributed. In the Bayesian
framework, this means that the likelihood function (or sampling distribution) is given
by:
p({t1, .., tn}|λA) ∝ (λT)
n
exp(−λT)
where T =
Pn
k=1 tk. We can choose an improper prior probability p(λ), for example
an uniform distribution for λ, or a Jeffreys prior:
p(λ) ∝
1
λ324 Bayesian Inference and Stochastic Processes
corresponding to a non-informative prior suitable when observing times between events
(Insua et al., 2012). Both those choices are ‘conjugate’ to the likelihood and therefore
they bring to similar posterior distributions for λ, for Gam(n+1, T) and for Gam(n, T).
We choose the last. Our aim is to estimate the (posterior) probability distributions
of λA, λB and λC given the observations and, as a last step, to compute the probability
that a bus ‘B’ arrives first. If the waiting times of the three bus lines are independent,
with the above choice for prior and likelihood we have a closed-form expression for
those probabilities:
p(λA|t
A
1
..tA
k
) = Gam(nA, TA), nA = k
p(λB|t
B
1
..tB
l
) = Gam(nB, TB), nB = l
p(λC |t
C
1
..tC
m) = Gam(nC , TC ), nC = m
The knowledge of analytical expression of the distribution, also allows the computation
of the mean value and the variance of the expected λ’s:
E [λA] = NA
TA
Var [λA] = NA
T
2
A
E [λB] = NB
TB
Var [λB] = NB
T
2
B
E [λC ] = NC
TC
Var [λC ] = NC
T
2
C
In order to compute
p

λB
λA + λB + λC

we can resort to the Monte Carlo method. We extract N samples from each of the
three distributions above, obtaining:
λA1
, λA2
, ..., λAN
λB1
, λB2
, ..., λBN
λC1
, λC2
, ..., λCN
Now, for the assumed independence, the sequence (λA1
, λB1
, λC1
)...(λAN , λBN , λCN )
consists of N independent samples from the joint distribution for λAλBλC . As a con￾sequence of the law of large numbers, this last empirical distribution approximates the
‘true’ distribution, and we can estimate the required probability P(min(TA, TB, TC ) =
TB) simply by making an average. The following code shows how to do it in R.
## Code_9_10.R
# Arrival times
# Total observation time
t <- 120Bayesian analysis of a Poisson process: the waiting-time paradox 325
#Arrival times for bus A
lambda<-1/10
set.seed(123)
N<-rpois(1,lambda*t)
unifs<-runif(N,0,t)
(arrivals.A<-sort(unifs))
#Arrival times for bus B
lambda<-1/15
set.seed(987)
N<-rpois(1,lambda*t)
unifs<-runif(N,0,t)
(arrivals.B<-sort(unifs))
#Arrival times for bus C
lambda<-1/20
set.seed(543)
N<-rpois(1,lambda*t)
unifs<-runif(N,0,t)
arrivals.C<-sort(unifs)
N1 <- length(arrivals.A)
T1 <- arrivals.A[N1]
N2 <- length(arrivals.B)
T2 <- arrivals.B[N2]
N3 <- length(arrivals.C)
T3 <- arrivals.C[N3]
#Jeffreys (improper) Prior = 1/lambda
# Posterior = Gamma(n,T)
# yi = lambda_i
set.seed(111)
lam1 <- rgamma(50000,N1,T1)
set.seed(222)
lam2 <- rgamma(50000,N2,T2)
set.seed(333)
lam3 <- rgamma(50000,N3,T3)
hist(lam1,freq=FALSE,main="",xlab=expression(lambda[A]))
lines(seq(0,0.25,0.001),dgamma(seq(0,0.25,0.001),N1,T1))
hist(lam2,freq=FALSE,main="",xlab=expression(lambda[B]))
lines(seq(0,0.25,0.001),dgamma(seq(0,0.25,0.001),N2,T2))
hist(lam3,freq=FALSE,main="",xlab=expression(lambda[C]),ylim=c(0,15))
lines(seq(0,0.25,0.001),dgamma(seq(0,0.25,0.001),N3,T3))
c(N1/T1,mean(lam1))
c(N2/T2,mean(lam2))
c(N3/T3,mean(lam3))
p <- lam2/(lam1+lam2+lam3)
(h <- hist(p,freq=FALSE,main="",
xlab=expression(p(lambda[B]/paste(Sigma,lambda[i])))))
mean(p)
In the case implemented in the above code, the mode of the average probability
distribution that bus B is the first to arrive at the bus stop is about 28%. The mode,
which identifies the maximum value of the posterior probability, practically coincides
with the mean value due to the shape of the probability distribution. Figures 9.18
show the posterior probability distributions of λA, λB and λC (superimposed on the326 Bayesian Inference and Stochastic Processes
empirical distributions) and the probability distribution of the required parameter
λB/(λA + λB + λC ). Note that the focus of Bayesian analysis is on distributions, not
just on point estimates.
lA
density
0.00 0.05 0.10 0.15 0.20 0.25
0 5 10 15
lB
density
0.00 0.05 0.10 0.15 0.20
0 5 10 15
lC
density
0.00 0.05 0.10 0.15 0.20 0.25
0 5 10 15
p (lB Sli)
density
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0 1 2 3 4
Fig. 9.18 From top-left to bottom-right, posterior probability distributions of λA, λB, λC
and λB/(λA + λB + λC ).
The discussion above, as the Monte Carlo method itself, is grounded in the as￾sumption that the law of large numbers (and the central limit theorem) is valid, a very
common situation actually. We will consider a curious example where that is not true
in the following section.
9.4 Bayesian analysis applied to a lighthouse
This example is taken and reworked from Gull (1988) in ‘Bayesian inductive inference
and maximum entropy’, in Maximum entropy and Bayesian methods in science and
engineering, Kluwer.
The example is remarkable for at least three reasons. (1) It shows how the Bayesian
approach is a ‘paradigm’ that includes techniques normally used in a more or less
uncritical way. (2) It shows how the mean is not always the best estimate of a random
quantity, and how the solution exists even when the central limit theorem does not
hold. (3) Finally, it shows how the information present in the data always wins in the
end, if it is sufficient, and how the influence of the first choice of the a priori probability
becomes negligible as the experimental data grows.Bayesian analysis applied to a lighthouse 327
9.4.1 Description
A lighthouse is in a certain position on a straight stretch of coastline, at a position X0
along the beach, measured from an arbitrarily chosen origin, and at a distance of Y0
from the sea. The lighthouse is in constant rotation and emits short collimated flashes,
at random time intervals (and therefore θ angles). Photo-detectors on the beach record
the flash, but not the θ angle from which the beam is coming.
The experimental data is the set of {xk} positions of the photo-detectors that have
been activated by a flash.
Suppose, for simplicity (just so as not to have to infer two parameters, but only
one) we know the distance Y0. What is the X0 position? How can we estimate it from
the data?
X0
Y0
q
sea
land
photo detectors
x1 x2 xk
X0 = distance along the coast
Y0= distance from the sea
Fig. 9.19 Schematic of the problem.
9.4.2 Solution
Each xk corresponds to an azimuth value θk. For the light beam to be visible, the
angle must be between −π/2 and π/2 (extremes not included), i.e.:
θk ∈

−
π
2
,
π
2

Obviously, if the angle is exactly ±π/2 the light beam does not hit the coast. Realis￾tically we can assign a uniform probability density to the azimuth θk, that is to the
k-th datum. If we denote by X the unknown position (while Y0 is known):
p(θk|X, Y0) = 1
π
The value 1
π
comes from the integration of the uniform probability distribution:
Z +∞
−∞
p(θ)dθ = 1
In this case, the integral is definite:
p(θk|X, Y0) = Z π/2
−π/2
p(θ)dθ =
1
π/2 − (−π/2) =
1
π
Trigonometric considerations, when X0 is known, allow us to say that:328 Bayesian Inference and Stochastic Processes
Y0tan(θk) = xk − X0
By knowing X0, Y0 and xk, the tan(θk) is obtained and the angle is obtained with the
arc tangent function.
The position of the photo-detector activated by the light beam is therefore:
xk = X0 + Y0tan(θk)
and the tangent of the angle is:
tan(θk) = xk − X0
Y0
therefore the angle is:
θk = atan 
xk − X0
Y0

We use the variable transformation from θ to x to get the probability density of xk. If
x = x(θ), for dx and dθ infinitesimal:
p(x)dx = p(θ)dθ
therefore:
p(x) = p(θ)




dθ
dx




(the reason for the absolute value is that it must be a length ratio, i.e. always positive).
The derivative dθ/dx is given by the derivative of the arc tangent with respect to x
which is:
dθ
dx =
Y0
[Y
2
0 + (xk − X0)
2]
while p(θ) = 1/π as shown above.
Finally:
p(xk|X0, Y0) = Y0
π [Y
2
0 + (xk − X0)
2]
In summary, if we know the (X0, Y0) position of the lighthouse, the probability of
recording a flash at the xk position has a Cauchy distribution. The Cauchy distribution
is explained in detail in the next chapter about probability distributions. The Cauchy
distribution is often used in statistics as an example of a ‘pathological’ distribution
since both its expected value and its variance are undefined. The Cauchy distribution
does not have finite moments of order greater than or equal to one.
Note: If we did not know the distance Y0 of the lighthouse from the sea, we would
have to estimate two parameters, a somewhat more complex problem (the solution of
which risks overshadowing what we are interested in showing here).Bayesian analysis applied to a lighthouse 329
To estimate (infer) the X parameter (the position of the light beam), we need to
estimate the posterior probability of X, given Y0 and the records {xk}:
p(X|xk, Y0)
From Bayes’ theorem:
p(X|{xk}, Y0) = p({xk}|X, Y 0)p(X|Y0)
p({xk}|Y0)
The term in the denominator does not depend on the parameter sought. The first
term of the numerator is what is called the ‘likelihood’ of the data, the second is the a
priori probability of X. When we have no idea what a priori distribution a variable has,
it is reasonable to take it uniform in a sensible range [Xmin, Xmax] and zero outside:
p(X|Y0) = p(X) = 
α, X ∈ [Xmin, Xmax]
0, X /∈ [Xmin, Xmax]
(9.38)
If the data xk are independent, as is reasonable to assume, the probability p({xk}|X, Y 0)
is the product of the probabilities of the single events xk, therefore:
p({xk}|X, Y0) = Y
N
k=1
p(xk|X, Y0)
Take the logarithm of the posterior probability p({xk}|X, Y 0):
L = log(p({xk}|X, Y0) = β −
X
N
k=1
log(Y
2
0 + (xk − X)
2
)
where β includes everything that does not depend on the X parameter. The esti￾mate of the position X0 is obtained by looking for the maximum of the a posteriori
distribution, that is, theoretically looking for the value of X which is a solution of:
dL
dX = 2X
N
k=1
xk − X
Y
2
0 + (xk − X)
2
= 0 (9.39)
9.4.3 Numerical procedure
The explicit solution of (9.39) is not analytically feasible. Instead of solving it numeri￾cally, it is more instructive to see how the posterior probability exp(L) behaves as the
number of detections {xk} changes. This is what the following R code does, where we
assume Y0 = 1 km and the ‘true’ value of X0 2 km. The code generates N angles θk
and from these it calculates xk, since the true value of X0 is known.
## Code_9_11.R
## Where is the light ?
# distance from sea
Y0 <- 1
# distance along the coast330 Bayesian Inference and Stochastic Processes
X0 <- 2
# possible values of X (positions of photo-detectors)
dx <- 0.05
X <- seq(-5,5,dx)
Nx <- length(X)
######################################################################
# number of measurements
# to change to observe the effect on the distribution a posteriori
N <- 10
######################################################################
# tetak <- runif(k,-pi/2,pi/2)
# instead of taking theta between -pi/2 and pi/2
# theta is selected such to determine "possible" x
# included between -x_max and x_max
x.max <- 50
# Here the angle is obtained using the arctan function, where x.max in an angle
tetak.max <- atan(x.max)
tetak <- runif(N,-tetak.max,tetak.max)
tetak
# compute the positions of the detectors activated by flash light
xk <- X0+Y0*tan(tetak)
# What is the distribution of the positions ?
hist(xk,main="")
L <- rep(0,Nx)
for (i in 1:Nx){
lk <- log(Y0^2+(xk-X[i])^2)
L[i] <- sum(lk)
}
hist(lk)
# posteriori probability
post <- exp(-L)
plot(X,dx*post/sum(post),t="l",ylab="p(X|x,Y0)")
abline(v=2,col="blue",lty=2)
abline(v=mean(xk),col="red")
9.4.4 Results
This is what happens (typically, the calculation is stochastic ...) as the number of
detections changes.
The true value (dashed line) and the average value of xk (solid line) are superim￾posed on the probability density curve. Due to the choice of a uniform distribution for
θ (which is reflected in a Cauchy distribution for likelihood), and a uniform a priori
probability, with little data the maximum posterior probability rarely hits the real
position X0.
For N = 100 detections, the maximum a posteriori probability starts hitting the
true value (Fig. 9.22) almost always, but the average value of xk can be very far from
it.
At first sight this thing is surprising, because we are used to attribute almost
‘magical’ properties to the average value, by virtue of the central limit theorem, which
asserts that for a sample {x1...xn}, collected from a distribution with mean µ andBayesian analysis applied to a lighthouse 331
−4 −2 0 2 4
0.0000 0.0010
X
p(X|x,Y0)
Fig. 9.20 N = 4.
−4 −2 0 2 4
0.0000 0.0015
X
p (X|x,Y0)
−4 −2 0 2 4
0.0000 0.0010 0.0020
X
p (X|x,Y0)
−4 −2 0 2 4
0.0000 0.0015
X
p (X|x,Y0)
−4 −2 0 2 4
0.0000 0.0010 0.0020
X
p (X|x,Y0)
Fig. 9.21 N = 10.
−4 −2 0 2 4
0.000 0.003 0.006
X
p(X|x,Y0)
−4 −2 0 2 4
0.000 0.003 0.006
X
p(X|x,Y0)
Fig. 9.22 N = 100.
variance σ
2
the distribution of the mean value ¯x tends to a normal distribution with
mean µ and variance σ
2/n, for n → ∞.
The problem here is that the Cauchy distribution violates the validity conditions of
this theorem, because it has large tails and as such gives a moment of order 2 of infinite
value. From this example we can observe that although the central limit theorem is
not valid, and therefore the average is not a sensible estimate of the position of the332 Bayesian Inference and Stochastic Processes
lighthouse, we can still calculate the a posteriori distribution, and the maximum of this
gives us the correct position of the lighthouse. We note, incidentally, that this procedure
coincides with the search for the maximum likelihood! In addition to shedding light
on the latter, whose motivation is often unclear in any other way, it is evident that
nothing prevents us from using information on the position of the lighthouse, if we
have it, and using it by imposing a different form of a priori probability, and making
the calculation of the true position correct and faster.
9.5 Exercises
Exercise 9.1 If H1, H2, ..., Hn are mutually exclusive hypotheses, the Bayes theorem asserts
that the probability of a particular hypothesis Hi after the verification of an event A is given
by:
P {Hi|A} =
P {Hi} P {A|Hi}
P {A}
where:
P {A} =
Xn
i=1
P {Hi} P {Hi|A}
Demonstrate that the Bayes theorem is a corollary of the multiplication probability the￾orem, stating that the probability (or, also, the probability density) of an event C being the
product of two events A and B is given by the product between the probability of A and that
of B conditioned on A.
Hint: Use the multiplication theorem to express the probability of every event AHi.
Exercise 9.2 In an archery club two archers A and B shoot an arrow each at the same
target. Based on the previous performances of the two archers, we know that the probabilities
of centring the target are 0.8 for archer A and 0.5 for archer B.
If only one arrow is stuck in the target board, what is the probability that it comes from
archer B?
Hint: enumerate all possible mutually-exclusive hypotheses, for instance ‘both archers hit
the target’ etc.
Exercise 9.3 A typical application of the Bayes formula is in medical diagnostics. Suppose
a diagnostic test T for lycantrophe, executed on a blood sample, is 99% effective. This means
that if you actually transform into a wolf on full moon nights, test T on your blood has 99%
chance to result positive: P {T|HL} = 0.99, where HL denotes the hypothesis ‘the tested
individual is a lycanthrope’.
On the other side, suppose that the probability of a false positive test, P

T|HL
	
is 10%:
you are not a werewolf but the test is positive anyway. Knowing that lycanthropy is (luckily!)
a rare condition, let say that its incidence in the population is 1%, if your brother Jack results
positive to the test T estimate the probability that you better stay away from him especially
on full moon nights.
Exercise 9.4 With reference to Exercise 9.3, what is the probability that Jack is actually a
lycanthrope, if the test T on his blood gave a negative result?
Exercise 9.5 A bag contains an unknown number of dice, which you know can be of three
types:Exercises 333
C: conventional, with faces enumerated from 1 to 6
E: even, with faces enumerated with even numbers from 2 to 12
R: repeated, two faces enumerated with 1, two with 2, two with 3
You blindly extract a die from the bag, and throw it four times obtaining the sequence 2 2 1 3.
Assuming a uniform prior for the three different hypotheses (P {HC } = P {HE} = P {HR} =
1/3), and knowing the likelihood L (the probability of a given result):
(1) Fill a table like the following describing how the prior is updated after each result of the
sequence or, which is the same, reporting the posterior probability after each outcome.
(2) Compute the posterior probability that the die is ‘C’ after the full sequence.
(3) Compute the posterior probability that the die is ‘E’ after the full sequence.
The table could be something like:
outcome = “2” outcome = “2” outcome = “1’ outcome = “3’
Hyp Prior lik L Post lik L Post lik L Post lik L Post
HC 1/3 1/6
HE 1/3 1/6
HR 1/3 1/3
Hint: the likelihood of a particular outcome is the probability of that result, given the die, and
of course it is zero if that outcome is impossible.
Exercise 9.6 How do the results of Exercise 9.5 change if you use a different prior? For
example, suppose you are almost certain that dice of the ‘E’ type are not present in the bag,
and someone has told you that there are presumably twice the number of conventional dice
with respect to repeated ones.
Exercise 9.7 With reference to the bus waiting-time of Section 9.3, modify the R code to
compute:
(1) The probability p(λB > λA) for the given simulated data
(2) The quantiles of the empirical probability distribution of λB/
Pλi
Exercise 9.8 With reference to Exercise 9.7 how do things change assuming a uniform prior
probability instead of 1/λ?
Exercise 9.9 The JAGS language is a ‘natural’ extension of R for MCMC calculations. It
is therefore useful to be acquainted with it. With reference to Section B.0.2 of Appendix B
write an R code to compare normally-distributed random data generated in R with a sample
extracted in JAGS, for example comparing the two histograms.
Note: despite the identical name, dnorm in JAGS is defined in terms of the precision instead
of the standard deviation.
Exercise 9.10 We treated the problem of a fair coin at the beginning of this chapter. Suppose
you want to decide about the fairness of a coin by an experiment consisting of counting the334 Bayesian Inference and Stochastic Processes
number of heads obtained in n flips. You know that the likelihood P(θ|h), the probability of
obtaining h heads in n flips is the Binomial(n, h) given the probability θ of a head:
P(θ|h) = 
n
h

θ
h
(1 − θ)
n−h
Your prior expresses the information you have about the coin. If you have no reason to
think the coin is fair, you can assume a uniform prior or, which is the same, a Beta(1, 1)
distribution. If you think the coin is fair, you assign a more ‘informative’ prior, such as
Beta(k, k) with k>1. The greater is k, the narrower is the prior distribution around the value
θ = 0.5.
Obtain the posterior distribution as a function of k and write down an R code to compute
and plot likelihood, prior and posterior for n = 100 and h = 40. Maintaining the same ratio
h/n, comment on how things change if n = 10 or n = 1000.
Exercise 9.11 Repeat Exercise 9.10 using JAGS. The JAGS model to be used in rjags is
something like
H ~ dbin(theta,N)
theta ~ dbeta(k,k)
to be inserted into a textConnection command or to be read from a text file. Here H is
the number of heads, whose distribution identifies the likelihood, and theta is the searched
probability.
Experiment by changing the number of burn-in steps (using the rjags function update)
and the number of extracted samples (via the coda.samples function).
Plot the posterior distribution and compare it to the analytical one obtained in the pre￾vious exercise.
Exercise 9.12 This problem concerns with a probability puzzle based on an (in)famous
television game, known as Monty Hall. There are three doors A, B, C. Behind one door is a
car, behind the others, goats. The contestant picks a door, say A, which remains closed. The
host, who knows what’s behind the doors, opens another one of the remaining doors, say C,
where he knows there is a goat. The host asks the contestant ‘Do you want to maintain your
first choice A or change your mind and pick the door B?’. The answer is not indifferent, even
though at a first sight it seems so. Prove via Bayes theorem that it is advantageous for the
contestant to switch his choice. Try also to simulate the game.10
Genetic algorithms: an
evolutionary-based global random
search
We are born by accident into a purely random universe.
Our lives are determined by entirely fortuitous combinations of genes.
Whatever happens happens by chance.
Robert Silverberg, The Stochastic Man
10.1 Introduction
In 1975 John H. Holland, professor of the University of Michigan, published the first
edition of his book Adaptation in natural and artificial systems, summarising the result
of his research activity on the application of the biological − namely, genetic − mech￾anisms to adaptive processes of different nature. As is well known, adaptive processes
play a role in several fields, including those of artificial intelligence and computational
mathematics. The book was essentially based on the following questions:
How does evolution produce increasingly fit organisms in environments which are highly
uncertain for individual organisms? [. . . ] How does an organism use its experience to modify
its behaviour in beneficial ways (i.e. how does it learn or ‘adapt under sensory guidance’)?
And from the above questions, he asked himself:
How can computers be programmed so that problem-solving capabilities are built up by
specifying ‘what is to be done’ rather than ‘how to do it’?
His work was destined to become a milestone in this topic and to stimulate the
development of one of the most smart and robust optimization techniques of the 20th
century, the genetic algorithms (GA). GAs provide a stochastic method for the solu￾tion of problems of maximum/minimum search and of global optimization (Goldberg,
1989) based on the mechanics of natural selection and mimicking natural genetics. GAs
belong to a more general class of heuristic methods inspired by biological evolutionism,
known as evolutionary algorithms (EA) (Michalewicz, 1996). A common charac￾teristics of EAs, as shown in the following, is the codification method: a tentative￾solution for the problem is obtained in terms of a data structure that is treated as a
chromosome and manipulated by means of genetic operators (e.g. crossover, mutation,
inversion etc.) in order to reach the actual solution.
Leaving aside for the moment the genetic interpretation, a GA is a search model
operating in a n-dimensional hyper-space where, after generating an initial distribution336 Genetic algorithms: an evolutionary-based global random search
of hyperpoints, a solution is searched for by applying suitable operators to such a set
of points to generate different configurations in the search space.
Turning back to the genetic interpretation, a GA simulates the Darwinian evolu￾tion process: it transforms a set of mathematical objects (chromosomes), each having
associated a fitness property (measuring its ‘distance’ from the true solution), into a
new set through a ‘survival of the fittest’ strategy.
10.2 Terminology and basics of GA
The solution of the problem must be representable in terms of a set of parameters
(genes) whose union constitutes the chromosome. An objective function, called the
fitness function, must be identified. The fitness function measures the goodness of a
tentative-solution.
The GA begins with the generation of an initial population of tentative solutions,
i.e. with a set of chromosomes. Such an initial population can be randomly generated
(that is the typical case) or it can be built starting from a given number of individuals,
whose closeness to the actual solution is postulated or known. It should be noted
that usually the terms individual and chromosome have the same meaning. In
other words, the GA individual is haploid. Models employing diploid individuals, i.e.
possessing a couple of chromosomes, have been proposed too, but their use in problem
solving is marginal.
Before discussing the function of the operators used in GA, a short definition of
the biological terms that we will use in the following is in order.
10.2.1 Biological terms
Chromosome sequence of nucleotides (DNA) carrying all the information needed for
cell growth, survival and reproduction.
Gene chromosome segment having functional autonomy. Through the synthesis of
RNA, a gene controls protein synthesis. In other words, a gene is a functional
unit regulating a series of operations in a living organism.
Genome the overall gene pool of an organism.
Haploid the condition of cells having a single series of chromosomes.
Diploid the condition of cells having a double series of chromosomes.
Alleles a couple representing the same gene (in diploid organisms) or a gene and its
mutants (in haploid organisms).
Genotype the individuals seen as a gene ‘carrier’, i.e. as carrier of a given biological
potentiality.
Phenotype the individual seen under the profile of his ‘visible’ properties.
Crossover genetic phenomenon consisting of the exchange of segments between chro￾matids (newly created chromosomes). After crossover, the final chromosome al￾ternates segments coming from the original ones.
Mutation the modification of the nucleotide sequence in a gene.
Asexual reproduction a type of reproduction by which offspring arise from a single
organism, and inherit the genes of that parent onlyTerminology and basics of GA 337
Sexual reproduction it happens through the meeting and fusion of two different
reproductive cells (called gametes).
Inversion a chromosome segment is detached and re-attached in an inverted position.
Translocation a chromosome segment is detached and attached to a different chro￾mosome.
10.2.2 Representation of the tentative solutions
The tentative-solution for a given problem usually consists of a certain number of
real parameters. For example, in a problem where a function f of four real variables
(X, Y, Z, T) is to be maximized, a tentative solution consists of a sequence of four real
numbers. In order to conceptually simplify the treatment of the GA, such a sequence
will be represented by four binary strings of fixed length. Each string represents a
gene, whilst the binary string consisting of the concatenation of the four parameters
is the chromosome.
The population we were talking about at the beginning of this section is therefore
a set of binary strings. To obtain a satisfying solution, i.e. a string (Xs, Ys, Zs, Ts)
maximizing the f function, requires the application of suitable ‘genetic’ operators
to the individuals composing the population. Such operators can involve couples or
single individuals. Before describing in detail the most commonly used operators, some
specifications are in order concerning the fitness function.
In the simple case introduced above, the fitness function coincides with the function
f. In more complex, realistic cases, the fitness function associates a number to a
particular tentative solution, that number being increasingly higher as the guessed
solution approaches the actual one. That association is not necessarily a function
of the unknown variables. Indeed, GAs can be applied also to problems where the
variables involved are not numerical.
10.2.3 Genetic operators
In this section the two most common operators are described: the crossover operator
and mutation. Those operators always intervene in the reproduction process, that is
the actual ‘engine’ of the search algorithm. The role of operators will be clear in the
description of the simple GA, described in the following sections.
Crossover. The crossover operator involves two chromosomes identifying different
individuals (i.e. different tentative solutions). In the most common case, where the
genome consists of a single chromosome, the function of the crossover operator is to
exchange a chromosome section between two individuals.
Let us consider the situation depicted in Fig. 10.1.
The upper chromosomes (PARENTS) give rise to the lower ones (OFFSPRING)
through the cross exchange of genetic material.
In that simple case, the crossover involves the following steps:
1. a cut point is established, identifying where to cut and reassemble the chromosome
couple (X-point in Fig. 10.1);
2. the C1A and C2A sections are inherited without any alteration by the offspring;338 Genetic algorithms: an evolutionary-based global random search
C1A
C1A
C2A
C2A
C2B
C2B
X-point
X-point
X-point
X-point
C1B
PARENTS
OFFSPRING
C1B
Fig. 10.1 Crossover between two individuals.
3. the C1B section is chained to the C2A, while C2B attaches to the C1A section.
The choice of the X-point is usually random.
If the chromosome C1 is the binary string 10011001, C2 is 10111100, and the
X-point corresponds to the third bit, we obtain:
Parents: 10011001 10111100
Offspring: 10011100 10111001
The crossover operator has a key role in the search for the solution of a problem.
GA’s power in exploring the solution space largely depends on that operator, which
allows us to obtain different individuals (tentative solutions) from the initial genetic
material.
Mutation. The crossover operator fully exploits the information contained in the
initial genetic material, but it is not able to ‘create’ new genetic material, i.e. material
not belonging to one of the initial parents. We can understand this by means of a
simple example. Suppose the parents are the following two 4-bit strings:
1100 1010
Starting from those parents, the following offspring couples can be generated:
1010 1100 if the X-point is between the first and the second bit
1110 1000 if the X-point is between the second and the third bit
1100 1010 if the X-point is between the third and the fourth bit
It should be noted that in the above case new individuals are obtained only when
the cut point is in the middle of the chromosomes. Moreover, it should also be clearSimple genetic algorithm 339
that, for example, a 1 in the fourth position cannot be generated, as well as the creation
of a chromosome like 0010 is not possible.
These limitations are removed by the mutation operator, which can simply consist
of the random complement of a bit in the chromosome, as shown in Fig. 10.2.
CHROMOSOME
1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0
CHROMOSOME
M-point
Fig. 10.2 Mutation in a chromosome.
As for the X-point the selection of the mutation point (M-point) is random.
10.3 Simple genetic algorithm
This section describes the most elemental genetic algorithm, known as simple genetic
algorithm (SGA) . The SGA consists of a given number of reproduction cycles applied
to a population that remains constant in time. The reproduction process consists of:
1. Selection of a couple of individuals;
2. Mating (through the application of the crossover operator), optionally followed
by mutation;
3. Computation of the fitness function for the offspring.
Steps 1 to 3 are executed on every couple extracted from the population, until
all individuals are considered. Disregarding (for the moment) the modality used for
selection and mating, the above sequence of steps is repeated until a predefined termi￾nation condition happens (e.g. an individual reaches a target fitness value, the allowed
maximum number of generations is exceeded, the average fitness of the population
falls below a given limit).
Figure 10.3 shows the flow diagram of SGA.
10.3.1 GA at work: selection and reproduction
One of the key points of the algorithm is the choice of the procedure used for selecting
the individuals to be ‘mated’. Such a procedure can be conducted in several ways,
always bearing in mind that the purpose is to select the most suited (fit) individuals,
that will be subsequently employed several times in the reproduction process to the
detriment of the least fit ones, so as to propagate useful ‘genetic material’ to the future
generations. A simple rank-based approach consists of evaluating the fitness of every
member of the initial population and to sort it based on such a characteristic. Every
‘potential’ parent is associated a mating probability based on its rank.340 Genetic algorithms: an evolutionary-based global random search
START
GENERATE
INITIAL
POPULATION
SELECT
FITTEST
INDIVIDUALS
MAKE NEW POPULATION
BY “CROSSOVER” AND
“MUTATION”
STOP
YES NO
TERMINATION
?
Fig. 10.3 simple genetic algorithm.
Another selection method, largely used in GA practice, is the so-called roulette
wheel selection (RWS). In the RWS approach every individual ‘occupies’ a given num￾ber of roulette slots proportionally to its selection probability. If the initial population
consists in four individuals A, B, C and D, respectively having fitness:fA = 50, fB = 30,
fC = 15, fD = 5, their selection probabilities are: pA = 0.5, pB = 0.3, pC = 0.15 and
pD = 0.05.
In the RWS, A is considered to occupy 50% of the slots, B 30% of them, and
so on. At each roulette spin, A has 50% probability of being selected, B 30% and
so on. At each spin any selected individual is stored to be successively mated, up
to the achievement of the number of couple needed for generating a new population
having the same numerosity of the parent population (eight times, in the considered
example). The storing place is called the ‘mating pool’. By excluding the possibility of
an individual to mate with itself, in the process of selecting N couples, an individual
having probability p will be chosen p · N on average. Coming back to the example,
the A individual will be selected four times on average for the insertion in the mating
pool. Likewise, B will be selected two times, C and D only once on average.
Note that neither religion nor moral commitments affect GAs individuals: they are
allowed to mate with more than one partner, in the interests of the species! :)
Once the chromosomes to be reproduced have been identified, a couple of parents
are randomly formed. The crossover operator is therefore applied with (usually high)
probability pX, as described in Section 10.2.3. In other words, the most part of the
couples exchange genetic material between them, but nevertheless a non-zero number
of individuals are transmitted unchanged to the successive generation. That couldSimple genetic algorithm 341
appear not too smart at first sight, as a chromosome not being a solution of the problem
continues to be a wrong solution of the problem. But, if its fitness is sufficiently high to
be placed in the mating pool, there is a chance that it contains useful genetic material,
e.g. in the effort of finding five unknown parameters (genes) it could contain one correct
parameter, the other four being completely wrong.
After mating (crossover), the individuals of the offspring generation are subjected
to mutation with (usually low) probability pM. The choice of pM is dictated by a
compromise: if pM = 0, no new material is introduced, if pM = 1 the GA transforms
in a random search algorithm.
Let us see how the process works using a simple example.
10.3.2 An optimization problem: the Prof. Koza fast-food chain
This example is borrowed from (Koza, 1996). It is very meaningful, although very
simple, and it should be useful for giving a clear idea of how GAs work.
A person is suddenly involved in the management of a chain of four fast-foods,
without any previous experience. He is faced with the following binary choices, for
maximizing the gain:
• Hamburger price (P): should it be high or low?
• Which beverage (B): coke or beer?
• Service (S): should it be slow and careful, or fast but hasty?
The manager only owns the weekly recording of the gains of the four fast-foods.
The goal is to find the combination of choices bringing the highest total gain.
Implementation
The problem variables are three binary quantities: P, B and S. The chromosome con￾sists of a 3-bit string. The search space consists of 23 = 8 possible commercial strate￾gies.
Representation
Each bit corresponds to a choice. The first bit is associated with the P variable, the
second with B, the third with S. The following table shows the eight possible choices
for a particular restaurant.
Price Beverage Service Representation (PBS)
high beer careful 0 0 0
high beer fast 0 0 1
high coke careful 0 1 0
high coke fast 0 1 1
low beer careful 1 0 0
low beer fast 1 0 1
low coke careful 1 1 0
low coke fast 1 1 1
Table 10.1 Search space.342 Genetic algorithms: an evolutionary-based global random search
The chain manager has no idea which the most important variable is, and he/she
does not possess any information concerning the ‘gradients’, i.e. how any variable influ￾ences the profit. Moreover, the manager cannot be sure that a step-by-step procedure
is effective in maximizing the gain (e.g. optimize P, then optimize B and readjust P,
etc.), because the three variables can be interrelated and the relation is not necessarily
a linear one. What makes things more difficult is that the reason for the variation
in the number of customer in a restaurant is unknown, and it cannot be known be￾cause clients do not motivate their choices and no warranty exists that their tastes are
constant in time.
A possible strategy is to try, for a week, a different random strategy in the four
fast-foods. This choice is such that the expected gain is the average available in the
search space: by favouring the diversity, the probability of obtaining an ‘average’ profit
is maximized. In the meanwhile the information that the manager can obtain in a week
is maximized.
The above is exactly how GAs work. Indeed, a GA seeks for a solution in the
search space by evaluating how much a number of individuals are ‘compatible’ with
the environment. If no a priori information is available, such individuals are randomly
generated to ensure the presence of the necessary diversity in the population. At every
generation (iteration, as we should call it in conventional algorithms) the individuals
are evaluated through the fitness function. In the example at hand, such a function
measures the profit associated to a particular commercial strategy.
Suppose the initial population is that indicated in Table 10.2.
Fast-food Price Beverage Service Chromosome
1 high coke fast 0 1 1
2 high beer fast 0 0 1
3 low coke careful 1 1 0
4 high coke careful 0 1 0
Table 10.2 Initial population.
We arbitrarily assume the decimal value corresponding to binary strings of Table
10.1 as the profit (the value of the fitness function) associated to a given strategy
(i.e. the gain that would come from the weekly takings). Of course, such a choice is
meaningless; in a real situation the fitness function should be related to the actual
earnings. With such an assumption the strings in Table 10.2 have the fitness values f
shown in Table 10.3.
Note that the profit value for the different four strategies is the only information
used for computing the results of Table 10.3.
The next step is the application of the reproduction operator to the initial popu￾lation, as described in Section 10.3.1. The total fitness of generation 0 is 12, thus the
four individuals of Table 10.3 have fitness 1/4, 1/12, 1/2, 1/6. At each selection, the
best individual (110) has 50% probability of being chosen for mating, whilst the worse
one (001) has about 8% probability. In terms of RWS (see Section 10.3.1), the fittest
individual occupies half of the wheel slots, while the least fit occupies only 1/12 of
them.Simple genetic algorithm 343
i P BMi Fitness f(P BMi)
1 011 3
2 001 1
3 110 6
4 010 2
Total 12
Minimum 1
Average 3
Maximum 6
Table 10.3 Generation 0.
Selection takes place by spinning the roulette four times, to obtain the four indi￾viduals to mate. Given the selection probability of the four individuals, on the average
we will obtain two times the third chromosome of Table 10.3 and one time the first
and the fourth, as shown at the bottom of Table 10.4, where the individuals P BMi
are denoted by Ci
.
Population at generation 0
i Ci f(Ci) P
f(Ci)
f(Ci)
1 011 3 0.25
2 001 1 0.08
3 110 6 0.50
4 010 2 0.17
Individuals selected for mating
i Ci f(Ci) P
f(Ci)
f(Ci)
1 011 3 0.25
2 110 1 0.50
3 110 6 0.50
4 010 2 0.17
Table 10.4 Selection.
Remember that GA is a stochastic method, thus Table 10.4 is only the most prob￾able situation. The selection process could have produced four individuals having the
lowest fitness, although with very low probability. If we compute the average and total
fitness of the individuals at the bottom of Table 10.4 (pay attention, it not yet a pop￾ulation, because mating has not yet occurred) we see that the operation of selecting
proportional to the fitness has the first effects: the total fitness rises from 12 to 17, the
average fitness increases from 3 to 4.5, the minimum fitness from 1 to 2. The maximum
fitness is unchanged, as no new genetic material has been introduced.
Recall that the main operator used to create new individuals is the crossover.
To apply the crossover operator, couples of parents must be chosen from among the
individuals listed at the bottom of Table 10.4. Since the crossover destroys the parents
for giving rise to the offspring, its application to all the individuals selected for mating
is not appropriate because that can push us away from the solution. Indeed, if one of344 Genetic algorithms: an evolutionary-based global random search
the individuals at a given generation is close to 111, which is the true solution of the
problem, this individual should be given the chance to be conserved in the successive
generation.
Having chosen two parents, for example 011 and 110, an X-point is randomly
selected. Suppose it happens to be located between the second and third bits. The
generated offspring are 010 and 111, respectively having fitness values of 2 and 7.
Suppose the crossover operator to be applied to 50% of the cases, i.e. two individuals
out of four selected for mating are transferred unchanged. If the mutation operator is
not applied to any individual, the population at generation 1 is that shown in Table
10.5.
i Ci f(Ci)
1 111 7
2 010 2
3 110 6
4 010 2
Total 17
Minimum 2
Average 4.25
Maximum 7
Table 10.5 Population at generation 1.
The crossover has increased the maximum fitness from 6 to 7, producing an in￾dividual whose fitness is greater than those of its parents. In this simple example,
two generations were sufficient to reach the best possible result. The best commercial
strategy corresponds to the chromosome 111, i.e. it consists in selling the hamburgers
at low price, serving coke as a beverage, and offering a fast service. In more complex,
real problems obtaining the best solution is not possible, in general. To find the true
solution will require a termination criterion to be established, and several generations
will be needed.
10.3.3 Schemata theory. In other words, why genetic algorithms work
We have seen in the previous section how GAs work, but the question is: why do
they work? Let us reconsider the example developed in Section 10.3.2. What did the
manager learn after experimenting for a week? First of all, he learnt that the average
fitness in the search space is 3. By analysing the four strategies employed during the
week, he observes that the strategy 110 is 200% better than the average, 001 is 35%
of the average and so on.
This analysis suggests to the manager several strategies for the following week:
1. He can use the same strategy as the first week, i.e. he can set up random com￾mercial strategies in the four fast-foods. This choice clearly does not exploit at all
the information gained in the first week and corresponds to a random search in
the solution space (Rastrigin, 1963). This kind of search strategy quickly becomes
unrealizable with increasing size of the search space. The chromosome (i.e. theSimple genetic algorithm 345
string of unknown parameters) of the simple example was three bits long, and it
was based on an alphabet of length 2. In the general case, when the chromosome
consists of a string of length L built with an alphabet of size K, the search space
contains KL points. Assuming we can analyse 109 points/second, one year of elab￾oration allows processing about 3 · 1016, corresponding to binary strings (K = 2)
of length L = 55. That length corresponds to 14 single precision real numbers, or
to 7 double precision real parameters, i.e. to a rather small problem size.
2. He can try to fully exploit the obtained information, by applying to all the fast￾foods the best strategy (110) resulting from the first week of experimentation.
With this approach the obtained profit is doubled with respect to the average
gain, but it does not allow us to establish whether a more productive strategy
exists. If the manger’s objective is to maximize the gain, this second strategy is
also unsatisfactory.
3. He can adopt a search strategy based on the consideration that if a profit ‘6’ is
possible, perhaps a greater one is also possible.
The knowledge of the existence of a strategy giving an outcome of 6 poses a practical
problem. When, during the experimentation, the manager obtains a profit 5, he feels he
has lost a unit of gain. The strategy of point 3 is a compromise between the ‘explorative’
will, whose purpose is that of determining the best strategy (but that can occasionally
produce gains lower than 6), and the ‘conservative’ will, which exploiting the current
knowledge would push to a sub-optimal choice.
We can try to understand why the strategy 110 gives an over-average profit. Table
10.6 shows the possible explanations. A character # in the binary strings on the right
means that the associated parameter is non-influential.
Explanation binary string
It is the low price 1 # #
It is coke # 1 #
It is the careful service # # 0
It’s the combination between low price and coke 1 1 #
It’s the combination between low price and careful service 1 # 0
It’s the combination between coke and careful service # 1 0
It’s the combination between low price, coke and careful service 1 1 0
Table 10.6 Explaining the goodness of the 110 strategy.
The strings in Table 10.6 are called schemata. Formally, in a problem based on
strings of length L and consisting of characters belonging to an alphabet of size K,
a schemata is a string whose characters belong to an alphabet of size K + 1 (K
characters from the original alphabet, plus the # symbol). Let us consider, for example,
the schemata ##0 referring to the hypothesis that the goodness of the 110 strategy
depends only on the careful service and it is independent on the values of the remaining
variables. That string clearly contains, as elements, the strings of the subset {000, 010,
100, 110}. All the strings belonging to a schemata share a number of characteristics,
in the above example S = ‘careful service’.346 Genetic algorithms: an evolutionary-based global random search
Geometrically, a schemata describes a subset of points in the search space ex￾hibiting some kind of similarity. By considering the three bits P, B, S composing the
chromosome as spatial coordinates in a 3D reference frame, the strings belonging to
the schemata ##0 identify points on the plane S = 0. More generally, an L-order
schemata (the order is the number of non-# characters) identifies a point, a schemata
of order L − 1 identifies a hyperplane of co-dimension 1, a schemata of order L − 2
represents a hyperplane of co-dimension 2, etc. The geometrical representation of the
solution space is a hypercube, the schemata being a partition of it.
A string of length L belongs to a schemata if its j-th character corresponds to the
j-th character of the schemata, or such a position contains a #, for j = 1...L. We said
that there exist KL different strings of length L composed with characters from an
alphabet of size K. Remembering that the size of the schemata alphabet is K + 1, the
number of different schemata is (K + 1)L. The number of different schemata in the
fast-food example can be easily verified to be 27.
Let us have a look at Table 10.7. Each individual of the population belongs to
KL schemata, eight in the above example. Schemata have a key role in the genetic
search for the solution of a problem. Table 10.6 showed the possible explanations for
the above-average behaviour of the 110 strategy. Let us write down a similar table for
the strategy 010, corresponding to a low profit, to understand what makes the 110
strategy better than the 010.
Schemata Hyperplane Geometry Number of individuals Schemata
order size in the schemata number
3 0 point 1 8
2 1 line 2 12
1 2 plane 4 6
0 3 cube 8 1
Table 10.7 Schemata and their geometrical representation for the Koza Fast-food problem.
Explanation binary string
It is the high price 0 # #
It is coke # 1 #
It is the careful (but slow) service # # 0
It’s the combination between high price and coke 0 1 #
It’s the combination between high price and slow service 0 # 0
It’s the combination between coke and slow service # 1 0
It’s the combination between high price, coke and slow service 0 1 0
Table 10.8 Explaining the below-average outcome of the 010 strategy.
By comparing Tables 10.6 and 10.8, it is clear that the second, third and sixth
row are contradictory because they give the same explanation to opposite phenomena.
Those three hypotheses must therefore be discarded. If we were to repeat the procedure
bringing to Tables 10.6 and 10.8 for the remaining strings (011 and 001) we would
discard some more hypotheses, eventually getting the following explanation for theSimple genetic algorithm 347
goodness of the 110 strategy: it is due to the low price (1##) alone or in combination
with coke (11#) or with slow, careful service (1#0).
Following the point of view of a pioneer of GA, John Holland, schemata can be
seen as competing explanations (Holland, 1992). Each schemata has a fitness value
associated, that is the average fitness of the strings belonging to it. The simultaneous
presence of high and low fitness strings inside a schemata eventually brings us to dis￾carding such a schemata, as happens for the configurations (#1#) and (#10) in Tables
10.6 and 10.8). Conversely, the high fitness value associated with a particular string
implicitly attributes such a fitness to all KL schemata associated to such a string.
This feature is known as implicit parallelism of the search algorithm, and it is a pe￾culiar advantage of GA. The genetic algorithm, through the fitness-based selection,
directs the search procedure in ‘promising’ regions of the search space, i.e. it leads
towards schemata having above-average fitness. Crossover and mutation operate so
as to maintain the population diversity, implicitly taking into consideration that the
best current solution at a given generation can be wrong (false minimum/maximum).
Intuitively, the rejection of a particular string (through the assignment of a low re￾production probability) acts as a partial, but temporary, rejection of the schemata to
which it belongs.
A rigorous mathematical theory of GAs has not been fully developed, with the
exception of the schemata theorem, described in the following section, that in some
ideal conditions explains the power of GA. Anyway, apart from the partial theoretical
demonstrations or intuitive justifications, what is actually convincing is the amazing
effectiveness of the results obtained in various search/optimization problems.
10.3.4 The schemata theorem
Let A1 . . . AN be a population of strings at time t, and let us denote by S a generic
schemata whose the subset A1 . . . Am belongs, i.e.:
S = {Ai
, i = 1...m} (10.1)
The fitness of the string Ai will be denoted by fi
, while ft is the total fitness of
the population at time (generation) t.
ft =
X
N
i=1
fi (10.2)
Selecting proportionally to their fitness value the individuals to be inserted in the
mating pool (i.e. to be propagated to generation t + 1), the selection probability for a
generic individual Ak is fk/ft. The expected number of chromosomes Ak at time t + 1
is therefore Nfk/ft.
From the definition of schemata (10.1), the expected number of strings belonging
to the S schemata is:
N
Pm
i=1 fi
PN
j=1 fj
(10.3)348 Genetic algorithms: an evolutionary-based global random search
Denoting by f(S) the average fitness of the schemata S at generation t:
f(S) =
Pm
i=1 fi
m
(10.4)
we can eventually compute the expected number of strings belonging to the schemata
S at generation t + 1:
n(S, t + 1) = n(S, t)
f(S)
¯f
(10.5)
where n(S, t) = m (number of strings belonging to the schemata S at time t) and
¯f = ft/N is the average fitness of the population al time t.
It should be noted that if a schemata S is above the average of a quantity c
¯f, (10.5)
becomes:
n(S, t + 1) = n(S, t)
¯f + c
¯f
¯f
= (1 + c)n(S, t) (10.6)
and, in the simplifying hypothesis that c remains constant starting from the initial
population:
n(S, t + 1) = n(S, 0)(1 + c)
t
(10.7)
Equation (10.7) shows that, in the above hypothesis, reproduction allocates an
exponentially increasing number of individuals to the above-average schemata.
Before analysing the effect of crossover and mutation on the time behaviour of a
schemata, let us define two characteristics of schemata:
Defining length, δ(S), is the distance between the positions of the extreme defined
values. For example, if S = #011###1, δ(S) = 8 − 2 = 6, distance between the
first 0 and the last 1 of the string.
Order, o(S), it is the number of specified positions (e.g. o(S) = 4 in the above defined
S).
The crossover reduces the survival probability of a schemata. Limiting ourselves to
the simple crossover introduced in 10.2.3, the survival probability of a schemata S of
length L is:
ps = 1 − δ(S)/(L − 1) (10.8)
because the schemata is destroyed if the crossover point, chosen among the L − 1
possible positions, is within the defining length. Denoting the crossover probability by
pc, the previous equation becomes:
ps ≥ 1 − pc
δ(S)
L − 1
(10.9)
Assuming reproduction and crossover as independent from one another, at generation
t + 1 the number of strings belonging to the schemata S is therefore:A simple application: non linear fitting 349
n(S, t + 1) = n(S, t)
f(S)
¯f
ps (10.10)
By a similar reasoning, the probability of destroying a schemata due to mutation,
given the mutation probability pm and the schemata order o(S), is:
m = (1 − pm)
o(S)
(10.11)
Denoting by c and m the destruction probabilities respectively due to crossover
and mutation, eqn (10.5) becomes:
n(S, t + 1) ≥ n(S, t)
f(S)
¯f
(1 − m)(1 − c) (10.12)
Equation (10.12) states that the expected number of strings belonging to a schemata
at generation t + 1 is much greater the more the fitness of the schemata is above
average. Given an equal average fitness, less specified schemata are favoured, i.e. short
and low-order schemata.
Considering the usually low value of pm, we obtain the following relation for the
update of the number of strings in a schemata:
n(S, t + 1) ≥ n(S, t)
f(S)
¯f

1 − pc
δ(S)
L − 1
− o(S)pm

(10.13)
Equation (10.13) is the symbolic expression of the schemata theorem, whose state￾ment is the following:
In a genetic algorithm, short, low-order above-average schemata contain a number of
strings exponentially increasing with increasing generations.
10.4 A simple application: non linear fitting
Let us apply GA to solve a simple, but realistic, problem. Suppose we have N couples of
numbers (Xi
, Yi), coming from the measurement of a quantity Y dependent on another
quantity X. The function Y = f(X) will in general depend on a certain number of
parameters that we can determine so as to minimize the mean squared error due to
measurement uncertainty and to the various error sources:
ε =
X
N
i=1
(f(Xi) − Yi)
2
(10.14)
The couples (X, Y ) are shown in Fig. 10.4.
The graph shape of Fig. 10.4 suggests an exponential function, with three unknown
parameters:
Y = A · e
−BX + C (10.15)350 Genetic algorithms: an evolutionary-based global random search
0 2 4 6 8 10
0.0 0.4 0.8
X
Y
Fig. 10.4 Data for the non linear fitting problem.
10.4.1 Solution using a standard method
We can solve the fitting problem using the nls function of the stats R package:
## Code_10_1.R
# Non linear fitting with genetic algorithm
setwd("C:/RPA/code/genetic_algorithm")
dat <- read.table("data/exp_data.txt")
attach(dat)
plot(V1,V2,xlab="X",ylab="Y")
fit <- nls(V2~A*exp(-B*V1)+C,start=list(A=1,B=1,C=0.01))
summary(fit)
print(coef(fit))
X <- seq(min(V1),max(V1),length=101)
lines(X,predict(fit,list(V1=X)),col="red")
obtaining the following result:
A B C
0.880675632 0.971162486 0.001310648
with 99.7% explained variance R2
. The fitting result is shown in Fig. 10.5.
10.4.2 Genetic solution
The R language has the awesome feature of possessing a huge number of application
packages, due to its free nature. Therefore, it is not surprising that an excellent GA
package exists, developed by (Scrucca, 2013; Scrucca, 2017). The GA package is loaded
by:
library(GA)A simple application: non linear fitting 351
0 2 4 6 8 10
0.0 0.4 0.8
X
Y
Fig. 10.5 Non linear fitting using nls in R.
The genetic solution of the above example can be obtained, for example, evolving a
population of 100 individuals, each consisting of a binary chromosome of 60 bits. The
chromosome codifies three real parameters (A,B,C), so there are 20 bits available for
each parameter. The choice of a binary codification is mainly for didactic purposes.
We will see that the chromosome can be efficiently coded directly as a real vector of
parameters.
The correspondence between genes and real parameters is established in the present
case by fixing the range of variability of each gene between vmin = 0.0 and vmax = 1.0.
Given any minimum and maximum value, vmin and vmax, the binary representation
of a gene G is such that the string Gmin = G1 . . . G20 is composed of all zeros and the
string Gmax = G1 . . . G20 is composed of ones. Using the function binary2decimal of
the GA package, shortened to b2d in the following, a generic binary string G between
Gmin = 00000000000000000000 and Gmax = 11111111111111111111 is decoded as:
v = vmin +
vmax − vmin
dmax
b2d(G) (10.16)
where dmax = b2d(Gmax).
The reciprocal of the sum of squared residuals can be assumed as the fitness func￾tion:
f itness =
 X
i=1...N
(yi − yˆi)
2
!−1
(10.17)352 Genetic algorithms: an evolutionary-based global random search
where yi are the experimental data and ˆyi the values expected from (10.15).
The first GA will adopt a crossover probability 0.7 and a mutation probability 0.1.
In other words, 70% of the individuals are crossed during the reproduction phase (or,
30% of the individuals selected in the mating pool pass unaltered to the successive
generation), while 10% of bits is flipped (mutation). The following table summarizes
the parameters of the algorithm.
SGA parameters:
—————————————————————————–
Total population size 100
Chromosome length (lchrom) 60
Maximum # of generations (maxgen) 100
Crossover probability (pcross) 0.7
Mutation probability (pmutation) 0.1
Table 10.9 GA parameters for the non-linear fitting problem.
The following code shows the GA implementation of the non-linear fitting.
## Code_10_2.R
# Non linear fitting with GA
# Load GA library
library(GA)
# Initialize
pop.size<- 100
chrome.len <- 60
max.gen <- 100
xover.prob <- 0.7
mut.prob <- 0.1
# Read data
setwd("C:\RPA\code\genetic_algorithm\data")
dat <- read.table("exp_data.txt")
X <- dat$V1
Y <- dat$V2
# Define fitness function
f <- function(x) {
chrom.a <- x[1:20]
a <- a1+(a2-a1)*binary2decimal(chrom.a)/dmax
chrom.b <- x[21:40]
b <- b1+(b2-b1)*binary2decimal(chrom.b)/dmax
chrom.c <- x[41:60]
c <- c1+(c2-c1)*binary2decimal(chrom.c)/dmax
N <- length(X)
ret <- 0
for (i in 1:N)
ret <- ret + (Y[i] - a*exp(-b*X[i])+c)^2
1/ret
}
# Exec GA
a1 <- 0
a2 <- 1.0
b1 <- 0A simple application: non linear fitting 353
b2 <- 1.0
c1 <- 0
c2 <- 1.0
dmax = binary2decimal(c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1))
GA <- ga(type = "binary", fitness = f, popSize = pop.size,
nBits = chrome.len, maxiter = max.gen, pmutation = mut.prob,
pcrossover = xover.prob,keepBest = TRUE)
result <- unname(GA@solution)
a <- a1+(a2-a1)*binary2decimal(result[1:20])/dmax
b <- b1+(b2-b1)*binary2decimal(result[21:40])/dmax
c <- c1+(c2-c1)*binary2decimal(result[41:60])/dmax
param <- c(a,b,c)
# Output
max(GA@fitness)
min(GA@fitness)
mean(GA@fitness)
param
# Plot
# Exponential function
fexp <- function(x,p)
{
a <- p[1]
b <- p[2]
c <- p[3]
a*exp(-b*x)+c
}
plot(X,Y)
curve(fexp(x,param),from=0,to=10,add=TRUE,col="red",lty=2)
# Diagnostics
plot(GA@summary[,1],t="l",xlab="Generation",ylab="Fitness")
Table 10.10 shows an excerpt of the initial population, randomly generated. The real
number on the right of each binary chromosome is the computed fitness.
1) 110010011011011110000001011100101010011111111000011001101111 0.14237
2) 011000011101101100110001000011100011010011110001001110101011 0.07571
3) 001000001110110110011011111001011111010110001010111001001101 0.10940
4) 011100010001011001010001111100001100011001000111001010010111 1.14938
5) 011000111111010111011111010001000000101101101101001010001010 0.18846
6) 111001011100100100000110011011101011110010110000101111000010 0.14303
7) 111100101000000001001001101001001100001000111101110110100110 1.32572
8) 110011111100000010100011000111110010010100110000111110110101 2.54225
9) 000110111110110110110011000100110100010100110011011100011110 0.44466
10) 100101100001100111001101011011000111111110000100011001010111 0.15706
.
.
.
.
.
.
100) 010010000011111000011010101011010000001101100111111101101001 0.19736
Table 10.10 Initial population.
The initial population characteristics are summarized in Table 10.11.354 Genetic algorithms: an evolutionary-based global random search
min = 0.04359 max = 6.40820 avg = 0.762564
Global Best Individual so far, Generation 0:
10111110000110010110 10000111110101001100 00011010100000001000
A = 0.7425754 B = 0.5305906 C = 0.1035234 Fitness = 6.40820
Table 10.11 Initial population characteristics.
After ten generations, i.e. after applying ten times the sequence selection-reproduction-
(crossover, mutation), the situation is that shown in Table 10.12. Table 10.13 summa￾rizes the situation of Table 10.12.
1) 010101110011000110001111001011110100110100101000010001001010 0.69597
2) 101101101010000000111001101101111011001100001000100001101000 16.44704
3) 010110110011000110001111001011110100110100101000010001110111 0.715674
4) 000011100001100000000000101011111111010000001100111010000101 0.80283
5) 111001100000000000000000101010011011101100011000111010000101 0.15559
6) 010111010010001101110101111100001100110000101010100100011110 0.98354
7) 111100100100000110000000101101100100111011000010101000110000 1.29843
8) 110011001100111101110010010011100010100010001000111011100010 0.73859
9) 111110110011000110001100110011100010101100001000110001101000 28.45917
10) 011010110011000110001111011000101110101001001000110000001101 0.36534
.
.
.
.
.
.
100) 110011001100111101110010110001110100110000111000101000000101 2.56668
Table 10.12 Population at generation 10.
min = 0.05260 max = 67.75427 avg = 6.23810
Global Best Individual so far, Generation 10:
11001101100101111001 11001100010100100111 00000011100000100000
A = 0.80309468 B = 0.79813366 C = 0.01370241
Fitness = 67.75427
Table 10.13 Characteristics of the population at generation 10.
Note that the maximum fitness increases from 6.4 to 67.8 in ten generations, i.e.
the error is reduced to one tenth. Figure 10.6 shows the behaviour of the maximum
fitness as a function of the generation number. Observe that evolution is not linear,
rather it proceeds in jumps (e.g. see the sharp jumps around generation 20 and 30)
as a consequence of the stochastic nature of GA. Around generation 40 the algorithm
reaches the minimum error (maximum fitness) compatible with the binary represen￾tation. Indeed, due to the 20-bit code, the available real numbers are multiples of
vmin + (vmax − vmin)/(220 − 1).
Table 10.14 shows the final population after 100 generations. Its characteristics are
summarized in Table 10.15. Note that the final value of the maximum fitness is about
60 times the initial one. The average fitness of the population is high as well, about
500 times the initial one, denoting a global convergence of the population towards theA simple application: non linear fitting 355
0 2 0 4 0 6 0 8 0 100
50 150 250 350
generation
fitness
Fig. 10.6 Fitness value vs the number of generations
solution.
The fitting parameters A, B and C are very close to those obtained with nls. The
result of the GA fitting is shown in Fig. 10.7.
1) 111000010100001101001111011011110110100100000000000000011101 347.0354
2) 111000010100001101011111011011110010100100000000000000011100 347.0414
3) 111000010100001110001111010111110111100100000000000000011100 346.3811
4) 111000011100001111001111010111110110100100000000001111111101 342.1954
5) 111000010100001110001111011011100010100100000000000000001101 347.1046
6) 111000011100001110001111011011100110100100000000000110011101 345.6851
7) 111000010100001111001111010111100110100100000000000000001100 346.3201
8) 111000011100001101001111011011110010100100000000000000001101 347.2116
9) 111000011100001111001111011111100110100100000000000000011101 347.1570
10) 111000011100001101001111011011110110100100000000000000011101 347.1757
.
.
.
.
.
.
100) 111000010100101110001111010111110111100100000000000000011100 346.3827
Table 10.14 Population at generation 100.
A word of caution is necessary at this stage. The intrinsic randomness of GAs is
evident from the plot of Fig. 10.8, showing the fitness behaviour in different runs.356 Genetic algorithms: an evolutionary-based global random search
min = 9.55632 max = 347.2292 avg = 318.6861
Global Best Individual so far, Generation 100:
11100001110000111100 11110111111001101001 00000000000000001101
A = 8.818940e-01 B = 9.683628e-01 C = 1.239778e-05
Fitness = 347.2292
Table 10.15 Population characteristics at generation 100.
0 2 4 6 8 10
0.0 0.4 0.8
X
Y
Fig. 10.7 GA solution of the non-linear fitting problem.
0 2 0 4 0 6 0 8 0 100
0 100 200 300
generation
fitness
Fig. 10.8 GA fitness behaviour in several runs.Advanced genetic algorithms 357
The realization of the maximum fitness is not guaranteed by a single run of the
algorithm. In the example at hand, if we had set to 50 the maximum number of
generations, we would have obtained a sub-optimal solution in three cases out of five.
The GA approach therefore requires us to run the algorithm several times and to
choose the best solution.
10.5 Advanced genetic algorithms
We introduce a new GA operator, the elitism. Elitism allows obtaining a faster conver￾gence in several problems. Other criteria for reaching a solution will also be described,
for example the external supply of genetic material during the creation of the initial
population, or the recourse to a population of variable size.
10.5.1 Elitism
We saw that the simplest GA procedure is exposed to the possibility of a disappear￾ance (temporary or definitive) of an individual having a relative maximum value. For
example, the individual k at generation i can be selected for crossover and as a conse￾quence it is no longer present in the population at generation i+1. On the one hand,
that guarantees the variability of the population and it therefore helps in avoiding
being trapped into a false minimum; on the other hand, the lost of the best individual
of a generation can slow down the convergence to the solution.
To preserve the best individual of a generation, it can be propagated to the following
one, giving it another chance to contribute with its genetic material to the future
generations. If the total number of individuals must be conserved (constant population)
this operation requires an individual in the following generation to be suppressed,
usually that presenting the lowest fitness value.
The elitism operation can have side effects. In a minimum search problem, the
objective function can exhibit a deep local minimum and the best individual at gen￾eration i can be located there. Propagating that individual to the generation i+1
increases the probability of being trapped in a local minimum, i.e. elitism increases
the probability of a premature (false) convergence.
In the GA library, by default 5% of the best fitness individuals survive in the next
generation (i.e. the elitism operator was already present, although undeclared, in the
previous analysis). The exact number of survivors is fixed by setting the parameter
elitism in the ga function.
We can investigate the efficiency of elitism by applying it to the previous non-linear
fitting example. Figure 10.9 compares the fitness behaviour with elitism percentages
of 1% (solid line), 2%, 5% and 10%. The increasing convergence speed is manifest:
although the final fitness value is comparable in the four cases, the elitism operator
significantly reduces the computation time, usually reducing the number of jumps too.
A GA can also work without elitism (i.e., by setting elism = 0), but in this case a
greater number of generations and a larger population size is usually needed. Moreover,
if the best-fitness individual is not propagated to the next generation, the maximum
fitness does not necessarily increase with each generation. As an example Fig. 10.10
shows the results for a GA with elitism = 0, having a population size of 500 and 200358 Genetic algorithms: an evolutionary-based global random search
0 2 0 4 0 6 0 8 0 100
0 100 200 300
generation
fitness
Fig. 10.9 Influence of elitism on the GA convergence.
generations. Although the correct fitting parameters are obtained (Fig. 10.10 (bot￾tom)), the maximum fitness behaviour is erratic. Incidentally, in the particular case of
Fig. 10.10 the maximum fitness is obtained at generation 37.
0 50 100 150 200
0 100 200 300
generation
fitness
0 2 4 6 8 10
0.0 0.4 0.8
X
Y
Fig. 10.10 Fitness behaviour (top) and fitting result (bottom) for a GA without elitism.Advanced genetic algorithms 359
10.5.2 Inseminated and variable-size populations
Up to now, the initial population has been randomly generated. Sometimes, an a priori
knowledge is available of the region of the solution space containing the true solution
(e.g. the solution is a real positive number between 0 and π). In such a case, the
initial population can also be forced to contain a small number of individuals with
assigned characteristics, in other words it can be artificially inseminated. This action
corresponds to giving a ‘seed’ in other algorithms.
Clearly, assigning a string having a particular bit combination initially forces the
production of individuals containing that genetic material, at least partially. Therefore,
as discussed for elitism, the artificial insemination can greatly increase the convergence
speed but it also increases the probability of an untimely convergence to a false solu￾tion.
Note that all populations treated so far were constant-size. This constraint can be
removed, allowing the number of individuals to vary in time. It is sometimes useful to
start with a small population, allowing it to grow with increasing generations. A theo￾rem has been developed by Goldberg concerning the ‘implicit parallelism’ (Goldberg,
1989): a genetic algorithm processes in parallel O(N3
) schemata per generation, im￾plying that doubling the population size eventually halves the total computation time.
This means that the computational cost due to the increasing size of the population
is at least partially compensated by the increase in the number of schemata processed
by the GA.
10.5.3 Other genetic operators
Only the imagination can limit the operators suitable for application in a GA. For
example, inversion and translocation are operators directly borrowed from biology
(see Section 10.2.1) and sometimes used in GAs.
As an example, the inversion of the chromosome C=C1 . . . CN produces the chro￾mosome C
i=CN . . . C1. Inversion can be applied in any GA. Translocation, instead,
requires variable-length chromosomes. Indeed, this operation consists in detaching from
C a section Sc=Ci
. . . Cj and in attaching it to another chromosome C’, producing
C
0
1
. . . C0
N Sc or ScC
0
1
. . . C0
N .
Finally, a note about crossover. The simple crossover introduced and used through￾out this text is neither the only possible one nor the most used. At least two other
crossover modalities are widely used in the literature: the two-point crossover and
uniform crossover.
The former requires chromosomes A1 . . . AN and B1 . . . BN to be cut in two points
i e j (randomly chosen), to obtain:
A1 . . . Ai−1Bi
. . . BjAj+1 . . . AN
and
B1 . . . Bi−1Ai
. . . AjBj+1 . . . BN360 Genetic algorithms: an evolutionary-based global random search
In the latter case, a crossover ‘mask’ is established, and subsequently the alleles of
the parent A corresponding to 1’s in the mask are copied in the offspring C together
with the alleles 0’s of the parent B coinciding with the mask. For example, if the
parents are A = A1A2A3A4 and B = B1B2B3B4 and the chosen mask is 1011, we
obtain C = A1B2A3A4.
10.5.4 Real coded GA
The binary coding is historically motivated and, above all, it makes the algorithm
easier to understand. Nevertheless, a direct real coding is perfectly equivalent and at
least equally efficient.
0 2 4 6 8 10
0.0 0.4 0.8
X
Y
Fig. 10.11 Exponential fitting with real-coded GA.
The GA package by default uses real chromosomes. Figure 10.11 shows the result
of the exponential fitting example using a real-coded GA with a population size of
100 and 200 generations. The fitness behaviour with increasing generations is shown
in Fig. 10.12. The following R code shows the details of the computation.
## Code_10_3.R
# Real coded GA
setwd("C:/RPA/code/genetic_algorithm")
dat <- read.table("data/exp_data.txt")
X <- dat$V1
Y <- dat$V2
# Initialization
pop.size<- 100
max.gen <- 200
xover.prob <- 0.7
mut.prob <- 0.1
# fitness function
f <- function(x)
{
a <- x[1]Parameter estimation of ARMA models 361
0 50 100 150 200
50 150 250
generation
fitness
Fig. 10.12 Fitness behaviour for exponential fitting using real-coded GA.
b <- x[2]
c <- x[3]
N <- length(X)
ret <- 0
for (i in 1:N)
ret <- ret + (Y[i] - a*exp(-b*X[i])+c)^2
1/ret
}
# Exec GA
library(GA)
GA <- ga(type = "real-valued", fitness = f, lower = c(0.,0.,0.),
upper = c(1,1,1),maxiter=max.gen,popSize = pop.size)
result <- unname(GA@solution)
print(result)
# Plot
plot(X,Y)
curve(fexp(x,result),from=0,to=10,add=TRUE,col="red",lty=2)
# Diagnostics
plot(GA@summary[,1],t="l",xlab="Generation",ylab="Fitness")
10.6 Parameter estimation of ARMA models
We have met autoregressive moving-average processes in Chapter 6. An ARMA(p, q)
model is used to analyse a stationary time series having an autoregressive component
of order p and a moving-average component of order q.
AR(p) models, a subset of them where the moving-average component is negligibly
small, were met in Section 9.1, where the linear relation between temperature anomaly
and carbon dioxide concentration in Antarctic ice has been shown to be affected by
autocorrelation. We have seen that the residual error of the linear regression is a
time series corresponding to an AR(1) process, whose regression parameters (vector
of regression coefficients β = (β0, β1), error variance σ
2 and correlation coefficient
ρ) were estimated by the MCMC algorithm. Such a problem could be faced as well362 Genetic algorithms: an evolutionary-based global random search
by means of a genetic algorithm, choosing a chromosome consisting of the unknown
parameters and obtaining the best individual (solution) by maximizing the likelihood
which would play the role of the fitness function.
As a complex example of GA, we will try to obtain the best-fit parameters of
an ARMA(p, q) model describing the annual runoff of the Loire river, determined
to be an ARMA(1,1) in Section 6.6.1 by a “manual” procedure involving the arima
function of the R base package stats. The problem is the following. We have a set
of data, the standardized annual runoff of Loire from 1860 to 1980, and we want to
fit an ARMA(p, q) model, finding the order (p, q) and the values of the autoregressive
coefficients φ1, ..., φp and of the moving-average coefficients θ1, ..., θq. We will discuss
what the fitting function is in a while. Before that, we have to understand what is the
chromosome.
The chromosome naturally appears to be the sequence of unknown parameters,
e.g. something like:
p q φ1φ2...φp θ1θ2...θq
Such a definition is not easy to implement, because of the variable number of genes.
A simple solution is to define a maximum value for p and q, for instance p = q = 4, to
implement a fixed-length chromosome as:
p q φ1φ2φ3φ4 θ1θ2θ3θ4
and to discard the φi
, θk genes exceeding the best-fit p and q values maximizing the
fitness function, i.e for i > p and k > q.
Now, a short digression is in order concerning the fitness function. In Section 6.6.1
we searched for the minimum value of the Akaike’s information criterion (AIC) func￾tion, expressed by (6.19), here repeated for convenience:
AIC = 2(p + q + 1) − 2log(L)
which essentially corrects the log-likelihood for the number of the parameters in the
model. The reciprocal of AIC is a good candidate for the fitness function.
The log-likelihood of an ARMA model can be obtained by the definition of the
process {Xt} (remember the arbitrary sign of φ’s and θ’s coefficients):
Xt = φ1Xt−1 + ... + φpXt−p + t + θ1t−1 + ... + θqt−q (10.18)
Following (Madsen, 2008), for a given set of parameters:
ΘT = (φ1, .., φp, θ1, ..., θq)
the expected value of X at time t if the process at time t − 1 is known, i.e. knowing
XT
t−1 = (Xt−1, Xt−2, ..., X1), is:1
1That is usually called ‘one-step prediction’.Parameter estimation of ARMA models 363
Xˆ
t|t−1(Θ) ≡ E [Xt|Θ, Xt−1] = Xp
i=1
φiXt−i +
Xq
k=1
θkt−k(Θ)
where the dependence on the parameter vector Θ and conditioning on the previous
time step have been explicitly indicated. The white noise terms t here play the role
of residuals, or ‘prediction errors’ and, of course, depend on the fitting parameters Θ.
The prediction error at time t for a given choice of the parameters is:
t(Θ) = Xt − Xˆ
t|t−1(Θ)
Assuming zero residuals for times 1 ... max(p,q), i.e. 1 = 2 = ... = max(p,q) = 0
(this introduces a negligible error in the procedure when the number of observations
is very much larger than p and q, as usually happens), we can compute the errors t
at any time by a recursive procedure. Explicitly, for the simple case p = 1, q = 1 and
N observations:
1 = 0 → Xˆ
2|1 = φX1
2 = X2 − Xˆ
2|1 → Xˆ
3|2 = φX2 + θ2
...
N−1 = XN−1 − XˆN−1|N−2 → XˆN|N−1 = φXN−1 + θN−1
N = XN − XˆN|N−1
In order to get an explicit expression for the AIC we need the likelihood function.
We will assume, as for maximum likelihood method, that {t} has a normal distribution
and variance Var [t] = σ
2

. The likelihood is the joint probability distribution for the
observations for given values of the parameters Θ and σ
2

. Its explicit dependence from
the parameters is not simple (Box et al., 1994), with the exception of low-order cases
(p = 1 or q = 1). In general, the log-likelihood is an expression like:
log(L(X, Θ)) = −
1
2

Nlog(2π) + log|Γ(Θ)| + XT Γ(Θ)−1X

where Γ is the covariance matrix of the process, depending on the parameter vector Θ.
If we solve (10.18) for t, since they have been assumed as independent and normally
distributed, the joint probability density function of 
T = (1, 2, ..., N ) is normal:
p(|Θ, σ2

) = (2πσ2

)
−N/2
exp "
−
1
2σ
2

X
N
t=1

2
t
#
and the likelihood of the ARMA model, which is the joint probability density of {Xt},
is something like (Abraham and Ledolter, 1983):
L(Θ, σ2

|X) = f(Θ, σ2

)exp "
−
1
2σ
2

X
N
t=1
ˆ
2
t
#
(10.19)
where the function f depends on the orders p and q, and the expected value of t can be
computed with the recursive scheme discussed above. Thus, an approximate expression
of the log-likelihood can be computed starting from the above one-step prediction364 Genetic algorithms: an evolutionary-based global random search
errors (Madsen, 2008), sometimes called ‘innovations’ (Shumway and Stoffer, 2006),
ignoring the function f (which is assumed equal to 1).
log(L(Θ, σ2

)) ≈ −
N
2

log(σ
2

) + log(2π)

−
1
2σ
2

X
N
t=p+1

2
t
(Θ) (10.20)
where the conditioning on the observed data X has been dropped from the notation.
We have seen that what the best solution does is to exhibit the minimum AIC (6.19)
rather than the maximum likelihood value. Therefore, the solution will correspond to a
minimum of the sum of squared one-step prediction errors, S(Θ) = P
2
t
(Θ), corrected
for the orders p and q. A convenient fitness function F(p, q, Θ) (to be maximized) is
minus this last expression, that is:
F(p, q, Θ) = p + q + 1 −
X
N
t=p+1

2
t
(Θ) (10.21)
To summarize, if we neglect the influence of the covariance matrix on the true
likelihood, we can implement the fitness function in the GA as a quantity mainly
related to the sum of the squared residuals. The p and q orders of the ARMA, and
of course the coefficients constituting Θ, enter the game through the estimates Xˆ
t|t−1
and directly in the expression of the AIC-like function (10.21).
That is the approach adopted in the following R code. The first code section,
coming from Chapter 6, computes the standardized annual runoff.
## Code_10_4.R
# GA for ARIMA optimization
setwd("C:/RPA/code/genetic_algorithm")
dat <- read.table("data/loire_runoff.txt",h=TRUE)
names(dat)
x <- ts(dat$DISCHRG)
# annual average
Y <- dat$YEAR
N <- length(dat$YEAR)
years <- Y[1]:Y[N]
n <- Y[N]-Y[1]+1
X <- rep(0,n)
for (i in 1:n){
X[i] <- sum(x[(12*(i-1)+1):(12*(i-1)+12)])
}
X[113] <- 0.5*(X[112]+X[114])
# Standardized annual runoff
X.m <- mean(X)
X.sd <- sd(X)
X <- ts((X-X.m)/X.sd,start=1863)
plot(X,xlab="Year",ylab=expression(paste("Annual runoff (m"^"3","/s)")),
mgp=c(2,0.7,0))
The fitness function (10.21) is computed as follows.
# CONTINUE Code_10_4.R
# Count the number of observations
n <- length(X)
# Initialize errorParameter estimation of ARMA models 365
e <- rep(1, n)
# Fitness function
fitness.fun <- function(crom){
g <- numeric
mu <- crom[1]
P <- round(crom[2])
Q <- round(crom[3])
if (P>Q)
for (i in 1:P) e[i] <- 0
else
e[1] <- 0
# Take into account a non-zero mean value
XX <- X-mu
fi <- c()
teta <- c()
if (P>0)
for (i in 1:P) fi = cbind(fi,crom[i+3])
if (Q>0)
for (j in 1:Q) teta = cbind(teta,crom[j+P+3])
if (P==0)
n_start <- 2
else
n_start <- P+1
for (t in (n_start : n)){
X_hat <- 0
if (P>0){
for (i in 1:P)
X_hat = X_hat+fi[i]*XX[t-i]
}
if (Q>0 & Q<=P){
for (i in 1:Q)
X_hat = X_hat+teta[i]*e[t-i]
}
e[t] <- (XX[t] - X_hat)
}
g <- e%*%e
return(-g-(P+Q+1))
}
Finally, the GA procedure using the GA package is the following.
# CONTINUE Code_10_4.R
# Load GA library
library(GA)
# Initialize GA
pop.size<- 30
max.gen <- 300
xover.prob <- 0.7
mut.prob <- 0.1
# Compute
GA <- ga(type = "real-valued", fitness = fitness.fun,
lower = c(-0.01,0,0,-0.9,-0.9,-0.9,-0.9),
upper=c(0.01,2,2,0.9,0.9,0.9,0.9),
maxiter=max.gen,popSize = pop.size,elitism=4)
# Show solution366 Genetic algorithms: an evolutionary-based global random search
sol <- GA@solution[1,]
ARMA.params(sol)
# Plot diagnostics
plot(GA)
Remember that p and q are integers, so the round value of the relevant ‘gene’ must
be taken. That’s true always in the interpretation of the resulting best solution; the
function ARMA.params is defined as follows:
# CONTINUE Code_10_4.R
ARMA.params <- function(crom){
mu <- crom[1]
p <- round(crom[2])
q <- round(crom[3])
fi <- c()
teta <- c()
if (p>0)
for (i in 1:p) fi = cbind(fi,crom[i+3])
if (q>0)
for (j in 1:q) teta = cbind(teta,crom[j+p+3])
cat("mean",mu,"\n")
cat("AR",p,fi,"\n")
cat("MA",q,teta,"\n")
}
Figure 10.13 shows the fitness result in ten different runs.
0 50 100 150 200 250 300
−114.0 −113.0 −112.0
generation
fitness
Fig. 10.13 Fitness behaviour for GA analysis of the annual Loire runoff data.Solving the travelling salesman problem 367
The best solution has the following parameters:
mean 0.001330264
AR 1 -0.4853239
MA 1 0.7153637
very close to those computed by arima(X,order=c(1,0,1)):
Coefficients:
ar1 ma1 intercept
-0.4535 0.6841 0.0016
Figure 10.14 shows the diagnostics of the best solution described above. The solid
line is the fitness of the best ‘individual’, while the dots connected by dashed lines are
the average fitness values at each generation.
0 50 100 150 200 250 300
−145 −135 −125 −115
generation
fitness value
Fig. 10.14 Fitness behaviour for the best GA solution.
10.7 Solving the travelling salesman problem
We have encountered the travelling salesman problem (TSP) in Section 8.3 where it
was approached by means of simulated annealing, based on the Metropolis algorithm.
The TSP would ask for the best possible route, but the problem itself has stimulated
the development of heuristic-search (HS) algorithms and it has nonetheless served as
a benchmark of such algorithms. What is such an algorithm aimed to do in a problem
like TSP, actually? Essentially it tries to find a good solution, possibly sub-optimal
instead of the best solution.368 Genetic algorithms: an evolutionary-based global random search
GA are algorithms of the HS family. The peculiarity of the GA solution to TSP,
with respect to the example discussed above, is in the chromosome structure. In all
problems treated so far the genes (parameters) composing the chromosome were always
quantitative or, in other words, they have a ratio measurement scale. In the TSP the
natural definition of the chromosome is a succession of labels identifying the cities to
be visited. Such labels could possibly be represented by numbers, but without any
numeric meaning, i.e. the genes have a nominal scale.
While the fitness function, for example consisting of the inverse of the total distance
travelled by the salesman, is a numeric value as it should be; the peculiar coding of the
chromosome does not allow to define the main GA operators, crossover and mutation,
as in Section 10.2.3. An example should clarify why.
Suppose, in a TSP involving six cities, labelled by the first alphabetic letters, you
have generated two possible routes: A B C D E F and B D A E F C. A crossover point
could be randomly chosen between the second and third letters:
A B | C D E F
B D | A E F C
With the rules of Section 10.2.3, mating between those parents would generate the
following offspring:
A B | A E F C
B D | C D E F
which are clearly not valid routes, because cities cannot be visited more than one time.
A reasonable solution for the crossover is a procedure known as ordered crossover,
which consist of choosing two points, for example second and fourth, and generating
an offspring by a scheme like the following:
Parents:
A | b c | D E F
B | D A | E F C
Offspring:
D | b c | A E F
where the lower-case portion of the chromosome is the genetic material coming from
the first parent, and the remaining are labels chosen in order from the second parent.
Such a solution generates proper permutations of the tentative solutions, and that is
what we need.
Similar considerations hold for the mutation operator: here we have not any bit
to flip from 0 to 1 or vice versa The solution close to bit flipping is to exchange two
adjacent sites, for a randomly chosen position (e.g. the second):Solving the travelling salesman problem 369
Original: A B C D E F Mutated: A C B D E F
Ordered crossover is the default method for chromosomes of the permutation type
in the GA package (Scrucca, 2013). Indeed, such a package can be usefully employed
for finding an heuristic solution to TSP. The following code shows how to do it.
## Code_10_5.R
# Travelling salesman with GA
library(GA)
set.seed(12345)
# N cities: 1 2 .. N
N <- 15
# city coordinates
coords <- matrix(nrow=N,ncol=2)
test <- TRUE
while (test) {
coords[,1] <- trunc(10*runif(N))
coords[,2] <- trunc(10*runif(N))
# avoid coincident cities
test <- anyDuplicated(coords)
}
distances <- matrix(rep( 0, len=N*N), nrow = N)
for (i in 1:N){
for (j in 1:N){
distances[i,j] <- sqrt((coords[i,1]-coords[j,1])^2+(coords[i,2]-coords[j,2])^2)
}
}
# fitness function
inverse_distance <- function(x) {
N <- length(x)
dist.total <- distances[1,x[1]]
for (i in 1:(N-1))
dist.total <- dist.total + distances[x[i],x[i+1]]
dist.total <- dist.total + distances[x[N],1]
1/dist.total
}
TSP.ga <- ga(type = "permutation", fitness = inverse_distance, lower = 2, upper = N,
elitism = 1, maxiter = 500, popSize = 100)
# show GA parameters
summary(TSP.ga)
route.best <- unname(TSP.ga@solution)
# choose the first one
route.best <- c(1, route.best[1,])
# plot TSP
plot(0:10,0:10,type="n",xlab="",ylab="")
text(coords[1,1],coords[1,2],labels=1,col="black")
text(coords[1,1],coords[1,2],labels="O",col="black",cex=2)
for (i in (2:N))
text(coords[i,1],coords[i,2],labels=i,col="red")
# best route find by GA
for (i in 1:(N-1))370 Genetic algorithms: an evolutionary-based global random search
arrows(coords[route.best[i],1],coords[route.best[i],2],coords[route.best[i+1],1],
coords[route.best[i+1],2],col="blue",angle=20,length=0.1)
arrows(coords[route.best[N],1],coords[route.best[N],2],coords[route.best[1],1],
coords[route.best[1],2],col="blue",angle=20,length=0.1)
# plot algorithm convergence
plot(TSP.ga)
The code above randomly generates the position of 15 cities in the domain (0, 10)×
(0, 10), in arbitrary units. The cities are labelled with numbers from 1 to 15, city ‘1’
being the home of the salesman where he should come back to after visiting the other
14 cities, as required by the TSP. As the first city is fixed, the number of possible
permutation is 14 factorial, instead of 15 factorial. Then, the number of possible paths
is only 8.7 × 1010 instead of 1.3 × 1012. An exhaustive approach, i.e computing all
routes and choosing the shortest, is out of the question!
Figure 10.15 shows the route compute by the above code, and Fig. 10.16 the relative
fitness behaviour.
0 2 4 6 8 10
0 2 4 6 8 10
O1
2
3
4 5
6
7
8
9
10
11 12
13
14
15
Fig. 10.15 GA solution of a TSP involving 15 cities.
The following Code_10_6.R implements the GA solution for the TSP of the 20
Italians provinces.
# Code_10_6.R
# load GA library
library(GA)
# Read the province coordinates and mutual distances
setwd("C:\RPA\code\genetic_algorithm\data")
city.coords <- read.table("provinces.txt",h=T)
cx <- as.matrix(city.coords[,3:4])
labs <- city.coords$Code
distances <- read.table("distances.txt",h=TRUE)
distances <- data.matrix(distances)
N <- 20Solving the travelling salesman problem 371
# distance function
dista <- function(x) {
ret <- M_Cities[x[1],x[2]]
for (i in 2:20)
ret <- ret + M_Cities[x[i],x[i+1]]
as.numeric(ret)}
# fitness function
inverse_distance <- function(x) {
N <- length(x)
dist.total <- distances[1,x[1]]
for (i in 1:(N-1))
dist.total <- dist.total + distances[x[i],x[i+1]]
dist.total <- dist.total + distances[x[N],1]
1/dist.total
}
TSP.ga <- ga(type = "permutation", fitness = inverse_distance, lower = 2, upper = N,
elitism = 1, maxiter = 500, popSize = 100)
# show GA parameters
summary(TSP.ga)
route.best <- unname(TSP.ga@solution)
# choose the first one
route.best <- c(1, route.best[1,],1)
# plot TSP
route <- route.best
xmin <- 38
xmax <- 46.5
ymin <- 7
ymax <- 17
# plot TSP
plot(seq(ymin,ymax,(ymax-ymin)/10),seq(xmin,xmax,(xmax-xmin)/10),
type="n",xlab="Longitude",ylab="Latitude")
text(coords[1,2],coords[1,1],labels=labs[1],col="black",cex=0.8)
text(coords[1,2],coords[1,1],labels="O",col="black",cex=2.5)
for (i in (2:20))
text(coords[i,2],coords[i,1],labels=labs[i],col="red",cex=0.8)
# best route find by GA
for (i in 1:19)
arrows(coords[route[i],2],coords[route[i],1],coords[route[i+1],2],
coords[route[i+1],1],col="blue",angle=20,length=0.1)
arrows(coords[route[20],2],coords[route[20],1],coords[route[1],2],
coords[route[1],1],col="blue",angle=20,length=0.1)
# Uncomment to plot algorithm convergence
# plot(TSP.ga)
# Show solution and route length
print(route.best)
print(dista(route.best))
As a matter of comparison between the solution outlined in Section 8.3, we can
apply the genetic approach to the problem of finding the best route between the 20
regional ‘capitals’ of Italy. The GA solution appears usually better than that based on
SA: the route length of the travel in Fig. 10.17 is 6318 km, i.e. about 1500 km shorter.372 Genetic algorithms: an evolutionary-based global random search
0 100 200 300 400 500
0.012 0.018 0.024
generation
fitness value
best
mean
Fig. 10.16 Fitness behavior for the GA solution of the TSP.
8 1 0 1 2 1 4 1 6
38 40 42 44 46
longitude
latitude
OAQ
PZ
CZ
NAP
BO
TS
RM
GE
MI
AN
CB
TO
BA
CA
PA
FI
TN
PG
AO
VE
Fig. 10.17 GA solution of the TSP involving the 20 main Italian provinces.10.8 Concluding remarks
Genetic algorithms work amazingly well, in spite of a non-fully developed theoretical
framework. The ease of implementation and their conceptual clarity, joined with the
power of the biological paradigm, makes them eligible for the solution of search and
minimization problems in several fields.
GA does not require rigorous assumptions about probability distributions, i.e. they
are intrinsically non-parametric, and they do not request an inversion procedure, work￾ing in a forward direction. It is only necessary to define a function to be maximized
(or minimized), and to generate solutions almost randomly.
The simplicity of the method should not be a surprise. After all, it is simply a matter
of generating and manipulating strings!
10.9 Exercises
Exercise 10.1 The following is a toy-problem, obviously not needing GA to be solved. Sup￾pose you want to find the positive root of the polynomial:
f(x) = x
2 − 4x − 12
i.e. the value x = 6.
Your population consists of binary strings of four bits. The solution is, of course, the string
0110. Applying by hand a very rough GA, without mutation and with fixed crossover point
in the middle of the chromosome, suppose you have obtained 0101 as the fittest individual.
Remembering the mating procedure introduced in Section 10.3 which, given two individ￾uals X1X2X3X4 and Y1Y2Y3Y4, generates the new chromosomes X1X2Y3Y4 and Y1Y2X3X4,
how many chromosomes (among the 16 possible) would give rise to the proper solution
through crossover and mating with 0101?
Exercise 10.2 With reference to Exercise 10.1, define a fitness function suitable to be max￾imized such, for instance:
f itness(x) = 1/(10 + abs(x
2 − 4x − 12))
Write an R code to compute the fitness value of all possible chromosomes, and plot it.
Exercise 10.3 With reference to Exercise 10.2, implement a real-coded genetic algorithm
based on the GA R library, taking the example in Section 10.5.4 as a starting point. Begin with
a population of 10 individuals, 100 generations, crossover and mutation probabilities of 0.7 and
0.1 respectively, then play with numbers to see how the values of the four parameters affect
the result. Compare the fitness histories for 10, 100 and 1000 individuals with a proportional
number of generations, using GA@summary[,1].
Exercise 10.4 Download the data for the example in Section 10.5.4 and modify the fitness
function. Does the result significantly change if fitness is computed in terms of the absolute
residuals instead of the residual sum of squares? How is convergence affected, if at all, by the
choice of the fitness function?
Exercise 10.5 Following the lines of the previous exercises, write a GA code for fitting data
generated by the following:
Exercises 373374 Genetic algorithms: an evolutionary-based global random search
# Generate random signal, composition of sine and cosine
t <- seq(0,10,1/10)
A1 <- runif(1)
A2 <- runif(1)
T1 <- 1
T2 <- 0.5
f1 <- runif(1)/T1
f2 <- runif(1)/T2
s <- A1*sin(2*pi*f1*t)+A2*cos(2*pi*f2*t)
plot(t,s,t="l",xlab="t (s)",ylab="s(t)")
where the functional form of the signal s is supposed to be known:
s(t) = A1sin(2πf1t) + A2cos(2πf2t)
but A1, A2, f1, f2 are unknown.
Write down the fitness function, and try to obtain a solution with a population of 100
chromosomes, with 100 generations. Repeat the whole code several times, to study how well
different sine/cosine compositions can be managed.
Hint: we have four parameters, A1, A2, f1, f2, instead of the three in the code of Section 10.5.4.
The fitness function must be modified accordingly.
Exercise 10.6 Section 10.7 shows how a quasi-optimal solution can be obtained for 15 cities.
In a simpler case, say five cities including the starting one, the possible routes can be enu￾merated and all the relative distances can be computed. Modify the code to generate only
five cities and compare the result of the heuristic algorithm to the true best solution. Write
a simple R code to make that work.11
The Problem of Accuracy
Accuracy and clarity of statement are mutually exclusive.
Niels Bohr
11.1 Estimating accuracy
In Chapter 3, we introduced some features of stochastic processes and their realizations,
as stationarity, ergodicity, time and ensemble averages, convergence and so on. In the
following, we show how such properties are realized in simulated series. We stressed
that the convergence is conditio sine qua non to assign accuracy to estimates. In fact,
only if the chain has achieved stationarity, do average and variance estimates make
sense. In other words, we have to assess that the piece of chain we have generated,
and on which we base our estimates, is representative of the underlying stationary
distribution of the whole Markov chain.
As stressed by Cowles and Carlin (1996, p. 883), such a notion of convergence refers
to something which is not ‘a single number or even a distribution, but rather a sample
from a distribution’ (authors’ italics). The problem of convergence will be discussed in
a later section. In the following, we will deal with the problem of assigning an accuracy
to the estimated parameters when the series has (presumably) reached stationarity.
Consider the discrete parameter stationary time series Xt = (X1, X2, . . . , Xn). Let
µ, σ2
0
, γk and ρk (k = 0, . . . , n − 1) be the mean, variance, covariance and autocorrela￾tion function of Xt, respectively. Note that γ0 = σ
2
0
, and ρk = γk/γ0. The variance of
the estimator X¯ of µ, is given by:
σ
2 = Var 
X¯

=
σ
2
0
n
+ 2
nX−1
k=1
(n − k)
n
γk
= σ
2
0
"
1
n
+ 2
nX−1
k=1
(n − k)
n
ρk
# (11.1)
This equation is well known in the time series literature. For the general problem
of estimating the standard error σ one can see, e.g. (Ripley, 1987), where various
approaches are discussed and where it is stressed that finding a reliable value for σ
2
is
not always a simple matter. In statistical mechanics σ
2
is usually written as (see e.g.
Binder, 1992):376 The Problem of Accuracy
σ
2 = σ
2
0

1 +
2τ
δt 
(11.2)
where τ is the ‘integrated correlation time’:
τ =
Z ∞
0
ρ(t)dt
and δt is the time interval between two successive observations. This is a quite im￾portant parameter, giving the whole information on the correlation structure of the
observed data. However, in general the time dependence of the correlation function is
not explicitly known, so that we have to rely on an estimate of τ which could be:
τˆ =
X∞
k=0
ρˆk
with a suitable cutoff in the summation.
A plugged-in estimate of ρk is given by ˆρk = ˆγk/γˆ0, where ˆγk is the sample auto￾covariance function:
γˆk =
1
n
nX−k
i=1
[(Xi − µˆ)(Xi+k − µˆ)] , k = 0, . . . , n − 1 (11.3)
The estimate ˆρk requires in turn the evaluation of (11.3), but we know (Priestley,
1989) that the above estimator ˆγk must be used with caution, since it is not consistent
even with divisor n − k rather than n.
In the field of Markov chains, if the chain is reversible the autocovariance function
γk declines smoothly to zero and is positive for all k, so one might think of cutting the
sequence ˆγk off when it becomes negative. This procedure, however, is only successful
if the sample autocovariance function also goes smoothly to zero. But this is not always
the case. So also in this context, the estimate of τ is a problematic task.
Other strategies have been proposed, for instance, Geyer (1992) suggested exploit￾ing the function Γk = γ2k + γ2k+1, which is a non-negative, non-increasing convex
function of k, proposing three different estimators in order to smooth the sample au￾tocovariance function, if bumps are present. However, also in this case, the method
and its success, depend on the particular form of the sample autocovariance function.
In conclusion, the estimate of τ requires in any case the computations of intermediate
estimates and, moreover, sometimes rather arbitrary approximations are involved.
11.2 Averaging time series
Consider now the following time series (written with three decimal places):
[1] 0.000 0.185 1.754 0.448 0.323 0.423 1.089 0.740 2.651 2.247 ...
................................................................
[1191] 2.197 2.405 1.533 0.287 0.439 0.099 0.698 -0.195 0.418 0.991
That is a realization of the stochastic process:
yi = yi−1 φ + zi (11.4)
where zi ∼ N (0, σ). This kind of process, named AR(1), was described in Chap￾ter 6, where we saw that an AR(1) process is stationary if and only if |φ| < 1 orAveraging time series 377
−1 < φ < 1. The case where φ = 1 corresponds to a random walk process with a zero
drift (see Chapter 5). The above realization of the process is obtained with the R code
Code_11_1.R shown below (with zi ∼ N (0, 1)).
## Code_11_1.R
# Averaging time series
set.seed(2)
nsteps<- 1200 # number of iterations
phi<- 0.90
z<- rnorm(nsteps,mean=0,sd=1)
y<- numeric()
y[1]<- 0 # initial state of the process.
for(i in 2:nsteps) {
y[i]<- y[i-1]*phi+z[i]
}
#y # comment if you do not wish the y values printed
plot(1:nsteps,y,type="l",xlab="steps",ylab="y",cex.lab=1.3,lty=1,
font.lab=3,lwd=1.5)
Figure 11.1 shows a time series plot of one realization of the eqn (11.4) AR(1) process.
0 200 400 600 800 1000 1200
−4 −2 0 2 4 6 8
steps
y
Fig. 11.1 One realization of the eqn (11.4) AR(1) process.
The plot shows typical features of a stationary series:
i ) it extends roughly along the horizontal direction.
ii ) the variance apparently remains almost constant.
iii ) no patterns are evident in relative long time periods over or under the line y = 0.
The autocorrelation function for time series can be estimated using the acf R function,
that we have frequently encountered in the previous chapters.
corr<-acf(y,type="correlation",lag.max=50,plot=TRUE,main=" ",
xlab="lag",ylab="ACF",cex.lab=1.2,cex.axis=1.2,font.lab=3,lwd=1.5)
#corr # comment if you do not wish the acf values printed378 The Problem of Accuracy
Figure 11.2 shows the extension of the autocorrelation in our series. The ACF data
exceeds the confidence bands (dashed horizontal lines) for the first 20 time lags, indi￾cating a strong autocorrelation of the time series.
0 1 0 2 0 3 0 4 0 50
0.0 0.2 0.4 0.6 0.8 1.0
lag
ACF
Fig. 11.2 Autocorrelation plot for the AR(1) series. Dashed lines: 95% confidence intervals
assuming white noise input.
The lines:
ta<-1
tb<-1200
lt<- length(y[ta:tb])
m.temp<- mean(y) # time average (sample mean)
m.temp
se.temp<- sd(y)/sqrt(lt) # standard error of the m.temp
se.temp
give the time average ¯y = 0.5928 and the standard error of the time average ˆσt =
0.06977. Note that ¯y = 0.5928 is a time average (that is the reason for the subscript
‘t’), the arithmetic average of the i−th outcome of the process, in which the y
i
’s form
an i.i.d. sample. Another realization of the process can be obtained with another seed
in the command set.seed(.). A different time average will result. For instance, with
set.seed(3) it results ¯y = −0.1053 and ˆσt = 0.06490.
We also recall that, for an AR(1) process, the standard error can be analytically
obtained:
Var [yi
] = sd2
1 − φ2
in our case is sd = 1, so the standard error is σt = Var [yi
] /
√
l = 0.06623 (σt is the
‘theoretical’ quantity, while ˆσt is its estimate), in good agreement with ˆσt.
The following example can be given because we know the generator process. In this
manner, we can estimate the ensemble average executing a number of different trajec￾tories and performing the average at a certain time. We add the following instructions
to the code Code_11_1.R:
## Code_11_2.RAveraging time series 379
# Ensemble averages
set.seed(2) # reset random numbers if desiderata
nsteps<- 1200 # nsteps can be redefined
nhists<- 100 # number of trajectories
ta<-1 # ta and tb can be redefined
tb<-1200
lt<- length(y[ta:tb])
y<-numeric()
matr.history<-matrix(,nhists,nsteps+1) # to save each trajectory
ymed<-numeric() # to obtain the 'mean trajectory'
mstep<- 600 # compute the ensemble average at step = mstep
# to plot only splot histories out of all nhists
splot<- 8
# to prepare the plot
plot(c(1,1), type="n",ylim=c(-10,10),xlim=c(0,nsteps),cex.lab=1.3,
xlab="t",ylab="y",font.lab=3,lwd=1.5)
for(l in 1:nhists){ # starting loop on histories
y[1]<- 0
for(i in 2:nsteps){ # starting loop on steps
y[i]<- y[i-1]*phi+rnorm(1)
matr.history[l,i]<- y[i]
} # ending loop on steps
} # ending loop on histories
for(l in 1:nhists){ # to add i=1
matr.history[l,1]<- 0 }
for(l in 1:nhists) { # to plot histories from 1 to splot
if(l<=splot)lines(matr.history[l,],lty=3,lwd=1) }
for(i in 1:nsteps){
ymed[i]<- mean(matr.history[,i]) # mean trajectory
}
lines(ymed,lwd=1,col="black")
abline(v=mstep,lwd=1.5,lty=4)
# print(matr.history[,mstep]) # to print the y_{mstep}^l (for each history)
m.ens<- mean(matr.history[,mstep]) # ensemble average
m.ens
se.ens<- sd(matr.history[,mstep])/sqrt(nhists) # standard error
se.ens
Note the matrix matr.history where all the nhists trajectories (number of rows)
and mstep (number of columns) are stored. The last parameter is the step at which
the averages ¯y are computed. In Fig. 11.3 only eight trajectories (dotted lines) out
of all 100 are shown for clarity. The ‘mean trajectory’ on the eight trajectories (solid
line) and the step t = 600 (dot-dashed line) are also reported.
Using the data coming from the AR(1) process simulation, eqn (11.4), we can
compare these averages. Considering, for instance, the k-th trajectory, the time average
is given by:
(y
k
1 + y
k
2 + y
k
3 + · · · + y
k
1200)/1200
for example:
(0.0000 + (−0.0.2925 + (−0.0045) + ... + (−3.1627) )/1200 = −0.1053380 The Problem of Accuracy
0 200 400 600 800 1000 1200
−10 −5 0 5 10
t
y
Fig. 11.3 Realizations of the eqn (11.4) AR(1) process. The plotted trajectories are eight
(pointed lines). The continuous line represents the ‘mean trajectory’ on the eight trajectories.
The vertical dot-dashed line indicates the time point (t = 600) at which the average is
performed.
The ensemble average computation can be schematized as below (the subscript explic￾itly refers to the time step):
1 y600 = −2.2638.
2 y600 = 0.2798.
3 y600 = −4.3918.
... y600 = . . . .
100 y600 = 0.2230.
Bold numbers 1, 2, 3, . . . 100 enumerate the 100 trajectories, at the step 600 used for
the average computation:
(y
1
600 + y
2
600 + y
3
600 + · · · + y
100
600)/100
for example:
−2.2638 + 0.2798 + (−4.3918) + ... + 0.2230 )/100 = −0.22555
Time averages are averages in the ‘horizontal direction’ (see Chapter 3), and give
synthetic information on the time dependence of the process. Ensemble averages are
averages in the ‘vertical direction’ (see Chapter 3), and give an estimate µˆ of the mean
µ of the process. We remark that ˆµ is an ‘estimate’, since it is supposed that the 100
considered values are a sample of all the possible realization of the y600’s. So it results
in the ensemble average: ˆµ = −0.22555 and σe = 0.2549, the subscript e referring to
an ‘ensemble average’.
We have seen that if the successive realizations of the process are in the form of a
finite time series of correlated data, the accuracy associated to the estimate ˆµ of theThe batch means method 381
mean of the process (and to other characteristic features) cannot be computed it can
when we deal with an i.i.d. process. Several methods are proposed to overcome such
a problem, two of them will be discussed in the following.
11.3 The batch means method
The idea underlying the batch means method is to divide the time series realized by
a random process in pieces, or batches of the same length, to compute the mean of
each batch, and to compute the global mean of these means. The batch length has to
be large enough to ensure that realizations belonging to different batches are nearly
statistical independent, while inside each batch the correlation is retained.
Let Yn be a realization of length n of a random process:
Yn : {y1, y2, y3, . . . , yn} (11.5)
The sequence Yn is divided into h adjacent non-overlapping batches, each of length
l. For convenience, we assume that n is a multiple of h, that is n = h l. Denote the
batches as: Y
bm
i
, i = 1, . . . , h. In general, the i-th batch Y
bm
i with starting point yj
contains l elements, i.e.:
Y
bm
i ≡ (yj , yj+1, . . . , yj+l−1)
with 1 ≤ i ≤ h and i l − (l − 1) ≤ j ≤ i l.
For instance, assume n = 18, l = 3 and i = 5, so we have j = 5 × 3 − (3 − 1) = 13,
and j + 1 = 14, j + (3 − 1) = 15, then Y
bm
5 = (y13, y14, y15).
Figure 11.4 schematically shows how the sequence Yn (n = 18) is divided into h = 6
adjacent non-overlapping batches Y
bm
i
, i = 1, . . . , 6.
batches
Y
1
bm Y
2
bm Y
3
bm Y
4
bm Y
5
bm Y
6
bm
numbered observations
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
Fig. 11.4 Example of a sequence Yn, n = 18 divided into h = 6 adjacent non-overlapping
batches Y
bm
i
, i = 1, . . . , 6.
Denote the sample means of the h batches as:
µˆ1, µˆ2, µˆ3, . . . , µˆh382 The Problem of Accuracy
where:
µˆj =
1
l
X
jl
i=(j−1)l+1
yi
, j = 1, . . . , h
An estimate ˆµ for the mean µ of the process is given by the average of these h ’coarse￾grained’ or ’time-smoothing’ (Wood, 1968) observations ˆµj . So ˆµ is given by the ‘overall
mean’, or grand mean of the individual batch means:
µˆ =
1
h
X
h
i=1
µˆi
An estimate of the variance of ˆµ is given by the sample variance ˆσ
2
:
σˆ
2 =
1
h − 1
X
h
i=1
(ˆµi − µˆ)
2
(11.6)
Then the batch means estimate of the standard error is (the subscript bm stands for
‘batch means’):
σˆbm =
r
σˆ
2
h
Every batch of size l has the same mean, equal to the sample mean of the entire process
Yn, while the standard error ˆσbm depends on the length of the batches. The essential
problem is to select an appropriate batch size l. If the batch size l is sufficiently large,
the batch means ˆµi
, i = 1, . . . , h are approximately i.i.d. normal random variables
with mean ˆµ. In this case, we can apply ‘classical’ statistical methods and derive,
for instance, confidence intervals for µ. Alternatively, if l is too small, the means
µˆi
, i = 1, . . . , h remain correlated, so the results are meaningless. On the other hand,
if the number of terms in the summation eqn (11.6) becomes too small, the estimated
results might be unreliable.
A small note, n not always is a multiple of h, that is not always n = h l exactly.
In this case, we use the function floor() that returns the largest integer not greater
than the given number; for example, if n = 12 and h = 5, then floor(12/5) = 2.
With this expedient, some observations are lost, but the remaining series is divided
into batches having the same number of realizations.
The batch mean method is implemented in the R code Code_11_3.R below.
## Code_11_3.R
# Batch mean method
set.seed(2)
nsteps<- 1200 # number of iterations
phi<- 0.90
z<- rnorm(nsteps,mean=0,sd=1) # sd: the standard deviation
y<- numeric()
y[1]<- 0 # initial state of the process.
for(i in 2:nsteps) {
y[i]<- y[i-1]*phi+z[i]
}
# y # comment if you do not wish the y values printedThe batch means method 383
plot(1:nsteps,y,type="l",xlab="steps",ylab="y",cex.lab=1.3,lty=1,
font.lab=3,lwd=1.5)
# autocorrelation function
par(mai=c(1.02,1.,0.82,0.42)+0.1) # to control the margin size
corr<-acf(y,type="correlation",lag.max=50,plot=TRUE,main=" ",
xlab="lag",ylab="ACF",cex.lab=1.2,cex.axis=1.2,font.lab=3,lwd=1.5)
# corr # comment if you do not wish the acf values printed
ta<-1
tb<-1200
lt<- length(y[ta:tb] )
# y[ta:tb] # comment if you do not wish the y[ta:tb] values printed
m.temp<- mean(y) # time average
m.temp
se.temp<- sd(y)/sqrt(lt) # standard error of the m.temp (sample mean)
se.temp
# .....................................................
# batch means method
set.seed(2) # reset random numbers if desired
nsteps<- 1200 # nsteps can be redefined
phi<- 0.90
z<- rnorm(nsteps,mean=0,sd=1)
y<- numeric()
y[1]<- 0 # initial state of the process.
for(i in 2:nsteps) {
y[i]<- y[i-1]*phi+z[i]
}
ta<- 1 # ta, tb, redefined; the code can run autonomously
tb<- nsteps
x<- numeric()
mbatch<- numeric()
se.batch<- numeric()
se.sebm<- numeric()
l<- numeric()
mj<- numeric()
n<- length(y[ta:tb] ) # n is the length of the series
m.temp<- mean(y) # time average (reported again)
m.temp
se.temp<- sd(y)/sqrt(n) # standard error of the m.temp (sample mean)
se.temp
## the interval [ta,tb] is divided in batches
## each batch has l observations, i.e., l = batch size
l<- c(1,5,10,15,20,25,30,40,50,60,70,80,90,100,120)
# one can choose the initial value of l (lbmi) and the final one (lbmf)
lbmi<- 1
lbmf<- 15
h<- floor(n/l) # number of batches for each l
# starting most external loop on the batch size l
for (k in lbmi:lbmf) {
j<- ta-l[k]
for (i in 1:h[k]) { # starting loop inside each of the h batches
j<- j+l[k]
mj[i]<-mean(y[j:(j+(l[k]-1))]) # mean of each batch
} # ending loop inside each h batch
# print(mj[1:h[k]]) # to print the mean of each batch
mbatch[k]<-mean(mj[1:h[k]])
se.batch[k]<- sd(mj[1:h[k]])/sqrt(h[k])
se.sebm[k]<- se.batch[k]/sqrt(2.*(h[k]-1))384 The Problem of Accuracy
} # ending loop on the batch size l
# summary of quantities in [lbmi:lbmf]
l[lbmi:lbmf] # batch size
h[lbmi:lbmf] # corresponding number of batches
mbatch[lbmi:lbmf] # check: they must be all equal
se.batch[lbmi:lbmf]
se.sebm[lbmi:lbmf]
par(mai=c(1.02,1.,0.82,0.42)+0.1)
plot(l[lbmi:lbmf],se.batch[lbmi:lbmf], type="b",ylim=c(0.05,0.42),
xlab="batch size",
ylab=expression(hat(sigma)[bm]),
font.lab=3,lwd=1.5,cex.lab=1.2)
arrows(l,se.batch, l,se.batch+se.sebm, length=0.05,angle=90)
arrows(l,se.batch, l,se.batch-se.sebm, length=0.05,angle=90)
The first part of the code, up to batch means, is a copy of Code_11_1.R reported for
convenience. The time average of the series and its standard error are computed again.
As we have seen, the time average ¯y (m.temp) results in 0.5928, and the standard error
σt (se.temp) is equal to 0.06977.
The part of the code that follows can run autonomously, if desired. The code is
structured in two loops. The outer is on the number of batches h or, equivalently,
on the size of the batches l. It is worth noticing that the code can vary the length
of the batches to find the most appropriate one. Suppose, for instance, h = 40. In
that case the series Yn, n = 1200, is divided into 40 batches and each batch contains
l = 1200/40 = 30 observations.
The inner loop is inside each batch h. For each batch with l[k], the code computes
the mean of each batch mj[i] (ˆµi
, i = 1, . . . , h) and the standard error of the mean
se.batch[l] (ˆσbm).
The accuracy of the estimate of ˆσbm, se.sebm[hi:hf] (s.e.(ˆσbm)) can also be
estimated. It is given by (see also Flyvbjerg and Petersen, 1989):
s.e.(ˆσbm) = s
D4 − σˆ
4
bm
4(h − 1)ˆσ
2
bm
(11.7)
where:
D4 =
1
h
X
h
i=1
(ˆµi − µˆ)
4
in the normal approximation:
s.e.(ˆσbm) = ˆσbm ×

1
(2. ∗ (h − 1)1/2
(11.8)
The final results are:
batch size l[hi:hf]
[1] 1 5 10 15 20 25 30 40 50 60 70 80 90 100 120
corresponding number of batches h[hi:hf]
[1] 1200 240 120 80 60 48 40 30 24 20 17 15 13 12 10The batch means method 385
mbatch[hi:hf] mean of each batch with different l. Check: they must be all the same
and equal to the time average m.temp.
[1] 0.5928017 0.5928017 0.5928017 0.5928017 0.5928017 0.5928017
[7] 0.5928017 0.5928017 0.5928017 0.5928017 0.5903281 0.5928017
[13] 0.5952046 0.5928017 0.5928017
standard error se.batch[hi:hf] (ˆσbm) of ˆµ.
[1] 0.06976656 0.14517286 0.18872174 0.21394611 0.23450259 0.25353158
[7] 0.25251638 0.27594069 0.29530061 0.24951920 0.24837992 0.26547587
[13] 0.22170795 0.33959625 0.26176558
standard error se.sebm[hi:hf](s.e.(ˆσbm)) of the standard error ˆσbm, computed with
eqn (11.8).
[1] 0.001424698 0.006640052 0.012233014 0.017020639 0.021587714 0.026149785
[7] 0.028591850 0.036232788 0.043539699 0.040477359 0.043907782 0.050170223
[13] 0.045255946 0.072402163 0.061698740
Figure 11.5 shows ˆσbm, the estimate of the standard error of ˆµ, as a function of the
size l of the batches. The error bars of these estimates are also added.
0 20 40 60 80 100 120
0.1 0.2 0.3 0.4
batch size
σbm
^
Fig. 11.5 Estimated standard errors ˆσbm of the estimated mean ˆµ of eqn (11.4) AR(1)
process, as a function of the size of the batches l. The error bars represent the standard error
s.e.(ˆσbm), eqn (11.8) of ˆσbm.
It appears that ˆσbm increases up to around l = 25 (h = 48), and it remains
almost constant up to l = 80 (h = 15). It is said that ˆσbm has reached a plateau.386 The Problem of Accuracy
The mean of the values of ˆσbm in the interval l = [25, 80] (h = [48, 15]) = 0.2630 can
be assumed as the standard error of ˆµ. The value ˆσbm = 0.2630, derived from one
series only, appears in full agreement with σe = 0.2549 obtained from an ensemble of
100 simulated trajectories. Note, in passing, that with l = 1 the series remains as the
original one, and obviously ˆσbm (l = 1) is equal to σt = 0.06977.
Up to l < 25, the batch means ˆµi
, i = 1, . . . , 40 are not independent, as is also
shown in Figure 11.6 (left) with l = 10 (h = 120), where the large spike at lag 0 is
followed by a decreasing wave that alternates ACF bars exceeding the positive and
negative dashed lines. Instead, Figure 11.6 (right) shows that for l = 60 (h = 20) the
µˆi
’s become approximately independent, and only the lag 0 ACF is greater than the
95% confidence limits.
0 5 10 15
−0.4 0.0 0.4 0.8
lag
ACF
0 10 20 30 40 50
−0.2 0.2 0.6 1.0
lag
ACF
Fig. 11.6 Autocorrelation plots for the mean of each batch mj. Left: h = 120 (l = 10), right:
h = 20 (l = 60). Dashed lines: 95% confidence intervals assuming white noise input.
As we have already said, the means of each batch, ˆµj = 0.5928, are all equal,
independently of the length of the batches, and equal to the ‘grand mean’ estima￾tor ˆµ. However, looking at the mbatch[hi:hf] values, we see that mbatch[11] and
mbatch[13] are slightly different from 0.5928, due to the effect of the function floor():
for those batch means, it is not exactly n = h l.
This section ends with some historical and bibliographical references. The idea of
breaking the original chain into independent statistical pieces was already exploited
for a long time, for instance, Wood (1968), Friedberg and Cameron (1970), Landau
(1976), Binder (1992), where the ˆµi
’s are named ‘coarse-grained’ or ‘time-smoothing’
values.
The batch means method was reintroduced in statistical literature by Carlstein
(1986), so sometimes it is called with the name of ‘Carlstein’s method’ or ‘Carlstein’s
rule’ (Hall et al., 1995), or ‘bootstrap with disjoint blocks’ (Kunsch, 2015 ¨ ).
A number of papers deal with the choice of the batch lengths, usually without
the possibilities of varying l in a unique run, see among others Whitmer (1984) and
Morales et al. (1990). An exception is in Flyvbjerg and Petersen (1989), where the
batch sizes are automatically determined by halving them each time, rather than
tentatively varying them in search of a plateau. We also point out that the behaviourThe moving block bootstrap method 387
approaching the plateau is an indirect indication of the strength of correlation.
Geyer (2011) applies the batch means method to an AR(1) process, Bennett et al.
(1981) propose a variant of the batch means method by inserting spacers between the
batches of observations. Pedrosa and Schmeiser (1993) use ‘overlapping batch means’,
in which the batches have realizations in common. This variant is also the first step of
the moving block bootstrap method discussed in the next section.
11.4 The moving block bootstrap method
The main difference with the ‘classic’, or i.i.d., bootstrap is that in the moving block
bootstrap (MBB), blocks of observations are resampled instead of single observations.
The main difference with the batch means method is that in the MBB method the
batches (or ‘blocks’) are overlapped.
As in the i.i.d. bootstrap, also in the MBB a number of replications are formed to
compute the statistic of interest on each of them. As in the batches means method,
also in the MBB the block size must be ‘sufficiently’ large such that the correlation
between observations belonging to different blocks has decayed off. Reliable estimates
cannot be obtained if the number of blocks is too small.
If the assumption of independent random variables is violated, as occurs when
observations are serially correlated, the bootstrap is not applicable in the form sketched
above, because this method ignores the dependence structure of the data. However, the
problem can be overcome in a way which conserves the spirit of Efron’s original idea,
that is by resampling blocks of observations instead of individual observations. This
improvement makes the original bootstrap more robust against serial dependence.
11.4.1 Introduction to the MBB
Theoretical foundations of the MBB are in Kunsch (2015 ¨ ) and Liu and Singh (1992). A
short account is reported in Mignani and Rosa (1995) where the MBB was applied for
the first time in statistical mechanics in a study concerning the Ising model. Perhaps,
the first idea to exploit such computer-intensive methods for Monte Carlo correlated
data is found in a paper on lattice gauge theory (Gottlieb et al., 1986).
Consider the discrete parameter stationary time series eqn (11.5)
Yn : {y1, y2, y3, . . . , yn}
Let ˆµ be an estimate of the mean of the process µ and ˆσbm the standard error of ˆµ,
estimated via the batch means method. In the example presented in Code_11_3.R, we
have ˆµ = 0.5928 and ˆσbm = 0.2630.
In order to estimate σ through the MBB, the observed time series Yn, is divided
into overlapping blocks of l observations each and all possible contiguous blocks of
length l are considered. Let us follow the example below.
We have seen above how to divide the realizations of the process Yn, n = 1200 in
h = 40 batches with l = 30 observations each. Consider now only n = 18 observations
with h = 6
0.0000000, 0.1848492, 1.7542096, 0.4484130, 0.3233199, 0.4234082,
1.0890221, 0.7404219, 2.6508536, 2.2469813, 2.4399339, 3.1776933,
2.4672286, 1.1808367, 2.8449820, 0.2494147, 1.1030779, 1.0285768388 The Problem of Accuracy
and suppose we divide them into h = 6 batches with l = 3 (see Figure 11.4):
batch no. 1: 0.0000000, 0.1848492, 1.7542096
batch no. 2: 0.4484130, 0.3233199, 0.4234082
............................................
batch no. 6: 0.2494147, 1.1030779, 1.0285768
With the MBB, we again divide the series into batches, now calling them blocks,
with l = 3, but the blocks are overlapped and all possible contiguous blocks are
considered. In this way, q blocks are obtained, with q = n − l + 1, here q = 16 (for a
better choice of this number, see the following subsection). The 16 blocks are reported
below with four significant decimal digits.

0.0000, 0.1848, 1.7542


1

0.1848, 1.7542, 0.4484


2

1.7542, 0.4484, 0.3233


3

0.4484, 0.3233, 0.4234


4

0.3233, 0.4234, 1.0890


5

0.4234, 1.0890, 0.7404


6

1.0890, 0.7404, 2.6508


7

0.7404, 2.6508, 2.2469


8

2.6508, 2.2469, 2.4399


9

2.2469, 2.4399, 3.1776


10

2.4399, 3.1776, 2.4672


11

3.1776, 2.4672, 1.1808


12

2.4672, 1.1808, 2.8449


13

1.1808, 2.8449, 0.2494


14

2.8449, 0.2494, 1.1030


15

0.2494, 1.1030, 1.0285


16
Call the blocks Q1, Q2, . . . , Qq. For example, Q3 = |1.7542, 0.4484, 0.3233|3. In general
the i-th block Qi with starting point yi contains l elements, i.e.:
Qi ≡ (yi
, yi+1, . . . , yi+l−1)
with 1 ≤ i ≤ q.
From these q blocks Qi (i = 1, . . . , q) we draw h (h l = n) blocks at random with
replacement. The starting point of each block is randomly selected from a uniform
distribution of integers (1, . . . , n), so that all Qi
’s are equally likely to be drawn. The
h selected blocks, placed one after another, form the new full size series Q∗
:
Q
∗ = Q
∗
1
, Q∗
2
, . . . , Q∗
h
where Q∗
1
is one of the possible Qi
. In this sense, the blocks are ‘moving’: each Qi may
lie in any point of the new formed series Q∗
.
In the example, a resampled series is formed with h = 6 blocks. If the random
number i is equal to 4, Q4 ≡ (y4, y5, y6), then Q∗
1 = Q4 = 0.4484, 0.3233, 0.4234. A
further block is picked up, for instance i = 10, so Q∗
2 = Q10 = 2.2469, 2.4399, 3.1776.
This procedure is executed h times, so that the first bootstrap sample is formed, for
instance: Q∗ = Q4, Q10, Q1, Q10, Q7, Q8 that is:
0.4484, 0.3233, 0.4234, 2.2469, 2.4399, 3.1776,
0.0000, 0.1848, 1.7542, 2.2469, 2.4399, 3.1776,
1.0890, 0.7404, 2.6508, 0.7404, 2.6508, 2.2469
Note that some blocks may be extracted more than once, while others are never drawn.
The procedure is now perfectly analogous to that of the i.i.d. bootstrap. We are
interested in the standard error of the estimate ˆµ, that is computed by the arithmetic
mean:
µˆ =
1
n
(y1 + y2, + . . . , yn)The moving block bootstrap method 389
in the same way, on the first bootstrap sample, the first bootstrap replication is com￾puted:
ˆθ
∗
1 =
1
h
(Q
∗
1 + Q
∗
2 + Q
∗
3
, + . . . , Q∗
h
)
Recall that, in general, the ‘bootstrap replication’ is the value of the statistics referred
to the bootstrap sample (see Appendix A).
This drawing procedure is repeated many times, to obtain B (B ≈ 100 − 1000)
bootstrap replications:
ˆθ
∗
1
,
ˆθ
∗
2
,
ˆθ
∗
3
, . . . ,
ˆθ
∗
B
From the bootstrap replications, a bootstrap estimate of the standard error of the
estimate ˆµ is derived. Call it ˆσ
∗
:
σˆ
∗
(
ˆθ
∗
) = "X
B
b=1
(
ˆθ
∗
b − ¯θ
∗
)
2
B − 1
#1/2
(11.9)
where:
¯θ
∗ =
X
B
b=1
ˆθ
∗
b
B
(11.10)
As in the batch means method, we search for blocks sizes l for which observations
belonging to different blocks are independent of one another. In practice, the plot of
σˆ
∗ vs l shows that ˆσ
∗
increases until it reaches a plateau, where it remains nearly
constant. This is a sign that the blocks are actually i.i.d. random variables under the
MBB scheme and, at the same time, inside each block the correlation is retained. For
a review of variants of this method, see Politis (2003)
11.4.2 The MBB in R
An extremely schematic example allows us to further enter into the details of the
MBB method, directly comparing it with the batch means method. Suppose we have
an observed series of length n = 12, in which the data are numbered as:
Yn : {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}
If the batch means method is applied, the series is divided, for instance, in h = 4
batches, of length l = 3:
batch no. 1 1+2+3
3
= ¯y1
batch no. 2 4+5+6
3
= ¯y2
batch no. 3 7+8+9
3
= ¯y3
batch no. 4 10+11+12
3
= ¯y4
In general:
y¯i =
1
l
X
i l
j=i l−(l−1)
y
i
j
where j refers to the j-th observation in the i-th batch. For example:390 The Problem of Accuracy
y¯2 =
1
3
X
2×3
j=2×3−(3−1)
y
i
j =
1
3
(y
2
4 + y
2
5 + y
2
6
)
The estimate ˆµ of the mean of the process µ is given by:
µˆ =
1
h
X
h
i=1
y¯i =
1
lh
X
h
i=1
X
i l
j=i l−(l−1)
y
i
j
If the batch size is large enough, the sample means of the batches are approximately
independent. Their sample variances are estimates of the variance of each ¯yi
, given by
eqn (11.6).
Compare now the MBB procedure with the batch means method. Consider all
possible contiguous blocks of l = 3 observations each, that is:
[1 2 3] [2 3 4] [3 4 5] [4 5 6] [5 6 7]
[6 7 8] [7 8 9] [8 9 10] [9 10 11] [10 11 12]
Now the number of blocks is no more h = n/l = 4 (still supposing n = h l = 12), but
q = n − l + 1, in the example q = 12 − 3 + 1 = 10.
In the example, if only q = 10 blocks are considered, the observation 12 appears
only in the block [10 11 12], while all the observations yi
’s must be present l times.
To overcome this drawback, observations 1 and 2 are added. In general, to take into
account all the observations, l−1 initial observations have to be added, so the number
of blocks becomes q + (l − 1), here = 12 with the two added blocks [11 12 1] and
[12 1 2]. In this way, also the observations 1 and 2 are present l = 3 times.
As a further example, suppose l = 5, so q = 12 − 5 + 1 = 8, but 5 − 1 observations
must be added after the last one, and the number of blocks becomes q = 12.
[1 2 3 4 5] [2 3 4 5 6]
[3 4 5 6 7] [4 5 6 7 8]
[5 6 7 8 9] [6 7 8 9 10]
[7 8 9 10 11] [8 9 10 11 12]
adding
[9 10 11 12 1] [10 11 12 1 2]
[11 12 1 2 3] [12 1 2 3 4]
More formally, using the notion of congruence, it can be written:
yn+r = y(n+r)( mod n)
For instance, (n = 12, l = 3) with r = 2:
y12+2 = y(12+2)( mod 12) = y2
so in the place of the observation 14, which does not exist, the observation 2 is used
to form the block [12 1 2].The moving block bootstrap method 391
The idea of wrapping the observations around in a circle, before forming the blocks,
is due to Politis and Romano (1992), who call the method ‘circular block-resampling
bootstrap’. By this method, the bootstrap distribution is centred around its mean
instead of around the sample mean. This allows preventing the bias due to the reduced
weight assigned to the observations yi
’s not appearing l times in each block, that is if
i > n − l + 1. Incidentally, similar procedures, involving data wrapping in a circle are
exploited in statistical mechanics to reduce border effects. These kinds of ‘tricks’ are
known as ‘periodic boundary conditions’.
Figure 11.7 schematically shows how to look at the observations arranged in a
circle. All of them are present l times in each block, represented by an arc.
10
11
12
1
2
3
4
5
6
7
8
9
Fig. 11.7 Observations wrapped around in a circle to have all of them present in each block,
depicted with arcs, an equal number of times (n = 12, l = 3).
Let us see how to construct a resampled series in the example with l = 5. The MBB
method in implemented in the R code Code_11_4.R, where the function ceiling() is
used, returning the smallest integer greater than the given number.
In that case, ceiling(12/5) = 3, so we need three blocks, and the resampled
series will have 15 observations. The beginning of each block i is picked at random
1 ≤ i ≤ 12). For instance:
i = 10 → block: [10 11 12 1 2]
i = 12 → block: [12 1 2 3 4]
i = 1 → block: [1 2 3 4 5]
The bootstrap sample is then:
(10 11 12 1 2 12 1 2 3 4 1 2 3 4 5)
On this bootstrap sample the bootstrap replication is computed. From here on￾wards, all proceeds analogously to the i.i.d. bootstrap. If the statistics of interest is
the standard error of the mean, the mean of the first bootstrap sample above is com￾puted. This computation is repeated B times to obtain the mean of the B bootstrap392 The Problem of Accuracy
replications eqn (11.10) and a bootstrap estimate eqn (11.9) of the standard error of
the mean ˆµ.
## Code_11_4.R
# Bootstrap
set.seed(2)
nsteps<- 1200 # number of iterations
phi<- 0.90
z<- rnorm(nsteps,mean=0,sd=1) # sd: the standard deviation
y<- numeric()
y[1]<- 0 # initial state of the process.
for(i in 2:nsteps) {
y[i]<- y[i-1]*phi+z[i]
}
# y # comment if you do not wish the y values printed
plot(1:nsteps,y,type="l",xlab="steps",ylab="y",cex.lab=1.3,lty=1,
font.lab=3,lwd=1.5)
# autocorrelation function
par(mai=c(1.02,1.,0.82,0.42)+0.1) # to control the margin size
corr<-acf(y,type="correlation",lag.max=50,plot=TRUE,main=" ",
xlab="lag",ylab="ACF",cex.lab=1.2,cex.axis=1.2,font.lab=3,lwd=1.5)
# corr # comment if you do not wish the acf values printed
ta<-1
tb<-1200
lt<- length(y[ta:tb] )
# y[ta:tb] # comment if you do not wish the y[ta:tb] values printed
m.temp<- mean(y) # time average
m.temp
se.temp<- sd(y)/sqrt(lt) # standard error of the m.temp (sample mean)
se.temp
# .....................................................
# Moving Block Bootstrap (MBB) method
set.seed(2) # reset random numbers if wanted
nsteps<- 1200 # nsteps can be redefined
phi<- 0.90
z<- rnorm(nsteps,mean=0,sd=1)
y<- numeric()
y[1]<- 0 # initial state of the process.
for(i in 2:nsteps) {
y[i]<- y[i-1]*phi+z[i]
}
ta<- 1 # ta, tb, redefined; the code can run autonomously
tb<- nsteps
x<- numeric()
xb<- numeric()
ntot<- numeric()
mb<- numeric()
nbl<- numeric()
seb<- numeric()
mmbb<- numeric()
sembb<- numeric()
se.sembb<- numeric()
n<- length(y[ta:tb]) # n is the length of the series
x[1:n]<- y[ta:tb] # x[1:n] are named the realizations
# x # to see the realizations in [ta,tb]
m.temp<- mean(y) # time average (reported again)
m.tempThe moving block bootstrap method 393
se.temp<- sd(y)/sqrt(n) # standard error of the m.temp (sample mean)
se.temp
# l = block length, i.e., each block has l observations
l<-c(1,2,4,6,8,10,15,20,25,30,35,40,45,50,55,60,65)
llmb<- length(l)
# one can choose the initial value of lmb (lmbi) and the final one (lmbf)
lmbi<- 1
lmbf<- 17
h<- n/l # number of blocks
q<- n-l+1 # number of overlapped blocks
# for each block, B replications are computed
B<- 400 # number of replications
# starting most external loop on the block length
for (k in lmbi:lmbf) {
# ... periodic boundary conditions:
# ... each x[i] has to be appeared l times
# ... adding the first l-1 observations
# total observations, after adding the first l-1 observations
ntot[k]<- n+(l[k]-1)
n1<- n+1
if(l[k]>1){ # if l[k]=1 the loop to form the "new series" is skipped
ii=0
for (i in n1:ntot[k]) {
ii<- ii+1
x[i]<- x[ii]
} # ending "if l[k]=1"
}
# x is the "new series" = the original one + (l-1) initial realizations
# print(x) # to see the "new series"
nbl[k]<- ceiling(n/l[k]) # number of blocks to form the resampled series
# # starting loop on the B replications
for (b in 1:B) { # loop on replications
nn<- 0
for (j in 1:nbl[k]) { # loop on the number of sampled blocks
i<- sample(n,1,replace=TRUE) # beginning of a block picked at random
# a block with length l is formed (initial 'i' included)
for (ll in i:(i+(l[k]-1))) {
nn<- nn+1
xb[nn]<- x[ll]
} # the block is finished
} # ending loop on the number of sampled blocks
# print(xb) # to print the replications
mb[b] <- mean(xb)
# print(mb[b]) # to print the mean of each replication
seb[b]<- sqrt(var(xb))
# print(seb[b]) # to print the s.e. of each replication
} # ending loop on the replications
mmbb[k]<- mean(mb)
sembb[k]<-sum((mb-mmbb[k])^2/(B-1))^(1/2) # to plot
se.sembb[k]<- sembb[k]/sqrt(2.*(nbl[k]-1))*sqrt(2./3.)
} # ending most external loop on the length of blocks
# summary of quantities in [lmbi:lmbf]
l[lmbi:lmbf]
h[lmbi:lmbf]
q[lmbi:lmbf]
ntot[lmbi:lmbf]
nbl[lmbi:lmbf]
mmbb[lmbi:lmbf]
sembb[lmbi:lmbf]
se.sembb[lmbi:lmbf]394 The Problem of Accuracy
par(mai=c(1.02,1.,0.82,0.42)+0.1)
plot(l[lmbi:lmbf],sembb[lmbi:lmbf], type="b",ylim=c(0.05,0.35),
xlab="block length",font.lab=3,lwd=1.5,cex.lab=1.3,
ylab=expression(hat(sigma)*"*"))
arrows(l,sembb, l,sembb+se.sembb, length=0.05, angle=90)
arrows(l,sembb, l,sembb-se.sembb, length=0.05, angle=90)
As in Code_11_3.R the first part of the above code, up to MBB, simulates the AR(1)
process eqn (11.4). The following part of the code is similar to that of Code_11_2.R.
The series is divided into h blocks, each containing l = n/h data values, namely l is
the ‘block length’ varying inside a loop going from lmbi to lmbf (in the code equal to 1
and 17, respectively), that is l = 1, . . . , 65. If, for instance, l = 40, this means that the
series Yn, n = 1200 is divided into 30 blocks, each block containing l = 1200/30 = 40
observations.
The outer loop (for (k in lmbi:lmbf) ...) is on the number of the block lengths
l<- c(1,2,4,...,65), given at the beginning, so that the blocks are not yet over￾lapped. The observations are prepared to be replicated with two operations. As a first
step, the observations are wrapped, adding the first l − 1 observations: ntot[k]<-
n+(l[k]-1). Then the length of blocks to constitute the resampled series is up￾dated: nbl[k]<- ceiling(n/l[k]). The loop on the replications (for (b in 1:B)
...) is executed to assemble the bootstrap samples. The beginning of each ‘mov￾ing’ block is chosen by a random sampling with replacement from the vector 1 . . . n:
i<- sample(n,1,replace=TRUE). Finally, the average of the B bootstrap replications
(eqn 11.10) and a bootstrap estimate (eqn 11.9) of the standard error of each block h
(h[lmbi:lmbf]) is computed. The result obtained with B = 400 is:
block length l[lmbi:lmbf]
[1] 1 2 4 6 8 10 15 20 25 30 35 40 45 50 55 60 65
corresponding number of non-overlapping blocks h[lmbi:lmbf]
[1] 1200.00000 600.00000 300.00000 200.00000 150.00000 120.00000
[7] 80.00000 60.00000 48.00000 40.00000 34.28571 30.00000
[13] 26.66667 24.00000 21.81818 20.00000 18.46154
number of overlapping blocks q[lmbi:lmbf] (q = n − l + 1)
[1] 1200 1199 1197 1195 1193 1191 1186 1181 1176 1171 1166 1161 1156 1151 1146
[16] 1141 1136
length of the blocks ntot[lmbi:lmbf] after adding the first l − 1 observations
[1] 1200 1201 1203 1205 1207 1209 1214 1219 1224 1229 1234 1239 1244 1249 1254
[16] 1259 1264
length of the blocks after the ceiling function nbl[lmbi:lmbf]
[1] 1200 600 300 200 150 120 80 60 48 40 35 30 27 24 22
[16] 20 19
means of each bock with different l, mmbb[lmbi:lmbf]. These are close to each other
and close to the time average m.temp = 0.5928017.The moving block bootstrap method 395
[1] 0.5884565 0.5991287 0.5898515 0.5980998 0.6150934 0.6047890 0.6034414
[8] 0.6026749 0.5775105 0.5785146 0.6138597 0.5456157 0.5511164 0.5808206
[15] 0.5775882 0.5929105 0.5837608
bootstrap estimate for the standard error of the estimate ˆµ sembb[lmbi:lmbf] (ˆσ
∗
)
[1] 0.07006526 0.09598453 0.13178510 0.15841488 0.17378288 0.19585105
[7] 0.21543579 0.23066210 0.25532195 0.24458536 0.26150826 0.26335080
[13] 0.26030662 0.25882132 0.28130114 0.26969193 0.27711156
se.sembb[lmbi:lmbf]
standard error \texttt{se.sembb[lmbi:lmbf]}($\mbox{s.e.} (\hat\sigma^*)$) of
the standard error $\hat\sigma^*$.
[1] 0.001168241 0.002264265 0.004400176 0.006483489 0.008219648 0.010365537
[7] 0.013994058 0.017337625 0.021501987 0.022611925 0.025893176 0.028234170
[13] 0.029473921 0.031158426 0.035440612 0.035721569 0.037710106
It was observed, (Kunsch, 2015 ¨ ), (Hall et al., 1995), that the variance of the MBB
estimators is less than that of the batch means estimators, because in the MBB the
blocks are overlapped. The reduction with respect to the batch means s.e.(ˆσbm) is
about a factor p
(2/3).
Figure 11.8 shows ˆσ
∗
, bootstrap estimate of the standard error of the estimate ˆµ,
as a function of the block length l.
0 10 20 30 40 50 60
0.05 0.15 0.25 0.35
block length
^σ
*
Fig. 11.8 Estimated standard errors ˆσ
∗
of the estimated mean ˆµ of eqn (11.4) AR(1) process,
as a function of the block length l. The error bars represent the standard error s.e.(ˆσ
∗
) of ˆσ
∗
.
It appears that ˆσ
∗
increases up to around l = 25, where it reaches a plateau,
remaining nearly constant up to l = 65. As before, that is a sign that the mutual
independence of the blocks has been achieved. The mean of the values of ˆσ
∗
in the
interval l ∈ [25, 65] is 0.2636, which can be assumed as the standard error of ˆµ, is
almost the same value as ˆσbm = 0.2630 obtained by the batch means method.396 The Problem of Accuracy
In essence, in the batch means method the statistics of interest are recomputed
in each of these smaller blocks of the type (yi
, yi+1, . . . , yi+l−1), while in the MBB
the statistics are recomputed in each of the new, full size series (Q∗
1
, Q∗
2
, . . . , Q∗
h
). In
Mignani and Rosa (2001) it is shown, by means of computer experiments, that there
are cases where the MBB outperforms other methods based on subseries, as the batch
means method. This happens, for instance, with highly correlated states in relatively
short chains.
For a recent review of variants of both the batch means method and the MBB, see
Politis (2003).
11.5 Convergence diagnostic with the MBB method
We will show that the MBB method, used to assign accuracy to estimates in the
presence of considerable correlations between successive realizations, can also help to
shed light on the problem of convergence.
Since the beginnings of MCMC, it has been noticed that a ‘formally’ ergodic chain
may nevertheless be ‘computationally’ non-ergodic or, in Wood and Parker’s words,
‘quasi-ergodic’ (Wood and Parker, 1957). These authors warn against situations in
which the state space may be divided into two (or more) ‘pockets’, formally belonging
to the same ergodic class, but linked through an ‘isthmus’ of states of very small, but
non-vanishing, probabilities. Suppose, for instance, that two simulations are run in
parallel. If this quasi-ergodicity arises, two very different sequences might be obtained,
not converging to a common sequence and consequently not yielding a final estimate.
Individually each sequence might not reveal any flaws, appearing to have converged
to its own stationary distribution. In such circumstances, the results from any one
sequence would be unreliable. Examples can be found in Gelman and Rubin (1992a)
and, by the same authors, in an article with a very eloquent title (Gelman and Rubin,
1992b). The notorious ‘witch’s hat’ distribution (Matthews, 1989) serves as a further
example giving the (erroneous) impression of convergence for the chain generated by
the Gibbs algorithm, even though the sequence has not explored most of the target
distribution.
A somewhat artificial example can be explanatory. Return to the code Code_8_2.R
of Chapter 8 in which the ‘target density’ is now the function:
f(x) = exp(−x
2
) x
2
(11.11)
reported in Fig. 11.9.
By applying the Metropolis-Hastings algorithm with
set.seed(2)
nsteps<- 10000
x0<- -10
delta<- 0.75
burn.in<- 2000
Fig. 11.10 is obtained. We see that the realizations are roughly around the lines x(i) = 1
and x(i) = −1, with sudden sign inversions. By changing random number sequences,
different histories are obtained, but all show this kind of ‘sign inversion’, shown in
Fig. 11.10. In such situations, speaking of convergence is obviously meaningless. AnConvergence diagnostic with the MBB method 397
−4 −2 0 2 4
0.0 0.1 0.2 0.3
x
target density
Fig. 11.9 Target density f(x) = exp(−x
2
) x
2
. The dashed lines are at the two maximum
values −1 and +1.
expedient, sometimes used in statistical mechanics, is to take the absolute values of
all the realizations. Doing this, the computed time average results to be 1.145, in this
example.
0 2000 4000 6000 8000 10000
−4 −2 0 2 4
delta = 0.75
iterations
x(i)
Fig. 11.10 Realizations of the Metropolis-Hastings algorithm with target density
eqn (11.11), and δ = 0.75. Dotted line at the burn-in = 2000 iterations, dashed line at
the two maximum values −1 and +1 of the target density.398 The Problem of Accuracy
11.5.1 The Gelman and Rubin method
A variety of methods to determine whether stationarity is achieved (convergence di￾agnostics) have been proposed. Often, different authors give different suggestions, but
all agree that there is no single method to deal with convergence, and each tool has
its pros and cons. A rather common approach, e.g. (Fosdick, 1963; Wood, 1968; Rip￾ley, 1987), consists in running more than one chain, all with the same parameters
but different starting configurations. By comparing the results, one can detect possi￾ble dangers and try to circumvent them. This approach prescription was formalized
and improved by Gelman and Rubin (G-R) and now it is one of the most popular
convergence diagnostics methods. On the other hand, it has also been observed that
sometimes many short chains appear to converge to a unique, but completely wrong,
distribution (Geyer, 1992). So that it should be more advisable to trust results based
on a single long run. For an extended overview of the efforts in this research area,
the reader is referred to the articles by (Cowles and Carlin, 1996) and by (Brooks
and Roberts, 1992), and to Chapter 8 of the book by (Robert and Casella, 1999),
where almost all convergence diagnostics available in the literature are discussed and
compared.
We use the basic version of the G-R method, as reported, for instance, in (Evans
and Swartz, 2000). It consists essentially of the following steps. Generate m different
chains at different starting values. Let the scalar x
(j)
i
be the i−th realization in the
j-th chain, (i = 1, . . . , n; j = 1, . . . , m) and y
(j)
i = g(x
(j)
i
) be the quantity of interest.
Define B the between-chain variance and W the within-chain variance, respectively,
where:
B =
n
m − 1
Xm
j=1

y
(j) − y
2
and:
W =
1
m
Xm
j=1
s
2
j
with:
y
(j) =
1
n
Xn
i=1
y
(j)
i
y =
1
m
Xm
j=1
y
(j)
and:
s
2
j =
1
n − 1
Xn
i=1

y
(j)
i − y
(j)
2
Consider the quantity:
R =
B/n + [(n − 1)/n]W
W
(11.12)
The idea is that for small n the numerator in (11.12) overestimates variability, since
starting values are typically quite dispersed, whereas W underestimates variability
because the different chains remain close to their initial values. Thus, we expect that
R becomes close to 1 when each of the m chains approaches the target distribution.Convergence diagnostic with the MBB method 399
Consider the following simple example, the simulation of the bivariate normal dis￾tribution N2(µ, Σ), where µ = (1, 2)0
is the mean vector and Σ is the covariance matrix
given by:
Σ = 
1.0 0.4
0.4 1.0

As the candidate-generating density we choose the random walk generating density
y
(u)
i = x
(u)
i + z
(u)
i u = 1, 2
where x
(1)
i
and x
(2)
i
are the two components of the chain at the i-th step, and z
(u)
i
is the i-th realization of the bivariate uniform on the interval [−δ(u), +δ(u)] (refer to
Code_8_4.R). In this example, in order to have a relative slow convergence, we choose
δ(1) = δ(2) = 2, and the starting points are x
(1)
0 = −10 and x
(2)
0 = 10. The length of
the chain is n = 4000 moves. In the following we present the results only for the first
component x
(1), because those for the second one are pretty similar.
The results obtained by the MBB method, with 200 replications, are reported in
Fig. 11.11. We see that a clear plateau is attainable only if the burn-in r is at least
500 moves. To compare the above results with those derived by the G-R diagnostics,
0.5
0.4
burn-in
[moves]
none
100
200
500
1000
block length [moves]
0.3
0.2
0.1
20 40 60 80 100
σ*
Fig. 11.11 Moving block bootstrap estimates ˆσ
∗
of the standard errors as a function of the
block length of the first component x
(1) of the bivariate normal distribution defined in the
text. The burn-in times for different runs are also reported.
we consider two approaches. In the first approach, 200 chains are generated starting
at different initial points x
(1)
0
, x
(2)
0
and the G-R method is applied to these multiple
chains, with the burn-in time equal to zero. The results, shown in Fig. 11.12 (dotted
lines), give the same indications of the MBB method, i.e. R reaches the value 1 after
about 500 iterations.
We propose a second way to exploit the G-R diagnostics, consisting of using the
MBB replications as ‘virtual’ multiple chains. To do so, we memorize all the 200 MBB
replications used to obtain the results reported in Fig. 11.11 and the G-R method is
applied to these series, regarded as multiple chains starting at different initial points.400 The Problem of Accuracy
The behaviour of R and W for such MBB replications is reported in Fig. 11.12 (solid
lines), showing that, although in the initial region the graphs of both R and W are
less smooth with respect to those derived from actual (‘true’) multiple chains (dotted
lines), also in this case the plots suggest that the chain converges after about 500
iterations.
R
moves
R
W
W
0 500 1000 1500
0.01
0.1
1
1
10
100
Fig. 11.12 Evolutions of R (scale on the left) and W (scale of the right) for the first com￾ponent x
(1) of the bivariate normal distribution defined in the text. Solid and dotted lines
refer to MBB replications and actual multiple chains, respectively.
The conclusion (if any) may be summarized by the old idiom ‘to kill two birds with
one stone’, namely, to assess accuracy and explore convergence with only the MBB
method. The MBB is a very powerful non-parametric technique for handling statis￾tical errors in MCMC estimates. So there are several reasons to rely on the MBB to
estimate the simulation accuracy. When no plateau appears to be clearly distinguish￾able, it is not hard to search for a possible one by increasing the burn-in period. If we
succeed in it, we are hopeful that the accuracy of estimates has been derived only after
convergence has been reached. Eventually, having the MBB replications available, one
can try to perform further checks, for example by means of the G-R method or with
other diagnostic tools based on multiple chains, without needing to execute further
runs.
11.6 Exercises
Exercise 11.1 Write a code to estimate, by means of the bootstrap, the standard error of
the standard error both plug-in and correct.
Exercise 11.2 Demonstrate the expression in eqn (11.7).
Hint: use the operator notation for the variance of σˆ
2
bm.
Exercise 11.3 Write a code to compare the distribution of means of a set of trajectories
with that of a unique trajectory but obtained with bootstrap.Exercises 401
Exercise 11.4 Write a code to assign the standard error to the estimate by the batch means
method and by the moving block bootstrap method. The target density of the MCMC algo￾rithm is the standard normal.
Exercise 11.5 A the end of Section 11.5.1 we proposed to exploit the G-R diagnostics using
the MBB replications as ‘virtual’ multiple chains. Consider the simulation of the bivariate nor￾mal distribution leading to Fig. 11.11 and implement a code in R using the MBB replications
as described in the text.12
Spatial Analysis
I think there's a great beauty to having problems. That's one of the ways we learn.
Herbie Hancock, Twitter
In the previous chapters the concept of the stochastic process was applied to time
series. Many applications in the earth sciences and engineering, including geology,
soil science, crop science, ecology, forestry and atmospheric sciences are dealing with
models that compute the dependence of properties measured at different locations,
therefore as a function of space. Geostatistics is the discipline that applies statis￾tical concepts to spatial-dependent data. Geostatistics provides concepts and tools
employed to derive information of spatial continuity common in natural phenomena
and processes.
The application of random processes to problems of spatial analysis began in the
early years of the twentieth century, when engineers and statisticians investigated
methodologies for evaluating mineral resources in mining operations. Danie G. Krige,
a South African statistician and mining engineer, was a pioneer in the new-born field of
geostatistics (Krige, 1951). The Kriging technique was later formalized by the French
mathematician and engineer Georges Matheron (Matheron, 2019), who also coined the
term Kriging, in honour of Krige’s contribution.
Many books are available on geostatistics, with both general concepts and applica￾tions to specific disciplines (Isaaks and Srivastava, 1989; Cressie, 1993; Kitanidis, 1997;
Webster and Oliver, 2007; Leuangthong et al., 2008; Chun and Griffith, 2013). The
fundamental concepts of geostatistics are based on classical statistics including data
distribution tools (mean, median, standard deviation) and bi-variate statistics (covari￾ance, correlation coefficient, t-test and F-test), integrated into concepts of geometry
such as distances, angles, lines, polygons and so forth.
Sources of spatial data are many, including satellite imagery, drone images, Geo￾graphical Positioning System (GPS), data acquired by direct sampling on the ground,
satellite radar, digital representation of maps, contours, digital elevation models and
many others. The integration of earth observation methods and computing techniques
are studied by the field of geo-informatics, which deals with the science and technology
of the structure of spatial information, classification, qualification, storage, modelling
and others.
12.1 Geostatistical perspective
The fundamental notions of stochastic processes were described in Chapter 2. Random
variables Xt’s with probability space (Ω, F, P) take given values in a measurable space,Geostatistical perspective 403
whose values are called states. All the values taken by a variable are called its state
space and it was denoted as S. In the examples presented, random variables changed
as a function of time, leading to time series analysis. Clearly, a natural process such
as precipitation or air temperature has also a spatial component, namely it changes
with location.
The formal concept of a stochastic process applied to a spatial process is the one
of random fields. A random field is defined as Z(s), where s is a vector indicating
a position in space, s = (x, y, z). The vector is continuous in R, R2 or R3
. Indeed,
the domain can be one-, two- or three-dimensional. If a transect for a given variable
is measured the process is simply one-dimensional, if a variable changes on a surface
(such as precipitation) the problem is two-dimensional. However, in geology, hydroge￾ology, seismology and soil sciences the domain is often three-dimensional, R3
, since the
variable of interest often displays a variation not only on the x, y coordinates (surface),
but also with depth (z).
Z(s) encompasses all the possible states and, analogously with the concepts applied
to time series, all the values taken by a variable are called the state space. A probability
density function of a random field is defined, encompassing all values in the space.
Associated with the probability density function are statistical moments such as the
expected value, variance and covariance functions.
Spatial random fields are random fields whose properties are determined by the
position in space that are spatially correlated. A spatial random variable is then
{Zs; s ∈ T}, where the set T is the parametric space. T is in R or its subsets. In
analogy with the concepts applied to a continuous-time (-parameter) stochastic pro￾cess, T can range from T = (−∞, ∞), or T = [0, ∞), or T = [a, b), or T = [a, b].
Mathematically, they are a set of a random variable Zs tagged with a location s, and
the location is part of a domain Ω:
{Z(s) : s ∈ Ω} (12.1)
where Z is the random variable over the locations s, where s is a spatial index.
In analogy with the definition of a random variable for time series, all the variables
Zs are defined in the same space Ω, then each Zs is a random function of two arguments
of different nature: the variable of probabilistic nature ω ∈ Ω indicates the event, the
variable of mathematical nature s ∈ T creates an order in the random variables family.
For instance, the variable with probabilistic nature could be the concentration of a
given chemical species in an agricultural field, and the variable with mathematical
nature is its position in space.
When the concept of a stochastic process applied to time series was introduced in
Chapter 2, time was defined as a physical process with an arrow, a direction, a before
and an after, a past and a future. It is therefore expected that a realization xt at time t
of the random variable Xt is closer to observations xt−1 and xt+1, rather than to those
farther in time. In stochastic analysis of spatial processes the concept is conceptually
analogous. It is expected that a realization zs at location s of the random variable Zs
is closer to observations zs−h and zs+h, rather than to those farther away in space.
In this case, the variable h is a distance that can span a few orders of magnitude404 Spatial Analysis
depending on the observed process with incremental values of h, such as k × h and
(k = 1, ..., n).
The spatial stochastic process {Zs; s ∈ D ⊂ Rd}, in a more complete manner,
should be written as:
{Z(ω, s); ω ∈ Ω, s ∈ D}
to highlight the fact that the particular realization of the stochastic process at space
s depends on the particular event ω ∈ Ω.
If the space is fixed, s = s. Then Zs (ω) ≡ Z(ω, s) is a random variable and, if the
possible outcomes of the ‘trial’ are ω1, ω2, . . . , the possible realizations of Zs (ω) are
given by:
Zs (ω1) = z1, Zs¯ (ω2) = z2, . . . ,
where the subscript i (i = 1, 2, . . .) of the zi
’s numbers the different possible re￾alizations of the same random variable Zs(ω) at time s = s. The same concepts can
then be applied to Z(s, ω) as a function of s, by fixing ω = ω in Ω, so we have Zs (ω).
Here, a particular outcome at spaces s = s1, s = s2, . . . , that is:
Zs1
(ω1) = z1, Zs2
(¯ω2) = z2, . . . , (12.2)
in this case the subscripts i (i = 1, 2, . . .) of the zi
’s, number the locations at location
s1 at which the event ω1 has occurred, s2 at which the event ω2, has occurred, etc. For
each fixed ω, the sequence (z1, z2, . . .) is also called spatial realization of the process,
and the index is often referred to as spatial points with increments of s, s1, s2, s3 and so
forth, starting from an origin of a Cartesian system. Most commonly a georeferenced,
x and y coordinate system is used where the point s0 corresponds to the origin of a
Cartesian system in geographic coordinates.
The state of a spatial random field can usually be decomposed into (Varouchakis,
2018):
Z(s) = Z
0
λ
(s) + mZ(s) + e(s) (12.3)
where mZ(s) is a deterministic trend, Z
0
λ
(s) is a function expressing a spatial cor￾relation and e(s) is random noise. The trend is commonly used to develop models
or regression of the mean value, while the correlation part is used to determine the
covariance matrix for spatial correlation.
Overall, all the possible sample paths resulting from an experiment constitute an
ensemble. Similarly to time series analysis where only a realization (time series) of
a specific process is available, in geostatistical analysis often only one sample is col￾lected at each point in space, therefore only one realization is observed (not multiple
realizations). For practical reasons, observations are often limited and therefore only
spatial samples are available, which are a discrete measurement of the variable. The
observation set consists of sample locations at which the random variable is observed.
However, the purpose of the geostatistical analysis is to estimate a continuous
surface from limited spatial points, therefore the stochastic process is continuous.
The process is observed over a finite number of locations {s1, s2, ..., sN } and random
variables are defined associated with those locations {Z(s1), Z(s2), ..., Z(sN )}.Geostatistical perspective 405
So what is the sampling process in a geostatistical analysis? How is it possible to
treat a pattern (map) as if it is made by multiple realizations of a stochastic process?
The idea is that each portion of a map is generated by different maps, therefore an
infinite number of maps can be generated, corresponding to the spatial stochastic
process. Clearly, to operate within this framework, the notion of stationarity is applied
as introduced in Chapter 3.
12.1.1 Stationarity in spatial processes
In Chapter 3 the concept of stationarity was introduced. It was pointed out that it is
difficult to make inferences about the process when, for instance, only a single real￾ization of the process is available. If one time series is collected, it is indeed difficult
to derive information about the stochastic process from a single realization. To ob￾tain meaningful information, stationarity (restriction of heterogeneity) and ergodicity
(independency) were described.
It is possible to conceptualize the stochastic process as a number generator that
generates maps for the whole surface. All the generated maps have identical first
moments (stationarity) but because of the their stochastic nature, they are also dif￾ferent. Therefore a surface is generated, a spatial subsample is taken and the mean is
computed. Then another map is generated, another subset is selected and the mean
computed. Since the mean is assumed to be the same, each measurement in space is
visualized as realizations of different maps.
For instance, when a network of weather stations is represented on a map, each
value for a given variable (for example, air temperature) represents a realization of
all the infinite realizations of that variable on the map. Under the conditions of sta￾tionarity, if there are two hundred monitoring stations, each value of air temperature
collected at each single station, is a realization of the same stochastic process.
Since the process is assumed to be the same (stationarity), it does not matter if the
measurements are observed all at the same time or at different moments in time, since
they are still different realizations of the stochastic process. Obviously, this is true if
only the spatial process is analysed. If a multivariate, space-time stochastic process is
analysed, the dependence of the variable with time becomes important.
As described above for time series, the assumption of stationary is a strong notion
of equilibrium and it is a clear restriction upon the heterogeneity of the spatial data. In
accordance with the description provided for time series, the application of stationarity
to spatial data is to imagine dividing the surface into spacial chunks, where all the
pieces are ‘statistically similar’, in the sense that statistical properties do not vary over
space.
When a stochastic process is weakly stationary or simply stationary, the first and
second moments exist and do not vary throughout space. The expected value is finite
and constant, at all time points:
µs = E [Zs] = µ < ∞, ∀s
A relevant application for the expected value in geostatistics is the determination of
large scale trends and spatial dependence patterns. For instance, it is obviously not
realistic to assume that the mean air temperature has the same value (stationarity)406 Spatial Analysis
over a large geographical area, and in particular where differences in altitude are
present. Different kind of linear or non-linear models of the mean can be used.
Variance and autocovariance are finite and constant, at all time points:
σ
2
Z = E
(Zs − µs)
2

The spatial autocovariance is written as:
γ(s1, s2) = E [(Zs1 − µs1
)(Zs2 − µs2
)]
It is worth noting that under conditions of spatial isotropy, the direction in space for
the calculation of geostatistical properties does not change. Therefore performing an
analysis in north-south or south-north direction does not change the results. However,
in real conditions isotropy is not common and is more often anisotropic, with the
variables and the distributions assuming different values in different directions. In case
of isotropic conditions covariance and correlation length provide information about the
process. The application of correlation to spatial processes is described in the section
below.
Overall, the assumption of stationarity (or weak stationarity) is that the mean
value and the variance are constant. Strict stationarity imposes that the probability
density function is the same at all spatial points, regardless of the transformations
where the points are interchanged between each other, but preserving their position
in space.
So after all, how can a process that has the same mean and the same variance be
of interest? The informative part is related to the spatial correlation of the process.
The variable of interest is the covariance. A few restrictions are also imposed on the
covariance. In particular, the covariance is not a function of absolute location but
only of spatial separation. As described in the example below the important aspect
is always the distance between pairs where the observation was collected. If it is the
same variable at two locations, then it is an autocovariance, but if there are multiple
variables at multiple locations it is a crosscovariance.
In many cases error terms are assumed to be white noise, with no systematic trend
(mean is zero) and no differences in variability (variance is constant). By exploiting the
structure of the covariance it is possible to get better predictions (in a sense of higher
precision). The idea again is to obtain information about a deterministic structure
that appears as noise, but since it is not white noise, it provides information about
the process. The two most common methods for obtaining information about spatial
dependence are the correlogram and the semivariogram.
12.2 Correlation coefficient and correlogram
One of the most common statistical coefficients used to establish the strength of a
correlation between two variables is the correlation coefficient. In Chapter 3, the au￾tocovariance and autocorrelation functions were introduced. It is calculated as:
ρ =
1
n
Pn
i=1(Xi − µX)(Yi − µY )
σXσY
(12.4)
where µ is the mean and σ is the standard deviation. The numerator is the covariance.Correlation coefficient and correlogram 407
This well-known coefficient ranges between −1 and 1. It is a coefficient used for the
interpretation of scatterplots. Scatterplots are plots where, on Cartesian coordinates,
values for typically two variables are displayed. The correlation coefficient quantifies
the distance of the data from a straight line (linear correlation). If ρ = +1, then the
scatterplot will be a straight line with a positive slope; if ρ = −1, then the scatterplot
will be a straight line with a negative slope. For ρ < 1 then the scatterplot appears as
a cloud of points. The scattering of the points around the line increases at decreasing
values of ρ. The concept is discussed here since it is important to understand spatial
analysis tools such as the correlogram.
To describe the concept of correlogram, an example is presented. Measurements of
soil’s silt content along two transects were performed. Figure 12.1 shows the measure￾ments for the two transects at incremental values (easting) of h = 5 metres. The top
plate depicts transect 1 and the bottom plate shows transect 2.
0 50 100 150 200 250
0 50 100 150 200 250
0.0 0.1 0.2 0.3 0.4 0.5
silt content [−]
0.0 0.1 0.2 0.3 0.4 0.5
distance [m]
silt content [−]
h t
transect 1
transect 2 Fig. 12.1 Silt content values for two soil transects (Transect 1 top and Transect 2 bottom).
The measurements was made at 5 metres increments (direction west to east). The arrow
indicates the head (h) and tail (t), corresponding to measurements at zs and zs+h. In this
case the one-dimensional position of the transect origin is at s = 0.
A first statistical analysis of the data showed that the following results: Z¯
1 = 0.29, σ2
Z1 =
0.00022 and σZ1 = 0.015 for transect 1 and Z¯
2 = 0.29, σ2
Z2 = 0.0007 and σZ2 = 0.021
for transect 2. The two datasets have the same mean and a slightly different standard
deviation. Histogram plots of the two distributions (see Fig. 12.2) showed similar dis￾tributions, resembling a normal distribution. A first look at the summary statistics
could mislead one to think that the two data sets have similar statistical properties.
A powerful approach to analysing spatial data is based on the idea of measuring
correlation coefficients between pairs of data, selected at different spatial points s,408 Spatial Analysis
frequency
0.25 0.30 0.35
0 5 10 15 20 25
silt content
frequency
0.25 0.30 0.35
0 5 10 15
transect 1
transect 2
Fig. 12.2 Distributions for Transect 1 and 2.
separated by a distance h.
ρ =
1
n
Pn
i=1(Zi − µZi
)(Zj − µZj
)
σZiσZj
(12.5)
where Zi and Zj are measurements of the same variable at two positions (i and j),
µZi and µZj are the means and σZi
, σZj
the standard deviations. The indexes i, j are
locations separated by the distance h. Note that because of the lags, some numbers
at the beginning of the series do not have a corresponding values as shown in the
tables below. To compute a correct value of ρ, to have the same value of n for the two
columns, the first pairs must be eliminated by the computation.
In Fig. 12.1 the measurement of a spatial random value Z at position s is indicated
by Zs for the head and the measurement at position Zs+h is indicated with tail. A
table can then be created with the values of Zs at head and tail.Correlation coefficient and correlogram 409
Distance Tail Head
x t h
0 0.301 NA
5 0.305 0.301
10 0.304 0.305
15 0.297 0.304
20 0.301 0.297
25 0.308 0.301
30 0.301 0.308
35 0.302 0.301
40 0.289 0.302
... ... ...
The indicated (NA) not available refers to the fact that no data are available before the
point at s = 0. The concept is similar to the one of the ‘lag’ operator for time series
as described in Chapter 6. The use of one spatial step (tail) ahead and the creation of
this table is indeed called Lag 1. In this table, for each value of head there corresponds
a value of tail that is the value at the previous space interval. The same example is
shown for a Lag 2 analysis:
Distance Tail Head
x t h
0 0.301 NA
5 0.305 NA
10 0.304 0.301
15 0.297 0.305
20 0.301 0.304
25 0.308 0.297
30 0.301 0.301
35 0.302 0.308
40 0.289 0.301
... ... ...
In this Lag 2 table, for each value of head there is a value of tail corresponding
to the value of two previous space intervals (2h). The procedure can be repeated for
many incremental values of space intervals. In this case, since the Zh value is related to
the previous one, the lag operator is named ‘backwards shift’ or ‘backshift’ operator.
For each of these pairs a scatterplot is plotted for incremental values of lags (from
Lag1 to Lag5), corresponding to incremental distances of 5, 10, 15, 20 and 25 metres.
Correlation coefficients were computed for each pairs at incremental distances.
The scatterplot for Transect 1 (Fig. 12.3) depicts a good correlation coefficient
(0.832) for pairs 5 metres apart with decreasing R values at increasing distances. The
points indicated in the plots are 5 metres apart in the upper left, 10 metres apart in
the upper right, 20 metres apart in the lower left and 25 metres apart in the lower
right panel. As the distances of pairs of sample increases, the correlation between the
samples decreases (see Fig. 12.4).
On the other hand the correlation coefficients for the Transect 2 were very low,
indicating no correlation between the data pairs regardless of the separation distance.410 Spatial Analysis
0.26 0.30 0.34
0.26 0.30 0.34
0.26 0.30 0.34
0.26 0.30 0.34
0.26 0.30 0.34
0.26 0.30 0.34
0.26 0.30 0.34
0.26 0.30 0.34
R = 0.832 R = 0.636
R = 0.253 R = 0.06
z(s)
z(s -2h)
z(s - 4h)
z(s - 5h)
z(s - h)
z(s)
Fig. 12.3 Scatterplots for Transects 1 and 2. The value z(s) indicates the value of the variable
(silt content) at different lags.
The correlation coefficients can be plotted against the incremental value of distances
(Lag 1, Lag 2, Lag 3 and so forth) to create a correlogram. The correlogram provides
information about the spatial dependence. It is function of distance between points
and it estimates the correlation between points in space. Figure 12.5 depicts the two
correlograms for the corresponding Transects 1 and 2.
Overall, the two correlograms are very different, indicating that the two samples,
although they have the same mean, are very different. Since the lags were at increments
of 5 metres, Transect 1 depicts a correlation of 0.83 at one lag (5 metres apart) and
0.6 at two lags (10 metres apart), while Transect 2 displayed a poor correlation 0.48 at
Lag 1. In this example, a one-dimensional transect is presented, but in many cases the
distribution of points in space are two- or three-dimensional; therefore the correlation
can also be a function of direction (angle). The R code below shows the computations
described above.
## Code_12_1.R
# Scatter plots and correlogram
library(sp)
library(gstat)
library(quantmod)
# =========================================
setwd("C:/RPA/code/spatial_analysis")
# =========================================Correlation coefficient and correlogram 411
R = 0.487 R = 0.101
510.0 = R 510.0- = R
0.26 0.30 0.34
0.26 0.30 0.34
0.26 0.30 0.34
0.26 0.30 0.34
0.26 0.30 0.34
0.26 0.30 0.34 0.30 0.34
z(s)
z(s -2h)
z(s - 4h)
z(s - 5h)
z(s - h)
z(s)
Fig. 12.4 Scatterplots for Transects 1 and 2. The value z(s) indicates the value of the variable
(silt content) at different lags.
1 2 3 4 5
0.0 0.2 0.4 0.6 0.8 1.0
lag
correlation value
1 2 3 4 5
0.0 0.2 0.4 0.6 0.8 1.0
lag
Fig. 12.5 Correlograms for Transects 1 (left) and 2 (right).
# load data
geodata1 = read.csv("data/transect_1.csv")
geodata2 = read.csv("data/transect_2.csv")
plot(geodata1$s,geodata1$z,xlab=("Distance [m]"),ylab=("Silt content [-]"),ylim=c(0,0.5))
plot(geodata2$s,geodata2$z,xlab=("Distance [m]"),ylab=("Silt content [-]"),ylim=c(0,0.5))
#edit(geodata)
#Compute basic statistics
mu1<- mean(geodata1$z) #mean412 Spatial Analysis
V1<- var(geodata1$z) #variance
S1<- sd(geodata1$z) #standard deviation
mu2<- mean(geodata2$z) #mean
V2<- var(geodata2$z) #variance
S2<- sd(geodata2$z) #standard deviation
mu1
V1
S1
##Compute lagged vectors and save as numerics into dataframe
geodata1$lag1<-Lag(geodata1$z, k = 1)
geodata1$lag1<-as.numeric(geodata1$lag1)
geodata1$lag2<-Lag(geodata1$z, k = 2)
geodata1$lag2<-as.numeric(geodata1$lag2)
geodata1$lag3<-Lag(geodata1$z, k = 3)
geodata1$lag3<-as.numeric(geodata1$lag3)
geodata1$lag4<-Lag(geodata1$z, k = 4)
geodata1$lag4<-as.numeric(geodata1$lag4)
geodata1$lag5<-Lag(geodata1$z, k = 5)
geodata1$lag5<-as.numeric(geodata1$lag5)
edit(geodata1)
##Compute lagged vectors and save as numerics into dataframe
geodata2$lag1<-Lag(geodata2$z, k = 1)
geodata2$lag1<-as.numeric(geodata2$lag1)
geodata2$lag2<-Lag(geodata2$z, k = 2)
geodata2$lag2<-as.numeric(geodata2$lag2)
geodata2$lag3<-Lag(geodata2$z, k = 3)
geodata2$lag3<-as.numeric(geodata2$lag3)
geodata2$lag4<-Lag(geodata2$z, k = 4)
geodata2$lag4<-as.numeric(geodata2$lag4)
geodata2$lag5<-Lag(geodata2$z, k = 5)
geodata2$lag5<-as.numeric(geodata2$lag5)
edit(geodata2)
##Plot scatter plots
str(geodata1)
plot(geodata1$z,geodata1$lag1,xlim=c(0.2,0.4),ylim=c(0.2,0.4),xlab=("x+h"),ylab=("x"))
abline(a=0,b=1,col="blue")
plot(geodata1$z,geodata1$lag2,xlim=c(0.2,0.4),ylim=c(0.2,0.4),xlab=("x+2h"),ylab=("x"))
abline(a=0,b=1,col="blue")
plot(geodata1$z,geodata1$lag3,xlim=c(0.2,0.4),ylim=c(0.2,0.4),xlab=("x+3h"),ylab=("x"))
abline(a=0,b=1,col="blue")
plot(geodata1$z,geodata1$lag4,xlim=c(0.2,0.4),ylim=c(0.2,0.4),xlab=("x+4h"),ylab=("x"))
abline(a=0,b=1,col="blue")
plot(geodata1$z,geodata1$lag5,xlim=c(0.2,0.4),ylim=c(0.2,0.4),xlab=("x+5h"),ylab=("x"))
abline(a=0,b=1,col="blue")
##=======================Correlations============================
corlag1<-cor(geodata1$z,geodata1$lag1,use="complete.obs")
corlag2<-cor(geodata1$z,geodata1$lag2,use="complete.obs")
corlag3<-cor(geodata1$z,geodata1$lag3,use="complete.obs")
corlag4<-cor(geodata1$z,geodata1$lag4,use="complete.obs")
corlag5<-cor(geodata1$z,geodata1$lag5,use="complete.obs")
correlations1<-c(corlag1,corlag2,corlag3,corlag4,corlag5)
correlations1
##===========================================================
##Plot scatter plots
plot(geodata2$z,geodata2$lag1,xlim=c(0.2,0.4),ylim=c(0.2,0.4),xlab=("x+h"),ylab=("x"))
abline(a=0,b=1,col="blue")
plot(geodata2$z,geodata2$lag2,xlim=c(0.2,0.4),ylim=c(0.2,0.4),xlab=("x+2h"),ylab=("x"))Semivariogram 413
abline(a=0,b=1,col="blue")
plot(geodata2$z,geodata2$lag3,xlim=c(0.2,0.4),ylim=c(0.2,0.4),xlab=("x+3h"),ylab=("x"))
abline(a=0,b=1,col="blue")
plot(geodata2$z,geodata2$lag4,xlim=c(0.2,0.4),ylim=c(0.2,0.4),xlab=("x+4h"),ylab=("x"))
abline(a=0,b=1,col="blue")
plot(geodata2$z,geodata2$lag5,xlim=c(0.2,0.4),ylim=c(0.2,0.4),xlab=("x+5h"),ylab=("x"))
abline(a=0,b=1,col="blue")
##=======================Correlations==========================
corlag1<-cor(geodata2$z,geodata2$lag1,use="complete.obs")
corlag2<-cor(geodata2$z,geodata2$lag2,use="complete.obs")
corlag3<-cor(geodata2$z,geodata2$lag3,use="complete.obs")
corlag4<-cor(geodata2$z,geodata2$lag4,use="complete.obs")
corlag5<-cor(geodata2$z,geodata2$lag5,use="complete.obs")
correlations2<-c(corlag1,corlag2,corlag3,corlag4,corlag5)
correlations2
12.3 Semivariogram
The second most common method for quantifying spatial dependence is the semivar￾iogram function. The variogram is the magnitude of the variance of the difference as
a function of displacement:
2γ(s, h) = Var [Zs+h − Zs] (12.6)
To compute the variance, the expected values must be computed. If the process Zs
is (second-order) stationary, the expected values are the same and their difference is
zero:
E [Zs+h − Zs] = E [Zs+h] − E [Zs] = 0 (12.7)
The variance of the difference between these two random variables is the expected
value of their square minus the square of their expected value. The expected value is
zero, therefore the only thing that remains is the first term:
Var [Zs+h − Zs] = E [(Zs+h − Zs)
2
] − 0 (12.8)
therefore the semivariogram is 1
2
the expected value of the squared difference.
γ(s, h) = 1
2
E

(Zs+h − Zs)
2

(12.9)
The value 1
2
is obtained by the computation of the Euclidean distance between pairs
of point.
The expected values can then be estimated. The semivariogram is therefore an
average of squared differences for a given distance interval. To obtain a variogram,
every observation relative to any other observation is computed. The process is to
take the difference between the values of the same variable in each direction for each
point and for each distance as shown in Fig. 12.6. It is the same concept described
above for the transect, but the spatial relationship between pairs of numbers can be
in two or three dimension and also depending on the angle (directional variograms).414 Spatial Analysis
0 10 20 30 40 50
0 10 20 30 40 50
X
Y
Fig. 12.6 Schematization for semivariogram computation.
The visualization of this calculation is called the variogram cloud plot, where the
squared differences are plotted against the incremental displacement. We will see fur￾ther ahead how to obtain a variogram cloud in R.
Because of the assumed stationarity, the semivariogram is a function of the dis￾placement h only:
E

(Zs+h − Zs)
2

= 2Var [Zs] − 2Cov [Zs+h, Zs] (12.10)
Denoting by C(h) the covariance function Cov [Zs+h, Zs], we obtain a notable re￾lation between the semivariogram and covariance functions:
γ(h) = C(0) − C(h) (12.11)
Note that the variogram definition (12.6) does not require the process to be station￾ary. For example, for a non-stationary process whose first differences are stationary1
E [Zs+h − Zs] = 0 (12.12)
the semivariogram is always defined while the correlation/covariance function is not
necessarily:
Var [Zs+h − Zs] = 2γ(s, h) (12.13)
The semivariogram can be empirically estimated by the ‘classical’ formula developed
by its creator Georges Matheron (Cressie, 1993):
γˆ(h) ≡
1
2|N(h)|
X
i,j∈N(h)
(Z(si) − Z(sj ))2
(12.14)
for all couples si
, sj , where |N(h)| is the number of distinct elements such that si−sj =
h.
1Such a process is defined as intrinsic stationary.Semivariogram 415
12.3.1 Variogram model
The main usefulness of variograms is connected to data interpolation (kriging) for
obtaining continuous spatial maps. For that purpose, the empirical variogram (12.14)
can be approximated by an analytical function interpolating it.2
Figure 12.7 shows the filled contour-plot of a Gaussian function with added white
noise : Z(X, Y ) = 10 · exp(−(X − 5)2 − (Y − 3)2
) + .
X
Y
2
4
6
8
2 4 6 8
−2
0
2
4
6
8
10
Fig. 12.7 Gaussian surface with noise.
For such a smooth function points close to one another are similar than distant
points. That is confirmed by the empirical semivariogram (circle symbols) shown in
Fig. 12.8. The continuous line is a Gaussian model γM(h) fitting the experimental
variogram:
γM(h) = A

1 − e
−h
2/r2

+ B (12.15)
In the above case three parameters are sufficient to obtain a proper fitting: that’s a
usual situation with variograms of stationary spatial processes. For historical reasons
related to mining, which incidentally gave the main acceleration to the development of
geostatistics, the three parameters A, B and r are denoted ‘sill’, ‘nugget’ and ‘range’,
respectively (Isaaks and Srivastava, 1989). In essence, the sill is the plateau reached
by the variogram γ at large distances, the range is the distance at which γ reaches this
plateau, while the nugget is a non-zero value at h = 0 due to sampling errors, short
2That also has the side-effect of avoiding to obtain an inconsistent semivariogram function, not
respecting mathematical requirements such as the negative-definiteness of the variogram function
itself (Cressie, 1993).416 Spatial Analysis
scale variability or other effects causing the sample variogram to be discontinuous in
the origin (where it should be zero).
distance
semivariance
0.5
1.0
1.5
2.0
10 20 30 40
Fig. 12.8 Semivariogram of data in Fig. 12.7.
The following code shows how the above example is implemented in R.
## Code_12_2.R
# Semivariogram
# Load libraries
library(gstat)
library(sf)
library(reshape)
library(lattice)
# First part: generation of artificial data
# Generate Gaussian surface Z(X,Y) with added noise on a grid
X <- seq(0,10,0.1)
Y <- seq(0,10,0.1)
N <- length(X)
Noise <- matrix(rnorm(N*N,mean=0,sd=0.4),ncol=N)
exp(-(X-5)^2) -> eX
exp(-(Y-3)^2) -> eY
# create data matrix Z
Z <- 10*eX %*% t(eY)
Z <- Z+Noise
grid <- expand.grid(X=X, Y=Y)
levelplot(Z~X*Y,grid)
# Second part: semivariogram computation
# Convert Z to dataframe
Z.df <- melt(Z)
names(Z.df) <- c("X","Y","Z")
Z.df$X <- Z.df$X-1
Z.df$Y <- Z.df$Y-1Semivariogram 417
# Convert dataframe to sf class
Z.sf <- st_as_sf(Z.df,coords=c("X","Y"))
# compute empirical semivariogram
vgm <- variogram(Z~1, Z.sf)
plot(vgm)
# model fitting
vgm.model <- fit.variogram(vgm, vgm(sd(Z)^2, "Gau", sqrt(max(X)^2+max(Y)^2), 0.1))
plot(vgm,vgm.model)
The code above makes use of four R packages:
reshape to restructure and aggregate data by the melt function;
lattice for graphics, here to make a levelplot of the Gaussian data;
gstat for spatial and spatiotemporal geostatistical modelling, here used for computing
the semivariogram;
sf providing classes for analysing spatial data.
The last two, in particular, will be extensively used in the analyses of the following
sections. The sf package (Pebesma, 2018), for example, contains classes and functions
operating on them (methods, in the object-oriented language) for manipulating spatial
objects.
Looking at the R code above, the instruction st_as_sf(Z.df,coords=c("X","Y"))
assigns the coordinates X and Y of the dataframe Z.df to the sf object Z.sf which
‘rearranges’ the information included in the former dataframe. Practically, this opera￾tion transforms the ordinary R dataframe object into a sf dataframe, a format more
easily managed by the functions of the package gstat, like variogram. The sf package
(see the related R help and pdf document) gives support for simple features, a stan￾dardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing
data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions
and datum transformations, as the R manual literally explains.
By inspecting the object vgm.model we obtain nugget, sill and range of the Gaus￾sian model:
vgm.model
model psill range
1 Nug 0.1542854 0.0000
2 Gau 2.2321708 16.3435
i.e. nugget = 0.1542854, sill = 2.2321708 and range = 16.3435. Indeed, the plot of Fig.
12.8 could be obtained ‘by hand’ by means of the following lines of code.
plot(vgm$dist,vgm$gamma,ylab="Semivariogram",xlab="Distance")
dmax <- max(vgm$dist)
d <- seq(0,dmax,0.1)
Nugget <- vgm.model$psill[1]
Sill <- vgm.model$psill[2]
Range <- vgm.model$range[2]
Gam <- Nugget+Sill*(1-exp(-d^2/Range^2))
lines(d,Gam,col="blue")
A note is in order concerning the choice of the analytical function used for fitting
the experimental variogram, which cannot be arbitrarily chosen. Among the admissible
models, the most frequently used, having sill s, nugget n and range a, are the following:418 Spatial Analysis
Exponential
γ(h) = n + s [1 − exp(−h/a)]
Gaussian
γ(h) = n + s

1 − exp(−h
2
/a2
)

Spherical
γ(h) = n + s

1.5h/a − 0.5(h/a)
3

, if |h| ≤ a, γ(h)= s otherwise
A general-purpose sill-free model is the
Power function
γ(h) = n + βhθ
which includes, as a particular case, the frequently used linear model:
γ(h) = n + βh
R allows us to generate other variogram models by the function vgm,
3
in particular
the Mat´ern model (Minasny and McBratney, 2005), very often used in soil analysis.
The Mat´ern semivariance is expressed in terms of the modified Bessel function of the
second kind. Its explicit expression can be found in the given reference.
12.3.2 Spatial prediction
Experimental spatial data are usually sparse (not equispaced) and limited in number.
For example, Fig. 12.9 shows the spatial distribution of environmental monitoring
stations in the Emilia-Romagna region in Northern Italy.
Based on a discrete set of data, for example the measured maximum temperatures
in a given day/week/month/year, the purpose is that of estimating the maximum
temperature everywhere in the geographic domain of interest. Once the predictor vari￾able of interest Z(s) (e.g. the maximum temperature) has been individuated, we can
explore its spatial correlation structure by computing the semivariogram cloud, i.e.
plotting all possible squared differences (Z(si) − Z(sj ))2
(actually, one half of them)
as a function of the distance dij among every couple of stations. The resulting plot,
like that in Fig. 12.10, is a graphical representation of dissimilarity between stations,
in the specific case suggesting a linear increase of semivariance with the inter-point
distance.
The example above concerns spatial measurements of maximum temperatures in
Emilia-Romagna (Antolini et al., 2015) on November, 21, 2020.
Indeed, computing the empirical (or sample) semivariogram by equation (12.14),
we obtain the plot of Figure 12.11 evidencing what was suggested from the cloud.
The sample semivariogram can depend on the direction, i.e. it can be not simply
a function of the distance between spatial locations. In such a case, the parameters of
the semivariogram model, in particular sill and range, depend on the direction. It is a
frequently encountered situation in geostatistics, where quantities like temperatures,
3Note the difference between variogram and vgm, the former computing the sample semivariogram,
the latter the semivariogram model.Semivariogram 419
Fig. 12.9 Environmental monitoring stations in Emilia-Romagna, Italy.
distance
semivariance
50
100
150
20000 40000 60000 80000 100000
Fig. 12.10 Semivariogram cloud420 Spatial Analysis
precipitation etc. changes differently in different directions, i.e. with latitude and lon￾gitude. We can investigate the dependence on direction (anisotropy) by computing
the directional variogram in R by specifying the parameter alpha in the variogram
function. From the help of variogram, alpha specifies the direction in plane (x,y),
in positive degrees clockwise from positive y (north): alpha=0 for direction north (in￾creasing y), alpha=90 for direction east (increasing x); optional a vector of directions
in (x,y).
Figure 12.12 shows the anisotropic semivariogram in directions increasing from
0, corresponding to north, to 135, corresponding to south-east. The influence of the
cardinal direction on the daily maximum temperature is evident.
distance
semivariance
5
10
15
20
20000 40000 60000 80000 100000
Fig. 12.11 Semivariogram of maximum temperature values in Emilia-Romagna on Novem￾ber, 21, 2020.
We will consider the variogram in the north direction for the analysis that follows.
It is rather evident that the stochastic process ‘maximum temperature in a given day
on a given spatial extent’, of which the data sample under consideration is a realization,
is not stationary at all. Remembering the definition (12.3), it is reasonable that such
a behaviour is due to a trend ‘hidden’ in the process. Altitude is highly probable to
be at least in part responsible for it, together with other variables related to station
locations. Figure 12.13 shows that the maximum temperature is linearly correlated to
altitude (R2 = 0.9482).
With reference to (12.3) we are interested in investigating the correlation structure
of the residuals, i.e. of the de-trended stochastic process. Compare the semivariogram
cloud of the residuals, Fig. 12.14, with that of Fig. 12.10: the linear dependence on
distance has disappeared. Note also that the residuals are approximately normally
distributed (Fig. 12.15).
Figures from 12.9 to 12.16 have been generated by the following code.Semivariogram 421
distance
semivariance
10
20
30
40
20000 60000 100000
0 45
90
20000 60000 100000
10
20
30
40
135
Fig. 12.12 Semivariogram of maximum temperatures in Emilia-Romagna on November 21
2020, in directions north (0 degrees), north-east (45 degrees), east (90 degrees) and south-east
(135 degrees).
0 500 1000 1500
−5 0 5 10
altitude (m)
Tmax
Fig. 12.13 Maximum temperature versus altitude in Emilia-Romagna on November, 21,
2020.422 Spatial Analysis
distance
semivariance
5
10
15
20000 40000 60000 80000 100000
Fig. 12.14 Semivariogram cloud of residuals.
Tmax residuals
frequency
−2 0 2 4
0 20 40 60 80 Fig. 12.15 Statistical distribution of residuals.
## Code_12_3.R
# Semivariogram with anisotropy
# Load libraries
library(gstat)
library(sf)
# Read data
setwd("C:/RPA/code/spatial_analysis")Semivariogram 423
distance
semivariance
0.1
0.2
0.3
0.4
0.5
0.6
20000 40000 60000 80000 100000
Fig. 12.16 Semivariogram of residuals, with superimposed model (solid line).
geo_data = read.csv("data/20201121_weather.csv")
xy <- geo_data[,c(1,2)]
# Read Emilia-Romagna shapefile and plots station locations
ER.shape <- st_read("RER.shp", quiet=TRUE)
plot(ER.shape$geometry)
points(xy,pch=17,col="red",cex=0.5)
# Semivariogram cloud
Tmax.vgm.cloud <- variogram(Tmax ~ 1, locations = ~UTM.X + UTM.Y, data = geo_data,
cloud=TRUE)
plot(Tmax.vgm.cloud)
# Convert dataframe geo_data into object of the sf class
geo_data.sf <- st_as_sf(geo_data,coords=c("UTM.X","UTM.Y"),crs="EPSG:32632")
vgm <- variogram(Tmax~1,geo_data.sf)
plot(vgm)
# Anisotropy
vgm.aniso <- variogram(Tmax~1,geo_data.sp, alpha = c(0, 45, 90, 135))
plot(vgm.aniso)
# Linear dependence
Tmax.fit <- lm(Tmax~Alt,data=geo_data)
plot(Tmax~Alt,geo_data,xlab="Altitude (m)",ylab=expression(paste(T[max])))
abline(Tmax.fit,col="red")
# Work on residuals of linear model
# Semivariogram cloud
Tmax.res <- Tmax.fit$residuals
Tmax.vgm.cloud <- variogram(Tmax.res~1,locations = ~UTM.X + UTM.Y, data = geo_data,
cloud=TRUE)
plot(Tmax.vgm.cloud)
# Empirical semivariogram of residuals424 Spatial Analysis
Tmax.vgm <- variogram(Tmax.res~1,locations = ~UTM.X + UTM.Y, data = geo_data)
plot(Tmax.vgm)
Tmax.vgm.model <- vgm(0.2, "Lin", 150000, 0.5)
plot(Tmax.vgm,Tmax.vgm.model)
Semivariograms are computed in R by the function variogram of the gstat library.
The second-last line of code uses vgm to define ‘by eye’ an analytical model fitting the
experimental variogram, in this case a linear one. It must be clear that the choice
of the model is constrained to the requirement of a definite-negative semivariogram
function. A thorough discussion of this topic is out of the scope of the book. Readers
interested to deepen their knowledge of that subject are encouraged to consult the
cited books on geostatistics.
12.3.3 Kriging
The linear relationship between maximum temperature and altitude has been estab￾lished on the covariates (Tmax, Alt) relative to the sparse irregular grid of envi￾ronmental stations. But, luckily, altitude is known on very dense regular-grid maps
known as DEM (digital elevation maps). Now, we are faced with the problem of how
to interpolate temperatures at points between those on the irregular grid, taking into
consideration the altitude.
Let us take a step backwards. Suppose our stochastic process Z(s) is given by:
Z(s) = µ + (s) (12.16)
where µ ∈ R is a constant, possibly unknown, s ∈ D ⊂ Rd and (s) includes the last
two terms of (12.3), i.e. it is a second-order stationary process with zero mean and
covariance function:
C(h) = E [(s)(s + h)]
related to the semivariogram γ(h) by (12.11).
One of the most diffused procedures for obtaining an interpolation over a grid is
the ‘kriging’ (Cressie, 1993), which performs a spatial analysis of the data utilizing the
variogram. We will just ‘scratch’ the surface of the theoretical basis of the kriging pro￾cedure, just to help our understanding of the importance of the covariance/correlation
structure for the analysis of spatial data. The situation is pretty similar to what we
encountered in the analysis of time series and ARMA processes, where almost all the
relevant information about the process was contained in the correlation structure.
Reducing to the simplest case − a set of sampled points (s1, s2, ..., sn), one single
point s0 where producing the interpolation − the objective of kriging is to estimate
Z(s0) given the values z1 = z(s1), ..., zn = z(sn) via a linear predictor, so as to have
that minimum variance. We write:
Zˆ(s0) = Xn
i=1
λizi (12.17)Semivariogram 425
The problem is reduced to finding n weights λi whose sum is taken equal to one, to
give an unbiased estimate: P
i
λi = 1. If the process is stationary, the expected error
is zero: E h
Zˆ(s0) − z(s0)
i
= 0, while the variance of the estimate is:
Var h
Zˆ(s0) − z(s0)
i
= E
Zˆ(s0) − z(s0)
2

= 2Xn
i=1
λiγ(si
, s0) −
Xn
i=1
Xn
j=1
λiλjγ(si
, sj )
The procedure involving a Lagrange multiplier ψ is straightforward. We define a func￾tion f(λi
, ψ) = Var h
Zˆ(s0) − z(s0)
i
− 2ψ (
Pn
i=1 λi − 1) and find the values of λi and
ψ minimizing f:



∂f(λi),ψ
λi
= 0, i = 1, .., n
∂f(λi),ψ
ψ = 0
Pλi = 1
or:
Xn
i=1
λiγ(si
, sj ) + ψ = γ(sj , s0),∀j
subject to P
i
λi = 1.
Therefore, we see that the so-called ‘ordinary’ kriging, applicable to (second-order)
stationary processes with unknown mean value4
consists of a system of equations
giving the weights λi
in terms of the semivariances. Of course, that can be put in
matrix form:
Aλ = b (12.18)
where:
A =







γ(s1, s1) γ(s1, s2) . . . γ(s1, sn) 1
γ(s2, s1) γ(s2, s2) . . . γ(s2, sn) 1
.
.
.
.
.
.
.
.
.
.
.
.
γ(sn, s1) γ(sn, s2) . . . γ(sn, sn) 1
1 1 . . . 1 0







λ =







λ1
λ2
.
.
.
λn
ψ







4The trivial case where the mean is known is called ‘simple’ kriging.426 Spatial Analysis
b =







γ(s1, s0)
γ(s2, s0)
.
.
.
γ(sn, s0)
1







The solution is formally: λ = A−1
b. The advantage of the kriging procedure, with
respect to other interpolation techniques, is that it automatically provides an estimate
of the interpolation error, the kriging variance ˆσ
2
(s0) = b
t
λ.
When we interpolate over a grid of points (a block) B ⊂ D, i.e. in a subset of
the domain D of the process instead of a single point, the procedure is formally the
same although in that case the average semivariogram between sample points and B
substitutes the values γ(si
, s0) and an additional term ¯γ(B, B) involving the within￾block average variance comes into the play (Webster and Oliver, 2007):
γ¯(B, B) = 1
|B|
2
Z
B
Z
B
γ(s, s0
)dsds0
We are now able to come back to the example concerning the maximum tempera￾ture in Emilia-Romagna at a given date. Besides simple and ordinary kriging, a more
general situation is when the variable of interest (Tmax in the present case) depends
on one or more external variables (e.g. altitude). In those cases, the extension of or￾dinary kriging is called ‘universal kriging’, or sometimes ‘kriging with trend’ or, when
the main variable Z is linearly related to an external variable Y , ‘regression kriging’. In
essence, regression kriging consists of regressing Z over Y , determining the correlation
structure of the regression residuals, and performing the kriging procedure on such
residuals. We will see this in a while.
The data file 20201121_weather.csv contains maximum temperature, minimum
temperature and precipitation for the Emilia-Romagna region in Northern Italy (An￾tolini et al., 2015) on November, 21, 2020. We used the column containing the max￾imum temperatures for illustrating the computation of a semivariogram in Section
12.3.2. After loading it into a dataframe, the function head is useful to have an idea
of its content, displaying the first lines of the file.
setwd("C:/RPA/code/spatial_analysis")
geo_data = read.csv("data/20201121_weather.csv")
head(geo_data)
The result:
UTM.X UTM.Y Alt Tmax Tmin Prec
1 655965 4960223 23 10.5 -2.1 12.2
2 763298 4857077 629 4.6 1.5 28.0
3 800020 4874061 5 10.8 9.0 24.2
4 776616 4870711 680 4.0 0.9 17.4
5 643237 4944226 54 11.4 -1.3 17.0
6 825967 4860576 7 10.9 9.3 19.8
shows that the first two columns are the universal transverse mercator (UTM) coor￾dinates of the environmental stations, the other four columns being the altitude (in
metres), maximum and minimum temperature (in Celsius) and precipitation (in mm).Semivariogram 427
We are ready to apply regression kriging to the maximum temperature. We have
computed the semivariogram of the residuals of Tmax ∼ Alt, and quantified the pa￾rameters of a reasonable model (linear, see Fig. 12.16). We need a regular grid of
points, covering the region of interest (all Emilia-Romagna) and including the altitude
of every point of that grid: indeed, kriging mathematically is a function having the
sample space as a domain, and the grid B ⊂ D as a codomain. The grid B can be
obtained by tif files containing the DEMs of geographical regions. In R, we will use
the function read_stars from the stars package:
## Code_12_4.R
# Kriging
# Load the stars package
library(stars)
library(ggplot2)
# import DEM data of Emilia-Romagna
setwd("C:/RPA/code/spatial_analysis")
ER.DEM <- read_stars("data/dem450_ER.tif")
# give the name Alt to the data column
names(ER.DEM) <- 'Alt'
# Set the CRS
st_crs(ER.DEM) <- st_crs(geo_data.sf)
# Show Altitude map
ggplot() + geom_stars(data = ER.DEM) +
coord_equal()+
theme_void() +
scale_fill_steps(n.breaks=8,low="lightyellow2",high="red",na.value="white") +
scale_x_discrete(expand=c(0,0))+
scale_y_discrete(expand=c(0,0))+
labs(fill="Alt") +
theme(panel.border = element_rect(linetype = "solid", fill = NA),
legend.key.size = unit(0.95, 'cm'))+
theme(plot.margin=unit(c(1,1,1,1),"cm"))
Altitudes are distributed as shown in Fig. 12.17.
0
250
500
750
1000
1250
1500
1750
2000
Alt
Fig. 12.17 Elevation Map of Emilia-Romagna.
The characteristics of the downloaded grid can be inspected by simply typing the
name of the variable ER.DEM:428 Spatial Analysis
stars object with 2 dimensions and 1 attribute
attribute(s):
Min. 1st Qu. Median Mean 3rd Qu. Max. NA's
Alt -6.84 16.68 107.7 296.9984 510.16 2048 101857
dimension(s):
from to offset delta refsys point values x/y
x 1 635 515776 450 +proj=utm +zone=1 +datum=... FALSE NULL [x]
y 1 337 4998818 -450 +proj=utm +zone=1 +datum=... FALSE NULL [y]
The function st_crs of the sf package allows us to view the coordinate reference
system (CRS) of the stars object ER.DEM:
ER.crs <- st_crs(ER.DEM)
ER.crs$input
ER.crs$input gives:
> ER.crs$input
[1] "+proj=utm +zone=1 +datum=WGS84"
It is mandatory for the regular grid (this last) and the sample grid to share the same
CRS. Using the functions of the gstat package, ordinary kriging on residuals is per￾formed by means of the function krige.
# Kriging
res.kriged <- krige(Tmax.res~1,geo_data.sf,ER.DEM,model=Tmax.vgm.model)
# show kriging results (predicted residuals)
ggplot() + geom_stars(data = res.kriged) +
coord_equal()+
theme_void() +
scale_fill_steps(n.breaks=8,low="blue",high="orange",na.value="white") +
scale_x_discrete(expand=c(0,0))+
scale_y_discrete(expand=c(0,0))+
labs(fill="Pred") +
theme(panel.border = element_rect(linetype = "solid", fill = NA),
legend.key.size = unit(0.95, 'cm'))+
theme(plot.margin=unit(c(1,1,1,1),"cm"))
The result of the last line is shown in Fig. 12.18.
−0.50
−0.25
0.00
0.25
0.50
0.75
1.00
Pred
Fig. 12.18 Kriging of residuals.
The last step is to add up the residual map with a map obtained from the trend.
That is what the last lines of code do. The final map is inserted into a geographicalSemivariogram 429
context: Figure 12.19 compares values and positions of the sample data (environmental
stations, left) with the continuous interpolated Tmax over all the region (right).
Tmax
-6
-4
-2
0
2
4
6
8
10
12
Tmax
-6
-4
-2
0
2
4
6
v
10
NA
Fig. 12.19 Top: maximum temperatures at measurement stations. Bottom: maximum tem￾perature over all the region.
This last passage requires the library mapview. Here is the R code.
## Code_12_5.R
predict(Tmax.fit,ER.DEM) -> Tmax.trend430 Spatial Analysis
matrix(unname(Tmax.trend),nrow=635,ncol=337) -> Tmax.trend1
Tmax.pred1 <- res.kriged
tmp <- res.kriged$var1.pred
# sum trend and residual kriging
Tmax.pred1$var1.pred <- tmp+Tmax.trend1
# plot
ggplot() + geom_stars(data = Tmax.pred1) +
coord_equal()+
theme_void() +
scale_fill_steps(n.breaks=8,low="blue",high="orange",na.value="white") +
scale_x_discrete(expand=c(0,0))+
scale_y_discrete(expand=c(0,0))+
labs(fill="Pred") +
theme(panel.border = element_rect(linetype = "solid", fill = NA),
legend.key.size = unit(0.95, 'cm'))+
theme(plot.margin=unit(c(1,1,1,1),"cm"))
# load required libraries
library(mapview)
library(RColorBrewer)
pal <- colorRampPalette(brewer.pal(9, "YlOrRd"))
# regression kriging plot on geographic map
mapview(Tmax.pred1['var1.pred'],col.regions=pal,legend=TRUE,layer="Tmax")
# stations and measured values on geographic map
mapview(geo_data.sf[,2],legend=TRUE,layer="Tmax",col.regions=pal)
The procedure involving linear regression + kriging of residuals + recombination of
maps can be automated in a single command using the universal kriging format in the
krige function. In this case, the variogram is computed inserting the linear dependence
of Tmax from Alt into the formula. The result is identical to that of Fig. 12.19 (right).
## Code_12_6.R
# Semivariogram
vgm1 <- variogram(Tmax~Alt,geo_data.sf)
vgm1.model = vgm(0.2, "Lin", 150000, 0.5)
plot(vgm1,vgm1.model)
# Universal kriging
TTT <- krige(Tmax~Alt,geo_data.sf,ER.DEM,model=vgm1.model)
mapview(TTT['var1.pred'],col.regions=pal,legend=TRUE,layer="Tmax")
12.4 Spacetime analysis
We can only scratch the surface of the complex, intriguing topic of spatio-temporal
analysis. We will do it by means of a simple real-world example.
The recording of a physical quantity like the ambient temperature in a space region
clearly subtends a process that is stochastic both in space and in time. Indeed, we
have already encountered random time series, whose properties are described by the
autocorrelation structure (or the spectrum), while in the previous section we analysed
spatial stochastic processes in terms of their semivariogram, very closely related to
their covariance structure.
Limiting ourselves to a two-dimensional space domain D ⊂ R2
, the obvious fact
that spatial characteristics change in time, naturally leads - at least conceptually - to
adding a third dimension (the time) to the spatial ones and to consider the stochastic
process in a domain D × T, where T ⊂ R. If z = {z(sk, tk), k = 1..n} is a sample of
observed values of a stochastic variable Z, the purpose of a spatiotemporal analysis isSpacetime analysis 431
usually that of predicting Z at unobserved time/space locations or that of interpolating
it, as we did by means of kriging in the analysis of purely spatial phenomena.
We could naturally be tempted to treat time simply as an additional ‘space’ di￾mension, and to apply the spatial analysis to a problem defined on a domain D0 ⊂ R3
instead of D × T ⊂ R2 × R. Apart from computational matters (one more dimension
greatly increases the computational load), it must be understood that time is intrin￾sically different from space, because it is ‘directional’ and, moreover, time grids are
usually more regular and dense than space grids: a measurement station can record
data every minute/hour/day for very long periods but, of course, measurement stations
cannot be placed everywhere!
Formally, a spacetime process Z(s, t) is defined as:
{Z(s, t) : s ∈ D(t) ⊂ R
2
, t ∈ T ⊂ R}
where both the spatial and temporal components can be discrete or continuous.
Suppose Z(s, t) has mean µ(s, t). The covariance between to spacetime random
variables is defined by:
Cov [Z(s1, t1), Z(s2, t2)] = E [(Z(s1, t1) − µ(s1, t1))(Z(s2, t2) − µ(s2, t2))]
Second-order stationarity requires that the following relations hold for any location
s and time t:
E [Z(s, t)] = µ
Cov [Z(s + h, t + u), Z(s, t)] = Cov [Z(h, u), Z(0, 0)] ≡ C(h, u)
for any spatial shift h and temporal shift u.
As in the spatial case, we understand that some kind of stationarity is needed (or,
at least, desirable) to estimate the spacetime covariance structure. If the process is
second-order stationary, we obtain the familiar expression for the semivariogram:
γ(h, u) = 1
2
Var [Z(s + h, t + u) − Z(s, t)]
as in the purely spatial case. Furthermore, as in the spatial case, in order to predict
values of the process Z at any location/time we will need:
1. to estimate the experimental (sample) semivariogram;
2. to individuate a suitable analytical function (model) to fit the sample semivari￾ogram.
Spatiotemporal semivariogram models are usually not simply inherited from the
spatial semivariogram by adding one more dimension. We cannot enter the jungle of
spacetime models, a discipline still a research matter. We will only give a taste of
that topic by developing an example on the time dependence of the same data used in
the previous section, concerning the daily recording of maximum temperatures in the
Emilia-Romagna region of Italy.432 Spatial Analysis
We will see that a reasonable model for a spacetime semivariogram is the ‘sepa￾rable’ one.5 Remaining in the familiar territory of second-order stationary processes,
a separable semivariogram comes out when the spacetime covariance decomposes into
separate spatial and temporal covariances, or:
C(h, u) = Cs(h)Ct(u)
where the space and time components usually have different parameters to allow space
or time anisotropy. As a consequence, the semivariogram of a separable spacetime
process is given by an expression like:
γ = Sst (γs(h) + γt(u) − γs(h)γt(u)) (12.19)
where Sst is a ‘joint’ sill, and the spatial and temporal parts have their own sills (S),
nuggets (N) and ranges (A):
γs(h) = Ns + Ss [1 − exp(−h/As)]
γt(u) = Nt + St [1 − exp(−u/At)]
when both the spatial and temporal part are assumed to be exponential.
Before studying a practical spatiotemporal example problem, some words are in
order about data representation. We will use the gstat R library, whose functions for
spatiotemporal analysis work well with data organized in ‘long format’ dataframes,
as explained in the document of the spacetime package. In a long format dataframe
every row refers to a single time and space position (Wikle, Zammit-Mangion and
Cressie, 2019), as for example:
Year Month Day Station UTM.X UTM.Y Alt Tmax proc date
2 2019 1 1 10 659.314 4929.813 100 4.2 Tmax 2019-01-01
3 2019 1 2 10 659.314 4929.813 100 2.3 Tmax 2019-01-02
4 2019 1 3 10 659.314 4929.813 100 6.4 Tmax 2019-01-03
5 2019 1 4 10 659.314 4929.813 100 6.9 Tmax 2019-01-04
6 2019 1 5 10 659.314 4929.813 100 5.6 Tmax 2019-01-05
7 2019 1 6 10 659.314 4929.813 100 14.0 Tmax 2019-01-06
8 2019 1 7 10 659.314 4929.813 100 7.5 Tmax 2019-01-07
9 2019 1 8 10 659.314 4929.813 100 3.0 Tmax 2019-01-08
10 2019 1 9 10 659.314 4929.813 100 9.0 Tmax 2019-01-09
11 2019 1 10 10 659.314 4929.813 100 8.9 Tmax 2019-01-10
A long-format table as the above can be easily generated from the maximum tempera￾ture data (containing the station codes) and from the station description file (contain￾ing the coordinates of the meteo stations). With the development of the stars package,
a simpler alternative is to transform the data into a stars object. The following code
has that purpose.
## Code_12_7.R
# Spacetime
# required libraries
5Non-separable models are more complicated and they can take into account of complex spacetime
interactions.Spacetime analysis 433
library(stars)
library(zoo)
library(xts)
library(mapview)
setwd("C:/RPA/code/spatial_analysis")
# Read maximum daily temperatures in years 2019-2020 and station coordinates
Tmax_data <- read.csv("data/ER_Tmax_2019-2020.csv",h=TRUE,check.names = FALSE)
# Read info about measurement stations
Stations <- read.csv("data/ER_stations.csv",h=TRUE)
cod.station <- names(Tmax_data)
cod.station[-1] -> cod.station
subset(Stations,Stations$ID_station%in%cod.station) -> Stations.sel
# Convert into a stars object:
# Step 1: Read in a zoo object
Tmax.zoo <- read.zoo(Tmax_data,index.column=1,format = "%d/%m/%Y")
str(Tmax.zoo)
# Step 2: Convert into an xts (time series)
xts(Tmax.zoo) -> Tmax.xts
str(Tmax.xts)
# Step 3: select stations having Tmax measurement, and obtain geometry
Stations.dat <- subset(Stations.sel,select=c("ID_station","Altitude","UTM_X","UTM_Y"))
space_part <- st_as_sf(Stations.dat,coords=c("UTM_X","UTM_Y"),crs = "EPSG:32632")
# Show measurement stations
space_part.geom <- st_geometry(space_part)
mapview(space_part.geom)
# Step 4: add Altitude data from file Stations
tmp <- subset(Stations.sel,select=c("ID_station","Altitude"))
Alt_data <- Tmax_data
nnn <- as.character(tmp$ID_station)
for (i in 1:length(nnn))
Alt_data[nnn[i]] <- tmp$Altitude[i]
# Read altitudesa in a zoo object
Alt.zoo <- read.zoo(Alt_data,index.column=1,format = "%d/%m/%Y")
# Convert altitudes into xts
xts(Alt.zoo) -> Alt.xts
# Step 5: Make a stars object having two attributes: TMAX and ALT
st_as_stars(list(TMAX = as.matrix(Tmax.xts),ALT = as.matrix(Alt.xts))) %>%
st_set_dimensions(names = c("time", "station")) %>% # nomi colonne
st_set_dimensions("time", index(Tmax.xts)) %>% # trasforma time da char a POSIXct
st_set_dimensions("station", space_part.geom) -> dat1.st
# Verify object
class(dat1.st)
dat1.st
We are now ready for spacetime (ST) analysis, using the same gstat package
employed for spatial kriging. Data frames, although used for data storing, cannot be
used directly for ST analyses which require to recast data into objects of the stars
class. The object dat1.st built with the above code contains all data we need:
stars object with 2 dimensions and 2 attributes
attribute(s):
Min. 1st Qu. Median Mean 3rd Qu. Max.
TMAX -7.8 10.60 16.7 17.39916 24.2 40.8
ALT -1.0 67.75 487.5 509.28889 832.5 1637.0
dimension(s):
from to offset delta refsys point
time 1 731 2019-01-01 1 days Date NA
station 1 180 NA NA WGS 84 / UTM zone 32N TRUE434 Spatial Analysis
values
time NULL
station POINT (655965 4960223),...,POINT (686582 4929444)
Stars objects are lists of arrays with a metadata table describing dimensions. We see
from the above that dat1.st has two attributes (TMAX and ALT) and two dimensions
(time and station). The latter consists in the 180 selected stations, while the former
includes the 731 days from 2019 January 1st to 2020 December 31st. The construction
of the stars object passes through a conversion ff the dataframe into a zoo object
(class of ordered observations) and, successively, into an extensible time series (class
xts) extending zoo.
An ST analysis on the full dataframe relative to two years of daily recordings
from 282 stations (206,142 numbers) is computationally expensive and, however, the
ST variogram computed on the full dataset (two years of daily recordings) cannot be
modelled with ‘standard’ functions (exponential, spherical, Gaussian etc.). The time
dependence on a yearly base could be described by a quasi-deterministic function
taking into account the ‘seasonal’ behaviour of the temperature.
The analysis of a shorter period, e.g. one month, could be of more interest. In￾deed, as the gstat authors quote, local kriging is an attractive alternative to a kriging
on the complete data set (Graeler, Pebesma and Heuvelink, 2016). The left plot in
Fig. 12.20 shows the sample semivariogram computed on the subset of the full data
matrix corresponding to June 2020, obtained by the following lines of code:
## Code_12_8.R
# Variogram for space time
# load required libraries
library(tidyverse)
# Subset June 2020
dat1.st %>% filter(time>="2020-06-01",time<="2020-06-30") -> dat1_june.st
# sample semivariogram
v1.st <- variogramST(TMAX~1+ALT, dat1_june.st, width=10000, cutoff=300000, tlags = 1:10)
plot(v1.st)
# no map
plot(v1.st,map=FALSE)
We see from the code above that the sample semivariogram is computed, as without
time, by taking Alt as a natural covariate, because the maximum temperature at a
given time value is known to be almost linearly related to altitude. Before carrying
out the analysis, it is useful to transform the coordinates into kilometres.
## Code_12_9.R
# Transform coordinates
# Latitude and longitude in km
st_as_stars(list(TMAX = as.matrix(Tmax.xts),ALT = as.matrix(Alt.xts))) %>%
st_set_dimensions(names = c("time", "station")) %>% # column names
st_set_dimensions("time", index(Tmax.xts)) %>% # transform time da char a POSIXct
st_set_dimensions("station", space_part.geom/1000) -> dat2.st
dat2.st %>% filter(time>="2020-06-01",time<="2020-06-30") -> dat2_june.st
# verify geometry
plot(st_geometry(dat2_june.st))
# sample semivariogram
v2.st <- variogramST(TMAX~1+ALT, dat2_june.st, width=10, cutoff=300, tlags = 1:10)
plot(v2.st)
# no mapSpacetime analysis 435
plot(v2.st,map=FALSE)
The procedure for computing the ST semivariogram model is pretty similar to that
introduced in Section 12.3.2 for spatial data, the main difference being the ‘guessed’
model function which, this time, is two-dimensional. In particular, as mentioned before,
we have to deal with a separable model (12.19). The model is specified in R as follows.
## Code_12_10.R
# Define model and starting parameters
v2.sepVgm <- vgmST(stModel = "separable",
space = vgm(1, "Exp",50, nugget = 0.1),
time = vgm(1, "Exp",1, nugget = 0.1),
sill = 10)
# Compute model parameters
(v2.sepVgm <- fit.StVariogram(v2.st, v2.sepVgm))
# Plot fitted semivariogram model
plot(v2.st,v2.sepVgm)
The first line specifies the guessed spatial and temporal parts, both as being described
by an exponential function. The vgmST function defines the model and guesses the
parameters, and fit.StVariogram computes the best fitting values taking into account
the sample variogram. The right plot of Fig. 12.20 compares the model variogram to
the experimental one. The agreement is not perfect but the general behaviour with
distance and time lag appears to be reasonable.
distance
time lag (days)
2
4
6
8
10
50 100 150 200 250
4
6
8
10
12
14
distance
time lag (days)
2
4
6
8
10
50 100 150 200 250
separable
4
6
8
v
12
Fig. 12.20 Sample spacetime semivariogram (left) and separable model (right).436 Spatial Analysis
2020-06-01 2020-06-09
32.5
30.0
27.5
25.0
22.5
20.0
17.5
15.0
12.5
Pred
2020-06-17 2020-06-25
Fig. 12.21 Spatio-temporal kriging for June 2020.
One could question if the parameters of the fitting − nugget, sill and range of the
separated components of equation (12.19) and the common sill value − are really the
best one, for the assumed function (12.19). Actually they are not. A genetic algorithm
can be used to find a slightly better set of values (see the discussion and code at the
end of the chapter), but the difference is not a substantial one, and the result of the
successive ST kriging is negligibly affected by that.
As a conclusion of the current example, we will see how the procedure for ST kriging
closely remembers that introduced for spatial data. First of all we need a regular grid
for the interpolation.
## Code_12_11.R
# Load DEM of Emilia-Romagna region
ER.DEM <- read_stars("dem450_ER.tif")
names(ER.DEM) <- "ALT"
# make a grid in km
xmin <- min(st_get_dimension_values(ER.DEM,"x"))/1000
xmax <- max(st_get_dimension_values(ER.DEM,"x"))/1000
ymin <- min(st_get_dimension_values(ER.DEM,"y"))/1000
ymax <- max(st_get_dimension_values(ER.DEM,"y"))/1000
newgrid <- expand.grid(x = seq(xmin, xmax, by = 4.5),
y = seq(ymin, ymax, by = 4.5))Spacetime analysis 437
# Transform to dataframe for successive elaborations
as.data.frame(newgrid) -> newgrid.df
as.data.frame(ER.DEM) -> ER.DEM.df
# Convert coordinates in kilometres
ER.DEM1.df <- ER.DEM.df
ER.DEM1.df$x <- ER.DEM1.df$x/1000
ER.DEM1.df$y <- ER.DEM1.df$y/1000
# Add altitude to grid points
newgrid.df$ALT <- NA
for (i in 1:nrow(newgrid.df)){
ER.DEM1.df$ALT[ER.DEM1.df$x>=(newgrid.df$x[i]-2) & ER.DEM1.df$x<(newgrid.df$x[i]+2) &
ER.DEM1.df$y>=(newgrid.df$y[i]-2) & ER.DEM1.df$y<(newgrid.df$y[i]+2)] -> xxx
newgrid.df$ALT[i] <- mean(xxx)
}
# Delete NA values, re-obtaining the region shape
na.omit(newgrid.df) -> newgrid.df
# Transform into sf class
st_as_sf(newgrid.df,coords=c("x","y")) -> newgrid.sf
newgrid.geom <- st_geometry(newgrid.sf)
# Verify geometry
plot(newgrid.geom)
# time part of the stars object
t = st_get_dimension_values(dat2_june.st, 1)
st_as_stars(list(ALT = matrix(newgrid.sf$ALT, length(t), length(newgrid.geom),
byrow=TRUE))) %>%
st_set_dimensions(names = c("time", "station")) %>%
st_set_dimensions("time", t) %>%
st_set_dimensions("station", newgrid.geom) -> newgrid.st
# verify geometry
plot(st_geometry(newgrid.st))
Now we have a spatial grid for kriging. The following code allows us to predict the
maximum temperatures in the whole Emilia-Romagna region for all days of 2020 June
30. The results are stored in the variable June.
## Code_12_12.R
# Regression kriging
June <- krigeST(TMAX~1+ALT, data=dat2_june.st, newdata = newgrid.st,
modelList = v2.sepVgm,
progress = TRUE)
# show object details
June
June is a stars object having an attribute (the predicted temperature) in two dimen￾sions (coordinates and time):
stars object with 2 dimensions and 1 attribute
attribute(s):
Min. 1st Qu. Median Mean 3rd Qu. Max.
var1.pred 8.660726 23.64098 26.64732 26.34693 29.16412 35.18578
dimension(s):
from to offset delta refsys point
sfc 1 1019 NA NA NA TRUE
time 1 30 2020-06-01 1 days Date NA
values
sfc POINT (745.5006 4851.893),...,POINT (574.5006 4991.393)
time NULL
We now define a time grid consisting of four dates for ‘prediction’, using the pre￾viously defined spatial grid and we perform the kriging procedure by the command
krigeST.438 Spatial Analysis
## Code_12_13.R
# Space time kriging
t = as.Date("2020-06-01") + seq(0, 30, by=8)
st_as_stars(list(ALT = matrix(newgrid.sf$ALT, length(t), length(newgrid.geom),
byrow=TRUE))) %>%
st_set_dimensions(names = c("time", "station")) %>%
st_set_dimensions("time", t) %>%
st_set_dimensions("station", newgrid.geom) -> newgrid.st
# Regression kriging
June.4dates <- krigeST(TMAX~1+ALT, data=dat2_june.st, newdata = newgrid.st,
modelList = v2.sepVgm,
progress = TRUE)
# show object details
June.4dates
You can compare June.4dates to the previous kriging result to verify that it actually
refers to four dates in June:
stars object with 2 dimensions and 1 attribute
attribute(s):
Min. 1st Qu. Median Mean 3rd Qu. Max.
var1.pred 12.35336 23.50748 25.37122 25.20052 27.04161 32.64812
dimension(s):
from to offset delta refsys point
sfc 1 1019 NA NA NA TRUE
time 1 4 2020-06-01 8 days Date NA
values
sfc POINT (745.5006 4851.893),...,POINT (574.5006 4991.393)
time NULL
Finally, the kriging result can be plotted by using ggplot as shown in the code
below. The steps for transforming the krige stars object into another stars object
explicitly including the x and y coordinates are described in the comments. The result
is shown in Fig. 12.21.
## Code_12_14.R
# For the purpose of using ggplot() with geom_stars()
# Step 1: Obtain the geometry of June.4dates
st_geometry(June.4dates) -> June.4dates.geom
xy <- do.call(rbind, June.4dates.geom) %>% as_tibble() %>% setNames(c("x","y"))
x <- xy$x
y <- xy$y
t = as.Date("2020-06-01") + seq(0, 30, by=8)
# Step 2: Convert stars object into an array and add x and y columns to it
# Then, convert in dataframe and reorder columns to have x and y as first two
June.4dates %>% pull(1) -> June.4dates.arr
June.4dates.arr <- cbind(June.4dates.arr,x,y)
June.4dates.df <- data.frame(June.4dates.arr)
June.4dates.df[,c(5,6,1:4)] -> June.4dates.df
# Step 3: rebuild stars object
June.4dates.new <- st_as_stars(June.4dates.df,dims=c("x","y"))
merge(June.4dates.new) -> June.4dates.new
setNames(June.4dates.new,"Tmax.pred") -> June.4dates.new
June.4dates.new = st_set_dimensions(June.4dates.new, names=c("x","y","time"))
June.4dates.new = st_set_dimensions(June.4dates.new, "time", values = t, names = "time",
point = TRUE)
# Plot results
ggplot() + geom_stars(data = June.4dates.new) +
coord_equal() +On the optimization of the spatio-temporal variogram 439
facet_wrap(~time) +
theme_void() +
scale_fill_steps(n.breaks=8,low="blue",high="orange",na.value="grey95") +
scale_x_discrete(expand=c(0,0))+
scale_y_discrete(expand=c(0,0))+
labs(fill="Pred") +
theme(panel.border = element_rect(linetype = "solid", fill = NA),
legend.key.size = unit(0.95, 'cm'))+
theme(plot.margin=unit(c(1,1,1,1),"cm"))
Look in particular at line 15 of the above code:
merge(June.4dates.new) -> June.4dates.new.
The merge operation merges the four attributes originated by the code above, one
for each of the four dates:
stars object with 2 dimensions and 4 attributes
attribute(s):
Min. 1st Qu. Median Mean 3rd Qu. Max. NA's
V1 14.43113 23.11631 25.64355 24.62988 26.54723 28.59462 933
V2 12.53825 21.95759 24.31657 23.27692 25.21789 26.52291 933
V3 12.35336 22.92491 24.98222 24.00450 25.70258 27.27618 933
V4 19.41741 27.79361 29.39283 28.89077 30.43315 32.64812 933
dimension(s):
from to offset delta refsys point values x/y
x 1 61 522.751 4.5 NA NA NULL [x]
y 1 32 4993.64 -4.5 NA NA NULL [y]
into a single attribute whose values are ordered by an additional dimension that a
successive line of code renames time:
stars object with 3 dimensions and 1 attribute
attribute(s):
Min. 1st Qu. Median Mean 3rd Qu. Max. NA's
Tmax.pred 12.35336 23.50748 25.37122 25.20052 27.04161 32.64812 3732
dimension(s):
from to offset delta refsys point values x/y
x 1 61 522.751 4.5 NA NA NULL [x]
y 1 32 4993.64 -4.5 NA NA NULL [y]
time 1 4 2020-06-01 8 days Date TRUE NULL
The new stars object, having one attribute and three dimensions, can conveniently
be plotted under ggplot() by means of geom_stars() by selecting the different times
with facet_wrap(~ time).
12.5 On the optimization of the spatio-temporal variogram
We now offer a few words to show how genetic algorithms (GA) can be usefully
employed for computing a better set of parameters for the semivariogram model of
Fig. 12.20. Remembering what was presented in Chapter 10, we can define a fitness
function (or objective function) as the sum of the squared differences between the
sample semivariogram γ and the model semivariogram ˆγ computed on the same grid
Gd × Gt ⊂ R2
(distances × time-lags). We can compute such a function as follows:
SSD =
X
d,t∈Gd×Gt
(γ(d, t) − γˆ(d, t))2
(12.20)440 Spatial Analysis
This function also is the ‘core’ of the GA procedure, which will try to maximize the
reciprocal of (12.20). The chromosome of the current problem is a sequence of seven
‘genes’: ns, ss, as, nt, st, at, Sst, i.e. nugget, sill and range of both components and the
cumulative sill. The distance and time grids are those of the object v2.st, the sample
semivariogram computed by variogram in the previous section. It consists of 31 space
points (from 0 to 295 km) and 10 time lags (from 1 to 10 days). In the following code,
such a grid of 31 × 10 spacetime points is arranged in matrix form, suitable to be
plotted as a filled contour plot by means of the function levelplot of the package
lattice.
The sample variogram v2.st is an object of class ‘StVariogram data.frame’, some￾thing like an ordinary dataframe:
> class(v2.st)
[1] "StVariogram" "data.frame"
> head(v2.st)
np dist gamma id timelag spacelag avgDist
1 5220 0.000000 3.381070 lag0 1 0 0.000000
2 10672 6.854489 3.946344 lag0 1 5 6.854489
3 29348 15.382297 4.008843 lag0 1 15 15.382297
4 45472 25.263122 4.076748 lag0 1 25 25.263122
5 48430 35.097244 4.123429 lag0 1 35 35.097244
6 58870 44.850224 4.135518 lag0 1 45 44.850224
The matrices are created as follows:
## Code_12_15.R
# Matrices creation for GA analysis
library(lattice)
# rearrange v2.st in matrix form
tmp <- v2.st
dd <- tmp$spacelag
tt <- tmp$timelag
gg <- tmp$gamma
XY <- expand.grid(X=dd,Y=tt)
MX <- matrix(XY$X,nrow=10,ncol=31,byrow=TRUE)
MY <- matrix(rep(1:10,31),10,31)
GG <- matrix(gg,10,31,byrow=TRUE)
levelplot(GG ~ MX + MY,col.regions=heat.colors,
xlab="distance",ylab="time lag")
# Semivariogram model
Ns = 0
Ss = 1
As = 1e5
Nt = 0.002744451
St = 0.997255549
At = 1088.299
Sst = 889.53781502222
gam.t = Nt+St*(1-exp(-MY/At))
gam.s = Ns+Ss*MX/As
gam = Sst*(gam.s+gam.t-gam.s*gam.t)
levelplot(gam ~ MX + MY,col.regions=heat.colors,
xlab="distance",ylab="time lag")
# plot the difference between sample and model semivariogram
dif <- abs(gam-GG)
levelplot(dif ~ MX + MY,col.regions=heat.colors,On the optimization of the spatio-temporal variogram 441
xlab="distance",ylab="time lag")
# Sum of squared differences
(SSD <- sum(dif^2))
To verify the correctness of the grid, we can reproduce a plot like that on the left of
Fig. 12.20 by the function levelplot of the package lattice (or by filled.contour,
included in the standard graphics library).
The result is shown in Fig. 12.22 (left).
distance
time lag
2
4
6
8
10
50 100 150 200 250
4
6
8
10
12
14
distance
time lag
2
4
6
8
10
50 100 150 200 250
4
6
8
10
12
Fig. 12.22 Filled contour plot of the sample (left) and model (right) variogram recalculated
on the Gd × Gt grid.
The model semivariogram v2.sepV gm is an object of class ‘StVariogramModel list’:
> class(v2.sepVgm)
[1] "StVariogramModel" "list"
> v2.sepVgm
space component:
model psill range
1 Nug 0 0e+00
2 Lin 1 1e+05
time component:
model psill range
1 Nug 0.002744451 0.000
2 Exp 0.997255549 1088.299
sill: 889.53781502222
where linear spatial dependence and exponential time dependence have been assumed.
We can test the result of the fitting procedure by computing the two-dimensional
semivariogram (12.19) and plotting the density plot, as shown in Fig. 12.22 (right).
Figure 12.23 maps the difference inside the expression (12.20). Squaring and adding
the plotted values we obtain SSD = 229.55.
The GA code is readily implemented in R by means of the functions of the library
GA. The following code defines the fitness function, sets the GA parameters (crossover
and mutation probability, number of individuals, maximum number of generations)
and invokes the ga function to obtain a solution. Note that, as remarked in Chapter
10, the solution is itself stochastic, so the algorithm should be run a number of times
(also with different choice of the GA parameters) to obtain the ‘minimum’ of the
minima. We can obtain a solution like:442 Spatial Analysis
distance
time lag
2
4
6
8
10
50 100 150 200 250
0.0
0.5
1.0
1.5
2.0
Fig. 12.23 Difference between sample and model semivariogram.
solution: 0.007663696 1.510145 69998.53 0.003812302 1.461451 467.5601 260.8526
space component:
Nugget 0.007663696
Sill 1.510145
Range 69998.53
time component:
Nugget 0.003812302
Sill 1.461451
Range 467.5601
cumulative Sill 260.8526
for which SSD = 210.94, i.e. a value about 8% lower than that obtained by means of
fit.StVariogram. Of course, kriging is not heavily affected by such a difference but,
anyway, Fig. 12.21 could be recalculated.
The GA code follows.
## Code_12_16.R
# Semivariogram with GA
library(GA)
# sample semivariogram
x <- MX[1,] # spacelag
y <- MY[,1] # timelag
z <- GG # gamma value
# fitness function
zeta <- function(crom){
crom <- matrix(crom, ncol = 7)
n_s <- crom[,1]
s_s <- crom[,2]
a_s <- crom[,3]
n_t <- crom[,4]
s_t <- crom[,5]
a_t <- crom[,6]
S_st <- crom[,7]
g = 0;
for (i in 1:31){
gamma_s <- n_s + s_s*x[i]/a_sOn the optimization of the spatio-temporal variogram 443
for (j in 1:10){
gamma_t <- n_t + s_t*(1-exp(-y[j]/a_t))
z.fit <- S_st*(gamma_s+gamma_t-gamma_s*gamma_t)
Diff <- (z[j,i]-z.fit)
if (!is.na(Diff)){
g <- g + Diff^2
}
}
}
return (1/g)
}
# GA solution
pop.size<- 30
max.gen <- 1000
xover.prob <- 0.7
mut.prob <- 0.1
# seed = 246 for reproducibility with the book results
# it should be removed in a real-world analysis
GA <- ga(type = "real-valued", fitness = zeta,
lower = c(0,1,1e4,0,0,100,100),
upper = c(0.05,2,1e5,0.05,2,2000,1000),
maxiter=max.gen,popSize = pop.size,seed=246)
(solution <- unname(GA@solution))
# Post processing
n_s <- solution[1]
s_s <- solution[2]
a_s <- solution[3]
n_t <- solution[4]
s_t <- solution[5]
a_t <- solution[6]
S_st <- solution[7]
gam_t = n_t+s_t*(1-exp(-MY/a_t))
gam_s = n_s+s_s*MX/a_s
gam_st = S_st*(gam_s+gam_t-gam_s*gam_t)
# plot the solution
levelplot(gam_st ~ MX + MY,col.regions=heat.colors,xlab="distance",ylab="time lag")
# difference between sample and model semivariogram
D <- abs(gam_st-GG)
# plot the error
levelplot(abs(D) ~ MX + MY,col.regions=heat.colors,xlab="distance",ylab="time lag")
# Sum of squared differences
(SSD <- sum(D^2))
Note that a seed has been imposed in the ga function, such that the identical result
presented here can be replicated. Of course, that is not useful in a real-world analysis:
GA are stochastic algorithms, so a series of runs are usually necessary to find the
‘best’ solution, leaving the algorithm free to randomly generate the initial population
of chromosomes for each run. For the particular case above, Fig. 12.24 shows the
convergence of the genetic algorithm.444 Spatial Analysis
0 200 400 600 800 1000
0.000 0.002 0.004
generation
fitness value
Best
Mean
Median
Fig. 12.24 Convergence of the genetic algorithm.
12.6 Exercises
Exercise 12.1 Demonstrate equation (12.11), i.e. that for a second-order stationary process
the semivariogram is expressed in terms of the covariance C(h) and the variance C(0):
γ(h) = C(0) − C(h) (12.21)
where:
C(h) = Cov [Zs+h, Zs]
and:
C(0) = Var [Zs]
Exercise 12.2 If a variogram has a ‘sill’, what does this mean in terms of the covariance
function (when it is defined, of course)?
Exercise 12.3 Replicate for Tmin the analysis of Section 12.3.2 on the data contained into
the file 20201121 weather.csv conducted for Tmax, limited to the computation of the sample
and model variogram. Determine ‘by eye’ reasonable parameters for a linear semivariogram
model.
Exercise 12.4 With reference to exercise 12.3, guess an exponential model instead of a linear
one and use fit.variogram to refine its parameters.
Exercise 12.5 Perform the spatial kriging procedure on the Tmin data of the last two exer￾cises, and compare the interpolation results obtained by the linear and exponential variogram
model.Exercises 445
Exercise 12.6 Using the precipitation data in 20201121 weather.csv (variable P rec) try to
conduct a variogram analysis like that developed for Tmax in the chapter, or for Tmin. What
you can tell about the dependence on altitude?
Exercise 12.7 Use a linear variogram model in Exercise 12.6 and compute a spatial kriging.
If res.kriged is the result of analysis on the residuals of linear dependence of P rec on Alt:
krige(Prec.res~1,geo_data.sf,ER.DEM,model=Prec.vgm.model) -> res.kriged
plot both ‘prediction’ and ‘variance’ as follows, and discuss what you see:
plot(res.kriged['var1.pred'])
plot(res.kriged['var1.var'])13
How Random is a Random Process?
Le hasard est le plus grand de tous les artistes.
Honore' de Balzac, La vieille fille
13.1 Random hints about randomness
Let us consider the following sequence:
seq π 1 4 1 . . . . . . 6 7 9 1000 digits
seq π reports the beginning and the end of the first 1000 decimal digits of π. Is such a
sequence a random sequence? We would answer ‘no’, knowing that the sequence comes
from the irrational number π. Indeed, with this knowledge we are able to say that the
1001-th digit is 3. However, for a person who does not know the decimal digits of π,
probably she or he would think that the sequence was random. Such an opinion would
be confirmed by submitting the sequence to statistical tests.
In the literature a great number of tests for randomness are reported that, if applied
to the above sequence, would confirm that the sequence is random. Nevertheless, we
know that the sequence is generated by a mathematical formula, therefore we say that
seq π is not random, a claim however not supportable only on the basis of a statistical
test. Now the difference is between the mathematical formula to generate a sequence
and the statistical properties of such a sequence. A formula to compute π as:
π/2 = 2/1 × 2/3 × 4/3 × 4/5 × 6/5 × 6/7 × 8/9 × . . .
does not inform us about how many 0’s followed by 1, by 2, etc. there are. Conversely,
from any segment of the sequence, for instance 9 3 2 3 8 4 6 2 6 4, it is not possible to
derive any formula to compute π.
This example of π seems to suggest that the randomness of a sequence is a notion
relative to the mathematical generation rules. Is then randomness a subjective concept
depending on knowing or not knowing how the sequence has been generated? If so,
what is random for me, unaware of the generation method, it is not random for people
who know it. This and similar questions will be discussed in the following.
TRN 0.8535 0.8038 0.6865 . . . . . . 0.8276 0.8168 0.1555 1000 digits
PRN 0.2655 0.3721 0.5728 . . . . . . 0.2821 0.1912 0.2655 1000 digits
Consider now the TRN and PRN sequences (written with four decimal digits).
Both sequences pass statistical tests for randomness, but there is a profound differenceRandom hints about randomness 447
between them. The numbers in the TRN sequence are truly random numbers, while
those of PRN are pseudo random numbers. On the site www.random.org randomness
is generated based on atmospheric noise, hence it derives from physical phenomena and
it is transferred into a computer (do not try to exactly reproduce the TRN sequence
written above, because all sequences downloaded from the site are always different
from one another).
On the contrary, the PRN sequence is reproducible, since it is generated by an algo￾rithm. In our opinion, it is very useful if we can obtain the same sequences when Monte
Carlo methods are applied in computer experiments. This PRN sequence shown above
is obtained with the R function runif(.), combined with set.seed(1), already used
many times in the previous chapters. This function generates random deviates of the
uniform distribution within defined intervals. The command ?RNG, or help(RNG), can
usefully be executed to list random number generators available in R.
Remark 13.1 In the 1970s, a number of generator were proposed, the most common called
linear congruential methods. Although we have made use of functions already implemented in R,
it is instructive to learn something about them. The name derives from the notion of congruence.
Given two integer numbers a and b, it is said that a and b are congruent modulo c, written:
a ≡ b (mod c)
if (a − b) is divisible by c, or equivalently if a and b have the same remainder when divided by c.
In particular, the following generator is called a mixed congruential generator:
ri+1 ≡ (ari + b) (mod m)
where ri, a, b, m are not negative integers. They are called:
a constant multiplier
b increment
m modulus
r0, the initial value, is called the seed. Random numbers between zero and one can be derived
from ui = ri/m.
Congruential methods give rise to periodic sequences. The period of the sequences, as well as
their statistical properties, greatly depend on the choice of a, b, m and r0. As an example, let us
choose:
r0 = 2, a = 3, b = 1, m = 16
Then:
r1 ≡ (3 × 2 + 1) (mod 16)
Now we divide 7 by 16 and take the remainder 7, so that: r1 = 7, and so on. It results in the
sequence:
2 7 6 3 10 15 14 11 2 7 . . .
after 11 the sequence starts over with 2, so the period is 8.448 How Random is a Random Process?
Not all generators proposed in the literature work equally well. Suppose we have
generated a sequence. In order to establish its randomness we submit it to statistical
tests. For example, are the numbers uniformly distributed? Are they independent?
Does the sequence contain long monotonic segments? And, as we will discuss shorty,
what features do these sequences, aggregated in couples, triples, n-ples, exhibit? In￾deed, a ‘good’ random number generator should ensure that n-ples of numbers are
random too, for n as large as possible, i.e. it would be desirable to find the same
properties also in the n-dimensional space, for example a sequence grouped in n-ples
should fill uniformly the space. Numbers generated by linear congruential methods
tend, instead, to fall in planes.
An historical anecdote is worth be tolding. There was once the IBM infamous
generator named RANDU. While for the pairs of generated numbers, it had a good
statistical behaviour, the triplets lay on planes. Press et al. (1992) report that when
one of the authors informed those concerned about the infelicitous performance of
RANDU, he was assured ‘We guarantee that each number is random individually, but
we don’t guarantee that more than one of them is random’. Unfortunately RANDU was
widely used in many scientific works.
The default random number generator in R is the ‘Mersenne-Twister’, whose num￾bers are equi-distributed in 623 dimensions.
We just discussed about sequences generated by physical systems and mathematical
algorithms. This leads, in turn, to considering physical randomness and mathematical
randomness. Given the sequence 1 0 1 0 1 0 . . . of a certain length, we feel that it is
absolutely not random. But it could well be the results of the toss of a coin, realized
by a tossing device. So, the distinction to be kept in mind is between the generation
of random events and the sequence of random events.
RandProc.1 −1 − 1 + 1 . . . · · · − 1 − 1 − 1 1000 digits
RandProc.2 +1 + 1 + 1 . . . · · · + 1 + 1 + 1 1000 digits
The sequences RandProc.1 and RandProc.2 (written with four decimal digits)
are time series of the random process described by the i.i.d. random variables:
X1, X2, X3, . . . , Xn, . . .
such that:
(
P(Xi = +1) = p
P(Xi = −1) = q = 1 − p
Transitions from state i to state i + 1 happen with probability p or to state i − 1
with probability q = 1 − p. This process is a random walk on a line, dealt with in
Chapter 5. In the sequence RandProc.1 it is p = 0.5, while p = 0.7 in the sequence
RandProc.2. Both sequences are obtained by a random algorithm, now the question
is: are the two sequences equally random? How can we measure their randomness?
13.2 Characterizing mathematical randomness
Does there exist a way to objectively define the randomness of a number sequence? A
first difference, already seen before, is between mathematical randomness: what formalCharacterizing mathematical randomness 449
criteria a number sequence must meet to be random, and physical randomness: what
empirical properties a sequence of events must have to be random.
Randomness and interpretation (or, better, conception) of probability are inti￾mately related to one another. The von Mises’ theory of probability (von Mises, 1928)
(about von Mises see von Plato, 1994), is among the first and most well-known propos￾als characterizing mathematical randomness. We will briefly investigate the connection
between randomness and the frequency conception of probability.
Randomness can be defined by means of two axioms applied to sequences with
certain properties, called collectives. Using the words of von Mises (1928, p. 15):
a collective is a mass phenomenon or a repetitive event, or, simply, a long sequence of ob￾servations for which there are sufficient reason to believe that the relative frequency of the
observed attribute would tend to a fixed limit if the observation were indefinitely continued.
This limit will be called the probability of the attribute considered within the given collective.
Clearly, in von Mises’ belief, it makes sense to talk about probability only if it refers
to collectives.
Consider an infinite sequence of elements, e.g. the results of infinite tosses of an
indestructible coin. Let f = n/N the frequency of a certain attribute A, for instance
the ‘number of tails’, determined on the basis of N observations (tosses). Von Mises
postulates that the limit for N → ∞ of f exists and it is finite. This limit is called the
probability of the attribute A in the given sequence. Then the first axiom is:
1st Axiom Existence of limits. The relative frequency of that event A has a limit
value.
However, the sequence must be structured in a specific way, to be defined as ‘col￾lective’ on which to construct a theory of probability (erst das Kollektiv, dann die
Wahrscheinlichkeit). The second axiom postulates randomness:
2sd Axiom Insensitivity to place selection. The limit value of the relative frequency
in the sequence remains unchanged despite any possible selection that can applied.
Moreover, the limit values of all the sub-sequences derived by place selections are
equal to the limit of the original sequence.
This axiom is known as Regellosigkeitaxiom, literally ‘without rules’, i.e it is an ‘axiom
of irregularity’.
The von Mises definition of randomness is somehow related to the gambling house,
and the concept of insensitive-to-place selection is also called principle of impossibility
of a successful gambling system. Once again, games of chance, roulettes, coin tosses,
and so on, are often evoked when defining randomness and they contribute, in a sense,
to the foundation of probability. Perhaps it is because people effectively realize in
such contexts, for example entering in a casino, how chance works in real life. Fyodor
Dostoyevsky’s gambler knows well those places, when he says:
[. . . ] with a sort of fear, a sort of sinking in my heart, I could hear the cries of the croupiers–
‘Trente et un, rouge, impair et passe’, ‘Quarte, noir, pair et manque’. How greedily I gazed
upon the gaming-table, with its scattered louis d’or, ten-gulden pieces, and thalers;
According to von Mises’ ideas, games of chance are indeed to be regarded as the
paradigm of the notion of randomness. A sequence is called random when it exhibits
the same characteristics as a game of chance, for instance the result of the roll of a die450 How Random is a Random Process?
or of the spin of a roulette wheel. In games of chance racking the brain on strategies
to improve the chance to win is pointless. That is the essence of randomness, after all:
random is what you cannot forecast.
A very simple example. In an infinite sequence of this type:
1 0 1 0 1 0 1 0 1 0 1 0 . . .
the limit to infinity of the relative frequency of 0’s is 1/2.
Applying the rule: select the elements in sites 2 × i, i = 1, 2, 3, . . .. we obtain the
sub-sequence:
0 0 0 0 0 0 0 0 . . .
lefting the sequence
1 1 1 1 1 1 1 1 . . .
In the former subsequence, the relative frequency of 0’s has limit 1, while in the latter
such a limit is 0. Both are different from 1/2, the original limit. The original sequence,
therefore, is not random, being not insensitive to place selection.
The von Mises’ proposal aroused consensus and criticisms. In particular, it was
argued that the application of a mathematical concept (existence of the limit of a series)
to a non-mathematical property of the series itself, such as randomness (irregularity),
can be contradictory. Waismann (1930, p. 7) writes:
[. . . ] anyone who stipulates that a convergent series of numbers be constructed in an irregular
way − and this is what is at issue − stipulates the impossible; neither the mathematical series
nor the empirical one can fulfil these requirements. Against this it must be said emphatically
that a statistical series does not have the properties of a mathematical one; that the mark
of the accidental, unforeseeable, cannot be transferred from the empirical structure to the
mathematical one without destroying the a priori necessity which is the characteristic mark
of mathematics.
Now we briefly report some remarks by Popper (1934) to the von Mises’ axioms, focus￾ing on his proposed concept of randomness. The impossibility of applying a gambling
system to infinite sequences, rules out any effective control of the randomness of the
sequence. A sequence, even though regular ‘at the beginning’, may present itself as a
collective, as long as ‘at the end’ it becomes irregular. A sequence of the type:
1 0 1 0 1 0 1 0 1 0 1 0 . . .
consisting in, say, one million 0’s and 1’s, is ‘at the beginning’ regular, but if after those
millions, ‘at the end’ it becomes insensitive to place selection, it will be legitimate to
say it is a random sequence. Popper proposes a different type of random sequence,
that is: a finite sequence such that, ‘for each beginning segment, whether short or
long, is as random as the length of the segment permits’ (Appendix *VI, On Objective
Disorder or Randomness). In this consists the ‘objective characterization of disorder
or randomness, as a type of order’ (Appendix *VI).
Such sequences are called ideally random sequences (Appendix *VI) and are con￾structed by means of a mathematical rule, as described in Appendix *IV (A Method of
Constructing Models of Random Sequences). Popper says that ‘there are two tasks toCharacterizing mathematical randomness 451
be performed: the improvement of the axiom of randomness – mainly a mathematical
problem; and the complete elimination of the axiom of convergence’ (ibid., p. 142).
The axiom of randomness is replaced by a weaker requirement of irregularity. For that
purpose Popper introduces the concept of freedom from after effects (ibid., p. 159).
Consider the sequence (or alternative):
(α) 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 . . .
with a thousand zeros and ones. Now ‘select from α all terms with the neighbourhood￾property β of immediately succeeding a one’. The selected subsequence will have the
structure:
(α.β) 1 0 1 0 1 0 1 0 1 0 . . .
both the relative frequency of the ones and that of the zeros did not change, remaining
equal to 1/2, as in α. Then we may say that ‘the alternative α is insensitive to selection
according to the property β; or, more briefly, that α is insensitive to selection accord￾ing to β’. In other words, Popper says that ‘α is free from any after effect of single
predecessors’ or briefly, ‘α is 1-free’. This notion of freedom from after effects, may be
extended to n-free. For example, a 3-free alternative is obtained from the generating
period
1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 . . .
Popper (ibid., p. 151) writes:
It will be seen that the intuitive impression of being faced with an irregular sequence becomes
stronger with the growth of the number n of its n-freedom.
A sequence n-free from after-effects for every n, is called ‘absolutely free’. The
concept of n-freedom assumes that of relative frequency. The insensitivity to selec￾tion according to certain predecessors is the relative frequency with which a property
occurs.
Since the sequences are mathematically constructed, it makes sense to define limits
of relative frequencies. With words by Popper: (ibid., p. 155, author’s italics):
The use of this concept [limit of relative frequencies] gives rise to no problem so long as we
confine ourselves to reference-sequences which are constructed according to some mathemat￾ical rule. We can always determine for such sequences whether the corresponding sequence
of relative frequencies is convergent or not. The idea of a limit of relative frequencies leads
to trouble only in the case of sequences for which no mathematical rule is given, but only an
empirical rule (linking, for example the sequence with tosses of a coin); for in these cases the
concept of limit is not defined.
Randomness, as described by Popper, entails convergence in itself, with no need of
a convergence axiom. The Appendix *IV of the cited work reports how to construct
such sequences ‘according to some mathematical rule’.
Using the notion of absolutely free sequences, Popper solves what he calls ‘the
fundamental problem of the theory of chance’: ‘the seemingly paradoxical inference
from the unpredictability and irregularity of singular events to the applicability of the
rules of the probability calculus to them is indeed valid’ (ibid., p. 180).
Physical randomness is derived from mathematical randomness: ‘an empirical se￾quence is random to the extent to which tests show its statistical similarity to an ideal452 How Random is a Random Process?
sequence’ (Appendix *VI). It is also possible to establish the degree n of randomness
on the basis of its n-freedom from after effect.
In summary, from the perspective of Popper, randomness should not be inter￾preted as our lack of knowledge. Mathematical randomness is rigorously defined for
both finite and infinite sequences, since random sequences are the result of mathemat￾ical constructions. Physical randomness is assessed from comparison with mathemat￾ical randomness. Statistical tests evaluate how close a physical random sequence to a
mathematical one.
Here we have limited ourselves to the notion of random sequence, as discussed in
the Logic. Further philosophical problems are dealt with in the Logic. For Popper’s
philosophy see, among others, Keuth (2005). To mention the evolution of Popper’s
philosophy, in 1957 he proposed at the Ninth Symposium of the Colston Research
Society a new interpretation of probability, called ‘propensity interpretation’. At the
base of this proposal there is the belief that ‘propensities’ are physically real, like
Newtonian forces, and that indeterminism is not a reflection of an epistemological
state, but is inherent in physical systems, even prima facie deterministic. A sequence
is random so far as it is the result of the propensities of the generating conditions (for
this aspect of Popper’s thought, see Galavotti, 2005).
As opposed to the above approach, there is a branch of the physical-mathematical
thinking that has roots essentially founded on the Bayes theorem (whose applications
were described in Chapter 9). Sir Harold Jeffreys, one of the eminent members of this
branch (E. T. Jaynes dedicated his book to him (Jaynes, 2003) with the epigraph ‘to
Sir Harold Jeffreys: who saw the truth and preserved it.’), gives his perspective about
that (Jeffreys, 1931), referring to statistical mechanics:
[. . . ] the kinetic theory of gases deals essentially with fluctuating motions that we do not
know in detail. It uses the principles of classical mechanics, but uses them to derive relations
between statistical properties, and hence is essentially an application of probability theory.
Thus, he clearly states that randomness comes into the game due to a lack of knowl￾edge. If we could know the initial position and velocity of the gas molecules we will
not need to treat the problem in terms of probabilities.
Moreover, the so-called subjective approach to probability goes further. DeFinetti
(1995) in his essay on the philosophy of probability discusses the randomness or not
of a sequence like the following:
1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0
He says (translated from Italian):
[. . . ] a frequentist would say ‘this sequence is not random [. . . ] because it has been constructed
taking three ‘1’ followed by three ‘0’, then four ‘1’ followed by four ‘0’ etc.’.
and he adds:
[. . . ] if numbers were in a different order he would say: ‘OK, this sequence appear as random.’
His conclusion is that both sequences may have been randomly chosen from the 224
possible ones, the only actual difference being our perception of order/regularity or
disorder/irregularity.Characterizing mathematical randomness 453
13.2.1 Randomness and complexity
Around 1965, Solomonoff (1964), Kolmogorov (1965) and Chaitin (1966) indepen￾dently proposed a definition of the randomness of a sequence of numbers on the basis
of algorithmic complexity. The basic idea is that the complexity of a sequence can be
defined by the length of the shortest binary program for computing that sequence.
Consider the sequences:
a) 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0
b) 1 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 0 0 0 1
Suppose we have to input them into a computer. For the first one, it is enough to
give the instruction ‘print 1 0 ten times’. If we think of extending the sequence, this
instruction will be ‘print 1 0 N times’, with N equal to one hundred thousand, one
million, and so on. The amount of information (number of bits) in such a type of
instructions is clearly much less than the number of bits of the sequences generated by
them. Recall that a sequence of n binary digits holds n bits, since each digit conveys
one bit of information by definition.
On the contrary, the same reasoning is no longer valid for the sequence b). To
transmit it to a computer, the shortest instruction consists in rewriting the sequence
as it is, through an instruction of the type: ‘print 1 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 0 0 0 1’.
Even if the sequence continued (without a rule of construction), the number of bits
in the instruction would always be not less than the number of bits of the sequence.
From this a new definition of random sequence can be derived: a number sequence
is random, if the information contained inside cannot be compressed. More formally
(Martin-L¨of, 1966), consider a binary sequence of length n:
x = x1, x2, . . . , xn
Let A be a ‘universal machine’, e.g. a computer, and p a code (another binary string),
with length l(p), which gives A(p, n) = x when executed by A, the elements of the
sequence x constituting the data on which the function operates.
The Kolmogorov complexity of a sequence x relative to a ‘machine’ A is defined
as:
KA(x|n) = min l(p)
that is, the Kolmogorov complexity of a string x with respect to the computer A is
the minimum length of all possible programs read by the computer A which print out
the string x. Note that we have assumed that l(x), the length n of x, is known by the
computer, otherwise we somehow have to inform the computer about the end of the
sequence (Cover and Thomas, 1991).
If such a program p does not exist, we write:
KA(x|n) = +∞
In this case, we say that the sequence has the maximum complexity. It appears that
the complexity measure depends on A, but Kolmogorov (1965) proved that a universal
machine always exists that, at least asymptotically, is as efficient as any other machine.
Therefore the index A can be omitted.454 How Random is a Random Process?
A sequence (finite or infinite) is defined as a random sequence when it has maximum
complexity. Therefore a random sequence cannot be computed by means of a finite
algorithm, i.e. the contained information cannot be encoded. The only way to state a
random sequence is to rewrite the sequence itself.
Martin-L¨of (1966) proved that random sequences, in the above specified sense, have
the same properties required by the probability theory to characterize certain sequences
as random. It is clear, for instance, that a maximum complexity sequence must pass
all tests for randomness. Suppose we put all these tests together in a unique ‘universal
test’ defined as follows. A test is ‘universal’ if, when a sequence passes it, it passes
every other conceivable test. Martin-L¨of (1966) shows that the maximum complexity
sequences pass the universal test. Therefore, the random sequence definition agrees
with the intuitive idea that a sequence is random if it is so unpredictable that no
mathematical expedient, humanly conceivable, is able to reveal some property in it.
Come back to the two sequences a) and b). Both are not random, since an algorithm
exists which can allow a computer to generate them. But the complexity degree of a)
is less than that of b), since the algorithm to generate a) is shorter than that needed
to generate b).
Some properties of the Kolmogorov complexity are reported in the following (Cover
and Thomas, 1991). Let U be a universal computer. Then for any computer A we have:
KU (x) 6 KA(x) + c
for every string x and c independent of x. Then we always have:

KU (x) − KA(x)

 6 c
The complexity KA(x|l(x)) is less than the length l(x).
A sequence of n 0’s has a finite Kolmogorov complexity c (the computer knows n):
K(000 . . . 0|n) = c, ∀n
Similarly for n decimal digits of π.
In the next section the Shannon entropy H will be introduced. We will see that,
for a sequence with entropy H, the Kolmogorov complexity K is approximately equal
to H.
The basic features discussed before, concerning the notion of randomness, are cer￾tainly useful for determining the degree of complexity of the sequence at hand, but
they are not helpful if we intend to generate random numbers.
13.3 Entropy
The notion of entropy is thoroughly dealt with in Huffaker et al. (2017). The term
‘entropy’ was created in the mid 19th century by the German physicist Rudolf Clausius
in the context of classical thermodynamics. Originally the thermodynamic entropy was
defined as δS = δQ/T. The infinitesimal increase of the entropy δS is equal to the
infinitesimal element δQ of heat, for a system in a reversible infinitesimal process at
absolute temperature T. The second law of thermodynamics can be expressed sayingEntropy 455
that in an irreversible process, entropy always increases with time, so the change in
entropy S2−S1 is positive. Since, entropy increases with time it is implied that there is
a direction of time, a ‘time’s arrow’. What is measured is the difference S2−S1 between
states 1 and 2 of a body. Heat and temperature are physical quantities experimentally
measured.
The idea that increasing entropy means nothing more than loss of information, was
expressed by scientists like Born, Rosenfeld, Brillouin and others. Born (1949, p. 72),
for instance, thought that ‘Irreversibility is a consequence of the explicit introduction
of ignorance into the fundamental laws’.
In statistical mechanics, Ludwig Boltzmann linked the entropy to the concept of
probability, through the well-known formula S = k log W, i.e. the entropy S is propor￾tional to the logarithm of the probability W (thermodynamische Wahrscheinlichkeit),
and the constant k will be called Boltzmann constant.
This equation is based on the assumption that all the microstates of the system
are equally probable, so W represents the number of real microstates corresponding
to the same macrostate. In Chapter 3, we posed the example of two dice, and quoted
Boltzmann about the extraction from an urn of black and white balls, arguing that the
most probable result is that corresponding to the greatest number of permutations.
Now, we can rephrase that concept by saying that, according to the above formula,
the system spontaneously reaches the state of equilibrium, where the entropy S has
its maximum value, because all other states are extremely unlikely. For an extended
discussion on the concept of probability in classical statistical physics, see von Plato
(1994).
13.3.1 Shannon’s entropy
In 1948 Claude Shannon (1948) proposed a measure of the amount of information
contained in a message sent along a transmission line. He, following some advice from
von Neumann called this measure ‘entropy’. Surely, the entropy defined in statistical
mechanics and that defined in information theory have a common source in the proba￾bility context, but it does not imply they have the same meaning. Perhaps, the choice
of a common name has not been the best one. We will return to this issue later.
Let X be a discrete random variable with support SX, i.e. the set of all possible
values of X. In the context of information theory the support is called an ‘alphabet’.
If H has to be a measure of information, it must satisfy the following requirements, as
Shannon himself pointed out:
1. If the probabilities pi are all the same (uniform distribution), that is pi = 1/n,
then H is a monotonically increasing function of n.
2. H must be a function of the probability distribution {p1, . . . , pn}, independently
of how events are gathered within this distribution.
For the latter point, consider the following example. Let X be a random variable with
distribution:
P {X = a} = 0.5, P {X = b} = 0.2, P {X = c} = 0.3
We can also say that the events {b} and {c} are realized half the time. When it
happens, the event {b} occurs with probability 0.4 and the event {c} with probability456 How Random is a Random Process?
0.6. Then, the above distribution can also be written:
P {X = a} = 0.5, P {X = Y } = 0.5, P {Y = b} = 0.4, P {Y = c} = 0.6
H must be the same in the two cases.
Only the function H written below satisfies these requirements:
H = −k
Xn
i=1
pi
log pi
The constant k depends on the selected unit of measurement, that is on the base of the
logarithm. If k = 1 and if the base of the logarithm is 2, Shannon called the measure
H = −
Xn
i=1
pi
log2 pi
the entropy of the set of probabilities {p1, p2, . . . , pn}. With the chosen basis 2, the
unit of measurements is expressed in the term bit. It should be understood that the
notation H(X) does not mean that X is an argument of the function H but, rather,
that it is a function of the probability distribution {p1, p2, . . . , pn} of X. This allows
us to distinguish, for instance, the entropy of the random variable X, H(X), from the
entropy of the random variable Y , H(Y ).
Important properties of the Shannon’s entropy are:
1. H is equal to 0 if and only if X is known with certainty, that is all the probabilities
pi are zero, except one which is 1:
P {X = j} = 1 and P {X = i} = 0, ∀i 6= j
This means that H takes the value 0 only when the result is certain, otherwise is
always H > 0.
2. Given a certain dimension n, H is maximal when all the probabilities pi are equal,
pi = 1/n, uniform distribution, situation with the greatest uncertainty.
The quantity − log2 P {X = i}, or − log2 p(xi), or − log2 pi
is called also surprisal.
When pi
is small, we are ‘surprised’ if the result is that associated to xi
. Then, if pi
is
small − log2 pi
is big. If pi
is big, the surprise is small, as − log2 pi has a small value.
All this is in agreement with the interpretation of H as a measure of the amount of
uncertainty: the more uncertain we are of an outcome, the more surprised we are if
this result occurs.
Are there two worlds? The world of atoms, in which gas of molecules expands and
the magnitude of the physical entropy increases, and the world of ‘emotions’ in which
a quantity with the same name measures our surprise, uncertainty, lack of knowledge.
Might the entropy be the bridge, as someone said, between the two worlds? That is,
of course, a philosophical question, quite outside the scope of this book.
Statistical mechanics and information theory have undoubtedly influenced one an￾other. The crucial point is that randomness in physical phenomena is interpreted as
a lack of knowledge about the phenomena themselves. In this view, entropy is con￾sidered as a measure of incompleteness of knowledge. Starting in 1957, E. T. JaynesEntropy 457
developed an alternative foundation for statistical mechanics. Jaynes’s articles are col￾lected in Rosenkrantz (1983). The focus of Jaynes’s thinking, in the interweaving of
physics, philosophy and statistics, is the principle of maximum entropy (also written
as MaxEnt). It asserts that entropy is maximized by the uniform distribution when no
constraint is imposed on the probability distribution. This principle is just (or better:
an extension of) the principle of indifference: equal probabilities have to be assign to
each occurrence if there is no reason to think otherwise. Among all the possible proba￾bility distribution consisting with the data at hand, we have to choice the distribution
pi = (p1, . . . , pn) that maximized the function:
H(p1, . . . , pn) = −
Xn
i=1
pi
log pi
In this equation, Shannon’s entropy is easily recognizable as a measure of the amount
of uncertainty accounted for by the probability distribution (p1, . . . , pn). Such a dis￾tribution, once obtained, can be exploited in statistical mechanics to derive the ther￾modynamical properties of the system or can be introduced into the Bayes-Laplace
formula in the role of an a priori probability. As a consequence, how to assign the a pri￾ori probabilities should be guaranteed on a sound basis, not by more or less arbitrary
evaluations. Moreover, the a priori probabilities are updated when new information
is received. So we understand the intimate connection between entropy and Bayes’
theorem.
In his view of statistical mechanics, concerning irreversibility, Jaynes, as Born
quoted before (Born, 1949), expressed the belief (Jaynes, 1957, p. 171): ‘[. . . ] it is
not the physical process that is irreversible, but rather our ability to follow it’.
Concerning entropy, Jaynes supports the idea that entropy has to be interpreted
in a subjective sense, as a measure of ‘our degree of ignorance as to the unknown
microstate when the only information we have consists of the macroscopic thermody￾namic parameters’ (Jaynes, 1965, p. 396). Perhaps with a bit of provocation, he also
wrote (Jaynes, 1965, p. 398) (author’s italics):
[. . . ] Even at the purely phenomenological level, entropy is an anthropomorphic concept. For
it is a property, not of the physical system, but of the particular experiment you or I choose
to perform on it.
As expected, Jaynes’s conception aroused both consent and criticism. For a critical
analysis of Jaynes’s viewpoint see, among others, Denbigh and Denbigh (1985).
Going back to Shannon’s entropy, the second property introduced before becomes:
2. Let a time series of a random process x1, x2, . . . , xn, and pi the probability of each
xi
. It is:
H 6 log n
the equal sign holding when pi = 1/n, ∀i, that is the xi are uniformly distributed,
the situation with the maximum uncertainty and, consequently, with maximum
entropy H.458 How Random is a Random Process?
Consider an alphabet with two symbols: H (head) and T (tail). We describe the
tosses of the coin with a random walk on a line. The binary entropy function is the
function:
H(p) = −p log2 p − (1 − p) log2
(1 − p)
If p = 0.5 (symmetric random walk), it is H(0.5) = 1, while if p = 0.75, H(0.7) =
0.8813. Of course, if p is equal to 0 or 1, H(p) = 0. It is like saying that there is no
randomness if the coin has two tails or two heads, and the most random outcomes are
obtained with a fair coin. This example answers the question put at the beginning of
this chapter, concerning the sequences RandProc.1 and RandProc.2. They are not
equally random and H(p) may be taken as a measure of their randomness.
Some variants of H are reported in the following. The joint entropy of two discrete
random variables X and Y is defined as:
H(X, Y ) = X
i
X
j
p(xi
, yj ) log2 p(xi
, yj )
The conditional entropy is defined as:
H(X

Y ) = X
i
X
j
p(xi
, yj ) log2 p(xi

yj )
H(X

Y ) measures the mean uncertainty of X, if the Y value is known. It is also:
H(X, Y ) = H(X) + H(Y

X)
As a consequence:
H(Y

X) = H(X, Y ) − H(X
that is, conditioning reduces entropy.
Consider now the mutual information. Let X and Y be two discrete random vari￾ables. The mutual information between them I(X; Y ) is defined as:
I(X; Y ) = X
i
X
j
p(xi
, yj ) log2
p(xi
, yj )
p(xi) p(yj )
where p(xi
, yj ) is the joint probability distribution function of X and Y , and p(xi)
and p(yj ) are the marginal probability distribution functions of X and Y , respectively.
Note that I(X; Y ) is in bits.
Introducing the entropy H(X), I(X; Y ) can be written:
I(X; Y ) = H(X) + H(Y ) − H(X, Y )
= H(X) − H(X|Y )
= H(Y ) − H(Y |X)
The mutual information is a measure of how much the knowledge of X (Y ) reduces
uncertainty about Y (X). If the knowledge of Y reduces our uncertainty about X,
then we say that Y carries information about X. This implies that I(X; Y ) is 0, ifEntropy 459
and only if X and Y are independent random variables, that is p(xi
, yj ) = p(xi) p(yj ),
or if one of the two variables has zero entropy. This measure is symmetric, that is
I(X; Y ) = I(Y ; X), and it is always non-negative.
We compute now the entropy for the sequence PRN generated by the function
runif(.), with the code Code_13_1.R
## Code_13_1.R
# Shannon's entropy of pseudo random numbers sequence
cell<-numeric()
entr<-numeric()
probk<- numeric()
ncell<-100 # number of cells
n<-1000 # number if iterations
set.seed(1)
u<-runif(n,0,1) # generate n uniform random number
cell[1:ncell]<-0
entr<-0
for(j in 1:n) { # starting loop on the iterations
y<- u[j]
k<-trunc(ncell*y)+1
cell[k]<-cell[k]+1
} # ending loop on the iterations
for(k in 1:ncell){ # loop to compute entropy
prob<-cell[k]/n
probk[k]<- prob
if(prob>0){entr<- entr-prob*log2(prob)}
} # ending loop to compute entropy
# cell # uncomment to print cell
# hist(cell, xlim=c(0,20), freq=FALSE) uncomment to see the distribution of the cells
entr
entr_max<- log2(ncell) # analytically computed entropy
entr_max
Very simply, the interval [0, 1] is divided into N (ncells) subintervals Ik:
Ik =

(k − 1)/N, k/N
, k = 1, . . . , N
and the number of iterates in each interval is computed. The probability pk (prob)
that an iterate falls in the Ik interval is estimated by the frequency of occurrence in
this interval (prob<-cell[k]/n). H is maximal, entr_max<- log2(ncell), if all the
intervals are equally probable.
The result is H = 6.5862, and the maximum entropy (entr_max) = 6.6439. The
small difference is due to the fact that the cells are not equally filled up. We have to
remember that results from numerical computations are approximations of analytical
ones, and such approximations may be very sensitive to the dimension of the generated
sequences. In effect, by executing 5000 runs (ncases), every time with sequences 10000
iterations long (n) and ncells = 500, the mean of 5000 computed entropies results to
be 8.9295, standard error 0.0023, closer to the maximum entropy 8.9658, with respect
to the previous result.
The R package Entropy (Hausser and Strimmer, 2021) implements various estima￾tors of entropy for discrete random variables. Moreover other functions, as Kullback￾Leibler divergence, mutual information, and others, can be estimated. See Hausser and
Strimmer (2019), for a detailed statistical comparison of the estimators available in
the package.460 How Random is a Random Process?
## Code_13_2.R
# Shannon's entropy of pseudo random numbers sequence
estimate with the function `entropy'
#install.packages("entropy", dep=TRUE) # to install the package if not present
library("entropy")
### 1D example ####
# sample from continuous uniform distribution
set.seed(1)
n<- 1000
x = runif(n)
# hist(x, xlim=c(0,1), freq=FALSE) # uncomment to see the x distribution
# discretize into (n/numBins) categories
numBins<-100
y = discretize(x, numBins, r=c(0,1))
#y # # uncomment to print the intervals
# compute entropy from counts
entropy(y,unit="log2") # empirical estimate near theoretical maximum
log2(numBins) # analytically computed entropy
Note ‘unit= ’ to choose the unit in which entropy is measured, and with unit="log2"
it is in bits. The results are the same as the ones obtained by PRN_entr code, as it
must be.
13.3.2 Sρ Entropy
Another metric entropy measure is due to Granger, Maasoumi and Racine (Granger
et al., 2004) and references therein, often denoted as Sρ. It is defined as:
Sρ(k) = 1
2
Z +∞
−∞
Z +∞
−∞
q
f(Xt,Xt+k)(x1, x2) −
q
fXt
(x1)fXt+k
(x2)
2
dx1dx2
where fXt
(·) and f(Xt,Xt+k)(·, ·) are the probability density function of Xt and of the
vector (Xt, Xt+k), respectively. This quantity is a metric measure of the dependence
between two series or between elements of the same series. In other words, it can been
interpreted as a nonlinear autocorrelation function, that is, if Sρ exceeds the confidence
band at lag k, then there is a significant correlation between Xt and Xt+k, which are
distant k steps in the sequence. Similarly, if two series Xt and Yt are independent, Sρ =
0. This entropy Sρ is a normalized version of the Bhattacharya–Hellinger–Matusita
distance (Maasoumi and Racine, 2002) and meets the following six properties:
• It is defined for both continuous and discrete variables. In the case of binary series
the measure becomes:
Sρ(k) = 1
2
X
1
i =0
X
1
j =0
p
P {Xt = i, Xt+k = j} − p
P{Xt = i} P{Xt+k = j}
2
• It is normalized and varies between 0 and 1. It takes the value 0 if Xt and Xt+k
are independent.
• It takes the value 1 if there is a measurable exact (non-linear) relationship, say
Y = m(X), between the variables.
• It reduces to a function of the linear correlation coefficient in the case of a bivariate
normal distribution.Entropy 461
• It is a metric measure, not only a divergence measure; indeed, it obeys the trian￾gular inequality and is a commutative operator.
• It is invariant with respect to continuous, strictly increasing transformations α(.).
This is useful since X and Y are independent if and only if α(X) and α(Y )
are independent. Invariance is important since otherwise clever or inadvertent
transformations would produce different levels of dependence.
The function Srho.test in the R package tseriesEntropy (Giannerini, 2017) imple￾ments the entropy measure Sρ of serial and cross dependence for integer or categorical
data. A simple example is to compute Sρ of the RandProc.1. The code is:
## Code_13_3.R
## S entropy
#install.packages("tseriesEntropy", dep = TRUE)
# to install the package if not present
library(tseriesEntropy)
x <- rbinom(n=1000,size=1,prob=0.5)
x<- as.integer(2*x-1)
S<- Srho(x,lag.max=6)
and the output is:
Srho computed on 6 lags
--------------------------------------------------------------------------
1 2 3 4 5 6
0.0000215000 0.0003135686 0.0002334794 0.0001630541 0.0001552996 0.0006956368
--------------------------------------------------------------------------
Data type : integer-categorical
Stationary version : TRUE
very close to 0, Xt and Xt+k being independent.
In Huffaker et al. (2017), we used the package tseriesEntropy to construct tests
for non-linear serial dependence for continuous and categorical time series.
13.3.3 Approximated entropy
With the approximated entropy (ApEn) we look at the patterns of a sequence. We
can say that a sequence is random if there is no repetition of patterns, while it is
somewhat predictable if there are present patterns repeating themselves throughout
the series. High ApEn levels indicate randomness and unpredictability, low levels mean
the existence of repeated patterns. The ApEn was proposed by Steve Pincus in the
final decade of the last century, originally in the field of medicine, in particular in
the framework of heart rate time series (Pincus, 1991; Pincus et al., 1991; Pincus and
Singer, 1996; Pincus and Kalman, 1997), but it has been expanded to further different
fields from psychology to finance. The ApEn is characterized by the authors (Pincus
and Singer, 1996, p. 2083):
ApEn(m, r, N)(u) measures the logarithmic frequency with which blocks of length m that
are close together remain close together for blocks augmented by one position. Thus, small
values of ApEn imply strong regularity, or persistence, in a sequence u. Alternatively, large
values of ApEn imply substantial fluctuation, or irregularity, in u.
The symbol m, r, N, u will be defined below. It is remarkable that the ApEn can
potentially distinguish order in series generated by both stochastic and deterministic462 How Random is a Random Process?
processes, that is time series deriving from mathematical algorithms, from random
processes and from periodic and chaotic systems.
The ApEn is defined as follows (Pincus and Singer, 1996).
Given quantities
1. N: length of the sequence
2. m (m < N): length of the compared patterns
3. r: margin of tolerance
4. sequence of real numbers u = (u(1), u(2), ...u(N))
Defined quantities
1. distance d(xi
, xj ) between two blocks xi and xj :
d(xi
, xj ) = max
k=1,2,...,m

u(i + k − 1) − u(j + k − 1)


where:
x(i) = (u(i), u(i + 1). . . , u(i + m − 1) and
x(j) = (u(j), u(j + 1). . . , u(j + m − 1)
Two blocks are ‘close together’ if d(xi
, xj ) 6 r. Note d(xi
, xj ) is the maximum
distance of the scalar components.
2. C
m
i
(r) = number of j < N − m + 1 such that d(xi
, xj ) 6 r
N − m + 1
The numerator of C
m
i
(r) counts, within resolution r, the number of blocks of
length m that are approximately the same as a given block acting as template.
Appropriately C
m
i
(r) is also called correlation sum because it acts like a measure
of the summed correlation of vector (block) xi with all other vectors.
3. Φ
m(r) = 1
N − m + 1
PN−m+1
i
log C
m
i
(r)
Then the ApEn with parameters m and r for a sequence of N elements is given by:
ApEn(m, r, N)(u) = Φm(r) − Φ
m+1(r), m ≥ 1
ApEn(0, r, N)(u) = −Φ
1
(r)
We have to keep in mind that the choice of m and r are quite critical. Different
choices might potentially lead to opposite results. The authors in their works suggest
m about 2-3, and r between 0.1 and 0.25 standard deviations of the sequence. If the
data are affected by noise, r has to be larger than most of the noise.
To see how the ApEn works, let us limit ourselves to binary sequences of 0’s and
1’s. In this case, the resolution r must be r < 1. Therefore in binary sequences, to set
r < 1 is equivalent to considering ‘close together’ two elements if they are equal. ToEntropy 463
set r > 1 would be meaningless, since the inequality d(xi
, xj ) 6 r would always be
fulfilled. So we control if any matches exist in the blocks xi and xj , that is whether:

u(i + k − 1) − u(j + k − 1)

 = 0 or 1
It can be simply written ApEn(m, N)(u), omitting r.
With r = 0, and for simplicity m = 1, C
m
i
(r) and Φm(r) become:
Φ
1 =
1
N
X
N
i=1
log C
1
i
C
1
i =
number of j 6 N such that d(xi
, xj ) = 0
N
Let k be the number of 1’s in the sequences, if xi = 1, the number of matches will
be k, that is d(xi
, xj ) = 0 will be fulfilled k times, so that C
1
i = k/N. If xi = 0, the
number of matches will be N − k, therefore Φ1
is given by:
Φ
1 =
1
N

k log k
N
+ (N − k) log (N − k)
N

then:
Φ
1 =
k
N
log k
N
+
N − k
N
log N − k
N
and by putting k/N = p and (N − k)/N = q, we have:
ApEn(0, N) = −(p log p + q log q)
which is just the Shannon’s entropy.
From infinite sequence u = (u(1), u(2), ...) and r < 1, it is selected the sequence
u
(N) = (u(1), u(2), ..., u(N)) of finite length N, and the ApEn(m)(u) for the sequence
u(N), with N → ∞ is defined as (Pincus, 2008):
ApEn(m)(u) = lim
N→∞
ApEn(m, N)(u
N )
For an infinite binary sequence u, the concept of computationally random sequence (C￾random) is introduced. A sequence is C−random if ApEn(m)(u) is equal to log 2, ∀m ≥
0. Recall that Shannon’s H is maximal for a uniform distribution. The extension to
a sequence with an alphabet of k symbols is immediate. In this case, a sequence is
C−random if ApEn(m)(u) is equal to log k, ∀m ≥ 0.
The Code_13_4.R below computes the ApEn in R. We compare two short (N = 20)
binary sequences, to show how the ApEn can identify randomness even in relatively
short strings. The examined sequences were proposed by Chaitin (1975) and utilized
as classic example of application of the ApEn by other authors, for instance (Pincus
and Kalman, 1997). The sequences are:
a) 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
b) 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0464 How Random is a Random Process?
The two sequences are equiprobable, as they are all the 2−20 sequences.
## Code_13_4.R
# Approximated entropy
# binary sequence
m<- 1
r<- 0.1
N<- 20
u<- rep(c(0,1), 10)
nsum<- N-m
id<-rep(0,nsum)
ic<-rep(0,nsum)
i <-0
while(i < nsum){
i <- i+1
j <- 0
while(j < nsum){
j <- j+1
k <- 1
while(k <= m){
diff <- abs(u[i + k - 1] - u[j + k - 1])
if(diff > r) break
k <- k+1
}
if(k > m){
ic[i] <- ic[i] + 1
diff <- abs(u[i + m] - u[j + m])
if(diff <= r) id[i] <- id[i] + 1
}
}
}
rate <- sum(log(id/ic)) # base = exp(1)
ApEn <- -1* rate/nsum
print(c(N,m,r,ApEn))
Sequence a) is clearly non random, indeed it results ApEn(1, 20) = 0.
print(c(N,m,r,ApEn))
[1] 20.0 1.0 0.1 0.0
Note the parameters m = 1 and r < 1. As Pincus and Kalman (1997) point out, the
result ApEn(1, 20) = 0 means that there are no blocks {0, 0} or {1, 1} in a). On the
contrary, the sequence b) shows as random since ApEn(1, 20) = 0.677453, very close
to the maximum value log 2 = 0.693147.
Let us slightly ‘perturb’ the string a) by substituting a 0 with a 1. The result is now
ApEn(1, 20) = 0.263369 and with two substitutions ApEn(1, 20) = 0.379489. These
illustrative examples show how the ApEn is able to detect irregularities in series and
also to characterize a sequence on the basis of its irregularity level, from full order
(ApEn = 0) up to the complete disorder when the ApEn reaches its maximum value.
Consider now the periodic sequence of length N = 1000:
0 0 1 1 0 0 1 1 . . . 0 0 1 1
that is 250 blocks 0 0 1 1. In the code Code_13_4.R, the input string is now given by
u<- rep(c(0,0,1,1),250) and with m = 1 the result is ApEn(1, 1000) = 0.693146,
that is the sequence is random. On the other hand, with m = 2, ApEn(2, 1000) = 0,A final note 465
that is the periodicity is identified. As we have already said, different choices of m can
potentially lead to opposite results.
An applicative, comparative and exhaustive study between several entropy-based
indicators of independence and correlation, applied to series generated by complex
deterministic processes, stochastic and chaotic processes, is in (Bertozzi, 2011).
13.4 A final note
Chapter 2 began with a section entitled The Philosopher and the Gambler. It is un￾deniable that everything revolving around the concepts of randomness, stochasticity,
probability closely interlinks with ‘chance’ (in particular, games of chance) and philos￾ophy. So, at the end of this book we ‘close the circle’ with a few words about different
points of view related to what ‘random’ actually means.
Chaitin (1975) writes:
Although randomness can be precisely defined and can even be measured, a given number
cannot be proved to be random.
To some that might seem obvious, but it is certainly not trivial. We have seen
indeed that the notion of (mathematical) randomness makes sense if referred to a well￾defined sequence, which could be a Kollektive, mathematically constructed sequences,
sequences having maximum complexity, and so on.
Poincar´e’s conception of randomness, has a central role in the topic of chaos (Huf￾faker et al., 2017) and his ideas have been rediscovered and enhanced much later.
Poincar´e does not believe in subjective randomness, i.e. due to lack of knowledge;
rather he thinks that randomness derives from the intrinsic nature of certain phenom￾ena. He analyses several sources of randomness, and among them he finds what we
now call ‘sensitivity to initial conditions’ (Poincar´e, 1908, Chapter 4):
A very small cause, which escapes us, determines a considerable effect, which we cannot
ignore, and we then say that this effect is due to chance.
On the opposite philosophical side, there is the so-called ‘subjective’ definition of
randomness, whose roots are based on the thinking of philosophers like David Hume
− see, for example, (Hume, 2007) − who, in his well-known Treatise of the Human
Nature writes:
On the other hand, as chance is nothing real in itself, and, properly speaking, is merely
the negation of a cause, its influence on the mind is contrary to that of causation; and ’tis
essential to it, to leave the imagination perfectly indifferent, either to consider the existence
or non-existence of that object, which is regarded as contingent. A cause traces the way to our
thought, and in a manner forces us to survey such certain objects, in such certain relations.
Chance can only destroy this determination of the thought, and leave the mind in its native
situation of indifference.
In other words, chance represents insufficient knowledge or indifference. By using a
tautology, random is what the human mind ‘feels’ as random or, more precisely, what
the lack of knowledge forces us to treat by means of a probabilistic approach.
In this sense, the work of Lorenz (1963) and Lorenz (1995) on dynamical systems
that are highly sensitive to initial conditions, was indeed a deterministic description
of an apparently random process. The deterministic process could be unveiled from466 How Random is a Random Process?
the apparent randomness by using the appropriate conceptual and mathematical tools
called deterministic chaos theory (Huffaker et al., 2017).
The two different viewpoints on randomness essentially reflect themselves in the
dispute between ‘objective’ and ‘subjective’ probability. One of the re-founding fathers
of Bayesian probability, Jeffreys (1939), writes:
I should query whether any meaning can be attached to ‘objective’ without a previous analysis
of the process of finding out what is objective If it is done from experience it must begin
with sensations, which are peculiar to the individual [. . . ] We must and do begin with the
individual, and we never get rid of him, because every new ‘objective’ statement must be
made by some individual and appreciated by other individuals.
Both viewpoints have strengths and weaknesses. We are not interested, here, in
embracing one side or the other. Whatever our (and your) opinion is about the essence
of randomness, when the phenomenon we are studying appears as a stochastic process
− that is it does not fit into a fully deterministic framework − or we do not have
access to the system parameters to solve it with deterministic tools, we need to treat
it with probabilistic tools.Appendix A
Bootstrap
The name bootstrap was introduced by Efron (1979) and described more fully in Efron
and Tibshirani (1993). This name comes from the expression ‘to pull oneself up by
one’s bootstrap’, as the Baron Munchhausen did in the book written in 1785 by Rudolf ¨
Erich Raspe. It is a name that evokes something apparently ‘miraculous’, indeed in
statistics a ‘bootstrap estimate’ is an estimate with no a priori assumptions, but that
can only be obtained by using the data at hand.
The idea underlying the bootstrap method is as simple as it is powerful. Suppose
we have a random sample of independent observations:
x = (x1, x2, . . . , xn)
which are distributed according to some unknown probability distribution F. Let θ =
t(F) be the unknown population parameter of interest, such as population mean.
The letter ‘t’ represents the procedure to be applied to F to obtain θ, t may be
simple formulae or computer-based methods. Since F is unknown, we must exploit
what we have to hand, the sample x. Let ˆθ(x) be an estimate of θ based on the
observed sample x. We wish also to estimate how good is this estimation, for instance
through the standard error σ(
ˆθ) of ˆθ(x). The bootstrap method gives a non-parametric
solution for estimating a sampling distribution and assessing the statistical accuracy
of an estimation. This method involves resampling the original data x, and performing
inference from the resampled data.
The bootstrap method prescribes what follows.
1. F is estimated by Fb, the empirical distribution function, constructed by putting
probability mass 1/n on each xi
.
2. A bootstrap sample x
∗
:
x
∗ = (x
∗
1
, x∗
2
, . . . , x∗
n
)
is generated from Fb by making independent random draws with replacement from
the data.
3. A bootstrap replication ˆθ
∗
:
ˆθ
∗ = s(x
∗
) = s(x
∗
1
, x∗
2
, . . . , x∗
n
)
is computed, s being the statistic of interest.
Let us remember some terminology. ˆθ, named (sample) statistic, is a function that
associates each n-tuple (x1, x2, . . . , xn) to a real number. If the statistic ˆθ is used to468 Bootstrap
estimate a parameter ˆθ it is called estimator or summary statistics. The value assumed
by the estimator is named estimate. Now we put t in the place of s and Fb in the place
of F
ˆθ = t(Fb)
We estimate the function θ = t(F) of the probability distribution F with the same
function of the empirical distribution Fb. This is the plug-in principle. Efron (1989)
writes:
The bootstrap [. . . ] is essentially the oldest idea in the statistical book: substitute the em￾pirical distribution of the data for the (unknown) true distribution in anything you wish to
estimate.
What does such principle say? It says: have you to estimate the population mean?
Use the sample mean. Do you need the population variance? Compute the sample
variance, and so on. The idea is old, but the instruments used to implement it really
are not. As opposed to the evangelical parable discernment ‘And no one pours new wine
into old wineskins’, now old wine is just put into new wineskins. The old wine is still
the conceptual and technical statistical methodology developed in previous centuries,
the new wineskins are the computational techniques able to handle enormous amount
of data with great speed and little expense.
Replacing F with Fb is correct thanks the law of large numbers. This law says: let
(X1, X2, . . . , Xn) be n i.i.d. random variables. If, and only if, E [Xn] = µ, ∀n, it is:
1
n
Xn
i=1
Xi
a.s. −−−−→ n→∞
µ
That is the convergence is ‘almost sure’.1
In our case the {Xn} are the Fb(x) (they
depend on n even though not explicitly noted) and µ is F(x), therefore, from a certain
x ∈ R:
Fb(x)
a.s. −−−−→ n→∞
F(x) ⇔ Fb(x) − F(x)
a.s. −−−−→ n→∞
0
Moreover, the Glivenko-Cantelli theorem ensures that such a convergence is not
only punctual, but also uniform, which is a more strong property.
Formally, let (X1, X2, . . . , Xn) be n i.i.d. random variables, with cumulative dis￾tribution F, then:
sup
x∈R
|Fb(x) − F(x)|
a.s. −−−−→ n→∞
0 (A.1)
To compute the empirical distribution function Fb, in principle all the possible
bootstrap replications are necessary, i.e. we would need all the bootstrap samples. The
1We encountered several types of convergence in Chapter 3. A succession {Xn} almost surely
converges to X if the probability P {limn→∞ Xn = X} is one.Bootstrap standard error 469
number of bootstrap samples of length k that can be extracted from n observations is
given by the number of combinations with repetition
C
R
n,k =

n + k − 1
k

In our case k = n, then, putting C
R
n,n = m, the number of distinct bootstrap samples
is
m =

2n − 1
n

=
(2n − 1)!
n!(n − 1)!
If n = 9, a small enough value, m is 24310. But with n = 20, it is m = 68923264410.
Therefore the computation of seFb(
ˆθ
∗
) becomes impracticable, so it is rightly called an
‘ideal bootstrap estimate’. The problem is solved by computing only a number B of
bootstrap samples, obtaining the so-called ‘Monte Carlo bootstrap estimate’.
A.1 Bootstrap standard error
Recall some formulae. Let X be a random variable with probability distribution func￾tion F. Let µF = EF [X] be the expected value of X and σ
2
F
the variance:
VarF [X] = σ
2
F = EF

(X − µF )
2

(A.2)
The standard deviation σF of X is defined as the square root of the variance:
σF =
p
VarF [X]
Define now the standard error. By ‘standard error’ se(·) is meant the standard
deviation of a summary statistic (or estimator), and se(·) is used to give a measure of
the statistical accuracy of an estimate.
Consider the random variable sample mean X. The standard error seF (X) is given
by:
seF (X) = q
VarF

X

= σF /
√
n
The central limit theorem (under quite general conditions on the F) states that for
large sample sizes the sample mean X is approximately distributed as a normal with
mean µF and variance σ
2
F
/n:
X ≈ N (µF , σ2
F /n)
Note that ‘approximately’ refers to normality, not to µF and σ
2
F
/n, which are exact.
From the properties of the normal, it follows:
P

|x¯ − µF | < σF /
√
n
	
= 0.683 and P

|x¯ − µF | < 2σF /
√
n
	
= 0.954
Let ¯x be a realization of X, that is the mean of the sample x = (x1, x2, . . . , xn). In
other words, we can expect ¯x to be within one standard error of the mean µF about
68% of the time and within two standard errors about 95% of the time.470 Bootstrap
In the case of the sample mean, the estimate ˆσ
2
F
of σ
2
F
is given by:
σˆ
2
F =
1
n − 1
Xn
i=1
(xi − x¯)
2
(A.3)
then, the estimate seb (¯x) of se(X) is given by:
seb (¯x) = "
1
n(n − 1)
Xn
i=1
(xi − x¯)
2
#1/2
(A.4)
By applying the plug-in principle we replace µF = EF [X] with µFb = EFb[X], and we
write µFb = ¯x. Then we have:
σ
2
Fb = EFb

(X − µFb)
2

=
1
n
Xn
i=1
(xi − x)
2
(A.5)
a bit different from eqn (A.3) for the denominator n in the place of n − 1. Note that
σ
2
Fb
is a distorted estimator with respect to σ
2
F
, while eqn (A.3) is a correct estimator.
An estimator is called ‘correct’ if its mean is equal to the what is to be estimated.
The random variable sample mean X has the same mean µ of the random variable
X of the population, then it is a correct estimator. From eqn (A.5) we estimate the
standard error of the variable X, that is seF (X) = σF /
√
n, by putting in the place of
σF its plug-in estimate σFb. Then the estimated standard error applying the plug-in
principle is:
seb (x) = σFb/
√
n =
"
1
n2
Xn
i=1
(xi − x)
2
#1/2
(A.6)
The difference between eqn (A.4) and the above eqn (A.6) is n
2
in the place of n(n−1).
Obviously, if n is not too small, the two estimates are the same. It might be noticed
that the plug-in principle has been applied twice. The first was when µF was estimated
with µFb = x, the second when seF (X) was estimated with seFb(X).
We estimate now the standard error using the bootstrap method. The prescriptions
introduced before proceed as follows:
4. B bootstrap replications are computed, each corresponding to a bootstrap sample
x
∗
b
, b = 1, . . . , B:
ˆθ
∗
1 = s(x
∗
1
),
ˆθ
∗
2 = s(x
∗
2
), . . . ,
ˆθ
∗
B = s(x
∗
B)
5. seFb(
ˆθ
∗
) is estimated by the estimator standard deviation of the B replications
(
ˆθ
∗
1
,
ˆθ
∗
2
, . . . ,
ˆθ
∗
B):
seB(
ˆθ
∗
) = "X
B
b=1

ˆθ
∗
b − ˆθ
∗
(·)
2
B − 1
#1/2
(A.7)
where:
ˆθ
∗
(·) = 1
B
X
B
b=1
ˆθ
∗
bBootstrap standard error 471
We can look at the observed samples as a population, therefore it does not vary. What
varies are the replications ˆθ
∗
b
. They are i.i.d. random variables, then, for the strong
law of large numbers, their sample mean converges almost certainly to the expected
value of the population, that is to the mean of all the replications. To sum up: the
standard error ˆθ of the statistics seF (
ˆθ) is estimated by the standard deviation of all
the replications seFb(
ˆθ
∗
) that, in turn, is estimated by the standard deviation of B
replications seB(
ˆθ
∗
).
There are two main packages for bootstrapping in R: bootstrap and boot. The
package bootstrap is considered as help for the book Efron and Tibshirani (1993),
Here we report the code Code_1_Appendix_A.R to show how the bootstrap works in
a very simple case.
## Code_1_Appendix_A.R
# bootstrap standard error of the mean
set.seed(123)
thb<-numeric() # vector of bootstrap replications
B<-200 # number of replications
x<- c(80,82,78,81,80,79,84) # observed data
nx<-length(x)
x_bar<-mean(x)
x_bar
se_mean<- sd(x)/sqrt(length(x)) # correct standard error
# se_mean<-sum((x-x_bar)^2/(nx*(nx-1)))^(1/2) # explicit formula
# the plug-in standard error by the function:
sig2.pl<-function(x){var(x)*(length(x)-1)/length(x)}
# distorted estimator to compute se plug-in
se_pl<-sqrt(sig2.pl(x)/length(x)) # se plug-in
for(b in 1:B) { # starting loop on replications
# sample(...) extracts a sample of size length(x)
# from the elements of x, with replacement
xb<-sample(x,length(x),replace=TRUE)
thb[b]<- mean(xb) # b-th bootstrap replication
} # ending loop on replications
mthb<- mean(thb) # bootstrap mean of replications
mthb
seb<-sqrt(var(thb)) # bootstrap standard error
#seb<-sum((thb-mthb)^2/(B-1))^(1/2) # explicit formula
se_mean # se correct
se_pl # se plug-in
seb # se bootstrap
quant<-quantile(thb, probs = c(0.025, 0.1587, 0.5, 0.8413, 0.975))
quant
half.68<- (quant[4]-quant[2])/2
half.68
shapiro.test(thb)
hist(thb,lty=1,main="",xlim=c(78,83),font.lab=3,freq=T,
cex.lab=1.2,xlab="replications",ylab="frequency")
abline(v=quant[2],lty=2,col="black",lwd=2)
abline(v=quant[4],lty=2,col="black",lwd=2)
abline(v=quant[3],lty=3,col="black",lwd=3)
Suppose the observed data are measures of one side of a rectangular piece of land,
performed by different people. Figure A.1 shows the replications distribution thb,
that is ˆθ
∗
b
, 1 6 b 6 B. The vertical lines mark the quantiles corresponding to 15.87%472 Bootstrap
and 84.13% quantiles (dashed lines), and 50.0% quantile (dotted line). The quantiles
are:
2.5% 15.87% 50% 84.13% 97.5%
79.14286 79.85714 80.42857 81.28571 81.85714
Note that the bootstrap mean of replications, mthb = 80.505, is very close to the mean
of the observed data x_bar = 80.571. Half of the width of the interval [16%, 84%]
equal to 0.7143 (half.68) may be considered as the 68% bootstrap confidence interval,
in this case close to the standard error se_mean = 0.7514, eqn (A.4). The Shapiro￾Wilk normality test gives a p-value equal to 0.2765, therefore the normality of the
distribution cannot be rejected, being greater than 0.05.
replications
frequency
78 79 80 81 82 83
0 1 0 2 0 3 0 4 0 5 0
Fig. A.1 Distribution of B = 200 bootstrap replications ˆθ
∗
b
, 1 6 b 6 B. The dashed lines
define the 15.87% and 84.13% quantiles, while the dotted line marks the 50% quantile.
A.2 Parametric bootstrap
Suppose we have only one measurement of the longer side of that piece of land, for
instance x = 81 m. Suppose also one measurement has been done of the shorter side,
y = 58 m. We compute the area A = (81 × 58) = 4698 m2
. The example is somewhat
artificial, as it is supposed that the two opposite sides are equal. The problem is to esti￾mate the uncertainty on A. Each of the two measurements has its uncertainty ∆x and
∆y which is supposed to be ∆x = ∆y = 2 m. Uncertainty in measurement may be in￾strument sensitivity, small change in observations, or other quantities, as the standard
deviation, determined once for all through appropriate experimental operations.Parametric bootstrap 473
We can now apply the error propagation law. The measurement of n quantities has
as a result xi ± ∆xi
, i = 1, . . . , n, where ∆xi are the uncertainties, assumed as known.
From the measured quantities a new quantity has to be calculated. The procedure
may be a simple analytical formula, as in this example A = f(x, y), but also it may
be a complex computer code. In this case, the measured quantities xi
, i = 1, . . . , n,
are the input of the code and the values of the derived quantities qj , j = 1, . . . , m are
the output. It may be m = 1, but in general it is m 6= n. The problem is that of the
evaluation of the errors ∆qj of the derived quantities in terms of the combined effect
of the errors of the direct measurements.
In our example the error propagation law is written:
A + ∆A = (x + ∆x) (y + ∆y)
= xy + x∆y + y∆x + ∆x∆y
Usually ∆x  x and ∆y  y, therefore the last term can be neglected. Now A = xy,
then:
∆A = x∆y + y∆x
and as relative errors it is written:
∆A
A
=
∆x
x
+
∆y
y
by replacing the standard deviations with the measurement uncertainties, the result
is:
∆A
A
=
s
∆x
x
2
+

∆y
y
2
With the measured data, it is:
∆A
4698
=
s
2
812
+

2
582
= 0.04241136
so that:
∆A = 0.04241136 × 4698 = 199.2486
and the final result is written as A = (4700 ± 200) cm2
, the uncertainty is rounded to
three significant figures.
Let us approach the problem with ‘the brute force application of the bootstrap’
(Efron, 1979, p. 17). The parametric bootstrap differs from the ‘non-parametric’ ver￾sion outlined above in the steps 1 and 2 in the bootstrap algorithm. At the step 1,
instead of estimating F by the empirical distribution Fb, F is estimated by the distri￾bution Fbpar derived from a parametric model for the data. So, at the step 2, instead
of sampling with replacement from the data, we generate B samples from Fbpar After
that, we proceed exactly as in steps 3, 4, and 5. The Fbpar is the normal distribution
with mean x (y) and standard deviation ∆x (∆y). It is assumed that the data come
from a normal distribution.
The code Code_2_Appendix_A.R below reports the estimate of the uncertainty of
the area A in the above example with the parametric bootstrap.474 Bootstrap
## Code_2_Appendix_A.R
# parametric bootstrap
## Now I know only that the available data are:
x<-81
Dx<- 2
y<-58
Dy<-2
A<- 4698 # measured area 81*58
Ab<- numeric()
set.seed(1)
B<-400
for(b in 1:B) { # starting loop on replications
# generate one value of x and y under the normality hypothesis
xb<- rnorm(1,x,Dx)
yb<- rnorm(1,y,Dy)
Ab[b]<- xb*yb # b-th bootstrap replication
} # ending loop on replications
mA_b<- mean(Ab) # bootstrap mean of replications
mA_b
DA_b<-sqrt(var(Ab)) # bootstrap standard error
DA_b
quant<-quantile(Ab, probs = c(0.025, 0.1587, 0.5, 0.8413, 0.975))
quant
half.68<- (quant[4]-quant[2])/2
half.68
hist(Ab,lty=1,main="",xlim=c(4000,5800),font.lab=3,freq=T,
cex.lab=1.2,xlab="replications",ylab="frequency",ylim=c(0,80))
abline(v=quant[2],lty=2,col="black",lwd=2)
abline(v=quant[4],lty=2,col="black",lwd=2)
abline(v=quant[3],lty=3,col="black",lwd=3)
Figure A.2 shows the replications distribution Ab. The vertical lines mark the quantiles
corresponding to 15.87% and 84.13% quantiles (dashed lines), and 50.0% quantile
(dotted line). The quantiles are:
2.5% 15.87% 50% 84.13% 97.5%
4291.436 4491.453 4689.361 4902.506 5103.030
Note that, as expected, the bootstrap mean of replications mA_b = 4696.941 and the
bootstrap accuracy DA_b = 203.5017 are almost equal to the corresponding quantities,
A = 4698 and ∆A = 200, derived analytically by the measured data. Half of the width
of the interval [16%, 84%] equal to 205.5264 (half.68) may be considered as the 68%
bootstrap confidence interval, close to the DA_b. We conclude with a warning. If the
observations can be no longer considered the realization of mutually independent ran￾dom variables, the bootstrap is not applicable in the form outlined above, because the
dependence structure of the data is disregarded. So the (non-parametric or paramet￾ric) bootstrap for independent and identically distributed variables has to be replaced
by techniques based on block resampling, as discussed in Chapter 11.Parametric bootstrap 475
replications
frequency
4000 4500 5000 5500
0 2 0 4 0 6 0 8 0
Fig. A.2 Distribution of B = 400 generated replications Ab. The dashed lines define the
15.87% and 84.13% quantiles, while the dotted line marks the 50% quantile.Appendix B
JAGS
Just Another Gibbs Sampler, or JAGS, is a platform-independent program for sim￾ulation/analysis of Bayesian model using Markov Chain Monte Carlo (MCMC). The
language, developed and implemented by Martyn Plummer (Plummer, 2003) has been
developed to be compatible with BUGS (Bayesian inference Using Gibbs Sampling), a
language for Bayesian inference developed at the end of the 1980 (Lunn et al., 2012).
We have seen in Chapter 9 that Bayesian inference is based on determining the
probability distribution of the unknown parameters involved in the problem. JAGS
computes these probability distributions (and the summarizing parameters of interest
such as their moments) by computer simulation based on (Gibbs) sampling and Monte
Carlo integration. Suppose a random variable X has a probability distribution p(x)
and that we are able to generate a large number of values x1, x2, ..., xN from that
distribution. From the (strong) law of large numbers we know that we can approximate
the expected value of X by:
E[X] ≈
1
N
X
N
k=1
xk
Any summary of p(x) can be approximated in a similar way, i.e. by computing sum￾maries of sampled values ‘extracted’ from that distribution. For example, the proba￾bility that X lies in a given interval (xL, xR) can be simply computed by a counting
procedure:
P(xL < X < xR) ≈
number of values xk ∈ (xL, xR)
N
B.0.1 The JAGS language
The JAGS language is very similar to R. We will approach its syntax by means of an
example: an example is worth a thousand words.
Suppose you are flipping a fair coin, and you are interested in computing the proba￾bility that you get no more than k heads in N tosses. We know from probability theory
that the probability of getting exactly k heads is given by the binomial distribution:
P(k) = 
N
k
 1
2
k 
1
2
N−k
and, as a consequence, the required probability is:JAGS 477
P r(x ≤ k) = X
k
i=0
P(i)
The JAGS model for computing via MCMC simulation the probability of no more
than three heads in ten flips is:
model {
Y ~ dbin(0.5,10)
Pr <- step(3.1 - Y)
}
In the preceding code, we define a variable Y having a binomial distribution
Binomial(N, p) with sample size N = 10 and p = 0.5. The required probability
P r = P rob(Y ≤ 3) is computed by means of the function step, defined in JAGS by:
step(X) = 
1 X ≥ 0
0 X < 0
The JAGS model can be written in a file, but it is definitely better to include it
in a string and use the R function textConnection to use the model inside R. The
R interface to JAGS is the R package rjags. A complete R code for the coin tossing
simulation is shown below.
Let us analyse every step of the listing. For not too complex (too long) models
textConnection usefully avoids the need to write a model to a file, to be read by
the jags.model function, which is the key-point function of rjags. The jags.model
requires as input:
• a model file;
• a list or environment containing the data (possibly empty);
• a specification of initial values (optional), in form of list or function;
• the number of parallel chains.
In the example, we choose a single chain and we to leave JAGS the task of ran￾domly generating the initial values. An empty data list completes the model defi￾nition, because we have no data here. The function coda.samples sets a monitor
for the ‘nodes’ defined in the second argument (corresponding to variable.names, see
help(coda.samples)), and outputs the results of the analysis in an MCMC object.
The remaining lines extract Y and P from the MCMC list and perform various
summarizing procedures. The exact result is P r(x ≤ 3) = 0.171875. The result of the
JAGS computation is close to that value, but it obviously changes from one simulation
to another and also depends on the number of iterations and other MCMC parameters.
For example, it could result in P r(x ≤ 3) ≡ sum(out$P r == 1)/length(out$P r) =
0.1735 when the distribution of ‘heads’ is that of Figure B.1
## Code_1_Appendix_B.R
# Use rjags library
library(rjags)
# Export JAGS model
model <- textConnection("model {
Y ~ dbin(0.5,10)
Pr <- step(3.1 - Y)
}")478 JAGS
# Init
jags.inits <- NULL
# Data
jags.data <- list()
# Perform Bayesian analysis using JAGS
model <- jags.model(model, data=jags.data, inits=jags.inits, n.chains=1)
update(model, n.iter=10000)
# Parameters
jags.params <- c("Y","Pr") # parameters to be monitored
samples <- coda.samples(model, jags.params,10000)
# Analysis of the simulation results
plot(samples)
summary(samples)
# extract P and Y
out <- do.call(rbind.data.frame, samples)
names(out)
# Probability
summary(out$Pr)
sum(out$Pr==0)/length(out$Pr)
sum(out$Pr==1)/length(out$Pr)
# Flipping results
summary(out$Y)
hist(out$Y,breaks=c(0,1,2,3,4,5,6,7,8,9,10),main="",freq=FALSE,xlab="Number of heads")
number of heads
density
0 2 4 6 8 10
0.00 0.05 0.10 0.15 0.20 0.25
Fig. B.1 Distribution of heads.
B.0.2 Extracting samples from a distribution
JAGS computation is based on simulation using data extracted from a distribution.
Indeed, JAGS can be used for sampling from one of the several distributions included
in the language. The following R code shows how to draw 1000 samples from a Poisson
distribution with parameter λ = 2. Of course, the same result can be obtained directlyJAGS 479
in R using a single line of code. Figure B.2 compares the samples extracted with the
two methods.
Poisson
frequency
0 2 4 6
0 100 200 300 400 500
Poisson
frequency
0 2 4 6
0 100 200 300 400 500
Fig. B.2 Samples extracted from a Poisson distribution, in R (dark grey) and JAGS (light
gray).
## Code_2_Appendix_B.R
N <- 1000
lambda <- 2
# using R
Pois.R <- rpois(N,lambda)
# USING RJAGS
library(rjags)
# Model
model <- textConnection("model {
Y ~ dpois(lambda);
}")
# Init
jags.inits <- NULL
# Data
jags.data <- list("lambda"=lambda)
# Run
model <- jags.model(model, data=jags.data, inits=jags.inits, n.chains=1)
update(model, n.iter=N)
# Parameters
jags.params <- c("Y") # parameters to be monitored
samples <- coda.samples(model, jags.params,N)
out <- do.call(rbind.data.frame, samples)
Pois.J <- out$Y
# Compare R and JAGS results480 JAGS
par(mfrow=c(1,2))
hist(Pois.R,breaks=max(Pois.R),ylim=c(0,500),col="red",main="",xlab="Poisson")
summary(Pois.R)
hist(Pois.J,breaks=max(Pois.J),col="blue",xlab="Poisson",ylim=c(0,500),main="")
B.0.3 Regression example
The following example, extracted from the ‘JAGS User Manual’, is a bit more compli￾cated than the previous ones and it is perfect for explaining the JAGS syntax in some
more detail. It implements a linear regression analysis:
## Code_3_Appendix_B.R
model {
for (k in 1:N) {
Y[k] ~ dnorm(mu[k], tau)
mu[k] <- alpha + beta*(x[k] - x.bar)
}
x.bar <- mean(x)
alpha ~ dnorm(0.0, 1.0E-4)
beta ~ dnorm(0.0, 1.0E-4)
sigma <- 1.0/sqrt(tau)
tau ~ dgamma(1.0E-3, 1.0E-3)
}
Let us briefly analyse the content of the previous listing. The regression model is
defined in terms of a set of relations, each one defining a node in the model.1 Nodes
on the left of a relation are defined in terms of nodes on the right-hand side. We see
two kinds of relation:
• Deterministic relations, written in terms of the symbol ‘<-’, like x.bar <- mean(x),
whose meaning is identical to an assignment in R.
• Stochastic relations, written in terms of the symbol ‘∼’, like alpha ~ dnorm(0.0,
1.0E-4), which defines a random variable alpha as normally distributed with
mean 0 and 1E-4, i.e. variance 1E4. Note that the coincidence of the name dnorm
with the analogous R function can be misleading: the former is defined in terms
of the precision, i.e. the reciprocal of variance, while the latter is in terms of the
standard deviation. Therefore, we have the following equivalence:
dnormJAGS(m, p) ≡ dnormR(m, 1/
√
p)
The for loop at the very beginning of the model definition means:
Y1 ∼ dnorm(mu1, tau)
Y2 ∼ dnorm(mu2, tau)
...
YN ∼ dnorm(muN , tau)
i.e. it allows us to generate N values Yk having normal distribution with mean muk and
precision tau, with the values mu defined by the subsequent deterministic relation. In
1
Interested readers can deepen this relationship with the Bayesian framework and the ‘directed
graphical model’ in (Lunn et al., 2012).JAGS 481
such a model Y[k] and x[k] represent the vector of observed dependent and indepen￾dent values, respectively. Assuming normally distributed residuals in the regression
model: k = Yk − (α + βxk) the dependent variable is normal, and (α + βxk) is its
expected value, i.e. the mean.
The JAGS code for linear regression is inserted into a textConnection as before.
We apply JAGS linear regression to data coming from the dataset cars included in
the standard R Datasets Package (see library(help = "datasets")). Note from
the R listing that the covariate (the X data) is ‘centred’, i.e. mean(X) is subtracted
from each X value, because this reduces the posterior correlation between the angular
coefficient and the intercept term, and high levels of posterior correlation are known
to be problematic for Gibbs sampling.
Figure B.3 compares the regression line computed by JAGS (solid line) with the
linear fitting in R (dashed line).
## Code_4_Appendix_B.R
# Read data
data(cars)
X <- cars$speed
X <- X-mean(X)
Y <- cars$dist
# Perform JAGS computation
N <- length(X) # number of data points
# Model
mod <- textConnection("model {
mu ~ dnorm(0, 0.01);
tau.pro ~ dgamma(0.001,0.001);
sd.pro <- 1/sqrt(tau.pro);
beta ~ dnorm(0,0.01)
for(i in 1:N) {
predY[i] <- mu + beta*X[i];
Y[i] ~ dnorm(predY[i], tau.pro);
}
}")
# Init and definitions
jags.data = list("Y"=Y,"N"=N,"X"=X)
jags.params=c("sd.pro","mu","beta")
jags.inits <- list("tau.pro" = 1, "mu" = 0, "beta" = 0)
# Run
model <- jags.model(mod, data=jags.data, inits=jags.inits, n.chains=1)
update(model, n.iter=10000)
samples <- coda.samples(model, jags.params,10000)
out <- do.call(rbind.data.frame, samples)
names(out)
# Comparison with R linear fitting
plot(X,Y)
abline(lm(Y~X),col="red")
abline(mean(out$mu),mean(out$beta),col="blue")
# Coefficients
coefficients(lm(Y~X))
c(mean(out$mu),mean(out$beta))
In these short notes we have only scratched the surface of Bayesian inference using
JAGS. JAGS is very powerful and can do much more, like model comparison, analysis
of time and spatial series, non-parametric Bayesian analysis, and more.482 JAGS
−10 −5 0 5 10
0 20 40 60 80 100
car speed
stopping distance
Fig. B.3 Linear regression on the cars data, in R (dashed line) and JAGS (solid line).List of Symbols
Lowercase Latin symbols
a partial autocorrelation
c speed of light
e random noise
f frequency
g gravitational acceleration
h spatial distance
kB Boltzmann constant
i imaginary number
mZ deterministic trend in spatial analysis
t time
v velocity
w angular frequency
x where referred to a random variable is a realization of X
Uppercase Latin symbols
A amplitude
B backshift operator
Bern Bernoulli process
Bin binomial distribution
D diffusion coefficient
E expected value
energy
Exp exponential distribution
F distribution function
F family of subsets
F future set of events
Gam gamma distribution
L lag operator
P probability
P past set of events
Poiss Poisson distribution
R autocorrelation of a deterministic signal
S entropy
Fourier transform
S state space of a random variable
T parametric space for random variables
T set of all transient states484 List of Symbols
Var variance
Weib Weibull distribution
X random variable
Z random fields
Lowercase Greek symbols
γ autocovariance function
λ intensity parameter in Poission distribution
µ mean
ρ autocorrelation function
σ standard deviation
τ integrated correlation time
ω angular frequency
Upper case Greek symbols
∆ difference operator
Θ estimator
Ω probability space [-]List of R Codes
Code 2 1.R Throw of two dice
Code 3 1.R Simulate the system motion
Code 3 2.R Weather in the Land of Oz
Code 3 3.R Recurrence time in Markov chain
Code 3 4.R Birth-death process simulation
Code 3 5.R Buffon’s needle problem
Code 4 1.R Distribution of number of arrivals, time
Code 4 2.R Poisson process simulation
Code 4 3.R Poisson process simulation
Code 4 4.R Poisson process simulation
Code 4 5.R Nonhomogeneous Poisson process
Code 5 1.R Simple symmetric random walk
Code 5 2.R Simple symmetric random walk
Code 5 3.R Random walk with absorbing barriers
Code 5 4.R Random walk with reflecting barriers
Code 5 5.R Two-dimensional random walk
Code 5 6.R Brownian motion
Code 5 7.R Stochastic differential equation
Code 6 1.R White noise
Code 6 2.R Moving−average process
Code 6 3.R Autoregressive model (1)
Code 6 4.R Random walk with reflecting barriers
Code 6 5.R Autocorrelation structure of AR(2) and MA(2)
Code 6 6.R Mixed models
Code 6 7.R Random walk as AR process
Code 6 8.R Difference operator
Code 6 9.R Seasonal ARIMA
Code 6 10.R Best fit SARIMA orders
Code 6 11.R Temperature analysis
Code 6 12.R Loire river analysis
Code 6 13.R Analyse monthly runoff
Code 6 14.R Compute annual average
Code 6 15.R Manual search for best fitting486 List of R Codes
Code 7 1.R Single sinusoid
Code 7 2.R Sum of three sinusoidal signals
Code 7 3.R Gaussian noise
Code 7 4.R White noise smoothed periodogram
Code 7 5.R Red and blue noise
Code 7 6.R Temperature analysis
Code 7 7.R Searching hidden periodicity
Code 7 8.R Signal with trend (SSA)
Code 7 9.R Application to real data (temperature)
Code 7 10.R Application to real data (beer)
Code 8 1.R M-H algorithm target density : Standard Normal
Code 8 2.R M-H algorithm target density : Standard Normal
Code 8 3.R M-H algorithm target density : Bivariate Normal
Code 8 4.R Read the province coordinates and mutual distances
Code 9 1.R Unfair coin tossing example
Code 9 2.R Bayesian analysis of temperature and CO2
Code 9 3.R Linear regression using JAGS
Code 9 4.R Plot results
Code 9 5.R OLS regression
Code 9 6.R Radar data analysis
Code 9 7.R Radar data analysis
Code 9 8.R Radar data analysis
Code 9 9.R Bayesian analysis of a Poisson process
Code 9 10.R Bayesian analysis of a Poisson process
Code 9 11.R Where is the light?
Code 10 1.R Non linear fitting with genetic algorithm
Code 10 2.R Non linear fitting with genetic algorithm
Code 10 3.R Real coded GA
Code 10 4.R GA for ARIMA optimization
Code 10 5.R Travelling salesman with GA
Code 11 1.R Averaging time series
Code 11 2.R Ensemble averages
Code 11 3.R Batch mean method
Code 11 4.R Bootstrap method
Code 12 1.R Scatter plots and correlogram
Code 12 2.R Semivariogram
Code 12 3.R Semivariogram with anisotropy and trend
Code 12 4.R Kriging
Code 12 5.R KrigingList of R Codes 487
Code 12 6.R Semivariogram
Code 12 7.R Spacetime
Code 12 8.R Variogram for space time
Code 12 9.R Transform coordinates
Code 12 10.R Define model and starting parameters
Code 12 11.R Load DEM of Emilia-Romagna region
Code 12 12.R Regression kriging
Code 12 13.R Space time kriging
Code 12 14.R Plot results kriging
Code 12 15.R Matrices creation for GA analysis
Code 12 16.R Semivariogram with GA
Code 13 1.R Shannon’s entropy of pseudo random numbers
Code 13 2.R Shannon’s entropy of pseudo random numbers sequence
Code 13 3.R S entropy
Code 13 4.R Approximated entropy
Code Append A 1.R Bootstrap standard error of the mean
Code Append A 2.R Parametric bootstrap
Code Append B 1.R MCMC and JAGS
Code Append B 2.R Draw data from a Poisson distribution
Code Append B 3.R Linear regression with JAGS
Code Append B 4.R Comparison of JAGS regression with linear fittingReferences
Abraham, B. and Ledolter, J. (1983). Statistical Methods for Forecasting. Wiley &
Sons, USA.
Akaike, H. (1974). A new look at the statistical model identification. IEEE Trans￾actions on Automatic Control, 19, 716–723.
Antolini, G., Auteri, L., Pavan, V., Tomei, F., Tomozeiu, R., and Marletto, V. (2015).
A daily high-resolution gridded climatic data set for Emilia-Romagna, Italy, during
1961-2010. Journal of Climatology, 08.
Applegate, D. L., Bixby, R. E., Chvat`al, V., and Cook, W. J. (2006). The Traveling
Salesman Problem: A Computational Study. Princeton University Press, Princeton.
Bachelier, L. (1900). Th´eorie dela sp´eculation. Annales Scientifiques de l’Ecole Nor- ´
male Sup´erieure, 27, 21–86. English translation ‘Random Character of Stock Market
Prices’, translated by A. J. Boness in P. H. Cootner (ed.), 1964, MIT Press, Cam￾bridge.
Ben-Ameur, W. (2004). Computing the initial temperature of simulated annealing.
Computational Optimization and Applications, 29, 369–385.
Bennett, L. F., Goldsman, D., and Swain, J. J. (1981). Spaced batch means. Opera￾tions Research Letters, 10, 255–263.
Berger, A., Crucifix, M., Hodell, D., Mangili, C., McManus, J. F., Otto-Blisner, B.,
POl, K., Raynaud, D., Skinner, L. C., Tzedakis, C., Wolff, E. W., Yin, Q., Abe￾Ouchi, A., Barbante, C., Brovkin, V., Cacho, I., Ferretti, P., Ganopolski, A., Grimalt,
J., and Riveiros, N. Vazquez (2015, 11). Interglacials of the last 800,000 years.
Reviews of Geophysics, 162–219.
Bernardo, J. M. and Smith, A. F. M. (1994). Bayesian Theory. Wiley & Sons, USA.
Bertozzi, F. (2008). Relazione su i processi stocastici. Technical report, Universit´a
degli Studi di Bologna. In Italian.
Bertozzi, F. (2011). Indicatori di correlazione e di disordine basati sul concetto di
entropia. Technical report, Universit`a degli Studi di Bologna. Dissertation thesis.
Dottorato di ricerca in metodologia statistica per la ricerca scientifica. In Italian.
Binder, K. (1992). Introduction. In The Monte Carlo Method in Condensed Matter
Physics (ed. K. Binder), pp. 1–22. Springer, Germany.
Born, M. (1949). Natural Philosophy of Cause and Change. Oxford University Press,
Oxford, UK.
Box, G. E. P., Jenkins, G. M., and Reinsel, G .C. (1994). Time Series Analysis:
Forecasting and Control. Prentice Hall, New Jersey, USA.
Bretthorst, G. L. (1988). Bayesian Spectrum Analysis and Parameter Estimation.
Springer, Germany.
Broemeling, L. D. (2018). Bayesian Inference for Stochastic Processes. CRC Press,
USA.References 489
Brooks, S. P. and Roberts, G. O. (1992). Convergence assessment techniques for
Markov chain Monte Carlo. Statistics and Computing, 8, 319–335.
Brown, R. (1828). A brief account of microscopical observations made in the months
of June, July and August 1827, on the particles contained in the pollen of plants;
and on the general existence of active molecules in organic and inorganic bodies.
The Philosophical Magazine, 4, 161–173.
Brush, S. G. (1964). L. Boltzmann, Lecture on Gas Theory (Translated by S. G.
Brush). University of California Press, Berkeley and Los Angeles.
Brush, S. G. (1968). A history of random processes: I. Brownian Movement from
Brown to Perrin. Archive for History of Exact Sciences, 5, 1–36.
Buffon, George LeClerc (1777). Essai d’Arithm´etique Morale. Supplem´ent a l’Histoire
Naturelle, Volume 4.
Carlstein, E. (1986). The use of subseries values for estimating the variance of a
general statistic from a stationary sequence. The Annals of Statistics, 14, 1171–
1179.
Cern`y, V. (1985). Thermodynamical approach to the traveling salesman problem: an ˇ
efficient simulation algorithm. Journal of Optimization Theory and Applications, 45,
41–51.
Chaitin, G. J. (1966). On the length of programs for computing binary sequences.
Journal of the Association for Computing Machinery, 13, 547–569.
Chaitin, G. J. (1975). Randomness and mathematical proof. Scientific Ameri￾can, 232, 47–52.
Chib, S. (1993). Bayes regression with autoregressive errors: a Gibbs sampling ap￾proach. Journal of Econometrics (58), 275–294.
Chun, Y. and Griffith, D. A. (2013). Spatial Statistics and Geostatistics. SAGE, Los
Angeles, USA.
Cochrane, D. and Orcutt, G. H. (1949). Application of least squares regression to
relationships containing auto-correlated error terms. Journal of the American Sta￾tistical Association, 44, 32–61.
Costantini, D. and Garibaldi, U. (2004). The Ehrenfest fleas: from model to theory.
Synthese, 139, 107–142.
Cover, T. M. and Thomas, J. A. (1991). Elements of Information Theory. Wiley &
Sons, New York.
Cowles, M. K. and Carlin, B. P. (1996). Markov chain Monte Carlo convergence diag￾nostic: A comparative review. Journal of the American Statistical Association, 91,
883–904.
Cressie, N. A. C. (1993). Statistics for Spatial Data. Wiley & Sons, New York.
DeFinetti, B. (1995). Filosofia della probabilit`a. Il Saggiatore, Milano.
Delft, D. van (2014). Paule Ehrenfest’s final years. Physics Today, 67, 41–47.
Denbigh, K. G. and Denbigh, J. S. (1985). Entropy in Relation to Incomplete Knowl￾edge. Cambridge University Press, Cambridge, UK.
Dobrow, R. P. (2016). Introduction to Stochastic Processes with R. Wiley & Sons,
USA.
Eaves, J. L. and Reedy, E. K. (1987). Principles of Modern Radar. Chapman & Hall,
UK.490 References
Efron, B. (1979). Bootstrap methods: Another look at the jackknife. The Annals of
Statistics, 7, 1–26.
Efron, B. (1989). Computer-intensive statistical inference. Current Contents, 29, 16.
Efron, B. and Tibshirani, R. J. (1993). An Introduction to the Bootstrap. Chapman
& Hall, New York.
Ehrenfest, P. and Ehrenfest, T. (1907). Uber zwei bekannte Einw ¨ ¨ande gegen das
Boltzmannsche H-Theorem. Physikalische Zeitschrift, 8, 311–316.
Einstein, A. (1905). Investigations on the Theory of the Brownian Movement. An￾nalen der Physik, 17, 549–560. Edited with notes by R. Furth, translated by A. D. ¨
Cowper, 1956, Dover.
Einstein, A. (1925). Quantentheorie des einatomigen idealen gases. Berliner
Berichte, 23, 3–14.
Evans, M. and Swartz, T. (2000). Approximating Integrals via Monte Carlo and
Deterministic Methods. Oxford University Press, Oxford, UK.
Fanaee-T, H. and Gama, J. (2013). Event labeling combining ensemble detectors and
background knowledge. Progress in Artificial Intelligence, 1–15.
Feller, W. (1970). An Introduction to Probability Theory and its Applications (3rd
edn). Wiley & Sons, New York.
Fiser, O. (2010). The Role of DSD and Radio Wave Scattering in Rain Attenua￾tion. In Geoscience and Remote Sensing, New Achievements (ed. P. Imperatore and
D. Riccio), Chapter 23, pp. 437–456. Intech Open, India.
Flyvbjerg, H. and Petersen, H. G. (1989). Error estimates on averages of correlated
data. Journal of Chemical Physics, 91, 461–466.
Fosdick, L. D. (1963). Monte Carlo computations on the Ising lattice. In Methods in
Computational Physics (ed. B. Alder, S. Fernbach, and M. Rotenberg), pp. 245–280.
Academic Press, New York.
Friedberg, R. and Cameron, J. E. (1970). Test of the Monte Carlo method: fast
simulation of a small Ising lattice. The Journal of Chemical Physics, 52, 6049–
6058.
Galavotti, M. C. (2005). Philosophical Introduction to Probability. CSLI Pubblica￾tions, Stanford University.
Gelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. B. (2004). Bayesian Data
Analysis. Chapman & Hall/CRC, New York, USA.
Gelman, A. and Rubin, D. B. (1992a). Inference from iterative simulation using
multiple sequences. Statistical Science, 7, 457–511.
Gelman, A. and Rubin, D. B. (1992b). A single series from the Gibbs sampler provides
a false sense of security. In BBayesian of Statistics 4 (ed. J. M. Bernardo, J. O.
Berger, A. P. Dawid, and A. F. M. Smith), pp. 625–632. Oxford University Press,
Oxford, UK.
Geyer, C. J. (1992). Practical Markov chain Monte Carlo. Statistical Science, 7,
473–511.
Geyer, C. J. (1996). Estimation and optimization of functions. In Markov Chain
Monte Carlo in Practice (ed. W. R. Gilks, S. Richardson, and D. Spiegelhalter), pp.
241–258). Chapman and Hall, London.
Geyer, C. J. (2011). Introduction to Markov Chain Monte Carlo. In Handbook ofReferences 491
Markov Chain Monte Carlo (ed. S. Brooks, A. Gelman, G. Jones, and X.-L. Meng),
pp. 3–48. CRC Press-Chapman and Hall/CRC.
Geyer, C. J. (2019). Package mcmc: Marco chain monte carlo.
Giannerini, S. (2017). Package tseriesEntropy: Entropy Based Analysis and Tests
for Time Series.
Glaz, J. and Balakrishnan, N. (1999). Scan Statistics and Applications. Springer,
Germany.
Goldberg, D. E (1989). Genetic Algorithms in Search, Optimization, and Machine
Learning. Addison Wesley, Reading, Massachusetts.
Golyandina, N., Korobeynikov, A., and Zhigljavsky, A. (2018). Singular Spectrum
Analysis with R. Springer, Germany.
Gottlieb, S., Mackenzie, P. B., Thacker, H. B., and Weingarten, D. (1986). Hadronic
coupling constants in lattice gauge theory. Nuclear Physics B, 263, 704–730.
Graeler, B., Pebesma, E., and Heuvelink, G. (2016). Spatio−temporal interpolation
using gstat. The R Journal, 8, 204–218.
Granger, C. W., Maasoumi, E., and Racine, J. (2004). A dependence metric for
nonlinear time series. Journal of Time Series Analysis, 25, 649–669.
Grimmett, G. (2018). Probability on Graphs (2nd edn). Cambridge University Press,
Cambridge, UK.
Grimmett, G. and Stirzaker, D. (2001). Probability and Random Processes (3rd edn).
Oxford University Press, Oxford, UK.
Hacking, I. (1975). The Emergence of Probability. Cambridge University Press,
Cambridge, UK.
Hadfield, J. (2019). Package MCMCglmm: Mcmc generalised linear mixed models.
Hall, P., Horowitz, J. L., and Jing, B.-Y. (1995). On blocking rules for the bootstrap
with dependent data. Biometrika, 8, 561–574.
Hastings, K. (1970). Monte Carlo sampling methods using Markov chains and their
applications. Biometrika, 57, 97–109.
Hausser, J. and Strimmer, K. (2019). Entropy inference and the james-stein estima￾tor, with application to nonlinear gene association networks. Journal of Machine
Learning Research, 10, 1469–1484.
Hausser, J. and Strimmer, K. (2021). Package entropy: Estimation of Entropy,
Mutual Information and Related Quantities.
Hipel, K. W. and McLeod, A. I. (1994). Time Series Modelling of Water Resources
and Environmental Systems. Elsevier, Amsterdam, The Netherlands.
Hoff, P. D. (2009). A First Course in Bayesian Statistical Methods. Springer, Ger￾many.
Holland, J. H. (1992). Adaptation in Natural and Artificial Systems. MIT Press,
Cambridge, UK.
Huffaker, R., Bittelli, M., and Rosa, R. (2017). Non Linear Time Series Analysis
with R (1st edn). Oxford University Press, Oxford, UK.
Hulme, M. (1999). Global monthly precipitation, 1900−1999.
Hume, D. (2007). A Treatise of Human Nature. A critical edition. Oxford University
Press, Oxford, UK.492 References
Hyndman, R. J. and Fan, Y. (1996). Sample quantiles in statistical packages. Amer￾ican Statistician, 50, 361–365.
Iacus, S. M. and Masarotto, G. (2003). Laboratorio di Statistica con R. McGraw-Hill,
Italy.
Insua, D. R., Ruggeri, F., and Wiper, M. P. (2012). Bayesian Analysis of Stochastic
Process Models. Wiley & Sons, USA.
Isaaks, E. H. and Srivastava, R. M. (1989). Applied Geostatistics. Oxford University
Press, Oxford, UK.
Jaynes, E. T. (1957). Information theory and statistical mechanics II. The Physical
Review, 108, 171–190. Also in Rosenkrantz (1983), pp. 19-38.
Jaynes, E. T. (1965). Gibbs vs. Boltzmann entropies. American Journal of
Physics, 33, 391–398. Also in Rosenkrantz (1983), pp. 79-86.
Jaynes, E. T. (2003). Probabillity Theory: the logic of science. Cambridge University
Press, Cambridge, UK.
Jeffreys, H. (1931). Scientific Inference. Cambridge University Press, Cambridge,
UK.
Jeffreys, H. (1939). Theory of Probability. Oxford University Press, Oxford, UK.
Jenkins, G.M. and Watts, D.G. (1968). Spectral analysis and its applications. Holden￾Day, San Francisco, U.S.A.
Jones, P. W. and Smith, P. (2018). Stochastic Processes: An Introduction. CRC
Press, Boca Raton, Florida, USA.
Kac, M. (1959). Probability and Related Topics in Physical Sciences. Lectures in
applied mathematics series, Vol. 1A. American Mathematical Society, Providence.
Keuth, H. (2005). The Philosophy of Karl Popper. Cambridge University Press,
Cambridge, UK.
Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P. (1983). Optimization by simulated
annealing. Science, 220, 671–680.
Kitanidis, P.K. (1997). Introduction to geostatistics. Applications to hydrogeology.
Cambridge University Press, Cambridge, UK.
Klein, M. J. (1985). Paul Ehrenfest. The Making of a Theoretical Physicist Vol. 1.
Elsevier, Amsterdam, The Netherlands. Volume 2 has never been published.
Kolmogorov, A. (1965). Three approaches to the quantitative definition of informa￾tion. Problems of Information Transmission, 1, 1–7.
Kolmogorov, A. N. (1950). Foundations of the theory of probability (Translation of
Grundbegriffe der Wahrscheinlchksrechnung, Springer, Berlin, 1933 edn). Chelsea,
G.
Koza, J. R. (1996). Genetic Programming. MIT Press, Cambridge, UK.
Krige, D. G. (1951). A statistical approach to some basic mine valuation problems
on the witwatersrand. Journal of the Chemical, Metallurgic and Mining Society of
South Africa, 52(6), 119–139.
Kunsch, H. R. (2015). The jackknife and the bootstrap for general stationary obser- ¨
vations. The Annals of Statistics, 17, 1217–1241.
Landau, D. P. (1976). Finite-size behavior of the Ising square lattice. Physical Review
B, 13, 2997–3011.
Laplace, P.S. (1825). Essai Philosophique sur les probabilit´es. Cambridge UniversityReferences 493
Press, Cambridge, UK.
Lawler, G. F. (1995). Introduction to Stochastic Processes. Chapman and Hall/CRC,
New York.
Lawler, G. F. (2006). Introduction to Stochastic Processes (2nd edn). Chapman and
Hall.
Leuangthong, O., Khan, K. D., and Deutsch, C. V. (2008). Solved Problems in
Geostatistics. Wiley & Sons, New Jersey, USA.
Lin, S. (1965). Computer solutions of the traveling salesman problem. Bell System
Technical Journal, 44, 2245–2269.
Liu, R. Y. and Singh, K. (1992). Moving blocks jackknife and bootstrap capture weak
dependence. In Exploring the Limits of Bootstrap (ed. R. Lepage and L. Billard),
pp. 3–43. Wiley & Sons, New York.
Lord Rayleigh (1880). On the resultant of a large number of vibrations of the same
pitch and of arbitrary phase. Philosophical Magazine and Journal of Science, 10,
73–78. Reprinted in Scientific Papers, Vol. I (1869-1881), p. 491.
Lord Rayleigh (1899). On James Bernoulli’s theorem in probabilities. Philosophical
Magazine and Journal of Science, 47, 246–251. Reprinted in Scientific Papers, Vol.
IV (1892-1901), p. 370.
Lord Rayleigh (1905). The problem of the random walk. Nature, 72, 1476–4687.
Reprinted in Scientific Papers, Vol. I (1869-1881), p. 491.
Lorenz, E. N. (1963). Deterministic nonperiodic flow. Journal of the atmospheric
sciences, 20, 130–141.
Lorenz, E. N. (1995). The essence of chaos (2nd edn). University of Washington
Press, Seattle.
Luethi, D., Floch, M. Le, Bereiter, B., Blunier, T., Barnola, J., Siegenthaler, U.,
Raynaud, D., Jouzel, J., Fischer, H., Kawamura, K., and Stocker, T. F. (2008).
High-resolution carbon dioxide concentration record 650,000-800,000 years before
present. Nature, 453, 379–382.
Lunn, D., Jackson, C., Best, N., Thomas, A., and Spiegelhalter, D. (2012). The BUGS
Book: A Practical Introduction to Bayesian Analysis. Chapman & Hall/CRC Texts
in Statistical Science. Taylor & Francis.
Maasoumi, E. and Racine, J. (2002). Entropy and predictability of stock market
returns. Journal of Econometrics, 107, 291–312.
Madsen, H. (2008). Time Series Analysis. Chapman and Hall, UK.
Martin, A. D., Quinn, K. M., and Park, J. H. (2018). Package MCMCpack: Markov
chain Monte Carlo (mcmc) package.
Martin-L¨of, P. (1966). The definition of random sequences. Information and Con￾trol, 9, 602–619.
Masuda, N. and Hiraoka, T. (2020). Waiting-time paradox in 1922. Northeast Journal
of Complex Systems, 1–19.
Matheron, G. (2019). Matheron’s Theory of Regionalised Variables. (Ed. V.
Pawlowsky-Glahn and J. Serra). Oxford University Press”
Oxford, UK.
Matthews, P. (1989). A slowly mixing Markov Chain with implications for Gibbs
sampling. Statistics and Probability Letters, 17, 231–236.
Medhi, J. (2003). Stochastic Models in Queueing Theory (2nd edn). Academic Press,494 References
Amsterdam, The Netherlands.
Metropolis, N. (1987). The Beginning of the Monte Carlo Method. Los Alamos
Science Special Issue, 15, 125–130.
Metropolis, N., Rosembluth, A. W., Rosembluth, M. N., Teller, A. H., and Teller, E.
(1953). Equation of state calculations by fast computing machines. The Journal of
Chemical Physics, 21, 1087–1092.
Michalewicz, Z. (1996). Genetic Algorithms + Data Structures = Evolution Programs.
Springer, Berlin. Germany.
Mignani, S. and Rosa, R. (1995). The moving block bootstrap to assess the accuracy
of statistical estimates in Ising model simulations. Computer Physics Communica￾tions, 92, 203–213.
Mignani, S. and Rosa, R. (2001). Markov chain monte carlo in statistical mechanics:
the problem of accuracy. Technometrics, 43, 347–355.
Minasny, B. and McBratney, A. B. (2005). The mat´ern function as a general model
for soil variograms. Geoderma (128), 192–207.
Morales, J. J., Nuevo, M. J., and Rull, L. F. (1990). Statistical error methods in
computer simulations. Journal of Computational Physics, 89, 432–438.
Øksendal, B. (2003). Stochastic Differential Equations: An Introduction with Appli￾cations (6th edn). Springer, Germany.
Ore, O. (1960). Pascal and the invention of probability theory. The American Math￾ematical Monthly, 67, 409–419.
Pearson, K. (1905). The problem of the random walk. Nature, 72, 294.
Pebesma, Edzer (2018). Simple Features for R: Standardized Support for Spatial
Vector Data. The R Journal, 10(1), 439–446.
Pedrosa, (A. C. and Schmeiser, B. W. (1993). Asymptotic and finite-sample cor￾relations between obm estimators. In Proceedings of the 1993 Winter Simulation
Conference (ed. G. W. Evans, M. Mollaghasemi, E. C. Russel, and W. E. Biles), pp.
481–488.
Peterson, S. C. and Noble, P. B. (1972). A two-dimensional random-walk analysis of
human granulocyte movement. Biophysical Journal, 12, 1048–1055.
Petit, J., Jouzel, J., Raynaud, D., Barkov, N., Barnola, J. M., Basile-Doelsch, I.,
Bender, M., Chappellaz, J., M. Davis, M, Delaygue, G., Delmotte, M., Kotlyakov,
V. M., Legrand, M., Lipenkov, V., Lorius, C., Pepin, L., Ritz, C., Saltzman, E.,
and Stievenard, M. (1999, 06). Climate and atmospheric history of the past 420,000
years from the Vostok ice core, Antarctica. Nature, 399, 429–436.
Pincus, S. (1991). Approximate entropy as a measure of system complexity. Proceed￾ings of the National Academy of Sciences USA, 88, 2297–2301.
Pincus, S. (2008). Approximate entropy as an irregularity measure for financial data.
Econometric Reviews, 27, 329–362.
Pincus, S., Gladstone, I. M., and Ehrenkranz, R. A. (1991). A regularity statistic for
medical data analysis. Journal of Clinical Monitoring and Computing, 7, 335–345.
Pincus, S. and Kalman, R. E. (1997). Not all (possibly) ‘random’ sequences are
created equal. Proceedings of the National Academy of Sciences USA, 94, 3513–
3518.References 495
Pincus, S. and Singer, B. H. (1996). Randomness and degrees of irregularity. Pro￾ceedings of the National Academy of Sciences USA, 93, 2083–2088.
Plummer, M. (2003). JAGS: A program for analysis of Bayesian graphical models
using Gibbs sampling. 3rd International Workshop on Distributed Statistical Com￾puting (DSC 2003); Vienna, Austria, 124.
Poincar´e, J. H. (1908). Science et m´ethode. Flammarion, Paris. English edition
‘Science and Method’, 1914, Thomas Nelson & Sons, London. Reprinted Thoemmes
Press, Bristol 1996.
Poisson, S. D (1837). Recherches sur la probabilit´e des jugements en mati`ere crim￾inelle et en mati`ere civile. Bachelier, Paris.
Politis, D. N. (2003). The impact of bootstrap methods on time series analysis.
Statistical Science, 18, 219–230.
Politis, N. D. and Romano, J. P. (1992). A circular block-resampling procedure for
stationary data. In Exploring the Limits of Bootstrap (ed. R. Lepage and L. Billard),
pp. 366–381. Wiley & Sons, New York.
Popper, K. R. (1934). Logik der Forschung. Springer, Wien. English enlarged edition
‘The Logic of Scientific Discovery’, 1959, Hutchinson, London, third revised edition
1968.
Poularikas, A. D. (1999). The Handbook of Formulas and Tables for Signal Processing.
CRC Press, USA.
Preece, D. A., Ross, G. J. S., and Kirby, S. P. J. (1988). Bortkewitsch’s horse-kicks
and the generalised linear model. Journal of the Royal Statistical Society: Series D
(The Statistician), 37, 313–318.
Press, H., Teukolsky, S. A., Vetterling, T., and Flannery, P. (1992). Numerical Recipes
in Fortran 77: the Art of Scientific Computing (2nd edn). Cambridge University
Press, Cambridge, UK.
Rastrigin, L. A. (1963). The convergence of the random search method in the extremal
control of a many parameter system. Automation and Remote Control, 10, 1337–
1342.
Richards, A. M. (2014). Fundamentals od Radar Signal Processing. McGraw Hill,
New York, USA.
Ripley, B. D. (1987). Stochastic Simulation. Wiley & Sons, New York.
Robert, C. and Casella, G. (2011). A history of markov chain monte carlo – subjective
recollections from incomplete data. Statistical Science, 26, 102–115.
Robert, C. P. (1994). The Bayesian Choice: A Decision-Theoretic Motivation. Oxford
University Press, Oxford, UK.
Robert, C. P. and Casella, G. (1999). Monte Carlo Statistical Methods. Springer￾Verlag, New York.
Rosa, R. (1993). At the birth of quantum statistics: Challenge and defence of the ‘a
priori’ statistical counting. Philosophia Naturalis, 22, 84–105.
Rosenkrantz, R. D. (1983). E. T. Jaynes: Papers on Probability, Statistics and Sta￾tistical Physics. Reidel, Dordrecht.
Ross, S.M. (2014). Introduction to Probability Models (11th edn). Academic Press,
San Diego, CA, USA.496 References
Ross, S. M. (2019). Introduction to Probability Models (12th edn). Elsevier, Amster￾dam, Netherlands.
Salas, J.D. and Smith, R.A. (1981). Physical basics of stochastic models of annual
flows. Water Resource Research, 17, 428–430.
Scrucca, L. (2013). GA: A package for genetic algorithms in R. Journal of Statistical
Software, 53(4), 1–37.
Scrucca, L. (2017). On some extensions to GA package: hybrid optimisation, paral￾lelisation and islands evolution. The R Journal, 9(1), 187–206.
Sen, A. and Srivastava, M. (1990). Regression Analysis: Theory Methods and Appli￾cations. Springer, Germany.
Shannon, C. E. (1948). A mathematical theory of communication. The Bell System
Technical Journal, 27, 379–423.
Shumway, R.H. and Stoffer, D.S. (2006). Time Series Analysis and Its Applications.
Springer, New York, U.S.A.
Sivia, D. S. (1996). Data Analysis: a Bayesian Tutorial. Oxford University Press,
Oxford, UK.
Smith, A. F. M. (1991). Bayesian computational methods. Philosophical Transac￾tions: Physical Sciences and Engineering, 337, 369–386.
Smith, A. F. M. (1992). Discussion of C. J. Geyer and E. A. Thompson ‘Constrained
Monte Carlo Maximum Likelihood for Dependent Data’. Journal of the Royal Sta￾tistical Society. Series B (Methodological), 54, 657–699.
Smoluchowski, M. (1906). Zur kinetischen theorie der brownschen molekularbewe￾gung und der suspensionen. Annalen der Physik, 21, 756–780.
Solomonoff, J. (1964). A formal theory of inductive inference. Part I. Information
and Control, 7, 1–22.
Song, S. and Song, J. (2013). A note on the history of the Gambler’s ruin problem.
Communications for Statistical Applications and Methods, 20, 1–12.
Stoica, P. and Moses, R. (2005). Spectral Analysis of Signals. Pearson Prentice Hall,
New Jersey, USA.
Tijms, H. C. (2003). A First Course in Stochastic Models. Wiley & Sons, U.S.A.
Todhunter, I. (1865). A History of the Mathematical theory of Probability. Cambridge
University Press, Cambridge, UK.
Varouchakis, E. A. (2018). Geostatistics: Mathematical and statistical basis. In
Spatiotemporal Analysis of Extreme Hydrological Events. Elsevier, Amsterdam, The
Netherlands.
von Mises, R. (1928). Wahrscheinlickeit, Statistik und Wahrheit. Springer, Wien.
English edition ‘Probability, Statistics and Truth’, Allen and Unwin, 1939, London￾New York. Reprinted, Dover, 1968, New York.
von Plato, J. (1994). Creating Modern Probability. Cambridge University Press,
Cambridge, UK.
Vorosmarty, C. J., Fekete, B. M., and Tucker, B. A. (1998). Global River Discharge,
1807-1991, ORNL Distributed Active Archive Center.
Waismann, F. (1930). Logische analyse des wahrscheinlichkeitsbegriffs. Erkentnis, 1,
228–248. English edition ‘A Logical Analysis of the Concept of Probability’, in
‘Philosophical Papers’, B. McGuinness (ed.), Reidel, 1977, Dordrecht, Holland.References 497
Webster, R. and Oliver, M.A. (2007). Geostatistics for Environmental Scientists.
Wiley & Sons, UK.
Whitmer, C. (1984). Over-relaxation methods for Monte Carlo simulations of
quadratic and multiquadratic actions. Physical Review D, 29, 306–311.
Wikle, C. K., Zammit-Mangion, A., and Cressie, N. (2019). Spatio-Temporal Statistics
with R. Chapman and Hall/CRC, New York.
Wood, W. W. (1968). Monte Carlo studies of simple liquid models. In The Physics of
Simple Liquids (ed. H. Temperley, J. Rowlinson, and G. Rushbrooke), pp. 115–230.
North-Holland, Amsterdam.
Wood, W. W. and Parker, F. R. (1957). Monte Carlo equation of state of molecules
interacting with the Lennard-Jones potential. I. Supercritical isotherm at about
twice the critical temperature. Journal of Chemical Physics, 27, 720–733.
Yates, R. D. and Goodman, D. J. (2015). Probability and Stochastic Processes (3rd
edn). Wiley & Sons.
Ye, L., Hanson, L. S., Ding, P., Wang, D., and Vogel, R. M. (2018). The probability
distribution of daily precipitation at the point and catchment scales in the United
States. Hydrology and Earth System Sciences, 22, 6519–6531.
Yule, G. U. (1927). On a method of investigating periodicities in disturbed series with
special reference to Wolfer’s sunspot numbers. Philosophical Transactions, 226,
267–298.Index
absorbing barrier, 147
code, 148
Antarctica exmaple, 297
ApEn, 461
code, 463, 464
computationally random, 463
AR process, 196
ARIMA process, 210
ARMA process, 204
asymptotic transition matrices, 38
autocovariance
function, 17
autoregressive process, 196
Bayes
theorem, 294
Bayesian
analysis, 300
approach, 297
paradigm, 294
regression, 303
birth-death process, 73
boot(R package), 471
bootstrap
empirical distribution function, 467
error propagation, 473
ideal estimate, 469
in R, 471
Monte Carlo estimate, 469
origin of the name, 467
plug-in principle, 468
replication, 467
sample, 467
bootstrap(R package), 471
Brownian motion
drift parameter, 183
first passage times, 179
expected value, 180
fractal dimension, 181
independent increment, 173
random set, 181
reflection principle, 180
standard, 174
stationary increment, 173
stochastic differential equation, 183
stopping time, 179
strong Markov property, 179
time homogeneity, 174
Cantor set, 182
Hausdorff–Besicovitch dimension, 182
Cesaro
convergence, 50
Chapman-Kolmogorov equation, 35, 42, 62
closed class, 44
closed set, 44
Communicability
property, 42
complexity, 453
conditional entropy, 458
conditional probability, 294
counting process, 108
dead time, 138
detailed balance, 74
DFT, 232
difference equation, 153
distribution
equilibrium, 38
invariant, 38
stationary, 38
entropy
Shannon, 456
Shannon , 456
code, 459
surprise, 456
equilibrium probability, see stationary
probability distribution
Ergodic
hypothesis, 25
processes, 20
theory, 25
Ergodicity
ergodicity, 18
estimator, 83
expected duration, 157
fair game, 155
FFT, 234
first visit time, 51
Fourier transform, 232
discrete, 232
fast, 234
fractal dimension
Hausdorff–Besicovitch, 181
gambler’s ruin, 152
genetic algorihm, 335
genetic algorithmIndex 499
crossover, 337
elitism, 357
fitness function, 336
mutation, 338
operators, 337
, 359
schemata theorem, 347
selection, 340
simple, 339
hitting time, 51
identity matrix, 40
increments, 108
independent increments, 108
infinitesimal generator matrix, 65
initial distribution, 29
initial state, 28
initiation function, 31
integrated ARMA process, 210
intensity matrix, 65
, 67
inter arrival time, 110
invariant probability, see stationary
probability distribution
JAGS, 300
joint entropy, 458
kriging, 424
ordinary, 425
spatial, 424
universal, 430
lag operator, 409
last exit time, 51
likelihood, 294
linear congruential method, 447
mixed, 447
MA process, 191
higher order, 195
invertibility, 195
Markov chain
states transition diagram, 29
aperiodic, 50
asymptotically stationary, 41
continuous time, 61
discrete time, 27
embedded, 70
ergodic, 41
, 55
irreducible, 42
example, 43
, 44
periodic
convergence, 50
reducible
example, 43
regular, 61
time homogeneous, 27
, 61
Markov process, 25
Markovian property, 27
markovian property, 25
, 27
mathematical randomness, 448
matrix transition function, 61
, 64
maximum complexity, 453
mean recurrence time, 54
memoryless process, 69
Monte Carlo
method, 22
recurrence time, 58
steps, 12
moving average process, 191
mutual information, 458
noise
colored, 248
spectrum, 188
white, 187
noise spectrum, 246
NP-hard problem, 284
occupation time, 39
one-step transition probability, 27
operator
lag, 190
, 409
partially reflecting barrier, 161
partition theorem, 154
perfectly reflecting barriers, 159
periodogram, 240
smoothed, 244
physical randomness, 449
plug-in principle, 470
Poison process
first definition, 110
intensity, 111
rate, 111
Poisson
horse-kick, 107
judiciary statistics, 107
V-1 and V-2 flying bombs, 107
Poisson distribution, 110
Poisson process, 73
, 321
second definition, 111
third definizion, 120
Popper
empirical random sequence, 451
freedom from after effects, 451
ideally random sequence, 450
propensity interpretation, 452
posterior probability, 294
power spectral density, 240
, 241
Principle of Maximum Entropy, 457
prior probability, 294
, 304
probability
geometric definition of the, 82
probability vector, 33
process
AR, 196
ARIMA, 210
ARMA, 204
autoregressive, 196500 Index
autoregressive moving-average, 204
integrated ARMA, 210
MA, 191
moving average, 191
SARIMA, 212
seasonal ARIMA, 212
pure birth process, 73
quantiles, 151
queueing model, 78
M/M/1, 78
radar detection, 309
random time, 51
random walk
expectation, 142
first return, 144
position, 142
recurrent, 142
simple, 140, 208
symmetric, 140
transient, 142
two-dimensional, 165
code, 168
final distance, 167, 171
maximum distance, 171
minimum distance, 171
recurrence, 166
unrestricted, 147
variance, 142
rate matrix, 65
recurrence property, 53
reflecting barrier
code, 159
reflecting barriers, 159
seasonal ARIMA process, 212
semivariogram, 413
semivariogram cloud, 420
Shapiro-Wilk test, 145, 472
simulated annealing, 284
singular spectrum analysis, 253, 254
singular value decomposition, 254
smoothing window, 247
sojourn time, 59, 67
spacetime processes, 430
spatial kriging, 424
spatial process, 404, 405
spectral analysis, 250
singular, 253
spectrum, 232, 234
blue, 248
noise, 246
power density, 240, 241
red, 248
spectrum of stochastic signal, 237
SSA, 254
principal components, 257
trajectory matrix, 255
standard deviation, 83
standard error, 83
state
absorbing, 42, 43
accessible, 42
aperiodic, 50
communicating, 42
null recurrent, 54
periodic, 49
positive recurrent, 54
recurrent, 51
transient, 51
State space
definition, 13, 403
stationary increments, 109
stationary transition probability, 27
stochastic
hydrology, 219
stochastic matrix, 28
stochastic process
spatial, 404
stochastic semigroup, 62
stopping time, 51
strong Markov property, 52
SVD, 254
temporal resolution, 138
test/universal, 454
Time series
definition, 14
transience property, 53
transition frequency, 63
transition intensity, 62
transition matrix, 28
in one step, 28
transition probability, 27
n-step, 35
two-step, 35
transition probability function, 61
Travelling Salesman Problem, 284
TSP, 284
annealing code, 287
genetic algorithm, 367
update function, 30
variogram, 413
cloud, 420
experimental, 417
model, 415
sample, 417
von Mises
collective, 449
Existence of limits, 449
Insensitive to place selection, 449
von Mises, R., 449
waiting time, 109
paradox, 321
