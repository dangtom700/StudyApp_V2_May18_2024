Contents
List of Figures xviii
List of Algorithms xix
Acknowledgments xxi
Nomenclature & Abbreviations xxiii
1 Introduction 1
I Background 7
2 Linear Algebra 9
2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2 Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.3 Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.4 Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.4.1 Linear Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.4.2 Eigen Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3 Probability Theory 17
3.1 Set Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.2 Probability of Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.3 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.3.1 Discrete Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.3.2 Continuous Random Variables . . . . . . . . . . . . . . . . . . . . . . . 23
3.3.3 Conditional Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . 24
3.3.4 Multivariate Random Variables . . . . . . . . . . . . . . . . . . . . . . . 25
3.3.5 Moments and Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3.4 Functions of Random Variab les . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
3.4.1 Linear Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
3.4.2 Linearization of Nonlinear Functions . . . . . . . . . . . . . . . . . . . . 31j.-a. goulet vi
4 Probability Distributions 35
4.1 Normal Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4.1.1 Univariate Normal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4.1.2 Multivariate Normal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.1.3 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.1.4 Example: Conditional Distributions . . . . . . . . . . . . . . . . . . . . 40
4.1.5 Example: Sum of Normal Random Variables . . . . . . . . . . . . . . . 40
4.2 Log-Normal Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2.1 Univariate Log-Normal . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2.2 Multivariate Log-Normal . . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.2.3 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
4.3 Beta Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
5 Convex Optimization 47
5.1 Gradient Ascent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
5.2 Newton-Raphson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
5.3 Coordinate Ascent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
5.4 Numerical Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
5.5 Parameter-Space Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . 54
II Bayesian Estimation 57
6 Learning from Data 59
6.1 Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
6.2 Discrete State Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
6.2.1 Example: Disease Screening . . . . . . . . . . . . . . . . . . . . . . . . . 61
6.2.2 Example: Fire Alarm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
6.2.3 Example: Post-Earthquake Damage Assessment . . . . . . . . . . . . . . 65
6.3 Continuous State Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
6.3.1 Likelihood: f (D|x) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
6.3.2 Evidence: f (D) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
6.3.3 Posterior: f (x|D) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.3.4 Number of Observations and Identifiability . . . . . . . . . . . . . . . . 70
6.4 Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
6.4.1 Prior: f(✓) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
6.4.2 Likelihood: f (D|✓) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
6.4.3 Posterior PDF: f(✓|D) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
6.5 Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
6.5.1 Monte Carlo Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
6.5.2 Monte Carlo Sampling: Continuous State Variables . . . . . . . . . . . . 75
6.5.3 Monte Carlo Sampling: Parameter Estimation . . . . . . . . . . . . . . . 78
6.6 Conjugate Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
6.7 Approximating the Posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82probabilistic machine learning for civil engineers vii
6.7.1 Maximum Likelihood and Posterior Estimates . . . . . . . . . . . . . . . 82
6.7.2 Laplace Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
6.8 Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
7 Markov Chain Monte Carlo 89
7.1 Metropolis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
7.2 Metropolis-Hastings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
7.3 Convergence Checks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
7.3.1 Burn-In Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
7.3.2 Monitoring Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
7.3.3 Estimated Potential Scale Reduction . . . . . . . . . . . . . . . . . . . . 94
7.3.4 Acceptance Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
7.3.5 Proposal Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
7.4 Space Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
7.5 Computing with MCMC Samples . . . . . . . . . . . . . . . . . . . . . . . . . . 99
III Supervised Learning 105
8 Regression 107
8.1 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
8.1.1 Mathematical Formulation . . . . . . . . . . . . . . . . . . . . . . . . . 108
8.1.2 Overfitting and Cross-Validation . . . . . . . . . . . . . . . . . . . . . . 110
8.1.3 Mathematical Formulation >1-D . . . . . . . . . . . . . . . . . . . . . . 113
8.1.4 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
8.2 Gaussian Process Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
8.2.1 Updating a GP Using Exact Observations . . . . . . . . . . . . . . . . . 116
8.2.2 Updating a GP Using Imperfect Observations . . . . . . . . . . . . . . . 118
8.2.3 Multiple Covariates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
8.2.4 Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
8.2.5 Example: Soil Contamination Characterization . . . . . . . . . . . . . . 121
8.2.6 Example: Metamodel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
8.2.7 Advanced Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . 124
8.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
8.3.1 Feedforward Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . 127
8.3.2 Parameter Estimation and Backpropagation . . . . . . . . . . . . . . . . 132
8.3.3 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
8.3.4 Example: Metamodel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
9 Classification 139
9.1 Generative Classifiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
9.1.1 Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
9.1.2 Example: Post-Earthquake Structural Safety Assessment . . . . . . . . . 143
9.2 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144j.-a. goulet viii
9.3 Gaussian Process Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
9.4 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
9.5 Regression versus Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
IV Unsupervised Learning 155
10 Clustering and Dimension Reduction 157
10.1 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
10.1.1 Gaussian Mixture Models . . . . . . . . . . . . . . . . . . . . . . . . . . 157
10.1.2 K-Means . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
10.2 Principal Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
11 Bayesian Networks 167
11.1 Graphical Models Nomenclature . . . . . . . . . . . . . . . . . . . . . . . . . . 169
11.2 Conditional Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
11.3 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
11.4 Conditional Probability Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 173
11.4.1 Fully Observed Bayesian Network . . . . . . . . . . . . . . . . . . . . . 173
11.4.2 Partially Observed Bayesian Network . . . . . . . . . . . . . . . . . . . 176
11.5 Dynamic Bayesian Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
12 State-Space Models 181
12.1 Linear Gaussian State-Space Models . . . . . . . . . . . . . . . . . . . . . . . . 182
12.1.1 Basic Problem Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
12.1.2 General Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
12.1.3 Forecasting and Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . 190
12.1.4 Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
12.1.5 Limitations and Practical Considerations . . . . . . . . . . . . . . . . . 193
12.2 State-Space Models with Regime Switching . . . . . . . . . . . . . . . . . . . . 194
12.2.1 Switching Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
12.2.2 Example: Temperature Data with Regime Switch . . . . . . . . . . . . . 197
12.3 Linear Model Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
12.3.1 Generic Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
12.3.2 Component Assembly . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
12.3.3 Modeling Dependencies Between Observations . . . . . . . . . . . . . . 205
12.4 Anomaly Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
13 Model Calibration 213
13.1 Least-Squares Model Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . 215
13.1.1 Illustrative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
13.1.2 Limitations of Deterministic Model Calibration . . . . . . . . . . . . . . 218
13.2 Hierarchical Bayesian Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . 218
13.2.1 Joint Posterior Formulation . . . . . . . . . . . . . . . . . . . . . . . . . 218probabilistic machine learning for civil engineers ix
13.2.2 Predicting at Unobserved Locations . . . . . . . . . . . . . . . . . . . . 223
V Reinforcement Learning 227
14 Decisions in Uncertain Contexts 229
14.1 Introductory Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
14.2 Utility Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
14.2.1 Nomenclature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
14.2.2 Rational Decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
14.2.3 Axioms of Utility Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 231
14.3 Utility Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
14.4 Value of Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
14.4.1 Value of Perfect Information . . . . . . . . . . . . . . . . . . . . . . . . 236
14.4.2 Value of Imperfect Information . . . . . . . . . . . . . . . . . . . . . . . 237
15 Sequential Decisions 241
15.1 Markov Decision Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
15.1.1 Utility for an Infinite Planning Horizon . . . . . . . . . . . . . . . . . . 245
15.1.2 Value Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
15.1.3 Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
15.1.4 Partially Observab le Markov Dec ision Process . . . . . . . . . . . . . . . 251
15.2 Model-Free Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 252
15.2.1 Temporal Di↵erence Learning . . . . . . . . . . . . . . . . . . . . . . . . 252
15.2.2 Temporal Di↵erence Q-Learning . . . . . . . . . . . . . . . . . . . . . . 254
Bibliography 259
Index 267Nomenclature & Abbreviations
General Mathematical Symbols
R Real domain ⌘ (1, 1)
R
+ Real positive domain ⌘ (0, 1)
Z Real intege r domain ⌘ {· · · , 1, 0, 1, 2, · · · }
(0, 1) Continuous close interval between 0 and 1,
which include s 0 and 1
(0, 1] Continuous op e n interval between 0 and 1,
which include s 0 and not 1
limn!1
A limit when n tends to infinity
8 For all
: Such that
xˇ The true value for x
xˆ An approximat ion of x
P Sum operation ¬ The negation symbol
Q
Product operation
R dx Integral operat ion with re spe ct to x
dv(x)
dx ⌘ rxv Derivative or gradient of v(x) with respect to x
@v(x,y,z)
@x Partial de rivative of v(x, y, z) with respect to x
|x| Absolute values of x
⇡ Approximat ely equ al
/ Proportional to
⌘ Equivalent
ln(x) ⌘ loge(x) Natural logarithm of x
ln(exp(x)) = x
exp(x) ⌘ e
x Exponential function of x, = 2.71828x
, exp(ln(x)) = x
x An infinitesimal interval for x
A , B A implies B and B implies Aj.-a. goulet xxiv
Linear Algebra
x A scalar variable
x A column vec tor, x = [x1 x2 · · · xX
]|
X A matrix
xi ⌘ [x]i i
th element of a vector
xij ⌘ [X]ij {i, j}
th element of a matrix
X = diag(x) Square matrix X where the terms on the main diagonal are the
elements of x and 0 elsewhere
x = diag(X) Vector x consisting in the main diagon al terms of a mat rix X
I The identity matrix, i.e., a square matrix with 1 on the main
diagonal and 0 elsewhere
blkdiag(A, B) Block diagonal matrix where matrices A and B are concate￾nated on the main diagonal of a single matrix
| Transposition operator : [X]ij = [X|
] ji
· Scalar product
⇥ Matrix multiplication
 Hadamar (element-wise) product
||x||p L
p
-norm of a vector x
det(A) ⌘ |A| Determinant of a Matrix A
tr(A) Sum of the elements on the main diagonal of A
! A transformation from a space to another
Jy,x The Jacobian matrix so that [Jy,x]k,l =
@yk @xl @g(x)
@xi Partial de rivative of g(x) with respect to the i
th variable xi
rg(x) A gradient vector, =
h
@g(x)
@x1
· · · @g(x)
@xn i
H The Hessian matrix containing second-order partial derivatives,
[H[g(x)]]ij =
@
2g(x)
@xi@xj
Probability Theory and Random Variables
A = {E1, E2 , · · · } A set is described by a calligraphic letter
#A Number of eleme nts in a set A
S Universe/sampling space, i.e., the en se mble of possible re sults
x An elementary event, x 2 S
x 2 S x belongs to the sampling space S
E An ensemble of eleme ntary events
E ⇢ S E is a subset of S
E ✓ S E is a subset or is equal to S
E = S A certain event
E = ; An impossible event
E The complement of the e vent E
Pr(·) The probability of an event (2 (0, 1))probabilistic machine learning for civil engineers xxv
[ Union operation for events, i.e., “or”
\ Intersec tion operation for events, i.e., “and”
X A random variable
X A vector of random variables
f ( x) ⌘ fX (x) Probability density function of a random variable X
X ⇠ f (x) X is distributed as described by its marginal probability density
function f(x)
X ⇠ f (x) X is distributed as described by its joint probability density
function f(x)
d
! Converges in distribution
x, xi Realization of a random variab le x : X ⇠ f(x)
F(x) ⌘ FX (x) Cumulat ive distribution (or mass) function of a random variable
X
(x) Cumulative distribution function of a standard Normal random
variable with mean eq ual to 0 and variance equal to 1
p(x) ⌘ pX (x) Probability mass function of a random variable X
X ? Y The random variables X and Y are statistic ally independent
X ? Y |z The random variables X and Y are conditionally independent
given z
X|y The random variable X is condition ally dependent on y
E[X] Expectation operation for a random variable X
var[X] Variance operation for a random variable X
cov(X, Y ) Covariance operation for a pair of random variables X, Y
X Coefficient of variation of a random variable X
µX The mean of a random variable X

2
X The variance of a random variable X
⇢ The correlation coefficient
µX The mean values for a vector of rand om variables X
⌃X A covariance matrix for a vector of random variables X
RX A correlation matrix for a vector of random variables X
DX A standard deviation matrix for a vector of random variables X
N(x;µ, 
2
) The prob ability density function of a univariate Normal random
variable X, parameterized by its me an and variance
N (x; µ X, ⌃X) The joint probability density function of a multivariate Nor- mal random variable X, parameterized by its mean vect or and
covariance matrix
ln N (x;, ⇣ ) The prob ab ility density function of a log-normal random vari- able X, parameterized by its mean and standard deviation
defined in the log space
B(x;↵, ) The prob ab ility density function of a Beta random variable X, parameterized by ↵ and 
U (x; 0, 1) The uniform probability density function for a ran dom variable
X defined for the interval (0, 1)
(x) The dirac-delta functionj.-a. goulet xxvi
f(y|x ) ⌘ L(x|y) The likelihoo d, i.e., the prior probability density of observing
Y = y given x
Optimization
f˜(✓) Target function we want maximize ˜f
0(✓) =
df˜(✓)
d✓ First derivative of a function
f˜00(✓) =
d
2 ˜f(✓)
d✓
2 Second derivat ive of a func tion
arg max
✓ ˜f (✓) The values of ✓ that maximize ˜f(✓)
max
✓ f˜(✓) The maximal value of f˜(✓)
✓
⇤ An optimal value
d Search dire c tion
 Scale factor for the search direction
I(i) An indic at or vector for which all values are equal to 0, except
the i
th, which is equ al to on e
Sampling
✓ Vector of parameters
q (✓
0 |✓) Proposal distribution describing the probability of moving to ✓
0
from ✓
f˜(✓) Target distribution from which we want to draw samples
↵ Acceptance probability
 Acceptance rate
Rˆ Estimated potential scale reduction
Utility & Decisions
A = {a1, · · · , aA} A set of possible actions
x 2 X ✓ Z An outcome from a set of discrete states
L A lottery
U(a, x) Utility given a state x and an action a
U(a) ⌘ E[U(a, X)] Expected utility conditional on an action a
U(s) ⌘ U(s, ⇡) ⌘
E[U(s, ⇡)]
Long-term expect ed utility conditional on a current stat e s and
that a policy ⇡ is followed
⇡(s) A policy defining an action a to take, conditional on a state s
r(s, a, s
0) The reward for being in state s, taking the action a and ending
in state s
0 Q(s, a) Action-utility function
 Discount fact or
↵ The learning rateprobabilistic machine learning for civil engineers xxvii
✏ The probability that an agent takes a random action rather
than following the policy ⇡(s)
 An assignment in a recurrent equation, e.g., x 2x
Abbreviations
AI Artificial intelligence
BN Bayesian network
CDF Cumulat ive distribution func tion
CLT Central limit theorem
CMF Cumulat ive mass funct ion
CPT Conditional probability table
CV Cross-validation
DAG Directed acyclic graph
DBN Dynamic Bayesian network
e.g. For example
EM Expectation maximization
EPSR Estimated potential scale reduction
FCNN Fully connected feed forward neural network
GA Gradient ascent
GMM Gaussian mixture model
GP Gaussian process
GPC Gaussian process classification
GPR Gaussian process regression
GPU Graphical processing unit
HMM Hidden Markov model
i.e. That is
iid Independent identically distributed
KF Kalman filter
KS Kalman smoother
LDA Linear discriminant analysis
LLOCV Leave-one-out cross-validation
MAP Maximum a-poste riori
MCMC Markov chain M onte Carlo
MDP Markov de cision process
ML Machine learning
MLE Maximum likelihood estimate
NB Na¨ıve Bayes
NN Neural network
NR Newton-Raphson
PCA Principal component analysis
PDF Probability density functionj.-a. goulet xxviii
PMF Probability mass function
POMDP Partially obse rvable Markov decision process
PSD Positive semi-de finite
QDA Quadratic discriminant analysis
RL Reinforcement learning
R.V. Random variable
SKF Switching Kalman filte r
SSM State-space model
sym. Symmetric
TD Temporal di↵erence
VOI Value of information
VPI Value of perfect information1
Introduction
Machine learning (ML) describes a family of methods that allows
learning from data what relationships exist between quantities of
inte re st. The goal of learning relationships between quantities of
Civil engineering examples Geotechnics: From a set of discrete observations of soil resistances mea- sured across space, we want to predict
the resistance for other unobserved
locations. Environment: For a lake, we want
to build a model linking the e↵ect of
temperature and the usage of fertilizers with the prevalence of cyanobacteria, fish mortality, and water color. Transportation: We want to predict
the demand for public transportation
services from multiple heterogeneous data sources such as surveys and transit card usage. Structures: From observations made on the displacement of a structure over
time, we want to detect the presence of anomalies in its behavior.
inte re st is to gain information about how a system works and to
make predictions for unobserved quantitie s. This new knowledge
can the n be employed to support decision making.
Learning from data is not new in the field of engineering. As
depicted in figure 1.1a, without machine learning, the task of
learning is typically the role of the engineer, who has to figure out
the relationships between the variables in a data set, then build
a hard-coded model of the syste m, and then return predictions
by evaluating this model with the help of a computer. The issue
with hard-coding rules and equations is that it becomes increasingly
difficult as the number of factors considered increases. Figure
1.1b depicts how, with machine learning, the role of the human
is now shifted from learning to programming the computer so
Data Human Computer Predictions
Information
Inputs Learning Outputs
(a) Without machine learning
Data
Human Computer Predictions
Information
Inputs Learning Outputs
(b) With machine learning
Figure 1.1: Comparison of how, in the
context of engineering, relationships are
learned from data with and without using machine learning.j.-a. goulet 2
it can learn. The question is, How do we program a computer
to learn? The quick answer is this: by defining a generic model
that can be adapted to a wide array of problems by changing its
parameters and variables. For most machine learning methods,
learning consists in inferring these parameters and variables from Note: For a generic model y = g(x;✓), x
and y are the input and output variables of interest that are evaluated by the model. ✓ is a parameter characterizing the model
behavior.
data. Notwithstanding the specificities for reinforcement learning
that we will cover later, the big picture is the following: learning
about parameters and variables is done by quantifying, for a given
set of their values, how good the model is at predicting observed
data. This task typically leads to nonunique solutions, so we have
to make a choice: either pick a single set of values (that is, those
leading to the best prediction s) or consider all possible values
that are compatible with data. In figure 1.1b, the dashed arrow
represents the practical reality where ML typically relies on a
human decision for the selection of a particular mathematical
method adapted to a given data set and problem.
One term often employed together with machine learning is ar￾tificial intelligence (AI). ML and AI are closely interconnected, yet
they are not synonyms. In its most general form, artificial intel￾ligence consists in the reproduction of an intelligent behavior by
a machine. AI typically involves a system that can perceive and
inte rac t dynamically with its environment through the process
of making rational decisions. Note that such an AI system does
not have to take a physical form; in most c ases it is actually only
a computer program. In AI systems, decisions are typically not
hard-coded or learned by imitat ion ; instead, the AI system chooses
actions with the goal of maximizing an objective function that
is given to it. Behind such an AI system that interacts with its
environ ment are machine learning methods that allow extracting
information from observations, predict future system responses, and choose the optimal action to take. As depicted in the Venn dia￾gram in figure 1.2, machine learning is part of the field of artificial
intelligen ce. ML
AI
Figure 1.2: The field of artificial intelli- gence (AI) includes the field of machine
learning (ML).
Now, going back to machine learning, why opt for a probabilistic
approach to it? It is because the task of learning is intrinsically
uncertain. Uncertainties arise from the imperfect models employed
to learn from data, because data itself often involves imperfect
observations. Therefore, probabilities are at the core of machine
learning methods for representing the uncertainty associated with
our lack of knowledge. If we do not want to use machine learning as
a black box, but instead to understand how it works, going through
the probabilistic path is essential. In machine learning, there are three main subfie lds: supervisedprobabilistic machine learning for civil engineers 3
learning, unsupervised learning, and reinforcement learning. Su- pervised learning applies in the context where we want to build a
model describing the relationships between the characteristics of a
system defined by covariates and observed system responses that Note: We employ the generic term system
to refer to either a single object or many
interconnected objects that we want to
study. We use the term covariates for variables describing the characteristics or
the properties of a system.
are typically either continuous values or categories. With unsuper￾vised learning, the objective is to discover structures, patterns, sub- groups, or even anomalies without knowing what the right answer is
because the target outputs are not observed. The third subfield is
reinforcement learning, which involves more abstract concept s than
supervised and unsupervised learning. Reinforcement learning deals
with sequential decision problems where the goal is to learn the
optimal action to choose, given the knowledge that a system is in
a particular state. Take the example of infrastructure maintenance, where, given the state of a structure today, we must choose between
performing maintenance or doing nothing. The key is that there is
no data to train on with respect to the decision -making behavior
that the computer should reproduce. With reinforcement learning,
the goal is to identify a policy describing the optimal act ion to
perform for each possible state of a system in order to maximize
the long-term accumulation of rewards. Note that the classific ation
of machine learning methods within supervised, unsupervised, and
reinforcement learning has limitations. For many met hods, the
frontie rs are blurre d because there is an overlap between more than
one ML subfield with respect to the mathematical formulations
employed as well as the applications. This book is intended to help making machine learning concepts
accessible to civil engineers who do not have a specialized back￾ground in statistics or in computer science. The goal is to dissect
and simplify, through a step-by-step review, a selection of key ma￾chine learning concepts and methods. At the end, the reader should
have acquired sufficient knowledge to understand dedicated ma￾chine learning literature from which this book borrows and thus
expand on advanced methods that are beyond the scope of this
introductory work. The diagram in figure 1.3 depicts the organization of this book,
where arrows represent the depen de ncies between di↵erent chapters. Colored regions indicate to which machine learning subfield each
chapte r belongs. Before introducing the fundamentals associated
with each machine learning subfield in Parts II–V, Part I covers
the background knowle dge required to understand machine learn￾ing. This background knowledge includes linear algebra (chapter
2), where we review how to harness the potential of matrices to
describe systems; probability theory (chapter 3) and probabilityj.-a. goulet 4
2
3
4
5
6
7
8
9
11
12
10
14
$
15
13
Background
Bayesian
Est
mi
at oi n
S pu e vr si L de gni nrae
Un
us
pervised
Learn
ni g
Reinforcement Learning
Figure 1.3: Each square node describes a
chapter, and arrows represent the depen- dencies between chapters. Shaded regions group chapters into the five parts of the book. Note that at the beginning of each
part, this diagram is broken down into
subparts in order to better visualize the dependencies between the current and
previous chapters.
distributions (chap ter 4) to describe our incomplete knowledge; and
convex optimization (chapter 5) as a first method for allowing a
computer to learn about model parameters. Part II first covers Bayesian estimation (chapter 6), which is
behind the formulation of supervised and unsupervised learning
problems. Second, it covers Markov chain Monte Carlo (MCMC)
methods (chapter 7), allowing one to perform Bayesian estimation
in c omp le x cases for which no analytical solution is available.
Part III explores methods and concepts associated with super- vised learning. Chapter 8 covers regression methods, where the goal
is to build models describing continuous-valued system responses as
a function of covariates. Chapter 9 presents classification methods, which are analogous to regression except that the system responses
are cat e gorie s rather than continuous values. Part IV introduces the notions associated with unsupervised
learning, where the task is to build models that can extract the
underlying struc ture present in data without having access to
direct observations of what this underlying structure should be.
In chapter 10, we first approach unsupervised learning through
clustering and dimension reduction. For clustering, the task is to
identify subgroup s within a set of observed covariates for whichprobabilistic machine learning for civil engineers 5
we do not have access to the subgroup labels. The role of dimen￾sion reduction is, as its name implies, to reduce the number of
dimensions required to represent data while minimizing the loss
of information. Chapter 11 presents Bayesian networks, which
are graph-based probabilistic methods for modeling dependencies
within and between systems through their joint probability. Chapter
12 presents state-space models, which allow creating probabilistic
models for time-dependent systems using sequences of observations. Finally, chapter 13 presents how we can employ the concepts of
probabilistic infe re nc e for the purpose of model calibration. Model
calibration refe rs to the task of using observations to improve our
knowle dge associated with hard-coded mathematical models that
are commonly employed in engineering to describe systems. This
application is classified unde r the umbrella of unsupervised learning
because, as we will see, the main task consists in inferring hidden￾state variables and parameters, for which observations are not
available.
Part V presents the fundamental notions necessary to define
reinforcement learning problems. First, chapter 14 presents how
rational decision s are made in uncertain contexts using the utility
theory. Chapter 15 presents how to e xte nd rational decision making
to a sequential context using the Markov decision process (MDP). Finally, building on the MDP theory, we introduce the fundamental
concepts of reinforcement learning, where a virtual agent learns how
to take optimal decisions through trial and e rror while interacting
with its environment.2
Linear Algebra
Linear algebra is employed in a majority of machine learning meth￾ods and algorithms. Before going further, it is essential to under- stand the mathematical notation and basic operations.
2.1 Notation
We employ lowercase letters x, s, v, · · · in order to describe variables
x : scalar variable x : column vector X : matrix xi ⌘ [x]i
: i
th element of a vector xij ⌘ [X]ij : {i, j}
th element of a matrix
that can lie in specific domains such as real numbers R, real posi- tive R
+, integers Z, closed intervals [·, ·], open intervals (·, ·), and so
on. Often, the problems studied involve multiple variables that can Examples of variables x belonging to di↵erent domains x 2 R ⌘ (1,1)
2 R
+ ⌘ (0,1)
2 Z ⌘ {· · · , 1, 0, 1, 2, · · · }
be regrouped in arrays. A 1-D array or vector containing sc alars is
represented as
x =
2
6
6
6
4
x1
x2
.
.
.xn
3
7
7
7
5
.
By convention, a vector x implicitly refers to a n ⇥ 1 column vector. For example, if each element xi ⌘ [x]i
is a real number [x]i 2 R for
all i from 1 to n, then the vector belongs to the n-dimensional real
domain Rn. This last state me nt can be expressed mathematically as
[x]i 2 R, 8i 2 {1 : n} ! x 2 R
n
. In machine learning, it is common
to have 2-D arrays or matrices, X =
2
6
6
4
x11 x12 · · · x1n x21 x22 · · · x2n
.
.
.
.
.
.
.
.
.
.
.
. xm1 xm2 · · · xmn
3
7
7
5
,
where, for example, if each xij ⌘ [X] ij 2 R, 8i 2 {1 : m}, j 2 {1 :
n} ! X 2 R
m⇥n. Arrays beyond two dimensions are referred to as
tensors. Although tensors are widely employed in the field of neural
networks, they will not be treated in this book.j.-a. goulet 10
There are several matrices with specific properties: A diagonal
matrix is square and has only terms on its main diagonal, Y = diag(x) =
2
6
6
4
x1 0 · · · 0
0 x2 · · · 0
.
.
.
.
.
.
.
.
.
.
.
. 0 0 · · · xn
3
7
7
5
n⇥n
.
An identity matrix I is similar to a diagonal matrix except that
elements on the main diagonal are 1, and 0 everywhere else,
I =
2
6
6
4
1 0 · · · 0
0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
. 0 0 · · · 1
3
7
7
5
n⇥n
.
A block diagonal matrix concatenates several matrices on the main
A =

1 2
3 4

, B =
2
4
4 5 6
7 8 9
10 11 12
3
5 .
blkdiag(A, B) =
2
6
6
6
4
1 2 0 0 0
3 4 0 0 0
0 0 4 5 6
0 0 7 8 9
0 0 10 11 12
3
7
7
7
5
diagonal of a single matrix, blkdiag(A, B) =
 A 0
0 B

. We can manipulate the dime nsions of matrices using the transpo- sition operation so that indices are permuted [X|]ij = [X]ji
. For
example,
X =
 x11 x12 x13
x21 x22 x23
 ! X| =
2
4
x11 x21
x12 x22
x13 x23
3
5. The trace of a square matrix X corresponds to the sum of the
elements on its main diagonal,
tr(X) = Xn
i=1
xii .
2.2 Operations
2 1 0 1 2 5
0
5
10
x
y
y = 3x + 1 = ax + b a = 3, b = 1
Figure 2.1: 1-D plot representing a linear system, y = ax + b.
2
0
2 2
0
2
0
10
x1 x2
y
y = x1 + 2x2 + 3 = a
|x + b a =

1
2

, x =
x1 x2

, b = 3
Figure 2.2: 2-D plot representing a linear system, y = a
| x + b.
In the context of machine learning, linear algebra is employed
because of its capacity to model linear systems of equations in a
format that is compact and well suite d for computer calculations. In a 1-D case, such as the one represented in figure 2.1, the x space
is mapped into the y space, R ! R, through a linear (i.e., affine)
function. Figure 2.2 presents an ex amp le of a 2-D linear function
where the x space is mapped into the y space, R
2 ! R. This can
be generalized to linear systems y = Ax + b, defining a mapping
so that Rn ! Rm, where x and y are respectively n ⇥ 1 and m ⇥ 1probabilistic machine learning for civil engineers 11
vect ors. The product of the matrix A with the vector x is defined
as [Ax] i = P
j
[A] ij · [x]j. In more general cases, linear algebra is employed to multiply a
matrix A of size n ⇥ k with another matrix B of size k ⇥ m, so the
result is a n ⇥ m matrix, C = AB
= A ⇥ B. The matrix multiplication operation follows [C]ij = P
k
[A]ik ·[B]kj, as illustrated in figure 2.3. Following the requirement on the size of
the matrices multiplied, this operation is not generally commutative
so that AB 6= BA. Matrix multiplication follows several properties
such as the following:
Distributivity A(B + C) = AB + AC
Associativity A(BC) = (AB)C
Conjugate transposability (AB)| = B|A|.
Figure 2.3: Example of matrix multiplica- tion operation C = AB.
When the matrix multiplication operator is applied to n ⇥ 1 vectors,
it reduces to the inner product, x
|y ⌘ x · y = [x1 · · · xn] ⇥
2
6
4
y1
.
.
. y n
3
7
5 = Pn
i=1 xiyi. Another common operation is the Hadamar product or element￾wise product, which is represented by the symbol . It consists in
multiplying each term from matrices Am⇥n and Bm⇥n in order to
obtain Cm⇥n, C = A  B
[C]ij = [A]ij
· [B]ij
. The element-wise product is seldom em ployed to defin e math em ati- cal equations; however, it is extensively employed when implement￾ing these equations in a computer language. Matrix addition is by
definition an element-wise operation that applies only to matrices of
same dimensions, C = A + B
[C]ij = [A]ij + [B]ij . One last key operation is the matrix inversion A1
. In order to
be invertible, a matrix must be square and must not have linearly
dependent rows or columns. The product of a matrix with its in￾Linearly dependent vectors Vectors x1 2 R n and x2 2 Rn are linearly
dependent if a nonzero vector y 2 R2 exists, such that y1x1 + y2x2 = 0.j.-a. goulet 12
verse is e qual to the identity matrix A1A = I. Matrix inversion is
particularly useful for solving linear systems of equations, Ax = b, A1 Ax = A1b, Ix = A1b, x = A1b.
||x||2 = q
x
2
1 + x
2
2
||x||1 = |x1| + |x2 |
||x||1 = max |x1, x2 | = |x1|
Figure 2.4: Examples of applications of
di↵erent norms for computing the length of a vector x. 2.3 Norms
Norms measure how large a vector is. In a generic way, the Lp
-norm
is defined as
||x||p = X
i
|[x] i|p!1/p
. Special cases of inte re st are
||x||2 = sX
i
[x]2
i ⌘ p
x
|x (Euclidian norm)
||x||1 = X
i
|[x] i| (Manhattan norm)
||x||1 = max
i
|[x]i | . (Max norm)
These cases are illustrated in figure 2.4. Among all cases, the L2
- norm (Euclidian distance) is the most common. For example,
§8.1.1 presents for the context of linear regression how choosing a
Euclidian norm to measure the distance between observations and
model predictions allows solving the parameter estimation problem
analytically.
2.4 Transformations
Machine learning involves transformations from one space to an￾other. In the context of linear algebra, we are interested in the
special case of linear transformations.
2.4.1 Linear Transformations
3 2 1 0 1 2 3 3 2 1
0
1
2
3
x
01
x
02 (a) A =
⇥ 1 0
0 1
⇤
, det(A) = 1
3 2 1 0 1 2 3 3 2 1
0
1
2
3
x
01
x
02 (b) A =
⇥ 1 0
0 2
⇤
, det(A) = 2
3 2 1 0 1 2 3 3 2 1
0
1
2
3
x
01
x
02
(c) A =
⇥ 1.5 0
0 1
⇤
, det(A) = 1.5
Figure 2.5: Examples of linear transforma- tions x
0 = Ax.
Figure 2.1 presented an example for a R ! R linear transforma- tion. More generally, a n ⇥ n square matrix can be employed to
perform a R
n ! R
n
linear transformation through multiplication. Figures 2.5a–c illustrate how a matrix A transforms a space x into
another x
0 using the matrix product operation x
0 = Ax. The de￾formation of the circle and the underlying grid (see (a) ) show the
e↵ect of various transformations. Note that the terms on the mainprobabilistic machine learning for civil engineers 13
diagonal of A control the transformations along the x
01 and x
02 axes,
and the nondiagonal terms control the tran sformation dependency
between both axes, (see, for example, figu re 2.6).
The determinant of a square mat rix A measures how much the
transformation contracts or expands the space: • det(A) = 1: preserves the space/volume • det (A ) = 0: collapses the spac e/volume along a subset of dimen￾sions, for example, 2-D space ! 1-D space (see figu re 2.7)
In the examples presented in figure 2.5a–c, the determinant quan￾tifies how much the area/volume is changed in the transformed
space; for the circle, it corresponds to the change of area caused by
the transformation. As shown in figure 2.5a, if A = I, the transfor- mation has no e↵ect so det(A) = 1. For a square matrix [A]n⇥n, det(A) : Rn⇥n ! R.
2.4.2 Eigen Decomposition
Linear transformations operate on several dimensions, such as in
the case presented in figure 2.6 where the transformation introduces
dependency between variables. Eigen decomposition enables finding
a linear transformation that removes the dependency while preserv￾ing the area/volume. A square matrix [A]n⇥n can be decomposed
in eigenvectors {⌫1 , · · · ,⌫n} and eigenvalues {1, · · · , n }. In its
matrix form, A = Vdiag()V1
, where
V = [⌫1 · · · ⌫n]
 = [1 · · · n]|. Figure 2.6 presents the eigen decomposition of the transformation
x
0 = Ax. Eigenvectors ⌫1 and ⌫2 describe the new referential into
which the transformation is indepe ndently applied to each axis. Eigenvalues 1 and 2 describe the transformation magnitude along
each eigenvector.
3 2 1 0 1 2 3 3 2 1
0
1
2
3
⌫2
⌫1 2⌫2
1 ⌫1
x
01
x
02
x
0 = Ax A =

1 0.5
0.5 1

V = [⌫1 ⌫2 ] =
 0.71 0.71
0.71 0.71

 = [0.5 1.5]| Figure 2.6: Example of eigen decomposi- tion, A = Vdiag()V1.
3 2 1 0 1 2 3 3 2 1
0
1
2
3
x
01
x
02 A =
⇥ 1 0.99
0.99 1
⇤
, det(A) = 0.02
Figure 2.7: Example of a nearly singular
transformation.
A matrix is positive definite if all eigenvalues > 0, and a matrix
is positive semidefinite (PSD) if all eigenvalues  0. The deter- minant of a matrix corresponds to the product of its eigenvalues. Therefore, in the case where one eigenvalue equals zero, it indicates
that two or more dimensions are linearly dependent and have col￾lapsed into a single one. The transformation matrix is then said
to be singular . Figure 2.7 presents an examp le of a nearly singular
transformation. For a positive semidefinite mat rix A and for anyj.-a. goulet 14
vector x, the following relation holds:
x
|Ax  0. This property is employed in §3.3.5 to define the requirements for
an admissible covariance matrix. A more exhaustive review of linear algeb ra can be found in
dedicated textbooks such as the one by Kreyszig.1
1 Kreyszig, E. (2011). Advanced engineering mathematics (10th ed.). Wiley3
Probability Theory
“La t´ehorie des probabilit´es n’est, au fond, que le bon sens r´eduit au calcul; elle fait appr´ecier avec exactitude ce que les esprits
justes sentent par une sorte d’instinct” — Pierre-Simon, marquis de Laplace (1749–1827)
The interpretati on of probability theory employed in this book
follows Laplace’s view of “common sense reduced to calculus.” It
means that probabilities describe our state of knowledge rather
than intrinsically aleatory phenomena. In practice, few phenomena
are actually intrinsically unpredictable. Take, for example, a coin as
displayed in figure 3.1. Whether a coin toss results in eithe r head s
or tails has nothing to do with an inherently aleatory process. The
outcome appears unpredictable because of the lack of knowledge
about the coin’s initial position, speed, and accele rat ion. If we could
gather information about the coin’s initial kinematic conditions, the
outcome would become predictable. Devices that can throw coins
with repeatable initial kinematic conditions will lead to repeatable
outcomes.
Figure 3.1: A coin toss illustrates the
concept of epistemic uncertainty. (Photo: Michel Goulet)
Figure 3.2 presents another example where we consider the elas￾tic modulus 1 E at one specific location in a dam. Notwithstanding
1Elastic modulus relates the stress  and
strains ✏ in Hooke’s law  = ✏E.
long-term e↵ects such as creep,2 at any given location, E does
2Creep is the long-term (i.e., years)
deformation that occurs under constant stress.
not vary with time: E is a de terministic, yet unknown constant. Probability is employed here as a tool to desc ribe our incomplete
knowledge of that constant.
E?
Figure 3.2: The concrete elastic modulus E at a given location is an example of
deterministic, yet unknown quantity. The possible values for E can be described
using the probability theory.
There are two types of uncertainty: aleatory and epistemic. aleatory uncertainty is characterized by its irreducibility; no in￾formation can either reduce or alter it. Alternately, epistemic
uncertainty refers to a lack of knowledge that can be altered by new
information. In an engineering context, aleatory uncertainties arise
when we are concerned with future realizat ions that have yet to
occur. Epistemic uncertainty applies to any other case dealing with
deterministic, yet unknown quantities. This book approaches machine learning using probability theory
because in many practical engineering problems, the number of
observations available is limited, from a few to a few thousand. In
such a context, the amount of information available is typicallyj.-a. goulet 18
insufficient to eliminate epistemic uncertainties. When large data
sets are available, probabilistic and deterministic met hods may lead
to indistinguishable results; the opposite occurs whe n little dat a
is available. Therefore, the less we know about it, the stronger the
argument for approaching a problem using probability theory. In this chapter, a review of set theory lays the foundation for
probability theory, where the central part is the concept of random
variables. Machine learning methods are built from an ensemble
of funct ion s organize d in a clever way. Therefore, the last part of
this chapter looks at what happens when random variables are
introduced into deterministic functions. For specific notions related to prob ability theory that are outside
the scope of this chapter, the reader should refer to dedicated
textbooks such as those by Box and Tiao;3 Ang and Tang.4
3Box, G. E. P. and G. C. Tiao (1992). Bayesian inference in statistical analysis. Wiley
4Ang, A. H.-S. and W. H. Tang (1975).
Probability concepts in engineering plan- ning and decision, Volume 1—Basic Principles. John Wiley
3.1 Set Theory
Set: Ensemble of events or elements. Universe/sampling space (S): Ensem- ble of all possible events. Elementary event (x): A single event, x 2 S. Event (E): Ensemble of elementary events. E ⇢ S : Subset of S
E = S : Certain event
E = ; : Impossible event
E : Complement of E
A set describes an ensemble of elements, also referred to as events. An elemen tary event x refers to a single event among a sampling
space (or universe) denoted by the calligraphic lett er S. By defini- tion, a sampling space contains all the possible events, E ✓ S. The
special case where an event is equ al to the sampling space , E = S,
is called a certain event. The opposite, E = ;, whe re an event is
an e mpty set, is called a null event. E refers to the complement of
a set , that is, all eleme nts belonging to S and not to E. Figu re 3.3
illustrates these concepts using a Venn diagram.
Figure 3.3: Venn diagram representing
the sampling space S, an event E, its complement E, and an elementary event x.
(a) Union operation
(b) Intersection operation Figure 3.4: Venn diagrams representing the
two basic operations.
Let us consider the example,5 of the stat e of a structu re follow￾5This example is adapted from Armen Der Kiureghian’s course, CE229, at University
of California, Berkeley.
ing an earthquake, which is described by a sampling space,
S = {no damage, light damage , important damage , collapse} = {N, L, I, C}.
In that context, an event E1 = {N, L} could contain the no damage
and light damage events, and another event E2 = {C} could contain
only the collapsed state. The complements of these events are,
respectively, E1 = {I, C} and E2 = {N, L, I}. The two main operations for events, union and intersection, are
illustrated in figure 3.4. A union is analogous to the “or” operator, where E1 [ E2 holds if the event belongs to e ithe r E1, E2, or
both. The intersection is analogous to the “and” operator, where
E1 \ E 2 ⌘ E1E2 holds if the event belongs to both E1 and E2. As
a convention, intersection has priority over union. Moreover, both
operations are commutative, associative, and distributive. Given a set of n events {E1 , E2, · · · , En} 2 S, E1, E2, · · · , En,probabilistic machine learning for civil engineers 19
the events are mutually exclusive if EiEj = ;, 8i 6= j, that is,
if the intersection for any pair of events is an empty set. Events
E1, E2 , · · · , En are collectively exhaustive if [
n
i=1Ei = S, that is, the
union of all events is the sampling space. Events E1, E2 , · · · , En are
mutually exclusive and collectively exhaustive if they satisfy both
properties simultaneously. Figure 3.5 presents examples of mutu- ally exclusive (3.5a), collectively exhaustive (3.5b), and mutually
exclusive and collectively exhaustive (3.5c–d) events. Note that
the di↵erence between (b) and (c) is the absence of overlap in the
latter.
Union (“or”)
E1 [ E2 Intersection (“and”)
E1 \ E2 ⌘ E1E2 Commutativity
E1 [ E2 = E2 [ E1, E1E2 = E2E1 [
n
i=1 Ei = E1 [ E2 [ · · · [ En \n
i=1 Ei = E1 \ E2 \ · · · \ En Associativity
(E1[E2)[E3 = E1[(E2 [E3) = E1[E2[E3 Distributivity
E1(E2 [ E3) = (E1E2 [ E1E3)
(a) Mutually exclusive
(b) Collectively exhaustive
(c) Mutually exclusive and collec- tively exhaustive
(d) Mutually exclusive and collec- tively exhaustive Figure 3.5: Venn diagrams representing
the concepts of mutual exclusivity and
collective exhaustivity for events.
3.2 Probability of Events
Pr(Ei) denotes the probability of the event Ei
. The re are two main
interpretati ons for a probability: the Frequentist and the Bayesian. Frequentists interpret a probability as the number of occurrences of
Ei relative to the number of samples s, as s goes to 1, Pr(Ei) = lims!1#{Ei}
s
. For Bayesians, a probability measures how likely is Ei in compar￾ison with other events in S. This interpretation assumes that the
nature of uncertainty is episte mic, that is, it describes our knowl- edge of a phenom en on. For instance, the probability depends on
the available knowledge and can change when new information
is obtained. Throughout this book we are adopting this Bayesian
interpretation. By definition, the probability of an event is a number between
zero and one, 0  Pr(Ei )  1. At the ends of this spectrum, the
probability of any event in S is one, Pr(S ) = 1, and the probability
of an emp ty set is zero, Pr(;) = 0. If two events E1 and E2 are
mutually exclusive, then the probability of the events’ union is the
sum of e ach event’s probability. Because the union of an event and
its complement are the sampling space, E [ E = S (see figure 3.5d),
and because Pr(S) = 1, then the probability of the complement is
Pr(E) = 1  Pr(E). When events are not mutually exclusive, the gene ral addition
rule for the probability of the union of two events is
Pr(E1 [ E2 ) = Pr(E1) + Pr(E2 )  Pr(E1E2). This general addition rule is illustrated in figure 3.6, where if we
simply add the probability of each event without accounting for the
subtraction of Pr(E1 E2), the probability of the intersection of both
events will be counted twice.j.-a. goulet 20
Figure 3.6: Venn diagram representing the addition rule for the probability of events.
Pr(E1|E2) denotes the probability of the event E1 conditional
on the realizat ion of the event E2 . This cond itional probab ility
is defined as the joint probability for both events divided by the
probability of E2, Note: Pr(E2 ) 6= 0 on the denominator because a division by 0 is not finite. Pr(E1|E2) =
Pr(E1E2)
Pr(E2 )
, Pr(E2) 6= 0. (3.1)
Conditional probability
Marginal probability
Joint probability
Note: Statistical independence (? ) be- tween a pair of random variables implies
that learning about one random variable does not modify our knowledge for the other.
The probability of a single event is referred to as a marginal prob￾ability. A joint probability designates the probability of the inter- section of events. The terms in equation 3.1 can be rearranged to
explicitly show that the joint probability of two events {E1, E2 } is
the product of a conditional probab ility and its associated marginal, Pr(E1E2 ) = Pr(E1 |E2 ) · Pr(E2) = Pr(E2 |E1 ) · Pr(E1).
In c ase s where E1 and E2 are statistically independent, E1 ? E2, conditional probabilities are equal to the marginal, E1 ? E2 ⇢
Pr(E1|E2) = Pr(E1 )
Pr(E2|E1) = Pr(E2 ).
In the special case of statistically independent events, the joint
probability reduces to the product of the marginals, Pr(E1E2 ) = Pr(E1 ) · Pr(E2). The joint prob ability for n events can be broken down into n  1
conditionals and one marginal probability using the chain rule, Pr(E1E2 · · · En) = Pr(E 1|E2 · · · En)Pr(E2 · · · En ) = Pr(E1|E2 · · · En)Pr(E2|E3 · · · En)Pr(E3 · · · En) = Pr(E1|E2 · · · En)Pr(E2|E3 · · · En)· · ·Pr(En1|En)Pr(En). Let us define {E1, E2, E3, · · · , En } 2 S, a set of mutually e xc lu- sive an d colle ct ively ex haustive events, that is, EiE j = ;, 8i 6=
j, [n
i=1Ei = S – and an event A belonging to the same samplingprobabilistic machine learning for civil engineers 21
space, that is, A 2 S. This context is illustrat ed using a Venn dia￾gram in figure 3.7. The probability of the event A can be obtained
by summing the joint probability of A and each event Ei
, Pr(A) =Xn
i=1
Pr(A|Ei) · Pr(Ei ) | {z } Pr(AEi )
. (3.2)
Figure 3.7: Venn diagram representing the
conditional occurrence of events. This operation of obtaining a marginal probability from a joint is
called marginalization. The addition rule for the union of E1 [ E2
conditional on A is
Pr(E1[ E 2|A) = Pr(E1 |A) + Pr(E2 |A)  Pr(E1E2|A), and the intersection rule is
Pr(E1E2 |A) = Pr(E1|E2, A) · Pr(E2 |A). Using the definition of a conditional probability in equation 3.1, we
can break Pr(AE i) into two di↵erent products of a conditional and
its associated marginal probability, Pr(AEi) = Pr(A|Ei) · Pr(Ei) = Pr(Ei|A) · Pr(A) | {z } Pr(Ei|A) · Pr(A) = Pr(A|Ei) · Pr(Ei).
(3.3)
Reorganizing the right-hand terms of equation 3.3 leads to Bayes
rule,
Pr(Ei
|A) =
Pr(A|Ei) · Pr(Ei )
Pr(A)
.
Posterior probability
Evidence
Conditional probability
Prior probability
On the left-hand side is the posterior probability: the probability
of the event Ei given the realization of the event A. On the nu- merator of the right-hand side is the product of the conditional
probability of the event A given the event Ei, times the prior prob￾ability of Ei
. The te rm on the denom inator is referred to as the
evidence and act s as a normalization cons tant, which ensures that
P
i Pr(Ei
|A) = 1. The normalization const ant Pr(A) is obtained
using the marginalization operation presented in equation 3.2.
In pract ical applicat ions, Pr(A) is typically difficult to estimat e. Chapters 6 and 7 pre se nt analytic as well as numerical methods
for tackling this challenge. Figure 3.8 illustrat es the condition al
occurrence of events in the context of Bayes rule.
Pr(E1|A) =
Pr(A|E1) Pr(E1)
Pr(A) =
Pr(AE1)
Pr(A) =
Pr( )
Pr( )
Figure 3.8: Venn diagram representing the
conditional occurrence of events in the
context of Bayes rule.j.-a. goulet 22
3.3 Random Variables
Set theory is relevant for introducing the concepts related to proba￾bilities. However, on its own, it has a limited applicability to practi- cal problems that require defining the concept of random variables. A ran dom variable is denoted by a capital letter X. Contrarily to
what its name implies, a random variable is not intended to de￾scribe only intrinsically random events; in our case, it describes lack
of knowledge. A random variable X does not take any specific value.
Instead, it takes any value in its valid sampling space x 2 S and,
as we will see shortly, the probability of occurrence of each value
is typically not equal. Values of x are either called realizations or
outcomes and are elementary events that are mutually exclusive and
collectively exhaustive. A sampling space S for a rand om variable
can either be discrete or continuous. Continuous cases are always
infinite, whereas discrete ones can either be finite or infinite. Figure
3.9 illustrates how the concepts of events and sampling space can be
transposed from a Venn diagram representation to the domain of a
random variable .
3.3.1 Discrete Random Variables Figure 3.9: Parallel between a Venn
diagram and a continuous domain to
represent a random variable. In the case where S is a disc ret e domain, the prob ability that
X = x is described by a probability mass function (PMF). In terms
of not at ion Pr(X = x)⌘ pX (x) ⌘ p(x) are all equ ivalent. Moreover, we typically describe a random variable by defining its sampling
space and its probability mass function so that x : X ⇠ pX(x ). The
symbol ⇠ reads as distributed like. Analogously to the probability of NotationX : Random variable x : Realization of X
pX(x) : Probability that X = x
events, the probability that X = x must be
0  pX (x)  1, and the sum of the probability for all x 2 S follows
X
x pX(x) = 1. For the post-earthquake structural safety example introduced in
§3.1, where
S = 8
><
>:
no damage (N)
light damage (L )
important damage (I) collapse (C)
9
>=
>;
,
the sampling space along with the probability of each event can
be represented by a probability mass function as depicted in fig￾ure 3.10.
Figure 3.10: Representation of a sampling
space for a discrete random variable.probabilistic machine learning for civil engineers 23
The event corresponding to damages that are either light or impor- tant corresponds to L [ I ⌘ {1  x 2}. Bec ause the events x = 1
and x = 2 are mutually exclusive, the probability
Pr(L [ I) = Pr({1  X  2}) = pX(x = 1) + pX(x = 2). The probability that X takes a value less than or equal to x is
described by a cumulative mass function (CMF), Pr(X  x) = FX (x) = X
x0xpX(x
0). Figure 3.11 presents on the same graph the probability mass func￾tion (PMF) and the cumulative mass function. As its name indi- cates, the CMF corresponds to the cumulative sum of the PMF. Inversely, the PMF can be obtained from the CMF following
pX (xi) = FX(xi )  FX (xi1 ). Figure 3.11: Comparison of a probability
pX(x ) and a cumulative FX(x) mass
function. 3.3.2 Continuous Random Variables
The concepts presented for discrete sampling spaces can be ex￾tended for cases where S is a continuous domain. Because continu- ous domains are inevitably infinite, the probability that a random
variable takes a specific value X = x is zero, Note: Here, the probability equal to
zero does not mean that a specific value x is impossible. Take, for example, a
random variable defined in the interval
(0,1), for which all the outcomes are
equally probable. The probability that
X = 0 .23642 is only one out of an infinite number of possibilities in (0, 1).
Pr(X = x) = 0. For continuous random variables, the probability is only defi ned for
intervals x < X  x + x, Pr(x < X  x + x) = fX (x)x, where fX(x) ⌘ f (x) denotes a probability density function (PDF). A PDF must always be greater than or equal to zero fX (x )  0;
however, unlike for the discrete case where 0  pX (x)  1, f X(x)
can take values greater than one because it describes a probability
density rather than a probability. In order to satisfy the property
that Pr(S) = 1, the integral of fX (x) over all possible values of x
must be one,
Z +1
1
fX (x)dx = 1. The probability that Pr(X  x) is given by the cumulative density
function (CDF),Pr(X  x) = FX (x) =
Z
x1
fX(x
0)dx
0.j.-a. goulet 24
For a random variable x 2 R : X ⇠ fX(x), the CDF evaluated
at the lower and upper bounds is, respectively, FX (1) = 0 and
FX(+1) = 1. Notice that the CDF is obtained by integrating
the PDF, and inversely, the PDF is obtained by di↵erentiating the
CDF, FX(x) =
Z x1
fX (x
0)dx
0 $ fX(x) =
dFX(x)
dx
. Moreover, because FX(x) is the integral of fX (x) and fX (x)  0,
FX(x) is nondecreasing. Figure 3.12 presents examples of probabil￾ity density and cumulative distribution func tion .
(a) Probability density function
nondecreasing
(b) Cumulative distribution function Figure 3.12: Examples of PDF and CDF
for a continuous random variable.
3.3.3 Conditional Probabilities
Conditional probabilities describe the probability of a random
variable’s outcomes, given the realiz at ion of anothe r variable. The
conditional notation for discrete random variables follows
X|y ⇠ p(x|y) ⌘ pX|y
(x) ⌘ Pr(X = x|y) =
pXY (x, y)
pY (y)
,
and the conditional notation for continuou s rand om variables
follows, X|y ⇠ f (x|y) ⌘ fX|y(x|y) =
f XY (x, y)
fY (y)
. Conditional probabilities are employed in Bayes rule to infer the
posterior knowledge associated with a random variable, given the
observations made for another. Let us revisit the post-earthquake structural safety example
introduced in §3.1, where the damage state x 2 S. If we measure
the peak ground acceleration (PGA) after an earthquake y 2 R+ , Peak ground acceleration is a metric quantifying the intensity of an earthquake using the maximal acceleration recorded
during an event. we can employ the conditional probability of having struc tural
damage given the PGA value to infer the structural state of a
building that itself has not been observed. Figure 3.13 illustrates
schematically how an observation of the peak ground acceleration
y can be employed to infer the struc tural stat e of a building x, using conditional probabilities. Because the structural state X is a
discrete random variable, p( x|y) describes the posterior probability
of e ach state x 2 S, given an observed value of PGA y 2 R
+. f( y) is a normalization const ant obt ained by marginalizing X from
f(x, y) and evaluating it for the particular observed value y, f(y) = X
x2S
f(y|x) · p(x). The posterior is obtained by multiplying the likelihood of observing
the particular value of PGA y given each of the structural statesprobabilistic machine learning for civil engineers 25
x, time s the prior probab ility of each structu ral stat e , and then
dividing by the probability of the observation y itself. Conditional
probabilities can be employed to put in relation any combination
of c ontinuous and discrete random variables. Chapter 6 further
explores Baye sian estimation with applied examples.
Figure 3.13: Schematic example of how ob- servations of the peak ground acceleration
y can be employed to infer the structural state of a building x using conditional
probabilities.
3.3.4 Multivariate Random Variables
It is common to study the joint occurrence of multiple phenomena.
In the context of prob ability theory, it is done using multivariate
random variables. x = [x1 x2 · · · x n]|
is a vector (column) con￾taining realizations for n random variables X = [X1 X2 · · · Xn]|
, x : X ⇠ pX(x) ⌘ p (x), or x : X ⇠ fX (x) ⌘ f (x). For the discret e
case, the probability of the joint realizat ion x is described by
pX(x) = Pr(X1 = x1 \ X2 = x2 \ · · · \ X n = xn), where 0  pX(x)  1. For the continuous case, it is
fX(x)x = Pr(x1 < X1  x1 + x1 \ · · · \ x n < Xn  xn + xn),
for x ! 0. Not e that fX(x) can be > 1 because it describes a
probability density. As mentioned earlier, two random variables X1
and X2 are statistically independent (?) if
pX1|x2 (x1 |x2) = p X1(x1).
If X1 ? X2 ? · · · ?nX, the joint PMF is defined by the product of
its marginals,
pX1 :Xn
(x1 , · · · , xn) = pX1 (x1 )pX2 (x2)· · · p Xn
(xn). For the general case where X1 , X2, · · · , Xn are not statistically
independent, their joint PMF can be defined using the chain rule, pX1:X n
(x1 , · · · , xn) = pX1|X2:Xn
(x1|x2, · · · , x n)· · ·
·pXn1 |Xn
(xn1|x n) · pXn
(xn ). The same rules apply for continuous random variables except
that pX(x) is replaced by fX (x). Figure 3.14 presents examples of
marginals and a bivariate joint probab ility density function.
The multivariate cumulative distribution function describes the
probability that a set of n random variables is simultaneously lesser
or equal to x, FX (x) = Pr(X1  x1 \ · · · \ X n  xn).j.-a. goulet 26
The joint CDF is obt aine d by integrating the joint PDF over each
dimension from its lower bound up to x, an d inversely, the joint
PDF is obtained by di↵erentiating the CDF, FX (x) =
Z x1 1
· · ·Z x n 1
fX(x
0)dx
0 $ fX (x)dx =
@
nFX (x)
@x1 · · · @x n
. A multivariate CDF has values 0  FX (x)  1, and its value is zero
at the lowest bound for any dimension and one at the upper bound
for all dimensions,FX1:Xn
(x1, · · · , x n1,1) = 0
FX1:Xn
(+1, · · · , +1, +1) = 1. Figure 3.15 presents an example of marginals and a bivariate
cumulative distribution function.
5
0
5 5
0
5
0
0.1
0.2
0.3
0.4
x1 x2
fX1X2(x1
, x2) 5 0 5 5
0
5
x1
x2
Figure 3.14: Examples of marginals and a bivariate probability density function.
5
0
5 5
0
5
0
0.5
1
x1 x2
FX1X2(x1
, x2)
5 0 5
5
0
5
x1
x2
Figure 3.15: Examples of marginals and a bivariate cumulative distribution function.
The operation consisting of removing a random variable from
a joint set is calle d marginalization. For a set of n joint ran dom
variables, we can remove the i
th variable by summing over the i
th
dimension,X
xn pX1 :Xn
(x1 , · · · , x n) = pX1 :Xn1 (x1, · · · , xn1 ).
If we marginalize all variables by summing over all dimensions, the
result is
X
x1
· · · X
x n pX (x) = 1. For the example prese nted in figure 3.16, X1 ? X2 so the joint
PMF is obtained by the product of its marginals. It is possible to
obtain the marginal PMF for x1 from the joint through marginaliz￾ing, P3
i=1p X(x 1,i):
Marginalization
R 1
1 fX1X2
(x1, x 2)dx2 = f X1
(x1)
P
x 2 pX1X2
(x1, x2) = pX1
(x1)
FX1X2
(x1 , +1) = FX1
(x1)
pX(x1 , x2) =
8
>>
<
>>
>:
x2 = 1 x2 = 2 x2 = 3 X
3
i=1
pX(x1 ,i)
x1 = 1 0.08 0.015 0.005 0.1
x1 = 2 0.4 0.075 0.025 0.5
x1 = 3 0.32 0.06 0.02 0.4
.
Marginalization applies to continuous random variables using
integration, Z 1
1
fX1:Xn
(x1, · · · , xn)dxn = fX1 :Xn1 (x1, · · · , xn1
), where again, if we integrate over all dimensions, the result is
Z 1
1
· · · Z 1
1
fX (x)dx = 1.probabilistic machine learning for civil engineers 27
For both continuous and discrete rand om variables, we can marginal￾ize a random variable by evaluating its CDF at its upper bound,
FX1 :Xn
(x1 , · · · , x n1,+1) = F X1:Xn1 (x1 , · · · , xn1 ).
pX1
(x1)
8
<
:
p X1
(1) = 0.1
p X1
(2) = 0.5
p X1
(3) = 0.4
pX2
(x2)
8
<
:
pX2
(1) = 0.8
pX2
(2) = 0.15
pX2
(3) = 0.05
pX(x1, x2) = pX1
(x1 ) · pX2
(x2)
3
0 2
3 1
0.2
2
1
0.4
0.6
0.8
Figure 3.16: Examples of marginals and
bivariate probability mass functions.
3.3.5 Moments and Expectation
The moment of order m, E[Xm ] of a random variable X is defined
as
E[Xm] =
Z xm · fX (x)dx (continuous) = X
i
xm
i
· pX(xi) (discrete), where E[·] denotes the expectation op erat ion. For m = 1, E[X] = µX
is a measure of position for the centroid of the probability density
or mass funct ion . This centroid is analogou s to the conc ep t of
center of gravity for a solid body or cross section. An expected value
Expected value E[X] =
Z x · fX (x)dx (continuous) =X
i x i · p X(xi) (discrete)
refers to the sum of all possible values weighted by the ir probability
of occurrence. A key property of the expectation is that it is a
linear operation so that
E[X + Y ] = E[X] + E[Y ]. The notion of expectation can be extended for any function of
random variable s g(X), E[g(X)] =
Z
g(x) · fX (x)dx. The expectation of the function g (X) = (X  µX)m is referred to as
centered moment of order m, E[(X  µX )m] =
Z
(x  µX) m · fX (x)dx. For the special cases where m = 1, E[(X  µX )1
] = 0, and for m = 2,
E[(X  µX) 2
] = 
2
X = var[X] = E[X2
]  E[X]2
, where X denotes the standard deviation of X ; an d var[·] den ot es
the variance operator that measures the dispersion of the prob￾ability density function with respect to its mean. The notion of
variance is analogous to the concept of moment of inertia for a
cross section. Toget he r, µX and X are metrics describing the cen￾troid and dispersion of a random variable. Anoth er ad imen sionalj.-a. goulet 28
dispersion metric for describing a random variable is the coefficient
of variation, X =
X
µX
. Note that X only applies for µX 6= 0.
Given two random variables X, Y , their covariance, cov(X, Y ),
is defined by the expectation of the product of the mean-centered
variables, E[(X  µX )(Y  µY)] = cov(X, Y ) = E[XY ]  E[X] · E[Y ] = ⇢XY · X · Y . The correlation coefficient ⇢ XY can take a value between -1 and 1,
which quantifies the linear dependence between X and Y , ⇢XY =
cov(X, Y )
XY
, 1  ⇢XY  +1. A positive (negative) correlation indicates that a large outcome for
X is associated with a high probability for a large (small) outcome
for Y . Figu re 3.17 prese nts examp les of scatt e r plots gen erat ed for
di↵erent correlation coefficients.
Figure 3.17: Examples of scatter plots between the realizations of two random
variables for di↵erent correlation coeffi- cients ⇢.
In the special case where X and Y are independent, the correla￾tion is zero, X ? Y =) ⇢XY = 0. Note that the inverse is not
true; a correlation coefficient equal to zero does not guarantee the
independence, ⇢ij = 0 =6) Xi ? Xj
. This happens because correla￾tion only measures the linear dependence between a pair of random
variables; two random variables can be nonlinearly dependent, yet
have a correlation coefficient equal to zero. Figure 3.18 presents an
example of a sc att e r plot with quad rat ic dependence yet no linear
dependence, so ⇢ ⇡ 0.
Figure 3.18: Example of scatter plot where
there is a quadratic dependence between
the variables, yet the correlation coefficient
⇢ ⇡ 0.
Correlation also does not imply causality. For example, the
number of flu cases is negatively correlated with the te mperature;
when the seasonal temperatures drop during winter, the number
of flu cases incre ases. Noneth eless, the cold itse lf is not causing
the flu; someone isolated in a cold climate is unlikely to contract
the flu because the virus is itself unlikely to be present in the
environment. Instead, studie s have shown that the flu virus has
a high er tran smissibility in the cold and dry cond ition s that are
prevalent during winter. See, for e xam ple, Lowen and Steel.6
6 Lowen, A. C. and J. Steel (2014). Roles of humidity and temperature in shaping
influenza seasonality. Journal of Virol- ogy 88(14), 7692–7695
For a set of n random variables X1 , X2 , · · · , Xn , the covariance
matrix defines the dispersion of each variable through its variance
located on the main diagonal, and the dependence between vari- ables through the pairwise covariance located on the o↵-diagonal
terms, ⌃ =
2
64

2
X1
· · · ⇢1nX 1Xn
.
.
.
.
.
.
.
.
.
.
.
. sym. 
2
Xn 3
75
.probabilistic machine learning for civil engineers 29
A covariance matrix is symmetric (sym.), and each term is defined
following [⌃]ij = cov (Xi, Xj ) = ⇢ij XiXj
. Bec ause a variable
is linearly correlated with itself (⇢ = 1), the main diagonal terms
reduce to [⌃] ii = 
2
Xi . A covariance matrix has to be positive semi- definite (see §2.4.2) so the variances on the main diagonal must be
> 0. In orde r to avoid singular cases, there should be no line arly
dependent variable s, that is, 1 < ⇢ij < 1, 8i 6= j.
3.4 Functions of Random Variables
Let us consider a continuous random variable X ⇠ fX(x) and a
monotonic deterministic function y = g(x). The function’s out- A monotonic function g(x) takes one variable as input and returns one variable as output and is strictly either increasing
or decreasing. put Y is a ran dom variable because it takes as input the random
variable X. The PDF fY (y) is defined knowing that for each in- finitesimal part of the domain dx, there is a corresponding dy, an d
the probability over both domains must be equal, Pr(y < Y  y + dy) = Pr(x < X  x + dx)
fY (y) | {z } 0
dy = fX(x) | {z } 0
dx. The change-of-variable rule for fY (y) is defined by
f Y(y) = fX (x)
 dx
dy

= fX (x)
 dy
dx
1
= fX (g1
(y))
 dg(g1
(y))
dx
 1
, where multiplying by
dx
dy
accounts for the change in the size of the
neighborhood of x with respect to y, an d whe re the absolute value
ensures that fY (y)  0. For a function y = g(x) and its inverse
x = g1
(y), the gradient is obtained from
dy
dx
⌘
dg(x)
dx
⌘
dg( =x
z }| { g1
(y))
dx
.
0 20 40
0
20
40
y
0 20 40 x
fX( ) x
0
20
40
f Y (y)
Figure 3.19: Example of 1-D nonlinear
transformation y = g (x ). Notice how the nonlinear transformation causes the modes
(i.e., the most likely values) to be di↵erent
in the x and y spaces.
A transformation from a space x to an- other space y requires taking into account
the change in the size of the neighborhood
fY (y) = fX (x)
dx
dy

. Figure 3.19 presents an example of nonlinear transformation y =
g(x). Notice how, because of the nonline ar transformation, the
maximum for f X(x
⇤
) and the maximum for fY (y
⇤
) do not occur for
the same locations, that is, y
⇤ 6= g(x
⇤
). Given a set of n random variables x 2 R
n
: X ⇠ fX(x), we can
generalize the transformation rule for an n to n multivariate func￾tion y = g(x), as illustrated in figure 3.20a for a case where n = 2.
As with the univariate case, we need to account for the change inj.-a. goulet 30
the neighborhood size when going from the original to the trans￾formed space, as illustrated in figure 3.20b. The transformation is
then defined by
(a) 2-D transformation
(b) E↵ect of a 2-D transformation on the neighborhood size Figure 3.20: Illustration of a 2-D transfor- mation.
f Y(y)dy = fX (x)dx
f Y(y) = fX (x)
 dx
dy

, where | dx
dy
| is the inverse of the determinant of the Jacobian matrix, dx
dy
 = |detJy,x|1
dy
dx
 = |detJy,x| . The Jacobian is an n ⇥ n matrix containing the partial derivatives of
yk with respect to xl , evaluated at x so that [Jy,x ]k,l =
@yk @xl ,
Jy,x =
2
6
4
@y1 @x1
· · · @y1 @xn
.
.
.
.
.
.
.
.
. @y n
@x1
· · · @y n
@xn
3
7
5 =
2
6
4
rg1 (x)
.
.
. rgn (x)
3
7
5 .
Note that each row of the Jacobian matrix corresponds to the
gradient vect or evaluated at x, rg(x) =
 @g(x)
@x1
· · · @g(x)
@xn

. The determinant (see §2.4.1) of the Jacobian is a scalar quantifying
the size of the neighborhood of dy with respect to dx.
3.4.1 Linear Functions
(a) Generic linear transformation
-1
0
1
2
3
4
5
-1 0 1 2 3 4 5
(b) Linear transformation y = 2x Figure 3.21: Examples of transformations
through a linear function.
Figure 3.21b illustrates how a function y = 2x transforms a random
variable X with mean µX = 1 and standard deviation X = 0.5
into Y with mean µY = 2 and standard deviation y = 1. In the
machine learning context, it is common to employ linear func tions
of random variables y = g(x) = ax + b, as illustrated in figure 3.21a.
Given a rand om variable X with mean µX and variance 
2
X, the
change in the neighborhood size simplifies to
 dy
dx
 = |a|.
In such a case, because of the linear property of the expectation
operation (see §3.3.5),
µY = g(µ X) = aµX + b, Y = |a|X.probabilistic machine learning for civil engineers 31
Let us consider a set of n random variables X defined by its mean
vector and covariance matrix, X =
2
6
4
X1
.
.
. Xn
3
7
5 , µX =
2
6
4
µX1
.
.
. µ Xn
3
7
5 , ⌃X =
2
6
4

2
X 1
· · · ⇢1nX1 Xn
.
.
.
.
.
.
.
.
.
.
.
. sym. 
2
Xn
3
7
5,
and the variables Y = [Y1 Y2 · · · Yn]| obtained from a linear
function Y = g(X) = AX + b so that
2
4
3
5
n⇥1 | {z } Y
=
2
4
3
5
n⇥n
| {z } A = Jy,x
⇥
2
4
3
5
n⇥1 | {z } X
+
2
4
3
5
n⇥1 | {z } b
.
The function outputs Y (i.e., the mean vector), covariance matrix,
Note: For linear functions Y = AX+ b, the Jacobian Jy,x is the matrix A itself.
and the joint covariance are then described by
µ Y = g(µX) = AµX + b
⌃Y = A⌃X A|
⌃XY = ⌃XA|
9
>=
>;
 X
Y

,  µX
µY

,  ⌃X ⌃XY
⌃
|
XY ⌃Y

.
If instead of having an n ! n function, we have an n ! 1
function y = g(X) = a
| X + b, then the Jac obian simplifies to the
gradient vector rg(x) =
h
@g(x)
@x1
· · · @g(x)
@xn i
, which is again equal to
the vector a
|
,
⇥ ⇤
1⇥1 | {z } Y =
⇥ ⇤
1⇥n
| {z }
a|=rg(x)
⇥
2
4
3
5
n⇥1 | {z } X
+
⇥ ⇤
1⇥1 | {z }
b
.
The function output Y is then described by
µY = g(µX) = a
|µX + b

2
Y = a
|⌃Xa.
3.4.2 Linearization of Nonlinear Functions
Because of the analytic simplicity associated with linear functions
of ran dom variables, it is common to approximate nonlinear func￾tions by linear ones using a Taylor series so that
The Hessian H(µX ) is an n ⇥ n matrix
containing the 2
nd-order partial derivatives evaluated at µX . See §5.2 for details.
g(X) ⇡
2
nd
-order approximation z }| {
g(µX) +
Gradient
z }| { rg(µX )(X  µX)
| {z }
1
st
-order approximation +
1
2
(X  µX)|
Hessian
z }| { H(µX)(X  µ X) + · · ·
| {z } mth
-order approximation
.j.-a. goulet 32
In pract ice , the serie s are most often limited to the first-orde r
approximation, so for a one-to-one function, it simplifies to
Y = g(X) ⇡ aX + b. Figure 3.22 presents an example of such a linear approximation
for a one -to-one transformation. Lineariz ing at the expected value
µx minimizes the approximation errors because the linearization
is then centered in the region associated with a high probability
content for fX(x). In that case, a corresponds to the gradient of
g(x) evaluated at µX, -1
0
1
2
3
4
5
-1 0 1 2 3 4 5
Figure 3.22: Example of a linearized
nonlinear transformation.
a =

dg(x)
dx

x=µX
. For the n ! 1 multivariate case , the linearized transformation leads
to
Y = g(X) ⇡ a
|X + b = rg(µX)(X  µX) + g(µX ), where Y has a mean and variance equal to
µY ⇡ g(µX )

2
Y ⇡ rg(µX)⌃X rg(µX)|. For the n ! n multivariate case, the linearized transformation leads
to
Y = g(X) ⇡ AX + b = JY,X(µX )(X  µX ) + g(µX ), where Y is described by the mean vector and covariance matrix, µ Y ⇠= g(µX )
⌃ Y ⇠= JY,X(µX )⌃XJ
|
Y,X(µX ). For multivariate nonlinear functions, the gradient or Jacobian is
evaluated at the expected value µX .4
Probability Distributions
The definition of probability distributions fX (x) was left aside in
chapter 3. This chapter presents the formulation and properties for
the probability distributions employed in this book: the Normal
distribution for x 2 R, the log-normal for x 2 R+ , and the Beta for
x 2 (0, 1).
4.1 Normal Distribution
The most widely employed probability distribution is the Normal, also known as the Gaussian, distribution. In this book, the names
Gaussian and Normal are emp loyed interchangeably when describ￾ing a probability distribution. This section covers the math em atical
foundation for the univariate and multivariate Normal and then
details the properties explaining its widespread usage.
4.1.1 Univariate Normal
Univariate Normal x 2 R : X ⇠ N (x; µ, 
2)
4 2 0 2 4
0
0.1
0.2
0.3
µ
µ +  µ  
x
fX( ) x
(a) Probability density function (PDF)
4 2 0 2 4
0
0.2
0.4
0.6
0.8
1
µ
µ +  µ  
x
FX( ) x
(b) Cumulative distribution function (CDF)
Figure 4.1: Representation of the univari- ate Normal for µ = 0,  = 1.
The probability density function (PDF) for a Normal random
variable is defined over the real numbers x 2 R. X ⇠ N (x; µ, 
2
) is
parameterized by its mean µ and variance 
2
, so its PDF is
fX(x) = N(x; µ, 
2
) =
1
p
2⇡
exp 

1
2 ✓
x  µ
 ◆2!
. Figure 4.1 presents an e xam ple of PDF and cumulative distribution
function (CDF) with parameters µ = 0 and  = 1. The mode—
that is, the most likely value—corresponds to the mean. Changing
the mean µ causes a translation of the distribution. Increasing
the standard deviation  causes a proportional increase in the
PDF’s dispersion. The Normal CDF is presented in figure 4.1b. Its
formulation is obtained through integration, where the integral canj.-a. goulet 36
be formulate d using the error function erf(·), FX(x) =
Z x1
1
p
2⇡
exp 

1
2 ✓
x
0  µ
 ◆2!
dx
0 =
1
2 ✓
1 + erf ✓
x  µ
p
2 ◆◆
.
fX(x) =
constant z }| { 1
p
2⇡
exp
0
B
B
B
B@

1
2
quadratic z }| { 0
B
B@
x  µ
 | {z }
linear
1
C
CA
21
C
C
C
CA
| {z }
exponential
4 2 0 2 4 10
0
10 µ- µ µ+
x
x µ 
 4 2 0 2 4
0
10
20 µ- µ µ+
x
x µ 
 2 4 2 0 2 4
0
1 µ- µ µ+
x
e1
2
( x µ 

)2 4 2 0 2 4
0
0.4 µ- µ µ+
x
1 p
2⇡ e
1
2( x µ 
 )2 Figure 4.2: Illustration of the univariate Normal probability density function
formulation for µ = 0,  = 1.
Figure 4.2 illustrates the successive steps taken to construct
the univariate Normal PDF. Within the innermost parenthesis of
the PDF formulation is a linear function xµ

, which centers x on
the mean µ and normalizes it with the standard deviation . This
first term is then squared, leading to a positive number over all
its domain except at the mean, where it is equal to zero. Taking
the negative exponential of this second term leads to a bell-shaped
curve, where the value equals one (exp(0) = 1) at the mean x = µ
and where there are inflexion points at µ ± . At this step, the curve
is proportional to the final Normal PDF. Only the normalization
constant is missing to ensure that R 1
1 f(x)dx = 1. The normal￾ization constant is obtained by integrat ing the ex ponential term, Z +1
1
exp 

1
2 ✓
x  µ
 ◆2!
dx = p
2⇡. (4.1)
Dividing the exponential te rm by the normalization constant in
equation 4.1 results in the final formulation for the Normal PDF. Note that for x = µ, f(µ) 6= 1 because the PDF has been normal￾ized so its integral is one.
4.1.2 Multivariate Normal
The joint probability density function (PDF) for two Normal
random variables {X1, X2 } is given by
fX1X2 (x1, x2) =
1
2⇡12p
1  ⇢2
exp 

1
2(1  ⇢
2 ) ✓
x1  µ1
1 ◆2
+
✓
x2  µ2
2 ◆22⇢ ✓
x1  µ1
1 ◆ ✓
x2  µ2
2 ◆!!
. There are three terms within the parentheses inside the exponential. The first two are analogous to the quadratic terms for the univari- ate case. The third one includes a new parameter ⇢ describing the
correlation coefficient between X1 and X2. Together, these thre e
terms describe the equation of a 2-D ellipse centered at [µ1 µ2]|
. Multivariate Normal x 2 Rn : X ⇠ N (x; µX, ⌃ X ) In a more general way, the probability density function for n
random variables X = [X1 X2 · · · Xn]| is described by x 2 Rn :
X ⇠ N (x;µX, ⌃X ), where µX = [µ1 µ2 · · · µ n]|
is a vectorprobabilistic machine learning for civil engineers 37
containing mean values and ⌃X is the covariance matrix,
⌃X = DX RXDX =
2
6
6
4

2
1 ⇢1212 · · · ⇢1n 1n 
2
2
· · · ⇢2n 2n
.
.
.
.
.
.
.
.
.
.
.
. sym. 
2n
3
7
7
5
n⇥n
.
DX is the standard deviation matrix containing the standard
D X : Standard deviation matrix R X : Correlation matrix ⌃X : Covariance matrix
deviation of each random variable on its main diagonal, and RX is
the symmetric (sym.) correlation matrix containing the correlation
coefficient for each pair of random variab le s, DX =
2
6
6
4
1 0 0 0
2 0 0
.
.
. 0
sym. n
3
7
7
5
, RX =
2
6
6
4
1 ⇢ 12 · · · ⇢1n 1 · · · ⇢2n
.
.
.
.
.
.
.
.
. ⇢n1n
sym. 1
3
7
7
5
.
Note that a variable is linearly correlated with itself so the main
diagonal terms for the correlation matrix are [RX]ii = 1, 8i. The
multivariate Normal joint PDF is described by
fX (x) =
1
(2⇡)n/2(det ⌃X )1/2 exp ✓
1
2
(x  µ X)|⌃X
1
(x  µX )◆
, where the terms inside the exponential describe an n-dimensional
ellipsoid centered at µX. The directions of the principal axes of this
ellipsoid are described by the eige nvector (see §2.4.2) of the covari- ance matrix ⌃X, and their lengths by the eigenvalues. Figure 4.3
presents an example of a covariance matrix decomp osed into its
eigenvector and eigenvalues. The curves overlaid on the joint PDF
describe the marginal PDFs in the eigen space.
2
0
2 2
0
2
0
0.2
0.4
x1
x2
fX1X2(x1
, x2) 2 0 2 2
0
2
⌫2

⌫1
2⌫2
1⌫1
x1
x2 ⌃ =

1 0.8
0.8 1

V =
 ⌫1⌫2
 =

0.71 0.71 0.71 0.71

 = [1.8 0.2]| Figure 4.3: Example of bivariate PDF with
µ1 = µ2 = 0, 1 = 2 = 1, and ⇢ = 0.8, for which the covariance matrix is decomposed
into its eigenvector and eigenvalues.
For the multivariate Normal joint PDF formulation, the term
on the left of the exponential is again the normaliz ation con stant, which now include s the det e rminant of the covariance matrix. As presented in § 2.4.1, the determinant quantifies how much the
covariance matrix ⌃X is scaling the space x. Figure 4.4 presents
examples of bivariate Normal PDF and CDF with parameters
µ1 = 0, 1 = 2, µ2 = 0, 2 = 1, and ⇢ = 0.6. For the bivariate
CDF, notice how evaluat ing the upper bound for one variable leads
to the marginal CDF, represented by the bold red line, for the other
variable.
4.1.3 Properties
A multivariate Normal random variable follow several prope rties. Here, we insist on six:j.-a. goulet 38
1. It is completely defined by its mean vector µX and covariance
matrix ⌃X . 2. Its marginal distributions are also Normal, and the PDF of any
marginal is given by
xi
: Xi ⇠ N (xi
; [µX]i
, [⌃X]ii).
3. The absence of correlation implies statistical independence. Note
that this is not gene rally true for other types of random variables
(see §3.3.5),
⇢ij = 0 , X i ? Xj . 4. The central limit theorem (CLT) states that, under some con￾ditions, the asymptotic distribution obtained from the nor- malized sum of independent identically distributed (iid) ran￾dom variables (normally distributed or not) is Normal. Given
Xi
, 8i 2 {1, · · · , n}, a set of iid random variables with ex￾pected value E[Xi
] = µX and finite variance 
2
X, the PDF of
Y = Pn
i=1 Xi approaches N (nµX, n
2
X ), for n ! 1. More
formally, the CLT states that
p
n
 Yn  µ X
 d
! N (0, 
2
X), where
d
! means converges in distribution. In practice , when
observing the outcomes of real-life phenomena, it is common to
obtain empirical distributions that are similar to the Normal
distribution. We can see the paralle l where these phen ome na are
themselves issued from the superposition of several phen ome na. This property is key in explaining the widespread usage of the
Normal probability distribution.
5
0
5 5
0
5
0
0.1
0.2
0.3
0.4
x1 x2
fX1X2(x1
, x2) 5 0 5 5
0
5
x1
x2 (a) Bivariate Normal PDF
5
0
5 5
0
5
0
0.5
1
x1 x2
FX1X2 (x1
, x2) 5 0 5 5
0
5
x1
x2 (b) Bivariate Normal CDF
Figure 4.4: Examples of bivariate PDF and
CDF for µ1 = µ2 = 0, 1 = 2,  2 = 1, and
⇢ = 0.6.
5. The output from linear functions of Normal random variables
are also Normal. Given x : X ⇠ N(x; µX, ⌃X) and a linear
function y = Ax + b, the properties of linear tran sformations
described in §3.4.1 allow obtaining
Y ⇠ N (y;AµX + b, A⌃XA
|
). Let us consider the simplified case of a linear function z = x + y
for two random variables x : X ⇠ N (x; µX, 
2
X ), y : Y ⇠ N(y; µY , 
2
Y
). Their sum is described by
Z ⇠ N (z; µX + µY
| {z }
µ Z
, 
2
X + 
2
Y + 2⇢XY X Y
| {z } 
2Z
).probabilistic machine learning for civil engineers 39
In the case where both variables are statistically independent
X ? Y , the variance of their sum is equal to the sum of their
respective variance. For the general case describing the sum
of a set of n correlated normal random variables Xi such that
X ⇠ N (x; µX, ⌃ X), Z = Pn
i=1 Xi ⇠ N
⇣
z;Pn
i=1
[µX]i
, Pn
i=1 Pn
j=1
[⌃] ij⌘
. (4.2)
As we will see in the next chapters, the usage of linear models
is widespread in machine learning because of the analytical
tractability of linear functions of Normal random variables. 6. Conditional distributions are Normal. For instance, we can
partition an ensemble of n random variables X in two subsets so
that
X =
 Xi
X j

, µ =
 µi
µj

, ⌃ =
 ⌃i ⌃ij
⌃ji ⌃j

, where ⌃i describes the covariance matrix for the i
th
subset of
random variables and ⌃ij = ⌃
|
ji describes the covariance be￾tween the random variables belonging to subsets i and j. It is
mentioned in §3.3.4 that a conditional probability density func￾tion is obtained from the division of a joint PDF by a marginal
PDF. The same concept applies to defi ne the conditional PDF of
Xi given a vector of observations Xj = xj
, fXi |xj(xi | Xj = xj | {z }
observations
) =
fXiXj (xi, xj )
fX j(xj ) = N (xi; µi|j , ⌃i|j), where the conditional mean and covariance are
µi|j = µi + ⌃ij⌃
1
j (xj  µj )
⌃i|j = ⌃i  ⌃ij ⌃
1
j ⌃
|
ij
. (4.3)
If we simplify this setup for only two random variables X1 and
X2 , where we want the conditional PDF of X1 given an observa￾tion X2 = x2 , then equation 4.3 simplifies to
µ1|2 = µ1 + ⇢1
x2  µ2
2

2
1|2 = 
2
1
(1  ⇢
2
). In the special case where the prior mean µ1 = µ2 = 0 and the
prior standard deviations 1 = 2 > 0, then the conditional mean
simplifies to the observation x2 times the correlation coefficient
⇢. Note that the conditional variance 
2
1|2
is independent of the
observed value x2 ; it only depends on the prior variance and the
correlation coefficient. For the special case where ⇢ = 1, then

2
1|2 = 0.j.-a. goulet 40
4.1.4 Example: Conditional Distributions
For the beam example illustrated in figure 4.5, our prior knowledge
for the resistance {X1 , X2 } of two adjacent beams is
X1 ⇠ N (x1; 500, 1502
) [kN·m]
X2 ⇠ N (x2; 500, 1502
) [kN·m], and we know that the beam resistan ces are correlat ed with ⇢ 12 =
0.8. Such a correlation could arise because both beams were fab￾ricated with the same process, in the same factory. This prior
knowledge is described by the joint bivariate Normal PDF,
(a) Multi-beam bridge span. (Photo: Archives Radio-Canada)
(b) Concrete beams Figure 4.5: Example of dependence be- tween the resistance of beams.
fX1 X2 (x1 , x2) = N(x; µX , ⌃X)
8
>><
>:
µX =

500
500 
⌃X =

1502 0.8 · 1502
0.8 · 1502 1502 
.
If we observe that the resistance of the second beam x 2 = 700 kN·m, we can employ conditional probabilities to estimat e the PDF of the
strength X1 , given the observation x2 ,
fX1 |x2(x1|x2 ) = N(x1 ; µ1|2, 
2
1|2
), where
µ1|2 = 500 + 0.8 ⇥ 150
observation z}|{ 700 500
150 = 660 kN·m
1|2 = 150p
1  0.8
2 = 90 kN·m. Figure 4.6 presents the joint and conditional PDFs correspon ding
to this example. For the joint PDF, the highlighted pink slice corre￾sponding to x2 = 700 is proportional to the conditional probability
fX1|x 2 (x1 |x2 = 700). If we want to obtain the conditional distribu- tion from the joint PDF, we have to divide it by the marginal PDF
fX2 (x2 = 700). This ensures that the conditional PDF for x1 inte- grates to 1. This example is trivial, yet it sets the foundations for
the more advanced models that will be presented in the following
chapters.
0
1,000
0
700
1,000
·105
x1 x2
fX1
,X2(x1
, x2)
0
1,000
0
700
1,000
·103
x1 x2
fX1
|X2 (x1
|x2) µ1|2 =660 1|2 =90
Figure 4.6: Joint prior PDF fX1 X2
( x1, x2) and conditional PDF fX1|x2
(x1|x2) de- scribing the resistance of beams.
4.1.5 Example: Sum of Normal Random Variables
Figure 4.7: Steel cables each made from
multiple wires. (This example is adapted
from Armen Der Kiureghian’s CE229
course at UC Berkeley.)
Figure 4.7 presents steel cables where each one is made from dozens
of individual wires. Let us consider a cable made of 50 steel wires, each having a resistance xi
: Xi ⇠ N (xi
; 10,3
2
) kN. We use
equation 4.2 to compare the cable resistance Xcable = P
50
i=1 Xi
depending on the correlat ion coefficient ⇢ij
. With the hypothesisprobabilistic machine learning for civil engineers 41
that Xi ? Xj , ⇢ij = 0, all nondiagonal terms of the covariance
matrix [⌃X ]ij = 0, 8i 6= j, which lead s to
Xcable ⇠ N (x; 50 ⇥ 10 kN, 50 ⇥ (3 kN)2 | {z } Xcable= 3p
50 ⇡ 21 kN
). With the hypothesis ⇢ij = 1, all terms in [⌃X]ij = (3kN)2
, 8i, j, so
that
Xcable ⇠ N (x; 50 ⇥ 10 kN, 50 2 ⇥ (3 kN)2 | {z } X cable= 3 kN⇥50 =150 kN
).
Figure 4.8 presents the resulting PDFs for the cable resistan ce, given each hypothesis. These results show that if the uncertainty
in the resistance for each wire is independent, there will be some
cancellation; some wires will have a resistance above the mean,
and some will have a resistance below. The resulting coefficient of
variation for ⇢ = 0 is cable = 21
500 = 0.11, which is approximately
three times smaller than wire = 3
10 = 0.3, the variability associated
with each wire. In the opposite case, if the resistance is linearly
correlated (⇢ = 1), the uncertainty adds up as you incre ase the
number of wires, so  cable = 150
500 = wire.
0 500 1,000
0
1
2
·102
⇢ = 1
⇢ = 0
x (cable)
fX ( ) x
Figure 4.8: Probability density function
of the cable resistance depending on the
correlation between the strength of each wire.
4.2 Log-Normal Distribution
The log-normal distribution is obtained by transforming the Normal
distribution through the function ln x. Because the logarithm
function is only defined for positive values, the domain of the log￾normal distribution is x 2 R+.
4.2.1 Univariate Log-Normal
Univariate log-normal x 2 R
+ : X ⇠ ln N (x; , ⇣) x
0 2 R : X0 = ln X ⇠ N(ln x; , ⇣
2
)
µX : mean of X
 : mean of ln X (= µln X) 
2X : variance of X
⇣
2 : variance of ln X(= 
2
ln X )
The random variable X ⇠ ln N(x;, ⇣ ) is log-normal if ln X ⇠ N(ln x; , ⇣
2
) is Normal. Given the transformation function x
0 =
ln x, the change of var iable rule presented in §3.4 requires that
N(x
0 ;,⇣
2
) z }| { fX 0(x
0) dx
0 = fX (x)dx
f X0 (x
0)
 dx
0 dx
 = fX (x) | {z }
ln N(x;,⇣)
, where the derivative of ln x with respect to x is
dx
0 dx =
d ln x
dx =
1
x
. Therefore, the analytic formulation for the log-normal PDF is given
by the product of the tran sformation’s derivative and the Normalj.-a. goulet 42
PDF evaluated for x
0 = ln x,
fX(x) =
1
x
· N (lnx; , ⇣
2
) =
1
x
· 1
p
2⇡⇣
exp 

1
2✓
ln x  
⇣ ◆2!
, x > 0. The univariate log-normal PDF is parameterized by the mean
(µln X = ) and variance (
2
ln X = ⇣
2
) defined in the log-transformed
space (ln x). The mean µX and variance 
2
X of the log-normal
random variable can be transformed in the log-space using the
relations
 = µln X = lnµX 
⇣
22
⇣ = ln X = s
ln✓
1 +
⇣
X
µX ⌘2◆ = p
ln(1 + 
2
X ). (4.4)
Note that for X < 0.3, the standard deviation in the log-space is
approximately equal to the coefficient of variation in the original
space, ⇣ ⇡ X. Figure 4.9 presents an example of log-normal PDF
plotted (a) in the original space and (b) in the log-transformed
space. The mean and standard deviation are {µX = 2, X = 1} in
the original space and { = 0.58, ⇣ = 0.47} in the log-transformed
space.
0 1 2 3 4 5 6 7
0
0.2
0.4
µX µ X   X µX + X
x
fX ( ) x
(a) Original space
3 2 1 0 1
0
1   ⇣  + ⇣
ln x
fln X(ln x) (b) Log-transformed space Figure 4.9: Univariate log-normal probabil￾ity density function for {µX = 2, X = 1}
and { = 0.58, ⇣ = 0.47}.
4.2.2 Multivariate Log-Normal
Multivariate log-normal x 2 (R
+ )n X1, X2 , · · · , Xn are jointly log-normal if ln X1, ln X2, · · · , lnXn are : X ⇠ ln N(x; µln X, ⌃ln X)
jointly Normal. The multivariate log-normal PDF is parameterized
by the mean values (µln Xi = ), variances (
2
ln Xi = ⇣
2
), and
correlation coefficients (⇢ln Xi ln Xj ) defined in the log-transformed
space. Correlation coefficients in the log-space ⇢ln Xi ln Xj are related
to the correlation coefficients in the original space ⇢Xi Xj using the
relation
⇢ln Xi ln Xj =
1
⇣ i⇣j
ln(1 + ⇢XiXjXi Xj ), where ⇢ln Xi ln Xj ⇡ ⇢Xi Xj for Xi , Xj ⌧ 0.3. The PDF for two
random variables {X1, X2} such that {x1, x2 } > 0 is
f X1 X2
(x1 , x2) =
1 x1x2p
2⇡⇣1⇣2q
1  ⇢
2
ln
exp 

1
2(1  ⇢
2
ln
) ✓
ln x1  1 ⇣1 ◆2+
✓
ln x2  2 ⇣2 ◆22⇢ln ✓
ln x1  1 ⇣1 ◆✓
ln x2  2 ⇣2 ◆2!!
.probabilistic machine learning for civil engineers 43
Figure 4.10 presents an example of bivariat e log-normal PDF
with parameters µ1 = µ2 = 1.5, 1 = 2 = 0.5, and ⇢ = 0.9. The
general formulation for the multivariate log-normal PDF is
fX(x) = ln N (x; µln X, ⌃ln X) = 1
(Qn
i=1 xi)(2⇡)n/2 (det ⌃ln X )1/2 exp

1
2
(ln x  µ ln X) |⌃
1
ln X(ln x  µln X)
, where µln X and ⌃ln X are respectively the mean vector and covari- ance matrix defined in the log-space.
0
1
2
3
0
1
2
3
0
1
2
3
x1 x2
fX1X2(x1
, x2) 0 1 2 3
0
1
2
3
x1
x2
Figure 4.10: Example of bivariate log- normal probability density function for µ1 = µ2 = 1.5, 1 = 2 = 0.5, and ⇢ = 0.9.
4.2.3 Properties
Because the log-normal distribution is obtained through a trans￾formation of the Normal distribution, it inherits several of its
properties. 1. It is completely defined by its mean vector µln X and covariance
matrix ⌃ln X. 2. Its marginal distributions are also log-normal, and the PDF of
any marginal is given by
xi : Xi ⇠ ln N(xi; [µ ln X]i, [⌃ln X]ii). 3. The absence of correlation implies statistical independence. Remember that this is not generally true for other types of
random variables (see §3.3.5),
⇢ij = 0 , X i ? Xj . 4. Conditional distributions are log-normal, so the PDF of Xi given
an observation Xj = xj is given by
fXi |xj(xi| Xj = x j | {z }
observations
) = ln N(xi; µln i|j , ⌃ln i|j ), where the conditional mean vector and covarian ce are
µln i|j = µln i + ⌃ln ij⌃
1
j (ln xj  µln j )
⌃ln i|j = ⌃ln i  ⌃ln ij⌃
1
j ⌃
|
ln ij
. (4.5)
5. The multiplication of jointly log-normal random variables is
jointly log-normal so that for X ⇠ ln N (x;X, ⇣ X) and Y ⇠
ln N(y; Y , ⇣Y ), where X ? Y , Z = X · Y ⇠ ln N(z; Z, ⇣Z ) 
Z = X + Y
⇣
2
Z = ⇣
2
X + ⇣
2
Y
. Because the product of log-normal rand om variables can be
transformed in the sum of Normal random variables, the proper- ties of the central limit theorem presented in §4.1.3 still hold.j.-a. goulet 44
4.3 Beta Distribution
The Beta distribution is defined over the interval (0,1). It can be
scaled by the transformation x
0 = x · (b  a) + a to model bounded
quantities within any range (a, b). The Beta probability density
function (PDF) is defined by Beta x 2 (0, 1) : X ⇠ B(x;↵, )
E[X] =
↵
↵ + 
var[X] =
↵
(↵ + )2(↵ +  + 1)
fX(x) = B(x;↵, ) =
x
↵1
(1  x)1
B(↵, )
8
<
:
↵ > 0
 > 0
B(↵, ) : Beta function, where ↵ and  are the two distribution parameters, and the Beta
function B(↵, ) is the normalization constant so that
B(↵, ) =
Z 1
0
x
↵1
(1  x)1dx. A common application of the Bet a PDF is to employ the interval
(0, 1) to model the probability density of a probability itself. Let us
consider two mutually exclusive and collectively exhaustive events, for example, any event A and its complement A, S = {A, A}. If the
probability that the event A occurs is uncertain, it can be described
by a random variable so that
⇢
Pr(A) = X
Pr(A) = 1  X, where x 2 (0, 1) : X ⇠ B(x; ↵, ). The parameter ↵ can be inter- preted as pseudo-counts representing the number of observations
of the event A, and  is the number of observations of the comple￾mentary event A . This relation between pseudo-counts and the Beta
distribution, as well as practical applications, are further de taile d
in chapter 6. Figure 4.11 presents examples of Beta PDFs for three
sets of parameters. Note how for ↵=  = 1, the Beta distribution is
analogous to the Uniform distribution U(x; 0, 1).
0 0.2 0.4 0.6 0.8 1
Figure 4.11: Three examples of the Beta probability density function evaluated for di↵erent sets of parameters {↵, }.5
Convex Optimization
Note: In the context of this book, we are only concerned by target functions f˜(✓) 2 R that are continuous and twice di↵erentiable. When building a model in the context of machine learning, we
often seek optimal model parameters ✓, in the sense where they
maximize the prior probability (or probability density) of predicting
observed dat a. Here, we denote by f˜(✓) the target function we want
to maximize. Optimal parameter values ✓
⇤ are those that maximize
the function ˜f(✓),
Note: In order to be mathematically
rigorous, equation 5.1 should employ the operator 2 rather than = to recognize that
✓
⇤ belongs to a set of possible solutions maximizing
˜ ✓
⇤ = f(✓). arg max
✓ ˜f(✓). (5.1)
(a) Convex set (b) Non-convex set
Figure 5.1: Examples of a convex and a non-convex set.
✓1
✓2
˜f( ) ✓
✓1
✓2
(a) f˜(✓): Concave,  ˜f(✓): Convex
✓1
✓2
˜f( ) ✓
✓1
✓2
(b) ˜f(✓): Non-concave,  ˜f(✓): Non-convex Figure 5.2: Representations of convex/
concave and non-convex/non-concave
functions.
With a small caveat that will be covered below, convex opti- mization methods can be employed for the maximization task in
equation 5.1. The key aspect of convex optimization methods is
that, under certain condition s, they are guaranteed to reach optimal
values for convex functions. Figure 5.1 presents examples of convex
and non-convex sets. For a set to be convex , you must be able to
link any two points belonging to it without being outside of this
set. Figure 5.1b presents a case where this property is not satisfied.
For a convex function, the segment linking any pair of its points lies
above or is equal to the function. Conversely, for a concave function, the opposite holds: the segment linking any pair of points lies below
or is equal to the function. A concave function can be transformed
into a convex one by taking the negative of it. Therefore, a max￾imization problem formulated as a concave optimization can be
formulated in terms of a convex optimization following
✓
⇤ = arg max
✓ ˜f(✓) | {z } Concave optimization ⌘ arg min
✓  ˜f(✓) | {z } Convex optimization
.
In this chapter, we refer to convex optimization even if we are inter- ested in maximizing a con cave function, rather than minimizing a
convex one. This choice is justified by the prevalence of convex opti- mization in the literature. Moreover, note that for several machine
learning methods, we seek ✓
⇤ based on a minimization proble mj.-a. goulet 48
where  ˜f (✓) is a function of the di↵erence between observed val- ues an d those predic ted by a model. Figure 5.2 presents examples
of convex/concave and non-convex/non-concave func tion s. Non￾convex/non-concave functions such as the one in figure 5.2b may
have several local optima. Many functions of practical interest are
non-convex/non-concave. As we will see in this chapter, convex
optimization methods can also be employed for non-convex/non￾concave functions given that we choose a proper starting location.
This chapter presents the gradient ascent and Newton-Raphson
methods, as well as practical tools to be employed with them.
For full-depth details regarding optimization methods, the reader
should refer to dedicated textbooks.1
1Bertsekas, D. P., A. Nedi, and A. E. Ozdaglar (2003). Convex analysis and opti- mization. Athena Scientific; Chong, E. K. P. and S. H. Zak (2013). An introduction to
optimization (4th ed.). Wiley; and Nocedal, J. and S. Wright (2006). Numerical op￾timization. Springer Science & Business Media Derivative
f˜0(✓) ⌘
df˜(✓)
d✓
Gradient
rf˜(✓) ⌘ r✓f˜(✓) =
h
@f˜(✓)
@✓1 @˜f(✓)
@✓2
· · · @ f˜(✓)
@✓n
i| Maximum of a concave function
✓
⇤ = arg max
✓ ˜f(✓) : d
˜f(✓
⇤)
d✓ = 0
5.1 Gradient Ascent
A gradient is a vector containing the partial derivatives of a func￾tion with re spect to its variables. For a continuous function, the
maximum is located at the point where its gradient equals zero.
Gradient ascent is based on the principle that as long as we move
in the direc tion of the gradient, we are moving toward a maximum. For the unidimensional case, we choose to move to a new position
✓new defined as the old value ✓old plus a search direction d defined
by a scaling factor  times the derivative estimat ed at ✓old,  is also known as the learning rate or step
length. ✓new = ✓old +  · ˜f
0(✓old ) | {z }
d
.
A common pract ice for setting  is to employ backtracking line
search where a new position is accepted if the Armijo rule
2 is
2Armijo, L. (1966). Minimization of
functions having Lipschitz continuous first
partial derivatives. Pacific Journal of
Mathematics 16 (1), 1–3
satisfied so that
f˜(✓new)  f˜(✓old) + c · df˜0(✓old), with c 2 (0, 1). (5.2)
0 2 4 6 8 10 12
0
0.5
1 Armijo’s
inadmissible ✓new
c = 0
c
=
1
✓
˜f( )✓
✓old ✓new
0 2 4 6 8 10 12 0.5
0
0.5
✓
˜f
0( )✓ Figure 5.3: Example of application of
the Armijo rule to test if ✓new has suffi- ciently increased the objective function in
comparison with ✓old.
Figure 5.3 presents a comparison of the application of equation
5.2 with the two extreme cases, c = 0 and c = 1. For c = 1, ✓new is
only accepted if f˜(✓new) lies above the plane defin ed by the tan gent
at ✓old . For c = 0, ✓new is on ly accept ed if ˜f(✓new) > ˜f(✓old). The
larger c is, the stricter is the Armijo rule for ensuring that sufficient
progress is made by the current step. W ith backtracking line search, we start from an initial value of 0 and red uce it until equation 5.2
is sat isfie d. Algorithm 1 presents a minimal version of the gradient
ascent with backtracking line search.probabilistic machine learning for civil engineers 49
Algorithm 1: Gradient ascent with backtracking line search
1 initialize  = 0, ✓old = ✓0, define ✏, c, ˜f(✓)
2 while | ˜f
0(✓old )| > ✏ do
3 compute ⇢
f˜(✓old) (Function value) ˜f
0(✓old) (1
st derivative)
4 compute ✓new = ✓ old +  ˜f
0(✓old ) | {z }
d
5 if ˜f(✓new ) < ˜f(✓old) + c · d
˜f
0(✓old) then
6 assign  = /2 (Backtracking)
7 Goto 4
8 assign  = 0, ✓ old = ✓new
9 ✓
⇤ = ✓old
0 2 4 6 8 10 12
0
0.5
1
loop #1
✓old =3.50, 0 =3
f˜(✓old) =0.21
˜f
0(✓old ) =0.32
✓new = ✓old +0f˜0(✓old) =4.47
˜f(✓ new)=0.82
0
✓
˜f( )✓ 0 2 4 6 8 10 12 0.5
0
0.5
✓
˜f
0( )✓ 0 2 4 6 8 10 12
0
0.5
1
loop #2
✓old =4.47, 0 =3
˜f(✓old) =0.82
˜f
0(✓old ) =0.72
✓new = ✓old +0 ˜f
0(✓old) =6.64
f˜(✓ new)=-0.03
✓new = ✓old +
 0 2 ˜f
0(✓old ) =5.55
f˜(✓ new)=0.70
✓new = ✓old +
 0 4 f˜0(✓old ) =5.01
˜f(✓ new)=1.01
 0
0/2
 0/4
✓
˜f( )✓ 0 2 4 6 8 10 12 0.5
0
0.5
✓
˜f
0( )✓ Figure 5.4: Example of application of
gradient ascent with backtracking for finding the maximum of a function.
Figure 5.4 presents the first two steps of the application of
algorithm 1 to a non-convex/non-concave function with an initial
value ✓0 = 3.5 and a scaling factor 0 = 3. For the second step,
the scaling factor  has to be reduced twice in order to satisfy the
Armijo rule. One of the diffic ulties with gradient ascent is that the
convergence speed depends on the choice of 0 . If 0 is too small, several steps will be wasted and convergence will be slow. If 0 is
too large, the algorithm may not converge.
0 2 4 6 8 10 12
0
0.5
1
✓
˜f( )✓ 0 2 4 6 8 10 12 0.5
0
0.5
✓
˜f
0( )✓
Figure 5.5: Example of application of
gradient ascent converging to a local maximum for a function.
Figure 5.5 presents a limitation common to all convex optimiza- tion methods when applied to functions involving local maxima; if
the starting location ✓0 is not locat ed on the slope segment leading
to the global maximum, the algorithm will most likely miss it and
converge to a local maximum. The task of selecting a proper value
✓0 is nontrivial because in most cases, it is not possible to visualize
f˜(✓). This issue can be tackled by attempting multiple starting
locations ✓0 and by using domain knowledge to identify proper
starting locations.
✓ 1
✓2 Gradient Ascent (GA)
GA with momentum
Figure 5.6: Comparison of gradient ascent with and without momentum.
Gradient ascent can be applied to search for the maximum of a
multivariate function by replacing the univariate derivative by the
gradient so that
✓ new = ✓ old +  · r✓ ˜f(✓old). As illustrat ed in figure 5.6, because gradient ascent follows the
direction where the gradient is maximal, it often displays an os- cillatory pattern. This issue can be mitigated by introducing a
momentum term in the calculation of ✓new, 3
3Rumelhart, D. E., G. E. Hinton, and R. J. Williams (1986). Learning representations by back-propagating errors. Nature 323, 533–536
vnew =  · vold +  · r✓ ˜f(✓old), ✓new = ✓old + vnew where v can be interpreted as a velo c ity that carries the momentum
from the previous iterations.j.-a. goulet 50
5.2 Newton-Raphson
The Newton -Raphson method allows us to adaptively scale the
search direction vector using the second-order derivative f˜00(✓).
Second-order derivatives ˜f
00(✓) ⌘
d
2 f˜(✓)
d✓2 ˜f
00i (✓) ⌘
@
2f˜(✓)
@✓
2
i Hessian
hH[˜f(✓)]i
ij =
@
2 ˜f(✓)
@✓i@✓j
Knowing that the maximum of a function corresponds to the point
where the gradient is zero, ˜f
0(✓) = 0, we can find this maximum by
formulating a linearized gradient equation using the second-order
derivative of f˜(✓) and then set it equal to zero. The analytic formu￾lation for the linearized gradient function (see §3.4.2) approximated
at the current location ✓old is
˜f
0(✓) ⇡ ˜f
00(✓old ) · (✓  ✓old ) + ˜f
0(✓old). (5.3)
We can estimate ✓new by setting equation 5.3 equal to zero, and
then by solving for ✓, we obt ain
✓new = ✓old 
˜f
0(✓old) ˜f
00(✓old)
. (5.4) 0 2 4 6 8 10 12 40 20
0
✓
˜f( ) ✓
0 2 4 6 8 10 12 10
0
10
✓
˜f0( )✓
0 2 4 6 8 10 12 4 3 2 1
0
✓
˜
f
00( )✓
Figure 5.7: Example of application of
Newton-Raphson to a quadratic function.
Let us consider the case where we want to find the maximum of
a quadratic function (i.e., / x
2
), as illustrat ed in figure 5.7. In the
case of a quadratic funct ion , the algorithm converges to the exact
solution in one ite ration, no matter the starting point, becau se the
gradient of a quadratic funct ion is exactly described by the linear
function in equation 5.3.
Algorithm 2 presents a minimal version of the Newton-Raphson
method with backtracking line search. Note that at line 6, there
is again a scaling factor , which is employed because the Newton￾Raphson method is exact on ly for quadratic funct ion s. For more
general non-convex/non-concave functions, the linearized gradient is
an approximation such that a value of  = 1 will not always lead to
a ✓new satisfying the Armijo rule in equation 5.2.
Figure 5.8 presents the application of algorithm 2 to a non￾convex/non-concave function with an initial value ✓0 = 3.5 and a
scaling factor 0 = 1. For each loop, the pink solid line represents
the linearized gradient function formulated in equation 5.3. Notice
how, for the first two iterations, the second derivative f˜00(✓) > 0.
Having a positive second derivative indicates that the linearization
of ˜f
0(✓) eq uals zero for a minimum rather than for a maximum. One simple option in this situat ion is to define  =  in order
to en sure that the next step moves in the same direction as the
gradient. The convergence with Newton-Raphson is typically faster
than with gradient ascent.probabilistic machine learning for civil engineers 51
Algorithm 2: Newton-Raphson with backtracking line search
1 initialize  = 0 = 1, ✓old = ✓0 , define ✏, c, ˜f(✓)
2 while | ˜f
0(✓old )| > ✏ do
3 compute: ✓old !
8
<
:
˜f(✓ old) (Function evaluation) ˜f
0(✓ old) (First derivative)
f˜00(✓ old) (Second derivative)
4 if ˜f
00(✓ old) > 0 then
5  = 
6 compute ✓new = ✓ old 
˜f
0(✓old )
f˜00(✓old) | {z }
d
7 if ˜f(✓new) < ˜f(✓old) + c · d
˜f
0(✓old) then
8 assign  = /2 (Backtracking)
9 Goto 6
10 assign  = 0, ✓ old = ✓new
11 ✓
⇤ = ✓old
0 2 4 6 8 10 12
0
0.5
1 loop #1
✓old =3.5
˜f(✓old ) =0.21
 =1
f˜0(✓old ) = 0.32
˜f
00(✓old) = 0.64
> 0 !  = 
✓new = ✓ old 
˜f
0(✓old
)
f˜00(✓old
) =4.01
˜f(✓new ) =0.46
˜f( )✓ 0 2 4 6 8 10 12 0.5
0
0.5
˜f
0( )✓ 0 2 4 6 8 10 12 2 1
0
1
✓
˜f00( )✓ 0 2 4 6 8 10 12
0
0.5
1 loop #2
✓old =4.01
˜f(✓old ) =0.46
 =1
f˜0(✓old ) = 0.70
˜f
00(✓old) = 0.64
> 0 !  = 
✓new = ✓ old 
˜f
0(✓old
) ˜f
00(✓old
) =5.11
f˜(✓new ) =0.99
˜f( )✓ 0 2 4 6 8 10 12 0.5
0
0.5
˜f
0( )✓ 0 2 4 6 8 10 12 2 1
0
1
✓
˜f00( )✓ 0 2 4 6 8 10 12
0
0.5
1 loop #3
✓old =5.11
f˜(✓old ) =0.99
 =1
˜f
0(✓old ) = 0.31
f˜00(✓old) = 1.94
< 0 ! OK
✓new = ✓ old 
˜f
0(✓old
) ˜f
00(✓old
) =4.95
f˜(✓new ) =1.02
˜f( )✓ 0 2 4 6 8 10 12 0.5
0
0.5
˜f
0( )✓ 0 2 4 6 8 10 12 2 1
0
1
✓
˜f 00( )✓ Figure 5.8: Example of application of
the Newton-Raphson algorithm with
backtracking line search for finding the maximum of a function.
The Ne wton-Raphson algorithm can be employed for identifying
the optimal values ✓
⇤ = [✓
⇤
1 ✓
⇤
2
· · · ✓
⇤n ]|
in domains having multiple
dimensions. The equation 5.4 developed for univariate cases can be
extended for n-dimensional domains by following
✓ new = ✓ old  H[ ˜f(✓old)]1
· r ˜f(✓old). (5.5)
H[f˜(✓)] denotes the n ⇥ n Hessian matrix containing the second￾order partial derivatives for the function f˜(✓) evaluated at ✓. The
Hessian is a symmetric matrix where each term is defined by
hH[˜f(✓)] i
ij =
@
2 ˜f(✓)
@✓i@✓j =
@
@✓i 
@f˜(✓)
@✓j !
=
@
@✓j 
@
˜f(✓)
@✓i !
.
✓1 ✓2
˜f( ) ✓
Figure 5.9: Example of saddle point ✓ where @
2f˜(✓)
@✓2
1 < 0 and
@
2 ˜f(✓)
@✓ 2
2 > 0.
When the terms on the main diagonal of the Hessian matrix are
positive, it is an indic ation that our lineariz ed gradient points to￾ward a minimum rat he r than a maximum. As we did in algorithm 2,
it is then possible to move toward the maximum by reversing the
search direction. One issue with gradient-based multi-dimensional
optimization is saddle points. Figure 5.9 presents an example of a
saddle point in a function for which one second-order partial deriva￾tive is negat ive

@
2 ˜f(✓)
@✓
2
1 < 0

and the other is positive

@
2 ˜f(✓)
@✓
2
2 > 0

.j.-a. goulet 52
In such a case, one option is to regularize the Hessian matrix by
substracting a constant ↵ on its main diagonal in order to ensure
that all terms are negat ive:
H˜ [f˜(✓)] = H[f˜(✓)]  ↵I. Avoiding being trapped in saddle points is a key challenge in opti- mization. The reader interested in more advanced strategies for that
purpose should consult spe cialize d literature.4 4Nocedal, J. and S. Wright (2006). Nu- merical optimization. Springer Science &
Business Media; Dauphin, Y. N., R. Pas- canu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio (2014). Identifying and
attacking the saddle point problem in high- dimensional non-convex optimization. In Advances in Neural Information Processing
Systems, 27, 2933–2941; and Goodfellow, I., Y. Bengio, and A. Courville (2016). Deep
learning. MIT Press
In pract ic e, second -order methods such as Newton-Raphson can
be employed when we have access to an analytical formulation
for e valuating the Hessian, H[ ˜f (✓)]. Otherwise, if we have to em- ploy numerical derivatives (see §5.4) to estimat e the Hessian, the
computational demand quickly becomes prohibitive as the num- ber of variables increases. For example, if the number of variables
is n = 100, by considering the symmetry in the Hessian matrix, there are 100 + 1
2
(100 ⇥ 100  100) = 5050 second-order partial
derivatives to estimate for each Newton-Raphson iteration. Even
for c omp utationally efficie nt functions ˜f(✓), the ne c essary number
of evaluations becomes a challenge. In addition, for a large n, there
is a substantial e↵ort requ ire d to invert the Hessian in equation
5.5. Therefore, even if Newton-Raphson is more efficient in terms
of the number of iterations req uired to reach convergence, this does
not take into account the time necessary to obtain the second-order
derivatives. When dealing with a large number of variables or for
functions f˜(✓) that are computationally expensive, we may revert
to using momentum-based gradient-ascent methods.
5.3 Coordinate Ascent
One alternative to the methods we have seen for multivariate
optimization is to perform the search for e ach variable separately, which c orre sponds to solving a succession of 1D optimization
problems. This approach is known as coordinate optimization.5 This
5Wright, S. J. (2015). Coordinate descent algorithms. Mathematical Program- ming 151 (1), 3–34; and Friedman, J., T. Hastie, H. Ho¨fling, and R. Tibshirani
(2007). Pathwise coordinate optimization. The Annals of Applied Statistics 1 (2), 302–332
approach is known to perform well when there is no strong coupling
between parameters with respe ct to the values of the objective
function. Algorithm 3 presents a minimal version of the coordinate
ascent Newton-Raphson using backtracking line search. Figure 5.10
presents two intermediate steps illustrat ing the applicat ion of
algorithm 3 to a bidimensional non-convex/non-concave function .probabilistic machine learning for civil engineers 53
Algorithm 3: Coordinate ascent using Newton-Raphson
1 initialize  = 0 = 1, ✓old = ✓0 = [✓1 ✓2 · · · ✓n ]|
2 define ✏, c, ˜f(✓)
3 while ||r✓ ˜f(✓old )|| > ✏ do
4 for i 2 {1 : n} do
5 compute
8
<
:
˜f(✓old) (Function evaluation) ˜f
0i (✓old) (i
th 1
st partial derivative) ˜f
00i (✓old) (i
th 2
nd partial derivative)
6 if f˜00 i (✓old) > 0 then
7  = 
8 compute [✓new]i = [✓old]i
f˜0i (✓old) ˜f
00 i (✓ old) | {z }
d
9 if ˜f(✓new) < ˜f(✓old) + c · d
˜f
0i (✓old) then
10 assign  = /2 (Backtracking)
11 Goto 8
12 assign  = 0, ✓old = ✓new
13 ✓
⇤ = ✓new
5
0
5 5
0
5
loop #5 ! [✓]1 ✓old = [0.41  0.31]|  =1
˜f(✓old) = 3.19
˜f
01 (✓old) = 0.72
˜f
00 1 (✓old) = 0.41
< 0 ! OK
[✓new]1=[✓old ]1 
f˜01
(✓)
f˜00 1
(✓) = 2.13
✓new = [2.13  0.31]| ˜f(✓new) = 3.01
✓2 ✓1
˜f( ) ✓ 5 0 5 5
0
5
✓ 1
✓2
5
0
5 5
0
5
loop #6 ! [✓]2 ✓old = [2.13 0.31]|  =1
˜f(✓old) = 3.01
˜f
02(✓old) = 0.93
˜f
002 (✓old ) = 0.77
< 0 ! OK
[✓new]2=[✓ old]2 
˜f
02
(✓) ˜f
002
(✓) = 0.90
✓old = [2.13 0.90]| ˜f(✓new) = 2.52
✓2 ✓1
˜f( ) ✓ 5 0 5 5
0
5
✓1
✓2 Figure 5.10: Example of application of
the coordinate ascent Newton-Raphson
algorithm with backtracking for finding the maximum of a 2-D function.
5.4 Numerical Derivatives
One aspect that was omitted in the previous sections is How do
we obtain the first- and second-order derivatives? When a twice
di↵erentiable formulation for ˜f (✓) exists, @f˜(✓)
@✓i and @
2f˜(✓)
@✓
2
i can
be expressed analytically. When analytic formulations are not
available, derivatives can be estimated numerically using either
a forward, backward, or central di↵erentiation scheme.6 Here, we
6Nocedal, J. and S. Wright (2006). Nu- merical optimization. Springer Science &
Business Media; and Abramowitz, M. and
I. A. Stegun (1972). Handbook of mathe- matical functions with formulas, graphs, and mathematical table. National Bureau of
Standards, Applied Mathematics
only focus on the central di↵erentiation method. Note that forward
and backward di↵erentiations are not as accurate as central, yet
they are computationally cheaper. As illustrat ed in figure 5.11, first- and secon d-order partial derivatives of f˜(✓) with respect to the i
th
element of a vector ✓ = [✓1 ✓2 · · · ✓n]| are given by
Figure 5.11: Illustration of 1-D numerical
derivatives.
˜f
0i (✓) =
@f˜(✓)
@✓i ⇡
˜f(✓ + I(i)✓)  ˜f(✓  I(i)✓)
2✓
f˜00 i (✓) =
@
˜f(✓)
@✓
2
i ⇡
˜f(✓ + I(i)✓) f˜(✓)
✓ 
˜f(✓)  ˜f(✓  I(i)✓)
✓
✓ =
˜f(✓ + I(i)✓)  2
˜f(✓) + ˜f(✓  I(i)✓)
(✓)2
, where ✓ ⌧ ✓ is a small perturbat ion to the value ✓i and I(i) is a
Examples
I(3) = [0 0 1 0 · · · 0]|
I(1) = [1 0 0 0 · · · 0]|j.-a. goulet 54
n ⇥ 1 indicator vector, for which all values are equal to 0, except the
i
th
, which is equal to one.
Figure 5.12: Illustration of 2-D partial numerical derivatives.
As illustrat ed in figure 5.12, numerical derivatives can also be
employed to estimate each term of the Hessian matrix, hH[ ˜f(✓)]i
ij
⇡
@
˜f(✓+✓)
@✓ j 
@f˜(✓✓)
@✓j
2✓i
, where terms on the numerator are defined as
@
˜f(✓+✓)
@✓j =
˜f(✓+I(i )✓i+I(j)✓j )  ˜f(✓+I(i )✓i I(j)✓j)
2✓j
@
˜f(✓✓)
@✓j =
˜f(✓I(i )✓i+I(j)✓j )  ˜f(✓I(i )✓i I(j)✓j)
2✓j
.
In pract ic e, the full Hessian matrix can only be estimated numer￾ically when the number of variables nis small or when evaluating
˜f(✓) is computationally cheap.
5.5 Parameter-Space Transformation
When optimizing using either the gradient ascent or the Ne wton - Raphson method, we are likely to run into difficulties for parame￾ters ✓ that are not defined in an unbounded space. In such a case,
the efficiency is hindered because the algorithms may propose new
positions ✓new that lie outside the valid domain. When trying to
identify optimal parameters for probability distributions such as
those described in chapter 4, common domains for parameters are
as follows: • Mean parameters: µ 2 R
• Standard deviations:  2 R
+
• Correlation coefficients: ⇢ 2 (1, 1) • Probability: Pr(X = x) 2 (0, 1)
One solution to this problem is to perform the optimization in a
transformed space ✓
tr such that
✓
tr = g(✓) 2 R. For each ✓, the choice of transformation funct ion g(✓) depends on
its domain. Figure 5.13 pre se nts examples of transformations for
✓ 2 R, ✓ 2 R+, and ✓ 2 (a, b). Note that in the simplest case, where
✓ 2 R, no transformation is required, so ✓
tr = ✓.
(a) ✓
tr = ✓
(b) ✓
tr = ln(✓)
(c) ✓
tr =  ln⇣
ba
✓a  1⌘
Figure 5.13: Examples of transformation
functions.probabilistic machine learning for civil engineers 55
For ✓ 2 R
+, a common transformation is to take the logarithm
✓
tr = ln(✓), and its inverse transformation is ✓ = e
✓
tr. The analyti- cal derivatives for the transformation and its inverse are
d✓
tr d✓ =
1
✓
, d✓
d✓
tr = e
✓
tr. For parameters bounded in an interval ✓ 2 (a, b), a possible
transformation is the scaled logistic sigmoid function
✓
tr =  ln ✓
b  a
✓  a  1◆
,
and its inverse is given by
✓ =
b  a
1 + e✓ tr + a. The derivative of the transformation and its inverse are
d✓
d✓
tr =
a  b
(✓  a)(✓  b)
d✓
tr d✓ =
b  a
(1 + e ✓tr )2e✓
tr . Note that the derivative of these transformations will be employed
in chapter 7 when performing paramete r-space transformations
using the change-of-variable rule we have seen in §3.4. The transfor- mations presented here are not unique, as many other functions can
be employed. For further details about parameter space transforma- tions, the reader is invited to refer to Gelman et al.7
7 Gelman, A., J. B. Carlin, H. S. Stern, and
D. B. Rubin (2014). Bayesian data analysis
(3rd ed.). CRC Press6
Learning from Data
Learning context
p(unknown|known)
x: Constant
✓: X ⇠ f(x; ✓)
X: Random variable
y: Observation
When scalars x are employed to model physical phenomena, we
can use empirical observations y to learn about the probability (or
probability density) for each value that x can take. When X is a
random variable, we can use observations to learn about the param- eters ✓ describing its probability density (or mass) function f (x; ✓). This chapter presents the general Bayesian formulation, as well as
approximate methods derived from it, for learning about unknown
state variables and parameters, given known data. The applications
presented in this chapter focus on simple models in order to keep
the attention on the Bayesian methods themselves. We will see in
chapters 8–13 how this theory can be generalized for applications
to machine learning models having complex architectures. For an
in-depth review of Bayesian methods applied to data analysis, the
reader may refer to specialized textbooks such as those by Gelman
et al.1 or Box and Tiao.2 1 Gelman, A., J. B. Carlin, H. S. Stern, and
D. B. Rubin (2014). Bayesian data analysis
(3rd ed.). CRC Press 2Box, G. E. P. and G. C. Tiao (1992). Bayesian inference in statistical analysis. Wiley
6.1 Bayes
Learning from data can be seen as an indirect problem; we observe
some quantities y, and we want to employ this information to infer
knowledge about hidden-state variables x and model parameters Note: The term hidden describes a state variable that is not directly observed. ✓, where y depends on x and ✓ through f(y|x; ✓). In chapter 3, we saw that such a dependence can be tre at ed using conditional
probabilities. Notation: ; and | are both employed
to describe a conditional probability or probability density. The distinction is that
; is employed when ✓ are parameters of a PDF or PMF, for example, f(x; ✓) = N(x; µ, 
2
), and | denotes a conditional
dependence between variables, for example, f(y|x). Besides these semantic distinctions, both are employed in the same way when it comes to performing calculations.
In the first part of this chapter, we solely focus on the estimation
of hidden-state variables x. Given two random variables X ⇠ f (x)
and Y ⇠ f(y), their joint probability density funct ion (PDF) is
obtained by the product of a conditional and a marginal PDF,
f(x, y) = (
f(x|y) · f(y)
f(y|x) · f(x). (6.1)j.-a. goulet 60
In the case where y is known and X is not, we can reorganize the
terms of equation 6.1 in order to obtain Bayes rule,
f(x|y) =
f(y|x) · f(x)
f(y)
,
which describes the posterior PDF (i.e., our posterior knowledge)
of X given that we have observed y. Let us consider X, a random
variable so that X ⇠ f(x), and given a set of observations D =
{y1, y2, · · · , yD} that are realizations of the random variables Y = Note: From now on, the number of ele- ments in a set or the number of variables
in a vector is defined by a typewriter-font upper-case letter; for example, D is the number of observations in a set D, and X
is the number of variables in the vector x = [x1 x2 · · · xX]|
. A set of observations D = {y1, y2 , · · · , yD}
e.g., D = {1.2, 3.8, · · · , 0.22}, (y 2 R)
D = {3, 1, · · · , 6}, (y 2 Z+)
D = {blue, blue, · · · , red}, (y 2 {blue, red, green})
[Y1 Y2 · · · YD]| ⇠ f (y). Our posterior knowledge for X given the
observations D is described by the conditional PDF
f(x|D) | {z }
posterior =
likelihood z }| { f(D|x)·
prior z}|{ f(x)
f(D) | {z }
evidence
.
Prior f (x) describes our prior knowledge for the values x that a
random variable X can take. The prior knowledge can be expressed
in multiple ways. For instance, it can be based on heurist ics such
as expert opinions. In the case where data is obtained sequentially, the posterior knowledge at time t  1 becomes the prior at time t.
In some cases, it also happens that no prior knowledge is available;
then, we should employ a non-informative prior, that is, a prior Note: A uniform prior and a non￾informative prior are not the same
thing. Some non-informative priors are uniform, but not all uniform priors are non-informative.
that reflects an absence of knowledge (see §6.4.1 for further details
on non-informative priors) .
Likelihood f (Y = y|x) ⌘ f (D|x) describes the likelihood, or the
conditional probability density, of observing the event {Y = D}, given the values that x can take. Note that in the special case of an
exact observation where D = y = x, then
f(D|x) =
⇢
1, y = x
0, otherwise.
In such a case, the observations are exact so the prior does not
play a role; by observing y you have all the information about x
because y = x. In the general case where y : Y = fct(x), that is, y is a realization from a stochastic process, which is a function of
x—f(D|x) describes the prior probability density of observing D Here, the term prior refers to our state of
knowledge that has not yet been influenced
by the observations in D. given a specific set of values for x.
Evidence f(Y = y) ⌘ f (D) is called the evidence and consist s in a
normalization constant ensuring that the post er ior PDF integratesprobabilistic machine learning for civil engineers 61
to 1. In §3.3 we saw that for both the discrete and the continuous
cases, X
x p(x|D) | {z }
discrete case
= 1, Z 1
1
f(x|D)dx
| {z }
continuous case
= 1.
For that property to be true, the normalization constant is defined
as the sum/integral over the entire domain of x, of the likelihood
times the prior,
p(D) = X
x p(D|x) · p(x) | {z }
discrete case
, f(D) =
Z
f(D|x) · f(x)dx
| {z }
continuous case
.
So far, the description of learning as an indirect problem is
rather abstract. The next sections will apply this concept to simple
problem configurations involving discrete and continuous random
variables.
6.2 Discrete State Variables
Discrete state variable: A discrete
random variable X employed to represent
the uncertainty regarding the state x of a
system.
This section presents how the probability of occurrence for discrete
state variables are estimated from observations. In all the examples
presented in this section, the uncertainty associated with our
knowledge of a hidden random variable X is epistemic because Reminder: The term hidden refers to a variable that is not observed. a true value xˇ exists, yet it is unknown. Moreover, x is not directly
observable; we can only observe y, where p(y|x) describes the
conditional probability of an observation y given the value of the
discrete hidden-state variable x.
6.2.1 Example: Disease Screening
Entire population
Person without disease Pr(¬disease) =
99%
Person with
disease Pr(disease) = 1%
Pr(test+|disease)=80% Pr(test+|¬disease)=10%
1% ⇥ 80% 99% ⇥ 10%
Figure 6.1: Visual interpretation of Bayes
rule in the context of disease screening. (Adapted from John Henderson (CC BY).)
Figure 6.1 illustrates the context of a rare disease for which the
probability of occurrence in the general population is 1 percent,
and a screening test such that if one has the disease, the prob- ability of testing positive is Pr(test+|disease) = 0.8; and if one
does not have the disease, the probability of testing positive is
Pr(test+|¬disease) = 0.1. The sample space for the variable describ￾ing the hidden state we are interested in is x 2 {disease,¬disease},
for which the prior probability is Pr(X = disease) = 0.01 and
Pr(X = ¬disease) = 1  Pr(X = disease) = 0.99. For a single observat ion, D = {test+}, the posterior probabilityj.-a. goulet 62
of having the disease is
Pr(disease | test+) =
Pr(test+|disease)·Pr(disease)
Pr(test+|disease)·Pr(disease)+Pr(test+|¬disease)·Pr(¬disease) = 1%⇥80%
(1%⇥80%)+(99%⇥10%) = 7.5%,
and the posterior probability of not having the disease is
Pr(¬disease | test+) = 1  Pr(disease | test+) =
Pr(test+|¬disease)·Pr(¬disease)
Pr(test+|disease)·Pr(disease)+Pr(test+|¬disease)·Pr(¬disease) = 99%⇥10%
(1%⇥80%)+(99%⇥10%) = 92.5%.
If we push this example to the extreme case where the disease
is so rare that only one human on Earth has it (see figure 6.2) and
where we have a highly accurate screening test so that
test+ !
(
Pr(test + |disease) = 0.999
Pr(test + |¬disease) = 0.001,
the question is If you test positive, should you be worried? The
answer is no; the disease is so rare that even with the high test
accuracy, if we tested every human on Earth, we would expect
⇡ 0.001 ⇥ (8 ⇥ 10
9
) = 8⇥ 10
6
false positive diagnoses. This example
illustrates how in extreme cases, prior informat ion may outweigh
the knowledge you obtain from observations.
Humans
Only one has the disease
Figure 6.2: The example of a disease so
rare that only one human on Earth has it.
6.2.2 Example: Fire Alarm Note: P
x p(y = |x) does not have to
equal one for the likelihood; however, it
does for the posterior, P
x p(x|y) = 1. It is common to observe that when a fire alarm is triggered in
a public building, people tend to remain calm and ignore the
alarm. Let us explore this situation using condit ional probabili- ties. The sample space for the hidden st at e we are interested in is
x 2 {fire,¬fire} ⌘ { , } and the sample space for the observation
is y 2 {alarm, ¬alarm} ⌘ { , }. If we assume that
p( |x) =
⇢
Pr( | ) = 0.95
Pr( | ) = 0.05,
then the question is Why does no one react when a fire alarm is
triggered? The issue here is that p( |x) describes the probability of
an alarm given the state; in order to answer the question, we need
to calculate Pr( t
| ), which depends on the prior probability that
there was a fire at time t  1 as well as at the transition probabilities
p(xt |xt1), as illustrated in figure 6.3.
t
0.05
0.95
Pr = 0.99
Pr = 0.01
t  1
0.99
0.01
1
Pr =?
Pr =?
Figure 6.3: Fire alarm example.probabilistic machine learning for civil engineers 63
We assume that at t  1, before the alarm is triggered, our prior
knowledge is
p(xt1) =
⇢
Pr( t1) = 0.01
Pr( t1) = 0.99. The transition probability between t  1 and t, at the moment when
the alarm is triggered, is described by the conditional probabilities
p(xt|xt1) =
8
>
<
>
:
Pr( t
| t1) = 0.99
Pr( t
| t1 ) = 0.01
Pr( t
| t1) = 1
Pr( t | t1 ) = 0. By combining the prior knowledge at t  1 with the transition
probabilities, we obtain the joint prior probability of hidden states
at both time steps,
p(xt
, xt1) =
8
>>
>
>
><
>>
>
>
:
Pr( t
, t1) = Pr( t| t1) · Pr( t1) = 1 · 0.01 = 0.01
Pr( t
, t1) = Pr( t| t1) · Pr( t1) = 0.01 · 0.99 = 0.0099
Pr( t
, t1) = Pr( t | t1 ) · Pr( t1) = 0.99 · 0.99 = 0.9801
Pr( t
, t1) = Pr( t | t1) · Pr( t1) = 0 · 0.01 = 0. We obtain the marginal prior probability of each state x at a time t
by marginalizing this joint probability, that is, summing over states
at t  1,
p(xt) =
8
>>
<
>
>:
Pr( t) = Pr( t
, t1) + Pr( t
, t1) = 0.01 + 0.0099 = 0.0199 ⇡ 0.02
Pr( t) = Pr( t
, t1 ) + Pr( t
, t1) = 0.9801 ⇡ 0.98.
If we combine the information from the prior knowledge at t and
the observation, we can compute the posterior probability of the
state xt
, given that the alarm has been triggered,
Pr( t| ) =
Pr( , t ) z }| { Pr( | t ) · Pr( t)
(Pr( | t) · Pr( t)) + (Pr( | t) · Pr( t)) | {z } Pr( ) =
0.95 · 0.02
(0.95 · 0.02) + (0.05 · 0.98) = 0.28
Pr( t
| ) = 1  Pr( t
| ) = 0.72.j.-a. goulet 64
Overall, despite an alarm being triggered, the probability that there
is a fire remains less than 30 percent.
We can explore what would happen at time t + 1 if, after the
alarm has been triggered, a safety official informs us t hat there
is a fire and that we need to evacuate. We assume here that the
conditional probability of receiving such information given the state
is
p( |x) =
⇢
Pr( | ) = 0.95
Pr( | ) = 0.05, as illustrated in figure 6.4. Here we assume that given the state, the
probability of receiving the information from the safety official is
conditionally independent from the alarm, that is, Pr( | , ) =
Pr( | ). If, as we did for the estimation at time t, we combine
the information from the prior knowledge at time t, the transition
probabilities, and t he observation at time t+ 1, we can compute t he
posterior probability of the hidden stat e x t+1, given that the alarm
has been triggered and that we received the information from a
safety official. First, the marginal prior probabilit ies of each state at
time t + 1 are estimat e d from the posterior probability at t combined
with the transition probabilities so that
Pr( t+1| ) =
Pr( t+1, t| ) z }| { Pr( t+1 | t ) · Pr( t| ) +
Pr( t+1, t| ) z }| { Pr( t+1 | t) · Pr( t| ) = 0.01 · 0.72 + 1 · 0.28 = 0.29
Pr( t+1| ) = 1  Pr( t+1| ) = 0.71.
t
0.05
0.95
Pr = 0.99
Pr = 0.01
t  1
0.99
0.01
1
Pr = 0.72
Pr = 0.28
t + 1
0.99
1
0.01
0.05
0.95
Pr = ?
Pr = ?
Figure 6.4: Example illustrating the role of
the prior in sequential data acquisition and
interpretation.
Then, the posterior probability at t + 1 is estimated following
Pr( t+1| , ) =
Pr( , t+1| ) z }| { Pr( | t+1) · Pr( t+1| )
(Pr( | t+1)·Pr( t+1| ))+(Pr( | t+1)·Pr( t+1| )) | {z } Pr( | ) =
0.95 · 0.29
(0.95 · 0.29) + (0.05 · 0.71) = 0.89
Pr( t
| ) = 1  Pr( t
| ) = 0.11. This example illustrates how when knowledge about a hidden
state evolves over time as new information becomes available, the
posterior at t is employed to estimate the prior at t + 1.probabilistic machine learning for civil engineers 65
6.2.3 Example: Post-Earthquake Damage Assessment
Given an earthquake of intensity
3 3This example is adapted from Armen Der Kiureghian’s CE229 course at UC Berkeley. X : x 2 {light (L), moderate (M), important (I)}
and a structure that can be in a state (e.g., see figure 6.5)
Y : y 2 {damaged ( D ) , undamaged (D)}. These two random variables are illustrated using a Venn diagram in
figure 6.6. The likelihood of damage given each earthquake intensity
is
p(D|x) = 8
<
:
Pr(Y = D|x = L) = 0.01
Pr(Y = D|x = M) = 0.10
Pr(Y = D|x = I) = 0.60,
Figure 6.5: Extensive damage after San Francisco’s 1906 earthquake.
Figure 6.6: Venn diagram illustrating
the sample space where the earthquake
intensities light (L), moderate (M), and
important (I) are mutually exclusive and
collectively exhaustive events. The post- earthquake structural states of damaged
(D) and undamaged (D) intersect the
earthquake intensity events.
and the prior probability of each intensity is given by
Note: For the likelihood P
x p(D|x) =6 1;
however, for the prior P
x p(x) = 1, and for
the posterior P
x p(x|D) = 1.
p(x) = 8
<
:
Pr(X = L) = 0.90
Pr(X = M) = 0.08
Pr(X = I) = 0.02. We are interested in inferring the intensity of an earthquake given
that we have observed damaged buildings y = D following an event,
so that
p(x|D) =
p(D|x) · p(x)
p(D)
. The first step to solve this problem is t o compute the normalization
constant,
p(y = D) =
Pr(Y =D,X=L) z }| { p(D|L) · p(L) +
Pr(Y =D,X=M) z }| { p(D|M) · p(M) +
Pr(Y =D,X=I) z }| { p(D|I) · p(I) = 0.01 · 0.90 + 0.10 · 0.08 + 0.6 · 0.02 = 0.029. Then, the posterior probability of each state is calculat e d following
Pr(x = I|y = D) =
p(D|I)·p(I)
p(D) = 0.60·0.02
0.029 = 0.41
Pr(x = M|y = D) =
p(D|M)·p(M)
p(D) = 0.10·0.08
0.029 = 0.28
Pr(x = L|y = D) =
p(D|L)·p(L)
p(D) = 0.01·0.90
0.029 = 0.31. We see that despite the prior probability of a high-intensity earth- quake being small (i.e., Pr(X = I) = 0.02), the posterior is signifi- cantly higher because of the observation indicat ing the presence of
structural damage.j.-a. goulet 66
6.3 Continuous State Variables
This section presents examples of applications of hidden-state
estimation for continuous cases. Compared with discret e cases, continuous ones involve more abstract concepts in t he definition
of the likelihood f (D|x) and evidence f(D). Like for the discrete
case, if our model is y = x and we observe y, then we know all
there is to know about x so the conditional probability of y given
x is nonzero only for the true value xˇ. In this section, we will cover
cases involving imperfect observations rather than perfect ones.
6.3.1 Likelihood: f(D|x)
In practice, we are interested in the case where xˇ is a true yet un- known hidden-state variable and where we have access to imprecise
observations yi
, where the observation model is either
yi = xˇ + vi (direct observation)
yi > xˇ + vi (upper-bound censored observation)
yi < xˇ + vi (lower-bound censored observation), where in all three cases the observation error, vi
: Vi ⇠ N(v; 0, 
2
V
), Vi ? Vj , 8i =6 j. A direct observation corresponds to the standard Note: Vi ? Vj , 8i 6= j indicates that the
random variable Vi is independent of Vj ,
for all i =6 j (i.e., that all observation errors are independent of each other).
case where the observation is the true stat e xˇ on which an observa- tion error v is added. V is a Normal random variable employed to
describe observation errors. V is often chosen to be Normal in order
to simplify the problems by maintaining analyt ical tractability for
linear models (see §4.1.3). Lower- and upper-bound observations are
called censored observations where we only know that the hidden- state variable is either respectively great e r or smaller than the
observed value y.
Direct observations For a direct observation y, the likelihood is
formulated from the additive observation model so that the PDF of
Y is described by the Normal PDF Y ⇠ N (y; x, 
2
V
). Because xˇ is
Y = x + V
Observation: N(y; x, 
2V
)
Hidden state: constant
Measurement error: N(v; 0,  2V
)
unknown, the likelihood denoted by f (y|x) or L(x|y) describes the
prior probability density of observing {Y = y} given {xˇ = x},
f(y|x) = L(x|y) =
1
p
2⇡ · V
exp 

1
2
✓
y  x
V
◆2!
. (6.2)
The leftmost surface in figure 6.7 corresponds to equation 6.2
plotted with respect to both x and y, for V = 1. The pink dashed
curve represents the explicit evaluation of f (y|x = 0), where the
evaluation at x = 0 is arbitrarily chosen for illustration purpose,probabilistic machine learning for civil engineers 67
and the red solid curve reverses the relat ionship to represent the
likelihood of the observation y = 1 for any value x, where f (y =
1|x) = L(x|y = 1).
5
0
5 5
0
5
y x
f y x ( | )
y =-1.0|x =0.0
5 0 5
y
f y x ( | = 0)
5 0 5 x
L | (x y =
1)
Figure 6.7: Representation of the likelihood
function for a direct observation. The
leftmost surface describes f (y|x), the
rightmost graph describes f(y = 1|x) =
L(x|y = 1), and the center graph
describes f(y|x = 0).
Keep in mind that the likelihood is not a PDF with respect to
x so it does not integrate to 1. Accordingly, the rightmost graph in
figure 6.7 is a function quantifying for each value x how likely it is
to observe the value y = 1. Upper-bound observations For an upper-bound observation y > xˇ+ v, the likelihood is described by the cumulative distribution function
(CDF) of Y ⇠ N (y;x, 
2
V
),
f(y|x) =
CDFN(y;x,
2V
) z }| { Z y1
1
p
2⇡V
exp 

1
2
✓
y
0  x
V
◆2!
dy
0
=
1
2 +
1
2
erf✓
y  x
p
2V
◆
.
(6.3)
The surface in figure 6.8 corresponds to equat ion 6.3 plotted with
respect to x and y, for V = 1. The pink dashed curve represents
the explicit evaluation of f(y|x = 0), where t he evaluation at
x = 0 is arbit r arily chosen for illustration purpose, and the red solid
curve reverses the relationship to represent the likelihood of the
observation y = 1, given any value x, L(x|y = 1).
5
0
5 5
0
5
y x
f y x ( | )
y =-1.0|x =0.0
5 0 5
y
f y x ( | = 0)
5 0 5 x
L | (x y = 1)
Figure 6.8: Representation of the likelihood
function for an upper-bound censored
observation. The leftmost surface describes f(y|x), the rightmost graph describes f(y = 1|x) = L(x|y = 1), and the center graph describes f(y|x = 0).j.-a. goulet 68
Lower-bound observations For a lower-bound observation y < xˇ + v, the likelihood is described by the complement of the cumulative
distribution function of Y ⇠ N (y; x, 
2
V
),
f(y|x) = 1 
CDFN(y;x,
2V
) z }| { Z y1
1
p
2⇡V
exp 

1
2
✓
y
0  x
V
◆2!
dy
0
= 1 
✓
1
2 +
1
2
erf ✓
y  x
p
2V
◆◆
.
(6.4)
The surface in figure 6.9a corresponds to equat ion 6.4 plotted with
respect to x and y, for V = 1. The pink dashed curve represents
the explicit evaluation of f(y|x = 0), where the evaluation at x = 0
is arbitrarily chosen for illustration purpose, and again the red
curve reverses the relationship to represent the likelihood of the
observation y = 1, given any value x, L(x|y = 1). Note that
in figure 6.9b, if instead of having V = 1, V tends to zero, the
smooth transition from low likelihood values to high ones would
become a sharp jump from zero to one.
5
0
5 5
0
5
y x
f y x ( | )
y =-1.0|x =0.0
5 0 5
y
f y x ( | = 0)
5 0 5 x
L | (x y =
1)
(a) V = 1
5
0
5 5
0
5
y x
f y x ( | )
y =-1.0|x =0.0
5 0 5
y
f y x ( | )
5 0 5 x
L | (x y)
(b) V = 0
Figure 6.9: Representation of the likelihood
function for a lower-bound censored
observation. The leftmost surface describes f(y|x), the rightmost graph describes f(y = 1|x) = L(x|y = 1), and the center graph describes f(y|x = 0).
Note: The notion of conditional inde- pendence implies that given x, Yi
is
independent of Yj , that is, Yi ? Yj |x , f(yi
|yj
, x) = f(yi
|x),
so that f (yi , yj |x) = f(yi
|x) · f(yj |x). This assumption is often satisfied because the observation model follows the form
y = x + v, where v is the realization of an observation
error so that Vi ? Vj , 8i 6= j.
Multiple observations All cases above dealt with a single observa- tion. When multiple observations are available, one must define the
joint likelihood for all observations given x. With the assumption
that observations are conditionally independent given x , the jointprobabilistic machine learning for civil engineers 69
PDF is obtained from the product of marginals,
f(D|x) = f(Y1 = y1, · · · , YD = yD|x)
=
D1 ! z }| { Y
D
i=1
f(Yi = yi
|x)
= exp X
D
i=1
ln
f(Yi = yi|x)!
.
(6.5)
Note that practical difficulties related to numerical underflow
or overflow arise when trying to evaluate the product of several
marginal likelihoods f(yi|x). This is why in equation 6.5, the prod- uct of the marginals is replaced with the equivalent yet numerically
more robust formulation using the exponential of the sum of the log
marginals.
6.3.2 Evidence: f (D)
5 0 5
0
0.1
x
f
y
x
f
x ( | ) ( )
(a) S = 7, xs =
10
7
, fˆ(D) = 0.2158
5 0 5
0
0.1
x
f
y
x
f
x ( | ) ( )
(b) S = 10, xs = 10
10
, fˆ(D) = 0.2198
5 0 5
0
0.1
x
f
y
x
f
x ( | ) ( )
(c) S = 20, xs = 10
20
, fˆ(D) = 0.2197
5 0 5
0
0.1
f
y
x
f
x ( | ) ( )
(d) S = 100, x s =
10
100
, ˆf(D) = 0.2197
Figure 6.10: Examples of application of the
rectangle integration rule.
The evidence acts as a normalization constant obtained by inte￾grating the product of the likelihood and the prior over all possible
values of x,
f(D) =
Z
f(D|x) · f(x)dx.
This integral is typically difficult to evaluate because it is not ana￾lytically tractable. Figure 6.10 presents a naive way of approaching
this integral using the rectangle rule, where the domain containing
a significant probability content is subdivided in S bins of width
xs in order to approximate
ˆf(D) =X
S
s=1
f(D|xs) · f( xs)xs .
This approach is reasonable for trivial 1-D problems; however, it
becomes computationally intractable for problems in larger dimen- sions (i.e., for x 2 R
X) because the number of grid combinations t o
evaluate increases exponentially with the number of dimensions X
to integrate on. Section 6.5 presents an introduct ion to sampling
methods that allow overcoming this challenge. Chapt e r 7 w ill intro- duce more advanced sampling methods allowing the estimation of
the normalization const ants for high-dimensional domains.j.-a. goulet 70
6.3.3 Posterior: f(x|D)
The posterior is the product of the likelihood function times the
prior PDF divided by the normalization constant,
f(x|D) =
f(D|x) · f(x)
f(D)
. Let us consider two direct observations D = {y1 = 1, y2 = 0}
obtained from the observat ion model
yi = x + vi
, vi
: Vi ⇠ N (v; 0, 1
2
), Vi ? Vj
, 8i 6= j. The likelihood is thus given by f(D|x) = N (y1 ;x, 1
2
) · N (y2; x, 1
2
). We assume the prior PDF follows a uniform PDF within the range
from -5 to 5, f (x) = U(x;5, 5). As presented in figure 6.10, the
normalization constant is estimated using the rectangle integration
rule to fˆ(D) = 0.2197. Figure 6.11 presents the prior, the likelihood, and the posterior for this example.
5 0 5
0
0.1
0.2
x
f x (D| )
Likelihood
5 0 5
0
0.1
0.2
x
f x( )
Prior
5 0 5
0
0.2
0.4
0.6
x
f x( |D)
Posterior Figure 6.11: Example of prior, likelihood, and posterior.
6.3.4 Number of Observations and Identifiability
In the extreme case where no observation is available (i.e., D = ;), the posterior is equal to the prior f (x|;) = f (x). In order to
explore what happens at the other end of the spect rum where an
infinite amount of data is available, it is required to distinguish
between identifiable and non-identifiable problems. A problem is
identifiable when, given an infinite number of direct observations, we are theoretically able to retrieve the true hidden state of a
deterministic system xˇ. For non-identifiable problems, even an
infinite number of direct observations does not allow retrieving the
true hidden state of the system xˇ, because multiple equally valid
solutions exist.
Figure 6.12: Example of two streams merging in a single river. (Photo: Ashwini Chaudhary)
Example: Non-identifiable problem Take the example illustrate d
in figure 6.12, where we want to estimate the contaminant concen- tration in two streams x1 and x2 and where we only have access to
contaminant concentration observations
yi = x1 + x2 + vi
, vi
: Vi ⇠ N (v; 0, 2
2
), Vi ? Vj , 8i 6= j,probabilistic machine learning for civil engineers 71
which are collected at the output of a river after both streams
have merged. We assume that our prior knowledge is uniform over
positive concentration values, that is, f(x1, x2) / 1. The resulting
posteriors are presented in figure 6.13 for D = 1 and D = 100
observations, along with the true values marked by a red cross. This problem is intrinsically non-identifiable because, even when
the epistemic uncertainty has been removed by a large number
of observations, an infinite number of combinations of x1 and x2
remain possible. 0 10 20 30 40
0
10
20
30
40
x1
x2
(a) D = 1, f (x1, x2) / 1
0 10 20 30 40
0
10
20
30
40
x1
x2 (b) D = 100, f (x1, x2 ) / 1
Figure 6.13: Posterior contours for an
example of non-identifiable problem.
Non-identifiable problems can be regularized using prior knowl- edge. If, for example, we know that the prior probability for the
contaminant concentration in the first stream follows p(x1) = N (x1; 10,5
2
), the posterior will now have a single mode, as pre- sented in figure 6.14.
0 10 20 30 40
0
10
20
30
40
x1
x2
Figure 6.14: Posterior contours for an
example of non-identifiable problem for D = 1 and which is constrained by the prior knowledge f(x1, x2) / N(x 1; 10, 5
2
).
Well-constrained identifiable problems For well-constrained prob￾lems where a true value xˇ exists, given an infinite number of direct
observations (D ! 1) that are conditionally independent given x, the posterior tends to a Dirac delt a function, which is nonzero only
for x = xˇ,
f(x|D) =
f(D|x) · f(x)
f(D) ! (x 
true value z}|{ xˇ ) | {z } Dirac delta function
.
Figure 6.15: Example of posterior PDF
tending toward the Dirac delta function.
Figure 6.15 presents an example of such a case where the posterior
has collapsed over a single value. The posterior expected value then
corresponds to the true value
E[X|D] ! xˇ
|{z}
true value
,
and the posterior variance tends to zero because no epistemic
uncertainty remains about the value of x,
var[X|D] ! 0
|{z} xˇ: constant
.
Note that in most practical cases we do not have access to an
infinite number of observations. Moreover, keep in mind that the
less data we have access to, the more important it is to consider the
uncertainty in the posterior f(x|D).
6.4 Parameter Estimation
In this section, we will explore how to ext end Bayesian estimation
for the parameters ✓ defining the probability density function ofj.-a. goulet 72
a hidden random variable x : X ⇠ f(x) ⌘ f(x; ✓). This setup is
relevant when the system responses are st ochastic. For example, even in the absence of observation errors, di↵erent samples from
a concrete batch made from the same mix of components would
have di↵erent resistances due to the uncertainty associated with the
intrinsic heterogeneity of the material. The post e rior probability
Notation
xi
: X ⇠ fX (x; ✓)
✓: PDF’s parameters {x1, x2, · · · , xD}: realizations of the hidden
variable X
density function sought is now
f(✓|D) =
f(D|✓) · f(✓)
f(D)
, where f(✓) is the prior PDF of unknown parameters, f(D|✓) is
the likelihood of observations D = {y1, · · · , yD}, and f(✓|D) is the
posterior PDF for unknown parameters ✓. After having estimated
the posterior PDF for parameters f(✓|D), we can marginalize this
uncertainty in f(x; ✓) in order to obtain the posterior predictive
PDF for the hidden variable X,
f(x|D) =
Z
f(x; ✓) · f(✓|D)d✓. (6.6)
Parameters of
Random variable
Figure 6.16: Example of Bayesian parame- ter estimation.
Figure 6.16 presents an example where we want t o est imat e the
joint posterior probability of the mean µ X and standard deviation
X defining the PDF of X, which describes the variability in t he
resistance of a given concrete mix. Here, the concret e resistance
X is assumed to be a random variable. It means that unlike in
previous sections, no true value exists for the resistance because it
is now a stochastic quantity. Despite this, true values can still exist
for the parameters of f(x;✓), that is, {µˇX , ˇX }. This example will
be further explored in §6.5.3.
6.4.1 Prior: f(✓)
f (✓) describes our prior knowledge for the values that ✓ can
take. Prior knowledge can be based on heurist ics (expert knowl- edge), on the posterior PDF obtained from previous data, or on
a non-informative prior (i.e., absence of prior knowledge). A non￾Non-informative prior ✓ = {µX , X }, N(x; µX, 
2X )
f(µX ) / 1
f(X ) / 1X
f(µX , X ) / 1X
, if µX ?  X
informative prior for parameters such as the mean of a Normal
PDF is described by a uniform density f(µ) / 1. For standard
deviations, the non-informative prior typically assumes that all
orders of magnitudes for  have equal probabilities. This hypoth- esis implies that f (ln) / 1. By using the change of variable rule
presented in §3.4, it leads to f () /
1
, because the derivative of
this transformation is d ln 
d =
1
. Note that the two non-informat ive
priors described for µ and  are improper because R f(✓)d✓ = 1, which does not respect the fundamental requirement that a PDFprobabilistic machine learning for civil engineers 73
integrates to 1, as presented in §3.3. For this reason we cannot draw
samples from an improper prior. Nevertheless, when used as a prior, an improper PDF can yield to a proper post e rior (see §7.5).
6.4.2 Likelihood: f(D|✓)
f(D|✓) ⌘ f(Y = y|✓) ⌘ f(y|✓) ⌘ L(✓|y) are all equivalent ways of
describing the likelihood of observing specific values y, given ✓. In
the special case where y = x so that realizations of x : X ⇠ f (x; ✓)
are directly observed, the likelihood is
f(y|✓) = f(y; ✓) = f(x; ✓).
In a more general case where observations D = {yi
, 8i 2 {1 : D}}
are realizations of an additive observation model yi = xi + vi
, where
vi : V ⇠ N (v; 0, 
2
V
) and xi : X ⇠ N ( x; µX , 
2
X | {z }
✓
), the likelihood is
Y = X + V
Observation: N(y; µX , 
2X + 
2V
)
Hidden-state R.V.: N(x; µX, 
2X )
Measurement error: N(v; 0, 2V
) f(y|✓) = N (y; µX, 
2
X + 
2
V
). This choice of an additive observation model using normal random
variables allows having an analytically tractable formulation for
the likelihood. If we want to employ a multiplicative observation
model, y = x · v, then choosing v : V ⇠ ln N (v; 0, 
2
lnV
) and
x : X ⇠ ln N (x;µln X , 
2
ln X | {z }
✓
) as lognormal random variables also
preserves the analytical tractability for the likelihood that follows
f(y|✓) = ln N( y;µln X, 
2
ln X + 
2
lnV ).
6.4.3 Posterior PDF: f(✓|D)
In §6.3.4, when the number of independent observations D ! 1, Note: The term independent observations
is employed as a shortcut for the more
rigorous term that should be in this case conditionally independent given x. we saw that the posterior for X tends to a Dirac delta function
f (x|D) ! (x  ˇx). The situation is di↵erent for the est imat ion of
parameters describing the PDF of a hidden random variable (R.V.);
when D ! 1, it is now the posterior PDF for ✓ that tends to a
Dirac delta function as presented in figure 6.17,
Figure 6.17: The posterior PDF for pa- rameters reduces to a Dirac delta function when the number of independent observa- tions tends to infinity.
f(✓|D) =
f(D|✓) · f(✓)
f(D) ! (✓ 
true value z}|{ ˇ✓ ) | {z } Dirac delta function
,
where the posterior expected value tends toward the true parameter
value and the variance of the post er ior tends to zero, E[✓|D] ! ✓ˇ
|{z}
true valuej.-a. goulet 74
var[✓|D] ! 0
|{z}
ˇ✓ : constant
.
In such a situation, the epistemic uncert ainty associated with
parameter values ✓ vanishes and the uncertainty about X reaches
its irreducible level, f(x|D) ! f(x; ˇ✓).
In practical cases where the number of observations D < 1, uncertainty remains about the parameters f(✓|D), so f (x|D) is typ￾ically more di↵used than f (x; ✓ˇ). The uncertainty associated with
parameters f (✓|D) can be marginalized as described in equation 6.6
in order to obtain the posterior predictive PDF for t he hidden ran- dom variable. This integral is typically not analytically t r act able, so it is common to resort to the Monte Carlo sampling methods
presented in the next section in order to compute the posterior
predictive expected value and variance for f(x|D).
6.5 Monte Carlo
Monte Carlo sampling is a numerical integration method that
allows performing Bayesian estimation for analyt ically intractable
cases. In this section, we will first see how Monte Carlo can be
employed as an integration method. Then, we will explore how this
method fits in the context of Bayesian estimation.
6.5.1 Monte Carlo Integration
We employ the example of a unit diameter circle wit h area a =
xr
2 = 0.785 to demonstrate how we can approximate this area
with Monte Carlo numerical integration. A circle of diamet er d = 1
(r = 0.5) is described by the equation
(x  0.5) 2 + (y  0.5)2 = r
2
. This equation is employed in the definition of the indicator function
I(x, y) =
⇢
1 if (x  0.5)2 + (y  0.5)2  r
2
0 otherwise.
0 0.5 1
0
0.5
1
x
y (a) S = 100, ˆE[I(X, Y )] = 0.800
0 0.5 1
0
0.5
1
x
y
(b) S = 1000,Eˆ[I(X, Y )] = 0.759
0 0.5 1
0
0.5
1
x
y
(c) S = 10000, ˆE[I(X, Y )] = 0.785
Figure 6.18: Examples of Monte Carlo numerical integration for which the theoret￾ical answer is a = ⇡r
2 = 0.785.
We then define a bivariate uniform probability density in the range
{x, y} 2 (0, 1), where fXY (x, y) = 1. The circle area a can be
formulated as the integral of the product of the indicator function
times fXY (x, y),
a
|{z}
area =
Z
y
Z
x
I(x, y) · fXY (x, y)dxdy = E[I(X, Y )].
(6.7)probabilistic machine learning for civil engineers 75
We saw in §3.3.5 that the integral in equation 6.7 corresponds to
the expectation of the indicator function I (X, Y ). Given S ran- dom samples {xs, ys}, s 2 {1, 2, · · · , S} generated from a uniform
probability density fXY (x, y), the expectation operation is defined
as
a = E[I(X, Y )] = lim
S!1
1
SX
S
s=1
I(xs, ys).
In practice, because the number of samples S is finite, our estima- tion will remain an approximation of the expectation,
a ⇡ Eˆ[I(X, Y )] =
1
SX
S
s=1
I(xs , ys ).
Figure 6.18 presents realizations for S = {10
2
, 10
3
, 10
4} samples as
Note: The hat in Eˆ[I(X, Y )] denotes
that the quantity is an approximation of
E[I(X, Y )].
well as their approximation of the area a. As the number of samples
increases, the approximation error decreases and Eˆ[I (X, Y )] tends
to the true value of a = ⇡r
2 = 0.785. The interesting property of Monte Carlo integration is that
the estimation quality, measured by the variance varhEˆ[I(X, Y )]i
, depends on the number of samples and not on the number of
dimensions.4 This property is key for the application of the Monte 4 MacKay, D. J. C. (1998). Introduction
to Monte Carlo methods. In Learning in
graphical models, pp. 175–204. Sprigner Carlo method in high-dimensional domains.
6.5.2 Monte Carlo Sampling: Continuous State Variables
Monte Carlo sampling
h(x): Sampling PDF
xs : X ⇠ h(x) s 2 {1, 2, · · · , S}
Monte Carlo methods are now applied to the Bayesian estimation
context presented in §6.3 where we are interested in estimating
continuous state variables. The Monte Carlo integration method is
first employed for approximating the evidence
f(D) =
Z
f(D|x) · f(x)dx = E[f(D|X)]. A Monte Carlo approximation of the evidence can be computed
using importance sampling where
ˆf(D) =
1
S X
S
s=1
f(D|xs) · f( xs)
h(xs)
, xs : X ⇠ h(x) : Sampling PDF,
where samples xs : X ⇠ h(x) are drawn from any sampling
distribution such that h(x) > 0 for any x for which the PDF on the
numerator f (D|x) · f(x) > 0. If we choose the prior PDF as sampling
distribution, h(x) = f(x), the formulation simplifies to
ˆf(D) =
1
S X
S
s=1
f(D|xs), xs : X ⇠ h(x) = f(x)j.-a. goulet 76
because the prior and the sampling distribution cancel out.
The performance of the Monte Carlo integration depends on
how close our sampling distribution h(x) is to the posterior f (x|D). Note that sampling from the prior, h(x) = f(x), is often not a
viable option when the number of observations D is large. This is be- cause in such a case, the likelihood is the product of several terms
so the probability content typically ends up being concentrated in a
small region that is not well described by the prior, as illustrated in
figure 6.19. Figure 6.19: Example of difficult situation
encountered when trying to sample from
the prior.
The posterior mean and variance can be approximated with
Monte Carlo integration using
E[X|D] =
Z x · f( x|D)dx
⇡
1
S X
S
s=1
xs · f(D|xs) · f( xs)
f(D)
· 1
h(xs)
, xs : X ⇠ h(x)
⇡
1
S X
S
s=1
xs · f(D|xs)
f(D)
, xs : X ⇠ h(x) = f(x)
and
var[X|D] =
Z
(x  E[X|D])2
· f(x|D)dx
⇡
1
S  1 X
S
s=1
(xs  E[X|D])2
· f(D|xs) · f( xs)
f(D)
· 1
h(xs)
, xs : X ⇠ h(x)
⇡
1
S  1 X
S
s=1
(xs  E[X|D])2
· f(D|xs)
f(D)
, xs : X ⇠ h(x) = f(x).
For both the posterior mean and variance, when choosing the prior
as the sampling distribution, h(x) = f(x), it cancels out from the
formulation.
Figure 6.20: Example of Bayesian estima- tion with censored observations. (Photo: Elevate)
Example: Concentration estimation Figure 6.20 depicts a labora- tory test for measuring a deterministic mercury concentration xˇ. Observations follow the observation model
⇢
yi = xˇ + vi
, if xˇ + vi  10µg/L
yi = error, if xˇ + vi < 10µg/L, where the measuring device has a minimum threshold and the
mutually independent observation errors are described by
vi
: Vi ⇠ N (v; 0, 2
2
), Vi ? Vj
, 8i 6= j. We want to estimate the posterior PDF describing t he concentra- tion if the observation obtained is D = {error} for two hypothesesprobabilistic machine learning for civil engineers 77
regarding the prior knowledge: (1) uniform in the interval (0,50),
f (x) = U (x; 0, 50), and (2) a normal PDF with mean 25 and stan- dard deviation 10, f(x) = N (x; 25, 10
2
). Figure 6.21 presents the
results for the (a) uniform prior and ( b) normal prior. Note that
only 100, out of the 10
5 Monte Carlo samples that were employed
to estimate the evidence f (D), are displayed. This example illus- trates how information can be extracted from censored observations. Note also how the choice of prior influences the posterior knowledge. The choice of prior is particularly import ant here because limited
information is carried by the censored observations.
0 25 50
0.0
0.1
0.2
x
f
x( )
0 25 50
0.0
0.5
1.0
x
f
x (D| )
0 25 50 0.0
0.1
x
f
x( |D)
(a) Uniform prior, U(0, 50), f(D) ⇡ 0.20, E[x|D] = 5.2, var[x|D] = 10.4
0 25 50
0.0
0.1
0.2
x
f
x( )
0 25 50
0.0
0.5
1.0
x
f
x (D| )
0 25 50
0.0
0.2
x
f
x( |D)
(b) Normal prior, N(25, 10
2), f(D) ⇡ 0.07, E[x|D] = 6.2, var[x|D] = 18.0
Figure 6.21: Comparative example of a uniform and normal prior PDF for performing Bayesian estimation with
censored observations.
We consider a di↵erent context where we employ D = 100 direct
observations, D = {25.4,23.6,24.1, · · · , 24.3}1⇥100, and the prior is
p(x) = U(x; 0,50). The prior, likelihood, and posterior are presented
in figure 6.22. Note that here, out of the 100 Monte Carlo samples
displayed, none lie in the high probability region for the likelihood. As a result, it leads to a poor approximation of the evidence where
the true value is f (D) = 1.19 ⇥ 1078 and where the approximation
0 25 50
0.0
0.1
0.2
x
f
x( )
0 25 50
x
f
x (D| )
0 25 50
x
f
x( |D)
Figure 6.22: Example where drawing
Monte Carlo samples from the prior leads
to a poor estimate of the posterior because no sample falls in the high probability
region.j.-a. goulet 78
is ˆf(D) =
1
S X
S
s=1
0
@
1
Y
00
j=1
f(y j|xs)1
A = 1.95 ⇥ 1083
. Although it is possible to solve this problem by increasing the
number of samples, this approach is computationally inefficient and
even becomes impossible for high-dimensional domains. Chapter 7
presents advanced Monte Carlo methods for solving this limitation.
6.5.3 Monte Carlo Sampling: Parameter Esti mati on
For D observations D = {y1, y2, · · · , yD}, and samples ✓s ⇠ h(✓) =
f (✓), s 2 {1, 2, · · · , S} drawn from the prior PDF of parameters, we
can estimate the evidence
ˆf(D) =
1
S X
S
s=1
0
@Y
D
j=1
f(yj|✓s)1
A (6.8)
as well as t he posterior expected values and variance for parameters
Eˆ[✓|D] =
1
S X
S
s=1
 
✓s · QD
j=1 f(yj |✓s)
f(D)
!
cˆ o v ( ✓|D) =
1
S  1 X
S
s=1
 
(✓s  E[✓|D])(✓s  E[✓|D])|
· QD
j=1 f(yj |✓s)
f(D)
!
.
Figure 6.23: Example of samples taken
from the same concrete batch.
Example: Concrete strength We consider the case where we want
to characterize the resistance R of a given concrete batch, where
the intrinsic variability is estimated by testing the resistance from
several samples, as depicte d in figure 6.23. We assume that the true
yet unknown parameter values are µˇR = 42 MPa and ˇR = 2 MPa
and that the resistance R is described by a log-normal random
variable
R ⇠ ln N (r; µˇln R , ˇln R ) MPa. The unknown parameters we want to learn about are ✓ = [µR R ]|
. Here, R is a hidden random variable because it is not directly
observed; only imprecise observations yi are available. The set of
observations D = {y1 , y2, · · · , yD} is employed to estimate the
posterior PDF for the parameters ✓ that are controlling the PDF of
the hidden random variable X,
Posterior PDF
z }| { f(µR, R|D) =
Likelihood z }| { f(D|µR , R)·
Prior PDF
z }| { f(µR, R )
f(D)
| {z }
Normalization constant
.probabilistic machine learning for civil engineers 79
The joint prior knowledge for parameters f (µR, R ) =f (µR) · f( R), (µR ? R) is obtained from the product of the marginal PDFs,
f(µR ) = ln N(µR; µlnµR
, lnµ R
) | {z } µµR = 45 MPa µR = 7 MPa
, f(R) = ln N(R ; µlnR
, lnR
). | {z } µR = 5 MPa R = 2 MPa The prior PDF for µR and R are themselves described by log- normal PDFs with respective mean {µµ R
, µR} and variance
{
2
µR
, 
2R
}.
20
40
60
0
5
10
µR
R
f µ( R, R)
{µˇR , ˇR}
20
40
60
0
5
10
µR
R
f µ (D| R
, R
)
20
40
60
0
5
10
µR
R
f µ( R, R|D)
10 30 50 70
r
f r( |D)
Predictive – f (r|D) Reality – f (r; µˇR, ˇR)
(a) D = 1
20
40
60
0
5
10
µR
R
f µ( R
, R
)
20
40
60
0
5
10
µR
R
f µ (D| R, R)
20
40
60
0
5
10
µR
R
f µ( R
, R
|D)
10 30 50 70
r
f r( |D)
(b) D = 5
20
40
60
0
5
10
µR
R
f µ( R
, 
R
)
20
40
60
0
5
10
µR
R
f µ (D| R, 
R)
20
40
60
0
5
10
µR
R
f µ( R, 
R|D)
10 30 50 70
r
f r( |D)
(c) D = 100
Figure 6.24: Examples of Bayesian esti- mation as a function of the number of observations employed, D = {1, 5, 100}.
The observation model is ln yi = ln ri + vi
, where the observation
errors vi
: V ⇠ N (v; 0,0.01
2
) MPa. With these assumptions
regarding the observation model, the marginal likelihood for each
observation is
f(y|✓ln ) = f(y|µln R, ln R) = ln N(y;µln R,(
2
ln R +
=0.01
2 z}|{ 
2
V
)1/2
),
and the joint likelihood for t he entire set of observations is
f(D|
✓ln=fct(✓) z }| { µln R , ln R) = Y
D
j=1
1
yjp
2⇡p

2
ln R + 0.012
exp0
@
1
2
 p
ln yj  µln R

2
ln R + 0.012
!21
A
| {z } Log-Normal PDF
.
Note that the marginal and joint likelihood are defined as a func- tion of the log-space parameters. In order t o evaluate the likeli- hood for the parameters {µR, R}, we employ the transformation
functions given in equation 4.4. Note that the likelihood f (y|✓) de- scribes a probability density only for y and not for ✓. Therefore this
re-parametrization ✓ln = fct(✓) of the likelihood does not require
using the change of variable rule we saw in §3.4. Figure 6.24 presents the prior PDF for paramet e rs, the likeli- hood, the posterior, and the poster ior predictive for (a) D = 1, (b) D = 5, and (c) D = 100 observations. We see that, as the
number of observations D increases, the likelihood and posterior
for ✓ become concentrated around the true values, and the poste￾rior predictive for R tends toward the true PDF with parameters
µR ! µˇR = 42 MPa and R ! ˇR = 2 MPa.
6.6 Conjugate Priors
The previous section went through the process of using numerical
approximation methods for estimating the posterior PDF for the
parameters of a hidden random variable X. This complex processj.-a. goulet 80
is necessary when no analytical formulation exist s to describe the
posterior. For specific combinations of prior distribution and likelihood
function, the posterior PDF follows the same type of distribution
as the prior PDF. These specific combinations are referred to as
conjugate priors. For conjugate priors, analytic formulations are
available to describe the posterior PDF of parameters f(✓|D) as
well as the posterior predictive f(x|D).
Figure 6.25: Thumbtacks described by
a random variable with sample space S = {Head(H), Tail(T)}. (Photo: Michel Goulet)
One classic example of conjugate prior is the Beta-Binomial. Take, for example, the probability that a thumbtack (see figure
6.25) lands on either Head or Tail, S = {Head (H), Tail (T)}. We
can employ the Beta-Binomial conjugate prior to describe the
posterior PDF for x = Pr(Head|D) 2 (0,1), given a set of observa- tions D. It is suited in this case to assume that the prior PDF for
x = Pr( Head) follows a Beta PDF,
f(x) = B(x;↵, ) =
x
↵1
(1  x)1
B(↵, )
,
where B(↵, ) is the normalization constant. In this case, observa- tions are binary, so for a set containing D = DH + DT observations, heads are assigned the outcome H: DH = {H, H, · · · , H}1⇥DH and
tails are assigned the outcome T: DT = {T, T, · · · , T} 1⇥DT
. In that
case, the likelihood describing the probability of observing DH heads
and DT tails follows a Binomial distribution,
Pr(Head|x) = x Pr(Tail|x) = 1  x Pr(DH|x) = x
DH Pr(DT|x) = (1  x)
DT Pr(D|x) = xDH (1  x)
DT p(D|x) / x
DH (1  x)
DT .
The multiplication of the prior and the likelihood remains analyti- cally tractable so that
f(x|D) / x
DH (1  x)
DT
· x
↵1
(1  x)1
B(↵, )
,
and by recalculating the normalization constant, we find that the
posterior also follows a Beta PDF with parameters ↵+ DH and  +DT,
f(x|D) =
x
↵+DH 1
(1  x)+DT 1
B(↵ + DH,  + DT ) = B(x; ↵ + DH,  + DT ).
The analytic formulation for conjugate priors wit h their param- eters, likelihood, posterior, and posterior predictive can be found
online
5 or in specialized publications such as the compendium by
5Wikipedia (2017). Conjugate prior. URL https://en.wikipedia.org/wiki/
conjugate prior. Accessed November 8, 2019
Murphy.6
6Murphy, K. P. (2007). Conjugate Bayesian
analysis of the Gaussian distribution. Citeseerprobabilistic machine learning for civil engineers 81
Figure 6.26: Example of application of conjugate priors on traffic data. (Photo: John Matychuk)
Example: Traffic safety We take as another example an intersec- tion where the number of users is u = 3 ⇥ 10
5 /yr and where 20
accidents occurred over the last 5 years. Not e that for the purpose
of simplicity, the number of passages without accidents is taken
as the total number of users rather than this number minus the
number of accidents. This current information can be summarized
as
Dold : ⇢ DN = 5 · (3 ⇥ 10
5
) (passages without accident, N)
DA = 20 (accidents, A). Our prior knowledge for x, the probability of accident per user,
is described by the Beta PDF f (x) = B(x; 1,1), where the parame- ters are ↵0 = 0 = 1, which corresponds to a uniform distribution
over the interval (0, 1). The posterior PDF describing the daily
probability of accident given the observations Dold is thus also
described by the Beta PDF,
f(x|Dold) = B(x; ↵0 + D A,  0 + DN).
107 106 105 104 0
2
4
6
x = Pr(accident/user)
f
x( |Dold) Figure 6.27: Posterior PDF for the prob- ability of an accident before refection works.
Figure 6.27 presents the posterior PDF t r ansformed in the log10
- space using the change of variable rule (see §3.4),
f(log10 (x)|Dold ) = f(x|Dold) ·
d log10
(x)
dx
1
= f(x|Dold) · x ln 10. Now, we consider that following refection works made on the
intersection in order to decrease the probability of accidents, we
observe a = 1 accident over a period of one year, that is, 3 ⇥ 10
5
users without accident. In order to estimat e the new probability
of accident, we assume that our previous post er ior knowledge
f (x|Dold ) is an upper-bound for X, because the refection works
made the intersection safer. The new posterior then becomes
f(x|D) / x
1
· (1  x)3⇥10
5 | {z }
likelihood
·(1  F(x|Dold)) | {z }
prior
.
Note that because the prior is now described by the complement
of the CDF of X|Dold , which is not a conjugate prior, the post e￾rior does not have an analytic solution anymore, and the evidence
f (Dnew) must be approximated using numerical integration. Fig- ure 6.28 presents the prior, 1 F(x|Dold) and posterior PDF , f (x|D),
for the probability of an accident after t he intervention. The poste￾rior predictive probability of an accident before the refection works
was 1.3 ⇥ 10 5 compared to 5.8 ⇥ 106 after.
107 106 105 104 0
0.2
0.4
0.6
0.8
1
x = Pr(accident/user)
Pr(x
0 > x|Dold)
(a) Prior - 1  F (x|Dold)
107 106 105 104 0
0.5
1
1.5
x = Pr(accident/user)
f x( |D)
(b) Posterior - f(x|D)
Figure 6.28: Prior and posterior PDF for
the probability of an accident after refec- tion works.j.-a. goulet 82
6.7 Approximating the Posterior
For complex machine learning models such as those that will be
covered in the next chapters, conjugate priors are often not applica- ble, and it can be computationally prohibit ive to rely on sampling
methods for performing Bayesian estimation. In t hose cases, we
may have to employ point estimates rather than a probability den- sity function for describing the posterior PDF of model parameters.
6.7.1 Maximum Likelihood and Posterior Esti mates
Instead of estimating the posterior using a Bayesian approach, a
common approximation is to find a single set of optimal parameter
values ✓
⇤
that maximizes the likelihood (MLE) or the product of
the likelihood and the prior (MAP) where
✓
⇤ = 8
<
:
arg max
✓
f(D|✓) Maximum likelihood est imat e (MLE)
arg max
✓
f(D|✓) · f(✓) Maximum a-posteriori (MAP). Optimization methods covered in chapter 5 can be employed to
identify ✓
⇤
. Note that it is common to perform t he optimization
using a log-transformed objective function, for example, ln(f(D|✓)),
for numerical stability reasons (see equation 6.5). Note that it is
equivalent to optimize either the original or the log-t r ansformed
objective function,
arg max
✓
f(D|✓) ⌘ arg max
✓
ln f(D|✓) . Estimating only the most likely value wit h respect to either
the MAP or MLE criterion can lead to a negligible approximation
error when applied to identifiable problems (see §6.3.4), where the
amount of independent data is large; in t hat case, the posterior
is concentrated around a single value f (✓|D) ! (✓
⇤
). When the
set of observations is small, in the case of a multimodal posterior, or for non-identifiable problems, estimating only ✓
⇤
is no longer
a good approximation of the posterior. The definition of what
is a small or a large data set is problem dependent. In simple
cases, data sets containing fewer than 10 observations will typically
lead to significant epistemic uncertainty associated with unknown
parameter values. Cases involving complex models, such as those
treated in chapter 12, may need tens of thousands (D / 10
5
) of
observations to reach the point where the epist em ic uncertainty
associated with unknown parameter values is negligible. We saw in §5.5 that it is common to resort to parameter -space
transformations in order to perform optimizat ion in an uncon-probabilistic machine learning for civil engineers 83
strained domain. For an MLE, the quantity maximized is the
likelihood of observations y, given the parameters ✓. Because
f (y|✓) describes a probability density only for y and not for ✓, a
parameter-space transformation for an MLE does not require using
the change of variable rule we saw in §3.4. On the other hand, the
MAP seeks to maximize the product of the likelihood and the prior, f (y|✓)·f (✓). Here, the prior is a probability density, so the change
of variable rule applies when we employ space transformations. Nevertheless, as we saw in figure 3.19, when a PDF is subject to
a nonlinear transformation, the modes in t he original and trans￾formed spaces do not coincide. Therefore, the MLE is invariant
to parameter-space transformations and the MAP is not.7 Note 7 Jermyn, I. H. (2005). Invariant Bayesian
estimation on manifolds. The Annals of
Statistics 33 (2), 583–605
that Bayesian estimation is invariant to parameter-space transfor- mations, given that we apply the change of variable rule. We will
apply parameter-space transformations in the context of Bayesian
estimation in chapter 7.
6.7.2 Laplace Approximation
Another method for approximating a posterior PDF for the un- known parameters of a hidden random variable is the Laplace
approximation. This met hod approximates the post er ior as a mul- tivariat e Normal with a posterior mean vector equal to eit her the
MLE or MAP estimate ✓
⇤
, and a poste r ior covariance ⌃✓
⇤ ,
f(✓|D) | {z }
posterior ⇡ N (✓;
MLE/MAP estimate z}|{ ✓
⇤
, ⌃✓
⇤ |{z} Laplace approximation
). When we choose the MLE approximation to describe the posterior
mode, the posterior covariance is approximated by t he inverse
Hessian matrix of the negative log-likelihood evaluated at ✓
⇤
, ⌃✓⇤ = H[
log-likelihood z }| {
ln f(D|✓
⇤
)]1
=

@
2
ln f(D|✓)
@✓@✓
|
1
✓
⇤
. Details regarding the numerical estimation of the Hessian are
presented in §5.4. In order to include prior knowledge, we have to
estimate the Laplace approximation using ln(f(D|✓
⇤
)· f (✓
⇤
)) instead
of the log-likelihood ln f(D|✓
⇤
) alone. The relation between the second derivative of the log-likelihood
evaluated at ✓
⇤ and the covariance ⌃✓
⇤ can be explained by look￾ing at the multivariate Normal with probability density function
described byj.-a. goulet 84
f(✓) =
1
(2⇡)
P/2(det ⌃✓)1/2 exp✓
1
2
(✓  µ✓)|⌃
1
✓
(✓  µ✓)◆
, where P is the number of parameters ✓ = [✓1 ✓2 · · · ✓P]|
. By taking
the natural logarithm of f(✓), we obtain
ln f(✓) = 
P
2
ln 2⇡ 
1
2
ln det⌃✓ 
1
2
(✓  µ✓)|⌃
1
✓
(✓  µ✓), where the derivative of ln f(✓) with respect to ✓ is
@ ln f(✓)
@✓ = 
1
2
@

(✓  µ✓)| ⌃
1
✓
(✓  µ✓)
@✓ =  1
2 2⌃
1
✓
(✓  µ✓ ) = ⌃ 1
✓
(✓  µ✓).
If we di↵erentiate this last expression a second time with respect to
✓, we obtain
@
2
ln f(✓)
@✓@✓
| = ⌃
1
✓
. This development shows how the Laplace approximation allows esti- mating the covariance matrix using the post e rior PDF’s curvatures.
In the case of a Gaussian log-posterior, the second-order deriva- tives are constant, so the curvatures can be evaluated anywhere
in the domain. In practice, we employ the Laplace approximation
when the posterior is not Gaussian; in order to minimize the ap- proximation error, we evaluate the second-order derivatives of the
log-posterior at the MLE or MAP ✓
⇤
, where the probability density
is the highest.
4
0
4
4
0
4
0
0.1
0.2
✓ 2 ✓1
f( ) ✓|D
True posterior Laplace approximation
Figure 6.29: Example of an application of
the Laplace approximation to describe a posterior PDF. Figure 6.29 presents an example of an application of the Laplace
approximation, where a generic posterior PDF for two parameters
is approximated by a bivariate Normal PDF. The Hessian matrix
evaluated at the mode ✓
⇤ = [1.47 0.72]|
is
H[ ln f(D|✓
⇤
)] =

0.95 0.02
0.02 0.95

,
so the Laplace approximation of the posterior covariance is de- scribed by
⌃✓
⇤ = H[ ln f(D|✓
⇤
)]1 =

1.05 0.02 0.02 1.05

. The closer the posterior is to a multivariate Normal, the better
the Laplace approximation is. Note that for problems involving
several parameters, computing the Hessian matr ix using numerical
derivat ives becomes computationally prohibitive. One possibleprobabilistic machine learning for civil engineers 85
approximation is to estimate only the main diagonal of the Hessian. The main diagonal of the Hessian contains informat ion about the
variances of the posterior with respect to each variable without
providing information about the covariances (i.e., the pairwise
linear dependencies between variables).
In addition to being employed to approximate the posterior PDF,
the Laplace approximation can be used to est imat e the evidence
f(D). With the Laplace approximation, the post e rior is
f(✓|D) ⇡ N (✓; ✓
⇤
, ⌃✓⇤) =
1
(2⇡)
P/2
(det ⌃✓⇤ )1/2 | {z }
(a)
exp ✓
1
2
(✓  ✓
⇤
)|⌃
1
✓⇤ (✓  ✓
⇤
)◆
| {z }
(b)
.
(6.9)
In equation 6.9, (a) corresponds to the normalization constant
for the exponential term indicated in (b), for which the value is
exp(0) = 1 when evaluated at the mode ✓
⇤
(see §4.1). When
estimating
˜f (✓
⇤
) using either an MAP ˜f(✓
⇤
) = f (D|✓
⇤
) · f (✓
⇤
) or an
MLE ˜f(✓
⇤
) = f (D|✓
⇤
), its value is not going to equal 1. T herefore, the normalization constant in (a) must be multiplied by the value
of the unnormalized posterior evaluated at the mode ✓
⇤
so that
ˆf(D) = ˜f(✓
⇤
) · (2⇡)
P/2
(det ⌃✓
⇤ )1/2
. (6.10)
The reader interested in the formal derivation of this result from a
Taylor-series expansion should consult MacKay.8 8MacKay, D. J. C. (2003). Information
theory, inference, and learning algorithms. Cambridge University Press 6.8 Model Selection
In the concrete strength estimation example presented in §6.5.3,
it is known that the data was generated from a log-normal distri- bution. In real-life cases, the PDF form for the hidden random
variable X may either remain unknown or not correspond to an
analytically tractable probability density function. In these cases, it
is required to compare the performance of several model classes at
explaining the observations D. Given a set of M model classes m 2 M = {1, 2, · · · , M}, the
posterior probability of each class given D is described using Bayes
rule,
p(m|D) | {z } model class posterior=
model likelihood z }| { f(D|m)·model class prior z }| { p(m)
f(D
{1:M}
) | {z }
evidence
, (6.11)
where p(m|D) and p(m) are respectively the posterior and prior
Note: With Bayesian model selection, the objective is not to identify the single best model; it allows quantifying the posterior probability of each model, which can
be employed for performing subsequent
predictions.j.-a. goulet 86
probability mass functions for the model classes. The likelihood of
a model class m is obtained by marginalizing the model parameters
from the joint distribution of observations and parameters,
f(D|m) =
Z
f(D|✓
{m}
) · f( ✓
{m}
)d✓, (6.12)
where ✓
{m}
is the set of paramet e rs associated with the model class
m. The likelihood of a model class in equation 6.12 corresponds to
its evidence, which also corresponds to t he normalization constant
from the posterior PDF for its parameters,
f(D|m) ⌘
normalization constant for model class m
z }| { f(D
{m}
). (6.13)
The normalization constant for the model comparison problem
presented in equation 6.11 is obtained by summing the likelihood of
each model class times its prior probability,
f(D
{1:M}
) = X
m2M
f(D|m) · p(m).
Note that using the importance-sampling procedure presented in
equation 6.8 for estimating the normalization constants in equation
6.13 is only a viable option for simple problems involving a small
number of parameters and a small number of observations. Section
7.5 further discusses the applicability of more advanced sampling
methods for estimat ing f(D). Here, we revisit the concrete strength example presented in
§6.5.3. We compare the posterior probability for two model classes
m 2 M = {L,N} (i.e., a log-normal and a Normal PDF); note that
we know the correct hypothesis is m = L because simulated data
were generated using a log-normal PDF.
For D = 5 observations, the evidence of each model is
f(D|L) = 8.79 ⇥ 107
f(D|N) = 6.44 ⇥ 107
. With the hypothesis that the prior probability of each model is
equal, p(L) = p(N) = 0.5, the posterior probability for model classes
m are
p(L|D) =
8.79 ⇥ 107 ⇥ 0.5
8.79 ⇥ 107 ⇥ 0.5 + 6.44 ⇥ 107 ⇥ 0.5 = 0.58
p(N|D) = 1  p(L|D) =
6.44 ⇥ 107 ⇥ 0.5
8.79 ⇥ 107 ⇥ 0.5 + 6.44 ⇥ 107 ⇥ 0.5 = 0.42.probabilistic machine learning for civil engineers 87
This result indicates that both models predict with almost equal
performance the observations D. Therefore, if we want to make
predictions for unobserved quantities, it is possible to employ both
models where the probability of each of them is quantified by its
posterior probability p(m|D). In practice, because estimating the evidence f (D|m) is a com- putationally demanding task, it is common to resort to approxima- tions such as the Akaike information criterion (AIC) and Bayesian
information criterion (BIC) for comparing model classes. Note t hat
the BIC is a special case derived from t he Laplace approximation
in equation 6.10 for estimating f(D). Other advanced approximate
methods for comparing model classes are the Deviance information
criterion 9
(DIC) and the Watanabe-Akaike information criterion10 9 Spiegelhalter, D. J., N. G. Best, B. P. Carlin, and A. van der Linde (2002). Bayesian measures of model complexity
and fit. Journal of the Royal Statistical
Society: Series B (Statistical Methodol- ogy) 64 (4), 583–639
10 Watanabe, S. (2010). Asymptotic
equivalence of Bayes cross validation and widely applicable information criterion
in singular learning theory. Journal of
Machine Learning Research 11, 3571–3594
(WAIC). The reader interested in these methods should refer to
specialized textbooks.11 Another common model selection technique
11 Burnham, K. P. and D. R. Anderson
(2002). Model selection and multimodel
inference: A practical information-theoretic approach (2nd ed.). Springer; and Gelman, A., J. B. Carlin, H. S. Stern, and D. B. Rubin (2014). Bayesian data analysis (3rd
ed.). CRC Press
widely employed in machine learning is cross-validation; the main
idea consists in employing a first subset of data to estimate the
model parameter and then test the predict ive performance of the
model using a second independent data set. The details regarding
cross-validation are presented in §8.1.2, where the method is em- ployed to discriminate among model structures in the context of
regression problems.7
Markov Chain Monte Carlo
The importance sampling method was introduced in chapt e r 6 for
estimating the normalization constant of a posterior probability
density function (PDF) as well as its expected values and covari- ance. A limitation of this approach is related to the difficulties
associated with the choice of an efficient sampling distribution h(·). The sampling methods covered in this chapter address this limita￾tion by providing ways of sampling directly from the posterior using
Markov chain Monte Carlo (MCMC) methods.
Figure 7.1: Example of random walk
generated with MCMC.
Markov chain Monte Carlo takes its name from the Markov
property, which states:
Given the present, the future is independent from the past. It means that if we know the current state of the system, we can
predict future states without having to consider past states. Start￾ing from Xt ⇠ p(xt), we can transition to xt+1 using a transition
probability p(xt+1|xt) that only depends on xt
. It implicitly conveys
that the conditional probability depending on xt
is equivalent to
the one depending on x1:t, p(xt+1|xt) = p(xt+1|x1, x2, · · · , xt). A Markov chain defines the joint distribution for rand om variables
by c ombining the chain rule (see §3.3.4) and the Markov property
so that
p(x1:T) = p(x1)p(x2|x1)p(x3|x2)· · · p(xT
|xT1) = p(x1)Y
T
t=2
p(xt
|xt1). The idea behind MCMC is to construct a Markov chain for which
the stationary distribution is the posterior. Conceptually, it corre￾sponds to randomly walking through the parameter space so that
the fraction of steps spent at exploring each part of the domain
is proportional to the posterior density. Figure 7.1 presents an ex-j.-a. goulet 90
ample of a random walk generated with MCMC that follows the
density described by the underlying contour plot.
Several MCMC methods exist: Metropolis-Hastings, Gibbs sam￾pling, slice sampling, Hamiltonian Monte Carlo, and more. This
chap ter covers only the Metropolis and Metropolis-Hastings meth￾ods be cau se they are the most acc e ssible. The re ade r interested in
advanced methods should refer to dedicated textbooks such as the
one by Brooks et al.1 1Brooks, S., A. Gelman, G. Jones, and
X.-L. Meng (2011). Handbook of Markov Chain Monte Carlo. CRC Press 7.1 Metropolis
The Metropolis algorithm2 was de veloped during the Second World 2 Metropolis, N., A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller
(1953). Equation of state calculations by
fast computing machines. The Journal of
Chemical Physics 21 (6), 1087–1092
War while working on the Manhattan project (i.e., the atomic
bomb) at Los Alamos, Ne w Me xic o. M et ropolis is not the most
efficient sampling algorithm, yet it is a simple one allowing for an
easy introduction to MCMC methods. Notation
Initial state: ✓0 Target distribution: ˜f(✓)
Proposal distribution: q(✓
0|✓)
The Metropolis algorithm requires defining an initial state for
a set of P parameters ✓0 = [✓1 ✓2 · · · ✓ P]|
0
, a target distribution
˜f (✓) = f (D|✓) · f (✓) corresponding to the unnormalized posterior
we want to sample from, an d a proposal distribution, q(✓
0|✓), which
describes where to move next given the current parameter values. The proposal must have a nonzero probability to transit from the
current state to any state supported by the target distribution and
must be symmetric, that is,
q(✓
0|✓) = q(✓|✓
0). The Normal distribution (see §4.1) is a common general-purpose
proposal distribution,
q(✓
0|✓) = N (✓
0; ✓, ⌃q), where the mean is defined by the current position and the proba￾bility to move in a region around the current position is controlled
by the covariance matrix ⌃q
. Figure 7.2 presents an example of 1-D
and 2-D Normal proposal distributions.
(a) N (✓
0; ✓, 
2
q
)
(b) N
✓
0 ; [✓1 ✓2 ]|, diag(⇥
2
q1 
2
q2 ⇤
)
Figure 7.2: Examples of 1-D and 2-D
proposal distributions, q(✓
0 T |✓). he Metropolis algorithm is recursive so at the s
th
step, given a
current position in the parameter space ✓ = ✓s, we employ q(✓
0|✓)
to propose moving to a new position ✓
0. If the target distribution
evaluated at the proposed location is greater than or equal to the
current one, that is, ˜f(✓
0)  ˜f(✓)—we accept the proposed location.
If the proposed location has a target value that is lower than the
current one, we accept moving to the proposed location with a
probability equal to the acceptance ratio
f˜(✓
0 ) ˜f(✓)
. In the case where
Note: In order to accept a proposed
location with a probability equal to r =
˜f(✓
0 )/
˜f(✓), we compare it with a sample u
taken from U(0, 1). If u  r, we accept the move; otherwise, we reject it.probabilistic machine learning for civil engineers 91
the proposed location is rejected, we stay at the current location
and ✓s+1 = ✓s. Each step from the Metropolis sampling method is
formalized in algorithm 4.
Algorithm 4: Metropolis sampling
1 define ˜f(✓) (target distribution)
2 define q(✓
0|✓) (proposal distribution)
3 define S (numbe r of samples)
4 initialize S = ; (set of samples)
5 initialize ✓0 (initial starting location)
6 for s 2 {0, 1, 2, · · · , S  1} do
7 define ✓ = ✓s 8 sample ✓
0 ⇠ q(✓
0|✓)
9 compute ↵ =
f˜(✓
0) ˜f(✓) (acceptance ratio)
10 compute r = min(1, ↵)
11 sample u ⇠ U(0, 1)
12 if u  r then
13 ✓s+1 = ✓
0 14 else
15 ✓s+1 = ✓s 16 S {S [ {✓s+1}} (add to the set of samples)
If we apply this re cu rsive algorithm ove r S iterations, the result
is that the fraction of the steps spent exploring each part of the
domain is proportional to the density of the target distribution
˜f (✓). Note that this last statement is valid under some conditions
regarding the chain starting location and number of samples S that
we will further discuss in §7.3. If we select a target distribution that
is an unnormalized posterior, it implies that each sample ✓s is a
realization of that posterior, ✓s : ✓ ⇠ f(D|✓) · f(✓).
5 0 5
0
0.2
0.4 s =1
✓ = ✓s =1 ! ˜f(✓) =0.14083
✓
0 =0.54333 ! ˜f(✓
0) =0.15954 ↵ =
f˜(✓
0) ˜f(✓) =1.1329 ↵  1 ! ✓s+1 = ✓
0
✓
˜f( )✓
✓
✓
0
5 0 5
0
1
2
✓
s
(a) s = 1
5 0 5
0
0.2
0.4 s =2
✓ = ✓s =0.54333 ! ˜f(✓) =0.15954
✓
0 =0.15416 ! ˜f(✓
0) =0.15232 ↵ =
˜f(✓
0)
f˜(✓) =0.95477 ↵  1 ! u =0.71914 u < ↵ ! ✓s+1 = ✓
0
✓
˜f( )✓
✓
✓
0
5 0 5
0
1
2
3
✓
s
(b) s = 2
5 0 5
0
0.2
0.4 s =3
✓ = ✓s =0.15416 ! ˜f(✓) =0.15232
✓
0 =0.75426 ! ˜f(✓
0 ) =0.15452 ↵ =
˜f(✓
0) ˜f(✓) =1.0144 ↵  1 ! ✓ s+1 = ✓
0
✓
˜f( )✓
✓
✓
0
5 0 5
0
1
2
3
4
✓
s
(c) s = 3
5 0 5
0
0.2
0.4
✓
˜f( )✓ 5 0 5
0
1,000
2,000
✓
s
(d) s = 2400
Figure 7.3: Step-by-step example of 1-D
Metropolis sampling.
Figure 7.3 presents a step-by-step application of algorithm 4 for
sampling a given target density f˜(✓ ). The proposal distribution
employed in this example has a standard deviation q = 1. At step
s = 1, the target value at the proposed location ✓
0 is greater than
at the current location ✓ , so the move is accepted. For the second
step, s = 2, the target value at the proposed location is smaller
than the value at the current location. The move is nevertheless
accepted because the random number u drawn from U(0, 1) turned
out to be smaller than the ratio
f˜(✓
0)
f˜(✓)
. At step s = 3, the target
value at the proposed location ✓
0 is greater than the value at the
current location ✓, so the move is accepted. Figure 7.3d presents the
chain c ontaining a total of S = 2400 samples. The superposition
of the sample’s histogram and the target density confirms that the
Metropolis algorithm is sampling from ˜f(✓).j.-a. goulet 92
7.2 Metropolis-Hastings
The Metropolis-Hastings algorithm3
is identical to the Metropo- 3 Hastings, W. K. (1970). Monte Carlo
sampling methods using Markov chains and their applications. Biometrika 57 (1), 97–109
lis algorithm except that it allows for nonsymmetric transition
probabilities where
q(✓
0 |✓) =6 q(✓|✓
0). The main change with Metropolis-Hastings is that when the pro￾posed location ✓
0 has a target value that is lower than the current
location ✓, we acc ep t the proposed location with a prob ability
equal to the ratio
f˜(✓
0 )
f˜(✓) times the ratio
q(✓|✓
0 )
q(✓0 |✓)
, that is, the ratio of
the probability density of going from the proposed location to the
current one, divided by the probability density of going from the
current location to the proposed one.
Applying Metropolis-Hastings only requires replacing line 9 in
algorithm 4 with the new acceptance ratio,
↵ =
f˜(✓
0) ˜f(✓)
· q(✓|✓
0)
q(✓0|✓)
.
In the particular case where transition probabilities are symmetric, that is, q(✓
0|✓) = q(✓|✓
0), then Metropolis-Hastings is equivalent
to Metropolis. Note that when we employ a Normal proposal
density for parameters defined in the unbounded real domain, ✓ 2 RP
, then there is no need for Metropolis-Hastings. On the other
hand, using Metropolis-Hastings for sampling bounded domains
requires modifying the proposal density at each step. Figure 7.4
illustrates why if we employ a truncated Normal PDF as a proposal, q(✓
0|✓) 6= q(✓|✓
0) because the normalization constant needs to be
recalculated at each iteration. Section 7.4 shows how to leverage
transformation functions ✓
tr = g(✓) in order to transform a bounded
domain into an unbounded one ✓
tr 2 R so that the Metropolis
method can be employed.
0 ✓ ✓
0
q(✓|✓
0)
q(✓
0|✓)
q(✓
0|✓)
q(✓|✓
0)
Figure 7.4: Example illustrating how, when using a truncated Normal PDF as a proposal, q(✓
0|✓) =6 q(✓|✓
0 ).
7.3 Convergence Checks
So far, the presentation of the Metropolis and Metropolis-Hastings
algorithms overlooked the notions of convergen ce. For that , we need
to further address two aspects: the burn-in phase and convergence
metrics.
7.3.1 Burn-In Phase
For samples taken with an MCMC method, in order to be con￾sidered as realizations from the stationary distribution describingprobabilistic machine learning for civil engineers 93
the target PDF, a chain must have forgotten where it started from. Depending on the choice of initial state ✓0 and transition PDF, the sampling procedure may initially stay trapped in a part of the
domain so that the sample’s density is not representative of the
target density. As depicted in figure 7.5, this issue requires dis- carding samples taken before reaching the stationary distribution. These discarded samples are called the burn-in phase. In practice,
it is common to discard the first half of each chain as the burn-in
phase and then perform the convergence check that will be further
detailed in §7.3.2. 5 0 5
5
0
5
Starting location
!
✓ 1
✓2
Figure 7.5: The impact of the initial starting location on MCMC sampling. Figure 7.6 presents an example adapted from Murphy,4 where
4 Murphy, K. P. (2012). Machine learning: A probabilistic perspective. MIT Press
given a current location x 2 {0, 1, · · · , 20}, the transition model
is defined so there is an equal probability of moving to the nearest
neighbor either on the left or on the right. If we apply this random
transition an infinite number of times, we will reach the stationary
distribution p
(1)(x) = 1/21, 8x. Each graph in figure 7.6 presents
the probability p
(n)(x) of being in any state x after n transitions
from the initial state x0 = 17. Here, we see that even after 100
transitions, the chain has not yet forgotten where it started from
because it is still skewed toward x = 17. After 400 transitions, the
initial state has been forgotten, because in this graph we can no
longer infer the chain’s initial value.
0 5 10 15 20 p
(0)(x) 0 5 10 15 20 p
(1)(x) 0 5 10 15 20 p
(2)(x) 0 5 10 15 20 p
(3)(x) 0 5 10 15 20 p
(10)(x) 0 5 10 15 20 p
(100)(x) 0 5 10 15 20 p
(200)(x) 0 5 10 15 20 p
(400)(x) Figure 7.6: The impact of the initial starting location using a stochastic process.
7.3.2 Monitoring Convergence
Monitoring convergence means assessing whether or not the M CM C
samples belong to a stationary distribution. For one-dimensional
problems, it is possible to track the convergence by plotting the
sample numbers versus their values and identifying visually whether
or not they belong to a stationary distribution (e.g., see figure 7.3).
When sampling in several dimensions, this visual check is limited.
Instead, the solution is to generate samples from multiple chains
(e.g., 3–5), each having a di↵erent starting location ✓0. The station￾arity of these chains can be quantified by comparing the variance
within and between chains using the estimated potential scale re￾duction (EPSR). Figure 7.7 illustrat es the not at ion e mployed to
describe samples from multiple chains, where ✓s,c identifies the s
th
sample out of S, from the c
th chains out of C. Note that because the
final number of samples desired is S, a quantity equal to 2S samples
must be generat ed in orde r to accou nt for those discarded during
the burn-in period.j.-a. goulet 94
Burn-in samples “Stationary” samples
(discarded)
Chain #1
Chain #2
Chain #
Chain #c
Initial states Samples #1 Samples #
Figure 7.7: The notation for samples taken
from multiple chains.
7.3.3 Estimated Potential Scale Reduction
The estimated potential scale reduction
5 metric denoted by Rˆ is
5Gelman, A. and D. B. Rubin (1992). Inference from iterative simulation using multiple sequences. Statistical Science 7(4), 457–472
computed from two quantities: the within-chains and between￾chains variances. The within-chains mean ✓·c and variance W are
estimated using
✓·c =
1
SX
S
s=1
✓s,c and
W =
1
C X
C
c=1
 
1
S  1 X
S
s=1
(✓s,c  ✓·c)2 !
. The property of the within-chains variance is that it underesti- mates the true variance of samples. The between-chains mean ✓·· is
estimated using
✓·· =
1
CX
C
c=1
✓·c,
and the variance between the means of chains is given by
B =
S
C  1X
C
c=1
(✓·c  ✓··)2
. Contrarily to the within-chain variance W , the between-chain
estimate
Vˆ =
S  1
S
W +
1
S
B
overestimates the variance of samples. The metric Rˆ is defined as
the square root of the ratio between Vˆ and W , Rˆ = s
VˆW
. Because Vˆ overestimates and W underestimates the variance,
Rˆ should be greater than one. As illustrated in figure 7.8, a valueprobabilistic machine learning for civil engineers 95
Rˆ ⇡ 1 indicates that the upper and lower bounds have converged
to the same value. Otherwise, if Rˆ > 1, it is an indication that
convergence is not reached and the number of samples needs to
be increased. In practice, convergence can be deemed to be met
when Rˆ < 1 + ✏ ⇡ 1.1. When generating MCMC samples for
✓ = [✓1 ✓2 · · · ✓P ]|
, we have to comput e the EPSR Rˆ
i for each
dimension i 2 {1, 2, · · · , P}.
true value Number of samples (s)
Figure 7.8: The EPSR metric to check for convergence. Figures 7.9a–c compare the EPSR convergence metric Rˆ for the
Metropolis method applied to a 2-D target distribution using a
di↵erent number of samples. Figures 7.9a–c are using an isotropic
bivariate Normal proposal PDF with q = 1. From the three tests, only the one employing S = 104
samples meets the criterion that we
have set here to Rˆ < 1.01.
5 0 5 5
0
5
✓1
✓2
q(✓
0|✓)
5 0 5 5
0
5
✓1
✓2
c = 1
5 0 5 5
0
5
✓1
✓2
c = 2
5 0 5 5
0
5
✓1
✓2
c = 3
(a) S = 100,  = 60% , Rˆ = [1.1074 1.0915]
5 0 5 5
0
5
✓1
✓2
q(✓
0|✓)
5 0 5 5
0
5
✓1
✓2
c = 1
5 0 5 5
0
5
✓1
✓2
c = 2
5 0 5 5
0
5
✓1
✓2
c = 3
(b) S = 1000,  = 60% , Rˆ = [1.0042 1.0107]
5 0 5 5
0
5
✓1
✓2
q(✓
0|✓)
5 0 5 5
0
5
✓1
✓2
c = 1
5 0 5 5
0
5
✓1
✓2
c = 2
5 0 5 5
0
5
✓1
✓2
c = 3
(c) S = 10,000 :,  = 60% , Rˆ = [1.0005 1.0011]
5 0 5 5
0
5
✓1
✓2
q(✓
0|✓)
5 0 5 5
0
5
✓1
✓2
c = 1
5 0 5 5
0
5
✓1
✓2
c = 2
5 0 5 5
0
5
✓1
✓2
c = 3
(d) S = 1000,  = 38% , Rˆ = [1.0182 1.0194]
Figure 7.9: Comparison of the ESPR
convergence metric Rˆ for 100, 1,000, and
10,000 MCMC samples.j.-a. goulet 96
7.3.4 Acceptance Rate
The acceptance rate  is the ratio between the number of steps
where the proposed move is accepted and the total number of steps. Figures 7.9a–c have an acceptance rate of 60 percent.
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.4
0.8
1.2
Acceptance rate
Relative speed
Figure 7.10: Relative convergence speed
of MCMC for P  5 as a function of the acceptance rate . (Adapted from Rosenthal (2011).)
In the case presented in figure 7.9d, the standard deviation of the
proposal was doubled to q = 2. It has the e ↵ec t of re ducing the
acceptance rate to 38 percent. The convergence speed of MCMC
methods is related to the acceptance rate. For ideal cases involving
Normal random variables, the optimal acceptance rate is approxi- mately 23 percent for parameter spaces having five dimensions or
more (i.e., P  5), and approximately 44 percent for cases in one
dimension (P = 1).6 Figure 7.10 presents the relative convergence 6Rosenthal, J. S. (2011). Optimal proposal
distributions and adaptive MCMC. In Handbook of Markov Chain Monte Carlo, 93–112. CRC Press
speed as a function of the acceptance rate for a case involving five
dimensions or more (P  5). Note that there is a wide range of
values for  with similar efficiency, so the optimal values should not
be sought strictly.
5 0 5 5
0
5
✓1
✓2
q(✓
0|✓)
5 0 5 5
0
5
✓1
✓2
c = 1
5 0 5 5
0
5
✓1
✓2
c = 2
5 0 5 5
0
5
✓1
✓2
c = 3
(a) S = 1000,  = 34% , Rˆ = [1.0421 1.0389]
5 0 5 5
0
5
✓1
✓2
q(✓
0|✓)
5 0 5 5
0
5
✓1
✓2
c = 1
5 0 5 5
0
5
✓1
✓2
c = 2
5 0 5 5
0
5
✓1
✓2
c = 3
(b) S = 1000 :,  = 28% , Rˆ = [1.2031 1.1952]
5 0 5 5
0
5
✓1
✓2
q(✓
0|✓)
5 0 5 5
0
5
✓1
✓2
c = 1
5 0 5 5
0
5
✓1
✓2
c = 2
5 0 5 5
0
5
✓1
✓2
c = 3
(c) S = 1000,  = 37% , Rˆ = [1.0064 1.0048]
Figure 7.11: Comparison of the EPSR
convergence metric Rˆ for 1,000 MCMC
samples using di↵erent proposal distribu- tions.probabilistic machine learning for civil engineers 97
Figure 7.11 presents another set of examples applied to a target
distribution having a high (negative) correlation between the
dimensions ✓1 and ✓2, and using a fixed number of samples equal
to S = 1000. In comparison with the case presented in (a), the case
in (b) displays worse Rˆ values. This di↵erence is due to the poor
choic e of correlat ion co effic ie nt for the proposal PDF in (b); the
correlation coefficient of the proposal has a sign that is the opposite
of the one for the target PDF. If, like in figure 7.11c, the proposal is
well selec te d for the target distribution, the convergence speed will
be higher. For trivial cases, it may be possible to infer efficient parame ters
for proposals from trials and errors or from heuristics. However, for more complex cases involving a large number of dimensions, manual tuning falls short. The ne xt sec tion presents a met hod for
automatically tuning the proposal covariance matrix.
7.3.5 Proposal Tuning
One generic way to define the proposal PDF is using a multivariate
Normal centered on the current state ✓ and with covariance matrix

2⌃✓
⇤ , q(✓
0|✓) = N (✓
0 ; ✓, 
2⌃✓⇤ ), 
2 =
2.4
2
P
, where the scaling factor  depends on the number of parameters
P. The covariance mat rix ⌃✓⇤ is an approximation of the posterior
covariance obt ained using the Laplace approximation (see §6.7.2)
calculated for the maximum a-posteriori value (MAP), ✓
⇤
. The
MAP value can be estimated using gradient-based optimization
techniques such as those presented in §5.2.
Algorithm 5 presents a simple implementation of the Metropolis
algorithm with convergence checks and with a tuning procedure
for the scaling factor . Note that this algorithm is a minimal
example intended to help the reader understand the algorithm flow. Implementations with state-of-the-art efficiency include several
more steps.
7.4 Space Transformation
When dealing with parameters that are not defined in the un- bounded real space, one solution is to employ the Metropolis￾Hastings algorithm to account for the non-reversibility of the
transitions caused by the domain constraints, as illustrated in
figure 7.4.j.-a. goulet 98
Algorithm 5: Convergence check and scaling parameter tuning
1 define ˜f(✓), S, C, ✏
2 initialize ✓0,c, i = 0
3 compute MAP: ✓
⇤
(e.g., gradient-based optimization)
4 compute ⌃✓ ⇤ = H[ ln f˜(✓
⇤
)]1
(Laplace approximation)
5 define q(✓
0|✓) = N (✓
0 ; ✓, 
2⌃✓
⇤ ), 
2 = 2.4
2 P
6 for chains c 2 {1, 2, · · · , C} do
7 Sc = ;
8 for samples s 2 {0, 1, 2, · · · , S  1} do
9 Metropolis algorithm: Sc {Sc [ {✓s+1,c}}
10 if i = 0 (Pilot run) then
11 for c 2 {1, 2, · · · , C} do
12 Sc = {✓S/2,c
, · · · , ✓S,c} (discard burn-in samples)
13 compute  (acceptance rate)
14 if  < 0.15 then
15 
2 = 
2 /2, Goto 5 (decrease scaling factor)
16 else if  > 0.50 then
17 
2 = 2
2
, Goto 5 (increase scaling factor)
18 compute Rˆ
p, 8p 2 {1, 2, · · · , P} (EPSR)
19 if Rˆ
p > 1 + ✏ for any p then
20 ✓0,c = ✓S,c (restart using the last sample)
21 S = 2
i ⇥ S (increase the number of samples)
22 i = i + 1, Goto 6
23 converged
Another solution is to transform each constrained parameter
in the real space in order to employ the Metropolis algorithm. Transformation functions ✓
tr = g(✓i ) suited for this purpose are
described in §3.4. When working with transformed parameters, the
proposal distribution has to be defined in the transformed space,
q(✓
tr
0 |✓
tr) = N (✓
tr
0; ✓
tr
, 
2⌃
tr✓⇤ ), where ⌃tr✓
⇤ is estimated using the Laplace approximation, which is
itself defined in the transformed space. The target probability can
be evaluated in the transformed space using the inverse tran sforma- tion function g1
(✓
tr ), its gradient|rg (✓)|1
✓
tr evaluated at ✓
tr
, and
the change of variable rule, Change of variable rule
fY(y) = fX(x) |det J y,x| 1
(see §3.4)
f˜(✓
tr ) = f˜( =✓
z }| { g 1
(✓
tr)) · P
i
Y=1
1
|rig(✓)|✓
tr .probabilistic machine learning for civil engineers 99
Because each transformation function g(✓i) depends on a single pa￾rameter, the determinant of the diagonal Jacobian matrix required
Determinant of a diagonal matrix
det diag(x) = QX
i=1 xi
for the transformation g(✓) simplifies to the product of the inverse
absolute value of the gradient of each transformation function
evaluated at ✓i = g1
(✓
tr
i ). Algorithm 6 presents a simple imple￾mentation of the Metropolis algorithm applied to a transformed
space.
Algorithm 6: Metropolis with transformed space 1 define ˜f(✓) (target distribution)
2 define g(✓) ! ✓
tr
, g1
(✓
tr ) ! ✓ (transformation functions)
3 define rg(✓) (transformation gradient)
4 define S (numbe r of samples)
5 initialize S = ; (set of samples)
6 initialize ✓0 (initial starting location)
7 compute MAP: ✓
⇤tr (e.g., gradient-based optimization)
8 compute ⌃tr✓ ⇤ = H[ ln ˜f(✓
⇤tr )]1
(Laplace approximation)
9 for s 2 {0, 1, 2, · · · , S  1} do
10 define ✓
tr = g(✓s )
11 sample ✓
0tr ⇠ N (✓
0tr
; ✓
tr
, 
2⌃tr✓
⇤ )
12 compute ↵ =
˜f(g1
(✓
0tr ))
f˜(g1(✓tr ))
· Y
P
i=1
|ri g(✓)|✓tr |rig(✓
0)|✓
0 tr 13 compute r = min(1, ↵)
14 sample u ⇠ U(0, 1)
15 if u  r then
16 ✓s+1 = g1
(✓
0 tr)
17 else
18 ✓s+1 = ✓s 19 S {S [ {✓s+1}}
7.5 Computing with MCMC Samples
When employing an MCMC method, each sample ✓s is a realization
of the target distribution f˜(✓). When we are interested in perform￾ing a Bayesian estimation for parameters using a set of observations
D = {y1 , y2, · · · , yD }, the unnormalized target distribution is defined
as the product of the likelihood and the prior, ˜f(✓) = f(D|✓) · f(✓). The posterior expected values and covariance for f (✓|D) are then
obtained by computing the empirical average and covariance of thej.-a. goulet 100
samples ✓s,E[✓|D] =
Z ✓ · f(✓|D)d✓
⇡
1
SX
S
s=1
✓s
cov(✓|D) = E
⇥
(✓  E[✓|D])(✓  E[✓|D])|
⇤
⇡
1
S  1X
S
s=1
(✓s  E[✓|D])(✓s  E[✓|D])|
. Given fX (x;✓), a probability density function defined by the
parameters ✓. The posterior predictive density of X given observa￾tions D is obtaine d by marginalizing the uncertainty of ✓, X|D ⇠ f(x|D) =
Z
f X(x; ✓) ·
✓s z }| { f(✓|D) d✓. Predictive samples xs can be generated by using samples ✓s and
evaluating them in fX(x; ✓s), so
xs : X|✓s ⇠ fX(x; ✓s) | {z }
sample from X using MCMC samples ✓s
.
The posterior predictive expected value and variance for X|D ⇠
f(x|D) can then be estimated as
E[X|D] =
Z x · f(x|D)dx
⇡
1
S X
S
s=1
xs
var[X|D] = E[(X  E[X|D])2
]
⇡
1
S  1 X
S
s=1
(xs  E[X|D])2
. Given a func tion that depends on parameters ✓ such that
z = g(✓), posterior predictive samples from that function can
be obtained by evaluating it for randomly selec ted samples ✓s,
zs : Z|✓s = g(✓s )
| {z }
sample from Z using MCMC samples ✓s
.
The posterior predictive expected value and variance for the func-probabilistic machine learning for civil engineers 101
tion output are thus once again
E[Z|D] =
Z
z · f(z|D)dz
⇡
1
SX
S
s=1
zs
var[Z|D] = E[(Z  E[Z|D])2
]
⇡
1
S  1X
S
s=1
(z s  E[Z|D])2
. As we saw in §6.8, performing Bayesian model selection requires
computing the evidence
f(D) =
Z
f(D|✓) · f(✓)d✓.
It is theoretically possible to estimate this normalization constant
from Metropolis-Hastings samples using the harmonic mean of the
likelihood.7 This method is not presented here because it is known 7 Newton, M. A. and A. E. Raftery (1994). Approximate Bayesian inference with the weighted likelihood bootstrap. Journal of the Royal Statistical Society. Series B
(Methodological) 56(1), 3–48
for its poor performance in practice.8 The annealed importance
8Neal, R. (2008). The harmonic mean of the likelihood: Worst
Monte Carlo method ever. URL
https://radfordneal.wordpress.com/
2008/08/17/the-harmonic-mean-of-the
-likelihood-worst-monte-carlo-method
-ever/. Accessed November 8, 2019
sampling
9
(AIS) is a method combining annealing optimization
9Neal, R. M. (2001). Annealed importance
sampling. Statistics and Computing 11 (2), 125–139
methods, importance sampling, and MCMC sampling. Despite
being one of the efficient methods for estimating f (D), we have to
ke ep in mind that estimating the evidence is intrinsically difficult
when the number of parameters is large and when the posterior is
multimodal or when it displays nonlinear dependencies. Therefore, we should always be careful whe n estimat ing f(D) because no
perfect black-box solution is currently available for estimating it. Example: Concrete tests We are revisiting the exam ple pre sented
in §6.5.3, where Bayesian estimation is employed to characterize
the resistance R of a concrete mix. The resistance is now modeled
as a Normal random variable with unknown mean and varianc e,
R ⇠ N(r; µR, 
2
R). The observation model is y = r + v, v : V ⇠ N (y; 0,0.012
)MPa , where V describes the observation errors that
are independent of each other, that is, Vi ? Vj, 8i 6= j. Our goal
is to estimate the posterior PDF for the resistance’s mean µR and
standard deviation R, Posterior PDF
z }| { f(µR, R |D) =
Likelihood z }| { f(D|µR,  R)· Prior PDF
z }| { f(µR , R)
f(D)
| {z }
Evidence
.
In order to reflect an absence of prior knowledge, we employ
non-informative priors (see §6.4.1) for both parameters, so thatj.-a. goulet 102
f (µR ) / 1, f(R) / 1/R, and thus f(µR, R) / 1/R . The
likelihood of an observation y , given a set of parameters ✓, is
f (y|✓) = N (y; µR, 
2
R + 
2
V = 0.012
). Because of the conditional
independence assumption for the observations, the joint likelihood
for D observations is obtained from the produc t of the marginals,
f(D|µR, R) = Y
D
i=1
1
p
2⇡p

2
R + 
2
V
exp 2
4
1
2 
yi  µR
p

2
R + 
2
V
!23
5 . | {z } Normal PDF
In this example, we assume we only have three observations D =
{43.3, 40.4, 44.8} MPa.
The parameter  R 2 (0, 1) = R+ is constrained to positive
real numbers. Therefore, we perform the Bayesian estimation in the
transformed space ✓
tr
, ✓
tr = [✓
tr1 ✓
tr2
]| = [µR ln(R)]|
✓ = [µR R]| = [✓
tr1 exp(✓
tr2
)]|
. Note that no transformation is applied for µR because it is already
defined over R. The Newton-Raphson algorithm is employed in order to identify
the set of parameters maximizing the target PDF defined in the
transformed space, ˜f(✓
tr) = f(D|✓
tr) · f(✓
tr). The optimal values found are
✓
⇤tr = [43.0 0.6]|
✓
⇤ = [43.0 1.8]|. The Laplace approximation evaluated at ✓
⇤tr
is employed to esti- mate the posterior covariance
⌃
tr✓ ⇤ = H[ ln ˜f(✓
⇤tr)]1 =

1.11 0
0 0.17 
. This estimation is employed with a scaling factor 
2 = 2.88 in
order to initialize the proposal PDF covariance. A total of C = 4
chains are gene rate d, where each c ontains S = 104
samples. The
scaling factor obtained after tuning is 
2 = 1.44, which lead s to
an acceptance rate of  = 0.29 and EPSR convergence metrics
Rˆ = [1.0009 1.0017].probabilistic machine learning for civil engineers 103
The posterior mean and posterior covarian ce are estimated from
the MCMC samples ✓s, Eˆ[✓|D] =
1
SX
S
s=1
✓s = [42.9 3.8]|
(Poste rior mean)
cˆ o v ( ✓|D) =
1
S  1 X
S
s=1
(✓s  E[✓|D])(✓s E[✓|D])| =

8.0 2.5
2.5 20.6

(Poste rior covariance). The posterior predictive mean and variance for the concrete resis- tance, which include the epistemic uncertainty about the parame￾ters µR and R , are
Eˆ[R|D] =
1
S X
S
s=1
rs = 42.9
vˆar[R|D] =
1
S  1X
S
s=1
(rs  E[R|D])2 = 40.5, where samples rs : R ⇠ N (r;µR,s, 
2
R,s ) are generated using the
MCMC samples ✓s = [µR,s R,s]|
.
25
40
55
0
5
10
µR R
f
µ( R, 
R)
{µˇR , ˇR }
25
40
55
0
5
10
µR R
f
µ (D| R, 
R)
25
40
55
0
5
10
µR R
f
µ( R, 
R|D) 30 40 50
r
˜f r( |D)
Predictive PDF
True PDF
Figure 7.12: Example of application of
MCMC sampling for the estimation of the
resistance of a concrete mix. The cross
indicates the true (unknown) parameter values.
Figure 7.12 presents the prior, likelihood, posterior, and posterior
predictive PDFs. Samples ✓s are represented by dots on the poste￾rior. Note how the posterior predictive does not exactly correspond
with the true PDF for R, which has for true values µˇR = 42 MPa
and ˇ R = 2 MPa. This discrepancy is attributed to the fact that
only 3 observations are available, so epistem ic unce rtainty remains
in the estimation of parameters {µR , R}. In figure 7.12, this uncer- tainty translates in heavier tails for the posterior predictive than for
the true PDF.8
Regression
Data D = {(xi, yi ), 8i 2 {1 : D}}
xi 2 R
X
: 8
<
:
Covariate Attribute Regressor yi 2 R : Observation Model
g(x) ⌘ fct(x)
Regression consists in the task of modeling the relationships be￾tween a system’s responses and the covariates describing the
properties of this system. Here, the generic term system is em- ployed because regression can be applied to many types of prob lems. Figure 8.1 presents examples of applications in the field of civil engi- neering; for instan ce , one could model the relationship between (a)
the number of pathogens and chlorine concentration, (b) the num- ber of accidents at a road intersection as a function of the hour of
the day, and (c) conc re te strength as a function of the water-cement
ratio. These examples are all functions of a single covariate; later in
this chapter we will see how regression is generalized to model the
responses of a system as a function of any number of covariates.
[ ] cl+
Pathogens[ppm]
(a)
hour of the day # of accidents
(b) Co Water/cement ratio ncrete strength
(c)
Figure 8.1: Examples of applications of regression analysis in the context of civil engineering.
In regression problems, the data sets are described by D pairs of
observations D = {(xi, yi), 8i 2 {1 : D}} where xi = [x1 x2 · · · xX]|
i 2
RX are covariates , also called attributes or regressors, an d yi 2 R
1
is
an observed system response. A regression model g(x) ⌘ fct(x) 2 R1
is thus a X ! 1 function, that is, a function of X covariates leading
to one output. This chapter covers three regression methods for
building g(x): linear regression, Gaussian process regression, and
neural networks.
8.1 Linear Regression
Linear regression is one of the easiest methods for approaching
regression problems. Contrarily to what its nam e may lead us to
think, linear re gression is not limite d to cases where the relation￾ships between the covariates and system responses are linear. The
term linear refers to the linear dependence of the model with re￾spect to its parameters. In the case involving a single covariate x,j.-a. goulet 108
the model employed in linear regression takes the form
g(x) = b0 + b11(x) + b22(x) + ... + b BB(x), where ✓ = b = [b0 b1 · · · bB]|
is a vector of model parameters and
i(x) is a basis function applied on the covariate x. Basis functions
are nonline ar functions employed whe n we want to apply linear
regression to describe nonlinear relationships. Figure 8.2 presents
two examples of linear regression applied to (a) linear and (b)
nonlinear relationships.
0 0.2 0.4 0.6 0.8 1
0
2
4
6
x
y
training set g(x)
(a) Linear: g(x) = b0 + b1x
0 0.2 0.4 0.6 0.8 1 4 2
0
2
4
6
x
y
training set g(x)
(b) Nonlinear: g(x) = b0 + b1 sin(10x)3 Figure 8.2: Examples of linear regression
applied to linear and nonlinear relation- ships.
8.1.1 Mathematical Formulation
For the simplest case where there is only one covariate, the data
set consists in D pairs of covariates and system responses D =
{(xi
, yi), 8i 2 {1 : D}}, where a covariate xi 2 R, and its associated
observation yi 2 R. The observation model is defined as
y
|{z} Observation=
Model z}|{ g(x) + v, v : V ⇠ N(v; 0, 
2
V
) | {z } Observation error
, where v describes the observation errors that are assumed to be
independent of each other, Vi ? Vj
, 8i 6= j. Note that observed
covariates x are assumed to be exact and free from any observation
errors. Le t us consider the special case of a linear basis function
(x) = x where the model takes the form
g(x) = b0 + b1x = [b0 b1]
1
x

. (8.1)
We can exp re ss the model for the entire data set using the following
matrix notation, Note: The 1 in
⇥1x
⇤
stands for the absence of dependence on a covariate for the bias parameter b 0. b =

b0
b1

, y =
2
6
6
4
y1
y2
.
.
.yD
3
7
7
5
, X =
2
6
6
4
1 x1
1 x2
.
.
.
.
.
. 1 xD
3
7
7
5
, v =
2
6
6
4
v1
v2
.
.
.vD
3
7
7
5
. With this matrix notation, we reformulate the observation model as
y = Xb + v, v : V ⇠ N (v; 0, 
2
V
· I), where I is the identity mat rix indicat ing that all measurement
errors are independent from each other and have the same variance.
If we separate the set of observations D = {Dx, Dy } by isolating
covariates Dx and syste m responses Dy , the joint likelihood ofprobabilistic machine learning for civil engineers 109
observed system responses given the set of observed covariates and
model parameters ✓ = b is expressed as
f(Dy |Dx , ✓) ⌘ f(Dy = y|Dx = x, ✓ = b) = Y
D
i=1
N (yi
; g(xi), 
2
V
)
/ Y
D
i=1
exp 

1
2
yi  g(xi )2

2
V
!
= exp 

1
2 X
D
i=1
yi  g(xi)2

2
V
!
= exp ✓
1
2
2
V
(y  Xb)|
(y  Xb)◆
. The goal is to estimate optimal parameters ✓
⇤ = b
⇤
, which maxi- mize the likelihood f (Dy |Dx, ✓), or equivalently, which maximize
the log-likelihood, Note: Because we have assumed that V
is the same for all observations, optimal
parameters values ✓
⇤ are independent of
the observation-error standard deviation. ln f (Dy |Dx , ✓) = 
1
2
2
V
(y  Xb)|
(y  Xb)
/ 
1
2
(y  Xb)|
(y  Xb). The set of data D employed to obtain the maximum likelihood Note: arg max b
ln f (Dy|Dx, ✓) ⌘ arg min
b
J(b) estimate (MLE) for parameters ✓ = b is called the training set. In the context of linear regression, the object ive of maximizing
ln f (Dy |Dx, ✓) is often replaced by the equivalent objective of
minimizing a loss function J(b) corresponding to the sum of the
squares of the prediction errors,
J(b) =
1
2 X
D
i=1
y i  g(xi) 2 =
1
2
(y  Xb)|
(y  Xb) =  ln f(Dy|Dx, ✓). Figure 8.3: The minimum of a continuous
function corresponds to the location where
its derivative equals zero. Figure 8.3 presents a loss function J(b) as a function of the model
parameters b. Optimal parameter values ✓
⇤ = b
⇤ are defined as
b
⇤ = arg min
b
1
2
(y  Xb)|
(y  Xb). As for other continuous functions, the minimum of J(b
⇤
) coincide s
with the point where its derivative equals zero,
@J(b
⇤
)
@b = 0 = X|Xb
⇤  X|y. 0 0.2 0.4 0.6 0.8 1
0
2
4
6
x
y
Jtrain (b) = 1.98
training set g(x)
Figure 8.4: Example of linear regression minimizing the performance metric J
computed on the training set.
By reorganizi ng the terms from the last equation, optimal parame￾ters are defined as
b
⇤ = (X
|X)1X
|y. (8.2)j.-a. goulet 110
Figure 8.4 presents an example of linear regression employing a
linear basis function
g(x) = b 0 + b 1x. The optimal parameters found using the training set are b
⇤ =
[1.14 3.71]|, for which Jtrain(b
⇤
) = 1.98.
We can extend the formulation presented above for the general
case where the relationship between x and y is modeled by the
summation of multiple basis functions, g(x) = b0 + b11(x) + b22 (x) + · · · + bBB(x) = Xb, where i (x) can be any linear or nonlinear function, for example:
i(x) = x
2
, i(x) = sin(x), · · · . In a matrix form, this general
model is expressed as
b =
2
6
6
4
b0
b1
.
.
.bB
3
7
7
5
, y =
2
6
6
4
y1
y2
.
.
.yD
3
7
7
5
, X =
2
6
6
4
1 1 (x1) 2(x1 ) · · · B(x1)
1 1 (x2) 2(x2 ) · · · B(x2)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 1  1(xD) 2(xD ) · · · B(xD)
3
7
7
5
.
Remember that with linear regression, the model is necessarily
linear with respect to i(x) and b, yet not x. Figure 8.5 presents
two examples of linear regression applied to nonlinear relationships
between a system response and a covariate.
0 0.2 0.4 0.6 0.8 1
0
2
4
6
x
y
training set g(x)
(a) g(x) = b0 + b1x
2
0 0.2 0.4 0.6 0.8 1 4 2
0
2
4
6
x
y
training set g(x)
(b) g(x) = b0  b 2sin(10x)3 Figure 8.5: Examples of linear regression
applied to nonlinear relationships.
8.1.2 Overfitting and Cross-Validati on
One of the main challenges with linear regression, as with many
other machine learning methods, is overfitting. Overfitting happens
when you select a model class or model structure that represents
the data in your training set well, yet it fails to provide good
predictions for oth er covariates that were not seen during training.
A highly complex model capable of adapting to the training data is
said to have a large capacity.
(a) Low bias &
low variance
(b) High bias &
low variance
(c) Low bias &
high variance
(d) High bias &
high variance
Model complexity
Error
Optimal complexity
(bias)
Valida tion error
Tra ini nin g e rror (variance )
Underfitting Overfitting
(e) Bias-variance tradeo↵
Figure 8.6: Illustration of the bias and
variance when talking about predictive performance and model complexity.
Overfitting with a model that has too much capacity corresponds
to the case (c) in figure 8.6, where prediction errors are displaying a
low bias when evaluated with the dat a employed to train the model;
however, when validated with the true objective of pred ict ing
unobserved data, there is a high variance. Case (b) represents the
situation where a model underfits the data because its low capacity
causes a large bias and a small varianc e. As depicted in (e), we
want the optimal model complexity o↵ering a tradeo↵ between the
prediction bias and variance by minimizing the validation error. This is known as the bias-var iance tradeo↵ . The conc ep t illustrat ed
in figure 8.6 will be revisited at the end of this section.probabilistic machine learning for civil engineers 111
In the context of the linear regression, overfitting arises when
one chooses basis functions (x) that have too much capacity and
that will be able to fit the data in a training set but not the data
of an independent validation set. Figure 8.7a presents an example
where dat a is generated from a linear basis function and where the
regression model also employs a linear basis function to fit the data.
Figure 8.7b presents the case where the same data is fitted with
a model using polynomial basis functions of order 5. Notice how
when more complex basis functions are introduced in the model, it
becomes more flexible, and it can better fit the data in the training
set. Here the fitting ability is quantified by the loss function J that
is lower for the polynomial model than for the linear one.
0 0.2 0.4 0.6 0.8 1
0
2
4
6
x
y
Jtrain (b) = 1.98
training set g(x)
(a) g(x) = b0 + b1x
0 0.2 0.4 0.6 0.8 1
0
2
4
6
x
y
Jtrain
(b) = 1.72
training set g(x)
(b) g (x) = b0 +b1 x+b2x
2 +b3x
3 +b4 x
4 +
b5x
5 Figure 8.7: Example of (a) a model with
correct complexity, and (b) a model with
too much capacity.
One solution to this challenge is to replace the MLE proce￾dure for optimizing the parameters b (see equ at ion 8.2) with the
Bayesian estimation procedure as described in §6.3. The appro￾priate model complexity should then be selected by employing
Bayesian model selection as presented in §6.8. In practice, this is
seldom performed because calculating the evidence f (D|mi) for a
model is computationally demanding. An alternative is to perform
ridge regression,1 which is a special case of Bayesian linear regres￾1 Hoerl, A. E. and R. W. Kennard (1970).
Ridge regression: Biased estimation for nonorthogonal problems. Technomet￾rics 12 (1), 55–67
sion that employs regularization to constrain the parameters (see
§8.3.3).
Another gen eral procedure to identify the optimal model com￾plexity is to employ cross-validation. The idea is to avoid employing
the same data to (1) estimate optimal parameters, (2) discriminate
between model classes, and (3) test the final model performance
using the loss function. To start with, the data is separated into a
training set and a test set. In order to maximize the utilization of
the data available, we can rely on the N-fold cross-validation pro￾cedure, where the training set is again divided into N subsets c alle d
folds; one at a time , each subset is employed as an independent val￾idation set while the training is performed on the remaining folds. The total loss is obtained by summing the loss obtained for each
validation subset. The model class selected is the one with the low- est aggregated loss. Once a model class is selected as the best one,
its parame ters can be retraine d using the entire training set, and
its pre dict ive performance is e valuated on the independent test set
that was not employed to estimate parameters or to discriminate
between model classes.
fold
#
1
2
3
training subset validation subset
entire data set
testing set
test
training set
Figure 8.8: The N -fold cross-validation
procedure.
Figure 8.8 pre se nts an example of a 3-fold cross-validation where
the green -shaded boxes describe the part of the data set employed
to train the model, the blue boxes describe the part of the data set
employed to compare the predictive performance while choosingj.-a. goulet 112
between model classes, and the red box re presents the independent
test set that is only employed to quantify the model performance on
unobserved data. It is essential that for each fold, the training and
validation sets, and the test set are kept separated. Typically, we
would employ 80 percent of the data for the training and validation
sets and 20 percent for the test set.
It is common to employ 10-fold cross-validation because it of￾fers a balance between the computational e↵ort required and the
amount of dat a available to estimate the model parameters. For N
folds, the model needs to be trained N times, which may involve a
substantial computational e↵ort. On the other hand, using few folds
may lead to poor parameter estimates. For example, if we only em￾ploy 2 folds, for each of them the model is only trained on half the
training set. Whe n N = Dtrain, the procedure is called leave-one-out
cross-val idation (LOOCV). This approach leads to the best esti- mation of validation errors because more dat a is employed during
training, yet it is the most computationally demanding. Algorithm
7 summarizes the N-fold cross-validation procedure for choosing
between M model classes. Algorithm 7: N-fold cross-validation 1 define {x
train
, y
train} (training set with Dtrain observations)
2 define {x
test
, y
test} (test set)
3 define gm (x), 8m 2 {1 : M} (models)
4 define N (N-folds)
5 d = randperm(Dtrain) (random permutation of [1 : Dtrain ]|
)
6 n = round(Dtrain/N) (# observations per fold)
7 for m 2 {1, 2, · · · , M} do
8 for i 2 {1, 2, · · · , N} do
9 xt = x
train
; yt = y
train
(initialize training set)
10 k =d
⇣n(i  1) + 1

:min(i·n, Dtrain)⌘
(val. set indices)
11 xv = x
train
(k, :); yv = y
train
(k, :) (validation se t)
12 xt(k, :) = [ ]; yt (k, :) = [ ] (remove validation set)
13 X = fct(xt) (training model-matrix)
14 b
⇤
i = (X|X)1X|yt (estimate parameters)
15 X = fct(xv) (validation model-matrix)
16 Ji = J(b
⇤
i
, yv) (i
th validation-set loss)
17 Jm
validation = PN
i=1 Ji (aggregated loss for model m)
18 m⇤ = arg min m
J m
validation
(select best model)
19 X = fct(x
train ) (training model-matrix for m⇤
)
20 b
⇤= (X|X)1X|y
train (estimate parameters)
21 X = fct(x
test ) (test-set model matrix)
22 Jm⇤
test = J(b
⇤
, y
test) (test-set loss for m⇤
)
0 0.2 0.4 0.6 0.8 1
0
2
4
6
x
y
J train (b) = 0.43 | J validation (b) = 1.56
training set validation set g(x)
(a) Fold 1/2
0 0.2 0.4 0.6 0.8 1
0
2
4
6
x
y
J train (b) = 1.53 | J validation (b) = 0.46
training set validation set g(x)
(b) Fold 2/2
Figure 8.9: Example of 2-fold cross- validation for the model g1(x) = b0 + b1x.
0 0.2 0.4 0.6 0. 8 1
0
2
4
6
x
y
Jtrain (b) = 0.7 | J validation (b) = 5.42
training set validation set g(x)
(a) Fold 1/2
0 0.2 0.4 0.6 0.8 1
0
2
4
6
x
y Jtrain (b) = 0.11 | J validation
(b) = 15.47
training set validation set g(x)
(b) Fold 2/2
Figure 8.10: Example of 2-fold cross- validation for the model g2(x) = b0 + b1x +
b2 x
2 + b3x
3 + b4x
4 + b 5x
5
.probabilistic machine learning for civil engineers 113
Figures 8.9 and 8.10 present a 2-fold cross-validation procedure
employed to choose between a linear model g1(x) and a fifth-order
polynomial one g2 (x), g1(x) = b0 + b1 x (linear)
g2(x) = b0 + b1 x + b2x
2 + b3x
3 + b4x
4 + b5x
5
(5
th order). For the linear model, the sum of the losses for the two training and
validation set s is
P Jtrain = 1.97
P Jvalidation = 2.02 )
g1 (x) ,
and for the fifth-order polynomial model, the sum of the losses for
the two training and validation set s is
PJtrain = 0.81
P Jvalidation = 20.89 )
g2 (x) . These results indicate that the fifth-order polynomial function is
better at fitting the training set; however, it fails at predicting
data that was not seen in the training set. This is refl ected in the
loss func tion for the validation set, which is lower for the linear
model having the c orre ct complexity, compared to the fifth-order
polynomial function that is overcomplex.
0 0.2 0.4 0.6 0.8 1
0
2
4
6
x
y
training set validation set g(x)
(a) Fold 1/2
0 0.2 0.4 0.6 0.8 1
0
2
4
6
x
y
training set validation set g(x)
(b) Fold 2/2
Figure 8.11: Example of 2-fold cross- validation for the model g3(x) = b0 + b1x, where the data follows the real model
gˇ(x) = 1 + 5x
4 .
We now come back to the targets presented in figure 8.6; the
model g1 (x) presented in figure 8.9 corresponds to the target in
figure 8.6a, where the complexity is optimal so there is both a low
bias and a low variance. The model g2(x) presented in figure 8.10
corresponds to figure 8.6c, where because the capacity is too high,
the model closely fits the training data in each fold (low bias), yet
the model is drastically di↵erent (i.e., high variance) for each fold. Here, the high model capacity allowed to overfit the noise contained
in the data. Finally, the target in figure 8.6b corre sponds to the
new case illustrat ed in figure 8.11, where the model does not have
enough capacity to fit the data. The result is that the model is
highly biased, yet there is almost no variability between the models
obtained for each fold.
8.1.3 Mathematical Formulation >1-D
Linear regression can be employed for cases involving more than
one covariate. The data set is then D = {(xi, yi), 8i 2 {1 : D}}, where xi = [x1 x2 · · · xX ]|
i 2 RX
is an X by one vector associated
with an observation yi 2 R
1
. The model is an X ! 1 function,
g(x) ⌘ fct(x) 2 R1
, which is defined using basis funct ion s so that
g(x) = b0 + b11 (x1) + b22(x2) + · · · + bBB(xX) = Xb.j.-a. goulet 114
The matrix notation for the model defined over the entire data set
is
b =
2
6
6
6
4
b0
b1
.
.
. bB
3
7
7
7
5
, y =
2
6
6
6
4
y1
y2
.
.
.yD
3
7
7
7
5
, X =
2
6
6
6
4
1 1(x1,1) 2(x1,2) · · · B(x1,X)
1 1(x2,1) 2(x2,2) · · · B(x2,X)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 1 1(xD,1) 2(xD,2) · · · B(xD,X)
3
7
7
7
5
,
where [xi
]j ⌘ xi,j refers to the j
th component of the x vector for
the i
th observation. Figure 8.12 presents an example of multivariate
linear regression for x = [x1 x2]| and for which the model is
g(x) = b 0+ b1x
1/2
1 + b2x
2
2
. Note that even if the model now involves multiple covariates, the
optimal model parameters b
⇤ are still estimated using equation 8.2.
0
0.5
1
0
0.5
1
0
5
10
15
x 1 x2
y
training set g(x)
Figure 8.12: Example of application of
linear regression to a 2-D data set.
8.1.4 Limitations
0 0.2 0.4 0.6 0.8 1
0
2
4
6
x
y
training set g(x)
(a) Example of heteroscedasticity where
the variance of errors is a function of
the covariate x
0 0.2 0.4 0.6 0.8 1
0
2
4
6
x
y
training set g(x)
(b) Example of a bias introduced by an
outlier on the bottom right corner
0 0.2 0.4 0.6 0.8 1
0
2
4
6
x
y
training set
test set g(x)
(c) Example of biased extrapolation for unobserved covariates Figure 8.13: Illustrations of some of the
limitations of the linear regression method
presented in §8.1.1.
Some of the limitations of the linear regression method presented
in §8.1.1 are illustrat ed in figure 8.13: (a) it assumes homoscedastic
errors and is thus not suited for the cases where the variance of
errors de pends on the covariate x, that is, heteroscedastic cases, (b) it is sensitive to outlier s , so a single biased observation may
severely a↵ect the model, and (c) it does not di↵erentiate between
interpolation and extrapolation. Interpolation refers to predictions
made between covariates for which observation s are available to
train the model, and ex trapolation refers to predictions made
beyond the domain the model has been trained for. In (c), the
predictions performed with g(x) are good at interpolating between
data points in the training set, yet they o↵er a poor performance
for extrapolating beyond them on the test data. A main limitation
with the linear regression formulation presented here is that it does
not provide uncertainty e stimat es indicating that its predictive
performance degrades for covariate values that are far from those
available in the training set.
Another key limitation not illustrated in figure 8.13 is that the
performance of linear regression depends on our ability to hand￾pick the correct basis func tion s. For cases involving one or two
covariates, this task is feasible from a simple visual representation
of the data set. This task becomes difficult when the number of
covariates X > 2. In short, line ar regression is simple and parameter
estimation is quick, yet it is outperformed by modern techniques
such as Gaussian process regression and neural networks, which
will be covered in the next sections. Note that the cross-validationprobabilistic machine learning for civil engineers 115
technique covered here is not limited to linear regression and can be
employed with a wide range of machine learning methods.
8.2 Gaussian Process Regression
Gaussian process regression (GPR) works by describing the prior
knowledge of system responses over its covariate domain using a
joint multivariate Normal probability density function (PDF). Then,
it employs the properties of the multivariate Normal (see §4.1.3) in
order to update the joint prior PDF using empirical observations.
Figure 8.14: Example of a 100 km long
pipeline.
Pipeline example Take for example the 100 km long pipeline
illustrated in figure 8.14, for which we are interested in quantifying
the temperature at any coordinate x 2 (0, 100) km. Given that
we know the pipeline temperature to be y = 8
C at x = 30 km, what is the temperature at x = 30.1 km? Because of the proximity
between the two locations, it is intuitive that the temperature has
to be close to 8
C. Then, what is the temperature at x = 70 km?
We cannot say for sure that it is going to be aroun d 8
C because
over a 40 km distance, the change in temperature can reach a few
degrees. This example illustrat es how we can take advantage of the
underlying dependencies between system responses for di↵erent
observed covariates x, for predicting the responses for unob se rved
covariates. Note: As for linear regression (see §8.1), covariates x are assumed to be determinis- tic constants that are free from observation
errors. Let us consider system responses defined by a single covariate xi
so that
gi |{z}
observation=
model z }| { g(xi). The data set is defined as D = {(xi, gi ), 8i 2 {1 : D}} = {Dx , Dg }, where for D observations, Dg = {g} = {[g1 g2 · · · gD]|}, and Dx = {x} = {[x1 x2 · · · xD ]|},
so for each observed gi , there is an associated covariat e xi. A Gaus- sian process is a multivariate Normal random variable defined over
a domain described by covariates. In the case of Gaussian process
regression, a set of system responses is assumed to be a realization
from a Gaussian process g(x) : G(x) ⇠ N (g (x);µ G , ⌃ G), where µG
is the mean vector [µ G] i = µG(xi) and the covariance matrix ⌃ G is
[⌃ G]ij = ⇢(xi
, x j) · G (xi ) · G (xj ), ⇢(xi
, xj ) = fct(xi  xj). ⌃ G defines the covariance between a discret e set of Gaussian ran￾dom variables for which the pairwise correlation ⇢(xi
, xj) betweenj.-a. goulet 116
G(xi) and G(xj) is a function of the distance between the covari- ates xi and xj. Note that µ G and ⌃ G depend implicitly on the
covariates x. Figure 8.15: Representation of the prior knowledge for the temperature along
a pipeline. The dashed horizontal red
line represents the prior mean, and the pink-shaded area is the ±2G confidence
region. Overlaid colored curves represent
five realizations of the Gaussian process, gi : G ⇠ N(g(x); µG, ⌃ G).
Pipeline example (continued) Figure 8.15 illustrates our prior
knowledge of the temperature for coordinates x = [0 1 · · · 100]| km, G ⇠ N (g(x); µG, ⌃ G) | {z } Prior knowledge
, [⌃ G]ij = ⇢(x i, xj )
2
G, where each colored curve represents a di↵erent realization of the
Gaussian process G. The prior means and standard deviations
are assumed to be equal for all locations x, so µG (x) = µG =
0
C and G(xi) = G = 2.5
C. The correlation between the
temperature at two locations G(xi) and G(xj ) is defined here by a
square exponential correlation function
⇢(xi
, xj) = exp ✓
1
2
(xi  xj )2
`
2 ◆
, where ` is the length-scale parameter. Figure 8.16 presents the
square-exponential correlat ion function plotted for parameter values
` = {1, 2, 5}.
Figure 8.16: Square-exponential correlation
function.
In figure 8.15, for a length-scale of ` = 25 km, the corre lat ion for the
three pairs of coordinates x = {30, 31, 70} is
⇢(x1 = 30 km, x2 = 31 km) = 0.999
⇢(x1 = 30 km, x3 = 70 km) = 0.278
⇢(x2 = 31 km, x3 = 70 km) = 0.296
9
=
;
for ` = 25 km. Figure 8.17 presents examples of realizations for the same Gaussian
process using di↵erent lengt h-sc ale parameters.
￾100 20 40 60 80 100
0
10
x [km]
g ￾[ C]
(a) ` = 1 km
￾100 20 40 60 80 100
0
10
x [km]
g ￾[ C]
(b) ` = 10 km
￾100 20 40 60 80 100
0
10
x [km]
g ￾[ C]
(c) ` = 100 km
Figure 8.17: Examples of Gaussian process
realizations for di↵erent length-scale parameter values. The dashed horizontal red line represents the prior mean, and the pink-shaded area is the ±2G confidence
region.
8.2.1 Updating a GP Using Exact Observations
Let us consider D = {(xi, gi), i 2 {1 : D}}, a set of D exact obse rva￾tions (i.e., without observation errors), an d x⇤ = [x⇤1 x ⇤2 · · · x⇤P ]|, a vector of P covariates corresponding to either observed or unob￾served lo cat ion s where we want to obtain predictions. Thprobabilistic machine learning for civil engineers 117
is to obtain the posterior PDF f(g⇤ |x⇤, D). Before obtaining this
posterior PDF, we have to defin e the joint prior knowledge for
the syste m responses at both the observed x and predict ion x⇤
locations,⇢ G
G⇤ 
, µ =
⇢
µG
µ ⇤ 
, ⌃ =
 ⌃ G ⌃ G⇤ ⌃
|
G⇤ ⌃ ⇤

| {z } Prior knowledge
,
where µ G and µ⇤ are, respectively, the prior me ans at observed
and at prediction locations; [⌃ G]ij = ⇢(xi
, xj )
2
G and [⌃ ⇤]ij =
⇢(x⇤i
, x⇤j )
2
G are their respective covariances. The covariance be￾tween observed and predict ion locations is described by [⌃ G⇤ ]ij =
⇢(xi
, x⇤j )
2
G. The posterior probability of G⇤ at prediction locations
x⇤, conditioned on the observations D, is given by fG⇤ |x⇤,D
(g⇤|x⇤, D) = N(g⇤; µ⇤|D
, ⌃ ⇤|D
), where the posterior mean vector and covarianc e
matrix are
µ
⇤|D = µ G⇤ + ⌃
|
G⇤⌃ 1
G (g  µG) ⌃ ⇤|D = ⌃ ⇤ ⌃
|
G⇤⌃ 1
G ⌃ G⇤ . | {z } Posterior knowledge These two equat ions are analogous to those presented §4.1.3 for
the conditional distribution of a multivariate Normal (⌘ Gaussian).
Nomenclature x = [x1 x2 · · · x D]| : Covariates for D
observed locations x⇤ = [x⇤1 x ⇤2 · · · x⇤P]|: Covariates for P
prediction locations g = [g1 g 2 · · · gD ]|: System responses for D
observed locations g⇤ = [ g⇤1 g⇤2 · · · g⇤P ]|: Model predictions
for P locations ⌃ G: Prior covariance of G for observed
locations x
⌃ ⇤: Prior covariance of G⇤ for prediction
locations x⇤ ⌃ G⇤ : Prior covariance of G between
observed and prediction locations, x and x⇤, respectively
Strength and limitations One of the main di↵erences between the
Gaussian process regression and other re gre ssion methods that
encode the relationships contained in the data set through a para￾metric model is that GPR is a nonparametric approach, for which
predictions depend explicitly on the observations available in the
data set. The upside, in comparison with other methods such as
linear regression, is that there is no requirement to define basis
functions that are compatible with the system responses. GPR thus
allows modeling highly nonlinear responses using few parameters. Another key feature of GPR is that it is an intrinsically proba￾bilistic method that allows di↵erentiating between the prediction
quality in interpolation and extrapolation situations. For example,
figures 8.19c and 8.19d show how interpolating between observation
leads to a narrower confidence re gion than when extrapolating.
One of the main downsides of Gaussian process regression is that
when the data set is large ( D > 1000), building the matrices describ￾ing the joint prior knowledge becomes computationally demanding
and it is then necessary to employ approximate methods.2
2 Qui˜nonero-Candela, J. and C. E. Ras- mussen (2005). A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research 6, 1939–1959j.-a. goulet 118
Pipeline example (continued) We consider the case where the
pipeline temperature g2 = 2
C is observed for the covariate x2
and we want to use this information to obtain the posterior PDF
describing the tem perature G1 at a location x1. Given µG = 0
C, G = 2.5
C, and ⇢(x1, x2) = 0.8, the joint PDF describing the prior
probability of the temperature at locations x1 and x2 is defined by
fG 1G2 (g1 , g2) = N (g; µ G, ⌃ G)
8
><
>:
µG = [0 0]| ⌃ G =

2.5
2 0.8 · 2.5
2
0.8 · 2.5
2 2.5
2 
. The conditional probability of G1 given the observation g 2 is
fG1 |g2 (g1 |g2) =
fG 1G2 (g1, g 2)
fG2 (g2) = N (g1; µ
1|2
, 
2
1|2
), where the posterior me an and standard deviation are
µ
1|2 = µ1 + ⇢1
g2µ2 ￾2 = 0 + 0.8 ⇥ 2.5 20
2.5 = 1.6
C

1|2 = 1p
1  ⇢2 = 2.5p
1  0.8
2 = 1.5
C.
￾5
0
5
￾2
0
g1 g2
f G1
,G2(g1, g2)
￾5
0
5
￾2
0
g1 g2
f G1|g2 (g1|g2)
Figure 8.18: Joint Gaussian prior knowl- edge of the temperature at two locations and conditional PDF for the temperature g1, given the observation g 2 = 2.
Again, the se last two equations are analogous to those presented
in §4.1.3. The joint prior and posterior PDFs are presented in
figure 8.18. Figure 8.19 presents the generalization of this update
process for P = 101 predict e d locations x⇤ = [0 1 · · · 100]| km
and using, respectively, D = {0, 1,2, 3} observations. Note how the
uncertainty reduces to zero at locations where observations are
available . Realizations of the updated Gaussian process are then
required to pass by all observed values. The correlation embedded
in the joint prior knowledge allows sharing the information carried
by observations for predicting the system responses at adjacent
unobserved covariate values.
￾100 20 40 60 80 100
0
10
x [km]
g ￾[ C]
µG ± 2￾
µG gi
(a) D = {( [ ] |{z} x
, [ ] |{z}g
)}
￾100 20 40 60 80 100
0
10
x [km]
g ￾[ C]
(b) D = {([31] |{z} x
, [0.4] | {z } g
)}
￾100 20 40 60 80 100
0
10
x [km]
g ￾[ C] (c) D = {([31 70] | {z } x
, [0.4 3.2] | {z } g
)}
￾100 20 40 60 80 100
0
10
x [km]
g ￾[ C] (d) D = {([31 70 30] | {z } x
, [0.4 3.2  0.6] | {z } g
)}
Figure 8.19: Examples of how the Gaussian
process describing the pipeline temperature
evolves as it is updated with di↵ erent data
sets D.
8.2.2 Updating a GP Using Imperfect Observations
We now explore how to extend the formulation presented in the
previous section to the case where observations are contaminated by
noise. The observation model is now defined as
y
|{z}
observation=
model z}|{ g(x) + v
|{z} measurement error
, where v : V ⇠ N (v ; 0, 
2
V
) describes zero-mean independent
observation errors so that Vi ? Vj 8i 6= j. Because g (x) : G(x) is
now indirectly observed through y, we have to redeprobabilistic machine learning for civil engineers 119
knowledge between the observat ions Y and the model responses G⇤ at prediction locations. The joint prior knowledge is described by
⇢ Y
G⇤ 
, m =
⇢
µ Y
µ G⇤ 
, ⌃ =
 ⌃ Y ⌃ Y⇤ ⌃
|
Y⇤ ⌃ ⇤

| {z }
prior knowledge
.
In the case where the observation noise has a mean equal to zero,
the mean vector for observations equals the prior mean for the
system responses, µ Y = µ G. The global covariance matrix is then
defined using
[⌃ Y]ij = ⇢(xi , xj)
2
G + 
2
V
ij , where ij = 1 if i = j
[⌃ ⇤]ij = ⇢(x⇤i
, x⇤j )
2
G ij = 0 if i 6= j
[⌃ Y⇤]ij = ⇢(xi
, x⇤j)
2
G. Given the prediction locations x⇤ and the observations D, the
posterior knowledge for the system responses G⇤ is
fG⇤|x ⇤,D
(g⇤|x⇤, D) = N (g ⇤; µ ⇤|D
, ⌃ ⇤|D
), where the posterior mean vector µ
⇤|D
and covariance matrix ⌃
⇤|D
are
µ ⇤|D = µ G ⇤ + ⌃
|
Y⇤⌃ 1
Y (y  µ Y) ⌃ ⇤|D = ⌃ ⇤  ⌃
|
Y⇤⌃ 1
Y ⌃ Y⇤. | {z }
posterior knowledge
￾100 20 40 60 80 100
0
10
x [km]
g ￾[ C]
µG ± 2￾
µG yi
(a) D = {( [ ] |{z} x
, [ ] |{z} y
)}
￾100 20 40 60 80 100
0
10
x [km]
g ￾[ C]
(b) D = {([31] |{z} x
, [0.4] | {z } y
)}
￾100 20 40 60 80 100
0
10
x [km]
g ￾[ C] (c) D = {([31 70] | {z } x
, [0.4 3.2] | {z } y
)}
￾100 20 40 60 80 100
0
10
x [km]
g ￾[ C] (d) D = {([31 70 30] | {z } x
, [0.4 3.2  0.6] | {z } y
)}
Figure 8.20: Examples of how the Gaussian
process describing the pipeline temperature
evolves as it is updated with di↵ erent
data sets D, with an observation standard
deviation V = 1
C.
Pipeline example (continued) We continue our study of the
pipeline example by considering noisy observations characterized
by a measurement error standard deviation V = 1
C. Figure 8.20
presents the updated Gaussian process for P = 101 prediction loca￾tions x⇤ = [0 1 · · · 100]| km, and using, respectively, D = {0, 1,2,3}
noisy obse rvations. Notice how, contrarily to results presented in
figure 8.19, uncertainty now remains in the posterior at observed
locations. The larger V is, the less information observations c arry
and the less they modify the prior knowledge.
8.2.3 Multiple Covariates
yi |{z} observation=
model z }| { g(xi ) + v
|{z} measurement error xi = [x1 x2 · · · xX ]|
i | {z }
covariates D = {(xi , y i), 8i 2 {1 : D}} = {Dx , Dy}
In the case where the regression is performed for multiple covariates
x = [x1 x2 · · · xX]|
, it is necessary to define a joint correlation func￾tion defi ning the correlation coefficient associated with a pair of co￾variate vectors. Typically, the joint correlation function is obtained
from the product of a marginal correlation function ⇢([xi
]k, j.-a. goulet 120
for each covariate,
⇢(xi , xj ) = Y
X
k=1
⇢

[xi ]k , [xj ]k
. The multivariate formulation for the square-exponential correlation
function is
⇢(xi , xj) = Y
X
k=1
exp ✓
1
2
([xi]k  [xj ]k)2
`
2
k ◆
⌘ exp✓
1
2
(xi  xj )|diag(`)2
(xi  xj )◆
, ￾5
0
5
￾5
0
5
0
0.5
1
[xi] 1￾ [xj ]1
[xi]2￾ [xj ]2
⇢(xi, x j) Figure 8.21: Bivariate square-exponential correlation function with parameters `1 = 1
and `2 = 1.5. where there is one parameter `k for each covariate. Figure 8.21
presents an example of a bivariate square-exponential correlation
function. One relevant feature with this correlation function is that
the lengt h-scales `k are direc tly measuring the importance of each
covariate for predicting the system responses. As a simple heuris- tic, we can say that if `k is greater than two times the empirical
standard deviations of the observed covariates Xk , then the k
th
covariate has a limited influe nc e on the system response predictions. The reason is because the correlation coefficient remains close to
one for any pairs of this k
th covariate. In the opposite case , where
`k is smalle r than the empirical standard deviation Xk , we can
conclude that the k
th covariate has a strong influence on the system
response predictions.
8.2.4 Parameter Estimation
Up to now, we have assumed that we knew the parameters such as
the observation standard deviation V, the Gaussian process prior
standard deviation G, and the length-scales ` = [`1 `2 · · · `X]|
. In
practice, we need to learn these parameter s ✓ = [V G `
|
]|
from ✓ = [V G `|]| can be referred to
as hyperparameters because they are parameters of the prior. the training data D = {Dx, Dy} = {(xi, yi ), i 2 {1 : D}}. Following
Bayes rule, the posterior PDF for parameters given dat a is
f(✓|D) | {z }
posterior =
likelihood z }| { f(Dy|Dx , ✓)·
prior z}|{ f(✓)
f(Dy ) | {z }
constant
. Often, evaluating f(✓|Dx , Dy) is computationally expensive, so a
maximum likelihood estimation approach (see §6.7) is typically
employed. The optimal parameter values ✓
⇤ are then
✓
⇤ = arg max
✓
likelihood
z }| {
f(Dy|Dx, ✓) ⌘ arg max
✓
log-likelihood
z }| {
lnf(Dy |Dx,probabilistic machine learning for civil engineers 121
The likelihood function f(Dy |Dx, ✓) describes the joint prior Theoretically, here, we should refer to
f(Dy|Dx , ✓) as the marginal likelihood be- cause the hidden variables g are marginal￾ized. probability density of observations Dy, given a set of parameters ✓, f(Dy|Dx, ✓) =
Z
f(Dy |g) · f(g|Dx, ✓)dg = N (Dy; µ Y, ⌃ Y ). The log-likelihood for a vector of observations y = [y1 y2 · · · yD] |
is
given by
ln f (Dy|Dx, ✓) = ln N (y; µY, ⌃ Y) = 1
2
(yµ Y)| ⌃ 1
Y (yµ Y) 1
2
ln |⌃ Y |D
2
ln 2⇡. The optimal parameters ✓ Figure 8.22: Log-likelihood maximization. ⇤ correspond to the parameter values for
which the derivative of the likelihood equals z ero, as illustrated in
figure 8.22. For µY = 0, the derivative of the log-likelihood is
@
@✓j
ln f (Dy |Dx , ✓) =
1
2 y
|⌃ 1
Y
@⌃ Y
@✓j ⌃ 1
Y y 
1
2
tr✓
⌃ 1
Y
@⌃ Y
@✓j ◆
. Efficient gradient-based optimization algorithms for learning pa￾rameters by maximizing the log-likelihood are already implemented
open-source packages such as the GPML3
, GPStu↵
4, and pyGPs5
3 Rasmussen, C. E. and H. Nickisch (2010). Gaussian processes for machine learning
(GPML) toolbox. The Journal of Machine Learning Research 11, 3011–3015
4 Vanhatalo, J., J. Riihim¨aki, J. Har- tikainen, P. Jyl¨anki, V. Tolvanen, and
A. Vehtari (2013). GPstu↵: Bayesian mod- eling with Gaussian processes. Journal of
Machine Learning Research 14, 1175–1179
5 Neumann, M., S. Huang, D. E. Marthaler, and K. Kersting (2015). pyGPs: A python
library for Gaussian process regression and
classification. The Journal of Machine Learning Research 16(1), 2611–2616
toolboxes.
8.2.5 Example: Soil Contamination Characterization
This sect ion presents an applied example of GPR for the character￾ization of contaminant c onc entration in soils. Take, for example, a
set of observations yi of soil contaminant concentrations, D = {Dx, Dy } = {(li
, yi ), i 2 {1 : 116}}, where li is a covariate vector associated with each concentration
observation. These covariates denote the geographic coordinates
(longitude, latitude, and depth) of e ach observation,
li = [x y z]|
i | {z }
geodesic coordinates
. Each observation is represented in figure 8.23 by a circle where its
size is proportional to the contaminant concentration. The goal
is to employ Gaussian process regression in order to model the
contaminant concentration across the whole soil volume, given the
available observations for 116 discrete locations.
-8
-4
z [m]0
100
200
x [m]
300 100
50
400 0
-50
500
y [m]
Figure 8.23: Example of application of
GPR for soil contamination characteri- zation. The iso-contours correspond to Pr([ ] > 55mg/kg) = 0 .5, and the size of each circle is proportional to the observed
contaminant concentration. (Adapted from Quach et al. (2017).)
Because a concentration must be a positive number, and be￾cause several of the observations available are close to zero, the
observation model is defined in the log-space,
ln y
|{z}
observation=
true contaminant [ ] in log space
z}|{
g(l) + v
|{z} meas. error in log space
,j.-a. goulet 122
where g(l) models the natural logarithm of the true contaminant
concentration. When tran sformed back to the original space using
the exponential function, the model predictions are restricted to
positive values. The observation errors are assumed to be desc ribed
by the ze ro-me an independent Normal random variable v : V ⇠ N(v ; 0, 
2
V
), Vi ? Vj , 8i =6 j. The correlation between covariates is
modeled using a square-exponential correlation function, and the
prior mean at any location is assumed to be equal to one in the
original spac e, which corresponds to zero in the log-transformed
space.
The paramet ers to be estimat ed from the data are the length￾scale along e ach direct ion, the Gaussian process standard deviation,
and the observation noise standard deviation,
✓ = {`x, `y, `z,  G, V }. The maximum likelihood estimat e (MLE) parameter values are
✓
⇤ = arg max
✓
log-likelihood z }| {
ln f (Dy|Dx , ✓) =
8
>>
<
>>
:
`x = 56.4 m
`y = 53.7 m
`z = 0.64 m
G = 2.86
V = 0.18
9
>>
=
>>
;
.
The optimal values for the length-scales {`x, `y , `z} are all smaller
Covariate importance metric
If `k  Xk
: Imp(Xk ! y) ⇡ 0
If `k < Xk
: Imp(Xk ! y)  0
than the range of each covariate, as depict ed in figure 8.23. This
confirms that all three coordinates have an influence on the contam￾inant concentration. Figure 8.23 prese nts the iso-contours describing
Pr([ ] > 55mg/kg) = 0.5, where l⇤ are the covariates describing the
x, y, z geographic c oordinate of a 3-D grid covering the entire soil
volume for the site studied . Further details about this example are
presented in the paper from Quach et al.6 6 Quach, A. N. O., D. Tabor, L. Dumont, B. Courcelles, and J.-A. Goulet (2017). A machine learning approach for character￾izing soil contamination in the presence of physical site discontinuities and aggre- gated samples. Advanced Engineering
Informatics 33, 60–67
8.2.6 Example: Metamodel
In civil engineering, it is common to employ computationally de￾manding models such as the finite element models pre se nted in
figure 8.24. When performing probabilistic studies related to struc￾tural or system reliability, it is often required to have millions of
model realizations for di↵erent parameter values. In such a case,
for the purpose of compu tational efficiency, it may be interesting
to replac e the finite element model with a regression model built
from a set of simulated responses. Such a model of a model is called
a metamodel or a surrogate model. Because observations are the
output of a simulation, the observation model does not include any
observation error and follows the formulation presented in §8.2.1.probabilistic machine learning for civil engineers 123
| {z }
gi |{z}
simulation=
model z }| { g(xi), xi = [x1 x2 · · · xX
]|
i | {z }
covariates
Figure 8.24: Examples of finite element structural models.
In this example, we built a metamodel for the Tamar Bridge
finite element model.7 Figure 8.25a presents an overview of the
7 Westgate, R. J. and J. M. W. Brownjohn
(2011). Development of a Tamar Bridge finite element model. In Conference Proceedings of the Society for Experimental
Mechanics, Volume 5, pp. 13–20. Springer
bridge finite element model, and figure 8.25b presents the modal
displacement associated with its first natural frequency, which is
the response modeled in this example. A set of D = 1000 model
(a) Model representation (b) 1
st vibration mode
Figure 8.25: Tamar Bridge finite element model.
simulations is obtained by evaluating the finite element model for
several sets of param et ers that are generated from the prior f(✓). There are five model parameters that are treated as covariates of
the GP regression model, xi = [x1 x2 · · · x5]|
i
: the sti↵ness of
the three expansion joints located on the bridge deck, along with
the initial strain in the cables on the main and side spans. The
locations of elements a↵ected by these parameters are illustrated in
figure 8.26. This problem employs the square-exponential correla￾Saltash side Plymouth side
Saltash side deck
expansion joint stiffness Plymouth side deck
expansion joint stiffness Saltash tower deck
expansion joint stiffness
Main cable
initial strains
Sidespan cables
initial strains Figure 8.26: Tamar Bridge model parame- ter description.j.-a. goulet 124
tion function, so there is a total of six parameters to be learned,
✓ = {`1, `2 , `3, `4, `5, G }. Figure 8.27 compares the GPR model predictions with the true
finite ele ment model outputs gi
. Note that in order to obtain a
meaningful comparison between predicted and measured values,
it is essential to test the model iteratively using a cross-validation
procedure, where at each step, the observation c orresponding to
the pred ict ion location in the validation se t is removed from the
training set.
0.34 0.36 0.38 0.4
0.34
0.36
0.38
0.4
g
g⇤
g = g⇤
(gi , µ⇤|D,i)
µ⇤|D,i± ￾⇤|D,i
Figure 8.27: Comparison of the GPR model predictions with the finite-element model predictions using the leave-one-out
8.2.7 Advanced Considerations cross-validation procedure. There are several aspects that should be considered in order to take
advantage of the full potential of Gaussian process regre ssion. Key
aspects are related to (1) the definition of the prior mean functions, (2) the choice of covariance and correlation functions, and (3) the
formulation required for modeling cases involving heteroscedastic
errors. Prior mean functions In the examples presented in §8.2, the
prior mean was always assumed to be a known constant. If needed,
this assumption can be relax ed by treating the prior mean as a
parameterized function to be inferred from data. In its simplest
case, the mean can be assumed to be a single unknown constant. In more advanced cases we could describe our prior knowledge of
the mean as a parameterized function that depends on covariates
x. For example, if there is a single covariate, the mean could be
modeled by the affine function m(x) = ax + b, where {a, b} are
parameters that ne ed to be estimat ed from data. When using
complex funct ion s to describe the prior mean, it is essential to
keep in mind that this procedure is sensitive to overfitting in a
similar way as linear regression (see §8.1.2). Employing a highly
flexible parameterization may lead to an increased log-likelihood
in the training set, yet it may not materialize in the prediction
performance evaluated on the test set. It is therefore essential
to choose the best prior parameterization using methods such as
cross-validation (see §8.1.2) or Bayesian model selection (see §6.8).
Covariance functions In this section, only the square-exponential
correlation function was described; nonetheless, several other
choices exist. In his the sis, Duvenaud
8 presents several covariance
8 Duvenaud, D. (2014). Automatic model construction with Gaussian processes. PhD
models. A common one is the Matern-class correlation function, for thesis, University of Cambridgprobabilistic machine learning for civil engineers 125
which a special case is defined as
⇢(xi
, xj) = exp ✓
|xi  xj
|
` ◆
. Figure 8.28 represents this correlation function for several length￾scale values. Figure 8.28: An example of Matern-class correlation function. When using the square-exponential correlation function with
covariate values that are close to each other, it is c ommon to obtain
correlation coefficients ⇢(xi
, xj) close to one. This has the adverse
e↵ect of leading to singular or near-singular covariance matrices
(see §2.4.2). M at ern-class correlat ion functions can mitigate this
problem because of its rapid reduction of the correlation coefficient, with an increase in the absolute distance between covariates. Another correlat ion function particularly useful for engineering
applications is the periodic one defined as
⇢(xi, xj ) = exp 

2
`
2
sin✓
⇡
xi  xj
p ◆2 !
.
￾10 ￾5 0 5 10
0
1
x i ￾ xj
⇢(xi, xj)
(a) p = 5, ` = 1
￾10 ￾5 0 5 10
0
1
x i ￾ xj
⇢(xi, xj )
(b) p = 5, ` = 0.5
￾10 ￾5 0 5 10
0
1
x i ￾ xj
⇢(xi, xj )
(c) p = 10, ` = 1
Figure 8.29: Examples of periodic correla- tion functions.
Figure 8.29 presents examples of the period ic correlation function
for periods p = {5,10} and lengt h-scale parameters ` = {0.5, 1}. As its name indicates, this function is suited for modeling periodic
phenomena, where events spaced by a distance close to the period p
will have a high corre lation. The len gt h-scale ` controls how fast the
correlation decreases as the distance between covariates gets away
from an integer that is a multiple of the period p. Figure 8.30 presents an example of the periodic correlation
function employed to model the historical daily average temper- atures re corded at Montreal airport from 1953 to 1960. The blue
circles represent the training data that were employed to learn the
parameters: the prior mean value µ, the Gaussian process prior
standard deviation G, the observation stan dard deviation V , and
1953
1954
195
1956
1957
1958
1959
196
￾ 0
40
￾20
0
20
40
Year
Temperature [ ￾C]
µ⇤|D ± 2q ￾⇤|D+ ￾
2
V
Training data Test data µ⇤|D
Figure 8.30: Example of application of a periodic covariance function to model
temj.-a. goulet 126
the period p. The pink crosses represent the test data that was
not employed to perform predictions or to learn paramete rs. Most
observations from the test set lie within the ±2 posterior interval. It shows how Gaussian process regression can, even with such a
simple model with only four parame ters, capture the temperature
pattern as well as its uncertainty. For more complex problems, highly complex covariance struc￾tures can be built by adding together simpler ones, such as those
described in this section (see Rasmussen and William s9
for details).
9 Rasmussen, C. E. and C. K. Williams
(2006). Gaussian processes for machine
learning. MIT Press The choice regarding which correlation or covariance structure is
most approp riate should, as for the prior mean functions, be based
on methods such as cross-validation (see §8.1.2) or Bayesian model
selection (see §6.8).
Homo- and heteroscedasticity So far, this section has only treated
cases associated with homoscedastic errors such as the generic case
presented in figure 8.31a. In engineering applications, it is common
to be confronted with cases where the variances 
2
V or 
2
G depend
on the covariate values. A generic example of heteroscedastic cases
is pre sented in figu re 8.31b, which has been modeled by considering
that the observation-error standard deviation V is itself modeled
as a Gaussian process that depends on covariate values.10 This
10 Tolvanen, V., P. Jyla¨nki, and A. Vehtari
(2014). Expectation propagation for nonstationary heteroscedastic Gaussian
process regression. In IEEE International
Workshop on Machine Learning for Signal
Processing (MLSP), 1–6. IEEE
method is implemented in the GPstu↵
11
toolbox.
11 Vanhatalo, J., J. Riihim¨aki, J. Har- tikainen, P. Jyl¨anki, V. Tolvanen, and
A. Vehtari (2013). GPstu↵: Bayesian mod- eling with Gaussian processes. Journal of
Machine Learning Research 14, 1175–1179
x
y
(a) Homoscedastic x
y
(b) Heteroscedastic
Figure 8.31: Examples of homo- and
heteroscedastic cases where the shaded
regions represent µY ± 2 Y .
8.3 Neural Networks
The idea behind neural networks originates from the perceptron
model12 developed in the 1950s. Neural networks became extremely 12 Rosenblatt, F. (1958). The perceptron: A
probabilistic model for information storage and organization in the brain. Psychological
Review 65(6), 386
popular in the period from 1980 until the mid-1990s when they
became overshad owed by other emerging methods. It was not until
the early 2010s, with the appellation of deep learning, that they
again took the leading role in the development of machine learning
methods. In deep learning, the label deep refers to the great numberprobabilistic machine learning for civil engineers 127
of layers in the model. The rise of deep learning as the state-of-the- art meth od was caused by three main factors:13
(1) the increasing
13 Goodfellow, I., Y. Bengio, and
A. Courville (2016). Deep learning. MIT
Press size of the data sets to train on, (2) the increase in c omp utational
capacity brought by graphical processing units (GPUs), and (3)
the improvements in the methods allowing to train deeper models. One limitat ion is that in order to achieve the performance it is
renowned for, deep learning requires large data sets containing
from thousands to millions of labeled example s for covariates xi
and syste m responses yi
. In the field of computer vision, speech
recognition, and genomics, it is common to deal with data sets of
such a large size; in the civil engineering context, it is not yet the
case.
This sect ion presents the basic formulation for the simplest
neural ne twork architecture: the feedforward neural network. The
reader interested in a detailed review of neural networks and their
numerous architectures—such as convolutional neural networks
(CNN), generative adversarial networks (GAN), and recurrent neu- ral networks (RNN, LSTM)—should refer to specialized text books
such as the one by Goodfellow, Bengio, and Courville (2016).
8.3.1 Feedforward Neural Networks
Feedforward is the simplest architecture for neural networks, where
information is tran sferred from the input layer to the output layer
by propagating it into layers of hidden variables. The idea is to ap￾proximate complex functions by a succession of simple combinations
of hidden variables organized in layers.
x1
x2
x3
b
z y
w1 w 2 w3
Figure 8.32: Representation of linear regres- sion as a feedforward neural network. The
leftmost nodes represent covariates, the white node is a hidden variable, and the
rightmost node represents the observation.
Linear regression The linear regression method presented in §8.1 is
employed to introduce the concepts surroun ding feedforward neural
networks. Figure 8.32 pre sents a regression problem where three
covariates x = [x1 x2 x3 ]| are associated with a single observed
system response y = z + v, v : V ⇠ N (v; 0, 
2
V
), where v is a realiza￾tion of a zero-mean normal observation error. z represents a hidden
variable, which is defined by the linear combination of covariate
values using three weight parameters and one bias parameter,
z = w1x1 + w2x2 + w3x3 + b.
In figure 8.32, the leftmost nodes represent covariates, the white
node is a hidde n variable, and the rightmost node is the observa￾tions yi
: Y ⇠ N (y; z, 
2
V
). Arrows represent the dependencies
defined by the weight and bias parameters {w1 , w2 , w3 , b}. Weight
parameter wi describes the slope of a plane with respect to eachj.-a. goulet 128
variable, and b the plan e’s intercept (e.g., see figure 2.2). With the
current configuration, this model is limited because it is only a
linear funct ion , that is, the hidden variable z is a linear function
with respect to covariates x.
Linear regression with multiple layers Now we explore the e↵ect of
modifying the model as presented in figure 8.33 by adding multiple
hidden variables, where e ach one is modeled as a linear combination
of either the covariates or the hidden variables on the preceding
layer. First, let us define the notation employed to describe this
network: z
(l)
i represents the i
th hidden variable on the lth layer. In
the first layer, the two hidden variab les are defined by
z
(1)
1 = w
(0)
1,1 x1 + w
(0)
1,2x2 + w
(0)
1,3 x3 + b
(0)
1
z
(1)
2 = w
(0)
2,1 x1 + w
(0)
2,2x2 + w
(0)
2,3 x3 + b
(0)
2
,
x1
x2
x3
b
(0) 1
b
(0) 2
z
(1) 1
z
(1) 2
b
(1)
z
(2) 1 y
Figure 8.33: Representation of linear
regression using multiple layers. The
leftmost nodes represent covariates, the white nodes the hidden variables, and the
rightmost node the observation.
Nomenclature
z
(l)
i
: the i
th hidden variable on the lth
layer w
(l)
i,j
: the weight defining the dependence between the i
th variable of the (l + 1)
th
layer and the j
th hidden variable of the
(l)
th
layer b
(l)
i
: the plane’s intercept for the i
th hidden
variable of the (l + 1)
th layer xj: j
th covariate
where w
(0)
i,j
is the weight defining the dependence between the j
th
variable of the 0
th
layer (i.e., the first covariate itself) and the i
th
hidden variable of the 1
st
layer. Following the same notation, b
(0)
i
describes the plane’s intercept for the i
th hidden variable of the 1
st
layer. The only hidden variable on the second layer is defined as a
linear combination of the two hidden variables on layer 1, so
z
(2)
1 = w
(1)
1,1z
(1)
1 + w
(1)
1,2z
(1)
2 + b
(1)
. The observation model is now defined by y = z
(2)
1 + v, v : V ⇠ N(v ; 0, 
2
V
), so each observation of the system response is defined
as yi : Y ⇠ N (y; z
(2)
1
, 
2
V
). Note that z
(2)
1
implicitly depends on
the covariates x, so that there are covariates implicitly associated
with each observation yi
. The complete data set is D = {Dx, Dy} =
{(xi
, yi), 8i 2 {1 : D}}. In this second case, adding hidden variables did not help us
to generalize our model to nonlinear system responses because
linear functions of linear functions are themselves still linear. The re￾fore, no matter how many layers of hidden variable s we add, the
final model remains a hyperplane. In order to describe nonline ar
functions, we need to intro duce nonlinearities using activation
functions. Activation functions An activation function describes a nonlinear
transformation of a hidden variable z. Common activation functions
are the logistic, the hyperbolic tangent (tanh), and the rectifiedprobabilistic machine learning for civil engineers 129
linear unit (ReLU),

s (z) =
1
1 + exp(z) =
exp(z)
exp(z) + 1
(logistic)

t (z) =
e
z  ez e
z + ez (tanh)

r (z) = max(0, z) (ReLU). These three activation funct ion s are presented in figure 8.34. The
ReLU activation function is currently widely employed because
it facilitat es the training of models by mitigating the problems
associated with vanishing gradients (see §8.3.2).
￾3 ￾2 ￾1 0 1 2 3 ￾1
0
1
z
￾( ) z
Logistic tanh ReLU
Figure 8.34: Comparison of the logistic,
tanh, and ReLU activation functions.
Feedforward neural networks: A simple case In a feedforward
network, a hidden variable z
(j)
i
is passed through an activation
function,
a
(j)
i =  ⇣
z
(j)
i ⌘
, before getting fed to the next level. The output of the activation
function is calle d the activation unit. Figure 8.35 presents the
expanded and compact forms for a simple feedforward network with
three covariates in its input layer, on e hidden layer containing two
activation units (green nodes), and one output layer containing
a single activation unit (blue node). Note that in the compact
forms (i.e ., (b) and (c)), hidden variables z
(j)
i are not explicitly
represented anymore. The activation units on the first layer are
a
(1)
1 =  ⇣
z
(1)
1 ⌘
a
(1)
2 =  ⇣
z
(1)
2 ⌘
. The outpu t unit a
(O)
is equal to the hidden variable z
(O)
, which is
itself defined as a linear combination of the activation units on the
hidden layer so that
a
(O) = z
(O) = w
(1)
1 a
(1)
1 + w
(1)
2 a
(1)
2 + b
(1)
. The observation model is now defined by y = a
(O) + v, v : V ⇠ N(v ; 0, 
2
V
), so each observat ion of the system response is defined as
yi : Y ⇠ N (y; a
(O)
, 
2
V
).
x1
x2
x3
b
(0)
z
(1) 1
z
(1) 2
a
(1) 1
a
(1) 2
b
(1)
z
O a
O y
(a) Expanded-form variable nomenclature
x1
x2
x3
b(0) b
(1)
y
(b) Compact-form representation of activation functions
x1
x2
x3
b(0)
a
(1) 1
a
(1) 2
b
(1)
a
(O) y
(c) Compact-form variable nomenclature Figure 8.35: A simple feedforward neural network.
Fully connected feedforward neural network We now describe
formulation for the fully connected feedforward neural net work
(FCNN) with L hidden layers, each having A activation units. The
example presented includes a single output observation. In practice , FCNN can be employed for any number of outpu t variaj.-a. goulet 130
Figure 8.36 presents a graphical representation for an FCNN
where (a) presents the connectivity with the activation functions
and (b) the nomenclature employed for weights w
(l)
i,j and hidden
units a
(l)
i
. The activation unit i in the first layer is described by a
linear combination of covariates,
a
(1)
i =  ⇣
z
(1)
i ⌘ =  ⇣w
(0)
i,1 x1 + w
(0)
i,2 x2 + · · · + w
(0)
i,XxX + b
(0)
i ⌘
.
In subseq ue nt layers, the activation unit i in layer l + 1 is a linear
combination of all the activation units in layer l as described by
x1
x2
.
.
.
xX
b
(0)
.
.
.
b
(1)
.
.
.
b
(2)
· · ·
· · ·
.
.
.
· · ·
b
(L1)
.
.
.
b
(L)
y
(a) Representation of the network structure with its activation functions
x1
x2
.
.
.
xX
b
(0)
a
(1)
1
a
(1)
2
.
.
.
a
(1)
A
b
(1)
a
(2)
1
a
(2)
2
.
.
.
a
(2)
A
b
(2)
· · ·
· · ·
.
.
.
· · ·
b
(L1)
a
(L)
1
a
(L)
2
.
.
.
a
(L)
A
b
(L)
a
(O) y
w
(1) w1,1
(1) 1,1 w
(1) 1,2 w
(1) 2,1 w
(1) 1,i w
(1)
i,1 w
(1) 1,A w
(1) A,1
b
(1) 1
(b) Variable nomenclature: The activation unit 1 in layer 2 is a
(2) 1 = ⇣
z
(2) 1 ⌘ = ⇣w
(1) 1,1a
(1) 1 + w
(1) 1,2a
(1) 2 + · · · + w
(1) 1,Aa
(1) A + b
(1) 1 ⌘
Figure 8.36: A fully connected neural network having L hidden layers, each
having A activation units. The leftmost nodes describe the input layer containing X
covariates. The green inner nodes describe
the unobserved hidden layers where an
activation unit i in layer l is a
(l)
i = (z
(l)
i ). The blue node describes the output layer where a
(O) = 
O (z
(O) ) = z
(O)
for regression
problems. The rightmost node describes
the observed system responses, which are
realizations yi : Y ⇠ N (y;a
(O)
, 
2V
).probabilistic machine learning for civil engineers 131
a
(l+1)
i =  ⇣
z
(l+1)
i ⌘ =  ⇣w
(l)
i,1a
(l)
1 + w
(l)
i,2a
(l)
2 + · · · + w
(l)
i,Aa
(l)
A + b
(l)
i ⌘
.
Activation unit in layer l + 1
Hidden variable in layer l + 1
Activation function
Weights Bias
Activation units from layer l
The feedforward propagation for all the units contained in a layer
can be expressed with a matrix notation as a
(l+1) = (z
(l+1) ), that
is, 2
6
6
6
4
a
(l+1)
1
a
(l+1)
2
.
.
. a
(l+1)
A
3
7
7
7
5
A⇥1 | {z }
a(l+1)
= 
0
B
B
B
B
B
B@
2
6
6
6
4
z
(l+1)
1
z
(l+1)
2
.
.
. z
(l+1)
A
3
7
7
7
5
A⇥1 | {z }
z(l+1)
1
C
C
C
C
C
CA
,
where the hidden variables are given by z
(l+1) = W(l)a
(l) + b
(l)
, that
is, 2
6
6
6
4
z
(l+1)
1
z
(l+1)
2
.
.
. z
(l+1)
A
3
7
7
7
5
A⇥1 | {z }
z(l+1)
=
2
6
6
6
4
w
(l)
1,1 w
(l)
1,2
· · · w
(l)
1,A
w
(l)
2,1 w
(l)
2,2
· · · w
(l)
2,A
.
.
.
.
.
.
.
.
.
.
.
. w
(l)
A,1 w
(l)
A,2
· · · w
(l)
A,A
3
7
7
7
5
A⇥A | {z } W(l)
⇥
2
6
6
6
4
a
(l)
1
a
(l)
2
.
.
. a
(l)
A
3
7
7
7
5
A⇥1 | {z }
a(l)
+
2
6
6
6
4
b
(l)
1
b
(l)
2
.
.
. b
(l)
A
3
7
7
7
5
A⇥1 | {z } b(l)
.
The variables z
(O) and a
(O) on the output layer are obtained from a
linear combination of the activation units on the last hidden layer
L,a
(O) = 
O(z
(O)) = z
(O) = w
(L)
1 a
(L)
1 + w
(L)
2 a
(L)
2 + · · · + w
(L)
A a
(L)
A + b
(L)
. Because we are dealing with a regression problem, we can consider
that the output activation func tion 
O(·) is linear. The observation
model is defined by y = a
(O) +v ⌘ a
(O) (x) +v, v : V ⇠ N(v; 0, 
2
V
), so each observation of the system response is defined as y i
: Y ⇠ N (y; a
(O)(x), 
2
V
). Neural networks are characterized by their large number of pa￾rameters to be estimated. For example , a fully connected network
with L layers, each having A activation units, has A ⇥ A ⇥ (L  1)
weight parameters and A ⇥ L bias parame te rs for its hidden layers. For L = A = 10, it leads to more than 1000 parameters to be learned
from the data set D. In comparison, the GPR model presented in
§8.2 had fewer than 10 parameters. Moreover, it is not uncommon
for modern deep neural networks to be parameterized by millions
of paramete rs. As a consequence, learning these parameters (i.e.,j.-a. goulet 132
training the neural network) req uire s large data sets and is a com￾putationally demanding task.
8.3.2 Parameter Estimation and Backpropagation
The task of learning model paramet ers is referred to as training
a neural network. In general, there are so many parameters to be
learned that the utilization of second-order optimization methods
such as Newton-Raphson (see §5.2) are computationally prohibitive. Instead, neural ne tworks rely on gradient descent algorithms (see
§5.1), where the gradient of the loss function with respect to each
parameter is obtained using the backpropagation method.
Log-likelihood As for other types of regression models, the per￾formance of a neural network is measured using quantities derived
from the likelihood of observations f (Dy|Dx, ✓w, ✓b), where {✓w , ✓b}
are respectively the weight and bias parameters. By assuming that
observations are conditionally independent given a
(O)
, the joint
likelihood for a data se t D = {Dx, Dy} = {(xi
, yi), 8i 2 {1 : D}} is
defined as
f(Dy|Dx, ✓w, ✓b ) =Y
D
i=1
N (y i
; a
(O)(xi
; ✓w , ✓b), 
2
V
), where a
(O) (xi
; ✓w, ✓b ) ⌘ a
(O) explicitly defines the dependence of the
output layer on the bias and weight parameters from all layers. In
practice, neural networks are traine d by minimizing a loss function
J(D; ✓w, ✓b), which is derived from the negative log-likelihood, Note: The derivation of the loss function
for a neural network is analogous to the derivation of the loss function presented in
§8.1.1 for linear regression. ln f (Dy|Dx, ✓ w, ✓b ) = X
D
i=1
ln
N (yi; a
(O)(xi; ✓w, ✓b), 
2
V
)
/ 
1
DX
D
i=1
1
2
y i  a
(O)(xi; ✓w, ✓b)2
= 
1
DX
D
i=1
J(yi; xi, ✓w , ✓b) = J(D; ✓w , ✓b). Note that the loss function J (D; ✓w, ✓b) corresponds to the mean- square error (MSE). The optimal set of parameters ✓
⇤ = {✓
⇤w, ✓
⇤
b}
are thus those which minimize the MS E, ✓
⇤ = arg min
✓
J(D; ✓w, ✓b).probabilistic machine learning for civil engineers 133
Gradient descent The optimal values ✓
⇤ are sought iteratively
using a gradient-descent algorithm, where the updated parameter
values are
✓ ✓  r✓J(D; ✓w , ✓b )
and where  is the learning rate (see §5.1). The gradients are esti- mated using the backpropagation algorithm that will be presented
next. The parameters ✓0 = {✓w , ✓b} are typically initialized ran￾domly following ✓i ⇠ N (✓; 0, ✏
2
), where ✏ ⇡ 102
. The random
initialization is intended to avoid having an initial symmetry in the
model. The objective of having initial values close to zero is that
multiplying covariates and hidden variables by weights w
(l)
i,j = 0 will
initially limit the model capacity. Not e that a common practice is
to normalize covariates and observations in the data set.
Backpropagation The backpropagation
14 method employs the 14 Rumelhart, D. E., G. E. Hinton, and
R. J. Williams (1986). Learning repre- sentations by back-propagating errors. Nature 323, 533–536
Feedforward neural network
J =
1
2
(yi  a
(O) )2 a
(O) = 
W(L) a
(L) + b
(L) | {z } z(O)

a
(l+1) = 
W(l)a
(l) + b
(l) | {z } z(l+1)

Chain rule of derivation
@J
@W(L) =
@J
@a
(O)
@a
(O)
@z
(O) | {z } ￾
(O)
i
@z
(O)
@W(L) | {z } a
(L)
i @J
@b(L) =
z }| { @J
@a(O)
@a
(O)
@z(O)
@z
(O)
@b(L) | {z } 1 @J
@W(l) =
@J
@z
(l+2)
@z
(l+2)
@a
(l+1)
@a
(l+1)
@z
(l+1) | {z } ￾
(l+1)
i
@z
(l+1)
@W(l) | {z } a
(l)
i @J
@b(l) =
z }| { @J
@z(l+2) | {z } ￾
(l+2)
i
@z
(l+2)
@a(l+1) | {z } W(l+1)
@a
(l+1)
@z(l+1) | {z } r￾(z(l+1) )
@z
(l+1)
@b(l) | {z } 1
chain rule of derivation in order to backpropagate the model pre￾diction errors through the network. For a single observation (xi, yi), the pred ict ion error 
(O)
i
is define d as the discrepancy between the
predicted value a
(O)(xi
;✓w , ✓b) and the observation yi
, times the
gradient of the output’s activation function,

(O)
i = @
@z
(O) J(yi; xi, ✓ w, ✓b) = @
@a
(O) J(yi
; xi
, ✓w, ✓b) · @a
(O)
@z
(O) = 
yi  a
(O)(xi
; ✓w , ✓b) 
· @
@z
(O) 
O(z
(O)) (general case) = a
(O)(xi
; ✓ w, ✓b )  yi (regression).
In the context of regression, the term @
@z
(O) 
O(z
(O)) is equal to one
because a
(O) = 
O(z
(O)) = z
(O)
. Note that in order to evaluate the
prediction error for an observation yi, we first need to evaluate the
model prediction for the current parameter values {✓w, ✓b} and a
vector of covariat es xi
. For the i
th observation in the training set,
the gradient vector of the loss function with respe ct to either weight
or bias parameters on the lth
layer is
rW(l)
i J(yi
; xi
, ✓w , ✓b) = @J
@W(l) = 
(l+1)
i (a
(l)
i )|
rb
(l)
i J(yi
; xi
, ✓w , ✓b) = @J
@b(l) = 
(l+1)
i
. For hidden layers, the responsibility of each activation unit for the
prediction error is estimated using

(l+1)
i =
⇣
(W(l+1))|
(l+2)
i ⌘  r(z
(l+1)), where  is the Hadamar (element-wise) product, and r denotes the
gradient vecj.-a. goulet 134
The common procedure is to perform parameter updates not
using each observation individually, but by using mini-batches. A
mini-batch Dmb ⇢ D is a subset of observations of size n that is
a power of 2 ranging from 32 to 256. The power of 2 constraint
is there to maximize the computational efficiency when working
with grap hical processing units (GPUs). The gradient of the loss
function is then the average of the gradients obtained for each
observation in a mini-bat ch,
Note: Several of the advances made
in neural networks can be attributed
to advancements in GPU calculations. Nowadays most nontrivial models are
trained using GPUs. rW(l) J(Dmb
; ✓w, ✓b) =
1
n X
i:yi 2D
mbrW(l)
i J(yi
; xi
, ✓w , ✓b )
rb (l) J(Dmb
; ✓w, ✓b) =
1
n X
i:yi 2D
mbrb
(l)
i J(y i
; xi
, ✓w, ✓b ). During training, an epoch corresponds to performing one pass of
parameter update for all mini-batches contained in the training
set. Neural networks are typically trained using stochastic gradient
descent, which computes the gradient on the subset of data con￾tained in mini-batches rather than on the full data set. The reader
should re fe r to specialized literature
15 for descriptions of stochastic 15 Goodfellow, I., Y. Bengio, and
A. Courville (2016). Deep learning. MIT
Press gradient descent optimization methods such as Adagrad, RMSprop, and Adam.
8.3.3 Regularization
Because of the large number of parameters involved in the def￾inition of a neural network, it is prone to overfitting, where the
performance obtained during training will not generalize for the
test set . Here, we present two regularization procedures to prevent
overfitting: weight decay and early stopping. Weight decay The first regularization strategy is to employ maxi- mum a-posteriori (MAP) estimation rather than a maximum likeli- hood estimate (MLE). The prior knowledge for model parameters
in an MAP allows us to prevent overfitting by limiting the model
complexity. For instance, we can define a prior for the weights in
the vector ✓w so that f (w
(l)
i,j) = N(w
(l)
i,j
,0, 1/↵). A large ↵ value
forces weight paramet ers to be close to zero, which in turn reduces
the model capacity. If ↵ is small, little penalization is applied on
weights and the model is then free to be highly nonlinear and can
more easily fit the data in the training set.
With MAP, the posterior is now approximated by the product ofprobabilistic machine learning for civil engineers 135
the likelihood and the prior, so
f(✓w, ✓b
|D) / f(Dy |Dx, ✓w, ✓b ) · f(✓w, ✓b) = Y
D
k=1
N (yk ; a
(O)(xk; ✓w , ✓b ), 
2
V
) · Y
i,j,l
N (w
(l)
i,j
; 0, 1/↵)
ln f (✓w, ✓b
|D) / 
1
D X
D
k=1
J(yk; xk, ✓w , ✓b ) 
↵
2 X
i,j,l
(w
(l)
i,j) 2
/ 
⇣
J(D; ✓ w, ✓b) +
↵
2
✓
|w✓ w⌘
. We can then redefine the loss function as
J˜(D; ✓w , ✓b) = J(D; ✓w, ✓b) + ↵⌦(✓w ), where ⌦(✓), the regularization or weight decay term, is
⌦(✓w ) =
1
2
||✓w||2 =
1
2
✓
|w ✓w. Defining a Gaussian prior for weights corresponds to L2
-norm
regularization. This choice for regularization is not the only one
available , yet it is a common one. The gradient of the loss function
with respect to weight parameters has to be modified to include the
regularization term, so
rW(l) J˜(D
mb
; ✓w , ✓b) = J(D
mb
; ✓w , ✓b) + ↵W(l)
.
In short, weight decay allows controlling the model capacity with
the hyperparameter ↵. The hyperparameter ↵ then needs to be
learned using a method such as cross-validation (see §8.1.2).
Early stopping Given that we initialize each parameter randomly, following ✓i ⇠ N(✓; 0, ✏
2
), with ✏ ⇡ 102
, the model will initially
have a poor performance at fitting the observations in the training
set. As the number of optimization epochs increases, the fitting
ability of the model will improve and, as a result, the loss for the
training set J(Dtrain
; ✓w, ✓b ) will decrease. If we keep optimizing
parameters, there is a point where the model will gain too much
capacity, and it will start overfitting the data in the training set.
Thus, a second option to prevent overfitting is to monitor the loss
function evaluated at each epoch for an independent validation
set J(Dvalidation
;✓ w, ✓b ). Note that the data in the validation set
must not be employed to train the model parameters. The optimal
parameter values ✓
⇤ = [✓
⇤w ✓
⇤
b
]| are taken as those corresponding
to the epoch for which the loss evaluated for the validation set is
minimal. This procedure is equivalent to treating the number ofj.-a. goulet 136
training epochs as a hyperparameter to be optimiz ed using the
validation set . Figure 8.37 presents an example of early stopping where we
see that the loss for the training set keeps decreasing with the
number of epochs. For the loss evaluated on the validation set, the
minimum is reached at seven epochs. Beyond seven epochs, the
fitting ability we gain on the training set does not generalize on the
validation set . 0 2 4 6 8 10 12
10￾6
10￾5
10￾4
10￾3
10￾2
Epochs
J( ; D
✓w, ✓b
)
Train Validation Early stop
Figure 8.37: Example of early stopping when the loss function for the validation
set stops decreasing. 8.3.4 Example: Metamodel
Here we revisit the example presented in §8.2.6, where we construct
a metamodel for the responses of a struct ure using five covariates
and a set of D = 1000 simulations. For this ex amp le, we employ
a tanh activation func tion with A = 10 activation units on each
of the L = 2 hidden layers. Figure 8.38 compares the system
responses with the model predictions obtained in a leave-one-out
cross-validation setup where for each fold, 90 percent of the data
is employed for training the model and the remaining 10 percent
to validate when the training should be stopped. The loss function
J(D; ✓ w, ✓b), evaluated using the leave-one-out c ross-validation
procedure, is equal to 1.32 ⇥ 105
. In comparison, the equivalent
mean-square error ob tained for the GPR model presented in §8.2.6
is 8.8 ⇥ 106
. It shows that neural networks are not necessarily
outperforming other methods, because they are typically best suited
for problems involving a large number of covariates and for which
large data sets are available.
0.34 0.36 0.38 0.4
0.34
0.36
0.38
0.4
yi
a
( ) O
i
Figure 8.38: Comparison of the system
responses with the model predictions obtained in a leave-one-out cross-validation
s9
Classification
[ ] cl+
Pr(pathogens) 0
1
Obs. class
+1
-1
(a)
PGA
Pr(damage) 0
1
Obs. class
+1
-1
(b)
Figure 9.1: Examples of application of classification analysis in the context of civil engineering.
Classification is similar to regression, where the task is to predict a
system response y, given some covariates x describing the system
properties. With regression, the system responses were continuous
quantities; with classification, the system responses are now discrete
classes. Figure 9.1 presents two examples of classification applied to
the civil engineering context; in (a), the sigmoid-like curve describes
the probability of having pathogens in water, as a function of the
chlorine concentration. In this problem, observations are either 1 or +1, respectively corresponding to one of the two classes
{pathogens, ¬pathogens}. The second example, (b), presents the
probability of having structural damage in a building given the
peak ground accelerat ion (PGA) associated with an earthquake. Observations are either 1 or +1, respectively corresponding to one
of the two classes {damage, ¬damage}. For classification, the data consists in D pairs of covariates
xi 2 RX and system responses yi 2 {1, +1}, so D = {(xi
, yi), 8i 2
{1 : D}}. For problems involving multiple classes, the system re - sponse is yi 2 {1, 2, · · · , C}. The typical goal of a classification Data D = {(xi , yi ), 8i 2 {1 : D}}
xi 2 R :8
<
:
Covariate Attribute Regressor yi 2 {1, +1} : Observation for binary
classification
yi 2 {1,2, · · · , C} : Observation for multi- classes classification
method is to obtain a mathematical model for the probability of
a syste m response given a covariate, for example, Pr(Y = +1|x). This problem can be modeled with either a generative or a discrim￾inative approach. A generative classifier models the probability
density function (PDF) of the covariates x, for each class y, an d
then use these PDFs to compute the probability of classes given the
covariate values. A discriminative classifier directly builds a mo del
of Pr(Y = +1|x) without modeling the probability density function
(PDF) of covariates x, for each class y. In practice, discriminative
approaches are most common because they typically perform better
on med ium and large data sets,1 and are often easier to use. This
1Ng, A. and M. Jordan (2002). On
discriminative vs. generative classifiers: A
comparison of logistic regression and naive Bayes. In Advances in neural information
processing systems, Volume 14, pp. 841–
848. MIT Press
chapter presents the generic formulation for probabilistic generative
classifiers and three discriminative classifiers: logistic regression,j.-a. goulet 140
Gaussian process classification, and neural networks. We will see
that generative classifiers are best suited for small data sets, Gaus- sian process classification is suited for medium-size ones, and neural
networks for large ones. In order to simplify the e xplanations, this
chapter presents each method while focusing on the context of
binary classification, that is, y 2 {1, +1}.
9.1 Generative Classifiers
The basic idea of a generative classifier is to employ a set of obser- vations D to model the conditional PDF of covariates x 2 RX given
each class y 2 {1,+1}. The n these PDFs are employed in combi- nation with the prior probability of each class in order to obtain the
posterior probability of a class given a covariate x. Ge ne rat ive clas- sifiers are most appropriate when working with problems involving
a small number of covariates and observation s because they allow
us to explicitly include domain-specific knowledge regarding f(x|y)
or tailored features such as censored data.
9.1.1 Formulation
The posterior probability of a class y given covariates x can be
expressed using Bayes rule,
posterior z }| { p(y|x) =
likelihood z }| { f(x|y)·
prior z}|{ p(y)
f(x) |{z}
evidence
.
The posterior probability of a class y conditional on a covariate
value x is defined according to
p(y|x) =
⇢
Pr(Y = 1|x) if y = 1
Pr(Y = +1|x) = 1  Pr(Y = 1|x) if y = +1. The prior probability of a class is described by the probabilities Note: the prior knowledge can either be based on expert knowledge, i.e., p(y), or be
empirically estimated using observations,
i.e., p(y|D) =
p(D|y) · p(y)
p(D)
p(y) =
⇢
Pr(Y = 1) if y = 1
Pr(Y = +1) if y = +1. The prior and posterior are describe d by probability mass funct ions
because y 2 {1, +1} is a discrete variab le . The likelihood of
covariates x conditional on a class y is a multivariate conditional
probability density function that follows
f(x|y) =
⇢
f(x|y = 1) if y = 1
f(x|y = +1) if y = +1.probabilistic machine learning for civil engineers 141
The normalization constant (i.e., the evidence) is obtained by
summing the product of the likelihood and prior over all classe s, f(x) = X
y2{1,1}
f(x|y) · p(y). A key step with a generative classifier is to pick a distribution Note: The notation f(x|y; ✓) ⌘ f (x|y, ✓)
indicates that the conditional probability
f(x|y) is parameterized by ✓. type for each f(x|yj ; ✓j ), 8j and then estimat e the parameters ✓j
for these PDFs using the data set D, that is, f (✓j|D). For a class
y = j, the parameters of f (x|y j) ⌘ f(x|yj
;✓j ) are estimated
using only observations Dj
: {(x, y)i
, 8i : yi = j}, that is, the
covariates xi such that the associated syste m response is yi = j. The estimation of parameters ✓ can be performed using either a
Bayesian approach or a deterministic one as detailed in §6.3. Like
for the likeliho od, the prior probability of each class p(y|D) can be
estimated using either a Bayesian approach or a deterministic one
using the relat ive freque nc y of observations where yi = j. With
a Bayesian approach, the posterior predictive probability mass
function (PMF) is obtained by marginalizing the uncertainty about
the posterior parameter estimate,
posterior predictive z }| { p(y|x, D) =
Z
likelihood z }| { f(x|y; ✓)·
prior z }| { p(y; ✓)
f(x; ✓) | {z }
normalization constant
·
posterior ✓ z }| { f(✓|D) d✓. When either a maximum a-posteriori (MAP) or a maximum likeli- hood estimate (M LE) is employed instead of a Bayesian estimation
approach, the posterior predictive is replaced by the ap proximation, pˆ(y|x, D) =
f(x|y; ✓
⇤
) · p(y; ✓
⇤
)
f(x; ✓
⇤)
, where ✓
⇤ are the MLE or MAP estimates for parameters.
MLE: ✓
⇤ = arg max
✓
f(D; ✓)
✓ = {µx, x}
D = {xi 2 R, 8i 2 {1 : D}}
f(x; ✓) = N (x; µx , 
2x
)
f(D; ✓) = YD
i=1N(xi; µx, 
2x)
ln f(D; ✓) / XD
i=1✓ 
1
2
(xi  µx)2 2x + ln
1x ◆
@ ln f(D;✓)
@µx = XD
i=1xi  Dµx = 0
! µ
⇤x =
1
D XD
i=1 xi
@ ln f(D;✓)
@
2x = XD
i=1
(xi  µx)2
(
2x
)2 
D
2x = 0
! 
2⇤x =
1
D XD
i=1
(xi  µx)2
✓ = pj
D = {y i 2 {1 : C}, 8i 2 {1 : D}}
! Dj = #{i : yi = j}  D
f(y; ✓) =
⇢
pj
for y = j
1  pj for y 6= j
f(D; ✓) / p
Dj
j (1  pj)
(DDj )
ln f(D; ✓) / Dj
ln p j + (D  D j) ln(1  pj)
@ ln f(D;✓)
@pj =
Dj
pj 
(D  Dj)
1  pj = 0
! p
⇤
j =
Dj
D
Maximum likelihood estimate In the special case where f (x|y =
j; ✓
⇤
) = N
x; µ
⇤x|yj
, 
2⇤x|yj  and the numbe r of available data D is
large, one can employ the MLE approximation for the parameters
µx|yj and 
2x|yj
which is defined by
µ
⇤x|y j = 1
Dj P
i xi

2⇤x|yi = 1
Dj P
i (xi  µ
⇤x|yj ) 2 )
, 8{i : yi = j}, where Dj = #{i : yi = j} denotes the number of observations in a
data set D that belongs to the j
th class. The MLE approximation
of p(y = j; ✓
⇤
) is given by
p(y = j; ✓
⇤
) =
Dj
D
.j.-a. goulet 142
Figure 9.2 presents an example of application of a generative
classifier where the parameters for the normal PDFs are estimated
using the MLE approximation. The bottom graph overlays the
empirical histograms for the data associated with each class along
with the PDFs f (x|y = j, ✓
⇤
). The top graph presents the classifier
p(y = +1|x, ✓
⇤
), which is the probability of the class y = +1 given
the covariate value x. If we assume that the prior probability of
each class is equal, this conditional probability is computed as
p(y = +1|x) =
f(x|y = +1) · p(y = +1)
f(x|y = 1) · p(y = 1) + f(x|y = +1) · p(y = +1) =
f(x|y=+1)
f(x|y=1)+f(x|y=+1)
, for p(y = 1) = p(y = +1).
2 0 2 4 6 8 10
0
0.5
1
x
p y( =
+1|x)
{xi, yi = 1} {xi, yi = +1}
2 0 2 4 6 8 10
0
0.2
0.4
x
f
x y ( | )
f(x|y = 1) f(x|y = +1)
Figure 9.2: Example of generative classifier using marginal PDFs. Method f(x|y = j)
Naive X i ? Xk |y, 8i 6= k
Bayes e.g. B(x1 ; ↵, ) · N(x2 , µ, 
2
)
LDA N (x; µx,j , ⌃x)
QDA N (x; µx,j , ⌃x,j )
(a) Naive Bayes
(b) LDA, ⌃x,1 = ⌃x,+1
(c) QDA, ⌃x,1 6= ⌃x,+1 Figure 9.3: Examples of PDF f(x|y = j )
for naive Bayes, LDA, and QDA.
Multiple covariates When there is more than one covariate x =
[x1 x2 · · · xX
]| describing a system, we must define the joint PDF
f(x|y). A common approach to define this joint PDF is called naive
Bayes (NB). It assumes that covariates Xk are indep en dent of
each other so their joint PDF is obtained from the produc t of the
marginals, f(x|y = j) = Y
X
k=1
f(xk |y = j). The method employing the special case where the joint PDF
of c ovariates is described by a multivariate normal f(x|y ) = N (x; µ, ⌃) is called quadratic discriminant analysis (QDA). The
label quadratic refers to the shape of the boundary in the co￾variate domain where the probability of both classes are equal, Pr(y = +1|x) = Pr(y = 1|x). In the case where the covariance
matrix ⌃ is the same for each class, the boundary becomes linear, and this special case is named linear discriminant analysis (LDA). Both the naive Bayes, QDA, and LDA methods typically employ
an M LE approach to estimate the parameters of f (x|y = j). Figure
9.3 presents examples of f (x|y = j) for the three approaches: NB, LDA, and QDA. Figu re 9.4 presents an example of an ap plic ation
of quadrat ic discriminant analysis where the joint probabilities
f(x|y) are multivariate Normal PDFs as depicted by the contour
plots. The mean vector and covariance matrices describing these
PDFs are estimat ed using an MLE approximation. Because there
are two covariates, the classifier f (y = +1|x ) is now a 2-D surface
obtained following
p(y = +1|x) =
f(x|y = +1) · p(y = +1)
f(x|y = 1) · p(y = 1) + f(x|y = +1) · p(y = +1).probabilistic machine learning for civil engineers 143
Note that for many problems, generative methods such as naive
Bayes and quadratic discriminant analysis are outperformed by
discriminative approaches such as those presented in §9.2–§9.4.
2 2 6 10 2
2
6
10
x1
x2 {xi
|yi = 1} {xi
|yi = +1}
2
2
6
102
2
6
10
0
0.5
1
x1 x2
p y( =
+1|x)
Figure 9.4: Example of quadratic discrimi- nant analysis where the joint probabilities f(x|yi ) depicted by contour plots are de- scribed by multivariate Normal PDFs. The parameters of each PDF are estimated
using an MLE approach.
9.1.2 Example: Post-Earthquake Structural Safety Assessment
We present here an example of a dat a-driven post-earthquake
structural safety assessme nt. The goal is to assess right after an
earthquake whether or not buildings are safe for occupation, that
is, y 2 {safe, ¬safe}. We want to guide this assessment based on
empirical data where the observed covariate x 2 (0, 1) describes the
ratio of a building’s first natural frequency f [Hz] after and before
the earthquake, x =
fpost-earthquake fpre-earthquake
. Figure 9.5 presents a set of 45 empirical observations2 collected
2Goulet, J.-A., C. Michel, and A. Der Ki- ureghian (2015). Data-driven post- earthquake rapid structural safety as- sessment. Earthquake Engineering &
Structural Dynamic s 44(4), 549–562
in di↵erent countries and desc ribing the frequency ratio x as a
function of the damage index d 2 {0, 1, 2,3, 4, 5}, which ranges from
undamaged (d = 0) up to collapse (d = 5). Note that observations
marked by an arrow are censored data, where x is an upper-bound
censored observation (see §6.3) for the frequency ratio. The first
step is to employ this data set D = {( xi |{z} 2(0,1)
, di |{z} 2{0:5}
), 8i 2 {1 : 45}} in
order to learn the conditional PDF of the frequency ratio X, given
a dam age index d. Bec ause x 2 (0,1), we choose to describe its
conditional PDF using a Beta distribution,
Algeria Japan Martinique
Spain
Italy
USA
0 1 2 3 4 5
0.2
0.4
0.6
0.8
1
Damage Index [d ]
Upper bound
observation
Figure 9.5: Empirical observations of the
ratio between fundamental frequency of
buildings after and before an earthquake.
f(x; ✓(d)) = B(x; µ(d), (d) | {z }
✓
). Note that the Beta distribution is parameterized here by its mean
and standard deviat ion and not by ↵ and  as desc ribed in §4.3.
The justification for this choice of parameterization will become
clear when we present the constraints that are linking the prior
knowledge for di↵erent damage index e s d. For a given d, the poste￾rior PDF for the parameters ✓(d) = {µ(d), ( d)} is obtained using
Bayes, posterior z }| { f(✓(d)|D) =
likelihood z }| { f(D|✓(d))·
prior z }| { f(✓(d))
f(D) | {z }
constant
,
and the poste rior predic tive PDF of X is obtained by marginalizing
the uncertainty associated with the e stimat ion of ✓(d),
f(x|d, D) =
Z
f(x; ✓(d)) · f(✓(d)|D)d✓.j.-a. goulet 144
By assuming that observations are conditionally independe nt given
x, the joint likelihood of observations is
f(D|✓(d)) = Y
{i:di =d}
L(xi |µ(d), (d)), where the marginal likelihood for each observation (see §6.3) is
L(xi |µ(d),(d))=(
f(xi |µ(d),(d)), xi
: direct observation
F(xi |µ(d),(d)), xi: censored observation
and where f (·) denotes the PDF and F(·) denotes the cumulative
distribution function (CDF). The prior is assumed to be uniform
in the range (0,1) for µ(d) and uniform in the range (0,0.25) for
(d). For the mean parameters µ(d), the prior is also constrained
following µ(0) > µ(1) > · · · > µ(5). These const raints reflec t our
domain-specific knowledge where expect the mean frequency ratio
to decrease with increasing values of d. The Metrop olis algorithm presented in §7.1 is employed to
generate S = 35,000 samp les (Rˆ  1.005 for C = 3 chains) from the
posterior for the parameters ✓(d). Figure 9.6a presents the contours
of the posterior PDF for each pair of parameters {µ(d), (s)}
along with a subset of samples. The predictive PDFs f(x|d, D) of
a freq uen cy ratio x conditional on d are presented in figure 9.6b.
The predictive probability of each damage index d given a freq ue nc y
ratio x is obtained following
p(d|x, D) =
Z
f(x; ✓(d)) · p(d)
P5
d
0 =0 f(x; ✓(d
0)) · p(d
0)
· f(✓(d)|D)d✓. (9.1)
The integral in equation 9.1 can be approximated using the Markov
chain Monte Carlo (MCMC) sample s as described in §7.5. Figure
9.7 presents the posterior predictive probability p(d|x, D), which
is computed while assuming that there is an equal prior probabil￾ity p(d) for each damage index. This classifier can be employed
to assist inspectors during post-earthquake safety assessment of
structures.
0 0.5 1
0
0.1
0.2
µ(5)
(5)
0 0.5 1
0
0.1
0.2
µ(4)
(4)
0 0.5 1
0
0.1
0.2
µ(3)
(3)
0 0.5 1
0
0.1
0.2
µ(2)
(2)
0 0.5 1
0
0.1
0.2
µ(1)
(1)
0 0.5 1
0
0.1
0.2
µ(0)
(0)
(a) Posterior PDFs for ✓(d)
0 0.25 0.5 0.75 1 x
f x( |d, D) d=0 d=1 d=2 d=3 d=4 d=5
(b) Posterior predictive PDF
Figure 9.6: Posterior PDFs of means and standard deviations and posterior predictive PDFs of frequency ratios x for each damage index d.
0 0.25 0.5 0.75 1
0
1
x
p d( |x, D) d=0 d=1 d=2 d=3 d=4 d=5
Figure 9.7: Posterior predictive probability
of a damage index d given a frequency
ratio x.
9.2 Logistic Regression
Logistic regression is not a state-of-the-art approach, yet its preva￾lence, historical importance, and simple formulation make it a good
choice to introduce discriminative regression methods. Logistic
regression is the extension of linear regression (see §8.1) for clas- sification problems. In the context of regression, a linear function
RX ! R is defined so that it transforms an X-dimensions covariateprobabilistic machine learning for civil engineers 145
domain into a single output g(x) = Xb 2 R. In the context of clas- sification, system respon ses are discrete, for example, y 2 {1,+1}. The idea with classification is to transform the output of a linear
function g(x) into the (0,1) interval describing the probability
Pr(y = +1|x). We can transform the output of a linear model
Xb in the interval (0, 1) by passing it through a logistic sigmoid
function (z), (z) =
1
1 + exp(z) =
exp(z)
exp(z) + 1
, as plot te d in figure 9.8. The sigmoid is a transformation R ! (0, 1)
such that an input defined in the real space is squashe d in the
interval (0,1). In short, logistic regression maps the covariates x to
the probability of a class, x 2 R
X | {z }
covariates ! g(x) = Xb 2 R
| {z }
hidden/latent variable ! (g(x)) 2 (0, 1) | {z } Pr(Y =y|x)
. Note that g(x) is considered as a latent or hidden variable because
it is not directly observed; only the class y and its associated covari- ate x are.
6 4 2 0 2 4 6
0
1
z
( ) z Figure 9.8: The logistic sigmoid function (z) = (1 + exp(z))1.
Figure 9.9 presents the example of a function g(x) = 0.08x  4
(i.e., the red line on the horizontal plane), which is passed through
a logistic sigmoid function. The blue curve on the leftmost vertical
plane is the logistic sigmoid function, and the orange curve on the
right should be interpreted as the probability the class y = +1,
given a covariate x, Pr(y = +1|x). For four di↵erent covariates xi, simulated observations yi 2 {1, +1} are depicted by crosses. 0
25
50
75
1005
0
5
0
0.25
0.5
0.75
1
x
g(x)
g(x) (g(x)) (x) yi
Figure 9.9: Example of a function g(x) =
0.08x  4 passed through the logistic
sigmoid function.
In pract ice , we have to follow the reverse path, where observa￾tions D = {(xi
, yi), 8i 2 {1 : D}} are available and we need to
estimate the parame ters b and the basis functions (x) defining
the model matrix X. For a given data set and a choice of basis
functions, we separate the observations xi
in order to build two
model matrices: one for xi
: yi = 1, X(1)
, an d another for
xi
: yi = +1, X(+1)
. The likelihood of an observation y = +1 is
given by Pr(y = +1|x) = (Xb), and for an observation y = 1
the likelihood is Pr(y = 1|x) = 1  (Xb). With the hypothesis
that observations are conditionally independent, the joint likelihood
is obtained from the product of the marginal likelihood or, equiva￾lently, the sum of the marginal log-likelihood of observations given
parameters b,
ln p(D|b) = Xln((X(+1)b)) + Xln(1  (X(1)b)). Optimal parameters b
⇤ can be inferred using an MLE procedure
that maximizes ln p( D|b). Unfortunately, contrarily to the optimiza-j.-a. goulet 146
tion of parameters in the case of the linear regression presented
in §8.1, no close-form analytic solution exists here to identifyb
⇤
. With linear regression, the derivative of the log-likelihood (i.e., the loss function ) was linear, leading to an analytic solution for
b
⇤
:
@J(b)
@b = 0. With logistic regression, the derivative of the
log-likelihood is a nonlinear function, so we have to resort to an
optimization algorithm (see chapter 5) to identify b
⇤
. Example: Logistic regression Figure 9.10 presents three examples
involving a single covariate x and a linear model g (x) = b1x + b0, and where paramete rs b = [b0 b1]| are estimated with respectively
5, 10, and 100 observations. The true parameter values employed to
generate simulated observations are ˇb = [4 0.08]|
. The correspond￾ing functions g(Xˇb) and (g(X ˇb)) are represented by dashed lines, and those estimat ed using MLE parameters g(Xb
⇤
) and (g(Xb
⇤
))
are represented by solid line s. We can observe that as the number
of ob se rvations increases, the classifier converges toward the one
employed to generate the data.
0
25
50
75
1005
0
5
0
0.5
1
x
g(x)
g(x) (g(x)) (x) yi
(a) D = 5, b⇤ = [3.1 0.09]|
0
25
50
75
1005
0
5
0
0.5
1
x
g(x)
(b) D = 10, b
⇤ = [4.4 0.08]|
0
25
50
75
1005
0
5
0
0.5
1
x
g(x)
(c) D = 100, b
⇤ = [3.9 0.08]| Figure 9.10: Example of application of
logistic regression.
This case is a trivial one because, in this closed -loop simulation, the model structure (i.e., as defined by the basis functions in model
matrix X) was a perfect fit for the problem. In pract ic al cases, we
have to select an appropriate set of basis functions j(xi ) to suit
the problem at hand. Like for the linear regression, the selection of
basis functions is prone to overfitting, so we have to employ either
the Bayesian model selection (see §6.8) or cross-validation (see
§8.1.2) for that purpose.
Civil engineering perspectives In the field of transportation en￾gineering, logistic regression has been extensively employed for
discrete choice modeling
3 because of the interpretability of the
3Ben-Akiva, M. E. and S. R. Lerman
(1985). Discrete choice analysis: Theory and application to travel demand. MIT
Press; and McFadden, D. (2001). Economic
choices. American Economic Review 91 (3), 351–378
model parameters b in the context of behavioral economics. How- ever, for most benchmark proble ms, the predictive capacity of
logistic regression is outperformed by more modern techniques such
as Gaussian process classification and neural networks, which are
presented in the next two sections.
9.3 Gaussian Process Classification
Gaussian process classification (GPC) is the extension of Gaussian
process regression (GPR; see §8.2) to classification problems. In the
context of GPR, a function RX ! R is defined so that it transforms
an X-dimensions covariate domain into a single output g( x) 2 R. In
the context of classific ation, the system response is y 2 {1,+1}.probabilistic machine learning for civil engineers 147
Again, the idea with classification is to transform g(x)’s outputs
into the (0,1) interval describing the probability Pr(Y = +1|x). For GPC, the transformation in the interval (0, 1) is done using the
standard Normal CDF prese nted in figure 9.11, where (z) denotes
the CDF of Z ⇠ N (z; 0, 1). 4 3 2 1 0 1 2 3 4
0
0.5
1
z
(z) Figure 9.11: The standard Normal CDF, where (z ) denotes the CDF of Z ⇠ N(z; 0, 1).
Like the logistic sigmoid function presented in §9.2, (z) is
a tran sformation R ! (0, 1), such that an input defined in the
real space is mapped in the interval (0, 1). Note that choosing
the transformation function (z) instead of the logistic function
is not arbitrary; we will see later that it allows maintaining the
analytic tractability of GPC. We saw in §8.2 that the function
g(x⇤), describing the system responses at prediction locations x⇤, is
modeled by a Gaussian process,
fG⇤|x ⇤,D(g⇤ |x⇤, D) ⌘ f(g ⇤|x⇤, D) = N (g ⇤; µ⇤|D , ⌃⇤|D).
0
25
50
75
1005
0
5
0
0.5
1
x
g(x)
µG ± 2g µG gi (g(x)) Pr(Y = 1|x)
Figure 9.12: Example of Gaussian process G(x) evaluated through (G(x)) and whose uncertainty is marginalized in order
to describe Pr(Y = +1|x).
In order to compute Pr(Y = +1|x⇤, D), that is, the probability
that Y = +1 for a covariate x⇤ and a set of observations D, we
need to transform g⇤ in the interval (0,1) and then marginalize the
uncertainty associated with f(g⇤|x⇤, D) so that
Pr(Y = +1|x⇤, D) =
Z (g⇤) · f(g⇤|x⇤, D)dg ⇤. (9.2)
If we choose the standard normal CDF, with (z) as a transforma- tion function, the integral in equation 9.2 follows the closed-form
solution
Pr(Y = +1|x⇤, D) =  
E[G⇤|x⇤, D]
p
1 + var[G⇤|x⇤, D] !
, (9.3)
where for the i
th prediction locations, E[G⇤|x⇤ , D] ⌘ [µ⇤|D] i and
var[G⇤|x⇤, D] ⌘ [⌃⇤|D ]ii
. Figu re 9.12 illustrates how the uncer- tainty related to a Gaussian process evaluated through (G (x)) is
marginalized in order to describe Pr(Y = +1|x, D). So far, we have seen how to employ the standard normal CDF
to transform a Gaussian process g⇤ 2 R, with PDF f(g⇤|x⇤, D)
obtained from a set of observations D = {Dx, Dy } = {(xi
, yi 2
R), 8i 2 {1 : D}}, into a space (g⇤) 2 (0,1). The issue is that
in a classification setup, g (x) is not directly observable ; only yi 2
{1, +1} is. This require s inferring for each covariate x 2 Dx the
mean and standard deviations for latent variables G(x). For that,
Note: The qualification latent refers to variables that are not directly observed and
that need to be inferred from observations we need to rewrite equation 9.3 in order to explicitly include the of the classes y 2 {1, +1}.j.-a. goulet 148
conditional dependence over g so that
Pr(Y = +1|x⇤, D) =
Z (g⇤) · f(g⇤ |x⇤, g, D)dg⇤ =  
E[G⇤ |x⇤, g, D]
p
1 + var[G⇤ |x⇤, g, D]!
. For the i
th prediction locations, E[G⇤|x⇤, g, D] ⌘ [µ⇤|g,D ]i and
var[G⇤|x⇤, g, D] ⌘ [⌃⇤|g,D] ii
, where
f(g⇤|g) ⌘ f(g⇤ |x⇤ , g, D) = N(g⇤; µ⇤|g,D, ⌃⇤|g,D ). (9.4)
Equation 9.4 presents the posterior probability of the Gaussian
process outcomes g⇤ at prediction location x⇤ , given the data set
D = {Dx, Dy} = {(xi
, yi 2 {1, 1}), 8i 2 {1 : D}}, an d the inferred
values for g at observed locat ions x. The mean and covariance
matrix for the PDF in equation 9.4 are, respect ively, Note: The prior mean for the Gaussian
process at both observed µ⇤ and predicted
µG locations are assumed to be equal to µ⇤ zero. |g,D =
=0 z}|{ µ⇤ +⌃
|
G⇤⌃
1
G (µG|D 
=0 z}|{ µG )
⌃⇤|g,D = ⌃⇤  ⌃
|
G⇤⌃
1
G|D⌃G⇤ , where µG|D and ⌃G|D are the mean vector and covariance ma￾trix for the inferred latent observations of G(x ) ⇠ f(g|D) = N(g;µG|D, ⌃G|D). The posterior PDF for inferred latent variables
G(x) is
f(g|D) =
p(Dy|g) · f (g|Dx)
p(Dy|Dx)
/ f(Dy |g) · f (g|Dx).
(9.5)
The first term corresponds to the joint likelihood of observations
Dy given the associat ed set of inferred latent variables g. With
the assumption that observations are conditionally independent
given gi
, the joint likelihood is obtained from the product of the
marginals so that Note: Because of the symmetry in the
standard normal CDF, p(y = +1|g) = (g) and
p(y = 1|g) = 1  (g) = (g). It explains why the marginal likelihood
simplifies to p(y|g) = (y · g).
p(Dy |g) = QD
i=1 p(yi |gi) = QD
i=1 (yi · gi ). The second term in equation 9.5, f(g|Dx ) = N(g; µG = 0, ⌃G)
is the prior knowledge for the inferred latent variables G( x). The
posterior mean of f(g|D), that is, µG|D, is obtained by maximizing
the logarithm of f(g|D) so that
Note: As detailed in §8.2, the prior covariance [⌃G] ij = ⇢(xi, xj )
2G depends on the hyperparameters ✓ = {G , `}, which
also need to be inferred from data. µG|D = g
⇤ = arg max
g
ln f (g|D) = arg max
g
ln f (Dy |g)  1
2g
|⌃
1
G g  1
2
ln |⌃G| 
D
2
ln 2⇡.probabilistic machine learning for civil engineers 149
The maximum g
⇤ corresponds to the location where the derivative
equals zero so that
@ ln f(g|D)
@g = rg ln f (g|D) = r ln f(Dy |g)  ⌃
1
G g = 0. (9.6)
By isolating g in equation 9.6, we can obtain optimal values g
⇤
iteratively using
g ⌃G r ln f(Dy|g), (9.7)
where the initial starting location can be taken as g = 0. The
uncertainty associated with the optimal se t of inferred latent vari- ables µG|D = g
⇤
is estimated using the Laplace approximation (see
§6.7.2). The Laplace app roximation for the covariance matrix corre￾sponds to the inverse of the Hessian for the negative log-likelihood
evaluated at µG|D, so that
⌃G|D = H[ln f(g|D)]1 =
⇣⌃
1
G  diagrr ln f(Dy |µG|D)⌘1
.
0
25
50
75
1003
0
3
0
0.5
1
x
g(x)
Pr(Y = 1|x) y i Pr(Y = 1|x,D)
(g(x)) µG⇤|G ± 2g µG⇤|G µG|D± 2G|D
(a) D = 25
0
25
50
75
1003
0
3
0
0.5
1
x
g(x)
(b) D = 50
0
25
50
75
1003
0
3
0
0.5
1
x
g(x)
(c) D = 100
Figure 9.13: Example of application of
Gaussian process classification using the package GPML with a di↵erent number of observations D 2 {25, 50, 100}.
Note that we can improve the efficiency for computing the MLE
in equation 9.8 by using the information contained in the Hessian
as we did with the Newton-Raphson method in §5.2. In that case,
optimal values g
⇤ are obtained iteratively using
g g  H[ ln f(g|D)]1
· rg ln f (g|D). (9.8)
Figure 9.13 presents an application of GPC using the Gaussian
process for machine learning (GPML) package with a di↵erent
number of ob servations, D 2 {25, 50, 100}. In this figure, the green
solid lines on the horizontal planes describe the inferred confidence
intervals for the latent variables, µG|D ± 2G|D . We can see that
as the number of observations increases, the inferred function
Pr(Y = +1 |x⇤, D) (orange solid line) tends toward the true function
(orange dashed line).
Figure 9.14 presents the application of Gaussian process classifi- cation to the post-earthquake struc tural safety assessment example
introduced in §9.1.2. Here, the multiclass problem is transformed
into a binary one by modeling the probability that the damage
index is either 0 or 1. The main advantage of GPC is that the prob￾lem setup is trivial when using existing, pre-implemented packages. With the generat ive approaches pre sented in §9.1, the formulation
has to be specifically tailored for the problem. Nevertheless, note
that a generative approach allows including censored data, whereas
a discriminative approach such as the GPC presented here c ann ot .j.-a. goulet 150
In pract ice , all the operations required to infer the latent vari- ables G(x) as well as the parameter ✓ ={g , `} in GPC are already
implemented in the same open-source packages dealing with GPR, for example, GPML, GPstu↵, and pyGPs (see §8.2) .
0 0.2 0.4 0.6 0.8 1
0
0.5
1
x
Pr(D
= 0| D x, )
y = 1
y = 0
Figure 9.14: Example of application of
Gaussian process classification to the post- earthquake structural safety evaluation
data set.
Strengths and limitations With the availability of Gaussian process
classification implemented in a variety of open-source packages, the setup of a GPC problem is trivial, even for proble ms involving
several covariat es x. When the number of observations is small, the
task of inferring latent variable s g becomes increasingly difficult
and the performance decreases. Note also that the formulation
presented assumes that we only have error-free direct observat ion s. Moreover, because of the comput ationally demanding procedure of
inferring latent variables g, the performance is limited in the case
of large data sets, for example, D > 105
. Given the limitations for
both small and large data sets, the GPC presented here is thus best
suited for medium-size data sets. For more details about the Gaussian process classification and its
extension to multiple classes, the reader should refer to dedicated
publications such as the book by Rasmussen and Williams4 or the
4Rasmussen, C. E. and C. K. Williams
(2006). Gaussian processes for machine
learning. MIT Press tutorial by Ebden.5
5Ebden, M. (2008, August). Gaussian
processes: A quick introduction. arXiv preprint (1505.02965)
9.4 Neural Networks
The formulation for the feedforward neural network presented in
§8.3 c an be adapted for binary classification problems by replacing
the linear outpu t activation func tion by a sigmoid function as
illustrated in figure 9.15a. For an observation y 2 {1,+1}, the
log of the probability for the outcome yi = +1 is modeled as being
proportional to the hidden variable on the output layer z
(O)
, an d
the log of the probability for the outcome yi = 1 is assumed to be
proportional to 0,
lnPr(Y = +1|x, ✓) / z
(O) = W(L)a
(L) + b
(L)
lnPr(Y = 1|x, ✓) / 0. These unnormalized log-probabilities are tran sformed into a proba￾bility for each possible outcome by taking the exponential of each of
them and then normalizing by following
Pr(Y = +1|x, ✓) =
exp(z
(O))
exp(z
(O)) + exp(0)
| {z }
=1 = (z
(O))
Pr(Y = 1|x, ✓) =
exp(0)
exp(z
(O)) + exp(0) = (z
(O) ).probabilistic machine learning for civil engineers 151
As pre se nted in §8.3.1, this procedure for normalizing the log￾probabilities of y is equivalent to evaluating the hidden variable
z
(O) (or its negative) in the logistic sigmoid function. Thus, the
likelihood of an observation y 2 {1, +1}, given its associated
covariates x and a set of parameters ✓ = {✓w , ✓b}, is given by
· · ·
· · ·
· · ·
· · ·
b(L1)
.
.
.
b
(L)
z
(O) y
(a)
· · ·
· · ·
· · ·
· · ·
b
(L1)
.
.
.
b(L)
z
(O) 1
y
z
(O) 2
.
.
.
z
(O) C
(b)
Figure 9.15: Nomenclature for a feed￾forward neural network employed for classification: (a) represents the case for binary classification y 2 {1,+1}; (b) the
case for C classes y 2 {1, 2, · · · , C}.
p(y|x, ✓) = (y · z
(O)). With neural networks, it is common to minimize a loss function
J(D; ✓w, ✓b ) that is defin ed as the negative joint log-likelihood for a
set of D observations, J(D; ✓w, ✓b ) =  ln p(Dy |Dx, ✓) = X
D
i=1
ln(yi · z
(O)
i ).
In the case where the observed system responses can belon g
to multiple classes, y 2 {1, 2, · · · , C}, the output layer needs to
be modifie d to include as many hidden states as the re are classes, z
(O) = [z
(O)
1 z
(O)
2
· · · z
(O)
C
]|, as presented in figure 9.15b. The log￾probability of an observation y = k is assumed to be proportional to
the k
th hidden state zk ,
lnPr(Y = k|x, ✓) / z
(O)
k
. The normalization of the log-probabilities is done using the softmax
activation function, where the probability of an observation y = k is
given by
p(y = k|x, ✓) = softmax(z
(O)
, k) =
exp(z
(O)
k
)
PC
j=1 exp(z
(O)
j )
. By assuming that observations are conditionally independe nt from
each other given z
(O)
, the loss function for the entire data set is
obtained as in §8.3.2 by summing the log-probabilities (i.e., the
marginal log-likelihoods) for each observation,
J(D; ✓w , ✓b) =  X
D
i=1
ln p(yi|xi , ✓). (9.9)
Minimizing the loss funct ion J(D;✓ w, ✓b ) defined in equation 9.9
corresponds to minimizing the cross-entropy. Cross-entropy is a
Note: Given two PMFs, p(x) and q(x), the
cross-entropy is defined as H(p, q) = X
x p(x) ln q(x). concept from the field of information theory
6
that measures the
6MacKay, D. J. C. (2003). Information
theory, inference, and learning algorithms. Cambridge University Press
similarity between two probab ility distributions or mass functions. Note that in the context of classification, the gradient r
O(z
(O))
in the backpropagation algorithm (see §8.3.2) is no longer equal toj.-a. goulet 152
one because of the sigmoid function. Moreover, like for the regres- sion setup, a neural network classifier is best suited for problems
with a large number of covariates and for which large data sets are
available.
9.5 Regression versus Classification
Note that when the data and context allow for it, it can be prefer- able to formulate a problem using a regression approach rather
than using classification. Take, for example, the soil contami- nation example pre sented in §8.2.5. The data set available de￾scribes the contaminant concentration ci measured at coordinat es
li = {xi , yi, zi}. Using regression to model this problem results in
a func tion describing the PDF of contaminant concentration as a
function of coordinate s l, f (c|l, D). A classification setup would
instead mo de l the probability that the soil contamination exceeds
an ad missible value cadm., Pr(C  cadm.|l, D ). Here, the drawback
of c lassification is that information is lost when transforming a
continuous system response C into a categorical event {C  cadm.}. For the classification setup, the information available to build the
model is whethe r the contaminant concentration is above (c i = +1)
or below (ci = 1) the admissible concentration cadm. for multiple
locations li
. The issue with a c lassification approach is that becau se
we work with categories, the information about how far or how
close we are from the threshold cadm. is lost.10
Clustering and Dimension Reduction
This chapter covers two key problems associated with unsupervised
learning: clustering and dimension reduction. Clustering consists
in discovering patterns or subgroups among a set of covariates. Clustering is considered unsuper vised learning because the cluster
indexes are not available to train a model; cluster indexes have to
be learned from the observed covariates themselves. With dimension
reduction, the goal is to identify a lower-dimensional subspace for
a set of covariates, in which the data can be represented with a
negligible loss of information. In this chapter, we will introduce
two clustering methods, Gaussian mixture models and K-means,
and a dimension reduction technique, that is, principal component
analysis.
(a) Old Faithful geyser
0 2 4 6
40
60
80
100
Eruption time [min]
Time to next eruption [min]
Cluster #1
Cluster #2
(b) Eruption data Figure 10.1: Eruption durations and time between eruptions can be clustered into
two groups.
10.1 Clustering
Figure 10.1 presents the Old Faithful geys er data set1 where the
1Adelchi, A. and A. W. Bowman (1990).
A look at some data on the Old Faithful
geyser. Journal of the Royal Statistical So- ciety. Series C (Applied Statistics) 39(3), 357–365
covariates xi = [x1 x2]|
i describe the eruption time and the time to
the next eruption. Figure 10.1 displays two distinct behaviors: short
eruptions with short times to the next ones and long eruptions with
a long time to the next ones. The goal with clustering is to discover
the existence of such an underlying structure in data.
Note that when working with data sets D = Dx = {xi, 8i 2 {1 :
D}}, xi 2 RX
, with covariates defined in three or fewer dimensions
(X  3), it is then trivial to visually identify clusters. The interest of
having clustering methods is in identifying patterns in cases where
X > 3, such that the dimensionality of the data does not allow for a
visual separation of clusters. Data D = Dx = {xi , 8i 2 {1 : D}} xi 2 RX
: ⇢
Covariate Attribute
10.1.1 Gaussian Mixture Models
Clustering can be formulated as the task of fitting a Gaussianj.-a. goulet 158
mixture model (GMM), on a set of observed covariates. A Gaus- sian mixture distribution is defined as the sum of K Normal (i.e., Gaussian) probability density functions (PDFs), each weighted by
a probab ility p(k), 8k 2 {1 : K}. The joint PDF for a Gaussian
mixture is defined by
3 0.25 2.5 5.25 8
p(1) = 0.4
p(2) = 0.6
x
P
D
F : f ( ) N1(1, 1
2 ) N2(4, 1.5
2) f12 (x)
(a) Univariate
x1 x2
f ( ) x
f12(x) N1(µ1
, ⌃1) N2(µ2
, ⌃2)
(b) Bivariate Figure 10.2: Examples of Gaussian mixture PDFs.
f(x) = X
K
k=1
f(x, k) = X
K
k=1
f(x|k) · p(k) = X
K
k=1
N (x; µk, ⌃k) · p(k),
(10.1)
where ✓ = {(µk, ⌃k), 8k 2 {1 : K}} describes the PDF parameters. Figure 10.2 presents examples of 1-D and 2-D Gaussian mixture
PDFs. The issue is that in practice, we do not know ✓ or p(k), so
we have to learn P = {✓, p(k)} from Dx. Given the hypothesis that
Dx contains observations that are conditionally indepen de nt given
P, the joint likelihood defined from equ at ion 10.1 is
f(Dx|✓, p(k)) = Y
D
i=1
X
K
k=1
N (xi
; µk , ⌃k) · p(k),
and equivalently, the log-likelihood is
ln f (Dx|✓, p(k)) = X
D
i=1
ln X
K
k=1
N (xi
; µk, ⌃k) · p(k)!
. (10.2)
If we choose a maximum likelihood estimate (MLE) approach to
Gaussian mixture parameters P = {(µk , ⌃k) | {z } ✓
, p(k), 8k 2 {1 : K}}
estimate the parameters, the ir optimal values are defined as
P
⇤ = arg max
P
ln f (Dx|✓, p(k)). (10.3)
When the optimization in equation 10.3 employs methods such
as those presented in chapter 5, it is referre d to as hard cluster￾ing. Hard clustering is a difficult task because of several issues
such as the const raints on ⌃k that require it to be positive semi- definite (see §3.3.5), and because of non-identifiability (see 6.3.4).
Soft cluster ing solves these issue s by employing the expectation
maximization method to estimate P⇤
. Expectation maximization
2
(EM) is an iterat ive meth od that 2 Dempster, A. P., N. M. Laird, and D. B. Rubin (1977). Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (methodological) 39(1), 1–38
allows identifying optimal parameters P
⇤
for problems involving
latent (i.e., unobserved) variables. In the context of clustering, the latent variable is the unobserved cluster index k. If we couldprobabilistic machine learning for civil engineers 159
observe D = {Dx, Dk } = {(xi , ki ), 8i 2 {1 : D}}, the complete data
log-likelihood for one observation (xi, ki) would be
ln f (xi
, ki|P) = ln f(xi
, ki|P) = ln
N (xi; µki, ⌃ki ) · p(ki )
. Because complete data is not available, EM identifies optimal
parameters by maximizing the expected complete data log-likelihood, Q(P
(t)
,P(t1) ), which depends on the sets of parameters defined for
two successive iterations, t  1 and t. For a single observation xi
, Q(P
(t)
,P
(t1) ) =X
K
k=1
ln
 N (xi; µ
(t)
k
, ⌃
(t)
k
) | {z }
fct.(P(t) )
·p
(t) (k)
· p(k|xi,P
(t1) ) | {z }
fct.(P(t1))
,
where p(k|xi,P(t1) ) is the posterior probability mass funct ion
(PMF) for the latent cluster labels k obtained at the previous
iteration t  1. This last quantity is referred to as the responsibility
of a cluster k. For an entire data set D = {Dx} = {xi
, 8i 2 {1 : D}}, the expected complete dat a log-likelihood is
Q(P
(t)
,P
(t1) ) = X
D
i=1
 X
K
k=1
ln
N (xi
; µ
(t)
k
, ⌃
(t)
k
) · p
(t)(k)
· p(k|xi ,P
(t1) )!
. The EM method identifies the optimal parameter values P⇤
from
Q(P
(t)
,P(t1) ) by iterat ively performing the expectation and maxi- mization steps detailed below. Expectation step For an ite rat ion t, the expectation step computes
the probability p
(t1)
ik
that an observed covariate x i
is associated
with the cluster k, given the parameters P(t1) so that
p
(t1)
ik = p(k|xi,P
(t1)) = N (xi ; µ
(t1)
k
, ⌃
(t1)
k
) · p
(t1)(k)
PK
k=1 N (xi; µ
(t1)
k
, ⌃
(t1)
k
) · p
(t1)(k)
. (10.4)
The particularity of the expectation step is that the estimation
of the responsibility p
(t1)
ik
in equation 10.4 employs the PDF
parameters ✓
(t1) = {(µ
(t1)
k
, ⌃
(t1)
k
), 8k 2 {1 : K}} and the
marginal PMF p
(t1) (k) estimated at the previous iteration t  1.
Maximization step In the maximization step, the goal is to iden￾tify
P
(t)⇤ = {✓
(t)⇤
, p
(t)⇤
(k)} = arg max
P(t)
Q(P
(t)
,P
(t1) ),j.-a. goulet 160
where the optimization is broken into two parts. The PDF’s new set
of optimal parameters ✓
(t)
is obtained through the maximization
problem
✓
(t)⇤ = arg max
✓(t)
Q(P
(t)
,P
(t1)), where the maximum corresponds to the value ✓ such that the
derivative of the function is zero,
0 =
@ lnQ(P (t)
,P(t1))
@✓
=
@ X
D
i=1
 X
K
k=1
ln
N (xi
; µ
(t)
k
, ⌃
(t)
k
) · p
(t)(k)
· p
(t1)
ik !
@✓
=
@ X
D
i=1
X
K
k=1
p
(t1)
ik
· ln N (xi; µ
(t)
k
, ⌃
(t)
k
)
@✓ +
=0 z }| { X
D
i=1
X
K
k=1
p
(t1)
ik
· ln p
(t) (k)
@✓ =
@ X
D
i=1
p
(t1)
ik h
ln det⌃
(t)
k + (xi  µ
(t)
k
) |
(⌃
(t)
k
)1
(xi  µ
(t)
k
)i
@✓
. By isolating the mean vector µ
(t)
k and the covariance matrix ⌃
(t)
k
, the new estimates are
µ
(t)
k = PD
i=1 xi · p
(t1)
ik
p
(t1)
k
⌃
(t)
k = PD
i=1
(xi  µ
(t)
k
)(xi  µ
(t)
k
)|
· p
(t1)
ik
p
(t1)
k
9
>
>=
>
>;
p
(t1)
k = X
D
i=1
p
(t1)
ik
.
By taking the derivative of lnQ(P
(t)
,P
(t1)) with respect to p( k)
and repeating the same procedure, the new marginal PMF for each
cluster k is given by
p
(t)(k) =
p
(t1)
k
D
.
In practice, the values µ
(0)
k are initialized randomly, ⌃
(0)
k = 
2I is
taken as a diagonal matrix, and p
(0) (k) = 1/K is initially assumed
to be equal for each cluster. Algorithm 8 summarizes the steps
of the expectation maximization algorithm for estimating the
parameters of a Gaussian mixture model.
0 2 4 6
40
60
80
100
Eruption time [min]
Time to ne
xt eruption [min]
(a) Expectation step, t = 1
0 2 4 6
40
60
80
100
Eruption time [min]
Time to ne
xt eruption [min]
(b) Maximization step, t = 1
0 2 4 6
40
60
80
100
Eruption time [min]
Time to ne
xt eruption [min]
(c) Expectation step, t = 2
0 2 4 6
40
60
80
100
Eruption time [min]
Time to ne
xt eruption [min]
(d) Maximization step, t = 2
Figure 10.3: The first two iterations of
the EM algorithm for a Gaussian mixture model applied on the Old Faithful data.
Figure 10.3 presents the first two steps of the EM algorithm
for a Gau ssian mixture model applied to the Old Faithful exam￾ple introduced in figure 10.1. At the first iteration t = 1, the two
cluster centers µ
(0)
k as well as their covariances ⌃
(0)
k are initialized
randomly. In the expectation step, the cluster responsibility is es- timated for each data point xi
. As de pict e d in figure 10.3a, a dataprobabilistic machine learning for civil engineers 161
Algorithm 8: EM for Gaussian mixture models 1 define Dx, ✏, ln f(Dx|P
(0)) = 1, ln f(Dx|P
(1) ) = 0, t = 1;
2 initialize ✓
(0) = {µ
(0)
k
, ⌃
(0)
k }, p
(0)(k) = 1/K;
3 while | ln f (Dx|P
(t))  ln f(Dx|P
(t1))| > ✏ do
4 for i 2 {1, 2, · · · , D} (Expectation step) do
5 p
(t1)
i = PK
k=1 N (xi
; µ
(t1)
k
, ⌃
(t1)
k
) · p
(t1)(k)
6 for k 2 {1, 2, · · · , K} do
7 p
(t1)
ik = N (xi
; µ
(t1)
k
, ⌃
(t1)
k
) · p
(t1) (k)
p
(t1)
i
8 for k 2 {1, 2, · · · , K} (Maximization step) do
9 p
(t1)
k = X
D
i=1
p
(t1)
ik
10 µ
(t)
k = PD
i=1 xi · p
(t1)
ik
p
(t1)
k
11 ⌃
(t)
k = PD
i=1
(xi  µ
(t)
k
)(xi  µ
(t)
k
) |
· p
(t1)
ik
p
(t1)
k
12 p
(t) (k) =
p
(t1)
k
D
13 ln f (Dx|P
(t)) =X
D
i=1
ln "X
K
k=1
N (xi
; µ
(t)
k
, ⌃
(t)
k
) · p
(t) (k)#
14 t = t + 1
point belongs to the cluster associated with the highest responsi- bility. At this point, due to the crude initialization of {µ
(0)
k
, ⌃
(0)
k }, the quality of the cluster membership assignment is poor. The
maximization step for t = 1 updates the initial cluster centers and
covariances. The updated contours for the clusters using µ
(1)
k and
⌃
(1)
k are depicted in figure 10.3b. Note that the responsibility for
each data point xi has not changed, so the cluste r membership
assignment remains the same. Figure 10.3c presents the expectation
step for the second iteration, where we can notice the change in
cluster membership assignment in comparison with the previous
steps in (a) and (b). In (d), the contours of each cluster are pre￾sented for parameters {µ
(2)
k
, ⌃
(2)
k }, which have been updated for the
second time. Figure 10.4 presents the final clusters along with their
contours obtained after t = 12 iterations.
0 2 4 6
40
60
80
100
Eruption time [min]
Time to next eruption [min]
Figure 10.4: Example of an application
of a Gaussian mixture model to the Old Faithful geyser data set. Contours
represent the clusters identified using the EM algorithm after t = 12 iterations.
0 0.25 0.5 0.75 1
0
0.25
0.5
0.75
1
x1
x2
Figure 10.5: Example of an application of a Gaussian mixture model to overlapping
clusters. Contours represent the clusters
identified using the EM algorithm.
Figure 10.5 presents a second example of a Gaussian mixture
model for K = 3 generic clusters. Note that despite the overlap
between clusters, the method performed well because, like in thej.-a. goulet 162
previous case, each cluster is well approximated by a multivariate
Normal. Gaussian mixture models may display a poor performance
when the clusters do not have ellipsoidal shapes. In such a case,
we should resort to more ad vanced meth ods such as spectral clus￾tering.3 Furthe r details regarding the derivation and theoret ical 3Ng, A., M. Jordan, and Y. Weiss (2002). On spectral clustering: Analysis and
an algorithm. In Advances in neural
information processing systems, Volume 14, pp. 849–856. MIT Press
justification of the EM methods are presented by Murphy
4 and
4Murphy, K. P. (2012). Machine learning: A probabilistic perspective. MIT Press
Bishop.5
5Bishop, C. M. (2006). Pattern recognition
and machine learning. Springer
Estimating the number of clusters In the previous examples, we
have assumed that we knew the number of clusters K. In most
practical applications, this is not the case and K must be estimated
using methods such as cross-validation (see §8.1.2) or Bayesian
model selection (see §6.8) .
10.1.2 K-Means
K-means 6 is one of the most widespre ad cluste ring methods due
6 Lloyd, S. (1982). Least squares quanti- zation in PCM. IEEE Transactions on
Information Theory 28(2), 129–137
to its simplicity. It can be seen as a special case of the Gaussian
mixture model presented in §10.1.1, where the probability of each
cluster is p(k) = 1/K, the covariance of each cluster is diagonal
⌃k = 
2I, and we seek to infer the cluster means µk 2 RX
. With
Gaussian mixture models, we were computing the probability that
the covariates xi belong to the cluster k; with K-means, the cluster
membership assignment k
⇤
i for a covariate xi is deterministic so
that it corresponds to
p(k|xi, ✓) =
⇢
1, if k = k
⇤
i
0, otherwise, where a covariate xi is assigned to the cluster associated with the
mean it is the close st to,
k
⇤
i = arg min
k
||xi  µk||. The mean vector for the k
th cluster is estimated as the average
location of the covariates xi associated with a cluster k so that
µk =
1
#{i : k
⇤
i = k} X
i:k⇤
i =k
xi
, where # {i : k
⇤
i = k} is the number of observed covariates assigned
to the cluster k.
0 0.25 0.5 0.75 1
0
0.25
0.5
0.75
1
x1
x2
(a) Real clusters
0 0.25 0.5 0.75 1
0
0.25
0.5
0.75
1
x1
x2 (b) Clusters approximated with K- means, where crosses describe each
cluster center. Figure 10.6: Example of application of
K-means. Notice how some samples were misclassified due to the hypothesis related
to the circular shape of each cluster, that
is, ⌃k = 
2 I.
The general procedure consists in initializing µ
(0)
k
randomly,
iteratively assigning each observation to the cluster it is the closest
to, and re c omp uting the mean of each cluster. Figure 10.6 presents
in (a) the true cluste r and in (b) the cluster membership assign - ment computed using K-means, where the mean of each cluster isprobabilistic machine learning for civil engineers 163
presented by a cross. Note that because clusters are assumed to
be spherical (i.e., ⌃k = 
2I), the cluster membership assignment
is only based on the Euclidian distanc e between a covariate and
the center of each cluster. This explains why in figure 10.6b several
observed covariates are assigned to the wrong cluster. Algorithm 9
summarizes the steps for K-means clustering. Algorithm 9: K-means 1 define x, ✏, t = 0
2 initialize µ
(0)
k
, and µ
(1)
k = µ
(0)
k + ✏
3 while ||µ
(t)
k  µ
(t1)
k
||  ✏ do
4 for i 2 {1, 2, · · · , D} do
5 k
⇤
i = arg min
k
||xi  µ
(t1)
k
||2
6 for k 2 {1, 2, · · · , K} do
7 µ
(t)
k = 1
#{i:k
⇤
i =k} P
i:k
⇤
i =k xi
8 t = t + 1
10.2 Principal Component Analysis
Principal component analysis (PCA)7
is a dime nsion reduc tion 7Hotelling, H. (1933). Analysis of a com- plex of statistical variables into principal components. Journal of Educational
Psychology 24 (6), 417
technique often employed in conjunction with clustering. The
goal of PCA is to transform the covariate space in a new set of
orthogonal axes for which the variance is maximized for the first
dimension. Note that in order to perform PCA, we must work with
normalized data with mean zero and with variance equal to one for
each covariate.
Let us assume, our observations consist of a set of raw unnormal￾ized covariates D = {xi
, 8i 2 {1 : D}}, where xi = [x1 x2 · · · xX]|
i
. We normalize each value xij = [xi]j in this data set by subtract ing
from it the empirical average µˆ j = 1
D PD
i=1 xij and dividing by the
empirical standard deviation ˆj =

1
D1 PD
i=1
(xij  µˆj)2 1/2
so that
x˜ij =
xij  µˆj
ˆj
. From our set of normalized covariates ˜xi = [x˜ 1 x˜2 · · · x˜X]|
i
, we seek
orthogonal vectors ⌫i 2 R X
, 8i 2 {1 : X} so that our tran sformation
matrix, V = [⌫1 ⌫2 · · · ⌫X
] , (10.5)
is orthonormal, that is, V|V = I. We can transform the covariates ˜xi
into the new spac e using zi = V|˜xi
. The tran sformed data is
then referred to as the score. Here, we want to find the orthogonalj.-a. goulet 164
transformation matrix V that minimizes the reconstruction error
J(V) =
1
DX
D
i=1
||˜xi  Vzi||2
. (10.6)
The optimal solution minimizing J(V) is ob taine d by taking V
as the matrix containing the eigenvectors (see §2.4.2) from the
empirical covariance matrix estimated from the set of normalized
covariates, ⌃ˆ =
1
D  1 X
D
i=1 ˜xix˜
|
i
. With PCA, the eigenvectors ⌫i in V are sorted according to the
decreasing order of its associated eigenvalues. PCA finds a new
set of orthogonal reference axes so that the variance for the first
principal component is maximized in the transformed space. This
explains why we need to work with normalized dat a so that the
variance is similar for each dimension in the original space. If it is
not the case, PCA will identify a tran sformation that is biased by
the di↵erence in the scale or the units of covariates. Note that an
alternative to performing principal component analysis using the
eigen decomposition is to employ the singular value decomposition
8
8Van Loan, C. F. and G. H. Golub (1983). Matrix computations. Johns Hopkins University Press
procedure. Figure 10.7 presents a PCA transformation applied to the Old
Faithful geyse r example . In (a), the data is normalized by subtract -
ing the empirical average and dividing by the empirical standard
deviation for each covariate. We then obtain the PCA transforma- tion matrix by performing the eigen decomposition of the empirical
covariance matrix estimated from normalized covariates, V = [⌫1 ⌫2] =
 0.71 0.71
0.71 0.71 
. We can see in figure 10.7b that when transformed in the PCA
space, the variance is maximized along the first principal compo￾nent.
2 1 0 1 2 2 1
0
1
2
Eruption time: x˜1 =
x1µˆ1 ˆ1 Time to next eruption:
x˜2 =
x2ˆµ2 ˆ2 (a) Normalized and centered space
2 1 0 1 2 2 1
0
1
2
Principal component #1: z 1
Principal co
mponent #2: z2 (b) Principal component space Figure 10.7: Example of application
of principal component analysis to the normalized and centered Old Faithful data
set.
Fraction of the variance explained In the PCA space, the eigenval- ues describe the variance associat ed with e ach princ ipal com ponent. The fraction of the variance explained by each principal c omponent
I is quantified by the ratio of the eigenvalue associated with the i
th
component divided by the sum of eigenvalues, I =
i
P
X
i=1i
.
In the example presented in figure 10.7, the fraction of the variance
explained by each principal component is I = {0.9504, 0.0496}.probabilistic machine learning for civil engineers 165
Dimension reduction PCA can be employed to redu ce the numbe r
of dimensions describing the covariate space. This is achieved by
removing the eigenvectors, that is, columns from the matrix V, that
are associated with a negligible fraction of the variance explained. Figure 10.8a presents the example of a data se t with K = 3 clusters
and where covariates xi 2 R
3
. In figure 10.8b, we see clusters
identified using the K-means algorithm. By performing PCA on the
normalized and centered data, we find the transformation matrix
V =
2
4
0.7064 0.3103 0.6362
0.1438 0.9430 0.3002
0.6931 0.1206 0.7107
3
5 , where the fraction of the variance explained by each principal
component is I = {85.5,14.2,0 .3}. We can see here that the third
principal component explains a negligible fraction of the variance.
This indicates that the original data set defined in a 3-D space can
be represented in a 2-D space with a negligible loss of informat ion. Such a 2-D space is obtained by removing the last column from the
original PCA matrix,V2 =
2
4
0.7064 0.3103
0.1438 0.9430
0.6931 0.1206
3
5,
and then performing the transformation zi = V
|
2˜xi. Figure 10.8c
presents K-means clusters for the covariates transformed in the 2-D
PCA space, zi 2 R2
.
0
0.25
0.5
0.75
1
0
0.25
0.5
0.75
1
0
0.25
0.5
0.75
1
x 1 x2
x3 (a) Real clusters in a 3-D space
0
0.25
0.5
0.75
1
0
0.25
0.5
0.75
1
0
0.25
0.5
0.75
1
x1 x2
x3 (b) K-means clusters in a 3-D space
1 0.5 0 0.5 1 0.5
0.25
0
0.25
0.5
Principal component #1: z1 Principal component
#2: z2 (c) K-means clusters in the 2-D
principal component space Figure 10.8: Example of application of
the principal component analysis for dimensionality reduction.
Note that principal component analysis is limited to linear di- mensional reduction. For nonlinear cases, we can opt for more
advanced methods such as t-distributed stochastic neighbor em- bedding (t-SNE). 9 In addition to being employed in unsupervised
9Van der Maaten, L. and G. Hinton (2008). Visualizing data using t-SNE. Journal of
Machine Learning Research 9, 2579–2605
learning, PCA and other dimensionality reduction techniques can
be employed in the context of regre ssion and classific ation (se e
chapters 8 and 9) . In such a case, instead of modeling the relation￾ship between the observed system responses and the covariates, the relationship is modeled between the observed system respon se s
and the transformed covariates represented in a lower-dimensional
space where inter-covariate dependence is removed using PCA or
any other dimension-reduction technique.11
Bayesian Networks
Bayesian networks were introduced by Pearl1 and are also known 1 Pearl, J. (1988). Probabilistic reasoning in
intelligent systems: Networks of plausible
inference. Morgan Kaufmann as belief networks and directed graphical models. They are the result
of the combination of probability theory covered in chapter 3 with
graph theory, which employs graphs defined by links and nodes. We saw in §3.2 that the chain rule allows formulating the joint
probability for a set of random variables using conditional and
marginal probabilities, for example, p(x1, x2, x3 ) = p(x3|x2, x1)p(x2|x1)p(x1). Bayesian networks (BNs) employ nodes to represent random vari- ables and directed links to describe the dependencies between them. Bayesian networks are probabilistic models where the goal is to
learn the joint probability defined over the e ntire network. The joint
probability for the structure encoded by these nodes and links is
formulated using the chain rule. The key with Bayesian networks is
that they allow building sparse models for which efficient variable
elimination algorithms exist in order to estimate any conditional
probabilities from the joint probability. BNs can be categorized as unsupervised learning
2 where the goal 2 Ghahramani, Z. (2004). Unsupervised
learning. In Advanced lectures on ma- chine learning, Volume 3176, pp. 72–112. Springer
is to estimate the joint probability density function (PDF) for a set
of observed variable s. In its most general form we may seek to learn
the structure of the BN itself. In this chapter, we restrict ourselves
to the case where we know the graph structure, and the goal is to
learn to predict unobserved quantities given some observed ones. T
V
F
Temperature: t 2 {cold, hot}
Virus: v 2 {yes, no}
Flu: f 2 {sick, ¬sick}
Figure 11.1: Example of Bayesian network
for representing the relationships between
temperature, the presence of the flu virus, and being sick from the flu virus.
Flu virus example Section 3.3.5 presented the distinction between
correlation and causality using the flu virus example. A Bayesian
network can be employed to model the dependence between the
temperature, the presence of the flu virus, and being sick from the
flu. We model our knowledge of the se three quantities using discrete
random variables that are represented by the nodes in figure 11.1.
The arrows represent the dependencies between variab les: The
temperature T a↵ects the virus prevalence V , which in turns a↵ectsj.-a. goulet 168
the probability of catching the virus an d being sick F . The absence
of a link between T and F indicates that the temperature and
being sick from the flu are conditionally independen t from each
other. In the context of this example, conditional independence
implies that T and F are independent when V is known. The joint
probability for T , V , and F ,
p(f, v,t) =
=p(f|v,t) z }| { p(f|v)· p(v|t) · p(t) | {z }
p(v,t)
,
is obtained using the chain rule where, for each arrow, the cond i- tional probabilities are described in a conditional probability table.
Virus example: Marginal and condi- tional probability tables p(t) = {p(cold), p(hot)} = {0.4, 0.6}
p(v|t) = 8
<
:
t = cold t = hot v = yes 0.8 0.1
v = no 0.2 0.9
p(f|v) = 8
<
:
v = yes v = no
f = sick 0.7 0
f = ¬sick 0.3 1
Joint probability using chain rule
p(v,t) = p(v|t) · p(t) = 8
<
:
t = cold t = hot v = yes 0.8 ⇥ 0.4 0.1 ⇥ 0.6
v = no 0.2 ⇥ 0.4 0.9 ⇥ 0.6 = 8
<
:
t = cold t = hot v = yes 0.32 0.06
v = no 0.08 0.54
p(f, v,t) = p(f|v) · p(v,t)
=
8
>>
><
>>
>:
f = sick t = cold t = hot v = yes 0.32⇥0.7 0.06⇥0.7
v = no 0.08 ⇥ 0 0.54 ⇥ 0
f = ¬sick t = cold t = hot v = yes 0.32⇥0.3 0.06⇥0.3
v = no 0.08 ⇥ 1 0.54 ⇥ 1
=
8
>
>
<
>>
>:
f = sick t = cold t = hot v = yes 0.224 0.042
v = no 0 0
f = ¬sick t = cold t = hot v = yes 0.096 0.018
v = no 0.08 0.54
Variable elimination: Marginalization
p(f,t) = X
v p(f, v,t) = 8
<
:
t = cold t = hot
f = sick 0.224 0.042
f = ¬sick 0.176 0.558
p(f) =X
t p(f,t) =
⇢
f = sick 0.266
f = ¬sick 0.734
p(t|f) = p(f,t)/p(f) = 8
<
:
t = cold t = hot
f = sick 0.84 0.16
f = ¬sick 0.24 0.76
In the case where we observe F = f , we can employ the
marginalization operation in order to obtain a conditional prob￾ability quantifying how the observation f 2 {sick, ¬sick} changes
the probability for the tempe rat ure T, p(t|f) =
p(f,t)
p(f) = P
v p(f, v,t)
P
tP
v p(f, v,t)
.
In minimalistic problems such as this one, it is trivial to calculate
the joint probability using the chain rule and eliminating variables
using marginalization. However, in practical cases involving dozens
of variables with as many links between them, these calculat ions
become computationally demanding. Moreover, in practice, we
seldom know the marginal and conditional probabilities tables. A key interest of working with directed graphs is that efficient
estimation methods are available to perform all those tasks. Bayesian networks are applicable not only for discrete ran￾dom variables but also for continuous ones, or a mix of both. In
this chapter, we restrict ourselves to the study of BN for discrete
random variables. Note that the state-space models presented in
chapter 12 can be seen as a time-dependent Bayesian network using
Gaussian random variables with linear dependence models. This
chapter presents the nomenclature employed to define graphic al
models, the methods for performing infe re nc e, and the methods
allowing us to learn the conditional probabilities defining the de￾pendencies between random variables. In addition, we present an
introduction to time-dependent Bayesian networks that are referred
to as dynamic Bayesian networks. For advanced topics regarding
Bayesian networks, the reader is invited to consult specialized text￾book s such as the one by Nielsen and Jensen3 or Murphy’s PhD
3 Nielsen, T. D. and F. V. Jensen (2007). Bayesian networks and decision graphs. Springer
thesis.4
4 Murphy, K. P. (2002). Dynamic Bayesian
networks: representation, inference and
learning. PhD thesis, University of Califor- nia, Berkeleyprobabilistic machine learning for civil engineers 169
11.1 Graphical Models Nomenclature
Bayesian networks employ a special type of graph: directed acyclic
graph (DAG). A DAG G = {U, E} is defined by a set of node s U
interconnected by a set of directed links E. In order to be acyclic,
the directed links between variables cannot be defined such that
there are self-loops or cycles in the graph. For a set of random vari- ables, there are many ways to define links between variables, each
one leading to the same joint probability. Note that direct e d links
between variables are not required to describe causal relationships. Despite causality not being a requirement, it is a key to efficien cy ;
if the directed links in a graphical model are assigned following the
causal relationships, it generally produces sparse models requiring
the definition of a smaller number of conditional probabilities than
noncausal counterparts.
X2 X3
X1
X4
(a) Directed acyclic graph (DAG)
X2 X3
X1
(b) Directed graph
containing a cycle Figure 11.2: Example of dependence
represented by directed links between
hidden (white) and an observed (shaded) nodes describing random variables.
Figure 11.2a presents a directed acyclic graph and (b) presents a
directed graph containing a cycle so that it cannot be modeled as a
Bayesian network. In figure 11.2a random variables are represented
by nodes, where the observed variable X4 depends on the hidden
variables X2 and X3. The directions of links indicate that X2 and
X3 are the parent of X4 , that is, parents(X4 ) = {X2, X 3}, and
consequently, X4 is the child of X2 and X3. Each child is asso c iat ed
with a conditional probability table (CPT) whose size depends on
the number of parents, p(xi
|parents(Xi)). Nodes without parents
are described by their marginal prior probabilities p(xi ). The joint
PDF p(U) for the entire Bayesian network is formulated using the
chain rule,
p(U) = p(x1, x2, · · · , xX) = Y
X
i=1
p(xi
|parents(Xi)). This application of the chain rule requires that given its parents, each node is independent of its other ancestors.
(a) Cyanobacteria
seen under a micro- scope
(b) Cyanobacteria bloom in a rural environment
Figure 11.3: Example of cyanobacteria bloom. (Photo: NASA and USGS)
Cyanobacteria
Fish mortality Water color
Temperature Fertilizer
(a) Bayesian network semantic representa- tion
C
M W
T F
(b) Bayesian network represented with
random variables Figure 11.4: Bayesian network for the
cyanobacteria example.
Cyanobacteria example We explore the example illustrated in
figure 11.3 of cyanobacteria blooms that can o cc ur in lakes, rivers, and estuaries. Cyanobacteria blooms are typically caused by the
use of fertilizers that wash into a water body, combined with warm
temperatures that allow for bacteria reproduction. Cyanobacteria
blooms can cause a change in the water color and can cause fish or
marine life mortality. We employ a Bayesian network to describe
the joint probability for a set of random variables consisting of the
temperature T , the use of fertilizer F , the presence of cyanobacteria
in water C, fish mortality M, and the water color W. In figure 11.4,j.-a. goulet 170
the definition of the graph structure and its links follow the causal
direction where the presence of cyanobacteria in water C depends
on the temperature T and the presence of fertilizer F . Both the
fish mortality M and water color W depend on the presence of
cyanobacteria in water C.
11.2 Conditional Independence
As stated earlier, the absence of a link between two variables in- dicates that the pair is conditionally independent. In the case of
a serial connection, as illustrated in figure 11.5a, the absence of
a link between A and C indicates that these two are independent
if B is known, that is, A ? C|{B = b}. Therefore, as long as B
is not observed, A and C depend on each other through B. It is
equivalent to say that given its parent (i.e., B), C is independent
of all its other non-descendants. In the case of a diverging connec￾tion (figure 11.5b), again the absence of link between A and C
indicates that these two are independent if B is observed. The case
of a converging connection represented in figure 11.5c is di↵erent
from the two others; the absence of link between A and C implies
that both variables are independent unless B, or one of its descen￾dants, is observed. When B, or one of its descendants is observed,
the knowledge gained for B also modifies the knowledge for A and
C. We say that A and C are d-separated in the case of a serial or
diverging connection, where the intermediary variable B is observed, or in the case of a converging connection, where neither the inter- mediary variable B nor one of its descendants is observed. For any
d-separated variables, a change in the knowledge for on e variable
does not a↵ect the knowledge of the others.
A B C
(a) Serial connection
B
A C
(b) Diverging
connection
B
A C
D
(c) Converging
connection Figure 11.5: Cases where A and C are
conditionally independent for the di↵erent
types of connection in Bayesian networks.
C
M W
T F
(a) Serial connection: T ?
M|c and T ? W|c
C
M W
T F
(b) Diverging connection: M ? W|c
C
M W
T F
(c) Converging connection: T⇢? F
Figure 11.6: The concept of conditional in- dependence illustrated using the cyanobac- teria example.
Cyanobacteria example In the example presented in figure 11.4b,
the sets of variables {T, C, M}, {T, C, W}, {F, C,M}, and {F, C,W}
are all examples of serial connections. If as illustrated in fig￾ure 11.6a, we observe the presence of cyanobacteria, that is, C = yes, then the temperature T becomes independent of fish
mortality M and water color W, that is, T ? M|c, T ? W|c. It
implies that gaining knowledge about water temperature would
not change our knowledge about either fish mortality or water
color because these two quantities only depend on the presence of
cyanobacteria, which is now a certainty given that C was observed.
For the diverging connection between variables {M, C,W} illus- trated in figure 11.6b, observing C = c causes the fish mortality
M to be indep en dent from the water color W , M ? W|c. Gainingprobabilistic machine learning for civil engineers 171
knowledge about the occurrence of fish mortality would not change
our knowledge about water color be cau se the se two quantities only
depend on the presence of cyanobacteria, which is a certainty when
C is observed. For the converging connection between variables
{T, C, F} represented in figure 11.6c, despite the absence of a link
between T and F, the temperature is not independent from the
use of fertilizer F if we observe C or one of its descendants, that
is, M or W. Without observing C , M, or W, the knowledge gained
about the use of fertilizer has no impact on our knowledge of the
temperature. On the other hand, if we observe the presence of
cyanobacteria (C = yes), then knowing that no fertilizer is present
in the environment (F = no) would increase the probability that
the temperature is high because there is only a small probability of
having cyanobacteria without fertilizer and with cold temperature.
11.3 Inference
The finality of Bayesian networks is not only to define the joint
probability for random variables; it is to perform inference, that
is, to compute the conditional probability for a set of unobserved
random variables, given a set of observed ones. Let us conside r
U = {X1, X2, · · · , XX}, the set of random variables corresponding
to the nodes defining a system, a subset of observed variables
D ⇢ U, and another subset of query variables Q ⇢ U, such that
Q \ D = ;. Following the definition of condition al probabilities
presented in §3.2, the posterior probability for the variables in Q
given observations D is
p(Q|D) =
p(Q, D)
p(D) = P
U\{Q,D} p(U)
P
U\{D} p(U)
,
where U \ {Q, D} describes the set of variables belonging to U
while excluding those in {Q, D}. For the cyanobacteria example
Cyanobacteria example
U =
8
>
><
>>
:
T : t 2 {cold, hot}
F : f 2 {yes, no}
C : c 2 {yes, no}
M : m 2 {yes, no}
W : w 2 {clear, green}
9
>
>=
>>
;
presented in figure 11.4b, let us consider we want the posterior
probability p(m|w = green), that is, the probability of fish mortality
M given that we have observed colored water, W = green. This
joint probability is described by
p(m|w = green) =
p(m, w = green)
p(w = green) ,j.-a. goulet 172
where both terms on the right-hand side can be obtained through
the marginalization of the joint probability mass function (PMF), p(m, w = green) = X
t
X
f
X
c p(t, f, c, m, w = green)
p(w = green) = X
m
p(m, w = green). This approach is theoretically correct; however, in practice, calcu￾lating the joint probability p(U) quickly becomes computationally
intractable with the increase in the number of variables in U . The
solution is to avoid computing the full joint probability table and
instead proceed by eliminating variables sequentially.
Cyanobacteria example: CPT
p(t) = {p(cold), p(hot)} = {0.4, 0.6}
p(f) = {p(yes), p(no)} = {0.2, 0.8}
p(c|t, f) =
8
>
>
<
>
>
:
c = yes t = cold t = hot
f = yes 0.5 0.95
f = no 0.05 0.8
c = no t = cold t = hot
f = yes 0.5 0.05
f = no 0.95 0.2
p(m|c) = 8
<
:
c = yes c = no m = yes 0.6 0.1 m = no 0.4 0.9
p(w|c) =8
<
:
c = yes c = no w = clear 0.7 0.2 w = green 0.3 0.8
Variable elimination The goal of variable elimination is to avoid
computing the full joint probability table p(U) by working with its
chain-rule factorization. For the cyanobacteria example, the joint
probability is
p(U) = p(t) · p(f) · p(c|t, f) · p(w|c) · p(m|c), (11.1)
where computing p(m, w = green) corresponds to
p(m, w = green) = X
t
X
f
X
c p(t) · p(f) · p(c|t, f) · p(w = green|c) · p(m|c) = X
c p(w = green|c) | {z }
2⇥1
· p(m|c) | {z }
2⇥2
·X
t
p(t) |{z}
2⇥2
·X
f
p(f) |{z}
2⇥2
· p(c|t, f). | {z }
2⇥2⇥2 | {z }
p(c,f|t): 2⇥2⇥2 | {z }
p(c|t): 2⇥2 | {z }
p(c,t): 2⇥2 | {z }
p(c): 2⇥1 | {z }
p(m,c): 2⇥2 | {z }
p(m,w=green,c): 2⇥2 | {z }
p(m,w=green): 2⇥1 = {0.1354, 0.3876}.
Variable elimination
p(c, f|t) =
8
>
><
>
>:
c = yes t = cold t = hot
f = yes 0.5·0.2 0.95·0.2
f = no 0.05·0.8 0.8·0.8
c = no t = cold t = hot
f = yes 0.5·0.2 0.05·0.2
f = no 0.95·0.8 0.2·0.8
p(c|t) = 8
<
:
t = cold t = hot c = yes 0.14 0.83
c = no 0.86 0.17
p(c,t) = 8
<
:
t = cold t = hot c = yes 0.14·0.4 0.83·0.6
c = no 0.86·0.4 0.17·0.6
p(c) =
⇢
c = yes 0.554
c = no 0.446
p(m, c) =8
<
:
c = yes c = no m=yes 0.6·0.554 0.1·0.446 m=no 0.4·0.554 0.9·0.446
p(m, w = green, c) = 8
<
:
c = yes c = no m=yes 0.3324·0.3 0.0446·0.8 m=no 0.2216·0.3 0.4014·0.8
p(m, w8= green) = <
:
w = green m=yes 0.1354 m=no 0.3876
p(w = green) = 0.1354 + 0.3876 = 0.523
Inference
p(m|w = green) =
p(m, w = green)
p(w = green) = 8
<
:
w = green m=yes 0.1354/0.523 m=no 0.3876/0.523 = 8
<
:
w = green m=yes 0.26 m=no 0.74
Note how several terms in equation 11.1 are independent from the
variables in the summation operators. It allows us to take out of
the sums the terms that do not depend on them and then perform
the marginalization sequentially. This proced ure is more efficient
than working with full probability tables. In the previous equation, braces refer to the ordering of operations and indicate the size of
each probability table. The variable elimination procedure allows
working with probability tables containing no more than 2
3 = 8probabilistic machine learning for civil engineers 173
entries. In comparison, the full joint probability table for p(U )
contains 2
5 = 32 entries. The efficiency of the variable elimination approach depends
on the ordering of operations. The common method for ordering
operations while performing variable elimination is the junction
tree method. This method transforms the graph into a tree and
then employs clustering to group nodes. The reader interested
in the details of this inference method should refer to specialized
textbooks. Note that the methods covered ab ove are exact inference
methods. In the case where the number of variables is so large that
exact methods become computationally prohibitive, we can also
resort to approximate metho ds based on Monte Carlo sampling.
11.4 Conditional Probability Estimation U = xi T F C M W
1 cold no yes yes clear 2 hot no yes yes clear 3 hot yes no yes green
.
.
.D hot no no yes green
(a) Fully observed Bayesian network
U = xi T F C M W
1 cold ? yes ? clear 2 ? ? yes yes clear 3 ? yes no yes ?
.
.
.D hot no no ? green
(b) Partially observed Bayesian network
Figure 11.7: Example of data set for
learning conditional probability tables.
In the previous sections, it was always assumed that the probabili- ties contained in conditional probability tables (CPTs) were known. In practice this is seldom the case; CPTs must be learned from
data. There are two typical learning setups; in the first setup, the
Bayesian network is fully observed so each observation Di = {xi}
consists in a joint realization xi
: X ⇠ p(U). Figure 11.7a presents
a table where each line is one observation for the fully observed
BN employed in the cyanobacteria example. In the second learning
setup, the BN is only partially observed so that some variables are
not observed for the realization of xi : X ⇠ p(U ). Figure 11.7b
presents a data set for the cyanobacteria example where the BN is
partially observed.
11.4.1 Fully Observed Bayesian Network
For a fully observed Bayesian network, the maximum likelihood es- timate (MLE) for the conditional probability Pr(X1 = x1|X2 = x2)
can be estimated as the ratio between the number of realizations of
a specific joint outcome {X1 = x1 , X2 = x2}, divided by the total
number of realizations of the outcome {X2 = x2 }, Pˆr(X1 = x1|X2 = x2) = pˆ(x1 |x2 ) = #{X1 = x1 , X2 = x2 }
#{X2 = x2}
. (11.2)
We must be careful in the case where a specific outcome is not Reminder: #{ · } denotes the number of elements in a set. observed in the data set so #{X1 = x1, X2 = x2 } = 0. In such
a case, the MLE will lead to a conditional probability equal to
zero. This situation is problematic because we might not have
observed this specific outcome yet, simply because the number of
observations is too limited. A solution is to employ a maximumj.-a. goulet 174
a-posteriori (MAP) estimation by adding one observation count to
each possible outcome. Given that x1 2 {1, 2, · · · n}, equation 11.2
becomes
Pˆr(X1 = x1|X2 = x2) = #{X1 = x1, X2 = x2} + 1
#{X2 = x2 } + n
. (11.3)
Cyanobacteria example From the data set D presented in fig￾ure 11.7a, we can estimate each CPT involved in the definition of
p(U ) in equation 11.1. For example, we might want to obt ain the
MLE estimate for the probability of having cyanobacteria given
a hot temperature and the presence of fertilizer, p(c = yes|t =
hot, f = yes). In such a case, we compute the number of realiz ations
of {c = yes,t = hot, f = yes} in D, divided by the number of
realizations of {t = hot, f = yes}. If we want to employ the MAP
estimate instead of the MLE, we add one count to the numerator
and two to the denominator so that
pˆ(c = yes|t = hot, f = yes) = #{c = yes,t = hot, f = yes} + 1
#{t = hot, f = yes} + 2
. (11.4)
The same method applies for any entry in a conditional probability
table. In the case of the estimation of marginal probabilities such as
p(t = hot), the calculation simplifies to the number of events where
{t = hot} divided by D+ 2, the number of joint observations in the
data set plus the number of possible outcomes for t,
pˆ(t = hot) = #{t = hot} + 1
D + 2
. Before going further, we will explore the theoretical justification
for the MLE and MAP in equations 11.2 and 11.3. Let us consider
the simplified case where a network is made from X random vari- ables U = {X} = {[X1 X2 · · · XX]| }, where xk 2 {0,1}, 8k 2 {1 :
X} . Here, the definition of a specific structure for the ne twork is not
necessary; we simply assume that the network is structured as a
DAG, so that for each Xk , we have p(x k|parents(Xk )). The quan￾tity we seek to estimate from the data is the condition al probability
✓k = p(xk = 1|parents(Xk)), where because xk is a binary variable,
we also have 1  ✓k = p(xk = 0|parents(Xk)). We describe the prior
probability for ✓k by a Beta PDF (see §4.3), Note: We add +1 to the parameters of the Beta prior because without them, B(✓k;↵0, 0 ) / ✓↵01 k
(1  ✓k)01
, which would lead to an MAP equal to
✓ˆ
k =
↵0 + ↵  1 ↵0 + 0 + ↵ +   2
.
f(✓k ) = B(✓k; ↵0+1, 0 +1) =
✓↵0 k
(1  ✓k) 0 B(↵0 + 1, 0 + 1)
/ ✓↵0 k
(1  ✓k )0 .probabilistic machine learning for civil engineers 175
We have a data set
D = {D1 , D2, · · · , DD} = {U = x1, U = x2, · · · , U = xD}
containing D realizations from the fully observed Bayesian network
U. Here, we are interested in the probability of the joint realization
of X k = 1, along with a specific combination of its pare nts, that
is, parents(Xk ) ⌘ x
pa(k)
. The number of such outcomes in the data
set D is ↵ = #{xk = 1, x
pa(k)} 2 {0, 1, · · · , D}. Analogously, the
number of realizations in the data set of Xk = 0 along with the
same specific combination of its parents is  = #{xk = 0, x
pa(k) } 2
{0,1, · · · , D}, and note that ↵ +  = #{x
pa(k)}. The posterior PDF
f(✓k |D) is formulated following
f(✓k|D) / f (D|✓k) · f(✓k )
/ 0
@Y
D
j=1
p(xj|✓k)1
A· ✓ ↵0 k
(1  ✓k)0
/ 0
@Y
D
j=1
Y
X
i=1
pXi 
[xj
] i|x
pa(i)
j
, ✓k1
A · ✓↵0 k
(1  ✓k )0
. We saw in §6.3 that it is the same value ˆ✓k that maximizes either
f( ✓k |D) or ln f(✓k|D). Therefore, the log-posterior is formulated
following
ln f(✓k|D) / ln
✓ ↵0 k
(1  ✓k )0  +X
D
j=1
X
X
i=1
ln pXi 
[xj
]i|x
pa(i)
j
, ✓k
, where products were replaced by sums. We saw in §5.1 that the
MAP estimator ˆ✓k corresponds to the location where the derivative
of the log-posterior equals zero. The derivative of the log-posterior
is given by
↵ = #{xk = 1, x
pa(k)} 2 {0, 1, · · · , D}
 = #{xk = 0, x
pa(k)} 2 {0, 1, · · · , D}
@ ln f(✓ k |D)
@✓ k =
@ ln(✓↵0 k
(1  ✓k)0)
@✓k
+
@ PD
j=1
ln pXk
[xj
] k|x
pa(k)
j
, ✓k
@✓k =
@ ln(✓↵0 k
(1  ✓k)0)
@✓k
+
@ ln(✓↵
k
(1  ✓k ) )
@✓k =
@ ln(✓↵0 k
(1  ✓k)0)
@✓k
+
@ (↵ ln ✓k +  ln(1  ✓k ))
@✓ k =
(↵0 + 0)✓k  ↵0
✓k (✓k  1) +
(↵ + )✓k  ↵
✓k (✓k  1) =
(↵0 + 0 + ↵ + )✓k  (↵0 + ↵)
✓k (✓k  1) . Note that when taking the derivative of ln f (✓k|D) with respect to
✓k, the sum with respect to i simplifies to a single term involvingj.-a. goulet 176
the parameter ✓k we are trying to estimate, that is, the conditional
log-probability ln pXk (xk |parents(Xk)). By setting the derivative
@ ln f(✓k |D)
@✓k = 0, we obtain the MAP estimator
(↵0 + 0 + ↵ + )✓k (↵0 + ↵)
✓k (✓k  1) = 0 ! ˆ✓k =
↵0 + ↵
↵0 + 0 + ↵ + 
. When we employ the prior parameters ↵0 = 0 = 1, the MAP
estimator simplifies to
ˆ✓k =
↵ + 1
↵ +  + 2
, which is analogous to the formulation given in equat ion 11.3.
If instead of having a binary state xk 2 {0,1} we have xk 2
{0, 1, · · · , n}, then the prior PDF is a Dirichlet distribution, and the The Dirichlet distribution is a gener- alization of the Beta distribution for multivariate domains. same principles apply for the derivation of the MAP estimator.
11.4.2 Parti ally Observed Bayesi an Network
It is common in practice not to be able to observe every variable
in a Bayesian network. In such a case, the metho d presented for
a fully observed BN is not directly applicable, and we can resort
to the expectation maximization (EM) method
5 (see §10.1.1). The
5 Note: Here, we need to satisfy a key hy- pothesis; the underlying process controlling whether a data xi
is available or missing must be independent of its actual value
so that we can say that data is missing at random. For the cyanobacteria example, the hypothesis of independence for the missing data would not be satisfied if, for example, experimentalists were reluctant
to go and collect samples when the water
temperature is T = cold.
EM method consists in repeating two steps recursively until con￾vergence: in the expectation step, we employ the current values con￾tained in the conditional probability tables, along with the inferen ce
procedure presented in §11.3 in order to replace observation counts
by the expected number of observation counts. Take the cyanobacte- ria example, where we want to estimate p(c = yes|t = hot, f = yes)
from a partially observed Bayesian network. We need to replace
the explicit number of realizations #{c = yes,t = hot, f = yes} in
equation 11.4 by the expected number of counts, E[#{c = yes,t = hot, f = yes}] =X
D
i=1
p(c = yes,t = hot, f = yes|Di).
For the first observation D1 = {x1} = {cold, ? , yes, ? , clear}
contained in the data set presented in figure 11.8, the conditional
probability equals zero because the event where the temperature is
t = hot is impossible because we already know for this observation
that the temperature is t = cold,
U = xi T F C M W
1 cold ? yes ? clear 2 ? ? yes yes clear 3 ? yes no yes ?
4 hot yes yes yes green
.
.
.D hot no no ? green Figure 11.8: Example of partially observed
Bayesian network for the cyanobacteria p(c = yes,t = hot, f = yes|D1 ) = 0. data set.probabilistic machine learning for civil engineers 177
For the second observation D2 = {x2} = { ? , ? , yes, yes, clear}, the joint probability to be inferred using the procedure presented in
§11.3 and the current values contained in the CPTs is
p(c = yes,t = hot, f = yes|D2) = p(t = hot, f = yes|c = yes, m = yes, w = clear) 2 (0, 1).
In the case where no variables are missing for an observation, as in
the fourth observation D4, then
p(c = yes,t = hot, f = yes|D4 ) = 1. Once the expectation procedure is completed for all observations
in D, the maximization step c onsists in comput ing eithe r the MLE
or MAP for updat ing the probab ilities contained in CPTs. The
maximization step employs the method presented in §11.4.1 for
fully observed Bayesian networks, this time using the expect e d
number of counts, for example,
pˆ(c = yes|t = hot, f = yes) =
E[#{c = yes,t = hot, f = yes}] + 1
E[#{t = hot, f = yes}] + 2
. The expectation maximization method is intrinsically iterative, whereas the expectation step employs the CPT e ntries found at the
last iteration during the maximization step. Both steps are repeated
until a steady solution is reached.
11.5 Dynamic Bayesian Network
So far we have looked at Bayesian networks for modeling static
systems that do not have a temporal comp onent. When a BN is
defined over time steps, we call it a dynamic Bayesian network
(DBN). Just like for Bayesian networks, the dependence between
variables is described by directed links and conditional probability
tables; the same holds for the dependence between variables over
successive time steps. Figure 11.9a presents the expanded rep re se n- tation of a dynamic Bayesian network, where pairs of hidden stat es
Xt1 and Xt are linked by an arrow that points in the direc tion of
time. Figure 11.9b represents the same network using a compact
notation where the representation of the same state at di↵erent
time steps is replaced by a single state Xt with the double arrows
indicating the links between di↵erent time steps. Figure 11.10 shows
how the notion of time could be introdu ce d in the cyanobacteria
example where the double self-loop arrows indicate the presence of
links between these variables over subsequent time ste ps.
Xt1 Xt Xt+1
Yt1 Yt Yt+1
(a) Expanded DBN
Xt
Yt
(b) Compact DBN
Figure 11.9: Equivalent expanded and
compact representations of a dynamic Bayesian network. In (b), the double￾lined arrow represents the conditional relationship between time steps.
Ct Mt Wt
Tt Ft
Figure 11.10: Dynamic Bayesian network
for the cyanobacteria example. For DBNs, we can apply the sam e conditional probability estima￾tion methods we presented for Bayesian networks in §11.4. In thej.-a. goulet 178
case of inference, specialized algorithms are available to perform
variable elimination for DBN. The details for such algorithms can
be found in dedicated literature.6 6 Murphy, K. P. (2002). Dynamic Bayesian
networks: representation, inference and
learning. PhD thesis, University of Califor- Hidden Markov models Note that the dynamic Bayesian network nia, Berkeley
presented in figure 11.9 represents a special case called a hidden
Markov model (HMM ). An HMM has only one hidden-state variable
at each time t along with one observed variable. The model is
Markovian because the future (Xt+1) is independent of the past
(Xt1) given the present (Xt ), that is, Xt+1 ? Xt1|xt
. The
HMM model is thus defined by an observation model p(yt|xt) and
a transition model p(xt+1|xt). The fire alarm example presented
in §6.2.2 can be described by a hidden Markov model. The reader
interested in specialized inference algorithms for HMM should
refer to dedicated literature. In this book we will instead focus
our attention on the case of a dynamic Bayesian network using
continuous state variable s, that is, state-space models. These models
are described at length in the next chapter. The reader interested
about the relationships between Bayesian networks, HMM, state￾space models, and the Markov decision process (see chapter 15)
should consult the papers by Diard, Bessi`ere and Mazer;7 and
7 Diard, J., P. Bessiere, and E. Mazer
(2003). A survey of probabilistic models using the Bayesian programming method- ology as a unifying framework. In Inter- national Conference on Computational
Intelligence, Robotics and Autonomous Systems (IEEE-CIRAS), Singapore
Ghahramani.8
8 Ghahramani, Z. (2001). An intro- duction to hidden Markov models and
Bayesian networks. International Jour- nal of Pattern Recognition and Artificial
Intelligence 15(1), 9–4212
State-Space Models
State-space models are suited to analyze time-series data. In chap￾ter 8, we saw that it is possible to model time series using regres- sion methods (e .g., see figure 8.30). Regression is suited for model￾ing time series displaying a stationary or trend-stationary baseline
Note: A time series or process is said to be stationary when its statistical proper- ties, such as its mean and variance, are
constant through time, as illustrated in
figure 12.1d. A time series is said to be
trend-stationary if the trend (linear or not) can be removed from it. on which is added recurrent patterns, as displayed in figures 12.1a–
b. In the case of a time series consisting in nonstationary processes
such as the one in figure 12.1c , regression methods typically have
a limite d predictive capacity when extrapolating beyond the cases
covered in the training set.
(a) Stationary base￾line
(b) Trend-stationary
baseline and periodic pattern
(c) Nonstationary
process
(d) Stationary pro- cess Figure 12.1: The concept of time-series
stationarity.
Figure 12.2: A cannon-man trajectory
can be described using the kinematics equations xt+1 = xt + x˙ tt + 1
2 x¨tt2 + w
x˙ t+1 = x˙ t + x¨tt + w˙ x¨t+1 = x¨t + w¨, where w = [w w˙ w¨]| is a perturbation in
the trajectory.
State-space models (SSM ) analyze time series in a di↵erent way
than regre ssion models do. Regression is used in a supervised setup
in order to describe the dependence between system responses and
covariates, whereas in the case of time series, the covariates are
related to time. On the other hand, state-space models bears that
name because it uses a dynamic model to describe the dependence
between the hidden states of a system at subsequent time steps. Take, for example, the task illustrated in figure 12.2 of predicting
the landing location of a projectile; using kinematic equations, we
can derive a linear model of the projectile position at time t0 + t, from the initial position x0, the initial velocity x˙ 0, and the exit an￾gle ✓0. This task is not well suited for regression met hods, yet it c an
easily be formulated as an SSM because there is a kinemat ic model
available for describing the dependence between the hidden states
at a time t and t + t. SSM is a probabilistic method where, given
a model’s structure, the task is to learn how to predict the hidden￾state variables xt+1 given the current knowledge for xt
. Stat e-space
models can be classified as unsupervised learning because it builds
a joint probability density for a set of hidde n-state variables while
most of them are typically never observed. In the context of civil engineering we can, for exam ple, use
SSMs for modeling the degradation of infrastructures over time.j.-a. goulet 182
As illustrat ed in figure 12.3, we employ the state variable x to
describe the condition of a structure. Starting from a known initial
condition x0, we may develop a dynamic model f (x1|x0), similar to
the one in the projectile example, in order to describe the kinematic
process behind degradation. Because our model will inevitably
be an approximation of the real degradation kinematics, we will
recursively update our predictions using observations yt as they
become available. The theory presented in this chapter allows
combining probabilistically the information contained in the prior
knowledge f (xt1), the model f (xt
|xt1), and the observation yt
, in
order to ob tain a posterior estimat e f (xt |y1, · · · , yt ). The procedure
is employed recursively to go from a time step t  1 to t.
Observation:
Predicted state: Updated state:
Initial state:
Figure 12.3: Example of state-space model where the state xt at a time t is estimated
from the combination of an observation yt, a dynamic model describing the conditional
probability f(xt|xt1 ), and the prior knowledge for xt1.
SSMs allow performing several tasks such as removing (i.e., filtering) the obse rvation errors, forecasting future states, and
estimating hidden states that are not directly observable. For
example, in the infrastructure degradation example , even if only the
condition xt is observed, an SSM allows estimating two additional
hidden states : the degradation speed x˙ t and its acceleration x¨t
. State-space models take the ir origin from the field of control theory, where the breakthrough application of the method was the guiding
system for the Apollo mission in the 1960s (see figure 12.4). Figure 12.4: Apollo mission service module. (Photo: NASA)
12.1 Linear Gaussian State-Space Models
We can describe the mathematical formulation for SSMs using
the same theory we covered in §3.4.1, for the linear functions of
random variables, and in §4.1.3, where we explored the analytic
formulation of the multivariate Normal conditional probability
density function (PDF). Let us c onsider a linear model Y = AX + b, where X ⇠ N (x; µX, ⌃X), and a vector of observations y. We saw
in §3.4.1 that the output for linear functions of Normal random
variables is also Normal, µY = AµX + b
⌃Y = A⌃X A|
⌃XY = ⌃XA|
9
>=
>;
 X
Y

,  µX
µY

,  ⌃X ⌃XY
⌃
|
XY ⌃Y

.
In §3.4.1, we saw that the conditional PDF of X given the observa￾tions y is obtained by dividing the joint PDF of X and Y by the
PDF of Y evaluated for observations y,
posterior knowledge
z }| {
fX|y(x| Y = y
| {z }
observations
) =
prior knowledge
z }| {
fXY (x, y)
fY (y) = N (x; µX|y, ⌃X|y),probabilistic machine learning for civil engineers 183
where the conditional mean vector and covariance matrix are
µX|y = µX + ⌃XY ⌃
1
Y
(y  µY ), ⌃X|y = ⌃X  ⌃XY ⌃
1
Y ⌃
|
XY.
12.1.1 Basic Problem Setup
We employ a basic setup involving a single hidden-state variable
in order to illustrate how the notions of linear functions of random
variables and the multivariate Normal conditional PDF are behind
the stat e-space model theory. We use t 2 {0,1, · · · , T} to describe
discrete time stamps, xt to describe a hidden state of a system at a A hidden state xt is a deterministic, yet unknown, quantity. time t, and yt to describe an imperfect observation of xt . Observation and transition models The relation between an obser- vation and a hidden state is described by the observation model,
yt = xt + vt | {z }
observation model
,
observation error z }| { vt : V ⇠ N (v; 0, 
2
V
). We call the observation yt
imperfect because what we observe is
the true state contaminated by zero-mean Normal-distributed
observation errors. xt is referre d to as a hidden state because it is
not dire ct ly observed; only yt
is. The dynamic model describing the
possible transitions between successive time steps from a state xt1
to xt
is described by the transition model,
xt = xt1 + wt | {z }
transition model
, model error z }| { wt
: W ⇠ N (w; 0, 
2
W ), (12.1)
where wt is a zero-mean Normal-distributed model error describing
the disc re pan cy between the reality and our ide aliz ed transition
model. The transition model in equation 12.1 is Markovian (see
chapter 7) because a state xt only depen ds on the state xt1 and
is thus independent of all previous states. This aspect is key in the
computational efficiency of the method.
Notation
y1:t ⌘ {y1, y2, · · · , yt}
µt|t ⌘ E[Xt|y1:t]
µt|t1 ⌘ E[Xt|y1:t1] 
2
t|t ⌘ var[Xt |y1:t]
Notation In order to keep the notation simple, we opt for compact
abbreviations; a set of observations {y1, y2, · · · , yt } is described by
y1:t and the posterior conditional expected value for Xt given y1:t , E[Xt |y1:t ], is summarized by µt|t
. For the subscript, t|t
, the first
index refers to X t and the second to y1:t
. Following the same nota￾tion, E[Xt
|y1:t1] is equivalent to µt|t1
. The posterior conditional
variance for Xt given y1:t
, var[Xt
|y1:t
], is summarized by 
2
t|t
.j.-a. goulet 184
Prediction step At time t = 0, only our prior knowledge E[X0] =
µ0 and var[X0] = 
2
0 are available. From our prior knowledge at Note: An absence of prior knowledge for X0 can be described by var[X0] ! 1. t = 0, the transition model xt = xt1 + wt
, and the observation
model yt = xt + vt
, we can compute our joint prior knowledge at
t = 1 for X1 and Y1 , f (x1, y1) = N([x1, y1 ]|
; µ, ⌃), where the mean
vector and covariance matrix are de scribed by
µ =
 µX
µY
 =
 E[X1]
E[Y1]

(12.2)
⌃ =
 ⌃X ⌃XY
⌃Y X ⌃Y
 =

var[X1] cov(X1, Y1)
cov(Y1, X1 ) var[Y1]

. (12.3)
The expect ed value E[X1] and variance var[X1] are obtained by
propagating the uncertainty associated with the prior knowledge for
X0 through the transition model (xt = xt1 + w t), Note: We use the notation µ1 and 1
rather than µ1|0 and 1|0 because there is E[X1] ⌘ µ1 = µ0 no observation at t = 0. var[X1] ⌘ 
2
1 = 
2
0 + 
2
W. The expect ed value E[Y 1] and variance var[Y1 ] are obtained by
propagating the uncertainty associated with the prior knowledge for
X1 through the observation model (yt = xt + vt), E[Y1 ] = µ0
var[Y1 ] = 
2
0 + 
2
W | {z }  2
1 +
2
V
.
The covarian ce cov(X1, Y1) = 
2
1 = 
2
0 + 
2
W is equal to var[X1 ]. Update step We can obtain the posterior knowledge at time t = 1
using
f(x1 |y1) =
f(x 1, y1 )
f(y1) = N (x1;µ1|1, 
2
1|1
), where the posterior mean and variance are
µ1|1 = µX + ⌃XY ⌃
1
Y
(y1  µY )

2
1|1 = ⌃X  ⌃XY ⌃
1
Y ⌃
|
XY
. (12.4)
Note that the terms in equation 12.4 correspond to those defin ed
in equat ions 12.2 and 12.3. The posterior f (x1|y1) contains the
information coming from the prior knowledge at t = 0, the system
dynamics described by the transition model, and the observation at
time t = 1.
The predict ion and update steps are now generalized to transi- tion from t  1 ! t. From our posterior knowledge at t  1, the tran￾sition model xt = xt1 + wt
, and the observation model yt =xt + vt
,probabilistic machine learning for civil engineers 185
we compute our joint prior knowledge at t, f(xt, yt |y1:t1) = N ([xt yt]|
; µ, ⌃), where the mean and covariance are
µ =
 µX
µY
 =
 E[Xt
|y1:t1]
E[Yt |y1:t1]

, ⌃ =
 ⌃X ⌃XY
⌃Y X ⌃Y
 =

var[Xt
|y1:t1 ] cov(Xt
, Yt
|y1:t1)
cov(Yt
, Xt|y1:t1) var[Yt|y1:t1]

. Each term of the mean vector an d covariance matrix is obtained
following
E[Xt |y1:t1] ⌘ µt|t1 = µt1|t1
var[Xt |y1:t1] ⌘ 
2
t|t1 = 
2
t1|t1 + 
2
W
E[Yt |y1:t1] = µt|t1
var[Yt |y1:t1] = 
2
t1|t1 + 
2
W
| {z } 
2
t|t1 +
2
V
cov(Xt
, Yt |y1:t1
) =
z }| { 
2
t1|t1 + 
2
W . The posterior knowledge at t is given by
Note: cov(Xt, Yt |y1:t1 )
Following the covariance properties, cov(X1+X2 , X3+X 4) = cov(X1,X3)
· · · + cov(X1,X 4)
· · · + cov(X2,X 3)
· · · + cov(X2,X 4)
therefore, cov(X2,Y2|y1)= cov(X1|1+W,X 1|1+W +V ) = cov(X1|1,X1|1)
· · · + cov⇠⇠⇠:⇠0,X1|1?W
(X1|1
,W)
· · · + cov⇠⇠⇠:⇠0,X 1|1?V
(X1|1,V )
· · · + cov⇠⇠⇠:⇠0,W? X1|1
(W,X1|1
)
· · · + cov(W,W)
· · · + cov⇠⇠:⇠0,W?V
(W,V ) = cov(X1|1
,X1|1
)+cov(W,W) = 
2
1|1+
2W.
f(xt
|y1:t) =
f(xt
, yt
|y1:t1)
f(yt
|y1:t1) = N (xt
;µt|t
, 
2
t|t), where posterior mean and variance are
µt|t = µX + ⌃XY ⌃
1
Y
(yt  µY )

2
t|t = ⌃X  ⌃XY ⌃
1
Y ⌃
|
XY
. The marginal likelihood f(yt
|y1:t1) describing the prior probability Note: f (yt|y1:t1) ⌘ f (yt|y1:t1 ;✓ ), that
is, we can represent either implicitly or explicitly the dependency on the model
parameters ✓ that enter in the definition of
the transition and observation equations. See §12.1.4 for further details. We refer to f(yt|y1:t1) as the marginal
likelihood because the hidden state xt was marginalized. In the case of the multivariate Normal, the solution to
R f(yt , xt|y1:t1 )dxt is analytically
tractable; see §4.1.3.
density of observing y t given all observations up to time t1 is given
by
f(yt |y1:t1) = N (yt
;µt|t1 | {z } E[Yt|y1:t1]
, 
2
t1|t1 + 
2
W + 
2
V
| {z }
var[Yt|y1:t1]
).
Example: Temperature time series We now look at an illustrative
example where we apply the prediction and update steps recursively
to estimat e the temperature xt over a period of three hours using
imprecise observations made every hour for t 2 {1, 2, 3}. Our prior
knowledge is described by E[X0] = 10 C and [X0 ] = 7
C . The
transition and observation models are
yt = xt+ vt | {z }
observation model
,
observation error z }| { vt : V ⇠ N (0, 3
2 |{z} 
2V
),
xt = xt1 + wt
| {z }
transition model
, model error
z }| { wt : W ⇠ N (0, 0.5
2
|{z} 
2W
).j.-a. goulet 186
Performing the prediction step from t = 0 ! t = 1 leads to
E[X1 ] = E[X0 ] = 10
E[Y1 ] = E[X1 ] = 10
var[X1 ] = var[X0] + 
2
W = 49.25
var[Y1 ] = var[X1] + 
2
V = 58.25
cov(X1 , Y1) = var[X1] = 49.25
⇢XY =
cov(X1,Y1) [X1][Y1] = 0.92. At the time t = 1, the observed temperature is y1 = 4.8, leading to
a likelihood f (yt
|y1:t1) = 0.04. Performing the update step leads to
E[X1|1
] = E[X1 ] +
cov(X1,Y1)(y1E[Y1]) var[Y1] = 5.6
var[X1|1] = var[X1] 
cov(X1 ,Y1)2 var[Y1] = 7.6. Following the Gaussian conditional example presented in §4.1.4,
figure 12.5a presents graphically each of the quantities associated
with the prior knowledge at t = 1 obtained by propagating the
knowledge of X0 in the transition and observation models, the
likelihood of yt
, and the posterior knowledge. By repeating the
same predict ion and update steps for t = 2, where y2 = 12 .1, we
obtain
Prediction step
E[X2|1
] = E[X1|1
] = 5.6
E[Y2|1
] = E[X2|1
] = 5.6
var[X2|1
] = var[X1|1
] + 
2
W = 7.9
var[Y2|1] = var[X2|1] + 
2
V = 16.9
cov(X2|1, Y2|1) = var[X2|1] = 7.9
⇢XY =
cov(X2|1,Y 2|1) [X2|1][Y2|1] = 0.68
f(y2|y1) = 0.03
Update step
E[X2|2 ] = E[X2|1] +
cov(X2|1 ,Y2|1)(y2E[Y2|1 ]) var[Y2|1] = 8.6
var[X2|2
] = var[X2|1] 
cov(X2|1,Y2|1)2 var[Y 2|1] = 4.2. These results are illustrated in figure 12.5b. The details of subse - quent ste ps are not presented because they consist in repeating
recursively the same prediction and update steps. Figure 12.5c
shows how, in only three time steps, we went from a di↵use knowl- edge describing X0 to a more precise estimate for X2.
0 1 2 3
0
5
10
15
20
Time - t
State - xt µt|t ⌘ E[Xt|y1:t ]
µt|t ± t|t ⌘ E[Xt|y 1:t] ± [Xt|y1:t] xˇ t (true values)
y t µt|t1 ⌘ E[Xt|y 1:t1] = E[Yt|y 1:t1]
µt|t1±  t|t1 ⌘ E[Xt|y1:t1 ] ± [Xt|y1:t1 ]
E[Yt|y1:t1 ] ± [Yt |y1:t1]
0
10
20
0
4.8
20
yt xt
f x( t, yt|y1:t1)
0 4.8 20
0
yt
f y( t|y1:t1)
0
10
20
4.8 0
20
yt xt
f x( t|y1:t)
(a) t = 1
0 1 2 3
0
5
10
15
20
Time - t
State - xt
0
10
20
0
12.1
20
y xt
t
f x( t, yt|y1:t1)
0 12.1 20
0
yt
f y( t|y1:t1)
0
10
20
0
12.1
20
y xt t
f x( t|y1:t)
(b) t = 2
0 1 2 3
0
5
10
15
20
Time - t
State - xt
0
10
20
0
7.4
20
y xt t
f x( t, yt|y1:t1)
0 7.4 20
0
yt
f y( t|y1:t1)
0
10
20
0
7.4
20
yt xt
f x( t|y1:t)
(c) t = 3
Figure 12.5: Step-by-step probabilistic hidden-state estimation. Note that the pink
curve overlaid on the surfaces describes either the joint or the conditional PDF
evaluated for the observed value yt.
12.1.2 General Formulation
We now extend the prediction and update steps with a formulation
compatible with multiple hidden states. For example, considerprobabilistic machine learning for civil engineers 187
the case where want to use a SSM to estimate the hidden states
xt = [xt x˙ t ]|
, that is, the temperature xt and its rate of change x˙ t. The observation model then becomes
yt = xt + vt ⌘
C=[1 0] z}|{ C x t + vt
, where the observation matrix C = [1 0] indicates that an observa￾tion y t is a function of xt but not of the rate of change x˙ t. vt is a
realization of an observation error V ⇠ N( v; 0, 
2
V
). In the general
case, observation errors are described by a multivariate Normal
random variable V ⇠ N (v; 0,R), where R is the covariance matrix. The transition model for the two hidden states is
xt = xt1 + t · x˙t1 + wt
x˙t = x˙t1 + w˙t  ⌘ xt =
A=

1 t
0 1  z}|{ A xt1 +
wt =
 wt w˙ t
 z}|{ wt
, where the transition matrix A describes the model kinematics and
wt are realiz at ions of the model errors W ⇠ N(w; 0,Q) that a↵ect
each of the hidden states. Observation model
yt = Cxt + vt, v : V ⇠ N (v; 0, R) | {z } Observation errors C: Observation matrix R: Observation errors covariance Transition model xt =Axt1+wt , w : W ⇠ N (w; 0, Q) | {z } Transition errors A: Transition matrix Q: Transition errors covariance
In the prediction step, we compute our prior knowledge at t from
our posterior knowledge at t  1, using the tran sition model and the
observation model, µt|t1 = Aµt1|t1
⌃t|t1 = A⌃t1|t1 A| + Q
E[Yt|y1:t1] = Cµt|t1
cov(Yt |y1:t1 ) = C⌃t|t1C| + R
cov(Xt
,Yt |y1:t1 ) = ⌃t|t1C|.
(12.5)
These terms are collectively describing our joint prior knowledge
f (xt
,yt |y1:t1) = N ([xt yt
]|;µ, ⌃) with the mean vector and
covariance matrix defined as
µ =
 µX
µY
 =
 µt|t1
E[Yt|y1:t1]

⌃ =
 ⌃X ⌃XY
⌃
|
XY ⌃Y
 =
 ⌃t|t1 cov(Xt
,Yt|y1:t1)
cov(Xt,Yt |y1:t1)|
cov(Yt |y1:t1)

.
In the update step, the posterior knowledge is obtained using the
conditional multivariate Normal PDF,
f(xt
|y1:t ) =
f(xt
,yt
|y1:t1)
f(yt
|y1:t1) = N (xt
; µt|t
, ⌃t|t ), which is defined by its poste rior me an vector and covariance matrix, µt|t = µX + ⌃XY⌃
1
Y (yt  µY )
⌃t|t = ⌃X  ⌃XY⌃
1
Y ⌃
|
XY. (12.6)j.-a. goulet 188
The posterior knowledge f (xt |y1:t) combines the information com￾ing from the prior knowledge {µ0 , ⌃0}, the sequence of observation
y1:t
, and the system dynamics described by the transition model. Equations 12.5 and 12.6, obtained using the theory presented in
§3.4.1 and §4.1.3 are referred to as the Kalman filter 1 where they 1 Kalman, R. E. (1960). A new approach
to linear filtering and prediction problems. Transactions of the ASME–Journal of
Basic Engineering 82 (Series D), 35–45
are reorganized as
Prediction step
µt|t1 = Aµt1|t1 Prior mean
⌃t|t1 = A⌃t1|t1 A| + Q Prior covariance
Update step
f(xt|y1:t) = N (xt; µt|t
, ⌃ t|t)
µt|t = µt|t1 + Kt rt Posterior expected value
⌃t|t = (I  KtC)⌃t|t1 Posterior covariance
rt = yt ˆyt Innovation vector ˆyt = Cµt|t1 Prediction observations vector
Kt = ⌃t|t1 C| G1
t Kalman gain matrix
Gt = C⌃t|t1C| + R Innovation covariance mat rix
In the name Kalman filter, the term filter refers to the action
of removing the observation errors from an observed time series. Filtering is an intrinsically recursive operation that is suited for
applications where a flow of data can be continuously processed
as it gets collected. Note that in the classic Kalman formulation,2 2 Simon, D. (2006). Optimal state estima￾tion: Kalman, H infinity, and nonlinear approaches. Wiley
there are additional control terms for including the e↵ect of exter- nal action s on the system. These terms are omitted here becau se , contrarily to the field of aerospace, where flaps, rudders, or trust
vectors are employed to continuously modify the state (i.e., position,
speed, and acceleration) of a vehicle traveling through space, civil
systems seldom have the possibility of such dire ct control actions
being applied to them.
Example: Temperature time series We want to estimate xt = [xt x˙t ]|
, the tempe rat ure and its rate of change over a period of 5 hours us￾ing impre cise observations made every hour for t 2 {1, 2, · · · , 5}. The observation and transition models are respectively
y t =
[1 0]
z}|{ C xt +
V ⇠N(0,
2V )
z}|{
vt
,
 xt x˙ t 
z}|{
xt =

1 t
0 1 
z}|{ A xt1 +
W⇠N(0,Q) z}|{
 wt w˙ t 
z}|{ wt
. Our prior knowledge for the temperature is E[X0] = 10C , [X0] =probabilistic machine learning for civil engineers 189
2
C, and for the rate of change, E[X˙ 0 ] = 1
C , [X˙ 0] = 1.5
C. We assume that our knowledge for X0 and X˙ 0 is independent
(X0 ? X˙ 0 ! ⇢X0X˙0 = 0), so the prior mean vector and covariance
matrix are
µ0 =

10
0

, ⌃0 =

2
2 0
0 1.5
2 
. The observation-errors standard deviation is V = 4
C. Model
errors are defined by the covariance matrix Q, which describes
the errors on the temperature estimate and its rate of change. We
assume that these errors are caused by a constant acceleration that
takes place during the interval t, which is not accounted for in the
model, that builds on the hypothesis of a constant speed and no
acceleration. In order to define the covariance matrix Q, we need
to define a new vector zt = [zt z˙t z¨t
]| that desc ribes a realization
of Zt ⇠ N (zt
;0, ⌃Z), that is, the e↵ect of a constant acc elerat ion
having a random magnitude such that Zt ? Zt+i
, 8i 6= 0. As
illustrated in figure 12.6, having a constant acceleration z¨t over a
time ste p of length t causes a change of z¨tt in the trend (i.e., speed) and a change of z¨t
t22
in the level. We can express the errors
caused by the constant acceleration in zt
in terms of our transition
errors wt as
(acceleration)
(trend/speed)
(level)
Figure 12.6: The error caused on the level and the trend by a constant acceleration z¨t over a time step of length t.
Wt = AzZt ⇠ N(wt
;0, Az⌃z A
|
z | {z } Q
),
where
Az =

0 0 t2 2
0 0 t

, ⌃z =
2
4
0 0 0
0 0 0
0 0 
2
W
3
5.
Therefore, the defin ition of the matrix Q for the transition errors
Wt
is
Q = Az⌃z A|
z = 
2
W
" t44
t32
t32 t2
#
.
For further details regarding the derivation of the proce ss noise
covariance matrices, the reader should consult the textbook by Bar- Shalom, Li, and Kirubarajan.3
In this example, the noise parameter
3Bar-Shalom, Y., X. Li, and T. Kirubara￾jan (2001). Estimation with applications to
tracking and navigation: Theory algorithms and software. John Wiley & Sons
is equal to 
2
W = 0.1 and t = 1. Figure 12.7 presents the state
estimates for the temperatu re and its rate of change along with the
true values that were employe d to generate simulated observations.
0 1 2 3 4 5 10 5
0
5
10
15
Time - t
State - xt µt|t⌘ E[X t|y1:t]
µt|t± t|t ⌘ E[Xt|y1:t ] ± [Xt|y1:t]
yt ˇx t (true values)
µt+1|t ⌘ E[Xt+1|y1:t] = E[Yt+1 |y1:t]
µt+1|t ± t+1|t ⌘ E[Xt+1 |y1:t] ± [Xt+1|y1:t]
E[Yt+1|y1:t] ± [Yt+1 |y1:t ]
0 1 2 3 4 5 4 2
0
2
Time - t
Rate of change - x˙ t Figure 12.7: Example of hidden-state
estimation for a constant-speed model.j.-a. goulet 190
12.1.3 Forecasting and Smoothing
The filtering method presented in the pre viou s section estimates
f (xt|y1:t), that is, the posterior probab ility density of hidden states
xt, given all the observations up to time t. With this procedure, acquiring new data at a time t + 1 does not modify our knowledge
about xt
. This second task of estimating the posterior probability
of hidden states xt given all the observations up to time T > t, f (xt|y1:T), is referred to as smoothing. A third task consists in
forecasting the hidden states xt+n, n time steps in the future, given all the observations up to time t, that is, f (xt+n|y1:t ). The
three tasks of filtering, smoothing, and forecasting are illustrated
schematically in figure 12.8.
Filtering
0 t
Smoothing
0 t
Forecasting
0 t
Figure 12.8: Comparison of filtering, smoothing, and forecasting, where the blue
region corresponds to the data employed to
estimate xt or forecast xt+n . Forecasting: f(xt+n|y1:t) In the context of SSMs, forecasting is
trivial because it only involves recursively propagating the current
knowledge µt|t
, ⌃t|t at time t through the prediction step. In short, forecasting from a time t to t + 1 follows
f(xt+1|y1:t) = N (xt+1; µt+1|t
, ⌃t+1|t)
µt+1|t = Aµt|t
⌃t+1|t = A⌃t|tA| + Q. Note that whenever data is missing at a given time step, only the
prediction step is recursively applied until new data is obtained. Therefore, missing dat a is treated in the same way as forecasting
using the transition model. This is an interesting feature for SSMs, which do not require any special treatment for cases where data is
missing.
0 1 2 3 4 5 6 7 8 9 10 10 5
0
5
10
15
Time - t
State - xt µt|t ⌘ E[X t|y1:t]
µt|t ± t|t ⌘ E[Xt|y1:t] ± [Xt |y1:t]
ytˇxt (true values)
µt+1|t ⌘ E[Xt+1
|y1:t] = E[Yt+1
|y1:t]
µt+1|t ± t+1|t⌘ E[Xt+1|y1:t] ± [Xt + 1|y1:t]
0 1 2 3 4 5 6 7 8 9 10 4
2
0
2
Time - t
Rate of change - ˙xt Figure 12.9: Example of hidden-state
forecast for a constant-speed model.
Example: Temperature time series (continued) We build on the
temperature estimation example presented in figure 12.7 to illus- trate the concept of forecasting. The goal here is to forecast the
temperature for five hours after the last time step for which data is
available. The result is presented in figu re 12.9. Note how, because
no observation is available to update the predictions, the standard
deviation of the estimated hidden states keeps increasing because
at each time step, the model error covariance matrix Q is added
recursively to the hidden-state covariance matrix.
Smoothing: f (xt|y1:T) With smoothing, we perform the reverse
process of updating the knowledge at a time t, using the knowledge
at t + 1. Smoothing starts from the last time step T estimated using
the filtering procedure described in §12.1.2 and moves recursivelyprobabilistic machine learning for civil engineers 191
backward until the initial time step is reached. The information flow
during filte ring and smoothing is illustrated in figure 12.10. The
Filter:
Smooth:
t
µ0
⌃0
0
µ1|1
⌃1|1
...
1
µt|t
⌃t|t
t
... µT1|T1
⌃ T1|T1
T  1
µT|T
⌃T|T
T
µT1|T
⌃T1|T
... µt|T
⌃t|T
... µ1|T
⌃1|T
µ0|T
⌃0|T
Figure 12.10: Information flow for the filtering and smoothing processes.
posterior knowledge f (xt |y1:T ) combines the information coming
from the prior knowledge {µ0 , ⌃0}, the entire sequence of observa￾tion y1:T
, and the transition model. The smoothing procedure can
be employed to learn initial hidden states {µ0, ⌃0} ! {µ0|T
, ⌃0|T}. Updating initial values using the smoother typically improves the
model quality. The most common algorithm for performing smooth￾ing is the RTS Kalman smoother,4 which is defined following
4Rauch, H. E., C. T. Striebel, and F. Tung
(1965). Maximum likelihood estimates of lin- ear dynamic systems. AIAA Journal 3(8), 1445–1450 f(xt
|y1:T) = N (xt
; µt|T
, ⌃t|T)
µt|T = µt|t + Jt µt+1|T  µt+1|t
⌃t|T = ⌃t|t + Jt⌃t+1|T  ⌃t+1|t J
|
t
Jt = ⌃t|tA|⌃
1
t+1|t
. As illustrat ed in figure 12.10, the RTS Kalman smoother is per￾formed iterat ively starting from the Kalman filter step t = T  1,
where the mean vector µt+1|T = µT|T and covariance matrix
⌃t+1|T = ⌃T|T are already available from the last Kalman filter
step.
0 1 2 3
0
5
10
15
20
Time - t
State - xt
µt|t µt|t ± t|t µt|T ± t|T µt|T xˇt
Figure 12.11: Comparison of filtering, smoothing, and the true states.
Example: Smoothing temperature estimates Figure 12.11 presents
the RTS smoothing procedure applied to the te mpe rat ure esti- mation exam ple initially presented in figure 12.5. These results
show how, between the filtering and smoothing procedures, the
mean at t = 2 went from 8.6 to 8.4, and the variance from 4.2
to 1.8. The uncertainty is reduced with smoothing because the
estimate var[X2|3] is based on one more data point than var[X2|2 ]. Figure 12.11 presents the entire smoothed estimate, which almost
coincides with the true values and has a narrower uncertainty
confidence region than the filtering estimat es.j.-a. goulet 192
12.1.4 Parameter Estimation
The definition of matrices A, C, Q, and R involved in the observa￾tion and transition models requ ire s estimat ing several paramet ers ✓
using observations D = {y 1:T}. As for other methods presented in
previous chapters, the posterior for paramet e rs is
f(✓|D) =
f(D|✓)f(✓)
f(D)
.
In this Bayesian formulation for estimating the posterior PDF
for paramet ers, the key component is the marginal likelihood, f (D|✓) ⌘ f (y1:T
|✓). With the hypothesis that observations are
conditionally independent given the states xt, the joint marginal
likelihood (i.e., the joint prior probability density of observations We refer to f(y1:T|✓) as the joint marginal
likelihood because the hidden variables x1:T are marginalized. In the remainder of this chapter, for the sake of brevity, we use the term likelihood instead of marginal
likelihood.
given a set of parameters) is
f(y1:T
|✓) = Y
T
t=1
f(yt
|y1:t1 , ✓), where the marginal prior probability of each observation is obtained
using the filtering procedure, f(yt
|y1:t1, ✓) = N (yt
; Cµt|t1
, C⌃t|t1C| + R). The posterior f (✓|D) can be estimated using the sampling tech￾niques5 presented in chapter 7. However, because evaluating
5Nguyen, L. H., I. Gaudot, and J.-A. Goulet (2019). Uncertainty quantification
for model parameters and hidden state variables in Bayesian dynamic linear models. Structural Control and Health
Monitoring 26(3), e2309
f (y1:T|✓) can be computationally demanding, most practical appli- cations found in the literature only employ the maximum likelihood
estimate (MLE) rath er than estimating the full posterior with a
Bayesian approach. For an MLE, the optimal set of parameters is
then defined as
✓
⇤ = arg max
✓
log-likelihood z }| {
ln f (y1:T
|✓), where becau se often f (yt
|y1:t1 , ✓) ⌧ 1, the product of the
marginals QT
t=1 f (yt|y1:t1 , ✓) ⇡ 0 is sensitive to zero underflow
issues. W hen the contrary happens, so that f(yt|y1:t1 , ✓)  1,
the proble m of numerical overflow arises. In order to mitigat e these
issues, the log-likelihood is employed instead of the likelihood,
ln f (y1:T
|✓) = X
T
t=1
ln f (yt
|y1:t1, ✓).
Figure 12.12: The MLE optimal parame- ters correspond to the location where the derivative of the log-likelihood equals zero.
As illustrat ed in figure 12.12, the optimal set of parame te rs
✓
⇤ corresponds to the location where the derivative of the log￾likelihood equals ze ro. The values ✓
⇤ can be found using a gradientprobabilistic machine learning for civil engineers 193
ascent method such as the Newton-Raphson algorithm presented in
§5.2. Note that because Newton-Raphson is a convex optimization
method, and because SSMs typically involve a non-convex (see
chapter 5) and non-identifiable (see §6.3.4) likelihood function , the
optimization is partic ularly sensitive to initial values ✓0. It is thus
essential to initialize parameters using engineering knowledge and
to try several initial starting locations. Note that a closed-form expectation maximization method exists
for estimating the parameters of SSMs.6 However, even if it is mat h- 6Ghahramani, Z. and G. E. Hinton (1996). Parameter estimation for linear dynamical systems. Technical Report CRG-TR-96-2, University of Toronto, Dept. of Computer Science
ematically elegant, in the context of civil engineering applications
such as those presented in the following se ct ion s, it is still sensitive
to the selection of the initial values and is thus easily trapped in
local maxima.
12.1.5 Limitations and Practical Considerations
There are some limitations to the Kalman filter formulation. First,
it is sensitive to numerical errors when the covariance is rapidly
reduced in the measurement step, for example, when accurate
measurements are used or after a period with missing dat a, or
when there are orders of magnitude separating the eigenvalues
of the covariance matrix. Two alternative formulations that are
numerically more robust are the UD filter and square -root filter. The reader interested in these methods should consult specialized
textbooks such as the one by Simon7 or Gibbs.8 7 Simon, D. (2006). Optimal state estima￾tion: Kalman, H infinity, and nonlinear approaches. Wiley
8Gibbs, B. P. (2011). Advanced Kalman
filtering, least-squares and modeling: A
practical handbook. Wiley
A second key limiting aspect is that the formulation of the
Kalman filte r and smoother are only applicable for linear observa￾tion and transition models. When problems at hand require nonlin- ear models, there are two main types of alternatives: First, there
are the unscented Kalman filter 9 and extended Kalman filter,10 9Wan, E. A. and R. van der Merwe (2000). The unscented Kalman filter for nonlinear estimation. In Adaptive Systems for Signal
Processing, Communications, and Control
Symposium, 153–158. IEEE
10 Ljung, L. (1979). Asymptotic behavior of
the extended Kalman filter as a parameter estimator for linear systems. IEEE
Transactions on Automatic Control 24 (1), 36–50
which allow for an approximation of the Kalman algorithm while
still describing the hidden states using multivariate Normal random
variables. The second option is to employ samp ling methods such
as the particle filter,11 where the hidden-state variables are de￾11 Del Moral, P. (1997). Non-linear filtering:
interacting particle resolution. Comptes Rendus de l’Acad´emie des Sciences—Series
I—Mathematics 325(6), 653–658
scribed by particles (i.e., samples) that are propagated through the
transition model. The weight of each particle is updated using the
likelihood computed from the observation model. Particle filtering is
suited to estimat e PDFs that are not well represented by the multi- variate Normal. The reader interested in these nonlinear-compatible
formulations can consult Murphy’s12
textbook for further details. 12Murphy, K. P. (2012). Machine learning: A probabilistic perspective. MIT Pressj.-a. goulet 194
12.2 State-Space Models with Regime Switching
The state-space model theory we have covered so far is applicable
for a set of system responses following a same regime. Figures
12.13a–b are both examples of behaviors where SSMs are applic able. Figure 12.13c presents an example where a standard SSM is not
applicable because there is a switch from a z e ro-spe ed regime to a
constant-spe ed regime.
(a) Local level (b) Local trend
(c) Regime switching
Figure 12.13: Comparison between single- and multiple-regime models.
Estimating the probability of hidden states along with the
probability of each regime involves computational challenges. For
example, if we consider a case with S = 2 possible regimes, we
then have 2 possible transitions along with 2 models to describe
each regime. We start from a time step t, where the system can be
in regime 1 with probability ⇡
1
1 and in regime 2 with probability
⇡
2
1 = 1  ⇡
1
1
, as illustrated in figure 12.14a. There are 2 regime s ⇥
2 possible transition paths, leading to 4 possible combinations to
describe the joint probability at times t and t + 1. Going from t + 1
to t + 2, each of the 4 paths at t+ 1 bran ches into 2 new ones at
t+ 2, leading to a total of now 8 paths. We see through this example
that the number of paths to c onside r increases exponentially (S
T)
with the number of time steps T. Even for short time series, this
approach becomes computationally prohibitive. In this section, we
present the generalized pseudo Bayesian algorithm of order two,13
13Blom, H. A. P. and Y. Bar-Shalom
(1988). The interacting multiple model algorithm for systems with Markovian
switching coefficients. IEEE Transactions on Automatic Control 33 (8), 780–783
which is behind the switching Kalman filter .14 The key aspect of
14Murphy, K. P. (1998). Switching Kalman
filters. Citeseer; and Kim, C.-J. and C. R. Nelson (1999). State-space models with
regime switching: Classical and Gibbs- sampling approaches with applications. MIT
Press
this method is to collapse the possible transition paths at each time
step, as illustrated in figure 12.14b.
12.2.1 Switching Kalman Filter
We first explore the switching Kalman filter (SKF) while consid- ering only S = 2 possible regimes. The goal is to estimate at each
time step t the probab ility of each regime, along with the estimates
for the hidden states corresponding to each of them. The notation
that will be employed in the SKF is model i
where the superscript on the mean vector and covariance matrix
refers to the regime number, which is described by the variable
s 2 {1,2, · · · , S}. ⇡
i
t|t
is the shorthand notation for the probability
of the regime i at time t, given all the available observations up to
time t, Pr(St = i|y1:t).
(a) Regime switching without collapse
Merge
(b) Regime switch￾ing with merge
(SKF)
Figure 12.14: Comparison of regime
switching with and without merging the
state at each time step. (Adapted from
Nguyen and Goulet (2018).)probabilistic machine learning for civil engineers 195
Filter with model
Filter with model
t-1model i
Figure 12.15: Switching Kalman filter: How initial states transition through two distinct models.
Filtering step As presented in figure 12.15, starting from regime 1,
at t  1, there are two possible paths, each of them corresponding
to one of the regime switches. The two possible paths j 2 {1,2} are
explored by estimating the posterior state estimates {µ
1,j
t|t
, ⌃
1,j
t|t }, along with their likelihood L
1,j
t|t
, using the filtering procedure pre- sented in §12.1.2,
(µ
i,j
t|t
, ⌃
i,j
t|t
,L
i,j
t|t) = Filter(µ
i
t1|t1
, ⌃
i
t1|t1
,yt ;Aj , Cj,Qj ,Rj ). (12.7)
In equation 12.7, the initial states are obtained from the regime i, and the matrices {Aj
, Cj
,Qj
,Rj } are those from the model j.
t
Merge
model 1
model 1
model 2
Figure 12.16: The switching Kalman filter merge step.
Once the procedure presented in figure 12.15 is repeated for
each model, the next step consists in merging the state estimates
that land ed in a same regime. Figure 12.16 presents an example
where the state estimates from the model starting from regime
1 at t  1 and transiting through regime 1 are merged with the
state estimat es from the model starting from regime 2 at t  1
and transiting through regime 1. This merge step is performe d by
considering that the final state estimates {µ
i
t|t
, ⌃i
t|t}, along with its
probability ⇡
i
t|t
, describe a multivariate Normal, itself obtained by
mixing the state estimates originating from all paths that transited
from t  1 ! t through model j.
3 0.25 2.5 5.25 8
⇡1 =0.4 ⇡2 =0.6
x
P
D
F : f( ) N (x1, 1, 1
2) N (x2, 4, 1
2) f(x12)
Figure 12.17: Example of a Gaussian mixture PDF.
Gaussian mixture We now take a quick detour from the switching
Kalman filte r in order to review the mathematical details related
to Gaussian mixtures (see §10.1.1) that are behind the merge step.
For example, we have two states X1 and X2 described by Gaussian
random variables so that
X1 ⇠ N (x1;µ1, 
2
1
)
X2 ⇠ N (x2;µ2, 
2
2
). The outcome of the random process, where ⇡1 and ⇡2 = 1 
⇡1 correspond to the probability of each state, is described by a
Gaussian mixture,
X12 ⇠ f (x12) = ⇡1N (x12 ;µ1, 
2
1
) + ⇡2 N (x12 ;µ2 , 
2
2
).j.-a. goulet 196
Figure 12.17 presents an example of two Gaussian PDFs combining
in a Gaussian mixture. We can approximate a Gaussian mixture by
a single Gaussian15 f(x12 ) ⇡ N (x12; µ12, 
2
12
) having a mean and
15Runnalls, A. R. (2007). Kullback-Leibler approach to Gaussian mixture reduction. IEEE Transactions on Aerospace and
variance given by Electronic Systems 43 (3), 989–999
µ12 = ⇡1 µ1 + ⇡2µ2

2
12 = ⇡1 
2
1 + ⇡2
2
2 + ⇡1⇡2(µ1  µ2 )2
. Figure 12.18 presents four examples of Gaussian mixtures ap￾proximated by a single Gaussian PDF. Notice how in (b), when the
PDFs of X1 and X2 have the same parameters, the result coincides
with the two distributions. Also, by comparing figures (c ) and (d), we see that the greater the distance |µ1  µ2| is, the greater 12 is. 2 0 2 4 6
⇡1 =0.9 ⇡2 =0.1
x
P
D
F : f( ) N1(1, 1
2) N2(3, 1
2 ) N12(1.2, 1.17
2)
(a)
2 0 2 4 6
⇡1=0.3 ⇡2 =0.7
x
P
D
F : f( ) N1(2, 1
2) N2(2, 1
2) N12(2, 1
2)
(b)
2 1.5 5 8.5 12
⇡1 =0.3 ⇡2 =0.7
x
P
D
F : f ( ) N1 (2, 1
2 ) N2 (5, 1
2 ) N12(4.1, 1.7
2)
(c)
2 1.5 5 8.5 12
⇡1 =0.9 ⇡2 =0.1
x
P
D
F : f( ) N1(2, 0.5
2) N2(8, 0.5
2) N12 (2.6, 1.87
2 )
(d)
Figure 12.18: Examples of Gaussian mixtures.
Merge step Going back to the switching Kalman filter, the merge
step is based on the approximation of a Gaussian mixture by a
single Gaussian PDF. The joint posterior probability of the regimes
St1 = i and St = j is proportional to the product of the likelihood
L
i,j
t|t
, the prior probability ⇡
i
t1|t1
, and the probability Zi,j of
transiting from st1 = i to st = j, Mi,j
t1,t|t = Pr(St1 = i, St = j|y1:t ) =
L
i,j
t|t
· z
i,j
· ⇡
i
t1|t1
P
i P
j L
i,j
t|t
· Z i,j
· ⇡
i
t1|t1
. (12.8)
The posterior probability of the regime j at time t is obtained by
marginalizing the joint posterior in equation 12.8. For a regime j, this operat ion corresponds to summing the probability of all paths
transiting through model j, irrespective of their origin, ⇡
j
t|t = X
S
i=1
Mi,j
t1,t|t
. The posterior states for each regime j, µ
j
t|t
, ⌃
j
t|t
, are estimated
using
Wi,j
t1|t = Pr(St1 = i|st = j, y1:t ) = Mi,j
t1,t|t
⇡
j
t|t
µ
j
t|t = X
S
i=1
µ
i,j
t|tWi,j
t1|t
µ = µ
i,j
t|t  µ
j
t|t
⌃
j
t|t = X
S
i=1
hWi,j
t1|t(⌃
i,j
t|t + µµ
|
)i
. The general overview of the switching Kalman filter is presented
in figure 12.19. Note that in addition to the parame ters definingprobabilistic machine learning for civil engineers 197
each filtering model, the SKF req uires defin ing the prior probab ility
of each regime ⇡
j
0 along with the S ⇥ S transition matrix Z defining
the probability of transiting from a state at time t  1 to another at
time t + 1.
Filter with model 1
Filter with model 2
t-1 t
Merge
Filter with model 1
Filter with model 2
Merge
model 1
model 2
model 1
model 2
Figure 12.19: General overview of the
switching Kalman filter procedure.
12.2.2 Example: Temperature Data with Regime Switch
17-01 17-05 17-08 17-12
10
11
12
13
Time [YY-MM]
Temperature
yt ± V yt
Figure 12.20: Example of data set where
there is a switch between a zero-speed
regime and a constant-speed regime.
This examp le explores an application of the switching Kalman
filter to temperature data subject to a regime switch. Figure 12.20
presents the data set where there is a switch between a zero-speed
regime and a constant-speed regime. Note that with the SKF, the
filtering model associat ed with each regime must have the same
hidden stat e s. In this example, the two stat e s xt = [xt x˙ t
]| are
the tempe rat ure and its rate of change, with initial states for both
models initialized as
µ0 =

10
0

, ⌃0 =

0.012 0
0 0.0012 
, ⇡0 =

0.95
0.05 
. Note that the initial states {µ0, ⌃0 ,⇡0} can be estimated using the
smoothing algorithm. The first regime is described by a local level
where the rate of change is forced to be equal to 0 in the transition
matrix [A]:,2 = 0,
 xt x˙ t

z}|{ xt =

1 0
0 0 
z}|{ A xt1 +
W⇠N(0,Q11/12)
z}|{
 wt w˙ t

z}|{ wt (Regime 1).j.-a. goulet 198
The second regime is described by a local trend model where
 xt x˙ t
 z}|{ xt =

1 t
0 1  z}|{ A xt1 +
W⇠N(0,Q22/21) z}|{
 wt w˙ t
 z}|{ wt (Regime 2). Describing the four possible paths, that is, {1 ! 1, 1 ! 2, 2 !
1, 2 ! 2}, requires the definition of four covariance matrices for the
transition-model errors, Q11 = Q21 = Q22 = 0, Q12 =

0 0
0 
2
12

.
In [Q12]2,2, the MLE optimal parameter estimated using empirical
data is 12 = 2.3⇥104
. There is a single observation model for both
regimes defined following
yt =
[1, 0] z}|{ C xt +
V ⇠N(0,0.5
2
) z}|{ vt
. The MLE for terms on the main diagonal are zii = 0.997 so that
the complete transition matrix is defi ne d as
Z =

0.997 0.003
0.003 0.997 
. Figure 12.21 presents the outcome from the switching Kalman filter:
(a) prese nts the probability of the local trend regime, which marks
the tran sition between the zero-speed and constant-speed regimes;
(b) presents the level describing the filtered system responses where
the observation errors were removed; and (c) presents the rate of
change of the system responses, that is, the trend .
17-01 17-05 17-08 17-12
0
0.5
1
Time [YY-MM]
Pr( = S M2) (a) Probability of the local trend regime
17-01 17-05 17-08 17-12
10
11
12
13
Time [YY-MM] x
L
[Temperature]
µt|t ±  t|t µt|t
(b) Level
17-01 17-05 17-08 17-12
0.00
0.01
0.02
0.03
Time [YY-MM] x
LT
[Temperature]
(c) Trend
Figure 12.21: Example of hidden-states estimation using the switching Kalman
filter.
12.3 Linear Model Structures
This sect ion presents how to build linear models from ge ne ric
components. This approach is referred to as Bayesian dynamic
linear models by Goulet and Nguyen,16 and more commonly as
16Nguyen, L. H. and J.-A. Goulet (2018). Anomaly detection with the switching
Kalman filter for structural health mon￾itoring. Structural Control and Health
Monitoring 25(4), e2136; and Goulet, J.-A. (2017). Bayesian dynamic linear models for structural health monitoring. Structural
Control and Health Monitoring 24(12), 1545–2263
Bayesian forecasting or dynamic linear models in the literature
17. 17Harrison, P. J. and C. F. Stevens (1976). Bayesian forecasting. Journal of the Royal
Statistical Society: Series B (Method- ological) 38(3), 205–228; West, M. and
J. Harrison (1999). Bayesian forecasting and dynamic models. Springer; and West, M. (2013). Bayesian dynamic modelling. In Bayesian theory and applications, 145–166. Oxford University Press
Note that the term linear does not mean that these models can
only be employed to describe linear relationships with respect to
time; it refers to the linearity of the observation and transition
equations. Five main components are reviewed here: local level, local trend,
local accele rat ion, periodic, and autoregressive. As illustrated
in figure 12.22, these components can be seen as the buildingprobabilistic machine learning for civil engineers 199
Figure 12.22: Generic components that can be seen as the building blocks, which, once assembled, allow modeling complex
empirical responses. From left to right:
local level, local trend, local acceleration, b periodic, and autoregressive. locks that, once assembled, allow modeling complex time series. Each component has its own specific mathematical formulation
and is intended to model a specific behavior. We will first see the
mathematical formulation associated with each component; then
we present how to assemble components to form {A, C , Q, R}
matrices.
12.3.1 Generic Components
Local level (x
LL)
Local level A local level describes a quantity that is locally con- stant over a time step . It is described by a single hidden variable
x
LL
. Because of its locally constant nature, its transition model
predicts that the hidden state at t is equal to the one at t  1, so the
transition coefficient is 1. The standard de viat ion describing the pre￾diction error for the locally constant model is 
LL
W . Local levels are
part of the hidden variables that contribute to obse rvations, so the
coefficient in the observation matrix equals 1. The hidden-state vari- ables as well at the formulation for the tran sition and observation
model matrices are
x
LL = x
LL
,ALL = 1, C
LL = 1, QLL = (
LL)2
. Figure 12.23 presents realizations of a stochastic process defined by
the tran sition model of a local level. We can see that the process
variability inc rease s as the standard deviation 
LL
increases.
0 5 10 15 20 25 15
0
15
Time - t
x
LL
t (a) 
LL = 0.01
0 5 10 15 20 25 15
0
15
Time - t
x
LL
t (b) 
LL = 0.1
0 5 10 15 20 25 15
0
15
Time - t
x
LL
t
(c) 
LL= 1
Figure 12.23: Examples of local level components. The initial state x
LL0 = 0 is the
same for all realizations.
Level (x
L)
Local trend (x
LT)
Local trend A local trend describes a quantity that has a locally
constant rate of change over a time step. It acts jointly with a
hidden variable describing the level. It is thus defi ne d by two hid- den variables [x
L x
LT
]|. Because of its locally constant nature, its
transition model pre dic ts that the hidden state at t is equal to
the current one, x
LT
t = x
LT
t1
, so the transition coefficient is 1. Thej.-a. goulet 200
hidden stat e x
L
t = x
L
t1 + x
LT
t1t is described by the current level
value plus the change brought by the local trend, multiplied by the
time ste p length t. As we have seen in §12.1.2, the definition of
the process-noise covariance matrix QLT requires considering the
uncertainty that is caused by a constant acceleration with a random
magnitude that can occur over the span of a time step. Analytically,
it corresponds to
QLT = (
LT)2 " t44
t3 2
t32 t2
#
, where 
LT
is the process noise parameter. In most cases involv￾ing a local trend, only the hidden variable representing the level
contributes to observations, so the coefficient in the observation
matrix for x
LT equals 0. The hidden-state estimation as well as the
formulation for the transition and observation model matrices are
x
LT =
 x
L
x
LT 
, ALT =

1 t
0 1

, C
LT =

1
0
|
. Figure 12.24 presents realizations of a stochastic process defined by
the tran sition model of a local trend. We can see that the hidden￾state variable x
LT
is locally constant and that x
L has a locally con￾stant speed. A local trend is employed to model a system response
that changes over time with a locally constant speed.
0 5 10 15 20 25 30
0
Time - t
x
L
t 0 5 10 15 20 25 3
1
1
Time - t
x
LT
t (a) 
LT = 0.001
0 5 10 15 20 25 30
0
Time - t
x
L
t 0 5 10 15 20 25 3
1
1
Time - t
x
LT
t (b) 
LT = 0.01
0 5 10 15 20 25 30
0
Time - t
x
L
t 0 5 10 15 20 25 3
1
1
Time - t
x
LT
t (c) 
LT = 0.1
Figure 12.24: Examples of local trend
components. The initial state [x
L0 x
LT0
] =
[0  1]| is the same for all realizations.
Level (x
L)
Trend (x
T)
Local acceleration (x
LA)
Local acceleration A local acceleration describes a quantity that
has a locally constant acceleration over a time step. It acts jointly
with a level hidden variable and a trend hidden variable. It is thus
defined by three hidden-state variables [x
L x
T x
LA
]|
. Because of
its locally constant nature, its transition model predicts that the
hidden stat e at t is equal to the current one, x
LA
t = x
LA
t1
, so the
transition coefficient is 1. The hidden state x
T
t = x
T
t1 + x
LA
t1t isprobabilistic machine learning for civil engineers 201
described by the current value plus the change brought by the local
acceleration multiplied by the time step length t. The hidden
state x
L
t = x
L
t1 + x
T
t1t +x
LA
t1
t2 2
. The hidden-state variables
as well as the formulation for the transition and observation model
matrices are
x
LA =
2
4
x
L
x
T
x
LA
3
5, ALA =
2
4
1 t t2 2
0 1 t
0 0 1
3
5 , C
LA =
2
4
1
0
0
3
5
|
,
Q
LA = (
LA)2 2
6
4
t4 4
t3 2
t22
t3 2 t2 t
t2 2 t 1
3
7
5 .
Figure 12.25 presents realizations of a stochastic process defined by
the transition model of a local acceleration. We can see that the x
LA
hidden variable is locally constant, x
T has a locally constant speed,
and x
L has a locally constant acceleration. A local acceleration is
employed to mode l a system response that changes over time with a
locally constant acceleration.
0 5 10 15 20 25 30 20 10
0
Time - t
x
L
t 0 5 10 15 20 25 3
1
1
Time - t
x
T
t 0 5 10 15 20 25 0.2 0.1
0
0.1
Time - t
x
LA
t (a) 
LA = 0.0001
0 5 10 15 20 25 30 20 10
0
Time - t
x
L
t 0 5 10 15 20 25 3
1
1
Time - t
x
T
t 0 5 10 15 20 25 0.2 0.1
0
0.1
Time - t
x
LA
t (b) 
LA = 0.001
0 5 10 15 20 25 30 20 10
0
Time - t
x
L
t 0 5 10 15 20 25 3
1
1
Time - t
x
T
t 0 5 10 15 20 25 0.2 0.1
0
0.1
Time - t
x
LA
t (c) 
LA = 0.01
Figure 12.25: Examples of local accel- eration components. The initial state
[x
L0 x
T0 x
LA0
] = [0  0.5  0.5]| is the same
for all realizations.
Periodic (x
S1
, x
S2 )
Periodic Periodic components are expressed in their Four- rier form where they can be described by two hidden variables
x
S = [x
S1 x
S2
]|
. Here, only the first hidden variable contributes
to observations, so the coefficients in the ob se rvation matrix are
1 for x
S1 and 0 for x
S2
. The hidden-state variables as well as the
formulation for the transition and observation model matrices are
x
S =

x
S1
x
S2

, AS =

cos! sin! sin ! cos!

, C
S =

1
0

|
,j.-a. goulet 202
Q
S = (
S)2 
1 0
0 1

, where the frequency ! =
2⇡·t
p
is defined as a function of the period
p and the time step lengt h. Figure 12.26 presents realiz at ions of the
hidden variables describing the periodic process. A periodic compo￾nent is e mployed to model sine -like periodic phenomena with known
period p. For example, in the context of civil engineering, it can
be employed to model the e↵ect of daily and seasonal temperature
changes.
0 5 10 15 20 25 3
0
3
Time - t
x
S1
t 0 5 10 15 20 25 3
0
3
Time - t
x
S2
t (a) p = 10, 
S = 0.01
0 5 10 15 20 25 3
0
3
Time - t
x
S1
t 0 5 10 15 20 25 3
0
3
Time - t
x
S2
t (b) p = 10, 
S = 0.001
0 5 10 15 20 25 3
0
3
Time - t
x
S1
t 0 5 10 15 20 25 3
0
3
Time - t
x
S2
t (c) p = 10, 
S = 0.0001
Figure 12.26: Examples of periodic compo- nents. The initial state [x
S10 x
S20
] = [0 1] | and the period p = 10 are the same for all realizations.
Autoregressive (x
AR)
Autoregressive The aut oregr essive (AR) component is described
by a single hidden variable x
AR that pre sents a dependence on itself
over consecutive time steps. Here, we only present the autoregres- sive process of order 1, where the current state only depends on the
previous one,
x
AR
t = 
ARx
AR
t1 + w
AR
, w
AR
: WAR ⇠ N(w
AR
; 0,(
AR)2
). 
AR
is the autoregression coefficient defining the transition matrix. The autoregr essive hidden-state variable contributes to the obser- vation, so the coefficient in the observation matrix equals 1. The
state variable as well as the formulation for the transition and
observation model matrices are
x
AR = x
AR
, AAR = 
AR
, C
AR = 1, QAR = (
AR )2
. Figure 12.27 presents examples of realizations of an autoregressive
process for di↵erent parameters. Like for other components, the
variability increases with the process-error standard deviation 
AR
. A key property of the autoregressive process is that for 0 < 
AR < 1,
it leads to a stationary process for which the standard deviation is
(
AR,0
)2 =
(
AR)2
1  (
AR)2
.probabilistic machine learning for civil engineers 203
0 5 10 15 20 25 5
0
5
Time - t
x
AR
t (a) 
AR = 0.9, 
AR = 0.25
0 5 10 15 20 25 5
0
5
Time - t
x
AR
t (b) 
AR = 0.99, 
AR = 0.25
0 5 10 15 20 25 5
0
5
Time - t
x
AR
t (c) 
AR = 1.1, 
AR = 0.25
0 5 10 15 20 25 5
0
5
Time - t
x
AR
t (d) 
AR = 0.9, 
AR = 0.5
0 5 10 15 20 25 5
0
5
Time - t
x
AR
t (e) 
AR = 0.99, 
AR = 0.5
0 5 10 15 20 25 5
0
5
Time - t
x
AR
t (f) 
AR = 1.1, 
AR = 0.5
Figure 12.27: Examples of autoregressive
components. The shaded region corre- sponds to the ±
AR,0 confidence region for
the process’s stationary parameters. Note
that cases (c) and (f) are nonstationary
because 
AR > 1. The initial state x
AR0 = 0
is the same for all realizations.
The confidence region corresponding to ±
AR,0
is represented by the
shaded area. Notice that for 
AR  1 the process is nonstationary, which explains why examples (c) and (f) do not display the ±
AR,0
region. The special case where 
AR = 1 corresponds to a random
walk, which is equivalent to the local level component because
xt = xt1 + wt
. The autoregr essive (AR) component plays a key role in the
model. It is there to describe the model errors that have inherent
dependencies over time steps; the definition of a transition model
involves some approximation errors, and because the tran sition
model is employed to make predictions over successive time steps, the pred ict ion errors are correlated across time. Moreover, note
that this correlation tends to one as the time-step length tends to
zero. If we do not employ an AR process to describe these time￾dependent model errors, our ability to correctly estimate other
hidden-state variables can be diminished.
12.3.2 Component Assembly
A time series of observations y1:T is modeled by selecting the rel- evant components and then assembling them to form the global
matrices {A, C , Q, R}. The global hidden-state variable vector x is
obtained by concatenating in a column vector the state variables
defining e ach selected component. The observation matrix C is
obtained by concatenating in a row vector the spe cific C vectors
defining e ach selected component. The transition matrix A and
the process noise covariance Q are obtained by forming block di- agonal matrices (blkdiag(·, ·)). A and Q each employs the specificj.-a. goulet 204
terms defining the selected components. For a single time series, the
observation error covariance matrix R contains only one term, 
2
V
. Example: Climate change This example studies daily average tem￾perature data recorded at Montreal’s YUL airport for the period
1953–2013, as presented in figure 12.28. The goal is to quantify the
current yearly rate of change in the average temperature. Because
53-01 68-04 83-07 98-10 13-12 40 20
0
20
40
Time [YY-MM]
yt [C]
Figure 12.28: Montreal YUL airport daily
average temperature observations for the period 1953–2013.
data is recorded daily, the time-step length is t = 1 day. Figure
12.28 shows a yearly seasonal pattern with frequency ! = 2⇡
365.2422
. Note: The length of a solar year is 365.2422 days. In addition to the periodic component, this model requires a local
trend to quantify the rate of change and an autoregressive one
to capture the time-correlated model prediction errors. For these
components, the hidden-state variables are
xt = [ x
L |{z} x
LT |{z} x
S1 x
S2 | {z } x
AR |{z}
]|
.
The specific formulation for each component is described below:
Local trend component - LT
ALT =

1 1
0 1

QLT = (
LT)2
· 
1/4 1/2
1/2 1

CLT =
⇥ 1 0
⇤
Periodic component - S
AS =

cos! sin! sin ! cos!

, ! = 2⇡
365.2422
QS = 0 (2 ⇥ 2 matrix)
CS =
⇥ 1 0
⇤
Autoregressive component - AR
AAR = [
AR
]
QAR = [(
AR)2
]
CAR = [1]probabilistic machine learning for civil engineers 205
The global matrices describing the model are obtained by assem￾bling the components so that
A = blkdiag(ALT
,AS
,AAR)
Q = blkdiag(QLT
,QS
,QAR)
C = [CLT CS CAR
]
R = 
2
V
. The prior knowledge defining the initial stat e s is defined as
E[X0] = [5 0 0 0 0]|
cov[X0] = diag([100 106 100 100 100]). The unknown parameters and their MLE optimal values learne d
from data using the Newton-Raphson algorithm are
✓
⇤ = [
AR
, 
LT
, 
AR
, V ]| = [0.69 2.2 ⇥ 106 3.4 2.1 ⇥ 104
]|
.
53-01 68-04 83-07 98-10 13-12
6
7
8
Time [YY-MM]
x
L
[C]
µt|T ± t|T µt|T
(a) Level component
53-01 68-04 83-07 98-10 13-12 2
0
2
·104
Time [YY-MM] x
LT
[C/day] (b) Local trend component
53-01 68-04 83-07 98-10 13-12 10
0
10
20
Time [YY-MM]
x
S1
[C]
(c) Periodic (amplitude)
53-01 68-04 83-07 98-10 13-12 10 5
0
5
10
Time [YY-MM]
x
AR
[C] (d) Autoregressive component
Figure 12.29: Hidden-state estimates for
the temperature time series.
Figure 12.29 presents the smoothed estimates for hidden-state
variables xt
. Notice in figure 12.29a how the long-term average
temperature drops from the 1950s to the 1970s and then increases
steadily. Figure 12.29b presents the rate of change in
C/day. When
reported on an annual basis, the annual rate of change in 2013 is
estimated to be µT|T = 0 .06 C/year with a standard deviation
T|T = 0.05 C/year.
12.3.3 Modeling Dependencies Between Observations
State-space models can be employed to model the depend en cies
between several observations. Take, for example, the seasonal
variations in the air temperature a↵ecting the displacement of a
structure. If we observe both quantities, that is, the temperature
and the displacement, we can perform their joint estimation.
In the general case where there are D observed time series, we
have to select relevant components for each time series and then
assemble them together. The global state vector xt for all time
series and all compone nts is obtained by concatenating in a column
vector the state vectors x
j
t
, 8j 2 {1, 2, · · · , D} from each time series. The global observation (C), transition (A), process noise covariance
(Q), and observation covariance (R) matric es are all built in the
same way by concatenating block diagonal matrices from each time
series, C = blkdiag(C1
, C2
, · · · , CD)
A = blkdiag(A1
,A2
, · · · , AD)
Q = blkdiag(Q1
,Q2
, · · · , QD)
R = blkdiag(R1
,R2
, · · · , RD).j.-a. goulet 206
The global matrix C requires special att e ntion because it contains
the informat ion regarding the time-series dependencies. In the C
matrix, each row corresponds to a time series. The com pon ent-wise
representation of the observation matrix is
C =
2
6
6
6
6
6
6
4
C1 Cc1,2
· · · Cc1,j
· · · Cc1,D
Cc2,1 C2
· · · Cc2,j
· · · Cc2,D
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. Cc
i,1 Cc
i,2
· · · Cc
i,j
· · · Cc
i,D
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. CcD,1 CcD,2
· · · CcD,j
· · · CD
3
7
7
7
7
7
7
5
,
where the o↵-diagonal terms Cc
i,j are row vectors with the same
number of eleme nts as the mat rix Cj
. When there is no dependence
between observations, the o↵-diagonal matrices Cc
i,j contain only
zeros. Wh en dependence exists between observations, it can be
encoded in a dependence matrix such as
D =
2
6
6
6
6
6
4
1 d1,2 · · · d1,j
· · · d1,D
d2,1 1 · · · d2,j · · · d2,D
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. di,1 di,2 · · · 1 · · · di,D
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. dD,1 dD,2 · · · dD,j
· · · 1
3
7
7
7
7
7
5
,
where di,j 2 {0, 1} is employed to indicate whether (1) dependence
does exist or (0) does not. When di,j = 0, Cc
i,j = [0]; when
dependence exists an d di,j = 1, the o↵-diagonal matrix Cc
i,j = h
i|j
1 
i|j
2
· · · 
i|j
kj i 2 R
kj contains kj regression coefficients 
i|j
k
describing the line ar dependence between the k
th component from
the j
th
time series and the i
th
time series. All regression coefficients

i|j are treated as unknown parameters to be learne d from data.
13-08 14-04 15-01 15-10
25.5
26.0
26.5
Time [YY-MM]
Displacement y
D
t [m
m]
(a) Displacement [mm]
13-08 14-04 15-01 15-10 30 15
0
14
30
Time [YY-MM]
Temperature y
T
t [C]
(b) Temperature [C]
Figure 12.30: Observed time series for the displacement and temperature of a bridge.
Example: Bridge displacement versus temperature We consider the
joint estimat ion of the hidden-state variables for two time series:
the displacem ent and temperature measured on a bridge from 2013
until 2015. Both time series are presented in figure 12.30, where we
can see a negative correlation between the observed displacement
and temperature seasonal cycles. The temperature is modeled using a local le vel, two periodic
components with respective periods of 365.2422 and 1 day, and an
autoregressive component,
x
T
t = [x
T,LL
t
|{z}
x
T1,S1
t x
T1,S2
t
| {z }
p=365.24
x
T2,S1
t x
T2,S2
t
| {z }
p=1
x
T,AR
t
|{z}
]|
.probabilistic machine learning for civil engineers 207
All matrice s defining the transition and the observation models
are obtained by assembling the specific mathematical formulation
associated with each component,
AT = block diag⇣
1,h
cos !
T1 sin !
T1  sin !
T1 cos !
T1 i
, h
cos !
T2 sin !
T2  sin !
T2 cos !
T2 i
, 
T,AR ⌘
QT = block diag 0, 0, 0, 0, 0,(
T,AR)2
CT = [1 1 0 1 0 1]
RT = (
T
V
)2
, where !
T1 = 2⇡
365.2422 and !
T2 = 2⇡
1
. The displacement is modeled using a local level and an autore￾gressive component,
x
D
t = [x
D,LL
t |{z} x
D,AR
t |{z}
]|
. Matrices defining the transition and the observation models are
again obtained by assembling the specific mathematical formulation
associated with each component,
AD = block diag (1, 
D,AR )
QD = block diag0,(
D,AR)2 
CD = [1 1]
RD = (
D
V
)2
. The global model matrices for the joint data set y1:T =
 y
D
1:T
y
T
1:T

are assembled following
x =
 x
D
x
T 
A = block diag (AD
,AT)
C = block diag (CD
, CT)
R = block diag (RD
,RT)
Q = block diag (QD
,QT). The depend en ce matrix is defined as D =

1 1
0 1

, which indicates
that the displacement observations depend on the hidden states
defined for the temperature. The assembled global observation
matrix is thus
C =
 CD CcD|T
0 CT  =

1 1 0 
D|T1 0 
D|T2 0 
D|T2 0 0 1 1 0 1 0 1

, where the first row corresponds to the displacement and the second
to the temperature. Note that there are two regression coefficients
describing the depe nden ce between the displacement observationsj.-a. goulet 208
and the temperature hidden-state variables: 
D|T1 , the long-term
seasonal e↵ects described by the first periodic component, and 
D|T2 , the short-te rm ones described by the second periodic component
and the autoregressive term. MLE optimal model parameter values
estimated from data are
✓ = [ 
D|T1 
D|T2 
D,AR 
T,AR 
D,AR 
T,AR 
D
V 
T
V
]| = [0.017 0.01 0.982 0.997 0.024 0.28 105 104
] |. The resulting smoothed hidden-state estimations are presented in
figure 12.31. The periodic behavior displayed in figure 12.30a is
not prese nt in any of the displacement components presented in
figures 12.31a–b. This is because the periodic behavior displayed
in the displacement is modeled through the dependence over the
temperature’s hidden states.
13-08 14-04 15-01 15-10
25
26
27
Time [YY-MM]
x
L D, [m
m]
µt|T ±  t|T µt|T
(a) Level: Displacement
13-08 14-04 15-01 15-10 0.3
0.0
0.3
Time [YY-MM]
x
AR,D
[m
m] (b) Autoregressive: Displacement
13-08 14-04 15-01 15-10
5
6
7
8
Time [YY-MM]
x
L T, [C] (c) Level: Temperature
13-08 14-04 15-01 15-10 10
0
10
20
Time [YY-MM]
xS1,T
[C] (d) Per. (p = 365.24 days): Temp.
13-08 14-04 15-01 15-10 1
0
1
2
Time [YY-MM]
x
S1,T
[C] (e) Per. (p = 1 day): Temp.
13-08 14-04 15-01 15-10 5
0
5
10
Time [YY-MM]
x
AR,T
[C] (f) Autoregressive: Temperature Figure 12.31: Hidden-state estimates for
the displacements and temperatures.
12.4 Anomaly Detection
It is possible to detect anomalies in time series using the regime￾switching method presented in §12.2. The idea is to create two
models representing, respectively, a normal and an abnormal regime.
The key strength of this approach is to c onsider not only the like￾lihood of each model at each time step but also the information
about the prior probability of each regime as well as the probability
of switching from one regime to another. In the context of anomaly
detection, it allows reducing the number of false alarms while in- creasing the de tec tion capacity. In a general context, the number of
regimes is not limited, so several models can be tested against each
other. Example: Anomaly detection This example illustrates the concept
of anomaly detection for observations made on a dam displacement. Figure 12.32 presents observations showing a downward trend as
well as a yearly periodic pattern. Because no temperat ure observa￾tions are available for a joint estimation, this case is thus treated as
a single time series. The components employed are a local accelera￾tion, two periodic components with respective periods p = 365.2422
and p = 162.1211 days, and one autoregressive component. In
this case, we want to employ two di↵erent models, where one is
constrained to have a constant speed and the other a constant ac￾celeration. The vector of hidden-state variables for both regimes
is
xt = [ x
L
t
|{z}
x
T
t
|{z}
x
LA
t
|{z}
x
T1,S1
t x
T1,S2
t
| {z }
p=365.24
x
T2,S1
t x
T2,S2
t
| {z }
p=162.12
x
AR
t
|{z}
]|
.probabilistic machine learning for civil engineers 209
02-12 06-03 09-07 12-10 16-02 −25.28
−13.25
−2.78
Time [YY-MM]
Displ, yt [m
m]
Figure 12.32: Example of a dam displace- ment time series that contains an anomaly
that requires using the switching Kalman
filter.
Global model matrices A(j)
, C(j)
,Q(j)
,R(j)
for regimes j 2 {1, 2}
are assembled following the procedure described in §12.3.2. The
only things di↵erentiating the two models are the transition ma￾trices an d the process noise covariance for the local acceleration
component,
A(1) =
2
4
1 t 0
0 1 0
0 0 0
3
5, A(2) =
2
4
1 t t 2 2
0 1 t
0 0 1
3
5 . The model associated with the first regime has the particularity
that the terms in the transition matrix that corresponds to the
acceleration are set equal to zero. The covariance matrice s for the
transition errors from regimes 1 ! 2 and 2 ! 1 are respectively
02-12 06-03 09-07 12-10 16-02
0
1
Time [YY-MM]
Pr(S)
Normal
! Abnormal
Figure 12.33: Probability of regimes 1 and
2 obtained using the SKF algorithm.
Q12 =
2
4
(
LA)2
· t2 20 0 0
0 (
LTT)2
· t 3 3 0
0 0 (
LA)2
· t
3
5
Q21 =
2
4
(
LT)2
· t3 3 0 0
0 (
LTT)2
· t 0
0 0 0
3
5 . The transition matrix is parameterized by z
11 and z
22 as defined in
Z =

z
11 1  z
11
1  z
22 z
22 
. Figure 12.33 presents the probability of each regime as a function
of time; and figure 12.34 presents the hidden-state estimates for
the level, trend, and acceleration. This example displays a switch
between the normal and the abnormal regimes during a short time
span. These results illustrate how anomalies in time series can be
detected using regime switching. They also illustrate how the base￾line respon se of a system can be isolated from the raw observations
contaminated by observation errors and external e↵ects caused by
environmental factors. The model allows estimating that before the
regime switch, the rate of change was -2 mm/year, and it increased
in magnitude to -2.9mm/year after the switch.j.-a. goulet 210
02-12 06-03 09-07 12-10 16-02 −26.33
−13.29
−0.74
Time [YY-MM]
x
L [mm]
(a) Level, x
L
t
02-12 06-03 09-07 12-10 16-02 −120
−3.71
100
·10−3
Time [YY-MM]
xT [mm/12-hr]
-2.0 mm/year -2.9 mm/year
!
(b) Trend, x
T
t
02-12 06-03 09-07 12-10 16-02 −480
0
200
·10−5
Time [YY-MM]
x
LA
[m
m/144-hr
2
]
-0.0054 mm/year2 -0.012 mm/year2
!
(c) Local acceleration, x
LA
t
Figure 12.34: Expected values µt|t and
uncertainty bound µt|t ± t|t for hidden
states resulting from a combination of regimes 1 and 2 using the SKF algorithm.
All the details related to this example can be found in the study
performed by Nguyen and Goulet.18 18Nguyen, L. H. and J.-A. Goulet (2018). Anomaly detection with the switching
Kalman filter for structural health mon￾itoring. Structural Control and Health
Monitoring 25(4), e2136
The package OpenBDLM19 allows using the state-space model’s
19Gaudot, I., L. H. Nguyen, S. Khazaeli, and J.-A. Goulet (2019, May). OpenBDLM, an open-source software for structural
health monitoring using bayesian dynamic
linear models. In 13th Proceedings from
the 13th International Conference on Applications of Statistics and Probability
in Civil Engineering (ICASP)
theory an d includes the formulation for the various components
defined for Bayesian dynamic linear models.13
Model Calibration
Figure 13.1: Example of hard-coded finite- element model for which we want to infer
the parameter values ✓ = [✓1 ✓2]| from sets of observations (xi
, yi ).
In civil engineering, model calibration is employed for the task of
estimating the parameters of hard-coded physics-based models using
empirical observations D = {(xi
, yi) , 8i 2 {1 : D}} = {Dx, Dy}. The term hard-coded physics-based refers to mathematical models
made of constitutive laws and rules that are themselves based on
physics or domain-specific knowledge. In contrast, models presented
in chapters 8, 9, and 12 are referred to as empirical models because
they are built on empirical data with little or no hard-coded rules
associated with the underlying physics behind the phenomena stud￾ied. The hard-coded model of a system is described by a function
g(✓, x), which depends on the covariat e s x = [x1 x2 · · · xX]| 2 RX
as well as its model paramete rs ✓ = [✓1 ✓2 · · · ✓P ]|
. The observed
system responses Dy = {y1, y2, · · · , yD} depend on covariat es
Dx = {x1, x2, · · · , xD}. Figure 13.1 presents an example of a model
where the parameters ✓ describe the boundary conditions and the
initial tension in cables. For this example, the covariates x and x
⇤ describe the locations where predictions are made, as well as the
type of predictions, that is, displacement, rotation, strain, and so
on. The vector x describes covariates for observed locations, and x
⇤
those for the unobserved locations.
Nomenclature Observed system responses Dy = {y1, y2, · · · , yD}
Covariates Dx = {x1, x2, · · · , xD}
Parameter values ✓ = [✓1 ✓2 · · · ✓P]| Model predictions g(✓, x) = [g(✓, x1) · · · g(✓, xD)]| Model calibration is not in itself a subfield of machine learning.
Nevertheless, in this chapter, we explore how the machine learning
methods presented in previous chapters can be employed to address
the challenges assoc iate d with the prob abilistic infe ren ce of model
parameters for hard-coded models. This application is classified
under the umbrella of unsupervised learning because, as we will
see, the main task consists in inferring hidden-state variables and
parameters that are themselves not observed. Problem setups There are several typic al set ups where we want
to employ em pirical ob servations in conjunction with hard-codedj.-a. goulet 214
models. For the example presented in figure 13.1, if we have a prior
knowledge for the joint distribution of model parameters f(✓), we
can propagate this prior knowledge through the model in order
to quantify the unce rtainty associated with predic tion s. This un- certainty propagation can be, for instance, performed using either
Monte Carlo sampling (see §6.5) or first-order linearization (see
§3.4.2). When the uncertainty in the prior knowledge f(✓) is weakly
informative, it may le ad to a large variability in model predict ions. In that situation, it becomes interesting to employ empirical obser- vations to reduce the unce rtainty related to the prior knowledge of
model parameters. The key with model calibration is that model
parameters are typically not directly observable. For instance, in
figure 13.1 it is often not possible to directly measure the bound￾ary condition properties or to measure the cable internal tension.
These properties have to be infe rred from the obse rved structural
responses Dy. Another key aspect is that we typically build physics- based models g(✓, x) because they can predict quantities that
cannot be observed. For example, we may want to learn about
model parameters ✓ using observations of the stat ic re sponse of a
structure defined by the covariates x and then employ the model
to predict the unobserved responses g(✓, x
⇤
), defined by other co￾variates x
⇤
. The vector x
⇤ may describe unobserved lo cat ion s or
quantities such as the dynamic behavior instead of the stat ic one
employed for parameter estimation. One last possible problem setup
is associated with the selection of model classes for describing a
phenomenon.
There are three main challenges associated with model calibra￾tion: observation errors, prediction errors, and model complexity. The first challenge arises because the observations available Dy
are in most cases contaminated by observation errors. The second
is due to the discrepancy between the prediction of hard-coded
physics-based models g(✓, x) and the real system responses; that
is, even whe n paramet e r values are known, the model remains an
approximation of the re ality. The third challenge is related to the
difficulties associated with choosing a model structure having the
right c om ple xity for the task at hand. In §13.1, we explore the
impact of these challenges on the least-squares model calibration
approach, which is still ex ten sively used in practice. Then, the
subsequent sections explore how to address some of these chal￾lenges using a hierarchical Bayesian approach combining concepts
presented in chapters 6 and 8.probabilistic machine learning for civil engineers 215
13.1 Least-Squares Model Calibration
!
What we will see in §13.1 is an example of what not to do when calibrating a hard- coded physics-based model as well as why
one should avoid it.
This section presents the pitfalls and limitations of the common
least-squares deterministic model calibration. With least-squares
model calibration, the goal is to find the model parameters ✓
that minimize the sum of the square of the di↵erences between
predicted and measured values. The di↵erence between predicted
and observed value s at one observed location xi
is defined as the
residual
r(✓,(xi, yi)) = yi  g(✓, xi). For the entire data set containing D data points, the least-squares
loss function is de fined as
J(✓, D) = X
D
i=1
r(✓,(xi
, yi))2 = kr(✓, D)k
2
. Because of the square exponent in the loss function, its value is
positive J(✓, D)  0. The task of identifying the optimal parameters
✓
⇤
is formulated as a minimization problem where the goal is to
minimize the sum of the square of the residual at each observed
location,
✓
⇤ = arg min
✓
J(✓, D)
⌘ arg max
✓  J(✓, D) | {z } Optimization problem
. Gradient-based optimization methods such as those presented in
chapter 5 can be employed to identify ✓
⇤
.
8
>><
>
:
K = 1.75 ⇥ 10
11 N·mm/rad
L = 10 m
P = 5 kN
I = 6.66 ⇥ 10
9 mm4 Eˇ = 35 GPa Figure 13.2: The reference model gˇ(Eˇ, x)
that is employed to generate simulated
observations.
13.1.1 Illustrative Examples
This section presents three examples based on simulated data
using the reference beam model gˇ(Eˇ , x) presented in figure 13.2
along with the values for its paramet e rs. Observations consist in
displacement measurements made for positions x1 = L/2 and x2 =
L. The first example corresponds to the idealized context where
there is no observation or pred ict ion errors because the model
employed to infer paramet er values is the same as the reference
model employed to generat e synthetic data. The second example
employs a simplified model to infer parameters. Finally, the third
example employs an overcomplex model to infer parameters. Note
that the last two examples inc lude obse rvation errors. For all three
cases, there is a single unknown parameter: the elastic modulus E , which characterizes the flexural sti↵ness of the beam.j.-a. goulet 216
Example #1: Exact model without observation errors The first
example illustrates an idealized context where there are no obser- vation or prediction errors because the model employed to infer
parameter values g(E, x) is the same as the one employed to gener- ate synthetic data gˇ(Eˇ, x). Figure 13.3 compares the predicted and
true deflection curves along with observed values. He re , because of
Example #1
Reference model gˇ(Eˇ,x) z }| {
Interpretation model g(E,x) z }| { the absence of model or measurement errors, the optimal parameter
also coincides with its true value ✓
⇤ = E⇤ = Eˇ, and the predicted
deflection obtained using the optimal parameter value ✓
⇤ coincides
with the true deflec tion . Note that the loss function evaluated for ✓
⇤ equals zero. Even if least-squares model calibration worked perfectly
in this example, this good performance cannot be generalized to
real case studies because in this oversimplified example, there are
no model or measurement errors.
0 1 2 3 4 5 6 7 8 9 10 12 10 8 6 4 2
0
Position [m]
Deflection [m
m]
Predicted response True deflection Observations 0 10 20 30 40
0
5
10
15
20
Parameter value [GPa] Loss function, J( ) ✓, D
E
⇤ ˇE
Figure 13.3: Least-squares model cali- bration where the interpretation model
is identical to the reference model, gˇ(Eˇ, x) = g(Eˇ, x), and where there are no measurement errors.
Example #2
Reference model gˇ(Eˇ,x) z }| {
Interpretation model g(E,x) z }| {
Example #2: Simplified model with observation errors The second
example presents a more realistic case, where the model g(E, x)
employed to infer paramet ers contains a simplification in compari- son with the real system described by the reference model gˇ(Eˇ , x). In this case, the simplification consists in omitting the rotational
spring that is responsible for the nonzero initial rotation at the
support. In addition, observations are a↵ect e d by observation errors
vi : V ⇠ N(v; 0, 1) mm, where the observation model is defi ned as
yi = gˇ(Eˇ, xi) + vi, Vi ? Vj , 8i 6= j. Figure 13.4 compares the predicted and true deflection curves
along with observed values. We see that because of the combination
of model simplification and observation errors, we obtain a higher
loss for the correct parameter value Eˇ = 35GPa than for the
optimal value E⇤ = 22 GPa. As a result, the model predictions
are also biased for any prediction location x. This second example
illustrates how deterministic least-squares model calibration may
identify wrong parameter values as well as lead to inaccurate
predictions for measured and unmeasured locations.probabilistic machine learning for civil engineers 217
0 1 2 3 4 5 6 7 8 9 10 12 10 8 6 4 2
0
Position [m]
Deflection [m
m]
Predicted response True deflection Observations 0 10 20 30 40
0
5
10
15
20
Parameter value [GPa] Loss function, J( ) ✓, D
E⇤ ˇE
Figure 13.4: Least-squares model calibra- tion where the interpretation model is
simplified in comparison with the reference model, gˇ(Eˇ, x) 6= g(Eˇ , x), and where there are measurement errors.
Example #3: Overcomplex model with observation errors The
third example illustrates the pitfalls of performing model calibra￾tion using a simplified yet overcomplex interpretation model, that
is, a model that contains simplifications yet also contains too many
parameters for the problem at hand. Here, overcomplexity is caused
by the use of two parameters for describing the elastic modu lus, E1
and E2. Like in the second example, obse rvations are a↵ected by
observation errors vi : V ⇠ N(v; 0, 1) mm.
Example #3
Reference model gˇ(Eˇ,x) z }| {
Interpretation model g(E 1,E2,x) z }| { Figure 13.5 compares the predicted and true deflection curves
along with observed values. Note how the over-parameterization
allowed to wrongly match both observations. Parameter values
identified are biased in comparison with the true value, and the
predictions made by the model are also biased. Because in practice
the true deflection and parameter values remain unknown, the
biggest danger is overfitting where we could believe that the model
is capable of providing accurate predictions because the calibration
wrongly appears to be perfect. In the context of least-squares model
calibration, this issue can be mitigated using cross-validation (see
§8.1.2), where only subsets of observations are employed to train
the model, and then the remaining data is employed to test the
model’s predictive capacity. This procedure would reveal that the
interpretat ion model is poorly defined. In a more general way, we
can also use Bayesian model sele ct ion (see §6.8) for quantifying the
probability of each model class.
0 1 2 3 4 5 6 7 8 9 10 12 10 8 6 4 2
0
Position [m]
Deflection [m
m]
Predicted response True deflection Observations 0 25
0
5
10
15
20
Parameter value [GPa]
Loss function, J , (✓ D)E⇤
1 E
⇤
2 Eˇ Figure 13.5: Least-squares model calibra- tion where the interpretation model is overcomplex in comparison with the ref- erence model, gˇ(Eˇ , x) 6= g(Eˇ1, Eˇ2, x), and where there are measurement errors.j.-a. goulet 218
13.1.2 Limitations of Deterministic Model Calibration
Deterministic model calibration using methods such as least-squares
residual minimization performs well when model and measurement
errors are negligible and when data sets are large; the issue is that
in practical engineering applications, the errors associated with
measurements and especially hard-coded models are not negligible, and data sets are typically small. As illustrated in figure 13.6, for
engineering systems, no matter how much e↵ort is devoted to
building a hard-coded physics-based model, it will always remain
a simplified version of the reality. Therefore, model simplifications
introduce pre dict ion errors that need to be considered explicitly as
hidden variables to be inferred in addition to mo del parameters ✓.
Figure 13.6: In complex engineering
contexts, no matter how good a hard-coded
physics-based model is, it never perfectly
represents the reality. (Photo: Tamar Bridge, adapted from Peter Edwards)
13.2 Hierarchical Bayesian Estimation
This section presents a Bayesian hierarchical method for jointly
estimating model parameters as well as prediction errors. The justi- fication for this approach is that no matter how good a hard-coded
model is, it remains an approximation of the reality. Therefore,
in addition to the model parameters, we need to infer prediction
errors. There is typically an issue of non-identifiability (see §6.3.4)
between model parame ters and pre dict ion errors. As we will see,
the problem is made identifiable using prior knowledge. In addition,
note that because of observation errors, observation s yi are not
exact representations of the reality either, that is, model(✓) =6 reality
model(✓) + errorw = reality =6 observation
model(✓) | {z }
g(✓)
+ error w | {z } w + errorv | {z }
v = reality + errorv | {z }
v = observation
| {z }
y
.
In the previous equation, observations y = [y1 y 2 · · · yD]| are
modeled by the sum of model predictions g(✓, x) 2 RD
, which
are a function of covariat e s x = [x1 x2 · · · xD
]| 2 R
X⇥D and
model parameters ✓ = [✓1 ✓2 · · · ✓P
]| 2 RP
; prediction errors
w(x) = [w(x1) w(x2) · · · w (xD)]| 2 RD
, which are also a function of
covariates x; an d measurement errors v = [v1 v2 · · · vD]| 2 RD
.
13.2.1 Joint Posterior Formulation
The model predictions g(✓, x) depend on the known covariates x
describing the prediction locations and unknown model parame￾ters ✓ describing physical properties of the system. The observed
structural responses are described byprobabilistic machine learning for civil engineers 219
y = g(✓, x) + w(x) + v, (13.1)
Observations Deterministic predictions from a hard-coded model
Model parameters Covariates Measurement errors
Prediction errors
where measurement errors are assumed to be independent of the
covariates x. All the terms in equation 13.1 are considered as
deterministic quantities because, for a given set of observations
D = {Dx, Dy} = {(xi, yi ), 8i 2 {1 : D}}, quantities x, y, ✓, w(x), and v are not varying, yet values for ✓, w(x), and v remain
unknown. The prior knowledge for model parameters is described by the
random variable ⇥ ⇠ f(✓; zp), where z p are the hyperparameters, that is, parameters of the prior probability density function (PDF). When hyperparameters zp are unknown, the random variable Zp
is described by a hyper-prior f (z p) and a hyper-posterior f(zp|D), that is, the prior and posterior PDFs of hyperparameters. Unlike
the parameter values ✓, for which true values may exist, hyperpa￾rameters zp are parameters of our prior knowledge, for which, by
nature, no true value exists. When hyperparameters zp are assumed
to be fixed constants that are not learned from data, they can be
implicitly included in the prior PDF formulation f(✓; zp) = f(✓). The concept of prior, posterior, hyper-prior, and hyper-posterior
not only applies to unknown parameters ✓ but also to prediction
errors w(x) and measurement errors v. Note that, unlike for hyper- parameters zp and zw, a true value can exist for zv. When it exists, the true value for zv can typically be infe rre d from the stat istical
precision of the measuring instruments employed to obtain y. In
this section, in order to simplify the notation and to allow focusing
the attention on model parameters ✓ and prediction errors w(x), we assume that zp and zv are known constants. The formulation for the joint posterior PDF for unknown model
parameters ✓, prediction errors w(x), and prediction errors prior
PDF hyperparameters zw follows:
posterior z }| { f(✓, w(Dx), zw |D) =
likelihood z }| { f(Dy|✓, w(Dx), Dx )·
prior z }| { f(✓) · f (w(Dx)|zw)·
hyper-prior z }| { f(z w)
f(Dy)
. (13.2)
Model parameters Hyperparameters for the prediction errors prior PDF
Prediction errors at location Dx Dataj.-a. goulet 220
Likelihood The formulation for the joint posterior in equation 13.2
allows the explicit quantification of the depen de nc e between model
parameters and prediction errors at observed locations. In the
case where several sets of model parameters and prediction errors
{✓, w(Dx)} can equally explain the observations, then the problem
is non-identifiable (see §6.3.4) and several sets of values will end
up having an equal posterior prob ability, given that the prior
probability of each set is also equal. Here, the likelihood of data Dy
is formulated considering that g(✓, Dx) and w(Dx) are unknown
constants, so the only aleatory quantities are the observation errors
V ⇠ N (v; 0, ⌃v ). The formulation of the likelihood is then
f(Dy|✓, w(Dx), Dx) = f

(
constants z }| { g(✓, Dx ) + w(Dx) +
N(v;0,⌃v) z}|{ V ) = Dy  = N (y = Dy ;
µy z }| { g(✓, Dx) + w(Dx), ⌃y z}|{ ⌃v ) =
exp ✓
1
2
Dy  (g(✓, Dx ) + w(Dx))|⌃
1 v Dy  (g(✓, Dx) + w(Dx))◆
(2⇡)
D/2p
det ⌃v / exp ✓
1
2
Dy  (g(✓, Dx) + w(Dx)|⌃
1 v Dy  (g(✓, Dx ) + w(Dx))◆
= exp 

1
2X
D
i=1
yi  (g(✓, xi) + w(xi))2
2v !
, for Vi ? Vj
, 8i 6= j.
Prior It is necessary to define a mathematical formulation for the
prior PDF of prediction errors, W ⇠ f(w|zw ). A convenient choice
consists in describing the prior PDF for prediction errors using a
Gaussian process analogous to the one described in §8.2. With the
assumption that our model is unbiased, that is, the prior expected
value for prediction errors is 0, this Gaussian process is expressed
as
W ⇠ N (w; 0, ⌃w ), where
[⌃w ]ij = w(xi) ·  w(xj) · ⇢(x i
, xj
, `), and
w (x) = fct(x, zs), ⇢(xi, xj, `) = exp
 1
2
(xi  xj )|⌃
1
` (xi  xj )
, z w = [zs `]|
. The prediction error standard deviation w (x) is typically covariate
dependent, so that it needs to be represented by a problem-specific
function fct(x, zs), where zs are its parameters. The correlation
structure can be represented by a square-exponential covariance
function, where ⌃` is a diagonal matrix, parameterized by the
length-scale parameters ` = [`1 `2 · · · `X
]|. `k is the hyperpa￾rameter describing how the correlation decreases as the distanceprobabilistic machine learning for civil engineers 221
(xk,i  xk,j) 2
increases. Note that this choice for the correlation
structure is not exclusive and others can be employed as described
in §8.2. The hyperparameters regrouped in z w need to be learned
from data.
Example #4
Reference model gˇ(Eˇ,x) z }| {
Interpretation model g(E,x) z }| {
Dx = {5, 10} m
Dy = {4.44, 11.28} mm
0 2 4 6 8 10 −12 −10 −8 −6 −4 −2
0
position [m]
Deflection [mm]
True deflection Predicted
(a) Comparison of the real and pre- dicted deflection obtained using the
true parameter value Eˇ.
0 2 4 6 8 10 −12 −10 −8 −6 −4 −2
0
position [m]
Deflection [m
m] (b) The prior knowledge for pre- diction errors is described using a Gaussian process. Figure 13.7: Graphical representation
of the prior knowledge for prediction
errors arising from the use of a simplified
interpretation model.
Example #4: Joint posterior estimation We are revisiting example
#2, where the model employed to infer parameters g(E, x) contains
simplifications in comparison with the reference model gˇ(Eˇ, x)
describing the real system studied. In this case, the simplification
consists in omitting the rotational spring allowing for a nonzero
initial rotation at the support. In addition, observations are a↵ected
by independents and identically distributed obse rvation errors
vi : V ⇠ N (v; 0, 1) mm. Figure 13.7a presents the true deflection obtained from the
reference model as well as from the interpretation model using the
true parameter value Eˇ = 35GPa. We can see that for any lo c ation
other than x = 0, there is a systematic prediction error. The prior
knowledge for these the ore tically unknown prediction errors is
modeled using a zero-mean Gaussian process, f(w) = N (w ; 0, ⌃w), where each term of the covariance matrix is defi ne d as
[⌃w ]ij =
i z }| {
(a · xi)·
j z }| {
(a · xj )·
⇢ij z }| {
exp ✓
1
2
(xi  xj)2
`
2 ◆
. (13.3)
The prior knowledge for prediction errors is parameterized by zw =
[a `]|
. Figure 13.7b presents the marginal ± w confidence interval
around the predicted deflection. The prior knowledge for each pa￾rameter and hyperparameter is elastic modulus N (E; 20, 202
)GPa,
model error prior scale factor N (a; 104
, (5⇥ 104
)2
), and length￾scale N (`; 5, 502
) m. Note that all of those priors are assume d to be
independent from each other, so the joint prior can be described by
the product of the marginals. The analytic formulation of the poste￾rior PDF for the unknown model parameter ✓ = E, the prediction
errors w(5), w(10), and the hyperparameters a, ` is known up to a
constant, so that
f(E, w(5), w(10), a, `|Dy ) / N (y1; g(E, 5) + w(5) , 1
2
) · · ·
·N (y2 ; g(E, 10) + w(10) , 1
2
) · · ·
·N ([w(5) w(10)]|
; 0, ⌃w) · · ·
·N (E; 20, 202
) · · ·
·N (a; 104
,(5 ⇥ 104
)2
) · · ·
·N (`; 5, 502
).j.-a. goulet 222
Using the Metropolis sampling method presented in §7.1, three par- allel chains, containing a total of S = 105
joint samp les, are taken
from the unnormalized posterior f (E, w(5), w(10), a, `|Dy ), where
each joint sample is described by {E, w(5), w(10), a, `}s . The first
half of each chain are discarded as burn-in samples; the Metropolis
acceptance rate is approximately 0.25, and the estimated potential
scale reduction (EPSR, i.e., chain stationarity metric; see §7.3.3)
is below 1.1 for all parameters and hyperparameters. The M arkov
chain Monte Carlo (MCMC) poste rior samp le s are pre sented in
figure 13.8, where, when it exists, the true value is represented by
the symbol ⇤. Note the almost perfect correlation in the posterior
between w1(5) and w2(10). It happens because, as illustrated in
figure 13.7, whenever the model either under- or overestimates the
displacement, it does for both locations. Notice that there is also
a clear dependence between prediction errors wi and the elastic
modulus E, where small (large) values for E are compensated by
large positive (negative) prediction errors. These are examples of
non-identifiability regularized by prior knowledge, as described in
§6.3.4.
20 40 60 80
E [GPa]
20 40 60 80
0.5
1
1.5
·103
a [m
m]
0 0.5 1 1.5
·10 a 3 [mm]
20 40 60 80
50
100
150
l [m]
0 0.5 1 1.5
·103
50
100
150
0 50 100150
l [m]
20 40 60 80 4 2
0
2
4
w1 [m
m]
0 0.5 1 1.5
·103 4 2
0
2
4
0 50 100150 4 2
0
2
4
42 0 2 4 w1 [mm]
20 40 60 80 5
0
5
w2 [m
m]
0 0.5 1 1.5
·103
5
0
5
0 50 100150 5
0
5
42 0 2 4 5
0
5
5 0 5 w2[mm]
Figure 13.8: Bivariate scatter plots and
histograms describing the marginal poste- rior PDFs. The symbol ⇤ indicates the true values.probabilistic machine learning for civil engineers 223
13.2.2 Predicting at Unobserved Locations
In practical applications, we are typically interested in predicting
the structure’s responses u at unobserved locations x⇤. Analogous
to equation 13.1, the predicted values at unobserved locations are
defined as
u = g(E, x⇤) + w(x⇤). When predicting at unobserved locations, we want to consider
the knowledge obtained at observed locations, as described by the
posterior samples displayed in figure 13.8. When using MCMC
samples (see §7.5), we can app roximate the posterior predictive
expected values and covariances by following
E[U|D] =
Z u · f (u|D)du
⇡
1
SX
S
s=1
us (Posterior predictive mean)
cov(U|D) = E[(U  E[U|D])(U  E[U|D])|
]
⇡
1
S  1 X
S
s=1
(us  E[U|D])(us  E[U|D])|
. (Posterior predictive covariance)
In order to compute E[U|D ] and cov (U|D), we have to gener- ate realizations of us from f (u|D) using the MCMC samples
{E, w(5), w(10), a, `}s obtained from the unnormalized posterior
f(E, w (5), w(10), a, `|Dy). Through that process, we must first
generate realizations for the covariance matrices {⌃s⇤
, ⌃sw⇤, ⌃sw}
describing the model errors prior PDF. These realizations are then
used to update the posterior mean vector and covariance mat rix
describing the model errors at prediction lo c at ion s, so that
8
>><
>:
ws(5)
ws(10)
as `s
9
>>=
>;
!
8
<
:
⌃s⇤ ⌃sw⇤ ⌃sw
9
=
;
!
(
µ
s⇤|w = ⌃
s|w⇤(⌃sw)1
[ws(5) ws(10)]|
⌃s⇤|w = ⌃s⇤  ⌃
s|w⇤ (⌃sw)1⌃sw⇤. Realizations of the model errors at prediction locations ws⇤ can be
Note: From equation 13.3,
[⌃ s⇤]ij =
i z }| {
(as ·x
⇤
i )· j z }| {
(as ·x
⇤
j )·
⇢ij z }| { exp✓
9
1
2
(x
⇤
ix
⇤
j
)2
`2
s ◆
[⌃sw⇤]ij =(as ·xi)·(as ·x
⇤
j )·exp✓
9
1
2
(xix
⇤
j
)2
`2
s ◆
[⌃sw ]ij =(as ·xi)·(as ·xj )·exp✓
9
1
2
(xixj
)2
`2
s ◆
sampled from the multivariate Gaussian PDF defined by
ws⇤ : W⇤ ⇠ N(w⇤; µ
s⇤|w, ⌃s⇤|w). Finally, we can obtain a realization of the structure’s behavior at
predicted locations by summing the realization of model errors
ws⇤ with the model predictions evaluated for the model parameter
MCMC sample Es, us = g(Es, x⇤) + ws⇤.j.-a. goulet 224
Figure 13.9a presents a comparison of the posterior predictive
model prediction samples us with the deflection of the reference
model. Figure 13.9b presents the same comparison, this time using
the posterior predictive mean vector and con fide nc e interval. Notice
how the confidence interval and the smoothness of the realiz ations
match the true deflection profile . 0 2 4 6 8 10 −20 −15 −10 −5
0
5
position [m]
deflection [m
m]
(a)
0 2 4 6 8 10 −20 −15 −10 −5
0
5
position [m]
deflection [mm]
True deflection Observations
(b)
Figure 13.9: Comparison of the posterior predictive model predictions with the displacement of the reference model.
The scatter plots in figure 13.8 show that significant uncer- tainty remains regarding the param et e r E and prediction errors
w(5), w(10). However, because there is a strong dependenc e in the
joint posterior PDF describing these variables, the posterior un- certainty for the defle ct ion remains limited . It hap pens because
abnormally small (large) values for the elastic modulus Es result
in large (small) negative displacements, which are compensated for
by large positive (negative) predict ion errors ws (5), ws(10). This
example illustrates why, when calibrating model parameters, it is
also necessary to infer model prediction errors because parameters
and prediction errors are typically dependent on each other. The most difficult part of such a hierarchical Bayesian estimation
procedure is to define an appropriate function for describing the
prior knowledge of model prediction errors. This aspect is what
is limiting its practical application to complex case studies where
the prediction errors depend not only on the prediction locations
but also on the prediction types, for example, rotation, strain, acceleration, and so on. When several formulations for the prior
knowledge of predic tion errors are available, the probability of each
of them can be estimated using Bayesian model selection (see §6.8)
or cross-validation (se e §8.1.2). Furthe r de tails about hierarchical
Bayesian estimation applied to model calibration can be found in
Kennedy and O’Hagan;1 Brynjarsd´ottir and O’Hagan.2 1 Kennedy, M. C. and A. O’Hagan (2001). Bayesian calibration of computer models. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 63(3), 425–464
2 Brynjarsd´ottir, J. and A. O’Hagan (2014). Learning about physical parameters: The
importance of model discrepancy. Inverse Problems 30 (11), 11400714
Decisions in Uncertain Contexts
This chapter explores how to make rational decisions regarding
actions to be taken in a context where the state variables on which
a dec ision is based are themselves uncertain. Basic contextual
notions will be presented through an illustrative example before
introducing the formal mathematical notation associated with
rational decisions and value of information.
14.1 Introductory Example
(a) Given an m3 of possibly
contaminated soil. Pr = 0.9
Pr = 0.1
$0
9$10
9$10K
9$100
E[$| ]=9$1K
E[$| ]=9$100
(b) State probabilities, values, and
expected values. Figure 14.1: Decision context for a soil contamination example.
In orde r to introdu ce the notions associat ed with utility theory, we re visit the soil contamination example presented in §8.2.5,
where we have a cubic meter of potentially contaminated soil. The
two possible states x 2 { , } are e ithe r contaminated or not
contaminated, and the two possible actions a 2 { , } are e ithe r to
do nothing or to send the soil to a recycling plant where it is going
to be decontaminated. A soil sample is defined as contaminated
if the pollutant concentration c exceeds a threshold value c >
adm.. The issue is that for most m3
in an industrial site, the re
are no observations made in order to verify whether or not it is
contaminated. Here, we rely on a regre ssion model built from a set
of discret e observations to predict for any x, y, z coordinates what
our knowledge of the contaminant conc entration is as described
by its expected value and variance. In this example, our current
knowledge indicates that, for a specific m3
, Pr(C  adm.) ⌘ Pr(X = ) = 0.9
Pr(C > adm.) ⌘ Pr(X = ) = 0.1. The optimal decision regarding the action to choose depends
on the value incurred by taking an action given the state x. In
this example we have two states and two actions, so there are four
possibilities to be defined, as illustrated in figure 14.1. The incurredj.-a. goulet 230
value ($) for each pair of stat e s x and actions taken a are
$(a, x) = $ 
, ,
, ,
 ⌘

$0 $10K
$100 $100 
, where doing nothing when the soil is not contaminated incurs no
cost, decontaminating incurs a cost of $100/m3 whether or not the
soil is contaminated, and omitting the decontamination when it is
actually contaminated (i.e., c > adm.) incu rs a cost of $10 K in
future legal fees and compensatory damages.
Actions Do nothing
Recycle States Not contaminated
Contaminated
:$100
Pr=0.1
:$100
Pr=0.9
:$10K
Pr=0.1
:$0
Pr=0.9
E[$| ] = $100
E[$| ] = ⇠⇠⇠⇠ $1 000
Figure 14.2: Decision tree for the soil contamination example.
Figure 14.2 depicts a decision tree illustrating the actions along
with the probability of each out c ome. The optimal action to be
taken must be sele ct e d based on the expect ed value conditional on
each action,
E[$| ] = $( , ) ⇥ Pr(X = ) + $( , ) ⇥ Pr(X = ) = ($0 ⇥ 0.9) + ($10K ⇥ 0.1) = $1 000
E[$| ] = $( , ) ⇥ Pr(X = ) + $( , ) ⇥ Pr(X = ) = ($100 ⇥ 0.9) + ($100 ⇥ 0.1) = $100 . Because E[$| ] > E[$| ], the optimal action is thus to send the
soil to a recycling plant where it will be decontaminated. As you
can e xpect , changing the probability of each state or changing the
incurred relative value ($) for a pair of states x and actions taken
a will influence the optimal action to be taken. The next section
presents the utility theory, which formalizes the method employed
in this introductory ex amp le .
14.2 Utility Theory
Utility theory defines how rational decisions are made. This section
presents the nomenclature associated with utility theory, it formal￾izes what a ration al decision is, and it presents its fundamental
axioms.
14.2.1 Nomenclature
A = {a1, · · · , aA} A set of possible actions x 2 X ✓ Z or ✓ R
An outcome from
a set of possible
states Pr(X = x) ⌘ Pr(x) Probability of a
state x U(a, x)
Utility given a
state x and an
action a
A decision consists in the task of cho osing an action ai from a set of
possible actions A = {a1 , a2, · · · , aA}. This decision is based on our
knowledge of on e or several state variables x, which can either be
discrete or continuous. Our knowledge of a state variable is eithe r
described by its probability density X ⇠ f(x) or mass function
X ⇠ p(x) = Pr(X = x). The utility U(a, x) quantifies the relat iveprobabilistic machine learning for civil engineers 231
preferences we have for the joint result of taking an action a while
being in a state x. Soil contamination ex ample For the example presented in the in- troduction of this chapter, the actions and stat e s can be formulated
as binary disc rete variables; a 2 { , } ⌘ {0, 1} and x 2 { , } ⌘
{0, 1}. The probability of each state is Pr(X = x) = {0.9, 0.1}, an d
the utility of each pair of act ions, and states, is
U(a, x) = U

, ,
, ,
 ⌘

$0 $10K
$100 $100 
.
14.2.2 Rational Decisions
In the context of the utility theory, a rational decision is defined
as a decision that maximiz es the expected utility. In an uncertain
context, the perceived benefit of an action ai
is measured by the
expected utility, U(a) ⌘ E[U(a, X)]. When X is a disc re te random
variable so that x 2 X = {1, 2, · · · , X}, the expected utility is
E[U(a, X)] = X
x2X
U(a, x) · p(x).
Instead, when X is a continuous rand om variable, the expect e d
utility is
E[U(a, X)] =
Z U(a, x) · f(x)dx. The optimal action a
⇤
is the on e that maximizes the expected
utility, a
⇤ = arg max
a2A
E[U(a, X)], U(a
⇤
) ⌘ E[U(a
⇤
, X)] = max
a2A
E[U(a, X)].
14.2.3 Axioms of Utility Theory
The axioms of utility the ory are based on the concepts of lotteries. A lot tery L is defined by a set of possible outcomes x 2 X =
{1, 2, · · · , X}, each having its own probability of occurre nc e pX(x) =
Pr(X = x), so that
L = [{pX(1), x=1}; {p X(2), x=2}; · · · ; {pX (X), x=X}]. (14.1)
A dec ision is a choice made between several lott erie s. For the soil
contamination example, there are two lotteries, each one corre￾sponding to a possible action. If we choose to send the soil to a
recycling facility, we are certain of the outcome, that is, the soil isj.-a. goulet 232
not contaminated after its treatment. If we choose to do nothing,
there are two possible outcome s; either no significant contaminant
was present with a probability of 0.9, or the soil was wrongly left
without treatment with a probability of 0 .1. These two lotteries can
be summarized as
L = [{1.0,( , )}; {0.0,( , )}]
L = [{0.9,( , )}; {0.1,( , )}]. The nomenclature employed for ordering preferences is Li  Lj
if
we prefer Li over Lj
, Li ⇠ Lj
if we are indi↵e re nt about Li and Lj
, and Li ⌫ Lj if either we prefer Li over Lj or are indi↵e re nt. The
axioms of utility theory
1 define a rational behavior. Here, we review 1Von Neumann, J. and O. Morgenstern
(1947). The theory of games and economic
behavior. Princeton University Press these axioms following the nomenclature employed by Russell and
Norvig:2
2Russell, S. and P. Norvig (2009). Artificial
Intelligence: A Modern Approach (3rd ed.). Orderabi Prentice-Hall lity
(Completeness)
A dec ision maker has well-defined prefe re nces
for lotteries so that one of
(Li  Lj), (Lj  Li), (Li ⇠ Lj) holds. Transitivity
Preferences over lotteries are transitive, so that
if (Li  Lj) and (Lj  Lk), then (Li  Lk). Continuity
If the preferences for lotterie s are ordered follow￾ing (Li  Lj  Lk ), then a probab ility p exists
so that [{p, Li}; {1  p,Lk}] ⇠ Lj
.
Substitutability
(Independence)
If two lotteries are equivalent (Li ⇠ Lj), then the lottery Li or Lj can be substituted
by anot he r equivalent lott e ry Lk following
[{p, Li}; {1  p,Lk }] ⇠ [{p, Lj }; {1  p,Lk }]. Monotonicity
If the lott ery Li
is preferred over Lj
, Li  Lj
, then for a probab ility p greater than q,
[{p, Li}; {1  p,Lj}]  [{q, Li}; {1  q,Lj}]. Decomposability
The decomposability property assumes that
there is no reward associat ed with the decision￾making process itself, that is, there is no fun in
gambling.
Li  Lj Li is preferred over Lj
Li ⇠ Lj
Li and Lj are indi↵er- ent
Li ⌫ Lj
Li
is preferred over Lj or are indi↵erent
If the preferences over lotteries can be defined following all these
axioms, then a rat ion al decision maker should choose the lotte ry
associated with the action that maximizes the expected utility.
14.3 Utility Functions
The axioms of utility the ory are used to define a utility function
so that if a lottery Li
is preferred over Lj
, then the lotte ry Li
must have a greater utility than Lj
. If we are indi↵erent about
two lott e rie s, the ir utilities must be equal. These properties areprobabilistic machine learning for civil engineers 233
summarized as
U(Li ) > U(Lj) , Li  Lj
U(Li ) = U(Lj) , Li ⇠ Lj
. The expected utility of a lottery (see equation 14.1) is define d as the
sum of the utility of each possible outcome in the lottery multiplied
by its probability, E[U(L)] = X
x2X
p(x) · U(x). Keep in mind that a utility function defines the relative preferences
between outcomes and not the absolute one. It means that we can
transform a utility function through an affine function,
U
tr(x) = wU(x) + b, w > 0, without changing the preferences of a decision maker. In the case
where we multiply a utility funct ion by a negative constant w < 0,
this is no longe r true; inste ad of maximizin g the expect ed utility, the problem would then c onsist in minimizing the expected loss, a
⇤ = arg min
a E[L(a, X)].
Nonlinear utility functions We now look at the example of a
literal lottery as the term is commonly employed outside the field
of utility theory. This lottery is defined like so: you receive $200 if
a coin toss lands on heads, and you pay $100 if a coin toss lands on
tails. This situation is formalized by the two lotteries respectively
corresponding to taking the bet ( ) or passing ( ), L = [{
1
2
, + }; {
1
2
,  }]
L = [{1, $0}], where you are only certain of the outcome if you choose to pass. The question is, Do you want to take the bet? In practice, when
people are asked, most would not accept the bet, which contradicts
the utility theory principle requiring that one chooses the lottery
with the highest expecte d utility. For this problem,
E[$(L )] = 1
2 ⇥ +$200 +
1
2 ⇥ $100 = +$50
E[$(L )] = $0, which indic at e s that a rational decision maker should take the
bet. Does this mean that the utility theory does not work or that
people are acting irrationally? The answer is no; the reason for this
behavior is that utility functions for quantities such as monetary
value are typically nonlinear.j.-a. goulet 234
Risk aver se versus risk seeking Instead of direct ly defining the
utility for being in a stat e x while having taken the action a, we
can defi ne it for continuous quantities such as a monetary value.
We define a va lue v(a, x) associat ed with the outcome of being in
a stat e x while having taken the action a, an d U(v(a, x)) ⌘ U(v) is
the utility for the value associated with x and a. Figure 14.3 presents examples of utility functions expressing dif￾ferent risk behaviors. When a utility function is linear with respect
to a value v, we say that it represents a risk-neutral behavior, that
is, doubling v also doubles U(v). In comm on cases, individuals are
not displaying risk-neutral behavior because, for example, gaining
or losing $1 will impact behavior di↵erently depe nding on whether
a person has $1 or $1,000,000. A risk-aver se behavior is charac￾terized by a utility funct ion having a negative second derivative
so that the change in utility for a change of value v decreases as
v increases. The consequence is that given the choice between a
certain event of receiving $100 and a lottery for which the expected
gain is E[L] = $100, a risk-averse decision maker would prefer the
certain event. The opposite is a risk-seeking behavior, where there
is a small change in the utility funct ion for small values and large
changes in the utility funct ion for large values. When facing the
same previous choice, a risk-see king decision maker would prefer the
lottery over the certain event.
0 0.2 0.4 0.6 0.8 1
v 0
1
Utility, (v) Risk seeking Risk neutral Risk averse
Figure 14.3: Comparison between risk
-seeking, -neutral, and -averse behaviors for utility functions.
0 0.5 1
0
1
E[ (v a1 , X)] = E[ ( v a2, X)] v(a, x)
p v a, x ( | )
Action a1 Action a2
0 1
0
1
v(a, x)
Utility, U( ) v
0
1
E[U(v(a1,X))]=E[U(v(a2,X))]
p(U(v)|a, x)
(a) Risk neutral
0 0.5 1
0
1
E[ (v a1, X)] = E[ ( v a2 , X)] v(a, x)
p v a, x ( | )
Action a1 Action a2
0 1
0
1
v(a, x)
Utility, U( ) v
0
1
E[U(v(a1
,X))]
E[U(v(a2
,X))]
p(U(v)|a, x)
(b) Risk averse Figure 14.4: Discrete case: Comparison of
the e↵ect of a risk-neutral and a risk-averse behavior on the expected utility.
Let us consider a generic utility function for v 2 (0, 1) so that,
U(v) = v
k
, where
8
<
:
0 < k < 1 Risk averse
k = 1 Neutral
k > 1 Risk seeking. Figure 14.4 compares the e↵ect of a risk-neu tral and a risk-seeki ng
behavior on the conditional expe ct e d utilities E[U(v(ai , X))]. In
this example, there is a binary random variable X describing
the possible state x 2 {1, 2}, where the probability of each out￾come depe nds on an action ai
, an d where v(ai
, x) is the value
of being in a state x while the action ai was taken. This illus- trative example is designed so that the expected value for both
actions are equal, E[v(a1 , X)] = E[v(a2, X)], but not their vari- ance, var[v(a1, X)] > var[ v(a2 , X)]. With a risk-neu tral behav￾ior in (a) , the expec ted utilitie s remain equ al for both actions, E[U (v(a 1, X))] = E[U(v(a 2, X))]. For the risk-averse behavior dis- played in (b), the expect e d utility is higher for action 2 than for
action 1, E [U(v(a2 , X))] > E[U(v(a1 , X))], because the variability in
value is greater for action 1 than for action 2.
Figure 14.5 presents the same example, this time for a c ontinu- ous ran dom variable. The transformation of the probability densityprobabilistic machine learning for civil engineers 235
function (PDF) from the value space to the utility space is done
following the change of variable rule prese nted in §3.4. Again, for
a risk-neu tral behavior (a), the expect ed utility is equal, and for
a risk-averse behavior (b), the expec ted utility is higher for action
2 than for action 1, E[U(v (a2 , X))] > E[U(v(a1 , X))], because the
variability in the value is greater for action 1 than for action 2.
People and organizations typically display a risk-averse behavior, which must be considered if we want to properly model optimal
decisions. In such a case, an optimal decision ai
is the on e that
maximizes E[U(v (ai
, X))]. Although such a dec ision is optimal
in the sense of utility the ory, it is not the one with the highest
expected monetary value. Only a neutral attitude toward risks
maximizes the expected value.
For the bet example presented at the beginning of the section,
we disc usse d that the action chosen is typically to not take the bet,
even if tak ing it would result in the highest expect e d gain. Part of
it has to do with the repeatability of the bet; it is a certainty that
the gain per play will tend to the expected value as the number
of times played increases. Nevertheless, $100 might appear as an
important sum to gamble because if a person plays and loses, he
or she might not a↵ord to play again to make up for the loss. In
the civil engineering context, risk aversion can be seen through a
similar scenario, where if someone loses a large amount of mone y
as the conseq uen ce of a decision that was rightfully maximizi ng
the expected utility, that person might not keep his or her job
long enough to compen sat e for the current loss with subsequ ent
profitable decisions.
0 0.5 1 E[ (v a1, X)]=E[ (v a2, X)]
v(a, x)
f v a,x ( | )
Action a 1 Action a 2
0 1
0
1
v(a, x)
Utility, U( ) v
0
E[U(v(a1 ,X))]=E[U(v(a2,X))]
f(U(v)|a,x)
(a) Risk neutral
0 0.5 1 E[ (v a1,X)]=E[ ( v a2,X)]
v(a, x)
f v a,x ( | )
Action a 1 Action a 2
0 1
0
1
v(a, x)
Utility, U( ) v
0
E[U(v(a1, X))]
E[U(v(a2, X))]
f(U(v)|a,x)
(b) Risk averse Figure 14.5: Continuous case: Comparison
of the e↵ect of a linear (risk-neutral) and a nonlinear (risk-averse) utility function on
the expected utility.
Risk-averse behavior is the reason of being for insurance com￾panies, who have a neutral attitude toward the risks they are
providing insurance for. What they do is cover the risks associated
with events that would incur small monetary losses in comparison
with the size of the company. Insurers seek to diversify the events
covered in order to maximize the indepe ndence of the probability
of each event. The objective is to avoid being exposed to payments
so large that it would jeopardize the solvency of the company. In- dividuals who buy the insurance are risk averse, so they are ready
to pay a premium ove r the expect e d value in order not to be put
in a risk-ne utral position. In other words, they accept paying an
amount higher than the expected costs to an insurer, who itself has
to pay the expected cost because its exposure to losses is spread
over thousands if not million s of custome rs.j.-a. goulet 236
14.4 Value of Information
Value of information quantifies the value associat e d with the action
of gat he ring additional informat ion about a state X in order to
reduce the epistemic uncertainty associated with its knowledge. In
this section, we will cover two cases: perfect and imperfect informa￾tion.
14.4.1 Value of Perfect Information
In cases where the value of a state x 2 X = {1, 2, · · · , X} is imper￾fectly known, one possible action is to collect additional information
about X. W ith the current knowledge of X ⇠ p(x), the expected
utility of the optimal action a
⇤
is
U(a
⇤
) ⌘ E[U(a
⇤
, X)] = max
a X
x2X
U(a, x) · p(x) = X
x2X
U(a
⇤
, x) · p(x). (14.2)
If we gather perfect informati on so that y = x 2 X, we can
then directly observe the true state variable, and the utility of the
optimal action becomes
U(a
⇤
, y) = max
a U(a, y). However, because y has not been observed yet, we must consider all
possible observations y = x weighted by their probability of occur- rence so the expect ed utility conditional on perfect informat ion is
U(a˜
⇤
) ⌘ E[U(a˜
⇤
, X)] = X
x2X
max
a U(a, x) · p(x). (14.3)
Notice how in equat ion 14.2, the max operation was outside of
the sum, whereas in e quat ion 14.3, the max is inside. In equation
14.2, we comput e the expect e d utility where the state is a random
variable and for an optimal action that is common for all states. In eq uat ion 14.3, we assume that we will know the true state once
the observation becomes available, so we will be able to take the
optimal action for each state. Consequently, the e xpected utility
is calculated by weighting the utility corresponding to the optimal
action for each stat e, times the probab ility of occurren ce of that
state. The value of perfect information (VPI) is defin ed as the
di↵erence between the expected utility conditional on perfect
information and the expect ed utility for the optimal action,
VPI(y) = U(a˜
⇤
)  U(a
⇤
)  0.probabilistic machine learning for civil engineers 237
Because the expected utility estimated using perfect information
E[U (a˜
⇤
, X)] is greater than E[U(a
⇤
, X)], the VPI(y) must be greater
or eq ual to zero. The VPI quantifies the amount of mon ey you
should be willing to pay to obtain perfect informat ion. Note that
the concepts presented for discrete random variables can be ex￾tended for continuous on es by rep lac ing the sums by integrals.
Soil contamination ex ample We apply the concept of the value of
perfect information to the soil contamination example. The current
expected utility conditional on the optimal action is
U(a, x) x = x =
a = $100 $100 a = $0 $10K
y={ , }
y =
:$100
Pr=1
⇠:⇠$1⇠00
Pr=0
:$10K
Pr=1
⇢:$⇢0
Pr=0
Pr =
0.1
y =
⇠:⇠$1⇠00
Pr=0
:$100
Pr=1 ⇠:⇠$1⇠0⇠K
Pr=0
:$0
Pr=1
Pr =
0.9
U = $10
U = $0
U =⇠$⇠10⇠0
U = ⇠$⇠10⇠K
U = $100
Figure 14.6: Decision tree illustrating the value of perfect information.
E[U( , X)] = ($0 ⇥ 0.9) + ($10K ⇥ 0.1) = $1K
E[U( , X)] = ($100 ⇥ 0.9) + ($100 ⇥ 0.1) = $100 = U(a
⇤
),
so the current optimal action is to send the soil to a recycling plant
with an associat ed expec ted utility of - $100. Figure 14.6 presents
the decision tree illustrating the calculation for the value of perfect
information. The expected utility conditional on perfect informat ion
is
U(a˜
⇤
) = X
x2X
max
a U(a, x) · p(x) = $0 ⇥ 0.9
| {z }
y=x= + $100 ⇥ 0.1
| {z }
y=x= = $10 . Having access to perfec t informat ion red uc es the expect e d cost
because there is now a probability of 0.9 that the observation will
indicate that no treatment is required and only a probability of
0.10 that tre atm ent will be required . Moreover, the possibility of
the worst case associat ed with a false negative where {a = , x =
} is now removed from the possibilities. The value of perfect
information is thus
VPI(y) = U(a˜
⇤
)  U(a
⇤
) = $90 .
It mean s that we should be willing to pay up to $90 for perfe ct
information capable of indicating whether not the m3 of soil is
contaminated.
14.4.2 Value of Imperfect Information
It is common that observations of state variables are imperfec t, so that we want to compute the value of imperfect informat ion. For the case of a discrete state variab le x that belongs to the se t
X = {x1 , x2, · · · , xX}, the expect ed costs conditional on imperfec tj.-a. goulet 238
information is obtained by marginalizing over both the possible
states x 2 X and the possible observations y 2 X, so that
U(a˜
⇤
) = X
y2X
max
a ⇣ X
x2X
U(a, x) · p(y|x) · p(x) | {z }
p(y,x)
⌘
.
p(x, y) y = y =
x = 0.9·1=0.9 0.9·0=0 x = 0.1·0.05=0.005 0.1·0.95=0.095
p(y) 0.905 0.095
p(x|y) y = y =
x = 0.9
0.905
0
0.095 x = 0.005
0.905
0.095
0.095
y={ , }
y =
$100
Pr=
0.095
0.095
:$100
Pr=
0
0.905
:$10K
Pr=
0.09
0.0
5
95
:$0
Pr=
0
0.095
Pr =
0.095
y =
:$100
Pr=
0.005
0.905
:$100
Pr=
0.9
0.905
:$10K
Pr=
0.00
0. 5
905
:$0
Pr=
0.9
0.905
Pr =
0.905
U = $59.5
U = $55.25
U =⇠$⇠10⇠0
U = ⇠$⇠10⇠K
U = $100
Figure 14.7: Decision tree illustrating the value of imperfect information.
Soil contamination ex ample We apply the concept of the value
of imperfec t informat ion to the soil contamination example . The
conditional probability of an observation y conditional on the state
x is
p(y|x)⇢
Pr(y = |x = ) = 1
Pr(y = |x = ) = 0.95, where the observation is perfect if the soil is not contaminated, and the probab ility of a correct classification is 0.95 if the soil is
contaminated. Figure 14.7 depicts the decision tree for calculating the value
of informat ion for this example , where the probability of each
observation is obtained by marginalizing the joint probability for
both observations and stat e s, p(y) = X
x2X
p(y|x) · p(x). The expected utility condition al on impe rfec t information is then
U(a˜
⇤
) = X
y2X
max
a ⇣X
x2X
U(a, x) · p(y|x) · p(x) | {z }
p(x,y)
⌘ = $0 ⇥ 0.9
| {z } x= + $10K ⇥ 0.005
| {z } x=
| {z }
y=
+ $100 ⇥ 0
| {z } x= + $100 ⇥ 0.095
| {z } x=
| {z }
y= = $59.5 . The value of information is now defined by the di↵erence between
the expected utility conditional on imperfect information and the
expected utility for the optimal action, so that
VOI(y) = U(a˜
⇤
)  U(a
⇤
)  0 = $59.5  ($100) = $40.5 . By c omp aring this result with the value of perfect informat ion
that was equ al to $90, we see that having observation uncertainty
reduces the value of the informat ion. The reader interested in advanced notions related to rational
decisions and utility theory should consult specialized textbooks3
3Russell, S. and P. Norvig (2009). Artificial
Intelligence: A Modern Approach (3rd ed.). Prentice-Hall; and Bertsekas, D. P. (2007). Dynamic programming and optimal control
such as (4th ed.), Volume 1. Athena Scientific those by Russell and Norvig or Bertse kas.15
Sequential Decisions
States: s 2 S = {s1, s2, · · · , sS}
Actions: a 2 A = {a1, a 2, · · · , aA}
Policy: ⇡ = {a(s1), a(s2), · · · , a(s S)}
Reward: r(s, a, s
0) 2 R
Transition model: p(s
0|s, a)
In this chapter, we explore how to make optimal decisions in a
sequential context. Sequential decisions di↵er from the decision con￾text presented in chapter 14 because here, the goal is to select the
optimal action to be performed in order to maximize the current
reward as well as future ones. In that process, we must take into
account that our current action will a↵ect future states as well as
future decisions. Let us consider a system that can be in a state
s 2 S = {s1,s2, · · · , sS}. The set of actions a 2 A = {a1, a2, · · · , aA}
that should be performed given each state of the syste m is called a
policy, ⇡ = {a(s1), a(s2), · · · , a(sS)}.
In the context of sequential decisions, the goal is to identify
the optimal policy ⇡
⇤ describing for each possible state of the
system the optimal action to take. This task can be formulated
as a reinforcement learning (RL) problem, where the optimal Note: Reinforcement learning di↵ers
from supervised learning because the optimal policy is not learned using other examples of policies. RL also di↵ers from
unsupervised learning because the goal is not to discover a structure or patterns in
the data.
policy is learned by choosing the act ions a
⇤
(s) that maximize
the expected utility over a given planning horizon. In order to
compute the expected utility, we need a function describing for each
set of current state s, action a, and nex t stat e s
0 the associated
reward r(s, a, s
0) 2 R. In practical cases, it can happen that the
reward simplifies to a special case such as r(s, a, s
0) = r(s, a), r(s, a, s
0) = r(a, s
0), or even r(s, a, s
0) = r(s). The last part we need
to define is the Markovian transition model p(s
0|sj
, ai) = Pr(S
0 =
sk|s = sj
, ai), 8k 2 {1 : S}, that is,
p(s
0|sj, ai) ⌘
8
>><
>>
:
Pr(St+1 = s1|st = sj
, ai)
Pr(St+1 = s2|st = sj
, ai)
.
.
. Pr(St+1 = s S|st = s j, ai). The transition model describes, given the current state of the
Note: A transition model is analogous to
the concept of a conditional probability
system s table presented in chapter 11. j and an action taken ai
, the probability of ending in anyj.-a. goulet 242
state s
0 2 S at time t + 1. He re , the M arkov property (see chapter
7) allows formulating a model where the prob ability of each state
s
0 2 S at time t + 1 depends on the current state sj at time t and
is independent of any prev iou s stat es. Althou gh seq uential decision
problems could also be formulated for continuous state variables, s 2 R, in this chapter we restrict ourselves to the discret e case.
Example: Infrastructure maintenance planning The notions associ- ated with reinforcement learning and the sequential decision process
are illustrated through an application to infrastructure maintenanc e
planning. Let us consider a population of bridges (see figure 15.1)
for which at a given time t, each bridge can be in a stat e
(a) Representation of North America’s
road network, which includes hundreds of
thousands of bridges. (Photo: Commission
for Environmental Cooperation)
(b) Example of a given structure within
the network. (Photo: Michel Goulet)
Figure 15.1: Contextual illustration of a
sequential decision process applied to the problem of infrastructure maintenance planning.
s 2 S =

100 % | {z }
s1
, 80 % |{z}
s 2
, 60 % |{z}
s3
, 40 % |{z}
s4
, 20 % |{z}
s5
, 0 %|{z}
s6
 
, where 100 percent indicat e s a perfect condition , an d 0 percent
corresponds to a stat e whe re the damage is so ex te nsive that the
structure must be close d. The objective of sequential decision
making is to learn the optimal policy ⇡
⇤
(s) that identifies among a
set of possible actions, a 2 A = {do nothing (N), maintain (M),rep lace (R)}, what is the optimal one a
⇤
(s) to take at e ach time , depending on
the current condition s. In this example, a bridge condition s is rated every year from
the results of visual inspections. We assume we have a datab ase
D containing the con ditions ob tained from inspection results for N
structures each inspected T times,
D = 8
>
<
>
:
{s1, s 2, · · · , sT }1
{s1, s 2, · · · , sT }2
.
.
. {s1, s 2, · · · , sT}N
9
>
=
>
;
N⇥T
.
0 5 10 15 20
0%
20%
40%
60%
80%
100%
Time t State s
Figure 15.2: Example of bridge condition
time histories.
Each line in this dat ab ase corresponds to a given structure and
each column corresponds to a given time t 2 {1,2, · · · , T}. Figure
15.2 presents an example for N = 15 bridges and time histories of
length T = 20. One of them has been highlighted with a thicker
red line to better visualize how a structure transits from state to
state through time. In this data set, the action taking place at each
time t is a = do nothing. We can use this dat a set to estimat e
the parameters of the Markovian transition model p(st+1|st
, a =
do nothing) by emp loying the maximum likelihood estimat e for
each transition prob ab ility (see §11.4). The maximum likelihoodprobabilistic machine learning for civil engineers 243
estimate (MLE) corresponds to the number of struct ure s that, from
a stat e i at time t, transitioned to a state j at t + 1, divided by the
numbe r of structures that started from a state i at time t, Pˆr(St+1 = sj |st = si, at = do nothing) = #{st = si
, st+1 = s j}
#{st = si}
, where {s i,s j} 2 S
2 and the # symbol describes the number of
elements in a set. These results are stored in a transition matrix,
p(s t+1|st
, at = do nothing
| {z } N
) =
2
6
6
6
6
4
0.95 0.03 0.02 0 0 0
0 0.9 0.05 0.03 0.02 0
0 0 0.8 0.12 0.05 0.03
0 0 0 0.7 0.25 0.05
0 0 0 0 0.6 0.4
0 0 0 0 0 1
3
7
7
7
7
5
S⇥S
,
where each element [ ]ij corresponds to the probability of landing in
state st+1 = sj given that the system is currently in state st = si
. This matrix is analogous to the transition matrix we have seen
in §12.2. In the case of infrastructure degradation, the transition
matrix has only zeros below its main diagonal because, without
intervention, a bridge’s condition can only decrease over time, so
a transition from st = s i ! st+1 = sj can only have a nonz ero
probability for j  i. For maintenance and rep lacem ent action s, the tran sition matrix
can be defined deterministically as
p(st+1|s t
, at = maintain
| {z } M
) =
2
6
6
6
6
6
4
1 0 0 0 0 0
1 0 0 0 0 0
0 1 0 0 0 0
0 0 1 0 0 0
0 0 0 1 0 0
0 0 0 0 1 0
3
7
7
7
7
7
5
S⇥S
,
where from a current state st = s i
, the state becomes st+1 =
si1. For example, if at a time t the state is st = s3 = 60%, the
maintenance action will result in the state st+1 = s2 = 80%. Note
that if the state is st = s1 = 100%, it remains the same at t+ 1. For
a replacement action, no matter the initial state at t, the next state
becomes st+1= s 1 = 100% so,
p(st+1|st
, at = replace
| {z }
R
) =
2
6
6
6
4
1 0 0 0 0 0
1 0 0 0 0 0
1 0 0 0 0 0
1 0 0 0 0 0
1 0 0 0 0 0
1 0 0 0 0 0
3
7
7
7
5
S⇥S
.j.-a. goulet 244
In order to identify the optimal policy ⇡
⇤
, we need to define
the reward r(s, a, s
0). Here, we assume that the reward simplifies
to r (s, a) = r (s) + r(a), so it is only a function of the state s and
the action a. For a given structu re, the re ward r (s) is estimated as
a funct ion of the number of users quantified through the annual
average daily traffic flow (AADTF = 105 users/day), times a value
of $3/user, times the capacity of the bridge as a function of its Note: Here the value of $3/user does not necessarily represent the direct cost per user as collected by a toll booth. It instead
corresponds to the indirect value of the
infrastructure for the society.
condition. The capacity is defined as c(S) = {1, 1, 1,0.90,0.75, 0}, and the associated rewards are
r(S) = 10 5 users/day · 365 day · $3/user · c(S) = {109.5, 109.5, 109.5, 98.6, 82.1, 0}$M. The rewards for actions r (a) correspond to a cost, so their values
are lesser than or equal to zero,
r(A) = {0,5, 20}$M.
Agent
Environment
Figure 15.3: Schematic representation of
the interaction between an agent and its environment.
For the context of re inforc ement learning, we can generalize the
example above by introducing the conc ep t of an agent interacting
with its environment, as depicted in figure 15.3. In the context
of the previous example, the environment is the population of
structures that are degrading over time, and the agent is the hypo￾thetical entity acting with the intent of maximizing the long-term
accumulation of rewards. The environment interacts with the agent
by de fining, for e ach time t, the state st of the system and the re￾ward for being in that state s, taking a given action a, and e nding
in the next state s
0. The agent perceives the environment’s stat e
and selects an action a, which in turn can a↵ect the environment
and thus the state at time t + 1.
This chapter explores the task of identifying the optimal policy
⇡
⇤
(s) describing the optimal actions a
⇤
to be taken for each stat e s. This task is first formulated in §15.1 using the model-based method
known as the Markov decision process. Building on this me thod, §15.2 then presents two model-free reinforce ment learning meth ods:
temporal di↵erence and Q-learning.
15.1 Markov Decision Process
In order to formulate a Markov decision process (MDP), we need to
define a planning horizon over which the utility is estimated. Plan - ning horizons may be either finite or infinite. With a finite planning
horizon, the rewards are considered over a fixed period of time. In
such a case , the opt imal policy ⇡
⇤
(s,t) is nonstationary because it
depends on the time t. For the infrastructure maintenance example,probabilistic machine learning for civil engineers 245
a finite plan ning horizon c ould correspond to the case where we
want to identify the optimal actions to take over the next T = 20
ye ars, afte r which we know a structure is going to be demolished. The optimal action for a state st = 40 % would then not be the
same whether we are at t = 1 or at t = 19. For an infinite planning
horizon, the rewards are considered over an infinite period of time . In such a case, the opt imal policy ⇡
⇤
(s) is stationary as it does
not depend on the time t. It means that at any time t, the optimal
action to take is only a function of the current state s we are in. In
this chapter, we only study problems associated with an infinite
planning horizon, as they allow identifying a stationary optimal
policy ⇡
⇤
(s) rather than one which depends on time ⇡
⇤
(s,t), as is
the case with a finite planning horizon.
Finite planning horizon Nonstationary policy
Infinite planning horizon
Stationary policy
15.1.1 Utility for an Infinite Planning Horizon
The utility associated with an infinite plan ning horizon is de fined Note: Some references employ the term
utility, others, the term value to refer to
the sum of discounted rewards. as the re ward for being in the current state plus the sum of the
discounted rewards for all future states, U(st=0, st=1, · · · , st=1) = r(st=0 ) + r(st=1) + 
2r(st=2) + · · · = X1
t=0

tr(st ) 
max
s2S
r(s)
1  
.
(15.1)
For a discount factor  2 (0, 1], the discounted sum of rewards over
Note: In this subsection, the notation is
simplified by assuming that r( s, a, s
0 ) =
r(s). Nevertheless, the same reasoning
holds for the general case where the reward
is r(s, a, s
0).
an infinite planning horizon is a finite quantity. This is an essential
property because without a discount factor, we could not comp are
the performance of ac tion s that would e ach have infinite utilities. In the special case whe re there is a terminal state for which the
problem stops, the discount factor can be taken as  = 1 because
it will be possible to compute a finite estimat e for the utility. Note
that for the infrastructure maintenance planning example, there
is no terminal state and the discount factor can be interpreted as
an inte re st rat e using the tran sformation
1
  1. The issue with
our planning problem is that we do not know yet what will be the
future states st=1, st=2, · · ·, so we cannot com pute the utility using
equation 15.1. Instead, the expected utility for being in the state s0
is computed as
U(s0 , ⇡) ⌘ E[U(s0, ⇡)] = r(s0) + E
"X1
t=1

tr(St )#
. (15.2)
In equation 15.2, the probability of each state s at each time t > 0
is estimated recursively using the transition model p(st+1|st
, a),j.-a. goulet 246
where at each step, the action taken is the one defined in the pol￾icy ⇡(s). Here, we choose to simplify the notat ion by writing the
expected utility as E[U(s, ⇡)] ⌘ U(s, ⇡). Moreover, the notation
U(s, ⇡) ⌘ U(s) is employed later in this chapter in order to further
simplify the notation by making the dependency on the policy ⇡
implicit. For a state s, the optimal policy ⇡
⇤
(s) is defined as the action a
that maximizes the expected utility, ⇡
⇤
(s) = arg max
a2A X
s
02S
p(s
0|s, a) · 
r(s, a, s
0) + U(s
0, ⇡
⇤
)
. The difficulty is that, in this definition, the optimal policy ⇡
⇤
(s)
appears on both sides of the equality. The next two sections show
how this difficulty can be tackled using value and policy iteration.
15.1.2 Value Iteration
Equation 15.2 described the expected utility for being in a state s, where the dependence on the actions a was implicit. The defi nition
of this relationship with the explicit dependence on the actions is
described by the Bellman
1 equation, 1Bellman, R. (1957). A Markovian decision
process. Journal of Mathematics and
U Mechanics 6(5), 679–684 (s) = max
a2A X
s
02S
p(s
0|s, a) · 
r(s, a, s
0) + U(s
0)
, (15.3)
where the optimal action a
⇤
(s) is selected for each state through
the max operation. Again, the difficulty with this equation is
that the expected utility of a state U(s) appears on both sides of
the equality. One solution to this problem is to employ the value
iteration algorithm where we go from an iteration i to an iteration
i + 1 using the Bellman update step, U
(i+1)(s) max
a2A X
s
02S
p(s
0|s, a) · ⇣
r(s, a, s
0) + U
(i)(s
0)⌘
. (15.4)
In order to use the Bellman update, we start from the initial values
Note: The symbol indicates that the quantity on the left-hand side is recursively updated using the terms on the right-hand
side, which are themselves depending on
the updated term.
at iteration i = 0, for example, U
(0)(s) = 0, and then iterate until
we converge to a steady state . The value iterat ion algorithm is
guaranteed to converge to the exact expected utilities U
(1)(s) if
an infinite number of iterat ions is employed. The optimal act ion s
a
⇤
(s) identified in the process for each state s define the optimal
policy ⇡
⇤
. The sequence of steps for value iteration are detailed in
algorithm 10.
Example: Infrastructure maintenance planning We now apply the
value iteration algorithm to the infrastructure maintenance exampleprobabilistic machine learning for civil engineers 247
Algorithm 10: Value itera tio n
1 define: s2S (states)
2 define: a2A (actions)
3 define: r(s, a, s
0) (rewards)
4 define:  (discount factor)
5 define: p(s
0|s, a) (transition model)
6 define: ⌘ (convergence criterion)
7 initialize: U 0(s) (expected utilities)
8 while |U 0(s)  U(s)|  ⌘ do
9 U(s) U 0(s)
10 for s 2 S do
11 for a 2 A do
12 U(s, a) = X
s
02S
p(s
0|s, a) · 
r(s, a, s
0) + U(s
0)
13 ⇡
⇤
(s) a
⇤ = arg max
a2A
U(s, a);
14 U
0(s) U(s, a
⇤
)
15 return: U(s) = U 0(s), ⇡
⇤
(s)
defined at the beginning of this chapter. The last parameters that
need to be defined are the c onvergence criterion ⌘ = 103 and the
discount factor taken as  = 0.97, which corresponds to an interest
rate of 3 percent. Starting from U
(0)(S) = {0,0, 0, 0, 0, 0}$M, we
perform iteratively the Bellman update step for each state s,
U
(i+1)(s) max
a2A
8
>>
><
>>
>:
X
s
02S
p(s
0|s, N) · ⇣
r(s, N) + U
(i)(s
0)⌘
X
s
02S
p(s
0|s, M) · ⇣
r(s,M) + U
(i) (s
0)⌘
X
s
02S
p(s
0|s, R) · ⇣
r(s,R) + U
(i) (s
0)⌘
, where we choose the optimal action a
⇤
leading to the maximal ex￾pected utility. The expected utilities for two first and last iterations
are
U
(1)(S) = {
s=100%
z }| { 109.5,
80%
z}|{ 211 ,
60%
z}|{ 309 ,
40%
z}|{ 393 ,
20%
z}|{ 459 ,
0%
z}|{ 440 } $M
U
(2)(S) = {222.5, 329, 430, 511, 572, 550} $M
.
.
. U
(356)(S) = {3640, 3635, 3630, 3615, 3592, 3510} $M
U
(357)(S) = {3640, 3635, 3630, 3615, 3592, 3510} $M.
Example setup
S = {100, 80, 60, 40, 20, 0} %
A = {do nothing
| {z } N
, maintain | {z } M
, replace
| {z } R }
r(S) = {109.5, 109.5, 109.5, 98.6, 82.1, 0}$M
r(A) = {0, 5, 20}$M
r(s, a, s
0 ) = r(s, a) = r(s) + r(a)
p(s
0|s, N) = 2
6
6
6
6
4
0.95 0.03 0.02 0 0 0
0 0.9 0.05 0.03 0.02 0
0 0 0.8 0.12 0.05 0.03
0 0 0 0.7 0.25 0.05
0 0 0 0 0.6 0.4
0 0 0 0 0 1
3
7
7
7
7
5
p(s
0|s, M) =
2
6
6
6
6
4
1 0 0 0 0 0
1 0 0 0 0 0
0 1 0 0 0 0
0 0 1 0 0 0
0 0 0 1 0 0
0 0 0 0 1 0
3
7
7
7
7
5
p(s
0|s, R) =
2
6
6
6
6
4
1 0 0 0 0 0
1 0 0 0 0 0
1 0 0 0 0 0
1 0 0 0 0 0
1 0 0 0 0 0
1 0 0 0 0 0
3
7
7
7
7
5j.-a. goulet 248
Here, we e xplicitly show the calculations mad e to go from U
(1)(100%) =
$109.5M ! U
(2)(100%) = $222.5M,
U
(2) (100%) max
a2A
8
>
><
>>
:
0.95 · ($109.5M + 0.97 · $109.5M)· · · +0.03 · ($109.5M + 0.97 · $211M)· · · +0.02 · ($109.5M + 0.97 · $309M) = $222.5M
1 · (109.5  $5M + 0.97 · $109.5M) = $210.7M
1 · (109.5  $20M + 0.97 · $109.5M) = $195.7M
= $222.5M, where the optimal action to perform if you are in state s = 100%
is ⇡
⇤(2) (100%) = a
⇤ = do nothing. In order to complete a full
iteration, this operation needs to be repeated for each stat es s 2 S. Figure 15.4 presents the evolution of the expected utility of each
state as a function of the iteration number. Note how the expected
utilities converge to stationary values for all state variables. The
corresponding optimal policy is
⇡
⇤
(S) = {do nothing
| {z }
s=100%
, maintain
| {z }
80%
, maintain
| {z }
60%
, maintain
| {z }
40%
,replace | {z }
20%
,replace | {z }
0%
}.
0 100 200 300 400
0
1000
2000
3000
4000
iterations #
U( )s , [$M]
s = 100%
s = 80%
s = 60%
s = 40%
s = 20%
s = 0%
Figure 15.4: Value iteration algorithm
converging to stationary expected utilities.
Figure 15.5 presents the expected utility for each action and state
obtained using the value iteration algorithm. For each state, the
expected utility corresponding to the optimal act ion is highlighted
with a red border. In the Bellman update step of the value iterat ion algorithm, the max operation is compu tationally expensive if the number of
possible actions A is large. The difficulty is solved by the policy
iteration algorithm. 10 %
80%
60%
40%
20%
0%
3400
3450
3500
3550
s
U( ) ] s , [$M
Do nothing Maintain Replace
Figure 15.5: Expected utility for each
action and state obtained using the value
iteration algorithm. The expected utilities of the optimal action is highlighted with a
red border.
15.1.3 Policy Iteration
Remember that a policy ⇡ = {a(s1), a(s 2), · · · , a(sS)} defines an
action to be taken for each state s 2 S. For the calc ulat ion of theprobabilistic machine learning for civil engineers 249
expected utility of each state s, we can redu ce the comp utational
burden of the Bellman update step in equation 15.4 by replacing
the max operation with the action a defined by the policy ⇡
(i) (s), U
(i+1)(s) X
s
02S
p(s
0|s, ⇡
(i)(s) | {z }
a⇤ ) · 
r(s,⇡
(i)(s) | {z }
a⇤
, s
0) + U
(i)(s
0)
. (15.5)
If we employ equ ation 15.5 inste ad of equ at ion 15.4 to update the
expected utility, we are required to perform only one sum rather
than A sums, where A corresponds to the number of possible action s. Once the expected utility has been calc ulat ed for each state, the
policy can be optimized using
⇡
(i+1)(s) a
⇤ = arg max
a2A X
s
02S
p(s
0|s, a) · ⇣
r(s, a, s
0) + U
(i+1)(s
0)⌘
. We then repeat the successive steps of first calculating the expected
utility with the value iteration algorithm using a fixed policy, and
then update the policy. The details of the policy iteration procedure
are presented in algorithm 11, where the policy evaluation step
corresponds to the value iterat ion performed while e mploying equa￾tion 15.5 instead of equation 15.4. Note that identifying an optimal
policy using the policy or the value iteration algorithm lead s to
the same results, except that policy iteration is computationally
cheaper. Algorithm 11: Policy iteration
1 define: s 2 S (states)
2 define: a 2 A (actions)
3 define: p(s
0|s, a) (transition model)
4 define: r(s, a, s
0) (reward)
5 define:  (discount factor)
6 initialize: U(s) (expected utilities)
7 initialize: ⇡
0(s) 2 A (policy)
8 while ⇡
0(s) =6 ⇡(s) do
9 ⇡(s) = ⇡
0(s)
10 U(s) polic y evaluation(U(s), ⇡(s), p(s
0|s, a), r(s, a, s
0), )
11 for s 2 S do
12 ⇡
0(s) arg max
a2A X
s
0 2S
p(s
0|s, a) · 
r(s, a, s
0) + U(s
0)
13 return: U(s), ⇡
⇤
(s) = ⇡
0(s)
Example: Infrastructure maintenance planning Starting from
U
(0)(S ) = {0, 0, 0,0,0, 0}$M, we perform the policy iterationj.-a. goulet 250
algorithm where the Bellman update step in the policy evaluation
is defined by equation 15.5, where the optimal action a = ⇡
(i)(s) is
defined by the optimal policy at iteration i. The expected utilities
for the two first an d last iterations are
U
(1) (S) = {
s=100%
z}|{ 2063,
80%
z}|{ 1290,
60%
z}|{ 768 ,
40%
z}|{ 455 ,
20%
z}|{ 197 ,
0%
z}|{ 0 } $M
U
(2) (S) = {3483, 3483, 3468, 3457, 3441, 3359} $M
.
.
. U
(5) (S) = {3640, 3635, 3630, 3615, 3592, 3510} $M, and their corresponding policies are
A = {do nothing
| {z } N
, maintain
| {z } M
, replace
| {z } R }
⇡
(0) (S) = {N, N, N, N, N, N}
⇡
(1) (S) = {M, M, R, R, R, R}
⇡
(2) (S) = {N, N, M, M, R, R}
.
.
. ⇡
⇤
(S) = ⇡
(6) = ⇡
(5) (S) = {N, M, M, M, R, R}. Here, we e xplic itly show the calc ulat ions made to go from the
policy iterat ion lo op 1 ! 2, U
(1)(100%) = $2063M ! U
(2) (100%) =
$3483M, within which 351 policy evaluation calls are required,
U
(1) (100%) ⌘ U
(2)(1) (100%) =
⇡
(1) (100%) = M, maintain z }| { 1 · (109.5  $5M + 0.97 · $2063M) = $2106M
U
(2)(2) (100%) = 1 · (109.5  $5M + 0.97 · $2106M) = $2147M
.
.
. U
(2) (100%) = U
(2)(351) (100%) = 1 · (109.5  $5M + 0.97 · $3483M) = $3483M.
policy iteration loop
policy evaluation loop
State policy iteration loop1
Once the policy evaluat ion loop is completed, the policy must
be updated. Here, we explicitly look at the specific iteration
⇡
(1) (100%) = M ! ⇡
(2)(100%) = N, so that
⇡
(2)(100%) = arg max
a2A
8
>
><
>>
>:
X
s
02S
p(s
0|s, N) · ⇣
r(s, N) + U
(2)(s
0)⌘
X
s
02S
p(s
0|s, M) · ⇣
r(s,M) + U
(2)(s
0)⌘
X
s
02S
p(s
0|s, R) · ⇣
r(s,R) + U
(2)(s
0)⌘
= arg max
a2A
8
>>
<
>
:
0.95 · (109.5  $0M + 0.97 · $3483M)· · · +0.03 · (109.5  $0M + 0.97 · $3483M)· · · +0.02 · (109.5  $0M + 0.97 · $3468M) = $3488M
1 · (109.5  $5M + 0.97 · $3483M) = $3483M
1 · (109.5  $20M + 0.97 · $3483M) = $3468M
= N (do nothing).probabilistic machine learning for civil engineers 251
The policy iteration converges in five loops and it leads to the same
optimal policy as the value iteration, ⇡
⇤
(S) = {do nothing
| {z }
s=100%
, maintain | {z }
80%
, maintain | {z }
60%
, maintain | {z }
40%
,replace | {z }
20%
,replace | {z }
0%
}. Further det ails as well as advanced concepts regarding Markov
decision processes can be found in the te xtbooks by Russell and
Norvig, 2 and by Bertsekas.3 2Russell, S. and P. Norvig (2009). Artificial
Intelligence: A Modern Approach (3rd ed.). Prentice-Hall 3Bertsekas, D. P. (2007). Dynamic programming and optimal control (4th ed.), Volume 1. Athena Scientific
15.1.4 Partially Observable Markov Decision Process
One hypothesis with the Markov decision process presented in the
previous section is that state variables are exactly observed, so that
at the current time t, one knows in what stat e s the system is in. In such a case, the opt imal ac tion defi ne d by the policy ⇡(s) can
be directly selec ted . For the case we are now interested in, s is a
hidden-state variable, and the observed state y is defined through
the conditional probability p(y|s). This problem configuration is
called a partially observable MDP (POMDP). The challenge is that
at any time t, you do not observe exact ly what is the true stat e
of the system; consequently, you cannot simply select the optimal
action from the policy ⇡(s). Because the state is observed with uncertainty, at each time t, we nee d to desc ribe our knowledge of the state using a prob ability
mass function p(s) = {Pr(S = s1),Pr(S = s2 ), · · · ,Pr(S = sS )}. Given the PMF p(s) describing our current knowledge of s, an
action taken a, and an ob se rvation y 2 S, then the probability of
ending in any state s
0 2 S at time t + 1 is given by
p(s
0|a, y) =
p(y,s
0 |a) z }| {
p(y|s
0)
p(s
0 |a) z }| { X
s2S
p(s
0 ,s|a) z }| { p(s
0|s, a) · p(s)
p(y)
. With the MDP, the policy ⇡(s) was a funct ion of the stat e we are
in; with the POMDP we need to redefine the policy as being a
function of the probability of the state ⇡(p(s)) we are in. In the
conte xt of an MDP, the policy can be seen as a function defined
over discret e variable s. With a POM DP, if there are S possible
states, ⇡ (p(s)) is a function of a continuous domain with S  1
dimensions. This domain is continuous because Pr(S = s) 2 (0, 1),
and there are S  1 dimen sions because of the constraint req uiring
that
P
s2S p(s) = 1.
MDP
⇡(s), s 2 S|{z} discrete POMDP
⇡(p(s)), p(s) 2 (0, 1)
S
| {z }
continuousj.-a. goulet 252
Because the policy is now a func tion of prob ab ilities de fined in
a continuous domain, the value and policy iteration algorithms
presented for an MDP are not dire ct ly applicable. One particularity
of the POMDP is that the set of actions now includes additional
information gathering through observation. The reader interested
in the details of value and policy-iteration algorithms for POMDP
should refer to specialized literature such as the paper by Kael- bling, Littman, and Cassandra.4 Note that solving a POMDP is
4 Kaelbling, L. P., M. L. Littman, and
A. R. Cassandra (1998). Planning and
acting in partially observable stochastic domains. Artificial Intelligence 101 (1), 99–134
significantly more demanding than solving an MDP. For practical
applications, the exact solution of a POMDP is often intractable
and we have to resort to approximate methods.5
5Hauskrecht, M. (2000). Value-function
approximations for partially observable Markov decision processes. Journal of
Artificial Intelligence Research 13 (1), 33–94
15.2 Model-Free Reinforcement Learning
The Markov dec ision process presented in the pre viou s sec tion is
categorized as a model-based method because it requires knowing
the model of the environment that takes the form of the transition
probabilities p(s
0|s, a ). In some applications, it is not possible or
practical to define such a model because the number of states
can be too large, or can be continuous so that the definition of
transition probabilities is not suited. Model-free reinforcement
learning methods learn from the interaction between the agent and
the environment, as depicted in figure 15.3. This learning process is
typic ally don e by subjecting the agent to multiple episodes. In this
section, we will present two model-free methods suited for such a
case: temporal di↵erence learning and Q-learning.
15.2.1 Temporal Di↵erence Learning
The quantity we are interested in estimating is E[U(s, ⇡)] ⌘
U(s, ⇡) ⌘ U(s), that is, the expected utility for being in a state s. We defined in §15.1.1 that the utility is the sum of the discounted
rewards, U(s, ⇡) = X1
t=0

tr(st , at , st+1 ),
obtained while following a policy ⇡ so that at = ⇡(st). Moreover, we
saw in §6.5 that we can estimate an expected value using the Monte
Carlo method. Here , in the context where our agent can interact
with the environment over multiple episodes, we can estimate
U(s, ⇡) by taking the average over a set of re aliz at ions of U(s, ⇡). In a gen eral mann er, the average xT of a se t {x1, x2 , · · · , xT} can
be calculated incrementally by following
xt = xt1 + 1
t (xt  xt1).probabilistic machine learning for civil engineers 253
0 100 200 300 400 500
2
2.5
3
3.5
t
xt
xt = xt1 +
1
t(xt  xt1) x =
1T PT
t=1xt
Figure 15.6: The incremental calculation of an average for a set {x1 , x2, · · · , x500}, xt : X ⇠ N(x; 3, 1).
Figure 15.6 presents an example of an application of the incre￾mental average estimat ion for a set of T = 500 realizat ions of
xt
: X ⇠ N(x; 3, 1). We can apply the same princ iple for the
incremental estimation of the expected utility so that
U
(i+1) (s) U
(i)(s) +
1
N(s) ⇣U
(i)(s)  U
(i)(s)⌘
, (15.6)
where N(s) is the number of times a state has been visited. Esti- mating U(s) through Monte Carlo sampling is limited to problems
having a terminal state and req uire s evaluating the utilitie s over
entire episodes. Temporal di↵erence (TD) learning,6 allows going 6 Sutton, R. S. (1988). Learning to predict
by the methods of temporal di↵erences. Machine Learning 3 (1), 9–44
around these requirements by replacing the explicit utility calcula￾tion U
(i)(s) by the TD-target defined as r (s, a, s
0) + U
(i) (s
0), so that
equation 15.6 can be rewritten as
U
(i+1)(s) U
(i)(s) + ↵
⇣
r(s, a, s
0) + U
(i)(s
0) | {z } TD-target U
(i)(s)⌘
. (15.7)
Temporal di↵erence learns by updating recursively the state-utility
function U
(i+1)(s) by tak ing the di↵eren ce in expected utilitie s
estimated at consecutive times while the agent interacts with the
environm ent. In equation 15.7, ↵ is the learning rate defining how much is
learned from the TD update step. Typically, we want to employ a
learning rate that is initially large so our poor initial values for U(s)
are strongly influenced by the TD updates and then decrease ↵ as a
function of the number of time s a particu lar set of state and act ion
has been seen by the agent. In order to ensure the convergence of
the state-utility function, the learning rate should satisfy the two
following conditions, X1
N(s,a)=1
↵(N(s, a)) = 1, X1
N(s,a)=1
↵
2
(N(s, a)) < 1. A common learning rate function that satisfies both criteria is
↵(N(s, a)) =
c
c + N(s, a)
, where N(s, a) is the number of times a set of state and act ion has
been visited and c 2 R
+ is a positive constant. The TD-learning update step presented in equation 15.7 defines
the behavior of a passive agent, that is, an agent, who is only
evaluating the state-utility function for a predefined policy ⇡. In
other words, in its current form, TD-learning is suited to estimatej.-a. goulet 254
U(s) given that we already know the policy ⇡(s) we want the agent
to follow. In practice, this is seldom the case because instead, we
want an active agent, who will learn both the expected utilities and
the optimal policy. The ne xt se c tion prese nts temporal di↵erence
Q-learning, a common active reinforcement learning method capable
of doing both tasks simultaneously.
15.2.2 Temporal Di↵erence Q-Learning
Temporal di↵erence Q-learning
7 (often referred to as simply Q- 7Watkins, C. J. and P. Dayan (1992). Q-learning. Machine Learning 8 (3–4), 279–292
learning) is an active reinforcement learning method that consists
in evaluat ing the action-utility function Q(s, a), from which both
the optimal policy
⇡
⇤
(s) = arg max
a2A
Q(s, a)
and the resulting expected utilities
U(s) ⌘ U(s,⇡
⇤
) = max
a2A Q(s, a)
can be extracted. Analogously to equation 15.7, the TD update step
for the action-utility function Q(s, a) is defined as
Q
(i+1)(s, a) Q
(i)(s, a)+↵

r(s, a, s
0)+ max
a0 2A Q
(i)(s
0, a
0)Q
(i)(s, a)
. (15.8)
Equation 15.8 describes the behavior of an active agent, which
updates Q(s, a) by looking one step ahead and employing the action
a
0 that maximizes Q(s
0, a
0). Q-learning is categorized as an o↵- policy reinforcement learning method because the optimal one-step
look-ahead action a
0 employed to update Q(s, a) is not necessarily
the action that will be taken by the age nt to transit to the ne xt
state. This is because in order to learn efficiently, it is necessary
to find a trad eo↵ between exploiting the curre nt best policy and
exploring new policies. While learning the action-utility function using Q-learning, one
cannot only rely on the currently optimal action a
⇤ = ⇡
⇤
(s) in order
to transit between successive stat es, s and s
0. This is because such
a greedy agent that always selects the optimal action is likely to
get stuck in a local maxima because of the poor initial estimates
for Q(s, a). One simple solution is to employ the ✏ -greedy strategy, which consists in selecting the action randomly with probability
✏

N(s)

and otherwise selecting the action from the optimal policy
⇡
⇤
(s). Here, N(s) is again the number of times a state has beenprobabilistic machine learning for civil engineers 255
visited. Therefore, an ✏-greedy agent selects an action following
a = (
ai
: p(a) = 1
A
, 8a, if u < ✏N(s) Random action
arg max
a2A
Q(s, a), if u  ✏N(s) Greedy action, where u is a sample taken from a uniform distribution U(0, 1).
✏N(s)
should be defined so that it ten ds to zero when the number
of times a state has been visited N(s ) ! 1. Algorithm 12 details
the sequence of ste ps for temporal di↵erence Q-learning. Algorithm 12: Temporal di↵erence Q-learning
1 define: s 2 S (states)
2 define: a 2 A (actions)
3 define: r(s, a, s
0) (rewards)
4 define:  (discount factor)
5 define: ✏ (epsilon-greedy function)
6 define: ↵ (learning rate function)
7 initialize: Q(0)(s, a) (action-utility function)
8 initialize: N(s, a) = 0, 8{s 2 S, a 2 A} (action-state counts)
9 i = 0
10 for episodes e 2 {1 : E} do
11 initialize: s
12 for time t 2 {1 : T} do
13 u : U ⇠ U(0, 1)
14 a = 8
<
:
ai : p(a) = 1
A
, 8a, if u < ✏N(s) Random
arg max
a2A
Q(i)(s, a), if u  ✏N(s) Greedy
15 observe: r(s, a, s
0), s
0 16 Q(i+1)(s, a) Q(i)(s, a)· · · 17 +↵
N(s, a)
r(s, a, s
0)+ max
a02A Q(i) (s
0, a
0)Q(i) (s, a)
18 N(s, a) = N(s, a) + 1
19 s = s
0 20 i = i + 1
21 return: U(s) = max
a2A Q(i)(s, a)
22 ⇡
⇤
(s) = arg max
a2A
Q(i)(s, a)
Example: Infrastructure maintenance planning We revisit the
infrastructure maintenance example, this time using the temporal
di↵erence Q-learning algorithm. The problem definition is identical
to that in §15.1, except that we now initialize the action-utilityj.-a. goulet 256
functions as Q(0)(s, a) = $0 M, 8{s 2 S, a 2 A}. Also, we define the
iteration-dependent learning schedule as
↵
N(s, a) =
c
c + N(s, a)
and the exploration schedule by
✏N(s) =
c
c + N(s)
, where c = 70. We choose to learn over a total of 500 episodes, each con sisting in 100 time ste ps. For each new episode, the state
is initialized randomly. Figure 15.7 presents the evolution of the
expected utility U(s) for each stat e as a funct ion of the number of
episodes. Given that we employed 500 episodes, each comprising
100 200 300 400 500
0
1000
2000
3000
4000
Episode #
U( )s
s=100% s=80% s=60% s=40% s =20% s=0% Figure 15.7: Expected utility convergence
for the Q-learning algorithm.
100 ste ps, the tot al number of iterations is 50,000. For the last
iteration, the expected utilities are
U
(50 000)(S) = {3640, 3635, 3630, 3615, 3592, 3510} $M. Those values are identical to the one s obt aine d using either the
policy- or value-iteration algorithm in §15.1. Analogously, the
optimal policy derived from the action-utility function is also
identical and corresponds to
⇡
⇤
(S) = {do nothing
| {z }
s=100%
, maintain
| {z }
80%
, maintain
| {z }
60%
, maintain
| {z }
40%
,replace | {z }
20%
,replace | {z }
0%
}.
100 200 300 400 500
0
0.2
0.4
0.6
0.8
1
Episode #
✏
Figure 15.8: Evolution of ✏ N(s)
for each
state s as a function of the number of episodes.
Figure 15.8 present ✏N(s) as a funct ion of the number of
episodes, that is, for each state, the prob ability that the agent
selects a random action. For the learning rate ↵
N(s, a)
, which is
not displayed here, there are three times as many curves because it
also depends on the ac tion a. For this simple infrastructure maintenance example, using an
active reinforcement learning method such as Q-learning is notprobabilistic machine learning for civil engineers 257
beneficial in com parison with passive meth ods such as the policy￾or value-ite ration algorithm in §15.1. Nevertheless, active reinforce￾ment learning becomes necessary for more advanced contexts where
the number of states is large and often c ontinuous. The re ade r
interested in learning more about topics related to on-policy rein￾forcement learning methods such as sarsa, advanced explorat ion
strategies, efficient action-utility function initialization, as well as
continuous function approximation for Q-le arning, should con sult
dedicated textbooks such as the one by Sutton and Barto.8
8 Sutton, R. S. and A. G. Barto (2018). Reinforcement learning : An introduction
(2nd ed.). MIT Press
