Information Science and Statistics
Günther Palm
Novelty, 
Information 
and Surprise
Second EditionInformation Science and Statistics
Series Editors
M.I. Jordan, Department of Computer Science, University of California, Berkeley,
CA, USA
Robert Nowak, Department of Electrical and Computer En, University of
Wisconsin-Madison, Madison, WI, USA
Bernhard Schoelkopf, Empirische Inferenz, Max Planck Inst Intelligent System,
Tübingen, GermanyThe fields of computer science, communications, probability and statistics have
become increasingly intertwined. Two significant trends characterize this develop￾ment: algorithmic issues arising throughout probability and statistics, and proba￾bilistic methods playing increasingly important roles in the design and analysis of
information systems.
This series will feature graduate textbooks and advanced monographs that
involve the integration of algorithmic and probabilistic or statistical aspects. The
series will include books on fundamental theoretical issues and more applied books
that present new algorithms and architectures of general interest. Suitable topics
include randomized algorithms and combinatorics, graphical models, machine
learning, algorithmic game theory, source coding and error control coding, network
information theory, simulation, rare events, distributed and robust optimization,
networking, information retrieval, information management, speech processing,
statistical natural language processing, computer vision, and robotics.Günther Palm
Novelty, Information
and Surprise
Second EditionGünther Palm
Neural Information Processing
University of Ulm
Ulm, Germany
ISSN 1613-9011 ISSN 2197-4128 (electronic)
Information Science and Statistics
ISBN 978-3-662-65874-1 ISBN 978-3-662-65875-8 (eBook)
https://doi.org/10.1007/978-3-662-65875-8
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microfilms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional affiliations.
This Springer imprint is published by the registered company Springer-Verlag GmbH, DE, part of
Springer Nature.
The registered company address is: Heidelberger Platz 3, 14197 Berlin, GermanyPersonal History of the Book
The idea of writing this book first occurred to me in 1984, when I received an
invitation to stay for a year in Berlin as a fellow of the “Wissenschaftskolleg.” Then
I thought I could use the freedom gained by the detachment from my daily duties
and the secretarial help offered by the Wissenschaftskolleg to produce a manuscript.
A few years before that, prompted by my attempts to understand an earlier article
by my friend and colleague, Charles Legendy (Legéndy (1975), see also Legéndy
and Salcman (1985) ), I had started to investigate a broader definition of information
(Palm 1981). Charles had tried to cast his ideas in brain theory in a framework of
information theory; the essential idea was that each neuron in the brain tries to get
as much information as possible from the activity of the other neurons and tries to
provide as much information as possible for them by its own activity. At that time
informational ideas for neuroscience were quite popular in Tübingen, possibly due
to the influence of Ernst Pfaffelhuber (Pfaffelhuber 1972). Certainly also my mentor
at the MPI in Tübingen, Valentino Braitenberg, often pointed out the importance
of information theory (e.g., Braitenberg 1977, 2011), and I needed the essentials
for my calculations of associative memory capacity (Palm 1980). It turned out that
Charles did not use exactly the usual concept of information as defined in Shannon’s
information theory, but rather something very similar, that I had called “surprise” in
my article. In this article, I already had defined most of the essential concepts of
the new theory. So when I started to work on the manuscript in Berlin, my problem
seemed to be not whether I would manage to finish it, but whether there was enough
material for a whole book.
During the time in Berlin I realized that the ideas that had to be developed
actually had an interesting but complicated kinship to the ideas that I had developed
in my thesis on topological and measure-theoretical entropy in ergodic theory
(see Palm 1975, 1976b). I also realized that they had a bearing on the classical
discussions related to physical entropy in thermodynamics and the direction of time
which are also related to optimizational ideas based on entropy (Jaynes 1957, 1982).
At that time, I started to read about Helmholtz and to look into some historical
papers by Carnap, Reichenbach, Brillouin, and others. Fortunately, Carl Hempel, a
vvi Personal History of the Book
well-known philosopher of science, was visiting at the same time as a fellow of the
“Wissenschaftskolleg” and could help guide my studies in this area.
The year went by very quickly and the manuscript had grown considerably, but
there were now more loose ends than before. Back at the Max Planck Institute in
Tübingen I managed to use part of my time to complete the first version of the
manuscript. I sent it to MIT Press and I was again lucky that the manuscript was
seen as potentially interesting although not yet publishable by the reviewers and
in particular by Harry Stanton, who encouraged me to keep on working on the
manuscript.
In spite of this encouragement, I did not find the time to work on the book for
a number of years. Instead I became more deeply involved in brain research and
work on methods in neuroscience due to a number of new personal contacts, in
particular to Ad Aertsen, Peter Johannesma, and George Gerstein, who came to
Tübingen to work in the creative atmosphere in Valentin Braitenberg’s group. I had
the possibility of discussing with them (among many other things) my statistical
ideas related to the concept of surprise and its possible use in neuroscience. This
led to the first appearance of the term surprise in some methodological papers on
spike train analysis (Palm et al. 1988, Aertsen et al. 1989) and in the widely used
multiunit analysis program by Ad Aertsen. Since that time some of my colleagues
(in particular Ad Aertsen and Moshe Abeles) have been pushing me to write these
ideas up properly.
After I had left Tübingen in 1988 to become a professor for theoretical brain
research at the University of Düsseldorf, I started to use the concepts of description
and novelty regularly in the teaching of courses on information theory, first in
Düsseldorf and later (after 1991) in Ulm where I became the director of an institute
for neural information processing in the computer science department. During the
brief period in Düsseldorf, one of my friends from student days in Tübingen, Laura
Martignon, joined our group and started to take up work on the book again. She put
some of her experience in teaching into the manuscript and helped to make some
parts more readable. Later, in Ulm, she also taught a course on the subject. Together
we submitted the book again to MIT Press and were again encouraged to complete
the manuscript. The book seemed to be almost ready for the second time. However,
in the next years, we both found no time to continue working on it, although since
that time I am using it regularly in the teaching of courses on information theory.
Only in the summer of 1997, I did find some time again to work on the book. I
partially reorganized it again and isolated the few missing pieces, many of them in
the exercises. Fortunately, I found a very talented student, Andreas Knoblauch, who
had taken part in one of the courses on information theory and was willing to work
on the solutions of many of the exercises. Some peace to put it all together and finish
most of the remaining work on the manuscript was provided during my sabbatical
semester toward the end of 1998 by Robert Miller at the University of Dunedin in
New Zealand.
Unfortunately, the book was still not completely finished by the end of the
millennium. Meanwhile, I had more academic and administrative duties as chairman
of a collaborative research center and as dean of the computer science departmentPersonal History of the Book vii
in Ulm. So it was only toward the end of 2006 when I could take up the book project
again. This time I added more motivating examples to the text. And again I found a
talented student, Stefan Menz, who integrated everything into a neat LATEX-version
of the text. During the last years, we also had the opportunity to pursue some of the
new ideas and applications of information theory with excellent PhD students in an
interdisciplinary school on “Evolution, Information and Complexity,” see Arendt &
Schleich (2009).
This book would never have been finished without the help, encouragement, and
inspiration from all the people I mentioned and also from many others whom I did
not mention. I would like to thank them all!
References
Legéndy, C. R. (1975). Three principles of brain function and structure. Interna￾tional Journal of Neuroscience, 6, 237–254.
Legéndy, C. R., & Salcman, M. (1985). Bursts and recurrences of bursts in the spike
trains of spontaneously active striate cortex neurons. Journal of Neurophysiology,
53(4), 926–939.
Palm, G. (1981). Evidence, information and surprise. Biological Cybernetics, 42(1),
57–68.
Pfaffelhuber, E. (1972). Learning and information theory. International Journal of
Neuroscience, 3, 83.
Braitenberg, V. (1977). On the texture of brain. Springer.
Braitenberg, V. (2011). Information—der Geist in der Natur. Schatthauer.
Palm, G. (1980). On associative memory. Biological Cybernetics, 36, 167–183.
Palm, G. (1975). Entropie und Generatoren in dynamischen Verbänden. PhD Thesis,
Tübingen.
Palm, G. (1976b). Entropie und Erzeuer in dynamischen Verbänden. Zeitschrift für
Wahrscheinlichkeitstheorie und Verwandte Gebiete, 36, 27–45.
Jaynes, E. T. (1957). Information theory and statistical mechanics. Physical Review,
106(4), 620–630.
Jaynes, E. T. (1982). On the rationale of maximum entropy methods. Proceedings
IEEE, 70, 939–952.
Palm, G., Aertsen, A. M. H. J., & Gerstein, G. L. (1988). On the significance of
correlations among neuronal spike trains. Biological Cybernetics, 59(1), 1–11.
Aertsen, A. M. H. J., Gerstein, G. L., Habib, M. K., & Palm, G. (1989). Dynamics
of neuronal firing correlation: modulation of “effective connectivity”. Journal of
Neurophysiology, 61(5), 900–917.
Arendt, W., & Schleich, W. P. (Eds.) (2009). Mathematical analysis of evolution,
information, and complexity. Wiley.Preface to the Second Edition
The first edition contained a number of typographical errors and small inconsisten￾cies, some of which I found myself while others were pointed out to me by eager
readers. These have been corrected. I went through the exercises again, removed
or added some, and modified several of them to make them easier to understand
and sometimes also to do. I have only made a few more substantial changes in
Chapters 11, 12, and 13, where I added further examples (in Sects. 12.4, 13.3.1, and
14.1) and some more specific calculations (in Sects. 14.1 and 14.3) that may help
to relate my ideas to more classical statistical approaches. I am grateful to Junji
Ito who prepared three new figures for Chap. 12 from previous joint work. I also
added a new appendix on similarity theory which is of interest in itself and may also
be useful in the understanding of mutual information and the so-called Kullback￾Leibler distance. The appendix is based on material from a course I gave at the
University of Ulm in the winter term of 2004/05 and from the PhD thesis of Heiner
Markert. For this reason, it is coauthored with him. We are both happy to be able to
publish this material together in this book.
ixContents
1 Introduction ................................................................. 1
Organization of the Book ................................................... 4
Philosophy of the Book ..................................................... 4
References ................................................................... 8
Part I Surprise and Information of Descriptions
2 Prerequisites from Logic and Probability Theory ...................... 13
2.1 Logic and Probability of Propositions .............................. 13
2.2 Mappings, Functions, and Random Variables ...................... 15
2.3 Measurability, Random Variables, and Expectation Value ........ 18
2.4 Technical Comments ................................................ 20
References ................................................................... 20
3 Improbability and Novelty of Descriptions .............................. 21
3.1 Introductory Examples .............................................. 21
3.2 Definition and Properties ............................................ 24
3.3 Descriptions .......................................................... 26
3.4 Properties of Descriptions ........................................... 29
3.5 Information and Surprise of Descriptions .......................... 35
3.6 Information and Surprise of a Random Variable ................... 41
3.7 Technical Comments ................................................ 42
3.8 Exercises ............................................................. 43
References ................................................................... 45
4 Conditional and Subjective Novelty and Information .................. 47
4.1 Introductory Examples .............................................. 47
4.2 Subjective Novelty ................................................... 48
4.3 Conditional Novelty ................................................. 50
4.4 Information Theory for Random Variables ......................... 55
xixii Contents
4.5 Technical Comments ................................................ 57
4.6 Exercises ............................................................. 58
References ................................................................... 60
Part II Coding and Information Transmission
5 On Guessing and Coding .................................................. 63
5.1 Introductory Examples .............................................. 63
5.2 Guessing Strategies .................................................. 65
5.3 Codes and Their Relation to Guessing Strategies .................. 66
5.4 Kraft’s Theorem ..................................................... 68
5.5 Huffman Codes ...................................................... 69
5.6 Relation Between Codeword Length and Information ............. 70
5.7 Technical Comments ................................................ 72
5.8 Exercises ............................................................. 72
References ................................................................... 74
6 Information Transmission ................................................. 75
6.1 Introductory Examples .............................................. 75
6.2 Transition Probability ............................................... 77
6.3 Transmission of Information Across Simple Channels ............ 79
6.4 Technical Comments ................................................ 84
6.5 Exercises ............................................................. 84
References ................................................................... 87
Part III Information Rate and Channel Capacity
7 Stationary Processes and Their Information Rate ...................... 91
7.1 Introductory Examples .............................................. 91
7.2 Definition and Properties of Stochastic Processes ................. 93
7.3 The Weak Law of Large Numbers .................................. 95
7.4 Information Rate of Stationary Processes .......................... 96
7.5 Transinformation Rate ............................................... 99
7.6 Asymptotic Equipartition Property ................................. 100
7.7 Technical Comments ................................................ 102
7.8 Exercises ............................................................. 102
References ................................................................... 103
8 Channel Capacity ........................................................... 105
8.1 Information Channels ............................................... 105
8.2 Memory and Anticipation ........................................... 106
8.3 Channel Capacity .................................................... 107
8.4 Technical Comments ................................................ 110
8.5 Exercises ............................................................. 110
References ................................................................... 111Contents xiii
9 How to Transmit Information Reliably with Unreliable
Elements (Shannon’s Theorem)........................................... 113
9.1 The Problem of Adapting a Source to a Channel .................. 113
9.2 Shannon’s Theorem ................................................. 114
9.3 Technical Comments ................................................ 117
9.4 Exercises ............................................................. 117
References ................................................................... 117
Part IV Repertoires and Covers
10 Repertoires and Descriptions.............................................. 121
10.1 Introductory Examples .............................................. 122
10.2 Repertoires and Their Relation to Descriptions .................... 125
10.3 Tight Repertoires .................................................... 131
10.4 Narrow and Shallow Covers ........................................ 133
10.5 Technical Comments ................................................ 135
10.6 Exercises ............................................................. 136
References ................................................................... 137
11 Novelty, Information, and Surprise of Repertoires ..................... 139
11.1 Introductory Examples .............................................. 139
11.2 Definitions and Properties ........................................... 141
11.3 Finding Descriptions with Minimal Information ................... 150
11.4 Technical Comments ................................................ 156
11.5 Exercises ............................................................. 156
References ................................................................... 157
12 Conditioning, Mutual Information, and Information Gain............ 159
12.1 Introductory Examples .............................................. 159
12.2 Conditional Information and Novelty .............................. 160
12.3 Mutual Novelty and Transinformation ............................. 163
12.4 Information Gain, Novelty Gain, and Surprise Loss ............... 164
12.5 Conditional Information of Continuous
Random Variables ................................................... 172
12.6 Technical Comments ................................................ 174
12.7 Applications in Pattern Recognition, Machine Learning,
and Life Science ..................................................... 175
12.8 Exercises ............................................................. 176
References ................................................................... 177
Part V Information, Novelty and Surprise in Science
13 Information, Novelty, and Surprise in Brain Theory ................... 181
13.1 Understanding Brains in Terms of Processing
and Transmission of Information ................................... 181
13.2 Neural Repertoires ................................................... 186xiv Contents
13.3 Experimental Repertoires in Neuroscience ......................... 187
13.3.1 The Burst Repertoire ....................................... 188
13.3.2 The Pause Repertoire ...................................... 192
13.3.3 The Coincidence Repertoire ............................... 192
13.3.4 The Depolarization Repertoire ............................ 195
13.4 Neural Population Repertoires: Semantics and Syntax ............ 195
13.5 Conclusion ........................................................... 197
13.6 Technical Comments ................................................ 198
13.6.1 Coincidence ................................................ 200
13.6.2 Coincidental Patterns ...................................... 200
13.6.3 Spatiotemporal Patterns ................................... 200
References ................................................................... 202
14 Surprise from Repetitions and Combination of Surprises ............. 211
14.1 Combination of Surprises ........................................... 212
14.2 Surprise of Repetitions .............................................. 214
14.3 Surprise of Repetitions of Patterns .................................. 216
14.4 Technical Comments ................................................ 218
References ................................................................... 218
15 Entropy in Physics.......................................................... 221
15.1 Classical Entropy .................................................... 221
15.2 Modern Entropies and the Second Law ............................ 224
15.3 The Second Law in Terms of Information Gain .................... 227
15.4 Technical Comments ................................................ 230
References ................................................................... 230
Part VI Generalized Information Theory
16 Order- and Lattice-Structures ............................................ 235
16.1 Definitions and Properties ........................................... 235
16.2 The Lattice D of Descriptions ...................................... 242
16.3 Technical Comments ................................................ 243
Reference .................................................................... 243
17 Three Orderings on Repertoires ......................................... 245
17.1 Definition and Basic Properties ..................................... 245
17.2 Equivalence Relations Defined by the Orderings .................. 248
17.3 The Joins and Meets for the Orderings ............................. 250
17.4 The Orderings on Templates and Flat Covers ...................... 255
17.5 Technical Comments ................................................ 257
17.6 Exercises ............................................................. 257
References ................................................................... 257
18 Information Theory on Lattices of Covers............................... 259
18.1 The Lattice C of Covers ............................................. 259
18.2 The Lattice Ff of Finite Flat Covers ............................... 261Contents xv
18.3 The Lattice R of (Clean) Repertoires ............................... 262
18.4 The Lattice T of Templates ......................................... 263
18.5 The Lattice P of Partitions .......................................... 265
18.6 Technical Comments ................................................ 265
18.7 Exercises ............................................................. 266
References ................................................................... 266
A Fuzzy Repertoires and Descriptions...................................... 267
A.1 Basic Definitions .................................................... 268
A.2 Definition and Properties of Fuzzy Repertoires .................... 270
Reference .................................................................... 273
B Similarity Theory ........................................................... 275
B.1 Definitions and Elementary Observations .......................... 275
B.2 Homomorphisms Between Weak-Metric Spaces .................. 280
References ................................................................... 288
Index ............................................................................... 289List of Symbols
Notation Description.
A,B Propositions.
E Expectation value.
H Entropy.
I Information rate / Information as random variable.
Nd (ω) The novelty provided by ω for the description d.
P,Q Transition Probabilites.
R Range of a function.
Sd (ω) Surprise (of an outcome ω) of d.
T Transinformation rate.
X, Y, Z Random variables.
B Borel σ-algebra.
C Set of all covers.
C Set of complex numbers.
F Set of flat covers.
G and Gpq (d) Novelty gain.
I Information.
IG Information gain.
Z Set of integer numbers.
L Average length of a code.
M Mutual novelty.
L Number of questions in a guessing strategy.
N Set of natural numbers.
N Average novelty.
Npq (d) Subjective novelty.
N Novelty as a random variable.
P Set of partitions.
Q Set of rational numbers.
R Set of real numbers.
R Set of repertoires.
S Surprise.
xviixviii List of Symbols
SL Surprise loss.
S Surprise as a random variable.
T Set of tight covers or templates.
T Transinformation.
Var Variance.
α, β, γ , δ Letters for covers and repertoires.
e¯ Average error of a Transition Probability.
c The capacity of a channel. Also letter for a code.
C A channel.
 The completion of a description, e.g., d˜.
X,Y, Q Stochastic proccesses.
The direction of a description, e.g., d.
d, b, c Descriptions.
d∩ The tightening of a description d.
e Error probability of a Bayesian guess.
p, q Letters for probabilities.
(, 	, p) Probability space.
	 σ-algebra of propositions or events.
ω elementary event, element of .List of Figures
Fig. 1.1 Examples of covers (top) and the induced hierarchical
structures (bottom) .................................................. 3
Fig. 3.1 Model for descriptions of events .................................... 26
Fig. 3.2 Example of propositions about positions on a table ................ 28
Fig. 3.3 Plot of function h(p) ................................................ 36
Fig. 3.4 Plot of function I(p) ................................................ 39
Fig. 5.1 Example of a question strategy ..................................... 64
Fig. 5.2 Example of a question strategy ..................................... 64
Fig. 5.3 Tree picturing a question strategy .................................. 66
Fig. 6.1 An information channel ............................................. 78
Fig. 6.2 Channel example .................................................... 79
Fig. 6.3 Three different channels ............................................ 86
Fig. 7.1 Channel example ..................................................... 92
Fig. 8.1 Data transmission on a channel ..................................... 106
Fig. 8.2 Two channels ......................................................... 111
Fig. 8.3 Two simple channels ................................................ 111
Fig. 9.1 Channel and stationary process ...................................... 114
Fig. 10.1 Illustration of tightness .............................................. 128
Fig. 10.2 Illustration of cleanness. The vertically hatched region on
the left can be removed, because it is the union of the two
diagonally hatched regions .......................................... 130
Fig. 10.3 Illustration of the product of two repertoires ....................... 133
Fig. 10.4 The hierarchy for α = {A, B, C, D, E, F, G, H}
where  = {1, 2, 3, 4, 5, 6}, and A = {1, 2, 3, 4, 6},
B = {2, 4, 5, 6}, C = {1, 2, 3, 4}, D = {1, 6},
E = {4, 5, 6}, F = {3}, G = {4}, H = {5} ......................... 134
Fig. 10.5 Example for a shallow repertoire (a) and for a chain (b) ........... 135
Fig. 10.6 Illustration of classes of repertoires ................................. 136
Fig. 13.1 A single neuron ...................................................... 187
xixxx List of Figures
Fig. 13.2 Burst novelty as a function of time for two individual
neuron spike trains and a simulated Poisson spike train.
(a) Unstimulated neuron in a fly, (b) stimulated neuron in
cat LGN, (c) Geiger counter. The extremely high novelty
in (b) is due to fast non-stationary spike generation
caused by visual stimulation. Even in an unstimulated
neuron, (a) the average novelty is a bit larger than in the
Poisson process (c) .................................................. 189
Fig. 13.3 Burst novelty as a function of time for a neuron in
monkey motor cortex. The higher novelty toward the
end of the recording is related to a hand movement the
monkey was trained to perform (see Brochier et al., 2018) ....... 190
Fig. 13.4 Burst surprise as a function of burst novelty, “burst-N2S
curve” ................................................................ 190
Fig. 13.5 Burst novelty-to-surprise (N2S) curves for four
different gamma distributions (shape factor
κ = 0.5, 1.0, 1.25, 3.33). Modified from Ito et al., 2019 ........ 191
Fig. 13.6 Poisson novelty-to-surprise (N12S) curves for four
different gamma distributions. Modified from Ito et al.,
2019 .................................................................. 192
Fig. 17.1 The repertoires α, β, γ , δ = α ∪ β,  = β ∪ γ ,
ζ = γ ∪ α, η = α ∪ {}, κ = η ∪ β, and λ =  ∪ {}
on  = {1, 2, 3, 4, 5, 6} illustrate possible relationships
between the orderings ≤1, ≤2, and ≤3 (see text) .................. 246
Fig. 17.2 Repertoires illustrating that ≤2 is not a lattice. Both γ1
and γ2 are minimally larger than α and β. Thus, there is
no unique meet for ≤2 .............................................. 252Chapter 1
Introduction
Nowadays, there are many practical applications of information theory in fields like
pattern recognition, machine learning, and data mining (e.g., Deco and Obradovic
1996; MacKay 2005), used in particular in the life sciences (e.g., Bialek et al.
(2007); Herzel et al. (1994); Koepsell et al. (2009); Schmitt and Herzel (1997);
Taylor et al. (2007); Tkacik and Bialek ( ˇ 2007)), i.e., far beyond the classical appli￾cations in communication technology. However, Claude Shannon’s groundbreaking
original concept of information has remained essentially unchanged.
The main purpose of this book is to extend classical information theory to
incorporate the subjective element of interestingness, novelty, or surprise. These
concepts can only be defined relative to a person’s interests, intentions, or purposes,
and indeed classical information theory has often been criticized for not being able
to incorporate these ideas. Actually, classical information theory comes quite close
to this, when introducing the information contained in a proposition or statement A
(as − log2 p(A)). But, in everyday life, most commonly this information is not really
transferred if a person X tells statement A to another person Y , for example, because
Y may not be interested in A or because Y may rather be interested in the fact that it
is X who is telling A. An interesting extension of information theory could consider
the following question: If Y is interested in B instead of A and perhaps B largely
overlaps with A, then how much information does Y obtain from being told A?
This question and other similar ones will be answered in this book; they are not
totally alien to classical information theory. This means that our new theory does
not have to go far from it. In a technical sense, it can be regarded as only a slight (but
perhaps important) extension of Shannon’s definition of information from partitions
to covers.
This needs some explanation: Shannon tried to define his information as an
objective almost physical quantity (measured in bit). This led him to define infor￾mation for random variables X. For a discrete random variable X with finitely many
possible values x1, x2,...,xn, he defined I (X) = −
i p[X = xi] log2 p[X = xi],
i.e., as the average of − log2 p[X = xi] where the possible outcomes [X = xi] of
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_1
12 1 Introduction
X are now the propositions A of interest. Again, this definition presupposes that
we are equally interested in all outcomes xi of the random variable. This may not
always be the case. For example, if we consider a roulette game represented by a
random variable X with possible values in {0, 1, 2,..., 36}, then a person who has
put money on the number 13, the row 31, 32, 33, and the odd numbers, certainly
wants to know the outcome number, but he will be more interested in the numbers
mentioned above. The new concept of novelty defined here takes care of this aspect
of information which goes slightly beyond classical information theory.
Shannon’s definition uses the statements [X = xi]. In classical probability
theory, these are called “events” and modeled as subsets of a universal set  of
all possible events. These sets actually form a partition of , meaning that they are
mutually exclusive and cover all of . A cover is a more general set of “events” than
a partition, where mutual exclusiveness is not required. My idea, the foundation
of this theory, is to use a cover (instead of a partition) to model the set of all
propositions or statements a person is interested in and to define information for
such covers.
Starting from a definition of information on partitions, the step to covers appears
as a rather straightforward generalization, namely omitting the requirement of
mutual exclusiveness or disjointness of the propositions. Technically, partitions
have a number of very useful properties which seem to be necessary to prove
even the most elementary theorems of information theory (e.g., the monotonicity
and subadditivity of information). So the main body of this book is devoted to the
development of a workable extension of classical information theory to (possibly)
overlapping sets of propositions called repertoires or covers, which is the theory of
novelty and which coincides with classical information theory when the covers are
partitions.
This move from partitions to covers allows to turn our attention to the rich
logical structures that are possible between the propositions in arbitrary covers.
In general, these structures can be described most adequately as hierarchies (see
Fig. 1.1). This turn to the possible logical structures that may underlie definitions
of novelty, information, or even physical entropy provides a new perspective
for the interpretation of these concepts, for example, in thermodynamics and in
neuroscience. This may be more a philosophical point, but it can have practical
implications (and it was one of my strongest motivations to write this book). More
about it can be found in Part V.
When we consider arbitrary covers α = {A1,...,An} of a probability space 
instead of partitions, it actually makes sense to distinguish different information￾like concepts, which we call information, novelty, and surprise. In a nutshell, the
information of α is the minimum average number of yes–no questions that we need
to determine for every ω ∈  one element A ∈ α that describes it (i.e., ω ∈ A)—the
information needed for α.
The novelty of α is the average maximum information I (A) = − log2 p(A) that
we can obtain for ω ∈  from an A ∈ α that describes it—the novelty obtained
from α. This means the novelty for ω is N(ω) = max{−log2p(A): ω ∈ A ∈ α}.1 Introduction 3
A1
A3
A1 A2
A3
A1 A2 A3 A4
A4
A4
A2
A C B
Ω
Ω
A B
A ∩ B
A ∩ B
C
C2 C3
C1
C2
C3
Ω C1
Ω
Fig. 1.1 Examples of covers (top) and the induced hierarchical structures (bottom)
The surprise of α has a more statistical flavor: if we have a very rich cover α, in
the extreme case α may contain all singletons {ω} (ω ∈ ), then we always obtain a
lot of novelty, but this is not really surprising. So we say that we get really surprised
when observing ω ∈  through α, when N(ω) is large compared to other values
N(ω
) (ω = ω). This leads to the definition of surprise as
S(ω) = − log2 p[N ≥ N(ω)]=− log2 p({ω ∈ : N(ω
) ≥ N(ω)}).
Besides introducing the new concepts of novelty and surprise, this book also
extends classical Shannon information. Therefore, one may hope that this new more
general treatment of information can be used to solve some problems that could
not be solved with classical information theory. This is indeed the case when we
consider problems which need overlapping propositions for their formulation. Let
me give one example for this here, which is treated more extensively in Chap. 11 and
which appears rather innocent on the surface. You are observing a shell game where
there are eight shells on the table and there is a coin (say 1 e) under two of them.
How much information do you get, when somebody points to one of the shells and
tells you that it contains a coin? How would you devise an optimal guessing strategy
that determines the position of one of the two coins, and how many questions do you
need on average?
You cannot answer these questions properly only with classical information
theory, and the correct answers are given in Chap. 11.4 1 Introduction
Organization of the Book
The book is organized into six parts.
Part I Introduces the basic concepts on an elementary level. It contains a brief
introduction to probability theory and the new concept of a description, which
maps  into 	 for a probability space (, 	, p), i.e., it associates with every
elementary event ω ∈  a proposition A ∈ 	 describing it. Already on the level
of descriptions, we can distinguish information, novelty, and surprise.
Part II Recapitulates classical coding theory. It is not essential for the new concepts
developed in this book, but for students of information theory it is necessary to
understand the practical meaning of information and the related notions defined
here.
Part III Introduces some more background on stochastic processes which is neces￾sary to prove Shannon’s classical theorem, the backbone of information theory.
The material in this part is not new, but in my opinion some proofs become a
bit easier due to the new concept of description.
Part IV Contains the core ideas of this book. It defines various structures on the set
of all covers and motivates their introduction by various practical examples. It
also contains the definitions of information, novelty, and surprise for covers or
repertoires and shows how these numbers can be calculated in practice.
Part V Shows some applications of our new and more general view of information
theory in neuroscience, brain theory, and the physics of entropy.
Part VI Concentrates on the mathematical structures on which the new generalized
information theory is built. It harvests and combines the mathematical results
obtained in previous parts (mainly in Part IV). It defines six new and interesting
lattices of covers which could become the subject of further mathematical
investigations.
This book contains a mathematical theory of information, novelty, and surprise.
It should, however, be readable for everyone with a basic mathematical background
(as given in the first year of most scientific curricula) and the patience and stamina
necessary to follow through a mathematical kind of exposition and to do at least
some of the exercises.
Philosophy of the Book
The book provides a brief and comprehensive exposition of classical information
theory, from a slightly unusual point of view, together with the development of a new
theory of novelty and surprise. These new concepts are defined together with the
concept of information as a complementary companion. The word surprise invokes
many mostly subjective notions, and this subjectivity is brought in on purpose to
complement the seemingly more “objective” notion of information.Philosophy of the Book 5
The “subjective vs. objective” debate has a long history in the field of probability
theory and statistics. There it focuses on the question of the nature or the origin
of probability, e.g., de Finetti (1974); Jeffreys (1939); Jeffrey (1992, 2004);
Kerridge (1961); Keynes (1921). This discussion can be and has been carried over
to information theory, but this is not the purpose of this book. Instead I have
discovered an additional source of subjectivity in probability and information theory
that is perhaps less relevant in probability but has a definite impact on information
theory once it is brought into the focus of attention. Classical information theory
tried to determine the amount of information contained in a message (as a real
number measured in bits). To do this it has to presuppose that the message is
exchanged between two agents (that are normally assumed to be people, not
animals, computers, or brain regions), who have already agreed on a common
language for the expression of the message. This approach makes it possible to
develop information theory in a discrete framework, dealing mainly with a finite
alphabet from which the messages are composed. Thus classical information theory
does not consider the process of perception or of formation of the messages. It starts
where this process has ended.
I believe and I will actually show in this book that information theory needs
only a slight modification to include this process of perception and formation of a
message. This process is captured in the definition of a description which connects
every event ω that can happen, with a proposition d(ω) about it. When we describe
actual events that have happened, we will normally be unable to give an exact
account of them, let alone of the total state of the world around us. We are restricted
both by our senses and by our language: we cannot sense everything that there is and
we cannot express everything that we sense (at least not exactly). So the description
d(x) that we give about x will usually be true not only for x but also for other events
y which are somewhat similar to x.
Now it is quite clear that classical information theory does not deal with events
x that happen, but rather with descriptions of these events or propositions about
those events. Unfortunately, this simple fact is obscured by the usual parlance in
probability theory and statistics where a proposition is called an “event” and an
event is called an “elementary event.” This leads to such strange phrases as “ is the
certain event” and “∅ is the impossible event.” What is meant is that  is the trivial
proposition, which is true because it says nothing about the event x, and that ∅ is
never true because it is a self-contradictory statement (like “A and not A”) about the
event x. Due to this strange use of the language (which is usually carried over from
probability theory to information theory), it may easily happen that this humanoid
or language-related aspect of the concept of information is forgotten in practical
applications of information theory. This is harmless when information theory is
used to optimize telephone lines, but it may become problematic when information
theory is applied in the natural sciences. This had already happened in the nineteenth
century when the term entropy was introduced in statistical mechanics to explain the
second law of thermodynamics, and it is happening again today when information
theory is used in theoretical neuroscience or in genetics, e.g., Küppers (1986). One
problem is that in such applications the concept of information may be taken to6 1 Introduction
be more “objective” than it actually is. This argument eventually has led me and
also others to some rather subtle and probably controversial criticisms of these
applications of information theory, e.g. Bar-Hillel and Carnap (1953); Knoblauch
and Palm (2004); Palm (1985, 1996), which really were not the main motive for
writing this book and which are neither part of the theory of novelty and surprise
nor its most interesting applications. In a nutshell, the situation can be described
as follows. Classical Shannon information theory requires an agreement between
the sender and the receiver of the messages whose information content has to be
determined. So information travels from a human sender to a human receiver, and
the agreement concerns the code, i.e., the propositional meaning of the symbols
that constitute the message; this implies that both sender and receiver use the
same description of events. The slightly broader information theory developed here
includes the situation where information about a purely physical observation is
extracted by human observation, so only the receiver needs to be a human. In many
scientific and everyday uses of information terminology, however, neither the sender
nor the receiver is human. And the problem is not merely that instead of the human
we have some other intelligent or intentional being, for example, an alien, a monkey,
a chess-computer, or a frog. Information terminology is also used for completely
“mechanical” situations. For example, a cable in my car carries the information that
I have set the indicator to turn left to the corresponding lights. Or a cable transmits
visual information from a camera to the computer of my home intrusion-warning
system.
In biology and in particular in neuroscience, this common use of information
terminology may interfere in strange ways with our ontological prejudices, for
example, concerning the consciousness of animals, because on the one hand
information terminology is handy, but on the other hand we don’t want to imply
that the receiver of the information has the corresponding properties comparable to
a human. For example, we may want to quantify the amount of visual information
the optic nerve sends to the brain of a frog (Atick 1992; Letvin et al. 1959), without
assuming that the frog (let alone its brain) has made some agreement with its
eyes about the meaning of the signals that are sent through the nerve. Similarly,
we can easily classify the maximal amount of information that can be expressed
by our genes, but we get into much deeper waters when we try to estimate how
much information is actually transmitted, and who is the sender and who is the
receiver. Does father Drosophila transmit some information (e.g., about how to
behave) to his son by his genes? Or is it somehow the whole process of evolution
that produces information (Küppers 1986, see also Taylor et al. 2007) and who is
the receiver of this information? The usual way out of this dilemma is to avoid
the question who might be the sender and who might be the receiver altogether.
In the technical examples eventually both are always humans, anyway. In the case
of information transmission by genes, neurons, or the optic nerve one can argue
that we are just interested in the physical properties of the “device” that limit the
amount of transmittable information, i.e., the channel capacity. In all these three
cases actually, the channel capacity has been estimated shortly after its definition by
Shannon (1948). In the case of the genome, the capacity is simply twice the numberPhilosophy of the Book 7
of base pairs of the DNA molecule. But this leads to the question how much of this
capacity is actually used and, invariably in biology, to the suspicion that it is much
less than the capacity. So it is found out that most of the genetic information is “not
used” or “not coding,” or that most of the visual information available from our optic
nerve is not “consciously perceived,” neither by us nor by our experimental animals.
Somehow people don’t easily accept such statements perhaps because they seem to
conflict with the idea that animals are well designed.
Perhaps these observations again reflect the original “dilemma of the receiver”
which we tried to circumvent with the rather safe capacity argument. My position
is that, at least in neuroscience, the more subjective turn to information theory
presented here may help to alleviate this problem and to find better estimates of
transinformation (or rather mutual novelty) flows in neural systems that are in
between the two extremes of neurophysiolocigal channel capacity (e.g., MacKay
and McCulloch 1952) and transinformation from experimental stimuli to behavioral
responses (e.g., Borst and Theunissen 1999; Eckhorn et al. 1976) because received
novelty < transmitted information < information capacity. Chapter 13 contains
some ideas and first results in this direction.
The new concepts of novelty and surprise may also help to capture some puzzling
aspects of everyday use of information that are hard to understand in terms of clas￾sical information theory. In many expositions of mathematical information theory, it
is obvious from the beginning that information in fact deals with propositions. This
is the case when information is primarily defined for partitions, i.e., for complete
sets of mutually exclusive propositions (about the event ω). Although I sympathize
with this approach, I believe that this way of defining information is still too narrow,
because it does not allow for mutually overlapping propositions. Indeed, I think that
in everyday life we usually work with partially overlapping concepts which cannot
be sharply separated from each other, or even with hierarchies of concepts which
contain each other. Why should it not also be possible to allow such propositions in
information theory?
Such a more general approach makes it possible to understand the use of the term
novelty or surprise in everyday language where it seems to be unjustified from the
point of view of statistics or classical information theory. For example, if in the state
lottery the numbers (1, 2, 3, 4, 5, 6) were drawn we would be much more surprised
than by the numbers (5, 11, 19, 26, 34, 41), although, of course, both sequences
should have exactly the same small chance of being drawn. The reason for our
surprise in the first case seems to be that this sequence can be exactly described
in a very simple way: it consists of the first six numbers. On the other hand, there
seems to be no simple description for the second sequence. To remember it one
really would have to memorize all the six numbers. Now it is much more probable
to obtain a sequence of numbers in the lottery that does not admit a simple exact
description than to obtain a sequence like (1, 2, 3, 4, 5, 6) that does. In the special
case of (1, 2, 3, 4, 5, 6), we could argue for example that there are only two such
extremely simple sequences, namely the last 6 and the first 6 numbers. Of course,
there are various degrees of simplicity and our surprise will vary accordingly. For
example, the sequence (22, 23, 24, 25, 26, 27) is simple because it is a sequence of8 1 Introduction
consecutive numbers. Therefore, it is also very surprising, but less surprising than
(1, 2, 3, 4, 5, 6) because there are 44 such sequences including the two very simple
ones. A mathematician may even find the sequence (5, 11, 19, 26, 34, 41) a little
surprising, because it contains 4 prime numbers. But since there are many possible
sequences containing 4 prime numbers (many more than 44, but much less than all
possible sequences), his surprise will certainly be not as large as for the sequence
(22, 23, 24, 25, 26, 27).
But if we combine all these different reasons for finding surprise, we may
eventually find something surprising in almost every sequence. In this case, it would
seem naive to add all surprises we can get for a given sequence from various
considerations, rather one would believe that the “real” surprise provided by any
concrete sequence becomes less when everything is considered as surprising. This
obviously confused wording leads to the distinction between novelty and surprise
that is also made in this book.
References
Atick, J. J. (1992). Could information theory provide an ecological theory of sensory processing?
Network: Computation in Neural Systems, 3, 213–251.
Bar-Hillel, Y., & Carnap, R. (1953). Semantic information. In London information theory
symposium (pp. 503–512). Academic.
Bialek, W., de Ruyter van Steveninck, R. R., & Tishby, N. (2007). Efficient representation as a
design principle for neural coding and computation. Neural Computation, 19(9), 2387–2432.
Borst, A., & Theunissen, F. E. (1999). Information theory and neural coding. Nature Neuroscience,
2(11), 947–957.
Deco, G., & Obradovic, D. (1996). An Information-theoretic approach to neural computing.
Springer.
de Finetti, B. (1974). Theory of Probability (Vol. 1). Wiley.
Eckhorn, R., Grüsser, O.-J., Kröller, J., Pellnitz, K., & Pöpel, B. (1976). Efficiency of different neu￾ronal codes: Information transfer calculations for three different neuronal systems. Biological
Cybernetics, 22(1), 49–60.
Herzel, H., Ebeling, W., & Schmitt, A. (1994). Entropies of biosequences: The role of repeats.
Physical Review E, 50(6), 5061–5071.
Jeffreys, H. (1939). Theory of probability. Oxford University Press.
Jeffrey, R. C. (1992). Probability and the art of judgment. Cambridge University Press.
Jeffrey, R. C. (2004). Subjective probability: The real thing. Cambridge University Press.
Kerridge, D. F. (1961). Inaccuracy and inference. Journal of the Royal Statistical Society. Series B
(Methodological), 23(1), 184–194.
Keynes, J. M. (1921). A treatise on probability. MacMillan.
Knoblauch, A., & Palm, G. (2004). What is Signal and What is Noise in the Brain? BioSystems,
79, 83–90.
Koepsell, K., Wang, X., Vaingankar, V., Wei, Y., Wang, Q., Rathbun, D. L., Usrey, W. M., Hirsch,
J. A., & Sommer, F. T. (2009). Retinal oscillations carry visual information to cortex. Frontiers
in Systems Neuroscience, 3, 1–18.
Küppers, B.-O. (1986). Der Ursprung biologischer Information—Zur Naturphilosophie der
Lebensentstehung. Piper.
Letvin, J. Y., Maturana, H. R., McCulloch, W. S., & Pitts, W. H. (1959). What the frog’s eye tells
the frog’s brain. Proceedings of the IRE, 47(11), 1940–1951.References 9
MacKay, D. J. C. (2005). Information theory, inference, and learning algorithms. Cambridge
University Press.
MacKay, D. M., & McCulloch, W. S. (1952). The limiting information capacity of a neuronal link.
Bulletin of Mathematical Biology, 14(2), 127–135.
Palm, G. (1985). Information und entropie. In H. Hesse (Ed.), Natur und Wissenschaft. Konkurs￾buch Tübingen.
Palm, G. (1996). Information and surprise in brain theory. In G. Rusch, S. J. Schmidt,
& O. Breidbach (Eds.), Innere Repräsentationen—Neue Konzepte der Hirnforschung,
DELFIN Jahrbuch (stw-reihe edition) (pp. 153–173). Suhrkamp.
Schmitt, A. O., & Herzel, H. (1997). Estimating the entropy of DNA sequences. Journal of
Theoretical Biology, 188(3), 369–377.
Shannon, C. E. (1948). A mathematical theory of communication. Bell Systems Technical Journal,
27, 379–423, 623–656.
Taylor, S. F., Tishby, N., & Bialek, W. (2007). Information and fitness. arXiv:0712.4382v1.
Tkacik, G., & Bialek, W. (2007). Cell biology: Networks, regulation, pathways. In R. A. ˇ
Meyers (Ed.) Encyclopedia of complexity and systems science (pp. 719–741). Springer.
arXiv:0712.4385 [q-bio.MN].Part I
Surprise and Information of DescriptionsChapter 2
Prerequisites from Logic and Probability
Theory
This chapter lays the probabilistic groundwork for the rest of the book. We introduce
standard probability theory. We call the elements A of the σ-algebra “propositions”
instead of the more common “events.” We reserve the word “event” for the elements
of the probability space .
2.1 Logic and Probability of Propositions
We begin with an intuitive explanation of the basic object of probability theory,
i.e., with the probabilistic model of the world: the so-called probability space
(, 	, p) (for a formal definition, see Definition 2.4 on page 19).
Here  is a set. It can be taken to stand for the set of all possible events. Thus,
any element ω ∈  will be called an event.
It should be realized that in a given context, one needs only to distinguish certain
relevant events, but even then there may be many—even infinitely many—different
relevant events, whereas it would certainly require a theory about the world to set
up a set of really all possible events.
	 is a set of subsets of . It stands for the set of all possible propositions about
events, in a given context. An element A ∈ 	 will be called a proposition. The
identification of propositions describing events with subsets of  is straightforward:
a set A stands for the proposition “the event ω is in A”; thus, an event ω fulfills a
proposition A, exactly if ω ∈ A.
At this point, we could already introduce some syntactical restrictions on the set
	 of all propositions. For example, one could argue that in any language, we can
express only a finite number of propositions. There are two reasons for this:
1. The use of discrete symbols in language, i.e., its digital character—the fact that
the products of any language are sequences of symbols from a finite set (often
called the alphabet)
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_2
1314 2 Prerequisites from Logic and Probability Theory
2. The fact that we only live for a limited time and therefore are only able to produce
sequences of a limited finite length
When we refer to the set 	 of all possible propositions, we will usually allow
for sequences of finite but unlimited length. Thus, we shall usually allow at least
for a countably infinite 	. In probability theory, one usually assumes that 	
contains all sets {ω} (for all ω ∈ ); in this case, of course #(	) ≥ #(),
i.e., 	 has rather more elements than , or a larger or equal cardinality (denoted
by #). Of course, the necessary mathematical assumptions on 	 will always be
explicitly stated.
In real languages, it may well happen that  has a larger cardinality than 	. For
example, if an event ω involves the exact position of a glass on a table and this is
described by a pair (x, y) of real numbers (coordinates), then  has the cardinality
of the continuum, which is more than countable. On the other hand, 	 may still be
countable or even finite. If, however,  is finite, then 	 will normally contain more
elements than , but cannot be infinite, and in fact #(	) ≤ 2n, when #() = n (the
cardinality of 	 is at most 2n, where n is the number of elements in ).
There is another restriction on 	 that we will always assume:
	 should be closed under the logical operations. This has to be explained: We
may join propositions to new propositions by means of logical operations. If A and
B are propositions, we may also say A “and” B, A “or” B, A “and not” B and the
like. The mathematical counterpart for this is the three set operations:
• The intersection of two propositions: A ∩ B holds for an event ω, if ω fulfills A
and ω fulfills B.
• A ∪ B holds for ω if ω fulfills A or if it fulfills B.
• A¯ holds for ω, if ω does not fulfill A. Instead of A¯, the notation Ac (the
“complement” of A) will be used in the subsequent text.
Definition 2.1 A (nonempty) set 	 of subsets of a set  is closed under the logical
operations, if
(i) For any A in 	, Ac is also in 	.
(ii) For any two A, B in 	, A ∩ B and A ∪ B are also in 	.
A set 	 of subsets of a set  is called an algebra, if it is closed under the logical
operations and contains .
In the following, we will always assume that 	 ⊆ P() is an algebra; here P()
denotes the set of all subsets of . We will even require a little more as is customary
in probability theory (see Definition 2.4).
The set  itself, interpreted as a proposition, says that ω ∈ ; in other words, it
“says nothing” or it holds for every event ω: it is a tautology. Its negation c holds
for no event ω. As a set, c is called the empty set and written as c = ∅.
Finally, p is a mapping that assigns a positive number p(A) to each A ∈ 	. p(A)
is called the probability of the proposition A, i.e., the probability that an event ω
fulfills A.2.2 Mappings, Functions, and Random Variables 15
We will assume that the following requirements are fulfilled by p:
1. p() = 1
2. p(∅) = 0
3. For every two A, B ∈ 	 with A ∩ B = ∅, we have p(A ∪ B) = p(A) + p(B)
Requirement 3 is the most important one, called the additivity of p, whereas 1
and 2 merely specify the range of values of p.
Proposition 2.1 The following holds for A, B ∈ 	:
(i) If A ⊆ B, then p(A) ≤ p(B)
(ii) 0 ≤ p(A) ≤ 1 (0 = p(∅) ≤ p(A) ≤ p() = 1)
(iii) p(A ∪ B) = p(A) + p(B) − p(A ∩ B)
(iv) p(Ac) = 1 − p(A)
Proof
(i) If A ⊆ B, then B = A ∪ (B \ A) and so by 3:
p(B) = p(B \ A) + p(A) ≥ p(A), since p(B \ A) is not negative.
(ii) Is obvious from Proposition (i).
(iii) A ∪ B = A ∪ (B \ A) and B = (B \ A) ∪ (A ∩ B). Thus by 3, we get
p(B) = p(B \ A) + p(A ∩ B) and
p(A ∪ B) = p(A) + p(B \ A) = p(A) + p(B) − p(A ∩ B).
(iv)  = A ∪ Ac and so 1 = p() = p(A) + p(Ac). 
2.2 Mappings, Functions, and Random Variables
Mappings and functions are really the same: they associate objects with each other.
More exactly, if A and B are two sets, a mapping or function f from A to B
associates with each element of A exactly one element of B. The element associated
with f to a ∈ A is denoted as f (a). Equivalently, we may say and write that f maps
a to b or f : a → b, or f (a) = b. A mapping f from a set A to a set B is denoted as
f : A → B. Mappings form a cornerstone of mathematics, so we should fix some
more notations related to mappings.
Definition 2.2 Let f : A → B be a mapping. We define
f −1(b) := {a ∈ A: f (a) = b} for any b ∈ B,
f −1(B
) := {a ∈ A: f (a) ∈ B
} for any B ⊆ B,16 2 Prerequisites from Logic and Probability Theory
f (A
) := {b ∈ B : there is an a ∈ A with f (a) = b}
= {f (a): a ∈ A
} for any A ⊆ A.
R(f ) := f (A) is called the range of f.
For a ∈ A, by definition, f (a) is always one unique element of B. Conversely,
for b ∈ B the so-called inverse image f −1(b) need not be a unique element of A, but
will in generally be a subset of A. This subset may even be empty, which happens if
b is not in the range of f , i.e., b /∈ R(f ). If f −1(b) contains exactly one element of
A for every b ∈ B, the mapping f is called bijective or invertible, but if it contains
at most one element of A for every b ∈ B, the mapping f is called injective or
one-to-one.
Proposition 2.2 Let f : A → B be any mapping. For any subsets A ⊆ A ⊆ A
and B ⊆ B ⊆ B, we have:
(i) f (A) ⊆ f (A
) and f −1(B) ⊆ f −1(B
),
(ii) f −1(f (A
)) ⊇ A
,
(iii) f (f −1(B
)) = B ∩ R(f ),
(iv) f (f −1(f (A
))) = f (A
) and
(v) f −1(f (f −1(B
))) = f −1(B
).
Proof
(i) is obvious, it is called the monotonicity of f and f −1 on sets.
(ii) If a is in A
, then f (a) is in f (A
). By definition of f −1, this means that
a ∈ f −1(f (A
)).
(iii) If b ∈ f (f −1(B
)), this means that b = f (a) for some a ∈ f −1(B
), but
a ∈ f −1(B
) means that b = f (a) ∈ B
. This shows that f (f −1(B
)) ⊆
B
. Clearly, f (f −1(B)) ⊆ f (A) = R(f ). Conversely, b ∈ B ∩ R(f )
means that b = f (a) for some a ∈ A and f (a) ∈ B
. This means a ∈
f −1(B
) and therefore b = f (a) ∈ f (f −1(B
)).
(iv), (v) By means of the monotonicity, they can be deduced easily from the
inclusions (ii) and f (f −1(B
)) ⊆ B (from (iii)). 
In (ii) we do not have an equation and it is indeed possible that f −1(f (A
)) = A
.
Can you give an example?
Definition 2.3 A mapping X:  → R is called a (real) random variable and a
mapping X:  → Rn is called a random vector (for n ≥ 2).
In the following, we will normally use uppercase letters like X, Y , Z for random
variables.
A mapping X:  → R, which assigns a real number X(w) to each event w ∈ ,
can be understood as a payoff-function: if the event w occurs, the player gets paid
X(w)—if X(w) happens to be negative, he has to pay, e.g., if X(w) = −5, he has
to pay 5 units, or if X(w) = 3, he gets 3 units. By means of the probability p, it is2.2 Mappings, Functions, and Random Variables 17
possible to determine the average payoff of a random variable X; it will be called
the expectation value of X and denoted as E(X).
How can it be found? Let us first make a few simple observations:
1. If we consider two payoff-functions X and Y , then a player who is payed the sum
X + Y , i.e., X(w) + Y (w) for every event w, will have as his average payoff the
sum of the average payoffs for X and for Y :
E(X + Y ) = E(X) + E(Y )
2. If a player gets 1 unit in case a proposition A ∈ 	 holds and nothing otherwise,
his average payoff will be exactly p(A). To write this down, we introduce the
function 1A by
1A(w) := 
1 if ω ∈ A,
0 if ω /∈ A.
Then we have E(1A) = p(A).
3. If a payoff-function X is positive, i.e., X(ω) ≥ 0 for every ω ∈ , then clearly
its expectation value is also positive, i.e., E(X) ≥ 0.
With these three observations, we can calculate E(X) for a large class of
functions X. If X attains finitely many values a1,...,an, we can consider the sets
Ai = [X = ai]={ω ∈ : X(ω) = ai}.
If these sets are in 	, we can write X as
X = n
i=1
ai1Ai
and determine
E(X) = n
i=1
aip(Ai).
In most of the following, considering finite or at most countable-valued functions
will be sufficient. Yet, sometimes we will need the distinction between discrete and
continuous mappings X:  → M. Discrete will mean that X has at most countably
many values in the set M, whereas continuous will mean that this is not the case. For
continuous functions (like ω → ω2 on R), we need some basic results of measure
theory.18 2 Prerequisites from Logic and Probability Theory
2.3 Measurability, Random Variables, and Expectation Value
It is one of the topics of measure theory to determine exactly the class of functions
X for which we can compute the expectation E(X). These functions are called
integrable and the expectation is written as an integral.1 This class of integrable
functions is found through the process of approximation.
If X(ω) = lim
n→∞ Xn(ω), for every ω ∈ , and E(Xn) is known, then E(X)
should be determinable as E(X) = lim
n→∞ E(Xn). For this idea to work, we need yet
another property of the probability p:
For a sequence An of sets in 	 with An+1 ⊆ An, it is clear that
0 ≤ p(An+1) ≤ p(An), and therefore lim
n→∞ p(An) ≥ 0 exists. Now it is required
that 
∞
n=1
An ∈ 	 and p( 
∞
n=1
An) = lim
n→∞ p(An). This requirement is in fact
equivalent to Definition 2.4.(i).
With this idea, we can determine E(X) for many functions X. Consider a
function X:  → R with |X(ω)| ≤ M for every ω ∈ . Now we take an integer n
and the sets
Ak =

k
n < X ≤
k + 1
n

=
	
ω ∈ :
k
n < X(ω) ≤
k + 1
n


for k = −M · n, . . . , M · n.
Then clearly,
Xn := 
M·n
k=−M·n
k
n · 1Ak ≤ X ≤ Xn +
1
n =: X
n.
Therefore, X(ω) = lim
n→∞ Xn(ω) for every ω ∈  and
E(X) = lim
n→∞ E(Xn) = lim
n→∞

M·n
k=−M·n
k
n · p

k
n < X ≤
k + 1
n

.
A slightly different argument for the same computation of E(X) is the following:
From Xn ≤ X ≤ X
n, it is clear that E(Xn) ≤ E(X) ≤ E(X
n). And from X
n −
Xn = 1
n , we get E(X
n) − E(Xn) = 1
n and this shows lim
n→∞ E(X
n) = lim
n→∞ E(Xn)
which then is equal to E(X).
In order to determine E(Xn) or E(X
n), we need to know the probabilities of
propositions like [a ≤ X ≤ b] for a, b ∈ R, i.e., these propositions [a ≤ X ≤ b]
1 For more details, see any book on probability or measure theory, e.g., Ash (1972); Bauer (1972);
Billingsley (1979); Halmos (1950); Jacobs (1978); Lamperti (1966).2.3 Measurability, Random Variables, and Expectation Value 19
should belong to 	. This requirement is called measurability of X in terms of 	. If
not stated otherwise, functions on  are always assumed to be measurable.
In the remainder, we shall briefly define some basic technical concepts of
measure and probability theory for further reference.
Definition 2.4
(i) An algebra 	 that contains 
∞
n=1
An for every sequence of propositions An ∈ 	
is called a σ-algebra. Usually, a probability p is defined on a σ-algebra and it
is required that p be σ-additive, i.e., that p
 
∞
n=1
An

= ∞
n=1
p(An) for any
sequence An of mutually disjoint sets in 	.
(ii) A triple (, 	, p), where  is a set, 	 is a σ-algebra of subsets of , and p
is a (σ-additive) probability, is called a probability space.
(iii) Given any set S of propositions, we call the smallest σ-algebra containing S
the σ-algebra generated by S and denote it by σ (S).
(iv) Given a function X:  → R, we consider the propositions [X ≤ a]={ω ∈
: X(ω) ≤ a} = X−1((−∞, a]) for a ∈ R. Let S = {[X ≤ a]: a ∈ R}, and
then σ (X) := σ (S) is called the σ-algebra (of propositions) concerning X.
(v) Given a function X:  → M with values in some finite or countable set M,
we consider S = {[X = m]: m ∈ M}. Here σ (X) := σ (S) is again called the
σ-algebra (of propositions) concerning X.
(vi) A function X on  is called measurable in terms of a set 	 of propositions, if
σ (X) ⊆ σ (	).
(vii) Given a probability space (, 	, p), a function X:  → R is called a random
variable, if it is measurable in terms of 	. This is the case exactly if [X ≤ a]
∈ 	 for all real numbers a, i.e., if S ⊆ 	 (see Definition (iv)). The function
FX : R → [0, 1] defined by FX(a) = p[X ≤ a] is called the distribution
function of X.
The concept of a distribution function helps to compute expectation values
for random variables by means of real integration. In fact, for any measurable
function h: R → R, the function h ◦ X:  → R is a random variable and
one can show that E(h ◦ X) =  h(x)dFX(x) =  h(x)F˙
X(x)dx (if either
integral is finite). In this way, one can, for example, compute the moments
Mi(X) := E(Xi
) =  xi
dFX(x) for i = 1, 2, 3,....
Observe that a function 1A is measurable if and only if A ∈ 	. From this
observation, one can infer that also step functions 
i ai1Ai (with all Ai ∈ 	) are
measurable.
We end this introductory section with two examples of probability spaces, one
continuous and one discrete.20 2 Prerequisites from Logic and Probability Theory
Example 2.1 Let  = [0, 1] and 	 = B(), where B() is the Borel-σ-algebra.2
Define p((a, b)) = b − a for a<b ∈ [0, 1]. p defines a probability measure on
[0, 1].
3 All elementary events ω ∈ [0, 1] have probability 0. 
Example 2.2 Let  = {1, 2,..., 6}, 	 = P() and p({ω}) = 1
6 for every ω ∈ .
This space represents the “fair dice.” For easier reference, we simply call this space
D = (, 	, p). 
Similarly, we define some finite probability spaces for further reference.
Definition 2.5 Dk = (, 	, p) where  = {1, 2, 3, 4, 5, 6}k, 	 = P() and
p(ω) = 6−k for every ω ∈ .
En = (, 	, p) where  = {1,...,n}, 	 = P(), and p(ω) = 1
n for every
ω ∈ .
Dk describes the throwing of k dice. The individual dice are described by the
random variables Xi :  → {1,..., 6} with Xi(ω1, ω2,...,ωk ) = ωi.
2.4 Technical Comments
The first goal in writing this book was to focus on the new concepts in a rather
intuitive spirit. Therefore, some formulations will appear rather naive for a trained
mathematician. In this spirit, I could generate an interesting, meaningful, and correct
theory only for finite objects. On the other hand, most definitions can almost equally
well be formulated for infinite objects, so I could not resist the temptation of
formulating this theory in its natural generality. The intuitive ideas still remain valid
as far as I can see, but this approach will occasionally lead into some technical
difficulties, which are very common and well-known in classical probability theory.
They have to do with measurability, sets of probability (or measure) 0, and the
like. Clearly, the most reasonable framework for dealing with such subtleties is the
classical probability space (, 	, p), which is the basis for all subsequent chapters.
References
Ash, R. B. (1972). Real analysis and probability. Academic Press.
Bauer, H. (1972). Probability theory and elements of measure theory. Holt, Rinehart and Winston.
Billingsley, P. (1979). Probability and measure. Wiley.
Halmos, P. R. (1950). Measure theory. Van Nostrand.
Jacobs, K. (1978). Measure and integral. Academic Press.
Lamperti, J. (1966). Probability: A survey of the mathematical theory. Benjamin/Cummings.
2 B() is the smallest σ-algebra containing all open intervals (a, b) ⊆ [0, 1].
3 See Bauer (1972), for example; p is called the Lebesgue measure.Chapter 3
Improbability and Novelty
of Descriptions
In this chapter, we define the information of an event A ∈ 	, or in our terminology,
the novelty of a proposition A as − log2 p(A). We further define the important
new concept of a description and extend the definition of novelty from events to
descriptions. Finally, we introduce the notions of completeness and directedness of
descriptions and thereby the distinction between surprise and information, which
are opposite special cases of novelty. This deviation from classical information
theory is further elaborated in the fourth part of this book. The interested expert
may go directly from Chap. 4 to Part IV.
3.1 Introductory Examples
You meet somebody (Mr. Miller) and ask him about his children. During the
conversation, he tells you two facts: “I have two children” and “This is my son”
(pointing to a person next to him). Given these two facts, what is the probability that
he has a daughter?
The task seems to be the evaluation of a conditional probability concerning the
children of Mr. Miller. If we take his two statements at face value, this probability
is p[ he has a daughter | he has a son and he has two children ].
However, the situation is not so simple. For example, Mr. Miller’s statement
“I have two children” is also true, if he has three or more children. Now, if he has
two or more children, at least one of them a son, and for each child the probability
of being a daughter is about 1
2 , then the probability asked for is certainly at least 1
2 ,
maybe larger, depending on the number of children he actually has. But there is
another, more plausible way of interpreting the two statements: you consider the
situation of this conversation and what else Mr. Miller could have said. For example,
if you asked him “Do you have two children?” and he simply answered “yes,”
then it may be that he actually has more than two children. However, if he could
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_3
2122 3 Improbability and Novelty of Descriptions
equally well answer your question with “I have three children” instead of “I have
two children,” you would expect that he chooses the first statement if he actually
has three children, since this describes his situation more accurately. This is what
we normally expect in a conversation.
For example, it may have started by Mr. Miller pointing out “This is my son.”
Then you may have asked “Do you have more children?” and he may have answered
“Yes, I have two.” In this case, one statement would actually mean [he has two
children and no more]. Let us turn to the other statement “This is my son.” We would
assume that Mr. Miller happened to be accompanied by one of his children. Indeed,
if he would be accompanied by two of them, we would expect him to mention
both of them in a usual conversation. So the other statement means [Mr. Miller was
accompanied by one of his two children and this was his son]. Now we can work
out the desired probability if we assume that it is equally probable that Mr. Miller is
accompanied by one or the other of his two children, if he is accompanied by just
one of them (which seems quite reasonable).
Mathematically, the situation can be described by three random variables X1 and
X2 ∈ {m, f } for child one and two, and C ∈ {0, 1, 2} for his companion: C = 0
means he is not accompanied by just one child, C = 1, he is accompanied by child
one, C = 2 for child two. We assume that
p(X1 = m) = p(X1 = f ) = p(X2 = m) = p(X2 = f ) = 1
2
and p[C = 1] = p[C = 2] = p and work out the answer to the problem.
The face value of the statement that he has a son (assuming he has exactly two
children) is Am = [X1 = m or X2 = m]. With Af = [X1 = f or X2 = f ], the
face-value probability that we asked for is
p(Af |Am) = p(Af ∩ Am)
p(Am) =
1
2
3
4
= 2
3
.
The other interpretation of the same statements is what is described more formally in
this chapter. Mr. Miller describes the situation differently, depending on the values
of X1, X2, and C:
• If X1 = m and C = 1, he says Am,
• If X2 = m and C = 2, he says Am,
• If X1 = f and C = 1, he says Af ,
• If X2 = f and C = 2, he says Af ,
• If C = 0, he says nothing (i.e., ) about the sex of his children.
Since he has said Am, we have to ask for the set of all conditions under which he
says Am. This is called A
m in our theory:
A
m = [X1 = m, C = 1]∪[X2 = m, C = 2].3.1 Introductory Examples 23
The desired probability is
p(Af |A
m) = p(Af ∩ A
m)
p(A
m)
= p[X1 = m, X2 = f, C = 1] + p[X1 = f, X2 = m, C = 2]
p[X1 = m, C = 1] + p[X2 = m, C = 2]
=
p
4 + p
4
p
2 + p
2
= 1
2
.
Another example for this distinction between A and A
 is the famous Monty Hall
problem (Gardner 1969, 1959; see also Bapeswara-Rao and Rao 1992; Gillman
1992; Granberg and Brown 1995; Selvin 1975; Seymann 1991).
A quizmaster M gives the successful candidate C the opportunity to win a sports
car S. There are three doors and the sports car is behind one of them (behind each
of the other two doors is a goat). The candidate points at a door and if the sports car
is behind it, it is his. Now the quizmaster opens one of the other doors and shows
him a goat behind it (he knows where the sports car is). Then he asks the candidate
whether he wants to change his previous decision. Should the candidate change?
Again it is a problem of conditional probabilities. It can be described by
three variables S,C,M ∈ {1, 2, 3}, describing the position of the sports car, the
initial choice of the candidate, and the door opened by the quizmaster. There is
a restriction on M, namely, S = M = C. By opening one door (door i), the
quizmaster effectively makes a statement Ai = [ the sports car is not behind door i]
(i = 1, 2, 3).
Here the face-value probability is p[S = C|Ai]. For reasons of symmetry, we
may assume that all these probabilities are the same for i = 1, 2, 3. Thus, we may
assume C = 1 and i = 2. Then p[S = 1|A2, C = 1] = p[S = 1|S = 1 or
S = 3] = 1
2 .
When, however, we ask for the conditions under which the quizmaster says Ai
(i.e., for A
i), the answer is [M = i]. Thus, the desired probability is
p[S = C|M = i] = p[S = C,M = i]
p[M = i] .
Again for reasons of symmetry, we may assume C = 1 and i = 2. Thus,
p[S = 1|M = 2, C = 1] = p[S = 1, C = 1, M = 2]
p[M = 2, C = 1]
= p[S = 1, C = 1, M = 2]
p[S = 1, C = 1, M = 2] + p[S = 3, C = 1, M = 2]24 3 Improbability and Novelty of Descriptions
=
1
3 ·
1
3 ·
1
2
1
3 ·
1
3 ·
1
2 +
1
3 ·
1
3
= 1
3
.
3.2 Definition and Properties
In this chapter, we will define the novelty (on this level, we might as well call
it information or surprise) of a proposition, which should be a measure of its
“unexpectedness.”
The first idea is that a proposition certainly is the more surprising, the more
improbable, i.e., the less probable it is.
A real function f : R → R is called isotone (or increasing), if x ≤ y implies
f (x) ≤ f (y), and it is called antitone (or decreasing), if x ≤ y implies f (x) ≥
f (y).
Given a probability p on  and an antitone real function f , the function f ◦ p
(defined by (f ◦ p)(A) = f (p(A)) for A ∈ 	) may be called an improbability.
Definition 3.1 For a proposition A ∈ 	, we define the novelty N of A as1
N (A) := − log2 p(A).
We note that N is an improbability, since x → − log2 x is antitone. But why
did we choose f = − log2—why the base 2? The basic idea is that N (A) should
measure the number of yes–no questions needed to guess A. This will become much
clearer in Chap. 5; here we just want to give a hint to make this choice of f = − log2
plausible.
Obviously, with one yes–no question, we can decide between two possibilities,
with two questions between four possibilities, with three questions between eight
possibilities, and so on, since we can use the first question to divide the eight
possibilities into two groups of four possibilities each, and decide which group it
is, and then use the two remaining questions for the remaining four possibilities.
In this way, with each additional question, the number of possibilities that we can
decide between is doubled. This means that with n questions, we can decide between
2n possibilities. If we want to find out the number of questions from the number of
possibilities, we have to use the inverse relationship, i.e., for k possibilities, we need
log2 k questions.
The most important property that is gained by the choice of a logarithmic
function is the additivity of novelty: N (A ∩ B) = N (A) + N (B) for independent
1 This definition is the classical basic definition of information or entropy, which goes back to
Boltzmann (1887) (see also Brush 1966).3.2 Definition and Properties 25
propositions A and B. To explain this, we have to expand a little on the notion of
independence.
Given two propositions A and B, we may try to find a statistical relation between
the two. For example, we might ask whether it has an influence on the probability
of A to occur, when we already know that B is true. The question is whether the
probability p(A) is the same as the so-called conditional probability of A given B,
which is defined as
pB(A) = p(A|B) :=
p(A ∩ B)
p(B) .
If p(A|B) = p(A), then there is no such influence, and we call A and B
independent.
Of course, we could also reverse the roles of A and B, and say that A and B
are independent if p(B|A) = p(B). It turns out that this condition is essentially
equivalent to the other one, because
p(B|A) = p(A ∩ B)
p(A) = p(A ∩ B)
p(B) ·
p(B)
p(A) = p(A|B) ·
p(B)
p(A) equals p(B)
if p(A|B) = p(A).
Of course, all of this only makes sense if p(A) and p(B) are not zero.
It is also clear that the two equivalent conditions are essentially the same as
p(A ∩ B) = p(A) · p(B), because p(A|B) = p(A) also implies
p(A) · p(B) = p(A|B) · p(B) = p(A ∩ B).
These considerations are summarized in the following definition.
Definition 3.2 Two propositions A and B are called independent, if
p(A ∩ B) = p(A) · p(B).
Proposition 3.1 If we define the conditional novelty of A given B as
N (A|B) = − log2 p(A|B),
then we have
(i) N (A ∩ B) = N (B) + N (A|B)
(ii) N (A ∩ B) = N (A) + N (B) if A and B are independent.
Proof Obvious. 26 3 Improbability and Novelty of Descriptions
3.3 Descriptions
Let us come back to an observation already made in Sect. 2.1. In general, the set
 of all possible events may be very large. In a typical physical model, the events
ω ∈  would be represented as real vectors ω = (ω1,...,ωn) ∈ Rn and thus 
would be larger than countable. On the other hand, 	 may well be countable, and
so a description of an element ω ∈  by propositions A ∈ 	 would be essentially
inexact. Moreover, different persons may use different propositions to describe the
same event ω. For example, when we walk on the street, we may see an event ω; let
us say we see a car passing by. But this is not an exact description, and if we say
that we see a blue Mercedes driven by an old man passing by very slowly, this still
is not an exact description and somebody else might rather describe the same event
as “Mr. Miller is driving with his wife’s car into town.”
What goes on here is that
1. Given the event, somebody describes it by a statement in a certain language.
2. Then this statement is interpreted again in our model  of the possible events as
a proposition A, i.e., as the set A of all events y which are also described by the
same statement (Fig. 3.1).
The point of view taken by a particular observer in describing the events ω ∈ 
by propositions A ∈ 	 constitutes a particular description of these events. This
process of description can be defined mathematically as a mapping.
Definition 3.3 Given (, 	, p), a mapping d :  → 	 that assigns to each ω ∈ 
a proposition d(ω) = A ∈ 	 such that ω ∈ A is called a description.
World
description
x
Language World
interpretation
x
A = d(x)
Fig. 3.1 Model for descriptions of events3.3 Descriptions 27
In addition, we require2 that for every A ∈ R(d)
p[d = A] = p({ω ∈ : d(ω) = A}) = 0.
This means that we do not want to bother with propositions that may happen
only with probability 0. This additional requirement is quite helpful for technical
reasons, but it is quite restrictive and it rules out some interesting examples (see
Example 3.3). Note that the requirement that ω ∈ d(ω) means that the description
has to be true. So every event ω is described by a true proposition about it.
Example 3.1 Consider the throwing of a dice, i.e., the space of possible events
 = {1, 2, 3, 4, 5, 6}. Consider the following descriptions:
(1) “Even vs. odd”: For A = {2, 4, 6} and B = {1, 3, 5} = Ac, we define the
description e by e : 1 → B, 2 → A, 3 → B, 4 → A, 5 → B, 6 → A.
(2) “Small vs. large”:
d : 1 → {1, 2, 3, 4} = A, 2 → A, 3 → A,
4 → {3, 4, 5, 6} = B, 5 → B, 6 → B.
(3) “Pairs”:
c : 1 → {1, 2}, 2 → {2, 3}, 3 → {3, 4},
4 → {3, 4}, 5 → {4, 5}, 6 → {5, 6}. 
Example 3.2 Try to define some descriptions that make use of the propositions
indicated in Fig. 3.2 about locations on a square table including some logical
operations on them.3 
Example 3.3 Without the requirement added to Definition 3.3, a description d may
have an uncountable range R(d). Here are two examples for this.
Take  = R and a (continuous) probability p on (R,B) and δ > 0. For ω ∈ 
we define d(ω) = (ω − δ,ω + δ) and c(ω) = [ω,∞) = {x ∈ R: x ≥ ω}.
Both c and d are interesting descriptions, but for every A in R(c) or R(d), we
observe p[c = A] = 0 and p[d = A] = 0. 
2 This requirement obviously implies that the propositions [d = A] are in 	 for every A ∈ R(d).
It also implies that R(d) is finite or countable.
3 For example, one can describe points x ∈ A\C by d(x) = A, points x in A∩C by d(x) = A∩C,
points x in B \ C by d(x) = B, and points x in B ∩ C by d(x) = C.28 3 Improbability and Novelty of Descriptions
Fig. 3.2 Example of
propositions
about positions on a table A B
C
D
E
Definition 3.4 A finite or countable collection {A1,...,An} or {Ai : i ∈ N} of
(measurable) subsets of  is called a (measurable) partition, if
(i) 
i
Ai =  and
(ii) p(Ai ∩ Aj ) = 0 for i = j .
For a partition, we always have 
i
p(Ai) = 1 and essentially4 every ω ∈  is in
exactly one of the sets Ai.
Usually, part (ii) of the definition says Ai ∩ Aj = ∅ for i = j . Here we
again disregard sets of probability 0 as “essentially empty.” In the following, we
will sometimes identify sets (A and B) that differ only by an essentially empty set,
i.e., we may write A = B (essentially), meaning that
p[A = B] = p(A\B) + p(B\A) = 0.
Example 3.4 Given a partition α = {A1,...,An}, we may define dα(ω) := Ai for
ω ∈ Ai. Obviously, dα is a description. 
4 From Definition (ii), it is obvious that the probability that ω is in two sets, e.g., Ai and Aj , is 0.
So, disregarding propositions with probability 0, every ω ∈  is in exactly one of the sets Ai. We
will usually disregard propositions with probability 0 and this is meant by the word “essentially.”3.4 Properties of Descriptions 29
3.4 Properties of Descriptions
Definition 3.5 Given a description d :  → 	, we consider the novelty mapping
N : 	 → R defined by N (A) = − log2 p(A) and call
Nd (ω) = (N ◦ d)(ω) = N (d(ω))
thenovelty provided by ω for the description d. Note that Nd :  → R is a random
variable.5 We further define the average novelty for the description d as
N (d) := E(Nd ).
For this definition, it is required that N ◦ d is measurable. Let us illustrate this
in the case where d has finite range (see Definition 2.2). Let us say d takes the
values A1,...,An, then N ◦ d also has a finite range, and the value of N ◦ d is
− log2 p(Ai), when the value of d is Ai.
This occurs on the set A
i := {ω ∈ : d(ω) = Ai} = d−1(Ai). Now it is clear
that N ◦ d is a step function, namely,
N ◦ d = n
i=1
− log2 p(Ai)1A
i
.
Clearly, in this case, we have to require that the sets A
i = d−1(Ai) are in 	 for
i = 1,...,n, and then we can calculate
N (d) = E(N ◦ d) = −n
i=1
p

A
i

log2 p(Ai).
But these sets A
i do have still another significance. Let us ask what we can infer
about the event ω that has taken place from its description d(ω) = Ai given by
an observer. The obvious answer is of course that he tells us that ω is in Ai. But
if we know the attitude of the observer well enough, we can infer even more. If
we know his procedure of description, i.e., the mapping d he uses, we can infer
that ω is such that d(ω) = Ai, i.e., we can infer that ω ∈ [d = Ai]={y : d(y) =
Ai} = d−1(Ai) =: A
i, and A
i is a more exact information about ω than Ai, because
A
i ⊆ Ai. (Indeed every ω in A
i satisfies d(ω) = Ai).
Let us give an example of this kind of inference. If an observer says: “the car
went slowly,” this may be interpreted literally as “speed of the car below 30 km/h,”
say, but when we know that the observer would say that a car goes “very slowly,”
when its speed is below 10 km/h, we can even infer that the speed of the car was
between 10 km/h and 30 km/h. Of course, an accurate observer might have said that
5 Due to the additional requirement in Definition 3.3, the function Nd is measurable. However, it
may happen that E(Nd ) is infinite. For an example, see Proposition 3.17.30 3 Improbability and Novelty of Descriptions
the car goes slowly but not very slowly, but it is quite common experience that this
additional information is neither mentioned nor used (nor of any interest).
If this kind of additional inference is already explicitly contained in a
description d, then we call it complete. Given an arbitrary description d, it is quite
easy to construct another description d˜ that gives exactly this additional implicit
information; this description d˜ we call the completion of d.
Definition 3.6 For a description d and for A ∈ R(d), we define
(i) A
 := [d = A]={ω ∈ : d(ω) = A}
(ii) The description d˜ by d(ω) ˜ = {ω ∈ : d(ω) = d(ω
)}.
d˜ is called the completion of d. A description d is called complete if d = d˜.
Proposition 3.2 The following properties of a description d are equivalent:
(i) d is complete,
(ii) If d(ω) = d(ω
) then d(ω) ∩ d(ω
) = ∅,
(iii) The range R(d) of d is a partition6 of .
Proof This should be obvious from the definitions. If not, the reader should try to
understand the definition of d˜ and why the range of d˜ must be a partition. 
If we ask how surprising the outcome of a fixed description d will be, we again
encounter the sets A
i. The idea here is to consider the surprise of one particular
outcome in comparison to all the other (usual) outcomes of the description d.
As a first step, we can order the sets Ai according to their probability such that
p(A1) ≤ p(A2) ≤ ... ≤ p(An). Then we can say that A1 is more surprising
than A2 and so on. To quantify the amount of surprise we really get from Ai, we
determine the probability pi that d gives us at least as much surprise as Ai does.
This is pi = p[d(x) = A1 ∨ d(x) = A2 ∨ ... ∨ d(x) = Ai]. Since d can take only
one value for every ω ∈ , this is the sum of probabilities:
pi = 
i
j=1
p[d = Aj ] = 
i
j=1
p(A
j ).
So for our description d, the surprise of Ai is − logpi, whereas its novelty is simply
− logp(Ai), and its information is − log p(A
i). Given a description d, we can
construct in the above fashion another description d which gives the surprise of d.
6 Strictly speaking, here a partition should be defined by Ai ∩ Aj = ∅ instead of p(Ai ∩ Aj ) = 0
(compare Definition 3.4). If we disregard 0-probability-propositions, we should interpret d = d˜ in
Definition 3.6 as p[d = d˜] = 1 and we should use the weaker formulation p(d(ω) ∩ d(ω
)) = 0
in part (ii) of this definition.3.4 Properties of Descriptions 31
Definition 3.7 For a description d and for A ∈ R(d), we define
A := {B
: B ∈ R(d), p(B) ≤ p(A)}
= [p ◦ d ≤ p(A)]
= {ω ∈ : p(d(ω)) ≤ p(A)}
and the description d by
d(ω) := {ω
: p(d(ω
)) ≤ p(d(ω))}.
A description d is called directed, if d = d.
Definition 3.8 A description d is called symmetric, if for every x,y ∈ , x ∈ d(y)
implies y ∈ d(x).
We can now reintroduce the set-theoretical operations that are defined on
propositions, on the level of descriptions; in particular, we have the natural ordering
and the union and intersection of descriptions:
Definition 3.9 We say that a description c is finer than a description d, or that d is
coarser than c and write c ⊆ d, if c(ω) ⊆ d(ω) for every ω ∈ .
With this definition, we see that the completion d˜ of any description d is always
finer than d. This is because for any ω ∈ , x ∈ d(ω) ˜ means d(x) = d(ω) and this
implies x ∈ d(x) = d(ω). Obviously, we also have d˜ ⊆ d, since d(ω
) = d(ω)
implies p(d(ω
)) ≤ p(d(ω)).
Usually, d ⊆ d, but d ⊆ d is also possible.
Example 3.5 Let  = {1,..., 6}.
d(ω) := {ω}, b(ω) = {1,...,ω}, and
c(1) = {1,..., 4}, c(2) = {1,..., 5}, c(i) =  for i > 2.
Then
d(ω) = , b(ω) = b(ω), and
c(1) = {1}, c(2) = {1, 2}, c(i) =  for i > 2.
Thus, we have b = b, c ⊂ c, and d ⊃ d. 
Definition 3.10 For any two descriptions c and d, we define
(i) c ∩ d by (c ∩ d)(ω) := c(ω) ∩ d(ω),
(ii) c ∪ d by (c ∪ d)(ω) := c(ω) ∪ d(ω),
(iii) dc by dc(ω) = (d(ω))c ∪ {ω} for every ω ∈ .32 3 Improbability and Novelty of Descriptions
Note that the complement or negation has to be slightly adjusted in order to keep
the property that ω ∈ dc(ω). Still the complement has the nice property that d ∩ dc
is the finest possible description, namely, d ∩dc(ω) = {ω}, and d ∪dc is the coarsest
possible description, namely, d ∪ dc(ω) = .
The point is that descriptions have to be true, i.e., ω ∈ d(ω), and therefore the
flat negation of a description cannot be a description. But given d(ω), one can try
to describe the same event ω in the opposite or most different way, which is dc(ω).
It is like saying a glass of wine is half-empty instead of half-filled.
Proposition 3.3 c ⊆ d implies N ◦ c ≥ N ◦ d, and this implies N (c) ≥ N (d).
Proof Obvious. 
This property of the novelty is called monotonicity. It is one of the essential
properties that are needed for developing information theory.
Proposition 3.4 Let c and d be two descriptions. Then c˜ ∩ d˜ ⊆ c
∩ d.
Proof Let ω ∈ ˜c(ω) ∩ d(ω) ˜ . Then c(ω
) = c(ω) and d(ω
) = d(ω). Therefore,
c ∩ d(ω
) = c ∩ d(ω), i.e., ω ∈ c
∩ d(ω). 
Unfortunately, c˜ ∩ d˜ = c
∩ d in general, as the following example shows.
Example 3.6 Let  = {1,..., 6} and define
c(1) = c(2) = {1, 2}, c(3) = c(4) = c(5) = c(6) = {3, 4, 5, 6}
and
d(1) = {1, 2, 3, 4, 5}, d(3) = {2, 3, 4, 5, 6},
d(2) = {1, 2, 3, 4}, d(4) = {1, 3, 4, 5, 6},
d(5) = d(6) = .
We observe that d˜ ⊂ ˜c = c ⊂ d. Here d(ω) ˜ = {ω} for ω = 1, 2, 3, 4 and d(˜ 5) =
d(˜ 6) = {5, 6}. Thus, c˜ ∩ d˜ = d˜ and c ∩ d = c implying c
∩ d = ˜c = c. 
Also c ⊆ d does not imply c˜ ⊆ d˜, in general. The above is even an example
where c ⊂ d and c˜ ⊃ d˜. These problems can be circumvented, if we consider
so-called consequential or tight descriptions. The reason for the second name will
become clear in Sect. 10.3.
Definition 3.11 A description d is called consequential or tight, if for every
x,y ∈ 
x ∈ d(y) implies d(x) ⊆ d(y).3.4 Properties of Descriptions 33
Proposition 3.5 Let c, d be descriptions.
(i) d˜ is tight and d is tight.
(ii) If c and d are tight, then c ∩ d is tight.
Proof
(i) Let x ∈ d(y) ˜ . Then d(x) = d(y) and therefore d(x) ˜ = d(y) ˜ .
Let x ∈ d(y). Then p(d(x)) ≤ p(d(y)).
Let ω ∈ d(x). Then p(d(ω)) ≤ p(d(x)) ≤ p(d(y)), i.e., ω ∈ d(y).
Thus, d(x) ⊆ d(y).
(ii) Let x ∈ c ∩ d(y). Then c(x) ⊆ c(y) and d(x) ⊆ d(y) since c and d are tight.
Thus, c ∩ d(x) = c(x) ∩ d(x) ⊆ c(y) ∩ d(y) = c ∩ d(y). 
Proposition 3.6 If c and d are tight descriptions, then
(i) c ⊆ d implies c˜ ⊆ d˜,
(ii) c
∩ d = ˜c ∩ d˜.
Proof
(i) We show that c˜ ⊆ d˜ which implies N (c˜) ≥ N (d)˜ . Take any x ∈ . For
y ∈ ˜c(x) we have to show y ∈ d(x) ˜ . Let y ∈ ˜c(x), i.e., c(y) = c(x). This
implies
d(y) ⊇ c(x)  x and d(x) ⊇ c(y)  y.
By tightness of d, we get d(x) ⊆ d(y) and d(y) ⊆ d(x), i.e., d(x) = d(y).
This means y ∈ d(x) ˜ .
(ii) From (i) we get c
∩ d ⊆ ˜c and c
∩ d ⊆ d˜, thus cd ⊆ ˜c∩d˜. The reverse inclusion
is Proposition 3.4. 
Proposition 3.7 For a tight description d, the following are equivalent:
(i) d is symmetric
(ii) d is complete.
Proof
(ii) ⇒ (i): d˜ is symmetric by definition.
(i) ⇒ (ii): If x ∈ d(y), then also y ∈ d(x) and tightness implies that
d(x) = d(y), i. e., x ∈ d(y) ˜ . Thus, d(y) = d(y) ˜ . 
Proposition 3.8 For a tight description d, the following are equivalent:
(i) d is directed.
(ii) d(ω) = {ω
: d(ω
) ⊆ d(ω)}={ω : p(d(ω
)) ≤ p(d(ω))} = d(ω) for
ω ∈ .
(iii) d(ω) ⊆ d(ω
) or d(ω
) ⊆ d(ω) for any ω,ω ∈ .34 3 Improbability and Novelty of Descriptions
Proof Let d be tight.
(i) ⇒ (ii): ω ∈ d(ω) implies d(ω
) ⊆ d(ω) which in turn implies p(d(ω
)) ≤
p(d(ω)). Therefore, d(ω) ⊆ {ω
: d(ω
) ⊆ d(ω)}⊆{ω
: p(d(ω
)) ≤
p(d(ω))} = d(ω) By (i) all these sets are equal.
(ii) ⇒ (iii): Assume p(d(ω)) ≥ p(d(ω
)). Then d(ω) ⊇ d(ω
), or vice versa.
(iii) ⇒ (i): p(d(ω
)) ≤ p(d(ω)) implies d(ω
) ⊆ d(ω) by (iii) and therefore ω ∈
d(ω). Thus, d(ω) ⊆ d(ω). Conversely, ω ∈ d(ω) implies d(ω
) ⊆ d(ω) which
in turn implies p(d(ω
) ≤ p(d(ω)) 
Definition 3.12 For a description d, we define the description d∩ by
d∩(ω) := {A ∈ R(d): ω ∈ A}.
d∩ is called the tightening of d.
Proposition 3.9 For any description d, the following holds:
(i) d∩ ⊆ d and therefore N (d∩) ≥ N (d),
(ii) d∩ is tight
(iii) d∩ = d, if and only if d is tight
Proof
(i) Is obvious
(ii) d∩ is tight: ω ∈ d∩(ω) implies that d∩(ω
) = {A ∈ R(d): ω ∈ A} ⊆
d∩(ω).
(iii) If d is tight and ω ∈ A ∈ R(d), then d(ω) ⊆ A. Thus, d(ω) ⊆ d∩(ω). Together
with (i) we obtain d = d∩ 
The interpretation of d∩ becomes obvious, when we consider R(d) as the set
of propositions that a person (or machine) is willing (or able) to make about an
“event” ω. If ω happens, we may collect all he, she, or it can say (correctly) about ω.
This is d∩(ω).
In Example 3.6, the description c is complete and therefore tight, while d is
not tight. In Proposition 3.5, we have seen that also directed descriptions are tight.
However, there are many tight descriptions d which are different both from d and d˜.
The following is an example for this (cf. Exercise 1).
Example 3.7 Let  = {1, 2,..., 6} and define
d(1) = d(2) = {1, 2, 3, 4},
d(3) = d(4) = {3, 4} and
d(5) = d(6) = {3, 4, 5, 6}.
3.5 Information and Surprise of Descriptions 35
3.5 Information and Surprise of Descriptions
The information of a description is defined as the average novelty provided by its
completion.
Definition 3.13 Let d be a description on (, 	, p). We denote by I(d) the
number N (d)˜ = E(N ◦ d)˜ and call it the information provided by d. For ω ∈  we
also define the random variable:
Id (ω) := N (d(ω)). ˜
If d is complete, I(d) coincides with the usual concept of Shannon information
on (, 	, p) provided by d (see Shannon and Weaver 1949 and Proposition 3.14).
It is now easy to prove.
Proposition 3.10 For any description d, we have
0 ≤ Nd ≤ Id
and
0 ≤ N (d) ≤ I(d) = N (d)˜ = E(Id ).
Proof see Exercise (4) on page 44.
We first show that d˜ ⊆ d. This implies N (d)˜ ≥ N (d) by Proposition 3.3. 
The first elementary observation on I(d) is that it is always positive, since it is
the sum of positive terms −p(A)  log p(A)  .
7 Figure 3.3 shows a plot of the function
h(p) = −p log2 p for p ∈ [0, 1].
Definition 3.14 Let d be a description.
(i) We define the surprise (of an outcome ω) of d by
Sd (ω) := − log2 p(d(ω)) = N (d(ω)).
(ii) We define the surprise of a description d by
S(d) := E(Sd ).
Proposition 3.11 For every description d, we have
0 ≤ S(d) ≤
1
ln 2
and S(d) ≤ I(d).
7 If p(A)  = 0 for some A
in the range of d˜, we set p(A)  log p(A)  = 0 since lim
x→0+ x log x = 0.36 3 Improbability and Novelty of Descriptions
Fig. 3.3 Plot of function
h(p)
h(p)
p
0 0.25 0.5 0.75 1
0
0.25
0.5
0.75
Proof The second inequality follows from d˜ ⊆ d.
For the first, it is sufficient8 to consider descriptions d with finite R(d). We may
further assume that d is directed, i.e., that R(d) = {A1,...,An} with A1 ⊆ A2 ⊆
... ⊆ An. Then S(d) = − n
i=1
p(Ai \ Ai−1)logp(Ai) (A0 = ∅). Let pi := p(Ai),
and then
S(d) = −n
i=1
(pi − pi−1)log pi ≤ n
i=1
−

pi
pi−1
log2(x)dx
≤ − 
1
0
log2(x) dx = 1
ln 2 
Unfortunately, both, information and surprise, do not have the property of
monotonicity in general. However, information is monotonic on tight descriptions.
Proposition 3.12 If c and d are tight descriptions, then c ⊆ d implies I(c) ≥ I(d).
Proof Follows from Proposition 3.6(i). 
From Proposition 3.5, it is obvious that both complete and directed descriptions
are tight. So we have monotonicity of information also on complete and on directed
descriptions.
8 Here we rely on the approximation argument as in Sect. 2.3 for the calculation of an expectation
value. If all the finite sum approximations satisfy the same inequality (≤ 1
ln 2 ), then this inequality
is also satisfied by the limit.3.5 Information and Surprise of Descriptions 37
Novelty and surprise are both smaller than information, but how do they relate
to each other? Usually, novelty will be much larger than surprise. For example,
a complete description d with p(d(ω)) = 1
n for every ω ∈  has N (d) = I(d) =
log2 n, but S(d) = 0. The following example shows, however, that N < S is also
possible.
Example 3.8 Let (, 	, p) = E16 and9
d : 1 → {1,..., 11} 6 →  14 → {7,..., 16}
2 → {1,..., 12} 7 →  15 → {8,..., 16}
.
.
. .
.
. 16 → {9,..., 16}
5 → {1,..., 15} 13 → 
Ordering d(ω) by the increasing values of p(d(ω)) for ω ∈ , we obtain
d(16), d(15), d(14), d(1), d(2), . . . , d(5), d(6), . . . , d(13), where from d(6) on,
we have p(d(i)) = 1. Thus
d : 16 → {16} 1 → {1, 14, 15, 16} 6 → 
15 → {15, 16} 2 → {1, 2, 14, 15, 16} 7 → 
14 → {14, 15, 16} .
.
. .
.
.
5 → {1, 2, 3, 4, 5, 14, 15, 16} 13 → 
Thus, Nd (ω) < Sd (ω) for ω /∈ {6,..., 13} and Nd (ω) = Sd (ω) = 0 for
ω ∈ {6,..., 13}. Thus, N (d) < S(d) in this example. 
It is also quite easy to characterize the extreme cases where two of the three
quantities N , I, and S coincide. This is done in the next proposition.
Proposition 3.13 Let d be a description, and then
(i) N (d) = I(d) implies d = d˜ essentially
(ii) S(d) = I(d) implies d ≡  essentially
(iii) If d is tight, then N (d) = S(d) implies d = d essentially
Proof
(i) We have N (d) = E(Nd ), I(d) = E(Nd˜), and Nd ≤ Nd˜. If for some
ω ∈ , Nd (ω) < Nd˜(ω), the same is true for all ω ∈ d(ω) ˜ , and therefore
N (d) < I(d). Thus, Nd (ω) = Nd˜(ω) and therefore p(d(ω)) = p(d(ω)) ˜
for every ω ∈ . Since d(ω) ˜ ⊆ d(ω), this implies that d(ω) ˜ and d(ω) are
essentially equal.
9 See Definition 2.3.38 3 Improbability and Novelty of Descriptions
(ii) Since S(d) = E(Nd ), I(d) = E(Nd˜), and d˜ ⊆ d, we can again infer that
Nd = Nd˜ and that d(ω) = d(ω) ˜ for every ω ∈ . For some ω we have
d(ω) = ; thus, d(ω) ˜ = , i.e., d ≡ .
(iii) ω ∈ d(ω) ⇒ d(ω
) ⊆ d(ω) ⇒ Nd (ω
) ≥ Nd (ω) ⇒ ω ∈ d(ω) 
For further reference, we provide a simple comparison of the three basic formulae
to calculate N , I, and S.
Proposition 3.14 For any description d, we have
(i) N (d) = − 
A∈R(d)
p(A)  logp(A),
(ii) I(d) = − 
A∈R(d)
p(A)  logp(A)  ,
(iii) S(d) = − 
A∈R(d)
p(A)  logp(A).
Proof Here we definitely need the additional requirement of Definition 3.3. Then
we just have to compute the expectation of a step function with at most countably
many values. Let us show (iii), for example,
S(d) = E(Sd ) = 
x∈R(Sd)
p[Sd = x] · x = − 
A∈R(d)
p[d = A] · log p(A). 
Example 3.9 Let
d(ω) =

A for ω ∈ A and p = p(A),
 otherwise.
Then
I(d) = −p log2 p − (1 − p)log2(1 − p) =: I (p).
This function is plotted in Fig. 3.4. 
In everyday life, the common use of the word information is closely related
with the common use of the word uncertainty. In fact, we expect the information
provided by the description of a phenomenon to be equivalent to the amount of
uncertainty the description eliminates. The concept of uncertainty has been treated
in the contexts of thermodynamics and statistical physics (Brush 1966) and has been
expressed in terms of entropy (a term introduced by Clausius (1865) and defined by
a formula similar to Proposition 3.14.(ii) by Boltzmann 1887). It is one of our aims
to elucidate the relation between entropy and information in depth (see Chap. 15).
For the moment, we limit ourselves to observing that the information of a partition
is also called its entropy by many authors.3.5 Information and Surprise of Descriptions 39
Fig. 3.4 Plot of function
I(p)
I(p)
p
0
0
0.25
0.25
0.5
0.5
0.75
0.75
1
1
In a nutshell, the words information, novelty, and surprise introduced here can be
distinguished or characterized as follows:
• Information you get whether you can use it or not, whether it is interesting or not,
• Novelty measures how much of this is new and interesting for you,
• Surprise is provided by an event that is comparatively improbable; if everything
is equally improbable, nothing is surprising.
The following proposition characterizes descriptions that provide “no informa￾tion.”
Proposition 3.15 The information of a description d is zero if and only if all sets
in the range of its completion d˜ have probability zero except for one set A
 with
p(A)  = 1. In accordance with our additional requirement in Definition 3.3, this is
equivalent to d ≡ .
Proof Clearly, the condition is sufficient: if p(A)  = 1 for one set in the range of
d˜ and p(B)  = 0 for the rest, then I(d) = 0. Now, if I(d) = 0, we
must have p(A)  log p(A)  = 0 for all A
 ∈ d() ˜ 
. Therefore, p(A)  is 0 or 1. Since
A
∈d() ˜ p(A)  = 1, exactly one of the sets A
 ∈ d() ˜ must satisfy p(A)  = 1. 
This proposition corresponds to the natural expectation that a description
provides no “new” information if we are certain about the outcome of the situation.
The other extreme case that provides maximal information and corresponds to
maximal uncertainty is attained by descriptions on which the probability measure is
uniformly distributed, as stated by the following proposition.
Proposition 3.16 For a fixed n, consider all descriptions whose range has n
elements, that is, all d on (, 	, p) with d() = {A1,...,An}. I(d) attains a40 3 Improbability and Novelty of Descriptions
maximum of log2 n for d = d˜ and p(Ai) = 1
n for each i (= 1, . . . , n). The same is
true for N (d).
Proof See Exercise (8) on page 44. 
Proposition 3.17 A description with infinite range can have infinite novelty.
Proof We give an example for such a description. Let  = R+, p(x) = (x + e)−1
(ln(x + e))−2, where
∞
0
p(x)dx =
∞
e
x−1(ln x)−2dx =

− 1
ln x
∞
e = 1
Let α = {[i − 1, i) = Ai : i ∈ N} and d(x) = Ai for x ∈ Ai ∈ α. Then I(d) =
− ∞
i=1
p(Ai)log p(Ai) where p(Ai) = p(xi) for some xi ∈ Ai, so
p(Ai) < p(i − 1) = (i − 1 + e)−1(ln(i − 1 + e))−2 ≤
1
e for i ∈ N
and
p(Ai) > p(i) = (i + e)−1(ln(i + e))−2.
Thus
−p(Ai)log2 p(Ai) > −p(i)log2 p(i)
> (i + e)−1(ln(i + e))−2 log2(i + e)
= (i + e)−1(ln(i + e))−1(ln 2)
−1
> (ln 2)
−1
i+
e+1
i+e
1
x ln x
dx.
Therefore
I(d) > (ln 2)
−1
∞
1+e
(x ln x)−1 dx = (ln 2)
−1
ln ln x
∞
1+e = ∞. 
Finally, we consider two potential properties of information, novelty and sur￾prise, that are essential for the further development of a useful information theory.
The first property, already shown in Proposition 3.3 to hold for novelty, is
monotonicity, i.e., the requirement that finer descriptions should have larger novelty,3.6 Information and Surprise of a Random Variable 41
information, and surprise. Unfortunately, it holds neither for information nor for
surprise, because c ⊆ d does not imply c˜ ⊆ d˜, nor c ⊆ d. From Example 3.6 we
can easily create an example where c ⊂ d and N (c) > N (d), but I(c) < I(d).
However, we get monotonicity of I for tight descriptions because of Proposition 3.6.
Counterexamples against monotonicity of surprise are easy to find. For example,
for an equally distributed discrete random variable X, clearly X ⊂ X≥, but
S(X)  = 0, whereas S(X≥) > 0.
The other important property of classical information is its subadditivity:
I(c ∩ d) ≤ I(c) + I(d). This will be shown in the next chapter (Proposition 4.5).
It is quite easy to see that the novelty N as defined here does not have this property,
i.e., in general, N (c ∩ d)  N (c) + N (d). An example for this can be obtained
by considering a description d and its complement dc (see also Exercise 7.) Also
surprise does not have this property (see Exercise 16). We will see in the next
section that information has this property.
In order to obtain both properties, monotonicity and subadditivity, for both,
information and novelty, the definitions given on the level of descriptions in this
chapter are not sufficient. We have to elevate these definitions to a higher level,
which is done in Part III of the book.
The other possibility is to consider only descriptions with particular properties,
for example, tight or complete descriptions. For complete descriptions, information
and novelty coincide and clearly have both properties. This is the framework of
classical information theory.
3.6 Information and Surprise of a Random Variable
For a measurable mapping X:  → M with values in some finite or countable set
M, we may consider the corresponding description X, which is only concerned with
the values of X on  and defined by
X(ω)  := {ω ∈ : X(ω
) = X(ω)}.
We see that the description X is always complete and that our above definition of
d˜ for a description d is just a special case of the definition of X. With the aid of
the complete description X, we can define the information contained in a random
variable X as I(X) := N (X)  . For later reference, we restate this definition.
Definition 3.15 For a discrete random variable X, we define its (average)
information content as
I(X) := N (X). 
Remark Usually, information is defined for partitions or for discrete random
variables (Ash 1965; Billingsley 1978; Gallager 1968; Shannon and Weaver 1949).42 3 Improbability and Novelty of Descriptions
Since partitions correspond to complete descriptions in our terminology and since
for any random variable X its description X is complete, this definition coincides
with the usual one.
Given an arbitrary random variable X:  → R, it may happen that
p[X = x] = 0 for any x ∈ R. In such a case, N (X)  would be infinite and we would
need a different workable definition (see Sect. 12.5). Therefore, it is sometimes
useful to consider other descriptions concerning X, different from X. For example,
one may be interested in the largeness or the smallness of the values of X. This
leads to the definitions of the descriptions X≥ and X≤. Or one may be interested in
the values of X only up to a certain limited accuracy. This leads to the definition of
X .
Definition 3.16 For a random variable X:  → R, we define the descriptions
X≥(ω) = {ω ∈ : X(ω
) ≥ X(ω)} and
X≤(ω) = {ω ∈ : X(ω
) ≤ X(ω)} and
X (ω) = {ω ∈ : |X(ω
) − X(ω)| < } for any  > 0 and ω ∈ .
Proposition 3.18 For a random variable X, we have N (X≥) = S(X≥) and
N (X≤) = S(X≤) = N (−X≥).
Definition 3.17 For a random variable X, we define the surprise of X as
S(X) := N (X≥).
This definition provides a simple relation between the surprise of a description d
and the surprise of its novelty Nd (see Definition 3.5).
Proposition 3.19 For any description d, we have
Sd (ω) = N ([Nd ≥ Nd (ω)]) and therefore S(d) = N (N≥
d ) = S(Nd ).
3.7 Technical Comments
Here we introduce information, novelty, and surprise as expectation values of
appropriate random variables. For Shannon information, this idea was occasionally
used (e.g., Khinchin 1957), but it was always restricted to partitions, i.e., to
complete descriptions in our terminology. The more general idea of an arbitrary
description, although quite natural and simple, has never appeared in the literature.
In this exposition, I have adopted the strategy to disregard propositions of zero
probability, because this provides an unrestricted application domain for the ideas3.8 Exercises 43
introduced. As mentioned in Chap. 2, this more general approach entails some
technical difficulties involved in some of our definitions and propositions, mostly
concerning measurability and nonempty sets of probability 0. These difficulties are
dealt with in some of the footnotes. Another possibility would have been to develop
everything for discrete probability spaces first, assuming p(ω) = 0 for every ω ∈ ,
and extend it to continuous spaces later. This is often done in elementary treatments
of information theory.
The new concept of surprise will provide a bridge from information theory to
statistical significance. In earlier papers (Palm 1981), I have called it normalized
surprise.
The concept of novelty was first introduced in Palm (1981) by the name of
“evidence.” The problem here is to find yet another word, which has not too many
different connotations. Today, I believe that “novelty” is the more appropriate word,
for such reasons. Proposition 3.14 gives the classical definition of information
(Shannon and Weaver 1949).
The concept of a consequential description (Definition 3.11) and the following
Propositions 3.5–3.9 are perhaps a bit technical. These ideas are taken up again in
Part IV.
3.8 Exercises
(1) For the descriptions given in Examples 3.1 and 3.5–3.7, determine their
completion, their tightening,their novelty, their information, and their surprise.
(2) Let  = {0,..., 999} and consider the following random variables describing
these numbers:
X(ω) := first digit of ω,
Y (ω) := last digit of ω,
Z(ω) := number of digits of ω, for every ω ∈ .
What are the corresponding descriptions; what is the information content of X,
Y , and Z; and what is the corresponding surprise (assuming equal probabilities
for all thousand numbers)?
(3) Measuring the height of a table by means of an instrument with an inaccuracy
of about 1 mm can be described by two different descriptions on  =
[500, 1500] (these are the possible table heights in mm):
d1(ω) :=  ∩ [ω − 0.5, ω + 0.5] and
d2(ω) := [i, i + 1] for ω ∈ [i, i + 1) for i = 500, 501,..., 1499.44 3 Improbability and Novelty of Descriptions
What is the completion in these two cases and what is the average novelty,
information, and surprise (assuming a uniform distribution of table heights
on )?
(4) Prove Proposition 3.10 on page 35.
(5) Is it true that I(X) = I(X2) for any discrete random variable X?
(6) Give an example for a pair (X, Y ) of two random variables where
(X, Y )  = X ∩ Y
 = X
· Y .
(7) Let  = {1,...,n} and X = id:  → R. For equal probabilities on , what
is N (X≥), N (X≤), N (X)  ? What is the limit for n → ∞ in each of the three
cases? Observe that X = X≤ ∩ X≥. For which values of n is N (X≤ ∩ X≥) >
N (X≤) + N (X≥)?
(8) Prove Proposition 3.16 on page 39.
Hint: Remove the constraint n
i=1
p(A
i) = 1 by expressing p(A
n) as a
function of p(A
1), . . . , p(A
n−1). Then compute a local extremum by setting
the derivatives of I(d) to 0.
(9) Given a probability space (, 	, p) and the events A,B ∈ 	. We say
• A supports B, if p(B|A) > p(B)
• A weakens B, if p(B|A) < p(B)
If A supports B, which of the relations “supports” and “weakens” hold for the
following expressions?
(a) A and Bc
(b) B and A
(c) Ac and Bc
(d) Bc and A
(10) Let c, d be descriptions. We say
• c supports d, if p(c ∩ d) > p(c) · p(d)
• c weakens d, if p(c ∩ d) < p(c) · p(d)
• c is independent of d, if p(c ∩ d) = p(c) · p(d)
Let  = {1,..., 6}. Give examples for c and d such that they
(a) Weaken each other
(b) Support each other and
(c) Are independent of each other
(11) Determine the tightening of the descriptions in Example 3.1 and 3.6.
(12) Is it possible that N (d∩) > N (d)˜ ? If yes, give an example; if no, give a proof.
Hint: Consider, for example, (, 	, p) = D2 (two dice) with the random
variables X1, X2 for the two dice. Let d(1, 1) = [X1 = 1], d(1, i) = [X2 =
i] and d(i, 1) = [X2 = 1] for i > 1, and d(i, j ) = [X1 = i] for i, j > 1.References 45
Then d∩(i, j ) = {(i, j )} for every (i, j ) ∈ .
(13) Given a description d on (, 	, p). The function P :  → [0, 1] defined
by P (ω) = p(d(ω)) is a random variable. Can you give an example for a
description d for which
(a) p[P ≤ x] = x,
(b) p[P ≤ x] = x2,
(c) p[P ≤ x] = 1
2 + x
2
for every x ∈ [0, 1]?
Hint: This problem is a bit nasty because you have to consider the probability
of a probability. The basic idea to construct such examples is to consider
 = [0, 1] with the uniform distribution and a continuous, strictly increasing
function Y on it. For the description d(x) := [Y ≤ x]∪{x}, we obtain
P (x) = p[Y ≤ x] = p(Y −1([0, x])) = Y −1(x), and therefore p[P ≤ a] =
p[Y −1 ≤ a] = Y (a).
(14) Determine all complete, all directed, and all tight descriptions on  =
{1, 2, 3}.
(15) Let  = {1,..., 8}. Let c(i) = {1, 2, 3, 4} for i = 1, 2, 3, 4, and c(i) = 
for i = 5, 6, 7, 8, and d(i) = {2,..., 6} for i = 2,..., 6, d(i) =  for
i = 1, 7, 8. Calculate N , S, and I for c, d, and c ∩ d.
(16) Let  = {1,..., 6}, c(1) = c(2) = {1, 2}, c(i) =  for i = 3,..., 6, and
d(1) = d(6) = , d(i) = {2,..., 5} for i = 2,..., 5. Calculate N , S, and I
for c, d, and c ∩ d.
References
Ash, R. B. (1965). Information theory. Interscience.
Billingsley, P. (1978). Ergodic theory and information. Robert E. Krieger Publishing Co.
Boltzmann, L. (1887). Über die mechanischen Analogien des zweiten Hauptsatzes der Thermo￾dynamik. Journal für die reine und angewandte Mathematik (Crelles Journal), 100, 201–212.
Brush, S. G. (1966). Kinetic theory: Irreversible processes (Vol. 2). Pergamon Press, Oxford.
Bapeswara-Rao, V. V., & Rao, M. B. (1992). A three-door game show and some of its variants.
The Mathematical Scientist, 17, 89–94.
Clausius, R. J. E. (1865). Über verschiedenen für die Anwendung bequeme Formen der Hauptgle￾ichungen der mechanischen Wärmetheorie. Annales de Physique, 125, 353–400.
Gallager, R. G. (1968). Information theory and reliable communication. Wiley.
Gardner, M. (1969). The unexpected hanging and other mathematical diversions. Simon and
Schuster.
Gardner, M. (1959). Mathematical games column. Scientific American.
Gillman, L. (1992). The car and the goats. American Mathematical Monthly, 99(1), 3–7.
Granberg, D., & Brown, T. A. (1995). The Monty hall Dilemma. Personality and Social Psychology
Bulletin, 21(7), 711–723.
Khinchin, A. (1957). Mathematical foundations of information theory. Dover Publications, Inc.
Palm, G. (1981). Evidence, information and surprise. Biological Cybernetics, 42(1), 57–68.
Selvin, S. (1975). On the Monty Hall problem [Letter to the editor]. The American Statistician,
29(3), 134.46 3 Improbability and Novelty of Descriptions
Seymann, R. G. (1991). Comment on let’s make a deal: The player’s Dilemma. The American
Statistician, 45(4), 287–288.
Shannon, C. E., & Weaver, W. (1949). The mathematical theory of communication. University of
Illinois Press.Chapter 4
Conditional and Subjective Novelty
and Information
This chapter introduces some more involved versions of the concepts of novelty
and information, such as subjective and conditional novelty.
4.1 Introductory Examples
You are playing cards and you are about to bet on a card that it is an ace. Your
neighbor, who could have seen the value of the card, whispers to you “Do not do
that! If this card is an ace, my uncle is the pope.” The card turns out to be an ace.
Now you know that the pope is his uncle.
From the point of view of information theory, here you have combined two
statements of low information content to achieve a statement of very high infor￾mation content. Classical information theory tells you that this is not possible
(on average): the combined information content cannot exceed the sum of the two
separate information contents, or equivalently, the conditional information of X
given Y cannot exceed the unconditional information.
This is true for random variables X and Y and also for complete descriptions
(as is shown in this chapter). However, it is not always true for arbitrary descriptions.
More exactly, we are able to show additivity of novelty for arbitrary descrip￾tions, i.e., the combined novelty of c and d is the sum of the novelty of c and
the conditional novelty of d given c. But this conditional novelty can be much
larger than the unconditional novelty. Perhaps the simplest example of this is the
description X by a random variable X and its complement Xc as defined in Sect. 3.
In this case, for every ω ∈  we have X(ω)  = [X = X(ω)] and Xc(ω) = [X =
X(ω)]∪{ω}. So X(ω)  ∩ Xc(ω) = {ω}.
The combined information of X and Xc completely determines ω ∈ , whereas
the information of X and of Xc both can be rather small.
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_4
4748 4 Conditional and Subjective Novelty and Information
To come back to our example, let us assume there are two random variables X
and Y on . X ∈ {1,..., 8} determines the value of a card (in a deck of 32 cards),
so I (X) = N (X)  = 3 bit. The novelty of Xc is even smaller, because p(Xc(ω)) ≥
p[X = X(ω)] = 7
8 for every ω ∈  and so N (Xc) ≤ − log2
7
8 ≈ 0.1926.
The variable Y ∈ {1,..., 220} indicates one out of 220, i.e., roughly a million
people who is the nephew of the pope.
Now we can consider the two descriptions c(ω) = {ω
: X(ω
) = X(ω)} and
d(ω) = {ω : X(ω
) = X(ω) or Y (ω
) = Y (ω)}. Here
Nc(ω) = − log2 p(c(ω)) = − log2
1
8 = 3,
Nd (ω) = − log2 p(d(ω)) = − log2( 7
8 + 1
8 · 2−20) ≈ 0.1926 and
Nc∩d (ω) = − log2 p(c(ω) ∩ d(ω))
= − log2 p({ω
: X(ω) = X(ω
) and Y (ω) = Y (ω
)})
= − log2( 1
8 · 2−20) = 23.
Another example for this phenomenon is the following. One person tells you:
“John is quite heavy. I am sure his weight is at least 80 kilos.” Another person
tells you: “John is not so heavy. I am sure his weight is at most 80 kilos.” If both
statements are true, you know that John’s weight is exactly 80 kilos. If X denotes
John’s weight, we may assume that the first person describes it by X≥ and the second
person by X≤. Now clearly X≥ ∩ X≤ = X (see Sect. 3), which can have large
information content, and we have seen (Proposition 3.11) that the novelty of both
X≥ and X≤ is at most log2 e ≈ 1.4427.
4.2 Subjective Novelty
Before we can proceed, we need some additional notation. Up to now, we have only
considered one probability p on 	. In the following, we shall sometimes consider
different probabilities p, q,... on 	, and, if necessary we shall write Nq (A) for the
novelty of A taken with respect to the probability q and Eq (X) for the expectation
of X for the probability q. Furthermore, we shall write
Nq (d) := Eq (Nq ◦ d) and similarly for Iq and Sq .
Definition 4.1
Npq (d) := Ep(Nq ◦ d)4.2 Subjective Novelty 49
is called the subjective novelty of d (believing in q while p is the correct
probability).
Gpq (d) := Npq (d) − Np(d)
is called the novelty gain between q and p.
For a discrete random variable X and two probabilities p and q, we define the
subjective information of X as
Npq (X) := Npq (X), 
the information gain as
Gpq (X) := Gpq (X), 
and the subjective surprise as
Spq (X) := Npq (X≥)
Remark For d = d
, Gpq (d) is also called the information gain or the Kullback–
Leibler distance between p and q (with respect to d) (Kullback 1959, 1968;
Kullback and Leibler 1951).
Proposition 4.1 Gpq (d) ≥ 0 for any complete description d.
Proof The assertion Gpq (d) = Ep(− log(q ◦ d) + log(p ◦ d)) ≥ 0 is independent
from the base of the logarithm. Since the proof is the least clumsy for the natural
logarithm, which satisfies the simple inequality ln x ≤ x − 1, we use it here:
−Gpq (d) = Ep

log q(d(ω))
p(d(ω))
= 
D∈R(d)
p(D) · log q(D)
p(D)
ln x ≤ x − 1
≤ 
D∈R(d)
p(D)  q(D)
p(D) − 1

= 
D∈R(d)
q(D) − 
D∈R(d)
p(D) = 1 − 1 = 0.
Thus, Gpq (d) ≥ 0. Here the summation extends over all propositions D in the range
R(d) of d. 50 4 Conditional and Subjective Novelty and Information
Remark The inequality ln x ≤ x −1 is very important for information theory; it can
be used to prove most of the interesting inequalities.
The following example shows that Gpq (d) can be negative for descriptions d that
are not complete.
Example 4.1 Take a description d defining a bet on A ⊆ , i.e., d(ω) := A for
ω ∈ A and d(ω) :=  for ω /∈ A. If p(A) < q(A), then
Gpq (d) = p(A) · log2
p(A)
q(A)
< 0.

Proposition 4.2 If c and d are two complete descriptions, then c ⊆ d implies
Gpq (c) ≥ Gpq (d).
Proof We use the idea of Proposition 4.1:
Gpq (d) − Gpq (c) = Ep

log p(d(ω))
q(d(ω))
q(c(ω))
p(c(ω))
= 
C∈R(c)
p(C)log p(d(ω))
q(d(ω))
q(C)
p(C)
= 
D∈R(d)
p(D) 
C⊆D
p(C)
p(D) logp(D)
q(D) ·
q(C)
p(C)
≤ 
D
p(D) 
C⊆D
p(C)
p(D)p(D) · q(C)
p(C) · q(D) − 1

= 
D
p(D) 
C⊆D
q(C)
q(D) − 
C⊆D
p(C)
p(D)
   =0

= 0

4.3 Conditional Novelty
When we use conditional probabilities, i.e., probabilities pA on  given by
pA(B) = p(B|A), we shall further simplify the notation by writing NA instead
of NpA and similarly for EA, NA, IA, and SA.4.3 Conditional Novelty 51
Clearly, NA(d) will be referred to as the novelty of d under the condition A, or
the novelty of d given A. Similarly for IA(d) and SA(d). Sometimes the following
notation is also useful:
EA(X) = E(X|A) conditional expectation,
NA(d) = N (d|A) conditional novelty,
SA(d) = S(d|A) conditional surprise, and
IA(d) = I(d|A) conditional information
Now we want to define the novelty of a description d given another description c.
The simplest way of defining this is to consider the mapping p(d|c):  → R,
defined by
p(d|c)(ω) := p(d(ω)|c(ω)) for each ω ∈ .
Definition 4.2 The mapping
Nd|c(ω) := − log2 p(d(ω)|c(ω))
defines the novelty of d given c for the event ω ∈ . In addition, we define
N (d|c) := E(Nd|c).
But there is also a different way of interpreting the novelty of d given c. Indeed,
if we know the description c for an event ω, we also know that ω is described by c,
i.e., that c(ω) = c, i.e., ω ∈ [c = c(ω)] = c(ω). Thus, we might as well say that
the novelty of d given c is really N (d|c ).
There is still another quite reasonable definition for the novelty of d given c,
namely, to use the average of Nc(ω)(d) (the novelty of d given c(ω)), i.e., to
define N 
(d|c) = E(Nc(ω)(d)). In general, N 
(d|c) = N (d|c) (see the following
example), but it turns out that the two definitions coincide if c is complete.
Example 4.2 Consider  = {1, 2} with probabilities p({1}) = 3
4 and p({2}) = 1
4 .
Define descriptions c, d with
c(1) = {1}, c(2) = {1, 2}, d(1) = {1} and d(2) = {2}.
Then we have R(c) = {C1, C2} = {{1},{1, 2}}, R(d) = {D1, D2} = {{1},{2}},
and correspondingly R(c ) = {C
1,C
2} = {{1},{2}} = {D1, D2} = R(d)  .52 4 Conditional and Subjective Novelty and Information
Now we can calculate
N (d|c) = −
2
i=1

2
j=1
p(C
i ∩ Dj )log2 p(Dj |Ci) = ... = 1
2
and
N 
(d|c) = Eω(Sc(ω)(d)) = 
2
i=1
p(C
i)ECi(sCi(d))
= −
2
i=1
p(C
i)

2
j=1
p(Dj |Ci)log2 p(Dj |Ci) = ... = 1
2 − 3
16 log2 3.
So in general, N 
(d|c) = N (d|c). 
Proposition 4.3 For c complete, we have
N (d|c) = N 
(d|c) = N (d|c ).
Proof Clearly, N (d|c) = N (d|c ) since c = c. Now N 
(d|c) is defined as
E(Nc(ω)(d)) = 
C∈c()
p(C)NC(d)
= 
C∈c()
p(C)EC(− log2 p(d(ω)|C))
= 
C∈c()
p(C)EC(Nd|c)
= E(Nd|c(ω)) = N (d|c). 
We now collect some properties of novelty N and information I concerning the
relation ⊆ and the intersection of descriptions.
Proposition 4.4 Let c and d be two descriptions. Then Nc + Nd|c = Nd∩c. Thus
N (c) + N (d|c) = N (d ∩ c).
Further the following propositions are equivalent:
(i) Nd|c = Nd
(ii) Nc|d = Nc
(iii) p(c(ω) ∩ d(ω)) = p(c(ω)) · p(d(ω)) for every ω ∈ .
Proof Obvious. 
The first equation in this proposition implies is called the additivity of novelty.
In classical information theory, monotonicity and subadditivity are the most
important properties of information. Since for complete descriptions information4.3 Conditional Novelty 53
and novelty coincide with the classical concept of information, both measures have
these two properties on complete descriptions, but for general descriptions, novelty
is monotonic (Proposition 3.3) and not subadditive (Exercises (3.7) and (4.5) on
page 58), whereas information is subadditive (cf. the following Proposition 4.5)
but not monotonic (cf. the following example). The last assertion is quite obvious
because c ⊆ d does not imply c ⊆ d (see Example 3.6 and the subsequent
discussion). The subadditivity of information is the subject of the next proposition.
Example 4.3 Consider  = {1,..., 32} with equal probabilities on , i.e.,
(, 	, p) = E32. Define
a(1) = {1},
a(32) = {32}, and for all other i ∈ 
a(i) = {2,..., 31}.
and define
b(i) = {1,..., 31} for i = 1,..., 16,
b(i) = {2,..., 32} for i = 17,..., 32.
Then a ⊆ b but I (a) ≤ I (b).
Another example was given in Example 3.4. 
Proposition 4.5
I(c ∩ d) ≤ I(c) + I(d).
and
I(c ∩ d) = I(c) + I(d)
if and only if p

(c∩ d)(ω)
= p(c(ω)) · p(d(ω))  for (almost) every ω ∈ .
Proof Let R(c ) = {C1,...,Cn} and R(d)  = {D1,...,Dn}. We have c ∩ d ⊆
c
∩ d. Thus
I(c ∩ d) = N (c
∩ d) ≤ N (c ∩ d)  ≤ N (c ) + N (d), 54 4 Conditional and Subjective Novelty and Information
because
N (c ∩ d)  − N (c ) − N (d)  = −n
i=1

k
j=1
p(Ci ∩ Dj )log2 p(Ci ∩ Dj )
+n
i=1
p(Ci)log2 p(Ci)+

k
j=1
p(Dj )log2 p(Dj )
= n
i=1

k
j=1
p(Ci ∩ Dj ) · log2
p(Ci)p(Dj )
p(Ci ∩ Dj )
≤ n
i=1

k
j=1
p(Ci ∩ Dj ) · 1
ln 2 ·
p(Ci)p(Dj )
p(Ci ∩ Dj ) − 1

= 1
ln 2
⎛
⎝n
i=1

k
j=1
p(Ci)p(Dj )−
n
i=1

k
j=1
p(Ci ∩ Dj )
⎞
⎠
= 0.
The inequality holds because log2 x = ln x
ln 2 ≤ x − 1
ln 2 .
In order to obtain equality in both inequalities in this proof, we need that
p(Ci ∩ Dj ) = p(Ci) · p(Dj ) for every i and j (with Ci ∩ Dj = ∅) and that
p

(c∩ d)(ω)
= p 
c(ω) ∩ d(ω)  
for every ω ∈ . 
In order to obtain additivity also for information, we have to define conditional
information.
Definition 4.3 For two descriptions c and d, we define the conditional information
of d given c as
I(d|c) := N (d
|c ).
Proposition 4.6 Let c and d be tight descriptions. Then
I(c ∩ d) = I(c) + I(d|c).
Proof Obvious from Proposition 4.4 and Proposition 3.6. 
Together with Proposition 3.6, we have now shown that on tight descriptions,
the information I has all the classical properties: monotonicity, subadditivity, and
additivity. The novelty N , however, lacks subadditivity.4.4 Information Theory for Random Variables 55
Proposition 4.7 Let c and d be two descriptions and R(c) finite. Then
I(d|c) = 
A∈R(c )
p(A)I(d|A).
Proof follows immediately from Proposition 4.3. 
Definition 4.4 For two descriptions c and d, we define their mutual novelty as
M(c, d) := N (c) + N (d) − N (c ∩ d)
and their transinformation1 as
T (c, d) := I(c) + I(d) − I(c ∩ d).
Proposition 4.8 Let c and d be descriptions. Then
(i) T (c, d) ≥ 0,
(ii) M(c, d) = N (c) − N (c|d) = N (d) − N (d|c),
(iii) If c and d are tight, then T (c, d) = I(c) − I(c|d) = I(d) − I(d|c).
Proof
(i) From Proposition 4.5,
(ii) From Proposition 4.4,
(iii) From Proposition 4.6. 
4.4 Information Theory for Random Variables
We have defined information in general for random variables by I(X) := N (X)  .
Now we can also define I(Y |X) := N (Y
|X)  . Furthermore, we can define X  Y
if σ (X) ⊆ σ (Y ) (see Definition 2.4). With these notions, we get the usual relations
on the information of random variables.
Proposition 4.9 Let X, Y , and Z be discrete random variables.
(i) The pair (X, Y ) is also a random variable, defined by
(X, Y )(ω) := (X(ω), Y (ω))
and we have (X, Y ) " = X ∩ Y
.
1 Classical information theory defines T (c, d) as the transinformation or mutual information.
Here we distinguish between transinformation and mutual novelty.56 4 Conditional and Subjective Novelty and Information
(ii) I(X, Y ) ≤ I(X) + I(Y ) (subadditivity)
(iii) X  Y implies I(X) ≤ I(Y ) (monotonicity)
(iv) I(X, Y ) = I(X) + I(Y |X) (additivity)
(v) 0 ≤ I(Y |X) ≤ I(Y )
(vi) X  Y implies I(X|Z) ≤ I(Y |Z) and I(Z|X) ≥ I(Z|Y )
Proof Exercise (8) 
Definition 4.5 The transinformation or mutual information between two discrete
random variables X and Y is defined as
T (X, Y ) := I(X) + I(Y ) − I(X, Y ).
From Proposition 4.9.(ii), we can infer that T (X, Y ) ≥ 0.
Proposition 4.10 Let X and Y be two random variables, then
T (X, Y ) = I(X) − I(X|Y ) = I(Y ) − I(Y |X).
Proof This follows immediately from Proposition 4.4. 
Proposition 4.10 points at an interpretation of T (X, Y ): I(X) is the average
novelty we get from a value of X, I(X|[Y = b]) = I[Y = b](X) is the average novelty
we still get from a value of X when we already know that Y = b, and I(X|Y ) is the
average novelty we get from a value of X when we know a (random) value of Y . The
difference is the amount of novelty obtained from X that is removed by knowing Y .
In other words, T (X, Y ) is the amount of information about X that we get from
knowing Y .
Proposition 4.11 Let X, Y , and Z be three random variables. Assume that X and
Z are independent given Y , i.e.,
p[X=a,Z =c | Y =b] = p[X=a | Y =b] · p[Z =c | Y =b]
for every a ∈ R(X), b ∈ R(Y ) and c ∈ R(Z). Then,
T (X, Z) ≤ min(T (X, Y ), T (Y, Z)).
Proof
T (X, Z) = I(X) − I(X|Z) ≤ I(X) − I(X|(Y, Z))
(∗)
= I(X) − I(X|Y ) = T (X, Y ).4.5 Technical Comments 57
Equality (∗) holds because
−NX|(Y,Z) " (ω) = log2
p

X(ω)  ∩ Y (ω)  ∩ Z(ω)  
p

Y (ω)  ∩ Z(ω)  
= log2
p

X(ω)  ∩ Z(ω)  |Y (ω)  
p

Z(ω)  |Y (ω)  
= log2
p

X(ω)  |Y (ω))p(  Z(ω)  |Y (ω)  
p

Z(ω)  |Y (ω)  
= −NX|Y
(ω) 
The concept of mutual information can also be defined for three and more random
variables. We define T (X, Y, Z) := T (X, Y )−T (X, Y | Z). This definition is again
symmetric in X, Y, and Z, because
T (X, Y, Z) = I(X) + I(Y ) + I(Z)
− I(X, Y ) − I(Y, Z) − I(X, Z)
+ I(X, Y, Z).
T (X, Y, Z) can have both positive and negative values in contrast to T (X, Y ) (see
Exercise 15). This definition can of course be extended to more than three variables,
see, for example, Attneave (1959); Bell (2003).
4.5 Technical Comments
This chapter already contains the essential definitions and proofs for the theory of
novelty and surprise to be developed further in Part VI of this book. It also proves
the basic properties of information (additivity, subadditivity, and monotonicity)
for random variables, which are the fundament of classical information theory
(cf. classical books like Cover and Thomas 1991; Cziser and Körner 1982; Heise
and Quattrocchi 1989; Reza 1994).
The basic ideas of information theory introduced so far are also sufficient
for the widespread recent practical applications of information theory in pattern
recognition, machine learning, and data mining (e.g., Amari and Nagaoka 2000;
Deco and Obradovic 1996; MacKay 2005), most of them based on the concepts
of transinformation and information gain or Kullback–Leibler distance. In theory,
some of these applications actually need the continuous versions of these concepts.
This issue is discussed in Chap. 12.58 4 Conditional and Subjective Novelty and Information
4.6 Exercises
(1) What is T (X, Y ), T (Y, Z), T (Z, X) for the three random variables X, Y , and
Z defined in Exercise (3.2)?
(2) Compute N (a), N (b), I(a), I(b), M(a, b), and T (a, b) for Example 4.3 on
page 53.
(3) Compute T (c, d) for Example 3.6 on page 32 and T (c, d), T (c, e), T (d, e)
for Example 3.1 on page 27.
(4) Given two dice, i.e.,  = {1,..., 6}×{1,..., 6}. Let X1 = first dice, X2 =
second dice, D = “doublets,” i.e.,
D(i, j ) =

1 if i = j,
0 otherwise.
Determine T (X1, X2), T (X1,D), T (X2,D), T (X1, (X2, D)), T ((X1, X2),
D).
(5) Find examples that show that in general N (c ∩ d) ≤ N (c) + N (d),
i.e., M(c, d) ≥ 0, is false.
(6) In a container, there are w white and r red balls. Two balls are drawn
consecutively. What is the uncertainty or novelty of the outcome of the first
drawing as compared to the (conditional) uncertainty of the second drawing?
(7) There are 12 balls of equal size, but one of them is slightly heavier or lighter
than the others. You have a simple balance scale with two pans and are allowed
to put any number of balls on either pan in order to find out the odd ball and
the direction of its weight difference to the others. How many weighings (each
with three possible outcomes) do you need? How can information theory help
in answering that question?
(8) Show Proposition 4.9 on page 55.
(9) Show the following:
Proposition 4.12
(i) I(X|Y, Z) + I(Y |Z) = I(X, Y |Z),
(ii) X ⊆ Y
 implies I(X|Z) ≥ I(Y |Z) and I(Z|X) ≤ I(Z|Y ),
(iii) I(X|Y, Z) ≤ min{I(X|Y ), I(X|Z)}.
(10) Prove or refute each of the following statements:
(a) T (X, Z) ≤ max(T (X, Y ), T (Y, Z))
(b) T (X, Z) ≤ T (X, Y ) + T (Y, Z)
(c) T ((X, Z), Y ) ≤ T (X; Y ) + T (Y ;Z).
(11) There are three different “40-cent coins,” each of these coins consisting of two
20-cent coins glued together (head-to-head, head-to-tail, or tail-to-tail). One
of these three coins is drawn randomly and placed on a table. The random
variables X and Y denote the top and bottom side of the drawn coin and can4.6 Exercises 59
take the values “h” (head) or “t” (tails). Describe this experiment by means
of a probability space. Compute p[X = Y ], p[X = t], p[X = Y |Y = t], p[X =
t|Y = h], and T (X, Y ).
(12) Let X, Y, Z be discrete random variables. Show the following:
2I(X, Y, Z) ≤ I(X, Y ) + I(Y, Z) + I(X, Z).
Hint: You can begin with I (X, Z) = I (X) + I (Z|X) ≥ I (X) + I (Z|(X, Y ))
and add to this I (X, Y ) = I (X, Y, Z) − I (Z|(X.Y )) and I (Y, Z) =
I (X, Y, Z) − I (X|(Y, Z)).
(13) Let  = {1,..., 6} and define d:
d(1) = {1}, d(2) = {1, 2, 3, 4}, d(3) = {1, 3, 5}
d(4) = {2, 4, 6}, d(5) = {3, 4, 5, 6}, d(6) = {6}.
Define a description c as c(i):= {1,...,i} for every i ∈ . Use Nc|d as defined
in Definition 4.2.
(a) Calculate the expectation value E(Nc|d ).
(b) Compare E(Nc|d ) with N (c ∩ d) − N (d).
(c) Show the following or give a counterexample:
(i) Does E(Nc|d ) ≥ 0 hold in general?
(ii) Does E(Nc|d ) ≤ N (c) hold in general?
(14) On a small group of islands, an old telegraph system was found. The inhab￾itants used it to play games of dice between the different isles. The device
is still working, but not like it should do. Experiments showed the following
transmission behavior:
1 → 6 2 → 3 3 → (3, 5)
4 → (2, 4) 5 → 2 6 → (1, 5)
The transmission in ambiguous (faulty) cases is equally probable, i.e., if one
tries to transmit a 3, a 3 is received with a probability of 50%; however, a 5 is
also received with probability of 50%. Let X denote the input value (outcome
of a dice cast) and Y the actual output value. Specify a suitable guessing
strategy such that the actual value of X can be determined from Y with the
smallest possible error. Let Z be the guessed value (i.e., Z is a function
of Y ). Calculate the error probability p[X = Z], the conditional probability
p([X = Z|Z = k]) for each k, and the transinformation T (X, Z).
(15) T (X, Y, Z) as defined on page 57 can have both positive and negative values
in contrast to T (X, Y ). Give examples for this.60 4 Conditional and Subjective Novelty and Information
References
Amari, S., & Nagaoka, H. (2000). Methods of information geometry. AMS and Oxford University
Press.
Attneave, F. (1959). Applications of information theory to psychology. Holt, Rinehart and Winston.
Bell, A. J. (2003). The co-information lattice. In Proceedings of the 4th International Symposium
on Independent Component Analysis and Blind Signal Separation (ICA2003), (pp. 921–926).
Cover, T. M., & Thomas, J. A. (1991). Elements of information theory. Wiley-Interscience.
Cziser, I., & Körner, J. (1982). Information theory. Academic Press.
Deco, G., & Obradovic, D. (1996). An information-theoretic approach to neural computing.
Springer.
Heise, W., & Quattrocchi, P. (1989). Informations- und Codierungstheorie. Springer.
Kullback, S. (1959). Information theory and statistics. Wiley.
Kullback, S. (1968). Information theory and statistics. Dover.
Kullback, S., & Leibler, R. A. (1951). On Information and Sufficiency. The Annals of Mathematical
Statistics, 22(1), 79–86.
MacKay, D. J. C. (2005). Information theory, inference, and learning algorithms. Cambridge
University Press.
Reza, F. M. (1994). An introduction to information theory. Dover Publications, Inc.Part II
Coding and Information TransmissionChapter 5
On Guessing and Coding
In Chap. 3, we defined the novelty of a proposition as a special function of its
probability p(A). We motivated the definition N (A) = − log2 p(A) by the idea
that N (A) should measure the number of yes–no questions needed to guess A. We
have extended this notion of novelty to the (average) novelty of a description d
as E(N ◦ d) = N (d), and we introduced the slightly more complicated notion
of information I(d) = N (d)  . We shall now investigate the strategies for smart
guessing, and, in so doing, we will attain a better understanding of concepts like
novelty and information in terms of the number of yes–no questions.
This chapter introduces the Huffman code (Huffman 1952). The ideas of coding
and optimizing average codeword length are essential for understanding the concept
of information and the closely related concepts of novelty and surprise. The chapter
contains no new material. Error-correcting codes (e.g., Hamming 1950; Bose and
Ray-Chaudhuri, 1960) are not considered, because they lead to difficult, more
algebraic considerations and are not related to the new concepts developed in this
book.
5.1 Introductory Examples
Assume you have to find out the value of a dice by asking yes–no questions. How
many questions do you need?
One strategy is depicted in Fig. 5.1. With this strategy, we need two or three
questions. For 1 or 2, we need two questions, and for 3, 4, 5, or 6, we need three.
On average we need 1
3 · 2 + 2
3 · 3 = 2 2
3 questions. Is there a better strategy?
In this section, we will show how to find the best guessing strategy, that there is
no better strategy for the dice, that a formula for the average number of questions in
an optimal strategy, and finally, that this number is close to the information content
(for the dice the information is log2 6 ≈ 2.585).
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_5
6364 5 On Guessing and Coding
Is it odd?
yes
Is it 1?
yes
1
no
Is it 3?
yes
3
no
5
no
Is it 2?
yes
2
no
Is it 4?
yes
4
no
6
First question:
Second question:
Third question:
Fig. 5.1 Example of a question strategy
Is there an ace among the first 3 cards?
yes
Is it card 1?
yes
1
no
Is it card 2?
yes
2
no
3
no
Is card 4 or 5 an ace?
yes
Is it card 4?
yes
4
no
5
no
Is it card 6?
yes
6
no
7,8
First question:
Second question:
Third question:
Fig. 5.2 Example of a question strategy
A similar example is the following: You are shown a deck of eight cards
containing two aces. The deck is shuffled and the cards are put on the table (face
down). Your task is to find one ace by asking yes–no questions. How many questions
do you need?
This task turns out to be much harder. In fact, the theory provided in this chapter
does not solve it, nor does classical information theory. We will return to it in
Chap. 11. A reasonable strategy for this task could be the following (see Fig. 5.2):
This strategy needs three questions, except when card 1 is an ace; in this case, it
needs two. The probability for this is 1
4 . So on average, this strategy needs 1
4 · 2 +
3
4 · 3 = 2 3
4 questions.
Is there a better strategy? The answer will be given in Chap. 11. There are good
reasons why the best strategy should need between two and three questions, but it
may be surprising that the result is closer to three. After all, with three questions,5.2 Guessing Strategies 65
one can find one ace among eight cards. Adding another ace to the deck doubles the
probability of hitting an ace (from 1
8 to 1
4 ), so should we not be able to do it in two
questions? This proves to be impossible after a few trials. Another argument for an
information content of two goes as follows. Let us assume one of the aces is black
and one red. In order to determine the color of the ace we have found, we need one
additional bit of information. To find out one ace plus its color (e.g., the red ace),
we need three questions, i.e., 3 bits of information. So the information content of
the localization of one ace should be 3 − 1 = 2 bits (if information is additive).
Well, it turns out that in this problem, information is not additive.
5.2 Guessing Strategies
Consider the following problem:
Given a description d with finite range, find guessing strategies for d,
i.e., sequences of yes–no questions in order to determine d(ω) for any given
(but unknown to the asker) event ω, minimizing the average number of questions
needed.
Let us assume that R(d) = {A1,...,Ak}, and let Li be the number of questions
needed in a particular guessing strategy to determine the proposition Ai. Further￾more, let L(ω) denote the number of questions needed in that guessing strategy to
determine d(ω). Then
E(L) = 
k
i=1
Lip({ω ∈ : d(ω) = Ai}) = 
k
i=1
p 
A
i

Li,
where A
i = [d = Ai] as defined in Definition 3.6.
More generally, assume that we want to guess the values of a random variable
X:  → A, where A = {a1,...,an}. Let p[X = ai] = pi. We may summarize
this situation in the “scheme”
a1 ... an
p1 ...pn

.
Again we denote by Li the number of questions needed (in a certain guessing
strategy) to determine ai, and then the average number of questions needed is
E(L) = n
i=1
piLi.
A fixed guessing strategy starts with a fixed first question. Then, depending on
the first answer (yes or no), there are two alternative fixed second questions. Again
in every possible case after the second answer, there is a fixed third question, and so66 5 On Guessing and Coding
000
0
001
1
0
01
1
0
1000
0
0
1010
0
1
0
1110
0
1
1
1 1
2
3
4
Fig. 5.3 Tree picturing a question strategy
on. A useful way of picturing such a guessing strategy and its outcomes is by means
of a tree (Fig. 5.3). It has a certain number of levels; at each level (l = 1,...,k), the
number b(l) of branches corresponds to the number of different cases after the lth
question. It is clear that b(l) ≤ 2l for yes–no questions, and b(l) will indeed usually
be smaller, because whenever one value ai of X is determined (at level Li), we stop
asking questions and therefore cut off the subsequent branches at higher levels. The
number of possible branches at levels l>Li that are thus cut off is 2l−Li . The
highest level k in the tree is, of course, k = n
max
i=1
Li.
At the level k,
n
i=1
2k−Li of the possible 2k branches are used or cut off by the
questioning strategy. Clearly, n
i=1
2k−Li can be at most 2k, i.e., n
i=1
2−Li ≤ 1. This
relation is known as Kraft’s inequality.
5.3 Codes and Their Relation to Guessing Strategies
With the help of trees as pictured in Fig. 5.3, we can now work out an optimal
guessing strategy for X. But before we proceed, we mention a connection between
guessing strategies and codes.
Definition 5.1 A code of the set A = {a1,...,an} in the alphabet B = {b1,...,bm}
is an invertible mapping c : A → B∗, where B∗ is the set of all sequences (of finite
length) of elements from B. Thus
B∗ = ∞
i=1
Bi
,
where Bi is the set of sequences of length i.5.3 Codes and Their Relation to Guessing Strategies 67
Now a fixed guessing strategy can as well be understood as a code of A for
B = {0, 1} if a sequence, like 001011 in B∗, is interpreted as a protocol of the
answers to the fixed sequence of questions in the strategy (0 identified with “no” and
1 with “yes”). In this code, each ai will be uniquely determined by the corresponding
0-1-sequence of length Li.
So the number Li of questions needed in a fixed guessing strategy to determine
ai corresponds to the length l of the codeword c(ai). And the problem of finding an
optimal guessing strategy, which was our starting point, is equivalent to the problem
of finding a 0-1-code for A of minimal average length. The average length of a code
c is defined as
L(c) := n
i=1
pil(c(ai)).
It should be noted that the codes c that occur in this correspondence to guessing
strategies have a particularly nice property: they are irreducible (or prefix-free).
Definition 5.2 A code c : A → B∗ is called irreducible, if there are no two a, a
in A such that c(a) is a beginning (or prefix) of c(a
). A codeword b is called a
beginning of b
, if l(b) ≤ l(b
) and bi = b
i for i = 1, . . . , l(b).
Example 5.1 For A = {a, b, c, d, e, f, g} and B = {0, 1}, consider the two codes
c and c defined by
c(a) = 0, c(b) = 10, c(c) = 1100, c(d) = 1101,
c(e) = 1110, c(f ) = 11110, c(g) = 11111
and
c
(a) = 0, c
(b) = 10, c
(c) = 110, c
(d) = 1101,
c
(e) = 1110, c
(f ) = 0101, c
(g) = 1111.
Which of the two codes is irreducible (compare Exercise 6)? 
If we identify a probability vector p = (p1, p2,...,pn) with a scheme
a1 a2 ... an
p1 p2 ...pn

we may ask for an optimal irreducible 0-1-code for p, i.e., an irreducible code
c : {1,...,n}→{0, 1}∗ with minimal average length L(c). We define L(p) as the
average length L(c) of this code.68 5 On Guessing and Coding
5.4 Kraft’s Theorem
The following theorem relates guessing strategies and irreducible codes.
Theorem 5.1 (Kraft) Let A = {a1,...,an} and let L1,...,Ln be integers. The
following propositions are equivalent:
(i) There is a questioning strategy for A taking L1,...,Ln questions for the n
items in A.
(ii) There is an irreducible 0-1-code for A with codeword lengths L1,...,Ln.
(iii) n
i=1
2−Li ≤ 1.
Proof
(i) ⇐⇒ (ii): is obvious.
(ii) ⇐⇒ (iii): Let us repeat the proof of this fact in the language of coding:
Let c be an irreducible code for A with L1,...,Ln. Let k = n max
i=1
Li.
For a 0-1-sequence w ∈ {0, 1}l with l ≤ k, let Mk(w) = {w ∈
{0, 1}k : w is a beginning of w
}. Then clearly Mk(w) has 2k−l(w) elements.
Now the sets Mk(c(ai)) and Mk(c(aj )) are disjoint for i = j , because the code
c is irreducible. Thus
n
i=1
2k−Li = n
i=1
# (Mk(c(ai))) = #
#
n
i=1
Mk(c(ai))$
≤ #

{0, 1}
k

= 2k
.
Therefore, n
i=1
2−Li ≤ 1.
(iii) ⇐⇒ (ii): Assume that L1 ≤ L2 ≤ ... ≤ Ln.
We select an arbitrary codeword c(a1) of length L1 for a1. Then we select
an arbitrary codeword c(a2) of length L2, which does not have c(a1) as a
beginning. This means that the sets ML2 (c(a1)) and ML2 (c(a2)), defined above,
have to be disjoint. We repeat this procedure until we select c(an). It will work
as long as in every step j , there is a suitable codeword left, i.e., a word of length
Lj that has none of the words c(a1), . . . , c(aj−1) as a beginning. But this is the
case as long as 2Lj >
j
−1
i=1
2Lj−Li , i.e., 1 >
j
−1
i=1
2−Li , which by (iii) is true up
to j = n. 
Now we come back to our original problem of finding an optimal guessing
strategy or a code with minimal average length. The solution to this problem is
the Huffman code. We shall prove this in the next section.5.5 Huffman Codes 69
Example 5.2 Consider the scheme
⎛
⎝
ABCDEF G
1
4
1
4
1
8
1
8
1
8
1
16
1
16
⎞
⎠
Find an optimal irreducible 0-1-code for it. 
5.5 Huffman Codes
Let the scheme  a1 ... an
p1 ... pn
 where p1 ≥ p2 ≥ ... ≥ pn be given. We decide
to distinguish the two possibilities with the lowest probabilities by the very last
question. Let us say, for instance, the last question leads with “yes” or “1” to an and
with “no” or “0” to an−1. This decision leaves us with the reduced scheme
 a1 ... an−2 an−1
p1 ... pn−2 pn−1 + pn

where an and an−1 have merged into an−1. The reduced scheme may now be
reordered so that the probabilities in the lower row decrease from left to right. We
continue this procedure with the same recipe until n = 1. The code obtained by
means of this procedure is called the Huffman code (cf. Huffman 1952).
Lemma 5.1 If c is an optimal code for a scheme with probabilities (p1,...,pn)
that has the corresponding codeword lengths L1,...,Ln and pi<pj , then Li≥Lj .
Proof Otherwise one would get a better code c by choosing c
(ai) = c(aj ),
c
(aj ) = c(ai) and c
(ak) = c(ak) ∀ k = i, j :
L(c) − L(c
) = n
r=1
prLr −
⎛
⎝
r=i,j
prLr + piLj + pjLi
⎞
⎠
= piLi + pjLj − piLj − pjLi
= (pj − pi)(Lj − Li) > 0, if Lj > Li. 
So we can assume that p1 ≥ p2 ≥ ... ≥ pn and that for an optimal code
L1 ≤ L2 ≤ ... ≤ Ln.
Lemma 5.2 Any optimal irreducible code c has an even number of longest
codewords which are pairs of the form (w0, w1) with w ∈ {0, 1}∗.
Proof If w0 was a longest codeword and w1 not, then w could be used as a shorter
codeword instead of w0. 70 5 On Guessing and Coding
Theorem 5.2 The Huffman code h is an optimal irreducible code for the scheme
a1 ... an
p1 ... pn

,
and L(p) can be calculated recursively by
L(p1,...,pn) = L(p1,...,pn−2, pn−1 + pn) + pn−1 + pn, (∗)
if (p1,...,pn) was ordered such that pn−1 and pn are the two smallest elements of
the vector.
Proof By induction on n: Let c be an optimal code for p, and then by Lemmas 5.1
and 5.2, we can assume that the codewords for an−1 and an are among the longest
codewords and (possibly by exchanging longest codewords) that they are of the form
c(an−1) = w0 and c(an) = w1.
Thus, (c(a1), . . . , c(an−2), w) = c is a code for (p1,...,pn−2, pn−1 + pn) and
L(c) = L(c
) + pn−1 + pn. Clearly, c has to be optimal, because otherwise it could
be shortened leading also to a shorter code for p. This proves (∗).
By construction, the Huffman codes h also fulfill (∗), i.e.,
L

h(p1,...,pn)

= L

h(p1,...,pn−2, pn−1 + pn)

+ pn−1 + pn,
so they are optimal. 
In summary, we have now solved the problem of finding an optimal guessing
strategy (or an optimal irreducible code ) for the values a1,...,an of a random
variable X, where the probabilities p[X = ai] = pi are known. It turns out that
the answer (given by the Huffman code) is closely related to the information I(X).
Indeed, Kraft’s inequality shows that the codeword lengths or the numbers Li of
questions needed to determine the ai satisfy n
i=1
2−Li ≤ 1, and this inequality can
be used to establish a relationship between codeword length and information (see
Proposition 5.2 on page 71).
5.6 Relation Between Codeword Length and Information
Proposition 5.1 For numbers pi ≥ 0 and qi ≥ 0 with n
i=1
pi = 1 and n
i=1
qi ≤ 1,
we have
−n
i=1
pi · log2 pi ≤ −n
i=1
pi · log2 qi.5.6 Relation Between Codeword Length and Information 71
Proof This statement has essentially been proved in Proposition 4.1. As in Propo￾sition 4.1, we use the properties of the natural logarithm in the proof. Our assertion
follows from the fact that
−n
i=1
pi ln pi +n
i=1
pi ln qi = n
i=1
pi ln qi
pi
ln x ≤ x − 1
≤ n
i=1
pi(
qi
pi
− 1)
= n
i=1
qi −n
i=1
pi ≤ 0. 
If we now take qi = 2−Li in Proposition 5.1, then Theorem 5.1 holds for the
lengths Li and we see that
I(X) = −n
i=1
pi log2 pi ≤ −n
i=1
pi log2 qi = n
i=1
piLi = E(L).
On the other hand, if we try to work with Li = − log2 pi, then obviously
n
i=1
2−Li ≤ n
i=1
2−(− log2 pi) = 1,
and we can construct a corresponding guessing strategy, which has
E(L) = n
i=1
pi · Li = n
i=1
pi(− log2 pi) ≤ n
i=1
pi(− log2 pi + 1) = I(X) + 1.
Thus, the optimal guessing strategy (or code) has an average number of questions
E(L), which is close to I(X), and we have shown the following.
Proposition 5.2 If L is the number of questions needed in an optimal guessing
strategy for X, then
I(X) ≤ E(L) ≤ n
i=1
pi− log2 pi < I(X) + 1.
Proof See above. 
The interpretation of information as the average number of questions needed
in an optimal strategy or as the minimal average codeword length can be made
even more precise if we consider not only the guessing of one random variable
X but of a sequence of similar random variables Xn :  → A. We shall see this in
Chap. 7. The idea is simply that the difference of 1 in the estimate of Proposition 5.2
can be made arbitrarily small compared to I(X) if one considers variables X with72 5 On Guessing and Coding
sufficiently high information content, or alternatively, if one considers the guessing
of many independent samples of values from the same random variable X. This idea
is carried out below (see Proposition 7.6 on page 98).
Of course, we can apply this interpretation also to conditional information. For
example, for two random variables X and Y , the conditional information I[Y=b](X)
based on the conditional probability p[Y=b](A) = p(A|[Y = b]) can be interpreted
as the number of yes–no questions needed to guess the value of X if one knows that
Y = b. Similarly, I(X|Y ) corresponds to the average number of yes–no questions
needed to guess the value of X if one knows the value of Y .
5.7 Technical Comments
This short chapter contains the classical results on optimal “noiseless” coding that
go back to Shannon (1948; see also Fano 1961; Huffman 1952). They provide
the essential justification for the basic definition (Definition 3.1) of information or
novelty in Chap. 3. They are given here for the sake of comprehensiveness although
they are not related to the new ideas concerning novelty and surprise.
5.8 Exercises
(1) Try to find an optimal guessing strategy for Example 5.2 on page 69 and draw
the corresponding tree.
(2) Given a deck of 16 cards, among them 4 aces. Let Ai = [the ith card is the first
ace, counted from the top of the deck]. Find an optimal code for the scheme
A1...A16
p1...p16

with the corresponding probabilities.
(3) Given a dice, i.e.,  = {1,..., 6} with equal probabilities, find an optimal
code for it.
(4) Consider the information I as a function on probability vectors
p = (p1,...,pn) with pi ≥ 0 and n
i=1
pi = 1 defined by
I(p) = −n
i=1
pi log2 pi.5.8 Exercises 73
Show the following:
Proposition 5.3
(i) For q ∈ (0, 1) and p, p probability vectors, we have
q · I(p) + (1 − q) · I(p
) + I(q, 1 − q) ≥ I(q · p + (1 − q) · p
)
≥ q · I(p) + (1 − q) · I(p
).
(ii) On probability vectors of length n, I takes its maximum at
I

1
n ,..., 1
n

= log2 n
and its minimum at
I(1, 0,..., 0) = 0.
(5) For which probability vectors p = (p1,...,pn) is I(p) = L(p)?
(6) Which of the two codes c and c defined in Example 5.1 is irreducible? For the
other code, find a sequence in B∗ that can be interpreted as the concatenated
code of two different sequences in A∗.
(7) The following table1 contains the frequencies of letters in the German
alphabet. Construct a Huffman code for it.
A 6.51 % H 4.76 % O 2.51 % V 0.67 %
B 1.89 % I 7.55 % P 0.79 % W 1.89 %
C 3.06 % J 0.27 % Q 0.02 % X 0.03 %
D 5.08 % K 1.21 % R 7.00 % Y 0.04 %
E 17.40 % L 3.44 % S 7.27 % Z 1.13 %
F 1.66 % M 2.53 % T 6.15 %
G 3.01 % N 9.78 % U 4.35 %
(8) Can it be that there are two optimal codes with different codeword lengths for
the same probability vector p?
(9) A game with five (or six) uniformly distributed outcomes is played repeatedly.
The result shall be transmitted binary with a maximum of 2.5 bit available for
each result. For which n ∈ N exists a proper n-tuple code?
(10) Six cards (three aces, three kings) are placed side by side on a table in random
order. One wants to find the position of an ace. Determine an optimal guessing
strategy for this purpose.
(11) Answer the following questions:
1 Modified from Beutelspacher (1996).74 5 On Guessing and Coding
(1) Is there a binary code with six codewords of length 1,2,2,2,2, and 2?
(2) Is there a binary code with six codewords of length 1,3,3,3,3, and 3?
(3) Is there a prefix-free binary code with six codewords of length 1,3,3,3,3,
and 3?
(4) Is there a prefix-free binary code with six codewords of length 2,3,3,3,3,
and 3?
References
Beutelspacher, A. (1996). Kryptologie, vol. 7. Vieweg.
Bose, R. C., & Ray-Chaudhuri, D. K. (1960). On a class of error correcting binary group codes.
Information and Control, 3, 68–79.
Fano, R. M. (1961). Transmission of information: A statistical theory of communications. Wiley.
Hamming, R. V. (1950). Error detecting and error correcting codes. Bell Systems Technical Journal,
29, 147–160.
Huffman, D. A. (1952). A method for the construction of minimum redundancy codes. Proceedings
of the IRE, 40, 1098–1101.
Shannon, C. E. (1948). A mathematical theory of communication. Bell Systems Technical Journal,
27, 379–423, 623–656.Chapter 6
Information Transmission
This chapter introduces the concept of a transition probability and the problem
of guessing the input of an information channel from its output. It gives a first
idea on the classical results of Shannon, without introducing the technicalities of
stationary stochastic processes and the proof of Shannon’s theorem. This material is
provided in the next three chapters. Since it is not necessary for the understanding
of Parts IV, V, and VI, one can move directly to Part IV after this chapter.
6.1 Introductory Examples
In a game of dice, you are betting on sixes. When the dice is thrown, you can
put down 1 e, betting for the 6, and you get 5 e if you are right. You have two
independent “experts” E1, E2 who can predict the sixes, E1 makes 10 % errors, and
E2 makes 20 % errors. What do you do if E1 predicts a 6 and E2 not, or vice versa?
To explain this more clearly, X ∈ {1,..., 6} represents the value of the dice,
E1, E2 ∈ {0, 1} use the value 1 to predict the six.
We assume that
p[E1 = 1|X = 6] = p[E1 = 0|X = 6] = 0.1 and
p[E2 = 1|X = 6] = p[E2 = 0|X = 6] = 0.2.
From this one can compute the expected win in each of the four cases. If the
expected win is more than 1 e, it is reasonable to play:
E(W|E1 = 1, E2 = 1) = 5 · p[X = 6|E1 = 1, E2 = 1]
= 5 ·
p[X = 6, E1 = 1, E2 = 1]
p[E1 = 1, E2 = 1]
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_6
7576 6 Information Transmission
= 5 ·
1
6 · 0.9 · 0.8
1
6 · 0.9 · 0.8 +
5
6 · 0.1 · 0.2
= 5 · 0.72
0.72 + 5 · 0.02 = 5 ·
0.72
0.82
> 4
E(W|E1 = 0, E2 = 0) = 5 ·
p[X = 6, E1 = 0, E2 = 0]
p[E1 = 0, E2 = 0]
= 5 · 0.1 · 0.2
0.1 · 0.2 + 5 · 0.9 · 0.8
= 5 ·
0.02
3.62
< 0.01
E(W|E1 = 0, E2 = 1) = 5 ·
p[X = 6, E1 = 0, E2 = 1]
p[E1 = 0, E2 = 1]
= 5 · 0.1 · 0.8
0.1 · 0.8 + 5 · 0.9 · 0.2
= 5 ·
0.08
0.98 = 0.40
0.98
<
1
2
E(W|E1 = 1, E2 = 0) = 5 ·
p[X = 6, E1 = 1, E2 = 0]
p[E1 = 1, E2 = 0]
= 5 · 0.9 · 0.2
0.9 · 0.2 + 5 · 0.1 · 0.8
= 5 ·
0.18
0.58 = 0.45
0.29
> 1.5
The most interesting cases are those of conflicting expert opinion. Our result is
that one should rely on the better of the two experts, which seems obvious.
This case can also be used to show the difference between maximum likel￾ihood and maximum Bayesian probability. Here we ask the following question:
Given the two conflicting expert opinions, is it more likely that X = 6 or that
X = 6? The correct (and Bayesian) interpretation of this question compares
p[X = 6|E1, E2] with p[X = 6|E1, E2]. A more sloppy interpretation of this
question might compare p[E1, E2|X = 6] with p[E1, E2|X = 6]. In fact, the two
interpretations may lead to different results. Here it happens for E1 = 1, E2 = 0.6.2 Transition Probability 77
To see this we can compute the so-called likelihood ratio:
p[E1 = 1, E2 = 0|X = 6]
p[E1 = 1, E2 = 0|X = 6] = 0.9 · 0.2
0.1 · 0.8 = 0.18
0.08
> 1
and compare it with the Bayesian probability ratio:
p[X = 6|E1 = 1, E2 = 0]
p[X = 6|E1 = 1, E2 = 0] = p[X = 6, E1 = 1, E2 = 0]
p[X = 6, E1 = 1, E2 = 0] = 1
5
·
0.18
0.08 = 0.18
0.4
< 1.
This result means that in the case of conflicting values of our experts E1 and E2, the
most probably correct guess on X is that X = 6 rather than X = 6. However, we
may still bet successfully on X = 6.
6.2 Transition Probability
In Chap. 4, we introduced the transinformation T (X, Y ) between two random
variables (with finite ranges) X and Y as a measure for the amount of information
that X tells about Y or vice versa. The fact that T (X, Y ) is symmetric in X and Y
already indicates that T (X, Y ), for example, the correlation, does not say anything
about the causality of the dependence between X and Y . It does not matter whether
the values of X have a causal influence on those of Y or vice versa or whether there
is, for example, a common cause for the values of both X and Y .
If there is indeed something like a causal link or a mechanism that works in the
direction from X to Y , then it may be possible to say a little more than one gets from
the interpretation of the transinformation.
This is indeed the case, but before we can proceed to explain this in more detail,
we have to give a mathematical foundation for this mechanism that works in
one direction only. This specification turns out to coincide with the concept of a
transition probability. It also describes an information channel leading from X to Y
as depicted in Fig. 6.1.
Definition 6.1 A transition probability P : A  B from a (input-)set1 A to a
(output-)set B is a family (pa)a∈A of probability measures on B.
The idea in this definition is the following: given one input value a ∈ A, the
“mechanism” that relates this input value a to an output value b ∈ B need not
specify precisely one b ∈ B with certainty, but may rather lead to a—more or
less peaked—probability distribution pa on B for the possible outcomes b ∈ B.
In this sense, a transition probability p from A to B can be regarded as the stochastic
1 This definition is correct for finite sets A. In general, there are some more technical requirements
concerning measurability (see, e.g., Bauer 1972) which we do not mention here.78 6 Information Transmission
Fig. 6.1 An information
channel X P Y
version of a mapping f : A → B. Clearly, when B is not a finite set, a σ-algebra on
B has to be specified, for which the (pa)a∈A are probability measures.
Definition 6.2 For two random variables X and Y with values in A and B,
respectively, and a transition probability P : A  B, we write P : X  Y and
say that X is transmitted into Y by P, or P links Y to X, if for every a ∈ A and
every (measurable) M ⊆ B
p[Y ∈ M|X = a] = pa(M).
This definition essentially means that the two random variables X and Y are
linked through the transition probability P.
The knowledge of the channel “mechanism,” i.e., the probabilities pa for each
a ∈ A, makes it possible to determine the joint probabilities:
pab = pr[X = a,Y = b] = pr[X = a] · pa(b) = q(a)pa(b),
given a probability distribution q on A.
From these joint probabilities, one can also obtain the corresponding probability
distribution on B by
pr[Y = b] = 
a∈A
pr[X = a,Y = b].
This probability distribution on B resulting from q by means of the transition
probability P : A  B is denoted as P ◦ q.
We can then insert these joint probabilities into our formulae for I(X), I(Y ),
I(X, Y ), T (X, Y ), etc. In this way, we can, for example, determine the transinfor￾mation between two random variables that are related or “connected” by a transition
probability.
For finite sets A and B, the transition probabilities pa(b) for a ∈ A and b ∈ B
can also be arranged in a matrix. This matrix P = (pab)a∈A,b∈B with pab = pa(b)
is called a transition matrix.
If A = B, the goal of information transmission obviously is to preserve the input
at the output. In this case, the transition matrix P is also called the confusion matrix;
ideally P should be the identity matrix, and with n = |A|,
e = 1
n

a=b
pab = 1
n

a∈A
(1 − paa) = 1 − 1
n

a∈A
paa
is called the average error of P.6.3 Transmission of Information Across Simple Channels 79
Fig. 6.2 Channel example 0 0
1 1
2 2
3 3
4 4
5 5
6 6
7 7
8 8
9 9
We can also consider a transition probability P : A  B as a very simple
example of an information channel that connects an input from the alphabet A to
an output from the alphabet B. We will encounter more general and more complex
models of information channels in Chap. 8.
Example 6.1 A mapping m: A → B can also be regarded as a transition
probability, and so it can also be taken as a very simple model for a channel. We
simply define the conditional probabilities pm given by m as pm
a (b) = 1 if b = m(a)
and pm
a (b) = 0 if b = m(a).
For example, take A = B = {0,..., 9} and define m(i) = m(i + 1) = i for
i = 0, 2, 4, 6 and m(8) = 8 and m(9) = 9. This channel can also be depicted as in
Fig. 6.2.
In this channel, we can obviously use the digits 0, 2, 4, 6, 8, and 9 for perfect
transmission. 
6.3 Transmission of Information Across Simple Channels
How much information can we transmit across a channel? Let us consider another
example for illustration:
Example 6.2 Consider A = B = {0,..., 9} and the following simple channel
P : A  B : pi(i) = pi(i + 1) = 1
2 for i = 0,..., 8 and p9(9) = p9(0) = 1
2 .
Here there are two approaches to answer the above questions.
1. If we take simply the equal distribution p(i) = 1
10 for every i on A, we
can compute the transinformation across the channel. We define two random
variables X and Y on A that are linked by P and take p as the distribution
of X. Then we find I(X) = I(Y ) = log2 10 and I(X; Y ) = log2 20. Thus,
T (X, Y ) = log2 5.80 6 Information Transmission
2. We can safely use this channel, if we take only odd (or only even) digits as input,
because 1 can become 1 or 2, 3 can become 3 or 4, and so on. Thus, from the
output 6, for example, we can infer that the input was 5, or from 3 we can infer
the input 3. Thus, we can transmit safely the five numbers {1, 3, 5, 7, 9}, which
means log2 5 bit. 
Unfortunately, not all examples work out as nicely, and so one eventually needs
a rather complicated theory that again makes use of sequences of inputs and outputs
in a similar way as we just remarked after Proposition 5.2. With these techniques,
one can eventually prove that (up to an arbitrarily small error probability) one can
indeed get as many bits of information safely (or with very high fidelity) through a
channel as is computed by maximizing the transinformation.2 This quantity is called
the channel capacity.
Definition 6.3 Let P : A  B be a transition probability. We define its capacity as
c(P ) := max{T (X, Y ): X:  → A and P : X  Y }.
Thus, the maximum is taken over all input random variables X with values in
A and the output Y is linked to X by P. For A = a1,...,an, we could as well
maximize over all probability vectors p = (p1,...,pn) where pi = p(ai) =
p[X = ai]. The maximization can then be carried out by the ordinary methods of
real analysis (Exercises 10 and 11).
The meaning of this definition of capacity is quite easy to understand in view of
the remarks at the end of Chap. 5. The transinformation T (X, Y ) = I(X)−I(X|Y )
measures the amount of uncertainty about X that is removed by knowing Y , or
the amount of information about X provided by Y , or the average reduction in the
number of yes–no questions needed to guess the value of X that one gets from
knowing the value of Y . In simple words, T (X, Y ) measures how much the channel
output Y says about the channel input X. And the channel capacity is the maximum
that the channel output can say about the channel input for an appropriately chosen
input.
Definition 6.4 Let P : A  B and Q: B  C be two transition probabilities we
define by their composition R = Q ◦ P as the transition probability R : A  C
given by
ra(c) := 
b∈B
pa(b) · qb(c).
Note that for finite sets A = a1,...,an and B = b1,...,bn, a transition
probability P : A  B can also be regarded as a matrix P = (pij ), namely,
Pij = Pai(bj ) for i = 1,...,n and j = 1,...,m. Then the composition of
2 This statement is Shannon’s famous theorem. See Chap. 9.6.3 Transmission of Information Across Simple Channels 81
transition probabilities R = Q ◦ P simply corresponds to matrix multiplication
R = Q · P.
Before we move on to some special cases, let us consider in the general situation,
where P : X  Y , the guessing problem: given an output [Y = b], what is the most
probable input that has led to it?
The answer is obviously given by maximizing the conditional probability p[X =
a|Y = b] over the possible values of a. The resulting value a∗ is also called the
Bayesian guess given Y = b. It can be computed more easily if one uses the
Bayesian formula which relates p[X = a|Y = b] to p[Y = b|X = a] = pa(b).
Indeed,
p[X = a|Y = b] = p[X = a,Y = b]
p[Y = b] = p[X = a] · p[Y = b|X = a]
p[Y = b] ,
and so
p[X = a∗|Y = b] > p[X = a|Y = b]
if and only if
p[X = a∗] · pa∗ (b) > p[X = a] · pa(b).
Thus, one has to maximize simply the forward transition probability pa(b),
weighted by the so-called a priori probability p[X = a]. And in many practical
cases, the a priori probabilities are all equal and can also be left out.
Definition 6.5 Given a transition probability P : A  B (A and B finite sets), an a
priori probability p for A and an output b ∈ B, then a∗ = G(b) is called a Bayesian
guess for b, if p(a∗) · pa∗ (b) ≥ p(a) · pa(b) for every a ∈ A.
The mapping G: B  A can also be regarded as a channel, and one can now
consider the combined channel Q = G ◦ P : A  A which is defined by the
transition probabilities qa(A
) := pa(G−1(A
)) for any a ∈ A and any A ⊆ A.
A good measure for the fidelity of the whole procedure of transmitting a message
in A through a channel and guessing it from the channel’s output is the error
probability, which is defined as e = p[G(Y ) = X], where again P : X  Y . It
can be calculated as follows:
e = 
a∈A

b∈B
p[X = a] · pa(b) · 1[G(b)=a].
If we observe that 
a∈A
p[X = a]pa(b) = p[Y = b], then
e = 
b∈B

p[Y = b] − p[X = G(b)] · pG(b)(b)
= 1−

b∈B
p[X = G(b)]·pG(b)(b).82 6 Information Transmission
Clearly, this error probability is not the same as the average error e introduced
in Chap. 6.2 because it depends on the probabilities of the input variable X. The
error probability of the combined channel from A back to A is also related to
the transinformation of this channel, because, if the error probability is low, the
transinformation has to be high. Clearly, this also means that the transinformation of
the original channel P was high. This is made explicit in the following proposition.
Since T (X, Y ) = I(X) − I(X|Y ), high transinformation means that I(X|Y ) has to
be close to zero. So we consider the relation between I(X|Y ) and e.
Proposition 6.1 Let P : A  B be a transition probability and P : X  Y . Let
G: B → A be a Bayesian guess and e = p[X = G(Y )] and n = #(A) − 1. Then
(i) I(X|Y ) ≤ e(log2 n + 1
ln 2 ) − e log2 e, which goes to zero for e → 0.
(ii) Conversely, I(X|Y ) ≥ 2e.
Proof We need a few definitions and observations. We introduce the random
variable Z = 1[X=G(Y )]. Then
E(Z) = e = 
b∈B
p[X = G(Y )|Y = b]p[Y = b].
We define eb = p[X = G(Y )|Y = b], so e = 
b
p[Y = b] · eb.
(ii) We first consider the case that Y = b and consider I(X|[Y = b]) = Ib(X)
which is just the information of X for the probability p[Y=b] and denoted as
N[Y=b](X)  , i.e., N[Y=b](X)  = Ib(X).
From Proposition 4.7 we know that I(X|Y ) = 
b∈B
p[Y = b]I(X|[Y = b]).
Now
Ib(X) = Ib(X, Z) = Ib(Z) + Ib(X|Z)
= − eb log2 eb − (1 − eb)log2(1 − eb)
+ eb · N[Y=b,X=G(b)](X)  + (1 − eb) · N[Y=b,X=G(b)](X). 
The first equality holds because knowing the values of both X and Y , we
also know Z.
The last term on the right is zero because we know X = G(b). It is quite
easy to estimate the second last term since for a = G(b), we have
p[X = a|Y = b] ≤ p[X = G(b)|Y = b] = 1 − eb
and so
p[X = a|X = G(b), Y = b] ≤
1 − eb
eb
.6.3 Transmission of Information Across Simple Channels 83
This implies that N[Y=b,X=G(b)](X)  ≥ − log2( 1−eb
eb ). We use this estimate
only for eb > 1
2 . For eb ≤ 1
2 we obtain
Ib(X) ≥ −eb log2 eb − (1 − eb)log2(1 − eb) ≥ 2eb
and also for eb > 1
2 we get
Ib(X) ≥ −eb log2 eb − (1 − eb)log2(1 − eb) − eb log2
1 − eb
eb

= log2(1 − eb) ≥ 2eb
Thus, I (X|Y ) = 
b
p[Y = b]Ib(X) ≥ 2e.
(i) We have
I (X|Y ) = I ((X, Z)|Y ) ≤ I (X, Z)
= I (Z) + I (X|Z)
≤ −e log2 e − (1 − e)log2(1 − e) + e log2 n. 
We end this chapter with a few remarks on the practical computation of the
channel capacity. A case that is quite common in applications is a so-called
uniformly disturbing channel. We call a transition probability P : A  B uniformly
disturbing, if all conditional probabilities pa (for every a ∈ A) on B have the same
information I(Pa), i.e., if I(Pa ) = c for all a ∈ A. Such a channel produces
the same amount of uncertainty on its output for every input a. For a uniformly
disturbing channel P, it is easy to compute c(P ) because P : X  Y implies
T (X, Y ) = I(Y ) − I(Y |X) = I(Y ) − c. So
c(P ) = max{I(Y ): P : X  Y, X:  → A} − c.
Proposition 6.2 For a uniformly disturbing channel, we have c(P ) = max{I(Y ):
P : X  Y, X:  → A} − c.
Proof See above. 
In many cases, I(Y ) can be maximized quite easily and often the maximal
possible value of I(Y ), namely, I(Y ) = log2 #(B) can be obtained. This is the
case, for example, for so-called symmetric channels, where one has to choose X
equally distributed to make also Y equally distributed yielding I(Y ) = log2 #(B).
Another case occurs when the matrix P is invertible, so that one can compute the
distribution for X that makes Y equally distributed. In these cases, one simply gets
c(P ) = log2 #(B) − c.84 6 Information Transmission
The next three chapters need a little more mathematics on stochastic processes
than the rest of the book; they may be skipped because they are not necessary for
the understanding of the subsequent chapters.
6.4 Technical Comments
This chapter defines transition probabilities as the simplest model of information
channels. It relates transinformation across channels with the ideas of Bayesian
inference. It does not contain new material. A result like Proposition 6.1 that relates
transinformation to error probabilities is needed for the proof of the “inverse”
Shannon’s theorem (see Proposition 9.1). The idea probably goes back to Fano (e.g.,
Fano, 1961).
6.5 Exercises
(1) A lonesome islander has tried to repair the telegraph from Exercise (4.14) so
that he can play dice with his neighbors again. Since he has never learned to
fix electronic devices, the attempted repair failed. Now the telegraph shows
the following transmission behavior:
1 → 6 2 → (2, 4, 6) 3 → (3, 5)
4 → (2, 4) 5 → (2, 6) 6 → (1, 3, 5, 6)
The transmission in ambiguous (faulty) cases is equally probable, e.g., if one
tries to transmit a 3, a 3 is received with a probability of 50 %; however, a 5 is
also received with probability of 50 %. The same notion as in Exercise (4.14)
is applied here. Determine if the repair improved or corrupted the telegraph.
Calculate
(a) the error probability p[X = Z],
(b) the conditional probability p([X = Z|Z = k]),
(c) the transinformation T (X, Z).
and compare the results with Exercise (4.14). How must the game of dice
(i.e., the set of transmittable symbols) be restricted, if one wants to use the
telegraph in its current state for perfect transmission?
(2) Given a symmetric binary channel, i.e., a channel with the transition
probability
P =
1 − p p
p p − 1

.6.5 Exercises 85
Here p stands for the error probability of the channel. Calculate the channel
capacity. What is the result of T (X, Y ) for p = 0.17?
Hint: Use T (X, Y ) = I(Y ) − I(Y |X).
(3) The channel from Exercise (2) with error probability p = 0.17 shall now be
used to transmit symbols. Which error probability per bit results from optimal
guessing by the use of g : {0, 1}3 → {0, 1}? Determine the transinformation
T (X, g(Y )), where X and Y denote the sent and received bit, respectively.
(4) Let P be the symmetric binary channel from Exercise (2). Calculate the
transition probability of channel Q that results from applying the channel P
twice.
(5) Consider three discrete random variables X, Y, and Z. Xand Y are called
“independent given Z,” if for every a, b, and c
p[X = a,Y = b|Z = c] = p[X = a|Z = c] · p[Y = b|Z = c].
Show that T (X, Y ) ≤ T (X, Z) if X and Y are independent given Z.
(6) A random experiment has seven different outcomes with the probabilities
1
3 , 1
3 , 1
9 , 1
9 , 1
27 , 1
27 , 1
27 . The experiment is executed once a day and the result
is transmitted via a phone line. The phone company offers two rates. Rate
A transmits binary digits for 0.20 e per digit, and rate B transmits ternary
digits for 0.32 e per digit. Determine a code and choose a transmission mode
such that the expected costs are minimized. In particular, answer the following
questions:
(a) Which transmission rate shall be used?
(b) Which code shall be used?
(c) What are the expected costs?
(d) If the costs for rate B are changed, for which costs would you revise your
decision?
(e) Do the previous answers change if the experiment is executed many times
a day?
(7) Eight shells are placed on a street. Underneath two of the shells, there lies
1 e, respectively. Someone points at one of the shells and says that there is a
euro lying underneath it. How much information is conveyed?
This situation can be modeled with the probability space
 = {ω ⊆ {1, 2,..., 8}: |ω| = 2} with 	 = P() and the uniform
distribution p(ω). What is p(ω) for ω ∈ ? We define the random variables
Xk := 1{ω: k∈ω} for k = 1, 2,..., 8. Calculate I(Xk ), I(X1, X3), and
I(X1, X2, X3).
Define the description dk by dk(ω) = {ω : k ∈ ω
} if k ∈ ω, and dk(ω) = 
if k /∈ ω. Calculate N (Xk ) and I(dk).
Next, we define d(ω) := 
8
k=1
dk(ω). What is N (d)?
Let c(ω) := {ω : max(ω
) ∈ ω}. Calculate N (c) and I(c).86 6 Information Transmission
1 1
0 0
p
p
a
1 1
0 0
p
2
p
b
2 2
1 1
0 0
p
p p
c
Fig. 6.3 Three different channels
Finally, let the random variable X(ω) := max(ω). Calculate E(X) and I(X).
(8) Devise a guessing strategy for the 8-shells-exercise that is optimal for guessing
one shell with a euro underneath it.
(9) Calculate the capacities for the following channels (Fig. 6.3):
For which values of p do the channels (a), (b), and (c) have the capacity 0?
(10) Let the following channel transmit the results of a game of dice (fair dice):
P = 1
16 ·
⎛
⎜
⎜
⎜
⎜
⎜⎜
⎜
⎝
842110
384100
138310
013831
001483
011248
⎞
⎟
⎟
⎟
⎟
⎟⎟
⎟
⎠
.
Calculate P (Y = 6|X = 6), P (Y = 6|X = 6), N(d6), and N(d6|Y ), where
d6(x, y) =

[X = 6] if x = 6,
[X] if x = 6.
(11) In a game show, one can bet 10 e on whether a blindly cast dice shows a six
or not. If the guess is correct, one wins 60 e; otherwise, one loses the 10 e.
You happen to know a staff member of the show, who reveals insider
information to you. Before the show is broadcast, you come to know if the dice
is showing a six or not. Via the channel given in Exercise (10), your informant
sends “1” if the outcome is not a six; otherwise, he sends a “6.” Actually, he
had used the channel before to transmit the cast number, but has decided that it
is better to just send a 1 or a 6. After receiving the information, you can decide
whether to take part in the game or not. How does this information need to be
evaluated for maximizing profit? How much is the average profit per show?References 87
References
Bauer, H. (1972). Probability theory and elements of measure theory. Holt, Rinehart and Winston.
Fano, RM. (1961). Transmission of Information: A Statistical Theory of Communication. Wiley.Part III
Information Rate and Channel CapacityChapter 7
Stationary Processes and Their
Information Rate
This chapter briefly introduces the necessary concepts from the theory of stochastic
processes (see, e.g., Doob, 1953; Lamperti, 1977) that are needed for a proper
definition of information rate and channel capacity, following Shannon.
The purpose of Part III is to give a compact development of the main results of
classical information theory, including Shannon’s theorem. I believe that the use of
the concept of a description and in particular X as defined in Chap. 3 may simplify
the notation and perhaps also the understanding. These results need the terminology
of stochastic processes. For this reason, they are usually regarded as technically
demanding and are often not included in introductory textbooks such as Topsøe
(1974). Part III can be skipped by experts who already know the classical results on
channel capacity and by beginners who only want to understand basic information
theory and the new concepts of novelty and surprise introduced in this book.
7.1 Introductory Examples
We are given a channel (see Fig. 7.1). We want to use it to transmit sequences of
many digits from signals {0,..., 9} by using a suitable code. How many channel
symbols do we transmit for each digit in the long run? What is the error probability
and the transinformation of this procedure per digit?
We can use this channel to transmit the ten digits directly, with some uncertainty,
which we can compensate by sending the digits twice or three times. On the other
hand, we can transmit five symbols (e.g., a, b, c, d, e) safely, so we could code
sequences of ten digits into longer sequences of five symbols, and decode them
again after passing the channel.
Since 53 = 125, three symbols would suffice to code two digits. Another longer
code would result for 513 = 1220703125 > 109.
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_7
9192 7 Stationary Processes and Their Information Rate
Fig. 7.1 Channel example 0 0
1 1
2 2
3 3
4 4
5 5
6 6
7 7
8 8
9 9
0.8
0.8
0.8
0.8
0.8
0.8
0.8
0.8
0.8
0.8
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
The transinformation of the channel C is 2.6 bit.1 Sending the digits twice would
result in an error probability of 4
160 at most, because only two transmission errors
would lead to a wrong guess at the channel output. This yields a transinformation of
3.10245 bit.2 This means a bit-rate of 3.10245/2 = 1.55123 bits per channel use.
The method of coding two digits in three symbols which are transmitted reliably
yields a bit-rate of 2
3 log 10 = 2.2146. The last method, coding 9 digits in 13
symbols, yields a bit-rate of 9
13 log 10 = 2.2998.
All these bit-rates stay below the capacity of the channel C. In this section,
we work toward the demonstration that for sufficiently long sequence codes, it is
possible to achieve bit-rates close to the capacity with small error probabilities. To
this end, we have to consider sequences of random inputs and outputs at the channel
under consideration. These sequences are called stochastic processes.
1 c = log2 10 + 8
10 log2 8 + 2
10 log2 2 − log2 10 = 2.6 bit.
2 bit-rate = log2 10 + 68
100
 16
17 log2 16
17 + 1
17 log2 1
17
 = log2 10 + 68
100
 64
17 − log2 17 =
log2 10 − 0.322757 · 0.68 = 3.10245.7.2 Definition and Properties of Stochastic Processes 93
7.2 Definition and Properties of Stochastic Processes
We start this section by reviewing some general material from the theory of
stochastic processes in Definition 7.1 to Proposition 7.3 (compare Doob, 1953;
Lamperti, 1966).
Definition 7.1 A sequence X = (Xn)n∈N of measurable functions Xn :  → A is
called a (discrete) (stochastic) process (with values) in A.
We usually assume that A is finite or countable and that p[Xn = a] = 0 for
every a ∈ A.
Definition 7.2 A stochastic process with values in A is called stationary, if
p[X1 ∈ A1, X2 ∈ A2,...,Xn ∈ An] = p[Xt+1 ∈ A1, Xt+2 ∈ A2,...,Xt+n ∈ An]
for every t,n ∈ N and every choice of measurable sets A1,...,An ⊆ A.
Stationarity of a process simply means that the probabilities for the process to
take certain values do not change, when the whole observation of the process is
shifted in time.
In the following, we shall describe some important classes of stochastic processes
that are extensively studied in probability theory. We shall need these ideas later, but
they are not essential for the understanding of the concept of information.
Definition 7.3 A sequence (Xn)n∈N of measurable functions Xn :  → A is called
independent, if for every n ∈ N and every choice of (measurable) sets A1,...,An ⊆
A we have
p[X1 ∈ A1,...,Xn ∈ An] = p[X1 ∈ A1] · p[X2 ∈ A2] · ... · p[Xn ∈ An].
It is called identically distributed, if for every n and every (measurable) set B ⊆ A:
p[Xn ∈ B] = p[X1 ∈ B].
Proposition 7.1 A process X = (Xn)n∈N with independent identically distributed
random variables Xn (in short an i.i.d. process) is stationary.94 7 Stationary Processes and Their Information Rate
Proof
p[X1 ∈ A1,...,Xn ∈ An] = 'n
i=1
p[Xi ∈ Ai] = 'n
i=1
p[X1 ∈ Ai]
= 'n
i=1
p[Xt+i ∈ Ai]
= p[Xt+1 ∈ A1,...,Xt+n ∈ An]. 
Definition 7.4 A process X is a Markov process, if ∀n ∈ N ∀A1,...,An ⊆ A
p[Xn ∈ An | Xi ∈ Ai for i = 1,...,n − 1] = p[Xn ∈ An | Xn−1 ∈ An−1].
This means that in a Markov process, Xn depends only on Xn−1 and not on
earlier outcomes of the process once Xn−1 is known. Another way of stating this is
the following: for a Markov process, the future is independent of the past given the
presence.
Proposition 7.2 Let X be a stationary Markov process on a finite set A. Let
P : A  A be given by pab = p[X2 = b|X1 = a]. Then
p[X1 = a1, X2 = a2,...,Xn = an] = p[X1 = a1] ·
n
'−1
i=1
Paiai+1 .
In addition, for A = {a1,...,ak}, the row vector q with qi = p[X1 = ai] satisfies
q · P = q, where Pij = Paiaj .
Proof
p[X1 = a1,...,Xn = an] = p[X1 = a1] ·
n
'−1
i=1
p[Xi+1
= ai+1|X1 = a1,...,Xi = ai]
= p[X1 = a1] ·
n
'−1
i=1
p[Xi+1 = ai+1|Xi = ai]7.3 The Weak Law of Large Numbers 95
= p[X1 = a1] ·
n
'−1
i=1
p[X2 = ai+1|Xi = ai]
qi = p[X1 = ai]=p[X2 = ai] = 
j
p[X2 = ai|X1 = aj ] · qj = 
j
Pj i · qj . 
7.3 The Weak Law of Large Numbers
Definition 7.5 A stochastic process X = (Xn)n∈N with values in R satisfies the
weak law of large numbers (w.l.l.n.), if for every  > 0
lim
n→∞ p[|Yn − E(Yn)| > ] = 0,
where Yn = 1
n
n
i=1
Xi.
This obviously means that for large n, the average Yn of the first n random
variables Xi will be with very high probability very close to a constant value—its
expectation. Thus, the scatter in Yn will become negligible.
Proposition 7.3 Every i.i.d. process with finite E(X2
1) satisfies the w.l.l.n.
Proof For the proof, we need a well-known estimate of p[|Yn−E(Yn)| ≥ ], called
the Chebyshev inequality:
For any random variable X and any  > 0, we have
 · 1[|X|≥] ≤ |X|
and thus
 · p[|X| ≥ ] ≤ E(|X|).
Now we consider the function
X := (Yn − E(Yn))2 =
1
n
n
i=1

Xi − E(Xi)

2
in the Chebyshev inequality and obtain
2 ·p[(Yn −E(Yn))2 ≥ 2] ≤ E(X) = 1
n2
n
i=1
n
j=1
E((Xi −E(Xi))(Xj −E(Xj ))).96 7 Stationary Processes and Their Information Rate
For i = j these expectations are E (Xi − E(Xi)) E(Xj − E(Xj )) = 0, and for
i = j they are
E((Xi − E(Xi))2) = E(X2
i ) − (E(Xi))2 = Var(Xi) = Var(X1).
Thus
E(X) = 1
n

E(X2
1) − (E(X1))2

= 1
n Var(X1)
and therefore
p[(Yn − E(Yn))2 ≥ 2] = p[|Yn − E(Yn)| ≥ ] ≤
Var(X1)
2 · n → 0 for n → ∞. 
The classes of stochastic processes introduced up to this point will be helpful
in the subsequent discussion of the average information generated by a stochastic
process.
7.4 Information Rate of Stationary Processes
In what follows, we study the information rate of sequences of random variables.
Recall that I(X1, X2) = N (X1 ∩ X2). This can be generalized to any number of
random variables. The following lemma characterizes maximum information rate of
n random variables.
Lemma 7.1 For random variables X1,...,Xn, Xi : →A, A finite,
I(X1,...,Xn) is maximized if the random variables are independent.
Proof From Proposition 4.9.(ii), it follows that
I(X1,...,Xn) ≤ n
i=1
I(Xi).
If X1,...,Xn are independent, we obtain by Proposition 4.9.(iv) that
I(X1,...,Xn) = n
i=1
I(Xi). 7.4 Information Rate of Stationary Processes 97
Proposition 7.4 For a stationary process X = (Xn)n∈N in A, the limit
I(X) = lim
n→∞
1
n
I(X1,...,Xn)
exists.
Proof Recall that
I(X1, X2) = I(X1) + I(X2|X1) (7.1)
as stated in Proposition 4.9.(iv).
From this equality, we infer
I(X1,...,Xn) = I(X1) +n
i=2
I(Xi|X1,...,Xi−1) for each n ∈ N. (7.2)
It is also not difficult to check (see Proposition 4.12.(iii) or 4.9.(iv) and consider
the stationarity of X) that for each i ∈ N
I(Xi|X1,...,Xi−1) ≤ I(Xi−1|X1,...,Xi−2). (7.3)
Thus, in (7.2) every term in the sum on the right-hand side is less than or equal
to its predecessor. Furthermore, we have
I(X1) = I(X2) ≥ I(X2|X1)
by Proposition 4.9.(v).
Therefore
n · I(Xn|X1,...,Xn−1) ≤ I(X1,...,Xn) (7.4)
and
I(X1,...,Xn) (7.1) = I(X1,...,Xn−1) + I(Xn|X1,...,Xn−1)
(7.3)
≤ I(X1,...,Xn−1) + I(Xn−1|X1,...,Xn−2)
(7.4)
≤ I(X1,...,Xn−1) +
1
n − 1
I(X1,...,Xn−1)
= n
n − 1
I(X1,...,Xn−1),98 7 Stationary Processes and Their Information Rate
and so
1
n I(X1,...,Xn) ≤
1
n − 1
I(X1,...,Xn−1).
This means that the sequence 
1
n I(X1,...,Xn)

is decreasing. Since it is always
positive, the limit exists.
Definition 7.6 The limit in Proposition 7.4 is called the information rate I (X) of
the process X = (Xn)n∈N.
It is important to observe that the information rate I (X) of a process X coincides
with the average information needed for determining the value taken by one of the
random variables Xn, when knowing the previous ones, as the following proposition
shows.
Proposition 7.5 Given a stationary process X = (Xn)n∈N, then
lim
n→∞ I(Xn|X1,...,Xn−1) = I (X).
Proof By (7.3) the limit exists. And now we write
bn = 1
n
n
i=1
I(Xi|X1,...,Xi−1)
and observe that bn converges to this same limit.
By (7.2) bn = 1
n I(X1,...,Xn).
Proposition 7.6 Let X be a stationary process in A. Let cn denote an optimal 0-1-
code for (X1,...,Xn), then
lim
n→∞
1
n
L(cn) = I (X).
Proof
1
n
I(X1,...,Xn) ≤
1
n
L(cn) ≤
1
n
I(X1,...,Xn) +
1
n
by Proposition 5.2.
Proposition 7.7 If the process X = (Xn)n∈N is i.i.d., then
I(X1,...,Xn) = n · I(X1) and I (X1) = I(X).
Proof Obvious.7.5 Transinformation Rate 99
The last two propositions show that the information content of a random variable
can be precisely identified with the average number of yes–no questions needed in
repeated guessing of this variable for one determination of its value.
A similar interpretation as for I(X) can now also be given for
I(X|Y ) = 
b
p[Y = b] · I(X|Y = b).
Here I(X|Y = b) can be interpreted as the average number of yes–no questions
needed to determine the value of X, if one knows that Y = b. Thus, I(X|Y ) is the
average number of questions needed to determine the value of X, if one knows the
value of Y . The equation I(X|Y ) ≤ I(X) fits with the idea that knowledge of Y can
only help to determine X.
The equality I(X|Y ) = I(X), which holds when X and Y are independent, fits
with the idea that knowledge of Y does not actually help to determine X, when Y
and X are independent.
T (X, Y ) = I(X) − I(X|Y ) = I(Y ) − I(Y |X)
can thus be interpreted as the amount of information that knowledge of Y contributes
to the determination of X, or vice versa.
If we further apply this interpretation to the novelty and information of a
description d, then N (d) is the average novelty obtained from one application of the
description d to a particular event x, i.e., the average number of yes–no questions
that would have been needed to guess the corresponding proposition d(x) that has
been provided by d, whereas I(d) = S(d)  is the average information (=number
of yes–no questions) needed to predict the outcome of the description d for one
particular event x.
The limiting procedure of Proposition 7.5 has indeed been employed to define
the information rate of English writing [and also for some other languages—see,
e.g., Attneave (1959) and Topsøe (1974)] from observed n-block probabilities.
7.5 Transinformation Rate
Definition 7.7 For two processes X = (Xn)n∈N and Y = (Yn)n∈N, we define the
transinformation rate between X and Y as
T (X,Y) = lim
n→∞
1
n
T ((X1,...,Xn), (Y1,...,Yn)).100 7 Stationary Processes and Their Information Rate
Defining the pair-process (X,Y) of the two processes X and Y as
(X,Y) = ((Xn, Yn)n∈N), then obviously
T (X,Y) = I (X) + I (Y) − I (X,Y),
in accordance with Definition 4.5.
Consider now a process X = (Xn)n∈N in a finite set A. We may define the
random vectors Xn = (X1,...,Xn) and the random variables:
In(ω) := N 
X1(ω) ∩ X2(ω) ∩ ... ∩ Xn(ω)
= N (Xn(ω)).
Obviously, In depends on Xn and E(In) = I(X1,...,Xn) = I(Xn).
7.6 Asymptotic Equipartition Property
We have shown that for a stationary process X , the limit limn→∞
1
n I(X1,...,Xn) =
I (X) = I exists. Now we are interested not only in the averages but also in the
individual values of the functions In(x), where x = (x1,...,xn) is a particular value
of the random vector (X1,...,Xn). We want to investigate, for which individual
vectors x they come close to the average I for large n. It will turn out that this can
happen for “most” x, i.e., with high probability.
Definition 7.8 A process X = (Xn)n∈N with values in a finite set A is said to
have the asymptotic equipartition property (a.e.p.), if the sequence (In+1 − In)n∈N
satisfies the w.l.l.n.
This definition actually means that for processes with a.e.p., 1
n In comes close to
I with high probability.
Proposition 7.8 An i.i.d. process X = (Xn)n∈N with values in a finite set A satisfies
the a.e.p.
Proof Because the Xi are independent, we have
In = N 
X1 ∩ ... ∩ Xn

= n
i=1
N 
Xi

.
Thus, In+1 − In = N 
Xn+1
 depends on Xn+1 only. Therefore, the sequence
(In+1 − In)n∈N is i.i.d. and by Proposition 7.3 satisfies the w.l.l.n.7.6 Asymptotic Equipartition Property 101
Let us work out the meaning of the asymptotic equipartition property in some
more detail. It means that p
(
(
(
1
n In − I
(
(
(
< 
→ 1 for any  > 0. Here I = I (X) is
the information rate of the process X = (Xn)n∈N. If we define
Hn, =
	
ω ∈ :
(
(
(
(
1
n
In(ω) − I
(
(
(
(
< 

and
An, = {(X1(ω), . . . , Xn(ω)) ∈ An : ω ∈ Hn, },
then it is clear that for every  > 0, there is an n ∈ N such that p(Hn, ) = p(An, ) >
1 − .
The sets Hn, and An, are called the high-probability sets for the process
X = (Xn)n∈N on A. The sequences a = (a1,...,an) ∈ An, ⊆ An are called
high-probability sequences. Thus, the majority of sequences in An are high￾probability sequences, and it turns out that all the high-probability sequences have
about the same probability, i.e., the high-probability sequences form an almost￾equal-probability partition of An, . This is the reason for the name “asymptotic
equipartition property.”
In order to see why each of the high-probability sequences has about the same
probability, we now proceed to estimate this probability. From this estimate, we can
also get an estimate on the total number of high-probability sequences.
In the above discussion, we have introduced an obvious probability p on An,
namely, p(a1,...,an) = p[X1 = a1,...,Xn = an].
For a ∈ An and any ω ∈  where (X1(ω), . . . , Xn(ω)) = a, we have
In(ω) = − log2 p(a).
Now the a.e.p. yields the following estimates for a ∈ An, :
I −  < − log2 p(a)
n < I + , thus 2−n(I−) > p(a) > 2−n(I+).
If we sum these inequalities for all a ∈ An, , we obtain the following estimates
for #An, :
#An, > p(An, ) · 2n(I−) > (1 − ) · 2n(I−),
#An, < p(An, ) · 2n(I+) ≤ 2n(I+).
Thus, we have proved the following.102 7 Stationary Processes and Their Information Rate
Proposition 7.9 Let X = (Xn)n∈N be a process with values in A that has the a.e.p.
and the information rate I . Then there is for every  > 0 an n ∈ N and a set
An, ⊆ An of so-called high-probability sequences satisfying
(i) p(An, ) > 1 − ,
(ii) 2−n(I−) > p(a) > 2−n(I+) for every a ∈ An, ,
(iii) (1 − ) · 2n(I−) < #An, < 2n(I+).
The classes of processes that satisfy the w.l.l.n. or the a.e.p. have been more
thoroughly investigated in the mathematical literature, in particular in a branch of
“ergodic theory” which analyzes dynamical systems from a probabilistic point of
view (see also Gray, 1990). It turns out that they are in fact rather large compared to
the very special case of i.i.d. processes, since they contain, for example, the class
of all “ergodic” processes (see, e.g., Billingsley, 1978 or Friedman, 1970; Walters,
1982). It is the requirement of independence of the variables Xi that is so strong,
and it has actually been weakened in several interesting ways (Billingsley, 1978;
Gray, 1990).
7.7 Technical Comments
This chapter only contains classical material from information theory (e.g., Cover &
Thomas, 1991) and the theory of stochastic processes (e.g., Doob, 1953; Lamperti,
1966, 1977) that is needed to prove Shannon’s theorem. The exposition is rather
brief. I think this is made possible by the use of the description X by a discrete
random variable X (defined in Sect. 3.6) which considerably simplifies the notation.
The “nonconstructive” approach to stochastic processes taken here, by simply
considering collections of random variables without explicitly constructing a possi￾bly underlying space  and a σ-algebra 	, i.e., by disregarding or circumventing
most of the problems successfully solved by measure theory, may still look a
bit tedious here and there, but can be taught to and grasped by students with
limited mathematical experience. The definition of information as the expectation
of a random information variable N and the corresponding introduction of the
description X provided by a random variable X (see Chap. 3) are most useful in
this context. The material presented, however, is classical (Ash, 1965; Khinchin,
1957; Shannon, 1948). See also Gray (1990) for a more advanced presentation.
7.8 Exercises
(1) Let A = {1,...,n} and (Xi)i∈N be independent identically distributed on
A such that p[Xi = k] = 1
n for every i ∈ N and k ∈ A. For any B ⊆ A
obviously p(B) = 1
n #(B). The relative frequency with which the Xi hit the setReferences 103
B is defined as Ym = m−1 m
i=1
1B ◦ Xi. Can one say that the random variables
Ym converge in some sense to p(B)?
(2) Let (Xi)i∈N and (Yi)i∈N be processes on A and B, respectively. The process
(Xi, Yi)i∈N has values in A × B. Show the following:
(a) If (Xi, Yi)i∈N is i.i.d., so are the processes (Xi)i∈N and (Yi)i∈N.
(b) If (Xi, Yi)i∈N is i.i.d., then T ((Xi)i∈N, (Yi)i∈N) = T (X1, Y1).
(3) Is it always true that the combined process (Xi, Yi)i∈N is i.i.d. if the processes
(Xi)i∈N and (Yi)i∈N are i.i.d.?
(4) Let (Xi)i∈N be independent identically distributed random variables on the
finite set A with p[X1 = a] = 0 for every a ∈ A. What is the probability
that every word from A∗ occurs infinitely often in a sequence (Xi)i∈N? Here
A∗ denotes the set of all finite words on A, given by
A∗ = 
i∈N
Ai
with Ai = {a1a2 ...ai : aj ∈ A for j = 1,...,i}.
Hint: Determine the probability that a given word ω ∈ A∗ just occurs finitely
many times.
(5) Let (Xi)i∈N be an i.i.d. process on {0, 1}. Let p0 = p[X1 = 0] and p1 =
p[X1 = 1]. Consider the random variable In defined in Sect. 7.5. We want to
determine the distribution of its values:
W (In) =
)
− 1
n log2(pk
0pn−k
1 ): k = 0, 1,...,n*
.
How often does each of these values occur, and with which probability? What
is the expectation of the probabilities themselves, i.e., of Pn = 2−nIn ? For
p0 = 1
8 and n = 800, what is E(In) and what is the probability that In deviates
not more than 5% from E(In)?
References
Ash, R. B. (1965). Information theory. Interscience.
Attneave, F. (1959). Applications of information theory to psychology. Holt, Rinehart and Winston.
Billingsley, P. (1978). Ergodic theory and information. Robert E. Krieger Publishing.
Cover, T. M. & Thomas, J. A. (1991). Elements of information theory. Wiley.
Doob, J. L. (1953). Stochastic processes. Wiley.
Friedman, N. A. (1970). Introduction to ergodic theory. Van Nostrand Reinhold Company.
Gray, R. M. (1990). Entropy and information theory. Springer.
Khinchin, A. (1957). Mathematical foundations of information theory. Dover Publications.
Lamperti, J. (1966). Probability : A survey of the mathematical theory. Benjamin/Cummings.104 7 Stationary Processes and Their Information Rate
Lamperti, J. (1977). Stochastic processes—a survey of the mathematical theory. Applied Mathe￾matical Sciences 23 Springer.
Shannon, C. E. (1948). A mathematical theory of communication. Bell Systems Technical Journal,
27, 379–423, 623–656.
Topsøe, F. (1974). Informationstheorie: eine Einführung. Teubner Verlag.
Walters, P. (1982). An introduction to ergodic theory. Springer.Chapter 8
Channel Capacity
In this chapter, we extend the definitions of Chap. 6 to real information channels that
handle sequences of symbols instead of single symbols. This extension is needed to
take limits of very long sequences in order to define information rate (Definition 6.5)
and in turn to define transinformation rate and channel capacity. This leads to the
proof of Shannon’s famous theorem in the next chapter.
8.1 Information Channels
The concept of a transition probability (Definition 6.1) is very closely related to
the concept of an information channel. The basic idea is that a channel is a special
kind of transition mechanism that deals with sequences of values from finite sets
A and B.
Definition 8.1 Let A and B be finite sets. A channel C with input alphabet A and
output alphabet B is a transition probability C : AN  BN from1
AN = {(a1, a2, a3, . . .): ai ∈ A} to BN = {(b1, b2, b3, . . .): bi ∈ B}.
One has to imagine that at discrete time-steps t = 1, 2, 3,... symbols ai from
the “alphabet” A are fed into the channel, leading to the outputs b1, b2, b3,... at
(almost) the same times (compare Fig. 8.1 on page 106). So, in a sense b1 belongs
to a1, b2 to a2, etc.
1 In this very general definition, one needs the canonical σ-algebras on AN and BN that are
generated by the so-called cylinder sets (cf. Bauer, 1972 and see also Definition 8.2).
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_8
105106 8 Channel Capacity
Fig. 8.1 Data transmission
on a channel
t = 1
t = 3
t = 2
t = 4
a1 b1 a2 b2
a3 b3 a4 b4
The simplest examples for channels are the so-called deterministic channels,
which are defined simply by a mapping from AN to BN; the first two examples
are of this type.
Example 8.1 For a mapping f : A→B and a = (a1, a2, . . .) ∈ AN, define
Ca(M) = 1 if the sequence (f (a1), f (a2), . . .) ∈ M and Ca(M) = 0 otherwise for
measurable sets M ⊆ BN. 
This example (as well as the next one) is a simple construction of a deterministic
mapping f : AN → BN. Here the output value at any time t depends only at the
corresponding input value at the same time.
Example 8.2 For a mapping f : A × A→B, a fixed b0 ∈B and any a ∈ AN,
and any measurable M ⊆ BN, define Ca(M) = 1 if M contains the sequence
(b0, f (a1, a2), f (a2, a3), f (a3, a4), f (a4, a5), . . .) and Ca(M) = 0 otherwise. 
The next example is the opposite extreme, where the output probability on BN
does not depend on the input a ∈ AN at all.
Example 8.3 Given a probability measure p on BN, for any a ∈ AN and any
measurable M ⊆ BN, define Ca(M) := p(M). 
8.2 Memory and Anticipation
Definition 8.2 An information channel C from A to B is said to have memory
bound k and anticipation bound l, if for every m ≤ n the probability Ca({b =
(b1, b2, . . .): bm ∈Bm, bm+1 ∈Bm+1,...,bn ∈Bn}) depends only on the coordi￾nates am−k,...,an+l of the input a = (a1, a2, a3, . . .).
More exactly: for every a and a in A with ai = a
i for i = m − k,... , n + l, we
have
Ca

{b = (b1, b2, . . .): bm ∈ Bm,...,bn ∈ Bn}

=
Ca

{b = (b1, b2, . . .): bm ∈ Bm,...,bn ∈ Bn}

.8.3 Channel Capacity 107
Definition 8.3 The memoryspan of a channel is its lowest memory bound k. The
anticipationspan of a channel is its lowest anticipation bound l. A channel with
finite memory and anticipationspan is called a channel with finite memory and
anticipation.
In the following, we shall concentrate on the simplest type of channel: the
channel without memory and anticipation, also called the simple channel.
Let (pa)a∈A be a transition probability from A to B, where A and B are both
finite sets.
We can then construct an information channel with input alphabet A and output
alphabet B that simply works independently on each term ai in an input sequence
(a1, a2, . . .) and produces the corresponding term bi of the output sequence
(b1, b2, . . .).
For a ∈ AN we thus define
Ca[b1 ∈ B1,...,bn ∈ Bn] := pa1 (B1) · pa2 (B2) · ... · pan (Bn). (8.1)
Definition 8.4 Given a transition probability P = (pa)a∈A from A to B, where A
and B are finite sets, then (8.1) defines the simple channel corresponding to P.
It is obvious from (8.1) that the simple channel has memoryspan 0 and also
anticipationspan 0.
It should be remarked, however, that zero memory- and anticipationspan do not
fully characterize the simple channel. In a sense it has two additional properties:
“time invariance” and no internal memory (see also Ash, 1965; Feinstein, 1958).
For two stochastic processes X and Y, the existence of a simple channel
transmitting X into Y in the sense of Definition 6.2 implies that X and Y have
some properties in common.
Proposition 8.1 Let C : AN  BN be a simple channel and C : X  Y, and then
(i) If X is stationary, so is Y,
(ii) If X is independent, so is Y.
Proof Left as an exercise to the reader (Exercise 8).
8.3 Channel Capacity
For a channel C : AN  BN from A to B, we can now define its information
transferring capacity as the maximal amount of transinformation that can pass
through the channel.108 8 Channel Capacity
Definition 8.5 The channel capacity c of a channel C : AN  BN is defined by2
c(C) := sup{T (X,Y): X stationary process on A,
Y process on B and C : X  Y}.
Example 8.4 Given a transition probability P : A × A  B and a probability p0
on B, one can easily define a channel with memory:
Ca[b1 ∈ B1, b2 ∈ B2,...,bn ∈ Bn] = p0(B1) · P(a1,a2)(B2) · ... · P(an−1,an)(Bn).

Example 8.5 Given a transition probability P : A × B  B, one can define more
complicated channels. For instance, we may define
Ca[b1 = k1, b2 = k2,...,bn = kn] = p0(k1) · P(a1,k1)(k2) · ... · P(an−1,kn−1)(kn).

In the general case, it is difficult to compute channel capacities. There are,
however, special and yet very useful cases where this is easier.
Proposition 8.2 Let C be a simple channel with input alphabet A and output
alphabet B, both finite sets, given by a transition probability P : A  B. Then
c(C) = max{T (X, Y ): X:  → A, Y :  → B, P : X  Y },
= max
q
{I(P ◦ q) −
a∈A
q(a) · I(pa)},
where the maximum3 extends over all probability vectors q = (qa)a∈A and I denotes
the information of a probability vector as defined in Exercise (5.4) on page 72.
Proof Let A = {a1,...,am}, B = {b1,...,bn}. For two stationary processes X
and Y with C : X  Y, we have
T (X,Y) = I (Y) − lim
n→∞
1
n I

(Y1,...,Yn) | (X1,...,Xn)

= I (Y) − lim
n→∞
1
n N 
(Y
1 ∩ ... ∩ Y
n) | (X1 ∩ ... ∩ Xn)

2 Of course, a general definition should not make requirements on the processes involved. Yet
Shannon’s theorem relies on strong properties and it seems adequate to restrict the definition to
stationary processes.
3 The maximum is attained because the set of all probability vectors q is compact.8.3 Channel Capacity 109
= I (Y) + lim
n→∞
1
n E log2 p

(Y
1 ∩ ... ∩ Y
n) | Xn
(∗) (8.1) = I (Y) + lim
n→∞
1
n
E

log2
'n
i=1
PXi

Yi


= I (Y) + lim
n→∞
1
n
n
i=1
E

log2 PXi

Yi

stationarity = I (Y) + E

log2 PX1

Y1

= I (Y) +
a∈A

b∈B
p[X1 = a]Pa(b)log2 Pa(b)
   depends only on X1
.
(∗) holds because for Xn = x ∈ An and Y n = y ∈ Bn, we have
p(Y
1 ∩ ... ∩ Y
n | Xn) = p([Y n = y]|[Xn = x]) = 'n
i=1
pxi(yi).
Because Y is stationary (Proposition 8.1 on page 107) and the distribution of
Y1 only depends on the distribution of X1, we can maximize T (X,Y) for given
distribution of X1 if we choose X i.i.d., because in this case also Y is i.i.d.
(Proposition 8.1 on page 107) and thus I(Y) is maximized (Lemma 7.1 on page 96),
whereas the second term remains unchanged. In this case, I (Y) = I(Y1) (by
Proposition 7.7) and
T (X,Y) = I(Y1) +
a∈A

b∈B
p[X1 = a]pa(b)log2 pa(b)
= I(Y1) −
a∈A
p[X1 = a]I (pa)
= I(Y1) − I(Y1|X1) = T (X1, Y1).
(8.2)
Thus, T (X,Y) can be maximized by maximizing T (X1, Y1), where p: X1 
Y1. This proves the first equation; the second follows from (8.2). 
Given a transition probability p from A to B and another one q from B to C,
it is possible to connect the two together to a transition probability q ◦ p from
A to C. One simply defines for a ∈ A and M ⊆ C the transition probability
(q ◦ p)a (M) as the average with respect to the probability pa on B of qb(M),
i.e., (q ◦ p)a(M) = Epa (qb(M)). The same definition can be used to connect
channels together. For channels without anticipation (the physically normal case),
we can give this definition more explicitly.110 8 Channel Capacity
Definition 8.6 Let C : AN  BN and P : BN  CN be two channels without antic￾ipation. We define (P ◦ C): AN  CN by
(P ◦ C)a[c1 ∈ C1,...,cn ∈ Cn] = 
m1,...,mn
pa[b1 = m1,...,bn = mn]
· qm[c1 ∈ C1,...,cn ∈ Cn]
where m ∈ BN starts with (m1,...,mn).
8.4 Technical Comments
This chapter again contains only classical material. We use a quite general definition
of an information channel.
8.5 Exercises
(1) What is the memory- and anticipationspan of Examples 8.1–8.5?
(2) How could one modify Example 8.4 with P : A × A  B in order to define a
channel with memoryspan 4?
(3) When two channels with finite memory and anticipation are connected, how
does the memory- and anticipationspan of the composed channel depend on
those of the two channels?
(4) Show that for a simple channel (see Definition 8.4 on page 107) Cp defined by
a transition probability p: A  B, the capacity c(Cp) is attained for an i.i.d.
input process on A.
(5) Show that the channel capacity c(p) of a channel p: AN  BN, where
Ipa (b1,...,bn) is independent of a ∈ AN for every n ∈ N, can be obtained
by maximizing {I (Y): X process on A, p: X  Y}.
(6) In Example 8.1 take B = {0, 1}, A ⊆ A, and consider the mapping f that has
f (a) =

1 for a ∈ A
,
0 otherwise .
In Example 8.2 take A = {1,...,n}, B = A × A and f as the identity
mapping. With these specifications, what is the channel capacity in Exam￾ples 8.1–8.3?
(7) Determine the channel capacity of the following simple channels with input
alphabet {0, 1} and {00, 01, 10, 11}, respectively, and output alphabet {0, 1, 2}
in Fig. 8.2.References 111
Fig. 8.2 Two channels
1
1−p 1
0
1 − p
0
2
00
01
10
11
0
1
0.5
2
0.5
Fig. 8.3 Two simple
channels
0 2
1 1
2 0.25 0
0.5
0.25
0.5
0 3
1 2
2 1
3 0
0.5
(8) Prove Proposition 8.1.
(9) For the simple channels C given in Fig. 8.3, what is their capacity and what is
the capacity of C · C?
(10) Show the following:
Proposition 8.3 Let C : AN  BN be a channel. Its capacity
c(C) satisfies c ≤ min(#(A), #(B)).
References
Ash, R. B. (1965). Information theory. Interscience.
Bauer, H. (1972). Probability theory and elements of measure theory. Holt, Rinehart and Winston.
Feinstein, A. (1958). Foundations of information theory. McGraw-Hill Book Company.Chapter 9
How to Transmit Information Reliably
with Unreliable Elements (Shannon’s
Theorem)
The goal of our rather technical excursion into the field of stationary processes
was to formulate and prove Shannon’s theorem. This is done in this last chapter
of Part III.
Shannon’s theorem is one of the most important results in the foundation of
information theory (Shannon & Weaver, 1949). It says that the channel capacity
c determines exactly what can effectively be transmitted across the channel. If you
want to transmit less than c bits of information per time unit across the channel, you
can manage to do it in such a way that you can recover the original information from
the channel output with high fidelity (i.e., with low error probabilities). However,
if you want to transmit more than c bits per time unit across the channel, this cannot
be done with high fidelity. This theorem again underlines the fact that information
is incompressible (like water) and that a given channel can only transmit a given
amount of it in a given time.
9.1 The Problem of Adapting a Source to a Channel
The situation is the following: we have a channel CAB from A to B and a stationary
process U with values in C satisfying the a.e.p. Since we cannot directly connect the
“source”-process U to the channel C, we have to construct an appropriate adapter.
We want to construct a block-coding h: Cn → A∗ by which we can couple U to
the channel CAB as in Fig. 9.1. From the channel output Y, it should be possible to
determine U with low error probability.
In this section, we shall prove Shannon’s theorem for the simplest type of
channel: the “memoryless” or simple channel. Proofs for channels with memory
become technically more complicated (see, e.g., Kieffer, 1981; Pfaffelhuber, 1971;
Wolfowitz, 1964), but rely on the same basic ideas.
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_9
113114 9 How to Transmit Information Reliably with Unreliable Elements (Shannon’s. . .
C
U
h
Coding
A
X
CAB
Channel
B
Y
Fig. 9.1 Channel and stationary process
9.2 Shannon’s Theorem
Theorem 9.1 (Shannon 1949) Let CAB be a simple channel from A to B with
capacity c. Let U be a stationary process on the alphabet C satisfying the a.e.p.
and I (U) = r < c. Then for any δ > 0, there is an n ∈ N and a mapping
m: Cn → An such that the values of U can be determined from the outputs of
the combined channel CAB ◦ m with an error probability less than δ.
Proof For the proof, we first consider an input process X on A, which is i.i.d. and
has T (X,Y) = c (and CAB : X  Y). This is possible by Proposition 8.2.
From the a.e.p., we can infer that for any  > 0, there is an n ∈ N such that
(i) p
(
(
(
1
n · N 
X1 ∩ ... ∩ Xn

− I (X)
(
(
(
> 
< 
(ii) p
(
(
(
1
n · N 
Y
1 ∩ ... ∩ Y
n

−I (Y)
(
(
(
> 
< 
(iii) p
(
(
(
1
n · N 
X1 ∩ ... ∩ Xn ∩ Y
1 ∩ ... ∩ Y
n

− I (X,Y)
(
(
(
> 
< 
(iv) p
(
(
(
1
n · N 
U
1 ∩ ... ∩ U
n

− I (U)
(
(
(
> 
< 
From Proposition 7.8 we can also estimate the number of the corresponding
high-probability sequences, i.e., N = #(An, ), #(Bn, ), M = #(Cn, ), and also the
number P of high-probability pairs.
Now the idea is to consider only the high-probability elements in Cn, An, Bn,
and An × Bn and to map each high-probability element c = (c1,...,cn) ∈ Cn,
onto a different randomly chosen a = (a1,...,an) that is the first element in a high￾probability pair (a, b). This procedure will work if there are more such a’s than
there are high-probability c’s, and if the probability of finding the first element a
from the second element b in a high-probability pair (a, b) is sufficiently high. In
this case, we can guess first a and then c from the channel output b. In order to
carry out the proof, we now have to estimate the number of these a’s appearing in
high-probability pairs (a, b).
1. Given a high-probability a, we estimate the number Na of high-probability
pairs (a,b) containing a as follows: We use the abbreviations X = (X1,...,Xn),
Y = (Y1,...,Yn), and U = (U1,...,Un) and consider only high-probability9.2 Shannon’s Theorem 115
elements ω ∈ . Then
p(Y
|X)  = p(X, Y ) "
p(X)  ≥ 2−n(I (X,Y)−I (X)+2).
Thus
1 ≥ 
b∈Bn
p(b|a) ≥ Na2−n(I (X,Y)−I (X)+2) and Na ≤ 2n(I (X,Y)−I (X)+2).
Now we have to make sure that
M ≤
P
2n(I (X,Y)−I (X)+2) ≤
P
Na
.
Because of the estimates for M and P from Proposition 7.7, this is true if
2n(I (U)+) ≤ (1 − )2n(I (X)−3).
Since I(X) ≥ T (X,Y) = c>r = I(U), this is certainly true for sufficiently
small .
2. Given a high-probability b ∈ Bn, we estimate the number Nb of high-probability
pairs (a, b) in An × Bn containing b similarly to (1):
p(X|Y )  = p(X, Y ) "
p(Y )  ≥ 2−n(I (X,Y)−I (Y)+2).
Thus
1 ≥ 
a∈An
p(a|b) ≥ Nb2−n(I(X,Y)−I(X)+2) and Nb ≤ 2n(I(X,Y)−I(X)+2).
This number we use to estimate the probability that there is at most one m(c)
occurring as first component among the Nb pairs, for each of the high-probability
b’s at the channel output. More exactly, for a fixed high-probability c, we take
a = m(c) as channel input and obtain b as channel output. Now we ask for
the probability pf that there is another c such that (m(c
), b) is also a high￾probability pair. For fixed b let nb be the number of codewords m(c
) such that
(m(c
), b) is a high-probability pair. Now we can estimate
pf ≤ p[nb ≥ 1] < E(nb)
= M ·
Nb
N
≤ 2n(I (U)+I (X,Y)−I (Y)−I (X)+4)
= 2n(4+r−c).116 9 How to Transmit Information Reliably with Unreliable Elements (Shannon’s. . .
Since I(U)+I(X,Y)−I(Y)−I(X) = r−c < 0, this probability is sufficiently
small for sufficiently large n and sufficiently small .
This means that a high-probability c will be coded into a channel input a in such
a way that with high-probability a can be determined from the channel output b,
and from a one can determine c. What is the error probability in this procedure?
An error may occur when c is not in the high-probability group, or (a, b) is not
in the high-probability group, or b is not in the high-probability group, or if there
is more than one m(c) in Nb. Otherwise, we know which a we have chosen to
correspond to the output b and we know which c has been mapped by m onto a.
Taking our various estimates together, the probability of error is at most 3 +
2n(4+r−c), and it remains to choose  sufficiently small and n sufficiently large
to finish the proof of the theorem. 
There is also a converse of Shannon’s theorem which essentially says that a
source with information rate r that is greater than the channel capacity c cannot
be connected to the channel in such a way that it can be retrieved from the
channel output with high reliability. The proof of this theorem rests essentially on
Proposition 6.1.
Proposition 9.1 Let CAB be a channel with capacity c, U a stationary process with
alphabet C, and I(U) = r > c.
Then there is a δ > 0 such that for any coding m: Cn → An the values of U can
only be determined from the outputs of the combined channel CAB ◦ m with an error
probability of at least δ.
In order to prove Proposition 9.1, we need the following lemma.
Lemma 9.1 Let X, Y be two random variables and m: W (X)→W (Y ), a mapping
such that p[m(X) = Y ] < δ. Then T (X, Y ) > I(Y ) · (1 − δ).
Proof T (X, Y ) ≥ T (m(X), Y ) = I(Y ) − I(Y |m(x)). Let E = [m(X) = Y ]. On
E we have I(Y |m(X)) = 0. On Ec we have I(Y |m(X)) ≤ I(Y ). Thus
T (X, Y ) ≥ p(E) · I(Y ) > I(Y ) · (1 − δ). 
Proof of Proposition 9.1 Define the processes U, X, and Y. Assume that for any
δ > 0 there is a coding m: Cn → An and a decoding f : Bn → Cn such that
p[f (Yn) = Un] < δ. Then by the lemma T (Un, Yn) > I(Un) · (1 − δ), and
therefore
T (Xn, Yn) ≥ T (Un, Yn) > I(Un) · (1 − δ).
Since this holds for every δ > 0, we have T (X,Y) ≥ I (U) and therefore
c ≥ T (X,Y) ≥ I (U) = r. References 117
9.3 Technical Comments
In this chapter, we reproduce essentially the classical proof of Shannon’s theorem
(Shannon, 1948). Several improved versions of this proof have appeared over the
last 50 years perhaps starting with McMillan (1953), giving better constructions
for the choice of high-probability pairs (Feinstein, 1954, 1959) or allowing more
general conditions on the channel. See Gray (1990) for an overview of those more
advanced ideas.
9.4 Exercises
(1) Let P ◦ C be the compound channel obtained from connecting two channels
P and C.
(a) Show that c(P ◦ C) ≤ min{c(P), c(C)}.
(b) Give an example where c(P) = 0 and c(C) = 0, but c(P ◦ C) = 0. This can
be done with deterministic channels P and C.
(c) Show that for any  > 0, one can construct a deterministic “adapter”
channel R such that c(P ◦ R ◦ C) ≥ min{c(P), c(C)} − .
(d) What would be a good adapter channel for P = C, for the two channels of
Exercise (8.9)?
References
Feinstein, A. (1954). A new basic theorem of information theory. IRE Transactions on Information
Theory, 4, 2–22.
Feinstein, A. (1959). On the coding theorem and its converse for finite-memory channels.
Information and Control, 2, 25–44.
Gray, R. M. (1990). Entropy and information theory. Springer.
Kieffer, J. (1981). Block coding for weakly continuous channels. IEEE Transactions on Informa￾tion Theory, 27(6), 721–727.
McMillan, B. (1953). The basic theorems of information theory. Annals of Mathematical Statistics,
24, 196–219.
Pfaffelhuber, E. (1971). Channels with asymptotically decreasing memory and anticipation. IEEE
Transactions on Information Theory, 17(4), 379–385.
Shannon, C. E. (1948). A mathematical theory of communication. Bell Systems Technical Journal,
27, 379–423, 623–656.
Shannon, C. E. & Weaver, W. (1949). The mathematical theory of communication. University of
Illinois Press.
Wolfowitz, J. (1964). Coding theorems of information theory (2nd ed.). Springer-Verlag.Part IV
Repertoires and CoversChapter 10
Repertoires and Descriptions
This chapter introduces the notion of a cover or repertoire and its proper descrip￾tions. Based on the new idea of relating covers and descriptions, some interesting
properties of covers are defined.
Definition 10.1 For a probability space (, 	, p), a cover α is a subset of 	 such
that ∪α = .
1
In general, a cover may be a finite or an infinite set of propositions. For a basic
understanding of the concepts, it will be much easier to consider finite covers.
Part II on coding and information transmission illustrated that information is
an “objective” quantity, which quantifies the amount of information needed to
determine the value of a random variable. On the other hand, novelty is slightly
more “subjective” in the sense that it takes explicitly into account a particular
interpretation or description d of events x, and so it is a measure of the novelty
provided by viewing the world (the events) through d. Of course, interest in the
value of a particular random variable X also implies a particular description of
events, namely, X, but this description is of a special clear-cut type: we have said
it is complete. After having investigated the clear-cut, complete descriptions that
are needed for optimal guessing and coding in Parts II and III, we now want to
come back to the more personal, biased, one-sided descriptions of events. The
opposite extremes to complete descriptions are descriptions that express a very
particular interest pointing only to one direction. They are the directed descriptions,
corresponding to the “narrow” covers defined in Sect. 10.4.
The “worldview” of a person could be characterized as the collection α of all
propositions that (s)he will eventually use (externally or internally) to describe
events in the world. We may also understand α as the collection of all propositions
a person is potentially interested in. Such a collection α will also be called a
repertoire, meaning the repertoire of all elementary observations (propositions),
1 In addition, we usually may require that p(A) = 0 for every A ∈ α.
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_10
121122 10 Repertoires and Descriptions
through which a person views or describes the occurring events x ∈ . One could
assume that such a repertoire should be closed under the logical operations NOT,
AND, and OR, i.e., complement, intersection, and union. This would make it an
algebra. We do not take this point of view here, in particular with respect to negation.
Indeed, “non-table” would not appear as such a natural elementary description of a
thing as “table.”
Definition 10.2 For a cover α and a description d, we define the novelty provided
by d for α (novelty of d for α) by Nα(d) := E(Nα
d ) where
Nα
d (ω) = sup{N (A): d(ω) ⊆ A ∈ α}.
Proposition 10.1 For any finite cover α and any description d, we have
(i) Nα(d) ≤ N (d)
(ii) Nα(d) = N (d) if and only if R(d) ⊆ α.
2
Proof Obvious. 
10.1 Introductory Examples
In the first example of Chap. 5, one is interested in the value of a dice, i.e., in the
six propositions Ai = [X = i] with (i = 1,..., 6). This interest is expressed
in the repertoire α = {[X = i]: i = 1,..., 6}, which is a partition. When one
is only interested in the value X = 6, this can be described by two repertoires:
α1 = {[X = 6],[X = 6]}, and α2 = {[X = 6], }; α1 is a partition, α2 not, because
α2 describes all results different from 6 by “saying nothing.” Here it is a matter of
taste, whether you argue that being interested in the value 6, you should also be
interested in being told that it is not a 6, or you should simply not be interested.
In this book, I have introduced the general concept of novelty in order to be able
to distinguish two opposite extremes of it, namely, information and surprise. α1
describes the information given to someone who is interested in the 6: telling him,
it is not a 6, also provides information. α2 describes the surprise given to someone
who is interested in the 6: telling him, it is not a 6, gives no surprise, and it says
nothing.
The “surprise idea” can also be illustrated by considering statistical tests: in
a statistical test, you are interested in showing that a test-statistics T you have
computed from your data is surprisingly, or significantly, large. This interest can
be captured by the repertoire α = {[T ≥ a]: a > 0}. In this case, [T ≥ a] is the
more surprising, the smaller p[T ≥ a], and indeed p[T ≥ a] is called the level of
significance of the test result T = a.
2 More exactly: for every B ∈ R(d), there is A ∈ α with p(A B) = 0.10.1 Introductory Examples 123
In the second example of Chap. 5, eight cards are placed on the table and one
has to locate an ace among them. In this case, one is interested in the repertoire
α = {[Ci = ace]: i = 1,..., 8}, where Ci ∈ {ace, no ace} describes the i-th card.
Here α is not a partition, because there are two aces among the eight cards.
In the example of Chap. 3, the Monty Hall problem, there is the location of the
sports car S ∈ {1, 2, 3} and the candidate’s guess C ∈ {1, 2, 3}. The quizmaster is
allowed to make a true statement from the repertoire α = {[S = i]: i = 1, 2, 3}.
Here, however, there is another restriction given by the situation: he cannot chose to
say [S = C], because he should not open the door the candidate has chosen.
Further examples for repertoires can be considered in the context of the game
of lotto: six numbers are drawn from L = {1,..., 49} without replacement. If you
have guessed them right, you get a lot of money. Usually, the six numbers are not
displayed in the order they are drawn, but ordered by their size. Thus, we have six
random variables X1 < X2 < X3 < X4 < X5 < X6 with values in L.
If you regularly take part in the game, you want to know the six numbers. This
interest is described by
α = {[X1 = a1, X2 = a2,...,X6 = a6]: a1 < a2 < a3 < a4 < a5 < a6 ∈ L},
which is a partition with large information content (6 pairs of digits have an
information content of about 40 bit, since they are less than 49, it will be about
6 bit less). Normally, you are unable to remember these six numbers, if you
are just told them. You may, however, be able to remember them if they are
“interesting” configurations, for example, (1, 2, 3, 4, 5, 6). It seems natural to say
that this configuration is more surprising than any of the usual configurations,
like (4, 5, 18, 27, 34, 40). Some people may be interested in particular numbers
like 3, 13, 33, or perhaps prime numbers. Here I only want to consider a few
particular configurations: it is surprising if three or more numbers come in a
row. This interest in configurations can be expressed by the repertoire α =
{[3 in a row],[4 in a row],[5 in a row],[6 in a row], }. Another interesting feature
may be the “compactness” of the sequence, i.e., X6 − X1. So we may be interested
in α2 = {[X6−X1 = a]: a ∈ L}. Since small values of X6−X1 are more surprising
than large values, this interest may be even better expressed by β2 = {[X6 − X1 ≤
a]: a ∈ L}. In addition, we may be surprised by large values of X1 or by small
values of X6 as described by
α1 = {[X1 = a]: a ∈ L}, β1 = {[X1 ≥ a]: a ∈ L},
α6 = {[X6 = a]: a ∈ L}, β6 = {[X6 ≤ a]: a ∈ L}.
Let us play a bit with these seven repertoires. First, we should note that X1, X6,
and X6 − X1 do not really take all values in L. For example, X6 cannot be less than
6 and X1 cannot be more than 44. So [X6 = 3] is the empty set, so is [X6 ≤ 4].
Now it does not really matter whether we put the empty set into a repertoire or not.
The empty set cannot be used to describe anything. In our definitions, we usually124 10 Repertoires and Descriptions
assume that ∅ ∈/ α; normally, we even assume that p(A) = 0 for all propositions in
a repertoire.
Let us now define the repertoire α more exactly: A3 = [3 in a row] means
A3 = [Xi+2 = Xi + 2 for i = 1, 2, 3 or 4].
Similarly
A4 = [Xi+3 = Xi + 3 for i = 1, 2 or 3],
A5 = [Xi+4 = Xi + 4 for i = 1 or 2] and
A6 = [X6 = X1 + 5].
So α = {A6, A5, A4, A3, }.
If we use the five propositions in α to describe lottery drawings, we first observe
that α is no partition; on the contrary, A6 ⊆ A5 ⊆ A4 ⊆ A3 ⊆ .
So, if a particular drawing ω ∈  is in A5, for example, it is also in A4 and in
A3, so it may be correctly described by each of these propositions. However, if there
are 5 in a row, we would expect the description by A5, because this is the most exact
description that fits ω, i.e., the smallest set A ∈ α that contains ω.
More generally, for ω ∈  we call αω = {A ∈ α : ω ∈ A} the set of all
possible descriptions of ω in α. What we assume is that a minimal proposition
in αω should be chosen to describe ω. A is “minimal” means here that there is
no other proposition in αω that is contained in A. Such a “minimal” description
is called a proper description in Definition 10.3. Even if we consider only proper
descriptions, there may be several minimal propositions in αω, in general. But for
our repertoire, α defined above, there is indeed only one minimal proposition in αω
for every ω ∈ . So all elements in A6 are described by A6, all elements in A5 \ A6
by A5, all elements in A4 \ A5 by A4, and so on. Also the other repertoires β1, β2,
and β6 have this property.
If we describe the lottery by any of the repertoires α, α2, β2, α6, or β6, we
can understand why ω = (1, 2, 3, 4, 5, 6) is much more surprising than most
other drawings. Indeed, for a repertoire α, it would be reasonable to define the
novelty Nα(ω) as the maximal novelty of all propositions in αω, i.e., Nα(ω) :=
max{N (A): A ∈ αω}. For ω = (1, 2, 3, 4, 5, 6), we then obtain
Nα(ω) = N (A6), which is much larger than N (A5), N (A4), etc.
Nα2 (ω) = N ([X6 − X1 = 5]), again the largest value in α2.
Nβ2 (ω) = N ([X6 − X1 ≤ 5]), the same.
Nα6 (ω) = N ([X6 = 6]), even larger.
Nβ6 (ω) = N ([X6 ≤ 6]), again the same.10.2 Repertoires and Their Relation to Descriptions 125
Next we could try to combine the repertoires defined so far. The simplest com￾bination of two repertoires α and β is their union α ∪ β. So, for example, we
could use α ∪ β6 to describe lottery drawings. For example, ω = (4, 5, 6, 7, 9, 10)
could be described by A4 or by [X6 ≤ 10]. Both would be minimal propositions
about ω in α ∪ β6, and in fact it is not so easy to say which of the two has lower
probability, i.e., larger novelty. If we wanted to impress, we would of course choose
the description of those two that has the larger novelty value. We would get an even
more impressive novelty value if we would allow to combine those two statements
by logical “and.” This leads to the definition of another combination α · β of two
repertoires α and β: α · β := {A ∩ B : A ∈ α, B ∈ β}. In α · β6 we would describe
our ω above by A4 ∩ [X6 ≤ 10]. Also in all these combinations, (1,2,3,4,5,6) has a
uniquely high novelty value.
There is a possibility of ordering repertoires: we can say α ≥ β if for every
ω ∈  any description of ω in terms of β can be inferred from a description of ω in
terms of α. For example, α1 ≥ β1, because [X1 = a] implies [X1 ≥ a]. Similarly,
α2 ≥ β2 and α6 ≥ β6. Also α2 can be inferred from α1 and α6, more exactly from
α1 · α6, because [X1 = a and X6 = b] implies [X6 −X1 = b −a]. In the same way,
β2 can be inferred from β1 and β6, or again β1 · β6, i.e., β2 ≤ β1 · β6.
In summary, we have found the relations
β2 ≤ β1 · β6 ≤ α1 · α6 and β2 ≤ α2 ≤ α1 · α6.
Interestingly, we also have α6 ≤ α1 · α2, but not β6 ≤ β1 · β2. Also α is not related
in any simple way to any of the other repertoires.
Much more could be said about these examples, but I want to stop here and leave
further elaborations and perhaps calculations of probabilities to the reader and to the
exercises.
10.2 Repertoires and Their Relation to Descriptions
From the mathematical point of view, repertoires are essentially characterized as
covers. They are well-studied structures, mainly in topology. We propose a system￾atic treatment of repertoires or covers which collects their important properties in
their relation to descriptions.
It may be easier for the beginner to assume that covers α are finite sets. Also
adding a few propositions A with p(A) = 0 to α does not really change the
information provided by a cover α; thus, we can safely use the notion of repertoires,
as defined in Definition 10.4, which is not quite as general as covers.
Example 10.1 The most trivial examples of covers α are the whole σ-algebra 	
and {}, which have an extremely high or an extremely low information content,
respectively. 126 10 Repertoires and Descriptions
Example 10.2 Another example could be the set E∞ of all propositions about
events ω ∈  that can be formulated in the English language, or the sets En of
all propositions that can be formulated in sentences of at most n letters. Obviously,
each En is finite, En+1 ⊇ En and 
∞
n=1
En = E∞ is countable.
In line with the observations made in Sect. 10.1, it is obvious that the novelty
of the same event ω can be larger if described in En with larger n (n ≥ k
implies NEn (ω) ≥ NEk (ω)). So the real surprise of ω should be evaluated as some
combination of NEn (ω) and n, the idea being that an event ω is more surprising if it
can be described with fewer words (or letters) and has smaller probability.
Also in this line of thinking, one could try to take the shortest English expression
(i.e., the smallest n such that En contains the proposition) as a measure for the
information content of a proposition. A more formalized version of this idea indeed
leads to an alternative, algorithmic approach to information theory by Chaitin
(1975, 1977, 1987) and Kolmogorov (1965, 1968), or to the so-called description
complexity. 
Example 10.3 Another example close to the simple extreme {} is the cover
{,A}, where 0 < p(A) < 1, which describes a simple bet on the proposition
A about events ω ∈ , “I bet that A.” A different example is the cover {A, Ac}. 
Further examples are provided by the range R(d) of a description d, or by the
cover depicted in Fig. 3.2 in Sect. 3.3.
In this section, we want to study the relationship between covers or repertoires
(see Definition 10.4) and descriptions.
Definition 10.3 Given a cover α, a description d is called consistent with or in terms
of or a choice from α, if d(x) ∈ α for every x ∈ . A choice from a cover α is called
proper, if for every x ∈ , there is no element A of α such that x ∈ A ⊂ d(x).
3
We denote by D(α) the set of all proper choices4 from α.
The word “choice” stems from the idea that a particular description d is always
a choice in the sense that for a given x ∈ , d(x) has to be chosen from those
propositions A ∈ α that contain x. Of course, it may also be possible that there is
no choice (or rather only one choice) for a repertoire α.
Proposition 10.2 A cover α admits only one description d, if and only if A∩B = ∅
for any A = B ∈ α.
Proof Obvious. 
3 In line with our general strategy to disregard sets of probability 0, we can interpret A ⊂ d(x) as
p(A \ d(x)) = 0 and p(d(x) \ A) > 0.
4 In the definition of D(α), we understand a description d simply as a mapping d :  → α with
ω ∈ d(ω), i.e., without the additional requirement of Definition 3.3.10.2 Repertoires and Their Relation to Descriptions 127
Such a cover is called disjoint or a partition (cf. Definition 3.6). Note that the
requirement that p(A) = 0 for every A ∈ α implies that a disjoint cover α has to be
countable and 
A∈α
p(A) = 1.
For finite covers, there are always proper choices. Unfortunately, this may no
longer be true for infinite covers as the following example shows.
Example 10.4 α = {(a, b): a<b ∈ R = }. 
Let me explain the idea behind the concept of a proper choice or description:
If you ask someone for a description of Mr. Miller, and in the course of this
description he says “Mr. Miller has two children,” then you usually assume not
only that Mr. Miller has two children but also that Mr. Miller has no more than
two children. On the other hand, if Mr. Miller had three children, it would still be
correct to say that he has two children, since having three children implies having
two children.
In the repertoire of the person describing Mr. Miller, there are certainly propo￾sitions about Mr. Miller of the type “Mr. Miller has n children.” Among these
“Mr. Miller has two children” and “Mr. Miller has three children” are both correct
choices, if Mr. Miller has three children. But we assume that our informer will
choose the stricter of these two statements in this case. His proper choice of
statement should be such that there is no stricter statement available (in his
repertoire) about Mr. Miller that is true.
In the following, we will be mostly concerned with proper choices. We might ask
whether there is always a proper choice for a repertoire α.
We define α(x) := {A ∈ α : x ∈ A}. An element M ∈ α(x) is called minimal, if
there is no A ∈ α(x) with A ⊂ M. Thus, a proper choice d picks for every x ∈ 
a minimal element d(x) ∈ α(x). If α(x) is a finite set for every x ∈ , as will be
usually the case, then it is clear that α(x) has minimal elements and that a proper
choice exists.
Definition 10.4 A cover α is called finitary, if
(i) p(A) = 0 for every A ∈ α and
(ii) For almost every ω ∈  and every A ∈ α containing ω, there is a minimal
proposition B ∈ α with ω ∈ B ⊆ A. Minimality of B means that ω ∈ C ⊆ B
and C ∈ α implies C = B.
A finitary cover is also called a repertoire.
Example 10.5 Take  = R and
α1 = {(a − δ, a + δ), a ∈ R} for δ > 0,
α2 = {(a,∞), a ∈ R},128 10 Repertoires and Descriptions
α3 = {[a,∞), a ∈ R},
α4 = {{ω}: ω ∈ R}.
Which of these covers are finitary? 
Obviously, finite covers with property (i) are finitary. This requirement is used to
ensure the existence of proper choices also from infinite covers.
Proposition 10.3 Let α be a finitary cover. For every ω ∈  and every A ∈ α with
ω ∈ A, there is a proper choice4 d ∈ D(α) such that d(ω) ⊆ A.
Proof We obtain a proper choice d ∈ D(α) by choosing for every ω ∈  a minimal
B ∈ α with ω ∈ B
. Now take ω ∈  and A  ω. Then there is a minimal B ∈ α
with ω ∈ B ⊆ A. We define d(ω) := B. 
For further reference, we define two particular repertoires, a very fine one, called
λ (for large), and a very coarse one, called σ (for small).
Example 10.6 Let  be finite or countable and p(ω) = 0 for every ω ∈ . On
(, 	, p) we define two covers or repertoires:
λ = {{ω}: ω ∈ } and σ = { \ {ω}: ω ∈ }.
Obviously, λ is a partition, whereas the sets in σ are widely overlapping. 
Definition 10.5 A repertoire α is called tight if there is exactly one proper choice
from it. In this case, this choice is called dα.
Example 10.7 The pictures in Fig. 10.1 illustrate tightness.
The repertoire in the right picture is not tight, because for the lower left corner,
either the horizontally hatched region or the diagonally hatched region can be (part
of) a proper choice. Thus, there is more than one proper choice for this repertoire. In
the left picture, the horizontally hatched region cannot be chosen because it contains
each of the diagonally hatched regions and will therefore not be a proper choice in
those regions. 
Fig. 10.1 Illustration of
tightness
tight not tight10.2 Repertoires and Their Relation to Descriptions 129
Example 10.8 A bet on the truth of a single proposition A can also be described by
a repertoire, namely, by α = {A, } (cf. Example 10.3). This repertoire is tight and
the only choice dα is
dα(x) =

A for x ∈ A and
 for x /∈ A.
For a random variable X, we defined the description X≥ in Definition 3.16. The
range α = R 
X≥
of this description is a tight repertoire and dα = X≥ again.
Any partition α is obviously tight, and so is the range of the description X for
any discrete random variable X on . 
Of course, it is straightforward how to associate a cover to a given description d:
we simply consider the range R(d) = {d(x): x ∈ }, which is a repertoire, if R(d)
is finite. Vice versa, we can associate exactly one proper description to a repertoire,
only when it is tight.
Does the process of going from a repertoire to a proper choice and back always
lead to the same initial repertoire? The answer is obviously no. Even for a tight
repertoire α, it is clear that R(dα) ⊆ α, but it may happen that α contains more
elements than R(dα). For example, if R(dα) = α and  /∈ α, we may consider the
repertoire α = α ∪ {}, and then R(dα) = α. But of course this slight difference
between α and α does not seem to be essential.
Definition 10.6 For a cover α, the set
αc := {R(d): d ∈ D(α)}
is called the cleaned version5 of α.
By forming αc, we have removed from α all unnecessary propositions that are
not used in proper choices. These propositions are exactly the (nontrivial) unions of
other propositions in α. Indeed, if A = ∪β is not an element of β for some β ⊆ α,
then for every x ∈ A, A itself is not a proper choice, because x ∈ B ∈ β and B ⊂ A.
The following definition is therefore natural.
Definition 10.7 A cover α is called clean, if for any β ⊆ α with ∪β /∈ β, we have
∪β /∈ α. A tight and clean repertoire is called a template.
One can easily check that the cleaned version αc of a repertoire α is clean
(Exercise 10).
The pictures in Fig. 10.2 illustrate the concept of cleanness.
The right picture does not contain any nontrivial unions, whereas the left one has
the vertically hatched region as union of the two diagonally hatched regions.
5 If D(α) = ∅, we obtain αc = ∅, so αc is not a cover. In this case, we add  to αc. To avoid this
redefinition, one could define αc only for repertoires.130 10 Repertoires and Descriptions
Fig. 10.2 Illustration of
cleanness. The vertically
hatched region on the left can
be removed, because it is the
union of the two diagonally
hatched regions
not clean clean
Definition 10.8 For any cover α, we define
α∪ := {∪β : β ⊆ α}.
The definition of α∪ describes the opposite process of cleaning. In view of the
above discussion, the following proposition appears quite obvious.
Proposition 10.4 For two repertoires6 α and β, we have
(i) αc ⊆ α ⊆ α∪
(ii) D(α) = D(β) implies αc = βc
(iii) D(αc) = D(α) = D(α∪)
(iv) αc∪ = α∪ = α∪∪ and α∪c = αc = αcc
Proof
(i) Obvious,
(ii) Is obvious from the definitions of αc and βc,
(iii) d ∈ D(α) clearly implies d ∈ D(αc). Conversely, d ∈ D(αc) is a choice
from α. If d was not proper, then there is a choice c ⊂ d from α. This is
a contradiction because c is also a choice from αc. d ∈ D(α∪) implies that
d ∈ D(α) because of the above discussion, since a nontrivial union cannot be
chosen by a proper choice. d ∈ D(α) clearly implies d ∈ D(α∪).
(iv) Clearly, αc∪ ⊆ α∪ = α∪∪. To show that α∪ ⊆ αc∪, take B ∈ α∪. For any
x ∈ B, there is a proper choice d(x) ⊆ B from α. Thus, B = 
x∈B
d(x).
The second pair of equations follows from (ii) and (iii).

Proposition 10.5 For repertoires α and β, the following are equivalent:
(i) D(α) = D(β)
(ii) αc = βc
(iii) α∪ = β∪
(iv) αc ⊆ β ⊆ α∪
6 We need this assumption only for the first equation in (iii) and (iv).10.3 Tight Repertoires 131
Proof
(i) ⇔ (ii)‘:⇒’ : is obvious from the definitions of αc.
‘⇐’ : Assume d ∈ D(α), but d /∈ D(β).
This would imply ∃c ∈ D(β): c ⊂ d which contradicts αc = βc.
(ii) ⇒ (iii): With Proposition 10.4(iv), αc = βc ⇒ αc∪ = βc∪ ⇒ α∪ = β∪.
(iii) ⇒ (iv): With Proposition 10.4(iv), α∪ = β∪ ⇒ α∪c = β∪c ⇒ αc = βc.
With Proposition 10.4(iv), αc = βc ⊆ β ⊆ β∪ = α∪.
(iv) ⇒ (ii): (iv) implies αcc ⊆ βc ⊆ α∪c. (ii) follows from αcc = α∪c = αc by
applying Proposition 10.4(iv).

These conditions define an equivalence relation (cf. Definition 16.2) between
repertoires which can be easily extended to covers.
Definition 10.9 For two covers α and β, we write α ∼ β, if α∪ = β∪. This defines
an equivalence relation ∼ on the set of covers.
For repertoires, this equivalence coincides with the conditions of Proposi￾tion 10.5. Also, statement (iv) of Proposition 10.5 describes the equivalence class of
a repertoire α, i.e., all covers β that are equivalent to α, as {β : αc ⊆ β ⊆ α∪}.
10.3 Tight Repertoires
In this section, we concentrate on the characterization of tight repertoires α, i.e., on
those repertoires for which there is a unique choice dα. We begin by investigating
intersections within repertoires. It turns out that stability under intersections is
essentially equivalent to tightness. Then we characterize those descriptions that lead
to tight repertoires.
Proposition 10.6 A repertoire α is tight if and only if for every A,B ∈ α with
A ∩ B = ∅ and any z ∈ A ∩ B, there is a C ∈ α with z ∈ C ⊆ A ∩ B.
Proof If α is tight, the second condition holds obviously for C = dα(z). If α is
not tight, then there are two proper choices c = d from α, i.e., ∃x ∈  where
c(x) = d(x). So c(x), d(x) ∈ α and x ∈ c(x) ∩ d(x) = ∅, but there is no C ∈ α
with x ∈ C ⊆ c(x) ∩ d(x) since c and d are proper choices. 
Definition 10.10 A cover α is called ∩-stable, if A∩B ∈ α for any two A, B ∈ α.
For any cover α, we define
α∩ := {∩β : ∅ = β ⊆ α, β finite}.
The essential property of ∩-stable repertoires is presented in the following
proposition.132 10 Repertoires and Descriptions
Proposition 10.7 Every ∩-stable repertoire is tight.
Proof Let α be ∩-stable. If α is not tight, there are proper choices c and d and
x ∈  such that c(x) = d(x).
Now x ∈ c(x) ∩ d(x) ∈ α is in contradiction to the properness of c or d. 
Obviously, α∩ is ∩-stable and therefore tight. Thus, α∩ is called the tightening
of α. The unique choice d∩ from α∩ can be defined by dα∩(x) :=  α(x). The
natural question now is whether the converse holds, i.e., whether every tight
repertoire is ∩-stable. This turns out to be “almost” true, i.e., up to equivalence ∼.
Proposition 10.8 If a repertoire α is tight, then α∪ is ∩-stable.
Proof Let d be the unique choice from α. For any A,B ∈ α and any x ∈ A ∩ B,
we have d(x) ⊆ A ∩ B. Thus, A ∩ B = {d(x): x ∈ A ∩ B} ∈ α∪. 
In view of Proposition 10.6, it may seem natural to call a description d tight if
for any x, y ∈  such that d(x) ∩ d(y) = ∅, and any z ∈ d(x) ∩ d(y) one has
d(z) ⊆ d(x) ∩ d(y). This condition is the same as Definition 3.11.
The tightening of a description d is defined in Definition 3.12 as the unique
description d∩ that is compatible with the tightening R(d)∩ of its range R(d). With
these observations, we get the following proposition.
Proposition 10.9 The following properties of a description d are equivalent:
(i) d is tight.
(ii) R(d) is tight and d ∈ D(R(d)).
(iii) d coincides with its tightening.
Proof
(i) ⇒ (ii): follows essentially from Proposition 10.6.
(ii) ⇒ (iii) : Let d be a proper choice for R(d) and d a proper choice for R(d)∩.
d
(x) ⊆ d(x), because R(d)∩ ⊇ R(d), and if d
(x) = A ∩ B,
with A,B ∈ R(d), for example (there could be more than two sets
intersecting which does not change the argument), then
d(x) ⊆ A,B ⇒ d(x) ⊆ A ∩ B = d
(x).
(iii) ⇒ (i): Assume d is not tight ⇒ ∃x,y : d(x) ∩ d(y) = ∅, ∃z ∈ d(x) ∩ d(y)
such that d(z) ⊆ d(x) ∩ d(y). Thus, d∩(z) ⊆ d(z) ∩ d(x) ∩ d(y) ⊂
d(z), and therefore d∩ = d.

It may indeed happen that d ∈ D(R(d)) as the following example shows.
Example 10.9 Take  = {1,..., 6}, A = {5, 6} and define d(x) =  for x ∈
{1,..., 5}, and d(6) = A. Then R(d) = {,A} is tight and its only proper choice
dR(d) has dR(d)(5) = A. 10.4 Narrow and Shallow Covers 133
a b a · b
Fig. 10.3 Illustration of the product of two repertoires
Proposition 10.10 For tight repertoires α and β, the following are equivalent:
(i) α ⊆ β∪,
(ii) dα ⊇ dβ.
Proof
(i) ⇒ (ii): dα(x) ∈ β∪ implies that there is a B ∈ β with x ∈ B ⊆ dα(x). Thus,
dβ(x) ⊆ B ⊆ dα(x).
(ii) ⇒ (i): For A ∈ α and x ∈ A, we have dα(x) ⊆ A. Thus A ⊆ 
x∈A
dβ(x) ⊆

x∈A
dα(x) ⊆ A, showing A ∈ β∪.

We close this section making use of intersections to define a basic operation on
repertoires.
Definition 10.11 For two covers α and β, we define the product
α · β := {A ∩ B : A ∈ α, B ∈ β}.
Figure 10.3 illustrates how the product of two simple repertoires is formed.
Proposition 10.11 For two tight covers α and β, the product α · β is tight and
dα·β = dα ∩ dβ.
Proof Obviously, for every A ∈ α · β with x ∈ A, we have dα(x)∩ dβ(x) ⊆ A. For
this reason, there is only one proper choice for α · β. 
It will be seen later, in Part VI, that for tight repertoires α and β, their product
α · β is essentially the smallest tight repertoire containing α and β.
10.4 Narrow and Shallow Covers
Covers are almost arbitrary collections of sets from the σ-algebra 	. One can define
further properties of covers by considering the ordering by set inclusion “⊆.” In
this ordering, they can appear as multilayered hierarchical structures (cf. Fig. 10.4),134 10 Repertoires and Descriptions
Fig. 10.4 The hierarchy for
α = {A, B, C, D, E, F , G, H}
where  = {1, 2, 3, 4, 5, 6},
and A = {1, 2, 3, 4, 6},
B = {2, 4, 5, 6},
C = {1, 2, 3, 4}, D = {1, 6},
E = {4, 5, 6}, F = {3},
G = {4}, H = {5}
Ω
A
C
F G
D
B
E
H
and one can characterize them, for example, by the breadth and depth of these
hierarchies. The following definitions describe the two extreme cases.
Definition 10.12 A cover α is called narrow, if for any two A, B ∈ α either A ⊆ B
or B ⊆ A. A narrow, clean repertoire is called a chain.
For further reference, we formulate the following obvious proposition.
Proposition 10.12
(i) Narrow covers are ∩-stable.
(ii) Finite narrow covers are clean.
(iii) Any finite narrow cover α satisfies α = αc = α∪.
(iv) Narrow repertoires are tight.
(v) Chains are templates.
Proof
(i), (ii) Let α be a narrow cover. For A,B ∈ α the smaller of the two is A ∩B, and
the larger of the two is A ∪ B, which implies (ii).
(iii) Follows from the above (and Proposition 10.7).
(iv), (v) Follows from (i) and Proposition 10.7.

Example 10.10 The following is an example of an infinite narrow cover that is not
clean. Take  = N and α = {{1,...,n} : n ∈ N}∪{}. 
The next definition introduces another class of repertoires which is the extreme
opposite of the previously defined one (Fig. 10.5).10.5 Technical Comments 135
Fig. 10.5 Example for a
shallow repertoire (a) and for
a chain (b)
a b
Definition 10.13 A cover α is called shallow or flat, if there are no two sets
A,B ∈ α satisfying A ⊆ B. Furthermore, we define the flattening7 of a cover α
as αf := {A ∈ α : A maximal in α}, if αf is a cover.
Shallow or flat covers are obviously clean repertoires. Clearly, a cover is shallow
if and only if every choice for it is proper. So for shallow covers, the distinction
between choices and proper choices is unnecessary. {} is the only cover that is
shallow and narrow.
The flattening is a very bold operation on covers or repertoires; it can easily
make them uninteresting, i.e., very coarse. Indeed, every cover α is equivalent to
(Def. 10.9)
α ∪ {} and (α ∪ {})f = {}.
Proposition 10.13 A cover is a partition8 if and only if it is shallow and tight.
Proof Clearly, every partition is shallow and tight. Let α be shallow and tight. Take
A = B ∈ α and assume that there is an x ∈ A ∩ B. Then dα(x) is contained in A
and in B, and thus α cannot be shallow. This shows that α is disjoint and therefore
a partition. 
The picture in Fig. 10.6 illustrates the different classes of repertoires and how
they are related with each other.
10.5 Technical Comments
Together with Chap. 3, this chapter and the next one contain the central ideas of
this book. I developed these ideas over the last 30 years as witnessed by a few
scattered publications (Palm, 1981, 1985, 1996, 2007). Of course, the idea of a cover
7 The flattening of an arbitrary cover may not exist, because αf may not be a cover. An example
for this is α = R(X≥) for a random variable X with R(X) = R. In this case, α has no
maximal elements. If the flattening exists, it is clearly flat. Usually, we consider finite covers which
guarantees that the flattening exists.
8 Here we are using the countable version of Definition 3.4.136 10 Repertoires and Descriptions
all repertoires
tight
template
clean
chain partition shallow
{Ω}
Fig. 10.6 Illustration of classes of repertoires
is classical in topology. In measure theory and probability, it is usually replaced
by a partition. In this chapter, we reinterpret a cover as a repertoire of possible
propositions and thereby create a new universe of more complex structures that
may be used in information theory. The impact of these structures on information
theory will be discussed more formally in Part VI. The connection to the results
obtained in Part I is made by the introduction of proper (i.e., minimal) descriptions
from a repertoire α. The types of covers introduced in this chapter and illustrated in
Fig. 10.6 are essential for the further development of information theory in Part VI.
10.6 Exercises
(1) For the examples of repertoires α in Examples 10.3, 10.7, and 10.9, what is
αc, which of them are tight, and what is the cardinality (number of elements)
of D(α) in each case?
(2) For which repertoires with n elements on a finite space  does D(α) have the
largest (resp. smallest) cardinality? Give an example for each case. What is
the result if || = n?
(3) Prove the following:
Proposition 10.14 For any repertoire α, α∪ is an algebra if and only if αc is
a partition.References 137
(4) Let α be a repertoire and |α| = n. What is the maximal number of elements in
α∪ and in α∩?
(5) When |α| = n and α is a clean repertoire, or a shallow repertoire, respectively,
what is the maximal number of elements in α∪ and in α∩?
(6) Let α be a repertoire and |α| = n. Consider the unique proper choice d for α∩.
What is the maximal number of elements in R(d)? How is R(d)  related to the
algebra σ (α) generated by α?
(7) Given a description d, is there a difference between R(d∩) and R(d)∩?
(8) Given a description d, is there a difference between (d)  ∩ and (d
∩)?
(9) Give an illustrating example for each of the set theoretical differences and
intersections of sets in Fig. 10.6.
(10) Show that αc is clean for any repertoire.
(11) Show that for finite  (and p(ω) = 0 ∀ ω ∈ ) all covers are repertoires.
(12) Give an example that this is not the case when  is countable.
(13) For finite  with || = n,
(a) which are the largest and which are the smallest tight repertoires and what
are their cardinalities?
(b) the same for templates,
(c) the same for flat covers.
(14) Show that σ (Example 10.6) is flat, but not tight (for || > 2).
References
Chaitin, G. J. (1975). Randomness and mathematical proof. Scientific American, 232(5), 47–52.
Chaitin, G. J. (1977). Algorithmic information theory. IBM Journal of Research and Development,
21, 350–359.
Chaitin, G. J. (1987). Algorithmic information theory, volume I of Cambridge tracts in theoretical
computer science. Cambridge University Press.
Kolmogorov, A. N. (1965). Three approaches to the quantitative definition of information.
Problems in Information Transmission, 1, 4–7.
Kolmogorov, A. N. (1968). Logical basis for information theory and probability theory. IEEE
Transactions on Information Theory, 14, 662–664.
Palm, G. (1981). Evidence, information and surprise. Biological Cybernetics, 42(1), 57–68.
Palm, G. (1985). Information und entropie. In H. Hesse (Ed.), Natur und Wissenschaft. Konkurs￾buch Tübingen.
Palm, G. (1996). Information and surprise in brain theory. In G. Rusch, S. J. Schmidt, &
O. Breidbach (Eds.), Innere Repräsentationen—Neue Konzepte der Hirnforschung, DELFIN
Jahrbuch (stw-reihe edition) (pp. 153–173). Suhrkamp.
Palm, G. (2007). Information theory for the brain. In V. Braitenberg, & F. Radermacher (Eds.),
Interdisciplinary approaches to a new understanding of cognition and consciousness: vol.
20 (pp. 215–244). Wissensverarbeitung und Gesellschaft: die Publikationsreihe des FAW/n
Ulm, Ulm.Chapter 11
Novelty, Information and Surprise
of Repertoires
This chapter finally contains the definition of novelty, information, and surprise for
arbitrary covers and in particular for repertoires and some methods for their practical
calculation. We give the broadest possible definitions of these terms for arbitrary
covers, because we use it occasionally in Part VI. Practically, it would be sufficient
to define everything just for repertoires. It turns out that the theories of novelty and
of information on repertoires are both proper extensions of classical information
theory (where complementary theorems hold), which coincide with each other and
with classical information theory, when the repertoires are partitions.
11.1 Introductory Examples
In this chapter, we will acquire the tools to solve one example mentioned in Chaps. 5
and 10. Eight cards are on the table, two of them are aces. One has to find out the
position of one ace. We can describe this situation by the probability space  =
{(a1, a2): 1 ≤ a1 < a2 ≤ 8}, || = 28, or equivalently by two random variables
X1 and X2 ∈ {1,..., 8}, X1 < X2, giving the positions of the two aces. We are
interested in the propositions Ai = {X1 = i or X2 = i}. So let α = {Ai : i =
1,..., 8}. The problem of finding an optimal guessing strategy for α can be divided
into two problems: First we have to find a description d in terms of α with minimal
information content, i.e., I(d) = N (d)  should be as small as possible. Then we
find an optimal guessing strategy for d
. This problem has been solved in Chap. 5.
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_11
139140 11 Novelty, Information, and Surprise of Repertoires
Coming back to the first problem, this is the problem of calculating I (α) and it
is essentially solved by Proposition 11.8. This proposition tells us that we have to
consider “descriptions by ordering” of α. In this case, all orderings give the same
result, so we can as well consider the ordering already provided, starting with A1
and ending with A8. For this ordering, the description by ordering is defined as
follows:
All elements of A1 are described by A1, all elements of A2 \ A1 are described by
A2, all elements in A3 \ (A2 ∪ A1) are described by A3, and so on.
Thus, d(ω) describes ω by the set Ai  ω with the smallest index. In other words,
d(ω) gives the position of the first ace. This actually means that d
= X1. So we have
reduced our problem to the familiar problem of determining an optimal code for the
random variable X1. It is easy to calculate the probabilities that [X1 = i] and to
work out I(X1) ≈ 2.61 (cf. Exercise 7).
Next I want to give some examples illustrating the definition of surprise S(α).
Someone, Mr. Miller, wants to prove the efficiency of a certain medication by a
statistical test. To this end, he has acquired 2000 data points: each comparing the
result with and without medication on the same patient. A textbook on statistical
testing suggests a certain test T and mentions that this test should be carried out
with at least 10, better 20, data points. Of course, it is also recommended to use as
many data points as possible.
Now Mr. Miller has the idea to divide his 2000 data points into 100 batches of 20
and perform 100 tests with the idea to report the best result.
He finds out that six of his tests lead to significant results and two are even highly
significant. Instead of simply mentioning one of those highly significant results, he
now starts to wonder that two significant results should somehow be more significant
than one significant result. A friend tells him that this is obviously the case because
the 100 tests were performed on different persons and therefore can be assumed
to be independent. Thus, the significance probabilities can simply be multiplied.
But what about the nonsignificant results? Well, says his friend, even if they have
rather high-significance probabilities pi, these are certainly smaller than 1 and so
they can safely be multiplied also, but they will just make the resulting significance
probability smaller.
What do you think about this argumentation?
The 100 statistical tests can be described by 100 random variables Ti, and we
can indeed assume them to be independent. The significance probability pi of Ti
is p(T ≥
i ), which is also described by the cover αi = {[Ti ≥ x]: x > 0}. The first
idea, to report the best result, corresponds to the repertoire α = 100

i=1
αi, whereas the
suggestion of the friend corresponds to the repertoire β = α1 · α2 · ... · α100.11.2 Definitions and Properties 141
What Mr. Miller and his friend intend to report as significance of ω is in fact the
probability related to the novelty Nα(ω) or Nβ(ω).
In a proper statistical test, one should not report these values without comment.
Rather, one should report the probability that this amount of novelty (Nα(ω) or
Nβ(ω)) can be obtained under the 0-hypothesis, i.e., under chance assumption. The
negative logarithm of this probability is exactly what we have defined as the surprise
Sα(ω) (resp. Sβ (ω)). For example, under the 0-hypothesis, one would expect about
5 of 100 statistical tests to be significant on the 5% level.
It turns out to be not so easy to compute the surprise values for the examples α
and β given here. This will be carried out in Chap. 14.1.
Another problem of this type could happen to a person who considers the
statistics of earthquakes. He has the impression that there are periods in time where
there are particularly many earthquakes. In his data, he has 1000 earthquakes and
their times can be described by random variables 0 < X1 < X2 < ... < X1,000.
He wants to substantiate his impression by statistics. So he considers the proposi￾tions Akit = [Xi+k − Xi ≤ t] for all k ≥ 2 and 0 < t, i.e., the periods where k + 1
earthquakes happened in a short time t. As a 0-hypothesis for the probabilities of
these events, he takes the exponential distribution on the random variables Xi. So
for each event of this type in his data, he obtains a probability. Can he use these
probabilities as significance probabilities?
The propositions of interest can be put together in the cover
α = {Akit : k ≥ 2, 0 < t, i = 1,..., 1, 000 − k}.
For any ω ∈  or any collection of values for X1, X2,...,X1,000, we can deter￾mine the differences Xi+k − Xi = tik , find tk = min
i
tik , compute the probabilities
p[Xi+k − Xi ≤ tk] = p[Xk+1 − X1 ≤ tk], and take the minimum p∗ of these
probabilities. This probability again corresponds to the novelty Nα(ω) by Nα(ω) =
− logp∗. Again, for statistical significance, we need to compute the probability that
p∗ is not larger than the value found, or equivalently that Nα ≥ Nα(ω). This is the
surprise Sα(ω). Again this computation is not so simple: we will come back to this
problem in Chap. 13.
11.2 Definitions and Properties
In this section, we shall extend the concepts of novelty, information, and surprise to
the framework of repertoires. We begin by introducing five quantities.142 11 Novelty, Information, and Surprise of Repertoires
Definition 11.1 Let α be a repertoire. We define1,2,3
N (α) := max{N (d): d ∈ D(α)},
I(α) := min{I(d): d ∈ D(α)},
+I(α) := min{I(d): d ∈ D(α), N (d) = N (α)},
S(α) := max{S(d): d ∈ D(α)},
S
+(α) := max{S(d): d ∈ D(α), N (d) = N (α)}.
These definitions are formulated in the most general way for arbitrary repertoires.
Usually, they will be applied to finite repertoires. In more general cases, some
technical details have to be observed as indicated in the footnotes. Clearly, N (α)
is called the novelty of α, both I(α) and +I(α) may be called the information of α,
and both S(α) and S
+(α) may be called the surprise of α.
Proposition 11.1 For any repertoire α, the following inequalities hold:
1
ln 2 ≥ S(α) ≥ S
+(α) ≤ N (α) ≤ +I(α) ≥ I(α).
Proof
(i) For any d ∈ D(α), S(d) ≤ 1
ln 2 by Proposition 3.11; thus, S
+(α) ≤ 1
ln 2 .
(ii) S(α) ≥ S
+(α) is obvious.
(iii) +I(α) ≥ I(α) is obvious.
(iv) N (α) ≤ +I(α), because I(d) ≥ N (d) for any d.
(v) S
+(α) ≤ N (α). Assume S(d) = S
+(α) which implies N (d) = N (α).
Let d(ω) = A, and then p(A) = min{p(B): ω ∈ B ∈ α}.
Now A = [p(d) ≤ p(A)]={ω
: p(d(ω
)) ≤ p(A)} ⊇ A and we are done.
Indeed ω ∈ A ⇒ p(d(ω
)) ≤ p(A) ⇒ ω ∈ A.

1 For arbitrary, possibly uncountable, repertoires, the max may be a sup (i.e., not attained). It may
also be that the expectation does not exist or is infinite for some d ∈ D(α); in this case, the max is
defined as ∞. For finite repertoires α, we will see that the max exists and is finite.
2 For arbitrary, possible uncountable covers, the min may be a inf (i.e., not attained). It may also
be that the expectation does not exist or is infinite for all d ∈ D(α); in this case, the min is defined
as ∞. For finite repertoires, α we will see that the min exists and is finite.
3 If there is no d ∈ D(α) with N (d) = N (α), this definition is not reasonable. In this case,
it should be replaced by +I(α) = lim an→N(α)
min{I(d): d ∈ D(α), N (d) ≥ an} and S
+(α) =
lim an→N(α) max{S(d): d ∈ D(α), N (d) ≥ an}.11.2 Definitions and Properties 143
We now can ask in which cases we have equalities or strict inequalities in
Proposition 11.1.
1. Usually, S
+(α) = S(α), but S(α) > S
+(α) is also possible when S
+(α) is very
small (see Example 11.2).
2. Usually, S
+(α) < N (α) and N can become much larger than S (see Exam￾ple 11.1). S
+(α) = N (α) can only happen when α is a chain (Proposition 11.4).
3. N (α) <+I(α) is usual and again +I can be much larger than N (see Exam￾ple 11.1). +I(α) = N (α) can only happen when α contains a “small” partition
(Proposition 11.3).
4. +I(α) > I(α) is most commonly the case, but also +I(α) = I(α) is easily
possible. When for any ω ∈  there are no different proper choices A,B ∈ α
with p(A) = p(B), then obviously +I(α) = I(α). But even when for some
ω ∈  there are different proper choices A,B ∈ α with p(A) = p(B), it is still
possible that +I(α) = I(α).
5. Between I(α) and N (α), every relation is easily possible. Here it is possible that
I is much larger than N and vice versa (see the following examples).
Example 11.1 Take (, 	, p) = D and α = {{1, 2, 3, 4,},{4, 5, 6}} and β =
{{1, 2, 3, 4},{4, 5, 6},{3, 4}}. In both cases, there are two possible choices (for
4 ∈):
N (α) = 1
2 log2
6
4 +
1
2 log2 2 = 1
2 log2 3,
N (β) = 1
3 log2 3 +
1
3 +
1
3 log2
3
2 = 2
3 log2 3,
I(α) = 4
6 log2
6
4 +
1
3 log2 3 = log2 3 − 2
3
,
I(β) = 1
3 log2 3 +
1
6 log2 6 +
1
2 = 1
2 log2 3 +
2
3
,
+I(α) = 1,
+I(β) = log2 3,
S(α) = 1
2 = S
+(α),
S(β) = 1
3 log2 3 +
1
3 log2
3
2 = S
+(β),
where N (α) > I(α) and N (β) < I(β). 
Example 11.2 A more extreme example is
γ = {{1,..., 5},{1, 6},{2, 6},{3, 6},{4, 6},{5, 6}}.144 11 Novelty, Information, and Surprise of Repertoires
Here N (γ ) is considerably larger than I(γ ). Also S(γ ) is larger than S
+(γ ):
N (γ ) = log2 3, I(γ ) = 5
6 log2
6
5 +
1
6 log2 6, S(γ ) = 1
3 log2 3,
+I(γ ) = 1
3 log2 3 +
2
3 log2 6, S
+(γ ) = 0.

An interesting property of novelty is that it can be maximized (or minimized)
locally, i.e., at every ω ∈ . This means that we can define the novelty of ω as4
Nα(ω) = sup {N (A): A ∈ αω}.
for every ω ∈  and N (α) = E(Nα). In this aspect, novelty and information differ
from each other since the “information of ω” depends strictly on d(ω)  and not on
the proposition d(ω). Similarly, surprise can be defined locally by S
+(α) = S(Nα).
Proposition 11.2 For every d ∈ D(α) with N (d) = N (α), we have
S(d) = N 
N≥
α

.
Therefore
S
+(α) = S(Nα).
Proof Because N (d) = N (α), we have Nd (ω) = N (d(ω)) = Nα(ω) for every
ω ∈ . Thus, S(d) Proposition2.10 = N (N≥
α ) = S(Nα). 
In the following example, we consider two simple infinite repertoires.
Example 11.3 Let us take the probability space (R+,B, p), where p is the
exponential distribution, i.e., p([0, x]) = 
x
0
e−t
dt = 1 − e−x. On  = R+ we
consider α = {[a, a + 1]: a ≥ 0} and β = {[a,∞): a ≥ 0}. D(α) is very large,
and D(β) is very small: it contains just one description b : x → [x,∞).
Here N (α) = N (d) for d : x → [x,x + 1], yielding N (α) = log2(e) −
log2(1 − e−1) after some computation (see the next example), S(α) = S
+(α) =
S(d) = S(β) = 1
ln 2 . On the other hand, I(α) = I(c) for c : x → [$x%, $x% + 1],
which is again computed in the next example, but +I(α) = I(d) = ∞. N (β) =
N (b) for b : x → [x,∞), S(β) = S
+(β) = S(b) = N (b), and I(β) = +I(β) =
I(b) = ∞.
4 Nα is a random variable if α is at most countable. It may happen that Nα(ω) = ∞ on a set of
nonzero probability. In this case, of course, E(Nα) = ∞.11.2 Definitions and Properties 145
Next we calculate
N (b) =
 ∞
0
e−x · (− log e−x )dx = 1
ln 2  ∞
0
x · e−xdx = 1
ln 2  ∞
0
 ∞
0
1[y≤x]e−x dydx
= 1
ln 2  ∞
0
e−y dy = 1
ln 2 ≈ 1.44269,
N (d) =
 ∞
0
e−x · (− log(e−x − e−x−1))dx = 1
ln 2  ∞
0
e−x · (x − ln(1 − e−1))dx
= 1 − ln(1 − e−1)
ln 2 ≈ 2.10442,
N (c) = ∞
j=0
(e−j − e−j−1)(− log(e−j − e−j−1))
= (1 − e−1) · 1
ln 2 ·
∞
j=0
e−j (j − ln(1 − e−1))
= 1 − e−1
ln 2 ·
 e−1
(1 − e−1)2 − ln(1 − e−1)
1 − e−1

= 1
ln 2 ·
 e−1
1 − e−1 − ln(1 − e−1)

≈ 1.50134.

Example 11.4 Let us now consider Example 10.5 and evaluate N , I and the other
quantities for the exponential distribution. For α1 and x ∈ R+, we obtain
Nα1 (x) = sup{N (A): x ∈ A ∈ α1}
= − log2

e−x − e−(x+2δ)
= − log2(e−x) − log2(1 − e−2δ
)    =d>0
= x
ln 2 + d.
So N (α1) = 
∞
0
( x
ln 2 + d)e−x dx = 1
ln 2 + d.
For the calculation of +I(α1), we need the definition given in the footnote to
Definition 11.1 and obtain +I(α1) = ∞.146 11 Novelty, Information, and Surprise of Repertoires
I(α1) is obtained as I(d) for the description d that corresponds to the partition
{(2iδ, 2(i + 1)δ): i ∈ N ∪ {0}}, which actually covers only R+ \ Z where
Z = {2iδ : i ∈ N ∪ {0}}
is a 0-probability set.
We get
I(α1) = I(d) = ∞
i=0
e−2iδ (1 − e−2δ
)(
2iδ
ln 2 + d)
= (1 − e−2δ
)
∞
i=0
2iδ
ln 2e−2iδ +∞
i=0
de−2iδ
= 2δ
ln 2 · e−2δ
1 − e−2δ + d.
For δ → 0, I becomes 1
ln 2 +d which goes to infinity like − log2 δ. For increasing
δ, the information I decreases monotonically toward 0.
α2 is not a repertoire.
For the exponential distribution, α3 is the same as β from Example 11.3.
For α3 and x ∈ R+, we obtain
Nα3 (x) = max{N (A): x ∈ A ∈ α3} = log(e−x) = x
ln 2.
So N (α3) = 
∞
0
x
ln 2 e−x dx = 1
ln 2 . Actually, for α3 we have exactly one proper
description, namely, d(x) = [x,∞).
Again +I(α2) = ∞.
Here I(α2) = 0 is obtained for a description d that has d(x) = (0,∞) for every
x > 0.
α4 is not a repertoire, but it would be quite obvious that Nα4 (x) = ∞ for any
x ∈ R and we obtain N (α4) = +I(α4) = I(α4) = ∞. 
Now we proceed to prove some of the statements made in the remarks following
Proposition 11.1.
Definition 11.2 Let α be a cover. A proposition A ∈ α is called small in α, if for
almost every x ∈ A and every B ∈ α with x ∈ B, we have p(B) ≥ p(A).
Proposition 11.3 For a repertoire α, the following statements are equivalent:
(i) N (α) = +I(α) < ∞.
(ii) α contains a small partition, i.e., a partition of small propositions.11.2 Definitions and Properties 147
Proof
(ii) ⇒ (i): Let α be a partition that is small in α. For x ∈ A ∈ α define d(x) =A.
Then N (α) = N (d) = − 
A∈α
p(A)log2 p(A) = I(d) = +I(α).
(i) ⇒ (ii): Let +I(α) = I(d) where N (d) = N (α) = +I(α). Thus, N (d) = I(d)
and d is complete, i.e., R(d) is a partition. For x ∈ d(ω), we have
Nd (x) = Nα(x), i.e., p(d(x)) = min{p(A): x ∈ A ∈ α}. Thus, R(d)
is small.

Proposition 11.4 For any countable repertoire α, the following statements are
equivalent:
(i) N (α) = S
+(α).
(ii) α is a chain.
Proof
(ii) ⇒ (i): Let α be a chain, and then d is tight with D(α) = {d} and d is directed.
Thus, N (α) = N (d) = S(d) = S(α).
(i) ⇒ (ii): E(Nα) = N (α) = S(α) = E(Sα). Let α = {Ai : i ∈ Z} and p(Ai) ≤
p(Ai+1) and p(Ai+1 \ Ai) = 0.
Let Bi := 
j≤i
Aj . For ω ∈ Ai \ Ai−1, we have Nα(ω) = N (Ai) and
Sα(ω) = N (Bi). Since Ai ⊆ Bi, we have N (Ai) ≥ N (Bi), so Nα ≥
Sα. In order to get E(Nα) = E(Sα), we need that N (Ai) = N (Bi) and
therefore p(Bi \ Ai) = p(Bi) − p(Ai) = 0, i.e., Bi = Ai essentially.
Thus, α = {Bi : i ∈ Z} is a chain.

In the following example, it happens that S(α) = N (α) although α is not a chain.
So, unfortunately, Proposition 11.4 only works with S
+, not with S.
Example 11.5 Take  = {1,..., 6} with
p(1) = a ≈ 0.03 p(2) = 0.2 − a p(3) = 0.3 = p(6) p(4) = 0.1 = p(5).
For α = {{1, 2, 3, 4}    A1
,{1, 2, 3, 5}    A2
,{2, 3, 4, 5, 6}    A3
}, we get
N (α) = 0.7 log2
10
6 − 0.3 log2(1 − a),
S
+(α) = 0.7 log2
10
7 = 0.36020,
S(α) = −0.4 log2 0.4 = 0.52877.148 11 Novelty, Information, and Surprise of Repertoires
It turns out that S(α) = N (α) for the right choice of a, namely, a = 0.0293.
S(α) is obtained for the description d which has d(1) = d(2) = d(4) = A1,
d(5) = A2, and d(6) = d(3) = A3. 
Before we proceed, we notice that all quantities defined in Definition 11.1 really
only depend on D(α). Thus, it is useful to define the following equivalence relation
and ordering on repertoires.
Definition 11.3 Two repertoires α and β are called equivalent α ∼ β,
if D(α) = D(β). Furthermore, we define α ≤ β by α ⊆ β∪.
The definition of ∼ coincides with the more general Definition 10.9 (i.e., α∪ =
β∪). Also the ≤-relation as defined here can be used for arbitrary covers.
Proposition 11.5
(i) The relation ∼ is an equivalence relation on repertoires.
(ii) For α ∼ β, we have N (α) = N (β), I(α) = I(β), +I(α) = +I(β), S(α) =
S(β) and S
+(α) = S
+(β).
(iii) We have α ∼ β if and only if α ≤ β and β ≤ α.
(iv) For α ≤ β we have N (α) ≤ N (β).
Proof (i), (ii) are obvious.
(iii) ‘⇒’: By Proposition 10.4 D(α) = D(β) implies αc = βc which implies5
α∪ = αc∪ = βc∪ = β∪.
1. ‘⇐’: α ⊆ β∪ implies α∪ ⊆ β∪∪ = β∪ and vice versa. So α∪ = β∪: Again by
Proposition 10.4 D(α) = D(α∪) = D(β∪) = D(β).
(iv) Take c ∈ D(α). For ω ∈ , c(ω) ∈ α ⊆ β∪, so there is a minimal B ∈ β with
ω ∈ B ⊆ c(ω). Now we define d(ω) = B and obtain d ∈ D(β) with d ⊆ c
and therefore N (d) ≥ N (c).

In particular, αc and α∪ have the same information, novelty, and surprise as
a repertoire α. The equivalence classes of ∼ have already been determined in
Proposition 10.3. Property (iv) does not hold for I and +I; see Exercise (11).
The next proposition shows an interesting property of N .
Proposition 11.6 Let α and β be two repertoires.
(i) Nα∪β(ω) = max(Nα(ω), Nβ(ω)) ∀ω ∈ 
(ii) N (α), N (β) ≤ N (α ∪ β) ≤ N (α) + N (β)
Proof Exercise (6). 
5 For this we need α and β to be finitary.11.2 Definitions and Properties 149
The most important inequalities to be proved about information measures are
monotonicity and subadditivity. Propositions 11.5 and 11.6 mean that the novelty
N is monotonic as well as subadditive for the ordering ≤ (α ∪ β is the natural
supremum of α and β for this ordering as we will see in Chap. 17). The same is not
true for information as the following counterexample shows.
Example 11.6 Consider two dice, i.e., (, 	, p) = D2, with the two random
variables X1 and X2. We define the following repertoires:
α = {[X1 ≤ 3],[X1 > 3]},
β = {[X2 ≤ 3],[X2 > 3]},
γ = R(X1),
δ = R(X2).
Observe that
I(β ∪ γ ) = min(I(β), I(γ )) = I(β) < I(γ )
and thus I is not monotonic for ≤. And that
I(α ∪ δ) = I(α) = 1 and I(β ∪ γ ) = I(β) = 1,
whereas
I((α ∪ δ) · (β ∪ γ )) = I(γ · δ) = I(γ ) + I(δ) > I(α) + I(β)
and even
I((α ∪ δ) ∪ (β ∪ γ )) = I(δ) > I(α) + I(β)
and thus I is not subadditive (i.e., in general I(α ∪ β) and also I(α · β) can be
greater than I(α) + I(β)). 
Of course, we should be able to obtain monotonicity and subadditivity also for I
for reasonably large subclasses of repertoires. This is indeed the case for tight and
for flat repertoires as we will see in Chap. 18.
In order to focus and simplify this discussion of information properties on various
sets of covers, we formally introduce symbols for the most important sets. They will
be the subject of Part VI.
Definition 11.4 Let (, 	, p) be a probability space.
(i) C(	) is the set of all (measurable) covers,
(ii) R(	) is the set of all repertoires,
(iii) T(	) is the set of all templates,150 11 Novelty, Information, and Surprise of Repertoires
(iv) F(	) is the set of all flat covers,
(v) P(	) is the set of all (possibly countable) partitions.
Next we ask the question for which repertoires α we can achieve maximal and
minimal values of S, S
+, I, +I, and N . To this end, we consider a finite or countable
space (, 	, p) with p(ω) = 0 for every ω ∈ . It is quite obvious that the minimal
value for all these quantities S = S
+ = N = +I = I = 0 is achieved for α = {}.
On the other hand, the maximal value for N = +I = I is achieved for the partition
λ introduced in Example 10.6. For || = n it is at most log n (for p(ω) = 1
n for
every ω ∈ ) and for infinite  it can be infinite (Proposition 3.17). The maximal
value of S and S
+is rather small, which is the subject of Exercises (11.13). The final
problem we have to tackle in this chapter is how to compute I and +I in concrete
cases. (Computing N , S
+, and S is in fact quite simple as we saw above.) The next
section is devoted to this problem.
11.3 Finding Descriptions with Minimal Information
Computing I and +I for a repertoire α means to find a description d in α with
minimal information. The following lemma is a first step toward this end.
Lemma 11.1 For a probability vector p = (p1,...,pn) with two components
pi < pj and any number q between pi and pj , we may form the vector p by
p
k := pk for k = i, j and
p
i := q and
p
j := pi + pj − q.
Then I(p
) > I(p).
Proof Let h(x) := −x log2 x for x > 0; h is a convex function in the sense that
its second derivative is negative, i.e., h < 0, and I(p) = n
i=1
h(pi). Let r =
q − pi
pj − pi
∈ (0, 1). Then
r · pi + (1 − r)pj = −q + pj + pi = p
j
and
r · pj + (1 − r)pi = q = p
i.11.3 Finding Descriptions with Minimal Information 151
Thus
h(p
i) + h(p
j )>r · h(pj ) + (1 − r)h(pi) + (1 − r)h(pj ) + r · h(pi)
= h(pj ) + h(pi).

This lemma provides the essential justification for the following idea: for any
d ∈ D(α), the propositions in R(d)  that are used for d
will be contained in elements
of α. The idea now is that we can restrict our search to partitions β built from
propositions B that are formed by means of complements and intersections from α.
The following definition introduces descriptions defined by orderings within
repertoires which are built using set differences.
Definition 11.5 Let α be a repertoire with n elements.6 A one-to-one mapping
a : {1,...,n} → α is called an ordering of α. Given an ordering a of α, the
description da for α by the ordering a is defined as
da(ω) = a(1) for every ω ∈ a(1),
da(ω) = a(2) for every remaining ω ∈ a(2), i.e., for ω ∈ a(2) \ a(1),
da(ω) = a(3) for every remaining ω ∈ a(3), i.e., for ω ∈ a(3) \ (a(1) ∪ a(2)),
da(ω) = a(n) for ω ∈ a(n) \ (a(1) ∪ ... ∪ a(n − 1)).
Any description d such that d = da for an ordering a of α is called a description
by ordering of α. Note that dα(ω) = a(k) for k = min{i : ω ∈ a(i)}.
Proposition 11.7 For any finite repertoire7 α, the minimum
min{I(d): d description in α}
is obtained at a description da by ordering of α.
Proof Let d be any description for α, R(d)  = {D1,...,Dn}. R(d)  is a partition,
I(d) = n
i=1
h(p(Di)), and we may in addition assume that p(D1) ≥ ... ≥ p(Dn).
Define A1 := d(D1).
6 This definition can be easily extended to countable repertoires with a : N → α.
7 This proposition can be easily extended to countable repertoires, and even to arbitrary repertoires.
This is because any “reasonable” description d has a countable range R(d) ⊆ α (see Defini￾tion 3.3).152 11 Novelty, Information, and Surprise of Repertoires
Now consider the description d1 defined by
d1(ω) =

A1 for ω ∈ A1,
d(ω) otherwise.
Let h(x) = −x log x for x > 0 as in the proof of Lemma 11.1.
We have p(A1 \ D1) = n
i=2
p(A1 ∩ Di) and therefore
I(d1) = h(p(D1) +n
i=2
p(A1 ∩ Di)) +n
i=2
h(p(Di) − p(A1 ∩ Di)).
Lemma 11.1 shows that I(d1) ≤ I(d).
Now we reorder R(d
1) = {A1 = D1
1 , D1
2 ,...,D1
n} such that again p(D1
1 ) ≥
p(D1
2 ) ≥ ... ≥ p(D1
n) and define A2 := d1(D1
2). Then we define
d2(ω) =

A2 for ω ∈ A2 \ A1,
d1(ω) otherwise.
Again Lemma 11.1 shows that I(d2) ≤ I(d1). So we proceed until we obtain dn
with I(dn) ≤ ... ≤ I(d1) ≤ I(d); dn is a description by ordering of α. 
Note that in Proposition 11.7 we have not yet considered proper descriptions. The
condition of properness further constrains the order in which we subtract subsets.
This is the subject of the next two propositions.
Definition 11.6 For a repertoire α, we define A := A \
{B ∈ α : B ⊂ A} for
any A ∈ α and α := {A : A ∈ α} \ {∅} called the difference repertoire of α.
α is a cover with the interesting property that for ω ∈  and A ∈ α, we have
ω ∈ A if and only if A is minimal in αω.
The idea in the definition of α is the same that led to the definition of the
completion d
of a description d. When we know that a person could also take the
description B ⊂ A instead of A which would be more exact, we can assume that ω
is not in B, when he describes ω just by A. So actually we can infer that ω ∈ A 
when it is described by A. However, this kind of completion is only partial, because
in general α = α , i.e., α is not yet flat. The following is an example for this
(cf. Exercise 1).
Example 11.7 Let  = {1,..., 6} and
α = {{1},{1, 2},{1, 3},{3, 4},{2, 4, 6},{2, 3, 5, 6}}.
Then α = {{1},{2},{3},{3, 4},{2, 4, 6},{2, 3, 5, 6}},11.3 Finding Descriptions with Minimal Information 153
α = {{1},{2},{3},{4},{4, 6},{5, 6}},
α = {{1},{2},{3},{4},{6},{5, 6}},
α = {{1},{2},{3},{4},{5},{6}}.

Proposition 11.8 For any repertoire α, its difference repertoire α satisfies
α ≥ α.
Proof Clearly, α is a repertoire and α ≥ α because any A ∈ α can be
written as A = ∪{B : B ∈ α, B ⊆ A}. Indeed, for any ω ∈ A consider
{B ∈ α : ω ∈ B ⊆ A}. If B∗ is minimal in this set, then ω ∈ B 
∗ . 
Proposition 11.9 For any flat cover α, we have α = α.
Proof If α is flat, then for any A ∈ α clearly {B ∈ α : B ⊂ A}=∅. 
Proposition 11.10 For any repertoire α, we have
I(α) = min{I(d): d description in α }
and
+I(α) = min{I(d): d description in α and N (d) = N (α)}.
Proof If d is a proper description in α, then d
(ω) := d(ω) defines a description in
α ; in fact, ω ∈ d(ω) because d is proper. Moreover, d(ω
) = d(ω) ⇒ d
(ω
) =
d
(ω) and therefore d
⊆ d
 implying N (d)  ≥ N (d

). Vice versa, if d is a description
in α and for each A ∈ α we arbitrarily choose one A ∈ α such that A = A ,
then d
(ω) := (d(ω)) defines a proper description in α. Indeed, d is proper because
ω ∈ d(ω) = d
(ω) . Moreover, d(ω) = d(y) ⇒ d
(ω) = d
(y) and therefore
d
⊆ d
 implying N (d)  ≥ N (d

). 
Propositions 11.7 and 11.10 can be applied directly to calculate I(α); they
also can be combined to give a simple characterization of the description d that
minimizes the information.
Definition 11.7 Let α be a finite repertoire. A description d ∈ D(α) is called
orderly, if there is an ordering a of α such that d is defined as follows:
For ω ∈  let k(ω) := min{i : a(i) is minimal in αω}, and then d(ω) :=
a(k(ω)).
Obviously, any orderly description is proper.154 11 Novelty, Information, and Surprise of Repertoires
Proposition 11.11 Let α be a finite8 repertoire. The minimum in the definition
of I(α) is obtained at an orderly description in α. Similarly, the minimum in the
definition of +I(α) is obtained at an orderly description.
Proof First, we form the partial completion α of α. From Propositions 11.7
and 11.10, we know that there is an ordering of αp, so that we can write α =
{A 
1 ,...,A 
n }, and a description d in α with d
(ω) = A 
k for k = min{i : ω ∈
A 
i } such that I(α) = I(d
).
If we now define
d(ω) := Ak for k = min{i : Ai is minimal in αω} = min{i : ω ∈ A 
i },
then d(y) = Ak = d(ω) ⇒ k = min{i : ω ∈ A 
i } ⇒ d
(y) = A 
k = d
(ω).
Thus, d
= d
 and I(d) = I(d
). 
Example 11.8 We take the throwing of two dice X1 and X2 as our basic experi￾ment, i.e., (, 	, p) = D2. We consider a simple example of a repertoire α. Take
A1 = {(5, 5), (6, 6)}, A2 = [X1 = X2], A3 = [X1 ≥ 5],
A4 = [X1 = 1]∪[X2 = 1], A5 = , and α = {A1, A2, A3, A4, A5}.
Next we form α = {A
1,...,A
5} where A
1 = A1, A
2 = A2 \ A1, A
3 = A3 \ A1,
A
4 = A4, and A
5 =  \
4
i=1 Ai.
Now we have to find a choice d from α that minimizes I. To this end, we notice
that we have only one choice for the elements in A
1 and A
5. We have a real choice
for A
2 ∩ A
4 = {(1, 1)} and for A
3 ∩ A
4 = {(5, 1), (6, 1)}. So we only have to
consider the ordering of A2, A3, and A4 to find the optimal orderly description for
α (or ordered description for α ). Let us explicitly consider the cases:
{A1, A2, A3, A4, A5} leads to the partition {A
1, A
2, A
3, A
4 \ (A
2 ∪ A
3), A
5} = α1
{A1, A2, A4, A3, A5} leads to the partition {A
1, A
2, A
4 \ A
2, A
3 \ A
4, A
5} = α2
{A1, A3, A2, A4, A5} leads to the partition {A
1, A
3, A
2, A
4 \ (A
2 ∪ A
3), A
5} = α1
{A2, A3, A4, A2, A5} leads to the partition {A
1, A
3, A
4 \ A
3, A
2 \ A
4, A
5} = α3
{A1, A4, A2, A3, A5} leads to the partition {A
1, A
4, A
2 \ A
4, A
3 \ A
4, A
5} = α4
{A1, A4, A3, A2, A5} leads to the partition {A
1, A
4, A
3 \ A
4, A
2 \ A
4, A
5} = α4
For each of these partitions, we can calculate the information. The best of these six
possibilities is α4 with I(α4) ≈ 1, 7196.
8 This proposition actually holds for arbitrary repertoires in the same way as Proposition 11.7, if
there is a description d ∈ D(α) which satisfies the additional condition in Definition 3.3.11.3 Finding Descriptions with Minimal Information 155
The following is a slightly more complicated example:
Take
A1 = [X1, X2 ∈ {2, 4, 6}], A2 = {(1, 1), (6, 6)}, A3 = [X1 + X2 ∈ {3, 5, 7, 9, 11}],
A4 = [X1 + X2 = 4], A5 = [X1 = X2], A6 = [X1 = 5]∪[X2 = 5], and
A7 = [X1 + X2 = 5].

In many cases, one obtains the minimum information description in α by the
following recipe:
1. Consider the largest element in α0 := α , i.e.,
A1 := arg max{p(A): A ∈ α0}.
2. Define α1 := {A \ A1 : A ∈ α0} and repeat, i.e.,
A2 := arg max{p(A): A ∈ α1} and α2 := {A \ A2 : A ∈ α1}.
The following example shows that this procedure does not always work.
Example 11.9 Let (, 	, p) = E34 and α = {A1,...,A6} with
A1 = {1,..., 18},
A2 = {19,..., 34},
A3 = {1,..., 8, 31, 32, 33, 34},
A4 = {9,..., 16, 27, 28, 29, 30},
A5 = {17, 23, 24, 25, 26},
A6 = {18, 19, 20, 21, 22}.
Beginning with A1, we obtain a description d1 with d1(ω) = A1 for ω ∈ A1 and
so on. This is the description we obtain with the “rule of thumb.” We get I1 =
I (d1) = log 34 − 18
34 log 18 − 16
34 log 4 = 1.93868. However, beginning with A2,
we obtain a description d2 with d2(ω) = A2 for ω ∈ A2 and so on, leading to
I2 = I (d2) = log 34 − 18
34 log 18 − 16
34 log 8 = 1.46809. 156 11 Novelty, Information, and Surprise of Repertoires
11.4 Technical Comments
This is the central chapter of this book. Here we introduce and define the new
concepts of novelty and surprise for covers or repertoires, thereby extending the
classical definition of information for partitions. Actually, Definition 11.1 defines
five new terms: novelty and two versions of information and of surprise. I think it is
a matter of taste whether one prefers to use I or +I for the information of repertoires,
and S or S
+for the surprise. The theory works equally well for both versions.
We also show how these new measures can be calculated and a few of their
elementary properties. These will be investigated extensively in Part VI. For finite
covers, most results of this chapter have already been worked out in Palm (1975,
1976a, 1976b, 1981).
11.5 Exercises
(1) Calculate I, +I, N , S
+, and S for the repertoires α in Examples 10.3,
10.7, 10.9, 11.7, 11.8, and 11.9.
(2) For the repertoires α in Examples 10.7, 10.9, 11.7, 11.8, and 11.9, which are
not tight, maximize N (d) − I(d) and E(N ◦ d/N ◦ d) on D(α).
(3) What is the surprise of a chain α = {A1,...,An}, and what is the information?
What is the maximal surprise of a chain of n elements, and what is the maximal
information?
(4) What is the surprise of a continuous chain, i.e., a repertoire X≥ for a
continuous random variable X? Compare this result with Exercise (3).
(5) Given two independent continuous random variables X and Y , what is the
surprise of X≥ ∩ Y ≥ and of X≥ ∪ Y ≥? What is the surprise of X≥ ∩ X≤?
(6) Prove Proposition 11.6 on page 148.
(7) Work out the introductory example on page 139!
(8) Like in Exercise (6.7), 1 e coins are placed beneath 2 out of 8 shells. They are
lying under shells 1 and 2 with a probability of 0.5; otherwise, they are lying
under any pair of shells with equal probability. If they are lying beneath shells
3 and 4, then there is also a 2 e coin under shell 5.
The task is to choose a shell with the highest possible amount of money lying
underneath it. Let  = {{i, j }: i, j = 1, 2,..., 8;i = j }. Define a suitable
repertoire α which holds the interesting propositions for this problem.
Let the shell game be repeated several times. How many bits of information
does one informer (who knows the positions of the coins) need on average
to tell the guesser which shell to choose (and what amount of money lies
beneath it)?
Hint: Find an appropriate description d with propositions from α with minimal
information I(d) = I(α).References 157
(9) Give some more examples of repertoires yielding different values for I vs. +I
vs. N vs. S.
(10) Give an example for two covers α and β where
(a) I(α ∪ β) < I(α), I(β),
(b) I(α ∪ β) > I(α), I(β).
(11) Give an example for two covers α and β where
(a) α ≤ β, but I(α) > I(β) and +I(α) > +I(β).
(12) For || = n and p(ω) = 1
n∀ω ∈ ,
(a) which are the flat repertoires with minimal N > 0?
(b) which are the flat repertoires with minimal I > 0?
(13) Let  = {1,...,n} and p = (p1,...,pn) with p1 ≤ p2 ≤ ... ≤ pn.
(a) Which repertoire α achieves the maximal value for S
+and S?
(b) Which repertoire α achieves the smallest value of I, N , or +I, respectively,
that is not zero?
(14) For the probability space E5, find the largest clean cover α. What is its
cardinality? Compute I(α), N (α), and S(α).
(15) Show that for countable  the maximal value of I, N , and +I is achieved for
the partition λ.
(16) Show that α ≤ β implies α∩ ≤ β∩.
References
Palm, G. (1975). Entropie und Generatoren in dynamischen Verbänden. PhD Thesis, Tübingen.
Palm, G. (1976a). A common generalization of topological and measure-theoretic entropy.
Astérisque, 40, 159–165.
Palm, G. (1976b). Entropie und Erzeuer in dynamischen Verbänden. Zeitschrift für Wahrschein￾lichkeitstheorie und verwandte Gebiete, 36, 27–45.
Palm, G. (1981). Evidence, information and surprise. Biological Cybernetics, 42(1), 57–68.Chapter 12
Conditioning, Mutual Information,
and Information Gain
In this chapter, we discuss the extension of three concepts of classical informa￾tion theory, namely, conditional information, transinformation (also called mutual
information), and information gain (also called the Kullback–Leibler distance) from
descriptions to (reasonably large classes of) covers. This extension will also extend
these concepts from discrete to continuous random variables.
12.1 Introductory Examples
In this chapter, we will define a new measure for the deviation between two
probability distributions: the surprise loss, which is an interesting complement to
the information gain or Kullback–Leibler distance (Kullback & Leibler, 1951). The
following example introduces the idea of surprise loss:
In the classical Popperian view of science development, a proponent of a new
theory will try to devise an experiment that turns up a result which is predicted by
the new theory and contradicts the old theory. In other words, he can predict the truth
of a proposition A which is believed to be false by the old theory. In many areas of
modern science, in particular, in biology, psychology, or neuroscience, the situation
usually is not quite as simple: often one can only hope to find a proposition A that
has a very high probability based on the new theory and a very low probability based
on the old theory.
In this situation, scientific prediction assumes the nature of a bet: the proponent
of the new theory bets on A, hoping it will be the outcome of the experiment. If A
happens indeed, he can claim that he has predicted something to happen that has
a very low probability for the old theory and thus makes it seem unlikely that the
old theory is true. Of course, the success of a scientific theory should not depend
on a single bet. In many cases, where the prediction of a theory has some stochastic
component (as is often the case in the life sciences) and the probability of A is
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_12
159160 12 Conditioning, Mutual Information, and Information Gain
somewhat larger than 1
2 for the new theory and somewhat smaller than 1
2 for the old
theory, one can construct a new proposition A (e.g., by independent repetition of
experiments), which has a probability close to 0 for the old theory and close to 1
for the new theory. Another common version of this procedure is a statistical test: in
this case, the proponent of the new theory bets not just on one proposition A, but on
a chain of propositions A1 ⊂ A2 ⊂ A3 ⊂ ... ⊂ An, each of which is much more
likely for the new theory than for the old one. If ω ∈ Ak \ Ak−1 happens, he reports
pold(Ak) as the “significance” of his result.1
Now we want to assume that both the “old” probability q and the “new”
probability p are known and want to construct a set A for which the novelty Npq (α)
is maximal (with α = {A, }). Then we consider the same problem for chains
α = {A1,...,An, }. This is done in Sect. 12.4.
12.2 Conditional Information and Novelty
Our goal is to define I(α|β) and N (α|β) for suitably large classes of repertoires
or covers in such a way that it retains the most important properties. Already in
Chap. 11, we have seen that this enterprise will be rather hopeless for the surprise
S, because S has quite different properties from I.
We begin by listing criteria that a good definition should meet (formulated for I,
but the same is meant for N ) in order to come close to the theory of information for
random variables (see Proposition 4.9).
Criteria We would like to have the following:
1. Positivity: I(α|β) ≥ 0,
2. Identity: I(α|α) = 0,
3. Two-way monotonicity: α ≤ β ⇒ I(α|γ ) ≤ I(β|γ ) and I(γ |α) ≥ I(γ |β),
4. Additive symmetry: I(α) + I(β|α) = I(β) + I(α|β).
Note that requirement 4 is necessary for additivity2:
I(α) + I(β|α) = I(α ∨ β) = I(β) + I(α|β).
Also, with a proper definition of α ∨ β which yields additivity, monotonicity in
the second argument is equivalent to subadditivity because
I(α ∨ β) = I(α) + I(β|α) ≤ I(α) + I(β|) = I(α) + I(β).
1 See, e.g., Chow (1996).
2 cf. Sect. 4.3, Proposition 11.6 and Chap. 18. α ∨ β should be the smallest repertoire that is larger
than α and β. This of course depends on the ordering ≤ of repertoires or of subclasses of repertoires
(see Chaps. 16 and 17). For repertoires α ∨ β, the ordering defined in Definition 11.3 is simply
α ∪ β, whereas for templates, it turns out to be α · β (cf. Chap. 17).12.2 Conditional Information and Novelty 161
All these requirements are obviously fulfilled for information (= novelty) on
the set P of partitions, i.e., in classical information theory. Given the definitions
of I(α) and N (α), the most obvious idea is to reduce I(α|β) to I(a|b) and
N (α|β) to N (a|b) by appropriate minima and/or maxima over a ∈ D(α) and
b ∈ D(β). The requirement of two-way monotonicity suggests a min − max or
max − min construction. In order to achieve identity, we have to use max − min
(see Example 12.1). This leads to the following definition.
Definition 12.1 For any two repertoires α, β, we define
I(α|β) := max
b∈D(β)
min
a∈D(α)
I(a|b) and N (α|β) := max
a∈D(α)
min
b∈D(β)
N (a|b).
When we consider the sets R and F of repertoires and flat covers, respectively,
we observe that identity is easily fulfilled, and monotonicity is half-fulfilled. It holds
in the first argument, but not in the second, if we use the proper ordering, namely,
 (this will be defined and analyzed in Chap. 17) for I and ≤ for N . Additive
symmetry is not fulfilled. This is also shown in the next example.
Example 12.1 In general, we do not get I (α|α) = 0 with the min–max construc￾tion: consider (, 	, p) = D2, X1 = first coordinate, X2 = second coordinate, and
α = X1 ∪ X2. For any (i, j ) ∈ , there are exactly two choices from α, namely,
[X1 = i] and [X2 = j ].
For any b ∈ d(α) and any ω ∈ , we take ab(ω) = b(ω). Thus, ab(ω) ∩b(ω) ⊆
ab(ω) ∩ b(ω) = {ω}. Then
I (ab|b) = N (ab|b) = N (ab∩b)−N (b) = log 36−N(b) ≥ log 36−log 12 = log 3
and
N (ab|b) = N (ab ∩ b) − N (b) = log 36 − log 6 = log 6.
Thus
min
b∈D(α)
max
a∈D(α)
I(a|b) ≥ min
b∈D(α)
I(ab|b) ≥ log 3 > 0
and similarly for N . Thus, the min–max definition of I or N would not satisfy
identity.
To construct a counterexample against additive symmetry, we have to remember
the following:
I (β) + I (α|β) = min
b I (b) + max
b
min
a I (a|b),
N(α) + N(β|α) = maxa N(a) + max
b
min
a N(b|a).162 12 Conditioning, Mutual Information, and Information Gain
Now let α = X1 and β = X1 ∪ X2. Then from Definition 12.1, we find
I(β) + I(α|β) = log 6 + log 6, N (β) + N (α|β) = log 6,
I(α) + I(β|α) = log 6 + 0, N (α) + N (β|α) = log 6 + log 6.
So we cannot hope to obtain additivity in general. 
Still the two definitions of conditional information and novelty may be quite
interesting when they are interpreted in a game-theoretical way:
Player A chooses a proper choice from α and player B a proper choice
from β, A with the goal to minimize information and to maximize novelty and
B the opposite goal. In this book, I do not want to explore this game-theoretical
interpretation further.
When we are considering the set T of templates, there is only one proper choice
from α and from β. So we can forget about the minima and maxima and expect a
reasonable theory (in particular, looking back at Sect. 4.3).
Proposition 12.1 On T we have
(i) I(α|α) = 0 and N (α|α) = 0,
(ii) α ≤ β implies I(α|γ ) ≤ I(β|γ ), I(γ |α) ≥ I(γ |β), N (α|γ ) ≤ N (β|γ ) and
(iii) I(α) + I(β|α) = I(α · β) and N (α) + N (β|α) = N (α · β).
for any tight covers α, β, and γ . In particular, the same holds for partitions α, β,
and γ .
Proof Let a be the only proper choice from α, and b the same for β, and c for γ .
(i) Obviously, I(a|a) = N (a|a) = 0.
(ii) N (a) + N (b|a) = N (a ∩ b) by Proposition 4.4 and the same holds for a and b and therefore for I (Proposition 4.6).
(iii) α ≤ β implies a ⊇ b and therefore p

a(ω)|c(ω)
≥ p

b(ω)|c(ω)
for every
ω ∈ , so N (a|c) ≤ N (b|c). Similarly, the assertions on I have been shown
for tight descriptions (Propositions 4.5 and 3.6).

With this proposition, we have extended all essential properties of classical
information to the information I on T, and almost all properties (except mono￾tonicity in the second argument) to the novelty N on T. Indeed, we cannot get this
monotonicity for N , and equivalently we cannot get subadditivity, as is shown in
the following example.
Example 12.2 N is not subadditive on T. Let (, 	, p) = E16, and
α = ,
{i, . . . , 16}: i ∈ 
-
,
β = ,
{1,...,i}: i ∈ 
-
,
α · β = ,
{i, . . . , j }: i ≤ j ∈ 
-
∼ ,
{i}: i ∈ 
-
.12.3 Mutual Novelty and Transinformation 163
So N (α) = N (β) ≤ 1
ln 2 and N (α · β) = log2 16 = 4. Thus, N (α · β) >
N (α) + N (β). 
This discussion shows that it is not at all straightforward how to define mutual
information for arbitrary covers, as long as we do not have additive symmetry,
because it may happen that the expressions I(α)+I(β)−I(α∨β), I(α)−I(α|β),
and I(β) − I(β|α) all give different results. For templates we can safely define
mutual information, because of Proposition 12.1.
12.3 Mutual Novelty and Transinformation
Here we just define the mutual information or transinformation T (α, β) and the
mutual novelty M(α, β) for templates.
Definition 12.2 For any two templates α and β, we define
T (α, β) := I(α)+I(β)−I(α·β) and M(α, β) := N (α)+N (β)−N (α·β).
Proposition 12.2 Let α and β be two templates, dα and dβ the corresponding
unique proper descriptions, dα·β = dα ∩ dβ, and
Nα = Ndα , Nβ = Ndβ , Nα·β = Ndα·β and Iα = Idα , Iβ = Idβ , Iα·β = Idα·β
the corresponding random variables (defined in Definitions 3.5 and 3.13). Then
(i) T (α, β) = E 
Iα + Iβ − Iαβ 
= E

log p ◦ d
αβ
p ◦ d
α · p ◦ d
β

≥ 0 and
(ii) M(α, β) = E 
Nα + Nβ − Nαβ
= E

log p ◦ dαβ
p ◦ dα · p ◦ dβ

.
Proof Everything is obvious from the definitions, and the positivity in (i) was
shown in Proposition 4.9.(ii). 
We observe that it is possible that M(α, β) < 0 (see Example 12.2).
Proposition 12.3 Let α, β be templates. Then
(i) T (α, β) = I(α) − I(α|β) = I(β) − I(β|α),
(ii) T (α, β) ≤ min (I(α), I(β)) ,
(iii) T (α, β) = 0, if and only if α, β are independent,
(iv) T (α, α) = I(α)
(v) M(α, β) = N (α) − N (α|β) = N (β) − N (β|α),
(vi) M(α, β) ≤ min (N (α), N (β)) ,
(vii) M(α, β) = 0, if p ◦ dαβ = (p ◦ dα) · (p ◦ dβ) almost everywhere,
(viii) M(α, α) = N (α).164 12 Conditioning, Mutual Information, and Information Gain
Proof All assertions follow directly from Proposition 12.2. 
Example 12.3 a = (X−Y )≥, b = (X+Y )≤, c = (X+Y )≥, α = R(a), β = R(b),
γ = R(c). What is M(α, β)? See Exercise (8). 
12.4 Information Gain, Novelty Gain, and Surprise Loss
In Chap. 11 we extended the concept of information and surprise to repertoires.
In what follows, we do the same for information gain (see Definition 4.1) and
surprise loss. The definition of surprise loss will be analogous to that of information
gain; we choose a different wording here because gaining information is usually
associated with loosing surprise.
As in classical information theory, information gain and the new related terms
may actually be defined in a more general setting where information and novelty
often become infinite. We will achieve this goal here by defining these terms for
arbitrary covers in Definition 12.5. However, we start by considering repertoires.
Definition 12.3 Given two probability distributions p and q on , we define the
subjective novelty of a repertoire α as
Npq (α) := max{Npq(d): d ∈ D(α)}
and the novelty gain as
Gpq (α) := max{Gpq (d): d ∈ D(α)}.
Remarks
1. The subjective novelty can be maximized locally for every ω ∈  (compare
Definition 11.1 on page 141), and we can also define the novelty gain locally as
a random variable
Gpq (α)(ω) = max{Nq (d(ω)) − Np(d(ω)): d ∈ D(α)} ≥ Nq,α(ω) − Np,α(ω)
and then
Gpq (α) = Ep(Gpq (α)(ω)) ≥ Npq (α) − Np(α).
2. It is easy to see that Gpq (α) can be negative; a simple example is α = {A, }
with p(A) < q(A). It is also possible that Gpq (α) > Npq (α) − Np(α). An
example is given in Exercise (7).
3. These concepts can also be defined and are often finite for infinite repertoires.
This is quite obvious for finitary repertoires, but it is known that in many cases12.4 Information Gain, Novelty Gain, and Surprise Loss 165
where α is not finitary and both Np and Npq are infinite, the difference Gpq can
still be defined in a reasonable way (compare Definition 12.5).
Essentially, there are two possibilities:
(a) Either there is a proposition A for which p(A) = 0 and q(A) = 0; then
we can obtain infinite novelty if we believe that p is the true probability,
but in fact it is q. Something can happen which we believe to be essentially
impossible.
(b) Or if there is no such proposition, then the theorem of Radon–Nikodym3
shows that q has a continuous density function f with respect to p and we
can use f to define the information or novelty gain.
This reasoning is traditionally based on the concept of information gain, which
is usually defined for partitions (or algebras) α. It naturally leads to the following
definitions.
For the definition of information gain, we first restrict ourselves to the set T of
templates again.
Definition 12.4 Let p and q be probabilities on . Let α be a template and dα be
the unique proper choice from α. We define the novelty gain as
Gpq (α) := Gpq (dα), and
the information gain as
IGpq (α) := Gpq (d
α).
It is obvious that this definition of novelty gain coincides with Definition 12.3.
Proposition 12.4 Let p and q be probabilities on . If α is a partition, then
Gpq (α) = IGpq (α).
Proof There is only one choice dα ∈ D(α) and dα = d
α by Proposition 3.2. 
This proposition shows that IG could as well be defined as the novelty gain of
partitions. By analogy we can now define the surprise loss SL as the novelty gain
of chains. This is also the idea explained in the introductory example.
We use the name “surprise loss” because the semantics of information and
surprise appears to be opposite: when you gain information, you loose surprise.
With this idea, it is possible to define G, SL, and IG even for arbitrary infinite
covers.
3 See, e.g., Bauer (1972).166 12 Conditioning, Mutual Information, and Information Gain
Definition 12.5 For two probabilities p and q on  and for an arbitrary cover α,
we define
SLpq (α) := sup ,
Gpq (dβ): β ⊆ α∪, β finite chain- and
IGpq (α) := sup ,
Gpq (dβ): β ⊆ σ (α), β finite partition-
.
Definition 12.5 defines information gain and surprise loss as the solutions of
two optimization problems: namely to find a partition with maximal information
gain and to find a chain with maximal surprise loss. The second problem is the one
we posed in the introductory example. Proposition 12.7 gives the solution to these
problems.
Proposition 12.5 Definition 12.5 for IGpq (α) is consistent with Definition 12.4 for
templates.
Proof Let α be a template and dα the corresponding description. Let β ⊆ σ (α)
be a finite partition and dβ the corresponding description. We consider one ω ∈ .
dβ(ω) = B \ A, where both A and B are unions of elements of α∩.
Since α is a template, for any ω ∈ A, we have dα ⊆ A, so dα(ω
) = dα(ω)
because ω /∈ A. On the other hand, dα(ω) ⊆ B and thus d
α(ω) ⊆ B \ A = dβ(ω).
Thus IG(dα) ≥ IG(dβ) by Proposition 4.2. 
Proposition 12.6 For any cover α, we have
SLpp(α) = 0 = IGpp(α).
Proof Obvious. 
Proposition 12.7 Let p and q be probabilities on . If α is a σ-algebra and f is the
density4 of p with respect to q on α, then we define F (ω) = p(f ≥(ω))/q(f ≥(ω)).
(i) IGpq (α) = Ep(log2 f ).
(ii) SLpq (α) = Ep(log2 F ).
(iii) SLpq (α) ≥ IGpq (α) ≥ 0.
Proof For a finite repertoire α, the finite algebra generated by α is already a
σ-algebra and it is equivalent to a partition β. For this partition, we may consider
the only choice dβ. Then Definition 12.4 becomes
IGpq (α) = Ep(sq ◦ dβ − sp ◦ dβ) = Ep

log2
p(dβ(x))
q(dβ(x))
.
4 By the Radon–Nikodym theorem (e.g., Bauer, 1972).12.4 Information Gain, Novelty Gain, and Surprise Loss 167
On the other hand, the density of p with respect to q on a finite σ-algebra like
σ (α) = σ (β) is defined by
f (x) = p(dβ (x))
q(dβ(x)).
This shows (i) in the finite case. For the infinite case, we need an approximation
argument.
In (i) and (ii), we have to show that the supremum in Definition 12.5 is actually
attained at the given formula. In (iii) because of Proposition 12.8, we only have to
show the first inequality. We first observe that for any proposition B ∈ α, we can
interpret p(B)
q(B) = Eq (1B · f )
q(B)
=: f¯B as the average of f on B with respect to the
probability q. More formally, we can define the conditional probability qB (as in
Chap. 4 and observe that p(B)
q(B) = EqB (f ).
(i) By definition or construction of the expectation E, it is sufficient to take any
finite partition α = {A1,...,An} and show that Gpq (α) ≤ Ep(log2 f ) since
Ep(log2 f ) can be approximated by sufficiently large partitions. Now
Gpq (α) = n
i=1
p(Ai)log2
p(Ai)
q(Ai)
.
We use the trick of Proposition 12.8 againand take the natural logarithms to
show
n
i=1
p(Ai)ln p(Ai)
q(Ai) ≤ Ep(ln f ).
Indeed
n
i=1
p(Ai)ln p(Ai)
q(Ai) = n
i=1
q(Ai)EAi(f )ln EAi(f )
and
Ep(ln f ) = Eq (f · ln f ) = n
i=1
q(Ai)EAi(f · ln f ).168 12 Conditioning, Mutual Information, and Information Gain
Now for each i, we have
EAi(f )ln EAi(f ) − EAi(f · ln f ) = EAi

f · ln
EAi(f )
f

≤ EAi

f ·
EAi(f )
f − 1
 = 0.
(ii) Let α = {A1,...,An} be an arbitrary partition and β = {B1,...,Bn} with
Bi := 
i
j=1
Aj the corresponding chain. First, I will show that in order to
maximize SLpq (β), the best ordering of the propositions Ai in α is the one
where f has larger values on Ai than on Ai+1, so that α and β are actually
ordered in the direction of f ≥. Then I will show that refining α will increase
the surprise loss. This shows that the expectation in Definition 4.1 of Gpq (f ≥)
is actually obtained as the surpremum over all partitions.
By definition
SLpq (β) = n
i=1
p(Ai)log2
p(Bi)
q(Bi) = n
i=1
p(Ai)log2 EBi(f ).
In order to maximize this, the sets Ai should be chosen and ordered in such
a way that EBi(f ) becomes as large as possible. Since Bi+1 includes Bi and
Bn =  (implying EBn (f ) = 1), the best choice is to have the largest values
of f on A1 and generally f should be larger on Ai than on Ai+1. This implies
EAi(f ) ≥ EAi+1 (f ) and also EBi(f ) ≥ EBi+1 (f ). Incidentally, it also implies
that EBi(f ) ≥ EAi(f ) since Bi contains on average larger values of f than in
Ai. This already shows (iii).
Furthermore, if we can split a set Ai into two sets A
i and A
i with EA
i
(f ) ≥
EA
i
(f ), the surprise gain will increase, because it can only become larger on
A
i and remains the same on A
i .
(iii) We show that for every x ∈  in fact
F (x) = p(f ≥(x))/q(f ≥(x)) = Eq(f |f ≥ f (x)) ≥ f (x).
Indeed,
Eq (f |f ≥ f (x)) = Eq (f · 1[f ≥f (x)])/q[f ≥ f (x)]
= Ep(1[f ≥f (x)])/q[f ≥ f (x)]
= p(f ≥(x))/q(f ≥(x)).
12.4 Information Gain, Novelty Gain, and Surprise Loss 169
Proposition 12.8 For any cover α, we have IGpq (α) ≥ 0.
Proof The proof is the continuous version of the proof of Proposition 4.1. Again, we
use the natural logarithm instead of log2 in this proof and the inequality ln x ≤ x−1:
−IGpq (α) = Ep

log2
1
f

and
Ep

ln
1
f

= Eq

f · ln
1
f

≤ Eq

f
 1
f − 1

= Eq(1) − Eq(f ) = 0.

Example 12.4 Take  = [0, 1] with the usual Lebesgue measure q. Let p be given
by a density f with respect to q. Define
α := {[x,x + δ]: 0 ≤ x ≤ 1 − δ} ∪ ,0, δ
2

,

1 − δ
2 , 1
-
for some δ > 0 and
β := {[x,x + δ]: 0 ≤ x ≤ 1 − δ, 0 <δ< 1}.
What is Gpq (α) and Gpq (β)? See Exercise (9). 
Definition 12.6 For any two probability distributions p and q on (, 	), we define
(i) IG(p, q) := IGpq (	) and
(ii) SL(p, q) := SLpq (	)
These definitions use the concepts of information gain and surprise loss to define
measures for the distance between two probability distributions. Information gain
IG is the same as the Kullback–Leibler distance, whereas SL is a new distance
measure.
Proposition 12.9 Let (, 	) be a measurable space and p, q be two probabilities
on (, 	), and let p have a density f with respect to q. Then
(i) IG(p, q) = IGpq (	) = Ep(log2 f ),
(ii) SL(p, q) = Gpq (f ≥) = Ep(log2 p ◦ f ≥ − log2q ◦ f ≥) and
(iii) SL(p, q) ≥ IG(p, q) ≥ 0.170 12 Conditioning, Mutual Information, and Information Gain
Proof This follows directly from Proposition 12.7. Observe that
log2 p ◦ f ≥ − log2 q ◦ f ≥ = log2 F
for the function F defined in Proposition 12.7. 
Example 12.5 Consider  = [0, 1] with the Borel-sets 	, the equidistribution q
on [0, 1], and p = 3x2q, i.e., p((a, b)) = 
b
a
3x2 dx = b3 − a3. We compute
IG(p, q) and SL(p, q) from Proposition 12.9 with f (x) = 3x2:
IG(p, q) =

1
0
3x2 log2(3x2
)dx
SL(p, q) =

1
0
3x2
log2(p[f ≥ f (x)]) − log2(q[f ≥ f (x)])

dx, so
ln 2 · IG =

1
0
3x2 · 2 ln x dx +

1
0
3x2 ln 3 dx
= −6 · 1
32 + ln 3 = ln 3 − 2
3 ≈ 0.432
and IG ≈ 0.623;
ln 2 · SL =

1
0
3x2
ln(1 − x3
) − ln(1 − x)
dx
y=1−x3
=

1
0
ln y dy −

1
0
3(1 − y)2 ln y dy
= −1 + 3 − 6 ·
1
4 + 3 ·
1
9 = 1
2 +
1
3 = 0.833
and SL ≈ 1.202.12.4 Information Gain, Novelty Gain, and Surprise Loss 171
For comparison we can also compute IG(q, p) and SL(q, p):
ln 2 · IG = − 
1
0
ln(3x2)dx = 2 − ln 3 ≈ 0.9014
ln 2 · SL =

1
0

ln(q[f ≥ x]) − ln(p[f ≥ x])

dx
=

1
0

ln x − ln x3
dx = −2

1
0
ln x dx = 2
It is plausible that these two values are larger than the other two computed before,
because the density of q with respect to p has much larger values than the density
f (x) = 3x2 of p with respect to q.
Similar effects can also be seen by maximizing the surprise loss SLpq (α) for a
simple bet α = {A, } (see also Exercise (4). 
Example 12.6 In this example, we consider the classical problem where both
probabilities p and q are essentially Gaussian distributions with different means.
More exactly, we consider the classical statistical situation where independent
random samples have been drawn from a distribution which is either p or q. We
assume that the probability p, also called H0, is N(0,σ) and the probability q is
N(1,σ).
We consider  = Rn and on  the random variables Xi : (x1,...,xn) → xi. We
define the two probabilities p and q on  by requiring that the Xi are independent
and N(0,σ)-distributed for q, and N(1,σ)-distributed for p. Then we calculate
IG(p, q) and SL(p, q).
First, we determine the density f from the Gaussian densities of p and q:
f (x) = exp(

i xi/σ2 − n/(2σ2)).
This already confirms the classical result that the sum or equivalently the average
of the Xi is the best statistics to show a difference between p and q, i.e., a difference
in the means. With this we get
ln(2) · IG(p, q) =
= (σ.
(2π ))−n · σ −2

(

i
xi − n/2) exp(−

i
(xi − 1)
2/(2σ2))dλ
= σ −2(

i
(
.
(2π )σ )−1

xi · exp(−(xi − 1)
2
/(2σ2
))dxi − n/2) = n/(2σ2).172 12 Conditioning, Mutual Information, and Information Gain
And
ln(2) · SL(p, q) = (
.
(2π )σ )−n

(ln(p[f ≥ f (x)])
− ln(q[f ≥ f (x)])) exp(−

i
(xi − 1)
2
/2σ2)dλ.
Now f (x) = exp(S/σ2 − n/(2σ2)) depends monotonically on the random
variable S := 
i Xi which is again Gaussian.
Therefore, p[f ≥ f (x)] = p[S ≥ S(x)] and
ln(2) · SL(p, q) = Ep(ln(p[S ≥ S(x)] − ln(q[S ≥ S(x)])
= −1 − Ep(ln(q[S ≥ S(x)]) = −1 − Eq (ln(q[S ≥ S(x) + n]).
Here q is N(0, σ√n)-distributed and this expression can be numerically evalu￾ated.
We observe that both expressions, IG(p, q) and SL(p, q), depend monoton￾ically on the so-called signal-to-noise ratio SNR between the two probability
distributions for the random variable S, which is defined as the difference in
expectation divided by the variance. In this case, it is SNR = √n/σ. 
The transinformation T (X, Y ) between two random variables X and Y , which
was introduced in Chap. 5, can obviously also be regarded as the information gain
between the common distribution of X and Y (i.e., the distribution of (X, Y )) and
the product of the distributions of X and of Y . Thus, Proposition 4.9.(ii), which
shows the positivity of the transinformation, can be regarded as a special case of
Proposition 4.1 or 12.8. We can use this observation to extend the definition of
transinformation from discrete to continuous random variables. This will be carried
out in the next section.
12.5 Conditional Information of Continuous
Random Variables
In Chap. 4 we have presented classical information theory for discrete random
variables. Sometimes it may be useful to extend some of these concepts to
continuous random variables (see, e.g., Cover & Thomas, 1991; Kolmogorov, 1956;
Shannon, 1948), in spite of the fact that by any reasonable definition information
will be infinite for such variables because events can occur with arbitrarily small
probabilities.
The idea is to use only conditional information as defined in Definition 12.1
and to describe a continuous random variable X by the cover (σ-algebra) σ (X) =
σ

{[X ≤ a]: a ∈ R}

instead of simply using the description X as in Chap. 4. With
this idea, we can reproduce all basic theorems, i.e., Proposition 4.9.
Unfortunately, we cannot directly define I(X) by I(σ (X)), because usually
σ (X) is not a repertoire. So we have to use the method of definition that we12.5 Conditional Information of Continuous Random Variables 173
used for information gain (Definition 12.5) and work with finite partitions or finite
subalgebras of σ (X).
Definition 12.7 Let X, Y be arbitrary random variables. We define
I(X) := sup{I(α) : α partition, α ⊆ σ (X)},
I(X|Y ) := supα⊆σ (X) infβ⊆σ (Y ) I(α|β),
where both sup and inf are extended over all partitions.
Clearly, for discrete random variables X and Y , these definitions coincide with
Definition 4.4.
There is a connection to another common definition of conditional information
that is worth mentioning.
For A ∈ 	, p(A) = 0 and a random variable X, we can define the random
variable5 p(A|X). Based on this, for a partition α, we define the random variable
I (α|X) := −
A∈α p(A|X)log2 p(A|X) and
I(α|X) := E(I (α|X)).
Proposition 12.10 For two random variables X and Y , we have I(Y |X) =
sup{I(α|X) : α finite partition ⊆ σ (Y )}.
Now we can show a useful continuous analog to Proposition 4.9.
Proposition 12.11 Let U, X, Y , and Z be arbitrary random variables.
(i) I((X, Y )|U ) ≤ I(X|U ) + I(Y |U ),
(ii) X  Y implies I(X|U ) ≤ I(Y |U ) and I(U|X) ≥ I(U|Y ),
(iii) I((X, Y )|U ) = I(X|U ) + I(Y |(X, U )).
Proof These statements follow directly from their discrete analogs in Proposi￾tion 4.9. 
We can use the idea of Definition 12.5 also for an extension of our previous
Definition 12.2 of transinformation.
Definition 12.8 Let α and β be arbitrary covers. We define the transinformation
T (α, β) = sup ,
T (α
, β
): α ⊆ σ (α) and β ⊆ σ (β) partitions-
.
As in Proposition 12.5, we can again show that this definition is consistent with
Definition 12.2 for templates.
5 p(A|X) is a random variable that depends on the value of X, i.e., p(A|X) = f (X), where
f (x) = p(A|[X = x]) which can be properly defined for almost every x ∈ R(X).174 12 Conditioning, Mutual Information, and Information Gain
Definition 12.9 For two arbitrary random variables X, Y , we define the transinfor￾mation
T (X, Y ) := T

σ (X), σ (Y )
.
Proposition 12.12 Let X, Y be two random variables on (, 	, p). Consider the
probability distributions pX · pY and p(X,Y ) on R2. Then
T (X, Y ) := IG(p(X,Y ), pX · pY ) ≥ 0.
Proof
(i) For discrete random variables X and Y , this can be calculated directly using
Proposition 12.7: the (discrete) density f of p(X,Y ) = p with respect to q =
pX · pY at (x, y) ∈ R2 is
f (x, y) = p[X = x,Y = y]
p[X = x] · p[Y = y]
and
T (X, Y ) = I (X) + I (Y ) − I (X, Y ) = E

log2
p(X ∩ Y ) 
p(X)  · p(Y ) 

= E(log2 f ).
(ii) In the general case, we simply have to observe that our definitions fit together:
T (X, Y ) is defined in Definitions 12.9 and 12.8, and the information gain is
defined in Definitions 12.6 and 12.5. Both definitions reduce the calculation
to finite partitions of R2, where we have shown the equality in (i). In order
to see that the two resulting suprema are the same, we need an approximation
argument and Proposition 4.2.

12.6 Technical Comments
This chapter has tried to find plausible ways of extending the classical concepts of
information gain and Kullback–Leibler (KL) distance to arbitrary covers, leading to
Definitions 12.1, 12.3, 12.6, 12.7, 12.8, and 12.9. In classical information theory,
a similar reasoning is used to extend these concepts from discrete to continuous
random variables.
From the point of view of applications, these concepts have been used to measure
the distance between two probabilities p and q and to approximate an unknown
probability p by parameterized known probabilities q in optimization or learning12.7 Applications in Pattern Recognition, Machine Learning, and Life Science 175
algorithms in the fields of pattern recognition or artificial neural networks (see
Atick, 1992; Brown, 2009; Coulter et al., 2009; Dayan & Abbott, 2001; Deco &
Obradovic, 1996; Erdogmus et al., 2003; Hinton & Ghahramani, 1997; Kamimura,
2002; Linsker, 1989b; Ozertem et al., 2006; Pearlmutter & Hinton, 1987).
In general, expressions of information gain or conditional information are used
quite often for optimization in pattern recognition (Amari, 1967; Amari & Nagaoka,
2000; Amari et al., 1996; Battiti, 1994; Hyvärinen, 2002; Mongillo & Denève, 2008;
Principe et al., 2000; Torkkola & Campbell, 2000).
Actually, one can distinguish three different but related lines of argumentation
that converge on the use of information theory for a better understanding of learning
algorithms:
1. The statistical or Bayesian approach (e.g., MacKay, 2005): Here the idea is
essentially that what is learned is a common distribution p(x, y) of the two
variables (or sets of variables) X and Y , often called data and labels, whose
relationship has to be learned. Here it is common to use the KL distance to
measure the distance between the currently learned distribution and the true
distribution p.
2. The statistical physics approach is actually very similar, but arises from the
tradition in statistical physics to use the “principle of maximal ignorance”
(Jaynes, 1957, 1982), which then leads to approaches that maximize entropy
(i.e., information) or transinformation.
3. Approaches that try to understand and mimic biological learning processes. Here
the idea often is that biological learning has the goal to optimize the “neural
representation” of the “learning situation,” i.e., of the values of the variables
X and Y , now interpreted as “stimulus” or stimulus situation and “response” of
the animal. Very often this leads again to maximization of the transinformation
between the neural representation Z and the variable X or Y or both (e.g., Atick,
1992; Barlow, 1989; Linsker, 1989b, 1992, 1997; Zemel & Hinton, 1995).
For these purposes, we do not need to consider repertoires. However, this chapter
also introduces a new novelty-based measure that could be used for learning or
optimization: the surprise loss which could be used in a similar way as information
gain. For these applications, the most important results are Propositions 12.7
and 12.9. In this book, we do not try to elaborate these possibilities further.
12.7 Applications in Pattern Recognition, Machine Learning,
and Life Science
In most practical applications, you are faced with the problem of learning a kind
of relation or structure from data. More specifically, the data are points xi in a
high-dimensional space Rd , and they may be associated with discrete labels or with
output values yi ∈ Rl
. The problem then can be formulated as finding a probabilistic176 12 Conditioning, Mutual Information, and Information Gain
model, i.e., a common distribution for the pairs (X, Y ) ∈ Rd+l that fits the given
data best. A partial question may be how much information the data X provide
about the labels Y , i.e., the transinformation or the information gain between the
joint distribution of X and Y and the product of their individual distributions.
The “learning process,” i.e., the process of approximating the joint distribution
by a sequence of distributions that are calculated from the data, can often be
well described again in terms of the information gain between these distributions
(Amari, 1982, 1985; Amari & Nagaoka, 2000; MacKay, 2005). Practically, all
recent applications of information theory to such data-driven learning problems
in the fields of pattern recognition, machine learning, or data mining are of this
type. They are based on the use of information gain or Kullback–Leibler distance to
measure the distance between probabilities.
In the life sciences, there are two particular fields of application where arguments
based on information terminology have a particular additional appeal: the neuro￾sciences (which are the subject of the next section) and molecular and cell biology
where the obvious link is the information contained in the DNA (Grosse et al., 2000;
Herzel et al., 1994; Mac Dónaill, 2009; Schmitt & Herzel, 1997; Slonim et al., 2005;
Taylor et al., 2007; Tkacik & Bialek, ˇ 2007; Weiss et al., 2000).
12.8 Exercises
(1) Given twoprobabilities p and q on a finite , find a function X on  for which
Ep(Nq ◦ X≥) is maximal! Do the same for Gpq (X≥). What is the difference?
(2) Compare the surprise values obtained in Exercise (1) with SL(q, p), SL(p, q),
IG(q, p), and IG(p, q)! Are all relations between these numbers possible?
(3) Give examples for two continuous probabilities p = q on [0, 1] such that
(a) IG(p, q) = IG(q, p),
(b) SL(p, q) = SL(q, p).
(4) Is SL always larger than IG ?
(5) For  = {0, 1}, p = (0.7, 0.3), and q =

1
3 , 2
3

, determine the chain with
maximal subjective surprise. Compare this to Exercise (1)
(6) Given n independent random variables X1,...,Xn, what is the novelty and
what the surprise of X≥
1 ∩ X≥
2 ∩ ... ∩ X≥
n ?
The solution is of interest for the evaluation of a number of statistical tests
for the same hypothesis undertaken in independent settings (e.g., by different
research groups). For this reason, we will give the result here: the novelty for a
particular event x is of course the sum of the novelties N [Xi ≥ Xi(x)], and if
this sum is s, then the surprise is
2−s n−1
k=0
(s · ln 2)k
k! .References 177
(7) Consider α = {A,B, } with p(A) = q(A) = 1
4 , p(A ∩ B) = q(A ∩ B) = 1
5 ,
p(B) = 2
3 , q(B) = 1
3 . Compute SLpq (α), IGpq (α), Gpq (α), and Npq (α).
(8) Compute T (α, β) and M(α, β) for Example 12.3.
(9) Compute IGpq (α), Gpq (α), IGpq (β), and Gpq (β) for Example 12.4.
References
Amari, S. (1967). A theory of adaptive pattern classifiers. IEEE Transactions on Electronic
Computers, 16(3), 299–307.
Amari, S. (1982). Differential geometry of curved exponential families—curvature and information
loss. Annals of Statistics, 10, 357–385.
Amari, S. (1985). Differential-geometrical methods in statistics. Springer.
Amari, S., & Nagaoka, H. (2000). Methods of information geometry. AMS and Oxford University
Press.
Amari, S., Cichocki, A., & Yang, H. H. (1996). A new learning algorithm for blind signal
separation. In D. S. Touretzky, M. C. Mozer, & M. E. Hasselmo (Eds.), Advances in neural
information processing systems (Vol. 8, pp. 757–763). MIT Press.
Atick, J. J. (1992). Could information theory provide an ecological theory of sensory processing?
Network: Computation in Neural Systems, 3, 213–251.
Barlow, H. B. (1989). Unsupervised learning. Neural Computation, 1, 295–311.
Battiti, R. (1994). Using mutual information for selecting features in supervised neural net learning.
Neural Networks, 5, 537–550.
Bauer, H. (1972). Probability theory and elements of measure theory. Holt, Rinehart and Winston.
Brown, G. (2009). A new perspective for information theoretic feature selection. In Proceedings
of the 12th International Conference on Artificial Intelligence and Statistics (AI-STATS 2009).
Chow, S. L. (1996). Statistical significance: Rationale, validity and utility. Sage Publications.
Coulter, W. K., Hillar, C. J., & Sommer, F. T. (2009). Adaptive compressed sensing—a new class
of self-organizing coding models for neuroscience. arXiv:0906.1202v1.
Cover, T. M., & Thomas, J. A. (1991). Elements of information theory. Wiley.
Dayan, P., & Abbott, L. F. (2001). Theoretical neuroscience: Computational and mathematical
modeling of neural systems. MIT Press.
Deco, G., & Obradovic, D. (1996). An Information-theoretic approach to neural computing.
Springer.
Erdogmus, D., Principe, J. C., & II, K. E. H. (2003). On-line entropy manipulation: stochastic
information gradient. IEEE Signal Processing Letters, 10(8), 242–245.
Grosse, I., Herzel, H., Buldyrev, S., & Stanley, H. (2000). Species independence of mutual
information in coding and noncoding DNA. Physical Review E, 61(5), 5624–5629.
Herzel, H., Ebeling, W., & Schmitt, A. (1994). Entropies of biosequences: The role of repeats.
Physical Review E, 50(6), 5061–5071.
Hinton, G., & Ghahramani, Z. (1997). Generative models for discovering sparse distributed
representations. Philosophical Transactions of the Royal Society B: Biological Sciences,
352(1358), 1177–1190.
Hyvärinen, A. (2002). An alternative approach to infomax and independent component analysis.
Neurocomputing, 44–46, 1089–1097.
Jaynes, E. T. (1957). Information theory and statistical mechanics. Physical Review, 106(4), 620–
630.
Jaynes, E. T. (1982). On the rationale of maximum entropy methods. Proceedings IEEE, 70, 939–
952.
Kamimura, R. (2002). Information theoretic neural computation. World Scientific.178 12 Conditioning, Mutual Information, and Information Gain
Kolmogorov, A. N. (1956). On the Shannon theory of information transmission in the case of
continuoussignals. IRE Transactions on Information Theory, IT-2, 102–108.
Kullback, S., & Leibler, R. A. (1951). On information and sufficiency. The Annals of Mathematical
Statistics, 22(1), 79–86.
Linsker, R. (1989b). How to generate ordered maps by maximizing the mutual information between
input and output signals. Neural Computation, 1(3), 402–411.
Linsker, R. (1992). Local synaptic learning rules suffice to maximize mutual information in a linear
network. Neural Computation, 4, 691–702.
Linsker, R. (1997). A local learning rule that enables information maximization for arbitrary input
distributions. Neural Computation, 9, 1661–1665.
MacKay, D. J. C. (2005). Information theory, inference, and learning algorithms. Cambridge
University Press.
Mac Dónaill, D. (2009). Molecular informatics: Hydrogen-bonding, error-coding, and genetic
replication. In 43rd Annual Conference on Information Sciences and Systems (CISS 2009).
MD: Baltimore.
Mongillo, G., & Denève, S. (2008). On-line learning with hidden Markov models. Neural
Computation, 20, 1706–1716.
Ozertem, U., Erdogmus, D., & Jenssen, R. (2006). Spectral feature projections that maximize
shannon mutual information with class labels. Pattern Recognition, 39(7), 1241–1252.
Pearlmutter, B. A., & Hinton, G. E. (1987). G-maximization: An unsupervised learning procedure
for discovering regularities. In J. S. Denker (Ed.), AIP Conference Proceedings 151 on Neural
Networks for Computing (pp. 333–338). American Institute of Physics.
Principe, J. C., Fischer III, J., & Xu, D. (2000). Information theoretic learning. In S. Haykin (Ed.),
Unsupervised adaptive filtering (pp. 265–319). Wiley.
Schmitt, A. O., & Herzel, H. (1997). Estimating the entropy of DNA sequences. Journal of
Theoretical Biology, 188(3), 369–377.
Shannon, C. E. (1948). A mathematical theory of communication. Bell Systems Technical Journal,
27, 379–423, 623–656.
Slonim, N., Atwal, G., Tkacik, G., & Bialek, W. (2005). Estima ˇ ting mutual information and multi￾information in large networks. arXiv:cs/0502017v1.
Taylor, S. F., Tishby, N., & Bialek, W. (2007). Information and fitness. arXiv:0712.4382v1.
Tkacik, G., & Bialek, W. (2007). Cell biology: Networks, regulation, pathways. In R. A. Mey- ˇ
ers (Ed.) Encyclopedia of complexity and systems science (pp. 719–741). Springer.
arXiv:0712.4385 [q-bio.MN]
Torkkola, K., & Campbell, W. M. (2000). Mutual information in learning feature transformations.
In ICML ’00: Proceedings of the Seventeenth International Conference on Machine Learning
(pp. 1015–1022). Morgan Kaufmann.
Weiss, O., Jiménez-Montano, M., & Herzel, H. (2000). Information content protein sequences.
Journal of Theoretical Biology, 206, 379–386.
Zemel, R. S., & Hinton, G. E. (1995). Learning population codes by minimizing description length.
Neural Computation, 7, 549–564.Part V
Information, Novelty and Surprise
in ScienceChapter 13
Information, Novelty, and Surprise
in Brain Theory
13.1 Understanding Brains in Terms of Processing
and Transmission of Information
In biological research, it is common to assume that each organ of an organism
serves a definite purpose. The purpose of the brain seems to be the coordination and
processing of information which the animal obtains through its sense organs about
the outside world and about its own internal state (Bateson, 1972). An important
aspect of this is the storage of information in memory and the use of the stored
information in connection with the present sensory stimuli. Thus, the brain deals
with information (Hebb, 1949), and therefore after Shannon’s formal definition of
information (Shannon, 1948), it seemed most appropriate to use this new theory in
brain research. Information theory has therefore become an important ingredient in
the theory and modeling of neural networks and brains (e.g., Edelman & Tononi,
2000; Palm, 1982, 1992; Shaw & Palm, 1988; Tononi et al., 1992, 1994).
Three aspects of the handling of information can be distinguished in approaches
to a functional understanding of brains:
1. Transmission of information (in particular in the cable-like nerves or fiber
bundles connecting different parts of the brain).
2. Storage and retrieval of information in memory.
3. Processing of information.
Classical (Shannon) information theory provides a quantitative measure for the
amount of information that is contained in a message or that can be maximally
transmitted through a cable or that can be stored in and retrieved from memory
(Palm, 1980). This kind of theory is directly relevant for the first two aspects of
information handling, but less relevant for the third.
The third aspect is the subject of computer science, and even in this discipline,
computation alone is not exactly in the focus of interest. In fact, one often
considers a duality between representation and computation of information. In
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_13
181182 13 Information, Novelty, and Surprise in Brain Theory
many computational problems, the difficulty of a problem depends essentially on
its representation. Thus, the issue of representation has to be studied with about
the same intensity as the problems of computation. In a way, computation can be
regarded simply as a transformation between different representations of the same
information.
Here we do not address problems of computation but focus on the issue of
representation and on the use of information theory in this context. The first
question to be discussed is a methodological or even philosophical one: how is it
possible to use rather technical concepts from information theory to discuss issues
of representation in neuroscience and brain research?
In the 1950s and 1960s, the use of information measurements in brain research
and experimental psychology became quite popular (e.g., Barnard, 1955; MacKay
& McCulloch, 1952; Quastler, 1956a, 1956b; Abeles & Lass, 1975; Attneave,
1959; Barlow, 1961; Cherry, 1966; Eckhorn et al., 1976; Gerstein & Mandelbrot,
1964; Johannesma, 1981; Massaro, 1975; Miller, 1962; Perkel & Bullock, 1967;
Pfaffelhuber, 1972; Srinivasan et al., 1982; Uttley, 1979; Wenzel, 1961; Yovits et al.,
1962); for example, the information transmission rates of a single neuron, of the
optic nerve, or of conscious reactions to stimuli were determined and discussed,
also during the following years. Similarly, the information storage capacity of short￾term, long-term, and some other memories was discussed. In my early work, I used
information theory to investigate the optimal storage capacity of neural associative
memories (Palm, 1980, 1987b; Palm & Sommer, 1992) based on Hebbian synaptic
plasticity (Hebb, 1949). This led me to the prediction that spiking activity in
associative neural populations should be sparse. An important theme in the early
discussions was the question of the “neural code” (see Perkel & Bullock, 1967),
i.e., whether the neurons use a kind of Morse code, where patterns of exact time
intervals between spikes are crucial, whether the single spikes of a neuron can be
interpreted simply as binary “yes” or “no” signals, or whether it is the vigor or
frequency of the single neuron’s spiking that signals some degree of intensity (either
for the size of some measured variable or for the certainty of a proposition).
This discussion has been revived and deepened in the last years from the 1990s
to the present (e.g., Adelman et al., 2003; Atick, 1992; Barlow et al., 1989; Bialek
et al., 1991; Borst & Theunissen, 1999; Brenner et al., 2000; Brunel & Nadal,
1998; Butts & Goldman, 2006; Butts et al., 2007; Coulter et al., 2009; Dan et al.,
1996; Dean et al., 2005; Eckhorn, 1999; Gerstner et al., 1997; Golomb et al., 1997;
Gutnisky & Dragoi, 2008; Kjaer et al., 1994; Koepsell & Sommer, 2008; Linsker,
1988; Nakahara & Amari, 2002; Optican et al., 1991; Optican & Richmond, 1987;
Panzeri & Schultz, 2001; Rieke et al., 1997; Rolls et al., 1997; Seriès et al.,
2004; Shadlen & Newsome, 1994; Softky, 1995; Tononi et al., 1994; Tsodyks &
Markram, 1997; van Essen et al., 1991; Wang et al., 2010). This is not surprising
since the available experimental evidence about neural activity, spike trains of
single and multiple neurons, and neural systems has increased dramatically over
the last 50 years. However, the discussion of the “neural code” still remained in the
context of classical experimental paradigms and traditional Shannon information
(one notable exception was Charles (Legéndy, 2009, 1975; Legéndy & Salcman,13.1 Understanding Brains in Terms of Processing and Transmission of. . . 183
1985), who used an early version of the idea of novelty introduced in this book).
One question which is still being discussed concerns spike rate vs. single spike
timing: is it just the rate of spikes in a neuron’s output spike train that contains the
information it conveys (Abbott, 1994; Dan et al., 1996; Deadwyler & Hampson,
1997; Gerstner et al., 1997; Golomb et al., 1997; Kang & Sompolinsky, 2001; Kjaer
et al., 1994; Nirenberg & Latham, 2003; Optican et al., 1991; Optican & Richmond,
1987; Panzeri & Schultz, 2001; Panzeri et al., 1999; Seriès et al., 2004; Shadlen
& Newsome, 1998; Treves & Panzeri, 1995) or is there additional information in
the precise timing of individual spikes? Since there is no clock in the brain, the
second alternative requires a temporal reference. This could be in the preceding
spikes of the neuron itself. This idea leads to the observation of suspicious interspike
interval patterns (Abeles & Gerstein, 1988; Abeles et al., 1993; Baker & Lemon,
2000; Dayhoff & Gerstein, 1983a, 1983b; Grün et al., 2002a, 2002b; Grün et al.,
1999; Martignon et al., 2000, 1995, 1994; Tetko & Villa, 1992). Or the reference
could be spikes of other neurons. This leads to the idea of spike patterns across
populations of neurons, which would be very hard to observe experimentally, or at
least to the very common idea of coincidence or synchronicity of spikes in two or
more neurons which could be measured by correlation (Engel et al., 2001; Grün
et al., 1994b; König et al., 1995; Palm et al., 1988; Tsodyks et al., 2000). Today,
there is a lot of experimental evidence for the importance of both spike rates and
synchronicity. The idea of a population code as opposed to single neuron codes, be
it in terms of spike frequencies, ordering of spike latencies (Guyonneau et al., 2004;
Loiselle et al., 2005; Perrinet et al., 2003; Thorpe et al., 2004), or spike coincidences
in the population, has also become quite popular. An important aspect here is the
sparseness of co-occurring spikes in a typical population of excitatory neurons
(see Furber et al., 2007; Hyvärinen & Karhunen, 2001; Palm, 1980, 1982, 1987a;
Palm & Sommer, 1992). The population idea has been considered to derive rules
for synaptic plasticity and learning. The guiding principle in these theories is that
the neural interconnectivity should be modified by synaptic plasticity (the synapses
form the connections between neurons) in such a way that it creates population
activities that maximize the information content about the stimuli (Barlow, 1989;
Bell & Sejnowski, 1995; Haft & van Hemmen, 1998; Linsker, 1989a,b, 1992, 1997;
Yang & Amari, 1997) that are believed to be represented in that neural population
or cortex area (e.g., visual information in the visual cortex). More recently, details
of the precise timing of pre- and postsynaptic spikes and the resulting influence
on synaptic efficiency have been the focus of experimental, information theoretical,
and neural modeling studies (Bi & Poo, 1998; Bialek et al., 1991; Dan & Poo,
2006; Guyonneau et al., 2004; Hosaka et al., 2008; Izhikevich, 2007; Kempter et al.,
1999; Markram et al., 1997; Masquelier et al., 2009; Morrison et al., 2008; Pfister
& Gerstner, 2006; van Rossum et al., 2000).
Another issue is the variability of neural responses. The same neuron (e.g.,
in the visual cortex) will respond to repetitions of the same (visual) stimulus
not exactly in the same way. There may be large variations in both the rate
and the fine timing of the spikes. This leads to the question of what is the
“signal” (i.e., information about the experimental stimulus) and what is the “noise”184 13 Information, Novelty, and Surprise in Brain Theory
(this may also be information about something else) in the neural spike train
(Abeles et al., 1995; Arieli et al., 1996; Bair & Koch, 1996; Butts & Goldman, 2006;
Christodoulou & Bugmann, 2001; Knoblauch & Palm, 2004; Mainen & Sejnowski,
1995; Shadlen & Newsome, 1998; Softky & Koch, 1992, 1993; Stevens & Zador,
1998).
The ensuing discussions can get technically quite involved and detailed, but still
they often try to avoid questions about the purpose of the activity of an individual
neuron or a cortical area for the information processing of the behaving animal. It
is very likely that the purpose of the primary visual cortex (V1), for example, is not
just to present visual information. This information is presented quite efficiently
in the 106 fibers of the optic nerve. In the visual cortex, there are at least two
magnitudes more neurons—to represent just the same information? It can be that
certain important features that are implicit in the visual information in the optic
nerve are made explicit in the representation in V1. These can become important
presumably in terms of the behavioral responses and goals of the animal and may be
used by other brain areas to produce reasonable behavior. It may even be (and there
is experimental evidence for this) that general signals concerning the state of the
whole animal and in particular its attentiveness and even motivational or emotional
aspects also contribute to the neural responses in V1. These would normally be
regarded as “noise” with respect to the representation of the experimental visual
stimulus.
From this kind of argument, it becomes evident that the search for a “neural
code,” even in a rather peripheral area like V1, ultimately requires an integrative
information processing theory of the whole brain. In fact, a number of brain theories,
often for partial functionalities, have already been published (Edelman & Tononi,
2000; Grossberg, 1999; Hawkins & Blakeslee, 2004; Hecht-Nielsen, 2007; Palm,
1982; Shaw & Palm, 1988) and can serve as a background for these ideas on coding
and information theory (see also Tononi et al., 1992, 1994).
The whole idea of a neural code may even be (and has been) criticized from a
philosophical point of view (e.g., Bar-Hillel & Carnap, 1953; see also Palm, 1985):
the use of information theory may seem rather inconspicuous with respect to the so￾called mind–brain (or mind–body) problem, in particular since information theorists
never hesitate to admit that information in the technical sense does not concern the
“meaning” of the messages. Thus, information can be introduced almost innocently
in a scientific theory of the brain that ends with a theory of consciousness or at
least points toward such a theory. I do not wish to say that such a theory is totally
impossible (e.g., I sympathize with the theory put forward by Edelman and Tononi,
2000), but I think one should be aware of the points in the argument where aspects of
intentionality are brought in. The problem with this use of information terminology
in brain research is essentially that the concept of information or entropy may appear
to be an objective one (since entropy is, after all, a concept of physics; see Chap. 15),
but contains in fact a strong subjective and teleological element, namely, the choice
of the repertoire, the description, or the partition through which the world is viewed.
This aspect was rather hidden in classical information theory. In this new broader
approach, it becomes more evident.13.1 Understanding Brains in Terms of Processing and Transmission of. . . 185
In this use of information theory in brain research, where the receivers and
senders of information are not people but perhaps parts of a brain and we can
only indirectly try to infer their purposes, let alone the “code” that is used to
transmit information between them, classical information theory may be a little
too restrictive. One would like to be able to deal more directly with instances
of information extraction, formation of languages or language-like descriptions
of physical events, the formulation of more goal-directed purposeful variants of
information, and the like. A need for this is expressed more or less explicitly in
many attempts to use information theoretical ideas in the theoretical understanding
of brains (e.g., Abeles, 1991; Barlow, 1989; Barlow & Földiák, 1989; Barlow et al.,
1989; Coulter et al., 2009; Optican & Richmond, 1987; Palm, 1982; Uttley, 1979).
For example, the definition of the information contained in a stimulus (Butts, 2003)
should somehow reflect the importance of this stimulus for the animal, or at least
the importance of the neurons responding to this stimulus.
I believe the concepts of novelty, surprise, description, and repertoire, as
developed in this book, could help us to find a better and more appropriate use of
information theory in brain research. It may be possible to use these new concepts
in order to get rid of some (implicit) assumptions about the neural structures that
are to be analyzed in an information theoretical way. My intention is not to criticize
the ubiquitous recent applications of information theory in neuroscience and brain
research [a good review was given in Borst and Theunissen (1999); see also the
recent books Gerstner and Kistler (2002), Kamimura (2002), Rieke et al. (1997)].
Rather, I want to show where the particular new concepts introduced here, which
bear a certain ambivalence between information- and significance-related concepts,
can be successfully employed to achieve a better understanding or a theoretical
underpinning of argumentations that have been put forward in these fields and that,
in fact, have partially inspired me to introduce these new concepts. My exposition
will mainly consist of various applications of the concepts of novelty and surprise
introduced in Chap. 11.
The concept of information is clearly very useful in the neurosciences. The
activation of a neuron in the brain is considered as a signal for other neurons; it
represents some message or partial description about the state of the outside world
or of the animal itself. The neuron derives its activation from its synaptic inputs
which come from other neurons or (sometimes) directly from sense organs. If we
take together the knowledge about the activations of all neurons in the brain into a
so-called activity state or activity vector, this should contain a rather comprehensive
description of the state of the environment and of the animal itself, in other words: of
the current situation. Such a representation is necessary as far as it concerns aspects
of the situation that are vital for the animal, because it is the basis on which the
animal has to choose, plan, and perform the proper action in the given situation.
Clearly, every single neuron in the brain can only be monitoring very few
specific aspects of the whole situation, and these aspects can well be described
mathematically by a repertoire or by a description.186 13 Information, Novelty, and Surprise in Brain Theory
In this chapter, such neural repertoires will be considered in three contexts:
1. Considering the synaptic inputs and outputs of a neuron as its environment,
what are the events there that a neuron is interested in, and how can it convey
interesting signals to other neurons? In terms of surprise and repertoires, we
could formulate the question as follows: what could be the repertoire on a
neuron’s input space that describes the interest of the neuron, and how can the
neuron signal the amount of novelty or surprise it observes through its repertoire
to other neurons? This perspective on neural activity was first formulated
explicitly by Legéndy (1975).
2. Considering the electrophysiologist who probes the activation of one or several
neurons, what are the events he should be interested in, i.e., through which
repertoire should he observe the neuron(s), and how much surprise can he expect
to get?
3. Considering the activity state of the brain as a whole, how does it describe the
state of the world including the animal, and what are the syntactical constraints
on such a description that go along with it being composed from single neuron
descriptions?
13.2 Neural Repertoires
Neural activity is transmitted by electrical signals, called action potentials or spikes.
Conventionally, spikes are regarded as unitary events, and so the sequence of spikes
produced by a neuron is described by a (stochastic) point process. We denote by Tn
the time of occurrence of the n-th spike of the neuron, so the sequence T1, T2, T3,...
describes the firing of the neuron. The input space of a neuron is given by its afferent
axons, each of which produces a spike sequence. Thus, the input space is described
by the combined process (T i
n ), where each entry T i
n denotes the time of occurrence
of the n-th spike in the i-th afferent (n = 1, 2, 3,... , i = 1,...,A where A is the
number of afferents).
Which events in this afferent space are interesting for the neuron?
To answer this question, we need some kind of model for the neuron (see, e.g.,
Holden, 1976; MacGregor, 1987). If we assume a fairly simple but widely accepted
model of a neuron, the degree of interest of the neuron can be described by just one
physical variable: the depolarization of the neuron. This depolarization D is a real
function of the combined process (T i
n ), and the neuron’s “interest” corresponds to
the one-dimensional repertoire D≥. The depolarization D is high whenever many
excitatory afferents are spiking at about the same time, and when no inhibitory
afferents are spiking at about of before this time. Again a simple model describes
the effects of each afferent spike by a postsynaptic potential (PSP) and the total
depolarization d simply as the sum of these potentials. The time course of the PSP
may differ between different afferents, and there are excitatory EPSPs which are
depolarizing and thus positive and inhibitory IPSPs which are hyperpolarizing and13.3 Experimental Repertoires in Neuroscience 187
D
Fig. 13.1 A single neuron
thus negative. This simple model leads to the equation
D(t) = 
n

i
hi(t − T i
n ), (13.1)
where hi is the PSP of the i-th afferent (compare Fig. 13.1).
This means that a neuron is interested in sudden bursts of spikes in its excitatory
afferents, in coincident spiking of its afferents, and possibly also in pauses of
its inhibitory afferents which coincide with interesting events in the inhibitory
afferents.
If the neuron wants to transmit the surprise it gets out of this interest, i.e., out
of the description D≥, then it has to produce an event which is interesting for the
next neuron. Thus, it should respond itself with a burst of spikes to an interesting
event. This is what neurons in fact do. The next neuron (or better one of the neurons
to which our neuron is an afferent) may get excited by a burst in our neuron alone,
but of course, the next neuron is more likely to get excited if the burst in our neuron
coincides with bursts or single spikes in some more of its afferents. But our neuron
will not have much influence on this; this coincidence can only be detected and
properly signaled by the next neuron.
13.3 Experimental Repertoires in Neuroscience
The electrophysiologist who probes neural activity with his electrodes is in a
situation that is quite similar to that of the single neuron. If he records intracellularly,
he has access to the depolarization variable D; if he records extracellularly, he has188 13 Information, Novelty, and Surprise in Brain Theory
access to the spike train(s); and if he records from several neurons simultaneously,
they can be regarded as his “afferents.” In addition, he also has information about
the outside world, which he can see through his own eyes, not through the eyes
of the animal. Often he records some particular features of this state of the world
which he believes to be interesting for the animal and perhaps even for the particular
neuron(s) he is recording from. Often he controls these features as a so-called
stimulus. Of course, it is well possible that the animal or even the recorded neuron
is interested in or reacts to other aspects of the experimental situation which are
not captured by the stimulus as defined by the experimenter. In addition to the
neural responses, the experimenter may also record behavioral responses of the
animal. In behavioral experiments with trained animals, the animal is rewarded for
certain responses to the different experimental stimuli. In these cases, one can use
information to measure how well the different stimulus response configurations are
differentiated by the animal or by the neuron or neural population recorded from.
Usually, the experimenter will look out for interesting neural events in order to
correlate them with observations on the stimulus or the behavioral response. In
order to define an interesting neural event, he normally has to rely on the same
features that are available to the single neuron, i.e., on bursts and coincidences.
In multiunit recordings, one may also try to combine these features into a one￾dimensional repertoire using plausible models for the functions hi and working with
(13.1). Thus, the experimenter may create a “burst repertoire,” a “pause repertoire,”
a “coincidence repertoire,” and even a depolarization repertoire in order to evaluate
his data. In contrast to the neuron, however, the experimenter will use not only the
novelty created by these repertoires but also the statistically more relevant surprise.
In the following, we will briefly describe these repertoires.
13.3.1 The Burst Repertoire
We consider again the times Ti of the occurrences of spikes in a single neuron,
i.e., the space
 = {(Ti)i∈N : Ti+1 ≥ Ti}.
On  we define the burst repertoire as
αn = {Akn
t : k ∈ N and t ∈ R},
where Akn
t = [Tn − Tn−k ≤ t].
The novelty or surprise of αn is the burst novelty or burst surprise (cf. Legéndy
& Salcman, 1985; Palm, 1981) obtained at the moment of the n-th spike in the
train. One can plot for real spike trains this kind of burst novelty against time (see
Palm, 1981).13.3 Experimental Repertoires in Neuroscience 189
9
bit
a
b
c
bit
bit
6
60
40
20
3
3
Fig. 13.2 Burst novelty as a function of time for two individual neuron spike trains and a simulated
Poisson spike train. (a) Unstimulated neuron in a fly, (b) stimulated neuron in cat LGN, (c) Geiger
counter. The extremely high novelty in (b) is due to fast non-stationary spike generation caused by
visual stimulation. Even in an unstimulated neuron, (a) the average novelty is a bit larger than in
the Poisson process (c)
For any concrete spike train ω ∈ , we can calculate its novelty with respect to α,
and we can, for example, compare it with the average novelty of α to see whether it
was really surprisingly surprising (cf. Chap. 11). Some experimental and theoretical
analysis of this model was given by Palm (1981). For these calculations, one needs
a probability distribution p on the set  of possible spike trains. This distribution
should reflect some of the statistical properties of the observed spike trains, but only
to a certain degree. It should also be possible to regard it as the “naive” distribution
against which the surprise of the actually observed spike train can be measured. For
this reason, I have considered the Poisson distribution on  for the calculation of
burst novelty and surprise shown in Figs. 13.2, 13.3, and 13.4.190 13 Information, Novelty, and Surprise in Brain Theory
Fig. 13.3 Burst novelty as a function of time for a neuron in monkey motor cortex. The higher
novelty toward the end of the recording is related to a hand movement the monkey was trained to
perform (see Brochier et al., 2018)
0 5 10 15 20
0
2
4
6
8
10
12
14
16
novelty
surprise
Fig. 13.4 Burst surprise as a function of burst novelty, “burst-N2S curve”
A slightly more general model is the so-called renewal process (see also Grün
et al., 1994b; Grün & Rotter, 2010). Here we observe the so-called interspike
interval distribution, i.e., the probability distribution of the time intervals between
successive spikes D = Tn+1 − Tn, and we assume that this random variable
D is independent of n and, in addition, that subsequent interspike intervals are
independent. In this case, the variables Tn − Tn−k in αn have the same distribution
for all n.And this distribution is the distribution of the sum of k identical independent
versions of D. These distributions are quite well understood in probability theory
(e.g., Doob, 1953). A special case is the Poisson distribution which arises from the
assumption that D is exponentially distributed, i.e., p[D ≤ t] = 1 − e−λt. Even for
this simple distribution, I have not been able to calculate the relation between burst13.3 Experimental Repertoires in Neuroscience 191
Fig. 13.5 Burst
novelty-to-surprise (N2S)
curves for four different
gamma distributions (shape
factor
κ = 0.5, 1.0, 1.25, 3.33).
Modified from Ito et al., 2019
novelty and burst surprise analytically. So I have used computer simulation of the
Poisson process to obtain Fig. 13.4.
The idea to use burst surprise to determine the significance of individual bursts
against a “zero hypothesis” of Poisson distributed spiking has been taken up in the
experimental literature and used for the determination of response latencies of single
neurons in single trials (Ito et al., 2019). To simplify the calculation, the authors
also introduced a version of the burst novelty measure, called strict novelty (see
Definition 3 in Ito et al., 2019).
In this context, the zero hypothesis was refined to a gamma process with an
experimentally determined shape factor in addition to the observed firing rate. For
this purpose, similar novelty-to-surprise (N2S) curves as in Fig. 13.4 were generated
for different values of the shape factor κ, where κ = 1 corresponds to the Poisson
distribution (see Fig. 13.5). All these curves look quite similar, so it does not seem to
matter much which N2S curve is used (for which shape factor). For this reason, one
could also use the original Poisson novelty N1 to evaluate bursts, simply because
it is easier to calculate. But then, in order to determine the statistical significance
of this novelty value, one has to determine the distribution of N1 (i.e., the N12S
curve) under the right zero hypothesis, which is determined from the spike train data.
Assuming a gamma distribution, this is just given by the experimentally observed
estimate of the shape factor κ and the spike frequency, but it could also be a more
complex interspike interval distribution, if this can be estimated from the data.
Figure 13.6 shows the N12S curves for the simple Poisson novelty N1 evaluated
or sampled with different gamma distributions to determine the corresponding burst
surprise. Here one can see that the N12S curves depend quite strongly on the shape
factor of the zero (gamma) distribution.
The general idea behind this approach is to use a simple, easy to calculate
“approximation” Na of the novelty N as our statistical variable, when the novelty
itself is too complicated to compute from the zero hypothesis and then to compute
or sample the distribution of Na or equivalently the surprise S(Na) instead of S(N)
from the correct zero hypothesis to determine statistical significance.192 13 Information, Novelty, and Surprise in Brain Theory
Fig. 13.6 Poisson
novelty-to-surprise (N12S)
curves for four different
gamma distributions.
Modified from Ito et al., 2019
13.3.2 The Pause Repertoire
Again we consider  = {(Ti)i} and now the repertoire is βn = {Bn
t }, where Bn
t =
[Tn−Tn−1 ≥ t]. The novelty or surprise of βn is the pause novelty or pause surprise
obtained at the moment of the n-th spike. This has (to my knowledge) not yet been
calculated and displayed for real spike trains. In many experimental situations, it
seems to be less interesting than the burst repertoire. We observe that βn is a chain
and so surprise and novelty coincide for βn.
13.3.3 The Coincidence Repertoire
We now consider several spike trains, or the corresponding times T i
n of the
occurrences of the n-th spike in the i-th neuron. Thus
 = {(T i
n ): n ∈ N, i = 1,...,m}.
The coincidence repertoire is defined as
γt = {Ck
t : k ∈ N, > 0}, (13.2)
where
Ck
t =
/

n

i
1[t−≤T i
n≤t] ≥ k
0
= 
at least k spikes co-occurred in the short time interval [t − , t]

.13.3 Experimental Repertoires in Neuroscience 193
This repertoire can be used to measure the novelty for coincidence at time t. Observe
that this repertoire is the union of repertoires:
γt = {Ck
t : k ∈ N},
which correspond to the description D≥ of the (13.1), when hi(t) := 1[0≤t≤].
In many experimental paradigms,  is in fact fixed, for example, as a time bin width,
so the repertoire γt is used, which is a chain, and surprise coincides with novelty.
In this case, one can calculate the coincidence surprise for multiple coincidences
of spikes observed in k time bins or “sites” across different neurons and/or temporal
instances. To this end, we have to calculate the probability that more than n such
coincidences of k spikes in the k bins are observed during a period of L observations
of these bins (e.g., repetitions of a stimulus) given that each bin j (j = 1,... , k) has
received nj spikes. This probability is calculated for the naive hypotheses that there
is no interaction between these k sites, i.e., that the k stochastic sequences of length
L are stochastically independent. For ease of modeling, we assume that the time
bins are short enough such that there can be at most one spike in each bin. Thus, we
get k independent binary sequences xj , each of length L, i.e., xj = (xj
1 ,...,xj
L)
where the probabilities pj = pr[xj
i = 1] are unknown and all xj
i are independent.
The task is to calculate
pk(n) = pr
L
i=1
'
k
j=1
xj
i = n
(
(
(

L
i=1
xj
i = nj for j = 1,...,k
(13.3)
and
Pk(n) = nk
i=n
pk(i). (13.4)
Here xj
i ∈ {0, 1} and therefore 1k
j−1 xj
i = 1 exactly if there is a k-coincidence,
so the first sum in (13.4) counts the k-coincidences, where the other sums count the
number of spikes in the j -th site during the observation. This probability has first
been calculated for k = 2 in Palm et al. (1988) and was incorporated as an analysis
tool in early versions of the Joint Peri-Stimulus-Time-Histogram (JPSTH) program
(Aertsen et al., 1989), later for higher values of k by Grün et al. (Grün et al., 2002a,
2002b) and extended to more general recording conditions (Grün et al., 1999; Gütig
et al., 2002).
The calculation is comparatively easy if we proceed by induction on k. The fact
that we do not know the parameters pj does not matter because all binary sequences
(xj
1 ,...,xj
L) containing nj ones are equally probable and therefore we can use
combinatorial counting arguments of these L
nj

sequences.194 13 Information, Novelty, and Surprise in Brain Theory
Obviously, for k = 1 we have
p1(n) =

1 if n = n1,
0 otherwise.
Assume that we know pk−1(n) for all n (obviously pk−1(n) = 0 for n>n1).
To calculate pk(n), we observe that for getting exactly n k-coincidences, we have
to have at least n coincidences in the first k −1 spike sequences and at least n spikes
in the k-th sequence, and if we have these (k −1)-coincidences in i ≥ n places, then
exactly n of the nk spikes of the k-th sequence have to be in these i places, and the
remaining ones have to be in the remaining L − i places.
Thus
pk(n) = n1
i=n
pk−1(i)
i
n
 L−i
nk−n

L
nk
 (for n ≤ nk ), (13.5)
where of course pk(i) = 0 for i > min{nj : j = 1,...,k − 1}.
From the recursion (13.5), we immediately get
p2(i) =
n1
i
L−n1
n2−i

L
n2

and
P2(n) = n2
i=n
n1
i
L−n1
n2−i

L
n2
 .
The formulae become a bit simpler if we assume that the sites have been
reordered in such a way that n1 ≤ n2 ≤ n3 ... ≤ nk and furthermore that spikes are
relatively sparse such that n1 + nk ≤ L.
In this case,
p3(n) = n1
i2=n
i1
i2
L−i1
n2−i2

L
n2

i2
n
L−i2
n3−n

L
n3

for n ≤ n1, where we have put i1 := n1. By induction we get
pk(ik) = 
i1
i2=n
··· 
ik−2
ik−1=n
k
'−1
j=1
 ij
ij+1
 L−ij
nj+1−ij+1

 L
nj+1
 (13.6)13.4 Neural Population Repertoires: Semantics and Syntax 195
and
Pk(n) = 
i1
i2=n
··· 
ik−1
ik=n
k
'−1
j=1
 ij
ij+1
 L−ij
nj+1−ij+1

 L
nj+1
 . (13.7)
13.3.4 The Depolarization Repertoire
Here we again take  = {(T i
n)} and now simply the repertoire δ = R(D≥) where
D is defined in (13.1), with some specific choice for the functions hi. Very common
is the choice
hi(x) = e−x/τ or hi = xe−x/τ . (13.8)
For fixed τ , this repertoire is of course a chain, and surprise coincides with
novelty. If we take hi = 1[0,τ ], we obtain a kind of burst repertoire for the “unified”
spike train of all observed neurons. And we have a similar computational problem
as for the burst surprise when we try to consider (13.8) with variable τ and try to
compute the surprise.
13.4 Neural Population Repertoires: Semantics and Syntax
The firing of the neurons in the brain signifies something in the outside world. If we
want to interpret neural activity in this way, we have to consider as the neural input
space not only the direct afferents to the single neuron as in (13.2), but we have to
consider ultimately the state of the whole world, or more specifically, the state of the
animal and of its environment. Indeed, the firing of the afferents to a single neuron
in the brain is determined by the firing of their afferents and so on, and eventually all
this neural activation is derived from activation in the sensory afferents to the brain
which provide information about the internal state of the animal and the (external)
state of its environment.
If we consider the repertoire of a neuron as determined by its depolarization D,
it is certainly a one-dimensional repertoire of propositions about the state x ∈  of
the world. Interpreting a neuron’s activity as evidence for a proposition about the
external world in this way is quite useful in neuroscience. This idea is related to the
concept of a receptive field [for a discussion of this aspect, see, e.g., Johannesma
(1981), Aertsen and Johannesma (1981), or Krone et al. (1986)].
The topic of this section is how to combine these various one-dimensional
descriptions provided by many (or even all) neurons in the brain. In the math￾ematical terminology introduced in this book, this combination or assembly of
individual neural descriptions can be formulated very easily: if the firing of one196 13 Information, Novelty, and Surprise in Brain Theory
neuron n relates the surprise or evidence for its description dn, then the activity state
combined of several neurons n ∈ N provides the description dN = ∩n∈N dn.
What does this imply for the corresponding assembly repertoire αN of a large set
of N neurons?
1. First of all, such a repertoire is obviously ∩-stable and therefore tight, so that
there is a one-to-one correspondence between the assembly repertoire αN and
the assembly description dN .
2. It is important to notice also that this assembly repertoire αN need not be closed
under complementation or negation. One could argue against this, because, after
all, if one neuron does not fire, its not-firing also provides information, for
example, for the absence of a feature. But it is very doubtful whether the brain
will be capable of making use of this information in every case. Normally, the
firing of a neuron is much more significant than its not-firing, simply because
most neurons do not fire most of the time [compare also Legéndy (2009),
Legéndy (1975), or Legéndy and Salcman (1985)]. Also the not-firing will in
most cases have no significant effect on the postsynaptic neurons—at least when
the neuron is excitatory (which is the clear majority). A more detailed discussion
of this aspect can be found in Palm et al. (1988).
Therefore, it is reasonable not to assume that the repertoire αN is closed under
negations in general. There are certainly some relative complements, which are rep￾resented by neural activity. For example, the absence of activation in an inhibitory
neuron may be a necessary condition for the firing of one of its postsynaptic neurons.
This means that this postsynaptic neuron can represent a proposition of the form A
and not B, where A and B are represented by neural activation. For the repertoire
αN , this means that it may contain A, B, A ∩ B, and A \ B for some propositions A
and B. Thus, it may occasionally happen that a proposition A ∈ αN is completely
split up into disjoint subcases A1,...,An (whose union then is A). We do not
believe, however, that a typical neural assembly repertoire αN contains a partition
of the whole space X. This is simply because each significant proposition signaled
by such a repertoire is so improbable that there are not enough of them to cover the
whole space X.
All this discussion has a few formal consequences on the repertoire αN : it need
not be clean (i.e., free of unions). And it is usually neither closed under negation
nor closed under unions.
The description dN gives a direct interpretation of neural activity patterns in
terms of the outside world, i.e., the stimuli delivered to the animal and its behavioral
response.13.5 Conclusion 197
13.5 Conclusion
We can use concepts from information theory in trying to understand the functioning
of brains on many different levels, ranging from the neuron to the entire organism or
person. On each level, one can distinguish the novelty or the surprise obtained from
the incoming signals by the unit under consideration from the information provided
by these signals and transmitted to this unit. In every case, this information and the
corresponding transmission channel capacity is larger than the surprise obtained.
The discrepancy between information and surprise is most obvious for the whole
organism, when we consider how little surprise we typically get out of how much
input information.
As for the relation between novelty and surprise in the brain, this issue is
more relevant for the statistical evaluation of neurophysiological observations. It
has been the subject of controversial discussions in the literature, in particular
concerning the significance of spatiotemporal spike patterns (citations are collected
on p. 176),without the use of the terminology introduced in this book. Now this
topic can be formulated as a neat mathematical problem.
Problem For a reasonable probability distribution for neural spike trains (like the
Poisson distribution which can often serve as a good zero hypothesis), and for all the
repertoires α defined in this chapter, one should try to calculate the average novelty
N (α) and the novelty statistics, i.e., prob [Nα ≤ t] for all t ∈ R.
This problem is actually quite easy for the Poisson distribution and most of the
repertoires (the neuronal, the coincidence, and the population repertoire); it is harder
(we do not know the analytical answer yet) for the burst repertoire. There are a few
results dealing with some instances of this problem that can be found in the literature
of the last 30 years. Most of these results are collected in the technical comments
below.
Once again, one can roughly identify the three quantities novelty, information,
and surprise introduced in this book with three viewpoints on brain activity:
novelty with the subjective view, seeing the world through the “eyes” of individual
neurons or neural populations (Legéndy, 1975; Letvin et al., 1959), information (or
transinformation) with the functional view of measuring the neuron’s contribution to
the animal’s experimental performance (Barlow, 1961; Borst & Theunissen, 1999;
Nemenman et al., 2008), and surprise with the physiological statistical view that
tries to find significant patterns of neural activation (Abeles & Gerstein, 1988;
Aertsen et al., 1989; Dayhoff & Gerstein, 1983a; Grün et al., 1994b, 2002a;
Martignon et al., 1995; Palm et al., 1988).198 13 Information, Novelty, and Surprise in Brain Theory
13.6 Technical Comments
Many applications of classical information theory in neuroscience have appeared
during the last 30 years. Here I can only group a number of these papers according
to the topics briefly discussed in the beginning of this chapter:
1. Signal vs. noise in the variability of neural responses: Abbott (1994), Brunel
and Nadal (1998), Butts (2003), Butts and Goldman (2006), Christodoulou and
Bugmann (2001), Golomb et al. (1997), Kang and Sompolinsky (2001), Kjaer
et al. (1994), Knoblauch and Palm (2004), Mainen and Sejnowski (1995), Nawrot
et al. (2008), Shadlen and Newsome (1994, 1998), Softky and Koch (1992, 1993),
Softky (1995), Stevens and Zador (1998), Nakahara and Amari (2002), Nakahara
et al. (2006), Hansel and Sompolinsky (1996).
2. Rate coding vs. fine timing of spikes: Abbott (1994), Abeles et al. (1995), Aertsen
et al. (1989), Aertsen and Johannesma (1981), Bach and Krüger (1986), Bair and
Koch (1996), Barlow (1961), Bethge et al. (2002), Bialek et al. (1991), Brette
(2015), Brown et al. (2004), Cessac et al. (2008), Deadwyler and Hampson
(1997), Dean et al. (2005), Denève (2008), Eckhorn et al. (1976), Engel et al.
(2001), Gerstein and Aertsen (1985), Gerstner and Kistler (2002), Gerstner et al.
(1997), Golomb et al. (1997), Gütig et al. (2002), Kjaer et al. (1994), König
et al. (1995), Kostal et al. (2007), Krone et al. (1986), Krüger and Bach (1981),
Legéndy and Salcman (1985), Mainen and Sejnowski (1995), Markram et al.
(1997), Morrison et al. (2008), Nakahara and Amari (2002), Nirenberg and
Latham (2003), Palm et al. (1988), Panzeri and Schultz (2001), Panzeri et al.
(1999), Perkel and Bullock (1967), Pfister and Gerstner (2006), Rieke et al.
(1997), Schneideman et al. (2003), Seriès et al. (2004), Shadlen and Newsome
(1994), Smith and Kohn (2008), Softky (1995), Softky and Koch (1993), Tsodyks
and Markram (1997), Vaadia et al. (1995).
3. Population code: Aertsen and Johannesma (1981), Amari and Nakahara (2005),
Amari and Nakahara (2006), Barlow (1989), Bethge et al. (2002), Bialek et al.
(2007), Brenner et al. (2000), Brunel and Nadal (1998), Butts and Goldman
(2006), Coulter et al. (2009), Dan et al. (1996), Deadwyler and Hampson (1997),
Dean et al. (2005); Furber et al. (2007), Georgopoulos et al. (1986),Gutnisky
and Dragoi (2008), Kang and Sompolinsky (2001), Krone et al. (1986), Legéndy
(2009), Linsker (1989b), Linsker (1992), Nirenberg and Latham (2005), Osborne
et al. (2008), Prut et al. (1998), Rolls et al. (1997), Schneideman et al. (2003),
Zemel and Hinton (1995).
4. Significance of spike patterns: Abeles (1991), Abeles et al. (1995), Abeles et al.
(1993), Abeles and Gerstein (1988), Baker and Lemon (2000), Brown et al.
(2004), Cessac et al. (2008), Dan and Poo (2006), Dayhoff and Gerstein (1983a,
1983b), Gerstein and Aertsen (1985), Gerstein (2004), Grün et al. (1994b, 2002a,
2002b, 1999), Gütig et al. (2002), Hosaka et al. (2008), Maldonado et al. (2008),
Martignon et al. (2000), Martignon et al. (1995), Masquelier et al. (2009),
Nakahara and Amari (2002), Palm et al. (1988), Pfister and Gerstner (2006),13.6 Technical Comments 199
Riehle et al. (1997), Roelfsema et al. (2004), Stark and Abeles (2009), Staude
et al. (2010), Tetko and Villa (1992), Torre et al. (2016).
5. Neural coding in the visual system and natural scene statistics: Adelman et al.
(2003), Atick (1992), Atick and Redlich (1990, 1992), Barlow (1989), Bethge
et al. (2002), Butts et al. (2007), Dan et al. (1996), Dong and Atick (1995),
Field and Chichilnisky (2007), Gutnisky and Dragoi (2008), Haft and van
Hemmen (1998), Hoyer and Hyvärinen (2002), Hyvärinen and Karhunen (2001),
Hyvärinen and Hoyer (2001), Hyvärinen et al. (2009), Koepsell and Sommer
(2008), Koepsell et al. (2009), Krone et al. (1986), Legéndy (2009), Linsker
(1989b), Linsker (1989a), McClurkin et al. (1991), Optican et al. (1991), Optican
and Richmond (1987), Rolls et al. (1997), Seriès et al. (2004), Wang et al. (2010).
Perhaps the first paper that tried to understand the organization of the brain from
the point of view of each single neuron in terms of the information that a single
neuron could obtain from its afferents and transmit to its efferents was written by
Legendy (1975). Putting ourselves in the position of one single neuron, we can ask
ourselves “What could it be interested in?” and “How could it transfer this into
surprising signals on its own axon?”
The second question is very familiar to the single neuron physiologists who
record the spike trains from single neurons and try to make sense of it. What they
listen to are “bursts,” i.e., temporary increases in spiking activity. The surprising
event being that the neuron fires comparatively many spikes in comparatively short
time intervals. Typically, bursts consist of anything between 5 and 50 spikes (for a
more elaborated statistics, see Legéndy and Salcman, 1985). The analysis of bursts
in terms of surprise or novelty has been initiated by Palm (1981) and Legéndy and
Salcman (1985).
The first question has also become a technical question for those neurophysiolo￾gists who started to do multiunit recordings. In this case, the physiologist’s afferents
are the neurons he records from. Three answers have been given:
(a) Coincidence: A surprisingly large number of afferents (to the neuron or to the
physiologists’ electrode) fire within the same (short) time window.
(b) Afferent patterns: A certain pattern (i.e., subset of the afferents) fires within
the same short time window. This was probably most often the case when a
physiologist was surprised by a large coincidence (Abeles et al., 1993; Bach &
Krüger, 1986; Krüger & Bach, 1981; Legéndy & Salcman, 1985).
(c) Spatiotemporal patterns: A certain spatiotemporal pattern extended over a
longer time window (or a certain “melody,” if we identify the afferent neurons
with different musical notes) is repeated surprisingly often. Actually, for
“melodies” of even a small length, already a single repetition is surprising
(Dayhoff & Gerstein, 1983a, 1983b).
Since I do not believe that single neurons possess a memory for detailed spatiotem￾poral patterns of a long duration (let us say more than 50 ms), the possibility (c) is
perhaps mainly of interest to the multiunit physiologists as a problem of statistical
evaluation of their recordings. And some of them indeed used conceptual ideas that200 13 Information, Novelty, and Surprise in Brain Theory
are closely related if not identical with the surprise concept (cf. Abeles & Gerstein,
1988; Abeles et al., 1993; Aertsen et al., 1989; Brown et al., 2004; Gerstein &
Aertsen, 1985; Martignon et al., 1995; Palm et al., 1988; Tetko & Villa, 1992).
13.6.1 Coincidence
The most straightforward possibility for a single neuron’s surprise is in fact (a).
A typical neuron needs a certain number of coincident input spikes in order to
become sufficiently active. Additional bursting of some of these inputs may help so
that perhaps the better formulation of the surprising event has both the ingredients
of bursting and of coincidence: all afferents taken together fire surprisingly many
spikes within a surprisingly short time interval. The mathematical analysis of this
kind of novelty or surprise is identical with the analysis of burst novelty or surprise
as described above.
13.6.2 Coincidental Patterns
The single neuron could also have access to the surprise provided by short-time
patterns (b) through the additional mechanism of Hebbian learning. If a specific
combination of its afferent fires together a few times, the neuron can react more
strongly to exactly these afferents in the future, thus emphasizing this specific
combination. It was Hebb’s idea to assume that this could be done by a mechanism
of synaptic plasticity that strengthens only those afferent connections to a neuron
that have together succeeded in activation it. Since a neuron is best activated by
coincident input activity, this mechanism creates a detector for surprising coincident
afferent patterns. Hebb synapses have been a purely speculative idea for a number
of years, but since the early 1990s, they have entered neurobiological reality, and
the more detailed biochemical mechanisms of synaptic plasticity are intensely
investigated today and that are modeled in more detailed mechanisms of STDP (Bi
& Poo, 1998; Bliss & Collingridge, 1993; Dan & Poo, 2006; Hosaka et al., 2008;
Izhikevich, 2007; Izhikevich & Desai, 2003; Linsker, 1989b; Lisman & Spruston,
2005; Markram et al., 1997; Masquelier et al., 2009; Morrison et al., 2007, 2008;
Pfister & Gerstner, 2006; Song et al., 2000; van Rossum et al., 2000).
13.6.3 Spatiotemporal Patterns
The case (c) of spatiotemporal patterns has also been analyzed in the literature
(Dayhoff & Gerstein, 1983a, 1983b; Grün et al., 1994a, 1994b, 2002a, 2002b;
Quaglio et al., 2017, 2018). Here the relation between novelty and surprise is13.6 Technical Comments 201
particularly instructive. At first sight, it seems that each spiking pattern of a certain
length L may be surprising by itself. We can model this situation again for a set K
of observed neurons using discrete time, i.e.,
 =
	
bi
j
i∈K
j∈N
: bi
j ∈ {0, 1}


.
A pattern of length L is a sequence 
ci
j
i∈K
j=1,...,L
=: c. If we are interested in
all patterns of length L, our cover is simply γ = {Cc : c pattern of length L}, where
Cc =
)
ω =

bi
j

∈ : bi
j = ci
j ∀j = 1,...,L ∀i = 1,...,k*
. In this case, every
pattern would be very surprising, but exactly for this reason, this large calculated
novelty is not really surprising.
Let us model this situation more completely: We assume as a naive proba￾bility assignment that all bi
j are independent and have the same probability q
of being a 1. Thus, if Nc is the number of 1’s in a pattern c, we simply have
p(c) = qNc (1 − q)Lk−Nc .
If q = 1/2, then all patterns are equally improbable: p(c) = 2−Lk, and the
novelty for each pattern is equally high: S(c) = Lk. If q is small, which is typical
for spike trains, then r := (1 − q)Lk is not incredibly small between 0 and 1 and
p(c) =
 q
1 − q
Nc
· r, implying N (c) = Nc · log
1 − q
q
+ log r.
So the novelty increases with Nc and our measurement reduces essentially to the
coincidence surprise of Sect. 13.6.2.
Another interpretation of the pattern novelty and surprise, in particular, in
the case where the 1’s are much more surprising than the 0’s, would be to
consider only 1’s, i.e., the occurrences of spikes in a pattern. This leads more
or less to the same statistics: in this case, we describe a pattern c by the
proposition Dc =
)bi
j

∈ : bi
j = 1 for all i, j where ci
j = 1
*
and form the cover
δ := {Dc : c pattern of length L}.
If again Nc denotes the number of 1’s in a pattern c, then p(Dc) = qNc .
Therefore, N (Dc) = Nc · (− log q) for all patterns c.
This is quite obvious because in every case the novelty of a proposition in
the cover δ depends only on the total number of spikes implied by it. Thus, the
calculation of the corresponding surprise reduces essentially to the calculation of
burst surprise (see Sect. 13.3.1).
After these observations, one may argue that the real surprise in the case of
spatiotemporal patterns does not lie in the fact that each of the patterns is very
surprising by itself, but in the repetitive occurrence of one (or a few) of these
patterns. This case is best treated as the surprise of repetition (of an improbable
event). This kind of problem has already been analyzed to some extent by Dayhoff202 13 Information, Novelty, and Surprise in Brain Theory
and Gerstein (1983b), Abeles and Gerstein (1988), and Gerstein (2004). We will
return to it in the next chapter.
References
Abbott, L. F. (1994). Decoding neuronal firing and modeling neural networks. Quarterly Reviews
of Biophysics, 27, 291–331.
Abeles, M. (1991). Corticonics: Neural circuits of the cerebral cortex. Cambridge University Press.
Abeles, M., & Gerstein, G. L. (1988). Detecting spatiotemporal firing patterns among simultane￾ously recorded single neurons. Journal of Neurophysiology, 60(3), 909–924.
Abeles, M., & Lass, Y. (1975). Transmission of information by the axon: II. The channel capacity.
Biological Cybernetics, 19(3), 121–125.
Abeles, M., Bergman, H., Margalit, E., & Vaadia, E. (1993). Spatiotemporal firing patterns in the
frontal cortex of behaving monkeys. Journal of Neurophysiology, 70(4), 1629–1638.
Abeles, M., Bergman, H., Gat, I., Meilijson, I., Seidemann, E., Tishby, N., & Vaadia, E. (1995).
Cortical activity flips among quasi stationary states. Proceedings of the National Academy of
Sciences of the United States of America, 92, 8616–8620.
Adelman, T. L., Bialek, W., & Olberg, R. M. (2003). The information content of receptive fields.
Neuron, 40(13), 823–833.
Aertsen, A. M. H. J., & Johannesma, P. I. M. (1981). The spectro-temporal receptive field.
A functional characteristic of auditory neurons. Biological Cybernetics, 42(2), 133–143.
Aertsen, A. M. H. J., Gerstein, G. L., Habib, M. K., & Palm, G. (1989). Dynamics of neuronal
firing correlation: Modulation of “effective connectivity”. Journal of Neurophysiology, 61(5),
900–917.
Amari, S.-i., & Nakahara, H. (2005). Difficulty of singularity in population coding. Neural
Computation, 17, 839–858.
Amari, S., & Nakahara, H. (2006). Correlation and independence in the neural code. Neural
Computation, 18(6), 1259–1267.
Arieli, A., Sterkin, A., Grinvald, A., & Aertsen, A. M. H. J. (1996). Dynamics of ongoing activity:
Explanation of the large variability in evoked cortical responses. Science, 273(5283), 1868–
1871.
Atick, J. J. (1992). Could information theory provide an ecological theory of sensory processing?
Network: Computation in Neural Systems, 3, 213–251.
Atick, J. J., & Redlich, A. N. (1990). Towards a theory of early visual processing. Neural
Computation, 2(3), 308–320.
Atick, J. J., & Redlich, A. N. (1992). What does the retina know about natural scenes? MIT Press.
Attneave, F. (1959). Applications of information theory to psychology. Holt, Rinehart and Winston.
Bach, M., & Krüger, J. (1986). Correlated neuronal variability in monkey visual cortex revealed
by a multi-microelectrode. Experimental Brain Research, 61(3), 451–456.
Bair, W., & Koch, C. (1996). Temporal precision of spike trains in extrastriate cortex of the
behaving macaque monkey. Neural Computation, 8(6), 1185–1202.
Baker, S. N., & Lemon, R. N. (2000). Precise spatiotemporal repeating patterns in monkey primary
and supplementary motor areas occur at chance levels. Journal of Neurophysiology, 84, 1770–
1780.
Bar-Hillel, Y., & Carnap, R. (1953). Semantic information. In London information theory
symposium (pp. 503–512). Academic.
Barlow, H. B. (1961). Possible principles underlying the transformation of sensory messages. MIT
Press.
Barlow, H. B. (1989). Unsupervised learning. Neural Computation, 1, 295–311.References 203
Barlow, H. B., & Földiák, P. (1989). Adaptation and decorrelation in the cortex. In C. Miall, R. M.
Durbin, & G. J. Mitcheson (Eds.), The computing neuron (pp. 54–72). Addison-Wesley.
Barlow, H. B., Kaushal, T. P., & Mitchison, G. J. (1989). Finding minimum entropy codes. Neural
Computation, 1(3), 412–423.
Barnard, G. A. (1955). Statistical calculation of word entropies for four Western languages. IEEE
Transactions on Information Theory, 1(1), 49–53.
Bateson, G. (1972). Steps to an ecology of mind. London: Intertext Books.
Bell, A. J., & Sejnowski, T. J. (1995). An information-maximisation approach to blind separation
and blind deconvolution. Neural Computation, 7, 1129–1159.
Bethge, M., Rotermund, D., & Pawelzik, K. (2002). Optimal short-term population coding: When
Fisher information fails. Neural Computation, 14, 2317–2351.
Bi, G.-Q., & Poo, M.-M. (1998). Synaptic modifications in cultured hippocampal neurons:
Dependence on spike timing, synaptic strength, and postsynaptic cell type. The Journal of
Neuroscience, 18, 10464–10472.
Bialek, W., de Ruyter van Steveninck, R. R., & Tishby, N. (2007). Efficient representation as a
design principle for neural coding and computation. Neural Computation, 19(9), 2387–2432.
Bialek, W., Reike, F., de Ruyter van Steveninck, R. R., & Warland, D. (1991). Reading a neural
code. Science, 252, 1854–1857.
Bliss, T. V. P., & Collingridge, G. L. (1993). A synaptic model of memory: Long-term potentiation
in the hippocampus. Nature, 361, 31–39.
Borst, A., & Theunissen, F. E. (1999). Information theory and neural coding. Nature Neuroscience,
2(11), 947–957.
Brenner, N., Strong, S., Koberle, R., Bialek, W., & de Ruyter van Steveninck, R. (2000). Synergy
in a neural code. Neural Computation, 12(7), 1531–1552.
Brette, R. (2015). Philosophy of the spike: Rate-based vs. spike-based theories of the brain.
Frontiers in Systems Neuroscience, 9, 151. https://doi.org/10.3389/fnsys.2015.00151
Brochier, T., Zehl, L., Hao, Y., Duret, M., Sprenger, J., Denker, M., Grün, S. & Riehle, A. (2018).
Massively parallel recordings in macaque motor cortex during an instructed delayed reach-to￾grasp task. Scientific Data, 5(1), 1–23.
Brown, E. N., Kass, R. E., & Mitra, P. P. (2004). Multiple neural spike train data analysis: State-of￾the-art and future challenges. Nature Neuroscience, 7, 456–461. https://doi.org/10.1038/nn1228
Brunel, N., & Nadal, J.-P. (1998). Mutual information, Fisher information, and population coding.
Neural Computation, 10(7), 1731–1757.
Butts, D. A. (2003). How much information is associated with a particular stimulus? Network:
Computation in Neural Systems, 14(2), 177–187.
Butts, D. A., & Goldman, M. (2006). Tuning curves, neuronal variability and sensory coding.
PLOS Biology, 4, 639–646.
Butts, D. A., Weng, C., Jin, J., Yeh, C.-I., Lesica, N. A., Alonso, J.-M., & Stanley, G. B. (2007).
Temporal precision in the neural code and the timescales of natural vision. Nature, 449(7158),
92–95.
Cessac, B., Rostro-González, H., Vasquez, J.-C., & Viéville, T. (2008). To which extend is
the “neural code” a metric? In Proceedings of the Conference NeuroComp 2008. Informal
Publication.
Cherry, C. (1966). On human communication. MIT Press.
Christodoulou, C., & Bugmann, G. (2001). Coefficient of variation (CV) vs mean inter-spike￾interval (ISI) curves: What do they tell us about the brain? Neurocomputing, 38–40, 1141–1149.
Coulter, W. K., Hillar, C. J., & Sommer, F. T. (2009). Adaptive compressed sensing—a new class
of self-organizing coding models for neuroscience.
Dan, Y., & Poo, M.-M. (2006). Spike timing-dependent plasticity: From synapse to perception.
Physiology Review, 86, 1033–1048.
Dan, Y., Atick, J. J., & Reid, R. C. (1996). Efficient coding of natural scenes in the lateral geniculate
nucleus: Experimental test of a computational theory. Journal of Neuroscience, 16(10), 3351–
3362.204 13 Information, Novelty, and Surprise in Brain Theory
Dayhoff, J. E., & Gerstein, G. L. (1983a). Favored patterns in spike trains. I. Detection. Journal of
Neurophysiology, 49(6), 1334–1348.
Dayhoff, J. E., & Gerstein, G. L. (1983b). Favored patterns in spike trains. II. Application. Journal
of Neurophysiology, 49(6), 1349–1363.
Deadwyler, S. A., & Hampson, R. E. (1997). The significance of neural ensemble codes during
behavior and cognition. Annual Review of Neuroscience, 20, 217–244.
Dean, I., Harper, N. S., & D. McAlpine (2005). Neural population coding of sound level adapts to
stimulus statistics. Nature Neuroscience, 8(12), 1684–1689.
Denève, S. (2008). Bayesian spiking neurons I: Inference. Neural Computation, 20, 91–117.
Dong, D. W., & Atick, J. J. (1995). Statistics of natural time-varying images. Network, 6(3), 345–
358.
Doob, J. L. (1953). Stochastic Processes. Wiley.
Eckhorn, R. (1999). Neural mechanisms of scene segmentation: Recordings from the visual cortex
suggest basic circuits for linking field models. IEEE Transactions on Neural Networks, 10(3),
464–479.
Eckhorn, R., Grüsser, O.-J., Kröller, J., Pellnitz, K., & Pöpel, B. (1976). Efficiency of different neu￾ronal codes: Information transfer calculations for three different neuronal systems. Biological
Cybernetics, 22(1), 49–60.
Edelman, G. M., & Tononi, G. (2000). A universe of consciousness: How matter becomes
imagination. Basic Books.
Engel, A., Fries, P., & Singer, W. (2001). Dynamic predictions: Oscillations and synchrony in
top-down processing. Nature Reviews Neuroscience, 2(10), 704–716.
Field, G. D., & Chichilnisky, E. J. (2007). Information processing in the primate retina: Circuitry
and coding. Annual Review of Neuroscience, 30, 1–30.
Furber, S. B., Brown, G., Bose, J., Cumpstey, J. M., Marshall, P., & Shapiro, J. L. (2007). Sparse
distributed memory using rank-order neural codes. IEEE Transactions on Neural Networks, 18,
648–659.
Georgopoulos, A., Schwartz, A., & Kettner, R. (1986). Neuronal population coding of movement
direction. Science, 4771(233), 1416–1419
Gerstein, G. L. (2004). Searching for significance in spatio-temporal firing patterns. Acta Neurobi￾ologiae Experimentalis Journal, 64, 203–207
Gerstein, G. L., & Aertsen, A. M. (1985). Representation of cooperative firing activity among
simultaneously recorded neurons. Journal of Neurophysiology, 54(6), 1513–1528.
Gerstein, G. L., & Mandelbrot, B. (1964). Random walk models for the spike activity of a single
neuron. Biophysical Journal, 4(1), 41–68.
Gerstner, W., & Kistler, W. M. (2002). Spiking neuron models. Cambridge University Press.
Gerstner, W., Kreiter, A. K., Markram, H., & Herz, A. V. M. (1997). Neural codes: Firing rates
and beyond. Proceedings of the National Academy of Sciences of the United States of America,
94(24), 12740–12741.
Golomb, D., Hertz, J., Panzeri, S., Treves, A., & Richmond, B. (1997). How well can we estimate
the information carried in neuronal responses from limited samples? Neural Computation, 9(3),
649–665.
Grossberg, S. (1999). How does the cerebral cortex work? Learning, attention and grouping by the
laminar circuits of visual cortex. Spatial Vision, 12, 163–186.
Grün, S., Aertsen, A. M. H. J., Abeles, M., Gerstein, G., & Palm, G. (1994a). Behavior￾related neuron group activity in the cortex. In Proceedings 17th Annual Meeting European
Neuroscience Association. Oxford University Press.
Grün, S., Aertsen, A. M. H. J., Abeles, M., Gerstein, G., & Palm, G. (1994b). On the significance of
coincident firing in neuron group activity. In N. Elsner, & H. Breer (Eds.), Sensory transduction
(p. 558). Thieme.
Grün, S., Diesmann, M., & Aertsen, A. (2002a). Unitary events in multiple single-neuron spiking
activity: I. Detection and significance. Neural Computation, 14(1), 43–80.
Grün, S., Diesmann, M., & Aertsen, A. (2002b). Unitary events in multiple single-neuron spiking
activity: II. Nonstationary data. Neural Computation, 14(1), 81–119.References 205
Grün, S., Diesmann, M., Grammont, F., Riehle, A., & Aertsen, A. (1999). Detecting unitary events
without discretization of time. Journal of Neuroscience, 94(1), 121–154.
Grün, S., & Rotter, S. (Eds.) (2010). Analysis of spike trains. Springer.
Gütig, R., Aertsen, A., & Rotter, S. (2002). Statistical significance of coincident spikes: Count￾based versus rate-based statistics. Neural Computation, 14(1), 121–153.
Gutnisky, D. A., & Dragoi, V. (2008). Adaptive coding of visual information in neural populations.
Nature, 452(7184), 220–224.
Guyonneau, R., VanRullen, R., & Thorpe, S. J. (2004). Temporal codes and sparse representations:
A key to understanding rapid processing in the visual system. Journal of Physiology – Paris,
98, 487–497.
Haft, M., & van Hemmen, J. L. (1998). Theory and implementation of infomax filters for the retina.
Network, 9, 39–71.
Hansel, D., & Sompolinsky, H. (1996). Chaos and synchrony in a model of a hypercolumn in visual
cortex. Journal of Computational Neuroscience, 3(1), 7–34.
Hawkins, J., & Blakeslee, S. (2004). On intelligence. Times Books, Henry Holt and Company.
Hebb, D. O. (1949). The organization of behavior: A neuropsychological theory. Wiley.
Hecht-Nielsen, R. (2007). Confabulation theory. The mechanism of thought. Springer.
Holden, A. V. (1976). Models of the stochastic activity of neurons. Springer.
Hosaka, R., Araki, O., & Ikeguchi, T. (2008). STDP provides the substrate for igniting synfire
chains by spatiotemporal input patterns. Neural Computation, 20(2), 415–435.
Hoyer, P. O., & Hyvärinen, A. (2002). A multi-layer sparse coding network learns contour coding
from natural images. Vision Research, 42(12), 1593–1605.
Hyvärinen, A., & Hoyer, P. O. (2001). A two-layer sparse coding model learns simple and complex
cell receptive fields and topography from natural images. Vision Research, 41(18), 2413–2423.
Hyvärinen, A., Hurri, J., & Hoyer, P. O. (2009). Natural image statistics. Springer.
Hyvärinen, A., & Karhunen, J. (2001). Independent component analysis. Wiley.
Ito, J., Lucrezia, E., Palm, G., & Grün, S. (2019), Detection and evaluation of bursts in terms of
novelty and surprise. Mathematical Biosciences and Engineering, 16(6): 6990–7008.
Izhikevich, E. M. (2007). Solving the distal reward problem through linkage of STDP and
dopamine signaling. Cerebral Cortex, 17, 2443–2452.
Izhikevich, E. M., & Desai, N. S. (2003). Relating STDP to BCM. Neural Computation, 15, 1511–
1523.
Johannesma, P. I. M. (1981). Neural representation of sensory stimuli and sensory interpretation of
neural activity. Advanced Physiological Science, 30, 103–125.
Kamimura, R. (2002). Information theoretic neural computation. World Scientific.
Kang, K., & Sompolinsky, H. (2001). Mutual information of population codes and distance
measures in probability space. Physical Review Letter, 86(21), 4958–4961.
Kempter, R., Gerstner, W., & van Hemmen, J. L. (1999). Hebbian learning and spiking neurons.
Physical Review E, 59, 4498–4514.
Kjaer, T. W., Hertz, J. A., & Richmond, B. J. (1994). Decoding cortical neuronal signals: Network
models, information estimation, and spatial tuning. Journal of Computational Neuroscience, 1,
109–139.
Knoblauch, A., & Palm, G. (2004). What is Signal and What is Noise in the Brain? BioSystems,
79, 83–90.
Koepsell, K., & Sommer, F. T. (2008). Information transmission in oscillatory neural activity.
Biological Cybernetics, 99, 403–416.
Koepsell, K., Wang, X., Vaingankar, V., Wei, Y., Wang, Q., Rathbun, D. L., Usrey, W. M., Hirsch,
J. A., & Sommer, F. T. (2009). Retinal oscillations carry visual information to cortex. Frontiers
in Systems Neuroscience, 3, 1–18.
König, P., Engel, A. K., & Singer, W. (1995). Relation between oscillatory activity and long-range
synchronization in cat visual cortex. Proceedings of the National Academy of Sciences of the
United States of America, 92, 290–294.
Kostal, L., Lansky, P., & Rospars, J.-P. (2007). Neuronal coding and spiking randomness. European
Journal of Neuroscience, 26(10), 2693–2701.206 13 Information, Novelty, and Surprise in Brain Theory
Krone, G., Mallot, H., Palm, G., & Schüz, A. (1986). Spatiotemporal receptive fields: A dynamical
model derived from cortical architectonics. Proceedings of the Royal Society of London. Series
B, Biological Sciences, 226(1245), 421–444.
Krüger, J., & Bach, M. (1981). Simultaneous recording with 30 microelectrodes in monkey visual
cortex. Experimental Brain Research, 41(2), 191–194.
Legéndy, C. (2009). Circuits in the brain—a model of shape processing in the primary visual
cortex. Springer.
Legéndy, C. R. (1975). Three principles of brain function and structure. International Journal of
Neuroscience, 6, 237–254.
Legéndy, C. R., & Salcman, M. (1985). Bursts and recurrences of bursts in the spike trains of
spontaneously active striate cortex neurons. Journal of Neurophysiology, 53(4), 926–939.
Letvin, J. Y., Maturana, H. R., McCulloch, W. S., & Pitts, W. H. (1959). What the frog’s eye tells
the frog’s brain. Proceedings of the IRE, 47(11), 1940–1951.
Linsker, R. (1988). Self-organization in a perceptual network. Computer, 21, 105–117.
Linsker, R. (1989a). An application of the principle of maximum information preservation to linear
systems. In D. S. Touretzky (Ed.), Advances in neural information processing systems (Vol. 1,
pp. 186–194). Morgan Kaufmann.
Linsker, R. (1989b). How to generate ordered maps by maximizing the mutual information between
input and output signals. Neural Computation, 1(3), 402–411.
Linsker, R. (1992). Local synaptic learning rules suffice to maximize mutual information in a linear
network. Neural Computation, 4, 691–702.
Linsker, R. (1997). A local learning rule that enables information maximization for arbitrary input
distributions. Neural Computation, 9, 1661–1665.
Lisman, J., & Spruston, N. (2005). Postsynaptic depolarization requirements for LTP and LTD: A
critique of spike timing-dependent plasticity. Nature Neuroscience, 8(7), 839–841.
Loiselle, S., Rouat, J., Pressnitzer, D., & Thorpe, S. J. (2005). Exploration of rank order coding with
spiking neural networks for speech recognition. Proceedings of International Joint Conference
on Neural Networks, 4, 2076–2078.
MacGregor, R. J. (1987). Neural and brain modeling. Academic.
MacKay, D. M., & McCulloch, W. S. (1952). The limiting information capacity of a neuronal link.
Bulletin of Mathematical Biology, 14(2), 127–135.
Mainen, Z. F., & Sejnowski, T. J. (1995). Reliability of spike timing in neocortical neurons.
Science, 268(5216), 1503–1506.
Maldonado, P., Babul, C., Singer, W., Rodriguez, E., Berger, D., & Grün, S. (2008) Synchronization
of neuronal responses in primary visual cortex of monkeys viewing natural images. Journal of
Neurophysiology, 100(3):1523–1532.
Markram, H., Luebke, J., Frotscher, M., & Sakmann, B. (1997). Regulation of synaptic efficacy by
coincidence of postsynaptic APs and EPSPs. Science, 275, 213–215.
Martignon, L., Deco, G., Laskey, K., Diamond, M., Freiwald, W. A., & Vaadia, E. (2000).
Neural coding: Higher-order temporal patterns in the neurostatistics of cell assemblies. Neural
Computation, 12(11), 2621–2653.
Martignon, L., von Hasseln, H., Grün, S., Aertsen, A. M. H. J., & Palm, G. (1995). Detecting
higher-order interactions among the spiking events in a group of neurons. Biological Cybernet￾ics, 73(1), 69–81.
Martignon, L., von Hasseln, H., Grün, S., & Palm, G. (1994). Modelling the interaction in a set
of neurons implicit in their frequency distribution: A possible approach to neural assemblies.
In F. Allocati, C. Musio, & C. Taddei-Ferretti (Eds.), Biocybernetics (Cibernetica Biologica)
(pp. 268–288). Rosenberg & Sellier.
Masquelier, T., Guyonneau, R., & Thorpe, S. (2009). Competitive STDP-based spike pattern
learning. Neural Computation, 21(5), 1259–1276.
Massaro, D. W. (1975). Experimental psychology and human information processing. Rand
McNally & Co.References 207
McClurkin, J. W., Gawne, T. J., Optican, L. M., & Richmond, B. J. (1991). Lateral geniculate
neurons in behaving priimates II. Encoding of visual information in the temporal shape of the
response. Journal of Neurophysiology, 66(3), 794–808.
Miller, J. G. (1962). Information input overload. In M. C. Yovits, G. T. Jacobi, & G. D. Goldstein
(Eds.), Self-organizing systems (pp. 61–78). Spartan Books.
Morrison, A., Aertsen, A., & Diesmann, M. (2007). Spike-timing-dependent plasticity in balanced
random networks. Neural Computation, 19(6), 1437–1467.
Morrison, A., Diesmann, M., & Gerstner, W. (2008). Phenomenological models of synaptic
plasticity based on spike timing. Biological Cybernetics, 98, 459–478.
Nakahara, H., & Amari, S. (2002). Information geometric measure for neural spikes. Neural
Computation, 14, 2269–2316.
Nakahara, H., Amari, S., & Richmond, B. J. (2006). A comparison of descriptive models of a single
spike train by information geometric measure. Neural Computation, 18, 545–568.
Nawrot, M. P., Boucsein, C., Rodriguez Molina, V., Riehle, A., Aertsen, A., & Rotter, S. (2008).
Measurement of variability dynamics in cortical spike trains. Journal of Neuroscience Methods,
169, 374–390.
Nemenman, I., Lewen, G. D., Bialek, W., & de Ruyter van Steveninck, R. R. (2008). Neural coding
of natural stimuli: Information at sub-millisecond resolution. PLoS Computational Biology,
4(3), e1000025.
Nirenberg, S., & Latham, P. (2003). Decoding neural spike trains: How important are correlations?
Proceedings of the National Academy of Science of the United States of America, 100, 7348–
7353.
Nirenberg, S., & Latham, P. (2005). Synergy, redundancy and independence in population codes.
Journal of Neuroscience, 25, 5195–5206.
Optican, L. M., Gawne, T. J., Richmond, B. J., & Joseph, P. J. (1991). Unbiased measures
of transmitted information and channel capacity from multivariate neuronal data. Biological
Cybernetics, 65(5), 305–310.
Optican, L. M., & Richmond, B. J. (1987). Temporal encoding of two-dimensional patterns by
single units in primate inferior temporal cortex. III. Information theoretic analysis. Journal of
Neurophysiology, 57(1), 162–178.
Osborne, L. C., Palmer, S. E., Lisberger, S. G., & Bialek, W. (2008). The neural basis for
combinatorial coding in a cortical population response. Journal of Neuroscience, 28(50),
13522–13531.
Palm, G. (1980). On associative memory. Biological Cybernetics, 36, 167–183.
Palm, G. (1981). Evidence, information and surprise. Biological Cybernetics, 42(1), 57–68.
Palm, G. (1982). Neural assemblies, an alternative approach to artificial intelligence. Springer.
Palm, G. (1985). Information und entropie. In H. Hesse (Ed.), Natur und Wissenschaft. Konkurs￾buch Tübingen.
Palm, G. (1987a). Associative memory and threshold control in neural networks. In J. L. Casti, &
A. Karlqvist (Eds.), Real brains: Artificial minds (pp. 165–179). Elsevier.
Palm, G. (1987b). Computing with neural networks. Science, 235, 1227–1228.
Palm, G. (1992). On the information storage capacity of local learning rules. Neural Computation,
4, 703–711.
Palm, G., Aertsen, A. M. H. J., & Gerstein, G. L. (1988). On the significance of correlations among
neuronal spike trains. Biological Cybernetics, 59(1), 1–11.
Palm, G., & Sommer, F. T. (1992). Information capacity in recurrent McCulloch–Pitts networks
with sparsely coded memory states. Network, 3(2), 177–186.
Panzeri, S., & Schultz, S. R. (2001). A unified approach to the study of temporal, correlational,
and rate coding. Neural Computation, 13(6), 1311–1349.
Panzeri, S., Schultz, S. R., Treves, A., & Rolls, E. T. (1999). Correlations and the encoding of
information in the nervous system. Proceedings of the Royal Society of London Series B;
Biological Science, 266(1423), 1001–1012.
Perkel, D. H., & Bullock, T. H. (1967). Neural coding. Neurosciences Research Program Bulletin,
6(3), 223–344.208 13 Information, Novelty, and Surprise in Brain Theory
Perrinet, L., Samuelides, M., & Thorpe, S. J. (2003). Coding static natural images using spike event
times: Do neurons cooperate? IEEE Transactions on Neural Networks, 15, 1164–1175.
Pfaffelhuber, E. (1972). Learning and information theory. International Journal of Neuroscience,
3, 83.
Pfister, J.-P., & Gerstner, W. (2006). Triplets of spikes in a model of spike timing-dependent
plasticity. The Journal of Neuroscience, 26(38), 9673–9682.
Prut, Y., Vaadia, E., Bergman, H., Haalman, I., Slovin, H., & Abeles, M. (1998). Spatiotemporal
structure of cortical activity: Properties and behavioral relevance. Journal of Neurophysiology,
79(6), 2857–2874.
Quaglio, P., Yegenoglu, A., Torre, E., Endres, D. M., & Grün, S. (2017). Detection and evaluation
of spatio-temporal spike patterns in massively parallel spike train data with SPADE. Frontiers
in Computational Neuroscience, 11, 41.
Quaglio, P., Rostami, V., Torre, E., & Grün, S. (2018). Methods for identification of spike patterns
in massively parallel spike trains. Biological Cybernetics, 112(1), 57–80. https://doi.org/10.
1007/s00422-018-0755-0
Quastler, H. (1956a). Information theory in psychology: Problems and methods. Free Press.
Quastler, H. (1956b). Studies of human channel capacity. In E. Cherry (Ed.), Information Theory,
3rd London Symposium (p. 361). Butterworths.
Riehle, A., Grün, S., Diesmann, M., & Aertsen, A. (1997). Spike synchronization and rate
modulation differentially involved in motor cortical function. Science, 278(5345), 1950–1953.
https://doi.org/10.1126/science.278.5345.1950
Rieke, F., Warland, D., de Ruyter van Steveninck, R., & Bialek, W. (1997). Spikes: Exploring the
neural code. MIT Press.
Roelfsema, P. R., Lamme, V. A. F, & Spekreijse, H. (2004). Synchrony and covariation of firing
rates in the primary visual cortex during contour grouping. Nature Neuroscience, 7(9), 982–
991.
Rolls, E. T., Treves, A., & Tovee, M. J. (1997). The representational capacity of the distributed
encoding of information provided by populations of neurons in primate temporal visual cortex.
Experimental Brain Research, 114(1), 149–162.
Schneideman, E., Bialek, W., & M. J. II. Berry (2003). Synergy, redundancy, and independence in
population codes. Journal of Neuroscience, 23, 11539–11553.
Seriès, P., Latham, P., & Pouget, A. (2004). Tuning curve sharpening for orientation slectivity:
Coding efficiency and the impact of correlations. Nature Neurosience, 7(10), 1129–1135.
Shadlen, M. N., & Newsome, W. T. (1994). Noise, neural codes and cortical organization. Current
Opinion in Neurobiology, 4(4), 569–579.
Shadlen, M. N., & Newsome, W. T. (1998). The variable discharge of cortical neurons: Implications
for connectivity, computation, and information coding. Journal of Neuroscience, 18(10), 3870–
3896.
Shannon, C. E. (1948). A mathematical theory of communication. Bell Systems Technical Journal,
27, 379–423, 623–656.
Shaw, G., & Palm, G. (Eds.) (1988). Brain theory reprint volume. World Scientific.
Smith, M. A., & Kohn, A. (2008). Spatial and temporal scales of neuronal correlation in primary
visual cortex. Journal of Neuroscience, 28(48), 12591–12603.
Softky, W., & Koch, C. (1992). Cortical cells should fire regularly, but do not. Neural Computation,
4, 643–646.
Softky, W. R. (1995). Simple codes versus efficient codes. Current Opinion in Neurobiology, 5(2),
239–247.
Softky, W. R., & Koch, C. (1993). The highly irregular firing of cortical cells is inconsistent with
temporal integration of random EPSPs. Journal of Neuroscience, 13(1), 334–350.
Song, S., Miller, K. D., & Abbott, L. F. (2000). Competitive Hebbian learning through spike￾timing-dependent synaptic plasticity. Nature Neuroscience, 3, 919–926.
Srinivasan, M. V., Laughlin, S. B., & Dubs, A. (1982). Predictive coding: A fresh view of inhibition
in the retina. Proceedings of the Royal Society of London Series B: Biological Science,
216(1205), 427–459.References 209
Stark, E., & Abeles, M. (2009). Unbiased estimation of precise temporal correlations between spike
trains. Journal of Neuroscience Methods, 179(1), 90–100. https://doi.org/10.1016/j.jneumeth.
2008.12.029. http://www.sciencedirect.com/science/article/pii/S0165027009000053
Staude, B., Grün, S., & Rotter, S. (2010). Higher-order correlations in non-stationary parallel spike
trains: Statistical modeling and inference. Frontiers in Computational Neuroscience, 4. https://
doi.org/10.3389/fncom.2010.00016
Stevens, C. F., & Zador, A. M. (1998). Input synchrony and the irregular firing of cortical neurons.
Nature Neuroscience, 1(3), 210–217.
Tetko, I. V., & Villa, A. E. P. (1992). Fast combinitorial methods to estimate the probability of
complex temporal patterns of spikes. Biological Cybernetics, 76, 397–407.
Thorpe, S. J., Guyonneau, R., Guilbaud, N., Allegraud, J.-M., & VanRullen, R. (2004). Spikenet:
Real-time visual processing with one spike per neuron. Neurocomputing, 58–60, 857–864.
Tononi, G., Sporns, O., & Edelman, G. M. (1992). Reentry and the problem of integrating multiple
cortical areas: Simulation of dynamic integration in the visual system. Cerebral Cortex, 2(4),
310–335.
Tononi, G., Sporns, O., & Edelman, G. M. (1994). A measure for brain complexity: Relating
functional segregation and integration in the nervous system. Neurobiology, 91, 5033–5037.
Torre, E., Quaglio, P., Denker, M., Brochier, T., Riehle, A., & Grün, S. (2016) Synchronous spike
patterns in macaque motor cortex during an instructed-delay reach-to-grasp task. Journal of
Neuroscience, 36(32), 8329–8340.
Treves, A., & Panzeri, S. (1995). The upward bias in measures of information derived from limited
data samples. Neural Computation, 7, 399–407.
Tsodyks, M., & Markram, H. (1997). The neural code between neocortical pyramidal neurons
depends on neurotransmitter release probability. Proceedings of the National Academy of
Sciences of the United States of America, 94(2), 719–723.
Tsodyks, M., Uziel, A., & Markram, H. (2000). Synchrony generation in recurrent networks with
frequency-dependent synapses. The Journal of Neuroscience, 20, 1–5.
Uttley, A. M. (1979). Information transmission in the nervous system. Academic.
Vaadia, E., Haalman, I., Abeles, M., Bergman, H., Prut, Y., Slovin, H., & Aertsen, A. M. H. J.
(1995). Dynamics of neuronal interactions in monkey cortex in relation to behavioural events.
Nature, 373, 515–518.
van Essen, D. C., Olshausen, B., Anderson, C. H., & Gallant, J. L. (1991). Pattern recognition,
attention and information bottlenecks in the primate visual system. Proceedings of SPIE
Conference on Visual Information Processing: From Neurons to Chips, 1473, 17–27.
van Rossum, M. C. W., Bi, G. Q., & Turrigiano, G. G. (2000). Stable Hebbian learning from spike
timing-dependent plasticity. The Journal of Neuroscience, 20, 8812–8821.
Wang, X., Hirsch, J. A., & Sommer, F. T. (2010). Recoding of sensory information across the
retinothalamic synapse. The Journal of Neuroscience, 30, 13567–13577.
Wenzel, F. (1961). Über die Erkennungszeit beim Lesen. Biological Cybernetics, 1(1), 32–36.
Yang, H. H., & Amari, S. (1997). Adaptive online learning algorithms for blind separation:
Maximum entropy and minimum mutual information. Neural Computation, 9, 1457–1482.
Yovits, M. C., Jacobi, G. T., & Goldstein, G. D. (Eds.) (1962). Self-organizing systems. In
Proceedings of the Conference on Self-Organizing Systems held on May 22, 23, and 24, 1962
in Chicago, Illinois. Spartan Books.
Zemel, R. S., & Hinton, G. E. (1995). Learning population codes by minimizing description length.
Neural Computation, 7, 549–564.Chapter 14
Surprise from Repetitions
and Combination of Surprises
In this chapter we consider the surprise for a repertoire which represents the interest
in several statistical tests which were performed more or less independently. Then
we consider the surprise obtained from repetitions of the same low-probability
event.
The interest in combining evidence from several statistical tests is not uncommon
in practical situations. One example occurs when several researchers have carried
out statistical studies to evaluate the efficiency of a new drug or a new scientific
hypothesis. In neuroscience, one example is the evaluation of firing coincidence
within a small group of neurons, which was carried out as in the preceding chapter
not only for one combination of “sites” but for several different combinations,
leading to a number more or less independently performed statistical tests on the
same set of neurons. A particular example is the correlation analysis for two neurons
but for different time bins with respect to a stimulus in the so-called JPSTH (Aertsen
et al. 1989). In statistics the kind of analysis that can be carried out in these situations
is sometimes referred to as meta-analysis (Hartung et al. 2008; Hedges and Olkin
1985). The obvious question in such a situation is: “How significant is an effect
which was studied in several instances and was found to be significant in some
cases and insignificant in others?”
For example, one should not be very surprised if 5 out of 100 significance tests
which had been performed were significant at the 5% level. Of course, if one doesn’t
know of the 95 insignificant tests, one still may be impressed by the 5 reported
significant results. This is a severe problem for many practical attempts of meta￾analysis, which is more related to the sociology of science and cannot be solved
mathematically.
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_14
211212 14 Surprise from Repetitions and Combination of Surprises
14.1 Combination of Surprises
In our mathematical analysis of this problem, we assume that a number of mea￾surements or statistical tests X1,...,Xn were performed. These Xi are real valued
random variables, and we assume in our first analysis that they are independent. The
statistical interest is expressed by the descriptions X≥
i (i = 1, . . . , n). The common
interest in all n tests is expressed by the description d = ∩n
i=1 X≥
i , i.e., d(ω) =
∩n
i=1[Xi ≥ Xi(ω)]. Our task is to calculate the surprise of d, which we may call the
combined surprise.
First we observe that
Nd (ω) = − log2
'n
i=1
p[Xi ≥ Xi(ω)]
= −n
i=1
log2 p[Xi ≥ Xi(ω)]
= n
i=1
Yi(ω),
if we define the random variables Yi by
Yi(ω) := − log2 p[Xi ≥ Xi(ω)].
Now S(d(ω)) = − log2 p[Nd ≥ Nd (ω)].
We can calculate this probability, because the random variables Yi –being derived
from Xi– are also independent. We also observe that Yi is monotonically increasing
with Xi.
What is the distribution of Yi? To calculate this, we consider the statement
Yi(ω) ≥ t ⇔ − log2 p[Xi ≥ Xi(ω)] ≥ t
⇔ p[Xi ≥ Xi(ω)] ≤ 2−t
⇔ Xi(ω) ≥ G(t), where G(t) is defined by p[Xi ≥ G(t)] = 2−t
.
Now we define Fi(t) := p[Yi ≥ t] = p[Xi ≥ G(t)] = 2−t for t ≥ 0.
Thus, Yi is continuously distributed on R+ with density fi(t) = −F˙
i(t) = 2−t ·
ln 2.
Now we can compute S(d(ω)) from Nd (ω) = s, because
S(d(ω)) = − log2 p[Nd ≥ s]=− log2 p[
n
i=1
Yi ≥ s].14.1 Combination of Surprises 213
For n = 2 we get
p[Y1 + Y2 ≥ s] =  ∞
0
p[Y2 ≥ s − t]f1(t) dt
=
 s
0
F2(s − t)f1(t)dt +
 ∞
s
f1(t) dt
=
 s
0
2t−s
2−t
(ln 2)dt +
 ∞
s
2−t
(ln 2) dt
= s · 2−s ln 2 + 2−s
.
In the same way, we can compute
p
/
n
i=1
Yi ≥ s
0
= 2−s
#
n−1
i=0
(s · ln 2)i
i!
$
and therefore
S(s) = s − log2
#
n−1
i=0
(s · ln 2)i
i!
$
.
Based on this calculation, there is an easy description how to calculate the combined
surprise (i.e., the combined significance) of n independent statistical tests: First, we
calculate the novelty or “naive” combined surprise s by summing up the individual
novelties. Then we calculate the combined surprise S(s) by the above formula.
A different way of combining surprises from a number of measurements or
statistical tests is to report the best, i.e., most surprising result. This idea is
analyzed in what is called “statistical meta-analysis” (Gurevitch et al. 2018). Again
several statistical tests X1,...,Xn are given, but now we consider the repertoire
α = {[Xi ≥ xi] : i = 1 . . . , n, xi ∈} and we want to calculate the surprise S(α) or
rather the surprise Sα(ω) for a specific outcome ω of all the tests.
The novelty Nα indeed computes the “best result,” i.e. if Xi = xi for all i, then
Nα = maxi=1,...,n(− log2 p[Xi ≥ xi]) = t.
The corresponding surprise is − log2 p[Nα ≥ t], and in order to compute this
probability, we again use the random variables Yi defined by
Yi(ω) = − log2 p[Xi ≥ Xi(ω)].
Now p[Nα ≥ t] = p[∃i : Yi ≥ t] = p(
i([Yi ≥ t])
= 1 − p(
i[Yi < t]) = 1 − p[Yi < t]
n = 1 − (1 − p[Yi ≥ t])
n,
where again p[Yi ≥ t] = 2−t
.
So for Nα = t we obtain
Sα = − log2(1 − (1 − 2−t
)
n) ≈ − log2(1 − exp(−n · 2−t
)).214 14 Surprise from Repetitions and Combination of Surprises
14.2 Surprise of Repetitions
If you take part in a lottery, you would be very surprised if your number
is drawn. Somehow this is true for every number, but every time the lottery
is played, one number is drawn and one particular surprising event comes
true. However, this is not really surprising. Correspondingly, the repertoire
{{x}: x possible outcome of the lottery} has a very high novelty, but zero surprise.
Maybe it is really surprising if the same number is drawn twice in a lottery within a
reasonably short period of time. If the same number is drawn three times, this is of
course much more surprising.
We now want to investigate the surprise obtained from repetitions of improbable
events. To do this we first develop a model for sequences of individually improbable
events, then we will describe a repetition-repertoire and evaluate its novelty and
surprise. The repetition of unlikely events is of interest not only for lotteries but
also in some investigations in brain research where the experimenters looked for
repetitions of unlikely events in the firing of small groups of neurons (Quaglio et al.
2017, 2018; Stella et al. 2019; Torre et al. 2013) (see also the next section).
We are interested in sequences of (random) experiments, where for each time
step, a large number of very unlikely outcomes can happen. We model this by
considering random variables Xt
i for t = 1,...,T and i = 1,...,n, where T and n
are quite large integers, t stands for the time of the repetition of the experiment, and
Xt
i = 1 signifies that the unlikely event number i occurred at time t. For simplicity
we assume that Xt
i ∈ {0, 1} and P[Xt
i = 1] = p for all i and t and that for t = t

the random vectors (Xt
i)i=1,...,n and (Xt
i )i=1,...,n are independent from each other.
Then we count the number of repetitions of each of the unlikely events by
Ni = 
T
t=1
Xt
i .
Of course, we assume that p is very small but n should be large enough such
that p · n is not small; it could even be equal to 1 as in the case of the lottery. In
many cases the events [Xt
i = 1] and [Xt
j = 1] are mutually exclusive for i = j ; in
some cases, we may assume them to be independent. Most often it is something in
between, i.e., p[Xt
i = 1, Xt
j = 1] ≤ p[Xt
i = 1] · p[Xt
j = 1].
This also implies that p[Ni ≥ k,Nj ≥ k] ≤ p[Ni ≥ k] · p[Nj ≥ k] and
p[Ni < k,Nj < k] ≥ p[Ni < k] · p[Nj < k].
In addition, we can often assume that Xs
i and Xt
i are independent1 for s = t. In
such cases it may be possible to compute directly the probabilities that such events
are repeated several times in the sequence, depending on the joint distribution of the
variables (Xt
i)i=1,...,n.
1 This is typically not the case for the analysis of spike patterns.14.2 Surprise of Repetitions 215
If we cannot make such assumptions, it is still possible to compute –at least
approximately– the probabilities for three or more repetitions, if p · T is small,
e.g., ≤ 1
8 (to give a definite value). This can be done by the Poisson approximation.
We now assume that Ni obeys the Poisson distribution (which is approximately
true when the variables xt
i are independent over time, but also in most other practical
cases). This means that
p[Ni = k] = e−λ ·
λk
k!
with λ = p · T.
Intuitively, we may say that we are surprised by the repetition of an unlikely
event, if [Ni = k] occurs for some i and k ≥ 3. The corresponding repertoire is
α = {[Ni ≥ k]: i = 1,...,n, k ≥ 3}.
The novelty of the event [Ni ≥ k] is
− log
⎛
⎝e−λ∞
j=k
λj
j !
⎞
⎠ = − log(qk).
The surprise of this event is − log2 of the probability
p
#
n
i=1
[Ni ≥ k]
$
= 1 − p
#
n
i=1
[Ni < k]
$
≤ 1 − 'n
i=1
p[Ni < k]
= 1 − (1 − qk)
n.
Thus,
S = − log2(1 − (1 − qk)
n).
We have tabulated some values for the novelty and the surprise of the repetition
event [Ni = k] for different values of λ = p · T and n. We show two tables for
k = 3 and k = 4 (Tables 14.1 and 14.2).
Another interesting range is around the parameters of the lottery. Here we assume
that p · n = 1. Here we consider different values of T and n (Table 14.3).216 14 Surprise from Repetitions and Combination of Surprises
Table 14.1 Surprise of three
repetitions [Ni = 3]
1
λ \n 20 100 1000 10,000
8 7.4019 5.0971 1.9624 0.0764
10 8.3389 6.0259 2.8029 0.3455
15 10.0564 7.7371 4.4456 1.4155
20 11.2831 8.9624 5.6534 2.4594
30 13.0198 10.6982 7.3802 4.0972
50 15.2163 12.8944 9.5733 6.2599
100 18.2054 15.8835 12.5617 9.2408
Table 14.2 Surprise of four
repetitions [Ni = 4]
1
λ \n 20 100 1000 10,000
8 12.4071 10.0857 6.7698 3.5071
10 13.666 11.3443 8.0249 4.7278
15 15.9675 13.6456 10.3242 7.0073
20 17.6084 15.2865 11.9647 8.6444
30 19.929 17.6071 14.2852 10.9636
50 22.8615 20.5396 17.2177 13.8958
100 26.85 24.5281 21.2061 17.8842
Table 14.3 Surprise of three repetitions in the lottery [Ni = 3]
T \n 104 105 106 107 108 1010
10 19.196 25.8386 32.4823 39.1262 45.77 59.0577
30 14.4441 21.084 27.7275 34.3713 41.0151 54.3029
100 9.2444 15.8741 22.5167 29.1604 35.8042 49.092
1000 0.3609 5.9332 12.5523 19.1947 25.8385 39.1262
15,000 0.0 0.0 1.233 7.4801 14.118 27.4055
14.3 Surprise of Repetitions of Patterns
Here we consider a special case of the surprise of repetitions which is of interest in
some branches of neuroscience (e.g., Quaglio et al. 2017; Torre et al. 2013) and in
so-called frequent itemset mining (Luna et al. 2019). For example, the state lottery
in Germany, called “Lotto,” draws 6 numbers from 49 every week. This particular
lottery yields different amounts of money to the player depending on how many of
these numbers he got right (for 3, 4, 5, or 6 right numbers).
One can consider particular 6-element sets of numbers between 1 and 49 as
“patterns” with subjectively interesting properties and bet on them; I have discussed
some of this in Sect. 10.1. If somebody has the suspicion that certain patterns
occur preferably, one could use the surprise of repetitions for a statistical test of
this suspicion against the obvious “null hypothesis” that these numbers are drawn
at random (and independently on different days). Now it is possible to consider
not only the surprise of repetitions of complete patterns (which is covered in the
preceding section) but also of partial patterns of size 3, 4, or 5. We will see that
this version of the problem is much more complicated. Of course, a smaller pattern14.3 Surprise of Repetitions of Patterns 217
will need more repetitions to be surprising. To discuss this formally, we introduce
essentially the same model as in the last section, define an appropriate repertoire α,
and try to calculate its novelty and surprise.
Again we consider random variables Xt
i ∈ {0, 1} for t = 1,...,T and i =
1,..., 49, where Xt
i = 1 signifies that the number i was drawn at time t. For
simplicity we assume that P[Xt
i = 1] = p = 6/49 for all i and t and that for
t = t
 the random vectors (Xt
i)i=1,...,n and (Xt
i )i=1,...,n are independent from each
other. Now, however, we assume that for every time t exactly six of the variables Xt
i
are equal to 1.
Since we want to consider different pattern sizes, we now have to construct a
more complex repertoire α. We consider subsets A of L = {1,..., 49} and random
variables Xt
A ∈ {0.1} which are equal to 1 iff Xt
i = 1 for every i ∈ A. Of course,
these variables can only be non-zero for |A| ≤ 6.
Again we count the number of occurrences of subset A by NA = T
t=1 Xt
A and
now we define the repertoire α = {[NA ≥ k] : k ≥ 3, 3 ≤ |A| ≤ 6}.
The most obvious complication compared to the last section is the complex
structure of dependencies between the variables Xt
A for the same time t. This,
however, does not affect the computation of the novelty of the events [NA ≥ k].
To this end, we first have to compute the probabilities p[Xt
A = 1] = ps, which
are the same for all t and for all sets A of the same size s = |A|. Then we again
assume that ps · T is small and use the Poisson approximation as in the preceding
section to determine the probabilities p[NA ≥ k] = qsk for s = |A| and the
corresponding novelty − log2(qsk).
The surprise of the event [NA ≥ k] is − log2 of the probability
p[Nα ≥ − log2(qsk)] = p[∃m ≤ T, B ⊆ L : |B| = r, qrm ≤ qsk]
= p
#

6
r=3
{[NB ≥ m]: |B| = r, qrm ≤ qsk}
$
= p
#
6
r=3
{[NB ≥ mr]: |B| = r}
$
= 1 − p
#

6
r=3
{[NB ≥ mr]: |B| = r}
$
Here we defined mr := min{l : qrl ≤ qsk}.
Up to this point, the calculation had followed the line of the last section, but now
the events in the intersection are clearly far from independent, and it is impossible to
move from the intersection to the product of probabilities or to calculate or at least
approximate this probability in some other practical way. In addition there is the
problem that now we have two parameters s and k that can increase the novelty and
also the surprise of [NA ≥ k]. This situation is similar to the problem of calculating
burst surprise treated in Sect. 13.3.1. In addition, here one could use extensive218 14 Surprise from Repetitions and Combination of Surprises
simulations of the lottery to solve it. Actually, this idea of combining rather
straightforward computation of novelty with the determination of surprise or of
significance probabilities by extensive sampling would also work in more complex
cases, for example, when the probabilities of “drawing” the different numbers in the
lottery or “items” are not the same. This can happen in the applications mentioned
above, i.e., in neuroscience or in frequent itemset mining.
14.4 Technical Comments
The topics of this chapter are of general interest in statistics and have occasionally
been treated in the context of statistical significance (which is closely related to
the surprise defined here), in particular, in the statistics of “rare events” (related to
surprise of repetitions) and in statistical meta-analysis (related to combination of
surprises). The formula derived here for the combination of surprises had already
been found by Fisher, as I was told. I had developed it in the context of the analysis
of joint peri-stimulus-time histograms (JPSTH, Gerstein and Perkel 1969), and it
was part of an integrated computer program for the construction and analysis of
JPSTHs in neurophysiology (Aertsen et al. 1989). The statistical problems involved
here have been treated repeatedly in the physiological literature (e.g., Baker and
Lemon 2000, Abeles and Gat 2001, Grün 2009).
References
Abeles, M., & Gat, I. (2001) Detecting precise firing sequences in experimental data. Journal of
Neuroscience Methods, 107(1–2), 141–154.
Aertsen, A. M. H. J., Gerstein, G. L., Habib, M. K., & Palm, G. (1989). Dynamics of neuronal
firing correlation: Modulation of “effective connectivity”. Journal of Neurophysiology, 61(5),
900–917.
Baker, S. N., & Lemon, R. N. (2000). Precise spatiotemporal repeating patterns in monkey primary
and supplementary motor areas occur at chance levels. Journal of Neurophysiology, 84, 1770–
1780.
Gerstein, G. L., & Perkel, D. H. (1969). Simultaneously recorded trains of action potentials:
analysis and functional interpretation. Science, 164(3881), 828–830.
Grün, S. (2009) Data-driven significance estimation of precise spike correlation. Journal of
Neurophysiology, 101, 1126–1140 (invited review).
Gurevitch, J., Koricheva, J., Nakagawa, S., & Stewart, G. (2018). Meta-analysis and the science of
research synthesis. Nature, 555(7695), 175–182.
Hartung, J., Knapp, G., & Sinha, B. (2008). Statistical meta-analysis with applications. Wiley
Series in Probability and Statistics. Wiley.
Hedges, L., & Olkin, I. (1985). Statistical methods for meta-analysis. Academic.
Luna, J. M., Fournier-Viger, P., & Ventura, S. (2019). Frequent itemset mining: A 25 years review.
Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9(6), e1329.References 219
Quaglio, P., Yegenoglu, A., Torre, E., Endres. D. M., & Grün, S. (2017). Detection and evaluation
of spatio-temporal spike patterns in massively parallel spike train data with SPADE. Frontiers
in Computational Neuroscience, 11, 41.
Quaglio, P., Rostami, V., Torre, E., Grün, S. (2018). Methods for identification of spike patterns in
massively parallel spike trains. Biological Cybernetics, 1–24. https://doi.org/10.1007/s00422-
018-0755-0
Stella, A., Quaglio, P., Torre, E., & Grün, S. (2019). 3d-SPADE: Significance evaluation
of spatio-temporal patterns of various temporal extents. Biosystems, 185, 104022. https://
doi.org/10.1016/j.biosystems.2019.104022, http://www.sciencedirect.com/science/article/pii/
S030326471930053X
Torre, E., Picado-Muiño, D., Denker, M., Borgelt, C., Grün, S. (2013) Statistical evaluation of
synchronous spike patterns extracted by frequent itemset mining. Frontiers in Computational
Neuroscience, 7, 132.Chapter 15
Entropy in Physics
15.1 Classical Entropy
The term entropy was created in statistical mechanics; it is closely connected to
information, and it is this connection that is the theme of this chapter. We first
describe the notion in its historical context of classical statistical mechanics.
In the nineteenth century, the law of conservation of energy was discovered. But
daily experience told the engineers that in the transformation of energy, something
was always lost. An energy efficiency of 100% could not be achieved. Therefore,
it was decided to be impossible to build a perpetuum mobile (of the second kind).
This apparent contradiction to the law of energy conservation had to be explained
in statistical mechanics and phenomenological thermodynamics; it was done more
or less along the following lines: energy is actually not lost; it only becomes less
“usable” or “useful.” The “usefulness” of energy is in this explanation closely
related to its transformability. A form of energy can only be transformed into less
useful forms of energy, but not vice versa. In the models of statistical mechanics, this
usefulness of energy is related to its “orderliness.” For example, the kinetic energy
of a moving car is a relatively well-ordered form of energy, because all molecules
of the car do move (more or less) with the same velocity in the same direction.
When the driver steps on the brakes and the car stops, the kinetic energy of the car is
transformed by friction into heat energy. But this heat energy is basically also kinetic
energy; it is the kinetic energy of the unordered motion of the molecules of the air
surrounding the car. Thus, the energy has been conserved during the deceleration, it
was only transferred from an ordered motion to an unordered motion.
This idea of an overall increase in unordered energy, i.e., a decrease in useful,
ordered energy during energy transformation or energy transfer, was then formu￾lated as the second law of thermodynamics (the first law stating the conservation of
energy). It says that the entropy (of a closed system) cannot decrease. The entropy
H is thus regarded as a measure for disorder, the so-called negentropy −H as a
measure for order, i.e., usefulness of energy.
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_15
221222 15 Entropy in Physics
In statistical mechanics H is defined by Boltzmann’s formula
H = k · ln p,
which resembles our formula for novelty (resp. information). We now want to
discuss in which sense −H can be identified with novelty N and what could be
the corresponding repertoire.
The classical papers in statistical mechanics concerned with Boltzmann’s
H-theorem, which was an attempt to prove the second law, were mainly addressed
to two problems.
Problem 15.1 The problem of the spatial distribution of gas molecules in a
container.
In a container there are N molecules of a (so-called ideal) gas, and one wants to
know the number N1 of molecules on the left half or the number N2 of molecules
on the right half of the container. The problem is to show that for any starting
distribution of molecules the dynamics goes into the direction of bringing N1 toward
N2, i.e., that in the long run there will be equal numbers of molecules in equal
volumes, which means an equalization of pressure. Here the complete state of the
gas is clearly viewed through the description N1, when we consider N1 as a function
N1 :  → N on the state space .
Problem 15.2 The problem of the velocity distribution of gas molecules in a
container.
In a container there are N molecules of a gas. If one divides the range R of
possible velocities of individual molecules into small intervals Ri (i = 1, . . . , n),
one may ask for the number Ni of molecules whose velocities are in the subrange
Ri. In this case the problem is to show an asymptotic development of the state of
the gas toward the so-called Maxwell distribution of velocities. Here one is only
interested in the vector (n1,...,nn), called the velocity distribution, which is again
a function of the state ω ∈ .
The corresponding description can be written as
N1 ∩ N2 ∩ ... ∩ Nn.
It corresponds to a partition of  into the sets
Ak1... kn := {ω ∈ : Ni(ω) = ki for i = 1,...,n}.
In both cases there is an underlying “physical repertoire,” describing those
propositions about the microstate ω ∈  that the physicist is interested in. In both
cases the repertoire is even a partition.
Now we are ready to elaborate the idea of the degradation of order or usefulness
of energy a little further. Think again of the decelerating car. The use of the brake has
transformed an ordered velocity distribution of molecules into a less ordered one.15.1 Classical Entropy 223
Now the idea is that an ordered velocity distribution means a stronger restric￾tion on the (micro-)state ω ∈ . In terms of the velocity-distribution repertoire,
this leads quite naturally to the definition: the velocity distribution k1,...,kn is
more ordered than l1,...,ln, if Ak1... kn is less probable than Al1... ln , i.e., if
p(Ak1... kn ) ≤ p(Al1... ln ).
Thus, order is now defined as improbability with respect to a certain “physical”
repertoire α or the corresponding description d = dα. This idea leads us directly
to Boltzmann’s formula. Again, an additivity requirement is used to motivate the
logarithm, and we obtain the negentropy
−H (ω) = −k ln p(d(ω)) = k(ln 2)Nα(ω).
The positive factor k is determined by thermodynamical considerations. (It is not
dimensionless: k = 1.38 × 10−23 J
K .)
Actually, a more extreme definition of entropy could also be envisaged in this
context, which is based on the use of surprise instead of novelty and produces the
same qualitative prediction, but of course differs quantitatively:
−H (ω) = k(ln 2)Sα(ω).
It may be interesting to consider the quantitative differences of these two different
“entropies” in more detail. However, I am not enough of a physicist to do this.
There is a problem that we did not consider yet. How is the probability p on the
physical state space  determined?
In classical mechanics there is indeed a unique probability measure p on the state
space , the so-called Liouville measure, which is distinguished by the fact that it
is invariant under the classical (Hamiltonian) dynamics. If this Liouville measure
is used for the calculation of the probabilities p for the two problems mentioned
above, it leads to the classical results that have been obtained by Boltzmann through
combinatorial considerations (his so-called complexion probabilities).
In this way, one can for a classical mechanical model of a natural phenomenon
calculate the entropy value for a specific description d(ω) of a state ω ∈ . This
understanding of entropy as a special case of information or novelty, namely, for
a particular description d that reflects the interest of the physicist or engineer, is
slightly different from the conventional modern point of view as expressed, for
example, by Brillouin (1962), but closer to the classical point of view [for more
details, see Palm (1985)]. It is also related to the idea of basing thermodynamical
predictions and even other much broader applications on the principle of maximal
ignorance, maximization of entropy or “infomax” as it is called today. In the exper￾imental and engineering context, this mode has by and large worked reasonably
well up to the present day: empirically, the second law of thermodynamics has
always held.
On the other hand, it has been shown that the second law cannot strictly be true
in the framework of Hamiltonian dynamics (Poincaré 1890). After his own attempts
to prove the second law, Boltzmann has also accepted the counterarguments (a good224 15 Entropy in Physics
collection of historical papers on thermodynamics can be found in Brush (1966))
and finally arrived at the conclusion that the second law does not hold strictly but
only in a probabilistic sense: the increase in entropy that is invariably observed
experimentally is not really sure but only overwhelmingly probable. The various
proofs of the second law that followed Boltzmann’s first attempt have either turned
out to be faulty or have been given for slightly different dynamics (compare also
Palm (1985)). In statistical physics the state-space dynamics considered up to now
is often referred to as “microdynamics” and distinguished from “macrodynamics,”
which is regarded as a coarser approximation to microdynamics. Now the problem
is how to reconcile the reversibility of the underlying Hamiltonian microdynamics
with the apparent irreversibility of most processes in macrodynamics. Dynamical
systems theory provides a framework for the study of this relationship between
micro- and macrodynamics. In particular, chaotic and strongly mixing dynamical
systems have this property of being totally predictable given the knowledge of the
exact state x, but largely unpredictable, if only a coarse state description is known,
that may, for example, be given by a partition α of propositions on x or by a
description of x. This argumentation has even led to the definition of the dynamical
entropy as an invariant for the classification of dynamical systems by Kolmogorov
and Sinai Kolmogorov (1958, 1959). It is the amount of information needed to
determine the next macrostate (after one unit of time) given complete knowledge
about all previous macrostates. For many dynamical systems, this value is nonzero
(for any nontrivial partition α), and this loss of information may actually be the
reason for the second law of thermodynamics.
This indeed has some implication for the consequences of the second law
of thermodynamics for the long-term development of the universe. In the early
days of thermodynamics (and even today), many people believed that the whole
universe would have to evolve into a state of maximal entropy (“Wärmetod”). Our
interpretation of the second law as dependent on an engineering, in particular human
view on the observable world (or even just on any kind of coarse graining of
the underlying physical microstates), implies that it only holds for the physicist’s
or the engineer’s repertoire. It is in fact not a law of physics but a law that describes
the relation between the physical world and the (human) observer. Thus, it does not
necessarily hold for the whole universe in the long run, but only in these parts that
we observe, for comparatively “short” periods of time (maybe centuries) with high
probability.
15.2 Modern Entropies and the Second Law
In “modern” physics, the concept of entropy has many facets, ranging from the
very theoretical to the very practical, almost engineering. This whole range is
probably necessary to connect the phenomenological concept of entropy in classical
thermodynamics to the purely theoretical concepts that are used to derive certain
theoretical distributions in statistical mechanics or to classify even more abstract15.2 Modern Entropies and the Second Law 225
dynamical systems in ergodic theory (Shields 1973; Smorodinsky 1971; Walters
1982).
The more theoretical entropies are easily described from the point of view of
information theory:
1. The entropy used in statistical mechanics for the derivation of theoretical
distributions is (apart from the sign) what we have called the information gain
in Definition 12.6, defined in terms of a density function f usually with respect
to the Lebesgue measure on the phase-space  ⊆ Rn.
2. The dynamical entropy used in ergodic theory for the classification of dynamical
systems is essentially what we have called the information rate in Definition 7.6
on page 98.
Both entropies are not used to prove (or at least understand theoretically) the
second law of thermodynamics from the underlying (micro-)dynamics. In fact, it
has turned out to be very hard to arrive at a proper theoretical understanding of the
second law at all. After all, the second law is an empirical law.
In the following, we shall try to consider another version of physical entropy
which is closer to the phenomenological one and to the classical considerations
around the second law of thermodynamics. In order to formulate such an entropy,
we have to rely on a theoretical model of the dynamics that are studied in
thermodynamical experiments. Classically, one considers Hamiltonian dynamics on
a so-called state space . We shall consider a dynamical system, i.e., (, 	, p; ϕ),
where (, 	, p) is a probability space and ϕ :  →  is a mapping that describes
the evolution of a state x in time: on one time unit x moves to ϕ(x) and then to ϕ2(x)
and so forth. The probability p should be the unique probability on  that makes
sense for the physicist (see above) and that is invariant under the motion mapping ϕ.
In this setting the physicist is normally not capable of knowing exactly the
point x in state space (for a gas in a box, it would mean knowing all positions
and velocities of all molecules). Instead, he knows the values of certain macro￾observables, which are certain real valued functions f1,...,fn on the state
space  (e.g., pressure, temperature, energy). And he wants to describe or even
predict the evolution of the system over time in terms of these macro-observables.
In our language, this means that the physicist looks at the system through a
certain description d, which is given by d := f  for a single observable f
(see Definition 3.16 on page 42), or by dm := f 
1 ∩ ... ∩ f 
m for m observables
that he observes simultaneously.
Now it is straightforward to define the entropy of a state x by
H (x) = −(N ◦ d)(x)
as above.226 15 Entropy in Physics
There is one additional point that should be mentioned: usually the macro￾observables are not instantaneous measurements on the system but rather time
averages. If g :  → R is an (instantaneous) observable, we define the time
average as
gn(x) :=
1
n
n−1
i=0
g(ϕi
(x)).
We may now observe that in ergodic dynamical systems, the averages gn
converge to a constant function G at least in probability (. . . ). This means that for
any  > 0 we can get
p[|gn − G| > ] < 
for sufficiently large n.
For the set M := {x ∈ : |gn(x) − G| ≤ }, we have p(M) > 1 − . Next, we
consider the observable f = gn and a fixed measurement accuracy δ > 0.
Then
p(f δ (x)) = p{y ∈  : |gn(y) − gn(x)| < δ} ≥ p(M) > 1 − 
for x ∈ M and  < δ
2 .
If the average novelty N (gδ ) is finite for the original instantaneous observable g,
one can easily find a constant c such that N ◦ gδ
n(x) < c for almost every x ∈ .
Therefore, the average novelty of f δ = gδ
n can be estimated as
N (f δ
) ≤ −(1 − ) · log(1 − ) +  · c,
which goes to zero as  → 0.
This means that for sufficiently large n the entropy for the average observable
f = gn will be almost zero for any fixed measuring accuracy δ.
If we start the system in an initial condition where we know that (f ≈ r), this
corresponds to the negative entropy value
H (f ≈ r) = log p[|f − r| < δ].
If we measure f again, averaging over some large number n of time steps, we will
almost always find f ≈ G and the average entropy we will get for this measurement
will be close to zero since
p[|f − G| < δ] > 1 − .
In practice “large n” often means times below 1 s, and so this shows that we have
to expect the entropy to increase in time on the scale of seconds.15.3 The Second Law in Terms of Information Gain 227
This argument is a theoretical underpinning for the second law of thermo￾dynamics, and its flavor of vagueness combined with triviality is typical for all
such arguments. Still, it is in my opinion the best way of showing that entropy
is a quantity defined by the macro-observables for the system that is increasing
or constant over time and therefore makes certain developments (the entropy￾increasing ones) irreversible. This phenomenon has been observed in connection
with heat engines, where the theoretical description of the state space  would be a
product of several spaces i describing several subsystems, and the dynamics could
be changed by “connecting” or disconnecting” some of these subsystems. In any
case, such a system can be led through irreversible processes, and for certain proce￾dural sequences of connecting and disconnecting the phenomenological formula for
entropy shows this irreversibility, but without reference to the state-space models
of statistical mechanics, and before the times of Boltzmann’s famous H-theorem,
there has been no general argument for the second law.
15.3 The Second Law in Terms of Information Gain
In this section, we finally present a different, fairly recent approach to proving the
second law. Again, it is based on a practically reasonable, but generally invalid,
approximation. Let us look a little more closely at the evolution of the entropy for
“small” times t, i.e., for times that are small compared to the relaxation time.
To simplify the picture, assume that our observables f1,...,fn are discretized,
i.e., R(fi) is a finite set of real numbers for i = 1,...,m. Then the whole vector
f = (f1,...,fn) has a finite range in Rn and we may write
R(f ) = {r1,...,rN } ⊆ Rn.
Then we can define pi = p[f = ri
].
Starting with a certain observation [f ∈ B] (e.g., [f = rj ] or [f3 = s,f4 = s
]),
we can in principle calculate the probability distribution pk of the whole vector f
of observable k time steps ahead:
pk
i = p[f ◦ ϕk = ri
|f ∈ B] = 
r∈B
p[f ◦ ϕk = ri
|f = r] · p[f = r]
p[f ∈ B]
So we obtain pk
i given [f ∈ B] as a weighted sum of the probabilities pk
i given
[f = r], summed over all r ∈ B. The weights are again a probability vector
p0
i =
⎧
⎪⎨
⎪⎩
p[f = ri
]
p[f ∈ B]
for ri ∈ B and
0 otherwise.228 15 Entropy in Physics
This means that we can describe the probability distribution pk given any observa￾tion (f ∈ B) by means of a transition matrix
Mk
ij := p([f ◦ ϕk = rj ]|[f = ri
]),
i.e., we can describe our original system in terms of a Markov process.
With this notation, we may now look at the development of the entropy over time.
At our initial time t = 0, we have the observation [f = ri] and the entropy
H0 = H ([f = ri
]) = −N ([f = ri
]).
At later times t = k we can only consider an average entropy
Hk = −E(Np(f
◦ ϕk)|f = ri
).
If we introduce the probability vector q with q := p[f =ri], we can write this
Hk = −Nqp(f
◦ ϕk)
= 
N
j=1
p[f ◦ ϕk = rj |f = ri
] · log p[f = rj ]
= 
N
j=1
Mk
ij · log pj .
We see that this is just the negative of the subjective information between the
probability vectors p and 
Mk
ij 
j=1,...,N
, i.e.,
Hk = −S(ei · Mk, p),
where ei = (0,..., 0, 1, 0,..., 0) is the i-th unit vector. Here we use the notation
of Definition 4.1 in the following way: two probability vectors p, q ∈ [0, 1]
n
considered as distributions for a random variable X with R(X) = {1,...,n}. Then
S(q, p) := Spq (X) and G(q, p) := Gpq (X).
It should be clear that we can only speak of probabilities for the values of the
observables k time steps ahead (k ≥ 1), even when we initially (at t = 0) knew the
values of all the observables fi.
This will usually be the case when
1. The observables (f1,...,fn) = f do not determine the state x ∈ 
2. The time of one time step is not too small.15.3 The Second Law in Terms of Information Gain 229
Furthermore, in this argument, the time of one time step should also not be too
large, i.e., small compared to the relaxation time, because otherwise the observables
f1,...,fn will be almost constant already after one single time step (as in the
argument of the last section).
With time steps in this medium range, one may try a further simplification. This
simplification will, however, change the dynamics of the system, i.e., whereas the
transition matrices Mk defined above still describe the same dynamical model, only
viewed through a coarse repertoire (namely, through the description of states x in
terms of the observables (f1,...,fn)), we will now define a kind of averaged or
coarse-grained dynamics. We simply assume that the transition probabilities are
given by a first-order Markov process, i.e., defining
Nij := M1
ij = p[f ◦ ϕ = rj |f = ri
],
we assume that Mk
ij = (Nij )k.
With this simplified dynamics, we can investigate mathematically the time
evolution of the probability vectors pk given any initial observation [f ∈ B]
(compare Schlögl (1966)).
First, we observe that the vector p = (p1,...,pn) defined above satisfies
(pN)j = n
i=1
piNij = n
i=1
p[f = ri
]p([f ◦ ϕ = rj |f = ri
])
= p([f ◦ ϕ = rj ]) = p[f = rj ] = pj ,
since p is ϕ-invariant, i.e. p is N-invariant.
Defining pk as the probability vector p0 ◦ Nk for an initial vector p0 that
corresponds to the initial observation [f ∈ B] by
p0
i = p[f = ri
]
p[f ∈ B]
for ri ∈ B and p0
i = 0; otherwise, it is well known from ergodic theory
(e.g., Walters (1982)) that pk → p, if p is the only N-invariant probability vector y.
Thus, I(pk ) → I(p) and S(pk , p) → S(p, p) = I(p). Since S(pk , p) ≥ I(pk )
and I(p) will usually be small for typical thermodynamical measurements f , one
can hope that S(pk , p) will decrease toward I(p) and thus Hk = −S(pk , p) will
increase. Unfortunately, this is not the case in general. As an example, one may
consider the start vector p0 = ei, where pi ≥ pj for every j = 1,...,N. Then
usually Hi < H0.230 15 Entropy in Physics
In this situation, Schlögl (1966) has suggested the information gain −G(pk , p) of
pk with respect to p, instead of the average entropy Hk = −S(pk , p), as a measure
for entropy. In the situation mentioned above, then obviously
G(pk , p) = S(pk , p) − I(pk ) → 0
and it can indeed be shown that G(pk , p) decreases toward 0 in general.
Proposition 15.1
0 ≤ G(pk+1, p) ≤ G(pk, p)
for every k ∈ N. 
Proof The positivity of G(q, p) has been shown in Proposition 4.1. The main
inequality is again shown by means of the inequality log x ≤ x − 1 and some
straightforward calculations using pN = p. 
15.4 Technical Comments
This chapter does not contain new technical material. It tries to give some reasonable
arguments for the second law of thermodynamics which certainly have been given
before by several authors during the last 120 years. The argument leading to
Proposition 15.1 is based on ideas of Schloegl (1966; 1971a; 1971b). The slightly
unorthodox viewpoint provided by our theory of novelty or surprise of repertoires
leads to the impression that the second law is not really a law of physics that
describes the objective dynamics of the physical world but rather a law that describes
the dynamics of our knowledge about the state of the world as described by the
repertoire that is available to us.
References
Brillouin, L. (1962). Science and information theory (2nd ed.). Academic.
Brush, S. G. (1966). Irreversible processes: Kinetic theory, vol. 2. Pergamon Press.
Kolmogorov, A. N. (1958). A new invariant for transitive dynamical systems. Doklady Akademii
nauk SSSR, 119, 861–864.
Kolmogorov, A. N. (1959). Entropy per unit time as a metric invariant of automorphism. Doklady
Akademii nauk SSSR, 124, 754–755.
Palm, G. (1985). Information und Entropie. In H. Hesse (Ed.), Natur und Wissenschaft. Konkurs￾buch Tübingen.
Poincaré, H. (1890). Sur le problème des trois corps et les équations de la dynamique. Acta
Matematica XIII, 13, 1–270.References 231
Schlögl, F. (1966). Zur statistischen Theorie der Entropieproduktion in nicht abgeschlossenen
Systemen. Zeitschrift für Physik A, 191(1), 81–90.
Schlögl, F. (1971a). Fluctuations in thermodynamic non equilibrium states. Zeitschrift für Physik
A, 244, 199–205.
Schlögl, F. (1971b). On stability of steady states. Zeitschrift für Physik A, 243(4), 303–310.
Shields, P. (1973). The theory of Bernoulli Shifts. The University of Chicago Press.
Smorodinsky, M. (1971). Ergodic theory, entropy (Vol. 214). Springer.
Walters, P. (1982). An introduction to ergodic theory. Springer.Part VI
Generalized Information TheoryChapter 16
Order- and Lattice-Structures
In this part, we condense the new mathematical ideas and structures that have been
introduced so far into a mathematical theory, which can be put into the framework
of lattice theory. In the next chapter, we will obtain a better understanding of the
order structure (defined in Definition 11.3) on the set of all covers. In anticipation
of this, we now introduce a number of basic concepts concerning order and lattices
(Birkhoff 1967).
16.1 Definitions and Properties
An order relation is a binary relation with certain properties on a set S. It is usually
written as x ≤ y for x,y ∈ S. As for any relation r, also an order relation can be
described by the set R of all pairs (x, y) of elements from S, for which the relation
holds, i.e., by R = {(x, y): x,y ∈ S and xry}.
The following nomenclature is most commonly used for simple relations
including order relations.
Definition 16.1 A relation r on a set S is called
(i) Reflexive, if xrx for every x ∈ S.
(ii) Symmetric, if xry implies yrx for every x,y ∈ S.
(iii) Transitive, if xry and yrz implies xrz for every x,y,z ∈ S.
(iv) Antisymmetric, if xry and yrx implies x = y for every x,y ∈ S.
(v) Strictly antisymmetric, if xry implies not yrx for every x,y ∈ S.
(vi) Connecting, if xry or yrx for every x,y ∈ S.
(vii) Irreflexive, if not xrx for every x ∈ S.
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_16
235236 16 Order- and Lattice-Structures
Definition 16.2
(i) Any reflexive, symmetric, and transitive relation is called an equivalence. For
equivalences, we often use the symbol ∼.
(ii) Any reflexive, antisymmetric, and transitive relation is called a partial order
(p.o.) relation.
(iii) Any reflexive and transitive relation is called an ordering or a preorder.
(iv) Any partial order relation that is connecting is called a total order relation.
(v) Any transitive, irreflexive relation is called a strict order relation.
Proposition 16.1 Every strict order relation is strictly antisymmetric.
Proof Assuming xry and yrx implies xrx which contradicts irreflexivity. 
Definition 16.3 Let ∼ be an arbitrary equivalence and x ∈ S. The set x :=
{y ∈ S : x ∼ y} is called the equivalence class of x.
Proposition 16.2 Let ∼ be an arbitrary equivalence.
(i) The mapping d : S → P(S) which maps x into d(x) = x is a description.
(ii) The set α = {d(x): x ∈ S} is a partition of S.
Proof
(i) Clearly x ∈ x.
(ii) If z ∈ x ∩ y, then x ∼ z and y ∼ z and thus x ∼ y, i.e., x = y.

Usually, S has much more members than α, because many elements x of S have
the same d(x). More exactly: d(x) = d(y) if and only if x ∼ y. The set α is
also referred to as α = S/ ∼ (reads: S modulo tilde). One says that every x ∈ S is
identified with its equivalence class x and writes x = y/ ∼ if and only if x = y.
Given a partition α we can define an equivalence ∼ by x ∼ y if and only if x and y
are in the same element of α (or equivalently dα(x) = dα(y)). Thus, there is a one￾to-one correspondence between partitions and equivalence relations. Furthermore,
any mapping f : S → T (T any set) gives rise to an equivalence relation ∼f ,
namely, x ∼f y if and only if f (x) = f (y). The corresponding partition is given by
the complete description f
.
This chapter is a brief introduction to elementary order theory and provides
a slight generalization for some concepts from the most frequently considered
partially ordered sets (p.o.-sets) to what we call orderings.
Before we proceed with a more detailed discussion of order and orderings, here
are some examples.
Example 16.1
(1) The usual ≤ relation on Z (the integer numbers) is a total order relation.
(2) The relation ≤ on Zn defined by (x1,...,xn) ≤ (y1,...,yn) if and only if
xi ≤ i yi for every i = 1,...,n is a partial order relation.16.1 Definitions and Properties 237
(3) The usual ⊆ relation between sets is a partial-order relation on P(M) (the sets
of all subsets of M).
(4) The relation ≤ between real functions, defined by f ≤ g if and only if
f (x) ≤ g(x) for every x ∈ X is a partial-order relation on the set of all real
valued functions f : X → R on a set X.
(5) The relation ⊆ between descriptions as introduced in Chap. 3 is also a partial￾order relation. 
If a partial order is not a total-order relation, it can happen that two elements x
and y are uncomparable, i.e., neither x ≤ y nor x ≥ y. In these cases we need a
better definition for the “minimum” or the “maximum” of the two. The more general
terms are the “meet” x ∧y and the “join” x ∨y of x and y, which are used in analogy
to sets, where the meet x ∧ y is the intersection and the join x ∨ y is the union of the
two sets x and y.
Intuitively, x ∧y is the largest element, that is, ≤ x and ≤ y and conversely x ∨y
is the smallest element that is ≥ x and ≥ y. We will define this in the even more
general settings of orderings (Definition 16.5).
Definition 16.4 Let ≤ be an ordering on a set S. We define two relations < and ∼
as follows:
(i) x<y if and only if x ≤ y and not y ≤ x.
(ii) x ∼ y if and only if x ≤ y and y ≤ x.
Proposition 16.3 Let ≤ be an ordering on S.
(i) The relation < is a strict order relation.
(ii) The relation ∼ is an equivalence.
(iii) The relation ≤ on S/ ∼ is a partial order relation.
Proof
(i) < is obviously irreflexive. We have to show transitivity: if x<y and y<z,
then x<z. To show that not z ≤ x, we assume z ≤ x and with x ≤ y obtain
z ≤ y, which contradicts y<z.
(ii) ∼ is obviously reflexive, transitive, and symmetric.
(iii) We have to show that ≤ is antisymmetric on the set S/∼ of equivalence classes.
If x ≤ y and y ≤ x, then x ∼ y by definition. Thus, x and y belong to the
same equivalence class, i.e., x = y in S/∼.

Definition 16.5 Let ≤ be an ordering on a set S and M ⊆ S.
(i) x is called minimal in M, if x ∈ M and for every y ∈ M, y ≤ x implies
y ∼ x.
(ii) x is called maximal in M, if x ∈ M and for every y ∈ M, x ≤ y implies
y ∼ x.
(iii) x is called a lower bound for M, if x ≤ y for every y ∈ M.238 16 Order- and Lattice-Structures
(iv) x is called an upper bound for M, if y ≤ x for every y ∈ M.
(v) x is called a largest element of M (also written x = max(M)), if x ∈ M and
x is an upper bound for M.
(vi) x is called a smallest element of M (also written x = min(M)), if x ∈ M and
x is a lower bound for M.
(vii) If the set U of all upper bounds of M has a smallest element x, this is called
the smallest upper bound (s.u.b.) of M and written as x = sup(M) = ∨(M)
(supremum).
(viii) If the set L of all lower bounds of M has a largest element x, this is called
the largest lower bound (l.l.b.) of M and written as x = inf(M) = ∧ (M)
(infimum).
(ix) If M = {x,y} then sup(M) is written as x ∨ y and called the join and inf(M)
is written as x ∧ y and called the meet of x and y.
Remarks
• Largest and smallest elements are unique up to equivalence.
• Two minimal elements of a set M are either equivalent or incomparable.
• If all minimal elements of a set M are equivalent, then they all are smallest
elements.
• If two minimal elements of a set M are not equivalent, then the set M has no
smallest elements.
• For a partial order, both the largest and smallest elements of M are unique.
• For a total order, every maximal element is the largest element of M, and every
minimal element is the smallest element of M.
Proposition 16.4 A smallest element of a set M is a largest lower bound of M and
a largest element of M is a smallest upper bound of M.
Proof Let x be the smallest element of M, i.e., x ∈ M and y ≥ x for every y ∈ M.
Thus, x is a lower bound for M. If z is a lower bound for M, then z ≤ x ∈ M. Thus
x is the largest lower bound for M. 
Remark A set M without a smallest (or largest) element may still have a l.l.b. (or
s.u.b.) outside M.
Proposition 16.5 Let ≤ be an ordering on S and M ⊆ S.
(i) x is minimal in M if and only if there is no y ∈ M satisfying y<x.
(ii) x is maximal in M if and only if there is no y ∈ M satisfying x<y.
Proof This follows directly from the definition. 
The following (well-known) example shows that even for a total-order relation,
an infinite set M which has an upper bound may neither have a maximal element,
nor a largest element, nor a smallest upper bound. Consider the usual ordering ≤ on
Q (the rational numbers) and M = {x ∈ Q: 0 ≤ x ≤ √2}.16.1 Definitions and Properties 239
Proposition 16.6 Let ≤ be an ordering on S and M ⊆ S. If M is finite, then M has
maximal and minimal elements.
Proof We can start with any element x ∈ M. Either x is already maximal or there
is an y ∈ M with y ≥ x. Now we proceed with y until we arrive at a maximal
element. The procedure ends in finitely many steps because M is finite. 
This does not imply that M has a largest or a smallest element, not even that it
has an upper or a lower bound.
For example, let  = {1,..., 6} and S = (P() \ , ⊆). Consider
M = {{1},{3},{5},{1, 2, 3},{2, 3, 4},{3, 4, 5},
{4, 5, 6},{1, 3},{2, 4},{3, 5},{4, 6},{2, 6}}
The minimal elements of M are
{1},{3},{5},{2, 4},{4, 6},{2, 6}.
Thus, M has no smallest element, but it has a lower bound in S, namely, ∅.
The maximal elements of M are
{1, 2, 3},{2, 3, 4},{3, 4, 5},{4, 5, 6},{2, 6}.
Note that {2, 6} is both maximal and minimal in M. M has no largest element and
no upper bound in S (because  /∈ S).
In the following, we want to investigate the algebraic structure of the operations
“join” ∨ and “meet” ∧ introduced in Definition 16.5.(ix). Since these are in general
not uniquely defined, but only up to equivalence, it is more convenient to consider
the equivalence classes S/ ∼ for an ordering ≤ on S. On S/ ∼, i.e., if we identify
equivalent elements of S, the ordering ≤ becomes a partial order relation and join
and meet are unique (if they exist).
Thus, in the following we will always assume that we have a partial order relation
≤ on a set S.
Proposition 16.7 Let (Mi)i∈I be arbitrary many subsets of S and let xi = ∨Mi for
every i ∈ I . Then ∨{xi : i ∈ I }=∨(∪i∈IMi). Let yi = ∧Mi for every i ∈ I . Then
∧{yi : i ∈ I }=∧(∪i∈I Mi).
The equation implies that the left-hand side exists if the right-hand side exists
and vice versa.240 16 Order- and Lattice-Structures
Proof
y is an upper bound of {xi : i ∈ I } ⇔ y ≥ xi for every i ∈ I
⇔ y ≥ x for every x ∈ Mi for every i ∈ I
⇔ y ≥ x for every x ∈ ∪i∈I Mi
⇔ y is an upper bound of ∪i∈I Mi.
Thus, {xi : i ∈ I } and ∪i∈IMi have the same upper bounds and therefore the same
smallest upper bounds. 
Definition 16.6 Let ≤ be a partial-order relation on a set S. If for any two x,y ∈ S
both x ∧ y and x ∨ y exists, then (S, ≤, ∧, ∨) is called a lattice.
Proposition 16.8 Let (S, ≤, ∧, ∨) be a lattice. Then the following is true:
(i) x ∧ y = x ⇔ x ∨ y = y ⇔ x ≤ y.
(ii) (x ∧ y) ∨ x = x ∧ (y ∨ x) = x for any x,y ∈ S.
(iii) x ∧ y = y ∧ x and x ∨ y = y ∨ x for any x,y ∈ S.
(iv) x ∧ x = x and x ∨ x = x for any x ∈ S.
Proof
(i) If x ≤ y, then x is the smallest element of {x,y}. Therefore, x is the largest
lower bound of {x,y}, i.e., x = x ∧ y. Similarly, y is the largest element of
{x,y} and therefore y = x ∨ y. Conversely, x = x ∧ y ≤ y and y = x ∨ y ≥ x.
(ii) x ≥ x ∧ y and therefore (by (i)) x = (x ∧ y) ∨ x, x ≤ x ∨ y and therefore
(by (i)) x = (x ∨ y) ∧ x.
(iii) obvious from the definition
(iv) x is the smallest and largest element of {x,x}={x}.

From each lattice operation ∧ and ∨, we can retrieve the partial order ≤ by
defining x ≤ y if and only if x ∧ y = x (⇔ x ∨ y = y).
Proposition 16.9 Both lattice operations are associative and commutative, and for
any finite set M = {x1,...,xn} ⊆ S we have ∨M = x1 ∨ x2 ∨ ... ∨ xn (any order,
any bracketing) and ∧M = x1 ∧ x2 ∧ ... ∧ xn (any order, any bracketing).
Proof By induction on n:
n = 1 : ∨{x1} = x1 = x1 ∨ x1.
n = 2 : ∨{x1, x2} = x1 ∨ x2 = x2 ∨ x1 by definition.
n → n + 1 : Let M = {x1,...,xn+1} and Mi = M \ {xi}. We have to show
∨M = (x1 ∨ ... ∨ ✚xi ∨ ... ∨ xn+1) ∨ xi, where the first
bracket contains all xj (j = 1,...,n + 1) except xi in any
order and bracketing. By the induction assumption (x1 ∨ ... ∨16.1 Definitions and Properties 241
✚xi ∨ ... ∨ xn+1) = ∨Mi. Since M = Mi ∪ {xi} we get from
Proposition 16.7
∨M = ∨{∨Mi, ∨{xi}} = (∨Mi) ∨ xi.

Remark From this it follows that a finite lattice S always has a maximal element,
namely, ∨S, and a minimal element, namely, ∧S. Usually, these are called 1 and 0,
respectively, i.e., 1 = ∨S and 0 = ∧S.
0 and 1 are also used to name the smallest and largest element of a poset.
Proposition 16.10 Let (S, ≤, ∧, ∨) be a lattice, M ⊆ S and x ∈ S. Then
(i) (∨M) ∧ x ≥ ∨{y ∧ x : y ∈ M} if the two suprema exist.1
(ii) (∧M) ∨ x ≤ ∧{y ∨ x : y ∈ M} if the two infima exist.
Proof Let z = ∨M, then z ≥ y for every y ∈ M. Thus, z ∧ x ≥ y ∧ x (because
every lower bound for {x,y} is also a lower bound for {x,z}). So z ∧ x is an upper
bound for {y ∧ x : y ∈ M}, and therefore z ∧ x ≥ ∨{y ∧ x : y ∈ M}. The second
proof works the same way. 
In general, the two reversed inequalities do not hold. When they hold (for finite
sets), the lattice is called distributive.
Definition 16.7 A lattice (S, ∧, ∨) is called distributive, if
(i) x ∧ (y ∨ z) = (x ∧ y) ∨ (x ∧ z).
(ii) x ∨ (y ∧ z) = (x ∨ y) ∧ (x ∨ z) for all x,y,z ∈ S.
Example 16.2 S = {∅,{1},{2},{3},{1, 2, 3}} with ⊆ is not distributive. For
x = {1}, y = {2}, and z = {3} we get
(x ∧ y) ∨ z =∅∨ z = z, but (x ∨ z) ∧ (y ∨ z) = {1, 2, 3}∧{1, 2, 3}={1, 2, 3},
and
(x ∨ y) ∧ z = {1, 2, 3} ∧ z = z, but (x ∧ z) ∨ (y ∧ z) =∅∨∅=∅.
Proposition 16.11 Let ≤ be a partial order on S and S finite.
(i) If x ∧ y exists for any x,y ∈ S then (S, ≤) is a lattice.
(ii) If x ∨ y exists for any x,y ∈ S then (S, ≤) is a lattice.
1 All suprema exist if M is finite, but the inequalities also hold for infinite sets.242 16 Order- and Lattice-Structures
Proof If x ∧ y exists, then ∧M exists for any finite set M ⊆ S. Given x and y in S,
we have to determine x∨y. Let M be the set of all upper bounds of x and y. M is not
empty because y ∈ M. Since S is finite, M is finite and we claim that x ∨ y = ∧M.
Indeed, ∧M ≥ x, since z ≥ x for every z ∈ M. Similarly ∧M ≥ y. If z ≥ x and
z ≥ y, then z ∈ M and therefore z ≥ ∧M. 
16.2 The Lattice D of Descriptions
Let (, 	, p) be a probability space. In this section, we consider the partially
ordered set (D, ⊆) of descriptions (Definition 3.3).
Proposition 16.12 (D, ⊆) is a lattice with join c ∪ d and meet c ∩ d.
Proof
(i) c ⊆ d means c(ω) ⊆ d(ω) for every ω ∈ . Thus, ⊆ is a partial order just like
the set relation ⊆.
(ii) (c∪d)(ω) ⊇ c(ω)for every ω ∈ . So c∪d ⊇ c and also c∪d ⊇ d. Conversely,
b(ω) ⊇ c(ω) and b(ω) ⊇ d(ω) for every ω ∈  implies b(ω) ⊇ c(ω) ∪ d(ω)
for every ω ∈ .
(iii) Similar to (ii).

Proposition 16.13 (D, ⊆) has a smallest and largest element.
(i) The smallest element is called 0 and defined by 0(ω) = {ω} for ω ∈ .
(ii) The largest element is called 1 and defined by 1(ω) =  for ω ∈ .
Proof obvious. 
Proposition 16.14 (D, ⊆) is a distributive lattice.
Proof For example, (b ∩ (c ∪ d))(ω) = b(ω) ∩ (c(ω) ∪ d(ω)) = (b(ω) ∩ c(ω)) ∪
(b(ω) ∩ d(ω)) = ((b ∩ c) ∪ (b ∩ d))(ω) for every ω ∈ . 
Definition 16.8
(i) In a lattice (L, ≤) with smallest and largest element 0 and 1, the complement
ac of an element a ∈ L is defined by the relations a ∧ ac = 0 and a ∨ ac = 1.
(ii) A distributive lattice with 0 and 1 in which every element has a complement is
called a Boolean algebra.
Proposition 16.15 (D, ⊆) is a Boolean algebra with dc as defined in Defini￾tion 3.10.
Proof For every ω ∈ , we have d(ω) ∪ dc(ω) =  and d(ω) ∩ dc(ω) = {ω}. Reference 243
Proposition 16.16
(i) c ⊆ d implies N (c) ≥ N (d).
(ii) I(c ∩ d) ≤ I(c) + I(d).
Proof See Propositions 3.3 and 4.5. 
In Chap. 3 we already considered the order properties of descriptions. There it
appeared more natural to consider the inverse ordering ⊇. Of course, (D, ⊇) is also
a distributive lattice, and Proposition 16.16 shows that N is monotonic and I is
subadditive on (D, ⊇). We already know from Chapter 3, that I is not monotonic
and N not subadditive.
16.3 Technical Comments
This chapter puts together known results from the theory of order and lattices
(Birkhoff 1967) which will be needed in the next two chapters. Some of the concepts
are presented in a slightly more general fashion than usual, because we cannot
assume our orderings to be antisymmetric. Section 16.2 contains a first application
of these ideas, recasting the results of Chaps. 3 and 4 in the lattice D of descriptions.
Reference
Birkhoff, G. (1967). Lattice theory (3rd ed.). American Mathematical Society.Chapter 17
Three Orderings on Repertoires
The set of all repertoires has an interesting structure, when we “look at” a repertoire
α in terms of its proper descriptions D(α). This means that we should consider
two repertoires to be essentially the same if they have the same proper descriptions,
or we should say that α is more refined than β if the proper descriptions in α are
contained in those in β.
This idea leads to two almost equally reasonable definitions of an ordering of
repertoires, which we will call ≤1 and ≤2. We will analyze these two orderings and
a third one in more detail.
17.1 Definition and Basic Properties
Definition 17.1 For two repertoires α and β, we define α ≤1 β by the following
condition:
For any description c ∈ D(α) there is a description d ∈ D(β) such that d ⊆ c.
Definition 17.2 For two repertoires α and β, we define α ≤2 β by the following
condition:
For any description d ∈ D(β) there is a description c ∈ D(α) such that d ⊆ c.
Definition 17.3 For two covers α and β, we define α ≤3 β by the following
condition: For any B ∈ β there is an A ∈ α such that B ⊆ A.
Definition 17.1 leads to a very natural ordering of repertoires which is almost
the same as set containment. Actually, it coincides with the relation already defined
in Definition 11.3. Definition 17.2 leads to a more complicated ordering which is
almost the same as the one used in (Palm 1976a,b) in connection with topological
entropy. Both definitions obviously coincide for tight covers. Definition 17.3 leads
to a third ordering which is known in the literature (Adler et al. 1965; Walters 1982).
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_17
245246 17 Three Orderings on Repertoires
a b g
d z
h k l
Fig. 17.1 The repertoires α, β, γ , δ = α ∪ β,  = β ∪ γ , ζ = γ ∪ α, η = α ∪ {}, κ = η ∪ β,
and λ =  ∪ {} on  = {1, 2, 3, 4, 5, 6} illustrate possible relationships between the orderings
≤1, ≤2, and ≤3 (see text)
The example repertoires shown in Fig. 17.1 illustrate possible relationships
between the three orderings ≤1, ≤2, and ≤3. For many covers the orderings lead
to the same reasonable results. For example, all orderings agree that cover α is finer
than cover β, i.e., β ≤i α and α i β for i = 1, 2, 3. Similarly, all orderings agree
that α and γ are incomparable, i.e., α i γ and γ i α, and that η and κ are
equivalent, i.e., η ≤i κ and κ ≤i η.
However, there are also many cases where the orderings will lead to different
results. Actually, if a cover is finer than another cover with respect to one of the
three orderings, it may be that the two covers are incomparable with respect to any
of the two remaining orderings. For example, in Fig. 17.1 we have β ≤1 ζ , but
β j ζ and ζ j β for j = 2, 3. Similarly, we have  ≤2 η, but  j η and η j 
for j = 1, 3, and we have η ≤3 γ , but η j γ and γ j η for j = 1, 2.
There are even many examples where the comparison of one ordering contradicts
the result of any of the two remaining orderings. For example, in Fig. 17.1 we have
β ≤1 λ, but λ ≤j β for j = 2, 3. Similarly, we have  ≤2 β, but β ≤j  for
j = 1, 3. And finally, we have β ≤3 κ, but κ ≤j β for j = 1, 2.
The most basic differences between the three orderings can be understood in
terms of their monotonicity properties with respect to adding a set.17.1 Definition and Basic Properties 247
Proposition 17.1 Let α be a repertoire, and A ⊆ . Then the following statements
are true:
(i) α ≤1 α ∪ {A}
(ii) α ∪ {A} ≤3 α
Proof
(i) Let a ∈ D(α). We define a second description a ∈ D(α ∪ {A}) as follows: for
any ω ∈  we choose a
(ω) = A if A ⊆ a(ω), and otherwise a
(ω) = a(ω).
Obviously a ⊆ a.
(ii) Obvious: For any A ∈ α we have also A ∈ α ∪ {A} and A ⊇ A
.

For the ordering ≤2 no order can be predicted, when adding an element A to a
repertoire α. For example, with the covers in Fig. 17.1: β 2  := β ∪ γ and also
δ := β ∪ α 2 β.
Remark Note that the definition of ≤2 differs from the definition of ≤3 only
in demanding proper choices for ≤2, but not for ≤3. Analogously, it would be
possible to define a further ordering ≤4 from the definition of ≤1 by leaving out
the restriction to proper choices. Thus, for two repertoires α and β, we could define
α ≤4 β by the following condition: For any A ∈ α and any x ∈ A there is B ∈ β
such that x ∈ B ⊆ A. However, it is easy to see that this definition would be
equivalent to the definition of ≤1.
In the following we will see that the three orderings ≤1, ≤2, and ≤3 define
equivalence classes on the set of all repertoires.
Proposition 17.2 The relations ≤1, ≤2, and ≤3 are reflexive and transitive on
R, i.e.,
(i) α ≤i α
(ii) α ≤i β and β ≤i γ implies α ≤i γ
for repertoires α, β, γ , and i = 1, 2, 3.
Proof
(i) Obvious since A ⊆ A for any choice A := c(ω) from α and ω ∈ .
(ii) ≤1: Let a ∈ D(α). α ≤1 β implies the existence of b ∈ D(β) such that b ⊆ a.
With β ≤1 γ , it follows the existence of c ∈ D(γ ) such that c ⊆ b ⊆ a.
≤2: Let c ∈ D(γ ). β ≤2 γ implies the existence of b ∈ D(β) such that c ⊆ b.
With α ≤2 β, it follows the existence of a ∈ D(α) such that c ⊆ b ⊆ a.
≤3: Let C ∈ γ . β ≤3 γ implies the existence of B ∈ β such that C ⊆ B. With
α ≤3 β, it follows the existence of A ∈ α such that C ⊆ B ⊆ A.

In fact, ≤1, ≤2, and ≤3 are not even partial orders, because they are not
antisymmetric, i.e., α ≤i β and β ≤i α do not imply that α = β. An example248 17 Three Orderings on Repertoires
for this situation is given by the covers δ and  shown in Fig. 17.1. Indeed, δ ≤i 
and  ≤i δ for i = 1, 2, 3, but δ = .
This situation naturally leads to the definition of equivalence relations ∼i for
i = 1, 2, 3, which are defined by ordering in both directions.
Definition 17.4 For i = 1, 2, 3 we define α ∼i β by α ≤i β and β ≤i α. We also
define α ∼ β by D(α) = D(β).
Obviously ∼1, ∼2, ∼3, and ∼ are equivalence relations, i.e., reflexive, sym￾metric, and transitive. In the following section, we will determine the equivalence
classes of the relations ∼i. Note that ∼1=∼ has already been defined and analyzed
in Chap. 10.
17.2 Equivalence Relations Defined by the Orderings
In the following, we will find that the equivalence relations for orderings ≤1 and ≤2
are the same and that the corresponding equivalence classes are the repertoires that
have the same proper choices, i.e., ∼1 =∼2 =∼. For ∼3 it turns out that two covers
α and β are in the same equivalence class if they have the same flattenings αf and
βf (see Definition 10.13 on p. 134).
Proposition 17.3 The following are equivalent for two repertoires α and β.
(i) α ∼1 β
(ii) α ∼2 β
(iii) D(α) = D(β)
Proof It is obvious that (iii) ⇒ (i) and (iii) ⇒ (ii).
(i) ⇒ (iii): Let a ∈ D(α), then there is a b ∈ D(β) with b ⊆ a. Conversely, for
b there is an a ∈ D(α) with a ⊆ b ⊆ a. Since a and a are both
minimal descriptions, a = a and therefore a = b. Thus, a ∈ D(β).
Exchanging a and b in this argument shows D(β) ⊆ D(α).
(ii) ⇒ (iii): is shown in the same way.

This indeed shows that ∼1=∼2=∼ (of Definition 10.9). The following proposi￾tion further characterizes the equivalence classes for the orderings ≤1 and ≤2.
Proposition 17.4 For any two repertoires α and β, we have α ∼ β if and only if
αc ⊆ β ⊆ α∪.
Proof See Definition 17.4, Proposition 17.3, and Proposition 10.5. 
Hence, we see that the equivalence relation ∼ essentially disregards unions of
elements of a cover α, because these are never used in proper descriptions. The
following proposition determines the equivalence classes for the third ordering ≤3.17.2 Equivalence Relations Defined by the Orderings 249
Proposition 17.5 For any two finite covers α and β, we have α ∼3 β if and only if
α and β contain the same maximal sets, i.e., αf = βf .
Proof
“⇒”: Let α ≤3 β, β ≤3 α, and A ∈ αf ⊆ α. We have to show A ∈ βf . Since
β ≤3 α there must be B ∈ β with B ⊇ A. Let us assume B = A. Since
α ≤3 β there must be an A ∈ α with A ⊇ B ⊃ A which contradicts the
maximality of A in α. In the same way, it can be shown that for any B ∈ βf
we have B ∈ αf .
“⇐”: Let αf = βf . For any A ∈ α there is an A ∈ αf with A ⊇ A. Thus, also
A ∈ β ⊇ αf which shows α ≤3 β. β ≤3 α can be shown in the same way.

In order to further characterize the equivalence classes for the ordering ≤3,
we introduce the following definition.
Definition 17.5 For any cover α we define
α⊆ := {A = ∅: ∃A ∈ α : A ⊆ A
}
Thus, α⊆ contains all nonempty subsets of the propositions in α. The following
proposition shows that the equivalence relation ∼3 essentially disregards subsets of
propositions of a cover α.
Proposition 17.6 For any two covers α and β, we have α ≤3 β if and only if
α⊆ ⊇ β⊆.
Proof
“⇒” Let B ∈ β⊆. Then there is B ∈ β with B ⊇ B and A ∈ α with A ⊇ B
(since α ≤3 β). Thus, B ∈ α⊆.
“⇐” Let B ∈ β. Then B ∈ β⊆ ⊆ α⊆. Thus, there is an A ∈ α with A ⊇ B.

Proposition 17.7 For any two covers α and β, we have α ∼3 β, if and only if
α ⊆ β⊆ and β ⊆ α⊆.
Proof We first observe that α ⊆ β⊆ is equivalent to α⊆ ⊆ β⊆. With this
observation, Proposition 17.7 immediately follows from Proposition 17.6. 
Proposition 17.8 For any two finite covers α and β, we have α ∼3 β, if and only if
αf ⊆ β ⊆ α⊆.250 17 Three Orderings on Repertoires
Proof
“⇐”: αf ⊆ β ⊆ α⊆ clearly implies αf = βf .
“⇒”: Assume αf = βf but not αf ⊆ β ⊆ α⊆. Then A ∈ αf with A /∈ β or B ∈ β
with B /∈ α⊆. The first case implies A /∈ βf which contradicts αf = βf .
Similarly, the second case implies B /∈ αf which also contradicts αf = βf .

We can now consider equivalent covers as equal. For the orderings ≤1 and ≤2,
we take the set R(	) of all repertoires on  modulo the equivalence relation ∼ and
call it R. Mathematicians write R := R(	)/ ∼ for this. Similarly, we can write
C := C(	)/ ∼3 for the set C(	) of all covers of  and the third ordering ≤3. Now
the three orderings are antisymmetric modulo the respective equivalence relations.
Proposition 17.9 For any two finite covers the following are equivalent:
(i) α ∼3 β
(ii) α⊆ = β⊆
(iii) αf = βf
(iv) αf ⊆ β ⊆ α⊆
Proof The proof is immediate from Propositions 17.7 and 17.8. 
As we will see in the next section, the set R of all repertoires turns out to be a
lattice for ordering ≤1, but not for ordering ≤2. Similarly, the set C of all covers
turns out to be a lattice for ordering ≤3. A lattice is quite a strong order structure.
This means that now the two orderings ≤i (i = 1, 3) are antisymmetric and for any
two covers α and β we have a join α ∨i β and a meet α ∧i β as defined in the last
section.
17.3 The Joins and Meets for the Orderings
We first will determine the joins and meets for the first ordering relation ≤1. For this,
it turns out to be useful to characterize ≤1 in terms of set inclusion by application
of Proposition 17.4.
Proposition 17.10 For any two repertoires α and β, we have α ≤1 β if and only if
α ⊆ β∪.
Proof
“⇐”: Clear by αc ⊆ α ⊆ β∪ (Proposition 17.4).
“⇒”: Assume α ≤1 β and α  β∪. Then there is an A ∈ α with A /∈ β∪. Therefore,
there is an ω1 ∈ A− {B ∈ β∪ : B ⊆ A}. Choose a description a ∈ D(α) with
a(ω1) = A ⊆ A, where A ∈ αc. Since there is no B ∈ β∪ with B ⊆ A,17.3 The Joins and Meets for the Orderings 251
we have b(ω1)  a(ω1) for any description b ∈ D(β), which contradicts
α ≤1 β.

This shows that ≤1 is (modulo ∼) the same as ⊆ on R (and that it coincides with
Definition 10.9).
Proposition 17.11 For any two repertoires α and β, the following statements are
true:
(i) α ∪ β ≥1 α and α ∪ β ≥1 β.
(ii) If a repertoire γ satisfies γ ≥1 α and γ ≥1 β, then γ ≥1 α ∪ β.
(iii) α∪ ∩ β∪ ≤1 α and α∪ ∩ β∪ ≤1 β.
(iv) If a repertoire γ satisfies γ ≤1 α and γ ≤1 β, then γ ≤1 α∪ ∩ β∪.
Proof The statements follow easily from Proposition 17.10:
(i) Clearly (α ∪ β)∪ ⊇ α and (α ∪ β)∪ ⊇ β.
(ii) γ ≥1 α and γ ≥1 β is equivalent to γ∪ ⊇ α and γ∪ ⊇ β. Thus, clearly
γ∪ ⊇ α ∪ β.
(iii) Clearly α∪ ∩ β∪ ⊆ α∪ and α∪ ∩ β∪ ⊆ β∪.
(iv) γ ≤1 α and γ ≤1 β is equivalent to γ ⊆ α∪ and γ ⊆ β∪. This clearly implies
γ ⊆ (α∪ ∩ β∪)∪.

This means that the join ∨1 of α and β is simply α ∪ β and the meet ∧1 of α and
β is α∪ ∩ β∪. It also implies that the join and the meet are uniquely defined up to
equivalence. Indeed, if another cover δ besides α ∪ β would satisfy (i) and (ii), then
by (ii) δ ≥1 α ∪ β and also α ∪ β ≥1 δ and so δ ∼ α ∪ β.
In the following we show that ≤2 is not a lattice on R. For this, the following
proposition turns out to be helpful.
Proposition 17.12 Let α, β, and γ be repertories with γ ≥2 α and γ ≥2 β.
Then for any c ∈ D(γ ) and ω ∈ ,
c(ω) ⊆ {A ∩ B : ∃a ∈ D(α), b ∈ D(β): A = a(ω), B = b(ω)}.
Proof Let c ∈ D(γ ). By definition of ≤2 there are a ∈ D(α) and b ∈ D(β) with
a ⊇ c and b ⊇ c, and thus c(ω) ⊆ a(ω) ∩ b(ω). 
It is sufficient to give an example of two covers α and β such that it is not
possible to find a unique smallest cover which is larger than both α and β. By
Remark 17.1 this shows that the join does not always exist. Proposition 17.4 implies
that also the meet does not always exist. Figure 17.2 illustrates such an example.
Here  = {1, 2, 3, 4, 5, 6} and with A1 = {1, 2}, A1 = {1, 3, 4}, and B = {1, 3, 5},
the repertoires are α = {A1, A2, } and β = {B,}. Let γ be a cover with γ ≥2 α
and γ ≥2 β. With Proposition 17.12 we can constrain the proper choices of γ for252 17 Three Orderings on Repertoires
135
246
a
135
246
b
135
246
g 1
135
246
g 2
<2
Fig. 17.2 Repertoires illustrating that ≤2 is not a lattice. Both γ1 and γ2 are minimally larger than
α and β. Thus, there is no unique meet for ≤2
each ω ∈ . Thus, for any c ∈ D(γ ) and ω ∈ , there must be c(ω) ⊆ Cω for
C1 = (A1 ∪ A2) ∩ B1 = {1, 3},
C2 = A1 ∩  = {1, 2},
C3 = A2 ∩ B1 = {1, 3},
C4 = A2 ∩  = {1, 3, 4},
C5 = B1 ∩  = {1, 3, 5},
C6 = .
By requiring γ to be as small (i.e., coarse) as possible, we have to avoid choosing
subsets of the Cω. Thus, for ω = 6, 5, 4, 3 we will have to choose c(ω) = Cω for
any c ∈ D(γ ). However, for ω = 1, 2 we cannot properly choose c(1) = C1 and
c(2) = C2. Then there would be also c ∈ D(γ ) with c
(1) = C2 and thus b  c for
any b ∈ D(β) which would contradict β ≤2 γ . Instead we have two choices: either
we take c(2) = C2 which implies choosing c(1) = {1} or, alternatively, we can take
c(2) = {2} which allows c(1) = C1. Thus, we remain with two incomparable and
presumedly smallest covers
γ1 = {C6, C5, C4, C3, C2,{1}} and γ2 = {C6, C5, C4, C3,{2}}
that both are larger than α and β with respect to ≤2 (see Fig. 17.2). This suggests
that there is no unique meet for ≤2. In order to finally prove this supposition,
assume that there is a cover δ lying between α, β, and γ1, γ2, i.e., satisfying δ ≥2 α,17.3 The Joins and Meets for the Orderings 253
δ ≥2 β, and δ ≤2 γ1, δ ≤2 γ2. As we will see, this leads to a contradiction if we try
to find a proper choice d ∈ D(δ) for ω = 1, 2: for any c1 ∈ D(γ1), c2 ∈ D(γ2), we
have to choose
c1(1) = {1}, c1(2) = {1, 2} and c2(1) = {1, 3}, c2(2) = {2}.
Since δ ≤2 γ1, γ2 there must be d ∈ D(δ) with
d(1) ⊇ {1, 3},d(2) ⊇ {1, 2}.
On the other hand, since δ ≥2 α, β we also have to require with Proposition 17.12
that
d(1) ⊆ C1 = {1, 3},d(2) ⊆ C2 = {1, 2}.
Thus, d(1) = {1, 3} and d(2) = {1, 2}, and therefore δ ⊇ {{1, 2},{1, 3}}. But then
there is another proper choice d ∈ D(δ) with d
(1) = {1, 2} which contradicts
δ ≥2 β since b(1) = {1, 3, 5} for any b ∈ D(β). This argumentation not only
proves that with respect to ≤2, there is no unique meet for α, β, but also that there is
no unique join for γ1 and γ2. In any case, C or R is not a lattice with respect to ≤2.
In the following we show that ≤3 is indeed a lattice ordering on C. For any two
covers α and β, there is a unique meet α ∪ β and a unique join α⊆ ∩ β⊆ ∼3 α · β.
Proposition 17.13 For any two covers α and β, the following statements are true:
(i) α⊆ ∩ β⊆ = (α · β)⊆
(ii) α⊆ ∩ β⊆ ≥3 α and α⊆ ∩ β⊆ ≥3 β.
(iii) If a cover γ satisfies γ ≥3 α and γ ≥3 β, then γ ≥3 α⊆ ∩ β⊆.
(iv) (α ∪ β) ≤3 α and (α ∪ β) ≤3 β.
(v) If a cover γ satisfies γ ≤3 α and γ ≤3 β, then γ ≤3 α ∪ β.
Proof
(i), (ii) Let M ∈ α⊆ ∩ β⊆. Then there must be M ∈ α and M ∈ β with M ⊇ M
and M ⊇ M, which proves (ii). Thus, M ⊆ M ∩ M ∈ α · β which
proves (i).
(iii) Assume γ ≥3 α, γ ≥3 β, but γ 3 α⊆ ∩ β⊆. Then there is C ∈ γ such
that M  C for all M ∈ α⊆ ∩ β⊆. This is a contradiction because there are
A ∈ α and B ∈ β with A ⊃ C and B ⊃ C, and thus C ∈ α⊆ ∩ β⊆.
(iv) α ≥3 α ∪ β with Proposition 17.1.
(v) Let M ∈ α ∪ β, and e.g., M ∈ α. Then there is a C ∈ γ with M ⊆ C.

In the following we will write ≤ for the ordering ≤1, α ∧β for the meet w.r.t. ≤1,
α ∨ β for the join with respect to ≤1, and  for the ordering ≤3, α  β for the meet,
α  β for the join w.r.t ≤3.254 17 Three Orderings on Repertoires
In Definition 11.4 we defined the sets C(	), R(	), T(	), F(	), and P(	). To
this we add Ff (	) := {α ∈ F(	): α finite}. These sets can be made into lattices
by considering the proper orderings and using the proper equivalence relations. This
leads to the definition of five lattices.1
Definition 17.6 For a probability space (, 	, p), we define
C := C(	)/ ∼3, R := R(	)/ ∼, T := T(	), P := P(	)/ ∼, F := F(	).
(i) (C, ) is the set of covers
(ii) (R, ≤) is the set of repertoires
(iii) (T, ≤) is the set of templates
(iv) (P, ≤) is the set of partitions
(v) (F, ) is the set of flat covers
(vi) (Ff , ) is the set of finite flat covers
In view of the equivalence relations ∼1 and ∼3 and their characterization in
Propositions 17.4 and 17.7, we can identify (R, ≤) with the p.o.-set of clean
repertoires, and in addition to (C, ) modulo ∼3 we are led to consider the p.o.-
set (F, ) of flat covers, which is the same as (C, ) for finite .
This means that only for C and only for infinite , we have to work with
equivalence classes. In fact, if we consider the set Cf ⊆ C of finite covers, we
obtain Cf = Ff by Proposition 17.9. The elements of the other four lattices can
be essentially identified with particularly nice representatives in their equivalence
classes, i.e., R = clean repertoires, T = templates, P = partitions, and F = flat
covers.
Proposition 17.14
(i) (R, ∧, ∨) is a distributive lattice.
(ii) (C, , ) is a distributive lattice.
Proof We have seen that on R modulo ∼1 every repertoire α can be represented
by α∪ and with this ≤ becomes ⊆, i.e., α ⊆ β if and only if α∪ ⊆ β∪. Thus,
α ∧ β ∼ α∪ ∩ β∪ and α ∨ β ∼ α∪ ∪ β∪ ∼ (α ∪ β)∪. Similarly, on C modulo ∼3
every repertoire α can be represented by α⊆ and with this  becomes ⊇, i.e., α  β
if and only if α⊆ ⊇ β⊆. Thus, α  β ∼ α⊆ ∪ β⊆ and α  β ∼ α⊆ ∩ β⊆. With these
remarks the proofs of the distributive laws reduce to those for set intersection and
union. For example,
α ∧ (β ∨ γ ) = α∪ ∩ (β∪ ∪ γ∪) = (α∪ ∩ β∪) ∪ (α∪ ∩ γ∪) = (α ∧ β) ∨ (α ∧ γ )

1 The lattice property of C, and R follows from Propositions 17.11 and 17.13. For T, P, and Ff it
will be shown in the next section. F is not a lattice, in general.17.4 The Orderings on Templates and Flat Covers 255
17.4 The Orderings on Templates and Flat Covers
Now we consider two interesting subsets of R and C, namely, the set T of templates
(i.e., tight, clean repertoires) and the set F of flat covers.
Since tight repertoires have exactly one proper description, it is clear that the two
orderings ≤1 and ≤2 coincide on tight repertoires. It is now necessary to determine
the join and meet of tight repertoires again because the ordinary join of α and β
need not be tight and we are now looking for the smallest tight repertoire that is
larger than α and β.
T = clean and tight repertoires
Since flat covers have only proper descriptions, it is clear that ≤2 and ≤3
coincide on F and the ordering is described by curved symbols . Again we have to
determine the join  and meet  on F. It turns out that the join does not generally
exist for arbitrary flat covers. However, it does exist for finite flat covers.
Proposition 17.15 For two flat covers α, β ∈ F we have
α  β = (α ∪ β)f .
Proof follows from Propositions 17.13 and 17.7. 
Proposition 17.16 For two finite flat covers α, β ∈ Ff we have
α  β = (α · β)f .
Proof follows from Propositions 17.13 and 17.7. 
Example 17.1 Let  = [−1, 1] and consider
α = {[−1, 0),[0, 1]},
β =
	−1
n − 1
2
, 1 − 1
n

: n = 2, 3,...

.
In this case α  β does not exist.
Proposition 17.17
(i) On T the orderings ≤1 and ≤2 coincide.
(ii) On F the orderings ≤2 and ≤3 coincide.
(iii) On P all three orderings coincide.256 17 Three Orderings on Repertoires
Proof
(i) Is clear because templates allow only one proper description.
(ii) Is clear because for flat covers every description is proper, or every element is
minimal.
(iii) Is clear because P = T ∩ F.

Proposition 17.18 For two repertoires α and β in T, the following holds:
(i) The join of α and β in T is α · β.
(ii) The meet of α and β in T is α∪ ∩ β∪.
Proof
(i) Clearly α · β is tight, if α is tight and β is tight. Clearly dα·β = dα ∩ dβ; thus,
α · β ≤ α and α · β ≤ β.
If γ ≤ α and γ ≤ β, then dγ ⊆ dα and dγ ⊆ dβ. Thus, dγ ⊆ dα∩dβ = dα·β,
i.e., γ ≤ α · β.
(ii) α∪ ∩ β∪ is tight: Let B,C ∈ α∪ ∩ β∪.
B,C ∈ α∪ ⇒ B ∩ C ∈ α∪, B,C ∈ β∪ ⇒ B ∩ C ∈ β∪.
Thus, B ∩ C ∈ α∪ ∩ β∪. This shows that α∪ ∩ β∪ is ∩-stable.

Proposition 17.19 For tight covers α, β, and γ , the following are equivalent:
(i) γ is the join of α and β in T.
(ii) γ ∼ α · β.
(iii) γ ∼ (α ∪ β)∩.
(iv) dγ = dα ∩ dβ.
Proof
(i) ⇔ (ii): follows from Proposition 17.18.
(ii) ⇔ (iii) : since α · β ∼ (α ∪ β)∩.
(ii) ⇒ (iv): since dα·β = dα ∩ dβ.
(iv) ⇒ (ii): γ ∼ R(dγ ) = R(dα ∩ dβ = R(dα·β) ∼ α · β.

It is surprising that the two “opposite” (set theoretical) orderings ≤1 and ≤3
coincide on partitions P = F ∩ T.References 257
17.5 Technical Comments
The three orderings introduced in this chapter coincide for partitions; two of them
have appeared in the literature in quite different contexts. Definition 17.3 has been
introduced in the study of topological entropy by Adler et al. (1965); Goodwyn
(1969); Goodman (1971), which is a counterpart for measure-based entropy in
ergodic theory (see Hopf 1937; Keynes and Robertson 1969; Krieger 1970; Palm
1976a,b). The definition is needed as a generalization of the common refinement of
partitions. In this context also, the definition of α · β as the natural maximum of
α and β is introduced. The problem of finding the natural minimum, however, is
not considered, because it is not needed for the definition of topological entropy.
Definition 17.1, which is the the most natural in the context of this book (see
Proposition 17.10 and Definition 11.3), is in some sense opposite to Definition 17.3
and relies on the notion of description but could as well be formulated with
elements of α and β. It has been first introduced in Palm (1976a,b) and used for
the generalization of information. The lattice structures provided by these orderings
have not yet been discovered and studied in the literature.
17.6 Exercises
(1) Let  = {0, 1}. Determine the lattices C, R,T, P, and F.
(2) Let  = [0, 1] with 	 the Borel sets and p the equidistribution. Let αx =
{A ∈ 	 : p(A) < x}, βx = {A ∈ 	 : p(A) ≤ x}, and γx = {(a, b): b−a<x}.
What is the order relation between αx and αy , between αx and γy, and between
βx and γy for arbitrary x and y?
(3) (, 	, p) as in Exercise (2). What are the minimal and the maximal elements
in C, R,T, P, and F?
(4) Let  = {1, 2, 3, 4} and consider C, R, T, P, and F. Is there always a smallest
and a largest element in these p.o. sets? If yes, what is it? If these are left out,
what are the minimal and maximal elements in the remaining p.o. sets?
(5) Is σ (Definition 10.6) always the second smallest element in the lattices C, R,
T, P, and F?
References
Adler, R. L., Konheim, A. G., & McAndrew, M. H. (1965). Topological entropy. Transactions of
the American Mathematical Society, 114, 309–319.
Goodwyn, L. W. (1969). Topological entropy bounds measure-theoretic entropy. Proceedings of
the American Mathematical Society, 23, 679–688.
Goodman, T. N. T. (1971). Relating topological entropy and measure entropy. Bulletin of the
London Mathematical Society, 3, 176–180.258 17 Three Orderings on Repertoires
Hopf, E. (1937). Ergodentheorie. Springer. Reprinted by Chelsea Publishing Co., New York
edition.
Keynes, H. B., & Robertson, J. (1969). Generators for topological entropy and expansivness.
Mathematical Systems Theory, 3, 51–59.
Krieger, W. (1970). On entropy and generators of measure-preserving transformations. Transac￾tions of the American Mathematical Society, 149, 453–464.
Palm, G. (1976a). A common generalization of topological and measure-theoretic entropy.
Astérisque, 40, 159–165.
Palm, G. (1976b). Entropie und Erzeuer in dynamischen Verbänden. Z. Wahrscheinlichkeitstheorie
verw. Geb., 36, 27–45.
Walters, P. (1982). An introduction to ergodic theory. Springer.Chapter 18
Information Theory on Lattices of Covers
Classical information theory considers the information I on the lattice (P, ∧, ∨)
of partitions. The development of information theory rests essentially on three
properties of information:
1. Monotonicity, i.e., α ≤ β ⇒ I(α) ≤ I(β),
2. Subadditivity, i.e., I(α ∨ β) ≤ I(α) + I(β),
3. Additivity, i.e., I(α ∨ β) = I(α) + I(β|α).
The third property requires a definition of conditional information, which has been
discussed in Sect. 12. There it is shown that we cannot expect additivity on R and
F, but we have it on P and T (Proposition 12.1). We will not repeat this discussion
here. Now we have only to collect the results from Chaps. 11 and 17 in order to
extend the first two properties from P to the lattice T of tight covers or templates
and to the lattice Ff of finite flat covers. The lattice structure or at least the partial
order  has already been discussed and used in ergodic theory in the context of
topological entropy (Adler et al. 1965, Walters 1982, Goodwyn 1969 and Goodman
1971). First, we will see how far we can get with these properties on the full lattices
C of all covers and R of all repertoires, which can be identified (by Proposition 10.5)
with the lattice of clean repertoires.
18.1 The Lattice C of Covers
In Definition 11.1, we defined our information concepts only for repertoires, not
for general covers. In general, both I and N would often turn out to be infinite
for arbitrary, non-finitary covers, but we can use a slightly different definition (see
Definition 18.3).
We will consider a slightly more general version of the lattice C compared to
Definition 17.6. Usually we have based our definitions of covers and repertoires
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8_18
259260 18 Information Theory on Lattices of Covers
on the σ-algebra 	. Covers, however, are most often used in topology where they
consist of open sets instead of measurable sets, and we now want to include the
consideration of open covers and also of covers built from other types of sets in our
definition. We again start with a probability space (, 	, p).
Definition 18.1 Let α be a subset of 	 (or even of P()).
(i) α is called ∩ -stable,
1 if A ∩ B ∈ α for any A,B ∈ α,
(ii) α is called ∪ -stable, if A ∪ B ∈ α for any A,B ∈ α.
Definition 18.2 Let α be a ∩-stable and ∪-stable subset of 	 containing .
(i) A subset γ of α is called an α -cover if  γ = .
(ii) The set of all α-covers is called C(α).
On C(α) we consider the ordering  or ≤3 as defined in the last section, and the
corresponding equivalence relation ∼3.
(C(α), ) is an ordering and (C(α)/ ∼3, ) is a p.o. set. We define Cα :=
C(α)/∼3. The lattice C defined in Definition 17.6 was C	.
Proposition 18.1 (Cα, ) is a lattice with γ  δ = γ · δ and γ  δ = γ ∪ δ.
Proof see Proposition 17.13. 
Proposition 18.2 (Cα, ) is a distributive lattice.
Proof see Proposition 17.14. 
Proposition 18.3
(i) (Cα, ) has a smallest element, namely, {}.
(ii) If {ω} ∈ α for all ω ∈ , then (Cα, ) has a largest element, namely,
,
{ω}: ω ∈ 
-
.
Proof Obvious. 
Unfortunately, I and N are not defined on Cα and cannot be well defined,
because ∼3 is different from ∼1 as we saw in the last section. However, for the
subset Fα ⊆ C(α) of flat α-covers, I and N are well defined, because ≤3 and ≤2
coincide and flat covers are repertoires. It is in fact possible to give a different
definition of I on Cα that coincides with our Definition 11.1 for flat α-covers
(see Palm 1976a).
Proposition 18.4 For γ ∈ Fα we have γ = γ and
I(γ ) = inf{I(β): β  γ, β partition}.
1 Compare Definition 10.10.18.2 The Lattice Ff of Finite Flat Covers 261
Proof The first statement was shown in Proposition 11.10. For the second, we can
use Proposition 11.11:
for any description d in γ , we have I(d) = I(d)  = I(β), where β := R(d)  is a
partition and β  γ .
Vice versa, for any partition β  γ , we can define a description d in γ which is
constant on every B ∈ β. Thus, I(d) ≤ I(β). 
Definition 18.3 For γ ∈ Cα we define
I
(γ ) := inf{I(β): β  γ, β partition}.
Proposition 18.5 For γ,δ ∈ Cα the following holds:
(i) γ  δ implies I
(γ ) ≥ I
(δ),
(ii) I
(γ  δ) ≤ I
(γ ) + I
(δ).
Proof
(i) Follows directly from Definition 18.3.
(ii) Let β ≥ γ , β partition and β ≥ δ, β partition. Then β  β = β · β is a
partition, β  β  γ  δ and
I(β  β
) = I(dβ  dβ)
(Prop. 4.5)
≤ I(dβ) + I(dβ) = I(β) + I(β
).
Thus, I
(γ  δ) ≤ I
(γ ) + I
(δ).

So we get monotonicity as well as subadditivity for I on Cα.
18.2 The Lattice Ff of Finite Flat Covers
On flat covers F the orderings ≤2 and ≤3 coincide, called . Thus I is monotonic
on F. (Ff , ) is a lattice with join and meet α β = (α ∪β)f and α β = (α ·β)f
by Propositions 17.15 and 17.16.
On F we have I(α) = min{I(β): β  α, β is a partition} because of
Proposition 18.4. On F, however, the join does not always exist (see Example 17.1).
For this reason we will consider the sublattice of finite flat covers in this section.
Proposition 18.6 I is monotonic and subadditive on Ff .
Proof see Proposition 18.5. 
Example 18.1 N is not monotonic and not subadditive on Ff .262 18 Information Theory on Lattices of Covers
Consider (, 	, p) = D and α = ,
{1, 3, 5},{2, 4, 6}
-
, β = ,
{1, 2, 3, 4},{3, 4,
5, 6}
-
. Then α  β = α ∪ β and N (α  β) = 1. So we have α  β  β and
N (α  β) > N (β) = log2
3
2 .
Consider (, 	, p) = E4 and α = ,
{1, 2, 3},{1, 2, 4}
-
, β = ,
{2, 3, 4},{1, 3, 4}
-
.
Then α  β = α · β = ,
{2, 3},{2, 4},{1, 3},{1, 4}
- and N (α  β) = 1, but
N (α) + N (β) = 2 · log2
4
3 < 1. 
Proposition 18.7
(i) (Ff , ) has a largest element, namely, ω = ,
{ω}: ω ∈ 
-
, if  is finite.
(ii) (Ff , ) has a smallest element, namely, {}.
Proof Obvious. 
Proposition 18.8 (Ff , ) is a distributive lattice. It is a sublattice of (C, ).
Proof As mentioned above α  β = (α ∪ β)f and α  β  (α · β)f , and for finite
flat covers these are equivalent (∼3) to (α ∪ β) and (α · β), respectively. 
18.3 The Lattice R of (Clean) Repertoires
The ordering ≤=≤1 on R is defined in such a way that N is monotonic on R. With
this partial order R becomes a lattice (see Proposition 17.14) with meet and join:
α ∧ β = α∪ ∩ β∪ and α ∨ β = α ∪ β.
Proposition 18.9 N is monotonic and subadditive on R.
Proof see Propositions 11.5 and 11.6. 
Example 18.2
• I is not monotonic on R:
Consider (, 	, p) = D and
α = ,
{1, 3, 5},{2, 4, 6}
- and β = ,
{1, 2},{3, 4},{5, 6}
-
.
Here α ∨ β = α ∪ β ≤ β, but I(β) = log2 3 > I(α ∨ β) = 1.18.4 The Lattice T of Templates 263
• I is not subadditive on R:
Consider (, 	, p) = D2 and the corresponding random variables X1, X2.
Now take
α = ,
[X1 = i]: i = 1,..., 6
-
∪ ,
[X2 = 1],[X2 = 1]
- and
β = ,
[X2 = i]: i = 1,..., 6
-
∪ ,
[X1 = 1],[X1 = 1]
-
.
Then I(α) = I(β) = 1
6 log 6 + 5
6 log 6
5 = log 6 − 5
6 log 5 and I(α ∨ β) = log 6.
Observe that I(α ∨ β) − I(α) − I(β) = − log 6 + 5
3 log 5 > 0.
Proposition 18.10
(i) (R, ≤) has a largest element, namely, ω := ,
{ω}: ω ∈ 
-
, if  is countable,
i.e., p(ω) = 0 for every ω ∈ .
(ii) (R, ≤) has a smallest element, namely, {}.
Proof Obvious. 
Proposition 18.11 R is a distributive lattice.
Proof see Proposition 17.14. 
18.4 The Lattice T of Templates
On T the orderings ≤1 and ≤2 coincide and are called ≤. The p.o. set T can be
identified with the p.o. set of tight descriptions (see Definition 3.11).
Proposition 18.12 For any two templates α and β, we have α ≤ β if and only
if dα ⊇ dβ. In addition, (T, ≤) is a lattice with join α ∨ β = α · β and meet
α ∧ β = α∪ ∩ β∪. Thus, T is a p.o. subset, but not a sublattice of R, because the
join is different.
Proof see Proposition 17.18. 
Proposition 18.13 For any template α we have
I(α) = I(dα), N (α) = N (dα), S(α) = S(dα).
Proof see Definition 11.1. 
Proposition 18.14 On (T, ≤) both N and I are monotonic. I is also subadditive.
Proof The corresponding assertions on dα, dβ, and dα·β = dα ∩ dβ for templates α
and β have been shown in Propositions 3.6 and 4.5. 264 18 Information Theory on Lattices of Covers
The following example shows that N is not subadditive.
Example 18.3 α = {A, }, β = {B,} ⇒ α · β = {A ∩ B, A,B, }, p(A) =
p(B) = 5
8 , p(A ∩ B) = 1
4
N (α) = 5
8 log2
8
5 = N (β)
N (α · β) = 1
4 · 2 +
3
8 +
3
8

· log2
8
5
N (α · β) − N (α) − N (β) = 1
2 − 1
2 log2
8
5 = 1
2 log2 5 − 1 > 0

Proposition 18.15
(i) (T, ≤) has a largest element, namely, ω = {{ω}: ω ∈ }, if  is countable.
(ii) (T, ≤) has a smallest element, namely, {}.
Proof Obvious. 
Proposition 18.16 (T, ≤) contains P as a sublattice.
Proof For two partitions α and β, both α ∩ β = α∪ ∩ β∪ and α ∪ β = α · β are
again (equivalent to) partitions. 
The following example shows that (T, ≤) is not a distributive lattice.
Example 18.4 Consider  = {1, 2, 3, 4} and
α = ,
{1, 2},{3, 4}
-
, β = ,
{1, 3},{2, 4}
-
, γ = ,
{1, 4},{2, 3}
-
.
Then
(α  β)  γ = ,
{1},{2},{3},{4}
-
 γ = γ
(α  γ )  (β  γ ) = {}  {}={}
Proposition 18.17 (T, ≤) is a p.o. subset, but not a sublattice of (D, ⊇) and
of (R, ≤).
Proof The join α ∨ β in R is α ∪ β which usually is not tight. So the join of α and
β in T is (α ∪ β)∩ ∼ (α · β) (see Proposition 17.19) which is somewhat larger. The
meet α ∧ β in R and T is the same.
Conversely, the join in T is the same as in D, whereas the meet in D is the union
of descriptions which does not correspond to the meet in T. 18.6 Technical Comments 265
18.5 The Lattice P of Partitions
The partitions P form a subset of the set R of all repertoires (or covers C). On P the
three orderings of Chap. 17 coincide,2 because P = T ∩ F (see Proposition 10.13).
With this ordering, called ≤, P is a lattice. Indeed, α ∨ β = α · β and α ∧ β =
α∪∩β∪ (see 17.11). It is also clear that any two partitions α and β coincide whenever
α ≤ β and β ≤ α. The lattice P has a smallest element, namely, {}, and a largest
element ω := {{ω}: ω ∈ }.
3 Since partitions are tight covers, each partition α
corresponds to exactly one description dα, and this description is complete. So I
and N actually coincide on P. On P we can easily prove the ideal theorems on I
and N .
Proposition 18.18 (P, ≤) is a sublattice of the lattice (T, ≤). It is a p.o. subset,
but not a sublattice of (F, ).
Proof The join and meet in P are the same as in T, namely, α ∨ β = α · β and
α ∧ β = α∪ ∩ β∪. In F they are α  β = (α · β)f = α · β (for partitions α, β) and
α  β = (α ∪ β)f . So the join is the same, but the meet is different. 
Proposition 18.19 On (P, ≤) both I and N are monotonic and subadditive.
Proof N = I and I is monotonic and subadditive on T (Proposition 18.14). 
Like T, also P is not a distributive lattice as can be seen from Example 18.4.
18.6 Technical Comments
By collecting our previous results, this chapter demonstrates that it is possible to
extend classical information theory to T, F, and even R without losing the most
important properties of information. The lattices T, F, and R that are introduced
here may be interesting and could further be investigated from a purely lattice
theoretical point of view. We have determined these lattices for small finite ,
i.e., #() = 2, 3, and 4. We don’t know how the size of these lattices grows
with n = #().
In addition, we have introduced information theory for arbitrary covers by
various kinds of sets (e.g., measurable covers, open covers, closed covers) in
Sect. 18.1 (Definitions 18.2 and 18.3).
2 It is surprising that the “opposite” orderings ≤ and  coincide on P.
3 Here our requirement added to Definition 10.1 leads to the requirement that p(ω) = 0 for every
ω ∈ . So P only has a largest element, if  is countable.266 18 Information Theory on Lattices of Covers
18.7 Exercises
(1) For #() = 2, 3, and 4 determine the lattices P, T, F, and R.
(2) For #() = 5, 6 determine the lattice P.
(3) Find an example for small #() that shows that T and P are not distributive
lattices.
(4) A complement Ac of a lattice element A satisfies A ∧ Ac = 0 (the smallest
element) and A ∨ Ac = 1 (the largest element).
For each of the lattices P, T, Ff , and R, find examples for elements A that do
and do not have a complement.
(5) Are there examples of these lattices that have a unique second-largest or second￾smallest element?
References
Adler, R.L., Konheim, A.G., & McAndrew, M.H. (1965). Topological entropy. Transactions of the
American Mathematical Society, 114, 309–319.
Goodwyn, L.W. (1969). Topological entropy bounds measure-theoretic entropy. Proceedings of
the American Mathematical Society, 23, 679–688.
Goodman, T. N. T. (1971). Relating topological entropy and measure entropy. Bulletin of the
London Mathematical Society, 3, 176–180.
Walters, P. (1982). An introduction to ergodic theory. Springer.Appendix A
Fuzzy Repertoires and Descriptions
Here we want to introduce a generalization of descriptions and repertoires to fuzzy
sets. In many applications of information theoretic ideas, in particular to neural
networks and learning systems, it appears quite natural to consider fuzzy sets, and
the generalization of our concepts is actually quite straightforward.
The idea of making sets fuzzy, goes back to Zadeh (1965): the membership of
a point x to a set A is not expressed by a binary value, but instead by a degree of
membership, a number between 0 and 1, where 1 indicates certainty that x belongs
to A and 0 indicates certainty that x does not belong to A.
Given a probability space (, 	, p) we replace the propositions A ∈ 	 by fuzzy
propositions, i.e., random variables A:  → [0, 1].
Mathematically speaking, we replace the σ–algebra 	 by the lattice M of all
random variables on  with values in [0, 1]. M is also called the set of fuzzy
propositions or membership functions.
On M we have the ordering A ≤ B defined by p[A ≤ B] = 1 and the equality
A = B meaning again p[A = B] = 1, as the corresponding equivalence.
With this ordering M is a distributive lattice and
(A ∧ B)(ω) = min(A(ω), B(ω)),
(A ∨ B)(ω) = max(A(ω), B(ω)) .
M has a smallest element 0 and a largest element 1.
We can use the order structure of M to define repertoires, and we use the new
viewpoint of fuzzy membership for a different definition of descriptions, which
actually reveals some more structure compared to Chap. 3, because now descriptions
are a special kind of fuzzy relations.
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8
267268 A Fuzzy Repertoires and Descriptions
Definition A.1
1. A measurable function D : ( × ,	 × 	) → [0, 1] is called a fuzzy relation.
2. A fuzzy relation D is called fuzzy description, if for every A ∈ 	 with p(A) = 0
and almost every x ∈ A, there is a subset B ∈ 	, x ∈ B ⊆ A, p(B) = 0, such
that D = 1 on B × B.
3. The range R(D) of D is defined as R(D) = {Dω : ω ∈ }. Here Dω denotes
the random variable obtained by fixing the first argument in D, i.e., Dω(ω
) :=
D(ω, ω
).
Definition A.2 The adjoint D of a fuzzy relation D is defined by D
(ω, ω
) :=
D(ω
, ω). A fuzzy description D is called symmetric, if D = D
.
Definition A.3 The composition of two fuzzy relations C and D is defined as
C ◦ D(ω, ω
) := supC(ω, x) ∧ D(x, ω
) or by (C ◦ D)ω = supx Cω(x) ∧ Dx .
Definition A.4 A fuzzy relation D is called
1. Reflexive, if D(ω, ω) = 1 for every ω ∈ ,
2. Symmetric, if D = D
,
3. Transitive, if D ◦ D ≥ D.
A.1 Basic Definitions
Now we can reformulate the main ideas of this book in the framework of fuzzy sets.
To keep this appendix short, definitions are just given in a condensed summary form,
and most proofs are omitted because they are simple translations of the analogous
proofs in the book.
Definition A.5
1. A fuzzy proposition A ∈ M is called essential, if p[A = 1] = 0.
2. A fuzzy cover α is a countable set of essential fuzzy propositions A ∈ M
with 6{1[A=1] : A ∈ α} = 1.
Proposition A.1 The range R(D) of any fuzzy description D is a fuzzy cover.
Proposition A.2 Any description d in the sense of Definition 3.3 can be viewed as
a fuzzy description D :  ×  → {0, 1} by D(ω, ω
) := 1[ω
∈d(ω)].
Such a description D, satisfying D(ω, ω
) ∈ {0, 1} for every ω,ω ∈ , is called
crisp.A Fuzzy Repertoires and Descriptions 269
Definition A.6 The crisp version of a fuzzy cover α is defined as αcr = {Acr : A ∈
α} where Acr := [A = 1].
The crisp version of a fuzzy description D is defined as Dcr(ω) := [Dω = 1].
Clearly the crisp version of a fuzzy cover is a cover and the crisp version of a
fuzzy description is a description.
Definition A.7 A description d :  → 	 is called symmetric, if ω ∈ d(ω) implies
ω ∈ d(ω
) for every ω,ω ∈ .
Proposition A.3
Any fuzzy description is reflexive.
The crisp version of a symmetric fuzzy description is symmetric.
The crisp version of a transitive fuzzy description is tight.
Definition A.8 Let D be a fuzzy description. We define
1. Its completion: D(ω, ω  
) := 1[Dω=Dω] or1 D(ω)  := {ω : Dω = Dω},
2. Its novelty: ND :  → R, ND(ω) = − log2 E(Dω), N (D) := E(ND),
3. Its surprise: S(D) := N (N≥
D ),
4. Its information: I(D) = N (D)  .
D corresponds to a complete description or a partition as before.
Proposition A.4 N (D) ≤ I(D).
We have a natural ordering ≤ on fuzzy descriptions with corresponding join and
meet:
(C ∧ D)(ω, ω
) = min(C(ω, ω
), D(ω, ω
)),
(C ∨ D)(ω, ω
) = max(C(ω, ω
), D(ω, ω
)) .
For this ordering N is clearly monotonic, but not subadditive. I, however, is not
monotonic, but subadditive. In fact, the extension of I to fuzzy descriptions does
not provide any new results, because D is nothing more than a partition. In order
to get a more interesting information theory, we again extend our definitions from
fuzzy descriptions to fuzzy repertoires. We also introduce fuzzy templates and fuzzy
partitions.
Definition A.9 Let α be a fuzzy cover. A description D is a description in (terms
of) α, if R(D) ⊆ α, i.e., if for almost every ω ∈  there is an A ∈ α such that
Dω = A. D(α) denotes the set of all fuzzy descriptions in α.
1 The first definition defines D as a fuzzy relation and the second as a description in the sense of
Chap. 2.270 A Fuzzy Repertoires and Descriptions
Definition A.10
1. Novelty
N (α) = sup{N (D) : D ∈ D(α)}
2. Information
I(α) = lim
ε→0
inf{I(D) : D ∈ D(α), N (D) ≥ N (α) − ε}
For a practical calculation of N and I, it is useful to require some additional
regularity from the fuzzy cover α. As in Chap. 10 these requirements are provided
by the notion of a repertoire.
A.2 Definition and Properties of Fuzzy Repertoires
Definition A.11 Let α be a fuzzy cover. For A ∈ α, we define
1. A := Acr \
{Bcr : B ∈ R(α), E(B) < E(A)} and
2. α := {A : A ∈ α, p(A) = 0}.
A is the set of all ω ∈  for which A is the description with maximal novelty. If
this description is unique for (almost) every ω ∈ , we again call the repertoire α
tight.
Definition A.12 Let α be a fuzzy cover.
1. α is called a fuzzy repertoire, if α is a cover.
2. α is called tight, if α is a partition.
3. α is called a fuzzy partition, if αcr is a partition.
4. α is called a fuzzy template, if for all A,B ∈ α with p[A = 1, B = 1] = 0 also
A ∧ B ∈ α.
If α is a fuzzy repertoire, also I(α) can be calculated easily. It simply reduces to
the calculation of I(α), or more exactly I(αf ) (see Definition 10.13).
Again we define templates as being stable against logical conjunction, in analogy
to our results in Chap. 10. However, tightness is now a much weaker condition.
Proposition A.5 Every fuzzy template is tight.
Proof Let α be a fuzzy template. If α is not a partition, then there are A,B ∈ α
with p(A ∩ B) = 0. Thus, p[A = 1, B = 1] = 0, and therefore A ∧ B ∈ α.
Clearly E(A∧B) ≤ E(A), and E(A∧B) < E(A) would imply (A∧B)cr∩A = ∅
and even Bcr ∩ A = ∅ because A ⊆ Acr, but this would imply B ∩ A = ∅. So
E(A ∧ B) = E(A). Together with A ∧ B ≤ A, this implies A ∧ B = A. Similarly
A ∧ B = B is shown. This contradicts A = B. A Fuzzy Repertoires and Descriptions 271
Definition A.13 Let α be a fuzzy cover. We define
α∧ := {∧β : β = ∅, β finite,β ⊂ α, ∧β essential}.
α∧ is called the fuzzy consequence of α. Clearly α∧ is a fuzzy template.
It is now quite easy to discuss the practical computation of N (α) and I(α) along
the lines of Chap. 11, at least for fuzzy repertoires.
N (α) can again be calculated as the expectation of a random novelty variable Nα.
Nα(ω) := lim
ε→0
sup{− log E(A) : A(ω) ≥ 1 − ε, A ∈ α}
Then N (α) = E(Nα).
Based on this we may define an even fuzzier version of novelty:
Nf (ω) := sup{−A(ω)log E(A) : A ∈ α}
and
Nf (α) := E(Nf ).
Clearly Nf ≥ Nα.
A perhaps even simpler and more general version of defining Nα is the following:
Nα(ω) := 
−log2E(A) for ω ∈ A,
∞ for ω /∈  α.
Also I can be calculated based on α. S is calculated as S(α) = N (N≥
α ), and
Sf (α) = N (N≥
f ).
Definition A.14 Let α and β be two fuzzy repertoires. α ≤ β means that for every
C ∈ D(α) there is a D ∈ D(β) with D ≤ C.
It turns out that the union α ∪ β of two fuzzy repertoires is the natural join for
this ordering.
Proposition A.6
(i) α ≤ β implies N (α) ≤ N (β).
(ii) N (α ∪ β) ≤ N (α) + N (β).
Proposition A.7 Let α be a fuzzy repertoire. Then I(α) = I(αf ).
When we consider the set Tf of fuzzy templates, we have a natural join α ∨ β of
two templates α and β, i.e., the smallest template that is larger than α ∪ β. Now we
can show that on fuzzy templates I is monotonic and subadditive.272 A Fuzzy Repertoires and Descriptions
Definition A.15 For two fuzzy repertoires α and β, we define
1. α ∨ β := {A ∧ B : A ∈ α, B ∈ β,A ∧ B essential} and
2. α · β := {A · B : A ∈ α, B ∈ β,A · B essential}.
Proposition A.8
1. For two tight fuzzy repertoires α and β, α ≤ β implies I (α) ≤ I (β).
2. For two fuzzy templates α and β, we have I(α ∨ β) = I(α · β) ≤ I(α) + I(β).
The set Pf of fuzzy partitions with the ordering ≤ turns out to be a sublattice of
Tf with join α ∨ β. Even on Pf novelty N and information I do not coincide. Of
course, on Tf and in particular on Pf we still have N ≤ I and also monotonicity
of N and I. Proposition A.8 shows that I is subadditive on Tf and on Pf in
particular. However, N is not subadditive, neither on Tf nor on Pf , as the following
example shows.
Example A.1 Let  = {1,..., 8} and α = {A1, A2, A3, A4}, β =
{B1, B2, B3, B4} as defined in the table below (for 0 <a< 1).
Then N (α · β) and N (α ∨ β) may be larger than N (α) + N (β). In fact,
N (α · β) = 1
2 log
8
1 + 2a +
1
2 log 8 = 3 − 1
2 log(1 + 2a)
N (α) = N (β) = 1
4 log 4 +
1
4 log
8
2 + 2a +
1
4 log
8
2 + 4a +
1
4 log
8
2 + 6a
= 1
2 +
3
2 − 1
4 (log(1 + a) + log(1 + 2a) + log(1 + 3a))
1 2 3 4 5 6 7 8
A1 1 1 a a a a a a
A2 0 0 1 1 a a a a
A3 0 0 0 0 1 1 a a
A4 0 0 0 0 0 0 1 1
B1 1 0 0 0 0 0 0 1
B2 a 1 1 0 0 0 0 0
B3 a a a 1 1 0 0 0
B4 a a a a a 1 1 0A Fuzzy Repertoires and Descriptions 273
Proposition A.9
1. For a tight fuzzy repertoire α, we have
N (α) = −
A∈α
p(A)log2 E(A) and
I(α) = I(α) = −
A∈α
p(A)log2 E(A) .
2. For a fuzzy partition α, we have
N (α) = −
A∈α
p(Acr)log2 E(A) and
I(α) = I(αcr) = −
A∈α
p(Acr)log2 E(Acr) .
Reference
Zadeh, L. A. (1965). Fuzzy sets. Information and Control, 8, 338–353.Appendix B
Similarity Theory
Heiner Markert and Günther Palm
Similarity theory deals with the notion of similarity of patterns (which usually
are elements of some vector space). Similarity is introduced as a complementary
concept to the well-known notion of distance or metric, and relationships between
these two concepts are established. In particular, similarity can also be defined
for descriptions, leading to the notions of information similarity and information
distance.
The most complex final construction in this appendix (Proposition B11 and
B12, showing that every distance on a finite space is equivalent to the Hamming
distance) is due to Stellmann (1992). In his thesis he discussed methods of creating
sparse binary similarity preserving codes (see Palm (2013) and Market (2009)).
This problem has perhaps been our principal motivation for developing a theory
of similarity and its relation to distance.
B.1 Definitions and Elementary Observations
The basic notion of similarity theory is the so-called weak metric:
Definition B.1 (Weak Metric) A weak metric on a space X is a symmetric
function
m : X × X → R+,
where the equation
m(x, y) = m(y, x) ∀x,y ∈ X
holds. The pair (X, m) is called a weak-metric space.
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8
275276 B Similarity Theory
Now we introduce two complementary notions called similarity and distance, the
latter being more general than the conventional notion of a distance, which is called
a metric.
Definition B.2 (Distance) Let (X, d) be a weak-metric space.
Then d : X × X → R is called a distance (or dissent), if
d(x, x) = 0 ∀x ∈ X.
The Space (X, d) is called a distance space. The distance is said to be strict, if
d(x, y) = 0 )⇒ x = y ∀x,y ∈ X.
The notion of similarity will now be defined formally.
Definition B.3 (Similarity) Let (X, s) be a weak-metric space. The function
s : X × X → R is called a similarity (or consent), if
s(x, x) ≥ s(x, y) ∀x,y ∈ X.
The pair (X, s) is called a similarity space. The similarity s is said to be
strict, if 2s(x, y) < s(x, x) + s(y, y) ∀x,y ∈ X with x = y.
superstrict, if s(x, y) < s(z, z) ∀x,y,z ∈ X, x = y, and
normed, if s(x, x) = s(y, y) ∀x,y ∈ X.
Remark B.1 A superstrict similarity is always strict.
Proposition B.1 Let (X, s) be a similarity space with a strict and normed similarity
s. Then, s is superstrict.
Proof For a fixed n ∈ R+ and all z ∈ X, the relation s(z, z) = n holds because s
is normed. Now choose x,y ∈ X such that s(x, y) ≥ n, i.e., assume that s is not
superstrict. Then, it follows from the fact that s is a similarity, that
s(x, y) ≤ s(x, x) = n,
i.e., s(x, y) = n.
Because s is strict, s(x, y) < n must hold if x = y, which contradicts the
assumption that s is not superstrict. 
The triangle inequality is well known for metric spaces. A similar concept also
applies to distance and similarity spaces.
Definition B.4 (Triangle Inequality)
Let (X, d) be a distance space: then d is said to fulfill the triangle inequality iff
d(x, z) ≤ d(x, y) + d(y, z) ∀x,y,z ∈ X.B Similarity Theory 277
Let (X, s) be a similarity space: then s is said to fulfill the triangle inequality iff
s(x, y) + s(y, z) ≤ s(x, z) + s(y, y) ∀x,y,z ∈ X.
Remark B.2 A strict distance d on X that fulfills the triangle inequality is usually
called a metric. A strict similarity that fulfills the triangle inequality will be called
overlap or nearness.
Example B.1 (Hamming Space) The hamming space X is given as X = {0, 1}
n
for a fixed n ∈ N. Define for x,y ∈ X
dH (x, y) = n
i=1
|xi − yi| = ||x − y||
and
sH (x, y) = x · y = n
i=1
xi · yi.
Then, dH is a distance measure on X, called the Hamming distance. The Hamming
overlap or Hamming similarity is given by sH .
Remark B.3 The Hamming distance and the Hamming similarity are both strict and
the triangle inequality holds.
A distance on a space X can be transformed into a similarity and vice versa.
Definition B.5 (Induced Similarity/Distance) Let (X, d) be a distance space. Let
dmax = maxx,y∈Xd(x, y). Then,
sd : (x, y) → dmax − d(x, y)
is the similarity on X induced by d.
Let now a similarity space (X, s) be given. The function
ds : (x, y) → 1
2 (s(x, x) + s(y, y)) − s(x, y)
is called the distance (on X) induced by s.
Remark B.4 The induced similarity sd on a distance space (X, d) is always normed.278 B Similarity Theory
In particular, this implies that if a similarity s is transformed into the induced
distance ds and this distance is transformed back to the similarity s(ds), then the
relation s(ds) = s will in general not hold.
The following theorem states which properties of a similarity will be kept by the
above transformation.
Proposition B.2 Let (X, d) be a distance space. For the induced similarity sd , the
following properties hold:
1. The triangle inequality holds for sd iff it holds for d.
2. sd is strict iff d is strict.
The equivalent properties hold for the induced distance ds on a similarity space
(X, s):
1. The triangle inequality holds for ds iff it holds for s.
2. ds is strict iff s is strict.
Proof The proof is given by simple insertion into the definitions. 
Since many examples for distances and metric spaces are known, as opposed to
similarities, we now give a few examples for similarity spaces. As an exercise one
may determine the corresponding distances.
Example B.2 Let (, 	, p) be a probability space. We define a similarity on 	 by
s(A, B) = p(A ∩ B).
Example B.3 Let H be a real Hilbert space with inner product (x, y) → x · y
and corresponding norm. The inner product defines a similarity on the unit ball
B = {x ∈ H : x · x = 1} by s(x, y) = x · y + 1.
The first similarity is an overlap; the second does not fulfill the triangle inequality.
This is the case because the corresponding distance is the square of the Hilbert
space norm. If we use the Hilbert space norm as a distance (which is a metric), we
obtain a slightly different similarity based on the inner product, namely, s
(x, y) =
2 − √(2 − 2x · y). For a better understanding of these quite typical examples, we
now consider a more general type of example in which both of the above similarities
can be introduced.
Example B.4 Let (, 	, p) be a probability space. We consider the space X =
L1(p) of all p-integrable functions and in particular I = {x ∈ X: 0 ≤ x ≤ 1}. On I
we can define an inner product x · y = E(xy), which is a candidate for a similarity
as in example B.3, called s2. We can also define a similarity s1 which generalizes
Example B.2, namely, s1(x, y) = E(x ∧ y), where x ∧ y is the pointwise minimum
of the functions x and y. In fact, if we identify sets A,B ∈ 	 with their indicator
functions 1A and 1B, we get s1(1A, 1B) = s2(1A, 1B) = p(A ∩ B). It turns out
that s1 fulfills the triangle inequality, but s2 does not. This can be understood by
considering the triangle inequality for x, y, z ∈ R+. For the minimum the triangleB Similarity Theory 279
inequality is always fulfilled, but for the product it is not fulfilled when y is between
x and z.
Question: On which subset S of I iss2 an overlap (if we assume that the functions
in S are allowed to take the values 0 and 1)? Taking the Hilbert space H = L2(p)
and its unit ball B as in example B.3, how does s on B relate to s2 on S?
In the following example, we extend Example B.2 from sets to descriptions
(Chap. 3), which combines similarity theory with my extension of information
theory.
Example B.5 Let (, 	, p) be a probability space. For two descriptions c and d,
we define s(c, d) = E(p(c ∩ d)) and s
(c, d) = E(p(c|d) + p(d|c)).
Both s and s are obviously similarities. However, only s fulfills the triangle
inequality. The intuitive problem with s is that it measures not only the similarity
between the descriptions c and d but also the size or probability of the propositions
(sets) c(x) and d(x). This leads to the idea to introduce some kind of normalization
as in s
, but then we lose the triangle inequality. In view of information theory, a
natural way out of this dilemma could be to use the logarithms of these probabilities
and to define the information similarity as sI (c, d) = T (c, d). This similarity also
fulfills the triangle inequality (for the proof we need subadditivity of information I).
The corresponding information distance is dI (c, d) = I (c|d) + I (d|c).
Given the discussion on transinformation and mutual novelty in Chaps. 4 and 12,
one cannot reasonably define this information similarity and information distance
for novelty (instead of information), but one can define it for descriptions, for
templates, and for random variables.
Definition B.6 (Induced order) Let (X, s) be a similarity space and x,y ∈ X.
Then,
x * y ⇐⇒ ∀z ∈ X : s(x, z) ≤ s(y, z)
defines a relation on (X, s).
(X, s) is called ordered, if
s(x, x) = s(y, x) ⇐⇒ x * y ∀x,y ∈ X.
Remark B.5 It is easy to see that * is transitive on X. If s is a strict similarity, then
* is a partial order.
For x * y, it follows that s(x, x) ≤ s(y, x) holds. From Definition B.3 of the
similarity, it follows that s(x, x) ≥ s(y, x). Hence, equality follows.
On the other hand, let s(x, x) = s(y, x). Then in an ordered space X, x * y
holds. This yields s(x, z) ≤ s(y, z) ∀z ∈ X.
Proposition B.3 Let (X, s) be a similarity space with triangle inequality (i.e., the
triangle equation holds for s). Then (X, s) is ordered.280 B Similarity Theory
Proof The statement is proved if
s(x, x) = s(x, y) )⇒ ∀z ∈ X : s(x, z) ≤ s(y, z) ∀x,y ∈ X
is shown. Choose an arbitrary z ∈ X. From the triangle inequality
s(x, x) + s(y, z) ≥ s(y, x) + s(x, z)
follows. Apply this for s(x, x) = s(x, y) to finish the proof. 
Example B.6 The Hamming space X = {0, 1}n with Hamming similarity is
ordered.
Proof The triangle inequality holds for the Hamming similarity. 
Proposition B.4 Let (X, s) be a superstrict similarity space. Then (X, s) is
ordered, but the order is trivial, i.e., x * y holds only if x = y.
Proof Superstrictness means that s(x, y) < s(z, z) for all x,y,z ∈ X and x = y.
In particular, s(y, x) = s(x, x) can only hold for x = y, and in this case, of course,
x * y. 
B.2 Homomorphisms Between Weak-Metric Spaces
In this section we introduce an equivalence between weak metrics by the existence
of a monotonic “translation” map f that translates the values of one weak metric
into those of the other. With this weaker notion of isometry, we can show that every
finite strict distance or similarity space is weakly isometric to a Hamming space.
This means that (under mild conditions) for any matrix of similarity or distance
values of a finite set of objects, we can construct binary codewords for these objects
that weakly preserve the similarity or distance.
Definition B.7 (Isometric Mapping Between Weak-Metric Spaces) Given two
weak-metric spaces (X, m) and (X
, m
), a mapping f : X → X is called an
isometry, if
m((x, y)) = m
(f (x), f (y)) ∀x,y ∈ X.
f : X → X is called a weak isometry, if a strictly monotonic function g : R+ →
R+ exists with
g(m(x, y)) = m
(f (x), f (y)) ∀x,y ∈ X.B Similarity Theory 281
Definition B.8 (Equivalence of Weak Metrics) Let (X, m) be a weak-metric
space and m a different weak metric on X. Define a relation ∼ between these two
weak metrics by
m ∼ m ⇐⇒ id is a weak isometry between (X, m) and (X, m
),
i.e., m ∼ m
, if a strictly monotonic function g exists with
g(m(x, y)) = m
(x, y) ∀x,y ∈ X.
Remark B.6 ∼ defines an equivalence relation on the weak metrics on X.
Remark B.7 Let a distance space (X, d) and the induced similarity sd be given.
Then sd is equivalent to d. If (X, s) is a normed similarity space, the induced
distance ds is equivalent to s.
Proof The equation sd = dmax − d holds. Let g(x) = dmax − x, then g is strictly
monotonic and sd (x, y) = g ◦ d(x, y).
A normed similarity s fulfills s(x, x) = n for all x ∈ X and a fixed n ∈ R+. The
induced distance ds can now be written as
ds(x, y) = 1
2
(s(x, x) + s(y, y)) − s(x, y) = n − s(x, y).
Hence, for g
(x) = n − x, the statement ds(x, y) = g ◦ s(x, y) holds, and g is
strictly monotonic. 
Proposition B.5 Let (X, s) and (X, s
) be two similarity spaces with s ∼ s such
that a strictly monotonic increasing function g : R+ → R+ exists with
g(s(x, y)) = s
(x, y).
If (X, s) is ordered, then (X, s
) is ordered.
Proof A space (X, m) is ordered iff
m(x, x) = m(y, x) ⇐⇒ ∀z ∈ X : m(x, z) ≤ m(y, z). (B.1)
If s ∼ s
, there is a strictly monotonic function g with
g(s(x, y)) = s
(x, y)
for all x,y ∈ X.282 B Similarity Theory
For strictly monotonic increasing g, it follows that
g(s(x, x)) = g(s(y, x)) ⇐⇒ s(x, x) = s(y, x) ⇐⇒ ∀z : s(x, z) ≤ s(y, z) (B.2)
⇐⇒ ∀z : g(s(x, z)) ≤ g(s(y, z)). (B.3)
For strictly monotonic decreasing g, the order of the elements is reversed: if
x * y holds under s, for s the equation y * x follows with Eq.B.2. This contradicts
the definition of the order (repeated above in Eq.B.1), as it finally states
s
(x, x) = s
(y, x) ⇐⇒ ∀z : s
(x, z) ≥ s
(y, z).

Proposition B.6 Let (X, m) be a weak-metric space. Then, a weak metric m exists
on X that is bounded and equivalent to m.
Proof Set, for example, g = arctan|R+ in Definition B.7. 
Proposition B.7 Let (X, d) be a distance space. Then, an equivalent distance d
exists on X that fulfills the triangle inequality.
Proof Let c = maxx,y,z∈X{d(x, z) − d(x, y) − d(y, z)} ≥ 0 and define d
(x, y) =
d(x, y) + c ∀x,y ∈ X. Then the triangle inequality holds for d
:
d
(x, z) − d
(x, y) − d
(y, z) = d(x, z) − d(x, y) − d(y, z) − c ≤ 0.

Remark B.8 Of course, it is possible to choose, e.g., c = maxx,y∈X{d(x, y)} or any
other value c ≥ c. The value chosen in the above proof however is the minimal one.
Proposition B.8 Let (X, s) be a similarity space and lets be superstrict. Then there
exists an equivalent similarity s such that the triangle inequality holds for s
.
Proof Because of theorem B.6, s can be assumed to be bounded.
Let now c = maxx,y,z∈X{s(x, y) + s(y, z) − s(x, z) − s(y, y)} ≥ 0 and define
s
(x, y) =
	 s(x, y) , if x = y
s(x, y) + c , if x = y .
Furthermore, define the function g : R → R as
g(x) =
	
x + c , if x ≥ minx∈X{s(x, x)}
x , otherwise .B Similarity Theory 283
Then,
g(s(x, x)) = s(x, x) + c = s
(x, x)
follows, and for x,y ∈ X, x = y, we get
g(s(x, y)) = s(x, y) = s
(x, y).
The triangle inequality holds for s because of
s
(x, y)+s
(y, z)−s
(x, z)−s
(y, y) = s(x, y)+s(y, z)−s(x, z)−s(y, y)−c ≤ 0,
if x,y,z are pairwise different. In all other cases, the triangle inequality follows in
an analogous way. 
Proposition B.9 Let (X, s) be a finite ordered similarity space (i.e., X is finite).
Then, a similarity s exists that is equivalent to s such that the triangle inequality
holds for s
.
Proof Let s1(x, y) := s(x, y). A new similarity si is constructed from si−1 as
follows:
First, define
ci−1(y) = maxx,z∈X{si−1(x, y) + si−1(y, z) − si−1(x, z) − si−1(y, y)},
and let
s∗
i = min{si−1(y, y) : y ∈ X, ci−1(y) > 0}. (B.4)
Let
c∗
i = max{ci−1(y) : y ∈ X, si−1(y, y) = s∗
i }.
Set
si
(x, y) =
	 s(x, y) , if si−1(x, y) < s∗
i
s(x, y) + c∗
i , if si−1(x, y) ≥ s∗
i
(B.5)
for all x,y ∈ X. The function
g(x) =
	 x , if x<s∗
i
x + c∗ , otherwise284 B Similarity Theory
shows that si and si−1 are equivalent, as si = g ◦ si−1 holds with strictly monotonic
g. Because g is increasing, theorem B.5 holds and hence si is ordered if si−1 is
ordered. In the present case, this means that all similarities si are ordered.
We will show below that the triangle inequality holds for si for all y ∈ X with
si−1(y, y) ≤ s∗
i (i.e., si
(y, y) ≤ s∗
i + c∗
i ), i.e., the relation
∀x,y,z ∈ X si
(y, y) ≤ s∗
i + c∗
i )⇒ si
(x, y) + si
(y, z) ≤ si
(x, z) + si
(y, y)
(B.6)
will be proved.
Assume for the moment that formula B.6 holds. Apply the iterative process of
generating si as long as possible, i.e., until it becomes impossible to choose an s∗
satisfying equation B.4. The similarity with the highest index will be called smax.
Note that the iteration will terminate because X is finite. Then,
smax (y, y) ≤ s∗
max + c∗
max
will hold for all y ∈ X for the corresponding values of s∗
max and c∗
max, and the
theorem is proved.
For the proof of inequality B.6, four cases need to be distinguished.
Case 1: si−1(y, y) < s∗
i
First, note that
si
(x, z) ≥ si−1(x, z), (B.7)
and
si
(x, y) ≤ si
(y, y)
si
(y, z) ≤ si
(y, y) (B.8)
hold.
From Eq.B.4 it follows that ci−1(y) ≤ 0; hence,
si−1(x, y) + si−1(y, z) ≤ si−1(x, z) + si−1(y, y).
With Eq.B.7 it follows that si−1(x, z) ≤ si
(x, z). Equations B.8 and B.5 yield
si−1(y, y) = si
(y, y), si−1(x, y) = si
(x, y) and si−1(y, z) = si
(y, z), which
shows Eq.B.6.
Case 2: si−1(y, y) = s∗
i , but si−1(x, y) < s∗
i and si−1(y, z) < s∗
i Because of
si−1(y, y) = s∗
i , the following statement holds:
si−1(x, y) + s[i−1(y, z) − si−1(x, z) − si−1(y, y) ≤ ci−1(y) ≤ c∗
i .B Similarity Theory 285
This immediately yields
si−1(x, y) + si−1(y, z) ≤ si−1(x, z) + si−1(y, y) + c∗
i .
Equations B.5 and B.8 yield si−1(x, y) = si
(x, y) and si−1(y, z) = si
(y, z).
Equation B.5 gives c∗
i + si−1(y, y) = si
(y, y), and with Eq.B.7, it follows that
si
(x, y) + si
(y, z) ≤ si
(x, z) + si
(y, y),
which shows Eq.B.6.
Case 3: si−1(y, y) = s∗
i and si−1(x, y) = s∗
i = si−1(y, y) In this case, the
equation
∀z ∈ X : si−1(y, z) ≤ si−1(x, z)
holds because (X, si−1) is ordered. Because of si ∼ si−1 with strictly
monotonic increasing g, it follows that
∀z ∈ X : si
(y, z) ≤ si
(x, z).
Adding si
(x, y) = si−1(x, y) + c∗
i on the left hand side and si
(y, y) =
si−1(y, y) + c∗
i on the right hand side yields
si
(x, y) + si
(y, z) ≤ si
(x, z) + si
(y, y),
because the added terms are identical.
Case 4: si−1(y, y) = s∗
i and si−1(y, z) = s∗
i This case is analogous to case 3.

Proposition B.10 Let (X, s) be a similarity space, and let the triangle inequality
be valid for s. Then, (X, s) is ordered.
Proof We show
s(y, y) = s(y, x) ⇐⇒ ∀z ∈ X : s(y, z) ≤ s(x, z).
Let for the first step s(y, y) = s(y, x). With the triangle inequality
s(x, y) + s(y, z) ≤ s(x, z) + s(y, y)
it follows that
s(y, z) ≤ s(x, z)
for all z ∈ X.286 B Similarity Theory
For the opposite direction, assume that for all z ∈ X
s(y, z) ≤ s(x, z)
holds for fixed x,y ∈ X. This means in particular that
s(y, y) ≤ s(x, y)
holds. From the definition of a similarity, it follows that at the same time
s(x, y) ≤ s(y, y)
holds, yielding
s(y, y) = s(x, y).

Remark B.9 The former two theorems show that for a finite similarity space (X, s)
the statements
• the triangle inequality holds for s
• (X, s) is ordered
are equivalent up to the relation ∼, i.e., there exists a similarity s on X with s ∼ s
such that the above two statements are equivalent for s
.
Proposition B.11 Let (X, s) be a finite superstrict similarity space. Then a weak
isometry
f : (X, s) → 
{0, 1}
n, sH

exists for at least one n ∈ N. Here, sH denotes the Hamming similarity (Hamming
overlap).
Proof The proof is divided into three steps.
Step 1 (Definition of the rank):
Let S = {s(x, y) : x,y ∈ X} be the value set of the similarity s on X. Define
N = |S|. Then, there exists exactly one function r˜ : {1,...,N} → S such that
i<j )⇒ ˜r(i) < r(j ) ˜ ∀i, j ∈ {1,...,N}.
Let r = ˜r−1 be the inverse of r˜, r : S → {1,...,N}.B Similarity Theory 287
We call rs(x, y) := r(s(x, y)) the rank of s(x, y) for x,y ∈ X.
The rank defines a similarity on X, and s ∼ rs.
Step 2 (Construction of f for x = y):
Let Hn = {0, 1}n be the n-dimensional Hamming space, and let a function
f : X → Hn be given for a fixed n. Then, a new function f˜ : X → Hk for
k ≥ n can be defined by
f˜ : x →
	
f (x)i , if i ≤ n
0 , otherwise .
The desired function f : (X, s) → (Hn, sH ) will iteratively be determined.
First, set r∗
0 = rs, n0 = 0 and f0 : (X, s) → H0, where H0 = ∅ and hence
f0(x) = ∅ for all x ∈ X. Let further δi be the i-th unit vector in Hn.
j -th iteration:
Set A = {(x, y) ∈ X × X : x = y,r∗
j (x, y) > 0}. Choose one (x, y) ∈ A and
set
r∗
j+1(a, b) =

r∗
j (a, b) − 1 , if (a, b) = (x, y)
r∗
j (a, b) , otherwise ,
as well as
fj+1 =
	
X → Hj+1
z → fj (z) + δj+1 · 1[z=xorz=y]
.
The iteration will be repeated until A = ∅. This will happen after a finite
number of iterations as r∗ are decreased in every step and finally will be zero.
Let n be the minimum number of iterations until A = ∅ holds. Define r∗ = r∗
n
and f = fn. For x,y ∈ X, x = y, this yields
sH (f (x), f (y)) = r∗(x, y).
Step 3 (Adjustment of the self-similarities):
The function f determined in step 2 already maps into the Hamming space as
desired for the theorem for elements x,y ∈ X with x = y. The elements with
x = y however need additional treatment to ensure that the properties of a weak
isometry are respected. For this it is enough to increase the similarity of vectors
f (z) with themselves accordingly in order to achieve s(z, z) = sH (f (z), f (z)).
This is done by the following steps:
• For each z ∈ X with r(z, z) > 0, increase the dimension of the target
Hamming space to n + r(z, z).
• Modify f such that f : X → Hn+r(z,z).288 B Similarity Theory
• For i>n, add δi to f (i.e., set the value to one) for all i ∈ {n + 1,...,n +
r(z, z) to f .
• Set n = n + r(z, z)
The resulting function f meets all the requirements of the theorem.

Proposition B.12 Let (X, d) be a finite strict distance space. There exists a weak
isometry
f : (X, d) → 
{0, 1}
n, dH

for at least one n ∈ N, where dH denotes the Hamming distance.
Proof Because of theorem B.6, d is equivalent to a bounded distance d1 on X. Now
d1 induces an equivalent similarity sd1 on X as in definition B.5. Because d is strict,
it follows that sd1 is superstrict, and theorem B.11 yields the existence of a weak
isometry
f : (X, sd1 ) → 
{0, 1}
n, sH

.
Example B.1 showed that
dH (x, y) = n − sH (x, y),
meaning that the Hamming distance is equivalent to the Hamming similarity. This
proves the theorem. 
References
Markert, H. (2009). Neural associative memories for language understanding and robot control.
PhD Thesis, Universität Ulm.
Palm, G. (2013) Neural associative memories and sparse coding. Neural Network, 37, 165–171.
Stellmann, U. (1992). Ahnlichkeitserhaltende Codierung. Institute of Neural Information Process￾ing, PhD Thesis. University of Ulm, Germany.Index
A
Additivity, 15, 24, 52
novelty, 24
α-cover, 260
Algebra, 14
Alphabet, 13, 105, 107, 114, 116
index, 105
input, 107, 108
output, 105, 107, 108
Anticipation, 107, 109, 110
bound, 106
finite, 107
span, 107, 110
Antitone, 24
A-priori probability, 81
Asymptotic equipartition property, 100,
100–102, 113, 114
Average entropy, 228, 230
Average error, 78
Average information, 96, 98, 99
Average length, 67, 68, 71
Average novelty, 29, 44, 197, 226
Average number of questions, 65
B
Bayesian guess, 81
Beginning, 67
Boolean algebra, 242
Burst novelty, 188
Burst repertoire, 188, 197
Burst surprise, 188
C
Capacity, 80, 107, 108, 110, 113, 114, 116, 182
Chain, 134, 156, 176
Channel, 75, 78, 92, 105, 105–110, 113, 114,
116, 117
without anticipation, 107, 109, 110
capacity, 105, 107, 108, 110, 113, 116, 197
deterministic, 106
finite memory, 107
with memory, 107, 108
without memory, 107
memoryless, 107, 110
simple, 107
Chebyshev-inequality, 95
Choice from, 126
Cleaned version of α, 129
Clean repertoire, 129, 196
Closed under, 14, 14
Code, 66, 67–71, 98, 121
Huffman, 68–70
irreducible, 67, 68, 70
optimal, 67, 69, 98
Codeword, 67, 67–71
beginning, 67
Coincidence repertoire, 188, 192, 197
Complement, 242
Complete description, 30, 41, 42, 49, 52, 121
Completion, 30, 39, 43, 44, 269
Composition, 80, 268
Conditional information, 54
Conditional novelty, 25, 50, 51
Conditional probability, 25, 50
© Springer-Verlag GmbH Germany, part of Springer Nature 2022
G. Palm, Novelty, Information and Surprise, Information Science and Statistics,
https://doi.org/10.1007/978-3-662-65875-8
289290 Index
Confusion matrix, 78
Consequential, 32
Consistent with, 126
Continuous mapping, 17
Continuous random variable, 156
Convex function, 150
Countable-valued function, 17
Cover, 1, 121, 125, 126, 129, 131, 133, 135,
169, 246
clean, 129
disjoint, 127
finitary, 127
flat, 135, 135
∩-stable, 131
narrow, 134
partition, 127
product, 133
shallow, 135, 135
tight, 128, 129
Crisp, 268
version, 269
D
Depolarization repertoire, 188, 195
Description, 26, 26, 27, 29–31, 35, 41–43, 51,
53, 63, 65, 85, 99, 121, 126, 127,
129, 132, 137, 151, 152, 185, 187,
193, 195, 196, 222
complete, 30, 121
by ordering, 151, 152
tight, 32, 132, 263
Description in, 269
Deterministic channel, 106
Difference repertoire, 152
Directed, 31, 45
Discrete mapping, 17
Disjoint cover, 127
Disjoint repertoire, 127
Distributed, 93, 95, 98, 100, 102, 103, 110, 114
identically, 93, 95, 98, 100, 102, 103, 110,
114
independent, 93, 95, 98, 100, 102, 103, 110,
114
independent identically, 93, 95, 98, 100,
102, 103, 110, 114
Distribution, 164, 172, 197
finite, 164
function, 19
product, 172
Distributive, 241
Dynamical entropy, 224
E
Element
minimal, 127
Elementary event, 20
Entropy, 38, 221, 223–228, 230
average, 228
Equivalence class, 236
Equivalent, 148
Error probability, 81, 113, 114, 116
Essential, 268
Essentially, 28
Event, 13, 14, 16, 17, 20, 26, 27, 29, 121, 122,
126, 186–188, 211
elementary, 20
Excitatory postsynaptic potential (EPSP), 186
Expectation-value, 17, 95
F
Finitary, 127
Finite function, 17
Finite memory, 110
Flat cover, 135
Flattening of a repertoire, 135
Function, 15
convex, 150
countable-valued, 17
finite, 17
integrable, 18
Fuzzy, 267
consequence, 271
descriptions, 268, 269
partitions, 270
propositions, 267, 267
relation, 268
templates, 270
H
High-probability, 114–116
element, 114, 115
pair, 114, 115
sequence, 101, 102, 114
Huffman code, 69, 70
I
Identically distributed, 93
Improbability, 24
Independent, 93
Independent identically distributed, 93, 95, 98,
100, 102, 103, 110, 114
Independent proposition, 25Index 291
Information, 35, 38, 43, 44, 52, 53, 55, 63,
70–72, 77, 96, 98, 99, 121, 125, 141,
142, 144, 149, 156, 181, 185, 221
average, 41, 96, 98, 99
gain, 49, 165, 172, 225, 227, 230
rate, 98, 99, 101, 102, 116, 225
subadditive, 149
Inhibitory postsynaptic potential (IPSP), 186
Input alphabet, 105, 107, 108
Input process, 110, 114
Input sequence, 107
Integrable function, 18
In terms of, 126
∩-stable, 196, 260
cover, 131
repertoire, 132
Irreducible, 67, 68
code, 67, 68, 70
Isotone, 24
J
Join, 238
K
Kraft’s inequality, 66, 70
Kullback–Leibler distance, 49
L
Largest element, 238
Largest lower bound (l.l.b.), 238
Lattice, 240, 250
Lebesgue-measure, 169, 225
Length, 66–68, 70, 71
average, 67, 68, 71
Liouville-measure, 223
Lower bound, 237
M
Mapping, 15
continuous, 17
discrete, 17
identity, 110
Markov process, 94, 228, 229
Maximal, 237
Measurability, 19
Measurable, 19, 19, 29, 41, 78, 93
Meet, 238
Memory, 107, 108, 110, 113
bound, 106
finite, 107, 110
internal, 107
span, 107, 110
Minimal, 237
Minimal element, 127
Monotonicity, 40
Monty Hall problem, 23
Mutual information, 56
of random variables, 56
Mutual novelty, 55, 163
of descriptions, 55
N
Narrow, 133
Negentropy, 221, 223
Neural repertoire, 186, 197
Novelty, 24, 29, 32, 43, 48, 50, 51, 53, 63, 99,
121, 141, 144, 149, 176, 181, 185,
186, 188, 192, 222, 269
additivity, 24
average, 29, 44, 63
burst, 188
conditional, 25, 50, 51
of d for α, 122
gain, 49, 164, 165
pause, 192
provided by d for α, 122
subjective, 49, 164
O
Optimal code, 69, 70, 98, 121, 140
Optimal guessing strategy, 66–68, 70, 71, 121
Ordering, 149, 151, 151
Output alphabet, 105, 107, 108
Output sequence, 107
P
Pair-process, 100
Partition, 1, 28
Partition of a repertoire, 127, 129, 222
Pause novelty, 192
Pause repertoire, 188, 192
Pause surprise, 192
Payoff-function, 16, 17
Population repertoire, 197
Postsynaptic potential (PSP), 186, 187
Prefix, 67
Probability, 14, 16, 18, 19, 24, 25, 43, 44, 77,
78, 105, 107–110, 114–116, 176,
225–228
a-priori, 81
conditional, 25, 50292 Index
distribution, 77, 78, 164, 197, 227
error, 113, 116
measure, 77, 78
space, 13, 19, 225
transition, 77, 78, 105, 107–110
vector, 150, 227–229
Process, 93, 98, 110
discrete, 93
input, 110, 114
Markov, 94
stationary, 93, 107
stochastic, 93
Product, 133
Product of covers, 133
Product of distributions, 172
Product of repertoires, 133
Proper, 126
Proper choice, 126, 127–129, 132, 135, 137
Proper description, 126, 127, 129
Proposition, 13, 14, 17–19, 24–27, 121, 126,
127, 129, 144, 195, 196
independent, 25
small, 146
R
Random variable, 16, 41–44, 55, 56, 85, 95,
98–100, 121, 129, 156, 172, 176
continuous, 156
discrete, 41, 85
independent continuous, 156
Random vector, 16
Range, 16, 268
Reflexive, 268
Relation
antisymmetric, 235
connecting, 235
equivalence, 236
irreflexive, 235
ordering, 236
partial order, 236
reflexive, 235
strictly antisymmetric, 235
strict order, 236
symmetric, 235
total order, 236
transitive, 235
Repertoire, 121, 126, 127, 128, 129, 131–133,
135–137, 141, 146, 151, 156, 166,
184–188, 192, 193, 195–197, 211,
214, 215, 222–224, 229, 230
burst, 188, 197
clean, 129, 196
coincidence, 188, 192, 197
depolarization, 188, 195
disjoint, 127
finite, 166
infinite, 164
∩-stable, 132
neural, 181, 186, 197
partition, 127, 129, 222
pause, 188, 192, 197
population, 197
product, 133
shallow, 135, 135
tight, 128, 129, 131–133, 135, 136, 196
S
Scheme, 65, 67, 69, 70
Sequence, 66–68, 71, 107
input, 107
output, 107
Sequence of measurable functions
identically distributed, 93
independent, 93
Shallow cover, 135, 135
Shallow repertoire, 135
Shannon’s theorem, 113, 114, 116
σ-additive, 19
σ-algebra, 19, 78, 125, 166, 167
Small proposition, 146
Simple channel, 107, 110
Smallest element, 238
Smallest upper bound (s.u.b.), 238
Spike, 186–188, 192
Stationary, 114
Stationary process, 93, 107, 113
Stochastic process, 93, 95–103, 114, 116
i.i.d., 93, 95, 98, 100, 102
stationary, 93, 97, 98, 100, 113, 114, 116
Subadditive information, 149
Subjective information, 49, 228
Subjective novelty, 49, 164
Subjective surprise, 49, 176
Surprise, 35, 43, 44, 141, 141, 156, 176,
186–188, 192, 196, 269
average, 99
burst, 188
loss, 165
of an outcome ω, 35
pause, 192
random variable, 42
subjective, 49, 176
Surprising, 30
Symmetric, 31, 268, 269Index 293
T
Tautology, 14
Tight, 32, 128
description, 32, 132
repertoire, 128, 129, 131–133, 135, 136,
196
Tightening, 34, 43, 132
Time average, 226
Transinformation, 55, 56, 77, 78, 163, 172–174
of covers, 173
of descriptions, 55
of random variables, 56, 174
rate, 99
Transition matrix, 228, 229
Transition probability, 77, 78, 105, 107–110
Transitive, 268
U
Uncertainty, 38
Uncomparable, 237
Uniformly disturbing, 83
∪-stable, 260
Upper bound, 238
W
Weak law of large numbers, 95, 95, 100, 102
Y
Yes–no question, 24, 63, 65, 99
