Graduate Texts in Mathematics
Partial 
Diferential 
Equations
Wolfgang Arendt
Karsten Urban
An Introduction to Analytical and 
Numerical Methods
Translated by 
J. B. KennedyGraduate Texts in Mathematics 294Graduate Texts in Mathematics
Series Editor:
Ravi Vakil, Stanford University
Advisory Board:
Alexei Borodin, Massachusetts Institute of Technology
Richard D. Canary, University of Michigan
David Eisenbud, University of California, Berkeley & SLMath
Brian C. Hall, University of Notre Dame
Patricia Hersh, University of Oregon
June Huh, Princeton University
Eugenia Malinnikova, Stanford University
Akhil Mathew, University of Chicago
Peter J. Olver, University of Minnesota
John Pardon, Princeton University
Jeremy Quastel, University of Toronto
Wilhelm Schlag, Yale University
Barry Simon, California Institute of Technology
Melanie Matchett Wood, Harvard University
Yufei Zhao, Massachusetts Institute of Technology
Graduate Texts in Mathematics bridge the gap between passive study and creative
understanding, offering graduate-level introductions to advanced topics in mathemat￾ics. The volumes are carefully written as teaching aids and highlight characteristic
features of the theory. Although these books are frequently used as textbooks in
graduate courses, they are also suitable for individual study.Wolfgang Arendt • Karsten Urban
Partial Differential Equations
An Introduction to Analytical and Numerical
Methods
Translated from the German by James B. KennedyWolfgang Arendt
Institute of Applied Analysis
Ulm University
Ulm, Baden-Wurttemberg, Germany
Translated by
James B. Kennedy
Department of Mathematics
University of Lisbon
Lisbon, Portugal
Karsten Urban
Institute of Numerical Mathematics
Ulm University
Ulm, Baden-Wurttemberg, Germany
English translation of the original German edition published by Springer Spektrum, Berlin, Germany, 2018
ISSN 0072-5285 ISSN 2197-5612 (electronic)
Graduate Texts in Mathematics
ISBN 978-3-031-13378-7 ISBN 978-3-031-13379-4 (eBook)
https://doi.org/10.1007/978-3-031-13379-4
Mathematics Subject Classification: 35-00, 35-04, 35Axx, 35Jxx, 35Kxx, 35Lxx, 65Mxx, 65M60, 65Nxx,
65N30
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland
AG 2023
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, specifically the rights of reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or
information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional affiliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, SwitzerlandLebesgue’s cusp, see Section 6.9: The Dirichlet Problem.For Frauke and AlmutForeword by the Translator
When the authors approached me to ask whether I would be interested in translating
their book on partial differential equations into English, as it turns out one of my
first thoughts was exactly what the authors had written in the preface of the book:
given the large number of excellent textbooks on PDEs available out there, why the
need for another?
Of course, it must also be said that given the enormous breadth of the subject there
are about as many different approaches to PDEs as there are books on them, and no
two books are quite the same. This particular book also seemed to be enjoying some
popularity in Germany, which may or may not be related to the authors’ original
stated motivation of combining material from different undergraduate PDE courses
within the scope of the Bologna reforms to higher education as applied to their home
country.
In fact, while this motivation may stem from what to an outsider looks a rather
arcane educational reform somewhere in Europe, the resulting book is, if not unique,
then certainly very rare. Especially in the Anglosphere, there is still an unhealthy
tendency in many places to divide the world of PDEs rather sharply into pure topics
and applied ones. Triangulations of domains for finite elements are all too often kept
at rather more than arm’s length from anyone dealing with Banach (or even Hilbert!)
spaces, and vice versa.
But of course this is nonsense. To do numerics well, to understand what one is
doing, one needs to understand the theory. Yet in a subject whose raison d’être is to
be found in its broad applicability to problems coming from everything from physics,
via chemistry and biology to economics and finance, a theory with no grounding in
this reality with all its accompanying intuition will be not just less useful, but also
less complete.
While there are many good books which intertwine PDE theory with applications
to, or motivations coming from, one or more of the sciences, I would wager that
it is extremely rare to intertwine all three: the modeling, a modern (weak) solution
theory of PDEs, and both theoretical and practical aspects of numerical analysis of
PDEs. But as the title indicates, that is exactly what this book offers.
ixx Foreword by the Translator
I will not attempt an in-depth summary of the contents here, which the authors
provided in a much better form than I could in the preface. A couple of words
on the prerequisites and particularities as seen by an anglophone outsider might
nevertheless be helpful. A strong grounding in elementary analysis, preferably as far
as the Lebesgue integral, is essential, as is the sort of linear algebra one needs to
be able to do functional analysis; as for the rest, in all essential points the book is
generally self-contained. As is usual for the subject, a level of mathematical maturity
corresponding to about a third or fourth-year level is implicitly assumed.
The first three chapters, on elementary modeling and methods, should be ac￾cessible to a diligent third-year student. The theory in earnest starts in the fourth
chapter on the essentials of operators in Hilbert space, followed by a treatment of
Sobolev spaces. A welcome point of distinction is the rather thorough treatment
of Sobolev spaces in one dimension before moving on to the theory of weak solu￾tions, in particular for elliptic equations, in higher dimensions. The book is rounded
out with an extensive chapter on numerical methods and then a short chapter on
computer-aided calculation (concretely using Maple®). When treating the numerical
methods, as before the philosophy is to start with very simple cases, finite difference
and finite element methods in one dimension, and gradually build up to more general
situations.
The equations most comprehensively treated, and which form something of a
leitmotif throughout the different stages of the book, are Poisson’s equation (the
Laplacian), the heat equation, and the wave equation on Euclidean domains: they
appear in the modeling phase, as inspiration for the development of the Hilbert space
theory, the Sobolev space theory, the weak solution theory, and then the numerics.
Another particularity, however, is worthy of note: the treatment of the Black–Scholes
equation from mathematical finance.
In translating the book I have tried to maintain the spirit and flavor of the original
as much as possible. Most of the terminology and notation used here are more or
less standard and should be easily recognizable to practitioners in the area, even
if some choices are not canonical (for example inhomogeneous rather than non￾homogeneous, coercivity rather than coerciveness, the Lax–Milgram theorem rather
than lemma). Care should be taken with exactly what is meant by Gauss’s theorem,
but this is perhaps always the case.
I would like to extend my thanks to the authors for their enthusiastic and far￾reaching support throughout the process, as well as the publisher for taking the
initiative to propose an English version in the first place.
Lisbon, Portugal James Kennedy
December 2020Preface
Numerous processes in nature, medicine, economics, and technology can be de￾scribed by partial differential equations (PDEs). This alone is enough to explain the
enormous interest in PDEs, as witnessed by the huge number of publications in the
area. It is thus no surprise that there are so many textbooks on the market. So why
write another one?
Since more than a decade, we observe a trend towards introductory lectures which
combine multiple topics. Analysis and numerics of PDEs are well suited to such a
union. Yet there are virtually no introductory textbooks for such courses available
on the market. This combination is also well justified in practical terms. It is a fact
of life that PDEs cannot be solved exactly on general domains (as is necessary for
applications). Exactly what is meant by this will be explained in due course in the
book. In such cases, one is reliant on computer-based approximation procedures,
that is, on numerical methods. In recent years, it has become increasingly apparent
that good numerical methods rest upon modern insights from the analysis of PDEs.
As such, the unification of analytic and numerical methods is of prime importance
both within mathematics and in applications.
We have tried to write an introductory textbook which brings together and inter￾twines the analytic and numerical aspects of PDE theory. The choice of contents is
marked by this objective. At the same time, we attempt to build a bridge to the realm
of applications. The book begins with a chapter on modeling, that is, the translation
of a given problem from applications into the language of mathematics, here into
a PDE. We describe the categorization of PDEs and discuss elementary solution
methods. It turns out that both modeling and numerical processes lead to the use
of Hilbert spaces. Our introduction shows that the associated methods are also the
“correct” ones in a mathematical sense. We describe Hilbert space methods as sim￾ply as possible. Statements about the maximal regularity of solutions are particularly
important for numerical methods. In addition to classical numerical methods (finite
differences and finite elements), we give a few pointers on how to solve at least some
PDEs using computer-based formula manipulation systems such as Maple®.
xixii Preface
Finally, we have included a topic in the book which is of particular interest for
students of financial mathematics. The pricing of risky products on financial and
insurance markets requires a deeper mathematical analysis. These days, PDE-based
models are increasingly finding application, especially in the finance and insurance
industries. In this book, we treat the Black–Scholes equation as a key example of
such a PDE.
A couple of words about the structure of the book might assist in its use. It consists
of three parts:
A Elementary Methods and Modeling (Chapters 1–3)
B Hilbert Space Methods (Chapters 4–8)
C Numerical and Computer-Based Methods (Chapters 9 and 10)
Part A can be read completely independently of the others; it conveys the essence of
PDEs in concrete situations. Only basic knowledge of calculus is needed here. The
results on Fourier series used in this part are presented with detailed proofs.
Part B is of gradually increasing difficulty in terms of contents and exposition.
It contains an introduction to Hilbert and Sobolev spaces, the latter of which are
first considered in one dimension. A particularity is the systematic use of Sobolev
spaces when treating harmonic functions and the Dirichlet problem. As a most
efficient tool, the spectral theorem for self-adjoint compact operators is used to treat
the prototypes of elliptic, parabolic, and hyperbolic equations in an elegant way. In
the more advanced sections of Part B, properties at the boundary of a domain and
integration by parts are used. Again, the needed material is presented with detailed
proofs.
In Part C, we describe the finite difference method and give an introduction to
the finite element method. By restricting to linear elements on triangles in two space
dimensions, we have chosen a very simple situation in which the essential ideas
should nevertheless become transparent. We treat elliptic, parabolic and hyperbolic
problems. All proofs are presented in a detailed and self-contained manner, which
builds upon the material presented in Part A and B. Numerical experiments are
presented.
Each chapter ends with a set of exercises, which in many cases give information
going beyond what was handled in the chapter. Solutions can be found on the book’s
homepage. Sections marked with an asterisk ∗ provide useful additional information.
The book is organized so as to be gradually increasing in difficulty, especially in
terms of the theory, and we have tried to keep it as elementary as possible. Thus it
should be accessible to students even at an early stage of their studies.
Ulm, Germany Wolfgang Arendt
Ulm, Germany Karsten Urban
May 2022
The address of the book’s webpage containing the codes used in Chapters 9 and 10
as well as solutions to the exercises (in German only) reads:
sn.pub/HVTfL5Acknowledgments
This book never could have been written without the help of numerous colleagues
and friends. Our thanks go to the publisher, Springer Verlag (at that time Spektrum
Akademischer Verlag), and in particular Dr. Andreas Rüdinger and Bianca Alton,
for the opportunity to write the original German version of this book, and for their
support in realizing it.
We also wish to thank many colleagues for a huge number of valuable tips
and suggestions, in particular Tom ter Elst, Wilhelm Forst, Stefan Funken, Rüdiger
Kiesel, Werner Kratz, Stig Larsson, and Delio Mugnolo. We also thank Thomas
Richard (Scientific Computers) as well as our colleagues at Ulm, Markus Biegert,
Iris Häcker, Daniel Hauer, Sebastian Kestler, Michael Lehn, Robin Nittka, Mario
Rometsch, Manfred Sauter, Kristina Steih, Timo Tonn, and Faraz Toor, and our
students, for many remarks, additions, and careful proofreading of the manuscript.
Very special thanks go to Daniel Daners (Sydney) who produced the wonderful
image of Lebesgue’s cusp by using Maple® (Figure 6.4 and reference [10]).
We are thankful to a number of colleagues for suggestions for improvement to
the previous German editions: Ferdinand Beckhoff and Silke Glas (University of
Twente), Stefan Hain (Ulm University), Julian Henning (Ulm University), Norbert
Köckler (University of Paderborn), Markus Kunze (University of Konstanz), Tobias
Nau (Ulm University), Patrick Winkert (Technical University of Berlin), and Marco
Zank (University of Vienna).
We are immensely grateful to Petra Hildebrand for her excellent work in preparing
the manuscript in LATEX and for producing numerous figures and graphics.
When finishing the second German edition, we did not expect to have the chance
to work on another edition so soon. This was possible mainly due to two people,
namely Dr. Thomas Hempfling from Springer, who made the English version possible
from the editorial side, and Dr. James B. Kennedy (University of Lisbon), who
undertook the enormous task of translating the German version into English. But
James did much more than taking care of language and style with the highest
competence. He also gave us many suggestions concerning the mathematical content
which significantly improved the book. We are extremely grateful for the great work
that James did!
xiiiAbout the Authors
Wolfgang Arendt studied mathematics and physics in
Berlin, Nice, and Tübingen. After obtaining his doctor￾ate and habilitation in Tübingen, he spent eight years
as a professor at the University of Besançon. From
1995 to 2018, he was in charge of the Institute of Ap￾plied Analysis of Ulm University, where he is now an
emeritus professor. His areas of expertise are centered
around functional analysis and the theory of partial
differential equations. Wolfgang Arendt has had re￾search stays in Berkeley, Nancy, Oxford, Zurich, Syd￾ney, Auckland, New Delhi, Stanford University, and the
University Gustave Eiffel in Paris. He is editor-in-chief
of the Journal of Evolution Equations.
Karsten Urban studied mathematics and computer
science in Bonn and Aachen, obtaining his doctorate
and habilitation at RWTH Aachen. He took up a pro￾fessorship at Ulm University in 2002, where he has led
the Institute of Numerical Mathematics since 2003.
His research is focused on numerical methods for par￾tial differential equations, with a particular emphasis
on concrete applications in science and technology. In
addition to guest professorships at Pavia, Utrecht, and
M.I.T. he has had research stays in Turin, Cambridge,
Marseilles, Göteborg, and Cornell University. In 2005,
he was awarded the teaching prize of the German state
of Baden-Württemberg. He is editor-in-chief of the
journal Advances in Computational Mathematics.
xvAbout the Translator
James Kennedy studied economics and mathematics in Sydney, completing his doc￾torate in mathematics there in 2010. In addition to postdoctoral positions in Lisbon
and Stuttgart, where he obtained his habilitation, he spent two years as a Humboldt
postdoctoral fellow in Ulm. Since 2017 he has been in Lisbon, first as a research
fellow and, since 2020, as a professor. His interests are focused around the theory of
partial differential equations, in particular its intersection with mathematical physics,
operator theory, and spectral theory.
xviiContents
List of figures xxiii
1 Modeling, or where do differential equations come from 1
1.1 Mathematical modeling ....................... 2
1.2 Transport processes ......................... 4
1.3 Diffusion .............................. 8
1.4 The wave equation ......................... 9
1.5 The Black–Scholes equation . . . . . . . . . . . . . . . . . . . . 11
1.6 Let’s get higher dimensional . . . . . . . . . . . . . . . . . . . . 13
1.7* But there’s more . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
1.8 Classification of partial differential equations . . . . . . . . . . . 25
1.9* Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
1.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2 Classification and characteristics 29
2.1 Characteristics of initial value problems on R . . . . . . . . . . . 30
2.2 Equations of second order . . . . . . . . . . . . . . . . . . . . . . 39
2.3* Nonlinear equations of second order . . . . . . . . . . . . . . . . 43
2.4* Equations of higher order and systems . . . . . . . . . . . . . . . 44
2.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3 Elementary methods 49
3.1 The one-dimensional wave equation . . . . . . . . . . . . . . . . 50
3.2 Fourier series . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.3 Laplace’s equation . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.4 The heat equation . . . . . . . . . . . . . . . . . . . . . . . . . . 74
3.5 The Black–Scholes equation . . . . . . . . . . . . . . . . . . . . 90
3.6 Integral transforms . . . . . . . . . . . . . . . . . . . . . . . . . 96
3.7 Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
3.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
4 Hilbert spaces 115
4.1 Inner product spaces . . . . . . . . . . . . . . . . . . . . . . . . 116
4.2 Orthonormal bases . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.3 Completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
xixxx Contents
4.4 Orthogonal projections . . . . . . . . . . . . . . . . . . . . . . . 125
4.5 Linear and bilinear forms . . . . . . . . . . . . . . . . . . . . . . 128
4.6 Weak convergence . . . . . . . . . . . . . . . . . . . . . . . . . . 135
4.7 Continuous and compact operators . . . . . . . . . . . . . . . . . 138
4.8 The spectral theorem . . . . . . . . . . . . . . . . . . . . . . . . 139
4.9* Comments on Chapter 4 . . . . . . . . . . . . . . . . . . . . . . . 150
4.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
5 Sobolev spaces and boundary value problems in dimension one 155
5.1 Sobolev spaces in one variable . . . . . . . . . . . . . . . . . . . 156
5.2 Boundary value problems on the interval . . . . . . . . . . . . . . 164
5.3* Comments on Chapter 5 . . . . . . . . . . . . . . . . . . . . . . . 176
5.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
6 Hilbert space methods for elliptic equations 181
6.1 Mollifiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
6.2 Sobolev spaces on Ω ⊆ Rd . . . . . . . . . . . . . . . . . . . . . 189
6.3 The space H1
0 (Ω) . . . . . . . . . . . . . . . . . . . . . . . . . . 196
6.4 Lattice operations on H1(Ω) . . . . . . . . . . . . . . . . . . . . 200
6.5 The Poisson equation with Dirichlet boundary conditions . . . . . 204
6.6 Sobolev spaces and Fourier transforms . . . . . . . . . . . . . . . 207
6.7 Local regularity . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
6.8 Inhomogeneous Dirichlet boundary conditions . . . . . . . . . . . 219
6.9 The Dirichlet problem . . . . . . . . . . . . . . . . . . . . . . . . 222
6.10 Elliptic equations with Dirichlet boundary conditions . . . . . . . 231
6.11 H2-regularity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
6.12* Comments on Chapter 6 . . . . . . . . . . . . . . . . . . . . . . . 236
6.13 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
7 Neumann and Robin boundary conditions 241
7.1 Gauss’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
7.2 Proof of Gauss’s theorem . . . . . . . . . . . . . . . . . . . . . . 247
7.3 The extension property . . . . . . . . . . . . . . . . . . . . . . . 254
7.4 The Poisson equation with Neumann boundary conditions . . . . . 258
7.5 The trace theorem and Robin boundary conditions . . . . . . . . . 262
7.6* Comments on Chapter 7 . . . . . . . . . . . . . . . . . . . . . . . 265
7.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
8 Spectral decomposition and evolution equations 269
8.1 A vector-valued initial value problem . . . . . . . . . . . . . . . . 270
8.2 The heat equation: Dirichlet boundary conditions . . . . . . . . . 274
8.3 The heat equation: Robin boundary conditions . . . . . . . . . . . 280
8.4 The wave equation . . . . . . . . . . . . . . . . . . . . . . . . . 283
8.5 Inhomogeneous parabolic equations . . . . . . . . . . . . . . . . 295
8.6* Space/time variational formulations . . . . . . . . . . . . . . . . 304
8.7* Comments on Chapter 8 . . . . . . . . . . . . . . . . . . . . . . . 308
8.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308Contents xxi
9 Numerical methods 313
9.1 Finite differences for elliptic problems . . . . . . . . . . . . . . . 315
9.2 Finite elements for elliptic problems . . . . . . . . . . . . . . . . 330
9.3* Extensions and generalizations . . . . . . . . . . . . . . . . . . . 355
9.4 Parabolic problems . . . . . . . . . . . . . . . . . . . . . . . . . 360
9.5 The wave equation . . . . . . . . . . . . . . . . . . . . . . . . . 379
9.6* Comments on Chapter 9 . . . . . . . . . . . . . . . . . . . . . . . 406
9.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408
10 Maple®, or why computers can sometimes help 413
10.1 Maple® . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414
10.2 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
Appendix 423
A.1 Banach spaces and linear operators . . . . . . . . . . . . . . . . . 423
A.2 The space C(K) . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
A.3 Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
A.4 More details on the Black–Scholes equation . . . . . . . . . . . . 428
References 435
Index of names 439
Index of symbols 443
Index 445List of Figures
1.1 Horizontal pipe with constant cross-section. ............. 4
1.2 Solid horizontal thin rod with constant cross-section of area A. ... 9
1.3 An elastic string fixed at both ends. . . . . . . . . . . . . . . . . . . 10
1.4 Tension in a small section of the string. . . . . . . . . . . . . . . . . 10
1.5 Displacement of a membrane subject to a force. . . . . . . . . . . . 16
1.6 Displacement of a membrane (cross-section). . . . . . . . . . . . . 16
1.7 Double oscillator . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.1 Characteristic: the solution is constant. . . . . . . . . . . . . . . . . 31
2.2 Characteristics of the linear transport equation. . . . . . . . . . . . 33
2.3 Characteristics for (2.5). . . . . . . . . . . . . . . . . . . . . . . . 34
2.4 Characteristics: transport equation with variable coefficients. . . . . 34
2.5 The set G := {(t, x) : t > 0, x < 1
t } and curve Λ. . . . . . . . . . . . 35
2.6 Rarefaction wave for Burgers’ equation. . . . . . . . . . . . . . . . 38
2.7 Burgers’ equation: shock. . . . . . . . . . . . . . . . . . . . . . . . 39
3.1 Wave equation: domain of dependence. . . . . . . . . . . . . . . . 52
3.2 ΩT together with its parabolic boundary ∂∗ΩT . . . . . . . . . . . . 77
3.3 Rectangular pulse with its Fourier transform. . . . . . . . . . . . . 98
3.4 Exponentially decaying pulse with its Fourier transform. . . . . . . 99
4.1 Points on the unit sphere. . . . . . . . . . . . . . . . . . . . . . . . 117
4.2 The orthogonal projection of u ∈ H onto the subspace F ⊂ H. . . . 126
6.1 Mollifiers in one and two dimensions. . . . . . . . . . . . . . . . . 183
6.2 Segment condition. . . . . . . . . . . . . . . . . . . . . . . . . . . 229
6.3 Depiction of the exterior sphere condition for d = 2. . . . . . . . . . 230
6.4 The Lebesgue cusp set. . . . . . . . . . . . . . . . . . . . . . . . . 230
6.5 The set Ωα,r0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
6.6 A domain Ω with a non-convex corner at z. . . . . . . . . . . . . . 235
7.1 A normal C1-graph in R2. . . . . . . . . . . . . . . . . . . . . . . . 243
7.2 A domain without the extension property. . . . . . . . . . . . . . . 256
7.3 A set Ω which does not have the extension property. . . . . . . . . . 268
xxiiixxiv List of Figures
9.1 Green’s function with respect to one variable. . . . . . . . . . . . . 319
9.2 Convergence history, finite differences in 1D. . . . . . . . . . . . . 325
9.3 Equidistant grid. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
9.4 Finite difference method on the L-shaped domain. . . . . . . . . . . 327
9.5 Equidistant mesh for curvilinear Ω. . . . . . . . . . . . . . . . . . . 328
9.6 Lexicographic ordering of the mesh points. . . . . . . . . . . . . . 329
9.7 Reduction to a reference triangle. . . . . . . . . . . . . . . . . . . . 336
9.8 Reference triangle: inradius and circumradius. . . . . . . . . . . . . 337
9.9 Points in the reference triangle. . . . . . . . . . . . . . . . . . . . . 337
9.10 Triangulation of a polygon . . . . . . . . . . . . . . . . . . . . . . 341
9.11 Non-admissible partitions. . . . . . . . . . . . . . . . . . . . . . . 342
9.12 Hat functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
9.13 Convergence history, finite elements in 1D. . . . . . . . . . . . . . 353
9.14 Test examples in 2D . . . . . . . . . . . . . . . . . . . . . . . . . . 354
9.15 Convergence history, finite elements in 2D. . . . . . . . . . . . . . 354
9.16 Heat equation: finite differences . . . . . . . . . . . . . . . . . . . 366
9.17 Horizontal (left) and vertical (right) method of lines. . . . . . . . . 366
9.18 Convergence, heat equation, Crank–Nicolson and implicit Euler. . . 377
9.19 Leapfrog method . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
9.20 Wave equation: domain of dependence. . . . . . . . . . . . . . . . 384
9.21 Wave equation, finite differences. . . . . . . . . . . . . . . . . . . . 387
9.22 Convergence of finite elements for the wave equation, 1D. . . . . . . 403
9.23 Wave equation: evolution in time . . . . . . . . . . . . . . . . . . . 405
9.24 Convergence of FE for the wave equation, 1D, u0  C0. . . . . . . . 405
9.25 Convergence rate, wave equation via finite elements, 2D . . . . . . . 406
10.1 Maple® solution of the heat equation. . . . . . . . . . . . . . . . . 417Chapter 1
Modeling, or where do differential equations
come from
Partial differential equations describe numerous phenomena in nature, technology,
medicine, economics, ... In this first chapter we shall describe the derivation of the
partial differential equations associated with several prominent examples, using the
laws of nature and mathematical facts. One calls such a derivation (mathematical)
modeling. The examples should also illustrate the variety of such partial differential
equations as they arise in diverse real problems. A first, somewhat rough classifica￾tion will be given at the end of the chapter.
Chapter overview
1.1 Mathematical modeling .............................. 2
1.2 Transport processes ................................. 4
1.3 Diffusion ...................................... 8
1.4 The wave equation ................................. 9
1.5 The Black–Scholes equation ............................ 11
1.6 Let’s get higher dimensional ............................ 13
1.7* But there’s more .................................. 19
1.8 Classification of partial differential equations ................... 25
1.9* Comments ..................................... 26
1.10 Exercises ...................................... 27
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4_1
12 1 Modeling, or where do differential equations come from
1.1 Mathematical modeling
We begin with a few basic remarks about modeling with partial differential equations.
One should however exercise caution with the use of the term “modeling”, since
modeling is undertaken in many scientific disciplines, sometimes with different
meanings. Even within mathematics the term is sometimes used in a different way
from here, such as in stochastics or financial mathematics.
1.1.1 Modeling with partial differential equations
Modeling with partial differential equations typically proceeds in three steps:
1. Specification of the process or phenomenon to be modeled
2. Application of (natural) laws
3. Formulation of the mathematical problem
In the first step, the specification of the process to be modeled, one first has to
clarify precisely which real-world process should be modeled. As an example, in this
section we will consider gas flow in the exhaust system of a vehicle. Let us assume
that one is interested in the volume and the spatial distribution of CO2 for a given
engine output. Obviously here a number of processes play a role, such as
• chemical reactions between various compounds in the air and the exhaust;
• combustion processes in the engine;
• flow of the exhaust gases through the exhaust pipe.
It should quickly become clear that the modeling of the whole process is very
complex. Thus one often makes simplifying assumptions. For example, one could
start by ignoring the reaction processes and the combustion and concentrate purely
on the flow problem.
The second step is the application of (natural) laws: we need to establish the
relationships between the quantities (e.g., physical ones) which are necessary to
describe the process. This is usually done via the application of principles which
are known from the relevant discipline, for example natural laws (e.g. force is mass
times acceleration, Newton’s second law).
However, often the application of such laws alone is not enough to yield a partial
differential equation; generally, some mathematical theory is still required. This could
be in the form of integral transforms, passage to the limit or basic theorems from
analysis. Only after this step does one (sometimes with considerable creativity) reach
a formulation of the mathematical problem, whose solutions describe the process
being modeled. Here, once again, one often needs to make simplifying assumptions,
for example that functions arising in the problem are differentiable sufficiently often.
In the example of the exhaust gas flow mentioned above, by invoking the principles
of conservation of mass and conservation of momentum as well as Newton’s third law1.1 Mathematical modeling 3
(the action-reaction law), one obtains the Navier–Stokes equations, the fundamental
equations of fluid and gas dynamics, cf. Section 1.7.4.
Our derivations are not only designed to illustrate the utility of the theory of partial
differential equations in understanding natural processes; we are also interested in
the other direction. Understanding the physical situation described by a certain
equation can help enormously with understanding its mathematical properties and
with developing a mathematical intuition for the problem.
One of the equations which we will consider does not describe a natural process,
but rather a problem from finance. The solutions of the Black–Scholes equation
specify the fair price of an option which gives the right to buy a given stock for a
determined price at a future point in time, cf. Section 1.5. Our derivation here will
use the same approach used by Black, Scholes and Merton, namely via stochastic
differential equations. It will not be possible to give a complete derivation of all the
necessary mathematics here; however, the arguments given here should help to make
the model more transparent.
1.1.2 Modeling is only the first step
Of course, one is not finished after deriving a partial differential equation to describe
a process. The equation might have infinitely many solutions, or it may have none at
all. In order to answer these and other natural questions, one needs to perform a math￾ematical analysis of the equation. In addition to the question of the well-posedness of
the equation (in the sense of Hadamard, that is, existence, uniqueness and continuous
dependence on the data) one generally wishes to describe the qualitative behavior
of the solution(s), or to reduce the problem or the equation to one which is already
understood. This mathematical analysis of partial differential equations is a central
topic of this book.
If one wishes to understand the process more fully or, say, optimize certain
parameters in the problem, then one needs, in addition to the mathematical analysis
of the problem, some concrete representation of the solution. In some cases this can
be given explicitly via a formula (one then speaks of an analytic solution), but often
the equation cannot be solved explicitly. One can then use approximation methods
(usually on the computer), that is, numerical methods. In this book we will also deal
with this aspect of partial differential equations.
Let us return to the example of exhaust gas flow one last time. The Navier–
Stokes equations cannot be solved analytically. It turns out that if one uses suitable
numerical iteration methods, one very often ends up having to solve linear partial
differential equations of second order, in particular the Poisson equation. Linear
partial differential equations of second order will be treated intensively in this book.
They also represent a springboard to more complex equations, such as those which
we will at least briefly introduce in Section 1.7*.4 1 Modeling, or where do differential equations come from
1.2 Transport processes
Consider a narrow pipe P with constant cross-section of area A ∈ R+ (in m2),
orientated so as to be parallel to the x-axis. In what follows we will consider a
section of this pipe with x-values of the form [a, b] with a, b ∈ R, a < b, cf.
Figure 1.1.
a b x
P
A
Fig. 1.1 Horizontal pipe with constant cross-section of area A ∈ R+.
Suppose that the pipe is filled with flowing water. We assume that the pipe is very
thin, that is, the cross-sectional area A is taken as being “small”. This means that we
only need to consider the flow in the horizontal direction; all other directions may
be ignored.
1.2.1 Conservation laws
We wish to describe the flow of water through the pipe P mathematically. To this end
we denote by u = u(t, x) the density (measured in kg/m3) of the water at position
x ∈ (a, b) and at time t. The total mass of water in the interval [x, x + Δx]⊂[a, b]
(with Δx sufficiently small) at time t is thus given by
∫ x+Δx
x
u(t, y) A dy. (1.1)
We now want to describe the flow of water in the time period from t to t +Δt, Δt > 0,
t ∈ R. The difference in the mass of water in the section of pipe [x, x + Δx] at the
two points in time is clearly
∫ x+Δx
x
(u(t + Δt, y) − u(t, y)) A dy. (1.2)
How can the mass of water to change between times t and t + Δt? There are two
possible causes, namely
• the incoming and outgoing flow of water;
• any sources or sinks.1.2 Transport processes 5
In physics, the term flux is used to describe the amount (and direction) of any
quantity such as mass, energy, number of particles etc., which passes through a given
surface per unit of time. In our case, we denote the flux of water by ψ(t, x), that is,
this is how much water is flowing through the cross-section of pipe, per second and
square meter, at time t and at the point x. Hence
∫ t+Δt
t
Aψ(τ, x) dτ
is the total mass of water which flows through the pipe in the time interval [t, t +Δt].
We can describe any sources and sinks via a function f = f (t, x) which specifies
how much water is added to (or removed from) the pipe, per second and square
meter, in a section of length 1 at time t and at the point x. Thus
∫ t+Δt
t
∫ x+Δx
x
f (τ, y) A dy dτ (1.3)
is the total mass of water which is added (or removed) in the section of pipe [x, x+Δx]
in the time interval [t, t + Δt]. If f > 0, then we speak of a source; if f < 0, then f
represents a sink.
Now one of the fundamental principles of physics asserts that in a closed system
mass can neither be created nor destroyed; this is the principle of conservation of
mass. We can formulate this mathematically in the form of a conservation law. In
words, this principle states that
change in total mass of water = inflow − outflow + sources.
For all terms in this equation we have derived corresponding mathematical expres￾sions, in (1.2) and (1.3). If we bring everything together, then we obtain the following
equation in the section of pipe [x, x + Δx]:
∫ x+Δx
x
A(u(t + Δt, y)−u(t, y))dy =
=
∫ t+Δt
t
(Aψ(τ, x)− Aψ(τ, x + Δx)) dτ +
∫ t+Δt
t
∫ x+Δx
x
f (τ, y) A dy dτ (1.4)
On the left-hand side we have the difference in the mass of water in the section of
pipe [x, x + Δx] between times t + Δt and t; on the right-hand side we have the
total inflow (or outflow) through the cross-section x, minus the inflow (or outflow)
through the cross-section x + Δx, during the time interval [t, t + Δt], to which is
added the mass of water coming from sources (or removed by sinks) in the time
interval [t, t + Δ]. Hence (1.4) is, as intimated, a conservation law.6 1 Modeling, or where do differential equations come from
1.2.2 From a conservation law to a differential equation
Now u and ψ can vary strongly in time and space. The equation (1.4) becomes
more precise as the time and space intervals become smaller. Let us suppose that the
functions u and ψ are continuously differentiable. We start by dividing (1.4) by (AΔt)
and passing to the limit as Δt → 0. We may then exchange the order of integration
and differentiation to obtain
∫ x+Δx
x
∂
∂t
u(t, y) dy = ψ(t, x) − ψ(t, x + Δx) +
∫ x+Δx
x
f (t, y) dy.
If we next divide by Δx and pass to the limit as Δx → 0, then our equation
becomes
∂
∂t
u(t, x) = − ∂
∂x
ψ(t, x) + f (t, x), (1.5)
provided f is continuous in t and x. We often write (1.5) in the following abbreviated
form:
ut + ψx = f . (1.6)
It is clear from the derivation that this is a pure conservation equation. It should
also be clear that we can argue completely analogously when u does not represent
water density but rather the density of some other quantity such as energy, charge,
bacteria, particles, automobiles, molecules, etc. The pipe could then be, for example,
a conductor, an artery, a road or a nerve tract. For this reason, in general u = u(t, x)
is often referred to as a state variable, as it describes the state of the system.
In many models the flux ψ(t, x) depends in a particular way on the density u(t, x),
that is, ψ(t, x) = φ(t, x, u(t, x)) for some function φ : [0, ∞) × R × R → R. In this
case, (1.6) becomes a partial differential equation in the unknown function u. In the
following three sections we will meet examples of such a dependency. A further,
particularly important, example is the case of diffusion, where ψ depends on the
derivative in the space variable(s), ux; see Section 1.3. But there can very easily
be other kinds of dependence, for example, on temperature, velocity, acceleration,
concentration etc.
1.2.3 The linear transport equation
Let us return to the example of the water pipe. We will suppose that f = 0, that is,
that there are no sources and no sinks. We will also assume that the water is moving
with constant velocity c ∈ R. The flux thus has the form ψ(t, x) = c · u(t, x), and the
flux function in this case takes the form1.2 Transport processes 7
φ = φ(t, x, u) := c · u(t, x), c ∈ R. (1.7)
This function is linear in u, that is, the flux is proportional to the density u. Hence
(1.6) reduces to
ut + c ux = 0. (1.8)
This equation is known as a linear transport equation (also convection equation or
advection equation). Clearly, (1.8) is a homogeneous differential equation.
1.2.4 The convection-reaction equation
Let us assume that the state u decays in time at a constant rate λ < 0, perhaps via
a radioactive decay process. This corresponds to the ordinary differential equation
ut = λu, whose solution is u(t, x) = u(0, x) eλt
. This can be expressed as a source
term via
f (t, x, u(t, x)) := λ · u(t, x) (1.9)
In this case, that is, taking (1.9) as the flux function, (1.6) becomes
ut + cux − λu = 0. (1.10)
This equation (which is still linear and homogeneous in u) is sometimes called a
convection-reaction equation. All terms of zeroth order in the unknown state u are
referred to as reaction terms.
A notable feature of the equations (1.8) and (1.10) is that both are linear in u.
This is of course the case in only very few realistic problems; often, linear equations
represent a highly simplified model of reality.
1.2.5* Burgers’ equation
Here we wish to introduce a first nonlinear model. Let u(t, x) be the density of traffic, that is, the
number of vehicles at time t and location x on a single-lane road. In order to be able to model
traffic jams, we may take as a simple first ansatz for the flux function
ψ = ψ(u) = α · u · (β − u),
where α, β > 0 are certain constants. The following considerations show that this is indeed a first,
simple model for traffic congestion. If the density u of traffic is not too high, that is, 0 ≤ u 
 β,
then ψ is approximately proportional to u, ψ ≈ αβu, so that the flow is approximately the same as
in the linear transport equation (1.8) with transport velocity c = αβ. This matches our intuition:
if there are few vehicles underway, then the flow of traffic is smooth and uniform. However, if the
density of traffic is higher, say, u ≈ β, then ψ ≈ 0; the increasing density of vehicles causes the flow
of traffic to break down. In the absence of source terms (which means that there are no bifurcations8 1 Modeling, or where do differential equations come from
or intersections along the road) the equation for the traffic density is
ut + α(u · (β − u))x = 0. (1.11)
In this form the equation is somewhat unwieldy. If, however, we introduce the change of variables
v(t, x) := β − 2u
 t
α, x

, (1.12)
then (we omit the arguments to improve readability)
vt + vvx = − 2
αut + (β − 2u)(−2ux ) =

− 2
α

(ut + αβux − 2αuux )
=

− 2
α

(ut + α(u(β − u))x ).
By (1.11), this means that
vt + vvx = 0. (1.13)
This equation is called Burgers’ equation, named after the Dutch physicist Johannes Martinus
Burgers (1895–1981). It was originally introduced in 1915 by the English mathematician Harry
Bateman (1882–1946). Now we have vvx = 1
2
∂
∂x v2 = 1
2 (v2)x , which means that the flux is given
by ψ(t, x, v) := 1
2 v(t, x)
2. The equation (1.13) is also known as a nonlinear transport equation.
Additional viscosity. Of course it is possible to refine this model. One such possibility is to
make the assumption that drivers will not just reduce their speed when the traffic density is already
very high (that is, when u ≈ β), but already whenever the traffic density increases, that is, when
ux > 0. We may thus add the term −ε˜ux to φ in the above model, where ε >˜ 0, so that now
φ(t, x, u, ux ) = αu(t, x)(β − u(t, x)) − ε˜ux (t, x). As above we transform u into the function v
given by (1.12) and obtain
vt + vvx = εvx x (1.14)
with the viscosity coefficient ε := ε˜
α > 0. We call (1.14) the viscous Burgers’ equation.
The difference between (1.14) (also known as the inviscid Burgers’ equation) and the previous
equations is that in (1.14) a term of second order, namely vx x , appears, which means we are dealing
with a second-order partial differential equation.
1.3 Diffusion
Instead of the pipe of Figure 1.1 we will now consider a solid rod R whose cross￾section again has very small area A ∈ R+; see Figure 1.2.
We are interested in the temperature θ = θ(t, x) at time t and at the point x ∈ [a, b].
We will assume that the rod is homogeneous, that is, that its density ρ ∈ R+ is
constant. As usual, density means mass per unit of volume; here, density is measured
in kg/m3.
To derive the heat equation we need the notion of the specific heat capacity c of
a body. This is defined as the energy (measured here in joules) that must be added
to one unit of mass (here 1 kg) in order to increase its temperature by one unit of1.4 The wave equation 9
a b x
R
A
Fig. 1.2 Solid horizontal thin rod with constant cross-section of area A.
temperature (here 1 Kelvin); it is measured in J/(kg K). The temperature is thus
proportional to the amount of heat per unit of mass, and so it corresponds to the
density from Section 1.2.1.
The flux ψ describes how much heat is transferred (“flows”) through the cross￾section of the rod per second and square meter; as before, we have the conservation
law θt(t, x) + ψx(t, x) = 0. The law of heat conduction (Fourier’s law) states that at
every point x the rate of heat transfer ψ at x is proportional to the rate of decay of
the temperature, −θx, that is, ψ(t, x) = −k(x) θx(t, x). If the factor of proportionality
k(x), also known as the thermal conductivity, is independent of x, then we obtain
the heat equation
θt − k θxx = 0. (1.15)
This equation represents the simplest possible model of heat diffusion. A first
refinement can be made by dropping the assumption that k is constant on the whole
rod R. If k depends on x, then (1.15) becomes
θt − (k(x) θx)x = θt − k
(x) θx − k(x) θxx = 0. (1.16)
Thus both first and second order derivatives in the space variable(s) appear, although
the equation is still linear in θ. If we assume that the thermal conductivity is addi￾tionally dependent on the temperature, k = k(x, θ), then (1.16) becomes a nonlinear
equation of the form θt − (k(θ) θx)x = 0.
1.4 The wave equation
In order to derive our third (and final) type of spatially one-dimensional equation
we consider a perfectly elastic string with constant density ρ0, which is fixed at both
ends, cf. Figure 1.3. We denote by S the constant tension of the string.
We bring the string from its resting position (for example, by plucking it) and
wish to determine the vertical displacement u = u(t, x) in dependence on time and
horizontal position. In doing so, we assume that the displacement is “small”, so that
horizontal movement can be neglected. We also assume that the string can return10 1 Modeling, or where do differential equations come from
a b x
Fig. 1.3 An elastic string fixed at both ends.
to its original position, that is, that no plastic changes to its shape occur. We now
consider a small part [x, x + Δx] of the string, as depicted in Figure 1.4.
a b
x
u(x )
x x + Dx
S(x )
S(x + Dx )
b
a
Fig. 1.4 Tension in a small section [x, x + Δx] of the string.
All we need for our derivation is Newton’s second law
force = mass × acceleration,
named after Sir Isaac Newton (1642–1727). The acceleration in the vertical direction
is equal to the second derivative of the vertical displacement of u with respect to the
time variable, that is,
acceleration = ∂2
∂t2 u(t, x) = utt(t, x).
Thus the force acting on the section of the string with length Δx is given by
(ρ0 Δx) utt(t, x). In order to obtain a differential equation for the displacement u(t, x),
we first need to derive a relationship between the force and the tension S. Figure 1.4
shows the tangential components S(x) and S(x + Δx) of S at the points x, x + Δx ∈
[a, b]. From these, we can also obtain the horizontal components of S, which due to
the assumption that S is constant have the form
S(x + Δx) cos β = S(x) cos α = S. (1.17)
The vertical components can equally be ascertained using Figure 1.4, and the differ￾ence of the two agrees with the force which we determined using Newton’s second
law:1.5 The Black–Scholes equation 11
S(x + Δx) sin β − S(x) sin α = (ρ0 Δx) utt(t, x). (1.18)
We wish to derive a relationship with u(t, x). Once again using Figure 1.4, we see
that tan α = ux(t, x) and tan β = ux(t, x + Δx). If we divide (1.18) by S and invoke
(1.17), then we are led to
ρ0 Δx
S utt(t, x) = tan β − tan α = ux(t, x + Δx) − ux(t, x).
Division by ρ0 Δx
S and passing to the limit as Δx → 0 finally yields
utt − c2 uxx = 0, (1.19)
the wave equation with wave speed
c2 = S
ρ0
, c > 0.
This equation was originally discovered by Jean-Baptiste le Rond d’Alembert in
1746.
1.5 The Black–Scholes equation
The majority of the partial differential equations considered thus far have their origin
in the natural sciences. We now wish to briefly describe a famous equation from
another discipline, namely economics, or more precisely financial mathematics. The
underlying model was first introduced in 1973 by the American economist Fischer
Sheffey Black and the Canadian economist Myron Samuel Scholes. The American
mathematician and economist Robert Carhart Merton was also involved in their
work but was not a co-author of the seminal paper from 1973, and hence his name is
often, wrongly, omitted in this context. In 1997, Merton and Scholes were awarded
the Nobel Memorial Prize in Economic Sciences “for a new method to determine the
value of derivatives”, to quote the official citation. Fischer Black had died in 1995,
two years before the prize was awarded; however, he was recognized posthumously
during the award ceremony.
An option depends first of all on an underlying such as for example an exchange
rate, a share or block of shares, or the price of an asset which is traded on an exchange.
In any case, this underlying has a market value, which we will denote by S(t). This
function is an example of a stochastic process (see Appendix A.4).
An option on this underlying is a financial product (a contract between a provider
such as a bank, and a client) which guarantees the owner of the option the right (but
not the obligation) to buy or sell the underlying at a given time T (the maturity) for a
previously agreed price K (the strike). If the right is to buy, then we speak of a call
option; if the right is to sell then it is a put option. Here we will only consider the case12 1 Modeling, or where do differential equations come from
where the underlying can only be bought or sold at the maturity date; such an option
is called a European option, although the adjective has no geographical significance
whatsoever. These days there is a large number of complex financial products, whose
study would take us far too far afield; for details we refer for example to [15].
We wish to determine the “fair” price V(0, y) of an option in dependence on the
current share (or asset) price y, where “fair” means that at this price there are no
riskless profit opportunities: no-one can make gains from trade without incurring
the risk of a loss; in this case we speak of an “arbitrage-free” valuation. In doing so
it will be opportune to consider, for each point in time t ∈ [0, T], which price V(t, y)
at that point in time t ∈ [0, T] and share price y would in fact be fair.
We shall consider the case of a European call on an underlying, say a share, that
is, the right to buy the share for an agreed price K at time T. One is hence speculating
on rising share prices. If the share price at time T is lower than K, then the option is
worthless, since one could buy the same share on the free market for a more favorable
price. If, however, the price S(T) of the underlying is above K at time T, then the
underlying can be resold immediately on the stock exchange for the current price
S(T) > K. Hence at the time of maturity the option has the value V(T, S(T)), where
V(T, y) = (y − K)
+ :=

y − K if y > K,
0 otherwise. (1.20)
We also call this function the payoff (or payoff function). Clearly, (1.20) is a terminal
condition, as opposed to an initial condition of the type we saw for instance in the
case of the transport equation. We also consider that the Black–Scholes equation
may be interpreted as running “backwards in time”.
The modeling requires some background in financial mathematics and is de￾scribed in more detail in Appendix A.4. Finally, one arrives at the Black–Scholes
equation
Vt +
1
2
σ2 y2
Vyy + r yVy − rV = 0, (t, y)∈(0, T) × R, (1.21)
for the unknown function V : (0, T) × R+ → R+, where σ > 0 is a constant related
to the volatility of the financial product and r > 0 is a fixed interest rate, see
Section A.4.3. We will see in Section 3.5 that the equation (1.21) admits a unique
polynomially bounded solution V which takes on the final value prescribed in (1.20).
The initial value V0 := V(0, S0) of this solution V at the point y = S0 with the known
share price S0 at initial time t = 0 is the fair price of the option. In Section 10.1.5,
we will give a Maple®-sheet for this problem.1.6 Let’s get higher dimensional 13
1.6 Let’s get higher dimensional
In both the above examples of the pipe and the string we neglected the vertical
component and thus obtained a partial differential equation in one space dimension.
The Black–Scholes equation, too, is a time-dependent one-dimensional equation
since y ∈ R is a one-dimensional variable. Of course, in many real situations this
approach represents an oversimplification. We thus now wish to consider the case of
more than one space variable.
1.6.1 Transport processes
As above, we begin by describing transport processes. Here, however, we replace
the pipe P from Figure 1.1 by a general domain Ω ⊂ Rd, d = 2, 3, in which the
transportation should take place. We again let u = u(t, x) : [t1, t2] × Ω → R denote
the density, which is allowed to depend on both time and space. We shall consider a
conservation law on a control volume V ⊂ Ω, which may be for example a rectangle
or box parallel to the coordinate axes, or a ball. Then just as in (1.1) the mass in V
is given by
∫
V
u(t, x) dx. (1.22)
In the absence of sources and sinks, the principle of conservation of mass states that
any change in mass can only be caused by in and outflows. In terms of V this means
in and outflows through its boundary ∂V. We express this change as a flux function
ϕ = ϕ(t, x), ϕ = (ϕ1,...,ϕd), which specifies what quantity of the substance in
question (such as water) exits per second and square meter through a small part of
the surface. More precisely, ϕ : R+ × Ω → Rd is a (continuously differentiable)
function for which
∫ t+Δt
t
∫
∂V
ϕ(s, z) ν(z) dσ(z) ds
gives the mass of the substance which exits through the surface ∂V of a small control
volume V during the time interval [t, t + Δ]. Here V should be a small set with
C1-boundary such that V ⊂ Ω. We denote by ν(z) the outer unit normal vector to V
at the point z ∈ ∂V, and by dσ the surface measure on ∂V (see Chapter 7 for the
precise definitions). Our conservation law (in the absence of sources and sinks) then
reads
∫
V

u(t + Δt, x) − u(t, x)
	
dx = −
∫ t+Δt
t
∫
∂V
ϕ(s, z) ν(z) dσ(z) ds.14 1 Modeling, or where do differential equations come from
Here the left-hand side gives the difference between the quantity of the substance
in V at time t + Δ and at time t; the right-hand side expresses the quantity of the
substance which flows through the surface during this time. If we divide this equation
by Δt and let Δt tend to 0, then we obtain (assuming sufficient differentiability of the
functions involved)
∫
V
ut(t, x) dx +
∫
∂V
ϕ(t, z) ν(z) dσ(z) = 0.
By the divergence theorem (see Corollary 7.6),
∫
∂V
ϕ(t, z) ν(z) dσ(z) =
∫
V
div ϕ(t, x) dx,
where div ϕ := 
d
i=1
∂
∂i ϕi(x) refers to the space variables. We thus obtain
∫
V
ut(t, x) dx +
∫
V
div ϕ(t, x) dx = 0
for every control volume V. Now any continuous function f : Ω → R satisfies
f (x) = lim
ε↓0
1
|B(x, ε)| ∫
B(x,ε)
f (y) dy,
where x ∈ Ω, B(x, ε) := {y ∈ Rd : |x − y| < ε}, and |B(x, ε)| is the volume of
B(x, ε). We finally arrive at the infinitesimal version
ut(t, x) + div ϕ(t, x) = 0 (1.23)
of the conservation law. This passage to the limit is called localization. We now
obtain a partial differential equation for u, provided ϕ can be expressed as a function
of u. If for example a liquid is flowing in the direction b = (b1,..., bd)
T (where
|b| = (b2
1 + ··· + b2
d)
1/2 = 1) with constant speed c > 0, then the flux has the form
ϕ(t, x) = c b · u(t, x). In this case, the conservation law reads
ut(t, x) + c b · ∇u(t, x) = 0,
where ∇u(t, x) =  ∂u
∂x1
,..., ∂u
∂xd
	T is the gradient in the space variables. This is the
transport equation, which describes a pure transport process in a fixed direction and
with constant speed.
1.6.2 Diffusion processes
In the case of diffusion processes, the flux ϕ in the conservation law (1.23) depends
on the rate of change of the particle density. In the simplest case this function is of1.6 Let’s get higher dimensional 15
the form ϕ(t, x) = −c(x) ∇ u(t, x). This means that the particles move away from
locations of higher density towards sets where the density is lower. If we substitute
this expression into (1.23), then we obtain ut(t, x) = div
c(x) ∇u(t, x)
	
. If the factor
of proportionality c is independent of the space variable x, c ≡ c(x), then this
becomes the heat equation
ut(t, x) = c Δu(t, x).
This equation describes chemical diffusion processes (for example ink in water) as
well as the diffusion of heat. But in population models as well (such as for bacteria)
it is possible to observe such a diffusion: each individual observes the population
density in its vicinity and has the tendency to migrate in the direction of the greatest
decrease in this density (given by the gradient of u).
1.6.3 The wave equation
In Section 1.4 we met the wave equation in one space dimension as a model for the
displacement of a string. If instead of a string we consider a taut elastic membrane,
then we can derive the corresponding higher-dimensional wave equation with similar
arguments. The components of the tension in (1.17) and (1.18) now need to be
considered in every direction, and so uxx must be replaced by Δu. Hence the general
form of the wave equation reads
utt − c2Δu = f . (1.24)
1.6.4 Laplace’s equation
We now turn to the derivation of Laplace’s equation, an equation that will accompany
us for a large part of the book.
We consider an elastic membrane (without bending strength), such as the skin
of a drum. Suppose that the membrane has the form of a two-dimensional domain
Ω ⊂ R2, and that the membrane is fixed, and held taut, along its boundary. Now
suppose that a vertical force f : Ω → R, measured in N/m2, acts on the membrane;
we are interested in the vertical displacement u : Ω → R, measured in m. Since the
membrane is fixed at the boundary, we obtain the boundary condition
u|∂Ω = 0, (1.25)
which means that u(x) = 0 for all x ∈ ∂Ω and we can thus implicity assume that
u ∈ C(Ω), where Ω = Ω ∪ ∂Ω (Figure 1.5).
Now we will apply Newton’s first law, the principle of inertia, which states that
a body remains at rest as long as the sum of all forces acting on it is zero. In other16 1 Modeling, or where do differential equations come from
Fig. 1.5 Displacement of a membrane Ω subject to a vertical force.
words, a body behaves in such a way as to minimize its internal energy; for this
reason a fixed membrane does not have any “bumps”.
The total energy J is the sum of the strain energy J1 and the potential energy J2,
J = J1 + J2. We will derive expressions for these two components separately. By
Hooke’s law, to deform an elastic body a force F is necessary which is proportional
to the deformation s, that is, F = αs, where α is often called elasticity (in the case of
a spring, this is the spring constant). The energy stored in a body results from work
performed on the body. Work, in turn, is the product of force and displacement,
work = force × displacement. (1.26)
Fig. 1.6 Displacement of a membrane (cross-section): the section of curve given by the graph of u
in [x, x + Δx] is approximated by the dotted line segment of length Δx

1 + u(x)2.
Thus the strain energy is proportional to the deformation of the surface area, with
factor of proportionality α. The surface area of the membrane at rest is ∫
Ω 1 dx, that is,
the Lebesgue measure of the domain Ω. We start by considering the one-dimensional
case (which may also be thought of as a cross-section of the higher-dimensional case)
and an interval [x, x+Δx], cf. Figure 1.6. Here the length of the string at rest is Δx, or1.6 Let’s get higher dimensional 17
∫ x+Δx
x

1 + u
(s)2 ds after displacement. This curve integral may be approximated
by the line through (x, u(x)) with slope u
(t), that is,
∫ x+Δx
x

1 + u
(s)2 ds ≈ Δx

1 + u
(x)2,
cf. Figure 1.6. For functions of several variables this approximation is given analo￾gously by dx
1 + |∇u(x)|2 (with the usual Euclidean norm |x|
2 := x2
1 + ··· + x2
d,
x = (x1,..., xd)
T ∈ Rd). We now integrate over Ω in order to obtain the change in
the surface area and hence the strain energy:
J1 = J1(u) ≈ α
∫
Ω

1 + |∇u(x)|2 − 1

dx. (1.27)
Since J1 clearly depends on the function u (the displacement), we sometimes refer to
J1 as an energy functional. We wish to simplify this expression further. For x close
to zero we can use the linear Taylor approximation √
1 + x = 1 + 1
2 x + O(x2). When
the distortion |∇u(x)| is small we thus have

1 + |∇u(x)|2 − 1 ≈ 1
2
|∇u|
2
for the gradient ∇u. We thus obtain the following expression for the strain energy (at
least approximately, here within the scope of the theory of linear elasticity)
J1(u) ≈ α
2
∫
Ω
|∇u|
2 dx. (1.28)
The potential energy is generated by the external force F and determined by (1.26)
as the product of force and displacement as
J2(u) = −
∫
Ω
f (x) u(x) dx. (1.29)
Our overall goal is thus to minimize the energy functional
J(u) = α
2
∫
Ω
|∇u|
2 dx −
∫
Ω
f (x) u(x) dx. (1.30)
As usual, to determine a minimum (or maximum) we consider the zeros of the
first derivative. In the case of a functional, this means that the first variation should
vanish, that is,
d
dε J(u + εv)|ε=0 = 0 (1.31)
for every possible displacement v for which v|∂Ω = 0. So we wish to determine the
first variation. For the second term in (1.30) we have18 1 Modeling, or where do differential equations come from
d
dε
∫
Ω
f (x) (u(x) + εv(x)) dx =
∫
Ω
f (x)v(x) dx
independently of ε. For the first term we obtain
d
dε
∫
Ω
|∇(u + εv)|2 dx = 2
∫
Ω
∇(u + εv)·∇v dx ε→0 −→ 2
∫
Ω
∇u · ∇v dx,
whence
d
dε J(u + εv)|ε=0 = α
∫
Ω
∇u · ∇v dx −
∫
Ω
f (x)v(x) dx = 0. (1.32)
We then modify the first term by integrating by parts and exploiting the fact that
u|∂Ω = v|∂Ω = 0:
∫
Ω
∇u · ∇v dx =

d
i=1
∫
Ω
∂
∂xi
u ∂
∂xi
v dx
=

d
i=1
∫
∂Ω
v ∂
∂xi
u νi ds −

d
i=1
∫
Ω
∂2
∂x2
i
u v dx =
∫
Ω
(−Δu)v dx,
where ν = (ν1,...,νd) is the outer unit normal to Ω. Hence the condition that
−α
∫
Ω
(Δu)v dx =
∫
Ω
f v dx (1.33)
for all (sufficiently smooth) functions v : Ω → R such that v|∂Ω = 0, is necessary for
u to minimize the energy functional J. This implies that
−αΔu = f in Ω (1.34)
with what are known as Dirichlet boundary conditions
u|∂Ω = 0 on ∂Ω. (1.35)
We have shown that the boundary value problem (BVP) (1.34), (1.35) is the Euler–
Lagrange equation of the minimization problem
u = arg min{J(v) : v displacement with v|∂Ω = 0}.
We call the equation (1.34) Poisson’s equation; if f = 0, then
Δ u = 0 in Ω (1.36)
is Laplace’s equation; we call C2-solutions of (1.36) harmonic functions. We refer
to Theorem 4.24 for an abstract treatment of Poisson-type problems, to Section 6.51.7* But there’s more 19
for a systematic treatment of Poisson’s equation, as well as to Sections 9.1 and 9.2
for its numerical approximation.
1.7* But there’s more
The equations which we have introduced to date will be studied over the course of this book. Of
course, these equations are only a small sample of the immeasurable number of partial differential
equations which arise in real problems. This section is devoted to collecting a few additional
prominent examples; however, we will neither go into any detail as regards the modeling leading to
them, nor will we give a mathematical analysis of these equations at any later point.
1.7.1 The KdV equation
This equation in one spatial variable was first suggested by Joseph Boussinesq in 1877. It was
rediscovered in 1895 by Diederik Korteweg and Gustav de Vries as a means to describe and analyze
waves on shallow water in narrow channels. It reads
ut − 6 u ux + uxxx = 0 (1.37)
and, as one sees, is a nonlinear third-order equation. The equation originally derived by Korteweg
and de Vries had a somewhat different form; however, their version can be transformed into the
now far more common form (1.37). The equation is often referred to under the abbreviation KdV
equation. As noted above, we will forgo a description of its derivation.
The KdV equation delivers a mathematical explanation of an experimental observation. We have
all seen two effects when watching waves: the wave can spread out (like ripples in a pond; in this
case we speak of dispersion), and it can break. Dispersion of waves is a linear effect, while breaking
can only be explained as a nonlinear phenomenon. The two effects would seem to be mutually
exclusive; it was thus all the more surprising when in 1834 the young British engineer John Scott
Russell observed that the effects can balance each other and lead to waves which propagate without
changing their shape. Such waves are called solitons and can be described mathematically by the
profile
u(t, x) = A

sech x − vt
L
2
,
where L is the breadth of the wave, v its velocity and A its amplitude.1 These solitons are in fact
solutions of the KdV equation, meaning that it does indeed represent a mathematical justification
of the phenomenon observed by Russell. Such standing waves can occur, for example, in tsunamis.
A further area of application of the KdV equation surrounds what is often called the Fermi–
Pasta–Tsingou–Ulam experiment, named after the American mathematician Mary Tsingou (born
1928), the Italian nuclear physicist Enrico Fermi (1901–1954), the American physicist and computer
scientist John R. Pasta (1918–1984) and the Polish mathematician Stanisław Marcin Ulam (1909–
1984). Up until 1955 people were convinced that the energy of a system of coupled oscillators subject
to a small nonlinear perturbation would distribute uniformly among all of the natural frequencies
of the system. Hence the results of the computer experiments of Fermi, Pasta, Tsingou and Ulam in
1955 were highly surprising. The three showed that the energy distribution displays quasi-periodic
1 Here sech(x) = cosh−1(x) denotes the hyperbolic secant function.20 1 Modeling, or where do differential equations come from
behavior, that is, the energy distribution almost always returns back to the initial distribution. It
took until 1965 for the groundwork of the explanation of this phenomenon to be laid, when Martin
David Kruskal and Norman J. Zabusky managed to show that the Fermi–Pasta–Tsingou–Ulam
experiment can be described by the KdV equation.
The mathematical solution of the KdV equation is due to Clifford Gardner, John M. Greene,
Martin D. Kruskal and Robert Miura in 1967, who used inverse scattering theory from quantum
mechanics and thus connected two previously completely unrelated areas of mathematics. This also
led to the recognition of the connection between the KdV equation and the Schrödinger equation.
Peter David Lax would eventually develop a unified mathematical approach to the two, which could
also be applied to other soliton equations.
1.7.2 Geometric differential equations
Differential equations whose origin is in a geometric variational problem are often called geometric
partial differential equations. Such equations are generally nonlinear, and the mathematical analysis
of such equations is a subject of ongoing research. Here we wish to give a brief introduction to two
examples.
The Monge–Ampère equation
Gaspard Monge (1746–1818) is, among other things, credited with the invention of descriptive
geometry. He studied the problem of soil (or more generally mass) transportation, as arose for
example in public works involving excavation and/or filling of earth. It was this context that Monge
introduced the first form of the partial differential equation in 1784 that would later bear the name
Monge–Ampère equation. The French physicist André-Marie Ampère (after whom the SI base unit
for electric current is named) considered this nonlinear partial differential equation in 1820 when
studying the geometry of surfaces.
The general form of the equation reads
det(H(u)) = f, (1.38)
where
H(u) = (uxi, xj )i, j=1, . . .,d = 



ux1, x1 ··· ux1, xd
.
.
. .
.
.
uxd, x1 ··· uxd, xd




= D2(u)
is the Hessian matrix (or Hessian, for short) of the unknown function u : Ω → R (which may for
example be a parametrization of a surface). The right-hand side
f = f (x, u, ux1,..., uxd ) : Ω × R × Rd → R
is a given function (for example, the curvature of a surface). In the two-dimensional case (d = 2),
with the notation (x1, x2) =: (x, y) the equation can be simplified to ux x uy y − u2
x y = f . We say
that this equation is fully nonlinear, since it is nonlinear (here quadratically) in all terms of the
highest order derivatives (here the second derivatives).
If we interpret f as a given curvature, then the solutions of the Monge–Ampère equation
describe surfaces with this curvature (this problem is also known as the Minkowski problem). The
problem was solved in 1953 by Louis Nirenberg. A further, unexpected, application of the complex1.7* But there’s more 21
Monge–Ampère equation was found in 1978 in the area of string theory, in the context of what are
known as Calabi–Yau manifolds.
The minimal surface equation
A surface M ⊂ R3 is said to be a minimal surface with boundary ∂M = Γ if M has minimal
surface area among all surfaces with the same, given, boundary Γ. One might imagine a soap film
which does not enclose any air (that is, without bubbles). If u again denotes the displacement, this
time of a surface from a flat horizontal position, then a minimal surface is by definition a minimizer
of the surface area functional
A(x) =
∫
Ω

g(u(x)) dx, g(u) := det H(x), ∂Ω = Γ, u|Γ = 0,
where H(x) is the Hessian, as above. The Euler–Lagrange equation associated with this functional
is called the minimal surface equation and reads
(1 + u2
y )ux x − 2ux uy ux y + (1 + u2
x )uy y = 0.
1.7.3 The plate equation
When we derived Laplace’s equation we were modeling an elastic membrane. Since the membrane
may be regarded as being “thin”, we could ignore any and all bending resistance in the material.
If instead of a membrane we consider a clamped plate (with a certain thickness), then we can no
longer justify this simplification. Let us again assume that there is a vertical force acting on the
plate which may be described by a function f : Ω → R, and that the geometry of the plate may be
described by the domain Ω.
A derivation analogous to the derivation of Laplace’s equation leads to the partial differential
equation
Δ2u := ΔΔu = f, (1.39)
which we call the plate equation; the expression Δ2u is referred to as the Bilaplacian of the function
u. Observe that this is a fourth-order problem.
The problem of the fixed membrane leads, via the fact that the membrane is fixed at the boundary,
to a boundary value problem. This is naturally the case here as well. But the non-negligible thickness
of the plate leads to an additional boundary condition; a clamped plate may be specified by
u|∂Ω = 0, ∂
∂ν u = 0 on ∂Ω. (1.40)
Thus in addition to the Dirichlet boundary condition we see the appearance of Neumann boundary
conditions which feature the outer normal derivative (see (7.4)). However, in the context of fourth￾order problems these two conditions together are known as Dirichlet conditions. This model is also
sometimes called a Kirchhoff plate.22 1 Modeling, or where do differential equations come from
1.7.4 The Navier–Stokes equations
The Navier–Stokes equations are the foundation of fluid and gas dynamics; they describe the flow
of Newtonian fluids such as water, air, and many oils and gases.
We consider a domain Ω ⊂ Rd which is filled with a fluid. This fluid may be described by its
density ρ = ρ(t, x), its velocity vector u = u(t, x) = (u1,..., ud)
T (where ui gives the velocity in
the ith coordinate direction) and the energy e = e(t, x), where “energy” refers to the total energy
(internal and kinetic). Now it is possible to combine ρ, u and e into a (d + 2)-dimensional state
vector; this leads to the recognition that we are dealing with a system of equations (as opposed to
a scalar equation, as we have always considered until now). A difficulty that often arises when
treating systems is that the individual components may be coupled in a very complicated manner.
Instead of ρ, u and e, it is more convenient to work with the vector
U = U(t, x) := 


ρ
ρu
ρe



∈ Rd+2
, (1.41)
where ρu = ρ(t, x) u(t, x) is the density of the mass flow, that is, the impulse per unit of volume,
and ρe = ρ(t, x) e(t, x) gives the total energy per unit of volume. We call the vector U the state
vector. If we denote by
δi, j :=

1 if i = j,
0 otherwise, (1.42)
the Kronecker delta for i, j ∈ N, then ei := (δ1,i,..., δd,i)
T = (0,..., 0, 1, 0,..., 0)
T is the ith
canonical unit vector. With this notation we can define
Fi = Fi(U) := 


ρui
(ρui)u + pei
ui (ρe + p)



,
where p = p(t, x) is the pressure of the fluid. The vectors Fi model the convective terms; the
diffusive terms are represented by the vector
Gi = Gi(U) := 


0
−τi
− 
d
j=1 ujτi, j + qi



,
where τ = τ(u) = (τj,i)i, j=1, . . .,d, and τi, j := μ
 ∂uj
∂xi + ∂ui
∂xj
	
− δj,i
2
3 μ div u is the viscous stress
tensor, with τi the ith column vector. The number μ ∈ R+ denotes the dynamic viscosity, and
q = (q1,..., qd)
T , qi = −λ ∂T
∂xi the heat flow with temperature T and heat conductivity λ. Putting
this all together, the (compressible) Navier–Stokes equations read
Ut +
d
i=1
∂
∂xi
Fi(U) +
d
i=1
∂
∂xi
Gi(U) = 0. (1.43)
Due to the dependence of the convective flows Fi on U, this is a system of nonlinear partial
differential equations.1.7* But there’s more 23
Incompressible Navier–Stokes equations
If the density ρ is constant in space and time, ρ = ρ(t, x) ≡ const, then we call the fluid incompress￾ible. Strictly speaking there are no incompressible fluids in real life, but in the case of water or air
(for example) with low velocity, one may assume constant density as a reasonable approximation. In
this case the first component in (1.43), namely ρt + div(ρu) = 0, known as the continuity equation,
can be simplified to
div u = 0. (1.44)
One can then check that the remaining equations reduce to
ρ ut + ρ (u · ∇)u − η Δu + ∇p = f. (1.45)
Here the Laplacian is to be understood componentwise, Δu = (Δu1,..., Δud)
T , and the abbrevi￾ation of the convective terms is to be read as
(u · ∇)u = 


d
j=1
uj
∂
∂xj
ui


i=1, . . .,d
.
Thus we obtain d equations in (1.45) and one in (1.44) for the d + 1 unknowns u and p. Both
equations together, that is, the pair (1.44, 1.45)
ρ ut − η Δu + ρ (u · ∇)u + ∇p = f,
div u = 0, (1.46)
are known as the Navier–Stokes equationsfor incompressible fluids. In the form (1.46) the equations
are also called non-steady state; if the velocity is constant in time, then the derivative in the time
variable vanishes and we obtain the steady-state (or stationary) Navier–Stokes equations
−ν Δu + (u · ∇)u + ∇p = f,
div u = 0 (1.47)
in the unknowns u = u(x), p = p(x) and with right-hand side f = f(x). The quantity ν = ηρ−1
is called the kinematic viscosity and is often identified with the inverse of the Reynolds number,
ν = Re−1. The Reynolds number captures the relationship between the forces of inertia and viscosity.
Although the Navier–Stokes equation “only” has a quadratic nonlinearity in the term (u · ∇)u
and the coupling via the incompressibility condition (u · ∇)u appears to be weak, the mathematical
theory of this equation is exceptionally difficult. The Navier–Stokes equations feature among the
Millennium problems; for the solution of any of these problems the Clay Mathematics Institute will
award a prize of one million US dollars [20]. For the incompressible Navier–Stokes equations, even
the proof of the existence of a local solution for arbitrarily small times is an open problem.
The Stokes problem
A simplification in the equations results if the nonlinear convection term is suppressed; this is
reasonable in the case of extremely viscous fluids. In the resulting Stokes problem the diffusivity of
the impulse, that is, the kinematic viscosity, is many orders of magnitude larger than the thermal
diffusivity; consequently, the convective term (inertia) may be neglected. An area of application is
the study of currents on the surface of planets, in geodynamics. The equations read −ν Δu+ ∇p = f
and div u = 0.24 1 Modeling, or where do differential equations come from
1.7.5 Maxwell’s equations
Maxwell’s equations are a system of four equations at the heart of classical electromagnetism and
the theory of electric circuits. These four equations describe the generation of electric and magnetic
fields by charges and currents, as well as the interaction of these fields. These electric and magnetic
fields are regarded as non-stationary, that is, time dependent, and the interactions over time are part
of the model.
The essential scientific contribution of Maxwell was to create a unified theory which united the
following laws:
• Ampère’s law (electrodynamic law),
• Faraday’s law (magnetodynamic law),
• Gauss’s law (electrostatic law), and
• the magnetostatic law.
In order to obtain the mathematical consistency of the continuity equation in electrodynamics
ρt + ∇ · j = 0 with the current density j = j(t, x) = (j1, j2, j3)
T , Maxwell inserted an additional
term, which he called displacement current, to Ampère’s law. There is a clear analogy to continuity
equations from fluid mechanics, where instead of j the expression ρu is involved. In electrodynam￾ics, vector fields are usually typeset in bold; in fluid mechanics on the other hand they are marked
with an arrow.
We now wish to give the equations. We denote by E = E(t, x) = (E1, E2, E3)
T the electric
field strength and by B = B(t, x) = (B1, B2, B3)
T the magnetic field strength, as well as by ρ the
charge density, which is a given constant. Then the four equations read
Bt + rot E = 0 (magnetodynamic law, Faraday) (1.48a)
divE = 4π ρ (electrostatic law, Gauss) (1.48b)
Et − rot B = −4π j (electrodynamic law, Ampère) (1.48c)
divB = 0 (magnetostatic law) (1.48d)
These four equations together are Maxwell’s equations. Here we have used the notation rot E :=
∇ × E, that is,
rot E =





∂
∂x2 E3 − ∂
∂x3 E2
∂
∂x3 E1 − ∂
∂x1 E3
∂
∂x1 E2 − ∂
∂x2 E1





.
Clearly, (1.48) constitutes a coupled system of non-stationary linear partial differential equations.
A particularity of these equations is the appearance of the two differential operators div and rot
together.
1.7.6 The Schrödinger equation
The Schrödinger equation is the foundational equation of non-relativistic quantum mechanics. It
was first formulated as a wave equation by Erwin Schrödinger (1887–1961), in 1926. Schrödinger’s
work would later earn him the Nobel prize in physics, in 1933. Solutions of the Schrödinger
equation, which are also known as wave functions, describe the evolution of the state of a quantum
system in space and time.
The Schrödinger equation is a postulate (similar to Newton’s axioms in classical physics)
and as such cannot strictly speaking be mathematically derived from other laws. The equation1.8 Classification of partial differential equations 25
was postulated taking into account certain basic principles of physics; Schrödinger drew upon
knowledge of quantum-mechanical phenomena which were already known in his time. There are
also numerous parallels between his theory and the theory of optics.
Unlike all the equations we have previously considered, the Schrödinger equation is complex￾valued, a necessary consequence of the quantum-mechanical modeling. The equation for a single
particle (such as an elementary particle or an atom) subject to a potential V = V(t, x), whose state
is described by the wave function ψ, reads
iψt = − 2
2m Δψ + V(t, x)ψ. (1.49)
Here i = √
−1 is the imaginary unit,  = h/2π is the reduced Planck constant (where Planck’s
constant, or the quantum of electromagnetic action, is given by h = 6.626 · 10−34 Js) and m is the
mass of the particle. The unknown is the complex-valued wave function ψ = ψ(t, x) : Ω× [0, T] →
C. The right-hand side of (1.49) can also be written as

− 2
2m Δ + V(t, x)

ψ =: Hˆ ψ
with Hamiltonian Hˆ .
1.8 Classification of partial differential equations
As previously announced, we wish to attempt to classify partial differential equations
according to certain properties. The following properties allow us to define a first set
of categories:
1.) Dimension (of the space variables)
2.) Order of the equation (with respect to both space and time)
3.) Algebraic type of the equation
We will meet a further classification somewhat later (in Chapter 2), which is of
more practical importance for the mathematical study of such equations. There we
will also give an overview of the partial differential equations introduced above and
order these according to the criteria listed here (see Table 2.1 on page 47).
1.) Dimension
This category is clear, but often not particularly significant. It is, however, possible to
find examples of partial differential equations whose behavior is strongly dependent
on the space dimension.
2.) Order of the equation
The order of a partial differential equation is the highest order derivative appearing
in the equation. The viscous Burgers’ equation, for example, is of second order (due
to the term ε uxx).
3.) Algebraic type of the equation
Here we mean, firstly, the division between linear and nonlinear equations. “Linear”
means that the equation(s) is (are) linear in the unknown function(s). This is also26 1 Modeling, or where do differential equations come from
a purely algebraic property of the equation(s). Nonlinear equations can be divided
into further subcategories (as done in [28]):
• Semilinear equations:
These are linear in the term in which the highest-order derivative appears; more
precisely, if the order is k, say, then they must have the form

|α|=k
aα(x) Dαu + a0(Dk−1
u,..., Du, u, x) = 0,
where α = (α1,...,αd)
T ∈ Nd is a multi-index with |α| := α1 + ··· + αd,
x ∈ Rd, and Dαu := ∂|α|
u
∂x
α1
1 ···∂x
αd
d
denotes the corresponding derivative. All the
equations introduced above are semilinear with the exception of the minimal
surface equation and the Monge–Ampère equation.
• Quasilinear equations:
These are partial differential equations whose variable coefficients may depend
on the solution and its derivatives, but only up to a degree strictly less than
the order of the equation; the equation must still be linear in the derivatives of
highest order. If the order of the equation is k, then these have the form

|α|=k
aα(Dk−1
u,..., Du, u, x) Dαu + a0(Dk−1
u,..., Du, u, x) = 0.
All of the equations considered thus far except the Monge–Ampère equation are
quasilinear. The minimal surface equation, in particular, is quasilinear but not
semilinear.
• Fully nonlinear equations:
Here all terms involving the highest-order derivatives are nonlinear. The Monge–
Ampère equation is an example of such a fully nonlinear (here quadratic) equa￾tion.
The real background (that is, the natural or technical process which should be
modeled by a differential equation) is of course essential for determining the prop￾erties of the differential equation modeling it. We will return to this point later, cf.
Table 3.1 on page 110.
1.9* Comments
Jean Le Rond d’Alembert (1717–1783) is considered one of the pioneers of the modeling of physical
phenomena via partial differential equations. His first contribution Réflexions sur la cause générale
des vents was awarded the prize of the Prussian academy in 1747. The physical assumptions he
made in this work were however relatively unrealistic and led to a certain amount of dispute.
In the same year d’Alembert derived the wave equation from the Newtonian laws of motion, as a
description of a vibrating string. We have attempted to reproduce his elegant and physically accurate
derivation above. We refer to [38, end of Ch. VI] for a description of the rather fascinating history
of this problem, which was also decisive for our modern understanding of functions. Indeed, every1.10 Exercises 27
continuous initial position of the string is physically meaningful, and we will see later that in this
case there is always a unique solution (see Section 3.1.2). In this example there are physical reasons
for requiring the introduction of weak solutions; see Exercise 5.13. D’Alembert would also become
well known for a completely different activity: in 1751, together with Denis Diderot he published
the first volume of what was probably the most famous early encyclopedia. This would end up
totaling 35 volumes, the last of which was published in 1780.
The study of heat diffusion may be traced back to Fourier, who incidentally was also responsible
for the term greenhouse effect (l’effet de serre), which he invented in his groundbreaking work
Théorie analytique de chaleur (The Analytic Theory of Heat) in 1822. Fourier participated in
Napoleon’s Egyptian campaign and even became the secretary of the Institut d’Egypte there.
After his return to France, he was appointed Prefect of the Departement Isère in 1802. His tenure is
considered to have been a success; there he was responsible, for example, for (successfully) draining
swamps. Not long after his return to Paris in 1815, Fourier became secretary of the Académie des
sciences, in 1817.
Louis Bachelier (1870–1946) is regarded as the founder of financial mathematics. He obtained
his doctorate in 1900 under Henri Poincaré with a dissertation on the topic Théorie de la Spécu￾lation. He was well ahead of his time, already working with Brownian motion five years before
Albert Einstein and a full 23 years before Norbert Wiener would provide a rigorous mathematical
construction. He also gave a formula for the price of options 73 years before the publication of
the famous Black–Scholes formula. His book Le jeu, la chance et le hasard, published in 1914,
was very successful, but his work remained essentially unknown for a long period of time. After
having held academic positions in Paris, Dijon and Rennes, Bachelier was a professor in Besançon
(Franche-Comté) from 1927 until his retirement.
1.10 Exercises
Exercise 1.1 Derive the equations of motion for the undamped double oscillator
(spring/mass system) depicted in Figure 1.7. In doing so, you should work out the
balance of forces on each of the oscillators.
Suggestion: Use Hooke’s law: if a spring is stretched by a force, then its change in
length is proportional to the strength of the force, F = k s (where s is the change in
length and k is a constant depending on the spring).
Exercise 1.2 Suppose that the displacement u of a vibrating string is restricted by
a certain obstacle in such a way that u ≥ g pointwise. Find the corresponding
differential inequality that the displacement must satisfy.
Suggestion: Consider the vertical displacement S(x) analogous to the derivation of
the wave equation and formulate the restriction by the obstacle pointwise.
Exercise 1.3 Let a given fluid have density ρ(t, x) and velocity vector u(t, x). Derive
the continuity equation ρt + div(ρu) = 0 from physical principles.
Suggestion: Use the principle of conservation of mass and the divergence theorem.
Exercise 1.4 For the equation ut = uxx + u determine all solutions of the form
u(t, x) = ϕ(x − ct) (the traveling waves).28 1 Modeling, or where do differential equations come from
s1(t)
s2(t)
k1
k2
c1
c2
F1(t)
F2(t)
mass m1
mass m2
Fig. 1.7 Double oscillator with masses m1, m2, damping c1, c2, spring constants k1, k2 and
external forces F1(t), F2(t). To be determined are the vertical displacements s1(t), s2(t).
Exercise 1.5 Derive the equation utt + a2uxxxx = 0, a ∈ R for the displacement
u(t, x) of a rod of length  resting at both ends.
Suggestion: Use the relation Q = ∂M
∂x between the bending moment M and the
shear force Q. The relation between M and the desired vertical displacement u reads
M = −EIuxx, where E is the modulus of elasticity and I is the moment of inertia.
These two quantities enter into the equation via the constant a.Chapter 2
Classification and characteristics
In the preceding chapter we encountered a whole series of partial differential equa￾tions arising from natural processes as well as economic models. Given the huge
variety of such equations, it should not be surprising that there is no unified mathe￾matical theory and no unified method for solving partial differential equations, and
indeed such a method cannot exist. It is however natural to ask whether they can be
divided into categories for which a more or less complete theory can be developed.
Here we will introduce the principal equation types elliptic, parabolic and hyper￾bolic. In Chapter 6 we will then develop a method with which we can investigate
elliptic equations systematically. For parabolic and hyperbolic equations we will use
the same method in Chapter 8 (separation of variables and spectral decomposition);
however, it will turn out that these equations have very different properties from each
other. Numerical schemes for all three types will be introduced in Chapter 9.
For first-order equations there is in fact a general solution method, the method of
characteristics. We will start this chapter with an introduction to this method, which
is based on the idea of reducing the problem to an ordinary differential equation.
Chapter overview
2.1 Characteristics of initial value problems on R ................... 30
2.2 Equations of second order ............................. 39
2.3* Nonlinear equations of second order ........................ 43
2.4* Equations of higher order and systems ....................... 44
2.5 Exercises ...................................... 45
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4_2
2930 2 Classification and characteristics
Notation
We will use the following notation. If Ω is a set in Rd, then we denote by C(Ω)
the space of continuous real-valued functions on Ω. If Ω is open, then C1(Ω) is the
space of continuous functions u : Ω → R whose partial derivatives exist and are
all continuous. The space of functions whose partial derivatives admit continuous
extensions in C(Ω) will be denoted by C1(Ω). If u ∈ C1(Ω), then we write ∂u
∂xj
for its
partial derivatives and also for their continuous extensions in the case that u ∈ C1(Ω).
Spaces of functions which admit derivatives of higher order can be defined similarly:
if Ω ⊂ Rd is open, then we set
C2(Ω) :=

u ∈ C1(Ω) :
∂u
∂xj
∈ C1(Ω), j = 1,..., d

,
that is, u ∈ C2(Ω) if all its partial derivatives of second order ∂2
∂xi ∂xj
are in C(Ω).
We may also define inductively
Ck+1(Ω) :=

u ∈ C1(Ω) :
∂u
∂xj
∈ Ck (Ω), j = 1,..., d

, k ≥ 2.
Finally, C∞(Ω) := ∞
k=1 Ck (Ω) will be the space of infinitely differentiable real￾valued functions on Ω. We likewise define Ck (Ω) inductively by
Ck+1(Ω) := {u ∈ C1(Ω) :
∂u
∂xj
∈ Ck (Ω), j = 1,..., d}, k ≥ 0,
and C∞(Ω) := 
k ∈N Ck (Ω). We will sometimes have occasion to consider mixed
spaces of the form Ck (Ω) ∩ C(Ω); this is the space of all continuous functions on Ω
whose restriction to Ω belongs to Ck (Ω). If u ∈ C(Ω), then the set
supp u := {x ∈ Ω : u(x)  0} (2.1)
is called the support of u. The set Ω\ supp u is the largest open subset of Ω on which
u vanishes identically. We denote by Cc(Ω) the space of those functions u ∈ C(Ω)
whose support is a compact subset of Ω; in this case, we say u has compact support.
We also set Ck
c (Ω) := Cc(Ω) ∩ Ck (Ω), k ∈ N ∪ {∞}.
2.1 Characteristics of initial value problems on R
This section is dedicated to the method of characteristics in the simplest possible
case, namely for linear equations in two variables. We will then give an insight into
the much more complicated nonlinear situation via the example of Burgers’ equation.2.1 Characteristics of initial value problems on R 31
2.1.1 Homogeneous problems
We first look at problems on [0, T] ×R, that is, the time variable is in [0, T] and there
is one space variable. The initial value u0 : R → R is thus a function on R.
We start with the first-order homogeneous partial differential equation
ut(t, x) + a(t, x) ux(t, x) = 0, x ∈ R , t ∈ (0, T), (2.2a)
u(0, x) = u0(x), x ∈ R, (2.2b)
where T > 0 is given. This is an initial value problem, also known as a Cauchy
problem. The initial value u0 : R → R, a continuous function, is given. The partial
differential equation (2.2a) is of first order and has a variable coefficient a = a(t, x).
This is a given function a : R × R → R, which we will assume to be continuously
differentiable. We will forget the initial condition (2.2b) for the time being and only
consider the partial differential equation (2.2a). The basic idea consists of finding
curves in space-time R × R on which every solution of (2.2a) is constant. Such a
curve is called a characteristic of the equation (2.2a), cf. Figure 2.1.
t
x x0
g (t)
Fig. 2.1 Characteristic: the solution u(t, γ(t)) is constant and equal to u0(x0).
We first make the following ansatz. Let Γ : J → R2 be a curve of the form
Γ(s) = (s, γ(s)) for an open interval J ⊂ R and γ ∈ C1(J). Then Γ is a characteristic
if and only if
0 = d
ds u(s, γ(s)) = ut(s, γ(s)) + γ(s)ux(s, γ(s))
for all s ∈ J and every solution u of the equation. Thus Γ is a characteristic if γ
satisfies the ordinary differential equation
γ(s) = a(s, γ(s)), s ∈ J. (2.3)32 2 Classification and characteristics
We now wish to study the initial value problem (2.2) using solutions of this differ￾ential equation. Let us suppose that u ∈ C1([0, T] × R) is such a solution of (2.2);
we wish to determine u(t, x) for given 0 < t ≤ T and x ∈ R. By the Picard–Lindelöf
theorem1 on existence and uniqueness of solutions to ordinary differential equations
[3, (7.6)], the equation (2.3) has a unique maximal solution γ ∈ C1(J) such that
γ(t) = x. (2.4)
Here J ⊂ R is an open interval containing t. The maximality of J means that
lims→a+ |γ(s)| = ∞ in the case that J = (a, b) with a > −∞ (that is, if the left
endpoint is finite), and analogously in the case of a finite right endpoint b. If 0 ∈ J
(which is not always the case; see Example 2.5), then we know that
u(t, x) = u(t, γ(t)) = u(0, γ(0)) = u0(γ(0)).
In this case we have determined u(t, x) by solving the initial value problem (2.3),
(2.4) (which is actually a final – also known as terminal – value problem). We wish
to formulate this as a theorem.
Theorem 2.1 Let u ∈ C1([0, T] × R) be a solution of (2.2) and γ ∈ C1([0, t]) a
solution of (2.3), (2.4) for some (t, x)∈(0, T] × R. Then
u(t, x) = u0(γ(0))
and u is constant along the characteristic Γ.
Thus if we know that a solution of (2.3), (2.4) exists on [0, T] for every x, then
we can conclude that (2.2) has at most one solution. This is for example the case if
there exists an L > 0 such that |a(s, x)| ≤ L(1 + |x|) for all x ∈ R and s ∈ [0, T]
(see Exercise 2.8). In general (2.2) can, however, have multiple solutions for a given
initial value (see Example 2.5). At any rate, the initial value problem (2.3), (2.4)
gives a strategy for determining solutions; this is the method of characteristics. We
will now consider a few examples.
Example 2.2 (Linear transport equation) The initial value problem for the linear
transport equation is given by
ut + a ux = 0, x ∈ R, t > 0,
u(0, x) = u0(x), x ∈ R,
that is, we have (2.2) with constant coefficient a ≡ const. In this case the ordinary
differential equation (2.3) reduces to γ(s) = a. Its solutions γ ∈ C1(R) have the form
γ(s) = c + as, s ∈ R, where c ∈ R. Thus in this case the characteristics are straight
lines, see Figure 2.2.
Now let x ∈ R and t > 0. The straight line through the point (t, x) is given by
γ(s) = x + a(s − t), s ∈ R, and so by Theorem 2.1 we have u(t, x) = u0(γ(0)) =
1 Also known as the Cauchy–Lipschitz theorem.2.1 Characteristics of initial value problems on R 33
u0(x − a t). If u0 ∈ C1(R), then it is easy to check that this formula defines a solution
of the linear transport equation. We have thus proved existence and uniqueness of
solutions using the characteristics. 
x
t
t = x −c
a
Fig. 2.2 For the linear transport equation the characteristics are straight lines.
Characteristics can also help to explain the role of the choice of initial and bound￾ary values when it comes to existence and uniqueness of solutions of a given partial
differential equation. For the meantime we limit ourselves to first-order equations
and again consider the transport equation, although this time on an interval in space,
which corresponds to a rectangle in space-time.
Example 2.3 We seek the solutions u ∈ C1([0, T]×[0, 1]), u = u(t, x), of the equation
ut + 2T ux = 0. (2.5)
For (t, x)∈(0, T)×(0, 1), a characteristic passing through the point (t, x) is given
by γ(s) = x − 2T(t − s), s ∈ R. This line passes through two sides of the rectangle
[0, T]×[0, 1], cf. Figure 2.3. If u is a solution of (2.5), then it takes on the same value
at both points where the line hits the boundary of the rectangle. With this principle
in mind we can see which boundary conditions we can impose on the rectangle in
order to obtain existence and uniqueness of solutions. For example, we could impose
conditions on the two sides {(0, x) : 0 ≤ x ≤ 1} and {(t, 0) : 0 ≤ t ≤ T}; then every
characteristic has exactly one point of intersection with this part of the boundary. At
the origin these conditions must be compatible; for example, it is easy to show that
if g ∈ C1([0, 1]) and h ∈ C1([0, T]) with g(0) = h(0) and h
(0) = −2Tg
(0), then
there is exactly one solution u ∈ C1([0, T]×[0, 1]) of (2.5) for which u(0, x) = g(x),
x ∈ [0, 1] and u(t, 0) = h(t), t ∈ [0, T], see Exercise 2.6. 34 2 Classification and characteristics
t
x
T
1
Fig. 2.3 Characteristics γ(s) = 2T s + (x − 2T t) for (2.5).
Example 2.4 (Transport equation with variable coefficients) Our next example is
the problem
ut + x ux = 0, x ∈ R, t > 0,
u(0, x) = u0(x), x ∈ R,
corresponding to (2.2) with a(t, x) = x. The ordinary differential equation for the
characteristics reads γ(s) = γ(s), s ∈ R, which has solutions of the form γ(s) = ces.
In order to determine the solution of the partial differential equation at the point (t, x),
one can determine the characteristic which passes through this point, that is, x = cet
,
and thus γ(s) = xe−t
es. This yields γ(0) = xe−t and u(t, x) = u0(xe−t
); it is easy
to check that this is in fact the solution. So in this case the characteristics (s, xes−t
)
are not straight lines as they were above, but rather take on the form depicted in
Figure 2.4. 
t
x
Fig. 2.4 Curved characteristics for the transport equation with variable coefficients.
Here the characteristics also facilitate a physical interpretation of the equation.
One can say that the initial data are transported along the characteristics; this means
that, at least in the case of pure transport processes (that is, convection and no2.1 Characteristics of initial value problems on R 35
diffusion) the initial data already completely determine the solution. As mentioned
earlier, we will now give an example in which uniqueness of solutions is violated.
Example 2.5 Let u0 ∈ C1
c (R), that is, u0 : R → R is continuously differentiable and
vanishes outside a bounded subset of R. We seek a function u ∈ C1((0, ∞) × R) ∩
C([0, ∞) × R) satisfying
ut(t, x) = x2
ux(t, x), t > 0, x ∈ R, (2.6a)
u(0, x) = u0(x), x ∈ R. (2.6b)
We wish to apply the method of characteristics. For (t, x)∈(0, ∞) × R we thus
need to solve the problem γ(s) = −γ(s)
2, s ∈ R, γ(t) = x. The solution is given by
γ(s) = (s − t + 1
x )
−1 if x  0, and γ ≡ 0 for x = 0, with maximal solution interval
J =
⎧⎪⎪⎪⎪⎨
⎪⎪⎪⎪
⎩

t − 1
x , ∞

, if x > 0, 
−∞, t − 1
x

, if x < 0,
R, if x = 0.
In particular, 0 ∈ J if and only if x < 1
t , and in this case
u(t, x) = u(t, γ(t)) = u(0, γ(0)) = u0(γ(0)) = u0
 x
1 − xt

.
Thus there exists at most one solution in the set G := {(t, x) : t > 0, x < 1
t }, cf.
Figure 2.5.
t
x
G

Fig. 2.5 The set G := {(t, x) : t > 0, x < 1
t } and curve Λ.36 2 Classification and characteristics
Conversely, if we define
u(t, x) = u0
 x
1 − xt

for x <
1
t
,
then u ∈ C1(G) and limt↓0 u(t, x) = u0(x) for all x ∈ R. In addition, u(t, x) = 0 in a
neighborhood of the curve Λ = {(t, 1
t ) : t > 0} since u0 vanishes outside a bounded
set. It is easy to see that this u solves the equation (2.6a) in G. Now, if u1 ∈ C1
c (R) is
an arbitrary function and we set
w(t, x) := u1
 x
1 − xt

for t > 0, x >
1
t
as well as w(t, x) := 0 for t > 0 and x = 1
t , then w solves the initial value problem
(2.6a) with w(0, x) = 0, so that u + w solves (2.6). Hence uniqueness does not hold
here. 
2.1.2 Inhomogeneous problems
We now consider the inhomogeneous problem
ut + a(t, x) ux = b(t, x), x ∈ R, t > 0, u(0, x) = u0(x), x ∈ R, (2.7)
for a given continuous function b : [0, ∞) × R → R. In this example, if b  0 it is
not clear how to find curves on which all solutions are constant. However, it turns
out that there are curves on which the solution of an ordinary differential equation is
enough. In this section we will also refer to such curves as characteristics. We will
again consider the ordinary differential equation
γ(s) = a(s, γ(s)), s ∈ R. (2.8)
Theorem 2.6 Let u ∈ C1((0, ∞) × R) ∩ C([0, ∞) × R) be a solution of (2.7). For
t > 0 and x ∈ R let γ ∈ C1([0, t]) be a solution of (2.8) with γ(t) = x. Then
u(t, x) = u0(γ(0)) +
∫ t
0
b(s, γ(s)) ds.
Proof By the chain rule, (2.8) and (2.7), we have, for s ∈ [0, t],
d
ds u(s, γ(s)) = ut(s, γ(s)) + ux(s, γ(s)) γ(s)
= ut(s, γ(s)) + a(s, γ(s)) ux(s, γ(s)) = b(s, γ(s)).
This shows that the function γ satisfies an inhomogeneous first-order linear differen￾tial equation. Physically speaking, one can interpret this as saying that the speed of2.1 Characteristics of initial value problems on R 37
u along the characteristic is equal to the external force b. Since u(0, γ(0))=u0(γ(0)),
it follows from the Fundamental Theorem of Calculus that
u(t, x) = u(t, γ(t)) = u0(γ(0)) +
∫ t
0
b(s, γ(s)) ds,
which proves the claim. 
In this case u is not constant along the characteristics, but as before u(t, γ(t)) is
uniquely determined by the initial value u(0, γ(0)) (as the solution of an initial value
problem of an ordinary differential equation), if (2.8) is a solution in [0, t].
Example 2.7 (Inhomogeneous linear transport equation) We consider the linear
transport equation with the special inhomogeneity b(t, x) = x,
ut + ux = x, x ∈ R, t > 0, u(0, x) = u0(x), x ∈ R.
As in the homogeneous case we obtain the characteristics γ(s) = s + c. Let (t, x) ∈
(0, ∞) × R. Then γ(s) = x + s − t is the characteristic passing through (t, x), and we
obtain the following formula for solutions of the partial differential equation:
u(t, x) = u0(γ(0)) +
∫ t
0
γ(s) ds = u0(x − t) + t

x − t
2

.
This is indeed a solution if u0 ∈ C1(R). 
2.1.3* Burgers’ equation
Clearly, general quasilinear equations are not covered by the above method, since in the case of
quasilinear equations the coefficient a can depend on the solution. However, at least in the special
case of Burgers’ equation we can find characteristics, as we shall now proceed to do. We recall from
Section 1.2.5* that the initial value problem for (the inviscid) Burgers’ equation reads
ut + u ux = 0, u(0, x) = u0(x).
Formally, we will use the same ansatz as for the linear equations: for a solution u ∈ C1([0, ∞) ×R)
of Burgers’ equation and (t, x)∈(0, ∞) × R, as above we consider the differential equation
γ(s) = u(s, γ(s)), γ(t) = x, (2.9)
since here the right-hand side of (2.9) is the coefficient of ux . Now of course we cannot solve the
equation in this form, since it contains the unknown solution of Burgers’ equation (we are supposing
that we wish to find u, and so we do not already know it). We thus require a further condition. This
second equation comes from Burgers’ equation itself:
γ(s) = ut + γ(s)ux = ut + uux = 0.
This means that in this case the characteristics through (t, x) are again straight lines, and we have
γ(s) = γ(t)(s − t) + x = u(t, x)(s − t) + x. Hence the solution of (2.9) exists for all s ≥ 0. Since38 2 Classification and characteristics
d
ds u(s, γ(s)) = ut + γ(s)ux = ut + uux = 0,
we see that u is again constant along the characteristic (s, γ(s)); hence in particular
u(t, x) = u(t, γ(t)) = u(0, γ(0)) = u0(γ(0)) = u0(x − tu(t, x)).
This is an implicit equation for u.
We now wish to study an important special class of initial functions u0, namely those u0 which
are linear, or more precisely of the form u0(x) = α x, α ∈ R, α  0. By the above considerations
we obtain u(t, x) = α x − α t u(t, x), and so the solution of Burgers’ equation is
u(t, x) = α x
1 + α t
. (2.10)
It is easy to check that (2.10) is indeed a solution. Now let Gc := {(t, x) : t ≥ 0, x ∈ R, u(t, x) =
c}, where u has the form (2.10). We distinguish between two cases.
1st case: α > 0. In this case, by (2.10) and since 1 + αt > 0, u(t, x) has the same sign as x,
and the level lines Gc have the form
t = x
c − 1
α .
The characteristics thus have the same form as the one depicted in Figure 2.6. The flow is “thinned
out”; we speak of a rarefaction wave. In this case the solution is unique.
t
x
Fig. 2.6 Rarefaction wave for Burgers’ equation.
2nd case: α < 0. In this case the numerator in (2.10) vanishes, more precisely when t = |α|
−1
.
The characteristics Gc thus meet at the point (0, 1
|α|
), as depicted in Figure 2.7. This means that
particles with different (initial) velocities collide at time |α|
−1. The discontinuity this creates is
known as a shock or sometimes an implosion, as the solution breaks down at this point.
These two phenomena, shock and rarefaction, appear in a whole class of nonlinear partial
differential equations, namely nonlinear hyperbolic equations. The simple example considered
above should hopefully already give an idea about why the study of such equations can be so
challenging. In this book we will not pursue this subject any further.2.2 Equations of second order 39
t
x
1
| | a
Fig. 2.7 For the Burgers’ equation a shock occurs when α < 0; the solution becomes discontinuous
at (|α|
−1
, 0) if the initial function is non-constant.
2.2 Equations of second order
We now consider second-order linear partial differential equations, whose general
form is

d
i,j=1
ai,j uxi,xj +

d
i=1
ai uxi + a0 u = f, (2.11)
on a set Ω ⊆ Rd, where uxi := ∂u
∂xi
, uxi,xj := ∂2u
∂xi∂xj
, for given ai,j, ai, a0 ∈ R,
i, j = 1,..., d and a function f : Ω → R. If we consider solutions u ∈ C2(Ω) of
(2.11), then uxi,xj = uxj,xi (this is the statement of Schwarz’s theorem). This means
that we can always assume that
ai,j = aj,i, i, j = 1,..., d, (2.12)
by replacing ai,j by 1
2 (ai,j + aj,i) if necessary. We classify (2.11) according to its
principal part
Au :=

d
i,j=1
ai,j uxi,xj (2.13)
and often imagine terms of lower (first, zeroth) order as perturbations of the principal
part.
We seek a coordinate system with respect to which (2.11) takes on as simple
a form as possible. This leads to the following classification. Denote the desired
coordinate system by ξ = (ξ1,...,ξd)
T and suppose the transformation has the form
ξ = Bx, x ∈ Ω ⊆ Rd, where B ∈ Rd×d is orthogonal. If we now replace u ∈ C2(Ω)
by v(x) := u(B−1 x), that is, u(x) = v(Bx) = v(ξ), then we have40 2 Classification and characteristics
uxi =

d
k=1
vξk bk,i, B = (bk,i)k,i=1,...,d,
that is,
∂
∂xi
=

d
k=1
bk,i
∂
∂ξk
, ∂2
∂xi∂xj
=

d
k=1

d
=1
bk,ib,j
∂2
∂ξk∂ξ
.
Hence, for the principal part,
Au :=

d
i,j=1
ai,j uxi,xj =

d
k,=1




d
i,j=1
bk,i ai,jb,j



vξk,ξ = A˜v,
where A˜ := BABT . Obviously the transformed principal part A˜ is particularly simple
when it is a diagonal matrix. Now since A is symmetric, we can diagonalize it,
and so we choose B as the corresponding orthogonal transformation, so that A˜ =
diag (λ1,...,λd). Then λ1,...,λd are exactly the eigenvalues of A. The classification
is particularly simple when d = 2; for the meantime we will limit ourselves to this
case. Then A has two real eigenvalues λ1, λ2. Assume that A  0.
Definition 2.8 When d = 2, the differential equation (2.11) is called
(a) parabolic if one eigenvalue of A is zero;
(b) elliptic if both eigenvalues have the same sign; and
(c) hyperbolic if λ1 and λ2 have different signs. 
Recall that A = (ai,j)i,j=1,2. The following alternative characterization is imme￾diate.
Lemma 2.9 When d = 2, (2.11) is
(a) parabolic if and only if det(A) = 0;
(b) hyperbolic if and only if det(A) < 0;
(c) elliptic if and only if det(A) > 0.
Proof For the eigenvalues λ of Awe have 0 = det(A−λI) = λ2−(a1,1+a2,2)λ+det(A),
that is, λ1,2 = 1
2 (a1,1+a2,2)±1
4 (a1,1 + a2,2)2 − det(A), from which the claim follows
immediately. 
Next, we wish to give a normal form for each of the three categories; each normal
form will be representative for its respective category. To simplify notation we write
(x, y) := (x1, x2); then after applying the above coordinate transformation a general
second-order linear partial differential equation with constant coefficients in two
variables (x, y) has the form
λ1uxx + λ2uyy + a1ux + a2uy + a0u = f .2.2 Equations of second order 41
Here λ1 and λ2 are the eigenvalues of A. If λ1, λ2  0, then we may assume that
|λ1| = |λ2| = 1 by replacing u by u(x|λ1|
−1/2
, y|λ2|
−1/2
). Now the normal forms are
as follows.
Parabolic equations: Since one eigenvalue is zero (without loss of generality
λ2), the normal forms reads
uxx + a1ux + a2uy + a0u = f .
To prevent this equation from reducing to an ordinary differential equation in x
parametrized in y, we require that a2  0. This can also be expressed as requiring
that the matrix (A, (a1, a2)
T ) ∈ R2×3 has full rank 2. It is easy to see that the heat
equation ut − uxx = f (with y = t) is parabolic.
Hyperbolic equations: In this case the eigenvalues have different signs. The
normal form is
uxx − uyy + a1ux + a2uy + a0u = f .
We see that the wave equation utt − uxx = f is hyperbolic.
Elliptic equations: Since both eigenvalues have the same sign, the normal form
is
uxx + uyy + a1ux + a2uy + a0u = f ;
for example, Poisson’s equation uxx + uyy = f is elliptic.
The names of the three types of equations come from the fact that
• x2
α1
− y
α2
= 1 describes a parabola,
• x2
α1
− y2
α2
= 1 describes an hyperbola, and
• x2
α1
+ y2
α2
= 1 describes an ellipse
if α1 > 0 and α2 > 0. The association with partial differential equations is, however,
purely formal: these three types of curves have no connection to the respective types
of partial differential equations.
Of course, we can immediately generalize this approach to second-order linear
partial differential equations in R2 with variable coefficients, by requiring that the
above conditions hold at every point (x, y) ∈ R2. However, in this case the type may
depend on the point (x, y).
It is clear from the above derivation that in dimension d = 2 this categorization is
complete, that is, a second-order linear partial differential equation of the form (2.11)
with real coefficients is either parabolic, hyperbolic or elliptic, as long as A  0. It
is possible to generalize the above classification to equations of second order in Rd,42 2 Classification and characteristics
d > 2, via the use of eigenvalues; however, it will turn out that this classification is
no longer complete. We recall that a symmetric matrix is said to be positive definite
if all its eigenvalues are positive. If they are all non-negative, then we call the matrix
positive semidefinite.
Definition 2.10 Given the linear partial differential equation
−

d
i,j=1
ai,j(x) uxi,xj (x) +

d
i=1
bi(x) uxi (x) + c(x) u(x) = f (x) (2.14)
with coefficients A(x) = (ai,j(x))i,j=1,...,d ∈ Rd×d, with ai,j = aj,i, as well as
b(x) = (bi(x))i=1,...,d ∈ Rd and c(x) ∈ R, for x ∈ Ω ⊆ Rd, we say that (2.14) is
(a) elliptic in x ∈ Ω if A(x) or −A(x) is positive definite;
(b) hyperbolic in x ∈ Ω if A(x) or −A(x) has exactly one negative and (d − 1)
positive eigenvalues;
(c) parabolic in x ∈ Ω if A(x) or −A(x) is positive semidefinite but not definite and
the matrix (A(x), b(x)) ∈ Rd×(d+1) has maximal rank d. 
Remark 2.11 In Definition 2.10 we can replace part (c) by:
(c’) parabolic in x ∈ Ω if A(x) has exactly one eigenvalue equal to 0 and all other
eigenvalues have the same sign.
This is indeed equivalent to (c), since if A(x) or −A(x) is positive semidefinite but
not definite, then all nonzero eigenvalues have the same sign. In addition, it follows
from the full rank of (A(x), b(x)) that only one eigenvalue vanishes. The converse
implication is trivial. 
Let us now consider the three examples mentioned above.
Example 2.12 The Laplacian in Rd has the form
Δu(x) =

d
i=1
∂2
∂x2
i
u(x), x ∈ Rd,
so in (2.14) we have A(x) ≡ diag(1,..., 1). Since this matrix is positive definite,
Poisson’s equation is elliptic. 
Example 2.13 Now we consider the wave equation utt − Δu = f . It is easy to see
that in this case the matrix A takes on the form
A(x) ≡ A =





−1 0 ··· 0
0 1 .
.
. . . .
0 1





,
and this matrix has −1 as a simple eigenvalue and 1 as an eigenvalue with multiplicity
d (note that here (t, x) ∈ Rd+1, that is, we have an equation in d + 1 dimensions). As
such, the wave equation in Rd+1 is hyperbolic. 2.3* Nonlinear equations of second order 43
Remark 2.14 An equation in space and time of the form ut + Lu = f , where L is
a second-order elliptic differential operator in the space variables x ∈ Ω ⊆ Rd, is
parabolic. In particular, if the coefficient matrix of L from (2.14) is positive definite,
then we are dealing with the most important case of a parabolic equation. 
Proof No terms of the form utt or ut xi , i = 1,..., d appear. Hence A has the form
A =

0 0
0 A˜

∈ R(d+1)×(d+1)
with coefficient matrix A˜ ∈ Rd×d of L. This is positive definite by assumption, and
so A is positive semidefinite but not definite. Hence the equation is parabolic as
asserted. 
Example 2.15 The heat equation ut − Δu = f is parabolic. 
It is already clear from Definition 2.10 that the three categories introduced there
do not give a complete classification in Rd if d > 2, since they do not cover all
possibilities for the eigenvalues of the coefficient matrix. One sometimes also comes
across the term ultrahyperbolic for the situation where the eigenvalues λk of A satisfy
λk  0 for all k = 1,..., d, and at least two eigenvalues are positive and at least two
are negative.
At the end of this chapter we will give an overview of the partial differential equa￾tions introduced thus far and classify these according to this scheme; see Table 2.1
(page 47).
2.3* Nonlinear equations of second order
Thus far we have only considered linear equations; now we wish to generalize the above classification
to nonlinear equations of second order. We first write a general second-order nonlinear partial
differential equation in the form
F(x, u(x), uxi (x), uxi, xj (x)) = f (x), (2.15)
where x ∈ Ω ⊆ Rd is the independent variable (which may involve both space and time), f : Ω → R
is a given right-hand side and u : Ω → R is the desired solution. Thus (2.15) describes a scalar
equation (that is, (2.15) is an identity involving real numbers, not vectors). The equation itself is
described by the (nonlinear) function F : Ω×R×Rd ×Rd×d → R. Thus we may regard the function
F as being applied to the variables u ∈ R, q = (qi)i=1, . . .,d ∈ Rd and p = (pi j )i, j ∈ Rd×d, that
is, F(x, u, q, p); and we define the matrix
A(x) = A(u, x) := Fp

x, u(x), uxi (x), uxi xj (x)
	
i, j=1, . . .,d (2.16)
where x ∈ Ω and we use the shorthand notation Fp(x, u, q, p) := ∂
∂p F(x, u, q, p) for the gradient
with respect to the last d × d variables. Now we approximate (2.15) using the linear (first order)
Taylor polynomial of F, that is, we write the equation as44 2 Classification and characteristics
∇u(x)
T A(x) ∇u(x) + G(x, u, ∇u) = f (x), (2.17)
where the function G(x, p, q) contains the terms of zeroth order of the Taylor series in p. We then
say that (2.15) is elliptic (or parabolic, hyperbolic) with respect to u at the point x ∈ Ω if (2.17) is
of the corresponding type.
Example 2.16 Consider the Monge–Ampère equation in two space variables (call them x and y),
ux x (x, y) uy y (x, y) − u2
x y (x, y) − f (x, y) = 0. If u ∈ C2(Ω), then its mixed partial derivatives are
equal, that is, ux y uy x = ux y ux y = u2
x y , and we may write the function F as F(x, y, u, q, p) =
p1,1 p2,2 − p1,2 p2,1. Hence we obtain
Fp(x, y; u) =
 uy y (x, y) −uy x (x, y)
−ux y (x, y) ux x (x, y)

.
We can use Sylvester’s criterion to study when this matrix is positive definite. For this we
consider the principal minors, which read (starting from the bottom) ux x for the first and
det Fp(x, y; u) = ux x uy y − ux y uy x for the second. Now by invoking the differential equation we
see that det Fp(x, y; u) = ux x uy y − ux y uy x = ux x uy y − u2
x y = f , and so the matrix Fp(x, y; u)
is positive definite if
ux x (x, y) > 0, f (x, y) > 0. (2.18)
Hence the Monge–Ampère equation is elliptic in all points (x, y) in which the condition (2.18) is
satisfied. If one of the terms in (2.18) is positive or negative and the other is zero, then the equation
is parabolic there; if one is positive and one negative, then it is hyperbolic. We thus have an example
in which the type of equation depends not just on the point in space but also on the unknown
solution u. 
2.4* Equations of higher order and systems
When treating first-order equations we looked for curves (t, x(t)) along which the solutions are
constant, or at least determined by an initial value problem for an ordinary differential equation.
For equations of second order one looks for local coordinate systems which permit a reduction to
some normal form. It is clear that such an approach becomes increasingly complicated as the order
of the equation increases.
In some cases, however, it is easy to reduce equations of higher order to lower-order systems.
As an example we consider the plate equation Δ2u(x) = f (x), x ∈ Ω ⊂ Rd. Via the change of
variables v = (v1, v2) := (u, Δu) this equation can be reduced to the second-order system
Δv −

v2
0

=

0
f

,
If we transfer the above classification scheme to systems in the natural way, then it follows that the
plate equation is elliptic.
The incompressible Navier–Stokes equations may be written in the form
ut − Δu + G(∇u, u, p) = f;
hence they are parabolic. However, the (quadratic) nonlinearity (u · ∇)u causes substantial dif￾ficulties, both analytically and numerically. For the KdV equation there is no such obvious re￾duction. Maxwell’s equations are in general hyperbolic. However, in certain special cases (e.g.2.5 Exercises 45
time-harmonic, two-dimensional, perfect conductors, etc.) they can be reduced to a parabolic or
even elliptic problem, see e.g. [24, I.§4]. The Schrödinger equation iψt = (− 2
2m Δ + V(t, x))ψ
reads, with ψ = u + iv,
iut − vt =

− 2
2m Δ + V(t, x)

(u + iv)
and can thus be reformulated as a system


ut
vt

=

0 − 2
2m Δ + V(t, x) 2
2m Δ − V(t, x) 0
 
u
v

.
The coefficient matrix of the principal part thus has the form A =
 0 A
−A 0

, where A is a
symmetric positive definite matrix. Hence A has complex eigenvalues with different signs. If in the
above definitions we were to permit complex eigenvalues, then the Schrödinger equation would be
hyperbolic. However, the Schrödinger equation is rarely called hyperbolic in the literature, largely
owing to the fact that hyperbolic equations have a finite speed of propagation, which is not the case
for the Schrödinger equation. In this book we will limit ourselves to the study of the standard types:
elliptic, parabolic and hyperbolic.
2.5 Exercises
Exercise 2.1 Classify the following equations in R2:
(a) (∂1u(x))2 + ex2∂2u(x) = sin(x1), x = (x1, x2)
T,
(b) ∂2
1 u(x) + ex2∂2u(x) = sin2(x1), x = (x1, x2)
T,
(c) ∂2
1 u(x) + exp(∂2u(x)) = sin3(x1), x = (x1, x2)
T .
Exercise 2.2 Consider the problem
∂
∂x1
u(x) +
∂
∂x2
u(x) = u(x)
2
, x= (x1, x2)
T ∈ R2
,
u(x1, −x1) = x1, x1 ∈ R.
Solve this equation in a suitable domain using the method of characteristics.
Exercise 2.3 Determine and sketch the characteristics of the partial differential equa￾tion (2x2 − 3x1)∂x1 u − x2∂x2 u = x2
2 (2x2 − 5x1) and determine the solution u(x1, x2)
for the initial value u(x1, 1) = x1 + 1.
Exercise 2.4 Determine the type (elliptic, parabolic or hyperbolic) of each of the
following partial differential equations, where (x, y)
T ∈ R2:
(a) −2uxx + uxy + uyy = 0,
(b) 5
2 uxx + uxy + uyy = 0,
(c) 9uxx + 12uxy + 4uyy = 0.46 2 Classification and characteristics
Exercise 2.5 Determine the type of the differential equation in R2
(x2 − 1) ∂2
∂x2 u + 2xy
∂2
∂x∂y
u + (y2 − 1) ∂2
∂y2 u = x ∂
∂x
u + y
∂
∂y
u, (x, y)
T ∈ R2.
Exercise 2.6 Let a > 0 be a constant and let g, h ∈ C1([0, 1]) be such that −ag
(0)=
h
(0) and g(0) = h(0). Show that there exists a unique u ∈ C1([0, 1]×[0, 1]) which
satisfies
ut(t, x) + aux(t, x) = 0, t, x ∈ (0, 1),
u(0, x) = g(x), x ∈ [0, 1],
u(t, 0) = h(t), t ∈ [0, 1].
Use the method of characteristics.
Exercise 2.7 (Gronwall’s lemma) Show that if y : (α, t] → R is continuous and
c, λ ≥ 0 are such that
|y(s)| ≤ c + λ
∫ t
s
|y(r)| dr, s ∈ (α, t],
then |y(s)| ≤ c eλ(t−s)
, s ∈ (α, t]. Also show the following variant of this lemma: if
y ∈ C([a, b]) and c, λ ≥ 0 satisfy
y(t) ≤ c + λ
∫ t
a
y(s) ds, t ∈ [a, b],
then y(t) ≤ ceλ(t−a)
, t ∈ [a, b].
Suggestion: Set W(t) := c + λ
∫ t
a y(s) ds, and start by considering W
(t)
W(t) in the case
c > 0. Prove the claim in this case and then consider the case c = 0 afterwards.
Exercise 2.8 Let a : R × R → R be continuously differentiable. Assume that for
each T > 0 there is an L ≥ 0 such that |a(t, x)| ≤ L(1 + |x|) for all t ∈ [0, T] and
x ∈ R. Show that in this case (2.2) admits at most one solution.
Suggestion: Use Gronwall’s lemma from Exercise 2.7.
Exercise 2.9 Show that there is no function u ∈ C1((0, ∞) × R) which satisfies
ut(t, x) = x2
ux(t, x), t > 0, x ∈ R, lim
t↓0
u(t, x) = sin x.
Exercise 2.10 (Triviality of the characteristics for Laplace’s equation) Determine
all curves of the form Γ(t) = (γ1(t), γ2(t)) with γ1, γ2 ∈ C1([0, 1]) on which all
solutions of Laplace’s equation uxx + uyy = 0 are constant.2.5 Exercises 47
Table 2.1 Classification of partial differential equations (alg. type: algebraic type, lin: linear, sl:
semilinear, ql: quasilinear, fnl: fully nonlinear, misc.: variable or other type).
equation
(name and formula)
alg. type
order
(space, time)
dimension
type
Linear transport equation
ut + cux = f lin 1 1 1 —–
Burgers’ equation
ut + u ux = f ql 1 1 1 —–
Viscous Burgers’ equation
ut + u ux = εuxx sl 2 1 1 parabolic
Heat equation
ut − Δu = f lin 2 1 d parabolic
Wave equation
utt − Δu = f lin 2 2 d hyperbolic
Poisson’s equation
−Δu = f lin 2 0 d elliptic
KdV equation
ut − 6 u ux + uxxx = 0 sl 3 1 1 —–
Black–Scholes equation
Vt + 1
2σ2S2VSS + (r − δ)SVS − rV = 0 lin 1 2 1 parabolic
Monge-Ampère equation
det(H(u)) = f fnl 0 2 d misc.
Minimal surface equation
(1 + u2
y)uxx − 2uxuyuxy + (1 + u2
x)uyy = 0 ql 2 0 2 misc.
Plate equation
Δ2u = f lin 4 0 d elliptic
Navier–Stokes equations (incompressible)
ut − ρ(u · ∇)u − μ Δu + ∇p = f, div u = 0 sl 2 1 d parabolic
Navier–Stokes equations (compressible)
Ut + div Fm(U) + div Gm(U) = 0 ql 2 1 d hyperbolic
Maxwell’s equations
div B = 0, Bt + rot E = 0, hyperbolic/
div E = 4πρ, Et − rot B = −4π j lin 1 1 3 misc.
Schrödinger equation
i ψt =

− 2
2m Δ + V(t, x)

ψ lin 2 1 2d misc.Chapter 3
Elementary methods
In this chapter we will derive explicit solutions for a number of partial differential
equations. The equations we will be considering here represent a number of important
models such as those of the vibrating string, heat diffusion and the pricing of options.
But they are additionally prototypes for various partial differential equations which
we met in Chapter 1. To be more precise, we will obtain solutions for
– a hyperbolic equation (the wave equation on an interval),
– an elliptic equation (Laplace’s equation on rectangles and the disk), and
– a parabolic equation (the heat equation and also the Black–Scholes equation).
These solutions illustrate in particular how differently the three types of equations
behave.
The principal method used in this chapter is based on separation of variables; it re￾duces the equations under consideration to ordinary differential equations and yields
special solutions. Fourier series (whose basic properties are derived in Section 3.2)
permit us to superimpose these solutions and thus to prove complete existence and
uniqueness results for our initial-boundary value problems. In the process, we will
also meet the maximum principle as a way of obtaining strong a priori estimates on
solutions. Finally, we will show how in some cases one can derive explicit formulae
for the solution with the help of integral transforms such as the Fourier and Laplace
transforms.
Chapter overview
3.1 The one-dimensional wave equation ........................ 50
3.2 Fourier series .................................... 55
3.3 Laplace’s equation ................................. 63
3.4 The heat equation .................................. 74
3.5 The Black–Scholes equation ............................ 90
3.6 Integral transforms ................................. 96
3.7 Outlook ....................................... 109
3.8 Exercises ...................................... 110
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4_3
4950 3 Elementary methods
3.1 The one-dimensional wave equation
We first consider the one-dimensional wave equation (1.19) with constant wave speed
c > 0,
utt − c2uxx = 0. (3.1)
This is a homogeneous equation, cf. Section 1.4.
3.1.1 D’Alembert’s formula on R × R
Let u ∈ C2(R2) be a solution of (3.1). Motivated by the characteristics x ±ct ≡ const
we make the change of variables
ξ := x + ct, τ := x − ct, (3.2)
and set
v(τ, ξ) := u(t, x) = u

ξ − τ
2c ,
ξ + τ
2

.
Since u ∈ C2(R2), applying the chain rule and (3.2) gives
vξ = 1
2c
ut +
1
2
ux,
vξτ = 1
2c
−1
2c
utt +
1
2c
1
2
ut x +
1
2
−1
2c
uxt +
1
2
1
2
uxx = 1
4c2 (c2uxx − utt) = 0.
Hence the function vξ is independent of τ, and so there exists a function F : R → R
such that vξ (τ, ξ) = F(ξ) for all τ ∈ R. Since v ∈ C2(R2), we must have F ∈ C1(R).
Let ϕ be an antiderivative of F, that is, we suppose that ϕ ∈ C2(R) satisfies ϕ = F.
By the Fundamental Theorem of Calculus, for every τ ∈ R there exists a constant
ψ(τ) (the constant of integration) such that v(τ, ξ) = ϕ(ξ) + ψ(τ). Since ϕ ∈ C2(R),
v ∈ C2(R2), it follows that ψ ∈ C2(R). We have thus shown that
u(t, x) = v(τ, ξ) = v(x − ct, x + ct) = ϕ(x + ct) + ψ(x − ct).
Conversely, every such function is a solution of the wave equation (3.1), as is easy
to check by explicit calculation. We thus have the following result, which states that
the solution is composed of two waves, one moving left and one moving right, both
with speed c.
Theorem 3.1 A function u ∈ C2(R2) solves the wave equation (3.1) if and only if
there exist two functions ϕ, ψ ∈ C2(R) such that
u(t, x) = ϕ(x + ct) + ψ(x − ct) (3.3)3.1 The one-dimensional wave equation 51
for all t, x ∈ R.
We next wish to express ϕ and ψ in terms of the initial data, in this case the initial
position u0(x) = u(0, x) and the initial velocity u1(x) = ut(0, x). So let u be given by
(3.3); then
u0(x) := u(0, x) = ϕ(x) + ψ(x), u1(x) := ut(0, x) = cϕ
(x) − cψ
(x). (3.4)
Differentiating the first equation yields
u
0(x) = ϕ
(x) + ψ
(x). (3.5)
Multiplication by the constant c and addition of the second equation in (3.4) leads
to cu
0(x) + u1(x) = 2cϕ
(x), that is, ϕ
(x) = 1
2 u
0(x) + 1
2c u1(x). Hence there exists a
constant (of integration) c1 ∈ R such that
ϕ(x) = 1
2
u0(x) +
1
2c
∫ x
0
u1(s)ds + c1
Multiplying (3.5) by c and subtracting the second equation in (3.4) from the result
yields cu
0(x) −u1(x) = 2cψ
(x), and so there exists another constant (of integration)
c2 ∈ R such that
ψ(x) = 1
2
u0(x) − 1
2c
∫ x
0
u1(s)ds + c2 = 1
2
u0(x) +
1
2c
∫ 0
x
u1(s)ds + c2
for all x ∈ R. Substituting these expressions into (3.3) finally leads to
u(t, x) = 1
2

u0(x + ct) + u0(x − ct)

+
1
2c
∫ x+ct
x−ct
u1(s)ds + c1 + c2.
By setting t = 0, we see that we must have c1 + c2 = 0. We have thus proved the
following theorem, in which the solution is expressed in terms of the initial position
and velocity.
Theorem 3.2 Given the initial data u0 ∈ C2(R) and u1 ∈ C1(R), there exists a
unique function u ∈ C2(R × R) for which
utt = c2 uxx in R2
, (3.6a)
u(0, x) = u0(x), x ∈ R, (3.6b)
ut(0, x) = u1(x), x ∈ R. (3.6c)
This function is given by
u(t, x) = 1
2
(u0(x + ct) + u0(x − ct)) +
1
2c
∫ x+ct
x−ct
u1(s)ds (3.7)
for all x, t ∈ R.52 3 Elementary methods
The representation (3.7) is known as d’Alembert’s formula for the solution of the
wave equation (3.6).
x
t
•
(t, x )
x − ct x + ct
Fig. 3.1 Wave equation: domain of dependence.
Remark 3.3 We already know that c is the speed of propagation of the wave. We see
from formula (3.7) that at the point (t, x) the solution only depends on the initial
values in the interval [x − ct, x + ct], cf. Figure 3.1. This is a typical property of
the wave equation. Another typical property is that the solution u does not gain
regularity over the course of time: u(t, ·) is not infinitely differentiable if u0 and u1
are not. We will see that the situation is completely different for the heat equation. 
Remark 3.4 Observe that (3.7) also makes sense if u0 is only continuous and u1 even
just integrable. In such a case, one could regard the function defined by (3.7) as a
weak solution of the equation; see Exercise 5.13. 
3.1.2 The wave equation on an interval
In the model of the vibrating string, as in other physical models, we do not consider
the wave equation on the whole real line, but rather on an interval [a, b] ⊂ R,
−∞ < a < b < ∞. In order to be consistent with the numerical treatment in Section
9.5 we also consider a finite time interval [0, T], where 0 < T < ∞. In this case, we
wish to find functions u ∈ C2([0, T]×[a, b]) for which
utt(t, x) = c2
uxx(t, x), t ∈ [0, T], x ∈ (a, b). (3.8)
HereC2([0, T]×[a, b])is the space of functions which are twice continuously partially
differentiable in (0, T)×(a, b), and all of whose partial derivatives admit a continuous
extension to [0, T]×[a, b]. We will again specify an initial position u0 ∈ C([a, b])
and an initial velocity u1 ∈ C([a, b]), and require that3.1 The one-dimensional wave equation 53
u(0, x) = u0(x), ut(0, x) = u1(x), x ∈ [a, b]. (3.9)
But on a bounded interval these initial conditions do not yet fully determine a unique
solution; we also need to impose boundary conditions in order to recover uniqueness.
One possibility is given by the (homogeneous) Dirichlet boundary conditions
u(t, a) = u(t, b) = 0, t ∈ [0, T]. (3.10)
Substituting these values into (3.9), we obtain the compatibility conditions for the
initial values
u0(a) = u0(b) = 0, u1(a) = u1(b) = 0. (3.11)
In fact, recall that we are imposing the condition u ∈ C2([0, T]×[a, b]); this smooth￾ness requirement also has the following consequence. If u(t, a) = 0 for all t ∈ [0, T],
then also ut(t, a) = 0 and in particular u1(a) = ut(0, a) = 0. A similar statement
holds for the right-hand endpoint, that is, u1(b) = ut(0, b) = 0.
This leads to the following initial-boundary value problem (IBVP): determine the
function u ∈ C2([0, T]×[a, b]) for which
utt = c2
uxx, t ∈ (0, T), x ∈ (a, b), (3.12a)
u(t, a) = u(t, b) = 0, t ∈ (0, T), (3.12b)
u(0, x) = u0(x), ut(0, x) = u1(x), x ∈ (a, b). (3.12c)
This problem has at most one solution; this will be shown using a theorem on
conservation of energy which holds for the solutions of (3.12a). We recall the
derivation of Laplace’s equation in Section 1.6.4: we saw there that the potential
energy is given by the integral of the square of the spatial derivative of u (multiplied
by the propagation speed). Here the corresponding expression is
Epot(t) :=
∫ b
a
c2 u2
x(t, x) dx,
and we again call this term the potential energy (without entering into the question
of the physical units of measurement involved). The kinetic energy
Ekin(t) :=
∫ b
a
u2
t (t, x) dx
is correspondingly defined as the (average square) time derivative of u. Using these
definitions we can now prove the following theorem for the solutions of the boundary
value problem (3.12a), (3.12b).
Theorem 3.5 (Conservation of energy) Let u ∈ C2([0, T]×[a, b]) be a solution of
(3.12a), (3.12b). Then the total energy E(t) := Ekin(t)+Epot(t), t ∈ [0, T], is constant
on (0, T).54 3 Elementary methods
Proof By Schwarz’s theorem, ut x = uxt for all t ∈ [0, T] and x ∈ (a, b) since
u ∈ C2([0, T]×[a, b]). Integrating by parts and using (3.12a), we obtain
d
dt
Ekin(t) =
∫ b
a
2ut(t, x) utt(t, x) dx = 2c2
∫ b
a
ut(t, x) uxx(t, x) dx
= 2c2ut(t, x) ux(t, x)




x=b
x=a
− 2c2
∫ b
a
ut x(t, x) ux(t, x) dx
= − d
dt
Epot(t),
where we have also used the boundary condition u(t, a) = u(t, b) = 0 and so (as
above) ut(t, a) = ut(t, b) = 0 for t > 0, as well. 
Uniqueness of solutions now follows from conservation of energy as follows. If
the initial-boundary value problem (3.12) has two solutions, then their difference u
is a solution of (3.12) with the homogeneous initial values u0 ≡ u1 ≡ 0. For this u,
however, conservation of energy implies that for all t ∈ (0, T)
E(t) =
∫ b
a

u2
t (t, x) + c2u2
x(t, x)
	
dx = E(0)
=
∫ b
a
(u1(x)
2 + c2u
0(x)
2) dx = 0,
that is, E(t) = 0 for all t ∈ (0, T). It follows that ut(t, x) = ux(t, x) ≡ 0 and so u is
constant. But since u(0, x) = 0, we conclude that u ≡ 0. We can summarize what we
now know in the following theorem.
Theorem 3.6 (Existence and uniqueness of solutions of the IBVP (3.12)) Let
u0 ∈ C2([a, b]) and u1 ∈ C1([a, b]) satisfy u0(a) = u
0 (a) = u0(b) = u
0 (b) = 0 and
u1(a) = u1(b) = 0. Then (3.12) has a unique solution u.
Proof We may assume that a = 0. We extend u0 and u1 to odd 2b-periodic functions
u˜0, u˜1 : R → R; then our assumptions imply that u˜0 ∈ C2(R) and u˜1 ∈ C1(R), see
Exercise 3.21. If we set
u(t, x) := 1
2
(u˜0(x + ct) + (u˜0(x − ct)) +
1
2c
∫ x+ct
x−ct
u˜1(s) ds,
then by Theorem 3.2 u is a solution of (3.6) and hence satisfies (3.12a) and (3.12c).
Since u˜0 and u˜1 are odd and 2b-periodic, we have u(t, a) = u(t, b) = 0 for all t ≥ 0,
as is easy to check. Hence u satisfies (3.12). Uniqueness was proved above. 
Remark 3.7 Another direct consequence of the conservation of energy is the contin￾uous dependence of the solution on the data; together with Theorem 3.6 this gives us
the well-posedness of the IBVP (3.12) for the wave equation. Continuous dependence
for the uniform norm can be proved with the help of the Closed Graph Theorem, see
Exercise 3.22. 3.2 Fourier series 55
We have already seen that it was d’Alembert who showed that a vibrating string
satisfies the (one-dimensional) wave equation, see Section 1.4. It was also d’Alem￾bert who showed in 1747 that the general solution is given by (3.3). But it was Euler
who discovered one year later that the solution is determined by the initial value and
the initial speed. We refer to [38, end of Ch. VI] for a full account of the fascinating
history of the problem. What we have not yet seen is the physical nature of the
solution, which is hidden in Theorem 3.6. In fact, it turns out that the solution can be
expressed as a superposition of simple vibrations, the harmonics, see Exercise 4.11.
To see this we will need to study Fourier series, our next subject.
3.2 Fourier series
Many elementary partial differential equations can be solved elegantly using Fourier
series. One of the reasons for this is that many such equations model phenomena
involving oscillations, as we saw in Chapter 1. Since Fourier series are a superposi￾tion of oscillating functions (more precisely sine and cosine functions, or complex
exponentials), it is natural to look for solutions in the form of Fourier series. But
there are also certain basic questions about Fourier series that are natural to ask,
such as when and in what sense the Fourier series of a continuous or differentiable
function converges. Here we will introduce Fourier series, collect a few essential
facts about them and prove a convergence result (Theorem 3.11).
* Remark 3.8 Leonhard Euler (1707–1783), Joseph-Louis Lagrange (1736–1813) and the Swiss
brothers Jakob (Jacob) Bernoulli (1655–1705) and Johann Bernoulli (1667–1748) as well as Jo￾hann’s son Daniel Bernoulli (1700–1782) all worked on series expansions of functions; one of the
motivations for doing so was already solving differential equations. But it was Jean Baptiste Joseph
Fourier (1768–1830) who claimed in his famous work Théorie analytique de la chaleur of 1822
that every function admits a convergent series expansion. These were later named Fourier series
in his honor. His claim originally met with rejection, in particular from Augustin Louis Cauchy
(1789–1857) and Niels Henrik Abel (1802–1829). Johann Peter Gustav Lejeune Dirichlet (1805–
1859) proved in 1829 in his work Sur la convergence des séries trigonométriques qui servent à
représenter une fonction arbitraire entre des limites données that Fourier’s claim is at least correct
for functions which are piecewise continuously differentiable; see Exercise 5.12. 
Here we will concentrate on continuous periodic functions and show the conver￾gence of their Fourier series with respect to a weaker notion of convergence, more
precisely convergence in the sense of Abel. This will have immediate consequences
for Laplace’s equation and the heat equation.
Definition 3.9 A function f : R → C is called 2π-periodic if f (t + 2π) = f (t) for
all t ∈ R. 
Typical examples are the trigonometric functions sin and cos. A further example
is the complex exponential function ek , defined for k ∈ Z by
ek (t) = eikt = cos(kt) + i sin(kt), t ∈ R. (3.13)56 3 Elementary methods
The functions {ek, k ∈ Z} satisfy the orthogonality relation
1
2π
∫ 2π
0
eikt e−it dt = δk, =

1, if k = ,
0, if k  , (3.14)
as is easy to check. We consider the {ek, k ∈ Z} to be “pure oscillations”. By
superimposing them we can construct more general 2π-periodic functions: let ck ∈ C,
k ∈ Z, be complex coefficients for which
∞
k=−∞
|ck | < ∞.
Then
f (t) =
∞
k=−∞
ck eikt, t ∈ R (3.15)
defines a 2π-periodic function f : R → C. This function is continuous since the
series (3.15) converges uniformly on R. We can also recover the coefficients ck from
f ,
ck = 1
2π
∫ 2π
0
f (t)e−ikt dt, k ∈ Z, (3.16)
as can be seen as follows: due to the uniform convergence of the series (3.15) we
may exchange the order of integration and summation. Hence, for m ∈ Z,
1
2π
∫ 2π
0
f (t)e−imt dt =
∞
k=−∞
ck
1
2π
∫ 2π
0
eikt e−imt dt = cm ,
where we have also used the orthogonality relation (3.14). These thoughts lead us to
the following definition.
Definition 3.10 (a) The space of continuous 2π-functions is defined to be C2π :=
{ f : R → C : f is continuous and 2π-periodic}.
(b) If f ∈ C2π and k ∈ Z, then we call
ck := 1
2π
∫ 2π
0
f (t)e−ikt dt (3.17)
the kth Fourier coefficient of f . The series
∞
k=−∞
ck eikt (3.18)
is called the Fourier series of f . 3.2 Fourier series 57
In general, the Fourier series (3.18) does not converge pointwise. In fact, for every
t ∈ R there is a continuous 2π-periodic function f : R → C such that the Fourier
series (3.18) of f has unbounded partial sums, and hence diverges, in the point t. A
first example was given in 1873 by Paul du Bois-Reymond, see [42, Thm. II.2.1] for
more information.
We will now prove the convergence of the Fourier series in a weaker sense
(Theorem 3.11). Let f : R → C be continuous and 2π-periodic; then the sequence
(ck )k ∈Z of Fourier coefficients of f is bounded, and the series
fr (t) =
∞
k=−∞
r|k |
ck eikt (3.19)
converges uniformly on R for every 0 < r < 1 and defines a function fr ∈ C2π. Our
goal is to prove the following statement, for which we will need a certain amount of
preparation.
Theorem 3.11 (Convergence of Fourier series in the sense of Abel) We have
lim
r ↑1 fr (t) = f (t)
uniformly in t ∈ R.
Remark 3.12 (Abel convergence) We say that a series
∞
k=1 ak converges to a number
c in the sense of Abel if
lim
r ↑1
∞
k=1
akrk = c.
Abel’s convergence theorem for power series, cf. [1], states that ordinary convergence
of the series, that is, limn→∞ 
n
k=1 ak = c, implies convergence to c in the sense of
Abel. The notion of convergence in the sense of Abel is, however, strictly weaker
than that of convergence of the series, as the above-mentioned example of a divergent
Fourier series shows. 
For the proof of Theorem 3.11 we need to introduce the ∞-norm:
 f ∞ = sup
t ∈R
| f (t)| .
Equipped with this norm, C2π becomes a normed vector space which is complete,
that is, a Banach space: every Cauchy sequence of functions with respect to this
norm has a limit which is also in C2π. By definition of the norm, a sequence ( fn)n∈N
in C2π converges uniformly to some f ∈ C2π if and only if limn→∞ fn = f with
respect to ·∞, that is, if and only if limn→∞  fn − f ∞ = 0.
Definition 3.13 Given functions f, g ∈ C2π, we define their convolution f ∗ g by
( f ∗ g)(t) := 1
2π
∫ 2π
0
f (t − s) g(s) ds. 58 3 Elementary methods
The following properties of convolutions are easy to see.
Lemma 3.14 For f, g, h ∈ C2π we have
(a) commutativity: f ∗ g = g ∗ f
(b) associativity: ( f ∗ g) ∗ h = f ∗ (g ∗ h)
(c) distributivity: ( f + g) ∗ h = f ∗ h + g ∗ h
These three properties together mean that C2π is an algebra. Moreover, the norm
·∞ is submultiplicative on C2π, which means that
 f ∗ g∞ ≤  f ∞g∞ (3.20)
for all f, g ∈ C2π, as follows immediately from the definition. Hence C2π is a
Banach algebra. The submultiplicativity of the norm implies in particular that the
convolution is continuous, in the following sense.
Lemma 3.15 (Continuity of convolution) Let fn, f, gn, g ∈ C2π be such that
lim
n→∞  fn − f ∞ = 0, lim
n→∞ gn − g∞ = 0.
Then
lim
n→∞  fn ∗ gn − f ∗ g∞ = 0.
In other words: fn ∗ gn −→ f ∗ g in the norm ·∞ as n → ∞.
Proof Applying Lemma 3.14, (3.20) and the triangle inequality yields
 fn ∗ gn − f ∗ g∞ =  fn ∗ (gn − g) + ( fn − f ) ∗ g∞
≤  fn ∗ (gn − g)∞ + ( fn − f ) ∗ g∞
≤  fn ∞gn − g∞ +  fn − f ∞g∞
→ 0 as n → ∞,
which proves the claim. 
The algebra C2π does not possess an identity element, that is, there is no function
u ∈ C2π such that u ∗ f = f for all f ∈ C2π. Instead, we can give an approximate
identity, that is, a family of functions jr ∈ C2π such that jr ∗ f → f for all f ∈ C2π.
We define these functions, analogously to (3.19), by
jr (t) :=
∞
k=−∞
r|k |
ek (t) (3.21)
for 0 ≤ r < 1. For each such r, this series converges uniformly on R and thus
indeed defines a function jr ∈ C2π. Using the formula for geometric series and the
assumption that 0 ≤ r < 1, we see that3.2 Fourier series 59
jr (t) =
∞
k=0
rk eikt +
∞
k=1
rk e−ikt = 1
1 − reit + re−it
1 − re−it
= 1 − re−it + re−it(1 − reit)
(1 − reit) (1 − re−it) = 1 − r2
1 − 2r cost + r2 . (3.22)
Hence, for all 0 ≤ r < 1,
jr (t) > 0, t ∈ R, (3.23)
and for 0 <δ< 2π
lim
r ↑1 jr (t) = 0 uniformly in t ∈ [δ, 2π − δ]. (3.24)
We see from the definition (3.21) of jr that
1
2π
∫ 2π
0
jr (t) dt = 1. (3.25)
Note that ∫ 2π
0 ek (t) dt = 0 whenever k  0.
We now have all the tools we need in order to show that the family (jr )0≤r<1 is
an approximate identity in C2π, as claimed above.
Theorem 3.16 (Approximate identity in C2π) Given f ∈ C2π, we have
lim
r ↑1 jr ∗ f = f,
where the convergence is with respect to the norm ·∞.
Proof Fix ε > 0 arbitrary. Since f is uniformly continuous, there exists a δ ∈ (0, π)
such that for all |s| ≤ 2δ we have | f (t − s) − f (t)| ≤ ε for all t ∈ R. Thus, by (3.25)
and the periodicity of the function f ,
(jr ∗ f )(t) − f (t) = 1
2π
∫ π
−π
jr (s) ( f (t − s) − f (t)) ds
= 1
2π
∫ δ
−δ
jr (s) ( f (t − s) − f (t)) ds
+
1
2π
∫
π≥ |s | ≥δ
jr (s) ( f (t − s) − f (t)) ds.
Hence
|(jr ∗ f )(t) − f (t)| ≤ ε 1
2π
∫ δ
−δ
jr (s) ds + 2 f ∞
1
2π
∫
π>|s |>δ
jr (s) ds.60 3 Elementary methods
By (3.24) and (3.25) it follows that lim supr ↑1  jr ∗ f − f ∞ ≤ ε. Since ε > 0 was
arbitrary, this proves the claim. 
Proof (of Theorem 3.11) We now merely need to compute jr ∗ f . Firstly, for f ∈ C2π
we have
(ek ∗ f )(t) = 1
2π
∫ 2π
0
eik(t−s) f (s) ds = 1
2π eikt ∫ 2π
0
e−iks f (s) ds = eikt ck,
where ck is the kth Fourier coefficient of f . Hence
 n
k=−n
r|k |
ek

∗ f =
n
k=−n
r|k |
(ek ∗ f ) =
n
k=−n
r|k |
ck ek .
Invoking the continuity of convolutions (see Lemma 3.15) we see that
jr ∗ f =
∞
k=−∞
r|k |
ck ek . (3.26)
Abel’s convergence theorem for Fourier series, Theorem 3.11, now follows from
Theorem 3.16, the theorem about the approximate identity, since the latter states that
fr (t) =
∞
k=−∞
r|k |
ck eikt = jr ∗ f −→ f as r ↑ 1,
where the convergence is with respect to ·∞. 
We note a consequence. We denote by
T :=
 
n∈N
Tn, Tn :=
 n
k=−n
ck ek : c−n,..., cn ∈ C
!
, n ∈ N,
the space of trigonometric polynomials.
Corollary 3.17 (Density of trigonometric polynomials in C2π) The space T is
dense in C2π with respect to the norm ·∞, that is, for every f ∈ C2π there exists a
sequence ( fn)n∈N in T such that limn→∞ fn = f with respect to ·∞.
Proof If f ∈ C2π, then f = limr ↑1 jr ∗ f converges uniformly. Hence for every
n ∈ N there exists an rn < 1 such that  f − jrn ∗ f ∞ ≤ 1
2n . Due to the uniform
convergence of the series (3.26) there exists an Nn ∈ N such that
"
"
"
"
jr ∗ f −

|k | ≤Nn
r|k |
ck ek
"
"
"
"
∞
≤
1
2n
.3.2 Fourier series 61
Choose fn := 

|k | ≤Nn r|k |
ck ek ; then fn ∈ T, and  f − fn ∞ ≤ 1
n by the triangle
inequality. 
Finally, we consider expansions of functions in terms of real sine and cosine
functions. Let f ∈ C2π and let ck be its kth Fourier coefficient. Since e−ikt eikx +
eikt e−ikx = 2 cos(kt) cos(k x) + 2 sin(kt) sin(k x) for all k ∈ N, it follows that
ck ek + c−k e−k = ak cos(kt) + bk sin(kt), where
ak = 1
π
∫ 2π
0
cos(kt) f (t) dt, bk = 1
π
∫ 2π
0
sin(kt) f (t) dt. (3.27)
We recall that by Definition 3.16
c0 = 1
2π
∫ 2π
0
f (t) dt. (3.28)
We thus have
(jr ∗ f )(t) = c0 +
∞
k=1
rk #
ak cos(kt) + bk sin(kt)
$
. (3.29)
We can now reformulate Abel’s convergence theorem (Theorem 3.11).
Corollary 3.18 For any f ∈ C2π, the series
f (t) = lim
r ↑1

c0 +
∞
k=1
rk %
ak cos(kt) + bk sin(kt)
& 
(3.30)
converges uniformly in t, where the Fourier coefficients ak, bk, c0 are defined by
(3.27) and (3.28).
If f is a real-valued function, then the coefficients c0, ak, bk are also real. Hence
the series(3.30)likewise consists exclusively of real terms. We now wish to formulate
an important special case explicitly.
Corollary 3.19 Let  > 0 and let g ∈ C([0, ]) be a real-valued function for which
g(0) = g() = 0. Set
bk = 2

∫ 
0
g(s)sin 
k

πs

ds.
Then
g(x) = lim
r ↑1
∞
k=1
rkbk sin 
kπ
 x

uniformly in x ∈ [0, π].62 3 Elementary methods
Proof 1st case: Let  = π. Since g(0) = 0, there exists exactly one function f ∈ C2π
such that f (t) = g(t) for all 0 ≤ t ≤ π and f (t) = − f (−t) for all t ∈ R. Consider the
Fourier coefficients ak , bk of f . Since f is odd, we have ak = 0 and
bk = 1
π
∫ π
−π
sin(kt) f (t) dt = 2
π
∫ π
0
sin(kt) g(t) dt.
Now the claim follows from Corollary 3.18.
2nd case: Let  > 0 be arbitrary. Define h(t) := g(t/π) for 0 ≤ t ≤ π; then
bk = 1
π
∫ π
0
h(t) sin(kt) dt = 1

∫ 
0
g(s) sin  kπs


ds, k ∈ N.
By what we have shown in the 1st case,
g

t

π
	
= lim
t↑1
∞
k=1
rk bk sin(kt)
uniformly in t ∈ [0, π]. Finally, making the substitution x = t/π completes the
proof. 
Corollary 3.20 Let g ∈ C([0, ]) be such that g(0) = g() = 0. Then there exist
trigonometric polynomials
gn(x) =
∞
k=1
bn
k sin  kπ
 x

,
where bn
k ∈ R satisfy bn
k = 0 for all k ≥ Nn, for some Nn ∈ N, such that
limn→∞ gn(x)= g(x) uniformly in [0, ]. In particular, limn→∞ bn
k = bk , k ∈ N,
where bk is the kth Fourier coefficient of g as in Corollary 3.19.
Proof By Corollary 3.19 there exists an 0 < rn < 1 such that




g(x) − ∞
k=1
rk
n bk sin  kπ
 x




≤
1
2n
, x ∈ [0, ].
Choose Nn ∈ N such that




∞
k=Nn+1
rk
n bk sin  kπ
 x




≤
1
2n
, x ∈ [0, ],
and set bn
k := rk
n bk for k ≤ Nn and bn
k = 0 for k > Nn. It now follows from the
triangle inequality that |g(x) − gn(x)| ≤ 1
n for all x ∈ [0, ] and n ∈ N. 3.3 Laplace’s equation 63
3.3 Laplace’s equation
In this section we wish to solve Laplace’s equation with Dirichlet boundary condi￾tions on certain special domains. To do so, we will use the method of “separation
of variables” together with Fourier series. We will start by considering rectangles
and then move on to the disk. Afterwards, we will prove what is known as the el￾liptic maximum principle, which not only yields uniqueness of solutions but also
an important a priori estimate on them, which we will then use when proving the
existence of solutions for arbitrary boundary data.
3.3.1 The Dirichlet problem on the unit square
We first consider Laplace’s equation on the unit square
Ω := (0, 1)
2 = {(x, y) ∈ R2 : 0 < x, y < 1}
with Dirichlet boundary conditions which are inhomogeneous on the upper edge of
Ω. The boundary value problem consists of finding a function u ∈ C(Ω) ∩ C2(Ω)
such that
Δu = 0, 0 < x, y < 1, (3.31a)
u(0, y) = u(1, y) = 0 0 ≤ y ≤ 1, (3.31b)
u(x, 0) = 0, 0 ≤ x < 1, (3.31c)
u(x, 1) = g(x), 0 < x < 1, (3.31d)
where g : [0, 1] → R is a given continuous function which satisfies g(0) = g(1) = 0.
We use the method of separation of variables to construct special solutions of the
equation (3.31). Since Ω = (0, 1)×(0, 1) is a Cartesian product of intervals, we may
hope to find a solution of (3.31a) of the simple form
u(x, y) = X(x) · Y(y) (3.32)
for certain functions X,Y ∈ C2(0, 1)∩C([0, 1]).1 We substitute this form into (3.31a)
and obtain 0 = Δu(x, y) = X(x)Y(y) + X(x)Y(y) for all x, y ∈ (0, 1). If we
suppose that 0  u(x, y) = X(x)Y(y) for all x, y ∈ (0, 1), then we can divide the
above equation by this term to obtain
1 We will often use spaces of the form Ck (Ω) ∩ C(Ω), where Ω ⊂ Rd. The functions in this
space are by definition k times continuously differentiable in the interior (that is, in Ω) and can be
extended continuously to the boundary ∂Ω. In particular, boundary values of such functions on ∂Ω
are well defined. See also the notation at the beginning of Chapter 2 on page 30. Moreover, we just
write Ck (0, 1) for Ck (Ω) with Ω = (0, 1).64 3 Elementary methods
− X(x)
X(x) = Y(y)
Y(y) . (3.33)
We now see that the left-hand side does not depend on y, and the right-hand side
does not depend on x. Hence both sides of (3.33) must be equal to the same constant,
independent of x and y. Thus there exists some λ ∈ R such that
−X(x) = λX(x), 0 < x < 1, (3.34a)
X(0) = X(1) = 0, (3.34b)
Y(y) = λY(y), 0 < y < 1, (3.34c)
Y(0) = 0. (3.34d)
The two ordinary differential equations (3.34a) and (3.34c) follow from the above
remarks. The boundary condition (3.34b) follows from (3.31b), since by the separa￾tion ansatz (3.32) we have 0 = X(0)Y(y) = X(1)Y(y), 0 ≤ y ≤ 1, which implies
(3.34b). The condition (3.34d) can be shown similarly. We see that (3.34a), (3.34b)
is in fact a boundary value problem for a second-order ordinary differential equation.
The functions sin(kπx) are obviously solutions of (3.34a), (3.34b) when λ = (kπ)
2,
k ∈ N.
Let us now consider (3.34c), (3.34d), another second-order ordinary differential
equation, but with one boundary condition missing. We will use this missing condi￾tion to satisfy the inhomogeneous condition (3.31d). If we write λ = β2, β > 0, then
the general solution of (3.34c) is
α1eβy + α2e−βy .
Substituting this in the boundary condition (3.34d) (that is, when y = 0), we obtain
0 = α1 + α2 and thus Y(y) = α1(eβy − e−βy) = 2α1 sinh(βy). Now we saw above that
there are solutions of (3.34a), (3.34b) whenever λ is of the form λk = (kπ)
2; hence
for βk = kπ we obtain the particular solution
uk (x, y) = sin(kπx) · sinh(kπy), k ∈ N.
Hence under suitable convergence assumptions the series
u(x, y) =
∞
k=1
ck uk (x, y), ck ∈ R, (3.35)
satisfies the conditions (3.31a) – (3.31c). We are still missing the boundary condition
(3.31d). Since g ∈ C([0, 1]) and g(0) = g(1) = 0, it admits an expansion in sine
functions of the form
g(x) =
∞
k=1
bk sin(kπx)3.3 Laplace’s equation 65
with Fourier coefficients
bk = 2
∫ 1
0
g(x)sin(kπx) dx. (3.36)
The convergence of this series, however, is in general only in the sense of Abel (see
Corollary 3.19). We will now proceed with calculations which are, at least for the
moment, merely formal. When y = 1 the representation (3.35) of u(x, 1) reads
u(x, 1) =
∞
k=1
ck sin(kπx)sinh(kπ).
Equating coefficients, we have
ck = bk
sinh(kπ)
, k ∈ N.
We thus obtain the function
u(x, y) :=
∞
k=1
bk
sinh(kπ)
sin(kπx) · sinh(kπy) (3.37)
as a candidate for the solution. This series converges uniformly in every rectangle of
the form [0, 1]×[0, b], b < 1. Note that for 0 ≤ y ≤ b < 1
sinh(kπy)
sinh(kπ) = ekπy − e−kπy
ekπ − e−kπ = ekπy
ekπ
1 − e−2kπy
1 − e−2kπ ≤ ekπy
ekπ = ekπ(y−1) ≤

eπ(b−1)
k
.
Since the series of partial derivatives of (3.37) of order k also converges uniformly in
every such rectangle for every k ∈ N, (3.37) defines a function u ∈ C∞([0, 1]×[0, 1))
and we have
Δu = 0 in (0, 1)×(0, 1).
We will see that
lim
y↑1
u(x, y) = g(x)
uniformly in x ∈ [0, 1]. Thus u solves the Dirichlet problem (3.31a) – (3.31d).
To prove this last step we need the maximum principle, which we will prove in
Section 3.3.3; this will also yield uniqueness of the solution.
3.3.2 The Dirichlet problem on the disk
We now wish to solve the Dirichlet problem on the disk66 3 Elementary methods
D := {x ∈ R2 : |x| < 1}
using Fourier series; we will even derive an explicit formula for the solutions. Here
we denote by |x| = (x2
1 + x2
2 )
1/2 the Euclidean norm of x = (x1, x2) ∈ R2.
To find solutions we will again apply the method of separation of variables.
Of course this technique only allows us to find very special solutions, namely
ones which themselves are characterized by separated variables. However, if we
superimpose these, we can construct general solutions. Abel’s convergence theorem
(Theorem 3.11) gives us exactly the convergence result we will need for this.
The Dirichlet problem reads as follows. Given g ∈ C(∂D), we wish to find a
function v ∈ C2(D) ∩ C(D) which solves
Δv(x) = 0, x ∈ D, (3.38a)
v(x) = g(x), x ∈ ∂D. (3.38b)
Here we also introduce the punctured disk
D := {x ∈ R2 : 0 < |x| < 1}.
At this stage it makes sense to introduce polar coordinates.
Change of variables: polar coordinates
Lemma 3.21 (Polar coordinates) Let v : D → R and u : (0, 1) × R → R be such
that u(r, θ) = v(r cos θ,r sin θ). Then u is twice continuously differentiable if and
only if v ∈ C2(D ), and in this case we have
Δv(r cos θ,r sin θ) = urr + ur
r + uθθ
r2 .
The proof is a simple exercise in calculating the various partial derivatives; see
Exercise 3.6.
The boundary of D is the unit circle
Γ := ∂D = {x ∈ R2 : |x| = 1}.
Let g ∈ C(Γ) and set f (θ) := g(cos θ,sin θ); then f ∈ C2π. The coordinate transfor￾mation of Lemma 3.21 leads us to the following problem:
find a function u ∈ C2((0, 1) × R) such that u(r, ·) ∈ C2π and
urr + ur
r + uθθ
r2 = 0, (3.39a)
lim
r ↑1
u(r, θ) = f (θ) uniformly in θ ∈ R, (3.39b)
lim
r ↓0
u(r, θ) =: c exists with uniform convergence in θ ∈ R. (3.39c)3.3 Laplace’s equation 67
If u ∈ C2((0, 1) × R) is a solution of (3.39) then we set
v(r cos θ,r sin θ) :=
⎧⎪⎪⎪⎪⎨
⎪⎪⎪⎪
⎩
f (θ) if r = 1,
u(r, θ) if 0 < r < 1,
c if r = 0 .
It is to be expected that v is a solution of (3.38). It is clear from Lemma 3.21 that
v ∈ C(D), v|Γ = g, v ∈ C2(D ) and Δv = 0 on D . It remains to show that v is twice
continuously differentiable at the origin. This could be derived from (3.39); however,
we will first look for a solution of (3.39) and return to the question of differentiability
later.
Separation of variables
In order to solve (3.39a) we will again use the method of separation of variables.
We will first try to find a solution of (3.39) of the form
u(r, θ) = v(r) w(θ), (3.40)
where v ∈ C2(0, 1) ∩C([0, 1]), and w ∈ C2(R) is a 2π-periodic function. In this case,
0 = urr + ur
r + uθθ
r2 = v(r)w(θ) + v
(r)
r w(θ) + v(r)
r2 w(θ).
If the functions do not vanish anywhere, then we may deduce that
r2 v(r)
v(r) + r
v
(r)
v(r) = −w(θ)
w(θ) . (3.41)
This equation should hold for arbitrary r ∈ (0, 1) and θ ∈ R. Since the left-hand side
is independent of θ and the right-hand side is independent of r, both sides of (3.41)
must be constant. We conclude that there exists some λ ∈ R such that
r2 v(r)
v(r) + r
v
(r)
v(r) = λ, (3.42)
w(θ)
w(θ) = −λ. (3.43)
These are two ordinary differential equations of second order. Since w should be
periodic, we require that
w(0) = w(2π), w
(0) = w
(2π).68 3 Elementary methods
The second equation (3.43) has solutions when λ = n2, n ∈ N, and in this case these
are given by
wn(θ) = an cos(nθ) + bn sin(nθ). (3.44)
The first equation (3.42) readsr2v+rv = n2v, corresponding to λ = n2. A solution
is v(r) = rn. Hence our ansatz (3.40) of separated variables leads us to the special
solution
un(r, θ) = rn(an cos(nθ) + bn sin(nθ)) (3.45)
of the partial differential equation (3.39a). It is also easy to check that functions of
the form (3.45) are indeed solutions of the equation (3.39a). Here an, bn ∈ R are
still arbitrary; however, in this case the functions (3.45) will not generally satisfy the
boundary condition (3.39b). We now set
u(r, θ) := c0 +
∞
n=1
rn #
an cos(nθ) + bn sin(nθ)
$ (3.46)
and choose c0, an, bn to be the Fourier coefficients of f . Then (3.46) defines a function
u : (0, 1) × R → R which is 2π-periodic in θ. By Abel’s convergence theorem in the
form of Corollary (3.18) we then have
lim
r ↑1
u(r, θ) = f (θ)
uniformly in θ ∈ R. Moreover,
lim
r ↓0
u(r, θ) = c0
uniformly in θ. Since the series of partial derivatives converges uniformly on [δ, 1−δ]
for every δ ∈ (0, 1), we conclude that the function u solves the problem (3.39).
We wish to express the function u explicitly in terms of f . To this end we first
recall that u(r, ·) = jr ∗ f ; see (3.29). Substituting the expression (3.22) for jr , we
obtain
u(r, θ) = 1
2π
∫ 2π
0
f (t) 1 − r2
1 − 2r cos(t − θ) + r2 dt.
If we switch back to Cartesian coordinates x = r cos(θ), y = r sin(θ), then x2 + y2 =
r2 and (x − cost)
2 + (y − sin t)
2 = 1 − 2r cos(t − θ) + r2. Hence
v(x, y) = u(r, θ) = 1 − x2 − y2
2π
∫ 2π
0
f (t)
(x − cost)2 + (y − sin t)2 dt.
Standard theorems on exchanging integral and derivative allow us to conclude that
v ∈ C2(D), in fact v ∈ C∞(D). We have thus proved the following result.3.3 Laplace’s equation 69
Theorem 3.22 (Poisson formula for the Dirichlet problem on the disk) Given
a continuous function g : Γ → R, the Dirichlet problem (3.38) has a solution
v ∈ C2(D) ∩ C(D) given by
v(r cos θ,r sin θ) = 1
2π
∫ 2π
0
f (t) 1 − r2
1 − 2r cos(t − θ) + r2 dt, (3.47)
where f (t) := g(cost,sin t).
Remark 3.23 We will see in the next section that (3.47) is the unique solution of
(3.38); see Corollary 3.27 (page 71). 
We call (3.47) the Poisson formula and
R(r, t) = 1 − r2
1 − 2r cost + r2
the Poisson kernel. The latter is an integral kernel for the solution, that is,
v(r cos(θ),r sin(θ)) = 1
2π
∫ 2π
0
f (t) R(r, t − θ) dt.
In the case of the disk we can thus express the solution v explicitly in terms of such
a kernel. This also allows us to deduce that v is infinitely differentiable in D. We will
see later that this is in fact the case for every solution of the homogeneous Laplace
equation, that is, for every harmonic function (Theorem 6.58).
3.3.3 The elliptic maximum principle
We have now managed to construct solutions for the Dirichlet problem in two cases:
the unit square and the disk; however, up until now, we have not been able to say
whether these are the only possible solutions or whether there are others. In fact, the
respective solutions are unique. This is a consequence of the maximum principle,
which we now wish to formulate and prove in a more general setting, namely for a
general domain Ω ⊂ Rd. We first require some preparation.
Definition 3.24 A function u ∈ C2(Ω) is called harmonic (subharmonic, superhar￾monic) if
Δu(x) = 0 (−Δu(x) ≤ 0, −Δu(x) ≥ 0)
for all x ∈ Ω. 
Now let Ω be an arbitrary bounded open set in Rd; then its boundary ∂Ω is a
compact set. It is in this general context that we formulate the following problem.70 3 Elementary methods
Dirichlet problem: given g ∈ C(∂Ω), find a function u ∈ C2(Ω) ∩ C(Ω) such that
Δu(x) = 0, x ∈ Ω, (3.48a)
u(x) = g(x), x ∈ ∂Ω. (3.48b)
We are thus looking for a harmonic function u in Ω which is continuous up to
the boundary ∂Ω and takes on the prescribed values g(x) there. Uniqueness of the
solution, as mentioned above, is a consequence of the following maximum principle.
Theorem 3.25 (Elliptic maximum principle) Let Ω ⊂ Rd be bounded and open
and let u ∈ C(Ω) ∩ C2(Ω) be subharmonic. Then
max
x∈Ω
u(x) = max
x∈∂Ω u(x).
Remark 3.26 Note that u has a maximum on Ω since Ω is compact and u is continuous
on Ω. Theorem 3.25 states that this maximum is attained on the boundary of Ω. 
Proof Suppose that u(x) ≤ M for all x ∈ ∂Ω; it suffices to show that u(x) ≤ M for
all x ∈ Ω. Without loss of generality we may assume that M = 0, since otherwise
we simply replace u by u − M.
We give a proof by contradiction. Suppose that
c := max
x∈Ω
u(x) > 0.
Set  := maxx∈Ω |x|
2 and choose ε > 0 in such a way that ε < c. Now define
v(x) := u(x) + ε|x|
2
, x ∈ Ω;
then maxx∈Ω v(x) ≥ c> ε, while on the boundary maxx∈∂Ω v(x) ≤ ε. Hence there
exists some x0 ∈ Ω such that v(x0) = maxx∈Ω v(x). Since x0 ∈ Ω, it follows that
∂2
∂x2
j
v(x0) = d2
dt2 v(x0 + tej)



t=0
≤ 0
for all j = 1,..., d, where ej is the jth unit coordinate vector. Hence Δv(x0) ≤ 0.
Now since
Δ(|x|
2) = Δ

d
i=1
x2
i

=

d
i=1
2 = 2d,
it follows that
−Δu(x0) = −Δv(x0) + 2dε ≥ 2dε .
This is a contradiction to the assumption that u is subharmonic. 3.3 Laplace’s equation 71
Corollary 3.27 (Uniqueness) Given g ∈ C(∂Ω), let u be a solution of the corre￾sponding Dirichlet problem (3.48). Then
min
s∈∂Ω g(s) ≤ u(x) ≤ max
s∈∂Ω g(s) for all x ∈ Ω. (3.49)
In particular for each g ∈ C(∂Ω) there exists at most one solution of (3.48).
Proof The second inequality follows from Theorem 3.25. If we apply this theorem
to the function −g instead of g, then we also obtain the first inequality. In particular
u ≡ 0 if g ≡ 0.
Uniqueness in the Dirichlet problem now follows by considering the difference
of two solutions of (3.48) for the same g. This difference is zero on ∂Ω due to the
identical boundary conditions, and so the above statement implies that the difference
is identically zero on the whole of Ω. 
It follows in particular that the solutions we constructed for the unit square and the
disk are indeed unique. But the maximum principle does not just yield the uniqueness
of solutions of the Dirichlet problem, it also gives us continuous dependence of the
solution (assuming one exists) on the data. More precisely, Corollary 3.27 implies
the a priori estimate
uC(Ω) ≤ gC(∂Ω), (3.50)
where g ∈ C(∂Ω) is the given boundary function and u is the solution of (3.48).
Here we consider the supremum norms
uC(Ω) = sup
x∈Ω
|u(x)| bzw. gC(∂Ω) = sup
x∈∂Ω
|g(x)|
on C(Ω) and C(∂Ω), respectively, which induce uniform convergence on the respec￾tive spaces. Thus if g, gn ∈ C(∂Ω), if u, un ∈ C(Ω) are solutions of (3.48) for g and
gn, respectively, and if gn converges uniformly on ∂Ω to g, then by linearity
un − uC(Ω) ≤ gn − gC(∂Ω),
that is, un converges uniformly in Ω to u.
We say that a problem is well-posed (in the sense of Hadamard, cf. Section 4.8)
if for every input function (in this case g ∈ C(∂Ω)) there exists a unique solution,
and this solution depends continuously on the input function. Here, the maximum
principle yields uniqueness and continuous dependence. We have already proved
existence in the cases of the square and the disk; we will now summarize our results.72 3 Elementary methods
3.3.4 Well-posedness of the Dirichlet problem for the square and the
disk
We start with the square Ω = (0, 1)×(0, 1). We first give the missing proof that the
function (3.37) really is a solution. For this, we use the a priori estimate (3.50).
Theorem 3.28 The problem (3.31) has a unique solution u, which is given by (3.37).
Moreover, u ∈ C∞((0, 1)×(0, 1)) and uC([0,1]2) ≤ gC([0,1]).
Proof Uniqueness and continuous dependence on g follow from the maximum
principle, as explained above. Let g ∈ C([0, 1]), g(0) = g(1) = 0, be the boundary
value for y = 1 in accordance with (3.31d). Then by Corollary 3.20 there exist
trigonometric polynomials of the form
gn(x) :=

Nn
k=1
bn
k sin(kπx)
such that limn→∞ gn = g in C([0, 1]). Set
un(x, y) =

Nn
k=1
bn
k
1
sinh(kπ)
sin(kπx)sinh(kπy); (3.51)
then un is the solution of (3.31) with boundary data gn in place of g when y = 1. It
follows from (3.50) that
un − um C(Ω) ≤ gn − gm C(∂Ω).
Thus (un)n∈N is a Cauchy sequence in C(Ω). Let w := limn→∞ un be the correspond￾ing limit in C(Ω). Since un(x, 1) = gn(x) → g(x), we have w(x, 1) = g(x) for all
x ∈ [0, 1]. Since the coefficients bn
k are bounded, limn→∞ bn
k = bk and
0 ≤
sinh(kπy)
sinh(kπ) ≤

eπy
eπ
k
,
it follows from (3.51) upon passing to the limit as n → ∞ that
w(x, y) =
∞
k=1
bk
1
sinh(kπ)
sin(kπx)sinh(kπy)
for all x ∈ [0, 1] and y ∈ [0, 1). Thus w is equal to the function u from (3.37)
on [0, 1]×[0, 1). But since w ∈ C([0, 1]×[0, 1]), it follows that limy↑1 u(x, y) =
w(x, 1) = g(x) uniformly in x ∈ [0, 1]. Thus the function defined by (3.37) is indeed
a (and hence the) solution of (3.31). We already observed in Section 3.3.1 that
u ∈ C∞((0, 1)×(0, 1)) and Δu = 0. This completes the proof. 3.3 Laplace’s equation 73
We can now show easily that the Dirichlet problem for the square is well-posed.
For this, we consider arbitrary boundary functions (instead of functions which vanish
on three sides). We recall that if Ω ⊂ Rd is bounded and open and g ∈ C(∂Ω), then
the corresponding Dirichlet problem is as follows: find u ∈ C2(Ω) ∩ C(Ω) such that
Δu = 0, in Ω, (3.52a)
u = g, on ∂Ω. (3.52b)
Theorem 3.29 Let Ω = (0, 1)×(0, 1) and g ∈ C(∂Ω). Then there exists a unique
solution u of (3.52). Moreover, u ∈ C∞(Ω) and uC(Ω) ≤ gC(∂Ω).
Proof We only need to prove existence.
1. The problem is linear in g: if g1, g2 ∈ C(∂Ω) and α, β ∈ R, and if u1, u2 are
solutions of (3.52) with boundary data g1 and g2, respectively, then αu1 + βu2
is the solution of (3.52) with boundary data αg1 + βg2.
2. Theorem 3.28 yields the existence of a solution for any function g ∈ C(∂Ω)
which equals 0 on three sides. Combining this with the observation in 1., we can
thus find a unique solution for every function g ∈ C(∂Ω) which vanishes in all
four corners.
3. The functions u1(x, y) := (1− x)·(1− y), u2(x, y) := x · (1− y), u3(x, y) := x · y,
u4(x, y) := (1 − x) · y are solutions which take on the value 1 at (0, 0), (1, 0),
(1, 1), (0, 1), respectively, and in the other three corners take on the value 0. Now
let g ∈ C(∂Ω) be arbitrary. Then the function
g0 := g − %
g(0, 0)u1 + g(1, 0)u2 + g(1, 1)u3 + g(0, 1)u4
&
|∂Ω ∈ C(∂Ω)
is equal to 0 in all four corners. Hence by 1. and 2. there exists a solution v
of (3.52) with boundary data g0. It follows that u = v + g(0, 0)u1 + g(1, 0)u2 +
g(1, 1)u3 + g(0, 1)u4 is the solution of (3.52). 
Small modifications of the above arguments show that the Dirichlet problem on
an arbitrary rectangle has a unique solution for any continuous boundary function.
We will generalize this statement to a broader class of domains in Chapter 6 (without,
however, finding explicit solutions).
For the disk D we have already found a solution, namely (3.46). We also now
know that this solution is unique. We have thus proved the following theorem.
Theorem 3.30 Let Ω = D = {x ∈ R2 : |x| < 1} and let g ∈ C(∂Ω). Then (3.52) has
exactly one solution, which is given by
u(r cos θ,r sin θ) = c0 +
∞
k=1
rk {ak cos(kθ) + bk sin(kθ)},
where c0 = 1
2π
∫ 2π
0 g(cost,sin t) dt and74 3 Elementary methods
ak = 1
π
∫ 2π
0
cos(kt) g(cost,sin t) dt, bk = 1
π
∫ 2π
0
sin(kt) g(cost,sin t) dt.
In particular, u ∈ C∞(Ω) and uC(Ω) ≤ gC(∂Ω).
The fact that
lim
r ↑1
u(r cos θ,r sin θ) = g(cos θ,sin θ)
uniformly in θ ∈ R corresponds to the statement of Abel’s convergence theorem for
Fourier series in this case. This also yields the density of the set of trigonometric
polynomials in C2π. There are however other ways to prove this density statement.
For example, it is a direct consequence of the theorem of Stone–Weierstrass (see
Theorem A.5). Once we know that the trigonometric polynomials are dense in
C2π, we can directly conclude from the maximum principle that u is the solution
of (3.53) using exactly the same arguments as we used in Theorem 3.28 for the
square. This leads to another proof of Abel’s convergence theorem for Fourier series
(Corollary 3.18) based on the elliptic maximum principle; see Exercise 3.4.
3.4 The heat equation
In this section we will consider the heat equation; we will also derive solution
formulae for this equation. To this end, we first study the case of one space dimension,
for which we use the method of separation of variables. Afterwards, we will prove
a maximum principle for the heat equation (known as the parabolic maximum
principle) and infer from it the uniqueness of the obtained solution, among other
things. This maximum principle is also valid for the heat equation on an arbitrary
domain.
We will then investigate the heat equation on the whole space Rd, where we can
give an explicit formula for the solutions, in Section 3.4.4.
3.4.1 Separation of variables
We consider the case of one space dimension, that is, d = 1 and Ω = (0, π). Our
problem is then as follows. Let u0 ∈ C([0, π]) be a given function such that
u0(0) = u0(π) = 0. (3.53)
We seek a continuous function u : [0, ∞) × [0, π] → R which is continuously
differentiable in t ∈ [0, ∞) and twice continuously differentiable in x ∈ (0, π), on
(0, ∞) × (0, π), such that3.4 The heat equation 75
ut = uxx, t > 0, x ∈ (0, π), (3.54a)
u(t, 0) = u(t, π) = 0, t ≥ 0, (3.54b)
u(0, x) = u0(x), x ∈ [0, π]. (3.54c)
Here (3.54c) is the initial condition and (3.54b) is the boundary condition (in this
case a homogeneous Dirichlet condition). We will prove existence and uniqueness of
solutions of the problem (3.54) in Section 3.4.3. We first look for a special solution
of (3.54a) via the ansatz of separation of variables; that is, we set
u(t, x) = w(t)v(x)
for functions v : [0, π] → R and w : [0, ∞) → R which are to be determined. Now
if u is a solution of (3.54a), then
0 = ut(t, x) − uxx(t, x) = v(x) w(t) − v(x) w(t), t > 0, x ∈ (0, π).
If v(x)  0 for all x ∈ (0, π) and w(t)  0 for all t > 0, then we may divide by these
terms to obtain
w(t)
w(t) = v(x)
v(x) = −λ
for all t > 0 and x ∈ (0, π). The existence of such a constant λ ∈ R follows, as in the
case of Laplace’s equation, from the fact that the first term is independent of x and
the second is independent of t.
This once again gives us two ordinary differential equations
w(t) + λ w(t) = 0, t > 0, v(x) + λ v(x) = 0, x ∈ (0, π).
The solution of the first equation reads w(t) = c e−λt for some constant c ∈ R, and
so u(t, x) = c e−λt v(x).
We only wish to consider bounded solutions2 and therefore assume that λ ≥ 0.
It follows that v, as the solution of the above second-order ordinary differential
equation, has the form
v(x) = a cos(
√
λ x) + b sin(
√
λ x) .
The boundary conditions (3.53) imply
a = v(0) = 0 and b sin(
√
λ π) = v(π) = 0.
This means that √
λ ∈ N, that is, λ = k2 for some k ∈ N, provided that u  0. We
thus obtain
2 In fact, it is possible to prove using the parabolic maximum principle in the next section that every
solution of (3.54) is bounded.76 3 Elementary methods
uk (t, x) = bk e−k2t sin(k x), k ∈ N,
as a family of special solutions.
Linear combinations of these functions are solutions of (3.54a), (3.54b). We now
wish to construct series of these special solutions which satisfy the initial condition
(3.54c). For this, we apply Corollary 3.19 to the function u0 and obtain
u0(x) = lim
r ↑1
∞
k=1
rk bk sin(k x),
where
bk = 2
π
∫ π
0
u0(s) sin(ks) ds.
This leads us to look for solutions of (3.54) of the form
u(t, x) :=
∞
k=1
bk e−k2t sin(k x). (3.55)
We expect that limt↓0 u(t, x) = u0(x). In any case, u satisfies the conditions (3.54a),
(3.54b), as can be seen as follows. We have
∞
k=1
|bk | km e−k2t < ∞
for all m ∈ N0 := {0, 1, 2,...} and t > 0. Hence the function u : (0, ∞) × [0, π] → R
defined by (3.55) is infinitely differentiable, and for all t > 0 and x ∈ (0, π) we have
ut(t, x) =
∞
k=1
bk (−k2) e−k2t sin(k x) =
∞
k=1
bk e−k2t d2
dx2 sin(k x) = uxx(t, x).
We have shown that u satisfies the equation (3.54b); moreover, the boundary condi￾tion (3.54b) is also satisfied. It remains to show that
lim
t↓0
u(t, x) = u0(x)
uniformly in x ∈ [0, π]. We will do this in Section 3.4.3 using the parabolic maximum
principle.
3.4.2 The parabolic maximum principle
As explained earlier, our goal here is to prove a maximum principle for the heat
equation. As done in the case of Laplace’s equation we will consider the general case
of an arbitrary bounded open set Ω ⊂ Rd and the heat equation in the form ut = Δu.3.4 The heat equation 77
So let Ω ⊂ Rd be a bounded open set with boundary ∂Ω. We consider a time
interval [0, T], T > 0, and denote by
ΩT := (0, T) × Ω
the corresponding cylinder in time-space coordinates. The set
∂∗
ΩT := ([0, T] × ∂Ω) ∪ ({0} × Ω)
is called the parabolic boundary of ΩT . If d = 2 and Ω is drawn as a set in the
horizontal (that is, xy) plane, then we can imagine the vertical z-axis as the time
axis, perpendicular to the space variables. In this case, ΩT is a piece of cylinder, like
a can, with horizontal cross-section Ω, and ∂∗ΩT is its boundary without the lid, cf.
Figure 3.2.
t
x1
x2

T
Fig. 3.2 ΩT together with its parabolic boundary ∂∗ΩT .
We now define the space C1,2(ΩT ) as the set of all continuous functions u :
(0, T) × Ω → R whose partial derivatives ut, ∂u
∂xj
, ∂2u
∂xi∂xj
, i, j = 1,..., d exist and
are continuous in ΩT . In particular, if u ∈ C1,2(ΩT ), then ut − Δu ∈ C(ΩT ). The
following maximum principle is an easy consequence of simple properties of maxima
of differentiable functions of one variable.
Theorem 3.31 (Parabolic maximum principle) Suppose that the function u ∈
C(ΩT ) ∩ C1,2(ΩT ) satisfies
ut(t, x) ≤ Δu(t, x) for all (t, x) ∈ ΩT .
Then max(t,x)∈ΩT u(t, x) = max(t,x)∈∂∗ΩT u(t, x).78 3 Elementary methods
Remark 3.32 Note that the function u is continuous on the compact set ΩT = [0, T]×
Ω and thus attains a maximum on this set. The parabolic boundary ∂∗ΩT is a compact
subset of ΩT , and the theorem states that the overall maximum of u on ΩT is attained
on this subset. 
Proof Fix 0 < T < T. We will show that
max
(t,x)∈ΩT
u(t, x) = max
(t,x)∈∂∗ΩT
u(t, x).
This implies the claim since
max
(t,x)∈ΩT
u(t, x) = sup
T<T
max
(t,x)∈ΩT
u(t, x)
= sup
T<T
max
(t,x)∈∂∗ΩT
u(t, x) = max
(t,x)∈∂∗ΩT
u(t, x).
1. We first assume that ut < Δu on ΩT. Since ΩT is compact and u is continuous
on ΩT, there exists a point (t0, x0) ∈ ΩT such that
u(t0, x0) = max
(t,x)∈ΩT
u(t, x).
We claim that (t0, x0) ∈ ∂∗ΩT. Indeed, if not, then 0 < t0 ≤ T and x0 ∈ Ω.
Since u(t0, x0) = max0<s≤T u(s, x0), it follows that ut(t0, x0) ≥ 0. But since
u(t0, x0) = maxy∈Ω u(t0, y), we obtain that ∂2
∂x2
j
u(t0, x0) ≤ 0 for all j = 1,..., d,
whence Δu(t0, x0) ≤ 0. This is a contradiction to our assumption, since then
0 ≤ ut(t0, x0) < Δu(t0, x0) ≤ 0. Hence (t0, x0) ∈ ∂∗ΩT, as claimed.
2. Set v : u − εt for some given ε > 0. Then vt = ut − ε ≤ Δu − ε < Δu = Δv. We
now apply 1. to v to obtain
max
(t,x)∈ΩT
u(t, x) = max
(t,x)∈ΩT
#
v(t, x) + εt
$
≤ max
(t,x)∈ΩT
#
v(t, x) + εT
$
= max
(t,x)∈∂∗ΩT
#
v(t, x) + εT
$
≤ max
(t,x)∈∂∗ΩT
#
u(t, x) + εT
$
.
Since ε > 0 was arbitrary, the claim of the theorem follows. 
As a consequence we obtain the following result on uniqueness of solutions of
the heat equation.
Corollary 3.33 (Uniqueness) Suppose that u, v ∈ C(ΩT ) ∩ C1,2(ΩT ) satisfy
ut = Δu and vt = Δv in ΩT .
If u = v on the parabolic boundary ∂∗ΩT , then u = v identically on the whole of
ΩT .3.4 The heat equation 79
Proof The difference w of the two solutions vanishes everywhere on the parabolic
boundary. It follows from the parabolic maximum principle that w(t, x) ≤ 0 for
all t ∈ [0, T] and x ∈ Ω. Since the same holds for −w in place of w, we see that
w(t, x) = 0 for all t ∈ [0, T] and x ∈ Ω. 
Thus, if two solutions of the heat equation agree on the parabolic boundary, then
they are identical to each other. This leads to a uniqueness result for the following
parabolic initial-boundary value problem (IBVP).
ut = Δu, in ΩT, (3.56a)
u(t, x) = 0, x ∈ ∂Ω, t ∈ [0, T], (3.56b)
u(0, x) = u0(x), x ∈ Ω. (3.56c)
We make the natural regularity assumption on u which guarantees that (3.56a)
makes sense: the function u should be once continuously differentiable with respect to
t and twice with respect to x; this corresponds exactly to the condition u ∈ C1,2(ΩT ).
Now we only require that the equation hold in the open set ΩT := (0, T)×Ω; however,
the function should be continuous up to the boundary, that is, u ∈ C(ΩT ). In (3.56b)
we impose the condition that the function vanishes on the boundary, that is, we
impose homogeneous Dirichlet boundary conditions. The condition (3.56c) is the
initial condition for u at t = 0. In order for a solution to exist, the boundary and initial
conditions must be compatible; for this, we require that the initial value u0 ∈ C0(Ω),
where
C0(Ω) := {u ∈ C(Ω) : u|∂Ω = 0}.
This guarantees that the conditions (3.56b) and (3.56c) are compatible with each
other. For the initial-boundary value problem (3.56) we have the following maximum
principle.
Theorem 3.34 (Parabolic maximum principle) Let u0 ∈ C0(Ω) be given. If u is a
solution of (3.56) with initial condition u0, then the following estimate holds:
min
y∈Ω
u0(y) ≤ u(t, x) ≤ max
y∈Ω
u0(y) (3.57)
for all t ∈ [0, T] and x ∈ Ω. In particular,
|u(t, x)| ≤ sup
y∈Ω
|u0(y)| (3.58)
for all t ∈ [0, T] and x ∈ Ω.
Proof Set
c := max
y∈Ω
u0(y).80 3 Elementary methods
Then c ≥ 0 since u0 = 0 on ∂Ω, and thus u ≤ c on ∂∗ΩT . Hence u ≤ c on ΩT by
Theorem 3.31. This proves the second inequality in (3.57). Replacing u by −u leads
to the first inequality. 
Corollary 3.35 (Uniqueness for the IBVP for the heat equation) Given an initial
value u0 ∈ C0(Ω), there exists at most one solution of (3.56).
Proof Theorem 3.34 proves the claim for u0 ≡ 0. The general case follows from
considering the difference of two solutions, analogous to the proof of Corol￾lary 3.33. 
3.4.3 Well-posedness of the parabolic initial-boundary value problem
on the interval
The parabolic maximum principle yields not only uniqueness of solutions of (3.56),
but also the a priori estimate (3.58). This allows us to prove well-posedness of the
problem in the case where Ω is an interval.
We consider the following initial-boundary value problem: given u0 ∈ C0(0, π),
we seek a solution u of
ut = uxx, in (0, ∞) × (0, π), (3.59a)
u(t, 0) = u(t, π) = 0, t ≥ 0, (3.59b)
u(0, x) = u0(x), x ∈ [0, π]. (3.59c)
Theorem 3.36 Let u0 ∈ C([0, π]) be such that u0(0) = u0(π) = 0. Then (3.59) has
a unique solution u ∈ C∞ 
(0, ∞) × [0, π]
	
∩ C
[0, ∞) × [0, π]
	
. Moreover, for this
solution we have uC([0,∞)×[0,π]) ≤ u0 C([0,π]).
Proof We have already proved uniqueness and continuous dependence on the data;
we still need to establish existence. Let u0n ∈ C([0, π]) be trigonometric polynomials
of the form
u0n(x) =
∞
k=1
bn
k sin(k x)
such that u0n → u0 inC([0, π]) (see Corollary 3.19). Here, for fixed n ∈ N, bn
k = 0 for
all but finitely many k ∈ N, while limn→∞ bn
k = bk (cf. the proof of Theorem 3.28).
Now for the initial value u0n the solution of (3.59) is given by
un(t, x) =
∞
k=1
bn
k e−k2t sin(k x). (3.60)3.4 The heat equation 81
We already derived this formula in Section 3.4.2, but it can be easily checked by
explicit computation. Now (3.58) implies that
un − umC([0,T]×[0,π]) ≤ un0 − um0 C[0,π]
for any T > 0. Consequently, (un)n∈N is a Cauchy sequence in the Banach space
C([0, T]×[0, π]), and so there exists a function u ∈ C([0, ∞) × [0, π]) such that
un(t, x) → u(t, x) uniformly in every rectangle [0, T]×[0, π], T > 0. In particular,
u(0, x) := limn→∞ un(0, x) = u0(x) for all x ∈ [0, π]. Meanwhile, it follows from
(3.60) that
u(t, x) =
∞
k=1
bk e−k2t sin(k x)
for all t > 0 and x ∈ [0, π]. We already saw at the end of Section 3.4.1 that
u ∈ C∞ 
(0, ∞) × [0, π]
	
and ut = uxx. Hence u is the solution of (3.59). 
For our numerical treatment of the problem in Chapter 9, it will be convenient to
transform the above result to the special case of the unit interval in space and a finite
interval in time.
Corollary 3.37 Let u0 ∈ C([0, 1]) be such that u0(0) = u0(1) = 0 and let 0 < T < ∞.
Then there exists a unique u ∈ C∞ 
(0, T]×[0, 1]
	
∩ C
[0, T]×[0, 1]
	
satisfying
ut = uxx, t ∈ (0, T], x ∈ [0, 1], (3.61a)
u(t, 0) = u(t, 1) = 0,t ∈ [0, T], (3.61b)
u(0, x) = u0(x), x ∈ [0, 1]. (3.61c)
This function u is given by
u(t, x) =
∞
k=1
e−k2π2t
bk sin(kπx),
where bk = 2
∫ 1
0 u0(x) sin(kπx) dx, k ∈ N and uC([0,T]×[0,1]) ≤ u0 C([0,1]).
The parabolic maximum principle shows continuous dependence on the data, and
that there can be at most one solution u ∈ C1,2 
(0, T)×(0, 1)
	
∩C
[0, T]×[0, 1]
	
. The
representation of this solution as a series shows that it is actually inC∞ 
(0, T]×[0, 1]
	
.
For the numerical treatment and in particular for a meaningful error estimate on
the approximate solutions we require somewhat more regularity at t = 0.
Theorem 3.38 Let u0 ∈ C4([0, 1]) be such that u(m)
0 (0) = u(m)
(1) = 0 for m = 0, 2, 4.
Then the solution u of (3.61) is inC2,4 
[0, T]×[0, 1]
	
, that is, u has partial derivatives
at least up to order two in t and four in x in the open rectangle (0, T)×(0, 1), and
these all admit continuous extensions to [0, T]×[0, 1].82 3 Elementary methods
Proof We set sk (x) := sin(kπx), λk := k2π2, b
(m)
k := 2
∫ 1
0 u(m)
0 (x)sk (x) dx, m =
2, 4. Then b
(2)
k = −λkbk , b
(4)
k = λ2
kbk , as is easy to see via integration by parts.
Differentiating componentwise, we obtain
uxx(t, x) =
∞
k=1
e−λk t
bk s
k (x) = −
∞
k=1
e−λk t
bkλk sk (x) =
∞
k=1
e−λk t
b
(2)
k sk (x)
for all t > 0 and x ∈ [0, 1]. Thus limt↓0 uxx(t, x) = u
0 (x) uniformly in x ∈ [0, 1], as
follows by applying Corollary 3.37 to u
0 in place of u0. The same argument shows
that
uxxxx(t, x) =
∞
k=1
e−λk t
bk s
(4)
k (x) =
∞
k=1
e−λk t
b
(4)
k sk (x)
converges uniformly to u(4)
0 (x). It follows from Exercise 3.20 that u ∈ C2,4 
[0, T] ×
[0, 1]
	
. 
3.4.4 The heat equation in Rd
We now wish to study the heat equation in the whole space Rd. This will also be
useful when we come to the Black–Scholes equation in a later section. We first
consider the one-dimensional case d = 1. Analogously to what we did above, we
denote by C1,2((0, ∞) × R) the space of those functions u = u(t, x) whose partial
derivatives ut, ux, uxx exist and are continuous in (0, ∞) × R.
Our goal is to find solutions u ∈ C1,2((0, ∞) × R) of the heat equation
ut = uxx, t > 0, x ∈ R. (3.62)
The following arguments will allow us to construct a solution. Assume that
u ∈ C1,2((0, ∞) × R) is a solution of (3.62) and let a > 0. Then
v(t, x) := u(at,
√
ax) (3.63)
also defines a solution of (3.62), as can be checked by direct computation. We will
attempt to find a solution u which is invariant under the change of variables (3.63),
that is, we want
u(t, x) = u(at,
√
ax), t > 0, x ∈ R
to hold for all a > 0. If we make the particular choice a = 1
t , then we obtain
u(t, x) = u

1, x
√
t

,3.4 The heat equation 83
whence u(t, x) = g
 x
√
t
	
for g(y) := u(1, y). If such a function u is a solution of
(3.62), then g ∈ C2(R) and
ut = g
 x
√
t
  − 1
2
 x
t3/2 , ux = g
 x
√
t
 1
√
t
, uxx = g  x
√
t
 1
t
.
This leads to
0 = ut − uxx = −1
t

1
2
pg
(p) + g(p)

,
where p := x
√
t
. If we set h(p) := g
(p), then h satisfies the ordinary differential
equation
1
2
p h(p) + h
(p) = 0, p ∈ R.
If h(p) > 0 for all p ∈ R, then
d
dp log 
h(p)
	
= h
(p)
h(p) = − p
2 =

−1
4
p2

.
This leads to log 
h(p)
	
= −1
4 p2 + c1 for some constant c1, and so h(p) = c2 e−p2/4
and
g(p) = c2
∫ p
0
e−r2/4dr + c3,
where c2, c3 are further constants. Thus
u(t, x) =
∫ x/
√
t
0
e−r2/4dr
and so also
ux(t, x) = 1
√
t
e−x2/4t
, t > 0, x ∈ R
are solutions, as can be checked easily by direct calculation. We have thus found a
special solution, to which we wish to give a name.
Definition 3.39 The function
g(t, x) = 1
√
4πt
e−x2/4t
, x ∈ R, t > 0, (3.64)
is called the Gaussian kernel or the fundamental solution of the heat equation.
84 3 Elementary methods
The Gaussian kernel has the following properties.
Theorem 3.40 For the function g defined by (3.64) we have g ∈ C∞((0, ∞) ×R) and
gt = gxx, t > 0, x ∈ R, (3.65a)
∫
R
g(t, x) dx = 1, t > 0. (3.65b)
Proof See Exercise 3.7. 
Not only is g itself a solution of the heat equation, but so too is the function
(t, x) → g(t, x − y) for any y ∈ R; the same is thus also true of linear combinations,
or more generally superpositions, of such functions. This leads us to the following
definition. Let u0 : R → R be continuous and exponentially bounded, that is, we
assume the existence of constants A, a ∈ R+ such that
|u0(x)| ≤ A ea|x |
, x ∈ R. (3.66)
We define
u(t, x) :=
∫
R
g(t, x − y)u0(y) dy = 1
√
4πt
∫
R
e−(x−y)
2/4t
u0(y) dy; (3.67)
then we have the following theorem.
Theorem 3.41 The function u defined by (3.67) has the following properties:
u ∈ C∞((0, ∞) × R), (3.68a)
ut = uxx, t > 0, x ∈ R, (3.68b)
lim
t↓0
u(t, x) = u0(x), (3.68c)
where (3.68c) holds uniformly on bounded sets. Moreover, there exist constants B,
ω ∈ R such that
|u(t, x)| ≤ Beω( |x |+t) (3.69)
for all x ∈ R and t > 0.
Thus the function u solves the initial value problem (3.68). We will see later
that u is the only solution which is exponentially bounded, that is, the only solution
for which an estimate of the form (3.69) holds. Before we come to the proof of
Theorem 3.41, we first transform the function u.
More precisely, by substitution we can also write u as follows:
u(t, x) =
∫ +∞
−∞
1
√
2π
e−z2/2
u0(x −
√
2tz) dz. (3.70)3.4 The heat equation 85
To see this, we make a first substitution w = x − y in (3.67) to obtain
u(t, x) =
∫ +∞
−∞
1
√
4πt
e−w2/4t
u0(x − w) dw.
If we now set z := w/
√
2t, then we obtain (3.70). It now follows directly from (3.70)
that
u ≡ 1 if u0 ≡ 1, (3.71)
u ≥ 0 if u0 ≥ 0, (3.72)
|u(t, x)| ≤ u0 ∞, (3.73)
where u0 ∞ = supx∈R |u0(x)|. In particular, u is bounded if u0 is.
Proof (of Theorem 3.41) The regularity statement (3.68a) and the equation (3.68b)
follow from the regularity of g, namely g ∈ C∞((0, ∞) × R), and gt = gxx, respec￾tively, after exchanging the order of integration and differentiation.
We will next prove (3.68c). So fix b > 0 arbitrary; then we wish to show that
limt↓0 u(t, x) = u0(x) uniformly in x ∈ [−b, b]. To this end, choose any sequence
tn ↓ 0; then, since u0 is uniformly continuous on compact intervals,
vn(z) := sup
|x | ≤b
|u0(x −

2tnz) − u0(x)| −→ 0 as n → ∞, for all z ∈ R.
Hence, by the Dominated Convergence Theorem (Theorem A.9)
sup
|x | ≤b
|u(tn, x) − u0(x)| ≤ ∫ +∞
−∞
1
√
2π
e−z2/2 vn(z) dz
converges to 0 as n → ∞. This theorem is applicable as the functions e−z2/2vn(z) are
easily seen to be dominated by the integrable function g(z) = 2Aea(b+
√
2t1z)
e−z2/2,
where a and A are the constants appearing in the exponential bound on u0 which we
assumed to hold.
It remains to prove (3.69). For this we will use Young’s inequality in the form
√
2t|z| ≤ εz2 + 2t
4ε for any ε > 0 (Lemma 5.22, (5.19) on page 173). It thus follows
from (3.70) and (3.66) that
|u(t, x)| ≤ 1
√
2π
∫+∞
−∞
e−z2/2Aea|x |
ea
√
2t |z |
dz
≤
1
√
2π
∫+∞
−∞
e−(1/2−εa)z2
dzAea|x |
e a
2ε t
for all x ∈ R and t > 0. 86 3 Elementary methods
We claimed above, and will see in the next section, that u is the only exponentially
bounded function which satisfies (3.68). First, however, we wish to obtain an anal￾ogous result in Rd. For this, we introduce radial functions in Rd, that is, functions
which only depend on the distance to the origin 0. For such functions, the Laplacian
reduces to an ordinary differential operator in the radial variable.
Lemma 3.42 (Radial Laplacian) Let Ω = {x ∈ Rd : ρ < |x| < R} be an annulus,
where 0 ≤ ρ < R, and let v ∈ C2(ρ, R) and u(x) := v(|x|). Then
(Δu)(x) =

vrr +
d − 1
r vr

(|x|). (3.74)
Proof This is an easy calculation; see Exercise 3.8. 
We now define the Gaussian kernel in Rd, analogously to Definition 3.39, by
g(t, x) := 1
(4πt)d/2 e−x2/4t
, t > 0, x ∈ Rd, (3.75)
where for x ∈ Rd we have written x2 := x2
1 + ··· + x2
d = |x|
2. This function satisfies
g ∈ C∞((0, ∞) × Rd), (3.76)
gt = Δg, t > 0, x ∈ Rd, (3.77)
∫
Rd
g(t, x) dx = 1, t > 0. (3.78)
Proof The function g(t, ·) is radial. Since g(t, x) = g1(t, x1)··· gd(t, xd), where
gj(t, xj) = 1
√
4πt
e
−x2
j /4t
,
(3.78) follows from (3.65b) by iterated integration. In order to prove (3.77) one can
set v(t,r) = t
−d/2e−r2/4t
, r > 0, t > 0, and show that vt = vrr + d−1
r vr . The claim
then follows from Lemma 3.42. 
Given Ω ⊂ Rd, a function v : Ω → R is said to be exponentially bounded if there
exists constants a, A ≥ 0 such that
|v(x)| ≤ A ea|x |
, x ∈ Ω.
For such functions we have the following result, analogous to Theorem 3.41.
Theorem 3.43 Let u0 : Rd → R be continuous and exponentially bounded, and
define
u(t, x) := 1
(4πt)d/2
∫
Rd
e−(x−y)
2/4t
u0(y) dy. (3.79)3.4 The heat equation 87
Then u ∈ C∞((0, ∞) × Rd) and
ut = Δu, t > 0, x ∈ Rd, (3.80a)
lim
t↓0
u(t, x) = u0(x) (3.80b)
uniformly in bounded sets in Rd. Moreover, u is exponentially bounded.
Proof The proof of Theorem 3.43 is entirely analogous to the proof of Theorem 3.41
and is left to the reader; see Exercise 3.9. 
The problem (3.80) is again an initial value problem. Given an initial value u0, we
obtain a corresponding solution via the convolution of u0 with the Gaussian kernel,
cf. (3.79). Since Rd does not have a boundary, there is no boundary condition; this
is replaced by the exponential boundedness. In particular, it is this condition which
guarantees uniqueness of the solution. We will now prove this with the help of the
following parabolic maximum principle in Rd. This should be compared with the
corresponding parabolic maximum principle for domains in Section 3.4.2. Here,
the “parabolic boundary” is simply the set {0} × Rd; however, we impose a growth
condition (3.81) on the function. We will also use the space C1,2 from Section 3.4.2.
Theorem 3.44 (Parabolic maximum principle) Given T > 0, let u : [0, T] ×Rd →
R be continuous with u ∈ C1,2((0, T) × Rd) and ut(t, x) = Δu(t, x), 0 < t < T,
x ∈ Rd. Suppose that there exist constants a ≥ 0 and A ≥ 0 such that
u(t, x) ≤ Aea|x |
2
, 0 ≤ t < T, x ∈ Rd. (3.81)
Then
sup
(t,x)∈[0,T]×Rd
u(t, x) = sup
z ∈Rd
u(0, z). (3.82)
Proof 1st case: Suppose that 4aT < 1. Choose ε > 0 small enough that 4a(T +ε) <
1; then γ := 1
4(T+ε) − a > 0. Now fix y ∈ Rd arbitrary; we need to show that
u(t, y) ≤ sup
z ∈Rd
u(0, z) =: c (3.83)
for all t ∈ [0, T). Let μ > 0 be given. Since for A
, B > 0 we have
lim
r→∞{A
ea( |y |+r)
2
− B
e(a+γ)r2
} = lim
r→∞ ea( |y |+r)
2
{A − B
e−a|y |
2
eγr2−2a|y |r }
= −∞,
we may choose r > 0 so large that
Aea( |y |+r)
2
− μ(4(a + γ))d/2e(a+γ)r2
≤ c.88 3 Elementary methods
Now set v(t, x) := u(t, x) − μ
(T+ε−t)d/2 e(x−y)
2/4(T+ε−t)
; then vt − Δv = 0 (use
Lemma 3.42). Set Ω = B(y,r) and ΩT = (0, T) × Ω. The parabolic maximum
principle of Theorem 3.31 applied to v yields
sup
(t,x)∈ΩT
v(t, x) ≤ sup
(t,x)∈∂∗ΩT
v(t, x).
Now the parabolic boundary ∂∗ΩT consists of two parts:
(a) the part where x ∈ Ω and t = 0. Here, v(0, x) ≤ u(0, x) ≤ c;
(b) the part where t ∈ [0, T] and x ∈ ∂Ω. Here, |x − y| = r, and by (3.81),
v(t, x) = u(t, x) − μ
(T + ε − t)d/2 er2/4(T+ε−t)
≤ Aea( |y |+r)
2
− μ
(T + ε − t)d/2 er2/4(T+ε−t)
≤ Aea( |y |+r)
2
− μ
(T + ε)d/2 er2/4(T+ε)
= Aea( |y |+r)
2
− μ(4(a + γ))d/2er2(a+γ) ≤ c.
Hence sup(t,x)∈∂∗ΩT v(t, x) ≤ c and so by the maximum principle
u(t, y) − μ
(T + ε − t)d/2 = v(t, y) ≤ c
for all 0 ≤ t ≤ T. We now obtain (3.83) by letting μ converge to 0.
2nd case: If 4aT ≥ 1, then we apply the 1st case successively to the time intervals
[0, T1], [T1, 2T1], ..., where T1 = 1
8a . 
The parabolic maximum principle gives us uniqueness of solutions on every time
interval. We may thus summarize our results in the form of the following statement
on well-posedness of a suitable initial value problem.
Theorem 3.45 (Heat equation in Rd) Let u0 : Rd → R be continuous and expo￾nentially bounded. Then the initial value problem
ut = Δu, 0 < t, x ∈ Rd, (3.84a)
u(0, x) = u0(x), x ∈ Rd. (3.84b)
admits a unique exponentially bounded solution u ∈ C1,2((0, ∞) × Rd) ∩ C([0, ∞) ×
Rd). This solution u is given by (3.79), and in particular u ∈ C∞((0, ∞) × Rd) and
uC([0,∞)×Rd) ≤ u0 C(Rd).
Proof We only have to prove uniqueness. If u1, u2 are two solutions, then u = u1−u2
satisfies (3.84) with u0 ≡ 0. It follows from (3.82) that u(t, x) ≤ 0, t ∈ [0, T] for all
x ∈ Rd, for any given T > 0. Exchanging u1 and u2 shows that u ≡ 0. 3.4 The heat equation 89
The condition that u ∈ C1,2((0, ∞) × Rd) is the minimal possible regularity
assumption we can make on the solution u in order that the equation (3.84a) should
make sense. In particular, we have the result that the solution is automatically
infinitely differentiable even if the initial value u0 is nowhere differentiable. Here
this is a consequence of the representation (3.79) of the solution in terms of the
Gaussian kernel. However, we will see later that solutions of the heat equation in a
domain (rather than Rd) are also automatically C∞ (see Chapter 8).
As noted above, since Rd does not have a boundary, there is no boundary condi￾tion; this is replaced by the assumption that u is exponentially bounded. There are
however other solutions which grow extremely quickly as |x|→∞ (see [28, § 2.3]).
Physically, we interpret the solutions of the heat equation as follows: if u0 is a
given distribution of heat in Rd, then the solution u(t, x) of (3.84) describes the
temperature at the point x ∈ Rd at time t. Another model described by the same
equation is the diffusion of, say, ink in water. Then u0 is the initial concentration,
that is, for any measurable set B ⊂ Rd,
∫
B u0(x) dx gives the quantity of ink in B.
The solution u(t, x) gives the concentration at time t. We refer to Chapter 1 for the
derivation of the heat equation.
The representation (3.79) of the solution in terms of the Gaussian kernel also
allows us to see that
∫
Rd
u(t, x) dx =
∫
Rd
u0(x) dx (3.85)
for all t > 0, that is, the total mass is conserved, as is to be expected. Nevertheless, we
are dealing here with an idealized model. Suppose that u0 is an initial concentration
such that ∫
Rd u0(x) dx = 1 and u0 ≥ 0 but u0(x) = 0 for all |x| ≥ ε, where ε > 0 is
given; that is, u0 is concentrated in a small ball. Let x be a point a long way from the
origin. Then no matter how small we take t > 0, we still have u(t, x) > 0, as can be
seen from (3.79). This is a contradiction to the theory of relativity; in fact, including
relativistic effects in the model leads to a nonlinear equation.
Finally, we wish to say something about the normalization of the Gaussian kernel.
The function g(t, ·) given in (3.75) is the density function of a normal distribution
with variance σ = 2t and expected value μ = 0. For this reason, the renormalized
function g(t/2, x) is generally preferred by probability theorists; this is the kernel of
the solutions of the equation
ut = 1
2
Δu.
That is, in place of the Laplacian Δ one works with 1
2Δ.90 3 Elementary methods
3.5 The Black–Scholes equation
In the previous section we determined all exponentially bounded solutions of the
heat equation
ut = uxx, t > 0, x ∈ R (3.86)
explicitly. Here, taking these solutions as our point of departure, via a suitable change
of variables and scaling we will obtain the polynomially bounded solutions of the
Black–Scholes equation, and finish by deriving a formula for option pricing.
We start by considering the general parabolic equation
wt = αwxx + βwx + γw, t > 0, x ∈ R. (3.87)
Here α, β, γ ∈ R are constants, with α > 0. Now let T > 0 and suppose that
u ∈ C1,2((0, T) ×R) is a solution of ut = uxx. We will modify u successively in order
to obtain a solution of the more general equation (3.87). To this end, let a, λ ∈ R be
two constants, yet to be determined.
1st Step: Define v(t, x) := eaxu(t, x), then
vt = eax ut,
vx = eax(au + ux),
vxx = eax(uxx + 2aux + a2u) = eaxut + 2aeax(au + ux) − a2ueax
= vt + 2avx − a2v
since ut = uxx. Thus the function v solves the equation vt = vxx − 2avx + a2v.
2nd Step: We now adjust the velocity by setting w(t, x) := eaxu(αt, x) = v(αt, x).
Then wt = α wxx − 2aα wx + αa2 w.
3rd Step: We next insert the factor e−λt
; more precisely, we set
w(t, x) := e−λt
eaxu(αt, x); (3.88)
then we have
wt(t, x) = αwxx − 2aαwx + (αa2 − λ)w. (3.89)
Hence w solves the equation (3.87) if
a = − β
2α
, γ = αa2 − λ. (3.90)
Conversely, if w is a solution of (3.89), then u(t, x) := eλt/αe−axw( t
α, x) solves the
equation ut = uxx, as is easy to calculate just as above. Finally, w ∈ C([0, T] × R)
if and only if u ∈ C([0, T] × R). Combining this with Theorem 3.45, we obtain the
following result.3.5 The Black–Scholes equation 91
Theorem 3.46 Let α, β, γ ∈ R, α > 0 and let 0 < T < ∞. Suppose that w0 : R → R
is continuous and exponentially bounded. Then there exists a unique exponentially
bounded function w ∈ C1,2((0, T) × R) ∩ C([0, T] × R) such that
wt = αwxx + βwx + γw, t ∈ (0, T], x ∈ R, (3.91a)
w(0, x) = w0(x), x ∈ R. (3.91b)
This function w is given by
w(t, x) = e−λt 1
√
2π
∫
R
e−z2/2ea
√
2αtzw0(x −
√
2αtz) dz, (3.92)
where a and λ are defined by (3.90). In particular, we have w ∈ C∞((0, T] × R) and
wC([0,T]×R) ≤ w0 C(R).
Proof We only have to prove (3.92). Since w(t, x) = e−λt
eaxu(αt, x), in particular
w(0, x) = eaxu(0, x) = eaxu0(x). Hence, by Theorem 3.45,
w(t, x) = e−λt
eax 1
√
2π
∫
R
e−z2/2e−a(x−
√
2αtz)
w0(x −
√
2αtz) dz,
= e−λt 1
√
2π
∫
R
e−z2/2ea
√
2αtzw0(x −
√
2αtz) dz,
as claimed. 
We now turn to the Black–Scholes equation. We first recall the model for a European
call option described in Section 1.5 (see also Appendix A.4). We take the strike K,
the timepoint of maturity T > 0, the volatility σ > 0 and the interest rate r > 0 as
given. As in (1.21) we will use the letter y ≥ 0 for the variable giving the current
asset price, not S as in the financial mathematics literature. We again consider the
case of a call option (that is, an option to buy), so that the payoff function in (1.20)
in the new variable y reads
y → (y − K)
+.
We wish to determine the fair price V(t, y) of this option at time t ∈ [0, T]. The
function V satisfies the following final value problem for all y > 0:
Vt(t, y) + σ2
2 y2
Vyy(t, y) + r yVy(t, y) − rV(t, y) = 0, 0 < t < T, (3.93a)
V(T, y) = (y − K)
+. (3.93b)
This problem is well-posed, and its solutions can be given explicitly. In order to
guarantee uniqueness of solutions, however, we need to assume that they grow at
most polynomially. We say that a function f : (0, ∞) → R is polynomially bounded
if there exist constants m ∈ N and c ≥ 0 such that92 3 Elementary methods
| f (y)| ≤ c

ym if y ≥ 1,
y−m if 0 < y < 1. (3.94)
Thus f is polynomially bounded if and only if the function g : R → R given by
g(x) := f (ex) is exponentially bounded. We next extend these terms to apply to
functions of time and space: we say that a function u : [0, T]×(0, ∞) → R is
polynomially bounded if there exist constants m ∈ N and c ≥ 0 such that (3.94)
holds for f = u(t, ·), for all t ∈ [0, T], where m and c are independent of t. Finally,
we will use the notation
N (x) := N0,1(x) = 1
√
2π
∫ x
−∞
e−z2/2 dz, x ∈ R,
that is, N is the (cumulative) distribution function of the standard normal distribution.
Theorem 3.47 There exists a unique polynomially bounded function
V ∈ C([0, T]×(0, ∞)) ∩ C1,2((0, T)×(0, ∞))
satisfying (3.93). Moreover, V ∈ C∞([0, T)×(0, ∞)), and V is given by the explicit
formula
V(t, y) = yN (d1) − Ke−r(T−t)
N (d2), (3.95)
where
d1 :=
log  y
K
	
+

r + σ2
2

(T − t)
σ
√
T − t , d2 := d1 − σ
√
T − t, 0 ≤ t < T. (3.96)
Proof We first consider the transformation t → T − t of the time variable, which
will convert the equation into one which runs “forward in time”. Setting W(t, y) :=
V(T − t, y) we obtain a function W ∈ C([0, T]×(0, ∞)) ∩ C1,2((0, T)×(0, ∞)) which
solves the initial value problem
Wt = σ2
2 y2
Wyy + r yWy − rW, y > 0, 0 < t < T, (3.97a)
W(0, y) = (y − K)
+ (3.97b)
if and only if V is a solution of (3.93). We next transform the problem (3.97) from
one on (0, ∞) to one on R. For any continuous function w : [0, T] ×R → R we define
a new function by
W(t, y) := w(t, log y), y > 0, 0 < t < T.
Let us suppose that w ∈ C1,2((0, T) × R), which is equivalent to W ∈ C1,2((0, T) ×
(0, ∞)). Then we have3.5 The Black–Scholes equation 93
Wy(t, y) = 1
y
wx(t, log y), that is, yWy(t, y) = wx(t, log y), and
Wyy(t, y) = − 1
y2 wx(t, log y) +
1
y2 wxx(t, log y)
= −1
y
Wy(t, y) +
1
y2 wxx(t, log y).
This implies that y2Wyy(t, y)+yWy(t, y) = wxx(t, log y), and soW solves the equation
(3.97a) if and only if
σ2
2 wxx(t, x) +

r − σ2
2

wx(t, x) − r w(t, x) = wt(t, x) (3.98)
for all 0 < t < T and x ∈ R. Moreover, W(0, y) = (y − K)
+ if and only if
w(0, x) = (ex − K)
+ =: w0(x), x ∈ R. (3.99)
Finally, W is polynomially bounded if and only if w is exponentially bounded. We
may thus apply Theorem 3.46 to infer that (3.97) has exactly one such solution W.
This function is in C∞((0, T]×(0, ∞)), and is given explicitly by
W(t, y) = w(t, log y) = e−λt 1
√
2π
∫
R
e−z2/2eaσ√
tzw0(log y − σ
√
tz) dz
= e−λt 1
√
2π
∫ +∞
−∞
e−z2/2eaσ√
tz (ye−σ√
tz − K)
+ dz =: I1 − I2,
where a := 1
2 − r
σ2 , λ := σ2
2 a2 + r and
I1 := ye−λt 1
√
2π
∫ 1
σ√
t log(y/K)
−∞
e−z2/2e(a−1)σ√
tz dz,
I2 := Ke−λt 1
√
2π
∫ 1
σ√
t log(y/K)
−∞
e−z2/2eaσ√
tz dz.
In order to calculate I1 we complete the square in the exponent of the integrand:
−z2/2 + (a − 1)σ
√
tz = −1
2
{z2 − 2(a − 1)σ
√
tz}
= −1
2
{(z − (a − 1)σ
√
t)
2 − (a − 1)
2σ2
t}.
Making the substitution z := z − (a − 1)σ
√
t leads to
I1 = ye−λt
N
 1
σ
√
t
log y
K − (a − 1)σ
√
t

e
1
2 (a−1)
2σ2t = y N (d
1),94 3 Elementary methods
where
d
1 = log y
K − (a − 1)σ2t
σ
√
t =
log y
K +

σ2
2 + r

t
σ
√
t
since
−λ +
1
2
(a − 1)
2σ2 = −σ2
2 a2 − r +
1
2
(a2 − 2a + 1)σ2 = −r − aσ2 + σ2
2 = 0.
In order to calculate I2 we again consider the exponent of the integrand:
−1
2
z2 + aσ
√
tz = −1
2
{z2 − 2aσ
√
tz} = −1
2
{(z − aσ
√
t)
2 − a2σ2
t}.
Making the substitution z = z − aσ
√
t, we obtain
I2 = Ke−λt
N
 1
σ
√
t
log y
K − aσ
√
t

e
1
2 a2σ2t = KN (d
2)e−r t,
where
d
2 = log y
K − aσ2t
σ
√
t = d
1 − σ
√
t,
where we have used that −λ + 1
2 a2σ2 = −r. Since V(t, y) = W(T − t, y), this gives
us the formula (3.95). 
Of particular interest is the solution V(0, y) at time t = 0. This is the price of the
option: if y is the share price at time t = 0, then the price for the option to buy the
share at time T for the price K is given by
V(0, y) = yN(d1) − Ke−rT N(d2), (3.100)
where
d1 :=
log  y
K
	
+

r + σ2
2

T
σ
√
T , d2 := d1 − σ
√
T.
This is the famous Black–Scholes formula, which to this day is in widespread use
in the finance industry for calculating prices. It can be evaluated for concrete data
using, for example, Maple® (cf. Section 10.1.5). Our derivation here was based on
explicitly solving the partial differential equation of Black and Scholes (3.93), which
we had in turn derived from the no arbitrage principle using stochastic techniques in
Appendix A.4. This was the approach taken in the groundbreaking work of Black,
Scholes and Merton in 1973. There is, however, another way to obtain the Black–3.5 The Black–Scholes equation 95
Scholes formula, which was developed by Cox, Ross and Rubinstein in 1979 and
which is presented in many textbooks (see for example [44] or [15]).
We shall finish this section by considering a certain question of consistency. The
solution V given by (3.95) depends, of course, on σ > 0. It will be interesting to
look at the limit of V(t, y) as σ → 0. Here we distinguish between two cases.
1st Case: log  y
k
	
+ rT > 0. In this case, limσ→0 d1 = limσ→0 d2 = ∞, whence
limσ→0 N (d1) = limσ→0 N (d2) = 1. We thus have
u(t, y) := lim
σ→0
V(t, y) = y − Ke−r(T−t)
.
In particular, the price of the option in the limit σ = 0 is given by u(0, y) = y−Ke−rT .
This is consistent with our model: if one invests the amount y − Ke−rT at the fixed
interest rate r, then after time T its value is
erT (y − Ke−rT ) = erT y − K.
If the volatility is σ = 0, that is, if the increase in value isr% with complete certainty,
then the share is worth erT y at time T (cf. the following Remark 3.48). If the option
to buy the share at time T for the price K is used, then the corresponding profit is
etT y − K, exactly equal to the profit from depositing the amount y − Ke−rT at the
fixed interest rate r. Hence this is exactly the price which should logically be paid
for this risk-free option.
2nd Case: If log  y
K
	
+ rT ≤ 0, then u(0, y) := limσ→0 V(0, y) = 0, that is, the
purchase price of the option is 0. Indeed, in this case, the deterministic evolution of
the share price leads to a value erT y at time T which is under the strike price K. In
this case, the option to buy the share for the price K is worthless.
* Remark 3.48 (Continuously compounded interest) Suppose that the fixed amount z0 is deposited
in a bank for a fixed interest rate r%, which refers to the return on the investment over the course
of a year. The bank pays interest after a certain period of time; suppose that this amount is added
to the deposit and hence itself attracts interest. Let us assume that the deposit earns interest for a
period of t years. If the bank pays interest only at the end of this period, then the total capital value
becomes z(t) = z0 + trz0 = (1 + tr)z0. If the bank pays out interest at the halfway point t
2 , then
the value of the capital at this time is
z
 t
2

= z0 + t
2
rz0 = 
1 + t
2
r
	
z0.
This larger value z
 t
2
	
then earns interest for the remaining t
2 years, and so has the value
z(t) = z
 t
2

+ z
 t
2
 t
2
r =

1 + t
2
r
2
z0
at time t. If the bank pays interest at the end of each time interval of length t
n , n ∈ N then the total
value at time t
n is now z( t
n ) = z0(1 + t
n r). This new amount earns interest in the period [ t
n, 2t
n ]
and results in the value
z

2t
n

= z
 t
n

+ z
 t
n
 t
nr = z0

1 + t
n r
296 3 Elementary methods
at time 2t
n . At time 3t
n the deposit then has the value
z

3t
n

= z

2t
n

+ z

2t
n
 t
nr = z0

1 + t
n r
3
.
Proceeding in this fashion, we end up with
z(t) = z0

1 + t
nr
n
(3.101)
at time t. This is the value of the capital after t years if interest is payed out every t
n years. But
this does not reflect the investment of the extra interest in time intervals smaller than t
n . To be fair
the bank should pay out interest continuously. In this case, the value z(t) of the capital after t years
given an initial deposit of z0, a yearly rate of interest of r% and continuous interest payments is
z(t) = lim
n→∞ z0

1 + t
n r
n
= z0er t .
The limit of 
1+ t
n r
	n as a formula for interest payments is due to Jakob Bernoulli (Acta Eruditorum,
1690), long before Euler introduced the letter e for the limit limn→∞ 
1+ 1
n
	n in a letter to Goldbach.
This is an example of how natural constants can not only play a role in the laws of physics but
also in the laws of finance. If money is deposited with compound interest, then regardless of the
planet on which it is deposited, Euler’s number e ≈ 2.71828 is the value of the capital which has
accumulated after one year (measured on that planet) if one unit of money was deposited at the rate
of 100% with continuous interest payments. 
3.6 Integral transforms
It is very common, especially when dealing with partial differential equations arising
from problems in engineering, to transform the partial differential equation using an
integral transform. This often permits the reduction of the equation to an ordinary
differential equation, which in turn can often be reduced to an algebraic equation,
and thus solved, via the use of such integral transforms. These transforms often
have a physical interpretation, for example the transformation of a wave in space￾time variables into phase space (the frequency spectrum). In phase space the partial
differential equation often becomes an ordinary differential equation. If one can solve
the resulting equation in phase space, then the solution of the original differential
equation can be obtained by applying the inverse transform.
3.6.1 The Fourier transform
We start with the Fourier transform, which was developed by Jean Baptiste Joseph
Fourier in 1822 in his work Théorie analytique de la chaleur, already mentioned
earlier. Here we will take a practical approach to Fourier transforms: we will show
how they can be used to solve equations explicitly. Later, in Chapter 6, they will be
used for a more systematic investigation in the context of Sobolev spaces.3.6 Integral transforms 97
Unlike in the previous sections, here we consider complex-valued functions. For
1 ≤ p < ∞ we define
Lp(R, C) :=

f : R → C measurable :
∫
R
| f (t)|p dt < ∞
'
.
If we identify functions which coincide almost everywhere, then Lp(R, C) becomes
a Banach space when equipped with the norm
 f p :=
 ∫
R
| f (t)|p dt1/p
.
The space L2(R, C) is a Hilbert space.
Remark 3.49 A remark about our notation is in order: the above spaces are often
denoted by Lp(Ω, C) in the literature. We will write p (the power appearing in the
integrand) as a subscript to distinguish it from the order of differentiation as appears
in spaces like Ck and Hk . In dimension one we also avoid double brackets, that is,
we write L2(0, 1) instead of L2((0, 1)) (for example), even though the latter would be
more consistent. 
Definition 3.50 For f ∈ L1(R, C) the function defined by
F f (ω) := ˆf (ω) := 1
√
2π
∫ ∞
−∞
f (t)e−iωt dt, ω ∈ R,
is called the Fourier transform (also the spectral function) of f . 
In the context of physics, the Fourier transform can be interpreted as the trans￾formation of a time-amplitude signal (t, f (t)) into its frequency spectrum (ω, ˆf (ω)),
in other words, ˆf (ω) gives the proportion of the waves in the signal which have
frequency ω.
Examples
Before we turn to the properties of F, we wish to start with a few simple examples.
Example 3.51 The rectangular pulse is defined by
f (t) =

1 , if |t| ≤ 1,
0 , otherwise,
cf. Figure 3.3 (left). Clearly f ∈ L1(R, C), and so the Fourier transformation of f
when ω  0 is given by
ˆf (ω) = 1
√
2π
∫ 1
−1
e−iωt dt = 1
√
2π
−1
iω e−iωt




t=1
t=−1
=
( 2
π
sinω
ω ,98 3 Elementary methods
while ˆf (0) = 
2/π. Thus ˆf is the sinc function (from the Latin sinus cardinalis), cf.
Figure 3.3 (right). 
f (t)
−1 1

ˆf ()
−3p −2p −p p 2p 3p
2/ p
Fig. 3.3 Rectangular pulse (left) and its Fourier transform (right).
Example 3.52 The exponentially decaying pulse has the form f (t) = e−α|t | for some
α > 0. Since for any R > 0
∫ R
−R
| f (t)| dt =
∫ R
−R
e−α|t | dt = 2
∫ R
0
e−αt dt = 2

− 1
α

e−αt




t=R
t=0
=

− 2
α

(e−αR − 1) R→∞
−→ 2
α
,
we see that f ∈ L1(R). The Fourier transform is easy to calculate:
ˆf (ω) = 1
√
2π
 ∫ ∞
0
e−αt
e−iωt dt +
∫ ∞
0
e−αt
eiωt dt'
= 1
√
2π
 (−1)
α + iωe−(α+iω)t




∞
t=0
+
1
−α + iωe(−α+iω)t




∞
t=0
'
= 1
√
2π
 1
α + iω +
1
α − iω
'
= 1
√
2π
α − iω + α + iω
α2 + ω2 =
( 2
π
α
α2 + ω2 .
In particular, ˆf is a rational function; see Figure 3.4. 3.6 Integral transforms 99
t
f (t)
1
1 2 3 
ˆf () 2
3
−3 −2 −1 0 1 2 3
Fig. 3.4 Exponentially decaying pulse (α = 1.5, left) and its Fourier transform (right).
Properties of the Fourier transform
We now wish to collect a few essential properties of Fourier transforms; we will
focus on those we will need when solving partial differential equations.
Theorem 3.53 (Properties and rules of calculation) Let f ∈ L1(R, C). Then:
(i) The Fourier transform ˆf : R → C is continuous and lim|ω|→∞ ˆf (ω) = 0.
(ii) Linearity: if f1,..., fn ∈ L1(R, C) and c1,..., cn ∈ C, then
F

n
k=1
ck fk

=
n
k=1
ck F fk .
(iii) If f is continuously differentiable with f  ∈ L1(R, C) and lim|t |→∞ f (t) = 0,
then
F ( f 
)(ω) = iω F f (ω). (3.102)
(iv) If ∫ ∞
−∞ |t f (t)| dt < ∞, then
d
dωF f (ω) = (−i)F (· f (·))(ω). (3.103)
(v) For any α ∈ R we have F ( f (· − α))(ω) = e−iαωF f (ω).
(vi) For any α ∈ R \ {0} we have F ( f (α·))(ω) = 1
|α|
F f
 ω
α
	
.
Proof Here we will only prove (iii) and (iv), as these two properties will play a
central role in what follows. The other statements are left to the reader as an exercise
(see Exercise 3.10).
(iii) For any R ≥ 0, if we integrate by parts and use the assumptions of the theorem,
then we have
1
√
2π
∫ R
−R
f 
(t)e−iωt dt = 1
√
2π
f (t)e−iωt




R
t=−R
+
iω
√
2π
∫ R
−R
f (t)e−iωt dt.
By assumption, the first term converges to 0 as R → ∞, while the second
converges to iω ˆf (ω).100 3 Elementary methods
(iv) Since we may exchange the order of differentiation and integration, the deriva￾tive is given by
d
dω
) 1
√
2π
∫∞
−∞
f (t)e−iωt dt*
= (−i) 1
√
2π
∫∞
−∞
t f (t)e−iωt dt = (−i) F (· f (·))(ω).
This proves the claim. 
One of the fundamental reasons for the importance of the Fourier transform has
to do with the following theorems. Given f, g ∈ L1(R, C), by Fubini’s theorem
f (·)g(t − ·) is in L1(R, C) for almost every t ∈ R and the convolution f ∗ g defined
by
( f ∗ g)(t) :=
∫ ∞
−∞
f (τ) g(t − τ) dτ, t ∈ R, (3.104)
is likewise in L1(R, C). But we can say more.
A function g : R → C is said to have compact support if it vanishes identically
outside a compact set (cf. the notation introduced at the beginning of Chapter 2 and
in particular equation (2.1)). We set
Cc(R, C) := {g ∈ C(R, C) : g has compact support} (3.105)
and
Cm
c (R, C) := Cc(R, C) ∩ Cm(R, C) for all m ∈ N.
Theorem 3.54 Let g ∈ Cm
c (R, C) for some m ∈ N and let f ∈ L1(R, C). Then the
convolution f ∗ g is m times continuously differentiable and
dm
dtm ( f ∗ g) = f ∗
 dm
dtm g

.
Proof Since g has compact support, supτ ∈R |g(τ)| < ∞, and so for the function
h(τ, t) := f (τ) g(t − τ) we have the bound
|h(τ, t)| ≤ | f (τ)| sup
σ∈R
|g(σ)| < ∞;
in particular, the convolution is well defined and continuous. This proves the claim
when m = 0. If m ≥ 1, then we have d
dt h(τ, t) = f (τ) d
dt g(t − τ), and so



d
dt h(τ, t)


 ≤ | f (τ)| sup
σ∈R



d
dt g(σ)


.3.6 Integral transforms 101
Hence f ∗ g ∈ C1(R) and
d
dt ( f ∗ g)(t) = d
dt ∫ ∞
−∞
h(τ, t) dτ =
∫ ∞
−∞
f (τ) d
dt g(t − τ) dτ =

f ∗ d
dt g

(t).
The rest follows inductively. 
We can now formulate and prove the above-mentioned essential property of
Fourier transforms.
Theorem 3.55 (Convolution theorem) If f, g ∈ L1(R, C), then f ∗g ∈ L1(R, C) and +f ∗ g = √
2π ˆf gˆ.
Proof It follows from Fubini’s theorem that f ∗ g ∈ L1(R, C). By definition
F ( f ∗ g)(ω) = 1
√
2π
∫ ∞
−∞
( f ∗ g)(t) e−iωt dt
= 1
√
2π
∫ ∞
−∞
∫ ∞
−∞
f (τ) g(t − τ) dτe−iωt dt.
We now introduce the change of variables t → s := t − τ, which leads to
F ( f ∗ g)(ω) = 1
√
2π
∫ ∞
−∞
∫ ∞
−∞
f (τ) g(s) dτ e−iω(s+τ) ds,
which proves the claim. 
We next have two theorems which state that the energy (here the norm in L2) is
preserved by the Fourier transform. We will not give the proof here.
Theorem 3.56 (Plancherel’s theorem) Let f, g ∈ L1(R, C) ∩ L2(R, C). Then ˆf, gˆ ∈
L2(R, C) and we have
∫ ∞
−∞
ˆf (ω)gˆ(ω) dω =
∫ ∞
−∞
f (t)g(t) dt .
Proof The proof can be found, for example, in [54, Thm. 9.13]. 
An immediate consequence of Plancherel’s theorem 3.56 is an important rela￾tion known as Parseval’s identity, which, as mentioned above, states that energy is
preserved under the Fourier transform.
Theorem 3.57 (Parseval’s identity) If f ∈ L1(R, C) ∩ L2(R, C), then ˆf ∈ L2(R, C)
and
∫ ∞
−∞
| ˆf (ω)|2 dω =
∫ ∞
−∞
| f (t)|2 dt,
that is,  ˆf L2(R) =  f L2(R).
Proof This follows from applying Plancherel’s theorem to f = g. 102 3 Elementary methods
Inverse Fourier transforms
As mentioned in the introduction to this section, we will use Fourier transforms
to convert certain partial differential equations into ordinary differential equations.
This is done principally using the formulae (3.102) and (3.103). Once the result￾ing ordinary differential equation has been solved, we need to invert the Fourier
transformation in order to obtain a concrete solution of the original equation.
Theorem 3.58 (Fourier inversion formula) If f ∈ L1(R, C) and ˆf ∈ L1(R, C), then
f (x) = 1
√
2π
∫ ∞
−∞
ˆf (ω)eiωx dω =: F−1( ˆf )(x)
for almost every x ∈ R.
Proof Compare for example [54, Thm. 9.11]. 
Solving differential equations using Fourier transforms
As announced, the properties of the Fourier transform established in Theorem 3.53
can be used to solve certain partial differential equations. We will illustrate this here
using a selection of examples. It is to be emphasized that this method serves to
determine a formula for a solution of a concrete differential equation. This does not
in general imply any statements about uniqueness or continuous dependence on the
data.
In each case the strategy consists of four steps:
1. Transform the original partial differential equation into an ordinary differential
equation.
2. Transform the initial conditions (if applicable).
3. Solve the initial value problem for the ordinary differential equation.
4. Applying the inverse transformation yields the desired formula for the solution.
In what follows, we will assume that all the functions we are dealing with satisfy
the conditions of the relevant theorems from the preceding sections, so that the
rules of calculation we stated there can be applied. Our goal is to show how explicit
solutions can be obtained.
Example 3.59 (IVP for the wave equation on R) Consider the initial value problem
(IVP) for the wave equation,
utt = uxx, t ≥ 0, x ∈ R, (3.106a)
u(0, x) = f (x), x ∈ R, (3.106b)
ut(0, x) = g(x), x ∈ R, (3.106c)
u(t, x) → 0, x → ±∞, t ≥ 0, (3.106d)3.6 Integral transforms 103
where f, g : R → R are given functions assumed to be sufficiently smooth, and
consistent with the asymptotic boundary conditions (3.106d). In this case we already
know the solution via d’Alembert’s formula (3.7). Here we wish to show using this
example how Fourier transforms can be used to obtain such formulae for the solution.
To this end we will perform the four steps sketched above.
1. Transformation of the differential equation
We define the Fourier transform of the solution to be determined with respect to
the space variable x,
U(t, ω) := F [u(t, ·)](ω) = 1
√
2π
∫ ∞
−∞
u(t, x) e−ixωdx.
Now we transform the equation (3.106a), which is possible due to the asymptotic
boundary conditions (3.106d). By the rules of calculation for Fourier transforms and
(3.106a) we have
F [uxx(t, ·)](ω) = F [utt(t, ·)](ω) = −ω2 U(t, ω).
2. Transformation of the initial conditions
Since we are considering the Fourier transform with respect to the space variable
x, the initial conditions are easy to transform.
Utt(t, ω) + ω2 U(t, ω) = 0, t ≥ 0, (3.107a)
U(0, ω) = F(ω), (3.107b)
Ut(0, ω) = G(ω). (3.107c)
This is clearly a (linear) ordinary differential equation in the time variable t, for each
fixed ω ∈ R: more precisely, it is an initial value problem for a second-order ordinary
differential equation.
3. Solution of the initial value problem
Using known methods from the solution theory for ordinary differential equations
we can solve (3.107). For any ω  0 the solution is
U(t, ω) = F(ω) cos(ωt) + G(ω)
sin(ωt)
ω .
If ω = 0, then we obtain U(t, 0) = F(0) + t G(0), that is, the continuous extension.
4. Inverse transformation
Finally, in order to obtain an expression for the solution u(t, x) of (3.106), we
apply the inversion formula of Theorem 3.58 to U:
u(t, x) = F−1[U(t, ·)](x)
= 1
√
2π
∫ ∞
−∞
U(t, ω) eiωx dω104 3 Elementary methods
= 1
√
2π
∫ ∞
−∞
F(ω) cos(ωt) eiωx dω +
∫ ∞
−∞
G(ω)
sin(ωt)
ω eiωx dω
'
=: u1(t, x) + u2(t, x).
For the first term, we apply the identities cos α = 1
2 (eiα + e−iα) and sin α = 1
2i(eiα −
e−iα) to obtain
u1(t, x) = 1
√
2π
∫ ∞
−∞
F(ω)
1
2
(eiωt + e−iωt
)eiωx dω
= 1
2
1
√
2π
∫ ∞
−∞
F(ω)eiω(t+x) dω +
1
2
1
√
2π
∫ ∞
−∞
F(ω)e−iω(t−x) dω
= 1
2
(F−1F)(x + t) +
1
2
(F−1F)(x − t)
= 1
2

f (x + t) + f (x − t)
	
.
For the second term we have
u2(t, x) = 1
√
2π
∫ ∞
−∞
G(ω)
1
ω
1
2i
(eiωt − e−iωt
)eiωx dω
= 1
2
1
√
2π
∫ ∞
−∞
1
iωG(ω)(eiω(x+t) − e−iω(x−t)
) dω.
Now we apply (3.102) in the form
G(ω) = gˆ(ω) = (iω)F )∫ •
−∞
g(ξ) dξ
*
and obtain
u2(t, x) = 1
2
1
√
2π
∫ ∞
−∞
F
)∫ •
−∞
g(ξ) dξ
*
(eiω(x+t) − e−iω(x−t)
) dω
= 1
2
∫ x+t
−∞
g(ξ) dξ −
∫ x−t
−∞
g(ξ) dξ
'
= 1
2
∫ x+t
x−t
g(ξ) dξ.
Thus we have derived d’Alembert’s formula (3.7)
u(t, x) = 1
2

f (x + t) + f (x − t)
	
+
1
2
∫ x+t
x−t
g(ξ) dξ.
We already obtained this formula in Theorem 3.2 via other methods; there we also
proved uniqueness of solutions. 
For our next example we will consider the heat equation on the real line. In
Section 3.4.4 we found a solution via an invariance argument; here we will show3.6 Integral transforms 105
that it can also be obtained using Fourier transforms. For this we assume that the
functions appearing in the arguments are such that the rules of calculation can be
applied.
Example 3.60 (IVP for the heat equation on R) We consider the initial value
problem for the heat equation on R; here we will again consider asymptotic boundary
conditions. The corresponding system reads
ut = uxx, x ∈ R , t ≥ 0 , (3.108a)
u(0, x) = f (x), x ∈ R, (3.108b)
u(t, x) → 0, x → ±∞. (3.108c)
As in Example 3.59 we will consider the Fourier transform of the desired solution
with respect to the space variable x ∈ R, that is,
U(t, ω) := F [u(t, ·)](ω).
1. Transformation of the differential equation
The equation (3.108a) becomes Ut(t, ω) = −ω2 U(t, ω) for all t ≥ 0, ω ∈ R.
2. Transformation of the initial conditions
The initial conditions take the form U(0, ω) = ˆf (ω), ω ∈ R, in phase space; hence
we obtain an initial value problem for a homogeneous linear ordinary differential
equation, which can be solved by standard means.
3. Solution of the initial value problem
The Fourier transform of the solution is thus U(t, ω) = ˆf (ω)e−ω2t
.
4. Inverse transformation
Applying the inversion formula we are led to the following formula for the solution
of (3.108)
u(t, x) = 1
√
2π
∫ ∞
−∞
ˆf (ω)e−ω2t
eiωx dω.
This formula can be further simplified using properties of Fourier transforms. If,
as in (3.64), we define the function g(t, x) := 1
√
4πt
e−x2/4t
, which by Exercise 3.5
satisfies gˆ(t, ω) = F (g(t, ·))(ω) = (2π)
−1/2 e−tω2
, then by the convolution theorem
we obtain
u(t, x) =
∫ ∞
−∞
ˆf (ω) gˆ(t, ω) eiωx dω = 1
√
2π
∫ ∞
−∞
F (g(t, ·) ∗ f )(ω) eiωx dω
= 
g(t, ·) ∗ f
	
(x) =
∫ ∞
−∞
g(t, x − y) f (y) dy.
For this reason we call the function g the fundamental solution of the heat equation.
We will treat this example in Section 10.1 with the help of Maple®. 106 3 Elementary methods
3.6.2* The Laplace transform
We have seen that in order to apply the method of Fourier transforms to solve partial differential
equations it is necessary to assume suitable decay of the solutions, that is, asymptotic boundary
conditions. If, however, other boundary conditions are given, then we need a different integral
transform. For the sake of completeness we will give a brief introduction to the Laplace transform
here, without however going into much detail. This transform is also highly significant in many
parts of the theory of partial differential equations; a systematic treatment can be found in [6], for
example. We denote by
L1,loc(R+, C) :=

f : [0, ∞) → C measurable :
∫ c
0
|f (t)| dt < ∞ for all c > 0
'
the space of locally integrable functions on R+ := [0, ∞).
Definition 3.61 For a function f ∈ L1,loc(R+, C) we set
L f (s) = F(s) :=
∫ ∞
0
f (t)e−s t dt = lim
c→∞ ∫ c
0
f (t) e−s t dt, s ∈ C,
if the indefinite integral exists, and call the resulting function the Laplace transform of f . 
Theorem 3.62 (Existence of the Laplace transform) Let f ∈ L1(R+, C) be exponentially
bounded, that is, we have the bound |f (t)| ≤ M eγt
, t ≥ 0, for some constants M ≥ 0 and
γ ∈ R. Then L f (s) exists for all s ∈ C for which Re s > γ.
Proof See Exercise 3.11. 
We call the number γ in Theorem 3.62 an exponential bound for the function f .
Remark 3.63 The pair f (t), F(s) = L f (s) is sometimes known as a Laplace correspondence,
especially in the engineering literature, written f (t) ◦ • F(s). 
For functions f, g ∈ L1,loc(R+, C), we define their convolution via
(f ∗ g)(t) :=
∫ t
0
f (t − s) g(s) ds.
This is consistent with (3.104) when f, g ∈ L1,loc(R+, C) and the functions are extended to R by 0.
In any case f ∗ g ∈ L1,loc(R+, C); moreover, f ∗ g is exponentially bounded if f and g are. We will
now summarize some essential properties and rules of calculation of Laplace transforms.
Theorem 3.64 (Properties of the Laplace transform) Let f , g, f1,..., fn ∈ L1,loc(R+, C) all be
exponentially bounded for the common exponential bound γ. Then we have:
(i) Decay for large s: limRe s→∞ L f (s) = 0.
(ii) Linearity: we have L 
n
k=1 ck fk
	
= 
n
k=1 ck L fk for any constants c1,..., cn ∈ C.
(iii) If f ∈ Cn(R+, C) is such that f (k)
, 1 ≤ k ≤ n, is exponentially bounded with bound γ, then
L f (n)
(s) = sn L f (s) − n−1
k=0
f (k)
(0) sn−1−k, Re s > γ.
(iv) We have  d
d s n
L f (s) = (−1)n L((·)n f )(s), Re s > γ.
(v) Translation: For all α > 0 we have L(f (· − α))(s) = e−αs L f (s), Re s > γ.
(vi) Convolution theorem: For f , g as above we have L(f ∗ g) = (L f ) (Lg).3.6 Integral transforms 107
Proof Here we will limit ourselves to proving (iii); for the other statements we refer to Exercise 3.17.
By assumption and integration by parts we have
∫ ∞
0
f 
(t)e−s t dt = f (t)e−s t




t=∞
t=0
+ s
∫ ∞
0
f (t)e−s t dt = −f (0) + sL f (s),
which is the statement for n = 1. For n > 1 the statement may be proved inductively. 
As with the Fourier transform we also require an inversion formula for the Laplace transform
in order to be able to solve (partial) differential equations.
For this we will require the following terminology. We say that a function f : [0, ] → C is
piecewise continuously differentiable (or more loosely piecewise smooth) if there exist 0 = t0 < t1 <
··· < tn =  and gj ∈ C1([tj−1, tj ]), j = 1,..., n, such that gj (t) = f (t) for all t ∈ (tj−1, tj ),
j = 1,..., n. A function f : R+ → C is then called piecewise continuously differentiable (or
piecewise smooth) if f |[0,] is piecewise continuously differentiable for every  > 0. In this case
the one-sided limits
f (t+) := lim
ε↓0 f (t + ε), f (t−) := lim
ε↓0 f (t − ε).
always exist, for every t > 0.
Theorem 3.65 (Inversion formula for the Laplace transformation) Let f : [0, ∞) → C be
piecewise smooth and exponentially bounded with exponential bound γ. Then for all x > γ
1
2π lim
R→∞ ∫ R
−R
L f (x + is)e(x+i s)t ds =
 1
2

f (t+) + f (t−)	
, if t > 0,
1
2 f (0+) , if t = 0.
Proof For the proof we refer to the literature, for example [6]. 
The inverse transform can be calculated, for example, using the residue theorem, especially
if the functions involved are rational functions. There are also extensive tables of both Laplace
transforms and inverse Laplace transforms of functions available, which can be consulted when
solving differential equations. Maple® is another useful tool, as we will demonstrate in Section
10.1. Here we wish to describe an example which shows how one can derive a formula for solutions
of a differential equation using Laplace transforms.
Example 3.66 (The heat equation on an interval) We will consider the following initial-boundary
value problem for the heat equation on the interval (0, 1),
ut − ux x = f (t, x) , f (t, x) := −(t2 + x)e−t x , x ∈ (0, 1), t ≥ 0,
u(0, x) = 1, x ∈ (0, 1),
u(t, 0) = a(t) := 1, u(t, 1) = b(t) := e−t , t ≥ 0.
(3.109)
The boundary conditions a(t) := 1 and b(t) := e−t are clearly exponentially bounded functions.
We will calculate the Laplace transform in the time variable t, and so we define
U(s, x) := L[u(·, x)](s).
As when working with Fourier transforms, we proceed in four steps.
1. Transformation of the differential equation
Using the rules for calculating Laplace transforms we can transform the equation as follows:
L[ut (·, x)](s) = s U(s, x) − u(0, x) = s U(s, x) − 1.108 3 Elementary methods
2. Transformation of the boundary conditions
Unlike the above examples involving Fourier transforms, here we need to transform not the
initial condition but the boundary conditions a(t) ≡ 1 and v(t) = e−t :
U(s, 0) = L[a](s) = s−1
, U(s, 1) = L[b](s) = (1 + s)
−1
, Re s > 0.
This transforms the partial differential equation (3.109) into a boundary value problem for an
ordinary differential equation of the form
−Ux x + s U = 1 + F(s, x) , for Re s > 0, x ∈ (0, 1),
U(s, 0) = s−1 , Re s > 0,
U(s, 1) = (1 + s)
−1 , Re s > 0,
(3.110)
where F(s, x) := L[f (·, x)](s) = −2(s + x)
−3 − x
s+x is the Laplace transform of the right-hand
side, cf. Exercise 3.18.
3. Solution of the boundary value problem
The above homogeneous linear boundary value problem for a second-order ordinary differential
equation can be solved by standard methods; the solution is U(s, x) = (s + x)
−1.
4. Inverse transformation
With the help of a partial fraction expansion one can check that u(t, x) = e−t x is a solution of
the problem (3.109), cf. Section 10.1 
We finish by giving an example in which Fourier and Laplace transforms can be combined.
Example 3.67 (The inhomogeneous heat equation on R) We consider the following initial value
problem for the inhomogeneous heat equation
ut = ux x + f , x ∈ R, t ≥ 0,
u(0, x) = u0(x), x ∈ R,
u(t, x) → 0 , as |x |→∞, t ≥ 0,
where u0 ∈ Cc (R) and f ∈ C([0, ∞) ×R) is bounded. We begin by taking the Fourier transform in
the space variable x and using the rules in Theorem 3.53 obtain the following initial value problem
for uˆ(t, ω) := F
u(t, ·)	
(ω), ω ∈ R:
uˆt (t, ω) = −|ω|
2 uˆ(t, ω) + ˆf (t, ω) , t ≥ 0, ω ∈ R,
uˆ(0, ω) = uˆ0(ω), ω ∈ R.
The second step is to calculate the Laplace transform with respect to the time variable t; using the
rules of calculation for Laplace’s equation from Theorem 3.64 we are led to an algebraic equation
for Uˆ (s, ω) := L
uˆ(·, ω)
	
(s), namely s Uˆ (s, ω) −uˆ0(ω) = −|ω|
2 Uˆ (s, ω)+Fˆ(s, ω) for all s ∈ C
such that Re s > 0, where Fˆ(s, ω) := L ˆf (·, ω)
	
(s). This equation is easy to solve; we obtain
Uˆ (s, ω) = 1
s + |ω|
2

uˆ0(ω) + Fˆ(s, ω)

.
Applying the inverse Laplace transform (cf. Exercise 3.11) and using the convolution theorem
(Theorem 3.64(iv)) gives
uˆ(t, ω) = uˆ0(ω) e−t|ω|
2
+
∫ t
0
e−|ω|
2(t−s) ˆf (s, ω) ds.3.7 Outlook 109
Finally we use that (cf. Exercise 3.5)
g(t, x) := 1
√
4πt
e−x2/4t satisfies gˆ(ω) = 1
√
2π
e−tω2
.
An application of the convolution theorem (Theorem 3.55) now leads to the formula
u(t, x) =
∫
R
g(t, x − y) u0(y) dy +
∫ t
0
∫
R
g(t − s, x − y) f (s, y) dy ds
for the solution, which expresses it in terms of convolutions with the source term f and the initial
function u0. 
3.7 Outlook
In this chapter we investigated the following three partial differential equations using
elementary methods:
uxx + uyy = 0 (Laplace’s equation)
ut = uxx (heat equation)
utt = uxx (wave equation)
As we saw when classifying equations in Chapter 2, the first is an elliptic, the second
a parabolic and the third a hyperbolic equation. It should already be clear from
our elementary study in this chapter that these three kinds of equations (elliptic,
parabolic and hyperbolic) evince very different properties. We wish to summarize
the most important of these.
Well-posedness: In all three cases we can prove existence and uniqueness of
solutions, as long as the equations are paired with suitable boundary conditions (and
initial conditions, as the case may be). Uniqueness (and continuous dependence on the
data) for Laplace’s equation is due to the elliptic maximum principle (Section 3.3.3),
in the case of the heat equation it is a consequence of the parabolic maximum
principle (Section 3.4.2), and for the wave equation it follows from the principle of
conservation of energy (Section 3.1.2).
Smoothing: Both Laplace’s equation and the heat equation exhibit a quite strong
smoothing effect: the solution of the Dirichlet problem is always in C∞(Ω) (The￾orems 3.28 and 3.30); the solution of the heat equation is in C∞((0, ∞) × Ω), that
is, smooth in both time and space variables for any initial values (Theorem 3.36,
Corollary 3.37 and Theorem 3.41). The solutions of the wave equation, on the other
hand, are no more regular than the data: for example, d’Alembert’s formula from
Theorem 3.2 shows that the solution is exactly as regular as the initial data.
Conservation of information: The “information” contained in the initial data is
preserved in the solution of the wave equation (here we are thinking, for example,
of the form of a signal); this can again be seen via d’Alembert’s formula (3.7). If the110 3 Elementary methods
initial data have compact support, then so too does the solution at every point in time.
The speed of propagation of solutions of the wave equation is finite. The situation is
completely different in the case of the heat equation. The explicit solution (3.79) in
terms of the Gaussian kernel shows that u(t, x) > 0 for all x ∈ Rd and t > 0 as long
as u0 ≥ 0 and u0  0. This means that the speed of propagation is infinite.
Asymptotic behavior as t → ∞: The solutions of the heat equation in Rd
converge to 0 as t → ∞. The same is true on an interval with Dirichlet boundary
conditions. The wave equation is completely different: the energy is constant in time,
see Theorem 3.5.
Both the heat equation and the wave equation are evolution equations: they de￾scribe the evolution in time of a certain state. We summarize the properties of the
solutions of these equations in the following table.
Table 3.1 Properties of different kinds of equations.
property wave/transport diffusion
well-posedness yes yes
maximum principle no yes
smoothing no C∞
conservation of info yes no
propagation speed ≤ C ∞
behavior for t → ∞ energy constant u(t, x) → 0
Over the course of the book we will study these three prototypical equations
(Laplace, heat and wave equations) in higher dimensions. Properties of parabolic
equations can generally be easily deduced from those of the corresponding elliptic
equations. For this reason, our principal focus in Chapters 5, 6 and 7 is on elliptic
equations. In Chapter 8 we turn to heat equations on domains. We will see that the
properties listed in the above table remain valid in higher dimensions and when the
equations are defined on more complicated domains.
3.8 Exercises
Exercise 3.1 Solve the following initial value problem for the telegraph equation
using Laplace transforms
uxx − a utt − b ut − c u = 0 , x ∈ R , t > 0 ,
where u(0, x) = ut(0, x) = 0, u(t, 0) = g(t) and limx→∞ u(t, x) = 0, and under the
assumption that b2 − 4ac = 0. Which conditions does g need to satisfy?3.8 Exercises 111
Exercise 3.2 Solve using Laplace transforms
ut = uxx , (t, x)∈[0, ∞) × [0, ∞) ,
where u(0, x) = 1 for all x > 0, u(t, 0) = 0 for all t > 0, and limx→∞ u(t, x) = 1.
Exercise 3.3 Using Fourier transforms, derive a formula for the solution of the
Dirichlet problem uxx + uyy = 0, (x, y) ∈ R× [0, ∞), with boundary values u(x, 0) =
f (x), x ∈ R. What conditions do we need to assume on f ?
Exercise 3.4 (Abelian convergence of Fourier series)
(a) Show that the space T of trigonometric polynomials is dense in C2π.
Suggestion: Use the theorem of Stone–Weierstrass (Theorem A.5).
(b) Let f ∈ C2π have Fourier series
f (t) = c0 +
∞
k=1

ak cos(kt) + bk sin(kt)
	
.
In accordance with (a), let fn(t) = c
(n)
0 + 
Nn
k=1

a(n)
k cos(kt) + b
(n)
k sin(kt)
	
be
such that  fn − f ∞ → 0 as n → ∞. Show that then c
(n)
0 → c0, a(n)
k → ak and
b
(n)
k → bk as n → ∞.
(c) Let un(r, t) = c
(n)
0 +
Nn
k=1 rk 
a(n)
k cos(kt)+b
(n)
k sin(kt)
	
. Show that un converges
uniformly in [0, 1] × R to a function u ∈ C([0, 1] × R).
(d) Show that u(r, t) = c0 + 
∞
k=1 rk 
ak cos(kt) + bk sin(kt)
	
for all 0 ≤ r < 1.
(e) Conclude that limr ↑1 u(r, t) = f (r) uniformly in t ∈ R. This is exactly the
convergence of the Fourier series of f in the sense of Abel (Theorem 3.11).
Exercise 3.5 For the function ga(t) := 1
√aπ e−x2/a, where a ∈ R+, show that gˆa(ω) =
(2π)
−1/2e−aω2/4. Use that ∫ ∞
−∞ e−x2
dx = √π.
Exercise 3.6 Prove Lemma 3.21.
Exercise 3.7 Prove Theorem 3.40.
Exercise 3.8 Prove Lemma 3.42.
Exercise 3.9 Prove Theorem 3.43.
Exercise 3.10 Prove Theorem 3.53 parts (i), (ii), (v) and (vi).
Exercise 3.11 Prove Theorem 3.62 (existence of the Laplace transform).112 3 Elementary methods
Exercise 3.12 Let u0 ∈ C2(R), u1 ∈ C1(R) be initial values for the initial value
problem for the wave equation
utt − c2uxx = 0 in R+ × R,
u(0, ·) = u0 in R,
ut(0, ·) = u1 in R.
Show using d’Alembert’s formula that the solution u ∈ C2(R × R) at the point
(t, x) only depends on the initial values u0(y), u1(y) in the domain of dependence
y ∈ A(t, x) := [x − c|t|, x + c|t|], cf. Figure 3.1 on page 52.
Exercise 3.13 Consider the Heaviside function H : R → R, defined by
H(x) :=

1, x ≥ 0,
0, otherwise.
Show that (H ∗ ϕ)(x) = ∫ x
−∞ ϕ(s) ds for all ϕ ∈ D(R).
Exercise 3.14 Solve the partial differential equation
x2uxx − y2
uyy + xux − yuy = 0
in R2 via a suitable variable transformation. How can uniqueness of solutions be
attained?
Exercise 3.15 Determine the solution u(t, x) of the initial-boundary value problem
uxx = 4utt, t > 0, x ∈ (0, 1),
u(t, 0) = u(t, 1) = 0, t > 0,
u(0, x) = sin(2πx), x ∈ [0, 1],
ut(0, x) = x(x − 1), x ∈ [0, 1].
Exercise 3.16 Use a Fourier series ansatz to solve the problem
utt + a2uxxxx = 0, t > 0, x ∈ (0, ),
u(t, 0) = u(t, ) = 0, t > 0,
uxx(t, 0) = uxx(t, ) = 0, t > 0,
u(0, x) = f (x), x ∈ (0, ),
ut(0, x) = g(x), x ∈ (0, ),
where f, g are odd 2-periodic functions and a ∈ R.
Exercise 3.17 Prove Theorem 3.64 parts (i), (ii), (iv), (v) and (vi).3.8 Exercises 113
Exercise 3.18 Show that:
(a) a(t) := 1, L[a](s) = s−1
(b) b(t) := e−t
, L[b](s) = (1 + s)
−1
(c) f (t, x) := −(t
2 + x)e−t x, L[ f (·, x)](s) = −2(s + x)
−3 − x
s+x
(d) U(s, x) := (s + x)
−1, L−1[U(·, x)](t) = e−t x
Exercise 3.19 (Harmonic functions do not see individual points) Let Ω ⊂ R2 be an
open set, x0 ∈ Ω and Ω∗ := Ω \ {x0}. We denote by H (Ω) := {u ∈ C2(Ω) : Δu = 0}
the space of harmonic functions on Ω. We wish to show that the sets H (Ω∗) ∩ C(Ω)
and H (Ω) agree. We let B := B(x0, R) be a ball in Rd and B∗ := B \ {x0}.
(a) Determine all functions f ∈ C2(0, R) such that r f (r) + f 
(r) = 0 for all
r ∈ (0, R).
(b) Let u ∈ H(B∗) ∩ C(B) and u|∂B = 0. Show that u is invariant under rotations
about x0: more precisely, u(x0 + y1) = u(x0 + y2) for all y1, y2 ∈ R2 such that
|y1| = |y2| ≤ R.
(c) Prove that if w ∈ C2(B∗) is invariant under rotations about x0, then there exists
a function w˜ ∈ C2(0, R) such that w˜(|x − x0|) = w(x) for all x ∈ B∗. Moreover,
this function satisfies Δw(x) = w˜ (r) + 1
r w˜ 
(r), where r := r(x) := |x − x0|.
(d) Let u and v be functions in H (B∗) ∩ C(B) such that u|∂B = v|∂B. Show that
u = v.
Suggestion: It can be shown using part (c) that w := u − v vanishes identically
in B.
(e) Let u ∈ H(B∗) ∩ C(B). Show that u ∈ H(B). Thus Ω∗ is not Dirichlet regular.
(f) Let u ∈ H(Ω∗). Show that there is a unique extension v ∈ H(Ω) of v to Ω if and
only if the limit limx→x0 u(x) exists.
(g) Does the assertion of part (f) also hold in dimension d = 1? In other words, is
every function which is in H ((−1, 0)∪(0, 1)) ∩ C([−1, 1]) also in H (−1, 1)?
Exercise 3.20 Let fn ∈ C2([0, 1]) and f, g ∈ C([0, 1]) be functions such that
limn→∞ fn = f and limn→∞ f 
n = g uniformly in [0, 1]. Show that ( f 
n)n∈N con￾verges uniformly in [0, 1] to a function h ∈ C([0, 1]), and deduce that f ∈ C2([0, 1]),
f  = h, f  = g.
Exercise 3.21 Let g ∈ C1([0, b]).
(a) Assume that g(0) = g(b) = 0. Show that the 2b-periodic odd extension G of g
is in C1(R).
(b) Assume that g
(0) = g
(b) = 0. Show that the 2b-periodic even extension of g
is in C1(R).
(c) Let G ∈ C1(R). Show that if G is odd (i.e., G(x) = −G(−x) for all x ∈ R) then
G is even (i.e., G(x) = G(−x) for all x ∈ R).
(d) Let g ∈ C2([0, b]) satisfy g(0) = g(b) = g(0) = g(b) = 0. Show that in this
case the 2b-periodic odd extension of g is in C2(R).114 3 Elementary methods
Exercise 3.22 (Well-posedness of the IBVP for the wave equation) Background:
For f ∈ C([a, b]) let  f C([a,b]) := supx∈[a,b] | f (x)] denote the supremum norm.
Then C1([a, b]) is a Banach space for
 f C1([a,b]) :=  f C([a,b]) +  f 
C([a,b]).
Similarly, C2([a, b]) is a Banach space for the norm
 f C2([a,b]) :=  f C1([a,b]) +  f C([a,b]).
Moreover, X := {u0 ∈ C1([a, b]) : u0(a) = u0(b) = 0} is a closed subspace of
C1([a, b]) and Y := {u1 ∈ C2([a, b]) : u1(a) = u1(b) = u
1 (a) = u
1 (b) = 0} a closed
subspace of C2([a, b]). Finally, C2([0, T]×[a, b]) is a Banach space for the norm
uC2 := uC + ut C + ux C + ut x C + utt C + uxx C,
where for v ∈ C2([0, T]×[a, b]), we set
v C := sup{|v(t, x)| : t ∈ [0, T], x ∈ [a, b]}.
The space
Z := {u ∈ C2([0, T]×[a, b]) : u satisfies (3.12a) and (3.12b)}
is a closed subspace of C2([0, T]×[a, b]). All these assertions are routine arguments
based on the completeness of C([a, b]).
The exercise consists in showing the following: Let u0 ∈ X, u1 ∈ Y and let u
be the solution of (3.12). Given n ∈ N, un
0 ∈ X, un
1 ∈ Y, let un be the solution of
(3.12) with u0, u1 replaced by un
0 , un
1 . Use the Closed Graph Theorem to show that
lim
n→∞ un − uC2 = 0.Chapter 4
Hilbert spaces
The goal of this chapter is to give an elementary but comprehensive introduction to
the theory of Hilbert spaces, where we wish to highlight the many-faceted interplay
of their geometric and analytic properties. This culminates in the theorem of Riesz–
Fréchet, which describes continuous linear forms in a Hilbert space. Important for
us will be that this theorem can be interpreted as an existence and uniqueness
result. A generalization known as the Lax–Milgram theorem, which we will give in
Section 4.5, is at the heart of the solution theory of elliptic equations which we will
present in Chapters 5, 6 and 7. For the reader who is principally interested in these
equations, the part of the current chapter up to and including Section 4.5 provides
sufficient background.
The rest of the chapter, from Section 4.6 on, is devoted to spectral theory. The key
result here, the spectral theorem of Section 4.8, asserts that, for certain operators,
the ambient Hilbert space has an orthonormal basis consisting of eigenvectors of the
operator. This result is the most important tool for studying evolution equations in
Chapter 8. The spectral theorem will permit us to apply the method of separation
of variables (more precisely, space and time variables will be separated) to far more
general situations than we could manage in Chapter 2 using just Fourier series.
Chapter overview
4.1 Inner product spaces ................................ 116
4.2 Orthonormal bases ................................. 120
4.3 Completeness .................................... 123
4.4 Orthogonal projections ............................... 125
4.5 Linear and bilinear forms .............................. 128
4.6 Weak convergence ................................. 135
4.7 Continuous and compact operators ......................... 138
4.8 The spectral theorem ................................ 139
4.9* Comments on Chapter 4 .............................. 150
4.10 Exercises ...................................... 151
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4_4
115116 4 Hilbert spaces
By a Hilbert space we mean a vector space equipped with an inner product, such
that the vector space is complete with respect to the norm induced by the inner
product. We will now proceed to introduce the necessary terms step-by-step.
4.1 Inner product spaces
Since all Hilbert spaces are inner product spaces, sometimes also called pre-Hilbert
spaces, we will naturally start with these. Let E be a vector space over the field
K = R or K = C. A mapping
(·, ·) : E × E → K f, g → ( f, g)
is called an inner product or scalar product if the following conditions are satisfied:
(a) ( f + g, h) = ( f, h) + (g, h), f, g, h ∈ E;
(b) (λ f, g) = λ( f, g), f, g ∈ E, λ ∈ K;
(c) ( f, g) = (g, f ), f, g ∈ E;
(d) ( f, f ) > 0 ( f  0), f ∈ E.
Notice that (c) implies that ( f, f ) = ( f, f ) ∈ R, for all f ∈ E. Thus (d) does in
fact make sense when K = C. We call (c) symmetry and (d) positive definiteness.
The symmetry property also implies
(a’) ( f, g + h) = ( f, g) + ( f, h), f, g, h ∈ E;
(b’) ( f, λg) = λ¯( f, g), f, g ∈ E.
Here and in what follows, λ¯ denotes the complex conjugate of the number λ ∈ C.
Inner products are thus linear in the first variable (that is, (a) and (b) hold), while
they are antilinear in the second (that is, (a’) and (b’) hold). We shall now consider
a few examples.
Example 4.1 (a) Let E = Rd, then (x, y) := 
d
j=1 xj yj = xT y defines the natural
inner product on Rd.
(b) Let E = Cd, then (x, y) := 
d
j=1 xj yj is the natural inner product on Cd.
(c) Let a < b and set C([a, b]) := { f : [a, b] → K : f continuous} to be the space
of continuous functions on [a, b]. Then
( f, g) :=
∫ b
a
f (t)g(t)dt
defines an inner product on C([a, b]). Observe that C([a, b]) is infinite dimen￾sional, while Rd and Cd are finite dimensional. 
We call a vector space E equipped with an inner product, or more precisely the
pair (E, (·, ·)), an inner product space (or sometimes pre-Hilbert space). We now
wish to establish a number of geometric properties of inner products. To this end,
we need to introduce a further real-valued quantity. The number x := 
(x, x) is4.1 Inner product spaces 117
called the norm of the vector x ∈ E; we also speak of the norm induced by the inner
product (·, ·). It has the following properties:
(N1)  f  = 0 ⇐⇒ f = 0, f ∈ E;
(N2) λ f  = |λ| f , λ ∈ K, f ∈ E;
(N3)  f + g≤ f  + g, f, g ∈ E.
Properties (N1) and (N2) follow directly from the axioms of inner products. The
triangle inequality (N3), however, is not obvious, and we will need a couple of
preliminaries in order to prove it. To this end, we first consider the instructive
example E = R2 with the natural inner product
(x, y) := x1 y1 + x2 y2 for x =

x1
x2

, y =

y1
y2

∈ R2.
By Pythagoras’ theorem in R2, the norm x =

x2
1 + x2
2 of x ∈ R2 is the distance
between x and the origin. If x, y ∈ E are on the unit sphere, that is, x = y = 1,
then we can write them as x = (cos θ1,sin θ1), y = (cos θ2,sin θ2). Hence the inner
product
(x, y) = cos θ1 · cos θ2 + sin θ1 · sin θ2 = cos(θ1 − θ2)
is the cosine of the angle between the two vectors x and y. In particular, x and y are
orthogonal (that is, perpendicular) to each other if and only if (x, y) = 0. If x, y ∈ R2,
x  0, y  0, are now two vectors with arbitrary lengths, then the points x
x  , y
y 
lie on the unit sphere. Since
(x, y) = x·y ·  x
x
, y
y

,
we see that in this more general case, x and y are still orthogonal to each other if and
only if (x, y) = 0, cf. Figure 4.1.
Fig. 4.1 Two points x, y ∈ R2 on the unit sphere and the angle between them.118 4 Hilbert spaces
This leads us to the following definition. Let E be a pre-Hilbert space over K.
Two vectors x, y ∈ E are said to be orthogonal (and we write x ⊥ y) if (x, y) = 0.
Using the axioms for inner products, we can obtain the following characterization
of orthogonality via the norm. We emphasize that here (E, (·, ·)) is an arbitrary inner
product space.
Theorem 4.2 (Pythagoras’ theorem for inner product spaces) Let E be an inner
product space. If f, g ∈ E are orthogonal, then  f + g2 =  f 2 + g2. In the case
where K = R also the converse holds.
Proof We have, using that ( f, g) = (g, f ),
 f + g2 = ( f + g, f + g) = ( f, f ) + ( f, g) + (g, f ) + (g, g)
= ( f, f ) + (g, g) + 2 Re( f, g) =  f 2 + g2 + 2 Re( f, g).
This shows that Re( f, g) = 0 if and only if  f + g2 =  f 2 + g2. 
For an arbitrary subset M ⊂ E of E, we denote by
M⊥ := { f ∈ E : f ⊥ g for all g ∈ M}
the orthogonal complement of M. One can see easily that M⊥ is always a (linear)
subspace of E, for any set M ⊂ E. Moreover, due to the definiteness of the inner
product, we always have M ∩ M⊥ = {0}. We next consider straight lines in inner
product spaces E.
Theorem 4.3 Let g ∈ E with g = 1. Consider the straight line G := {λg : λ ∈ K}
passing through 0 and g. Then, for any f ∈ E, we have f − ( f, g)g ∈ G⊥.
Proof Using the properties of the inner product, we obtain

f − ( f, g)g, λg
	
= ( f, λg)−( f, g)(g, λg) = λ¯( f, g)−( f, g)λ¯(g, g) = 0
for all f ∈ E, since by assumption (g, g) = 1. 
We call the vector P f := ( f, g) g the orthogonal projection of f onto the line G,
since P f ∈ G and f − P f ∈ G⊥, the latter holding by Theorem 4.3. We can now
prove a central inequality.
Theorem 4.4 (Cauchy–Schwarz inequality1) We have |( f, g)| ≤  f  g for all
f, g ∈ E.
1 Sometimes the name Cauchy–Bunyakovsky–Schwarz inequality is used, after the Russian math￾ematician Viktor Yakovlevich Bunyakovsky (1804–1889).4.1 Inner product spaces 119
Proof We proceed in two steps:
Step 1: g = 1. Then f = P f + f −P f = ( f, g)g+( f −P f ). Since ( f, g)g ⊥ ( f −P f ),
by Pythagoras’ theorem and (N2),
 f 2 = ( f, g) g2 +  f − P f 2 = |( f, g)|2 +  f − P f 2 ≥ |( f, g)|2
due to the positivity of the norm.
Step 2: Now let g ∈ E be arbitrary. The inequality is trivial if g = 0, so we may
suppose that g  0. We may then apply Step 1 to g1 := 1
g g to obtain the claim. 
The triangle inequality (that is, (N3)) is a direct consequence of the Cauchy–
Schwarz inequality. Let f, g ∈ E; then
 f + g2 = ( f + g, f + g) = ( f, f ) + ( f, g) + (g, f ) + (g, g)
=  f 2 + ( f, g) + ( f, g) + g2 =  f 2 + 2 Re ( f, g) + g2
≤  f 2 + 2|( f, g)| + g2 ≤  f 2 + 2 f  g + g2
= ( f  + g)2
,
which establishes (N3). Thus · is a norm on E. This permits us to define conver￾gence: if fn, f ∈ E, then we say that the sequence ( fn)n∈N converges to f (and write
limn→∞ fn = f or fn → f ), if limn→∞  fn − f  = 0.
Example 4.5 Let −∞ < a < b < ∞ and let E = C([a, b]) be equipped with the
corresponding norm:
( f, g) :=
∫ b
a
f (t) g(t) dt,  f  =
∫ b
a
| f (t)|2 dt 1
2
= ( f, f )
1
2
We also call this norm the 2-norm and write  f L2(a,b) :=  f . If there is no danger
of confusion, then we also write ·L2 . For fn, f ∈ E, we thus have fn → f in E if
and only if
∫ b
a
| fn(t) − f (t)|2 dt 1
2
→ 0 ,
that is, the 2-norm describes convergence in quadratic mean. 
Remark 4.6 In addition to the 2-norm, we will also sometimes consider the supremum
norm (∞-norm), which is given by
 f ∞ := sup
t ∈[a,b]
| f (t)| ≡  f C([a,b]) .
This leads to the notion of uniform convergence: fn → f with respect to ·∞ if
and only if for all ε > 0 there is some n0 ∈ N such that | fn(t) − f (t)| < ε for all
n ≥ n0 and all t ∈ [a, b]. 120 4 Hilbert spaces
4.2 Orthonormal bases
The goal of this section is to introduce orthonormal bases. To this end let (E, (·, ·))
be an inner product space over K = C or R. Let I = N, Z, or {1,..., d}. A family
{en : n ∈ I} in E is called orthonormal if
(en, em) =

1 if n = m,
0 if n  m.
Let us first consider the finite case I = {1,..., d}. Let
F := span{e1,..., ed} =


d
n=1
λnen : λ1,...,λd ∈ K
!
be the span (also known as the linear span or the linear hull) of {e1,..., ed}. For
f = 
d
n=1 λnen ∈ F and m ∈ {1,..., d}, we have
( f, em) =


d
n=1
λnen, em

= λm.
Hence f = 
d
n=1( f, en)en is the unique representation of f ∈ F as a linear combi￾nation of the vectors {e1,..., ed}. If we consider infinite index sets I, then we need
infinite series. To this end, we will again consider convergence with respect to the
norm associated with the inner product  f  := ( f, f )
1
2 .
Definition 4.7 A subset M of E is called dense in E if for every g ∈ E there is a
sequence ( fn)n∈N in M such that limn→∞ fn = g in (E, (·, ·)). The set M is called
total if
spanM :=
⎧⎪⎨
⎪
⎩
m
j=1
λjgj : m ∈ N, λ1,...,λm ∈ K, g1,..., gm ∈ M
⎫⎪⎬
⎪
⎭
is dense in E. 
Definition 4.8 Let I = N or Z. A family {en : n ∈ I} is called an orthonormal basis
of E if it is orthonormal and total in E. 
We next define when a series converges in E. Let fn ∈ E, n ∈ I, and let f ∈ E.
We say that the series 

n∈I fn converges to f and write

n∈I
fn = f
if limn→∞ 
n
m=1 fm = f for I = N; and if f = limn→∞ 
n
m=−n fn in the case I = Z.
An (infinite) series 

n∈I fn is said to be convergent if there exists a vector f ∈ E
such that 

n∈I fn = f .4.2 Orthonormal bases 121
Theorem 4.9 (Orthonormal expansion) Let I = N or Z and let {en : n ∈ I} be an
orthonormal basis of E, as well as f ∈ E. Then:
(a) Orthonormal expansion:


n∈I( f, en)en = f ;
(b) Parseval’s identity:


n∈I |( f, en)|2 =  f 2.
Proof Let En := span{em : m ∈ I, |m| ≤ n}. For f ∈ E define
Pn f :=

m∈I, |m| ≤n
( f, em) em.
We need to show that limn→∞ Pn f = f .
1. We claim that Pn f ∈ En and ( f − Pn f ) ∈ E⊥
n .
The first statement is clear. To show the second, choose |k| ≤ n. Then
( f − Pn f, ek ) = ( f, ek ) − 
|m| ≤n
( f, em)(em, ek ) = ( f, ek )−( f, ek ) = 0.
This shows that f − Pn f is orthogonal to all ek for which |k| ≤ n; and thus
( f − Pn f ) ∈ E⊥
n by definition.
2. By Pythagoras’ theorem,  f 2 = Pn f 2 +  f − Pn f 2. In particular, Pn f  ≤
 f  for all f ∈ E and all n ∈ N. If now f ∈ En for some n ∈ N, then Pm f = f
for all m ≥ n. Hence limm→∞ Pm f = f trivially in this case.
Now let f ∈ E and ε > 0 be arbitrary. By assumption {en : n ∈ I} is an
orthonormal basis of E, hence there exist an n ∈ N and a vector gn ∈ En such that
 f − gn  < ε/2. Thus Pmgn = gn for all m ≥ n and so, for all m ≥ n,
 f − Pm f  =  f − gn + Pmgn − Pm f ≤ f − gn  + Pm(gn − f )
≤ 2 f − gn  < ε.
Since ε > 0 was arbitrary, we have shown that limm→∞ Pm f = f for all f ∈ E. This
proves statement (a). But it also establishes (b), since, by Pythagoras’ theorem,
Pn f 2 =
"
"
"
"
"
"

|k | ≤n
( f, ek ) ek
"
"
"
"
"
"
2
=

|k | ≤n
( f, ek ) ek 2 =

|k | ≤n
|( f, ek )|2.
By (a),  f 2 = limn→∞ Pn f 2 for all f ∈ E. This proves (b). 
We already know one important example of an orthonormal basis. Consider
C2π := { f : R → C continuous, 2π-periodic}
from Chapter 3, which is an inner product space with respect to
( f, g) := 1
2π
∫ π
−π
f (t) g(t) dt.122 4 Hilbert spaces
Let ek (t) := eikt, t ∈ R. Then {ek : k ∈ Z} is an orthonormal basis of C2π. One can
establish the orthonormality simply by integrating. But we also saw that the space
J = span{ek : k ∈ Z} is dense in C2π with respect to the supremum norm ·∞
(Corollary 3.17). But since
 f L2 =

( f, f ) =
 1
2π
∫ π
−π
| f (t)|2 dt 1
2
≤  f ∞,
it follows immediately that J is also dense in C2π with respect to the norm ·L2
of the inner product space. We have thus shown that {ek : k ∈ Z} is an orthonormal
basis of C2π. Applying Theorem 4.9, we obtain the following classical result about
the Fourier series expansion of a function f ∈ C2π.
Theorem 4.10 Let f ∈ C2π, let ck = ( f, ek ) = 1
2π
∫ π
−π f (t)e−ikt dt be the kth Fourier
coefficient of f , for k ∈ Z, and let sn := 
n
k=−n ck ek be the nth Fourier polynomial
of f . Then limn→∞ sn − f L2 = 0.
The theorem says that the Fourier series of a function f ∈ C2π converges to f in
quadratic mean. In this case we also write
f =
∞
k=−∞
ck ek with respect to ·L2 .
While in general the series does not converge pointwise, we nevertheless have two
positive results: the quadratic convergence of Theorem 4.10 and convergence in the
sense of Abel from Theorem 3.11 (page 57).
The question arises as to whether every inner product space has an orthonormal
basis; this is indeed the case if the space is “not too big”. We wish to describe
this more precisely. We say that an inner product space E is separable if there is a
(countable) sequence (xn)n∈N in E such that the set {xn : n ∈ N} is dense in E. This
is always the case if there is a total sequence (un)n∈N in E, since we may then set (if,
say, K = R)
Fn :=

n
k=1
qkuk : n ∈ N , qk ∈ Q
!
;
if the sequence (un)n∈N is total, then F := /∞
n=1 Fn is dense in E. The set F is
countable as the countable union of countable sets. If K = C then we may replace Q
by Q + iQ.
Theorem 4.11 (Orthonormalization) Every separable inner product space E ad￾mits an orthonormal basis.
Proof Let(un)n∈N be a total sequence in E.We will assume that the space E is infinite
dimensional. We can then assume that the set {u1,..., un} is linearly independent4.3 Completeness 123
for each n ∈ N (otherwise we simply remove unnecessary vectors). Using the Gram–
Schmidt orthogonalization process, we will now construct an orthonormal sequence
(en)n∈N such that
span{u1,..., un} = span{e1,..., en} (4.1)
for all n ∈ N.
To this end, first set e1 := u1 −1u1 and for n ∈ N assume that e1,..., en have
already been found. Now define
wn+1 := un+1 −
n
j=1
(un+1, ej) ej .
Then (wn+1, ei) = 0 for all i = 1,..., n. Moreover, wn+1  0 due to our assumption
that the uk are linearly independent. Now set en+1 := wn+1 −1wn+1. Then (4.1) is
satisfied for n + 1 and the construction has been completed. The sequence (en)n∈N
is orthonormal, and total in E by (4.1). Hence (en)n∈N is an orthonormal basis.
If dim E < ∞, then we choose an arbitrary basis {u1,..., un} of E and orthonor￾malize it in accordance with the above scheme. 
The above proof clearly yields a constructive procedure to orthonormalize n
linearly independent vectors. The assumption of separability is satisfied in practically
all applications. In particular all inner product spaces which are useful for partial
differential equations, are separable.
4.3 Completeness
Let E be an inner product space over K = R or C, with inner product (·, ·). We
consider convergence in E with respect to the induced norm u := 
(u, u), as
well as Cauchy sequences with respect to this norm. The space E is said to be
complete if for every Cauchy sequence (un)n∈N in E there is a vector u ∈ E such that
limn→∞ un = u. A Hilbert space is a complete inner product space. In what follows,
we will always denote Hilbert spaces by H or V. We start by giving an example.
Example 4.12 (a) Each finite-dimensional inner product space is complete (see [2,
§2.6]).
(b) Let I = N or Z and
2(I) :=

(xn)n∈I ⊂ K : (xn)n∈I 2
2(I) :=

n∈I
|xn|
2 < ∞
!
.
This space is a Hilbert space with respect to the inner product
(x, y) :=

n∈I
xn yn124 4 Hilbert spaces
and the norm x2(I) := 
(x, x) induced by it. Moreover, if we define en :=
(0, 0,..., 1, 0,...) to be the sequence with exactly one 1, in position n, and 0 else￾where, then {en : n ∈ I} is an orthonormal basis. We refer to Exercise 4.1 for the
proof. 
Under the assumption of completeness, we can say more about the representation
of vectors with respect to an orthonormal basis.
Theorem 4.13 Let H be a separable Hilbert space and let {en : n ∈ I} be an
orthonormal basis, where I = N or Z. Let x = (xn)n∈I ∈ 2(I). Then the series
u = 

n∈I xn en converges in H and (u, en) = xn for all n ∈ I.
Proof We will give the proof for I = N; for I = Z one argues analogously. For every
ε > 0 there is a number n0 ∈ N such that

n≥n0
|xn|
2 < ε.
Set un := 
n
k=1 xk ek . Then, by Pythagoras’ theorem, for n, m ≥ n0 we have
un − um 2 =
"
"
"
"
"
n
k=m+1
xk ek
"
"
"
"
"
2
=
n
k=m+1
|xk |
2 < ε,
where we have assumed that n > m. This shows that (un)n∈N is a Cauchy sequence;
hence it has a limit u := limn∈N un, and
(u, em) = lim
n→∞(un, em) = xm,
which proves the claim. 
In the other direction, we saw in Theorem 4.9 that

n∈I
|(u, en)|2 = u2
H < ∞
for every u ∈ H. It follows that the mappingU : H → 2(I) given by u → 
(u, en)
	
n∈I
is linear and bijective with (Uu, Uv) = (u, v) for all u, v ∈ H. Such a mapping is
called unitary. This means that the two Hilbert spaces H and 2(I) are essentially
the same object: the mapping U preserves addition and scalar multiplication (that is,
the vector space structure of the spaces) as well as the inner product. Nevertheless,
Hilbert spaces often have a very different form from 2(I); two such examples follow.
Further examples are Sobolev spaces, to be introduced in Chapter 5.
Example 4.14 The space
L2((0, 2π), C) :=

f : (0, 2π) → C : f is measurable and ∫ 2π
0
| f (t)|2dt < ∞
'4.4 Orthogonal projections 125
is a complex Hilbert space with respect to the inner product
( f, g) := 1
2π
∫ 2π
0
f (t) g(t) dt,
where the integration is with respect to Lebesgue measure. The functions {ek :
k ∈ Z}, ek (t) = eikt, form an orthonormal basis. Hence the mapping defined as
U: L2((0, 2π), C) → 2(Z), f → (( f, ek ))k ∈Z is unitary. 
Proof We have already seen that {ek : k ∈ Z} is total in C2π. We can identify C2π
with the space F := { f : [0, 2π] → C : f is continuous and f (0) = f (2π)}. From
measure theory we know that F is dense in L2((0, 2π), C), see e.g. [54, Ch. III].
Hence {ek : k ∈ Z} is total in L2((0, 2π), C). 
The following Hilbert space will be used frequently throughout the book.
Example 4.15 Let Ω ⊂ Rd be an open set and K = R or C. We set
L2(Ω, K) :=

f : Ω → K : f is measurable and ∫
Ω
| f (x)|2dx < ∞
'
.
Then the space L2(Ω, K) is a Hilbert space over K with respect to the inner product
( f, g) := ( f, g)L2 :=
∫
Ω
f (x) g(x) dx.
Here dx is the Lebesgue measure on Ω. We set L2(Ω) := L2(Ω, R). 
4.4 Orthogonal projections
Let H be a Hilbert space over K = R or C. If M is a subset of H, then, analogously
to Section 4.1, we may define the orthogonal complement M⊥ of M by
M⊥ := {u ∈ H : (u, v) = 0 for all v ∈ M}.
The set M⊥ is a closed subspace of H. In what follows, we will need the parallelogram
identity
u + v 2 + u − v 2 = 2u2 + 2v 2 (4.2)
for all u, v ∈ H, which is an immediate consequence of the definition of the norm
via the inner product. The following theorem shows that every closed subspace of
H, that is, every subspace of H which is also closed as a set, has an orthogonal
complement.126 4 Hilbert spaces
Theorem 4.16 (Projection theorem) Let F be a closed subspace of a Hilbert space
H. Then H = F ⊕ F⊥. In words, every u ∈ H admits a unique decomposition
u = v + w, where v ∈ F and w ∈ F⊥.
The mapping P which maps u ∈ H to the vector v ∈ F for which u − v ∈ F⊥, is
called the orthogonal projection of H onto F, cf. Figure 4.2.
Fig. 4.2 The orthogonal projection of u ∈ H onto the subspace F ⊂ H.
Proof By the definiteness property (d) of the inner product, we have F ∩ F⊥ = {0};
the essential part of the projection theorem is the assertion that F + F⊥ = H. Given
u ∈ H, we need to find u0 ∈ F such that u − u0 ∈ F⊥. Consider the distance d of u
to F,
d := dist(u, F) := inf{u − v  : v ∈ F}.
We will show that this distance is realized, that is, that there exists a vector v0 ∈ F
for which d = u − v0 . For such a v0, we necessarily have u − v0 ∈ F⊥, that is,
(u − v0, z) = 0 for all z ∈ F. In particular, there is at most one minimizer. To show
this, choose an arbitrary z ∈ F with z = 1 (recall F is a vector space). Since also
v0 ∈ F, we have
u − v0 2 = d2 ≤ u − v0 − (u − v0, z)z2
= u − v0 2 − (u − v0, (u − v0, z)z)
− (u − v0, z)(z, u − v0) + |(u − v0, z)|2
= u − v0 2 − (u − v0, z) (u − v0, z)
− (u − v0, z) (u − v0, z) + |(u − v0, z)|24.4 Orthogonal projections 127
= u − v0 2 − |(u − v0, z)|2
,
whence (u − v0, z) = 0.
It remains to show that the distance is in fact realized. Choose a minimizing
sequence, that is, choose (vn)n∈N ⊂ F such that u − vn  → d as n → ∞. The
parallelogram identity (4.2) shows that
vn − vm2 = vn − u − (vm − u)2
= 2vn − u2 + 2vm − u2 − (vn − u) + (vm − u)2
= 2vn − u2 + 2vm − u2 − 4
"
"
"
vn + vm
2 − u
"
"
"
2
≤ 2vn − u2 + 2vm − u2 − 4d2
,
since vn+vm
2 ∈ F and hence  vn+vm
2 − u2 ≥ d2. This establishes that (vn)n∈N is
a Cauchy sequence, and hence it has a limit v0 := limn→∞ vn. Since F is closed,
v0 ∈ F; moreover, u − v0  = limn→∞ u − vn  = d. 
Remark 4.17 The above proof also shows that for any given u ∈ H, the orthogonal
projection Pu is the unique element of F with minimal distance to u, that is, u −
Pu = min{u − f  : f ∈ F}. In other words, the orthogonal projection is the
(unique) best approximation. 
As a consequence, we note the following very useful criterion for proving the
density of a subspace in a given Hilbert space.
Corollary 4.18 A subspace F of a Hilbert space H is dense in H if and only if
F⊥ = {0}.
Proof 1. Let F be dense in H and suppose x ∈ F⊥. Then by density of F there
is a sequence (xn)n∈N ⊂ F such that limn→∞ xn = x. But since x ∈ F⊥, we have
x2 = (x, x) = limn→∞(x, xn) = 0 and hence x = 0.
2. The closure F of F is a closed subspace of H. Suppose that F  H, then by
the projection theorem, F⊥  {0}. But since (F)
⊥ = F⊥, we also have F⊥  {0},
which completes the proof. 
In particular, we obtain a simple criterion for a sequence to be an orthonormal
basis of a complete space.
Corollary 4.19 Let {en : n ∈ I} be an orthonormal family in a Hilbert space H,
where I = N or Z. Then {en : n ∈ I} is an orthonormal basis of H if and only if, for
all v ∈ H, the condition (en, v) = 0 for all n ∈ I implies that v = 0.
Proof By definition, the orthonormal family {en : n ∈ I} is an orthonormal basis if
and only if span{en : n ∈ I} is dense in H. By Corollary 4.18, this is equivalent to
(span{en : n ∈ I})⊥ = {0}. The claim now follows from the easily verified statement
that for every subset M of H, M⊥ = (spanM)
⊥. 128 4 Hilbert spaces
4.5 Linear and bilinear forms
In this section we will introduce two efficient tools for the mathematical analysis of
partial differential equations: the theorem of Riesz–Fréchet and the Lax–Milgram
theorem. To simplify matters, and since we will almost exclusively be interested
in real partial differential equations, we will always take our underlying field to be
K = R, even though the results continue to hold with minimal modifications for
K = C.
Let H be a real Hilbert space. A linear form, also known as a (linear) functional,
is a linear mapping ϕ : H → R. It can be shown that it is continuous if and only if it
is bounded in the sense that there is a constant c ≥ 0 such that
|ϕ(v)| ≤ cv  for all v ∈ H. (4.3)
In this case we set
ϕ := inf{c ≥ 0 : |ϕ(v)| ≤ cv  for all v ∈ H};
then clearly |ϕ(v)| ≤ ϕ v  for all v ∈ H. We denote the set of all continuous
linear forms on H by H
. This set is a vector space with respect to pointwise addition
and scalar multiplication: for example, if ϕ, ψ ∈ H
, then ϕ + ψ is the mapping from
H to R defined by (ϕ + ψ)(v) = ϕ(v) + ψ(v), and so on. We refer to H as the dual
space of H.
Example 4.20 Let u ∈ H be fixed. Then ϕ(v) := (u, v), v ∈ H, defines a continuous
linear form ϕ on H, and moreover ϕ = u. 
Proof The linearity of ϕ follows from the bilinearity of the inner product. The
Cauchy–Schwarz inequality |ϕ(v)| = |(u, v)| ≤ u v , v ∈ H, implies that ϕ is
continuous and ϕ≤u. But u2 = (u, u) = ϕ(u) implies that ϕ≥u. 
The following theorem shows that every continuous linear form on H can be
represented by taking the inner product with a given element of H, as in Example
4.20.
Theorem 4.21 (Riesz–Fréchet) Let ϕ : H → R be a continuous linear form. Then
there exists a unique u ∈ H such that ϕ(v) = (u, v) for all v ∈ H.
Proof We establish existence and uniqueness of such a u ∈ H separately.
Uniqueness: This is a consequence of the positive definiteness of the inner prod￾uct: let u1, u2 ∈ H be such that (u1, v) = (u2, v) for all v ∈ H. With the choice
v = u1 − u2, this means that u1 − u2 2 = (u1 − u2, v) = 0, whence u1 = u2.
Existence: If ϕ = 0 (that is, ϕ(v) = 0 for all v ∈ H), then we choose u = 0. We
may thus assume that ϕ  0. Then its kernel ker ϕ := {v ∈ H : ϕ(v) = 0} is not equal
to H. Since ker ϕ is a closed subspace of H, it follows from the projection theorem
that (ker ϕ)
⊥  {0}. We may thus choose u1 ∈ (ker ϕ)
⊥ with u1  0; without loss4.5 Linear and bilinear forms 129
of generality, we suppose that ϕ(u1) = 1 (otherwise we multiply u1 by a suitable
scalar). For v ∈ H arbitrary, by definition of (ker ϕ)
⊥ we have v − ϕ(v)u1 ∈ ker ϕ.
Hence v − ϕ(v)u1 is orthogonal to u1, that is, (v − ϕ(v)u1, u1) = 0. It follows that
(v, u1) = ϕ(v)u1 2. If we choose u := u1 −2u1, then we conclude that ϕ(v) =
u1 −2(v, u1) = (v, u) = (u, v) for all v ∈ H. 
Remark 4.22 This theorem is also sometimes known as the Riesz representation
theorem; the unique element u = u(ϕ) is sometimes called the Riesz representative of
ϕ. There are however other representation theorems named after Riesz; see Theorem
7.10. To avoid ambiguity, we will always use the name theorem of Riesz–Fréchet for
Theorem 4.21 (which is also historically justified). 
Next we wish to generalize the theorem of Riesz–Fréchet to non-symmetric
bilinear forms; this will greatly expand the number of partial differential equations to
which the theorem can be applied to establish existence and uniqueness of solutions.
For the generalization, we need to replace the inner product by a more general bilinear
form. We will usually denote the domain of definition of such a bilinear form by V;
in what follows, we will thus take V to be a real Hilbert space. A bilinear form is
thus a mapping a : V × V → R such that
a(αu1 + βu2, v) = αa(u1, v) + βa(u2, v),
a(v, αu1 + βu2) = αa(v, u1) + βa(v, u2)
for all u1, u2, v ∈ V and all α, β ∈ R. We thus demand that the mappings a(·, v) and
a(v, ·) : V → R be linear for each v ∈ V. A bilinear form is said to be continuous if
limn→∞ a(un, vn) = a(u, v) whenever limn→∞ un = u and limn→∞ vn = v. One sees,
much as in Theorem A.1 (see Exercise 4.4), that a(·, ·) is continuous if and only if
there exists a constant C ≥ 0 such that
|a(u, v)| ≤ Cu v  for all u, v ∈ V. (4.4)
We shall use the abbreviation
a(u) := a(u, u), u ∈ V. (4.5)
A bilinear form is called coercive if there exists an α > 0 such that
a(u) ≥ αu2 for all u ∈ V, (4.6)
and symmetric if
a(u, v) = a(v, u) for all u, v ∈ V.
The following theorem will play a central role in the study of partial differential
equations in the sequel.
Theorem 4.23 (Lax–Milgram theorem) Let a : V × V → R be a continuous and
coercive bilinear form, and let ϕ ∈ V
. Then there is a unique u ∈ V such that130 4 Hilbert spaces
a(u, v) = ϕ(v) for all v ∈ V. (4.7)
Proof Once again we treat existence and uniqueness separately.
Uniqueness: This is a consequence of the coercivity. Let u1, u2 ∈ V be such that
a(u1, v) = a(u2, v) for all v ∈ V. Then a(u1 − u2, v) = 0 for all v ∈ V. If we choose
v = u1 − u2, then we arrive at a(u1 − u2) = 0. Since a is coercive, it follows that
u1 − u2 = 0.
Existence: Let u ∈ V be fixed. Then g(v) := a(u, v), v ∈ V defines a continuous
linear form on V. By the theorem of Riesz–Fréchet (Theorem 4.21) there is exactly
one Tu ∈ V such that a(u, v) = (Tu, v) for all v ∈ V. By the uniqueness of Tu and
the bilinearity of a(·, ·), the mapping T : V → V is linear. Since a(·, ·) is continuous,
it follows that |(Tu, v)| = |a(u, v)| ≤ Cu v  for all v ∈ V, where C is as in
(4.4). Hence Tu ≤ Cu (see Theorem A.1). The operator T : V → V is thus
continuous, with T  ≤ C.
The coercivity of a(·, ·) now implies that (Tu, u) ≥ αu2 for all u ∈ V, for some
α > 0. The Cauchy–Schwarz inequality implies that
αu2 ≤ a(u) = (Tu, u)≤Tu u
and hence
αu≤Tu for all u ∈ V. (4.8)
We now wish to show that T is surjective. To this end, we consider the range of T,
defined as
T(V) := {Tu : u ∈ V}. (4.9)
SinceT is linear,T(V) is a subspace ofV. We claim thatT(V) is closed. Given un ∈ V,
n ∈ N, assume that v := limn→∞ Tun exists; we have to show that v ∈ T(V). By (4.8)
and the linearity of T, we have αun − um ≤Tun − Tum . It follows that (un)n∈N
is a Cauchy sequence in the Hilbert space V, and hence the limit u := limn→∞ un
exists. Moreover,
Tu = lim
n→∞ Tun = v,
so that v ∈ T(V). Now by the projection theorem (Theorem 4.16), it follows that
T(V)⊕(T(V))⊥ = V. Suppose that u ∈ (T(V))⊥, so that (Tv, u) = 0 for all v ∈ V and
in particular for v = u. Then αu2 ≤ (Tu, u) = 0, whence u = 0. We have shown
that T(V)
⊥ = {0} and hence that T(V) = V, that is, T is surjective.
Now let ϕ ∈ V
. By the theorem of Riesz–Fréchet (Theorem 4.21) there is a
w ∈ V such that ϕ(v) = (w, v) for all v ∈ V. Since T is surjective, there exists u ∈ V
such that Tu = w. Hence, by definition of T, a(u, v) = (Tu, v) = (w, v) = ϕ(v) for all
v ∈ V. 4.5 Linear and bilinear forms 131
If the bilinear form in the Lax–Milgram theorem is also symmetric, then (4.7)
can be solved as a minimization problem. In such cases we speak of a variational
problem.
Theorem 4.24 (Variational Lax–Milgram) Let a : V × V → R be a continuous,
coercive and symmetric bilinear form, and let ϕ ∈ V
, u ∈ V. Then the following are
equivalent:
(i) We have a(u, v) = ϕ(v) for all v ∈ V.
(ii) For the functional J(v) := 1
2 a(v) − ϕ(v), we have J(u) ≤ J(v) for all v ∈ V.
Since the vector u is unique by Theorem 4.23, we may write u = arg minv∈V J(v).
Proof Statement (ii) is equivalent to J(u) ≤ J(u + w) for all w ∈ V (just choose
v = u + w ∈ V). Since
J(u + w) = 1
2
a(u + w) − ϕ(u + w)
= 1
2
a(u) + a(u, w) +
1
2
a(w) − ϕ(u) − ϕ(w),
is (ii) thus equivalent to
0 ≤ a(u, w) +
1
2
a(w) − ϕ(w) (4.10)
for all w ∈ V. By replacing w by tw for t > 0 in (4.10) we obtain
0 ≤ t a(u, w) +
1
2
t
2a(w) − tϕ(w).
Dividing by t yields 0 ≤ a(u, w) + 1
2 ta(w) − ϕ(w) for all t > 0 and w ∈ V. If we now
let t tend to 0, then we arrive at
0 ≤ a(u, w) − ϕ(w) for all w ∈ V.
If we now replace w by −w, then we obtain the reverse inequality 0 ≥ a(u, w) − ϕ(w)
for all w ∈ V. Thus a(u, w) = ϕ(w) for all w ∈ V. We have shown that (ii) implies
statement (i). Conversely, inequality (4.10) follows from (i) since a(w) ≥ 0 for all
w ∈ V. But (4.10) is equivalent to (ii), as we have seen. 
The inner product on V is an important special case of a continuous, coercive
and symmetric bilinear form on V. As such, the Lax–Milgram theorem contains the
theorem of Riesz–Fréchet as a special case. Theorem 4.24 is, among other things,
the basis for a number of efficient numerical methods for solving linear systems of
equations with symmetric positive definite matrices such as the conjugate gradient
method.
Remark 4.25 The coercivity of a and the Lax–Milgram theorem also immediately
yield continuous dependence on the data. Indeed, let u  0 be a solution of (4.7),
then αu2 ≤ a(u, u) = ϕ(u)≤ϕ u, that is,132 4 Hilbert spaces
u ≤ 1
α ϕ,
and this estimate also holds trivially for u = 0. For this reason, when it comes to the
well-posedness of variational problems, we will restrict ourselves to investigating
existence and uniqueness of solutions. 
We have already seen that the Cauchy–Schwarz inequality is an essential property
of the inner product. We will now give a second, less geometric proof of this
inequality in a general situation, which will be of use to us later.
Theorem 4.26 (Generalized Cauchy–Schwarz inequality) Let a : V × V → R be
bilinear and symmetric, and suppose a(u) ≥ 0 for all u ∈ V. Then for all u, v ∈ H
we have
|a(u, v)| ≤ a(u)
1
2 a(v)
1
2 .
Proof Let u, v ∈ H, then 0 ≤ a(u − tv) = a(u) − 2ta(u, v) + t
2a(v) for all t ∈ R. If
a(v) = 0, then it follows that 2ta(u, v) ≤ a(u) for all t ∈ R and so a(u, v) = 0. Hence
this claim is true in this case. If a(v)  0, then we make the special choice t = a(u,v)
a(v)
and obtain
0 ≤ a(u) − 2
a(u, v)
2
a(v) + a(u, v)
2
a(v) = a(u) − a(u, v)
2
a(v) .
Hence a(u, v)
2 ≤ a(u) a(v). 
4.5.1* Extensions and generalizations
So far, we have seen that certain variational problems of the form (4.7), associated with a continuous
and coercive bilinear form a : V × V → R, are well-posed. The converse of this statement is,
however, not true; that is, there are well-posed variational problems with more general bilinear
forms, as we will now see.
Let V and W be two real Hilbert spaces with norms ·V and ·W , respectively. We
will call V the trial space and W the test space. Analogously to what we did above, we will call
b : V × W → R a bilinear form if
b(αv1 + βv2, w) = αb(v1, w) + βb(v2, w), b(v, αw1 + βw2) = αb(v, w1) + βb(v, w2),
for all v1, v2, v ∈ V, w1, w2, w ∈ W and α, β ∈ R. Analogously to (4.4), we will say the bilinear
form b is continuous (or bounded) if there exists a C > 0 such that4.5 Linear and bilinear forms 133
|b(v, w)| ≤ C v V wW for all v ∈ V and w ∈ W.
Obviously, coercivity in the sense of (4.6) can have no direct equivalent here if V and W are
different. However, we can give a more general condition which is actually weaker than coercivity
in the case V = W. We will say that the bilinear form b satisfies an inf-sup condition if there exists
a constant β > 0 (called an inf-sup constant) such that
sup
w∈W
b(v, w)
wW
≥ β v V for all v ∈ V. (4.11)
Obviously, if V = W, then every coercive bilinear form satisfies (4.11) with α = β.
Theorem 4.27 (Banach–Nečas) Let b : V × W → R be a continuous bilinear form. Then the
following statements are equivalent:
(i) For each ϕ ∈ W there exists a unique u ∈ V such that
b(u, w) = ϕ(w) for all w ∈ W. (4.12)
(ii) (a) The condition (4.11) holds, and
(b) for all 0  w ∈ W there exists v ∈ V with b(v, w)  0. (4.13)
Proof We first prove that (ii) implies (i). Analogously to the proof of Theorem 4.23 we obtain a
continuous linear mapping T : V → W such that (Tv, w)W = b(v, w) and
Tv W ≤ C v V (4.14)
for all v ∈ V. The inf-sup condition (4.11) implies that
β v V ≤ sup
w∈W
b(v, w)
wW
= sup
w∈W
(Tv, w)W
wW
= Tv W for all v ∈ V,
whence T is injective and has closed image T(V) = {Tv : v ∈ V } (cf. the proof of Theorem 4.23).
To show that T(V) = W we apply Corollary 4.18. Let w0 ∈ (T(V))⊥, then b(v, w0) = (Tv, w0) = 0
for all v ∈ V. By (4.13) we have that w0 = 0; from Corollary 4.18 we obtain that T(V) = W. Hence
T is surjective and (i) is proved. Conversely, suppose that (i) holds. Then T is invertible and T −1 is
continuous, that is, there exists some β > 0 such that T −1wV ≤ β−1 wW for all w ∈ W. It
follows that, for all v ∈ V,
sup
w∈W
b(v, w)
wW
= sup
w∈W
(Tv, w)W
wW
= Tv W ≥ β v V ,
that is, (4.11) holds. Since T is, by assumption, an isomorphism, for each 0  w ∈ W there is a
0  v ∈ V such that Tv = w and hence b(v, w) = (Tv, w)W = (w, w)W = w2
W  0. Thus
(4.13) holds. 
Remark 4.28 (a) The above theorem establishes that the well-posedness of variational problems
for continuous bilinear forms is equivalent to the conditions (4.11) and (4.13). In this sense,
the Lax–Milgram theorem may be regarded as a special case of the theorem of Banach–Nečas.
(b) The name “inf-sup condition” comes from the following equivalent form of (4.11): there exists
a β > 0 such that
inf
v∈V sup
w∈W
b(v, w)
wW v V
≥ β. (4.11’)134 4 Hilbert spaces
(c) Let u ∈ V be the unique solution of (4.12); then the inf-sup condition implies that β u V ≤
supw∈W
b(u, w)
wW = supw∈W
ϕ(w)
wW = ϕW , that is, it implies the continuous dependence on
the data.
(d) Let v ∈ V. Then by the theorem of Riesz–Fréchet (Theorem 4.21) there exists a unique
sv ∈ W such that (sv, w)W = b(v, w) for all w ∈ W, where we have written (·, ·)W for the
inner product of the space W. Then
sv W = sup
w∈W
(sv, w)W
wW
= sup
w∈W
b(v, w)W
wW
,
that is, sv = arg supw∈W
b(v,w)W
wW . Hence we also call this sv the supremizer of v with respect
to b. It plays an important role in the numerics of variational problems (see also Remark 8.39).
(e) The above proof also shows that if the inf-sup condition (4.11) holds, then the mapping T is
surjective if and only if (4.13) holds. 
We now wish to introduce a second generalization of the Lax–Milgram theorem in the form of
Theorem 4.24. The following theorem due to J.-L. Lions may be considered “dual” to the theorem
of Banach–Nečas, Theorem 4.27. It has the advantage that one of the two spaces need not be
complete.
Theorem 4.29 (J.-L. Lions) Let V be a Hilbert space and let W be an inner product space. Let
b : V ×W → R be bilinear, and assume b(·, w) ∈ V for all w ∈ W. Then the following statements
are equivalent:
(i) For each ϕ ∈ W there exists a u ∈ V such that b(u, w) = ϕ(w) for all w ∈ W.
(ii) There exists a β > 0 such that
sup
v∈V
|b(v, w)|
v V
≥ β wW (4.15)
for all w ∈ W.
Proof We first show that (ii) implies (i). Let w ∈ W. Since b(·, w) ∈ V
, the theorem of Riesz–
Fréchet (Theorem 4.21) yields the existence of a unique Tw ∈ V such that b(v, w) = (v, Tw)V for
all v ∈ V. HenceT : W → V is linear, and the assumption (ii) says exactly that TwV ≥ β wW
for all w ∈ W. This means that T is injective and the inverse mapping T −1 : T(W) → W satisfies
the estimate T −1v W ≤ 1
β v V for all v ∈ T(W), the image of W under T. By Theorem A.2,
T −1 has a unique continuous extension R : T(W) → W˜ , where W˜ denotes the completion of W
(see Exercise 4.4).
Let u ∈ W. Then the desired relation ϕ(w) = b(u, w) = (u, Tw)V for all w ∈ W holds if and
only if ϕ(T −1v) = (u, v)V for all v ∈ T(W). This, in turn, is equivalent to ϕ(Rv) = (u, v)V for all
v ∈ T(W). Since ϕ ◦ R ∈ (T(W))
, the theorem of Riesz–Fréchet yields a unique u ∈ T(W) for
which this relation is satisfied.
We now show that (i) implies (ii). For this direction we need the uniform boundedness principle
in the following form: if wn ∈ W, n ∈ N such that supn∈N |f (wn)| < ∞ for all f ∈ W
, then
supn∈N wn W < ∞ (see, for example, [2, Thm. 7.2]).
Suppose now that (i) holds and assume that (ii) is false. Then there exist wn ∈ W such
that wn W = 1 but |b(·, wn) V  < 1
n . Let f ∈ W
. By (i) there exists a uf ∈ V such that
f (wn) = b(uf , wn), and hence |f (n wn)| = n |b(uf , wn)| < n uf V 1
n = uf V for all n ∈ N.
This means that supn∈N |f (n wn)| < ∞ for all f ∈ W and so, by the uniform boundedness
principle, the sequence (n wn)n∈N is bounded, a contradiction to wn W = 1. 
Remark 4.30 (a) In general, for each ϕ ∈ W there can be multiple u ∈ V which satisfy (i).
Uniqueness is equivalent to b separating the space (that is, for each 0  v ∈ V there is a4.6 Weak convergence 135
w ∈ W such that b(v, w)  0; or in other words, W is not in the orthogonal complement of V
with respect to b). This is exactly dual to the separation property (4.13) which appears in the
theorem of Banach–Nečas. The condition (4.15) means that
inf
w∈W sup
v∈V
|b(v, w)|
v V wW
≥ β > 0,
and so it is dual to (4.11).
(b) In the theorem of Lions we only require continuity in the first variable: b(·, w) ∈ V for all
w ∈ W; the roles of V and W with respect to inf and sup are swapped.
(c) If W → V, that is, if W ⊂ V and there exists a constant c > 0 such that wV ≤ c wW
for all w ∈ W, then condition (ii) in Theorem 4.29 is satisfied if there exists an α > 0 such
that b(w, w) ≥ α w2
W for all w ∈ W, that is, (ii) is satisfied if b is coercive on W.
Proof. Let w ∈ W and set v := 1
cwW w. Then v V ≤ c v W = 1, and so supv∈V
|b(w,v)|
v V ≥
1
cwW b(w, w) ≥ 1
cwW α w2
W = α
c wW . 
4.6 Weak convergence
One of the most important results of elementary analysis states that in any finite￾dimensional Hilbert space, every bounded sequence has a convergent subsequence.
In infinite-dimensional spaces, however, this statement is not true (see Example 4.32
below). Fortunately, it continues to hold if we use a weaker notion of convergence.
This is the content of Theorem 4.35, the main theorem of this section. Here we will
always take H to be a real Hilbert space.
Definition 4.31 (Weak convergence) Let (un)n∈N be a sequence in H.
(a) Let u ∈ H be given. We say that (un)n∈N converges weakly to u, and write
w − limn→∞un = u or un  u, n → ∞,
if limn→∞(un, v) = (u, v) for all v ∈ H.
(b) We say that the sequence (un)n∈N is weakly convergent if there exists some
u ∈ H such that w − limn→∞un = u. 
We first observe that weak limits are always unique: if for a further w ∈ H,
w = w − limn→∞un = u,
then (u, v) = (w, v) for all v ∈ H. If we choose v = u − w, then we see that
u − w2 = (u − w, v) = 0 and so u − w = 0. It is immediate that every convergent
sequence also converges weakly, and to the same limit. The converse, however, is
false in general, as the following example shows.
Example 4.32 Let {en : n ∈ N} be an orthonormal basis of H; then
∞
n=1
|(v, en)|2 = v 2 for all v ∈ H.136 4 Hilbert spaces
Since the terms of a convergent series tend to zero, it follows from this identity that
en  0 as n → ∞. But since en − em 2 = en 2 + em 2 = 2 for n  m by the
orthogonality of the sequence, (en)n∈N cannot have a convergent subsequence, even
though it is weakly convergent. 
Convergence in norm does however follow from weak convergence under an
additional condition; we have the following useful criterion:
Theorem 4.33 Let (un)n∈N be a sequence in H and u ∈ H. Then the following
statements are equivalent:
(i) limn→∞ un = u.
(ii) un  u and un →u as n → ∞.
Proof We will only show that (ii) implies (i), since the other implication is obvious.
We thus suppose that un  u and un →u as n → ∞. Then
un − u2 = (un − u, un − u) = un 2 − (u, un)−(un, u) + u2 → 0,
that is, (i) holds. 
The following lemma is often useful when one wishes to prove weak convergence.
Lemma 4.34 Let (un)n∈N be a bounded sequence in H, and assume that there exists
a total subset M of H, such that 
(un, w)
	
n∈N converges for all w ∈ M. Then (un)n∈N
is weakly convergent.
Proof It follows directly from our assumptions that
ϕ0(w) := lim
n→∞(un, w)
exists for all w ∈ F := spanM. Let c ≥ 0 be such that un  ≤ c for all n ∈ N;
then |ϕ0(w)| ≤ cw for all w ∈ F. The mapping ϕ0 : F → R is thus linear and
continuous. Since F is dense in H, ϕ0 has an extension ϕ ∈ H (see Theorem A.2).
By the theorem of Riesz–Fréchet (Theorem 4.21) there exists a u ∈ H such that
ϕ(v) = (u, v) for all v ∈ H. If v ∈ F, then
(u, v) = ϕ(v) = ϕ0(v) = lim
n→∞(un, v).
Now let v ∈ H and ε > 0 be arbitrary; then there exists w ∈ F such that
v − w ≤ ε. Since limn→∞(un, w) = (u, w) it follows that
lim sup
n→∞
|(un, v)−(u, v)| = lim sup
n→∞
|(un − u, v − w) + (un − u, w)|
≤ lim sup
n→∞
|(un − u, v − w)|
≤ lim
n→∞ un − u v − w
≤ (c + u)ε.4.6 Weak convergence 137
Since ε > 0 was arbitrary, limn→∞ |(un, v)−(u, v)| = 0, that is,
lim
n→∞(un, v) = (u, v),
which proves the assertion. 
The proof of the following theorem rests on a diagonal argument, an important
and frequently used method of proof in analysis.
Theorem 4.35 Let H be a real, separable Hilbert space. Then every bounded se￾quence in H admits a weakly convergent subsequence.
Proof We start with three preliminary remarks, to which we will refer during the
later course of the proof:
1. Every subsequence of a convergent sequence is itself convergent; moreover, it
converges to the same limit as the original sequence.
2. We may always alter finitely many terms in a sequence without affecting its
limit.
3. Every bounded sequence in R admits a convergent subsequence.
Now let un ∈ H be such that un  ≤ c for all n ∈ N. We wish to find a weakly
convergent subsequence of (un)n∈N. Let {wp : p ∈ N} be a total sequence in H. By
Lemma 4.34, it is sufficient to find a subsequence (unk )k ∈N of (un)n∈N such that
lim
k→∞(unk , wp)
exists for each p ∈ N. Now since |(un, wp)| ≤ cwp  for all n ∈ N, the sequence

(un, wp)
	
n∈N admits a convergent subsequence, for each p ∈ N.
We will construct a subsequence which works for all p ∈ N simultaneously. Let
p = 1, then there is a subsequence (un(1,k))k ∈N of (un)n∈N for which (un(1,k), w1)
converges as k → ∞. Here n(1, k) ∈ N is an index such that n(1, k) < n(1, k + 1) for
all k ∈ N. Now there is a subsequence (un(2,k))k ∈N of (un(1,k))k ∈N such that
lim
k→∞(un(2,k), w2)
exists. We proceed in this fashion, finding indices n(, k) ∈ N for , k ∈ N such that
(a) n(, k) < n(, k + 1);
(b) (n( + 1, k))k ∈N is a subsequence of (n(, k))k ∈N, that is, n( + 1, k)∈{n(, m):
m ∈ N} for all k ∈ N; in particular
(c) n( + 1, k) ≥ n(, k) for all k ∈ N;
(d) limk→∞(un(p,k), wp) =: cp exists for all p ∈ N. 
Set nk := n(k, k), then by (a) and (c) we have nk < n(k, k+1) ≤ n(k+1, k+1) = nk+1.
Now let p ∈ N. Since also nk ∈ {n(p, m) : m ∈ N} for all k ≥ p by (b), it finally
follows from (d) that limk→∞(unk , wp) = cp. 
Theorem 4.35 remains true if H is not separable (since each sequence is con￾tained in a separable, closed subspace). We shall finish by mentioning the following
theorem, whose proof we will however omit.138 4 Hilbert spaces
Theorem 4.36 Every weakly convergent sequence is bounded.
Proof The proof is based on the Baire category theorem and for it we refer to books
on functional analysis, such as [2, Remark 8.3(5)]. 
4.7 Continuous and compact operators
A linear mapping T is continuous if and only if the image of the unit sphere under
T is bounded (see Appendix A.1). If this image is even relatively compact (that is,
its closure is compact), then we call T compact. We have thus made the following
definition:
Definition 4.37 Let E and F be normed spaces. A linear mapping T : E → F is
called compact if the following condition is satisfied: for any bounded sequence
(un)n∈N ⊂ E there exists a subsequence (unk )k ∈N such that (Tunk )k ∈N converges in
F. 
Here we wish to consider such mappings in the special situation of Hilbert spaces,
where we have both convergence in norm and weak convergence. We will assume
throughout that H1 and H2 are real or complex separable Hilbert spaces.
Theorem 4.38 Let T : H1 → H2 be linear and continuous. Then T is weakly
continuous, that is, un  u in H1 implies Tun  Tu in H2.
Proof Let w ∈ H2, then ϕ(v) := (Tv, w)H2 defines a continuous linear form on
H1. By the theorem of Riesz–Fréchet, there is thus a unique T∗w ∈ H1 such that
(Tv, w)H2 = (v, T∗w)H1 for all v ∈ H1. If un  u in H1, then
(Tun, w)H2 = (un, T∗w)H1 → (u, T∗w)H1 = (Tu, w)H2 .
Since w ∈ H2 was arbitrary, it follows that Tun  Tu in H2. 
The converse of Theorem 4.38 is also true: every weakly continuous operator is
also continuous; see Exercise 4.2. Now we can describe compact operators as being
exactly those operators which “improve” convergence in the following sense.
Theorem 4.39 Let T : H1 → H2 be linear. Then the following statements are
equivalent:
(i) T is compact.
(ii) un  u in H1 implies Tun → Tu in H2.
For the proof we will use the following lemma which, despite looking like splitting
hairs, turns out to be extremely useful.
Lemma 4.40 Let (un)n∈N be a sequence in a normed space E and let u ∈ E. Then
the sequence converges to u if and only if every subsequence of (un)n∈N itself has a
subsequence which converges to u.4.8 The spectral theorem 139
Proof If the sequence does not converge to u, then there exists an ε > 0 and a
subsequence (unk )k ∈N such that unk − u ≥ ε for all k ∈ N. Hence no subsequence
of this subsequence can converge to u. The converse implication is clear. 
Proof (of Theorem 4.39) That (i) implies (ii) is seen as follows. Assume that un 
u in H1, then (un)n∈N is bounded by Theorem 4.36. By assumption there is a
subsequence (unk )k ∈N such that the limit (in norm) v := limk→∞ Tunk exists. Since T
is also weakly continuous, v = Tu. Lemma 4.40 now implies that limn→∞ Tun = Tu.
Now assume (ii). Let (un)n∈N be a bounded sequence in H1; then by Theorem 4.35
it has a weakly convergence subsequence (unk )k ∈N. By assumption, the sequence
(Tunk )k ∈N converges in norm, meaning that T is compact. 
4.8 The spectral theorem
We will often be able to write partial differential equations as operator equations of
the form Au = f , where A is a linear operator on some Hilbert space H and f ∈ H
is given. The desired unknown is u ∈ H. Most commonly H = L2(Ω) for an open
set Ω ⊂ Rd, and A is a differential operator. In this case, A cannot be defined for all
functions in H, but only for those in a certain subspace D(A) of H, the domain of
definition of A. More precisely, one should choose a set of functions in H which are
differentiable sufficiently many times to allow the application of A. We thus wish to
speak of linear mappings A : D(A) → H, where D(A) is a subspace of H; we will
capture this notion in the following definition.
Definition 4.41 Let H be a real Hilbert space.
(a) An operator on H is a pair (A, D(A)), where D(A) ⊆ H is a subspace of H and
A : D(A) → H is a linear mapping. We call D(A) the domain of A. To simplify
notation, in this case we will also speak of A as being an operator on H; to this
operator belong the domain D(A) and the linear mapping A : D(A) → H.
(b) Two operators A and B on H are equal (and we write A = B) if D(A) = D(B)
and Au = Bu for all u ∈ D(A).
(c) If A and B are operators on H, then we write A ⊂ B and say that A is a restriction
of B, or equivalently that B is an extension of A, if D(A) ⊂ D(B) and Au = Bu
for all u ∈ D(A). 
What we are calling an operator here, is often known as an unbounded operator,
although the word “unbounded” is to be interpreted as “not necessarily bounded”.
Thus an unbounded operator on H is simply a linear mapping whose domain is not
(necessarily) the whole of H. Often, the domain of an operator A is dense in H: we
then say that A is densely defined.
Example 4.42 Let H = L2(0, 1). We can consider the second derivative on (0, 1)
with various domains, for example:140 4 Hilbert spaces
(a) Let D(A0) := {u ∈ C2([0, 1]) : u(0) = u(1) = 0} and A0u := −u for all
u ∈ D(A0).
(b) Let D(A1) := {u ∈ C2([0, 1]) : u
(0) = u
(1) = 0} and A1u:= − u for all
u ∈ D(A1).
(c) Let D(B) := C2([0, 1]) and Bu := −u for all u ∈ D(B).
Then A0 ⊂ B, A1 ⊂ B, and A0  A1. The rule defining the linear mapping is
always the same; nevertheless, all three operators are different. Dirichlet boundary
conditions are encoded into the domain of A0, while Neumann boundary conditions
are encoded into the one of A1. We will return to this subject in the following chapter.

Let A be an operator on H. Our goal will be to analyze equations of the form
Au = f . (4.16)
In terms of the solvability of (4.16), there are three properties for which one may
reasonably hope:
1. Existence: for each f ∈ H there is a solution u ∈ D(A) of (4.16).
2. Uniqueness: for each f ∈ H there is at most one solution u ∈ D(A).
3. Continuity of the solution with respect to the given data: if fn, f ∈ H and
un, u ∈ D(A) are such that Aun = fn and Au = f , and fn → f in H, then we
must have un → u in H.
The existence statement is equivalent to the surjectivity of the mapping
A : D(A) → H;
uniqueness means that A is injective. Now since A is linear, the kernel (or null space)
of A, defined by
ker A := {u ∈ D(A) : Au = 0},
is a subspace of D(A), and A is injective if and only if ker A = {0}. Existence and
uniqueness together are thus equivalent to the bijectivity of the mapping A : D(A) →
H. In this case A−1 : H → D(A) is also linear, and since D(A) ⊂ H, we may consider
A−1 as a mapping from H to H. The requirement of continuity in 3. corresponds
exactly to the continuity of this mapping. If these three conditions are satisfied, then
we shall call the operator A invertible: to summarize, this means that A : D(A) → H
is bijective and A−1 : H → H is continuous. Following Hadamard, we say that the
problem (4.16) is well-posed if all three conditions listed above, namely existence,
uniqueness and continuous dependence on the data, are satisfied. Thus the problem
(4.16) is well-posed if and only if the operator A is invertible.
Remark 4.43 Put slightly differently, an operator A on H is invertible if the following
two conditions hold:
(a) for each f ∈ H there is exactly one u ∈ D(A) such that Au = f , and
(b) there exists a c ≥ 0 such that the a priori estimate u ≤ c  f  holds. 4.8 The spectral theorem 141
The following observation is useful for helping to understand the role of the
domain.
Remark 4.44 Let A be an invertible operator and let B be a restriction or an extension
of A. If B is invertible, then A = B. 
Proof 1. Let B ⊂ A. If B is surjective, then we claim that B = A. In fact, if
u ∈ D(A), then by surjectivity of B there exists a v ∈ D(B) such that Bv = Au. Since
A is injective and Av = Bv = Au, it follows that u = v.
2. Let A ⊂ B. If B is injective, then we claim that A = B. This time, let u ∈ D(B),
then by surjectivity of A there exists a v ∈ D(A) such that Av = Bu. Since B is
injective, as before we get u = v ∈ D(A). 
If λ ∈ R, then we may define the operator A − λI by
(A − λI)u := Au − λu, D(A − λI) := D(A).
Here I stands for the identity mapping. We say that λ ∈ R is an eigenvalue of A if
A − λI is not injective, that is, if there exists some u ∈ D(A) such that u  0 and
Au = λu.
Of particular interest are operators associated with a bilinear form. Here we will
continue to assume that H is a real Hilbert space, and we will denote its inner product
by (·, ·)H and its norm by ·H . Let V be another Hilbert space with inner product
(·, ·)V and norm ·V . We say that V is continuously imbedded in H and write
V → H if V ⊂ H and there exists a constant c ≥ 0 such that
uH ≤ cuV, u ∈ V. (4.17)
This means exactly that the identity mapping of V into H is continuous. We also
wish to assume that V is dense in H.
Now let a : V × V → R be a continuous bilinear form; thus there exists a C > 0
such that
|a(u, v)| ≤ CuV v V for all u, v ∈ V. (4.18)
We wish to associate an operator A on H with the form a(·, ·). To this end we first
define the domain of A by
D(A) := {u ∈ V : ∃ f ∈ H such that a(u, v) = ( f, v)H for all v ∈ V}, (4.19)
a subspace of H. If u ∈ D(A), then there exists an f ∈ H such that
a(u, v) = ( f, v)H for all v ∈ H. (4.20)
This f is unique. Indeed, if ˜f ∈ H is some other vector for which a(u, v) = ( ˜f, v)H
for all v ∈ V, then ( f − ˜f, v)H = 0 for all v ∈ V. Since V is dense in H, this continues
to hold for all v ∈ H. In particular,  f − ˜f 2
H = ( f − ˜f, f − ˜f )H = 0 and thus f = ˜f .142 4 Hilbert spaces
If u ∈ D(A) and if f ∈ H is the unique element of H for which (4.20) holds, then
we set Au = f . This defines a linear mapping A : D(A) → H; we call A the operator
associated with a(·, ·) on H. We shall now consider the equation
Au = f . (4.21)
Given f ∈ H, we seek u ∈ D(A) for which (4.21) holds. By the Lax–Milgram
theorem we have the following result:
Theorem 4.45 If a(·, ·) is continuous and coercive, then the problem (4.21) is well￾posed, that is, the operator A is invertible.
Proof Let α > 0 be such that a(u) ≥ αu2
V for all u ∈ V, and let f ∈ H. Then
F(v) := ( f, v)H defines a continuous linear form on V, as follows from the continuity
of the imbedding of V in H.
By the Lax–Milgram theorem there exists exactly one u ∈ V such that a(u, v) =
( f, v)H for all v ∈ V. This means exactly that u ∈ D(A) and Au = f . We have thus
shown that A is bijective. Now (4.17) also implies that
α
c u2
H ≤ αu2
V ≤ a(u, u) = ( f, u)H ≤  f H uH,
and hence uH ≤ c
α  f H . Since u = A−1 f , this shows that A−1  ≤ c
α . 
In place of A we can also consider the operator A+λI for λ ∈ R. It is an immediate
consequence of the definition that A + λI is associated with the bilinear form
aλ : V × V → R, aλ(u, v) := a(u, v) + λ(u, v)H .
This form is continuous since V is continuously imbedded in H. We say that the
bilinear form a(·, ·) is H-elliptic if there exist ω ∈ R and α > 0 such that
a(u) + ωu2
H ≥ αu2
V for all u ∈ V. (4.22)
We see that the form a(·, ·) is H-elliptic if and only if there is an ω ∈ R such that
aω(·, ·) is coercive. In this case, aλ(·, ·) is also coercive, and A + λI is invertible, for
all λ ≥ ω. The inequality (4.22) is also known as Gårding’s inequality, especially in
numerical mathematics.2
Now we make the additional assumption that the form is symmetric. One sees
easily that the associated operator A is then also symmetric, that is,
(Au, v)H = (u, Av)H for all u, v ∈ D(A).
If V = H and dim H < ∞, then we know from linear algebra that H admits an
orthonormal basis consisting of eigenvectors of A. We now wish to generalize this
2 This appellation is somewhat misleading since (4.22) is not a proved inequality but rather an
assumption.4.8 The spectral theorem 143
result to our infinite-dimensional setting. To do so, we need a stronger relationship
between V and H than just continuity of the imbedding. We say that V is compactly
imbedded in H, and write
V c
→ H,
if V ⊂ H and the imbedding u → u of V into H is compact. By Theorem 4.39 this
means exactly that
un  u in V implies un → u in H.
Under this additional assumption H admits an orthonormal basis of eigenvectors
of A. This is the statement of the following theorem, which also gives a precise
description of the domain of A in terms of the orthonormal basis.
Theorem 4.46 (Operator version of the spectral theorem)
Let dim H = ∞ and let V be compactly and densely imbedded in H. Let a : V ×V →
R be a symmetric, continuous and H-elliptic bilinear form. Then there exist an
orthonormal basis {en : n ∈ N} of H and numbers λn ∈ R with
λn ≤ λn+1, n ∈ N and lim
n→∞ λn = ∞,
such that the operator A associated with a(·, ·) is given by
D(A) =

u ∈ H :
∞
n=1
λ2
n(u, en)
2
H < ∞
!
, (4.23)
Au =
∞
n=1
λn(u, en)H en, u ∈ D(A). (4.24)
Before we prove the theorem, we first wish to state a few consequences.
Remark 4.47 Under the assumptions of the spectral theorem, we have:
(a) en ∈ D(A) and Aen = λnen. In particular, every λn is an eigenvalue of A. This
follows from (4.23) and (4.24).
(b) The λn are exactly the eigenvalues of A: indeed, if λ ∈ R is an eigenvalue,
then there exists some u ∈ D(A) such that Au = λu, with u  0. Hence
there exists an m ∈ N such that (u, em)H  0. By (4.24) it then follows that
λ(u, em)H = (Au, em)H = λm(u, em)H . Thus λ = λm.
(c) By Theorem 4.13 we know that, for any sequence (xn)n∈N ⊂ R, the series

∞
n=1 xnen converges in H if and only if 
∞
n=1 x2
n < ∞. The domain of A thus
consists of exactly those u ∈ H for which 
∞
n=1 λn(u, en)H en converges in H.
The spectral theorem has a number of notable consequences for the equation
Au − λu = f . (4.25)144 4 Hilbert spaces
The uniqueness of solutions of (4.25) corresponds exactly to λ not being an eigen￾value of A. The following theorem states that the equation is already well-posed,
that is, A − λI is invertible, as soon as A − λI is injective. In other words, what is
known as the Fredholm alternative holds for the equation (4.25): either the equation
has multiple solutions for f = 0, or it is well-posed.
Corollary 4.48 If λ ∈ R \ {λn : n ∈ N}, then A − λI is invertible. Moreover, given
f ∈ H, the solution of (4.25) is given by the series
u =
∞
n=1
(λn − λ)
−1( f, en)H en,
which converges in H.
Proof First observe that δ := inf{|λ − λn| : n ∈ N} > 0 since λn → ∞. Hence
|λ − λn|
−1 ≤ 1/δ for all n ∈ N. This means that
R f :=
∞
n=1
(λn − λ)
−1( f, en)H en
defines a continuous linear operator R : H → H. By Theorem 4.9,
R f 2
H =
∞
n=1
|λ − λn|
−2( f, en)
2
H ≤ δ−2  f 2
H
for all f ∈ H. Now for u ∈ D(A) we have
(A − λI)u =
∞
n=1
(λn − λ)(u, en)H en;
hence R(A − λI)u = u for all u ∈ D(A). In the other direction, since the sequence
 λn
λn − λ

n∈N
is bounded, we have R f ∈ D(A) and (A − λI)R f = f for all f ∈ H. 
Remark 4.49 (Self-adjoint operators) a) Let B be an operator on H with dense domain
D(B). Then the adjoint B∗ of B is defined as follows
D(B∗
) := {v ∈ H : ∃wv ∈ H s.t. (Bu, v)H = (u, wv)H for all u ∈ D(B)},
B∗
v := wv .
More precisely, for v ∈ D(B∗), B∗v is the unique element in H such that
(Bu, v)H = (u, B∗v)H for all v ∈ D(B).4.8 The spectral theorem 145
Uniqueness follows from the density of the domain.
The operator B is called self-adjoint if B = B∗ (with identical domains). Note
that B is symmetric (see the definition given above) if and only if B ⊂ B∗. For
self-adjointness we require in addition that D(B) = D(B∗).
b) The operator A in Theorem 4.46 is self-adjoint (as it is easy to see using the
theorem). In addition, A is bounded from below, i.e., (Au, u)H + ωu2
H ≥ 0 for all
u ∈ D(A) and some ω ≥ 0. Finally, A has compact resolvent, i.e., (λ− A)
−1 : H → H
is compact for all λ ∈ R \ {λn : n ∈ N}. Conversely, every adjoint operator which is
bounded and has compact resolvent is induced by a form exactly in the way described
in Theorem 4.46. 
We now turn to the proof of the spectral theorem; to begin with, we diagonalize
the form.
Theorem 4.50 (Form version of the spectral theorem) Suppose that dim H = ∞
and that V is compactly and densely imbedded in H. Let a : V × V → R be a
continuous, symmetric, H-elliptic bilinear form. Then there exist an orthonormal
basis {en : n ∈ N} of H and λn ∈ R with λn ≤ λn+1, n ∈ N, limn→∞ λn = ∞, such
that
V =

u ∈ H :
∞
n=1
|λn(u, en)|2
H < ∞
!
, (4.26)
a(u, v) =
∞
n=1
λn(u, en)H (en, v)H for all u, v ∈ V. (4.27)
Proof We may assume that a(·, ·) is coercive; if not, then we consider aω(·, ·) in place
of a(·, ·), where ω is the constant from (4.22). So we suppose that a(·, ·) is coercive,
that is, we assume that (4.6) holds for all u ∈ V. Then it follows that
√
αuV ≤ a(u)
1/2 ≤ √
CuV for all u ∈ V.
Since a(·, ·) is symmetric, a(u)
1/2 thus defines an equivalent norm on V (called the
energy norm ·a). Since the inner product of V does not appear explicitly in the
formulation of the theorem, we may as well suppose that in fact
(u, v)V = a(u, v) , u, v ∈ V.
We now define λ1 := inf{a(u) : u ∈ V, uH = 1}. It follows from the coercivity of
a, see (4.6), that λ1 > 0.
1. We claim that there exists an e1 ∈ V such that e1 H = 1, λ1 = a(e1) and
a(e1, v) = λ1(e1, v)H for all v ∈ V. (4.28)
To this end we consider the form a1(u, v) := a(u, v) − λ1(u, v)H , u, v ∈ V. Then
inf{a1(u) : u ∈ V, uH = 1} = 0. We can thus find vectors un ∈ V with un H = 1,146 4 Hilbert spaces
such that limn→∞ a1(un) = 0, and in particular the sequence (un)n∈N is bounded inV.
Theorem 4.35 allows us to assume without loss of generality that un  e1 in V, for
some e1 ∈ V (otherwise we pass to a subsequence). Since V is compactly imbedded
in H, it follows that un → e1 in H, and so e1 H = 1. The Cauchy–Schwarz
inequality now yields
a1(e1) = a1(e1 − un, e1) + a1(un, e1)
≤ a1(e1 − un, e1) + a1(un)
1/2a1(e1)
1/2.
Since f (v) := a1(v, e1) defines a continuous linear form on V and un  e1 in V, we
have limn→∞ a1(e1 − un, e1) = 0. Since a1(un) → 0, the above inequality implies
that a1(e1) = 0. Hence a(e1) = λ1. Furthermore, by Theorem 4.26, for v ∈ V we
have a1(e1, v) ≤ a1(e1)
1/2a1(v)
1/2 = 0. By replacing v by −v, we see that the reverse
inequality also holds. Thus a1(e1, v) = 0, that is, we have a(e1, v) = λ1(e1, v)H for
all v ∈ V. This proves (4.28).
2. We now consider the space V1 := {u ∈ V : (u, e1)H = 0}, which is a closed
subspace of V. It is not equal to {0} since V is dense in H and dim H = ∞. If we
apply 1. to the restriction of a(·, ·) to V1 × V1 and to H1 := {u ∈ H : (u, e1)H = 0},
then we obtain an e2 ∈ V1 such that e2 H = 1 and λ2 := a(e2) = min{a(u) :
u ∈ V1, uH = 1}, as well as a(e2, v) = λ2(e2, v)H for all v ∈ V1. Since by 1.
we have a(e2, e1) = a(e1, e2) = λ1(e1, e2)H = 0 = λ2(e2, e1)H , it follows that
a(e2, v) = λ2(e2, v)H for all v ∈ V (by the projection theorem).
3. If we continue in this fashion, we find a sequence (en)n∈N in V such that
en H = 1 and a monotonically increasing sequence (λn)n∈N in R, such that for all
n ≥ 1,
λn+1 = a(en+1) = min{a(u) : u ∈ Vn, uH = 1},
where Vn := {u ∈ V : (u, ek )H = 0, k = 1,..., n}, en+1 ∈ Vn and a(en, v) =
λn(en, v)H for all v ∈ V and all n ∈ N. In particular, the sequence (en)n∈N is
orthonormal.
4. We will now show that limn→∞ λn = ∞. Suppose not; then after passing to a
subsequence if necessary we may assume that the sequence a(en) = λn is bounded.
Then (en)n∈N is bounded in V, and so there exists a subsequence (enk )k ∈N which
converges weakly in V, say to w ∈ V. Then, by compactness of the imbedding of V in
H, in fact limk→∞ enk = w in H. Since en  0 in H as an orthonormal sequence in
H (see Example 4.32), it follows that w = 0. But this yields the desired contradiction
to wH = limk→∞ enk H = 1. We conclude that limn→∞ λn = ∞.
5. We will next show that {en : n ∈ N} is total in H. To this end, suppose that
v ∈ H is such that (en, v)H = 0 for all n ∈ N; then we wish to show that v = 0
(see Corollary 4.19). Suppose that v  0. Let v1 := v −1
H v; then v1 H = 1 and
(en, v1)H = 0 for all n ∈ N. Thus v1 ∈ Vn (as defined in 3.) for all n ∈ N, and so
a(v1) ≥ inf{a(u) : u ∈ Vn, uH = 1} = λn4.8 The spectral theorem 147
for all n ∈ N. This contradicts the fact that limn→∞ λn = ∞.
6. Finally, set e˜n := 1
√
λn
en. Then a(e˜n, e˜m) = 1
√
λn
1
√
λm
a(en, em) is equal to 1 if
n = m, and 0 otherwise. Since for u ∈ V we have that
(u, e˜n)V = a(u, e˜n) = 1
√
λn
a(u, en) = 
λn (u, en)H, (4.29)
it follows from (u, e˜n)V = 0 for all n ∈ N that u = 0. Hence {e˜n : n ∈ N} is an
orthonormal basis of V. By Theorem 4.9 we thus obtain, for all u, v ∈ V,
a(u, v) = (u, v)V =
∞
n=1
(u, e˜n)V (e˜n, v)V =
∞
n=1
λn(u, en)H (en, v)H .
In particular, 
∞
n=1 λn(u, en)
2
H = a(u) < ∞. In the other direction, let u ∈ H be
such that 
∞
n=1 λn(u, en)
2
H < ∞. Set xn := √
λn (u, en)H . Then by Theorem 4.13 the
series 
∞
n=1 xne˜n converges to an element w of V. Since xne˜n = (u, en)H en, we have
w = 
∞
n=1(u, en)H en = u and thus u = w ∈ V. This completes the proof of Theorem
4.50. 
It is remarkable that the assumptions of Theorem 4.50 already imply that the
space H is separable.
Proof (of Theorem 4.46 (the spectral theorem)) By Theorem 4.50, V is given by
(4.26), and a(·, ·) by (4.27). Let A be the operator associated with a(·, ·). Let u ∈ D(A)
with Au = f ; then by definition, a(u, v) = ( f, v)H for all v ∈ V. In particular, for
v = en, the equation ( f, en)H = a(u, en) = λn(u, en)H holds, whence
f =
∞
n=1
( f, en)H en =
∞
n=1
λn(u, en)H en.
In the other direction, let u ∈ H such that 
∞
n=1 λ2
n (u, en)
2
H < ∞. Then since λn → ∞
as n → ∞ we also have 
∞
n=1 λn (u, en)
2
H < ∞ and so u ∈ V. Moreover,
f :=
∞
n=1
λn (u, en)H en
in H. For n ∈ N, we have ( f, en)H = λn(u, en)H = a(u, en) and thus ( f, v)H = a(u, v)
for all v ∈ span{en : n ∈ N}. Since we already showed in the proof of Theorem 4.50
that {λ−1/2 n en : n ∈ N} is an orthonormal basis of V, it follows that ( f, v)H = a(u, v)
for all v ∈ V. Hence u ∈ D(A) and Au = f . 
Remark 4.51 (Gelfand triple) Let V and H be real Hilbert spaces such that V → H
(i.e., V is a subspace of H and v H ≤ c v V for all v ∈ V and some c ≥ 0). Also
assume that V is dense in H.
Then, for every fixed f ∈ H, ( f, ·)H defines a continuous linear form on V.
Moreover, since V is dense in H, the mapping f → ( f, ·)H : H → V is injective.148 4 Hilbert spaces
Identifying f ∈ H with ( f, ·)H we thus obtain a continuous imbedding H → V
.
Indeed,
( f, ·)H V = sup
v∈V
|( f, v)H |
v V
≤ sup
v∈V
 f H v H
v V
≤ c  f H,
which establishes the claimed continuity of the imbedding. We thus have two imbed￾dings
V → H → V
.
These three spaces, together with the corresponding imbeddings, are sometimes
called a Gelfand triple. For ϕ ∈ V one frequently uses the notation
ϕ, v := ϕ(v) (v ∈ V),
which is also known as a duality pairing. Thus, if ϕ = f ∈ H ⊂ V
, then
f, v = ( f, v)H (v ∈ V).
The bracket ·, · extends the scalar product of H. It is important to realize that
identifying H with a subspace of V prevents us from identifying V with V at the
same time: V becomes a proper subspace of V (unless V = H).
Now we assume that the imbedding of V in H is, in addition, compact. Then we
choose the scalar product of V as bilinear form and apply the form version of the
spectral theorem, Theorem 4.50. It yields an orthonormal basis (en)n∈N of H and a
sequence 1 ≤ λ1 ≤ λ2 ≤ ··· ≤ λn ≤ λn+1 with limn→∞ = ∞ such that
V =

v ∈ H :
∞
n=1
λn (v, en)
2
H < ∞
!
, (u, v)V =
∞
n=1
λn (u, en)H (en, v)H
for all u, v ∈ V. Now the imbedding of H in V can be described very elegantly. The
space
W :=

w = (wn)n∈N ⊂ R :
∞
n=1
w2
n
λn
< ∞
!
is a Hilbert space when equipped with the scalar product
(w, v)W =
∞
n=1
1
λn
wn vn, w, v ∈ W.
It is isometrically isomorphic to V if we define, for w ∈ W and v ∈ V,
w, v :=
∞
n=1
wn (en, v)H .4.8 The spectral theorem 149
Proof Let w = (wn)n∈N ∈ W. By the Cauchy–Schwarz inequality we have for v ∈ V,
∞
n=1
|wn (en, v)H | =
∞
n=1
1
√
λn
|wn|

λn |(en, v)H |
≤

∞
n=1
1
λn
w2
n
1/2 
∞
n=1
λn(en, v)
2
H
1/2
= wW v V .
Thus w, · ∈ V
.
Conversely, let ϕ ∈ V
. By the theorem of Riesz–Fréchet (Theorem 4.21) there
exists a unique u ∈ V such that
ϕ, v = (u, v)V =
∞
n=1
λn (u, en)H (en, v)H
for all v ∈ V. Moreover, ϕ2
V = u2
V = 
∞
n=1 λn (u, en)
2
H (see Example 4.20). Let
wn := λn (u, en)H = ϕ(en). Then 
∞
n=1
1
λn w2
n = 
∞
n=1 λn (u, en)
2
H . Thus, we have
w ∈ W, wW = ϕV and
ϕ, v =
∞
n=1
λn (u, en)H (en, v)H =
∞
n=1
wn (en, w)H = w, v
for all v ∈ V. 
Notice that the desired isomorphism is given by V → W : ϕ → (ϕ(en))n∈N. 
Let us again consider the situation described in the spectral theorem, Theorem
4.46. Here, λn is the nth eigenvalue of A, since we are assuming that the eigenvalues
are ordered as an increasing sequence. From the proof of Theorem 4.50 we can
extract the following formula for the first eigenvalue
λ1 = min{a(u) : u ∈ V, uH = 1}. (4.30)
The following max-min formula allows us more generally to calculate the nth eigen￾value of A from the form a(·, ·).
Theorem 4.52 (Max-min formula) Under the assumptions of the spectral theorem,
the nth eigenvalue of A is given by
λn = max
W⊂V
codimW≤n−1
min
u∈W
uH=1
a(u). (4.31)
Remark 4.53 Let W be a subspace of V. (a) We set codimW := dim W⊥, where
W⊥ := {v ∈ V : (w, v)V = 0 for all w ∈ W}.150 4 Hilbert spaces
(b) If U is a subspace of V such that U ∩ W = {0}, then dimU ≤ codimW. In fact,
denote by P : V → W⊥ the orthogonal projection. Then P|U is injective (because
for u ∈ U, Pu = 0 implies that u ∈ W ∩ U). Thus dimU ≤ dim W⊥. 
Proof (of Theorem 4.52) Let n > 1 and let W be a subspace of V with codimW ≤
n−1. Then by Remark 4.53 (b) there exists u ∈ span{e1,..., en}∩W with uH = 1.
For such a u, by (4.27),
a(u) =
n
k=1
λk (u, en)
2
H ≤ λn
n
k=1
(u, ek )
2
H = λn.
Hence inf{a(u) : u ∈ W, uH = 1} ≤ λn for every subspace W of V with
codimW ≤ n − 1. It remains to show that such a W exists for which inf{a(u) : u ∈
W, uH = 1} = λn. We claim that Wn−1 := {e1,..., en−1}⊥ will work. In fact, let
u ∈ Wn−1 such that uH = 1; then
a(u) =

k≥n
λk (u, ek )
2
H ≥ λn

k≥n
(u, ek )
2
H = λn.
Since en ∈ Wn−1 and a(en) = λn, we finally have min{a(u) : u ∈ Wn−1, uH =
1} = λn. 
If a(·, ·) is not symmetric, then the associated operator A is not in general a
diagonal operator. Nevertheless, it still satisfies the Fredholm alternative. For the
proof, we refer to [2, Thm. 12.8].
Theorem 4.54 (Fredholm alternative) Let H be a real separable Hilbert space and
let H be a Hilbert space which is compactly and densely imbedded in H. Suppose
that a : V × V → R is a continuous, H-elliptic bilinear form. Then for the operator
A associated with a, if λ ∈ R is not an eigenvalue of A, then A − λI is invertible.
4.9* Comments on Chapter 4
David Hilbert (1862–1943) was one of the pre-eminent mathematicians of the first half of last
century. At the International Congress of Mathematicians (ICM) in Paris in 1900, Hilbert presented
his famous 23 mathematical problems, which continue to exert considerable influence on math￾ematical research to this day. He was the central figure of the Göttingen school, which up until
the Nazis seized power in Germany in 1933 was one of the foremost mathematical institutions
worldwide. Between 1904 and 1910, Hilbert published a series of articles on integral equations, in
which bilinear forms on 2 appeared as an essential tool.
The definition of Hilbert spaces was only given in 1929, by John von Neumann. Decisive for the
success of Hilbert space theory was the invention of the Lebesgue integral, in the doctoral thesis of
Henri Lebesgue completed in 1902. Building on Lebesgue’s theory, Frigyes Riesz showed in 1907
that every element of 2(Z) is the sequence of Fourier coefficients of some function in L2(0, 2π). In
the same year, Ernst Fischer (1875–1954) proved that L2(0, 2π) is complete, which is equivalent to
Riesz’ result. This is why the name “theorem of Riesz–Fischer” is often used for the fact that L2(Ω)4.10 Exercises 151
is complete (see the appendix). Riesz and Maurice Fréchet (1878–1973) determined the set of all
continuous linear forms on L2(0, 1) independently of each other in 1907, thus proving Theorem
4.21.
Fréchet obtained his doctorate in Paris in 1906 under Hadamard; in his dissertation, metric
spaces are introduced for the first time. The theorem of Riesz–Fréchet is often known as the Riesz
representation theorem; however, there is another Riesz representation theorem for measures, which
will play an important role in Section 7.2.
It was Hilbert who proved a first version of the spectral theorem, Theorem 4.50. But subsequent
to his works on integral equations, it was operators and not bilinear forms which took centre stage.
Bounded operators on Hilbert spaces became a central concept in mathematics, and to this day
operator theory remains an important area of mathematical research. In the 1930s, quantum theory
was a key driving force in the development of functional analysis. The decisive breakthrough in the
mathematical formulation of quantum physics was made by John von Neumann, who had visited
Göttingen in 1926/27 and to whom the concept of unbounded operators is due (cf. Mathematische
Annalen, No. 33, 1932). An unbounded self-adjoint operator (as was defined in Section 4.8) models
an observable in quantum theory. The book Mathematische Grundlagen der Quantenmechanik
(Mathematical Foundations of Quantum Mechanics) by John von Neumann, which appeared in
1932 in German, describes the mathematical modeling of quantum theory as is still used today, and
indeed with great success. Although operators are ideal for describing equations, it is interesting
that bilinear forms reached their pinnacle as a method for solving partial differential equations
around 50 years after Hilbert’s work.
The Lax–Milgram theorem was published in 1954; since then it has become a standard tool for
every analyst and numericist. Peter David Lax counts among the most important mathematicians of
the second half of last century. He has made material contributions to the theory and the numerics
of partial differential equations, as well as mathematical physics and functional analysis. But he
was equally responsible for the proof of fundamental theorems in numerical mathematics. Lax was
born in 1926 in Budapest and emigrated in 1941 to the United States with his parents. He was a
professor at New York University and director of the Courant Institute of Mathematical Sciences
for many years. Among the numerous awards and distinctions he received was the Abel Prize in
2005.
The key notion of a well-posed problem, which we met in Section 4.8, was introduced by Jacques
Hadamard (1865–1963) in 1898, in an article on boundary value problems (which will be the topic
of Chapters 5, 6 and 7). In 1910, a year after Hadamard took up a position as professor for mechanics
at the Collège de France, he published his Leçons sur le calcul des variations, in which the notion
of a functional is introduced for the first time. Hadamard had an enormous influence on analysis; we
will meet him again when we see his contributions to the dispute about the variational formulation
of the Dirichlet problem in Chapter 6. The Fredholm alternative is particularly attractive when taken
together with Hadamard’s concept of well-posedness. In the setting of Theorem 4.54, for example,
it shows that well-posedness already follows from uniqueness of solutions alone. It was proved by
Erik Fredholm (1866–1927) in 1903; Fredholm had obtained his doctorate in Uppsala in 1893 and
became well known for his work on integral equations and spectral theory.
4.10 Exercises
Exercise 4.1 Show that the space 2(I) is a Hilbert space with respect to the inner
product (x, y) := 

n∈I xn yn, where I is either a finite index set, N, or Z.152 4 Hilbert spaces
Exercise 4.2 Let H1 and H2 be Hilbert spaces and T : H1 → H2 a linear mapping.
If on both H1 and H2 we consider two kinds of convergence, namely convergence
in norm and weak convergence, then there are four possibilities in total to define
sequential continuity of T. Which of these four are the same?
Suggestion: Use the closed graph theorem (see Theorem A.2).
Exercise 4.3 (Polarization identity)
(a) Let E be a real vector space and let a : E × E → R be bilinear and symmetric.
Show that a(u, v) = 1
4 (a(u + v) − a(u − v)) for all u, v ∈ E. Here a(u) := a(u, u),
u ∈ E.
(b) Let E1 and E2 be inner product spaces and let T : E1 → E2 be linear. Show that
the following statements are equivalent:
(i) (Tu, Tv)E2 = (u, v)E1 for all u, v ∈ E1.
(ii) T is isometric, that is, Tu = u for all u ∈ E1.
Thus T is isometric if and only if it preserves the inner product.
Exercise 4.4 (Completion) Let E be a separable real inner product space and let
{en : n ∈ N} be an orthonormal basis of E.
(a) Prove that the mapping J : E → 2, J(u) = ((u, en))n∈N is linear and isometric,
and has dense range.
(b) Deduce that there exists a Hilbert space H such that E is a dense subspace of H.
Suggestion: Identify E and J(E).
(c) Uniqueness: Let H1 and H2 be Hilbert spaces and Jk : E → Hk linear and
isometric with dense range, k = 1, 2. Show that there exists a unitary operator
U : H1 → H2 such that U J1 x = J2 x for all x ∈ H1.
Exercise 4.5 Let E be a real normed vector space and suppose a : E × E → R is
bilinear. We say that a(·, ·) is continuous if un → u, vn → v ⇒ a(un, vn) → a(u, v).
(a) Show that a is continuous if and only if there exists a constant C ≥ 0 such that
|a(u, v)| ≤ Cu v , u, v ∈ E.
(b) Let a be symmetric and a(u) ≥ 0, u ∈ E. Show that a is continuous if and only
if un → u ⇒ a(un) → a(u).
Suggestion: Use Theorem 4.26.
(c) Let H be a Hilbert space and T : H → H linear and symmetric, that is,
(Tu, v) = (u, Tv), u, v ∈ H. Show that T is continuous.
Suggestion: Closed graph theorem.
(d) Let H be a Hilbert space, a : H × H → R bilinear, symmetric and such that
un → u ⇒ a(un, v) → a(u, v) for all v ∈ H. Show using (c) that a is continuous.
Exercise 4.6 Consider the operators from Example 4.42. Determine ker A0, ker A1
and ker B. Which of the operators is invertible?
Exercise 4.7 Let A be an operator on the Hilbert space H. We say that A is bounded
if there exists c > 0 such that Au ≤ cu for all u ∈ D(A), cf. Appendix A.1.4.10 Exercises 153
(a) Let A be invertible. Show that A is bounded if and only if D(A) is closed in H.
(b) Let A be invertible and suppose that A−1 : H → H is compact. Show that A is
bounded if and only if dim H < ∞.
(c) We say that A is closed if the graph G(A) := {(u, Au) : u ∈ D(A)} of A is closed
in H × H. Show that if A is invertible, then A is closed.
(d) Let D(A) = H. Show that A is bounded if and only if A is closed.
(e) Show that A is closed if and only if D(A) is complete with respect to the norm
u2
A := u2
H + Au2
H .
Exercise 4.8 Let H be a Hilbert space and let {en : n ∈ N} be an orthonormal basis
of H. Let λn ∈ (0, ∞), λn ≤ λn+1, limn→∞ λn = ∞.
(a) Set V := {u ∈ H :

∞
n=1 λn(u, en)
2
H < ∞}. Show that
a(u, v) :=
∞
n=1
λn(u, en)H (en, v)H
defines an inner product on V with respect to which V is a Hilbert space.
(b) Show that V is compactly imbedded in H.
(c) Choose H = 2 and en = (0,..., 0, 1, 0 ...), where the 1 is the nth entry. Let A be
the operator associated with a(·, ·). Show that D(A) = {x ∈ 2 : (λn xn)n∈N ∈ 2}
and Ax = (λn xn)n∈N. Thus A is a diagonal operator.
Exercise 4.9 We consider the setup described in Theorem 4.46. Let λ be an eigen￾value of A. Show:
(a) ker(A − λI) is finite dimensional;
(b) given f ∈ H, (4.25) has a solution if and only if ( f, em)H = 0 for all m ∈ N such
that λm = λ.
Exercise 4.10 Let g ∈ C([0, π]) be real valued, and set
ak = 2
k
∫ π
0
cos(ks) g(s) ds, k ∈ N0, bk = 2
k
∫ π
0
sin(ks) g(s) ds, k ∈ N,
cf. Corollary 3.19.
(a) Show that 

∞
k=1
(a2
k + b2
k ) < ∞.
Suggestion: Use Theorem 4.9.
(b) Let g ∈ C1([0, π]) with g(0) = g(π) = 0. Show that 

∞
k=1
|bk | < ∞, and conclude
that g(x) = 

∞
k=1
bk sin(k x), where the series converges uniformly in x.
Suggestion: Apply (a) to a
k = 2
k
∫π
0
cos(ks) g
(s) ds and show that bk = a
k
k .154 4 Hilbert spaces
Exercise 4.11 Consider the initial-boundary value problem (3.12) (page 53) with
a = 0, b = π and assume that u0, u1 satisfy the hypotheses of Theorem 3.6 (page
54). The aim of this exercise is to show that the solution u of (3.12) is given by
u(t, x) =
∞
k=1
sin(k x)

ak cos(ckt) + bk sin(ckt)), (4.32)
where ak = 2
π
∫π
0
u0(x) sin(k x) dx and bk = 2
ckπ
∫π
0
u1(x) sin(k x) dx.
This shows that the solution u is a superposition of the harmonics uk (t, x) =
sin(k x)

ak cos(ckt) + bk sin(ckt)
	
. Observe that in the expression for uk (t, x) only
the amplitude of the wave sin(k·) is time dependent.
(a) Use Exercise 4.10 to show that 

∞
k=1

|ak | + |bk |
	
< ∞, so that (4.32) converges
uniformly to the continuous function u.
(b) Assume that u1 = 0 and let u˜0(x) = 

∞
k=1
ak sin(k x), x ∈ R.
Show that u˜0 ∈ C2(R) and u˜0|[0,π] = u0.
Suggestion: Use Exercise 3.21.
(c) Assume that u1 = 0. We know from Theorem 3.6 and its proof that u(t, x) = 1
2

u˜0(x + ct) + u˜0(x − ct)
	
. Use this to prove (4.32).
(d) Assume that u0 = 0. Let ck = 2
π
∫π
0
u1(x) sin(k x) dx. Define H : R → R by
H(x) := − 

∞
k=1
ck
k cos(k x). Show that H ∈ C1(R) and H = u˜1. By Theorem 3.6
and its proof the solution u of (3.12)is given by u(t, x) = 1
2c

H(x+ct)−H(x−ct)
	
.
Use this to show (4.32) in the case u0 = 0.
(e) Put (c) and (d) together to prove the claim.
Exercise 4.12 Let V → H → V be a Gelfand triple as in Remark 4.51; in particular,
V is taken to be dense in H. Show that H is dense in V
.
Suggestion: Use Corollary 4.18.Chapter 5
Sobolev spaces and boundary value problems in
dimension one
Sobolev spaces on an interval have special appeal: they consist of functions which
can be written as indefinite integrals of integrable functions. Numerous properties
and rules of calculation, such as the integration by parts formula, still hold, and
give us a version of calculus which is almost as practicable as the classical version
taught in first year. But what one gains by basing everything on integrable rather
than continuous functions, is the structure of a Hilbert space, namely the Hilbert
space of (weakly) differentiable functions. As such, the theorem of Riesz–Fréchet,
and more generally the Lax–Milgram theorem, can be applied to obtain existence
and uniqueness of solutions of differential equations with boundary conditions.
In this chapter we carry out the entire program in the simple situation of dimension
one, which we then repeat for higher dimensions in Chapters 6 and 7. But even in
dimension one we can see how efficiently and elegantly Hilbert space methods can
be applied. The contents of this chapter are essentially from the area of ordinary
differential equations: we consider problems of Sturm–Liouville type or – expressed
in the language of our model examples – stationary reaction and diffusion equations.
The various boundary conditions (Dirichlet, Neumann, mixed and Robin) do not
cause much trouble: Sobolev spaces on intervals consist of functions which are
continuous up to the boundary.
The chapter consists of two parts: the introduction to Sobolev spaces, and the
boundary value problems as applications. Some of the exercises at the end of the
chapter also give additional information about the theory, others lead to the solution
of further boundary value problems.
Chapter overview
5.1 Sobolev spaces in one variable ........................... 156
5.2 Boundary value problems on the interval ..................... 164
5.3* Comments on Chapter 5 .............................. 176
5.4 Exercises ...................................... 176
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4_5
155156 5 Sobolev spaces and boundary value problems in dimension one
5.1 Sobolev spaces in one variable
Let −∞ < a < b < ∞. We consider the space L2(a, b) of real-valued square
integrable functions on the interval (a, b). This is a real Hilbert space with respect
to the inner product
( f, g)L2 =
∫ b
a
f (x) g(x) dx,
which induces the norm
 f L2 :=

( f, f ) =
∫ b
a
( f (x))2 dx 1
2
.
In this section we wish to study weak derivatives. For this, we will need the following
class of C1-functions with compact support.
Let C1
c (a, b) denote the space of the functions v ∈ C1(a, b) with compact support,
that is,
C1
c (a, b) := {v ∈ C1(a, b) : ∃ε > 0 such that
v(x) = 0 for x ∈ [a, a + ε]∪[b − ε, b]}.
For f ∈ C1
c (a, b) one calls supp f := {x ∈ [a, b] : f (x)  0} the support of f , cf.
(2.1). We start by noting that every function f ∈ L2(a, b) can be approximated by
functions in C1
c (a, b).
Lemma 5.1 The space C1
c (a, b) is dense in L2(a, b).
Proof The statement follows, for example, from the definition of L2(a, b) (since the
step functions, which can be easily smoothed, are dense in L2(a, b), see Theorem
A.11). We will, however, also give a proof in higher dimensions (Corollary 6.9). 
If f ∈ C1([a, b]), that is, if f is continuously differentiable in the usual sense,
then by integration by parts, for all v ∈ C1
c (a, b) we obtain
−( f, v
)L2 = −
∫ b
a
f (x)v
(x) dx =
∫ b
a
f 
(x)v(x) dx = ( f 
, v)L2, (5.1)
where the boundary terms disappear since v(a) = v(b) = 0.
The integrals which appear here are clearly still well defined if f  is only Lebesgue
integrable. This observation leads us to the following definition.
Definition 5.2 Let f ∈ L2(a, b). A function g ∈ L2(a, b) is called a weak derivative
of f if
−
∫ b
a
f (x)v
(x) dx =
∫ b
a
g(x)v(x) dx for all v ∈ C1
c (a, b),5.1 Sobolev spaces in one variable 157
or, written differently, −( f, v
)L2 = (g, v)L2 . 
We next prove that weak derivatives are unique.
Lemma 5.3 A function f ∈ L2(a, b) has at most one weak derivative g ∈ L2(a, b).
Proof Suppose that h ∈ L2(a, b) is a second weak derivative of f . Then it follows
from the definition that
∫ b
a
g(x)v(x) dx =
∫ b
a
h(x)v(x) dx,
that is, (g − h, v)L2 = 0 for all v ∈ C1
c (a, b). Since C1
c (a, b) is dense in L2(a, b), it
follows that (g − h, v)L2 = 0 for all v ∈ L2(a, b). In particular, (g − h, g − h)L2 = 0
and thus g = h in L2(a, b). 
Remark 5.4 If f ∈ L2(a, b) ∩ C1(a, b) and f  ∈ L2(a, b), then f  is also the weak
derivative of f , as follows directly from (5.1). That is, if a derivative exists in the
classical sense, then it coincides with the weak derivative. We therefore denote the
weak derivative of a function f ∈ L2(a, b), if it exists, by f 
. 
We now define the Sobolev space H1(a, b) of order one as the space of those
functions in L2(a, b) which have a weak derivative in L2(a, b), that is,
H1(a, b) :=

f ∈ L2(a, b) : ∃ f  ∈ L2(a, b) such that
−
∫ b
a
f (x)v
(x) dx =
∫ b
a
f 
(x)v(x) dx ∀v ∈ C1
c (a, b)
'
.
We next give an example of a function which is weakly but not classically differ￾entiable.
Example 5.5 (Absolute value function) Let f (x) := |x|. Then f ∈ H1(−1, 1) and
f 
(x) = sign (x) :=
⎧⎪⎪⎨
⎪⎪
⎩
1, if x > 0,
0, if x = 0,
−1, if x < 0;
that is, the weak derivative is the piecewise derivative; see Exercise 5.2 for the
proof. 
It is immediate that H1(a, b) is a vector subspace of L2(a, b) and the mapping
f → f  : H1(a, b) → L2(a, b) is linear.
Theorem 5.6 The space H1(a, b) is a Hilbert space with respect to the inner product
( f, g)H1 := ( f, g)L2 + ( f 
, g
)L2 .158 5 Sobolev spaces and boundary value problems in dimension one
If necessary, we may also write (·, ·)H1(a,b). We denote the associated norm on
H1(a, b) by uH1(a,b) := 
(u, u)H1 .
Proof We need to show that the space H1(a, b) is complete with respect to the norm
 f 2
H1 :=
∫ b
a
| f (x)|2 dx +
∫ b
a
| f 
(x)|2 dx =  f 2
L2 +  f 
2
L2
.
Now the Cartesian product H := L2(a, b) × L2(a, b) is a Hilbert space with respect
to the norm
[ f, g]2
H :=  f 2
L2 + g2
L2
, where [ f, g] ∈ H is any pair, i.e., element of H.
The mapping j : H1(a, b) → H given by j( f ) := [ f, f 
] is linear, and isometric since
 j( f )H = ( f 2
L2 +  f 
2
L2
)
1/2 =  f H1 . Thus H1(a, b) is isometrically isomorphic
to its image j(H1(a, b)) =: F. We therefore need to show that F = {[ f, f 
] : f ∈
H1(a, b)} is closed in H. This will prove that F is complete, since H is. So let
[ f, g] ∈ F be given. Then there exist fn ∈ H1(a, b) such that fn → f and f 
n → g in
L2(a, b). Hence, for all v ∈ C1
c (a, b),
−
∫ b
a
f (x)v
(x) dx = (−1) lim
n→∞ ∫ b
a
fn(x)v
(x) dx
= lim
n→∞ ∫ b
a
f 
n(x)v(x) dx =
∫ b
a
g(x)v(x) dx.
This means that f  = g according to our definition, and thus [ f, g] ∈ F. 
The following results show that many properties of classical derivatives continue
to hold for weak derivatives. As always, we will identify functions which agree
almost everywhere.
Lemma 5.7 Let f ∈ H1(a, b) be such that f  = 0. Then f is constant.
Proof Fix ψ ∈ C1
c (a, b) such that ∫ b
a ψ(x) dx = 1. Let w ∈ C1
c (a, b); we claim that
there exists v ∈ C1
c (a, b) such that
v
(x) = w(x) − ψ(x)
∫ b
a
w(y) dy.
To see this, we define g(x) := w(x) − ψ(x)
∫ b
a w(y) dy. Then g ∈ C1
c (a, b) and
∫ b
a g(x) dx = 0. We can then set v(x) := ∫ x
a g(y) dy. Now since f  = 0 weakly, with
v defined as above, we have
0 = −
∫ b
a
f 
(x)v(x) dx =
∫ b
a
f (x)v
(x) dx5.1 Sobolev spaces in one variable 159
=
∫ b
a
f (x) w(x) dx −
∫ b
a
w(x) dx ∫ b
a
f (x) ψ(x) dx
=
∫ b
a

f (x) −  ∫ b
a
f (y) ψ(y) dy
  w(x) dx.
Since w ∈ C1
c (a, b) was arbitrary and C1
c (a, b) is dense in L2(a, b), it follows that
f (x) − ∫ b
a f (y) ψ(y) dy = 0 in L2(a, b), that is,
f (x) =
∫ b
a
f (y) ψ(y) dy ≡ const
for almost all x ∈ (a, b). 
In what follows, we will require Fubini’s theorem in the following form.
Lemma 5.8 (Fubini’s theorem) Let f, g ∈ L1(a, b). Then for a ≤ x ≤ b we have
∫ b
a
∫ x
a
g(y) dy f (x) dx =
∫ b
a
∫ b
y
f (x) dx g(y) dy.
Proof Let
1[a,x](y) =

1 if a ≤ y ≤ x,
0 otherwise.
The theorems of Fubini and Tonelli on exchanging the order of integration show that
∫ b
a
∫ b
a
1[a,x](y)g(y) f (x) dy dx =
∫ b
a
∫ b
a
1[a,x](y)g(y) f (x) dx dy.
This is exactly the desired identity. 
The following theorem is a weak version of the Fundamental Theorem of Calculus.
Theorem 5.9 (Lebesgue version of the fundamental theorem of calculus)
(a) Let g ∈ L2(a, b), c ∈ R and f (x) := c + ∫ x
a g(y) dy for x ∈ (a, b). Then
f ∈ H1(a, b) and f  = g.
(b) In the opposite direction, let f ∈ H1(a, b). Then there exists c ∈ R such that
f (x) = c +
∫ x
a f 
(y) dy for almost every x ∈ (a, b).
Proof (a) Let v ∈ C1
c (a, b). Since ∫ b
y v
(x) dx = v(b) − v(y) = −v(y), by Lemma 5.8
we have
−
∫ b
a
f (x)v
(x) dx = −
∫ b
a
∫ x
a
g(y) dy v
(x) dx − c
∫ b
a
v
(x) dx160 5 Sobolev spaces and boundary value problems in dimension one
= −
∫ b
a
∫ b
y
v
(x) dx g(y) dy =
∫ b
a
v(y) g(y) dy.
Thus g is the weak derivative of f .
(b) Let f ∈ H1
∫
(a, b) be given. Define the function w by w(x) := f (x) − x
a f 
(y) dy. Then w ∈ H1(a, b) and w = 0 by (a). Hence w is constant by Lemma
5.7. 
Corollary 5.10 We have H1(a, b) ⊂ C([a, b]).
The statement of the above Corollary 5.10 requires an explanation. In accordance
with the definition of L2(a, b), we identify functions which agree almost everywhere.
If two continuous functions agree almost everywhere, then they are identically equal
(since if they were to differ in a single point, then they would differ in a neighborhood
of that point). Theorem 5.9 permits us to identify each function f ∈ H1(a, b) with a
(or rather its) continuous representative,
f (x) = f (a) +
∫ x
a
f 
(y) dy (5.2)
for all x ∈ [a, b]. In the sequel we will always choose the continuous representative
of f ∈ H1(a, b). The following result follows directly from (5.2).
Corollary 5.11 A function f ∈ H1(a, b) is in C1([a, b]) if and only if f  ∈ C([a, b]).
Now C([a, b]) is a Banach space, that is, a complete normed space, with respect
to the norm
 f ∞ := sup
x∈[a,b]
| f (x)|.
In this norm, the imbedding of H1(a, b) in the space C([a, b]) is continuous, that is,
H1(a, b) → C([a, b]), cf. (4.17). But more is true.
Theorem 5.12 The imbeddings of H1(a, b) in C([a, b]) and L2(a, b) are compact.
Proof 1. The imbedding of H1(a, b) in C([a, b]) is continuous. This follows directly
from the closed graph theorem (Theorem A.2); see Exercise 5.11. However, we will
give a direct proof. We have to show that there exists a constant c ≥ 0 such that
 f ∞ ≤ c f H1(a,b) for all f ∈ H1(a, b). Assume that this statement is false. Then
there exists a sequence ( fn)n∈N in H1(a, b) such that  fn H1(a,b) ≤ 1, but  fn ∞ ≥ n.
By Theorem 5.9,
fn(x) = fn(a) +
∫ x
a
f 
n(y) dy. (5.3)
By the Cauchy–Schwarz inequality,5.1 Sobolev spaces in one variable 161




∫ x
a
f 
n(y) dy




≤ (b − a)
1
2
∫ b
a
| f 
n(y)|2 dy
 1
2
≤ (b − a)
1
2  fn H1(a,b)
≤ (b − a)
1
2 .
If ( fn(a))n∈N were bounded, then by (5.3), the sequence ( fn(x))n∈N would be
too. Thus limn→∞ | fn(a)| = ∞. Suppose without loss of generality that in fact
limn→∞ fn(a) = ∞. By what we have shown above, fn(x) ≥ fn(a)−(b − a)
1
2 and
thus we get limn→∞  fn L2 = ∞. But since on the other hand  fn L2 ≤  fn H1 ≤ 1,
we have found a contradiction.
2. We will now show that the imbedding of H1(a, b) in C([a, b]) is compact.
Denote by B := { f ∈ H1(a, b) :  f H1 ≤ 1} the unit sphere in H1(a, b). We wish to
show that this set is relatively compact in C([a, b]). We already know from (a) that
B is bounded in C([a, b]). We estimate f ∈ B with the help of Hölder’s inequality as
follows:
| f (x) − f (y)| =




∫ x
y
f 
(t) dt




≤
∫ x
y
| f 
(t)|2 dt 1
2
|y − x|
1
2
≤  f H1 |y − x|
1
2 ≤ |y − x|
1
2 .
This means that B is equicontinuous, and the claim follows from the Arzelà–Ascoli
theorem (Theorem A.6).
3. Since the imbedding of C([a, b]) in L2(a, b) is continuous, the second claim of
the theorem follows from the first one. 
Next we shall provide a weak version of the product rule. Since weak derivatives
are defined via integrals, this rule can be proved via Fubini’s theorem in the form of
Lemma 5.8.
Theorem 5.13 (Product rule and integration by parts) Let f, g ∈ H1(a, b). Then
(a) f · g ∈ H1(a, b) and ( f · g)
 = f 
g + f g
;
(b) integration by parts:
∫ b
a
f (x) g
(x) dx = f (b) g(b) − f (a) g(a) − ∫ b
a
f 
(x) g(x) dx.
Proof 1. By Lemma 5.8 we have
∫ b
a
f (x) g
(x) dx =
∫ b
a

f (a) +
∫ x
a
f 
(y) dy

g
(x) dx
= f (a) g(b) − f (a) g(a) +
∫ b
a
∫ b
y
g
(x) dx f 
(y) dy
= f (a) g(b) − f (a) g(a) +
∫ b
a
(g(b) f 
(y) − g(y) f 
(y)) dy162 5 Sobolev spaces and boundary value problems in dimension one
= f (a) g(b) − f (a) g(a) + g(b) f (b) − g(b) f (a)
−
∫ b
a
g(y) f 
(y) dy.
This proves part (b).
2. If in 1. we replace the upper limit of integration b by x ∈ [a, b), then we obtain
∫ x
a
f (y) g
(y) dy = f (x) g(x) − f (a) g(a) − ∫ x
a
f 
(y) g(y) dy,
and thus
f (x) g(x) = f (a) g(a) +
∫ x
a
{ f (y) g
(y) + f 
(y) g(y)} dy.
It now follows from Theorem 5.9 that f · g ∈ H1(a, b) and ( f · g)
 = f 
g + f g
.

We next show using integration by parts how one can glue together H1-functions
defined piecewise.
Theorem 5.14 Let −∞ < a = t0 < t1 < ... < tn = b be a partition of the interval
[a, b]. If f ∈ C([a, b]) is such that f|(ti−1,ti) ∈ H1(ti−1, ti) for all i = 1,..., N, then
f ∈ H1(a, b).
Proof Denote by f 
i ∈ L2(ti−1, ti) the weak derivative of f|(ti−1,ti) and define a
function g ∈ L2(a, b) by g(t) := f 
i (t) for t ∈ (ti−1, ti), g(t) = 0 for t ∈ {t0, t1,..., tN }.
We will show that g is the weak derivative of f , which in particular implies that
f ∈ H1(a, b). To this end, we letv ∈ C1
c (a, b) be arbitrary and obtain, upon integrating
by parts,
−
∫ b
a
fv dt = −

N
i=1
∫ ti
ti−1
fv dt
=

N
i=1
 ∫ ti
ti−1
f 
i v dt − ( f (ti)v(ti) − f (ti−1)v(ti−1))
=
∫ b
a
gv dt − f (tN )v(tN ) + f (t0)v(t0) =
∫ b
a
gv dt,
since v(tN ) = v(t0) = 0. Hence f  = g. 
We will need the following special case of Theorem5.14 in Chapter 9 on numerical
methods. A function f : I → R is called affine if there exist α, β ∈ R such that
f (x) = αx + β for all x ∈ I.
Remark 5.15 Let a = t0 < t1 < ... < tN = b be a partition of the interval [a, b].
Then the space A := { f ∈ C([a, b]) : f(ti−1,ti) is affine, i = 1,..., N} of piecewise5.1 Sobolev spaces in one variable 163
affine functions is contained in H1(a, b). The weak derivative of a function f in A is
constant on each of the intervals (ti−1, ti), that is, f  is a step function. 
We next wish to define Sobolev spaces of higher order. We set
H2(a, b) := { f ∈ H1(a, b) : f  ∈ H1(a, b)}.
Then, for f ∈ H2(a, b), f  := ( f 
)
 is in L2(a, b). Since f  ∈ H1(a, b) → C([a, b])
and f (x) = f (a) + ∫ x
a f 
(y) dy, it follows that H2(a, b) ⊂ C1([a, b]). Similarly to
Theorem 5.6 we see that H2(a, b) is a Hilbert space with respect to the inner product
( f, g)H2 := ( f, g)L2 + ( f 
, g
)L2 + ( f , g)L2 .
More generally, for k ∈ N we define inductively
Hk+1
(a, b) := { f ∈ H1(a, b) : f  ∈ Hk (a, b)},
and for f ∈ Hk+1(a, b) we set f (k+1) := ( f 
)
(k)
. Then Hk (a, b) is a Hilbert space with
respect to the inner product
( f, g)Hk :=

k
m=0
( f (m)
, g(m)
)L2,
where f (0) := f . We then have
Hk+1(a, b) ⊂ Ck ([a, b]) for all k ∈ N, (5.4)
as one can see easily by induction (see Exercise 5.11).
Finally, we wish to consider those Sobolev functions which are zero on the
boundary. Since H1(a, b) → C([a, b]), the spaces
H1
(a)
(a, b) := { f ∈ H1(a, b) : f (a) = 0},
H1
(b)
(a, b) := { f ∈ H1(a, b) : f (b) = 0} and
H1
0 (a, b) := { f ∈ H1(a, b) : f (a) = f (b) = 0}
are closed subspaces of H1(a, b). The following estimate plays a central role in the
study of boundary value problems.
Theorem 5.16 (Poincaré inequality) We have
∫ b
a
u(x)
2 dx ≤
1
2
(b − a)
2
∫ b
a

u
(x)
	2 dx
for all u ∈ H1
(a)
(a, b)∪H1
(b)
(a, b) .164 5 Sobolev spaces and boundary value problems in dimension one
Proof For u ∈ H1
(a)
(a, b), (5.2) yields the representation u(x) = ∫ x
a u
(y) dy. Hence
we may apply Hölder’s inequality to obtain the estimate
|u(x)| ≤ (x − a)
1
2
∫ b
a

u
(y)
	2 dy
 1
2
.
It follows that
∫ b
a
u(x)
2 dx ≤
∫ b
a
(x − a) dx ∫ b
a

u
(y)
	2 dy = 1
2
(b − a)
2
∫ b
a

u
(y)
	2 dy,
as claimed. Given u ∈ H1
(b)
(a, b), apply the first case to v defined by v(x) := u(a+b−x)
to prove the estimate in the second case. 
The Poincaré inequality shows in particular that
|u|H1(a,b) :=
∫ b
a

u
(x)
	2 dx 1
2
= u
L2
defines an equivalent norm on H1
(a)
(a, b) and on H1
0 (a, b), since it implies that
uH1(a,b) ≤ c |u|H1(a,b), u ∈ H1
(a)
(a, b), (5.5)
for some constant c > 0. One should, however, be aware that |·|H1(a,b) only defines
a seminorm on H1(a, b), since | f |H1(a,b) = 0 for any constant function f .
5.2 Boundary value problems on the interval
The problems we will consider in this section involve linear differential equations
with various boundary conditions; we will use them to demonstrate the effectiveness
of Hilbert space methods. Everything is based on the theorem of Riesz–Fréchet or,
more generally, the Lax–Milgram theorem. The same arguments also carry over to
domains in Rd, that is, where we replace the second derivative by the Laplacian
in two or more dimensions. For this, however, we require Sobolev spaces in higher
dimensions, among other tools, to which we will turn in the next chapter. Here, in
this section, we will always work on a bounded interval (a, b) ⊂ R.
5.2.1 Dirichlet boundary conditions
We will start by studying Dirichlet boundary conditions. Let the constant λ be
nonnegative and the function f ∈ L2(a, b) be given. We wish to consider the problem
of determining u ∈ H2(a, b) such that5.2 Boundary value problems on the interval 165
λu − u = f in (a, b), (5.6a)
u(a) = u(b) = 0. (5.6b)
Suppose that u is a solution of (5.6). If we multiply (5.6a) by a function v ∈
H1
0 (a, b) and integrate, then by integration by parts (Theorem 5.13) we obtain
∫ b
a
f (x)v(x) dx= λ
∫ b
a
u(x)v(x) dx −
∫ b
a
u(x)v(x) dx
= λ
∫ b
a
u(x)v(x) dx +
∫ b
a
u
(x)v
(x) dx
− u
(b)v(b) + u
(a)v(a)
= λ
∫ b
a
u(x)v(x) dx +
∫ b
a
u
(x)v
(x) dx
since v(a) = v(b) = 0. This motivates us to consider the bilinear form
a(u, v) := λ
∫ b
a
u(x)v(x) dx +
∫ b
a
u
(x)v
(x) dx
on H1
0 (a, b)×H1
0 (a, b). It is easy to check that this form is continuous and symmetric;
moreover, it is coercive since a(u)≥|u|
2
H1(a,b) ≥ 1
c u2
H1(a,b) by (5.5), recall (4.5).
We also define a continuous linear form F on H1
0 (a, b) by
F(v) :=
∫ b
a
v(x) f (x) dx.
By the Lax–Milgram theorem (Theorem 4.23) there exists a unique u ∈ H1
0 (a, b)
such that
a(u, v) = F(v) for all v ∈ H1
0 (a, b). (5.7)
We will now show that this u is a solution of (5.6). The identity (5.7) says that
∫ b
a
u
(x)v
(x) dx =
∫ b
a
( f (x) − λ u(x) )v(x) dx
for all v ∈ H1
0 (a, b) and in particular for all v ∈ C1
c (a, b). Since u ∈ L2(a, b) and
f −λu ∈ L2(a, b), by Definition 5.2 this means that u ∈ H1(a, b), so that u ∈ H2(a, b)
and −u = f − λu. Since also u ∈ H1
0 (a, b), it is thus a solution of (5.6).
We next prove uniqueness. Let u be a solution of (5.6), that is, let u ∈ H1
0 (a, b) ∩
H2(a, b)satisfy λu−u = f . We already showed above that in this case a(u, v) = F(v)
for all v ∈ H1
0 (a, b). But then uniqueness in the Lax–Milgram theorem means that
only one such u can exist.166 5 Sobolev spaces and boundary value problems in dimension one
Now we wish to investigate the regularity of the solution u of (5.6). It follows
directly from the representation of a function w ∈ H1(a, b) as an indefinite integral
(Theorem 5.9) that w ∈ H1(a, b) is in C1([a, b]) if and only if w ∈ C([a, b]). More
generally, one sees by induction that, for k ∈ N and w ∈ Hk (a, b),
w ∈ Ck ([a, b]) ⇐⇒ w(k) ∈ C([a, b]). (5.8)
Now let u be the solution of (5.6). Since u = λu − f , it follows from (5.8) with
k = 2 that u ∈ C2([a, b]) if and only if f ∈ C([a, b]). We thus obtain inductively that,
for any k ∈ N, u ∈ Ck+2([a, b]) if (and only if) f ∈ Ck ([a, b]). Such a statement is
known as a shift theorem, since the regularity of the solution is increased (shifted)
by exactly the order of the differential operator (here 2), compared with the orther
of the right-hand side. Summarizing, we have proved the following statement about
solvability and regularity of problem (5.6). Here we set C0([a, b]) := C([a, b)].
Theorem 5.17 (Dirichlet boundary conditions) Let λ ≥ 0 and f ∈ L2(a, b) be
given. Then problem (5.6) has exactly one solution u ∈ H2(a, b)∩H1
0 (a, b). Moreover,
u ∈ Ck+2([a, b]) if f ∈ Ck ([a, b]) for some k ∈ N0.
The case of inhomogeneous Dirichlet boundary conditions can be reduced to
problem (5.6). Suppose that we wish to find u ∈ H2(a, b) such that
λu − u = f in (a, b), (5.9a)
u(a) = A, u(b) = B, (5.9b)
for given f ∈ L2(a, b) and A, B ∈ R. We may then choose any function g ∈ C∞([a, b])
such that g(a) = A and g(b) = B, and solve the homogeneous problem (5.6) with
right-hand side ˜f := f − λg + g; let us call this solution u0 ∈ H2(a, b) ∩ H1
0 (a, b).
Then u := u0 + g solves the inhomogeneous problem (5.9) since λu − u = λu0 −
u
0 + λg − g = ˜f + λg − g = f and u(a) = u0(a) + g(a) = A, as well as
u(b) = u0(b)+g(b) = B. We will also apply this method of reduction to homogeneous
Dirichlet boundary conditions to domains in R2, in Chapter 7. Here we have shown
that for all A, B ∈ R and f ∈ L2(a, b) there is exactly one function u ∈ H2(a, b)
which solves problem (5.9).
5.2.2 Neumann boundary conditions
Now we turn to the case of Neumann boundary conditions. Let λ > 0 and f ∈ L2(a, b)
be given. Here we seek a solution u ∈ H2(a, b) of the problem
λu − u = f in (a, b), (5.10a)
u
(a) = u
(b) = 0. (5.10b)
One should note that H2(a, b) ⊂ C1([a, b]), and thus the Neumann boundary condi￾tion (5.10b) is well defined.5.2 Boundary value problems on the interval 167
Theorem 5.18 (Neumann boundary conditions) Let f ∈ L2(a, b) and λ > 0.
Then there exists a unique function u ∈ H2(a, b) such that (5.10) holds. Moreover,
u ∈ Ck+2([a, b]) if f ∈ Ck ([a, b]) for some k ∈ N0.
Proof Suppose that u ∈ H2(a, b) is a solution of (5.10) and v ∈ H1(a, b). Integrating
by parts (Theorem 5.13), we obtain
∫ b
a
f (x)v(x) dx = λ
∫ b
a
u(x)v(x) dx −
∫ b
a
u(x)v(x) dx
= λ
∫ b
a
u(x)v(x) dx +
∫ b
a
u
(x)v
(x) dx (5.11)
− (u
(b)v(b)−u
(a)v(a))
= λ
∫ b
a
u(x)v(x) dx +
∫ b
a
u
(x)v
(x) dx, (5.12)
where we used (5.10b) in the last step. We now consider the Hilbert space V :=
H1(a, b) and the bilinear form a : V × V → R given by
a(u, v) := λ
∫ b
a
u(x)v(x) dx +
∫ b
a
u
(x)v
(x) dx.
Clearly, a(·, ·) is continuous and coercive, since it corresponds exactly to the inner
product on V (cf. Theorem 5.6). We also define a continuous linear form F ∈ V by
F(v) := ∫ b
a f (x)v(x) dx. We have seen that if u ∈ H2(a, b) is a solution of (5.10),
then a(u, v) = F(v) for all v ∈ V. But by the Lax–Milgram theorem we know that
there exists a unique function u ∈ V such that a(u, v) = F(v) for all v ∈ H1(a, b);
only this function u can be a solution. This already proves uniqueness.
We will now show that u is actually a solution. To this end, we note that by
definition of a and F, we have
∫ b
a
u
(x)v
(x) dx =
∫ b
a
( f (x) − λu(x))v(x) dx for all v ∈ H1(a, b). (5.13)
If we take v ∈ C1
c (a, b) in (5.13), then we see that u ∈ H2(a, b) and u = λu − f by
definition of weak derivatives. We only need to check the boundary condition. For
this, we substitute f − λu = −u into (5.13) and obtain
∫ b
a
u
(x)v
(x) dx = −
∫ b
a
u(x)v(x) dx for all v ∈ H1(a, b).
Integration by parts (Theorem 5.13) now yields
−
∫ b
a
u(x)v(x) dx =
∫ b
a
u
(x)v
(x) dx − u
(b)v(b) + u
(a)v(a)168 5 Sobolev spaces and boundary value problems in dimension one
and thus
−u
(b)v(b) + u
(a)v(a) = 0 (5.14)
for all v ∈ H1(a, b). By choosing v ∈ H1(a, b) such that v(a) = 1 and v(b) = 0,
we see that u
(a) = 0. We now set v ≡ 1 ∈ H1(a, b) in (5.14) and conclude that
u
(b) = 0. Thus u is a solution of (5.10). The statement about regularity follows from
(5.8). 
One can treat inhomogeneous Neumann boundary conditions analogously to
inhomogeneous Dirichlet boundary conditions, namely by reduction to homogeneous
conditions.
5.2.3 Robin boundary conditions
Next, we wish to consider Robin boundary conditions. Let β0, β1 ≥ 0, λ > 0 and
f ∈ L2(a, b) be given.
Theorem 5.19 (Robin boundary conditions) There exists a unique u ∈ H2(a, b)
such that
λu − u = f in (a, b), (5.15a)
−u
(a) + β0u(a) = 0, u
(b) + β1u(b) = 0. (5.15b)
The condition (5.15b) is called a Robin boundary condition, or sometimes a
boundary condition of the third kind. Since H2(a, b) ⊂ C1([a, b]), this condition
makes sense for u ∈ H2(a, b). In the special case β0 = β1 = 0 we recover Neumann
boundary conditions.
Proof (of Theorem 5.19) Suppose that u ∈ H2(a, b) is a solution of (5.15). We then
have, for v ∈ H1(a, b),
∫ b
a
f (x)v(x) dx =
∫ b
a
λu(x)v(x) dx −
∫ b
a
u(x)v(x) dx
=
∫ b
a
λu(x)v(x) dx +
∫ b
a
u
(x)v
(x) dx − u
(b)v(b) + u
(a)v(a)
= λ
∫ b
a
u(x)v(x) dx +
∫ b
a
u
(x)v
(x) dx + β1 u(b)v(b) + β0 u(a)v(a).
This observation leads us to set V := H1(a, b) and define a : V × V → R by
a(u, v) := λ
∫ b
a
u(x)v(x) dx +
∫ b
a
u
(x)v
(x) dx + β1 u(b)v(b) + β0 u(a)v(a).5.2 Boundary value problems on the interval 169
Then a(·, ·) is bilinear, continuous and coercive. We again set
F(v) :=
∫ b
a
f (x)v(x) dx,
so that F ∈ V is a continuous linear form. Again by the Lax–Milgram theorem there
exists a unique u ∈ V such that a(u, v) = F(v) for all v ∈ H1(a, b). Hence in the
special case of a function v ∈ C1
c (a, b) we have
∫ b
a
u
(x)v
(x) dx =
∫ b
a
( f (x) − λu(x))v(x) dx,
and thus u ∈ H2(a, b) with −u = f −λu, by definition of weak derivatives. We then
substitute this identity into the relation a(u, v) = F(v), v ∈ H1(a, b) and obtain
−
∫ b
a
u(x)v(x) dx =
∫ b
a
( f (x) − λu(x))v(x) dx
= a(u, v) − λ
∫ b
a
u(x)v(x) dx
=
∫ b
a
u
(x)v
(x) dx + β1 u(b)v(b) + β0 u(a)v(a)
for all v ∈ H1(a, b). On the other hand, integrating by parts we see that
−
∫ b
a
u(x)v(x) dx =
∫ b
a
u
(x)v
(x) dx − u
(b)v(b) + u
(a)v(a)
and thus −u
(b)v(b) + u
(a)v(a) = β1 u(b)v(b) + β0 u(a)v(a) for all v ∈ H1(a, b).
It follows that −u
(b) = β1 u(b) and u
(a) = β0 u(a). Thus u is a solution of (5.15).
The Lax–Milgram theorem guarantees uniqueness of solutions. 
5.2.4 Mixed and periodic boundary conditions
We will now consider a somewhat more complicated differential operator. In Chapter
1 we saw how one can interpret spatial derivatives physically for problems involving
time and space variables. More precisely, we know that second derivatives describe
diffusion, first derivatives transport processes (convection) and terms of zeroth order
(i.e., no derivatives) reactive processes. The study of solutions which are constant
in time is an essential step towards understanding equations in space and time.
We thus wish to consider stationary equations in which terms of zeroth, first and
second order appear. Such general elliptic differential equations are referred to as
stationary diffusion-convection-reaction equations. As an example we will study170 5 Sobolev spaces and boundary value problems in dimension one
mixed boundary conditions, that is, Dirichlet boundary conditions at one endpoint
and Neumann boundary conditions at the other endpoint of the interval.
Theorem 5.20 (Mixed boundary conditions) Let p ∈ C1([a, b]) and r ∈ C([a, b])
with r ≥ 0. Suppose that there exists α > 0 such that p(x) ≥ α for all x ∈ [a, b], and
let f ∈ L2(a, b). Then there exists a unique solution u ∈ H2(a, b) of
−(p u
)
 + ru = f almost everywhere in (a, b), (5.16a)
u(a) = 0, u
(b) = 0. (5.16b)
If f ∈ C([a, b]), then u ∈ C2([a, b]).
One should observe that, by Theorem 5.13 (the product rule), pu ∈ H1(a, b) if
u ∈ H2(a, b), meaning that (5.16a) is well defined.
Proof (of Theorem 5.20) Suppose that u ∈ H2(a, b) is a solution of the boundary
value problem (5.16) and let v ∈ H1
(a)
(a, b) := {u ∈ H1(a, b) : u(a) = 0}. Then by
integrating by parts we obtain
∫ b
a
f (x)v(x) dx = −
∫ b
a
(p(x) u
(x)) v(x) dx +
∫ b
a
r(x) u(x)v(x) dx
=
∫ b
a
p(x) u
(x)v
(x) dx − p(b) u
(b)v(b) + p(a) u
(a)v(a)
+
∫ b
a
r(x) u(x)v(x) dx
=
∫ b
a
p(x) u
(x)v
(x) dx +
∫ b
a
r(x) u(x)v(x) dx
since u
(b) = 0 and v(a) = 0. It follows from Theorem 5.12 that the space H1
(a)
(a, b)
is closed in H1(a, b) and thus a Hilbert space. The bilinear form
a(u, v) :=
∫ b
a
p(x) u
(x)v
(x) dx +
∫ b
a
r(x) u(x)v(x) dx
is continuous on H1
(a)
(a, b) × H1
(a)
(a, b), since we have
|a(u, v)| ≤ c{u
L2 v
L2 + uL2 v L2 }
≤ 2c(u
2
L2 + u2
L2
)
1
2 (v
2
L2 + v 2
L2
)
1
2 = 2c uH1 v H1
for c := max{pC([a,b]), r C([a,b])}. Using the Poincaré inequality (5.5) and the
assumptions on the functions p and r, we can prove the coercivity of a(·, ·) as follows:
a(u) ≥ α
∫ b
a

u
(x)
	2 dx = α|u|
2
H1(a,b) dx ≥ α 1
c2 u2
H15.2 Boundary value problems on the interval 171
for all u ∈ H1
(a)
(a, b). Now let f ∈ L2(a, b), then F(v) := ∫ b
a f (x)v(x) dx defines a
continuous linear form F on H1
(a)
(a, b). By the Lax–Milgram theorem there exists a
unique u ∈ H1
(a)
(a, b) such that a(u, v) = ∫ b
a f (x)v(x) dx for all v ∈ H1
(a)
(a, b), that
is,
∫ b
a
p(x) u
(x)v
(x) dx +
∫ b
a
r(x) u(x)v(x) dx =
∫ b
a
f (x)v(x) dx (5.17)
for all v ∈ H1
(a)
(a, b). In particular, (5.17) holds for all v ∈ C1
c (a, b), whence pu ∈
H1(a, b) and −(pu
)
 = ( f −ru). If we substitute these identities into (5.17), then we
obtain by integration by parts that
∫ b
a
p(x) u
(x)v
(x) dx +
∫ b
a
r(x) u(x)v(x) dx =
∫ b
a
f (x)v(x) dx
= −
∫ b
a
(p(x) u
(x)) v(x) +
∫ b
a
r(x) u(x)v(x) dx
=
∫ b
a
p(x) u
(x)v
(x) dx − p(b) u
(b)v(b) + p(a) u
(a)v(a)
+
∫ b
a
r(x) u(x)v(x) dx
for all v ∈ H1
(a)
(a, b). Since v(a) = 0, it follows that −p(b) u
(b)v(b) = 0 for all
v ∈ H1
(a)
(a, b), and so u
(b) = 0. Since pu ∈ H1(a, b) and 1
p
∈ C1([a, b]) ⊂ H1(a, b),
it follows from Theorem 5.13 that u = 1
p (pu
) ∈ H1(a, b), that is, u ∈ H2(a, b).
Hence u is a solution of (5.16).
If u is a solution of (5.16), then u ∈ H1
(a)
(a, b), and the above argument shows
that a(u, v) = ∫ b
a f (x)v(x) dx for all v ∈ H1
(a)
(a, b). Uniqueness thus follows from
the Lax–Milgram theorem.
Finally, if f ∈ C([a, b]), then (pu
)
 = ru − f ∈ C([a, b]). Thus pu ∈ C1([a, b])
and so u ∈ C1(a, b]. It follows that u ∈ C2([a, b]). 
Periodic boundary conditions can also be treated using the Lax–Milgram theorem.
Theorem 5.21 (Periodic boundary conditions) Let p ∈ C1([a, b]) and r ∈ C([a, b])
be such that p(a) = p(b) and p(x) ≥ α > 0, as well as r(x) ≥ α > 0, for some α > 0
and all x ∈ [a, b]. Let f ∈ L2(a, b) be given. Then there exists a unique solution
u ∈ H2(a, b) of
−(pu
)
 + ru = f almost everywhere in (a, b), (5.18a)
u(a) = u(b), u
(a) = u
(b). (5.18b)
If f ∈ C([a, b]), then u ∈ C2([a, b]).172 5 Sobolev spaces and boundary value problems in dimension one
Proof We consider the space H1
per(a, b) := { f ∈ H1(a, b) : f (a) = f (b)}. Since
H1(a, b) is continuously imbedded in C([a, b]), the space H1
per(a, b) is a closed
subspace of H1(a, b) and thus a Hilbert space. The mapping
a(u, v) :=
∫ b
a
{p(x) u
(x)v
(x) + r(x) u(x)v(x)} dx
defines a continuous bilinear form on H1
per(a, b); since
a(u) ≥ α
∫ b
a

u
(x)
	2
+ u(x)
2

dx,
it is coercive. Now let f ∈ L2(a, b). By the Lax–Milgram theorem there exists a
unique u ∈ H1
per(a, b) such that a(u, v) = ∫ b
a f (x)v(x) dx for all v ∈ H1
per(a, b). If
we choose v ∈ C1
c (a, b), then it follows from the definition of weak derivatives that
−(pu
) ∈ H1(a, b) and −(pu
)
 + ru = f . Since pu ∈ H1(a, b), it follows from
Theorem 5.13 that u = 1
p (pu
) ∈ H1(a, b), and so u ∈ H2(a, b). If we substitute this
expression for f , then we obtain
∫ b
a
{p(x) u
(x)v
(x) + r(x) u(x)v(x)} dx = a(u, v) =
∫ b
a
f (x)v(x) dx
=
∫ b
a
{−(p(x) u
(x)) v(x) + r(x) u(x)v(x)} dx
=
∫ b
a
{p(x) u
(x)v
(x) + r(x) u(x)v(x)}dx
− p(b) u
(b)v(b) + p(a) u
(a)v(a)
for all v ∈ H1
per(a, b). Since v(a) = v(b) and p(a) = p(b), it follows that
0 = −p(b) u
(b)v(b) + p(a) u
(a)v(a) = (−u
(b) + u
(a)) p(a)v(a)
for all v ∈ H1
per(a, b). This leads to u
(a) = u
(b), and thus u is a solution of (5.18).
By inverting the above argument, one sees that, for any solution u of (5.18),
the equation a(u, v) = ∫ b
a f (x)v(x) dx holds for all v ∈ H1
per(a, b). Thus the Lax–
Milgram theorem also yields uniqueness. 
5.2.5 Non-symmetric differential operators
All problems we have considered thus far lead to symmetric bilinear forms. As
such, we could equally have used the theorem of Riesz–Fréchet in place of the Lax–
Milgram theorem. Now we will add a term to the differential operator of Theorem
5.20 which will make the associated form in fact non-symmetric. To keep things5.2 Boundary value problems on the interval 173
simple, we will limit ourselves to Dirichlet boundary conditions. In the proof of
coercivity we will use a simple but extremely useful inequality:
Lemma 5.22 (Young’s inequality) Let α, β ≥ 0 and ε > 0. Then
αβ ≤ εα2 +
1
4ε β2. (5.19)
Proof Since 0 ≤ (α − β)
2 = α2 − 2αβ + β2, we have αβ ≤ 1
2α2 + 1
2 β2. If, in this
inequality, we replace α by √
2εα and β by 1
√
2ε β, then we obtain (5.19). 
Theorem 5.23 Let p ∈ C1([a, b]) and r, q ∈ C([a, b]). Suppose that there exist
0 <β<α such that p(x) ≥ α and q(x)
2 ≤ 4βr(x) for all x ∈ [a, b]. Let f ∈ L2(a, b)
be given. Then there exists a unique function u ∈ H2(a, b) such that
−(pu
)
 + qu + ru = f almost everywhere in (a, b), (5.20a)
u(a) = u(b) = 0. (5.20b)
If f ∈ C([a, b]), then u ∈ C2([a, b]).
Proof This time, we define a continuous bilinear form on H1
0 (a, b) by
a(u, v) :=
∫ b
a
{p(x) u
(x)v
(x) + q(x) u
(x)v(x) + r(x) u(x)v(x) } dx.
Now
∫ b
a
p(x)

u
(x)
	2 dx ≥ α
∫ b
a

u
(x)
	2 dx,
and from Young’s inequality it follows that
|q(x) u
(x) u(x)| ≤ β

u
(x)
	2
+
1
4β
q(x)
2 u(x)
2
for u ∈ H1
0 (a, b). Hence
q(x) u
(x) u(x) ≥ −|q(x) u
(x) u(x)| ≥−β

u
(x)
	2 − 1
4β
q(x)
2
u(x)
2
and so, for all u ∈ H1
0 (a, b),
a(u) ≥ α
∫ b
a

u
(x)
	2 dx − β
∫ b
a

u
(x)
	2 dx +
∫ b
a

r(x) − q(x)
2
4β

u(x)
2 dx174 5 Sobolev spaces and boundary value problems in dimension one
≥ (α − β)
∫ b
a

u
(x)
	2 dx,
since (r − q2/4β) ≥ 0 by assumption. It now follows from the Poincaré inequality
(5.5) that a(·, ·) is coercive.
Now let f ∈ L2(a, b); then by the Lax–Milgram theorem there exists a unique
u ∈ H1
0 (a, b) such that a(u, v) = ∫ b
a f (x)v(x) dx for all v ∈ H1
0 (a, b). By choosing
v ∈ C1
c (a, b) in Section 5.2.1 or the proof of Theorem 5.18, we deduce that pu ∈
H1(a, b) and −(pu
)
 + qu + ru = f . It follows that u = 1
p (pu
) ∈ H1(a, b); hence
u ∈ H2(a, b), and u is a solution of (5.20). This proves existence.
Uniqueness follows once again from the Lax–Milgram theorem: one sees easily
that for each solution u ∈ H2(a, b) of (5.20) the equation a(u, v) = ∫ b
a f (x)v(x) dx
holds for all v ∈ H1
0 (a, b).
It remains to prove the regularity statement. Since u ∈ H2(a, b) ⊂ C1([a, b]), we
have qu+ru ∈ C([a, b]). Now if f ∈ C([a, b]), then it follows that (pu
)
 ∈ C([a, b]).
By Corollary 5.11, we have pu ∈ C1([a, b]); since 1
p
∈ C1([a, b]), we conclude that
u ∈ C2([a, b]). 
We say that equation (5.20a) is given in divergence form (since the leading term is
(pu
)

). Equations which are not in divergence form can easily be reduced to (5.20a):
Corollary 5.24 Given p, q,r ∈ C([a, b]), let α > 0 and 0 <β< 1 be constants
such that p(x) ≥ α > 0 and q(x)
2 ≤ 4βp(x)r(x) for all x ∈ [a, b]. Then for each
f ∈ L2(a, b) there exists a unique u ∈ H2(a, b) such that
−pu + qu + ru = f almost everywhere in (a, b), (5.21a)
u(a) = u(b) = 0. (5.21b)
If f ∈ C([a, b]), then u ∈ C2([a, b]).
Proof By Theorem 5.23 there exists a unique u ∈ H2(a, b) ∩ H1
0 (a, b) such that
−u + q
p u + r
p = f
p . This is equivalent to (5.21). If f ∈ C([a, b]), then also
f
p
∈ C([a, b]), and thus u ∈ C2([a, b]) by Theorem 5.23. 
5.2.6* A variational approach to singularly perturbed problems and
the transport equation
In this subsection we discuss another case where we have coercivity (in which, however, the
coercivity constant can be arbitrarily small), and then an example in which the Banach–Nečas
theorem can be used in the absence of coercivity.5.2 Boundary value problems on the interval 175
Example 5.25 Let 0 <ε< 1 be “small” and let f ∈ L2(0, 1). We consider the problem
−εu(x) + u
(x) + u(x) = f (x) for all x ∈ (0, 1),
u(0) = u(1) = 0.
The (non-symmetric) bilinear form a for the weak formulation of this boundary value problem
on V := H1
0 (0, 1) reads a(u, v) := ε(u
, v
)L2 + (u + u, v)L2 . It is easy to verify that a is
continuous. Regarding the coercivity, for all u ∈ H1
0 (0, 1) we have by Theorem 5.13 (b) that
(u
, u)L2 = −(u, u
)L2 , hence
(u
, u)L2 =
∫ 1
0
u
(x) u(x) dx = 0,
and so a(u, u) = ε u 2
L2 + u 2
L2 ≥ ε u 2
H1 . Hence the bilinear form is coercive with constant
of coercivity α = ε, and the above problem is well-posed. In fact, as in Theorem 5.23 one sees that
for each f ∈ L2(0, 1), there exists a unique solution u ∈ H2(0, 1). We will see later (Céa’s lemma,
Theorem 9.14) that the reciprocal of the coercivity constant has substantial influence on the error
analysis of finite element methods in numerics. Clearly, in this regard α = ε is an issue, which is
also the reason why such problems are called singulary perturbed. 
In the above example the limit as ε → 0 is of particular interest. As the diffusion part −εu(x)
vanishes when ε → 0, the second order problem is replaced by a first order problem. This also
implies that in the limit we have “too many” boundary conditions. One option for the limit is the
initial value transport problem:
u
(x) + u(x) = f (x) for all x ∈ (0, 1), (5.22a)
u(0) = 0. (5.22b)
We can also derive a variational formulation of this problem; here, however, we require different
trial and test spaces: we take V := H1
(0)
(0, 1) := {v ∈ H1(0, 1) : v(0) = 0} and W := L2(0, 1) and
set
b : V × W → R, b(v, w) := (v + v, w)L2 =
∫ 1
0
(v
(x) + v(x)) w(x) dx.
It is easy to show using the Cauchy–Schwarz inequality that b is continuous. We now show that the
theorem of Banach–Nečas (Theorem 4.27) guarantees well-posedness. To this end, we first check
the inf-sup condition (4.11): let 0  v ∈ V; then v + v ∈ W and
sup
w∈W
b(v, w)
wW
≥
b(v, v + v)
v + v W
= (v + v, v + v)L2
v + v L2
= v + v L2 .
Moreover, by Theorem 5.13,
v + v 2
L2 = (v + v, v + v)L2 = v
2
L2 + v 2
L2 + 2(v
, v)L2
= v
2
L2 + v 2
L2 + v(1)
2 ≥ v 2
H1,
and thus
sup
w∈W
b(v, w)
wW
≥ v + v L2 ≥ v H1 .
This estimate obviously also holds for v = 0; we have thus established (4.11) with β = 1. To prove
(4.13), we let 0  w ∈ W and define v(x) := ∫ x
0 w(s) ds. Then v ∈ V, v
(x) = w(x) and by
Theorem 5.13 (b)176 5 Sobolev spaces and boundary value problems in dimension one
b(v, w) = (v + v, w)L2 = w2
L2 + (v, v
)L2 = w2
L2 + 1
2
v(1)
2 > 0.
Thus the hypotheses of Theorem 4.27 are satisfied. Consequently, for all f ∈ L2(0, 1), there is a
unique v ∈ V such that b(u, v) = (f, w)L2 for all w ∈ W. This means that for all f ∈ L2(0, 1) there
exists a unique solution u ∈ H1(0, 1) of (5.22).
5.3* Comments on Chapter 5
The Fundamental Theorem of Calculus was discovered separately by Newton and Leibniz in 1680.
Of course, before the development of the Lebesgue integral it could not be stated in the form of
Theorem 5.9. And indeed, Lebesgue proved in 1904 that, for f ∈ L1(a, b) and its Lebesgue integral
F(t) := ∫ t
a f (s) ds, we have
lim
h→0
F(t + h) − F(t)
h = f (t)
for almost every t ∈ (a, b) (see [54, Ch. 7]). Since the notion of weak derivatives, which are defined
via integrals, is inherently simpler and more useful for treating partial differential equations, we
will not dwell further on this “almost everywhere” derivative. Further results on Sobolev spaces in
one variable can be found in the highly recommendable book [18] by H. Brezis.
5.4 Exercises
Exercise 5.1 Prove the statement of Example 5.5.
Exercise 5.2 Determine the weak derivative of the “hat function”
h(x) :=

x, 0 ≤ x ≤ 1,
2 − x, 1 < x ≤ 2.
Exercise 5.3 Let f ∈ L1(a, b) be given, and define the function g : [a, b] → R by
g(x) := ∫ x
a f (y) dy. Show that g is continuous.
Suggestion: Write g(x) = ∫ b
a 1[a,x](y) f (y) dy. Let xn ∈ [a, b], limn→∞ xn = x.
Show that 1[a,xn](y) f (y) → 1[a,x](y) f (y) and apply the Dominated Convergence
Theorem.
Exercise 5.4 Let λ > 0, A, B ∈ R, f ∈ L2(a, b). Show that there exists a unique
function u ∈ H2(a, b) such that λu − u = f in (a, b) and u
(a) = A, u
(b) = B.
Suggestion: Proceed as with the inhomogeneous Dirichlet boundary conditions just
after Theorem 5.17.
Exercise 5.5 (a) Show that ∫ 1
0 u(x)
2 dx ≤ 2

u(0)
2 + ∫ 1
0 (u
)(x))2 dx	 for all u ∈
H1(0, 1).5.4 Exercises 177
(b) Let α > 0. Show that a(u, v) := αu(0)v(0) +
∫ 1
0 u
(x)v
(x) dx defines a coercive
form on H1(0, 1).
(c) Let α > 0 and f ∈ L2(0, 1). Show that there exists a unique u ∈ H2(0, 1) such
that
−u(x) = f (x), x ∈ (0, 1),
−u
(0) + αu(0) = 0, u
(1) = 0.
Exercise 5.6 Suppose that the assumptions of Theorem 5.23 are satisfied, and in
addition r(x) > 0 for all x ∈ [a, b]. Show that, given f ∈ L2(a, b), there exists a
unique u ∈ H2(a, b) such that
−(pu
)
 + qu + ru = f almost everywhere in (a, b)
u
(a) = u
(b) = 0.
Exercise 5.7 Let −∞ < a < b < ∞ and f ∈ H2(a, b).
(a) Show that f  = 0 if and only if there exist constants c0, c1 ∈ R such that
f (x) = c0 + c1 x almost everywhere in (a, b).
(b) Show that f (x) = f (a) + f 
(a)(x − a) +
∫ x
a (x − y) f (y) dy almost everywhere.
Exercise 5.8 (Locally integrable weak derivatives) Let −∞ ≤ a < b ≤ ∞. In this
exercise, we will consider the vector space
L1,loc(a, b) :=

f : (a, b) → R measurable:
∫ d
c
| f (x)| dx < ∞ if a < c < d < b

of locally integrable functions on (a, b).
(a) Let f ∈ L1,loc(a, b) be such that ∫ b
a fv dx = 0 for all v ∈ C1
c (a, b). Show that
then f (x) = 0 almost everywhere. Suggestion: Use Lemma 5.1.
(b) We say that f ∈ L1,loc(a, b) is weakly differentiable if there exists a function
f  ∈ L1,loc(a, b) such that
−
∫ b
a
f (x)v
(x) dx =
∫ b
a
f 
(x)v(x) dx
for all v ∈ C1
c (a, b). Show that given f , there can be at most one such weak
derivative f 
.
(c) Show that
W1
1,loc(a, b) := { f ∈ L1,loc(a, b) : f is weakly differentiable}
is a vector space, and that f → f  : W1
1,loc(a, b) → L1,loc(a, b) is linear.178 5 Sobolev spaces and boundary value problems in dimension one
(d) Let f ∈ W1
1,loc(a, b) be such that f  = 0 almost everywhere. Show that there
exists c ∈ R such that f (x) = c almost everywhere. Suggestion: Modify the
proof of Lemma 5.7.
(e) Let f ∈ W1
1,loc(a, b) and x0 ∈ (a, b). Show that there exists c ∈ R such that
f (x) = c +
∫ x
x0 f 
(y) dy almost everywhere. Suggestion: Proceed analogously to
the proof of Theorem 5.9.
(f) Let f ∈ W1
1,loc(a, b) and c1 ∈ R, such that f 
(x) = c1 almost everywhere. Show
that there exists a constant c0 ∈ R such that f (x) = c1 x + c0 almost everywhere.
Exercise 5.9 (Harmonic functions) Let −∞ ≤ a < b ≤ ∞, and suppose that
f ∈ L2(a, b) satisfies
∫ b
a
f (x)v(x) dx = 0 for all v ∈ C2
c (a, b).
Show that f (x) = c0 + c1 x almost everywhere, for some constants c0, c1 ∈ R.
Note: Here Ck
c := {v : (a, b) → R : k times continuously differentiable , ∃a < c <
d < b, such that v(x) = 0, if x  [c, d]}, k ∈ N ∪ {∞}. Look for inspiration from
the proof of Lemma 5.7 and from Exercise 5.7.
Exercise 5.10 Let λ ∈ R, u0 ∈ R and f ∈ L2(0, T), where 0 < T < ∞, and set
u(t) = e−λt
u0 +
∫ t
0 e−λ(t−s) f (s) ds. Show that u is the unique solution of the problem
u ∈ H1(0, T), u
(t) = −λu(t) + f (t), u(0) = u0. (5.23)
Exercise 5.11 (a) Prove by induction that the inclusion
Hk+1(a, b) ⊂ Ck ([a, b])
holds for all k = 0, 1, 2, ..., where C0([a, b]) := C([a, b]).
(b) Show that Ck ([a, b]) is a Banach space with respect to the norm
 f Ck =

k
m=0
 f (m)
C([a,b]),
where f (0) = f .
(c) Show using the closed graph theorem (Theorem A.2) that Hk+1(a, b) →
Ck ([a, b]), that is, show that this imbedding is continuous.
Exercise 5.12 (Absolute convergence of Fourier series)
(a) Let f = u + iv, where u, v ∈ H1(0, 2π) and f (0) = f (2π). Show that the Fourier
series of f is absolutely convergent (to f ).
Suggestion: If ck is the kth Fourier coefficient of f (corresponding to (3.17))
and c
k is the kth Fourier coefficient of f  := u + iv
, then c
k = ikck .5.4 Exercises 179
(b) Deduce Dirichlet’s theorem from (a): if f ∈ C2π is piecewise continuously
differentiable (that is, there exist 0 = t0 < t1 < ··· < tn = 2π and gi ∈
C1([ti−1, ti]) such that f = gi on (ti−1, ti), i = 1,..., n), then the Fourier series of
f converges uniformly to f .
Suggestion: Use Theorem 5.14.
Exercise 5.13 (Weak solution of the wave equation) Consider
utt = uxx, t, x ∈ R (5.24)
on R2, where u = u(t, x).
(a) Suppose that u ∈ C2(R2) is a solution of (5.24). Show that then
∫
R
∫
R
u(t, x)(ϕtt(t, x) − ϕxx(t, x)) dx dt = 0 (5.25)
for all ϕ ∈ C2
c (R2).
(b) Let f : R → R be continuous and define u : R2 → R2 by u(t, x) = 1
2 ( f (x + t) +
f (x − t)), so that u is also continuous. Show that u is a weak solution of (5.24),
that is, (5.25) holds for all ϕ ∈ C2
c (R2).
Exercise 5.14 Define H1(R) analogously to how it is defined bounded intervals.
Show that H1(R) ⊂ C0(R) := {u ∈ C(R) : lim|x |→∞ u(x) = 0}.Chapter 6
Hilbert space methods for elliptic equations
We started by introducing Sobolev spaces in dimension one only, or more precisely
for functions defined on an open interval in R, in order to try and convey the
essential points of the mathematical theory in as elementary a way as possible.
In one space dimension, H1-functions are automatically continuous, and they can
be characterized by an indefinite integral (Theorem 5.9). This is no longer true in
higher dimensions, and new arguments are necessary in order to deduce properties
of weakly differentiable functions and Sobolev spaces.
In this chapter we will introduce Sobolev spaces in arbitrary space dimensions.
We limit ourselves to the Hilbert space theory, which is to say Sobolev spaces which
are built on L2(Ω) (and not Lp(Ω) for p  2). With the aid of the Lax–Milgram
theorem, we will then be in a position to study numerous elliptic problems. One can
also characterize Sobolev spaces on the whole space Ω = Rd in terms of Fourier
transforms; this can for example be used to prove interior regularity of solutions.
In this chapter we will restrict ourselves to Dirichlet boundary conditions. In doing
so, the statements which we make here require no deep analysis of the boundary and
are therefore comparatively simple to prove. A more detailed study of the boundary
will be undertaken in the next chapter.
Chapter overview
6.1 Mollifiers ...................................... 182
6.2 Sobolev spaces on Ω ⊆ Rd ............................. 189
6.3 The space H1
0 (Ω) .................................. 196
6.4 Lattice operations on H1(Ω) ............................ 200
6.5 The Poisson equation with Dirichlet boundary conditions ............. 204
6.6 Sobolev spaces and Fourier transforms ...................... 207
6.7 Local regularity ................................... 213
6.8 Inhomogeneous Dirichlet boundary conditions .................. 219
6.9 The Dirichlet problem ............................... 222
6.10 Elliptic equations with Dirichlet boundary conditions ............... 231
6.11 H2-regularity .................................... 233
6.12* Comments on Chapter 6 .............................. 236
6.13 Exercises ...................................... 237
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4_6
181182 6 Hilbert space methods for elliptic equations
6.1 Mollifiers
In this section we will introduce an important technique which will permit us to
approximate integrable functions by smooth functions. It is based on convolution
with smooth functions.
Let Ω be an open set in Rd. We say that the function u : Ω → R has compact
support if there exists a compact set K ⊂ Ω such that u(x) = 0 for all x ∈ Ω \ K. If
u is also continuous, then as before (cf. (2.1)) we call the set
supp u := {x ∈ Ω : u(x)  0}
the support of u. We denote by Cc(Ω) the space of all continuous real-valued
functions on Ω with compact support, cf. (3.105). For k ∈ N we also set
Ck
c (Ω) := Ck (Ω) ∩ Cc(Ω), C0
c (Ω) := Cc(Ω). (6.1)
The space
D(Ω) := C∞
c (Ω) := C∞(Ω) ∩ Cc(Ω) (6.2)
plays a special role; its elements are called test functions on Ω. The following special
test function  ∈ D(Rd) will be of particular importance:
(x) :=

c exp  1
|x |
2−1

, if |x| < 1,
0, if |x| ≥ 1, (6.3)
where c > 0 is chosen in such a way that
∫
Rd
(x) dx = 1. (6.4)
With the usual notation
B(x,r) := {y ∈ Rd : |x − y| < r}, B¯(x,r) := {y ∈ Rd : |x − y| ≤ r}
for x ∈ Rd and r > 0, we then have:
Lemma 6.1 The function  belongs to D(Rd) and supp = B¯(0, 1).
Proof Set
g(r) :=

c exp  1
r−1

, for r < 1,
0, for r ≥ 1.
Then g ∈ C∞(R). The function b(x) := |x|
2 = x2
1 + ··· + x2
d is in C∞(Rd), and so
 = f ◦ b ∈ C∞(Rd). It is immediate from the definition that supp = B¯(0, 1). 6.1 Mollifiers 183
We can define a whole sequence of test functions based on .
Definition 6.2 For n ∈ N we set
n(x) := nd (nx), x ∈ Rd. (6.5)
We call (n)n∈N a sequence of mollifiers (one sometimes sees the name Friedrichs
mollifier). We will always denote the functions defined in (6.5) and (6.3) by n and
, respectively. 
Figure 6.1 shows a few examples of the functions n in dimensions d = 1, 2.
Obviously, 1 = ,
0 ≤ n ∈ D(Rd), suppn ⊂ B¯

0,
1
n

, (6.6)
and
∫
Rd
n(x) dx = 1. (6.7)
Fig. 6.1 Mollifiers in one and two dimensions.
We will use these functions to smoothen non-smooth functions, as the name
“mollifier” already obliquely indicates. This is done using convolutions, a process
which we now describe in detail. For f, g ∈ L2(Rd) we define the convolution f ∗ g
of f and g analogously to the case d = 1 in Chapter 3, by
( f ∗ g)(x) :=
∫
Rd
f (x − y)g(y) dy (6.8)
for all x ∈ Rd. One should observe that f (x − ·) ∈ L2(Rd) for almost every x ∈ Rd,
so that ( f ∗ g)(x) = ( f (x − ·), g)L2(Rd) and
|( f ∗ g)(x)| ≤  f L2 gL2 (6.9)184 6 Hilbert space methods for elliptic equations
for all x ∈ Rd. In what follows, we will denote the sum of two sets K1, K2 ⊂ Rd by
K1 + K2 := {x + y : x ∈ K1, y ∈ K2}. Then the following statement holds.
Lemma 6.3 Let f, g ∈ L2(Rd) be such that f (x) = 0 whenever x  K1, and g(x) = 0
whenever x  K2. Then ( f ∗ g)(x) = 0 whenever x  (K1 + K2). If f and g are
continuous, then this may be written in the form
supp ( f ∗ g) ⊂ supp f + supp g. (6.10)
Proof If f (x − y)g(y)  0, then x − y ∈ K1 and y ∈ K2, that is, x ∈ K1 + K2. So if
x  K1 + K2, then f (x − y)g(y) = 0 for all y ∈ Rd and thus ( f ∗ g)(x) = 0. 
One can also make sense of formula (6.10) even if f and g are not continuous;
see Exercise 6.1. We next define the space
C0(Rd) =

f ∈ C(Rd) : lim
|x |→∞ f (x) = 0

. (6.11)
This is a Banach space with respect to the norm
 f ∞ := sup
x∈Rd
| f (x)|.
Theorem 6.4 Let f, g ∈ L2(Rd). Then f ∗ g ∈ C0(Rd) and f ∗ g = g ∗ f .
Proof Choose step functions fn and gn such that fn → f and gn → g in L2(Rd).
It follows by the Dominated Convergence Theorem that fn ∗ gn : Rd → R is
continuous. Indeed, if Q1 and Q2 are hyperrectangles in Rd (see Section A.3), then
1Q1 ∗ 1Q2 (x) = ∫
Q2 1Q1 (x − y) dy is continuous in x, since, if limn→∞ xn = x, then
1Q1 (xn − y) converges to 1Q(x − y) for all y ∈ Rd \ ∂Q1. Since the boundary ∂Q1
of Q1 has measure zero, the Dominated Convergence Theorem now implies that
limn→∞ 1Q1 ∗ 1Q2 (xn) = 1Q1 ∗ 1Q2 (x). This proves the continuity of fn ∗ gn.
Since fn and gn vanish outside the ball B(0,r), fn ∗ gn vanishes outside B(0, 2r),
and so fn ∗ gn ∈ Cc(Rd) ⊂ C0(Rd) (cf. Definition 6.1), and
|( fn ∗ gn)(x)−( f ∗ g)(x)| ≤ |( fn ∗ (gn − g))(x)| + |(( fn − f ) ∗ g)(x)|
≤  fn L2 gn − gL2 +  fn − f L2 gL2
→ 0, n → ∞
uniformly in x ∈ Rd. It follows that f ∗ g ∈ C0(Rd). Fubini’s theorem now implies
that f ∗ g = g ∗ f . 
Convolutions regularize functions in the following sense.
Theorem 6.5 Let f ∈ L2(Rd) and ϕ ∈ C1
c (Rd). Then ϕ ∗ f ∈ C1(Rd) and6.1 Mollifiers 185
∂
∂xj
(ϕ ∗ f ) = ∂ϕ
∂xj
∗ f, j = 1,..., d. (6.12)
In particular, if ϕ ∈ D(Rd), then ϕ ∗ f ∈ C∞(Rd).
For the proof we will use the following estimate together with the chain rule and
the Fundamental Theorem of Calculus. Let x, v ∈ Rd, then
ϕ(x + v) − ϕ(x) =
∫ 1
0
∂
∂t
ϕ(x + tv) dt =
∫ 1
0
∇ϕ(x + tv) · v dt,
where ∇ϕ(y) = (D1ϕ(y),..., Ddϕ(y)) ∈ Rd is the gradient of ϕ at the point y ∈ Rd
and x · y := xT y denotes the scalar product of x, y ∈ Rd. We thus have
∇ϕ(x + tv) · v =

d
j=1
∂ϕ
∂xj
(x + tv)vj,
and the estimate
|ϕ(x + v) − ϕ(x)| ≤ ∇ϕ∞ · |v| (6.13)
follows by the Cauchy–Schwarz inequality, where the norm of the gradient is given
by
∇ϕ∞ = sup
x∈Rd
|∇ϕ(x)|, |∇ϕ(x)| =

d
j=1
(Djϕ)(x)
2
1/2
.
We now come to the proof of the differentiation rule (6.12).
Proof (of Theorem 6.5) Let ej = (0,..., 0, 1, 0,..., 0)
T be the jth canonical basis
vector of Rd. Then limh→0
1
h (ϕ(x + hej − y) − ϕ(x − y)) = ∂ϕ
∂xj
(x − y). By (6.13),




1
h
(ϕ(x + hej − y) − ϕ(x − y))




≤ ∇ϕ∞.
Moreover,
(ϕ ∗ f )(x + hej)−(ϕ ∗ f )(x)
h =
∫
Rd
ϕ(x + hej − y) − ϕ(x − y)
h f (y) dy.
Passing to the limit as h → 0, it follows from the Dominated Convergence Theorem
that
∂
∂xj
(ϕ ∗ f )(x) =

∂ϕ
∂xj
∗ f

(x),
as asserted. 186 6 Hilbert space methods for elliptic equations
The following theorem shows how a function can be approximated by smooth
functions with the help of mollifiers.
Theorem 6.6 (Smoothing) Let f ∈ L2(Rd). Then n ∗ f ∈ C∞(Rd) ∩ L2(Rd) and
limn→∞ n ∗ f = f in L2(Rd).
Proof The proof proceeds in five steps.
1. It follows from Theorem 6.5 and an induction argument that n ∗ f ∈ C∞(Rd)
for all n ∈ N.
2. We will show that
∫
Rd
|(n ∗ f )(x)|2 dx ≤
∫
Rd
| f (x)|2 dx. (6.14)
Indeed, by Hölder’s inequality and (6.7),
|(n ∗ f )(x)| ≤ ∫
Rd
n(x − y)
1/2 n(x − y)
1/2| f (y)| dy
≤
 ∫
Rd
n(x − y) dy
1/2  ∫
Rd
n(x − y) f (y)
2 dy
1/2
=
 ∫
Rd
n(x − y) f (y)
2 dy
1/2
.
Hence, by Fubini’s theorem,
∫
Rd
(n ∗ f )(x))2 dx ≤
∫
Rd
∫
Rd
n(x − y) f (y)
2 dy dx
=
∫
Rd
∫
Rd
n(x − y) dx f (y)
2 dy =
∫
Rd
f (y)
2 dy,
where we have used (6.7). This establishes (6.14).
3. Let f = 1Q be the characteristic function of a d-dimensional rectangle Q;
then (n ∗ 1Q)(x) = ∫
|y |<1/n 1Q(x − y)n(y) dy converges to 1Q(x) as n → ∞
for all x ∈ Rd \ ∂Q. Since ∂Q has measure zero, it follows from the Dominated
Convergence Theorem that limn→∞ n ∗ 1Q = 1Q.
4. We see from 3. that n ∗ f → f in L2(Rd) whenever f is a step function, that
is, a linear combination of characteristic functions of rectangles.
5. Finally, let f ∈ L2(Rd) and ε > 0. By definition of the Lebesgue integral of f ,
there is a step function g such that  f − gL2 ≤ ε (see Theorem A.11). Hence, by
(6.14),
 f ∗ n − f L2 ≤  f ∗ n − g ∗ n L2 + g ∗ n − gL2 + g − f L2
≤ ( f − g) ∗ n L2 + g ∗ n − gL2 + ε
≤  f − gL2 + g ∗ n − gL2 + ε
≤ 2ε + g ∗ n − gL2 .6.1 Mollifiers 187
By 3., we have g ∗ n → g in L2(Rd) as n → ∞, whence
lim sup
n→∞
 f ∗ n − f L2 ≤ 2ε.
Since ε > 0 was arbitrary, the claim follows. 
Convolution with n permits us to define a large number of test functions, with
just a single function (namely  from (6.3)) as our starting point. The following test
functions will be particularly useful.
Lemma 6.7 Let Ω ⊂ Rd be open and K ⊂ Ω compact. Then there exists a function
ϕ ∈ D(Ω) such that 0 ≤ ϕ ≤ 1 and ϕ(x) = 1 for all x ∈ K.
Proof For n ∈ N such that 2
n < dist(K, ∂Ω), the set
Kn := K + B¯(0, 1/n) = {y ∈ Rd : ∃ x ∈ K such that |x − y| ≤ 1/n}
is compact. By Theorem 6.6, ϕ := 1Kn ∗ n ∈ C∞(Rd). Applying Lemma 6.3,
we have suppϕ ⊂ Kn + B¯(0, 1/n) ⊂ Ω. Now if x ∈ K, then x − y ∈ Kn for all
y ∈ B(0, 1/n), and so
ϕ(x) =
∫
|y |<1/n
1Kn (x − y)n(y) dy =
∫
|y |<1/n
n(y) dy = 1,
which proves the claim. 
We will often use the following notation. We will write
U  Ω (6.15)
if U is open and bounded, and U ⊂ Ω. This means in particular that U is compact;
it also means that U has positive distance to ∂Ω, that is,
dist(U, ∂Ω) := inf{|x − y| : x ∈ U, y ∈ ∂Ω} > 0.
In addition to the mollified functions n ∗ f we also wish to implement a cut-off
procedure in order to be able to localize our arguments; this is of particular value
when f does not have compact support. To this end, we define
Ωn :=

x ∈ Ω : dist(x, ∂Ω) >
1
n

∩ B(0, n). (6.16)
Then Ωn  Ωn+1  Ω and /
n∈N Ωn = Ω. By Lemma 6.7 we can find functions
ηn ∈ D(Rd), n ∈ N, such that
0 ≤ ηn(x) ≤ 1, x ∈ Rd, (6.17)
supp ηn ⊂ Ωn+1, (6.18)188 6 Hilbert space methods for elliptic equations
ηn(x) = 1, x ∈ Ωn. (6.19)
Given a function f ∈ L2(Ω), we define the extension of f by zero by
˜f (x) :=
 f (x) if x ∈ Ω,
0 if x ∈ Rd \ Ω. (6.20)
Then ηn(n ∗ ˜f ) ∈ D(Ω) for all n ∈ N and we arrive at the following result.
Theorem 6.8 (Smoothing and cut-off procedure) For f ∈ L2(Ω), we have
lim
n→∞ ηn(n ∗ ˜f ) = f in L2(Ω).
Proof We have
ηn(n ∗ ˜f ) − ˜f L2(Rd) ≤ ηn(n ∗ ˜f − ˜f )L2(Rd) + ηn ˜f − ˜f L2(Rd)
≤  n ∗ ˜f − ˜f L2(Rd) + ηn ˜f − ˜f L2(Rd)
by (6.17). Since limn→∞ ηn(x) = 1 for all x ∈ Ωn, it follows from the Dominated
Convergence Theorem that limn→∞ ηn ˜f = ˜f in L2(Rd). Now Theorem 6.6 implies
the claim. 
This theorem has two direct consequences.
Corollary 6.9 The space D(Ω) of test functions is dense in L2(Ω).
Corollary 6.10 Suppose that f ∈ L2(Ω) satisfies ∫
Ω f (x) ϕ(x) dx = 0 for all ϕ ∈
D(Ω). Then f = 0 almost everywhere.
To finish, we will show that if we mollify a uniformly continuous function, then
we obtain uniform convergence to the function as n → ∞.
Theorem 6.11 Let u : Rd → R be uniformly continuous. Then n ∗ u ∈ C∞(Rd)
and
lim
n→∞(n ∗ u)(x) = u(x)
uniformly in x ∈ Rd.
Here the convolution n ∗ u is defined by (6.8), as before.
Proof Fix x ∈ Rd and choose η ∈ D(Rd) such that η = 1 on B¯(x, 2). Then
u ∗ n = (ηu) ∗ n holds in a neighborhood of x. Since ηu ∈ L2(Rd), it follows from
Theorem 6.5 that u ∗ n ∈ C∞(Rd).
Now let ε > 0. Then there exists an n0 ∈ N such that |u(x − y) − u(x)| ≤ ε
whenever |y| ≤ 1/n0, and so, for all n ≥ n0,6.2 Sobolev spaces on Ω ⊆ Rd 189
|(n ∗ u)(x) − u(x)| =





∫
|y |<1/n

u(x − y) − u(x)

n(y) dy





≤ ε
∫
|y | ≤1/n
n(y) dy = ε,
which proves the claim. 
6.2 Sobolev spaces on Ω ⊆ Rd
After the preparatory work undertaken in Section 6.1 we can now introduce Sobolev
spaces on general domains in Rd. So let Ω be an open set in Rd; then as in Chapter 5
we shall define weak derivatives of functions on Ω via integration by parts. To start
with, we will suppose that f is a function which is continuously differentiable in the
classical sense, that is, f ∈ C1(Ω) with partial derivatives ∂f
∂xj
, j = 1,..., d.
Lemma 6.12 (Integration by parts) For f ∈ C1(Ω) and ϕ ∈ C1
c (Ω) we have
−
∫
Ω
f (x)
∂ϕ
∂xj
(x) dx =
∫
Ω
∂ f
∂xj
ϕ(x) dx. (6.21)
Proof 1st case: Ω = Rd. In this case, we have
−
∫
Rd
f (x)
∂ϕ
∂xj
(x) dx = −
∫
R
··· ∫
R
f (x1,..., xd) ∂ϕ
∂xj
(x1,..., xd) dx1 ··· dxd
=
∫
R
... ∫
R
∂ f
∂xj
(x1,..., xd) ϕ(x1,..., xd) dx1 ··· dxd
by integration by parts in the jth integral.
2nd case: Now suppose Ω is arbitrary. Choose U  Ω such that supp ϕ ⊂ U, and
also choose η ∈ D(Ω) such that η = 1 on supp ϕ and supp η ⊂ U (see Lemma 6.7).
Then 0f η ∈ C1(Rd) (where, as in the previous section, 0f η = ( f η)
∼ denotes the
extension of the function f η by zero in accordance with (6.20)). By the 1st case,
with the relevant extensions,
−
∫
Ω
f (x)
∂ϕ
∂xj
(x) dx = −
∫
Rd
0f η(x)
∂ϕ
∂xj
(x) dx
=
∫
Rd
∂(0f η)(x)
∂xj
ϕ(x) dx =
∫
Ω
∂ f
∂xj
(x) ϕ(x) dx,
since η = 1 on supp ϕ and supp ϕ ⊂ U  Ω. 190 6 Hilbert space methods for elliptic equations
We now take the identity (6.21) as the definition of the weak derivatives of a
function in L2(Ω): let f ∈ L2(Ω) and j ∈ {1,..., d}. If gj ∈ L2(Ω) satisfies
−
∫
Ω
f ∂ϕ
∂xj
(x) dx =
∫
Ω
gjϕ dx (6.22)
for all ϕ ∈ D(Ω), then we say that gj is the weak jth partial derivative of f . Weak jth
partial derivatives are unique, if they exist: if (6.22) holds both for gj and some other
function gˆj ∈ L2(Ω), then ∫
Ω gjϕ dx = ∫
Ω gˆjϕ dx for all ϕ ∈ D(Ω). Corollary 6.10
implies that gj = gˆj in L2(Ω). We will denote the weak jth partial derivative of f , if
it exists, by Dj f .
If f ∈ L2(Ω) and j ∈ {1,..., d} then for brevity we will simply write Dj f ∈ L2(Ω)
to indicate that the weak jth partial derivative exists in the above sense. With this
convention, the Sobolev space H1(Ω) may be defined as follows:
H1(Ω) := {u ∈ L2(Ω) : Dju ∈ L2(Ω) for all j = 1,..., d}.
If u ∈ H1(Ω), then Dju is thus the unique function in L2(Ω) for which
−
∫
Ω
u ∂ϕ
∂xj
dx =
∫
Ω
Dju ϕ dx
for all ϕ ∈ D(Ω).
Remark 6.13 (Comparison of classical and weak derivatives) Take u ∈ L2(Ω) ∩
C1(Ω). Then, invoking Lemma 6.12 we see that u is in H1(Ω) if and only if ∂u
∂xj
∈
L2(Ω) for all j = 1,..., d. In this case, the weak and classical partial derivatives
agree, that is, Dju = ∂u
∂xj
, j = 1,..., d (where equality is in the sense of L2(Ω)-
functions, that is, almost everywhere). Thus Dju is a generalization of the classical
partial derivatives. For functions u ∈ L2(Ω) ∩ C1(Ω) we will use both symbols ∂u
∂xj
and Dju, but ∂
∂xj will be used exclusively for the classical partial derivative. 
Remark 6.14 (Test functions) We used the space D(Ω) when defining weak deriva￾tives and thus also H1(Ω). This is particularly convenient when higher derivatives
are needed, as they will be in some arguments. However, the “integration by parts
formula” is automatically valid for the larger class C1
c (Ω), that is,
−
∫
Ω
u ∂ϕ
∂xj
dx =
∫
Ω
Dju ϕ dx
for all u ∈ H1(Ω), ϕ ∈ C1
c (Ω), j = 1,..., d, see Exercise 6.2. 
It follows immediately from the uniqueness of Dju that H1(Ω) is a vector space
and Dj : H1(Ω) → L2(Ω) is a linear mapping for all j = 1,..., d. Moreover,
(u, v)H1 := (u, v)L2 +

d
j=1
(Dju, Djv)L2
=
∫
Ω
u(x)v(x) dx +
∫
Ω
∇u(x)∇v(x) dx6.2 Sobolev spaces on Ω ⊆ Rd 191
defines an inner product on H1(Ω), whose corresponding norm is given by
u2
H1(Ω) :=
∫
Ω
u(x)
2 dx +
∫
Ω
|∇u(x)|2 dx.
Here we have taken the gradient ∇u of u ∈ H1(Ω) to be the vector
∇u(x) := (D1u(x),..., Ddu(x)).
If y = (y1,..., yd), z = (z1,..., zd) ∈ Rd, then we denote the natural inner product
and the Euclidean norm on Rd by y · z = yT z and |y| := √y · y, respectively. For
u, v ∈ H1(Ω), we therefore write ∇u(x)·∇v(x) = 
d
j=1 Dju(x) Djv(x), x ∈ Ω.
Moreover, (∇u · ∇v)(x) := ∇u(x)·∇v(x), x ∈ Ω, defines a function ∇u · ∇v ∈ L1(Ω).
Theorem 6.15 When equipped with the above inner product, H1(Ω) is a Hilbert
space.
Proof The proof of Theorem 6.15 is completely analogous to the proof of Theo￾rem 5.6 in the one-dimensional case. 
From the definition of the norm, we immediately obtain the following character￾ization of convergence in H1(Ω): if un, u ∈ H1(Ω), then limn→∞ un = u in H1(Ω) if
and only if
lim
n→∞ un = u and lim
n→∞ Djun = Dju in L2(Ω) for all j = 1,..., d.
We will make repeated use of this characterization. We next show how functions in
H1(Ω) can be approximated by C∞-functions.
Theorem 6.16 (Approximation theorem) Let u ∈ H1(Ω). Then there exist functions
un ∈ D(Rd), n ∈ N, such that
lim
n→∞ un = u in L2(Ω) (6.23)
and lim
n→∞
∂un
∂xj
= Dju in L2(U) (6.24)
for all U  Ω and all j = 1,..., d.
Proof We define un := ηn(n ∗ u˜) just as we did in the lead-up to Theorem 6.8,
where u˜ denotes the extension of u by zero to Rd as in (6.20). Then un ∈ D(Rd) and
limn→∞ un = u in L2(Ω) by Theorem 6.8. Now fix U  Ω; we will show that
∂un
∂xj
= n ∗ D
1ju (6.25)
holds on U, for all sufficiently large n. To this end, we choose an n0 ∈ N such that
U ⊂ Ωn0 (cf. (6.16)) and fix x ∈ U. Then for n ≥ n0 we have suppn(x − ·) ⊂ Ω and192 6 Hilbert space methods for elliptic equations
(n ∗ u˜)(x) = ∫
Ω u(y)n(x − y) dy. Hence, using Theorem 6.5 and the definition of
Dju, we obtain
∂
∂xj
(n ∗ u˜)(x) =
∫
Ω
u(y)
∂ n
∂xj
(x − y) dy = −
∫
Ω
u(y)
∂ n
∂yj
(x − y) dy
=
∫
Ω
Dju(y)n(x − y) dy = (n ∗ D
1ju)(x).
It now follows from Theorem 6.6 that limn→∞ ∂un
∂xj = Dju in L2(U). 
Remark 6.17 It is in general impossible to guarantee that convergence of the deriva￾tives (6.24) holds in the whole space L2(Ω) without imposing additional regularity
conditions on the boundary of Ω. Indeed, there exist bounded open sets Ω for which
C(Ω) ∩ H1(Ω) is not dense in H1(Ω) (for example Ω = (0, 1)∪(1, 2) in R); see
Exercise 6.8. However, the theorem of Meyers–Serrin (see [50] or, e.g., [28, Sec.
5.3.1]) states that C∞(Ω) ∩ H1(Ω) is dense in H1(Ω), for any open set Ω ⊂ Rd. 
There is also a converse of the approximation theorem (Theorem 6.16); with this
converse statement, we can completely describe the space H1(Ω) in terms of its
approximation properties.
Theorem 6.18 Let u, f1,..., fd ∈ L2(Ω), and suppose that there exist functions
un ∈ C1
c (Rd) such that un → u and ∂un
∂xj → fj as n → ∞ in L2(U), for all U  Ω
and j = 1,..., d. Then u ∈ H1(Ω) and Dju = fj for all j = 1,..., d.
Proof For ϕ ∈ D(Ω), we have
−
∫
Ω
u(x)
∂ϕ
∂xj
(x) dx = lim
n→∞ −
∫
Ω
un(x)
∂ϕ
∂xj
(x) dx
= lim
n→∞ ∫
Ω
∂
∂xj
un(x) ϕ(x) dx =
∫
Ω
fj(x)ϕ(x) dx
for all j = 1,..., d. The claim now follows from the definition of H1(Ω). 
We will now collect some rules of differentiation and, in the process, test the
suitability of our weak partial derivatives as defined above.
Theorem 6.19 Let Ω ⊂ Rd be open and connected, and let u ∈ H1(Ω) be such that
Dju(x) = 0 almost everywhere in Ω, for all j = 1,..., d. Then there exists a constant
c ∈ R such that u(x) = c almost everywhere.
Proof The proof consists of two steps.
1. Let B¯(x,r) ⊂ Ω for some x ∈ Ω and r > 0. We will show that there exists a
constant c ∈ R such that u(y) = c almost everywhere in B(x,r). To this end, take
un := ηn(n ∗ u˜) in accordance with Theorem 6.8 and choose n large enough that
1
n < dist(B¯(x,r), ∂Ω). Then by (6.25) we have ∂
∂xj
un = ∂
∂xj
(n ∗ u˜) = n ∗ D
1ju = 06.2 Sobolev spaces on Ω ⊆ Rd 193
in B(x,r), for all j = 1,..., d. It follows that un is constant in B(x,r), since by (6.13)
|un(y) − un(x)| ≤ ∇un ∞ · |x − y| = 0 for all y ∈ B(x,r). Since un → u in L2(Ω)
by Theorem 6.11, the claim follows.
2. By 1., there exists a constant c such that
U := {x ∈ Ω : u(y) = c for almost all y in a neighborhood of x}
is not empty. The set U is clearly open; by 1., U is also relatively closed in Ω. Since
Ω is connected, it follows that U = Ω. 
We next derive the product rule.
Theorem 6.20 (Product rule) Let u, v ∈ H1(Ω) be such that v, Djv ∈ L∞(Ω),
j = 1,..., d. Then uv ∈ H1(Ω) and
Dj(uv) = (Dju)v + u Djv. (6.26)
Proof By assumption, uv ∈ L2(Ω) and (Dju)v + u Djv ∈ L2(Ω). We need to show
that
−
∫
Ω
u(x)v(x) ∂
∂xj
ϕ(x) dx =
∫
Ω
(Dju(x)v(x) + u(x) Djv(x))ϕ(x) dx (6.27)
for all ϕ ∈ D(Ω).
1st case: The function u has an extension in D(Rd). Then uϕ ∈ D(Ω) for all
ϕ ∈ D(Ω) and thus, by definition of Djv,
−
∫
Ω
u(x)v(x) ∂
∂xj
ϕ(x) dx = −
∫
Ω
v(x) ∂
∂xj
(u(x) ϕ(x)) dx
+
∫
Ω
v(x)
 ∂
∂xj
u(x)

ϕ(x) dx
=
∫
Ω
(Djv(x) u(x) ϕ(x) + Dju(x)v(x))ϕ(x) dx.
This establishes (6.27) in this case.
2nd case: Now let u ∈ H1(Ω) be arbitrary. Then by Theorem 6.16 there exist
functions un ∈ D(Rd) such that un → u in L2(Ω) and Djun → Dju in L2(U) for all
U  Ω. The 1st case implies that Dj(un v) = (Dj un)v + un Djv, and so Dj(un v)
converges to (Dju)v + u Djv in L2(U) as n → ∞, for all U  Ω. Since unv → uv in
L2(Ω) as n → ∞, the claim follows from Theorem 6.18. 
Theorem 6.21 (Chain rule) Suppose that f ∈ C1(R)satisfies f (0) = 0 and | f 
(r)| ≤
M for all r ∈ R, where M > 0. Then f ◦ u ∈ H1(Ω) and Dj( f ◦ u) = ( f  ◦ u)Dju,
j = 1,..., d, for all u ∈ H1(Ω).194 6 Hilbert space methods for elliptic equations
Proof Fix u ∈ H1(Ω). Firstly, by assumption | f (r)| =



∫ r
0 f 
(s) ds


 ≤ M|r| for all
r ∈ R, whence |( f ◦ u)(x)| ≤ M|u(x)| for all x ∈ Ω and so
 f ◦ u2
L2 =
∫
Ω
|( f ◦ u)(x)|2 dx ≤ M2 u2
L2 < ∞.
Thus f ◦ u ∈ L2(Ω). Since f  is bounded, f  ◦ u ∈ L∞(Ω) and ( f  ◦ u)Dju ∈ L2(Ω),
where the latter assertion also uses that u ∈ H1(Ω). Let ϕ ∈ D(Ω); we need to show
that
−
∫
Ω
( f ◦ u)(x)
∂ϕ
∂xj
(x) dx =
∫
Ω
( f  ◦ u)(x)Dju(x)ϕ(x) dx. (6.28)
By the approximation theorem, Theorem 6.16, there exist functions un ∈ D(Rd)
such that un → u and Djun → Dju in L2(U) for all U  Ω. Choose U  Ω such
that supp ϕ ⊂ U. By passing to a subsequence if necessary, we may assume that
there exists g ∈ L2(Ω) such that |un| ≤ g and |Djun| ≤ g for all n ∈ N. We may also
assume that un → u and Djun → Dju as n → ∞ almost everywhere in U (see the
converse of the Dominated Convergence Theorem, Theorem A.10). By the classical
chain rule, ∂
∂xj
( f ◦ un) = ( f  ◦ un) ∂un
∂xj . Hence, by the integration by parts formula
(6.21),
−
∫
Ω
( f ◦ un)(x)
∂ϕ
∂xj
(x) dx =
∫
Ω
( f  ◦ un)(x)
∂un
∂xj
(x) ϕ(x) dx (6.29)
for all ϕ ∈ D(Ω). Since f ◦ un → f ◦ u, f  ◦ un → f  ◦ u and Djun → Dju almost
everywhere as n → ∞, the Dominated Convergence Theorem allows us to pass to
the limit in (6.29), and we obtain (6.28). 
Next we wish to prove a substitution rule for Sobolev spaces.
Theorem 6.22 (Substitution rule) Let Ω1, Ω2 ⊂ Rd be open and let F : Ω2 → Ω1
be bijective and satisfy the conditions F ∈ C1(Ω2, Rd), F−1 ∈ C1(Ω1, Rd) with
supx∈Ω2 DF(x) < ∞ and supy∈Ω1 DF−1(y) < ∞. If u ∈ H1(Ω1), then u ◦ F ∈
H1(Ω2) and
Dj(u ◦ F) =

d
i=1
(Diu) ◦ F ·
∂Fi
∂yj
.
Here F(y) = (F1(y),..., Fd(y))T , y ∈ Ω2, and (DF)(y) =

∂Fi
∂yj
(y)

i,j=1,...,d
is the
Jacobian matrix of F at y.
Proof Let u ∈ H1(Ω1). Then by the approximation theorem, Theorem 6.16, there
exist un ∈ D(Rd) such that un → u in L2(Ω1) and Djun → Dju in L2(U) for any
U  Ω1. Hence we have the convergence6.2 Sobolev spaces on Ω ⊆ Rd 195
(Diun ◦ F) · ∂Fi
∂yj
→ (Diu ◦ F) · ∂Fi
∂yj
in L2(U) as n → ∞, for all U  Ω2. Given ϕ ∈ D(Ω2), choose suppϕ ⊂ U  Ω2,
then
∫
V
(un ◦ F)
∂ϕ
∂yj
dy = −
∫
V

d
i=1

∂un
∂xi
◦ F
 ∂Fi
∂yj
· ϕ dy.
The result follows by passing to the limit as n → ∞. 
A special case is when F : Rd → Rd is an affine mapping, given by F(y) = By+b
for an invertible d × d matrix B and b ∈ Rd. For Ω2 ⊂ Rd, in this case F(Ω2) =: Ω1
is open and the substitution rule can be applied. Of particular interest is the case
where B is orthogonal: then F is isometric, that is, F(y) − F(z) = y − z for all
y, z ∈ Rd (and, conversely, every isometric mapping of Rd to itself is of the form
F(y) = By + b for an orthogonal matrix B).
Corollary 6.23 Let B be an orthogonal d × d matrix, b ∈ Rd, and F(y) = By + b for
y ∈ Rd. If Ω2 ⊂ Rd is open and Ω1 := F(Ω2), then the mapping u → u ◦ F defines
a unitary operator from H1(Ω1) to H1(Ω2).
We will now show the following connection between weak and classical deriva￾tives.
Theorem 6.24 Let u ∈ H1(Ω) and suppose that Dju ∈ C(Ω) for all j = 1,..., d.
Then u ∈ C1(Ω).
Proof Let U  Ω; we wish to show that u ∈ C1(U). Now we may assume that u has
compact support in Ω, since otherwise we may replace u by ηu for a suitable cut-off
function η ∈ D(Ω) and apply the product rule of Theorem 6.20.
By Theorem 6.27 we can also assume that Ω = Rd, since otherwise we may
replace u by its extension u˜. So we have that u ∈ H1(Rd) and u has compact support.
By assumption, Dju ∈ Cc(Rd). Consider the function un := n ∗ u ∈ D(Rd). Then
limn→∞ un = u in L2(Rd) by Theorem 6.6 and Djun = n ∗ Dju by Theorem 6.5,
and so
|Djun(x)| =




∫
Rd
n(x − y)Dju(y) dy




≤ Dju∞.
Thus  |∇un| ∞ ≤ c for all n ∈ N and some c ≥ 0, since Dju ∈ Cc(Rd). It follows
from (6.13) that |un(y) − un(x)| ≤  |∇un| ∞ · |y − x| ≤ c |y − x| for all n ∈ N and
all x, y ∈ Rd. This establishes that the sequence (un)n∈N is equicontinuous. Since
un → u in L2(Rd), we may assume that un(x) → u(x) almost everywhere (since
otherwise we pass to a subsequence; see Theorem A.10). In particular, there exist
x0 ∈ Rd and b ≥ 0 such that |un(x0)| ≤ b for all n ∈ N. Then |un(x)| ≤ b + c|x − x0|
for all n ∈ N and all x ∈ Rd. This means that the sequence (un)n∈N is bounded196 6 Hilbert space methods for elliptic equations
on every compact subset of Rd. By the theorem of Arzelà–Ascoli (Theorem A.6),
for each compact subset K of Rd there exists a subsequence of (un)n∈N which
converges uniformly in K to a continuous function w ∈ C(K). Since un(x) → u(x)
almost everywhere, the only possibility is that u = w almost everywhere. Thus u
is continuous (possibly after modification on a set of zero measure). For the jth
canonical basis vector of Rd, ej, and for x ∈ Rd, we have
un(x + tej) = un(x) +
∫ t
0
Djun(x + sej) ds.
Since Djun = n ∗ Dju converges uniformly to Dju by Theorem 6.11, passing to
the limit as n → ∞ we obtain
u(x + tej) = u(x) +
∫ t
0
Dju(x + sej) ds.
Hence ∂u
∂xj
(x) = Dju(x) and thus u ∈ C1(Rd). 
We finish this section by defining Sobolev spaces of higher order:
H2(Ω) := {u ∈ H1(Ω) : Dju ∈ H1(Ω), j = 1,..., d}.
That is, if u ∈ H2(Ω), then DiDj ∈ L2(Ω) for all i, j ∈ {1,..., d}. It is easy to show
that
DiDju = DjDiu (6.30)
holds for all i, j ∈ {1,..., d}, since the same is true for all test functions by Schwarz’s
theorem (see Exercise 6.3). We then define inductively
Hk+1
(Ω) := {u ∈ H1(Ω) : Dju ∈ Hk (Ω), j = 1,..., d}. (6.31)
This defines Hk (Ω) for all k ∈ N.
6.3 The space H1
0 (Ω)
We now wish to study to Sobolev spaces with weak homogeneous boundary condi￾tions on an open set Ω ⊂ Rd. If d ≥ 2, then not all functions in H1(Ω) are continuous
(see Exercise 6.5). Since in general the boundary ∂Ω of Ω has measure zero and
we identify functions in H1(Ω) which agree almost everywhere, it is meaningless to
write u|∂Ω = 0. The solution is to define a weak version of the boundary condition
“u|∂Ω = 0”. To this end we set H1
0 (Ω) := D(Ω)
H1(Ω)
, that is,
H1
0 (Ω) = {u ∈ H1(Ω) : ∃ ϕn ∈ D(Ω) such that lim
n→∞ ϕn = u in H1(Ω)}.6.3 The space H1
0 (Ω) 197
Then H1
0 (Ω) is a closed subspace of H1(Ω) and hence itself a Hilbert space. For
u ∈ H1(Ω) we interpret the condition u ∈ H1
0 (Ω) as a weak form of the statement
that u vanishes on the boundary of Ω. We now wish to study the space H1
0 (Ω). If
Ω = Rd, then Ω does not have a boundary, and indeed in this case H1
0 (Rd) = H1(Rd):
Theorem 6.25 The space of test functions D(Rd) is dense in H1(Rd).
Proof Let u ∈ H1(Rd). Then n ∗ u ∈ C∞(Rd) ∩ L2(Rd), and by Theorem 6.6 we
have limn→∞ n ∗ u = u in L2(Rd). One also sees as in the proof of (6.25) that
∂
∂xj
(n ∗ u) = n ∗ Dju.
Thus n ∗ u ∈ H1(Rd) ∩ C∞(Rd) and limn→∞ n ∗ u = u in H1(Rd). Now let
η ∈ D(Rd) be such that 0 ≤ η ≤ 1, η(x) = 1 for all |x| ≤ 1, and η(x) = 0 for
all |x| ≥ 2. Also let ηk (x) := η( x
k ). Then ηk ∈ D(Rd) satisfies ηk (x) = 1 for all
|x| ≤ k and 0 ≤ ηk ≤ 1. It follows that limk→∞ ηk (n ∗ u) = n ∗ u in L2(Rd) by the
Dominated Convergence Theorem. Moreover, as k → ∞,
∂
∂xj
(ηk (n ∗ u))(x) =
 ∂
∂xj
ηk

(x) (n ∗ u)(x) + ηk (x) ∂
∂xj
(n ∗ u)(x)
= 1
k
 ∂
∂xj
η
  x
k

(n ∗ u)(x) + ηk (x)(n ∗ Dju)(x)
converges to n ∗ Dju in L2(Rd) since ηk (x) = 1 for |x| ≤ k and the first term tends
to zero. Hence limk→∞ ηk (n ∗ u) = n ∗ u in H1(Rd). Since ηk (n ∗ u) ∈ D(Rd),
we finally obtain n ∗ u ∈ H1
0 (Rd), and so the theorem is proved upon passing to the
limit as n → ∞. 
One can easily extend the proof of Theorem 6.25 to the second derivative when
u ∈ H2(Rd). The following corollary of the proof will be useful later.
Corollary 6.26 Let u ∈ H2(Rd). Then there exist un ∈ D(Rd)such that limn→∞ un =
u, limn→∞ ∂
∂xj
un = Dju, and limn→∞ ∂
∂xi
∂
∂xj
un = DiDju in L2(Rd) for all i, j ∈
{1,..., d}.
If f : Ω → Rd is defined on an open set Ω ⊂ Rd, then analogously to (6.20) we
may define the extension
˜f (x) :=
 f (x) if x ∈ Ω,
0 in Rd \ Ω. (6.32)
If f is differentiable in Ω, then ˜f will generally not be differentiable on the boundary
of Ω; nevertheless, for H1
0 -functions we have the following theorem.
Theorem 6.27 Let u ∈ H1
0 (Ω). Then u˜ ∈ H1(Rd) and Dju˜ = D
1ju for all j = 1,..., d.198 6 Hilbert space methods for elliptic equations
Proof Let u ∈ H1
0 (Ω). Then there exists a sequence of functions un ∈ D(Ω) such
that un → u and ∂un
∂xj → Dju in L2(Ω) as n → ∞ for all j = 1,..., d. By (6.21) we
have, for all ϕ ∈ D(Rd),
−
∫
Rd
u˜(x)
∂ϕ
∂xj
(x) dx = lim
n→∞ −
∫
Ω
un(x)
∂ϕ
∂xj
(x) dx = lim
n→∞ ∫
Ω
∂un
∂xj
(x) ϕ(x) dx
=
∫
Ω
Dju(x) ϕ(x) dx =
∫
Rd
D
1ju(x) ϕ(x) dx.
This proves the claim. 
Corollary 6.28 Let u ∈ H1
0 (Ω) be such that Dju = 0 for all j = 1,..., d. Then u = 0.
Proof By Theorem 6.27 we have Dju˜ = 0 for all j = 1,..., d. It follows from
Theorem 6.19 that u˜ is constant. Since u˜ ∈ L2(Rd), the only possibility is that u˜ = 0.

Let Ω ⊂ Rd be open. We now wish to compare the classical condition u|∂Ω = 0
with the weak one, that is, u ∈ H1
0 (Ω). We will begin by showing that any function
u ∈ H1(Ω) which vanishes identically in a neighborhood of ∂Ω is in H1
0 (Ω). To this
end, we define
H1
c (Ω) := {u ∈ H1(Ω) : ∃ K ⊂ Ω compact such that u = 0 in Ω \ K}.
Theorem 6.29 We have H1
c (Ω) ⊂ H1
0 (Ω).
Proof Let u ∈ H1
c (Ω), that is, u(x) = 0 for all x ∈ Ω \ K, where K ⊂ Ω is compact.
Let n ∈ N be so large that 1
n < dist(K, ∂Ω). Then by Lemma 6.3 (n ∗ u˜)(x) = 0
whenever x  K + B¯(0, 1/n). Since K + B¯(0, 1/n) ⊂ Ω, we have n ∗ u˜ ∈ D(Ω).
Moreover, for all x ∈ Ω,
∂
∂xj
(n ∗ u˜)(x) =
∫
Rd
∂
∂xj
n(x − y) u˜(y) dy = −
∫
Rd
∂
∂yj
n(x − y) u˜(y) dy
= −
∫
Ω
∂
∂yj
n(x − y) u(y) dy =
∫
Ω
n(x − y)Dju(y) dy
=
∫
Rd
n(x − y)D
1ju(y) dy = (n ∗ D
1ju)(x).
It then follows from Theorem 6.6 that ∂
∂xj
(n ∗u˜) → Dju in L2(Ω) and n ∗u˜ → u in
L2(Ω) as n → ∞. We have thus approximated u by the test functions n ∗ u˜ ∈ D(Ω)
in H1(Ω). Thus u ∈ H1
0 (Ω). 
For a bounded open set Ω we set
C0(Ω) := { f ∈ C(Ω) : f|∂Ω = 0}. (6.33)6.3 The space H1
0 (Ω) 199
The following theorem shows that H1-functions with zero boundary conditions in
the classical sense are in H1
0 (Ω).
Theorem 6.30 If Ω ⊂ Rd is open and bounded, then H1(Ω) ∩ C0(Ω) ⊂ H1
0 (Ω).
We will delay the proof until the next section since we will require lattice opera￾tions which will only be introduced at a later point.
Remark 6.31 The converse statement to Theorem 6.30, H1
0 (Ω) ∩ C(Ω) ⊂ C0(Ω), is
false in general; here is an example. Let Ω := B \ {0}, where B := B(0, 1) = {x ∈
R3 : |x| < 1} is the ball in R3 of unit radius. Then Ω = B¯(0, 1). Let u ∈ D(B);
then u ∈ C(Ω), but u ∈ C0(Ω) only if u(0) = 0. But even if u(0)  0, we still have
u ∈ H1
0 (Ω). To see this, first choose a function f ∈ C∞(R) such that f (r) = 1 for
r ≥ 1 and f (r) = 0 for r ≤ 1
2 .
Set un(x) := f (n|x|)u(x). Then un(x) = 0 for all |x| ≤ 1
2n and thus un ∈ D(Ω).
Since un(x) = u(x) for |x| ≥ 1
n , the function un converges to u in L2(Ω) by the
Dominated Convergence Theorem. Moreover, by the product rule,
Djun(x) = f (n|x|)Dju(x) + n f 
(n|x|) xj
|x|
u(x).
The first term on the right-hand side converges to Dju in L2(Ω) as n → ∞; the
second tends to 0 in L2(Ω) as n → ∞ since
n2
∫
Ω
f 
(n|x|)2 dx = n2
∫
|x |<1/n
f 
(|nx|)2 dx =
∫
|x |<1
f 
(|y|)2 dy
n .
We thus have limn→∞ un = u in H1(Ω) and so u ∈ H1
0 (Ω) since H1
0 (Ω) is closed in
H1(Ω).
However, the implication is correct if Ω does not have any such “holes”, that is,
if Ω is equal to the interior of Ω; see [14]. 
Finally, we wish to prove a very useful and important inequality for the L2-norm
of H1
0 -functions. We say that Ω is contained in a strip, or bounded in one direction, if
there exist j0 ∈ {1,..., d} and M > 0 such that |xj0 | ≤ M for all x ∈ Ω. Obviously,
every bounded domain has this property.
Theorem 6.32 (Poincaré inequality) Let Ω ⊂ Rd be open and contained in a strip.
Then with M > 0 as above we have
uL2 ≤ 2M |u|H1 (6.34)
for all u ∈ H1
0 (Ω), where |u|H1 :=
∫
Ω |∇u(x)|2 dx1/2
= ∇uL2 .
Corollary 6.33 If Ω is contained in a strip, then |·|H1 defines an equivalent norm
on H1
0 (Ω).200 6 Hilbert space methods for elliptic equations
Proof (of Theorem 6.32) Without loss of generality suppose that j0 = 1. Let h ∈
C1([−M, M]) be a function such that h(−M) = 0. Then by the Cauchy–Schwarz
inequality
∫ M
−M
h(x)
2 dx =
∫ +M
−M




∫ x
−M
h
(y) dy




2
dx ≤
∫ M
−M
∫ x
−M
h
(y)
2 dy · (x + M) dx
≤ (2M)
2
∫ M
−M
h
(y)
2 dy.
Hence the inequality
∫
Ω
(u(x))2 dx ≤
∫
R
... ∫
R
∫ +M
−M
(2M)
2

∂u˜
∂x1
(x1,..., xd)
2
dx1 ··· dxd
≤ (2M)
2
∫
Ω
|∇u|
2 dx
holds for all u ∈ D(Ω). Since D(Ω) is dense in H1
0 (Ω), we conclude that (6.34) holds
for all u ∈ H1
0 (Ω). 
Remark 6.34 The statement of the Poincaré inequality also remains valid for bounded
Ω in the somewhat more general case that u only vanishes on a non-trivial part ΓD ⊂
∂Ω of the boundary in an appropriate sense (here D stands for Dirichlet boundary
conditions). We denote the corresponding space by H1
D(Ω). See Exercise 7.11 for a
precise statement and hints for the proof. This space plays an important role in the
study of problems with mixed boundary conditions, and in particular in numerics
(cf. [16, Ch. II]). 
6.4 Lattice operations on H1(Ω)
Given a function f : Ω → R we set
f +(x) :=

f (x), if f (x) > 0,
0, if f (x) ≤ 0,
as well as f − := (− f )
+ and | f | := f + + f −. It is immediate from the definitions
that f = f + − f −. Moreover, if f ∈ L2(Ω), then also | f |, f +, f − ∈ L2(Ω) and
 f L2 =  | f | L2 . One calls the mappings f → | f |, f +, f − lattice operations on
L2(Ω). We also define the sign function of f by
(sign f )(x) :=
⎧⎪⎪⎪⎨
⎪⎪⎪
⎩
1, if f (x) > 0,
0, if f (x) = 0,
−1, if f (x) < 0,6.4 Lattice operations on H1(Ω) 201
as well as the characteristic function
1{ f >0}(x) :=

1, if f (x) > 0,
0, if f (x) ≤ 0,
and 1{ f <0} := 1{−f >0}. We then have the following relations for the weak derivatives
of H1-functions.
Theorem 6.35 (Derivatives under lattice operations) If u ∈ H1(Ω), then also
|u|, u+, u− ∈ H1(Ω) and
Dju+ = 1{u>0} Dju (6.35)
Dju− = −1{u<0} Dju (6.36)
Dj |u| = (sign u) Dju (6.37)
for all j = 1,..., d.
Proof Let u ∈ H1(Ω). We will show that u+ ∈ H1(Ω) and (6.35) holds; the other
claims then follow immediately from this case, since u− = (−u)
+ and |u| = u+ + u−.
So set
fn(r) :=

(r2 + n−2)
1/2 − n−1 if r > 0,
0 if r ≤ 0;
then fn ∈ C1(R), fn(0) = 0, and
f 
n(r) =

r(r2 + n−2)
−1/2 if r > 0,
0 if r ≤ 0.
In particular, 0 ≤ f 
n(r) ≤ 1 for all r ∈ R; moreover, 0 ≤ fn(r) ≤ r+ for all n ∈ N
and r ∈ R, and limn→∞ fn(r) = r+. Now take ϕ ∈ D(Ω). By the chain rule,
−
∫
Ω
( fn ◦ u(x)) ∂ϕ
∂xj
(x) dx =
∫
Ω
( f 
n ◦ u)(x) (Dju)(x) ϕ(x) dx. (6.38)
By the monotonicity of fn, we have 0 ≤ fn ◦ u ≤ u+. Since fn ◦ u → u+ almost
everywhere, the Dominated Convergence Theorem implies that the left-hand side of
(6.38) converges to −
∫
Ω u+(x) ∂ϕ
∂xj
(x) dx. Since |( f 
n ◦ u)(x)| ≤ 1 and limn→∞( f 
n ◦
u)(x) = 1{u>0}(x) for all x ∈ Ω, the right-hand side of (6.38) converges to
∫
Ω
( f 
n ◦ u)(x) (Dju)(x) ϕ(x) dx n→∞
−−−−−→
∫
Ω
1{u>0}(x) Dju(x) ϕ(x) dx.
Hence, passing to the limit on both sides of (6.38), we obtain
−
∫
Ω
u+(x)
∂ϕ
∂xj
(x) dx =
∫
Ω
1{u>0}(x) (Dju)(x) ϕ(x) dx,202 6 Hilbert space methods for elliptic equations
and we have proved the statement of the theorem for u+. 
The following corollary is quite remarkable.
Corollary 6.36 (Stampacchia’s lemma) Suppose that u ∈ H1(Ω), c ∈ R and j ∈
{1,..., d}. Then (Dju)(x) = 0 almost everywhere in the set {x ∈ Ω : u(x) = c}.
Proof The proof proceeds in two steps.
1. If Ω is bounded, then we may assume that c = 0. Indeed, otherwise we replace
u by u − c. By (6.35) and (6.36), on B := {x ∈ Ω : u(x) = 0} we have Dju+(x) =
Dju−(x) = 0 for all x ∈ B. The claim now follows since Dju = Dju+ − Dju−.
2. For Ω arbitrary we have Ω = /
n∈N Ωn, where Ωn := Ω ∩ B(0, n). For each
n ∈ N, by 1., the set Mn := {x ∈ Ωn : u(x) = c, Dju(x)  0} has measure zero,
since Ωn is bounded. Hence /
n∈N Mn = {x ∈ Ω : u(x) = c, Dju(x)  0} likewise
has measure zero, and we have proved the claim. 
We will now show that the lattice operations are continuous.
Theorem 6.37 (Continuity of lattice operations) The mappings u → u+, u−, |u| :
H1(Ω) → H1(Ω) are continuous.
Proof Let un → u be a convergent sequence in H1(Ω). We will show that |un|→|u|
in H1(Ω); since u+ = 1
2 (u + |u|) and u− = (−u)
+, it will then follow directly that
u+
n → u+ and u−
n → u− in H1(Ω) as n → ∞. Now since ||un|−|u|| ≤ |un − u|, we
certainly have |un|→|u| in L2(Ω), and it remains to show that Dj |un| → Dj |u|
in L2(Ω) for all j ∈ {1,..., d}. By Lemma 4.40, it suffices to prove this for a
subsequence.
By the converse of the Dominated Convergence Theorem (Theorem A.10), we
may therefore assume that un(x) → u(x), Djun(x) → Dju(x) for all x ∈ Ω \ N,
where N is a set of measure zero. We may also assume that there exists a function
g ∈ L2(Ω) such that |Djun| ≤ g and |un| ≤ g for all n ∈ N.
Set B := {x ∈ Ω : u(x) = 0}, then by Stampacchia’s lemma (Corollary 6.36),
Dju(x) = 0 almost everywhere in B, and so
lim
n→∞ Dj |un(x)| = lim
n→∞ sign (un(x))Djun(x) = 0 = Dj |u(x)|
almost everywhere in Ω. If x  B, that is, u(x)  0, then limn→∞ sign un(x) =
sign u(x) whenever x  N. We thus have
Dj |un| = (sign(un))Djun → (sign u)Dju = Dj |u|
as n → ∞ almost everywhere in Ω. It now follows from the Dominated Convergence
Theorem that limn→∞ Dj |un| = Dj |u| in L2(Ω), and we have shown that |un|→|u|
in H1(Ω). 6.4 Lattice operations on H1(Ω) 203
Remark 6.38 Here is a second, abstract proof of Theorem 6.37. Let un → u in H1(Ω).
By Lemma 4.40 it suffices to show that |un|→|u| in H1(Ω) up to a subsequence.
Now un H1 =  |un| H1 , and so (|un|)n∈N is bounded in H1(Ω). By Theorem 4.35
we may thus assume that |un| converges weakly in H1(Ω) to some w ∈ H1(Ω) (after
passing to a subsequence if necessary). Then |un| also converges weakly to w in
L2(Ω).
But since |un|→|u| in L2(Ω) (as established at the beginning of the above
proof), it follows that w = |u|. We have thus shown that |un|  |u| in H1(Ω). Since
 |un| H1 = un H1 → uH1 =  |u| H1 , it follows from Theorem 4.33 that indeed
|un|→|u| in H1(Ω). 
Theorem 6.37 states that H1(Ω) is a sublattice of L2(Ω) and that the lattice
operations are continuous. In particular, for u, v ∈ H1(Ω), the functions
u ∨ v := max{u, v} = u + v + |u − v|
2 , (6.39a)
u ∧ v := min{u, v} = u + v − |u − v|
2 (6.39b)
are also in H1(Ω). We will now show that H1
0 (Ω) is a lattice ideal in H1(Ω).
Theorem 6.39 (a) If u ∈ H1
0 (Ω), then |u|, u+, u− ∈ H1
0 (Ω).
(b) If v ∈ H1(Ω) and u ∈ H1
0 (Ω), with 0 ≤ v ≤ u, then v ∈ H1
0 (Ω).
Proof (a) Let un ∈ D(Ω) converge to u in H1(Ω). By Theorem 6.37, also
lim
n→∞ |un| = |u|, lim
n→∞ u+
n = u+, lim
n→∞ u−
n = u− in H1(Ω).
Since |un|, u+
n, u−
n ∈ H1
c (Ω) ⊂ H1
0 (Ω) (see Theorem 6.29), the claim follows since
H1
0 (Ω) is closed.
(b) Let ϕn ∈ D(Ω) converge to u in H1(Ω), and set
vn := (ϕn ∧ v) ∨ 0 = max{min{ϕn, v}, 0}.
Then vn ∈ H1
c (Ω) since ϕn ∈ D(Ω), v ∈ H1
0 (Ω) and v ≥ 0. Moreover,
lim
n→∞ vn = 
( lim
n→∞ ϕn) ∧ v
	
∨ 0 = (u ∧ v) ∨ 0 = v
since u ≥ v ≥ 0. By Theorem 6.29 and the fact that H1
0 (Ω) is closed, we conclude
that v ∈ H1
0 (Ω). 
The following statements will be useful for the further analysis of elliptic prob￾lems. We define the positive cones of the spaces D(Ω) and H1
0 (Ω) by
D(Ω)+ := {ϕ ∈ D(Ω) : ϕ ≥ 0},204 6 Hilbert space methods for elliptic equations
H1
0 (Ω)+ := {u ∈ H1
0 (Ω) : u ≥ 0 almost everywhere},
respectively. Now by definition D(Ω) is dense in H1
0 (Ω); the same is true of their
positive cones.
Theorem 6.40 The space D(Ω)+ is dense in H1
0 (Ω)+.
Proof Let u ∈ H1
0 (Ω)+ and choose un ∈ D(Ω) such that limn→∞ un = u in H1
0 (Ω).
Then, by continuity of the lattice operations, limn→∞ u+
n = u+ = u in H1(Ω) as well.
Since 0 ≤ u+
n ∈ H1
c (Ω), we have k ∗ u+
n ≥ 0 for the mollifier k from Definition 6.2.
But since k ∗ u+
n ∈ D(Ω) for k large enough and k ∗ u+
n → u+
n in H1(Ω) as k → ∞
(note (6.25)), the theorem is proved. 
We finish by giving the proof of Theorem 6.30.
Proof (of Theorem 6.30) Let u ∈ H1(Ω) ∩ C0(Ω). It follows from Theorem 6.35
that u+, u− ∈ H1(Ω) ∩ C0(Ω). As such it is sufficient to consider positive functions,
so let u ≥ 0. Since Ω is bounded, we have u − 1
n ∈ H1(Ω) for all n ∈ N, and also
un := (u − 1
n )
+ ∈ H1(Ω) (by Theorem 6.35). Since u ∈ C0(Ω), the set {x ∈ Ω :
u(x) ≥ 1
n } is compact, and thus un ∈ H1
c (Ω) ⊂ H1
0 (Ω) by Theorem 6.29. Since the
lattice operations are continuous (Theorem 6.37), limn→∞ un = u in H1(Ω). Hence
u ∈ H1
0 (Ω) since H1
0 (Ω) is closed. 
6.5 The Poisson equation with Dirichlet boundary conditions
Let Ω be an open set and λ ≥ 0. Given f ∈ L2(Ω), our goal is to analyze the Poisson
equation with reaction term
−Δu + λu = f in Ω, (6.40a)
u|∂Ω = 0. (6.40b)
We start by considering the equation (6.40a) by itself. Let us first suppose that
u is a classical solution of (6.40a), that is, that u ∈ C2(Ω) and (6.40a) holds for
Δu = 
d
j=1
∂2u
∂xj
. If we multiply this equation by a test function ϕ ∈ D(Ω) and
integrate by parts twice (see (6.21)), then we obtain
λ
∫
Ω
u(x) ϕ(x) dx −
∫
Ω
u(x)Δϕ(x) dx =
∫
Ω
f (x) ϕ(x) dx. (6.41)
This identity is also meaningful if u is merely in L2(Ω). In this case, we shall call u
a weak solution of (6.40a).
Definition 6.41 Let f ∈ L2(Ω). A function u ∈ L2(Ω) is called a weak solution of
(6.40a) (without considering the boundary condition (6.40b)) if6.5 The Poisson equation with Dirichlet boundary conditions 205
λ
∫
Ω
u(x) ϕ(x) dx −
∫
Ω
u(x) Δϕ(x) dx =
∫
Ω
f (x) ϕ(x) dx
for all ϕ ∈ D(Ω). We also say that (6.40a) holds weakly. 
If u is actually in H1(Ω), then we can bring its weak derivatives into play and
obtain the following description of weak solutions.
Lemma 6.42 The function u ∈ H1(Ω) is a weak solution of (6.40a) if and only if
λ
∫
Ω
u(x) ϕ(x) dx +
∫
Ω
∇u(x) ∇ϕ(x) dx =
∫
Ω
f (x) ϕ(x) dx (6.42)
for all ϕ ∈ H1
0 (Ω).
Proof Let ϕ ∈ D(Ω), then also ∂ϕ
∂xj ∈ D(Ω) for all j = 1,..., d and hence, by
definition of weak derivatives,
∫
Ω
∇u(x)∇ϕ(x) dx =

d
j=1
∫
Ω
Dju(x)
∂ϕ
∂xj
(x) dx = −

d
j=1
∫
Ω
u(x)
∂2ϕ
∂x2
j
(x) dx
= −
∫
Ω
u(x) Δϕ(x) dx.
Hence, if ϕ ∈ D(Ω), then (6.42) and (6.41) are equivalent. If (6.42) holds for all
ϕ ∈ D(Ω), then the identity also holds for ϕ ∈ H1
0 (Ω) by density. 
We will henceforth interpret the boundary condition (6.40b) as requiring that
u ∈ H1
0 (Ω). Then Lemma 6.42 suggests that we should consider the bilinear form
a(u, v) := λ
∫
Ω
u(x)v(x) dx +
∫
Ω
∇u(x) ∇v(x) dx
on H1
0 (Ω)×H1
0 (Ω). This form is continuous since by the Cauchy–Schwarz inequality
|a(u, v)| ≤ λuL2 v L2 +
∫
Ω
|∇u(x)|2 dx 1
2 ∫
Ω
|∇v(x)|2 dx 1
2
≤ (λ + 1)uH1 v H1 .
Moreover, if λ > 0, then a(u) ≥ min{λ, 1}u2
H1 , and the form is coercive, recall
(4.5). If Ω is contained in a strip, then the Poincaré inequality (6.34) holds, and so
∫
Ω
|∇u(x)|2 dx = 1
2
∫
Ω
|∇u(x)|2 dx +
1
2
∫
Ω
|∇u(x)|2 dx
≥
1
8M2
∫
Ω
u(x)
2 dx +
1
2
∫
Ω
|∇u(x)|2 dx ≥ αu2
H1,206 6 Hilbert space methods for elliptic equations
where α = min  1
8M2 , 1
2

. Hence, in this case the form is also coercive for λ = 0.
So suppose that λ > 0, or that Ω is contained in a strip. For f ∈ L2(Ω), the
mapping ϕ → ∫
Ω f (x) ϕ(x) dx is a continuous linear form on H1
0 (Ω) and so by the
Lax–Milgram theorem there exists a unique u ∈ H1
0 (Ω) which satisfies the equation
a(u, ϕ) = ∫
Ω f (x) ϕ(x) dx for all ϕ ∈ H1
0 (Ω). Hence (6.41) is satisfied and we have
proved the following theorem.
Theorem 6.43 Let λ ≥ 0 and suppose that at least one of the following two conditions
is satisfied: (a) λ > 0; (b) Ω is contained in a strip. Then for each f ∈ L2(Ω) there
exists a unique weak solution u ∈ H1
0 (Ω) of (6.40).
We have thus proven the well-posedness of the Poisson problem in a weak sense.
The question of regularity is, however, rather more difficult. On the one hand there
is the question of interior regularity, that is, for example, under which assumptions
we actually have u ∈ C2(Ω), so that the equation (6.40a) is then satisfied in the
classical sense; and then the question of whether u ∈ C(Ω) and u|∂Ω = 0, that
is, whether the boundary condition is satisfied in the classical sense. It turns out
that Fourier transforms will help us with the first problem; we will thus investigate
these transforms in the next section with a particular view to their differentiability
properties.
The second question is equivalent to the well-posedness of the Dirichlet problem,
and we refer to Section 6.9. Here, we wish to obtain a weak version of the maximum
principle. We recall that a function u ∈ C2(Ω) is called subharmonic if −Δu ≤ 0 (cf.
Definition 3.24). This is equivalent to
−
∫
Ω
u(x) Δϕ(x) dx ≤ 0 for all ϕ ∈ D(Ω)+. (6.43)
But this condition also makes sense if u ∈ L2(Ω). We therefore more generally call a
function u ∈ L2(Ω) subharmonic if it satisfies (6.43). If u ∈ H1(Ω) and c ∈ R, then
we say that
u ≤ c weakly on ∂Ω, (6.44)
if (u − c)
+ ∈ H1
0 (Ω). This is a generalization of the classical version. Indeed, if Ω
is bounded and u ∈ H1(Ω) ∩ C(Ω) with u(z) ≤ c for all z ∈ ∂Ω, then (u − c)
+ is
in H1(Ω) ∩ C0(Ω) and so also in H1
0 (Ω) by Theorem 6.30. The following theorem
generalizes the classical maximum principle, Theorem 3.25.
Theorem 6.44 (Weak maximum principle) Let u ∈ H1(Ω) be subharmonic and
c ∈ R. If u ≤ c weakly on ∂Ω, then u ≤ c almost everywhere in Ω.
Proof Since u ∈ H1(Ω), it follows from (6.43) that ∫
Ω ∇u ∇ϕ dx ≤ 0 for all ϕ ∈
D(Ω)+. Since D(Ω)+ is dense in H1
0 (Ω)+ (Theorem 6.40), this inequality continues
to hold for all ϕ ∈ H1
0 (Ω)+. In particular, we may choose ϕ = (u−c)
+. It then follows
from (6.35) that Djϕ = 1{u>c}Dju, and6.6 Sobolev spaces and Fourier transforms 207
0 ≥
∫
Ω
∇u(x)∇(u − c)
+(x) dx =
∫
Ω
∇(u − c)(x)∇(u − c)
+(x) dx
=
∫
Ω
|∇(u − c)
+(x)|2 dx.
Thus ∇(u − c)
+ = 0 almost everywhere. By Corollary 6.28, (u − c)
+ = 0 almost
everywhere as well. 
6.6 Sobolev spaces and Fourier transforms
We have already seen that the Fourier transform converts differentiation into mul￾tiplication by the variable. Since it is also an isomorphism of L2(Rd) to L2(Rd),
it should not be surprising that we can easily describe the image of our Sobolev
spaces under the Fourier transform. In addition to this characterization we will also
prove imbedding and regularity theorems: how often must a function be weakly
differentiable in order to be continuous or even C1? Such results are also known as
imbedding theorems. Although they hold for Rd, they can be extended to open sets
Ω ⊂ Rd via localization and can thus be used for the Poisson problem.
In this section the underlying field will always be C. We define complex-valued
Lp-spaces for 1 ≤ p < ∞ by
Lp(Rd, C) :=

f : Rd → C is measurable and ∫
Rd
| f (x)|p dx < ∞
'
.
Here, as usual, we identify two functions in Lp(Rd, C) if they agree almost every￾where. For f ∈ L1(Rd, C) we define the Fourier transform ˆf : Rd → C by
ˆf (x) := 1
(2π)d/2
∫
Rd
e−ix·y f (y) dy,
where as before, x · y = xT y denotes the inner product in Rd of vectors x =
(x1,..., xd), y = (y1,..., yd) ∈ Rd. For brevity, we set x2 = x · x = 
d
j=1 x2
j ; then
|x| := √
x2. If f ∈ L1(Rd, C), then
ˆf ∈ C0(Rd, C) :=

g : Rd → C is continuous and lim
|x |→∞ g(x) = 0

, (6.45)
see Exercise 6.14. This property is often referred to as the Riemann–Lebesgue
lemma. Thus the Fourier transform defines a linear operator from L1(Rd, C) to
C0(Rd, C), which is injective but not surjective. The Fourier transform on the space
L2(Rd, C) is rather better behaved, as we shall now see. For the proof of the following
theorem we refer, e.g., to [54, Thm. 9.13].208 6 Hilbert space methods for elliptic equations
Theorem 6.45 (Plancherel’s theorem) There exists a uniquely determined unitary
operator F : L2(Rd, C) → L2(Rd, C) such that F f = ˆf for all f ∈ L1 ∩ L2.
Moreover, (F−1 f )(x) = (F f )(−x) almost everywhere, for all f ∈ L2(Rd, C).
We also call the mapping F the Fourier transform. In particular, we have
(F f, F g)L2 = ( f, g)L2 for all f, g ∈ L2 (Parseval’s identity) (6.46)
f (x) = (2π)
−d/2
∫
Rd
eix·yF f (y) dy (6.47)
whenever f ∈ L2(Rd, C) and F f ∈ L1(Rd, C). It follows from Corollary 6.9 that the
test functions D(Rd, C) := D(Rd) + iD(Rd) are dense in L2(Rd, C). We now define
the complex Sobolev space
H1(Rd, C) :=

u ∈ L2(Rd, C) : there exists D1u,..., Ddu ∈ L2(Rd, C)
such that −
∫
Rd
u(x)
∂ϕ
∂xj
(x) dx =
∫
Rd
Dju(x)ϕ(x) dx
for all ϕ ∈ D(Rd)
'
.
It is obvious that H1(Rd, C) = {v+iw : v, w ∈ H1(Rd)} and Dj(v+iw) = Djv+iDjw
for all v, w ∈ H1(Rd). It thus follows from Theorem 6.25 that D(Rd, C) is also dense
in H1(Rd, C). Consequently, for u ∈ H1(Rd, C) we have
−
∫
Rd
u(x) Djϕ(x) dx =
∫
Rd
Dju(x) ϕ(x) dx for all ϕ ∈ H1(Rd, C). (6.48)
We now define the weighted space
Hˆ 1(Rd) := L2(Rd, (1 + x2)dx, C)
:=

f : Rd → C measurable: ∫
Rd
| f (x)|2(1 + x2) dx < ∞
'
.
This is a Hilbert space when equipped with the inner product
( f, g)Hˆ 1(Rd) :=
∫
Rd
f (x) g(x) (1 + x2) dx.
Theorem 6.46 We have F (H1(Rd, C)) = Hˆ 1(Rd), and F |H1(Rd,C) is a unitary oper￾ator. Moreover, for f ∈ H1(Rd, C), we have
F (Dj f )(x) = ix F f (x) almost everywhere. (6.49)
Proof Let f ∈ D(Rd, C), then6.6 Sobolev spaces and Fourier transforms 209
(F Dj f )(x) = (2π)
−d/2
∫
Rd
e−ix·y ∂ f
∂yj
(y) dy = (ixj)(F f )(x),
as one sees upon integrating by parts. Since D(Rd, C) is dense in H1(Rd, C), this
identity continues to hold for all f ∈ H1(Rd, C). In particular, for f, g ∈ H1(Rd, C),
by (6.46) we have
( f, g)H1 = ( f, g)L2 +

d
j=1
(Dj f, Djg)L2
= (F f, F g)L2 +

d
j=1
(F (Dj f ), F (Djg))L2
=
∫
Rd

F f (x) F g(x) +

d
j=1
ixj F f (x)ixj F g(x)

dx
=
∫
Rd
F f (x) F g(x) (1 + x2) dx = (F f, F g)Hˆ 1(Rd).
In particular, F : H1(Rd, C) → Hˆ 1(Rd) is isometric. We will now show that F
is surjective. Let g ∈ Hˆ 1(Rd), then the function x → ixjg is in L2(Rd, C) for all
j = 1,..., d. Hence, by Theorem 6.45, there exist functions f, f1,..., fd ∈ L2(Rd, C)
such that F f = g and (F fj)(x) = ixjg(x) for all j = 1,..., d. Now let ϕ ∈ D(Rd)
(which we may take real-valued), then by (6.46),
−
∫
Rd
f (x)Djϕ(x) dx = −
∫
Rd
f (x) Djϕ(x) dx = −
∫
F f (x) F (Djϕ)(x) dx
= −
∫
Rd
g(x)(−ixj) F ϕ(x) dx
=
∫
Rd
F fj(x) F ϕ(x) dx =
∫
Rd
fj(x) ϕ(x) dx
since ϕ was chosen real-valued. Hence f ∈ H1(Rd, C) and Dj f = fj for all j =
1,..., d. 
The above theorem explains the notation Hˆ 1(Rd), since this space is the image
of H1(Rd, C) under the Fourier transform. We recall that the higher order Sobolev
spaces Hk (Rd, C), k ∈ N are defined inductively by
Hk+1
(Rd, C) :=

u ∈ H1(Rd, C) : Dju ∈ Hk (Rd, C), j = 1,..., d

.
We denote by Hˆ k (Rd) the weighted space
Hˆ k (Rd) :=

f : Rd → C measurable :
∫
Rd
| f (x)|2(1 + x2)
k dx < ∞

.210 6 Hilbert space methods for elliptic equations
These spaces are nested, that is,
Hˆ k+1(Rd) ⊂ Hˆ k (Rd) ⊂ L2(Rd, C), k ∈ N. (6.50)
Theorem 6.47 We have F (Hk (Rd, C)) = Hˆ k (Rd) for all k ∈ N.
Proof The statement is true for k = 1 (see Theorem 6.46). Now suppose it is true
for k ≥ 1; we will show that it is also true for k + 1.
1. Let u ∈ Hk+1(Rd, C), then u ∈ H1(Rd, C) and Dju ∈ Hk (Rd, C). We have
F (Dju) = gj, where gj(x) = ixjFu(x). Now by the induction hypothesis, gj ∈
Hˆ k (Rd), that is, for all 1 ≤ j ≤ d we have
∫
Rd
x2
j |Fu(x)|2(1 + x2)
k dx < ∞
as well as Fu ∈ Hˆ k (Rd), whence ∫
Rd |Fu(x)|2(1 + x2)
k dx < ∞. Summing over
j and the expression for Fu yields ∫
Rd (x2 + 1) |Fu(x)|2 (1 + x2)
k dx < ∞, that is,
Fu ∈ Hˆ k+1(Rd).
2. Conversely, let g ∈ Hˆ k+1(Rd), then the functions gj defined by gj(x) := ixjg(x)
are in Hˆ k (Rd). Since also g ∈ Hˆ k (Rd), by the induction hypothesis there exist
functions u, uj ∈ Hk (Rd, C) such that Fu = g and Fuj = gj for all j = 1,..., d.
Since F (Dju) = gj, it follows that uj = Dju. Thus Dju = uj ∈ Hk (Rd, C) and so by
definition u ∈ Hk+1(Rd, C). 
We will use Theorem 6.47 to prove that Hk (Rd) ⊂ C0(Rd) for k > d/2. The
assumption k > d/2 is optimal; it shows that in higher dimensions one needs more
Sobolev regularity in order to guarantee continuity: the gap between Hk and C0
grows with the dimension. Alternatively, one could say that Hk contains wilder and
wilder functions in higher dimensions. For the proof of the claimed imbedding we
first need the following lemma.
Lemma 6.48 For k > d/2 we have Hˆ k (Rd) ⊂ L1(Rd, C).
Proof Let f ∈ Hˆ k (Rd), then by Hölder’s inequality
∫
Rd
| f (x)| dx =
∫
Rd
| f (x)| (1 + x2)
k/2 (1 + x2)
−k/2 dx
≤
 ∫
Rd
| f (x)|2 (1 + x2)
k dx 1
2  ∫
Rd
(1 + x2)
−k dx 1
2
.
The first term is finite by assumption, so it remains to show for the second one that
∫
Rd (1 + x2)
−k dx < ∞. But we have, using Theorem A.8,
∫
|x | ≥1
(1 + x2)
−k dx ≤
∫
|x | ≥1
(x2)
−k dx = σ(Sd−1)
∫ ∞
1
r−2krd−1 dr.6.6 Sobolev spaces and Fourier transforms 211
Since d − 2k < 0, this last integral is clearly finite. 
Now we come to the announced imbedding.
Theorem 6.49 (Sobolev imbedding theorem) For k > d/2 we have Hk (Rd, C) ⊂
C0(Rd, C).
Proof Let f ∈ Hk (Rd, C), then F f ∈ L1(Rd, C) by Lemma 6.48. It thus follows from
(6.45) that f ∈ C0(Rd, C), since f (x) = F−1(F f )(x) = F (F f )(−x) by Plancherel’s
theorem (Theorem 6.45). 
We thus have the following regularity result.
Corollary 6.50 For k ∈ N, k > d
2 and m ∈ N0 we have Hk+m(Rd, C) ⊂ Cm(Rd, C).
Proof The statement holds for m = 0 by Theorem 6.49, where as usual we have
defined C0(Rd, C) := C(Rd, C). Now suppose the statement is true for some m ∈
N0 and take u ∈ Hk+m+1(Rd, C). By the induction hypothesis we have Dju ∈
Hk+m(Rd, C) ⊂ Cm(Rd, C). It follows from Theorem 6.24 that u ∈ C1(Rd, C); since
Dju ∈ Cm(Rd, C), we obtain u ∈ Cm+1(Rd, C). 
We now wish to consider the Poisson equation in Rd. For u ∈ H2(Rd, C) we have
the relations F (Dju)(x) = ixj Fu(x), F (D2
j u)(x) = −x2
j Fu(x) and F (Δu)(x) =
−x2 Fu(x), and hence F (u−Δu)(x) = (1+ x2) Fu(x). Since u ∈ H2(Rd, C), we also
have Fu ∈ Hˆ 2(Rd) and thus (1 + x2)Fu ∈ L2(Rd, C). Conversely, if f ∈ L2(Rd, C),
then x → (1 + x2)
−1F f defines a function in Hˆ 2(Rd). Thus there exists a unique
u ∈ H2(Rd, C)such that Fu = (1+x2)
−1F f . Finally, since F (u−Δu) = Fu+x2Fu =
(1 + x2)Fu = F f , we conclude that u − Δu = f . This means we have shown the
following result:
Theorem 6.51 For all f ∈ L2(Rd, C) there exists a unique u ∈ H2(Rd, C) such that
u − Δu = f .
If f ∈ L2(Rd) is a real-valued function, then u ∈ H2(Rd) is also real-valued. In
this case, u is then clearly also a weak solution of u−Δu = f , that is, u coincides with
the unique weak solution which we found in Theorem 6.43 using the Lax–Milgram
theorem. But here we have gained additional regularity, since u is actually in H2(Rd).
This means that the weak derivatives DiDj are all in L2(Rd) and Δu = 
d
j=1 D2
j u. In
this case we speak of a strong solution. The term “classical solution” is reserved for
functions which are actually in C2(Rd). We next wish to strengthen the statement of
uniqueness in Theorem 6.51.
Lemma 6.52 If v, f ∈ L2(Rd, C) satisfy v −Δv = f weakly, then v ∈ H2(Rd, C), that
is, v is the unique solution from Theorem 6.51.212 6 Hilbert space methods for elliptic equations
Proof Let u ∈ H2(Rd, C) be the solution from Theorem 6.51 and set w := u − v.
Then
∫
Rd
w(x) (ϕ(x) − Δϕ(x)) dx = 0 for all ϕ ∈ D(Rd, C). (6.51)
Since the test functions are dense in H2(Rd, C) (see Corollary 6.26), (6.51) holds
for all ϕ ∈ H2(Rd, C). Choose a function ϕ ∈ H2(Rd, C) such that ϕ − Δϕ = w (the
existence of such a function is guaranteed by Theorem 6.51). Substituting this into
(6.51), we obtain ∫
Rd |w(x)|2 dx = 0 and hence w = 0. 
With this preparation we can now prove the following result on regularity.
Theorem 6.53 (Maximal regularity) Let f, u ∈ L2(Rd, C) satisfy Δu = f weakly.
Then u ∈ H2(Rd, C). If f ∈ Hk (Rd, C), then u ∈ Hk+2(Rd, C).
Proof Since u − Δu = u − f ∈ L2(Rd), it follows from Lemma 6.52 that u ∈
H2(Rd, C).
We will prove the second claim by induction on k. For k = 0, it corresponds exactly
to the first part if we set H0(Rd, C) = L2(Rd, C). So suppose that the statement is
true for some k ∈ N and let f ∈ Hk+1(Rd, C) ⊂ Hk (Rd, C). Then by assumption
u ∈ Hk+2(Rd, C), so that u − Δu = u − f = g ∈ Hk+1(Rd, C). It follows that
(1+ x2)Fu = F (u−Δu) = F g ∈ Hˆ k+1(Rd), whence Fu ∈ Hˆ k+3(Rd), and so finally
u ∈ Hk+3(Rd, C). 
Theorem 6.53 is also sometimes called a shift theorem; the reason is that the degree
of regularity (smoothness) of the solution is given by the degree of regularity of the
right-hand side plus the order of the differential operator. The index of regularity is
“shifted” by the order of the operator. We will return to this point in Chapter 9. Next,
however, we wish to use Fourier transforms to show that the imbedding of H1
0 (Ω) in
L2(Ω) is compact for bounded open Ω ⊂ Rd.
Theorem 6.54 (Rellich imbedding theorem) Let Ω ⊂ Rd be bounded and open.
Then the imbedding of H1
0 (Ω) in L2(Ω) is compact.
Proof Let un ∈ H1
0 (Ω) with un H1 ≤ c for all n ∈ N. We need to show that (un)n∈N
has a subsequence which converges in L2(Ω). To this end we consider the extension
fn := u˜n ∈ H1(Rd). Then also  fn H1 ≤ c; see Theorem 6.27. We may assume that
fn  f in H1(Rd) (otherwise we pass to a subsequence; see Theorem 4.35). We
also assume that f = 0 since otherwise we may replace fn by fn − f . We wish to
show that limn→∞ ∫
Rd | fn(x)|2 dx = 0. By Plancherel’s theorem, Theorem 6.45, this
is equivalent to
∫
Rd
|F fn(x)|2 dx → 0, n → ∞.
We will consider the expressions6.7 Local regularity 213
In(R) :=
∫
|x |<R
|F fn(x)|2 dx, Jn(R) :=
∫
|x | ≥R
|F fn(x)|2 dx,
for R > 0 to be determined.
1. Note that for fixed x ∈ Rd,
F(g) := 1
(2π)d/2
∫
Ω
e−ix·yg(y) dy, g ∈ H1
0 (Ω)
defines a continuous linear form F ∈ H1
0 (Ω)

. Thus
F fn(x) = 1
(2π)d/2
∫
Ω
e−ix·y fn(y) dy = 1
(2π)d/2
∫
Rd
1Ω(y)e−ix·y fn(y) dy
converges to 0 as n → ∞ for all x ∈ Rd, since fn  0 in H1(Rd). Since
|F fn(x)| ≤ |Ω|
1/2(2π)
−d/2  fn L2 ≤ c1 for all n ∈ N and the constant function
f (x) ≡ c1 is in L2(B(0, R)), it now follows from the Dominated Convergence
Theorem that limn→∞ In(R) = 0 for all R > 0.
2. Let ε > 0 and choose R > 0 such that c2(1 + R2)
−1 ≤ ε. Then, for all n ∈ N,
Jn(R) =
∫
|x | ≥R
|F fn(x)|2(1 + x2)
−1(1 + x2) dx
≤
1
1 + R2
∫
|x | ≥R
|F fn(x)|2(1 + x2) dx
≤
1
1 + R2 F fn 2
Hˆ 1(Rd) = 1
1 + R2  fn 2
H1(Rd) ≤
1
1 + R2 c2 ≤ ε.
If we use 1., then we obtain
lim sup
n→∞ ∫
Rd
|F fn(x)|2 dx ≤ lim sup
n→∞
(In(R) + Jn(R)) ≤ ε.
Since ε > 0 was arbitrary, it follows that limn→∞ ∫
Rd |F fn(x)|2 dx = 0. 
The proof shows that the theorem remains true if instead of assuming that Ω is
bounded we only assume that it has finite Lebesgue measure.
6.7 Local regularity
Using Fourier transforms we were able to show in the previous section that
Hk+m(Rd) ⊂ Cm(Rd) if k > d/2.
We now wish to prove a local version of this imbedding theorem. Suppose that Ω
is a nonempty open set in Rd. By local properties we mean statements which hold214 6 Hilbert space methods for elliptic equations
in the interior of Ω, or to be more precise, in every set U  Ω. In what follows, the
underlying field will always be K = R. We define
L2,loc(Ω) :=

f : Ω → R measurable :
∫
U
| f (x)|2 dx < ∞
whenever U  Ω
'
,
H1
loc(Ω) :=

u ∈ L2,loc(Ω) : there exists Dju ∈ L2,loc(Ω), j = 1,..., d,
such that ∫
Ω
u(x)
∂ϕ
∂xj
(x) dx = −
∫
Ω
Dju(x) ϕ(x) dx
for all ϕ ∈ D(Ω)
'
.
Thus for each u ∈ H1
loc(Ω) there exists a weak partial derivative Dju ∈ L2,loc(Ω),
which is defined via the integration by parts formula
−
∫
Ω
u(x)
∂ϕ
∂xj
(x) dx =
∫
Ω
Dju(x) ϕ(x) dx, ϕ ∈ D(Ω). (6.52)
Corollary 6.10 shows that Dju is unique. It follows from Lemma 6.12 that
C1(Ω) ⊂ H1
loc(Ω) and Dju = ∂ϕ
∂xj
, j = 1,..., d. (6.53)
We will frequently make use of the following observation: if U  Ω, then u|U ∈
H1(U) for all u ∈ H1
loc(Ω). The following characterization is essential in order to be
able to use results for Rd here. Given a function f : Ω → R, as usual we denote by ˜f : Rd → R the extension of f by 0 from Ω to the whole of Rd (as in (6.20)).
Lemma 6.55 We have H1
loc(Ω) = {u ∈ L2,loc(Ω) : η0u ∈ H1(Rd) ∀η ∈ D(Ω)}.
Moreover, for all u ∈ H1
loc(Ω) and all η ∈ D(Ω),
Dj(η0u) = [(Djη)u + ηDju]
∼. (6.54)
Proof Let u ∈ H1
loc(Ω) and η ∈ D(Ω). Then η0u, [(Djη)u + ηDju]
∼ ∈ L2(Rd), and
for ϕ ∈ D(Rd), if we apply the product rule to ηϕ ∈ D(Ω), then we obtain
−
∫
Rd
η0u(x)
∂ϕ
∂xj
(x) dx = −
∫
Ω
u(x)
∂(ηϕ)
∂xj
(x) dx +
∫
Ω
u(x)
∂η
∂xj
(x) ϕ(x) dx
=
∫
Ω
(Dju)(x)η(x) ϕ(x) dx +
∫
Ω
u(x) (Djη)(x) ϕ(x) dx
=
∫
Rd
[(Dju)η + uDjη]
∼(x)ϕ(x) dx.6.7 Local regularity 215
This proves (6.54). Conversely, let u ∈ L2,loc(Ω) be such that η0u ∈ H1(Rd) for
all η ∈ D(Ω). Choose sets Uk  Uk+1  Ω such that /
k ∈N Uk = Ω, and choose
functions ηk ∈ D(Ω)such that ηk = 1 onUk . Then for all ϕ ∈ D(Uk ) and j = 1,..., d
we have
∫
Rd
ϕ(x) Dj(ηku)
∼(x) dx = −
∫
Rd
(Djϕ)(x) η1ku(x) dx
= −
∫
Rd
(Djϕ)(x) η1mu(x) dx =
∫
Rd
ϕ(x) Dj(η1mu)(x) dx,
whence by Corollary 6.10
Dj(ηku)
∼ = Dj(ηmu)
∼ on Uk for all m ≥ k. (6.55)
We can thus set uj := Dj(ηku)
∼ on Uk as a well-defined function in L2,loc(Ω). Now
given ϕ ∈ D(Ω), then there exists some k ∈ N such that suppϕ ⊂ Uk , and so
−
∫
Ω
u(x)
∂ϕ
∂xj
(x) dx = −
∫
Rd
u1ηk (x)
∂ϕ
∂xj
(x) dx =
∫
Rd
Dj(u1ηk )(x) ϕ(x) dx
=
∫
Ω
uj(x) ϕ(x) dx.
Hence u ∈ H1
loc(Ω) and Dju = uj. 
We now define the local Sobolev space of second order by
H2
loc(Ω) := {u ∈ H1
loc : Dju ∈ H1
loc(Ω), j = 1,..., d}.
Thus for u ∈ H2
loc(Ω) we have DiDju ∈ L2,loc(Ω) for all i, j = 1,..., d. Since we
know that DiDjϕ = DjDiϕ for all test functions ϕ, we can deduce from the definition
of weak derivatives that also
DiDju = DjDiu. (6.56)
More generally, we define higher order spaces recursively:
Hk+1
loc (Ω) := {u ∈ H1
loc(Ω) : Dju ∈ Hk
loc(Ω), j = 1,..., d}.
It follows from (6.53) that Ck (Ω) ⊂ Hk
loc(Ω) for all k ∈ N. We can also obtain a
higher-order version of Lemma 6.55.
Lemma 6.56 We have for all k ∈ N that
Hk
loc(Ω) = {u ∈ L2,loc(Ω) : η0u ∈ Hk (Rd) for all η ∈ D(Ω)}.
Proof For k = 1 this is exactly the statement of Lemma 6.55. So let us assume that the
assertion is true for k ≥ 1, and let u ∈ Hk+1
loc (Ω) and η ∈ D(Ω). Then η0u ∈ H1(Rd),216 6 Hilbert space methods for elliptic equations
and by (6.54) and the induction hypothesis we have Dj(η0u) = [(Djη)u + ηDju]
∼ ∈
Hk (Rd) since Dju ∈ Hk
loc(Ω). Hence η0u ∈ Hk+1(Rd).
Conversely, let u ∈ L2,loc(Ω) be such that η0u ∈ Hk+1(Rd) for all η ∈ D(Ω). Then
u ∈ H1
loc(Ω) and
η2Dju = Dj(η0u) − (
3Djη) u ∈ Hk (Rd).
By the induction hypothesis, it follows that Dju ∈ Hk
loc(Ω), and so u ∈ Hk+1
loc (Ω) by
definition. 
We can now give the desired local imbedding theorem.
Theorem 6.57 Let Ω ⊂ Rd be open, and suppose m ∈ N0 and k ∈ N such that
k > d/2. Then Hk+m
loc (Ω) ⊂ Cm(Ω).
Proof Let u ∈ Hk+m
loc (Ω) and U  Ω. Choose η ∈ D(Ω) such that η = 1 on U. Then
by Corollary 6.50 u0η ∈ Hk+m(Rd) ⊂ Cm(Rd) and thus u ∈ Cm(U). 
We now wish to study the regularity of solutions of the Poisson equation
Δu = f . (6.57)
We will again obtain a shift theorem.
Theorem 6.58 (Local maximal regularity) Given f ∈ L2,loc(Ω), let u ∈ L2,loc(Ω)
be a weak solution of (6.57). Then u ∈ H2
loc(Ω). Moreover, if f ∈ Hk
loc(Ω), then
u ∈ Hk+2
loc (Ω). In particular, if f ∈ C∞(Ω), then u ∈ C∞(Ω) as well.
We will postpone the proof for a moment to examine the statement of the theorem
more closely. We recall that u ∈ L2(Ω) is a weak solution of (6.57) if
∫
Ω
u(x) Δϕ(x) dx =
∫
Ω
f (x) ϕ(x) dx for all ϕ ∈ D(Ω). (6.58)
Of course, here solutions are not going to be unique as we have not stipulated any
boundary conditions. Theorem 6.58 asserts that a weak solution u is automatically
in H2
loc(Ω). As such, u has weak partial derivatives Diu and DiDju, i, j, = 1,..., d,
which are all in L2,loc(Ω); and Δu is given by
Δu =

d
j=1
D2
j u.
Thus Theorem 6.58 may be viewed as a statement about interior maximal regularity:
all derivatives up to the order of the differential equation exist automatically as
elements of the space L2,loc(Ω), the function space in which we wish to solve the
equation. We also say that u is a strong solution of (6.57) if u ∈ H2
loc(Ω). Every
classical solution u ∈ C2(Ω) is also a strong solution.6.7 Local regularity 217
If ϕ, η ∈ C2(Ω), then the product rule for the Laplacian reads
Δ(ηϕ) = (Δη)ϕ + 2∇η ∇ϕ + η Δϕ, (6.59)
as one can easily verify. We will use this formula repeatedly in the following proof.
Proof (of Theorem 6.58) Let u, f ∈ L2,loc(Ω) satisfy −Δu = f weakly. The proof is
divided into three steps:
1. We first show that u ∈ H1
loc(Ω). To this end, take η ∈ D(Ω); we have to show
that η0u ∈ H1(Rd). Now for ϕ ∈ D(Rd) we have
F(ϕ) :=
∫
Rd
η0u(x)(ϕ − Δϕ)(x) dx
=
∫
Ω
{ηuϕ − uΔ(ϕη) + u(Δη)ϕ + 2u∇ϕ∇η}dx
=
∫
Ω
(ηu − f η + (Δη)u)(x) ϕ(x) dx + 2
∫
Ω
(∇ϕ(x)∇η(x)) u(x) dx.
Hence the mapping F : D(Rd) → R is linear and there exists a constant c ≥ 0 such
that |F(ϕ)| ≤ cϕH1(Rd) for all ϕ ∈ D(Rd). This means that F has a continuous
linear extension which maps H1(Rd) to R, and so, by the theorem of Riesz–Fréchet,
there exists a unique v ∈ H1(Rd) such that
∫
Rd
v(x) ϕ(x) dx +
∫
Rd
∇v(x)∇ϕ(x) dx =
∫
Rd
η0u(x)(ϕ − Δϕ)(x) dx
for all ϕ ∈ D(Rd). Since v ∈ H1(Rd), it follows that ∫
Rd v(ϕ − Δϕ) dx = ∫
Rd η0u(ϕ −
Δϕ) dx for all ϕ ∈ D(Rd). Since D(Rd) is dense in H2(Rd) by Corollary 6.26, it
follows that ∫
Rd (v − η0u)(ϕ − Δϕ) dx = 0 for all ϕ ∈ H2(Rd). Now, by Theorem 6.51,
there exists a ϕ ∈ H2(Rd) such that ϕ − Δϕ = (v − η0u). Thus ∫
Rd (v − η0u)
2 dx = 0,
which finally allows us to conclude that η0u = v is in H1(Rd).
2. We next show that given η ∈ D(Ω) we have η0u ∈ H2(Rd). It is not hard to show
using (6.59) that Δ(η0u) = [(Δη)u+2∇η ∇u+η f ]˜ =: g(η) weakly (see Exercise 6.15).
Since by 1. we know that Dju ∈ L2,loc(Ω), the function ∇η ∇u = 
d
j=1 DjηDju is in
L2(Ω) and so g(η) ∈ L2(Rd). It thus follows from Theorem 6.53 that η0u ∈ H2(Rd).
3. We now prove Theorem 6.58 by induction. For k = 0 the statement is proved
in 2. (where H0
loc(Ω) = L2,loc(Ω)). Now assume that the statement is true for k ≥ 0
and let f ∈ Hk+1
loc (Ω); we wish to show that u ∈ Hk+3
loc (Ω). Let η ∈ D(Ω), then by the
induction hypothesis we have u ∈ Hk+2
loc (Ω) and so g(η) ∈ Hk+1(Rd). It follows by
Theorem 6.53 that η0u ∈ Hk+3(Rd).
This completes the proof of Theorem 6.58. 
More generally, one can show that even if we only have u ∈ L1,loc(Ω) (in place of
u ∈ L2,loc(Ω) as in Theorem 6.58) but f ∈ C∞(Ω), then −Δu = f already implies218 6 Hilbert space methods for elliptic equations
that u ∈ C∞(Ω). The following theorem, which goes back to Sobolev, shows that the
maximal regularity property of Theorem 6.58 does not hold for classical derivatives.
This is one reason why the spaces Ck (Ω), k = 0, 1, 2,... are unsuited both to the
numerics of partial differential equations and to the study of nonlinear problems.
The negative statement of the following theorem illustrates clearly how advantageous
Sobolev spaces are.
Theorem 6.59 Let Ω ⊂ Rd be an arbitrary nonempty open set in dimension d ≥ 2.
Then there exist u, f ∈ Cc(Ω) such that Δu = f weakly, but u  C2(Ω).
Proof 1. We first give an example in R2. Let B := {(x, y) ∈ R2 : x2 + y2 < 1
4 }
and u(x, y) := (x2 − y2) log | log r|, (x, y) ∈ B, where r = (x2 + y2)
1/2. Then
u ∈ C2(B\ {0}). Since | log s| ≤ 1
s for s > 0 small and | log s| ≤ s for s > 0 large, we
have the estimate log | log r|≤| log r| ≤ 1
r forr > 0 small and so limr→0 u(x, y) = 0.
We will also denote the continuous extension of u to B by u. Now on B \ {0} we
have
ux = 2x log | log r| + (x3 − y2 x) 1
r2 log r
uxx = 2 log | log r| + (5x2 − y2) 1
r2 log r − (x4 − x2 y2)
2 log r + 1
r4(log r)2 .
Hence the function uxx is unbounded in B \ {0} since on the diagonal {(x, x) : |x| <
1
4 } we have
lim
x→0
uxx(x, x) = lim
x→0

2 log | log r| +
2
log r
'
= ∞.
Since u(x, y) = −u(y, x), we also have uyy(x, y) = −uxx(y, x). This means that the
singular term 2 log | log r| cancels in the expression for Δu = uxx + uyy and
Δu = (x2 − y2)
 4
r2 log r − 1
r2(log r)2

.
(cf. Exercise 6.24). It follows that limr→0 Δu(x, y) = 0.
Now let g denote the continuous extension of Δu to 0; we claim that Δu = g
weakly in B. To see this, we first observe that ux and uy are bounded in B \ {0} (in
fact u ∈ C1(B)). We set Bε := {(x, y) ∈ B : x2 + y2 > ε2} for given 0 <ε< 1
4 and
let ϕ ∈ D(B). Then by Green’s second identity (Corollary 7.7(c))
∫
Bε
u Δϕ dx =
∫
Bε
gϕ dx +
∫
∂Bε

u
∂ϕ
∂ν − ∂u
∂ν ϕ

dσ.
Letting ε → 0, we obtain
∫
B
u Δϕ dx =
∫
B
gϕ dx.6.8 Inhomogeneous Dirichlet boundary conditions 219
This proves that Δu = g weakly. Finally, we take η ∈ D(B) such that η(x, y) = 1 in
a neighborhood of 0; then ηu ∈ Cc(B) and f := ηg + 2∇η ∇u + (Δη)u is in C(B). It
is now easy to see that Δ(ηu) = f weakly (Exercise 6.15). This proves the theorem
for the set B.
2. By adding more coordinates and translating accordingly one easily obtains a
general example as claimed in the statement of the theorem. 
Remark 6.60 The function u from Theorem 6.59 is however in H2(Ω) ∩ H1
0 (Ω). This
follows from Theorem 6.58 since u has compact support. 
6.8 Inhomogeneous Dirichlet boundary conditions
Let Ω be a bounded open set in Rd with boundary ∂Ω. Given f ∈ L2(Ω) and
g ∈ C(∂Ω) we seek a solution of the Poisson problem
−Δu = f in Ω, (6.60a)
u|∂Ω = g on ∂Ω. (6.60b)
Here, both the equation (6.60a) and the boundary condition (6.60b) are inhomoge￾neous. If u ∈ L2(Ω) is a weak solution of (6.60a), then u is automatically in H2
loc(Ω)
by Theorem 6.58 and
Δu =

d
j=1
D2
j u ∈ L2,loc(Ω).
When considering the boundary condition (6.60b), we might first think of a classical
interpretation: if u ∈ C(Ω), then we may demand that u(z) = g(z) for all z ∈ ∂Ω.
However, it is not always possible to find solutions of (6.60a) which are continuous
up to the boundary. This leads us to define the condition (6.60b) in a weak sense.
For this, we need another assumption on the function g.
Definition 6.61 Let g ∈ C(∂Ω).
(a) An H1-extension of g is a function G ∈ C(Ω) ∩ H1(Ω) such that G|∂Ω = g.
(b) An H1-solution u of (6.60) is a function u ∈ H1(Ω) ∩ H2
loc(Ω) such that (6.60a)
holds and g has an H1-extension G such that
u − G ∈ H1
0 (Ω) (6.61)
is satisfied. 
We interpret the condition (b) as a weak form of the inhomogeneous Dirichlet
boundary condition (6.60b). We wish to show that this definition is independent of
the choice of the H1-extension G, that is, that if (6.61) holds for some H1-extension220 6 Hilbert space methods for elliptic equations
G of g, then it holds for all of them. To this end, observe that if G1 ∈ C(Ω)∩H1(Ω) is
another function which satisfies G1|∂Ω = g, then G1 − G ∈ C0(Ω) ∩ H1(Ω) ⊂ H1
0 (Ω)
by Theorem 6.30. Since u − G = (u − G1) + (G1 − G), it follows that u − G ∈ H1
0 (Ω)
if and only if u − G1 ∈ H1
0 (Ω).
We will see in the next section that not every continuous function g ∈ C(∂Ω)
has an H1-extension. However, we only require a very mild additional regularity
assumption on g in order to guarantee the existence of an H1-extension. If it has one,
then we have the following theorem on existence and uniqueness of solutions.
Theorem 6.62 Let f ∈ L2(Ω) and suppose that g ∈ C(∂Ω) has an H1-extension.
Then (6.60) admits a unique H1-solution.
Proof The idea of the proof is as follows. If u ∈ H1(Ω) is an H1-solution, then
v := u − G ∈ H1
0 (Ω) satisfies −Δv = f + ΔG weakly, that is,
∫
Ω
∇v(x)∇ϕ(x) dx =
∫
Ω
f (x)ϕ(x) dx −
∫
Ω
∇G(x)∇ϕ(x) dx
for all ϕ ∈ D(Ω). We will encounter this principle again when studying numerical
methods; it is known as “reduction to homogeneous boundary conditions”. It leads
us to the following proof, which relies on the Lax–Milgram theorem.
1. Existence: let G be an H1(Ω)-extension of g, then
F(ϕ) :=
∫
Ω
f (x)ϕ(x) dx −
∫
Ω
∇G(x)∇ϕ(x) dx, ϕ ∈ H1
0 (Ω),
defines a continuous linear form on H1
0 (Ω). As above,
a(v, ϕ) :=
∫
Ω
∇v(x)∇ϕ(x) dx, v, ϕ ∈ H1
0 (Ω)
defines a continuous and coercive bilinear form on H1
0 (Ω) (see Corollary 6.33).
Hence by the Lax–Milgram theorem there exists a unique v ∈ H1
0 (Ω) such that
a(v, ϕ) = F(ϕ) for all ϕ ∈ H1
0 (Ω). That is, for all ϕ ∈ H1
0 (Ω) we have
∫
Ω
∇v(x) ∇ϕ(x) dx =
∫
Ω
f (x) ϕ(x) dx −
∫
Ω
∇G(x) ∇ϕ(x) dx.
Now set u := G + v, then ∫
Ω ∇u(x) ∇ϕ(x) dx = ∫
Ω f (x) ϕ(x) dx for all ϕ ∈ D(Ω),
that is, −Δu = f weakly. It follows from Theorem 6.58 that u ∈ H2
loc(Ω).
2. Uniqueness: let u1 and u2 be two solutions, and let G1 and G2 be two H1-
extensions of g such that u1 − G1, u2 − G2 ∈ H1
0 (Ω). Then u = u1 − u2 = (u1 − G1) −
(u2 − G2) + (G1 − G2) ∈ H1
0 (Ω) and Δu = 0, whence u = 0 by Theorem 6.43. 
We next show that, for solutions of (6.60a) which are continuous up to the
boundary, the classical boundary condition implies the weak one provided only that6.8 Inhomogeneous Dirichlet boundary conditions 221
g has an H1-extension. This is remarkable because u could oscillate strongly on the
boundary and thus it is not clear a priori whether its derivatives are in L2(Ω).
Theorem 6.63 Let u ∈ C(Ω) ∩ H2
loc(Ω) satisfy (6.60a) and suppose that u(z) = g(z)
for all z ∈ ∂Ω. If g has an H1-extension G, then u ∈ H1(Ω) and u − G ∈ H1
0 (Ω).
Proof Let G ∈ H1(Ω) ∩ C(Ω) be such that G|∂Ω = g. Then v := u − G is in
C0(Ω) ∩ H1
loc(Ω). Now set vn := (v − 1
n )
+, n ∈ N; then vn ∈ Cc(Ω) ∩ H1
loc(Ω) ⊂
H1
c (Ω) ⊂ H1
0 (Ω) by Theorem 6.29. Moreover, by Theorem 6.35 we have Djvn =
1{v> 1
n }Djv, j = 1,..., d. Since −Δu = f , we have
∫
Ω
∇v(x) ∇ϕ(x) dx =
∫
Ω
f (x) ϕ(x) dx −
∫
Ω
∇G(x) ∇ϕ(x) dx. (6.62)
for all ϕ ∈ D(Ω). Fix n ∈ N and choose U  Ω such that vn(x) = 0 for all x  U.
Then vn ∈ C0(U) ∩ H1(U) ⊂ H1
0 (U), meaning that vn can be approximated in H1(U)
by test functions in D(U). Since v|U ∈ H1(U), (6.62) also holds with ϕ = vn. Since
∇v ∇vn = 
d
j=1 Djv Djv χ{v> 1
n } = |∇vn|
2, it follows that
∫
Ω
|∇vn(x)|2 dx =
∫
Ω
∇v(x)∇vn(x) dx
=
∫
Ω
f (x)vn(x) dx −
∫
Ω
∇G(x)∇vn(x) dx
≤  f L2 vn L2 + c1
∫
Ω
|∇vn(x)|2 dx 1
2
≤  f L2 vn L2 + c2
1
2
+
1
2
∫
Ω
|∇vn(x)|2 dx
by Young’s inequality, where c1 :=
∫
Ω |∇G(x)|2 dx 1
2
= |G|H1(Ω). Since vn L2 ≤
v L2 , it follows that (vn)n∈N is bounded in H1
0 (Ω). Hence, appealing to Theo￾rem 4.35, passing to a subsequence if necessary, we may assume that vn  w
for some w ∈ H1
0 (Ω). But since by definition vn → v+ in L2(Ω), it follows that
v+ = w ∈ H1
0 (Ω). By replacing u by −u, we see that v− = (−v)
+ ∈ H1
0 (Ω) as well.
We conclude that v ∈ H1
0 (Ω) and thus u = v + G ∈ H1(Ω), which completes the
proof. 
The essential point in Theorem 6.63 is that we have complete freedom when
choosing H1-extensions of g. Nevertheless, the solution itself is always in H1(Ω) as
long as such an extension exists.222 6 Hilbert space methods for elliptic equations
6.9 The Dirichlet problem
In this section we will study the Dirichlet problem, that is, the special case where
f = 0 in (6.60). Our goals are threefold:
(a) We will show that weak solutions can be characterized by minimal energy (that
is, minimal H1-seminorm |·|H1 ).
(b) We will give an example to show that even for very simple regular domains,
there exist classical solutions with infinite energy.
(c) We will investigate more closely which conditions on the domain guarantee that
the Dirichlet problem always admits a classical solution, for all boundary values.
Let Ω ⊂ Rd be bounded and open and denote by ∂Ω its boundary. Given g ∈ C(∂Ω),
the Dirichlet problem consists in finding a solution of
Δu = 0 in Ω, u|∂Ω = g, (6.63)
Let us briefly recall the interior regularity theorem, Theorem 6.58, which we obtained
for solutions of the Laplace equation: whenever u ∈ L2,loc(Ω) and Δu = 0 holds
weakly in Ω, the function u is in fact in C∞(Ω). Thus we may automatically suppose
that all solutions are C∞-functions. We still need to investigate in which sense the
boundary condition u|∂Ω = g is satisfied. If g has an H1-extension, that is, if there
exists a function G ∈ C(Ω)∩H1(Ω) which coincides with g on ∂Ω, then (6.63) admits
exactly one H1-solution by Theorem 6.62, namely, a function u ∈ H1(Ω) ∩ C∞(Ω)
such that Δu = 0 and u − G ∈ H1
0 (Ω). This H1-solution may be characterized by the
following minimality property.
Theorem 6.64 (Dirichlet principle) Let g ∈ C(∂Ω) have an H1-extension G and
let u be the H1-solution of (6.63). Then
∫
Ω
|∇u(x)|2 dx <
∫
Ω
|∇w(x)|2 dx
for all w ∈ H1(Ω) such that w − G ∈ H1
0 (Ω) and w  u. That is, the solution u is
exactly the function with minimal energy
|u|
2
H1(Ω) =
∫
Ω
|∇u(x)|2 dx
among all functions w in H1(Ω) which satisfy the boundary condition w = g in the
weak sense.
Proof Let w ∈ H1(Ω) be such that w − G ∈ H1
0 (Ω) and w  u, then ϕ := w − u =
w − G + G − u ∈ H1
0 (Ω). Since ϕ  0, we have ∫
Ω |∇ϕ|
2 dx > 0. Since Δu = 0 and
ϕ ∈ H1
0 (Ω), it follows that ∫
Ω ∇u(x)∇ϕ(x) dx = 0, and so6.9 The Dirichlet problem 223
∫
Ω
|∇w(x)|2 dx =
∫
|∇(u + ϕ)(x)|2 dx
=
∫
Ω
|∇u(x)|2 dx + 2
∫
Ω
∇u(x)∇ϕ(x) dx +
∫
Ω
|∇ϕ(x)|2 dx
=
∫
Ω
|∇u(x)|2 dx +
∫
Ω
|∇ϕ(x)|2 dx >
∫
Ω
|∇u(x)|2 dx,
which is exactly the claim. 
By definition, H1-solutions of the Dirichlet problem have finite energy. We now
wish to consider classical solutions: given g ∈ C(∂Ω), a classical solution of (6.63)
is a function u ∈ C2(Ω) ∩ C(Ω) such that Δu = 0 in Ω and u|∂Ω = g.
Theorem 6.65 Let u be a classical solution. Then the following three assertions are
equivalent:
(i) The function u has finite energy, that is, |u|H1(Ω) < ∞.
(ii) u is an H1-solution.
(iii) g has an H1-extension.
Proof (i) ⇒ (iii): Since u ∈ C(Ω), G = u is an H1-extension of u.
(iii) ⇒ (ii): This is exactly Theorem 6.63.
(ii) ⇒ (i) is clear since u ∈ H1(Ω). 
We will now illustrate with an example that there exist classical solutions which
do not have finite energy and thus are not H1-solutions. To this end we consider
the unit disk D in R2, that is, D = {x ∈ R2 : |x| < 1} with the unit circle
∂D = {x ∈ R2 : |x| = 1} as its boundary. Given g ∈ C(∂D), the Dirichlet problem
(6.63) admits the classical solution u ∈ C∞(D) ∩ C(D) given by
u(r cos θ,r sin θ) = c0 +
∞
k=1
rk

ak cos(kθ) + bk sin(kθ)

(6.64)
for 0 ≤ r < 1 and θ ∈ R, where
ak = 1
π
∫ 2π
0
g(cos θ,sin θ) cos(kθ)dθ, bk = 1
π
∫ 2π
0
g(cos θ,sin θ)sin(kθ)dθ,
c0 = 1
2π
∫ 2π
0
g(cos θ,sin θ)dθ;
see Theorem 3.30. We can now express the energy of u in terms of the Fourier
coefficients ak and bk as follows.
Lemma 6.66 Let u be defined as in (6.64). Then
∫
D
|∇u(x)|2 dx = π
∞
k=1
k(a2
k + b2
k ). (6.65)224 6 Hilbert space methods for elliptic equations
Proof We recall that for continuous functions f : D → [0, ∞) we have
∫
D
f (x) dx =
∫ 1
0
∫ 2π
0
f (r cos θ,r sin θ)dθ r dr, (6.66)
see Theorem A.12. Set v(r, θ) := u(r cos θ,r sin θ). By differentiating v in r and θ
and solving for ux and uy, we can show that
|∇u|
2(r cos θ,r sin θ) =

v2
r + v2
θ
r2

(r, θ), (6.67)
cf. Exercise 6.16. Plugging this into (6.66), we obtain
∫
D
|∇u(x)|2 dx =
∫ 1
0
r
∫ 2π
0

v2
r + v2
θ
r2

dθ dr. (6.68)
Now consider the function vk (r, θ) := rk (ak cos(kθ) + bk sin(kθ)). Its partial deriva￾tives vkr und vkθ satisfy v2
kr + v2
kθ
r2 = k2r2k−2(a2
k + b2
k ), and so
∫
D
|∇u(x)|2 dx =
∞
k=1
k2(a2
k + b2
k )
∫ 1
0
r
∫ 2π
0
r2k−2dθ dr = π
∞
k=1
k(a2
k + b2
k ),
which proves the claim. 
We can now describe the classical example of Hadamard from 1906.
Example 6.67 (Hadamard (1906)) Set
g(cos θ,sin θ) :=
∞
n=1
2−n cos(22nθ).
Then g ∈ C(∂D) since the series converges normally. Let u be the classical solution
of (6.63) with boundary value g (cf. Theorem 3.30). We claim that ∫
D |∇u|
2 dx = ∞,
which means in particular that u  H1(D). In fact, no function G ∈ H1(D) ∩ C(D)
exists which also satisfies G|∂D = g. 
Proof We have bk = 0 for all k ∈ N, while ak = 2−n for k = 22n and ak = 0 for all
k  {22n : n ∈ N}. By (6.65) we thus have
∫
D
|∇u(x)|2 dx = π
∞
n=1
22n2−2n = ∞.
The last assertion of the example follows from Theorem 6.65. 
We now know that even for a domain as regular as the unit disk the space6.9 The Dirichlet problem 225
W(∂Ω) := {g ∈ C(∂Ω) : ∃ G ∈ C(Ω) ∩ H1(Ω)such that G|∂Ω = g}
is a proper subspace of C(∂Ω). It is however dense; in fact, we have:
Lemma 6.68 The space D(∂Ω) := {G|∂Ω : G ∈ D(Rd)} is dense in C(∂Ω).
We will give two proofs of Lemma 6.68:
Proof (1st proof of Lemma 6.68) The space D(∂Ω) is a subalgebra ofC(∂Ω) which
in particular contains the constant functions. Let y, y¯ ∈ ∂Ω be any two distinct
points on the boundary, y  y¯. Then there exists an index j ∈ {1,..., d} for which
yj  y¯j. Let η ∈ D(Rd) be such that η ≡ 1 on Ω (see Lemma 6.7) and set
G(x) := η · xj, x ∈ Rd; then G ∈ D(Rd) and G(y)  G(y¯). This shows that D(∂Ω)
separates points in ∂Ω and so the claim follows from the Stone–Weierstrass theorem
(see Theorem A.5 on page 426). 
Proof (2nd proof of Lemma 6.68) Let g ∈ C(∂Ω), then by the Tietze extension
theorem there exists a G ∈ Cc(Rd) such that G|∂Ω = g. Let Gn = n ∗ G, then
Gn ∈ D(Rd), and limn→∞ Gn = G holds uniformly in Rd, see Theorem 6.11. 
If g ∈ W(∂Ω), then the H1-solution of (6.63) is bounded and continuous on Ω.
Lemma 6.69 Given g ∈ W(∂Ω), let u be the H1-solution of (6.63). Then
u(x) ≤ max
z ∈∂Ω g(z) for all x ∈ Ω. (6.69)
Bear in mind that u ∈ C∞(Ω).
Proof Set c := maxz ∈∂Ω g(z) and suppose that G ∈ C(Ω)∩H1(Ω)satisfies G|∂Ω = g.
Let v ∈ H1
0 (Ω) be the unique solution of
∫
Ω
∇v(x) ∇ϕ(x) dx =
∫
Ω
∇G(x) ∇ϕ(x) dx, ϕ ∈ H1
0 (Ω).
Then u := G + v is the H1-solution of (6.63); see the proof of Theorem 6.62.
Since g ≤ c, we have (G − c)
+ ∈ C0(Ω) ∩ H1(Ω) ⊂ H1
0 (Ω); moreover, since
(u − c)
+ ≤ (G − c)
+ + v+, it follows from Theorem 6.39(b) that (u − c)
+ ∈ H1
0 (Ω).
It now follows from the weak maximum principle (Theorem 6.44) that u(x) ≤ c for
all x ∈ Ω, that is, (6.69) holds. 
If we apply (6.69) to −g in place of g, then we obtain that
min
z ∈Ω g(z) ≤ u(x) ≤ max
z ∈Ω g(z) (6.70)
for all x ∈ Ω, where u is again the H1-solution of (6.63). Let us now consider the
mapping T : W(∂Ω) → Cb(Ω) which assigns to each g ∈ W(∂Ω) the H1-solution
of (6.63) which has boundary value g; here we have written
Cb(Ω) := {v : Ω → R : v is bounded and continuous},226 6 Hilbert space methods for elliptic equations
which is a Banach space with respect to the supremum norm
v ∞ := sup
x∈Ω
|v(x)|.
It follows from the uniqueness of H1-solutions that T is linear.
Furthermore, (6.70) implies that the mappingT : W(∂Ω) → Cb(Ω) is contractive,
that is,
Tg∞ ≤ gC(∂Ω) , g ∈ W(∂Ω). (6.71)
Since W(∂Ω) is dense in C(∂Ω), by Theorem A.2 the mapping T has a unique
contractive extension T˜ : C(∂Ω) → Cb(Ω) to the whole of C(∂Ω). Here we are
using the expression “contraction” in the non-strict sense, that is, for operators of
norm ≤ 1. Such operators are also sometimes called “non-expansive”.
Definition 6.70 Given g ∈ C(∂Ω), the function ug := T˜g is called the Perron
solution of (6.63). 
The Perron solution is harmonic and satisfies the maximum principle.
Theorem 6.71 Given g ∈ C(∂Ω), let ug be the Perron solution of (6.63). Then
(a) ug ∈ C∞(Ω) and Δug = 0 in Ω;
(b) minz ∈∂Ω g(z) ≤ ug(x) ≤ maxz ∈∂Ω g(z) for all x ∈ Ω.
Proof (a) Let gn ∈ W(∂Ω) be such that gn → g in C(∂Ω). Then it follows from the
definitions that ug(x) = limn→∞ ugn (x) uniformly in Ω. Since Δugn = 0, we have
that
∫
Ω
ug(x) Δϕ(x) dx = lim
n→∞ ∫
Ω
ugn (x) Δϕ(x) dx = 0
for all ϕ ∈ D(Ω). This establishes that Δug = 0 weakly in Ω. It now follows from
Theorem 6.58 that ug ∈ C∞(Ω).
(b) The claim follows by passing to the limit in (6.70). 
Remark 6.72 One can show that if u is a classical solution of (6.63), then ug = u.
Thus the Perron solution is a generalization of the classical solution. We refer to the
additional comments at the end of the chapter for more on this point. 
We now introduce an analyticity condition for points z ∈ ∂Ω on the boundary of
our domain.
Definition 6.73 A barrier function, or barrier for short, at the point z ∈ ∂Ω is a
function b ∈ C(Ω ∩ B), where B = B(z,r) is a ball of centre z and radius r > 0,
which satisfies the following three conditions:
(a) b is superharmonic in Ω ∩ B, that is,
∫
Ω
b(x) Δϕ(x) dx ≤ 0 for all 0 ≤ ϕ ∈ D(Ω ∩ B);6.9 The Dirichlet problem 227
(b) b(x) > 0 for all x ∈ Ω ∩ B \ {z}; and
(c) b(z) = 0.
The function b is called an H1-barrier if additionally b ∈ H1(B ∩ Ω). 
The following theorem then holds.
Theorem 6.74 Given z ∈ ∂Ω, suppose that there exists an H1-barrier at z. Then for
every g ∈ C(∂Ω) we have
lim
Ω#x→z
ug(x) = g(z), (6.72)
where ug is the Perron solution of (6.63).
Proof We first consider a special case.
1st case: there exists G ∈ D(Rd) such that G|∂Ω = g. Let v ∈ H1
0 (Ω) satisfy
−Δv = ΔG; then ug = G + v by definition of the H1-solution of (6.63). Now fix
ε > 0; we will show that
lim sup
Ω#x→z
ug(x) ≤ g(z) + ε. (6.73)
Let B = B(z,r) be a ball which is so small that, in addition to the existence of an
H1-barrier b ∈ H1(Ω ∩ B) ∩ C(Ω ∩ B), the inequality
G(x) − g(z) − ε ≤ 0 for all x ∈ B (6.74)
holds. This is possible since G is continuous at z and G(z) = g(z).
Now set w(x) := ug(x) − g(z) − ε − b(x), x ∈ Ω ∩ B. By property (b) of barriers,
by multiplying b by a positive constant if necessary, we may assume that
b(x)≥ug L∞(Ω) − g(z) for all x ∈ ∂B ∩ Ω. (6.75)
Then w+ ∈ H1
0 (Ω ∩ B), as we will show presently. Since w ∈ H1(Ω ∩ B) and
−Δw = Δb ≤ 0, it then follows from the weak maximum principle (Theorem 6.44)
that w ≤ 0, whence ug ≤ g(z) + ε + b. Since b(z) = 0, we finally obtain that
lim supΩ#x→z ug(x) ≤ g(z) + ε.
It remains to show that w+ ∈ H1
0 (Ω ∩ B). Now we have w ∈ H1(Ω ∩ B) and
w = v + G − G(z) − ε − b. Since G − G(z) − ε ≤ 0 and b ≥ 0 in Ω ∩ B, it follows that
w+ ≤ v+. Moreover, by (6.75), the inequality w(x) = ug(x) − G(z) − ε − b(x)≤−ε
holds for all x ∈ ∂B ∩ Ω. Since v ∈ H1
0 (Ω), there exist functions ϕn ∈ D(Ω) such
that ϕn → v in H1(Ω). Then the sequence ϕ+
n ∧ w+ converges to v+ ∧ w+ = w+
in H1(Ω ∩ B) (see Theorem 6.37). Now ϕ+
n ∧ w+ ∈ Cc(Ω ∩ B) ∩ H1(Ω ∩ B) since
ϕ+
n ∈ Cc(Ω) and w+ = 0 in a neighborhood of ∂B ∩ Ω. Thus ϕ+
n ∧ w+ ∈ H1
0 (Ω ∩ B),
and so finally w+ ∈ H1
0 (Ω ∩ B). This completes the proof of (6.73).
2nd case: take g ∈ C(∂Ω) arbitrary and fix ε > 0. Then there exists a function
h ∈ D(∂Ω) such that h − gC(∂Ω) ≤ ε/2. Hence, by Theorem 6.71(b),228 6 Hilbert space methods for elliptic equations
|ug(x) − g(z)| ≤ |ug(x) − uh(x)| + |uh(x) − h(z)| + |h(z) − g(z)|
≤ 2g − hC(∂Ω) + |uh(x) − h(z)| ≤ ε + |uh(x) − h(z)|
for all x ∈ Ω. It thus follows from the 1st case that lim supΩ#x→z |ug(x) − g(z)| ≤ ε.
Since ε > 0 was arbitrary, the proof is complete. 
It turns out that the barrier criterion is also a necessary condition for existence,
which allows us to formulate the following major theorem.
Theorem 6.75 The following assertions are equivalent.
(i) For each g ∈ C(∂Ω) there exists a classical solution u ∈ C2(Ω)∩C(Ω) of (6.63).
(ii) At every point z ∈ ∂Ω there exists an H1-barrier.
(iii) At every point z ∈ ∂Ω there exists a barrier.
Proof (i) ⇒ (ii): Given z ∈ ∂Ω, define g(x) = |z − x|
2; then g ∈ C∞(Rd). Let u be
the solution (6.63). By applying the strong maximum principle [28, 2.2 Theorem 4]
on every connected component of Ω, we see that u(x) > 0 for all x ∈ Ω \ {z}. Thus
u is an H1-barrier at z.
(ii) ⇒ (iii): This follows immediately from the definitions.
We refer to [25] for the implication (iii) ⇒ (i). 
We call the domain Ω Dirichlet regular if for every g ∈ C(∂Ω) there exists
a classical solution of the Dirichlet problem (6.63). By Theorem 6.75, Ω is thus
Dirichlet regular if and only if there exists an H1-barrier at every point z ∈ ∂Ω.
Example 6.76 In dimension d = 1, every bounded open subset Ω of R is Dirichlet
regular. 
Proof Let z ∈ ∂Ω and set b(x) := |x − z|. Then b ∈ C∞(R \ {z}) and Δb = 0 in
R \ {z}. Hence b is an H1-barrier at z. 
In the plane, the segment condition is sufficient for Dirichlet regularity.
Definition 6.77 The set Ω ⊂ Rd satisfies the segment condition if for every z ∈ ∂Ω
there exists an x0 ∈ Rd \ {z} such that λx0 + (1 − λ)z  Ω for all λ ∈ [0, 1], cf.
Figure 6.2 (page 229). 
Theorem 6.78 If Ω ⊂ R2 satisfies the segment condition, then Ω is Dirichlet regular.
Proof Let z ∈ ∂Ω. We may assume without loss of generality that z = 0 and
Ω ∩ B(0,r0)⊂{reiθ : 0 < r < r0, −π<θ<π} (otherwise we translate and rotate
Ω). Then
b(r, θ) := − log r
(log r)2 + θ2 , −π < θ < π, 0 < r < r0
defines a barrier. 6.9 The Dirichlet problem 229
Fig. 6.2 The segment condition is satisfied for the domains on the left and in the center, but not
for the domain on the right (the punctured disk).
Remark 6.79 (a) In the plane there is a very general and practical sufficient condition
for Dirichlet regularity: every bounded, open, simply connected set is Dirichlet
regular (see [23]).
(b) The punctured disk Ω = {x ∈ R2 : 0 < |x| < 1} (see Figure 6.2) is not Dirichlet
regular; see Exercise 3.19. 
We now consider higher dimensions, and start with a rather crude criterion.
Theorem 6.80 Let Ω ⊂ Rd be bounded and open, where d ≥ 2, and suppose that
Ω satisfies the exterior sphere condition, that is, for every z ∈ ∂Ω there exists an
x0 ∈ Rd \ {z} such that
Ω ∩ B(x0, |x0 − z|) = {z}, (6.76)
cf. Figure 6.3. Then Ω is Dirichlet regular.
Proof Let d ≥ 3 and z ∈ ∂Ω. Choose an x0 ∈ Rd \ {z} such that (6.75) holds. Then
b(x) :=
 1
|z − x0|
d−2 − 1
|x − x0|
d−2
'
defines a barrier. For d = 2 see [25, 4.18, p. 274]. 
Corollary 6.81 Every bounded, open convex set in Rd is Dirichlet regular.
Proof Let z ∈ ∂Ω. The Hahn–Banach theorem (see, e.g., [54, Thm. 12.12]) implies
the existence of a c ∈ Rd such that x · c < c · z for all x ∈ Ω. We suppose without
loss of generality that z = 0 and then choose x0 = c and r = |x0|. We claim that then
Ω∩B(x0, |x0−z|) = {z} = {0}. Indeed, suppose that |x−c|≤|x0−z| = r and x  0;
then x · c > 0 and so x  Ω. Now observe that |x|
2 −2x · c+r2 = (x −c)·(x −c) ≤ r2.
230 6 Hilbert space methods for elliptic equations
In Chapter 7 we will describe the degree of regularity of the boundary of an open
set. For example, the boundary may be C1 or only Lipschitz. Every polygon in R2
has Lipschitz boundary, as does every convex set. We mention the following result
without proof (see [25]).
Theorem 6.82 Let Ω ⊂ Rd be a bounded open set with Lipschitz boundary. Then Ω
is Dirichlet regular.
Fig. 6.3 Depiction of the exterior sphere condition for d = 2.
Fig. 6.4 The Lebesgue cusp set.
We mention that in R3 the continuity of the boundary does not automatically
imply Dirichlet regularity. By rotating a cusp, one can generate a domain in R3
which is not Dirichlet regular if the cusp is pointy enough and points into the domain
(see Figure 6.4). Such an example was given by Lebesgue in 1912. We refer to [10],6.10 Elliptic equations with Dirichlet boundary conditions 231
where the details can be found and Fig. 6.4 is produced by Maple®. This shows in
particular that the regularity of the given boundary function g does not imply the
existence of a classical solution: in this example, it is possible to choose the function
g ∈ ∂Ω in such a way that at every point z ∈ ∂Ω it has a harmonic extension to a
neighborhood of z in R3, but nevertheless no solution of the Dirichlet problem exists.
The Perron solution ug oscillates in the vicinity of the cusp point z0 and does not
have a continuous extension to z0. This example also shows that in R3 the segment
condition is not sufficient for the Dirichlet regularity of the domain.
6.10 Elliptic equations with Dirichlet boundary conditions
In this section, in place of the Laplacian we wish to consider more general elliptic
operators with variable coefficients. Let Ω ⊂ Rd be a bounded open set and suppose
that ai j, bj, c ∈ L∞(Ω), where the ai j satisfy, for all ξ ∈ Rd and x ∈ Ω,

d
i,j=1
ai j(x)ξiξj ≥ α|ξ|
2
, (6.77)
for some fixed α > 0. We refer to (6.77) as a uniform ellipticity condition.
Remark 6.83 In the symmetric case, that is, when ai j = aji, the condition (6.77)
is equivalent to the condition that the smallest eigenvalue of the coefficient-matrix
(ai j(x))i,j=1,...,d is at least as large as α. We wish, however, to allow also non￾symmetric matrices as well. 
Our goal is to study boundary value problems of the form
−

d
i,j=1
Di(ai jDju) +

d
j=1
bjDju + cu = f, (6.78a)
u|∂Ω = 0, (6.78b)
where f ∈ L2(Ω) is given. If ai j(x) = 1 for i = j and ai j = 0 for i  j, bj = c = 0,
then (6.78) reduces to the Poisson equation −Δu = f which we considered above.
The first thing to strike us about (6.78) is that under our very general assumptions,
the expression Di(ai jDju) may not be well defined, even for twice continuously
differentiable functions u. We will, however, once again work with weak solutions.
We first suppose that ai j ∈ C1(Ω). In this case, if u ∈ C2
c (Ω) is a solution of (6.78a),
then by integration by parts we obtain
∫
Ω
 
d
i,j=1
ai j(x)Dju(x)Diϕ(x) +

d
j=1
bj(x)Dju(x)ϕ(x) + c(x)u(x)ϕ(x)
'
dx232 6 Hilbert space methods for elliptic equations
=
∫
Ω
f (x)ϕ(x) dx (6.79)
for all ϕ ∈ D(Ω). Now observe that this expression is well defined as soon as
u, ϕ ∈ H1(Ω).
Definition 6.84 A weak solution of (6.78) is a function u ∈ H1
0 (Ω) for which (6.79)
holds for all ϕ ∈ D(Ω). 
This definition leads us to the bilinear form
a(u, v):=
∫
Ω

d
i,j=1
ai j(x)Dju(x)Div(x)+

d
j=1
bj(x)Dju(x)v(x)
+ c(x)u(x)v(x)
'
dx,
u, v ∈ H1
0 (Ω). Clearly, the form a : H1
0 (Ω) × H1
0 (Ω) → R is bilinear and continuous.
It is also coercive provided that the coefficients satisfy appropriate conditions. In
this case we obtain the existence and uniqueness of solutions of (6.78).
Theorem 6.85 Suppose, in addition to (6.77), that one of the following two conditions
holds:
(a) 
d
j=1 bj(x)
2 ≤ 2α c(x), x ∈ Ω, or
(b) bj ∈ C1(Ω) and 
d
j=1(Djbj)(x) ≤ 2c(x), x ∈ Ω.
Then for every f ∈ L2(Ω) there exists a unique solution of (6.78).
Proof Let f ∈ L2(Ω). By definition, a function u ∈ H1
0 (Ω) is a weak solution of
(6.78) if and only if
a(u, ϕ) =
∫
Ω
f (x)ϕ(x) dx for all ϕ ∈ D(Ω). (6.80)
Since D(Ω) is dense in H1
0 (Ω), this is equivalent to (6.80) holding for all ϕ ∈ H1
0 (Ω).
The mapping ϕ → ∫
Ω f (x)ϕ(x) dx defines a continuous linear form on H1
0 (Ω), and
so by the Lax–Milgram theorem there exists a unique weak solution, provided that
a(·, ·) is coercive. We will now show that this is indeed the case if (a) or (b) is
satisfied.
For (a), we apply Young’s inequality (5.19) to obtain
ubjDju ≤ α
2
(Dju)
2 +
1
2α b2
ju2.
Hence, by (6.77),
a(u) ≥ α
∫
Ω
|∇u(x)|2 dx − α
2
∫
Ω
|∇u(x)|2 dx − 1
2α
∫
Ω

d
j=1
bj(x)
2
u(x)
2 dx6.11 H2-regularity 233
+
∫
Ω
c(x)u(x)
2 dx ≥ α
2
∫
Ω
|∇u(x)|2 dx.
Since the expression

α
2
∫
Ω
|∇u(x)|2 dx 1
2
=
(α
2 |u|H1 (Ω)
defines an equivalent norm on H1
0 by the Poincaré inequality (Theorem 6.32), this
proves the coercivity of a(·, ·).
For (b) we integrate by parts and obtain, for u ∈ D(Ω),
∫
Ω
bj(x)Dju(x) u(x) dx =
∫
Ω
bj
1
2
Dju2 dx = −1
2
∫
Ω
Djbj(x)u(x)
2 dx
for all j = 1,..., d, whence
∫
Ω

d
j=1
bj(x)Dju(x)u(x) dx +
∫
Ω
c(x)u(x)
2 dx
= −1
2
∫
Ω

d
j=1
Djbj(x)u(x)
2 dx +
∫
Ω
c(x)u(x)
2 dx ≥ 0.
This shows that a(u) ≥ α
∫
Ω |∇u(x)|2 dx for all u ∈ D(Ω) and hence for all u ∈
H1
0 (Ω). 
6.11 H2-regularity
Let Ω be a bounded open set in Rd. We have seen that solutions of the Poisson
equation always belong to H2
loc(Ω). Here we wish to pursue the question of what
conditions actually guarantee that the solutions are in H2(Ω). This revolves around
the H2-regularity at the boundary. This question is also important for error estimates
for numerical methods (cf. Chapter 9) and depends in general on the regularity of
the boundary itself. A set Ω is said to be convex if whenever x, y ∈ Ω, we also have
λx + (1 − λ)y ∈ Ω for all λ ∈ (0, 1). We recall that for every f ∈ L2(Ω) there exists
a unique solution
u ∈ H1
0 (Ω) ∩ H2
loc(Ω), (6.81a)
− Δu = f, (6.81b)
of the Poisson equation (Theorems 6.43 and 6.58).234 6 Hilbert space methods for elliptic equations
Theorem 6.86 If Ω is convex, then for every f ∈ L2(Ω) the solution u of (6.81) is in
H2(Ω). Moreover, there exists a constant c2 > 0 such that uH2(Ω) ≤ c2  f L2(Ω).
We refer to [33] for the proof of the first assertion. The existence of the constant
c2 is then an immediate consequence of the Closed Graph Theorem.
Remark 6.87 The conclusion of Theorem 6.86 continues to hold when the open set
Ω has C2-boundary (see Definition 7.1) without necessarily being convex; see [18,
Theorem 9.25] or [28, §6.3, Theorem 4]. 
We next wish to give an example to show that the solution of (6.81) need not be
in H2(Ω) without any further assumptions on the boundary. We will consider the
following domain in R2 = C: for π<α< 2π and r0 > 0 we set
Ωα,r0 := {reiθ : 0 ≤ r < r0, 0 <θ<α},
cf. Figure 6.5. We start by considering the Dirichlet problem. Let h(z) := zβ, where
Fig. 6.5 The set Ωα,r0 .
β = π
α . Then h is holomorphic in Ωα,r0 and continuous in Ωα,r0 . Hence the imaginary
partv = Im h of h is harmonic. We have v(r cos θ,r sin θ) = rβ sin(βθ) and v|∂Ωα,r0
=
g, where g(r0eiθ ) = r
β
0 sin(βθ) for 0 <θ<α, and g(reiα) = g(r) = 0 for 0 ≤ r ≤ r0.
As with every holomorphic function, we have h
(z) = (Im h)y(z) + i(Im h)x(z), and
so vy + ivx = βzβ−1 for our function v. If we differentiate this function again, the
same argument yields β(β − 1)zβ−2 = vxy + ivxx. Since
∫
Ω
|zβ−2|
2 dz =
∫ r0
0
rαr(β−2)2 dr = α
∫ r0
0
r2β−3 dr = ∞,
it follows that v  H2(Ωα,r0 ). However, it follows from Theorem 6.63 that v ∈
H1(Ωα,r0 ) (which one can also check directly using (6.67)). We next modify this
example to exhibit a solution of the Poisson equation which is not in H2(Ω).6.11 H2-regularity 235
Let η ∈ D(R2) satisfy η(z) = 0 for |z| ≤ r0
3 and η(z) = 1 for z ∈ Ωα,r0 such
that |z| ≥ r0
2 . Then for G(z) := η(z) Im h(z) we have G ∈ C∞(Ωα,r0 ) and G|∂Ω = g.
Now let u ∈ H1
0 (Ωα,r0 ) be the unique function for which −Δu = ΔG =: f in Ωα,r0 .
Then we know from Theorem 6.63 that v = G + u. Hence u  H2(Ωα,r0 ), since
v  H2(Ωα,r0 ). We have thus shown the following result.
Example 6.88 There exists a function f ∈ C∞(Ωα,r0 ) such that the solution u of
(6.81) (with Ω = Ωα,r0 ) satisfies u ∈ C(Ωα,r0 ), but u  H2(Ωα,r0 ). 
One can extend this example to every bounded open set with a non-convex (that
is, inward-pointing) corner.
Definition 6.89 Let Ω ⊂ R2 be bounded and open and z ∈ ∂Ω. We say that Ω has
a non-convex corner at z, if in a suitably chosen Cartesian coordinate system there
exist an angle π<α< 2π and some ε > 0 such that
z + reiθ ∈ Ω for 0 < r ≤ ε, θ ∈ (0, α),
z + reiθ  Ω for 0 ≤ r < ε, α ≤ θ ≤ 2π.
Compare Figure 6.6. 
Fig. 6.6 A domain Ω with a non-convex corner at z.
Theorem 6.90 Let Ω ⊂ R2 be a bounded open set with a non-convex corner. Then
there exists a function u ∈ H1
0 (Ω) ∩ H2
loc(Ω) such that −Δu =: f ∈ L2(Ω), but
u  H2(Ω).
Proof Let U := {reiθ + z : 0 < r < ε, 0 <θ<α}. By Example 6.88 there exists a
function w ∈ H1
0 (U) ∩ H2
loc(U) such that −Δw = g ∈ L2(U), but w  H2(U0), where
U0 := U ∩ B 
z, ε
2
	
. Extend w by 0 on Ω\U; then w ∈ H1
0 (Ω) by Theorem 6.27. Now
let η ∈ D(Rd) satisfy suppη ⊂ B(z, ε) and η ≡ 1 on B 
z, ε
2
	
, and set u := wη. Then
u  H2(Ω), but u ∈ H1
0 (Ω) ∩ H2
loc(Ω) and −Δu = −ηg − 2∇w∇η − wΔη ∈ L2(Ω). 236 6 Hilbert space methods for elliptic equations
In Chapter 9 we will return to the example of the L-shaped domain Ω = (−1, 1)
2 \
[0, 1)
2. Clearly, Ω has a non-convex corner at the origin, meaning that the Poisson
problem is not H2-regular, that is, the conclusion of Theorem 6.86 does not hold.
6.12* Comments on Chapter 6
The Dirichlet problem is one of the oldest and most frequently studied problems in analysis. The
name was coined by Riemann in honor of his teacher Johann P.G.L. Dirichlet (1805–1859), who
lectured at the University of Berlin from 1831 to 1855 and succeeded Gauss (Gauß) at the University
of Göttingen in 1855. In his lectures on potential theory, he spoke about the Dirichlet principle (also
later named after him by Riemann). This led to a sustained mathematical dispute after Weierstrass
gave an example in 1869 which showed that the minimum of energy integrals need not be attained.
It was Arzelà in 1896 and Hilbert in 1900 who, completely independently of each other, gave the
first rigorous solution of the Dirichlet problem using the variational principle. But the question
of whether every function u ∈ C(Ω) such that Δu = 0 in Ω has finite energy ∫
Ω |∇u(x)|2 dx,
remained open. A first counterexample was obtained by Friedrich Prym in 1871; the example given
by Hadamard in 1906 became famous, and it is this example which we have reproduced here.
Mollifiers were introduced by K.O. Friedrichs in 1944 to big effect in the theory of partial
different equations. In [31, Vol. 1], Peter Lax, of Lax–Milgram fame, recounts the story of how
they got their name: it is an allusion, among other things, to the eponymous protagonist of Daniel
Defoe’s 1722 novel Moll Flanders, who managed to mollify her last husband after having led a
rather irregular life.
The definition of Perron solutions which we have given is adapted to our Sobolev space approach.
Oskar Perron (1880–1975) originally gave another (equivalent) definition in 1905: let Ω ⊂ Rd be
bounded and open and suppose g ∈ C(∂Ω) is given. Then a function v ∈ C(Ω) is called a
subsolution of the Dirichlet problem if
(a) lim supΩ#x→z v(x) ≤ g(z), z ∈ ∂Ω and
(b) −Δv ≤ 0 weakly, that is, ∫
Ω v(x)Δϕ(x) dx ≤ 0 for all 0 ≤ ϕ ∈ D(Ω).
A supersolution is a function w ∈ C(Ω) such that
(a) lim supΩ#x→z w(x) ≥ g(z), z ∈ ∂Ω and
(b) −Δw ≥ 0 weakly.
Then
u(x) := inf {w(x) : w is a supersolution} and u(x) := sup{v(x) : v is a subsolution}
exist for all x ∈ Ω. Furthermore, u = u, u ∈ C∞(Ω) is bounded, and Δu = 0. Finally, for all x ∈ Ω
we have
min
z∈∂Ω g(z) ≤ u(x) ≤ max
z∈∂Ω g(z).
If (6.63) has a classical solution u, then u = u. For all these statements we refer to [25]. In the
literature, the function u is called the Perron solution of the Dirichlet problem. One can show that
this notion of solution coincides with the one given in Definition 6.70 (see [10]). In particular,
u = ug if u is a classical solution of (6.63).
Oskar Perron was a remarkable mathematician. He served as a professor in the German cities
of Tübingen, Heidelberg and Munich, and contributed to many different areas of mathematics. Not
only is his solution of the Dirichlet problem (as just described) famous, so too is his work on positive
matrices (the Perron–Frobenius theory). Perron was in addition one of the German scientists who
were brave enough to take clear anti-Nazi positions during the Nazi dictatorship.6.13 Exercises 237
The most important operator in this book is the Laplacian. It bears the name of Pierre-Simon
Laplace (1749–1827), who studied the orbits of the planets; and in his famous work Traité de
Mécanique Céleste, which he published in 1799, the Laplace equation plays a decisive role. An
important question which Laplace addressed is the stability of the planetary system, which he solved
using Newtonian mechanics (he thought). Isaac Newton (1643–1727) himself did not believe in its
stability; rather, he thought that from time to time divine intervention was necessary in order to
“regulate” things.
We quote [29] on the subject: “When citizen Laplace showed General Napoleon the first edition
of his Exposition du Système du monde, the general said to him, ‘Newton spoke of God in his book.
I have looked through yours and did not find this word once.’ To which Laplace answered, ‘Citizen
and First Consul, I never needed this hypothesis.’” Laplace had already met Napoleon much earlier:
in 1784 he examined the 16-year-old Napoleon for the entrance exam of the latter to the École
Militaire in Paris. How different might the history of Europe and even the world have been had
Laplace not passed him? Napoleon, for his part, showed admiration for mathematics and in 1799
even appointed Laplace minister of the interior, although unlike Fourier he enjoyed little success in
such a political role: Napoleon dismissed him after only six weeks, later uttering the famous remark
that Laplace had brought the spirit of the “infinitesimally small” into the administration. Laplace
would eventually be named a marquis, in 1817.
Laplace is known for his formula for computing the determinant of a matrix in terms of its
minors, which he discovered as a young man, as well as for his seminal work on elementary
probability theory. His magnum opus on celestial mechanics was highly successful – but does not
make easy reading. His colleagues, and even Laplace himself, often had difficulties following its
correct but often sketchy arguments. He had the habit of replacing arguments by the phrase “Il est
aisé à voir” – it is easy to see. This stylistic device has become commonplace in mathematics, often
to the detriment of the reader.
6.13 Exercises
Exercise 6.1 (Support of a measurable function) Let Ω ⊂ Rd be open and f :
Ω → R measurable. Set Of := {x ∈ Ω : ∃ε > 0 such that f (x) = 0 almost
everywhere in B(x, ε)}. The set supp f := Ω \ Of is called the support of f .
(a) Show that Of is the largest open set in Ω on which f vanishes almost everywhere.
(b) Let f, g ∈ L2(Rd). Show that then supp f ∗ g ⊂ supp f + supp g.
Exercise 6.2 Let Ω ⊂ Rd be open and j ∈ {1,..., d}. Show that
−
∫
Ω
u Djv dx =
∫
Ω
Dju v dx
for all u ∈ H1(Ω) and v ∈ H1
0 (Ω).
Exercise 6.3 Let u ∈ H2(Ω), where Ω ⊂ Rd is open. Show that DiDju = DjDiu for
all i, j = 1,..., d.238 6 Hilbert space methods for elliptic equations
Exercise 6.4 Let Ω = D := {x ∈ Rd : |x| < 1}, d ≥ 3, and let u ∈ C1(Ω \ {0})
be such that ∫
0<|x |<1 |u(x)|2 dx < ∞ und ∫
0<|x |<1 |∇u(x)|2 dx < ∞. Show that
u ∈ H1(Ω) and (Dju)(x) = ∂u
∂x j (x) for all x  0.
Suggestion: Let ϕ ∈ D(Ω). Show that there exists ϕn ∈ D(Ω \ {0}) such that
ϕn → ϕ in H1(Ω). To this end, choose ϕn(x) := f (n|x|)ϕ(x) as in Remark 6.31.
Exercise 6.5 (a) Let Ω = D = {x ∈ Rd : |x| < 1} and u(x) = |x|
2α. Determine for
which α ∈ R the function u is in H1(Ω).
(b) Let d ≥ 3, and let Ω ⊂ Rd be open and nonempty. Show that H1(Ω) contains a
function which is not continuous.
Exercise 6.6 (a) Let Ω= D :={x ∈ R2 : |x| < 1} and suppose that η ∈ D(Ω)
satisfies η(x) = 1 for |x| ≤ 1
2 . Let u(x) = 
log 1
|x |
	1/4
·η(x). Show that u ∈ H1(Ω).
(b) Let Ω ⊂ R2 be open and nonempty. Show that H1(Ω) contains a function which
is not continuous.
Exercise 6.7 Show that D(Rd) is dense in H2(Rd).
Suggestion: Adapt the proof of Theorem 6.25.
Exercise 6.8 Let Ω = (0, 1)∪(1, 2). Show that C(Ω) ∩ H1(Ω) is not dense in H1(Ω).
Suggestion: Consider the function 1(0,1) ∈ H1(Ω).
Exercise 6.9 (Variant of the product rule) Let Ω ⊂ Rd be open. Show that if u and
v are in H1(Ω)∩L∞(Ω), then the product uv is in H1(Ω) and Dj(uv) = Dju v+u Djv.
Exercise 6.10 (Variant of the chain rule) Let f ∈ C1(R) such that supx∈R | f 
(x)| <
∞ and let Ω ⊂ Rd be bounded. Show that ϕ(u) := f ◦ u defines a mapping ϕ :
H1(Ω) → H1(Ω), and that Djϕ(u) = ( f  ◦ u) Dju.
Exercise 6.11 Let Ω ⊂ Rd be open and u and v in H1(Ω).
(a) Show that w := u ∧ v, defined by w(x) := min{u(x), v(x)}, is also in H1(Ω), and
determine its weak derivative.
(b) Show that (−u) ∧ (−v) = −(u ∨ v), and conclude then that (u ∨ v)(x) =
max{u(x), v(x)} also defines a function u ∨ v ∈ H1(Ω).
Exercise 6.12 Let Ω ⊂ Rd be open (but not necessarily bounded). Show:
(a) If u ∈ H1(Ω), then also u ∧ 1Ω ∈ H1(Ω). Determine the weak derivative of
u ∧ 1Ω.
(b) L∞(Ω) ∩ H1(Ω) is dense in H1(Ω).
Exercise 6.13 Consider the setup of Theorem 6.43 with λ > 0, and suppose that
f (x) ≤ 1 almost everywhere. Show that then also λ u(x) ≤ 1 almost everywhere.
Exercise 6.14 (Riemann–Lebesgue lemma) Let d ∈ N and as usual let C0(Rd) := #
u ∈ C(Rd) : lim|x |→∞ u(x) = 0
$
. Show:
(a) If u ∈ L1(Rd), then Fu ∈ C(Rd).
(b) The mapping u → Fu is linear and continuous from L1(Rd) into L∞(Rd).6.13 Exercises 239
(c) Let u ∈ D(Rd). Then there exists c ∈ R such that |Fu(x)| ≤ c
1+|x |
2 for all
x ∈ Rd. In particular, Fu ∈ C0(Rd).
Suggestion: Make use of the rules for calculating Fourier transforms.
(d) For every u ∈ L1(Rd) we have Fu ∈ C0(Rd).
Suggestion: Recall that C0(Rd) is closed in L∞(Ω). One may also use without
proof that D(Rd) is dense in L1(Rd); this theorem can be shown similarly to the
case L2(Rd), cf. Corollary 6.9.
Exercise 6.15 Let Ω ⊂ Rd be open, and let u ∈ H1
loc(Ω) as well as f ∈ L2,loc(Ω)
be such that Δu = f weakly. Show that for η ∈ D(Ω) the following identity holds
Δ(η0u) = [(Δη)u + 2∇η ∇u + f η]
∼.
Suggestion: Use (6.59).
Exercise 6.16 Prove the identity (6.67).
Exercise 6.17 Show directly, without using H1-barriers, that every bounded open
subset Ω of R is Dirichlet regular.
Suggestion: Use that Ω is a countable union of open intervals.
Exercise 6.18 Let Ω ⊂ Rd be bounded, open and convex. Given g ∈ C(∂Ω), let
u ∈ C(Ω) ∩ C2(Ω) be a classical solution of the Dirichlet problem (6.63). Suppose
that there exists G ∈ H2(Ω) ∩ C(Ω) such that G|∂Ω = g. Show that then u ∈ H2(Ω).
Exercise 6.19 Prove Corollary 6.23.
Exercise 6.20 (Invariance of the Poisson equation under isometries) Let B be an
orthogonal d × d matrix, b ∈ Rd, and F(y) = By + b for all y ∈ Rd. Let Ω2 ⊂ Rd be
an open set and Ω1 := F(Ω2). Suppose that f ∈ L2(Ω1) and u ∈ H1
0 (Ω1) ∩ H2
loc(Ω1)
satisfy −Δu = f weakly. Show that u◦F ∈ H1
0 (Ω2)∩H2
loc(Ω2) and −Δ(u◦F) = f ◦F.
Suggestion: Use that the function u ∈ H1
0 (Ω) is the unique solution of the problem
∫
Ω ∇u ∇v dx = ∫
Ω fv dx, v ∈ H1
0 (Ω).
Exercise 6.21 (More general linear elliptic differential operators) Let Ω ⊂ Rd
be bounded and open, and assume that ai j, bj, ci and e are in L∞(Ω). For the formal
differential operator Lu := − 
d
j=1 Dj


d
i=1 ai jDiu + bju

+ 
d
i=1 ciDiu + eu and a
given function f ∈ L2(Ω), one understands a weak solution of the problem
(D) Lu = f in Ω, u = 0 on ∂Ω,240 6 Hilbert space methods for elliptic equations
to be a function u ∈ H1
0 (Ω) such that
aL(u, v) :=

d
i,j=1
∫
Ω
ai jDiuDjv +

d
j=1
∫
Ω
bjuDjv +

d
i=1
∫
Ω
ciDiuv +
∫
Ω
euv
=
∫
Ω
fv
for all test functions v ∈ D(Ω). Finally, suppose that there exists α > 0 such that

d
i,j=1 ai jξiξj ≥ α|ξ|
2 for all ξ ∈ Rd. Show:
(a) The form aL is bilinear and continuous on H1
0 (Ω) × H1
0 (Ω)..
(b) If there exists δ < 2α such that 
d
j=1(bj + cj)
2 ≤ 2δe almost everywhere, then
(D) has a unique weak solution.
Suggestion: For x, y ∈ R and ε > 0 we have xy ≤ ε
2 x2 + 1
2ε y2 (Young’s
inequality, Lemma 5.22).
(c) If bi and ci are continuously differentiable and if 
d
i=1(Dibi + Dici) ≤ 2e holds
almost everywhere, then (D) has a unique weak solution.
(d) If all functions are sufficiently regular and u is a weak solution of the problem,
then the expression Lu given above is well defined. Moreover, in this case a
regular function u which is also in H1
0 (Ω) is a weak solution if and only if
Lu = f holds in the classical sense. Try to find regularity assumptions which
are as weak as possible.
Exercise 6.22 (Composition of H1-functions) Consider an open set Ω ⊂ R2 and a
straight line G ⊂ R2. Let u ∈ C(Ω) be differentiable in Ω \ G, with bounded partial
derivatives. Show that then u ∈ H1(Ω).
Suggestion: After rotating and translating, one may assume that G = R × {0}.
Exercise 6.23 (Sobolev imbedding)
(a) Let Ω ⊂ Rd be open and assume that 1 < p1, p2,r < ∞ are such that 1
p1 + 1
p2 = 1
r .
Further let f1 ∈ Lp1 (Ω1) and f2 ∈ Lp2 (Ω2). Show that f1 · f2 ∈ Lr (Ω).
Suggestion: Use Hölder’s inequality: if 1 < p < ∞, 1
p
+ 1
p = 1, g ∈ Lp(Ω), and
h ∈ Lp(Ω), then g · h ∈ L1(Ω).
(b) Let d = 2 and g(x) = (1 + |x|)−1. Show that g ∈ Lq(R2) for all 2 < q < ∞.
(c) Show that Hˆ1(R2) ⊂ Lp(R2) for all 1 < p ≤ 2, where Hˆ1(R2) = F H1(R2) (see
Theorem 6.46).
(d) Show that H1(R2) ⊂ Lq(R2) for all q ∈ [2, ∞).
Suggestion: Use that F−1(L2(Rd) ∩ Lp(Rd)) ⊂ Lp(Rd) for 1 < p < 2 (Haus￾dorff–Young theorem [54, Thm. 12.12]).
Exercise 6.24 Check the calculations in the proof of Theorem 6.59.Chapter 7
Neumann and Robin boundary conditions
In this chapter we first study elliptic partial differential equations with Neumann
boundary conditions ∂u
∂ν = 0. These require somewhat different techniques from
the ones we saw when treating Dirichlet boundary conditions. We start by defining
C1-domains and proving Gauss’s theorem. This is followed by analytic properties of
the Sobolev space H1(Ω) such as extension properties and the trace theorem, which
we will draw on when studying boundary value problems with Neumann and Robin
conditions.
Chapter overview
7.1 Gauss’s theorem .................................. 242
7.2 Proof of Gauss’s theorem .............................. 247
7.3 The extension property ............................... 254
7.4 The Poisson equation with Neumann boundary conditions ............ 258
7.5 The trace theorem and Robin boundary conditions ................ 262
7.6* Comments on Chapter 7 .............................. 265
7.7 Exercises ...................................... 266
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4_7
241242 7 Neumann and Robin boundary conditions
7.1 Gauss’s theorem
The aim of this section is to generalize the Fundamental Theorem of Calculus to
functions of more than one variable. We will also take the opportunity to define
regularity properties of the boundary of a domain. Such properties play a decisive
role when it comes to the regularity of (weak) solutions of the Poisson problem,
among other partial differential equations.
Given x, y ∈ Rd, as before we denote their inner product by x · y = xT y and by
|x| = √x · x the Euclidean norm in Rd. Throughout this section, Ω ⊂ Rd will be a
bounded open set with boundary ∂Ω.
Definition 7.1 Let U ⊂ Rd be open.
(a) We say that U ∩ ∂Ω is a normal C1-graph (for Ω) if there exist a function
g ∈ C1(Rd−1) and constants r > 0 and h > 0 such that
U = {(y, g(y) + s) : y ∈ Rd−1
, |y| < r, s ∈ R, |s| < h},
and such that for all x = (y, g(y) + s) ∈ U we have (cf. Figure 7.1)
x ∈ Ω if and only if s > 0,
x ∈ ∂Ω if and only if s = 0,
x  Ω if and only if s < 0.
(b) The set U ∩ ∂Ω is called a C1-graph (for Ω) if there exist an orthogonal d × d
matrix B and a vector b ∈ Rd such that φ(U) ∩ ∂(φ(Ω)) is a normal C1-graph
for φ(Ω), where φ(x) := B(x) + b, x ∈ Rd. Thus U ∩ ∂Ω is a C1-graph for
Ω if U ∩ ∂Ω is a normal C1-graph for Ω with respect to a different Cartesian
coordinate system.
(c) We say that Ω has a boundary of class C1, or just C1-boundary for short, if for
every z ∈ ∂Ω there exists an open neighborhood U of z such that U ∩ ∂Ω is a
C1-graph (for Ω). 
Remark 7.2 (Ck -boundary and Lipschitz boundary)
(a) Given k ∈ N, we define aCk -graph by requiring in part (a) of the above definition
that g ∈ Ck (Rd−1).
(b) We obtain a Lipschitz graph if we require that g : Rd−1 → R be Lipschitz
continuous (that is, that there exist L ≥ 0 such that |g(x) − g(y)| ≤ L|x − y| for
all x, y ∈ Rd−1).
(c) As before, we speak of a continuous graph if g is continuous.
(d) We say that Ω has Ck -boundary (or Lipschitz boundary, continuous boundary,
respectively) if for every z ∈ ∂Ω there exists an open neighborhood U ⊂ Rd of z
such that U ∩∂Ω is a Ck -graph (or a Lipschitz or continuous graph, respectively)
for Ω.
(e) A Ck -domain is a bounded, connected open set with Ck -boundary. We define
continuous domains and Lipschitz domains analogously.7.1 Gauss’s theorem 243
Fig. 7.1 A normal C1-graph in R2.
(f) We say that Ω has C∞-boundary if the boundary is Ck for all k ∈ N.
(g) Finally, we call Ω a C∞-domain if it is a Ck -domain for all k ∈ N. 
We have thus defined a hierarchy of classes of regularity. Note that since every
function g ∈ C1(Rd−1) is Lipschitz continuous on every compact set in Rd−1, a C1-
boundary is automatically also Lipschitz. Every hyperrectangle in Rd has Lipschitz
but not C1 boundary, as does every polygon in R2.
In what follows we suppose Ω ⊂ Rd to be a bounded open set with C1-boundary.
We wish to formulate Gauss’s theorem for such sets. To this end, we first need outer
normal vectors. These can be defined easily via local graphs which parametrize
the boundary (see (7.2)). However, in order to circumvent the need to prove the
independence of the definition from the choice of parametrization, we prefer to give
an intrinsic definition using tangent spaces.
Theorem 7.3 (Tangent space) Let z ∈ ∂Ω. A vector v ∈ Rd is called a tangent
vector to ∂Ω at z if there exist ε > 0 and ψ ∈ C1((−ε, ε),Rd) such that ψ(t) ∈ ∂Ω
for all t ∈ (−ε, ε), ψ(0) = z and ψ
(0) = v. The set
Tz := {v ∈ Rd : v is a tangent vector to ∂Ω at z}
is a (d − 1)-dimensional subspace of Rd. It is called the tangent space to ∂Ω at z.
Proof Let U ∩ ∂Ω be a C1-graph for Ω, where U is an open neighborhood of
z. We may assume that the graph is normal and use the notation from Defini￾tion 7.1(a). We set z := (z1,..., zd−1), that is, z = (z
, zd). Then |z
| < r and
zd = g(z
). Let ej be the jth canonical basis vector in Rd−1 and uj := (ej, ∂g
∂xj
(z
)),
j = 1,..., d − 1. Then the vectors u1,..., ud−1 are linearly independent. We will
show that Tz = span{u1,..., ud−1}. To this end, let u ∈ span{u1,..., ud−1}; then
there exist λ1,...,λd−1 ∈ R such that244 7 Neumann and Robin boundary conditions
u =

d−1
j=1
λj uj =

λ1,...,λd−1,

d−1
j=1
λj
∂g
∂xj
(z
)

∈ Rd.
Now set, for t ∈ R,
ψ(t) :=

z + t

d−1
j=1
λj ej, g

z + t

d−1
j=1
λj ej

.
Then, by Definition 7.1(a) ψ(t) ∈ ∂Ω for all |t| < ε, if ε > 0 is chosen sufficiently
small (since s = 0); and we have ψ(0) = z, as well as
ψ
(0) =

λ1,...,λd−1,

d−1
j=1
λj
∂g
∂xj
(z
)

= u.
This shows that u ∈ Tz .
Conversely, let u ∈ Tz . Then there exists a ψ ∈ C1((−ε, ε), Rd) such that ψ(t) ∈ ∂Ω
for all |t| < ε; and ψ(0) = z, ψ
(0) = u. Then ψd(t) = g(ψ1(t),...,ψd−1(t)) for
|t| < ε, where g ∈ C1(Rd−1) is the function from Definition 7.1(a). By the chain rule,
ψ
d(0) =

d−1
j=1
ψ
j(0)
∂g
∂xj
(z
). (7.1)
Now set λj := ψ
j
(0) for all j = 1,..., d − 1; then
u = ψ
(0) =

λ1,...,λd−1,

d−1
j=1
λj
∂g
∂xj
(z
)

∈ span{u1,..., ud−1}.
We have thus shown that Tz = span{u1,..., ud−1}. 
We can now define the outer normal vector intrinsically, that is, without using
parametrizations of the boundary. Since Tz is a (d − 1)-dimensional subspace of Rd,
the orthogonal space T⊥
z := {w ∈ Rd : w · v = 0 ∀v ∈ Tz } is one-dimensional.
Theorem 7.4 (Outer unit normal vector) Let Ω ⊂ Rd be a bounded open set with
C1-boundary and let z ∈ ∂Ω. Then there exists a unique vector ν(z) ∈ Rd such that
(a) |ν(z)| = 1;
(b) ν(z) ∈ T⊥
z ;
(c) ∃ε > 0 such that z + tν(z)  Ω if 0 < t < ε and z + tν(z) ∈ Ω if −ε < t < 0.
The mapping ν : ∂Ω → Rd, z → ν(z), is continuous.
The vector ν(z) is called the outer unit normal vector to Ω at z. We often abbreviate
this to outer unit normal or even just outer normal.7.1 Gauss’s theorem 245
Proof We keep the notation from the previous proof. Let
w := (∇g(z
), −1) =
 ∂g
∂x1
(z
),..., ∂g
∂xd−1
(z
), −1

.
1. We will show that Tz = w⊥. To this end, let b ∈ Tz ; then there exists a function
ψ ∈ C1((−ε, ε),Rd) such that ψ(t) ∈ ∂Ω for all |t| < ε, b = ψ
(0) and z = ψ(0).
Hence, by (7.1), ψ
d(0) = 
d−1
j=1 ψ
j
(0) ∂g
∂xj
(z
), that is, b · w = 0. We have thus shown
that Tz ⊂ w⊥ holds. But since dimTz = d − 1 = dim w⊥, the equality Tz = w⊥
follows.
2. Now we will show that z+tw  Ω if 0 ≤ t < ε and z+tw ∈ Ω if −ε < t < 0, for
suitable ε > 0. We set w = (w1,...,wd−1) and apply Taylor’s theorem to g to obtain
g(z
+tw
) = g(z
)+∇g(z
)·tw
+o(t) = zd+t|∇g(z
)|2+o(t), where o(t)represents an
error term with the property that limt→0 o(t)
t = 0. Thus(z+tw)d = zd−t < g(z
+tw
)
four 0 < t < ε and zd − t > g(z + tw
) for −ε < t < 0, if ε > 0 is chosen small
enough. This establishes the claim.
3. If we choose ν(z) := w
|w|
, then ν(z) satisfies the conditions of the theorem.
Since dimT⊥
z = 1, ν(z) is unique (since its sign is determined by (c)).
This concludes the proof. 
We observe that
ν(z) = (∇g(z
), −1)

|∇g(z
)|2 + 1
(7.2)
for z ∈ ∂Ω ∩ U if U is a normal C1-graph for Ω (with the notation from Defini￾tion 7.1(a), and where z = (z
, zd), z = (z1,..., zd−1)).
We can now formulate Gauss’s theorem, also sometimes known the Gauss–Ostro￾gradsky theorem, among other names. (Also note that we will derive corollaries of
this theorem which are also often called Gauss’s theorem in the literature.) We
will denote by C1(Ω) the set of all functions u ∈ C1(Ω) ∩ C(Ω), such that for
every j ∈ {1,..., d} the function Dju admits a continuous extension to Ω, where
Dju = ∂u
∂xj denotes the partial derivative of u in the jth coordinate direction. We
will also use the notation Dju for this continuous extension to Ω.
Theorem 7.5 (Gauss’s theorem) There exists a unique Borel measure σ on ∂Ω
such that
∫
Ω
(Dju)(x) dx =
∫
∂Ω
u(z)νj(z) dσ(z) for all j = 1,..., d (7.3)
and for all u ∈ C1(Ω). We call σ the surface measure on ∂Ω. Here ν ∈ C(∂Ω) is the
outer unit normal vector with components ν(z) = (ν1(z),...,νd(z)), z ∈ ∂Ω.
Gauss’s theorem may be regarded as the Fundamental Theorem of Calculus for
functions of two or more variables. To make the analogy clear we consider the246 7 Neumann and Robin boundary conditions
special case of a bounded interval Ω = (a, b); then ∂Ω = {a, b}. If we define
ν(a) = −1, ν(b) = 1 and σ(a) = σ(b) = 1, then by the one-dimensional fundamental
theorem applied to the function u ∈ C1[a, b], we have ∫ b
a u
(x) dx = u(b) − u(a) = ∫
{a,b} ν(z)u(z) dσ(z). We will prove Gauss’s theorem in the next section. In practice,
for the applications, it is sufficient to know the statement of the above Theorem 7.5
and its consequences; the actual construction of the measure is usually irrelevant.
We now turn to various corollaries of Gauss’s theorem. Often the theorem is
formulated for vector fields, that is, for functions u ∈ C1(Ω, Rd), more precisely
u = (u1,..., ud) with uj ∈ C1(Ω), j = 1,..., d. Then we can define the divergence
div u of u by
div u(x) :=

d
j=1
Djuj(x), x ∈ Ω,
so that div u ∈ C(Ω). We obtain the following equivalent formulation of Gauss’s
theorem, which is often known as the divergence theorem (as well as the Gauss–
Ostrogradsky theorem, etc.). It is popular among physicists because it has a number
of direct physical interpretations (see Section 1.6.1).
Corollary 7.6 (Divergence theorem) Let u ∈ C1(Ω, Rd), then
∫
Ω
div u(x) dx =
∫
∂Ω
u(z) · ν(z) dσ(z).
Proof By Theorem 7.5,
∫
Ω
div u(x) dx =

d
j=1
∫
Ω
(Djuj)(x) dx =

d
j=1
∫
∂Ω
uj(z)νj(z) dσ(z)
=
∫
∂Ω
u(z) · ν(z) dσ(z),
where u(z) · ν(z) denotes the inner product in Rd. 
In what follows we will sometimes omit the variables of integration if they are
clear from the context.
Corollary 7.7 (Integration by parts) If u, v ∈ C1(Ω), then
∫
Ω
Dju · v dx = −
∫
Ω
u Djv dx +
∫
∂Ω
u v νj dσ, j = 1,..., d.7.2 Proof of Gauss’s theorem 247
Proof By Theorem 7.5 and the product rule,
∫
∂Ω
u(x)v(x)νj dσ =
∫
Ω
Dj(uv)(x) dx
=
∫
Ω
(Dju)(x)v(x) dx +
∫
Ω
u(x)Djv(x) dx,
which proves the claim. 
We define C2(Ω) := {u ∈ C1(Ω) : Dju ∈ C1(Ω), j = 1,..., d}. Thus, if u ∈
C2(Ω) then its derivatives DiDju are in C(Ω) for all i, j = 1,..., d. For u ∈ C1(Ω)
the function ∂u
∂ν : ∂Ω → R given by
∂u
∂ν (z) := ∇u(z) · ν(z) =

d
j=1
Dju(z)νj(z) (7.4)
is called the (outer) normal derivative of u at z; for u ∈ C1(Ω) we always have
∂u
∂ν ∈ C(∂Ω).
Corollary 7.8 (Green’s identities) If u ∈ C2(Ω), then
(a) ∫
Ω
(Δu)(x) dx =
∫
∂Ω
∂u
∂ν (z) dσ(z),
(b) ∫
Ω
(Δu)(x)v(x) dx +
∫
Ω
∇u(x)∇v(x) dx =
∫
∂Ω
∂u
∂ν (z)v(z) dσ(z), v ∈ C1(Ω),
(c) ∫
Ω
(vΔu − uΔv) dx =
∫
∂Ω

∂u
∂ν v − u
∂v
∂ν 
dσ(z), v ∈ C2(Ω).
Part (b) is generally known as Green’s first identity, (c) is Green’s second identity.
Proof We start with (b). By Theorem 7.5 we have
∫
Ω
Dju Djv dx = −
∫
Ω
(D2
j u)v dx +
∫
Ω
Dj(Dju v) dx
= −
∫
Ω
(D2
j u)v dx +
∫
∂Ω
Dju(z)v(z)νj(z) dσ(z).
Summing over j = 1,..., d yields (b). For v ≡ 1, (b) reduces to (a). If in (b) one
swaps the functions u and v and subtracts the resulting identity from (b), then one
obtains (c). 
7.2 Proof of Gauss’s theorem
In order to prove Gauss’s theorem, it will be useful first to characterize measures as
positive linear forms.248 7 Neumann and Robin boundary conditions
Definition 7.9 Let K ⊂ Rd be a compact set andC(K) the vector space of continuous
real-valued functions on K. A positive linear form on C(K) is a linear mapping
ϕ : C(K) → R such that ϕ( f ) ≥ 0 for all f ≥ 0. Here we write f ≥ 0 if f ∈ C(K)
satisfies f (x) ≥ 0 for all x ∈ K. 
Every positive linear form is continuous with respect to the supremum norm on
C(K) (see Exercise 7.1). If μ is a Borel measure on K, then
ϕ( f ) :=
∫
K
f (x) dμ(x), f ∈ C(K) (7.5)
defines a positive linear form ϕ on C(K). The following theorem due to Riesz
establishes the converse: every positive linear form admits such a representation.
Theorem 7.10 (Riesz representation theorem) Let K ⊂ Rd be compact and let
ϕ : C(K) → R be a positive linear form on C(K). Then there exists a unique Borel
measure μ on K such that (7.5) holds.
Proof For the proof we refer to [54, Theorems 2.14 and 2.18]. 
We first show that the surface measure on Ω is uniquely determined by the
conditions in Gauss’s theorem. Throughout this section, Ω will be a bounded open
set with C1-boundary and ν = (ν1,...,νd)
T ∈ C(∂Ω, Rd) its outer unit normal.
Lemma 7.11 (Uniqueness of surface measures) Let ϕ : C(∂Ω) → R be a contin￾uous linear form which satisfies
ϕ(νju|∂Ω) = 0 for all u ∈ C1(Ω), j = 1,..., d. (7.6)
Then ϕ = 0.
Proof The set A := {u|∂Ω : u ∈ C1(Rd)} is a subalgebra of C(∂Ω) which contains
all constant functions and which separates points in ∂Ω. Hence, by the Stone–
Weierstrass theorem, Theorem A.5, A is dense in C(∂Ω). It follows that ϕ(gνj) = 0
for all g ∈ C(∂Ω), j = 1,..., d. By replacing g by gνj ∈ C(∂Ω), we see that
ϕ(gν2
j ) = 0 for all g ∈ C(∂Ω). Since 
d
j=1 νj(z)
2 = 1 for all z ∈ ∂Ω, it follows that
ϕ(g) = 0 for all g ∈ C(∂Ω). 
The uniqueness of the surface measure as described in Theorem 7.5, now comes
about as follows: let σ1, σ2 be two Borel measures on ∂Ω which both satisfy (7.3).
Then ϕ( f ) := ∫
∂Ω f (z) dσ1(z) − ∫
∂Ω f (z) dσ2(z) defines a continuous linear form on
C(∂Ω) for which (7.6) holds; thus ϕ = 0 by Lemma 7.11. Uniqueness in the Riesz
representation theorem now implies that σ1 = σ2.
For the proof of existence we use what is known as a partition of unity. This is a
construction which in many cases permits one to derive global properties from local
ones. Here, it will allow us to construct a positive linear form on C(∂Ω) starting with
ones which are only defined for functions with support in a small set.7.2 Proof of Gauss’s theorem 249
Theorem 7.12 (Partition of unity) Let K ⊂ Rd be a compact set and let Um ⊂ Rd,
m = 0, 1,..., M be open sets such that K ⊂ /M
m=0 Um. Then there exist ηm ∈ D(Rd)
such that
(a) 0 ≤ ηm(x) ≤ 1, x ∈ Rd, m = 0,..., M;
(b) supp ηm(x) ⊂ Um, m = 0,..., M;
(c) 
M
m=0 ηm(x) = 1 for all x ∈ K.
We call the collection of functions η0,...,ηM a partition of unity of K subordinate
to the open cover U0,..., UM .
Proof For each x ∈ K there exists at least one m such that x ∈ Um. Hence we
can find an r > 0 such that B¯(x,r) ⊂ Um. Since K is compact, it is covered a
finite collection of these balls, say B1,..., Bk . Let Km := /
B¯  ⊂Um B¯
 . Then Km
is compact and Km ⊂ Um, m = 0,..., M; moreover, K ⊂ /M
m=0 Km. Now choose,
for each m ∈ {0, 1,..., M}, a function ψm ∈ D(Rd) such that 0 ≤ ψm ≤ 1,
supp ψm ⊂ Um and ψm(x) = 1 for all x ∈ Km (see Lemma 6.7), and set
η0 := ψ0,
η1 := (1 − ψ0)ψ1,
η2 := (1 − ψ0)(1 − ψ1)ψ2,
.
.
.
ηM := (1 − ψ0)(1 − ψ1)···(1 − ψM−1)ψM .
Then we have 
n
m=0 ηm + (1 − ψ0)(1 − ψ1)···(1 − ψn) = 1 for all n ∈ {0, 1,..., M},
as is easy to prove by induction. In particular,

M
m=0
ηm + (1 − ψ0)(1 − ψ1)···(1 − ψM ) = 1,
whence 0 ≤ 
M
m=0 ηm = 1. Since for each x ∈ K there exists an m ∈ {0, 1,..., M}
such that ψm(x) = 1, we have 
M
m=0 ηm(x) = 1 for all x ∈ K. 
We now construct our surface measure. Let U ⊂ Rd be open with U ∩ ∂Ω  ∅.
We set
Cc(U ∩ ∂Ω) := {u ∈ C(∂Ω) : ∃K ⊂ U compact, u(z) = 0 for z ∈ ∂Ω \ K};
then Cc(U ∩ ∂Ω) is a vector space. By
Cc(U ∩ ∂Ω)

+ := {ϕ : Cc(U ∩ ∂Ω) → R linear,
such that ϕ( f ) ≥ 0 for f ≥ 0}
we denote the set of positive linear forms on Cc(U ∩ ∂Ω). Now let U ∩ ∂Ω be a
normal C1-graph for Ω. Using the notation from Definition 7.1(a), we set250 7 Neumann and Robin boundary conditions
ϕ( f ) :=
∫
|y |<r
f (y, g(y))
1 + |∇g(y)|2 dy (7.7)
for all f ∈ Cc(U ∩ ∂Ω). Here we integrate over the (d − 1)-dimensional ball of
radius r. Observe that (y, g(y)) ∈ ∂Ω ∩ U for all y ∈ Rd−1 with |y| < r, and so
ϕ ∈ Cc(U ∩ ∂Ω)

+. With this notation we have:
Lemma 7.13 Suppose u ∈ C1(Ω) with supp u ⊂ U. Then
∫
Ω
Dju(x) dx = ϕ(u|∂Ω νj), j = 1,..., d. (7.8)
Observe that u|∂Ω ∈ Cc(U ∩ ∂Ω) since supp u ⊂ U.
Proof Let B
r := {y ∈ Rd−1 : |y| < r} and h > 0 be as in Definition 7.1. The
mapping φ : B
r×(0, h) → U∩Ω defined by φ(y, s) = (y, g(y)+s)is a diffeomorphism
since g ∈ C1(Rd−1), and we have
∂φi
∂yj
(y, s) = ∂
∂yj
yi = δi,j, 1 ≤ i ≤ d − 1, 1 ≤ j ≤ d − 1,
∂φd
∂yj
(y, s) = ∂
∂yj
(g(y) + s) = (Djg)(y), 1 ≤ j ≤ d − 1,
∂φi
∂s
(y, s) = 0, 1 ≤ i ≤ d − 1, ∂φd
∂s
(y, s) = 1.
Hence the Jacobian matrix of φ has the form
Dφ(y, s) =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
1 0 0
. . . .
.
.
. . . .
.
.
0 1 0
D1g(y) ··· ··· Dd−1g(y) 1
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
∈ Rd×d,
with det Dφ(y, s) = 1. Hence, if f : Rd → R is any continuous function, then
∫
B
r
∫ h
0
f (y, g(y) + s) ds dy =
∫
|y |<r
∫ h
0
f (y, g(y) + s) ds dy
=
∫
U∩Ω
f (x) dx. (7.9)
Since supp u ⊂ U, it follows that for any 1 ≤ j ≤ d − 1
∫
Ω
Dju(x) dx =
∫
Ω∩U
Dju(x) dx =
∫
|y |<r
∫ h
0
(Dju)(y, g(y) + s) ds dy.
By the chain rule,7.2 Proof of Gauss’s theorem 251
∂
∂yj
u(y, g(y) + s) = (Dju)(y, g(y) + s) + (Ddu)(y, g(y) + s)(Djg)(y),
and so
∫
Ω
Dju(x) dx =
∫
|y |<r
∫ h
0
 ∂
∂yj
u(y, g(y) + s)
− (Ddu)(y, g(y) + s)(Djg)(y)
'
ds dy.
For the first term we use the Fundamental Theorem of Calculus in one variable
(namely yj) and obtain
∫
|y |<r
∫ h
0
∂
∂yj
u(y, g(y) + s) ds dy =
∫ h
0
∫
|y |<r
∂
∂yj
u(y, g(y) + s) dy ds = 0,
since supp u ⊂ U = {(y, g(y) + s) : |y| < r, |s| < h}. For the second term we again
use the fundamental theorem in one variable (this time s) to obtain
−
∫
|y |<r
∫ h
0
(Ddu)(y, g(y) + s)(Djg)(y) ds dy =
∫
|y |<r
u(y, g(y))(Djg)(y) dy
and thus ∫
Ω Dju(x) dx = ∫
|y |<r u(y, g(y))(Djg)(y) dy. By (7.2), for all 1 ≤ j ≤ d −1
we have
νj(y, g(y)) = Djg(y)

1 + |∇g(y)|2
and hence (cf. (7.7))
∫
Ω
Dju(x) dx =
∫
|y |<r
u(y, g(y))νj(y, g(y))
1 + |∇g(y)|2 dy = ϕ(νju|∂Ω ).
When j = d we have ∂
∂s u(y, g(y) + s) = (Ddu)(y, g(y) + s). Thus, using (7.9) and
similar arguments to the first case we obtain
∫
Ω
Ddu(x) dx =
∫
|y |<r
∫ h
0
(Ddu)(y, g(y) + s) ds dy
=
∫
|y |<r
∫ h
0
∂
∂s
u(y, g(y) + s) ds dy = −
∫
|y |<r
u(y, g(y)) dy
=
∫
|y |<r
u(y, g(y))νd(y, g(y))
1 + |∇g(y)|2 dy = ϕ(u|∂Ω νd)
since by (7.2) νd(y, g(y)) = −1
√
1+|∇g(y) |2
. 252 7 Neumann and Robin boundary conditions
The statement remains true if instead of normal C1-graphs we consider just C1-
graphs. For the sake of completeness we will give the argument.
Let U ⊂ Rd be open and let U ∩ ∂Ω be a C1-graph for Ω, that is, we suppose
that there exist an orthogonal matrix B and a vector b ∈ Rd such that ∂Ω: ∩ U: is a
normal C1-graph, where φ(x) = Bx + b and Ω: := φ(Ω), U: := φ(U). We may assume
that det B = 1 (otherwise we exchange two variables). The outer unit normal to Ω: at
φ(z) is given by ν˜(φ(z)) = Bν(z), for all z ∈ ∂Ω. Let ϕ˜ be the positive linear form on
Cc(U: ∩ ∂Ω:) which we constructed in Lemma 7.13. Now define ϕ ∈ Cc(U ∩ ∂Ω)

+
by
ϕ( f ) := ϕ˜( f ◦ φ−1), f ∈ Cc(U ∩ ∂Ω). (7.10)
Then we have the following analog of Lemma 7.13.
Lemma 7.14 Suppose u ∈ C1(Ω) with supp u ⊂ U. Then
∫
Ω
Dju(x) dx = ϕ(u|∂Ω νj), j = 1,..., d. (7.11)
Proof Let B = (bk j)k,j=1,...,d, u˜ = u ◦ φ−1. Then u˜ ∈ C1(Ω:), and by Lemma 7.13,
∫
Ω
Dju(x) dx =
∫
Ω
∂
∂xj
(u˜ ◦ φ)(x) dx =
∫
Ω

d
k=1
(Dku˜)(φ(x))bk j dx
=
∫
Ω:

d
k=1
Dku˜(y)bk j dy =

d
k=1
bk jϕ˜(u˜|
∂Ω: ν˜k )
= ϕ((u ◦ φ−1)|
∂Ω: (B−1ν˜)j) = ϕ(u|Γ νj),
which proves the claim. 
We can now use partitions of unity to combine our positive linear forms in order
to obtain one on C(∂Ω) which yields the surface measure on ∂Ω via the Riesz
representation theorem.
Proof (of Theorem 7.5) For each point z ∈ ∂Ω we can find an open neighborhood
U of z such that U ∩ ∂Ω is a C1-graph. Since ∂Ω is compact, we can thus choose
open sets U1,..., UM such that ∂Ω ⊂ /M
m=1 Um and such that Um ∩∂Ω is a C1-graph
for Ω. Now choose an open set U0 ⊂ Ω such that Ω ⊂ /M
m=0 Um. Let (ηm)m=0,...,M
be a partition of unity subordinate to this covering. Then ηm ∈ D(Rd), 0 ≤ ηm ≤ 1,
suppηm ⊂ Um for m = 0,..., M and 
M
j=0 ηj(x) = 1 for all x ∈ Ω. By Lemma 7.14,
for all j = 1,..., d, m = 1,..., M and u ∈ C1(Ω) with supp u ⊂ Um there exists a
function ϕm ∈ Cc(Um ∩ ∂Ω)

+ such that
∫
Ω
Dju(x) dx = ϕm(νju|∂Ω∩Um ). (7.12)7.2 Proof of Gauss’s theorem 253
We now define a positive linear form ϕ on C(∂Ω) by
ϕ( f ) :=

M
m=1
ϕm(ηm f ), f ∈ C(∂Ω). (7.13)
By the Riesz representation theorem there exists a Borel measure σ on ∂Ω such
that ϕ( f ) = ∫
∂Ω f (z) dσ(z) for all f ∈ C(∂Ω). Let u ∈ C1(Ω); then uη0 ∈ C1
c (Ω).
By Lemma 6.12, it follows that ∫
Ω Dj(uη0)(x) dx = 0 for all j ∈ {1,..., d} (see
Exercise 6.2). Since uηm ∈ Cc(Um ∩ ∂Ω) for all m ∈ {1,..., M}, it follows by
Lemma 7.14 that ∫
Ω Dj(ηmu)(x) dx = ϕm(ηmu|∂Ω νj), j = 1,..., d. Hence
∫
Ω
Dju(x) dx =
∫
Ω
Dj


M
m=0
ηmu

(x) dx =

M
m=0
∫
Ω
Dj(ηmu)(x) dx
=

M
m=1
∫
Ω
Dj(ηmu)(x) dx =

M
m=1
ϕm(ηmu|∂Ω νj)
= ϕ(u|∂Ω νj) =
∫
∂Ω
u|∂Ω (z)νj(z) dσ,
and our Borel measure satisfies the conditions of Gauss’s theorem. We have already
proved uniqueness; in particular, the above construction is independent both of the
choice of graphs and the partition of unity. 
This completes the proof of Gauss’s theorem. The proof also gives us the following
local description of the surface measure σ on ∂Ω.
Theorem 7.15 Let U ∩ ∂Ω be a normal C1-graph for Ω. Then, keeping the notation
from Definition 7.1,
∫
∂Ω
f (z) dσ(z) =
∫
|y |<r
f (y, g(y))
1 + |∇g(y)|2 dy (7.14)
for every continuous function f : ∂Ω ∩ U → R such that supp f ⊂ U.
Corollary 7.16 Let z ∈ ∂Ω and r > 0. Then σ(B(z,r) ∩ ∂Ω) > 0.
We will always take the surface measure on ∂Ω. The space L2(∂Ω) is then the
space of Borel-measurable functions b : ∂Ω → R such that ∫
∂Ω |b(z)|2 dσ(z) < ∞.
Here we identify functions whenever they agree σ-almost everywhere. We next note
the following density properties.
Theorem 7.17 The space F := {ϕ|∂Ω : ϕ ∈ D(Rd)} is dense in L2(∂Ω).
Proof By the Stone–Weierstrass theorem (Theorem A.5), F is dense in C(∂Ω) with
respect to the supremum norm ·∞. It also follows from general measure theory
that C(∂Ω) is dense in L2(∂Ω) (see for example [54, Theorem 3.14]). 254 7 Neumann and Robin boundary conditions
7.3 The extension property
We saw earlier that a function u ∈ H1
0 (Ω), when extended by zero, becomes a
function in H1(Rd). However, doing this for functions in H1(Ω) does not generally
yield weakly differentiable functions. But there is another possibility for extending
H1(Ω)-functions, as long as the boundary satisfies certain regularity conditions. We
start by giving the desired property a name.
Definition 7.18 An open set Ω ⊂ Rd is said to have the extension property if for
every u ∈ H1(Ω) there exists a w ∈ H1(Rd) such that w|Ω = u. 
A simple criterion for such sets Ω is as follows.
Theorem 7.19 Let Ω ⊂ Rd be a bounded open set with Lipschitz boundary. Then Ω
has the extension property.
In particular Ω has the extension property if the boundary is C1. We refer to [2,
A8.12], [28, Sec 5.5] or [18] for a complete proof and instead sketch the idea of the
proof here.
Proof (Idea) We shall assume for simplicity that Ω has C1-boundary. As in Sec￾tion 7.2, by using a partition of unity we can reduce the problem to the exercise
of constructing a local extension. Now let g ∈ C1(Rd−1), h > 0, r > 0, U :=
{(y, g(y) + s) : y ∈ Rd−1
, |y| < r, |s| < h}, and U+ := {(y, g(y) + s) ∈ U, s > 0},
where Ω ∩ U = U+. Also let u ∈ H1(Ω) satisfy u(x) = 0 for all x ∈ Ω \ K, where
K ⊂ U is compact. We define u˜ : U → R by
u˜(y, g(y) + s) :=

u(y, g(y) + s), s > 0,
u(y, g(y) − s), s ≤ 0,
and extend u˜ to Rd by 0. Then it can be proved that u˜ ∈ H1(Rd), see Exercise 7.10.
For the case of Lipschitz boundary we refer to [2, A8.12, page 275]. 
If Ω has the extension property, then we can also extend in a linear and continuous
fashion.
Theorem 7.20 Let Ω be a bounded open set in Rd which has the extension property,
and let U ⊂ Rd be open, such that Ω ⊂ U. Then there exists a continuous linear
operator E : H1(Ω) → H1
0 (U) such that (Eu)|Ω = u for all u ∈ H1(Ω).
We call E an extension operator.
Proof Let T : H1(Rd) → H1(Ω) be given by Tv = v|Ω ; then T is linear, con￾tinuous and by assumption surjective, and H1(Rd) = Ker T ⊕ (Ker T)
⊥, where
the orthogonal complement is taken in the Hilbert space H1(Rd). It follows that
T|
(Ker T)⊥ : (Ker T)
⊥ → H1(Ω) is an isomorphism, whose inverse S : H1(Ω) →
(Ker T)
⊥ ⊂ H1(Rd) is an extension operator for U = Rd. Observe that S is con￾tinuous by Theorem A.4. Now let U be an arbitrary open set such that Ω ⊂ U and7.3 The extension property 255
choose a test function ψ ∈ D(Rd) such that supp ψ ⊂ U and ψ(x) = 1 for all x ∈ Ω
(see Lemma 6.7). We now define Eu := ψ Su; then Eu ∈ H1
c (U) ⊂ H1
0 (U), see
Theorem 6.29. Clearly, (Eu)|Ω = u for all u ∈ H1(Ω). 
An important consequence of the extension property is the density of test functions
on Rd in H1(Ω). This is to be compared with the weaker density statement of
Theorem 6.16, which is valid for every open set in Rd.
Theorem 7.21 Let Ω be a bounded open set in Rd with the extension property and
let U ⊂ Rd be an open set such that Ω ⊂ U. Then {ϕ|Ω : ϕ ∈ D(U)} is dense in
H1(Ω).
Proof Let u ∈ H1(Ω); then Eu ∈ H1
0 (U), where E is the extension operator from
Theorem 7.20. Hence there exist ϕn ∈ D(U) such that ϕn → Eu in H1(U). Since
the extension mapping is continuous, it follows that ϕn|Ω → u = (Eu)|Ω . 
A further consequence is the compactness of the imbedding H1(Ω) → L2(Ω).
Theorem 7.22 Let Ω ⊂ Rd be a bounded open set with the extension property. Then
the imbedding of H1(Ω) in L2(Ω) is compact, as is the imbedding H2(Ω) → H1(Ω).
Proof 1. LetU be bounded and open with Ω ⊂ U. We know by the Rellich imbedding
theorem that the imbedding H1
0 (U) → L2(U) is compact (Theorem 6.54). Let E :
H1(Ω) → H1
0 (U) be an extension operator and suppose un ∈ H1(Ω) form a bounded
sequence, say, un H1(Ω) ≤ c. Then Eun H1
0 (U) ≤ cE, and so there exists a
subsequence (unk )k ∈N such that Eunk converges in L2(U). Hence unk = (Eunk )|Ω
converges in L2(Ω).
2. To prove the second assertion we use Theorem 4.39. Let un  u in H2(Ω); then
Djun  Dju in H1(Ω) for all j ∈ {1,..., d}. Since H1(Ω) → L2(Ω) is compact, we
deduce that Djun → Dju in L2(Ω). Since un  u in H1(Ω), we have that un → u in
L2(Ω) as well. Hence un → u in H1(Ω). It now follows from Theorem 4.39 that the
imbedding H2(Ω) → H1(Ω) is compact. 
For Lipschitz domains a more general extension property holds, which also al￾lows a similar conclusion for the higher derivatives. To show this requires a different
method from the simple reflection technique we sketched in the context of Theo￾rem 7.19, which no longer works here.
Theorem 7.23 (General extension property) Let Ω be a Lipschitz domain. Then
there exists a continuous linear operator E : H1(Ω) → H1(Rd) such that
(a) (Eu)|Ω = u for all u ∈ H1(Ω);
(b) EHk (Ω) ⊂ Hk (Rd) for all k ∈ N.
It follows directly from the closed graph theorem that the restriction Ek of E
to Hk (Ω) is a continuous linear operator from Hk (Ω) to Hk (Rd). We refer to [56,
Chapter 6] for the proof of Theorem 7.23.256 7 Neumann and Robin boundary conditions
This more general extension theorem allows us to transfer the imbedding theorems
which hold for Rd to Ω.
Corollary 7.24 Let Ω ⊂ Rd be a Lipschitz domain, and suppose k ∈ N satisfies
k > d
2 . Then Hk+m(Ω) ⊂ Cm(Ω) for all m ∈ N0, where C0(Ω) = C(Ω).
Proof We know from Corollary 6.50 that Hk+m(Rd) ⊂ Cm(Rd). Thus if u ∈
Hk+m(Ω) then Eu ∈ Cm(Rd). The claim then follows since u = (Eu)|Ω . 
We wish to formulate the case d = 2 explicity, as we will need it in this form in
Chapter 9.
Corollary 7.25 Let Ω ⊂ R2 be a Lipschitz domain (for example a polygon). Then
H2(Ω) ⊂ C(Ω).
The following domain has continuous boundary but does not enjoy the extension
property.
Example 7.26 Given α > 1, let Ω = {(x, y) ∈ R2 : 0 < x < 1, |y| < xα} (see
Figure 7.2). Then Ω does not have the extension property; we refer to Exercise 7.7
for the proof. In the limit case α = 1, Ω becomes a triangle, which of course does
have the extension property. For α > 1, the Lipschitz condition is violated only in
the vicinity of the point (0, 0). 
Fig. 7.2 Ω is a domain without the extension property (the figure corresponds to α = 1.4).
However, both the compactness of the imbedding of H1(Ω) in L2(Ω) and the
density statement of Theorem 7.21 continue to hold for the set Ω of Example 7.26.
In fact, these two properties require only the mildest regularity conditions on the
boundary which we have defined; for the proof of this statement we refer to [26, V.
Theorem 4.17, page 267 and V. Theorem 4.7, page 248].7.3 The extension property 257
Theorem 7.27 Let Ω ⊂ Rd be a bounded open set with continuous boundary. Then
the imbedding of H1(Ω) in L2(Ω) is compact; moreover, the space {ϕ|Ω : ϕ ∈ D(Rd)}
is dense in H1(Ω).
A consequence of the compactness of the imbedding is the following, second,
Poincaré inequality. While the first (Theorem 6.32) holds for functions in H1
0 (Ω), in
the second we consider functions in H1(Ω) with mean value 0.
Theorem 7.28 (Second Poincaré inequality) Let Ω ⊂ Rd be a bounded domain
with continuous boundary. Then there exists a constant c > 0 such that
∫
Ω
|u(x)|2 dx ≤ c
∫
Ω
|∇u(x)|2 dx (7.15)
for all u ∈ H1(Ω) such that ∫
Ω u(x) dx = 0.
The subspace
H1
m(Ω) :=

u ∈ H1(Ω) :
∫
Ω
u(x) dx = 0
'
(7.16)
of H1(Ω) is closed and thus itself a Hilbert space. The second Poincaré inequality
states that
|u|H1(Ω) :=
 ∫
Ω
|∇u(x)|2 dx 1
2
(7.17)
defines an equivalent norm on H1
m(Ω), as long as Ω is a bounded domain with
continuous boundary. On H1(Ω) the quantity |·|H1(Ω) is merely a semi-norm since
|1Ω|H1(Ω) = 0.
Proof (of Theorem 7.28) Suppose that the assertion is false. Then we can find func￾tions un ∈ H1(Ω) such that
∫
Ω
un(x) dx = 0 and lim
n→∞ ∫
Ω
|∇un(x)|2 dx = 0
but un L2(Ω) = 1 for all n ∈ N. Since the imbedding H1(Ω) → L2(Ω) is compact
by Theorem 7.27, there exists a subsequence (unk )k ∈N such that u := limk→∞ unk
exists as a function in L2(Ω), and uL2(Ω) = 1. Now let ϕ ∈ D(Ω); then for all
j = 1,..., d
∫
Ω
u(x) Djϕ(x) dx = lim
k→∞ ∫
Ω
unk (x) Djϕ(x) dx
= lim
k→∞ −
∫
Ω
Djunk (x) ϕ(x) dx = 0,258 7 Neumann and Robin boundary conditions
since limn→∞ ∫
Ω |∇un(x)|2 dx = 0. Hence u ∈ H1(Ω) and ∇u = 0. Since Ω is
connected, it now follows from Theorem 6.19 that u is constant. But since
∫
Ω
u(x) dx = lim
k→∞ ∫
Ω
unk (x) dx = 0,
we can only have u = 0. This is a contradiction to uL2(Ω) = 1. 
We now give an example to show that the imbedding H1(Ω) → L2(Ω) is not
always compact.
Example 7.29 Let d = 1 and Ω := (0, 1)\{2−n : n ∈ N}. Then the imbedding
H1(Ω) → L2(Ω) is not compact. To see this, we consider the sequence given by
un := cn1(2−(n+1)
,2−n), where cn := 2(n+1)/2, also un L2(Ω) = 1. Then un ∈ H1(Ω) and
u
n = 0, whence un H1(Ω) = 1. But since un is orthogonal in L2(Ω) to um whenever
n  m, we always have un − um 2
L2(Ω) = 2 for all n  m. As such, (un)n∈N cannot
admit a subsequence which converges in L2(Ω). 
It is equally possible to find a connected bounded open set Ω ⊂ Rd for which the
imbedding of H1(Ω) into L2(Ω) is not compact.
7.4 The Poisson equation with Neumann boundary conditions
In this section we will consider the Poisson equation with Neumann boundary
conditions. We wish to start by introducing a “reaction term” into the equation. Let
Ω be a bounded open set with C1-boundary, λ ∈ R and f ∈ L2(Ω). Then we seek a
solution of the problem
λu − Δu = f in Ω, (7.18a)
∂u
∂ν = 0 on ∂Ω. (7.18b)
We call u a classical solution of (7.18) if u ∈ C2(Ω) and (7.18) holds pointwise.
Here the expression
∂u
∂ν (z) = ∇u(z) · ν(z) =

d
j=1
∂u
∂xj
(z)νj(z)
is the normal derivative of u at z ∈ ∂Ω. If a classical solution exists, then f is
necessarily in C(Ω).
We wish to apply the Hilbert space techniques developed in Chapter 4, more
precisely, the theorem of Riesz–Fréchet or, more generally, the Lax–Milgram theo￾rem. To this end we need to introduce weak solutions of (7.18) which we can then7.4 The Poisson equation with Neumann boundary conditions 259
interpret in terms of an appropriate bilinear form. We multiply (7.18a) by ϕ ∈ C1(Ω),
integrate and apply Green’s first identity (Corollary 7.8(b)) to obtain
∫
Ω
f (x)ϕ(x) dx = λ
∫
Ω
u(x)ϕ(x) dx −
∫
Ω
Δu(x)ϕ(x) dx
= λ
∫
Ω
u(x)ϕ(x) dx +
∫
Ω
∇u(x)∇ϕ(x) dx
−
∫
∂Ω
∂u
∂ν (z)ϕ(z) dσ(z).
If we now use the boundary condition (7.18b), then this reduces to
∫
Ω
f (x)ϕ(x) dx = λ
∫
Ω
u(x)ϕ(x) dx +
∫
Ω
∇u(x)∇ϕ(x) dx (7.19)
for all ϕ ∈ C1(Ω). Since C1(Ω) is dense in H1(Ω) by Theorem 7.21, it follows
that (7.19) holds for all ϕ ∈ H1(Ω). But the equation (7.19) is meaningful for all
ϕ ∈ H1(Ω) provided only that u ∈ H1(Ω). Moreover, the outer normal no longer
appears, and so weak solutions can even be defined for arbitrary open sets.
Definition 7.30 Let Ω ⊂ Rd be open, λ ∈ R and f ∈ L2(Ω). A weak solution of
(7.18) is a function u ∈ H1(Ω) for which (7.19) holds for all ϕ ∈ H1(Ω). 
We have seen that in the case where Ω hasC1-boundary, every classical solution is
also a weak solution. Conversely, if u is a weak solution, then u is already a classical
solution provided that u is sufficiently regular. The following theorem makes precise
these assertions.
Theorem 7.31 Let Ω be a bounded open set with C1-boundary and f ∈ L2(Ω),
λ ∈ R.
(a) If u is a classical solution of (7.18), then u is also a weak solution.
(b) If u ∈ C2(Ω) is a weak solution of (7.18), then f ∈ C(Ω) and u is a classical
solution.
Proof We only need to prove (b). Let u ∈ C2(Ω) be a weak solution of (7.18). By
applying Green’s first identity, Corollary 7.8(b), to (7.19), we obtain
∫
Ω
f (x)ϕ(x) dx = λ
∫
Ω
u(x)ϕ(x) dx −
∫
Ω
Δu(x)ϕ(x) dx
for all ϕ ∈ D(Ω).We thus have ∫
Ω( f (x)−λu(x)+Δu(x))ϕ(x) dx = 0 for all ϕ ∈ D(Ω).
It now follows from Corollary 6.10 that f − λu + Δu = 0 almost everywhere, that is,
f = λu − Δu almost everywhere. Since λu − Δu ∈ C(Ω), f has a unique continuous
representative. If we identify f with this representative, then (7.18a) is satisfied
pointwise. To show that the boundary condition (7.18b) also holds, we now consider260 7 Neumann and Robin boundary conditions
(7.19) for ϕ ∈ C1(Ω) and exploit the fact that (as we have just shown) λu − Δu = f .
Thus, starting from the fact that u is a weak solution,
0 =
∫
Ω
(λu(x) − Δu(x))ϕ(x) dx =
∫
Ω
f (x)ϕ(x) dx
=
∫
Ω
λu(x)ϕ(x) dx +
∫
Ω
∇u(x)∇ϕ(x) dx
=
∫
Ω
λu(x)ϕ(x) dx −
∫
Ω
Δu(x)ϕ(x) dx +
∫
∂Ω
∂u
∂ν (z)ϕ(z) dσ(z)
for all ϕ ∈ C1(Ω), where we have again used Green’s first identity, Corollary 7.8(b).
Hence ∫
∂Ω
∂u
∂ν (z) ϕ(z) dσ(z) = 0 for all ϕ ∈ C1(Ω). Since the set {ϕ|∂Ω : ϕ ∈ C1(Ω)}
is dense in L2(∂Ω, dσ) by Theorem 7.17, it follows that ∂u
∂ν = 0 in L2(∂Ω, dσ),
that is, ∂u
∂ν (z) = 0 almost everywhere. But since ∂u
∂ν ∈ C(∂Ω) and by Corollary 7.16
ν(U) > 0 for every nonempty relatively open set U in ∂Ω, we conclude that ∂u
∂ν (z) = 0
for all z ∈ ∂Ω. 
Theorem 7.31 allows us to split the analysis of problem (7.18) into two parts:
(a) the proof of existence and uniqueness of weak solutions, and
(b) the study of the regularity of u.
We can solve part (a) quite easily via the theorem of Riesz–Fréchet; however, a
detailed investigation of regularity properties of weak solutions would take us well
outside the scope of this book, and so we restrict ourselves to a few remarks. For the
existence and uniqueness of weak solutions we require no regularity assumptions
whatsoever on Ω; the outer normal vector to Ω appears nowhere in Definition 7.30,
and it is not even required to exist.
Theorem 7.32 Let Ω ⊂ Rd be an arbitrary open set, λ > 0 and f ∈ L2(Ω). Then
problem (7.18) admits exactly one weak solution.
Proof We first define F ∈ H1(Ω)
 by F(v) := ∫
Ω f (x)v(x) dx, and observe that
a(u, v) := λ
∫
Ω u(x)v(x) dx +
∫
Ω ∇u(x)∇v(x) dx, u, v ∈ H1(Ω), defines an equivalent
inner product on H1(Ω), that is, the norm induced by a(·, ·) is equivalent to ·H1 .
By the theorem of Riesz–Fréchet (Theorem 4.21) there exists exactly one u ∈ H1(Ω)
such that a(u, v) = F(v) for all v ∈ H1(Ω). But this means exactly that (7.19) is
satisfied. This completes the proof. 
On the question of the regularity of the weak solution u, we know from The￾orem 6.58 that u ∈ H2
loc(Ω). This means that the partial derivatives DiDj are in
L2,loc(Ω) and the equation λu −Δu = f is an identity between functions in L2,loc(Ω).
We will not prove any further regularity properties; we do however mention that the
weak solution u of (7.18) is in C∞(Ω) provided f ∈ C∞(Ω) and Ω has C∞-boundary
(see [18, Théorème IX.26] or [32, Theorem 6.30]).7.4 The Poisson equation with Neumann boundary conditions 261
We next consider the case λ = 0. If Ω ⊂ Rd is bounded and open, then every con￾stant function is a weak solution of (7.18) with λ = 0 and f = 0. As a consequence,
weak solutions cannot be unique. But there is more: if, given f ∈ L2(Ω), there exists
a weak solution u of (7.18), then, by choosing ϕ ≡ 1 in (7.19), we deduce that
∫
Ω
f (x) dx = 0, (7.20)
that is, (7.20) is a necessary condition for the existence of a weak solution of (7.18),
if λ = 0. It turns out that this condition is also sufficient, as long as the second
Poincaré inequality is available.
Theorem 7.33 Let Ω be a bounded domain with continuous boundary and suppose
f ∈ L2(Ω) satisfies ∫
Ω f (x) dx = 0. If λ = 0, then (7.18) has a unique weak solution
u ∈ H1(Ω), and this solution satisfies ∫
Ω u(x) dx = 0.
Proof The set L2,0(Ω) := #
v ∈ L2(Ω) :
∫
Ω v(x) dx = 0
$
forms a closed subspace of
L2(Ω) and is thus a Hilbert space. The space H1
m(Ω) = H1(Ω) ∩ L2,0(Ω) from (7.16)
is likewise closed in H1(Ω) and thus also a Hilbert space. We now show that H1
m(Ω)
is dense in L2,0(Ω). Let g ∈ L2,0(Ω) be such that (w, g)L2(Ω) = 0 for all w ∈ H1
m(Ω).
By Corollary 4.18, it suffices to show that g = 0. Set e1 := |Ω|
−1/21Ω, where |Ω| is
the Lebesgue measure of Ω. If v ∈ H1(Ω), then w := v − (v, e1)L2(Ω)e1 ∈ H1
m(Ω),
since
∫
Ω
w(x) dx =
∫
Ω
v(x) dx −

|Ω|
−1/2
∫
Ω
v(x) dx |Ω|
−1/2
∫
Ω
1Ω(x) dx
= 0.
Since (g, e1)L2(Ω) = 0 and (g, w)L2(Ω) = 0, it follows that (g, v)L2(Ω) = 0. Summariz￾ing, we have shown that (g, v)L2(Ω) = 0 for all v ∈ H1(Ω); since H1(Ω) is dense in
L2(Ω), it follows that g = 0. Hence H1
m(Ω) is indeed dense in L2,0(Ω).
By (7.19), a(u, v) := ∫
Ω ∇u(x)∇v(x) dx defines an equivalent inner product on
H1
m(Ω). Let F ∈ H1
m(Ω)
 be given by F(x) := ∫
Ω f (x)w(x) dx, w ∈ H1
m(Ω). By the
Lax–Milgram theorem there exists a unique u ∈ H1
m(Ω) such that
a(u, w) =
∫
Ω
f (x)w(x) dx, w ∈ H1
m(Ω). (7.21)
Equivalently, u is a weak solution of (7.18), as can be seen as follows: we have
∫
Ω ∇u(x)∇e1(x) dx = 0 = ∫
Ω f (x)e1(x) dx. Since every v ∈ H1(Ω) may be written
in the form v = v − (v, e1)L2(Ω)e1 + (v, e1)L2(Ω)e1, where v − (v, e1)L2(Ω)e1 ∈ H1
m(Ω),
it follows from (7.21) that
∫
Ω
f (x)v(x) dx =
∫
Ω
f (x)(v(x)−(v, e1)L2(Ω)e1(x))dx + (v, e1)L2(Ω)( f, e1)L2(Ω)
= a(u, v − (v, e1)L2(Ω)e1) + (v, e1)L2(Ω)a(u, e1) = a(u, v),
which means that u is a weak solution for λ = 0. 262 7 Neumann and Robin boundary conditions
7.5 The trace theorem and Robin boundary conditions
Throughout this section we will take Ω ⊂ Rd to be a bounded open set with C1-
boundary, and we will denote by σ surface measure on ∂Ω; the space L2(∂Ω) =
L2(∂Ω, σ) is constructed with this measure.
Theorem 7.34 (Trace theorem) There exists a unique continuous linear operator
T : H1(Ω) → L2(∂Ω) such that Tu = u|∂Ω for all u ∈ C(Ω) ∩ H1(Ω).
This operator is called the trace operator. Now by Theorem 7.21, the space C1(Ω)
is dense in H1(Ω). We will prove below that there exists a constant c > 0 such that
u|∂ΩL2(∂Ω) ≤ cuH1(Ω) (7.22)
for all u ∈ C1(Ω). It follows from Theorem A.2 that there exists a unique operator
T ∈ L(H1(Ω), L2(∂Ω)) such that Tu = u|∂Ω for all u ∈ C1(Ω). That this identity
remains true for all u ∈ C(Ω) ∩ H1(Ω) requires an additional argument.
Proof (of Theorem 7.34) 1. Let U ⊂ Rd be open, such that U ∩ ∂Ω is a C1-graph
for Ω. We will show that there exists a constant c > 0 for which (7.22) holds for all
u ∈ C1(Ω) such that supp u ⊂ U. In order to prove this we may assume that U is a
normal C1 graph (otherwise we change coordinate system). We will use the notation
from Definition 7.1(a). So let u ∈ C1(Ω) satisfy supp u ⊂ U and set
c1 := sup
|y | ≤r

1 + |∇g(y)|2;
then by (7.14),
u2
L2(∂Ω) =
∫
|y |<r
u(y, g(y))2

1 + |∇g(y)|2 dy ≤ c1
∫
|y |<r
u(y, g(y))2 dy
= c1
∫
|y |<r
∫ h
0
− ∂
∂s
u(y, g(y) + s)
2 ds dy
= c1
∫
|y |<r
∫ h
0
−2u(y, g(y) + s)Ddu(y, g(y) + s) ds dy
≤ c1
∫
|y |<r
∫ h
0
(u(y, g(y) + s)
2 + Ddu(y, g(y) + s)
2) ds dy
= c1
∫
Ω
(u(x)
2 + (Ddu(x))2) dx ≤ c1 u2
H1(Ω)
,
where for the last equality we use the relation (7.9) and for the inequality just before
that we use Young’s inequality 2αβ ≤ α2+ β2 (Lemma 5.22). This establishes (7.22)
for all u ∈ C1(Ω) with supp u ⊂ U.
2. Since ∂Ω is compact, we can find open sets U1,..., Um ⊂ Rd such that ∂Ω∩Uk
is a C1-graph for Ω for each k, and /m
k=1 Uk ⊃ ∂Ω. By 1., for every k there exists7.5 The trace theorem and Robin boundary conditions 263
a ck ≥ 0 such that (7.22) is satisfied for all u ∈ C1(Ω) such that supp u ⊂ Uk , with
c = ck . Consider a partition of unity of ∂Ω subordinate to the cover U1,..., Uk , that
is, let ηk ∈ D(Rd) be such that 0 ≤ ηk ≤ 1, suppηk ⊂ Uk and 
m
k=0 ηk (x) = 1 for all
x ∈ ∂Ω. Now let u ∈ C1(Ω); then uk := u · ηk ∈ C1(Ω) satisfies supp uk ⊂ Uk and

m
k=1 uk = u on ∂Ω. Hence
uL2(∂Ω) =
"
"
"
"
m
k=1
uk|∂Ω
"
"
"
"
L2(∂Ω)
≤
m
k=1
uk|∂Ω L2(∂Ω)
≤
m
k=1
ck uk H1(Ω) ≤

max
k=1,...,m ck
 m
k=1
uk H1(Ω).
Since Dj(ηku) = (Djηk )u +ηkDju, it follows that ηkuH1(Ω) ≤ cuH1(Ω) for some
constant c > 0. We have thus shown that (7.22) holds for all u ∈ C1(Ω), which in light
of our earlier comments implies the existence of a unique T ∈ L(H1(Ω), L2(∂Ω))
such that Tu = u|∂Ω for all C1(Ω).
If u ∈ C(Ω) ∩ H1(Ω), then we can find functions un ∈ C∞(Ω), n ∈ N, which
converge to u in both H1(Ω) and C(Ω) (see [28, Proof of Theorem 3 in §5.3.3]).
Thus Tu = limn→∞ Tun = limn→∞ un|∂Ω = u|∂Ω, where the limits are in L2(∂Ω).
This completes the proof. 
We recall that H1
0 (Ω) is defined as the closure in H1(Ω) of the space of test
functions (see Section 6.3). It follows from Theorem 7.34 that Tu = 0 for all
u ∈ H1
0 (Ω). But the converse is also true (see [28, Sec. 5.5]), meaning that we obtain
a new characterization of H1
0 (Ω) (for bounded open sets Ω with C1-boundary).
Theorem 7.35 We have H1
0 (Ω) = {u ∈ H1(Ω) : Tu = 0}, where T is the trace
operator of Theorem 7.34.
Using the trace operator we can now define a weak form of the normal derivative
∂u
∂ν and thus also more general boundary conditions. We continue to assume that
Ω is a bounded open set in Rd with C1-boundary. Now if u ∈ C2(Ω), then by
Corollary 7.8,
∫
Ω
Δu(x)v(x) dx +
∫
Ω
∇u(x)∇v(x) dx =
∫
∂Ω
∂u
∂ν (z)v(z) dσ(z) (7.23)
for all v ∈ C1(Ω). This motivates the following definition.
Definition 7.36 Suppose that u ∈ H1(Ω) ∩ H2
loc(Ω), with Δu ∈ L2(Ω). We say that
∂u
∂ν exists as a function in L2(∂Ω) if there exists some b ∈ L2(∂Ω) for which
∫
Ω
Δu(x)v(x) dx +
∫
Ω
∇u(x)∇v(x) dx =
∫
∂Ω
b(z) (Tv)(z) dσ(z) (7.24)
for all v ∈ H1(Ω). In this case b ∈ L2(∂Ω) is determined uniquely and we set
∂u
∂ν := b. 264 7 Neumann and Robin boundary conditions
It is of course sufficient to check (7.24) for all v ∈ C(Ω) ∩ H1(Ω) or even
just for v = ϕ|Ω for all ϕ ∈ D(Rd), since these functions are dense in H1(Ω) by
Theorem 7.21. The claim that there can be at most one b ∈ L2(∂Ω) which satisfies
(7.24) follows from Theorem 7.17 and Corollary 4.18. We can now obtain existence
and uniqueness of solutions of the Poisson problem with Robin boundary conditions.
Theorem 7.37 Let Ω be a bounded domain in Rd with C1-boundary and suppose
that b : ∂Ω → [0, ∞) is a bounded measurable function such that b(z) is different
from 0 on ∂Ω on a set of positive surface measure. Then for every f ∈ L2(Ω) there
exists a unique solution u ∈ H1(Ω) ∩ H2
loc(Ω) of
−Δu = f, (7.25a)
∂u
∂ν + bTu = 0, (7.25b)
where T : H1(Ω) → L2(∂Ω) denotes the trace operator.
Proof We define the bilinear form a : H1(Ω) × H1
∫
(Ω) → R by a(u, v) :=
Ω ∇u(x)∇v(x) dx + ∫
∂Ω b(z)Tu(z)Tv(z) dσ(z). Then a(·, ·) is clearly continuous;
we now show that it is also coercive. If this were not the case, then we could find
un ∈ H1(Ω) such that un H1(Ω) = 1 but limn→∞ a(un) = 0. By Theorem 4.35
we may assume that un converges weakly to u ∈ H1(Ω). Since the imbedding
H1(Ω) → L2(Ω) is compact (Theorem 7.22), it follows from Theorem 4.39 that
un → u in L2(Ω). Hence uL2(Ω) = 1. Now since limn→∞ a(un) = 0 and b ≥ 0, it
follows that
lim
n→∞ ∫
Ω
|∇un(x)|2 dx = 0, (7.26)
whence
∫
Ω
u(x)Djϕ(x) dx = lim
n→∞ ∫
Ω
un(x)Djϕ(x) dx
= lim
n→∞ −
∫
Ω
(Djun(x))ϕ(x) dx = 0
holds for all ϕ ∈ D(Ω). It now follows from Theorem 6.19 that there exists c ∈ R such
that u(x) = c almost everywhere in Ω. Moreover, by (7.26) we have limn→∞ un = u
in H1(Ω). Hence limn→∞ Tun = Tu = c1∂Ω in L2(∂Ω) as well. Using the definition
of the bilinear form a(·, ·), we obtain
c2
∫
∂Ω
b(z) dσ(z) = lim
n→∞ ∫
∂Ω
b(z)|Tun(z)|2 dσ(z) = lim
n→∞ a(un) = 0.
By our assumptions on b it finally follows that c = 0, a contradiction to uL2(Ω) = 1.
We have thus proved that a(·, ·) is coercive. As usual F(v) := ∫
Ω f (x)v(x) dx defines7.6* Comments on Chapter 7 265
a continuous linear form F ∈ H1(Ω)

, and so by the Lax–Milgram theorem there
exists a unique u ∈ H1(Ω) such that
a(u, v) =
∫
Ω
f (x)v(x) dx for all v ∈ H1(Ω). (7.27)
But this is equivalent to (7.25). Indeed, if (7.27) holds, then ∫
Ω f (x)ϕ(x) dx =
a(u, ϕ) = ∫
Ω ∇u(x)∇ϕ(x) dx for all ϕ ∈ D(Ω), whence −Δu = f . Hence also
u ∈ H2
loc(Ω) by Theorem 6.58. If we substitute the relation f = −Δu into (7.27), then
we see that −
∫
Ω Δu(x)v(x) dx = ∫
Ω f (x)v(x) dx = a(u, v) = ∫
Ω ∇u(x)∇v(x) dx +
∫
∂Ω b(Tu)(Tv) dσ(z) for all v ∈ H1(Ω). This means that ∂u
∂ν = −bTu by Defini￾tion 7.36.
The proof that every solution of (7.25) satisfies condition (7.27) is similar.

We call (7.25b) Robin boundary conditions or boundary conditions of the third
kind. When b = 0 we recover Neumann boundary conditions.
7.6* Comments on Chapter 7
Gauss’s theorem was first discovered by Joseph-Louis Lagrange in 1792, and then rediscovered
independently by Carl Friedrich Gauss in 1813, George Green in 1825 and Mikhail V. Ostrogradsky
in 1831. This is why this theorem can be found under so many different names in the literature.
It is also possible to define surface measure for Lipschitz domains and correspondingly generalize
Gauss’s theorem to such domains. We refer to [2].
Neumann boundary conditions are named after Carl G. Neumann (1832–1925), who was a
professor in Germany and Switzerland, at the Universities of Halle, Basel, Tübingen and Leipzig.
He worked on the Dirichlet principle and is also responsible for the analog of geometric series for
matrices. It is for this reason that series of the form 
∞
k=0 Tk are known as Neumann series; if T is
an operator with norm T  < 1, then its Neumann series converges to (I − T)
−1.
Robin boundary conditions are named after Victor G. Robin (1855–1897), who held lectures on
mathematical physics at the Sorbonne and worked in the area of thermodynamics. They had also
been studied by Isaac Newton (1643–1727).
The Sobolev space H1 plays a central role in Chapters 6 and 7. It was the Dirichlet problem that
led Beppo Levi to study spaces of this kind for the first time in 1906 (shortly after the invention of
the Lebesgue integral). For this reason, this space (with various equivalent definitions) was initially
called a space of type (BL). Weak solutions started appearing in the 1930s; they are fundamental
in the famous (and still current) work published by J. Leray in 1934, in which the existence of a
weak global solution of the Navier–Stokes equation is shown. (Proving uniqueness would imply the
existence of a classical global solution, which is an open Millennium Problem.) Weak solutions of
the wave equation were considered by S. L. Sobolev (1908–1989) in 1936. It was also Sobolev who
undertook a systematic investigation of Sobolev spaces, which reached its pinnacle in his influential
book published in 1950. The fundamental Sobolev imbedding theorem goes back to 1938. In this
context we can formulate a special case as follows: the space H1
0 (Ω) is a subspace of Lp(Ω) for
p = 2d/(d − 2), if Ω is a bounded open set in Rd (see Exercise 7.7(a)).
Sobolev came from the famous mathematical school of St. Petersburg and joined the Steklov
Institute of Mathematics in Moscow in 1934, becoming director of the Institute for the Theory of266 7 Neumann and Robin boundary conditions
Partial Differential Equations there in 1935. Sobolev received a large number of distinctions; aged
only 31 he became a member of the Soviet Academy of Sciencies and was awarded the highest
possible distinctions in the Soviet Union (the Stalin Prize and the title Hero of Socialist Labor).
The Göttingen school (to which for example K. Friedrichs belonged) contented itself in the
1930s and 1940s with spaces of classically differentiable functions, without making use of the
fact that these can be completed to make a Hilbert space (of Lebesgue integrable functions). This
Hilbert space was long known under the letter H, while the Sobolev space H1(Ω) was denoted by
W. It was only shown in 1964 thanks to an ingenious trick of Meyers and Serrin tbat these two
spaces are in fact the same (cf. Remark 6.17). The short publication of Meyers–Serrin to this effect,
[50], has an appropriately short title: H = W.
7.7 Exercises
Exercise 7.1 Let K ⊂ Rd be compact. Show that every positive linear form on C(K)
is continuous (cf. Definition 7.9).
Exercise 7.2 (a) Let Ω ⊂ Rd be bounded and open with C1-boundary, d ≥ 2, and
let z ∈ Ω. Given u ∈ C(Ω)∩C1(Ω\ {z}), suppose that ∂u
∂xj
is bounded on Ω\ {z}
for all j = 1,..., d. Show that then u ∈ H1(Ω).
Suggestion: Multiply ∂u
∂xj by a function ϕ ∈ D(Ω) and integrate by parts on
Ω \ B¯(z, ε).
(b) Let B := {(x, y) ∈ R2 : x2 + y2 < 1
4 }, u(x, y) = (x2 − y2) log | log r|, r =
(x2 + y2)
1/2. Show that u ∈ H1(B) (cf. Theorem 6.59).
Exercise 7.3 Let Ω1, Ω2 ⊂ Rd be open, set Ω := Ω1 ∪ Ω2, and let u, f ∈ L2,loc(Ω).
Suppose that Δu = f holds weakly in Ω1 and in Ω2 (see Definition 6.41). Show that
Δu = f holds weakly in Ω.
Suggestion: Use a partition of unity.
Exercise 7.4 (Poincaré inequality) For the open set Ω ⊂ Rd, suppose that the
imbedding H1
0 (Ω) → L2(Ω) is compact.
(a) Show that there exists a constant c > 0 such that
∫
Ω
|u|
2 dx ≤ c
∫
Ω
|∇u|
2 dx for all u ∈ H1
0 (Ω).
Where is the compactness of the imbedding needed? Suggestion: Use the proof
of Theorem 7.28 for orientation.
(b) Give an example of an unbounded open set Ω ⊂ Rd for which the imbedding is
compact.
Suggestion: Use the remark after Theorem 6.54.
(c) Given an example of an open set for which the Poincaré inequality does not
hold.7.7 Exercises 267
Exercise 7.5 (Gauss’s theorem for triangles) A segment in R2 is a set of the form
[x, y] := {λx + (1 − λ)y : 0 ≤ λ ≤ 1}, where x, y ∈ R2, x  y are the endpoints of
[x, y]. We define a measure σ on [x, y] by ∫
[x,y] f dσ = |y−x|
∫ 1
0 f (λx+(1−λ)y) dλ
for all f ∈ C[x, y] (cf. Theorem 7.10). If T is a triangle with corners {t1, t2, t3}, then
∂T = [t1, t2]∪[t2, t3]∪[t1, t3]. Given f ∈ C(∂T) we define ∫
∂T f dσ = ∫
[t1,t2] f dσ +
∫
[t2,t3] f dσ +
∫
[t1,t3] f dσ.
(a) Let u ∈ C1(T). Show that ∫
T uxj dx = ∫
∂T uνj dσ. Here νj is the jth component
of the outer unit normal ν to T, j = 1, 2.
Suggestion: After rotating and translating if necessary, one may assume that
t1 = (0, 0), t2 = (b, 0), t3 = (d, c) for some 0 < d < b, c > 0.
(b) Show that ∫
T (Dju)v dx = −
∫
T uDjv dx +
∫
∂T νjuv dσ holds for all u, v ∈ C1(T)
and j = 1, 2.
Exercise 7.6 Let Ω ⊂ R2 be a polygon and {Tk : k = 1,..., n} an admissible
triangulation of Ω (see Section 9.2.7). Let u ∈ C(Ω) be such that u|Tk ∈ C1(Tk ) with
bounded derivatives, for all k = 1,..., n.
(a) Let ϕ ∈ D(Ω). Show using Exercise 7.5(b) that
−
∫
Ω
u ∂ϕ
∂xj
dx =
n
k=1
∫
Tk
∂u
∂xj
ϕ dx, j = 1, 2.
(b) Deduce from (a) that u ∈ H1(Ω) and Dju = ∂u
∂xj
on Tk for j = 1, 2 and
k = 1,..., n.
Exercise 7.7 (Domain with continuous boundary but not the extension property)
(a) Show that if a bounded domain Ω ⊂ R2 has the extension property, then H1(Ω) ⊂
Lq(Ω) for all q ∈ [2, ∞).
Suggestion: Use Exercise 6.13.
(b) Given α > 0, let Ωα := {(x, y) ∈ R2 : 0 < x < 1, |y| < xα}, and for β > 0
set u(x, y) := x−β. Show that u ∈ H1(Ωα) if β < α−1
2 , but u  Lq(Ω) if q is
sufficiently large. Deduce that Ωα does not have the extension property.
(c) Let α > 3. Show that u ∈ H2(Ωα) if β < α−3
2 and deduce that H2(Ωα)  C(Ωα).
Exercise 7.8 Given a curve Γ = {(x, γ(x)) : x ∈ [a, b]} such that γ ∈ C1([a, b]), we
define
∫
Γ
f dσ :=
∫ b
a
f (x, γ(x))
1 + γ
(x)2 dx
for every Borel-measurable function f : Γ → [0, ∞]. Let Ω = Ω4 be the domain
from Exercise 7.7 with α = 4 and let Γ := {(x, x4) : 0 ≤ x ≤ 1}. Then Γ is a compact
subset of ∂Ω.
(a) Show that u(x, y) = 1
x defines a function u ∈ H1(Ω).
(b) Show that ∫
Γ u2 dσ = ∞.268 7 Neumann and Robin boundary conditions
Exercise 7.9 (a) In dimension d = 1, let Ω := (0, 1
2 )∪( 1
2, 1). Show that Ω does not
have the extension property.
(b) Show that the connected open set Ω := (0, 1)
2 \ {(1/2, y) : 0 < y < 1/2} in R2
does not have the extension property (cf. Figure 7.3).
Suggestion: construct a function u ∈ H1(Ω) such that u = 0 on (0, 1/2)×(0, 1/4)
and u = 1 on (1/2, 1)×(0, 1/4). Apply Stampacchia’s lemma, Lemma 6.36.
(c) For the set Ω from (b), is the imbedding of H1(Ω) into L2(Ω) compact?

0 1
0
1
Fig. 7.3 The set Ω from Exercise 7.9 which does not have the extension property.
Exercise 7.10 (Proof that C1-domains have the extension property) Let Ω ⊂ Rd
be a C1-domain.
(a) Let u ∈ C1(Ω) and let u˜ be the extension of u described in the sketch of the proof
of Theorem 7.19. Show that u˜ ∈ H1(Rd) and u˜H1(Rd) ≤ cuH1(Ω), where the
constant c > 0 is independent of u.
(b) Use a partition of unity to deduce from (a) that there exists a continuous linear
mapping E : C1(Ω) → H1(Rd) such that (Eu)|Ω = u, where C1(Ω) is equipped
the H1(Ω)-norm.
(c) Use the property that C1(Ω) is dense in H1(Ω) (see Theorem 7.27) to conclude
that Ω has the extension property.
Remark: For a different, direct proof which does not use the density of C1(Ω) in
H1(Ω), see [18, Theorem 9.7].
Exercise 7.11 (Poincaré inequality on H1
D(Ω)) Let Ω ⊂ Rd be an open, connected,
bounded set with C1-boundary ∂Ω. Let ΓD ⊂ ∂Ω be a Borel set such that σ(ΓD) > 0.
Define H1
D(Ω) := {u ∈ H1(Ω) : (Tu)|ΓD = 0}, where T : H1(Ω) → L2(∂Ω) is the
trace operator.
(a) Show that H1
D(Ω) is weakly closed in H1(Ω), i.e., if un ∈ H1
D(Ω) and un  u in
H1(Ω), then u ∈ H1
D(Ω). Use Theorem 4.38.
(b) Show that there exists a constant c > 0 such that uL2(Ω) ≤ c ∇uL2(Ω) for all
u ∈ H1
D(Ω). Hint: See the proof of Theorem 7.28.Chapter 8
Spectral decomposition and evolution equations
The spectral decomposition of the Laplacian with suitable boundary conditions gives
us precise information about the solvability of the equation
λu − Δu = f .
But it also yields a method for proving the well-posedness of what are known as
evolution equations. These are equations which, as the name suggests, describe the
evolution of some quantity in time. We will treat the heat equation
ut = Δu
as well as the wave equation
utt = Δu.
Here, u = u(t, x) is a function which depends on a time variable t ≥ 0 and a space
variable x ∈ Ω, where Ω is a domain in Rd. In addition, the spectrum also gives
us information about the asymptotic behavior of the solutions of such equations as
t → ∞.
Chapter overview
8.1 A vector-valued initial value problem ....................... 270
8.2 The heat equation: Dirichlet boundary conditions ................. 274
8.3 The heat equation: Robin boundary conditions .................. 280
8.4 The wave equation ................................. 283
8.5 Inhomogeneous parabolic equations ........................ 295
8.6* Space/time variational formulations ........................ 304
8.7* Comments on Chapter 8 .............................. 308
8.8 Exercises ...................................... 308
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4_8
269270 8 Spectral decomposition and evolution equations
8.1 A vector-valued initial value problem
In this section we return to the situation covered by the spectral theorem (Theo￾rem 4.46): we have an operator A which can be represented as a diagonal operator
with respect to a certain orthonormal basis of the ambient Hilbert space; this oper￾ator defines an initial value problem whose solution can be determined easily as a
series over the basis vectors. Our first major goal in this chapter is to apply these
ideas in the case where A is the Laplacian with given boundary conditions. The inital
value problem then becomes a partial differential equation, more precisely the heat
equation. We start by collecting some simple properties of differentiable functions
which take values in a Banach space.
Definition 8.1 Let E be a real Banach space and I ⊂ R an interval.
(a) A function u : I → E is said to be differentiable if
u
(t) = lim
I #s→t
u(s) − u(t)
s − t
exists as an element of E, for every t ∈ I. The function is continuously dif￾ferentiable if u is additionally continuous. We denote by C1(I, E) the space
of continuously differentiable functions and by C(I, E) the space of continuous
functions from I to E. Since the variable t ∈ I often stands for time, we often
write u(t) in place of u
(t).
(b) We define recursively Ck+1(I, E) := {u ∈ C1(I, E) : u ∈ Ck (I, E)} and set
u(k+1)
(t) := (u
)
(k)
(t), k ∈ N. We also set C∞(I, E) := 
k ∈N Ck (I, E) for k = ∞.

We can define the Riemann integral of a continuous function u : [a, b] → E in
the same way as for real-valued functions: By π we denote a couple consisting of a
partition a = t0 < t1 < ··· < tn = b and a set of intermediate points si ∈ [ti−1, ti],
i = 1,..., n. Then S(π, u) := 
n
i=1 u(si)(ti − ti−1) gives us a Riemann sum associated
with π. Denote by |π| := maxi=1,...,n(ti − ti−1) the mesh size of such a partition with
intermediate points π. Then it is possible to show, just as in the case E = R, that there
exists a unique y ∈ E such that y = limm→∞ S(πm, u) for every sequence (πm)m∈N
of partitions with intermediate points for which limm→∞ |πm| = 0. This defines the
integral ∫ b
a u(t) dt := y. The usual basic properties of Riemann integrals continue to
hold here; this includes the Fundamental Theorem of Calculus: for u ∈ C1(I, E) and
a, b ∈ I we have ∫ b
a u
(s) ds = u(b) − u(a). Conversely, if w : I → E is continuous
and a ∈ I, u0 ∈ E, then u(t) := u0 +
∫ t
a w(s) ds defines a function u ∈ C1(I, E) for
which u = w. Rules for interchanging the order of differentiation and summation
hold just as in the scalar case.
Lemma 8.2 Let un ∈ C1(I, E), n ∈ N, be such that the series u(t) := 
∞
n=0 un(t) and
v(t) := 
∞
n=0 u
n(t) both converge uniformly on I. Then u ∈ C1(I, E) and u = v.
We now wish to take the assumptions of the spectral theorem (Theorem 4.46). That
is, we suppose that V and H are real Hilbert spaces, where H is infinite dimensional8.1 A vector-valued initial value problem 271
and V is compactly and densely imbedded in H, and we let a : V × V → R be a
continuous H-elliptic bilinear form. We will also assume that a(u) := a(u, u) ≥ 0
for all u ∈ V. Denote by A the operator on H associated with a; then the domain of
A takes the form D(A) = {u ∈ V : ∃ f ∈ H, a(u, v) = ( f, v)H for all v ∈ V}, and
given u ∈ D(A) the vector Au is the uniquely determined vector f ∈ H for which
a(u, v) = ( f, v)H for all v ∈ V (cf. Section 4.8, in particular (4.19) and (4.20) and the
following lines). By Theorem 4.46 there exist an orthonormal basis {en : n ∈ N} of
H and 0 ≤ λ1 ≤ ··· ≤ λn ≤ λn+1 ≤ ··· with limn→∞ λn = ∞, such that
D(A) =

u ∈ H :
∞
n=1
λ2
n|(u, en)H |
2 < ∞
'
(8.1)
and
Au =
∞
n=1
λn(u, en)H en, u ∈ D(A). (8.2)
In particular, en ∈ D(A) and Aen = λnen. It also follows from (8.2) that
(Au, en)H = λn(u, en)H for all u ∈ D(A). (8.3)
We will now consider the initial value problem defined by A: given u0 ∈ H,
the objective is to determine a function u ∈ C1((0, ∞),H) ∩ C([0, ∞),H) such that
u(t) ∈ D(A) for all t > 0 and
u(t) + Au(t) = 0, t > 0, (8.4a)
u(0) = u0. (8.4b)
Such a function u will be called a solution of (8.4); this function should solve an
ordinary differential equation with values in an infinite-dimensional Hilbert space.
Once again, we are requiring minimal regularity of u: it should be differentiable in
t > 0 and it should satisfy the differential equation (8.4a). In order for the initial
condition (8.4b) to make sense, we also require that u : [0, ∞) → H be continuous.
Since A is not defined on the whole of H, but rather only on its domain D(A), the
demand that u(t) ∈ D(A) for all t > 0 is necessary. Having made these remarks, we
now wish to prove the following theorem on existence and uniqueness of solutions.
Theorem 8.3 For every u0 ∈ H the problem (8.4) admits a unique solution u. This
solution is given by
u(t) =
∞
n=1
e−λnt
(u0, en)H en. (8.5)
Moreover, u ∈ C∞((0, ∞), H).272 8 Spectral decomposition and evolution equations
Proof 1. Uniqueness: Let u be a solution of (8.4). We consider the real-valued func￾tion un(t) := (u(t), en)H , n ∈ N. Then un : [0, ∞) → R is continuous, differentiable
on (0, ∞), and satisfies
un(t) = (u(t), en)H = −(Au(t), en)H = −λn(u(t), en)H = −λnun(t), t > 0,
where we have used (8.3). Thus un satisfies a linear differential equation on
(0, ∞) with initial value un(0) = (u0, en)H . It follows that un(t) = un(0)e−λnt =
(u0, en)H e−λnt
. By Theorem 4.9 on orthonormal expansions, it now follows that u(t)
is given by (8.5).
2. Existence: We define the function u by (8.5). By Theorem 4.13, the corre￾sponding series converges in H for all t ≥ 0, and moreover u(0) = u0. We will show
that u ∈ C([0, ∞),H). To this end we observe that T(t)u0 := 
∞
n=1 e−λnt
(u0, en)H en
defines a continuous linear operator T(t) : H → H which satisfies T(t) ≤ 1.
If u0 ∈ span{en : n ∈ N}, then obviously limh→0 T(t + h)u0 = T(t)u0. If
u0 ∈ H is arbitrary, then one can use the following approximation argument: choose
u1 ∈ span{en : n ∈ N} such that u0 − u1 H ≤ ε. Then
T(t + h)u0 − T(t)u0 H ≤ T(t + h)u0 − T(t + h)u1 H
+ T(t + h)u1 − T(t)u1 H + T(t)u1 − T(t)u0 H
≤ T(t + h) u0 − u1 H + T(t + h)u1 − T(t)u1 H
+ T(t) u1 − u0 H
≤ ε + T(t + h)u1 − T(t)u1 H + ε.
Hence lim suph→0 T(t + h)u0 − T(t)u0 H ≤ 2ε. Since ε > 0 is arbitrary, it follows
that limh→0 T(t + h)u0 − T(t)u0 H = 0. To show that u ∈ C∞((0, ∞), H), note that
for all ε ∈ (0, 1) and k ∈ N
sup
ε≤t ≤ 1
ε
sup
n∈N
λk
ne−λnt < ∞. (8.6)
Then for every k ∈ N the series
∞
n=1
(−1)
kλk
ne−λnt
(u0, en)H en (8.7)
converges uniformly in the interval [ε, 1
ε ], as long as 0 <ε< 1. By Lemma 8.2, (8.7)
is exactly the kth derivative of u. This shows that u ∈ C∞([ε, 1
ε ], H) for all
0 <ε< 1, which means that u ∈ C∞((0, ∞),H). Finally, by (8.2), u(t) = 
∞
n=0 −λne−λnt
(u0, en)H en = −Au(t) for all t > 0. 
To prove uniqueness of the solution we only needed the assumption that
u ∈ C1((0, ∞), H); however, the proof allows us to deduce that the solution is
automatically C∞. This is due to the special form (8.5) of the solution. We next
wish to prove a stronger differentiability property, which we do by replacing H by8.1 A vector-valued initial value problem 273
a smaller space. This property will be used in the next section to prove regularity
in the space variables. Natural smaller spaces to use are the domains of definition
of powers of A. Their definition and properties are the subject of the next lemma,
which follows directly from (8.1) and (8.2).
Lemma 8.4 For m = 1 we set A1 := A, and define the operators Am, m ∈ N,
recursively by D(Am+1) := {u ∈ D(Am) : Amu ∈ D(A)}, Am+1u := A(Amu). Then
we have
D(Am) =

u ∈ H :
∞
n=1
λ2m
n (u, en)
2
H < ∞

, Amu =
∞
n=1
λm
n (u, en)H en.
In particular, D(Am) is a Hilbert space with respect to the inner product (u, v)Am := 
∞
n=1 λ2m
n (u, en)H (en, v)H + (u, v)H . Finally, D(Am+1) → D(Am) → H for all
m ∈ N.
We can now formulate a stronger regularity statement for the solutions of (8.4).
Theorem 8.5 Given u0 ∈ H, let u be the solution of (8.4). Then we have that
u ∈ C∞((0, ∞), D(Am)) for all m ∈ N.
Proof We have, for k ∈ N,
u(k)
(t) =
∞
n=1
e−λnt
(−λn)
k (u0, en)H en. (8.8)
It follows that u(k)
(t)2
D(Am) = 
∞
n=1(e−2λnt
λ2k
n λ2m
n + 1)|(u0, en)H |
2. Since we still
have supn∈N e−λnε |λn|
p < ∞ for all p ∈ N and ε > 0, the series (8.8) converges
uniformly on [ε, 1
ε ], ε > 0. The claim now follows from Lemma 8.2. 
We finish by giving some additional information about the solutions.
* Comment 8.6 (Semigroups) If as in (8.5) we define the operator T(t) via the rule T(t)x := 
∞
n=1 e−λn t (x, en)H en for x ∈ H, then we obtain an operator T(t) ∈ L(H). This operator
satisfies (a) T(t + s) = T(t)T(s), t, s > 0 and (b) limt↓0 T(t)x = x, x ∈ H. We call such a family
(T(t))t >0 of linear operators on H a continuous semigroup (or C0-semigroup) if (a) and (b) hold.
Given such a semigroup, we call its generator B the operator given by
D(B) :=

x ∈ H : limt↓0
T(t)x − x
t
exists 
, Bx := lim
t↓0
T(t)x − x
t .
One can check that in our case B = A; see Exercise 8.8.
A further example of a semigroup is implicitly given in Theorem 3.43: the Gauss semigroup on
L2(Rd) defined by
(T(t)f )(x) := 1
(4πt)d/2
∫
Rd
e−(x−y)
2/4t f (y) dy,
x ∈ Rd, t > 0, f ∈ L2(Rd). Its generator is the operator B given by D(B) := H2(Rd), Bu := Δu.
We refer to [27, 41] and [6] for the theory of semigroups, which provides the right framework for
studying evolution equations.274 8 Spectral decomposition and evolution equations
8.2 The heat equation: Dirichlet boundary conditions
We will now apply the spectral theorem from the previous section to a special
operator, the Dirichlet Laplacian. Let Ω ⊂ Rd be a bounded open set; in this
section we will not assume any regularity of the boundary of Ω and consider the
Hilbert spaces H := L2(Ω) and V := H1
0 (Ω). Then H1
0 (Ω) is compactly and densely
imbedded in L2(Ω) (Theorem 6.54). Let a : H1
0 (Ω) × H1
0 (Ω) → R be given by
a(u, v) := ∫
Ω ∇u ∇v dx, then a is continuous, symmetric and coercive (the latter
property is a consequence of the Poincaré inequality, Theorem 6.32). Let A be the
operator on L2(Ω) associated with a; this operator can be described as follows.
Theorem 8.7 We have D(A) = {u ∈ H1
0 (Ω)∩H2
loc(Ω) : Δu ∈ L2(Ω)} and Au = −Δu.
Proof Let u ∈ D(A) with Au = f . Then by definition of A we have the identity
∫
Ω ∇u(x) ∇ϕ(x) dx = ∫
Ω f (x)ϕ(x) dx for all ϕ ∈ H1
0 (Ω) and in particular for all
ϕ ∈ D(Ω). This means that −Δu = f weakly. Now by Theorem 6.58 we have
u ∈ H2
loc(Ω). Conversely, if u ∈ H1
0 (Ω) ∩ H2
loc(Ω) and f := −Δu ∈ L2(Ω), then
∫
Ω
∇u(x) ∇ϕ(x) dx = −
∫
Ω
Δu(x)ϕ(x) dx =
∫
Ω
f (x)ϕ(x) dx
for all ϕ ∈ D(Ω). Since D(Ω) is dense in H1
0 (Ω), it thus follows that a(u, v) = ∫
Ω∇u ∇v = ∫
Ω fv for all v ∈ H1
0 (Ω). Hence u ∈ D(A) and Au = f . 
We call the operator ΔD := −A the Laplacian with Dirichlet boundary conditions,
or for short the Dirichlet Laplacian. To summarize, we have
D(ΔD) := {u ∈ H1
0 (Ω) ∩ H2
loc(Ω) : Δu ∈ L2(Ω)},
ΔDu = Δu, u ∈ D(ΔD).
The spectral theorem (Theorem 4.46) yields the existence of an orthonormal basis
of eigenvectors {en : n ≥ 1} of L2(Ω) and associated eigenvalues 0 < λD
1 ≤
λD
2 ≤ ··· ≤ λD
n ≤ λD
n+1 ≤ ··· such that limn→∞ λD
n = ∞, en ∈ D(ΔD) and
−ΔDen = λD
n en. The set {λD
n : n ∈ N} represents the totality of the eigenvalues of
the Dirichlet Laplacian (where some eigenvalues may be repeated); we often refer
to them simply as the Dirichlet eigenvalues.
If λ = λD
n for some n ∈ N, then the Poisson equation with reaction term
λv + Δv = f (8.9)
with v ∈ H1
0 (Ω) ∩ H2
loc(Ω) the unknown, no longer admits a unique solution: if v is
a solution, then so too is v + αen for all α ∈ R. On the other hand, Corollary 4.48
shows that (8.9) has a unique solution whenever λ  {λD
n : n ∈ N}.
By Theorem 4.50, the form a can be represented in diagonal form by8.2 The heat equation: Dirichlet boundary conditions 275
∫
Ω
|∇u|
2 dx =
∞
n=1
λD
n |(u, en)L2 |
2 (8.10)
for all u ∈ H1
0 (Ω). Since on the other hand we have Parseval’s identity (Theo￾rem 4.9(b))
∫
Ω
|u|
2 dx =
∞
n=1
|(u, en)L2 |
2
for all u ∈ L2(Ω), and λD
n ≤ λD
n+1, we obtain
∫
Ω
|∇u|
2 dx ≥ λD
1
∫
Ω
|u|
2 dx (8.11)
for all u ∈ H1
0 (Ω). This is exactly the Poincaré inequality (Theorem 6.32), but this
time with the optimal constant. Indeed, if we choose u = e1 in (8.11), then by (8.10)
we obtain equality; the estimate (8.11) is thus sharp.
We now turn to our main objective, the analysis of the heat equation with Dirichlet
boundary conditions. To this end we consider the following initial-boundary value
problem. Let u0 ∈ L2(Ω) be a given initial value. The problem is to find a u ∈
C∞((0, ∞) × Ω) such that
u(t, ·) ∈ H1
0 (Ω), t > 0, (8.12a)
ut(t, x) = Δu(t, x), t > 0, x ∈ Ω, (8.12b)
lim
t↓0
∫
Ω
|u(t, x) − u0|
2 dx = 0. (8.12c)
Here the Laplacian is to be taken with respect to the space variable x, that is,
Δu(t, x) :=

d
j=1
∂2u(t, x)
∂x2
j
, t > 0, x ∈ Ω.
The condition (8.12b) states that u satisfies the heat equation on (0, ∞)×Ω, while the
initial condition is specified in (8.12c). In fact, we have chosen an initial condition
u0 which is merely in L2(Ω). As such, the convergence of u(t, ·) to u0 in quadratic
mean as t ↓ 0, as stipulated by (8.12c), is a natural condition. The requirement that
u(t, ·) ∈ H1
0 (Ω), t > 0, means that u should satisfy the Dirichlet boundary condition
in the usual weak sense. If we take an eigenfunction en as the initial condition, then
u(t, x) := e−λD
n t
en(x), t > 0, x ∈ Ω
is a solution of (8.12a). Note that by Theorem 6.58 the function en possesses deri￾vatives of all orders; and we see that this special solution has “separated variables”.276 8 Spectral decomposition and evolution equations
In fact, our spectral decomposition method is nothing other than the method of
separation of variables (space and time variables in our case).
Now if u0 ∈ L2(Ω) is an arbitrary initial condition, then we can create a special
solution as a series of the above solutions. More precisely, by Theorem 4.9, we may
write u0 = 
∞
n=1(u0, en)L2(Ω) en, where the series converges in L2(Ω). The series
w(t) :=
∞
n=1
e−λnt
(u0, en)L2(Ω) en (8.13)
likewise converges in L2(Ω) for all t > 0. Now we know from Theorem 8.3 that w
solves the abstract Cauchy problem (8.4), and in fact w ∈ C∞((0, ∞), L2(Ω)). We
will now show using Theorem 8.5 that u defines a solution of (8.12). We stress that
the function w(t) defined by (8.13) is in L2(Ω) for each t > 0.
Theorem 8.8 We have w(t) ∈ C∞(Ω) for all t > 0. If we set
u(t, x) := w(t)(x), t > 0, x ∈ Ω, (8.14)
then u ∈ C∞((0, ∞) × Ω) and u satisfies (8.12).
Proof 1. Let A = −ΔD be the operator on L2(Ω) associated with the form a; then
D(A) ⊂ H2
loc(Ω). We start by showing by induction on m ∈ N that D(Am) ⊂ H2m
loc (Ω).
We know that the claim is true when m = 1. Now assume that it holds for some
m ∈ N and let v ∈ D(Am+1). Then by the induction hypothesis, Δv = Av ∈ D(Am) ⊂
H2m
loc (Ω). It follows by Theorem 6.58 that v ∈ H2m+2
loc (Ω).
Now let k ∈ N be such that k > d/4, then by Theorem 6.58, we have the inclusions
D(Am+k ) ⊂ H2m+2k
loc (Ω) ⊂ Cm(Ω) for all m ∈ N0. Now D(Am+k ) is a Hilbert space
(Lemma 8.4) which is continuously imbedded in L2(Ω). For U  Ω, the spaceCm(U)
is a Banach space with respect to the norm
v Cm(Ω) :=

|α| ≤m
Dαu∞,
where Dαu = ∂|α|
u
∂x
α1
1 ...∂x
αd
d
, α = (α1,...,αd) ∈ Nd
0 , |α| = α1 + ··· + αd and D0u =
u. In particular, v ∞ ≤ v Cm(Ω)
. By the closed graph theorem, it follows that
the restriction mapping v → v|U : D(Am+k ) → Cm(U) is continuous. Indeed, if
vn, v ∈ D(Am+k ), g ∈ Cm(U) satisfy vn → v in D(Am+k ) and vn|
U → g in Cm(U) as
n → ∞, then vn converges to v in L2(Ω) and to g uniformly in U. Hence v = g almost
everywhere in U. But since v is continuous (or, more precisely, v has a continuous
representative), we conclude that v = g, meaning that the graph of the restriction
mapping is closed.
By Theorem 8.5, w ∈ C∞((0, ∞), D(Am+k )); hence the mapping t → w(t)|U :
(0, ∞) → Cm(U) has derivatives of all orders. In particular, w(t) ∈ C(Ω) for all8.2 The heat equation: Dirichlet boundary conditions 277
t > 0. If we now set u(t, x) := w(t)(x), t > 0, x ∈ Ω, then it follows that u is infinitely
differentiable in t > 0, and differentiable in x ∈ Ω, and
ut(t, x) = w(t)(x) = Δw(t) =

d
j=1
∂2u(t, x)
∂x2
j
=: Δu(t, x).
Hence u ∈ C∞((0, ∞) × Ω) and u solves the heat equation (8.12a).
2. We have w(t) = 
∞
n=1 e−λD
n t
(u0, en)L2(Ω)en. Since
∞
n=1
λD
n |e−λD
n t
(u0, en)L2(Ω)|
2 < ∞,
it follows from Theorem 4.50 that w(t) ∈ H1
0 (Ω) for all t > 0. Hence u(t, ·) = w(t)
satisfies the boundary condition in (8.12).
3. Finally, it follows from Parseval’s identity (Theorem 4.9) that
∫
Ω
|u(t, x) − u0(x)|2 dx = w(t) − u0 2
L2(Ω) =
∞
n=1
|(w(t) − u0, en)L2(Ω)|
2
=
∞
n=1
(e−λD
n t − 1)
2|(u0, en)L2(Ω)|
2 → 0, as t → 0.
The convergence here follows from the Dominated Convergence Theorem applied
to the discrete space 2; alternatively, it can be shown as follows: given ε > 0 there
exists an N ∈ N such that 
∞
n=N+1 4|(u0, en)L2(Ω)|
2 ≤ ε/2. There exists a t0 > 0 such
that 
N
n=1(e−λD
n t − 1)
2|(u0, en)L2(Ω)|
2 ≤ ε/2 for all 0 ≤ t ≤ t0, whence
∞
n=1
(e−λD
n t − 1)
2|(u0, en)L2(Ω)|
2
=

N
n=1
(e−λD
n t − 1)
2|(u0, en)L2(Ω)|
2 +
∞
n=N+1
(e−λD
n t − 1)
2|(u0, en)L2(Ω)|
2 ≤ ε
for all 0 ≤ t ≤ t0. We have thus proved (8.12c). 
We next note a few further properties of the solution u defined by (8.13) and (8.14):
we have
∫
Ω
|u(t, x)|2 dx ≤ e−2λD
1 t
∫
Ω
|u0(x)|2 dx, t > 0, (8.15)
since
∫
Ω
|u(t, x)|2 dx = w(t)2
L2(Ω) =
∞
n=1
e−2λD
n t
|(u0, en)L2(Ω)|
2278 8 Spectral decomposition and evolution equations
≤ e−2λD
1 t
∞
n=1
|(u0, en)L2(Ω)|
2 = e−2λD
1 t
u0 2
L2(Ω)
,
where we have used Parseval’s identity twice. The energy of u can be expressed as
follows:
∫
Ω
|∇u(t, x)|2 dx =
∞
n=1
λD
n e−2λD
n t
|(u0, en)L2(Ω)|
2 < ∞; (8.16)
in particular, ∫
Ω |∇u(t, x)|2 dx is a monotonically decreasing function of t > 0. It
is bounded in t > 0 if and only if the initial value u0 is in H1
0 (Ω). By using the
conditions (8.15) and (8.16), we can now show that our solution from (8.13) is
unique.
Theorem 8.9 Given u0 ∈ L2(Ω), let u ∈ C1,2((0, ∞) × Ω) be a solution of (8.12)
such that
∫ T
0
∫
Ω
|u(t, x)|2 dx dt < ∞, T > 0, (8.17)
and
∫ T
ε
∫
Ω
|∇u(t, x)|2 dx dt < ∞, 0 <ε< T < ∞. (8.18)
Then u(t, x) = w(t)(x), t > 0, x ∈ Ω, where w is given by (8.13).
Proof Fix T > 0, then for ϕ ∈ C1
c (0, T) and v ∈ D(Ω) we have
−
∫ T
0
ϕ
(t)
∫
Ω
u(t, x)v(x) dx dt = −
∫
Ω
∫ T
0
ϕ
(t)u(t, x) dt v(x) dx
=
∫
Ω
∫ T
0
ϕ(t)ut(t, x) dt v(x) dx =
∫ T
0
∫
Ω
v(x)ut(t, x) dx ϕ(t) dt
=
∫ T
0
∫
Ω
v(x)Δu(t, x) dx ϕ(t) dt = −
∫ T
0
∫
Ω
∇v(x)∇u(t, x) dx ϕ(t) dt,
where ∇u(t, x) denotes the gradient with respect to the space variables x. Now fix
n ∈ N and set un(t) := ∫
Ω u(t, x)en(x) dx. Since we can approximate en by test
functions v in the H1(Ω)-norm, it follows from the above identity that
−
∫ T
0
ϕ
(t)un(t) dt = −
∫ T
0
∫
Ω
∇en(x)∇u(t, x) dx ϕ(t) dt
= −λn
∫ T
0
un(t)ϕ(t) dt. (8.19)8.2 The heat equation: Dirichlet boundary conditions 279
By (8.17) we have un ∈ L2(0, T). Since (8.19) holds for all ϕ ∈ C1
c (0, T), we obtain
that un ∈ H1(0, T) and u
n = −λnun (see Definition 5.2). It now follows from
Corollary 5.11 that un ∈ C1([0, T]). Hence the function un solves the given linear
differential equation with initial condition un(0) = (u0, en)L2(Ω), whence un(t) =
e−λnt
(u0, en)L2(Ω). Finally, we apply Theorem 4.9(a) on orthogonal expansions to
conclude that u(t, ·) = w(t) for all t ≥ 0. 
We recall the physical interpretation of the solution u of (8.12) which was derived
in Section 1.6.2. Consider, say, the case d = 3; then Ω is a body in space and u0 is an
initial distribution of heat in Ω. The value u(t, x) gives the temperature at the point
x at time t; the Dirichlet boundary condition determines that the temperature at the
boundary is always 0, at every point in time. We may imagine that the body is lying,
say, in an ice bath. We see from equation (8.15) that the temperature converges to 0
exponentially as t → ∞, which reflects what we might expect based on the model.
An even more illuminating example is that of diffusion, such as of ink in water. If u0
is the initial concentration, then u(t, x) gives the concentration of the ink at time t.
For a measurable set U ⊂ Ω, the total amount of ink in the part U of the body Ω at
time t is given by the integral ∫
U u(t, x) dx.
* Comment 8.10 (Classical Dirichlet boundary conditions) In this section we have worked with
arbitrary bounded open sets Ω ⊂ Rd, without imposing any conditions on the boundary. This
means that the solution u(t, x) need not always be continuous at the boundary. In fact, the following
statements about the continuity of solutions at the boundary hold.
1. Continuity of the first eigenfunction at the boundary.
The set Ω is Dirichlet regular (see Section 6.8) if and only if e1 ∈ C0(Ω), that is, limx→z e1(x) = 0
for all z ∈ ∂Ω, [7].
2. Continuity of the solution from Theorem 8.8 at the boundary.
If Ω is Dirichlet regular, then the solution u from Theorem 8.8 is continuous on (0, ∞) × Ω if one
sets u(t, z) = 0 pointwise for all t > 0 and z ∈ ∂Ω.
3. Inhomogeneous heat boundary value problem.
Let Ω be Dirichlet regular andT > 0. Also let g ∈ C(∂∗ΩT ), where we use ∂∗ΩT = ([0, T]×∂Ω)∪
({0} × Ω) to denote the parabolic boundary of (0, T) × Ω. Then there exists a uniquely determined
function u ∈ C([0, T] × Ω) such that u is infinitely differentiable in (0, T) × Ω, ut = Δu holds, and
u|∂∗ΩT = g [6, Theorem 6.2.8]. Uniqueness follows directly from the parabolic maximum principle
in Theorem 3.31.
* Comment 8.11 (Positivity) Let Ω ⊂ Rd be a bounded domain; we also assume that Ω is
connected, but make no assumptions on its boundary. Then the following statements hold.
1. Strict positivity.
If the initial value u0 ∈ L2(Ω) is a positive function (that is, u0(x) ≥ almost everywhere) which is
not 0 almost everywhere, then in fact u(t, x) > 0 for all t > 0 and all x ∈ Ω.
2. Special role of the first eigenfunction.
Under these assumptions on Ω, we have λD
1 < λD
2 (that is, the first eigenvalue is simple) and the
first eigenfunction e1 can be chosen in such a way that e1(x) > 0 for all x ∈ Ω.
For a systematic treatment of positivity in the context of the heat equation we refer to [5] and
[11].280 8 Spectral decomposition and evolution equations
8.3 The heat equation: Robin boundary conditions
In this section we take Ω ⊂ Rd to be a bounded domain with C1-boundary (see
Chapter 7). Our goal is to investigate the heat equation with Robin boundary condi￾tions (see (8.26)). In the case of elliptic equations, we already treated these boundary
conditions in Section 7.5. Here we also wish to consider the corresponding parabolic
equation. We start by defining a realization of the Laplacian with Robin boundary
conditions on L2(Ω), to which we then apply the spectral theorem. For this, we spec￾ify a bounded Borel measurable function b : ∂Ω → [0, ∞) and define the continuous
bilinear form a : H1(Ω) × H1(Ω) → R by
a(u, v) :=
∫
Ω
∇u(x)∇v(x) dx +
∫
∂Ω
bTuTv dσ(z),
where T : H1(Ω) → L2(∂Ω) denotes the trace operator from Theorem 7.34. Since
a(u) + u2
L2(Ω) ≥ u2
H1(Ω)
for all u ∈ H1(Ω), the form is L2(Ω)-elliptic (see (4.22) and (4.5)). Let A be the
operator on L2(Ω) associated with a. By definition, this means for u, f ∈ L2(Ω) that
u ∈ D(A) and Au = f if and only if u ∈ H1(Ω) and
a(u, v) =
∫
Ω
f (x)v(x) dx (8.20)
for all v ∈ H1(Ω). We now wish to describe the operator A. We will be using the
definition of the weak normal derivative introduced in Definition 7.36.
Theorem 8.12 We have D(A) = {u ∈ H1(Ω) ∩ H2
loc(Ω) : Δu ∈ L2(Ω), ∂u
∂ν + b(Tu) =
0}, and Au = −Δu for all u ∈ D(A).
Proof Let u ∈ D(A) and set f := Au. Then by (8.20),
∫
Ω
∇u(x)∇v(x) dx +
∫
∂Ω
bTuTv dσ =
∫
Ω
f (x)v(x) dx (8.21)
for all v ∈ H1(Ω). Hence, in particular, ∫
Ω ∇u(x)∇v(x) dx = ∫
Ω f (x)v(x) dx for
v ∈ D(Ω). it follows from Theorem 6.58 that u ∈ H2
loc(Ω) and −Δu = f . If we now
plug this expression for f into (8.21), then we obtain
∫
Ω
∇u(x)∇v(x) dx +
∫
Ω
(Δu)(x)v(x) dx = −
∫
∂Ω
bTuTv dσ (8.22)
for all v ∈ H1(Ω). This means exactly that ∂u
∂ν + b(Tu) = 0 (see Definition 7.36).
Conversely, suppose that u ∈ H1(Ω) ∩ H2
loc(Ω) is such that − f := Δu ∈ L2(Ω) and
∂u
∂ν +b(Tu) = 0. Then (8.22) holds for all v ∈ H1(Ω) by Definition 7.36. Hence (8.20)
is satisfied, that is, we have u ∈ D(A) and Au = f = −Δu. 8.3 The heat equation: Robin boundary conditions 281
We shall call the operator −A =: Δb the Laplacian with Robin boundary con￾ditions, or just Robin Laplacian. If b = 0 then we recover Neumann boundary
conditions, and so we call the operator Δ0 =: ΔN the Neumann Laplacian. Since
the imbedding H1(Ω) → L2(Ω) is compact, we can apply the spectral theorem
(Theorem 4.46) to obtain the existence of an orthonormal basis {en : n ∈ N} of
L2(Ω) and λb
n ∈ R such that 0 ≤ λb
1 ≤ ··· ≤ λb
n ≤ λb
n+1 ≤ ··· , limn→∞ λb
n = ∞ with
en ∈ D(Δb) and
−Δben = λb
n en, n ∈ N. (8.23)
If λ is a real number different from λb
n for all n ∈ N, then by Corollary 4.48 the
Poisson equation
λv + Δv = f in Ω, (8.24a)
∂v
∂ν + b Tv = 0 on ∂Ω, (8.24b)
has, for every f ∈ L2(Ω), a unique solution v ∈ H1(Ω) ∩ H2
loc(Ω).
We now turn to the parabolic equation. Let u0 ∈ L2(Ω) be a given initial value. If
we set
w(t) :=
∞
n=1
e−λb
n t
(u0, en)L2(Ω) en, (8.25)
then w is the unique solution of (8.4) for A = −Δb. In particular, we have w ∈
C∞((0, ∞), L2(Ω)) and w(t) ∈ H1(Ω) ∩ H2
loc(Ω), t > 0, satisfies
w(t) = Δw(t), t > 0, (8.26a)
∂w
∂ν + b Tw(t) = 0, t > 0, (8.26b)
lim
t↓0 w(t) = u0 in L2(Ω). (8.26c)
If we exploit the regularity properties of the Laplacian just as in the proof of
Theorem 8.8, then we see that w(t) ∈ C∞(Ω), t > 0. We can thus define
u(t, x) := w(t)(x), t > 0, x ∈ Ω, (8.27)
and obtain a function u ∈ C∞((0, ∞) × Ω) which solves the heat equation
ut(t, x) = Δu(t, x), t > 0, x ∈ Ω,
and which satisfies both the Robin boundary condition (8.26b) and the initial condi￾tion (8.26c).282 8 Spectral decomposition and evolution equations
We next wish to examine the asymptotic behavior of w(t) as t → ∞. We recall
that Ω is connected. The following theorem asserts that every solution decays to 0
exponentially fast.
Theorem 8.13 Assume that b(z)  0 on a subset of ∂Ω of positive surface measure.
Then the first eigenvalue λb
1 > 0, and
w(t)2
L2(Ω) ≤ e−λb
1 t
u0 2
L2(Ω)
, t > 0. (8.28)
Proof Suppose that λb
1 = 0. Then
0 = a(e1) =
∫
Ω
|∇e1|
2 dx +
∫
∂Ω
b(T e1)
2 dσ.
Since b ≥ 0, this implies that ∇e1(z) = 0 almost everywhere, and so, by Theo￾rem 6.46, e1 is a constant function, that is, e1(x) = c for all x ∈ Ω. In particular,
0 = a(e1) = c2 ∫
∂Ω b(z) dσ(z). If c  0, then the only possibility is that b(z) = 0
σ-almost everywhere, which is a contradiction to the assumptions of the lemma. 
In the case b = 0 we recover Neumann boundary conditions; in this case, we
denote the eigenvalues by λN
n := λ0
n. These are the eigenvalues of the Neumann
Laplacian −ΔN , for short, the Neumann eigenvalues. The Neumann Laplacian ΔN
is not injective: its kernel consists exactly of the constant functions. Indeed, by
definition, u ∈ ker ΔN if and only if u ∈ H1(Ω) and ∫
Ω ∇u ∇v dx = 0 for all
v ∈ H1(Ω). By Theorem 6.19 this is equivalent to u being constant. Thus λN
1 = 0
and λN
2 > 0. The constant eigenfunction normalized to have L2(Ω)-norm 1 is
e1 = |Ω|
−1/21Ω. We have established that in the case of Robin boundary conditions
the solution decays exponentially to 0, regardless of how small the absorption b on
the boundary may be. In the case of Neumann boundary conditions, that is, when the
absorption b is equal to 0, we encounter completely different asymptotic behavior,
namely convergence to a (nonzero) equilibrium.
Theorem 8.14 (Convergence to equilibrium) In the case of Neumann boundary
conditions, that is, when b = 0, we have
lim
t→∞ w(t) = |Ω|
−1
∫
Ω
u0 dx · 1Ω (8.29)
in L2(Ω).
Proof The statement follows from the simplicity of the first eigenvalue. More pre￾cisely, since λN
1 = 0 and e1 = |Ω|
−1/21Ω, we have
|Ω|
−1
∫
Ω
u0 dx1Ω = (u0, e1)L2(Ω)e1.8.4 The wave equation 283
Then
w(t)−(u0, e1)L2(Ω)e1 =
∞
n=2
e−λN
n t
(u0, en)L2(Ω) en.
By Parseval’s identity, it follows that
"
"
"w(t)−(u0, e1)L2(Ω)e1
"
"
"
2
L2(Ω)
=
∞
n=2
e−λN
n 2t
|(u0, en)L2(Ω)|
2
≤ e−λN
2 2t
∞
n=2
|(u0, en)L2(Ω)|
2
≤ e−λN
2 2t
u0 2
L2(Ω)
,
which was the claim. 
8.4 The wave equation
We start by analyzing the wave equation in an abstract form, just as we did with the
heat equation in Section 8.1. We then interpret the results we obtain in the special
case of the Laplacian with Dirichlet or Neumann boundary conditions. We suppose
that two real Hilbert spaces V and H are given, such that V is compactly imbedded in
H. We also take a : V × V → R to be a continuous, symmetric bilinear form which
is H-elliptic (see (4.22)). We finally wish to assume that
a(u) := a(u, u) ≥ 0, u ∈ V. (8.30)
Let A be the operator on H associated with a(·, ·). We shall study the following
abstract second order initial value problem. Throughout this section we will assume
that 0 < T < ∞. Let u0 ∈ D(A) and u1 ∈ H. The goal is to find a function
w ∈ C2([0, T], H) such that w(t) ∈ D(A) for t ∈ [0, T] and
w(t) + Aw(t) = 0, t ∈ [0, T], (8.31a)
w(0) = u0, w(0) = u1. (8.31b)
That is, u0 is the given initial value and u1 is the given initial velocity. By spectrally
decomposing A, we can reduce problem (8.31) to a one-dimensional second order
linear differential equation. Indeed, by the spectral theorem (Theorem 4.46) there
exist an orthonormal basis {en : n ∈ N} of H and numbers λn ∈ R with 0 ≤ λn ≤
λn+1, limn→∞ λn = ∞, such that A is given by
D(A) =

v ∈ H :
∞
n=1
λ2
n|(v, en)H |
2 < ∞
'
, Av =
∞
n=1
λn(v, en)H en; (8.32)284 8 Spectral decomposition and evolution equations
in particular, en ∈ D(A) and Aen = λnen for all n ∈ N. By (8.30), we have that
λn = (Aen, en)H = a(en) ≥ 0, n ∈ N. We now consider the following special initial
values.
(a) Take u0 = en and u1 = 0. Then w(t) = cos(
√
λnt)en is a solution of (8.31).
(b) Take u0 = 0 and u1 = en. If λn > 0, then w(t) = λ−1/2 n sin(
√
λnt)en defines a
solution of (8.31). If λn = 0, then w(t) = ten is a solution of (8.31).
We can now express the general solution of (8.31) as a series of solutions of these
special types and prove the following theorem.
Theorem 8.15 Let u0 ∈ D(A) and u1 ∈ V. Then (8.31) admits a unique solution
w ∈ C2([0, T], H) such that w(t) ∈ D(A) for all t ∈ [0, T].
Proof Uniqueness: Let w be a solution of (8.31), and set wn(t) := (w(t), en)H for
n ∈ N and t ∈ [0, T]. Then wn : [0, T] → R is twice differentiable with derivative
wn(t) = (w(t), en)H = −(Aw(t), en)H = −
∞
k=1
λk (w(t), ek )H (ek, en)H
= −λn(w(t), en)H = −λnwn(t), t ∈ (0, T).
Hence wn satisfies the second order linear differential equation wn(t) = −λnwn(t),
t ≥ 0, with initial conditions wn(0) = (u0, en)H , wn(0) = (u1, en)H . It follows that
wn(t) = cos(

λnt)(u0, en)H +
1
√
λn
sin(

λnt)(u1, en)H, if λn > 0, (8.33)
and
wn(t) = (u0, en)H + t(u1, en)H, if λn = 0. (8.34)
Indeed, it is easy to check that wn, thus defined, is a solution of the given differential
equation. But it is the unique solution, as follows from simple properties of the
theory of ordinary differential equations; alternatively, we may use the conservation
of energy property obtained below in Theorem 8.16. Since w(t) = 
∞
n=1 wn(t)en is
the unique expansion of w(t) with respect to the orthonormal basis {en : n ∈ N}, the
uniqueness statement has been proved.
Existence: We now define wn(t) by (8.33). Since 
∞
n=1 λ2
n|(u0, en)H |
2 < ∞ and

∞
n=1 λn|(u1, en)H |
2 < ∞ (see (4.26)), not only does w(t):= 
∞
n=1wn(t)en converge
uniformly in H, but so too do the series 
∞
n=1wn(t)en and 
∞
n=1wn(t)en.
Hence, by Lemma 8.2, w ∈ C2([0, T], H). Since 
∞
n=1 λ2
n |wn(t)|2 < ∞, we have
w(t) ∈ D(A) for all t ∈ [0, T]. Thus
w(t) =
∞
n=1
wn(t)en =
∞
n=1
wn(t)λnen = −Aw(t)
by definition of A. In addition,8.4 The wave equation 285
w(0) =
∞
n=1
(u0, en)H en = u0 and w(0) =
∞
n=1
(u1, en)H en = u1.
This shows that w solves (8.31), and we have proved existence of solutions. 
We also wish to note the following principle of conservation of energy.
Theorem 8.16 (Conservation of energy) Let w be the solution of (8.31). Then,
a(w(t)) + w(t)2
H = a(u0) + u1 2
H for all t ∈ [0, T]. (8.35)
One way to obtain (8.35) is via a direct calculation using the form of the solution
which we obtained in the previous proof (Exercise 8.1). More interesting, however,
is the following proof. Once we have (8.35), we can use it to give a second proof of
uniqueness in Theorem 8.15 (see below). In order to prove Theorem 8.16 we will
use the following chain rule.
Lemma 8.17 (Chain rule) Let b : V×V → R be continuous, symmetric and bilinear
and let u ∈ C1((a, b),V). Then
d
dt b(u(t)) = 2 b(u(t), u(t)),
where we have again set b(v) := b(v, v), v ∈ V.
Proof For arbitrary t ∈ (a, b), we have
b(u(t + h)) − b(u(t)) = b(u(t + h) − u(t), u(t + h)) + b(u(t), u(t + h) − u(t)).
Division by h  0 and then letting h → 0 yields the claim. 
Proof (of Theorem 8.16) We apply the previous Lemma 8.17 to a(w(t)) and to
w(t)2
H = (w(t), w(t))H to obtain
d
dt {a(w(t)) + w(t)2
H } = 2a(w(t), w(t)) + 2(w(t), w(t))H
= 2(Aw(t), w(t))H + 2(−Aw(t), w(t))H = 0.
Hence a(w(t)) + w(t)2
H is constant in t ∈ [0, T], and so by (8.31b),
a(w(t)) + w(t)2
H = a(w(0)) + w(0)2
H = a(u0) + u1 2
H,
which was the claim. 286 8 Spectral decomposition and evolution equations
Proof (2nd proof of uniqueness in Theorem 8.15) Let w and w be two solutions
of (8.31). Then w(t) := w(t) − w(t) defines a solution w ∈ C2([0, T], H) of (8.31a)
with initial conditions w(0) = w(0) = 0. By Theorem 8.16, a(w(t)) + w(t)2
H = 0
for all t ∈ [0, T]. It follows that w(t) ≡ 0, whence w(t) = w(0) = 0 for all t ∈ [0, T],
that is, w = w. 
Let u0 ∈ D(A) and u1 ∈ V. Then we know from Theorem 8.15 that the prob￾lem (8.31) has a unique solution u ∈ C2([0, T], H). For the numerical analysis in
Section 9.5 we will need stronger regularity properties of w; to obtain these we need
stronger assumptions on the initial values u0 and u1.
We recall the definition of D(A) from (8.32) and will continue to use the orthonor￾mal basis {en : n ∈ N} of H consisting of eigenvectors of A, i.e., Aen = λnen. Now
the square of A is defined by
D(A2) := {v ∈ D(A) : Av ∈ D(A)}, A2v := A(Av).
This leads to a hierarchy of spaces D(A2) ⊂ D(A) ⊂ V ⊂ H.
Theorem 8.18 (Regularity) Let u0 ∈ D(A), u1 ∈ V and assume that w ∈
C2([0, T], H) is the solution of (8.31).
(a) If u0 ∈ D(A2) and u1 ∈ D(A) with Au1 ∈ V, then w ∈ C3([0, T],V).
(b) If u0 ∈ D(A2) with A2u0 ∈ V and u1 ∈ D(A2), then w ∈ C4([0, T],V).
The proof will use the following facts.
Lemma 8.19 (a) Let v ∈ H. Then v ∈ V if and only if 

∞
n=1
λn (v, en)
2
H < ∞.
(b) The quantity
|v |2
V := v 2
H + a(v, v) =
∞
n=1
(1 + λn) (v, en)
2
H
defines an equivalent norm on V.
(c) Let cn ∈ C([0, T]) and γn ≥ 0 such that 

∞
n=1
γn < ∞. If λn|cn(t)|2 ≤ γn for all
t ∈ [0, T], n ∈ N, then w(t) := 

∞
n=1
cn(t) en converges in V uniformly with respect
to t ∈ [0, T]. In particular, w ∈ C([0, T],V).
(d) Let f ∈ C([0, T],V) and fn(t) := ( f (t), en)H . Then 

∞
n=1
fn(t) en converges in V
to f (t) uniformly with respect to t ∈ [0, T].
Proof Since a(·, ·) is H-elliptic there exist ω ≥ 0, α > 0 such that a(v, v)+ω v 2
H ≥
α v 2
V for all v ∈ V. This implies the existence of some ε > 0 such that
|v |2
V = a(v, v) + v 2
H ≥ ε v 2
V for all v ∈ V. (8.36)
Indeed, otherwise we could find (vn)n∈N ⊂ V, such that a(vn, vn)+vn 2
H < 1
n vn 2
V .
We may assume that vn V = 1 (replacing vn by vn/vn V otherwise). Since8.4 The wave equation 287
a(vn, vn) ≥ 0 it follows that a(vn, vn) → 0 and vn 2
H → 0 as n → ∞. This
contradicts the H-ellipticity, and (8.36) is proven. We have thus shown that | · |V
defines an equivalent norm on V. This norm is associated with the scalar product
[u, v]V := (u, v)H + a(u, v), i.e., |v |2
V = [v, v]V, u, v ∈ V.
Note that [en, v]V = (1+ λn) (en, v)H for all v ∈ V. Let eˆn := 1
√
1+λn
en, then, (eˆn)n∈N
is orthonormal in (V, [·, ·]V ); in fact, by Corollary 4.19, it is actually an orthonormal
basis of (V, [·, ·]V ). Then by Theorem 4.9 and Theorem 4.13, for cˆn ∈ R, the series

∞
n=1 cˆn eˆn converges in V if and only if 
∞
n=1 cˆ
2
n < ∞. Thus, for cn ∈ R, the series

∞
n=1 cn en = 
∞
n=1(1+λn)
1/2cn eˆn converges inV if and only if 
∞
n=1(1+λn) c2
n < ∞,
which is equivalent to 
∞
n=1 λnc2
n < ∞. Taking (4.27) in Theorem 4.50 into account
completes the proof of (a) and (b).
In order to show (c), assume that λn cn(t)
2 ≤ γn for all n ∈ N and t ∈ [0, T]. It
follows from (b) that w(t) ∈ V and















w(t) − 
N
n=1
cn(t) en















2
V
=















∞
n=N+1
cn(t) en















2
V
=
∞
n=N+1
|cn(t)|2 (1 + λn)
≤
∞
n=N+1
γn
 1
λN
+ 1

→ 0 as n → ∞.
It remains to show (d). Since | f (·)|2
V is a continuous function it follows from Dini’s
theorem1 that the series
∞
n=1
(1 + λn) | fn(t)|2 = | f (t)|2
V
converges uniformly on [0, T]. Thus, given ε > 0, there exists N ∈ N such that















f (t) − 
N
n=1
fn(t) en















2
V
=
∞
n=N+1
(1 + λn) | fn(t)|2 ≤ ε
for all t ∈ [0, T]. 
Proof (of Theorem 8.18) We know from Theorem 8.15 and its proof that (8.31) has
a unique solution w ∈ C2([0, T], H) given by w(t) = 
∞
n=1 wn(t) en, where wn, in
turn, is given by (8.33) or (8.34). Observe that by (8.33) for λn > 0 (and trivially for
λn = 0)
λn |wn(t)|2 ≤ 2λn(u0, en)
2
H + 2 (u1, en)
2
H (8.37)
1 If a monotone sequence of continuous functions converges pointwise on a compact set and if the
limit function is also continuous, then the convergence is uniform.288 8 Spectral decomposition and evolution equations
Since u0 ∈ V, we have 
∞
n=1 λn (u0, un)
2
H < ∞. Moreover, 
∞
n=1(u1, en)
2
H < ∞. It
follows from Lemma 8.19 (c) that the series 
∞
n=1 wn(t) en converges in V, uniformly
in [0, T]. In particular, w ∈ C([0, T],V).
Next we show that w ∈ C1([0, T],V). Observe that for λn > 0 by (8.33) and (8.34)
wn(t) = −

λn sin(

λnt) (u0, en)H + cos(

λnt) (u1, en)H .
Thus
λn |wn(t)|2 ≤ 2 λ2
n (u0, en)
2
H + 2 λn (u1, en)
2
H . (8.38)
Since u0 ∈ D(A), we have 
∞
n=1 λ2
n (u0, en)
2
H < ∞; and since u1 ∈ V, we also
have 
∞
n=1 λn (u1, en)
2
H < ∞. It follows from Lemma 8.19 (c) that 
∞
n=1 wn(t) en
converges in V uniformly in [0, T]. Lemma 8.2 implies that w ∈ C1([0, T],V) and
w(t) = 
∞
n=1 wn(t) en. Observe that so far we have only used that u0 ∈ D(A) and
u1 ∈ V.
In order to prove (a) we now assume that u0 ∈ D(A2) and u1 ∈ D(A) with
Au1 ∈ V. Then (A2u0, en)H = λn(Au0, en)H = λ2
n(u0, en)H , whence
∞
n=1
λ4
n (u0, en)
2
H =
∞
n=1
(A2
u0, en)
2
H = A2u0 2
H < ∞ and similarly
∞
n=1
λ3
n (u1, en)
2
H =
∞
n=1
λn (Au1, en)
2
H ≤ |Au1 |2
V < ∞.
Now wn = −λn wn(t) and so, by (8.37),
λn |wn(t)|2 ≤ 2 λ3
n (u0, en)
2
H + 2 λ2
n (u1, en)
2
H .
It follows from Lemma 8.19 (c) that 
∞
n=1 wn(t) en converges in V uniformly in [0, T]
and thus by Lemma 8.2 that w ∈ C2([0, T],V).
Finally, w(3)
n (t) = −λn wn(t). Thus by (8.38)
λn |w(3)
n (t)|2 ≤ 2 λ4
n (u0, en)
2
H + 2 λ3
n (u1, en)
2
H .
Again appealing to Lemma 8.19 and Lemma 8.2 we deduce the fact that w ∈
C3([0, T],V). Thus assertion (a) is proved.
The proof of (b) is similar: if in fact u0 ∈ D(A2), A2u0 ∈ V and u1 ∈ D(A2), then
∞
n=1
λ5
n (u0, en)
2
H ≤ |A2
u0 |2
V < ∞ and
∞
n=1
λ4
n (u1, en)
2
H ≤ |A2u1 |2
V < ∞.8.4 The wave equation 289
Note that w(4)
n (t) = λ2
n wn(t). Thus by (8.37)
λn |w(4)
n (t)|2 ≤ 2 λ5
n (u0, en)
2
H + 2 λ4
n (u1, en)
2
H .
Again Lemma 8.19 and Lemma 8.2 imply that w ∈ C4([0, T],V). 
We next consider the inhomogeneous wave equation with initial values
w(t) + A w(t) = f (t), (8.39a)
w(0) = u0, w(0) = u1. (8.39b)
We have the following result on existence and uniqueness.
Theorem 8.20 Let u0 ∈ D(A), u1 ∈ V and f ∈ C([0, T],V). Then there exists a
unique function w ∈ C2([0, T], H) satisfying w(t) ∈ D(A) for all t ∈ [0, T] which
solves (8.39).
Proof Since the difference of two solution of (8.39) is a solution of the homoge￾neous problem (8.31), uniqueness follows from Theorem 8.15. Moreover, since the
homogeneous problem has a solution, in order to prove existence for (8.39), we may
and will assume that u0 = u1 = 0.
The function fn given by fn(t) := ( f (t), en)H is continuous on [0, T], and if w
is a solution of (8.39), then wn(t) := (w(t), en)H solves wn + λn wn = fn, wn(0) =
wn(0) = 0. This means that necessarily
wn(t) = 1
√
λn
∫ t
0
sin(

λn(t − s)) fn(s) ds, if λn > 0 and
wn(t) =
∫ t
0
(t − s) fn(s) ds, if λn = 0,
as one verifies immediately. We now define wn by these expressions and show that
w(t) :=
∞
n=1
(wn(t), en)H en (8.40)
converges in H and defines a solution of (8.39). Since limn→∞ λn = ∞ there exists
N0 ∈ N such that λn > 0 for all n ≥ N0. By Hölder’s inequality, we have, for n ≥ N0,
λn wn(t)
2 ≤
∫ T
0
sin(

λn(t − s))2 ds ∫ T
0
fn(s)
2 ds ≤ T
∫ T
0
fn(s)
2 ds =: γn.
Since
∞
n=N0
γn ≤ T
∫ T
0
∞
n=1
fn(s)
2 ds = T
∫ T
0
 f (s)2
H < ∞,290 8 Spectral decomposition and evolution equations
it follows from Lemma 8.19 (c) that (8.40) converges in V uniformly with respect to
t ∈ [0, T]. Thus w ∈ C([0, T],V). Moreover, for n ≥ N0,
λ2
n wn(t)
2 ≤ T
∫ T
0
λn fn(s)
2 ds =: γ1
n.
Since
∞
n=N0
γ1
n = T
∫ T
0
∞
n=N0
λn fn(s)
2 ds ≤ T
∫ T
0
| f (s)|2
V ds < ∞,
it follows from the definition of A that w(t) ∈ D(A) and that 
∞
n=1 λn wn(t) en
converges to Aw(t) in H uniformly with respect to t ∈ [0, T]. In particular, Aw(·) ∈
C([0, T], H). Next we show that w ∈ C1([0, T],V). Let n ≥ N0. Note that
wn(t) =
∫ t
0
cos(

λn(t − s)) fn(s) ds.
Thus by Hölder’s inequality
λn wn(t)
2 ≤ T
∫ T
0
λn fn(s)
2 ds = γ1
n.
It follows from Lemma 8.19 (c) that 
∞
n=1 wn(t) en converges in V uniformly
with respect to t ∈ [0, T]. Lemma 8.2 implies that w ∈ C1([0, T],V) and
w(t) = 
∞
n=1 wn(t) en.
In order to prove that w ∈ C2([0, T], H) we note that wn(t) = −λn wn(t) + fn(t).
We have already seen that 
∞
n=1 −λn wn(t) en converges in H to −Aw(t) uniformly
with respect to t ∈ [0, T]. Since f ∈ C([0, T],V) it follows from Lemma 8.19
(d) that 
∞
n=1 fn(t) en converges in V uniformly with respect to t ∈ [0, T]. Thus
w ∈ C2([0, T], H) and w(t) + Aw(t) = f (t) for all t ∈ [0, T]. 
Next we establish the regularity results for the solutions of the inhomogeneous
wave equation which will be needed in Section 9.5.
Theorem 8.21 Let u0 ∈ D(A), u1 ∈ V, f ∈ C([0, T],V) and let w be the solution of
problem (8.39).
(a) If u0 ∈ D(A2), u1 ∈ D(A), Au1 ∈ V and f ∈ C2([0, T],V) with f (0) ∈ D(A),
then w ∈ C3([0, T],V).
(b) If u0 ∈ D(A2) with A2u0 ∈ V, u1 ∈ D(A2), f ∈ C3([0, T],V) such that f (0) ∈
D(A), A f (0) ∈ V and f (0) ∈ D(A), then w ∈ C4([0, T],V).
Proof In view of Theorem 8.18 we can and will assume that u0 = u1 = 0. Adopting
the notation of Theorem 8.20, we already know from the proof of that theorem
that w ∈ C1([0, T],V) and that w(t) = 
∞
n=1 wn(t) en converges in V uniformly with
respect to t ∈ [0, T].8.4 The wave equation 291
1. Assume that f ∈ C1([0, T],V). We show that w ∈ C2([0, T],V) (and not merely
in C2([0, T], H) as was the case in Theorem 8.20). Let N0 ∈ N such that λn > 0
for all n ≥ N0. For n ≥ N0, we have
wn(t) =
∫ t
0
cos(

λn(t − s)) fn(s) ds =
∫ t
0
cos(

λns) fn(t − s) ds.
Thus,
wn(t) =
∫ t
0
cos(

λns) fn(t − s) ds + cos(

λnt) fn(0).
We shall apply Lemma 8.19 (c) and estimate these two terms:
∞
n=N0
λn
∫ t
0
cos(

λns) fn(t − s) ds2
≤
∞
n=N0
λn T
∫ T
0
f 2
n ds
= T
∫ T
0
∞
n=N0
λn f 2
n ds ≤ T
∫ T
0
| fn(s)|2
V ds < ∞
for the first term. Since f (0) ∈ V, we obtain that 
∞
n=1 λn | fn(0)|2 < ∞. The claim
follows from Lemma 8.19 (c), which also shows that w(t) = 
∞
n=1 wn(t) en in V
uniformly on [0, T].
2. Now assume that f ∈ C2([0, T],V) and f (0) ∈ D(A). We have
w(3)
n (t) =
∫ t
0
cos(

λns) fn(t − s) ds + cos(

λnt) fn(0)
−

λn sin(

λnt) fn(0).
In order to apply Lemma 8.19 (c) we estimate the corresponding three terms:
(i)
∞
n=N0
λn
∫ t
0
cos(

λns) fn(t − s) ds2
≤
∞
n=N0
T
∫ T
0
λn fn(s)
2 ds
≤ T
∫ T
0
| f (s)|2
V ds < ∞;
(ii)
∞
n=N0
λn| fn(0)|2 < ∞ since fn(0) ∈ V;
(iii)
∞
n=N0
λn

λn sin(

λnt) fn(0)
2
≤
∞
n=N0
λ2
n fn(0)
2 < ∞
since f (0) ∈ D(A).
It follows that w ∈ C3([0, T],V) and w(3)
(t) = 
∞
n=1 w(3)
n (t) en in V uniformly on
[0, T]. This proves (a).292 8 Spectral decomposition and evolution equations
3. We now prove (b). We have
w(4)
n (t) =
∫ t
0
cos(

λns) f
(3) n (t − s) ds + cos(

λnt) fn(0)
−

λn sin(

λnt) fn(0) − λn cos(

λnt) fn(0).
In order to apply Lemma 8.19 (c) we now have to estimate these four terms:
(i)
∞
n=N0
λn
∫ t
0
cos(

λns) f
(3) n (t − s) ds2
≤
∞
n=N0
T
∫ T
0
λn f
(3) n (s)
2 ds
≤ T
∫ T
0
| f (3)
(s)|2
V ds < ∞;
(ii)
∞
n=N0
λn

cos(

λnt) fn(0)
	2 ≤
∞
n=N0
λn fn(0)
2 < ∞ since f (0) ∈ V;
(iii)
∞
n=N0
λn

−

λn sin(

λnt) fn(0)
	2 ≤
∞
n=N0
λ2
n fn(0)
2 < ∞
since f (0) ∈ D(A);
(iv)
∞
n=N0
λn

λn cos(

λnt) fn(0)
	2 ≤
∞
n=1
λ3
n fn(0)
2 =
∞
n=1
λn (A f (0), en(0))2
H
≤
∞
n=1
(1 + λn) (A f (0), en)
2
H = |A f (0)|2
V < ∞
Again, Lemma 8.19 (c) shows that w ∈ C4([0, T],V). 
We now wish to apply Theorem 8.15 to the Laplacian; in doing so, we will limit
ourselves to Dirichlet and Neumann boundary conditions. Let Ω ⊂ Rd be a bounded
domain; in the case of Neumann boundary conditions, we shall also assume that Ω is
of class C1. In both cases we will take H = L2(Ω). In the case of Dirichlet boundary
conditions, the following result is a direct corollary of Theorem 8.20.
Theorem 8.22 Assume that Ω is convex. Let u0 ∈ H1
0 (Ω) ∩ H2(Ω) and let u1 ∈
H1
0 (Ω) be given. If f ∈ C([0, T], H1
0 (Ω)), then there exists a uniquely determined
w ∈ C2([0, T], L2(Ω)) for which w(t) ∈ H1
0 (Ω) ∩ H2(Ω) for all t ∈ [0, T], w satisfies
w(t) = Δw(t) + f (t) for all t ∈ [0, T], and w(0) = u0, w(0) = u1. Moreover,
w ∈ C([0, T], H2(Ω)).
Proof Let A = ΔD it follows from Theorem 6.86 that D(A) = H1
0 (Ω) ∩ H2(Ω).
Thus the first assertion is a direct consequence of Theorem 8.20. It remains to
prove the last statement. Recall that the form a(·, ·) is coercive (by the Poincaré
inequality, Theorem 6.32). Thus the operator A is invertible. This implies that D(A)
is a Banach space with respect to the norm v A := Av L2(Ω) = Δv L2(Ω). Note that8.4 The wave equation 293
H1
0 (Ω)∩H2(Ω) is a closed subspace of H2(Ω), and v A ≤ v H2(Ω). This means that
the identity I : H2(Ω) ∩ H1
0 (Ω) → D(A) is continuous. By Theorem A.4 its inverse
I−1 is also continuous, i.e., there exists a constant β > 0 such that v H2(Ω) ≤ β v A.
Thus the two norms ·A and ·H2(Ω) are equivalent on H2(Ω) ∩ H1
0 (Ω). Since
w ∈ C2([0, T], H) and w = Δw + f , it follows that Aw(·) = Δw(·) ∈ C([0, T], H).
This implies that w ∈ C([0, T], D(A)). Since the two norms are equivalent we finally
obtain that w ∈ C([0, T], H2(Ω)). 
Theorem 8.21 yields the following regularity result, which will be useful for the
numerical solution of the wave equation in Section 9.5. For further regularity results
of the same kind, see also Exercise 8.11.
Theorem 8.23 Assume that Ω ⊂ Rd is bounded, open and convex.
(a) Let u0 ∈ H4(Ω) ∩ H1
0 (Ω) such that Δu0 ∈ H1
0 (Ω) and let u1 ∈ H2(Ω) ∩ H1
0 (Ω)
such that Δu1 ∈ H1
0 (Ω). Finally, let f ∈ C2([0, T], H1
0 (Ω)) with f (0) ∈ H2(Ω).
Then w ∈ C3([0, T], H1
0 (Ω)), where w is the solution from Theorem 8.22.
(b) Let u0 ∈ H4(Ω) ∩ H1
0 (Ω) such that Δu0 ∈ H1
0 (Ω), Δ2u0 ∈ H1
0 (Ω) and let
u1 ∈ H4(Ω) ∩ H1
0 (Ω) such that Δu1 ∈ H1
0 (Ω). Finally, let f ∈ C3([0, T], H1
0 (Ω))
such that f (0) ∈ H2(Ω), Δ f (0) ∈ H1
0 (Ω) and f (0) ∈ H2(Ω). Then, we have that
w ∈ C4([0, T], H1
0 (Ω)).
Proof Note that D(A) = H2(Ω) ∩ H1
0 (Ω) by Theorem 6.86. This implies that {u ∈
H4(Ω) ∩ H1
0 (Ω) : Δu ∈ H1
0 (Ω)} ⊂ {u ∈ H2(Ω) ∩ H1
0 (Ω) : Δu ∈ H2(Ω) ∩ H1
0 (Ω)} =
D(A2). Now the claim follows from Theorem 8.22. 
We recall that the domain of the Laplacian with Neumann boundary conditions
is defined as follows:
D(ΔN ) =

u ∈ H1(Ω) ∩ H2
loc(Ω) : Δu ∈ L2(Ω), ∂u
∂ν = 0
'
;
see Theorem 8.12 and the subsequent definition. If we choose A = ΔN in Theo￾rem 8.15 we obtain:
Theorem 8.24 Suppose that Ω ⊂ Rd is a bounded, open set with C1-boundary. Let
u0 ∈ D(ΔN ), u1 ∈ H1(Ω) and f ∈ C([0, T], H1(Ω)). Then there exists a uniquely
determined function w ∈ C2([0, T], L2(Ω)) for which w(t) ∈ D(ΔN ), w(t) = Δw(t) +
f (t) for all t ∈ [0, T], and w(0) = u0, w(0) = u1.
The corresponding theorem for the case of Robin boundary conditions is for￾mulated in Exercise 8.3. We next wish to compare the eigenvalues corresponding
to Dirichlet and to Neumann boundary conditions, respectively. As before, we will
denote by
0 < λD
1 < λD
2 ≤ λD
3 ≤ ··· ≤ λD
n ≤ λD
n+1 ≤ ···294 8 Spectral decomposition and evolution equations
the Dirichlet eigenvalues and by {en : n ∈ N} a corresponding orthonormal basis
of L2(Ω) consisting of eigenfunctions en ∈ D(ΔD), that is, −Δen = λD
n en. Then for
n ∈ N, w(t) = cos(

λD
n t)en, t ≥ 0, is the solution of (8.30) with initial conditions
u0 = en, u1 = 0. This is a stationary solution of the wave equation: its variation
in time consists only of multiplication by the factor cos(

λD
n t). These solutions are
often referred to as the normal modes of the domain Ω. If d = 2, then we may
imagine Ω to be a membrane or a tambourine; the eigenvalues λD
n give the natural
frequencies (or resonant frequencies) of the membrane.
Neumann boundary conditions describe the situation where the boundary is not
fixed. The Neumann eigenvalues
0 = λN
1 < λN
2 ≤ λN
3 ≤ ··· ≤ λN
n ≤ λN
n+1 ≤ ···
represent the natural frequencies of a free membrane, when d = 2. The following
theorem shows that such a free membrane has a deeper sound than the corresponding
fixed membrane described by Dirichlet boundary conditions.
Theorem 8.25 We have λN
n ≤ λD
n for all n ∈ N.
Proof Let a(u) = ∫
Ω |∇u|
2 dx for u ∈ H1(Ω). By Theorem 4.52,
λN
n = max
W⊂H1(Ω)
codimW≤n−1
min
u∈W
uL2 =1
a(u), λD
n = max
V ⊂H1
0 (Ω)
codimV ≤n−1
min
u∈V
uL2 =1
a(u).
Let W ⊂ H1(Ω) be a subspace of H1(Ω) with codim W ≤ n−1. By Remark 4.53, this
means exactly that W ∩U  {0} for every subspace U of H1(Ω) such that dimU ≥ n.
Now set V := W ∩ H1
0 (Ω), then we claim that codim V ≤ n − 1 in H1
0 (Ω). Indeed, if
U is a subspace of H1
0 (Ω) with dimU ≥ n, then U ∩W  {0} by choice of W. Hence
U ∩ V = U ∩ W  {0} as well, which implies that codim V ≤ n − 1 in H1
0 (Ω), as
claimed. Now since V ⊂ W, for this W we have
min
u∈W
uL2 =1
a(u) ≤ min
u∈V
uL2 =1
a(u) ≤ λD
n .
Since W was an arbitrary subspace of H1(Ω) for which codimW ≤ n − 1, it follows
that λN
n ≤ λD
n . 
* Comment 8.26 It is possible to say more, namely λN
n+1 < λD
n for all n ∈ N; see [30].
The Dirichlet eigenvalues depend on the domain Ω; we may write λD
n (Ω) to emphasize this
dependence: these give the resonant frequencies of the domain, which in principle can be heard.
A famous problem is the question as to whether these eigenvalues actually determine the domain;
more precisely, the question is whether the assumption that λD
n (Ω1) = λD
n (Ω2) for all n ∈ N (where
Ω1, Ω2 ⊂ Rd are two domains inRd), already implies that Ω1 and Ω2 are congruent (that is, whether
there exist an orthogonal d × d matrix B and a vector c ∈ Rd such that Ω2 = {Bx + c : x ∈ Ω1 }).
In 1966, Marc Kac [39] formulated this question as follows: “can one hear the shape of a drum?”
It was only in 1992 that a now-famous counterexample was given, by Gordon, Webb and Wolpert.8.5 Inhomogeneous parabolic equations 295
This takes the form of two polygons in R2 which are not congruent to each other but have the same
Dirichlet and Neumann eigenvalues. We refer to [12] for a recent presentation of this result in the
language of bilinear forms as used extensively throughout this book. However, the problem remains
open for domains with C∞-boundary in any dimension d ≥ 2, as well as for convex domains in
dimension d = 2 and 3.
8.5 Inhomogeneous parabolic equations
We next wish to study the inhomogeneous heat equation
ut(t, x) = Δu(t, x) + f (t, x), t > 0, x ∈ Ω (8.41)
with suitable initial and boundary conditions. In contrast to the inhomogeneous wave
equation, where the Riemann integral was used throughout, for the inhomogeneous
parabolic equation (8.41) we will need the Lebesgue integral of functions defined on
an interval with values in a Hilbert space H. We mention that this is a special case
of the Bochner integral for Banach space-valued functions (see [6, Sec. 1.1]), but
here we will not need this theory: we will follow a more direct approach, defining
the integral with the help of an orthonormal basis of H. Once we have introduced H￾valued Lebesgue integrals we are able to treat Sobolev spaces on an interval exactly
as in Chapter 4; the problem itself will again be treated via spectral decomposition.
What we gain from this approach (in comparison with the theory of classical
solutions in C1) is remarkable: an existence and uniqueness theorem with maximal
regularity, Theorem 8.32, which will be our main result for this section.
While in the case f = 0 all solutions of (8.41) are automatically C∞ in space and
time, the question of regularity becomes more subtle if f  0. Yet this question is
particularly important for the numerical treatment of (8.41), which we will discuss
in Chapter 9; thus the maximal regularity assertion of Theorem 8.32, together with
the estimates on solutions of (8.41), is of particular importance.
We first derive an abstract result which is formulated for coercive forms. For this,
we need Hilbert space-valued Sobolev spaces on an interval.
Let H be a real separable Hilbert space and let I ⊂ R be a time interval. A
function f : I → H is said to be (weakly) measurable if ( f (·), v)H is measurable for
all v ∈ H. In this case,  f (·)H is also measurable as the supremum of a sequence
of measurable functions (choose a sequence {vn : n ∈ N} which is dense in the unit
sphere of H and fn(t) := |( f (t), vn)|H ). We set
L1(I, H) := #
f : I → H measurable: ∫
I
 f (t)H dt < ∞$
.
Lemma 8.27 For each f ∈ L1(I, H) there exists a unique w ∈ H such that
∫
I
( f (t), v)H dt = (w, v)H, v ∈ H.296 8 Spectral decomposition and evolution equations
We set ∫
I f (t) dt := w. Lemma 8.27 is a simple consequence of the Riesz repre￾sentation theorem. We next observe that the mapping
f ∈ L1(I, H) →
∫
I
f (t) dt ∈ H
is a continuous linear operator. We will usually consider time intervals of the form
I = (0, T), where 0 < T ≤ ∞. We will denote by L2((0, T),H) the space of measurable
functions u : (0, T) → H for which ∫ T
0 u(t)2
H dt < ∞. If one identifies functions
which coincide almost everywhere (as we will always do), then L2((0, T),H) becomes
a Hilbert space with respect to the inner product
( f, g)L2((0,T),H) :=
∫ T
0
( f (t), g(t))H dt.
We also have L2((0, T),H) ⊂ L1((0, T),H) if T < ∞. For these and other, similar
properties, we refer to the exercises. The space L2((0, T),H) can be described easily
via the use of an orthonormal basis {en : n ∈ N} of H.
Lemma 8.28 (a) Let u ∈ L2((0, T),H) and set un(t) := (u(t), en)H . Then
∞
n=1
∫ T
0
|un(t)|2 dt < ∞. (8.42)
(b) Conversely, let un ∈ L2(0, T) satisfy (8.42). Then there exists a uniquely deter￾mined u ∈ L2((0, T),H) for which un = (u(·), en)H for all n ∈ N.
Proof (a) We have
∞
n=1
∫ T
0
|un(t)|2 dt =
∫ T
0
∞
n=1
|(u(t), en)H |
2 dt =
∫ T
0
u(t)2
H dt < ∞.
(b) It follows from (8.42) and the monotone convergence theorem (Theorem A.8,
applied to the partial sums) that 
∞
n=1 |un(t)|2 < ∞ almost everywhere. By al￾tering the functions on a set of measure zero if necessary, we may assume that

∞
n=1 |un(t)|2 < ∞ for all t ∈ (0, T). Hence the series u(t) := 
∞
n=1 un(t)en converges
in H for all t ∈ (0, T). Since (u(t), v)H = 
∞
n=1 un(t)(v, en)H for all v ∈ H, we see that
u is measurable. Finally, we have that ∫ T
0 u(t)2
H dt = ∫ T
0

∞
n=1 |un(t)|2 dt < ∞.
Hence u ∈ L2((0, T),H). 
Let u ∈ L2((0, T),H). We say that a function w ∈ L2((0, T),H) is a weak derivative
of u if
−
∫ T
0
u(t)ϕ(t) dt =
∫ T
0
w(t)ϕ(t) dt for all ϕ ∈ C1
c (0, T). (8.43)
In this case w is unique, and we set u := w. Here we use the notation u since
we imagine the variable t ∈ (0, T) to represent time. Obviously, the notation u8.5 Inhomogeneous parabolic equations 297
could equally be used, as it was in Chapter 5. Note that we have used scalar test
functions (that is, functions in C1
c (0, T) := C1
c ((0, T),R)) for the definition of the weak
derivative. We observe in passing that if (8.43) merely holds for all test functions
ϕ ∈ D(0, T), then a regularization argument (e.g., using mollifiers) shows that it
actually holds for all ϕ ∈ C1
c (0, T).
We now set H1((0, T),H) := {u ∈ L2((0, T),H) : the weak derivative u exists in
L2((0, T),H)}. The following statement can be proved exactly as in the scalar case.
Theorem 8.29 (a) The space H1((0, T),H) is a Hilbert space with respect to the
inner product (u, v)H1((0,T),H) := ∫ T
0 {(u(t), v(t))H + (u(t),v(t))H } dt.
(b) Let u ∈ H1((0, T),H). Then there exists a unique w ∈ C([0, T),H) such that
u(t) = w(t) almost everywhere. Moreover, w(t) = w(0) +
∫ t
0 u(t) dt for all t ∈ [0, T).
In what follows, in accordance with (b) we will always identify a function u ∈
H1((0, T),H) with its continuous representative w. With this identification, we may
write
H1((0, T),H) ⊂ C([0, T])H), (8.44)
and H1((0, T),H) ⊂ C([0, T], H) if T < ∞ (cf. Exercise 8.7). It is also possible to
describe H1((0, T),H) by means of an orthonormal basis of H.
Lemma 8.30 Let {en : n ∈ N} be an orthonormal basis of the Hilbert space H
and let u ∈ L2((0, T),H) as well as un := (u(·), en)H . If un ∈ H1(0, T), n ∈ N, and

∞
n=1
∫ T
0 un(t)
2 dt < ∞, then u ∈ H1((0, T),H) and
u2
L2((0,T),H) =
∞
n=1
∫ T
0
un(t)
2 dt.
This statement follows easily from Lemma 8.28. We next define
H2((0, T),H) := {u ∈ H1((0, T),H) : u ∈ H1((0, T),H)};
it follows from Theorem 8.29(b) that
H2((0, T),H) ⊂ C1([0, T),H). (8.45)
With this background, we now consider a self-adjoint operator A on H, defined
via a form in accordance with (4.19) and (4.20). More precisely, we take a further
Hilbert space V which is compactly and densely imbedded in H and a continuous,
coercive, symmetric bilinear form a : V ×V → R, and assume that A is the operator
on H associated with a. By the spectral theorem (Theorem 4.46) there exist an
orthonormal basis {en : n ∈ N} of H and numbers 0 < λ1 ≤ λ2 ≤ ··· ≤ λn ≤ λn+1
with limn→∞ λn = ∞, such that298 8 Spectral decomposition and evolution equations
D(A) =

u ∈ H :
∞
n=1
λ2
n|(u, en)H |
2 < ∞

, Au =
∞
n=1
λn(u, en)H en, u ∈ D(A).
We also have
V =

u ∈ H :
∞
n=1
λn|(u, en)H |
2 < ∞

und a(u, v) =
∞
n=1
λn(u, en)H (en, v)H
for all u, v ∈ V. Since λ1 > 0 (due to the coercivity of a), the space D(A) is a Hilbert
space with respect to the inner product
(u, v)D(A) =
∞
n=1
λ2
n(u, en)H (en, v)H = (Au, Av)H .
Moreover, with this inner product, A : D(A) → H is a unitary mapping. In what
follows, we will always take this inner product and the corresponding induced norm
on D(A).
Lemma 8.31 Let u ∈ L2((0, T),H) be such that 
∞
n=1 λ2
n
∫ T
0 un(t)
2 dt < ∞, where
un(t) = (u(t), en)H . Then u ∈ L2((0, T),D(A)) and
u2
L2((0,T),D(A)) =
∞
n=1
λ2
n
∫ T
0
|un(t)|2 dt.
Proof It follows from the monotone convergence theorem that series
∞
n=1
λ2
nun(t)
2 < ∞
converges for all t ∈ (0, ∞), after altering u on a set of measure zero if necessary.
Thus u(t) ∈ D(A) for all t ∈ (0, ∞). The sequence eˆn := 1
λn en, n ∈ N, forms an
orthonormal basis of D(A), and (u(t), eˆn)D(A) = λnun(t). The claim now follows
from Lemma 8.28. 
We can now prove the following theorem on existence and uniqueness of solutions
for the inhomogeneous evolution problem.
Theorem 8.32 Let 0 < T ≤ ∞, f ∈ L2((0, T),H) and u0 ∈ V. Then there exists a
unique u ∈ L2((0, T),D(A)) ∩ H1((0, T),H) which solves the initial value problem
u(t) + Au(t) = f (t) for almost every t ∈ (0, T), (8.46a)
u(0) = u0. (8.46b)
Moreover, the following estimates hold:8.5 Inhomogeneous parabolic equations 299
u2
L2((0,T),H) ≤
1
2λ1
u0 2
H +
1
λ2
1
 f 2
L2((0,T),H)
, (8.47)
u2
L2((0,T),H) ≤
1
2
a(u0) + 4  f 2
L2((0,T),H)
, (8.48)
u2
L2((0,T),D(A)) ≤
1
2
a(u0) +  f 2
L2((0,T),H)
. (8.49)
Note that L2((0, T),D(A)) is well defined since D(A) is a Hilbert space. The initial
condition (8.46b) makes sense because u ∈ C([0, T),H). It is remarkable that the
solution u has maximal regularity: both functions u and Au are in L2((0, T),H).
Proof (of Theorem 8.32) Uniqueness: Let u be a solution of (8.46) and consider, for
n ∈ N, the function un(t) := (u(t), en)H . Then un ∈ H1(0, T) and un(t) + λnun(t) =
fn(t) := ( f (t), en)H , un(0) = (u0, en)H . We then deduce, using elementary properties
of linear differential equations, that
un(t) = e−λnt

(u0, en)H +
∫ t
0
eλns fn(s) ds
(8.50)
(see Exercise 5.10). This establishes uniqueness; it will also help us with the proof
of existence.
Existence: We define un by (8.50), and prove using the above lemmata that
u(t) := 
∞
n=1 un(t)en is a solution of (8.46). In doing so, we will treat the u0-term
and the f -term separately.
1st case: f ≡ 0. Then un(t) = e−λnt
(u0, en)H , un(t) = −λnun(t), and so
∞
n=1
∫ T
0
un(t)
2 dt =
∞
n=1
|(u0, en)H |
2
∫ T
0
e−2λnt dt ≤
1
2λ1
u0 2
H .
By Lemma 8.28, there exists a unique u ∈ L2((0, T),H) for which holds that un(t) =
(u(t), en)H . For this function, we have u2
L2((0,T),H) ≤ 1
2λ1 u0 2
H . Since
∞
n=1
∫ T
0
un(t)
2 dt =
∞
n=1
λ2
n
∫ T
0
un(t)
2 dt ≤
∞
n=1
λ2
n|(u0, en)H |
2
∫ T
0
e−2λnt dt
≤
1
2
∞
n=1
λn|(u0, en)H |
2 = 1
2
a(u0),
we see that u ∈ H1((0, T),H) and u2
L2((0,T),H) ≤ 1
2 a(u0). Since
∞
n=1
∫ T
0
λ2
nun(t)
2 dt ≤
1
2
a(u0)
(as follows from the above estimate), by Lemma 8.31 we also have that u ∈
L2((0, T), D(A)) and u2
L2((0,T),D(A)) ≤ 1
2 a(u0).300 8 Spectral decomposition and evolution equations
2nd case: u0 = 0. Then un(t) = ∫ t
0 e−λn(t−s) fn(s) ds. We set gn(t) := e−λnt for
t ≥ 0 and gn(t) = 0 for t < 0; we likewise extend fn by 0 on (−∞, 0]∪(T, ∞). Then
(F gn)(s) = 1
√
2π
∫ ∞
0
e−ist e−λnt dt = 1
is + λn
1
√
2π
.
Since un = fn ∗gn, we have Fun = √
2π F fn · F gn. Now since the Fourier transform
is isometric on L2(R, C), it follows that
∫ T
0
|un(t)|2 dt = Fun 2
L2(R) = 2π F gnF fn 2
L2(R)
≤
1
λ2
n
F fn 2
L2(R) = 1
λ2
n
 fn 2
L2(R)
. (8.51)
Hence, on the one hand,
∞
n=1
∫ ∞
0
|un(t)|2 dt ≤
1
λ2
1
∞
n=1
∫ ∞
0
| fn|
2 dt = 1
λ2
1
 f 2
L2((0,∞),H)
,
and so by Lemma 8.28 there exists a unique u ∈ L2((0, ∞), H) such that un =
(u(·), en)H for all n ∈ N, and we have the estimate
u2
L2((0,∞),H) ≤
1
λ2
1
 f 2
L2((0,∞),H)
.
But on the other hand, by (8.51),
∞
n=1
λ2
n
∫ ∞
0
|un(t)|2 dt ≤  f 2
L2((0,∞),H)
,
and so, by Lemma 8.31, u ∈ L2((0, ∞), D(A)), and we have the estimate
u2
L2((0,∞),D(A)) ≤  f 2
L2((0,∞),H)
.
Finally, un(t) = λnun(t) + fn(t). Hence
∞
n=1
∫ ∞
0
un(t)
2 dt ≤
∞
n=1
∫ ∞
0
(2λ2
nu2
n(t) + 2 fn(t)
2) dt ≤ 4  f 2
L2((0,T),H)
by the above estimate. Thus (by Lemma 8.30) we get u ∈ H1((0, T),H) and
u2
L2((0,T),H) ≤ 4  f 2
L2((0,T),H)
. We thus finally obtain the estimates (8.47), (8.48)
and (8.49), by adding the separate estimates for u0 and f together. 
In order to be able to prove an error estimate for the numerical solution of
problem (8.46) via the finite element method, we require somewhat more regularity8.5 Inhomogeneous parabolic equations 301
of the solutions. We can arrange this by making stronger regularity demands on f
and u0.
Theorem 8.33 Let 0 < T < ∞, let u0 ∈ D(A) be such that Au0 ∈ V and suppose
that f ∈ L2((0, T), D(A)).
(a) Then the solution u of (8.46) is in H1((0, T), D(A)) and we have
u2
L2((0,T),D(A)) ≤
1
2
a(Au0) + 4  f 2
L2((0,T),D(A)).
(b) If in addition f ∈ H1((0, T), H), then u is also in H2((0, T), H), and
u2
L2((0,T),H) ≤ a(Au0) + 8  f 2
L2((0,T),D(A)) + 2 f 2
L2((0,T),H)
.
Moreover, u ∈ C1([0, T],V).
Proof (a) Let u0 ∈ D(A) be such that v0 := Au0 ∈ V and let f ∈ L2((0, T), D(A)).
Then A f ∈ L2((0, T),H). By Theorem 8.32 there exists a unique function v ∈
L2((0, T), D(A)) ∩ H1((0, T),H) such that v + Av = A f , v(0) = v0. Then u = A−1v is
the solution of (8.46). By Theorem 8.32 we also have the estimate
v 2
L2((0,T),H) ≤
1
2
a(v0) + 4A f 2
L2((0,T),H)
.
Since A−1 is an isometric isomorphism from H to D(A) and v ∈ H1((0, T),H), it
follows that also u ∈ H1((0, T), D(A)) and u = A−1v. Moreover,
u2
L2((0,T),D(A)) = v L2((0,T),H) ≤
1
2
a(Au0) + 4  f 2
L2((0,T),D(A)).
(b) Now assume additionally that f ∈ H1((0, T),H). Since u = A−1v and v+Av = A f ,
we have u = A−1v = −v + f ∈ H1((0, T),H). In addition, by the estimate in part (a),
u2
L2((0,T),H) ≤ 2v 2
L2((0,T),H) + 2 f 2
L2((0,T),H)
≤ a(Au0) + 8 A f 2
L2((0,T),H) + 2 f 2
L2((0,T),H)
,
which concludes the proof of the first assertion of b). The following interpolation
result will show that u ∈ C1([0, T],V). 
For the missing assertion in the proof above we need an interpolation result for
mixed Sobolev spaces. We add an integration by parts formula needed in the next
section. We consider a Gelfand triple V → H → V as in Remark 4.51 (page 147).
For that, let V, H be separable Hilbert spaces with inner products (·, ·)H , (·, ·)V and
induced norms ·H , ·V , such that V is continuously imbedded in H and also
dense in H.2 We identify H with a subspace of V
. For a, b ∈ R, a < b, we consider
the mixed Sobolev space
2 It will turn out to be convenient to use this boldface notation here as we will later associate D(A),
V and H to the boldface written spaces.302 8 Spectral decomposition and evolution equations
W(a, b) := L2((a, b), V) ∩ H1((a, b), V
).
Note that H1((a, b), V
) ⊂ C([a, b], V
).
Lemma 8.34 (Integration by parts) (a) W(a, b) ⊂ C([a, b], H)
(b) For u, v ∈ W(a, b) we have
∫ b
a
u(t), v(t) dt = (u(b), v(b))H − (u(a), v(a))H −
∫ b
a
v(t), u(t) dt.
Proof We refer to [25, Thm. XVIII.2] for a proof in the general case. Here we give
a different proof for the special case where the imbedding of V in H is compact (as
is the case in all our examples).
We assume that dim H = ∞ and use the results and notations from Remark 4.51.
Thus (en)n∈N is an orthonormal basis of H, V = {u ∈ H : 
∞
n=1 λn (en, u)
2
H < ∞}
with u2
V = 
∞
n=1 λn (en, u)
2
H , where 1 ≤ λ1 ≤ ··· ≤ λn ≤ λn+1 ≤ ··· and
limn→∞ λn = ∞. Moreover, we may identify the dual space as V = {y = (yn)n∈N ⊂
R :

∞
n=1
y2
n
λn < ∞} with the norm y2
V = 
∞
n=1
y2
n
λn , where the duality is given by
y, u = 
∞
n=1 yn (u, en)H for all y ∈ V and u ∈ V.
(a) Let u ∈ W(a, b) ⊂ C([a, b], V
). Define un(t) := (u(t), en)H . Since en ∈ V
we have un ∈ H1(a, b) and un(t) = (u(t), en)H . Since u ∈ L2((a, b), V) there exists
t0 ∈ (a, b) such that u(t0) ∈ V ⊂ H. By Theorem 5.13 applied to f = g = un one
has for t ∈ [a, b], using the Hölder and Young inequality,
un(t)
2 = un(t0)
2 + 2
∫ t
t0
un(s) un(s) ds = un(t0)
2 + 2
∫ t
t0
un(s)
√
λn

λn un(s) ds
≤ un(t0)
2 + 2
∫ t
t0
un(s)
2
λn
ds1/2 ∫ t
t0
λnun(s)
2 ds1/2
≤ un(t0)
2 +
∫ t
t0
un(s)
2
λn
ds +
∫ t
t0
λnun(s)
2 ds.
Thus 

∞
n=1
un(t)
2 < ∞ and hence u(t) ∈ H as well as
u(t)2
H ≤ u(t0)2
H +
∫ b
a
u(s)2
V ds +
∫ b
a
u(s)2
V ds < ∞
for all t ∈ [a, b].
We show that u ∈ C([a, b], H). Since (u(·), en)H ∈ C([a, b]) for all n ∈ N and
since the set {en : n ∈ N} is total in H, it follows from Lemma 4.34 that u is weakly
continuous. It follows from the estimate above that

u(t)2
H − u(t0)2
H

 ≤




∫ t
t0
u(s)2
V ds




+




∫ t
t0
u(s)2
V ds



 −→ 08.5 Inhomogeneous parabolic equations 303
as t → t0 for all t0 ∈ [a, b].3 Thus u(·)H ∈ C([a, b]). Now it follows from
Theorem 4.33 that u ∈ C([a, b], H).
(b) Let u, v ∈ W(a, b) and set un(t) := (u(t), en)H , vn(t) := (v(t), en)H . Then,
u(t), v(t) = 

∞
n=1
un(t)vn(t). Since
∞
n=1
|un(t)vn(t)| ≤ 
∞
n=1
un(t)
2
λn
1/2 
∞
n=1
λn vn(t)
2
1/2
≤ u(t)2
V + v(t)2
V
it follows that u(·), v(·) ∈ L1((a, b)). Moreover, by Theorem 5.13,
∫ b
a
u(t), v(t) dt =
∫ b
a
∞
n=1
un(t)vn(t) dt =
∞
n=1
∫ b
a
un(t)vn(t) dt
=
∞
n=1

un(b)vn(b) − un(a)vn(a) − ∫ b
a
vn(t) un(t) dt'
= u(b)v(b) − u(a)v(a) − ∫ b
a
v(t) u(t) dt,
which proves the lemma. 
Proof (of the last assertion of Theorem 8.33) We finally have to show that u ∈
C1([0, T], V). Let V = D(A), H = V and V = H (see Exercise 8.12). Then
u ∈ H1((0, T), H) ∩ L2((0, T), D(A)) = W(0, T). Thus the claim follows from
Lemma 8.34. 
We now wish to apply these abstract results to the heat equation with Dirichlet
boundary conditions. Let Ω ⊂ Rd be a bounded open convex set, H = L2(Ω),
V = H1
0 (Ω), and a(u, v) = ∫
Ω ∇u ∇v dx for all u, v ∈ H1
0 (Ω). By Theorem 6.86, the
associated operator on L2(Ω) is given by
D(A) = H2(Ω) ∩ H1
0 (Ω), Av = −Δv.
We note in passing that H2(Ω) ∩ H1
0 (Ω) is a closed subspace of H2(Ω).
Theorem 8.35 Let 0 < T ≤ ∞ and f ∈ L2((0, T), L2(Ω)), u0 ∈ H1
0 (Ω). Then there
exists a unique u ∈ L2((0, T),H2(Ω)) ∩ H1((0, T), L2(Ω)) such that
u(t) = Δu(t) + f (t), t ∈ (0, T), (8.52a)
u(t) ∈ H1
0 (Ω), t ∈ (0, T), (8.52b)
u(0) = u0. (8.52c)
3 We need the absolute values on the right-hand side as t might be smaller than t0.304 8 Spectral decomposition and evolution equations
Note that H1((0, T), L2(Ω)) ⊂ C([0, T),L2(Ω)), so that the initial condition (8.52c)
makes sense. We interpret condition (8.52b) as a Dirichlet boundary condition,
while (8.52a) states that u satisfies the inhomogeneous heat equation.
The proof of Theorem 8.35 follows directly from Theorem 8.32. For an error
analysis in the context of the numerical treatment of the problem via finite elements
we will require the following statement on regularity, which we can make under
somewhat stronger assumptions on the initial value u0 and the inhomogeneity f .
Theorem 8.36 Let 0 < T < ∞. Assume that there exists a constant c > 0 for which
the following statements hold: Let f ∈ L2((0, T), H2
(Ω) ∩ H1
0 (Ω)) and suppose that
u0 ∈ H2(Ω) ∩ H1
0 (Ω), with Δu0 ∈ H1
0 (Ω).
(a) Then the solution u of (8.52) is in H1((0, T), H2
(Ω) ∩ H1
0 (Ω)), and
u2
L2((0,T),H2(Ω)∩H1
0 (Ω)) ≤ c(Δu0 2
H1(Ω)) +  f 2
L2((0,T),H2(Ω))).
(b) If in addition f ∈ H1((0, T); L2(Ω)), then u ∈ H2((0, T); L2(Ω)) and
u2
L2(0,T;L2(Ω)) ≤ c(Δu0 2
H1(Ω)) +  f 2
L2((0,T),H2(Ω)) +  f 2
L2((0,T),L2(Ω))).
Moreover, u ∈ C1([0, T], H1
0 (Ω)).
Theorem 8.36 is a direct consequence of Theorem 8.33 and the observation that
H2(Ω) ∩ H1
0 (Ω) is a closed subspace of H2(Ω) and thus a Hilbert space.
8.6* Space/time variational formulations
The theorems of Banach–Nečas (Theorem 4.27) and Lions (Theorem 4.29), as generalizations of
the Lax–Milgram theorem, permit us to give a variational formulation of time-dependent problems
in space and time, and as such to give an alternative proof of their well-posedness. We will see
later, in particular in the case of parabolic problems, that this formulation is advantageous for the
numerical analysis of such problems.
Let us assume for the meantime that we have an elliptic problem in the spatial variables, that is,
we consider a separable Hilbert space H (such as H = L2(Ω)) and a second separable Hilbert space
V (such as V = H1
0 (Ω)) which is continuously and densely imbedded in H. We denote elements
of the dual space V by  and write
, v := (v), v ∈ V.
As previously, we identify H with a subspace of V by setting , v := (, v)H for every  ∈ H.
We now consider a coercive, but not necessarily symmetric, bilinear form a : V × V → R. Then
(Au)(v) := Au, v := a(u, v), u, v ∈ V, defines a linear operator A : V → V
. (Here we use
the notation A : V → V to distinguish this operator from A : D(A) → H.) We now wish to treat
the parabolic problem
u(t) + Au(t) = f (t), (8.53a)
u(0) = 0, (8.53b)8.6* Space/time variational formulations 305
using a variational approach. Here we assume that T > 0 is a given end time and f (t) ∈ V for
almost every t ∈ I := (0, T) is a given external force, f ∈ L2(I, V
). Then we have the following
theorem on existence and uniqueness of solutions, which goes back to J.-L. Lions.
Theorem 8.37 Given f ∈ L2(I, V
), there exists exactly one solution u ∈ H1(I, V
) ∩ L2(I, V)
of (8.53).
We recall that H1(I, V
) ⊂ C([0, T], V
), cf. Exercise 8.7, which means that u(0) is well
defined. For simplicity we will only consider the homogeneous initial condition u(0) = 0.
Proof (of Theorem 8.37) Let f ∈ L2(I, V
). In order to prove existence of a solution we use the
representation theorem of Lions (Theorem 4.29). For this we define V := L2(I, V) and W :=
{w ∈ C1([0, T], V) : w(T) = 0}; then V is a Hilbert space with respect to the norm v 2
V = ∫ T
0 v(t) 2
V dt, and W ⊂ V is an inner product space for the same norm. Now define a bilinear
form b : V × W → R by
b(v, w) := −
∫ T
0
(v(t), w(t))H dt +
∫ T
0
a(v(t), w(t)) dt; (8.54)
then b(·, w) ∈ V for all w ∈ W. In order to apply Theorem 4.29 we will show that condition (c) of
Remark 4.30 holds. Given w ∈ W, we apply the chain rule from Lemma 8.17 to obtain
b(w, w) =
∫ T
0
−1
2
d
dt w(t) 2
H dt +
∫ T
0
a(w(t), w(t)) dt
= 1
2 w(0) 2
H +
∫ T
0
a(w(t), w(t)) dt ≥ α w2
W.
This shows that the assumptions of Theorem 4.29 are indeed satisfied. Since, given f ∈ L2(I, V
),
the expression , w := ∫ T
0 f (t), w(t) dt defines a continuous linear form  ∈ W
, by Theo￾rem 4.29 there exists some u ∈ V such that b(u, w) = , w for all w ∈ W.
Since u ∈ V = L2(I, V) and A ∈ L(V, V
), we have Au ∈ L2(I, V
). For ϕ ∈ D(I)
(see (6.2)) and ψ ∈ V we can define a test function w ∈ W by w(t) := ϕ(t)ψ; for this function, we
have
∫ T
0
f (t), w(t) dt = b(u, w) = −
∫ T
0
(u(t), ψ)H ϕ(t) dt +
∫ T
0
Au(t), ψ ϕ(t) dt.
Since ψ ∈ V was arbitrary, it follows that
∫ T
0
f (t) ϕ(t) dt = −
∫ T
0
u(t) ϕ(t) dt +
∫ T
0
Au(t) ϕ(t) dt
for all ϕ ∈ D(I). It now follows by definition (see 8.43) that u ∈ H1(I, V
) and
u + Au = f . (8.55)
In order to show that u is indeed a solution, we still need to show that u(0) = 0. To this end take
ϕ ∈ C1([0, T]) such that ϕ(T) = 0 and ϕ(0) = 1, and set w(t) := ϕ(t) u(0), so that w ∈ W.
Replacing f by the expression on the left-hand side of (8.55) yields
∫ T
0
{−(u(t), w(t))H + a(u(t), w(t))} dt = b(u, w) =
∫ T
0
f (t), w(t) dt
=
∫ T
0
 u(t) + Au(t), w(t) dt =
∫ T
0
 u(t), w(t) dt +
∫ T
0
a(u(t), w(t)) dt306 8 Spectral decomposition and evolution equations
= −(u(0), w(0))H −
∫ T
0
u(t), w(t) dt +
∫ T
0
a(u(t), w(t)) dt,
since w(T) = 0. Hence (u(0), w(0))H = 0. But since w(0) = u(0), it follows that u(0) = 0, and we
have proved existence.
To prove uniqueness, we suppose that u ∈ H1(I, V
) ∩ L2(I, V) satisfies u + Au = 0 and
u(0) = 0. Then
∫ τ
0
{u(t), u(t) + a(u(t), u(t))} dt = 0
for all τ ∈ [0, T]. The coercivity of a(·, ·) implies in particular that ∫ τ
0  u(t), u(t)dt ≤ 0 for all
τ ∈ [0, T]. Part (b) of Lemma 8.34 now shows u(τ) H = 0 for all τ ∈ (0, T], whence u = 0. 
Remark 8.38 (a) It follows from Theorem 4.29 that the solution u of (8.53) satisfies the estimate
u L2(I,V ) ≤
1
α  f L2(I,V),
where α > 0 is the coercivity constant of a(·, ·). Observe that the imbedding constant c of
Remark 4.30(c) is equal to 1 here.
(b) There is another approach which can be used to prove Theorem 8.37: one may take V := {v ∈
H1(I, V
) ∩ L2(I, V) : v(0) = 0}, W = L2(I, V) and
b(v, w) :=
∫ T
0
 v(t), w(t) dt +
∫ T
0
a(v(t), w(t)) dt. (8.56)
In this case one simply “multiplies” the equation u + Au = f by w ∈ W (without integrating
by parts in the time variable).
It can be shown that the inf-sup condition for b(·, ·) (cf. Remark 8.39) and the assumptions of
the theorem of Banach–Nečas (Theorem 4.27) all hold. The inf-sup condition is of considerable
importance for the numerical treatment of the problem (cf. Theorem 9.42 on page 357) and
allows an estimate on the norm of the operator f → u in L2(I, V
) → V, to be contrasted with
the norm in L2(I, V
) → L2(I, V) as above. However, the surjectivity, the second condition
in the theorem of Banach–Nečas, is not easy to show. One can either use Lions’ theorem, as
we have done, or else a Galerkin method as in [55, Thm. 5.1] (cf. Section 9.2.1 on page 330).
(c) One can go further and show that for every f ∈ L2(I, V
) and every u0 ∈ H there exists a
unique u ∈ H1(I, V
) ∩ L2(I, V) such that u + Au = f , u(0) = u0. One can even replace
coercivity by the more general H-ellipticity condition (cf. (4.22)).
(d) The above analysis can also be performed in the same way on time-dependent, uniformly
elliptic operators A(t). 
Remark 8.39 Since the actual size of the inf-sup constant plays a major role in the numerical
analysis of the problem, we shall provide a lower bound in terms of the bilinear form in (8.56), with
V and W as in Remark 8.38(b), cf. [59, Prop. 1]. For this we will use a common technique, namely
determining the supremizer in the sense of Remark 4.28(d).
So let 0  v ∈ V. Denote by A∗ : V → V the mapping given by the mapping A∗u, w :=
a(w, u), (i.e., A∗ is associated with the bilinear form a∗ given by a∗(u, w) := a(w, u)). Then A∗
is an isomorphism and so the mapping L2(I, V) # w → A∗w ∈ L2(I, V
) is an isomorphism
too. Thence, there exists a unique zv ∈ L2(I, V) such that A∗zv (t) = v(t) for almost every t ∈ I.
In particular, a(φ, zv (t)) = ( v(t))(φ) =  v(t), φ for all φ ∈ V, and αzv (t) V ≤ v(t)) V  ≤
C zv (t) V for almost every t ∈ I, where α and C are, respectively, the coercivity and continuity
constants of the bilinear form a(·, ·). It follows that zv ∈ W. Now set w := zv + v ∈ W. Then
w2
W =
∫ T
0
w(t) 2
V dt =
∫ T
0
zv (t) + v(t) 2
V dt ≤ 2
∫ T
0
( zv (t) 2
V + v(t) 2
V )dt8.6* Space/time variational formulations 307
≤
2
α
∫ T
0
 v(t)) 2
V dt + 2v 2
L2(I,V ) = 2
α  v 2
L2(I,V) + 2v 2
L2(I,V )
≤ 2 max{α−1
, 1} v 2
V < ∞.
Now we also have, by definition of zv and Lemma 8.34,
b(v, w) =
∫ T
0
{v(t) w(t) + a(v(t), w(t))}dt
=
∫ T
0
{v(t), v(t) +  v(t), zv (t) + a(v(t), zv (t)) + a(v(t), v(t))} dt
= 1
2 v(T) 2
H +
∫ T
0
{a(zv (t), zv (t)) +  v(t), v(t) + a(v(t), v(t))} dt
= v(T) 2
H +
∫ T
0
{a(zv (t), zv (t)) + a(v(t), v(t))} dt.
The coercivity of a(·, ·) now implies that
b(v, w) ≥ α
∫ T
0
{ zv (t) 2
V + v(t) 2
V }dt
≥ α min{C−2
, 1}
∫ T
0
{v(t) 2
V  + v(t) 2
V }dt = α min{C−2
, 1} v 2
V
≥ α min{C−2
, 1} 1
√
2
min{
√
α, 1} v V wW.
Hence β˜ := α√
2 min{C−2
,
√
α, 1} is a lower bound on the inf-sup constant β, that is, β ≥ β >˜ 0. In
the case of the heat equation, with the norm v 2
V :=  v 2
L2(I,V) + v 2
L2(I,V ) + v(T) 2
H and the
energy norm φ2
V = a(φ, φ) on V, one can actually show that β = 1 [60], which is particularly
convenient for numerical purposes.
In [55, Thm. 5.1] one can also find a bound on the inf-sup constant, which however degenerates
as T → ∞, whereas β˜ is independent of the endpoint T. 
Remark 8.40 (Linear transport problems) Problems of the form u + b · grad u + cu = f in I × Ω,
equipped with suitable initial and boundary values, can also be treated using space/time variational
methods as here. In [19] the well-posedness of a suitable variational formulation u ∈ V is proved,
where b(u, w) = f (w), V = L2(I, L2(Ω)), the bilinear form b is given by
b(v, w) :=
∫ T
0
(v, − w − b · grad w + w(c −∇· b))L2(Ω) dt
and W is chosen appropriately. This uses the theorem of Banach–Nečas, Theorem 4.27. More is
shown: for certain special norms the inf-sup constant β equals 1. 
Remark 8.41 (Wave equation) An analogous approach can be used to handle what is known as a
very weak variational formulation of the wave equation u − Δu = f in I × Ω subject to suitable
initial and boundary conditions. Here as well one can prove well-posedness and β = 1 (for suitable
norms) if V := L2(I, L2(Ω)),
b(v, w) :=
∫ T
0
(v, w − Δw)L2(Ω) dt,
and the test space W is chosen appropriately, cf. [48, §9]. 308 8 Spectral decomposition and evolution equations
8.7* Comments on Chapter 8
In the present chapter we have described the evolution of physical systems. The essence of our
principal theorems (Theorem 8.3 for the result in abstract form, with concrete realizations in Sec￾tions 8.2 and 8.3) is that given any initial state the evolution of the system is determined for all time.
It is a far-reaching physical and philosophical question whether such a form of determinism cor￾rectly describes nature. Many fundamental physical laws can be formulated as evolution equations.
Laplace in his works motivated such evolution equations by conceiving of an exterior intelligence
as driving force, an intelligence which has become known as Laplace’s demon. He wrote, “Une
intelligence qui, à un instant donné, connaîtrait toutes les forces dont la nature est animée et la
situation respective des êtres qui la compose embrasserait dans la même formule les mouvements
des plus grands corps de l’univers et ceux du plus léger atome; rien ne serait incertain pour elle,
et l’avenir, comme le passé, serait présent à ses yeux” [45, pages 32–33] (“An intelligence which
at any given moment knew all forces which animate nature and the respective state of all beings
which compose it, could express in the same formula the motions of the greatest heavenly bodies
and those of the lightest atom; nothing would be uncertain for it; and the future, like the past, would
be the present in its eyes.”)
In our context, we know that such explicit formulae can only be derived in the simplest geometric
situations; and we must content ourselves with numerical approximations, the topic of the next
chapter. Regarding the philosophical background for the mathematics of evolution equations, we
refer to the epilog in the book of Engel and Nagel [27].
8.8 Exercises
Exercise 8.1 Let w(t) = 
∞
n=1 wn(t)en, where wn is given by (8.33). Show by direct
calculation that a(w(t)) + w(t)2
H = a(u0) + u1 2
H for all t ≥ 0.
Exercise 8.2 (Separated variables) Let A be an operator on a real Hilbert space H
and let f ∈ D(A), f  0.
(a) Show that there exists a unique solution u(t) = Au(t), t ∈ (0, T) of the form
u(t) = v(t) f , where v ∈ C1(0, T), v  0, if f is an eigenvector of A.
(b) Do the same for the equation u(t) = Au(t), t ∈ (0, T), where we now assume that
v ∈ C2(0, T).
Exercise 8.3 Let Ω be a bounded C1-domain and suppose that b : ∂Ω → [0, ∞) is
bounded and Borel measurable. Let u0 ∈ H1(Ω) ∩ H2
loc(Ω) be such that Δu0 ∈ L2(Ω)
and ∂u
∂v + b(z)(Tu)(z) = 0, and let u1 ∈ H1(Ω). Show that there exists a unique
function w ∈ C2(R+, L2(Ω)) such that w(t) ∈ H1(Ω) ∩ H2
loc(Ω), Δw(t) ∈ L2(Ω),
t ≥ 0 and
w(t) − Δw(t) = 0, t ≥ 0,
∂w(t)
∂ν + bT(w(t)) = 0,
w(0) = u0, w(0) = u1.8.8 Exercises 309
Exercise 8.4 Let b : ∂Ω → [0, ∞) be bounded and Borel measurable. Show that
λb
n ≤ λD
n , n ∈ N.
Exercise 8.5 Let I ⊂ R be an interval and H a separable Hilbert space.
(a) Let u ∈ L1(I, H). Show that there exists a unique element ∫
I u(t) dt of H such
that ∫
I
(u(t), v)H dt =
∫
I u(t) dt, v
H
for all v ∈ H.
(b) Show that "
"
∫
I u(t) dt"
"
H ≤ ∫
I u(t)H dt.
(c) Show that L1(I, H) is a vector space and that the mapping u → ∫
I u(t) dt :
L1(I, H) → H is linear.
(d) If 0 < T < ∞, then L2((0, T),H) ⊂ L1((0, T),H).
Exercise 8.6 Let H1, H2 be separable Hilbert spaces and let B : H1 → H2 be
continuous and linear. Show that u → B ◦ u defines a continuous linear mapping of
L2((0, T),H1) into L2((0, T),H2), 0 < T ≤ ∞, and that the mapping is unitary if B is
unitary.
Exercise 8.7 Let H be a separable Hilbert space and 0 < T ≤ ∞.
(a) Let u ∈ H1((0, T),H) be such that u(t) = 0 almost everywhere. Show that there
exists x ∈ H such that u(t) = x for almost every t ∈ (0, T).
Suggestion: Use the scalar result in Lemma 5.7.
(b) Letv ∈ L2((0, T),H) and x0 ∈ H. Define u : (0, T) → H by u(t) = x0+
∫ t
0 v(s) ds.
Show that u ∈ H1((0, T),H) and u = v. Also show that u ∈ C([0, T),H) if T < ∞.
(c) Let u ∈ H1((0, T),H). Show that u ∈ C([0, T),H) and u(t) = u(0) +
∫ t
0 u
(s) ds
for all t ∈ (0, T).
(d) Let u ∈ H1((0, T),H) be such that u ∈ C([0, T),H). Show that then u ∈
C1([0, T),H).
Exercise 8.8 Let {en : n ∈ N} be an orthonormal basis of the Hilbert space H and let
(λn)n∈N be a monotonically increasing sequence such that 0 ≤ λ1, limn→∞ λn = ∞.
Define T(t) : H → H, t ≥ 0, by T(t)x = 
∞
n=1 e−λnt
(x, en)H en. Show:
(a) T(t) ∈ L(H), T(t + s) = T(t)T(s), t, s ≥ 0, T(0) = I, limh→0 T(t + h)x = T(t)x
for all x ∈ H.
Suggestion: consider the proof of Theorem 8.3 for the last property.
(b) The limit limt↓0
1
t (T(t)x − x) =: Bx exists for each x ∈ H if and only if

∞
n=1 λ2
n(x, en)
2
H < ∞.
Suggestion: Let 
∞
n=1λ2
n(x, en)
2
H < ∞, and set y = −

∞
n=1λn(x, en)H en. Show
that T(t)x − x = ∫ t
0 T(s)y ds.
Exercise 8.9 Let H be a separable Hilbert space, λ ∈ R, T > 0, f ∈ L2((0, T),H),
and x0 ∈ H. Show that there exists a unique function u ∈ H1((0, T),H) such that
u(t) + λu(t) = f (t) almost everywhere in (0, T), u(0) = x0, and that this function is
given by u(t) = e−λt {x0 +
∫ t
0 eλs f (s) ds}.310 8 Spectral decomposition and evolution equations
Exercise 8.10 Take the assumptions made at the beginning of Section 8.4.
(a) Show that D∞(A) := 
k ∈N
D(Ak ) is dense in H.
Here D(Ak ) := {u ∈ D(Ak−1) : Ak−1u ∈ D(A)}, Ak := A(Ak−1u) for k ≥ 2.
(b) Let u0, u1 ∈ D∞(A) and f = 0. Show that the solution w of (8.39) is in
C∞([0, T],V).
(c) Let m ∈ N0, f ∈ Cm([0, T], H), where C0([0, T], H) = C([0, T], H). Define
wn(t) := 1
√
λn
∫ t
0 sin(s
√
λn) fn(t − s) ds if λn > 0 and wn(t) := ∫ t
0 s fn(t − s) ds
if λn = 0, where fn(t) = ( f (t), en)H . Show that 
∞
n=1 w(k) n en converges in V
uniformly with respect to t ∈ [0, T], k = 0,..., m.
Deduce from this that w(·) = 
∞
n=1 wn(·) en ∈ Cm([0, T],V).
(d) Let u0 = u1 = 0 and f ∈ C2([0, T], H). Show that (8.39) has a unique solution w.
(e) Let u0, u1 ∈ D∞(A) and f ∈ C∞([0, T], H). Show that (8.39) has a unique solution
w and that w ∈ C∞([0, T],V).
Exercise 8.11 Let Ω ⊂ Rd be bounded, open and convex, and suppose that u0 ∈
C2(Ω), u1 ∈ C1(Ω) are such that u0|∂Ω = u1|∂Ω = 0, and f ∈ C2([0, T], L2(Ω)).
(a) Show that under these assumptions the problem
w(t) = Δw(t) + f (t), t ∈ [0, T],
w(t) ∈ H2(Ω) ∩ H1
0 (Ω), t ∈ [0, T],
w(0) = u0, w(0) = u1,
has a unique solution w ∈ C2([0, T], L2(Ω)).
(b) Assume that u0 ∈ C4(Ω), u0|∂Ω = Δu0|∂Ω = 0 and u1 ∈ C3(Ω), u1|∂Ω =
Δu1|∂Ω = 0 as well as f ∈ C3([0, T], L2(Ω)). Show that the solution w of (a) is in
C3([0, T], H1
0 (Ω)).
(c) Asume that u0 ∈ C5(Ω)such that u0|∂Ω = Δu0|∂Ω = Δ2u0|∂Ω = 0 and u1 ∈ C4(Ω)
such that u0|∂Ω = Δu0|∂Ω = 0 as well as f ∈ C4([0, T], L2(Ω)). Show that now
w ∈ C4([0, T], H1
0 (Ω)).
(d) Assume that u0, u1 ∈ C∞(Ω) such that Δku0|∂Ω = Δku1|∂Ω = 0 for all k ∈ N0
and let f ∈ C∞([0, T], L2(Ω)). Show that in this case w ∈ C∞([0, T], L2(Ω)).
Exercise 8.12 Let H be a real Hilbert space with orthonormal basis (en)n∈N. Let
λn ∈ (0, ∞), 0 < λ1 ≤ ··· ≤ λn ≤ λn+1, limn→∞ λn = ∞. Consider the Hilbert
spaces
V :=

u ∈ H :
∞
n=1
λn (u, en)
2
H < ∞
!
with scalar product (u, v)V := 

∞
n=1
λn (u, en)H (en, v)H and
D :=

u ∈ H :
∞
n=1
λ2
n (u, en)
2
H < ∞
!8.8 Exercises 311
with scalar product (u, v)D := 

∞
n=1
λ2
n (u, en)H (en, v)H . Thus D → V → H.
Show that H = D
, where the duality is given by
f, u =
∞
n=1
λn (u, en)H (en, f )H
for all u ∈ D, f ∈ H. In particular  f H =  f D.Chapter 9
Numerical methods
Thus far we have met and modeled a number of partial differential equations, con￾sidered elementary solution methods for them and finally developed mathematical
theories to handle them. Everything up until now has been “analytic” in nature, that
is, we can perform everything on paper, and the solutions we have constructed satisfy
their respective partial differential equations “exactly” in the desired sense, even if
we first had to find a suitable notion of solution (classical, strong or weak) for each
problem.
But such methods are sometimes not enough. For example, we have proved
existence and uniqueness of a (weak) solution to the Poisson equation with inho￾mogeneous Dirichlet boundary conditions, but we can only find an explicit formula
for the solution on a few very special domains. Even disks and rectangles were
treated using Fourier series, which after all can only be calculated approximately. If
one wants a representation of the solution in any other even slightly more complex
situation, one needs approximation procedures to calculate it.
In such cases, the use of computers in combination with solution methods from
numerical mathematics can help. Finite differences and finite elements are two very
frequently used methods, which we shall introduce in this chapter.
Chapter overview
9.1 Finite differences for elliptic problems ....................... 315
9.2 Finite elements for elliptic problems ........................ 330
9.3* Extensions and generalizations ........................... 355
9.4 Parabolic problems ................................. 360
9.5 The wave equation ................................. 379
9.6* Comments on Chapter 9 .............................. 406
9.7 Exercises ...................................... 408
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4_9
313314 9 Numerical methods
What are numerical methods?
Numerical methods have the goal of determining an approximate solution to a
given problem, via the application of an algorithm. The Swiss mathematician Heinz
Rutishauser is considered one of the pioneers of modern numerical mathematics,
and also of computer science. In his words, “numerical mathematics is concerned
with finding a computational way to solve mathematically formulated problems”.
The desired accuracy of the approximation is often determined by the real problem
being studied. Think, for example, of the solution of a partial differential equation
which describes the statics of some structural element, say of a building, and the
structural engineer requires that the approximate solution should differ from the exact
solution by at most 5%. Such tolerances are also often determined by approximations
or simplifications already contained in the model. In such cases it is sensible to
determine an approximate solution in such a way that the error roughly corresponds
to the error introduced by the model; anything beyond that could well be wasted
effort.
At this juncture mathematicians are required, since a number of mathematical
questions arise directly in the context of any such numerical method; in particular:
• How accurate is the approximate solution; can one estimate the error precisely?
Of course the exact solution is unknown; nevertheless, the distance between it
and the approximation needs to be estimated.
• How quickly can one obtain the approximate solution, and how much effort
(computation time) is required to calculate it? This is known as the complexity
of the method; if the complexity is low, then we speak of an efficient method.
• What is the effect of unavoidable errors (such as roundoff errors or inaccuracies
in the measurement of the input quantities) on the result? If small input errors
result in small output errors, then we call the algorithm stable. This is clearly
closely related to the well-posedness of a problem, or to be more precise, to the
continuous dependence on the data.
• How does the method behave when essential parameters such as the domain, the
coefficients in the partial differential equation or the right-hand side are altered?
Is the algorithm robust to such changes?
These are classical questions in numerical mathematics, for short, numerics. To
answer them, one often falls back on exactly those functional analytic tools that were
introduced in the previous chapters. Of course the actual construction of suitable
numerical methods is another matter, as is their practical implementation, that is,
their programming. Here one requires input from computer science and information
technology, in areas such as compiler construction, algorithm development, computer
architecture, software engineering and management, and so on. It would go well
beyond the scope of this book to discuss these latter topics; our goal here is to
discuss the mathematical aspects of the numerical methods we shall discuss. For
their implementation we will use a programming language chosen more or less
arbitrarily, or a corresponding numerical library.9.1 Finite differences for elliptic problems 315
9.1 Finite differences for elliptic problems
Difference methods (finite differences) are perhaps the simplest method available
for solving partial differential equations numerically. They are especially suitable
when the domain is rectangular. But they can also be used in combination with
discrete maximum principlesto yield existence results for classical solutions of partial
differential equations, which can be derived as a consequence of the convergence of
the method. Here we will introduce the method and its essential properties; but we
will also point out the limits of its applicability.
We will first decribe the finite difference method (FDM) via a simple example in
one space dimension (that is, a boundary value problem for an ordinary differential
equation), as this case is significantly easier to understand, and then afterwards
we consider the two-dimensional analog, that is, partial differential equations. In
Section 9.4.1 we will then describe the solution of parabolic problems using FDMs
and Section 9.5.1 is devoted to FDMs for the wave equation.
9.1.1 FDM: the one-dimensional case
We start by considering the boundary value problem
−u(x) = f (x), x ∈ (0, 1), (9.1a)
u(0) = u(1) = 0. (9.1b)
Before coming to the FDM we first wish to summarize a few known theoretical
results for the problem (9.1) which we will require in this section. We will also
write (9.1) in the form Lu = f , where L is the differential operator defined by
(9.1); later, we will define a discrete approximation Lh of L. We will use the norm
v ∞ := supx∈[0,1] |v(x)| for v ∈ C([0, 1]).
Boundary value problems
Existence and uniqueness of solutions of (9.1) were proved in Section 5.2.1; here we
wish to write the solutions in terms of the corresponding Green’s function.
Lemma 9.1 The following statements hold.
(a) For every f ∈ C([0, 1]) there exists exactly one solution u ∈ C2([0, 1]) of (9.1).
This is given by
u(x) =
∫ 1
0
G(x, s) f (s) ds, (9.2)316 9 Numerical methods
where G is the Green’s function
G(x, s) := min{x, s}(1 − max{x, s}) =

s(1 − x), if 0 ≤ s ≤ x ≤ 1,
x(1 − s), if 0 ≤ x ≤ s ≤ 1. (9.3)
(b) Shift theorem: if f ∈ Cm([0, 1]), m ≥ 0, then u ∈ Cm+2([0, 1]).
(c) Monotonicity: if f ≥ 0, f ∈ C([0, 1]), then u ≥ 0.
(d) Stability (maximum principle): for every f ∈ C([0, 1]), we have u∞ ≤ 1
8  f ∞.
Proof (a) If we set F(s) := ∫ s
0 f (t) dt, then we may write the solution of (9.1a)
as u(x) = c1 + c2 x −
∫ x
0 F(s) ds, where c1, c2 ∈ R are constants of integration. We
now invoke the boundary conditions (9.1b) to obtain 0 = u(0) = c1, 0 = u(1) =
c2 −
∫ 1
0 F(s) ds. Next, by integration by parts, we have
∫ x
0
F(s) ds = x F(x) − ∫ x
0
s f (s) ds =
∫ x
0
(x − s) f (s) ds
and thus
u(x) = c1 + c2 x −
∫ x
0
F(s) ds = x
∫ 1
0
(1 − s) f (s) ds −
∫ x
0
(x − s) f (s) ds
=
∫ x
0
%
x(1 − s)−(x − s)
&
f (s) ds +
∫ 1
x
x(1 − s) f (s) ds
=
∫ x
0
s(1 − x) f (s) ds +
∫ 1
x
x(1 − s) f (s) ds =
∫ 1
0
G(x, s) f (s) ds,
that is, (9.2). This proves uniqueness of the solution u ∈ C2([0, 1]) of (9.1)). It is
easily checked that the function defined by (9.2) is indeed a solution.
(b) This follows directly from (9.2).
(c) Since G(x, s) ≥ 0, this follows from (a).
(d) It also follows from the representation of u in (a) and the fact that G ≥ 0 that
|u(x)| ≤ ∫ 1
0
G(x, s) | f (s)| ds ≤  f ∞
∫ 1
0
G(x, s) ds
=  f ∞
∫ x
0
s(1 − x) ds +
∫ 1
x
x(1 − s) ds'
=  f ∞

(1 − x)
1
2
x2 + x(
1
2 − x +
1
2
x2)

=  f ∞
 1
2
x(1 − x)

,
that is, u∞ ≤  f ∞ maxx∈[0,1]

1
2 x(1 − x)

= 1
8  f ∞. 9.1 Finite differences for elliptic problems 317
Discretization
We now wish to solve (9.1) approximately. Of course, for the simple problem (9.1)
we already know the exact solution, given by (9.2); thus we do not actually need any
approximation. However, (9.1) is to be understood as a simple model example to
help understand the FDM; the explicit solution formula will assist when determining
the error.
We first replace the interval [0, 1] (which consists of uncountably many points
x ∈ [0, 1]) by a discrete (and finite) set of points, or nodes. The simplest way to do
so is to distribute these points uniformly in [0, 1], that is,
xi := ih, h := 1
N + 1
, i = 0,..., N + 1, N ∈ N. (9.4)
We call h > 0 the mesh size and
Ωh := {xi : 0 ≤ i ≤ N + 1}, Ω˚ h := Ωh \ {0, 1}, (9.5)
an equidistant grid (or uniform mesh), consisting of uniformly distributed nodes xi.
Clearly, one cannot consider derivatives of functions on such a discrete set as Ωh;
hence we replace derivatives by difference quotients. For example, we may replace
the second derivative by the central difference quotient
Δhv(x) := 1
h2

v(x + h) − 2v(x) + v(x − h)

. (9.6)
The following error estimate is well known in analysis, but since it will play an
important role in what follows, we include the proof.
Lemma 9.2 If v ∈ C4([0, 1]), then for all x ∈ [h, 1 − h] we have
|v(x) − Δhv(x)| ≤ h2
12 v(4)
∞. (9.7)
Proof We consider a Taylor expansion of v(x ± h) about x and obtain
v(x ± h) = v(x) ± hv
(x) +
h2
2 v(x) ± h3
6 v(x) +
h4
24 v(4)
(ξ±),
where ξ+ ∈ (x, x + h) and ξ− ∈ (x − h, x), respectively. Summing the two expansions
yields
v(x + h) + v(x − h) = 2v(x) + h2v(x) +
h4
24 {v(4)
(ξ+) + v(4)
(ξ−)},
from which the claim follows. 318 9 Numerical methods
Observe that the restriction x ∈ [h, 1 − h] is necessary in order to prevent Δhv
from involving points outside the interval [0, 1]. The above estimate is thus only valid
in the interior mesh Ω˚ h.
Error analysis
The final step in the construction of our FDM consists of restricting (9.1a) to Ωh,
replacing the second derivative u by Δh and using the boundary conditions. This
yields an approximation uh = (ui)i=1,...,N ∈ RN , ui ≈ u(xi) via the following system
of equations:
u0 = uN+1 = 0, (9.8a)
− ui+1 + 2ui − ui−1 = h2 fi, 1 ≤ i ≤ N, (9.8b)
where fi := f (xi). In matrix form this may be written as Ahuh = h2 fh =: ˜fh, where
fh := ( fi)i=1,...,N ∈ RN and
Ah :=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
2 −1 0
−1 2 . . .
. . . . . . . . .
. . . 2 −1
0 −1 2
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
∈ RN×N . (9.9)
For the analysis of the approximation obtained in this fashion, the following
definitions will be useful.
Definition 9.3 (a) We call any mapping wh : Ωh → R a grid function. The set of all
grid functions will be denoted by Vh, and we set V0
h := {wh ∈ Vh : wh(0) = wh(1) =
0}.
(b) We define the discrete operator Lh : V0
h → V0
h by
(Lhwh)i := −Δhwh(xi), 1 ≤ i ≤ N, (Lhwh)0 := (Lhwh)N+1 := 0,
for wh ∈ V0
h . 
With this definition, (9.8) is equivalent to the problem
Find uh ∈ V0
h such that Lhuh = fh, fh := ( f (xi))1≤i≤N .
For this problem, we define discrete Green’s functions Gk ∈ V0
k , 1 ≤ k ≤ N, as
solutions of the discrete problem
LhGk = δk
, 1 ≤ k ≤ N, (9.10)9.1 Finite differences for elliptic problems 319
where δk ∈ V0
h is defined by
δk (xi) := δi,k =

1, if i = k,
0, otherwise,
for all 1 ≤ i ≤ N. We will now establish a relationship between the Green’s function
G from (9.3) and its discrete counterparts.
Lemma 9.4 We have Gk (xi) = h G(xi, xk ) for all 1 ≤ i, k ≤ N.
x
xk (1 − xk )
G (xk , ·)
xk 1
Fig. 9.1 Green’s function as a function of its second argument, for fixed xk ∈ Ωh.
Proof Fix a grid point xk ∈ Ωh, then on the one hand we have, for all 1 ≤ i ≤ N,
i  k, (LhG(·, xk ))(xi) = G(·, xk )(xi) = 0, since G, in accordance with (9.3), is a
linear function of each of its arguments when the other is fixed, cf. Figure 9.1. On
the other hand, for i = k, by definition
(LhG(·, xk ))(xk ) =

− 1
h2

{G(xk+1, xk ) − 2G(xk, xk ) + G(xk−1, xk )}
=

− 1
h2

{xk (1 − xk+1) − 2xk (1 − xk ) + xk−1(1 − xk )}
=

− 1
h2

{k h(N + 1 − k − 1)h − 2k h(N + 1 − k)h
+ (k − 1)h(N + 1 − k)h}
= (−1)
#
(N + 1 − k)(k − 2k + k − 1) + k
$
= N + 1 = 1
h
;
hence, by (9.10), h(LhG)(·, xk ))(xi) = δi,k = (LhGk )i, for all 1 ≤ i ≤ N, which
proves the claim. 
We shall now investigate the questions of existence and uniqueness of solutions
of the discrete problem; to this end, we will once again begin by defining a couple
of relevant quantities.320 9 Numerical methods
Definition 9.5 Given vh, wh ∈ Vh, we define the discrete inner product
(vh, wh)h := h
N
+1
i=0
ci vi wi, (9.11)
where c0 := cN+1 := 1
2 , ci := 1, i = 1,..., N and vi := vh(xi), wi := wh(xi), as well
as the discrete norm vh h := 
(vh, vh)h. 
Clearly, (vh, wh)h corresponds to the composite trapezoidal rule for approximating
the exact inner product (v, w)L2(0,1) := ∫ 1
0 v(x) w(x) dx if vh, wh are interpolations of
the functions v, w ∈ C([0, 1]) in Ωh. That is, we have (vh, wh)h = Th(v w) with the
usual definition of the composite trapezoidal rule
Th(v) := h

1
2
v(0) +

N
i=1
v(xi) +
1
2
v(1)

.
The following stability analysis is also referred to as an energy method. The discrete
maximum norm is defined by
vh h,∞ := max
0≤i≤N+1
|vh(xi)|, vh ∈ Vh;
v h,∞ may be defined likewise for v ∈ C([0, 1]).
Lemma 9.6 The following statements hold.
(a) The operator Lh is symmetric: (Lhvh, wh)h = (vh, Lhwh)h for all vh, wh ∈ V0
h .
(b) The operator Lh is positive definite, that is, (Lhvh, vh)h ≥ 0 for all vh ∈ V0
h ; and
moreover (Lhvh, vh)h = 0 if and only if vh = 0.
(c) Discrete maximum principle: if Lhuh = fh, then uh h,∞ ≤ 1
8  fh h,∞.
(d) The operator Lh is bounded: (Lhvh, wh)h ≤ 4vh h wh h for all vh, wh ∈ V0
h .
(e) The eigenvectors and eigenvalues of Lh are given by zk
h = (zk
j )j=0,...,N+1, where
zk
j = √
2h sin(jkhπ) and λk = 4
h2 sin2( khπ
2 ), k = 1, ..., N.
Remark 9.7 Together, these assertions guarantee the well-posedness of the discrete
problem (9.8): (a) and (b) together imply the existence and uniqueness of solutions,
(c) the continuous dependence of the solution on the data. In other words, (c) is
a stability result. Observe that the norm in (c) is not induced by the inner product
(·, ·)h; however, since all norms in finite-dimensional space are equivalent, it makes
no difference. 
Proof (of Lemma 9.6) (a) Since vh, wh ∈ V0
h , we have v0 = vN+1 = w0 = wN+1 = 0;
and we also define v−1 := w−1 := 0. Then, by summation by parts,
(Lhvh, wh)h = h
N
+1
i=0
ci
1
h2

− vi+1 + 2vi − vi−1

wi9.1 Finite differences for elliptic problems 321
=

− 1
h
 
N
i=1

(vi+1 − vi)−(vi − vi−1)

wi
= 1
h

N
i=0
(vi+1 − vi) (wi+1 − wi).
Again summing by parts, we obtain
(Lhvh, wh)h =

− 1
h
 
N
i=1
vi

(wi+1 − wi)−(wi − wi−1)

= (Lhwh, vh)h.
(b) By (a) we have (Lhvh, vh)h = 1
h

N
i=0(vi+1 − vi)
2 ≥ 0 for all vh ∈ V0
h . If
(Lhvh, vh)h = 0, then vi = vi+1 for all i. But since v0 = vN+1 = 0 (as vh ∈ V0
h ),
it follows that vh ≡ 0.
(c) Using the representation of uh = L−1
h fh in terms of the discrete Green’s
functions and the fact that G(x, y) ≥ 0, we have
|u(xk )| ≤ 
N
i=1
Gi
(xk )| f (xi)|
≤  f h,∞

N
i=1
h G(xk, xi) =  f h,∞
 1
2
xk (1 − xk )

(9.12)
(cf. Exercise 9.2), which yields the claim.
(d) As in (a) we have (Lhvh, wh)h = 1
h

N
i=0(vi+1 − vi) (wi+1 − wi), where we
have used the homogeneous boundary values. Now we apply Hölder’s and Young’s
inequalities to obtain
(Lhvh, wh)h ≤
1
h

N
i=0
(vi+1 − vi)
2
1/2 
N
i=0
(wi+1 − wi)
2
1/2
≤ 4

1
h

N
i=0
v2
i
1/2 
1
h

N
i=0
w2
i
1/2
= 4 vh h wh h,
as claimed.
(e) By definition, we have
(Lh zk
h)j = 1
h2 (−zk
j+1 + 2zk
j − zk
j−1)
= h−3/2

− sin 
k(j + 1)πh
	
+ 2 sin 
k jπh
	
− sin 
k(j + 1)πh
	
.
Since sin 
k(j ± 1)πh
	
= sin(k jπh) cos(kπh) ± cos(k jπh)sin(kπh), we have322 9 Numerical methods
(Lh zk
h)j = h−3/2 sin(k jπh)(2 − 2 cos(kπh)) = h−3/2 sin(k jπh) 4 sin2
 kπh
2

= 4
h2 sin2
 kπh
2
 √
2h sin(k jπh) = λk (zk
h)j,
for all j = 1,..., N, which completes the proof. 
Theorem 9.8 (Discrete positivity) Suppose that f ∈ C([0, 1]) satisfies f (x) ≥ 0 for
all x ∈ [0, 1], and let uh be the solution of (9.8). Then ui ≥ 0 for all 0 ≤ i ≤ N + 1.
Proof Since LhGk = δk , we define uh ∈ V0
h by uh := 
N
k=1 f (xk ) Gk . Then, firstly,
we have u0 = uN+1 = 0, which is the conclusion for i ∈ {0, N + 1}. Moreover,
Lhuh = 
N
k=1 f (xk ) LhGk = 
N
k=1 f (xk ) δk = fh, meaning that uh, thus defined, is
indeed the unique discrete solution. Since G(x, y) ≥ 0 and Gk (xi) = h G(xi, xk ) ≥ 0,
the assumption that f (x) ≥ 0 immediately implies ui ≥ 0 for all 1 ≤ i ≤ N. 
Definition 9.9 Given f ∈ C([0, 1]), let u ∈ C2([0, 1]) be the unique solution of
Lu = f . Then the local truncation error τh ∈ V0
h is defined by
τh(xi) := (Lhu)(xi) − f (xi), 1 ≤ i ≤ N.
We call eh := u−uh the discretization error. Finally, we say that a family {Lh}h>0 of
discrete operators Lh : V0
h → V0
h is of consistency order p ∈ N if τh h,∞ = O(hp)
as h → 0+. 
In our case, the above defined Lh is of consistency order 2 as we shall see next.
Lemma 9.10 For all f ∈ C2([0, 1]) we have τh h,∞ ≤ h2
12  f ∞.
Proof It follows from Lemma 9.1(b) that u ∈ C4([0, 1]), and so, by Lemma 9.2,
τh h,∞ = max
0≤i≤N+1
|(Lhu)(xi) − f (xi)| = max
0≤i≤N+1
| − Δh(xi) + u(xi)|
≤
h2
12 u(4)
∞ = h2
12  f ∞,
since −u(x) = f (x) for all x ∈ (0, 1). 
We will also need the following relationship between the local truncation error
and the discretization error:
Lheh = Lhu − Lhuh = Lhu − fh = τh. (9.13)
We can now formulate and prove a convergence theorem for the FDM for the
problem (9.1).9.1 Finite differences for elliptic problems 323
Theorem 9.11 (Convergence of the FDM) If f ∈ C2([0, 1]), then
u − uh h,∞ ≤
h2
96  f ∞. (9.14)
Proof It follows from the discrete maximum principle (Lemma 9.6(c)) that, for the
equation Lhuh = fh, we have the estimate uh h,∞ ≤ 1
8  fh h,∞. If we apply this
to (9.13), then we obtain eh h,∞ ≤ 1
8 τh h,∞. Combined with Lemma 9.10, this
yields eh h,∞ ≤ 1
8 τh h,∞ ≤ h2
96  f h,∞. 
Notice in particular that the error estimate follows from the consistency and the
discrete maximum principle; this gives us an indication of what will be necessary
when considering other difference operators than Δh via the central difference quo￾tient. One needs to prove these two properties, and then one obtains a corresponding
convergence property.
Numerical solution and experiments
The above error analysis gives us the error arising from the discretization. However,
this implicitly assumes that we have an exact solution of the system Lhuh = fh. In
the one-dimensional case under consideration this is indeed possible (up to machine
precision), since the matrix Ah in the linear system Ahuh = ˜fh has a special form, as
we saw in (9.9). Indeed, Ah is a symmetric tridiagonal matrix, for which the entries
off the main diagonal are constant. (We recall that a matrix is called tridiagonal if all
entries are zero except those on the main diagonal and the two adjacent diagonals,
above and below it.) A square matrix of size N which has at most c nonzero entries in
each row is called sparse. Here 0 < c 
 N is a fixed (small) constant. For tridiagonal
matrices we obviously have c = 3; for such matrices it is possible to derive a special
Cholesky decomposition, which in turn leads to a recursion scheme, see e.g. [52,
Ch. 3.7.1]. One can thus solve the linear system with linear cost, that is, with O(N)
operations, where N ∈ N is the dimension of the system.
Remark 9.12 (a) When working with non-equidistant grids one uses specially
adapted difference quotients; in this case, the matrix Ah is generally no longer
symmetric, but still tridiagonal.
(b) The tridiagonality is a consequence of the use of second-order difference oper￾ators. If one uses formulae of higher order, then the matrix will no longer have
just three nontrivial diagonals, but rather a band structure.
(c) It is easy to see that in the case of Dirichlet boundary conditions it is sufficient to
restrict to the treatment of homogeneous boundary conditions, cf. Section 6.8.
Other boundary conditions (Neumann, Robin) need special treatment; one needs
to adjust the difference quotients at the boundary. Variable coefficients may be
treated with special FDMs. We refer to the numerical mathematics literature
[16, 35, 46, 52]. 324 9 Numerical methods
To summarize, the advantage of the FDM in dimension one is its simplicity,
especially as regards the actual implementation. The greatest disadvantage is perhaps
the assumption of Theorem 9.11, that f ∈ C2([0, 1]), which is a very restrictive
demand.
We finish by describing a numerical experiment. Why an “experiment”?
On the one hand, the error analysis described above is of asymptotic nature, that is,
the statements are valid in the regime h → 0+, in other words, under the assumption
that h is “sufficiently small”. It could be the case that the method does not actually
have the desired convergence properties for “reasonable” step sizes h (that is, which
do not lead to unmanageably large systems). Whether this is actually the case, can
often only be established through experiments.
Another point of interest is the constants in the error estimates. In the above anal￾ysis we could determine these explicitly ( 1
8 and 1
96 ) thanks to the discrete Green’s
functions and explicit knowledge of the solutions. But in the case of variable coef￾ficients, or in higher dimensions, this is often impossible; one can merely show that
such constants exist. If one wishes to estimate their size, then this is generally only
possible via numerical experiments. The same is true for the computational cost of a
method, which in the theorems is expressed as an O-term. But the precise constants
that these terms hide can have an enormous influence on runtime.
Additionally, when analyzing the method we always assumed that the linear
systems which appear can always be solved exactly; the influence of roundoff errors,
for example, was always neglected.
Finally, in the error analysis of a numerical method one always makes certain
assumptions, which may not be satisfied in a given application. Even if one cannot
actually apply the theory in such a case, there is at least a chance that the assumptions
made were too restrictive and the method still delivers the desired result. This can
only be established by suitable experimentation.
We now wish to describe such an experiment. In the case of FDMs the assumption
that f ∈ C2([0, 1]) is extremely restrictive for the solution of the boundary value
problem; we wish to see to what extent the one-dimensional FDM also works for
less regular functions. We will consider two different right-hand sides. Firstly, we
choose the function usin(x) := sin(2πx), which is clearly C∞, and set fsin := −u
sin.
For this example, we expect quadratic convergence (O(h2)), since the assumptions
of the above theorems are satisfied. We also define
fα(x) :=

1, 0 ≤ x ≤ α,
−1,α< x ≤ 1, (9.15)
which has a discontinuity at the point α ∈ (0, 1). Here we will take α = 0.5; the
solution is thus no longer C2. One can show in an analogous fashion to above that
the FDM still converges in this case, but only linearly (O(h)) and not quadratically;
see Exercise 9.1.
In Figure 9.2 we give a log–log plot of the convergence history (that is, the error
in dependence on the dimension of the linear system, the number of unknowns).9.1 Finite differences for elliptic problems 325
This permits us to read off the order of convergence as the (negative) gradient of the
line, which is taken as a linear least squares fit to the data. As expected, we obtain
quadratic convergence for usin. For uα, α = 0.5, we obtain linear convergence, that
is, a line with slope −1. We see that the error analysis is “sharp” in this case, that is,
we obtain linear convergence and nothing more.
101 102 103 104
10−9
10−8
10−7
10−6
10−5
10−4
10−3
10−2
10−1
100
2
1
1
fsin
fα
Fig. 9.2 Convergence history for the FDM, for the right-hand side fα in (9.15) (dashed) and fsin
(solid line). In each case the error u − uh h,∞ is plotted as a function of the number of mesh
points N, as a log–log plot. The linear convergence for fα and the quadratic convergence for fsin
are clearly recognizable.
This, and all further numerical examples in this chapter, were written in Matlab
(Mathworks®) and tested with version R2020a.
9.1.2 FDM: the two-dimensional case
We next consider the Dirichlet problem on the unit square,
−Δu(x) = f (x), x ∈ Ω := (0, 1)
2
, u(x) = 0, x ∈ Γ := ∂Ω. (9.16)
Discretization
We apply the same idea as in the one-dimensional case and define an equidistant
grid (also called uniform mesh) analogous to (9.5)
Ωh := {(x, y) ∈ Ω : x = k h, y = h, 0 ≤ k,  ≤ N + 1} (9.17)326 9 Numerical methods
for h := 1
N+1 , N = Nh ∈ N, as above. The boundary now consists of more than two
points, namely
∂Ωh := Γ ∩ Ωh = {(x, y) ∈ Γ : x = k h or y = h, 0 ≤ k,  ≤ N + 1} (9.18)
= {(x, y) ∈ Γ : x = k h and y = h, 0 ≤ k,  ≤ N + 1}, (9.19)
cf. Figure 9.3. As in the one-dimensional case we define Ω˚ h := Ωh \ ∂Ωh.
Fig. 9.3 Equidistant grid on Ω = [0, 1]
2. The interior points (Ω˚ h) are represented by the solid
black dots, ∂Ωh by the empty white circles (◦).
We will again approximate the derivatives, in this case second order partial
derivatives, by the central difference quotients (9.6), that is,
Δu(x, y) = ∂2
∂x2 u(x, y) +
∂2
∂y2 u(x, y)
≈ u(x + h, y) − 2u(x, y) + u(x − h, y)
h2
+ u(x, y + h) − 2u(x, y) + u(x, y − h)
h2
= u(x + h, y) + u(x − h, y) + u(x, y + h) + u(x, y − h) − 4u(x, y)
h2
=: Δh(x, y). (9.20)
Entirely analogously to Lemma 9.2, one can show using Taylor expansions that
Δu − Δhu∞ = O(h2) as h → 0+ if u ∈ C4(Ω). One thus obtains consistency order
2, as before. With similar (albeit technically more involved) methods, one can prove
a convergene theorem which states that on the square one has
u − uh h,∞ = O(h2) as h → 0+, if u ∈ C4(Ω).
We now wish to show that the regularity assumption u ∈ C4(Ω) is particularly
restrictive in dimension two: even if the right-hand side is smooth, the solution may
not have the required regularity.9.1 Finite differences for elliptic problems 327
Example 9.13 We consider the L-shaped domain Ω := (−1, 1)
2 \ {[0, 1)× (−1, 0]} ⊂
R2. In Example 6.88 we found a function f ∈ C∞(Ω) such that the solution of the
Poisson problem −Δu(x) = f (x), x ∈ Ω and u(x) = 0, x ∈ Γ := ∂Ω, belongs to
H1
0 (Ω) ∩ H2
loc(Ω) but not H2(Ω). In this example, u ∈ C(Ω) and u|Γ = 0. Hence the
L-shaped domain does not allow maximal H2-regularity. In particular, there is no
shift theorem as in Lemma 9.1 (b). 
0 −1 −0.5 0 0.5 1 1
10−6
10−4
10−2
100
0.001 0.01 0.05 10−3
10−2
1
1.3
Fig. 9.4 Finite difference method on the L-shaped domain, error between approximate and exact
solutions (left) and convergence history (right)
Now one could of course hope that the FDM nevertheless converges at the optimal
rate since, for example, the analytic estimates are not optimal. We thus describe a
numerical experiment with a given weak solution u ∈ H1(Ω)\{C2(Ω) ∩ C(Ω)},
which we already met in Section 6.11:
u(x, y) = r2/3 sin 
2
3
ϕ

, (x, y) = (r cos ϕ,r sin ϕ).
In Figure 9.4 we show the error for a uniform mesh of size h = 1
80 (left) and the
convergence history (right). Note that here we have displayed the mesh size h on
the x-axis; thus the curve has positive slope (unlike in Figure 9.2). We first observe
that the corners do in fact cause problems, as large errors occur in these points.
We also note that the inverse of the slope of the line in the plot of the convergence
history is approximately 1.3 (cf. the accompanying triangle), which gives the order
of convergence. This is noteworthy for two reasons:
• the lack of H2-regularity would appear to lead to a rate of convergence which is
genuinely lower than 2;
• the FDM converges in this case even though the solution is not C4; the rate of
convergence is also superlinear.
A further problem with the FDM occurs if the domain has a more complicated
geometry than one just involving right angles, as was the case with (0, 1)
2 and the
L-shaped domain, cf. Figure 9.5.328 9 Numerical methods

˚ h
∂h
Fig. 9.5 Equidistant mesh Ω˚ h (solid dots) and boundary mesh ∂Ωh(◦) for a curvilinear domain
Ω ⊂ R2.
For curvilinear domains Ω the definitions (9.17) and (9.18) no longer make sense;
instead, we use
Ω˚ h := {(x, y) ∈ Ω : x = k · h, y =  · h, k,  ∈ Z},
∂Ωh := {(x, y) ∈ Γ : x = k · h or y =  · h, k,  ∈ Z}.
It is immediate that Ωh does not represent the geometric structure of Ω well; the
discrepancy between ∂Ωh and Γ is even more pronounced.
Numerical solutions
We now wish to determine the numerical solutions. To this end, we first describe the
linear system of equations Ahuh = fh. The exact form of the matrix Ah ∈ RN2
h×N2
h
will of course depend on the numbering of the mesh points. If one chooses what is
known as the lexicographic order (cf. Figure 9.6)
zk = (xi, yj), xi = ih, yj = j h, k := (j − 1)N + i,
then the linear system corresponds to a block tridiagonal matrix
Ah =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
Bh Ch 0
Ch Bh
. . .
. . . . . . . . .
. . . Bh Ch
0 Ch Bh
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
∈ RN2
h×N2
h
with blocks9.1 Finite differences for elliptic problems 329
Bh =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
4 −1 0
−1 4 . . .
. . . . . . . . .
. . . 4 −1
0 −1 4
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
∈ RNh×Nh, Ch = diag (−1) ∈ RNh×Nh,
right-hand side fh = (h2 f (zk ))k=1,...,N2
h
, k = (j − 1)Nh + i, 1 ≤ i, j ≤ Nh, and
solution vector uh = (u(zk ))k=1,...,N2
h
∈ RN2
h .
1 2 3 4
5 6 7 8
9 10 11 12
13 14 15 16
Fig. 9.6 Lexicographic ordering of the mesh points, Ω = (0, 1)
2, N = 5.
In this case it is not possible to derive a simple recursion formula for the Cholesky
decomposition. But since Ah is sparse, symmetric and positive definite, it is pos￾sible, and indeed natural, to use an iterative numerical procedure to determine an
approximate solution of the linear system. Starting with an initial value u(0)
, we can
calculate a sequence of approximations u(k)
h , k = 1, 2,... converging to the solution
of the system Ahuh = fh as k → ∞. Since Ah ∈ RNh×Nh (here Nh = N2
h in the
above notation) is sparse and an iteration step requires a matrix-vector multiplication
in just about every known iterative method, for the step from u(k)
h to u(k+1)
h the cost is
linear, that is, one requires O(Nh) operations. The total cost will, of course, depend
on the total number of iterations, which may be measured as follows: fix an initial
value u(0)
h and a corresponding initial error u(0)
h −uh  with respect to a suitable norm
·. We would like to know how many steps are necessary to reduce this initial error
by a given factor ε ∈ (0, 1). This clearly describes the speed of convergence of the
method; to obtain an estimate on this speed one may prove a bound of the form
u(k+1)
h − uh  ≤ ρk
h u(k)
h − uh 
for some convergence factor ρh ∈ (0, 1). If this holds, then the iteration requires330 9 Numerical methods
k =
; log ε
log ρh
<
steps to reduce the error by the desired factor.
Now it may happen that this error reduction degenerates for small step sizes
h → 0+, that is, ρh → 1. This would mean that the method converges more and
more slowly as h becomes smaller. Since at the same time the dimension of the
matrix grows as h becomes smaller, such a method may become rather inconvenient.
It is thus desirable to apply a method which is asymptotically optimal, that is,
ρh ≤ ρ0 < 1, h → 0+. In other words: the error reduction of an asymptotically
optimal method is independent of the step size h, that is, the size Nh of the matrix.
The question of the construction of such asymptotically optimal methods was
open for a long time. These days there are at least two known kinds of methods,
namely the conjugate gradient method with what is known as the BPX preconditioner
(named after Bramble, Pasciak and Xu, 1990 [17]), and the multigrid method (see
for example [16, Ch. V]). Wavelet methods are another option, cf., e.g., [58].
As we have seen, the total cost depends decisively on the size of the matrix, that
is, the number of mesh points. In the two dimensional case we have Nh = N2
h . In
dimension d one has Nd
h mesh points, which means that even using optimal solution
methods with linear cost, i.e., O(Nd
h ) operations for a given error reduction, one
eventually succumbs to the curse of dimensionality.
9.2 Finite elements for elliptic problems
The finite element method (FEM) was developed in the 1950s to deal with computa￾tions arising in automotive and aeronautical engineering, in particular for structural
mechanics; it has since become a standard tool in many areas. The mathematical
theory has come a long way, even if the FEM is still the subject of current research.
Here we will give a short introduction which is directly connected to the analytical
methods for partial differential equations treated in the previous chapters.
9.2.1 The Galerkin method
Unlike the finite difference method, the finite element method is based on the weak
formulation of a given partial differential equation. As we will see, this has the
advantage of permitting us to avoid the strong regularity assumptions necessary for
the FDM.
We start with the variational formulation of a boundary value problem for an
elliptic partial differential equation. Let H be a Hilbert space with inner product (·, ·),
V → H a second Hilbert space continuously imbedded in H, and a : V × V → R9.2 Finite elements for elliptic problems 331
a continuous, coercive bilinear form. By the Lax–Milgram theorem, Theorem 4.24,
for each f ∈ H there exists exactly one u ∈ V such that
a(u, v) = ( f, v), v ∈ V. (9.21)
Since V is in general infinite dimensional (e.g. H1
0 (Ω)), (9.21) cannot be used directly
for numerics. The solution is to consider finite-dimensional subspaces
Vh ⊂ V, dim(Vh) =: Nh < ∞,
where the index “h” is used to suggest an analogy to mesh size. We will see later how
one can construct Vh based on a net (or mesh) of mesh size h, whereby significantly
more general nets than the classical FDM mesh will be allowed. This also enables us
to avoid the second major disadvantage of FDM, namely the restriction to domains
with simple geometry.
Given a finite-dimensional (“discrete”) space Vh ⊂ V, we consider the following
discrete problem: find uh ∈ Vh such that
a(uh, χ) = ( f, χ), χ ∈ Vh. (9.22)
We call uh the discrete solution of (9.21) in Vh. Since Vh is a subspace of V and we
are considering the same bilinear form a(·, ·) as in (9.21), the Lax–Milgram theorem
guarantees the existence and uniqueness of a solution uh ∈ Vh of (9.22), as well as
its stability (continuous dependence on the data, here f ). Thus the discrete problem
is well posed. We will see in Section 9.2.2 that, upon construction a basis of Vh,
(9.22) leads to a linear system of equations of dimension Nh = dimVh.
As with FDMs we wish to investigate the error u − uh. It turns out that one can
make statements about this error under very weak assumptions.
Theorem 9.14 (Céa’s lemma) Suppose that the bilinear form a : V × V → R is
continuous, that is, there existsC > 0 such that a(u, v) ≤ CuV v V for all u, v ∈ V,
and coercive, that is, there exists α > 0 such that a(u, u) ≥ αu2
V for all u ∈ V.
Then, given f ∈ H, for the solutions u of (9.21) and uh of (9.22) we have
u − uh V ≤
C
α inf
χ∈Vh
u − χV =:
C
α distV (u,Vh).
Proof Since Vh ⊂ V, in (9.21) we can, in particular, choose any χ ∈ Vh as a test
function, and in doing so obtain a(u, χ) = ( f, χ) for all χ ∈ Vh. We then substract
(9.22) from this identity, which yields
a(u − uh, χ) = ( f, χ)−( f, χ) = 0, χ ∈ Vh. (9.23)
This equation is known as the Galerkin orthogonality relation for the error eh :=
u − uh, which states that this error is perpendicular to the test space Vh with respect
to the bilinear form a(·, ·). Now by coercivity and the Galerkin orthogonality332 9 Numerical methods
αu − uh 2
V ≤ a(u − uh, u − uh)
= a(u − uh, u − χ) + a(u − uh, χ − uh) = a(u − uh, u − χ)
for arbitrary χ ∈ Vh, since χ − uh ∈ Vh. Due to the continuity of a(·, ·) we also have
αu − uh 2
V ≤ a(u − uh, u − χ) ≤ Cu − uh V · u − χV ;
dividing by u − uh V and taking the infimum over all χ ∈ Vh now yields the
claim. 
The statement of Céa’s lemma links numerical mathematics with approximation
theory, since Theorem 9.14 states that the error eh = u − uh is, up to the constant C
α
which depends on the problem, essentially the best approximation of u in Vh. Thus
the size of the error depends on the approximation property of the trial space Vh. In
what follows, we will analyze the extent to which this is in fact the case.
9.2.2 Triangulation and approximation on triangles
In order to construct a concrete trial space Vh, we may consider a geometric subdi￾vision of Ω, similar to Ωh for the FDM, but substantially better. On this subdivision
(mesh, grid, net) we may define functions (piecewise), which can then be chosen as
basis functions for Vh; that is, the trial space will be the set of linear combinations
of the basis functions.
We will restrict ourselves to open polygons Ω ⊂ R2 in the plane; the geometric
subdivision becomes more complicated in higher-dimensional spaces Rd, d > 2.
On domains whose boundary is not polygonal (but rather, for example, curvilinear),
additional terms appear in the error estimates, which we will not consider here, cf.
Section 9.3*.
Definition 9.15 We call a family of open sets T := {Ti }N
i=1, N ∈ N, a partition of Ω
if it has the following properties:
(a) Ti ⊂ Ω is open, i = 1,..., N;
(b) Ti ∩ Tj = ∅, i  j, i, j = 1,..., N;
(c) /N
i=1 Ti = Ω.
We call each T ∈ T an element of the partition, and speak of a triangulation if each
T ∈ T is an open triangle. 
In numerics one can also consider generalized triangles with curvilinear bound￾aries, or partitions into other geometric objects such as quadrilaterals; such partitions
are also called triangulations. Here, however, we will only consider partitions into
triangles. Later we will also demand certain “goodness” properties of the triangula￾tions (cf. Definitions 9.24 and 9.26). On the individual triangles, we will exclusively
consider affine functions (we also speak of linear elements). These will then be glued
together to create continuous functions on Ω, which will yield the trial space Vh. By
Céa’s lemma (Theorem 9.14) we know that the quantity9.2 Finite elements for elliptic problems 333
inf
χ∈Vh
u − χH1(Ω) =: distH1(Ω)(u,Vh)
will be of central importance for the analysis of the error. We saw in Chapter 6 that
the solution of the Poisson equation is in H2(Ω) under suitable assumptions on Ω. We
will investigate how well one can approximate arbitrary H2-functions by piecewise
affine functions (cf. Corollary 9.28).
9.2.3 Affine functions on triangles
Let T ⊂ R2 be an open triangle, to be fixed throughout. In keeping with common
practice, we will say that a function v : T → R is affine if
v

λx + (1 − λ)y
	
= λ v(x) + (1 − λ)v(y), x, y ∈ T, 0 ≤ λ ≤ 1. (9.24)
We start with the following characterization.
Lemma 9.16 The following statements hold.
(a) The set P1(T) of affine functions from T to R is a vector space of dimension
three. Let t1, t2, t3 be the three vertices of T, then for each i ∈ {1, 2, 3} there exists
exactly one vi ∈ P1(T) such that vi(tj) = δi,j, i, j = 1, 2, 3. The set {v1, v2, v3}
forms a basis of P1(T) known as its Lagrange basis.
(b) For each v ∈ P1(T) there are uniquely determined coefficients a, b, c ∈ R such
that
v(x) = a + bx1 + cx2, x = (x1, x2) ∈ T. (9.25)
Proof (a) Every x ∈ T has a unique expansion in terms of the barycentric coordinates
(a1, a2, a3), namely as
x = a1 t1 + a2 t2 + a3 t3, ai ≥ 0, i = 1, 2, 3, a1 + a2 + a3 = 1, (9.26)
cf. Exercise 9.13. Since for any v ∈ P1(T) we have v(x) = a1v(t1)+a2v(t2)+a3v(t3),
we see that v is uniquely determined by its values at the vertices t1, t2, t3. Now, for
each i ∈ {1, 2, 3}, define vi : T → R by vi(x) := ai, with x as in (9.26). Then
every v ∈ P1(T) has a unique expansion v = α1v1 + α2v2 + α3v3, where αj = v(tj),
j = 1, 2, 3. We have shown that {v1, v2, v3} is a basis of P1(T).
(b) It is easy to see that the functions in (9.25) are affine. That they form a vector
space E of dimension three, is the subject of Exercise 9.14. Thus E = P1(T). 
As an immediate consequence of the above lemma, we see that
P1(T) = #
p : T → R : p(x) = a + bx1 + cx2, a, b, c ∈ R, x = (x1, x2)
$
.
Affine functions can be defined between arbitrary convex sets. A function f : Rd →
Rd is affine if and only if f − f (0) is linear; that is, affine functions are translations334 9 Numerical methods
of linear functions. Sometimes one also simply calls affine functions linear; this is
where the name linear elements in numerical analysis comes from.
9.2.4 Norms on triangles
A first difficulty arises because when deriving error estimates for variational prob￾lems we need to control Sobolev norms, yet we have defined the Lagrange basis in
terms of interpolation. The following auxiliary result is key to linking these concepts.
Again let T ⊂ R2 be a fixed open triangle.
Lemma 9.17 Let T ⊂ R2 be an open triangle with vertices t1, t2, t3. Then
|v |H2(T) := |v|H2(T) +

3
i=1
|v(ti)|, v ∈ H2(T),
defines a norm which is equivalent to ·H2(T), where
|v|
2
H2(T) =
∫
T
(|D2
1v(x)|2 + 2|D1D2v(x)
2|
2 + |D2
2v(x)|2) dx.
Proof Since the imbedding H2(T) → C(T) is continuous (cf. Corollary 7.25), there
exists a constant c(T) > 0 such that
sup
x∈T
|v(x)| ≤ c(T) v H2(T), v ∈ H2(T), (9.27)
whence |v |H2(T) ≤ (1 + 3c(T))v H2(T), which is the upper bound. In particular,
| · |H2(T) is a continuous semi-norm on H2(T).
Let us assume for a contradiction that the lower bound v H2(T) ≤ C|v |H2(T),
v ∈ H2(T) is false for all C > 0. Then there exists a sequence (vk )k ∈N ⊂ H2(T) such
that vk H2(T) = 1 and |vk |H2(T) ≤ 1
k , k ∈ N. By Theorem 7.22 the imbedding
H2(T) → H1(T) is compact. Hence there exists a subsequence of (vk )k ∈N which
is convergent in H1(T); for simplicity we will denote this subsequence by (vk )k ∈N.
Now |vk |H2(T) ≤ |vk |H2(T) ≤ 1
k → 0 as k → ∞, and so
vk − v 2
H2(T) = vk − v 2
H1(T) + |vk − v |
2
H2(T)
≤ vk − v 2
H1(T) + 
|vk |H2(T) + |v |H2(T)
	2 → 0
as k,  → ∞. Hence (vk )k ∈N is also a Cauchy sequence in H2(T) with limitv ∈ H2(T).
For this limit, due to the continuity of | · |H2(T), on the one hand we have
|v |H2(T) = lim
k→∞ |vk |H2(T) = 0, (9.28)9.2 Finite elements for elliptic problems 335
and on the other
v H2(T) = lim
k→∞ vk H2(T) = 1. (9.29)
Since |v |H2(T) = 0 it follows that |v|H2(T) = 0 and hence v ∈ P1(T) by the following
lemma. But since |v(ti)| ≤ |v |H2(T) = 0, i = 1, 2, 3, we must have v ≡ 0. This is a
contradiction to (9.29). 
Lemma 9.18 Let T ⊂ R2 be an open triangle and suppose that v ∈ H2(T) is such
that DiDjv = 0 for all i, j = 1, 2. Then there exists an affine function p ∈ P1(T) such
that p = v almost everywhere in T.
Proof By Theorem 6.19 there exists a constant cj ∈ R such that Djv = cj, j = 1, 2.
Define w(x) := v(x) − c1 x1 − c2 x2, x = (x1, x2) ∈ T; then Djw = 0, j = 1, 2. Hence
there exists c ∈ R such that w = c, and so v(x) = c + c1 x1 + c2 x2 almost everywhere.

9.2.5 Transformation into a reference element
It is useful for both analytic and computational purposes to transform an arbitrary
element T into a reference element Tˆ. We will choose for our reference triangle the
lower left half of the unit square
Tˆ := {(x1, x2) ∈ R2 : x1, x2 > 0, 0 < x1 + x2 < 1},
as depicted in Figure 9.7. Again let T ⊂ R2 be a fixed open triangle. Then there
exists an affine mapping F : Tˆ → T which maps Tˆ bijectively onto T, that is,
F(xˆ) = b + B xˆ, xˆ ∈ Tˆ, F(Tˆ) = T, (9.30)
for an invertible matrix B ∈ R2×2 and a point b ∈ T. Clearly, p ◦ F ∈ P1(Tˆ)
for all p ∈ P1(T), that is, the set of affine functions is invariant under such affine
transformations. This will allow us to reduce the proof of many statements about T
to their counterparts on the reference element Tˆ. We note that by Lemma 9.17 there
exist constants 0 < cˆ ≤ Cˆ < ∞ such that
cˆ |vˆ |H2(Tˆ) ≤ vˆ H2(Tˆ) ≤ Cˆ |vˆ |H2(Tˆ)
, vˆ ∈ H2(Tˆ). (9.31)336 9 Numerical methods


T Tˆ F−
F
Fig. 9.7 Reduction to a reference triangle.
We will reduce the error analysis to one on the reference triangle Tˆ. To this end,
we need to investigate the effect of the mapping F and its inverse F−1 : T → Tˆ,
given by F−1(x) = B−1 x − B−1b. Let x := (x2
1 + x2
2 )
1/2, x = (x1, x2) ∈ R2, be the
Euclidean norm and
A := sup
x  ≤1
Ax = sup
x =1
Ax = sup
x0
Ax
x , A ∈ R2×2
,
the induced operator norm. Given the triangle T, we denote its inradius by ρT and
its circumradius by rT , cf. Figure 9.8 for the reference triangle Tˆ. It is decisive for
the error estimates we wish to prove that the norm of the transformation matrix B
can be estimated by the circumradius, and the norm of its inverse by the inradius.
This is the subject of the next lemma.
Lemma 9.19 With the above notation we have B ≤
√
2
√
2−1 rT and B−1  ≤ 1
√
2 ρT
.
Proof Let ρˆ and rˆ be the inradius and the circumradius, respectively, of the reference
triangle Tˆ, that is, ρˆ = 1 − 1
2
√
2 and rˆ = 1
2
√
2, cf. Figure 9.8. Now let xˆ ∈ R2 be
such that xˆ = 2 ˆρ. Then there exist two points yˆ, zˆ ∈ Tˆ such that xˆ = yˆ − zˆ, cf.
Figure 9.9. Hence Fyˆ, Fzˆ ∈ T and Bxˆ = Fyˆ − Fzˆ ≤ 2 rT , and thus
B = sup
xˆ =2 ˆρ
Bxˆ
xˆ ≤ rT
ρˆ =
√
2
√
2 − 1
rT .
Conversely, if we begin with a point x ∈ R2 such that x = 2ρT , then we can
find y, z ∈ T such that x = y − z. Hence B−1 x = F−1 y − F−1z ≤ 2 ˆr. It now
follows as above that
B−1 = sup
x =2ρT
B−1 x
x ≤ rˆ
ρT
= 1
√
2 ρT
.
This proves the claim. 9.2 Finite elements for elliptic problems 337
Fig. 9.8 Circumradius rˆ = 1
2
√
2 and inradius ρˆ = 1 − 1
2
√
2 ≈ 0.293 of the reference triangle Tˆ.
Fig. 9.9 Points yˆ, zˆ ∈ Tˆ such that xˆ = yˆ − zˆ.
In order to be able to reduce error estimates on T to ones on Tˆ, we need to
investigate the effects of the transformation on the Sobolev norms. We recall the
definition of the function F : Tˆ → T given by F(xˆ) = Bxˆ +b (Definition 9.30). Now,
given v : T → R, we define the function vˆ : Tˆ → R by
vˆ(xˆ) := v(F(xˆ)) = v(x),
where x = F(xˆ). Then
∂
∂xˆi
vˆ(xˆ) = ∂
∂xˆi
v(F(xˆ)) =

2
j=1
∂
∂xj
v(x) ∂
∂xˆi
(F(xˆ))j =

2
j=1
∂
∂xj
v(x) Bj,i,338 9 Numerical methods
where B = (Bi,j)i,j=1,2. We thus have
∇(v ◦ F) = (∇v) ◦ F · B. (9.32)
(Note that here the gradient is to be interpreted as a row vector, and on the right-hand
side of (9.32) the dot stands for matrix multiplication.) This leads to the following
estimate.
Lemma 9.20 Given v ∈ H1(T), for vˆ := v ◦ F we have vˆ ∈ H1(Tˆ) and
|vˆ|
H1(Tˆ) ≤ | det B|
−1/2 B |v|H1(T), |v|H1(T) ≤ | det B|
1/2 B−1  |vˆ|
H1(Tˆ).
Proof Using (9.32) and the transformation formula for the change of variables from
Tˆ to T we have
|vˆ|
2
H1(Tˆ) = |v ◦ F|
2
H1(Tˆ) =
∫
Tˆ
|∇(v ◦ F)|2 dxˆ =
∫
Tˆ
|(∇v) ◦ F · B|
2 dxˆ
≤ B2
∫
Tˆ
|(∇v) ◦ F|
2| det B| dxˆ · | det B|
−1
= B2| det B|
−1
∫
T
|∇v|
2 dx,
from which the first inequality follows. The second one may be shown analogously.

We will need a similar estimate on |vˆ|
H2(Tˆ) later. For this we require an auxiliary
result for matrices involving the Hilbert–Schmidt norm
A2
HS :=
n
i,j=1
a2
i j, A := (ai j)i,j=1,...,d ∈ Rd×d.
In numerics this is often also called the Schur norm or the Frobenius norm.
Lemma 9.21 For all A, B ∈ Rd we have BAHS ≤ B·AHS, where · is the
matrix norm induced by the Euclidean vector norm ·.
Proof We have A2
HS = 
d
i=1 Aei 2, where ei := (δ1,i,...,δd,i)
T ∈ Rd, cf. (1.42),
and hence BA2
HS = 
d
i=1 BAei 2 ≤ B2 
d
i=1 Aei 2 = B2 · A2
HS. This
shows the claim. 
We can now give the following estimate, as announced above.
Lemma 9.22 For all v ∈ H2(T) we have vˆ := v ◦ F ∈ H2(Tˆ), and the inequality
|vˆ|
H2(Tˆ) ≤ | det B|
−1/2 B2|v|H2(T) holds.
Proof We have
|vˆ|
2
H2(Tˆ) =
∫
Tˆ



∂2
∂xˆ2
1
vˆ(xˆ)



2
+ 2



∂
∂xˆ1
∂
∂xˆ2
vˆ(xˆ)



2
+



∂2
∂xˆ2
2
vˆ(xˆ)



2
'
dxˆ.9.2 Finite elements for elliptic problems 339
Since x = Fxˆ and vˆ(xˆ) = v(x), it follows that
∂
∂xˆi
∂
∂xˆj
vˆ(xˆ) = ∂
∂xˆi


2
k=1
∂
∂xk
v(x) Bk,j

=

2
k=1
∂
∂xk
 ∂
∂xˆi
vˆ(xˆ)

Bk,j
=

2
k,=1
∂
∂xk
 ∂
∂x
v(x)B,i

Bk,j =

2
k,=1
B,i
∂
∂xk
∂
∂x
v(x) Bk,j
= (BTHv(x)B)i,j
for i, j = 1, 2, where Hv(x) :=  ∂
∂xi
∂
∂xj
v(x)
	
i,j=1,2 is the Hessian. We thus have
|vˆ|
2
H2(Tˆ) =
∫
Tˆ
Hˆ vˆ(xˆ)2
HS dxˆ = | det B|
−1
∫
T
BTHv(x)B2
HS dx
≤ | det B|
−1 B4
∫
T
Hv(x)2
HS dx = | det B|
−1 B4 |v|
2
H2(T)
,
whence the claim. 
9.2.6 Interpolation for finite elements
As we have seen, Céa’s lemma states that the norm of the error u − uh H1(Ω) is, up
to a constant, essentially the error of the best approximation in Vh to the solution u.
Now, in order to be able to estimate this best approximation, we will construct an
interpolation operator Ih : C(Ω) → Vh. Since we are taking Ω ⊂ R2 to be a polygon,
we have H2(Ω) ⊂ C(Ω), cf. Corollary 7.25. Given any u ∈ H2(Ω), u − IhuH1(Ω)
is then an upper bound on the error in the best approximation. We will define the
interpolation operator piecewise on each T ∈ T.
For every v ∈ C(T) there exists a unique affine function IT v ∈ P1(T) such that
IT v(ti) = v(ti), i = 1, 2, 3, where t1, t2, t3 are, again, the vertices of T. In this way we
can define a local interpolation operator
IT : C(T)→P1(T).
We next wish to estimate the interpolation error for IT in terms of the semi-norm
|·|H2(T).
Theorem 9.23 Let T ⊂ R2 be an open triangle with inradius ρT and circumradius
rT , and let Cˆ be as in (9.31). Then for all v ∈ H2(T) we have
v − IT v L2(T) ≤ 12C rˆ 2
T |v|H2(T),
|v − IT v|H1(T) ≤ 9Cˆ r2
T
ρT
|v|H2(T),340 9 Numerical methods
v − IT v H1(T) ≤ 9C rˆ 2
T

2 + ρ−2
T |v|H2(T).
Proof Let F(xˆ) = Bxˆ + b be the affine transformation mapping Tˆ onto T. If t
ˆ
1, t
ˆ
2, t
ˆ
3
are the vertices of Tˆ, then ti := F(t
ˆ
i), i = 1, 2, 3, are the vertices of T. Now if
v ∈ H2(T), then vˆ := v ◦ F ∈ H2(T) and, moreover,
I
=T v = ITˆv,ˆ (9.33)
as can be seen as follows: since both IT v : T → R and F : Tˆ → T are affine, so is
I
=T v = IT v ◦ F as the composition of affine functions. Hence both functions in (9.33)
are affine. Since they agree at the three points t
ˆ
i, i = 1, 2, 3, they are identical. This
shows (9.33). Now since (ITˆvˆ − vˆ)(t
ˆ
i) = 0, i = 1, 2, 3, by (9.31), we have
ITˆvˆ − vˆ H2(Tˆ) ≤ Cˆ |ITˆvˆ − vˆ |H2(Tˆ) = Cˆ |ITˆvˆ − vˆ|
H2(Tˆ) = Cˆ |vˆ|
H2(Tˆ)
, (9.34)
since ITˆvˆ ∈ P1(Tˆ) and thus |ITˆvˆ|
H2(Tˆ) = 0.
We next prove the L2-estimate. Using the above estimate, we have
v − IT v L2(T) = | det B|
1/2 v
>− IT v L2(Tˆ) = | det B|
1/2 vˆ − ITˆvˆ L2(Tˆ)
≤ | det B|
1/2 vˆ − ITˆvˆ H2(Tˆ) ≤ Cˆ| det B|
1/2 |vˆ|
H2(Tˆ).
Now we apply Lemma 9.22 and obtain
v − IT v L2(T) ≤ Cˆ B2 |v|H2(T) ≤ Cˆ 2
(
√
2 − 1)2 r2
T |v|H2(T)
≤ 12C rˆ 2
T |v|H2(T),
where we have also used Lemma 9.19. This establishes the first estimate. For the
proof of the second inequality (involving the H1-semi-norm) we use Lemma 9.20
and obtain
|v − IT v|H1(T) ≤ | det B|
1/2 B−1  |vˆ − ITˆvˆ|
H1(Tˆ)
≤ | det B|
1/2 B−1  vˆ − ITˆvˆ H2(Tˆ)
≤ Cˆ | det B|
1/2 B−1  |vˆ|
H2(Tˆ) ≤ Cˆ B2 B−1  |v|H2(T),
where we have also used (9.34) and Lemma 9.22. Combined with Lemma 9.19, this
yields
|v − IT v|H1(T) ≤ Cˆ
 √
2
√
2 − 1
rT
2 1
√
2 ρT
|v|H2(T)
= Cˆ
√
2
(
√
2 − 1)2
r2
T
ρT
|v|H2(T) ≤ 9Cˆ r2
T
ρT
|v|H2(T).9.2 Finite elements for elliptic problems 341
We finally turn to the H1-norm. The above estimate implies
v − IT v 2
H1(T) = v − IT v 2
L2(T) + |v − IT v|
2
H1(T)
≤

144 Cˆ2
r4
T + 81 Cˆ2 r4
T
ρ2
T

|v|
2
H2(T)
≤ 81 Cˆ2
r4
T

2 + ρ−2
T
	
|v|
2
H2(T)
,
and thus the claim. 
9.2.7 Finite element spaces
Up until now we have only considered a single, fixed triangle T ⊂ R2. We now return
to the triangulation of a polygon Ω ⊂ R2 introduced in Section 9.2.2; an example is
depicted in Figure 9.10. On Ω we define the vector space
VT := {v ∈ C0(Ω) : v|T ∈ P1(T), T ∈ T}
of globally continuous, piecewise affine functions which satisfy homogeneous
Dirichlet boundary conditions. Here, as before, C0(Ω) = {u ∈ C(Ω) : u|∂Ω = 0}.
Note that VT is a subspace of H1
0 (Ω) (see Exercise 7.6 and Theorem 6.30). We call
the elements of VT linear finite elements, as the functions in VT are piecewise affine.
1 2
3
4
5
6
7
8
9
10
Fig. 9.10 Triangulation of a polygon Ω. The interior nodes are numbered arbitrarily.
We next wish to determine the dimension of the space VT; this will also be the
dimension of the linear system of equations which we will have to solve in order
to determine the numerical solution in VT. Here we will also impose an additional
“goodness” property on the triangulation.342 9 Numerical methods
Definition 9.24 A triangulation T will be called admissible if the following condi￾tions are satisfied:
(i) If, given Ti, Tj ∈ T, Ti ∩ T j consists of exactly one point, then this point is a
vertex of both Ti and Tj.
(ii) If, given Ti, Tj ∈ T, i  j, Ti ∩ T j consists of more than one point, then Ti ∩ T j
is an edge of both Ti and Tj. 
Fig. 9.11 Non-admissible partitions. On the left, condition (i) of Definition 9.24 is violated, on
the right condition (ii).
Figure 9.11 should help to clarify this definition, as it describes two non-admissi￾ble partitions: conditions (i) and (ii) prevent hanging nodes. Let T be an admissible
triangulation. If z ∈ Ω is a vertex of two distinct triangles in T, then we call z a
node. We will denote by T˚ the set of interior nodes, i.e., all nodes lying in Ω, and
by |T | ˚ the number of interior nodes, cf. Figure 9.10. Using T˚ we can now construct
a basis of VT, as follows.
Lemma 9.25 Let T be an admissible triangulation of a polygon Ω ⊂ R2. Then for
every ti ∈ T˚ = {t1,..., tN }, N = |T | ˚ , there exists a unique ϕi ∈ VT such that
ϕi(tj) = δi,j, j = 1,..., N. (9.35)
The functions {ϕ1,...,ϕN } form a basis of VT, known as the Lagrange basis or the
nodal basis of VT.
Proof Fix i ∈ {1,..., N} and let T ∈ T be a triangle which has ti as a vertex. There
exists a unique fT ∈ P1(T) such that fT (ti) = 1 but fT vanishes in the other two
vertices of T. Now let S ∈ T be such that T ∩ S  ∅, S  T. We will show that
fT = fS on T ∩S. This will mean that ϕi(x) := fT (x) is well defined for all x ∈ T, and
so ϕi ∈ VT will be the desired function which satisfies (9.35). Since the triangulation
is admissible, T ∩ S is either a node or a common edge, as hanging nodes have been
ruled out. If T ∩ S = {z} is a common vertex of T and S, then fT (z) = fS(z). In the
second case we may write T ∩ S = {λx + (1 − λ)y : 0 ≤ λ ≤ 1}, where x and y are
two common vertices of T and S. Then we have, for all 0 ≤ λ ≤ 1,9.2 Finite elements for elliptic problems 343
fT (λx + (1 − λ)y) = λ fT (x) + (1 − λ) fT (y) = λ fS(x) + (1 − λ) fS(y)
= fS(λ + (1 − λ)y),
meaning that fS = fT on T ∩ S. This proves (9.35).
We still need to show that {ϕ1,...,ϕN } is a basis of VT. By (9.35) it is clear
that these functions are linearly independent. Now take any u ∈ VT, then v :=
u − 
N
i=1 u(ti)ϕi ∈ VT and we have v(tj) = 0 for all j = 1,..., N. Since v ∈ VT
vanishes on the boundary ∂Ω of Ω, it follows that v(x) = 0 for every vertex x of
every triangle T ∈ T. Since V|T ∈ P1(T), we thus have v|T = 0 for all T ∈ T, and so
v ≡ 0. This means that u admits the expansion u = 
N
i=1 u(ti)ϕi. 
Using the Lagrange basis we can now define an interpolation operator
IT : C0(Ω) → VT as follows: ITu :=

N
i=1
u(ti)ϕi, u ∈ C0(Ω). (9.36)
That is, ITu is the uniquely determined element of VT which coincides with u in
all nodes. The operator IT is linear and continuous, and I2
T = IT, that is, it is a
continuous projection of C0(Ω) onto VT. We also call IT the Clément operator. We
now wish to study convergence properties of such triangulations. We will consider
a family {Th}h>0 of regular triangulations of the polygon Ω; the notation Th is used
to indicate the assumption, which we will always take, that
rT ≤ h for all T ∈ Th.
We will show that under suitable assumptions
v − ITv L2(Ω) = O(h2) as h → 0+, for all v ∈ H2(Ω).
Of course we will need to eliminate the possibility that the triangles may degenerate
for small h; such a case could occur if the ratio of the longest to the shortest size
should blow up as h ↓ 0. The following definition will prevent this from happening.
Definition 9.26 A family {Th}h>0 of triangulations is said to be quasi-uniform if
rT ≤ h for all T ∈ Th and there exists some κ ≥ 1 such that
rT
ρT
≤ κ
for all T ∈ Th and all h > 0. 
The condition of quasi-uniformity imposes that the ratio of the circumradius to
the inradius is uniformly bounded by κ, for all T ∈ Th and all h > 0. Thus narrow,
drawn-out triangles are ruled out as h → 0+. For every h > 0 we now consider the
finite element space
Vh := {v ∈ C0(Ω) : v|T ∈ P1(T) for all T ∈ Th}, (9.37)344 9 Numerical methods
as well as the Clément operator Ih : C0(Ω) → Vh defined above. For these we have
the following error estimates.
Theorem 9.27 Let {Th}h>0 be a quasi-uniform family of admissible triangulations
of a polygon Ω ⊂ R2. Then the following error estimates hold:
v − Ihv L2(Ω) ≤ 12C hˆ 2 |v|H2(Ω), v − Ihv H1(Ω) ≤ 16Cˆ κ h |v|H2(Ω),
for all v ∈ H2(Ω) and all 0 < h ≤ 1, where Cˆ is as in (9.31).
Proof By assumption, rT ≤ h for all T ∈ Th. Furthermore,
v − Ihv 2
L2(Ω) =

T ∈Th
v − Ihv 2
L2(T)
≤ 144Cˆ2 h4 
T ∈Th
|v|
2
H2(T) = 144Cˆ2 h4 |v|
2
H2(Ω)
by Theorem 9.23. The proof of the second inequality is entirely analogous, using
that ρT ≤ rT ≤ h ≤ 1; indeed, we have that 81(2 + ρ−2
T ) ≤ 243 ρ−2
T and so, by
Theorem 9.23,
v − Ihv 2
H1(Ω) =

T ∈Th
v − Ihv 2
H1(T) ≤ 81Cˆ2 
T ∈Th
r4
T (2 + ρ−2
T ) |v|
2
H2(T)
≤ 243Cˆ2 
T ∈Th
r2
T
r2
T
ρ2
T
|v|
2
H2(T) ≤ 243Cˆ2 h2 κ2 
T ∈Th
|v|
2
H2(T)
= 243Cˆ2 h2 κ2|v|
2
H2(Ω)
,
whence the second estimate. 
As a consequence of Theorem 9.27 we have the following estimates, which will
be important in conjunction with Céa’s lemma.
Corollary 9.28 Under the assumptions of Theorem 9.27, the following estimates
hold for all v ∈ H2(Ω) and 0 < h ≤ 1:
inf
χ∈Vh
v − χL2(Ω) ≤ 12C hˆ 2 |v|H2(Ω),
inf
χ∈Vh
v − χH1(Ω) ≤ 16Cˆ κ h |v|H2(Ω).
9.2.8 The Poisson problem on polygons
The above analysis yielded estimates on the error in the best approximation in Vh
of an arbitrary function v ∈ H2(Ω). We now wish to apply these estimates to the9.2 Finite elements for elliptic problems 345
solution u of the elliptic boundary value problem (9.21). For this, we need to specify
the bilinear form in question, which in turn will determine the constants C and
α in Céa’s lemma (Theorem 9.14). The final step will be to replace the norm of
the unknown solution by an expression which only depends on known data of the
problem.
We will again restrict ourselves to the Poisson problem −Δu = f in a convex
polygon Ω ⊂ R2 with homogeneous Dirichlet boundary conditions u = 0 on ∂Ω.
So let Ω ⊂ R2 be a convex polygon. By Theorem 6.86 the Poisson problem is H2-
regular, that is, for every f ∈ L2(Ω) there exists a unique u ∈ H1
0 (Ω) ∩ H2(Ω) such
that −Δu = f . In what follows, we will call this u the solution of the Poisson problem
(with right-hand side f ). Its H2-norm can be controlled in the following sense.
Theorem 9.29 There exists a constant c2 > 0, which depends only on Ω, such that
uH2(Ω) ≤ c2  f L2(Ω)
for all f ∈ L2(Ω), where u is the solution of the Poisson problem with right-hand
side f .
Proof The mapping A−1 : L2(Ω) → H2(Ω) which maps each f ∈ L2(Ω) to the
unique solution of the Poisson problem, is clearly linear (see Theorem 8.7 for the
notation A−1). It follows from the closed graph theorem that A−1 is continuous.
Thus uH2(Ω) = A−1 f H2(Ω) ≤ A−1 L(L2(Ω),H2(Ω))  f L2(Ω), which was exactly
the claim with c2 := A−1 L(L2(Ω),H2(Ω)). 
∫
The bilinear form associated with the Poisson problem is given by a(u, v) =
Ω ∇u ∇v dx, u, v ∈ H1
0 (Ω).
Lemma 9.30 For the bilinear form associated with the Poisson problem we have,
for all u, v ∈ H1
0 (Ω),
|a(u, v)| ≤ uH1(Ω) v H1(Ω), a(u, u) ≥ αu2
H1(Ω)
for some constant α > 0.
Proof The first inequality is trivial, the second follows from the Poincaré inequality
(Theorem 6.32). 
The first estimate states that in Céa’s lemma (Theorem 9.14) we may take the
constant C to be C = 1. We now have all the necessary ingredients for the proof of
the following central theorem, which delivers linear convergence of the error with
respect to the H1-norm.
Theorem 9.31 Let {Th}h>0 be a quasi-uniform family of admissible triangulations
of a convex polygon Ω ⊂ R2. Given f ∈ L2(Ω), let u be the solution of the Poisson
problem and let uh ∈ Vh be the discrete solutions via linear elements in accordance346 9 Numerical methods
with (9.22). Then there exists a constant c > 0, which depends only on κ and Ω, such
that, for all 0 < h ≤ 1,
u − uh H1(Ω) ≤ c h f L2(Ω).
Proof By Theorem 9.14 (Céa’s lemma), Theorem 9.29 and Corollary 9.28, we have
u − uh H1(Ω) ≤
1
α inf
χ∈Vh
u − χH1(Ω) ≤
1
α16Cˆ κ h |u|H2(Ω)
≤
1
α16Cˆ κ h uH2(Ω) ≤
1
α16 c2 Cˆ κ h  f L2(Ω)
and thus the claim, with c = 1
α 16 c2 Cˆ κ. 
An L2-estimate
In this section we will show how one can obtain L2-estimates in the concrete case of
the Poisson problem. It turns out that linear convergence with respect to the H1-norm
in Theorem 9.31 leads to quadratic convergence in the L2-norm. The general result
is due to Aubin (1967) and Nitsche (1968) and is valid for general (not necessarily
symmetric) forms. The essential step consists of considering a so-called dual problem
to the actual variational problem of interest, which is as follows: given g ∈ L2(Ω),
let w ∈ H1
0 (Ω) the solution of
a(v, w) = (g, v), v ∈ H1
0 (Ω).
In the case of a symmetric form a(·, ·) and the right-hand side considered here the
dual problem has exactly the same form as the original Poisson problem; as such, here
the designation “dual problem” is superfluous (but not in the case of non-symmetric
or even nonlinear problems, which we will not be considering here), but perhaps
useful to highlight the key step in the proof (see Exercise 9.16 for a more general
statement). This is how we gain an order of convergence in the passage from the
H1-norm to the L2-norm.
Theorem 9.32 (Aubin–Nitsche, special case) Under the assumptions of Theo￾rem 9.31 we have, for all 0 < h ≤ 1,
u − uh L2(Ω) ≤ c2 h2  f L2(Ω),
where c is the constant from Theorem 9.31.9.2 Finite elements for elliptic problems 347
Proof Fix 0 < h ≤ 1. By the theorem of Riesz–Fréchet there exists an g ∈ L2(Ω)
such that gL2(Ω) ≤ 1 and
u − uh L2(Ω) = (u − uh, g)L2(Ω).
We now consider the solution w ∈ H1
0 (Ω) of the dual problem, here the Poisson
problem with right-hand side g. Using the Galerkin orthogonality a(u − uh, χ) = 0
for all χ ∈ Vh, we obtain
u − uh L2(Ω) = (u − uh, g)L2(Ω) = a(u − uh, w) = a(u − uh, w − χ)
for arbitrary χ ∈ Vh, where in the second step we chose the test function v = u − uh
in the dual problem, that is, (g, u − uh) = a(u − uh, w). The continuity of the bilinear
form a(·, ·) and the error estimate of Theorem 9.31 yield
u − uh L2(Ω) ≤ u − uh H1(Ω) w − χH1(Ω) ≤ c h  f L2(Ω) w − χH1(Ω).
Since χ ∈ Vh is arbitrary, we may choose χ = wh, the discrete solution of the dual
problem. Again applying Theorem 9.31, we obtain
u − uh L2(Ω) ≤ c h  f L2(Ω) c h gL2(Ω) ≤ c2 h2  f L2(Ω),
whence the claim. 
We thus obtain the error estimate u − uh L2(Ω) = O(h2), h → 0+, that is, the
same order as with the FDM (which we had proved there for the discrete maximum
norm ·h,∞). However, the extremely restrictive assumption u ∈ C4(Ω) needed for
the FDM is not needed for the FEM. The significantly greater flexibility with regards
to the geometry of the domain Ω is an additional advantage.
A suboptimal L∞-estimate
Up until now, all estimates have been formulated in terms of Lebesgue norms. To
finish we wish to prove an estimate (of first order) with respect to the supremum
norm
v C(Ω) := sup
x∈Ω
|v(x)|, v ∈ C(Ω).
We speak here of a “suboptimal” estimate since with rather more effort one could
prove a higher order of convergence (cf. Section 9.3*). For the desired estimate we
require what is known as an inverse estimate, for which we will however need a
further assumption on the triangulation.348 9 Numerical methods
Definition 9.33 A family {Th}h>0 of triangulations is called uniform if rT ≤ h for
all T ∈ Th and there exists some κ ≥ 1 such that h
ρT ≤ κ for all T ∈ Th and all h > 0.

Since rT ≤ h it is clear that the requirement of uniformity is stronger than the
quasi-uniformity of Definition 9.26. We can now introduce and prove the announced
inverse estimate.
Lemma 9.34 (Inverse estimate) Let {Th}h>0 be a uniform family of admissible
triangulations of a polygon Ω ⊂ R2. Then the inequality  χC(Ω) ≤ 3 κ
h  χL2(Ω)
holds for all χ ∈ Vh, cf. (9.37).
Proof Let T ∈ Th, then χ|T ∈ P1(T), so that χ2
|T is a quadratic polynomial. We now
use a quadrature formula on T which is exact for cubic polynomials, cf. [57, Ch. 8.8,
Tn: 3-3]. In order to be able to formulate this, we will need the following notation:
let ti, i = 1, 2, 3 be the vertices T, mi,j, i < j, i = 1, 2, j = 2, 3, the midpoints of the
edges, and s the center of mass of T. Then we have, for χ ∈ Vh,
 χ2
L2(T) =
∫
T
χ(x)
2 dx = |T|
60 
3

3
i=1
χ(ti)
2 + 8

i<j
χ(mi,j)
2 + 27χ(s)
2

≥ π
20 ρ2
T

max
i=1,2,3
| χ(ti)|2
,
since |T| ≥ πρ2
T . We obtain  χC(Ω) = maxi=1,2,3 | χ(ti)| by χ|
T ∈ P1(T); hence
we have shown that  χC(T) ≤ ( 20
π )
1/2 1
ρT  χL2(T) ≤ 3
ρT  χL2(T). Since  χC(Ω) =
supT ∈Th  χC(T)
, the claim follows. 
We now come to the suboptimal L∞-estimate.
Theorem 9.35 Let {Th}h>0 be a uniform family of admissible triangulations of a
polygon Ω ⊂ R2. Given f ∈ L2(Ω), let u be the solution of the Poisson problem
and uh ∈ Vh the discrete solution via linear elements in accordance with (9.22).
Then there exists a constant C > 0, which depends only on the domain Ω, such that
u − uh C(Ω) ≤ Cκ h  f L2(Ω)
. Here κ is the constant from Definition 9.33.
Proof Since FT : Tˆ → T is bijective, we have, for every T ∈ Th,
u − IT uC(T) = uˆ − ITˆ uˆ
C(Tˆ) ≤ c1 |uˆ − ITˆ uˆ|
H2(Tˆ) = c1 |uˆ|
H2(Tˆ)
,
where c1 is the imbedding constant of H2(Tˆ) → C(Tˆ) and where we use that the
second derivatives of ITˆ uˆ vanish. Now, by Lemma 9.22 and Lemma 9.19,
|uˆ|
H2(Tˆ) ≤ | det BT |
−1/2 BT 2 |u|H2(T)
≤ | det BT |
−1/2 2
(
√
2 − 1)2
r2
T |u|H2(T).9.2 Finite elements for elliptic problems 349
Next, since |Tˆ| = 1
2 ,
| det BT | = 2
∫
Tˆ
| det BT | dxˆ = 2
∫
T
dx = 2 |T| ≥ 2 π ρ2
T ≥ 2 π κ−2
r2
T,
whence, by Theorem 9.29 and the fact that rT ≤ h and
√
2
(
√
2−1)2 √π ≤ 5,
|uˆ|
H2(Tˆ) ≤ 5 κ h |u|H2(T) ≤ 5 κ h |u|H2(Ω) ≤ 5 c2 κ h  f L2(Ω).
Taking the maximum over u, we obtain the following estimate for the mapping
Ih : C0(Ω) → Vh: u − IhuC(Ω) ≤ 5c1c2κh f L2(Ω). In fact, we observe that
uh − Ihu belongs to Vh and so the inverse estimate of Lemma 9.34 holds. We
apply Theorem 9.27 to the term uh − IhuL2(Ω) and the quadratic estimate from
Theorem 9.32 to u − uh L2(Ω) to obtain
u − uh C(Ω) ≤ u − IhuC(Ω) + uh − IhuC(Ω)
≤ 5c1c2κh f L2(Ω) + 3
κ
h uh − IhuL2(Ω)
≤ 5c1c2κh f L2(Ω) + 3
κ
h u − IhuL2(Ω) + 3
κ
h u − uh L2(Ω)
≤ 5c1c2κh f L2(Ω) + 3 κ
h
12 C hˆ 2 uH2(Ω) + 3
κ
h
h2c2  f L2(Ω)
≤ hκ (5c1c2 + 36Ccˆ 2 + 3c2)  f L2(Ω),
which proves the claim for C := 5c1c2 + 36Ccˆ 2 + 3c2. 
We record a further property of uniform triangulations for later use.
Remark 9.36 If {Th}h>0 is a uniform family of admissible triangulations, then the
number of elements to which each node belongs can be controlled independently of
h. In fact, since rT
ρT ≤ h
ρT ≤ κ is bounded independently of h, the smallest angle of
each triangle is likewise bounded from below independently of the mesh size h. 
With this we can now prove a further variant of the inverse estimate. This often
goes by the name of Bernstein’s inequality in the approximation theory literature.
Theorem 9.37 (Inverse estimate) Let {Th}h>0 be a uniform family of admissible
triangulations of a polygon Ω ⊂ R2. Then there exists a constant Cinv > 0 such that
∇χ2
L2(Ω) ≤ Cinvh−2  χ2
L2(Ω) for all χ ∈ Vh.
Proof By Remark 9.36 it suffices to show the claim in the case Ω = T; the rest
follows by summing over the elements. To treat Ω = T, in turn we may reduce
everything to the reference element Tˆ, on which we use barycentric coordinates.
Then every χˆ on Tˆ may be written as χˆ = 
3
i=1 αiϕˆi, where ϕˆ1(xˆ) := 1 − xˆ1 − xˆ2,
ϕˆ2(xˆ) := xˆ1, ϕˆ3(xˆ) := xˆ2 and α := (αi)i=1,2,3. An elementary calculation shows that
∇ˆ χˆ 2
L2(Tˆ) = ((α2 −α1)
2 +(α3 −α1)
2) χTˆ 2
L2(Tˆ) = 1
2 (2α2
1 +α2
2 +α2
3 −α1α2 −α1α3) ≤
3
2 |α|
2. On the other hand, another elementary calculation (if necessary using Maple®,350 9 Numerical methods
cf. Chapter 10) shows that ϕˆi 2
L2(Tˆ) = 1
12 and (ϕˆi, ϕˆj)L2(Tˆ) = 1
24 for i  j. Thus
 χˆ 2
L2(Tˆ) = 1
12 (α2
1 +α2
2 +α2
3 +α1α2 +α1α3 +α2α3) = 1
36 (α1 +α2 +α3)
2 + 1
18 |α|
2, and
so ∇ˆ χˆ 2
L2(Tˆ) ≤ 1
27  χˆ 2
L2(Tˆ)
. We now apply Lemma 9.19 and, setting Cinv := 1
54 ,
obtain the estimate
∇χ2
L2(T) ≤ | det B| B−1 2 ∇ˆ χˆ 2
L2(Tˆ) ≤ Cinvh−2| det B|  χˆ 2
L2(Tˆ)
= Cinvh−2  χ2
L2(T)
,
which proves the claim. 
Corollary 9.38 (Inverse Cauchy–Schwarz inequality) Under the assumptions of
Theorem 9.37, for the bilinear form a(·, ·) associated with the Poisson problem the
inequality a(uh, vh) ≤ Cinvh−2 uh L2(Ω) vh L2(Ω) holds for all uv, vh ∈ Vh.
Proof By the Cauchy–Schwarz inequality and the inverse estimate, we have
a(uh, vh) = (∇uh, ∇vh)L2(Ω) ≤ ∇uh L2(Ω) ∇uh L2(Ω)
≤ Cinvh−2 uh L2(Ω) vh L2(Ω),
for all uh, vh ∈ Vh, which completes the proof. 
9.2.9 The stiffness matrix and the linear system of equations
We shall now describe how one can solve the discrete problem (9.22)
a(uh, χ) = ( f, χ), χ ∈ Vh
numerically, where a(·, ·) is a symmetric, coercive and continuous bilinear form.
Here Vh is a finite-dimensional Hilbert space with basis
Φh := {ϕ1,...,ϕNh } ⊂ Vh.
We seek a solution of the discrete problem in the form of a linear combination of
these basis elements,
uh =

Nh
i=1
ui ϕi ∈ Vh.
We form a vector from the coordinates uh := (ui)1≤i≤Nh ∈ RNh and use the basis
functions ϕj, j = 1,..., Nh, of Vh as test functions in (9.22). Thus the discrete
problem is equivalent to9.2 Finite elements for elliptic problems 351
( f, ϕj)L2(Ω) = a(uh, ϕj) =

Nh
i=1
a(ϕi, ϕj) ui = (Ah uh)j, j = 1,..., Nh,
where Ah := (a(ϕi, ϕj))1≤i,j≤Nh is the stiffness matrix. As with the FEM itself,
the name has its origins in structural mechanics, where the entries of Ah can be
interpreted as describing the stiffness of the elements being analyzed.
In order to calculate Ah one needs to calculate inner products (normally integrals
of products of derivatives), often numerically. There are many strategies available
to do this, and to construct the complete matrix efficiently, which can be found in
textbooks on the numerics of partial differential equations, e.g. [16, Ch. II.8].
Since a(·, ·) is symmetric and coercive, the matrix Ah is symmetric and positive
definite (s.p.d.). In the context of finite elements, Ah is additionally sparse, since
supp ϕi overlaps with supp ϕj for very few j  i, and only such entries a(ϕi, ϕj) can
be distinct from zero, as long as a(·, ·) is local, as in the Poisson problem. Thus, just
as with the FDM, one can bring efficient iterative solution procedures (such as the
BPX preconditioned conjugate gradient method, the multigrid method and wavelet
methods, as mentioned above) to bear to solve the linear system of equations.
9.2.10 Numerical experiments
Just as for the FDM, we now wish to describe several numerical experiments, in
order to complement the theoretical results and also demonstrate some interesting
quantitative effects.
One-dimensional examples
We start with the same one-dimensional example we saw in the context of the FDM.
In one dimension the elements T are open subintervals of Ω = (0, 1). To keep things
simple, suppose that the partition is equidistant, that is, h := 1
Nh , Nh > 1, and
xi := ih, 0 ≤ i ≤ Nh. Then we obtain
Th = {Ti }Nh
i=1
, Ti := (xi−1, xi), 0 = x0 < x1 < ··· < xNh−1 < xNh = 1.
For this partition we determine a basis of the space Vh consisting of piecewise linear
continuous functions. Clearly, Vh is identical to the spline space generated by linear
B-splines with respect to Th. These functions, which we shall call hat functions but
are also known as triangle functions and tent functions, are given by352 9 Numerical methods
Fig. 9.12 Hat functions ϕ1, ϕ2, ϕ3 with three interior nodes.
ϕi(x) :=
⎧⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪
⎩
1
h (x − xi−1), x ∈ (xi−1, xi),
1
h (xi+1 − x), x ∈ (xi, xi+1),
0, otherwise,
for 1 ≤ i ≤ Nh − 1. Figure 9.12 shows an example for Nh = 4 and three functions
ϕ1, ϕ2, ϕ3. Clearly the space Vh has dimension equal to the number of interior nodes
of the mesh, Nh = dim(Vh) = Nh − 1, where Nh is the number of elements (that is,
subintervals). Hence the resulting linear system of equations has dimension Nh =
Nh − 1.
One can check easily (cf. Exercise 9.3) that in the one-dimensional uniform
partition case the stiffness matrix corresponds exactly to the matrix Ah from the
FDM. However, the right-hand sides are different: we have
f FDM
i = h2 f (xi), f FEM
i = ( f, ϕi)L2(Ω) =
∫ 1
0
f (x) ϕi(x) dx.
For our experiments, on the one hand we again consider the function usin(x) :=
sin(2πx) ∈ C∞(0, 1), and on the other the solution of the two point boundary value
problem with discontinuous right-hand side fα in (9.15). Since fα is piecewise
constant (and in particular in L2), the corresponding solution uα belongs to H2(0, 1).
We thus expect quadratic convergence with respect to ·L2(Ω) in both cases. This
is exactly what Figure 9.13 shows us: we obtain two parallel lines with the same
slope, indicating quadratic convergence in both cases (here we have chosen ·h,∞
in order to facilitate the comparison with the FDM). We also see that the curve for
uα is below the other, so that the errors are quantitatively smaller even though the
qualitative behavior is the same. The reason for this is that the right-hand side of the
error estimate of Corollary 9.32 contains the L2-norm of the right-hand side, and this
norm is smaller for uα than for usin, that is,  fαL2(0,1) <  fsin L2(0,1). This explains
the quantitative difference.9.2 Finite elements for elliptic problems 353
101 102 103 104 105
10−11
10−10
10−9
10−8
10−7
10−6
10−5
10−4
10−3
10−2
10−1
100
2
1
h
usin
uα
Fig. 9.13 Convergence history for the FEM for the right-hand sides fα (dashed line) and fsin (solid
line) in (9.15). In both cases the error u − uh h,∞ is plotted against the number of degrees of
freedom Nh in a log-log scale. The quadratic convergence is clearly visible for both examples.
Two-dimensional examples
We now consider three examples in two space dimensions, as depicted in Figure 9.14.
The first is a square, for which we have already determined the exact solution in the
course of the book (Section 3.3.1); we thus know the analytic solution. This is not the
case for the other two examples. For the L-shaped domain we know that due to the
inward-pointing corner (the domain is not convex) we must expect a lower degree of
regularity of solutions and hence a lower order of convergence. The third example
is a “general” convex polygon, which will show the advantages of the FEM. In this
case we have H2-regularity. The triangulations were generated with the Matlab
command createpde under specification of a maximal mesh size h. As above, in
each case we will use piecewise linear elements for h = 2−k , k = 1,..., 7.354 9 Numerical methods
Fig. 9.14 Three illustrative examples for finite elements in 2D, the square (left), an L-shaped
domain (center) and a convex polygon (right). The respective FEM solutions are shown in the
second row.
10−2 10−1
10−6
10−5
10−4
10−3
10−2
10−1
1
2
1
1.6
Mesh size h
Square
L-shaped domain
Polygon
102 103 104 105
10−6
10−5
10−4
10−3
10−2
10−1
1
√2
1
√0.8
Number of unknowns h
Square
L-shaped domain
Polygon
Fig. 9.15 Convergence history for the three two-dimensional finite elements examples, left w.r.t.
mesh size h, right w.r.t. number of unknowns Nh.
In each case we have numerically determined a “reference solution” on a very
fine mesh, and then determined the error to this solution. In all numerical examples
the Lebesgue norms were calculated with the help of quadrature formulae of corre￾sponding orders. The errors and the orders of convergence are given in Figure 9.15.
On the left-hand side we see the error as a function of the maximal mesh size h in
a log-log plot. The orders of convergence of ca. 2 for the square and polygon and9.3* Extensions and generalizations 355
ca. 1.6 for the L-shaped domain, as predicted by the theory, can be seen clearly.
We see in particular that the FEM admits the same order of convergence on general
convex polygons as on the square. On the L-shaped domain we obtain a lower order
of convergence; since we know that the solution is in H1 but not in H2, this was
to be expected. It is also evident that the lack of regularity of the solution does
indeed result in a reduction of the rate of convergence. In this sense the convergence
estimates described above appear to be sharp.
On the right-hand side of Figure 9.15 we have plotted the error against the number
of unknowns Nh. This is interesting for two reasons: for a given maximal mesh size
h the number of elements varies, since on the one hand the domains are different
and on the other the generation of the meshes results in a different number of nodes.
If we had a uniform mesh, then we would have Nh ∼ h2 – we can recognize the
corresponding rate of convergence despite the differences in the mesh. The second
reason is that Nh also essentially determines the computational cost, and hence the
runtime.
9.3* Extensions and generalizations
9.3.1 The Petrov–Galerkin method
When treating the Galerkin method in Section 9.2.1 we were acting on the assumption that the trial
and test spaces of the bilinear forms were identical. We now wish to consider problems of the type
(4.12), that is, given f ∈ W
,
find u ∈ V such that b(u, w) = f (w) for all w ∈ W, (9.38)
where b : V ×W → R is a continuous bilinear form and the trial space V and the test space W are
now possibly distinct. In this case one may consider finite-dimensional subspaces of the form
Vh ⊂ V, Wh ⊂ W, 0  dim(Vh) = dim(Wh) = Nh < ∞, h > 0. (9.39)
Here we are supposing for simplicity that trial and test spaces have the same dimension, which
will result in a linear system with the same number of variables as equations to be solved. If one
does not wish to assume this, then one obtains a least squares problem. In the theorem of Banach–
Nečas, Theorem 4.27, the inf-sup-condition for b|Vh ×Wh guarantees the existence and uniqueness
of solutions of the discrete problem, that is, that there exists exactly one uh ∈ Vh such that
b(uh, wh) = f (wh), wh ∈ Wh . (9.40)
However, here we can quickly recognize an essential difference from the numerical approximation
of coercive problems using the Galerkin method. In the latter case the discrete problem “inherits”
the coercivity constant from the variational problem; we do not need to distinguish between α and
a potential αh. This is not the case with the inf-sup condition, since there is no reason for the
supremum with respect to w ∈ W to be attained in Wh. This means that the spaces Vh and Wh
need to be compatible with each other, in the sense that the inf-sup-condition for b|Vh ×Wh needs
to be uniform in h > 0.
Definition 9.39 The spaces Vh ⊂ V and Wh ⊂ W in (9.39) are said to satisfy a Ladyshenskaya–
Babuška–Brezzi (LBB) condition with respect to the bilinear form b : V × W → R if there exists a356 9 Numerical methods
constant β > 0 such that
inf
vh ∈Vh
sup
wh ∈Wh
b(vh, wh)
wh W vh V
≥ β (9.41)
for all h > 0. 
Note that the constant β in the LBB condition is not permitted to depend on h. It can be a very
delicate problem to construct discrete spaces Vh and Wh in such a way that the LBB condition is
satisfied; we refer to [16]. We also observe that the constant β in (9.41) need not be the inf-sup
constant of the bilinear form b (if the latter exists, which is often assumed); strictly speaking one
should introduce an additional symbol for this constant. However, by considering the minimum of
the two constants, one can survive with just one letter β for the two conditions.
Remark 9.40 So far the LBB condition does not imply that b satisfies an inf-sup condition (in fact,
the spaces Vh, Wh could be independent of h > 0). But we will see below that LBB does in fact
imply an inf-sup condition for b if we assume that the spaces Vh and Wh approximate V and W,
respectively, as h → 0+ (see (9.46), (9.47) below). 
We now wish to investigate in which form an analog of Theorem 9.14 (Céa’s lemma) holds; for
this, we assume the LBB condition. Let f ∈ W
. It follows from the theorem of Banach–Nečas,
Theorem 4.27, that (9.40) has a unique solution uh for all h > 0. Assume that u ∈ V is a solution1
of the continuous problem (9.38). Then Galerkin orthogonality holds, that is,
b(u − uh, wh) = 0 for all wh ∈ Wh . (9.42)
Now let vh ∈ Vh be arbitrary; then by the triangle inequality, the LBB condition and Galerkin
orthogonality (b(uh − vh, wh) = b(u − vh, wh)), we have
u − uh V ≤ u − vh V + uh − vh V ≤ u − vh V + 1
β sup
wh ∈Wh
b(uh − vh, wh)
wh W
= u − vh V + 1
β sup
wh ∈Wh
b(u − vh, wh)
wh W
≤

1 + C
β

u − vh V .
Since vh ∈ Vh was arbitrary, we have thus shown that
u − uh V ≤

1 + C
β

inf
vh ∈Vh
u − vh V .
As with Céa’s lemma, the Petrov–Galerkin approximation is thus, up to a multiplicative constant,
as good as the best approximation to u in the trial space Vh. However, the constant is significantly
worse – and, as we shall see, not optimal. We will next prove a result from [61], that the “1+” can
be eliminated. To this end we first require a result of T. Kato from [40].
Lemma 9.41 (Kato, 1960) Let H be a Hilbert space and suppose that P : H → H is idempotent,
i.e., P2 = P. If P  0 and P  I, then
P = I − P. (9.43)
1 Note that here we do not need to assume uniqueness of solutions, cf. the end of the section. Exis￾tence and uniqueness of solutions are guaranteed by the theorem of Banach–Nečas, Theorem 4.27
(page 133) in the case that the inf-sup condition (4.11) and the surjectivity condition (4.13) are
satisfied.9.3* Extensions and generalizations 357
Proof (1) First suppose that dim(H) = 2, so that necessarily rank(P) = rank(I − P) = 1. Hence
there exist a, b, c, d ∈ H such that Pv = (b, v)H a and (I − P)v = (d, v)H c for all v ∈ H, and
(a, b)H = (c, d)H = 1. Hence P = aH b H and I − P = c H d H . Moreover, for
all v ∈ H we have v = Pv + (I − P)v = (b, v)H a + (d, v)H c. Now we substitute b and d for
v and obtain, respectively, b = b 2
H a + (b, d)H c and d = (b, d)H a + d 2
H c. We also have
1 = (a, b)H = a2
H b 2
H +(a, c)H (b, d)H and 1 = (c, d)H = (a, c)H (b, d)H + c 2
H d 2
H ,
that is
a2
H b 2
H = 1 − (a, c)H (b, d)H = c 2
H d 2
H . (9.44)
In particular, P = I − P, which yields the claim when dim(H) = 2.
(2) Now let dim(H) > 2. We show that I − P≤P. Choose x ∈ H such that x H = 1.
Set X := span{x, Px }; then the space X, of dimension at most 2, is invariant under both P and
I − P. If dim(X) = 1, then x = Px and hence (I − P)x = 0. In the other case, dim(X) = 2,
by (1) we have (I − P)x H ≤ (I − P)|X  x H = P|X ≤P. In both cases we have
(I − P)x H ≤ Px H , that is, I − P≤P. Exchanging the roles of P and I − P yields
P≤I − P. This proves the claim. 
We are now able to prove the optimal estimate announced above.
Theorem 9.42 (Xu, Zikatanov, 2003) Let b : V × W → R be a continuous bilinear form with
continuity constant 0 < C < ∞, for which the LBB condition (9.41) holds for some constant β > 0.
Given u ∈ V let uh ∈ Vh be the solution of b(uh, wh) = b(u, wh) for all wh ∈ Wh. Then u and
uh satisfy the estimate
u − uh V ≤
C
β
inf
vh ∈Vh
u − vh V . (9.45)
Proof Define the mapping Ph : V → Vh by u → Phu := uh. Now Ph is clearly idempotent.
The LBB condition (9.41) implies that Ph  0 for all h > 0. If I − Ph = 0, then (9.45) is trivial.
Thus we may assume that I − Ph  0. In this case, by Lemma 9.41, Ph  = I − Ph . Now let
vh ∈ Vh be arbitrary. Since (I − Ph)vh = 0 it follows that u − uh V = (I − Ph)(u − vh) V ≤
I − Ph  u − vh V = Ph  u − vh V . Furthermore, by definition of Ph, for all v ∈ V
Ph v V ≤
1
β sup
wh ∈Wh
b(Phv, wh)
wh W
= 1
β sup
wh ∈Wh
b(v, wh)
wh W
≤
C
β v V .
Thus Ph  ≤ C
β , which shows the claim. 
So far the finite-dimensional subspaces Vh andWh were quite arbitrary. We now want to assume
that they approximate V and W, respectively, as h → 0+. In fact, if
for all v ∈ V we have dist(v, Vh) → 0 as h → 0+, (9.46)
then for a given f ∈ V and a solution u of (9.38) the solution uh of the discrete problem (9.40)
converges to u as h → 0+. In particular, this implies uniqueness of the solutions of (9.38). If, in
addition, we assume that
for all w ∈ W we have dist(w, Wh) → 0 as h → 0+, (9.47)
the LBB condition also implies existence for the continuous problem (9.38), as we will now see.
Theorem 9.43 Let f ∈ W
. Assume the LBB condition, along with (9.46) and (9.47). Then there
exist unique solutions u of (9.38) and uh of (9.40). Moreover, (9.45) holds, and limh→0 uh = u.358 9 Numerical methods
Proof The LBB condition implies that
uh V ≤
1
β sup
wh ∈Wh
b(uh, wh)
wh W
= 1
β sup
wh ∈Wh
|f (wh)|
wh W
≤
1
β  f W
for all h > 0. Hence (uh)h>0 is bounded. By Theorem 4.35 there exist some u ∈ V and a sequence
(hk )k∈N such that hk > 0, limk→∞ hk = 0, and uhk  u weakly in V as k → ∞. Now let w ∈ W,
then by (9.47) there exists a sequence (wk )k∈N, wk ∈ Whk , such that limk→∞ w − wk W = 0.
Hence, by the Galerkin orthogonality,
b(u, w) = lim
k→∞ b(uhk , w) = lim
k→∞ b(uhk , w − wk + wk ) = lim
k→∞ b(uhk , wk )
= lim
k→∞ f (wk ) = f (w).
This shows that u is a solution. Now Theorem 9.42 shows that (9.45) holds for u; hence also
u = limh→0 uh, since we are assuming (9.46). This implies uniqueness. 
Remark 9.44 (a) The converse of Theorem 9.43 also holds: if for each f ∈ W there exist unique
solutions uh of (9.40) and u of (9.38), respectively, and limh→0 uh = u, then the LBB
condition holds. See [8, Thm. 2.4].
(b) If V = W and b is coercive, then LBB holds for arbitrary appoximating subspaces, provided
that Vh = Wh for all h > 0. This continues to hold if (9.40) is well posed and the form is
only essentially coercive, i.e., if un  0 weakly in V and b(un, un) → 0 always implies that
un V → 0.
The converse is also true: if the form is not essentially coercive but (9.40) is well posed, then
there existVh = Wh which satisfy (9.46), but for which the LBB condition is violated, [8, Thm.
5.2]. An example of an essentially coercive form can be found by replacing the Laplacian in
the Poisson problem from Section 9.2.8 by Δ−λI for arbitrary λ ∈ R (this is also known as the
Helmholtz problem). If λ is not an eigenvalue of the Dirichlet Laplacian, then the continuous
problem is well posed. In this case, the finite element convergence theorems, Theorems 9.31
and 9.32, continue to hold, [8, Thm. 7.6].
(c) When computing an approximate solution uh of u numerically, one usually assumes the well￾posedness of (9.38). The main point of focus then typically consists of constructing LBB stable
discretizations Vh and Wh. In practice this often requires knowledge of analytic properties of
the continuous problem, and an approach adapted specifically to the problem. 
9.3.2 Further extensions
Higher orders of convergence. When dealing with convergence questions we have always restricted
ourselves to linear finite elements, the reason being that this leads to quadratic convergence in
L2(Ω), as long as the solution of the partial differential equation is in H2(Ω). We had seen that the
solutions have this degree of regularity (under certain assumptions) in Section 6.11. If one wishes for
a higher order of convergence, then one needs to use elements of higher order and, correspondingly,
make stronger regularity assumptions. Both the construction of elements of higher order and the
associated convergence analysis are technically more demanding; upon performing these steps, one
may obtain estimates of the form
u − uh H1(Ω) ≤ c · hk |u|H k+1(Ω)
, 0 < h ≤ 1, u ∈ Hk+1(Ω).
More information on such results can be found in books on the numerics of partial differential
equations, for example [16, 35, 46, 52].9.3* Extensions and generalizations 359
General L2-estimates. The above L2 error estimate can also be derived for general variational
problems. For this the following theorem is required, which may be obtained via a formalization of
the proof of Theorem 9.32, cf. [16, Thm. II.7.6].
Theorem 9.45 (Aubin–Nitsche lemma) Let V → H be a continuously imbedded and dense
subspace of a Hilbert space H, and Vh ⊂ V a finite-dimensional subspace of V. Let a(·, ·) be
bilinear, continuous and coercive. Then
u − uh H ≤ C u − uh V sup
g∈H
 1
gH
inf
χ∈Vh
ϕg − χV
'
,
where C > 0 is the continuity constant of a(·, ·), and, for given g ∈ H, ϕg, is the solution of the
dual problem a(v, ϕg) = (g, v) for all v ∈ V.
L∞-estimates. We have proved an error estimate which is “suboptimal” with respect to the supre￾mum norm. With considerably more effort it can be shown that the following estimate holds for
H2-regular problems in two-dimensional domains, cf., e.g., [21, Thm. 3.3.7]:
u − uh C(Ω) ≤ c h2 | log h|
3/2 ∇2u C(Ω)
, u ∈ C2(Ω).
More general elements. One can of course consider geometric partitions of Ω not just into triangles
but also, for example, quadrilaterals (or in the three-dimensional case tetrahedra and hexahedra).
Quadrilaterals have the advantage that one can easily construct piecewise polynomials via tensor
products; this results in simple elements and efficient computations. On the other hand, triangles
are more flexible when it comes to partitioning arbitrary polygons.
More general equations. Instead of the Poisson problem one could also consider convection￾diffusion-reaction equations (as in Theorem 5.23, for example). This results in a non-symmetric
system of equations, which needs to be solved using other numerical methods. In addition, in the
convection-dominated case numerical instabilities can arise which requires special discretizations.
Neumann and Robin boundary conditions must likewise be discretized suitably. There are other
problems of elliptic type, such as those listed in Table 2.1 (page 47), for which one can define
suitable finite elements; however, this generally requires problem-specific constructions.
Non-polygonal domains. We will close this section with two remarks on what happens when Ω is
not a polygon. In this case Ω can, obviously, no longer be partitioned exactly into triangles; at the
boundary there is a deviation. There are (at least) two strategies which may be adopted here; the
first involves considering “triangles” with curvilinear boundary, the other involves approximating
Ω by a polygon Ωh.
If ∂Ω is smooth, then one can use what are known as isoparametric elements at the boundary,
for which one side is curved. Of course, in this case the transformation onto the reference element
Tˆ will no longer be affine, which means that both the analysis and the numerical implementation
will be significantly more involved.
If Ω is convex and smooth, then the Poisson problem is H2-regular. If one chooses a quasi￾uniform family of admissible triangulations, in such a way that the vertices of the inscribed polygonal
domain of Ω lie on the boundary ∂Ω, then one can show that the geometric error which arises by
approximating Ω by Ωh, is of order of magnitude O(h2). This means that in this case one has the
same order of convergence as in the polygonal case, that is, O(h) in H1 and O(h2) in L2, [35,
§8.6]. Finally, if Ω is smooth but not convex, then the variational problem is still H2-regular (see
Remark 6.87) and one can still prove the optimal order of convergence, albeit by other methods.
Indeed, since in the non-convex case the vertices of a triangle may lie outside Ω, it follows that
Vh  V. In this case we may speak of a non-conforming partition. For such partitions other
techniques are needed for the error estimates; see for example [16, Ch. III].360 9 Numerical methods
9.4 Parabolic problems
We now wish to describe two numerical methods for parabolic equations. Here as
well we will restrict ourselves to a limited number of aspects of these methods and
refer any readers who wish for more information to [46, 52], for example. For these
problems we will require a discretization in both space and time. For the spatial
discretization, as above we will consider finite differences and finite elements.
9.4.1 Finite differences
As in the elliptic case, we will start with the finite difference method (FDM). For
this we will again consider the initial-boundary value problem for the heat equation
on the interval [0, 1], that is,
ut = uxx in (0, T)×(0, 1), (9.48a)
u(t, 0) = u(t, 1) = 0, for t ∈ [0, T], (9.48b)
u(0, x) = u0(x), x ∈ [0, 1]. (9.48c)
Here u0 ∈ C([0, 1]) should satisfy u0(0) = u0(1) = 0. By Corollary 3.37 problem
(9.48) has a unique solution u ∈ C∞ 
(0, T]×[0, 1]
	
∩ C
[0, T]×[0, 1]
	
. For the
following estimates we will need more regularity as t ↓ 0. We will thus assume
throughout this section that
u0 ∈ C4([0, 1]) with u(m)
0 (0) = u(m)
0 (1) = 0 for m = 0, 2, 4. (9.49)
In this case, we know by Theorem 3.38 that u ∈ C2,4([0, T]×[0, 1]), where as above
[0, T], T > 0, is the time interval under consideration. In order to construct a FDM
for (9.48) we clearly need a mesh in both space and time. For simplicity we choose
equidistant mesh sizes in both variables,
Δt = T
N, h = 1
M, M, N ∈ N.
Definition 9.46 (Difference operators) For brevity of notation we introduce the
following difference operators:
(a) Forward difference with respect to the time variable:
D+
Δtv(t, x) := 1
Δt

v(t + Δt, x) − v(t, x)
	
;
(b) Backward difference with respect to the time variable:
D−
Δtv(t, x) := 1
Δt

v(t, x) − v(t − Δt, x)
	
;
(c) Symmetric difference of second order with respect to the space variable:
D2
hv(t, x) := 1
h2

v(t, x + h) − 2v(t, x) + v(t, x − h)
	
. 
Using the FDM we will now determine an approximation9.4 Parabolic problems 361
Uk
i ≈ u(t
k
, xi), t
k := k Δt, xi := i h, k = 0,..., N, i = 0,..., M,
of the exact solution u of (9.48).
The explicit Euler method
To this end we discretize the time derivative u using the forward difference operator
D+
Δt and arrive at the explicit Euler method:
D+
ΔtUk
i = D2
hUk
i , 1 ≤ i ≤ M − 1, 0 ≤ k ≤ N, (9.50a)
Uk
0 = Uk
M = 0, 0 ≤ k ≤ N, (9.50b)
U0
i = u0,i = u0(xi),0 ≤ i ≤ M. (9.50c)
This system of equations for the unknowns Uk
i ∈ R can be easily solved. Firstly,
(9.50a) reads
Δt
−1 (Uk+1
i − Uk
i ) = h−2 (Uk
i+1 − 2Uk
i + Uk
i−1);
if we introduce the abbreviation
λ := Δt
h2 ,
then (9.50a) becomes
Uk+1
i = λ Uk
i+1 + (1 − 2λ)Uk
i + λ Uk
i−1 =: (EλUk )i,
complemented by the boundary conditions Uk+1
0 = Uk+1
M = 0. We can now recognize
the origin of the name “explicit”. Starting with the known initial condition (9.50c)
we can calculate Uk+1 := (Uk+1
i )0≤i≤M ∈ RM+1 directly from Uk , that is, Uk+1
is given explicitly in terms of Uk . In order to analyze (9.50) we (re-)introduce the
discrete maximum norm (in the space variables):
Uk h,∞ := max
0≤j≤M
|Uk
j | (recall h = 1
M ).
We suppose for the meantime that
0 < λ = Δt
h2 ≤
1
2
, that is, Δt ≤
1
2
h2. (9.51)
Then 1 − 2λ ≥ 0, and it follows that
EλUk h,∞ = Uk+1 h,∞ ≤ λUk h,∞ + |1 − 2λ| Uk h,∞ + λUk h,∞
= Uk h,∞ ≤ ··· ≤ U0 h,∞ ≤ u0 ∞.362 9 Numerical methods
Hence the operator Eλ is bounded (i.e. stable), with Eλ ≤ 1, in the case where
λ ≤ 1
2 .
Remark 9.47 It is not hard to see that the condition λ ≤ 1
2 is necessary for the
stability of the operator. To show this, choose the initial condition u0 so that U0
i =
(−1)
i sin(i h π), 0 ≤ i ≤ M. Then
U1
i = λU0
i+1 + (1 − 2λ)U0
i + λU0
i−1
= (−1)
i
{−λ sin((i + 1) h π) + (1 − 2λ)sin(i h π) − λ sin((i − 1) h π)}
= (−1)
i sin(i h π){−λ cos(h π) + (1 − 2λ) − λ cos(h π)}
+ (−1)
i
{−λ cos(i h π)sin(h π) + λ cos(i h π)sin(h π)}
= (1 − 2λ − 2λ cos(h π))U0
i
and, by induction Uk
i = (1 − 2λ − 2λ cos(h π))kU0
i . Now, for every λ > 1
2 we can
choose h so small that |1 − 2λ − 2λ cos(h π)| = 2λ(cos(h π) + 1) − 1 > 1, and so Uk
diverges as k → ∞. 
Remark 9.48 The condition (9.51) expresses the relationship between the respective
step sizes in time and space. Such a condition arises in general in the context
of numerical methods for time-dependent problems, not only parabolic ones (see
Section 9.5 below for the wave equation, a hyperbolic problem). It is often called
a Courant–Friedrichs–Lewy (CFL) condition. In the above case λ is also called the
CFL number. 
Analogously to the FDM for elliptic problems we define the local discretization
error as follows:
τk
i := D+
Δtuk
i − D2
huk
i , uk
i := u(t
k
, xi). (9.52)
For the analysis of the error τk we will require the following seminorm for functions
v ∈ C4([0, 1]):
|v|
C4 := sup
x∈(0,1)
|vxxxx(x)| = sup
x∈(0,1)




d4
dx4 v(x)



 = sup
x∈(0,1)
|v(4)
(x)|. (9.53)
We now have the following consistency estimate.
Theorem 9.49 Let λ ≤ 1
2 , suppose that (9.49) holds, and suppose that u ∈
C2,4([0, T]×[0, 1]) is the unique solution of (9.48). Then τk h,∞ ≤ c h2 |u0|
C4
for some constant c > 0.
Proof Performing a Taylor expansion of the exact solution u(t, x), we have that9.4 Parabolic problems 363
uk+1
i = u(t
k+1
, xi) = u(t
k
, xi) + (t
k+1 − t
k )u(t
k
, xi) +
1
2
(t
k+1 − t
k )
2
u(σk
, xi)
= uk
i + Δt u(t
k
, xi) +
1
2 (Δt)
2 u(σk
, xi)
for a suitable σk ∈ (t
k, t
k+1), and analogously
|uxx(t
k
, xi) − D2
hu(t
k
, xi)| ≤ 1
12
h2 max x∈(xi−1,xi+1)
|uxxxx(t
k
, x)|
for some ξi ∈ (xi, xi+1). Since ut = uxx, it follows that
τk
i = D+
Δtuk
i − D2
huk
i − (ut(t
k
, xi) − uxx(t
k
, xi))
= (D+
Δtu(t
k
, xi) − ut(t
k
, xi)) − (D2
hu(t
k
, xi) − uxx(t
k
, xi))
= 1
2
Δt utt(σk
, xi)−(D2
hu(t
k
, xi) − uxx(t
k
, xi)).
Since u ∈ C2,4([0, T]×[0, 1]), we have utt = (uxx)t = (ut)xx = uxxxx and hence,
since Δt ≤ 1
2 h2.
|τk
i | ≤ 1
2
Δt max t ∈(tk,tk+1)
|utt(t, xi)| +
1
12
h2 max x∈(xi−1,xi+1)
|uxxxx(t
k
, x)|
≤ c h2 max t ∈(tk,tk+1)
max x∈(xi−1,xi+1)
|uxxxx(t, x)|.
Finally, we use the smoothing property of parabolic equations, which in particular
states that |u(t, ·)|C4 ≤ |u0|
C4 for all t ∈ [0, T], as follows immediately from (8.13).
Hence τk h,∞ ≤ c h2 |u0|
C4 , which was the claim. 
With the help of the above consistency estimate we can now turn to the principal
convergence theorem.
Theorem 9.50 Let λ ≤ 1
2 , suppose that (9.49) holds, let u ∈ C2,4([0, T]×[0, 1]) be
the unique solution of (9.48), write uk := u(t
k, ·), and take Uk to be the discrete
solution of (9.50). Then there exists a constant c > 0 such that
Uk − uk h,∞ ≤ c tk h2 |u0|
C4
for all 0 ≤ k ≤ N.
Proof We define the error ek := Uk −uk . Since Uk is the discrete solution of (9.50),
it follows that D+
Δt ek
i − D2
hek
i = (D+
Δt − D2
h)Uk
i − (D+
Δt − D2
h)uk
i = −τk
i , that is,
(Δt)
−1(ek+1
i − ek
i ) = D2
hek
i − τk
i , and thus
ek+1
i = ek
i + Δt D2
hek
i − Δt τk
i = λek
i−1 + (1 − 2λ)ek
i + λek
i+1 − Δt τk
i
= (Eλek )i − Δt τk
i .364 9 Numerical methods
It follows by induction that
ek+1 = Eλek − Δt τk = Ek+1
λ e0 − Δt

k
=0
Ek−
λ τ .
Since e0 = U0 − u0 = 0, by Theorem 9.49 and the fact that Eλ ≤ 1,
ek h,∞ ≤ Δt

k−1
=0
Ek−
λ τ h,∞ ≤ Δt

k−1
=0
τ h,∞
≤ c k Δt h2 |u0|
C4 = c tk h2 |u0|
C4,
whence the claim. 
Remark 9.51 With the help of Fourier transforms and what is known as von Neumann
stability theory one can obtain L2-estimates on the errors, in particular Uk −
uk L2(0,1) ≤ c tk h2 |u0|H4(0,1), u0 ∈ H4(0, 1), cf. [46, Ch. 9]. 
The implicit Euler method
A major disadvantage of the explicit Euler method is the stability condition Δt ≤ 1
2 h2
(CFL), which in practice often leads to excessively small step sizes. Replacing (9.50a)
by
D−
ΔtUk+1
i = D2
hUk+1
i , 1 ≤ i ≤ M − 1, 0 ≤ k ≤ N − 1, (9.54)
leads to the implicit Euler method, which in full reads as follows Δt
−1 (Uk+1
i −Uk
i ) =
h2 (Uk+1
i+1 − 2Uk+1
i + Uk+1
i−1 ), that is,
(1 + 2λ)Uk+1
i − λUk+1
i+1 − λUk+1
i−1 = Uk
i . (9.55)
We thus obtain a linear system of equations involving a tridiagonal matrix whose
main diagonal entries are all 1 + 2λ, and the entries of the respective off-diagonals
above and below it are all −λ. In this case, Uk+1 is only implicitly given in terms
of Uk , even if the linear system may be simply solved in O(M) operations. We may
write (9.55) in matrix form as
BλUk+1 = Uk
, Bλ ∈ RM×M,
where Bλ is the symmetric tridiagonal matrix just described. Thus, for an appropriate
index i0, we get by (9.55)
Uk+1 h,∞ = |Uk+1
i0 | = 1
1 + 2λ


Uk
i0 + λUk+1
i0+1 + λUk+1
i0−1



≤
1
1 + 2λ

|Uk
i0 | + λ|Uk+1
i0+1| + λ|Uk+1
i0−1|
9.4 Parabolic problems 365
≤
2λ
1 + 2λ Uk+1 h,∞ +
1
1 + 2λ Uk h,∞.
Hence, we get Uk+1 h,∞ ≤ Uk h,∞ for all λ ≥ 0, which means that the method
is stable without any restriction on λ. A corresponding convergence result may be
obtained analogously to the one above; the error estimate in this case reads
Uk − uk h,∞ ≤ c tk (h2 + Δt) max
0≤t ≤tk
|u(t, ·)|C4,
meaning the method is of first order with respect to the time variable. If one wishes for
a higher order in time, one may consider, for example, the Crank–Nicolson method
or a Runge–Kutta method. We will not go into details here.
Numerical experiments
We will now illustrate the significance of the CFL condition by means of a numerical
experiment for the one-dimensional heat equation on a rod Ω = (0, 1) with end time
T = 1, right-hand side f ≡ 0 and initial condition u0(x) := x(1 − x). As t grows,
u(t, x) tends to 0. We choose different step sizes in space and time, Δt = T
N , h = 1
M .
The results of this experiment are depicted in Figure 9.16. In the left and the center
the explicit Euler method was applied; we see how sensitively the method reacts to
the CFL number λ = Δt/h2. For λ = 0.5 we see (center) that the method converges
stably, whereas if λ = 0.506 then large oscillations emerge (left, note the range up
to 107). On the right we see that the implicit Euler method also converges for very
course step sizes (here λ = 45 & 0.5).
9.4.2 Finite elements
Let Ω ⊂ R2 be a convex polygon and let 0 < T < ∞. We know from Theorem 8.35
that the inhomogeneous heat equation is well posed: that is, if u0 ∈ H1
0 (Ω) and
f ∈ C([0, T], L2(Ω)), then there exists a unique solution u ∈ L2((0, T), H2(Ω)) ∩
H1((0, T), L2(Ω)) of the problem
u(t) − Δu(t) = f (t), t ∈ (0, T), (9.56a)
u(t) ∈ H1
0 (Ω), t ∈ (0, T), (9.56b)
u(0) = u0. (9.56c)366 9 Numerical methods
-2
0 1
-1
0
107
1
0.5 0.5
2
1 0
0
0 1
0.1
0.2
0.5 0.5
0.3
1 0
-0.2
0 0
0
0.2
0.5 0.5
0.4
1 1
Fig. 9.16 Numerical solution of the heat equation via the FDM for the spatial variables, M = 30.
Left: explicit Euler method with N = 1780 (CFL λ = 0.506); center: explicit Euler method with
N = 1800 (λ = 0.5); right: implicit Euler method, N = 20 (λ = 45).
In terms of the associated bilinear form a : H1
0 (Ω) × H1
0 (Ω) → R, a(v, w) :=
(∇v, ∇w)L2(Ω), the solution u satisfies the equation
(u(t), v)L2(Ω) + a(u(t), v) = ( f (t), v)L2(Ω) (9.57)
for almost all t ∈ (0, T) and all v ∈ H1
0 (Ω).
x
t
x
t
t 0
t 1
.
.
.
t N
x0 x1 ··· x M
Fig. 9.17 Horizontal (left) and vertical (right) method of lines.
One possible strategy is to begin by discretizing in only one of the two variables,
viz. time t and space x; this is referred to as semi-discretization. First discretizing in
time and then in space is known as the horizontal method of lines, cf. Figure 9.17
(left). In this case, for each point in time t
k , k = 1,..., N, one obtains a boundary
value problem. These N problems can be solved in parallel. Here we will describe
the vertical method of lines, whereby one first discretizes the spatial domain Ω, for
example using the FEM, cf. Figure 9.17 (right). This leads to a system of ordinary9.4 Parabolic problems 367
differential equations. Let (Th)0<h≤1 be a quasi-uniform family of admissible triangu￾lations of Ω. We suppose that Th consists of Nh triangles (we are again assuming that
Ω is a polygon). Suppose that the corresponding finite element space Vh ⊂ H1
0 (Ω) is
generated by the Lagrange basis
Φh = {ϕh
1,...,ϕh
Nh }, dimVh = Nh.
Then the spatial semi-discrete problem is as follows: determine a function uh ∈
C1([0, T],Vh) such that
uh(0) = u0,h, (9.58a)
(uh(t), χ)L2(Ω) + a(uh(t), χ) = ( f (t), χ)L2(Ω), χ ∈ Vh, t > 0, (9.58b)
where u0,h ∈ Vh is an approximation of the initial condition u0. Problem (9.58) has
a unique solution uh ∈ C1([0, T],Vh). This can be shown by the classical Picard–
Lindelöf theorem on existence and uniqueness of solutions to ordinary differential
equations or by applying Theorem 8.22 to a|Vh ×Vh and Ph f ∈ C([0, T],Vh), where
Ph is the orthogonal projection of H to Vh. See also Exercise 9.18. Subsequently,
we will discretize the time in terms of finite difference approximations. This is the
reason why we need here that f is continuous in time.
Before we turn to the analysis of (9.58), we first wish to describe the form of the
semi-discrete problem more precisely. We are clearly searching for some
uh(t, x) =

Nh
i=1
αi(t)ϕh
i (x) ∈ Vh
with time-dependent coefficients αi(t), t ≥ 0. Then, setting χ = ϕh
j , (9.58b) reads

Nh
i=1
αi(t) (ϕh
i , ϕh
j )L2(Ω) +

Nh
i=1
αi(t) a(ϕh
i , ϕh
j ) = ( f (t), ϕh
j )L2(Ω), 1 ≤ j ≤ Nh,
and the initial conditions αi(0) = γi are given by u0,h = 
Nh
i=1 γiϕh
i . We define
the stiffness matrix Ah := (a(ϕh
i , ϕh
j ))i,j=1,...,Nh ∈ RNh×Nh , take the right-hand
side bh(t) := ( f (t), ϕh
j )L2(Ω))j=1,...,Nh ∈ RNh and introduce what is known as the
mass matrix Mh := ((ϕh
i , ϕh
j )L2(Ω))i,j=1,...,Nh ∈ RNh×Nh ; then our problem may be
represented in matrix form as
Mhαh(t) + Ahαh(t) = bh(t), t > 0, αh(0) = γ. (9.59)
We thus have a system of ordinary differential equations of dimension Nh. We will
come to the discretization of this system somewhat later; we first observe that Ah and
Mh are symmetric and positive definite, and so in particular Mh is regular. Hence
(9.59) reads
α h(t) + M−1
h Ahαh(t) = M−1
h bh(t), t > 0, αh(0) = γ.368 9 Numerical methods
This initial value problem has a unique solution αh ∈ C1([0, T];Rn) by the clas￾sical Picard–Lindelöf theorem; see also Exercise 9.18. We next perform a stability
analysis.
Lemma 9.52 We have the stability estimate
uh(t)L2(Ω) ≤ u0,h L2(Ω) +
∫ t
0
 f (s)L2(Ω) ds, t ∈ [0, T]. (9.60)
Proof We take vh = uh(t) as a test function in (9.58b) and obtain
(uh(t), uh(t))L2(Ω) ≤ (uh(t), uh(t))L2(Ω) + a(uh(t), uh(t))
= ( f (t), uh(t))L2(Ω).
For ε > 0 let gε(t) := (uh(t)2
L2(Ω)
+ε)
1/2. Then, gε ∈ C1([0, T]) and by Lemma 8.17
and the Cauchy–Schwarz inequality we have
g
ε(t) = (uh(t), uh(t))L2(Ω)
gε(t) ≤ ( f (t), uh(t))L2(Ω)
gε(t) ≤  f (t)L2(Ω)
uh(t)L2(Ω)
gε(t)
≤  f (t)L2(Ω)
Thus,
gε(t) = gε(0) +
∫ t
0
g
ε(s) ds ≤ gε(0) +
∫ t
0
( f (s)L2(Ω) ds
Letting ε ' 0 gives the desired inequality. 
The following operator will be extremely useful in the error analysis. We call
Rh : H1
0 (Ω) → Vh, defined by
a(Rhv, χ) = a(v, χ), v ∈ H1
0 (Ω), χ ∈ Vh, (9.61)
the Ritz projection. This is clearly the orthogonal projection onto Vh with respect
to the energy inner product a(u, v) := (∇u, ∇v)L2(Ω) (for the equivalence of the
induced norm, see Corollary 6.33). This projection also appeared in the proof of
Theorem 9.42. The error analysis will require the following estimates. As with the
elliptic equations we will restrict ourselves to the case of linear elements, that is, to
Vh as in (9.37).
Theorem 9.53 Let Ω be a convex polygon. For the space Vh of linear finite elements
as in (9.37) we have
(a) Rhv − v L2(Ω) ≤ Chv H1(Ω), v ∈ H1
0 (Ω),
(b) Rhv − v L2(Ω) ≤ Ch2 v H2(Ω), v ∈ H1
0 (Ω) ∩ H2(Ω),
where the constant C > 0 depends only on Ω and κ.9.4 Parabolic problems 369
Proof (a) Let v ∈ H1
0 (Ω) and choose g ∈ L2(Ω), gL2(Ω) = 1, such that Rhv −
v L2(Ω) = (Rhv − v, g)L2(Ω). Given this g, choose w ∈ H1
0 (Ω) ∩ H2(Ω) for which
−Δw = g, and let wh ∈ Vh be the corresponding discrete solution, i.e. a(wh, χ) =
(g, χ)L2(Ω), χ ∈ Vh. By the definition of Rh in (9.61) we have a(v − Rhv, χ) = 0,
v ∈ H1
0 (Ω), χ ∈ Vh, that is, the Galerkin orthogonality. Hence, using −Δu = g and
the test function χ = Rhv − v,
Rhv − v L2(Ω) = (Rhv − v, g)L2(Ω) = a(Rhv − v, w) = a(Rhv − v, w − wh)
≤ |Rhv − v|H1(Ω) w − wh H1(Ω) ≤ c1h|Rhv − v|H1(Ω),
where c1 > 0 is the constant in the error estimate of Theorem 9.31. Finally, I − Rh
is a contraction with respect to |·|H1(Ω) (as an orthogonal projection with respect to
a(·, ·)); hence |Rhv − v|H1(Ω) ≤ |v|H1(Ω). This yields the claim.
(b) Now let v ∈ H1
0 (Ω) ∩ H2(Ω), set f := −Δv, and consider the corresponding
discrete solution vh ∈ Vh, that is, a(vh, χ) = ( f, χ)L2(Ω), χ ∈ Vh. Since Rh is the
a-orthogonal projection of H1
0 (Ω) onto Vh, it follows that
|Rhv − v|H1(Ω) = |Rh(v − vh)−(v − vh)|H1(Ω) ≤ |vh − v|H1(Ω)
≤ c h f L2(Ω) ≤ c c2 hv H2(Ω),
where c is the constant from Theorem 9.31 and c2 the one of Theorem 6.86. Combin￾ing this with the estimate from part (a), we have Rhv−v L2(Ω) ≤ c1h|Rhv−v|H1(Ω) ≤
c c1 c2 h2 v 2
H2(Ω)
. 
We can now prove the desired error estimate for the semi-discrete approximation.
Theorem 9.54 Let 0 < T < ∞ and let Ω be a convex polygon. Moreover, let
f ∈ L2((0, T), H1
0 (Ω) ∩ H2(Ω)) ∩ H1((0, T), L2(Ω)), and let u0 ∈ H1
0 (Ω) ∩ H2(Ω)
such that Δu0 ∈ H1
0 (Ω). Finally, let u0,h ∈ Vh and suppose that uh is the solution of
(9.58). Then the following error estimate holds for the exact solution u of (9.56):
uh(t) − u(t)L2(Ω) ≤ u0,h − u0 L2(Ω)
+ c h2

u0 H2(Ω)+
√
t

Δu0 H1(Ω) +  f L2((0,t),H2(Ω))	
'
for all t∈ [0, T] and a constant c > 0 depending only on Ω and κ.
Proof Recall from Theorem 8.36 that by our assumptions we have that u ∈
H1((0, T), H2(Ω) ∩ H1
0 (Ω)) ∩ H2((0, T), L2(Ω)) ∩ C1([0, T], H1
0 (Ω)). In particular
u ∈ C([0, T], H2(Ω) ∩ H1
0 (Ω)). We rewrite the error uh − u using the Ritz projection
as
uh − u = (uh − Rhu) + (Rhu − u) =: θh + ρh. (9.62)370 9 Numerical methods
For the second term in (9.62) we may use the error estimate of Theorem 9.53 (b) for
Rh; let t ∈ [0, T], since u(t) ∈ H1
0 (Ω) ∩ H2(Ω) there exists a constant c > 0 such that
ρh(t)L2(Ω) = Rhu(t) − u(t)L2(Ω)
≤ c h2 u(t)H2(Ω) = c h2
"
"
"
"u(0) +
∫ t
0
u(s) ds
"
"
"
"
H2(Ω)
≤ c h2

u0 H2(Ω) +
∫ t
0
u(s)H2(Ω) ds
≤ c h2(u0 H2(Ω) + √
t uL2((0,t),H2(Ω))),
where we have used Theorem 8.29 (b) and Hölder’s inequality in the last step. We
now use Theorem 8.36 (a) (for T = t) to obtain
uL2((0,t),H2(Ω)) ≤ c (Δu0 2
H1(Ω) +  f 2
L2((0,t),H2(Ω)))
1/2
≤ c (Δu0 H1(Ω) +  f L2((0,t),H2(Ω))),
whence (with a possibly different constant c)
ρh(t)L2(Ω) ≤ c h2

u0 H2(Ω)+
√
t

Δu0 H1(Ω) +  f L2((0,t),H2(Ω)
	
.
For the first term θh = uh − Rhu in (9.62) we consider a related initial boundary
problem. Namely, for χ ∈ Vh we have
(θ
h(t), χ)L2(Ω) + a(θh(t), χ) =
=(uh(t), χ)L2(Ω) + a(uh(t), χ) − d
dt
Rhu(t), χ
L2(Ω)
− a(Rhu(t), χ)
= ( f (t), χ)L2(Ω) − (Rhu(t), χ)L2(Ω) − a(u(t), χ)
= (u(t) − Rhu(t), χ)L2(Ω)
by properties of the Ritz projection and since u is a solution of (9.56). Due to
Theorem 8.36, we have u ∈ C([0, T]; H1
0 (Ω)), which allows us to apply Rh to u. But
this equation says that
(θ
h(t), χ)L2(Ω) + a(θh(t), χ) = −(ρh(t), χ)L2(Ω), χ ∈ Vh. (9.63)
Hence the two terms in (9.62) are related. Since u ∈ C1([0, T], H1
0 (Ω)), we get
ρh ∈ C1([0, T], H1
0 (Ω)). Thus, we can apply the stability estimate (9.60) to (9.63) to
obtain
θh(t)L2(Ω) ≤ θh(0)L2(Ω) +
∫ t
0
ρh(s)L2(Ω) ds.
To deal with the term in the integral, we apply the error estimate in Theorem 9.53
(b) for the Ritz projection. Since u(t) ∈ H1
0 (Ω) ∩ H2(Ω), Theorem 9.53 (b) yields for9.4 Parabolic problems 371
all t ∈ [0, T]
ρh(t)L2(Ω) = Rhu(t) − u(t)L2(Ω) ≤ ch2 u(t)H2(Ω).
We also have by our assumption that u0 ∈ H1
0 (Ω)∩H2(Ω), so that again Theorem 9.53
(b) yields
θh(0)L2(Ω) = uh(0) − Rhu(0)L2(Ω) = u0,h − Rhu0 L2(Ω)
≤ u0,h − u0 L2(Ω) + u0 − Rhu0 L2(Ω)
≤ u0,h − u0 L2(Ω) + ch2 u0 H2(Ω).
The claim now follows upon putting all this together. 
The above proof also yields a result which can be found, e.g., in [46, Thm. 10.4]
or in slightly different form in [34, Thm. 5.10].
Corollary 9.55 Under the assumptions of Theorem 9.54 we have for all t ∈ [0, T]
uh(t) − u(t)L2(Ω) ≤ u0,h − u0 L2(Ω)
+ c h2

u0 H2(Ω) +
∫ t
0
u(s)H2(Ω) ds'
.
Remark 9.56 Both estimates derived above involve u0,h − u0 L2(Ω), the initial error.
So far, we did not specify the choice of the initial value u0,h. At least three appropriate
choices are useful: (i) Interpolation using the Clément operator in (9.36), (ii) the Ritz
projection (9.61) or (iii) the orthogonal projection i.e., (u0,h, χ)L2(Ω) = (u0, χ)L2(Ω)
for all χ ∈ Vh. Recalling Theorem 9.27, Theorem 9.53 or standard estimates for
the orthogonal projection (also known as Bramble-Hilbert lemma, see e.g. [16, Ch.
II,§6]) we have in all three cases that there exists a constant c > 0 such that
u0,h − u0 L2(Ω) ≤ ch2 u0 H2(Ω), u0 ∈ H1
0 (Ω) ∩ H2(Ω). (9.64)
This means that the term u0,h − u0 L2(Ω) is dominated by the remaining quantities
in the estimates in Theorem 9.54 or Corollary 9.55. Hence, we can eliminate the
initial error from the estimates as long as u0,h is chosen to satisfy (9.64). 
The implicit Euler method
The final step is a discretization of the time variable in the initial value problem for
the ordinary differential equation given in (9.59). We already saw with the FDM that
the explicit Euler method is only stable under restrictive assumptions on the step
size in time and space; here we will limit ourselves to the implicit Euler method
and a fixed step size Δt, that is, t
k = k Δt, k = 0, ..., N = T
Δt . In this case the time
discretization of (9.58) reads372 9 Numerical methods
(D−
ΔtUk
, χ)L2(Ω) + a(Uk
, χ) = ( f (t
k ), χ)L2(Ω), χ ∈ Vh, (9.65a)
U0 = u0,h. (9.65b)
Here Uk ∈ H1
0 (Ω), k = 1, ..., N and the difference operators are defined as follows
(see also Definition 9.46).
Definition 9.57 (Difference operators) For a sequence (Uk )k=0,...,N of vectors we
define (a) the forward difference as D+
Δt
Uk := 1
Δt

Uk+1 − Uk 	
, k = 0, ..., N − 1, and
(b) the backward difference by D−
Δt
Uk := 1
Δt

Uk − Uk−1	
, k = 1, ..., N. 
Thus (9.65a) reads for for k = 1, ..., N,
(Uk
, χ)L2(Ω) + Δt a(Uk
, χ) = (Uk−1 + Δt f (t
k ), χ)L2(Ω), χ ∈ Vh. (9.66)
Here we suppose that f ∈ C([0, T], L2(Ω)). We seek Uk of the form Uk (x) = 
Nh
i=1 αk
i ϕh
i (x) ∈ Vh, in which case (9.66) becomes

Nh
i=1
αk
i (ϕh
i , ϕh
j )L2(Ω) + Δt

Nh
i=1
αk
i a(ϕh
i , ϕh
j ) =
=

Nh
i=1
αk−1
i (ϕh
i , ϕh
j )L2(Ω) + Δt ( f (t
k ), ϕh
j )L2(Ω)
for j = 1,..., Nh. In matrix form this reads
(Mh + Δt Ah)αk
h = Mhαk−1
h + Δt bh(t
k ),
where αk
h = (αk
i )1≤i≤Nh ∈ RNh , Mh and Ah are the mass and stiffness matrices,
respectively, and the right-hand side bh(t
k ) is in RNh . Since Mh + Δt Ah is s.p.d.
(and thus of course regular), (9.65) has a unique solution. We start with a stability
result.
Lemma 9.58 For all k = 1, ..., N, we have the estimate
Uk L2(Ω) ≤ U0 L2(Ω) + Δt

k
j=1
 f (t
j
)L2(Ω).
Proof Choose χ = Uk in (9.66), then
Uk 2
L2(Ω) + Δt a(Uk
, Uk ) = (Uk−1
, Uk )L2(Ω) + Δt ( f (t
k ), Uk )L2(Ω)
≤ Uk L2(Ω)

Uk−1 L2(Ω) + Δt  f (t
k )L2(Ω)
	
.
Since a(Uk, Uk ) ≥ 0, it follows that
Uk L2(Ω) ≤ Uk−1 L2(Ω) + Δt  f (t
k )L2(Ω).9.4 Parabolic problems 373
We now apply this inequality recursively to obtain
Uk L2(Ω) ≤ Uk−2 L2(Ω) + Δt ( f (t
k−1)L2(Ω) +  f (t
k )L2(Ω))
≤ ··· ≤ U0 L2(Ω) + Δt

k
j=1
 f (t
j
)L2(Ω),
as claimed. 
As with the FDM we see that the stability result for the implicit Euler method
holds with no restrictions on Δt or h. We now come to the error estimate in this case.
Of course, this estimate depends on the solution. But given f and u0 it says that the
error is quadratic in h, but only linear in Δt.
Theorem 9.59 Let 0 < T < ∞ and let Ω ⊂ Rd be open, bounded and convex. Let
u0 ∈ H1
0 (Ω) ∩ H2(Ω) be such that Δu0 ∈ H1
0 (Ω), and let f ∈ L2((0, T), H2(Ω)) ∩
H1((0, T), L2(Ω)) be such that f (t) ∈ H1
0 (Ω) for all t ∈ [0, T]. Finally, let u be the
corresponding solution of (9.56) and u0,h ∈ Vh satisfy (9.64). There exists a constant
depending only on Ω such that for all k = 1, ..., N
Uk − u(t
k )L2(Ω) ≤ c h2

u0 H2(Ω) +
∫ tk
0
u(s)H2(Ω) ds'
+ c Δt
∫ tk
0
u(s)L2(Ω) ds.
Proof The assumptions on f imply that f ∈ C([0, T], H1
0 (Ω)). By Theorem 8.36,
we have that u ∈ H1((0, T), H2(Ω)) ∩ H2((0, T), L2(Ω)), in particular we obtain
u ∈ C([0, T], H2(Ω)). Moreover, u ∈ C1([0, T], H1
0 (Ω)) by the last assertion of
Theorem 8.36. Analogously to the proof of Theorem 9.54, we set for k = 1, ..., N
Uk − u(t
k ) = (Uk − Rhu(t
k )) + (Rhu(t
k ) − u(t
k )) =: θk
h + ρk
h
and estimate each of the latter two terms separately. Firstly, by Theorem 9.53,
ρk
h L2(Ω) = Rhu(t
k ) − u(t
k )L2(Ω) ≤ c h2 u(t
k )H2(Ω)
= c h2
"
"
"
"u0 +
∫ tk
0
u(s) ds
"
"
"
"
H2(Ω)
≤ c h2

u0 H2(Ω) +
∫ tk
0
u(s)H2(Ω) ds'
.
We shall derive an initial value problem for θk
h, just as we did earlier. We have, for
all χ ∈ Vh,374 9 Numerical methods
(D−
Δt θk
h, χ)L2(Ω) + a(θk
h, χ) =
= (D−
ΔtUk
, χ)L2(Ω) + a(Uk
, χ)−(D−
ΔtRhu(t
k ), χ)L2(Ω) − a(Rhu(t
k ), χ)
= ( f (t
k ), χ)L2(Ω) − (D−
ΔtRhu(t
k ), χ)L2(Ω) − a(u(t
k ), χ)
= (u(t
k ) − D−
ΔtRhu(t
k ), χ)L2(Ω) =: −(ωk
, χ)L2(Ω),
where the right-hand side is
ωk = RhD−
Δtu(t
k ) − u(t
k ) = (Rh − I)D−
Δtu(t
k ) + 
D−
Δtu(t
k ) − u(t
k )
	
=: ωk
1 + ωk
2 .
We proceed as we did previously, this time using Lemma 9.58:
θk
h L2(Ω) ≤ θ0
h L2(Ω) + Δt

k
j=1
ωj
L2(Ω)
≤ θ0
h L2(Ω) + Δt

k
j=1
ωj
1 L2(Ω) + Δt

k
j=1
ωj
2 L2(Ω).
In order to estimate the term involving ωj
1 we use the identity
ωj
1 = (Rh − I)D−
Δtu(t
j
) = (Rh − I) 1
Δt
∫ t j
t j−1
u(s) ds
= 1
Δt
∫ t j
t j−1
(Rh − I)u(s) ds,
that is,
Δt

k
j=1
ωj
1 L2(Ω) ≤

k
j=1
∫ t j
t j−1
(Rh − I)u(s)L2(Ω) ds
=
∫ tk
0
(Rh − I)u(s)L2(Ω) ds ≤ c h2
∫ tk
0
u(s)H2(Ω) ds
by Theorem 9.53. For the second term we use that u ∈ H2((0, T), L2(Ω)) and Theo￾rem 8.29
ωj
2 = u(t j
) − u(t j−1)
Δt − u(t
j
) = − 1
Δt
∫ t j
t j−1
(s − t
j−1) u(s) ds.
It follows that
Δt

k
j=1
ωj
2 L2(Ω) ≤

k
j=1
"
"
"
∫ t j
t j−1
(s − t
j−1) u(s) ds
"
"
"
L2(Ω)9.4 Parabolic problems 375
≤ Δt
∫ tk
0
u(s)L2(Ω) ds.
From these estimates we obtain
θk
h L2(Ω) ≤ θ0
h L2(Ω) + c h2
∫ tk
0
u(s)H2(Ω) ds + Δt
∫ tk
0
u(s)L2(Ω) ds.
Finally θ0
h L2(Ω) = U0 − Rhu(0)L2(Ω) = u0,h − Rhu0 L2(Ω) ≤ u0,h − u0 L2(Ω) +
u0 − Rhu0 L2(Ω) ≤ c h2 u0 H2(Ω) by (9.64) and Theorem 9.53. Combining all these
estimates yields the claim. 
The Crank–Nicolson method
The estimate of Theorem 9.59 reveals an essential disadvantage of the Euler method:
it is of second order in the space variable (h2) but only first order in time (Δt). As
already mentioned (albeit not in detail) in Section 9.4.1 (FDM) this disadvantage can
be overcome using the Crank–Nicolson method. We are now going to describe this
method, which can also analogously be used with the FDM in space. To this end, we
expand the semi-discrete equation symmetrically about the point t
k−1/2 := (k − 1
2 )Δt
and obtain
(D−
ΔtUk
, χ)L2(Ω) + a( 1
2 (Uk + Uk−1), χ) = ( f (t
k−1/2
), χ)L2(Ω), χ ∈ Vh, (9.67a)
U0 = u0,h, (9.67b)
or, for k = 1, ..., N,
(Uk
, χ)L2(Ω) + Δt
2 a(Uk, χ) = (Uk−1 + Δt f (t
k−1/2), χ)L2(Ω) − 1
2 a(Uk−1
, χ) (9.68)
for all χ ∈ Vh. In matrix form this reads
(Mh + Δt
2 Ah)αk
h = (Mh − 1
2 Ah)αk−1
h + Δt bh(t
k−1/2),
where αk
h = (αk
i )1≤i≤Nh ∈ RNh is the unknown vector, Mh and Ah are, again,
mass and stiffness matrices, respectively, and the right-hand side bh(t
k−1/2) ∈ RNh ,
analogously to the Euler method. One can obtain a stability result analogous to
Lemma 9.58 upon using χ = Uk + Uk−1 as a test function in (9.67). Indeed,
firstly, Δt (D−
Δt
Uk, Uk + Uk−1)L2(Ω) = Uk 2
L2(Ω) − Uk−1 2
L2(Ω) = (Uk L2(Ω) −
Uk−1 L2(Ω)) (Uk L2(Ω) + Uk−1 L2(Ω)). It now follows from (9.67) that
1
Δt
(Uk L2(Ω) − Uk−1 L2(Ω)) (Uk L2(Ω) + Uk−1 L2(Ω)) ≤
≤
1
Δt
(Uk L2(Ω) − Uk−1 L2(Ω)) (Uk L2(Ω) + Uk−1 L2(Ω))376 9 Numerical methods
+ 1
2 a(Uk + Uk−1
, Uk + Uk−1)
= ( f (t
k−1/2
), Uk + Uk−1)L2(Ω) ≤  f (t
k−1/2)L2(Ω) Uk + Uk−1 L2(Ω)
≤  f (t
k−1/2)L2(Ω)(Uk L2(Ω) + Uk−1 L2(Ω)),
that is, 1
Δt (Uk L2(Ω) − Uk−1 L2(Ω))≤ f (t
k−1/2)L2(Ω). This is equivalent to the
inequality Uk L2(Ω) ≤ Uk−1 L2(Ω) + Δt  f (t
k−1/2)L2(Ω), and so
Uk L2(Ω) ≤ U0 L2(Ω) + Δt

k
j=1
 f (t
j−1/2)L2(Ω).
The following error estimate can be proved analogously to Theorem 9.59, cf. [46].
Theorem 9.60 Let u0 ∈ H1
0 (Ω) ∩ H2(Ω) and f be such that the solution u of (9.56)
satisfies the condition u ∈ H2((0, T), H2(Ω)) ∩ H3((0, T), L2(Ω)). Assume that u0,h ∈
Vh satisfies (9.64). Then there exists a constant c > 0 depending only on Ω such that
for all k = 0, ..., N
Uk − u(t
k
, ·)L2(Ω) ≤ c h2

u0 H2(Ω) +
∫ tk
0
uH2(Ω) ds!
+ c (Δt)
2
∫ tk
0
(u*(s)L2(Ω) + Δu(s)L2(Ω)) ds.
Numerical experiments. We conclude this section with an experiment related to the
Crank–Nicolson method. Here we will choose the data of the initial-boundary value
problem in such a way that we can specify an exact solution and, correspondingly,
determine the right-hand side exactly. This means we can calculate all the errors
exactly and plot these against the number of unknowns, Nh (cf. Figure 9.18). Here
we are also choosing Δt = h. The respective linear and quadratic orders, which arise
exclusively from the discretization in time, are clearly recognizable.
9.4.3* Error estimates via space/time variational formulations
We showed in Section 8.6* that we can interpret parabolic initial-boundary value problems as
variational problems of the form
find u ∈ V such that b(u, w) = f (w), ∀w ∈ W, (9.69)
with distinct trial and test spaces
V := H1
(0)
(I,V
) ∩ L2(I,V) = {v ∈ H1(I, V
) ∩ L2(I, V): v(0) = 0} and W := L2(I,V).
Here I = (0, T) for some T > 0 and V, H are given separable Hilbert spaces such that V is
continuously and densely imbedded in H. Let a : V × V → R be a continuous, coercive bilinear9.4 Parabolic problems 377
101 102
10−5
10−4
10−3
1/ 2 1
1
Crank–Nicolson
Implicit Euler
Fig. 9.18 Convergence history for the Crank–Nicolson and implicit Euler methods for the heat
equation.
form and let b : V×W → R be given by (8.56). We saw in Section 8.6* that this space/time bilinear
form is continuous (with constant C) and that (9.69) is well posed. Moreover, in Remark 8.39 it was
proved that an inf-sup condition is satisfied for some constant β; both constants are independent of
T. Hence we can use the Xu–Zikatanov theorem, Theorem 9.42, to derive an error estimate for a
corresponding Petrov–Galerkin approximation. We now introduce a special discretization and show
that this is equivalent to the Crank–Nicolson method. This means that we can use the space/time
formulation to give another error estimate (which will turn out to be sharper than the one above).
Let Vδ ⊂ V, Wδ ⊂ W be finite-dimensional spaces and let uδ ∈ Vδ be a solution of the
discrete problem
b(uδ, wδ ) = f (wδ ), ∀wδ ∈ Wδ, (9.70)
where we restrict ourselves to the case H = L2(Ω), V = H1
0 (Ω). Let Vδ := SΔt ⊗ Vh, Wδ :=
QΔt ⊗ Vh,
δ := (Δt, h), (9.71)
where SΔt and Vh are finite element spaces spanned by piecewise linear functions and QΔt a finite
element space spanned by piecewise constant functions with respect to triangulations Tspace
h in
space and Ttime
Δt ≡ {t k−1 ≡ (k − 1)Δt < t ≤ k Δt ≡ t k, 1 ≤ k ≤ K } in time, Δt := T/K, as
appropriate.
We have SΔt = span {σ1
,...,σK }, where σk is the (interpolating) hat function with re￾spect to the nodes t k−1, t k und t k+1 (truncated to [0, T] when k = K), and also QΔt =
span{τ1
,...,τK }, where τk = χI k is the characteristic function on I k := (t k−1
, t k ) . Finally,
take Vh = span {φ1,...,φnh } to be the nodal basis with respect to Tspace
h . Given functions
vδ = 
K
k=1

nh
i=1 vk
i σk ⊗ φi ∈ Vδ and wδ = 
K
=1

nh
j=1 w
j τk ⊗ φj (with coefficients vk
i and w
j ,
respectively), we obtain
b(vδ, wδ ) =
∫
I
#
 vδ (t), wδ (t)V ×V + a(vδ (t), wδ (t))$
dt
=
K
k,=1
nh
i, j=1
vi
kwj

#
( σk, τ )L2(I ) (φi, φj )H + (σk, τ )L2(I ) a(φi, φj )
$
= vT
δ Bδwδ,378 9 Numerical methods
where
Bδ := Ntime
Δt ⊗ Mspace
h + Mtime
Δt ⊗ Aspace
h (9.72)
and Mspace
h := [(φi, φj )L2(Ω)]i, j=1, . . .,nh , Mtime
Δt := [(σk, τ )L2(I )]k,=1, . . ., K are the respective
mass matrices with respect to time and space, and the stiffness matrices are given by Ntime
Δt :=
[( σk, τ )L2(I )]k,=1, . . ., K and Aspace
h := [a(φi, φj )]i, j=1, . . .,nh . For our special choice of spaces
we obtain, denoting by δk, the discrete Kronecker delta,
( σk, τ )L2(I ) = δk, − δk+1,, (σk, τ )L2(I ) = Δt
2 (δk, + δk+1, ),
b(vδ, τ ⊗ φj ) =
nh
i=1
%
(v
i − v−1 i )(φi, φj )H + Δt
2 (v
i + v−1 i ) a(φi, φj )
&
= Δt
%
Mspace
h
1
Δt
(v − v−1) + Aspace
h v−1/2&
,
where v := (v
i )i=1, . . .,nh , v−1/2
i := 1
2 (v
i + v−1 i ), and correspondingly for v−1/2. Now we apply
the trapezoidal rule to approximate the integral with respect to the time variable on the right-hand
side,
f (τ ⊗ φj ) =
∫ T
0
f (t), τ ⊗ φj V×V dt
≈ Δt
2 f (t −1) + f (t  ), φj V×V = Δt
2 (f −1 + f  )j = Δt f −1/2
j ,
where f  := ( f (t  ), φj V×V )j=1, . . .,nh . Now we can write (9.70) as
1
Δt
Mspace
h (v − v−1) + Aspace
h v−1/2 = f −1/2
, v0 := 0, (9.73)
which is exactly the Crank–Nicolson method introduced above. We can thus use the space/time
variational formulation to derive error estimates for the Crank–Nicolson method.
Remark 9.61 We can also prove the inf-sup condition. To this end we consider a slightly modified
norm: given v ∈ V, we set v¯k := (Δt)
−1 ∫
I k v(t) dt ∈ V and v¯ := 
K
k=1 χI k ⊗ v¯i ∈ L2(I,V),
as well as |||v |||2
V, δ :=  v 2
L2(I,V) + v¯ 2
L2(I,V ) + v(T) 2
H . This averaging in time is in fact the
“natural” norm for the analysis of the Crank–Nicolson method. For the inf-sup and continuity
constants (recall the definition of δ in (9.71))
βδ := inf
vδ ∈Vδ
sup
wδ ∈Wδ
b(vδ, wδ )
|||vδ |||V, δ wδ W
, γδ := sup
vδ ∈Vδ
sup
wδ ∈Wδ
b(vδ, wδ )
|||vδ |||V, δ wδ W
,
we have βδ = γδ = 1, as long as a(·, ·) is assumed to be symmetric. We use the energy norm
φ2
V := a(φ, φ), φ ∈ V, [60]. A comprehensive treatment of stable space/time discretizations for
parabolic problems can be found in [4]. 
We now come to the error estimate announced above. For this, we suppose Ω to be convex. We
recall that Vδ = SΔt ⊗ Vh is a tensor product, and that both SΔt and Vh are finite element spaces.
Both admit corresponding Ritz projections as in (9.61); we shall call these RΔt : H1
(0)
(I) → SΔt
and Rh : H1
0 (Ω) → Vh, respectively. Then by Theorem 9.53,
RΔtσ − σ L2(I ) ≤ C2(Δt)
2 σ H2(I ), σ ∈ H1
(0)
(I) ∩ H2(I), (9.74a)
Rhφ − φL2(Ω) ≤ C2h2 φH2(Ω), φ ∈ H1
0 (Ω) ∩ H2(Ω). (9.74b)9.5 The wave equation 379
We now define the space/time Ritz projection Rδ : V → Vδ by Rδ := RΔt ⊗ Rh. Then we have,
for all v ∈ V,
Rδ v − v = (RΔt ⊗ Rh)v − (I ⊗ Rh)v + (I ⊗ Rh)v − v
= [(RΔt − I) ⊗ Rh]v + [I ⊗ (Rh − I)]v,
and we obtain the estimates Rδ v − v L2(I, L2(Ω)) ≤ C((Δt)
2 + h2) v H2(I, H2(Ω)) for all v ∈
V ∩ H2(I,H2(Ω)). Upon applying Theorems 9.42 and 9.45 (Xu–Zikatanov and the Aubin–Nitsche
lemma, respectively), we obtain the following result.
Theorem 9.62 Let u0 and f be such that the solution u of (9.56) satisfies u ∈ H2(I,H2(Ω)). There
exists a constant c > 0 depending only on Ω such that
u − uδ L2(I, L2(Ω)) ≤ c((Δt)
2 + h2) u H2(I, H2(Ω)),
u − uδ V ≤ c(Δt + h) u H2(I, H2(Ω))
(with δ = (Δt, h) as in (9.71)).
This estimate is better than the one of Theorem 9.60 in at least two respects. Firstly, it is stated in
terms of the “natural” norm, that is, the norm which matches the statement of the problem; secondly,
we obtain the same order under weaker regularity assumptions. Extensive numerical experiments
concerning the required regularity as well as the efficiency of space-time discretizations compared
with standard time-marching schemes are described in [37].
9.5 The wave equation
In this section we will describe elementary numerical methods for the solution of the
wave equation; we refer to Section 8.4 for the corresponding theory. As we did for
the heat equation, we will apply a semi-discretization utilizing the vertical method
of lines (Figure 9.17), which will lead to a system of initial value problems for
ordinary differential equations. We will then discretize the second time derivative
using the central difference quotients D2
Δt := D−
ΔtD+
Δt with respect to an equidistant
time discretization of mesh size Δt = T
N and nodes t
k := k Δt, k = 0,..., N in the
time interval [0, T]. For the space discretization we will consider finite differences
and finite elements, as above.
9.5.1 Finite differences
To keep things simple, we will consider the case of one space dimension, that is, the
initial-boundary value problem
utt − c2uxx = f in (0, T)×(0, 1), (9.75a)
u(t, 0) = u(t, 1) = 0 for all t ∈ (0, T), (9.75b)
u(0, x) = u0(x), ut(0, x) = u1(x) for all x ∈ (0, 1), (9.75c)380 9 Numerical methods
for given continuous functions u0, u1 : [0, 1] → R and f : [0, T]×[0, 1] → R
(we assume in particular that u0(0) = u0(1) = 0). Uniqueness was established in
Theorem 3.6 (page 54); that is, there exists at most one solution u ∈ C2([0, T]×[0, 1])
of (9.75). In the subsequent convergence analysis of various numerical schemes we
sometimes require higher regularity of the solution u of (9.75), which can often be
ensured by assuming appropriate properties of the data u0, u1 and f .
An explicit scheme: The leapfrog method
If we also approximate the second derivative in the space variable using the central
difference quotient at time t
k−1, on an equidistant mesh of size h := 1
M , M ∈ N, then
we obtain the discrete scheme
(Δt)
−2(Uk
i − 2Uk−1
i + Uk−2
i ) − c2h−2(Uk−1
i−1 − 2Uk−1
i + Uk−1
i+1 ) = f k−1
i (9.76)
(where f k
i := f (t
k, xi)) for determining an approximation Uk
i to uk
i := u(t
k, xi). We
still need the initial conditions for k = 0 and k = 1. A canonical choice for the initial
value is U0
i := u0(xi). Since we are using central difference quotients of second
order, we also require an approximation of second order to u1
i . This can be obtained
via a Taylor expansion
u(t
1
, xi) = u(0, xi) + Δt ut(0, xi) + (Δt)
2
2 utt(0, xi) + O((Δt)
3
)
= u0(xi) + Δt u1(xi) + (Δt)
2
2

c2uxx(0, xi) + f (0, xi)
	
+ O((Δt)
3
);
then, setting
λ := Δt
h ,
we have the desired O((Δt)
2)-approximation given by
U1
i := u0(xi) + Δt u1(xi) + c2
2 λ2{u0(xi−1) − 2u0(xi) + u0(xi+1)}
+ (Δt)
2
2 f 0
i . (9.77)
As for elliptic and parabolic problems, here we have defined the local discretiza￾tion error by
τk
i := f (t
k
, xi) − D2
Δtuk
i + c2 D2
huk
i , τk := (τk
i )i=1,...,M−1. (9.78)9.5 The wave equation 381
Theorem 9.63 (Consistency) Suppose the CFL condition λ ≤ c−1 holds, and let
u ∈ C4,4([0, T]×[0, 1]) be the unique solution of (9.75) and U the solution of (9.76)
with (9.77) and U0
i := u0(xi). Then τk h,∞ ≤ c2
12 h2|u|
C4 .
Proof By assumption,
τk
i = (u(t
k
, xi) − c2 uxx(t
k
, xi)) − (D2
Δtuk
i − c2 D2
huk
i )
= (u(t
k
, xi) − D2
Δtuk
i ) − c2(uxx(t
k
, xi) − D2
huk
i )
= (Δt)
2
12 utttt(σk
, xi) − c2 h2
12uxxxx(t
k
, ξi)
for some points σk ∈ (t
k−1
, t
k+1) and ξi ∈ (xi−1, xi+1)(cf. the proof of Theorem 9.49).
Since utttt = c2uxxtt = c4uxxxx, we also have that
|τk
i | ≤ c2
12 |c2(Δt)
2 − h2| max
(t,x)∈(tk−1,tk+1)×(xi−1,xi+1)
|uxxxx(t, x)|
≤ c2
12 |1 − cλ|h2 |u|
C4 .
The claim now follows immediately, since 1 − cλ ≤ 1 if λ ≤ c−1. 
Note that τk
i = 0 in the case λ = c−1, meaning that the method is exact in this
case. The name “leapfrog method” is derived from the image depicted in Figure 9.19
(left). In the literature the name is also used to describe central difference methods in
space and time for first order problems. In this case, the central point (t
k, xi) would
be missing from Figure 9.19 (left).
xi 1 xi xi +1
t k 1
t k
t k +1
xi 1 xi xi +1
t k 1
t k
t k +1
Fig. 9.19 Left: an explanation for the name “leapfrog” method. The value at(t k+1
, xi)is determined
from the other four. Right: difference stencil for the implicit method with θ = 1/2.
We shall now prove that the leapfrog method converges; for this we will impose a
number of assumptions on the data. To simplify the presentation we will use the same
notation as in Section 9.1.1. Setting Uk := (Uk
i )i=1,...,M−1, we denote the difference
operator in the space variable by (see Definition 9.3)382 9 Numerical methods
c2(LhUk )i = −
 c
h
2
(Uk
i−1 − 2Uk
i + Uk
i+1), i = 1,..., M − 1.
Then (9.76) reads (Δt)
−2(Uk − 2Uk−1 + Uk−2) + c2LhUk−1 = f k , where f k :=
( f k
i )i=1,...,M−1. Setting UΔt := (Uk )k=0,...,N , the leapfrog method (9.76) reads
(LΔt
UΔt
)k := Uk − (2I − (Δt)
2Lh)Uk−1 + Uk−2 = (Δt)
2 f k . (9.79)
Recall from Lemma 9.6 that Lh is bounded, symmetric and positive definite with
respect to the discrete inner product (·, ·)h of Definition 9.5. We will perform the
stability analysis with respect to the (discrete) L2-norm in time, i.e.,
UΔt
2
L2 :=

N
k=0
Uk 2
h.
Theorem 9.64 (Stability) Suppose that the CFL condition λ = Δt
h ≤ c−1 is satisfied.
Then the leapfrog method is L2-stable for the homogeneous problem ( f ≡ 0) in the
following sense: if LΔt
UΔt = 0, then there exists a constant C > 0 independent of
Δt such that UΔt L2 ≤ C(U0 h + U1 h).
Proof By Lemma 9.6, there exists an orthonormal basis {e1
, ..., eM−1} of eigen￾vectors of Lh corresponding to the positive eigenvalues μi = 4
h2 sin2(iπh
2 ), i =
1, ..., M − 1. We may expand each Uk in this basis, i.e., Uk = 
M−1
i=1 αk
i ei
, so that
LΔt
UΔt = 0 can be rewritten as
αk
i − 
2 − c2(Δt)
2μi
	
αk−1
i + αk−2
i = 0, k = 2, ..., N, i = 1, ..., M − 1.
For every i, this is a second order linear difference equation complemented by the
initial conditions α0
i = αi,0 and α1
i = αi,1. The usual ansatz to solve such equations
is αk
i = ci,1(ζi,1)
k + ci,2(ζ2,i)
k for ζi,1/2 and ci,1/2 to be determined. This leads to the
characteristic equation i(ζ) := ζ 2 − σi ζ + 1 = 0, where σi := 2 − c2(Δt)
2μi, with
roots ζi,1/2 = σi
2 ± ( σ2
i
4 − 1)
1/2. Finally, ci,1, ci,2 ∈ R are determined in such a way as
to satisfy the initial conditions, namely
ci,1 = α1
i − α0
i ζi,2
ζi,1 − ζi,2
, ci,2 = α0
i ζi,1 − α1
i
ζi,1 − ζi,2
.
Moreover, it is readily seen that c2
i,1 + c2
i,2 ≤ C0{(α0
i )
2 + (α1
i )
2} for some constant
C0 > 0. This leads to
LΔt
UΔt
2
L2 =

N
k=0
Uk 2
h =

N
k=0
"
"
"
"
"
M
−1
i=1
αk
i ei
"
"
"
"
"
2
h
=

N
k=0
M
−1
i=1
|αk
i |
29.5 The wave equation 383
=

N
k=0
M
−1
i=1
|ci,1ζ k
i,1 + ci,2ζ k
i,2|
2
≤ 2

N
k=0
M
−1
i=1

c2
i,1|ζi,1|
2k + c2
i,2|ζ k
i,2|
2k

.
This term is bounded as N → ∞ (Δt → 0) if and only if |ζi,1/2| ≤ 1, which holds
if |σi | ≤ 2, i.e., |2 − c2(Δt)
2μi | ≤ 2. By Lemma 9.6 (e), we have 0 < μi ≤ 4
h2 , so
that the CFL condition in fact ensures that |ζi,1/2| ≤ 1. This implies that there is a
constant Cˆ > 0 independent of N such that
LΔt
UΔt
2
L2 ≤ 2Cˆ
M
−1
i=1

c2
i,1 + c2
i,2

≤ 2C Cˆ 0
M
−1
i=1

(α0
i )
2 + (α1
i )
2

= 2C Cˆ 0(U0 2
h + U1 2
h),
as claimed. 
We can now formulate the convergence theorem announced earlier. In order to
do so, we introduce some further useful notation. We set FΔt := ((Δt)
2 f k )k=0,...,N ,
uk := (u(t
k, xi))i=1,...,M−1, UΔt := (uk )k=0,...,N , τk := (τk
i )i=1,...,M−1 and TΔt :=
(τk )k=0,...,N . Then LΔt
UΔt = FΔt and TΔt = FΔt − LΔt
UΔt
.
Theorem 9.65 (Convergence) Suppose that the CFL condition λ = Δt
h ≤ c−1 holds,
and let u ∈ C4,4([0, T]×[0, 1]) be the unique solution of (9.75) and U the unique
solution of (9.76) with (9.77) and U0
i := u0(xi). Then the leapfrog method converges;
more precisely, there exists a constant C(u, f ) > 0 such that
max
k=0,...,N
Uk − uk h ≤ C(u, f ) ((Δt)
2 + h2).
Proof Given uk := (u(t
k, xi))i=1,...,M−1, we set Ek := Uk − uk and EΔt :=
(Ek )k=0,...,N . Then E0 = 0 and we get
E1 = U1 − u1 = u0 + Δt u1 + c2
2 λ2Lhu0 + (Δt)
2
2 f 0
− u0 − Δt u1 − (Δt)
2
2 (c2u0
xx + f 0) + O((Δt)
3
)
= O((Δt)
2 + h2) + O((Δt)
3
).
Next, LΔt
EΔt = LΔt
UΔt − LΔt
UΔt = TΔt
, where TΔt L2 = O((Δt)
2 + h2)
by Theorem 9.63 under the given regularity assumptions. Theorem 9.64 now yields
EΔt L2 = O((Δt)
2+h2) (E0 h +E1 h), where the relevant constant only depends
on u and f . 384 9 Numerical methods
Theorem 9.65 shows that the convergence is in fact a corollary of stability and
consistency. This principle is also considered one of the fundamental theorems in the
analysis of finite differences for the numerics of (ordinary) differential equations. The
theorem is valid in much greater generality than what was stated here; for example,
often convergence on the one hand and stability and consistency on the other are
actually equivalent; and goes by the name Lax–Richtmyer theorem.
Fig. 9.20 Numerical domain of dependence for the wave equation: on the left the CFL condition
is satisfied and the domain of dependence is contained in the numerical domain of dependence. On
the right the step size is too large: the mesh points ◦ are in the domain of dependence but are not
used to approximate u(t, x); rather, only the • points are.
In Figure 9.20 we elucidate the meaning of the CFL condition. This condition
states that the domain of dependence of the wave equation (cf. Figure 3.1 on page 52)
must contain all the mesh points which are required to calculate the approximation
at a point (t, x) (this is also called the numerical domain of dependence). On the
left-hand side the CFL condition is satisfied, on the right-hand side it is not.
An implicit method
Just as with the heat equation, the CFL condition represents a restriction on the
possible choice of step size for space and time, even if it is less restrictive for the
wave equation than for the heat equation. We recall that implicit methods (such as
the implicit Euler method and the Crank–Nicolson method) for the heat equation, are
stable with respect to the choice of step size; we say they are unconditionally stable.
We shall describe a very common such implicit scheme for the wave equation. For
this, we take θ ∈ [0, 1
2 ] to be a parameter which we shall specify more precisely later,
and define
˜f k
θ := (1 − 2θ) f k−1 + θ( f k + f k−2),
so that
D2
ΔtUk + θLh(Uk + Uk−2) + (1 − 2θ)LhUk−1 = ˜f k
θ . (9.80)9.5 The wave equation 385
When θ = 0, this obviously corresponds to the explicit leapfrog method considered
above. Here, we define the local discretization error, analogously to the explicit case,
by
τk
θ := ˜f k
θ − D+
Δtuk − θLh(uk + uk−2)−(1 − 2θ)Lhuk−1. (9.81)
Theorem 9.66 (Consistency) Let u ∈ C4,4([0, T]×[0, 1]) be the unique solution of
(9.75) and U the solution of (9.80) with U1
i given by (9.77) and U0
i := u0(xi). Then
τk
θ h,∞ = O((Δt)
2 + h2) for all θ ∈ [0, 1
2 ].
Proof Firstly, a Taylor expansion shows that Lhuk − f k = −c2uk
xx + O(h2) − f k =
−uk + O(h2). Hence
τk
θ = θ( f k + f k−2) + (1 − 2θ) f k−1
−D+
Δtuk − θLh(uk + uk−2)−(1 − 2θ)Lhuk−1
= (1 − 2θ)u
k−1 + O(h2) − u
k−1 + O((Δt)
2) + θ(u
k + u
k−2)
= θ (u
k + u
k−2 − 2u
k−1) + O((Δt)
2 + h)
2)
= θ |u|
C4 O((Δt)
2) + O((Δt)
2 + h)
2) = O((Δt)
2 + h)
2),
whence the claim. 
The proof of stability is similar to the one in the explicit case.
Theorem 9.67 (Stability) The implicit method (9.80) is L2-stable for the homoge￾neous problem in the sense of Theorem 9.64
(a) for all 1/4 ≤ θ ≤ 1/2;
(b) for all 0 ≤ θ < 1/4, if the CFL condition λ = Δt
h ≤ (c
√
1 − 4θ)
−1 holds.
That is, in both of these cases, for the solution UΔt of the homogeneous problem, there
exists a constant C > 0 independent of Δt such that UΔt L2 ≤ C(U0 h + U1 h).
Proof We follow the lines of the proof of Theorem 9.64 and arrive at the dif￾ference equation αk
i − σi αk−1
i − αk−2
i = 0, where now σi := 2−(Δt)
2(1−2θ)μi
1+(Δt)2θμi , for
k = 2, ..., N, i = 1, ..., M − 1. As in the proof of Theorem 9.64 we need to show that
|σi | ≤ 2. The condition σi ≤ 2 is equivalent to 2 − (Δt)
2(1 − 2θ)μi ≤ 2 + 2(Δt)
2θ μi,
that is, 0 ≤ (Δt)
2μi(2θ + 1 − 2θ) = (Δt)
2μi, which is always satisfied since Lh is
positive definite. For the mirror condition σi ≥ −2 we have to distinguish between
the two cases (a) and (b).
(a) 0 ≤ θ < 1/4: σi ≥ −2 means that 2 − (Δt)
2(1 − 2θ)μi ≥ −2 − 2(Δt)
2θ μi, that
is, 4 ≥ (Δt)
2μi(1 − 4θ). But since μi ∈ (0, 4c2h−2] by Lemma 9.6 (e), this condition
is satisfied if 4 ≥ (1 − 4θ)4c2λ2, i.e. if λ ≤ (c
√
1 − 4θ)
−1.
(b) 1/4 ≤ θ ≤ 1/2: as above, if σi ≥ −2, then 4 ≥ (Δt)
2μi(1 − 4θ). But since
θ ≥ 1/4, we have 1 − 4θ ≤ 0, so that the condition is always satisfied. 386 9 Numerical methods
Remark 9.68 Note that in the explicit case θ = 0 we obtain exactly the same CFL
condition as in the above analysis. For θ ≥ 1/4 the implicit method is unconditionally
stable. When θ = 1/4 we obtain the Crank–Nicolson method. 
We now obtain the following statement, whose proof is completely analogous
to the one in the explicit case, see Theorem 9.65. This is in the spirit of the Lax–
Richtmyer theorem since we can again obtain the convergence of the method from
the relevant statements on consistency and stability.
Theorem 9.69 (Convergence) Let u ∈ C4,4([0, T]×[0, 1]) for the solution of (9.75).
Let U be the solution of (9.80) with (9.77) and U0
i := u0(xi). Then the method
converges
(a) for all 1/4 ≤ θ ≤ 1/2;
(b) for all 0 ≤ θ < 1/4, if the condition λ ≤ (c
√
1 − 4θ)
−1 holds.
That is, in both cases there exists a constant C(u, f ) > 0 such that
max
k=0,...,N
Uk − uk h ≤ C(u, f ) ((Δt)
2 + h2).
Numerical experiments
We shall now describe a numerical experiment related to the stability of the time
discretization. We consider a one-dimensional example with a smooth solution in
form of a wave, cf. Figure 9.21. We choose h = 1
50 and Δt = 1
100 , that is, λ = 0.5.
For the special choice c = 2.025 the CFL condition is (just) still satisfied, as one can
see from the left column of Figure 9.21. The errors start to oscillate at the end of the
time interval, but they are still within the tolerance specified by the step size. If one
should alter these numbers, even only slightly, then the error would quickly grow
beyond all bounds. However, in the case of the implicit method, things are much
more stable. Even with θ = 0.01 the error is significantly smoother; the condition
λ ≤ (c
√
1 − 4θ)
−1 of Theorem 9.67 is satisfied here. In this case, however, the error
can still rapidly become large if one alters the data. This is to be contrasted with what
happens in the unconditionally stable case θ = 0.5, which is very robust to changes
in the data. We thus see that the investigation of stability undertaken above yields a
sharp result, in the sense that even in the case of a small violation of the conditions
numerical instabilities emerge.9.5 The wave equation 387
-1
0 1
0.5 0.5
0
10-3
1 0
1
-2
0 1
0.5 0.5
0
10-3
1 0
2
-0.2
0 1
0.5 0.5
0
1 0
0.2
0 0.5 1
0
0.2
0.4
0.6
0.8
1 10-3
0 0.5 1
0
0.5
1
1.5
2 10-3
0 0.5 1
0
0.02
0.04
0.06
0.08
0.1
0.12
Fig. 9.21 The wave equation via finite differences. Left column: the explicit method (θ = 0,
leapfrog), middle column: θ = 0.01, right column: θ = 0.5, h = 1
50 , Δt = 1
100 . Upper row: error
function in the space and time variables, lower row: maximal error in the space variable per timestep
(note the differing scales on the y-axis).
Upon considering the error plots more carefully one can see that the errors in
the two implicit methods are quantitatively larger than in the explicit case – but
still within the predicted tolerance. This may be explained by the additional error
introduced when solving the linear system of equations in the implicit case.
9.5.2 Finite elements
Next we will describe Galerkin methods using finite elements for the discretization
of the space variables in the case of the wave equation on Ω ⊂ Rd, i.e.,
u(t) − c2Δu(t) = f (t) in (0, T) × Ω, (9.82a)
u(t) ∈ H1
0 (Ω) ∩ H2(Ω) for all t ∈ (0, T), (9.82b)
u(0) = u0, ut(0) = u1, (9.82c)
for given u0 ∈ H2(Ω)∩H1
0 (Ω), u1 ∈ H1
0 (Ω) and f ∈ C([0, T], H1
0 (Ω)). In this section,
we shall always assume that Ω ⊂ R2 is a convex polygon and 0 < T < ∞. We know
from Theorem 8.22 that there exists a unique solution u ∈ C2([0, T], L2(Ω)) of (9.82),
which we call the exact solution.
The first part of the following numerical analysis is based in part on [46, §13] and
[34, Ch. 5]. We will use the notation from Section 9.4.2. The spatial semi-discrete
analog of (9.58) for the wave equation reads388 9 Numerical methods
uh(0) = u0,h, uh(0) = u1,h, (9.83a)
(uh(t), χ)L2(Ω) + a(uh(t), χ) = ( f (t), χ)L2(Ω), χ ∈ Vh, t > 0, (9.83b)
where u0,h, u1,h ∈ Vh are suitable approximations of the initial conditions u0 and u1,
respectively. Here, we restrict ourselves to spaces Vh generated by linear Lagrange
elements on a family {Th}h>0 of admissible and quasi-uniform triangulations. The
matrix representation reads
Mhα h(t) + Ahαh(t) = bh(t), t > 0, αh(0) = γ0, α h(0) = γ1 (9.84)
for the right-hand side bh, where Mh and Ah are, respectively, the mass and stiffness
matrices, and we have the finite element representation of the initial values
u,h =

Nh
i=1
γ,iϕh
i , γ = (γ,i)i=1,...,Nh,  = 0, 1.
Since (9.84) is an initial value problem of a system of second order linear ordi￾nary differential equations with constant coefficients, the classical Picard–Lindelöf
theorem yields existence and uniqueness of a solution αh.
We again use the Ritz projection Rh introduced in (9.61) and recall the error
estimate of Theorem 9.53; this will give us the error estimate in the semi-discrete
case. For this, we use the following discrete weighted norm,
ϕ1,h :=

ϕ2
L2(Ω) + h2 |ϕ|
2
H1(Ω)
, ϕ ∈ H1(Ω),
and restrict ourselves to the case a(ϕ, ψ) = (∇ϕ, ∇ψ)L2(Ω), that is, a(ϕ) = |ϕ|
2
H1(Ω)
,
see (4.5), page 129.
Theorem 9.70 Let {Th}0<h≤h0 for some fixed h0 > 0 be a family of admissible
triangulations of a convex polygon Ω ⊂ R2 and let u be the exact solution of (9.82)
such that u ∈ C2([0, T], H2(Ω) ∩ H1
0 (Ω)). If u0,h, u1,h ∈ Vh, and uh is the solution of
(9.83), then
uh(t) − u(t)1,h + uh(t) − u(t)L2(Ω) ≤
≤ c (|u0,h − Rhu0|H1(Ω) + u1,h − Rhu1 L2(Ω))
+ c et h2 #
u(t)H2(Ω) + u(t)H2(Ω) + uL2((0,t),H2(Ω))$
for all t ≥ 0, where the constant c > 0 only depends on Ω and κ.
Proof We again use the Ritz projection to rewrite the error uh − u as uh − u =
(uh − Rhu) + (Rhu − u) =: θh + ρh, and start by estimating the second term ρh. Let
t ∈ (0, T). By Theorem 9.53 and its proof one has ρh(t)L2(Ω) ≤ c h2 u(t)H2(Ω);
|ρh(t)|H1(Ω) ≤ c hu(t)H2(Ω) and ρh(t)L2(Ω) ≤ c h2 u(t)H2(Ω). Putting these
together, for the second term we thus obtain9.5 The wave equation 389
ρh(t)1,h + ρh(t)L2(Ω) ≤ ch2(u(t)H2(Ω) + u(t)H2(Ω)). (9.85)
Now, as in the proof of Theorem 9.54, we derive an initial value problem for the
first term θh = uh − Rhu. Namely, for all χ ∈ Vh we have
(θ
h(t), χ)L2(Ω) + a(θh(t), χ) =
= (uh(t), χ)L2(Ω) + a(uh(t), χ)−( d2
dt2 Rhu(t), χ)L2(Ω) − a(Rhu(t), χ)
= ( f (t), χ)L2(Ω) − (Rhu(t), χ)L2(Ω) − a(u(t), χ)
= (u(t) − Rhu(t), χ)L2(Ω) = −(ρh(t), χ)L2(Ω). (9.86)
We now investigate the influence of the error coming from the inhomogeneous right￾hand side and the inhomogeneous initial conditions separately (this is also known as
the superposition principle): we set θh = ϑh + ζh to be such that
(ϑ
h(t), χ)L2(Ω) + a(ϑh(t), χ) = 0, ∀χ ∈ Vh, t > 0,
ϑh(0) = θh(0), ϑ
h(0) = θ
h(0),
( 
ζh(t), χ)L2(Ω) + a(ζh(t), χ) = −(ρh(t), χ)L2(Ω), ∀χ ∈ Vh, t > 0,
ζh(0) = 0, 
ζh(0) = 0.
Just as in Theorem 8.16, in the case of a homogeneous right-hand side the solution
satisfies conservation of energy, that is,
|ϑh(t)|2
H1(Ω) + ϑ
h(t)2
L2(Ω) = |θh(0)|2
H1(Ω) + θ
h(0)2
L2(Ω)
= |uh(0) − Rhu(0)|2
H1(Ω) + uh(0) − Rhu(0)2
L2(Ω)
= |u0,h − Rhu0|
2
H1(Ω) + u1,h − Rhu1 2
L2(Ω) (9.87)
for all t ≥ 0. Now, by Theorem 6.32 (the Poincaré inequality), the estimate
ϑh(t)L2(Ω) ≤ c |ϑh(t)|H1(Ω) holds, and we obtain, possibly for a different constant
c, that for 0 < h ≤ 1,
ϑh(t)1,h + ϑh(t)L2(Ω) ≤ c(|u0,h − Rhu0|H1(Ω) + u1,h − Rhu1 L2(Ω)). (9.88)
It remains to obtain an estimate on ζh, i.e., homogeneous initial conditions but an
inhomogeneous right-hand side. We use χ = 
ζh as a test function in the differential
equation for ζh to obtain
1
2
d
dt

 
ζh 2
L2(Ω) + |ζh(t)|2
H1(Ω)
	
= ( 
ζh(t), 
ζh)L2(Ω) + a(ζh(t), 
ζh(t))
= −(ρh(t), 
ζh(t))L2(Ω) ≤ ρh(t)L2(Ω)  
ζh(t)L2(Ω)
≤
1
2
(ρh(t)2
L2(Ω) +  
ζh(t)2
L2(Ω)
),390 9 Numerical methods
where we have also used the Cauchy–Schwarz inequality and Young’s inequality. We
now integrate this inequality over the interval [0, t] and use that ζh(0) = 
ζh(0) = 0,
to obtain the estimate
 
ζh(t)2
L2(Ω) + |ζh(t)|2
H1(Ω) ≤
∫ t
0
ρh(s)2
L2(Ω) ds +
∫ t
0
 
ζh(s)2
L2(Ω) ds.
We next apply Gronwall’s lemma (Exercise 2.7) to the function y defined as y(t) :=
 
ζh(t)2
L2(Ω) + |ζh(t)|2
H1(Ω) and obtain
 
ζh(t)2
L2(Ω) + |ζh(t)|2
H1(Ω) ≤ et
∫ t
0
ρh(s)2
L2(Ω) ds
= et
∫ t
0
Rhu(s) − u(s)2
L2(Ω) ds
≤ C2h4et
∫ t
0
u(s)2
H2(Ω)
ds = C2h4et
u2
L2((0,t),H2(Ω)),
also using Theorem 9.53. Here, as well, the Poincaré inequality (Theorem 6.32)
yields ζh(t)L2(Ω) ≤ c |ζh(t)|H1(Ω), and so, again upon taking a larger constant c if
necessary,
ζh(t)1,h + ζh(t)L2(Ω) ≤ ch2et
uL2((0,t),H2(Ω)). (9.89)
The estimates (9.85), (9.88) and (9.89) together yield the claim. 
Now, in order to obtain an estimate on the terms involving the Ritz projection, as
was the case for the heat equation we need to impose stronger regularity assumptions
on the initial values. Indeed, as in the proof of Theorem 9.54 we have the estimates
|u0,h − Rhu0|H1(Ω) ≤ |u0,h − u0|H1(Ω) + ch2 u0 H3(Ω), u0 ∈ H3(Ω), (9.90a)
u1,h − Rhu1 L2(Ω) ≤ u1,h − u1 L2(Ω) + ch2 u1 H2(Ω), u1 ∈ H2(Ω), (9.90b)
meaning that we obtain quadratic convergence if we take suitable approximations of
the initial conditions in the spatial variables, cf. also Section 9.5.2.
It remains to describe the time discretization, usually performed using finite
differences; in most textbooks this is only briefly treated, if at all. Our approach is
based on [49].
The leapfrog method
As with finite differences, the leapfrog method comes about by using the central
difference quotient of second order on an equidistant mesh in the time variable, that
is, one seeks an approximation Uk = 
Nh
i=1 αk
i ϕh
i such that for f k := f (t
k )9.5 The wave equation 391
U0 = u0,h, U1 = u1,h, (9.91a)
(D2
ΔtUk
, χ)L2(Ω) + a(Uk
, χ) = ( f k
, χ)L2(Ω), χ ∈ Vh, k ≥ 1. (9.91b)
Here u0,h and u1,h are approximations of u0 and u1, respectively, which will be
specified later, see Section 9.5.2. In matrix form this method reads
Mhαk
h = (Δt)
2(f k−1
h − Ahαk−1
h ) + Mh(2αk−1
h − αk−2
h ) (9.92)
for k ≥ 2. We recall that the leapfrog method is explicit in the case of a finite
difference discretization in the spatial variables. In (9.92), in each step a linear
system of equations with mass matrix Mh needs to be solved; thus the method is
actually implicit – however, such systems involving the mass matrix are actually
relatively simple to handle thanks to their good conditioning.
In addition to consistency and stability, numerical methods for the wave equation
need to satisfy a further condition, namely conservation of energy, which we saw
in Theorem 8.16. It would not make sense for a numerical method to violate this
fundamental, physically motivated property of the equation. However, it turns out
that the notion of “energy” depends on the discretization; this is a consequence of
the fact that the terms in the definition of the energy E(t) := u(t)2
L2(Ω)+a(u(t), u(t))
need to be discretized.
Definition 9.71 (Energy of the leapfrog method) Let Uk be the iterate of the
leapfrog method (9.91) at timestep k. Then the discrete energy of the leapfrog
method for the wave equation is defined by
Ek
LF := 1
2 D+
Δt
Uk 2
L2(Ω) + 1
2 a(Uk, Uk+1),
cf. Definition 9.46. 
In order to analyze the leapfrog method we require a special form of the CFL
condition; for this we require the constant Cinv from the inverse estimate of Theo￾rem 9.37. We say that the leapfrog method satisfies a CFL condition if the time step
size Δt is chosen so small that Cinv (Δt)
2
h2 < 2; more precisely:
Definition 9.72 The leapfrog method satisfies a CFL condition if there exists some
λ ∈ (0, 2) such that
Cinv
(Δt)
2
h2 ≤ 2 − λ (9.93)
holds. 
Lemma 9.73 Let {Th}h>0 be a uniform family of admissible triangulations of a
polygon Ω ⊂ R2, and suppose that Δt is chosen in such a way that the CFL condition
(9.93) is satisfied. Then Ek
LF ≥ λ
4 D+
Δt
Uk 2
L2(Ω)+ 1
4
%
a(Uk+1
, Uk+1)+a(Uk, Uk )
&
≥ 0.392 9 Numerical methods
Proof By the inverse Cauchy–Schwarz inequality (Corollary 9.38),
2 a(Uk
, Uk+1) = a(Uk+1
, Uk+1) + a(Uk
, Uk ) − a(Uk+1 − Uk
, Uk+1 − Uk )
= a(Uk+1
, Uk+1) + a(Uk
, Uk )−(Δt)
2 a(D+
ΔtUk
, D+
ΔtUk )
≥ a(Uk+1
, Uk+1) + a(Uk
, Uk ) − Cinv
(Δt)
2
h2 D+
ΔtUk 2
L2(Ω)
.
It follows that
Ek
LF = 1
2 D+
Δt
Uk 2
L2(Ω) + 1
2 a(Uk, Uk+1)
≥
 1
2 − 1
4
Cinv
(Δt)
2
h2

D+
ΔtUk 2
L2(Ω) +
1
4

a(Uk+1
, Uk+1) + a(Uk
, Uk )

.
Since 1
2 − 1
4Cinv (Δt)
2
h2 ≥ 1
2 − 1
4 (2 − λ) = λ
4 by the CFL condition (9.93), the claim
follows. 
We can now prove that the leapfrog method satisfies conservation of energy.
Theorem 9.74 (Conservation of energy) Let {Th}h>0 be a uniform family of ad￾missible triangulations of a polygon Ω ⊂ R2 and suppose Δt is chosen in such a way
that the CFL condition (9.93) is satisfied. If f ≡ 0, then Ek
LF = E0
LF, and if f  0,
then

Ek
LF ≤

E0
LF +

k
j=1
Δt
√
λ
 f j
L2(Ω).
Proof We use χ = Uk+1 − Uk−1 = (Uk+1 − Uk ) + (Uk − Uk−1) as a test function in
(9.91b) and obtain (D2
Δt
Uk, Uk+1 − Uk−1)L2(Ω) + a(Uk, Uk+1 − Uk−1) = ( f k, Uk+1 −
Uk−1)L2(Ω). We consider each of these terms separately. Firstly, (D2
Δt
Uk, Uk+1 −
Uk−1)L2(Ω) = 1
(Δt)2 ((Uk+1 − Uk )−(Uk − Uk−1), (Uk+1 − Uk ) + (Uk − Uk−1))L2(Ω) =
D+
Δt
Uk 2
L2(Ω) − D+
Δt
Uk−1 2
L2(Ω)
. It follows that
2(Ek
LF − Ek−1
LF ) = D+
ΔtUk 2
L2(Ω) + a(Uk
, Uk+1)
− D+
ΔtUk−1 2
L2(Ω) − a(Uk−1
, Uk )
= ( f k
, Uk+1 − Uk−1)L2(Ω).
If f ≡ 0, then the first part of the claim now follows by induction. For the case
f  0 we have, adding and subtracting Uk ,
2(Ek
LF − Ek−1
LF ) = ( f k
, Uk+1 − Uk )L2(Ω) + ( f k
, Uk − Uk−1)L2(Ω)
= Δt ( f k
, D+
ΔtUk )L2(Ω) + Δt ( f k
, D+
ΔtUk−1)L2(Ω)
≤ Δt  f k L2(Ω) (D+
ΔtUk L2(Ω) + D+
ΔtUk−1 L2(Ω))
≤ 2
Δt
√
λ
 f k L2(Ω) ((Ek
LF)
1/2 + (Ek−1
LF )
1/2),9.5 The wave equation 393
where in the last step we used Lemma 9.73. We assume for the meantime that
Ek
LF  0; then
(Ek
LF)
1/2 − (Ek−1
LF )
1/2 = Ek
LF − Ek−1
LF
(Ek
LF)1/2 + (Ek−1
LF )1/2 ≤
Δt
√
λ
 f k L2(Ω).
Observe that this inequality is still valid if Ek
LF = 0, since Ek−1
LF ≥ 0. Hence, in both
cases, (Ek
LF)
1/2 ≤ (Ek−1
LF )
1/2 + Δt √
λ  f k L2(Ω). An induction argument now yields the
claim. 
We recall that the conservation of energy in Theorem 8.16 was the key to the
uniqueness statement in Theorem 8.15. It should thus not come as a particular
surprise that the discrete form of conservation of energy yields the stability of the
leapfrog method.
Theorem 9.75 (Stability of the leapfrog method) Let {Th}h>0 be a uniform family
of admissible triangulations of a polygon Ω ⊂ R2 and suppose Δt is chosen in such
a way that the CFL condition (9.93) is satisfied. Then the leapfrog method is stable
in the sense that there exists a constant c > 0 independent of h and Δt such that for
k = 0, ..., N − 1
D+
ΔtUk L2(Ω) + Uk+1 H1(Ω) ≤ (9.94)
≤ c

D+
ΔtU0 L2(Ω) + U0 H1(Ω) + U1 H1(Ω) +

k
j=1
Δt  f j
L2(Ω)

.
Proof Using the coercivity of the bilinear form a and the energy estimate of
Lemma 9.73, we have
D+
ΔtUk L2(Ω) + Uk+1 H1(Ω) ≤ D+
ΔtUk L2(Ω) + α−1/2

a(Uk+1, Uk+1)
≤
(4
λ
Ek
LF + 2 α−1/2

Ek
LF ≤ C1

Ek
LF
for some constant C1 > 0 independent of Δt and h. We now use Young’s inequality
in the form 2a(φ, ψ) ≤ a(φ, φ) + a(ψ, ψ) and estimate, using the continuity constant
C of a,
E0
LF = 1
2
D+
ΔtU0 2
L2(Ω) +
1
2
a(U0
, U1)
≤
1
2
D+
ΔtU0 2
L2(Ω) +
1
4
a(U0
, U0) +
1
4
a(U1
, U1)
≤
1
2
D+
ΔtU0 2
L2(Ω) +
C
4 (U0 2
H1(Ω) + U1 2
H1(Ω)
).394 9 Numerical methods
Theorem 9.74 now yields the claim,

Ek
LF ≤

E0
LF + 2

k
j=1
Δt
√
λ
 f j
L2(Ω)
≤ c

D+
ΔtU0 L2(Ω) + U0 H1(Ω) + U1 H1(Ω) +

k
j=1
Δt  f j
L2(Ω)

for a suitable constant c > 0. 
In the next theorem we need the exact solution to be in C4([0, T], H1
0 (Ω)). The￾orem 8.23 gives conditions on the data u0, u1 and f which imply this degree of
regularity.
Theorem 9.76 (Convergence of the leapfrog method) Let {Th}0<h≤h0 for some
fixed h0 > 0 be a uniform family of admissible triangulations of a convex polygon
Ω ⊂ R2 and choose Δt in such a way that the CFL condition (9.93) is satisfied. Let u
be the solution of (9.82) and assume that u ∈ C4([0, T], H1
0 (Ω)). Let {Uk }k=0,...,N be
the approximate solutions yielded by the leapfrog method (9.91). Then there exists a
constant C > 0 independent of Δt and h such that for all k = 0,..., N − 1
D+
Δt(Uk − u(t
k ))L2(Ω) + Uk+1 − u(t
k+1)L2(Ω) + Uk − u(t
k )L2(Ω) ≤
≤ C

D+
Δt(U0 − Rhu0)L2(Ω) + U0 − Rhu0 H1(Ω) + U1 − Rhu1 H1(Ω)
+ T u − RhuC2([0,T],L2(Ω)) + T(Δt)
2 u(4)
C([0,T],H1(Ω))
. (9.95)
Proof As in the previous convergence proofs, we split up the error: Uk − u(t
k ) =
[Uk − Rhu(t
k )] + [Rhu(t
k ) − u(t
k )] =: θk
h + ρh(t
k ). To deal with the first part, we
derive a discretized differential equation for it: for all χ ∈ Vh, using the definition of
the leapfrog method and properties of the Ritz projection, we have
(D2
Δt θk
h, χ)L2(Ω) + a(θk
h, χ)
= (D2
ΔtUk
, χ)L2(Ω) + a(Uk
, χ)−(D2
ΔtRhu(t
k ), χ)L2(Ω) − a(Rhu(t
k ), χ)
= ( f k
, χ)L2(Ω) − (D2
ΔtRhu(t
k ), χ)L2(Ω) − a(u(t
k ), χ)
= (u(t
k ) − D2
Δtu(t
k ) + D2
Δtu(t
k ) − D2
ΔtRhu(t
k ), χ)L2(Ω)
= (σ(t
k ) − D2
Δt ρh(t
k ), χ)L2(Ω), (9.96)
where σ(t
k ) := u(t
k ) − D2
Δt
u(t
k ) is the cut-off error of the time discretization. We
next estimate the terms on the right-hand side of the differential equation (9.96)
individually. Firstly, a Taylor expansion yields the estimate (cf. Exercise 9.19)
σ(t
k )L2(Ω) ≤ (Δt)
2
12 u(4)
C([0,T],L2(Ω)).9.5 The wave equation 395
We estimate the second term in a similar fashion:
D2
Δt ρh(t
k )L2(Ω) ≤ ρh(t
k )L2(Ω) + (Δt)
2
12 ρ(4)
h C([0,T],L2(Ω))
= ρh(t
k )L2(Ω) + (Δt)
2
12 Rhu(4) − u(4)
C([0,T],L2(Ω))
≤ u − RhuC2([0,T],L2(Ω)) + C h (Δt)
2
12 u(4)
C([0,T],H1(Ω)),
where in the last step we used the boundedness of the Ritz projection, cf. Theo￾rem 9.53. We now apply the stability estimate of Theorem 9.75 to equation (9.96)
for θk
h (and observe that θk+1
h L2(Ω) ≤ cθk+1
h H1(Ω)):
D+
Δt θk
h L2(Ω) + θk+1
h L2(Ω) ≤ (9.97)
≤ c

D+
Δt θ0
h L2(Ω) + θ0
h H1(Ω) + θ1
h H1(Ω)
+

k
j=1
Δt σ(t
j
) − D2
Δt ρh(t
j
)L2(Ω)

and

k
j=1
Δt σ(t
j
) − D2
Δt ρh(t
j
)L2(Ω) ≤
≤ Δt

N
j=1
(σ(t
j
)L2(Ω) + D2
Δt ρh(t
j
)L2(Ω))
≤ Δt

N
j=1
 (Δt)
2
12 u(4)
C([0,T],L2(Ω)) + u − RhuC2([0,T],L2(Ω))
+ C
h(Δt)
2
12 u(4)
C([0,T],H1(Ω))
≤ T max{1,Ch}

u − RhuC2([0,T],L2(Ω)) + (Δt)
2 u(4)
C([0,T],H1(Ω))
. (9.98)
The estimates (9.97) and (9.98) still hold, albeit possibly for a larger constant, if
we add θk
h L2(Ω) to the left-hand side of (9.97). It remains to estimate the terms
in the second part of the error, ρh(t
k ) = Rhu(t
k ) − u(t
k ). A Taylor expansion about
t
k + Δt
2 yields D+
Δt ρh(t
k )L2(Ω) ≤ ρh(t
k + Δt
2 )L2(Ω) + (Δt)
2
24 ρ(3)
h C([0,T],L2(Ω)) (cf.
Exercise 9.19), and so
D+
Δt ρh(t
k )L2(Ω) + ρh(t
k+1
)L2(Ω) + ρh(t
k )L2(Ω) ≤
≤ ρh C([0,T],L2(Ω)) + (Δt)
2
24 ρ(3)
h C([0,T],L2(Ω)) + 2u − RhuC([0,T],L2(Ω))
≤ 3u − RhuC1([0,T],L2(Ω)) + Ch (Δt)
2
24 u(3)
C([0,T],H1(Ω)).396 9 Numerical methods
Combining the individual estimates on all the terms considered above yields
|D+
Δt(Uk − u(t
k ))L2(Ω) + Uk+1 − u(t
k+1)L2(Ω) + Uk − u(t
k )L2(Ω) ≤
≤ |D+
Δt θk
h L2(Ω) + θk+1
h L2(Ω) + θk
h L2(Ω)
+ |D+
Δt ρh(t
k )L2(Ω) + ρh(t
k+1)L2(Ω) + ρh(t
k )L2(Ω)
≤ c

D+
Δt θ0
h L2(Ω) + θ0
h H1(Ω) + θ1
h H1(Ω)
+ T max{1,Ch}

ρh C2([0,T],L2(Ω)) + (Δt)
2 u(4)
C([0,T],H1(Ω))
+ 3ρh C1([0,T],L2(Ω)) + Ch (Δt)
2
24 u(3)
C([0,T],H1(Ω)),
which results in the claimed estimate. 
Remark 9.77 The above proof can also be carried out for the stronger norm θ
h H1(Ω)
in place of θ
h L2(Ω); in this case the regularity assumptions and a few norms on the
right-hand side need to be adjusted. 
Corollary 9.78 For linear finite elements, under the assumptions of Theorem 9.76,
one has for k = 0, ..., N − 1
D+
Δt(Uk − u(t
k ))L2(Ω) + Uk+1 − u(t
k+1
)L2(Ω) + Uk − u(t
k )L2(Ω) ≤
≤ C

D+
Δt(U0 − Rhu0)L2(Ω) + U0 − Rhu0 H1(Ω)
+ U1 − Rhu1 H1(Ω) + T(h2 + (Δt)
2)uC4([0,T],H2(Ω))
, (9.99)
if u ∈ C4([0, T], H2(Ω) ∩ H1
0 (Ω)), where u is the solution of the wave equation.
Proof The claim follows from Theorem 9.76 using Theorem 9.53. 
We thus obtain a method of order O(h2 + (Δt)
2), if the initial values admit a
corresponding approximation. We can also see the dependence of the results on the
length T of the time interval.
The Crank–Nicolson method
The leapfrog method has the advantage of being efficient in each timestep; however,
it once again presupposes a CFL condition (9.93). To finish, we shall now describe
the Crank–Nicolson method in this case and show that this method is unconditionally
stably convergent. The method reads9.5 The wave equation 397
U0 = u0,h, U1 = u1,h, (9.100a)
(D2
ΔtUk
, χ)L2(Ω) +
1
4
a(Uk+1 + 2Uk + Uk−1
, χ) = (9.100b)
= 1
4
( f k+1 + 2 f k + f k−1
, χ)L2(Ω), χ ∈ Vh, k ≥ 1,
where, again, u0,h and u1,h are approximations of u0 and u1, respectively, to be
specified later (see page 401). In matrix form this method reads for k ≥ 1

Mh + (Δt)
2
4 Ah
	
αk+1
h = (Δt)
2
4 (f k+1
h + 2 f k
h + f k−1
h ) (9.101)
− (Δt)
2
4 Ah(2αk
h + αk−1
h ) + Mh(2αk
h − αk−1
h ).
We proceed in the same way as for the leapfrog method: we first define a suitable
notion of energy and show that this energy is conserved; we then prove stability and
so, finally, convergence of the method.
Definition 9.79 (Energy of the Crank–Nicolson method) Let Uk be the iterate of
the Crank–Nicolson method (9.100) at timestep k. Then the discrete energy of the
Crank–Nicolson method for the wave equation is defined by
Ek
CN := 1
2
D+
ΔtUk 2
L2(Ω) +
1
2
a(Uk+1/2
, Uk+1/2),
where Uk+1/2 := 1
2 (Uk+1 + Uk ). 
We now show that the Crank–Nicolson method conserves this energy, without
any CFL condition and under weaker assumptions on the finite element mesh.
Theorem 9.80 (Energy conservation) Let {Th}h>0 a family of admissible triangu￾lations of a polygon Ω ⊂ R2. Then the Crank–Nicolson method is energy conserving:
if f ≡ 0, then Ek
CN = E0
CN; if f  0, then for k = 1, ..., N

Ek
CN ≤

E0
CN +
Δt
4
√
2

k
j=1
 f j+1 + 2 f j + f j−1 L2(Ω).
Proof We use χ = Uk+1 − Uk−1 = (Uk+1 − Uk ) + (Uk − Uk−1) = (Uk+1 + Uk ) −
(Uk + Uk−1) ∈ Vh as a test function in (9.100b) to obtain the relation
(D2
ΔtUk
, (Uk+1 − Uk ) + (Uk − Uk−1))L2(Ω) =
= D+
ΔtUk 2
L2(Ω) − D+
ΔtUk−1 2
L2(Ω)
for the first term in (9.100b), and the relation 1
4 a(Uk+1 + 2Uk + Uk−1
, (Uk+1 +
Uk )−(Uk + Uk−1)) = a(Uk+1/2
, Uk+1/2) − a(Uk−1/2
, Uk−1/2) for the second term.
Hence the left-hand side of (9.100b) equals 2(Ek
CN − Ek−1
CN ), which in turn is equal
to 1
4 ( f k+1 + 2 f k + f k−1
, (Uk+1 − Uk ) + (Uk − Uk−1))L2(Ω) by (9.100b). In the case
f ≡ 0 this yields Ek
CN = Ek−1
CN , and the claim follows by induction. If f  0, then398 9 Numerical methods
2(Ek
CN − Ek−1
CN ) = 1
4 ( f k+1 + 2 f k + f k−1
, (Uk+1 − Uk ) + (Uk − Uk−1))L2(Ω)
≤  1
4 ( f k+1 + 2 f k + f k−1)L2(Ω)Δt(D+
Δt
Uk L2(Ω) + D+
Δt
Uk−1 L2(Ω))
≤
√
2
4 Δt  f k+1 + 2 f k + f k−1 L2(Ω)

Ek
CN +

Ek−1
CN 
.
The rest of the proof proceeds analogously to the proof of Theorem 9.74. 
Theorem 9.81 (Stability of the Crank–Nicolson method) Let {Th}h>0 be a family
of admissible triangulations of a polygon Ω ⊂ R2. Then the Crank–Nicolson method
is stable, that is, there exists a constant c > 0 independent of h and Δt such that for
k = 0, ..., N − 1
D+
ΔtUk L2(Ω) + Uk+1/2 H1(Ω) ≤ (9.102)
≤ c

D+
ΔtU0 L2(Ω) + U1/2 H1(Ω) +
Δt
4

k
j=1
 f j+1 + 2 f j + f j−1 L2(Ω)

.
Proof Using the coercivity of the bilinear form a and the energy estimate of Theo￾rem 9.80, we have
D+
ΔtUk L2(Ω) + Uk+1/2 H1(Ω) ≤
≤ D+
ΔtUk L2(Ω) + α−1/2

a(Uk+1/2, Uk+1/2)
≤ 2 max{1, α−1/2}

Ek
CN
≤ 2 max{1, α−1/2}

E0
CN +
Δt
4
√
2

k
j=1
 f j+1 + 2 f j + f j−1 L2(Ω)

≤ c

D+
ΔtU0 L2(Ω) + U1/2 H1(Ω) +
Δt
4

k
j=1
 f j+1 + 2 f j + f j−1 L2(Ω)

for a constant c > 0 independent of Δt and h, where we have also used Young’s
inequality, as before. 
Of course, we can also apply the triangle inequality to (9.102) to obtain the further
estimate
D+
ΔtUk L2(Ω) + Uk+1/2 H1(Ω) ≤
≤ c

D+
ΔtU0 L2(Ω) + U1/2 H1(Ω) + Δt

k+1
j=0
 f j
L2(Ω)

. (9.102’)
We again refer to Theorem 8.23 for conditions on the given data which imply the
regularity of the exact solution required in the next statement.9.5 The wave equation 399
Theorem 9.82 (Convergence of the Crank–Nicolson method) Let {Th}0<h≤h0 for
some fixed h0 > 0 be a family of admissible triangulations of a convex polygon
Ω ⊂ R2. Suppose that the exact solution u of (9.82) satisfies u ∈ C4([0, T], H1
0 (Ω)),
and let {Uk }k=0,...,N be the approximate solutions obtained via the Crank–Nicolson
method (9.100). Then there exists a constant c > 0 independent of Δt and h such
that for k = 0, ..., N − 1, setting t
k+1/2 := t
k + Δt
2
D+
Δt(Uk − u(t
k ))L2(Ω) + Uk+1/2 − u(t
k+1/2
)L2(Ω) ≤
≤ c

D+
Δt(U0 − Rhu0)L2(Ω) + U1/2 − Rhu(t
1/2)H1(Ω)
+ T u − RhuC2([0,T],L2(Ω)) + T(Δt)
2 u(4)
C([0,T],H1(Ω))
. (9.103)
Proof The proof is very similar to the one of Theorem 9.76, the convergence result
for the leapfrog method. We just sketch the main steps and again split up the error
term: Uk − u(t
k ) = [Uk − Rhu(t
k )] + [Rhu(t
k ) − u(t
k )] =: θk
h + ρh(t
k ). We insert the
first part θk
h into the left-hand side of the Crank–Nicolson method, then by properties
of the Ritz projections, for all χ ∈ Vh we have
(D2
Δt θk
h, χ)L2(Ω) + 1
4 a(θk+1
h + 2θk
h + θk−1
h , χ) =
= (D2
ΔtUk
, χ)L2(Ω) + 1
4 a(Uk+1 + 2Uk + Uk−1
, χ)
− (D2
ΔtRhu(t
k ), χ)L2(Ω) − 1
4 a(u(t
k+1) + 2u(t
k ) + u(t
k−1), χ)
= 1
4 ( f k+1 + 2 f k + f k−1
, χ)L2(Ω) − (D2
ΔtRhu(t
k ), χ)L2(Ω)
+ 1
4 (u(t
k+1) + 2u(t
k ) + u(t
k−1), χ)L2(Ω) − 1
4 ( f k+1 + 2 f k + f k−1
, χ)L2(Ω)
= (u(t
k ) − D2
Δtu(t
k ) + D2
Δtu(t
k ) − D2
ΔtRhu(t
k ), χ)L2(Ω)
+ 1
4 (u(t
k+1) − 2u(t
k ) + u(t
k−1), χ)L2(Ω)
= (σ(t
k ) − D2
Δt ρh(t
k ) + Δt
4 (D+
Δt
u(t
k ) − D−
Δt
u(t
k )), χ)L2(Ω), (9.104)
where σ(t
k ) := u(t
k ) − D2
Δt
u(t
k ) is the cut-off error of the time discretization. We
estimate the first two terms in (9.104) as above, that is, we have the bounds
σ(t
k )L2(Ω) ≤ (Δt)
2
12 u(4)
C([0,T],L2(Ω)) and
D2
Δt ρh(t
k )L2(Ω) ≤ u − RhuC2([0,T],L2(Ω)) + Ch (Δt)
2
12 u(4)
C([0,T],H1(Ω)).
The new term which appears here and not in the leapfrog method can be controlled
using another Taylor expansion,
D+
Δtu(t
k ) − D−
Δtu(t
k )L2(Ω) ≤ C Δt u(4)
C([0,T],L2(Ω)).400 9 Numerical methods
We now apply the stability estimate (9.102) to (9.104) for θk
h, setting θk+1/2
h := 1
2 (θk+1
h + θk
h) similar to Definition 9.79:
D+
Δt θk
h L2(Ω) + θk+1/2
h L2(Ω) ≤ c

D+
Δt θ0
h L2(Ω) + θ
1/2
h H1(Ω)
+ Δt

k
j=1
σ(t
j
) − D2
Δt ρh(t
j
) + Δt
4 (D+
Δt
u(t j
) − D−
Δt
u(t j
)L2(Ω)

≤ c

D+
Δt θ0
h L2(Ω) + θ
1/2
h H1(Ω)
+ T

u − RhuC2([0,T],L2(Ω)) + (Δt)
2 u(4)
C([0,T],H1(Ω)).
Next, we estimate the terms in the second part of the error, ρh(t
k ) = Rhu(t
k ) −u(t
k ).
As with the proof of convergence of the leapfrog method, a Taylor expansion about
t
k+1/2 = t
k + Δt
2 (cf. Exercise 9.19) yields (by Theorem 9.53, p. 368)
D+
Δt ρh(t
k )L2(Ω) + ρh(t
k+1/2
)L2(Ω) ≤
≤ ρh C([0,T],L2(Ω))+ (Δt)
2
24 ρ(3)
h C([0,T],L2(Ω)) + ρh(t
k+1/2
)L2(Ω)
≤ 2u − RhuC1([0,T],L2(Ω)) + Ch (Δt)
2
24 u(3)
C([0,T],H1(Ω)).
Finally, abbreviating uk+1/2 := 1
2

u(t
k+1) + u(t
k )
	
, we use a Taylor expansion of
Uk+1/2 −u(t)L2(Ω) about t
k+1/2 (cf. Exercise 9.19) to obtain the estimate Uk+1/2 −
u(t
k+1/2)L2(Ω) ≤ Uk+1/2 − uk+1/2 L2(Ω) + (Δt)
2
24 u(2)
C([0,T],L2(Ω)). Then, Uk+1/2 −
uk+1/2 = θk+1/2
h + 1
2 (ρh(t
k+1) + ρh(t
k )) and combining all the above estimates yields
the claim. 
Corollary 9.83 For linear finite elements on a uniform triangulation, under the
assumptions of Theorem 9.82, one has for k = 0, ..., N − 1
D+
Δt(Uk − u(t
k ))L2(Ω) + Uk+1/2 − u(t
k+1/2)L2(Ω) ≤ (9.105)
≤ C

D+
Δt(U0 − Rhu0)L2(Ω) + U1/2 − Rhu(t
1/2
)H1(Ω)
+ T(h2 + (Δt)
2)uC4([0,T],H2(Ω))
,
if u ∈ C4([0, T], H2(Ω) ∩ H1
0 (Ω)), where u is the solution of the wave equation.
Proof The estimate follows by (9.103) and the properties of the Ritz projector in
Theorem 9.53. 9.5 The wave equation 401
Initialization
As for finite differences (see, e.g., (9.77)), for the numerical implementation of
both the Crank–Nicolson and the leapfrog methods using finite elements for the
space variables we still require approximations of the initial values, U0 = u0,h and
U1 = u1,h in order to bound the first terms on the right-hand side of (9.90). The
process of finding these is sometimes known as initialization. The error estimates
which we have derived and proved in both cases, still contain terms which depend
on U0 − Rhu0 and U1 − Rhu1, respectively, which in particular involve the Ritz
projection from (9.61). This motivates the following result. We refer once more to
Theorem 8.23 for conditions on the given data which imply the required regularity
of the exact solution.
Theorem 9.84 (Initialization by elliptic projection) Let {Th}0<h≤h0 for some fixed
h0 > 0 be a uniform family of admissible triangulations of a convex polygon Ω ⊂ R2.
Let u be the exact solution of (9.82) and assume that u ∈ C3([0, T], H1
0 (Ω)). Define
U0 = u0,h := Rhu0, U1 = u1,h := Rh

u0 + Δt u1 + (Δt)
2
2 u(0)

,
where u0 and u1 are as in (9.100). Then there exists a constant C = C(u, T) > 0
independent of h and Δt ≤ 1 such that for uk+1/2 := 1
2 (u(t
k+1) + u(t
k ))
D+
Δt(U0 − Rhu0)L2(Ω) + U1/2 − Rhu1/2 H1(Ω) ≤ C (Δt)
2.
Proof Firstly, by the triangle inequality,
U1/2 − Rhu1/2 H1(Ω) ≤ 1
2 U0 − Rhu0 H1(Ω) + 1
2 U1 − Rhu(t
1)H1(Ω)
= 1
2 U1 − Rhu(t
1)H1(Ω)
by assumption. Using the definition (9.61) of the Ritz projection and the coercivity
and boundedness of the bilinear form a(·, ·), we obtain that, for any v ∈ H1
0 (Ω),
α Rhv 2
H1(Ω) ≤ a(Rhv, Rhv) = a(Rhv, v) = a(v, v) ≤ C v 2
H1(Ω)
,
that is, Rhv H1(Ω) ≤
C
α v H1(Ω). Similar to above we now use a Taylor expansion
of u(t
1)H1(Ω) = u(Δt)H1(Ω) about 0 to obtain (cf. Exercise 9.19)
u(Δt) − u(0) − Δt u(0) − (Δt)
2
2 u(0)H1(Ω) ≤ (Δt)
3
6 uC3([0,T];H1(Ω))
and thus, recalling that u(0) = u0 and u(0) = u1
U1 − Rhu(t
1)H1(Ω) = u1,h − Rhu(Δt)H1(Ω)
=
"
"
"
Rh

u0 + Δt u1 + (Δt)
2
2 u(0)

− Rhu(Δt)
"
"
"
H1(Ω)402 9 Numerical methods
≤
C
α
"
"
"u0 + Δt u1 + (Δt)
2
2 u(0) − u(Δt)
"
"
"
H1(Ω)
≤
√
C
6
√
α (Δt)
3 uC3([0,T];H1(Ω)).
Finally, it follows from our choice of u0,h that
D+
Δt(U0 − Rhu0)L2(Ω) = 1
Δt
(U1 − Rhu1)−(U0 − Rhu0)L2(Ω)
= 1
Δt
U1 − Rhu1 L2(Ω) ≤
1
Δt
U1 − Rhu1 H1(Ω)
≤
√
C
6
√
α (Δt)
2 uC3([0,T];H1(Ω)),
which yields the claim for C(u, T) :=
√
C
4
√
α uC3([0,T];H1(Ω)). 
Remark 9.85 Theorem 9.84 clearly yields the optimal convergence rates, if we use
the approximations of the initial values in Theorem 9.84 for the leapfrog and Crank–
Nicolson methods. However, calculating the Ritz projections requires the use of the
bilinear form a(·, ·) and thus the derivatives of u0, u1 and u(0) with respect to the
space variables, which is numerically costly. In practice the desired accuracy can
often be obtained by recourse to simple nodal interpolations. 
Numerical experiments
To finish, we will present the results of a few numerical experiments; these are
essentially from [36]. These experiments are concerned with two questions: (1) how
sharp is the CFL condition (9.93) in the case of the leapfrog method? We wish to
investigate this question of stability quantitatively; (2) how strict are the regularity
requirements in the proofs of convergence? We saw that strong assumptions were
necessary on the unknown solution (and also on the initial conditions) in order to
prove our convergence results. We wish to investigate the behavior of the methods
when these assumptions are not satisfied.
In addition to the methods considered above, viz. the leapfrog and the Crank–
Nicolson methods, we will also implement the implicit Euler method. As we already
saw when investigating the heat equation, it should not come as a surprise that this
method is only of first order in time, since the partial derivative in the time variable
is approximated by a forward difference quotient of first order. On the other hand,
we can expect stability of this method without an additional CFL condition.
The Case of One Space Dimension
We start with the case Ω = (0, 1), and consider the initial-boundary value problem
(9.75) with T = 1 and f ≡ 0. If u0 is at least piecewise continuous and u1 integrable,9.5 The wave equation 403
then we can use the solution formula of d’Alembert, see (3.7), to obtain the exact
solution and thus compute the (exact) error in the numerical approximation.
Stability analysis. We first investigate the CFL condition (9.93) for the leapfrog
method. To this end we choose u0(x) := sin(2πx), u1 ≡ 0 and Δt = h. We make two
choices for the wave speed c in (9.75): in the first case, c = 0.5, the CFL condition
is satisfied, in the other case, c = 1, it is not. The results are depicted in Figure 9.22.
The convergence of first and second order, respectively, when the CFL condition is
satisfied, is clearly recognizable (left). On the right-hand side we see that Crank–
Nicolson and implict Euler are stable, whereas the leapfrog method diverges for
coarse step sizes. We obtain similar results in the case u0 ≡ 0 und u1(x) := sin(2πx).
10−3 10−2 10−1
10−6
10−5
10−4
10−3
10−2
10−1
100
1
2
1 1
Mesh size h
Crank–Nicolson
Implicit Euler
Leapfrog
10−3 10−2 10−1
10−6
10−5
10−4
10−3
10−2
10−1
100
1
2
1 1
Mesh size h
Crank–Nicolson
Implicit Euler
Leapfrog
Fig. 9.22 Rate of convergence for FEM for the wave equation in one space dimension; comparison
of the leapfrog, Crank–Nicolson and implicit Euler methods. Left: c = 0.5, the CFL condition
is satisfied (the curves for Crank–Nicolson and leapfrog almost coincide), right: c = 1, the CFL
condition is violated.
Regularity assumptions. In the case of one space dimension and homogeneous
right-hand side, the smoothness of the solution depends exclusively on the initial
displacement u0 and velocity u1. We will consider various cases, listed in Table 9.1.
The cases 1 and 2 are exactly the ones which we considered in detail above.
The results of the convergence analysis are depicted in Table 9.2. We first ob￾serve that the implicit Euler method is at least quantitatively influenced by the CFL
condition; the convergence rate drops to 0.8. In order to make clearer the effects of
regularity in question, the table is ordered in such a way that the convergence rates
for leapfrog and Crank–Nicolson are in descending order – interestingly this order is
equal in the two cases. We see that discontinuous and non-continuously differentiable
initial displacements have the largest negative influence on the convergence rates.
The methods of second order (Crank–Nicolson and leapfrog) actually converge more
slowly than the first order method (Euler). The behavior in the case of discontinuous
initial velocity is comparable to that in the case of non-continuously differentiable
initial displacements: we only see convergence of first order.404 9 Numerical methods
Case u0 u1
1 smooth, CFL sin(2πx) ∈ C∞(Ω) ≡ 0 ∈ C∞(Ω)
2 smooth, no CFL sin(2πx) ∈ C∞(Ω) ≡ 0 ∈ C∞(Ω)
3 u0 ∈ C0(Ω) \ C1(Ω)

x, x < 0.5,
1 − x, otherwise. ≡ 0
4 u0  C0(Ω)

1, x ∈ [0.4, 0.6],
0, otherwise. ≡ 0
5 u1 ∈ C0(Ω) \ C1(Ω) ≡ 0

x, x < 0.5,
1 − x, otherwise.
6 u1  C0(Ω) ≡ 0

1, x ∈ [0.4, 0.6],
0, otherwise.
Table 9.1 Test cases for finite elements for the 1D wave equation.
Case/Conv. Rate Leapfrog Crank–Nicolson Implicit Euler
1 Smooth, CFL 2.05 2.07 0.95
2 Smooth, no CFL — 2.07 0.80
5 u1 ∈ C0 \ C1 1.76 1.80 0.95
6 u1  C0 1.06 1.05 0.83
3 u0 ∈ C0 \ C1 1.02 0.99 0.99
4 u0  C0 0.38 0.33 0.72
Table 9.2 Convergence rates of the various time discretization methods for finite elements for the
1D wave equation (sorted by convergence rate of leapfrog and Crank–Nicolson).
We wish to study the case of a discontinuous initial displacement (case 4) more
closely, cf. Figure 9.23. On the left-hand side we see the initial displacement and its
numerical approximation (which is the same for all methods). We can already see
oscillations, which arise through the finite difference approximation of the discon￾tinuous function u0. In the case of the leapfrog method, even for moderate step sizes
these oscillations build up so much that after just a few timesteps the approximation
falls outside the machine accuracy. For this reason the leapfrog approximation can
no longer be recognized on the right-hand side. There, the exact solution and the
approximate solutions are depicted at time t = 0.75. What we can recognize is the
differing behavior of the other two methods: the Crank–Nicolson method produces
oscillations (which do not build up as much as with the leapfrog method, due to the
stability of the former), whereas the implicit Euler method “blurs” the solution, that
is, produces a very smooth (and poor) approximation of the discontinuous solution.
The (poor) convergence behavior is depicted in Figure 9.24.9.5 The wave equation 405
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 -1
-0.5
0
0.5
1
1.5
Analytic Solution
Numerical Approximation
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 -1
-0.5
0
0.5
1
1.5
Analytic Solution
Crank–Nicolson
Implicit Euler
Fig. 9.23 Evolution of the analytic solution and numerical approximations of the 1D wave equation
in time, case 4 (h = 1
140 , Δt = 1
500 , T = 0.75). Left: initial time t = 0; right: time t = 0.75.
The Case of Two Space Dimensions
As with the heat equation, the two-dimensional case offers the opportunity of inves￾tigating how the geometry of the domain Ω ⊂ R2 affects the rate of convergence of
the methods. For this, as in Figure 9.14 (page 354), we will compare the cases of a
square, a convex polygon and the L-shaped domain. Since there is no closed formula
for the solution in these cases, we have calculated a reference solution numerically
on a very fine mesh using the implicit Euler method with very small timestep.
10−3 10−2 10−1
10−3
10−2
10−1
100
2
1
1
Mesh size h
Crank–Nicolson
Implicit Euler
Leapfrog
Fig. 9.24 Convergence rate of the FEM for the wave equation with discontinuous initial dis￾placement u0 in one space dimension; comparison of leapfrog, Crank–Nicolson and implicit Euler
methods.
We again test diverse choices of u0, u1 and the right-hand side f , in particular to
generate cases where the condition u ∈ C4([0, T], H1
0 (Ω) ∩ H2(Ω)) is either satisfied406 9 Numerical methods
or violated. The results are summarized in Figure 9.25. We choose functions for
u0, u1 and f of successively lower regularity in each case (in the nine pictures in
Figure 9.25, we have in each case, from left to right on the horizontal axis, C∞, C1,
C0 \ C1, discontinuous). In all cases the influence of the geometry of the domain
is considerably weaker than the influence of insufficient regularity of the data. This
also demonstrates the flexibility of the finite element method compared with finite
differences. In the case of a discontinuous right-hand side f the influence of the
domain is actually the opposite of what we know for elliptic and parabolic problems;
however, this will be far more a consequence of the specific choice of the data here,
in the sense that this particular f will be particularly well suited to the L-shaped
domain.
Leapfrog Crank–Nicolson Implicit Euler
0.5
1.0
1.5
Square
Polygon
L-Shaped Domain
u0
0.5
1.0
1.5
Square
Polygon
L-Shaped Domain
0.5
1.0
1.5
Square
Polygon
L-Shaped Domain
0.5
1.0
1.5
Square
Polygon
L-Shaped Domain
u1
0.5
1.0
1.5
Square
Polygon
L-Shaped Domain
0.5
1.0
1.5
Square
Polygon
L-Shaped Domain
0.5
1.0
1.5
Square
Polygon
L-Shaped Domain
f
0.5
1.0
1.5
Square
Polygon
L-Shaped Domain
0.5
1.0
1.5
Square
Polygon
L-Shaped Domain
Fig. 9.25 Convergence rates for the finite element method for the wave equation in two space
dimensions; comparison of leapfrog, Crank–Nicolson and implicit Euler methods for various ge￾ometries. Upper row: different initial displacements, middle row: different initial velocities, lower
row: different right-hand sides, in each case with decreasing regularity from left to right.
9.6* Comments on Chapter 9
The finite element method goes back to Alexander Hrennikoff (1941) and Richard Courant (1942).
While Hrennikoff discretized the domains using a rectangular mesh, similar to FDM, Courant used
triangles. After studies in Wrocław (at the time Breslau in Prussia), Zurich and Göttingen, Courant
obtained his doctorate in 1910 under David Hilbert; his thesis was entitled Über die Anwendung
des Dirichlet’schen Prinzipes auf die Probleme der konformen Abbildung (on the application of the
Dirichlet principle to problems of conformal mapping). In the same year he married Nerina Runge,
the daughter of the Göttingen mathematics professor Carl Runge (known among other things for9.6* Comments on Chapter 9 407
the Runge–Kutta method). He obtained his habilitation in 1912, before being drafted into the army
to fight in the First World War, where he was wounded. He was a professor in Göttingen from 1920
until 1933, when he left Germany, having been dismissed from his position by the Nazis owing both
to his Jewish heritage and his membership in the German Social Democratic Party. After a year in
Cambridge, Courant took up a professorship at New York University in 1935. There, from 1935
until 1958 he directed the institute which would come to bear his name in 1964, and which to this
day is one of the leading research institutes globally in applied mathematics. Among his successors
as director of the Courant Institute we can find such names as Louis Nirenberg and Peter Lax.
In 1928, together with K. Friedrichs and H. Lewy, Courant published his famous work Über die
partiellen Differenzengleichungen der mathematischen Physik (on the partial differential equations
of mathematical physics) in the Mathematische Annalen, which already contains the stability con￾dition later known as the CFL condition (cf. Remark 9.48). Friedrichs had obtained his doctorate
in 1925 in Göttingen under Courant, writing his dissertation on Die Randwert- und Eigenwert￾probleme aus der Theorie der elastischen Platten (boundary value and eigenvalue problems from
the theory of elastic plates). He accompanied Courant to New York and was, in turn, the doctoral
supervisor of Peter Lax in 1949. Robert Richtmyer worked with John von Neumann, among others,
and developed numerical methods for the solution of complex problems, investigated in turn by
Stanislaw Ulam. The resulting method is known today as the Monte Carlo method. In 1953 Richt￾myer moved to the Courant Institute in New York, where he published a joint paper with Peter Lax
on the Lax–Richtmyer equivalence theorem in 1956.
When developing the finite element method, Courant built on earlier work of Rayleigh, Ritz
and Galerkin, by constructing the finite-dimensional spaces in the Galerkin method with the help
of triangulations, as described above. As suitable as the finite element method is for treating elliptic
equations of second order (as we have seen), its later success was only made possible by the
development of sufficiently powerful computers. This is because the generation of a triangulation
and setting up the linear systems of equations (for which integrals need to be calculated numerically)
require significantly more computing power than is the case with the FDM. This also explains the
statement of Lothar Collatz in 1950, when the FDM seemed unchallenged: the difference method is
a generally applicable method for boundary value problems. It is easy to set up and even for coarse
mesh sizes generally yields, for relatively little calculation, a description of the solution function
which is often sufficient for technical purposes. In particular, in partial differential equations there
are areas where the difference method is the only practicable method and where other methods only
capture the boundary conditions with difficulty, if they are capable of doing so at all.2 How much
the world has changed!
Philippe Clément achieved a breakthrough in the convergence analysis of finite elements in
1975 with the eponymous interpolation operator and corresponding error estimates, [22]. It had
been known since the doctoral thesis of Jean Céa in 1964 that the error in the finite element
approximation was, up to a multiplicative constant, essentially the error of the best approximation
in the trial space. In order to estimate the error of the best approximation, one uses the Clément
operator, whose real brilliance consists in linking Sobolev regularity and interpolation.
2 “Das Differenzenverfahren ist ein bei Randwertaufgaben allgemein anwendbares Verfahren. Es ist
leicht aufstellbar und liefert bei groben Maschenweiten im Allgemeinen bei relativ kurzer Rechnung
einen für technische Zwecke oft ausreichenden Überblick über die Lösungsfunktion. Insbesondere
gibt es bei partiellen Differenzialgleichungen Bereiche, bei denen das Differenzenverfahren das
einzig praktisch brauchbare Verfahren ist und bei denen andere Verfahren die Randbedingungen
nur schwer oder gar nicht zu erfassen vermögen.”408 9 Numerical methods
9.7 Exercises
Exercise 9.1 Let fα be as in (9.15) and let uα ∈ C1([0, 1]) be the solution of Luα = fα
in accordance with (9.1). Define Lh in terms of the central difference quotient on
an equidistant mesh and show that for the solution uα,h of Lhuα,h = fα we have
uα − uα,h h,∞ = O(h) as h → 0+.
Exercise 9.2 For xi := ih, h = 1
N+1 , 0 ≤ i ≤ N + 1, show (9.12)), i.e. show that

N
k=1 G(xi, xk ) = 1
2h xi(1 − xi) for the Green’s function G from (9.3).
Exercise 9.3 Show that the stiffness matrix of the Dirichlet problem in one space
dimension is identical to the system matrix of the FDM with central difference
quotients. Here the mesh is equidistant in both cases.
Exercise 9.4 Let T be a triangle with verticest1, t2, t3. Show that for each i ∈ {1, 2, 3}
there exists exactly one vi ∈ P1(T) such that vi(tj) = δi,j, j = 1, 2, 3.
Exercise 9.5 Consider the initial value problem y
(t) = − 1
y(t)

1 − y(t)2, y(0) = 1
with solution y(t) = √
1 − t2, 0 ≤ t < 1. Why does the explicit Euler method yield
the solution y ≡ 1 independently of the step size?
Exercise 9.6 Let Ω = (a, b) ⊂ R, −∞ < a < b < ∞ and P := /
p∈N Pp(Ω), where
Pp(Ω) denotes the space of polynomials on Ω of degree no greater than p ∈ N. Show
that P is a normed space for the norm ·∞, but not a Banach space.
Exercise 9.7 Consider a variant ih of the Clément interpolation of L1(Ω) in the
space of linear finite elements Vh = X1,0
h (without homogeneous Dirichlet boundary
conditions) on quasi-uniform meshes {Th}, defined as follows. For every node ti we
consider the functional πi : L1(Ω) → R given by
πi(u) =
∫
ωi
uφi dx ∫
ωi
φi dx−1
,
with node basis function φi and ωi := suppφi. The interpolation is then defined
by ihu = 

xi ∈N(Th) πi(u)φi, where N(Th) denotes the set of nodes of Th. Prove the
following estimates:
(a) If u ≥ 0, then ihu ≥ 0.
(b) u − πi(u)L2(ωi) ≤ chi ∇uL2(ωi), where hi = maxT ⊂ωi hT .
(c) u − ihuL2(T) ≤ ch∇uL2(ω˜ T ), where ω˜T := /{K ∈ Th : K ∩ T  ∅}.
(d) u − ihuL2(Ω) ≤ ch∇uL2(Ω).
(e) u − ihuH−1(Ω) ≤ ch2 ∇uL2(Ω), where the dual space H−1(Ω) of H1
0 (Ω) is
equipped with the norm v H−1(Ω) = supφ∈H1
0 (Ω),φ0
(v,φ)
∇φL2(Ω) .9.7 Exercises 409
Exercise 9.8 Let T ⊂ R2 be a parallelogram. Prove that there exists an affine trans￾formation σ : Tˆ = (0, 1)
2 → T. What does the transformation that maps Tˆ onto a
general quadrilateral look like?
Exercise 9.9 Let the boundary value problem
−u(x) = f (x), x ∈ (0, 1),
u(0) = a, u(1) = b
be given, for fixed constants a and b and a continuous function f : [0, 1] → R.
Discretizing using finite differences with uniform mesh size h = 1/(N + 1), N ∈ N,
yields the linear system of equations Ahuh = fh as in (9.8). Show that the eigenvalues
of Ah are given by λj = 4
h2 sin2  jπh
2
	
, j = 1, 2,..., N, whose associated orthonormal
eigenvectores are v j = √
2h[sin(ijπh)]N
i=1, j = 1, 2,..., N.
Exercise 9.10 In 1870 Weierstrass found the following counterexample to the Dirich￾let principle, which consists of the following minimization problem for continuous
functions which does not have a solution: find a function u which minimizes the
functional
J(u) =
∫ 1
−1
[xu
(x)]2 dx
among all functions u ∈ C1([−1, 1]) such that u(−1) = 0 and u(1) = 1. Show using
the sequence of functions {un} defined by
un(x) = 1
2
+
1
2
arctan(nx)
arctan n , n = 1, 2,...
that this problem does, indeed, not have a solution.
Exercise 9.11 Let Ω ⊂ R2 be a simply connected polygon. Show that for any trian￾gulation of Ω the number of triangles plus the number of nodes minus the number
of edges always equals 1. Why does this not hold if Ω is multiply connected?
Exercise 9.12 Show that for any family of triangles the quasi-uniformity condition rT
ρT ≤ κ, T ∈ Th, h > 0 is equivalent to the existence of a strictly positive global
lower bound on the smallest interior angle of all triangles.
Exercise 9.13 Let T ⊂ R2 be an open triangle with vertices t1, t2, t3. Show that every
x ∈ T has a unique representation of the form x = a1t1 +a2t2 +a3t3, a1 +a2 +a3 = 1,
ai ≥ 0, i = 1, 2, 3.
Exercise 9.14 Let T ⊂ R2 be an open triangle and E := {v : T → R : ∃ a, b, c ∈
R such that v(x) = a + bx1 + cx2, x = (x1, x2) ∈ T}. Show that dim E = 3.410 9 Numerical methods
Exercise 9.15 Let A = (ai,j)i,j=1,...,M ∈ R(M−1)×(M−1)
, M ≥ 2, be a tridiagonal
matrix such that ai,i = 2 for i = 1,..., M − 1 and ai,i+1 = ai+1,i = −1 for 1 ≤ i ≤
M − 2. Show that, for all k = 1,..., M − 1,
Vk :=

sin 
ik π
M
 
i=1,...,M−1
, μk :=

2 − 2 cos 
k π
M
  = 4 sin2
 k
2
π
M

,
are, respectively, the eigenvalues and eigenvectors of A.
Exercise 9.16 (Convergence rate of the Galerkin approximation) Let V and H be
real Hilbert spaces such that V → H and let a : V × V → R be bilinear, continuous
and coercive. Set D(A) := {u ∈ V : ∃ f ∈ H such that a(u, v) = ( f, v)H for allv ∈ V}.
For 0 < h ≤ 1 let Vh ⊂ V be a finite-dimensional subspace of V. Finally, suppose
that for each u ∈ D(A) there exists a constant cu > 0 such that distV (u,Vh) :=
inf{u − χV : χ ∈ Vh} ≤ cu ϕ(h) for all 0 < h ≤ 1, where ϕ : (0, 1]→(0, ∞) is
some function such that limh↓0 ϕ(h) = 0.
(a) Prove the following statement: for f ∈ H, let u ∈ V, uh ∈ Vh be the solution
of the problem a(u, v) = ( f, v)H , v ∈ V and the solution of the approximating
problem a(uh, χ) = ( f, χ)H , χ ∈ Vh, respectively. Then there exists a constant
c1 > 0 such that u − uh V ≤ c1 ϕ(h)  f H for all 0 < h ≤ 1.
(b) Set D(A∗) := {w ∈ V : ∃g ∈ H such that a(v, w) = (v, g)H for all v ∈ V},
and suppose that for every w ∈ D(A∗) there exists a constant c∗
w > 0 such that
distV (w,Vh) ≤ c∗
w ϕ(h), 0 < h ≤ 1. Show that then there exists a constant c2 > 0
such that u − uh H ≤ c2 ϕ(h)
2  f H , 0 < h ≤ 1.
(c) Let H = L2(Ω), Ω ⊂ R2 be a polygon, let (Th)h>0 be a quasi-uniform family
of admissible triangulations, and let Vh be given by (9.37). Further suppose that
the form a is such that D(A) ⊂ H2(Ω) and D(A∗) ⊂ H2(Ω). Show that under
these assumptions there exists a constant c3 > 0 such that u − uh L2(Ω) ≤
c3 h2  f L2(Ω), 0 < h ≤ 1.
Suggestion: (a) Use Céa’s lemma and the uniform boundedness principle [2]. (b)
Apply (a) to the dual problem, as in the proof of Theorem 9.32(a).
Exercise 9.17 (Crank–Nicolson method) Prove Theorem 9.60.
Exercise 9.18 Let A ∈ Rn×n, g ∈ L2((0, T), Rn) and u0 ∈ Rn. Show that the problem
u(t) + Au(t) = g(t), t ∈ (0, T), u(0) = u0,
has a unique solution u ∈ H1((0, T), Rn) given by the formula u(t) = u0 +
∫ t
0 e−(t−s)Ag(s) ds.
Exercise 9.19 (Taylor expansion)
Let X be a Banach space, f ∈ Cn+1([a, b], X) and t0 ∈ (a, b). One can show as in the
case X = R that f (t) = Tn(t) + Rn(t) with9.7 Exercises 411
Tn(t) =
n
k=0
f (k)
(t0)
k! (t − t0)
k
, Rn(t) =
∫ t
t0
(t − s)
n
n! f (n+1)
(s) ds.
(a) Show that Rn(t)X ≤ |t − t0|
n+1
(n + 1)!  f (n+1)
C([a,b],X).
(b) Let n = 2. Show that Rn(t)X ≤ (b − a)
3
48  f (3)
C([a,b],X).
Suggestion: Choose t0 = 1
2 (a + b).
(c) If dim X = 1, then it is well-known that for each t ∈ [a, b] there exists a τ
between t and t0 such that Rn(t) = 1
(n+1)!(t − t0)
n+1 f (n+1)
(τ). Show that this is
no longer true if dim X ≥ 2.
Suggestion: Choose n = 0 and X = R2.Chapter 10
Maple®, or why computers can sometimes help
Maple® is a program developed and distributed by the company Maplesoft (also
in cooperation with universities and research institutes). It is a computer algebra
system, that is, a program which undertakes mathematical manipulations exactly
according to given rules; in particular, it does not commit roundoff errors as is the
case with numerical approximation methods.
Among other advantages, Maple® thus enables one to perform complicated cal￾culations on the computer, saving time and effort and reducing potential sources of
error. Of course, Maple® is not the only product on the market for computer algebra
systems; for example, MuPAD and Mathematica offer similar features. The following
remarks on the use of computer algebra systems are based on the syntax of Maple®;
the statements about strategies in general as well as the advantages and limitations
of such tools are, however, equally valid for all such programs. Most such computer
algebra systems also include some numerical methods; for Maple® this is the case
from Version 12 on.
In this section a number of basic technics will be introduced; however, we cannot,
nor do we wish to, lay claim to giving a complete treatise on the solution of the
partial differential equations presented here using Maple®.
Chapter overview
10.1 Maple® ....................................... 414
10.2 Exercises ...................................... 421
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4_10
413414 10 Maple®, or why computers can sometimes help
10.1 Maple®
We will, in particular, use the package PDEtools, originally developed at the Uni￾versity of Waterloo (Ontario, Canada) by Edgardo S. Cheb-Terrab among others,
together with Waterloo Maple Inc. Starting with the version R5, PDEtools is an
integral part of Maple®, meaning in particular that very comprehensive online doc￾umentation is available. This also means that the commands from PDEtools can be
called up directly without the need for a library.
In what follows, we will show a few examples of Maple® worksheets, all of
which were generated using Version 12 and tested up to Version 17. The program
commands are always given after the symbol “>”. The output from Maple® appears
in normal font (not typewriter style) and is additionally centered; such output can be,
and was, generated by Maple® using the LATEXexport command. At the head of each
of the following Maple® commands is the file name. The corresponding Maple®
worksheets (whence the file extension “mw”) are available on the website of the
book.
10.1.1 Elementary examples
We start with two elementary examples, for the linear transport equation and the
wave equation. For the linear transport equation, we first consider Worksheet 10.1.
File: LinTransport.mw
> restart:
> infolevel[pdsolve]:=5:
> eq := diff(u(t,x),t)+c*diff(u(t,x),x) = 0;
eq := ∂
∂t
u (t, x) + c ∂
∂x u (t, x) = 0
> pdsolve(eq);
Checking arguments ...
First set of solution methods (general or quase general solution)
Second set of solution methods (complete solutions)
Trying methods for first order PDEs
Second set of solution methods successful
Maple®-Worksheet 10.1: Linear transport equation.
The commands used are largely self-explanatory. Partial derivatives can be de￾clared using the command diff, so that LinTransport defines the homogeneous
linear transport equation. This partial differential equation is then solved (analyti￾cally) using pdsolve, a command from PDEtools. Here we have generated further10.1 Maple® 415
output using the command infolevel=5. We see that various solution strategies
(and heuristics) are tested internally. Of course, we obtain the same general solution
as in Example 2.2 with c = a. Freely choosable solution components are always
indicated in Maple® by an underscore “_”, thus _F1 is the initial value function
u0 ∈ C1(R) given in Example 2.2.
For our second example we consider the wave equation in Worksheet 10.2. Here,
as well, the Maple® commands used should be self-explanatory. We recover the
general solution formula of d’Alembert (3.3) with the two freely choosable functions
_F1 and _F2 (labeled there as ϕ and ψ).
File: WaveEqn.mw
> restart:
> eq:=diff(u(t,x),t,t) - diff(u(t,x),x,x)=0;
eq := ∂2
∂t2 u (t, x) − ∂2
∂x2 u (t, x) = 0
> pdsolve(eq);
u (t, x) = _F1 (x + t) + _F2 (x − t)
Maple®-Worksheet 10.2: Wave equation.
Of course Maple® cannot determine such general solution formulae “by itself”; it
is programmed so as to test whether, given a differential equation, internally known
solution methods or heuristics can be used. If a solution can be determined, then it
is returned as output, if not, then not. Thus Maple® can do exactly as much as it has
been taught to do.
10.1.2 Solutions via Fourier transforms
Here we consider the initial value problem for the heat equation, cf. Example 3.60.
We wish to solve this problem with the help of Fourier transforms; for this, we
consider the Maple® worksheet HeatExFourier (Worksheet 10.3 on page 416). We
are already familiar with the definition of the differential equation; the definition of
the initial conditions is, again, largely self-explanatory. Note that at this point in time
the function f (x) is still unknown. Here we assume asymptotic boundary conditions
in space, that is, that the solution decays as x → ±∞. Only afterwards (under point
1.) do we define the initial value f (x) as the characteristic function 1(0,1) of the
unit interval. In order to be able to use the integral transformations that Maple® can
perform, we need to load the corresponding package.
Under 2. we first express the function f as a linear combination of Heaviside
functions, which are known to Maple®. The reason for doing so is that Maple® also416 10 Maple®, or why computers can sometimes help
File: HeatExFourier.mw
> restart;
> eq:=diff(u(t,x),t)-a^2*diff(u(t,x),x,x)=0; > ICs:=u(0,x)=f(x);
eq := ∂
∂t
u (t, x) − a2 ∂2
∂x2 u (t, x) = 0
ICs := u (0, x) = f (x)
— 1. ———————————————– > a:=1; > f:=x->piecewise(x<0,0,x<1,1,0): > ’f(x)’=f(x);
a := 1
f (x) =
⎧⎪⎪⎪⎨
⎪⎪⎪
⎩
0 x<0
1 x<1
0 otherwise
> with(inttrans):
— 2. ———————————————–
> f:=unapply(convert(f(x),Heaviside),x);
f := x → Heaviside (x) − Heaviside (−1 + x)
> assume(t>0);
— 3. ———————————————–
> fourier(eq,x,w);
w2fourier(u (t, x), x, w) + ∂
∂t
fourier(u (t, x), x, w) = 0
— 4. ———————————————–
> ode:=subs(fourier(u(t,x),x,w)=s(t),%);
ode := w2s (t) + d
dt s (t) = 0
— 5. ———————————————–
> dsolve({ode,s(0)=fourier(f(x),x,w)},s(t));
s (t) = i

e−iw − 1
	
e−w2 t
w
— 6. ———————————————–
> sol:=invfourier(rhs(%),w,x);
sol := −1/2 erf 
1/2 −1 + x √
t

+ 1/2 erf 
1/2 x
√
t

> plot3d(sol,x=-1..2,t=0..1,orientation=[50,40], > numpoints=2000,axes=framed,color="gray");
— 7. ———————————————–
> pdetest(u(t,x)=sol,eq);
0
Maple®-Worksheet 10.3: Fourier transformation for the heat equation.10.1 Maple® 417
knows the Fourier transform of the Heaviside function and can thus easily deal with
such a linear combination of them. This also illustrates how one sometimes needs to
apply small “tricks” to help Maple® out.
In 3. we transform the differential equation with the help of the Fourier transform,
and 4. serves to write the ordinary differential equation conveniently in terms of
the unknown function s(t). This ordinary differential equation can be solved using
the command dsolve (cf. 5.), and in the case one obtains a closed formula for the
solution. In 6. we apply the inverse Fourier transformation. We then express the
solution sol in terms of the function erf, where
erf(x) := 2
√π
∫ x
0
e−t2
dt
is the error function, related but not identical to the distribution function of the
standard normal distribution. Here again, we can see that one already needs to
know in advance how to use Maple®. The integral erf does not have a closed
representation, meaning that Maple® cannot express the solution (as an inverse
Fourier transform) in terms of a simple formula. However, if one already knows
what the solution could look like, then one can try to express sol in terms of the
corresponding functions, as has been done successfully in this case. We next depict
the solution thus obtained graphically, cf. Figure 10.1 (left). Finally, in 7. we check
the solution by inserting it into the differential equation and obtaining the error value
0, as desired.
Fig. 10.1 Maple® solution of ut = ux x with u0 = χ[0,1) using Fourier transforms (left) and
Maple® solution of the inhomogeneous heat equation using Laplace transforms (right).418 10 Maple®, or why computers can sometimes help
10.1.3 Laplace transforms
For our next example we consider the initial-boundary value problem (3.109) for the
inhomogeneous heat equation on the interval [0, 1], that is,
ut − uxx = f (t, x) := −(t
2 + x)e−t x, (t, x) ∈ R+ × (0, 1),
u(0, x) = 1, x ∈ (0, 1),
u(t, 0) = a(t) := 1, u(t, 1) = b(t) := e−t
, t > 0,
cf. Example 3.66. The corresponding commands are listed in Worksheet 10.4 (named
HeatExLaplace, page 419). The declaration of the differential equation and the
boundary and initial conditions should now be clear. In 1. we then transform the
equation using Laplace transforms and substitute in the initial conditions. In 2. and
3. we set up the boundary value problem for the ordinary differential equation,
and solve it in 4. using dsolve. After the inverse transformation in 5., we wish to
represent the solution in terms of trigonometric functions, which does indeed lead to
a simple form of the solution, represented graphically in Figure 10.1 (right). Finally,
in 6. we check the result by inserting the solution into the original equation.
10.1.4 It can also be done numerically
As mentioned at the beginning of the chapter, in Version 12 the set of numerical
solution methods available in Maple® was expanded. We now wish to demonstrate
this briefly using the same example as just now, in Worksheet 10.5. The only dif￾ference is in the syntax of the additional option numeric in pdsolve. There is no
recognizable difference to the function depicted in Figure 10.1 (right), the solution
found using Laplace transforms.
Nevertheless, several warnings are called for. This example certainly shows how
simply one can obtain a numerical solution using Maple®; however, we do not know
which numerical method is used internally by Maple® to do so. We have already seen
that this depends strongly on the particular problem. Secondly, at least up until now
Maple® is known for not being optimally efficient in terms of calculation time. If
short calculation times are important, then it is advisable to use a specialist numerics
program instead.
10.1.5 Calculating function values
To finish, we return again to the solution formula for the Black–Scholes equation
from Sections 1.5 and 3.5, where we derived a closed formula for the solution (albeit
in terms of the distribution function N of the standard normal distribution, whose10.1 Maple® 419
File: HeatExLaplace.mw
> restart;
> f := (t,x) -> (-1)*(t^2+x)*exp(-t*x); > eq:=diff(u(t,x),t)-diff(u(t,x),x,x)=f(t,x); > a := t->1; > b := t->exp(-t); > ICs:=u(0,x)=1; > BCs:=u(t,0)=a(t), u(t,1)=b(t);
f := (t, x) → − 
t
2 + x

e−t x
eq := ∂
∂t
u (t, x) − ∂2
∂x2 u (t, x) = −

t
2 + x

e−t x
a := t → 1
b := t → e−t
ICs := u (0, x) = 1
BCs := u (t, 0) = 1, u (t, 1) = e−t
> with(inttrans):
— 1. ———————————————– > laplace(eq,t,s): > subs(ICs,%);
slaplace (u (t, x), t, s) − 1 − ∂2
∂x2 laplace (u (t, x), t, s) = −2 (s + x)
−3 − x
s + x
— 2. ———————————————–
> ode:=subs(laplace(u(t,x),t,s)=U(x),%);
ode := sU (x) − 1 − d2
dx2 U (x) = −2 (s + x)
−3 − x
s + x
— 3. ———————————————– > BC1 := U(0) = laplace(a(t),t,s): subs(BCs, %); > BC2 := U(1) = laplace(b(t),t,s): subs(BCs, %);
U (0) = s−1
U (1) = (1 + s)
−1
— 4. ———————————————–
> dsolve({ode, BC1, BC2},U(x));
U (x) = (s + x)
−1
— 5. ———————————————– > invlaplace(rhs(%),s,t): > sol:=collect(map(simplify,expand(%)),trig);
sol := 
et x 	−1
> plot3d(sol,x=0..1,t=0..1,orientation=[50,40], > numpoints=2000,axes=framed,color="gray");
— 6. ———————————————–
> pdetest(u(t,x)=sol,eq);
0
Maple®-Worksheet 10.4: Laplace transformation for solving the inhomogeneous heat
equation.420 10 Maple®, or why computers can sometimes help
File: HeatNumerical.mw
> restart: > f := (t,x) -> (-1)*(t^2+x)*exp(-t*x);
f := (t, x) → − 
t
2 + x

e−t x
> eq := diff(u(t,x),t) - diff(u(t,x),x,x) = f(t,x);
eq := ∂
∂t
u (t, x) − ∂2
∂x2 u (t, x) = −

t
2 + x

e−t x
> cond := u(0,x)=1, u(t,0)=1, u(t,1)=exp(-t);
cond := u (0, x) = 1, u (t, 0) = 1, u (t, 1) = e−t
> infolevel[’pdsolve/numeric’]:=3:
> Sol := pdsolve(eq,{cond},numeric);
Sol := module () export plot, plot3d, animate, value, settings; ... endmodule
> Sol:-plot3d(t=0..1,x=0..1.0,orientation=[50,40], > numpoints=2000,axes=box,axes=framed,color="gray");
Maple®-Worksheet 10.5: Numerical solution of the inhomogeneous heat equation.
File: BlackScholes.mw
> restart: > with(Statistics):
——————————————————————————————– > # Density of the standard normal distribution > N := proc(d) > CDF(Normal(0,1),d); > end proc:
——————————————————————————————– > # Formula for the value of a European call > BlackScholesCall:=proc(S,K,T,r,sigma) > local d1,d2; > d1:=(ln(S/K)+(r+sigma^2/2)*T)/(sigma*sqrt(T)); > d2:=d1-sigma*sqrt(T); > S*N(d1)-K*exp(-r*T)*N(d2); > end proc:
——————————————————————————————– > r := 0.05: > sigma := 0.5: > S := 100.0: > T := 1.0: > K := 100.0:
> BlackScholesCall(S,K,T,r,sigma);
21.79260422
Maple®-Worksheet 10.6: Calculating function values for the solution of the Black–
Scholes equation.10.2 Exercises 421
values cannot be calculated exactly). We wish to show how one can obtain these
values using Maple®, that is, one is essentially using Maple® as a “high-powered,
intelligent calculator”. For this, in Worksheet 10.6 we see two procedures; we have
used Maple® much like a programming language. The first procedure returns the
values of the function N. Since this can only be done approximatively due to the
integral involved, a numerical quadrature method is hidden behind the command
statevalf with the parameters cdf (cumulative density function) and normald
(normal distribution).
The procedure BlackScholesCall implements the solution formula derived and
proved in Theorem 3.47. The procedure thus returns the value of the option for the
given parameters for r, σ, S, T and K. The valuation formula was and is used in
exactly this way in banks. Here, however, the same warning applies as above. If
one often needs to use this formula (for example within the scope of Monte Carlo
simulations) then Maple® is rather less efficient than many other implementations
(for example in a higher programming language). On the other hand, this example
again demonstrates how easy Maple® is to use.
10.2 Exercises
Exercise 10.1 Write Maple® scripts to determine the respective solutions of the
partial differential equations listed in Table 2.1. In each case choose initial and/or
boundary conditions in such a way that it is possible to determine a solution formula
using Maple®.
Exercise 10.2 Write programs to solve the inhomogeneous heat equation using finite
differences and finite elements, respectively. Compare the runtimes with those of
Worksheet 10.5.Appendix
In this appendix we gather together the basic definitions and results from functional
analysis and integration theory that we require throughout the book. Moreover, we
add some more details concerning the derivation of the Black-Scholes equation.
A.1 Banach spaces and linear operators
Let E be a vector space over K = R or C. We usually consider real vector spaces,
but for some applications such as the Fourier transforms considered in Chapter 6 we
require K = C. Let · : E → [0, ∞) be a norm on E; that is, it satisfies
(N1)  f  = 0 ⇐⇒ f = 0 for all f ∈ E;
(N2) λ f  = |λ|  f , λ ∈ K, f ∈ E;
(N3)  f + g≤ f  + g, f, g ∈ E.
We say that (E,  · ) (or for brevity just E) is a normed vector space, or just
normed space for short. We sometimes write ·E for the norm in place of ·
(for example when various norms appear). If fn, f ∈ E, then we say that ( fn)n∈N
converges to f , and write limn→∞ fn = f or fn → f , if limn→∞  fn − f  = 0. A
sequence ( fn)n∈∞ has at most one limit. Every convergent sequence ( fn)n∈∞ is a
Cauchy sequence, that is, for all ε > 0 there exists an n0 ∈ N such that  fn − fm < ε
for all n, m ≥ n0. If conversely for every Cauchy sequence ( fn)n∈∞ there exists an
f ∈ E such that limn→∞ fn = f , then we say that E is complete. A Banach space is
a complete normed space. Suppose that E and F are normed spaces and T : E → F
is a linear mapping. Then we say that T is continuous if limn→∞ fn = f in E always
implies that limn→∞ T fn = T f in F. The linearity of T allows us to characterize the
property of continuity as follows.
Theorem A.1 The following statements are equivalent.
(i) T is continuous;
(ii) There exists a constant c ≥ 0 such that T f  ≤ c f  for all f ∈ E;
(iii) T  := sup f  ≤1 T f  < ∞.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4
423424 Appendix
If T is continuous, then T f ≤T   f  for all f ∈ E.
Proof (i) ⇒ (iii) We prove the contrapositive: suppose that T  = ∞; then there
exists a sequence of vectors fn ∈ E such that  fn  ≤ 1 but T fn  ≥ n for all n ∈ N.
If we set gn := 1
n fn, then limn→∞ gn = 0, but Tgn = 1
nT fn and so Tgn  ≥ 1. Thus
Tgn does not converge to 0 = T0, and T is not continuous at 0.
(iii) ⇒ (ii) Let g ∈ E be arbitrary, g  0, and set f := g−1g. Then  f  = 1,
and so g−1 Tg = T f ≤T . This shows that (ii) holds with c := T ; and in
particular (since the inequality obviously holds for f = 0), T f ≤T   f  for all
f ∈ E.
(ii) ⇒ (i) Suppose that limn→∞ fn = f . Since T fn − T f  = T( fn − f ) ≤
c f − fn , it follows that limn→∞ T fn − T f  = 0, that is, limn→∞ T fn = T f .

A set M ⊂ E is said to be bounded if there exists a constant c > 0 such that
x ≤ c for all x ∈ M. Condition (iii) in the above theorem asserts that the image
of the unit ball {x ∈ E : x ≤ 1} under T is bounded in F. For this reason,
continuous linear mappings are often also called bounded (linear) operators. We
call the expression T  the operator norm of T. Now consider the special case
F = K. A linear mapping ϕ : E → K is called a linear form (or functional). If ϕ is
continuous, then in accordance with the above definition
ϕ := sup
 f  ≤1
|ϕ( f )|
is the norm of ϕ. This satisfies |ϕ( f )| ≤ ϕ  f  for all f ∈ E. A subspace D ⊂ E
of E is said to be dense in E if for all f ∈ E there exists a sequence ( fn)n∈N ⊂ D
such that limn→∞ fn = f . If the codomain of a densely defined bounded operator
is complete, then the operator can be extended continuously to an operator on the
whole space.
Theorem A.2 (Continuous extension) Let F be a Banach space, E a normed space
and D a dense subspace of E. If T0 : D → F is a bounded linear operator, then it
has a unique continuous extension T : E → F. This operator is linear and satisfies
T  = T0 .
Proof Let f ∈ E be arbitrary. By assumption, there exist fn ∈ D such that
limn→∞ fn = f . Since T0 fn − T0 fm≤T0   fn − fm, we see that (T0 fn)n∈N
is a Cauchy sequence. We define T f := limn→∞ T0 fn (it is easy to show that
this definition is independent of the choice of sequence ( fn)n∈N); then it is also
not hard to show that T is linear. Moreover, we have T f  = limn→∞ T0 fn  ≤
lim supn→∞ T0   fn  = T0   f . This shows that T ≤T0 . On the other hand,
since T is an extension of T0, it is immediate that T0 ≥T . 
The following theorem gives us an efficient but simple criterion to check the
continuity of a linear mapping. An essential requirement is the completeness of the
underlying spaces. We refer to [2] for the proof.A.2 The space C(K) 425
Theorem A.3 (Closed graph theorem) Let E and F be Banach spaces and T :
E → F linear. Suppose that, whenever xn → x in E and T xn → y in F, we also
have T x = y. Then T is continuous.
This theorem often allows huge simplifications in proofs of continuity. In fact,
assume that limn→∞ xn = x; then the continuity of T requires two things:
(a) y := limn→∞ T xn exists, and
(b) y = T x.
The closed graph theorem allows us to omit the proof of (a), or, more precisely,
to assume convergence in (a) as our starting point. It only remains to prove the
identity (b). A direct consequence of the closed graph theorem is the bounded
inverse theorem.
Theorem A.4 (Bounded inverse theorem) Let E and F be Banach spaces and
T : E → F linear, bijective and continuous (that is, bounded). Then T−1 is linear
and continuous.
Proof The linearity is easy to check. For the proof of continuity we use the close
graph theorem. Let gn and g ∈ F be such that gn → g and T−1gn → f . Since T
is continuous, it follows that gn = TT−1gn → T f . Thus T f = g and so f = T−1g,
which is what we had to show. 
The bounded inverse theorem plays a role in the solution of equations: suppose
that T : E → F is linear and continuous; then to say that T is bijective means that
for every f ∈ F there exists a unique u ∈ E such that Tu = f . The bounded inverse
theorem tells us that in the solution u is automatically continuously dependent on
the given data f .
A.2 The space C(K)
Let K ⊂ Rd be a compact set. Then C(K) := { f : K → R : f continuous} is a
real Banach space, where the vector space structure is given by pointwise addition
( f + g)(x) := f (x) + g(x), x ∈ K and scalar multiplication (λ f )(x) := λ f (x), x ∈ K
(for f, g ∈ C(K) and λ ∈ R); and the norm is the supremum norm  f C(K) :=
supx∈K | f (x)|, f ∈ C(K). We refer to [2] for the proof of completeness. We wish to
discuss two remarkable theorems on C(K). The first gives a condition under which
a subspace of C(K) is dense in C(K), while the second gives a condition for the
compactness of subsets of C(K). Observe that if f, g ∈ C(K), then the product f · g
is also in C(K), where ( f · g)(x) := f (x) · g(x), x ∈ K. That is, C(K) is an algebra.
The following theorem gives a useful criterion for the density of a subalgebra of
C(K). It is due to Marshall Harvey Stone (1903–1989); see [53, Ch. 7.7] for a proof.
Theorem A.5 (Stone–Weierstrass) Let K ⊂ Rd be compact and let F be a subspace
of C(K) which has the following three properties:
(a) f, g ∈ F ⇒ f · g ∈ F;426 Appendix
(b) all constant functions belong to F;
(c) for all x, y ∈ K, x  y, there exists a function f ∈ F such that f (x)  f (y).
Then F is dense in C(K), that is, for every f ∈ C(K) there exists a sequence ( fn)n∈N
in F such that limn→∞ fn = f in C(K).
A subspace which satisfies (a) is called a subalgebra of C(K). If (c) holds, then
we say that F separates points in K. An immediate consequence of Theorem A.5
is Weierstrass’ theorem, which states that every continuous function on a compact
interval may be approximated uniformly by polynomials.
The second theorem we shall present has its motivation in an unfortunate property
of infinite-dimensional normed spaces: in such spaces one can always find a bounded
sequence which does not have a convergent subsequence. We call a set L in a normed
space relatively compact if every sequence in L has a convergent subsequence; we
say L is compact if it is additionally closed. Relatively compact sets are always
bounded, but boundedness alone is insufficient to imply relative compactness. In
spaces of the form E = C(K) one can identify exactly what is required in addition
to boundedness: the equicontinuity. A set L ⊂ C(K) is called equicontinuous if it
has the following property: let x0 ∈ K and ε > 0. Then there exists δ > 0 such that
|x − x0| < δ implies | f (x) − f (x0)| < ε for all f ∈ L. More precisely, we have the
following theorem [54, Thm. 11.28]:
Theorem A.6 (Arzelà–Ascoli) A subset L of C(K) is relatively compact if and only
if it is bounded and equicontinuous.
Example A.7 Let L := { f ∈ C([a, b]) :  f C(K) ≤ c,  f 
C(K) ≤ c} for some
c ≥ 0. Then L is relatively compact. 
Proof Clearly L is bounded. Moreover, given x0 ∈ [a, b] and f ∈ L, we have
| f (x) − f (x0)| = |
∫ x
x0 f 
(y) dy| ≤ c|x − x0|; this yields the equicontinuity. 
A.3 Integration
We assume familiarity with the Lebesgue integral and refer in case of necessity to
the compact introduction of Bartle [13]. Here we merely wish to make available the
central convergence theorems as well as a version of the theorem of Fubini–Tonelli
for annular domains. Throughout, we will suppose that Ω ⊂ Rd is an open set. If
f : Ω → [0, ∞] is measurable, then we denote by ∫
Ω f dx ∈ [0, ∞] its Lebesgue
integral over Ω. If ∫
Ω f dx < ∞, then f (x) < ∞ for almost every x ∈ Ω, and we say
that f is integrable. The monotone convergence theorem due to Beppo Levi reads
as follows.
Theorem A.8 (Monotone convergence) Let fn : Ω → [0, ∞] be measurable and
suppose that fn ≤ fn+1 pointwise almost everywhere, for all n ∈ N. Then f (x) :=A.3 Integration 427
supn∈N fn(x) defines a measurable function and ∫
Ω f (x) dx = supn∈N
∫
Ω fn(x). If
there exists a constant c ≥ 0 such that ∫
Ω fn dx ≤ c for all n ∈ N, then f is
additionally integrable.
This means that we may exchange the order of limit and integration. If the
functions are not positive, then the assertion remains true if there exists an in￾tegrable dominating function. This is the statement of the Dominated Conver￾gence Theorem, which is due to Lebesgue in 1910. We wish to formulate a
more general version for p-integrable functions. So let 1 ≤ p < ∞ and set
Lp(Ω) := #
f : Ω → R : f is measurable and ∫
Ω | f |
p dx < ∞$
. If we identify
two functions in Lp(Ω) whenever they agree with each other almost everywhere,
then  f Lp :=  ∫
Ω | f |
p dx	 1
p defines a norm with respect to which Lp(Ω) is a
Banach space. Of particular importance to us is the case p = 2; the space L2(Ω) is
a Hilbert space with respect to the inner product ( f, g) := ∫
Ω f (x)g(x) dx, which in￾duces the norm  f L2 = 
( f, f ). We can now formulate the Dominated Convergence
Theorem.
Theorem A.9 (Dominated convergence) Suppose that fn, f, g ∈ Lp(Ω) satisfy
| fn(x)| ≤ g(x) for almost every x ∈ Ω and all n ∈ N. If f (x) = limn→∞ fn(x) for
almost every x ∈ Ω, then limn→∞ fn = f in Lp(Ω), that is, limn→∞  fn − f Lp = 0.
The Dominated Convergence Theorem also admits a converse, if we allow passing
to subsequences.
Theorem A.10 (Converse of the Dominated Convergence Theorem) Suppose that
fn, f ∈ Lp(Ω) satisfy f = limn→∞ fn in Lp(Ω). Then there exist g ∈ Lp(Ω) and a
subsequence ( fnk )k ∈N such that
(a) | fnk (x)| ≤ g(x) almost everywhere, for all k ∈ N, and
(b) limk→∞ fnk (x) = f (x) almost everywhere.
If |Ω| < ∞, then L∞(Ω) ⊂ L2(Ω) ⊂ L1(Ω), where L∞(Ω) denotes the set of all
bounded measurable functions on Ω. A hyperrectangle in Rd is a set of the form
[a1, b1)×···×[ad, bd); if d = 2, then this reduces to a rectangle. We refer to linear
combinations of characteristic functions of hyperrectangles as step functions.
Theorem A.11 [47, Sec. 1.17] The space of all step functions is dense in Lp(Ω),
1 ≤ p < ∞.
We define Lp(Ω, C) := { f : Ω → C: Re f , Im f ∈ Lp(Ω}. Then Lp(Ω, C) is a
Banach space with respect to the norm  f Lp , for any 1 ≤ p < ∞. If f ∈ L1(Ω, C),
then we set ∫
Ω f dx = ∫
Ω Re f dx + i
∫
Ω Im f dx. The space L2(Ω, C) is a complex
Hilbert space with respect to the inner product ( f, g)L2 := ∫
Ω f (x) g(x) dx. Finally,
we wish to introduce an integration formula which recalls the theorem of Fubini–
Tonelli on interchanging the order of integration. Here, however, we will work
in polar coordinates; here we imagine integrating over the “rings” of an annular
domain like the rings of an onion. We denote by σ the surface measure on the sphere
Sd−1 = {x ∈ Rd : |x| = 1}, cf. Sections 7.1 and 7.2.428 Appendix
Theorem A.12 Let 0 ≤ R1 < R2 ≤ ∞ and Ω = {x ∈ Rd : R1 < |x| < R2}.
(a) Suppose that f : Ω → [0, ∞] is measurable or f : Ω → R is integrable. Then
∫
Ω f (x) dx = ∫ R2
R1
∫
Sd−1 f (r z) dσ(z)rd−1 dr.
(b) If in particular f : Ω → R is a continuous radial function, that is, we may write
f (x) = f (|x|)for all x ∈ Ω, then f is integrable if and only if ∫ R2
R1 | f (r)|rd−1 dr <
∞. Moreover, in this case, we have ∫
Ω f (x) dx = σ(Sd−1)
∫ R2
R1 f (r)rd−1 dr.
A.4 More details on the Black–Scholes equation
In addition to Section 1.5, we give some more details on the modeling leading to
the Black–Scholes equation. In the same manner as we cannot completely detail the
physical background of the equations introduced in Chapter 1, we do not describe all
required facts from economics and stochastics here. However, this appendix should
give a deeper understanding of the ideas leading to this famous equation. For more
background, in particular concerning the financial mathematics and the Itô integral,
we refer to [15, 43, 51].
In order to start with the actual modeling, we first need to fix our assumptions
about the behavior of the underlying. In the examples considered in Chapter 1,
we could appeal to physical laws; here we need to make an assumption about the
behavior of the share price in the future. This has two essential consequences:
• It is generally considered that the underlying (such as a share price) evolves
stochastically. We therefore need to model the behavior of S(t)stochastically and
as such require certain fundamentals from the theory of stochastic processes.
• The behavior of the underlying in the past is known from observing the market.
Whether this is a good predictor of the future, is naturally unknowable.
A.4.1 Basics of stochastics
We start by providing the necessary background material; we will however assume
familiarity with terms such as probability space and random variable.
Definition A.13 Let (Ω, A, P) be a probability space.1 We call any family X =
X(t)t ≥0 of random variables X(t) : Ω → R a stochastic process. We say that X has
a continuous path if the function t → X(t;ω) : [0, ∞) → R is continuous for all
ω ∈ Ω. 
1 Be aware of the double use of the notation Ω: in the context of differential equations, Ω ⊂ Rd
will denote the domain in the space variables; here, on the other hand, it is the underlying sample
space for the probability space.A.4 More details on the Black–Scholes equation 429
Example A.14 An example is the Wiener process W. This is a stochastic process
with continuous paths, for which in particular
P

{Wt ∈ (a, b)}	
= 1
√
2πt
∫ b
a
e− x2
2t dx,
that is, Wt is normally distributed. More precisely, W is uniquely determined by the
property that the increments Wt − Ws, t ≥ s > 0, are stochastically independent
and N (0, t − s)-distributed (that is, normally distributed with expectation μ = 0 and
variance σ = t − s), and W0 = 0. 
The next step is the definition of an integral with respect to a stochastic process,
for example in order to model cumulative yields. It is clear that standard notions of
integral such as those of Riemann or Lebesgue cannot be of use for general stochastic
processes since it is necessary to integrate functions which have infinite variation.
In stochastics, one defines the Itô integral of a stochastic process X with respect
to a second stochastic process W (with which we will again assume familiarity); we
will denote this integral by
∫ t
0
X(s) dW(s). (A.1)
The appelation “Itô integral” was chosen in honor of Kiyoshi Itô, who introduced the
corresponding calculus in 1951. We refer to the specialised literature for the details.
A.4.2 Black–Scholes model
Now we need to fix our assumptions on the future behavior of the price S of
the underlying. In the Black–Scholes model, one assumes that S(T) is a geometric
Brownian motion with drift μ and volatility σ. Here we will summarize its essential
properties.
Remark A.15 The geometric Brownian motion with drift μ and trend σ (sometimes
also called volatility, which is, however, μ − 1
2σ2) is the stochastic process S defined
by
S(t) := S(0) exp 
μt + σW(t) − 1
2
σ2
t

,
whereW denotes the Wiener processintroduced in Example A.14. It has the following
properties:
(a) S(t) is log-normally distributed (that is, log(S(t)) ∼N (μ, σ2)) with expectation
E(S(t)) = S(0) eμt and variance Var(S(t)) = S(0)
2e2μt
(eσ2t − 1).
(b) The paths S(t), t ∈ [0, T], are continuous.
(c) The following integral equation holds:430 Appendix
S(t) = S(0) +
∫ t
0
μ S(s) ds +
∫ t
0
σ S(s) dW(s), (A.2)
where the latter integral is to be understood as an Itô integral, as in Definition A.1.
The integral equation (A.2) is often written in the form
dS(t) = μ S(t) dt + σ S(t) dW(t). (A.3)
This equation may be referred to as an Itô differential equation with drift term
μ S(t) dt and diffusion σ S(t) dW(t). 
The next step is to determine the value of the option V(t, S(T)) in dependence
on the stochastic model for the share in (A.3). We therefore substitute the stochastic
process S from (A.3) for y in the value function V = V(t, y), as a model for the share
price. If we assume that V ∈ C2([0, T] ×R), then we can use the stochastic version of
the chain rule, known as Itô’s lemma, to obtain the stochastic differential equation
dV(t, S(t)) =

Vt + μ S(t)Vy +
1
2
σ2S(t)
2
Vyy 
dt + σS(t)Vy dW(t). (A.4)
A.4.3 The fair price
We can finally turn to the question of what a “fair” price might be. Here we will
consider a very simple market in which there are only two investment possibilities
available. On the one hand, one can deposit one’s money for a fixed interest rate
r > 0 and zero risk: if one deposits the amount B0 for t > 0 units of time, then at
time t one receives the amount B(t) = B0er t, cf. Remark 3.48. We further assume
that in our idealized market money can be borrowed for the same interest rate r > 0.
So if one borrows the amount Z0 at time 0, then at time t > 0 one needs to repay the
amount Z0er t. In our market the second investment possibility, which carries risk,
will be denoted by S and given by the geometric Brownian motion described above.
We imagine S as being the value of a share which does not pay dividends. We now
consider a portfolio (the value of a particular trading strategy)
X(t) = c1(t)B(t) + c2(t)S(t), (A.5)
where B(t) = er t, and c1, c2 are stochastic processes. Since S(0, ω) = S0, ω ∈
Ω, is deterministic (S0 is the known share price at time 0), we shall assume that
c1(0) and c2(0) are also deterministic (that is, they are numbers). We thus have
an investment strategy consisting of shares of total value c2(t)S(t) and a deposit
of value c1(t)B(t) earning a fixed interest rate. We also wish to assume that this
investment is self-financing, that is, profits will not be deducted and no additional
money will be injected. The change in the value of the portfolio over time thus
consists only of movements between the share component and the fixed-interest
deposit. Mathematically speaking, this means that the (cumulative) yield of theA.4 More details on the Black–Scholes equation 431
portfolio is given by the sum of the yield from interest and the change in the value
of the shares, Xyield(t) := ∫ t
0 c1(s) r ers ds +
∫ t
0 c2(s) dS(s). Written as a stochastic
differential equation, this takes the form dXyield(t) = c1(t) dB(t) + c2(t) dS(t). A
strategy is self-financing if and only if the value at time t is equal to the sum of the
initial value and the cumulative yield at time t, that is, if X(t) = X(0) + Xyield(t).
This means that dX(t) = dXyield(t) and so
dX(t) = c1(t) dB(t) + c2(t) dS(t). (A.6)
We now set ourselves the following task: find a continuous function V : [0, T] ×
R → R, V = V(t, y), which is continuously differentiable in t and twice continuously
differentiable in y in (0, T) ×R, together with a self-financing investment strategy c1,
c2 such that, for the process
Y(t) = c1(t) B(t) + c2(t) S(t) − V(t, S(t)), (A.7)
we have Y(t) = Y0er t, where Y0 = c1(0)B(0)+c2(0)S0 −V(0, S0). In other words, Y is
a risk-free investment, and the process X(t) has the same risk as the option V(t, S(t)).
We speak of a replicating portfolio. We start by showing that the above conditions
imply that V satisfies the Black–Scholes equation. Afterwards we will explain how
one can determine the fair price of the option.
The process Y satisfies the differential equation (written in stochastic notation)
dY(t) = r Y(t) dt. (A.8)
Now we insert the stochastic differential equations (A.8) (for dY(t)), (A.3) (for
dS(t)) and (A.4) (for dV(t)) into (A.6) and obtain (we omit the argument t)
dY = c1 dB + c2 dS − dV
= c1rBdt + c2(μSdt + σSdW)−(Vt + μSVy + σ2
2 S2
Vyy)dt − σSVy dW
=
?
c1 r B + c2 μ S −

Vt + μSVy +
1
2
σ2S2
Vyy  @dt
+
?
c2 σ S − σ SVy
@
dW. (A.9)
Since the portfolio Y(t) should be risk-free and self-financing, (A.8) and (A.6) hold;
thus dY = r Y dt = r(c1 B + c2 S − V) dt and so the term in the second line of (A.9)
needs to vanish, as can be seen by equating coefficients. Hence c2 − Vy(t, S(t)) = 0,
that is, c2(t) = Vy(t, S(t)). By equating the coefficients of dt we obtain
r

c1(t) B(t) + S(t)Vy(t, S(t)) − V(t, S(t))
=
=

c1(t)r B(t) − Vt(t, S(t)) − 1
2
σ2S(t)
2
Vyy(t, S(t))
.
The term rc1B cancels and we obtain the identity432 Appendix
Vt(t, S(t))+ σ2
2 S(t)
2
Vyy(t, S(t))+rS(t)Vy(t, S(t))−rV(t, S(t)) = 0. (A.10)
We recall that S is a stochastic process. Thus (A.10) means, when written out in full,
Vt(t, S(t, ω)) + σ2
2 S(t, ω)
2
Vyy(t, S(t, ω)) + rS(t, ω)Vy(t, S(t, ω))
−rV(t, S(t, ω)) = 0 (A.11)
for all t ∈ [0, T) and almost every ω ∈ Ω. We know that for each fixed time t > 0, for
each x ∈ [0, ∞) and ε > 0, one has S(t, ω)∈(x − ε, x + ε) with positive probability.
It follows that (A.11) holds for all t ∈ [0, T) and almost every ω ∈ Ω if and only if
V : (0, T) × R+ → R+ satisfies the Black–Scholes equation
Vt +
1
2
σ2 y2
Vyy + r yVy − rV = 0, (t, y)∈(0, T) × R. (A.12)
We have seen in Section 3.5 that the equation (A.12) admits a unique polynomially
bounded solution V which takes on the final value V(T, y) = (y − K)
+. The initial
value V0 := V(0, S0) of this solution V at the point y = S0 with share price S0 at initial
time t = 0 is the fair price of the option. To understand this, we imagine ourselves
in the role of a banker. The banker receives the amount V0 from her client at time
t = 0, and needs to pay back the amount (S(T) − K)
+, which depends on the share
price S(T) (a random variable), at time T. In order to generate this amount of money,
she proceeds as follows. She chooses a mixed, self-financing portfolio (A.5) in such
a way that Y(t) := X(t) − V(t, S(t)) is risk-free, that is, Y(t) = Y0er t. Here V is the
above solution of the Black–Scholes equation with V(T, y) = (y −K)
+. We have seen
that a necessary condition for the existence of this portfolio is that V satisfies the
Black–Scholes equation. Conversely, for the trading strategy of the banker, we need
to construct the processes c1, c2 (we have already seen that c2(t) = Vy(t, S(t)) needs
to hold); we abstain from giving the mathematical details. In practice, the banker
needs to reconfigure her portfolio on a daily (or even continuous) basis, an activity
referred to as hedging.
At time T the value of the portfolio is X(T) = Y0erT + (S(T) − K)
+, while the
initial investment in the portfolio is X(0) = Y0 +V0; this represents a difference from
the amount that the banker actually received from her client of Y0. The banker must
now distinguish between three cases and choose her strategy accordingly:
1st case: Y0 = 0: the banker receives exactly the amount to be paid back to the client
at time T from her portfolio.
2nd case:Y0 < 0: the banker receives only (S(T)−K)
+ − |Y0|erT from the portfolio
but is required to pay (S(T) −K)
+. But she also only needs to invest X(0) = V0 − |Y0|.
In this case, she invests the amount |Y0| at the fixed interest rate, which yields the
missing amount |Y0|erT at time t. In this way she can exactly afford the required
payment.
3rd case: Y0 > 0: the banker needs to invest X(0) = V0 +Y0 but has only received
V0 from the client. In this case, the solution is to borrow the difference Y0; at time TA.4 More details on the Black–Scholes equation 433
she must then pay back Y0erT . But since the portfolio yields (S(T) − K)
+ + Y0erT , it
covers both the loan repayment and the payment to the client.
In all three cases the price is fair: the banker can generate the promised payoff
(S(T) − K)
+ without risk.References
1. Abel, N.H. Untersuchungen über die Reihe 1+ m
1 x + m(m−1)
1·2 x2 +...+ m(m−1)(m−2)
1·2·3 x3 + ··· .
J. Reine Angew. Math., 1:311–339, 1826.
2. Alt, H.W. Linear Functional Analysis. Springer-Verlag, London, 2016. An application-oriented
introduction, Translated from the German edition by R. Nürnberg.
3. Amann, H. Ordinary Differential Equations. Walter de Gruyter & Co., Berlin, 1990. An
introduction to nonlinear analysis, Translated from the German by G. Metzen.
4. Andreev, R. Stability of space-time Petrov-Galerkin discretizations for parabolic evolution
equations. PhD thesis, ETH Zürich, No. 20842, 2012.
5. Arendt, W. Heat Kernels. Internet Seminar, Ulm University, 2005/06.
6. Arendt, W. and Batty, C.J.K. and Hieber, M. and Neubrander, F. Vector-valued Laplace Trans￾forms and Cauchy Problems, volume 96 of Monographs in Mathematics. Birkhäuser/Springer￾Verlag, Basel, 2nd edition, 2011.
7. Arendt, W. and Bénilan, P. Wiener regularities and heat semigroups on spaces of continuous
functions. In J. Escher and G. Simonett, editors, Topics on Nonlinear Analysis, pages 29–49.
Birkhäuser, Basel, 1998.
8. Arendt, W. and Chalendar, I. and Eymard, R. Galerkin approximation of linear problems in
Banach and Hilbert spaces. IMA J. Numer. Anal., 2020. online, doi 10.1093/imanum/draa067.
9. Arendt, W. and Daners, D. The Dirichlet problem by variational methods. Bull. Lond. Math.
Soc., 40(1):51–56, 2008.
10. Arendt, W. and Daners, D. Varying domains: stability of the Dirichlet and the Poisson problem.
Discrete Contin. Dyn. Syst., 21(1):21–39, 2008.
11. Arendt, W. and Grabosch, A. and Greiner, G. and Groh, U. and Lotz, H.P. and Moustakas, U.
and Nagel, R. and Neubrander, F. and Schlotterbeck, U. One-parameter Semigroups of Positive
Operators, volume 1184 of Lecture Notes in Mathematics. Springer-Verlag, Berlin, 1986.
12. Arendt, W. and ter Elst, A.F.M. and Kennedy, J.B. Analytical aspects of isospectral drums.
Oper. Matrices, 8(1):255–277, 2014.
13. Bartle, R.G. The Elements of Integration and Lebesgue Measure. John Wiley & Sons Inc.,
New York, 1995.
14. Biegert, M. and Warma, M. Removable singularities for a Sobolev space. J. Math. Anal. Appl.,
313(1):49–63, 2006.
15. Bingham, N.H. and Kiesel, R. Risk-neutral Valuation. Pricing and Hedging of Financial
Derivatives. Springer-Verlag, London, 2nd edition, 2004.
16. Braess, D. Finite Elements. Cambridge University Press, 3rd edition, 2007. Theory, fast
solvers, and applications in elasticity theory, Translated from the German by L.L. Schumaker.
17. Bramble, J.H. and Pasciak, J.E. and Xu, J. Parallel multilevel preconditioners. Math. Comp.,
55:1–22, 1990.
18. Brezis, H. Functional Analysis, Sobolev Spaces and Partial Differential Equations. Springer￾Verlag, New York, 2011.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4
435436 References
19. Brunken, J. and Smetana, K. and Urban, K. (Parametrized) First Order Transport Equations:
Realization of Optimally Stable Petrov-Galerkin Methods. SIAM J. Sci. Comput., 41(1):A592–
A621, 2019.
20. Carlson, J. and Jaffe, A. and Wiles, A. The Millenium Prize Problems. American Mathematical
Society, Providence, RI, 2006.
21. Ciarlet, P.G. The Finite Element Method for Elliptic Problems, volume 40 of Classics in
Applied Mathematics. Society for Industrial and Applied Mathematics (SIAM), Philadelphia,
PA, 2002.
22. Clément, P. Approximation by finite element functions using local regularization. Rev.
Française Automat. Informat. Recherche Opérationnelle Sér., 9(R-2):77–84, 1975.
23. Conway, J.B. Functions of One Complex Variable, volume 11 of Graduate Texts in Mathemat￾ics. Springer-Verlag, New York, 2nd edition, 1978.
24. Dautray, R. and Lions, J.-L. Mathematical Analysis and Numerical Methods for Science and
Technology. Vol. 1. Springer-Verlag, Berlin, 1st edition, 2000. Physical Origins and Classical
Methods.
25. Dautray, R. and Lions, J.-L. Mathematical Analysis and Numerical Methods for Science and
Technology. Vol. 2. Springer-Verlag, Berlin, 1st edition, 2000. Functional and Variational
Methods.
26. Edmunds, D.E. and Evans, W.D. Spectral Theory and Differential Operators. Oxford Mathe￾matical Monographs. The Clarendon Press Oxford University Press, New York, 1987.
27. Engel, K.-J. and Nagel, R. One-parameter Semigroups for Linear Evolution Equations, volume
194. Springer-Verlag, New York, 2000.
28. Evans, L.C. Partial Differential Equations, volume 19 of Graduate Studies in Mathematics.
American Mathematical Society, Providence, RI, 2nd edition, 2010.
29. Faye, H. Sur l’orígine du monde: Théories cosmogoniques des anciens et des modernes.
Gauthier-Villars et fils, 1896. https://gallica.bnf.fr/ark:/12148/bpt6k94881t/f113.image, last
accessed on 10.07.2020.
30. Filonov, N. On an inequality between Dirichlet and Neumann eigenvalues for the Laplace
Operator. St. Petersburg Math. J., 16:413–416, 2005.
31. Friedrichs, K.O. Selecta. Vol. 1,2. Contemporary Mathematicians. Birkhäuser, Boston, 1986.
32. Gilbarg, D. and Trudinger, N.S. Elliptic Partial Differential Equations of Second Order.
Springer-Verlag, Berlin, 3rd edition, 2001.
33. Grisvard, P. Elliptic Problems in Nonsmooth Domains, volume 24. Pitman, Boston, 1985.
34. Grossmann, C. and Roos, H.-G. Numerical Treatment of Partial Differential Equations.
Springer-Verlag, Berlin, 2007. Translated and revised from the 3rd (2005) German edition by
M. Stynes.
35. Hackbusch, W. Elliptic Differential Equations, volume 18. Springer-Verlag, Berlin, 2nd edition,
2017.
36. Henning, J. Die numerische Lösung der Wellengleichung mittels der Finite-Elemente-Methode
(in German). Bachelor’s thesis, Ulm University, 2019.
37. Henning, J. and Palitta, D. and Simoncini, V. and Urban, K. Matrix oriented reduction of
space-time Petrov-Galerkin variational problems. In J. Vermolen, C. Vuik, and M. Moller,
editors, Numerical Mathematics and Advanced Applications, ENUMATH 2019. Springer￾Verlag, Berlin, 2020, to appear.
38. Heuser, H. Gewöhnliche Differentialgleichungen (in German). B.G. Teubner, Stuttgart, 6th
edition, 2009.
39. Kac, M. Can one hear the shape of a drum? Amer. Math. Monthly, 73:1–23, 1966.
40. Kato, T. Estimation of iterated matrices, with application to the von Neumann condition.
Numer. Math., 2(1):22–29, Dec. 1960.
41. Kato, T. Perturbation Theory. Springer-Verlag, Berlin, 1966.
42. Katznelson, Y. An Introduction to Harmonic Analysis. Cambridge Univ. Press, 3rd edition,
2004.
43. A. Klenke. Probability theory – a comprehensive course. Springer, Cham, third edition, 2020.
44. Koch Medina, P. and Merino, S. Mathematical Finance and Probability. Birkhäuser, Basel,
2003.References 437
45. Laplace, P.-S. Essai philosophique sur les probabilités. Collection Epistémè. Christian Bour￾gois Éditeur, Paris, 5th edition, 1986.
46. Larsson, S. and Thomée, V. Partial Differential Equations with Numerical Methods. Springer￾Verlag, Berlin, 2009.
47. Lieb, E.H. and Loss, M. Analysis, volume 14 of Graduate Studies in Mathematics. American
Mathematical Society, Providence, RI, 2nd edition, 2001.
48. Lions, J.-L. and Magenes, E. Non-homogeneous Boundary Value Problems and Applications.
Vol. I. Springer-Verlag, New York-Heidelberg, 1972.
49. Longva, A.B. Finite element solutions to the wave equation in non-convex domains. Master’s
thesis, Norwegian University of Science and Technology (NTNU), Trondheim, Norway, 2017.
50. Meyers, N.G. and Serrin, J. H = W. Proc. Nat. Acad. Sci. USA, 51:1055–1056, 1964.
51. B. Øksendal. Stochastic differential equations. Springer, Berlin, sixth edition, 2003.
52. Quarteroni, A. and Sacco, R, and Saleri, F. Numerical Mathematics. Springer-Verlag, Berlin,
2nd edition, 2007.
53. Rudin, W. Principles of Mathematical Analysis. McGraw-Hill, 3rd edition, 1976.
54. Rudin, W. Real and Complex Analysis. McGraw-Hill, 3rd edition, 1987.
55. Schwab, C. and Stevenson, R. Space-time adaptive wavelet methods for parabolic evolution
problems. Math. Comp., 78(267):1293–1318, 2009.
56. Stein, E.M. Singular Integrals and Differentiability Properties of Functions. Princeton Math￾ematical Series, No. 30. Princeton University Press, 1970.
57. Stroud, A.H. Approximate Calculation of Multiple Integrals. Prentice-Hall Inc., Englewood
Cliffs, NJ, 1971.
58. Urban, K. Wavelet Methods for Elliptic Partial Differential Equations. Oxford Universtiy
Press, 2009.
59. Urban, K. and Patera, A.T. A new error bound for reduced basis approximation of parabolic
partial differential equations. C. R. Math. Acad. Sci. Paris, 350(3-4):203–207, 2012.
60. Urban, K. and Patera, A.T. An improved error bound for reduced basis approximation of linear
parabolic problems. Math. Comp., 83(288):1599–1615, 2014.
61. Xu, J. and Zikatanov, L. Some observations on Babuška and Brezzi theories. Numer. Math.,
94(1):195–202, 2003.Index of names
A
Abel, Niels H. (1802–1829), 55, 57
Ampère, André-Marie (1775–1836), 20, 24
Arzelá, Cesare (1847–1912), 161, 236, 426
Ascoli, Giulio (1843–1896), 161, 426
Aubin, Jean-Pierre (1939-), 346, 359
B
Babuška, Ivo (1926-), 355
Bachelier, Louis (1870–1946), 27
Baire, René L. (1874–1932), 138
Banach, Stefan (1892–1945), 133, 226, 229,
355, 423
Bateman, Harry (1882–1946), 8
Bernoulli, Daniel (1700–1782), 55
Bernoulli, Jakob (1655–1705), 55, 96
Bernoulli, Johann (1667–1748), 55
Bernstein, Sergei N. (1880–1968), 349
Black, Fischer S. (1938–1995), 11, 95
Borel, Félix É.J.É. (1871–1956), 245, 248, 253,
280, 308
Boussinesq, Joseph (1842–1929) , 19
Brezzi, Franco (1945-), 355
Bunjakowski, Viktor Y. (1804–1889), 118
Burgers, Johannes M. (1895–1981), 8, 37
C
Calabi, Eugenio (1923-), 20
Cauchy, Augustin-Louis (1789–1857), 31, 32,
55, 57, 118, 119, 128, 130, 132, 146, 160,
175, 185, 200, 205, 276, 350, 367, 368,
388, 390, 391, 423
Céa, Jean (1932-), 331, 332, 339, 344, 345,
407, 410
Cheb-Terrab, Edgardo S., 414
Clément, Philippe (1943-), 343, 371, 407
Collatz, Lothar (1910–1990), 407
Courant, Richard (1888–1972), 407
Cox, John C. (1943-), 95
Crank, John (1916–2006), 375, 396
D
d’Alembert, Jean-Baptiste (1717–1783), 11,
26, 52, 55, 104, 402, 415
de Vries, Gustav (1866–1934) , 19
Diderot, Denis (1713–1784), 27
Dini, Ulisse (1845–1918), 287
Dirichlet, Johann P.G.L. (1805–1859), 18, 55,
164, 222, 236, 265
Du Bois-Reymond, David Paul G. (1831–1889),
57
E
Einstein, Albert (1879–1955), 27
Euler, Leonhard P. (1707–1783), 18, 55, 96,
360, 364
F
Faraday, Michael (1791–1867), 24
Fejér, Leopold (1880–1959), 57
Fermi, Enrico (1901–1954), 19
Fischer, Ernst S. (1875–1954), 128, 150
Fourier, Jean B.J. (1768–1830), 9, 27, 55, 96
Fréchet, Maurice R. (1878–1973), 128, 150
Fredholm, Erik I. (1866–1927), 143, 151
Friedrichs, Kurt O. (1901–1982), 164, 183,
236, 362, 407
Frobenius, Ferdinand G. (1949–1917), 338
Fubini, Guido (1879–1943), 100, 101, 159,
161, 184, 186, 427
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4
439440 Index of names
G
Gårding, Lars J. (1919–2014), 142
Galerkin, Boris (1871–1945), 330, 331, 369,
407
Gardner, Clifford (1924–2013), 20
Gauss (Gauß), Carl F. (1777–1855), 14, 24, 83,
86, 89, 236, 243, 245, 265
Gelfand, Israel M. (1913–2009), 148, 154
Goldbach, Christian (1690–1764), 96
Gordon, Carolyn (1950-), 294
Gram, Jørgen P. (1850–1916), 122
Green, George (1793–1841), 247, 265
Greene, John M. (1928–2007), 20
Gronwall (Grönwall), Thomas H. (1877–1932),
46, 390
H
Hadamard, Jacques S. (1865–1963), 3, 71, 140,
151, 224, 236
Hahn, Hans (1879–1934), 229
Heaviside, Oliver (1850–1925), 417
Hesse, Ludwig O. (1811–1874), 20, 339
Hilbert, David (1862–1943), 123, 150, 236,
407
Hölder, Otto L. (1859–1937), 161, 164, 210,
289
Hrennikoff, Alexander (1896–1984), 407
I
Itô, Kiyoshi (1915–2008), 428, 429
K
Kac, Mark (1914–1984), 294
Kato, Tosio (1917–1999), 356
Kirchhoff, Gustav R. (1824–1887), 21
Korteweg, Diederik (1848–1941) , 19
Kronecker, Leopold (1823–1891), 22
Kruskal, Martin D. (1925–2006), 19, 20
L
Ladyshenskaya, Olga (1922–2004), 355
Lagrange, Joseph-Louis (1736–1813), 18, 55,
265, 342
Laplace, Pierre-Simon (1749–1827), 106,
236
Lax, Peter D. (1926-), 20, 129, 151, 236, 362,
384, 407
Lebesgue, Henri L. (1875–1941), 16, 150, 176,
184, 207, 230, 427
Leibniz, Gottfried W. (1646–1716), 176
Levi, Beppo (1875–1961), 296, 298, 426
Lewy, Hans (1904–1988), 362, 407
Lindelöf, Ernst (1870–1946), 32, 367, 388
Lions, Jacques-Louis (1928–2001), 134
Lipschitz, Rudolf (1832–1903), 32, 55, 265,
367, 388
M
Maxwell, James C. (1831–1879), 24
Merton, Robert C. (1944-), 11, 95
Meyers, Norman G. (1930-), 192, 266
Milgram, Arthur N. (1912–1961), 129, 151
Minkowski, Hermann (1864–1909), 20
Miura, Robert (1938–2018), 20
Monge, Gaspard (1746–1818), 20
N
Navier, Claude L.M.H. (1785–1836), 22
Nečas, Jindřich (1929–2002), 133
Neumann, Carl G. (1832–1925), 166, 265
Newton, Sir Isaac (1642–1727), 2, 22, 176,
236, 265
Nicolson, Phyllis (1917–1968), 375, 396
Nirenberg, Louis (1925–2020), 20, 407
Nitsche, Joachim A. (1926–1996), 346, 359
O
Ostrogradsky, Mikhail (1801–1862), 245, 265
P
Parseval des Chênes, Marc-Antoine
(1755–1836), 101, 121, 208, 278, 283
Pasta, John R. (1918–1984), 19
Perron, Oskar (1880–1975), 226, 236
Picard, Émile (1856–1941), 32, 367, 388
Plancherel, Michel (1885–1967), 101, 207
Poincaré, Jules H. (1854–1912), 27, 163, 164,
257
Poisson, Siméon-Denis (1781–1840), 18, 69
Prym, Friedrich (1841–1915), 236
Pythagoras of Samos (570-510 BC), 16, 117,
118, 121, 124
R
Rayleigh, John S. (1842–1919), 407
Rellich, Franz (1906–1955), 212, 255
Reynolds, Osborne (1842–1912), 23
Richtmyer, Robert (1910–2003), 384, 407
Riemann, Bernhard (1826–1866), 207, 236,
270
Riesz, Frigyes (1880–1956), 128, 129, 150,
248, 252
Ritz, Walther (1878–1909), 368, 378, 388, 401,
407
Robin, Victor G. (1855–1897), 168, 265
Ross, Stephen (1944–2017), 95
Rubinstein, Mark E. (1944–2019), 95Index of names 441
Runge, Carl (1856–1927), 407
Russell, John S. (1808–1882), 19
Rutishauser, Heinz (1918–1970), 314
S
Schmidt, Erhard (1876–1959), 122
Scholes, Myron S. (1941-), 11, 95
Schrödinger, Erwin (1887–1961), 20, 24
Schur, Issai (1875–1941), 338
Schwarz, Hermann (1843–1921), 118, 119,
128, 130, 132, 146, 160, 175, 185, 200,
203, 205, 350, 368, 390, 391
Serrin, James (1926–2012), 192, 266
Sobolev, Sergei L. (1908–1989), 157
Stampacchia, Guido (1922–1978), 202
Stokes, Sir George G. (1819–1903), 22
Stone, Marshall H. (1903–1989), 225, 426
Sylvester, James J. (1814–1897), 44
T
Taylor, Brooke (1685–1731), 17
Tietze, Heinrich F.F. (1880–1964), 225
Tonelli, Leonida (1885–1946), 159, 427
Tsingou Menzel, Mary (1928-), 19
U
Ulam, Stanislaw M. (1909–1984), 19, 407
V
von Neumann, John (1903–1957), 150, 364,
407
W
Webb, David L. (1953-), 294
Weierstrass (Weierstraß), Karl T.W. (1815–
1897), 74, 111, 225, 236, 248, 409,
426
Wiener, Norbert (1894–1964), 27, 428
Wolpert, Scott (1950-), 294
Y
Yau, Shing-Tung (1949-), 20
Z
Zabunsky, Norman J. (1929–2018), 19Index of symbols
C0([a, b]), 166
C(K), 425
C0(Ω), 79, 184, 207
C1,2, 77, 80, 87, 90
C2π , 57, 121, 125
C([a, b]), 52, 72, 116
C(Ω), 63, 71
C1
c (Ω), 278
C1
c (Ω), 156
C∞
c (Ω), 182
C∞
c (Ω), 139
Ck
c (Ω), 231
Ck
c (Ω), 178
Ck
c (Ω), 100
Cc (Ω), 100, 182, 218
Ck (Ω), 63
Ck ([a, b]), 166
Ck (I, E), 270
Ck (Ω), 247
D, 65, 223
D(A), 139
D(Ω), 305
D(Ω), 182, 208
D(Ω)+, 203
D(∂Ω), 225
D−
Δt , 360, 372
D+
Δt , 360, 372
D2
h, 360
δi, j , 22
Δh, 317, 326
dist, 187
dist, 126
dist, 331, 332
div, 14, 23, 24
div, 246
Dj , 245
ei, 22
f −, 200
f +, 12, 200, 206
H1
(0)
, 175
H1
(a)
(a, b), 163
H1
0 (Ω)+, 203
H1
0 (Ω), 163
H1
D(Ω), 200
H1
loc(Ω), 214
H1(Ω), 157
H1
per, 172
Hk (a, b), 166
Hˆ k (Rd), 208, 209
Hk
loc(Ω), 215
Hk (Ω), 163, 208, 209
H
, 128
→, 141, 143, 147, 154, 160, 163, 178, 255,
257, 258, 264, 266, 301, 311, 330, 334,
348, 359, 410 c
→, 143
ker, 128, 140
L1,loc, 177
L1,loc, 106
2, 151
L2(0, 1), 97, 150
L2,0(Ω), 261
L2(0, 2π), 97, 150
L2((0, 2π), C), 124
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4
443444 Index of symbols
L2,loc(Ω), 214
L2(Ω), 124, 125, 156, 208
L2(Ω, K), 125
Lh, 318
Lp(Ω), 97, 207, 427
∨, 203
∧, 203
M⊥, 118, 125, 127
N0, 76
∇, 14, 191
∇×, 24
Nh, 330, 331
·, 116, 128
|·|, 200
|·|, 16
·C(Ω)
, 347
·C(Ω)
, 119
|·|Ck , 362, 381
·h,∞, 320, 361
·H k , 191
|·|H k , 334
|·|H k , 164
·∞, 184, 226
·∞, 57, 119
·L2 , 156
·L2 , 119
Ωh, 325
Ω˚ h, 326, 328
∂Ωh, 326, 328
1, 184, 186, 187, 201, 206, 221, 238
1, 159, 176, 257, 258, 261, 264
1, 282
⊥, 118, 125, 127, 244, 245, 254
Re , 106, 108
rot, 24
(·, ·)h, 320
(·, ·)H k , 191
(·, ·)H k , 163
(·, ·)Hˆ k (Rd ), 208
span, 120
, 187, 189, 214
supp, 30, 182, 184
Tˆ, 335
Th, 343
Tz , 243
V, 305
VT, 341
Vh, 318
V0
h , 318
W(a, b), 302
W, 305
W(∂Ω), 224
w − lim, 135
, 135Index
A
Absolute value, 157
Adjoint, 145, 151, 297
Admissible, 342
Advection equation, 7
Affine, 332, 333, 335, 341
Algebra, 58
American option, 11
Ampère’s law, 24
Analytic, 3
Annulus, 86
Antilinear, 116
Approximate identity, 58, 59
Approximation theorem, 191, 194
A priori estimate, 140
Arbitrage, 12
Associated operator, 142
Asymptotically optimal, 330
Asymptotic boundary conditions, 103, 105
Aubin–Nitsche lemma, 359
B
Backward difference, 360, 372
Baire category theorem, 138
Banach algebra, 58
Banach space, 57, 97, 423
Barrier, 226
Barrier function, 226
Barycentric coordinates, 333
Bernstein’s inequality, 349
Best approximation, 127
Bilaplacian, 21
Bilinear form, 129, 132
Black–Scholes equation, 3, 12, 94, 418, 432
Block tridiagonal matrix, 328
Borel measure, 245, 253
Bound
exponential, 106
Boundary
parabolic, 77, 87
Boundary conditions, 15
asymptotic, 103, 105
Dirichlet, 18, 164, 166
mixed, 170
Neumann, 21, 166, 293
periodic, 171
Robin, 168, 265, 293
third kind, 168, 265
Boundary value problem, 18
Bounded, 132, 424
BPX preconditioner, 330, 351
B-splines, 351
Burgers’ equation, 37
C
C0-semigroup, 273
C∞-boundary, 243
C∞-domain, 243
Ck -boundary, 234, 242
Ck -domain, 242
Ck -graph, 242
normal, 242
Call option, 11
Cauchy problem, 31
Cauchy–Lipschitz theorem, 32, 367, 388
Cauchy–Schwarz inequality, 118, 119, 128,
130, 132, 146, 160, 175, 185, 200, 205,
368, 390, 391
generalized, 132
inverse, 350
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
W. Arendt, K. Urban, Partial Differential Equations, Graduate Texts in
Mathematics 294, https://doi.org/10.1007/978-3-031-13379-4
445446 Index
Cauchy problem, 276
Cauchy sequence, 423
Céa’s lemma, 331, 332, 339, 344–346, 356,
407
Central difference quotient, 317, 326
CFL condition, 362, 381, 391, 407
Chain rule, 193, 285
Characteristic function, 201
Characteristics, 31, 36
method of, 29
Charge density, 24
Circumradius, 336
Classical solution, 223, 258
Clément operator, 343, 371, 407, 408
Closed graph theorem, 234, 276, 345, 425
Coefficients
Fourier, 56
variable, 31, 231
Coercive, 129
Compact, 138, 426
imbedding, 143, 255
support, 100, 109, 156, 182, 187, 195, 219
Complement
orthogonal, 118, 125
Complete, 123, 423
Complexity, 314
Condition
initial, 12
terminal, 12
Cone
positive, 203
Congruent, 294
Conjugate gradient method, 330, 351
Conservation
of energy, 284, 285, 389, 391–393, 397
law, 5, 6, 9, 13
of mass, 2, 5, 13, 27
of momentum, 2
Consistency order, 322
Constant
coercivity, 129, 165, 169, 172, 205, 232,
264, 265, 306, 355
continuity, 129, 132, 141, 165, 169, 306
inf-sup, 133, 355
Planck, 25
Contained in a strip, 199
Continuity equation, 23
Continuous, 129, 132, 141, 423
boundary, 242
domain, 242
graph, 242
imbedding, 141
path, 428
semigroup, 273
Continuously differentiable, 270
Contraction, 226
Control volume, 13
Convection, 22
Convection equation, 7
Convergence, 120
Abel, 57
dominated, 427
to equilibrium, 282
in quadratic mean, 119
norm, 135
uniform, 119
weak, 135
Convex, 233
Convolution, 57, 69, 100, 106, 183
Convolution theorem, 101, 106
Crank–Nicolson method, 365, 396
Current density, 24
Curse of dimensionality, 330
D
D’Alembert’s formula, 52, 104
Dense, 120
densely defined, 139
Density, 4, 8, 421
Derivative
weak, 155–158, 161, 189, 190, 296
Diagonal argument, 137
Difference
backward, 360, 372
forward, 360, 372
symmetric, 360, 372
Differentiable, 270
Diffusion, 22, 89, 430
diffusion-convection-reaction equations, 169
Dini’s theorem, 287
Dirichlet
boundary conditions, 18, 53
eigenvalues, 274
Laplacian, 274
principle, 222, 409
problem, 69, 222
regular, 228
theorem, 179
Discrete
energy, 391, 397
Green’s function, 318
inner product, 319
maximum norm, 320
maximum principle, 315, 320
norm, 320
operator, 318
solution, 331
Discretization error, 322Index 447
Disk, 73
Dispersion, 19
Distribution function, 92
Divergence, 14, 246
form, 174
theorem, 246
Domain, 139, 141
of dependence, 52, 112, 384
Dominated convergence theorem, 213, 277
Drift, 429
Dual
problem, 346, 410
space, 128, 409
Duality pairing, 148
E
Efficiency, 314
Efficient, 314
Eigenvalue problem
generalized, 143
Elasticity, 16
Electric field strength, 24
Element, 332
Elliptic, 29, 42
maximum principle, 63, 69, 70, 109
Energy, 222
discrete, 391
kinetic, 53
method, 320
norm, 145, 378
potential, 16, 53
strain, 16
total, 16
Equation
Black–Scholes, 12, 94, 418, 432
Burgers, 37
continuity, 23
convection-reaction, 7
KdV, 19
Laplace, 18, 63
linear transport, 32
Maxwell, 24
minimal surface, 21
Monge–Ampère, 20, 44
Navier–Stokes, 2, 22, 23
plate, 21
Poisson, 18
scalar, 22
Schrödinger, 24
wave, 379
Equicontinuity, 426
Equidistant
grid, 317, 325, 326, 379, 390, 408
mesh, 327, 351
Euclidean norm, 17, 242, 336
Euler method
explicit, 360, 408
implicit, 364
European option, 11, 91
Evolution equation, 269
Expansion
barycentric coordinates, 333
Fourier, 55, 61, 122
orthonormal, 121, 272, 279, 284
Taylor, 17, 44, 245, 317, 326, 362, 380, 385,
394, 395, 399–401, 410
Expectation, 429
Exponential bound, 106
Exponentially bounded, 84, 86, 106
Exponentially decaying pulse, 98
Extension, 139, 188
operator, 254
property, 254–256, 267, 268
F
Fair price, 12
Faraday’s law, 24
Fermi–Pasta–Tsingou–Ulam experiment, 19
Final value problem, 32
Finite differences, 313, 315, 379
Finite difference method (FDM), 315, 360
Finite element methods (FEM), 330, 366
Finite elements, 313, 387
First variation, 17
Form
linear, 424
Forward difference, 360, 372
Fourier
coefficient, 56, 122, 223
expansion, 122
law, 9
polynomial, 122
series, 55, 56, 122
transform, 97, 181, 207, 208
inversion formula, 102
Fredholm alternative, 144, 150
Frobenius norm, 338
Fubini’s theorem, 101, 184, 186
Fully nonlinear, 20
Function
characteristic, 201
harmonic, 18, 178
radial, 85
sign, 200
wave, 24
Functional, 424
linear, 128
Fundamental solution of the heat equation, 105448 Index
Fundamental theorem of calculus, 36, 50, 176,
185, 242, 245, 270, 374
Lebesgue version, 159
G
Gårding’s inequality, 142
Galerkin method, 306, 330
Galerkin orthogonality, 331, 356, 369
Gaussian kernel, 83, 86, 89
Gauss semigroup, 273
Gauss’s law, 24
Gauss’s theorem, 245, 252, 265
Gelfand triple, 148, 154, 302
Generalized eigenvalue problem, 143
Geometric Brownian motion, 429
Gradient, 14, 185, 191
Gram–Schmidt orthogonalization, 122
Green’s
function, 315, 316, 408
discrete, 318
identities, 247, 258
Grid
equidistant, 317
function, 318
Gronwall’s lemma, 46, 390
H
H-elliptic, 142
H2-regular, 345
Hahn–Banach theorem, 229
Hamiltonian, 25
Hanging node, 342
Harmonic, 69
Harmonic function, 18, 178
Hat function, 351
Heat
capacity
specific, 8
conductivity, 22
equation, 8, 15, 104, 107, 269, 280, 360,
365
fundamental solution, 83, 105
flow, 22
Heaviside function, 112, 417
Hedging, 432
Hilbert–Schmidt norm, 338
Hessian, 20, 339
Hilbert space, 123, 138, 191
separable, 138
Hölder’s inequality, 161, 164, 210, 289
Hooke’s law, 16
Horizontal method of lines, 366
Hyperbolic, 29, 42
I
Identity, 141
Parseval, 101, 121, 208, 275, 277, 278, 283
Imbedding, 348
compact, 143, 255
continuous, 141
Implosion, 38
Incompressible, 23
Inequality
Bernstein, 349
Cauchy–Schwarz, 118, 119, 128, 130, 132,
146, 160, 175, 185, 200, 205, 368, 390,
391
generalized Cauchy–Schwarz, 132
Hölder, 161, 164, 210, 289
Inverse Cauchy–Schwarz, 350
Poincaré, 163, 170, 174, 199, 205, 257, 275,
292
Young, 173, 232, 240, 262
Inf-sup condition, 133
Inf-sup constant, 133
Initial condition, 12
Initial value, 283
Initial value problem, 31
Initial velocity, 283
Inner product, 116
Inner product space, 116, 134, 152
Inradius, 336
Integral kernel, 69
Integration by parts, 161, 165, 167, 169, 170,
189, 209, 246
Interior node, 342
Interior regularity, 206
Inverse estimate, 348, 349
Invertible, 140
Isometric, 195
Isoparametric elements, 359
Itô differential equation, 430
Itô integral, 428, 429
Itô’s lemma, 430
J
Jacobian, 194
K
KdV equation, 19
Kernel, 140
Kinematic viscosity, 23
Kinetic energy, 53
Kirchhoff plate, 21
Kronecker delta, 22Index 449
L
Lagrange basis, 342, 366
Laplace
equation, 18
Laplace correspondence, 106
Laplace transform, 106
inversion formula, 107
Laplace’s equation, 63
Laplacian, 274, 275
radial, 86
Lattice operations, 200, 202
Law
Fourier, 9
Newton’s first, 15
Newton’s second, 2
Newton’s third, 3
Lax–Milgram theorem, 129, 131, 142, 155,
164, 165, 167, 169, 170, 172, 174, 206,
211, 220, 232, 258, 261, 265, 331
Leapfrog method, 381, 390
Lebesgue measure, 125
Lebesgue version of the Fundamental Theorem
of Calculus, 159
Lexicographic order, 328
Linear elasticity, 17
Linear elements, 332, 334, 341, 353
Linear form, 128, 424
Linear transport equation, 7, 32
Lipschitz
boundary, 230, 242, 254
continuous, 230
domain, 242, 255, 256, 265
graph, 242
Local discretization error, 362, 380, 385
Locally integrable, 177
Local maximal regularity, 216
Local truncation error, 322
Localization, 14
Log-normal distribution, 429
L-shaped domain, 326, 327, 353, 405
M
Magnetic field strength, 24
Maple®, 413
Market value, 11
Mass matrix, 367
Mathematica, 413
Matlab, 325, 353
Matrix norm, 338
Maturity, 11, 91
Maximal regularity, 212, 299
Maximum principle, 69, 206, 316
elliptic, 70
parabolic, 74, 77, 79, 87
maximum principle, 69
Maxwell’s displacement current, 24
Maxwell’s equations, 24
Mesh, 270
equidistant, 351
size, 317
uniform, 317
Method
Crank–Nicolson, 365, 375, 396
Euler explicit, 360, 408
Euler implicit, 364, 371
leapfrog, 381, 390
of lines
horizontal, 366
vertical, 366
Runge–Kutta, 365
Millennium problems, 23
Minimal surface, 21
Mollifier, 183
Monge–Ampère equation, 20, 44
Monotonicity, 316
Multigrid method, 330, 351
MuPAD, 413
N
Natural frequency, 294
Navier–Stokes equations, 2, 22, 23
Neumann
boundary conditions, 21, 293
eigenvalues, 282
Laplacian, 281
Newton
first law, 15
second law, 2, 9, 10
third law, 3
Newtonian fluid, 22
Nodal basis, 342
Node, 342
Non-conformal, 359
Non-convex corner, 235
Non-steady state, 23
Nonlinear transport equation, 8
Norm, 116, 423
convergence, 135
discrete, 320
discrete maximum, 320
Euclidean, 17, 242, 336
operator, 336, 424
Normal distribution, 89
Normal mode, 294
Normed space, 423
Numerical methods, 3450 Index
O
Operator, 139
adjoint, 145, 151, 297
associated, 142
non-expansive, 226
norm, 336, 424
Option, 11
call option, 11
European, 91
put option, 11
Orthogonal, 118
complement, 118, 125
projection, 118, 126
Orthonormal, 120
basis, 120, 122
expansion, 121, 272, 279, 284
Outer normal derivative, 21
Outer unit normal vector, 244
P
Parabolic, 29, 42, 360
boundary, 77, 87
maximum principle, 74, 77, 79, 87
parabolic, 42
Parallelogram identity, 125
Parseval’s identity, 101, 121, 208, 275, 277,
278, 283
Partition, 332
Partition of unity, 249, 252
Path
continuous, 428
Payoff, 12, 433
Payoff function, 91
PDEtools, 414
Periodic, 55
Perron solution, 226, 236
Picard-Lindelöf theorem, 32, 367, 388
Piecewise smooth, 107
Plancherel’s theorem, 101
Planck’s constant, 25
Plate equation, 21
Poincaré inequality, 163, 170, 174, 199, 205,
257, 275, 292, 345, 389
Poisson
equation, 3, 18, 204, 211, 216, 219, 233, 236,
239, 274, 281, 313, 326, 333, 344–346,
348, 350, 351, 358
formula, 68, 69
kernel, 69
Polar coordinates, 66
Polynomially bounded, 91
Portfolio, 430
replicating, 431
Positive
cone, 203
definite, 42, 116, 320, 329, 351, 368, 382,
385
linear form, 248
semidefinite, 42
Potential energy, 16, 53
Pressure, 22
Principal part, 39
Principle of inertia, 15
Probability space, 428
Process
stochastic, 11, 428
Wiener, 429
Product rule, 161, 193
Projection
orthogonal, 118, 126
theorem, 126, 130
Punctured disk, 66
Put option, 11
Q
Quasi-uniform, 343, 409
Quasilinear, 26
R
Radial function, 85
Radial Laplacian, 86
Random variable, 428
Range, 130
Reaction terms, 7
Rectangular pulse, 97
Reduction to homogeneous boundary
conditions, 166, 220
Reference element, 335
Regularity
interior, 206
local maximal, 216
maximal, 212
Relatively compact, 426
Resolvent, 145
Restriction, 139
Reynolds number, 23
Riemann integral, 270
Riemann–Lebesgue lemma, 207, 238
Riemann sum, 270
Riesz representation theorem, 128, 248, 253
Riesz representative, 129
Ritz projection, 368, 388, 401
Robin
boundary conditions, 265, 280, 293
Laplacian, 281
Robust, 314
Runge–Kutta method, 365Index 451
S
Scalar equation, 22
Scalar product, 116
Schrödinger equation, 24
Schur norm, 338
Schwarz’s theorem, 39
Segment condition, 228
Self-adjoint, 145, 151, 297
Self-financing, 430
Semi-discretization, 366
Semigroup, 273
Semilinear, 26
Semi-norm, 334
Separable, 122
separation of variables, 66
Series
Fourier, 55, 56
Shift theorem, 166, 212, 216, 316, 327
Shock, 38
Sign function, 200
Sinc function, 98
Singularly perturbed problem, 175
Sink, 5
Sobolev imbedding theorem, 211, 265, 334
Soliton, 19
Solution
classical, 223, 258
discrete, 331
formula, 415
Perron, 226
stationary, 294
strong, 211, 216
weak, 204, 232, 259
Source, 5
Space
dual, 128
Hilbert, 191
inner product, 116, 134, 152
locally integrable functions, 106
pre-Hilbert, 116, 134, 152
trigonometric polynomials, 60
Span, 120
Sparse, 323, 329
Spectral theorem, 143, 147, 270, 281, 283
Sphere condition
exterior, 229
Stability, 314, 316
Stable
unconditionally, 384, 386
Stampacchia’s lemma, 202
Standard normal distribution, 92
State, 6, 22
State variable, 6
Stationary, 23
Stationary solution, 294
Steady state, 23
Step function, 186, 427
Stiffness matrix, 351, 367
Stochastic process, 11, 428
Stone–Weierstrass theorem, 225, 253, 426
Strain energy, 16
Strike, 11, 91
String theory, 20
Strong solution, 211, 216
Subalgebra, 225, 248
Subharmonic, 69, 206
Sublattice, 203
Submultiplicative, 58
Subsolution, 236
Substitution rule, 194
Superharmonic, 69, 226
Superposition principle, 389
Supersolution, 236
Support, 100, 182, 248
compact, 30, 156, 182
Supremizer, 134
Surface measure, 245
Sylvester’s criterion, 44
Symmetric, 116, 129, 142
Symmetric difference, 360, 372
System, 22
T
Tangent space, 243
Tangent vector, 243
Taylor expansion, 17, 44, 245, 317, 326, 362,
380, 385, 394, 395, 399–401, 410
Taylor’s theorem, 245
Telegraph equation, 110
Temperature, 22
Term
convective, 22
diffusive, 22
Terminal condition, 12
Terminal value problem, 32
Test function, 182
Test space, 132
Theorem
Abel’s convergence, 57, 60
approximation, 191, 194
Arzelà–Ascoli, 161, 426
Baire, 138
Cauchy–Lipschitz, 32, 367, 388
closed graph, 234, 276, 345, 425
convolution, 101, 106
Dini’s, 287
divergence, 246452 Index
Theorem (cont.)
dominated convergence, 184–186, 188, 194,
201, 202, 213, 277, 427
Fubini, 101, 159, 161, 184, 186
Fubini–Tonelli, 427
Gauss, 245, 252, 265
Hahn–Banach, 229
Hausdorff–Young, 240
Lax–Milgram, 129, 131, 142, 155, 164, 165,
167, 169, 170, 172, 174, 206, 211, 220,
232, 258, 261, 265, 331
Lax–Richtmyer, 384, 386
Lions, 134, 305
Meyers–Serrin, 192
monotone convergence, 296, 298, 426
Picard-Lindelöf, 32, 367, 388
Plancherel, 101, 208, 211, 212
projection, 126
Pythagoras, 118, 121, 124
Rellich imbedding, 212
Riesz representation, 128, 248, 253
Riesz–Fischer, 128
Riesz–Fréchet, 128, 130, 138, 164, 172, 217,
258, 260, 347
Schwarz, 39
shift, 166, 212, 216, 316, 327
Sobolev imbedding, 211, 334
spectral, 143, 147, 270, 281, 283
Stone–Weierstrass, 74, 225, 253, 426
Taylor, 245
Tietze extension, 225
Tonelli, 159
trace, 262
Theory of elasticity, 17
Theory of relativity, 89
Thermal conductivity, 9
Tietze extension theorem, 225
Total, 120, 122
Total energy, 16, 22
total energy, 53
Trace operator, 262, 263, 280
Trace theorem, 262
Transform
Fourier, 97, 208
Trial space, 132, 332
Triangle inequality, 117, 119
Triangulation, 332
Tridiagonal matrix, 323, 364
U
Ultrahyperbolic, 43
Unconditionally stable, 384, 386
Underlying, 11
Uniform, 348
mesh, 317
Uniform boundedness principle, 134, 410
Uniform convergence, 119
Uniform ellipticity, 231
Unit circle, 223
Unit disk, 223
Unitary, 124
V
Variable coefficients, 31, 231
Variance, 429
Variation
first, 17
Velocity vector, 22
Vertical method of lines, 366
viscosity coefficient, 8
Viscous stress tensor, 22
Volatility, 429
Von Neumann stability theory, 364
W
Wave equation, 11, 103, 269, 283, 379
Wave function, 24
Wavelet method, 330, 351
Weak
convergence, 135
derivative, 155–158, 161, 189, 190, 296
solution, 179, 204, 232, 259, 327
Weakly continuous, 138
Well-posed, 71, 132, 140
Wiener process, 429
Y
Young’s inequality, 173, 232, 240, 262
