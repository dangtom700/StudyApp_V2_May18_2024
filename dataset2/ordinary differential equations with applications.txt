Texts in Applied Mathematics 34
Carmen Chicone
Ordinary 
Differential 
Equations 
with Applications
Third EditionTexts in Applied Mathematics
Volume 34
Editors-in-Chief
Anthony Bloch, University of Michigan, Ann Arbor, MI, USA
Charles L. Epstein, University of Pennsylvania, Philadelphia, PA, USA
Alain Goriely, University of Oxford, Oxford, UK
Leslie Greengard, New York University, New York, NY, USA
Series Editors
J. Bell, Lawrence Berkeley National Laboratory, Berkeley, CA, USA
R. Kohn, New York University, New York, NY, USA
P. Newton, University of Southern California, Los Angeles, CA, USA
C. Peskin, New York University, New York, NY, USA
R. Pego, Carnegie Mellon University, Pittsburgh, PA, USA
L. Ryzhik, Stanford University, Stanford, CA, USA
A. Singer, Princeton University, Princeton, NJ, USA
A. Stevens, University of Munster, M ¨ unster, Germany ¨
A. Stuart, University of Warwick, Coventry, UK
T. Witelski, Duke University, Durham, NC, USA
S. Wright, University of Wisconsin, Madison, WI, USAThe mathematization of all sciences, the fading of traditional scientific bound￾aries, the impact of computer technology, the growing importance of computer
modelling and the necessity of scientific planning all create the need both in
education and research for books that are introductory to and abreast of these
developments. The aim of this series is to provide such textbooks in applied
mathematics for the student scientist. Books should be well illustrated and have
clear exposition and sound pedagogy. Large number of examples and exercises at
varying levels are recommended. TAM publishes textbooks suitable for advanced
undergraduate and beginning graduate courses, and complements the Applied
Mathematical Sciences (AMS) series, which focuses on advanced textbooks and
research-level monographs.Carmen Chicone
Ordinary Differential
Equations with Applications
Third EditionCarmen Chicone
Department of Mathematics
University of Missouri
Columbia, MO, USA
ISSN 0939-2475 ISSN 2196-9949 (electronic)
Texts in Applied Mathematics
ISBN 978-3-031-51651-1 ISBN 978-3-031-51652-8 (eBook)
https://doi.org/10.1007/978-3-031-51652-8
Mathematics Subject Classification: 34A30, 34A34, 34C15, 34C23, 37-01, 49-01
1st & 2nd editions: © Springer Science+Business Media, Inc. 1999, 2006
3rd edition: © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer
Nature Switzerland AG 2024
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher,
whether the whole or part of the material is concerned, specifically the rights of translation, reprinting,
reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical
way, and transmission or information storage and retrieval, electronic adaptation, computer software,
or by similar or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publi￾cation does not imply, even in the absence of a specific statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, expressed or implied, with respect to the material contained
herein or for any errors or omissions that may have been made. The publisher remains neutral with
regard to jurisdictional claims in published maps and institutional affiliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland
Paper in this product is recyclable.To Jenny, for giving me the gift of time.Series Preface
Mathematics is playing an ever more important role in the physical and
biological sciences, provoking a blurring of boundaries between scientific
disciplines and a resurgence of interest in the modern as well as the clas￾sical techniques of applied mathematics. This renewal of interest, both in
research and teaching, has led to the establishment of the series Texts in
Applied Mathematics (TAM).
The development of new courses is a natural consequence of a high level of
excitement on the research frontier as newer techniques, such as numerical
and symbolic computer systems, dynamical systems, and chaos, mix with
and reinforce the traditional methods of applied mathematics. Thus, the
purpose of this textbook series is to meet the current and future needs of
these advances and to encourage the teaching of new courses.
TAM will publish textbooks suitable for use in advanced undergraduate
and beginning graduate courses, and will complement the Applied Math￾ematical Sciences (AMS) series, which will focus on advanced textbooks
and research-level monographs.
Pasadena, California
New York, New York
College Park, Maryland
J. E. Marsden
L. Sirovich
S. S. Antman
viiPreface
This book is based on a two-semester course in ordinary differential equa￾tions that I have taught to graduate students for two decades at the
University of Missouri. The scope of the narrative evolved over time from
an embryonic collection of supplementary notes, through many classroom
tested revisions, to a treatment of the subject that is suitable for a year (or
more) of graduate study.
If it is true that students of differential equations give away their point
of view by the way they denote the derivative with respect to the inde￾pendent variable, then the initiated reader can turn to Chapter 1, note
that I write ˙x, not x
, and thus correctly deduce that this book is written
with an eye toward dynamical systems. Indeed, this book contains a thor￾ough introduction to the basic properties of differential equations that are
needed to approach the modern theory of (nonlinear) dynamical systems.
But this is not the whole story. The book is also a product of my desire to
demonstrate to my students that differential equations is the least insular
of mathematical subjects, that it is strongly connected to almost all areas
of mathematics, and it is an essential element of applied mathematics.
When I teach this course, I use the first part of the first semester to
provide a rapid, student-friendly survey of the standard topics encoun￾tered in an introductory course of ordinary differential equations (ODE):
existence theory, flows, invariant manifolds, linearization, omega limit sets,
phase plane analysis, and stability. These topics, covered in Sections 1.1–
1.8 of Chapter 1 of this book, are introduced, together with some of their
important and interesting applications, so that the power and beauty of
the subject is immediately apparent. This is followed by a discussion of
ixx Preface
linear systems theory and the proofs of the basic theorems on linearized
stability in Chapter 2. Then, I conclude the first semester by presenting one
or two realistic applications from Chapter 6. These applications provide a
capstone for the course as well as an excellent opportunity to teach the
mathematics graduate students some physics, while giving the engineering
and physics students some exposure to applications from a mathematical
perspective.
In the second semester, I introduce some advanced concepts related to
existence theory, invariant manifolds, continuation of periodic orbits, forced
oscillators, separatrix splitting, averaging, and bifurcation theory. Since
there is not enough time in one semester to cover all of this material in
depth, I usually choose just one or two of these topics for presentation in
class. The material in the remaining chapters is assigned for private study
according to the interests of my students.
My course is designed to be accessible to students who have only
studied differential equations during one undergraduate semester. While I
do assume some knowledge of linear algebra, advanced calculus, and anal￾ysis, only the most basic material from these subjects is required: eigen￾values and eigenvectors, compact sets, uniform convergence, the derivative
of a function of several variables, and the definition of metric and Banach
spaces. With regard to the last prerequisite, I find that some students are
afraid to take the course because they are not comfortable with Banach
space theory. These students are put at ease by mentioning that no deep
properties of infinite-dimensional spaces are used, only the basic definitions.
Exercises are an integral part of this book. As such, many of them are
placed strategically within the text, rather than at the end of a section.
These interruptions of the flow of the narrative are meant to provide an
opportunity for the reader to absorb the preceding material and as a guide
to further study. Some of the exercises are routine, while others are sections
of the text written in “exercise form.” For example, there are extended exer￾cises on structural stability, Hamiltonian and gradient systems on mani￾folds, singular perturbations, and Lie groups. My students are strongly
encouraged to work through the exercises. How is it possible to gain an
understanding of a mathematical subject without doing some mathematics?
Perhaps a mathematics book is like a musical score: by sight reading you
can pick out the notes, but practice is required to hear the melody.
The placement of exercises is just one indication that this book is
not written in axiomatic style. Many results are used before their proofs
are provided, some ideas are discussed without formal proofs, and some
advanced topics are introduced without being fully developed. The pure
axiomatic approach forbids the use of such devices in favor of logical order.
The other extreme would be a treatment that is intended to convey the
ideas of the subject with no attempt to provide detailed proofs of basic
results. While the narrative of an axiomatic approach can be as dry as
dust, the excitement of an idea-oriented approach must be weighed againstPreface xi
the fact that it might leave most beginning students unable to grasp the
subtlety of the arguments required to justify the mathematics. I have tried
to steer a middle course in which careful formulations and complete proofs
are given for the basic theorems, while the ideas of the subject are discussed
in depth and the path from the pure mathematics to the physical universe
is clearly marked. I am reminded of an esteemed colleague who mentioned
that a certain textbook “has lots of fruit, but no juice.” Above all, I have
tried to avoid this criticism.
Application of the implicit function theorem is a recurring theme in
the book. For example, the implicit function theorem is used to prove
the rectification theorem and the fundamental existence and uniqueness
theorems for solutions of differential equations in Banach spaces. Also, the
basic results of perturbation and bifurcation theory, including the continu￾ation of subharmonics, the existence of periodic solutions via the averaging
method, as well as the saddle node and Hopf bifurcations, are presented
as applications of the implicit function theorem. Because of its central
role, the implicit function theorem and the terrain surrounding this impor￾tant result are discussed in detail. In particular, I present a review of
calculus in a Banach space setting and use this theory to prove the contrac￾tion mapping theorem, the uniform contraction mapping theorem, and the
implicit function theorem.
This book contains some material that is not encountered in most treat￾ments of the subject. In particular, there are several sections with the
title “Origins of ODE,” where I give my answer to the question “What is
this good for?” by providing an explanation for the appearance of differ￾ential equations in mathematics and the physical sciences. For example, I
show how ordinary differential equations arise in classical physics from the
fundamental laws of motion and force. This discussion includes a deriva￾tion of the Euler-Lagrange equation, some exercises in electrodynamics,
and an extended treatment of the perturbed Kepler problem. Also, I have
included some discussion of the origins of ordinary differential equations in
the theory of partial differential equations. For instance, I explain the idea
that a parabolic partial differential equation can be viewed as an ordinary
differential equation in an infinite-dimensional space. In addition, traveling
wave solutions and the Gal¨erkin approximation technique are discussed.
In a later “origins” section, the basic models for fluid dynamics are intro￾duced. I show how ordinary differential equations arise in boundary layer
theory. Also, the ABC flows are defined as an idealized fluid model, and I
demonstrate that this model has chaotic regimes. There is also a section on
coupled oscillators, a section on the Fermi-Ulam-Pasta experiments, and
one on the stability of the inverted pendulum where a proof of linearized
stability under rapid oscillation is obtained using Floquet’s method and
some ideas from bifurcation theory. Finally, in conjunction with a treatmentxii Preface
of the multiple Hopf bifurcation for planar systems, I present a short intro￾duction to an algorithm for the computation of the Lyapunov quantities as
an illustration of computer algebra methods in bifurcation theory.
Another special feature of the book is an introduction to the fiber
contraction principle as a powerful tool for proving the smoothness of func￾tions that are obtained as fixed points of contractions. This basic method is
used first in a proof of the smoothness of the flow of a differential equation
where its application is transparent. Later, the fiber contraction principle
appears in the nontrivial proof of the smoothness of invariant manifolds
at a rest point. In this regard, the proof for the existence and smoothness
of stable and center manifolds at a rest point is obtained as a corollary of
a more general existence theorem for invariant manifolds in the presence
of a “spectral gap.” These proofs can be extended to infinite dimensions.
In particular, the applications of the fiber contraction principle and the
Lyapunov-Perron method in this book provide an introduction to some of
the basic tools of invariant manifold theory.
The theory of averaging is treated from a fresh perspective that is
intended to introduce the modern approach to this classical subject. A
complete proof of the averaging theorem is presented, but the main theme
of the chapter is partial averaging at a resonance. In particular, the
“pendulum with torque” is shown to be a universal model for the motion
of a nonlinear oscillator near a resonance. This approach to the subject
leads naturally to the phenomenon of “capture into resonance,” and it
also provides the necessary background for students who wish to read
the literature on multifrequency averaging, Hamiltonian chaos, and Arnold
diffusion.
I prove the basic results of one-parameter bifurcation theory—the saddle
node and Hopf bifurcations—using the Lyapunov-Schmidt reduction. The
fact that degeneracies in a family of differential equations might be unavoid￾able is explained together with a brief introduction to transversality theory
and jet spaces. Also, the multiple Hopf bifurcation for planar vector fields
is discussed. In particular, the Lyapunov quantities for polynomial vector
fields at a weak focus are defined and this subject matter is used to provide a
link to some of the algebraic techniques that appear in normal form theory.
Since almost all of the topics in this book are covered elsewhere, there is
no claim of originality on my part. I have merely organized the material in
a manner that I believe to be most beneficial to my students. By reading
this book, I hope that you will appreciate and be well prepared to use the
wonderful subject of differential equations.
Columbia, Missouri
June 1999
Carmen ChiconePreface to the Second Edition
This edition contains new material, new exercises, rewritten sections, and
corrections.
There are at least three nontrivial mathematical errors in the first edition:
The proof of the Trotter product formula (Theorem 2.24) is valid only in
case eA+B = eAeB; the Floquet theorem (Theorem 2.47) on the existence
of logarithms for matrices is valid only if the square of the real matrix
in question has all positive eigenvalues; and the proof of the smoothness
of invariant manifolds (Theorem 4.1) has a gap because the continuity of
a certain fiber contraction with respect to its base space is assumed. The
first two errors were pointed out by Mark Ashbaugh, the third by Mohamed
ElBialy. These and many other less serious errors are corrected.
While much of the narrative has been revised, the most substantial addi￾tions and revisions not already mentioned are the following: the introduc￾tory Section 1.9.3 on contraction is rewritten to include a discussion of the
continuity of fiber contractions and a more informative first application
of the fiber contraction theorem, which is the proof of the smoothness of
the solution of the functional equation F ◦ φ − φ = G (Theorem 1.234);
Section 3.1 on the Euler-Lagrange equation is rewritten and expanded to
include a more detailed discussion of Hamilton’s theory, a presentation
of Noether’s Theorem, and several new exercises on the calculus of varia￾tions; Section 3.2 on classical mechanics has been revised by including more
details; the application (in Section 3.5) of Floquet theory to the stability of
the inverted pendulum is rewritten to incorporate a more elegant dimen￾sionless model; a new Section 4.3.3 introduces the Lie derivative and applies
it to prove the Hartman-Grobman theorem for flows; multidimensional
xiiixiv Preface to the Second Edition
continuation theory for periodic orbits in the presence of first integrals
is discussed in the new Section 5.3.8, the basic result on the continuation
of manifolds of periodic orbits in the presence of first integrals in involution
is proved, and the Lie derivative is used again to characterize commuting
flows; and the subject of dynamic bifurcation theory is introduced in a new
Section 8.4 where the fundamental idea of delayed bifurcation is presented
with applications to the pitchfork bifurcation and bursting.
Over 160 new exercises are included, most with multiple parts. While a
few routine exercises are provided where I expect them to be helpful, most
of the exercises are meant to challenge students on their understanding of
the theory, stimulate interest, extend topics introduced in the narrative, and
point the way to applications. Also, most exercises now have lettered parts
for easy identification of portions of exercises for homework assignments.
As described in the Preface, the core first graduate course in ODE is
contained in selections from the first three chapters. The instructor should
budget class time so that all of the language and basic concepts of the
subject (existence theory, flows, invariant manifolds, linearization, omega
limit sets, phase plane analysis, and stability) are introduced and some
applications are discussed in detail.
In my experience, sensitivity to the preparation of students is essential
for a successful first graduate course in differential equations. Although
the prerequisites are minimal, there are certainly some students who are
unprepared for the challenges of a course based on this book if their expo￾sure to differential equations is limited to no more than one undergraduate
course where they studied only solution methods for linear second-order
equations. I have included some reviews (see Exercise 1.6) to serve as a
bridge from their first course to this book. In addition, I often use some
class time to review a few fundamental concepts (especially, the derivative
as a linear transformation, compactness, connectedness, uniform conver￾gence, linear spaces, eigenvalues, and Jordan canonical form) before they
are encountered in context.
The second edition contains plenty of material for second semester
courses, master’s projects, and reading courses. Professionals might also
find something of value.
I remain an enthusiastic teacher of the rich and important subject of
differential equations. I hope that instructors will find this book a useful
addition to their class design and preparation, and students will have a
clear and faithful guide during their quest to learn the subject.
Columbia, Missouri
August 2005
Carmen ChiconePreface to the Third Edition
The third edition maintains the style and spirit of the book’s origins:
dominance of dynamical systems theory for ordinary differential equations,
explanations written for students being introduced to the subject for the
first time, and a trove of exercises designed to test comprehension of new
concepts and challenge students in directions that lead to exploration and
research.
The text has been reorganized; necessary corrections—most notably
filling a gap in the proof of smoothness of stable manifolds—have been
made; existing sections have been revised; many new exercises have been
included; and new sections on control theory, optimal control, parameter
estimation, and global continuation of bifurcations have been added.
Much more material is included than would be presented in a typical first
course, which in my view must introduce several core topics all discussed in
the book: local existence, uniqueness, continuity with respect to parameters
and initial data, phase portraits, rest points, periodic orbits, invariant mani￾folds, stability, omega limit sets, limit cycles, Poincar´e maps, the Poincar´e￾Bendixson theorem, Lyapunov functions, linear systems, matrix expo￾nentials, Gronwall’s inequality, variation of parameters, Floquet theory,
linearized stability, the Grobman-Hartman theorem, persistence of rest
points and periodic orbits, the stable and center manifold theorems, and
local one-parameter bifurcation.
Time management in designing a syllabus will likely not allow in-depth
treatment of all core topics, existence theory for example. By design, many
auxiliary mathematical ideas are included in context and with exercises that
are worthy of study, for example, the implicit function theorem, contraction
xvxvi Preface to the Third Edition
mapping theorem, compactness, hyperbolicity, change of variables, mani￾folds, eigenvalues, canonical forms, derivatives of transformations, and so
on. Exposure to these ideas is perhaps more important for most beginning
students than their struggle with technical details in general theorems, a
problem that will abate with maturity in the subject. Desire to prove every￾thing should be balanced against a sensible survey of core topics with the
discussion of ideas used to construct proofs.
Additional subject matter in the book, some explicated in detail, is meant
to attract student interest for self study and provide material (in the same
writing style) for topics courses, reading courses, or master’s projects.
Of particular interest is new material related to optimization and its
interplay with ordinary differential equations via the Euler-Lagrange equa￾tion, the Legendre necessary condition, and gradient descent. Also, its
connection with Hamiltonian mechanics is expanded in several directions.
For instance, an enhanced version of Noether’s theorem is formulated and
proved. A concise introduction to control theory and optimal control is
presented up to and including a proof of the maximum principle for an
important and useful special case. Because the rich, beautiful, and timely
subject of optimization—in the advent of machine learning and artificial
intelligence—is in close contact with the theory of ordinary differential
equations, students should be made aware early in their careers.
A new section on global continuation through bifurcations, including
formulation and proof of the Crandall-Rabinowitz theorem, presents and
explains basic ideas sufficient to write a computer program that could be
used to approximate bifurcation curves.
Many new original exercises are included. They are designed to augment
and enhance this indispensable part of the book with the intent of testing
understanding and introducing relevant new ideas. Some exercises are
routine, some propose significant challenges, and some suggest new direc￾tions for further study or research. They are an integral part of the
book.
I hope very much that students find the new edition readable, enjoyable,
and instructive. Nothing would be more satisfying than to inspire readers to
master the subject, advance understanding of differential equations through
their research, or apply the theory to solve problems arising in engineering
or science. Discoveries await; applications abound.
Columbia, Missouri
October 2023
Carmen ChiconeAcknowledgments
I thank all the people who have offered valuable suggestions for corrections
and additions to this book, especially, Mark Ashbaugh, James Benson,
Oksana Bihun, Tanya Christiansen, Timothy Davis, Jack Dockery, Jaap
Elderling, Michael Heitzman, Peter Hinow, Mohamed S. ElBialy, Sergei
Kosakovsky, Marko Koselj, Krishna Pokharel, M. B. H. Rhouma, Stephen
Schecter, Robert Sacker, Douglas Shafer, Richard Swanson, Joan Torre￾grosa, Samuel Walsh, and Chongchun Zeng. Also, I thank Dixie Fingerson
for much valuable assistance in the mathematics library.
Invitation
Please send your corrections or comments.
E-mail: chiconeC@missouri.edu
xviiContents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix
1 Introduction to Ordinary Differential Equations .... 1
1.1 Existence and Uniqueness ................. 1
1.2 Types of Differential Equations . ............ 6
1.3 Geometric Interpretation of Autonomous Systems . . . 8
1.4 Flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1.5 Stability and Linearization . . . . . . . . . . . . . . . . 18
1.6 Stability and the Direct Method of Lyapunov . . . . . 25
1.7 Manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . 31
1.7.1 Introduction to Invariant Manifolds . . . . . . 32
1.7.2 Smooth Manifolds . . . . . . . . . . . . . . . . 41
1.7.3 Tangent Spaces . . . . . . . . . . . . . . . . . 50
1.7.4 Change of Coordinates . . . . . . . . . . . . . 58
1.7.5 Reparametrization of Time . . . . . . . . . . . 63
1.7.6 Polar Coordinates . . . . . . . . . . . . . . . . 66
1.8 Periodic Solutions . . . . . . . . . . . . . . . . . . . . . 82
1.8.1 The Poincar´e Map . . . . . . . . . . . . . . . . 83
1.8.2 Limit Sets and Poincar´e–Bendixson
Theory . . . . . . . . . . . . . . . . . . . . . . 92
1.9 Regular and Singular Perturbation . . . . . . . . . . . 109
1.10 Review of Calculus . . . . . . . . . . . . . . . . . . . . 113
1.10.1 The Mean Value Theorem . . . . . . . . . . . 118
1.10.2 Integration in Banach Spaces . . . . . . . . . . 120
xixxx Contents
1.11 Contraction . . . . . . . . . . . . . . . . . . . . . . . . . 126
1.11.1 The Contraction Mapping Theorem . . . . . . 127
1.11.2 Uniform Contraction . . . . . . . . . . . . . . 128
1.11.3 Fiber Contraction . . . . . . . . . . . . . . . . 132
1.11.4 The Implicit Function Theorem . . . . . . . . 139
1.12 Existence, Uniqueness, and Extension . . . . . . . . . . 140
2 Homogeneous Linear Systems . . . . . . . . . . . . . . . . 151
2.1 Gronwall’s Inequality . . . . . . . . . . . . . . . . . . . 152
2.2 Existence Theory . . . . . . . . . . . . . . . . . . . . . 153
2.3 Principle of Superposition . . . . . . . . . . . . . . . . 155
2.4 Linear Equations with Constant Coefficients . . . . . . 160
2.5 The Matrix Exponential . . . . . . . . . . . . . . . . . 166
2.6 Lie–Trotter and Baker–Campbell–Hausdorff
formulas . . . . . . . . . . . . . . . . . . . . . . . . . . 177
3 Stability of Linear Systems . . . . . . . . . . . . . . . . . 183
4 Stability of Nonlinear Systems . . . . . . . . . . . . . . . 189
4.1 Variation of Parameters and Solution
of Inhomogeneous Linear Systems . . . . . . . . . . . . 189
4.2 Alekseev-Gr¨obner formula . . . . . . . . . . . . . . . . 193
4.3 Stability of Nonlinear Systems . . . . . . . . . . . . . . 196
4.4 An Instability Criterion . . . . . . . . . . . . . . . . . . 202
5 Floquet Theory . . . . . . . . . . . . . . . . . . . . . . . . . 207
5.1 Lyapunov Exponents . . . . . . . . . . . . . . . . . . . 222
5.2 Hill’s Equation . . . . . . . . . . . . . . . . . . . . . . . 225
5.3 Periodic Orbits of Linear Systems . . . . . . . . . . . . 229
5.4 Stability of Periodic Orbits . . . . . . . . . . . . . . . . 231
6 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
6.1 Origins of ODE: Calculus of Variations . . . . . . . . . 245
6.2 Origins of ODE: Classical Physics . . . . . . . . . . . . 260
6.2.1 Motion of a Charged Particle . . . . . . . . . . 263
6.2.2 Motion of a Binary System . . . . . . . . . . . 264
6.2.3 Perturbed Kepler Motion and Delaunay
Elements . . . . . . . . . . . . . . . . . . . . . 273
6.2.4 Satellite Orbiting an Oblate Planet . . . . . . 280
6.2.5 The Diamagnetic Kepler Problem . . . . . . . 286
6.3 Coupled Pendula: Normal Modes and Beats . . . . . . 292
6.4 The Fermi-Ulam-Pasta Oscillator . . . . . . . . . . . . 296
6.5 The Inverted Pendulum . . . . . . . . . . . . . . . . . . 301
6.6 Origins of ODE: Partial Differential Equations . . . . . 307
6.6.1 Infinite-Dimensional ODE . . . . . . . . . . . 309
6.6.2 Gal¨erkin Approximation . . . . . . . . . . . . 322Contents xxi
6.6.3 Traveling Waves . . . . . . . . . . . . . . . . . 334
6.6.4 First Order PDE . . . . . . . . . . . . . . . . . 339
6.7 Control . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
6.7.1 Controllability of Time-Invariant Linear
Systems . . . . . . . . . . . . . . . . . . . . . . 347
6.7.2 Optimal Control . . . . . . . . . . . . . . . . . 363
6.7.3 Quadratic Regulator . . . . . . . . . . . . . . . 371
6.7.4 Optimal Control Example . . . . . . . . . . . 375
6.7.5 Parameter Estimation and the Adjoint
Method . . . . . . . . . . . . . . . . . . . . . . 385
7 Hyperbolic Theory . . . . . . . . . . . . . . . . . . . . . . . 395
7.1 Invariant Manifolds . . . . . . . . . . . . . . . . . . . . 395
7.2 Applications of Invariant Manifolds . . . . . . . . . . . 419
7.3 The Hartman–Grobman Theorem . . . . . . . . . . . . 422
7.3.1 Diffeomorphisms . . . . . . . . . . . . . . . . . 423
7.3.2 Differential Equations . . . . . . . . . . . . . . 429
7.3.3 Linearization via the Lie Derivative . . . . . . 434
8 Continuation of Periodic Solutions . . . . . . . . . . . . 445
8.1 A Classic Example: van der Pol’s Oscillator . . . . . . 446
8.1.1 Continuation Theory and Applied
Mathematics . . . . . . . . . . . . . . . . . . . 452
8.2 Autonomous Perturbations . . . . . . . . . . . . . . . . 454
8.2.1 Poincar´e’s Method of Continuation . . . . . . 455
8.2.2 Continuation of Periodic Orbits of Planar
Systems . . . . . . . . . . . . . . . . . . . . . . 457
8.2.3 Diliberto’s Theorem . . . . . . . . . . . . . . . 458
8.2.4 Preparation Theorem and Persistence
of Nonhyperbolic Periodic Orbits . . . . . . . 463
8.2.5 Continuation from an Annulus of Period
Orbits . . . . . . . . . . . . . . . . . . . . . . . 467
8.2.6 Periodic Orbits of Multidimensional
Systems with First Integrals . . . . . . . . . . 470
8.3 Nonautonomous Perturbations . . . . . . . . . . . . . . 477
8.3.1 Rest Points . . . . . . . . . . . . . . . . . . . . 480
8.3.2 Isochronous Period Annulus . . . . . . . . . . 481
8.3.3 The Forced van der Pol Oscillator . . . . . . . 485
8.3.4 Regular Period Annulus
and Lyapunov–Schmidt Reduction . . . . . . . 493
8.3.5 Limit Cycles–Entrainment–Resonance
Zones . . . . . . . . . . . . . . . . . . . . . . . 504
8.3.6 Lindstedt Series and the Perihelion
of Mercury . . . . . . . . . . . . . . . . . . . . 511xxii Contents
8.3.7 Entrainment Domains for van der Pol’s
Oscillator . . . . . . . . . . . . . . . . . . . . . 520
8.4 Forced Oscillators . . . . . . . . . . . . . . . . . . . . . 522
9 Homoclinic Orbits, Melnikov’s Method,
and Chaos . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529
9.1 Autonomous Perturbations: Separatrix Splitting . . . . 535
9.2 Periodic Perturbations: Transverse Homoclinic
Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545
9.3 Origins of ODE: Fluid Dynamics . . . . . . . . . . . . . 559
9.3.1 The Equations of Fluid Motion . . . . . . . . 560
9.3.2 ABC Flows . . . . . . . . . . . . . . . . . . . . 570
9.3.3 Chaotic ABC Flows . . . . . . . . . . . . . . . 573
10 Averaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591
10.1 The Averaging Principle . . . . . . . . . . . . . . . . . 591
10.2 Averaging at Resonance . . . . . . . . . . . . . . . . . . 601
10.3 Action-Angle Variables . . . . . . . . . . . . . . . . . . 618
11 Bifurcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623
11.1 One-Dimensional State Space . . . . . . . . . . . . . . 624
11.1.1 The Saddle-Node Bifurcation . . . . . . . . . . 624
11.1.2 A Normal Form . . . . . . . . . . . . . . . . . 626
11.1.3 Bifurcation in Applied Mathematics . . . . . . 627
11.1.4 Families, Transversality, and Jets . . . . . . . 629
11.2 Saddle-Node Bifurcation via Lyapunov-Schmidt
Reduction . . . . . . . . . . . . . . . . . . . . . . . . . 637
11.3 Poincar´e-Andronov-Hopf Bifurcation . . . . . . . . . . 643
11.3.1 Multiple Hopf Bifurcation . . . . . . . . . . . . 655
11.4 Dynamic Bifurcation . . . . . . . . . . . . . . . . . . . 674
11.5 Global Continuation and the Crandall-Rabinowitz
Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 682
11.5.1 Finite-Dimensional Approximation . . . . . . 683
11.5.2 Continuation . . . . . . . . . . . . . . . . . . . 684
11.5.3 Bifurcation . . . . . . . . . . . . . . . . . . . . 687
11.5.4 Summary . . . . . . . . . . . . . . . . . . . . . 689
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 693
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7111
Introduction to Ordinary Differential
Equations
This chapter is about the most basic concepts of the theory of differential
equations. We will answer some fundamental questions: What is a differen￾tial equation? Do differential equations always have solutions? Are solutions
of differential equations unique? But, the most important goal of this chap￾ter is to introduce a geometric interpretation for the space of solutions of a
differential equation. Using this geometry, we will introduce some of the ele￾ments of the subject: rest points, periodic orbits, and invariant manifolds.
Finally, we will review calculus in a Banach space setting and use it to prove
the classic theorems on the existence, uniqueness, and extension of solu￾tions. References for this chapter include [11], [14], [65], [67], [116], [122],
[140], [159], [204], [235], and [253].
1.1 Existence and Uniqueness
Let J ⊆ R, U ⊆ Rn, and Λ ⊆ Rk be open subsets, and suppose that
f : J × U × Λ → Rn is a smooth function. Here the term “smooth” means
that the function f is continuously differentiable. An ordinary differential
equation (ODE) is an equation of the form
x˙ = f(t, x, λ) (1.1)
where the dot denotes differentiation with respect to the independent vari￾able t (usually a measure of time), the dependent variable x is a vector of
state variables, and λ is a vector of parameters. As convenient terminology,
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
C. Chicone, Ordinary Differential Equations with Applications, Texts in
Applied Mathematics 34, https://doi.org/10.1007/978-3-031-51652-8 1
12 1. Introduction to Ordinary Differential Equations
especially when we are concerned with the components of a vector differ￾ential equation, we will say that equation (1.1) is a system of differential
equations. Also, when interested in changes with respect to parameters, the
differential equation is called a family of differential equations.
Example 1.1. The forced van der Pol oscillator
x˙ 1 = x2,
x˙ 2 = b(1 − x2
1)x2 − ω2x1 + a cos Ωt
is a differential equation with J = R, x = (x1, x2) ∈ U = R2,
Λ = {(a, b, ω, Ω) : (a, b) ∈ R2,ω > 0, Ω > 0},
and f : R × R2 × Λ → R2 defined in components by
(t, x1, x2, a, b, ω, Ω) → (x2, b(1 − x2
1)x2 − ω2x1 + a cos Ωt).
For fixed λ ∈ Λ, a solution of the differential equation (1.1) is a function
φ : J0 → U given by t → φ(t), where J0 is an open subset of J, such that
dφ
dt (t) = f(t, φ(t), λ) (1.2)
for all t ∈ J0.
Although, in this context, the words “trajectory,” “phase curve,” and
“integral curve” are also used to refer to solutions of the differential equa￾tion (1.1), it is useful to have a term that refers to the image of the solution
in Rn. Thus, the orbit of the solution φ is {φ(t) ∈ U : t ∈ J0}.
When a differential equation is used to model the evolution of a state
variable for a physical process, a fundamental problem is to determine the
future values of the state variable from its initial value. The mathematical
model is given by a pair of equations
x˙ = f(t, x, λ), x(t0) = x0,
where the second equation is called the initial condition, and the pair of
equations is called an initial value problem. Of course, a solution of an initial
value problem is just a solution of the differential equation that satisfies
the initial condition.
When the differential equation (1.1) is viewed as a family of differential
equations depending on the parameter vector and perhaps also on the initial
condition, corresponding families of solutions—if they exist—are specified
by listing the variables under consideration as additional arguments. For
example, t → φ(t, t0, x0, λ) is written to specify the dependence of the
family of solutions on the initial condition x(t0) = x0 and on the parameter
vector λ.1.1 Existence and Uniqueness 3
The fundamental issues of the general theory of differential equations
are the existence, uniqueness, extension, and continuity with respect to
parameters of solutions of initial value problems. Fortunately, all of these
issues are resolved by the following foundational results of the subject:
Every initial value problem has a unique solution that is smooth with respect
to initial conditions and parameters. Moreover, the solution of an initial
value problem can be extended in time until it either reaches the boundary of
the domain of definition of the differential equation or blows up to infinity.
The next three theorems are the formal statements of the foundational
results of the subject. They are proved in the latter sections of this chapter
after a review of calculus in infinite-dimensional spaces.
Theorem 1.2 (Existence and Uniqueness). If J ⊆ R, U ⊆ Rn, and
Λ ⊆ Rk are open sets, f : J × U × Λ → Rn is a smooth function, and
(t0, x0, λ0) ∈ J × U × Λ, then there exist open subsets J0 ⊆ J, U0 ⊆ U,
Λ0 ⊆ Λ with (t0, x0, λ0) ∈ J0 × U0 × Λ0 and a function φ : J0 × J0 ×
U0 × Λ0 → Rn given by (t, s, ξ, λ) → φ(t, s, ξ, λ) such that for each point
(t1, ξ1, λ1) ∈ J0 × U0 × Λ0, the function t → φ(t, t1, ξ1, λ1) is the unique
solution defined on J0 of the initial value problem given by the differential
equation (1.1) and the initial condition x(t1) = ξ1.
Recall that if k = 1, 2,...,∞, a function defined on an open set is called
Ck if the function together with all of its partial derivatives up to and
including those of order k are continuous on the open set. Similarly, a func￾tion is called real analytic if it has a convergent power series representation
with a positive radius of convergence at each point of the open set.
Theorem 1.3 (Continuous Dependence). If, for the system (1.1), the
hypotheses of Theorem 1.2 are satisfied, then the solution φ : J0 ×J0 ×U0 ×
Λ0 → Rn of the differential equation (1.1) is a smooth function. Moreover,
if f is Ck for some k = 1, 2,...,∞ (respectively, f is real analytic), then
φ is also Ck (respectively, real analytic).
As a convenient notation, |x| denotes the usual Euclidean norm of x ∈
Rn. But, because all norms on Rn are equivalent, the results of this section
are valid for an arbitrary norm on Rn.
Theorem 1.4 (Extension). If, for the system (1.1), the hypotheses of
Theorem 1.2 hold, and if the maximal open interval of existence of the
solution t → φ(t) (with the last three of its arguments suppressed) is given
by (α, β) ⊂ J with −∞ ≤ α<β< ∞, then |φ(t)| approaches ∞ or φ(t)
approaches the boundary of U as t → β.
In case there is some finite T and limt→T |φ(t)| approaches ∞, we say the
solution blows up in finite time.
The existence and uniqueness theorem is so fundamental in science that
it is sometimes called the “principle of determinism.” The idea is that the
future of a physical system (say the solar system) can be predicted when4 1. Introduction to Ordinary Differential Equations
its initial conditions are known. Although the principle of determinism as
a mathematical statement about solutions of differential equations is vali￾dated by the proof of the existence and uniqueness theorem, its interpreta￾tion for physical systems that might be modeled by differential equations
is not as clear as it might seem. The issue is that solutions of differential
equations can be very complicated. For example, the future state of the
system might depend so sensitively on the initial state of the system that
the imprecision of measurements of initial conditions leads to future states
far from those that would be obtained if the initial data were more precise.
For such systems, when the initial state is not known exactly, a desired
final state may be difficult (if not impossible) to predict. This subject is
treated in a later chapter on chaotic dynamics.
Which variables are specified as explicit arguments of the solution func￾tion φ of a differential equation depends on the context, as mentioned
above. A precise way to incorporate the initial condition at t = 0 is to
write t → φ(t, ξ) to denote the solution with φ(0, ξ) = ξ. Similarly, write
t → φ(t, ξ, λ) to include the parameter vector.
Example 1.5. The general solution of the differential equation ˙x = x2 is
given by the function
φ(t, ξ) = ξ
1 − ξt,
where φ(0, ξ) = ξ. The desire to incorporate the initial condition compli￾cates specification of the domain of φ because φ is not defined on the curve
ξt = 1. One way to bypass the domain problem is to say that for each ξ ∈ R,
the solution is defined (as implied by the existence theorem) for t in an open
interval containing t = 0. When the initial condition is not included, the
domain is more easily specified: For ξ > 0, let J := {t ∈ R : t < 1/ξ}. The
solution of the initial value problem ˙x = x2, x(0) = ξ is given by the func￾tion φ : J → R such that φ(t) = ξ/(1 − ξt). The interval J is the domain
because φ(t) → ∞ as t → 1/ξ, which illustrates one of the possibilities
mentioned in the extension theorem: blowup in finite time.
The least confusing way to specify a solution of a differential equation is
as in the last example by using a new symbol for its name. But, this is usu￾ally not done in practice or in this book. Often, the name of the dependent
variable in a differential equation (x in ˙x = x2) is used also as the name of
the solution function. For the example, the solution most likely would be
written x = a/(1−at) where a is the integration constant to be determined
by an initial condition. Often the dependent variable in a differential equa￾tion has already been chosen to be the name of a coordinate, for example,
x as the coordinate on R. In the differential equation this same name is
used for a function of the independent variable. This practice should not
cause confusion once it is understood and accepted.1.1 Existence and Uniqueness 5
Exercise 1.6. [Review Problems] This exercise consists of a few key exam￾ples that can be solved using techniques from elementary courses in differential
equations. (a) Recall methods used to solve first-order linear (scalar) differential
equations; that is, differential equations of the form ˙x + a(t)x = f(t). Solve the
initial value problem
x˙ = − 1
1 + t
x + 2, x(0) = 1.
(b) Recall methods used to solve forced second-order linear differential equations
with constant coefficients; that is, differential equations of the form
mx¨ + λx˙ + ω2
x = A cos Ωt.
Determine the general solution of
x¨ + ˙x + x = 2 cos t.
Also, determine the solution of the corresponding initial value problem for the
initial conditions x(0) = 1 and ˙x(0) = 0. (c) Find the general solution of
dy
dx = −x/y.
(d) Find the general solution of ˙x = x(1 − x). (e) Find the general solution of
(1 + a2
r2 ) sin θ dr
dθ + (r − a2
r ) cos θ = 0.
(f) Find the general solution of the differential equation
dy
dx = y
(y + 2)ey − 2x.
Show that the initial value problem with the initial condition y(2) = 0 has a
unique solution and find this solution. Solve the initial value problem
x˙ = 2x − (2 + y)e
y, y˙ = −y, x(0) = 2, y(0) = ln 2
and show that limt→∞(x(t), y(t)) = (1, 0). (g) Find f so that
φ(x, z, t) = f(z) cos(kx − ωt),
φxx + φzz = 0, φz(x, −h) = 0, and φx(x, 0) = −aω sin(kx − ωt).
Exercise 1.7. (a) For an integer n = 1 and scalar functions a and b, show that
the Bernoulli equation
dy
dx = a(x)y + b(x)yn
is transformed to a linear equation by the change of variables z = y1−n. (b) For
scalar functions a, b, and c, show that if a solution y1 of the Riccati equation
dy
dx = a(x) + b(x)y + c(x)y2
is known, then the general solution is given by y = y1+w, where w is the solution
of a Bernoulli equation. (c) Find the general solution of the differential equation
dy/dx = 1 − x2 + y2.6 1. Introduction to Ordinary Differential Equations
Exercise 1.8. Solve the following differential equation from fluid dynamics
(see [154, p. 328]):
f(x) − (f
(x))2 = −1, f(0) = 0, f
(0) = 0, limx→∞ f
(x)=1.
Hint: Let g := f and solve g = g2 − 1 with g(0) = 0 and limx→∞ g(x) = 1.
Guess that limx→∞ g
(x) = 0. Multiply both sides of the differential equation
by g and integrate once. Solve for the constant of integration using the guess
and solve the resulting first-order differential equation by separation of variables.
Answer:
g(x)=3βe√2 x − 1
βe√2 x + 1
2
− 2, β :=
√3 + √2
√3 − √2 .
Note: It is not clear if the original problem has a unique solution.
Exercise 1.9. Consider the differential equation ˙x = −√x, x ≥ 0. Find the
general solution, and discuss the extension of solutions.
Exercise 1.10. (a) Determine the maximal open interval of existence of the
solution of the initial value problem
x˙ = 1/x, x(0) = 1.
(b) What is the maximal interval of existence? (c) Discuss your answer with
respect to Theorem 1.4.
Exercise 1.11. Construct infinitely many different solutions of the initial value
problem
x˙ = x1/3
, x(0) = 0.
Why does Theorem 1.2 fail to apply in this case?
1.2 Types of Differential Equations
Differential equations may be classified in several different ways. In this
section, we note that the independent variable may be implicit or explicit,
and that higher-order derivatives may appear.
An autonomous differential equation is given by
x˙ = f(x, λ), x ∈ Rn, λ ∈ Rk; (1.3)
that is, the function f does not depend explicitly on the independent vari￾able. If the function f does depend explicitly on t, then the corresponding
differential equation is called nonautonomous.
In physical applications, we often encounter equations containing
second-, third-, or higher-order derivatives with respect to the independent
variable. These are called second-order differential equations, third-order
differential equations, and so on, where the order of the equation refers to
the order of the highest order derivative with respect to the independent
variable that appears explicitly in the equation.1.2 Types of Differential Equations 7
Recall that Newton’s second law—the rate of change of the linear momen￾tum acting on a body is equal to the sum of the forces acting on the body—
involves the second derivative of the position of the body with respect to
time. Thus, in many physical applications, the most common differential
equations used as mathematical models are second-order differential equa￾tions. For example, the natural physical derivation of van der Pol’s equation
leads to a second-order differential equation of the form
u¨ + b(u2 − 1) ˙u + ω2u = a cos Ωt. (1.4)
An essential fact is that every differential equation is equivalent to a
first-order system. To illustrate, consider the conversion of van der Pol’s
equation to a first-order system. For this, simply define a new variable
v := ˙u to obtain
u˙ = v,
v˙ = −ω2u + b(1 − u2)v + a cos Ωt. (1.5)
Clearly, this system is equivalent to the second-order equation in the sense
that every solution of the system determines a solution of the second-order
van der Pol equation, and every solution of the van der Pol equation deter￾mines a solution of this first-order system.
A differential equation of order m > 1 can be converted to an equivalent
first-order system by defining m − 1 new variables in the obvious manner.
For example, d3x
dt3 = f(x, t) is equivalent to
x˙ = y, y˙ = z, z˙ = f(x, t).
There are many possibilities for the construction of equivalent first-order
systems from higher-order differential equations. Using equation (1.4) as
an example, there is no requirement to define v := ˙u. For instance, v := au˙
(where a is a nonzero constant) also produces a family of equivalent first￾order systems.
A nonautonomous differential equation of the form ˙x = f(t, x), where
possible dependence on parameters has been suppressed, is “equivalent”
to the following autonomous system obtained by defining a new temporal
variable τ :
x˙ = f(τ,x),
τ˙ = 1. (1.6)
For example, if t → (φ(t), τ (t)) is a solution of this system with φ(t0) = x0
and τ (t0) = t0, then τ (t) = t and
φ˙(t) = f(t, φ(t)), φ(t0) = x0.
Thus, the function t → φ(t) is a solution of the initial value problem
x˙ = f(t, x), x(t0) = x0.8 1. Introduction to Ordinary Differential Equations
In particular, every solution of the nonautonomous differential equation
can be obtained from a solution of the autonomous system (1.6).
Because all ordinary differential equations (as defined here) correspond
to first-order autonomous systems, they are the main subject matter of this
book.
Exercise 1.12. Find a first-order system that is equivalent to the third-order
differential equation
x + xx − (x
)
2 +1=0
where  is a parameter and the  denotes differentiation with respect to the
independent variable.
Exercise 1.13. Find the solution of the initial value problem
x¨ = −x2
, x(0) = −1, x˙ (0) = 2
3 .
Hint: Write the second-order equation as a first-order system using ˙x = y, remove
the time dependence using the chain rule to obtain a first-order equation for y
with independent variable x, solve the first-order equation for y, substitute the
solution into ˙x = y, and solve for x as a function of t.
Exercise 1.14. (a) Is ˙x = x(t − 1) a differential equation? Explain. The right￾hand side is meant to be the function x evaluated at t − 1, not x times t − 1, a
notational confusion. But since x is a function, the interpretation is not necessary.
For the product, write (t−1)x. (b) Is ˙x = −x+ t
1 e−s2
ds a differential equation?
Explain. (c) Is ˙x = −x +  t
1 e−sx(s) dx a differential equation? Explain.
1.3 Geometric Interpretation of Autonomous
Systems
A geometric interpretation of the autonomous differential equation
x˙ = f(x), x ∈ Rn (1.7)
is discussed in this section. It sets the point of view generally taken in
dynamical systems theory and throughout this book.
The function given by x → (x, f(x)) defines a vector field on Rn associ￾ated with the differential equation (1.7). Here the first component of the
function specifies the base point and the second component specifies the
vector at this base point.
A solution t → φ(t) of (1.7) has the property that its tangent vector at
each time t is given by
(φ(t), φ˙(t)) = (φ(t), f(φ(t))).
In other words, if ξ ∈ Rn is on the orbit of this solution, then the tangent
line to the orbit at ξ is generated by the vector (ξ,f(ξ)), as depicted in
Figure 1.1.1.3 Geometric Interpretation of Autonomous Systems 9
Figure 1.1: Tangent vector field and associated integral curve.
Figure 1.2: Closed trajectory (left) and fictitious trajectory (right) for an
autonomous differential equation.
Two essential facts have been mentioned: (i) There is a one-to-one cor￾respondence between vector fields and autonomous differential equations.
(ii) Every tangent vector to a solution curve is given by a vector in the vec￾tor field. They suggest that the geometry of the associated vector field is
closely related to the geometry of the solutions of the differential equation
when the solutions are viewed as curves in a Euclidean space. This geo￾metric interpretation of the solutions of autonomous differential equations
provides a deep insight into the general nature of the solutions of differen￾tial equations, and at the same time suggests the “geometric method” for
studying differential equations: qualitative features expressed geometrically
are paramount; analytic formulas for solutions are of secondary importance.
Finally, let us note that the vector field associated with a differential equa￾tion is given explicitly. Thus, one of the main goals of the geometric method10 1. Introduction to Ordinary Differential Equations
is to derive qualitative properties of solutions directly from the vector field
without “solving” the differential equation.
As an example, consider the possibility that the solution curve starting
at x0 ∈ Rn at time t = 0 returns to the point x0 at t = τ > 0. Clearly,
the tangent vector of the solution curve at the point φ(0) = x0 is the same
as the tangent vector at φ(τ ). The geometry suggests that the points on
the solution curve defined for t>τ retraces the original orbit. Thus, it is
possible that the orbit of an autonomous differential equation is a closed
curve as depicted in the left panel of Figure 1.2. But an orbit cannot cross
itself as in the right panel of Figure 1.2. If there were such a crossing, then
there would have to be two different tangent vectors of the same vector
field at the crossing point.
The vector field corresponding to a nonautonomous differential equation
changes with time. In particular, if a solution curve “returns” to its starting
point, the direction specified by the vector field at this point generally
depends on the time of arrival. Thus, the curve will generally “leave” the
starting point in a different direction than it did originally. For example,
suppose that t → (g(t), h(t)) is a curve in R2 that has a transverse crossing
as in the right panel of Figure 1.2, and consider the following system of
differential equations:
dx
dt = g
(t), dy
dt = h
(t). (1.8)
We have just defined a differential equation with the given curve as a solu￾tion. Thus, every smooth curve is a solution of a differential equation, but
not every curve is a solution of an autonomous differential equation.
Solution curves of nonautonomous differential equations can cross them￾selves. But, this possibility arises because the explicit time variable is not
treated on an equal footing with the dependent variables. Indeed, if we
consider the corresponding autonomous system formed by adding time as
a new variable, then, in the extended state space (the domain of the state
and time variables), orbits cannot cross themselves. For example, the state
space of the autonomous system of differential equations
x˙ = g
(τ ), y˙ = h
(τ ), τ˙ = 1,
corresponding to the nonautonomous differential equation (1.8), is R3. The
system’s orbits in the extended state space cannot cross—the corresponding
vector field in R3 is autonomous.
If the autonomous differential equation (1.7) has a closed orbit and t →
φ(t) is a solution with its initial value on this orbit, then it is clear that
there is some T > 0 such that φ(T) = φ(0). In fact, as we will show in
the next section, even more is true: The solution is T-periodic; that is,
φ(t + T) = φ(t) for all t ∈ R. For this reason, closed orbits of autonomous
systems are also called periodic orbits.1.3 Geometric Interpretation of Autonomous Systems 11
Figure 1.3: A curve in phase space consisting of four orbits of an
autonomous differential equation.
Another important special type of orbit is called a rest point. To define
this concept, note that if f(x0) = 0 for some x0 ∈ Rn, then the constant
function φ : R → Rn defined by φ(t) ≡ x0 is a solution of the differential
equation (1.7). Geometrically, the corresponding orbit consists of exactly
one point. Thus, if f(x0) = 0, then x0 is a rest point. Such a solution is
also called a steady state, a critical point, an equilibrium point, or a zero
(of the associated vector field).
What are all the possible orbit types for autonomous differential equa￾tions? The answer depends on what we mean by “types.” But we have
already given a partial answer: An orbit can be a point, a simple closed
curve, or the homeomorphic image of an interval. A geometric picture of all
the orbits of an autonomous differential equation is called its phase portrait
or phase diagram. This terminology comes from the notion of phase space
in physics, the space of positions and momenta. For the record, the state
space in physics is the space of positions and velocities. But, in the present
context, the terms state space and phase space are synonymous; they are
used to refer to the domain of the vector field that defines an autonomous
differential equation. The fundamental problem of the geometric theory of
differential equations should be evident: Given an autonomous differential
equation, determine its phase portrait.
Because there are essentially only the three types of orbits mentioned in
the last paragraph, it might seem that phase portraits would not be too
complicated. But, as we will see, even the portrait of a single orbit can
be very complex. Indeed, the homeomorphic image of an interval can be a
very complicated subset in a Euclidean space. As a simple but important
example of a complex geometric feature of a phase portrait, note the curve
that crosses itself in Figure 1.1. Such a curve cannot be an orbit of an
autonomous differential equation. But if the crossing point on the curve is
a rest point of the differential equation, then such a curve can exist in the
phase portrait as a union of the four orbits depicted in Figure 1.3.12 1. Introduction to Ordinary Differential Equations
Figure 1.4: Phase portrait of the harmonic oscillator
Exercise 1.15. Consider the harmonic oscillator (a model for an undamped
spring) given by the second-order differential equation ¨u + ω2u = 0 with the
equivalent first-order system
u˙ = ωv, v˙ = −ωu. (1.9)
The phase portrait, in the phase plane, consists of one rest point at the origin
of R2 with all other solutions being simple closed curves as in Figure 1.4. Solve
the differential equation and verify these facts. Find the explicit time-dependent
solution that passes through the point (u, v) = (1, 1) at time t = 0. Note that
the system
u˙ = v, v˙ = −ω2
u
is also equivalent to the harmonic oscillator. Is its phase portrait different from
the phase portrait of the system (1.9)? Can you make precise the notion that two
phase portraits are the same?
Exercise 1.16. Suppose that F : R → R is a smooth, positive, periodic function
with period p > 0. (a) Prove: If t → x(t) is a solution of the differential equation
x˙ = F(x) and
T :=  p
0
1
F(y) dy,
then x(t + T) − x(t) = p for all t ∈ R. (b) What happens for the case where F is
periodic but not of fixed sign? Hint: Define G to be an antiderivative of 1/F. Show
that the function y → G(y +p)−G(y) is constant and G(x(b))−G(x(a)) = b−a.
Exercise 1.17. [Predator–Prey Model] (a) Find all rest points of the system
x˙ = −x + xy, y˙ = ry(1 − y
k ) − axy.
This is a model for a predator–prey interaction, where x is the quantity of preda￾tors and y is the quantity of prey. The positive parameter r represents the growth
rate of the prey, the positive parameter k represents the carrying capacity of the
environment for the prey, and the positive parameter a is a measure of how often1.3 Geometric Interpretation of Autonomous Systems 13
x
Figure 1.5: Phase portrait of ˙x = μ − x2 for μ = 0.
x
μ
Figure 1.6: Bifurcation diagram ˙x = μ − x2.
a predator is successful in catching its prey. Note that the predators will become
extinct if there are no prey. (b) What happens to the population of prey if there
are no predators?
In case our system depends on parameters, the collection of the phase
portraits corresponding to each choice of the parameter vector is called a
bifurcation diagram.
As a simple but important example, consider the differential equation
x˙ = μ − x2, x ∈ R, that depends on the parameter μ ∈ R. If μ = 0, then
the phase portrait, on the phase line, is depicted in Figure 1.5. If we put
together all the phase portrait “slices” in R × R, where a slice corresponds
to a fixed value of μ, then we produce the bifurcation diagram, Figure 1.6.
Note that if μ < 0, there is no rest point. When μ = 0, a rest point is
born in a “blue sky catastrophe.” As μ increases from μ = 0, there is a14 1. Introduction to Ordinary Differential Equations
“saddle-node” bifurcation; that is, two rest points appear. If μ < 0, this
picture also tells us the fate of each solution as t → ∞. No matter which
initial condition we choose, the solution goes to −∞ in finite positive time.
When μ = 0 there is a steady state. If x0 > 0, then the solution t → φ(t, x0)
with initial condition φ(0, x0) = x0 approaches this steady state; that is,
φ(t, x0) → 0 as t → ∞. Whereas, if x0 < 0, then φ(t, x0) → 0 as t → −∞.
In this case, we say that x0 is a semistable rest point. But if μ > 0 and
x0 > 0, then the solution φ(t, x0) → √μ as t → ∞. Thus, x0 = √μ is a
stable steady state. The point x0 = −√μ is an unstable steady state.
Exercise 1.18. Fix r > 0 and k > 0, and consider the family of differential
equations
x˙ = rx(1 − x/k) − λ
with parameter λ ∈ R. (a) Draw the bifurcation diagram. (b) This differential
equation is a phenomenological model of the number of individuals x in a popula￾tion with per capita growth rate r and carrying capacity k such that individuals
are removed from the population at the rate λ per unit time. For example, x could
represent the number of fish in a population and λ the rate of their removal by
fishing. Interpret your bifurcation diagram using this population model. Deter￾mine a critical value for λ such that if this value is exceeded, then extinction of
the population is certain. (c) What would happen if the rate λ varies with time?
Exercise 1.19. (a) Draw the bifurcation diagram for the family ˙x = λx − x3.
The bifurcation is called the supercritical pitchfork (see Chapter 11). (b) Draw
the bifurcation diagram for the subcritical pitchfork ˙x = λx + x3.
Exercise 1.20. Describe the bifurcation diagram for the family
x˙ = λ − x2
, y˙ = −y.
The name “saddle-node bifurcation” comes from the name of the rest point type
corresponding to the parameter value λ = 0.
1.4 Flows
The set of solutions of the autonomous differential equation (1.7)
x˙ = f(x), x ∈ Rn
have an important property: they form a one-parameter group that defines
a (local) phase flow. More precisely, let us define the function φ : R×Rn →
Rn as follows: For x ∈ Rn, let t → φ(t, x) denote the solution of the
autonomous differential equation (1.7) such that φ(0, x) = x.
We know that solutions of a differential equation may not exist for all
t ∈ R. But, for simplicity, let us assume that every solution does exist for1.4 Flows 15
all time. If this is the case, then each solution is called complete, and group
property for φ is expressed concisely as follows:
φ(t + s, x) = φ(t, φ(s, x)).
In view of this equation, if the solution starting at time zero at the point
x is continued until time s, when it reaches the point φ(s, x), and if a new
solution at this point with initial time zero is continued until time t, then
this new solution will reach the same point that would have been reached if
the original solution, which started at time zero at the point x, is continued
until time t + s.
The prototypical example of a flow is provided by the general solution
of the ordinary differential equation ˙x = ax, x ∈ R, a ∈ R. It is given by
φ(t, x0) = eatx0 and satisfies the group property
φ(t + s, x0) = ea(t+s)
x0 = eat(easx0) = φ(t, easx0) = φ(t, φ(s, x0)).
For the general case, suppose that for each x ∈ Rn t → φ(t, x) is the
solution of the differential equation (1.7) such that φ(0, x) = 0. Fix s ∈ R,
x ∈ Rn, and define
ψ(t) := φ(t + s, x), γ(t) := φ(t, φ(s, x)).
Note that φ(s, x) is a point in Rn. Therefore, γ is a solution of the differ￾ential equation (1.7) with γ(0) = φ(s, x). The function ψ is also a solution
of the differential equation because
dψ
dt = dφ
dt (t + s, x) = f(φ(t + s, x)) = f(ψ(t)).
Finally, note that ψ(0) = φ(s, x) = γ(0). We have proved that both t →
ψ(t) and t → γ(t) are solutions of the same initial value problem. Thus, by
the uniqueness theorem, γ(t) ≡ ψ(t). The idea of this proof—two functions
that satisfy the same initial value problem are identical—is often used in
the theory and the applications of differential equations.
By the theorem on continuous dependence, φ is a smooth function. Thus,
for each fixed t ∈ R, the function x → φ(t, x) is a smooth transformation
of Rn. In particular, at t = 0 the function x → φ(0, x) is the identity
transformation. Also
x = φ(0, x) = φ(t − t, x) = φ(t, φ(−t, x)) = φ(−t, φ(t, x)).
In other words, x → φ(−t, x) is the inverse of the function x → φ(t, x). In
fact, x → φ(t, x) is a diffeomorphism for each fixed t ∈ R.
In general, suppose that J × U is a product open subset of R × Rn.
Definition 1.21. A function φ : J × U → Rn given by (t, x) → φ(t, x) is
called a flow if φ(0, x) ≡ x and φ(t + s, x) = φ(t, φ(s, x)) whenever both
sides of the equation are defined.16 1. Introduction to Ordinary Differential Equations
Of course, if t → φ(t, x) defines the family of solutions of the autonomous
differential equation (1.7) such that φ(0, x) ≡ x, then φ is a flow.
Suppose that x0 ∈ Rn, T > 0, and that φ(T,x0) = x0; that is, the
solution returns to its initial point after time T. Then φ(t + T,x0) =
φ(t, φ(T,x0)) = φ(t, x0). In other words, t → φ(t, x0) is a periodic func￾tion with period T. The smallest number T > 0 with this property is called
the period of the periodic orbit through x0.
In the mathematics literature, the notations t → φt(x) and t → φt
(x)
are often used in place of t → φ(t, x) for the solution of the differential
equation,
x˙ = f(x), x ∈ Rn,
that starts at x at time t = 0. These notations emphasize that a flow
is a one-parameter family of transformations. Indeed, for each t, φt maps
an open subset of Rn (its domain) to Rn. Moreover, φ0 is the identity
transformation and φt ◦ φs = φs+t whenever both sides of this equation
are defined. Such a family of transformations is called a one-parameter
group of transformations. We will use all three notations. The only possible
confusion arises when subscripts are used for partial derivatives. But the
meaning of the notation will always be clear from the context in which it
appears.
Exercise 1.22. For each integer p, construct the flow of the differential equation
x˙ = xp.
Exercise 1.23. Consider the differential equation ˙x = t. Construct the family
of solutions t → φ(t, ξ) such that φ(0, ξ) = ξ for ξ ∈ R. Does φ define a flow?
Explain.
Exercise 1.24. Suppose that φt is a (smooth) flow. (a) Prove that there is an
autonomous differential equation whose flow is φt. (b) Can two different differ￾ential equations have the same flow?
Exercise 1.25. (a) Show that the family of functions φt : R2 → R2 given by
φt(x, y) = cos t − sin t
sin t cos t
 x
y

defines a flow on R2. (b) Find a differential equation whose flow is φt. (c) Repeat
parts (a) and (b) for
φt(x, y) = e
−2t
cos t − sin t
sin t cos t
 x
y

.
Exercise 1.26. Write ¨u + αu = 0, u ∈ R, α ∈ R as a first-order system.
(a) Determine the flow of the system, and verify the flow property directly. (b)
Describe the bifurcation diagram of the system. (c) Show that the system has
periodic orbits if α > 0 and determine their period(s).1.4 Flows 17
Figure 1.7: Phase portrait of an asymptotically stable (spiral) sink.
Exercise 1.27. (a) Determine the flow of the first-order system
x˙ = y2 − x2
, y˙ = −2xy.
Hint: Define z := x + iy. (b) Show that almost every orbit lies on a circle, and
the flow gives rational parameterizations for these orbits.
Exercise 1.28. Let φt be the flow of the differential equation ˙x = f(x) and
suppose that y is in the domain of f. Prove that Dφt(y)f(y) = f(φt(y)).
Exercise 1.29. Fluid moves through a round pipe with radius a. Suppose that
the center of the pipe is on the z-axis, the radial and angular fluid velocities both
vanish, and the axial velocity is given by u3(x, y, z) = x2 + y2 − a2. (a) What is
the (three-dimensional) fluid velocity at the pipe wall. (b) Determine the flow of
the fluid.
Exercise 1.30. [Evolution Families] Consider the nonautonomous differential
equation ˙x = f(t, x) and the first-order system ˙τ = 1, ˙x = f(τ,x). Show that if ψ
is the solution of the nonautonomous equation with initial condition x(t0) = x0,
then t → (t, ψ(t)) is the solution of the system with initial condition τ (t0) = t0,
x(t0) = x0. Prove the converse. Let φ(t, τ, x) denote the flow of the autonomous
system and define T and U by (T(t, τ, x), U(t, τ, x)) = φ(t − τ, τ,x). Show that
T(τ, τ,x) = τ and U(τ, τ,x) = x. By using the definition of the flow as the
solution of the system, show that T(t, τ, x) = t. Finally, use the group property
of the flow to show that U(t, τ, x) = U(t, s, U(s, τ, x)) whenever both sides of
the identity are defined. A family of functions U such that U(τ, τ,x) = x and
U(t, τ, x) = U(t, s, U(s, τ, x)) for t, s, τ ∈ R and x ∈ Rn is called an evolution
family. This exercise shows that the solution t → U(t, τ, x) of the nonautonomous
differential equation ˙x = f(t, x) such that U(τ, τ,x) = x is an evolution family.18 1. Introduction to Ordinary Differential Equations
U
δ
x
x0
Figure 1.8: The open sets required in the definition of Lyapunov stability.
The trajectory starting at x can leave the ball of radius δ but it must stay
in the ball of radius .
1.5 Stability and Linearization
Although rest points and periodic orbits are not typical for most auto￾nomous differential equations, these are often the most important orbits in
applications. In particular, common engineering practice is to run a process
in “steady state.” If the process does not stay near the steady state after
a small disturbance, then the control engineer will have to face a difficult
problem that is addressed in a later chapter. A first step is to introduce
the mathematical definition of stability and the classic methods used to
determine the stability of rest points and periodic orbits.
The concept of Lyapunov stability is meant to capture the intuitive notion
of stability—an orbit is stable if solutions that start nearby stay nearby.
To give the formal definition, let us consider the autonomous differential
equation
x˙ = f(x) (1.10)
defined on an open set U ⊂ Rn and its flow φt.
Definition 1.31. A rest point x0 of the differential equation (1.10) is stable
(in the sense of Lyapunov) if for each  > 0, there is a number δ > 0 such
that |φt(x) − x0| <  for all t ≥ 0 whenever |x − x0| < δ (see Figure 1.8).
There is no reason to restrict the definition of stability to rest points. It
can also refer to arbitrary solutions of the autonomous differential equation.1.5 Stability and Linearization 19
Figure 1.9: Phase portrait of an unstable rest point.
Definition 1.32. Suppose that x0 is in the domain of definition of the
differential equation (1.10). The solution t → φt(x0) of this differential
equation is stable (in the sense of Lyapunov) if for each  > 0, there is a
δ > 0 such that |φt(x) − φt(x0)| <  for all t ≥ 0 whenever |x − x0| < δ.
Figure 1.7 shows a typical phase portrait of an autonomous system in
the plane near a type of stable rest point called a sink. The stable rest
point depicted in Figure 1.4 is called a center. More precisely, a rest point
is a center if it is contained in an open set where every orbit (except the
rest point) is periodic.
A solution that is not stable is called unstable. A typical phase portrait
for an unstable rest point, a source, is depicted in Figure 1.9 (see also the
saddle point in Figure 1.1).
Definition 1.33. A solution t → φt(x0) of the differential equation (1.10)
is asymptotically stable if it is stable and there is a constant a > 0 such
that limt→∞ |φt(x) − φt(x0)| = 0 whenever |x − x0| < a.
We have just defined the notion of stability for solutions in case a definite
initial point is specified. The concept of stability for orbits is slightly more
complicated. For example, we have the following definition of stability for
periodic orbits (see also Section 5.4).
Definition 1.34. A periodic orbit Γ of the differential equation (1.10) is
stable if for each open set V ⊆ Rn that contains Γ, there is an open set
W ⊆ V such that every solution, starting at a point in W at t = 0, stays
in V for all t ≥ 0. The periodic orbit is called asymptotically stable if, in20 1. Introduction to Ordinary Differential Equations
addition, there is a subset X ⊆ W such that every solution starting in X
is asymptotic to Γ as t → ∞.
The definitions just given capture the essence of the stability concept, but
they do not give any indication of how to determine if a given solution or
orbit is stable. We will study two general methods, called the indirect and
the direct methods by Lyapunov, that can be used to determine the stability
of rest points and periodic orbits. In more modern language, the indirect
method is called the method of linearization and the direct method is called
the method of Lyapunov. Before we discuss these methods in detail, let us
note that for the case of the stability of special types of orbits, for example
rest points and periodic orbits, there are two main problems: (i) Locating
the special solutions. (ii) Determining their stability.
For the remainder of this section and the next, the discussion will be
restricted to the analysis for rest points. Our introduction to the meth￾ods for locating and determining the stability of periodic orbits must be
postponed until some additional concepts have been introduced.
Let us note that the problem of the location of rest points for the dif￾ferential equation ˙x = f(x) is exactly the problem of finding the roots of
the equation f(x) = 0. Of course, finding roots may be a formidable task,
especially if the function f depends on parameters and we wish to find
its bifurcation diagram. In fact, in the search for rest points, sophisticated
techniques of algebra, analysis, and numerical analysis are often required.
This is not surprising when we stop to think that solving equations is one
of the fundamental themes in mathematics. For example, it is probably not
too strong to say that the most basic problem in linear algebra, abstract
algebra, and algebraic geometry is the solution of systems of polynomial
equations. The results of all of these subjects are sometimes needed to solve
problems in differential equations.
Let us suppose that we have identified some point x0 ∈ Rn such that
f(x0) = 0. What can we say about the stability of the corresponding rest
point? One of the great ideas in the subject of differential equations—not to
mention other areas of mathematics—is linearization. This idea, in perhaps
its purest form, is used to obtain the premier method for the determina￾tion of the stability of rest points. The linearization method is based on
two facts: (i) Stability analysis for linear systems is “easy.” (ii) Nonlinear
systems can be approximated by linear systems. These facts are just reflec￾tions of the fundamental idea of differential calculus: A nonlinear function
is essentially linear if we consider its behavior in a sufficiently small neigh￾borhood of a point in its domain. Indeed, it often suffices to approximate
the graph of a function by its tangent lines.
To describe the linearization method for rest points, let us consider
(homogeneous) linear systems of differential equations; that is, systems of
the form ˙x = Ax where x ∈ Rn and A is a linear transformation of Rn. If the
matrix A does not depend on t—so that the linear system is autonomous—1.5 Stability and Linearization 21
then there is an effective method that can be used to determine the stability
of its rest point at x = 0. In fact, we will show in Chapter 2 that if all of
the eigenvalues of A have negative real parts, then x = 0 is an asymptot￾ically stable rest point for the linear system. (The eigenvalues of a linear
transformation are defined on page 160.)
If x0 is a rest point for the nonlinear system ˙x = f(x), then there is a
natural way to produce a linear system that approximates the nonlinear
system near x0: Simply replace the function f in the differential equation
with the linear function x → Df(x0)(x − x0) given by the first nonzero
term of the Taylor series of f at x0. The linear differential equation
x˙ = Df(x0)(x − x0) (1.11)
is called the linearized system associated with x˙ = f(x) at x0. By applying
the change of variables w := x − x0, the linearized system has the more
convenient form ˙w = Df(x0)w.
Alternatively, we may consider a family of solutions t → φ(t, ) of ˙x =
f(x) with parameter  such that φ(t, 0) = x0. In other words, the family
of solutions contains the constant solution corresponding to the rest point
at the parameter value  = 0. By Taylor’s theorem (with respect to  at
 = 0),
φ(t, ) = x0 + η(t, 0) + 
2R(t, )
for some function R. Hence, the family of first-order approximations of the
solutions in the family φ is given by
φ(t, ) = x0 + η(t, 0).
To determine the function t → η(t, 0), we use the differential equation and
Taylor’s theorem (with respect to  at  = 0) to obtain
η˙(t, ) = φ˙(t, )
= f(φ(t, ))
= f(x0 + η(t, ))
= Df(x0)η(t, 0) + 
2R(t, )
where 2R(t, ) is the remainder in the Taylor expansion of  → f(x0 +
η(t, )). After dividing by , we obtain the equation
η˙(t, ) = Df(x0)η(t, 0) + R(t, ),
and, by taking the limit as  → 0, it follows that
η˙(t, 0) = Df(x0)η(t, 0).
Of course, the initial condition for this equation is determined by the choice
of the family φ; in fact,
η(0, 0) = ∂φ
∂ (0, )



=0
.22 1. Introduction to Ordinary Differential Equations
Thus, the linearized differential equation at x0 (that is, ˙w = Df(x0)w) is
the same for every family φ, and every vector v in Rn is the initial vector
(w(0) = v) for some such family.
The “principle of linearized stability” states that if the linearization of
a differential equation at a steady state has a corresponding stable steady
state, then the original steady state is stable. In the notation of this section,
this principle states that if w(t) = 0 is a stable steady state for ˙w =
Df(x0)w, then x0 is a stable steady state for ˙x = f(x). The principle of
linearized stability is not a theorem, but it is the motivation for several
important results in the theory of stability of differential equations.
Exercise 1.35. Prove that the rest point at the origin for the differential equa￾tion ˙x = ax, a < 0, x ∈ R is asymptotically stable. Also, determine the stability
of this rest point in case a = 0 and in case a > 0.
Exercise 1.36. Use the principle of linearized stability to determine the stabil￾ity of the rest point at the origin for the system
x˙ = 2x + 2y − 3 sin x, y˙ = −2y + xy.
Let us suppose that x0 is a rest point of the differential equation ˙x =
f(x). By the change of variables u = x − x0, this differential equation is
transformed to the equivalent differential equation ˙u = f(u+x0) where the
rest point corresponding to x0 is at the origin. For g(u) := f(u + x0), we
have ˙u = g(u) and g(0) = 0. Thus, it should be clear that there is no loss
of generality if we assume that a rest point is at the origin. In this case,
the linearized equation is ˙w = Dg(0)w.
If f is smooth at x = 0 and f(0) = 0, then
f(x) = f(0) + Df(0)x + R(x) = Df(0)x + R(x)
where Df(0) : Rn → Rn is the linear transformation given by the derivative
of f at x = 0 and, for the remainder R, there is a constant k > 0 and an
open neighborhood U of the origin such that
|R(x)| ≤ k|x|
2
whenever x ∈ U. Because the stability of a rest point is a local property
(that is, a property that is determined by the values of the restriction of
the function f to an arbitrary open subset of the rest point) and in view of
the estimate for the size of the remainder, it is reasonable to expect that
the stability of the rest point at the origin of the linear system ˙x = Df(0)x
will be the same as the stability of the original rest point. This expectation
is not always realized. But we do have the following fundamental stability
theorem.1.5 Stability and Linearization 23
Theorem 1.37. If x0 is a rest point for the differential equation x˙ = f(x)
and if all eigenvalues of the linear transformation Df(x0) have negative
real parts, then x0 is asymptotically stable.
Proof. See Theorem 4.17. 
It turns out that if x0 is a rest point and Df(x0) has at least one eigen￾value with positive real part, then x0 is not stable. If some eigenvalues
of Df(x0) lie on the imaginary axis, then the stability of the rest point
may be very difficult to determine. Also, we can expect qualitative changes
to occur in the phase portrait of a system near such a rest point as the
parameters of the system are varied. These bifurcations are the subject of
Chapter 11.
Exercise 1.38. Prove: If ˙x = 0, x ∈ R, then x = 0 is Lyapunov stable. Consider
the differential equations ˙x = x3 and ˙x = −x3. Prove that whereas the origin
is not a Lyapunov stable rest point for the differential equation ˙x = x3, it is
Lyapunov stable for the differential equation ˙x = −x3. Note that the linearized
differential equation at x = 0 in both cases is the same; namely, ˙x = 0.
Exercise 1.39. Use Theorem 1.37 to show the asymptotic stability of the rest
point at the origin of the system
x˙ = −2y − 4z + (x + y)x2
, y˙ = x − 3y − z + (x + y)y2
, z˙ = −4z + (x + y)z2
.
If x0 is a rest point for the differential equation (1.10) and if the linear
transformation Df(x0) has all its eigenvalues off the imaginary axis, then
we say that x0 is a hyperbolic rest point. Otherwise x0 is called nonhy￾perbolic. In addition, if x0 is hyperbolic and all eigenvalues have negative
real parts, then the rest point is called a hyperbolic sink. If all eigenvalues
have positive real parts, then the rest point is called a hyperbolic source. A
hyperbolic rest point that is neither a source nor a sink is called a hyper￾bolic saddle. If the rest point is nonhyperbolic with all its eigenvalues on
the punctured imaginary axis (that is, the imaginary axis with the origin
removed), then the rest point is called a linear center. If zero is not an
eigenvalue, then the corresponding rest point is called nondegenerate.
If every eigenvalue of a linear transformation A has nonzero real part,
then A is called infinitesimally hyperbolic. If none of the eigenvalues of
A have modulus one, then A is called hyperbolic. This terminology can be
confusing: For example, if A is infinitesimally hyperbolic, then the rest point
at the origin of the linear system ˙x = Ax is hyperbolic. The reason for the
terminology is made clear by consideration of the scalar linear differential
equation ˙x = ax with flow given by φt(x) = eatx. If a = 0, then the linear
transformation x → ax is infinitesimally hyperbolic and the rest point at
the origin is hyperbolic. In addition, if a = 0 and t = 0, then the linear
transformation x → etax is hyperbolic. Moreover, the linear transformation24 1. Introduction to Ordinary Differential Equations
x → ax is obtained by differentiation with respect to t at t = 0 of the
family of linear transformations x → etax. Thus, in effect, differentiation—
an infinitesimal operation on the family of hyperbolic transformations—
produces an infinitesimally hyperbolic transformation.
The relationship between the dynamics of a nonlinear system and its
linearization at a rest point is deeper than the relationship between the
stability types of the corresponding rest points. The next theorem, called
the Hartman–Grobman theorem, is an important result that describes this
relationship in case the rest point is hyperbolic.
Theorem 1.40. If x0 is a hyperbolic rest point for the autonomous dif￾ferential equation (1.10), then there is an open set U containing x0 and
a homeomorphism H with domain U such that the orbits of the differen￾tial equation (1.10) are mapped by H to orbits of the linearized system
x˙ = Df(x0)(x − x0) in the set U.
Proof. See Section 7.3. 
In other words, the linearized system has the same phase portrait as the
original system in a sufficiently small neighborhood of the hyperbolic rest
point. Moreover, the homeomorphism H in the theorem can be chosen to
preserve not just the orbits as point sets, but their time parameterizations
as well.
Exercise 1.41. In the definition of asymptotic stability for rest points, the first
requirement is that the rest point be stable; the second requirement is that all
solutions starting in some open set containing the rest point be asymptotic to
the rest point. Does the first requirement follow from the second? Answer this
question for flows on the line and the circle. Hint: Consider flows on the circle
first. They are obtained by solving differential equations of the form ˙
θ = f(θ)
where f : R → R is 2π-periodic. Find a differential equation on the circle with
exactly one rest point that is semistable. See Figure 1.22 and Exercise 1.152 for
an explicit example of a planar system with a rest point such that every orbit is
attracted to the rest point, but the rest point is not asymptotically stable.
Exercise 1.42. Consider the mathematical pendulum given by the second￾order differential equation ¨u + sin u = 0. (a) Find the corresponding first-order
system. (b) Find all rest points of the first-order system, and characterize these
rest points according to their stability type. (c) Draw the phase portrait of the
system in a neighborhood at each rest point. (d) Solve the same problems for the
second-order differential equation given by
x¨ + (x2 − 1) ˙x + ω2
x − λx3 = 0.
Exercise 1.43. (a) Linearize at each rest point of the predator–prey model in
Exercise 1.17. What condition on the parameters (if any) implies the existence
of an asymptotically stable rest point? (b) Interpret the result of part (a) as a
statement about the fate of the predators and prey.1.6 Stability and the Direct Method of Lyapunov 25
Exercise 1.44. Show that the origin is an asymptotically stable rest point for
the system
x˙ = −11x − 48y − 16z + xyz,
y˙ = x + 3y + 2z + x2 − yz,
z˙ = 2y + 2z + sin x.
Exercise 1.45. Use the Hartman–Grobman theorem to describe (geometri￾cally) the behavior near the origin of the system
x˙ = 4y − x2
z,
y˙ = −x + 4y + xy2
,
z˙ = −10z + yz2
.
Exercise 1.46. Prove that there are open intervals U and V containing the
origin and a differentiable map H : U → V with a differentiable inverse such
that the flow φt of ˙x = −x is conjugate to the flow ψt of ˙x = −x + x2; that is,
H(φt(H−1(x))) = ψt(x) whenever x ∈ V .
Exercise 1.47. [Reversible systems] A planar system ˙x = f(x, y), ˙y = g(x, y)
is called reversible if it is invariant under the change of variables t → −t and
y → −y. For example, ˙x = −y, ˙y = x is reversible. (a) Prove that a linear center
of a reversible system is a center. (b) Construct a planar system with exactly
three rest points all of which are centers. Hint: The system ˙x = (x + 2)x(x − 2)y,
y˙ = (x − 1)(x + 1) has exactly two rest points both of which are centers.
1.6 Stability and the Direct Method of Lyapunov
Let us consider a rest point x0 for the autonomous differential equation
x˙ = f(x), x ∈ Rn. (1.12)
A continuous function V : U → R , where U ⊆ Rn is an open set with
x0 ∈ U, is called a Lyapunov function for the differential equation (1.12)
at x0 if
(i) V (x0) = 0,
(ii) V (x) > 0 for x ∈ U \ {x0},
(iii) the function V is continuously differentiable on the set U \ {x0}, and,
on this set, V˙ (x) := grad V (x) · f(x) ≤ 0.
The function V is called a strict Lyapunov function if, in addition,
(iv ) V˙ (x) < 0 for x ∈ U \ {x0}.26 1. Introduction to Ordinary Differential Equations
Figure 1.10: Level sets of a Lyapunov function.
Theorem 1.48 (Lyapunov’s Stability Theorem). If there is a Lya￾punov function defined in an open neighborhood of a rest point of the dif￾ferential equation (1.12), then the rest point is stable. If, in addition, the
Lyapunov function is a strict Lyapunov function, then the rest point is
asymptotically stable.
The idea of Lyapunov’s method is very simple. In many cases the level
sets of V are “spheres” surrounding the rest point x0 as in Figure 1.10.
Suppose this is the case and let φt denote the flow of the differential equa￾tion (1.12). If y is in the level set Sc = {x ∈ Rn : V (x) = c} of the function
V , then, by the chain rule, we have that
d
dtV (φt(y))



t=0
= grad V (y) · f(y) ≤ 0. (1.13)
The vector grad V is an outer normal for Sc at y. (Do you see why it must
be the outer normal?) Thus, V is not increasing on the curve t → φt(y)
at t = 0, and, as a result, the image of this curve either lies in the level
set Sc, or the set {φt(y) : t > 0} is a subset of the set in Rn with outer
boundary Sc. The same result is true for every point on Sc. Therefore, a
solution starting on Sc is trapped; it either stays in Sc, or it stays in the set
{x ∈ Rn : V (x) < c}. The stability of the rest point follows easily from this
result. If V is a strict Lyapunov function, then the solution curve definitely
crosses the level set Sc and remains inside the set {x ∈ Rn : V (x) < c} for
all t > 0. Because the same property holds at all level sets “inside” Sc, the
rest point x0 is asymptotically stable.
If the level sets of our Lyapunov function are as depicted in Figure 1.10,
then the argument just given proves the stability of the rest point. But the1.6 Stability and the Direct Method of Lyapunov 27
level sets of a Lyapunov function may not have this simple configuration.
For example, some of the level sets may not be bounded.
The proof of Lyapunov’s stability theorem requires a more delicate anal￾ysis. Let us use the following notation. For α > 0 and ζ ∈ Rn, define
Sα(ζ) := {x ∈ Rn : |x − ζ| = α},
Bα(ζ) := {x ∈ Rn : |x − ζ| < α},
B¯α(ζ) := {x ∈ Rn : |x − ζ| ≤ α}.
Proof. Suppose that  > 0 is given. In view of the definition of Lyapunov
stability, it suffices to assume that the closed ball B¯(x0) is contained in
the domain U of the Lyapunov function V . Because S(x0) is a compact
set not containing x0, there is a number m > 0 such that V (x) ≥ m for
all x ∈ S(x0). Also, there is some number δ > 0 such that δ< and
V (x) ≤ m/2 for all x in the compact set B¯δ(x0). If not, then for each k ≥ 2
there is a point xk in B¯/k(x0) such that V (xk) > m/2. The sequence
{xk}∞
k=2 converges to x0. Using the continuity of the Lyapunov function V
at x0, the limit of the sequence {V (xk)}∞
k=2 as k increases without bound
is V (x0), which is zero by the definition of V . But every element of the
sequence is larger than m/2, in contradiction.
Let φt denote the flow of differential equation (1.12). For every x ∈ U,
the existence theorem implies that there is some interval containing t = 0
on which the function t → φt(x) is defined. Using this fact,
d
dtV (φt(x))


t=0 = grad V (x) · f(x) = V˙ (x) ≤ 0.
Thus, the function t → V (φt(x)) is not increasing on its domain of defini￾tion.
For x ∈ Bδ(x0),
V (φ0(x)) = V (x) ≤ m/2 < m
and V is not increasing along the positive direction of the flow. Therefore,
V (φt(x)) < m for all t ≥ 0 for which the solution t → φt(x) is defined. For
all these values of t,
φt(x) ∈ B(x0).
If not, there is some T > 0 such that |φT (x)−x0| ≥ . Since t → |φt(x)−x0|
is a continuous function, there must then be some τ with 0 < τ ≤ T such
that |φτ (x) − x0| = . For this τ , we have V (φτ (x)) ≥ m, in contradiction.
Thus, φt(x) ∈ B(x0) for all t ≥ 0 for which the solution through x exists.
By the extension theorem, if the solution does not exist for all t ≥ 0, then
|φt(x)|→∞ as t → ∞, or φt(x) approaches the boundary of the domain of
definition of f. Since neither of these possibilities occur, the solution exists
for all positive time with its corresponding image in the set B(x0). Thus,
x0 is stable.28 1. Introduction to Ordinary Differential Equations
Suppose, in addition, that the Lyapunov function is strict and let x ∈
Bδ(x0). By the compactness of B¯(x0), either limt→∞ φt(x) = x0, or there
is a sequence {tk}∞
k=1 of real numbers 0 < t1 < t2 ··· with tk → ∞ such
that the sequence {φtk (x)}∞
k=1 converges to some point x∗ ∈ B¯(x0) with
x∗ = x0. If x0 is not asymptotically stable, then such a sequence exists for
at least one point x ∈ Bδ(x0).
Using the continuity of V ,
lim
k→∞ V (φtk (x)) = V (x∗).
For each natural number k,
V (φtk (x)) > V (x∗)
because the function t → V (φt(x∗)) is strictly decreasing. This property
also implies that
lim
k→∞ V (φ1+tk (x)) = limk→∞ V (φ1(φtk (x))) = V (φ1(x∗)) < V (x∗).
Thus, there is some natural number  such that V (φ1+t (x)) < V (x∗).
Clearly, there is also an integer j> such that tj > 1 + t. For this
integer, we have the inequalities V (φtj (x)) < V (φ1+t (x)) < V (x∗), in
contradiction. 
The next result can be used to prove a rest point is unstable.
Theorem 1.49. Suppose that x0 is a rest point of the differential equa￾tion (1.12) and V is a smooth function defined on an open neighborhood
U of x0. If V (x0)=0, V˙ (x) > 0 on U \ {x0}, and V has a positive value
somewhere in each open set containing x0, then x0 is not a stable rest point.
Proof. Suppose that x0 is stable, and let φt denote the flow of the differ￾ential equation. Choose  > 0 such that B¯(x0) is in the intersection of U
and the domain of f. There is some positive δ such that δ< and φt(x)
is in B(x0) whenever x ∈ Bδ(x0) and t ≥ 0. Because B¯(x0) is compact
and V is continuous, there is some α > 0 such that V (x) ≤ α whenever
x ∈ B¯(x0).
By the hypothesis, there is some x ∈ Bδ(x0) such that V (x) > 0 and
β := inf t≥0
V˙ (φt(x)) ≥ 0.
In other words, there is a sequence t1 < t2 < t3 < ··· such that limj→∞ tj =
∞ and limj→∞ V˙ (φtj (x)) = β.
Suppose that β = 0. Because the sequence {φtj (x)}∞
j=1 is contained in
the compact set B¯(x0), it contains a subsequence converging to some x∗ ∈
B¯(x0). By redefining the elements of the sequence {tj}∞
j=1 to correspond to1.6 Stability and the Direct Method of Lyapunov 29
those in this subsequence, {φtj (x)}∞
j=0 converges to x∗. By the continuity
of V˙ ,
lim
j→∞ V˙ (φtj (x)) = V˙ (x∗)=0.
Since V >˙ 0 on U \ {x0}, it follows that x∗ = x0. Because V is continuous,
limj→∞ V (φtj (x)) = V (x0) = 0. But, for each natural number j ≥ 1, the
positivity of the function V˙ implies V (φtj (x)) > V (x) > 0, in contradiction.
Hence, β > 0.
Note that
V (φt(x)) = V (x) +  t
0
V˙ (φs(x)) ds ≥ V (x) + βt
for all t ≥ 0. If t is sufficiently large, then V (φt(x)) > α, in contradiction
to the stability of x0. 
Example 1.50. The linearization of ˙x = −x3 at x = 0 is ˙x = 0. It provides
no information about stability. Define V (x) = x2 and note that V˙ (x) =
2x(−x3) = −2x4. Thus, V is a strict Lyapunov function, and the rest point
at x = 0 is asymptotically stable.
Example 1.51. Consider the harmonic oscillator ¨x + ω2x = 0 with ω > 0.
The equivalent first-order system
x˙ = y, y˙ = −ω2x
has a rest point at (x, y) = (0, 0). Define the total energy (kinetic energy
plus potential energy) of the harmonic oscillator to be
V = 1
2
x˙
2 +
ω2
2
x2 = 1
2
(y2 + ω2x2).
A computation shows that V˙ = 0. Thus, the rest point is stable. The energy
of a physical system is often a good choice for a Lyapunov function.
Exercise 1.52. As a continuation of example (1.51), consider the equivalent
first-order system
x˙ = ωy, y˙ = −ωx.
Study the stability of the rest point at the origin using Lyapunov’s direct method.
Exercise 1.53. Consider a Newtonian particle of mass m moving under the
influence of the potential U. The equation of motion (F = ma) is given by
mq¨ = − gradU(q)
where the position coordinate is denoted by q = (q1,...,qn). If q0 is a strict local
minimum of the potential, show that the equilibrium (q, q˙)=(q0, 0) is Lyapunov
stable. Hint: Consider the total energy of the particle.30 1. Introduction to Ordinary Differential Equations
Exercise 1.54. Determine the stability of the rest points of the following sys￾tems. Formulate properties of the unspecified scalar function g so that the system
has a rest point at the origin which is, respectively, stable, asymptotically stable,
and unstable.
1. ˙x = y − x3,’
y˙ = −x − y3
2. ˙x = y + αx(x2 + y2),
y˙ = −x + αy(x2 + y2)
3. ˙x = 2xy − x3,
y˙ = −x2 − y5
4. ˙x = y − xg(x, y),
y˙ = −x − yg(x, y)
5. ˙x = y + xy2 − x3 + 2xz4,
y˙ = −x − y3 − 3x2y + 3yz4,
z˙ = −5
2 y2z3 − 2x2z3 − 1
2 z7
Exercise 1.55. (a) Determine the stability of all rest points for the following
differential equations. For the unspecified scalar function g determine conditions
so that the origin is a stable and/or asymptotically stable rest point.
1. ¨x + x˙ + ω2x = 0,  > 0, ω > 0;
2. ¨x + sin x = 0;
3. ¨x + x − x3 = 0;
4. ¨x + g(x) = 0;
5. ¨x + x˙ + g(x) = 0,  > 0;
6. ¨x + ˙x3 + x = 0.
(b) The total energy is a good choice for the strict Lyapunov function required
to study system 5. It almost works. Modify the total energy to obtain a strict
Lyapunov function. Hint: See Exercise 4.19. (c) Prove the following refinement
of Theorem 1.48: Suppose that x0 is a rest point for the differential equation
x˙ = f(x) with flow φt and V is a Lyapunov function at x0. If, in addition, there
is a neighborhood W of the rest point x0 such that for each point p ∈ W \ {x0},
the function V is not constant on the set {φt(p) : t ≥ 0}, then x0 is asymptotically
stable (see Exercise 1.182). (d) Apply part (c) to system 5.
Exercise 1.56. Suppose that in addition to the hypotheses of Lyapunov’s sta￾bility theorem 1.48, the strict Lyapunov function V is defined on all of Rn and
lim|x|→∞ V (x) = ∞. (a) Prove that the rest point is globally asymptotically sta￾ble; that is every point converges to the rest point when evolved by the flow.
(b) Prove that if σ > 0, b > 0, and 0 <r< 1, then the origin is globally
asymptotically stable for the Lorenz system
x˙ = σ(y − x), y˙ = rx − y − xz, z˙ = xy − bz. (1.14)
(c) Suppose V is a Lyapunov function as in the introduction to this problem
except it is not strict. Formulate and prove a global asymptotic stability result
for this case by adding a hypothesis concerning invariant sets.1.7 Manifolds 31
Exercise 1.57. Suppose that f is a function such that f(x)+f
(x)+f(x)
3 = 0
for all x ≥ 0. Show that limx→∞ f(x) = 0 and limx→∞ f
(x) = 0.
Exercise 1.58. Suppose U : Rn → R is a smooth function and p ∈ Rn is such
that U(p) is a global minimum of U. Is p an asymptotically stable rest point
for the differential equation ˙x = − gradU(x)? If the answer is yes, prove the
statement. If not, formulate and prove a theorem that implies p is asymptotically
stable.
Exercise 1.59. [Basins of Attraction] Consider system 5 in Ex. (1.55). Note
that if g(0) = 0 and g
(0) > 0, then there is a rest point at the origin that is
asymptotically stable—a fact that can be proved using the principle of lineariza￾tion. (a) Construct a strict Lyapunov function for this system. The construction
of a strict Lyapunov function is not necessary to determine the stability of the
rest point, but a Lyapunov function can be used to estimate the basin of attrac￾tion of the rest point; that is the set of all points in the space that are asymptotic
to the rest point. Consider the (usual) first-order system corresponding to the
differential equation
x¨ + x˙ + x − x3 = 0
for  > 0. (b) Describe the basin of attraction of the origin. (c) Define a subset of
the basin of attraction, which you have described, and prove that it is contained
in the basin of attraction. (d) Prove the following general theorem. Let x0 be a
rest point of the system ˙x = f(x) in Rn with flow φt, and suppose that V : U → R
is a Lyapunov function at x0. If B is a closed neighborhood of x0 contained in U
such that φt(b) ∈ B whenever b ∈ B and t ≥ 0, and there is no complete orbit
in B \ {x0} on which V is constant, then x0 is asymptotically stable and B is in
the basin of attraction of x0 (see [140]).
Note: In engineering practice, physical systems (for example, a chemical plant
or a power electronic system) are operated in steady state. When a disturbance
occurs in the system, the control engineer wants to know if the system will return
to the steady state. If not, she will have to take drastic action. Do you see why
theorems of the type mentioned in this exercise (possible projects for the rest of
your mathematical life) might have practical value?
Exercise 1.60. (a) Prove that all solutions of the system of differential equa￾tions
x˙ = x − x3
, y˙ = x − y
are bounded in forward time. (b) Let f : R → R. Formulate and prove a theorem
concerning forward boundedness of all solutions of
x˙ = x − x3
, y˙ = f(x) − y.
1.7 Manifolds
In this section we will define the concept of a manifold as a generalization
of a linear subspace of Rn, and we will begin our discussion of the central
role that manifolds play in the theory of differential equations.32 1. Introduction to Ordinary Differential Equations
Let us note that the fundamental definitions of calculus are local in
nature. For example, the derivative of a function at a point is determined
once we know the values of the function in some neighborhood of that point.
This fact is the basis for the manifold concept: Informally, a manifold is
a subset of Rn such that, for some fixed integer k ≥ 0, each point in the
subset has a neighborhood that is essentially the same as the Euclidean
space Rk. To make this definition precise we will have to define what is
meant by a neighborhood in the subset, and we will also have to understand
the meaning of the phrase “essentially the same as Rk.” But these notions
should be intuitively clear: In effect, a neighborhood in the manifold is an
open subset that is diffeomorphic to Rk.
Points, lines, planes, arcs, spheres, and tori are examples of manifolds,
some of which have already been mentioned. Recall that a curve is a smooth
function from an open interval of real numbers into Rn. An arc is the
image of a curve. Every solution of a differential equation is a curve; the
corresponding orbit is an arc. Thus, every orbit of a differential equation is
a manifold. In particular, a periodic orbit is a one-dimensional torus. These
statements are true as long as the objects are viewed without reference to
how they are “embedded” in some larger space. For example, although
an arc in Rn where n > 1 is a manifold, it may not be a submanifold of
Rn because the arc may accumulate on itself (see Exercise 1.93). Thus,
although the intuitive notion of a manifold is a useful way to begin a
study of these objects as they relate to differential equations, it should be
clear that there are complications which can only be fully understood using
precise definitions.
We will discuss invariant manifolds, a precise approach to submanifolds
of Euclidean space, tangent spaces, coordinate transformations, and polar
coordinates as they relate to differential equations.
1.7.1 Introduction to Invariant Manifolds
Consider the differential equation
x˙ = f(x), x ∈ Rn, (1.15)
with flow φt, and let S be a subset of Rn that is a union of orbits of this
flow. If a solution has its initial condition in S, then the corresponding orbit
stays in S for all time, past and future. The concept of a set that is the
union of orbits of a differential equation is formalized in the next definition.
Definition 1.61. A set S ⊆ Rn is called an invariant set for the differen￾tial equation (1.15) if, for each x ∈ S, the solution t → φt(x), defined on its
maximal interval of existence, has its image in S. Alternatively, the orbit
passing through each x ∈ S lies in S. In addition, S is called an invariant
manifold if S is a manifold.1.7 Manifolds 33
Unstable Manifold Stable Manifold
Figure 1.11: Stable and unstable manifolds for the linear saddle at the
origin for the system ˙x = −x, ˙y = y.
We will illustrate the notion of invariant manifolds for autonomous differ￾ential equations by describing two important examples: the stable, unstable,
and center manifolds of a rest point, and the energy surfaces of Hamiltonian
systems.
The stable manifold concept is perhaps best introduced by discussing a
concrete example. Thus, let us consider the planar first-order system
x˙ = −x, y˙ = y,
and note that the x-axis and the y-axis are invariant one-dimensional man￾ifolds. The invariance of these sets follows immediately by inspection of
the solution of the uncoupled linear system. Note that a solution with ini￾tial value on the x-axis approaches the rest point (x, y) = (0, 0) as time
increases to +∞, and a solution with initial value on the y-axis approaches
the rest point as time decreases to −∞. Solutions on the x-axis move toward
the rest point; solutions on the y-axis move away from the rest point. For
this example, the x-axis is called the stable manifold of the rest point, and
the y-axis is called the unstable manifold (see Figure 1.11).
Similar invariant linear subspaces exist for all linear systems ˙x = Ax,
x ∈ Rn. In fact, the space Rn can always be decomposed as a direct sum
of linear subspaces: the stable eigenspace (stable manifold) defined to be
the A-invariant subspace of Rn such that the eigenvalues of the restriction
of A to this space are exactly the eigenvalues of A with negative real parts,
the unstable eigenspace (unstable manifold) corresponding similarly to the
eigenvalues of A with positive real parts, and the center eigenspace (center
manifold) corresponding to the eigenvalues with zero real parts. It turns out34 1. Introduction to Ordinary Differential Equations
Center Manifold
Stable Manifold
Figure 1.12: Phase portrait for a linear system with a one-dimensional
stable and a two-dimensional center manifold.
that these linear subspaces are also invariant sets for the linear differential
equation ˙x = Ax. Thus, they determine its phase portrait. For example,
Figure 1.12 shows the phase portrait of a linear system on R3 with a one￾dimensional stable manifold and a two-dimensional center manifold. Of
course, some of these invariant sets might be empty. In particular, if A
is infinitesimally hyperbolic (equivalently, if the rest point at the origin is
hyperbolic), then the linear system has an empty center manifold at the
origin.
Exercise 1.62. Discuss the existence of stable, unstable, and center manifolds
for the linear systems with the following system matrices:
⎛
⎝
−110
0 −1 0
0 02
⎞
⎠ ,
⎛
⎝
123
456
789
⎞
⎠ ,
⎛
⎝
010
−10 0
0 0 −2
⎞
⎠ .
Two important theorems in the subject of differential equations, the sta￾ble manifold theorem and the center manifold theorem, will be proved in
Chapter 7. We have the following formal definition.
Definition 1.63. The stable manifold of a rest point x0 for an autonomous
differential equation with (locally defined) flow φt is the set of all points x
in the domain of definition of φt such that limt→∞ φt(x) = x0. The unstable
manifold of x0 is the set of all points x in the domain of definition of φt
such that limt→−∞ φt(x) = x0.1.7 Manifolds 35
The stable manifold theorem states that a hyperbolic rest point has a
unique stable manifold (respectively, unstable manifold) that is tangent to
the corresponding stable (respectively, unstable) eigenspace of the corre￾sponding linearized system at the rest point and that these invariant sets
are indeed smooth manifolds.
The Hartman–Grobman theorem implies that a hyperbolic rest point has
stable and unstable invariant sets that are homeomorphic images of the
corresponding invariant manifolds for the corresponding linearized system,
but it gives no indication that these invariant sets are smooth manifolds.
The existence of stable and unstable invariant manifolds is essential to
our understanding of many features of the dynamics of differential equa￾tions. For example, their existence provides a theoretical basis for deter￾mining the analytic properties of the flow of a differential equation in the
neighborhood of a hyperbolic rest point. They also serve to bound other
invariant regions in the phase space. Thus, the network of all stable and
unstable manifolds forms the “skeleton” for the phase portrait. Finally, the
existence of the stable and unstable manifolds in the phase space, espe￾cially their intersection properties, lies at the heart of an explanation of
the complex motions associated with many nonlinear ordinary differential
equations. In particular, this phenomenon is fundamental in the study of
deterministic chaos (see Chapter 9).
For rest points of a differential equation that are not hyperbolic, the
center manifold theorem states the existence of an invariant manifold tan￾gent to the corresponding center eigenspace. This center manifold is not
necessarily unique, but the differential equation has the same (arbitrar￾ily complicated) phase portrait when restricted to any one of the center
manifolds at the same rest point. In particular, center manifolds cannot be
characterized by a simple dynamical property as in Definition 1.63. Analy￾sis using center manifolds is often required to understand many of the most
delicate problems that arise in the theory and applications of differential
equations. For example, the existence and smoothness properties of center
manifolds are foundational results in bifurcation theory (see Chapter 11).
Other types of invariant sets, for example, periodic orbits can have stable
manifolds, unstable manifolds and center manifolds. Indeed, the extension
of Definition 1.63 to a general invariant set is clear.
Exercise 1.64. Determine the stable and unstable manifolds for the rest point
of the system
x˙ = 2x − (2 + y)e
y, y˙ = −y.
Hint: See Exercise 1.6.
Exercise 1.65. (a) Determine a stable and a center manifold for the rest point
of the system
x˙ = x2
, y˙ = −y.
(b) Show that the system has infinitely many center manifolds.36 1. Introduction to Ordinary Differential Equations
Invariant manifolds, called energy surfaces, are useful in the study of
Hamiltonian systems of differential equations. To define this important
class of differential equations, let H : Rn × Rn → R be a smooth func￾tion given by
(q1,...,qn, p1,...,pn) → H(q1,...,qn, p1,...,pn),
and define the associated Hamiltonian system on R2n with Hamiltonian H
by
q˙i = ∂H
∂pi
, p˙i = −∂H
∂qi
, i = 1, . . . , n.
Let us note that the dimension of the phase space of a Hamiltonian system
is required to be even. The reason for this restriction will soon be made
clear.
As a prototypical example of a Hamiltonian system, let H : R2 → R be
given by H(x, y) := 1
2 (y2 + ω2x2). The associated Hamiltonian system is
the harmonic oscillator
x˙ = y, y˙ = −ω2x.
More generally, suppose that U : Rn → R and let H : Rn × Rn → R be
given by
H(q, p) = p2
2m
+ U(q)
where p2 := p2
1 + ··· + p2
n. A Hamiltonian in this form is called a classical
Hamiltonian. The corresponding Hamiltonian system
q˙ = 1
m
p, p˙ = − gradU(q)
is equivalent to Newton’s equation of motion for a particle influenced by
a conservative force (see Exercise 1.53). The vector quantity p := mq˙ is
called the (generalized) momentum, the function U is called the potential
energy, and the function p → 1
2m p2 = m
2 q˙
2 is called the kinetic energy.
The configuration space for a classical mechanical system is the space
consisting of all possible positions of the system, and the corresponding
Hamiltonian system is said to have n degrees of freedom if the configuration
space is locally specified by n coordinates (q1,...,qn). For example, for the
pendulum, the configuration space can be taken to be R with the coordinate
q1 specifying the angular position of the bob relative to the downward
vertical. It is a system with one degree of freedom. Of course, for this
example, the physical positions are specified by the angular coordinate q1
modulo 2π. Thus, the configuration space can also be viewed as a nonlinear
manifold—namely, the unit circle in the plane. This is yet another way in
which manifolds arise in the study of mechanical systems.
The phase space of a Hamiltonian system is the subset of Rn × Rn of all
positions and momenta specified by the coordinates (q1,...,qn, p1,...,pn).1.7 Manifolds 37
The dimension of the phase space is therefore even; it is the space in which
the Hamiltonian system evolves. The state space is also a subset of Rn ×
Rn, but it is the space of positions and velocities with the coordinates
(q1,...,qn, q˙1,..., q˙n) (see Chapter 6).
For c ∈ R and the Hamiltonian H : Rn × Rn → R, the corresponding
energy surface with energy c is defined to be the set
Sc = {(q, p) ∈ Rn × Rn : H(q, p) = c}.
If grad H(q, p) = 0 for each (q, p) ∈ Sc, then the set Sc is called a regular
energy surface.
Note that the vector field given by
grad H = (∂H
∂q ,
∂H
∂p )
is orthogonal to the Hamiltonian vector field given by
(
∂H
∂p , −∂H
∂q )
at each point in the phase space. Thus, the Hamiltonian vector field is
everywhere tangent to each regular energy surface. As a consequence of
this fact—a proof will be given later in this section—every energy surface
Sc is an invariant set for the flow of the corresponding Hamiltonian system.
Moreover, every regular energy surface is an invariant manifold.
The structure of energy surfaces and their invariance is important. Indeed,
the phase space of a Hamiltonian system is the union of its energy surfaces.
Or, as we say, the space is foliated by its energy surfaces. Moreover, each
regular energy surface of a Hamiltonian system with n degrees of freedom
has “dimension” 2n − 1. Thus, we can reduce the dimension of the phase
space by studying the flow of the original Hamiltonian system restricted to
each of these invariant subspaces. For example, the analysis of a Hamilto￾nian system with one degree of freedom can be reduced to the consideration
of just one space dimension where the solution of the Hamiltonian differ￾ential equation can be reduced to a quadrature. To see what this means,
consider the classical Hamiltonian H(q, p) = 1
2 p2 + U(q) on R × R and a
regular energy surface of H with energy h. Notice that, by using the Hamil￾tonian differential equations and the energy relation, we have the following
scalar differential equations
q˙ = p = dq
dt = ±(2(h − U(q)))1/2
for solutions whose initial conditions are on this energy surface. By separa￾tion of variables and a specification of the initial condition, the ambiguous
sign is determined and the solution of the corresponding scalar differential38 1. Introduction to Ordinary Differential Equations
equation is given implicitly by the integral (=quadrature)
 q(t)
q(0)
(2(h − U(q)))−1/2 dq = ±t.
This result “solves” the original system of Hamiltonian differential equa￾tions. The same idea works for systems with several degrees of freedom,
only the equations are more complicated.
Let us also note that the total energy of a Hamiltonian system might
not be the only conserved quantity. In fact, if F is a function on the phase
space with the property that grad F(q, p) is orthogonal to the Hamiltonian
vector field at every point (q, p) in an open subset of the phase space,
then the level sets of F are also invariant sets. In this case F is called an
integral, or first integral, of the Hamiltonian system. Thus, the intersection
of an energy surface and a level set of F must also be invariant, and, as
a consequence, the space is foliated with (2n − 2)-dimensional invariant
sets. If there are enough first integrals, then the solution of the original
system can be expressed in quadratures. In fact, for an n-degree-of-freedom
Hamiltonian system, it suffices to determine n “independent” first integrals
(see [13, §49]). For these reasons, it should be clear that energy surfaces, or
more generally, level sets of first integrals, are important objects that are
worthy of study. They are prime examples of smooth manifolds.
While the notion of an energy surface is naturally associated with Hamil￾tonian systems, the underlying idea for proving the invariance of energy
surfaces easily extends to general autonomous systems. In fact, if ˙x = f(x)
is an autonomous system with x ∈ Rn and the function G : Rn → R is
such that the vector grad G(x) is orthogonal to f(x) for all x in some open
subset of Rn, then every level set of G that is contained in this open set
is invariant. Thus, just as for Hamiltonian systems, some of the dynamical
properties of the differential equation ˙x = f(x) can be studied by restrict￾ing attention to a level set of G, a set that has codimension one in the
phase space (see Exercise 1.73).
Exercise 1.66. Find the Hamiltonian for a first-order system equivalent to the
model equation for the pendulum given by ¨θ+k sin θ = 0 where k is a parameter.
Describe the energy surfaces.
Exercise 1.67. Reduce the solution of the harmonic oscillator H(q, p) = 1
2 (p2+
ω2q2) where ω > 0 to a quadrature on each of its regular energy surfaces and carry
out the integration explicitly. (This is not the simplest way to solve the equations
of motion, but you will learn a valuable method that is used, for example, in the
construction of the solution of the equations of motion for the Hamiltonian system
mentioned in the previous exercise.)1.7 Manifolds 39
Middle Axis
Short Axis
Long Axis
Figure 1.13: A rigid body and its three axes of symmetry.
Exercise 1.68. (a) Show that
˙
I1 = I1 cos(θ1 − θ2),
˙
I2 = −I1 cos(θ1 − θ2),
˙
θ1 = −1 − sin(θ1 − θ2),
˙
θ2 = 1
is a Hamiltonian system. (b) Find a first integral that is independent of the
Hamiltonian.
Exercise 1.69. (a) The Hamiltonian system with Hamiltonian H = q2p1 −
q1p2+q4p3−q3p4+q2q3 has a rest point at the origin (see [164, p. 212]). Linearize
at the origin and determine the eigenvalues. What can you conclude about the
stability of the rest point? (b) Prove that the rest point at the origin is unstable.
Exercise 1.70. [Basins of Attraction] Consider the differential equation
x¨ + x˙ − x + x3 = 0
with parameter . (a) Show that the system with  = 0 corresponds to a classical
Hamiltonian system with a double-well potential and draw the phase portrait of
this system. (b) Draw the phase portrait of the system for  > 0. Note: In this
case, the term x˙ models viscous damping. (c) What is the fate of the solution
with initial condition (x(0), x˙ (0)) = (4, 0) for  = 0.1? Note: To solve this problem
you will probably have to resort to numerics. How do we know that the result
obtained by a numerical simulation is correct?40 1. Introduction to Ordinary Differential Equations
Exercise 1.71. [Gradient Systems] If H is a Hamiltonian, then the vector field
grad H is everywhere orthogonal to the corresponding Hamiltonian vector field.
What are the properties of the flow of grad H? More generally, for a smooth
function G : Rn → R (maybe n is odd), let us define the associated gradient
system
x˙ = grad G(x).
Because a conservative force is the negative gradient of a potential, many authors
define the gradient system with potential G to be ˙x = − grad G(x). The choice of
sign simply determines the direction of the flow. Prove the following statements:
(a) A gradient system has no periodic orbits. (b) If a gradient system has a
rest point, then all of the eigenvalues of its linearization at the rest point are
real. (c) In the plane, the orbits of the gradient system with potential G are
orthogonal trajectories for the orbits of the Hamiltonian system with Hamiltonian
G. (d) If x0 ∈ Rn is an isolated maximum of the function G : Rn → R, then
x0 is an asymptotically stable rest point of the corresponding gradient system
x˙ = grad G(x).
Exercise 1.72. Suppose H : R2 → R is a nonzero function. Consider the cor￾responding (usual) Hamiltonian XH and gradient ∇H vector fields. (a) Show
that XH and ∇H are everywhere orthogonal. (b) Let R(θ) be the usual rota￾tion matrix for a positive rotation through θ radians. The usual inner product
of R(0)XH(x, y) and XH(x, y) is the square of the norm of XH, which is non￾negative. Likewise, the inner product of R(π/2)XH(x, y) and XH(x, y) is zero.
Prove that if XH(x, y) is not zero, then the inner product of R(θ)XH(x, y) and
XH(x, y) is not zero for 0 ≤ θ < π/2. (c) A vector field is called gradient-like if
there is a function that decreases along its nonconstant orbits and vanishes at its
rest points. Are all the vector fields R(θ)XH gradient-like for 0 < θ ≤ π/2? (d)
Formulate and prove one or more general properties of the flows of gradient-like
vector fields.
Exercise 1.73. [Rigid Body Motion] A system that is not Hamiltonian, but
closely related to this class, is given by Euler’s equations for rigid body motion.
The angular momentum M = (M1, M2, M3) of a rigid body, relative to a coordi￾nate frame rotating with the body with axes along the principal axes of the body
and with origin at its center of mass, is related to the angular velocity vector Ω
by M = AΩ, where A is a symmetric matrix called the inertia matrix. Euler’s
equation is M˙ = M × Ω. Equivalently, the equation for the angular velocity is
AΩ=( ˙ AΩ) × Ω. If A is diagonal with diagonal components (moments of iner￾tia) (I1, I2, I3), show that Euler’s equations for the components of the angular
momentum are given by
M˙ 1 = −
 1
I2
− 1
I3

M2M3,
M˙ 2 =  1
I1
− 1
I3

M1M3,
M˙ 3 = −
 1
I1
− 1
I2

M1M2.
Assume that 0 < I1 ≤ I2 ≤ I3. Find some invariant manifolds for this system. Can
you use your results to find a qualitative description of the motion? As a physical1.7 Manifolds 41
example, take this book and hold its covers together with a rubber band. Then,
toss the book vertically three times, imparting a rotation in turn about each of its
axes of symmetry (see Figure 1.13). Are all three rotary motions Lyapunov stable?
Do you observe any other interesting phenomena associated with the motion?
For example, pay attention to the direction of the front cover of the book after
each toss. Hint: Look for invariant quadric surfaces; that is, manifolds defined as
level sets of quadratic polynomials (first integrals) in the variables (M1, M2, M3).
For example, show that the kinetic energy given by 1
2 AΩ, Ω is constant along
orbits. The total angular momentum (length of the angular momentum) is also
conserved. For a complete mathematical description of rigid body motion, see [13].
For a mathematical description of the observed “twist” in the rotation of the
tossed book, see [22]. Note that Euler’s equations do not describe the motion of
the book in space. To do so would require a functional relationship between the
coordinate system rotating with the body and the position coordinates relative
to a fixed coordinate frame in space.
1.7.2 Smooth Manifolds
Because the modern definition of a smooth manifold can appear quite
formidable at first sight, we will formulate a simpler equivalent definition
for the class of manifolds called the submanifolds of Rn. Fortunately, this
class is rich enough to contain the manifolds that are met most often in the
study of differential equations. In fact, every manifold can be “embedded”
as a submanifold of some Euclidean space. Thus, the class that we will
study can be considered to contain all manifolds.
Recall that a manifold is supposed to be a set that is locally the same as
Rk. Thus, whatever is meant by “locally the same,” every open subset of
Rk must be a manifold.
If W ⊆ Rk is an open set and g : W → Rn−k is a smooth function, then
the graph of g is the subset of Rn defined by
graph(g) := {(w, g(w)) ∈ Rn : w ∈ W}.
The set graph(g) is the same as W ⊆ Rk up to a nonlinear change of
coordinates. By this we mean that there is a smooth map G with domain
W and image graph(g) such that G has a smooth inverse. In fact, such a
map G : W → graph(g) is given by G(w)=(w, g(w)). Clearly, G is smooth.
Its inverse is the linear projection on the first k-coordinates of the point
(w, g(w)) ∈ graph(g); that is, G−1(w, g(w)) = w. Thus, G−1 is smooth as
well.
Open subsets and graphs of smooth functions are the prototypical exam￾ples of what we will call submanifolds. But these classes are too restric￾tive; they include objects that are globally the same as some Euclidean
space. The unit circle T in the plane, also called the one-dimensional
torus, is an example of a submanifold that is not of this type. Indeed,
T := {(x, y) : x2 + y2 = 1} is not the graph of a scalar function defined on42 1. Introduction to Ordinary Differential Equations
G(W)
W
S
G
Figure 1.14: A chart for a two-dimensional submanifold in R3.
an open subset of R. But every point of T is contained in a neighborhood
in T that is the graph of such a function. In other words, T is locally the
same as R. In fact, each point in T is in one of the four sets
S± := {(x, y) ∈ R2 : y = ±
1 − x2, |x| < 1},
S± := {(x, y) ∈ R2 : x = ±
1 − y2, |y| < 1}.
Submanifolds of Rn are subsets with the same basic property: Every point
in the subset is in a neighborhood that is the graph of a smooth function.
To formalize the submanifold concept for subsets of Rn, we must deal
with the problem that, in the usual coordinates of Rn, not all graphs are
given by sets of the form
{(x1,...,xk, gk+1(x1,...,xk),...,gn(x1,...,xk)) :
(x1,...,xk) ∈ W ⊆ Rk}.
Rather, we must allow, as in the example provided by T, for graphs of func￾tions that are not functions of the first k-coordinates of Rn. To overcome
this technical difficulty we will build permutations of the variables into our
definition.
Definition 1.74. Suppose that S ⊆ Rn and x ∈ S. The pair (W, G) where
W is an open subset of Rk for some k ≤ n and G : W → Rn is a
smooth function is called a k-dimensional submanifold chart for S at x
(see Figure 1.14) if there is an open set U ⊆ Rn with x ∈ U ∩ S such that
U ∩ S = G(W) and one of the following two properties is satisfied:
1) The integer k is equal to n and G is the identity map.
2) The integer k is less than n and G has the form
G(w) = A
 w
g(w)
1.7 Manifolds 43
where g : W → Rn−k is a smooth function and A is a nonsingular n × n
matrix.
Definition 1.75. The set S ⊆ Rn is called a k-dimensional smooth sub￾manifold of Rn if there is a k-dimensional submanifold chart for S at every
point x in S.
The map G in a submanifold chart (W, G) is called a submanifold coor￾dinate map. If S is a submanifold of Rn, then (even though we have not
yet defined the concept), let us also call a submanifold S of Rn a smooth
manifold.
As an example, let us show that T is a one-dimensional manifold. Con￾sider a point in the subset S+ = {(x, y) : x = 1 − y2, |y| < 1} of T. Define
the set W := {t ∈ R : |t| < 1}, the function g : W → R by g(t) = √1 − t2,
the set U := {(x, y) ∈ R2 : (x − 1)2 + y2 < 2}, and the matrix
A := 0 1
1 0
.
Then we have
T ∩ U =
	x
y

∈ R2 :
x
y

=
0 1
1 0  t
g(t)

, t ∈ W


.
Similarly, T is locally the graph of a smooth function at points in the subsets
S− and S±, as required.
A simple but important result about submanifold charts is the following
proposition.
Proposition 1.76. If (W, G) is a submanifold chart for a k-dimensional
submanifold of Rn, then the function G : W → G(W) ⊆ S is invertible.
Moreover, the inverse of G is the restriction of a smooth function that is
defined on all of Rn.
Proof. The result is obvious if k = n. If k<n, then define Π : Rn → Rk to
be the linear projection on the first k-coordinates; that is, Π(x1,...,xn) =
(x1,...,xk), and define
F : G(W) → W
by
F(s)=ΠA−1s.
Clearly, F is smooth as a function defined on all of Rn. Also, if w ∈ W,
then
F ◦ G(w) = F

A
 w
g(w)
 = ΠA−1A
 w
g(w)

= w.
If s ∈ G(W), then s = A
 w
g(w)

for some w ∈ W. Hence, we also have
G(F(s)) = G(w) = s.44 1. Introduction to Ordinary Differential Equations
This proves that F is the inverse of G. 
If S is a submanifold, then we can use the submanifold charts to define
the open subsets of S.
Definition 1.77. Suppose that S is a submanifold. The open subsets of
S are all possible unions of all sets of the form G(W) where (W, G) is a
submanifold chart for S.
The next proposition is an immediate consequence of the definitions.
Proposition 1.78. If S is a submanifold of Rn and if V is an open subset
of S, then there is an open set U of Rn such that V = S ∩ U; that is, the
topology defined on S using the submanifold charts agrees with the subspace
topology on S.
As mentioned above, one of the main reasons for defining the manifold
concept is to distinguish those subsets of Rn on which we can use the
calculus. To do so, let us first make precise the notion of a smooth function.
Definition 1.79. Suppose that S1 is a submanifold of Rm, S2 is a subman￾ifold of Rn, and F is a function F : S1 → S2. We say that F is differentiable
at x1 ∈ S1 if there are submanifold charts (W1, G1) at x1 and (W2, G2) at
F(x1) such that the map G−1
2 ◦ F ◦ G1 : W1 → W2 is differentiable at
G−1
1 (x1) ∈ W1. If F is differentiable at each point of an open subset V of
S1, then we say that F is differentiable on V .
Definition 1.80. Suppose that S1 and S2 are manifolds. A smooth func￾tion F : S1 → S2 is called a diffeomorphism if there is a smooth function
H : S2 → S1 such that H(F(s)) = s for every s ∈ S1 and F(H(s)) = s for
every s ∈ S2. The function H is called the inverse of F and is denoted by
F −1.
With respect to the notation in Definition 1.79, we have defined the
concept of differentiability for the function F : S1 → S2, but we have not yet
defined what we mean by its derivative. We have, however, determined the
derivative relative to the submanifold charts used in the definition. Indeed,
the local representative of the function F is given by G−1
2 ◦F ◦G1, a function
defined on an open subset of a Euclidean space with range in another
Euclidean space. By definition, the local representative of the derivative
of F relative to the given submanifold charts is the usual derivative in
Euclidean space of this local representative of F. In the next subsection,
we will interpret the derivative of F without regard to the choice of a
submanifold chart; that is, we will give a coordinate-free definition of the
derivative of F (see also Exercise 1.81).
Exercise 1.81. Prove: The differentiability of a function defined on a manifold
does not depend on the choice of submanifold chart.1.7 Manifolds 45
Exercise 1.82. (a) Show that ˙
θ = f(θ) can be viewed as a (smooth) differential
equation on the unit circle if and only if f is periodic. To be compatible with
the usual (angular) coordinate on the circle it is convenient to consider only 2π￾periodic functions (see Section 1.7.6). (b) Describe the bifurcations that occur
for the family ˙
θ = 1 − λ sin θ with λ ≥ 0. (c) For each λ < 1, the corresponding
differential equation has a periodic orbit. Determine the period of this periodic
orbit and describe the behavior of the period as λ → 1 (see [244, p. 98]).
We have used the phrase “smooth function” to refer to a function that is
continuously differentiable. In view of Definition 1.79, the smoothness of a
function defined on a manifold is determined by the smoothness of its local
representatives—functions that are defined on open subsets of Euclidean
spaces. It is clear that smoothness of all desired orders can be defined in
the same manner by imposing the requirement on local representatives.
More precisely, if F is a function defined on a manifold S, then we will
say that F is an element of Cr(S), for r a nonnegative integer, r = ∞,
or r = ω, provided that at each point of S there is a local representative
of F all of whose partial derivatives up to and including those of order r
are continuous. If r = ∞, then all partial derivatives are required to be
continuous. If r = ω, then all local representatives are all required to have
convergent power series representations valid in a neighborhood of each
point of their domains. A function in Cω is called real analytic.
In the subject of differential equations, specifying the minimum number
of derivatives of a function required to obtain a result often obscures the
main ideas that are being illustrated. Thus, as a convenient informality, we
will often use the phrase “smooth function” to mean that the function in
question has as many continuous derivatives as needed. In cases where the
exact requirement for the number of derivatives is essential, we will refer
to the appropriate class of Cr functions.
The next definition formalizes the concept of a coordinate system.
Definition 1.83. Suppose that S is a k-dimensional submanifold. The pair
(V, Ψ) is called a coordinate system or coordinate chart on S if V is an open
subset of S, W is an open subset of Rk, and Ψ : V → W is a diffeomorphism.
Exercise 1.84. Prove: If (W, G) is a submanifold chart for a manifold S, then
(G(W), G−1) is a coordinate chart on S.
The abstract definition of a manifold is based on the concept of coordi￾nate charts. Informally, a set S together with a collection of subsets S is
defined to be a k-dimensional manifold if every point of S is contained in at
least one set in S and if, for each member V of S, there is a corresponding
open subset W of Rk and a function Ψ : V → W that is bijective. If two
such subsets V1 and V2 overlap, then the domain of the map
Ψ1 ◦ Ψ−1
2 : Ψ2(V1 ∩ V2) → W146 1. Introduction to Ordinary Differential Equations
is an open subset of Rk whose range is contained in an open subset of
Rk. The set S is called a manifold provided that all such “overlap maps”
are smooth (see [137] for the formal definition). This abstract notion of a
manifold has the advantage that it does not require a manifold to be a
subset of a Euclidean space.
Exercise 1.85. Prove: If F : Rm → Rn is smooth and F(S1) ⊆ S2 for subman￾ifolds S1 and S2, then the restriction of F to S1 is differentiable.
Exercise 1.86. Prove: If α ∈ R, then the map T → T given by
(x, y) → (x cos α − y sin α, x sin α + y cos α)
is a diffeomorphism.
Now that we know the definition of a manifold, we are ready to prove
that linear subspaces of Rn and regular level sets of smooth functions are
manifolds.
Proposition 1.87. A linear subspace of Rn is a submanifold.
Proof. Let us suppose that S is the span of the k linearly independent
vectors v1,...,vk in Rn. We will show that S is a k-dimensional submanifold
of Rn.
Let e1,..., en denote the standard basis of Rn. By a basic result from
linear algebra, there is a set consisting of n − k standard basis vectors
fk+1,...,fn such that the vectors
v1,...,vk, fk+1,...,fn
are a basis for Rn. (Why?) Let us denote the remaining set of standard
basis vectors by f1,...,fk. For each j = 1,...,k, there are scalars λj
i and
μj
i such that
fj = 
k
i=1
λj
i vi + n
i=k+1
μj
i fi.
Hence, if (t1,...,tk) ∈ Rk, then the vector

k
j=1
tjfj −
k
j=1
tj
 n
i=k+1
μj
i fi

= 
k
j=1
tj

k
i=1
λj
i vi

is in S; and, relative to the basis f1,...,fn, the vector
(t1,...,tk, −

k
j=1
tjμj
k+1,..., −

k
j=1
tjμj
n)
is in S.1.7 Manifolds 47
Define g : Rk → Rn−k by
g(t1,...,tk) := 
−
k
j=1
tjμj
k+1,..., −

k
j=1
tjμj
n

and let A denote the permutation matrix given by Aej = fj . It follows that
the pair (Rk, G), where G : Rk → Rn is defined by
G(w) = A
 w
g(w)

is a k-dimensional submanifold chart such that G(Rk) = Rn ∩ S. In fact,
by the construction, it is clear that the image of G is a linear subspace of
S. Moreover, because the image of G has dimension k as a vector space,
the subspace G(Rk) is equal to S. 
As mentioned previously, linear subspaces often arise as invariant man￾ifolds of differential equations. For example, consider the differential equa￾tion given by ˙x = Ax where x ∈ Rn and A is an n × n matrix. If S is
an invariant subspace for the matrix A, for example, one of its generalized
eigenspaces, then, by Proposition 1.87, S is a submanifold of Rn. Also, S
is an invariant set for the corresponding linear system of differential equa￾tions. Although a complete proof of this proposition requires some results
from linear systems theory that will be presented in Chapter 2, the essen￾tial features of the proof are simply illustrated in the special case where
the linear transformation A restricted to S has a complete set of eigenvec￾tors. In other words, S is a k-dimensional subspace of Rn spanned by k
linearly independent eigenvectors v1,...,vk of A. Under this assumption,
if Avi = λivi, then t → eλit
vi is a solution of ˙x = Ax. Also, note that eλit
vi
is an eigenvector of A for each t ∈ R. Therefore, if x0 ∈ S, then there are
scalars (a1,...,ak) such that x0 = k
i=1 aivi and
t → 
k
i=1
eλit
aivi
is the solution of the ordinary differential equation with initial condition
x(0) = x0. Clearly, the corresponding orbit stays in S for all t ∈ R.
Linear subspaces can be invariant sets for nonlinear differential equations.
For example, consider the Volterra–Lotka system
x˙ = x(a − by), y˙ = y(cx − d).
In case a, b, c, and d are all positive, this system models the interaction
of the population y of a predator and the population x of its prey. For48 1. Introduction to Ordinary Differential Equations
this system, the x-axis and the y-axis are each invariant sets. Indeed, sup￾pose that (0, y0) is a point on the y-axis corresponding to a population of
predators with no prey, then t → (0, e−dty0) is the solution of the system
starting at this point that models this population for all future time. This
solution stays on the y-axis for all time, and, as there are is no spontaneous
generation of prey, the predator population dies out in positive time.
Let us now discuss level sets of functions. Recall that the level set with
energy c of a smooth function H : Rn → R is the set
Sc := {x ∈ Rn : H(x) = c}.
Moreover, the level set Sc is called a regular level set if grad H(x) = 0 for
each x ∈ Sc.
Proposition 1.88. If H : Rn → R is a smooth function, then each of its
regular level sets is an (n − 1)-dimensional submanifold of Rn.
It is instructive to outline a proof of this result because it provides our
first application of a non-trivial and very important theorem from advanced
calculus, namely, the implicit function theorem.
Suppose that Sc is a regular level set of H, choose a ∈ Sc, and define
F : Rn → R by
F(x) = H(x) − c.
Let us note that F(a) = 0. Also, because grad H(a) = 0, there is at least one
integer 1 ≤ i ≤ n such that the corresponding partial derivative ∂F /∂xi
does not vanish when evaluated at a. For notational convenience let us
suppose that i = 1. All other cases can be proved in a similar manner.
We are in a typical situation: We have a function F : R × Rn−1 → R
given by (x1, x2,...,xn) → F(x1,...,xn) such that
F(a1,...,an)=0, ∂F
∂x1
(a1, a2,...,an) = 0.
This calls for an application of the implicit function theorem. A preliminary
version of the theorem is stated here; a more general version will be proved
later (see Theorem 1.275).
If f : R × Rm → Rn is given by (p, q) → f(p, q), then, for fixed b ∈ Rm,
consider the function R → Rn defined by p → f(p, b). Its derivative at
a ∈ R will be denoted by fp(a, b). Of course, with respect to the usual
bases of R and Rn, this derivative is represented by an n ×  matrix of
partial derivatives.
Theorem 1.89 (Implicit Function Theorem). Suppose that F : Rm ×
Rk → Rm is a smooth function given by (p, q) → F(p, q). If (a, b) is in
Rm × Rk such that F(a, b)=0 and the linear transformation Fp(a, b) :
Rm → Rm is invertible, then there exist two open metric balls U ⊆ Rm and
V ⊆ Rk with (a, b) ∈ U ×V together with a smooth function g : V → U such1.7 Manifolds 49
that g(b) = a and F(g(v), v)=0 for each v ∈ V . Moreover, if (u, v) ∈ U×V
and F(u, v)=0, then u = g(v).
Continuing with our outline of the proof of Proposition 1.88, let us
observe that, by an application of the implicit function theorem to F,
there is an open set Z ⊆ R with a1 ∈ Z, an open set W ⊆ Rn−1 contain￾ing the point (a2,...,an), and a smooth function g : W → Z such that
g(a2,...,an) = a1 and
H(g(x2,...,xn), x2,...,xn) − c ≡ 0.
The set
U := {(x1,...,xn) ∈ Rn : x1 ∈ Z and (x2,...,xn) ∈ W} = Z × W
is open. Moreover, if x = (x1,...,xn) ∈ Sc ∩ U, then x1 = g(x2,...,xn).
Thus, we have that
Sc ∩ U = {(g(x2,...,xn), x2,...,xn):(x2,...,xn) ∈ W}
= {u ∈ Rn : u = A
 w
g(w)

for some w ∈ W}
where A is the permutation of Rn given by
(y1,...,yn) → (yn, y1,...,yn−1).
In particular, it follows that Sc is an (n − 1)-dimensional manifold.
Exercise 1.90. Show that Sn−1 := {(x1,...,xn) ∈ Rn : x1
2 + ··· + xn2 = 1} is
an (n − 1)-dimensional manifold.
Exercise 1.91. For p ∈ S2 and p = ±e3 (the north and south poles) define
f(p) = v where v, p = 0, v, e3 = 1−z2, and p×e3, v = 0. Define f(±e3) = 0.
Prove that f is a smooth function f : S2 → R3.
Exercise 1.92. Show that the surface of revolution S obtained by rotating
the circle given by (x − 2)2 + y2 = 1 around the y-axis is a two-dimensional
manifold. This manifold is diffeomorphic to a (two-dimensional) torus T2 := T×T.
Construct a diffeomorphism.
Exercise 1.93. Suppose that J is an interval in R and γ : J → Rn is a smooth
function. The image C of γ is, by definition, a curve in Rn. Is C a one-dimensional
submanifold of Rn? Formulate and prove a theorem that gives sufficient conditions
for C to be a submanifold. Hint: Consider the function t → (t
2, t3) for t ∈ R and
the function t → (1 − t
2, t − t
3) for two different domains: t ∈ R and t ∈ (−∞, 1).
Can you imagine a situation where the image of a smooth curve is a dense subset
of a manifold with dimension n > 1? Hint: Consider curves mapping into the
two-dimensional torus.
Exercise 1.94. Show that the closed unit disk in R2 is not a manifold. Actually,
it is a manifold with boundary. How should this concept be formalized?50 1. Introduction to Ordinary Differential Equations
Exercise 1.95. Prove that for  > 0 there is a δ > 0 and a root r of the
polynomial x3 − ax + b such that |r| <  whenever |a − 1| + |b| < δ.
Exercise 1.96. Show that if f : Rn → Rn, A is a nonsingular (n × n)-matrix
and || is sufficiently small, then the differential equation ˙x = Ax + f(x) has a
rest point.
1.7.3 Tangent Spaces
We have used, informally, the following proposition: If S is a manifold in
Rn, and (x, f(x)) is tangent to S for each x ∈ S, then S is an invariant
manifold for the differential equation x˙ = f(x). To make this proposition
precise, we will give a definition of the concept of a tangent vector on a
manifold. This definition is the main topic of this section.
Let us begin by considering some examples where the proposition on
tangents and invariant manifolds can be applied.
The vector field on R3 associated with the system of differential equations
given by
x˙ = x(y + z),
y˙ = −y2 + x cos z,
z˙ = 2x + z − sin y (1.16)
is “tangent” to the linear two-dimensional submanifold S := {(x, y, z) :
x = 0} in the following sense: If (a, b, c) ∈ S, then the value of the vector
function
(x, y, z) → (x(y + z), −y2 + x cos z, 2x + z − sin y)
at (a, b, c) is a vector in the linear space S. Note that the vector assigned
by the vector field depends on the point in S. For this reason, we will view
the vector field as the function
(x, y, z) → (x, y, z, x(y + z), −y2 + x cos z, 2x + z − sin y)
where the first three-component functions specify the base point, and the
last three components, called the principal part, specify the vector that is
assigned at the base point.
To see that S is an invariant set, choose (0, b, c) ∈ S and consider the
initial value problem
y˙ = −y2, z˙ = z − sin y, y(0) = b, z(0) = c.
Note that if its solution is given by t → (y(t), z(t)), then the function
t → (0, y(t), z(t)) is the solution of system (1.16) starting at the point
(0, b, c). In particular, the orbit corresponding to this solution is contained
in S. Hence, S is an invariant set. In this example, the solution is not1.7 Manifolds 51
defined for all t ∈ (−∞,∞). (Why?) But, every solution that starts in S
stays in S, as required by Definition 1.61.
The following system of differential equations
x˙ = x2 − (x3 + y3 + z3)x,
y˙ = y2 − (x3 + y3 + z3)y,
z˙ = z2 − (x3 + y3 + z3)z (1.17)
has a nonlinear invariant submanifold; namely, the unit sphere
S2 := {(x, y, z) ∈ R3 : x2 + y2 + z2 = 1}.
This fact follows from our proposition, provided that the vector field asso￾ciated with the differential equation is everywhere tangent to the sphere.
To prove this requirement, recall from Euclidean geometry that a vector
in space is defined to be tangent to the sphere if it is orthogonal to the
normal line passing through the base point of the vector. Moreover, the
normal lines to the sphere are generated by the outer unit normal field
given by the restriction of the vector field
η(x, y, z) := (x, y, z, x, y, z)
to S2. By a simple computation, it is easy to check that the vector field
associated with the differential equation is everywhere orthogonal to η on
S2; that is, at each base point on S2 the corresponding principal parts of
the two vector fields are orthogonal, as required.
We will give a definition for tangent vectors on a manifold that generalizes
the definition given in Euclidean geometry for linear subspaces and spheres.
Let us suppose that S is a k-dimensional submanifold of Rn and (G, W)
is a submanifold coordinate chart at p ∈ S. Our objective is to define the
tangent space to S at p.
Definition 1.97. The tangent space to Rk with base point at w ∈ Rk is
the set
TwRk := {w} × Rk.
We have the following obvious proposition: If w ∈ Rk, then the tangent
space TwRk, with addition defined by
(w, ξ)+(w, ζ) := (w, ξ + ζ)
and scalar multiplication defined by
a(w, ξ) := (w, aξ),
is a vector space that is isomorphic to the vector space Rk.
To define the tangent space of the submanifold S at p ∈ S, denoted
TpS, we simply move the space TwRk, for an appropriate choice of w, to S52 1. Introduction to Ordinary Differential Equations
with a submanifold coordinate map. More precisely, suppose that (W, G)
is a submanifold chart at p. By Proposition 1.76, the coordinate map G is
invertible. If q = G−1(p), then define
TpS := {p}×{v ∈ Rn : v = DG(q)ξ, ξ ∈ Rk}. (1.18)
Note that the set
V := {v ∈ Rn : v = DG(q)ξ, ξ ∈ Rk}
is a k-dimensional subspace of Rn. If k = n, then DG(q) is the identity
map. If k<n, then DG(q) = AB where A is a nonsingular matrix and the
n × k block matrix
B :=  Ik
Dg(q)

is partitioned by rows with Ik the k × k identity matrix and g a map from
W to Rn−k. Thus, we see that V is just the image of a linear map from Rk
to Rn whose rank is k.
Proposition 1.98. If S is a manifold and p ∈ S, then the vector space
TpS is well-defined.
Proof. If K is a second submanifold coordinate map at p, say K : Z → S
with K(r) = p, then we must show that the tangent space defined using
K agrees with the tangent space defined using G. To prove this fact, let us
suppose that (p, v) ∈ TpS is given by
v = DG(q)ξ.
Using the chain rule, it follows that
v = d
dtG(q + tξ)



t=0
.
In other words, v is the directional derivative of G at q in the direction ξ.
To compute this derivative, we simply choose a curve, here t → q +tξ, that
passes through q with tangent vector ξ at time t = 0, move this curve to
the manifold by composing it with the function G, and then compute the
tangent to the image curve at time t = 0.
The curve t → K−1(G(q + tξ)) is in Z (at least this is true for |t| suffi￾ciently small). Thus, we have a vector α ∈ Rk given by
α := d
dtK−1(G(q + tξ))



t=0.
We claim that DK(r)α = v. In fact, we have
K−1(G(q)) = K−1(p) = r,1.7 Manifolds 53
and
DK(r)α = d
dtK(K−1(G(q + tξ)))



t=0
= d
dtG(q + tξ)



t=0
= v.
In particular, TpS, as originally defined, is a subset of the “tangent space
at p defined by K.” But this means that this subset, which is itself a k￾dimensional affine subspace (the translate of a subspace) of Rn, must be
equal to TpS, as required. 
Exercise 1.99. Prove: If p ∈ S2, then the tangent space TpS2, as in Defini￾tion 1.18, is equal to
{p}×{v ∈ R3 : p, v = 0}.
Definition 1.100. The tangent bundle T S of a manifold S is the union
of its tangent spaces; that is, T S := 
p∈S TpS. Also, for each p ∈ S, the
vector space TpS is called the fiber of the tangent bundle over the base
point p.
Definition 1.101. Suppose that S1 and S2 are manifolds, and F : S1 → S2
is a smooth function. The derivative, also called the tangent map, of F is
the function F∗ : T S1 → T S2 defined as follows: For each (p, v) ∈ TpS1, let
(W1, G1) be a submanifold chart at p in S1, (W2, G2) a submanifold chart at
F(p) in S2, (G−1
1 (p), ξ) the vector in TG−1
1 (p)W1 such that DG1(G−1
1 (p))ξ =
v, and (G−1
2 (F(p)), ζ) the vector in TG−1
2 (F (p))W2 such that
ζ = D(G−1
2 ◦ F ◦ G1)(G−1
1 (p))ξ.
The tangent vector F∗(p, v) in TF (p)S2 is defined by
F∗(p, v) = 
F(p), DG2(G−1
2 (F(p)))ζ

.
Although definition 1.101 seems to be rather complex, the idea is nat￾ural: we simply use the local representatives of the function F and the
definition of the tangent bundle to define the derivative F∗ as a map with
two component functions. The first component is F (to ensure that base
points map to base points) and the second component is defined by the
derivative of a local representative of F at each base point.
The following proposition is obvious from the definitions.
Proposition 1.102. The tangent map is well-defined and it is linear on
each fiber of the tangent bundle.54 1. Introduction to Ordinary Differential Equations
The derivative, or tangent map, of a function defined on a manifold has
a geometric interpretation that is the key to understanding its applica￾tions in the study of differential equations. We have already discussed this
interpretation several times for various special cases. But, because it is so
important, let us consider the geometric interpretation of the derivative in
the context of the notation introduced in Definition 1.101. If t → γ(t) is a
curve—a smooth function defined on an open set of R—with image in the
submanifold S1 ⊆ Rm such that γ(0) = p, and if
v = ˙γ(0) = d
dtγ(t)



t=0
,
then t → F(γ(t)) is a curve in the submanifold S2 ⊆ Rn such that
F(γ(0)) = F(p) and
F∗(p, v) = 
F(p), d
dtF(γ(t))



t=0

.
We simply find a curve that is tangent to the vector v at p and move the
curve to the image of the function F to obtain a curve in the range. The
tangent vector to the new curve at F(p) is the image of the tangent map.
Proposition 1.103. A submanifold S of Rn is an invariant manifold for
the ordinary differential equation x˙ = f(x), x ∈ Rn if and only if
(x, f(x)) ∈ TxS
for each x ∈ S. If, in addition, S is compact, then each orbit on S is defined
for all t ∈ R.
Proof. Suppose that S is k-dimensional, p ∈ S, and (W, G) is a subman￾ifold chart for S at p. The idea of the proof is to change coordinates to
obtain an ordinary differential equation on W.
Recall that the submanifold coordinate map G is invertible and G−1 is
the restriction of a linear map defined on Rn. In particular, we have that
w ≡ G−1(G(w)) for w ∈ W. If we differentiate both sides of this equation
and use the chain rule, then we obtain the relation
I = DG−1(G(w))DG(w) (1.19)
where I denotes the identity transformation of Rn. In particular, for each
w ∈ W, we have that DG−1(G(w)) is the inverse of the linear transforma￾tion DG(w).
Under the hypothesis, we have that (x, f(x)) ∈ TxS for each x ∈ S.
Hence, the vector f(G(w)) is in the image of DG(w) for each w ∈ W.
Thus, it follows that
(w, DG−1(G(w))f(G(w))) ∈ TwRk,1.7 Manifolds 55
and, as a result, the map
w → (w, DG−1(G(w))f(G(w)))
defines a vector field on W ⊆ Rk. The associated differential equation on
W is given by
w˙ = DG−1(G(w))f(G(w)). (1.20)
Suppose that G(q) = p, and consider the initial value problem on W
given by the differential equation (1.20) together with the initial condition
w(0) = q. By the existence theorem, this initial value problem has a unique
solution t → ω(t) that is defined on an open interval containing t = 0.
Define φ(t) = G(ω(t)). We have that φ(0) = p and, using equation (1.19),
that
dφ
dt (t) = DG(ω(t)) ˙ω(t)
= DG(ω(t)) · DG−1(G(ω(t)))f(G(ω(t)))
= f(φ(t)).
Thus, t → φ(t) is the solution of ˙x = f(x) starting at p. Moreover, this
solution is in S because φ(t) = G(ω(t)). The solution remains in S as long
as it is defined within the submanifold chart. The same result is true for
every submanifold chart. Thus, the solution remains in S as long as it is
defined.
Suppose that S is compact and note that the solution just defined is
a solution of the differential equation ˙x = f(x) defined on Rn. By the
extension theorem, if a solution of ˙x = f(x) does not exist for all time,
for example, if it exists only for 0 ≤ t<β< ∞, then it approaches the
boundary of the domain of definition of f or it blows up to infinity as
t approaches β. As long as the solution stays in S, both possibilities are
excluded if S is compact. Since the manifold S is covered by coordinate
charts, the solution stays in S and it is defined for all time.
If S is invariant, p ∈ S and t → γ(t) is the solution of ˙x = f(x) with
γ(0) = p, then the curve t → G−1(γ(t)) in Rk has a tangent vector ξ at
t = 0 given by
ξ := d
dtG−1(γ(t))



t=0
.
As before, it is easy to see that DG(q)ξ = f(p). Thus, (p, f(p)) ∈ TpS, as
required. 
Exercise 1.104. Show that the function f(θ)=1 − λ sin θ defines a (smooth)
vector field on T1, but f(θ) = θ − λ sin θ does not.
Exercise 1.105. State and prove a proposition that is analogous to Proposi￾tion 1.103 for the case where the submanifold S is not compact.56 1. Introduction to Ordinary Differential Equations
Figure 1.15: The left panel depicts a heteroclinic saddle connection and a
locally supported perturbation. The right panel depicts the phase portrait
of the perturbed vector field.
Exercise 1.106. We have mentioned several times the interpretation of the
derivative of a function whereby a curve tangent to a given vector at a point is
moved by the function to obtain a new curve whose tangent vector is the direc￾tional derivative of the function applied to the original vector. This interpretation
can also be used to define the tangent space at a point on a manifold. In fact, let
us say that two curves t → γ(t) and t → ν(t), with image in the same manifold
S, are equivalent if γ(0) = ν(0) and ˙γ(0) = ˙ν(0). Prove that this is an equiv￾alence relation. A tangent vector at p ∈ S is defined to an equivalence class of
curves all with value p at t = 0. As a convenient notation, let us write [γ] for the
equivalence class containing the curve γ. The tangent space at p in S is defined
to be the set of all equivalence classes of curves that have value p at t = 0. Prove
that the tangent space at p defined in this manner can be given the structure of
a vector space and this vector space has the same dimension as the manifold S.
Also prove that this definition gives the same tangent space as defined in equa￾tion 1.18. Finally, for manifolds S1 and S2 and a function F : S1 → S2, prove
that the tangent map F∗ is given by F∗[γ]=[F ◦ γ].
Exercise 1.107. Let A be an invertible symmetric (n × n)-matrix. (a) Prove
that the set M := {x ∈ R2 : Ax, x = 1} is a submanifold of Rn. (b) Suppose
that x0 ∈ M. Describe the tangent space to M at x0. Hint: Apply Exercise 1.106.
Exercise 1.108. [General Linear Group] The general linear group GL(Rn) is
the set of all invertible real n × n-matrices where the group structure is given
by matrix multiplication (see also Exercise 2.63). (a) Prove that GL(Rn) is a
submanifold of Rn2
. Hint: Consider the determinant function. (b) Determine the
tangent space of GL(Rn) at its identity. Hint: Apply Exercise 1.106. (c) Prove
that the map GL(Rn)×GL(Rn) → GL(Rn) given by (A, B) → AB is smooth. (d)
Prove that the map GL(Rn) → GL(Rn) given by A → A−1 is smooth. Note: A
Lie group is a group that is also a smooth manifold such that the group operations
are smooth. The vector space TIGL(Rn) is called the Lie algebra of the Lie group
when endowed with the multiplication [A, B] = AB − BA.
Exercise 1.109. (a) Prove that the tangent bundle of the torus T2 is trivial;
that is, it can be viewed as TT2 = T2 × R2. (b) (This exercise requires some
knowledge of topology) Prove that the tangent bundle of S2 is not trivial.1.7 Manifolds 57
Exercise 1.110. Suppose that f : Rn → Rn is smooth and the differential
equation ˙x = f(x) has a first integral all of whose level sets are compact. Prove
that the corresponding flow is complete.
Exercise 1.111. Prove: The diagonal
{(x, y) ∈ Rn × Rn : x = y}
in Rn × Rn is an invariant set for the system
x˙ = f(x) + h(y − x), y˙ = f(y) + g(x − y)
where f, g, h : Rn → Rn and g(0) = h(0).
Exercise 1.112. [An Open Problem in Structural Stability] Let H(x, y, z) be
a homogeneous polynomial of degree n and η the outer unit normal on the unit
sphere S2 ⊂ R3. Show that the vector field XH = grad H −nHη is tangent to S2.
Call a rest point isolated if it is the unique rest point in some open set. Prove
that if n is fixed, then the number of isolated rest points of XH is uniformly
bounded over all homogeneous polynomials H of degree n. Suppose that n = 3,
the uniform bound for this case is B, and m is an integer such that 0 ≤ m ≤ B.
What is B? Is there some H such that XH has exactly m rest points? If not,
then for which m is there such an H? What if n > 3?
Note that the homogeneous polynomials of degree n form a finite-dimensional
vector space Hn. What is its dimension? Is it true that for an open and dense
subset of Hn the corresponding vector fields on S2 have only hyperbolic rest
points?
In general, if X is a vector field in some class of vector fields H, then X is
called structurally stable with respect to H if X is contained in some open subset
U ⊂ H such that the phase portrait of every vector field in U is the same; that is,
if Y is a vector field in U, then there is a homeomorphism of the phase space that
maps orbits of X to orbits of Y . Let us define Xn to be the set of all vector fields
on S2 of the form XH for some H ∈ Hn. It is an interesting unsolved problem to
determine the structurally stable vector fields in Xn with respect to Xn.
One of the key issues that must be resolved to determine the structural stability
of a vector field on a two-dimensional manifold is the existence of heteroclinic
orbits. A heteroclinic orbit is an orbit that is contained in the stable manifold
of a saddle point q and in the unstable manifold of a different saddle point p. If
p = q, such an orbit is called homoclinic. A basic fact from the theory of structural
stability is that if two saddle points are connected by a heteroclinic orbit, then
the local phase portrait near this orbit can be changed by an arbitrarily small
smooth perturbation. In effect, a perturbation can be chosen such that, in the
phase portrait of the perturbed vector field, the saddle connection is broken (see
Figure 1.15). Thus, in particular, a vector field with two saddle points connected
by a heteroclinic orbit is not structurally stable with respect to the class of all
smooth vector fields. Prove that a vector field XH in Xn cannot have a homoclinic
orbit. Also, prove that XH cannot have a periodic orbit. Construct a homogeneous
polynomial H ∈ H3 such that XH has hyperbolic saddle points p and q connected
by a heteroclinic orbit.
Is every heteroclinic orbit of a vector field XH ∈ X3 an arc of a great circle? The
answer to this question is not known. But if it is true that all heteroclinic orbits58 1. Introduction to Ordinary Differential Equations
S
Dg(g−1(y))f(g−1(y))
g−1(y)
y
f(g−1(y))
Dg
g
M
Figure 1.16: The “push forward” of a vector field f by a diffeomorphism
g : S → M.
are arcs of great circles, then the structurally stable vector fields, with respect to
the class X3, are exactly those vector fields with all their rest points hyperbolic
and with no heteroclinic orbits. Moreover, this set is open and dense in Xn. A
proof of these facts requires some work. But the main idea is clear: if XH has
a heteroclinic orbit that is an arc of a great circle, then there is a homogeneous
polynomial K of degree n = 3 such that the perturbed vector field XH+K has
no heteroclinic orbits for || sufficiently small. In fact, K can be chosen to be of
the form
K(x, y, z)=(ax + by + cz)(x2 + y2 + z2
)
for suitable constants a, b, and c. (Why?) Of course, the conjecture that hetero￾clinic orbits of vector fields in H3 lie on great circles is just one approach to the
structural stability question for X3. Can you find another approach?
There is an extensive and far-reaching literature on the subject of structural
stability (see, for example, [216] and [230]).
1.7.4 Change of Coordinates
The proof of Proposition 1.103 contains an important computation that is
useful in many other contexts; namely, the formula for changing coordinates
in an autonomous differential equation. To reiterate this result, suppose
that we have a differential equation ˙x = f(x) where x ∈ Rn, and S ⊆ Rn is
an invariant k-dimensional submanifold. If g is a diffeomorphism from S to
some k-dimensional submanifold M ⊆ Rm, then the ordinary differential
equation (or, more precisely, the vector field associated with the differential
equation) can be “pushed forward” to M. In fact, if g : S → M is the
diffeomorphism, then
y˙ = Dg(g−1(y))f(g−1(y)) (1.21)
is a differential equation on M. Since g is a diffeomorphism, the new differ￾ential equation is the same as the original one up to a change of coordinates
as schematically depicted in Figure 1.16.1.7 Manifolds 59
Example 1.113. Consider ˙x = x − x2, x ∈ R. Let S = {x ∈ R : x > 0},
M = S, and let g : S → M denote the diffeomorphism defined by g(x) =
1/x. Here, g−1(y)=1/y and
y˙ = Dg(g−1(y))f(g−1(y))
= −
1
y
−21
y − 1
y2

= −y + 1.
The diffeomorphism g defines the change of coordinates y = 1/x used to
solve this special form of Bernoulli’s equation; it is encountered in elemen￾tary courses on differential equations.
Exercise 1.114. According to the Hartman–Grobman theorem 1.40, there is a
homeomorphism (defined on some open neighborhood of the origin) that maps
orbits of ˙y = y to orbits of ˙x = x − x2. In this case, the result is trivial; the
homeomorphism h given by h(y) = y satisfies the requirement. For one- and
two-dimensional systems (which are at least twice continuously differentiable) a
stronger result is true: There is a diffeomorphism h defined on a neighborhood
of the origin with h(0) = 0 such that h transforms the linear system into the
nonlinear system. Find an explicit formula for h and describe its domain.
Exercise 1.115. [Bernoulli’s Equation] Show that the differential equation
x˙ = g(t)x − h(t)xn
is transformed to a linear differential equation by the change of coordinates y =
1/xn−1.
Coordinate transformations are very useful in the study of differential
equations. New coordinates can reveal unexpected features. As a dramatic
example of this phenomenon, we will show that all autonomous differential
equations are the same, up to a smooth change of coordinates, near each
of their regular points. Here, a regular point of ˙x = f(x) is a point p ∈ Rn,
such that f(p) = 0. The following precise statement of this fact, which is
depicted in Figure 1.17, is called the rectification lemma, the straightening
out theorem, or the flow box theorem.
Lemma 1.116 (Rectification Lemma). Suppose that x˙ = f(x), x ∈ Rn.
If p ∈ Rn and f(p) = 0, then there are open sets U, V in Rn with p ∈ U,
and a diffeomorphism g : U → V such that the differential equation in the
new coordinates, that is, the differential equation
y˙ = Dg(g−1(y))f(g−1(y)),
is given by ( ˙y1,..., y˙n) = (1, 0, 0,..., 0).60 1. Introduction to Ordinary Differential Equations
g
U
x y
V
Figure 1.17: The flow of a differential equation is rectified by a change of
coordinates g : U → V .
Proof. The idea of the proof is to “rectify” at one point, and then to
extend the rectification to a neighborhood of this point.
Let e1,..., en denote the usual basis of Rn. There is an invertible (affine)
map H1 : Rn → Rn such that H1(p) = 0 and DH1(p)f(p) = e1. (Why?)
Here, an affine map is just the composition of a linear map and a transla￾tion. Let us also note that e1 is the transpose of the vector (1, 0, 0,..., 0) ∈
Rn. If the formula (1.21) is used with g = H1, then the differential equation
x˙ = f(x) is transformed to the differential equation denoted by ˙z = f1(z)
where f1(0) = e1. Thus, we have “rectified” the original differential equa￾tion at the single point p.
Let φt denote the flow of ˙z = f1(z), define H2 : Rn → Rn by
(s, y2,...,yn) → φs(0, y2,...,yn),
and note that H2(0) = 0. The action of the derivative of H2 at the origin
on the standard basis vectors is
DH2(0,..., 0)e1 = d
dtH2(t, 0,..., 0)



t=0
= d
dtφt(0,..., 0)



t=0
= e1,
and, for j = 2,...,n,
DH2(0,..., 0)ej = d
dtH2(tej )



t=0
= d
dttej



t=0
= ej .
In particular, DH2(0) is the identity, an invertible linear transformation of
Rn.
To complete the proof we will use the inverse function theorem.
Theorem 1.117 (Inverse Function Theorem). If F : Rn → Rn is a
smooth function, F(p) = q, and DF(p) is an invertible linear transforma￾tion of Rn, then there exist two open sets U and V in Rn with (p, q) ∈ U×V ,1.7 Manifolds 61
together with a smooth function G : V → U, such that G(q) = p and
G = F −1; that is, F ◦ G : V → V and G ◦ F : U → U are identity
functions.
Proof. Consider the function H : Rn × Rn → Rn given by H(x, y) =
F(x) − y. Note that H(p, q) = 0 and that Hx(p, q) = DF(p) is invertible.
By the implicit function theorem, there are open balls U˜ and V contained in
Rn, and a smooth function G : V → U such that (p, q) ∈ U × V , G(q) = p,
and F(G(y)) = y for all y ∈ V . In particular, the function F ◦ G : V → V
is the identity.
Because F is continuous, the set U := F −1(V ) ∩ U˜ is an open subset
of U with p ∈ U and F(U) ⊂ V . If x ∈ U, then (x, F(x)) ∈ U × V and
H(x, F(x)) = 0. Thus, by the uniqueness of the implicit solution (as stated
in the implicit function theorem), G(F(x)) = x for all x ∈ U. In other
words G ◦ F : U → U is the identity function. 
By the inverse function theorem, there are two neighborhoods U and V of
the origin such that H2 : U → V is a diffeomorphism. The new coordinate,
denoted y, on U is related to the old coordinate, denoted z, on V by the
relation y = H−1
2 (z). The differential equation in the new coordinates has
the form
y˙ = (DH2(y))−1f1(H2(y)) := f2(y).
Equivalently, at each point y ∈ U, we have f1(H2(y)) = DH2(y)f2(y).
Suppose that y = (s, y2,...,yn) and consider the tangent vector
(y, e1) ∈ TyRn.
Also, note that (y, e1) is tangent to the curve γ(t)=(s + t, y2,...,yn) in
Rn at t = 0 and
DH2(y)e1 = d
dtH2(γ(t))



t=0 = d
dtφt(φs(0, y2,...,yn))



t=0
= f1(H2(s, y2,...,yn)) = f1(H2(y)).
Because DH2(y) is invertible, it follows that f2(y) = e1.
The map g := H−1
2 ◦ H1 gives the required change of coordinates. 
The idea that a change of coordinates may simplify a given problem is a
far-reaching idea in many areas of mathematics; it certainly plays a central
role in the study of differential equations.
Exercise 1.118. Show that the implicit function theorem is a corollary of the
inverse function theorem.
Exercise 1.119. Suppose that f : Rn → Rn is smooth. Prove that if || is
sufficiently small, then the function F : Rn → Rn given by F(x) := x + f(x) is
invertible in a neighborhood of the origin. Also, determine DF −1(0).62 1. Introduction to Ordinary Differential Equations
Exercise 1.120. [Newton’s Method] Recall Newton’s method: Suppose that
f : Rn → Rn is twice continuously differentiable and f(r) = 0. We have f(r) ≈
f(x) + Df(x)(x − r) (see Theorem 1.252) and 0 ≈ f(x) + Df(x)(x − r). Solve for
r to obtain r ≈ x − Df(x)
−1f(x). Finally turn this into an iterative procedure
to approximate r; that is, xn+1 = xn − Df(xn)
−1f(xn). Note: To implement
this procedure (on a computer) it is usually better to solve for w in the equation
Df(xn)w = −f(xn) and then put xn+1 = xn + w (Why?). (a) Is the function
F(x) := x − Df(x)
−1f(x) invertible near x = r? (b) Prove that if Df(r) is
invertible and |x0 − r| is sufficiently small, then there is a constant K > 0 such
that |xn+1 − r| ≤ K|xn − r|
2 and limn→∞ xn = r. (c) A sequence {xn}∞n=0
converges linearly to r if there is a constant λ > 0 (the asymptotic error) such
that limn→∞ |xn+1−r|/|xn−r| = λ; it converges quadratically if limn→∞ |xn+1−
r|/|xn − r|
2 = λ. Show by discussing an explicit example, that quadratically
convergent sequences converge much faster than linearly convergent sequences.
(d) Compare the rates of convergence, to the positive zero of the function f(x) =
(x2 − 2)/4, of Newton’s method and the iterative scheme xn+1 = xn − f(xn). (e)
The solution of the initial value problem
¨θ + sin θ = 0, θ(0) = π/4, ˙
θ(0) = 0
is periodic. Approximate the period (correct to three decimal places) using New￾ton’s method.
Exercise 1.121. [Flow Box with Section] Prove the following modification of
the rectification lemma. Suppose that x˙ = f(x), x ∈ R2. If p ∈ R2, the vector f(p)
is not zero, and there is a curve Σ in R2 such that p ∈ Σ and f(p) is not tangent
to Σ, then there are open sets U, V in R2 with p ∈ U and a diffeomorphism
g : U → V such that the differential equation in the new coordinates, that is, the
differential equation
y˙ = Dg(g−1
(y))f(g−1
(y))
is given by ( ˙y1, y˙2) = (1, 0). Moreover, the image of Σ ∩ U under g is the line
segment {(y1, y2) ∈ V : y1 = 0}. Generalize the result to differential equations on
Rn.
Exercise 1.122. Prove that the function given by
(x, y) → x2 + 2y + 1
(x2 + y + 1)2
is constant on the trajectories of the differential equation
x˙ = −y, y˙ = x + 3xy + x3
.
Show that the function
(x, y) →
 x
x2 + y + 1 , x2 + y
x2 + y + 1

is birational—that is, the function and its inverse are both defined by rational
functions. Finally, show that the change of coordinates given by this birational
map linearizes the differential equation (see [222]).1.7 Manifolds 63
Exercise 1.123. [Wa˙zewski’s Equation] Suppose that Ω ⊆ Rn is open and
f : Ω → Rn is a smooth map. (a) Prove that if t → x(t) is a solution of Wa˙zewski’s
equation ˙x = Df(x)
−1v, where v ∈ Rn, then f(x(t)) = f(x(0)) + tv. (b) Prove
a similar formula for the differential equation ˙x = (Df(x))−1(f(x) − f(v)). (c)
Suppose that 0 ∈ Ω, f(0) = 0, Df(x) is invertible for every x ∈ Ω, and the initial
value problem ˙x = (Df(x))−1v, x(0) = 0 has a solution, which exists at least
for |t| ≤ 1, for every choice of v ∈ Rn. Prove that f is invertible with a smooth
inverse. Hint: Define t → x(t, ξ) to be the solution of the initial value problem
and g(ξ) = x(1, ξ). Note: Wa˙zewski’s equation can be used to prove more general
results on the invertibility of smooth maps (see, for example, [193] and [236]).
1.7.5 Reparametrization of Time
Suppose that U is an open set in Rn, f : U → Rn is a smooth function, and
g : U → R is a positive smooth function. What is the relationship among
the solutions of the differential equations
x˙ = f(x), (1.22)
x˙ = g(x)f(x)? (1.23)
The vector fields defined by f and gf have the same direction at each
point in U, only their lengths are different. Thus, by our geometric inter￾pretation of autonomous differential equations, it is intuitively clear that
the differential equations (1.22) and (1.23) have the same phase portraits
in U. This fact is a corollary of the next proposition.
Proposition 1.124. If J ⊂ R is an open interval containing the origin
and γ : J → Rn is a solution of the differential equation (1.22) with γ(0) =
x0 ∈ U, then the function B : J → R given by
B(t) =  t
0
1
g(γ(s)) ds
is invertible on its range K ⊆ R. If ρ : K → J is the inverse of B, then
the identity
ρ
(t) = g(γ(ρ(t)))
holds for all t ∈ K, and the function σ : K → Rn given by σ(t) = γ(ρ(t)) is
the solution of the differential equation (1.23) with initial condition σ(0) =
x0.
Proof. The function s → 1/g(γ(s)) is continuous on J. So B is defined
on J and its derivative is everywhere positive. Thus, B is invertible on its
range. If ρ is its inverse, then
ρ
(t) = 1
B
(ρ(t)) = g(γ(ρ(t))),
and
σ
(t) = ρ
(t)γ
(ρ(t)) = g(γ(ρ(t)))f(γ(ρ(t))) = g(σ(t))f(σ(t)). 64 1. Introduction to Ordinary Differential Equations
Exercise 1.125. Use Proposition 1.124 to prove that differential equations (1.22)
and (1.23) have the same phase portrait in U.
Because the function ρ in Proposition 1.124 is the inverse of B, we have
the formula
t =
 ρ
0
1
g(γ(s)) ds.
Thus, if we view ρ as a new time-like variable (that is, a variable that
increases with time), then we have
dt
dρ = 1
g(γ(ρ)),
and therefore the differential equation (1.23), with the change of indepen￾dent variable from t to ρ, is given by
dx
dρ = dx
dt
dt
dρ = f(x).
In particular, this is just differential equation (1.22) with the independent
variable renamed.
The next proposition expresses the same results in the language of flows.
Proposition 1.126. Suppose that f : Rn → Rn, g : Rn → R is a positive
function, and φ is the flow of the differential equation x˙ = f(x). If the
family of solutions of the family of initial value problems
y˙ = g(φ(t, ξ)), y(0) = 0,
with parameter ξ ∈ Rn, is given by ρ : R × Rn → R, then ψ, defined by
ψ(t, ξ) = φ(ρ(t, ξ), ξ), is the flow of the differential equation x˙ = g(x)f(x).
Proof. By definition ψ(0, ξ) ≡ ξ, and by the chain rule
d
dtψ(t, ξ) = g(ψ(t, ξ))f(ψ(t, ξ)).

As a convenient expression, we say that the differential equation (1.22)
is obtained from the differential equation (1.23) by a reparametrization of
time.
In the most important special cases the function g is constant. If its con￾stant value is c > 0, then the reparametrization of the differential equation
x˙ = cf(x) by ρ = ct results in the new differential equation
dx
dρ = f(x).
Reparametrization in these cases is also called rescaling.1.7 Manifolds 65
Note that rescaling, as in the last paragraph, of the differential equation
x˙ = cf(x) produces a differential equation in which the parameter c has
been eliminated. This idea is often used to simplify differential equations.
Also, the same rescaling is used in applied mathematics to render the inde￾pendent variable dimensionless. For example, if the original time variable t
is measured in seconds, and the scale factor c has the units of 1/sec, then
the new variable ρ is dimensionless.
The next proposition is a special case of the following claim: Every
autonomous differential equation has a complete reparametrization (see
Exercise 1.132).
Proposition 1.127. If the differential equation x˙ = f(x) is defined on Rn,
then the differential equation
x˙ = 1
1 + |f(x)|
2 f(x) (1.24)
is defined on Rn and its flow is complete.
Proof. The vector field corresponding to the differential equation (1.24) is
smoothly defined on all of Rn. If σ is one of its solutions with initial value
σ(0) = x0 and t is in the domain of σ, then, by integration with respect to
the independent variable, we have that
σ(t) − σ(0) =  t
0
1
1 + |f(σ(s))|
2 f(σ(s)) ds.
Note that the integrand has norm less than one and use the triangle inequal￾ity (taking into account the possibility that t might be negative) to obtain
the following estimate:
|σ(t)|≤|x0| + |t|.
In particular, the solution does not blow up in finite time. By the extension
theorem, the solution is complete. 
Exercise 1.128. Consider the scalar differential equations ˙x = x and ˙x = x2
for x > 0. Find explicit expressions for the corresponding flows φ and ψ and
for the reparametrization function ρ, as in Proposition 1.126, so that ψ(t, ξ) =
φ(ρ(t, ξ), ξ). Show that no such relation holds if the restriction x > 0 is removed.
Does it matter that ψ is not complete? Consider instead the differential equations
x˙ = x and ˙x = x3.
Exercise 1.129. Consider the function g : (0, ∞) → R given by g(x) = x−n
for a fixed positive integer n. Construct the flow φt of the differential equation
x˙ = −x and the flow ψt of ˙x = −g(x)x on (0, ∞), and find the explicit expression
for the reparametrization function ρ such that ψt(x) = φρ(t)(x) (see [61]).66 1. Introduction to Ordinary Differential Equations
Exercise 1.130. Suppose that n is an integer. Solve the initial value problem
x˙ = y(x + y)
n, y˙ = x(x + y)
n, x(0) = 1, y(0) = 0.
Is the solution complete?
Exercise 1.131. Suppose that the solution γ of the differential equation ˙x =
f(x) is reparametrized by arc length; that is, in the new parametrization the
velocity vector at each point of the solution curve has unit length. Find an implicit
formula for the reparametrization ρ, and prove that if t > 0, then
|γ(ρ(t))|≤|γ(0)| + t.
Exercise 1.132. Suppose that ˙x = f(x) is a differential equation defined on
an open subset U of Rn. Show that the differential equation has a complete
reparametrization.
Exercise 1.133. [Solute Transport] A model for transport of a solute (moles of
salt) and solvent (volume of water) across a permeable membrane has the form
W˙ = A(k − M
W ), M˙ = B(k − M
W )
where k is a parameter representing the bulk solute concentration and A and
B are parameters that represent the permeability of the membrane (see [47]).
(a) The water volume W is a positive quantity. Show that the system can be
made linear by a reparametrization. (b) Determine the transformation between
solutions of the linear and nonlinear systems.
1.7.6 Polar Coordinates
There are several special “coordinate systems” that are important in the
analysis of differential equations, especially, polar coordinates, cylindrical
coordinates, and spherical coordinates. In this section, we will consider the
meaning of these coordinates in the language of differentiable manifolds,
and we will also explore a few applications, especially blowup of a rest point
and compactification at infinity. But, the main purpose of this section is to
provide a deeper understanding and appreciation for the manifold concept
in the context of the study of differential equations.
What are polar coordinates?
Perhaps the best way to understand the meaning of polar coordinates
is to recall the “angular wrapping function” definition of angular measure
from elementary trigonometry. We have proved that the unit circle T is a
one-dimensional manifold. The wrapping function P : R → T is given by
P(θ) = (cos θ,sin θ).
Clearly, P is smooth and surjective. But P is not injective. In particular,
P is not a diffeomorphism (see Exercise 1.134).
The function P is a covering map; that is, each point of T is contained
in an open set on which a local inverse of P is defined. Each such open set,1.7 Manifolds 67
π W
V
x
P
Ψ
r
θ y
Figure 1.18: The polar wrapping function P : R2 → R2 and a polar coor￾dinate system Ψ : V → W on the upper half-plane.
together with its corresponding inverse function, is a coordinate system, as
defined in Definition 1.83, that we will call an angular coordinate system.
The image of a point of T under an angular coordinate map is called its
angular coordinate, or simply its angle, relative to the angular coordinate
system. For example, the pair (V, Ψ) where
V := {(x, y) ∈ T : x > 0}
and Ψ : V → (−π
2 , π
2 ) is given by Ψ(x, y) = arctan(y/x) is an angular coor￾dinate system. The number θ = Ψ(x, y) is the angle assigned to (x, y) in this
angular coordinate system. Of course, there are infinitely many different
angular coordinate systems defined on the same open set V . For example,
the function given by (x, y) → 4π + arctan(y/x) on V also determines an
angular coordinate system on T for which the corresponding angles belong
to the interval ( 7π
2 , 9π
2 ).
As we have just seen, each point of T is assigned infinitely many angles.
But all angular coordinate systems are compatible in the sense that they all
determine local inverses of the wrapping function P. The totality of these
charts might be called the angular coordinates on T.
Exercise 1.134. Prove that T is not diffeomorphic to R.
Exercise 1.135. Find a collection of angular coordinate systems that cover the
unit circle.
Let us next consider coordinates on the plane compatible with the polar
wrapping function P : R2 → R2 given by
P(r, θ)=(r cos θ, r sin θ).
The function P is a smooth surjective map that is not injective. Thus, P
is not a diffeomorphism. It is also not a covering map. For example, P has
no local inverse at the origin of its range. But, P does have a local inverse68 1. Introduction to Ordinary Differential Equations
at every point of the punctured plane (that is, the set R2 with the origin
removed). Thus, in analogy with the definition of the angular coordinate
on T, we have the following definition of polar coordinates.
Definition 1.136. A polar coordinate system on the punctured plane is
a coordinate system (V, Ψ) where V ⊂ R2 \ {(0, 0)}, the range W of the
coordinate map Ψ is contained in R2, and Ψ : V → W is the inverse of
the polar wrapping function P restricted to the set W. The collection of
all polar coordinate systems is called polar coordinates.
If
V := {(x, y) ∈ R2 : y > 0}, W := {(r, θ) ∈ R2 : r > 0, 0 <θ<π},
and Ψ : V → W is given by
Ψ(x, y) = x2 + y2 , π
2 − arctan x
y

,
then (V, Ψ) is a polar coordinate system on the punctured plane (see Fig￾ure 1.18). By convention, the two slot functions defined by Ψ are named as
follows:
Ψ(x, y)=(r(x, y), θ(x, y)),
and the point (x, y) is said to have polar coordinates r = r(x, y) and θ =
θ(x, y).
The definition of cylindrical and spherical coordinates is similar to Defi￾nition 1.136 where the respective wrapping functions are given by
(r, θ, z) → (r cos θ, r sin θ, z),
(ρ, φ, θ) → (ρ sin φ cos θ, ρ sin φ sin θ, ρ cos φ). (1.25)
To obtain covering maps, the z-axis must be removed in the target plane in
both cases. Moreover, for spherical coordinates, the second variable must
be restricted so that 0 ≤ φ ≤ π.
Let us now consider a differential equation ˙u = f(u) defined on R2 with
the usual Cartesian coordinates u := (x, y). If (V, Ψ) is a polar coordinate
system on the punctured plane such that Ψ : V → W, then we can push
forward the vector field f to the open set W by the general change of
variables formula ˙y = Dg(g−1(y))f(g−1(y)) (see page 58). The new differ￾ential equation corresponding to the push forward of f is then said to be
expressed in polar coordinates.
Specifically, the (principal part of the) new vector field is given by
F(r, θ) = DΨ(P(r, θ))f(P(r, θ)).
Of course, because the expressions for the components of the Jacobian
matrix corresponding to the derivative DΨ are usually more complex than1.7 Manifolds 69
those for the matrix DP, the change to polar coordinates is usually easier
to compute if we use the chain rule to obtain the identity
DΨ(P(r, θ)) = [DP(r, θ)]−1 = 1
r
 r cos θ r sin θ
− sin θ cos θ

and recast the formula for F in the form
F(r, θ)=[DP(r, θ)]−1f(P(r, θ)).
In components, if f(x, y)=(f1(x, y), f2(x, y)), then
F(r, θ) =  cos θf1(r cos θ, r sin θ) + sin θf2(r cos θ, r sin θ)
−sin θ
r f1(r cos θ, r sin θ) + cos θ
r f2(r cos θ, r sin θ)

. (1.26)
Note that the vector field F obtained by the push forward of f in for￾mula (1.26) does not depend on the choice of the polar coordinate system;
that is, it does not depend on the choice of the local inverse Ψ. Thus, the
vector field F is globally defined except on the line in the coordinate plane
given by {(r, θ) ∈ R2 : r = 0}. In general this is the best that we can do
because the second component of the vector field F has a singularity at
r = 0.
In practice, perhaps the simplest way to change to polar coordinates is
to first differentiate in the formulas r2 = x2 + y2 and θ = arctan(y/x) to
obtain the components of F in the form
rr˙ = xx˙ + yy˙ = xf1(x, y) + yf2(x, y),
r2 ˙
θ = xy˙ − yx˙ = xf2(x, y) − yf1(x, y),
and then substitute for x and y using the identities x = r cos θ and
y = r sin θ.
Exercise 1.137. Change the differential equations to polar coordinates:
1. ˙x = −y + x(1 − x2 − y2), ˙y = x + y(1 − x2 − y2).
2. ˙x = 1 − y2, ˙y = x.
3. ˙x = (x2 + y2)y, ˙y = −(x2 + y2)x.
4. ˙x = y, ˙y = −x − (x2 − 1)y.
Exercise 1.138. Change the differential equations to Cartesian coordinates:
1. ˙r = ar, ˙
θ = b.
2. ˙r = r3 sin θ cos3 θ, ˙
θ = −1 + r2 cos4 θ.
3. ˙r = −r3, ˙
θ = 1.
Exercise 1.139. Show that the system
x˙ = y + xy, y˙ = −x − x2
has a center at the origin.70 1. Introduction to Ordinary Differential Equations
r
x
y
R
θ
Q
θ
r
Ψ
Figure 1.19: The polar wrapping function factored through the phase cylin￾der.
Exercise 1.140. Show that the systems
x˙ = y + 2xy, y˙ = −x + xy
and
u˙ = v, v˙ = −u − 1
√5
(2u2 + 3uv − 2v2
)
are the same up to a change of variables. Hint: The transformation is a rotation.
Exercise 1.141. (a) Prove that if a > 0, then every solution of the second￾order differential equation ¨x + x2x˙ + ax = 0 converges to zero as time increases
without bound. (b) What is a general statement about solutions for the case
a = 0?
A change to polar coordinates in a planar differential equation introduces
a singularity on the line {(r, θ) ∈ R2 : r = 0}. The next proposition states
that if the differential equation has a rest point at the origin, then this
singularity is removable (see [86]).
Proposition 1.142. If u˙ = f(u) is a differential equation on the plane and
f(0) = 0, then the corresponding differential equation in polar coordinates
has a removable singularity. Also, if f is class Cr, then the desingularized
vector field in polar coordinates is in class Cr−1.
Proof. Apply Taylor’s theorem to the Taylor expansions of the compo￾nents of the vector field f at the origin. 
Even if Proposition 1.142 applies, and we do obtain a smooth vector field
defined on the whole polar coordinate plane, the desingularized vector field
is not the push forward of the original vector field; that is, the desingularized1.7 Manifolds 71
vector field is not obtained merely by a change of coordinates. Remember
that there is no polar coordinate system at the origin of the Cartesian
plane. In fact, the desingularized vector field in polar coordinates is an
extension of the push forward of the original vector field to the singular
line {(r, θ) ∈ R2 : r = 0}.
It is evident from formula (1.26) that the desingularized vector field is
2π periodic in θ; that is, for all (r, θ) we have
F(r, θ + 2π) = F(r, θ).
In particular, the phase portrait of this vector field is periodic with period
2π. For this reason, let us change the point of view one last time and
consider the vector field to be defined on the phase cylinder; that is, on
T × R with θ the angular coordinate on T and r the Cartesian coordinate
on R.
The phase cylinder can be realized as a two-dimensional submanifold in
R3, for example, as the set
C := {(x, y, z) ∈ R3 : x2 + y2 = 1}.
For this realization, the map Q : R2 → C defined by Q(r, θ) = (cos θ,sin θ, r)
is a covering map. Here, R2 is viewed as the “polar coordinate plane.” Thus,
we can use the map Q to push forward the vector field F to the phase
cylinder (see Exercise 1.145). There is also a natural covering map R, from
the phase cylinder minus the set {(x, y, z) ∈ C : z = 0} onto the punctured
Cartesian plane, defined by
R(x, y, z)=(xz, yz). (1.27)
If the original vector field f vanishes at the origin, then it can be pushed
forward by Ψ to F on the polar plane, and F can be pushed forward by
Q to a vector field h on the phase cylinder. If finally, h is pushed forward
by R to the punctured Cartesian plane, then we recover the original vector
field f. In fact, by Exercise 1.145, the composition R ◦ Q ◦ Ψ, depicted in
Figure 1.19, is the identity map.
Even though the phase cylinder can be realized as a manifold in R3, most
often the best way to consider a vector field in polar coordinates is to view
the polar coordinates abstractly as coordinates on the cylinder; that is, to
view θ as the angular variable on T and r as the Cartesian coordinate on
R.
Exercise 1.143. Prove the following statements. If F is the push forward to
the polar coordinate plane of a smooth vector field on the Cartesian plane, then
F has the following symmetry:
F(−r, θ + π) = −F(r, θ).
If F can be desingularized, then its desingularization retains the symmetry.72 1. Introduction to Ordinary Differential Equations
Exercise 1.144. Prove that the cylinder {(x, y, z) ∈ R3 : x2 + y2 = 1} is a
two-dimensional submanifold of R3.
Exercise 1.145. Suppose that F is the push forward to the polar coordinate
plane of a smooth vector field on the Cartesian plane that vanishes at the origin.
Find the components of the push forward h of F to the phase cylinder realized
as a submanifold in R3. Show that the push forward of h to the Cartesian plane
via the natural map (1.27) is the original vector field f.
Exercise 1.146. [Hamiltonians and Gradients on Manifolds] Let
G : R3 → R
be a smooth map and consider its gradient. We have tacitly assumed that the
definition of the gradient in R3 is
grad G =
∂G
∂x ,
∂G
∂y ,
∂G
∂z

. (1.28)
But this expression for the gradient of a function is correct only on Euclidean
space, that is, R3 together with the usual inner product. The definition of the gra￾dient for a scalar function defined on a manifold, to be given below, is coordinate￾free.
Recall that if G : Rn → R, then its derivative can be viewed as a function from
the tangent bundle TRn to TR. If TR is identified with R, then on each tangent
space of Rn, the derivative of G is a linear functional. In fact, if we work locally
at p ∈ Rn, then DG(p) is a map from the vector space Rn to R. Moreover, the
assignment of the linear functional corresponding to the derivative of G at each
point of the manifold varies smoothly with the base point. From this point of
view, the derivative of the scalar-valued function G is a differential 1-form on Rn
that we will denote by dG. Finally, the derivative of G may be interpreted as the
differential of G. In this interpretation, if V is a tangent vector at p ∈ Rn and γ
is a curve such that γ(0) = p and ˙γ(0) = V , then
dG(V ) = d
dsG(γ(s))



s=0
.
If G is a scalar function defined on a manifold, then all of our interpretations for
the derivative of G are still viable.
The definition of the gradient requires a new concept: A Riemannian metric on
a manifold is a smooth assignment of an inner product in each tangent space of
the manifold. Of course, the usual inner product assigned in each tangent space of
Rn is a Riemannian metric for Rn. Moreover, the manifold Rn together with this
Riemannian metric is called Euclidean space. Note that the Riemannian metric
can be used to define length. For example, the norm of a vector is the square
root of the inner product of the vector with itself. It follows that the shortest
distance between two points is a straight line. Thus, the geometry of Euclidean
space is Euclidean geometry, as it should be. These notions can be generalized.
For example, let γ be a curve in Euclidean space that connects the points p and
q; that is, γ : [a, b] → Rn such that γ(a) = p and γ(b) = q. The length of γ is
defined to be
 b
a
γ˙ (t), γ˙ (t) dt,1.7 Manifolds 73
where the angle brackets denote the usual inner product. The distance between
p and q is the infimum of the set of all lengths of curves joining these points. A
curve is called a geodesic joining p and q if its length equals the distance from
p to q. For instance, the curve γ(t) = tq + (1 − t)p is a geodesic. Of course, all
geodesics lie on straight lines (see Exercise 6.17). Similarly, suppose that g is a
Riemannian metric on a manifold M and p, q ∈ M. The length of a curve γ that
connects p and q is defined to be
 b
a

gγ(t)( ˙γ(t), γ˙ (t)) dt
where gr(v, w) denotes the inner product of the vectors (r, v) and (r, w) in TrM.
Geodesics play the role of lines in the “Riemannian geometry” defined on a man￾ifold by a Riemannian metric.
The gradient of G : M → R with respect to the Riemannian metric g is the
vector field, denoted by grad G, such that
dGp(V ) = gp(V, grad G) (1.29)
for each point p ∈ M and every tangent vector V ∈ TpM. The associated gradient
system on the manifold is the differential equation ˙p = grad G(p).
(a) Prove that the gradient vector field is uniquely defined.
(b) Prove that if the Riemannian metric g on R3 is the usual inner product at
each point of R3, then the invariant definition (1.29) of gradient agrees with the
Euclidean gradient.
Consider the upper half-plane of R2 with the Riemannian metric
g(x,y)(V,W) = y−2
V,W (1.30)
where the angle brackets denote the usual inner product. The upper half-plane
with the metric g is called the Poincar´e or Lobachevsky plane; its geodesics are
vertical lines and arcs of circles whose centers are on the x-axis. The geometry
is non-Euclidean; for example, if p is a point not on such a circle, then there
are infinitely many such circles passing through p that are parallel to (do not
intersect) the given circle (see Exercise 6.19).
(c) Determine the gradient of the function G(x, y) = x2 + y2 with respect to
the Riemannian metric (1.30) and draw the phase portrait of the corresponding
gradient system on the upper half-plane. Also, compare this phase portrait with
the phase portrait of the gradient system with respect to the usual metric on the
plane.
If S is a submanifold of Rn, then S inherits a Riemannian metric from the
usual inner product on Rn.
(d) Suppose that F : Rn → R. What is the relationship between the gradient of
F on Rn and the gradient of the function F restricted to S with respect to the
inherited Riemannian metric (see Exercise 1.112)?
Hamiltonian systems on manifolds are defined in essentially the same way as
gradient systems except that the Riemannian metric is replaced by a symplectic
form. Although these objects are best described and analyzed using the calculus
of differential forms (see [13], [103], and [239]), they are easy to define. Indeed,
a symplectic form on a manifold is a smooth assignment of a bilinear, skew￾symmetric, nondegenerate 2-form in each tangent space. A 2-form ω on a vector74 1. Introduction to Ordinary Differential Equations
space X is nondegenerate provided that y = 0 is the only element of X such that
ω(x, y) = 0 for all x ∈ X. Prove: If a manifold has a symplectic form, then the
dimension of the manifold is even.
Suppose that M is a manifold and ω is a symplectic form on M. The Hamilto￾nian vector field associated with a smooth scalar function H defined on M is the
unique vector field XH such that, for every point p ∈ M and all tangent vectors
V at p, the following identity holds:
dHp(V ) = ωp(XH, V ). (1.31)
(e) Let M := R2n, view R2n as Rn × Rn so that each tangent vector V on M is
decomposed as V = (V1, V2) with V1, V2 ∈ Rn, and define
ω(V,W) := (V1, V2)
 0 I
−I 0
 W1
W2

.
Show that ω is a symplectic form on M and Hamilton’s equations are produced
by the invariant definition (1.31) of the Hamiltonian vector field.
(f) Push forward the Euclidean gradient (1.28) of the function G : R3 → R to
the image of a cylindrical coordinate map, define
G(r, θ, z) = G(r cos θ, r sin θ, z),
and show that the push forward gives the result
grad G =
∂G
∂r , 1
r2
∂G
∂θ ,
∂G
∂z

. (1.32)
(In practice, the function G is usually again called G. These two functions are
local representations of the same function in two different coordinate systems.)
(g) Recall the formula for the gradient in cylindrical coordinates from vector
analysis; namely,
grad G = ∂G
∂r er +
1
r
∂G
∂θ eθ +
∂G
∂z ez. (1.33)
Show that the gradient vector fields (1.32) and (1.33) coincide.
(h) Express the usual inner product in cylindrical coordinates, and use the invari￾ant definition of the gradient to determine the gradient in cylindrical coordinates.
(i) Repeat part (h) for spherical coordinates.
Exercise 1.147. [Electrostatic Potential] Suppose that two point charges with
opposite signs, each with charge q, placed a units apart and located symmetri￾cally with respect to the origin on the z-axis in space, produce the electrostatic
potential
G0(x, y, z) = kq
(x2 + y2 + (z − a
2
)
2
)
−1/2 − (x2 + y2 + (z + a
2
)
2
)
−1/2
where k > 0 is a constant and q > 0. If we are interested only in the field far
from the charges, the “far field,” then a is relatively small and therefore the first
nonzero term of the Taylor series of the electrostatic potential with respect to a
at a = 0 gives a useful approximation of G0. This approximation, an example
of a “far-field approximation,” is called the dipole potential in Physics (see [101,
Vol. II, 6-1]). Show that the dipole potential is given by
G(x, y, z) = kqaz(x2 + y2 + z2
)
−3/2
.1.7 Manifolds 75
Figure 1.20: Phase portrait for the differential equation (1.35) on the upper
half of the phase cylinder and its “blowdown” to the Cartesian plane.
By definition, the electric field E produced by the dipole potential associated with
the two charges is E := − grad G. Draw the phase portrait of the differential
equation ˙u = E(u) whose orbits are the “dipole” lines of force. Discuss the
stability of all rest points. Hint: Choose a useful coordinate system that reduces
the problem to two dimensions.
Blow Up at a Rest Point
As an application of polar coordinates, let us determine the phase portrait
of the differential equation in the Cartesian plane given by
x˙ = x2 − 2xy, y˙ = y2 − 2xy (1.34)
(see [86]). This system has a unique rest point at the origin that is not
hyperbolic. In fact, the system matrix for the linearization at the origin
vanishes. Thus, linearization provides no information about the phase por￾trait of the system near the origin.
Because the polar coordinate representation of a plane vector field is
always singular at the origin, we might expect that the polar coordinate
representation of a planar vector field is not particularly useful to deter￾mine the phase portrait near the origin. But this is not the case. Often
polar coordinates are the best way to analyze the vector field near the ori￾gin. The reason is that the desingularized vector field in polar coordinates
is a smooth extension to the singular line represented as the equator of the
phase cylinder. All points on the equator are collapsed to the single rest
point at the origin in the Cartesian plane. Or, as we say, the equator is the76 1. Introduction to Ordinary Differential Equations
blowup of the rest point. This extension is valuable because the phase por￾trait of the vector field near the original rest point corresponds to the phase
portrait on the phase cylinder near the equatorial circle. Polar coordinates
and desingularization provide a mathematical microscope for viewing the
local behavior near the “Cartesian” rest point.
The desingularized polar coordinate representation of system (1.34) is
r˙ = r2(cos3 θ − 2 cos2 θ sin θ − 2 cos θ sin2 θ + sin3 θ),
˙
θ = 3r(cos θ sin2 θ − cos2 θ sin θ). (1.35)
For this particular example, both components of the vector field have r
as a common factor. From our discussion of reparametrization, we know
that the system with this factor removed has the same phase portrait as
the original differential equation in the portion of the phase cylinder where
r > 0. Of course, when we “blow down” to the Cartesian plane, the push
forward of the reparametrized vector field has the same phase portrait as
the original vector field in the punctured plane; exactly the set where the
original phase portrait is to be constructed.
Let us note that after division by r, the differential equation (1.35) has
several isolated rest points on the equator of the phase cylinder. In fact,
because this differential equation restricted to the equator is given by
˙
θ = 3 cos θ sin θ(sin θ − cos θ),
we see that it has six rest points with the following angular coordinates:
0, π
4
, π
2
, π,
5π
4 , 3π
2 .
The corresponding rest points for the reparametrized system are all hyper￾bolic. For example, the system matrix at the rest point (r, θ) = (0, π
4 ) is
1
√2
−1 0
0 3
.
It has the negative eigenvalue −1/
√2 in the positive direction of the Carte￾sian variable r on the cylinder and the positive eigenvalue 3/
√2 in the posi￾tive direction of the angular variable. This rest point is a hyperbolic saddle.
If each rest point on the equator is linearized in turn, the phase portrait
on the cylinder and the corresponding blowdown of the phase portrait on
the Cartesian plane are found to be as depicted in Figure 1.20. Hartman’s
theorem can be used to construct a proof of this fact.
The analysis of differential equation (1.34) is very instructive, but per￾haps somewhat misleading. Often, unlike this example, the blowup pro￾cedure produces a vector field on the phase cylinder where some or all
of the rest points are not hyperbolic. Of course, in these cases, we can
treat the polar coordinates near one of the nonhyperbolic rest points as1.7 Manifolds 77
Cartesian coordinates; we can translate the rest point to the origin; and
we can blow up again. If, after a finite number of such blowups, all rest
points of the resulting vector field are hyperbolic, then the local phase por￾trait of the original vector field at the original nonhyperbolic rest point can
be determined. For masterful treatments of this subject and much more,
see [21], [85], [86], and [246].
The idea of blowup and desingularization are far-reaching ideas in mathe￾matics. For example, these ideas seem to have originated in algebraic geom￾etry, where they play a fundamental role in understanding the structure of
algebraic varieties [32].
Compactification at Infinity
The orbits of a differential equation on Rn may be unbounded. One way
to obtain some information about the behavior of such solutions is to (try
to) compactify the Cartesian space, so that the vector field is extended
to a new manifold that contains the “points at infinity.” This idea, due
to Henri Poincar´e [208], has been most successful in the study of planar
systems given by polynomial vector fields, also called polynomial systems
(see [8, p. 219] and [114]). In this section we will give a brief description
of the compactification process for such planar systems. We will again use
the manifold concept and the idea of reparametrization.
Let us consider a plane vector field, which we will write in the form
x˙ = f(x, y), y˙ = g(x, y). (1.36)
To study its phase portrait “near” infinity, let us consider the unit sphere
S2; that is, the two-dimensional submanifold of R3 defined by
S2 := {(x, y, z) : x2 + y2 + z2 = 1},
and the tangent plane Π at its north pole; that is, the point with coordi￾nates (0, 0, 1). The push forward of system (1.36) to Π by the natural map
(x, y) → (x, y, 1) is
x˙ = f(x, y), y˙ = g(x, y), z˙ = 0. (1.37)
The idea is to “project” differential equation (1.37) to the unit sphere by
central projection; then the behavior of the system near infinity is the same
as the behavior of the projected system near the equator of the sphere.
Central projection is defined as follows: A point p ∈ Π is mapped to
the sphere by assigning the unique point on the sphere that lies on the line
segment from the origin in R3 to the point p. To avoid a vector field specified
by three components, we will study the projected vector field restricted to a
coordinate system on the sphere where the vector field is again planar. Also,78 1. Introduction to Ordinary Differential Equations
to obtain the desired compactification, we will choose local coordinates
defined in open sets that contain portions of the equator of the sphere.
The central projection map Q : Π → S2 is given by
Q(x, y, 1) = (x(x2 + y2 + 1)−1/2, y(x2 + y2 + 1)−1/2,(x2 + y2 + 1)−1/2).
One possibility for an appropriate coordinate system on the Poincar´e sphere
is a spherical coordinate system; that is, one of the coordinate charts that
is compatible with the map
(ρ, φ, θ) → (ρ sin φ cos θ, ρ sin φ sin θ, ρ cos φ) (1.38)
(see display (1.25)). For example, if we restrict to the portion of the sphere
where x > 0, then one such coordinate map is given by
Ψ(x, y, z) := (arccos(z), arctan  y
x

).
The transformed vector field on the sphere is the push forward of the vector
field X that defines the differential equation on Π by the map Ψ ◦ Q. In
view of equation (1.38) and the restriction to the sphere, the inverse of this
composition is the transformation P given by
P(φ, θ) =  sin φ
cos φ cos θ, sin φ
cos φ sin θ

.
Thus, the push forward of the vector field X is given by
DP(φ, θ)
−1X(P(φ, θ)).
Of course, we can also find the transformed vector field simply by differen￾tiating with respect to t in the formulas
φ = arccos((x2 + y2 + 1)−1/2), θ = arctan  y
x

.
If the vector field is polynomial with maximal degree k, then after we
evaluate the polynomials f and g in system (1.37) at P(φ, θ) and take
into account multiplication by the Jacobian matrix, the denominator of
the resulting expressions will contain cosk−1 φ as a factor. Note that φ = π
2
corresponds to the equator of the sphere and cos( π
2 ) = 0. Thus, the vector
field in spherical coordinates is desingularized by a reparametrization of
time that corresponds to multiplication of the vector field defining the
system by cosk−1 φ. This desingularized system ([61])
φ˙ = (cosk+1 φ)(cos θf + sin θ g), ˙
θ = cosk φ
sin φ (cos θ g − sin θf) (1.39)
is smooth at the equator of the sphere, and it has the same phase portrait
as the original centrally projected system in the upper hemisphere. There￾fore, we can often determine the phase portrait of the original vector field1.7 Manifolds 79
“at infinity” by determining the phase portrait of the desingularized vec￾tor field on the equator. Note that because the vector field corresponding
to system (1.39) is everywhere tangent to the equator, the equator is an
invariant set for the desingularized system.
Spherical coordinates are global in the sense that all the spherical coor￾dinate systems have coordinate maps that are local inverses for the fixed
spherical wrapping function (1.38). Thus, the push forward of the original
vector field will produce system (1.39) in every spherical coordinate system.
There are other coordinate systems on the sphere that have also proved
useful for the compactification of plane vector fields. For example, the right
hemisphere of S2; that is, the subset {(x, y, z) : y > 0} is mapped diffeo￾morphically to the plane by the coordinate function defined by
Ψ1(x, y, z) = x
y
, z
y

.
Also, the map Ψ1 ◦ Q, giving the central projection in these coordinates, is
given by
(x, y, 1) →
x
y
, 1
y

.
Thus, the local representation of the central projection in this chart is
obtained using the coordinate transformations
u = x
y
, v = 1
y
.
Moreover, a polynomial vector field of degree k in these coordinates can
again be desingularized at the equator by a reparametrization correspond￾ing to multiplication of the vector field by vk−1. In fact, the desingularized
vector field has the form
u˙ = vk
f
u
v
, 1
v

− ugu
v
, 1
v
, v˙ = −vk+1g
u
v
, 1
v

.
The function Ψ1 restricted to y < 0 produces the representation of the
central projection in the left hemisphere. Similarly, the coordinate map
Ψ2(x, y, z) = y
x, z
x

on the sphere can be used to cover the remaining points, near the equator
in the upper hemisphere, with Cartesian coordinates (x, y, z) where y = 0
but x = 0.
The two pairs of charts just discussed produce two different local vector
fields. Both of these are usually required to analyze the phase portrait near
infinity. Also, it is very important to realize that if the degree k is even,
then multiplication by vk−1 in the charts corresponding, respectively, to
x < 0 and y < 0 reverses the original direction of time.80 1. Introduction to Ordinary Differential Equations
( 2
3 , 0)
y
v
z
x v u
u
x
u
y
(−2
3 , 0)
Figure 1.21: Phase portrait on the Poincar´e sphere for the differential equa￾tion (1.40).
As an example of compactification, let us consider the phase portrait of
the quadratic planar system given by
x˙ =2+ x2 + 4y2, y˙ = 10xy. (1.40)
This system has no rest points in the finite plane.
In the chart corresponding to v > 0 with the chart map Ψ1, the desin￾gularized system is given by
u = 2v2 − 9u2 + 4, v = −10uv (1.41)
where the symbol “  ” denotes differentiation with respect to the new inde￾pendent variable after reparametrization. The first order system (1.41) has
rest points with coordinates (u, v)=(±2
3 , 0). These rest points lie on the
u-axis: the set in our chart that corresponds to the equator of the Poincar´e
sphere. Both rest points are hyperbolic. In fact, ( 2
3 , 0) is a hyperbolic sink
and (−2
3 , 0) is a hyperbolic source.
In the chart with v < 0 and chart map Ψ1, the reparametrized local
system is given by the differential equation (1.41). But, because k = 2, the
direction of “time” has been reversed. Thus, the sink at ( 2
3 , 0) in this chart
corresponds to a source for the original vector field centrally projected to
the Poincar´e sphere. The rest point (−2
3 , 0) corresponds to a sink on the
Poincar´e sphere.
We have now considered all points on the Poincar´e sphere except those
on the great circle given by the equation y = 0. For these points, we must
use the charts corresponding to the map Ψ2. In fact, there is a hyperbolic
saddle point at the origin of each of these coordinate charts, and these
rest points correspond to points on the equator of the Poincar´e sphere. Of
course, the other two points already discussed are also rest points in these
charts.1.7 Manifolds 81
Figure 1.22: Phase portrait of Vinograd’s system (1.42).
The phase portrait of the compactification of system (1.40) is shown in
Figure 1.21. Because the x-axis is an invariant manifold for the original
vector field, the two saddles at infinity are connected by a heteroclinic
orbit.
Exercise 1.148. Prove that S2 is a two-dimensional submanifold of R3.
Exercise 1.149. Use spherical coordinates to determine the compactification
of the differential equation (1.40) on the Poincar´e sphere.
Exercise 1.150. Find the compactification of the differential equation
x˙ = x + y − y3
, y˙ = −x + y + x3
on the Poincar´e sphere using spherical coordinates. Show that the equator is a
periodic orbit. See [61, p. 411] for a stability analysis of this periodic orbit, but
note that there is a typographical error in the formula given for the desingularized
projection of this vector field.
Exercise 1.151. Draw the phase portrait of the vector field
x˙ = x2 + y2 − 1, y˙ = 5(xy − 1).
This example is studied by Poincar´e in his pioneering memoir on differential
equations ([208, Oeuvre, p. 66]; see also [159, p. 204]).
Exercise 1.152. [Vinograd’s System] Show that the phase portrait of Vino￾grad’s system
x˙ = x2
(y − x) + y5
, y˙ = y2
(y − 2x) (1.42)82 1. Introduction to Ordinary Differential Equations
agrees with Figure 1.22. In particular, show that while every orbit is attracted to
the origin, the origin is not asymptotically stable (see [127, p. 191] and [254]). (a)
Prove that the system has exactly one rest point. (b) Prove that the system is
invariant with respect to the transformation x → −x and y → −y. In particular,
the phase portrait is symmetric relative to the origin. (c) Prove that the x-axis
is invariant and all points on this set are attracted to the origin. (d) Prove that
there is an open set in the plane containing the origin such that every solution
starting in the set is attracted to the rest point at the origin. Hint: Consider the
isocline x2(y − x) + y5 = 0. Prove that every solution which enters the region
bounded by the isocline and the positive x-axis is attracted to the origin. Prove
that every solution starting near the origin in the upper half-plane eventually
enters this region. (e) Prove that the rest point at the origin has an elliptic
sector; that is, there are two solutions (one approaching the rest point in forward
time, one in backward time) such that, in one of the two regions subtended by the
corresponding orbits, every solution starting sufficiently close to the rest point
in this region is doubly asymptotic to the rest point (that is, such solutions are
asymptotic to the rest point in both the forward and backward directions). Hint:
Blow up the rest point. (f) Prove that every trajectory is attracted to the origin?
Hint: Compactify the system. Show that the system, written in the coordinates
given by x = 1/v and y = u/v, takes the form
u˙ = −u(u5 − v2
u2 + 3v2
u − v2
), v˙ = −v(u5 + v2
u − v2
) (1.43)
where the equation of the line at infinity is v = 0. Since the rest points at infinity
are highly degenerate, special weighted blowups compatible with the weighted
polar blowup (r, θ) → (r2 cos θ, r5 sin θ) are the best choice (see [21]). In fact, it
suffices to use the chart given by u = pq2 and v = q5 where system (1.43) has
the form
p˙ = −1
5
p(3p5 − 5q4
p2 + 13q2
p − 3), q˙ = −1
5
q(p5 + q2
p − 1).
This system has a semi-hyperbolic rest point (that is, the linearization has exactly
one zero eigenvalue). The local phase portrait at this rest point can be determined
by an additional blowup. (See Exercise 7.11 for an alternative method.) To finish
the proof, use the Poincar´e–Bendixson theorem 1.185. (g) Draw the global phase
portrait including the circle at infinity.
Exercise 1.153. (a) Describe the phase portrait of the one-dimensional ODE
x˙ = x(1 − x). (b) Prove that for real numbers a and b, every solution of ˙x =
a sin(bt)x(1 − x) that starts at time t = 0 with 0 ≤ x(0) ≤ 1 remains in the strip
{(t, x):0 ≤ x ≤ 1}. Generalize. (c) Does the solution in part (b) remain in the
strip {(t, x) : x(0) ≤ x ≤ 1}? Discuss.
1.8 Periodic Solutions
Stability of a rest point can often be determined by linearization or by
an application of Lyapunov’s direct method. In both cases, stability can
be determined by analysis in an arbitrary open ball that contains the rest1.8 Periodic Solutions 83
x
S
Σ
p
P(p)
Γ
Figure 1.23: A Poincar´e section Σ and the corresponding Poincar´e return
map. The trajectory starting at x is asymptotic to a periodic orbit Γ. The
trajectory passes through the section Σ at the point p and first returns to
the section at the point P(p).
point. For this reason, we say that the stability of a rest point is a local
problem. In contrast, to determine the stability of a periodic solution, the
behavior of the corresponding vector field in a neighborhood of the entire
periodic orbit must be taken into account. Because global methods must
be employed, the analysis of periodic solutions is much more difficult (and
more interesting) than the analysis of rest points. Some of the basic ideas
that are used to study the existence and stability of periodic solutions will
be introduced in this section.
1.8.1 The Poincar´e Map
A very powerful concept in the study of periodic orbits is the Poincar´e map.
It is a corner stone of the “geometric theory” of Henri Poincar´e [208], the
father of our subject. To define the Poincar´e map, also called the return
map, let φt denote the flow of the differential equation ˙x = f(x), and
suppose that S ⊆ Rn is an (n − 1)-dimensional submanifold. If p ∈ S and
(p, f(p)) ∈ TpS, then we say that the vector (p, f(p)) is transverse to S at
p. If (p, f(p)) is transverse to S at each p ∈ S, we say that S is a section
for φt. If p is in S, then the curve t → φt(p) “passes through” S as t passes
through t = 0. Perhaps there is some T = T(p) > 0 such that φT (p) ∈ S.
In this case, we say that the point p returns to S at time T. If there is
an open subset Σ ⊆ S such that each point of Σ returns to S, then Σ is
called a Poincar´e section. In this case, let us define P : Σ → S as follows:84 1. Introduction to Ordinary Differential Equations
P(p) := φT(p)(p) where T(p) > 0 is the time of the first return to S. The
map P is called the Poincar´e map, or the return map on Σ and T : Σ → R
is called the return-time map (see Figure 1.23). Because the solution of a
differential equation is smoothly dependent on its initial value, the implicit
function theorem can be used to prove that both P and T are smooth
functions on Σ (see Exercise 1.154).
Exercise 1.154. Prove that the return-time map T is smooth. Hint: Find a
function F : Rn → R so that F(u) = 0 if and only if u ∈ Σ and define G(t, u) =
F(φt(u)). If p ∈ Σ and T is the time of its first return, then apply the implicit
function theorem to G at (T,p) to solve for T as a function of p.
The following is a fundamental idea of Poincar´e: Fixed points of the
return map lie on periodic orbits. More generally, periodic points of the
Poincar´e map correspond to periodic solutions of the differential equation.
Here, if P denotes the return map, then we will say that p is a fixed point
of P provided that P(p) = p. A periodic point with period k is a fixed point
of the kth iterate of P—it passes through the Poincar´e section k − 1 times
before closing. Here, we do not insist that the domains of the iterates of
P coincide with Σ. In the subject of dynamical systems, P1 := P is the
first iterate; more precisely, the first iterate map associated with P. And,
as long as its domain is not empty, the kth iterate is defined inductively by
Pk := P ◦ Pk−1. Using this notation, p ∈ Σ is a periodic point with period
k if Pk(p) = p.
Often, instead of studying the fixed points of the kth iterate of the
Poincar´e map, it is more convenient to study the zeros of the corresponding
displacement function δ : Σ → Rn defined by δ(p) = Pk(p) − p. With this
definition, the periodic points of period k for the Poincar´e map correspond
to the roots of the equation δ(p) = 0.
If p ∈ Σ is a periodic point of the Poincar´e map of period k, then the
stability of the corresponding periodic orbit of the differential equation is
determined by computing the eigenvalues of the linear map DPk(p). In
fact, an important theorem, which we will prove in Section 5.4, states that
if Pk(p) = p and DPk(p) has all its eigenvalues inside the unit circle, then
the periodic orbit with initial point p is asymptotically stable.
Exercise 1.155. Suppose that A is an 2 × 2 matrix and consider the linear
transformation of R2 given by x → Ax as a dynamical system. (a) Prove: If the
spectrum of A lies inside the unit circle in the complex plane, then Akx → 0 as
k → ∞ for every x ∈ R2. (b) Prove: If at least one eigenvalue of A lies outside
the unit circle, then there is a point x ∈ R2 such that ||Akx|| → ∞ as k → ∞.
(c) Define the notion of stability and asymptotic stability for discrete dynamical
systems, and show that the origin is asymptotically stable for the linear dynami￾cal system associated with A if and only if the spectrum of A lies inside the unit1.8 Periodic Solutions 85
circle. (d) Suppose that the spectrum of A does not lie inside the unit circle. Give
a condition that implies the origin is stable. See Section 5.4 for the n × n case.
Exercise 1.156. [One-dimensional Dynamics] A discrete dynamical system
need not be invertible. For example, consider the quadratic family f : [0, 1] →
[0, 1] defined by f(x) = λx(1−x), for λ in the interval (0, 4]. It defines a dynami￾cal system via xn+1 = f(xn). (a) Prove: If λ < 1, then f has a globally attracting
fixed point. (b) If 1 <λ< 3, then f has a globally attracting nonzero fixed point.
(c) Prove: A bifurcation occurs at λ = 3 such that for λ > 3 there is a periodic
orbit with period two and this orbit is asymptotically stable for 3 <λ< 1 + √6.
(d) Prove: A bifurcation occurs at λ = 1+ √6 such that for λ > 1 + √6 there is a
periodic orbit with period four. (e) In fact, a countable sequence of such bifurca￾tions occur at λ1 = 3, λ2 = 1+√6, . . . so that a periodic orbit of period 2n is born
at λn. The sequence {λn}∞n=1 converges to a number λ∞ ≈ 3.57. It turns out that
limn→∞(λn−λn−1)/(λn+1−λn)=4.67 ··· . This is a universal constant (for fam￾ilies whose graphs have a unique nondegenerate maximum, e.g. x → λ sin(πx) for
x ∈ [0, 1]) called the Feigenbaum number. Verify these statements with numer￾ical experiments. (f) For λ = λ∞, the dynamical system has periodic points of
all periods. It is not difficult to prove that the system is chaotic at λ = 4. At
least, it is not difficult to prove that the dynamics are as random as a coin toss.
The ideas for a proof are in the next few exercises. (g) Prove that the tent map
h : [0, 1] → [0, 1] given by h(x)=2x, for 0 ≤ x ≤ 1/2 and h(x)=2 − 2x,
for 1/2 ≤ x ≤ 1 is semiconjugate to the quadratic map f(x)=4x(1 − x) via
g : [0, 1] → [0, 1] given by g(x) = sin2 πx; that is, f(g(x)) = g(h(x)) for x ∈ [0, 1].
(h) Prove: Every point in [0, 1] can be represented by a binary decimal expansion
x = .x1x2x3 ··· , where xn = 0 or xn = 1. (i) Prove: The map h acts on binary
sequences by the rule h(.x1x2x3 ··· ) = .(x1 ⊕ x2)(x1 ⊕ x3)(x1 ⊕ x4) ··· , where
⊕ is addition base two. (j) Use the binary representation of h to prove that for
an arbitrary sequence of coin tosses, say HHTTHT ··· , there is a point in [0, 1]
such that its iterates under h fall in the intervals H = [0, 1/2) and T = [1/2, 1] in
the order specified by the coin tosses. Note: Much more can be said about one￾dimensional dynamical systems (see, for example, [82], [111], [119], and [244]).
Although it is very difficult, in general, to find a suitable Poincar´e section
and to analyze the associated Poincar´e map, there are many situations
where these ideas can be used to great advantage. For example, suppose
that there is a Poincar´e section Σ and a closed ball B ⊆ Σ such that P :
B → B. Recall Brouwer’s fixed point theorem (see any book on algebraic
topology, for example, [169] or [179]).
Theorem 1.157 (Brouwer’s Fixed Point Theorem). Every continu￾ous map of a closed (Euclidean) ball into itself has at least one fixed point.
By this theorem, the map P must have at least one fixed point. In
other words, the associated differential equation has a periodic orbit pass￾ing through the set B. This idea is used in the following “toy” example.
See Exercise 1.165 for an application.86 1. Introduction to Ordinary Differential Equations
y
4π
y
τ
2π
τ
Figure 1.24: The phase cylinder for the differential equation (1.44).
Consider the nonautonomous differential equation
y˙ = (a cost + b)y − y3, a> 0, b> 0 (1.44)
and note that the associated vector field is time periodic with period 2π.
To take advantage of this periodicity property, let us recast this differential
equation—using the standard “trick”—as the first-order system
y˙ = (a cos τ + b)y − y3,
τ˙ = 1. (1.45)
Also, for each ξ ∈ R, let t → (τ (t, ξ), y(t, ξ)) denote the solution of sys￾tem (1.45) with the initial value
τ (0, ξ)=0, y(0, ξ) = ξ
and note that τ (t, ξ) ≡ t. Here, the order of the variables is reversed to
conform with two conventions: The angular variable is written second in
a system of this type, but the phase portrait is depicted on a plane where
the angular coordinate axis is horizontal.
The vector field corresponding to the system (1.45) is the same in every
vertical strip of width 2π in the plane considered with coordinates (τ,y).
Thus, from our geometric point of view, it is convenient to consider sys￾tem (1.45) as a differential equation defined on the cylinder T×R obtained
by identifying the line Σ := {(τ,y) : τ = 0} with each line {(τ,y) : τ = 2π}
where  is an integer (see Figure 1.24). On this cylinder, Σ is a section for
the flow. Moreover, if ξ ∈ R is the coordinate of a point on Σ, then the
associated Poincar´e map is given by
P(ξ) = y(2π, ξ)1.8 Periodic Solutions 87
τ
P(ξ)
τ = 0 τ = 2π
y
ξ
Figure 1.25: The Poincar´e map for the system (1.45).
whenever the solution t → (τ (t, ξ), y(t, ξ)) is defined on the interval [0, 2π].
By the definition of a Poincar´e map, the fixed points of P correspond to
periodic orbits of the differential equation defined on the phase cylinder.
Let us prove that the fixed points of P correspond to periodic solutions of
the original differential equation (1.44). In fact, it suffices to show that if
y(2π, ξ0) = ξ0 for some ξ0 ∈ R, then t → y(t, ξ0) is a 2π-periodic solution
of the differential equation (1.44).
By the extension theorem, there is some t∗ > 0 such that the function
t → z(t) given by z(t) := y(t+ 2π, ξ0) is defined on the interval [0, t∗). Note
that z(0) = y(2π, ξ0) = ξ0 and
z˙(t)= ˙y(t + 2π, ξ0)
= (a(cos(t + 2π)) + b)y(t + 2π, ξ0) − y3(t + 2π, ξ0)
= (a cost + b)y(t + 2π, ξ0) − y3(t + 2π, ξ0)
= (a cost + b)z(t) − z3(t).
Thus, t → z(t) is a solution of the differential equation (1.44) with the
same initial value as the solution t → y(t, ξ0). By the uniqueness theorem,
it follows that z(t) = y(t, ξ0) for 0 ≤ t<t∗. Hence, if t → y(t + 2π, ξ0)
blows up on the interval t∗ ≤ t ≤ 2π, then so does the function t → y(t, ξ0),
contrary to the hypothesis. Thus, t → y(t, ξ0) is defined on the interval
[0, 4π] and y(t+2π, ξ0) = y(t, ξ0) for 0 ≤ t ≤ 2π. By repeating the argument
inductively with z(t) = y(t+k2π, ξ0) for the integers k = 2, 3,..., it follows88 1. Introduction to Ordinary Differential Equations
that t → y(t, ξ0) is a 2π-periodic solution of the differential equation (1.44),
as required (Figure 1.25).
Because y(t, 0) ≡ 0, it follows immediately that P(0) = 0; that is, the
point ξ = 0 corresponds to a periodic orbit. To find a non-trivial periodic
solution, note that a cost + b ≤ a + b, and consider the line given by
y = a + b + 1 in the phase cylinder. The y-component of the vector field on
this line is
(a + b + 1)(a cos τ + b − (a + b + 1)2).
Since
a cos τ + b − (a + b + 1)2 ≤ (a + b + 1) − (a + b + 1)2 < 0,
the vector field corresponding to the first-order system “points” into the
region that lies below the line. In particular, if 0 ≤ ξ ≤ a + b + 1, then
0 ≤ P(ξ) ≤ a + b + 1; that is, P maps the closed interval [0, a + b + 1] into
itself. Hence, the Brouwer fixed point theorem can be applied to prove the
existence of a periodic orbit (see also Exercise 1.158). But, because P(0) =
0, this application of the Brouwer fixed point theorem gives no information
about the existence of non-trivial periodic solutions. The remedy, as we
will soon see, is to construct a P invariant closed interval that does not
contain ξ = 0.
Suppose that P
(0) > 1; that is, the trivial periodic solution is unstable.
Then, there is some number c such that 0 <c<a + b + 1 and P
(ξ) > 1
as long as 0 ≤ ξ ≤ c. By the mean value theorem, P(c) = P
(ξ)c for some
ξ, 0 <ξ<c. Thus, P(c) > c. Because P is a Poincar´e map, it is easy to
see that the interval c ≤ ξ ≤ a + b + 1 is mapped into itself by P and, as
a result, there is at least one fixed point in this interval. This fixed point
corresponds to a periodic solution of the differential equation (1.44).
To prove that P
(0) > 1 we will use a variational equation. This method
is employed very often in the analysis of differential equations. The present
elementary example is a good place to learn the basic technique. The idea is
simple: The derivative of the solution of a differential equation with respect
to its initial value is itself the solution of a differential equation.
Recall that P(ξ) = y(2π, ξ). Since
d
dty(t, ξ)=(a cost + b)y(t, ξ) − y3(t, ξ)
we have that
d
dtyξ(t, ξ)=(a cost + b)yξ(t, ξ) − 3y2(t, ξ)yξ(t, ξ).
Because y(0, ξ) = ξ, we also have the initial condition yξ(0, ξ) = 1. More￾over, at the point ξ = 0 the function t → y(t, ξ) is identically zero. Thus, if
t → w(t) is the solution of the variational initial value problem
w˙ = (a cost + b)w, w(0) = 1,1.8 Periodic Solutions 89
then P
(0) = w(2π).
The variational differential equation is linear. Its solution is given by
w(t) = e
 t
0 (a cos s+b) ds = ea sin t+bt.
In particular, we have
P
(0) = w(2π) = e2πb > 1,
as required. Moreover, this computation shows that the periodic solution
given by y(t) ≡ 0 is unstable. (Why?)
Exercise 1.158. Prove Brouwer’s fixed point theorem for a closed interval in
R. Hint: Use the intermediate value theorem.
Exercise 1.159. Find the initial point for the non-trivial periodic solution in
the interval 0 <ξ<a+b+ 1 for (1.44) as a function of a and b. Are there exactly
two periodic solutions?
Exercise 1.160. Find conditions on a(t) and on f that ensure the existence of
at least one (non-trivial) periodic solution for a differential equation of the form
y˙ = a(t)y + f(y).
Exercise 1.161. Consider the differential equation (1.44) on the cylinder, and
the transformation given by u = (y + 1) cos τ , v = (y + 1) sin τ that maps the
portion of the cylinder defined by the inequality y > −1 into the plane. What
is the image of this transformation? Find the differential equation in the new
coordinates, and draw its phase portrait.
We have proved that there is at least one 2π-periodic solution of the
differential equation (1.44) with initial condition in the interval 0 <ξ<
a + b + 1. But even more is true: This periodic orbit is stable and unique.
To prove this fact, let us suppose that 0 < ξ0 < a + b + 1 and P(ξ0) = ξ0,
so that the corresponding solution t → y(t, ξ0) is 2π-periodic.
To determine the stability type of the solution with initial value ξ0, it
suffices to compute P
(ξ0). As before, P
(ξ0) = w(2π) where t → w(t) is
the solution of the variational initial value problem
w˙ = [(a cost + b) − 3y2(t, ξ0)]w, w(0) = 1.
It follows that
P
(ξ0) = w(2π)
= e
 2π
0 a cos t+b−3y2(t,ξ0) dt
= e2πb−3
 2π
0 y2(t,ξ0) dt.90 1. Introduction to Ordinary Differential Equations
To compute  2π
0 y2 dt, note that because y(t, ξ0) > 0 for all t, we have
the following equality:
y˙(t, ξ0)
y(t, ξ0) = a cost + b − y2(t, ξ0).
Using this formula and the periodicity of the solution t → y(t, ξ0), we have
that
 2π
0
y2(t, ξ0) dt = 2πb −
 2π
0
y˙(t, ξ0)
y(t, ξ0)
dt = 2πb,
and, as a result,
P
(ξ0) = e2πb−3(2πb) = e−4πb < 1. (1.46)
Hence, every periodic solution in the interval [0, a + b + 1] is stable. The
uniqueness of the periodic solution is a consequence of this result. In fact,
the map P is real analytic. Thus, if P has infinitely many fixed points
in a compact interval, then P is the identity. This is not true, so P has
only a finite number of fixed points. If ξ0 and ξ1 are the coordinates of
two consecutive fixed points, then the displacement function, that is, ξ →
P(ξ) − ξ, has negative slope at two consecutive zeros, in contradiction.
Exercise 1.162. Find an explicit formula for the solution of the differential
equation (1.44) and use it to give a direct proof for the existence of a non-trivial
periodic solution.
Exercise 1.163. Prove that P(ξ) < 0 for ξ > 0, where P is the Poincar´e
map defined for the differential equation (1.44). Use this result and the inequal￾ity (1.46) to prove the uniqueness of the non-trivial periodic solution of the dif￾ferential equation.
Exercise 1.164. Show that the (stroboscopic) Poincar´e map for the differential
equation (1.44) has exactly one fixed point on the interval (0, ∞). How many fixed
points are there on (−∞, ∞)?
Exercise 1.165. Suppose that h : R → R is a T-periodic function, and h(t) <
1/4 for every t ∈ R. Show that the differential equation ˙x = x(1 − x) − h(t) has
exactly two T-periodic solutions. The differential equation can be interpreted as a
model for the growth of a population in a limiting environment that is subjected
to periodic harvesting (cf. [226]).
Exercise 1.166. Is it possible for the Poincar´e map for a scalar differential
equation not to be the identity map on a fixed compact interval and at the same
time have infinitely many fixed points in the interval?
Exercise 1.167. [Boundary Value Problem] (a) Prove that the Dirichlet bound￾ary value problem
x = 1 − x2
, x(0) = 0, x(2) = 01.8 Periodic Solutions 91
has a solution. Hint: Use the phase plane. Show that the first positive time T
such that the orbit with initial conditions x(0) = 0 and x
(0) = 0 reaches the
x-axis is T < 2 and for the initial conditions x(0) = 0 and x
(0) = 2/
√3, T > 2.
To show this fact use the idea in the hint for Exercise 1.13 to construct an
integral representation for T. (b) Find a solution of the boundary value problem
by shooting and Newton’s method (see Exercise 1.120). Hint: Use the phase plane
with x = y. Consider the solution t → (x(t, η), y(t, η)) with initial conditions
x(0) = 0 and y(0) = η and use Newton’s method to solve the equation y(2, η) = 0.
Note: The solutions with different choices for the velocity are viewed as shots.
The velocity is adjusted until the target is hit.
Exercise 1.168. Consider the nonautonomous differential equation
y¨ = (y + t + 2)(y + t + 1).
Prove: If 2π<T < ∞, then there is a solution such that ˙y is T-periodic. Hint:
The differential equation can be made autonomous by a change of variables.
Exercise 1.169. Consider the linear system
x˙ = ax, y˙ = −by
where a > 0 and b > 0 in the open first quadrant of the phase plane and let φt
denote its flow. (a) Show that L := {(ξ, 1) : ξ > 0} and M := {(1, η) : η > 0} are
transverse sections for the system. (b) Find a formula for the section map h from
L to M. (c) Find a formula for T : L → R, called the time-of-flight map, which
is defined by φT (ξ)(ξ, 1) = (1, h(ξ)).
Exercise 1.170. Compute the time required for the solution of the system
x˙ = x(1 − y), y˙ = y(x − 1)
with initial condition (x, y) = (1, 0) to arrive at the point (x, y) = (2, 0). Note
that this system has a section map y → h(y) defined from a neighborhood of
(x, y) = (1, 0) on the line given by x = 1 to the line given by x = 2. Compute
h
(0).
Exercise 1.171. Observe that the x-axis is invariant for the system
x˙ =1+ xy, y˙ = 2xy2 + y3
,
and the trajectory starting at the point (1, 0) crosses the line x = 3 at (3, 0).
Thus, there is a section map h and a time-of-flight map T from the line x = 1 to
the line x = 3 with both functions defined on some open interval about the point
(1, 0) on the line x = 1. Compute T
(0) and h
(0).
Exercise 1.172. Determine a condition imposed on the choice of parameter a
such that the second-order differential equation ¨x − xx˙ + ax = 0 has a periodic
orbit of period T = 1. Formulate and prove a result that provides evidence for
the validity of your condition.
Exercise 1.173. Research Problem: Consider the second-order differential equa￾tion
x¨ + f(x) ˙x + g(x)=0
where f and g are 2π-periodic functions. Determine conditions on f and g that
ensure the existence of a periodic solution.92 1. Introduction to Ordinary Differential Equations
1.8.2 Limit Sets and Poincar´e–Bendixson Theory
The general problem of finding periodic solutions for differential equations
is still an active area of mathematical research. Perhaps the most well￾developed theory for periodic solutions is for differential equations defined
on the plane. But, even in this case, the theory is far from complete. For
example, consider the class of planar differential equations of the form
x˙ = f(x, y), y˙ = g(x, y)
where f and g are quadratic polynomials. There are examples of such
“quadratic systems” that have four isolated periodic orbits—“isolated”
means that each periodic orbit is contained in an open subset of the plane
that contains no other periodic orbits (see Exercise 1.206). But, no one
knows at present if there is a quadratic system with more than four iso￾lated periodic orbits. The general question of the number of isolated peri￾odic orbits for a polynomial system in the plane has been open since 1905;
it is called Hilbert’s 16th problem (see [63], [143], [211], and [221]).
Although there are certainly many difficult issues associated with peri￾odic orbits of planar systems, an extensive theory has been developed that
has been successfully applied to help determine the dynamics of many
mathematical models. Some of the basic results of this theory will be
explained later in this section after we discuss some important general
properties of flows of autonomous, not necessarily planar, systems.
The properties that we will discuss enable us to begin to answer the
question “What is the long-term behavior of a dynamical system?” This
is often the most important question about a mathematical model. Ask
an engineer what he wants to know about a model ordinary differential
equation. Often his response will be the question “What happens if we
start the system running and then wait for a long time?” or, in engineering
jargon, “What is the steady-state behavior of the system?” We already
know how to answer these questions in some special circumstances where
the steady-state behavior corresponds to a rest point or periodic orbit. The
following definitions will be used to precisely describe the limiting behavior
of an arbitrary orbit.
Definition 1.174. Suppose that φt is a flow on Rn and p ∈ Rn. A point x
in Rn is called an omega limit point (ω-limit point) of the orbit through p if
there is a sequence of numbers t1 ≤ t2 ≤ t3 ≤··· such that limi→∞ ti = ∞
and limi→∞ φti (p) = x. The collection of all such omega limit points is
denoted ω(p) and is called the omega limit set (ω-limit set) of p. Similarly,
the α-limit set α(p) is defined to be the set of all limits limi→∞ φti (p) where
t1 ≥ t2 ≥ t3 ≥··· and limi→∞ ti = −∞.
Definition 1.175. The orbit of the point p with respect to the flow φt is
called forward complete if t → φt(p) is defined for all t ≥ 0. Also, in this
case, the set {φt(p) : t ≥ 0} is called the forward orbit of the point p. The1.8 Periodic Solutions 93
orbit is called backward complete if t → φt(p) is defined for all t ≤ 0 and
the backward orbit is {φt(p) : t ≤ 0}.
Proposition 1.176. The omega limit set of a point is closed and invariant.
Proof. The empty set is closed and invariant.
Suppose that ω(p) is not empty for the flow φt and x ∈ ω(p). Consider
φT (x) for some fixed T ∈ R. There is a sequence t1 ≤ t2 ≤ t3 ≤ ··· with
ti → ∞ and φti (p) → x as i → ∞. Note that t1 +T ≤ t2 +T ≤ t3 +T ≤···
and that φti+T (p) = φT (φti (p)). By the continuity of the flow, we have that
φT (φti (p)) → φT (x) as i → ∞. Thus, φT (x) ∈ ω(p), and therefore ω(p) is
an invariant set.
To show ω(p) is closed, it suffices to show that ω(p) is the intersection of
closed sets. In fact, we have that
ω(p) = 
τ≥0
closure {φt(p) : t ≥ τ}. 
Proposition 1.177. Suppose that p ∈ Rn and the orbit of the flow φt
through the point p is forward complete. If the forward orbit of p has com￾pact closure, then ω(p) is nonempty, compact, and connected.
Proof. The sequence {φn(p)}∞
n=1 is contained in the compact closure of
the orbit through p. Thus, it has at least one limit point x. In fact, there
is an infinite sequence of integers n1 ≤ n2 ≤ ··· such that φni (p) → x as
i → ∞. Hence, x ∈ ω(p), and therefore ω(p) = ∅.
Since ω(p) is a closed subset of the compact closure of the orbit through
p, the set ω(p) is compact.
To prove that ω(p) is connected, suppose to the contrary that there
are two disjoint open sets U and V whose union contains ω(p) such that
ω(p) ∩ U = ∅ and ω(p) ∩ V = ∅. There is some t1 > 0 such that φt1 (p) ∈ U
and some t2 > t1 such that φt2 (p) ∈ V . But the set K = {φt(p) : t1 ≤ t ≤
t2} is the continuous image of an interval, hence a connected set. Thus K
cannot be contained in U ∪ V . In particular, there is at least one τ1 > 0
such that φτ1 (p) is not in this union.
Similarly we can construct a sequence τ1 ≤ τ2 ≤··· such that
lim
i→∞ τi = ∞
and for each i the point φτi (p) is in the complement of U ∪ V . By the
compactness, the sequence {φτi (p)}∞
i=1 has a limit point x. Clearly, x is
also in ω(p) and in the complement of U ∪ V . This is a contradiction. 
Exercise 1.178. Construct examples to show that the compactness hypothesis
of Proposition 1.177 is necessary.94 1. Introduction to Ordinary Differential Equations
Exercise 1.179. Show that a reparametrization of a flow does not change its
omega limit sets. Thus, an omega limit set is determined by an orbit and its
direction, not the parametrization of the orbit.
Exercise 1.180. Suppose a flow is defined on Rn and A is a subset of this
space. The ω-limit set of A denoted ω(A) is the union of the ω-limit sets of the
points in A. (a) Prove that ω(ω(A)) ⊆ ω(A). (b) Construct an example where
ω(ω(A)) = ω(A).
Exercise 1.181. Suppose that U is an open subset of Rn, f : U → Rn and
t → γ(t) is a solution of the differential equation ˙x = f(x), and p ∈ Rn such that
limt→∞ γ(t) = p. (a) Show that p = ω(γ(0)). (b) Prove: If f is continuously dif￾ferentiable, then p is a rest point of the differential equation. (c) Is the hypothesis
on the smoothness of f in part (b) necessary? Prove that it is or prove the same
result with a weaker hypothesis.
Exercise 1.182. Suppose that x0 is a rest point for the differential equation
x˙ = f(x) with flow φt, and V is a Lyapunov function at x0. If, in addition, there
is a neighborhood W of the rest point x0 such that, for each point p ∈ W \ {x0},
the function V is not constant on the forward orbit of p, then x0 is asymptotically
stable. Hint: The point x0 is Lyapunov stable. If it is not asymptotically stable,
then there is a point p in the domain of V whose omega limit set ω(p) is also
in the domain of V such that ω(p) = {x0}. Show that V is constant on this
omega limit set (the constant is the greatest lower bound of the range of V on
the forward orbit through p).
Exercise 1.183. Suppose that the differential equation ˙x = f(x) with flow φt
has a compact invariant set K, and V : K → R is a continuously differentiable
function such that V˙ (x) ≤ 0 for every x ∈ K. If Ω is the largest invariant set in
{x ∈ K : V˙ (x)=0}, then every solution in K approaches Ω as t → ∞.
The ω-limit set of a point for a flow in Rn with n ≥ 3 can be very
complicated; for example, it can be a fractal.
Omega limit sets for smooth flows on R2 are well understood and not too
complicated. The reason is the deep fact about the geometry of the plane:
Theorem 1.184 (Jordan Curve Theorem). A simple closed (continu￾ous) curve in the plane divides the plane into two connected components,
one bounded and one unbounded, each with the curve as boundary.
Proof. Modern proofs of this theorem use algebraic topology (see for
example [238]). 
This result will play a central role in what follows.
In Rn for n ≥ 3, ω-limit sets may be very complex, for example, sets
with fractal dimension. The now classic example is the Lorenz equations
x˙ = σ(y − x), y˙ = rx − y − xz, z˙ = xy − bz (1.47)
already mentioned in Ex. (1.56). Evidence for the existence of a strange
attractor is beautifully presented in [244]. For the case b > 0 and r > 1,1.8 Periodic Solutions 95
y
x
Figure 1.26: A positively invariant annular region for a flow in the plane.
there is a solid ellipsoid containing the origin that is positively invariant.
Thus, every point in the ellipsoid has an ω-limit set. The divergence of
the vector field generating the Lorenz equations has (constant) negative
divergence. This implies the flow is volume contracting in forward time.
This implies, for example, that no point has its ω-limit set on a manifold
that is the boundary of a set with positive volume. For r larger than some
critical value, rest points of the system are not attractors (see Ex. (1.189)).
These facts do not eliminate the possibility that for r larger than the critical
value most trajectories are attracted to a periodic orbit. Evidence against
this possibility can be provided. This suggests the existence of an attractor
that is not a point, a periodic orbit, or a two-dimensional torus (the usual
classical attractors). In fact, an attractor of fractal dimension exists. At
this writing, there is no pencil-and-paper proof of this fact, but there are
computer-assisted proofs (see also Ex. (9.1)).
The fundamental result about limit sets for flows of planar differential
equations is the Poincar´e–Bendixson theorem. There are several versions
of this theorem; we will state two of them. The main ingredients of their
proofs will be presented later in this section beginning with Lemma 1.199.
Theorem 1.185 (Poincar´e–Bendixson). If Ω is a nonempty compact
ω-limit set of a flow in R2, and if Ω does not contain a rest point, then Ω
is a periodic orbit.
A set S that contains the forward orbit of each of its elements is called
positively invariant. An orbit whose α-limit set is a rest point p and whose96 1. Introduction to Ordinary Differential Equations
y
x
Figure 1.27: A limit cycle in the plane.
ω-limit is a rest point q is said to connect p and q. Note: the definition of
a connecting orbit allows p = q.
Theorem 1.186. Suppose that φt is a flow on R2 and S ⊆ R2 is a pos￾itively invariant set with compact closure. If p ∈ S and φt has at most a
finite number of rest points in the closure of S, then ω(p) is either (i) a
rest point, (ii) a periodic orbit, or (iii) a union of finitely many rest points
and a nonempty finite or countable infinite set of connecting orbits.
Exercise 1.187. Illustrate possibility (iii) of the last theorem with an example
having an infinite set of connecting orbits.
Exercise 1.188. We have assumed that all flows are smooth. Is this hypothesis
required for all the theorems in this section on ω-limit sets?
Exercise 1.189. Prove all mathematical statements made about the Lorenz
equations in this section (except the existence of a strange attractor). Hint: You
may need the result in Ex. (2.24).
Definition 1.190. A limit cycle Γ is a periodic orbit that is either the
ω-limit set or the α-limit set of some point that is in the phase space but
not in Γ.
A “conceptual” limit cycle is illustrated in Figure 1.27. In this figure,
the limit cycle is the ω-limit set of points in its interior (the bounded1.8 Periodic Solutions 97
-4 -2 0 2 4
-4
-2
0
2
4
Figure 1.28: Two orbits are numerically computed for the system ˙x =
0.5x − y + 0.1(x2 − y2)(x − y), ˙y = x + 0.5y + 0.1(x2 − y2)(x + y): one with
initial value (x, y) = (0.5, 0), the other with initial value (x, y) = (0, 5).
Both orbits approach a stable limit cycle.
component of the plane with the limit cycle removed) and its exterior
(the corresponding unbounded component of the plane). A limit cycle that
is generated by numerically integrating a planar differential equation is
depicted in Figure 1.28 (see [39]).
Sometimes the following alternative definition of a limit cycle is given. A
“limit cycle” is an isolated periodic orbit; that is, the unique periodic orbit
in some open subset of the phase space. This definition is not equivalent
to Definition 1.190 in general. The two definitions, however, are equivalent
for real analytic systems in the plane (see Exercise 1.194).
An annular region is a subset of the plane that is homeomorphic to the
closed annulus bounded by the unit circle at the origin and the concentric
circle whose radius is two units in length.
The following immediate corollary of the Poincar´e–Bendixson theorem
is often applied to prove the existence of limit cycles for planar systems.
Theorem 1.191. If a flow in the plane has a positively invariant annular
region S that contains no rest points of the flow, then S contains at least
one periodic orbit. If in addition, some point in S is in the forward orbit
of a point on the boundary of S, then S contains at least one limit cycle.
We will discuss two applications of Theorem 1.191 where the main idea
is to find a rest-point free annular region as depicted in Figure 1.26.
The first example is provided by the differential equation
x˙ = −y + x(1 − x2 − y2), y˙ = x + y(1 − x2 − y2). (1.48)98 1. Introduction to Ordinary Differential Equations
Note that the annulus S bounded by the circles with radii 1
2 and 2, respec￾tively, contains no rest points of the system. Let us show that S is positively
invariant. To prove this fact, consider the outer normal vector N on ∂S that
is the restriction of the vector field N(x, y)=(x, y, x, y) ∈ R2 × R2 to ∂S
and compute the dot product of N with the vector field corresponding to
the differential equation. In fact, the dot product
x2(1 − x2 − y2) + y2(1 − x2 − y2)=(x2 + y2)(1 − x2 − y2)
is positive on the circle with radius 1
2 and negative on the circle with radius
2. Therefore, S is positively invariant and, by Theorem 1.191, there is at
least one limit cycle in S.
The differential equation (1.48) is so simple that we can find a formula
for its flow. In fact, by changing to polar coordinates (r, θ), the transformed
system
r˙ = r(1 − r2), ˙
θ = 1
decouples, and its flow is given by
φt(r, θ) =  r2e2t
1 − r2 + r2e2t
 1
2
, θ + t

. (1.49)
Note that φt(1, θ) = (1, θ + t) and, in particular, φ2π(1, θ) = (1, θ + 2π).
Thus, the unit circle in the plane is a periodic orbit with period 2π. Here,
of course, we must view θ as being defined modulo 2π, or, better yet, we
must view the polar coordinates as coordinates on the cylinder T × R (see
Section 1.7.6).
If the formula for the flow (1.49) is rewritten in rectangular coordinates,
then the periodicity of the unit circle is evident. In fact, the periodic solu￾tion starting at the point (cos θ,sin θ) ∈ R2 (in rectangular coordinates) at
t = 0 is given by
t → (x(t), y(t)) = (cos(θ + t),sin(θ + t)).
It is easy to see that if r = 0, then the ω-limit set ω((r, θ)) is the entire
unit circle. Thus, the unit circle is a limit cycle.
If we consider the positive x-axis as a Poincar´e section, then we have
P(x) =  x2e4π
1 − x2 + x2e4π
 1
2
.
Here P(1) = 1 and P
(1) = e−4π < 1. In other words, the intersection point
of the limit cycle with the Poincar´e section is a hyperbolic fixed point of
the Poincar´e map; that is, the linearized Poincar´e map has no eigenvalue
on the unit circle of the complex plane. In fact, here the single eigenvalue
of the linear transformation of R given by x → P
(1)x is inside the unit
circle. It should be clear that in this case the limit cycle is an asymptotically1.8 Periodic Solutions 99
stable periodic orbit. We will also call such an orbit a hyperbolic stable limit
cycle. (The general problem of the stability of periodic orbits is discussed
in Chapter 2.)
As a second example of the application of Theorem 1.191, let us consider
the very important differential equation
¨θ + λ ˙
θ + sin θ = μ
where λ > 0 and μ are constants, and θ is an angular variable; that is, θ is
defined modulo 2π. This differential equation is a model for an unbalanced
rotor or pendulum with viscous damping λ ˙
θ and external torque μ.
Consider the equivalent first-order system
˙
θ = v, v˙ = − sin θ + μ − λv, (1.50)
and note that, since θ is an angular variable, the natural phase space for
this system is the cylinder T × R. With this interpretation we will show
the following result: If |μ| > 1, then system (1.50) has a globally attracting
limit cycle. The phrase “globally attracting limit cycle” means that there
is a limit cycle Γ on the cylinder and Γ is the ω-limit set of every point on
the cylinder. In other words, the steady-state behavior of the unbalanced
rotor, with viscous damping and sufficiently large torque, is stable periodic
motion. (See [161] for the existence of limit cycles in case |μ| ≤ 1.)
The system (1.50) with |μ| > 1 has no rest points. (Why?) Also the
quantity − sin θ + μ − λv is negative for sufficiently large positive values
of v, and it is positive for negative values of v that are sufficiently large
in absolute value. Therefore, there are numbers v− < 0 and v+ > 0 such
that every forward orbit is contained in the compact subset of the cylinder
A := {(v, θ) : v− ≤ v ≤ v+}. In addition, A is diffeomorphic to an annular
region in the plane. It follows that the Poincar´e–Bendixson theorem is valid
in A, and therefore the ω-limit set of every point on the cylinder is a limit
cycle.
Although there are several ways to prove that the limit cycle is unique,
let us consider a proof based on the following propositions: (i) If the diver￾gence of a vector field is everywhere negative, then the flow of the vector
field contracts volume (see Exercise 2.24). (ii) Every periodic orbit in the
plane surrounds a rest point (see Exercise 1.201). (A replacement for the
first proposition is given in Exercise 1.212; an alternate method of proof is
suggested in Exercise 1.217.)
To apply the propositions, note that the divergence of the vector field
for system (1.50) is the negative number −λ. Also, if |μ| > 1, then this
system has no rest points. By the second proposition, no periodic orbit of
the system is contractable on the cylinder (see panel (a) of Figure 1.29).
Thus, if there are two periodic orbits, they must bound an invariant annular
region on the cylinder as in panel (b) of Figure 1.29. But this contradicts
the fact that the area of the annular region is contracted by the flow. It100 1. Introduction to Ordinary Differential Equations
Ω
(a) (b)
Ω
Figure 1.29: Panel (a) depicts a contractable periodic orbit on a cylinder.
Note that the region Ω in panel (a) is simply connected. Panel (b) depicts
two periodic orbits that are not contractable; they bound a multiply con￾nected region Ω on the cylinder.
follows that there is a unique periodic orbit on the cylinder that is a globally
attracting limit cycle.
Exercise 1.192. Give a direct proof that the point (1/
√2, 1/
√2) on the unit
circle is an ω-limit point of the point (3, 8) for the flow of system (1.48).
Exercise 1.193. Discuss the phase portrait of system (1.50) for |μ| < 1.
Exercise 1.194. (a) Show that the set containing “limit cycles” defined as
isolated periodic orbits is a proper subset of the set of limit cycles. Also, if the
differential equation is a real analytic planar autonomous system, then the two
concepts are the same. Hint: Imagine an annular region consisting entirely of
periodic orbits. The boundary of the annulus consists of two periodic orbits that
might be limit cycles, but neither of them is isolated. To prove that an isolated
periodic orbit Γ is a limit cycle, show that every section of the flow at a point
p ∈ Γ has a subset that is a Poincar´e section at p. For an analytic system, again
consider a Poincar´e section and the associated Poincar´e map P. Zeros of the
analytic displacement function ξ → P(ξ) − ξ correspond to periodic orbits. (b)
Show that the polynomial (hence real analytic) system in R3 given by
x˙ = −y + x(1 − x2 − y2
),
y˙ = x + y(1 − x2 − y2
),
z˙ = 1 − x2 − y2 (1.51)
has limit cycles that are not isolated. (c) Determine the long-term behavior of
the system (1.51). In particular, show that
limt→∞ z(t) = z(0) − 1
2 ln(x2
(0) + y2
(0)).1.8 Periodic Solutions 101
Exercise 1.195. Show that the system
x˙ = ax − y + xy2
, y˙ = x + ay + y3
has an unstable limit cycle for a < 0 and no limit cycle for a > 0. Hint: Change
to polar coordinates.
Exercise 1.196. Show that the system
x˙ = y + x(x2 + y2 − 1) sin 1
x2 + y2 − 1
,
y˙ = −x + y(x2 + y2 − 1) sin 1
x2 + y2 − 1
has infinitely many limit cycles in the unit disk.
Exercise 1.197. Prove: An analytic planar system cannot have infinitely many
limit cycles that accumulate on a periodic orbit. Note: This (easy) exercise is a
special case of a deep result: An analytic planar system cannot have infinitely
many limit cycles in a compact subset of the plane, and a polynomial system
cannot have infinitely many limit cycles (see [89] and [143]).
Exercise 1.198. Consider the differential equation
x˙ = −ax(x2 + y2
)
−1/2
, y˙ = −ay(x2 + y2
)
−1/2 + b
where a and b are positive parameters. The model represents the flight of a
projectile, with speed a and heading toward the origin, that is moved off course
by a constant force with strength b. Determine conditions on the parameters
that ensure the solution starting at the point (x, y)=(p, 0), for p > 0, reaches
the origin. Hint: Change to polar coordinates and study the phase portrait of
the differential equation on the cylinder. Explain your result geometrically. The
differential equation is not defined at the origin. Is this a problem?
The next two lemmas are used in the proof of the Poincar´e–Bendixson
theorem. The first lemma is a corollary of the Jordan curve theorem.
Lemma 1.199. If Σ is a section for the flow φt and if p ∈ R2, then the
orbit through the point p intersects Σ in a monotone sequence; that is, if
φt1 (p), φt2 (p), and φt3 (p) are on Σ and if t1 < t2 < t3, then φt2 (p) lies
strictly between φt1 (p) and φt3 (p) on Σ or φt1 (p) = φt2 (p) = φt3 (p).
Proof. The proof is left as an exercise. Hint: Reduce to the case where
t1, t2, and t3 correspond to consecutive crossing points. Then, consider the
curve formed by the union of {φt(p) : t1 ≤ t ≤ t2} and the subset of Σ
between φt1 (p) and φt2 (p). Draw a picture. 
Lemma 1.200. If Σ is a section for the flow φt and if p ∈ R2, then ω(p)∩Σ
contains at most one point.102 1. Introduction to Ordinary Differential Equations
Proof. The proof is by contradiction. Suppose that ω(p) ∩ Σ contains at
least two points, x1 and x2. By rectification of the flow at x1 and at x2, that
is, by the rectification lemma (Lemma 1.116), it is easy to see that there are
sequences {φti (p)}∞
i=1 and {φsi (p)}∞
i=1 in Σ such that limi→∞ φti (p) = x1
and limi→∞ φsi (p) = x2. By the rectification lemma in Exercise 1.121,
such sequences can be found in Σ. Indeed, we can choose the rectifying
neighborhood so that the image of the Poincar´e section is a line segment
transverse to the rectified flow. In this case, it is clear that if an orbit
has one of its points in the rectifying neighborhood, then this orbit passes
through the Poincar´e section.
By choosing a local coordinate on Σ, let us assume that Σ is an open
interval. Working in this local chart, there are open subintervals J1 at x1
and J2 at x2 such that J1∩J2 = ∅. Moreover, by the definition of limit sets,
there is an integer m such that φtm(p) ∈ J1; an integer n such that sn > tm
and φsn (p) ∈ J2; and an integer  such that t > sn and φt (p) ∈ J1. By
Lemma 1.199, the point φsn (p) must be between the points φtm(p) and
φt (p) on Σ. But this is impossible because the points φtm(p) and φt (p)
are in J1, whereas φsn (p) is in J2. 
We are now ready to prove the Poincar´e–Bendixson theorem (Theo￾rem 1.185): If Ω is a nonempty compact ω-limit set of a flow in R2, and if
Ω does not contain a rest point, then Ω is a periodic orbit.
Proof. Suppose that ω(p) is nonempty, compact, and contains no rest
points. Choose a point q ∈ ω(p). We will show first that the orbit through
q is closed.
Consider ω(q). Note that ω(q) ⊆ ω(p) and ω(q) is not empty. (Why?)
Let x ∈ ω(q). Since x is not a rest point, there is a section Σ at x and a
sequence on Σ consisting of points on the orbit through q that converges
to x. These points are in ω(p). But, by the last corollary, this is impossible
unless every point in this sequence is the point x. Since q is not a rest point,
this implies that q lies on a closed orbit Γ, as required. In particular, the
limit set ω(p) contains the closed orbit Γ.
To complete the proof we must show ω(p) ⊆ Γ. If ω(p) = Γ, then we will
use the connectedness of ω(p) to find a sequence {pn}∞
n=1 ⊂ ω(p) \ Γ that
converges to a point z on Γ. To do this, consider the union A1 of all open
balls with unit radius centered at some point in Γ. The set A1 \ Γ must
contain a point in ω(p). If not, consider the union A1/2, (respectively A1/4)
of all open balls with radius 1
2 (respectively 1
4 ) centered at some point in
Γ. Then the set A1/4 together with the complement of the closure of A1/2
“disconnects” ω(p), in contradiction. By repeating the argument with balls
whose radii tend to zero, we can construct a sequence of points in ω(p) \ Γ
whose distance from Γ tends to zero. Using the compactness of ω(p), there
is a subsequence, again denoted by {pn}∞
n=1, in ω(p) \ Γ that converges to
a point z ∈ Γ.1.8 Periodic Solutions 103
Let U denote an open set at z such that the flow is rectified in a diffeo￾morphic image of U. There is some integer n such that pn ∈ U. But, by
using the rectification lemma, it is easy to see that the orbit through pn
has a point y of intersection with some Poincar´e section Σ at z. Because
pn is not in Γ, the points y and z are distinct elements of the set ω(p) ∩ Σ,
in contradiction to Lemma 1.200. 
Exercise 1.201. Suppose that γ is a periodic orbit of a smooth flow defined on
R2. Use Zorn’s lemma to prove that γ surrounds a rest point of the flow. That is,
the bounded component of the plane with the periodic orbit removed contains a
rest point. Note: See Exercise 1.232 for an alternative proof.
Exercise 1.202. Use Exercise 1.201 to prove Brouwer’s fixed point theorem for
the closed unit disk D in R2. Hint: First prove the result for a smooth function
f : D → D by considering the vector field f(x) − x, and then use the following
result: A continuous transformation of D is the uniform limit of smooth transfor￾mations [140, p. 253].
Exercise 1.203. Suppose that a closed ball in Rn is positively invariant under
the flow of an autonomous differential equation on Rn. Prove that the ball con￾tains a rest point or a periodic orbit. Hint: Apply Brouwer’s fixed point theorem
to the time one map of the flow. Explain the differences between this result and
the Poincar´e–Bendixson theorem.
Exercise 1.204. Construct an example of an (autonomous) differential equa￾tion defined on all of R3 that has an (isolated) limit cycle but no rest points.
Exercise 1.205. Prove: A nonempty ω-limit set of an orbit of a gradient system
consists entirely of rest points.
Exercise 1.206. Is a limit cycle isolated from all other periodic orbits? Hint:
Consider planar vector fields of class C1 and those of class C ω—real analytic
vector fields. Study the Poincar´e map on an associated transversal section.
The next theorem can often be used to show that no periodic orbits exist.
Proposition 1.207 (Dulac’s Criterion). Consider a smooth differential
equation on the plane
x˙ = g(x, y), y˙ = h(x, y).
If there is a smooth function B(x, y) defined on a simply connected region
Ω ⊆ Rn such that the quantity (Bg)x + (Bh)y is not identically zero and of
fixed sign on Ω, then there are no periodic orbits in Ω.
Proof. We will prove Bendixson’s criterion, which is the special case of
the theorem where B(x, y) ≡ 1 (see Exercise 1.210 for the general case). In
other words, we will prove that if the divergence of f := (g(x, y), h(x, y))
given by
div f(x, y) := gx(x, y) + hy(x, y)104 1. Introduction to Ordinary Differential Equations
is not identically zero and of fixed sign in a simply connected region Ω,
then there are no periodic orbits in Ω.
Suppose that Γ is a closed orbit in Ω and let G denote the bounded
region of the plane bounded by Γ. Note that the line integral of the one
form g dy − h dx over Γ vanishes. (Why?) By Green’s theorem, the integral
can be computed by integrating the two-form (div f) dxdy over G. Since,
by the hypothesis, the divergence of f does not vanish, the integral of the
two-form over G does not vanish, in contradiction. Thus, no such periodic
orbit can exist. 
The function B mentioned in the last proposition is called a Dulac function.
We end this section with a result about global asymptotic stability in
the plane.
Theorem 1.208. Consider a smooth differential equation on the plane
x˙ = g(x, y), y˙ = h(x, y)
that has the origin as a rest point. Let J denote the Jacobian matrix for
the transformation (x, y) → (g(x, y), h(x, y)), and let φt denote the flow of
the differential equation. If the following three conditions are satisfied, then
the origin is globally asymptotically stable.
Condition 1. For each (x, y) ∈ R2, the trace of J given by gx(x, y) +
hy(x, y) is negative.
Condition 2. For each (x, y) ∈ R2, the determinant of J given by
gx(x, y)hy(x, y) − gy(x, y)hx(x, y) is positive.
Condition 3. For each (x, y) ∈ R2, the forward orbit {φt(x, y):0 ≤ t <
∞} is bounded.
Proof. From the hypotheses on the Jacobian matrix, if there is a rest point,
the eigenvalues of its associated linearization all have negative real parts.
Therefore, each rest point is a hyperbolic attractor; that is, the basin of
attraction of the rest point contains an open neighborhood of the rest point.
This fact follows from Hartman’s theorem (Theorem 1.40) or Theorem 3.1.
In particular, the origin is a hyperbolic attractor.
By the hypotheses, the trace of the Jacobian (the divergence of the vector
field) is negative over the entire plane. Thus, by Bendixson’s criterion, there
are no periodic solutions.
Let Ω denote the basin of attraction of the origin. Using the continuity
of the flow, it is easy to prove that Ω is open. In addition, it is easy to
prove that the boundary of Ω is closed and contains no rest points.
We will show that the boundary of Ω is positively invariant. If not, then
there is a point p in the boundary and a time T > 0 such that either φT (p)
is in Ω or such that φT (p) is in the complement of the closure of Ω in
the plane. In the first case, since φT (p) is in Ω, it is clear that p ∈ Ω, in
contradiction. In the second case, there is an open set V in the complement1.8 Periodic Solutions 105
of the closure of Ω that contains φT (p). The inverse image of V under the
continuous map φT is an open set U containing the boundary point p. By
the definition of boundary, U contains a point q ∈ Ω. But then, q is mapped
to a point in the complement of the closure of Ω, in contradiction to the
fact that q is in the basin of attraction of the origin.
If the boundary of Ω is not empty, consider one of its points. The
(bounded) forward orbit through the point is precompact and contained in
the (closed) boundary of Ω. Thus, its ω-limit set is contained in the bound￾ary of Ω. Since the boundary of Ω contains no rest points, an application of
the Poincar´e–Bendixson theorem shows this ω-limit set is a periodic orbit,
in contradiction. Thus, the boundary is empty and Ω is the entire plane. 
Theorem 1.208 is a (simple) special case of the “Markus–Yamabe prob￾lem.” In fact, the conclusion of the theorem is true without assuming Con￾dition 3 (see [120]).
Exercise 1.209. Prove: If δ > 0, then the origin is a global attractor for the
system
u˙ = (u − v)
3 − δu, v˙ = (u − v)
3 − δv.
Also, the origin is a global attractor of orbits in the first quadrant for the system
u˙ = uv(u − v)(u + 1) − δu, v˙ = vu(v − u)(v + 1) − δv.
(Both of these first-order systems are mentioned in [257].)
Exercise 1.210. [Dulac’s Criterion] (a) Prove Proposition 1.207. (b) Use Dulac’s
criterion to prove a result due to Nikolai N. Bautin: The system
x˙ = x(a + bx + cy), y˙ = y(α + βx + γy)
has no limit cycles. Hint: Show that no periodic orbit crosses a coordinate axis.
Reduce the problem to showing that there are no limit cycles in the first quadrant.
Look for a Dulac function of the form xrys. After some algebra the problem
reduces to showing that a certain two-parameter family of lines always has a
member that does not pass through the (open) first quadrant.
Exercise 1.211. (a) Suppose that the system ˙x = f(x, y), ˙y = g(x, y) has a
periodic orbit Γ with period T and B is a positive real-valued function defined on
some open neighborhood of Γ (as in Dulac’s criterion). Prove that Γ is a periodic
orbit of the system ˙x = B(x, y)f(x, y), ˙y = B(x, y)g(x, y) with period
τ =
 T
0
1
B(x(t), y(t)) ds
where t → (x(t), y(t)) is a periodic solution of the original system whose orbit is
Γ. (b) How does the period of the limit cycle of system (1.48) change if its vector
field is multiplied by (1 + x2 + y2)
α? Hint: The solution ρ of the initial value
problem
ρ˙ = B((x(ρ), y(ρ)), ρ(0) = 0
satisfies the identity ρ(t + τ ) = ρ(t) + T.106 1. Introduction to Ordinary Differential Equations
Exercise 1.212. [Uniqueness of Limit Cycles] (a) Prove the following proposi￾tion: If the divergence of a plane vector field is of fixed sign in an annular region
Ω of the plane, then the associated differential equation has at most one periodic
orbit in Ω. Hint: Use Green’s theorem. (b) Recall Dulac’s criterion from Exer￾cise 1.210 and note that if the divergence of the plane vector field F is not of fixed
sign in Ω, then it might be possible to find a smooth function B : Ω → R such
that the divergence of BF does have fixed sign in Ω. As an example, consider the
van der Pol oscillator,
x˙ = y, y˙ = −x + λ(1 − x2
)y
and the “Dulac function” B(x, y)=(x2 + y2 − 1)−1/2. Show that van der Pol’s
system has at most one limit cycle in the plane. (The remarkable Dulac function
B was discovered by L. A. Cherkas.) (c) Can you prove that the van der Pol
oscillator has at least one limit cycle in the plane? Hint: Change coordinates
using the Li´enard transformation
u = x, v = y − λ(x − 1
3
x3
)
to obtain the Li´enard system
u˙ = v + λ(u − 1
3
u3
), v˙ = −u.
In Chapter 8 we will prove that the van der Pol system has a limit cycle if λ > 0
is sufficiently small. In fact, this system has a limit cycle for each λ > 0. For this
result, and for more general results about limit cycles of the important class of
planar systems of the form
x˙ = y − F(x), y˙ = −g(x),
see [116, p. 154], [140, p. 215], [159, p. 267], and [204, p. 250].
Exercise 1.213. (a) Prove that the system
x˙ = x − y − x3
, y˙ = x + y − y3
has a unique globally attracting limit cycle on the punctured plane. (b) Find all
rest points of the system
x˙ = x − y − xn, y˙ = x + y − yn,
where n is a positive odd integer and determine their stability. (c) Prove that
the system has a unique stable limit cycle. (d) What is the limiting shape of the
limit cycle as n → ∞?
Exercise 1.214. Let
x˙ = −y(1 − x2
), y˙ = x + y(1 − x2
).
Prove that every point (x, y) = (0, 0) such that |x| < 1 has the same ω-limit set
and identify this set.1.8 Periodic Solutions 107
Exercise 1.215. Does the system of differential equations
x˙ = 2x − 2x2 − y − 2xy,
y˙ = x + 198x2 + 2y + 4xy + 9y2
have a periodic orbit?
Exercise 1.216. (a) Show that the system
x˙ = x − y − x2
2 − xy,
y˙ = x + y − x2
2
has no periodic solutions. (b) Suppose that X is a polynomial vector field on the
plane of degree two and the system of polynomial equations grad(div X) · X = 0
and div X = 0 has a simultaneous solution consisting of infinitely many point in
the plane. Prove that the differential equation corresponding to X has no periodic
orbits. (c) Formulate and prove a theorem that has the statement in part (b) as
a special case.
Exercise 1.217. Show there is a unique limit cycle for system (1.50) with |μ| >
1 by proving the existence of a fixed point for a Poincar´e map and by proving
that every limit cycle is stable. Hint: Recall the analysis of system (1.45) and
consider dv/dθ.
Exercise 1.218. Can a system of the form
x˙ = y, y˙ = f(x) − af
(x)y,
where f is a smooth function and a is a parameter, have a limit cycle? Hint:
Consider a Li´enard transformation.
Exercise 1.219. Draw the phase portrait of the system
x˙ = y + 2x(1 − x2 − y2
), y˙ = −x.
Exercise 1.220. [Rigid Body Motion] The Euler equations for rigid body motion
are presented in Exercise 1.73. Recall that the momentum vector is given by
M = AΩ where A is a symmetric matrix and Ω is the angular velocity vector,
and Euler’s equation is given by M˙ = M ×Ω. For ν a positive-definite symmetric
matrix and F a constant vector, consider the differential equation
M˙ = M × Ω + F − νM.
Here, the function M → νM represents viscous friction and F is the external
force (see [17]). Prove that all orbits of the differential equation are bounded,
and therefore every orbit has a compact ω-limit set.
Exercise 1.221. (a) Prove that the origin is a center for the system ¨x+ ˙x2+x =
0. (b) Show that this system has unbounded orbits. (c) Describe the boundary
between the bounded and unbounded orbits?
Exercise 1.222. Draw the phase portrait for the system ¨x = x2 − x3. Is the
solution with initial conditions x(0) = 1
2 and ˙x(0) = 0 periodic?108 1. Introduction to Ordinary Differential Equations
Exercise 1.223. Draw the phase portrait of the Hamiltonian system ¨x + x −
x2 = 0. Give an explicit formula for the Hamiltonian and use it to justify the
features of the phase portrait.
Exercise 1.224. Let t → x(t) denote the solution of the initial value problem
x¨ + ˙x + x + x3 = 0, x(0) = 1, x˙ (0) = 0.
Determine limt→∞ x(t).
Exercise 1.225. Show that the system
x˙ = x − y − (x2 +
3
2
y2
)x, y˙ = x + y − (x2 +
1
2
y2
)y
has a unique limit cycle.
Exercise 1.226. Find the rest points in the phase plane of the differential
equation ¨x+(˙x2 +x2 −1) ˙x+x = 0 and determine their stability. Also, show that
the system has a unique stable limit cycle.
Exercise 1.227. Determine the ω-limit set of the solution of the system
x˙ = 1 − x + y3
, y˙ = y(1 − x + y)
with initial condition x(0) = 10, y(0) = 0.
Exercise 1.228. Show that the system
x˙ = −y + xy, y˙ = x +
1
2
(x2 − y2
)
has periodic solutions, but no limit cycles.
Exercise 1.229. Consider the van der Pol equation
x¨ + (x2 − ) ˙x + x = 0,
where  is a real parameter. How does the stability of the trivial solution change
with . Show that the van der Pol equation has a unique stable limit cycle for
 = 1. What would you expect to happen to this limit cycle as  shrinks to  = 0.
What happens for  < 0?
Exercise 1.230. Find an explicit nonzero solution of the differential equation
t
2
x2
x¨ + ˙x = 0.
Define new variables u = 2(3tx2)
−1/2, v = −4 ˙x(3x3)
−1/2 and show that
dv
du = 3v(v − u2)
2u(v − u) .
Draw the phase portrait of the corresponding first-order system
u˙ = 2u(v − u), v˙ = 3v(v − u2
).1.9 Regular and Singular Perturbation 109
Exercise 1.231. [Yorke’s Theorem] A theorem of James Yorke states that if
f : U ⊆ Rn → Rn is Lipschitz on the open set U with Lipschitz constant L and
Γ is a periodic orbit of x˙ = f(x) contained in U, then the period of Γ is larger
than 2π/L (see [266]). Use Yorke’s theorem to estimate a lower bound for the
period of the limit cycle solution of the system in Exercise 1.213 part (a). Note:
The period of the periodic orbit is approximately 7.5. Hint: Use the mean value
theorem and note that the norm of a matrix (with respect to the usual Euclidean
norm) is the square root of the spectral radius of the matrix transpose times the
matrix (that is; A = ρ(AT A)).
Exercise 1.232. [Poincar´e index] Let C be a simple closed curve not passing
through a rest point of the vector field X in the plane with components (f,g).
Define the Poincar´e index of X with respect to C to be
I(X, C) = 1
2π

C
d arctan  g
f

;
it is the total change the angle (f(x, y), g(x, y)) makes with respect to the (posi￾tive) x-axis as (x, y) traverses C exactly once counter clockwise (see, for example,
[67] or [159]). (a) Prove: The index is an integer. (b) Prove: The index does not
change with a deformation of C (as long as the deformed curve does not pass
through a rest point). (c) Prove: If C is smooth and T is a continuous choice of
the tangent vector along this curve, then I(T,C) = 1. In particular, the index
of a vector field with respect to one of its closed orbits is unity. (d) The index
of a point with respect to X is defined to be the index of X with respect to an
admissible curve C that surrounds this point and no other rest point of X. Prove:
The index of a regular point (a point that is not a rest point) is zero. (e) Prove:
A periodic orbit surrounds at least one rest point.
1.9 Regular and Singular Perturbation
The style of this section is a series of exercises leading the reader through
an elementary introduction to perturbation theory. Related topics are dis￾cussed in more detail elsewhere in this book. Students of the subject might
note the intention of the author to demonstrate how a mathematics text￾book should be read when coming to a subject for the first time.
Families of differential equations of the from
x˙ = f(x, y), y˙ = g(x, y),
where  is a small parameter, arise frequently in applied mathematics. There
is a natural and basic question: What is the relation between solutions
with  = 0 and those with  = 0. Answers are usually based on a far￾reaching idea. At  = 0 the problem should be solvable or at least easier to
analyze. Perturbations of such solutions should provide insight and, if they
are computable, useful approximations of solutions when  is small but not
zero. As a typical case, note that a corresponding initial value problem110 1. Introduction to Ordinary Differential Equations
(IVP) with (x(0), y(0)) = (x0, y0) for the unperturbed system ( = 0)
reduces to ˙x = f(x, y(0)), x(0) = x0 because ˙y = 0. Suppose its solution
is x = ξ(t) so that (x, y)=(ξ(t), y0) is the corresponding solution of the
unperturbed system. A natural and powerful idea is to seek the solution of
the IVP for  = 0 as a power series in powers of , say
x = ξ(t) + ξ1(t) + ξ2(t)
2 + ··· , y = y0 + η1(t) + η2(t)
2 + ··· .
Working formally, the next step is simple: Substitute the series into the
family of differential equations, expand f(x, y) and g(x, y) after substitution
in powers of , and collect terms in both equations according to like powers
of . The zeroth-order terms turn out to be (x, y)=(ξ(t), y0)—as they
must, and the first-order terms are obtained by solving (with a simple
integration) a differential equation for η1 with initial condition η1(0) = 0,
substitution of this solution into the differential equation for ξ1 with initial
condition ξ1(0) = 0, and solving the latter first-order linear differential
equation.
(a) Write out in mathematical formulas what is said in the last sentence
and derive the IVPs in abstract form that must be solved to obtain the first￾order terms. Apply the perturbation procedure to approximate y(ln t, ) for
the solution of the initial value problem
x˙ = x + y2, y˙ = (y − x), x(0) = 1, y(0) = 1
to first order in .
(b) Continuing with the perturbation procedure, show that ξ2 and η2 can
be determined by the same algorithm once the first-order solution is known.
Point out difficulties that might arise when x or y are vector variables. Show
that the scalar case is always solvable in principle. Argue that the regular
perturbation procedure can be continued to determine approximations to
all finite orders in .
(c) What can you say about phase portraits of the family
x˙ = (1 − x2 − y2)(x2 + y2)x + (1 − y(x2 + y2) − y3),
y˙ = (1 − x2 − y2)(x2 + y2)y + (x(x2 + y2) + xy2)
as  changes?
(d) Families of differential equations of the form
x˙ = f(x, y), y˙ = g(x, y) (1.52)
also arise frequently. To begin a perturbation scheme, the idea should be to
set  = 0, solve the resulting equations, and then seek a solution in powers
of . But, a problem arises immediately: At  = 0 the equations no longer
form a system of differential equations. Instead they form a differential1.9 Regular and Singular Perturbation 111
algebraic system (DAE) ˙x = f(x, y), g(x, y) = 0. Derive the DAE for the
IVP
x˙ = −x − y2, y˙ = x − y, x(0) = 1, y(0) = 1. (1.53)
Does it have a solution? Explain. Perhaps it would be wise to study DAEs
and to build a perturbation theory for their solutions.
(e) But, there are other issues: Consider the family of initial value prob￾lems x˙ = x, x(0) = 1, solve it in closed form, and show that x(2, ) blows
up as  goes to zero. Of course, the vector field x/ of the last differential
equation—and similarly for the more general system (1.52)—is not defined
at  = 0. Such systems are called singular and their analysis is called sin￾gular perturbation theory.
(f) A method that recovers the basic procedure from regular perturbation
is called geometric singular perturbation theory. In its simplest form, the
underlying idea is to recognize that there are (at least) two time scales
in the evolution of solution of system (1.52). For small nonzero epsilon,
g(x, y)/ is generally very large compared with f(x, y). So x is evolving on
a slow time scale compared to the evolution of y. The presence of different
time scales (which may be less apparent) is a signature of singular behavior.
Taking advantage of this difference suggests changing the temporal variable
from the original t to τ = t/. Show that this change, for  = 0 transforms
the singular system to the regular perturbation problem
x = f(x, y), y = g(x, y) (1.54)
where the prime denotes differentiation with respect to τ .
(g) The transformation is not defined at  = 0, but solutions found (or
approximated) for this regular problem with  = 0 can be transformed back
to solutions (or approximations) of the original singular problem. This is
a key insight for multi-time scale problems: there might be many different
transformations not defined at  = 0 that produce different singular lim￾its not defined in the dynamics of the original system. Perturbation from
these limits in the transformed problem can be transformed back to valid
solutions of the singular problem. Apply the geometric singular perturba￾tion procedure to the family of initial value problems (1.53) to approximate
(x(ln 2, ), y(ln 2, )) to at least first order in .
(h) The global geometry for the singular family in part (f) is typical of
more general phenomena. Reconsider the family (1.54). At  = 0 there is
a set of rest points given by F = {(x, y) : g(x, y)=0}. In the special case
where the partial derivative gy(x, y) does not vanish on F, this set is a
graph over the x-axis. Prove this fact. What happens if gx(x, y) does not
vanish on F?
(i) Continuing with part (g) under the assumption that gy(x, y) has fixed
sign on F, show that this set of rest points has the structure of a normally
hyperbolic invariant manifold. Parse the words: check that F is an invariant112 1. Introduction to Ordinary Differential Equations
manifold. The normally hyperbolic part requires that solutions expand (or
contract) in all directions normal to the manifold exponentially fast and
faster than any expansion or contraction on the manifold. Prove that this
is the case.
(j) There is an important theory that culminates in the statement that
normally hyperbolic manifolds persist (see the book [200] for an introduc￾tion to singular perturbation theory, the survey [149] for an introduction to
geometric singular perturbation theory, and [91] for the noncompact case).
An application (roughly speaking) is that for sufficiently small  there is a
(unique) normally hyperbolic manifold F near F. This fundamental result
is a hunting license for an invariant manifold near F. It usually does not
consist of rest points; it has its own dynamics. The problem is to approxi￾mate this manifold and reduce the singular family to a differential equation
on this manifold. This is done using regular perturbation. For simplicity
assume that F is a graph over the x-axis. The first step uses the hunt￾ing license: Seek the invariant manifold as the graph of y = h(x, ) for an
unknown function h that is known to exist uniquely by the persistence the￾ory. Invariance implies that y(t, ) = h(x(t, ), ) and therefore (dropping
some of the functional arguments) y = hx(x, )x
. Using the vector field
corresponding to the differential equation,
g(x, y) = hx(x, )f(x, y).
Expand  → h(x, ) in powers of  into a series of the form h(x, ) =
h(x, 0)+h1(x)+h2(x)2+··· . Use it to obtain the corresponding expansion
of y(t, ), substitute into the displayed equation, and collect terms with
like powers of . Show that this procedure can be used to determine the
hi in sequence starting with h1. Apply the procedure (with appropriate
modifications) to approximate perturbations of the invariant manifold near
the manifold of rest points of the differential equation (1.53) transformed
to the fast time τ .
(k) The series for h(x, ) determined in part (i) of course provides also the
desired reduction to an ODE on the invariant manifold: x = f(x, h(x, )).
Write this ODE to first order in  for the example derived from the IVP (1.53).
Also, write the ODE in the original slow time t.
(i) Describe the phase portraits of the planar families
1. x˙ = −x + y2, y˙ = −y + tanh x,
2. x˙ = y(1 − x2 + x4), y˙ = −y + tanh x,
3. x˙ = −x + (1 + x)y y˙ = x − (a + x)y
using geometric singular perturbation theory. For family (3), describe the
fate of the solution starting at (x, y) = (1, 0) as a varies on the interval
(0, 2) for small  ≥ 0.
(m) Consider the first-order three-dimensional family
x = y, y = z, z = y2 − xz − 1,1.10 Review of Calculus 113
which is equivalent to the third-order differential equation in Exercise 1.12.
See Section 9.3 and equation (9.71) for the origin of the exercises on this
three-dimensional first-order system of differential equations. Suppose that
the independent variable is τ and  is a small parameter. Change to fast
time t = τ / and show that the system is transformed to
x˙ = y, y˙ = z, z˙ = y2 − xz − 1.
(n) At  = 0, prove the existence of a normally hyperbolic invariant
manifold. Identify this manifold as a quadric surface. Draw a picture.
(o) Determine the stability type(s) of rest points on the invariant mani￾fold (which, in this case, is also called a slow-manifold).
(p) Show that the line y = 1 is invariant. Are there points on the per￾turbed slow-manifold attracted to the line y = 1?
1.10 Review of Calculus
The basic definitions of the calculus extend easily to multidimensional
spaces. In fact, these definitions are essentially the same when extended
to infinite dimensional spaces. Thus, we will begin our review with the
definition of differentiation in a Banach space.
Definition 1.233. Let U be an open subset of a Banach space X, let Y
denote a Banach space, and let the symbol   denote the norm in both
Banach spaces. A function f : U → Y is called (Fr´echet) differentiable at
a ∈ U if there is a bounded linear operator Df(a) : X → Y , called the
derivative of f, such that
lim
h→0
1
h
f(a + h) − f(a) − Df(a)h = 0.
If f is differentiable at each point in U, then the function f is called
differentiable.
Using the notation of Definition 1.233, let L(X, Y ) denote the Banach
space of bounded linear transformations from X to Y , and note that the
derivative of f : U → Y is the function Df : U → L(X, Y ) given by
x → Df(x).
The following proposition is a special case of the chain rule.
Proposition 1.234. Suppose that U is an open subset of a Banach space
and f : U → Y . If f is differentiable at a ∈ U and v ∈ U, then
d
dtf(a + tv)


t=0 = Df(a)v.114 1. Introduction to Ordinary Differential Equations
Proof. The proof is obvious for v = 0. Assume that v = 0 and consider
the scalar function given by
α(t) := 
1
t
(f(a + tv) − f(a)) − Df(a)v)
= 1
|t|
f(a + tv) − f(a) − Df(a)tv
for t = 0. It suffices to show that limt→0 α(t) = 0.
Choose  > 0. Since f is differentiable at a, there is some δ > 0 such that
1
h
f(a + h) − f(a) − Df(a)h < 
whenever 0 < h < δ. If |t| < δv−1, then tv < δ and
1
|t|v
f(a + tv) − f(a) − Df(a)tv < .
In particular, we have that α(t) ≤ v whenever |t| < δv−1, as required.

The following is a list of standard facts about the derivative; the proofs
are left as exercises. For the statements in the list, the symbols X, Y , Xi,
and Yi denote Banach spaces.
(i) If f : X → Y is differentiable at a ∈ X, then f is continuous at a.
(ii) If f : X → Y and g : Y → Z are both differentiable, then h = g ◦ f
is differentiable, and its derivative is given by the chain rule
Dh(x) = Dg(f(x))Df(x).
(iii) If f : X → Y1 ×···× Yn is given by f(x)=(f1(x),...,fn(x)), and if
fi is differentiable for each i, then so is f and, in fact,
Df(x)=(Df1(x),..., Dfn(x)).
(iv) If the function f : X1 ×X2 ×···×Xn → Y is given by (x1,...,xn) →
f(x1,...,xn), then the ith partial derivative of f at a1,...,an ∈
X1 ×···× Xn is the derivative of the function g : Xi → Y defined
by g(xi) = f(a1,...,ai−1, xi, ai+1,...,an). This derivative is denoted
Dif(a). Of course, if f is differentiable, then its partial derivatives
all exist and, if we define h = (h1,...,hn), we have
Df(x)h = n
i=1
Dif(x)hi.1.10 Review of Calculus 115
Conversely, if all the partial derivatives of f exist and are continuous
in an open set
U ⊂ X1 × X2 ×···× Xn,
then f is continuously differentiable in U.
(v) If f : X → Y is a bounded linear map, then Df(x) = f for all x ∈ X.
The Cr-norm of an r-times continuously differentiable function f : U →
Y , defined on an open subset U of X, is defined by
fr = f0 + Df0 + ···Drf0
where  0 denotes the usual supremum norm, as well as the operator
norms over U; for example,
f0 = sup u∈U
f(u)
and
Df0 = sup u∈U
 sup
x=1
Df(u)x

.
Also, let us use Cr(U, Y ) to denote the set of all functions f : U → Y such
that fr < ∞. Of course, the set Cr(U, Y ) is a Banach space of functions
with respect to the Cr-norm.
Although the basic definitions of differential calculus extend unchanged
to the Banach space setting, this does not mean that there are no new phe￾nomena in infinite-dimensional spaces. The following examples and exer￾cises illustrate some of the richness of the theory. The basic idea is that
functions can be defined on function spaces in ways that are not available
in the finite-dimensional context. If such a function is defined, then its dif￾ferentiability class often depends on the topology of the Banach space in a
subtle manner.
Example 1.235. Let X = C([0, 1]) and define F : X → X by
F(g)(t) := sin g(t)
(see [65]). We have the following proposition: The function F is continu￾ously differentiable and
(DF(g)h)(t) = (cos g(t))h(t).
To prove it, let us first compute
|F(g + h)(t) − F(g)(t) − DF(g)h(t)|
= |sin(g(t) + h(t)) − sin g(t) − (cos g(t))h(t)|
= |sin g(t) cos h(t) + cos g(t) sin h(t) − sin g(t) − (cos g(t))h(t)|
= |(−1 + cos h(t)) sin g(t)+(−h(t) + sin h(t)) cos g(t)|
≤ F(g)| − 1 + cos h(t)| +  cos ◦g| − h(t) + sin h(t)|
≤
1
2

F(g) h2 +  cos ◦g h2
.116 1. Introduction to Ordinary Differential Equations
This proves that F is differentiable.
The function DF : X → L(X, X) given by g → DF(g) is clearly contin￾uous, in fact,
DF(g1) − DF(g2) = sup
h=1
DF(g1)h − DF(g2)h
= sup
h=1
sup
t
|(cos g1(t))h(t) − (cos g2(t))h(t)|
≤ sup
h=1
sup
t
|h(t)||g1(t) − g2(t)|
= g1 − g2.
Thus F is continuously differentiable, as required.
Example 1.236. Let X := L2([0, 1]) and define F : X → X by
F(g)(t) = sin g(t).
The function F is Lipschitz, but not differentiable.
To prove that F is Lipschitz, simply recall that |sin x − sin y|≤|x − y|
and estimate as follows:
F(g1) − F(g2)2 =
 1
0
|sin g1(t) − sin g2(t)|
2 dt
≤
 1
0
|g1(t) − g2(t)|
2 dt
≤ g1 − g22.
We will show that F is not differentiable at the origin. To this end, let
us suppose that F is differentiable at the origin with derivative DF(0). We
have that F(0) = 0, and, by Proposition (1.234), all directional derivatives
of F at the origin exist. Therefore, it follows that
lims→0
F(sg) − F(0)
s
= lims→0
F(sg)
s = DF(0)g
for all g ∈ L2([0, 1]).
To reach a contradiction, we will first prove that DF(0) is the identity
map on L2([0, 1]). To do this, it suffices to show that DF(0)g = g for every
continuous function g ∈ L2([0, 1]). Indeed, this reduction follows because
the (equivalence classes of) continuous functions are dense in L2([0, 1]).
Let us assume that g is continuous and square integrable. We will show
that the directional derivative of F at the origin in the direction g exists
and is equal to g. In other words, we will show that
lims→0
F(sg)
s = g;1.10 Review of Calculus 117
that is,
lims→0
 1
0



sin(sg(t))
s − g(t)



2
ds = 0. (1.55)
Indeed, let us define
ψs(t) :=



sin(sg(t))
s − g(t)



2
, s> 0
and note that
ψs(t) ≤



sin(sg(t))
s


 + |g(t)|
2
.
Because |sin x|≤|x| for all x ∈ R, we have the estimates
ψs(t) ≤
|sg(t)|
|s| + |g(t)|
2
≤ 4|g(t)|
2.
Moreover, the function t → 4|g(t)|
2 is integrable, and therefore the function
t → ψs(t) is dominated by an integrable function.
If t is fixed, then
lims→0
ψs(t)=0.
To prove this fact, let us observe that |g(t)| < ∞. If g(t) = 0, then ψs(t)=0
for all s and the result is clear. If g(t) = 0, then
ψs(t) =


g(t)
sin(sg(t))
sg(t) − 1



2
=


g(t)



2


sin(sg(t))
sg(t) − 1



2
and again ψs(t) → 0 as s → 0.
We have proved that the integrand of the integral in display (1.55) is
dominated by an integrable function and converges to zero. Hence, the
required limit follows from the dominated convergence theorem and, more￾over, DF(0)g = g for all g ∈ L2([0, 1]).
Because DF(0) is the identity map, it follows that
lim
h→0
F(h) − h
h = 0.
But let us consider the sequence of functions {hn}∞
n=1 ⊂ L2([0, 1]) defined
by
hn(t) := π/2, 0 ≤ t ≤ 1/n,
0, t> 1/n.
Since
hn =
  1
0
(hn(t))2dt1/2
=
 1
n
π2
4
1/2
= 1
√n
π
2
,118 1. Introduction to Ordinary Differential Equations
it follows that hn → 0 as n → ∞. Also, let us note that
F(hn) − hn =
  1
0
|sin hn(t) − hn(t)|
2dt1/2
=
 1
n



1 − π
2



21/2
and therefore
limn→∞
F(hn) − hn
hn = limn→∞
√
1
n (1 − π
2 )
√
1
n
π
2
= 1 − π
2 π
2
= 0.
This contradiction proves that F is not differentiable at the origin. Is F
differentiable at any other point?
Exercise 1.237. Let GL(Rn) denote the set of invertible linear transformations
of Rn and let f : GL(Rn) → GL(Rn) be the function given by f(A) = A−1. Prove
that f is differentiable and compute its derivative.
Exercise 1.238. Consider the evaluation map
eval : Cr
(U, Y ) × U → Y
defined by (f,u) → f(u). Prove that eval is a Cr map. Also, compute its deriva￾tive.
Exercise 1.239. [Omega Lemma] (a) Suppose that f : R → R is a C2 function
such that the quantity supx∈R |f(x)| is bounded. Prove that F : X → X as in
Example 1.236 is C1. (b) The assumption that f is C2 can be replaced by the
weaker hypothesis that f is C1. This is a special case of the omega lemma (see [2,
p. 101]). If M is a compact topological space, U is an open subset of a Banach
space X, and g is in Cr(U, Y ) where Y is a Banach space and r ≥ 1, then the
map Ωg : C0(M,U) → C0(M, Y ) given by Ωg(f) = g ◦ f is Cr and its derivative
is given by
(DΩg(f)h)(m) = Dg(f(m))h(m).
Prove the omega lemma.
1.10.1 The Mean Value Theorem
The mean value theorem for functions of several variables is important. Let
us begin with a special case.
Theorem 1.240. Suppose that [a, b] is a closed interval, Y is a Banach
space, and f : [a, b] → Y is a continuous function. If f is differentiable
on the open interval (a, b) and there is some number M > 0 such that
f
(t) ≤ M for all t ∈ (a, b), then
f(b) − f(a) ≤ M(b − a).1.10 Review of Calculus 119
Proof. Let  > 0 be given and define φ : [a, b] → R by
φ(t) = f(t) − f(a) − (M + )(t − a).
Clearly, φ is a continuous function such that φ(a) = 0. We will show that
φ(b) ≤ .
Define S := {t ∈ [a, b] : φ(t) ≤ }. Since φ(a) = 0, we have that a ∈ S.
In particular S = ∅. By the continuity of φ, there is some number c such
that a<c<b and [a, c) ⊆ S. Moreover, since φ is continuous φ(t) → φ(c)
as t → c. Thus, since φ(t) ≤  for a ≤ t<c, we must have φ(c) ≤  and, in
fact, [a, c] ⊆ S.
Consider the supremum c∗ of the set of all c such that a ≤ c ≤ b and
[a, c] ⊆ S. Let us show that c∗ = b. If c∗ < b, then consider the derivative
of f at c∗ and note that because
lim
h→0
f(c∗ + h) − f(c∗) − f
(c∗)h
h = 0,
there is some h such that c∗ < c∗ + h<b and
f(c∗ + h) − f(c∗) − f
(c∗)h ≤ h.
Set d = c∗ + h and note that
f(d) − f(c∗)≤f(c∗ + h) − f(c∗) − f
(c∗)h + f
(c∗)h
≤ h + Mh
≤ ( + M)(d − c∗).
Moreover, since
f(d) − f(a)≤f(d) − f(c∗) + f(c∗) − f(a)
≤ ( + M)(d − c∗)+(M + )(c∗ − a) + 
≤ ( + M)(d − a) + ,
we have that
f(d) − f(a) − ( + M)(d − a) ≤ ,
and, as a result, d ∈ S, in contradiction to the fact that c∗ is the supremum.
Thus, c∗ = b, as required.
Use the equality c∗ = b to conclude that
f(b) − f(a) ≤ ( + M)(b − a) + 
≤ M(b − a) + (1 + (b − a))
for all  > 0. By passing to the limit as  → 0, we obtain the inequality
f(b) − f(a) ≤ M(b − a),
as required. 120 1. Introduction to Ordinary Differential Equations
Theorem 1.241 (Mean Value Theorem). Suppose that f : X → Y is
differentiable on an open set U ⊆ X with a, b ∈ U and a + t(b − a) ∈ U for
0 ≤ t ≤ 1. If there is some M > 0 such that
sup
0≤t≤1
Df(a + t(b − a)) ≤ M,
then
f(b) − f(a) ≤ Mb − a.
Proof. Define g(t) := f(a + t(b − a)). Clearly, g is differentiable on [0, 1]
and, by the chain rule, g
(t) = Df(a + t(b − a))(b − a). In particular,
g
(t)≤Df(a + t(b − a))b − a ≤ Mb − a.
Here, g : [0, 1] → Y and g
(t) ≤ Mb − a for 0 ≤ t ≤ 1. By the previous
theorem,
g(1) − g(0) ≤ Mb − a,
that is,
f(b) − f(a) ≤ Mb − a. 
1.10.2 Integration in Banach Spaces
This section is a brief introduction to integration on Banach spaces follow￾ing the presentation in [157]. As an application, we will give an alternative
proof of the mean value theorem and a proof of a version of Taylor’s theo￾rem.
Let I denote a closed interval of real numbers and X a Banach space with
norm  . A simple function f : I → X is a function with the following
property: There is a finite cover of I consisting of disjoint subintervals such
that f restricted to each subinterval is constant. Here, each subinterval can
be open, closed, or half open.
A sequence {fn}∞
n=1 of not necessarily simple functions, each mapping I
to X, converges uniformly to a function f : I → X if for each  > 0 there
is an integer N > 0 such that fn(t) − fm(t) <  whenever n, m > N and
t ∈ I.
Definition 1.242. A regulated function is a uniform limit of simple func￾tions.
Lemma 1.243. Every continuous function f : I → X is regulated.
Proof. The function f is uniformly continuous. To see this, consider F :
I ×I → X defined by F(x, y) = f(y)−f(x) and note that F is continuous.
Since the diagonal D = {(x, y) ∈ I × I : x = y} is a compact subset of
I × I (Why?), its image F(D) is compact in X. Hence, for each  > 0,
a finite number of -balls in X cover the image of D. Taking the inverse1.10 Review of Calculus 121
images of the elements of some such covering, we see that there is an open
cover V1,...,Vn of the diagonal in I × I such that if (x, y) ∈ Vi, then
F(x, y) < . For each point (x, x) ∈ D, there is a ball centered at (x, x)
and contained in I × I that is contained in some Vi. By compactness, a
finite number of such balls cover D. Let δ denote the minimum radius of
the balls in this finite subcover. If |x − y| < δ, then (x, y) ∈ Bδ(x, x) and
in fact (x, y) − (x, x) = |y − x| < δ. Thus, (x, y) ∈ Vi for some i in
the set {1,...,n}, and, as a result, we have that F(x, y) < ; that is,
f(y) − f(x) = F(x, y) < , as required.
Let us suppose that I = {x ∈ R : a ≤ x ≤ b}. For each natural number
n, there is some δ > 0 such that if |x − y| < δ, then f(x) − f(y) < 1
n .
Let us define a corresponding simple function fn by fn(x) = f(a) for a ≤
x ≤ a + δ
2 , fn(x) = f(a + δ
2 ) for a + δ
2 < x ≤ a + δ, fn(x) = f(a + δ) for
a + δ<x ≤ a + 3δ
2 , and so on until a + k δ
2 ≥ b. This process terminates
after a finite number of steps because I has finite length. Also, we have the
inequality fn(x) − f(x) < 1
n for all x ∈ I. Thus, the sequence of simple
functions {fn}∞
n=1 converges uniformly to f. 
Definition 1.244. The integral of a simple function f : I → X over the
interval I = [a, b] is defined to be
 b
a
f(t) dt := n
j=1
μ(Ij )vj
where I1,...,In is a partition of I, f|Ij (t) ≡ vj , and μ(Ij ) denotes the
length of the interval Ij .
Proposition 1.245. If f is a simple function on I, then the integral of f
over I is independent of the choice of the partition of I.
Proof. The proof is left as an exercise. 
Proposition 1.246. If f is a regulated function defined on the interval
I = [a, b], and if {fn}∞
n=1 is a sequence of simple functions converging
uniformly to f, then the sequence defined by n →  b
a fn(t) dt converges
in X. Moreover, if in addition {gn}∞
n=1 is a sequence of simple functions
converging uniformly to f, then
limn→∞  b
a
fn(t) dt = limn→∞  b
a
gn(t) dt.
Proof. We will show that the sequence n →  b
a fn(t) dt is Cauchy. For this,
consider the quantity

 b
a
fn(t) dt −
 b
a
fm(t) dt.122 1. Introduction to Ordinary Differential Equations
Using χL to denote the characteristic function on the interval L, we have
that, for some partitions of I and vectors {vi} and {wi},
fn(x) = 
k
i=1
χIi (x)vi, fm(x) = 
l
i=1
χJi (x)wi.
The partitions I1,...,Ik and J1,...,Jl have a common refinement; that
is, there is a partition of the interval I such that each subinterval in the
new partition is contained in one of the subintervals I1,...,Ik, J1,...,Jl.
Let this refinement be denoted by K1,...,Kp and note that
fn(x) = p
i=1
χKi (x)αi, fm(x) = p
i=1
χKi (x)βi.
Also, we have the inequality

 b
a
fn(t) dt −
 b
a
fm(t) dt = 
p
i=1
μ(Ki)αi −p
i=1
μ(Ki)βi
≤ p
i=1
μ(Ki)αi − βi.
There are points ti ∈ Ki so that
p
i=1
μ(Ki)αi − βi = p
i=1
μ(Ki)fn(ti) − fm(ti)
and, because p
i=1 μ(Ki) = b − a,
p
i=1
μ(Ki)fn(ti) − fm(ti) ≤ (b − a) maxi fn(ti) − fm(ti)
≤ (b − a) max x∈I fn(x) − fm(x).
By combining the previous inequalities and using the fact that the sequence
{fn}∞
n=1 converges uniformly, it follows that the sequence n →  b
a fn(t) dt
is a Cauchy sequence and thus converges to an element of X.
Suppose that {gn}∞
n=1 is a sequence of simple functions that converges
uniformly to f, and let us suppose that
 b
a
fn(t) dt → F,  b
a
gn(t) dt → G.
We have the estimates
F − G≤F −
 b
a
fn(t) dt + 
 b
a
fn dt −
 b
a
gn dt + 
 b
a
gn dt − G1.10 Review of Calculus 123
and

 b
a
fn dt −
 b
a
gn dt ≤ (b − a) max x∈I fn(x) − gn(x)
≤ (b − a) max x∈I
(fn(x) − f(x) + f(x) − gn(x)).
The desired result, the equality F = G, follows by passing to the limit on
both sides of the previous inequality. 
In view of the last proposition, we have the following basic definition:
Definition 1.247. Let f be a regulated function on the interval [a, b] and
{fn}∞
n=1 a sequence of simple functions converging uniformly to f in X.
The integral of f denoted  b
a f(t) dt is defined to be the limit of the sequence
n →  b
a fn dt in X.
Proposition 1.248. The functional f →  b
a f(t) dt, defined on the space
of regulated functions, is linear.
Proof. If f and g are regulated on the interval [a, b], with sequences of
simple functions fn → f and gn → g, then cfn + dgn → cf + dg and
 b
a
(cf + dg)(t) dt = limn→∞  b
a
(cfn + dgn)(t) dt.
But, for these simple functions, after a common refinement,
 b
a
cfn + dgn dt = n
i=1
μ(Ii)(cvi + dwi) = c
n
i=1
μ(Ii)vi + d
n
i=1
μ(Ii)wi. 
Proposition 1.249. If λ : X → R is a continuous linear functional and
if f : I → X is regulated, then the composition λf := λ ◦ f : I → R is
regulated, and
λ
 b
a
f(t) dt =
 b
a
(λf)(t) dt.
Proof. If {fn}∞
n=1 is a sequence of simple functions converging uniformly
to f and
fn(x) = 
i
χIi (x)vi,
then
λ(fn(x)) = 
i
χIi (x)λ(vi)
and, in particular, λfn is a simple function for each n. Moreover, λ ◦ f is
regulated by λfn.124 1. Introduction to Ordinary Differential Equations
A continuous linear functional, by definition, has a bounded operator
norm. Therefore, we have that
|λfn(x) − λf(x)| = |λ(fn(x) − f(x))|
≤ λfn(x) − f(x)
and



λ
 b
a
f(t) dt −
 b
a
λf(t) dt



≤



λ
 b
a
f(t) dt − λ
 b
a
fn(t) dt


 +



λ
 b
a
fn(t) dt −
 b
a
λf(t) dt



≤ λ  b
a
f(t) dt −
 b
a
fn(t) dt +



 b
a
λfn(t) dt −
 b
a
λf(t) dt


.
The result follows by passing to the limit as n → ∞. 
Proposition 1.250. If f : [a, b] → X is regulated, then

 b
a
f(t) dt ≤ (b − a) sup
t∈[a,b]
f(t). (1.56)
Proof. Note that the estimate (1.56) is true for simple functions; in fact,
we have

μ(Ii)vi ≤ μ(Ii) sup |vi| ≤ (b − a) sup |vi|.
Because f is regulated, there is a sequence {fn}∞
n=1 of simple functions
converging to f and, using this sequence, we have the following estimates:

 b
a
f(t) dt≤  b
a
f(t) dt −
 b
a
fn(t) dt + 
 b
a
fn(t) dt
≤   b
a
f(t) dt −
 b
a
fn(t) dt + (b − a) supx
fn(x)
≤   b
a
f(t) dt −
 b
a
fn(t) dt
+(b − a) supx
fn(x) − f(x) + (b − a) supx
f(x).
The desired result is obtained by passing to the limit as n → ∞. 
Let us now apply integration theory to prove the mean value theorem. We
will use the following proposition.
Proposition 1.251. Suppose that U is an open subset of X. If f : U → Y
is a smooth function, and x + ty ∈ U for 0 ≤ t ≤ 1, then
f(x + y) − f(x) =  1
0
Df(x + ty)y dt. (1.57)1.10 Review of Calculus 125
Proof. Let λ : Y → R be a continuous linear functional and consider the
function F : [0, 1] → R given by
F(t) = λ(f(x + ty)) =: λf(x + ty).
The functional λ is C1 because it is linear. Also, the composition of smooth
maps is smooth. Thus, F is C1.
By the fundamental theorem of calculus, we have that
F(1) − F(0) =  1
0
F
(t) dt,
or, equivalently,
λ(f(x + y) − f(x)) = λf(x + y) − λf(x)
=
 1
0
λ(Df(x + ty)y) dt
= λ
 1
0
Df(x + ty)y dt.
Here, f(x + y) − f(x) and  1
0 Df(x + ty)y dt are elements of Y , and λ has
the same value on these two points. Moreover, by our construction, this
is true for all continuous linear functionals. Thus, it suffices to prove the
following claim: If u, v are in X and λ(u) = λ(v) for all continuous linear
functionals, then u = v. To prove the claim, set w = u − v and note that
Z := {tw : t ∈ R} is a closed subspace of Y . Moreover, λ0 : Z → R
defined by λ0(tw) = tw is a linear functional on Z such that λ0(tw) =
|t|w = tw. Thus, λ0 = 1, and λ0 is continuous. By the Hahn–Banach
theorem, λ0 extends to a continuous linear functional λ on all of Y . But for
this extension we have, λ(w) = λ(1 · w) = w = 0. Thus, we have w = 0
and u = v. 
With the same hypotheses as in Proposition 1.251, the mean value the￾orem (Theorem 1.241) states that if x + t(z − x) ∈ U for 0 ≤ t ≤ 1, then
f(z) − f(x)≤z − x sup
t∈[0,1]
Df(x + t(z − x)). (1.58)
Proof. By Proposition 1.251 we have that
f(z) − f(x) = 
 1
0
Df(x + t(z − x))(z − x) dt.
Also, the function t → Df(x + t(z − x))(z − x) is continuous. Thus, the
desired result is an immediate consequence of Lemma 1.243 and Proposi￾tion 1.250. 126 1. Introduction to Ordinary Differential Equations
The next theorem is a special case of Taylor’s theorem (see [2, p. 93]
and Exercise 1.253).
Theorem 1.252 (Taylor’s Theorem). Suppose that U is an open subset
of X. If f : U → Y is C1 and x + th ∈ U for 0 ≤ t ≤ 1, then
f(x + h) = f(x) + Df(x)h +
 1
0
(Df(x + th)h − Df(x)h) dt.
Proof. By Proposition 1.251 we have
f(x + h) = f(x) +  1
0
Df(x + th)h dt
= f(x) +  1
0
((Df(x + th)h − Df(x)h) + Df(x)h) dt
= f(x) + Df(x)h +
 1
0
(Df(x + th)h − Df(x)h) dt,
as required. 
Exercise 1.253. (a) Prove the following generalization of Theorem 1.252. Sup￾pose that U is an open subset of X. If f : U → Y is Cr and x + th ∈ U for
0 ≤ t ≤ 1, then
f(x + h) = f(x) + Df(x)h + D2
f(x)h2 + ··· + Dr
f(x)hr
+
 1
0
(1 − t)
r−1
(r − 1)! (Dr
f(x + th)hr − Dr
f(x)hr
) dt.
(b) Suppose that f in part (a) is C2 and x0 ∈ U. Prove the following proposition:
If the closed ball B centered at x0 is contained in U, f(x0) = 0, and Df(x0) = 0,
then there is some positive number c such that |f(x)| ≤ c|x − x0|
2 for all x in B.
Exercise 1.254. (a) Prove that the function f : R3 → R3 given by
f(x, y, z)=(x2
2 , −y2
2 , xz sin(|x + π2
|)
is C2. (b) Let u ∈ R3 have components (a, b, c) and 0, as usual, denote the origin.
Compute f(0), Df(0)u, and D2f(0)(u, u). (c) Is f class C3.
1.11 Contraction
A map transforming a complete metric space into itself that moves each
pair of points closer together has a fixed point. This contraction princi￾ple has far-reaching consequences including the existence and uniqueness
of solutions of differential equations and the existence and smoothness of
invariant manifolds. The basic theory is introduced in this section.1.11 Contraction 127
1.11.1 The Contraction Mapping Theorem
In this section, let us suppose that (X, d) is a metric space. A point x0 ∈ X
is a fixed point of a function T : X → X if T(x0) = x0. The fixed point x0
is called globally attracting if limn→∞ T n(x) = x0 for each x ∈ X.
Definition 1.255. Suppose that T : X → X, and λ is a real number such
that 0 ≤ λ < 1. The function T is called a contraction (with contraction
constant λ) if
d(T(x), T(y)) ≤ λd(x, y)
whenever x, y ∈ X.
The next theorem is fundamental; it states that a contraction, viewed as
a dynamical system, has a globally attracting fixed point.
Theorem 1.256 (Contraction Mapping Theorem). If the function T
is a contraction on the complete metric space (X, d) with contraction con￾stant λ, then T has a unique fixed point x0 ∈ X. Moreover, if x ∈ X, then
the sequence {T n(x)}∞
n=0 converges to x0 as n → ∞ and
d(T n(x), x0) ≤
λn
1 − λd(T(x), x).
Proof. Let us prove first that fixed points of T are unique. Suppose that
T(x0) = x0 and T(x1) = x1. Because T is a contraction, d(T(x0), T(x1)) ≤
λd(x0, x1), and, because x0 and x1 are fixed points, d(T(x0), T(x1)) =
d(x0, x1). Thus, we have that
d(x0, x1) ≤ λd(x0, x1).
If x0 = x1, then d(x0, x1) = 0 and therefore λ ≥ 1, in contradiction.
To prove the existence of a fixed point, let x ∈ X and consider the
corresponding sequence of iterates {T n(x)}∞
n=1. By repeated applications
of the contraction property, it follows that
d(T n+1(x), T n(x)) ≤ λd(T n(x), T n−1(x)) ≤···≤ λnd(T(x), x).
Also, by using the triangle inequality together with this result, we obtain
the inequalities
d(T n+p(x), T n(x)) ≤ d(T n+p(x), T n+p−1(x)) + ··· + d(T n+1(x), T n(x))
≤ (λn+p−1 + ··· + λn)d(T(x), x)
≤ λn(1 + λ + ··· + λp−1)d(T(x), x)
≤
λn
1 − λd(T(x), x). (1.59)
Since 0 ≤ λ < 1, the sequence {λn}∞
n=1 converges to zero, and therefore
{T n(x)}∞
n=1 is a Cauchy sequence. Thus, this sequence converges to some
point x0 ∈ X.128 1. Introduction to Ordinary Differential Equations
We will prove that x0 is a fixed point of the map T. Let us first note
that, because the sequences {T n+1(x)}∞
n=0 and {T n(x)}∞
n=1 are identical,
limn→∞ T n+1(x) = x0. Also, by the contraction property, it follows that T
is continuous and
d(T n+1(x), T(x0)) = d(T(T n(x)), T(x0)) ≤ λd(T n(x), x0).
Therefore, using the continuity of T, we have the required limit
limn→∞ T n+1(x) = limn→∞ T(T n(x)) = T(x0).
To prove the estimate in the theorem, pass to the limit as p → ∞ in the
inequality (1.59) to obtain
d(x0, T n(x)) ≤
λn
1 − λd(T(x), x). 
Exercise 1.257. Suppose that X is a set and n is a positive integer. Prove: If
T is a function, T : X → X, and if T n has a unique fixed point, then T has a
unique fixed point.
Exercise 1.258. For α ≥ 0, let C[α, ∞) denote the Banach space of continuous
functions that are bounded in the supremum norm on the interval [α, ∞). (a) If
g ∈ C[0, ∞),  ∞
1 t|g(t)| dt < ∞ and α > 0 is sufficiently large, prove that the
integral equation
x(t)=1+  ∞
t
(t − s)g(s)x(s) ds
has a unique solution in C[α, ∞). (b) Relate the result of part (a) to the differ￾ential equation ¨x = −g(t)x and the existence of its solutions with specified limits
as t → ∞ (cf. [177, p. 132]).
1.11.2 Uniform Contraction
In this section we will consider contractions depending on parameters and
prove a uniform version of the contraction mapping theorem.
Definition 1.259. Suppose that A is a set, T : X × A → X, and λ ∈ R is
such that 0 ≤ λ < 1. The function T is a uniform contraction if
d(T(x, a), T(y, a)) ≤ λd(x, y)
whenever x, y ∈ X and a ∈ A.
For uniform contractions in a Banach space where the metric is defined
in terms of the Banach space norm by d(x, y) = x − y, we have the
following result (see [65]).1.11 Contraction 129
Theorem 1.260 (Uniform Contraction Theorem). Suppose that X
and Y are Banach spaces, U ⊆ X and V ⊆ Y are open subsets, U¯ denotes
the closure of U, the function T : U¯ × V → U¯ is a uniform contraction
with contraction constant λ, and, for each y ∈ V , let g(y) denote the unique
fixed point of the contraction x → T(x, y) in U¯. If k is a nonnegative integer
and T ∈ Ck(U¯ × V,X), then g : V → X is in Ck(V,X). Also, if T is real
analytic, then so is g.
Proof. We will prove the theorem for k = 0, 1. The proof for k > 1 uses an
induction argument. The analytic case requires a proof of the convergence
of the Taylor series of g.
By the definition of g given in the statement of the theorem, the identity
T(g(y), y) = g(y) holds for all y ∈ V . If k = 0, then
g(y + h) − g(y) = T(g(y + h), y + h) − T(g(y), y)
≤ T(g(y + h), y + h) − T(g(y), y + h)
+ T(g(y), y + h) − T(g(y), y)
≤ λg(y + h) − g(y) + T(g(y), y + h) − T(g(y), y),
and therefore
g(y + h) − g(y) ≤ 1
1 − λT(g(y), y + h) − T(g(y), y).
But T is continuous at the point (g(y), y). Thus, if  > 0 is given, there is
some δ > 0 such that
T(g(y), y + h) − T(g(y), y) <  whenever h < δ.
In other words, g is continuous, as required.
Suppose that k = 1 and consider the function g : V → U¯ given by
g(y) = T(g(y), y). We will prove that g is C1.
The first observation is simple. If g is C1, then, by the chain rule,
Dg(y) = Tx(g(y), y)Dg(y) + Ty(g(y), y).
In other words, if Dg(y) exists, we expect it to be a solution of the equation
z = Tx(g(y), y)z + Ty(g(y), y). (1.60)
We will prove that, for each y ∈ V , the mapping
z → Tx(g(y), y)z + Ty(g(y), y),
on the Banach space of bounded linear transformations from Y to X, is a
contraction. In fact, if z1 and z2 are bounded linear transformations from
Y to X, then
Tx(g(y), y)z1 + Ty(g(y), y) − (Tx(g(y), y)z2 + Ty(g(y), y))
≤ Tx(g(y), y)z1 − z2.130 1. Introduction to Ordinary Differential Equations
Thus, the map is a contraction whenever Tx(g(y), y) < 1. In fact, as we
will soon see, Tx(g(y), y) ≤ λ. Once this inequality is proved, it follows
from the contraction principle that for each y ∈ V equation (1.60) has a
unique solution z(y). The differentiability of the function y → g(y) is then
proved by verifying the limit
lim
h→0
g(y + h) − g(y) − z(y)h
h = 0. (1.61)
To obtain the required inequality Tx(g(y), y) ≤ λ, note that T is C1.
In particular, the partial derivative Tx is a continuous function and
lim
h→0
T(x + h, y) − T(x, y) − Tx(x, y)h
h = 0.
Let ξ ∈ X be such that ξ = 1 and note that for each  > 0, if we set
h = ξ, then we have
Tx(x, y)ξ = 
1

Tx(x, y)h
≤
1


T(x + h, y) − T(x, y) − Tx(x, y)h
+ T(x + h, y) − T(x, y)

≤ T(x + h, y) − T(x, y) − Tx(x, y)h
h +
λh
h .
Passing to the limit as  → 0, we obtain Tx(x, y)ξ ≤ λ, as required.
To prove (1.61), set γ = γ(h) := g(y + h) − g(y). Since, g(y) is a fixed
point of the contraction mapping T, we have
γ = T(g(y) + γ,y + h) − T(g(y), y).
Set
Δ := T(g(y) + γ,y + h) − T(g(y), y) − Tx(g(y), y)γ − Ty(g(y), y)h
and note that
γ = T(g(y) + γ,y + h) − T(g(y), y) − Tx(g(y), y)γ
−Ty(g(y), y)h + Tx(g(y), y)γ + Ty(g(y), y)h
= Tx(g(y), y)γ + Ty(g(y), y)h + Δ.
Also, since T is C1, we have for each  > 0 a δ > 0 such that Δ <
(γ + h) whenever γ < δ and h < δ.
The function h → γ(h) is continuous. This follows from the first part of
the proof since T ∈ C0. Thus, we can find δ1 > 0 so small that δ1 < δ and
γ(h) < δ whenever h < δ1, and therefore
Δ(γ(h), h) ≤ (γ(h) + h) whenever h < δ1.1.11 Contraction 131
For h < δ1, we have
γ(h) = Tx(g(y), y)γ + Ty(g(y), y)h + Δ(γ,h)
≤ λγ + Ty(g(y), y)h + (γ(h) + h)
and, as a result,
(1 − λ − )γ(h) ≤ (Ty(g(y), y) + )h.
If we take  < 1 − λ, then
γ(h) ≤ 1
1 − λ − 
(Ty(g(y), y) + )h := ψh,
and it follows that
Δ(γ(h), h) ≤ (1 + ψ)h, h < δ1, 0 << 1 − λ.
Finally, recall equation (1.60),
z = Tx(g(y), y)z + Ty(g(y), y),
and note that
(I − Tx(g(y), y))(γ(h) − z(y)h) = γ(h) − Tx(g(y), y)γ(h) − Ty(g(y), y)h
= Δ(γ(h), h).
Also, since Tx(g(y), y) <λ< 1, we have
(I − Tx(g(y), y))−1 = I +∞
j=1
Tj
x
and
(I − Tx(g(y), y))−1 ≤ 1
1 − Tx ≤
1
1 − λ.
This implies the inequality
γ(h) − z(y)h ≤ 
1 − λ(1 + ψ)h,
and the limit (1.61) follows.
By an application of the first part of the proof about solutions of contrac￾tions being continuously dependent on parameters, y → z(y) is continuous.
This completes the proof of the theorem for the case k = 1. 132 1. Introduction to Ordinary Differential Equations
Exercise 1.261. Let C[0, ∞) denote the Banach space of continuous functions
on the interval [0, ∞) that are bounded in the supremum norm and E[0, ∞)
the space of continuous functions bounded with respect to the norm |f|E :=
supt≥0 |et
f(t)|. (a) Prove that E[0, ∞) is a Banach space. (b) For f ∈ E[0, ∞)
and φ ∈ C[0, ∞) let
T(φ, f)(t) := 1 +  ∞
t
(t − s)f(s)φ(s) ds.
Prove that T : C[0, ∞) × E[0, ∞) → C[0, ∞) is C1. (c) Let V ⊂ E be the metric
ball centered at the origin with radius 1/2. Prove that there is a C1 function
g : V → C[0, ∞) such that T(g(f), f) = g(f). (d) Show that the space E[0, ∞)
can be replaced by other Banach spaces to obtain similar results. Can it be
replaced by C[0, ∞)?
1.11.3 Fiber Contraction
In this section we will extend the contraction principle to bundles. The
result of this extension, called the fiber contraction theorem [138], is useful
in proving the smoothness of functions that are defined as fixed points of
contractions.
Let X and Y be metric spaces. A map Γ : X × Y → X × Y of the form
Γ(x, y) = (Λ(x), Ψ(x, y)),
where Λ : X → X and Ψ : X × Y → Y , is called a bundle map over
the base Λ with principal part Ψ. Here, the triple (X × Y, X, π), where
π : X × Y → X is the projection π(x, y) = x, is called the trivial bundle
over X with fiber Y .
Definition 1.262. Suppose that μ ∈ R is such that 0 ≤ μ < 1. The
bundle map Γ : X ×Y → X ×Y is called a fiber contraction if the function
y → Ψ(x, y) is a contraction with contraction constant μ for every x ∈ X.
Theorem 1.263 (Fiber Contraction Theorem). Suppose that X and
Y denote metric spaces, and that Γ : X × Y → X × Y is a continuous fiber
contraction over Λ : X → X with principal part Ψ : X × Y → Y . If Λ has
a globally attracting fixed point x∞, and if y∞ is a fixed point of the map
y → Ψ(x∞, y), then (x∞, y∞) is a globally attracting fixed point of Γ.
Remark: The proof does not require the metric spaces X or Y to be
complete.
Proof. Let dX denote the metric for X, let dY denote the metric for Y ,
and let the metric on X × Y be defined by d := dX + dY . We must show
that for each (x, y) ∈ X × Y we have limn→∞ Γn(x, y)=(x∞, y∞) where
the limit is taken with respect to the metric d.1.11 Contraction 133
For notational convenience, let us denote the map y → Ψ(x, y) by Ψx.
Then, for example, we have
Γn+1(x, y) = (Λn+1(x), ΨΛn(x) ◦ ΨΛn−1(x) ◦···◦ Ψx(y))
and, using the triangle inequality, the estimate
d(Γn+1(x, y),(x∞, y∞)) ≤ d(Γn+1(x, y), Γn+1(x, y∞))
+ d(Γn+1(x, y∞),(x∞, y∞)). (1.62)
Note that
d(Γn+1(x, y), Γn+1(x, y∞)) = dY (ΨΛn(x) ◦ ΨΛn−1(x) ◦···◦ Ψx(y),
ΨΛn(x) ◦ ΨΛn−1(x) ◦···◦ Ψx(y∞)).
Moreover, if μ is the contraction constant for the fiber contraction Γ, then
we have
d(Γn+1(x, y), Γn+1(x, y∞)) ≤ μn+1dY (y, y∞),
and therefore, limn→∞ d(Γn(x, y), Γn(x, y∞)) = 0.
For the second summand of (1.62), we have
d(Γn+1(x, y∞),(x∞, y∞)) = dX(Λn+1(x), x∞)
+ dY (ΨΛn(x) ◦···◦ Ψx(y∞), y∞).
By the hypothesis that x∞ is a global attractor, the first summand on the
right-hand side of the last equality converges to zero as n → ∞. Thus, to
complete the proof, it suffices to verify the limit
limn→∞ dY (ΨΛn(x) ◦ ΨΛn−1(x) ◦···◦ Ψx(y∞), y∞)=0. (1.63)
Let us observe that
dY (ΨΛn(x) ◦···◦ Ψx(y∞), y∞) ≤ dY (ΨΛn(x) ◦···◦ Ψx(y∞), ΨΛn(x)(y∞))
+dY (ΨΛn(x)(y∞), y∞)
≤ μdY (ΨΛn−1(x) ◦···◦ Ψx(y∞), y∞)
+dY (ΨΛn(x)(y∞), y∞),
and by induction that
dY (ΨΛn(x) ◦ ΨΛn−1(x) ◦···◦ Ψx(y∞), y∞) ≤ n
j=0
μn−jdY (ΨΛj (x)(y∞), y∞).
For each nonnegative integer m, define am := dY (ΨΛm(x)(y∞), y∞). Each
am is nonnegative and
am = dY (Ψ(Λm(x), y∞), Ψ(x∞, y∞)).134 1. Introduction to Ordinary Differential Equations
Using the continuity of Ψ and the hypothesis that x∞ is a globally attract￾ing fixed point, it follows that the sequence {am}∞
m=0 converges to zero
and is therefore bounded. If A is an upper bound for the elements of this
sequence, then for each m = 0, 1,...,∞ we have 0 ≤ am < A.
Let  > 0 be given. There is some K > 0 so large that
0 ≤ ak <
1
2
(1 − μ)
whenever k ≥ K. Hence, if n ≥ K, then
n
j=0
μn−jaj =
K
−1
j=0
μn−jaj + n
j=K
μn−jaj
≤ A
K
−1
j=0
μn−j +
1
2
(1 − μ)
n
j=K
μn−j
≤ Aμn−K+1
1 − μ
+
1
2
.
Moreover, there is some N ≥ K such that
μn−K+1 < (1 − μ)
2A
whenever n ≥ N. In other words, limn→∞ n
j=0 μn−jaj = 0. 
As mentioned above, the fiber contraction principle is often used to prove
that functions obtained as fixed points of contractions are smooth. We will
use this technique as one method to prove that the flow defined by a smooth
differential equation is smooth, and we will use a similar argument again
when we discuss the smoothness of invariant manifolds. We will codify some
of the ideas that are used in applications of the fiber contraction principle,
and we will discuss a simple application to illustrate the procedure.
The setting for our analysis is given by a contraction Λ : C→C, where C
denotes a closed subset of a Banach space of continuous functions that map
a Banach space X to a Banach space Y . Let α∞ ∈ C denote the unique
fixed point of Λ, and recall that α∞ is globally attracting; that is, if α ∈ C,
then Λn(α) → α∞ as n → ∞.
Define the Banach space of all (supremum norm) bounded continuous
functions from X to the linear maps from X to Y and denote this space
by C(X, L(X, Y )). Elements of C(X, L(X, Y )) are the candidates for the
derivatives of functions in C. Also, let C1 denote the subset of C consisting
of all continuously differentiable functions with bounded derivatives.
The first step of the method is to show that if α ∈ C1, then the derivative
of Λ(α) has the form
(D(Λ(α)))(ξ) = Ψ(α, Dα)(ξ)1.11 Contraction 135
where ξ ∈ X and where Ψ is a map
Ψ : C × C(X, L(X, Y )) → C(X, L(X, Y )).
Next, define the bundle map
Γ : C × C(X, L(X, Y )) →C× C(X, L(X, Y ))
by
(α, Φ) → (Λ(α), Ψ(α, Φ))
and prove that Γ is a continuous fiber contraction. Warning: In the appli￾cations, the continuity of α → Ψ(α, Φ) is usually not obvious. It is easy
to be deceived into believing that a map of the form g → g ◦ α is contin￾uous when g and α are continuous because the definition of the mapping
involves the composition of two continuous functions. To prove that the
map is continuous requires the omega lemma (Exercise 1.239), one of its
relatives, or a new idea (see Theorem 1.265 and [90]).
Finally, pick a point α0 ∈ C1 so that Dα0 ∈ C(X, L(X, Y )), let (φ0, Φ0) =
(α0, Dα0), and define, for all positive integers n,
(φn+1, Φn+1) = Γ(φn, Φn).
By the fiber contraction principle, there is some Φ∞ ∈ C(X, L(X, Y )) such
that limn→∞(φn, Φn)=(α∞, Φ∞). By the construction of Ψ, if n ≥ 0,
then D(φn)=Φn. If the convergence is uniform (or at least uniform on
compact subsets of X), then we obtain the desired result, D(α∞)=Φ∞,
as an application of the following theorem from advanced calculus (see
Exercise 1.268).
Theorem 1.264. If a sequence of differentiable functions is uniformly con￾vergent and if the corresponding sequence of their derivatives is uniformly
convergent, then the limit function of the original sequence is differentiable
and its derivative is the limit of the corresponding sequence of derivatives.
Moreover, we have Φ∞ ∈ C(X, L(X, Y )), and therefore Φ∞ is continuous.
In particular, the fixed point α∞ is continuously differentiable.
We will formulate and prove a simple result to illustrate a typical appli￾cation of the fiber contraction principle. For this, let us consider specifi￾cally the linear space C0(RM, RN ) consisting of all continuous functions
f : RM → RN and let C0(RM, RN ) denote the subspace consisting of all
f ∈ C0(RM, RN ) such that the supremum norm is finite; that is,
f := sup
ξ∈RM
|f(ξ)| < ∞.
Of course, C0(RM, RN ) is a Banach space with the supremum norm. Also,
let B0
ρ(RM, RN ) denote the subset of C0(RM, RN ) such that, for
f ∈ B0
ρ(RM, RN ),136 1. Introduction to Ordinary Differential Equations
the Lipschitz constant of f is bounded by ρ; that is,
Lip(f) := sup
ξ1=ξ2
|f(ξ1) − f(ξ2)|
|ξ1 − ξ2| ≤ ρ.
The set B0
ρ(RM, RN ) is a closed subset of C0(RM, RN ) (see Exercise 1.267).
Hence, B0
ρ(RM, RN ) is a complete metric space with respect to the supre￾mum norm.
In case f ∈ C0(RM, RN ) is continuously differentiable, its derivative
Df is an element of C0(RM, L(RM, RN )), the space of continuous func￾tions from RM to the linear maps from RM to RN . The subset F of
C0(RM, L(RM , RN )) consisting of all elements that are bounded with respect
to the norm
Φ := sup
ξ∈RM
 sup
|v|=1
|Φ(ξ)v|

is a Banach space. The closed metric ball Fρ of radius ρ > 0 centered at
the origin of F (that is, all Φ such that Φ ≤ ρ) is a complete metric space
relative to the norm on F.
Theorem 1.265. If F : RN → RN and G : RM → RN are continuously
differentiable functions, F + G < ∞, DF < 1, and DG < ∞,
then the functional equation F ◦ φ − φ = G has a unique solution α in
B0
ρ(RM, RN ) for every ρ > DG/(1−DF). Moreover, α is continuously
differentiable and Dα < ρ.
Proof. Suppose that ρ > DG/(1 − DF) or DFρ + DG < ρ. If
φ ∈ B0
ρ(RM, RN ), then the function F ◦ φ − G is continuous. Also, we have
that
F ◦ φ − G ≤ sup
ξ∈RM
|F(φ(ξ))| + G ≤ sup
ζ∈RN
|F(ζ)| + G < ∞,
and, by the mean value theorem,
|F(φ(ξ)) − G(ξ) − (F(φ(η)) − G(η))|≤DF|φ(ξ) − φ(η)|
+DG|ξ − η|
≤ (DF Lip(φ) + DG)|ξ − η|,
where DF Lip(φ) + DG < ρ. It follows that the function F ◦ φ − G is
an element of the space B0
ρ(RM, RN ).
Let us define Λ : B0
ρ(RM, RN ) → B0
ρ(RM, RN ) by
Λ(φ)(ξ) := F(φ(ξ)) − G(ξ).
Also, note that by the hypothesis DF < 1. If φ1 and φ2 are in the space
B0
ρ(RM, RN ), then
|Λ(φ1)(ξ) − Λ(φ2)(ξ)| < DFφ1 − φ2;1.11 Contraction 137
that is, Λ is a contraction on the complete metric space B0
ρ(RM, RN ). There￾fore, there is a unique function α ∈ B0
ρ(RM, RN ) such that F ◦ α − α = G.
Moreover, if φ ∈ B0
ρ(RM, RN ), then limn→∞ Λn(φ) = α.
We will prove that the function α is continuously differentiable. To this
end, note that for φ ∈ B0
ρ(RM, RN ) and Φ ∈ Fρ we have
DF(φ(ξ))Φ(ξ) − DG(ξ)≤DFΦ + DG < ρ;
and, using this result, define Ψ : B0
ρ(RM, RN ) × Fρ → Fρ by
Ψ(φ, Φ)(ξ) := DF(φ(ξ))Φ(ξ) − DG(ξ).
The function Φ → Ψ(φ, Φ) is a contraction on Fρ whose contraction con￾stant is less than one and uniform over B0
ρ(RM, RN ); in fact, we have that
Ψ(φ, Φ1)(ξ) − Ψ(φ, Φ2)(ξ)≤DFΦ1 − Φ2.
Thus, we have defined a bundle map
Γ : B0
ρ(RM, RN ) × Fρ → Bρ(RM, RN ) × Fρ
given by
Γ(φ, Φ) := (Λ(φ), Ψ(φ, Φ)),
which is a uniform contraction on fibers. To prove that Γ is a continuous
fiber contraction, it suffices to show that Γ is continuous.
Note that Λ is a contraction, Φ → Ψ(φ, Φ) is an affine function, and
|Ψ(φ, Φ)(ξ) − Ψ(φ0, Φ0)(ξ)|≤DFΦ − Φ0 + Φ0DF ◦ φ − DF ◦ φ0.
Thus, to prove the continuity of Γ, it suffices to show that the function
φ → DF ◦ φ is continuous.
Fix φ0 ∈ B0
ρ(RM, RN ) and note that the function DF is uniformly contin￾uous on the closed ball B centered at the origin in RN with radius φ0+1.
Also, the image of φ0 is in B. By the uniform continuity of DF, for each
 > 0 there is a positive number δ < 1 such that |DF(ξ) − DF(η)| < 
whenever ξ,η ∈ B and |ξ − η| < δ. If φ − φ0 < δ and ξ ∈ RM, then
|φ(ξ)|≤φ − φ0 + φ0 < 1 + φ0;
and therefore, the image of φ is in the ball B. Thus, we have that
DF ◦ φ − DF ◦ φ0 <  whenever φ − φ0 < δ;
that is, DF is continuous at φ0 (cf. Exercise 1.274).
Let Φ∞ denote the unique fixed point of the contraction Φ → Ψ(α, Φ)
over the fixed point α. Also, let us define a sequence in B0
ρ(RM, RN ) × Fρ
as follows: (φ0, Φ0) = (0, 0) and, for each positive integer n,
(φn+1, Φn+1) := Γ(φn, Φn).138 1. Introduction to Ordinary Differential Equations
Note that Dφ0 = Φ0 and, proceeding by induction, if Dφn = Φn, then
Dφn+1 = D(Λ(φn)) = DF ◦ φnDφn − DG
= Ψ(φn, Dφn) = Ψ(φn, Φn)=Φn+1;
that is, Dφn = Φn for all integers n ≥ 0.
By the fiber contraction theorem, we have that
limn→∞ φn = α, limn→∞ Dφn = Φ∞.
The sequence {φn}∞
n=0 converges uniformly to α and the sequence of its
derivatives converges uniformly to Φ∞. By Theorem 1.264 we have that α
is differentiable with derivative Φ∞. Thus, α is continuously differentiable.

Theorem 1.265 is an almost immediate corollary of the uniform contrac￾tion principle (see Exercise 1.270). For more sophisticated applications,
there are at least three approaches to proving the smoothness of a map
obtained by contraction: the uniform contraction principle, the fiber con￾traction principle, and the definition of the derivative. While the main
difficulty in applying the uniform contraction principle is the proof of the
smoothness of the uniform contraction, the main difficulty in applying the
fiber contraction principle is the proof of the continuity of the fiber con￾traction, especially the continuity of the principle part of the fiber contrac￾tion with respect to the base point. The best choice is not always clear;
it depends on the nature of the application and the skill of the applied
mathematician.
Exercise 1.266. Let U denote an open subset of Rn. (a) Prove that the set of
bounded continuous functions from U to Rm is a Banach space with respect to the
supremum norm. (b) Show that the Banach space in part (a) is a complete metric
space with respect to the natural distance function derived from the supremum
norm. (c) Find the distance in the metric space of part (b) between the two
function f and g mapping the open unit disk in R2 to R3 by f(x, y)=(x, y, x2 +
y2) and g(x, y)=(x, y, x4 − y4). (d) Let V be a open ball in Rm and V¯ its
closure. Consider continuous functions from U to V , from U to V¯ and from V¯ to
Rn. Which of these are Banach spaces?
Exercise 1.267. Prove that B0
ρ(RM, RN ) is a closed subset of the Banach space
C0(RM, RN ).
Exercise 1.268. Prove Theorem 1.264.
Exercise 1.269. Give a direct proof of Theorem 1.265 in case F is linear.
Exercise 1.270. Prove Theorem 1.265 using the uniform contraction principle.
Hint: Construct a uniform contraction in RN with parameter space RM such that
the solution of the functional equation is the map that assigns to each point in
the parameter space the corresponding fixed point of the contraction.1.11 Contraction 139
Exercise 1.271. Suppose that F : R → R is continuous, and consider the
function Λ given by φ → F ◦ φ on C0(R, R). Show that C0(R, R) is a metric
space with metric d(f,g) := supξ∈R |f(ξ) − g(ξ)| and Λ : C0(R, R) → C0(R, R).
Construct a bounded continuous function F such that Λ is not continuous. Show
that your Λ is continuous when restricted to C0(R, R).
Exercise 1.272. Derive the smoothness statement in the uniform contraction
theorem as a corollary of the fiber contraction theorem.
Exercise 1.273. In the context of Theorem 1.265, prove that the solution of
the functional equation F ◦ φ − φ = G is Cr (r-times continuously differentiable)
if F and G are Cr, their Cr-norms are finite, and DF < 1. What condition
would be required to prove that the solution is C∞ or Cω (real analytic).
Exercise 1.274. Prove the following lemma: If (X, dX) and (Y, dY ) are met￾ric spaces, f : X → Y is continuous, and K ⊂ X is compact, then f is uni￾formly continuous on K. Moreover, for every  > 0 there is a δ > 0 such that
dY (f(x1), f(x2)) <  whenever dX(x1, x2) < δ and x2 ∈ K. Use this lemma to
prove the continuity of the map φ → DF ◦ φ in the proof of Theorem 1.265. The
lemma leads to only a slight improvement in the proof, but it simplifies the proof
of continuity for many other fiber contractions.
1.11.4 The Implicit Function Theorem
The implicit function theorem is one of the most useful theorems in analysis.
We will prove it as a corollary of the uniform contraction theorem.
Theorem 1.275 (Implicit Function Theorem). Suppose that X, Y ,
and Z are Banach spaces, U ⊆ X, V ⊆ Y are open sets, F : U ×V → Z is a
C1 function, and (x0, y0) ∈ U×V with F(x0, y0)=0. If Fx(x0, y0) : X → Z
has a bounded inverse, then there is a product neighborhood U0×V0 ⊆ U ×V
with (x0, y0) ∈ U0×V0 and a C1 function β : V0 → U0 such that β(y0) = x0
and F(β(y), y) ≡ 0. Moreover, if F(x, y)=0 for (x, y) ∈ U0 × V0, then
x = β(y).
Proof. Define L : Z → X by Lz = [Fx(x0, y0)]−1z and G : U × V → X
by G(x, y) = x − LF(x, y). Note that G is C1 on U × V and F(x, y)=0
if and only if G(x, y) = x. Moreover, we have that G(x0, y0) = x0 and
Gx(x0, y0) = I − LFx(x0, y0) = 0.
Since G is C1, there is a product neighborhood U0 × V1 whose factors
are two metric balls, U0 ⊆ U centered at x0 and V1 ⊆ V centered at y0,
such that
Gx(x, y) <
1
2
whenever (x, y) ∈ U0 × V1.
Let us suppose that the ball U0 has radius δ > 0. Note that the function
given by y → F(x0, y) is continuous and vanishes at y0. Thus, there is a140 1. Introduction to Ordinary Differential Equations
metric ball V0 ⊆ V1 centered at y0 such that
LF(x0, y) <
δ
2
for every y ∈ V0. With this choice of V0, if (x, y) ∈ U0 × V0, then, by the
mean value theorem,
G(x, y) − x0 = G(x, y) − G(x0, y) + G(x0, y) − x0
≤ G(x, y) − G(x0, y) + LF(x0, y)
≤ sup
u∈U0
Gx(u, y)x − x0 +
δ
2 ≤ δ.
In other words, G(x, y) ∈ U¯0; that is, G : U¯0 × V0 → U¯0.
Again, by the mean value theorem, it is easy to see that G is a uniform
contraction; in fact,
G(x1, y) − G(x2, y) ≤ sup
u∈U0
Gx(u, y)x1 − x2
≤
1
2
x1 − x2.
Thus, there is a unique smooth function y → β(y) defined on the open ball
V0 such that β(y0) = x0 and G(β(y), y) ≡ β(y). In particular,
β(y) = β(y) − LF(β(y), y)
and therefore F(β(y), y) ≡ 0, as required. 
1.12 Existence, Uniqueness, and Extension
In this section we will prove the basic existence and uniqueness theorems
for differential equations. We will also prove a theorem on extension of solu￾tions. While the theorems on existence, uniqueness, and extension are the
foundation for theoretical study of ordinary differential equations, there is
another reason to study their proofs. In fact, the techniques used in this
section are very important in the modern development of our subject. In
particular, the implicit function theorem is used extensively in perturbation
theory, and the various extensions of the contraction principle are funda￾mental techniques used to prove the existence and smoothness of invariant
manifolds. We will demonstrate these tools by proving the fundamental
existence theorem for differential equations in two different ways.
Suppose that J ⊆ R, Ω ⊆ Rn, and Λ ⊆ Rm are all open sets, and
f : J × Ω × Λ → Rn1.12 Existence, Uniqueness, and Extension 141
given by (t, x, λ) → f(t, x, λ) is a continuous function. Recall that if λ ∈ Λ,
then a solution of the ordinary differential equation
x˙ = f(t, x, λ) (1.64)
is a differentiable function σ : J0 → Ω defined on some open subinterval
J0 ⊆ J such that
dσ
dt (t) = f(t, σ(t), λ)
for all t ∈ J0. For t0 ∈ J, x0 ∈ Ω, and λ0 ∈ Λ, the initial value problem
associated with the differential equation (1.64) is given by the differential
equation together with an initial value for the solution as follows:
x˙ = f(t, x, λ0), x(t0) = x0. (1.65)
If σ is a solution of the differential equation as defined above such that in
addition σ(t0) = x0, then we say that σ is a solution of the initial value
problem (1.65).
Theorem 1.276. If the function f : J × Ω × Λ → Rn in the differential
equation (1.64) is continuously differentiable, t0 ∈ J, x0 ∈ Ω, and λ0 ∈
Λ, then there are open sets J0 ⊆ J, Ω0 ⊆ Ω, and Λ0 ⊆ Λ such that
(t0, x0, λ0) ∈ J0 × Ω0 × Λ0, and a unique C1 function σ : J0 × Ω0 × Λ0 →
Rn given by (t, x, λ) → σ(t, x, λ) such that t → σ(t, x, λ) is a solution
of the differential equation (1.64) and σ(0, x, λ) = x. In particular, t →
σ(t, x0, λ0) is a solution of the initial value problem (1.65).
Proof. The proof we will give is due to Joel Robbin [214]. Suppose that
σ is a solution of the initial value problem (1.65), δ > 0, and σ is defined
on the interval [t0 − δ, t0 + δ]. In this case, if we define τ := (t − t0)/δ and
z(τ ) = σ(δτ + t0) − x0, then z(0) = 0 and for −1 ≤ τ ≤ 1,
dz
dτ (τ ) = δσ˙(δτ + t0) = δf(δτ + t0, z + x0, λ0), (1.66)
at least if z + x0 ∈ Ω. Conversely, if the differential equation (1.66) has a
solution defined on a subinterval of −1 ≤ τ ≤ 1, then the differential equa￾tion (1.64) has a solution. Thus, it suffices to show the following proposi￾tion: If δ > 0 is sufficiently small, then the differential equation (1.66) has
a solution defined on the interval −1 ≤ τ ≤ 1.
Choose an open ball centered at the origin with radius r that is in Ω and
let U denote the open ball centered at the origin with radius r/2. Define
the Banach spaces
X := {φ ∈ C1([−1, 1], Rn) : φ(0) = 0}, Y := C([−1, 1], Rn)
where the norm on Y is the usual supremum norm, the norm on X is given
by
φ1 = φ + φ
,142 1. Introduction to Ordinary Differential Equations
and φ denotes the first derivative of φ. Also, let X0 denote the open subset
of X consisting of those elements of X whose ranges are in the open ball
at the origin with radius r/2.
Consider the function
F : (−1, 1) × J × U × Λ × X0 → Y
by
F(δ, t, x, λ, φ)(τ ) = φ
(τ ) − δf(δτ + t, φ(τ ) + x, λ).
We will apply the implicit function theorem to F.
We will show that the function F is C1. Since the second summand in the
definition of F is C1 by the omega lemma (see Exercise 1.239), it suffices
to show that the map d given by φ → φ is a C1 map from X to Y .
Note that φ ∈ Y for each φ ∈ X and d is a linear transformation.
Because
dφ≤dφ + φ = φ1,
the linear transformation d is continuous. Since the map d : X → Y is
linear and bounded, it is its own derivative. In particular, d is continuously
differentiable.
If (t0, x0, λ0) ∈ J × U × Λ, then F(0, t0, x0, λ0, 0)(τ ) = 0. Also, if we set
δ = 0 before the partial derivative is computed, then it is easy to see that
Fφ(0, t0, x0, λ0, 0) = d.
In order to show that Fφ(0, t0, x0, λ0, 0) has a bounded inverse, it suffices
to show that d has a bounded inverse. To this end, define L : Y → X by
(Ly)(τ ) =  τ
0
y(s)ds.
Clearly,
(d ◦ L) (y) = y and (L ◦ d) (ψ) = ψ.
Thus, L is an inverse for d. Moreover, since
Ly1 = Ly + (d ◦ L)y
≤ y + y ≤ 2y,
it follows that L is bounded.
By an application of the implicit function theorem to F, we have proved
the existence of a unique smooth function (δ, t, x, λ) → β(δ, t, x, λ), with
domain an open set K0 × J0 × Ω0 × Λ0 containing the point (0, t0, x0, λ0)
and range in X0 such that β(0, t0, x0, λ0) = 0 and
F(δ, t, x, λ, β(δ, t, x, λ)) ≡ 0.1.12 Existence, Uniqueness, and Extension 143
Thus, there is some δ > 0 such that
τ → z(τ, t0, x0, λ0) := β(δ, t0, x0, λ0)(τ )
is the required solution of the differential equation (1.66). Of course, this
solution depends smoothly on τ and all of its parameters. 
We will now consider a proof of Theorem 1.276 that uses the contraction
principle and the fiber contraction theorem. For this, it is convenient to
make a minor change in notation and to introduce a few new concepts.
Instead of working directly with the initial value problem (1.65), we will
study the solutions of initial value problems of the form
x˙ = F(t, x), x(t0) = x0 (1.67)
where there is no dependence on parameters. In fact, there is no loss of gen￾erality in doing so. Note that the initial value problem (1.65) is “equivalent”
to the following system of differential equations:
y˙ = f(t, y, λ), λ˙ = 0, y(t0) = y0. (1.68)
In particular, if we define x = (y, λ) and F(t,(y, λ)) := (f(t, y, λ), 0), then
solutions of the initial value problem (1.65) can be obtained from solutions
of the corresponding initial value problem (1.67) in the obvious manner.
Moreover, smoothness is preserved. Thus, it suffices to work with the initial
value problem (1.67).
Although the existence of a local solution for the initial value prob￾lem (1.67) can be proved using only the continuity of the function F, if F
is merely continuous, then a solution of the initial value problem may not
be unique. A sufficient condition for uniqueness is the requirement that F
is Lipschitz with respect to its second argument; that is, there is a constant
λ > 0 such that for each t ∈ J and for all x1, x2 ∈ Ω,
|F(t, x1) − F(t, x2)| ≤ λ|x1 − x2|
where |x| is the usual norm of x ∈ Rn. We will not prove the most general
possible result; rather we will prove the following version of Theorem 1.276.
Theorem 1.277. If the function F : J × Ω → Rn in the initial value
problem (1.67) is continuous and Lipschitz (with respect to its second argu￾ment), t0 ∈ J, and x0 ∈ Ω, then there are open sets J0 ⊆ J and Ω0 ⊆ Ω
such that (t0, x0) ∈ J0×Ω0 and a unique continuous function σ : J0×Ω0 →
Rn given by (t, x) → σ(t, x) such that t → σ(t, x) is a solution of the dif￾ferential equation x˙ = F(t, x) with σ(t0, x) = x. In particular, t → σ(t, x0)
is the unique solution of the initial value problem (1.67). If, in addition, F
is C1, then so is the function σ.144 1. Introduction to Ordinary Differential Equations
Proof. The function t → x(t) is a solution of the initial value problem if
and only if it is a solution of the integral equation
x(t) = x0 +
 t
t0
F(s, x(s)) ds.
In fact, if dx/dt = F(t, x), then the integral equation is obtained by an
integration. Conversely, if t → x(t) satisfies the integral equation, then the
differential equation
dx
dt = F(t, x(t))
is implied by the fundamental theorem of calculus.
Fix (t0, x0) ∈ J×Ω. Let b(t0, δ) and B(x0, ν) denote metric balls centered
at t0 and x0 with positive radii, respectively δ and ν, such that
¯b(t0, δ) × B¯(x0, ν) ⊆ J × Ω.
Since F is continuous on J × Ω, there is some number M > 0 such that
sup
(t,x)∈b(t0,δ)×B(x0,ν)
|F(t, x)| ≤ M.
Since F is Lipschitz on J × Ω, there is some number λ > 0 such that, for
each t ∈ J and all x1, x2 ∈ Ω,
|F(t, x1) − F(t, x2)| ≤ λ|x1 − x2|.
If F ∈ C1 on J × Ω, then there is some number K > 0 such that
sup
(t,x)∈b(t0,δ)×B(x0,ν)
DF(t, x) ≤ K, (1.69)
where, recall, DF(t, x) is the derivative of the map x → F(t, x) and
DF(t, x) := sup
{v∈Rn:|v|=1}
|DF(t, x)v|
with |x| the usual norm of x ∈ Rn.
Choose δ > 0 so that δλ < min(1, ν
2 ) and δM < ν
2 , define the Banach
space of bounded continuous Rn-valued functions on b(t0, δ) × B(x0, ν
2 )
with the norm
φ = sup
(t,x)∈b(t0,δ)×B(x0, ν
2 )
|φ(t, x)|,
and let X denote the complete metric space consisting of all functions in
this Banach space with range in B¯(x0, ν). In case F is C1, let us agree to
choose δ as above, but with the additional restriction that δK < 1. Finally,
define the operator Λ on X by
Λ(φ)(t, x) = x +
 t
t0
F(s, φ(s, x)) ds. (1.70)1.12 Existence, Uniqueness, and Extension 145
Let us prove that Λ : X → X. Clearly, we have Λ(φ) ∈ C(b(t0, δ) ×
B(x0, ν
2 ), Rn). In view of the inequality
|Λ(φ)(t, x) − x0|≤|x − x0| + 

 t
t0
|F(s, φ(s, x))| ds

≤ |x − x0| + δM
<
1
2
ν +
1
2
ν,
the range of the operator Λ is in B¯(x0, ν), as required.
The operator Λ is a contraction. In fact, if φ1, φ2 ∈ X, then
|Λ(φ1)(t, x) − Λ(φ2)(t, x)| ≤ 

 t
t0
|F(s, φ1(s, x)) − F(s, φ2(s, x))| ds

≤ δλφ1 − φ2,
and therefore
Λ(φ1) − Λ(φ2) ≤ δλφ1 − φ2,
as required. By the contraction principle, Λ has a unique fixed point. This
function is a solution of the initial value problem (1.67) and it is continu￾ously dependent on the initial condition.
If φ∞ denotes the fixed point of Λ, then d
dtφ∞(t, x) = F(t, φ∞(t, x)).
Because the functions φ∞ and F are continuous, it follows that, for each
fixed x ∈ B(x0, ν
2 ), the function t → φ∞(t, x) is C1. To show that φ∞ is C1,
it suffices to show that for each fixed t ∈ b(t0, δ) the function x → φ∞(t, x)
is C1. We will prove this fact using the fiber contraction principle. The idea
for this part of the proof is due to Jorge Sotomayor [237].
Let us define a Banach space consisting of the “candidates” for the deriva￾tives of functions in X with respect to their second arguments. To this end,
let L(Rn, Rn) denote the set of linear transformations of Rn and define the
Banach space
Y := C(b(t0, δ) × B(x0,
ν
2
), L(Rn, Rn))
consisting of all indicated functions that are bounded with respect to the
norm on Y given by
Φ := sup
(t,x)∈b(t0,δ)×B(x0, ν
2 )
Φ(t, x),
where, as defined above,
Φ(t, x) := sup
{v∈Rn:|v|=1}
|Φ(t, x)v|.146 1. Introduction to Ordinary Differential Equations
Let I denote the identity transformation on Rn, DF(t, x) the derivative
of the map x → F(t, x), and define Ψ : X × Y → Y by
Ψ(φ, Φ)(t, x) := I +
 t
t0
DF(s, φ(s, x))Φ(s, x) ds.
Also, define Γ : X × Y → X × Y by
Γ(φ, Φ) := (Λ(φ), Ψ(φ, Φ)).
The function Γ is a continuous bundle map. The proof of this fact is left
to the reader. Let us note, however, that the key result required here is
the continuity of the function φ → Ψ(φ, Φ). The proof of this fact uses the
continuity of DF and φ, and the compactness of the interval ¯b(t0, δ) (see
Exercise 1.274 and the proof of Theorem 1.265).
Let us prove that Γ is a fiber contraction. Recall that we have chosen the
radius of the time interval, δ > 0, so small that δK < 1, where the number
K is defined in equation (1.69). Using this fact, we have
Ψ(φ, Φ1)(t, x) − Ψ(φ, Φ2)(t, x)
= 
 t
t0
DF(s, φ(s, x))(Φ1(s, x) − Φ2(s, x)) ds
< δKΦ1 − Φ2,
as required.
Let φ0(t, x) ≡ x and note that (φ0, I) ∈ X × Y . By the fiber contraction
theorem (Theorem 1.263), the iterates of the point (φ0, I) under Γ converge
to a globally attracting fixed point, namely, (φ∞, Φ∞), where in this case
φ∞ is the solution of the initial value problem (the fixed point of Λ) and
Φ∞ is the unique fixed point of the contraction Φ → Ψ(φ∞, Φ) on Y .
We will prove that Dφ∞(t, ·)=Φ∞(t, ·). (The derivative denoted by
D is the partial derivative with respect to the second variable.) Let us
start with the equation Dφ0(t, x) = I, and for each integer n > 1 define
(φn, Φn) := Γn(φ0, I) so that
Φn+1(t, x) = Ψ(φn, Φn)(t, x) := I +
 t
t0
DF(s, φn(s, x))Φn(s, x) ds,
φn+1(t, x) = x +
 t
t0
F(s, φn(s, x)) ds.
Let us show the identity Dφn(t, ·)=Φn(t, ·) for each integer n ≥ 0. The
equation is true for n = 0. Proceeding by induction on n, let us assume
that the equation is true for some fixed integer n ≥ 0. Because we can
“differentiate under the integral,” the derivative
Dφn+1(t, x) = ∂
∂x(x +
 t
t0
F(s, φn(s, x)) ds)1.12 Existence, Uniqueness, and Extension 147
is clearly equal to
I +
 t
t0
DF(s, φn(s, x))Φn(s, x) ds = Φn+1(t, x),
as required. Thus, we have proved that the sequence {Dφn(t, ·)}∞
n=0 con￾verges to Φ∞(t, ·). Finally, by Theorem 1.264 we have that Dφ∞(t, ·) =
Φ∞(t, ·). 
Exercise 1.278. It is very easy to show that a C2 differential equation has a
C1 flow. Why? We have proved above the stronger result that a C1 differential
equation has a C1 flow. Show that a Cr differential equation has a Cr flow for
r = 2, 3,..., ∞. Also, show that a real analytic differential equation has a real
analytic flow.
So far we have proved that initial value problems have unique solutions
that exist on some (perhaps small) interval containing the initial time. If we
wish to find a larger interval on which the solution is defined, the following
problem arises. Suppose that the initial value problem
x˙ = f(t, x), x(t0) = x0
has a solution t → φ(t) defined on some interval J containing t0. Maybe
the solution is actually defined on some larger time interval J1 ⊇ J. If we
have a second solution ψ(t) defined on J1, then, by our local uniqueness
result, ψ(t) = φ(t) on J. But, we may ask, does ψ(t) = φ(t) on J1? The
answer is yes.
To prove this fact, consider all the open intervals containing J. The union
of all such intervals on which φ(t) = ψ(t) is again an open interval J∗; it
is the largest open interval on which φ and ψ agree. Let us prove that
J∗ ⊇ J1. If not, then the interval J∗ has an end point t1 ∈ J1 that is not
an endpoint of J1. Suppose that t1 is the right-hand endpoint of J∗. By
continuity,
φ(t1) = ψ(t1).
Thus, by our local existence theorem, there is a unique solution of the
initial value problem
x˙ = f(t, x), x(t1) = φ(t1)
defined in some neighborhood of t1. It follows that φ(t) = ψ(t) on some
larger interval. This contradiction implies that J∗ ⊇ J1, as required. In
particular, if a solution extends, then it extends uniquely.
Our existence theorem for solutions of initial value problems gives no
information about the length of the maximal interval of existence. For
example, recall that even if the vector field associated with a differential
equation has no singularities, solutions of the differential equation may not148 1. Introduction to Ordinary Differential Equations
exist for all t ∈ R. The classic example (already mentioned) is the initial
value problem
x˙ = x2, x(0) = 1.
The maximal interval of existence of the solution x(t) = (1 − t)−1 is the
open interval (−∞, 1). Moreover, the solution blows up to infinity at the
finite time t = 1 ( that is, x(t) → ∞ as t → 1−). In particular, the
solution cannot be continuously extended to t = 1. Exercise 1.10 provides
an example where the analogous extension exists.
Determination of the domain on which a solution is defined is often a
difficult problem, but (following the presentation in [140]), the next theorem
describes the behavior of the forward orbit of a solution that is not defined
for the entire time interval on which the differential equation is defined. As
an immediate corollary, there is an analogous result for backward orbits.
Why?
Theorem 1.279. Let U ⊆ Rn and J ⊆ R be open sets, t0 ∈ J, and x0 ∈ U.
Suppose that f : J × U → Rn is a C1 function and φ is the solution of the
initial value problem x˙ = f(t, x) and x(t0) = x0. If β ∈ J, t0 < β, and the
interval I := [t0, β) is the maximal forward interval of existence of φ, then
for every sequence {ti}∞
i=1 ⊂ I such that limi→∞ ti = β either the sequence
{φ(ti)}∞
i=1 is unbounded or it has a subsequence converging to a boundary
point of U.
Proof. Suppose that both alternatives in the statement of the theorem are
false; that is, there is a bounded sequence {ti}∞
i=1 ⊂ I as in the statement
of the theorem with no subsequence converging to a boundary point of U.
Its closure is a compact subset K of U. The strategy of the proof is to show
that in this case the interval of existence [t0, β) is not maximal.
The set [t0, β] × K is compact. Thus, there is some M > 0 such that
|f(t, x)| < M for each (t, x) ∈ [t0, β] × K. Moreover, the function φ :
[t0, β) → K is continuous.
We will show that the function φ extends continuously to the interval
[t0, β]. Note first that φ is uniformly continuous on [t0, β). In fact, if s1, s2 ∈
[t0, β) and s1 < s2, then
|φ(s2) − φ(s1)| =



 s2
s1
f(t, φ(t)) dt


 ≤ M|s2 − s1|. (1.71)
A standard theorem from advanced calculus states that φ extends contin￾uously to [t0, β]. For completeness we will prove this fact for our special
case.
Construct a sequence {tn}∞
n=1 of numbers in the interval [t0, β) that con￾verges to β, and recall that a convergent sequence is Cauchy. By inequal￾ity (1.71), the sequence {φ(tn)}∞
n=1 is also Cauchy. Hence, there is some
ω ∈ R such that φ(tn) → ω as n → ∞.1.12 Existence, Uniqueness, and Extension 149
Let us extend the function φ to the closed interval [t0, β] by defining
φ(β) = ω. We will prove that this extension is continuous. For this, it
suffices to show that if {sj}∞
n=1 is a sequence in [t0, β) that converges to β,
then limj→∞ φ(sj ) = ω. (Why?)
We have that
|φ(sj ) − ω|≤|φ(sj ) − φ(tj )| + |φ(tj ) − ω|.
Let  > 0 be given. If δ = /(2M), then |φ(s) − φ(t)| < /2 whenever
s, t ∈ [t0, β) and |s − t| < δ. Also, because
|sj − tj |≤|sj − β| + |tj − β|,
there is some integer N such that |sj − tj | < δ whenever j ≥ N, and
therefore
|φ(sj ) − ω| ≤ 
2 + |φ(tj ) − ω|
whenever j ≥ N. Moreover, since φ(tj ) → ω as j → ∞, there is some
N1 ≥ N such that |φ(tj ) − ω| < /2 whenever j ≥ N1. In particular, for
j ≥ N1, we have |φ(sj ) − ω| < , and it follows that φ(sj ) → ω. In other
words, φ extends continuously to β.
For t0 ≤ t<β, the function φ is a solution of the differential equation. In
particular, φ is continuously differentiable on [t0, β) and, on this interval,
φ(t) = φ(t0) +  t
t0
f(s, φ(s)) ds.
Moreover, since f is continuous and φ has a continuous extension, the map
s → f(s, φ(s)) is continuous on [t0, β]. Thus, if follows that
φ(β) = φ(t0) + limt→β−
 t
t0
f(s, φ(s)) ds
= φ(t0) +  β
t0
f(s, φ(s)) ds. (1.72)
By the existence theorem for differential equations, there is a number
δ > 0 such that the initial value problem
x˙ = f(t, x), x(β) = φ(β)
has a solution t → ψ(t) defined on the interval (β − δ, β + δ) ⊆ J. Let us
use this fact to define the continuous function γ : [t0, β + δ) → Rn by
γ(t) = 
φ(t), ift0 ≤ t ≤ β,
ψ(t), if β<t<β + δ.150 1. Introduction to Ordinary Differential Equations
For t0 ≤ t ≤ β, we have that
γ(t) = φ(t0) +  t
t0
f(s, γ(s)) ds. (1.73)
Also, in view of equation (1.72), if β<t<β + δ, then
γ(t) = φ(β) +  t
β
f(s, γ(s)) ds
= φ(t0) +  t
t0
f(s, γ(s)) ds.
In other words, equality (1.73) is valid on the interval [t0, β + δ). It follows
that γ is a solution of the initial value problem in the statement of the
theorem that extends the forward solution φ to the interval [t0, β +δ). This
violates the maximality of [t0, β). 2
Homogeneous Linear Systems
This chapter contains basic theory for differential equations of the form
x˙ = A(t)x, x ∈ Rn, (2.1)
where t → A(t) is a smooth n × n matrix-valued function. Such systems
appear in nonlinear differential equations
x˙ = A(t)x + f(x,t), x ∈ Rn,
where f is a smooth function such that f(0,t) = fx(0,t) ≡ 0. With this
hypothesis on f, the associated Eq. (2.1) is called the associated homoge￾neous linear system. It is the linearization of the latter differential equation
along its zero solution t → φ(t) ≡ 0.
In the important special case where t → A(t) is a constant function,
the solution of ˙x = Ax is reduced to a problem in linear algebra. Also,
by defining the matrix exponential, the flow of this autonomous system is
associated with a one-parameter linear group with generator A.
Although the behavior of the general nonautonomous system ˙x = A(t)x
is not completely understood, the special case where t → A(t) is a periodic
matrix-valued function is reducible to the constant matrix case. This result
is obtained by developing Floquet theory, a subject that will appear again
in the discussion of the stability of periodic nonhomogeneous systems. In
particular, Floquet theory is used in a stability analysis of the inverted
pendulum (see Section 6.5).
Because linear systems theory is so well developed, it is used extensively
in many areas of applied science. For example, linear systems theory is an
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
C. Chicone, Ordinary Differential Equations with Applications, Texts in
Applied Mathematics 34, https://doi.org/10.1007/978-3-031-51652-8 2
151152 2. Homogeneous Linear Systems
essential tool for electromagnetics, circuit theory, and the theory of vibra￾tion. In addition, the results of this chapter are a fundamental component
of control theory.
2.1 Gronwall’s Inequality
Continuity properties of matrix-valued functions are determined by viewing
the space of n × n matrices as Rn2
; that is, every matrix is viewed as an
element in the Cartesian space by simply listing the rows of the matrix
consecutively to form a row vector of length n2. An important general
inequality is proved. It is used in the next section to show that solutions of
linear systems cannot blow up in finite time.
Theorem 2.1 (Gronwall’s Inequality). Suppose that a<b and let α,
φ, and ψ be nonnegative continuous functions defined on the interval [a,b].
Moreover, suppose that α is differentiable on (a,b) with nonnegative con￾tinuous derivative α˙ . If, for all t ∈ [a,b],
φ(t) ≤ α(t) +  t
a
ψ(s)φ(s) ds, (2.2)
then
φ(t) ≤ α(t)e
 t
a ψ(s) ds (2.3)
for all t ∈ [a,b].
Proof. Assume for the moment that α(a) > 0. In this case α(t) ≥ α(a) > 0
on the interval [a,b].
The function on the interval [a,b] defined by t → α(t) +  t
a ψ(s)φ(s) ds
is positive and exceeds φ. Thus, we have that
φ(t)
α(t) +  t
a ψ(s)φ(s) ds
≤ 1.
Multiply both sides of this inequality by ψ(t), add and subtract ˙α(t) in the
numerator of the resulting fraction, rearrange the inequality, and use the
obvious estimate to obtain the inequality
α˙(t) + ψ(t)φ(t)
α(t) +  t
a ψ(s)φ(s) ds
≤
α˙(t)
α(t) + ψ(t),
which, when integrated over the interval [a,t], yields the inequality
ln 
α(t) +  t
a
ψ(s)φ(s) ds
− ln(α(a)) ≤
 t
a
ψ(s) ds + ln(α(t)) − ln(α(a)).2.2 Existence Theory 153
After we exponentiate both sides of this last inequality and use hypothe￾sis (2.2), we find that, for each t in the interval [a,b],
φ(t) ≤ α(t)e
 t
a ψ(s) ds. (2.4)
Finally, for the case α(a) = 0, we have the inequality
φ(t) ≤ (α(t) + ) +  t
a
ψ(s)φ(s) ds
for each  > 0. As a result of what we have just proved, we have the estimate
φ(t) ≤ (α(t) + )e
 t
a ψ(s) ds.
The desired inequality follows by passing to the limit (for each fixed t ∈
[a,b]) as  → 0. ✷
Exercise 2.2. What can you say about a continuous function f : R → [0, ∞) if
f(x) ≤
 x
0
f(t) dt?
Exercise 2.3. Prove the “specific Gronwall lemma” [227]: If, for t ∈ [a, b],
φ(t) ≤ δ2(t − a) + δ1
 t
a
φ(s) ds + δ3,
where φ is a nonnegative continuous function on [a, b], and δ1 > 0, δ2 ≥ 0, and
δ3 ≥ 0 are constants, then
φ(t) ≤
δ2
δ1
+ δ3

e
δ1(t−a) − δ2
δ1
.
Exercise 2.4. Suppose a is a positive constant and |x˙| ≤ ax. Prove that
x(τ )e
−a|t−τ| ≤ x(t) ≤ x(τ )e
a|t−τ|
.
Exercise 2.5. Show that the system
x˙ = y − xy2
, y˙ = x + x2
y
is complete; that is, every solution exists for all time.
2.2 Existence Theory
By general existence theory for ordinary differential equations, the initial
value problem
x˙ = A(t)x, x(t0) = x0 (2.5)
has a unique solution that exists on some open interval containing t0. The
following theorem states that this open interval can be extended to the
domain of A.154 2. Homogeneous Linear Systems
Theorem 2.6. If t → A(t) is continuous on the interval α<t<β and if
α<t0 < β (maybe α = −∞ or β = ∞), then the solution of the initial
value problem (2.5) is defined on the open interval (α,β).
Proof. Because the continuous function t → A(t) is bounded on each
compact subinterval of (α,β), it is easy to see that the function (t,x) →
A(t)x is locally Lipschitz with respect to its second argument. Consider the
solution t → φ(t) of the initial value problem (2.5) given by the general
existence theorem (Theorem 1.277) and let J0 denote its maximal interval
of existence. Suppose that J0 does not contain (α,β). For example, suppose
that the right-hand end point b of J0 is less than β. We will show that this
assumption leads to a contradiction. The proof for the left-hand end point
is similar.
If t ∈ J0, then we have
φ(t) − φ(t0) =  t
t0
A(s)φ(s) ds.
By the continuity of A and the compactness of [t0,b], there is some M > 0
such that 	A(t)	 ≤ M for all t ∈ [t0,b]. (The notation 	 	 is used for the
matrix norm corresponding to some norm | | on Rn.) Thus, for t ∈ J0, we
have the following inequality:
|φ(t)|≤|x0| +
 t
t0
	A(s)	|φ(s)| ds
≤ |x0| +
 t
t0
M|φ(s)| ds.
In addition, by Gronwall’s inequality, with ψ(t) := M, we have
|φ(t)|≤|x0|e
M  t
t0 ds = |x0|eM(t−t0)
.
Thus, |φ(t)| is uniformly bounded on [t0,b).
Because the boundary of Rn is empty, it follows from the extension the￾orem that |φ(t)|→∞ as t → b−, in contradiction to the existence of the
uniform bound. ✷
Exercise 2.7. UseGronwall’sinequality to prove the followingimportantinequal￾ity: Ift → β(t)andt → γ(t)aresolutionsof thesmoothdifferentialequation ˙x = f(x)
and both are defined on the time interval [0, T], then there is a constant L > 0 such
that
|β(t) − α(t)|≤|β(0) − α(0)|e
Lt.
Thus, two solutions diverge from each other at most exponentially fast. Also,
if the solutions have the same initial condition, then they coincide. Therefore,
the result of this exercise provides an alternative proof of the general uniqueness
theorem for differential equations.2.3 Principle of Superposition 155
Exercise 2.8. (a) Prove that if A is a linear transformation of Rn and f : Rn →
Rn is a (smooth) function such that |f(x)| ≤ M|x| + N for positive constants M
and N, then the differential equation ˙x = Ax + f(x) has a complete flow. (b)
Show that the flow of the system
x˙ = f(x, y), y˙ = g(x, y)
does not blow up in finite time if there are positive constants M and N such that
|f(x, y)| < M and |g(x, y)| ≤ M|y|
2 + N.
Exercise 2.9. Suppose that X(·, λ) and Y (·, λ) are vector fields on Rn with
parameter λ ∈ R, the vector fields agree to order N in λ (that is, X(x, λ) =
Y (x, λ) + O(λN+1), and the functions t → x(t, λ) and t → y(t, λ) are solutions
of the corresponding differential equations ˙x = X(x, λ) and ˙y = X(y, λ). If these
solutions have initial conditions at t = 0 that agree to order N in λ and both are
defined on the interval [0, T], prove that x(T,λ) and y(T,λ) agree to order N in
λ. Hint: First prove the result for N = 0.
2.3 Principle of Superposition
The foundational result about linear homogeneous systems is the principle
of superposition: The sum of two solutions is again a solution. A precise
statement of this principle is the content of the next proposition.
Proposition 2.10. If the homogeneous system (2.1) has two solutions φ1(t)
and φ2(t), each defined on some interval (a,b), and if λ1 and λ2 are num￾bers, then t → λ1φ1(t) + λ2φ2(t) is also a solution defined on the same
interval.
Proof. To prove the proposition, we use the linearity of the differential
equation. In fact, we have
d
dt(λ1φ1(t) + λ2φ2(t)) = λ1φ˙
1(t) + λ2φ˙
2(t)
= λ1A(t)φ1(t) + λ2A(t)φ2(t)
= A(t)(λ1φ1(t) + λ2φ2(t)).
✷
As a natural extension of the principle of superposition, we will prove
that the set of solutions of the homogeneous linear system (2.1) is a finite￾dimensional vector space of dimension n.
Definition 2.11. A set of n solutions of the homogeneous linear differ￾ential equation (2.1), all defined on the same open interval J, is called a
fundamental set of solutions on J if the solutions are linearly independent
functions on J.156 2. Homogeneous Linear Systems
Proposition 2.12. If t → A(t) is defined on the interval (a,b), then the
system (2.1) has a fundamental set of solutions defined on (a,b).
Proof. If c ∈ (a,b) and e1,..., en denote the usual basis vectors in Rn,
then there is a unique solution t → φi(t) such that φi(c) = ei for i =
1,...,n. Moreover, by Theorem 2.6, each function φi is defined on the
interval (a,b). Let us assume that the set of functions {φi : i = 1,...,n} is
linearly dependent and derive a contradiction. In fact, if there are scalars αi,
i = 1,...,n, not all zero, such that n
i=1 αiφi(t) ≡ 0, then n
i=1 αiei ≡ 0.
In view of the linear independence of the usual basis, this is the desired
contradiction. ✷
Proposition 2.13. If F is a fundamental set of solutions of the linear
system (2.1) on the interval (a,b), then every solution defined on (a,b) can
be expressed as a linear combination of the elements of F.
Proof. Suppose that F = {φ1,...,φn}. Pick c ∈ (a,b). If t → φ(t) is
a solution defined on (a,b), then φ(c) and φi(c), for i = 1,...,n, are all
vectors in Rn. We will show that the set B := {φi(c) : i = 1,...,n}
is a basis for Rn. If not, then there are scalars αi, i = 1,...,n, not all
zero, such that n
i=1 αiφi(c) = 0. Thus, y(t) := n
i=1 αiφi(t) is a solution
with initial condition y(c) = 0. But the zero solution has the same initial
condition. Thus, y(t) ≡ 0, and therefore n
i=1 αiφi(t) ≡ 0. This contradicts
the hypothesis that F is a linearly independent set, as required.

Using the basis B, there are scalars β1,...,βn ∈ R such that φ(c) = n
i=1 βiφi(c). It follows that both φ and n
i=1 βiφi are solutions with the
same initial condition, and, by uniqueness, φ = n
i=1 βiφi. ✷
Definition 2.14. An n × n matrix function t → Ψ(t), defined on an open
interval J, is called a matrix solution of the homogeneous linear system (2.1)
if each of its columns is a (vector) solution. A matrix solution is called
a fundamental matrix solution if its columns form a fundamental set of
solutions. In addition, a fundamental matrix solution t → Ψ(t) is called
the principal fundamental matrix solution at t0 ∈ J if Ψ(t0) = I.
If t → Ψ(t) is a matrix solution of the system (2.1) on the interval J,
then Ψ(˙ t) = A(t)Ψ(t) on J. By Proposition 2.12, there is a fundamental
matrix solution. Moreover, if t0 ∈ J and t → Φ(t) is a fundamental matrix
solution on J, then (by the linear independence of its columns) the matrix
Φ(t0) is invertible. It is easy to see that the matrix solution defined by
Ψ(t) := Φ(t)Φ−1(t0) is the principal fundamental matrix solution at t0.
Thus, system (2.1) has a principal fundamental matrix solution at each
point in J.
Definition 2.15. The state transition matrix for the homogeneous linear
system (2.1) on the open interval J is the family of fundamental matrix
solutions t → Ψ(t,τ ) parametrized by τ ∈ J such that Ψ(τ,τ ) = I, where
I denotes the n × n identity matrix.2.3 Principle of Superposition 157
Proposition 2.16. If t → Φ(t) is a fundamental matrix solution for the
system (2.1) on J, then Ψ(t,τ ) := Φ(t)Φ−1(τ ) is the state transition matrix.
Also, the state transition matrix satisfies the Chapman–Kolmogorov iden￾tities
Ψ(τ,τ ) = I, Ψ(t,s)Ψ(s,τ ) = Ψ(t,τ )
and the identities
Ψ(t,s)
−1 = Ψ(s,t), ∂Ψ
∂s (t,s) = −Ψ(t,s)A(s).
Proof. See Exercise 2.17. ✷
A two-parameter family of operator-valued functions that satisfies the
Chapman–Kolmogorov identities is called an evolution family.
In the case of constant coefficients, that is, in case t → A(t) is a constant
function, the corresponding homogeneous linear system is autonomous,
and therefore its solutions define a flow. This result also follows from the
Chapman–Kolmogorov identities.
To prove the flow properties, let us show first that if t → A(t) is a
constant function, then the state transition matrix Ψ(t,t0) depends only
on the difference t−t0. In fact, since t → Ψ(t,t0) and t → Ψ(t+s,t0+s) are
both solutions satisfying the same initial condition at t0, they are identical.
In particular, with s = −t0, we see that Ψ(t,t0) = Ψ(t − t0, 0). If we define
φt := Ψ(t, 0), then using the last identity together with the Chapman–
Kolmogorov identities we find that
Ψ(t + s, 0) = Ψ(t, −s) = Ψ(t, 0)Ψ(0, −s) = Ψ(t, 0)Ψ(s, 0).
Thus, we recover the group property φt+s = φtφs. Since, in addition, φ0 =
Ψ(0, 0) = I, the family of operators φt defines a flow. In this context, φt is
also called an evolution group.
If t → Φ(t) is a fundamental matrix solution for the linear system (2.1)
and v ∈ Rn, then t → Φ(t)v is a (vector) solution. Moreover, every solution
is obtained in this way. In fact, if t → φ(t) is a solution, then there is
some v such that Φ(t0)v = φ(t0). (Why?) By uniqueness, we must have
Φ(t)v = φ(t). Also, note that Ψ(t,t0)v has the property that Ψ(t0,t0)v = v.
In other words, Ψ “transfers” the initial state v to the final state Ψ(t,t0)v.
Hence, the name “state transition matrix.”
Exercise 2.17. Prove Proposition 2.16.
Exercise 2.18. [Cocycles] Suppose that φt is a flow on Rn. A cocycle over φt
is a family of functions, each mapping R×Rn to the set of linear transformations
of Rn such that Φ(0, u) = I and Φ(t + s, u) = Φ(t, φs(u))Φ(s, u). (To learn more
about why cocycles are important, see [52].) Suppose ˙u = f(u) is a differential
equation on Rn with flow φt. Show that the family of principal fundamental
matrix solutions Φ(t, u) of the family of variational equations ˙w = Df(φt(u))w
is a cocycle over the flow φt.158 2. Homogeneous Linear Systems
Exercise 2.19. [Time-dependent vector fields] The solution of a nonautonomous
differential equation ˙x = X(t, x) is an evolution family φt,s; that is, t → φt,s(ξ)
is the solution with initial condition φs,s(ξ) = ξ. (a) Prove that φ−t,t τ (ξ) +
φ−t,t x (ξ)X(t, ξ) = 0, where the subscripts denote partial derivatives—the reason
why superscripts are used to denote the evolution variables. Hint: Differentiate
the left-hand side of the Chapman–Kolmogorov identity φ−t,τφτ,t = φ−t,t with
respect to τ . (b) Define F(t, ξ) = φ−t,t(ξ). Prove that F(0, ξ) = ξ and
Ft(t, ξ) + Fx(t, x)X(t, x) = −X(−t, F(t, ξ)).
(c) Suppose that X is 2T-periodic in t. Prove that ξ lies on a periodic orbit if
and only if F(T,ξ) = ξ. (d) Suppose that X is 2T-periodic in t and X(t, x) =
−X(−t, x). Show that every orbit is periodic.
The linear independence of a set of solutions of a homogeneous linear
differential equation can be determined by checking the independence of a
set of vectors obtained by evaluating the solutions at just one point. This
useful fact is perhaps most clearly expressed by Liouville’s formula, which
has many other implications.
Proposition 2.20 (Liouville’s Formula). Suppose that t → Φ(t) is a
matrix solution of the homogeneous linear system (2.1) on the open interval
J. If t0 ∈ J, then
det Φ(t) = det Φ(t0)e
 t
t0 tr A(s) ds
where det denotes determinant and tr denotes trace. In particular, Φ(t) is a
fundamental matrix solution if and only if the columns of Φ(t0) are linearly
independent.
Proof. The matrix solution t → Φ(t) is a differentiable function. Thus, we
have that
lim
h→0
1
h[Φ(t + h) − (I + hA(t))Φ(t)] = 0.
In other words, using the “little oh” notation,
Φ(t + h)=(I + hA(t))Φ(t) + o(h). (2.6)
(The little oh has the following meaning: f(x) = g(x) + o(h(x)) if
lim
x→0+
|f(x) − g(x)|
h(x) = 0.
Thus, we should write o(±h) in equation (2.6), but this technicality is not
important in this proof.)
By the definition of the determinant of an n × n matrix, that is, if B :=
(bij ), then
det B = 
σ
sgn(σ)
n
i=1
bi,σ(i),2.3 Principle of Superposition 159
and the following result: The determinant of a product of matrices is the
product of their determinants, we have that
det Φ(t + h) = det(I + hA(t)) det Φ(t) + o(h)
= (1 + h tr A(t)) det Φ(t) + o(h),
and therefore
d
dt det Φ(t) = tr A(t) det Φ(t).
Integration of this last differential equation gives the desired result. ✷
Exercise 2.21. Find a fundamental matrix solution of the system
x˙ =
 1 −1/t
1 + t −1

x, t > 0.
Hint: x(t) = 1
t

is a solution.
Exercise 2.22. Suppose that every solution of ˙x = A(t)x is bounded for t ≥ 0
and let Φ(t) be a fundamental matrix solution. Prove that Φ−1(t) is bounded for
t ≥ 0 if and only if the function t →  t
0 tr A(s) ds is bounded below. Hint: The
inverse of a matrix is the adjugate of the matrix divided by its determinant.
Exercise 2.23. Suppose that the linear system ˙x = A(t)x is defined on an
open interval containing the origin whose right-hand end point is ω ≤ ∞ and the
norm of every solution has a finite limit as t → ω. Show that there is a solution
converging to zero as t → ω if and only if  ω
0 tr A(s) ds = −∞. Hint: A matrix
has a non-trivial kernel if and only if its determinant is zero (cf. [131]).
Exercise 2.24. [Transport Theorem] Let φt denote the flow of the system ˙x =
f(x), x ∈ Rn, and let Ω be a bounded region in Rn. Define
V (t) = 
φt(Ω)
dx1dx2 ··· dxn
and recall that the divergence of a vector field f = (f1, f2,...,fn) on Rn with
the usual Euclidean structure is
div f = n
i=1
∂fi
∂xi
.
(a) Use Liouville’s theorem and the change of variables formula for multiple
integrals to prove that
V˙ (t) = 
φt(Ω)
div f(x)dx1dx2 ··· dxn.
(b) Prove: The flow of a vector field whose divergence is everywhere negative
contracts volume.160 2. Homogeneous Linear Systems
(c) Suppose that g : Rn × R → R and, for notational convenience, let dx =
dx1dx2 ··· dxn. Prove the transport theorem:
d
dt 
φt(Ω)
g(x, t) dx =

φt(Ω)
gt(x, t) + div(gf)(x, t) dx.
(d) Suppose that the mass in every open set remains unchanged as it is moved
by the flow (that is, mass is conserved) and let ρ denote the corresponding mass￾density. Prove that the density satisfies the equation of continuity
∂ρ
∂t + div(ρf)=0.
(e) The flow of the system ˙x = y, ˙y = x is area preserving. Show directly that
the area of the unit disk is unchanged when it is moved forward two time units
by the flow.
Exercise 2.25. Construct an alternate proof of Liouville’s formula for the
n-dimensional linear system ˙x = A(t)x with fundamental matrix det Φ(t) by
differentiation of the function t → det Φ(t) using the chain rule. Hint: Compute d
dt det Φ(t) directly as a sum of n determinants of matrices whose components are
the components of Φ(t) and their derivatives with respect to t. For this computa￾tion note that the determinant is a multilinear function of its rows (or columns).
Use the multilinearity with respect to rows. Substitute for the derivatives of com￾ponents of Φ using the differential equation and use elementary row operations
to reduce each determinant in the sum to a diagonal component of A(t) times
det Φ(t).
2.4 Linear Equations with Constant Coefficients
In this section we will consider the homogeneous linear system
x˙ = Ax, x ∈ Rn (2.7)
where A is a real n × n (constant) matrix. We will show how to reduce
the problem of constructing a fundamental set of solutions of system (2.7)
to a problem in linear algebra. In addition, we will see that the principal
fundamental matrix solution at t = 0 is given by the exponential of the
matrix tA just as the fundamental scalar solution at t = 0 of the scalar
differential equation ˙x = ax is given by t → eat.
Let us begin with the essential observation of the subject: The solutions of
system (2.7) are intimately connected with the eigenvalues and eigenvectors
of the matrix A. To make this statement precise, let us recall that a complex
number λ is an eigenvalue of A if there is a complex nonzero vector v such
that Av = λv. In general, the vector v is called an eigenvector associated
with the eigenvalue λ if Av = λv. Moreover, the set of all eigenvectors
associated with an eigenvalue forms a vector space. Because a real matrix
can have complex eigenvalues, it is convenient to allow for complex solutions2.4 Linear Equations with Constant Coefficients 161
of the differential equation (2.7). Indeed, if t → u(t) and t → v(t) are real
functions, and if t → φ(t) is defined by φ(t) := u(t) + iv(t), then φ is called
a complex solution of system (2.7) provided that ˙u + iv˙ = Au + iAv. Of
course, if φ is a complex solution, then we must have ˙u = Au and ˙v = Av.
Thus, it is clear that φ is a complex solution if and only if its real and
imaginary parts are real solutions. This observation is used in the next
proposition.
Proposition 2.26. Suppose that A be a real n × n matrix.
(1) The function given by t → eλtv is a real solution of the differential
equation x˙ = Ax if and only if λ ∈ R, v ∈ Rn, and Av = λv.
(2) If v 
= 0 is an eigenvector of A with eigenvalue λ = α + iβ such
that β 
= 0, then the imaginary part of v is not zero. In this case, if
v = u + iw ∈ Cn, then there are two real solutions of x˙ = Ax:
t → eαt[(cos βt)u − (sin βt)w],
t → eαt[(sin βt)u + (cos βt)w].
Moreover, these solutions are linearly independent.
Proof. If Av = λv, then
d
dt(eλtv) = λeλtv = eλtAv = Aeλtv.
In particular, the function t → eλtv is a solution.
If λ = α + iβ and β 
= 0, then, because A is real, v must be of the form
v = u + iw for some u,w ∈ Rn with w 
= 0. The real and imaginary parts
of the corresponding solution
eλtv = e(α+iβ)t
(u + iw)
= eαt(cos βt + isin βt)(u + iw)
= eαt[(cos βt)u − (sin βt)w + i((sin βt)u + (cos βt)w)]
are real solutions of the system (2.7). To show that these real solutions
are linearly independent, suppose that some linear combination of them
with coefficients c1 and c2 is identically zero. Evaluation at t = 0 and at
t = π/(2β) yields the equations
c1u + c2w = 0, c2u − c1w = 0.
By elimination of u we find that (c2
1 + c2
2)w = 0. Since w 
= 0, both coeffi￾cients must vanish. This proves (2).
Finally, we will complete the proof of (1). Suppose that λ = α + iβ and
v = u+iw. If eλtv is real, then β = 0 and w = 0. Thus, in fact, λ and v are162 2. Homogeneous Linear Systems
real. On the other hand, if λ and v are real, then eλtv is a real solution. In
this case,
λeλtv = Aeλtv,
and we have that λv = Av. ✷
A fundamental matrix solution of system (2.7) can be constructed explic￾itly if the eigenvalues of A and their multiplicities are known. To illustrate
the basic idea, let us suppose that Cn has a basis B := {v1,...,vn} con￾sisting of eigenvectors of A, and let {λ1,...,λn} denote the corresponding
eigenvalues. For example, if A has n distinct eigenvalues, then the set con￾sisting of one eigenvector corresponding to each eigenvalue is a basis of Cn.
At any rate, if B is a basis of eigenvectors, then there are n corresponding
solutions given by
t → eλit
vi, i = 1,...,n,
and the matrix
Φ(t)=[eλ1t
v1,...,eλnt
vn],
which is partitioned by columns, is a matrix solution. Because det Φ(0) 
=
0, this solution is a fundamental matrix solution, and moreover Ψ(t) :=
Φ(t)Φ−1(0) is the principal fundamental matrix solution of (2.7) at t = 0.
A principal fundamental matrix for a real system is necessarily real. To
see this, let us suppose that Λ(t) is the imaginary part of the principal
fundamental matrix solution Ψ(t) at t = 0. Since, Ψ(0) = I, we must
have Λ(0) = 0. Also, t → Λ(t) is a solution of the linear system. By the
uniqueness of solutions of initial value problems, Λ(t) ≡ 0. Thus, even if
some of the eigenvalues of A are complex, the principal fundamental matrix
solution is real.
Continuing under the assumption that A has a basis B of eigenvectors,
let us show that there is a change of coordinates that transforms the system
x˙ = Ax, x ∈ Rn, to a decoupled system of n scalar differential equations.
To prove this result, let us first define the matrix B := [v1,...,vn] whose
columns are the eigenvectors in B. The matrix B is invertible. Indeed,
consider the action of B on the usual basis vectors and recall that the vector
obtained by multiplication of a vector by a matrix is a linear combination of
the columns of the matrix; that is, if w = (w1,...,wn) is (the transpose of)
a vector in Cn, then the product Bw is equal to n
i=1 wivi. In particular,
we have Bei = vi, i = 1,...,n. This proves that B is invertible. In fact,
B−1 is the unique linear map such that B−1vi = ei.2.4 Linear Equations with Constant Coefficients 163
Using the same idea, let us compute
B−1AB = B−1A[v1,...,vn]
= B−1[λ1v1,...,λnvn]
= [λ1e1,...,λnen]
=
⎛
⎜⎝
λ1 0
...
0 λn
⎞
⎟⎠ .
In other words, D := B−1AB is a diagonal matrix with the eigenvalues
of A as its diagonal elements. The diffeomorphism of Cn given by the
linear transformation x = By transforms the system (2.7) to ˙y = Dy, as
required. Or, using our language for general coordinate transformations,
the push forward of the vector field with principal part x → Ax by the
diffeomorphism B−1 is the vector field with principal part y → Dy. In
particular, the system ˙y = Dy is given in components by
y˙1 = λ1y1, ..., y˙n = λnyn.
Note that if we consider the original system in the new coordinates, then
it is obvious that the functions
yi(t) := eλit
ei, i = 1,...,n
are a fundamental set of solutions for the differential equation ˙y = Dy.
Moreover, by transforming back to the original coordinates, it is clear that
the solutions
xi(t) := eλit
Bei = eλit
vi, i = 1,...,n
form a fundamental set of solutions for the original system (2.7). Thus,
we have an alternative method to construct a fundamental matrix solu￾tion: Change coordinates to obtain a new differential equation, construct
a fundamental set of solutions for the new differential equation, and then
transform these new solutions back to the original coordinates. Even if A
is not diagonalizable, a fundamental matrix solution of the associated dif￾ferential equation can still be constructed using this procedure. Indeed, we
can use a basic fact from linear algebra: If A is a real matrix, then there is a
nonsingular matrix B such that D := B−1AB is in (real) Jordan canonical
form (see [67], [140], and Exercise 2.46). Then, as before, the system (2.7)
is transformed by the change of coordinates x = By into the linear system
y˙ = Dy.
We will eventually give a detailed description of the Jordan form and also
show that the corresponding canonical system of differential equations can164 2. Homogeneous Linear Systems
be solved explicitly. This solution can be transformed back to the original
coordinates to construct a fundamental matrix solution of ˙x = Ax.
Exercise 2.27. (a) Find the principal fundamental matrix solutions at t = 0
for the matrix systems:
1. ˙x =
0 1
1 0
x.
2. ˙x =
2 −1
1 2 
x.
3. ˙x =
0 1
0 0
x.
4. ˙x =
7 −8
4 −5

x.
(b) Solve the initial value problem for system 2 with initial value x(0) = (1, 0).
(c) Find a change of coordinates (given by a matrix) that diagonalizes the system
matrix of system 4. (d) Find the principal fundamental matrix solution at t = 3
for system 3.
Exercise 2.28. (a) Determine the flow of the first-order system that is equiv￾alent to the second-order linear differential equation
x¨ + ˙x + 4x = 0.
(b) Draw the phase portrait.
Exercise 2.29. Compute the principal fundamental matrix solution at t = 0
for the system ˙x = Ax where
A :=
⎛
⎝
123
014
001
⎞
⎠ .
Exercise 2.30. Reduction to Jordan form is only one of many computational
methods that can be used to determine the exponential of a matrix. Repeat
Exercise 2.29 using the method presented in [128].
Exercise 2.31. Determine the phase portrait for the system
x˙
y˙

=
 0 1
−1 −μ
 x
y

.
Consider the cases μ < −2, μ> 2, μ = 0, 0 <μ< 2, and −2 <μ< 0. For each
case, find the principal fundamental matrix solution at t = 0.
Exercise 2.32. Let a and b be positive real numbers and consider the system
x˙ = ax, y˙ = −by.
(a) Check that the system has a hyperbolic saddle point at the origin. (b) Let r
and  be positive real numbers, Λ the ray segment {(x, y):0 < x < , y = −r}2.4 Linear Equations with Constant Coefficients 165
and Σ = {(x, y) : x = r, −<y< 0}. Show that if  is sufficiently small, then
every solution starting on Λ crosses Σ at some finite time. (c) Let σ : Λ → Σ
be the section map given by σ(x) is the y-component of the crossing point of
the solution starting at (x, −r) on Λ. Determine an algebraic formula for σ(x).
(d) According to the choices of a and b, determine when (the distance) σ(x) is
less than, equal to, or greater than x. (e) Suppose σ(0) := 0. Is σ continuous at
0. (f) Does the right-hand derivative of σ exist at x = 0 and is it continuous?
(g) Generalize to the case ˙u = Au for u ∈ R2 and A a constant matrix with
eigenvalues a and −b. (h) Generalize to the case ˙u = Au + f(u) where f is C2,
f(0) = 0 and Df(0) = 0.
Exercise 2.33. [Euler’s Equation] Euler’s equation is the second-order linear
equation
t
2
x¨ + btx˙ + cx = 0, t> 0
with the parameters b and c. (a) Show that there are three different solution types
according to the sign of (b − 1)2 − 4c. Hint: Guess a solution of the form x = rt
for some number r. (b) Discuss, for each of the cases in part (a), the behavior
of the solution as t → 0+. (c) Write a time-dependent linear first-order system
that is equivalent to Euler’s equation. (d) Determine the principal fundamental
matrix solution for the first-order system in part (c) in case b = 1 and c = −1.
Exercise 2.34. (a) Show that the general 2 × 2 linear system with constant
coefficients decouples in polar coordinates, and the first-order differential equation
for the angular coordinate θ can be viewed as a differential equation on the unit
circle T1. (b) Consider the first-order differential equation
˙
θ = α cos2 θ + β cos θ sin θ + γ sin2 θ.
For 4αγ − β2 > 0, prove that all orbits on the circle are periodic with period
4π(4αγ − β2)
−1/2, and use this result to determine the period of the periodic
orbits of the differential equation ˙
θ = η+cos θ sin θ as a function of the parameter
η > 1. Describe the behavior of this function as η → 1+ and give a qualitative
explanation of the behavior. (c) Repeat the last part of the exercise for the
differential equation ˙
θ = η − sin θ where η > 1. (d) Show that an n-dimensional
homogeneous linear differential equation induces a differential equation on the
real projective space of dimension n − 1. (e) There is an intimate connection
between the linear second-order differential equation
y¨ − (q(t)+ ˙p(t)/p(t)) ˙y + r(t)p(t)y = 0
and the Riccati equation
x˙ = p(t)x2 + q(t)x + r(t).
In fact, these equations are related by x = −y/˙ (p(t)y). For example ¨y + y = 0
is related to the Riccati equation ˙u = −1 − u2, where in this case the change of
variables is x = ˙y/y. Note that the unit circle in R2, with coordinates (y, y˙), has
coordinate charts given by (y, y˙) → y/y ˙ and (y, y˙) → y/y˙. Thus, the transfor￾mation from the linear second-order equation to the Riccati equation is a local
coordinate representation of the differential equation induced by the second-order
linear differential equation on the circle. Explore and explain the relation between166 2. Homogeneous Linear Systems
this coordinate representation and the polar coordinate representation of the first￾order linear system. (f) Prove the cross-ratio property for Riccati equations: If
xi, i = 1, 2, 3, 4, are four linearly independent solutions of a Riccati equation,
then the quantity
(x1 − x3)(x2 − x4)
(x1 − x4)(x2 − x3)
is constant. (g) Show that if one solution t → z(t) of the Riccati equation is
known, then the general solution can always be found by solving a linear equation
after the substitution x = z + 1/u. (h) Solve the initial value problem
x˙ + x2 + (2t + 1)x + t
2 + t +1=0, x(1) = 1.
(see [226, p. 30] for this equation, and [81] for more properties of Riccati equa￾tions).
Exercise 2.35. The linearized Hill’s equations for the relative motion of two
satellites with respect to a circular reference orbit about the earth are given by
x¨ − 2ny˙ − 3n2
x = 0,
y¨ + 2nx˙ = 0,
z¨ + n2
z = 0
where n is a constant related to the radius of the reference orbit and the gravi￾tational constant. There is a five-dimensional manifold in the phase space corre￾sponding to periodic orbits. An orbit with an initial condition not on this mani￾fold contains a secular drift term. Determine the manifold of periodic orbits and
explain what is meant by a secular drift term. Answer: The manifold of periodic
orbits is the hyperplane given by ˙y + 2nx = 0.
2.5 The Matrix Exponential
Instead of writing out the explicit, perhaps complicated, formulas for the
components of the fundamental matrix solution of an n × n linear system
of differential equations, it is often more useful, at least for theoretical
considerations, to treat the situation from a more abstract point of view. In
fact, we will show that there is a natural generalization of the exponential
function to a function defined on the set of square matrices. Using this
matrix exponential function, the solution of a linear homogeneous system
with constant coefficients is given in a form that is analogous to the solution
t → etax0 of the scalar differential equation ˙x = ax.
Recall that the set of linear transformations L(Rn) (respectively L(Cn))
on Rn (respectively Cn) is an n2-dimensional Banach space with respect
to the operator norm
	A	 = sup
|v|=1
|Av|.2.5 The Matrix Exponential 167
Most of the theory we will develop is equally valid for either of the vector
spaces Rn or Cn. When the space is not at issue, we will denote the Banach
space of linear transformations by L(E) where E may be taken as either
Rn or Cn. The theory is also valid for the set of (operator norm) bounded
linear transformations of an arbitrary Banach space.
Exercise 2.36. Prove: L(E) is a finite-dimensional Banach space with respect
to the operator norm.
Exercise 2.37. Prove: (a) If A, B ∈ L(E), then AB≤AB. (b) If A ∈
L(E) and k is a nonnegative integer, then Ak≤Ak.
Exercise 2.38. The space of n × n matrices is a topological space with respect
to the operator topology. Prove that the set of matrices with n distinct eigenvalues
is open and dense. A property that is defined on the countable intersection of
open dense sets is called generic (see [140, p. 153–157]).
Proposition 2.39. If A ∈ L(E), then the series I + ∞
k=1
1
k!Ak is abso￾lutely convergent.
Proof. Define
SN := 1 + 	A	 +
1
2!	A2	 + ··· +
1
N!
	AN 	
and note that {SN }∞
N=1 is a monotone increasing sequence of real numbers.
Since (by Exercise 2.37) 	Ak	≤	A	k for every integer k ≥ 0, it follows
that {SN }∞
N=1 is bounded above. In fact,
SN ≤ eA
for every N ≥ 1. ✷
Define the exponential map exp : L(E) → L(E) by
exp(A) := I +∞
k=1
1
k!
Ak.
Also, let us use the notation eA := exp(A).
The main properties of the exponential map are summarized in the fol￾lowing proposition.
Proposition 2.40. Suppose that A,B ∈ L(E).
(0) If A ∈ L(Rn), then eA ∈ L(Rn).
(1) If B is nonsingular, then B−1eAB = eB−1AB.
(2) If AB = BA, then eA+B = eAeB.
(3) e−A = (eA)−1. In particular, the image of exp is in the general linear
group GL(E) consisting of the invertible elements of L(E).168 2. Homogeneous Linear Systems
(4) d
dt (etA) = AetA = etAA. In particular, t → etA is the principal fun￾damental matrix solution of the system (2.7) at t = 0.
(5) 	eA	 ≤ eA.
Proof. The proof of (0) is obvious.
To prove (1), define
SN := I + A +
1
2!A2 + ··· +
1
N!
AN ,
and note that if B is nonsingular, then B−1AnB = (B−1AB)n. Thus, we
have that
B−1SN B = I + B−1AB +
1
2!(B−1AB)
2 + ··· +
1
N!
(B−1AB)
N ,
and, by the definition of the exponential map,
lim
N→∞ B−1SN B = eB−1AB.
Using the continuity of the linear map on L(E) defined by C → B−1CB,
it follows that
lim
N→∞ B−1SN B = B−1eAB,
as required.
While the proof of (4) given here has the advantage of being self-contained,
there are conceptually simpler alternatives (see Exercises 2.41–2.42). As the
first step in the proof of (4), consider the following proposition: If s,t ∈ R,
then e(s+t)A = esAetA. To prove it, let us denote the partial sums for the
series representation of etA by
SN (t) := I + tA +
1
2!(tA)
2 + ··· +
1
N!
(tA)
N
= I + tA +
1
2!t
2A2 + ··· +
1
N!
t
N AN .
We claim that
SN (s)SN (t) = SN (s + t) + 
2N
k=N+1
Pk(s,t)Ak (2.8)
where Pk(s,t) is a homogeneous polynomial of degree k such that
|Pk(s,t)| ≤ (|s| + |t|)k
k! .
To obtain this identity, note that the kth-order term of the product, at
least for 0 ≤ k ≤ N, is given by

k
j=0
1
(k − j)!j!
sk−j t
j 
Ak =  1
k!

k
j=0
k!
(k − j)!j!
sk−j t
j 
Ak = 1
k!
(s + t)
kAk.2.5 The Matrix Exponential 169
Also, for N + 1 ≤ k ≤ 2N, the kth-order term is essentially the same, only
some of the summands are missing. In fact, these terms all have the form

k
j=0
δ(j)
(k − j)!j!
sk−j t
j 
Ak
where δ(j) has value zero or one. Each such term is the product of Ak and
a homogeneous polynomial in two variables of degree k. Moreover, because
|δ(j)| ≤ 1, we obtain the required estimate for the polynomial. This proves
the claim.
Using equation (2.8), we have the following inequality:
	SN (s)SN (t) − SN (s + t)	 ≤ 
2N
k=N+1
|Pk(s,t)| 	A	k
≤ 
2N
k=N+1
(|s| + |t|)k
k! 	A	k.
Also, because the series
∞
k=0
(|s| + |t|)k
k! 	A	k
is convergent, it follows that its partial sums, denoted QN , form a Cauchy
sequence. In particular, if  > 0 is given, then for sufficiently large N we
have
|Q2N − QN | < .
Moreover, since
Q2N − QN = 
2N
k=N+1
(|s| + |t|)k
k! 	A	k,
it follows that
lim
N→∞ 	SN (s)SN (t) − SN (s + t)	 = 0.
Using this fact and passing to the limit as N → ∞ on both sides of the
inequality
	esAetA − e(s+t)A	≤	esAetA − SN (s)SN (t)	
+ 	SN (s)SN (t) − SN (s + t)	
+ 	SN (s + t) − e(s+t)A	,
we see that
esAetA = e(s+t)A, (2.9)170 2. Homogeneous Linear Systems
as required.
In view of the identity (2.9), the derivative of the function t → etA is
given by
d
dt etA = lims→0
1
s
(e(t+s)A − etA)
= lims→0
1
s
(esA − I)etA
= 
lims→0
1
s
(esA − I)

etA
= 
lims→0
(A + R(s))
etA
where
	R(s)	 ≤ 1
|s|
∞
k=2
|s|
k
k! 	A	k ≤ |s|
∞
k=2
|s|
k−2
k! 	A	k.
Moreover, if |s| < 1, then 	R(s)	≤|s|eA. In particular, R(s) → 0 as
s → 0 and as a result,
d
dt etA = AetA.
Since ASN (t) = SN (t)A, it follows that AetA = etAA. This proves the
first statement of part (4). In particular t → etA is a matrix solution of
the system (2.7). Clearly, e0 = I. Thus, the columns of e0 are linearly
independent. It follows that t → etA is the principal fundamental matrix
solution at t = 0, as required.
To prove (2), suppose that AB = BA and consider the function t →
et(A+B)
. By (4), this function is a matrix solution of the initial value prob￾lem
x˙ = (A + B)x, x(0) = I.
The function t → etAetB is a solution of the same initial value problem. To
see this, use the product rule to compute the derivative
d
dt etAetB = AetAetB + etABetB,
and use the identity AB = BA to show that etAB = BetA. The desired
result is obtained by inserting this last identity into the formula for the
derivative. By the uniqueness of the solution of the initial value problem,
the two solutions are identical.
To prove (3), we use (2) to obtain I = eA−A = eAe−A or, in other words,
(eA)−1 = e−A.
The result (5) follows from the inequality
	I + A +
1
2!A2 + ··· +
1
N!
AN 	≤	I	 + 	A	 +
1
2!	A	2 + ··· +
1
N!
	A	N .
✷2.5 The Matrix Exponential 171
We have defined the exponential of a matrix as an infinite series and
used this definition to prove that the homogeneous linear system ˙x = Ax
has a fundamental matrix solution, namely, t → etA. This is a strong result
because it does not use the existence theorem for differential equations.
Granted, the uniqueness theorem is used. But it is an easy corollary of
Gronwall’s inequality (see Exercise 2.7). An alternative approach to the
exponential map is to use the existence theorem and define the function
t → etA to be the principal fundamental matrix solution at t = 0. Proposi￾tion 2.40 can then be proved by using properties of the solutions of homo￾geneous linear differential equations.
Exercise 2.41. Show that the partial sums of the series representation of etA
converge uniformly on compact subsets of R. Use Theorem 1.264 to prove part (4)
of Proposition 2.40.
Exercise 2.42. (a) Show that exp : L(E) → L(E) is continuous. Hint: For r >
0, the sequence of partial sums of the series representation of exp(X) converges
uniformly on Br(0) := {X ∈ L(E) : X < r}. (b) By Exercise 2.38, matrices
with distinct eigenvalues are dense in L(E). Such matrices are diagonalizable
(over the complex numbers). Show that if A ∈ L(E) is diagonalizable, then
part (4) of Proposition 2.40 holds for A. (c) Use parts (a) and (b) to prove
part (4) of Proposition 2.40. (d) Prove that exp : L(E) → L(E) is differentiable
and compute D exp(I).
Exercise 2.43. Define exp(A) = Φ(1) where Φ(t) is the principal fundamental
matrix at t = 0 for the system ˙x = Ax. (a) Prove that exp(tA) = Φ(t). (b) Prove
that exp(−A) = (exp(A))−1.
Exercise 2.44. [Laplace Transform] (a) Prove that if A is an n × n-matrix,
then
e
tA − I =
 t
0
AeτA dτ.
(b) Prove that if all eigenvalues of A have negative real parts, then
−A−1 =
 ∞
0
e
τA dτ.
(c) Prove that if s ∈ R is sufficiently large, then
(sI − A)
−1 =
 ∞
0
e
−sτ e
τA dτ ;
that is, the Laplace transform of etA is (sI − A)
−1. (d) Solve the initial value
problem ˙x = Ax, x(0) = x0 using the method of the Laplace transform; that
is, take the Laplace transform of both sides of the equation, solve the resulting
algebraic equation, and then invert the transform to obtain the solution in the
original variables. By definition, the Laplace transform of the (perhaps matrix
valued) function f is
L{f}s =
 ∞
0
e
−sτ f(τ ) dτ.172 2. Homogeneous Linear Systems
To obtain a matrix representation for etA, let us recall that there is a
real matrix B that transforms A to real Jordan canonical form. Of course,
to construct the matrix B, we must at least be able to find the eigenvalues
of A, a task that is equivalent to finding the roots of a polynomial of degree
n. Thus, for n ≥ 5, it is generally impossible to construct the matrix B
explicitly. But if B is known, then by using part (1) of Proposition 2.40,
we have that
B−1etAB = etB−1AB.
Thus, the problem of constructing a principal fundamental matrix is solved
as soon as we find a matrix representation for etB−1AB.
The Jordan canonical matrix B−1AB is block diagonal, where each block
corresponding to a real eigenvalue has the form “diagonal + nilpotent,” and
each block corresponding to a complex eigenvalue with nonzero imaginary
part has the form “block diagonal + block nilpotent.” In view of this block
structure, it suffices to determine the matrix representation for etJ where
J denotes a single Jordan block.
Consider a block of the form
J = λI + N
where N is the nilpotent matrix with zero components except on the super
diagonal, where each component is unity and note that Nk = 0. We have
that
etJ = et(λI+N) = etλI etN = etλ(I + tN +
t
2
2! N2 + ··· +
t
k−1
(k − 1)!Nk−1)
where k is the dimension of the block.
If J is a Jordan block with diagonal 2 × 2 subblocks given by
R =
α −β
β α 
(2.10)
with β 
= 0, then etJ is block diagonal with each block given by etR. To
obtain an explicit matrix representation for etR, define
P := 0 −β
β 0

, Q(t) := cos βt − sin βt
sin βt cos βt 
,
and note that t → etP and t → Q(t) are both solutions of the initial value
problem
x˙ =
0 −β
β 0

x, x(0) = I.
Thus, we have that etP = Q(t) and
etR = eαtetP = eαtQ(t).2.5 The Matrix Exponential 173
Finally, if the Jordan block J has the 2 × 2 block matrix R along its
block diagonal and the 2 × 2 identity along its super block diagonal, then
etJ = eαtS(t)etN (2.11)
where S(t) is block diagonal with each block given by Q(t), and N is the
nilpotent matrix with 2 × 2 identity blocks on its super block diagonal. To
prove this fact, note that J can be written as a sum J = αI + K where K
has diagonal blocks given by P and super diagonal blocks given by the 2×2
identity matrix. Since the n × n matrix αI commutes with every matrix,
we have that
etJ = eαtetK.
The proof is completed by observing that the matrix K can also be written
as a sum of commuting matrices; namely, the block diagonal matrix with
each diagonal block equal to P and the nilpotent matrix N.
We have outlined a procedure to find a matrix representation for etA. In
addition, we have proved the following result.
Proposition 2.45. If A is an n × n matrix, then etA is a matrix whose
components are (finite) sums of terms of the form
p(t)eαt sin βt and p(t)eαt cos βt
where α and β are real numbers such that α + iβ is an eigenvalue of A,
and p(t) is a polynomial of degree at most n − 1.
Exercise 2.46. [Jordan Form] Show that every real 2 × 2-matrix can be trans￾formed to real Jordan canonical form and find the fundamental matrix solutions
for the corresponding 2 × 2 real homogeneous linear systems of differential equa￾tions. Draw the phase portrait for each canonical system. Hint: For the case of a
double eigenvalue suppose that (A−λI)V = 0 and every eigenvector is parallel to
V . Choose a vector W that is not parallel to V and note that (A−λI)W = Y 	= 0.
Since V and W are linearly independent, Y = aV + bW for some real numbers a
and b. Use this fact to argue that Y is parallel to V . Hence, there is a (nonzero)
vector Z such that (A − λI)Z = V . Define B = [V,W] to be the indicated 2 × 2-
matrix partitioned by columns and show that B−1AB is in Jordan form. To solve
x˙ = Ax, use the change of variables x = By.
Exercise 2.47. Find the Jordan canonical form for the matrix
⎛
⎝
11 2
0 −2 1
0 0 −2
⎞
⎠ .
Exercise 2.48. Find the principal fundamental matrix solution at t = 0 for the
linear differential equation whose system matrix is
⎛
⎜⎜⎝
001 0
000 1
000 −2
0 a 2 0
⎞
⎟⎟⎠ ,174 2. Homogeneous Linear Systems
where a := 4−ω2 and 0 ≤ ω ≤ 1, by changing variables so that the system matrix
is in Jordan canonical form, computing the exponential, and changing back to
the original variables.
Exercise 2.49. Suppose that J = λI + N is a k × k-Jordan block and let
B denote the diagonal matrix with main diagonal 1, , 2,...,k−1. (a) Show
that B−1JB = λI + N. (b) Prove: Given  > 0 and a matrix A, there is a
diagonalizable matrix B such that A − B <  (cf. Exercise 2.38). (c) Discuss
the statement: A numerical algorithm for finding the Jordan canonical form will
be ill conditioned.
Exercise 2.50. (a) Suppose that A is an n × n-matrix such that A2 = I. Find
an explicit formula for etA. (b) Repeat part (a) in case A2 = −I. (c) Solve the
initial value problem
x˙ =
⎛
⎜⎜⎝
2 −5 8 −12
1 −2 4 −8
002 −5
001 −2
⎞
⎟⎟⎠
x, x(0) =
⎛
⎜⎜⎝
1
0
0
1
⎞
⎟⎟⎠ .
(d) Specify the stable manifold for the rest point at the origin of the linear system
x˙ =
⎛
⎜⎜⎝
2 −3 4 −4
1 −2 4 −4
002 −3
001 −2
⎞
⎟⎟⎠ x.
Exercise 2.51. Prove that det eA = etr A for every n × n matrix A. Hint: Use
Liouville’s formula 2.20.
Exercise 2.52. Find a matrix function t → A(t) such that
t → exp   t
0
A(s) ds
is not a matrix solution of the system ˙x = A(t)x. Show that the given exponential
formula is a solution in the scalar case. When is it a solution for the matrix case?
Exercise 2.53. Suppose that f is an analytic function defined in some open
neighborhood of the origin in the complex plane so that it has a convergent
power series representation at this point. Let p be a polynomial of degree n.
There exist an analytic function q (defined in an open neighborhood of the origin)
and a polynomial r of degree at most n − 1 such that f = qp + r. In case
f is a polynomial, the proof is essentially the result of long division. Another
important result is the Cayley–Hamilton theorem: Every square matrix satisfies
its characteristic polynomial. (a) Use the given facts to show that the value of a
polynomial function on an n × n-matrix can always be reduced to the evaluation
of a polynomial of degree at most n − 1 on the same matrix. (b) Matrix values
of an analytic function (that is, values obtained by substituting square matrices
into the power series representation of the function) can always be reduced to
the evaluation of a polynomial of degree at most n − 1 of the input matrix. (c)
Describe a method for determining the coefficients of the reduced polynomials of2.5 The Matrix Exponential 175
degree at most n−1 that appears in parts (a)–(c) and demonstrate the method for
computing values of the analytic function sin when evaluated on 2 × 2 matrices.
(d) Describe a method for computing the matrix exponential for square matrices
using the ideas in this exercise and provide illustrative examples. (e) Compare
the methodology of this exercise with Putzer’s algorithm [210].
Exercise 2.54. Let v ∈ R3, assume v 	= 0, and consider the differential equation
x˙ = v × x, x(0) = x0
where × denotes the cross product in R3. Show that the solution of the differential
equation is a rigid rotation of the initial vector x0 about the direction v. If the
differential equation is written as a matrix system
x˙ = Sx
where S is a 3 × 3 matrix, show that S is skew-symmetric and that the flow
φt(x) = etSx of the system is a group of orthogonal transformations. Show that
every solution of the system is periodic and relate the period to the length of v.
Exercise 2.55. Consider the linear system ˙x = A(t)x where A(t) is a skew￾symmetric n × n-matrix for each t ∈ R with respect to some inner product on
Rn, and let | | denote the corresponding norm. Show that |φ(t)| = |φ(0)| for every
solution t → φ(t).
Exercise 2.56. [An Infinite-Dimensional ODE] Let E denote the Banach space
C([0, 1]) given by the set of all continuous functions f : [0, 1] → R with the
supremum norm
f = sup s∈[0,1]
|f(s)|.
Choose real numbers a and b such that 0 ≤ a ≤ 1, define the operator U : E → E
given by (Uf)(s) = f(as), and the element g ∈ E given by given by s → bs. (a)
Find a formula for the solution of the initial value problem
x˙ = Ux, x(0) = g.
(b) Note that the zero function in E is a rest point of the differential equation.
Prove that it is not stable. (c) The operator U is a special case of the class of
operators defined by Uf = f ◦g for some continuous function g mapping the unit
interval into itself. Find a formula for the solution of the differential equation
in part (a) but this time with x(0) = f for an arbitrary element of E. Hint:
The formula might be an infinite series. Also show that the rest point at x = 0
is unstable for every choice of g. (d) Suppose U is defined by Uf = g ◦ f for
some continuous function g that maps the entire real line into the unit interval.
Explain why solving the differential equation of part (a) might be difficult. For
more on ordinary differential equations on infinite-dimensional Banach spaces see
Section 6.6.
Exercise 2.57. [Koopman Theory] For the linear differential equation ˙x = Ax,
the language of semi-group theory states that A is the infinitesimal generator of
the one-parameter group of linear transformations Ut := etA. The latter is called
a one-parameter group because Ut
Us = Ut+s and U0 = I. It would be called176 2. Homogeneous Linear Systems
a one-parameter semi-group if the same holds but only for nonnegative t and
s. This canonical example can be generalized to Banach spaces. A special case
arises from a given ODE ˙x = f(x) on Rn with flow φt: For g : Rn → R define
Ut
(g)(x) = g(φt(x)).
(a) Show that Ut
, called the Koopman group, is indeed a group of linear transfor￾mations. In analogy with the canonical example, where A is called the infinitesi￾mal generator of the group etA, the Koopman group has an infinitesimal gener￾ator. When given a one-parameter group, its derivative (or one-sided derivative
for semi-groups) with respect to the parameter at parameter value t = 0 is its
infinitesimal generator. For the canonical example d
dt etA|t=0 = A. (b) Show that
the infinitesimal generator for the Koopman group is the Koopman operator K
given by Kg = ∇g · f and γ(t) := Ut
g is a solution of the initial value problem
γ˙ = Kγ and γ(0) = g. Note that the Koopman group may operate on very general
classes of functions, for example all continuous, integrable, or square integrable
functions. These vector spaces can, with perhaps some restrictions, be endowed
with a norm so that they are complete linear (Banach) spaces. But only a subset
of the functions in such a Banach space can be expected to be differentiable with
derivatives that are elements of the Banach space. Thus, even for the special case
of the Koopman group, its infinitesimal generator acts only on a subset of one of
the usual function spaces. This is typical behavior that must be accommodated
in a rigorous semi-group theory (see, for example, [88]). A function g : Rn → R in
the context of this exercise is called an observable. Its values along a trajectory of
the flow φt is a measurement of the time evolution of the system. Alternatively, g
may be considered to be a coordinate function on Rn that might be better than
the usual coordinates for understanding some property of the dynamical system.
At least the Koopman group is intimately connected to the underlying dynam￾ical system. Suppose that there is an observable g and a real number such that
Kg = λg so that λ is an eigenvalue of K with eigenvector g. Similarly, g might
be taken to be complex valued so that complex eigenvalues are also defined. (c)
Show that λ = 0 is always an eigenvalue of the Koopman operator. (d) Show
that if g is an eigenvector of K with eigenvalue λ, then Ut
g = etλg. Thus simple
linear dynamics is obtained for observables (coordinate functions) that happen
to be eigenfunctions of K. A promising idea is to search for a (complete) set of
eigenvalues and eigenvectors in the hope that the linear dynamics in the corre￾sponding subspace generated by the eigenfunctions reveals unknown properties of
the underlying nonlinear dynamics of f. See [18] for the classical theory and [33]
for a modern outlook. The following exercises might be useful in understanding
some of the rudiments of the theory: (e) Show that if the ode ˙x = f(x) has a
first integral H (that is, H is constant along orbits) then λ = 0 is an eigenvalue
of the Koopman operator. (f) Determine a complete Koopman theory for the
linear ODE ˙x = ax. (g) Show that every real number λ is an eigenvalue for the
Koopman operator associated with the dynamics of ˙x = 1 − x2 restricted to the
interval (−1, 1). (h) A classical problem is to study dynamical systems on the
two-dimensional torus, which is the product of two circles. Of special interest
are the systems given by the constant vector field X = (1, a) pushed forward to
the torus via the covering map (x, y) → (e2πix, e2πiy), where the circles whose
product is the torus are each considered to be a unit circle in the complex plane.2.6 Lie–Trotter and Baker–Campbell–Hausdorff formulas 177
When a is a rational number, all orbits on the torus are periodic; when a is irra￾tional all orbits are dense in the torus. Prove this. The latter systems are special
cases of ergodic dynamical systems, which satisfy an important property stated
here for the toral systems under consideration: for an observable g whose absolute
value is integrable over the torus (with respect to the measure induced by the
projection of the usual measure on the plane) and the flow φt of the vector field
on the torus, the integral of g over the torus (that is, its space average) is equal
to its time average
limT→∞
1
T
 T
0
g(φt(x, y)) dt.
This fact is not difficult to prove under the assumption that g is representable
by a double Fourier series. For this exercise prove the result in case g(x) =
1 + sin 2πx + cos 2πy. The connection with the Koopman operator is signaled
by the appearance of the Koopman group in the definition of the time average.
Show that in general if a system is ergodic (so that time averages equal space
averages), then the eigenspace of the eigenvalue λ = 0 of the Koopman operator
consists entirely of constant functions.
2.6 Lie–Trotter and Baker–Campbell–Hausdorff
formulas
The scalar autonomous differential equation ˙x = ax has the principal fun￾damental solution t → eat at t = 0. We have defined the exponential map
on bounded linear operators and used this function to construct the analo￾gous fundamental matrix solution t → etA of the homogeneous autonomous
system ˙x = Ax. The scalar nonautonomous homogeneous linear differential
equation ˙x = a(t)x has the principal fundamental solution
t → e
 t
0 a(s) ds.
But, in the matrix case, the same formula with a(s) replaced by A(s) is
not always a matrix solution of the linear system ˙x = A(t)x (cf. [147] and
see Exercise 2.52).
As an application of the methods developed in this section we will for￾mulate and prove a special case of the Lie–Trotter product formula for
the exponential of a sum of two k × k-matrices when the matrices do not
necessarily commute (see [248] for the general case).
Theorem 2.58. If γ : R → L(E) is a C1-function with γ(0) = I and
γ˙(0) = A, then the sequence {γn(t/n)}∞
n=1 converges to exp(tA). In partic￾ular, if A and B are k × k-matrices, then
et(A+B) = limn→∞ 
e
t
n Ae
t
n B
n
.178 2. Homogeneous Linear Systems
Proof. Fix T > 0 and assume that |t| < T. We will first prove the following
proposition: There is a number M > 0 such that 	γj (t/n)	 ≤ M whenever
j and n are integers and 0 ≤ j ≤ n. Using Taylor’s theorem, we have the
estimate
	γ(t/n)	 ≤ 1 +
T
n 	A	 +
T
n
 1
0
	γ˙(st/n) − A	 ds.
Since σ → 	γ˙(σ) − A	 is a continuous function on the compact set S :=
{σ ∈ R : |σ| ≤ T}, we also have that K := sup{	γ˙(σ) − A	 : σ ∈ S} < ∞,
and therefore,
	γj (t/n)	≤	γ(t/n)	j ≤ (1 + T
n (	A	 + K))n.
To finish the proof of the proposition, note that the sequence {(1+ T
n (	A	+
K))n}∞
n=1 is bounded—it converges to exp(T(	A	 + K)).
Using the (telescoping) identity
etA − γn(t/n) = n
j=1

(e
t
n A)
n−j+1γj−1(t/n) − (e
t
n A)
n−jγj (t/n)

= n
j=1

(e
t
n A)
n−j e
t
n Aγj−1(t/n) − (e
t
n A)
n−jγ(t/n)γj−1(t/n)

,
we have the estimate
	etA − γn(t/n)	 ≤ n
j=1
e
n−j
n TA	e
t
n A − γ(t/n)		γj−1(t/n)	
≤ M	e
t
n A − γ(t/n)	
n
j=1
e(n−j)/nTA
≤ MneTA	e
t
n A − γ(t/n)	.
By Taylor’s theorem (applied to each of the functions σ → eσA and σ →
γ(σ)), we obtain the inequality
	e
t
n A − γ(t/n)	 ≤ T
n
J(n)
where
J(n) :=  1
0
	γ˙(st/n) − A	 ds +
 1
0
	A		e
st
n A − I	 ds
is such that limn→∞ J(n) = 0. Since
	etA − γn(t/n)	 ≤ MTeTAJ(n),
it follows that limn→∞	etA − γn(t/n)	 = 0, as required.
The second statement of the theorem follows from the first with A
replaced by A + B and γ(t) := etAetB. ✷2.6 Lie–Trotter and Baker–Campbell–Hausdorff formulas 179
The product formula in Theorem 2.58 gives a method to compute the
solution of the differential equation ˙x = (A + B)x from the solutions of
the equations ˙x = Ax and ˙x = Bx. Of course, if A and B happen to
commute (that is, [A,B] := AB − BA = 0), then the product formula
reduces to et(A+B) = etAetB by part (2) of Proposition 2.40. It turns out
that [A,B] = 0 is also a necessary condition for this reduction. Indeed, let
us note first that t → etAetB is a solution of the initial value problem
W˙ = AW + WB, W(0) = I. (2.12)
If t → et(A+B) is also a solution, then by substitution and a rearrangement
of the resulting equality, we have the identity
A = e−t(A+B)
Aet(A+B)
.
By computing the derivative with respect to t of both sides of this identity
and simplifying the resulting equation, it follows that [A,B] = 0 (cf. Exer￾cise 2.61).
What can we say about the product etAetB in case [A,B] 
= 0? The
answer is provided by (a special case of) the Baker–Campbell–Hausdorff
formula
etAetB = et(A+B)+(t
2/2)[A,B]+R(t,A,B) (2.13)
where R(0,A,B) = Rt(0,A,B) = Rtt(0,A,B) = 0 (see, for example, [252]).
To obtain formula (2.13), note that the curve γ : R → L(E) given by
t → etAetB is such that γ(0) = I. Also, the function exp : L(E) → L(E)
is such that exp(0) = I and D exp(0) = I. Hence, by the inverse function
theorem, there is a unique smooth curve Ω(t) in L(E) such that Ω(0) = 0
and eΩ(t) = etAetB. Hence, the function t → eΩ(t) is a solution of the initial
value problem (2.12), that is,
D exp(Ω)Ω =˙ AeΩ + eΩB. (2.14)
By evaluation at t = 0, we have that Ω(0) = ˙ A + B. The equality Ω(0) = ¨
[A,B] is obtained by differentiating both sides of equation (2.14) with
respect to t at t = 0. This computation requires the second derivative of
exp at the origin in L(E). To determine this derivative, use the power series
definition of exp to show that it suffices to compute the second derivative
of the function h : L(E) → L(E) given by h(X) = 1
2X2. Since h is smooth,
its derivatives can be determined by computing directional derivatives; in
fact, we have that
Dh(X)Y = d
dt
1
2
(X + tY )
2

t=0 = 1
2
(XY + Y X),
D2h(X)(Y,Z) = d
dtDh(X + tZ)Y


t=0 = 1
2
(Y Z + ZY ),180 2. Homogeneous Linear Systems
and D2 exp(0)(Y,Z) = 1
2 (Y Z + ZY ). The proof of formula (2.13) is com￾pleted by applying Taylor’s theorem to the function Ω.
Exercise 2.59. The second-order correction term in the Baker–Campbell–Hausdoff
formula (2.13) is (t
2/2)[A, B]. Prove that the third-order correction is
(t
3
/12)([A, [A, B]] − [B, [A, B]])
.
Exercise 2.60. Show that the commutator relations
[A, [A, B]] = 0, [B, [A, B]] = 0
imply the identity
e
tAe
tB = e
Ω(t) (2.15)
where Ω(t) := t(A+B)+(t
2/2)[A, B]. Is the converse statement true? Find (3×3)
matrices A and B such that [A, B] 	= 0, [A, [A, B]] = 0, and [B, [A, B]] = 0.
Verify identity (2.15) for your A and B. Hint: Suppose that [A, [A, B]] = 0 and
[B, [A, B]] = 0. Use these relations to prove in turn [Ω(t), Ω( ˙ t)] = 0, D exp(Ω)Ω =˙
exp(Ω)Ω, and [exp(Ω) ˙ , Ω] = 0. To prove the identity ( ˙ 2.15), it suffices to show that
t → exp(Ω(t)) is a solution of the initial value problem (2.12). By substitution
into the differential equation and some manipulation, prove that this function is
a solution if and only if
d
dt (e
−Ω(t)
AeΩ(t) − t[A, B]) = 0.
Compute the indicated derivative and use the hypotheses to show that it vanishes.
Exercise 2.61. Find n×n matrices A and B such that [A, B] 	= 0 and eAeB =
eA+B (see Problem 88-1 in SIAM Review, 31(1), (1989), 125–126).
Exercise 2.62. Let A be an n×n matrix with components {aij}. Prove: Every
component of eA is nonnegative if and only if the off-diagonal components of A
are all nonnegative (that is, aij ≥ 0 whenever i 	= j). Hint: The “if” direction is
an easy corollary of the Trotter product formula. But, this is not the best proof.
To prove both directions, consider the positive invariance of the positive orthant
in n-dimensional space under the flow of the system ˙x = Ax.
Exercise 2.63. [Lie Groups and Lax Pairs] Is the map
exp : L(E) → GL(E)
injective? Is this map surjective? Do the answers to these questions depend on
the choice of E as Rn or Cn? Prove that the general linear group is a submanifold
of RN with N = n2 in case E = Rn, and N = 2n2 in case E = Cn. Show that the
general linear group is a Lie group; that is, the group operation (matrix product)
is a differentiable map from GL(E) × GL(E) → GL(E). Consider the tangent
space at the identity element of GL(E). Note that, for each A ∈ L(E), the map
t → exp(tA) is a curve in GL(E) passing through the origin at time t = 0. Use
this fact to prove that the tangent space can be identified with L(E). It turns out
that L(E) is a Lie algebra. More generally, a vector space is called a Lie algebra
if for each pair of vectors A and B, a product, denoted by [A, B], is defined on2.6 Lie–Trotter and Baker–Campbell–Hausdorff formulas 181
the vector space such that the product is bilinear and also satisfies the following
algebraic identities: (skew-symmetry) [A, B] = −[B,A], and (the Jacobi identity)
[[A, B], C] + [[B,C], A] + [[C, A], B]=0.
Show that L(E) is a Lie algebra with respect to the product [A, B] := AB −BA.
For an elementary introduction to the properties of these structures, see [135].
The delicate interplay between Lie groups and Lie algebras leads to a far￾reaching theory. To give a flavor of the relationship between these structures,
consider the map Ad : GL(E) → L(L(E)) defined by Ad(A)(B) = ABA−1. This
map defines the adjoint representation of the Lie group into the automorphisms
of the Lie algebra. Prove this. Also, Ad is a homomorphism of groups: Ad(AB) =
Ad(A) Ad(B). Note that we may as well denote the automorphism group of L(E)
by GL(L(E)). Also, define ad : L(E) → L(L(E)) by ad(A)(B)=[A, B]. The map
ad is a homomorphism of Lie algebras. Now, ϕt := Ad(etA) defines a flow in L(E).
The associated differential equation is obtained by differentiation. Show that ϕt
is the flow of the differential equation
x˙ = Ax − xA = ad(A)x. (2.16)
This differential equation is linear; thus, it has the solution t → et ad(A)
. By the
usual argument it now follows that et ad(A) = Ad(etA). In particular, we have the
commutative diagram
L(E) ad −→ L(L(E))
⏐
⏐
exp
⏐
⏐
exp
GL(E) Ad
−→ GL(L(E)).
The adjoint representation of GL(E) is useful in the study of the subgroups of
GL(E), and it is also used to identify the Lie group that is associated with a given
Lie algebra. But consider instead the following application to spectral theory. A
curve t → L(t) in L(E) is called isospectral if the spectrum of L(t) is the same as
the spectrum of L(0) for all t ∈ R. We have the following proposition: Suppose
that A ∈ L(E). If t → L(t) is a solution of the differential equation (2.16), then
the solution is isospectral. The proof is just a restatement of the content of the
commutative diagram. In fact, L(t) is similar to L(0) because
L(t) = Ad(e
tA)L(0) = e
tAL(0)e
−tA.
A pair of curves t → L(t) and t → M(t) is called a Lax pair if
L˙ = LM − ML.
The sign convention aside, the above proposition shows that if (L, M) is a Lax
pair and if M is constant, then L is isospectral. Prove the more general result: If
(L, M) is a Lax pair, then L is isospectral.
Finally, prove that
d
dt

e
tAe
tBe
−tAe
−tB


t=0
= 0
and
d
dt

e
√tAe
√tBe
−√tAe
−√tB


t=0
= AB − BA. (2.17)182 2. Homogeneous Linear Systems
As mentioned above, [A, B] is in the tangent space at the identity of GL(E).
Thus, there is a curve γ(t) in GL(E) such that γ(0) = I and ˙γ(0) = [A, B].
One such curve is t → et[A,B]
. Since the Lie bracket [A, B] is an algebraic object
computed from the tangent vectors A and B, it is satisfying that there is another
such curve formed from the curves t → etA and t → etB whose respective tangent
vectors at t = 0 are A and B.
Exercise 2.64. Write a report on the application of the Lie–Trotter formula
to obtain numerical approximations of the solution of the initial value problem
x˙ = (A + B)x, x(0) = v with expressions of the form
T(t, n)v = (e
(t/n)Ae
(t/n)B)
nv.
For example, approximate x(1) for such systems where
A := a 0
0 b

, B :=  c −d
d c 
.
Compare the results of numerical experiments using your implementation(s) of
the “Lie–Trotter method” and your favorite choice of alternative method(s) to
compute x(1). Note that etA and etB can be input explicitly for the suggested
example. Can you estimate the error |T(1, n)v − eA+Bv|? Generalizations of this
scheme are sometimes used to approximate differential equations where the “vec￾tor field” can be split into two easily solved summands. Try the same idea to
solve nonlinear ODE of the form ˙x = f(x) + g(x) where etA is replaced by the
flow of ˙x = f(x) and etB is replaced by the flow of ˙x = g(x).3
Stability of Linear Systems
The main objectives of this chapter are formulations and proofs of basic
results related to the principle of linearized stability. Recall that a linear
homogeneous differential equation has a rest point at the origin. For the
special case of constant coefficient homogeneous linear differential equa￾tions, the stability of this rest point is directly related to the eigenvalues
of the system matrix. In particular, the following theorem is proved: If
the matrix A is constant and all of its eigenvalues have negative real parts,
then the zero solution (also called the trivial solution) is asymptotically
stable. It is an immediate corollary of
Theorem 3.1. Suppose that A is an n × n (real) matrix. The following
statements are equivalent:
(1) There is a norm | |a on Rn and a real number λ > 0 such that for
all v ∈ Rn and all t ≥ 0,
|etAv|a ≤ e−λt|v|a.
(2) If | |g is an arbitrary norm on Rn, then there is a constant C > 0
and a real number λ > 0 such that for all v ∈ Rn and all t ≥ 0,
|etAv|g ≤ Ce−λt|v|g.
(3) Every eigenvalue of A has negative real part.
Moreover, if −λ exceeds the largest of all the real parts of the eigenvalues
of A, then λ can be taken to be the decay constant in (1) or (2).
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
C. Chicone, Ordinary Differential Equations with Applications, Texts in
Applied Mathematics 34, https://doi.org/10.1007/978-3-031-51652-8 3
183184 3. Stability of Linear Systems
Corollary 3.2. If every eigenvalue of A has negative real part, then the
zero solution of x˙ = Ax is asymptotically stable.
Proof. We will show that (1) ⇒ (2) ⇒ (3) ⇒ (1).
To show (1) ⇒ (2), let | |a be the norm in statement (1) and | |g
the norm in statement (2). Because these norms are defined on the finite￾dimensional vector space Rn, they are equivalent; that is, there are con￾stants K1 > 0 and K2 > 0 such that for all x ∈ Rn we have
K1|x|g ≤ |x|a ≤ K2|x|g.
(Prove this!) Hence, if t ≥ 0 and v ∈ Rn, then
|etAv|g ≤
1
K1
|etAv|a ≤
1
K1
e−λt|v|a ≤
K2
K1
e−λt|v|g.
To show (2) ⇒ (3), suppose that statement (2) holds but statement (3)
does not. In particular, A has an eigenvalue μ ∈ C, say μ = α + iβ with
α ≥ 0. Moreover, there is at least one eigenvector v = 0 corresponding to
this eigenvalue. By Proposition 2.26, the system ˙x = Ax has a solution
t → γ(t) of the form t → eαt((cos βt)u − (sin βt)w) where v = u + iw,
u ∈ Rn and w ∈ Rn. By inspection, limt→∞ γ(t) = 0. But if statement (2)
holds, then limt→∞ γ(t) = 0, in contradiction.
To finish the proof we will show (3) ⇒ (1). Let us assume that state￾ment (3) holds. Since A has a finite set of eigenvalues and each of its
eigenvalues has negative real part, there is a number λ > 0 such that the
real part of each eigenvalue of A is less than −λ.
By Proposition 2.45, the components of etA are finite sums of terms of the
form p(t)eαt sin βt or p(t)eαt cos βt where α is the real part of an eigenvalue
of A and p(t) is a polynomial of degree at most n − 1. In particular, if the
matrix etA, partitioned by columns, is given by [c1(t),...,cn(t)], then each
component of each vector ci(t) is a sum of such terms.
Let us denote the usual norm of a vector v = (v1,...,vn) in Rn by |v|.
Also, |vi| is the absolute value of the real number vi, or (if you like) the
norm of the vector vi ∈ R. With this notation we have
|etAv| ≤ n
i=1
|ci(t)||vi|.
Because
|vi| ≤ n
j=1
|vj |
21/2 = |v|,
it follows that
|etAv|≤|v|
n
i=1
|ci(t)|.3. Stability of Linear Systems 185
If β1,...,β are the nonzero imaginary parts of the eigenvalues of A and
if α denotes the largest real part of an eigenvalue of A, then using the
structure of the components of the vector ci(t) it follows that
|ci(t)|
2 ≤ e2αt
2
n−2
k=0
|dki(t)||t|
k
where each coefficient dki(t) is a quadratic form in
sin β1t, . . . ,sin βt, cos β1t, . . . , cos βt.
There is a constant M > 0 that does not depend on i or k such that the
supremum of |dki(t)| for t ∈ R does not exceed M2. In particular, for each
i = 1,...,n, we have
|ci(t)|
2 ≤ e2αtM2
2
n−2
k=0
|t|
k,
and as a result
|etAv|≤|v|
n
i=1
|ci(t)| ≤ eαtnM|v|

2
n−2
k=0
|t|
k1/2
.
Because α < −λ < 0, there is some τ > 0 such that for t ≥ τ , we have
the inequality
e(λ+α)t
nM
2
n−2
k=0
|t|
k1/2 ≤ 1,
or equivalently
eαtnM
2
n−2
k=0
|t|
k1/2 ≤ e−λt.
In particular, if t ≥ τ , then for each v ∈ Rn we have
|etAv| ≤ e−λt|v|. (3.1)
To finish the proof, we will construct a new norm for which the same
inequality is valid for all t ≥ 0. In fact, we will prove that
|v|a :=  τ
0
eλs|esAv| ds
is the required norm.
The easy proof required to show that | |a is a norm on Rn is left to
the reader. To obtain the norm estimate, note that for each t ≥ 0 there186 3. Stability of Linear Systems
is a nonnegative integer m and a number T such that 0 ≤ T <τ and
t = mτ + T. Using this decomposition of t, we find that
|etAv|a =
 τ
0
eλs|esAetAv| ds
=
 τ−T
0
eλs|e(s+t)Av| ds +
 τ
τ−T
eλs|e(s+t)A| ds
=
 τ−T
0
eλs|emτAe(s+T)Av| ds
+
 τ
τ−T
eλs|e(m+1)τAe(T −τ+s)Av| ds.
Let u = T + s in the first integral, let u = T − τ + s in the second integral,
use the inequality (3.1), and, for m = 0, use the inequality |emτAeuAv| ≤
e−λmτ |v|, to obtain the estimates
|etAv|a =
 τ
T
eλ(u−T)
|e(mτ+u)Av| du +
 T
0
eλ(u+τ−T)
|e((m+1)τ+u)Av| du
≤
 τ
T
eλ(u−T)
e−λ(mτ)
|euAv| du
+
 T
0
eλ(u+τ−T)
e−λ(m+1)τ |euAv| du
≤
 τ
0
eλue−λ(mτ+T)
|euAv| du
= e−λt  τ
0
eλu|euAv| du
≤ e−λt|v|a,
as required. ✷
Recall that a square matrix is said to be infinitesimally hyperbolic if
all of its eigenvalues have nonzero real parts. The following corollary of
Theorem 3.1 is the basic result about the dynamics of hyperbolic linear
systems.
Corollary 3.3. If A is an n × n (real) infinitesimally hyperbolic matrix,
then there are two A-invariant subspaces Es and Eu of Rn such that Rn =
Es ⊕ Eu. Moreover, if | |g is a norm on Rn, then there are constants
λ > 0, μ > 0, C > 0, and K > 0 such that for all v ∈ Es and all t ≥ 0
|etAv|g ≤ Ce−λt|v|g,
and for all v ∈ Eu and all t ≤ 0
|etAv|g ≤ Keμt|v|g.3. Stability of Linear Systems 187
Also, there exists a norm on Rn such that the above inequalities hold for
C = K = 1 and λ = μ.
Proof. The details of the proof are left as an exercise. But let us note that
if A is infinitesimally hyperbolic, then we can arrange for the Jordan form
J of A to be a block matrix
J =
As 0
0 Au

where the eigenvalues of As all have negative real parts and the eigenval￾ues of Au have positive real parts. Thus, there is an obvious J-invariant
splitting of the vector space Rn into a stable space and an unstable space.
By changing back to the original coordinates, it follows that there is a
corresponding A-invariant splitting. The hyperbolic estimate on the stable
space follows from Theorem 3.1 applied to the restriction of A to its sta￾ble subspace; the estimate on the unstable space follows from Theorem 3.1
applied to the restriction of −A to the unstable subspace of A. Finally, an
adapted norm on the entire space is obtained as follows:
|(vs, vu)|
2
a = |vs|
2
a + |vu|
2
a. ✷
The basic result of this chapter—If all eigenvalues of the matrix A are in
the left half-plane, then the zero solution of the corresponding homogeneous
linear system is asymptotically stable—is a special case of the principle of
linearized stability. This result provides a method to determine the stability
of the zero solution that does not require knowing other solutions of the
system. As we will see, the same idea works in more general contexts. But,
additional hypotheses are required for most generalizations.
Exercise 3.4. Let
A :=
⎛
⎝
5 −4 −2
−2 7 −14
−10 20 −19
⎞
⎠ , B = 1
4
⎛
⎝
−2 4 −2
−3 6 −1
00 4
⎞
⎠ .
The system ˙x = Ax + αBx is asymptotically stable at α = 0. Does it remain
stable for α > 0? If not, for which values of α is it unstable?
Exercise 3.5. Prove that if α is a real number and A is an n × n real matrix
such that Av, v ≤ α|v|
2 for all v ∈ Rn, then etA ≤ eαt for all t ≥ 0. Hint:
Consider the differential equation ˙x = Ax and the inner product x, x ˙ . Prove the
following more general result suggested by Weishi Liu. Suppose that t → A(t)
and t → B(t) are smooth n × n matrix valued functions defined on R such that
A(t)v, v ≤ α(t)|v|
2 and B(t)v, v ≤ 0 for all t ≥ 0 and all v ∈ Rn. If t → x(t)
is a solution of the differential equation ˙x = A(t)x + B(t)x, then
|x(t)| ≤ e
 t
0 α(s) ds|x(0)|
for all t ≥ 0.188 3. Stability of Linear Systems
Exercise 3.6. Find Es, Eu, C, K, λ, and μ as in Corollary 3.3 (relative to the
usual norm) for the matrix
A := 2 1
0 −3

.
Exercise 3.7. As a continuation of Exercise 3.5, suppose that A is an n × n
matrix and that there is a number λ > 0 such that every eigenvalue of A has real
part less than −λ. Prove that there is an inner product and associated norm such
that Ax, x≤−λ|x|
2 for all x ∈ Rn and conclude that |etAx| ≤ e−λt|x|. This
gives an alternative method of constructing an adapted norm (see [140], p. 146).
Show that there is a constant C > 0 such that |etAx| ≤ Ce−λt|x| with respect
to the usual norm. Moreover, show that there is a constant k > 0 such that if B
is an n × n matrix, then |etBx| ≤ Ce(kB−A−λ)t
|x|. In particular, if B − A is
sufficiency small, then there is some μ > 0 such that |etBx| ≤ Ce−μt|x|.
Exercise 3.8. Suppose that A and B are n×n real matrices and all eigenvalues
of B are positive real numbers. Also, let BT denote the transpose of B. Show
that there is a value μ∗ of the parameter μ such that the rest point at the origin
of the system
X˙ = AX − μXBT
is asymptotically stable whenever μ>μ∗. Hint: X is a matrix valued variable.
Let λ be an eigenvalue of A (perhaps complex) and γ an eigenvalue of B with
corresponding eigenvectors u and v. The product of the matrices u and vT is an
n × n-matrix and an eigenvector of the linear transformation X → AX − XBT
with eigenvalue λ−γ. Show that in general the eigenvalues of the linear operator
X → AX − XBT are given by differences of eigenvalues of A and B. Prove this
first in case A and B are diagonalizable and then use the density of the diagonal￾izable matrices in the space of complex matrices or the continuous dependence
of eigenvalues on the components of the corresponding matrix (cf. [233], p. 331).
Exercise 3.9. Let A be an invertible n×n-matrix and consider the differential
equation B˙ = −ABA−1. The zero matrix is a rest point of the autonomous
system. State properties of A that imply this rest point is asymptotically stable.
Hint: This problem is meant to be open-ended. At least consider the cases n = 1
and n = 2.
Exercise 3.10. (a) Suppose that ˙x = f(x, t) is a smooth vector differential
equation and f(0, t) = 0 for all t ≥ 0. Write a definition of Lyapunov stability
for systems of this type. (b) Suppose that A is a constant square matrix and all
solutions of ˙x = Ax are bounded as t → ∞. Prove: If B is a continuous matrix
function of time, whose values are compatible with the dimensions of A, such
that  ∞
0 |B(t)| dt < ∞, then the zero solution of ˙x = Ax + B(t)x is Lyapunov
stable.4
Stability of Nonlinear Systems
Theorem 3.1 states that the zero solution of a constant coefficient homo￾geneous linear system is asymptotically stable if the spectrum of the coef￾ficient matrix lies in the left half of the complex plane. The principle of
linearized stability states that the same result is true for steady state solu￾tions of nonlinear equations provided that the system matrix of the lin￾earized system along the steady state solution has its spectrum in the left
half-plane. As stated, this principle is not a theorem. In this section, how￾ever, we will formulate and prove a theorem on linearized stability which
is strong enough for most applications. In particular, we will prove that
a rest point of an autonomous differential equation ˙x = f(x) in Rn is
asymptotically stable if all eigenvalues of the Jacobian matrix at the rest
point have negative real parts. Our stability result is also valid for some
nonhomogeneous nonautonomous differential equations of the form
x˙ = A(t)x + g(x, t), x ∈ Rn (4.1)
where g : Rn × R → Rn is a smooth function.
4.1 Variation of Parameters and Solution of
Inhomogeneous Linear Systems
A fundamental tool used in stability theory is the variation of parameters
formula proved in the next proposition.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
C. Chicone, Ordinary Differential Equations with Applications, Texts in
Applied Mathematics 34, https://doi.org/10.1007/978-3-031-51652-8 4
189190 4. Stability of Nonlinear Systems
Proposition 4.1 (Variation of Parameters Formula). Consider the
initial value problem
x˙ = A(t)x + g(x, t), x(t0) = x0 (4.2)
and let t → Φ(t) be a fundamental matrix solution for the homogeneous
system x˙ = A(t)x that is defined on some interval J0 containing t0. If t →
φ(t) is the solution of the initial value problem defined on some subinterval
of J0, then
φ(t) = Φ(t)Φ−1(t0)x0 + Φ(t)
 t
t0
Φ−1(s)g(φ(s), s) ds. (4.3)
Proof. Define a new function z by z(t)=Φ−1(t)φ(t). We have
φ˙(t) = A(t)Φ(t)z(t) + Φ(t) ˙z(t).
Thus,
A(t)φ(t) + g(φ(t), t) = A(t)φ(t) + Φ(t) ˙z(t)
and
z˙(t)=Φ−1(t)g(φ(t), t).
Also note that z(t0)=Φ−1(t0)x0.
By integration,
z(t) − z(t0) =  t
t0
Φ−1(s)g(φ(s), s) ds,
or, in other words,
φ(t) = Φ(t)Φ−1(t0)x0 + Φ(t)
 t
t0
Φ−1(s)g(φ(s), s) ds. ✷
In the special case where the function g in the differential equation (4.2)
is a constant with respect to its first variable, variation of parameters solves
the initial value problem once a fundamental matrix solution of the associ￾ated homogeneous system is determined. In case the homogeneous system
has constant coefficients
φ(t) = etAx0 +
 t
0
e(t−s)Ag(φ(s), s) ds. (4.4)
Exercise 4.2. Consider the linear system
u˙ = −δ2
u + v + δw, v˙ = −u − δ2
v + δw, w˙ = −δw
where δ is a parameter. Find the general solution of this system using matrix
algebra and also by using the substitution z = u+iv. Describe the phase portrait
for the system for each value of δ. Find an invariant line and determine the rate
of change with respect to δ of the angle this line makes with the positive w-axis.
Also, find the angular velocity of the “twist” around the invariant line.4.1 Variation of Parameters and Solution of Inhomogeneous . . . 191
Exercise 4.3. (a) Use variation of parameters to solve the system
x˙ = x − y + e
−t
, y˙ = x + y + e
−t
.
(b) Find the set of initial conditions at t = 0 so that limt→∞(x(t), y(t)) = (0, 0)
whenever t → (x(t), y(t)) satisfies one of these initial conditions.
Exercise 4.4. (a) Verify that the scalar functions y1 and y2 defined for x > 0
and given by
y1(x) = cos x √x , y2(x) = sin x √x
are solutions of the Bessel equation
x2
y + xy + (x2 − p2
)y = 0
for the case p = 1/2. If you have studied power series solutions of ODEs, derive
these solutions. (b) Derive the general solution of the differential equation
x2
y + xy + (x2 − 1
4
)y = x3/2 cos x.
Exercise 4.5. Suppose that g : Rn → Rn is smooth and consider the family of
solutions t → φ(t, ξ, ) of the family of differential equations
x˙ = Ax + x + 
2
g(x)
with parameter  such that φ(0, ξ, ) = ξ. Compute the derivative φ(1, ξ, 0). Hint:
Solve an appropriate variational equation using variation of parameters.
Exercise 4.6. The product Φ(t)Φ−1(s) appears in the variation of parameters
formula where Φ(t) is the principal fundamental matrix for the system ˙x = A(t)x.
Show that if A is a constant matrix or A is 1 × 1, then Φ(t)Φ−1(s) = Φ(t − s).
Prove that this formula does not hold in general for homogeneous linear systems.
Exercise 4.7. Give an alternative proof of Proposition 4.1 by verifying directly
that the variation of parameters formula (4.3) is a solution of the initial value
problem (4.2).
Exercise 4.8. Suppose that A is an n × n-matrix all of whose eigenvalues have
negative real parts. (a) Find a (smooth) function f : R → R so that a solution of
the scalar equation ˙x = −x + f(t) is not bounded for t ≥ 0. (b) Show that there
is a (smooth) function f : R → Rn so that a solution of the system ˙x = Ax+f(t)
is not bounded for t ≥ 0. (c) Show that if the system ˙x = Ax + f(t) does have a
bounded solution, then all solutions are bounded.
Exercise 4.9. Suppose that T > 0 and λ is a continuously differentiable func￾tion defined on the interval 0 ≤ t ≤ T. Consider the second-order initial value
problem ¨x + x = λ(t)x, x(0) = 1, and ˙x(0) = 0. (a) Prove that there is a
unique solution defined on 0 ≤ t ≤ T. (b) A classical and useful representation
of the solution, as well as an alternative existence proof, can be obtained from
the method of successive approximation. Use variation of parameters to obtain
an integral equation for x in the form
x(t) = h(t) +  t
0
k(s)x(s) ds.192 4. Stability of Nonlinear Systems
The function k is called the kernel of the linear transformation K given by
Kg(t) =  t
0 k(s)g(s) ds. Prove that K is a bounded linear transformation from
the space of continuous functions on 0 ≤ t ≤ T to itself with respect to the usual
supremum norm: g := max0≤t≤T |g(t)|. (c) Write the integral equation, called
a Volterra integral equation, in the more compact form x = h + Kx, note that
(I − K)x = h, and (formally) x = (I − K)
−1h. Show, with a formal calculation,
that x = ∞
n=0 Knh. This is the desired representation of x. Approximations can
be obtained by truncating the infinite series, called a Neumann series. (d) Prove
that the Neumann series converges in the supremum norm. Hint: The essential
integral estimate is made crudely by estimating over the maximum interval of
integration 0 ≤ t ≤ T and more delicately by estimating over the intervals [0, t].
Note: The differentiability of λ is not required.
Exercise 4.10. The one-way wave equation φt+cφx = 0 (where c is a constant)
is a test bed for numerical solutions of similar partial differential equations, espe￾cially those of the same form but with c a function of t or x. (a) A basic fact
is that the one-way wave equation can be solved exactly. Indeed, show that for
every differentiable function f : R → R, the partial differential equation has the
solution φ(t, x) = f(x − ct). (b) A method for approximating solutions is to con￾sider some finite spatial interval 0 ≤ x ≤ L and use the semi-discretization called
the method of lines. A spatial increment Δx is chosen, usually Δx = L/n for
some positive integer n, and the spatial derivative is approximated by a finite
difference to obtain n + 1 ordinary differential equations, for example, forward
difference
φ˙
j = −c
φj+1 − φj
Δx , j = 0, 1, 2, . . . , n,
backward difference
φ˙
j = −c
φj − φj−1
Δx , j = 0, 1, 2, . . . , n,
or central difference
φ˙
j = −c
φj+1 − φj−1
2Δx , j = 0, 1, 2, . . . , n.
For definiteness let c > 0. A good question: Which (if any) of the suggested
schemes produces useful approximations of solutions of the PDE? As a first test,
consider periodic boundary conditions: φj+n(t) = φj (t). In this case only n equa￾tions need to be considered. For the backward difference, it suffices to consider
the equations for j = 1, 2, 3,...,n. Write this system of equations in matrix form
Φ = ˙ AΦ and determine its dynamics. Discuss the implications of your results
for using solutions of this ODE to approximate solutions of the PDE. Hint: Use
the Gerschgorin Circle Theorem. (c) Repeat part (b) for forward differences.
latter (d) Repeat part (b) for central differences. (e) All of the parts of this exer￾cise can be explored for solutions with initial data defined on the real line without
periodic boundary conditions. Perhaps φ(t, 0) ≡ 1 and φ(t, L) ≡ 0 with compati￾ble initial data. (f) For the PDE with periodic boundary conditions, prove that
the integral of φ over the spatial domain does not change with time. This is why
such equations are called conservation laws. Are periodic boundary conditions
necessary? (g) For each finite difference approximation of the first derivative,4.2 Alekseev-Gr¨obner formula 193
determine the order of the error with respect to Δx. Hint: use Taylor expansions
at the point where the derivative is evaluated.
4.2 Alekseev-Gr¨obner formula
A nonlinear variation of constants formula, due to Vladimir Alekseev ([5])
and Wolfgang Gr¨obner ([118]), is often useful.
Consider two smooth functions f,g : R × Rn → Rn, a generic point
(t, s, ζ) in R × R × Rn, and the initial value problems
y = f(t, y), y(s) = ζ, (4.5)
z = f(t, z) + g(t, z), z(s) = ζ. (4.6)
Perhaps the functions f and g are defined only on a common subset
of R × Rn. Existence theory must be taken into account to check where
solutions exist. But for simplicity, these details are left to the reader. In
this spirit, assume that the differential equations have solutions φ, ψ : R ×
R×Rn → Rn. Using subscripts for partial derivatives and Leibniz notation
with respect to the variables (t, s, ζ) used as the arguments of φ and ψ,
these functions are chosen to satisfy the equations
φt(t, s, ζ) = f(t, φ(t, s, ζ)), φ(s, s, ζ) = ζ, (4.7)
ψt(t, s, ζ) = f(t, ψ(t, s, ζ)) + g(t, ψ(t, s, ζ)), ψ(s, s, ζ) = ζ. (4.8)
As the form of the differential equations suggests, the motivating idea is
to assume that the solution φ of the first differential equation (4.5) is given
and seek properties of the solution of the second (4.6) using this knowledge.
The goal is to determine a useful integral equation for the unknown solution
ψ.
By differentiating the differential equation (4.7) with respect to the (ini￾tial) vector ζ, with Dψ(t, s, ζ) used to denote the derivative of the function
ζ → ψ(t, s, ζ) with t and s held fixed, and with I used to denote the identity
operator, note that
∂
∂tDψ(t, s, ζ) = Df(t, ψ(t, s, ζ))Dψ(t, s, ζ), D(t, t, ζ) = I. (4.9)
The corresponding linear matrix equation W = Df(t, ψ(t, s, ζ))W is often
called the first variational equation or the linearization of the differential
equation (4.5) along the solution t → ψ(t, s, ζ).
Following a similar prescription, differentiate both sides of the equa￾tion (4.7) with respect to the (initial) time s to obtain
∂
∂tφs(t, s, ζ) = Df(t, φ(t, s, ζ))φs(t, s, ζ). (4.10)194 4. Stability of Nonlinear Systems
Of course, t → φs(t, s, ζ) is thus a vector solution of the matrix first vari￾ational equation. To obtain the initial condition (at t = s) for this special
solution, note that φ(s, s, ζ) = ζ, differentiate with respect to s, and note
that ψ is a solution of equation (4.5) to see that
φs(s, s, ζ) = −f(s, ζ). (4.11)
Using this initial data, the corresponding vector solution of the first varia￾tional equation provides the useful formula
φs(t, s, ζ) = −Dφ(t, s, ζ)f(s, ζ). (4.12)
Theorem 4.11 (Alekseev-Gr¨obner Formula). If φ and ψ are the respec￾tive solutions of the differential equations y = f(t, y) and z = f(t, z) +
g(t, z) with initial data φ(s, s, ζ) = ζ and ψ(s, s, ζ) = ζ, then
ψ(t, s, ζ) − φ(t, s, ζ) =  t
s
Dφ(t, τ, ψ(τ, s, ζ))g(τ,ψ(τ, s, ζ)) dτ. (4.13)
Proof. Define
α(t, τ, s, ζ) = φ(t, τ, ψ(τ, s, ζ))
and note that
α(t, τ, τ, η) = φ(t, τ, ζ), α(τ, τ, s, ζ) = ψ(τ, s, ζ). (4.14)
By differentiating α with respect to τ , using the definition of ψ, and employ￾ing formula (4.12), there is a fortuitous cancellation so that
ατ (t, τ, s, ζ) = φs(t, τ, ψ(τ, s, ζ)) + Dφ(t, τ, ψ(τ, s, ζ))ψt(τ, s, ζ)
= Dφ(t, τ, ψ(τ, s, ζ))g(τ,ψ(τ, s, ζ)).
The desired formula is obtained (immediately) by integration of both sides
of the last equality with respect to τ over the interval s ≤ τ ≤ t and the
identities (4.14). ✷
In case f is linear, the corresponding differential equation is y = A(t)y
for some matrix function A, and the solution operator is also linear. Suppose
it is given by φ(t, s, ζ) = Φ(t, s)ζ, and note that Dφ(t, s, ζ) = Φ(t, s).
In particular, this derivative does not depend on ζ. Substitution into the
Alekseev-Gr¨obner formula for this case produces the usual variation of
constants formula from linear theory:
ψ(t, s, ζ) = Φ(t, s)ζ +
 t
s
Φ(t, τ )g(τ, Φ(t, τ )) dτ. (4.15)
In this sense, formula (4.13) generalizes variation of constants to nonlinear
systems.4.2 Alekseev-Gr¨obner formula 195
For the autonomous case, there are functions Φ and Ψ, which are flows,
such that
φ(t, s, ζ) = Φ(t − s, ζ), ψ(t, s, ζ) = Ψ(t − s, ζ).
By inserting these into formula (4.13) and changing variables in the integral,
easy manipulations yield the equality
Ψ(t, ζ) = Φ(t, ζ) +  t
0
DΦ(t − τ, Ψ(τ,ζ))g(Ψ(τ,ζ)) dτ. (4.16)
Exercise 4.12. Suppose M, γ, and λ are smooth positive functions, and con￾sider the ODE
x˙ + (M(t) − λ(t))x = γ(t)
λ(t)
. (4.17)
A basic result from differential inequalities states that if ˙y ≤ F(y, t), ˙x = F(x, t),
and y(0) = x(0), then y(t) < x(t) for all t > 0 (as long as both solutions exist).
Perhaps this result is intuitively obvious, but it seems to require a nontrivial
proof, one of which is the objective of this exercise. The result implies that the
solution of differential equation (4.17) is an upper bound for every function y
such that
y˙ ≤ (−M(t) + λ(t))y + γ(t)
λ(t)
whenever x and y have the same initial data. Of course, the full power of the
differential inequality theorem is not needed for this application; a linear version
would suffice. (a) Formulate and prove the said linear version. (b) The stated
differential inequality theorem can be proved in many different ways and in more
generality. But, as stated, it is a direct consequence of the Alekseev-Gr¨obner
formula (4.13). Prove this fact. Hints: Note that if ˙y ≤ F(y, t), then there is a
nonpositive function G such that
y˙ = F(y, t) + G(y, t).
Prove that y−x ≤ 0 by framing the problem in the context of Alekseev’s formula
and computing xζ using the ( linear scalar) first variational equation
(xη)t = Fx(x(t, 0, η), t)xη
with appropriate initial condition.
Exercise 4.13. Consider the differential equation ˙x = −x3 and prove that
x(t, ξ) (the solution such that x(0, ξ) = ξ) is O(1/
√t) as t → ∞; that is, there is a
constant C > 0 such that |x(t, ξ)| ≤ C/√t as t → ∞. Next suppose that M and δ
are positive constants and g : R → R is such that |g(x)| ≤ Mx4 whenever |x| < δ.
Prove that if t → x(t, ξ) is the solution of the differential equation ˙x = −x3+g(x)
such that x(0, ξ) = ξ and |ξ| is sufficiently small, then |x(t, ξ)| ≤ C/√t. Hint:
First show that the origin is asymptotically stable using a Lyapunov function.
Write out the nonlinear variation of parameters formula, make an estimate, and
use Gronwall’s inequality.196 4. Stability of Nonlinear Systems
4.3 Stability of Nonlinear Systems
O(ξ0)
V
ξ
U
φ(T,ξ0)
φ(T,ξ) ξ0
Figure 4.1: Local stability as in Proposition 4.15. For every open set U
containing the orbit segment O(ξ0), there is an open set V containing ξ0
such that orbits starting in V stay in U on the time interval 0 ≤ t ≤ T
The next proposition states a useful stability result for solutions of
nonautonomous differential equations with respect to initial conditions.
The following lemma is used in its proof.
Lemma 4.14. Consider a smooth function f : Rn × R → Rn. If K ⊆ Rn
and A ⊆ R are compact sets, then there is a number L > 0 such that
|f(x, t) − f(y, t)| ≤ L|x − y|
for all (x, t),(y, t) ∈ K × A.
Proof. The proof of the lemma uses compactness, continuity, and the mean
value theorem. The details are left as an exercise. ✷
Recall that a function f as in the lemma is called Lipschitz with respect
to its first argument on K × A with Lipschitz constant L.
Proposition 4.15. Consider, for each ξ ∈ Rn, the solution t → φ(t, ξ) of
the differential equation x˙ = f(x, t) such that φ(0, ξ) = ξ. If ξ0 ∈ Rn is
such that the solution t → φ(t, ξ0) is defined for 0 ≤ t ≤ T, and if U ⊆ Rn
is an open set containing the orbit segment O(ξ0) = {φ(t, ξ0):0 ≤ t ≤ T},
then there is an open set V ⊆ U, as in Figure 4.1, such that ξ0 ∈ V and
{φ(t, ξ) : ξ ∈ V, 0 ≤ t ≤ T} ⊆ U; that is, the solution starting at each
ξ ∈ V exists on the interval [0, T], and its values on this interval are in U.
Proof. Let ξ ∈ Rn, and consider the two solutions of the differential equa￾tion given by t → φ(t, ξ0) and t → φ(t, ξ). For t in the intersection of the
intervals of existence of these solutions, we have that
φ(t, ξ) − φ(t, ξ0) = ξ − ξ0 +
 t
0
f(φ(s, ξ), s) − f(φ(s, ξ0), s) ds4.3 Stability of Nonlinear Systems 197
and
|φ(t, ξ) − φ(t, ξ0)|≤|ξ − ξ0| +
 t
0
|f(φ(s, ξ), s) − f(φ(s, ξ0), s)| ds.
We can assume without loss of generality that U is bounded, hence its
closure is compact. It follows from the lemma that the smooth function f
is Lipschitz on U × [0, T] with a Lipschitz constant L > 0. Thus, as long as
(φ(t, ξ), t) ∈ U × [0, T], we have
|φ(t, ξ) − φ(t, ξ0)|≤|ξ − ξ0| +
 t
0
L|φ(s, ξ) − φ(s, ξ0)| ds
and by Gronwall’s inequality
|φ(t, ξ) − φ(t, ξ0)|≤|ξ − ξ0|eLt.
Let δ > 0 be such that δeLT is less than the distance from O(ξ0) to the
boundary of U. Since, on the intersection J of the domain of definition of
the solution t → φ(t, ξ) with [0, T] we have
|φ(t, ξ) − φ(t, ξ0)|≤|ξ − ξ0|eLT ,
the vector φ(t, ξ) is in the bounded set U as long as t ∈ J and |ξ − ξ0| < δ.
By the extension theorem, the solution t → φ(t, ξ) is defined at least on
the interval [0, T]. Thus, the desired set V is {ξ ∈ U : |ξ − ξ0| < δ}. ✷
Turning to stability of rest points, a theoretical foundation for Lya￾punov’s indirect method is achieved by formulating a theorem derived from
the principle of linearized stability. In words, if a nonlinear system has a
rest point at the origin, its linearization at this point has an asymptoti￾cally stable rest point at the origin, and the nonlinear part of the system is
appropriately bounded, then the nonlinear system also has an asymptoti￾cally stable rest point at the origin.
Theorem 4.16. Consider the initial value problem (4.2) for the case where
A := A(t) is a (real) matrix of constants. If all eigenvalues of A have
negative real parts and there are positive constants a > 0 and k > 0 such
that |g(x, t)| ≤ k|x|
2 whenever |x| < a, then there are positive constants C,
b, and α that are independent of the choice of the initial time t0 such that
the solution t → φ(t) of the initial value problem satisfies
|φ(t)| ≤ C|x0|e−α(t−t0) (4.18)
for t ≥ t0 whenever |x0| ≤ b. In particular, the function t → φ(t) is defined
for all t ≥ t0, and the zero solution (the solution with initial value φ(t0) =
0), is asymptotically stable.198 4. Stability of Nonlinear Systems
Proof. By Theorem 3.1 and the hypothesis on the eigenvalues of A, there
are constants C > 1 and λ > 0 such that
etA ≤ Ce−λt (4.19)
for t ≥ 0. Fix δ > 0 such that δ<a and Ckδ − λ < 0, define α := λ − Ckδ
and b := δ/C, and note that α > 0 and 0 <b<δ<a.
If |x0| < b, then there is a maximal half-open interval J = {t ∈ R : t0 ≤
t<τ} such that the solution t → φ(t) of the differential equation with
initial condition φ(t0) = x0 exists and satisfies the inequality
|φ(t)| < δ (4.20)
on the interval J.
For t ∈ J, use the estimate
|g(φ(t), t)| ≤ kδ|φ(t)|,
the estimate (4.19), and the variation of parameters formula
φ(t) = e(t−t0)Ax0 + etA  t
t0
e−sAg(φ(s), s) ds
to obtain the inequality
|φ(t)| ≤ Ce−λ(t−t0)
|x0| +
 t
t0
Ce−λ(t−s)
kδ|φ(s)| ds.
Rearrange the inequality to the form
eλ(t−t0)
|φ(t)| ≤ C|x0| + Ckδ  t
t0
eλ(s−t0)
|φ(s)| ds
and apply Gronwall’s inequality to obtain the estimate
eλ(t−t0)
|φ(t)| ≤ C|x0|eCkδ(t−t0)
;
or equivalently
|φ(t)| ≤ C|x0|e(Ckδ−λ)(t−t0) = C|x0|e−α(t−t0)
. (4.21)
Thus, if |x0| < b and |φ(t)| < δ for t ∈ J, then the required inequality (4.18)
is satisfied for t ∈ J.
If J is not the interval [t0,∞), then τ < ∞. Because |x0| < δ/C and in
view of the inequality (4.21), we have that
|φ(t)| < δe−α(t−t0) (4.22)4.3 Stability of Nonlinear Systems 199
for t0 ≤ t<τ . In particular, the solution is bounded by δ on the interval
[t0, τ ). Therefore, by the extension theorem there is some number  > 0
such that the solution is defined on the interval K := [t0, τ + ). Using
the continuity of the function t → |φ(t)| on K and the inequality (4.22), it
follows that
|φ(τ )| ≤ δe−α(τ−t0) < δ.
By using this inequality and again using the continuity of the function
t → |φ(t)| on K, there is a number η > 0 such that t → φ(t) is defined on
the interval [t0, τ + η), and, on this interval, |φ(t)| < δ. This contradicts
the maximality of τ . ✷
Corollary 4.17. If f : Rn → Rn is smooth (at least class C2), f(ξ)=0,
and all eigenvalues of Df(ξ) have negative real parts, then the differential
equation x˙ = f(x) has an asymptotically stable rest point at ξ. Moreover,
if −α is a number larger than every real part of an eigenvalue of Df(ξ),
and φt is the flow of the differential equation, then there is a neighborhood
U of ξ and a constant C > 0 such that
|φt(x) − ξ| ≤ C|x − ξ|e−αt
whenever x ∈ U and t ≥ 0.
Proof. It suffices to prove the corollary for the case ξ = 0. By Taylor’s
theorem (Theorem 1.252), we can rewrite the differential equation in the
form ˙x = Df(0)x + g(x) where
g(x) :=  1
0
(Df(sx) − Df(0))x ds.
The function ξ → Df(ξ) is class C1. Thus, by the mean value theorem
(Theorem 1.241),
Df(sx) − Df(0)≤|sx| sup
τ∈[0,1]
D2f(τsx)
≤ |x| sup
τ∈[0,1]
D2f(τx).
Again, by the smoothness of f, there is an open ball B centered at the
origin and a constant k > 0 such that
sup
τ∈[0,1]
D2f(τx) < k
for all x ∈ B. Moreover, by an application of Proposition 1.250 and the
above estimates we have that
|g(x)| ≤ sup
s∈[0,1]
|x|Df(sx) − Df(0) ≤ k|x|
2
whenever x ∈ B. The desired result now follows directly from Theorem 4.16.
✷200 4. Stability of Nonlinear Systems
Exercise 4.18. Generalize Theorem 4.16 to the Poincar´e-Lyapunov Theorem:
Let
x˙ = Ax + B(t)x + g(x, t), x(t0) = x0, x ∈ Rn
be a smooth initial value problem. If
(1) A is a constant matrix with spectrum in the left half-plane,
(2) B(t) is the n×n matrix, continuously dependent on t such that B(t) → 0
as t → ∞,
(3) g(x, t) is smooth and there are constants a > 0 and k > 0 such that
|g(x, t)| ≤ k|x|
2
for all t ≥ 0 and |x| < a,
then there are constants C > 1, δ > 0, λ > 0 such that
|x(t)| ≤ C|x0|e
−λ(t−t0)
, t ≥ t0
whenever |x0| ≤ δ/C. In particular, the zero solution is asymptotically stable.
Exercise 4.19. [Lyapunov’s proof] The goal of this exercise is to construct an
alternative proof of linearized stability for autonomous systems using Lyapunov’s
direct method. (a) Consider the differential equation
x˙ = Ax + g(x), x ∈ Rn
where A is a real n × n (constant) matrix and g : Rn → Rn is a smooth function.
Suppose that every eigenvalue of A has negative real part, and that for some
a > 0, there is a constant k > 0 such that, using the usual norm on Rn,
|g(x)| ≤ k|x|
2
whenever |x| < a. Prove that the origin is an asymptotically stable rest point by
constructing a quadratic Lyapunov function. Hint: Let ·, · denote the usual inner
product on Rn, and let A∗ denote the transpose of the real matrix A. Suppose
that there is a real symmetric positive definite n × n-matrix B that satisfies
Lyapunov’s equation
A∗B + BA = −I
and define V : Rn → R by
V (x) = x, Bx.
Show that the restriction of V to a sufficiently small neighborhood of the origin is
a strict Lyapunov function. To estimate a certain inner product use the Schwarz
inequality. Finish the proof by showing that
B :=  ∞
0
e
tA∗
e
tA dt
is a symmetric positive-definite n × n matrix which satisfies Lyapunov’s equa￾tion. To do this, prove that A∗ and A have the same eigenvalues. Then use the
exponential estimates for hyperbolic linear systems to prove that the integral
converges. (b) Give an alternative method to compute solutions of Lyapunov’s4.3 Stability of Nonlinear Systems 201
equation using the following outline: Show that Lyapunov’s equation in the form
A∗B + BA = S, where A is diagonal, S is symmetric and positive definite, and
all pairs of eigenvalues of A have nonzero sums, has a symmetric positive-definite
solution B. In particular, under these hypotheses, the operator B → A∗B + BA
is invertible. Show that the same result is true without the hypothesis that A is
diagonal. Hint: Use the density of the subset of diagonalizable matrices in the
space of complex matrices and the continuity of the eigenvalues of a matrix with
respect to its components (see Exercises 3.8 and 11.1). (c) Prove that the origin
is asymptotically stable for the system ˙x = Ax + g(x) where
A :=
⎛
⎝
−12 0
−2 −1 0
0 0 −3
⎞
⎠ , g(u, v, w) :=
⎛
⎝
u2 + uv + v2 + wv2
w2 + uvw
w3
⎞
⎠
and construct the corresponding matrix B that solves Lyapunov’s equation.
Exercise 4.20. Suppose that f : Rn → Rn is conservative; that is, there is
some function g : Rn → R such that f(x) = grad g(x). Also, suppose that M
and Λ are symmetric positive-definite n × n matrices. Consider the differential
equation
Mx¨ +Λ˙x + f(x)=0, x ∈ Rn
and note that, in case M and Λ are diagonal, the differential equation can be
viewed as a model of n particles each moving according to Newton’s second law
in a conservative force field with viscous damping. (a) Prove that the function
V : Rn × Rn → R defined by
V (x, y) := 1
2
My, y +
 1
0
f(sx), x ds
decreases along orbits of the associated first-order system
x˙ = y, My˙ = −Λy − f(x);
in fact, V˙ = −Λy, y. Conclude that the system has no periodic orbits. (b)
Prove that if f(0) = 0 and Df(0) is positive definite, then the system has an
asymptotically stable rest point at the origin. Prove this fact in two ways: using
the function V and by the method of linearization. Hint: To use the function V
see Exercise 1.182. To use the method of linearization, note that M is invertible,
compute the system matrix for the linearization in block form, suppose there is
an eigenvalue λ, and look for a corresponding eigenvector in block form, that is
the transpose of a vector (x, y). This leads to two equations corresponding to the
block components corresponding to x and y. Reduce to one equation for x and
then take the inner product with respect to x.
Exercise 4.21. (a) Suppose γ is the solution of the initial value problem
x˙ = x2 − (x3 + y3 + z3
)x,
y˙ = y2 − (x3 + y3 + z3
)y,
z˙ = z2 − (x3 + y3 + z3
)z,
(x(0), y(0), z(0) = (1, −1, 0). (4.23)
Prove that limt→∞ γ(t) = (1, 0, 0). (b) Prove that the ω-limit set of every point
in R3 with respect to the flow of system (4.23) is a point in S2.202 4. Stability of Nonlinear Systems
4.4 An Instability Criterion
A natural question: Is a rest point unstable in case not all eigenvalues of
the linearized system matrix have negative real parts? Clearly the answer
is no. The rest point at the origin of the linear system ˙x = −y, ˙y = x is
Lyapunov stable and the eigenvalues of the system matrix have zero real
parts. The question should have been: Is a rest point unstable in case at
least one eigenvalue of the linearized system matrix has a positive real
part? The answer is yes. This fact is implied by a much more general
theory (see Theorem 7.1), which will be discussed later. At least this is
true for autonomous systems. A simpler result is discussed here mainly for
the pedagogical value of the proof techniques.
In preparation to formulate and prove an instability theorem, review
Taylor’s theorem, Ex. 1.253, and the Jordan normal form. The proof of
the next proposition requires a modification of the usual statement of the
Jordan normal form.
Proposition 4.22. If A is an n×n-real matrix whose eigenvalues all have
real parts larger than some positive number α, then there is a basis for Rn
such that for the natural corresponding inner product and associated norm,

Ax, x > α|x|
2;
or, in other words, there is an inner product and associated norm on Rn
such that

Ax, x > α|x|
2.
Proof. There is a number  > 0 such that the real part of every eigenvalue
exceeds α + . Change the basis of the vector space Rn such that in the
new basis A is in real Jordan normal form. Jordan blocks are sums of block
diagonal matrices and corresponding nilpotent matrices whose nonzero ele￾ments are all unity. By an additional change of basis (if necessary) these
nonzero elements can all be changed to  without affecting the zero ele￾ments or the block diagonal matrices. Two cases are considered to prove
this fact.
Suppose that J is a real Jordan block of the form λI + N, construct
the invertible diagonal matrix B whose main diagonal has components
Bii = 1/i−1, and check that BJB−1 = λI + N. Using the same symbol
J for the Jordan block in the new basis given by the columns of B−1,

Jx, x = λ|x|
2 + 
Nx, x.
By Cauchy’s inequality (ab < (a2 + b2)/2),

Nx, x ≥ −|x|
2
and

Jx, x ≥ (λ − )|x|
2 > α|x|
2.4.4 An Instability Criterion 203
For complex eigenvalues, real Jordan blocks are constructed from 2 × 2
diagonal and super diagonal blocks. The super diagonal blocks are identity
matrices and the diagonal blocks for the eigenvalue a + bi are  a −b
b a

. A
similar change of basis changes all the identity matrices to  times identity
without affecting the remaining structure of the Jordan block. In this case
the B matrix consists of 2 × 2 diagonal blocks each of the form identity
times the reciprocal of a power of . The inner product equality is

Jx, x = a|x|
2 + 
Nx, x
and the rest of the proof is the same as before. ✷
Proposition 4.23. If A is an n×n-real matrix whose eigenvalues all have
real parts larger than some positive number λ, U is an open subset of Rn
containing the origin, and g : U → Rn is a class C2 function such that
g(0) = 0 and Dg(0) = 0, then the rest point at the origin for the differential
equation x˙ = Ax + g(x) is not stable.
Proof. There is no loss of generality with two additional assumptions:

Ax, x > λ|x|
2 for all x ∈ Rn and there is a number c > 0 such that
|g(x)| ≤ c|x|
2 for all x ∈ U.
Let t → x(t) denote a solution of the differential equation and consider
the derivative of the inner product 
x, x ˙  with respect to t to obtain the
inequality
d
dt|x|
2 ≥ (2λ − 2c|x|)|x|
2. (4.24)
Choose positive numbers  and β such that λ − c > β > 0 and the ball
B(0) with radius  centered at the origin is contained in U. For the solution
with initial condition x0 ∈ B(0) at t = 0, the last two inequalities imply
that as long as the solution stays in the ball,
d
dt|x|
2 ≥ β|x|
2.
By integration,
|x|
2 ≥ |x0|
2eβt.
For every positive number less than , the distance of the solution from
the origin reaches the number in finite time. The rest point at the origin is
therefore not stable. ✷
The ideas used to prove Proposition 4.23 lie at the heart of the proof of
the next theorem.
Theorem 4.24. If A is an n × n-real matrix with at least one eigenvalue
whose real part is larger than some positive number α, U is an open subset
of Rn containing the origin, and g : U → Rn is a class C2 function such
that g(0) = 0 and Dg(0) = 0, then the rest point at the origin for the
differential equation x˙ = Ax + g(x) is not stable.204 4. Stability of Nonlinear Systems
Proof. There is no loss of generality with several additional assumptions:
U is a ball of radius r centered at the origin, there is a number c > 0 such
that |g(x)| ≤ c|x|
2 for all x ∈ U. The vector space Rn is equal to the product
Rk × R such that system matrix A is split into a k × k matrix U and an
× matrix S such that both are in Jordan normal form, all eigenvalues of
U have positive real parts, all eigenvalues of S have nonpositive real parts
and the system in component form with respect to the product is
˙
ξ = Uξ + g1(ξ,η),
η˙ = Sη + g2(ξ,η).
There is a positive number λ such that 
Uξ, ξ ≥ λ|ξ|
2 for all ξ ∈ Rk and
a positive number σ such that 
Sη, η ≤ σ|η|
2 for all η ∈ R. This latter
number σ can be chosen as small as desired. A choice is forthcoming.
Suppose the rest point at the origin is stable. For each ball B(0) centered
at the origin (of the product space) with radius  > 0 and contained in U,
there is another ball Bδ(0) centered at the origin with radius δ< such
that every solution with initial value in Bδ(0) stays in B(0) for all t > 0.
For all such solutions t → (ξ(t), η(t)), the hypothesis on g implies that
there is a positive number c such that
|g1(ξ,η)| ≤ c(
ξ, ξ + 
η, η)
|g2(ξ,η)| ≤ c(
ξ, ξ + 
η, η).
Consider the derivatives of the inner products 
 ˙
ξ, ξ and 
η, η ˙  with
respect to t to obtain the inequalities
d
dt|ξ|
2 ≥ 2λ|ξ|
2 − 2c(|ξ|
2 + |η|
2)|ξ|,
d
dt|η|
2 ≤ 2σ|η|
2 + 2c(|ξ|
2 + |η|
2)|η|.
Whenever ξ and η are nonzero, these inequalities imply that
d
dt|ξ| ≥ λ|ξ| − c(|ξ|
2 + |η|
2),
d
dt|η| ≤ σ|η| + c(|ξ|
2 + |η|
2). (4.25)
Suppose that |ξ(0)| > 0 and |η(0)| > 0 so that, for sufficiently small
t > 0, the equations (4.25) are valid. The solution stays in B(0). Thus,
because |ξ| <  and |η| < ,
d
dt|ξ| ≥ λ − c|ξ| − c|η|
and
d
dt|η| ≤ σ|η| + c|ξ| + c|η|.4.4 An Instability Criterion 205
Using these latter two inequalities,
d
dt(|ξ|−|η|) ≥ (λ − 2c)|ξ| − (σ + 2c)|η|.
Choose  and σ smaller if necessary so that
0 < σ ≤ λ − 4c.
Then β := λ − 2c > 0 and
d
dt(|ξ|−|η|) ≥ β(|ξ|−|η|).
By integration and with γ := |ξ(0)|−|η(0)|,
|ξ|≥|η| + γeβt. (4.26)
Inequality (4.26) is valid as long as the solution of the original differential
equation with the specified initial data remains in the ball of radius  and
both ξ and η do not vanish. In this case, the function t → |ξ(t)| is larger than
an (exponentially) increasing function by choosing (ξ(0), η(0)) in Bδ(0) and
γ > 0. It therefore grows at least until its values reach . This implies the
rest point is not stable, as required. But, the proof does not eliminate the
possibility that η reaches zero at some finite time T while |x(t)| <  for all
positive t such that t ≤ T.
In the latter case, where η(T0) = 0 at time t = T0, the solution is
such that |ξ(T0)| ≥ γeβT0 . Suppose that η(t) = 0 for some time interval
T0 <t<T1. During this time interval the first equation (4.25) with η = 0
holds; that is,
d
dt|ξ|
2 ≥ 2(λ − c)|ξ|
2.
Its solution implies that
|ξ(T1)| ≥ ξ(T0)eβ(T1−T0)/2.
Thus there are two types of time intervals: those with η = 0 and those where
|η| > 0. Over both, the initial value of |ξ| is multiplied by an exponential
factor with exponent at least β/2 times the duration of the time interval.
In other words,
|ξ(t)| ≥ γeβt/2.
Again, this is in contradiction to the supposed stability of the rest point at
the origin. ✷
The proof of Theorem 4.24 should be compared with those in [67] and [253].
In these references instability for time-dependent systems is considered. All
the ideas needed to reprove these results have been discussed.206 4. Stability of Nonlinear Systems
Exercise 4.25. Let
A =
⎛
⎝
−10
2
−12
2
−4
4 6−
2
4−
2
2−
4
2( − 1) 2( + 1)  − 2
⎞
⎠
and f : R3 → R3 the function given by
f(x, y, z) = 	x2
2 , −y2
2 , xz sin(|x + π2
|)


For u := (x, y, z), every member of the one-parameter family of differential equa￾tions ˙u = A()u + f(u) has a rest point at the origin. Formulate and prove
statements that specify values of  for which the rest point at the origin is stable,
asymptotically stable, or unstable.
Exercise 4.26. Equations of motion, for a Watt governor (with some parame￾ters specified) with governor arm angle θ and angular velocity ω, are
¨θ + δ ˙
θ − ω2 sin θ cos θ = −a sin θ,
d
dt ((b + sin2 θ)ω) = −c(θ − φ),
where a, b, c are positive, δ ≥ 0, and 0 < φ < π/2. (a) Prove that if δ = 0, then
the system has a unique unstable steady state. Hint: Analyze the characteristic
equation for a linearized system matrix at the steady state. (b) Prove that the
system is stabilized for sufficiently large δ.5
Floquet Theory
The subject of this chapter is linear systems of the form
x˙ = A(t)x, x ∈ Rn, (5.1)
where t → A(t) is a T-periodic continuous matrix-valued function. The
main theorem, Floquet’s theorem, gives a canonical form for correspond￾ing fundamental matrix solutions. This result will be used to show that
there is a periodic time-dependent change of coordinates that transforms
differential equation (5.1) into a homogeneous linear system with constant
coefficients.
Floquet’s theorem is a corollary of the following result about the range
of the exponential map.
Theorem 5.1. If C is a nonsingular n × n matrix, then there is an n × n
matrix B (which may be complex) such that eB = C. If C is a nonsingular
real n × n matrix, then there is a real n × n matrix B such that eB = C2.
Proof. If S is a nonsingular n × n matrix such that S−1CS = J is in
Jordan canonical form, and if eK = J, then SeKS−1 = C. As a result,
eSKS−1
= C and B = SKS−1 is the desired matrix. Thus, it suffices to
consider the nonsingular matrix C or C2 to be a Jordan block.
For the first statement of the theorem, assume that C = λI + N where
N is nilpotent; that is, Nm = 0 for some integer m with 0 ≤ m<n.
Because C is nonsingular, λ = 0 and we can write C = λ(I + (1/λ)N). A
computation using the series representation of the function t → ln(1 + t)
at t = 0 shows that, formally (that is, without regard to the convergence
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
C. Chicone, Ordinary Differential Equations with Applications, Texts in
Applied Mathematics 34, https://doi.org/10.1007/978-3-031-51652-8 5
207208 5. Floquet Theory
of the series), if B = (ln λ)I + M where
M =
m
−1
j=1
(−1)j+1
jλj Nj ,
then eB = C. But because N is nilpotent, the series are finite. Thus, the
formal series identity is an identity. This proves the first statement of the
theorem.
The Jordan blocks of C2 correspond to the Jordan blocks of C. The
blocks of C2 corresponding to real eigenvalues of C are all of the type
rI + N where r > 0 and N is real nilpotent. For a real matrix C all
the complex eigenvalues with nonzero imaginary parts occur in complex
conjugate pairs; therefore, the corresponding real Jordan blocks of C2 are
block diagonal or “block diagonal plus block nilpotent” with 2×2 diagonal
subblocks of the form
α −β
β α 
as in equation (2.10). Some of the corresponding real Jordan blocks for the
matrix C2 might have real eigenvalues, but these blocks are again all block
diagonal or “block diagonal plus block nilpotent” with 2 × 2 subblocks.
For the case where a block of C2 is rI + N where r > 0 and N is real
nilpotent a real “logarithm” is obtained by the matrix formula given above.
For block diagonal real Jordan block, write
R = r
cos θ − sin θ
sin θ cos θ

where r > 0, and note that a real logarithm is given by
ln rI +
0 −θ
θ 0

.
Finally, for a “block diagonal plus block nilpotent” Jordan block, factor
the Jordan block as follows:
R(I + N )
where R is block diagonal with R along the diagonal and N has 2 × 2
blocks on its super diagonal all given by R−1. Note that we have already
obtained logarithms for each of these factors. Moreover, it is not difficult
to check that the two logarithms commute. Thus, a real logarithm of the
Jordan block is obtained as the sum of real logarithms of the factors. ✷
Theorem 5.1 can be proved without reference to the Jordan canonical
form (see [6]).5. Floquet Theory 209
Theorem 5.2 (Floquet’s Theorem). If Φ(t) is a fundamental matrix
solution of the T-periodic system (5.1), then, for all t ∈ R,
Φ(t + T) = Φ(t)Φ−1(0)Φ(T).
In addition, there is a matrix B (which may be complex) such that
eT B = Φ−1(0)Φ(T)
and a T-periodic matrix function t → P(t) (which may be complex valued)
such that Φ(t) = P(t)etB for all t ∈ R. Also, there is a real matrix R and
a real 2T-periodic matrix function t → Q(t) such that Φ(t) = Q(t)etR for
all t ∈ R.
Proof. Since the function t → A(t) is periodic, it is defined for all t ∈ R.
Thus, by Theorem 2.6, all solutions of the system are defined for t ∈ R.
If Ψ(t) := Φ(t + T), then Ψ(t) is a matrix solution. Indeed, we have that
Ψ(˙ t) = Φ(˙ t + T) = A(t + T)Φ(t + T) = A(t)Ψ(t),
as required.
Define
C := Φ−1(0)Φ(T)=Φ−1(0)Ψ(0),
and note that C is nonsingular. The matrix function t → Φ(t)C is clearly
a matrix solution of the linear system with initial value Φ(0)C = Ψ(0). By
the uniqueness of solutions, Ψ(t) = Φ(t)C for all t ∈ R. In particular, we
have that
Φ(t + T) = Φ(t)C = Φ(t)Φ−1(0)Φ(T),
Φ(t + 2T) = Φ((t + T) + T) = Φ(t + T)C = Φ(t)C2.
By Theorem 5.1, there is a matrix B, possibly complex, such that
eT B = C.
Also, there is a real matrix R such that
e2T R = C2.
If P(t) := Φ(t)e−tB and Q(t) := Φ(t)e−tR, then
P(t + T) = Φ(t + T)e−T Be−tB = Φ(t)Ce−T Be−tB = Φ(t)e−tB = P(t),
Q(t + 2T) = Φ(t + 2T)e−2T Re−tR = Φ(t)e−tR = Q(t).
Thus, we have P(t + T) = P(t), Q(t + 2T) = Q(t), and
Φ(t) = P(t)etB = Q(t)etR,
as required. ✷210 5. Floquet Theory
Φ(T + τ )Φ−1(τ )v
v
t = τ t = τ + T
Figure 5.1: The figure depicts the geometry of the monodromy operator for
the system ˙x = A(t)x in the extended phase space. The vector v in Rn at
t = τ is advanced to the vector Φ(T + τ )Φ−1(τ )v at t = τ + T.
The representation Φ(t) = P(t)etB in Floquet’s theorem is called a Flo￾quet normal form for the fundamental matrix Φ(t). We will use this normal
form to study the stability of the zero solution of periodic homogeneous lin￾ear systems.
Let us consider a fundamental matrix solution Φ(t) for the periodic sys￾tem (5.1) and a vector v ∈ Rn. The vector solution of the system starting
at time t = τ with initial condition x(τ ) = v is given by
t → Φ(t)Φ−1(τ )v.
If the initial vector is moved forward over one period of the system, then
we again obtain a vector in Rn given by Φ(T + τ )Φ−1(τ )v. The operator
v → Φ(T + τ )Φ−1(τ )v
is called a monodromy operator (see Figure 5.1). Moreover, if we view the
periodic differential equation (5.1) as the autonomous system
x˙ = A(ψ)x, ψ˙ = 1
on the phase cylinder Rn × T where ψ is an angular variable modulo T,
then each monodromy operator is a (stroboscopic) Poincar´e map for our
periodic system. For example, if τ = 0, then the Poincar´e section is the fiber
Rn on the cylinder at ψ = 0. Of course, each fiber Rn at ψ = mT where
m is an integer is identified with the fiber at ψ = 0, and the corresponding
Poincar´e map is given by
v → Φ(T)Φ−1(0)v.5. Floquet Theory 211
The eigenvalues of a monodromy operator are called characteristic mul￾tipliers or Floquet multipliers of the corresponding time-periodic homoge￾neous system (5.1). The next proposition states that characteristic mul￾tipliers are nonzero complex numbers that are intrinsic to the periodic
system—they do not depend on the choice of the fundamental matrix or
the initial time.
Proposition 5.3. The following statements are valid for the periodic linear
homogeneous system (5.1).
(1) Every monodromy operator is invertible. Equivalently, every charac￾teristic multiplier is nonzero.
(2) All monodromy operators have the same eigenvalues. In particular,
there are exactly n characteristic multipliers, counting multiplicities.
Proof. The first statement of the proposition is obvious from the defini￾tions.
To prove statement (2), let us consider the principal fundamental matrix
Φ(t) at t = 0. If Ψ(t) is a fundamental matrix, then Ψ(t) = Φ(t)Ψ(0). Also,
by Floquet’s theorem,
Φ(t + T) = Φ(t)Φ−1(0)Φ(T) = Φ(t)Φ(T).
Consider the monodromy operator M given by
v → Ψ(T + τ )Ψ−1(τ )v
and note that
Ψ(T + τ )Ψ−1(τ ) = Φ(T + τ )Ψ(0)Ψ−1(0)Φ−1(τ )
= Φ(T + τ )Φ−1(τ )
= Φ(τ )Φ(T)Φ−1(τ ).
In particular, the eigenvalues of the operator Φ(T) are the same as the
eigenvalues of the monodromy operator M. Thus, all monodromy operators
have the same eigenvalues. ✷
Because
Φ(t + T) = Φ(t)Φ−1(0)Φ(T),
some authors define characteristic multipliers to be the eigenvalues of the
matrices defined by Φ−1(0)Φ(T) where Φ(t) is a fundamental matrix. Of
course, both definitions give the same characteristic multipliers. To prove
this fact, let us consider the Floquet normal form Φ(t) = P(t)etB and note
that Φ(0) = P(0) = P(T). Thus, we have that
Φ−1(0)Φ(T) = eT B.212 5. Floquet Theory
Also, by using the Floquet normal form,
Φ(T)Φ−1(0) = P(T)eT BΦ−1(0)
= Φ(0)eT BΦ−1(0)
= Φ(0)(Φ−1(0)Φ(T))Φ−1(0),
and therefore Φ−1(0)Φ(T) has the same eigenvalues as the monodromy
operator given by
v → Φ(T)Φ−1(0)v.
In particular, the traditional definition agrees with our geometrically moti￾vated definition.
Returning to consideration of the Floquet normal form P(t)etB for the
fundamental matrix Φ(t) and the monodromy operator
v → Φ(T + τ )Φ−1(τ )v,
note that P(t) is invertible and
Φ(T + τ )Φ−1(τ ) = P(τ )eT BP −1(τ ).
Thus, the characteristic multipliers of the system are the eigenvalues of
eT B. A complex number μ is called a characteristic exponent (or a Floquet
exponent) of the system, if ρ is a characteristic multiplier and eμT = ρ.
Note that if eμT = ρ, then μ + 2πik/T is also a Floquet exponent for each
integer k. Thus, while there are exactly n characteristic multipliers for the
periodic linear system (5.1), there are infinitely many Floquet exponents.
Exercise 5.4. Suppose that a : R → R is a T-periodic function. Find the
characteristic multiplier and a Floquet exponent of the T-periodic system ˙x =
a(t)x. Also, find the Floquet normal form for the principal fundamental matrix
solution of this system at t = t0.
Exercise 5.5. For the autonomous linear system ˙x = Ax a fundamental matrix
solution t → Φ(t) satisfies the identity Φ(T − t) = Φ(T)Φ−1(t). Show that,
in general, this identity does not hold for nonautonomous homogeneous linear
systems. Hint: Write down a Floquet normal form matrix Φ(t) = P(t)etB that
does not satisfy the identity and then show that it is the solution of a (periodic)
nonautonomous homogeneous linear system.
Exercise 5.6. Suppose as usual that A(t) is T-periodic and the Floquet normal
form of the principle fundamental matrix solution of the system ˙x = A(t)x at
t = 0 is given by P(t)etB. (a) Prove that there exists an integer k such that
tr B = 1
T
 T
0
tr A(t) dt + k 2πi
T .
Hint: Use Liouville’s formula 2.20. (b) Prove that the product of the characteristic
multipliers is given by exp( T
0 tr A(t) dt).5. Floquet Theory 213
Let us suppose that a fundamental matrix for the system (5.1) is rep￾resented in Floquet normal form by P(t)etB. We have seen that the char￾acteristic multipliers of the system are the eigenvalues of eT B, but the
definition of the Floquet exponents does not mention the eigenvalues of
B. Are the eigenvalues of B Floquet exponents? This question is answered
affirmatively by the following general theorem about the exponential map.
Theorem 5.7. If A is an n×n matrix and if λ1,...,λn are the eigenvalues
of A repeated according to their algebraic multiplicity, then λk
1,...,λk
n are
the eigenvalues of Ak and eλ1 ,...,eλn are the eigenvalues of eA.
Proof. We will prove the theorem by induction on the dimension n.
The theorem is clearly valid for 1 × 1 matrices. Suppose that it is true
for all (n − 1) × (n − 1) matrices. Define λ := λ1, and let v = 0 denote a
corresponding eigenvector so that Av = λv. Also, let e1,..., en denote the
usual basis of Cn. There is a nonsingular n×n matrix S such that Sv = e1.
(Why?) Thus,
SAS−1e1 = λe1,
and it follows that the matrix SAS−1 has the block form
SAS−1 =
λ ∗
0 A

.
The matrix SAkS−1 has the same block form, only with the block diago￾nal elements λk and Ak. Clearly the eigenvalues of this block matrix are λk
together with the eigenvalues of Ak. By induction, the eigenvalues of Ak are
the kth powers of the eigenvalues of A. This proves the second statement
of the theorem.
Using the power series definition of exp, we see that eSAS−1
has block
form, with block diagonal elements eλ and eA
. Clearly, the eigenvalues of
this block matrix are eλ together with the eigenvalues of eA
. Again using
induction, it follows that the eigenvalues of eA are eλ2 ,...,eλn . Thus, the
eigenvalues of eSAS−1
= SeAS−1 are eλ1 ,...,eλn . ✷
Theorem 5.7 is an example of a spectral mapping theorem. If we let
σ(A) denote the spectrum of the matrix A, that is, the set of all λ ∈ C
such that λI − A is not invertible, then, for our finite dimensional matrix,
σ(A) coincides with the set of eigenvalues of A. Theorem 5.7 can be restated
as follows: eσ(A) = σ(eA).
The next result uses Floquet theory to show that the differential equa￾tion (5.1) is equivalent to a homogeneous linear system with constant coef￾ficients. This result demonstrates that the stability of the zero solution can
often be determined by the Floquet multipliers.
Theorem 5.8. If the principal fundamental matrix solution of the T-periodic
differential equation x˙ = A(t)x (system (5.1)) at t = 0 is given by Q(t)etR214 5. Floquet Theory
where Q and R are real, then the time-dependent change of coordinates x =
Q(t)y transforms this system to the (real) constant coefficient linear system
y˙ = Ry. In particular, there is a time-dependent (2T-periodic ) change
of coordinates that transforms the T-periodic system to a (real) constant
coefficient linear system.
(1) If the characteristic multipliers of the periodic system (5.1) all have
modulus less than one; equivalently, if all characteristic exponents
have negative real part, then the zero solution is asymptotically stable.
(2) If the characteristic multipliers of the periodic system (5.1) all have
modulus less than or equal to one; equivalently, if all characteristic
exponents have nonpositive real part, and if the algebraic multiplic￾ity equals the geometric multiplicity of each characteristic multiplier
with modulus one; equivalently, if the algebraic multiplicity equals the
geometric multiplicity of each characteristic exponent with real part
zero, then the zero solution is Lyapunov stable.
(3) If at least one characteristic multiplier of the periodic system (5.1) has
modulus greater than one; equivalently, if a characteristic exponent
has positive real part, then the zero solution is unstable.
Proof. We will prove the first statement of the theorem and part (1). The
proof of the remaining two parts is left as an exercise. For part (2), note
that since the differential equation is linear, the Lyapunov stability may
reasonably be determined from the eigenvalues of a linearization.
By Floquet’s theorem, there is a real matrix R and a real 2T-periodic
matrix Q(t) such that the principal fundamental matrix solution Φ(t) of
the system at t = 0 is represented by
Φ(t) = Q(t)etR.
Also, there is a matrix B and a T-periodic matrix P such that
Φ(t) = P(t)etB.
The characteristic multipliers are the eigenvalues of eT B. Because Φ(0) is
the identity matrix, we have that
Φ(2T) = e2T R = e2T B,
and in particular
(eT B)
2 = e2T R.
By Theorem 5.7, the eigenvalues of e2T R are the squares of the character￾istic multipliers. These all have modulus less than one. Thus, by another
application of Theorem 5.7, all eigenvalues of the real matrix R have neg￾ative real parts.5. Floquet Theory 215
Consider the change of variables x = Q(t)y. Because
x(t) = Q(t)etRx(0)
and Q(t) is invertible, we have that y(t) = etRx(0), and therefore,
y˙ = Ry.
By our previous result about linearization (Lyapunov’s indirect method),
the zero solution of ˙y = Ry is asymptotically stable. In fact, by Theo￾rem 3.1, there are numbers λ > 0 and C > 0 such that
|y(t)| ≤ Ce−λt|y(0)|
for all t ≥ 0 and all y(0) ∈ Rn. Because Q is periodic, it is bounded.
Thus, by the relation x = Q(t)y, the zero solution of ˙x = A(t)x is also
asymptotically stable. ✷
While the stability theorem just presented is very elegant, in applied
problems it is usually impossible to compute the eigenvalues of eT B explic￾itly. In fact, because eT B = Φ(T), it is not at all clear that the eigenvalues
can be found without solving the system, that is, without an explicit rep￾resentation of a fundamental matrix. Note, however, that we only have to
approximate finitely many numbers (the Floquet multipliers) to determine
the stability of the system. This fact is important! For example, the stability
can often be determined by applying a numerical method to approximate
the Floquet multipliers.
Exercise 5.9. If the planar system ˙u = f(u) has a limit cycle, then it is possible
to find coordinates in a neighborhood of the limit cycle so that the differential
equation has the form
ρ˙ = h(ρ, ϕ)ρ, ϕ˙ = ω
where ω is a constant and for each ρ the function ϕ → h(ρ, ϕ) is 2π/ω-periodic.
Prove: If the partial derivative of h with respect to ρ is identically zero, then there
is a coordinate system such that the differential equation in the new coordinates
has the form
r˙ = cr, φ˙ = ω.
Hint: Use Exercise 5.4 and Theorem 5.8.
Exercise 5.10. View the damped periodically forced Duffing equation ¨x + ˙x −
x + x3 =  sin ωt on the phase cylinder. The unperturbed system ( = 0) has a
periodic orbit on the phase cylinder with period 2π/ω corresponding to its rest point
at the origin of the phase plane. Determine the Floquet multipliers associated with
this periodic orbit of the unperturbed system, that is, the Floquet multipliers of the
linearized system along the periodic orbit.
Exercise 5.11. Consider the system of two coupled oscillators with periodic
parametric excitation
x¨ + (1 + a cos ωt)x = y − x, y¨ + (1 + a cos ωt)y = x − y216 5. Floquet Theory
where a and ω are nonnegative parameters. (See Section 6.3 for a derivation of
the coupled oscillator model.) (a) Prove that if a = 0, then the zero solution
is Lyapunov stable. (b) Using a numerical method (or otherwise), determine the
Lyapunov stability of the zero solution for fixed but arbitrary values of the param￾eters. (c) What happens if viscous damping is introduced into the system? Hint:
A possible numerical experiment might be designed as follows. For each point in
a region of (ω, a)-space, mark the point green if the corresponding system has a
Lyapunov stable zero solution; otherwise, mark it red. To decide which regions
of parameter space might contain interesting phenomena, recall from your expe￾rience with second-order scalar differential equations with constant coefficients
(mathematical models of springs) that resonance is expected when the frequency
of the periodic excitation is rationally related to the natural frequency of the
system. Consider resonances between the frequency ω of the excitation and the
frequency of periodic motions of the system with a = 0, and explore the region
of parameter space near these parameter values. Although interesting behavior
does occur at resonances, this is not the whole story. Because the monodromy
matrix is symplectic (see [12, Sec. 42]), the characteristic multipliers have two
symmetries: If λ is a characteristic multiplier, then so is its complex conjugate
and its reciprocal. It follows that on the boundary between the stable and unsta￾ble regions a pair of characteristic exponents coalesce on the unit circle. Thus, it
is instructive to determine the values of ω, with a = 0, for those characteristic
multipliers that coalesce. These values of ω determine the points where unstable
regions have boundary points on the ω-axis.
Is there a method to determine the characteristic exponents without find￾ing the solutions of the differential equation (5.1) explicitly? An example
of Lawrence Marcus and Hidehiko Yamabe shows no such method can be
constructed in any obvious way from the eigenvalues of A(t). Consider the
π-periodic system ˙x = A(t)x where
A(t) =  −1 + 3
2 cos2 t 1 − 3
2 sin t cost
−1 − 3
2 sin t cost −1 + 3
2 sin2 t

. (5.2)
It turns out that A(t) has the (time independent) eigenvalues 1
4 (−1±√7 i).
In particular, the real part of each eigenvalue is negative. On the other hand,
x(t) = et/2
− cost
sin t

is a solution, and therefore the zero solution is unstable!
The situation is not hopeless. An important example (Hill’s equation)
where the stability of the zero solution of the differential equation (5.1)
can be determined in some cases is discussed in the next section.
Exercise 5.12. (a) Find the principal fundamental matrix solution Φ(t) at
t = 0 for theMarcus–Yamabe system;its systemmatrixA(t)is givenin display (5.2).5. Floquet Theory 217
(b) Find the Floquet normal form for Φ(t) and its “real” Floquet normal form. (c)
Determine the characteristic multipliers for the system. (d) The matrix function
t → A(t) is isospectral. Find a matrix function t → M(t) such that (A(t), M(t))
is a Lax pair (see Exercise 2.63). Is every isospectral matrix function the first
component of a Lax pair?
The Floquet normal form can be used to obtain detailed information
about the solutions of the differential equation (5.1). For example, if we use
the fact that the Floquet normal form decomposes a fundamental matrix
into a periodic part and an exponential part, then it should be clear that
for some systems there are periodic solutions and for others there are no
non-trivial periodic solutions. It is also possible to have “quasi-periodic”
solutions. The next lemma will be used to prove these facts.
Lemma 5.13. If μ is a characteristic exponent for the homogeneous linear
T-periodic differential equation (5.1) and Φ(t) is the principal fundamental
matrix solution at t = 0, then Φ(t) has a Floquet normal form P(t)etB such
that μ is an eigenvalue of B.
Proof. Let P(t)etB be a Floquet normal form for Φ(t). By the definition
of characteristic exponents, there is a characteristic multiplier λ such that
λ = eμT , and, by Theorem 5.7, there is an eigenvalue ν of B such that
eνT = λ. Also, there is some integer k = 0 such that ν = μ + 2πik/T.
Define B := B − (2πik/T)I and P(t) = P(t)e(2πikt/T)I . Note that μ is
an eigenvalue of B, the function P is T-periodic, and
P(t)etB = P(t)etB.
It follows that Φ(t) = P(t)etB is a representation in Floquet normal form
where μ is an eigenvalue of B. ✷
A basic result that is used to classify the possible types of solutions that
can arise is the content of the following theorem.
Theorem 5.14. If λ is a characteristic multiplier of the homogeneous lin￾ear T-periodic differential equation (5.1) and eT μ = λ, then there is a
(possibly complex) non-trivial solution of the form
x(t) = eμtp(t)
where p is a T-periodic function. Moreover, for this solution x(t + T) =
λx(t).
Proof. Consider the principal fundamental matrix solution Φ(t) at t = 0.
By Lemma 5.13, there is a Floquet normal form representation Φ(t) =
P(t)etB such that μ is an eigenvalue of B. Hence, there is a vector v = 0
such that Bv = μv. Clearly, it follows that etBv = eμtv, and therefore the
solution x(t) := Φ(t)v is also represented in the form218 5. Floquet Theory
x(t) = P(t)etBv = eμtP(t)v.
The solution required by the first statement of the theorem is obtained by
defining p(t) := P(t)v. The second statement of the theorem is proved as
follows:
x(t + T) = eμ(t+T)
p(t + T) = eμT eμtp(t) = λx(t). ✷
Theorem 5.15. Suppose that λ1 and λ2 are characteristic multipliers of
the homogeneous linear T-periodic differential equation (5.1) and μ1 and
μ2 are characteristic exponents such that eT μ1 = λ1 and eT μ2 = λ2. If
λ1 = λ2, then there are T-periodic functions p1 and p2 such that
x1(t) = eμ1t
p1(t) and x2(t) = eμ2t
p2(t)
are linearly independent solutions.
Proof. Let Φ(t) = P(t)etB (as in Lemma 5.13) be such that μ1 is an
eigenvalue of B. Also, let v1 be a nonzero eigenvector corresponding to the
eigenvalue μ1. Since λ2 is an eigenvalue of the monodromy matrix Φ(T), by
Theorem 5.7 there is an eigenvalue μ of B such that eT μ = λ2 = eT μ2 . It
follows that there is an integer k such that μ2 = μ+2πik/T. Also, because
λ1 = λ2, we have that μ = μ1. Hence, if v2 is a nonzero eigenvector of
B corresponding to the eigenvalue μ, then the eigenvectors v1 and v2 are
linearly independent.
As in the proof of Theorem 5.14, there are solutions of the form
x1(t) = eμ1t
P(t)v1, x2(t) = eμtP(t)v2.
Moreover, because x1(0) = v1 and x2(0) = v2, these solutions are linearly
independent. Finally, let us note that x2 can be written in the required
form
x2(t) = 
eμte2πkit/T 	e−2πkit/T P(t)v2
	
. ✷
The T-periodic system (5.1) has the Floquet normal form
t → Q(t)etR
where Q is a real 2T-periodic function and R is real matrix. By Theo￾rems 2.45 and 5.8, all solutions of the system are represented as finite sums
of real solutions of the two types
q(t)r(t)eαt sin βt and q(t)r(t)eαt cos βt,
where q is 2T-periodic, r is a polynomial of degree at most n−1, and α+iβ
is an eigenvalue of R. We will use Theorem 5.14 to give a more detailed
description of the nature of these real solutions.5. Floquet Theory 219
If the characteristic multiplier λ is a positive real number, then there is
a corresponding real characteristic exponent μ. In this case, if the periodic
function p in Theorem 5.14 is complex, then it can be represented as p = r+
is where both r and s are real T-periodic functions. Because our T-periodic
system is real, both the real and the imaginary parts of a solution are
themselves solutions. Hence, there is a real non-trivial solution of the form
x(t) = eμtr(t) or x(t) = eμts(t). Such a solution is periodic if and only if
λ = 1 or equivalently if μ = 0. On the other hand, if λ = 1 or μ = 0,
then the solution is unbounded either as t → ∞ or as t → −∞. If λ < 1
(equivalently, μ < 0), then the solution is asymptotic to the zero solution
as t → ∞. On the other hand, if λ > 1 (equivalently, μ > 0), then the
solution is unbounded as t → ∞.
If the characteristic multiplier λ is a negative real number, then μ can
be chosen to have the form ν + πi/T where ν is real and eT μ = λ. Hence,
if we again take p = r + is, then we have the solution
eμtp(t) = eνteπit/T (r(t) + is(t))
from which real non-trivial solutions are easily constructed. For example,
if the real part of the complex solution is nonzero, then the real solution
has the form
x(t) = eνt(r(t)cos(πt/T) − s(t)sin(πt/T)).
Such a solution is periodic if and only if λ = −1 or equivalently if ν = 0.
In this case the solution is 2T-periodic. If ν = 0, then the solution is
unbounded. If ν < 0, then the solution is asymptotic to zero as t → ∞. On
the other hand, if ν > 0, then the solution is unbounded as t → ∞.
If λ is complex, then we have μ = α+iβ and there is a solution given by
x(t) = eαt(cos βt + isin βt)(r(t) + is(t)).
Thus, there are real solutions
x1(t) = eαt(r(t)cos βt − s(t)sin βt),
x2(t) = eαt(r(t)sin βt + s(t)cos βt).
If α = 0, then both solutions are unbounded. But, if α < 0, then these
solutions are asymptotic to zero as t → ∞. On the other hand, if α > 0,
then these solutions are unbounded as t → ∞. If α = 0 and there are
relatively prime positive integers m and n such that 2πm/β = nT, then
the solution is nT-periodic. If no such integers exist, then the solution is
called quasi-periodic.
We will prove in Section 5.4 that the stability of a periodic orbit is deter￾mined by the stability of the corresponding fixed point of a Poincar´e map
defined on a Poincar´e section that meets the periodic orbit. Generically,
the stability of the fixed point of the Poincar´e map is determined by the220 5. Floquet Theory
eigenvalues of its derivative at the fixed point. For example, if the eigenval￾ues of the derivative of the Poincar´e map at the fixed point corresponding
to the periodic orbit are all inside the unit circle, then the periodic orbit is
asymptotically stable. It turns out that the eigenvalues of the derivative of
the Poincar´e map are closely related to the characteristic multipliers of a
time-periodic system, namely, the variational equation along the periodic
orbit. We will have much more to say about the general case later. Here
we will illustrate the idea for an example where the Poincar´e map is easy
to compute.
Suppose that
u˙ = f(u, t), u ∈ Rn (5.3)
is a smooth nonautonomous differential equation. If there is some T > 0
such that f(u, t + T) = f(u, t) for all u ∈ Rn and all t ∈ R, then the
system (5.3) is called T-periodic.
The nonautonomous system (5.3) is made “artificially” autonomous by
the addition of a new equation as follows:
u˙ = f(u, ψ), ψ˙ = 1 (5.4)
where ψ may be viewed as an angular variable modulo T. In other words,
we can consider ψ + nT = ψ whenever n is an integer. The phase cylinder
for system (5.4) is Rn ×T, where T (topologically the unit circle) is defined
to be R modulo T. This autonomous system provides the correct geometry
with which to define a Poincar´e map.
For each ξ ∈ Rn, let t → u(t, ξ) denote the solution of the differential
equation (5.3) such that u(0, ξ) = ξ, and note that t → (u(t, ξ), t) is the
corresponding solution of the system (5.4). The set Σ := {(ξ,ψ) : ψ = 0}
is a Poincar´e section, and the corresponding Poincar´e map is given by
ξ → u(T,ξ).
If there is a point p ∈ Rn such that f(p, t) = 0 for all t ∈ R, then the
function t → (p, t), or equivalently t → (u(t, p), t), is a periodic solution
of the system (5.4) with period T. Moreover, let us note that u(T,p) = p.
Thus, the periodic solution corresponds to a fixed point of the Poincar´e
map as it should.
The derivative of the Poincar´e map at p is the linear transformation of
Rn given by the partial derivative uξ(T,p). Moreover, by differentiating
both the differential equation (5.3) and the initial condition u(0, ξ) = ξ
with respect to ξ, it is easy to see that the matrix function t → uξ(t, p)
is the principal fundamental matrix solution at t = 0 of the (T-periodic
linear) variational initial value problem
W˙ = fu(u(t, p), t)W, W(0) = I. (5.5)
If the solution of system (5.5) is represented in the Floquet normal form
uξ(t, p) = P(t)etB, then the derivative of the Poincar´e map is given by5. Floquet Theory 221
uξ(T,p) = eT B. In particular, the characteristic multipliers of the vari￾ational equation (5.5) coincide with the eigenvalues of the derivative of
the Poincar´e map. Thus, whenever the principle of linearized stability is
valid, the stability of the periodic orbit is determined by the characteristic
multipliers of the periodic variational equation (5.5).
As an example, consider the pendulum with oscillating support
¨θ + (1 + a cos ωt)sin θ = 0.
The zero solution, given by θ(t) ≡ 0, corresponds to a 2π/ω-periodic solu￾tion of the associated autonomous system. A calculation shows that the
variational equation along this periodic solution is equivalent to the second￾order differential equation
x¨ + (1 + a cos ωt)x = 0,
called a Mathieu equation. The normal form for the Mathieu equation is
x¨ + (a − 2q cos 2t)x = 0,
where a and q are parameters.
Since, as we have just seen (see also Exercise 5.11), equations of Mathieu
type arise frequently in applications, the stability analysis of such equations
is important (see, for example, [13], [20], [116], [144], [168], and [265]). In
Section 5.2 we will show how the stability of the zero solution of the Mathieu
equation, and, in turn, the stability of the zero solution of the pendulum
with oscillating support, is related in a delicate manner to the amplitude
a and the frequency ω of the periodic displacement.
Exercise 5.16. This is a continuation of Exercise 2.54. Suppose that v : R →
R3 is a periodic function. Consider the differential equation
x˙ = v(t) × x
and discuss the stability of its periodic solutions.
Exercise 5.17. Determine the stability type of the periodic orbit discussed in
Exercise 5.10.
Exercise 5.18. (a) Prove that the system
x˙ = x − y − x(x2 + y2
),
y˙ = x + y − y(x2 + y2
),
z˙ = z + xz − z3
has periodic orbits. Hint: Change to cylindrical coordinates, show that the cylin￾der (with radius one whose axis of symmetry is the z-axis) is invariant, and recall
the analysis of equation (1.44). (b) Prove that there is a stable periodic orbit. (c)
The stable periodic orbit has three Floquet multipliers. Of course, one of them222 5. Floquet Theory
is unity. Find (exactly) a vector v such that Φ(T)v = v, where T is the period of
the periodic orbit and Φ(t) is the principal fundamental matrix solution at t = 0
of the variational equation along the stable periodic solution. (d) Approximate
the remaining two multipliers. Note: It is possible to represent these multipliers
with integrals, but they are easier to approximate using a numerical method.
5.1 Lyapunov Exponents
An important generalization of Floquet exponents, called Lyapunov expo￾nents, is introduced in this section. This concept is used extensively in
the theory of dynamical systems (see, for example, [119], [162], [195], and
[261]).
Consider a (nonlinear) differential equation
u˙ = f(u), u ∈ Rn (5.6)
with flow ϕt. If  ∈ R, ξ, v ∈ Rn, and η := ξ + v, then the two solutions
t → ϕt(ξ), t → ϕt(ξ + v)
start at points that are O() close; that is, the absolute value of the differ￾ence of the two points in Rn is bounded by the usual norm of v times .
Moreover, by Taylor expansion at  = 0, we have that
ϕt(ξ + v) − ϕt(ξ) = Dϕt(ξ)v + O(
2)
where Dϕt(ξ) denotes the derivative of the function u → ϕt(u) evaluated at
u = ξ. Thus, the first-order approximation of the difference of the solutions
at time t is Dϕt(ξ)v where t → Dϕt(ξ) is the principal fundamental matrix
solution at t = 0 of the linearized equation
W˙ = Df(ϕt(ξ))W
along the solution of the original system (5.6) starting at ξ. To see this
fact, just note that
ϕ˙ t(u) = f(ϕt(u))
and differentiate both sides of this identity with respect to u at u = ξ.
If we view v as a vector in the tangent space to Rn at ξ, denoted TξRn,
then Dϕt(ξ)v is a vector in the tangent space Tϕt(ξ)Rn. For each such v,
if v = 0, then it is natural to define a corresponding linear operator L,
from the linear subspace of TξRn generated by v to the linear subspace
of Tϕt(ξ)Rn generated by Dϕt(ξ)v, defined by L(av) = Dϕt(ξ)av where
a ∈ R. Let us note that the norm of this operator measures the relative
“expansion” or “contraction” of the vector v; that is,
L = sup a=0
|Dφt(ξ)av|
|av| = |Dφt(ξ)v|
|v| .5.1 Lyapunov Exponents 223
Our two solutions can be expressed in integral form; that is,
ϕt(ξ) = ξ +

 t
0
f(ϕs(ξ)) ds,
ϕt(ξ + v) = ξ + v +

 t
0
f(ϕs(ξ + v)) ds.
Hence, as long as we consider a finite time interval or a solution that is
contained in a compact subset of Rn, there is a Lipschitz constant Lip(f) >
0 for the function f, and we have the inequality
|ϕt(ξ + v) − ϕt(ξ)| ≤ |v| + Lip(f)

 t
0
|ϕs(ξ + v) − ϕs(ξ)| ds.
By Gronwall’s inequality, the separation distance between the solutions is
bounded by an exponential function of time. In fact, we have the estimate
|ϕt(ξ + v) − ϕt(ξ)| ≤ |v|et Lip(f)
.
The above computation for the norm of L and the exponential bound for
the separation rate between two solutions motivates the following definition
(see [162]).
Definition 5.19. Suppose that ξ ∈ Rn and the solution t → ϕt(ξ) of the
differential equation (5.6) is defined for all t ≥ 0. Also, let v ∈ Rn be a
nonzero vector. The Lyapunov exponent at ξ in the direction v for the flow
ϕt is defined to be
χ(p, v) := limsup t→∞
1
t
ln |Dφt(ξ)v|
|v|

.
As a simple example, let us consider the planar system
x˙ = −ax, y˙ = by
where a and b are positive parameters, and let us note that its flow is given
by
ϕt(x, y)=(e−atx, ebty).
By an easy computation using the definition of the Lyapunov exponents,
it follows that if v is given by v = (w, z) and z = 0, then χ(ξ,v) = b. If
z = 0 and w = 0, then χ(ξ,v) = −a. In particular, there are exactly two
Lyapunov exponents for this system. Of course, the Lyapunov exponents
in this case correspond to the eigenvalues of the system matrix.
Although our definition of Lyapunov exponents is for autonomous sys￾tems, it should be clear that since the definition only depends on the funda￾mental matrix solutions of the associated variational equations along orbits224 5. Floquet Theory
of the system, we can define the same notion for solutions of abstract time￾dependent linear systems. Indeed, for a T-periodic linear system
u˙ = A(t)u, u ∈ Rn (5.7)
with principal fundamental matrix Φ(t) at t = 0, the Lyapunov exponent
defined with respect to the nonzero vector v ∈ Rn is
χ(v) := limsup t→∞
1
t
ln |Φ(t)v|
|v|

.
Proposition 5.20. If μ is a Floquet exponent of the system (5.7), then the
real part of μ is a Lyapunov exponent.
Proof. Let us suppose that the principal fundamental matrix Φ(t) is given
in Floquet normal form by
Φ(t) = P(t)etB.
If μ = a + bi is a Floquet exponent, then there is a corresponding vector
v such that eT Bv = eμT v. Hence, using the Floquet normal form, we have
that
Φ(T)v = eμT v.
If t ≥ 0, then there is a nonnegative integer n and a number r such that
0 ≤ r<T and t = nT + r. Using this representation of t we have that
1
t
ln |Φ(t)v|
|v|

= 1
T
 nT
nT + r
 1
n
ln |P(nT + r)erBenμT v|
|v|

= 1
T
 nT
nT + r
 1
n
ln |enT a| +
1
n
ln |P(r)erBv|
|v|
.
Clearly, n → ∞ as t → ∞. Thus, it is easy to see that
limt→∞
1
T
 nT
nT + r
 1
n
ln |enT a| +
1
n
ln |P(r)erBv|
|v|
 = a. ✷
Let us suppose that a differential equation has a compact invariant set
that contains an orbit whose closure is dense in the invariant set. Then,
the existence of a positive Lyapunov exponent for this orbit ensures that
nearby orbits tend to separate exponentially fast from the dense orbit. But,
since these orbits are confined to a compact invariant set, they must also
be bounded. This suggests that each small neighborhood in the invariant
set undergoes both stretching and folding as it evolves with the flow. The
subsequent kneading of the invariant set due to this stretching and folding
would tend to mix the evolving neighborhoods so that they eventually inter￾twine in a complicated manner. For this reason, the existence of a positive5.2 Hill’s Equation 225
Lyapunov exponent is often taken as a signature of “chaos.” While this cri￾terion is not always valid, the underlying idea that the stretching implied
by a positive Lyapunov exponent is associated with complex motions is
important in the modern theory of dynamical systems.
Exercise 5.21. Show that if two points are on the same orbit, then the corre￾sponding Lyapunov exponents are the same.
Exercise 5.22. Prove the “converse” of Proposition 5.20; that is, every Lya￾punov exponent for a time-periodic system is a Floquet exponent.
Exercise 5.23. If ˙x = f(x), determine the Lyapunov exponent χ(ξ, f(ξ)).
Exercise 5.24. How many Lyapunov exponents are associated with an orbit of
a differential equation in an n-dimensional phase space.
Exercise 5.25. Suppose that x is in the omega limit set of an orbit. Are the
Lyapunov exponents associated with x the same as those associated with the
original orbit?
Exercise 5.26. In all the examples in this section, the lim sup can be replaced
by lim. Are there examples where the superior limit is a finite number, but the
limit does not exist? This is (probably) a challenging exercise! For an answer
see [162] and [195].
5.2 Hill’s Equation
A famous example where Floquet theory applies to give good stability
results is Hill’s equation,
u¨ + a(t)u = 0, a(t + T) = a(t).
It was introduced by George W. Hill in his study of the motions of the moon.
Roughly speaking, the motion of the moon can be viewed as a harmonic
oscillator in a periodic gravitational field. But this model equation arises in
many areas of applied mathematics where the stability of periodic motions
is an issue. A prime example, mentioned in the previous section, is the
stability analysis of small oscillations of a pendulum whose length varies
with time.
If we define
x := u
u˙

,
then Hill’s equation is equivalent to the first-order system ˙x = A(t)x where
A(t) =  0 1
−a(t) 0
.
We will apply linear systems theory, especially Floquet theory, to analyze
the stability of the zero solution of this linear T-periodic system.226 5. Floquet Theory
The first step in the stability analysis is an application of Liouville’s
formula (2.20). In this regard, you may recall from your study of scalar
second-order linear differential equations that if ¨u + p(t) ˙u + q(t)u = 0 and
the Wronskian of the two solutions u1 and u2 is defined by
W(t) := det u1(t) u2(t)
u˙ 1(t) ˙u2(t)

,
then
W(t) = W(0)e−  t
0 p(s) ds. (5.8)
Note that for the equivalent first-order system
x˙ =
 0 1
−q(t) −p(t)

x = B(t)x
with fundamental matrix Ψ(t), formula (5.8) is a special case of Liouville’s
formula
detΨ(t) = det Ψ(0)e
 t
0 tr B(s)ds.
At any rate, let us apply Liouville’s formula to the principal fundamental
matrix Φ(t) at t = 0 for Hill’s system to obtain the identity det Φ(t) ≡ 1.
Since the determinant of a matrix is the product of the eigenvalues of
the matrix, we have an important fact: The product of the characteristic
multipliers of the monodromy matrix, Φ(T), is 1.
Let the characteristic multipliers for Hill’s equation be denoted by λ1
and λ2 and note that they are roots of the characteristic equation
λ2 − (tr Φ(T))λ + det Φ(T)=0.
For notational convenience let us set 2φ = tr Φ(T) to obtain the equivalent
characteristic equation
λ2 − 2φλ +1=0
whose solutions are given by
λ = φ ± φ2 − 1.
There are several cases to consider depending on the value of φ.
Case 1: If φ > 1, then λ1 and λ2 are distinct positive real numbers such
that λ1λ2 = 1. Thus, we may assume that 0 < λ1 < 1 < λ2 with λ1 = 1/λ2
and there is a real number μ > 0 (a characteristic exponent) such that
eT μ = λ2 and e−T μ = λ1. By Theorem 5.14 and Theorem 5.15, there is a
fundamental set of solutions of the form
e−μtp1(t), eμtp2(t)
where the real functions p1 and p2 are T-periodic. In this case, the zero
solution is unstable.5.2 Hill’s Equation 227
Case 2: If φ < −1, then λ1 and λ2 are both real and negative. Also,
since λ1λ2 = 1, we may assume that λ1 < −1 < λ2 < 0 with λ1 = 1/λ2.
Thus, there is a real number μ > 0 (a characteristic exponent) such that
e2T μ = λ2
1 and e−2T μ = λ2
2. As in Case 1, there is a fundamental set of
solutions of the form
eμtq1(t), e−μtq2(t)
where the real functions q1 and q2 are 2T-periodic. Again, the zero solution
is unstable.
Case 3: If −1 <φ< 1, then λ1 and λ2 are complex conjugates each
with nonzero imaginary part. Since λ1λ¯1 = 1, we have that |λ1| = 1, and
therefore both characteristic multipliers lie on the unit circle in the complex
plane. Because both λ1 and λ2 have nonzero imaginary parts, one of these
characteristic multipliers, say λ1, lies in the upper half-plane. Thus, there is
a real number θ with 0 < θT < π and eiθT = λ1. In fact, there is a solution
of the form eiθt(r(t)+is(t)) with r and s both T-periodic functions. Hence,
there is a fundamental set of solutions of the form
r(t)cos θt − s(t)sin θt, r(t)sin θt + s(t)cos θt.
In particular, the zero solution is stable (see Exercise 5.32) but not asymp￾totically stable. Also, the solutions are periodic if and only if there are
relatively prime positive integers m and n such that 2πm/θ = nT. If such
integers exist, all solutions have period nT. If not, then these solutions are
quasi-periodic.
We have just proved the following facts for Hill’s equation: Suppose that
Φ(t) is the principal fundamental matrix solution of Hill’s equation at t = 0.
If |tr Φ(T)| < 2, then the zero solution is stable. If |tr Φ(T)| > 2, then the
zero solution is unstable.
Case 4: If φ = 1, then λ1 = λ2 = 1. The nature of the solutions depends
on the canonical form of Φ(T). If Φ(T) is the identity, then e0 = Φ(T) and
there is a Floquet normal form Φ(t) = P(t) where P(t) is T-periodic and
invertible. Thus, there is a fundamental set of periodic solutions and the
zero solution is stable. If Φ(T) is not the identity, then there is a nonsingular
matrix C such that
CΦ(T)C−1 = I + N = eN
where N = 0 is nilpotent. Thus, Φ(t) has a Floquet normal form Φ(t) =
P(t)etB where B := C−1( 1
T N)C. Because
etB = C−1(I +
t
T
N)C,
the matrix function t → etB is unbounded, and therefore the zero solution
is unstable.
Case 5: If φ = −1, then the situation is similar to Case 4, except the
fundamental matrix is represented by Q(t)etB where Q(t) is a 2T-periodic
matrix function.228 5. Floquet Theory
By the results just presented, the stability of Hill’s equation is reduced,
in most cases, to a determination of the absolute value of the trace of
its principal fundamental matrix evaluated after one period. While this is
a useful fact, it leaves open an important question: Can the stability be
determined without imposing a condition on the solutions of the equation?
It turns out that in some special cases this is possible (see [168] and [265]).
A theorem of Lyapunov [162] in this direction follows.
Theorem 5.27. If a : R → R is a positive T-periodic function such that
T

 T
0
a(t) dt ≤ 4,
then all solutions of the Hill’s equation x¨ + a(t)x = 0 are bounded. In
particular, the trivial solution is stable.
The proof of Theorem 5.27 is outlined in Exercises 5.32 and 5.35.
Exercise 5.28. Consider the second-order system
u¨ + ˙u + cos(t) u = 0.
Prove: (a) If ρ1 and ρ2 are the characteristic multipliers of the corresponding
first-order system, then ρ1ρ2 = exp(−2π). (b) The Poincar´e map for the system
is dissipative; that is, it contracts area.
Exercise 5.29. Prove: The equation
u¨ − (2 sin2 t) ˙u + (1 + sin 2t)u = 0
does not have a fundamental set of periodic solutions. Does it have a nonzero
periodic solution? Is the zero solution stable?
Exercise 5.30. Discuss the stability of the trivial solution of the scalar time￾periodic system ˙x = (cos2 t)x.
Exercise 5.31. Prove: The zero solution is unstable for the system ˙x = A(t)x
where
A(t) := 1/2 − cos t 12
147 3/2 + sin t

.
Exercise 5.32. Prove: If all solutions of the T-periodic system ˙x = A(t)x are
bounded, then the trivial solution is Lyapunov stable.
Exercise 5.33. For Hill’s equation with period T, if the absolute value of the
trace of Φ(T), where Φ(t) is the principal fundamental matrix at t = 0, is strictly
less than two, show that there are no solutions of period T or 2T. On the other
hand, if the absolute value of the trace of Φ(T) is two, show that there is such a
solution. Note that this property characterizes the boundary between the stable
and unstable solutions.5.3 Periodic Orbits of Linear Systems 229
Exercise 5.34. Prove: If a(t) is an even T-periodic function, then Hill’s equa￾tion has a fundamental set of solutions such that one solution is even and one is
odd.
Exercise 5.35. Prove Theorem 5.27. Hint: If Hill’s equation has an unbounded
solution, then there is a real solution t → x(t) and a real Floquet multiplier such
that x(t + T) = λx(t). Define a new function t → u(t) by
u(t) := x˙ (t)
x(t)
,
and show that u is a solution of the Riccati equation
u˙ = −a(t) − u2
.
Use the Riccati equation to prove that the solution x has at least one zero in the
interval [0, T]. Also, show that x has two distinct zeros on some interval whose
length does not exceed T. Finally, use the following proposition to finish the
proof. If f is a smooth function on the finite interval [α, β] such that f(α) = 0,
f(β) = 0, and such that f is positive on the open interval (α, β), then
(β − α)
 β
α
|f(t)|
f(t) dt > 4.
To prove this proposition, first suppose that f attains its maximum at γ and
show that
4
β − α ≤ 1
γ − α +
1
β − γ = 1
f(γ)
f(γ) − f(α)
γ − α − f(β) − f(γ)
β − γ

.
Then, use the mean value theorem and the fundamental theorem of calculus to
complete the proof.
Exercise 5.36. Prove: If t → a(t) is negative, then Hill’s equation ¨x+a(t)x = 0
has an unbounded solution. Hint: Multiply by x and integrate by parts.
5.3 Periodic Orbits of Linear Systems
In this section, we will consider the existence and stability of periodic solu￾tions of the time-periodic system
x˙ = A(t)x + b(t), x ∈ Rn (5.9)
where t → A(t) is a T-periodic matrix function and t → b(t) is a T-periodic
vector function.
Theorem 5.37. If the number one is not a characteristic multiplier of the
T-periodic homogeneous system x˙ = A(t)x, then (5.9) has at least one
T-periodic solution.230 5. Floquet Theory
Proof. Let us show first that if t → x(t) is a solution of system (5.9) and
x(0) = x(T), then this solution is T-periodic. Define y(t) := x(t + T). Note
that t → y(t) is a solution of (5.9) and y(0) = x(0). Thus, by the uniqueness
theorem x(t + T) = x(t) for all t ∈ R.
If Φ(t) is the principal fundamental matrix solution of the homogeneous
system at t = 0, then, by the variation of parameters formula,
x(T) = Φ(T)x(0) + Φ(T)

 T
0
Φ−1(s)b(s) ds.
Therefore, x(T) = x(0) if and only if
(I − Φ(T))x(0) = Φ(T)

 T
0
Φ−1(s)b(s) ds.
This equation for x(0) has a solution whenever the number one is not an
eigenvalue of Φ(T). (Note that the map x(0) → x(T) is the Poincar´e map.
Thus, our periodic solution corresponds to a fixed point of the Poincar´e
map.)
By Floquet’s theorem, there is a matrix B such that the monodromy
matrix is given by
Φ(T) = eT B.
In other words, by the hypothesis, the number one is not an eigenvalue of
Φ(T). ✷
Corollary 5.38. If A(t) = A, a constant matrix such that A is infinitesi￾mally hyperbolic (no eigenvalues on the imaginary axis), then the differ￾ential equation (5.9) has at least one T-periodic solution.
Proof. The monodromy matrix eT A does not have 1 as an eigenvalue. ✷
Exercise 5.39. Discuss the uniqueness of the T-periodic solutions of the sys￾tem (5.9). Also, using Theorem 5.8, discuss the stability of the T-periodic solu￾tions.
In system (5.9) if b = 0, then the trivial solution is a T-periodic solution.
The next theorem states a general sufficient condition for the existence of
a T-periodic solution.
Theorem 5.40. If the T-periodic system (5.9) has a bounded solution, then
it has a T-periodic solution.
Proof. Consider the principal fundamental matrix solution Φ(t) at t = 0
of the homogeneous system corresponding to the differential equation (5.9).
By the variation of parameters formula, we have the equation
x(T) = Φ(T)x(0) + Φ(T)

 T
0
Φ−1(s)b(s) ds.5.4 Stability of Periodic Orbits 231
Also, by Theorem 5.1, there is a constant matrix B such that Φ(T) = eT B.
Thus, the stroboscopic Poincar´e map P is given by
P(ξ) := Φ(T)ξ + Φ(T)

 T
0
Φ−1(s)b(s) ds
= eT B
ξ +

 T
0
Φ−1(s)b(s) ds
.
If the solution with initial condition x(0) = ξ0 is bounded, then the
sequence {Pj (ξ0)}∞
j=0 is bounded. Also, P is an affine map; that is, P(ξ) =
Lξ + y where L = eT B = Φ(T) is a real invertible linear map and y is an
element of Rn.
Note that if there is a point x ∈ Rn such that P(x) = x, then the
system (5.9) has a periodic orbit. Thus, if we assume that there are no
periodic orbits, then the equation
(I − L)ξ = y
has no solution ξ. In other words, y is not in the range R of the operator
I − L.
There is some vector v ∈ Rn such that v is orthogonal to R and the
inner product v, y does not vanish. Moreover, because v is orthogonal to
the range, we have
(I − L)ξ,v = 0
for each ξ ∈ Rn, and therefore
ξ,v = Lξ, v. (5.10)
Using the representation P(ξ) = Lξ + y and an induction argument, it
is easy to prove that if j is a nonnegative integer, then Pj (ξ0) = Lj ξ0 +
j−1
k=0 Lky. By taking the inner product with v and repeatedly applying
the reduction formula (5.10), we have
Pj (ξ0), v = ξ0, v + (j − 1)y, v.
Moreover, because v, y = 0, it follows immediately that
lim
j→∞|Pj (ξ0), v| = ∞,
and therefore the sequence {Pj (ξ0)}∞
j=0 is unbounded, in contradiction. ✷
5.4 Stability of Periodic Orbits
Consider a (nonlinear) autonomous system of differential equations on Rn
given by ˙u = f(u) with a periodic orbit Γ. Also, for each ξ ∈ Rn, define232 5. Floquet Theory
the vector function t → u(t, ξ) to be the solution of this system with the
initial condition u(0, ξ) = ξ.
If p ∈ Γ and Σ ⊂ Rn is a section transverse to f(p) at p, then, as a
corollary of the implicit function theorem, there is an open set Σ ⊆ Σ and
a function T : Σ → R, the time of first return to Σ
, such that for each
σ ∈ Σ, we have u(T(σ), σ) ∈ Σ
. The map P, given by σ → u(T(σ), σ), is
the Poincar´e map corresponding to the Poincar´e section Σ.
The Poincar´e map is defined only on Σ, a manifold contained in Rn. It
is convenient to avoid choosing local coordinates on Σ. Thus, we will view
the elements in Σ also as points in the ambient space Rn. In particular, if
v ∈ Rn is tangent to Σ at p, then the derivative of P in the direction v is
given by
DP(p)v = (dT(p)v)f(p) + uξ(T(p), p)v. (5.11)
The next proposition relates the spectrum of DP(p) to the Floquet multi￾pliers of the first variational equation
W˙ = Df(u(t, p))W.
Proposition 5.41. If Γ is a periodic orbit and p ∈ Γ, then the union of
the set of eigenvalues of the derivative of a Poincar´e map at p ∈ Γ and the
singleton set {1} is the same as the set of characteristic multipliers of the
first variational equation along Γ. In particular, zero is not an eigenvalue.
Proof. Recall that t → uξ(t, ξ) is the principal fundamental matrix solu￾tion at t = 0 of the first variational equation and, since
d
dtf(u(t, ξ)) = Df(u(t, ξ))ut(t, ξ) = Df(u(t, ξ))f(u(t, ξ)),
the vector function t → f(u(t, ξ)) is the solution of the variational equation
with the initial condition W(0) = f(ξ). In particular,
uξ(T(p), p)f(p) = f(u(T(p), p)) = f(p),
and therefore f(p) is an eigenvector of the linear transformation uξ(T(p), p)
with eigenvalue the number one.
Since Σ is transverse to f(p), there is a basis of Rn of the form
f(p), s1,...,sn−1
with si tangent to Σ at p for each i = 1,...,n−1. It follows that the matrix
uξ(T(p), p) has block form, relative to this basis, given by
1 a
0 b
5.4 Stability of Periodic Orbits 233
where a is 1 × (n − 1) and b is (n − 1) × (n − 1). Moreover, each v ∈ Rn
that is tangent to Σ at p has block form (the transpose of) (0, vΣ). As a
result, we have the equality
uξ(T(p), p)v =
1 a
0 b
  0
vΣ

.
The range of DP(p) is tangent to Σ at p. Thus, using equation (5.11)
and the block form of uξ(T(p), p), it follows that
DP(p)v =
dT(p)v + avΣ
bvΣ

=
 0
bvΣ

.
In other words, the derivative of the Poincar´e map may be identified with
b and the differential of the return-time map with −a. In particular, the
eigenvalues of the derivative of the Poincar´e map coincide with the eigen￾values of b. ✷
Exercise 5.42. Prove that the characteristic multipliers of the first variational
equation along a periodic orbit do not depend on the choice of p ∈ Γ.
Most of the rest of this section is devoted to a proof of the following
fundamental theorem.
Theorem 5.43. Suppose that Γ is a periodic orbit for the autonomous dif￾ferential equation u˙ = f(u) and P is a corresponding Poincar´e map defined
on a Poincar´e section Σ such that p ∈ Γ ∩ Σ. If the eigenvalues of the
derivative DP(p) are inside the unit circle in the complex plane, then Γ is
asymptotically stable.
There are several possible proofs of this theorem. The approach used
here is adapted from [140].
To give a complete proof of Theorem 5.43, we will require several prelim￾inary results. Our first objective is to show that the point p is an asymptot￾ically stable fixed point of the dynamical system defined by the Poincar´e
map on Σ.
Let us begin with a useful simple replacement of the Jordan normal form
theorem that is adequate for our purposes here (see [146]).
Proposition 5.44. An n × n (possibly complex) matrix A is similar to an
upper triangular matrix whose diagonal elements are the eigenvalues of A.
Proof. Let v be a nonzero eigenvector of A corresponding to the eigenvalue
λ. The vector v can be completed to a basis of Cn that defines a matrix
Q partitioned by the corresponding column vectors Q := [v, y1,...,yn−1].
Moreover, Q is invertible and
[Q−1v,Q−1y1,...,Q−1yn−1]=[e1,..., en],234 5. Floquet Theory
where e1,..., en denote the usual basis elements.
Note that
Q−1AQ = Q−1[λv, Ay1, . . . , Ayn−1]
= [λe1, Q−1Ay1,...,Q−1Ayn−1].
In other words, the matrix Q−1AQ is given in block form by
Q−1AQ =
λ ∗
0 A˜

where A˜ is an (n−1)×(n−1) matrix. In particular, this proves the theorem
for all 2 × 2 matrices.
By induction, there is an (n − 1) × (n − 1) matrix R˜ such that R˜−1A˜R˜
is upper triangular. The matrix (QR)−1AQR where
R =
1 0
0 R˜

is an upper triangular matrix with the eigenvalues of A as its diagonal
elements, as required. ✷
Let ρ(A) denote the spectral radius of A, that is, the maximum modulus
of the eigenvalues of A.
Proposition 5.45. Suppose that A is an n×n matrix. If  > 0, then there
is a norm on Cn such that A
 < ρ(A) + . If A is a real matrix, then the
restriction of the “-norm” to Rn is a norm on Rn with the same property.
Proof. The following proof is adapted from [146]. By Proposition 5.44,
there is a matrix Q such that
QAQ−1 = D + N
where D is diagonal with the eigenvalues of A as its diagonal elements, and
N is upper triangular with each of its diagonal elements equal to zero.
Let μ > 0, and define a new diagonal matrix S with diagonal elements
1, μ−1, μ−2,...,μ1−n.
A computation shows that
S(D + N)S−1 = D + SNS−1.
Also, it is easy to show—by writing out the formulas for the components—
that every element of the matrix SNS−1 is O(μ).
Define a norm on Cn, by the formula
|v|μ := |SQv| = SQv, SQv5.4 Stability of Periodic Orbits 235
where the angle brackets on the right-hand side denote the usual Euclidean
inner product on Cn. It is easy to verify that this procedure indeed defines
a norm on Cn that depends on the parameter μ.
Post-multiplication by SQ on both sides of the equation
SQAQ−1S−1 = D + SNS−1
yields the formula
SQA = (D + SNS−1)SQ.
Using this last identity we have that
|Av|
2
μ = |SQAv|
2 = |(D + SNS−1)SQv|
2.
Let us define w := SQv and then expand the last norm into inner products
to obtain
|Av|
2
μ = Dw, Dw + SNS−1w, Dw
+Dw, SNS−1w + SNS−1w, SNS−1w.
A direct estimate of the first inner product together with an application
of the Schwarz inequality to each of the other inner products yields the
following estimate:
|Av|
2
μ ≤ (ρ2(A) + O(μ))|w|
2.
Moreover, we have that |v|μ = |w|. In particular, if |v|μ = 1 then |w| = 1,
and it follows that
A2
μ ≤ ρ2(A) + O(μ).
Thus, if μ > 0 is sufficiently small, then Aμ < ρ(A) + , as required. ✷
Corollary 5.46. If all the eigenvalues of the n×n matrix A are inside the
unit circle in the complex plane, then there is an “adapted norm” and a
number λ, with 0 <λ< 1, such that |Av|a ≤ λ|v|a for all vectors v, real
or complex. In particular, A is a contraction with respect to the adapted
norm. Moreover, for each norm on Rn or Cn, there is a positive number C
such that |Anv| ≤ Cλn|v| for all nonnegative integers n.
Proof. Under the hypothesis, we have ρ(A) < 1; thus, there is a number
λ such that ρ(A) <λ< 1. Using Proposition 5.45, there is an adapted
norm so that Aa < λ. This proves the first part of the corollary. To
prove the second part, recall that all norms on a finite-dimensional space
are equivalent. In particular, there are positive numbers C1 and C2 such
that
C1|v|≤|v|a ≤ C2|v|
for all vectors v. Thus, we have
C1|Anv|≤|Anv|a ≤ |A|
n
a |v|a ≤ C2λn|v|.236 5. Floquet Theory
After dividing both sides of the last inequality by C1 > 0, we obtain the
desired estimate. ✷
We are now ready to return to the dynamics of the Poincar´e map P
defined above. Recall that Γ is a periodic orbit for the differential equation
u˙ = f(u) and P : Σ → Σ is defined by P(σ) = u(T(σ), σ) where T is the
return-time function. Also, we have that p ∈ Γ ∩ Σ.
Lemma 5.47. Suppose that V ⊆ Rn is an open set with compact closure
V¯ such that Γ ⊂ V and V¯ is contained in the domain of the function f. If
t∗ ≥ 0, then there is an open set W ⊆ V that contains Γ and is such that,
for each point ξ ∈ W, the solution t → u(t, ξ) is defined and stays in V on
the interval 0 ≤ t ≤ t∗. Moreover, if ξ and ν are both in W and 0 ≤ t ≤ t∗,
then there is a number L > 0 such that
|u(t, ξ) − u(t, ν)| < |ξ − ν|eLt∗ .
Proof. Note that V¯ is a compact subset of the domain of the function
f. By Lemma 4.14, f is globally Lipschitz on V with a Lipschitz constant
L > 0. Also, there is a minimum positive distance m from the boundary of
V to Γ.
An easy application of Gronwall’s inequality can be used to show that if
ξ,ν ∈ V , then
|u(t, ξ) − u(t, ν)|≤|ξ − ν|eLt (5.12)
for all t such that both solutions are defined on the interval [0, t].
Define the set
Wq := {ξ ∈ Rn : |ξ − q|eLt∗ < m}
and note that Wq is open. If ξ ∈ Wq, then
|ξ − q| < me−Lt∗ < m.
Thus, it follows that Wq ⊆ V .
Using the extension theorem (Theorem 1.279), it follows that if ξ ∈ Wq,
then the interval of existence of the solution t → u(t, ξ) can be extended as
long as the orbit stays in the compact set V¯ . The point q is on the periodic
orbit Γ. Thus, the solution t → u(t, q) is defined for all t ≥ 0. Using the
definition of Wq and an application of the inequality (5.12) to the solutions
starting at ξ and q, it follows that the solution t → u(t, ξ) is defined and
stays in V on the interval 0 ≤ t ≤ t∗.
The union W := 
q∈Γ Wq is an open set in V containing Γ with the
property that all solutions starting in W remain in V at least on the time
interval 0 ≤ t ≤ t∗. ✷
Define the distance of a point q ∈ Rn to a set S ⊆ Rn by
dist(q, S) = inf x∈S |q − x|5.4 Stability of Periodic Orbits 237
where the norm on the right-hand side is the usual Euclidean norm. Simi￾larly, the (minimum) distance between two sets is defined as
dist(A, B) = inf{|a − b| : a ∈ A, b ∈ B}.
(Warning: dist is not a metric.)
Proposition 5.48. If σ ∈ Σ and if limn→∞ Pn(σ) = p, then
limt→∞ dist(u(t, σ), Γ) = 0.
Proof. Let  > 0 be given and let Σ0 be an open subset of Σ such that
p ∈ Σ0 and such that Σ¯ 0, the closure of Σ0 is a compact subset of Σ. The
return-time map T is continuous; hence, it is uniformly bounded on the set
Σ¯ 0, that is,
sup{T(η) : η ∈ Σ¯ 0} = T ∗ < ∞.
Let V be an open subset of Rn with compact closure V¯ such that Γ ⊂ V
and V¯ is contained in the domain of f. By Lemma 5.47, there is an open
set W ⊆ V such that Γ ⊂ W and such that, for each ξ ∈ W, the solution
starting at ξ remains in V on the interval 0 ≤ s ≤ T ∗.
Choose δ > 0 so small that the set
Σδ := {η ∈ Σ : |η − p| < δ}
is contained in W ∩ Σ0, and such that
|η − p|eLT ∗
< min{m, }
for all η ∈ Σδ. By Lemma 5.47, if η ∈ Σδ, then, for 0 ≤ s ≤ T ∗, we have
that
|u(s, η) − u(s, p)| < .
By the hypothesis, there is some integer N > 0 such that Pn(σ) ∈ Σδ
whenever n ≥ N.
Using the group property of the flow, let us note that
Pn(σ) = u(
n
−1
j=0
T(Pj (σ)), σ).
Moreover, if t ≥ N−1
j=0 T(Pj (σ)), then there is some integer n ≥ N and
some number s such that 0 ≤ s ≤ T ∗ and
t =
n
−1
j=0
T(Pj (σ)) + s.238 5. Floquet Theory
For this t, we have Pn(σ) ∈ Σδ and
dist(u(t, σ), Γ) = min q∈Γ |u(t, σ) − q|
≤ |u(t, σ) − u(s, p)|
= |u(s, u(
n
−1
j=0
T(Pj (σ)), σ)) − u(s, p)|
= |u(s, P n(σ)) − u(s, p)|.
It follows that dist(u(t, σ), Γ) <  whenever t ≥ N−1
j=0 T(Pj (σ)). In other
words,
limt→∞ dist(u(t, σ), Γ) = 0,
as required. ✷
We are now ready for the proof of Theorem 5.43.
Proof. Suppose that V is a neighborhood of Γ. We must prove that there is
a neighborhood U of Γ such that U ⊆ V with the additional property that
every solution of ˙u = f(u) that starts in U stays in V and is asymptotic
to Γ.
We may as well assume that V has compact closure V¯ and V¯ is contained
in the domain of f. Then, by Lemma 5.47, there is an open set W that
contains Γ and is contained in the closure of V with the additional property
that every solution starting in W exists and stays in V on the time interval
0 ≤ t ≤ 2τ where τ := T(p) is the period of Γ.
Also, let us assume without loss of generality that our Poincar´e section
Σ is a subset of a hyperplane Σ and that the coordinates on Σ are chosen
so that p lies at the origin. By our hypothesis, the linear transformation
DP(0) : Σ → Σ has its spectrum inside the unit circle in the complex
plane. Thus, by Corollary 5.46, there is an adapted norm on Σ and a
number λ with 0 <λ< 1 such that DP(0) < λ.
Using the continuity of the map σ → DP(σ), the return-time map, and
the adapted norm, there is an open ball Σ0 ⊆ Σ centered at the origin
such that Σ0 ⊂ W, the return-time map T restricted to Σ0 is bounded by
2τ , and DP(σ) < λ whenever σ ∈ Σ0. Moreover, using the mean value
theorem, it follows that
|P(σ)| = |P(σ) − P(0)| < λ|σ|,
whenever σ ∈ Σ0. In particular, if σ ∈ Σ0, then P(σ) ∈ Σ0.
Let us show that all solutions starting in Σ0 are defined for all positive
time. To see this, consider σ ∈ Σ0 and note that, by our construction, the
solution t → u(t, σ) is defined for 0 ≤ t ≤ T(σ) because T(σ) < 2τ . We
also have that u(T(σ), σ) = P(σ) ∈ Σ0. Thus, the solution t → u(t, σ)5.4 Stability of Periodic Orbits 239
can be extended beyond the time T(σ) by applying the same reasoning
to the solution t → u(t,P(σ)) = u(t + u(T σ), σ)). This procedure can be
extended indefinitely, and thus the solution t → u(t, σ) can be extended
for all positive time.
Define U := {u(t, σ) : σ ∈ Σ0 and t > 0}. Clearly, Γ ⊂ U and also every
solution that starts in U stays in U for all t ≥ 0. We will show that U is
open. To prove this fact, let ξ := u(t, σ) ∈ U with σ ∈ Σ0. If we consider the
restriction of the flow given by u : (0,∞) × Σ0 → U, then, using the same
idea as in the proof of the rectification lemma (Lemma 1.116), it is easy to
see that the derivative Du(t, σ) is invertible. Thus, by the inverse function
theorem (Theorem 1.117), there is an open set in U at ξ diffeomorphic to
a product neighborhood of (t, σ) in (0,∞) × Σ0. Thus, U is open.
To show that U ⊆ V , let ξ := u(t, σ) ∈ U with σ ∈ Σ0. There is some
integer n ≥ 0 and some number s such that
t =
n
−1
j=0
T(Pj (σ)) + s
where 0 ≤ s<T(Pn(σ)) < 2τ . In particular, we have that ξ = u(s,Pn(σ)).
But since 0 ≤ s < 2τ and Pn(σ) ∈ W it follows that ξ ∈ V .
Finally, for this same ξ ∈ U, we have as an immediate consequence of
Proposition 5.48 that limt→∞ dist(u(t,Pn(ξ)), Γ) = 0. Moreover, for each
t ≥ 0, we also have that
dist(u(t, ξ), Γ) = dist(u(t, u(s,Pn(ξ))), Γ) = dist(u(s + t,Pn(ξ)), Γ).
It follows that limt→∞ dist(u(t, ξ), Γ) = 0, as required. ✷
A useful application of our results can be made for a periodic orbit Γ
of a differential equation defined on the plane. In fact, there are exactly
two characteristic multipliers of the first variational equation along Γ. Since
one of these characteristic multipliers must be the number one, the product
of the characteristic multipliers is the eigenvalue of the derivative of every
Poincar´e map defined on a section transverse to Γ. Because the determinant
of a matrix is the product of its eigenvalues, an application of Liouville’s
formula proves the following proposition.
Proposition 5.49. If Γ is a periodic orbit of period ν of the autonomous
differential equation u˙ = f(u) on the plane, and if P is a Poincar´e map
defined at p ∈ Γ, then, using the notation of this section, the eigenvalue λΓ
of the derivative of P at p is given by
λΓ = det uξ(T(p), p) = e
 ν
0 div f(u(t,p)) dt.
In particular, if λΓ < 1 then Γ is asymptotically stable, whereas if λΓ > 1
then Γ is unstable.240 5. Floquet Theory
The flow near an attracting limit cycle is very well understood. A next
proposition states that the orbits of points in the basin of attraction of the
limit cycle are “asymptotically periodic.”
Proposition 5.50. Suppose that Γ is an asymptotically stable periodic orbit
with period T. There is a neighborhood V of Γ such that if ξ ∈ V , then
limt→∞ |u(t + T,ξ) − u(t, ξ)| = 0 where | | is an arbitrary norm on Rn. (In
this case, the point ξ is said to have asymptotic period T.)
Proof. By Lemma 5.47, there is an open set W such that Γ ⊂ W and the
function ξ → u(T,ξ) is defined for each ξ ∈ W. Using the continuity of this
function, there is a number δ > 0 such that δ < /2 and
|u(T,ξ) − u(T,η)| < 
2
whenever ξ,η ∈ W and |ξ − η| < δ.
By the hypothesis, there is a number T ∗ so large that dist(u(t, ξ), Γ) < δ
whenever t ≥ T ∗. In particular, for each t ≥ T ∗, there is some q ∈ Γ such
that |u(t, ξ) − q| < δ. Using this fact and the group property of the flow,
we have that
|u(t + T,ξ) − u(t, ξ)|≤|u(T,u(t, ξ)) − u(T,q)| + |q − u(t, ξ)|
≤ 
2 + δ<
whenever t ≥ T ∗. Thus, limt→∞ |u(t + T,ξ) − u(t, ξ)| = 0, as required. ✷
A periodic orbit can be asymptotically stable without being hyperbolic.
In fact, it is easy to construct a limit cycle in the plane that is asymp￾totically stable whose Floquet multiplier is the number one. By the last
proposition, points in the basin of attraction of such an attracting limit
cycle have asymptotic periods equal to the period of the limit cycle. But,
if the periodic orbit is hyperbolic, then a stronger result is true: Not only
does each point in the basin of attraction have an asymptotic period, each
such point has an asymptotic phase. This is the content of the next result.
Theorem 5.51. If Γ is an attracting hyperbolic periodic orbit, then there
is a neighborhood V of Γ such that for each ξ ∈ V there is some q ∈ Γ such
that limt→∞ |u(t, ξ)−u(t, q)| = 0. (In this case, ξ is said to have asymptotic
phase q.)
Proof. Let Σ be a Poincar´e section at p ∈ Γ with compact closure, return
map P, and return-time map T. Without loss of generality, we will assume
that for each σ ∈ Σ we have (1) limn→∞ Pn(σ) = p, (2) T(σ) < 2T(p), and
(3) DT(σ) < 2DT(p).
By the hyperbolicity hypothesis, the spectrum of DP(p) is inside the unit
circle; therefore, there are numbers C and λ such that C > 0, 0 <λ< 1
and
|p − Pn(σ)| < Cλnp − σ.
Let
K := 2CDT(p)
1 − λ sup
σ∈Σ¯
p − σ + 3T(p).5.4 Stability of Periodic Orbits 241
Using the implicit function theorem, it is easy to construct a neighborhood
V of Γ such that for each ξ ∈ V , there is a number tξ ≥ 0 with σξ :=
u(tξ, ξ) ∈ Σ. Moreover, using Lemma 5.47, we can choose V such that every
solution with initial point in V is defined at least on the time interval −K ≤
t ≤ K. Indeed, by the asymptotic stability of Γ, there is a neighborhood V
of Γ such that every solution starting in V is defined for all positive time.
If we redefine V to be the image of V under the flow for time K, then every
solution starting in V is defined at least on the time interval −K ≤ t ≤ K.
We will show that if σξ ∈ Σ, then there is a point qξ ∈ Γ such that
limt→∞ |u(t, σξ) − u(t, qξ)| = 0.
Using this fact, it follows that if r := u(−tξ, qξ), then
limt→∞ |u(t, ξ) − u(t, r)| = limt→∞ |u(t − tξ, u(tξ, ξ)) − u(t − tξ, qξ)|
= limt→∞ |u(t − tξ, σξ) − u(t − tξ, qξ)| = 0.
Thus, it suffices to prove the theorem for a point σ ∈ Σ.
Given σ ∈ Σ, define
sn := nT(p) −
n
−1
j=0
T(Pj (σ)).
Note that
(n + 1)T(p) − nT(p) = T(Pn(σ)) + sn+1 − sn,
and, as a result,
|sn+1 − sn| = |T(p) − T(Pn(σ))| ≤ 2DT(p)p − Pn(σ).
Hence,
|sn+1 − sn| < 2DT(p)Cp − σλn
whenever n ≥ 0.
Because sn = s1 + n−1
j=1 (sj+1 − sj ) and
n
−1
j=1
|sj+1 − sj | < 2CDT(p)p − σ
n
−1
j=1
λj < 2CDT(p)
p − σ
1 − λ ,
the series ∞
j=1(sj+1 − sj ) is absolutely convergent—its absolute partial
sums form an increasing sequence that is bounded above. Thus, in fact,
there is a number s such that limn→∞ sn = s. Also, the sequence {sn}∞
n=1
is uniformly bounded; that is,
|sn|≤|s1| + 2CDT(p)
p − σ
1 − λ ≤ K.242 5. Floquet Theory
Hence, the absolute value of its limit |s| is bounded by the same quantity.
Let  > 0 be given. By the compactness of its domain, the function
u : [−K, K] × Σ¯ → Rn
is uniformly continuous. In particular, there is a number δ > 0 such that if
(t1, σ1) and (t2, σ2) are both in the domain and if |t1 − t2| + |σ1 − σ2| < δ,
then
|u(t1, σ1) − u(t2, σ2)| < .
In view of the equality,
u(nT(p), σ) = u(sn,Pn(σ)),
which follows from the definition of sn, we have
|u(nT(p), σ) − u(s, p)| = |u(sn,Pn(σ)) − u(s, p)|.
Since for sufficiently large n,
|sn − s| + |Pn(σ) − p| < ,
it follows that
limn→∞ |u(nT(p), σ) − u(s, p)| = 0.
Also, for each t ≥ 0, there is an integer n ≥ 0 and a number s(t) such that
0 ≤ s(t) < T(p) and t = nT(p)+s(t). Using this fact, we have the equation
|u(t, σ) − u(t, u(s, p))| = |u(s(t), u(nT(p), σ)) − u(s(t), u(nT(p), u(s, p))|.
Also, because q := u(s, p) ∈ Γ and Lemma 5.47, there is a constant L > 0
such that
|u(t, σ) − u(t, q)| = |u(s(t), u(nT(p), σ)) − u(s(t), q))|
≤ |u(nT(p), σ) − q|eLT(p)
.
By passing to the limit as n → ∞, we obtain the desired result. ✷
Necessary and sufficient conditions for the existence of asymptotic phase
are known (see [55, 87]). An alternate proof of Theorem 5.51 is given in [55].
Exercise 5.52. Find a periodic solution of the system
x˙ = x − y − x(x2 + y2
),
y˙ = x + y − y(x2 + y2
),
z˙ = −z,
and determine its stability type. In particular, compute the Floquet multipliers
for the monodromy matrix associated with the periodic orbit [145, p. 120].5.4 Stability of Periodic Orbits 243
Exercise 5.53. (a) Find an example of a planar system with a limit cycle such
that some nearby solutions do not have an asymptotic phase. (b) Contrast and
compare the asymptotic phase concept for the following planar systems that are
defined in the punctured plane in polar coordinates:
1. ˙r = r(1 − r), ˙
θ = r,
2. ˙r = r(1 − r)
2, ˙
θ = r,
3. ˙r = r(1 − r)
n, ˙
θ = r.
Exercise 5.54. Suppose that v = 0 is an eigenvector for the monodromy oper￾ator with associated eigenvalue λΓ as in Proposition 5.49. If λΓ = 1, then v and
f(p) are independent vectors that form a basis for R2. The monodromy operator
expressed in this basis is diagonal. (a) Express the operators a and b defined
in the proof of Proposition 5.41 in this basis. (b) What can you say about the
derivative of the transit time map along a section that is tangent to v at p?
Exercise 5.55. This exercise is adapted from [263]. Suppose that f : R2 → R
is a smooth function and A := {(x, y) ∈ R2 : f(x, y)=0} is a regular level set of
f. (a) Prove that each bounded component of A is an attracting hyperbolic limit
cycle for the differential equation
x˙ = −fy − ffx, y˙ = fx − ffy.
(b) Prove that the bounded components of A are the only periodic orbits of the
system. (c) Draw and explain the phase portrait of the system for the case where
f(x, y) = ((x − )
2 + y2 − 1)(x2 + y2 − 9).
Exercise 5.56. Consider an attracting hyperbolic periodic orbit Γ for an
autonomous system ˙u = f(u) with flow ϕt, and for each point p ∈ Γ, let Γp
denote the set of all points in the phase space with asymptotic phase p. (a)
Construct Γp for each p on the limit cycle in the planar system
x˙ = −y + x(1 − x2 − y2
), y˙ = x + y(1 − x2 − y2
).
(b) Repeat the construction for the planar systems of Exercise 5.53. (c) Prove
that F := 
p∈Γ Γp is an invariant foliation of the phase space in a neighborhood
U of Γ. Let us take this to mean that every point in U is in one of the sets in the
union F and the following invariance property is satisfied: If ξ ∈ Γp and s ∈ R,
then ϕs(ξ) ∈ Γϕs(p). The second condition states that the flow moves fibers of
the foliation (Γp is the fiber over p) to fibers of the foliation. (d) Are the fibers
of the foliation smooth manifolds?6
Applications
Is the subject of ordinary differential equations important? The ultimate
answer to this question is certainly beyond the scope of this book. But two
main points of positive evidence are
• Ordinary differential equations arise naturally from the foundations
of physical science.
• Ordinary differential equations are useful tools for solving physical
problems.
More evidence is provided in the book [46]. You will have to decide if the
evidence is sufficient. Warning: By paying too much attention to philosoph￾ical issues concerning the value of mathematical subjects, you might stop
learning and producing mathematics. Paying no attention might lead to a
poor choice of which subjects are worthy of study.
6.1 Origins of ODE: Calculus of Variations
Consider a smooth function L : Rk × Rk × R → R, a pair of points p1, p2 ∈
Rk, two real numbers t1 < t2, and the set C := C(p1, p2, t1, t2) of all smooth
curves q : R → Rk such that q(t1) = p1 and q(t2) = p2. Using this data,
the action of L is the function Φ : C → R given by
Φ(q) =  t2
t1
L(q(t), q˙(t), t) dt. (6.1)
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
C. Chicone, Ordinary Differential Equations with Applications, Texts in
Applied Mathematics 34, https://doi.org/10.1007/978-3-031-51652-8 6
245246 6. Applications
The Euler-Lagrange equation, an ordinary differential equation associ￾ated with the function L—called the Lagrangian—arises from the following
problem: Find the extreme points of the function Φ. This variational prob￾lem is the basis for Lagrangian mechanics.
Recall from calculus that an extreme point of a smooth function is a
point at which its derivative vanishes. To use this definition directly for the
function Φ, we would have to show that C has the structure of an infinite￾dimensional manifold and Φ is differentiable. This can be done. Instead, we
will bypass these requirements by redefining the notion of extreme point.
In effect, the idea is to apply the concept of directional derivative for a
scalar function on a space of curves. An extreme point is then defined to
be a point where the directional derivative vanishes in all directions.
Recall our geometric interpretation of the derivative of a smooth function
on a manifold: For a tangent vector at a point in the domain of the function,
choose a curve whose tangent at time t = 0 is the given vector, move the
curve to the range of the function by composing it with the function, and
then differentiate the resulting curve at t = 0 to produce a tangent vector
on the range. This tangent vector is the image of the original vector under
the derivative of the function. In the context of the function Φ on the space
of curves C, let us consider a curve γ : R → C. Note that for s ∈ R, the
point γ(s) ∈ C is itself a curve γ(s) : R → Rk. So, in particular, if t ∈ R,
then γ(s)(t) ∈ Rk. Rather than use the cumbersome notation γ(s)(t), it is
customary to interpret our curve of curves as a “variation of curves” in C,
that is, as a smooth function Q : R × R → Rk with “end conditions”
Q(s, t1) ≡ p1, Q(s, t2) ≡ p2.
In this interpretation, γ(s)(t) = Q(s, t).
Fix a point q ∈ C and suppose that γ(0) = q, or equivalently that
Q(0, t) = q(t). Then, as s varies we obtain a family of curves called a
variation of the curve q. The tangent vector to γ at q is, by definition, the
curve V : R → Rk × Rk given by t → (q(t), v(t)) where
v(t) := ∂
∂sQ(s, t)



s=0
.
Its end conditions are
v(t1) = ∂
∂sQ(s, t1)



s=0
, v(t2) = ∂
∂sQ(s, t2)



s=0
. (6.2)
Of course, v is usually not in C because it does not satisfy the required end
conditions
v(t1)=0, v(t2)=0.
The vector V may be considered to be an element in the “tangent space of
C at q” if v does have zero end conditions.6.1 Origins of ODE: Calculus of Variations 247
What is the directional derivative DΦ(q)V of Φ at q in the direction V ?
Following the prescription given above, we have the definition
DΦ(q)V := ∂
∂sΦ(Q(s, ·))



s=0
=
 t2
t1
∂
∂sL(Q(s, t),
∂Q
∂t (s, t), t)



s=0
dt
=
 t2
t1
∂L
∂q
∂Q
∂s +
∂L
∂q˙
∂2Q
∂s∂t



s=0
dt. (6.3)
After evaluation at s = 0 and an integration by parts,
DΦ(q)V =
 t2
t1
∂L
∂q˙
(q(t), q˙(t), t)
∂Q
∂s (0, t) dt
+
 t2
t1
∂L
∂q (q(t), q˙(t), t) − d
dt
∂L
∂q˙
(q(t), q˙(t), t)
∂Q
∂s (0, t) dt
(6.4)
In case v vanishes at the end points,
DΦ(q)V =
 t2
t1
∂L
∂q (q(t), q˙(t), t) − d
dt
∂L
∂q˙
(q(t), q˙(t), t)
v(t) dt. (6.5)
The curve q is called an extremal if DΦ(q)V = 0 for all tangent vectors
V . For each tangent vector there is a corresponding variation Q(s, t) :=
q(t) + sv(t) so that ∂Q/∂s = v. Thus, the curve q is an extremal if the
integral in equation (6.5) vanishes for all smooth functions v that vanish
at the end points t1 and t2.
Proposition 6.1. The curve q is an extremal if and only if it is a solution
of the Euler-Lagrange equation
d
dt
∂L
∂q˙

− ∂L
∂q = 0. (6.6)
Proof. If the curve q is a solution of the Euler-Lagrange equation, then
(by formula (6.5)) DΦ(q) = 0. Conversely, if DΦ(q) = 0, we will show that
q is a solution of the Euler-Lagrange equation. If not, then there is some
time τ in the open interval (t1, t2) such that the quantity
∂L
∂q (q(t), q˙(t), t) − d
dt
∂L
∂q˙
(q(t), q˙(t), t)

appearing in the formula (6.5) does not vanish when evaluated at τ . In
this case, this quantity has a constant sign on a closed interval containing
the point τ . Moreover, there is a smooth nonnegative function v that van￾ishes outside this closed interval and is such that v(τ ) = 1 (see Ex. 6.23).
Hence, DΦ(q)V = 0 with respect to the variation Q(s, t) := q(t) + sv(t), in
contradiction. ✷248 6. Applications
Identification of extremals via the Euler-Lagrange usually suffices in
Lagrangian mechanics. In other applications, determining maxima or min￾ima is desired. Theory for determining maxima and minima of functionals
follows the conceptual ideas familiar from finite-dimensional theory, but it
is complicated by technical problems encountered when working in infinite￾dimensional function spaces. The general theory is explained in books on
the calculus of variations (see, for example, [96]). Of course, the Euler￾Lagrange equation plays a fundamental role called the first-order necessary
condition. A maximum or minimum value of Φ must be a solution of the cor￾responding Euler-Lagrange equation (see Ex. (6.4)). As might be expected,
there are also second-order conditions akin to the second derivative test.
One of these is the Legendre necessary condition described next.
Consider again the variation Q given by
Q(s, t) = q(t) + sv(t),
where v(0) = 0 and v(T ) = 0. Using Taylor’s theorem applied for each fixed
t at s = 0 and an integration by parts in the first-order term as before,
Φ(q + sv)) =  T
0
L(q, q, t ˙ ) dt + s
 T
0
(Lq(q, q, t ˙ ) − ∂
∂tLq˙(q, q, t ˙ ))v dt
+
1
2
s2
 T
0
Lqq(q, q, t ˙ )(v, v)+2Lqq˙(q, q, t ˙ )(v, v˙)
+ Lq˙q˙(q, q, t ˙ )( ˙v, v˙) dt + O(s3). (6.7)
Assume q is an extremal so that the first-order term in the Taylor expan￾sion vanishes. If the function q is a minimum of Φ, then the second deriva￾tive must not be negative definite. Likewise, if the extreme point is a max￾imum, then the second derivative term is not positive definite. The second
partial derivative terms are symmetric, bilinear forms. Integration by parts
applied to integration of t → Lqq˙(q, q, t ˙ )(v, v˙), results in a new form for the
second-order term in the Taylor expansion:
 T
0
(Lqq(q, q, t ˙ ) − Lqqt˙ (q, q, t ˙ ))(v, v) + Lq˙q˙(q, q, t ˙ )( ˙v, v˙) dt.
Theorem 6.2 (Legendre Necessary Condition). If q minimizes the
functional Φ, then (in the notation of the expansion (6.7)) the bilinear
form Lq˙q˙(q(t), q˙(t), t) is positive semi-definite for 0 ≤ t ≤ T ; that is, for
every vector v in Rn and each t we have that Lq˙q˙(q(t), q˙(t), t)(v, v) ≥ 0 .
Proof. See Ex. (6.5). ✷
In mechanics, the Lagrangian L is taken to be the difference between
the kinetic energy and the potential energy of a particle. As before, the
corresponding Φ is called the action of the Lagrangian, and a curve q :6.1 Origins of ODE: Calculus of Variations 249
R → Rk is called a motion. Hamilton’s principle states: Every motion of a
physical particle is an extremal of its action. Of course, the motions of a
particle as predicted by Newton’s second law are the same as the motions
predicted by Hamilton’s principle (see Ex. 6.3).
Exercise 6.3. Prove: The motions of a particle determined by Newton’s second
law are the same as the motions determined by Hamilton’s principle. In this
context, Newton’s law states that the time rate of change of the momentum
(mass×velocity) is equal to the negative gradient of the potential energy.
Exercise 6.4. Prove: A maximum or minimum value of a functional Φ must be
a solution of the corresponding Euler-Lagrange equation. Hint: Recall the proof
that the derivative of a differentiable function must vanish at a maximum or
minimum of the function.
Exercise 6.5. (a) Prove the Legendre necessary condition for the scalar case.
Hint: Construct a variation with small-amplitude and large first derivative.
(b) Generalize part (a) to the vector case.
Exercise 6.6. (a) Prove Beltrami’s theorem: If the Lagrangian does not depend
explicitly on t (that is, ∂L/∂t = 0), then the Euler-Lagrange equation implies that
L(q, q˙) − q∂L/∂ ˙ q˙ is constant. (b) The most important problem in the history of
the calculus of variations is the brachistochrone: Determine the curve in a vertical
plane connecting two points such that a particle starting at rest and confined to
the curve acted on only by the force of gravity traverses the curve in the shortest
time. By choosing coordinates with the origin at the first (leftmost and higher
point) the x axis horizontal to the right and the y axis pointing down, the curve
is given by y as a function of x and the mathematical problem is to minimize
 1 + y(x)2/
√y dx. Hint: Start from Beltrami’s equation. (c) Another famous
problem is the hanging chain: Imagine a uniform chain of some fixed length 
hanging between two posts (perhaps of unequal height) so that it does not touch
the ground. Determine the shape of the chain. Basic physics dictates that the
chain will assume the shape that minimizes its potential energy. At each point
along the chain the potential energy is (up to a constant) its height above the
ground, say y(s) where s denotes arc length. The total potential energy is the
integral of this function over the length of the chain. The mathematical problem
is to minimize  y
1 + y(x)2 dx with the constraint  1 + y(x)2 dx = . Hint:
Start from Beltrami’s equation and use a Lagrange multiplier to account for the
constraint. (d) What would happen in the hanging chain problem if the mass of
the chain varied along its length? Imagine a string of round pearls changing in
size with the largest in the middle of the string.
Exercise 6.7. (a) Prove the Lagrange multiplier theorem: Suppose f : Rn → R
and g : Rn → R are at least continuously differentiable. If f has a local minimum
at x = a on the (constraint) set {x ∈ Rn : g(x)=0} and ∇g(a) = 0, then ∇f(a)
is a real multiple of ∇g(a). This multiple is called the Lagrange multiplier. (b)
Use part (a) to minimize the distance to the origin of the end state after time
t = 1 for the solution of the initial value problem ˙x = Ax + b as x(0) ranges over250 6. Applications
the unit sphere. For definiteness, suppose A and b are
A =
⎛
⎝
11 1
02 1
0 0 −3
⎞
⎠ , b =
⎛
⎝
−1
2
3
⎞
⎠ .
(c) Is there a general result that would apply at least for a nontrivial subclass of
matrices and vectors? (d) The Lagrange multiplier rule for optimizing f subject
to the nonconstant constraint g(x) = 0 is often stated by saying “the constrained
optimization problem is transformed into the unconstrained optimization prob￾lem for L(x, λ) := f(x) + λg(x), where L is called the Lagrangian and λ the
Lagrange multiplier.” Show for the two-dimensional case (x ∈ R2) that a con￾strained optimum of f at x = a and λ = μ is never a nondegenerate minimum or
maximum of L. It is always a saddle point.
Exercise 6.8. Suppose q is a continuous positive (scalar) function on the unit
interval and λ is a real parameter. Consider the boundary value problem
−y¨ + qy = λy, y(0) = 0, y(1) = 0.
(a) Prove: If the boundary value problem has a nonzero solution, then λ > 0.
Hint: Multiply by y and integrate. (b) In view of part (a), there is a basic prob￾lem: Is there a choice of λ such that the boundary value problem has a nonzero
solution. If so, determine the smallest such λ. Note that Ly := −y¨+qy is a linear
operator. The parameter λ is called an eigenvalue of L if there is a nonzero y
(called the corresponding eigenvector) that solves the corresponding boundary
value problem. (c) Solve the basic problem for the special case q ≡ 1. (d) Show:
If the smallest eigenvalue exists, then it must be the minimum (over nonzero y)
of the functional
y →
 1
0 ( ˙y)
2 + qy2 dt
 1
0 y2 dt .
Hint: Determine the first-order necessary condition for a minimum. (c) Test the
result of part(b) for the special case q ≡ 1. Note: This exercise is a glimpse into
Sturm-Liouville theory. The functional in part (d) is called a Rayleigh quotient.
One beautiful feature of Lagrangian mechanics, which is evident from
the definition of extremals, is the following fact: Lagrangian mechanics is
coordinate free. In particular, the form of the Euler-Lagrange equation does
not depend on the choice of the coordinate system. Thus, to describe the
motion of a particle, we are free to choose the coordinates q := (q1, ··· , qk)
as we please and still use the same form of the Euler-Lagrange equation.
As an illustration, consider the prototypical example in mechanics: a free
particle. Let (x, y, z) denote the usual Cartesian coordinates in space and
t → q(t) := (x(t), y(t), z(t)) the position of the particle as time evolves.
The kinetic energy of a particle with mass m in Cartesian coordinates is m
2 ( ˙x2(t)+ ˙y2(t)+ ˙z2(t)). Thus, the action functional is given by
Φ(q) =  t2
t1
m
2 ( ˙x2(t)+ ˙y2(t)+ ˙z2(t)) dt,6.1 Origins of ODE: Calculus of Variations 251
the Euler-Lagrange equations are simply
mx¨ = 0, my¨ = 0, mz¨ = 0, (6.8)
and each motion is along a straight line, as expected.
As an example of the Euler-Lagrange equations in a non-Cartesian coor￾dinate system, let us consider the motion of a free particle in cylindrical
coordinates (r, θ, z). To determine the Lagrangian, note that the kinetic
energy depends on the Euclidean structure of space, that is, on the usual
inner product. A computation shows that the kinetic energy of the motion
t → (r(t), θ(t), z(t))expressedincylindricalcoordinatesis m
2 ( ˙r2(t)+r2 ˙
θ2(t)+
z˙
2(t)). For example, to compute the inner product of two tangent vectors
relative to a cylindrical coordinate chart, move them to the Cartesian coor￾dinate chart by the derivative of the cylindrical coordinate wrapping func￾tion (Section 1.7.6), and then compute the usual inner product of their
images. The Euler-Lagrange equations are
mr¨ − mr ˙
θ2 = 0, m
d
dt(r2 ˙
θ)=0, mz¨ = 0. (6.9)
Clearly, cylindrical coordinates are not the best choice to determine the
motion of a free particle. But it does indeed turn out that all solutions of
system (6.9) lie on straight lines (see Ex. 6.11).
We will discuss some of the properties enjoyed by the Euler-Lagrange
dynamical system. For simplicity we will consider only the case of autonomous
Lagrangians, L : Rk × Rk → R.
Define a new variable
p := ∂L
∂q˙
(q, q˙). (6.10)
We will assume the Lagrangian is regular ; that is, ˙q is defined implicitly as
a function α : Rk × Rk → Rk in equation (6.10) so that ˙q = α(q, p). In this
case, the Hamiltonian H : Rk × Rk → R is defined by
H(q, p) = pq˙ − L(q, q˙) = pα(q, p) − L(q, α(q, p)).
This function is often written in the simple form
H = ∂L
∂q˙
q˙ − L.
The transformation from the Lagrangian to the Hamiltonian is called the
Legendre transformation.
Proposition 6.9. If the Lagrangian is regular, then the Hamiltonian is a
first integral of the corresponding Lagrangian dynamical system. Moreover,
the Lagrangian equations of motion are equivalent to the Hamiltonian sys￾tem
q˙ = ∂H
∂p (q, p), p˙ = −∂H
∂q (q, p).252 6. Applications
Proof. Using the definition of p and α, and the Euler-Lagrange equation,
we have that
d
dtH(q, p)= ˙pα(q, p) + p
d
dtα(q, p) − ∂L
∂q (q, α(q, p))α(q, p)
− ∂L
∂q˙
(q, α(q, p)) d
dtα(q, p)
= ( d
dt
∂L
∂q˙
(q, α(q, p)) − ∂L
∂q (q, α(q, p)))α(q, p)
= 0;
that is, H is constant along solutions of the Euler-Lagrange equation. We
also have
∂H
∂p = α + p
∂α
∂p − ∂L
∂q˙
∂α
∂p
= ˙q
and
∂H
∂q = p
∂α
∂q − ∂L
∂q − ∂L
∂q˙
∂α
∂p
= − ∂L
∂q
= − d
dt
∂L
∂q˙
= − p,˙
as required. ✷
A first-order system equivalent to the Euler-Lagrange equation has dimen￾sion 2k. Here, k is called the number of degrees of freedom. The space Rk
with coordinate q is called the configuration space; its dimension is the
same as the number of degrees of freedom. The space Rk ×Rk with coordi￾nates (q, q˙) is called the state space; it corresponds to the tangent bundle of
the configuration space. The space Rk ×Rk with coordinates (q, p) is called
the phase space; it corresponds to the cotangent bundle of the configura￾tion space. A regular level set of the Hamiltonian is a (2k − 1)-dimensional
invariant manifold for the dynamical system. Hence, the existence of this
first integral allows us to reduce the dimension of the dynamical system.
In effect, we can consider the (2k − 1)-dimensional first-order system on
each regular level set of H. Since the intersection of two invariant sets is
an invariant set, the dimension of the effective first-order system can be
reduced further if there are additional first integrals.
There is an important connection between symmetries and first integrals
of the Lagrangian equations of motion. Indeed, the interplay between sym￾metry and conservation laws is bedrock methodology in modern physics,6.1 Origins of ODE: Calculus of Variations 253
including quantum field theory. The mathematical formulation of this cor￾respondence is called Noether’s theorem (Emmy Noether 1882–1935). Its
full generality is beyond the scope of this book. A special case, appropriate
for the ordinary differential equations of classical mechanics, is discussed.
To appreciate Noether’s fundamental insight, reconsider the notation
developed for the derivation of the Euler-Lagrange equations and in partic￾ular consider variations Q of an extremal q of the action Φ of an autonomous
Lagrangian L. A slightly more general notion of variation is required that
allows the boundary (previously the end points in time t1 and t2) to move.
For this, let q be an extremal defined as before on the interval [t1, t2], α
a continuously differentiable scalar functions of a scalar variable such that
α(0) = t1, and ω a continuously differentiable scalar function of two scalar
variables such that ω(0, t) = t for each t in the interval [tt, t2]. Define the
action along the variation to be
Φ(Q(s, t)) =  ω(s,t)
α(s)
L(Q(s, t),
∂Q
∂t (s, t)) dt. (6.11)
A variation of q is called a symmetry if it leaves the action infinitesimally
invariant; that is
d
dsΦ(Q(s, t))



s=0
= 0 (6.12)
for each t in the interval [t1, t2].
Theorem 6.10 (Noether’s Theorem). If Q is a symmetry of the
extremal q of the action of the Lagrangian L defined on the interval [t1, t2],
then the function
t → L(q(t), q˙(t))∂ω
∂s (0, t) + ∂L
∂q˙
(q(t), q˙(t))∂Q
∂s (0, t)
is constant for t1 ≤ t ≤ t2.
Proof. By the hypothesis and (as usual) an integration by parts,
d
dsΦ(Q(s, t))



s=0
=L(q(t), q˙(t))∂ω
∂s (0, t) − L(q(t1), q˙(t1))α
(0)
+
 t
t1
∂L
∂q (q(t), q˙(t)) − d
dt
∂L
∂q˙
(q(t), q˙(t))∂Q
∂s (0, t) dt
+
∂L
∂q˙
(q(t), q˙(t))∂Q
∂s (0, t) − ∂L
∂q˙
(q(t1), q˙(t1))∂Q
∂s (0, t1)
=0.
The integral term vanishes because q is an extremal and therefore a solu￾tion of the Euler-Lagrange equation. To complete the proof, simply rewrite
the remaining equality with all terms containing time dependence on the
opposite side of the equation from those with no time dependence. ✷254 6. Applications
To apply Theorem 6.10 requires identification of a symmetry. Classical
symmetries include time-translation invariance, space-translation invari￾ance, and rotational invariance. In turn, these imply conservation of energy,
linear momentum, and angular momentum.
Time-translation invariance corresponds to Q(s, t) = q(t − s), α(s) =
s + t1, and ω(s, t) = s + t. Note that the system evolution starts at a
new time s + t1 always with the same initial condition. Using Noether’s
theorem, if the mechanical system has this invariance, the corresponding
first integral is
(q, q˙) → L(q, q˙) − ∂L
∂q˙
(q, q˙) ˙q,
which is the negative of the Hamiltonian. For the classical Lagrangian given
by kinetic energy minus potential energy, the Hamiltonian is kinetic energy
plus potential energy, which is the definition of total energy.
Space-translation invariance, in the direction v, corresponds to Q(s, t) =
q(t) + sv, α(s) = t1 and ω(s, t) = t. The conserved quantity is
(q, q˙) →
∂L
∂q˙
(q, q˙)v.
The partial derivative factor, previously named p in the derivation of the
Hamiltonian from the Euler-Lagrange equations, is (in the classical case)
the derivative of the kinetic energy mk
i=1 mq˙
2
i /2, where m denotes the
constant mass of the particle. This partial derivative is represented by the
vector with components mq˙i. Taking, for example, the direction v to be
the ith basis vector, this conserved quantity is the linear momentum (mass
times velocity).
Rotational invariance may be expressed by the variation Q(s, t) = φs(q(t),
where φs is a linear rotation in Rk. Taking α(s) = t1 and ω(s, t) = t, If these
correspond to a symmetry of a classical Lagrangian system and the gener￾ator of the rotation is the vector field X (that is, d/ds(φs(q)) = X(φs(q))),
then
(q, q˙) →
∂L
∂q˙
(q, q˙)X(q).
is a conserved quantity. To see that this quantity is angular momentum
in general requires an examination of generators of rotations in Rk. For
simplicity, consider a potential U : R → R and the classical Lagrangian
L(q1, q2, q˙1, q˙2) = m
2 ( ˙q2
1 + ˙q2
2) − U(q2
1 + q2
2). (6.13)
It may be considered the Lagrangian of the motion of a particle of mass m
in a potential field where the potential is measured by the distance from
the origin. The corresponding action is invariant under the linear flow
φs(x, y) = 	cos s − sin s
sin s cos s

 	x
y


.6.1 Origins of ODE: Calculus of Variations 255
With Q(s, t) := φs(q(t)), α(s) = t1, and ω(s, t) = t, Noether’s theorem
implies that
I(q1, q2, q˙1, q˙2) := q1q˙2 − q2q˙1
(the angular momentum of the particle) is a conserved quantity.
Noether’s theorem is of great theoretical significance, but in practice,
there are usually direct proofs of conservation laws for specific systems
that do not require the full machinery of the theorem once a symmetry is
identified.
The Lagrangian system corresponding to the Lagrangian (6.13) provides
a typical example of the use of conserved quantities in the analysis of the
system. For simplicity, consider the case m = 1. Conservation of angular
momentum I is a consequence of the rotational symmetry, a feature that
suggests the introduction of polar coordinates. In fact, the Lagrangian in
polar coordinates is given by
L = 1
2
( ˙r2 + r2 ˙
θ2) − U(r2),
and the Euler-Lagrange equations take the form
r¨ = r ˙
θ2 − 2rU
(r2), d
dt(r2 ˙
θ)=0. (6.14)
By integration of the second equation, which is equivalent to the statement
˙
I = 0, there is a constant κ such that ˙
θ = κ/r2. Thus, the Euler-Lagrange
equations decouple, and the reduced system is
r¨ = κ2
r3 − 2rU
(r2). (6.15)
This differential equation is itself an Euler-Lagrange equation (see Ex. 6.13).
Time symmetry (which is always present in regular classical Lagrangian
systems) is equivalent to the existence of the Hamiltonian
H(r, θ, pr, pθ) = 1
2
(p2
r + p2
θ
r2 ) + U(r2).
The introduction of new variables pr and pθ is required here because, by
the definition of p,
p = (˙r, r2 ˙
θ).
Hamilton’s equations are given by
r˙ = pr,
˙
θ = pθ/r2,
p˙r = p2
θ
r3 − 2rU
(r2),
p˙θ = 0.256 6. Applications
Due to the existence of the second first integral, these equations decouple.
Note that the existence of this integral actually reduces the dimension of
the first-order system by two dimensions. The effective Hamiltonian system
r˙ = pr,
p˙r = κ2
r3 − 2rU
(r2) (6.16)
with Hamiltonian
H = 1
2
(p2
r +
κ2
r2 ) + U(r2)
is obtained by fixing an angular momentum κ := pθ. This one degree￾of-freedom system can be solved by quadrature. In particular, by fixing
a value of the reduced Hamiltonian corresponding to a regular level set,
we obtain a one-dimensional curve in the two-dimensional reduced space
that corresponds to an orbit of the original Lagrangian system. Here, the
existence of two independent integrals for our Lagrangian, which has two
degrees of freedom, ensures that we can reduce the problem of finding
solutions of the original system to a quadrature; that is, we can reduce the
problem to the integration
t − t0 =
 r(t)
r(t0)
(2h − κ2/r2 − 2U(r2))−1/2 dr,
where h is the fixed total energy, the value of the (reduced) Hamiltonian
along the orbit. Some features of the solutions of the reduced system are
discussed in the exercises.
For a system with d degrees of freedom, the problem of finding solutions
can be reduced to a quadrature if there are d independent integrals (see [13,
Ch. 10]).
Exercise 6.11. (a) Show that all solutions of system (6.9) lie on straight lines.
Compare the parametrization of the solutions of system (6.9) with the solutions of
the system (6.8). Hint: If necessary, read Section 6.2.2, especially, the discussion
on the integration of the equations of motion for a particle in a central force
field; that is, Kepler’s problem. (b) Find the Euler-Lagrange equations for a free
particle in spherical coordinates and show all solutions lie on straight lines.
Exercise 6.12. Prove the following statement: Suppose that φt is the flow of
the differential equation ˙q = f(q) for q ∈ Rk and L : Rk × Rk → R is an
(autonomous) Lagrangian given by (q, q˙) → L(q, q˙). If
L(φs(q), Dφs(q) ˙q) = L(q, q˙)
for all q ∈ Rk and all s in some open interval that contains the origin in R, then
I(q, q˙) := ∂L
∂q˙
(q, q˙)f(q)
is a first integral of the corresponding Euler-Lagrange equation.6.1 Origins of ODE: Calculus of Variations 257
Exercise 6.13. Find a Lagrangian with Euler-Lagrange equation (6.15).
Exercise 6.14. (a) Determine the ω-limit set of the point (x, y) = (4/3, 4/9)
under the flow of the system
x˙ = y, y˙ = x − x2
.
(b) Repeat part (a) for the point (x, y) = (4/3, 2/5).
Exercise 6.15. (a) Discuss the qualitative features of the extremals for the
Lagrangian (6.13) with U(ξ) := ξ by studying the Euler-Lagrange equations (6.14)
and the reduced system (6.16). In particular, show that the reduced system has
periodic solutions. (b) Find a sufficient condition for the Lagrangian system (6.13)
to have periodic solutions. (c) Show that these periodic solutions are restricted
to invariant two-dimensional tori in the phase space. While the introduction
of polar coordinates is viable, it is better to use symplectic polar coordinates:
x = √2r cos θ and x = √2r sin θ. To see one reason for this, repeat the exercise
using these coordinates in the configuration space and recall Exercise 1.53. The
underlying reason why symplectic polar coordinates are preferable is more subtle:
they preserve the natural volume element in the configuration space. More gener￾ally, they preserve the symplectic form (see [13, Ch. 8]). (d) Repeat the exercise
for the Kepler potential U(ξ) = −ξ−1/2. Hint: See Section 6.2.2.
Exercise 6.16. Why is the shortest distance between two points in Euclidean
space a straight line? (a) Consider two points in the plane that lie on the graph of
a function y = f(x) for x = a and x = b. The distance along the graph between
them is  b
a
1+(f(x))2 dx. Show, using the Euler-Lagrange equation, that if f
is a minimizer, then its graph is a straight line. (b) Does the extremum satisfy
the Legendre necessary condition? (c) The argument in part (a) does not prove
that there is a function f that minimizes the integral. To prove the existence
of a minimizer of a functional is usually a nontrivial problem in the calculus of
variations. To begin, the class of admissible functions f must be specified. Specify
a class of functions and prove that it contains the desired minimizer. Hint: Reduce
to a finite-dimensional problem.
Exercise 6.17. [Geodesics] (a) Show that images in R3 of extremals of the
Lagrangians L( ˙x, y,˙ z˙)= ˙x2(t)+ ˙y2(t)+ ˙z2(t) and √L are the same. Hint: For the
second Lagrangian, consider curves parametrized by arc length. (b) Generalize to
the case of Riemannian metrics, that is, compare the extremals of the functionals
Φ(q) =  t2
t1
(gkq˙
kq˙

)
1/2 dt
and
Ψ(q) =  t2
t1
gkq˙
kq˙
 dt
where (q, q˙)=(q1, q2,...,qn, q˙
1, q˙
2,..., q˙
n), the Riemannian metric g is given in
components, the matrix (gij ) is positive definite and symmetric (see Ex. 1.146),
and the Einstein summation convention is observed: sum on repeated indices
over {1, 2,...,n}. The images of the extremals of Φ are called geodesics of the
Riemannian metric g. Show that the images of the extremals of Φ and Ψ are the258 6. Applications
same. (c) Show that the Euler-Lagrange equations for Ψ are equivalent to the
geodesic equations
q¨
i + Γi
kq˙
kq˙
 = 0
where
Γi
k = 1
2
gij (
∂gj
∂qk + ∂gkj
∂q − ∂gk
∂qj )
are the Christoffel symbols and (gij ) is the inverse of the matrix (gij ). The
Euler-Lagrange equations for Φ are the same provided that the extremals are
parametrized by arc length.
Exercise 6.18. [Surfaces of Revolution] (a) Determine the geodesics on the
cylinder {(x, y, z) ∈ R3 : x2 + y2 = 1} with respect to the Riemannian metric
obtained by restricting the usual inner product on R3. Hint: Use Exercise 6.17.
To determine the metric, use the local coordinates (r, θ) → (cos θ, sin θ, r), where
q = (r, θ) and ˙q = (˙r, ˙
θ). Compute the tangent vector vr in the direction of
r at (r, θ) from the curve (r + t, θ) and compute the tangent vector vθ in the
direction θ from the curve (r, θ + t). The vector ˙q on the cylinder is given by
rv˙ r + ˙
θvθ, and the square of its length with respect to the usual metric on R3
is the desired Lagrangian; in fact, L(r, θ, r,˙ ˙
θ)= ˙r2 + ˙
θ2. The geodesics on the
cylinder are orbits of solutions of the corresponding Euler-Lagrange equations.
(b) Repeat part (a) for the unit sphere in R3 and for the hyperboloid of one
sheet {(x, y, z) ∈ R3 : z = x2 + y2}. (c) Generalize your results to determine
the geodesics on a surface of revolution. In this case, prove that every meridian
is a geodesic and find a necessary and sufficient condition for a latitude to be a
geodesic. Hint: Use the parametrization
(r, θ) → (ρ(r) cos θ, ρ(r) sin θ, r),
where ρ is the radial distance from the axis of revolution to the surface. (d) Prove
Clairaut’s relation: The radial distance from the axis of revolution to a point on a
geodesic multiplied by the sine of the angle between the tangent to the geodesic
at this point and the tangent vector at the same point in the direction of the
axis of revolution is constant along the geodesic. Hint: Compute the cosine of
π/2 minus the “Clairaut angle” using a dot product. Clairaut’s relation is a
geometric interpretation of the first integral obtained from the Euler-Lagrange
equation with respect to the angle of rotation around the axis of revolution. (e)
Give an example of a curve with the Clairaut relation that is not a geodesic.
Exercise 6.19. [Poincar´e plane] Consider the Riemannian metric (Poincar´e
metric) on the upper half-plane given by (dx2 + dy2)/y2 and the correspond￾ing Lagrangian
L(x, y, x,˙ y˙) = 1
2
x˙
2 + ˙y2
y2 .
Solutions of the Euler-Lagrange equations are geodesics by Exercise 6.17. (a)
Show that every geodesic lies on a vertical line or a circle with center on the
x-axis. Hint: The Lagrangian L is a first integral of the motion; that is, the
length of the velocity vector along an extremal for L is constant. (b) A more
sophisticated approach to this problem opens a rich mathematical theory. Show
that the Euler-Lagrange equations for L are invariant under all linear fractional6.1 Origins of ODE: Calculus of Variations 259
transformations of the upper half-plane given by
z → az + b
cz + d
where z is a complex variable and the coefficients a, b, c, d ∈ R are such that
ad − bc = 1 and recall that linear fractional transformations preserve the set
of all lines and circles. Hint: Write the Euler-Lagrange equations in complex
form using z = x + iy and show that the resulting equation is invariant under
the transformations. Or, write the metric in complex form and show that it is
invariant under the transformations.
Exercise 6.20. [Strain Energy] Strain is defined to be the relative change in
length in some direction. For f : Rn → Rn, relative change in length at ξ ∈ Rn is
given by (|f(ξ)−f(x)|−|x−ξ|)/|x−ξ|. For a unit vector v ∈ TξRn, the strain at
ξ in the direction v is |Df(ξ)v| − 1. This formula is obtained by replacing x with
ξ + tv and passing to the limit at t → 0. Consider the class C of all continuously
differentiable invertible functions f : [0, 1] → [α, β] with f(0) = α and f(1) = β
and define the total strain energy to be E(f) =  1
0 (|f
(s)| −1)2 ds. (a) Show that
g(s)=(β − α)s + α is an extremal and (b) g is the minimum of E over C. Hints:
For part (b), compute E(g) and show directly that E(f) ≥ E(g) for every f ∈ C.
Use the fundamental theorem of calculus and the Cauchy-Schwarz inequality.
Exercise 6.21. [Minimal Surface Area] The determination of surfaces of rev￾olution with minimal surface areas is a classic and mathematically rich prob￾lem in the calculus of variations (see [29]). (a) Consider all smooth functions
q : R → (0, ∞) whose graphs pass through the two point (0, a) and (, b) in the
plane, where a, b, and  are positive. Prove that if the surface area of the surface
of revolution obtained by revolving the graph of q : [a, b] → (0, ∞) about the hori￾zontal axis {(t, y) : y = 0} is an extremal, then q(t) = c cosh((t−d)/c) for suitable
constants c and d; that is, this two-parameter family of functions solves the Euler￾Lagrange equation for the functional whose Lagrangian is L(q, q˙)=2πq1+ ˙q2.
Hint: To solve the Euler-Lagrange equation, use the first integral given by Propo￾sition 6.9. (b) Show that the solution of the Euler-Lagrange equation with the
boundary conditions imposed can have no solutions, one solution, or two solu￾tions. Hint: To show that there can be two solutions, consider (for example) the
case where a = b = 2 and  = 1. In this case q is a solution provided that
2 = c cosh(1/(2c)), and this equation for c has two positive roots. Which of these
two solutions corresponds to the smallest surface area? Caution: If a surface with
minimal surface area exists among the surfaces of revolution generated by suffi￾ciently smooth curves, then the minimizing curve is an extremal. But, of course,
there may be no such minimizer. Sufficient conditions for minima are known
(see [29]). In the case a = b = 2 and  = 1, one of the two cantenaries is in fact
the minimizer. Nature also knows the minimum solution. To demonstrate this
fact, dip two hoops in a soap solution and pull them apart to form a surface of
revolution.
Exercise 6.22. A derivative moved across an integral sign in display (6.3).
Justify the result.
Exercise 6.23. [Bump functions] Prove that if B1 ⊂ B2 are open balls in
Rn, then there is a C∞ function that has value one on B1 and vanishes on the260 6. Applications
complement of B2. Hint: Show that the function f defined to be exp(−1/(1−|x|
2)
for |x| < 1 and zero for |x| ≥ 1 is C∞

; then, consider the function x → x
−∞ f(s) ds/  ∞
−∞ f(s) ds, etc.
Exercise 6.24. The calculus of variations is a far-reaching subject that goes
beyond the case of one independent variable as in the Lagrangians of classical
mechanics. Consider, for example, a domain Ω contained in Rn and functions
f : Ω → R that vanish on the boundary of Ω. For the usual Euclidean norm,
some α ≥ 2, and a real parameter λ, let L(f, ∇f) := |∇f|
2 + λ|f|
α. The action,
as should be expected, is
Φ(f) = 
Ω
L(f, ∇f) dV,
where dV is the usual volume element in Rn. Derive the Euler-Lagrange equation.
6.2 Origins of ODE: Classical Physics
What is classical physics? Look at Section 18–2 in Richard Feynman’s
lecture notes [101]; you might be in for a surprise. The fundamental laws
of all of classical physics can be reduced to a few formulas. For example, a
complete theory of electromagnetics is given by Maxwell’s laws
div E = ρ/0,
curl E = −∂B
∂t ,
div B = 0,
c2 curl B = j
0
+
∂E
∂t
and the conservation of charge
div(j) = −∂ρ
∂t .
Here E is the called electric field, B is the magnetic field, ρ is the charge
density, j is the current, 0 is a constant, and c is the speed of light. The
fundamental law of motion is Newton’s law
dp
dt = F
“the rate of change of the momentum is equal to the sum of the forces.”
The (relativistic) momentum of a particle is given by
p := m
1 − v2/c2 v6.2 Origins of ODE: Classical Physics 261
where, as is usual in the physics literature, v := |v| and the norm is the
Euclidean norm. For a classical particle (velocity much less than the speed
of light), the momentum is approximated by p = mv. There are two fun￾damental forces: The gravitational force
F = −GMm
r2 er
on a particle of mass m due to a second mass M where G is the univer￾sal gravitational constant and er is the unit vector at M pointing in the
direction of m; and the Lorentz force
F = q(E + v × B)
where q is the charge on a particle in an electromagnetic field. That’s it!
The laws of classical physics seem simple enough. Why then is physics,
not to mention engineering, so complicated? The answer is that in almost
all realistic applications there are lots of particles and the fundamental laws
act together on all of the particles.
Rather than trying to model the motions of all the particles that are
involved in an experiment or a physical phenomenon which we wish to
explain, it is often more fruitful to develop constitutive force laws that
are meant to approximate the true situation. The resulting equations of
motion contain new “forces” that are not fundamental laws of nature.
But, by using well conceived constitutive laws, the predictions we make
from the corresponding equations of motion will agree with experiments or
observations within some operating envelope. While the constitutive laws
themselves may lead to complicated differential equations, these equations
are supposed to be simpler and more useful than the equations of motion
that would result from the fundamental force laws. Also, it is important
to realize that in most circumstances no one knows how to write down the
equations of motion from the fundamental force laws.
Let us consider a familiar example: the spring equation
mx¨ = −ω0x
for the displacement x of a mass (attached to the end of a spring) from
its equilibrium position. This model uses Newton’s law of motion, together
with Hooke’s restoring force law. Newton’s law is fundamental (at least if we
ignore relativistic effects); Hooke’s law is not. It is meant to replace the law
of motion that would result from modeling the motions of all the particles
that make up the spring. When Hooke’s (linear) law is not sufficiently
accurate (for example, when we stretch a spring too far from its equilibrium
position), we may refine the model to obtain a nonlinear equation of motion
such as
mx¨ = −ω0x + αx3,262 6. Applications
which is already a complicated differential equation.
Frictional forces are always modeled by constitutive laws. What law of
nature are we using when we add a viscous damping force to a Hookian
spring to obtain the model
mx¨ = −αx˙ − ω0x?
The damping term is supposed to model a force due to friction. But what
is friction? There are only two fundamental forces in classical physics and
only four known forces in modern physics. Friction is not a nuclear force
and it is not due to gravitation. Thus, at a fundamental level it must be a
manifestation of electromagnetism. Is it possible to derive the linear form of
viscous damping from Maxwell’s laws? This discussion could become very
philosophical!
These models introduce constitutive laws—in our example, restoring
force and friction forces laws—that are not fundamental laws of nature.
In reality, the particles (atoms) that constitute the spring obey the electro￾magnetic force law and the law of universal gravitation. But, to account for
their motions using these fundamental force laws would result in a model
with an enormous number of coupled equations that would be very difficult
to analyze even if we knew how to write it down.
The most important point for us to appreciate is that Newton’s law of
motion—so basic for our understanding of the way the universe works—is
expressed as an ordinary differential equation. Newton’s law, the classi￾cal force laws, and constitutive laws are the origin of ordinary differential
equations. It should be clear what Newton meant when he said “Solving
differential equations is useful.”
In the following subsections some applications of the theory of differen￾tial equations to problems that arise in classical physics are presented. The
first section briefly describes the motion of a charged particle in a con￾stant electromagnetic field. The second section is an introduction to the
two-body problem, including Kepler motion,Delaunay elements, and per￾turbation forces. The analysis of two-body motion is used as a vehicle to
explore a realistic important physical problem where it is not at all obvious
how to obtain useful predictions from the complicated model system of dif￾ferential equations obtained from Newton’s law. Perturbations of two-body
motion are considered in the final sections: Satellite motion about an oblate
planet is used to illustrate the “method of averaging,” and the diamagnetic
Kepler problem—the motion of an electron of a hydrogen atom in a con￾stant magnetic field—is used to illustrate some important transformation
methods for the analysis of models of mechanical systems.6.2 Origins of ODE: Classical Physics 263
6.2.1 Motion of a Charged Particle
Let us consider a few simple exercises to “feel” the Lorentz force (for more
see [101] and [155]). The equation of motion for a charged particle is
dp
dt = q(E + v × B)
where p is the momentum vector, q is a measure of the charge, and v is the
velocity. We will consider the motion of a charged particle (classical and
relativistic) in a constant electromagnetic field; that is, the electric field E
and the magnetic field B are constant vector fields on R3.
In case E = 0, let us consider the relativistic motion for a charged par￾ticle. Because the momentum is a nonlinear function of the velocity, it is
useful to notice that the motion is “integrable.” In fact, the two functions
p → 	p, p
 and p → 	p, B
 are constant along orbits. Use this fact to
conclude that v → 	v, v
 is constant along orbits, and therefore the energy
E := mc2
1 − v2/c2
is constant along orbits. It follows that the equation of motion can be recast
in the form
E
c2 v˙ = qv × B,
and the solution can be found as in Exercise 2.54. The solution of this
differential equation is important. For example, the solution can be used to
place magnetic fields in an experiment so that charged particles are moved
to a detector (see [101]).
Another important problem is to determine the drift velocity of a charged
particle moving in a constant electromagnetic field (see Ex. 6.25).
Exercise 6.25. Consider a classical particle moving in space with a constant
magnetic field pointing along the z-axis and a constant electric field parallel to
the yz-plane. The equations of motion are
mx¨ = qB3y, m ˙ y¨ = q(E2 − B3x˙ ), mz¨ = qE3. (6.17)
(a) Solve the system (6.17). (b) Note that the first two components of the solution
are periodic in time. Their average over one period gives a constant vector field
called the drift velocity field. Find this vector field. (c) Describe the motion of
the charged particle in space.
Exercise 6.26. Use the theory of linear differential equations with constant
coefficients to determine the motion for a “spatial oscillator” (see [155]) in the
presence of a constant magnetic field. The equation of motion is
v˙ = −ω2
0r + q
mv × B264 6. Applications
where r = (x, y, z) is the position vector, the velocity is v = (˙x, y,˙ z˙), and B =
(0, 0, B3). (This model uses Hooke’s law). By rewriting the equations of motion
in components, note that this model is a linear system with constant coefficients.
(a) Find the general solution of the system. (b) Determine the frequency of the
motion in the plane perpendicular to the magnetic field and the frequency in the
direction of the magnetic field.
6.2.2 Motion of a Binary System
Let us consider two point masses, m1 and m2, moving in three-dimensional
Euclidean space with corresponding position vectors R1 and R2. Also, let
us define the relative position vector R := R2 − R1 and its length r := |R|.
According to Newton’s law (using of course the usual approximation for
small velocities) and the gravitational force law, we have the equations of
motion
m1R¨1 = G0m1m2
r3 R + F1, m2R¨2 = −G0m1m2
r3 R + F2
where F1 and F2 are additional forces acting on m1 and m2, respectively.
The relative motion of the masses is governed by the single vector equation
R¨ = −G0(m1 + m2)
r3 R +
1
m2
F2 − 1
m1
F1.
By rescaling distance and time such that R = αR¯ and t = βt
¯ with
G0(m1 +m2)β2 = α3, we can recast the equations of motion in the simpler
form
R¨ = − 1
r3 R + F. (6.18)
We will study this differential equation.
The analysis of two-body interaction plays a central role in the history
of science. This is reason enough to study the dynamics of the differential
equation (6.18) and the surrounding mathematical terrain. As you will see,
the intrinsic beauty, rich texture, and wide applicability of this subject
make it one of the most absorbing topics in all of mathematics.
The following glimpse into celestial mechanics is intended to introduce
an important application of ordinary differential equations, to see some of
the complexity of a real-world application and to introduce a special form
of the dynamical equations that will provide some motivation for the theory
of averaging presented in Chapter 10.
There are many different approaches to celestial mechanics. For a
mathematician, perhaps the most satisfactory foundation for mechanics
is provided by the theory of Hamiltonian systems. Although we will use
Hamilton’s equations in our analysis, the geometric context (symplectic
geometry) for a modern treatment of the transformation theory for Hamil￾tonian systems (see [1], [13], and [180]) is unfortunately beyond the scope6.2 Origins of ODE: Classical Physics 265
of this book. To bypass this theory, we will present an expanded expla￾nation of the direct change of coordinates to the Delaunay elements given
in [60] (see also [56] and [59]). In the transformation theory for Hamiltonian
systems it is proved that our transformations are special coordinate trans￾formations called canonical transformations. They have a special property:
Hamilton’s equations for the transformed Hamiltonian are exactly the dif￾ferential equations given by the push forward of the original Hamiltonian
vector field to the new coordinates. In other words, to perform a canonical
change of coordinates we need only transform the Hamiltonian, not the dif￾ferential equations; the transformed differential equations are obtained by
computing Hamilton’s equations from the transformed Hamiltonian. The
direct method is perhaps not as elegant as the canonical transformation
approach—we will simply push forward the Hamiltonian vector field in the
usual way—but the direct transformation method is effective and useful.
Indeed, we will construct special coordinates (action-angle coordinates) and
show that they transform the Kepler system to a very simple form. More￾over, the direct method applies even if a nonconservative force F acts on
the system; that is, even if the equations of motion are not Hamiltonian.
Let us begin by rewriting the second-order differential equation (6.18) as
the first-order system
R˙ = V, V˙ = − 1
r3 R + F (6.19)
defined on R3×R3. Also, let us use angle brackets to denote the usual inner
product on R3 so that if X ∈ R3, then |X|
2 = 	X, X
.
The most important feature of system (6.19) is the existence of conserved
quantities for the Kepler motion, that is, the motion with F = 0. In fact,
total energy and angular momentum are conserved. The total energy of the
Kepler motion E : R3 × R3 → R is given by
E(X, Y ) := 1
2
	Y,Y 
 − 1
	X, X
1/2 , (6.20)
the angular momentum by
A(X, Y ) := X × Y. (6.21)
Let us also define the total angular momentum
G(X, Y ) := |A(X, Y )|. (6.22)
Note that we are using the term “angular momentum” in a nonstandard
manner. In effect, we have defined the angular momentum to be the vec￾tor product of position and velocity; in physics, angular momentum is the
vector product of position and momentum.266 6. Applications
Periastron
z
Line of Nodes
Ascending Node
y
g
h
i
v en
R(t)
x
Figure 6.1: The osculating Kepler orbit in space.6.2 Origins of ODE: Classical Physics 267
If t → (R(t), V (t)) is a solution of system (6.19), then
E˙ = d
dtE(R(t), V (t)) = 	F, V 
, A˙ = R × F. (6.23)
Thus, if F = 0, then E and A are constant on the corresponding orbit.
In particular, in this case the projection of the Kepler orbit into physical
space, corresponding to the curve t → R(t), is contained in a fixed plane
passing through the origin with normal vector given by the constant value
of A along the orbit. In this case, the corresponding plane normal to A is
called the osculating plane. At each instant of time, this plane contains the
Kepler orbit that would result if the force F were not present thereafter.
Refer to Figure 6.1 for a depiction of the curve t → R(t) in space and the
angles associated with the Delaunay elements, new coordinates that will
be introduced as we proceed.
Let us define three functions er : R3 → R3, and eb, en : R3 × R3→R3 by
er(X) = 1
|X|
X,
en(X, Y ) = 1
G(X, Y )
X × Y,
eb(X, Y ) = en(X, Y ) × er(X).
If X, Y ∈ R3 are linearly independent, then the vectors er(X), eb(X, Y ),
and en(X, Y ) form an orthonormal frame in R3. Also, if these functions are
evaluated along the (perturbed) solution t → (R, R˙), then we have
er = 1
r
R, en = 1
GR × R, e ˙ b = 1
rG(R × R˙) × R = 1
rG(r2R˙ − 	R, R˙ 
R).
(6.24)
(Note that subscripts are used in this section to denote coordinate direc￾tions, not partial derivatives.)
If ex, ey, ez are the direction vectors for a fixed right-handed usual Carte￾sian coordinate system in R3, and if i denotes the inclination angle of the
osculating plane relative to the z-axis, then
cosi = 	en, ez
. (6.25)
Of course, the angle i can also be viewed as a function on R3 × R3 whose
value at each point on the orbit is the inclination of the osculating plane.
The idea is that we are defining new variables: They are all functions defined
on R3 × R3.
If the osculating plane is not coincident with the (x, y)-plane, then it
meets this plane in a line, called the line of nodes. Of course, the line of
nodes lies in the osculating plane and is orthogonal to the z-axis. Moreover,
it is generated by the vector
ean := 	eb, ez
er − 	er, ez
eb.268 6. Applications
The angle of the ascending node h between the x-axis and the line of nodes
is given by
cos h = 1
|ean|
	ean, ex
. (6.26)
If the osculating plane happens to coincide with the (x, y)-plane, then there
is no natural definition of h. On the other hand, the angle h(t) is continuous
on each trajectory. Also, at a point where i(t) = 0, the angle h is defined
whenever there is a continuous extension.
Let us compute the orthogonal transformation relative to the Euler
angles i and h. This is accomplished in two steps: rotation about the z￾axis through the angle h followed by rotation about the now rotated x-axis
through the angle i. The rotation matrix about the z-axis is
M(h) :=
⎛
⎝
cos h − sin h 0
sin h cos h 0
0 01
⎞
⎠ .
To rotate about the new x-axis (after rotation by M(h)), let us rotate back
to the original coordinates, rotate about the old x-axis through the angle
i, and then rotate forward. The rotation about the x-axis is given by
M(i) :=
⎛
⎝
10 0
0 cosi − sin i
0 sin i cosi
⎞
⎠ .
Thus, the required rotation is
M := (M(h)M(i)M−1(h))M(h) = M(h)M(i).
In components, if the new Cartesian coordinates are denoted x
, y
, z
, then
the transformation M is given by
x =x cos h − y sin h cosi + z sin h sin i,
y =x sin h + y cos h cosi − z cos h sin i,
z =y sin i + z cosi. (6.27)
Also, by the construction, the normal en to the osculating plane is in the
direction of the z
-axis.
If polar coordinates are introduced in the osculating plane
x = r cos θ, y = r sin θ,
then the position vector along our orbit in the osculating coordinates is
given by
R(t)=(r(t) cos θ(t), r(t) sin θ(t), 0).6.2 Origins of ODE: Classical Physics 269
For a Kepler orbit, the osculating plane is fixed. Also, using the orthog￾onal transformation M, the vectors R and R˙ are given in the original fixed
coordinates by
R = M
⎛
⎝
r cos θ
r sin θ
0
⎞
⎠ , R˙ = M
⎛
⎝
r˙ cos θ − r ˙
θ sin θ
r˙ sin θ + r ˙
θ cos θ
0
⎞
⎠ . (6.28)
If X, Y ∈ R3, then, because M is orthogonal, we have 	MX,MY 
 = 	X, Y 
.
As a consequence of this fact and definition (6.20), it follows that the total
energy along the orbit is
E(R, R˙) = 1
2
( ˙r2 + r2 ˙
θ2) − 1
r
.
For arbitrary vectors X, Y ∈ R3, the coordinate-free definition of their
vector product states that
X × Y := (|X||Y |sin ϑ)η (6.29)
where ϑ is the angle between X and Y and η is the unit vector orthogonal
to X and Y such that the ordered triple (X, Y, η) has positive (right hand)
orientation. Also, note that M is orientation preserving (det M > 0). Using
this fact and the definition of the vector product, it follows that MX ×
MY = M(X × Y ) and the angular momentum along the orbit is
G(R, R˙) = r2 ˙
θ. (6.30)
Thus, using equation (6.23), there is a constant (angular momentum) Pθ
such that
r2 ˙
θ = Pθ, E = 1
2

r˙
2 +
P2
θ
r2

− 1
r
, (6.31)
and, because E˙ = 0, we also have
r2 ˙
θ = Pθ, r˙

r¨ − P2
θ
r3 +
1
r2

= 0.
If ˙r ≡ 0, r(0) = 0, and Pθ = 0, then the Kepler orbit is a circle; in fact,
it is a solution of the system ˙r = 0, ˙
θ = Pθr(0)−2. If ˙r is not identically
zero, then the motion is determined by Newton’s equation
r¨ = −
 1
r2 − P2
θ
r3

.
The equivalent system in the phase plane,
r˙ = Pr, P˙
r = − 1
r2 +
P2
θ
r3 (6.32)270 6. Applications
Pr
Separatrix Bounded, periodic orbits
Unbounded orbits
r
Figure 6.2: Schematic phase portrait of system (6.32). There is a center sur￾rounded by a period annulus and “bounded” by an unbounded separatrix.
The periodic orbits correspond to elliptical Keplerian motions.
is Hamiltonian with energy
E = 1
2

P2
r +
P2
θ
r2

− 1
r
.
It has a rest point with coordinates (r, Pr)=(P2
θ , 0) and energy −1/(2P2
θ ).
This rest point is a center surrounded by an annulus of periodic orbits,
called a period annulus, that is “bounded” by an unbounded separatrix as
depicted schematically in Figure 6.2. The separatrix crosses the r-axis at
r = 1
2P2
θ and its energy is zero. Thus, if a Kepler orbit is bounded, it has
negative energy.
Exercise 6.27. Prove all of the statements made about the phase portrait of
system (6.32).
Exercise 6.28. The vector product is defined in display (6.29) in a coordinate￾free manner. Suppose instead that X = (x1, x2, x3) and Y = (y1, y2, y3) in Carte￾sian coordinates, and their cross product is defined to be
X × Y := det
⎛
⎝
e1 e2 e3
x1 x2 x3
y1 y2 y3
⎞
⎠ .
Discuss the relative utility of the coordinate-free versus coordinate-dependent
definitions. Determine the vector product for vectors expressed in cylindrical or
spherical coordinates. Think about the concept of coordinate-free definitions; it
is important.6.2 Origins of ODE: Classical Physics 271
The rest of the discussion is restricted to orbits with negative energy and
positive angular momentum Pθ > 0.
The full set of differential equations for the Kepler motion is the first￾order system
r˙ = Pr, ˙
θ = Pθ
r2 , P˙
r = 1
r2
P2
θ
r − 1

, P˙
θ = 0. (6.33)
Note that the angle θ increases along orbits because ˙
θ = Pθ/r2 > 0. Also,
the bounded Kepler orbits project to periodic motions in the r–Pr phase
plane. Thus, the bounded orbits can be described by two angles: the polar
angle relative to the r-axis in the r–Pr plane and the angular variable θ.
In other words, each bounded orbit lies on the (topological) cross product
of two circles; that is, on a two-dimensional torus. Because the phase space
for the Kepler motion is foliated by invariant two-dimensional tori, we
will eventually be able to define special coordinates, called action-angle
coordinates, that transform the Kepler system to a very simple form. In
fact, the special class of Hamiltonian systems (called integrable Hamiltonian
systems) that have a portion of their phase space foliated by invariant tori
can all be transformed to a simple form by the introduction of action-angle
coordinates. Roughly speaking, the action specifies a torus and the “angle”
specifies the frequency on this torus. This is exactly the underlying idea for
the construction of the action-angle coordinates for the Kepler system.
To integrate system (6.33), introduce a new variable ρ = 1/r so that
ρ˙ = −ρ2Pr, P˙
r = ρ2(ρP2
θ − 1),
and then use θ as a time-like variable to obtain the linear system
dρ
dθ = − 1
Pθ
Pr, dPr
dθ = 1
Pθ
(ρP2
θ − 1). (6.34)
Equivalently, we have the “harmonic oscillator model” for Kepler motion,
d2ρ
dθ2 + ρ = 1
P2
θ
. (6.35)
It has the general solution
ρ = 1
P2
θ
+ B cos(θ − g) (6.36)
where the numbers B and g are determined by the initial conditions.
The Kepler orbit is an ellipse. In fact, by a rearrangement of equa￾tion (6.36) we have
r = P2
θ
1 + P2
θ B cos(θ − g)
.272 6. Applications
If we introduce a new angle, the true anomaly v := θ − g, and the usual
quantities—the eccentricity e, and a, the semimajor axis of the ellipse—
then
r = a(1 − e2)
1 + e cos v
. (6.37)
Exercise 6.29. Use the conservation of energy for system (6.33) to show that
dr
dθ = r
Pθ
	
2Er2 + 2r − P2
θ

1/2
.
Solve this differential equation for E < 0 and show that the Kepler orbit is an
ellipse.
Because the energy is constant on the Kepler orbit, if we compute the
energy at v = 0 (corresponding to its perifocus, the point on the ellipse
closest to the focus), then we have the corresponding values
r = a(1 − e), r˙ = 0, E = − 1
2a
. (6.38)
Moreover, from the usual theory of conic sections, the semiminor axis is
b = a
√
1 − e2, and the area of the ellipse is
πab = πa2
1 − e2 = πPθa3/2. (6.39)
The area element in polar coordinates is 1
2 r2 dθ. Hence, if the period of the
Kepler orbit is T, then
 2π
0
r2(θ)
2
dθ =
 T
0
r2(θ(t))
2 ˙
θ(t) dt = πPθa3/2.
Because r2 ˙
θ = Pθ, the integral can be evaluated. The resulting equation is
Kepler’s third law
T2 = 4π2a3 (6.40)
where, again, T is the orbital period and a is the semimajor axis of the
corresponding elliptical orbit. For later reference, note that the frequency
of the oscillation is
ω := 1
a3/2 . (6.41)
Exercise 6.30. (a) Kepler’s third law is given by equation (6.40) for scaled
distance and time. Show that Kepler’s third law in “unscaled” variables (with
the same names as in (6.40)) is given by
T 2 = 4π2
G0(m1 + m2)
a3
.6.2 Origins of ODE: Classical Physics 273
(b) Show that the magnitude of the physical angular momentum in the unscaled
variables for a Kepler orbit is
m2r2 ˙
θ = m2
	α3
β2 a(1 − e
2
)

1/2 = m2
	
G0(m1 + m2)a(1 − e
2
)

1/2
where r and t are unscaled, and with α and β as defined on page 264.
6.2.3 Perturbed Kepler Motion and Delaunay Elements
In this section we will begin an analysis of the influence of a force F on
a Keplerian orbit by introducing new variables, called Delaunay elements,
such that system (6.18) when recast in the new coordinates has a useful
special form given below in display (6.65).
Recall the orthonormal frame [er, eb, en] in display (6.24), and let
F = Frer + Fbeb + Fnen.
The functions L, G : R3 × R3 → R, two of the components of the Delaunay
coordinate transformation, are defined by
L(X, Y ) := (−2E(X, Y ))−1/2, G(X, Y ) := |A(X, Y )|
where E is the energy and A the angular momentum. Using the results in
display (6.23), we have that
L˙ = L3	F, R˙
, G˙ = 1
G	R × R, R˙ × F
.
Moreover, in view of the vector identities
	α, β × γ
 = 	γ,α × β
 = 	β, γ × α
,
(α × β) × γ = 	γ,α
β − 	γ,β
α,
and the results in display (6.24), it follows that
G˙ = 1
G	F,(R × R˙) × R
 = 1
G	F, rGeb
 = rFb. (6.42)
Also, using the formula for the triple vector product and the equality rr˙ =
	R, R˙
, which is obtained by differentiating both sides of the identity r2 =
	R, R
, we have
eb = 1
rG(R × R˙) × R = 1
rG(r2R˙ − 	R, R˙
R)
and
R˙ = ˙rer +
G
r
eb. (6.43)274 6. Applications
As a result, the Delaunay elements L and G satisfy the differential equations
L˙ = L3( ˙rFr +
G
r
Fb), G˙ = rFb. (6.44)
If the force F does not vanish, then the relations found previously for the
variables related to the osculating plane of the Kepler orbit are still valid,
only now the quantities a, e, v, and Pθ > 0 all depend on time. Thus, for
example, if we use equation (6.30), then
G = Pθ = a(1 − e2). (6.45)
Also, from equations (6.38), (6.43), and the definition of L we have
L = √a, e2 = 1 − G2
L2 , (6.46)
and
− 1
2a = 1
2
	R, ˙ R˙
 − 1
r = 1
2

r˙
2 +
a(1 − e2)
r2

− 1
r
. (6.47)
Let us solve for ˙r2 in equation (6.47), express the solution in the form
r˙
2 = − 1
ar2 (r − a(1 − e))(r − a(1 + e)),
and substitute for r from formula (6.37) to obtain
r˙ = e sin v
G . (6.48)
Hence, from equation (6.44),
L˙ = L3
Fr
e
G sin v + Fb
G
r

, G˙ = rFb. (6.49)
The Delaunay variable H is defined by
H := 	A, ez
 = G	
1
GR × R, e ˙ z
 = G cosi (6.50)
where i is the inclination angle of the osculating plane (see equation (6.25)).
To find an expression for H˙ , let us first recall the transformation equa￾tions (6.27). Because en has “primed” coordinates (0, 0, 1), it follows that
en has original Cartesian coordinates
en = (sin h sin i, − cos h sin i, cosi), (6.51)
and, similarly, R is given by
R = r(cos θ cos h − sin θ sin h cosi, cos θ sin h + sin θ cos h cosi,sin θ sin i).
(6.52)6.2 Origins of ODE: Classical Physics 275
Using equation (6.51) and equation (6.52), we have
eb = en × er
= 1
r
en × R
= (− sin θ cos h − cos θ sin h cosi,
− sin θ sin h + cos θ cos h cosi, cos θ sin i). (6.53)
By differentiating both sides of the identity Gen = A, using the equation
er ×eb = en, and using the second identity of display (6.23), it follows that
Ge˙n = A˙ − Ge˙ n
= rer × F − rFben
= r(Fber × eb + Fner × en) − rFben
= −rFneb. (6.54)
The equations
di
dt = rFn
G cos θ, h˙ = rFn sin θ
G sin i (6.55)
are found by equating the components of the vectors obtained by sub￾stitution of the identities (6.51) and (6.53) into (6.54). Finally, using the
definition (6.50) together with equations (6.49) and (6.55), we have the
desired expression for the time derivative of H, namely,
H˙ = r(Fb cosi − Fn sin i cos θ). (6.56)
Using the formula for R˙ given in equation (6.43), the identity 	er, ez
 =
sin θ sin i derived from equation (6.52), and the equation 	eb, ez
 = cos θ sin i
from (6.53), let us note that
	R, e ˙ z
 = ˙r sin isin θ +
G
r
sin i cos θ.
A similar expression for 	R, e ˙ z
 is obtained by differentiation of both sides
of the last equation in display (6.27) and substitution for di/dt from (6.55).
The following formula for ˙
θ is found by equating these two expressions:
˙
θ = G
r2 − rFn cosisin θ
G sin i . (6.57)
From equations (6.37) and (6.45),
r = G2
1 + e cos v
. (6.58)
Let us solve for ˙v in the equation obtained by logarithmic differentiation
of equation (6.58). Also, if we use the identity 1 − e2 = G2/L2 to find an
expression for ˙e, substitute for L˙ and G˙ using the equation in display (6.49),276 6. Applications
and substitute for ˙r using (6.48), then, after some simple algebraic manip￾ulations,
v˙ = G
r2 + Fr
G
e
cos v + Fb
G2
re sin v
G cos v
e − 2r
G − r2 cos v
eL2G

.
A more useful expression for ˙v is obtained by substituting for r from equa￾tion (6.58) to obtain
v˙ = G
r2 + Fr
G
e
cos v − Fb
G
e

1 +
r
G2

sin v. (6.59)
Recall equation (6.36), and define g, the argument of periastron, by g :=
θ − v. Using equations (6.57) and (6.59), the time derivative ˙g is
g˙ = −Fr
G
e
cos v + Fb
G
e

1 +
r
G2

sin v − Fn
r cosi
G sin i
sin(g + v). (6.60)
The last Delaunay element, , called the mean anomaly, is defined with
the aid of an auxiliary angle u, the eccentric anomaly, via Kepler’s equation
 = u − e sin u (6.61)
where u is the unique angle such that
cos u = e + cos v
1 + e cos v
, sin u = 1 − e cos u
√1 − e2
sin v. (6.62)
The lengthy algebraic computations required to obtain a useful expres￾sion for ˙
 are carried out as follows: Differentiate both sides of Kepler’s
equation and solve for ˙
 in terms of ˙u and ˙e. Use the relations (6.62) and
equation (6.58) to prove the identity
r = L2(1 − e cos u) (6.63)
and use this identity to find an expression for ˙u. After substitution using
the previously obtained expressions for ˙r, ˙e, and L˙ , it is possible to show
that
˙
 = 1
L3 +
r
eL

(−2e + cos v + e cos2 v)Fr − (2 + e cos v) sin vFb

. (6.64)
In summary, the Delaunay elements (L, G, H, , g, h) for a Keplerian
motion perturbed by a force F satisfy the following system of differential6.2 Origins of ODE: Classical Physics 277
equations:
L˙ = L3
Fr
e
G sin v + Fb
G
r

,
G˙ = rFb,
H˙ = r(Fb cosi − Fn sin i cos(g + v)),
˙
 = 1
L3 +
r
eL

(−2e + cos v + e cos2 v)Fr − (2 + e cos v) sin vFb

,
g˙ = −Fr
G
e
cos v + Fb
G
e

1 +
r
G2

sin v − Fn
r cosi
G sin i
sin(g + v),
h˙ = rFn
sin(g + v)
G sin i . (6.65)
Our transformation of the equations of motion for the perturbed Kepler
problem to Delaunay elements—encoded in the differential equations in
display (6.65)—is evidently not complete. Indeed, the components of the
force F, as well as the functions
r, e, cos v, sin v, cosi, sin i,
must be expressed in Delaunay elements. Assuming that this is done, it is
still not at all clear how to extract useful information from system (6.65).
Only one fact seems immediately apparent: If the force F is not present,
then the Kepler motion relative to the Delaunay elements is a solution of
the integrable system
L˙ = 0, G˙ = 0, H˙ = 0, ˙
 = 1
L3 , g˙ = 0, h˙ = 0.
In fact, by inspection of this unperturbed system, it is clear that the Kep￾lerian motion is very simple to describe in Delaunay coordinates: The three
“action” variables L, G, and H remain constant and only one of the angular
variables, namely , is not constant. In particular, for each initial condition,
the motion is confined to a topological circle and corresponds to uniform
rotation of the variable , that is, simple harmonic motion. The correspond￾ing Keplerian orbit is periodic.
The result is that two of the three angles that appear in system (6.65)
remain constant for unperturbed motions is a special, perhaps magical,
feature of the inverse square central force law. This special degeneracy of
Kepler motion will eventually allow us to derive some rigorous results about
the perturbed system, at least in the case where the force F is “small,” that
is, where F = F, the function F is bounded, and  ∈ R is regarded as a
small parameter.
As we have just seen, a suitable change of coordinates can be used to
transform the Kepler model equations to a very simple form. The underly￾ing reason, mentioned previously, is that a region in the unperturbed phase278 6. Applications
space is foliated by invariant tori. Due to the special nature of the inverse
square force law, each two-dimensional torus in this foliation is itself foli￾ated by periodic orbits, that is, by one-dimensional tori. In other words,
there is an open region of the unperturbed phase space filled with periodic
orbits.
The foliation in the Kepler model is exceptional. In the generic case, we
would not expect the flow on every invariant torus to be periodic. Instead,
the flow on most of the tori would be quasi-periodic. To be more precise,
consider a Poincar´e section Σ and note that a two-dimensional torus in the
foliation meets Σ in a one-dimensional torus T. The associated Poincar´e
map is (up to a change of coordinates) a (linear) rotation on T. The rotation
angle is either rationally or irrationally related to 2π, and the angle of
rotation changes continuously with respect to a parametrization of the tori
in the foliation. In this scenario, the set of “quasi-periodic tori” and the set
of “periodic tori” are both dense. But, with respect to Lebesgue measure,
the set of quasi-periodic tori is larger. In fact, the set of quasi-periodic tori
has measure one, whereas the set of periodic tori has measure zero (see
Ex. 6.31). For the special case of Kepler motion, the flow is periodic on
every torus.
Exercise 6.31. Consider the map P : R2 \ {(0, 0)} → R2 given by
P(x, y)=(x cos(x2 + y2
) − y sin(x2 + y2
), x sin(x2 + y2
) + y cos(x2 + y2
)).
(a) Prove that every circle C with radius r centered at the origin is an invariant
set. (b) Prove that if m and n are integers and r2 = 2πm/n, then every orbit of
P on C is periodic. (c) Prove that if no such integers exist, then every orbit of
P on C is dense. (d) Prove that the set of radii in a finite interval (for example,
the interval [1, 2]) corresponding to tori where all orbits of the Poincar´e map are
dense has measure one.
The origin of many important questions on the subject of differential
equations arises from the problem of analyzing perturbations of integrable
systems; that is, systems whose phase spaces are foliated by invariant tori.
In fact, if the phase space of a system is foliated by k-dimensional tori,
then there is a new coordinate system in which the equations of motion
have the form
˙
I = P(I,θ), ˙
θ = Ω(I) + Q(I,θ)
where I is a vector of “action variables,” θ is a k-dimensional vector of
“angle variables,” and both P and Q are 2π-periodic functions of the
angles. Poincar´e called the analysis of this system the fundamental problem
of dynamics. In other words, if we start with a “completely integrable”
mechanical system in action-angle variables so that it has the form ˙
I = 0, ˙
θ = Ω(I), and if we add a small force, then the problem is to describe6.2 Origins of ODE: Classical Physics 279
the subsequent motion. This problem has been a central theme in mathe￾matical research for over 100 years; it is still a prime source of important
problems.
Let us outline a procedure to complete the transformation of the per￾turbed Kepler system (6.65) to Delaunay elements. Use equation (6.45) and
the definition of L to obtain the formula
G2 = L2(1 − e2),
and note that from the definition of H we have the identity
cosi = H
G .
From our assumption that G = Pθ > 0, and the inequality 0 ≤ i<π,
we can solve for e and i in terms of the Delaunay variables. Then, all
the remaining expressions not yet transformed to Delaunay variables in
system (6.65) are given by combinations of terms of the form rn sin mv
and rn cos mv where n and m are integers. In theory we can use Kepler’s
equation to solve for u as a function of  and e. Thus, if we invert the
transformation (6.62) and also use equation (6.63), then we can express
our combinations of r and the trigonometric functions of v in Delaunay
variables.
The inversion of Kepler’s equation is an essential element of the transfor￾mation to Delaunay variables. At a more fundamental level, the inversion
of Kepler’s equation is required to find the position of a planet on its ellip￾tical orbit as a function of time. The rigorous treatment of the inversion
problem seems to have played a very important role in the history of 19th
century mathematics.
Problem 6.32. Write a report on the history and the mathematics related
to Kepler’s equation (see [69] and [105]). Include an account of the history of
the theory of Bessel functions ([68], [255]) and complex analysis ([16], [37]).
To invert Kepler’s equation formally, set w := u −  so that
w = e sin(w + ),
suppose that
w = ∞
j=1
wj ej ,
use the sum formula for the sine, and equate coefficients in Kepler’s equa￾tion for w to obtain the wj as trigonometric functions of . One method that
can be used to make this inversion rigorous, the method used in Bessel’s
original treatment, is to expand in Fourier series.280 6. Applications
It is easy to see, by an analysis of Kepler’s equation, that the angle  is
an odd function of u. Thus, after inverting, u is an odd function of  as is
e sin u. Thus,
e sin u = 2
π
∞
ν=1
  π
0
e sin u sin ν d
sin ν,
and, after integration by parts,
e sin u = 2
π
∞
ν=1
1
ν
 π
0
cos ν(e cos u
du
d ) d
sin ν.
By Kepler’s equation e cos u = 1 − d/du. Also, we have that
e sin u = 2
π
∞
ν=1
1
ν
 π
0
cos(ν(u − e sin u)) du
sin ν.
Bessel defined the Bessel function of the first kind
Jν(x) := 1
2π
 2π
0
cos(νs − x sin s) ds = 1
π
 π
0
cos(νs − x sin s) ds
so that
e sin u = ∞
ν=1
2
ν
Jν(νe) sin ν.
Hence, if we use the definition of the Bessel function and Kepler’s equation,
then
u =  +∞
ν=1
2
ν
Jν(νe) sin ν.
By similar, but increasingly more difficult calculations, all products of the
form
rn sin mu, rn cos mu, rn sin mv, rn cos mv,
where n and m are integers, can be expanded in Fourier series in  whose
νth coefficients are expressed as linear combinations of Jν(νe) and J
ν(νe)
(see [152] and [255]). Thus, we have at least one method to transform
system (6.65) to Delaunay elements.
6.2.4 Satellite Orbiting an Oblate Planet
The earth is not a sphere. In this section we will consider the perturbations
of satellite motion caused by this imperfection.
The law of universal gravitation states that two particles (point masses)
attract by the inverse square law. The earth is composed of lots of particles.
But, if the earth is idealized as a sphere with uniformly distributed mass,6.2 Origins of ODE: Classical Physics 281
then the gravitational force exerted on a satellite obeys the inverse square
law for the earth considered as a point mass concentrated at the center
of the sphere. But because the true shape of the earth is approximated
by an oblate spheroid that “bulges” at the equator, the gravitational force
exerted on a satellite depends on the position of the satellite relative to the
equator. As we will see, the equations of motion of an earth satellite that
take into account the oblateness of the earth are quite complex. At first
sight it will probably not be at all clear how to derive useful predictions
from the model. But, as an illustration of some of the ideas introduced so
far in this chapter, we will see how to transform the model equations into
action-angle variables. Classical perturbation theory can then be used to
make predictions.
Introduce Cartesian coordinates so that the origin is at the center of
mass of an idealized planet viewed as an axially symmetric oblate spheroid
whose axis of symmetry is the z-axis. The “multipole” approximation of
the corresponding gravitational potential has the form
−G0m1
r
+ U(r, z) + O
R0
r
3
where
U = −1
2
G0m1J2R2
0
r3

1 − 3
z2
r2

,
m1 is the mass of the planet, R0 is the equatorial radius, and J2 is a
constant related to the moments of inertia of the planet (see [108] and
[171]). Note that the first term of the multipole expansion is just the point
mass gravitational law that determines the Kepler motion.
The oblateness problem has been widely studied by many different meth￾ods, some more direct than our Delaunay variable approach (see [171], [207],
[108], and [227]). But, our approach serves to illustrate some general meth￾ods that are widely applicable.
As an approximation to the gravitational potential of the planet, let us
drop the higher order terms and consider Kepler motion perturbed by the
force determined by the second term of the multipole expansion, that is,
the perturbing force per unit of mass is the negative gradient of U. Since
the satellite has negligible mass compared to the planet, we may as well
assume that the motion of the planet is not affected by the satellite. Thus,
under this assumption and in our notation, if we let m2 denote the mass
of the satellite, then F2 = −m2 gradU and the equation of motion for the
satellite is given by
R¨ = −G0m1
r3 R − gradU. (6.66)
To use the general formulas for transformation to Delaunay variables
given in display (6.65), we must first rescale system (6.66). For this, let
β denote a constant measured in seconds and let α := (G0m1)1/3β2/3,282 6. Applications
so that α is measured in meters. Then, rescaling as in the derivation of
equation (6.18), we obtain the equation of motion
R¨ = − 1
r3 R + F (6.67)
where
Fx = − 
r5

1 − 5
z2
r2

x,
Fy = − 
r5

1 − 5
z2
r2

y,
Fz = − 
r5

3 − 5
z2
r2

z, (6.68)
and
 := 3
2
J2
R2
0
(G0m1)2/3β4/3
is a dimensionless parameter. It turns out that if we use parameter values
for the earth of
G0m1 ≈ 4 × 1014m3/sec2, R0 ≈ 6.3 × 106m, J2 ≈ 10−3,
then  ≈ 11β−4/3.
By adjusting our “artificial” scale parameter β, we can make the param￾eter  as small as we like. But there is a cost: The unit of time in the scaled
system is β seconds. In particular, if  is small, then the unit of time is
large. At any rate, the rescaling suggests that we can treat  as a “small
parameter.”
We have arrived at a difficult issue in the analysis of our problem that
often arises in applied mathematics. The perturbation parameter  in our
model system is a function of β. But we don’t like this. So we will view
β as fixed, and let  be a free variable. Acting under this assumption,
let us suppose that we are able to prove a theorem: If  > 0 is sufficiently
small, then the system . . . . Does our theorem apply to the original unscaled
system? Strictly speaking, the answer is “no”! Maybe our sufficiently small
values of  are smaller than the value of  corresponding to the fixed value
of β.
There are several ways to avoid the snag. For example, if we work harder,
we might be able to prove a stronger theorem: There is a function β → 0(β)
given by . . . such that if 0 ≤ 0(β), then the corresponding system . . . . In
this case, if β is fixed and the corresponding value of  is smaller than
0(β), then all is well. But, in most realistic situations, the desired stronger
version of our hypothetical theorem is going to be very difficult (if not
impossible) for us to prove. Thus, we must often settle for the weaker
version of our theorem and be pleased that the conclusion of our theorem6.2 Origins of ODE: Classical Physics 283
holds for some choices of the parameters in the scaled system. This might
be good. For example, we can forget about the original model, declare
the scaled model to be the mathematical model, and use our theorems to
make a prediction from the scaled model. If our predictions are verified by
experiment, then we might be credited with an important discovery. At
least we will be able to say with some confidence that we understand the
associated phenomena mathematically, and we can be reasonably certain
that we are studying a useful model. Of course, qualitative features of our
scaled model might occur in the original model (even if we cannot prove
that they do) even for physically realistic values of the parameters. This
happens all the time. Otherwise, no one would be interested in perturbation
theory. Thus, we have a reason to seek evidence that our original model
predicts the same phenomena that are predicted by the scaled model with a
small parameter. We can gather evidence by performing experiments with
a numerical method, or we can try to prove another theorem.
Returning to our satellite, let us work with the scaled system and treat
 as a small parameter. To express the components of the force resolved
relative to the frame [er, eb, en] in Delaunay elements, let us transform
the vector field F to this frame using the transformation M defined in
display (6.52) followed by a transformation to (er, eb, en)-coordinates. In
fact, the required transformation is
N :=
⎛
⎝
cos h − sin h 0
sin h cos h 0
0 01
⎞
⎠
⎛
⎝
10 0
0 cosi − sin i
0 sin i cosi
⎞
⎠
⎛
⎝
cos θ − sin θ 0
sin θ cos θ 0
0 01
⎞
⎠ .
Note that the angle θ is given by θ = g + v and the position vector is
given by R = (r, 0, 0) in the (er, eb, en)-coordinates. Using the usual “push
forward” change of coordinates formula
N −1F(N(R)),
it follows that the transformed components of the force are
Fr = − 
r4 (1 − 3 sin2(g + v) sin2 i),
Fb = − 
r4 sin(2g + 2v) sin2 i,
Fn = − 
r4 sin(g + v) sin 2i. (6.69)
Substitution of the force components (6.69) into system (6.65), followed
by expansion in Fourier series in , gives the equations of motion for a
satellite orbiting an oblate planet. While the resulting equations are quite
complex, it turns out that the equation for H is very simple; in fact, H˙ = 0.
This result provides a useful internal check for our formulas expressed in
Delaunay elements because it can be proved directly from the definition284 6. Applications
of H as the z-component of the angular momentum in the original Carte￾sian coordinates: Simply differentiate in formula (6.50) and then use for￾mula (6.23). Thus, we have extracted one prediction from the equations of
motion: The z-component of the angular momentum remains constant as
time evolves. Of course, the axial symmetry of the mass also suggests that
H is a conserved quantity.
Recall that system (6.65) has a striking feature: while the angle  is
changing rapidly in the scaled time, the actions L, G, and H together with
the other angles g and h change (relatively) slowly—their derivatives have
order . As a rough measure of the time scale on which the model is valid,
let us determine the time scale on which the satellite maintains “forward
motion” (that is, ˙
 > 0). Since L˙ = O(), we have that L(t) ≈ L(0) + C1t
where L(0) > 0 and C1 is a constant. By substitution of this expression
into the right-hand side of the equation
˙
 = 1
L3 + O()
and using C2 to approximate the O() term in this equation, the approx￾imation of the right-hand side can only vanish if t has order −4/3. As an
approximation, we may as well say that the forward motion for the per￾turbed solution is maintained at least on a time scale of order 1/, that is,
for 0 ≤ t ≤ C/, where C is some positive constant.
Because of the slow variation of the actions, it seems natural, at least
since the time of Laplace, to study the average motion of the slow variables
relative to . The idea is that all of the slow variables are undergoing rapid
periodic oscillations due to the change in , at least over a long time scale.
Thus, if we average out these rapid oscillations, then the “drift” of the slow
variables will be apparent. As mentioned before, we will make this idea
precise in Chapter 10. Let us see what predictions can be made after this
averaging is performed on the equations of motion of the satellite orbiting
the oblate planet.
The averages that we wish to compute are the integral averages over 
on the interval [0, 2π] of the right-hand sides of the equations of motion
in display (6.65). As we have seen, the variable  appears when we change
r, cos v, and sin v to Delaunay variables. Let us consider the procedure for
the variable G. Note that after substitution,
G˙ = − sin2 i
sin(2g + 2v)
r3 .
Using the sum formula for the sine, we see that we must find the average
	
sin 2v
r3 
 := 1
2π
 2π
0
sin 2v
r3 d
and the average 	(cos 2v)/r3
. (Warning: Angle brackets are used to denote
averages and inner products. But this practice should cause no confusion if6.2 Origins of ODE: Classical Physics 285
the context is taken into account.) The procedure for computing these and
all the other required averages for the Delaunay differential equations is evi￾dent from the following example. Differentiate in Kepler’s equation (6.61)
to obtain the identity
d
dv = (1 − e cos u)
du
dv
and likewise in the expression for cos u in display (6.62) to obtain
du
dv = 1 − e cos u
√1 − e2 .
Combine these results to compute
d
dv = r2
GL3
and use a change of variable in the original integral to find the average
	
sin 2v
r3 
 = 1
2πL3G
 2π
0
sin 2v
r
dv.
Finally, substitution for r from equation (6.58) and an easy integration are
used to prove that 	(sin 2v)/r3
 = 0 and 	(cos 2v)/r3
 = 0. As a result, it
follows that G˙ = 0. Similarly, the complete set of averaged equations of
motion are
L˙ = G˙ = H˙ = 0,
g˙ = − 1
2L3G4 (5 sin2 i − 4),
h˙ = − 1
L3G4 cosi (6.70)
where cosi = H/G. Let us note that the dependent variables that appear
in the averaged system (6.70) should perhaps be denoted by new symbols
so that solutions of system (6.70) are not confused with solutions of the
original system. This potential confusion will not arise here.
Finally, we have arrived at a system that we can analyze. In fact, the
(square root of the) semimajor axis of the osculating ellipse, the total angu￾lar momentum, and the z-component of the angular momentum are con￾stant on average. The argument of periastron g, or, if you like, the angle
from the equatorial plane to the line corresponding to the perigee (closest
approach of the satellite) is changing on average at a rate proportional to
4 − 5 sin2 i. If the inclination i of the osculating plane—an angle that is on
average fixed—is less than the critical inclination where sin2 i = 4
5 , then
the perigee of the orbit advances. If the inclination angle is larger than
the critical inclination, then the perigee is retrograde. Similarly, the rate
of regression of the ascending node—given by the angle h in the equatorial286 6. Applications
plane relative to the x-axis—is proportional to the quantity − cosi. Thus,
for example, if the orbit is polar (i = π
2 ), then the rate of regression is zero
on average. These observations indicate the importance of the critical incli￾nation, but a deeper analysis is required to predict the satellite’s motion
near the critical inclination (see [73]–[77]).
The averaging computation that we have just completed is typical of
many “first order” approximations in mechanics. Averaging is, of course,
only one of the basic methods that have been developed to make predictions
from “realistic” systems of ordinary differential equations that originate in
celestial mechanics.
Exercise 6.33. Because the perturbation force for the oblate planet comes from
a potential, the force is conservative. In fact, the perturbed system in this case is
Hamiltonian with the total energy, including the correction to the gravitational
potential, as the Hamiltonian function. It turns out that the coordinate change
to Delaunay variables is of a very special type, called canonical. For a canoni￾cal change of coordinates it is not necessary to change variables directly in the
equations of motion. Instead, it suffices to change variables in the Hamiltonian
function and then to derive the new equations of motion in the usual way from the
transformed Hamiltonian (see Section 1.7.1). Show, by constructing an example,
that a general change of coordinates is not canonical. Assume that the Delau￾nay coordinates are canonical, write the Hamiltonian in Delaunay variables, and
derive from it the Hamiltonian equations of motion in Delaunay variables. In
particular, show, using the form of the Hamiltonian differential equations, that
the average of L˙ over the angle  must vanish. This provides an internal check for
the formulas derived in this section. Do you see how one might obtain the aver￾aged equations directly from the Hamiltonian? One reason why the Hamiltonian
approach was not used in the derivation of the equations in Delaunay elements
is that we have not developed the theory required to prove that the change to
Delaunay variables is canonical. Another reason is that our approach works even
if the perturbing force is not conservative.
6.2.5 The Diamagnetic Kepler Problem
In this section we will derive equations of motion for the electron of the
hydrogen atom in a constant magnetic field. The purpose of the section
is to apply Delaunay variables, averaging, and the transformation from
Lagrangian to Hamiltonian mechanics. We will discuss a version of Larmor’s
theorem.
Consider the classical equations for the motion of an electron of an atom
in the presence of a constant magnetic field. Let us assume that the electron
is subjected to the Coulomb potential relative to the nucleus of the atom
and to the Lorentz force due to the constant magnetic field B. If q is the
charge of an electron and Z is the atomic number of the atom, and if, as
usual, R is the position of the electron relative to Cartesian coordinates
centered at the nucleus, V is the velocity of the electron, and r := |R|, then6.2 Origins of ODE: Classical Physics 287
the Coulomb potential is
U := kZq(−q)
r = −kZq2
r
where k is a constant. (Note the similarity to the gravitational potential!)
In our choice of units, q is measured in coulombs and kq2, often denoted
e2 in physics where of course e is not the eccentricity, has value kq2 ≈
(1.52 × 10−14)2Nm2 in mks units where N is used to denote newtons. For
the rest of this section let us suppose that Z = 1, the atomic number of
the hydrogen atom.
Let us assume that the constant magnetic field B is parallel to the z-axis
and the electric field E vanishes. Then, as we have seen previously, the
Lorentz force is given by
qV × B.
According to Newton’s law, the equations of motion are given in vector
form by
p˙ = qV × B − kq2
r3 R (6.71)
where p is the momentum. Because the speed of the electron of a hydrogen
atom is about one percent of the speed of light ([101]), let us use the
classical momentum p = mV .
Equation (6.71) is given in components by
mx¨ = −kq2
r3 x + qby,˙
my¨ = −kq2
r3 y − qbx,˙
mz¨ = −kq2
r3 z, (6.72)
and after rescaling as in equation (6.67) it has the form
R¨ = − 1
r3 R + F
where F := ( ˙y, −x,˙ 0),  := 2ωβ, and ω := 1
2m qb is called the Larmor
frequency. Using the formulas in Section 6.2.3, V = R˙ expressed in the
frame [er, eb, en] is given by
R˙ = e sin v
G er +
G
r
eb,
and, after some computation, it follows that
Fr =  G cosi
r ,
Fb = − e cosisin v
G ,
Fn = − sin i
G (e sin g + sin(g + v)).288 6. Applications
Although the equations of motion (6.65) for this choice of F are compli￾cated, the corresponding averaged system is simple; in fact, we have
H˙ = G˙ = L˙ = ˙g = 0, h˙ = − 
2
. (6.73)
This result is a special case of Larmor’s theorem: For a charged particle
influenced by a centrally symmetric field (for example, the Coulomb field)
and an axially symmetric weak magnetic field, there is a time-dependent
coordinate system rotating with uniform velocity about the axis of symme￾try of the magnetic field (at the Larmor frequency) such that, in the new
coordinates, the motion is the same as for the charged particle influenced
by the centrally symmetric field only.
To determine the Lagrangian and Hamiltonian formulation of the model
equation (6.71), let us recast system (6.71) in the form
d
dt(p − 1
2
q(R × B)) = 1
2
q(V × B) − gradU. (6.74)
This first step may appear to arrive out of thin air. In fact, it is just
the bridge from Newtonian to Lagrangian mechanics. As we will see in a
moment, system (6.74) is in the form of an Euler-Lagrange equation.
In physics, a new vector field A, called the vector potential, is introduced
so that B = curl A. For our constant magnetic field, an easy computation
shows that A = 1
2B × R. This vector field can be substituted into the left￾hand side of equation (6.74) and used in the rest of the computation. Let
us, however, continue using the original fields.
If we define the Lagrangian
L := 1
2m	p, p
 + q
2
	B × R, V 
 − U,
then, using a vector identity, we have
∂L
∂V = p − q
2
(R × B),
∂L
∂R = −q
2
∂
∂R	R, B × V 
 − gradU
= q
2
V × B − gradU;
that is, equation (6.74) with Q := R = (x, y, z) is exactly
d
dt
 ∂L
∂Q˙

= ∂L
∂Q. (6.75)
In view of our derivation of the Euler-Lagrange equation in Section 6.1, we
have reason to expect that there is a variational principle associated with
the differential equation (6.75). This is indeed the case (see [155]).6.2 Origins of ODE: Classical Physics 289
The position variable Q and the velocity variable Q˙ define a coordinate
system on R3 × R3. Let us define a new variable
P := ∂L
∂Q˙
(Q, Q˙) = p − q
2
Q × B (6.76)
and note that the relation (6.76) can be inverted to obtain Q˙ as a function
of Q and P. In fact, there is a function α such that
P ≡ ∂L
∂Q˙
(Q, α(Q,P)).
Thus, we have defined new coordinates (P, Q) on R3 × R3.
The reason for introducing P is so that we can define the Hamiltonian
H := PQ−L ˙ (Q, Q˙) = Pα(Q,P) − L(Q, α(Q,P)).
This terminology is justified by the following results:
∂H
∂P = Q˙ + 
P − ∂L
∂Q˙
 ∂α
∂P = Q˙ ,
∂H
∂Q = − ∂L
∂Q = − d
dt
∂L
∂Q˙ = −P˙ .
Thus, the original system is equivalent to the Hamiltonian system with
Hamiltonian H. In particular, H is constant along orbits.
By the definition of H, we have
H = 	p + q
2
(B × R), V 
 − 1
2m	p, p
 − q
2
	B × R, V 
 + U(Q)
= 1
2m	p, p
 + U(Q)
= 1
2m	P + q
2
Q × B,P + q
2
Q × B
 + U(Q).
For the constant magnetic field B := (0, 0, b), the Hamiltonian is
H = 1
2m

P2
1 + P2
2 + P2
3

+ ω(yP1 − xP2) + mω2
2 (x2 + y2) − kq2
r (6.77)
where ω := 1
2m qb is the Larmor frequency. Here, the magnitude b of the
magnetic field has units N sec/(coul m) and the Larmor frequency ω has
units 1/sec.
Recall that, in general, the Hamiltonian is constant along solutions of
the corresponding Hamiltonian system. Thus, H is a first integral of its
corresponding Hamiltonian system. An interesting feature of this system
is the existence of another independent first integral. In fact, the angular
momentum function
(x, y, z,P1,P2,P3) → yP1 − xP2 (6.78)290 6. Applications
is constant on orbits (see Ex. 6.35).
By Larmor’s theorem we expect that there is a rotating coordinate sys￾tem (rotating with the Larmor frequency about the axis of symmetry of
the magnetic field) such that the transformed system is Hamiltonian but
with the “angular momentum term” ω(yP1 − xP2) eliminated from the
corresponding Hamiltonian. Indeed, an easy computation shows that the
change of variables (x
, y
,P
1,P
2) → (x, y,P1,P2) given by
x = x cos Ωt − y sin Ωt, y = x sin Ωt + y cos Ωt,
P1 = P
1 cos Ωt − P
2 sin Ωt, P2 = P
1 sin Ωt + P
2 cos Ωt
transforms the Hamiltonian system to a new system whose Hamiltonian
(after renaming the variables) is
H = 1
2m

P2
1 + P2
2 + P2
3

+ (ω + Ω)(yP1 − xP2) + mω2
2 (x2 + y2) − kq2
r .
Hence, if Ω := −ω, then the Hamiltonian for the transformed system is
given by
H∗ = 1
2m

P2
1 + P2
2 + P2
3

+
mω2
2 (x2 + y2) − kq2
r . (6.79)
Because of the analogy with the perturbed Kepler motion, this is called
the diamagnetic Kepler problem ([121]). The corresponding Hamiltonian
equations of motion are
x˙ = 1
m
P1, y˙ = 1
m
P2, z˙ = 1
m
P3,
P˙
1 = −
kq2
r3 x + mω2x

,
P˙
2 = −
kq2
r3 y + mω2y

,
P˙
3 = −kq2
r3 z. (6.80)
Equivalently, we have the second-order system
mx¨ = −kq2
r3 x − mω2x,
my¨ = −kq2
r3 y − mω2y,
mz¨ = −kq2
r3 z (6.81)
which is given in vector form by
R¨ = −kq2m−1
r3 R + F0 (6.82)6.2 Origins of ODE: Classical Physics 291
where F0 = −ω2(x, y, 0). This last system is again analogous to the equa￾tion for relative motion of the perturbed two-body problem.
Let us note that the Kepler orbit of the electron is rotating with the
Larmor frequency ω about the axis of symmetry of the constant magnetic
field. For a weak magnetic field, the rotation frequency is slow. According
to our reduced Hamiltonian system in the rotating coordinates, the pertur￾bation due to the magnetic field in the rotating system has order ω2. Thus,
the main effect is rotation of the electron about the axis of symmetry of
the magnetic field. The second-order effects, however, are important.
As an instructive project, rescale system (6.81) and use equations (6.65)
to transform the diamagnetic Kepler problem to Delaunay elements. Also,
average the transformed equations and discuss the average motion of the
electron (see Ex. 6.37). It turns out that the diamagnetic Kepler problem
has very complex (chaotic) motions; it is one of the model equations studied
in the subject called quantum chaos, but that is another story ([107], [121]).
Exercise 6.34. Verify the equations (6.73).
Exercise 6.35. Prove that the function given in display (6.78) is constant on
orbits of the Hamiltonian system with Hamiltonian (6.77). What corresponding
quantity is conserved for system (6.71)?
Exercise 6.36. Transform equation (6.72) directly to equation (6.81) using a
rotating coordinate system.
Exercise 6.37. Show that system (6.81) can be rescaled in space and time to
the dimensionless form
R¨ = − 1
r3 R − ω2
β2
⎛
⎝
x
y
0
⎞
⎠ (6.83)
where β is measured in seconds. Define  := ω2β2 and show that the scaled
system is equivalent to the first-order system
x˙ = P1, y˙ = P2, z˙ = P3,
P˙1 = −x/r3 − x, P˙2 = −y/r3 − y, P˙3 = −z/r3.
Use the result of Exercise 6.35 and the new variables (ρ, θ, z, Pρ, Pθ, Pz) given by
x = ρ cos θ, y = ρ sin θ,
Pρ = cos θ P1 + sin θ P2, Pθ = x P2 − y P1, Pz = P3
to show that the differential equation expressed in the new variables decouples
so that the set of orbits with zero angular momentum correspond to solutions of
the subsystem
ρ˙ = Pρ, z˙ = Pz, P˙ρ = − ρ
(ρ2 + z2)3/2 − ρ, P˙z = − z
(ρ2 + z2)3/2 . (6.84)292 6. Applications
Exercise 6.38. Consider system (6.83) to be in the form
R¨ = − 1
r3 R + F.
Show that
Fr = −r cos2 θ, Fb = r sin θ cos θ, Fn = 0,
and the averaged Delaunay system is given by
L˙ = 0, G˙ = 
5
4
L2
(L2 − G2
) sin 2g, g˙ = −
1
4
L2
G(3 + 5 cos 2g).
Draw the phase cylinder portrait of the (g, G)-subsystem. Find the rest points
and also show that there is a homoclinic orbit.
Exercise 6.39. Consider the diamagnetic Kepler problem as a perturbation,
by the Coulomb force, of the Hamiltonian system with Hamiltonian
H∗
0 = 1
2m
	
P2
1 + P2
2 + P2
3


+ mω2
2 (x2 + y2
).
Write out Hamilton’s equations, change coordinates so that the equations of
motion corresponding to the Hamiltonian H∗
0 are in action-angle form (use polar
coordinates), and find the perturbation in the new coordinates. Is averaging rea￾sonable for this system?
6.3 Coupled Pendula: Normal Modes and Beats
Consider a pendulum of length L and mass m where the angle with positive
orientation with respect to the downward vertical is θ and g denotes the
gravitational constant (near the surface of the earth). The kinetic energy of
the mass is given by K := m
2 (L ˙
θ)2 and the potential energy is given by U :=
−mgLcos θ. There are several equivalent ways to obtain the equations of
motion; we will use the Lagrangian formulation. Recall that the Lagrangian
of the system is
L := K − U = m(L ˙
θ)2
2 + mgLcos θ, (6.85)
and the equation of motion is given by Lagrange’s equation
d
dt
∂L
∂ ˙
θ − ∂L
∂θ = 0.
Thus, the equation of motion for the pendulum is
mL2 ¨θ + mgLsin θ = 0. (6.86)
For two identical pendula coupled by a Hookian spring with spring con￾stant k, the potential energy due to the spring depends on (at least) the6.3 Coupled Pendula: Normal Modes and Beats 293
Figure 6.3: Two pendula connected by a spring. To build a simple bench
model, consider suspending two lead fishing weights on “droppers” from a
stretched horizontal section of monofilament.
placement of the spring and the choice of the plane for the pendulum
motion. For small oscillations, a reasonable linear approximation of the
true potential energy due to the spring is given by
1
2
ka2(θ1 − θ2)
2
where a is a dimensionless constant and  is the distance from the pivot
point of the pendulum to the point where the spring is attached. Here,
we do not model the physical attachment of the spring and the pendula.
Perhaps a different expression for the potential energy of the spring would
be required to model a laboratory experiment. Our model, however, is a
reasonable approximation as long as the spring moves in a fixed plane
where the spring is either stretched or twisted by the pendulum motion.
The corresponding Lagrangian is
L = m
2

(L ˙
θ1)
2 + (L ˙
θ2)
2
+ mgL(cos θ1 + cos θ2) − 1
2
ka2(θ1 − θ2)
2,
and the equations of motion are given by
mL2 ¨θ1 + mgLsin θ1 + ka2(θ1 − θ2)=0,
mL2 ¨θ2 + mgLsin θ2 − ka2(θ1 − θ2)=0.
The independent variable and system parameters are rendered dimen￾sionless by rescaling time via t = μs, where μ := (L/g)1/2, and introducing
the dimensionless constant α := kas2/(mgL) to obtain the system294 6. Applications
θ
1 + sin θ1 + α(θ1 − θ2)=0,
θ
2 + sin θ2 − α(θ1 − θ2)=0.
To study the motions of the system for “small” oscillations of the pen￾dula, the approximation sin θ ≈ θ—corresponding to linearization of the
system of differential equations at the origin—yields the model
θ
1 + (1 + α)θ1 − αθ2 = 0,
θ
2 − αθ1 + (1 + α)θ2 = 0. (6.87)
Although this linear second-order system can be expressed as a first order
system and solved in the usual manner by finding the eigenvalues and
eigenvectors of the system matrix, there is a simpler way to proceed. In
fact, if Θ is defined to be the transpose of the state vector (θ1, θ2), then
system (6.87) has the form
Θ = AΘ
where A is the matrix
	−(1 + α) α
α −(1 + α)


.
The idea is to diagonalize the symmetric matrix A by a linear change of vari￾ables of the form Θ = BZ, where B is an orthogonal matrix whose columns
are unit-length eigenvectors of A, so that the transformed differential equa￾tion, Z = B−1ABZ, decouples. For system (6.87), the component form of
the inverse change of variables Z = B−1Θ is
x = 1
√2
(θ1 + θ2), y = 1
√2
(θ1 − θ2),
and the decoupled system is
x = −x, y = −(1 + 2α)y.
There are two normal modes of oscillation. If y(s) ≡ 0, then θ1(s) −
θ2(s) ≡ 0 and the pendula swing “in phase” with unit frequency relative to
the scaled time. If x(s) ≡ 0, then θ1(s) + θ2(s) ≡ 0 and the pendula swing
“in opposing phase” with angular frequency (1 + 2α)1/2 in the scaled time.
The frequency of the second normal mode is larger than the frequency of
the first normal mode due to the action of the spring; the spring has no
effect on the first normal mode.
Consider the following experiment. Displace the second pendulum by a
small amount and then release it from rest. What happens?6.3 Coupled Pendula: Normal Modes and Beats 295
In our mathematical model, the initial conditions corresponding to the
experiment are
θ1 = 0, θ
1 = 0, θ2 = a, θ
2 = 0.
The predicted motion of the system, with β := √1+2α, is given by
x(s) = a
√2
cos s, y(s) = − a
√2
cos βs,
and
θ1(s) = a
2
(cos s − cos βs), θ2(s) = a
2
(cos s + cos βs).
Use the usual identities for cos(φ ± ψ) with
φ := 1 + β
2 s, ψ := 1 − β
2 s,
to obtain
θ1(s) = 
a sin β − 1
2
s

sin β + 1
2 s, θ2(s) = 
a cos
β − 1
2
s

cos
β + 1
2 s,
and note that each pendulum swings with quasi-frequency 1
2 (β + 1) and
(relatively) slowly varying amplitude. Also, the “beats” of the two pendula
are out of phase; that is, whereas the first pendulum is almost motion￾less and the second pendulum swings at maximum amplitude when s is
approximately an integer multiple of 2π/(β − 1), the second pendulum is
almost motionless and the first pendulum swings at maximum amplitude
when s is approximately an odd integer multiple of π/(β−1). This interest￾ing exchange-of-energy phenomenon can be observed even with very crude
experimental apparatus—try it.
Exercise 6.40. Suppose that the kinetic energy of a mechanical system is given
by 1
2 KΘ˙ , Θ˙  and its potential energy is given by 1
2 PΘ, Θ, where Θ is the state
vector, K and P are symmetric matrices, and the angle brackets denote the
usual inner product. If both quadratic forms are positive definite, show that they
can be simultaneously diagonalized. In this case, the resulting system decouples.
Solutions corresponding to the oscillation of a single component while all other
components are at rest are called normal modes. Determine the frequencies of
the normal modes (see [13]).
Exercise 6.41. (a) Find the general solution of the system
x¨ = a sin t(x sin t + y cos t) − x, y¨ = a cos t(x sin t + y cos t) − y.
Hint: Find a time-dependent transformation that makes the system autonomous.
(b) Find the Floquet multipliers corresponding to the zero solution. (c) For which
values of a is the zero solution stable; for which values is it stable?296 6. Applications
Figure 6.4: Representation of the Fermi-Ulam-Pasta coupled oscillator.
Exercise 6.42. Build a bench top experiment with two “identical” coupled
pendula (see Figure 6.3), and tune it until the beat phenomena are observed. Show
that a measurement of the length of the pendula together with a measurement of
the number of oscillations of one pendulum per second suffices to predict the time
interval required for each pendulum to return to rest during a beating regime.
Does your prediction agree with the experiment? How sensitive is the predicted
value of this time scale relative to errors in the measurements of the lengths of the
pendula and the timing observation? Approximate the spring constant in your
physical model?
Problem 6.43. Consider small oscillations of the coupled pendula in case
there are two different pendula, that is, pendula with different lengths
or different masses. What happens if there are several pendula coupled
together in a ring or in series? What about oscillations that are not small?
What predictions (if any) made from the linear model remain valid for the
nonlinear model? What happens if there is damping in the system?
6.4 The Fermi-Ulam-Pasta Oscillator
The analysis of the small oscillations of coupled pendula in Section 6.3 can
be generalized in many different directions. Here we will consider a famous
example due to Enrico Fermi, Stanialaw Ulam, and John R. Pasta [100] that
can be viewed as a model for a series of masses coupled to their nearest
neighbors by springs. The original model was obtained as the discretization
of a partial differential equation model of a string—one of the ways that
systems of ordinary differential equations arise in applied mathematics.
Let us consider N identical masses positioned on a line as in Figure 6.4,
and let us suppose that the masses are constrained to move only on this
line. Moreover, let us suppose that the masses are coupled by springs, but
with the first and last masses pinned to fixed positions. If xk denotes the
displacement of the kth mass from its equilibrium position; then, using
Newton’s second law, the equations of motion are given by
mx¨k = F(xk+1 − xk) − F(xk − xk−1), k = 1,...,N − 2
where F(xk+1 − xk) is the force exerted on the kth mass from the right
and −F(xk − xk−1) is the force exerted on the kth mass from the left.
One of the Fermi-Ulam-Pasta models uses the scalar force law
F(z) = α(z + βz2), α> 0, β ≥ 0,6.4 The Fermi-Ulam-Pasta Oscillator 297
to model the restoring force of a nonlinear spring. This choice leads to the
following equations of motion:
mx¨k = α(xk−1−2xk+xk+1)(1+β(xk+1−xk−1)), k = 1,...N −2 (6.88)
where we also impose the boundary conditions
x0(t) ≡ 0, xN−1(t) ≡ 0.
If we set β = 0 in the equations (6.88), then we obtain the linearization of
system (6.88) at the point corresponding to the rest positions of the masses.
The first objective of this section is to determine the normal modes and
the general solution of this linearization.
Let us define the state vector x with components (x1,...,xN−2), and let
us write the system in matrix form
x¨ = c2Qx (6.89)
where c2 = α/m and
Q =
⎛
⎜⎜⎜⎝
−2 100 ··· 0
1 −210 ··· 0
.
.
. ···
0 ··· 1 −2
⎞
⎟⎟⎟⎠ .
Because Q is a real negative definite symmetric matrix, the matrix has a
basis of eigenvectors corresponding to real negative eigenvalues. If v is an
eigenvector corresponding to the eigenvalue λ, then ec
√−λ itv is a solution
of the matrix system. The corresponding normal mode is the family of real
solutions of the form
x(t) = R cos(c
√
−λ t + ρ) v
where R and ρ depend on the initial conditions.
If v = (v1,...,vN−2) is an eigenvector of Q with eigenvalue λ, then
vk−1 − 2vk + vk+1 = λvk, k = 1,...,N − 2.
To solve this linear three-term recurrence, set vk = ak, and note that ak
gives a solution if and only if
a2 − (2 + λ)a +1=0. (6.90)
Also, note that the product of the roots of this equation is unity, and one
root is given by
r = 2 + λ + λ(4 + λ)
2 .298 6. Applications
Thus, using the linearity of the recurrence, the general solution has the
form
vk = μrk + νr−k
where μ and ν are arbitrary scalars. Moreover, in view of the boundary
conditions, v0 = 0 and vN−1 = 0, we must take μ + ν = 0 and rN−1 −
1/rN−1 = 0. In particular, r must satisfy the equation r2(N−1) = 1. Thus,
the possible choices for r are the roots of unity
r	 = eπi	/(N−1),  = 0, 1,..., 2N − 3.
We will show that the r	 for  = 1,...,N −2 correspond to N −2 distinct
eigenvalues of the (N − 2) × (N − 2) matrix Q as follows: The eigenvalue
λ	 = −4 sin2( π
2(N − 1))
corresponding to r	 is obtained by solving equation (6.90) with a = r	; that
is, the equation
e2πi	/(N−1) − (2 + λ)eπi	/(N−1) +1=0.
The remaining choices for r	 of course cannot lead to new eigenvalues. But
to see this directly consider the range of integers  expressed in the form
0, 1,...,N − 2, N − 1,(N − 1) + 1,...,(N − 1) + N − 2
to check that the corresponding r	 are given by
1, r1,...,rN−2, −1, −r1,..., −rN−2,
and the corresponding λ	 are
0, λ1,...,λN−2, −4, λN−2,...,λ1.
Here, the choices r = 1 and r = −1, corresponding to  = 0 and  = N − 1,
give vk ≡ 0. Hence, they do not yield eigenvalues.
The components of the eigenvectors corresponding to the eigenvalue λ	
are given by
vk = μ

eπi	k/(N−1) − e−πi	k/(N−1)
= 2iμ sin( πk
N − 1
)
where μ is a scalar. If μ = 1/(2i), then we have, for each  = 1,...,N − 2,
the associated eigenvector v	 with components
v	
k = sin( πk
N − 1
).6.4 The Fermi-Ulam-Pasta Oscillator 299
Because Q is symmetric, its eigenvectors corresponding to distinct eigen￾values are orthogonal with respect to the usual inner product. Moreover,
we have that
	v	
, v	

 =
N
−2
k=1
sin2( πk
N − 1
) = N − 1
2
where the last equality can be proved by first applying the identity
sin θ = eiθ − e−iθ
2i
and then summing the resulting geometric series. Thus, the vectors
 2
N − 1
1/2
v1,...,  2
N − 1
1/2
vN−2
form an orthonormal basis of RN−2.
The general solution of the system (6.88) with β = 0 is given by the
vector solution t → x(t) with components
xk(t) =  2
N − 1
1/2 N
−2
	=1
(γ	p	(t) + η	q	(t)) sin( πk
N − 1
)
where c2 = α/m,
p	(t) = cos(2ctsin( π
2(N − 1))), q	(t) = sin(2ctsin( π
2(N − 1))),
and γ	, η	 are real constants. In vector form, this solution is given by
x(t) =  2
N − 1
1/2 N
−2
	=1
(γ	p	(t) + η	q	(t))v	
;
it is the solution of the first-order system corresponding to the model (6.88)
with initial value
x(0) =  2
N − 1
1/2 N
−2
	=1
γ	v	
,
x˙(0) = 2c
 2
N − 1
1/2 N
−2
	=1
η	 sin( π
2(N − 1))v	
.
Moreover, let us note that if we use the orthonormality of the normalized
eigenvectors, then the scalars γ	 and η	 can be recovered with the inversion
formulas
γ	 = 	x(0),
 2
N − 1
1/2
v	

,
η	 = 	x˙(0),
 2
N − 1
1/2

2c sin π
2(N − 1)
−1
v	

.300 6. Applications
Now that we have determined the normal modes and the general solution
of the linearized system, let us use them to describe the Fermi-Ulam-Pasta
experiments.
If B is the matrix whose columns are the ordered orthonormal eigenvec￾tors of Q, then the linear coordinate transformation x = By decouples the
system of differential equations (6.88) with β = 0 into a system of the form
y¨k = c2λkyk, k = 1,...,N − 2
where λk is the eigenvalue corresponding to the kth column of B. Note
that the total energy of this kth mode is given by
Ek := 1
2

y˙
2
k − c2λky2
k

,
and this energy can be easily computed from the vector solution x(t) by
using the identity
 2
N − 1
1/2
	x(t), vk
 = yk.
Fermi, Ulam, and Pasta expected that after an initial excitation the aver￾ages over time of the linear mode energies Ek(t) of the nonlinear (β = 0)
oscillator (6.88) would tend to equalize after a sufficiently long time period.
The process leading to this “equipartition of energy” is called thermaliza￾tion. In fact, the purpose of their original experiment—numerical integra￾tion of the system starting with nonzero energy in only one normal mode—
was to determine the length of time required for thermalization to occur.
Contrary to their expectation, the results of the experiment suggested that
thermalization does not occur. For example, for some choices of the sys￾tem parameters, the energy becomes distributed among the various linear
modes for a while, but eventually almost all the energy returns to the ini￾tial mode. Later, most of the energy might be in the second mode before
returning again to the first mode, and so on. For other values of the sys￾tem parameters the recurrence is not as pronounced, but none of their
experiments suggested that thermalization does occur. The explanation for
this “beat phenomenon” and for the nonexistence of thermalization leads to
some very beautiful mathematics and mathematical physics (see the article
by Richard Palais [201]). It is remarkable that the first numerical dynamics
experiments performed on a computer (during 1954–55) turned out to be
so important (see [258]).
Exercise 6.44. Solve the differential equation (6.89) by converting it to a first￾order system and then finding a basis of eigenvectors.
Exercise 6.45. Describe the geometry of the modes of oscillation of the masses
in the Fermi-Ulam-Pasta model with respect to the linearized model (6.89). For
example, is it possible that all the masses move so that the distances between
adjacent masses stay fixed?6.5 The Inverted Pendulum 301
Exercise 6.46. Solve system (6.88) with β = 0 and periodic boundary condi￾tions: x0(t) = xN−1(t).
Exercise 6.47. RepeattheFermi-Ulam-Pastaexperiment.Beginwiththeparam￾eters
N = 32, m = 1.0, α = 1.0, β = 0.25,
and choose an initial condition so that the velocity of each mass is zero and all
the energy is in the first mode; for example, take
xk(0) =  2
N − 1
1/2
sin( πk
N − 1
).
Integrate system (6.88) numerically and output the mode energies for at least
the first few modes. Discuss how the mode energies change over time.
Exercise 6.48. Consider system (6.88) with its first boundary condition replaced
by x0(t) = A sin ωt. What is the general behavior of this system? Note: This
problem is open-ended; it does not seem to have a simple answer.
6.5 The Inverted Pendulum
Consider a pendulum with oscillating vertical displacement. We will outline
a proof of the following amazing fact: The inverted pendulum can be made
stable for certain rapid oscillations of small vertical displacement.
Historical comments and a very interesting description of the stabiliza￾tion phenomenon based on topological methods is given by Mark Levi
(see [160]). David Acheson’s book [4] includes results on the stabilization
of the double pendulum.
The equation of motion for the displaced inverted pendulum is obtained
as a modification of the pendulum model (6.86). For this, let H be a smooth
one-periodic function with unit amplitude where L is the length of the
pendulum and g is the gravitational constant. Let us also incorporate two
control parameters, the amplitude δ and the (relative) frequency Ω of the
vertical displacement, so that the displacement function is given by
t → δH(Ωt).
This displacement may be viewed as an external force with period 1/Ω by
taking the force to be
F := mLΩ2δH(Ωt) sin θ.
An alternative way to view the model is to imagine the pendulum inside
an “Einstein elevator” that is being periodically accelerated. In this case,
the external force is perceived as a time-dependent change in the gravita￾tional field. The new gravitational “constant” measured in some units, say
cm/sec/sec, is given by
g − Ω2δH(Ωt),302 6. Applications
and the equation of motion is obtained by replacing g by this difference in
the model (6.86). The minus sign is not important, it is there to make this
formulation compatible with the Lagrangian formulation. Indeed, with the
choice for the force given above and the Lagrangian (6.85), the Lagrange
equation
d
dt
dL
d ˙
θ − dL
dθ = F
yields the following equation of motion:
¨θ + g
L sin θ = δ
Ω2
L
H(Ωt) sin θ.
Let us rescale time with the change of the variable given by
t = 1
Ωs.
Also, after this change of time, let us use the scaled period and amplitude
of the displacement
α := g
LΩ2 , β := δ
L,
and the function is given by
a(s) := H(s)
to construct the dimensionless equation of motion
θ + (α − βa(s)) sin θ = 0 (6.91)
where the function s → a(s) is periodic with period one.
To study the stability of the rest point at θ = π corresponding to the
inverted pendulum, the equation of motion is linearized at θ = π to obtain
the periodic linear system
w − (α − βa(s))w = 0 (6.92)
with parameters α and β. While we will consider the two-parameter family
of differential equations in the entire parameter plane, only those systems
with parameters in the first quadrant correspond to the physical pendulum.
We will outline a proof of the following proposition:
Proposition 6.49. If a(s) = sin 2πs in the differential equation (6.92),
then the point (α, β) = (0, 0) is a boundary point of an open subset in
the first quadrant of the parameter plane such that the differential equation
corresponding to each point of this open set has a stable trivial solution.6.5 The Inverted Pendulum 303
Stability region
β
α
Figure 6.5: Stabilization region for the inverted pendulum.
The open set mentioned in Proposition 6.49 contains points close to the
origin of the parameter plane that correspond to high-frequency small￾amplitude displacements which stabilize the inverted pendulum. Proposi￾tion 6.49 is true if the normalized displacement a(s) = sin 2πs is replaced
by certain other periodic functions that will also be determined.
According to Proposition 6.49, the (periodic) trivial solution of the lin￾earized pendulum model equation is stable. Because such a periodic solu￾tion is not hyperbolic, we have no method that can be used to prove that the
corresponding rest position of the original nonlinear model of the inverted
pendulum is stable. In fact, the principle of linearized stability gives the
correct insight: the rest position of the original nonlinear model of the
inverted pendulum is stable whenever its linearization is stable. The proof
of this fact seems to require an analysis (KAM theory) that is beyond the
scope of this book (see [160]).
We will use Floquet theory, as in our analysis of Hill’s equation, to prove
Proposition 6.49.
Let Φ(s, α, β) denote the principal fundamental matrix at s = 0 for the
first-order system
w = z, z = (α − βa(s))w (6.93)
corresponding to the differential equation (6.92). Recall from our study
of Hill’s equation that the trivial solution of the system (6.93) is stable
provided that
|tr Φ(1, α, β)| < 2.
If (α, β) = (0, 0), then
Φ(1, 0, 0) = exp 	0 1
0 0

=
	1 1
0 1

.304 6. Applications
At this point of the parameter space tr Φ(1, 0, 0) = 2. Thus, it is reasonable
to look for nearby points where tr Φ(1, 0, 0) < 2. The idea is simple enough:
Under our assumption that H is smooth, so is the function τ : R2 → R
given by (α, β) → tr Φ(1, α, β). We will use the implicit function theorem
to show that the boundary of the region of stability, depicted in Figure 6.5,
is a smooth curve that passes through the origin of the parameter space
and into the positive first quadrant.
To compute the partial derivative τα(0, 0), let A(s) denote the system
matrix for the system (6.93) and use the matrix equation Φ = A(s)Φ to
obtain the variational initial value problem
Φ
α = A(s)Φα + Aα(s)Φ, Φα(0) = 0.
At (α, β) = (0, 0), the variational equation is given by the following (inho￾mogeneous) linear system
W =
	0 1
0 0

W +
	0 0
1 0

Φ(s, 0, 0)
=
	0 1
0 0

W +
	0 0
1 0
 	1 s
0 1

=
	0 1
0 0

W +
	0 0
1 s


.
It can be solved by variation of parameters to obtain
Φα(1, 0, 0) = W(1) = 	 1
2 ∗
∗ 1
2


;
and therefore, τα(0, 0) = 1.
Define the function g(α, β) := τ (α, β) − 2 and note that we now have
g(0, 0) = 0, gα(0, 0) = 1.
By an application of the implicit function theorem, there is a function β →
γ(β), defined for sufficiently small β, such that γ(0) = 0 and τ (γ(β), β) ≡ 2.
To determine which “side” of the curve Γ := {(α, β) : α = γ(β)} cor￾responds to the region of stability, let us consider points on the positive
α-axis. In this case, the linearized equation has constant coefficients:
w − αw = 0.
Its principal fundamental matrix solution at s = 0 evaluated at s = 1 is
given by
	 cosh √α √
1
α sinh √α
√α sinh √α cosh √α


,
and, for α > 0, we have τ (α, 0) = 2 cosh √α > 2.6.5 The Inverted Pendulum 305
By the implicit function theorem, the curve Γ in the parameter space
corresponding to tr Φ(1, α, β) = 2 is unique. Also, by the computation
above, the positive α-axis lies in the unstable region. Because τα(0, 0) = 1,
we must have τα(γ(β), β) > 0 as long as β is sufficiently small. Thus, it
follows that the trace of the monodromy matrix increases through the value
2 as the curve Γ is crossed. In particular, the trace of the monodromy matrix
is less than 2 on the left side of this curve; that is, Γ forms a boundary of
the stable region as depicted in Figure 6.5.
Finally, to determine the conditions on the periodic displacement so that
the restriction of Γ to β > 0 lies in the first quadrant, we will use the
equality
τβ(0, 0) = −
 1
0
a(s) ds
(see Ex. 6.50).
Because the original external excitation of the pendulum is periodic, the
average value of its second derivative (corresponding to the function s →
a(s)) is zero. In this case, we have that τβ(0, 0) = 0, or equivalently, γ
(0) =
0. A portion of the stability region will be as depicted in Figure 6.5 provided
that γ(0) > 0. In this case, if the amplitude, β = δ > 0, of the periodic
perturbation is sufficiently small, then there is a range of sufficiently high
frequencies Ω (recall that α := 1/Ω2) such that the linearized pendulum
motion is stabilized. By differentiation of the implicit relation τ (γ(β), β) =
2, it is easy to see that the required condition on the second derivative of
γ is equivalent to the inequality τββ(0, 0) < 0. Of course, this requirement
is not always satisfied (see Ex. 6.52), but it is satisfied for a(s) = sin(2πs)
(see Ex. 6.51).
Exercise 6.50. Prove that
τβ(0, 0) = −
 1
0
a(s) ds.
Exercise 6.51. Prove that τββ(0, 0) < 0 for the case a(s) = sin(2πs). Hint:
Compute the variational derivatives directly in terms of the second-order equa￾tion (6.92).
Exercise 6.52. Find a condition on the function a(s) so that τββ(0, 0) < 0.
Also, if a(s) is expressed as a convergent Fourier series, find the corresponding
condition in terms of its Fourier coefficients. Hint: If
a(s) = ∞
k=1
ak cos(2πks) + bk sin(2πks),
then
τββ(1, 0, 0) = 2∞
k=1
1
2πk bk
2
−∞
k=1
	 1
2πk

2
(a2
k + 3b
2
k).306 6. Applications
Exercise 6.53. Find an example of a periodic function s → a(s) with period
one such that τββ(0, 0) > 0. For this choice of the displacement, the inverted
pendulum is not stabilized for small β > 0.
Exercise 6.54. What can you say about the stability of the inverted pendulum
using Lyapunov’s theorem (Theorem 5.27) or Exercise 5.33?
Let us consider the stability of the noninverted pendulum. Note that the
linearization of the differential equation (6.91) at θ = 0 is given by
w + (α − βa(s))w = 0,
and let Ψ(s, α, β) denote the principal fundamental matrix solution at s = 0
of the corresponding homogeneous linear system. In this case, we have
tr Ψ(1, α, 0) = 2 cos √α.
Because the boundaries of the regions of instability are given by
|tr Ψ(1, α, β)| = 2,
they intersect the α-axis only if √α is an integer multiple of π. In view of
the equation α = g/(LΩ2), these observations suggest the zero solution is
unstable for small-amplitude displacements whenever there is an integer n
such that the period of the displacement is
1
Ω = n
2

2π
L
g
1/2
;
that is, the period of the displacement is a half-integer multiple of the
natural frequency of the pendulum. In fact, the instability of the pendulum
for a small-amplitude periodic displacement with n = 1 is demonstrated in
every playground by children pumping up swings.
The proof that the instability boundaries do indeed cross the α-axis
at the “resonant” points (α, β) = ((nπ)2, 0), for n = 1,...,∞, is obtained
from an analysis of the Taylor expansion of the function given by Ψ(1, α, β)
at each resonant point (see Ex. 6.55). Typically, the instability regions are
as depicted in Figure 6.6. The instability region with n = 1 is “open” at β =
0 (the tangents to the boundary curves have distinct slopes); the remaining
instability regions are “closed.” It is an interesting mathematical problem
to determine the general shape of the stability regions (see [116], [172], [173],
and [168]).
Exercise 6.55. Suppose that a(s) = sin(2πs) and set
g(α, β) = tr Ψ(1, α, β) − 2.6.6 Origins of ODE: Partial Differential Equations 307
(3π)
2
β
α
(π)
2 (2π)
2
Figure 6.6: Regions of instability (Arnold tongues) for the linearized pen￾dulum.
Show that gα((nπ)
2, 0) = 0 and gβ((nπ)
2, 0) = 0. Thus, the implicit function
theorem cannot be applied directly to obtain the boundaries of the regions of
instability, the boundary curves are singular at the points where they meet the
α-axis. By computing appropriate higher order derivatives and analyzing the
resulting Taylor expansion of g, show that the regions near the α-axis are indeed
as depicted in Figure 6.6. Also, show that the regions become “thinner” as n
increases.
6.6 Origins of ODE: Partial Differential Equations
In this section there is an elementary discussion of three big ideas:
• Certain partial differential equations (PDE) can be viewed as ordi￾nary differential equations with an infinite-dimensional phase space.
• Finite-dimensional approximations of some PDE are systems of ordi￾nary differential equations.
• Traveling wave fronts in PDE can be determined by ordinary differ￾ential equations.
While these ideas are very important and therefore have been widely stud￾ied, only a few elementary illustrations will be given here. The objective of
this section is to introduce these ideas as examples of how ordinary differ￾ential equations arise and to suggest some very important areas for further
study (see [38], [124], [123], [134], [198], [203], [232], [247], and [267]). We
will also discuss the solution of first-order PDE as an application of the
techniques of ordinary differential equations.
Most of the PDE mentioned in this section can be considered as mod￾els of “reaction-diffusion” processes. To see how these models are derived,308 6. Applications
imagine some substance distributed in a medium. The density of the sub￾stance is represented by a function u : Rn × R → R so that (x, t) → u(x, t)
gives its density at the site with coordinate x at time t.
For Ω a region in space with boundary ∂Ω, the rate of change of the
amount of the substance in Ω is given by the flux of the substance through
the boundary of Ω plus the amount of the substance generated in Ω; that
is,
d
dt 
Ω
u dV = −

∂Ω
X · η dS +

Ω
f dV
where X is the vector field representing the motion of the substance; dV
is the volume element; dS is the surface element; the vector field η is the
outer unit normal field on the boundary of Ω; and f, a function of density,
position and time, represents the amount of the substance generated in Ω.
The minus sign on the flux term is required because we are measuring the
rate of change of the amount of substance in Ω. If, for example, the flow
is all out of Ω, then X · η ≥ 0 and the minus sign is required because the
rate of change of the amount of substance in Ω must be negative.
If the divergence theorem is applied to rewrite the flux term and the time
derivative is interchanged with the integral of the density, then

Ω
ut dV = −

Ω
div X dV +

Ω
f dV.
Moreover, because the region Ω is arbitrary in the integral identity, it is
easy to prove the fundamental balance law
ut = − div X + f. (6.94)
To obtain a useful dynamical equation for u from equation (6.94), we
need a constitutive relation between the density u of the substance and
the flow field X. It is not at all clear how to derive this relationship from
the fundamental laws of physics. Thus, we have an excellent example of
an important problem where physical intuition must be used to propose a
constitutive law whose validity can only be tested by comparing the results
of experiments with the predictions of the corresponding model. Problems
of this type lie at the heart of applied mathematics and physics.
For equation (6.94), the classic constitutive relation—called Darcy’s,
Fick’s, or Fourier’s law depending on the physical context—is
X = −K grad u + μV (6.95)
where K ≥ 0 and μ are functions of density, position, and time; and V
denotes the flow field for the medium in which our substance is moving.
The minus sign on the gradient term represents the assumption that the
substance diffuses from higher to lower concentrations.6.6 Origins of ODE: Partial Differential Equations 309
By inserting the relation (6.95) into the balance law (6.94), we obtain
the dynamical equation
ut = div(K grad u) − div(μV ) + f. (6.96)
Also, if we assume that the diffusion coefficient K is equal to k2 for some
constant k, the function μ is given by μ(u, x, t) = γu where γ is a constant,
and V is an incompressible flow field (div V = 0); then we obtain the most
often used reaction-diffusion-convection model equation
ut + γ grad u · V = k2Δu + f. (6.97)
In this equation, the gradient term is called the convection term, the Lapla￾cian term is called the diffusion term, and f is the source term. Let us also
note that if the diffusion coefficient is zero, the convection coefficient is
given by γ = 1, the source function vanishes, and V is not necessarily
incompressible, then the dynamical equation (6.96) reduces to the law of
conservation of mass, also called the continuity equation, given by
ut + div(uV )=0. (6.98)
Because equation (6.97) is derived from general physical principles, this
PDE can be used to model many different phenomena. As a result, there
is a vast scientific literature devoted to its study. We will not be able to
probe very deeply, but we will use equation (6.97) to illustrate a few aspects
of the analysis of these models where ordinary differential equations arise
naturally.
6.6.1 Infinite-Dimensional ODE
A simple special case of the reaction-diffusion-convection model (6.97) is
the linear diffusion equation (the heat equation) in one spatial dimension,
namely, the PDE
ut = k2uxx (6.99)
where k2 is the diffusivity constant. This differential equation can be used
to model heat flow in an insulated bar. In fact, let us suppose that the bar is
idealized to be the interval [0, ] on the x-axis so that u(x, t) represents the
temperature of the bar at the point with coordinate x at time t. Moreover,
because the bar has a finite length, let us model the heat flow at the ends
of the bar where we will consider just two possibilities: The bar is insulated
at both ends such that we have the zero Neumann boundary conditions
ux(0, t)=0, ux(, t) = 0;
or, heat is allowed to flow through the ends of the bar, but the temperature
at the ends is held constant at zero (in some appropriate units) such that
we have the zero Dirichlet boundary conditions
u(0, t)=0, u(, t)=0.310 6. Applications
If one set of boundary conditions is imposed and an initial temperature
distribution, say x → u0(x), is given on the bar, then we would expect
that there is a unique scalar function (x, t) → u(x, t), defined on the set
[0, ] × [0,∞) that satisfies the PDE, the initial condition u(x, 0) = u0(x),
and the boundary conditions. Of course, if such a solution exists, then
for each t > 0, it predicts the corresponding temperature distribution
x → u(x, t) on the bar. In addition, if there is a solution of the boundary
value problem corresponding to each initial temperature distribution, then
we have a situation that is just like the phase flow of an ordinary differential
equation. Indeed, let us consider a linear space E of temperature distribu￾tions on the rod and let us suppose that if a function v : [0, ] → R is in E,
then there is a solution (x, t) → u(x, t) of the boundary value problem with
v as the initial temperature distribution such that x → u(x, t) is a function
in E whenever t > 0. In particular, all the functions in E must satisfy the
boundary conditions. If this is the case, then we have defined a function
(0,∞) ×E → E given by (t, v) → ϕt(v) such that ϕ0(v)(x) = v(x) and
(x, t) → ϕt(v)(x) is the solution of the boundary value problem with initial
temperature distribution v. In other words, we have defined a dynamical
system with (semi) flow ϕt whose phase space is the function space E of
possible temperature distributions on the bar. For example, for the Dirich￾let problem, we might take E to be the subset of C2[0, ] consisting of those
functions that vanish at the ends of the interval [0, ].
Taking our idea a step further, let us define the linear transformation A
on E by
Au = k2uxx.
Then, the PDE (6.99) can be rewritten as
u˙ = Au, (6.100)
an ordinary differential equation on the infinite-dimensional space E. Also,
to remind ourselves of the boundary conditions, let us write A = AN if zero
Neumann boundary conditions are imposed and A = AD for zero Dirichlet
boundary conditions.
Although the linear homogeneous differential equation (6.100) is so sim￾ple that its solutions can be given explicitly, we will see how the general
solution of the PDE can be found by treating it as an ordinary differential
equation.
Let us begin by determining the rest points of the system (6.100). In
fact, a rest point is a function v : [0, ] → R that satisfies the boundary
conditions and the second-order ordinary differential equation vxx = 0.
Clearly, the only possible choices are affine functions of the form v = cx+d
where c and d are real numbers. There are two cases: For AN we must
have c = 0, but d is a free variable. Thus, there is a line in the function
space E corresponding to the constant functions in E that consists entirely6.6 Origins of ODE: Partial Differential Equations 311
of rest points. For the Dirichlet case, both c and d must vanish and there
is a unique rest point at the origin of the phase space.
Having found the rest points for the differential equation (6.100), let us
discuss their stability. By analogy with the finite-dimensional case, let us
recall that we have discussed two methods that can be used to determine
the stability of rest points: linearization and Lyapunov’s direct method.
In particular, for the finite-dimensional case, the method of linearization is
valid as long as the rest point is hyperbolic, and, in this case, the eigenvalues
of the system matrix for the linearized system at the rest point determine
its stability type.
Working formally, let us apply the method of linearization at the rest
points of the system (6.100). Since this differential equation is already lin￾ear, we might expect the stability of these rest points to be determined from
an analysis of the position in the complex plane of the eigenvalues of the
system operator A. By definition, if λ is an eigenvalue of the operator AD
or AN , then there must be a nonzero function v on the interval [0, ] that
satisfies the boundary conditions and the ordinary differential equation
k2vxx = λv.
If v is an eigenfunction with eigenvalue λ, then we have that
 	
0
k2vxxv dx =
 	
0
λv2 dx. (6.101)
Let us suppose that v is square integrable, that is,
 	
0
v2 dx < ∞
and also smooth enough so that integration by parts is valid. Then, equa￾tion (6.101) is equivalent to the equation
vxv



	
0
−
 	
0
v2
x dx = λ
k2
 	
0
v2 dx.
Thus, if either Dirichlet or Neumann boundary conditions are imposed,
then the boundary term from the integration by parts vanishes, and there￾fore the eigenvalue λ must be a nonpositive real number.
For AD, if λ = 0, then there is no nonzero eigenfunction. If λ < 0, then
the eigenvalue equation has the general solution
v(x) = c1 cos αx + c2 sin αx
where α := (−λ)1/2/k and c1 and c2 are constants; and, in order to satisfy
the Dirichlet boundary conditions, we must have
	 1 0
cos α sin α
 	c1
c2


=
	0
0

312 6. Applications
for some nonzero vector of constants. In fact, the determinant of the matrix
must vanish, and we therefore have to impose the condition that α is an
integer multiple of π; or equivalently,
λ = −
nπk

2
with a corresponding eigenfunction given by
x → sin nπ
 x
for each integer n = 1, 2,...,∞. By a similar calculation for AN , we have
that λ = 0 is an eigenvalue with a corresponding eigenfunction v ≡ 1, and
again the same real numbers
λ = −
nπk

2
are eigenvalues, but this time with corresponding eigenfunctions
x → cos
nπ
 x.
The nature of the real parts of the eigenvalues computed in the last para￾graph and the principle of linearized stability suggest that the origin is an
asymptotically stable rest point for the Dirichlet problem. On the other
hand, the rest points of the Neumann problem seem to be of a different
type: each of these rest points would appear to have a one-dimensional
center manifold and an infinite-dimensional stable manifold. All of these
statements are true. But to prove them, certain modifications of the corre￾sponding finite-dimensional results are required. For example, the principle
of linearized stability is valid for rest points of infinite-dimensional ODE
under the assumption that all points in the spectrum of the operator given
by the linearized vector field at the rest point (in our case the operator A)
have negative real parts that are bounded away from the imaginary axis
in the complex plane (see, for example, [232, p. 114]). More precisely, the
required hypothesis is that there is some number α > 0 such that the real
part of every point in the spectrum of the operator is less than −α. In gen￾eral, the principle of linearized stability fails for rest points of differential
equations in infinite-dimensional spaces (see Ex. 6.57).
Recall that a complex number λ is in the spectrum of the linear operator
A if the operator A − λI does not have a bounded inverse. Of course, if
v = 0 is an eigenfunction with eigenvalue λ, then the operator A−λI is not
injective and indeed λ is in the spectrum. In a finite-dimensional space, if an
operator is injective, then it is invertible. Hence, the only complex numbers
in the spectrum of a finite-dimensional linear operator are eigenvalues. But,
in an infinite-dimensional space, there can be points in the spectrum that6.6 Origins of ODE: Partial Differential Equations 313
are not eigenvalues (see [88]). For example, let us define the space L2(0, )
to be all (real) functions v : [0, ] → R such that
 	
0
v2(x) dx < ∞ (6.102)
where we consider two such functions v and w to be equal if
 	
0
(v(x) − w(x))2 dx = 0,
and consider the operator B : L2 → L2 given by (Bf)(x) → xf(x). This
operator has no eigenvalues, yet the entire interval [0, ] is in its spectrum.
(Why?)
The operators AD and AN , considered as operators defined in L2(0, ),
have spectra that consist entirely of eigenvalues (pure point spectrum). To
prove this claim, note first that these operators are not defined on all of
L2. After all, a square integrable function does not have to be differen￾tiable. Instead, we can view our operators to be defined on the subset of
L2 consisting of those functions that have two derivatives both contained
in L2. Then, the claim about the spectra of AD and AN can be proved in
two steps. First, if a complex number λ is not an eigenvalue, then for all
w ∈ L2 there is some function v that satisfies the boundary conditions and
the differential equation
k2vxx − λv = w.
In other words, there is an operator B : L2 → L2 given by Bw = v such
that (A − λI)Bw = w. The boundedness of B is proved from the explicit
construction of B as an integral operator. Also, it can be proved that
B(A−λI)v = v for all v in the domain of A (see Ex. 6.56). Using these facts
and the theorem on linearized stability mentioned above, it follows that the
origin is an asymptotically stable rest point for the Dirichlet problem.
Exercise 6.56. Show that the spectrum of the operator in L2(0, ) given by
Av = vxx with either Dirichlet or Neumann boundary conditions consists only of
eigenvalues. Prove the same result for the operator Av = avxx + bvx + cv where
a, b, and c are real numbers.
Exercise 6.57. This exercise discusses an example (a slight modification of an
example in [52, p. 32]) of an infinite-dimensional linear differential equation such
that the spectrum of the system matrix is in the left half-plane and the trivial
solution is unstable. It requires some Hilbert space theory. For each n ≥ 1 let
An denote the n × n-matrix with ai,i+1 = 1 and all other components zero (that
is, the super diagonal is all ones, every other component is zero). Let H denote
the (complex) Hilbert space X = ⊕n≥1Cn with the 2-norm, and define the
operator A in H by Ax = {Anxn + 2πinxn}∞n=1. (a) Prove that An is nilpotent
and the spectrum of An is {0}. (b) Prove that A is an unbounded operator and314 6. Applications
it can be densely defined in H. (c) Prove that {2πin : n ≥ 1} are eigenvalues
of A. (d) Prove that the differential equation ˙x = (A − 1
2 I)x on H has the
solution Tt
x = {e−t/2e2πintetAn }∞n=1, which is a semi-group defined for t ≥ 0.
(e) Prove that the spectrum of A − 1
2 I lies in the left half-plane. Hint: Show
that the resolvent of A (that is, R(A, λ)=(A − λI)
−1) is a bounded operator
whenever the real part of λ is greater than zero using the following fact: The
resolvent of An can be represented as a finite sum. (f) Let pn := (1, 1,..., 1)/
√n ∈
Cn and let xn denote the element of H such that the nth component of xn is
pn and all other components are zero. Prove that xn = 1. (g) Prove that
limn→∞ |etAn pn − et
pn| = 0 for each fixed t ≥ 0. (h) Prove that Tt is not stable.
Hint: It suffices to show that, for the semi-group St generated by A, if C > 1 is
given, there is an element x ∈ H such that x = 1 and St
x ≥ Cet/2. Prove
this inequality using part (g).
In view of our results for finite-dimensional linear systems, we expect
that if we have a linear evolution equation ˙v = Av, even in an infinite￾dimensional phase space, and if Aw = λw, then etλw is a solution. This
is indeed the case for the PDE (6.99). Moreover, for linear evolution equa￾tions, we can use the principle of superposition to deduce that every lin￾ear combination of solutions of this type is again a solution. If we work
formally, that is, without proving convergence, and if we use the eigenval￾ues and eigenvectors computed above, then the “general solution” of the
Dirichlet problem is given by
u(x, t) = ∞
n=1
e−( nπk
 )2t
an sin nπ
 x,
and the general solution of the Neumann problem is given by
u(x, t) = ∞
n=0
e−( nπk
 )2t
bn cos
nπ
 x
where an and bn are real numbers.
If the initial condition u(x, 0) = u0(x) is given, then, for instance, for the
Dirichlet problem we must have that
u0(x) = ∞
n=1
an sin nπ
 x.
In other words, the initial function u0 must be represented by a Fourier
sine series. What does this mean? The requirement is that the Fourier sine
series converges to u0 in the space L2 endowed with its natural norm,
v :=   	
0
v2(x) dx1/2
.6.6 Origins of ODE: Partial Differential Equations 315
In fact, the inner product space L2 is a Hilbert space; that is, with respect
to this norm, every Cauchy sequence in L2 converges (see [223]). The precise
requirement for u0 to be represented by a Fourier sine series is that there
are real numbers an and corresponding L2 partial sums

N
n=1
an sin nπ
 x
such that
lim
N→∞ u0 − uN  = 0.
If the initial function u0 is continuous, then for our special case the cor￾responding solution obtained by Fourier series also converges pointwise to
a C2 function that satisfies the PDE in the classical sense. We will show in
a moment that this solution is unique, and therefore the special solutions
of the PDE obtained from the eigenvalues and corresponding eigenfunc￾tions do indeed form a fundamental set of solutions for our boundary value
problems.
There are several ways to prove that solutions of the diffusion equation
with a given initial condition are unique. We will use the “energy method”;
an alternative uniqueness proof is based on the maximum principle (see
Ex. 6.58). To show the uniqueness result, let us note that if two solutions
of either the Dirichlet or Neumann boundary value problem satisfy the
same initial condition, then the difference u of these two solutions is a
solution of the same boundary value problem but with initial value of the
zero function. Using an integration by parts, we also have the equality
d
dt  	
0
1
2
u2 dx =
 	
0
utu dx = k2
 	
0
uxxu dx = −k2
 	
0
u2
x dx.
It follows that the function
t →
 	
0
1
2
u2(x, t) dx
is not increasing, and therefore it is bounded above by its value at t = 0,
namely,
 	
0
1
2
u2(x, 0) dx = 0.
The conclusion is that u(x, t) ≡ 0, as required.
Exercise 6.58. Prove the maximum principle: If ut(x, t) = k2uxx(x, t) is a C2
function on the open rectangle (0, ) × (0, T) and a continuous function on the
closure of this rectangle, then the maximum of the function u is assumed either
on the line (0, ) × {0} or on one of the lines
{0} × [0, T], {} × [0, T].316 6. Applications
Also, use the maximum principle to prove the uniqueness of solutions of the
boundary value problem with initial condition for the diffusion equation. Hint:
Use calculus (see [243, p. 41]).
Exercise 6.59. Solve the PDE (6.99) by the method of separation of variables;
that is, assume that there is a solution of the form u(x, t) = p(x)q(t), substitute
this expression into the PDE, impose the boundary conditions, and determine
the general form of the functions p and q.
Using the explicit form of the Fourier series representations of the general
solutions of the heat equation with Dirichlet or Neumann boundary condi￾tions, we can see that these solutions are very much like the solutions of a
homogeneous linear ordinary differential equation: They are expressed as
superpositions of fundamental solutions and they obviously satisfy the flow
property ϕs(ϕt(v)) = ϕs+t(v) as long as s and t are not negative (the series
solutions do not necessarily converge for t < 0). Because of this restriction
on the time variable, the solutions of our evolution equation are said to be
semi-flows or semi-groups.
In the case of Dirichlet boundary conditions, if we look at the series
solution, then we can see immediately that the origin is in fact globally
asymptotically stable. For the Neumann problem there is a one-dimensional
invariant manifold of rest points, and all other solutions are attracted expo￾nentially fast to this manifold. Physically, if the temperature is held fixed
at zero at the ends of the bar, then the temperature at each point of the bar
approaches zero at an exponential rate, whereas if the bar is insulated at
its ends, then the temperature at each point approaches the average value
of the initial temperature distribution.
Our discussion of the scalar diffusion equation, PDE (6.99), has served
to illustrate the view that a (parabolic) PDE is an ordinary differential
equation on an infinite-dimensional space. Moreover, as we have seen, if
we choose to study a PDE from this viewpoint, then our experience with
ordinary differential equations can be used to advantage as a faithful guide
to its analysis.
Exercise 6.60. Verify the semi-flow property ϕs(ϕt(v)) = ϕs+t(v) for the solu￾tions of the scalar heat equation with Dirichlet or Neumann boundary conditions.
Generalize this result to the equation ut = uxx +f(u) under the assumption that
every initial value problem for this equation has a local solution. Hint: How is
the flow property proved for finite-dimensional autonomous equations?
Let us now consider the nonlinear PDE
ut = k2uxx + f(u, x, t), 0 < x < , t > 0 (6.103)
where f is a smooth function that represents a heat source in our heat
conduction model.6.6 Origins of ODE: Partial Differential Equations 317
To illustrate the analysis of rest points for a nonlinear PDE, let us assume
that the source term f for the PDE (6.103) depends only on its first argu￾ment, and let us impose, as usual, either Dirichlet or Neumann boundary
conditions. In this situation, the rest points are given by those solutions of
the ordinary differential equation
k2uxx + f(u) = 0 (6.104)
that also satisfy the Dirichlet or Neumann boundary conditions.
The boundary value problem (6.104) is an interesting problem in ordinary
differential equations. Let us note first that if we view the independent
variable as “time,” then the second-order differential equation (6.104) is
just Newton’s equation for a particle of mass k2 moving in a potential force
field with force −f(u). In addition, the corresponding first-order system in
the phase plane is the Hamiltonian system
u˙ = v, v˙ = −f(u)
whose total energy is given by
H(u, v) := k2
2
v2 + F(u)
where F, the potential energy, can be taken to be
F(u) :=  u
0
f(w) dw,
and, as we know, the phase plane orbits all lie on curves of constant energy.
We will use these facts below.
A rest point of the PDE (6.103) with our special form of f and Dirichlet
boundary conditions corresponds to a trajectory in the phase plane that
starts on the v-axis and returns to the v-axis again exactly at time x = .
On the other hand, a rest point for the PDE with Neumann boundary
conditions corresponds to a trajectory in the phase plane that starts on
the u-axis and returns to the u-axis at time x = .
Though the nonlinear boundary value problems that have just been
described are very difficult in general, they can be “solved” in some impor￾tant special cases. As an example, let us consider the following Dirichlet
boundary value problem
ut = uxx + u − u3, u(0, t)=0, u(, t) = 0 (6.105)
(see Ex. 6.63 for Neumann boundary conditions). Note first that the con￾stant functions with values 0 or ±1 are solutions of the differential equation
uxx +u−u3 = 0, but only the zero solution satisfies the Dirichlet boundary
conditions. Thus, there is exactly one constant rest point. Let us determine
if there are any nonconstant rest points.318 6. Applications
v
u
Figure 6.7: Phase portrait of the system ˙u = v v˙ = −u + u3.
The phase plane system corresponding to the steady state equation for
the PDE (6.105) is given by
u˙ = v, v˙ = −u + u3.
It has saddle points at (±1, 0) and a center at (0, 0). Moreover, the period
annulus surrounding the origin is bounded by a pair of heteroclinic orbits
that lie on the curve
1
2
v2 +
1
2
u2 − 1
4
u4 = 1
4
(see Figure 6.7). Using this fact, it is easy to see that the interval (0, 1/
√2)
on the v-axis is a Poincar´e section for the annulus of periodic orbits. Also, a
glance at the phase portrait of the system shows that only the solutions that
lie on these periodic orbits are candidates for nonconstant steady states for
the PDE; they are the only periodic orbits in the phase plane that meet the
v-axis at more than one point. Also, let us notice that the phase portrait
is symmetric with respect to each of the coordinate axes. In view of this
symmetry, if we define the period function
T :

0, 1
√2

→ R (6.106)
so that T(a) is the minimum period of the periodic solution starting at
u(0) = 0, v(0) = a, then
u(
1
2
T(a)) = 0, v(
1
2
T(a)) = −a.
Hence, solutions of our boundary value problem that correspond to rest
points for the PDE also correspond to periodic solutions whose half-periods
are exactly some integer submultiple of ; equivalently, these solutions cor￾respond to those real numbers a such that 0 <a< 1/
√2 and T(a)=2/n6.6 Origins of ODE: Partial Differential Equations 319
for some positive integer n. In fact, each such a corresponds to exactly
two rest points of the PDE; namely, x → u(x) and x → u( − x) where
x → (u(x), v(x)) is the phase trajectory such that u(0) = 0 and v(0) = a.
The number and position in the phase plane of all rest point solutions
of the PDE can be determined from the following three propositions: (i)
T(a) → 2π as a → 0+; (ii) T(a) → ∞ as a → (1/
√2)−; and (iii) T
(a) > 0
(see Ex. 6.61). Using these facts, it follows that there is at most a finite
number of rest points that correspond to the integers 1, 2,...,n such that
n < /π.
Exercise 6.61. Prove that the period function T given in display (6.106) has a
positive first derivative. Hint: Find the explicit time-dependent periodic solutions
of the first-order system ˙u = v, ˙v = −u + u3 using Jacobi elliptic functions. For
a different method, see [40] and [220].
Exercise 6.62. Find the rest points for the Dirichlet boundary value problem
ut = uxx + au − bu2
, u(x, 0) = 0, u(x, )=0
(see [48]).
Are the rest points of the PDE (6.105) stable? It turns out that the
stability problem for nonconstant rest points, even for our scalar PDE, is
too difficult to describe here (see [232, p. 530]). On the other hand, we can
say something about the stability of the constant rest point at the origin
for the PDE (6.105). In fact, let us note that if <π, then it is the only
rest point. Moreover, its stability can be determined by linearization.
Let us first describe the linearization procedure for a PDE. The correct
formulation is simple if we view the PDE as an ordinary differential equa￾tion on a function space. Indeed, we can just follow the recipe for linearizing
an ordinary differential equation of the form ˙u = g(u). Let us recall that if
z is a rest point and g is a smooth function, then the linearization of the
ordinary differential equation at z is
x˙ = Dg(z)(x − z),
or equivalently
w˙ = Dg(z)w
where w := x − z. Moreover, if the eigenvalues of Dg(z) all have negative
real parts, then the rest point z is asymptotically stable (see Section 4).
In order to linearize at a rest point of a PDE, let us suppose that the
function x → z(x) is a rest point for the PDE
ut = g(u)
where g(u) := uxx + f(u) and f : R → R is a differentiable function. If the
domain of AD is viewed as the function space C2[0, ], then the function320 6. Applications
g : C2[0, ] → C0[0, ] is differentiable. This statement follows because the
function u → uxx is linear and the function f is smooth. But there is a
subtle point: in the definition of g we must view the notation f(u) to mean
f ◦u where u ∈ C2[0, ]. The difficulty is that the smoothness of the function
u → f ◦u depends on the topology of the function space to which u belongs
(see Example 1.236).
Once we know that g is differentiable, its derivative can be easily com￾puted as a directional derivative; in fact,
Dg(z)v = d
dt g(z + tv)



t=0
= vxx + Df(z)v.
Therefore, by definition, the linearized equation at the rest point z is given
by
w˙ = wxx + Df(z(x))w. (6.107)
For a nonconstant rest point, the linearized equation (6.107) depends on
the space variable x. The determination of stability in this case is often
quite difficult—recall the stability analysis for periodic solutions of finite￾dimensional ordinary differential equations. For a constant rest point, the
linearized equation has the form ˙w = Aw where A is the linear operator
given by w → wxx+Df(z)w for z a fixed number. In this case, as mentioned
previously, it seems natural to expect the following result: If the spectrum
of A is in the open left half-plane and bounded away from the imaginary
axis, then the rest point is asymptotically stable. In fact, this result, when
properly interpreted, is true for the PDE (6.105). But to prove it, we have
to specify the function space on which the spectrum is to be computed
and recast the arguments used for ordinary differential equations in an
infinite-dimensional setting. For the PDE (6.105) the idea—derived from
our study of ordinary differential equations—of applying the principle of
linearized stability is justified, but some functional analysis is required to
carry it out (see [232, Chapter 11]).
Our example is perhaps too simple; there are PDE where the linearized
stability of a steady state can be easily proved, but the stability of the rest
point is an open question. The problem for a general PDE of the form
ut = Au + f(u)
is that the linear operator A, the function f, and the linearized operator
A+Df(z) must all satisfy additional hypotheses before the ODE arguments
for the validity of the principle of linearized stability can be verified in the
infinite-dimensional case. This fact is an important difference between the
theory of ordinary differential equations and the theory of PDE.
Let us put aside the theoretical justification of linearized stability and
reconsider the rest point at the origin for the PDE (6.105) where the lin￾earized system is given by
wt = wxx + w, w(0) = 0, w()=0.6.6 Origins of ODE: Partial Differential Equations 321
In this case, the spectrum of the differential operator defined by
Aw = wxx + w
consists only of eigenvalues (see Ex. 6.56). In fact, using the analysis of the
spectrum of the operator w → wxx given above, the spectrum of A is easily
obtained by a translation. In fact, the spectrum is

1 −
nπ

2
: n = 1, 2,...,∞

.
Because
1 −
nπ

2
≤ 1 −
π

2
,
the spectrum of A lies in the left half of the complex plane and is bounded
away from the imaginary axis if and only if 1 < π2/2. Hence, using this
fact and assuming the validity of the principle of linearized stability, we
have the following proposition: If <π, then the origin is the only steady
state and it is asymptotically stable.
Let us go one step further in our qualitative analysis of the PDE ut =
uxx + f(u) by showing that there are no periodic solutions. In fact, this
claim is true independent of the choice of  > 0 and for an arbitrary smooth
source function f. The idea for the proof, following the presentation in [232],
is to show that there is a function (essentially a Lyapunov function) that
decreases on orbits. In fact, let us define
E(u) = −
 	
0
1
2
u(x)uxx(x) + F(u(x))
dx
where F is an antiderivative of f and note that
E˙ = −
 	
0
1
2
utuxx +
1
2
uutxx + f(u)ut

dx.
After integration by parts twice for the integral of the second term in
the integrand, and after imposing either Dirichlet or Neumann boundary
conditions, it follows that
E˙ = −
 	
0
(uxx + f(u))ut dx = −
 	
0
(uxx + f(u))2 dx.
Hence, except for the rest points, the time derivative of E is negative along
orbits. In particular, there are no periodic orbits. Can the function E be
used to give a proof of the stability of the rest point at the origin?
For the PDE (6.105) with <π we have now built up a rather complete
picture of the phase portrait. In fact, we know enough to conjecture that
there is a unique rest point that is globally asymptotically stable. Is this
conjecture true?322 6. Applications
Exercise 6.63. Analyze the existence of rest points, the stability types of con￾stant rest points, and the phase portrait for the Neumann boundary value prob￾lem
ut = uxx + u − u3
, ux(0, t)=0, ux(, t)=0.
Note that there are three constant rest points. Use equation (6.107) to determine
their stability types.
6.6.2 Gal¨erkin Approximation
Since most differential equations, ODE or PDE, cannot be solved, it is nat￾ural to seek approximate solutions. For example, numerical methods are
often used to obtain approximate values of state variables. But the utility
of approximation methods goes far beyond number crunching: approxima￾tions are used to gain insight into the qualitative behavior of dynamical
systems, to test computer codes, and to obtain existence proofs. Indeed,
approximation methods are central elements of applied mathematics. In
this section we will take a brief look at a special case of Gal¨erkin’s method,
one of the classic approximation methods for PDE. It is one of an array of
methods that are based on the idea of finding finite-dimensional approxi￾mations of infinite-dimensional dynamical systems.
As a remark, let us note that other approximation methods for PDE are
based on the idea of finding finite-dimensional invariant (or approximately
invariant) submanifolds in the infinite-dimensional phase space. Recall that
rest points and periodic orbits are finite-dimensional invariant submani￾folds. But these are only the simplest examples. In fact, let us note that
a rest point or a periodic orbit might have an infinite-dimensional stable
manifold and a finite-dimensional center manifold. In this case, the local
dynamical behavior is determined by the dynamics on the center manifold
because nearby orbits are attracted to the center manifold. An important
generalization of this basic situation is the concept of an inertial manifold.
By definition, an inertial manifold M is a finite-dimensional submanifold
in the phase space that has two properties: M is positively invariant, and
every solution is attracted to M at an exponential rate (see [247]).
In general, if there is an attracting finite-dimensional invariant mani￾fold, then the dynamical system restricted to this invariant set is an ordi￾nary differential equation that models the asymptotic behavior of the full
infinite-dimensional PDE. In particular, the ω-limit set of every solution
lies on this manifold. Thus, the existence of such an invariant manifold
provides the theoretical basis for a complete understanding of the infinite￾dimensional dynamical system using the techniques of ordinary differential
equations. Unfortunately, it is usually very difficult to prove the existence of
attracting invariant manifolds. Even if an invariant manifold does exist, it is
often equally difficult to obtain a specification of the manifold that would
be required to reduce the original infinite-dimensional dynamical system
to an ordinary differential equation. As an alternative, an approximation6.6 Origins of ODE: Partial Differential Equations 323
method—such as Gal¨erkin’s method—that does not require the existence
of an invariant manifold can often be employed with great success.
The following philosophical question seems to accompany all theoreti￾cal approximation methods for PDE “Is the set of reduced equations—
presumably a system of nonlinear ordinary differential equations—easier
to analyze than the original PDE?” In general, the answer to this ques￾tion is clearly “no.” If, however, the finite-dimensional approximation is
low dimensional or of some special form, then often qualitative analysis
is possible, and useful insights into the dynamics of the original system
can be obtained. Perhaps the best answer to the question is to avoid the
implied choice between infinite-dimensional and finite-dimensional analysis.
The best approach to an applied problem is with a mind free of prejudice.
Often several different methods, including physical thinking and numerical
analysis, are required to obtain consistent and useful predictions from a
model.
Let us begin our discussion of the Gal¨erkin approximation method with
an elementary, but key idea. Recall that a (real) vector space H is an inner
product space if there is a bilinear form (denoted here by angle brackets)
such that if h ∈ H, then 	h, h
 ≥ 0 and 	h, h
 = 0 if and only if h = 0.
It follows immediately that if v ∈ H and 	v, h
 = 0 for all h ∈ H, then
v = 0. We will use this fundamental result to solve equations in H. Indeed,
suppose that we wish to find a solution of the (linear) equation
Au = b. (6.108)
If there is a vector u0 ∈ H such that 	Au0 − b, h
 = 0 for all h ∈ H, then
u0 is a solution of the equation.
Definition 6.64. Suppose that S is a subspace of the Hilbert space H.
A Gal¨erkin approximation of a solution of equation (6.108) is an element
uS ∈ S such that
	AuS − b, s
 = 0
for all s ∈ S.
Of course, every h ∈ H is an approximation of a solution. To obtain
a useful approximation, we will consider a sequence of subspaces, S1 ⊂
S2 ⊂··· , whose union is dense in H together with corresponding Gal¨erkin
approximations un ∈ Sn such that 	Aun − b, s
 = 0 for all s ∈ Sn. In this
case, we might expect that the sequence u1, u2,... converges to a solution
of the equation (6.108).
If H is a finite-dimensional inner product space and the subspaces
S1 ⊂ S2 ⊂ S3 ⊂···⊂ H
are strictly nested, then a corresponding sequence of Gal¨erkin approxima￾tions is finite. Thus, we do not have to worry about convergence. But, if H324 6. Applications
is an infinite-dimensional Hilbert space, then the approximating subspaces
must be chosen with care in order to ensure the convergence of the sequence
of Gal¨erkin approximations.
Let us recall that a sequence B = {νi}∞
i=1 of linearly independent ele￾ments in H is called a Hilbert space basis if the linear manifold S spanned
by B—all finite linear combinations of elements in B—is dense in H; that
is, if h ∈ H, then there is a sequence in S that converges to h in the natural
norm defined from the inner product. A Hilbert space that has such a basis
is called separable.
Gal¨erkin’s principle. Suppose that H is a Hilbert space, B = {νi}∞
i=1 is
a Hilbert space basis for H, and A : H → H is a linear operator. Also,
for each positive integer n, let Sn denote the linear manifold spanned by
the finite set {ν1,...,νn}. Then, for each positive integer n, there is some
un ∈ Sn such that 	Aun − b, s
 = 0 for all s ∈ Sn. Moreover, the sequence
{un}∞
n=1 converges to a solution of the equation Au = b.
The Gal¨erkin principle is not a theorem. In fact, the Gal¨erkin approxima￾tions may not exist or the sequence of approximations may not converge.
The applicability of the method depends on the equation we propose to
solve, the choice of the space H, and the choice of the basis B.
Existence of Weak Solutions
Let us consider the steady state equation
−uxx + g(x)u − f(x)=0, 0 < x < , (6.109)
with either Dirichlet or Neumann boundary conditions where f and g are
smooth functions defined on the interval [0, ]. The basic idea is to look for
a weak solution. To see what this means, note that if u is a solution of the
differential equation (6.109), then
 	
0
(−uxx + gu − f)φ dx = 0 (6.110)
whenever φ is a square integrable function defined on [0, ]. In the Hilbert
space L2(0, ) (see display (6.102)), the inner product of two functions v
and w is
	v, w
 :=  	
0
v(x)w(x) dx.
Therefore, if u is a solution of the equation (6.109), then equation (6.110)
merely states that the inner product of φ with the zero function in L2
vanishes. Moreover, if we define the operator Au = −uxx + gu and the
function b = f, then 	Au − b, φ
 = 0 whenever φ is in the Hilbert space
L2(0, ). Turning this analysis around, we can look for a function u such6.6 Origins of ODE: Partial Differential Equations 325
that 	Au − b, φ
 = 0 for all φ in L2. In this case u is called a weak solution
of the PDE.
Although L2 is a natural Hilbert spaces of functions, the following prob￾lem arises if we try to apply the Gal¨erkin method in L2 to the PDE (6.109):
the elements in L2 are not all differentiable; therefore, the operator A is
not defined on all of L2(0, ).
In which Hilbert space should we look for a solution? By asking this
question, we free ourselves from the search for a classical or strong solution
of the PDE (6.109), that is, a twice continuously differentiable function
that satisfies the PDE and the boundary conditions. Instead, we will seek
a weak solution by constructing a Hilbert space H whose elements are in
L2 such that a Gal¨erkin formulation of our partial differential equation
makes sense in H. If our boundary value problem has a classical solution,
and we choose the Hilbert space H as well as the Gal¨erkin formulation
appropriately, then the L2 equivalence class of the classical solution will
also be in H. Moreover, if we are fortunate, then the weak solution of the
boundary value problem obtained by applying the Gal¨erkin principle in H
will be exactly the equivalence class of the classical solution.
To construct the appropriate Hilbert space of candidate solutions for the
equation (6.110), let us first formally apply the fundamental method for
PDE (that is, integration by parts) to obtain the identity
 	
0
(−uxx + gu − f)φ dx =
 	
0
(uxφx + guφ − fφ) dx − uxφ



	
0
. (6.111)
If the functions φ and u are sufficiently smooth so that the integration by
parts is valid, then equation (6.110) is equivalent to an equation involving
only first derivatives with respect to the variable x, namely, the equation
 	
0
(uxφx + guφ) dx − uxφ



	
0
=
 	
0
fφ dx. (6.112)
In other words, to use equation (6.112) as a Gal¨erkin formulation of our
boundary value problem, we must define a Hilbert space H whose elements
have one derivative with respect to x in L2. Moreover, suppose that such
a Hilbert space H exists. If we find a function u ∈ H such that equa￾tion (6.112) holds for all φ ∈ H and u happens to be smooth, then the
integration by parts is valid and we also have a solution of equation (6.110)
for all smooth functions φ. Using this fact, it is easy to prove that u satisfies
the PDE (6.109) pointwise, that is, u is a classical solution (see Ex. 6.65).
Exercise 6.65. Suppose that u is a C2 function. If equation (6.110) holds for
every φ ∈ C∞, then prove that −uxx + g(x)u − f(x) = 0.
If Dirichlet boundary conditions are imposed, then the boundary condi￾tions must be built into the Hilbert space of test functions from which we326 6. Applications
select φ. In other words, we must impose the condition that the test func￾tions satisfy the Dirichlet boundary conditions. The appropriate Hilbert
space H1
D(0, ) is defined to be the completion with respect to the Sobolev
norm of the set of smooth functions on [0, ] that satisfy the Dirichlet
boundary conditions. Here, the Sobolev norm of a function φ is given by
φ1 :=   	
0
φ2(x) dx +
 	
0
φ2
x(x) dx1/2
.
The Sobolev space H1
D(0, ) is a Hilbert space with respect to the inner
product
	φ, ψ
1 =
 	
0
φψ dx +
 	
0
φxψx dx.
Informally, H1
D(0, ) is the space of functions that satisfy the Dirichlet
boundary conditions and have one derivative in L2.
We have the following Gal¨erkin or weak formulation of our Dirichlet
boundary value problem: Find u ∈ H1
D(0, ) such that
(u, φ) :=  	
0
(uxφx + guφ) dx =
 	
0
fφ dx = 	f,φ
 (6.113)
for all φ ∈ H1
D(0, ). (Note: In equation (6.113) the inner product 	f,φ
 is
the L2 inner product, not the H1 inner product.) If u is a weak solution
of the Dirichlet boundary value problem, then, using the definition of the
Sobolev space, we can be sure that u is the limit of smooth functions that
satisfy the boundary conditions. Of course, u itself is only defined abstractly
as an equivalence class, thus it only satisfies the boundary conditions in
the generalized sense, that is, u is the limit of a sequence of functions that
satisfy the boundary conditions.
For the Neumann boundary value problem, again using equation (6.111),
the appropriate space of test functions is H1(0, ), the space defined just like
H1
D except that no boundary conditions are imposed. This requires a bit of
explanation. First, we have the formal statement of the weak formulation
of the Neumann problem: Find a function u in H1(0, ) such that, with the
same notation as in display (6.113),
(u, φ) = 	f,φ

for all φ ∈ H1(0, ). We will show the following proposition: If u is smooth
enough so that the integration by parts in display (6.111) is valid and
the equivalence class of u in H1(0, ) is a weak solution of the Neumann
problem, then u satisfies the Neumann boundary conditions. In fact, if
φ ∈ H1
D(0, ) ⊂ H1(0, ), then φ is a limit of smooth functions that sat￾isfy the Dirichlet boundary conditions. By using integration by parts for a
sequence of smooth functions converging to φ in H1
D(0, ) and passing to6.6 Origins of ODE: Partial Differential Equations 327
the limit, we have the identity
 	
0
(−uxx + gu)φ dx =
 	
0
fφ dx (6.114)
for all φ ∈ H1
D(0, ). In other words, −uxx + gu − f is the zero element
of H1
D(0, ). By Exercise 6.66, the space H1
D(0, ) is a dense subspace of
H1(0, ). Thus, it is easy to see that the identity (6.114) holds for all
φ ∈ H1(0, ). Finally, by this identity, the boundary term in the integra￾tion by parts formula in display (6.111) must vanish for each φ ∈ H1(0, );
hence u satisfies the Neumann boundary conditions, as required. Our weak
formulation is therefore consistent with the classical boundary value prob￾lem: If a weak solution of the Neumann boundary value problem happens
to be smooth, then it will satisfy the Neumann boundary conditions.
Exercise 6.66. Prove that H1
D(0, ) is a dense subspace of H1(0, ).
Our analysis leads to the natural question “If a weak solution exists,
then is it automatically a strong (classical) solution?” The answer is “yes”
for the example problems formulated here, but this important regularity
result is beyond the scope of our discussion. Let us simply remark that the
regularity of the weak solution depends on the form of the PDE. It is also
natural to ask if our weak boundary value problems have solutions. While
the existence theory for boundary value problems is difficult in general, we
will formulate and prove an elementary result that implies the existence of
solutions for some of the examples that we have considered. The proof of
this result uses the contraction principle.
Let us suppose that H is a real Hilbert space, that ( , ) is a bilinear
form on H (it maps H × H → R), 	 , 
 is the inner product on H, and
  := 	 , 
1/2 is the natural norm. The bilinear form is called continuous
if there is a constant a > 0 such that
|(u, v)| ≤ auv
for all u, v ∈ H. The bilinear form is called coercive if there is a constant
b > 0 such that
(u, u) ≥ bu2
for all u ∈ H.
Theorem 6.67 (Lax-Milgram Theorem). If H is a real Hilbert space,
( , ) is a continuous and coercive bilinear form on H, and F is a bounded
linear functional F : H → R, then there is a unique u ∈ H such that
(u, φ) = F(φ)328 6. Applications
for every φ ∈ H. Moreover,
u ≤ 1
b
F.
Proof. The main tool of the proof is a standard result in Hilbert space
theory, the Riesz representation theorem: If F is a bounded linear func￾tional, then there is a unique f ∈ H such that F(φ) = 	f,φ
 for every
φ ∈ H (see [223]). In particular, this is true for the functional F in the
statement of the theorem.
If u ∈ H, then the function given by φ → (u, φ) is a linear functional on
H. To see that this functional is bounded, use the continuity of the bilinear
form to obtain the estimate
|(u, φ)| ≤ auφ
and note that u < ∞. The Riesz theorem now applies to each such
functional. Therefore, there is a function A : H → H such that
(u, φ) = 	Au, φ

for all φ ∈ H. Moreover, using the linearity of the bilinear form, it follows
that A is a linear transformation.
It is now clear that the equation in the statement of the theorem has a
unique solution if and only if the equation Au = f has a unique solution
for each f ∈ H.
By the continuity and the coerciveness of the bilinear form, if u, v, φ ∈ H,
then
	A(u − v), φ
 = (u − v, w) ≤ au − vφ, (6.115)
	A(u − v), u − v
 = (u − v, u − v) ≥ bu − v2. (6.116)
Also, by the Schwarz inequality, we have that
sup
φ≤1
|	v, φ
| ≤ v,
and, for φ := (1/v)v, this upper bound is attained. Thus, the norm of
the linear functional φ → 	w, φ
 is w. In particular, using the inequal￾ity (6.115), we have
Au − Av = sup
w≤1
	A(u − v), φ
 ≤ au − v. (6.117)
Define the family of operators Aλ : H → H by
Aλφ = φ − λ(Aφ − f), λ> 0,6.6 Origins of ODE: Partial Differential Equations 329
and note that Aλu = u if and only if Au = f. Thus, to solve the equation
Au = f, it suffices to show that for at least one choice of λ > 0, the operator
Aλ has a unique fixed point.
By an easy computation using the definition of the norm, equation (6.115),
the Schwarz inequality, and equation (6.117), we have that
Aλu − Aλv2 = (1 − 2λa + λ2a2)u − v2.
The polynomial in λ vanishes at λ = 0 and its derivative at this point is
negative. It follows that there is some λ > 0 such that the corresponding
operator is a contraction on the complete metric space H. By the contrac￾tion mapping theorem, there is a unique fixed point u ∈ H. Moreover, for
this u we have proved that (u, u) = F(u). Therefore,
uF≥	f,u
 ≥ bu2,
and the last statement of the theorem follows. ✷
Construction of Weak Solutions
The Lax-Milgram theorem is a classic result that gives us one way to prove
the existence of weak solutions for our boundary value problems. One way
to construct a solution, or at least a computable approximation of a solu￾tion, is to use the Gal¨erkin method described above. In fact, with the
previously defined notation, let us consider one of the finite-dimensional
Hilbert spaces Sn of H. Note that if the hypotheses of the Lax-Milgram
theorem are satisfied, then there is a unique un ∈ Sn such that
(un, s) = 	f,s
 (6.118)
for all s ∈ Sn with the additional property that
un ≤ 1
b
f (6.119)
where b is the coercivity constant. The Gal¨erkin principle is the statement
that the sequence {un}∞
n=1 converges to the unique solution u of the weak
boundary value problem. The approximation un can be expressed as a
linear combination of the vectors ν1,...,νn that, by our choice, form a
basis of the subspace Sn. Thus, there are real numbers c1,...,cn such that
un = n
j=1
cjνj .
Also, each element s ∈ Sn is given in coordinates by
s = n
i=1
siνi.330 6. Applications
Thus, the equation (6.118) is given in coordinates by the system of equa￾tions
n
j=1
cj (νj , νi) = 	f,νi
, i = 1, . . . n,
or, in the equivalent matrix form for the unknown vector (c1,...cn), we
have the equation
S
⎛
⎜⎝
c1
.
.
.
cn
⎞
⎟⎠ =
⎛
⎜⎝
	f,ν1

.
.
.
	f,νn

⎞
⎟⎠
where S, called the stiffness matrix—the terminology comes from the the￾ory of elasticity—is given by Sij := (νj , νi). Of course, by the Lax-Milgram
theorem, S is invertible and the matrix system can be solved to obtain the
approximation un.
Does the sequence of approximations {un}∞
n=1 converge? The first obser￾vation is that, by the inequality (6.119), the sequence of approximates is
bounded. Let u be the weak solution given by the Lax-Milgram theorem.
Subtract the equality (un, s) = 	f,s
 from the equality (u, s) = 	f,s
 to
see that
(u − un, s) = 0 (6.120)
for all s ∈ Sn. Also, using the coerciveness of the bilinear form, if φ ∈ Sn,
then
bu − un2 ≤ (u − un, u − un)=(u − un, u − un + φ − φ)
= (u − un, φ − un)+(u − un, u − φ).
Moreover, with un, φ ∈ Sn and equation (6.120), we have the inequality
bu − un2 ≤ (u − un, u − φ) ≤ au − unu − φ.
It follows that
u − un ≤ a
b
u − φ (6.121)
for all φ ∈ Sn.
Recall that the linear span of the sequence {νj}∞
j=1 is assumed to be
dense in H. Hence, for each  > 0 there is some integer m and constants
c1,...,cm such that
u −m
j=1
cjνj < .
If we set n = m and v = m
j=1 cjνj in the inequality (6.121), then
u − un ≤ a
b
.6.6 Origins of ODE: Partial Differential Equations 331
In other words, the sequence of Gal¨erkin approximations converges to the
weak solution, as required.
In the context of the steady state problem with which we started, namely,
the PDE (6.109), the Lax-Milgram theorem applies if g is bounded above
zero (see Ex. 6.68). For example, let g be a constant function given by
g(x) = λ > 0, consider Dirichlet boundary conditions, and let Sn denote
the span of the subset {ν1, ν2,...,νn} of H1
D(0, ), where
νj (x) := sin jπ
 x.
The smooth function f is represented by a Fourier (sine) series,
f(x) = ∞
j=1
aj sin jπ
 x,
on the interval (0, ), and the corresponding Gal¨erkin approximation is
un(x) = n
j=1
aj
λ + (jπ/)2 sin jπ
 x, (6.122)
exactly the partial sum of the Fourier series approximation of the solution
(see Exercise 6.69).
Exercise 6.68. Prove that if g is bounded above zero, then the bilinear form
(u, v) :=  
0
(uxvx + guv) dx
is continuous and coercive on the spaces H1
D and H1. Also, prove that if f is
smooth, then F(φ) :=  
0 fφ dx is a continuous linear functional on H1
D and H1.
Exercise 6.69. Suppose g is a negative constant. Find the stiffness matrix for
the Gal¨erkin approximation for the PDE (6.109) with Dirichlet boundary condi￾tions using the basis given by
νj (x) := sin jπ
 x, j = 1, 2,..., ∞
for H1
0 , and verify the approximation (6.122). Also, consider the PDE (6.109)
with Neumann boundary conditions, and find the Gal¨erkin approximations cor￾responding to the basis
1, cos πx
 , sin πx
 ,....332 6. Applications
Gal¨erkin Approximations and ODE
We have now seen one very simple example where the Gal¨erkin principle
can be turned into a theorem. Let us take this as a prototype argument
to justify the Gal¨erkin principle and proceed to our main objective in this
section: to see how the Gal¨erkin method leads to problems in ordinary
differential equations.
Let us consider the PDE
ut = uxx + f(x, t), 0 < x < , t > 0 (6.123)
with either Dirichlet or Neumann boundary conditions. The weak form
of this boundary value problem is derived from the integration by parts
formula
 	
0
(ut − uxx − f(x, t))φ dx =
 	
0
(utφ + uxφx − f(x, t))φ dx − uxφ



	
0
.
Just as before, we can formulate two weak boundary value problems.
The Dirichlet Problem: Find u(x, t), a family of functions in H1
D(0, )
such that
 	
0
(utφ + uxφx) dx =
 	
0
fφ dx
for all φ ∈ H1
D(0, ).
The Neumann Problem: Find u(x, t), a family of functions in H1(0, )
with the same integral condition satisfied for all φ ∈ H1(0, ).
To apply the Gal¨erkin method, choose ν1, ν2,... a linearly independent
sequence whose span is dense in the Hilbert space H1
D(0, ) or H1(0, ), and
define the finite-dimensional spaces Sn as before. The new wrinkle is that
we will look for an approximate solution in the subspace Sn of the form
un(x, t) = n
j=1
cj (t)νj (x)
where the coefficients are differentiable functions of time. According to the
Gal¨erkin principle, let us search for the unknown functions c1,...,cn so
that we have (un, s) = 	f,s
 for all s ∈ Sn. Expressed in coordinates, the
requirement is that the unknown functions satisfy the system of n ordinary
differential equations
n
j=1
c
j (t)
 	
0
νjνk dx +n
j=1
cj (t)
 	
0
(νj )x(νk)x dx =
 	
0
fνk dx
indexed by k = 1,...,n. In matrix form, we have the linear system of
ordinary differential equations
MC + SC = F(t)6.6 Origins of ODE: Partial Differential Equations 333
where M, given by
Mkj :=  	
0
νjνk dx
is called the mass matrix, S, given by
Skj :=  	
0
(νj )x(νk)x dx
is the stiffness matrix, and C := (c1,...,cn). If the initial condition for
the PDE (6.123) is u(x, 0) = u0(x), then the usual choice for the initial
condition for the approximate system of ordinary differential equations is
the element un
0 ∈ Sn such that
	un
0 , s
 = 	u0, s

for all s ∈ Sn. This “least squares” approximation always exists. (Why?)
We have, in effect, described some aspects of the theoretical foundations
of the finite element method for obtaining numerical approximations of
PDE (see [242]). But a discussion of the techniques that make the finite
element method a practical computational tool is beyond the scope of this
book.
The Gal¨erkin method was originally developed to solve problems in elas￾ticity. This application yields some interesting dynamical problems for the
corresponding systems of ordinary differential equations. Let us consider,
for instance, the PDE (more precisely the integro-PDE),
uxxxx +

α − β
 1
0
u2
x dx
uxx + γux + δut + utt = 0
that is derived in the theory of aeroelasticity as a model of panel flut￾ter where u(x, t) represents the deflection of the panel (see, for example,
the book of Raymond L. Bisplinghoff and Holt Ashley [28, p. 428] where
the physical interpretation of this equation and its parameters are given
explicitly). The boundary conditions for “simply supported” panel edges
are
u(0, t) = u(1, t)=0, uxx(0, t) = uxx(1, t)=0.
If we take just the first Fourier mode, that is, the Gal¨erkin approximation
with trial function
u1(x, t) = c(t) sin πx,
then we obtain the equation
c¨+ δc˙ + π2(π2 − α)c + π4
2 βc3 = 0. (6.124)
Let us note that if π2 − α < 0, then this Gal¨erkin approximation is a
form of Duffing’s equation with damping. We have already developed some334 6. Applications
of the tools needed to analyze this equation. In fact, most solutions are
damped oscillations whose ω-limits are one of two possible asymptotically
stable rest points (see Ex. 6.70). In contrast, if a periodic external force is
added to this system, then very complex dynamics are possible (see [141]
and Chapter 9).
Exercise 6.70. Draw representative phase portraits for the family of differen￾tial equations (6.124). How does the phase portrait depend on the choice of the
parameters?
Exercise 6.71. Consider the basis functions
νj (x) := sin(jπx/)
for H1
D(0, ). (a) Find the mass matrix and the stiffness matrix for the Gal¨erkin
approximations for the weak Dirichlet boundary value problem (6.123) with
f(x, t) := sin(πx/) cos ωt. (b) Solve the corresponding system of linear differ￾ential equations for the nth approximation un(x, t). (c) What can you say qual￾itatively about the solutions of the Gal¨erkin approximations? What long term
dynamical behavior of the PDE (6.123) is predicted by the Gal¨erkin approxi￾mations? (d) Find a steady state solution? (e) Repeat the analysis for f(x, t) =
cos ωt. (f) Do you see a problem with the validity of these formal computations?
(g) Formulate and solve analogous problems for Neumann boundary conditions.
Exercise 6.72. Consider a two (Fourier) mode Gal¨erkin approximation for the
PDE
ut = k2
uxx + u − u3 + a cos ωt, 0 < x < , t > 0
with either Dirichlet or Neumann boundary conditions. (a) What is the general
character of the solutions in the phase plane? Hint: Start with the case where
there is a time-independent source term (a = 0) and consider the stability of
the steady state solution of the PDE at u ≡ 0. (b) Is the (linearized) stability
criterion for the PDE reflected in the stability of the corresponding rest point in
the phase plane of the approximating ordinary differential equation? (c) Is the
ω-limit set of every solution of the approximation a rest point?
6.6.3 Traveling Waves
The concept of traveling wave solutions will be introduced in this section
for the classic reaction-diffusion model system
ut = k2uxx + au(1 − u), x ∈ R, t> 0 (6.125)
where k and a > 0 are constants.
The PDE (6.125), often called Fisher’s equation, can be used to model
many different phenomena. For example, this equation is a model of logistic
growth with diffusion ([102], [192]), and it is also a model of neutron flux6.6 Origins of ODE: Partial Differential Equations 335
in a nuclear reactor (see [203]). For a general description of this and many
other models of this type see [192] and [203].
Let us begin with the observation that equation (6.125) can be rescaled
to remove the explicit dependence on the system parameters. In fact, with
respect to the new time and space variables
τ = at, ξ =
√a
k x,
equation (6.125) can be recast in the form
uτ = uξξ + u(1 − u). (6.126)
Therefore, with no loss of generality, we will consider the original model
equation (6.125) for the case a = 1 and k = 1.
In some applications, Fisher’s equation is considered on the whole real
line (that is, (−∞ <x< ∞)) and the physically relevant boundary condi￾tions are
lim x→−∞ u(x, t)=1, limx→∞ u(x, t)=0. (6.127)
For example, u might measure the diseased fraction of a population, which
is distributed in some spatial direction (with spatial position x). The infected
portion of the population increases to unity in the negative spatial direction;
it decreases to zero in the positive x direction.
The basic idea is to look for a solution of equation (6.125) in the form of
a traveling wave, that is,
u(x, t) = U(x − ct)
where the wave form is given by the function U : R → R and the wave
speed is |c| = 0. For definiteness and with respect to our disease model,
let us assume that c > 0. By substituting the traveling wave ansatz into
Fisher’s equation, we obtain the second-order nonlinear ordinary differen￾tial equation
U¨ + cU˙ + U − U2 = 0,
which is equivalent to the phase plane system
U˙ = V, V˙ = −U − cV + U2. (6.128)
All solutions of the system (6.128) correspond to traveling wave solutions
of Fisher’s equation. But, a meaningful solution of a physical model must
satisfy additional properties. For example, in the case where u measures
the infected fraction of a population, we must have 0 ≤ u ≤ 1 and the
boundary conditions (6.127).
Fisher’s equation, for a population model in which we ignore diffusion,
reduces to the one-dimensional ordinary differential equation ˙u = u−u2 for336 6. Applications
V
U
Ω
Figure 6.8: The invariant region Ω for the system (6.128) in case c ≥ 2.
logistic growth, whose dynamics can be completely determined. In partic￾ular, the phase space is R, there is an unstable rest point at u = 0, a stable
rest point at u = 1, and a connecting orbit, that is, an orbit with α-limit
set {0} and ω-limit set {1}. For our disease model, this result predicts that
if some fraction of the population is infected, then the entire population is
eventually infected.
This suggests a natural question: Is there a traveling wave solution
u(x, t) = U(x − ct) for the PDE (6.125) such that 0 < u(x, t) < 1, u
satisfies the boundary conditions (6.127), and
limt→∞ u(x, t)=1, lim t→−∞ u(x, t) = 0?
In other words, is there an orbit—for the PDE viewed as an infinite￾dimensional ordinary differential equation in a space of solutions that incor￾porates the boundary conditions—connecting the steady states u ≡ 0 and
u ≡ 1 as in the case of the one-dimensional logistic model?
Let us note that all the required conditions are satisfied if 0 < U(s) < 1
and
lims→∞ U(s)=0, lim s→−∞ U(s)=1.
An answer to our question is given by the following proposition.
Proposition 6.73. The PDE (6.126) with the boundary conditions (6.127)
has a traveling wave solution (x, t) → u(x, t) = U(x−ct), with 0 < u(x, t) <
1, whose orbit connects the steady states u ≡ 0 and u ≡ 1 if and only if
c ≥ 2.
Proof. Note that the solution u(x, t) = U(x − ct) is a connecting orbit if
0 < U(s) < 1, and
lims→∞ U(s)=0, lim s→−∞ U(s)=1.6.6 Origins of ODE: Partial Differential Equations 337
The system matrix of the linearized phase plane equations (6.128) at the
origin has eigenvalues
1
2
(−c ± c2 − 4),
and its eigenvalues at the point (1, 0) are given by
1
2
(−c ± c2 + 4).
Therefore, if c > 0, then there is a hyperbolic sink at the origin and a
hyperbolic saddle at the point (1, 0). Moreover, if a connecting orbit exists,
then the corresponding phase plane solution s → (U(s), V (s)) must be on
the unstable manifold of the saddle and the stable manifold of the sink.
Note that if c < 2, then the sink at the origin is of spiral type. Hence,
even if there is a connecting orbit in this case, the corresponding function
U cannot remain positive.
Assume that c ≥ 2 and consider the lines in the phase plane given by
V = 1
2

− c + c2 − 4

U, V = 1
2

− c + c2 + 4
(U − 1). (6.129)
They correspond to eigenspaces at the rest points. In particular, the second
line is tangent to the unstable manifold of the saddle point at (U, V ) =
(1, 0). The closed triangular region Ω (see Figure 6.8) in the phase plane
bounded by the lines (6.129) and the line given by V = 0 is positively
invariant. This result is easily checked by computing the dot product of the
vector field corresponding to system (6.128) with the appropriate normal
fields along these lines to see that the vector field points into Ω at every
point on its boundary except for the rest points. Indeed, along the lines
defined by the equations in display (6.129), we have
V˙ − 1
2
(−c + c2 − 4)U˙ = U2 ≥ 0,
V˙ − 1
2
(−c + c2 + 4)U˙ = (U − 1)2 ≥ 0, (6.130)
and V˙ = −U + U2 < 0 for 0 <U< 1 along the line with equation V = 0.
Suppose (as we will soon see) that the unstable manifold at the saddle
intersects the region Ω. Then a solution that starts on this portion of the
unstable manifold must remain in the region Ω for all positive time. Thus,
the ω-limit set of the corresponding orbit is also in Ω. Because U˙ ≤ 0 in
Ω, there are no periodic orbits in Ω and no rest points in the interior of Ω.
By the Poincar´e-Bendixson theorem, the ω-limit set must be contained in
the boundary of Ω. In fact, this ω-limit set must be the origin.
To complete the proof, we will show that the unstable manifold at the
saddle has nonempty intersection with the interior of Ω. To prove this fact,
let us first recall that the unstable manifold is tangent to the line given by338 6. Applications
the second equation in display (6.129). We will show that the tangency is
quadratic and the unstable manifold lies above this line.
In the new coordinates given by
Z = U − 1, W = V,
the saddle rest point is at the origin for the equivalent first-order system
Z˙ = W, W˙ = Z − cW + Z2.
The additional change of coordinates
Z = P, W = Q + αP := Q +
1
2
(−c + c2 + 4)P
transforms the system so that the unstable manifold of the saddle point is
tangent to the horizontal P-axis. We will show that the unstable manifold
is above this axis in some neighborhood of the origin; it then follows from
the second formula in display (6.130) that the unstable manifold lies above
the P-axis globally.
Note that the unstable manifold is given, locally at least, by the graph
of a smooth function Q = h(P) with h(0) = h
(0) = 0. Since this manifold
is invariant, we must have that Q˙ = h
(P)P˙ , and therefore, by an easy
computation,
P2 − (c + α)h(P) = h
(P)(h(P) + αP). (6.131)
The function h has the form h(P) = βP2 + O(P3). By substitution of this
expression into equation (6.131), we obtain the inequality
β = (3α + c)
−1 > 0,
as required. ✷
Much more can be said about the traveling wave solutions that we
have just found. For example, the ω-limit set of most solutions of the
PDE (6.125), which start with physically realistic initial conditions, is the
traveling wave solution with wave speed c = 2. This result was proved by
Andrei N. Kolmogorov, Ivan G. Petrovskii, and Nikolai S. Piskunov [153]
(see also [19] and [27]). For a detailed mathematical account of traveling
wave solutions see the book of Paul C. Fife [102] and also [192] and [232].
Exercise 6.74. The reaction term for the PDE studied in Proposition 6.73 is
g(u) = u(1 − u). Prove a generalization of this proposition where the reaction
term is given by a function g that is C1 on (0, 1) and such that g is bounded
on (0, 1), g(0) = g(1) = 0, g(u) > 0 on (0, 1), g
(0) = 1, and g
(u) < 1 on (0, 1]
(see [153]).6.6 Origins of ODE: Partial Differential Equations 339
Exercise 6.75. Show that the PDE
ut − u2
ux = uxx + u, x ∈ R, t ≥ 0
has a nonconstant solution that is periodic in both space and time.
Exercise 6.76. For positive constants α and β, find a traveling wave solution
of the Korteweg-de Vries (KdV) equation
ηt + (1 + 3
2
αη)ηx +
1
3
βηxxx = 0, x ∈ R, t ≥ 0
in the form η(x, t) = N(x − ct) where N, N
, and N all vanish as the argument
of N approaches ±∞ and c > 1. The solution is an approximation of the “great
wave of translation” observed by John Scott Russell (1808-1882). Further analysis
of this equation led to the theory of solitons—nonlinear solitary waves that retain
their basic shape after interaction with other solitary waves of the same type (see,
for example, [243]).
6.6.4 First Order PDE
Consider the model equation (6.97) in case there is no diffusion, but the
medium moves with velocity field V ; that is, the differential equation
ut + γ grad u · V = f. (6.132)
This is an important example of a first-order partial differential equation.
Other examples are equations of the form
ut + (f(u))x = 0,
called conservation laws (see [232]), and equations of the form
St + H(Sq, q, t)=0,
called Hamilton-Jacobi equations (see [13]). We will show how such PDE
can be solved using ordinary differential equations in case the derivatives
of the unknown function appear linearly and there is only one space dimen￾sion. The theory can be generalized to fully nonlinear equations with several
space dimensions (see, for example, [94] and Exercises 6.82 and 6.83).
The equation (6.132), for γ = 1, is given by
ut + v(x, t)ux = f(u, x, t),
or, with a redefinition of the names of the functions, it has the more general
form
f(x, y, u)ux + g(x, y, u)uy = h(x, y, u). (6.133)340 6. Applications
We will solve the PDE (6.133) using the following basic idea: If the graph
G of a function z = u(x, y) is an invariant manifold for the first-order
system
x˙ = f(x, y, z), y˙ = g(x, y, z), z˙ = h(x, y, z), (6.134)
then u is a solution of the PDE (6.133). Indeed, because
(x, y) → (x, y, u(x, y), ux(x, y), uy(x, y), −1)
is a normal vector field on G, it follows from the results of Section 1.7.1 that
the manifold G is invariant if and only if the dot product of the vector field
associated with the system (6.134) and the normal field is identically zero;
that is, if and only if equation (6.133) holds. The orbits of the system (6.134)
are called characteristics of the PDE (6.133).
Perhaps it is possible to find an invariant manifold for the first-order
system (6.134) by an indirect method, but we will construct the invariant
manifold directly from appropriate initial data. To see how this is done, let
us suppose that we have a curve in space given by γ : R → R3 such that in
coordinates
γ(s)=(γ1(s), γ2(s), γ3(s)).
This curveis callednoncharacteristic atγ(0) (with respect to thePDE (6.133))
if
γ˙1(0)g(γ(0)) − γ˙2(0)f(γ(0)) = 0.
The geometric interpretation is clear: The projection of the tangent vec￾tor of the noncharacteristic curve at γ(0) onto its first two components is
transverse to the first two components of the tangent to the characteristic
at this point.
Let ϕt denote the flow of the system (6.134), and define H : R2 → R3 by
(s, t) → ϕt(γ(s)). (6.135)
Also, define H : R2 → R2 by projection of the image of H onto its first two
components. More precisely, let e1, e2, e3 be the usual basis vectors for R3
and let the usual inner product be denoted by angle brackets. Then H is
given by
(s, t) → (	ϕt(γ(s)), e1
,	ϕt(γ(s)), e2
).
The image of H is an invariant set for system (6.134). We will prove
the following proposition: if γ is a noncharacteristic curve at γ(0) and
both |s| and |t| are sufficiently small, then the image of H is an invariant
two-dimensional manifold.
Using this proposition, we will obtain the desired solution of our PDE by
simply reparameterizing the invariant manifold with the coordinate trans￾formation given by H.6.6 Origins of ODE: Partial Differential Equations 341
The idea of the proof of the proposition is to show that DH(0, 0) is
invertible and then apply the inverse function theorem. To show that H is
locally invertible at γ(0), note that
DH(0, 0)e1 = d
dsH(s, 0)



s=0
= d
ds (γ1(s), γ2(s))



s=0
= (˙γ1(0), γ˙2(0)),
and similarly
DH(0, 0)e2 = (f(γ(0)), g(γ(0))).
Because the curve γ is noncharacteristic at γ(0), the matrix representation
of DH(0, 0) has nonzero determinant and is therefore invertible. By the
inverse function theorem, H is locally invertible at the origin.
Let H = (H1, H2, H3) and suppose that H is invertible. We have the
identity
H(H−1(x, y)) = (x, y, H3(H−1(x, y))).
In other words, the surface given by the range of H is locally the graph of
the function u defined by
u(x, y) = H3(H−1(x, y)).
Hence, u is a local solution of the PDE (6.133). Moreover, since u(H(s, t)) =
H3(s, t), we have that
u(γ1(s), γ2(s)) = γ3(s).
This last equation is used to encode initial data for the PDE along a non￾characteristic curve.
We have now proved that if we are given a noncharacteristic curve, then
there is a corresponding local solution of the PDE (6.133). Also, we have
obtained a method to construct such a solution.
The method is very simple: Choose a noncharacteristic curve γ, which
encodes initial data via the formula u(γ1(s), γ2(s)) = γ3(s); determine the
flow φt of the first-order system (6.134) for the characteristics; define the
function H given by H(s, t) = φt(γ(s)), and invert the associated function
H given by H(s, t)=(H1(s, t), H2(s, t)). The corresponding solution of the
PDE (6.133) is u(x, y) = H3(H−1(x, y)).
As an example, let us consider the model equation
uτ + a sin(ωτ )ux = u − u2, 0 ≤ x ≤ 1, τ ≥ 0
with initial data u(x, 0) = u0(x) defined on the unit interval. A phenomeno￾logical interpretation of this equation is that u is the density of a species342 6. Applications
with logistic growth in a moving medium that is changing direction with
frequency ω and amplitude a. We have used τ to denote the time parame￾ter so that we can write the first-order system for the characteristics in the
form
τ˙ = 1, x˙ = a sin(ωτ ), z˙ = z − z2. (6.136)
To specify the initial data, define the noncharacteristic curve given by s →
(0, s, u0(s)). After solving system (6.136) and using the definition of H in
display (6.135), we have that
H(s, t) = 
t, s +
a
ω (1 − cos ωt), et
u0(s)
1 + u0(s)(et − 1)

.
Also, because H−1 is given explicitly by
H−1(τ,x)=(τ,x − a
ω (1 − cos ωτ )),
we have the solution
u(x, τ ) = eτu0(x − a
ω (1 − cos ωτ ))
1+(eτ − 1)u0(x − a
ω (1 − cos ωτ )). (6.137)
What does our model predict? For example, if the initial condition is
given by a positive function u0, then the ω-limit set of the corresponding
solution of the PDE is the constant function u ≡ 1, the solution correspond￾ing to no drift. On the other hand, if the initial population is distributed so
that some regions have zero density, then the fate of the initial population
is more complicated (see Ex. 6.77).
Exercise 6.77. What long term behavior for the corresponding model equation
is predicted by the solution (6.137)? How does your answer depend on the choice
of u0, a, and ω?
Exercise 6.78. Find the general solution of the PDE aux + buy = h(x, y) in
case a and b are constants and h is a given function. Hint: Solve the homogeneous
equation (h = 0) first.
Exercise 6.79. (a) Find the general solution of the PDE aux + buy = cu
in case a, b, and c are constants using the method developed in this section.
(b) Find the general solution using the following alternative method: Find an
integrating factor for the expression uy −(c/b)u; that is, find a function μ so that
(μ(y)u)y = uy − (c/b)u.
Exercise 6.80. Solve the PDE xux + yuy = 2u with u prescribed on the unit
circle. Hint: Define the noncharacteristic curve
s → (cos s, sin s, h(cos s, sin s)).
Exercise 6.81. (a) Find the general solution of the PDE xux − yuy = 2u. (b)
Give a geometric description of the noncharacteristic curves.6.6 Origins of ODE: Partial Differential Equations 343
Exercise 6.82. [Several Space Dimensions] Solve the PDE ut = a(x, y)ux +
b(x, y)uy with initial data u(0, x, y) = g(x, y). Hint: This is an example of a
first-order PDE with two space variables. The method of solution illustrates the
generalization of the theory presented in this section. (a) Write the PDE in the
standard form uτ − a(x, y)ux − b(x, y)uy = 0, where t is replaced by τ for nota￾tional convenience, and let ψt denote the flow of the differential equation
x˙ = a(x, y), x˙ = b(x, y).
Show that ψ˜t = ψ−t is the flow of the differential equation
x˙ = −a(x, y), x˙ = −b(x, y).
(b) The characteristics of the PDE are the solutions of the system
τ˙ = 1, x˙ = −a(x, y), x˙ = −b(x, y), z˙ = 0.
Show that the flow φt of this system has the form
φt(τ, x, y, z)=(t + τ,ψ−t(x, y), z).
(c) Because the solution u has three arguments, the initial data must be encoded
into a noncharacteristic surface of the form
γ(r, s)=(γ1(r, s), γ2(r, s), γ3(r, s), γ4(r, s)).
Define the notion of a noncharacteristic surface and show that the parametrized
surface given by the function γ(r, s) = (0, r, s, g(r, s)) is a noncharacteristic sur￾face. (d) Define H(r, s, t) = φt(γ(r, s)). Show that H4(r, s, t) = g(r, s). (e) Define
H to be the function H composed with the linear projection onto its first three
components. Show that H−1(τ, x, y)=(ψτ (x, y), τ ). (f) By the theory in this sec￾tion, the solution of the PDE with the given initial data is u(t, x, y) = g(ψt(x, y)).
Verify that this is the solution by direct substitution into the PDE. (g) Find the
solution of the initial value problem
ut = −yux + xuy, u(0, x, y)=2x2 + y2
.
Exercise 6.83. [Several Space Dimensions (Continued)] (a) Let X denote a
vector field on R2. Generalize the method outlined in Exercise 6.82 to cover the
initial value problem for the equation of continuity
ut + div(uX)=0, u(0, x, y) = g(x, y).
(b) Suppose that u denotes the density of some substance in a medium whose
instantaneous direction of motion in the physical three-dimensional space is
approximated by the vector field X(x, y, z)=(−(x + y), x − y, 0). Suppose that
the initial density is constant, say u(x, y, z) = 1, and determine the density at
time t > 0. Give a physical interpretation of your answer. (c) Repeat part (b) for
the initial density u(x, y, z) = x2 + y2.
Exercise 6.84. A function U that is constant along the orbits of an ordinary
differential equation is called an invariant function, or a first integral. In symbols,
if we have a differential equation ˙x = f(x) with flow ϕt, then U is invariant344 6. Applications
provided that U(φt(x)) = U(x) for all x and t for which the flow is defined.
Show that U is invariant if and only if gradU(x), f(x) ≡ 0. Equivalently, the
directional derivative of U in the direction of the vector field given by f vanishes.
Consider the differential equation
˙
θ = 1, φ˙ = α
where α ∈ R. Also, consider both θ and φ as angular variables so that the
differential equation can be viewed as an equation on the torus. Give necessary
and sufficient conditions on α so that there is a smooth invariant function defined
on the torus.
Exercise 6.85. A simple example of a conservation law is the (nonviscous)
Burgers’s equation ut + uux = 0. Burgers’s equation with viscosity is given by
ut + uux = 1
Reuxx,
where Re is called the Reynold’s number. It is a simple model that incorporates
two of the main features in fluid dynamics: convection and diffusion. Solve the
nonviscous Burgers’s equation with initial data u(x, 0) = (1 − x)/2 for −1 <
x < 1. Note that the solution cannot be extended for all time. This is a general
phenomenon that appears in the study of conservation laws that is related to the
existence of shock waves (see [232]). Also, consider the viscous Burgers’s equation
on the same interval with the same initial data and with boundary conditions
u(−1, t)=1, u(1, t)=0.
How are Gal¨erkin approximations found? A problem arises from these nonhomo￾geneous boundary conditions: there is no vector space of functions that satisfy
the boundary conditions. To overcome this difficulty, look for a solution of the
problem in the form
u(x, t) = v(x, t) + 1
2
(1 − x)
where v satisfies the equation
vt + (v +
1
2
(1 − x))(vx − 1
2
) = vxx
and zero Dirichlet boundary conditions. (a) Determine the Gal¨erkin approxima￾tions using trigonometric trial functions. (b) Use a numerical method to solve the
resulting differential equations, and thus approximate the solution of the PDE.
(c) For a numerical analyst’s approach to this problem, consider the Gal¨erkin
approximations with respect to the “test function basis” of Chebyshev polyno￾mials given by
T0(x)=1, T1(x) = x, T2(x)=2x2 − 1
and for n > 2 by
Tn+1(x)=2xTn(x) − Tn−1(x).
The Chebyshev polynomials are orthogonal (but not orthonormal) with respect
to the inner product
f,g :=  1
−1
f(x)g(x)(1 − x2
)
−1/2 dx.6.6 Origins of ODE: Partial Differential Equations 345
Also, the Chebyshev polynomials do not satisfy the boundary conditions. In spite
of these facts, proceed as follows: Look for a Gal¨erkin approximation in the form
un(x, t) = n
i=1
ci(t)Tn−1(x),
but only construct the corresponding system of differential equations for
c1, c2, c3,...,cn−2.
Then, define the last two coefficients so that the boundary conditions are satisfied
(see [104]). Compare numerical results. (d) Show that Tn(cos θ) = cos nθ. Thus
there is a relation to Fourier cosine series and the Fourier transform. This allows
for efficient computation of Chebyshev coefficients, the ci, via the fast Fourier
transform, an indispensable tool in scientific computing. (e) Burgers’s equation
can, in principle, be solved explicitly by the Hopf-Cole transformation. In fact, if
u is a solution of Burgers’s equation and ψ is defined so that ψx = u, then ψ is
defined up to a function that depends only on the time variable. An appropriate
choice of the antiderivative satisfies the equation
ψt +
1
2
ψ2
x = 1
Reψxx.
If φ is defined by the equation ψ = −(2/Re) ln φ, then
φt = 1
Reφxx.
Thus, solutions of the heat equation can be used to construct solutions of Burg￾ers’s equation. Because Burgers’s equation can be solved explicitly, this PDE is
a useful candidate for testing numerical codes.346 6. Applications
6.7 Control
A basic scenario in applied science is discussed in this section: Some process
that we wish to control is modeled by a system of differential equations
x˙ = f(x, t, u), (6.138)
where f is a function of the process state x, time t, and control u that is
to be chosen from some set of admissible functions U. The challenge is to
determine the extent to which the state of the process can be influenced as
desired with an appropriate choice of u.
Suppose that S is some subset of the state space, which in this section is
Rn. The system is called controllable (with respect to S) if for all x0 in the
state space there is some u in U such that the solution of the correspond￾ing ODE (6.138) with initial condition x(0) = x0 reaches a state in S at
some finite time t = T ≥ 0. Note: This definition of controllability is not
universally accepted. In the control theory literature, the definition given
here is often called reachability.
There is a basic and natural question: When is a control system control￾lable?
In case a system is controllable and there are several or an infinite number
of controls in U that may be applied to reach S, we may also ask which if any
of these controls steers a given state to the target set in the least duration
of time? The subject that addresses this question is called time-optimal
control. More generally, optimal control seeks to answer the question which
if any admissible controls optimize some observable as the process evolves.
A typical applied context for the fundamental control problems involves
sensors and actuators. Perhaps the state of the system can be measured
with a sensor that is modeled by a function g so that the sensor’s output
y is given by y = g(x) and in turn some function h of the sensor output
z = h(y) can be applied (by actuators) to influence the system via feedback
control; that is,
x˙ = f(x, t, h(g(x))). (6.139)
The operational envelope of the actuators would be encoded into the defi￾nition of a control set U from which h is to be chosen.
An extensive, mathematically rich, and eminently practical control the￾ory has been produced over a long time period by many researchers (see, for
example, [245], [206]). To approach modern applications, a familiarity with
the basic principles of the theory are required. Some of them are briefly
addressed here with the intention of introducing several important ideas
without attention to generality.
Mathematical control is beautiful, enticing, and important. It provides
the foundation and context for working with models of control systems that
are ubiquitous in science and engineering. In particular, the theory leads6.7 Control 347
to and underlies the development of numerical methods that are usually
necessary for obtaining useful results in real-world applications.
6.7.1 Controllability of Time-Invariant Linear Systems
An important special case of a mathematical control system is the linear
time invariant (LTI) differential equation
x˙ = Ax + Bu, (6.140)
where the system matrix A is n × n, the gain matrix B is n × m, and the
control u is selected from a set U of admissible functions mapping R to Rm.
To be useful, control systems should have a unique solution for every
admissible control. The existence theory in this book applies directly for
continuous controls. With slight generalization, piecewise continuous con￾trols also provide unique solutions. In fact, the desired solution for the
(LTI) system is obtained from the variation of parameters formula
x(t) = etAx0 +
 t
0
e(t−s)ABu(s) ds (6.141)
whenever the integration be defined. A sufficient condition is that u be
bounded and measurable (with respect to the Lebesgue measure on the
real line). This level of generality plays a role in the full development of
the theory, where a standard set of admissible controls U is the set of all
bounded measurable functions defined on the interval [0,∞) with range in
Rm. But for most purposes, it suffices to consider the subset of U whose
elements are piecewise continuous.
In this section, controllability is discussed for system (6.140) with respect
to the target set S consisting of a single point z in Rn.
Start with the special case where B is an invertible n × n-matrix and
x0 = z. Select some time τ > 0 and consider the possibility that there is a
constant control u(t) ≡ v that steers x0 to the target z. The required v, if
it exists, satisfies the equation
z = eτAx0 +
 τ
0
e(τ−s)ABv ds.
With the linear transformation Γ : Rn → Rn given by
Γw =
 τ
0
e(τ−s)ABw ds,
the equation in more compact form is Γv = z − eτAx0. If Γ is invertible,
then the desired solution exists.
A linear operator that transforms a finite-dimensional vector space into
itself is invertible if and only if zero is not an eigenvalue. Suppose v is a348 6. Applications
nonzero vector in Rn such that Γv = 0. Because B is invertible, w := Bv
is not zero, and
 τ
0
e(τ−s)Aw ds = 0.
Moreover, because eτA is invertible,
 τ
0
e−sAw ds = 0.
By integrating both sides of the ODE ˙x = Ax with initial condition x(0) =
w,
eτAw − w = A
 τ
0
e−sAw ds.
This formula implies Γ has zero as an eigenvalue if and only if one is an
eigenvalue of eτA or A has the eigenvalue 2πik/τ for some integer k. Thus,
in general Γ is not invertible. Because A is fixed, the final time τ can be
chosen so that Γ is invertible. This observation is worth recording.
Proposition 6.86. If A is an n × n real matrix, τ > 0 and there is no
nonzero integer k such that 2πik/τ is an eigenvalue of A , then the operator
on Rn given by
w →
 τ
0
e−sAw ds
is invertible. Given A, the operator is invertible except possibly for a count￾able set of positive real numbers τ .
In summary, the LTI is controllable in the special case where B is invert￾ible.
In the general case, especially in the most important case for application
when m<n, B is not invertible. Controllability is determined by the
interplay of A and B. The next proposition reduces the control problem to
the special case where the target consists of the single element z = 0.
Proposition 6.87. If the linear time-invariant control system (6.140) has
the property that every state can be steered to the origin by an admissible
(bounded measurable) control in finite (positive) time, then the system is
controllable.
Proof. Let x0 be an initial state and z the target state. There is a control
u1 that steers the system from x0 to the origin in time τ and a control u2
that steers the state z to the origin in time σ. The new control
u(t) = u1(t), 0 ≤ t ≤ τ ;
−e−σAu2(t − τ ),τ <t ≤ τ + σ,6.7 Control 349
is clearly admissible. It steers x0 to z in time σ + τ . The proof of this fact
is a computation that proceeds from the formula
x(σ + τ ) = e(σ+τ)A +
 σ+τ
0
e(σ+τ−s)ABu(s) ds
by splitting the integral over the intervals [0, τ ] and [τ,σ + τ ], making a
change of variables in the latter integration, and using the properties of u1
and u2. ✷
Further insight into some of the ideas required to proceed with the inves￾tigation of controllability is provided by examination of another special
case. Suppose that n = 2 and m = 1 so that A is 2 × 2 and B is 2 × 1.
Choose x0 = 0 in R2. It is steered to the origin in time τ > 0 if there is an
admissible control u such that
 τ
0
e(τ−s)ABu(s) ds = −eτAx0.
Because the exponential of a square matrix is invertible, it suffices to check
for the existence of an admissible control such that
 τ
0
e−sABu(s) ds = −x0.
If the vector
 τ
0
e−sAB ds
is parallel to x0, a constant control will suffice to steer x0 to the origin. If
not, perhaps a series of constant control steps would accomplish the same
task. Define Γτ : Rn → Rn by
Γτw :=  τ/2
0
e−sAw ds.
Proposition 6.86 implies that there is some τ > 0 such that Γτ is invertible.
Using this τ , consider two-step (scalar valued) controls u given by constants
u1 on the time interval [0, τ ] and u2 on [τ, 2τ ]. The desired result is a choice
for this control such that
 τ/2
0
e(τ−s)ABu1 ds +
 τ
τ/2
e(τ−s)ABu2 ds = −eτAx0.
With a change of variables and using the invertibility of exponentiated
matrices,
u1e
τ
2 AΓτB + u2ΓτB = −e
τ
2 Ax0.350 6. Applications
If the vectors ΓτB and e
τ
2 AΓτB were linearly independent, there would
be a choice of u1 and u2 that would steer x0 to the origin because every
vector in Rn would be in the span of these vectors. After checking that e
τ
2 A
and Γτ commute, their invertibility implies that the vectors in question are
independent if B and e
τ
2 AB are linearly independent. Note that the choice
of τ > 0 is arbitrary except for at most a countable set of real numbers.
Conjecture: If B and AB are linearly independent, then x0 can be steered to
the origin with an admissible (piecewise continuous) control. The conjecture
is true. (Why?) In fact this result is a special case of a much more general
theorem that is the subject of the remainder of this section.
Definition 6.88. The controllability matrix for control system (6.140) is
the m × (nm) matrix
G = (B, AB, A2B,...,An−1B) (6.142)
constructed from the columns of the listed m × n matrices.
Theorem 6.89. Every initial value (point in Rn) for system (6.140) can
be steered to the origin in finite time with a bounded measureable control if
and only if the controllability matrix has rank n; that is, it has n linearly
independent columns.
The proof of Theorem 6.89 presented here requires several steps, one of
which uses a key fact from linear algebra:
Theorem 6.90 (Cayley-Hamilton theorem). Every (real or complex)
square matrix satisfies its characteristic polynomial.
Proof. See exercise (6.101). ✷
Recall that the characteristic polynomial PA of a matrix A is a monic
polynomial of degree n. Rearrange the identity PA(A) = 0 by moving
all terms to the right except for the leading term An to see that An is
expressible as a linear combination of the matrices A0, A1, A2,...,An−1.
This fact is easily seen to imply
Corollary 6.91. The pth power for p ≥ n of an n×n real matrix A is a lin￾ear combination, over the real numbers, of the matrices A0, A1, A2,...,An−1.
Proposition 6.92. Let v be a vector in Rn and suppose A and B are matri￾ces as in system (6.140). The vector (with T denoting transpose) vT ApB
is zero for p = 0, 1, 2, 3,...,n − 1 if and only if vT etAB = 0 whenever
−∞ <t< ∞. Moreover, if vT etAB = 0 on a nonempty open interval, then
vT ApB is zero for p = 0, 1, 2, 3,...,n − 1.6.7 Control 351
Proof. Suppose that vT ApB = 0 for p = 0, 1, 2, 3,...,n−1. Corollary 6.91
implies vT ApB = 0 for p = 0, 1, 2, 3,...,∞. Because
etAB = ∞
k=0
t
kAkB, (6.143)
vT etAB = 0 for all t.
For the converse, suppose that vT etAB = 0 for all t in some nonempty
open interval (which might be the entire line) and consider the real analytic
function given by t → vT etAB defined on the real line. In general, an
analytic function is zero on an interval where a power series representation
is valid if and only if all coefficients of the power series vanish. ✷
The geometric part of the proof uses the concept of a convex set. Recall
that a subset of a vector space is called convex if for every pair of points
p and q in the subset all points of the line segment given by the convex
combination (1−s)p+sq for 0 ≤ s ≤ 1 are in the set. A basic and important
fact is
Theorem 6.93. Let C be a convex subset of Rn. If b is in the boundary
of C, there is a nonzero vector η in Rn such that 	x, η
≤	b, η
 for every
x in C. In particular, all points in the convex set are in one of the closed
half-planes bounded by the (support) plane given by the set of all x in Rn
such that 	b, η
 = 	x, η
.
Proof. Case 1: C is closed. In this case, the boundary point b is in C. Let B
be a ball in Rn centered at b. By the definition of boundary point, there is a
point z in this ball that is not in C. A standard result in point set topology
states that there is a point in the closed set C closest to z. To prove this
fact, let d denote the distance from z to b and consider a new ball B with
radius d centered at z. The intersection of the closure of B and the closed
set C is compact, and this intersection contains all points in C at distance
no more than d from z. The set of all distances of points in this set to z
is bounded below by zero and therefore has a greatest lower bound. There
is a sequence of points in the intersection whose distances to z converge to
this greatest lower bound. Compactness implies a subsequence converges
to a point y in the intersection. It is a desired point in C closest to z.
Let η = z − y and note that η is not zero. For every x is in C, the
definition of convexity implies y +s(x−y) is in C for 0 ≤ s ≤ 1. The square
of the distance from z to points on this line segment extends to a quadratic
polynomial function of s given by
s → 	η − s(x − y), η − s(x − y)
.
This function does not decrease at s = 0 because the distance from z to
y is minimal for distances from z to points in C. Hence, its derivative at352 6. Applications
s = 0, given by −2	(x − y), η
, is nonnegative. Consequently
	x, η
≤	y, η
 (6.144)
for all x in C, a result that implies the hyperplane consisting of all z in
Rn such that 	z,η
 = 	y, η
 passes through y on C with all other points
in this latter set in the half-space bounded by the hyperplane opposite the
direction of η. Also, η may be replaced by the unit vector in the same
direction and the inequality remains true.
Repeat the argument for a sequence of balls Bi, each centered at b, whose
radii decrease to zero as i increases without bound each time producing yi in
Bi, a unit vector ηi, and an inequality corresponding to inequality (6.144).
Because the unit ball in a finite-dimensional space is compact, there is a
subsequence of these unit vectors that converges to some new unit vector
η. By the choice of the sequence of balls, the corresponding subsequence
of {yi}∞
i=1 converges to b. By continuity of the function (p, q) → 	p, q
 the
desired inequality corresponding to inequality (6.144) holds with the new
η and y replaced by b. This completes the proof of Case 1.
For simplicity, a proof for b a boundary point of a not necessarily closed
convex set is given for a special case (adequate for the application to the
controllability problem).
Case 2: C is symmetric; that is, if x is in this set so is −x. The proof strategy
is to show that b is a boundary point of the closure of the convex set and
then apply Case 1 to the closure (That b is a boundary point of the closure
of the convex set is true without the symmetry assumption; but not true
in general without the assumption of finite dimensionality.)
Suppose that b is not a boundary point of the closure of C. There is a ball
B centered at b and contained in the closure; that is, b is in the interior of
the closure. Choose Cartesian coordinates in Rn so that b is at the origin
and consider the usual basis vectors e1, e2, e3,...,en. Multiply each of them
by half the radius of B to form a new basis {vi}n
i=1 all of whose elements
are in B. Because each vi is in the closure of C, there is a sequence of
points {vij}∞
j=1 in C converging to vi. There is some positive integer k such
that {vik}n
i=1 is a basis of Rn (see exercise (6.102)) and by construction
each of the vectors in this basis is in C. By the symmetry assumption, each
−vik is also in C; and, by the convexity, the cube consisting of all linear
combinations
s1v1k + s2v2k + s3v3k + ··· + snvnk, −1 ≤ si ≤ 1
is in C. In particular, the origin (corresponding to b) is in the interior of
this cube and hence in the interior of C. In particular, b is not a boundary
point of C. This is a contradiction to the assumption that b is in the interior
of the closure and proves that in fact b is a boundary point of the closure
of C.6.7 Control 353
Apply Case 1 to the boundary point b of the closure of C. The resulting
inequality holds for point in this closure and in particular for points in the
original set C. ✷
A foundation has been laid for a proof of Theorem 6.89:
Proof. Suppose all points in Rn can be steered to the origin in finite time
and the control matrix G has rank less than n. There is a nonzero vector
v orthogonal to every column of G. By Proposition 6.92, v is orthogonal to
the range of the matrix etAB for every ∞ <t< ∞. But, because v can be
steered to the origin, there is some τ > 0 such that
v +
 τ
0
e−sABu(s) ds = 0.
Taking the inner product of both sides with respect to v leads to a contra￾diction.
For the converse, assume G has rank n and there is some z in Rn that
cannot be steered to the origin by an admissible control in some finite time.
Let C denote the set of points in Rn that can be steered to the origin in
finite time.
By the rank condition, B must have a nonzero column c. Choose the
control function that has constant value equal to the usual basis vector in
Rn corresponding to the column number of c so that the product of B and
this basis vector is c. By Proposition 6.86 there is a τ > 0 such that
x0 := −
 τ
0
e−sAc ds = 0.
This x0 is thus a nonzero vector in C that can be steered to the origin
in finite time τ . In particular C is not empty. Consider the closed ball at
the origin of radius |x0|. All points on the line through the origin in the
direction x0 and in the ball can be steered to the origin in finite time.
Likewise all points on the line through the origin in the direction z that are
in the ball except the origin cannot be steered to the origin in finite time.
The existence of x0 and z implies that the origin is a boundary point of
C because every ball centered at the origin contains nonzero points on the
lines through the origin in these directions.
The set C is convex and symmetric. To prove convexity suppose that x1
and x2 are in this set with x1 steered to the origin in time σ and x2 in
time τ . If these times are not equal, suppose with no loss of generality,
that τ>σ. Extend the control that steers x0 to the origin by defining the
extension to vanish for t>σ so that
eσAx2 +
 τ
0
e(σ−s)ABu2(s) ds = 0.354 6. Applications
Using the invertibility of exponentiated matrices,
eτAx2 +
 τ
0
e(τ−s)ABu2(s) ds = 0.
The desired result follows directly from using the family of admissible con￾trols given by u(s, α) = (1−α)u1(s)+αu2(s) with the parameter α defined
on the unit interval. A proof of symmetry is simple: use the negative of the
control that steers x0 to the origin.
By Theorem 6.93 there is a nonzero vector η such that
	x, η
 ≤ 0 (6.145)
for every x in C. Moreover, for every such x, there is some τ > 0 and an
admissible control u such that
	η, x
 +
 τ
0
	η, e−sABu(s)
 ds = 0. (6.146)
By the supposition that a support plane for C exists at the origin, the
integral term is not negative. To reach a contradiction it suffices to find
a point x in Rn that is steered to the origin in finite time for which the
corresponding integral term is negative.
Each admissible control steers infinitely many points in Rn to zero. In
fact, for a given control u and every τ > 0, the point
x := −
 τ
0
	η, e−sABu(s)
 ds
is in C. Thus, to reach a contradiction it suffices to construct an admissible
control so that after taking the inner product with η the resulting integral
term as in Eq. (6.146) is negative.
Consider the integrand of the integral in Eq. (6.146). Choose τ > 0 and
consider the open interval (0, τ ). The vector-valued function t → ηT e−tAB
defined on this interval is not the zero function. If it were zero, Proposi￾tion 6.92 implies ηT ApB = 0 for p = 0, 1, 2, 3,...,n − 1 contrary to the
hypothesis that the control matrix has full rank. By continuity, there is a
subinterval J on which this function is bounded away from zero. For an
appropriate positive constant a, the function u defined on J by
u(t) = −aηT e−tAB
and by u(t) = 0 otherwise is an admissible control. Moreover, for this
control
 τ
0
	η, e−sABu(s)
 ds = −a

J
	η, e−sABηT e−sAB
 ds
= −a

J
|ηT e−sAB|
2 ds < 0.
✷6.7 Control 355
The set of admissible controls in applications is usually restricted to the
subset defined by imposing a uniform bound on the absolute values of the
controls. Conjecture: There is an open set containing the origin in Rn and
a number M > 0 such that every initial value for system (6.140) in this
set can be steered to the origin in finite time with a measurable control
satisfying the bound |u(s)| < M if and only if the controllability matrix
has rank n. In fact, this conjecture is true (see Ex. 6.106). For this more
restrictive set of admissible controls, a feature from linear systems theory
provides global control.
Proposition 6.94. If all eigenvalues of the system matrix A for the control
system (6.140) have negative real parts and the n × (mn) (controllability)
matrix has rank n, then the system is controllable for the set of all measur￾able uniformly bounded controls.
Proof. Let the system evolve to an appropriate ball around the origin with
the constant zero control and then steer the final point to the origin. ✷
A hunting license for viable controls is provided at the price of checking
that the controllability matrix has full rank. But, no algorithm is given
for determining a desired control. Much more is known, but in general
determining a viable control can be a formidable problem especially for
control systems with high-dimensional state spaces.
The controllability matrix is important for several aspects of control
theory of linear systems. Consider, for example, the control system with a
scalar input u so that B is n × 1. In this case convention dictates denoting
this matrix as a vector b so that the controllability matrix is the n × n
matrix
G = (b, Ab, A2b, . . . , An−1b).
In this case, the system is controllable if and only if G is invertible.
Proposition 6.95. Suppose b is an n × 1-vector, e1, e2, e3,...,en is the
usual basis of Rn, and A is an n × n-matrix with characteristic polynomial
a0+a1λ+a2λ2+···+an−1λn−1+λn . (1) If x˙ = Ax+bu is controllable, then
the change of variables x = Gy transforms the control system to y˙ = Ay +
e1u where A is the matrix (partitioned by columns) (e2, e3, e4,...,en, cn)
and cn is the transpose of the vector (−a0, −a1, −a2,..., −an−1). (2) There
is a linear change of variables that transforms y˙ = Ay + e1u to z˙ = AT z +
enu.
Proof. To prove (1) note that the transformed system is ˙y = G−1AG +
G−1bu. By definition of matrix multiplication Ge1 = b, as required. To
determine A, note that it suffices to show AG = GA. The last column of
the matrix AG is Anb. Apply the Cayley-Hamilton theorem to show that
this vector is the transpose of (−a0b, −a1Ab, −a2A2b, . . . , −an−1An−1b).
The proof is completed using the definition of matrix multiplication.356 6. Applications
Consider a change of variables z = P y. It transforms ˙y = Ay + e1u to
z˙ = PAP −1z + P e1u. In general, a matrix is similar to its transpose; but
here the choice of P must transform e1 to en; that is, the first column of P
must be en. In fact, the desired transformation, partitioned by columns, is
P = (en, AT en,(AT )
2en,...,(AT )
n−1en).
To prove this fact, show first that PA = AT P. Using the abbreviation
S = −a0en − a1(AT )en − a2(AT )
2en −···− an−1(AT )
n−1en,
P A = (AT en,(AT )
2en,...,(AT )
n−1en, S).
By inspection, the first n − 1 columns of AT P are the same as those in
PA. Because a matrix and its transpose have the same characteristic poly￾nomial, the Cayley-Hamilton theorem implies that the last column is also
the same.
To complete the proof it suffices to show that P is invertible. Consider
the partition of AT by rows: (e2, e3, e4,...,en, cT
n ) and use it to determine
the structure of P. The first column of P is en, the second en−1+	cn, en
en,
the third en−2 + 	cn, en
(en−1 + 	cn, en
en), and so on. Note that at each
iteration, the leading vector is ej and it is added to a vector in the span
of {ej+1, ej+2, ej+3,...,en}. This later fact implies the columns of P are
linearly independent. ✷
A useful application of Proposition 6.95 is to the problem of stabilization:
Suppose the (full) state x of the system ˙x = Ax is available for measurement
at every instant of time. In this case, the choice of an m × n-matrix K
allows (full state) proportional feedback u = −Kx (with the minus sign by
convention) for control; that is, the proportional feedback control system is
x˙ = Ax−BKx. In applications to engineering problems, full state feedback
is rarely available because only a few state variables might be available for
measurement. But, full state feedback provides a special case of the general
theory.
Proposition 6.96. If the control system x˙ = Ax + bu, where b is n ×
1, is controllable, then the eigenvalues of the full state feedback controlled
system x˙ = Ax − bKx (that is, the eigenvalues of the matrix A − bK)
can be assigned arbitrarily with an appropriate choice of the 1 × n-matrix
K. In particular, the system can be stabilized by assigning eigenvalues with
negative real parts.
Proof. Proposition 6.95 implies that there is no loss of generality in con￾sidering the case (using the notation of this proposition) where the full state
feedback system has the form ˙z = AT z − enKPz, where P is an invertible
n×n matrix. Let P be partitioned by columns, say P = (P1, P2, P3,...,Pn)6.7 Control 357
so that enKP is the n × n matrix whose first n − 1 rows are all zero and
whose nth row is (	k, P1
,	k, P2
,	k, P3
,...,	k, Pn
). Because P is invert￾ible, this last row may be taken to be an arbitrary vector in Rn by an
appropriate choice of k. The last row of AT is the list of adjustable coeffi￾cients of the characteristic equation of this matrix. The matrix AT −enKP
has the same form, with the feature that the eigenvalues are determined
by the last row. By adjusting the elements of the last row, eigenvalues can
be assigned as desired. ✷
Theorem 6.97. If the control system x˙ = Ax + Bu, where b is n × m, is
controllable, then the eigenvalues of the full state feedback controlled system
x˙ = Ax − BKx can be assigned arbitrarily with an appropriate choice of
the m × n gain matrix K.
Proof. Use Proposition 6.96. ✷
A standard and important control problem is balancing a broom stick
(with the broom head vertical) on the tip of a finger. A moment of reflection
should suggest many less prosaic applications. Versions of this in the lit￾erature are called inverted pendulum problems. Students of control theory
are wise to return to these regularly to test understanding and advances in
knowledge.
An inverted pendulum is imagined to be a weight of mass m rigidly
affixed to one end of a massless rigid rod of length L. The other end of
the rod is imagined to freely move on a horizontal infinite plane without
friction. The problem is to determine if admissible motions of the end of
the rod restricted to the plane can be designed to stabilize the pendulum
in its vertical position. While the setting is not entirely physically real￾istic, solving the abstract problem should serve as an important step in
understanding engineered devices that perform the desired task.
For the classic approach taken here, the first step is the construction
of a viable mathematical model that respects the correct (but perhaps
approximate) application of physical laws. For pendulums, the physics is
well approximated by the classical mechanics encoded in Newton’s laws of
motion.
A fruitful approach is to imagine a path γ in space traced out by the
weight at the end of the rod as it evolves in time. Newton’s second law in
this case is mγ¨ = F, where F is the sum of the forces acting on the weight.
The important force is due to the gravitational field of Earth. In usual
Cartesian coordinates, suppose the weightless end of the rod is restricted to
move in the xy-coordinate plane; and, the vertical direction has coordinate
z. In these coordinates, the gravitational force (to a sufficiently accurate
approximation) is simply the column vector with coordinates (0, 0, −mg),
where g is the gravitational constant for motions near the surface of Earth.
A more delicate issue is the existence of a force f that holds the weight at
the end of the rod. In truth, this force is exceptionally complicated to write358 6. Applications
down from first principles: it is electromagnetic in origin. Fortunately, the
only relevant fact is that it acts in the direction of the rod.
The equation of motion is
mγ¨ =
⎛
⎝
0
0
−mg
⎞
⎠ + f.
Of many possible ways to proceed, a useful representation of the vector
function γ is as the sum of two vectors: the position vector with coordinates
(x, y, 0) of the end of the rod in the xy-plane and the vector with this base
point that points in the direction of the weight and has magnitude L, the
length of the rod.
The latter vector translated to the origin, whose length is constant during
the motion, is perhaps best represented in spherical coordinates. Because
the usual pole for spherical coordinates is the vertical axis, the standard
chart is not the best for the study of motions that take place near this
direction. An alternative is to employ spherical coordinates whose pole is
the x-coordinate axis. With φ the angle the direction vector T along the
rod in the direction of the weight makes with the positive polar axis and
θ the angle of its projection in the yz-plane with respect to the positive
y axis,
T =
⎛
⎝
cos φ
sin φ cos θ
sin φ sin θ
⎞
⎠
and the equation of motion is recast in the form
m
⎛
⎝
x¨
y¨
0
⎞
⎠ + mT¨ = −mg
⎛
⎝
0
0
1
⎞
⎠ + f.
The vector T is in the direction of the rod and the force f.
There are three key ideas for the next steps of the derivation: determine
an orthogonal basis for the coordinate space consisting of T and two addi￾tional vectors, reduce the equation of motion by taking its inner product
with these additional vectors, and noticing that these inner products with
the unknown force f both vanish.
A simple calculation can be used to prove that a desired basis is formed
by the three vectors
T , N :=
⎛
⎝
0
sin θ
− cos θ
⎞
⎠ , B :=
⎛
⎝
− sin φ
cos φ cos θ
cos φ sin θ
⎞
⎠ .
The inner products of both sides of the equation of motion with N and
B produce two linear equations for φ¨ and ¨θ. Solving for these variables6.7 Control 359
and simplifying—a rather long calculation that can be done with computer
algebra—produces a reduced second-order equation of motion, which in
first-order form is
φ˙ = v,
v˙ = 1
L(Lw2 cos φ sin φ − g cos φ sin θ) + 1
L(¨x sin φ − y¨cos φ cos θ),
˙
θ = w,
w˙ = − 1
Lsin φ(g cos θ + 2Lvw cos φ)+¨y
sin θ
Lsin φ. (6.147)
In case motion of the end of the rod in the xy-plane has zero acceleration,
the equation of motion of the weight in the yz-plane (given by φ = π/2) is
governed by the pendulum equation ¨θ + g/Lcos θ = 0, where the upward
vertical position corresponds to θ = π/2.
Controllability theory in this section is developed for linear systems; it
does not apply directly to the nonlinear equations of motion for the inverted
pendulum. Stability of nonlinear systems, however, is addressed by the prin￾ciple of linearized stability. Thus, a natural first approach to controllability
and stabilization is via linearization at the upward vertical position where
both angular variables have value π/2. This is a rest point for the uncon￾trolled system. Motions near this point should be well approximated by the
linearized system.
In abstract form (where x is now a 4-vector and f is the nonlinear func￾tion given by the right-hand side), the control system is
x˙ = f(x, u).
Write x0 for the upward vertical state. Because f(x0, 0) = 0,
x˙ = fx(x0, 0)(x − x0) + fu(x0, 0)u + R(x, u)
= A(x − x0) + Bu + R(x, u), (6.148)
where R is the nonlinear remainder containing no zero or first-order terms
in x − x0 and u. Thus, the linearized control system is given by
˙
ξ = Aξ + Bu, (6.149)
where
A :=
⎛
⎜⎜⎝
0100
g
L 000
0001
0 0 g
L 0
⎞
⎟⎟⎠
, B := 1
L
⎛
⎜⎜⎝
0 0
1 0
0 0
0 1
⎞
⎟⎟⎠
, u := 	x¨
y¨


.
Theorem 6.89 implies that the linearized system is controllable and Theo￾rem 6.97 that it is stabilizable.360 6. Applications
There are infinitely many choices for a full state control that stabilizes
the linear system (6.149). Each of these is given by a 2 × 4 matrix K so
that the closed loop system matrix is A − BK and
	x¨
y¨


= −K
⎛
⎜⎜⎝
ξ1
ξ2
ξ3
ξ4
⎞
⎟⎟⎠ .
The block form and symmetry of A suggests choosing
K =
	a b 0 0
0 0 a b 

.
At least this choice reduces the search for eigenvalues to solving quadratic
polynomial equations (compare, Ex. 6.107). A viable choice of a and b is
a = 2g, b = 2gL.
All eigenvalues of the system matrix of the controlled system with the
corresponding control are equal to −
g/L. Of course, the upward vertical
position of the pendulum under this control is a stable equilibrium.
Using the linear feedback control in system (6.148), the linear part of the
closed loop system
x˙ = A(x − x0) − BK(x − x0) + R(x, −K(x − x0))
is stabilized. The theory of nonlinear stability via linearization implies that
the nonlinear system is also stabilized in the sense that the rest point is
asymptotically stable. A motion of the controlled system that is initially
sufficiently close to upward vertical will go to the upward vertical position.
But, there is no guarantee that the rest point is globally asymptotically
stable. This is a question of robustness of the control for the nonlinear sys￾tem, a topic that is left for future study. Also no attention has yet been
paid to the admissibility of the stabilizing feedback control. For engineer￾ing applications, actuators are limited in their effects and these limitations
must be taken into account when building an automatic control. The appli￾cable mathematical theory, which has been partly introduced here, is well
established.
Exercise 6.98. Fill in the details of the calculation outlined in the proof of
Proposition 6.87.
Exercise 6.99. Prove the statement: An analytic function is zero on an interval
where a power series representation is valid if and only if all coefficients of the
power series vanish.6.7 Control 361
Exercise 6.100. Suppose that S is a set of linearly independent vectors in Rn.
(a) Show that there is some  > 0 such that replacement of each vector v in S
with a vector whose distance from v is at most  produced a new set of vectors
that is linearly independent.
Exercise 6.101. Prove the Cayley-Hamilton theorem. Hints: Reduce to Jordan
form and check that the theorem holds for Jordan blocks. A more interesting
proof is to check that the theorem holds for all diagonal matrices and, in turn, all
diagonalizable matrices. Over the complex numbers the diagonalizable matrices
are dense in the set of all matrices. The map from matrices to characteristic
polynomials is continuous.
Exercise 6.102. Suppose that {vi}n
i=1 is a basis of Rn. Prove that for suffi￾ciently small  > 0 if there is a set of vectors {wi}n
i=1 in Rn such that |wi −vi| < 
for each i, then {wi}n
i=1 is a basis. Hint: Consider the determinant of the matrix
whose columns are the elements of the set of vectors.
Exercise 6.103. Suppose a uniform boundedness condition is imposed on the
admissible controls: For some fixed M > 0 every control u must satisfy the
requirement |u(t)| < M for all t ≥ 0. Work with the scalar case to show that in
general ˙x = ax + bu is not controllable.
Exercise 6.104. (a) Show that the one-dimensional system ˙x = −x + u is
controllable with measurable controls all bounded in norm by M = 1/2. (b)
Show that the system
x˙ = Ax + Bu,
where
A =
⎛
⎝
110
0 −1 0
2 −1 0
⎞
⎠ , B =
⎛
⎝
1
0
1
⎞
⎠
is not controllable, but if the zero component in B is replaced by unity the system
is controllable. (c) In the former case, find a point that cannot be steered to the
origin in finite time with a measurable control. (d) In the latter case exhibit a
(1 × 3)-matrix K such that A − BK has eigenvalues all equal to −1.
Exercise 6.105. Consider the control system
x¨ + δx˙ − x + x3 = u, δ > 0
with perhaps unbounded control u, which has been used as a model of a long
slender steel beam whose tip is attached to two fixed magnets on a rigid assembly
that can be moved horizontally according to the values of the function u (see [119],
Sec. 2.2). (a) Write the model equation as a first-order system, linearize it at the
origin, and write the result in the standard form ˙x = Ax+Bu where x is the state
vector of the first-order system. (b) Show that the linear control system found
in part (a) is controllable. (c) Determine a feedback control that stabilizes the
linear system by placing eigenvalues at the real values −1 and −2. (d) Does the
feedback stabilization found in part (c), when applied to the nonlinear control
system, steer initial states to the origin? Discuss.362 6. Applications
Exercise 6.106. Prove the conjecture on page 355 using the theory in this
section.
Exercise 6.107. (a) Construct an algorithm, which takes as input a control￾lable pair of matrices (A, B) and a desired set of eigenvalues for A − BK, whose
output is a matrix K that produces the desired result. (b) Implement your algo￾rithm in code and verify that it produces viable results.
Exercise 6.108. The linearlized control system (6.147) is controllable. Its sta￾bilization has been discussed in this section. Discuss control protocols for moving
the inverted pendulum from some given nonvertical position to another. Con￾struction of examples that can be analyzed by hand or construction of computer
code for approximating the desired controls might be included.
Exercise 6.109. (a) Use the methodology leading to the model (6.147) to write
the equation of motion for a bead moving without friction on a circular hoop
that is rotated about a diameter by an actuator that imposes a desired angular
velocity. (b) Discuss controllability. (c) Is it possible to move the bead from rest
at the bottom of the hoop to some fixed position on the hoop by controlling the
angular velocity?
Exercise 6.110. Suppose an LTI is controllable. Can every point-to-point con￾trol task be accomplished with piecewise constant controls?
Exercise 6.111. [Stable Matrices] Tests for a matrix A to be stable (that is to
have all of its eigenvalues in the open left half of the complex plane) are of course
useful in stability theory for differential equations. In most applications where A
is an n×n matrix with n > 4, numerical methods are employed. But because the
nature of the eigenvalues of A is desired and not their exact values, useful mathe￾matics has been developed that provides stability tests. (a) Semyon Gershgorin’s
circle theorem states that every eigenvalue of A lies in a disk bounded by a circle
in the complex plane whose center is at one of the diagonal components of A and
whose radius is the sum of the absolute values of the other elements in the same
row. Use this result to state a stability test. Consider also presenting a detailed
proof of Gershgorin’s theorem. (b) Prove that if B is an n × n-matrix such that
BA + AT B + I = 0, then B is symmetric; and moreover, if B is positive definite,
then A is stable. Hint: Consider Lyapunov’s stability theory. (c) To check that
a symmetric matrix is positive definite, a special case of Routh-Hurwitz theory
is used: If all determinants of upper-left square submatrices (n of them for an
n×n matrix) are positive, then the matrix is positive definite. More involved (but
interesting) algebra is used in the Routh-Hurwitz test, which is closely related to
Sturm’s theory concerning roots of polynomials. Research this topic and write a
report with a description, proofs, and examples.
Exercise 6.112. Suppose ˙x = A(t)x + B(t)u is a control system, where the
system matrix A is non-trivially time dependent. (a) Discuss controllability for
the scalar case with the hypothesis that B is constant. Formulate and prove a
theorem or give a counterexample to a natural conjecture. (b) Same as part (a)
but for B non-trivially time dependent. (c) Do your results, perhaps with extra
hypotheses, for parts (a) and (b) generalize to the n × n case?6.7 Control 363
6.7.2 Optimal Control
For context, recall the inverted pendulum discussed in the last section and
consider a new task: Return the weight to its vertical upright position from
some perturbed position, move the base point of the rod to some specified
location, and stabilize the pendulum over the new base point in the upright
vertical position. Such a motion can, in principle, be executed in many pos￾sible ways by taking advantage of sensors that record the system’s state
and actuators that produce the motion. In an applied situation, there is
an operational cost due to fuel consumption, electrical power usage, labor,
and other factors. Feasible controls that minimize the total cost are usually
desirable. How can the lowest cost motion be determined? In other applica￾tions, for example, in finance, the goal is to maximize profit. In both cases,
the desired controls are called optimal and the subject that studies their
design and implementation is called optimal control.
A brief introduction to the vast subject of optimal control is presented
in this section as part of the theory and applications of ordinary differen￾tial equations, with attention to necessary conditions satisfied by optimal
controls, and culminating with the formulation and proof of a special case
of the Pontryagin maximum principle, arguably the most important theo￾retical result of the subject.
Cost Functions
Consider a (perhaps nonlinear) control system
x˙ = f(t, x, u). (6.150)
Suppose the instantaneous cost c is a function of t, x, and u; and there
is also a terminal cost K. The total cost of operation, depending on the
control and the time interval 0 ≤ t ≤ T , is defined to be
J =
 T
0
c(x(t), u(t), t) dt + K(T , x(T )). (6.151)
The problem is to minimize J over a specified set of admissible controls. An
important special case, called time-optimal control, occurs when the instan￾taneous cost is constant, there is no terminal cost, and the cost depends
only on the upper limit of integration T .
As should be expected for optimization problems, much work and most of
the basic theory is devoted to determining the necessary conditions for the
existence of optimal controls. An important and practical class of examples,
which is analyzed later using the theory developed here, is optimal control
for the linear control system in the standard form ˙x = Ax + Bu and cost
functionals of the form
J =
 T
0
x(t)
TQx(t) + u(t)
T Ru(t) dt + x(T )
T Sx(T )364 6. Applications
where Q, R, and S are constant matrices such that S and Q are symmetric
and positive definite. In this class, the instantaneous cost (which regulates
the control system) involves quadratic functions of the state and control.
Naturally, this is called a quadratic regulator problem.
Inspection of the functional J defined in Eq. (6.151) suggests that the
integrand be considered a Lagrangian and the control system a constraint
for the optimization problem. Indeed, the existence of a differential equation
constraint is the essential feature of optimal control.
A Hamiltonian formulation plays a key role in optimal control theory
(see [245]). Let L be a Lagrangian and recall the action functional Φ given
by
Φ(q) =  T
0
L(q, q, t ˙ ). (6.152)
In this classical context, the control Hamiltonian H : Rn×Rn×Rn×R → R
is defined by
H(p, q, u, t) = 	p, u
 − L(q, u, t), (6.153)
where the angled brackets denote the usual inner product. Here, u is simply
a free variable that is not necessarily equal to ˙q.
Proposition 6.113. The following statements are equivalent: (1) The func￾tion q is an extremal of the functional Φ in Eq. (6.152) that satisfies the
Legendre necessary condition. (2) There is a function p such that the pair
of functions p and q satisfy Hamilton’s equations
q˙ = Hp(p, q, q, t ˙ ), p˙ = −Hq(p, q, q, t ˙ ), Hu(p, q, q, t ˙ )=0
for the control Hamiltonian (6.153), and the quadratic form
Huu(p, q, q, t ˙ )
is negative semi-definite.
Proof. Suppose that (1) is true. By the definition of the (control) Hamil￾tonian Hp(p, q, u, t) = u. Thus, with u = ˙q,
q˙ = Hp(p, q, q, t ˙ ).
Define the function p by p(t) = Lu(q(t), q˙(t), t) and note that Hq(p, q, u, t) =
−Lq(p, u, t) (where in this formula p, q, and u are variables in the domain of
H). Inserting the functions p and q and using the Euler-Lagrange equation
satisfied by the extremal q,
p˙ = d
dtLu(q(t), q˙(t), t) = −Lq(q(t), q˙(t), t) = −Hq(p(t), q(t), q, t ˙ ).6.7 Control 365
Because Hu(p, q, u, t) = p − Lu(q, u, t), we have that
Hu(p(t), q(t), q˙(t), t)=0
and the semi-definite requirement follows from the identity Huu(p, q, u, t) =
−Luu(q, u, t).
Suppose that (2) is true. Let u = ˙q, use the definition of H, and compute
Lq = −p, L ˙ q˙ = Hu − p.
By (2) the time derivative of Hu vanishes. This implies that L satisfies
the Euler-Lagrange equation. As before the Legendre necessary condition
follows from the property of Huu. ✷
As so beautifully discussed in [245], the informal statements Hu = 0
and Huu ≤ 0 suggest that H should have a maximum as a function of
the variable u in Rn; or, in more precise language, there should be a new
necessary condition for an extremal: the function defined on some subset
U ⊆ Rn with range in R given by u → H(p(t), q(t), u, t) should have a
maximum at ˙q(t) for each t. The latter condition is usually written
max
u∈U
H(p(t), q(t), u, t) = H(p(t), q(t), q˙(t), t).
In proper context, this equality is true. In fact, it is true in much more
generality. The next statement is the culmination of much of the classical
theory of optimal control and certainly one of the most practical achieve￾ments of twentieth-century applied mathematics.
Principle 6.114 (Pontryagin Maximum Principle). A minimum of
the functional
J(x, u) =  T
0
L(x, u, t) dt
subject to the constraint x˙ = f(x, u, t) and boundary conditions x(0) = x0
and x(T ) = x1 must satisfy the following conditions. There exists a function
p, called the costate, and a constant a ≥ 0 (the abnormal multiplier) such
that
(1) for each 0 ≤ t ≤ T , the pair (p(t), a) = (0, 0);
(2) for the control Hamiltonian defined by
H(p, q, u, a, t) = 	p, f(q, u, t)
 − aL(q, u, t),
the control Hamiltonian equations
q˙(t) = Hp(p(t), q(t), u(t), a, t), p˙(t) = −Hq(p(t), q(t), u(t), a, t)366 6. Applications
are satisfied on the interval 0 ≤ t ≤ T ; and,
(3) for each t in the same interval, the control u satisfies the maximum
condition
maxv H(p(t), q(t), v, a, t) = H(p(t), q(t), u(t), a, t)
where the vector v is ranges over the admissible values of the control.
Moreover, for the similar problem where the Lagrangian L and the vec￾tor field f are autonomous and the final time T is not specified (as in
time-optimal control), the necessary conditions are the same but with the
additional requirement that, for 0 ≤ t ≤ T ,
H(p(t), q(t), u(t), a)=0.
Precise statements and proofs of various versions of the maximum princi￾ple are given in books on optimal control theory (see, for example, [34],[95],
[158], [163], or [233]). Different hypotheses on the state, the control, and the
end condition may be imposed. In fact, formulating and proving theorems
that extend the applicability of the maximum principle has been a grand
challenge that has been met with great success. The full theory is used in
solving a range of practical problems. An elementary, but useful, theorem
of this type is formulated and proved here.
A change of notation, in keeping with the specific optimization problem,
is used. The state variable is changed from the general x to q so that the
dynamical constraint is ˙q = f(q, u, t) and the Lagrangian L is defined to
be a function of the variables (q, u, t). The optimal control problem is to
minimize
J(q, u) =  T
0
L(q, u, t) dt + K(T , q(T )) (6.154)
subject to the constraint ˙q = f(q, u, t) with boundary conditions.
Functionals such as (6.154) that contain end-state costs K(T , q(T )) for
some function K : R × Rn → R, are said to be in Bolza form; those that
contain such an end-state cost but with no running cost (L = 0) are in
Mayer form. A useful trick is to turn Bolza-form optimization problems
into Mayer form. To do this, introduce a new scalar variable y and a new
optimization problem: minimize
J(q, y, u) = K(T , q(T )) + y(T ) (6.155)
subject to the constraints ˙q = f(q, u, t) with boundary conditions and
y˙ = L(q, u, t) with y(0) = 0. The new Mayer form optimization problem is
equivalent to the Bolza-form problem. To prove one direction, suppose that
the Mayer form problem has a solution (q, u) and integrate both sides of the
scalar differential equation for y from 0 to T . At least for the formulations
discussed here, there is thus no loss of generality when restricting to Mayer
form. A simple case is discussed in the remainder of this section.6.7 Control 367
Problem 6.115. [Mayer Problem] For some specified set of admissible con￾trols u : [0, T ] → Rm and corresponding solutions t → q(t, u(t)) of the
initial value problem
q˙ = f(q, u, t), q(0) = q0,
minimize the functional
J(u) = K(q(T , u(T )).
To be practical (as previously mentioned), the admissible controls are
usually taken to be the subset U of all measurable functions defined on the
interval [0, T ] that are uniformly bounded by some fixed constant. More
generally, all functions have their range in some fixed subset U of Rm. This
generality adds a complication that has not been addressed: the function
t → f(q(t), u(t), t) is not necessarily continuous. Thus an existence theory
more general than discussed so far is needed to define what is meant by
a solution of the initial value problem. Such a theory exists. As a step in
this direction, the dynamical constraint may be replaced by the integral
equation
q(t) = q0 +
 t
0
f(q(s), u(s), s) ds
so that the requirement is integrability of s → f(q(s), u(s), s). A function
that is differentiable almost everywhere and Lebesgue integrable is called
absolutely continuous. The main theorem of the subject is the Carath´eodory
existence theorem. It states that an absolutely continuous solution exists
(locally of course) for the initial value problem when f is continuous in its
first and second variables and merely measurable in its third provided that
f over the domain of interest is bounded above by a Lebesgue integrable
function. For this section piecewise continuous functions suffice.
The next theorem states the promised simplified version of the Pontrya￾gin maximum principle.
Theorem 6.116. Suppose the state-control pair u and q solves the Mayer
problem (6.115), where K and f are continuously differentiable, U is the
nonempty subset of Rm containing the range of each admissible control,
and p (a row vector also called a covector) is the solution of the costate
final value problem
p˙(t) = −p(t)fq(q, u, t), p(T ) = −DK(q(T , u(T ))), (6.156)
then
max
v∈U p(t)f(q(t), v, t) = p(t)f(q(t), u(t), t)
for (almost every) 0 ≤ t ≤ T .368 6. Applications
See Ex. (6.117) for a check on the formulation of this theorem.
Proof. For fixed but arbitrary τ in the interval (0, T ] and v ∈ U, let ¯u :
[0, τ ) → U be the curve of admissible controls such that ¯u(0) = u (the
optimal control); and, for 0 <s<τ , define ¯u such that ¯u(s)(t) = u(t)
except on the interval τ − s ≤ t ≤ τ where it has the constant value v;
that is, ¯u(s)(t) = v. Variations of this type are called needle (or simple)
variations. As s goes to zero, the corresponding controls ¯u(s) converges to
u in the L1-norm; that is,
lim
s→0+
 T
0
|u¯(s)(t) − u(t)| dt = 0.
Let ¯q denote the corresponding curve of states obtained by solving the
family of constraint equations d
dt q¯(s) = f(¯q(s), u¯(s)(t), t) with initial con￾dition ¯q(s)(0) = q0.
Claim:
d
dsJ(¯u(s))


s=0 = −p(τ )(f(q(τ ), v, τ ) − f(q(τ ), u(τ ), τ )),
where for simplicity of notation the derivative with respect to s evaluated
at s = 0 is written in place of the right-hand derivative.
Accepting the claim and using the optimality of the pair u and q, the
function s → J(¯u(s)) cannot decrease with s, at least for sufficiently small
s > 0. Thus, the one-sided derivative must be nonnegative and, as a result,
−p(τ )(f(q(τ ), v, τ ) − f(q(τ ), u(τ ), τ )) ≥ 0.
But, this is the desired result: The control Hamiltonian is given by pf(q, u, t),
the vector v may be taken to be an arbitrary point in U, and the inequality
implies that
p(τ )f(q(τ ), u(τ ), τ )) ≥ p(τ )f(q(τ ), v, τ ),
as required.
It remains to prove the claim.
For each s, the state ¯q(s) corresponding to the control ¯u(s) and the
optimal state q corresponding to the optimal control u both satisfy the
state equation (with different controls) but with the same initial condi￾tion ¯q(s)(0) = q0. Consider the difference of these states; that is, (t, s) →
q¯(s)(t)−q(t). If t ≤ τ −s the control ¯u(s)(t) has value u(t) by the definition
of this needle variation. Hence, the solutions of the state equation with the
same initial condition for each of these controls are identical. In particular,
the desired difference vanishes. If τ − s ≤ t ≤ τ the two controls differ, but
the difference in the initial data (starting at t = τ − s instead of t = 0)6.7 Control 369
for the two-state equations still vanishes. By integrating both sides of the
differential equations, subtracting, and evaluating at t = τ ,
q¯(s)(τ ) − q(τ ) =  τ
τ−s
(f(¯q(s)(t), v, t) − f(q(t), u(t), t)) dt.
The right-hand side vanishes at s = 0. Its dependence on s is differen￾tiable because the state equation with constant control satisfies the usual
hypotheses due to the (very clever) construction and use of the needle vari￾ation. In case f is continuous and continuously differentiable in its first
variable (tacitly assumed), the desired derivative at s = 0 is computed
using calculus to obtain the expansion
q¯(s)(τ ) − q(τ )=(f(¯q(0)(τ ), v, τ ) − f(q(τ ), u(τ ), τ ))s + o(s)
= (f(q(τ ), v, τ ) − f(q(τ ), u(τ ), τ ))s + o(s). (6.157)
For t ≥ τ , Eq. (6.157) specifies the difference of the initial data for ¯q(s)
and q at t = τ ; and, using the expansion, the (right-hand) derivative of
s → q¯(s)(τ ) with respect to s at s = 0 is
q¯

(0)(τ ) = f(q(τ ), v, τ ) − f(q(τ ), u(τ ), τ ). (6.158)
The difference of the two states for t ≥ τ is again given by the usual
sum of initial data and an integral. By expanding the integrand, using the
usual manipulation that leads to the first variational equation, and using
Eq. (6.158),
q¯(s)(t) − q(t)=(f(q(τ ), v, τ ) − f(q(τ ), u(τ ), τ ))s + o(s)
+
 t
τ
(f(¯q(s)(r), u(r), r) − f(q(r), u(r), r)) dr
= (f(q(τ ), v, τ ) − f(q(τ ), u(τ ), τ ))s
+ s
 t
τ
fq(q(r), u(r), r)¯q
(0)(r) dr + o(s)
= (f(q(τ ), v, τ ) − f(q(τ ), u(τ ), τ ))s
+ s
 t
τ
∂
∂r q¯

(0)(r) dr + o(s)
=¯q
(0)(t)s + o(s). (6.159)
In particular, ¯q(s)(T ) = q(T )+¯q
(0)(T )s + o(s). Using it and Eq. (6.158),
d
dsJ(¯q(s))


s=0 = DK(q(T , u(T ))¯q
(0)(T ). (6.160)
The function t → q¯
(0)(t) is the solution of the variational initial value
problem
W˙ = fq(q(t), u(t), t)W, W(τ ) = f(q(τ ), v, τ ) − f(q(τ ), u(τ ), τ ).370 6. Applications
By hypothesis the costate p is the solution of the final value problem (6.156).
Note that
d
dtpW = −pfq(q(t), u(t), t)W + pfq(q(t), u(t), t)W = 0
and, because pW is constant,
p(t)¯q
(0)(t) ≡ p(T )¯q
(0)(T ) = −DK(q(T , u(T ))¯q
(0)(T )
and
p(t)¯q
(0)(t) ≡ p(τ )(f(q(τ ), v, τ ) − f(q(τ ), u(τ ), τ )).
Thus, in view of Eq. (6.160),
d
dsJ(¯q(s))


s=0 = DK(q(T , u(T ))¯q
(0)(T )
= −p(τ )(f(q(τ ), v, τ ) − f(q(τ ), u(τ ), τ )),
as required. ✷
Exercise 6.117. As a general procedure in formulating theorems, sanity checks
are always a good idea: Construct a simple example and check that the theorem
gives the correct result. (a) Check the validity Theorem 6.116 for the problem
of minimizing (q(1))2/2 with the dynamical constraint ˙q = u, initial condition
q(0) = 2, and the control constraint −1 ≤ u(t) ≤ 1. (b) Repeat (a) but with initial
condition q(0) = 1. (c) Repeat (a) but with control constraint −1 < u(t) < 1.
Exercise 6.118. Find the maximum value of
 1
0
x(t) + u(t) dt
subject to the dynamic constraint of ˙x = 1−u(t)
2 with initial condition x(0) = 1.
Consider two cases: unlimited controls and controls that are uniformly bounded.
For definiteness use the uniform bound |u(t)| ≤ 1. Hint: change the maximization
problem into a minimization problem.
Exercise 6.119. Why o(s) and not O(s2) in Eq. (6.157)?
Exercise 6.120. Let L be the Lagrangian L(q, q˙)= ˙q2/2−q2/2. (a) Determine
the Euler-Lagrange equation. (b) Show that the Legendre necessary condition is
satisfied. (c) Use part (b) to show that  π
0 q2 dt ≤  π
0 q˙
2 dt. (d) Recall Poincar´e’s
inequality. To what extent can it be proved using the idea suggested by the result
in part (c)? Discuss.
Exercise 6.121. An initial investment of x0 dollars in an account with con￾tinuously compounded interest at rate r and continuous continued investment u
grows according to the model initial value problem ˙x = rx + u and x(0) = x0.6.7 Control 371
After some period of time of length T the goal is to minimize  T
0 u(t) dt − x(T ).
The rate of continued investment should be bounded by some number M > 0.
Allow for negative interest rates and negative investments (withdrawals). (a) In
case r is constant, the best strategy is to invest continuously at rate M when
r ≥ 0 and divest at the same rate when r < 0. Does this strategy agree with
optimal control theory? (b) What is the best strategy when the interest rate is
variable with possibly some negative values. Discuss.
6.7.3 Quadratic Regulator
Let Q and S be symmetric n × n matrices and R an m × m symmetric
positive-definite matrix. Also, let U be a subset of Rm and U the set of
bounded measurable controls each defined on the interval [0, T ] with range
in U. The quadratic regulator problem is to minimize the functional J :
U → R given by
J(u) = x(T )
T Sx(T ) +  T
0
x(t)
TQx(t) + u(t)
T Ru(t) dt, (6.161)
where the state x is subject to the usual linear constraint ˙x = Ax + Bu
and initial condition x(0) = x0. In this section, for simplicity, U = Rm.
Under the imposed conditions, cost rises as the state x drifts away from
the origin or the control drifts from the zero control. Also, the end state
is penalized according to its distance from zero. So in the special form
considered here, the interpretation is to move a given initial state x(0) = x0
to the vicinity of the origin and keep it there with minimal cost. Of course
similar problems for moving a state and maintaining its distance from some
nonzero state reduces to the stated problem by a change of coordinates.
There are several possible approaches to solving quadratic regulator
problems. Of these, the closest to the calculus of variations is of course the
maximum principle. An important note: In all but the simplest applications
explicit solutions are not available and even if they were they would be too
complex to be practical. The theory is important for providing insight and
as the framework for constructing viable algorithms that can be used to
make numerical approximations (see, for example, [197], [205], and [256]).
To use Theorem 6.116, transform to the Mayer form where the new
functional (again denoted J) is given by
J(u) = x(T )
T Sx(T ) + y(T)
and constrained by the augmented state initial value problem
x˙ = Ax + Bu, x(0) = x0,
y˙ = x(t)
TQx(t) + u(t)
T Ru(t), y(0) = 0.372 6. Applications
The costate final value problem is
( ˙p1, p˙2) = −(p1, p2)
	 A 0
2xTQ 0


, (p1(T ), p2(T )) = −(2x(T )
T S, 1),
and the maximum principle states that
(p1(t), p2(t)) 	 Ax(t) + Bu(t),
x(t)TQx(t) + u(t)T Ru(t)


=
max
v∈U (p1(t), p2(t)) 	 Ax(t) + Bv,
x(t)TQx(t) + vT Rv 

.
The second component of the costate final-time problem implies that
p2(t) ≡ −1.
Thus it suffices to maximize
v → p1(t)Bv − vT Rv.
Using calculus and taking into account that a positive-definite symmetric
matrix is invertible, the unique maximum v in Rm is defined by
vT = 1
2
p1(t)BR−1.
It determines the desired optimal control u given by
u(t)
T = 1
2
p1(t)BR−1. (6.162)
To be useful, this result must be complemented by identifying p1 and the
corresponding optimal state x.
Using the optimal u and the symmetry of the system matrices, the state￾costate initial-final value problem is
x˙ = Ax +
1
2
BR−1BT pT
1 , x(0) = x0,
y˙ = xTQx +
1
4
p1B2R−1BT pT
1 , y(0) = 0,
p˙1 = −p1A + 2xTQ, p1(T ) = −2x(T )
T S.
(6.163)
Because the augmenting state variable y decouples from system (6.163),
it can be ignored. The remaining linear system for x and p1 consists of
m + n differential equations with m + n boundary conditions. Thus, a
unique solution is expected.
Important fact: Up to this point in the derivation, results would be the
same for the more general problem where matrices are time dependent.
For both cases, an algorithm for approximating solutions is suggested by6.7 Control 373
the form of the boundary value problem: Choose p1(0), solve or approxi￾mate the solution of the system of equations as an initial value problem, and
check the norm of the difference between the computed costate at t = T and
the necessary final costate. Stop when this difference is sufficiently small for
the application and use the computed approximation to approximate the
optimal control. If not, repeat the process (with perhaps a judicious update
of the choice of the initial costate). Continue until a sufficiently small dif￾ference is obtained or the computation goes on too long without making
progress toward decreasing the difference. Of course, similar ideas can be
used to approximate solutions of more general optimal control problems.
This leads to a viable method. But, there are several other methods based
on different theories that are also used. As usual in applied mathematics, a
solution method should be chosen after careful consideration of the special
properties that might be present for the desired application.
To gain some insight and to solve a quadratic regular problem, consider
the case where the state and controls are scalar functions. With obviously
chosen new notation, the boundary value problem to be solved is
x˙ = ax +
b2
2r
p1, x(0) = x0,
p˙1 = −ap1 + 2qx, p1(T ) = −2sx(T ),
(6.164)
where a, b, r, s, and q are positive. This system is not arbitrary, it comes
from a control system. The optimal control, if it exists, should depend on
the state and the control system is linear. From the theory of controllability,
an obvious choice for the control is state feedback u(t) = −k(t)x(t) for
some unknown scalar function k. The minus sign is there, as before, to
honor tradition. By Eq. (6.162),
−kx = b
2r
p1. (6.165)
This latter equality suggests that k might be obtained as the solution of a
differential equation. In case x0 = 0, an optimal control is u = 0 and the
problem is solved. Otherwise division by x(t) (at least for sufficiently small
t > 0) is justified and an easy calculation can be used to show that
˙
k = −bq
r − 2ak + bk2. (6.166)
From the relation (6.165) and the final value of the costate,
k(T ) = bs
r .
Given k, the optimal state x is determined by substitution for p1 = −2krx/b
or u = −kx in the corresponding form of the state equation.374 6. Applications
The differential equation for k is a Riccati equation, a type of nonlinear
differential equation that played an important role in the historical devel￾opment of the theory of ordinary differential equations and is discussed
in Ex. 5.35. It plays a central role in proving the desired optimal control
exists.
For the scalar constant coefficient case, the main issue for proving an
optimal control exists is to show that the Riccati equation has a bounded
solution with the given value at the final time. Recall that differential equa￾tions akin to ˙x = x2 have solutions that blow up in finite time. This possi￾bility must be avoided. Under the hypotheses in this section on its system
parameters, the one-dimensional autonomous dynamical system given by
Eq. (6.166) has two real rest points: a negative stable rest point and a pos￾itive unstable rest point. Both are hyperbolic. The desired terminal value
is positive. Thus, starting with an appropriately chosen initial value on or
near the positive rest point, the terminal value is reached exactly at the
specified terminal time. This is the key step in proving an optimal control
exists.
In case the coefficients of the Riccati equation are time-dependent or the
state-costate system is not scalar, existence is more difficult to prove.
For the matrix case, recall the route to the Riccati equation for the scalar
case starting with Eq. (6.165). It suggests that perhaps there is a matrix
valued function P defined on [0, T ] such that
pT
1 = −P x.
Substitution of this relation into the costate equation followed by the same
substitution in the state equation (6.163) are used to obtain the matrix,
final state, Riccati problem
P˙ = −2Q − (AT P + P A) + 1
2
PBR−1BT P, P(T )=2S. (6.167)
As before, if a solution of this final value problem exists on the interval
[0, T ], the optimal feedback control is
u = −1
2
R−1BT P x, (6.168)
and the optimal state is obtained by solving the state equation with this
control.
Exercise 6.122. (a) For definiteness, suppose that a, b, r, s, q, x0, and T are
all equal to one. Solve the scalar quadratic regulator problem (with state-costate
system (6.164)) for this case, specify the optimal control, the optimal state, and
the value of the functional at the optimal control. (b) Discuss your impressions
about solving optimal control problems. (c) Suppose the running state and control
are not penalized; that is, q and r are set to zero but the other parameters remain
the same. Solve the optimal control problem and compare the solution with the
result in part (a).6.7 Control 375
Exercise 6.123. (a) Suppose that B = 0. Discuss the quadratic regulator the￾ory for this degenerate case. (b) More generally, perhaps the system is not con￾trollable. Discuss the theory in this case. What can go wrong?
Exercise 6.124. Consider the scalar (constant coefficient) quadratic regulator
problem. An approximation of the optimal feedback gain might be obtained by
choosing a rest point of the Riccati equation instead of the solution of the final
value problem. This idea produces a constant gain and therefore a constant coef￾ficient linear state equation. Is the zero solution asymptotically stable when this
method is implemented? Formulate and prove a theorem.
Exercise 6.125. Prove (under the hypothesis in this section) that the desired
solution of the final value problem (6.167) is a symmetric matrix.
Exercise 6.126. Consider the matrix case for the quadratic regulator. With the
notation of this section and Ex. (6.125), solutions of the matrix Riccati equation
are symmetric. (a) Assume that P is a positive-definite rest point of the Riccati
equation and Q is positive definite. Prove that the origin is asymptotically stable
for the (closed loop) state equation with the associated control u as in Eq. (6.168).
Hint: A viable approach is to check that V (y)=1/2yT P y for y in Rn is a
Lyapunov function. (b) Is there a similar stability result in case Q is positive
semi-definite? (c) When does a positive-definite rest point (a solution of the
algebraic Riccati equation) exist? Explore this question in the universe of 2 × 2
matrices and report on your observations. Formulate and prove a theorem. Or,
find a relevant result in the literature and write a complete proof.
6.7.4 Optimal Control Example
A population (total weight of fish in a lake) w might be modeled at the
phenomenological level, which is a good place to start, by the logistic dif￾ferential equation
w˙ = αw(1 − w/K) − β + σ,
where the temporal variable τ is measured in some convenient unit (perhaps
years), α is a rate constant, K is the expected carrying capacity of the
environment, β is a rate constant measuring reductions in the population
due to death, predation, pollution, fishing, and so on; and, σ is a rate
constant measuring increases in the population due to stocking. The current
population is w(0) = w0 > 0. Population control via stocking is desired over
a time interval [0, τf ] to sustain a population near the carrying capacity at
the least possible cost. The optimization problem is modeled, by assuming
w and σ are continuous variables, by minimizing the integral
 τf
0
(1 − )
(w − K)2
K2 +  σ2
α2K2 dτ
over all (stocking protocols) σ : [0, T ] → [0,∞], where (x − K)2 and σ2
are made dimensionless so that comparison is meaningful and 0 ≤  ≤ 1376 6. Applications
is chosen according to the relative importance of maintaining the desired
population and lowering cost.
The control system is made dimensionless and the state is translated to
make the desired state correspond to zero via the change of variables
t = ατ, x = 1
K (w − K), u = 1
αK (σ − β),
δ = β
αK , T = ατf , x0 = 1
K (w0 − K), q = 1 − , r = 
(6.169)
that transforms the optimization model to the problem of minimizing the
functional J subject to the transformed dynamical constraint given by
J(u) =  T
0
qx2 + ru2 dt,
x˙ = −x(1 + x) − δ + u, x(0) = x0 > −1. (6.170)
Under the condition that r = 0, a change to Mayer form allows an appli￾cation of Theorem 6.116 to determine necessary conditions for optimality.
In fact, the boundary value system
x˙ = −x(1 + x) − δ + p
2r
, x(0) = x0,
p˙ = p(1 + 2x)+2qx, p(T )=0,
(6.171)
has a solution on the interval [0, T ] and the optimal control is given by
u = p
2r
. (6.172)
Inspection of system (6.171) suggests that finding a formula for the opti￾mal control is at least a difficult if not impossible task. As mentioned before,
even for simple control problems, writing an analytic formula for the opti￾mal control as a function of time is usually not possible. This is a very good
reason to study efficient methods of numerical approximation.
As in the analysis of the inverted pendulum in the section on control￾lability, another type of approximation is often used (especially for large￾scale systems where computational cost is taken into account): Linearize
the problem at the desired state and use the resulting (linear) optimal
control as an approximation of the desired optimal control. This is a par￾ticularly great idea when the control is applied over a short time, the state
is reassessed, a new linear control is computed, and the process is repeated
until the desired final time is reached. Optimal estimation of the current
state using the model equation and measurements of the actual current
state (especially when noise is taken into account) leads to another impor￾tant theory (called state estimation or data assimiliation) where a key idea
is the Kalman filter.6.7 Control 377
In the problem at hand, the desired state is x = 0. Linearization at this
state produces the dynamic constraint
x¯˙ = −x¯ − δ + u, x¯(0) = x0, (6.173)
costate final value problem
p¯˙ = ¯p − δ + u, p¯(T )=0,
and optimal control
u¯ = p¯
2r
.
It can be used to approximate the optimal control (6.172).
Exercise 6.127. What are appropriate units for q and r?
Exercise 6.128. Derive the optimization problem (6.170), show that all param￾eters and variables are dimensionless, and show that a solution of the transformed
problem can be used to produce a solution of the original stocking problem.
Exercise 6.129. Derive system (6.171) and Eq. (6.172) with reference to The￾orem 6.116.
Exercise 6.130. Formulate and prove a corollary of Theorem 6.116 where the
functional is in Bolza form.
Exercise 6.131. Set δ = 0. Sketch the phase portrait of system (6.171) and use
it to argue that the necessary condition is satisfied for some choice of p(0) = p0.
Exercise 6.132. Derive the linearized state equation (6.173) and corresponding
costate and optimal control.
Exercise 6.133. For the applied control problem in dimensionless form, x0 >
−1; that is, the natural state of the (fish) population is not zero. Show that
δ = 1/4 is a critical value for the viability of the population without intervention
via stocking.
Exercise 6.134. For the dimensionless optimal stocking problem, let x0 =
−1/4, T = 1/2, δ = 1/8, q = 5/9, and r = 4/9. Determine the costate initial
value that solves the necessary state-costate boundary value problem for the
linearized state constraint.
Exercise 6.135. (a) Consider the control system ¨x = u with x(0) = 0, a fixed
terminal time T > 0, and the cost function u → −x(T ) +  T
0 u(t)
2 dt. Determine
the optimal control. (b) Repeat part (a) with the cost function u → −x(T ) +
 T
0 u(t) dt. (c) Compare and contrast the solutions of (a) and (b).
Exercise 6.136. Consider, for 0 <a< 1, the control system
x˙ = −ax + xu, y˙ = (1 − u)x, x(0) = 1, y(0) = 0378 6. Applications
subjected to the control constraint 0 ≤ u(t) ≤ 1 and, for T = −2/a ln(1 − a),
the task of minimizing −y(T ). (a) Would the optimal control problem have the
same answer if it had been stated with the task of maximizing y(T ). Explain. (b)
Write the state-costate boundary value problem. (c) Write the maximum principle
maximization problem and show that the optimal control must be bang-bang;
that is, the control must take on only its boundary values, which in this case are
zero and one. Also state the condition that determines which boundary value is
to be chosen at a given time during the evolution. (d) When t = T determine
the control value from (b) and show that there is a time ts such that the control
does not change on the interval 0 ≤ ts ≤ t ≤ T . (e) Assume ts is the minimum
choice for the left endpoint of the interval in part (c). Solve the costate equations
over the interval in (c) and use the solution to determine ts. Show that at this
point a switch in the bang-bang control occurs. Thus the optimal control is not
continous. (f) Determine the optimal value of −y(T ).
Exercise 6.137. The Benjamin Gompertz (1938) tumor growth model is N˙ =
rN ln K
N . where N denotes the number of cancer cells, r the growth rate, and K
the maximum tumor size limited by its nutrient supply. Treatment by chemother￾apy might introduce a drug in the amount A/A0, where A0 is some normalization
factor, so that the rate of cell growth is decreased by some rate k. The control
system is then
N˙ = rN ln K
N − kN A
A0
.
In treatment the objective is to minimize the number of cancer cells using the
minimal amount of chemotherapy. A simple model cost function is
J = N(τ )
K + a
 τ
0
A(t)
A0
dt,
where a is again a rate and τ is the duration of the treatment. (a) Derive a dimen￾sionless form of the control system and objective function. (b) Convert to Mayer
form. (c) Write the costate equations and their terminal conditions. (d) Write the
Hamiltonian and discuss the application of the maximum principle. Explain why
a pencil-and-paper solution is unlikely. (e) (Numerics Project) Specify reasonable
parameter values and approximate the corresponding optimal control.
Exercise 6.138. Consider the control system
x˙ = x + y + 3u, y˙ = −y + 4u, x(0) = 0, y(0) = 0
with piecewise continuous control u : [0, 1] → [−1, 1]. (a) Prove that the point
in the plane with coordinates (ξ, η) = (10, 3) cannot be reached by the control
system. (b) Determine the minimum of (x(1) − ξ)
2 + (y(1) − η)
2 over the admis￾sible controls via an argument using the maximum principle and the structure
of the control system. (c) Define a function of two real variables whose zero set
contains the desired end state (x(1), y(1)) that minimize the functional of part
(b) but with (ξ, η) = (10, 3/2). Hint: Write out the state and costate equations
with all boundary conditions and incorporate the maximum principle as a func￾tion of the costate variables. The resulting boundary value problem should have
the same number of equations and boundary conditions. Hence it should be solv￾able. The desired function maps the optimal (x(1), y(1)) to (0, 0). (d) (Numerics6.7 Control 379
L h
Figure 6.9: Bead on wire.
Project) Find (x(1), y(1)) in part (c) with an error less than 10−3. Hint: the clas￾sical approach uses the function in part (c). There are other possibilities worth
exploring.
Exercise 6.139. (Numerics) (a) Write a compute code that produces viable
approximations for the values of p(0) = p0 for the costate in system (6.171) such
that for given parameter values and x0 the corresponding solution of the sys￾tem produces p(T ) within some specified distance from the origin. Compare the
linearized optimal control and optimal state to the optimal control and optimal
state. Discuss your observations about the viability of the approximation. For def￾initeness include numerical experiments with parameter values δ ∈ {0.2, 0.26},
T = 2.0, r = 0.8, q = 1 − r, and x0 = {−0.25, 0.1}. Interpret your results in a
manner suitable for presentation to an agency considering a stocking program.
Hint: State-costate phase portraits as in Ex. (6.131) are helpful in understand￾ing why solutions exist and how to choose initial guesses for p0. (b) Fishing
might be the main contributor to the size of β. Treat control of fishing with
no stocking as an optimal control problem for the same parameter regime. (c)
Applications of optimal control to fishery management and its economics have
been made with models not much more complicated than the one considered in
this exercise. Find an interesting example in the literature and write a detailed
report on its analysis. (d) Note that the methodology for approximating an opti￾mal solution proceeds by using the necessary conditions to obtain a state-costate
boundary value problem, this boundary value problem is then discretized in the
algorithms programed in a computer to approximate the solution. Another viable
and important methodology is to formulate the optimization problem, discretize
it, and then solve the resulting minimization problem that will be posed in some
finite-dimensional space. These are referred to as the indirect and direct methods.
Hybrid methodology is also used where the discretization preserves the necessary
conditions for optimality. Use the direct method to solve the optimal control
problem and discuss your observations about employing the two methods.
Exercise 6.140. Consider a bead of mass m sliding on a stiff wire that is
initially parallel to the Earth. The wire is supported at a fixed pivot that allows
it to swing up or down in a fixed vertical plane (see Figure 6.9). An actuator
designed to control the swing is placed on one side of the pivot and a sensor is380 6. Applications
placed on the other side of the pivot at distance L from the pivot. It measures the
vertical distance h to the wire so that a right triangle is formed with one vertex
at the pivot, its right angle at L and its height h. The basic problem: Starting
with the bead at rest on the sensor side of the pivot and the wire horizontal,
move the bead to rest at some other prescribed distance from the pivot on the
sensor side in some preassigned time using minimum effort, perhaps measured by
the force exerted by the actuator, by controlling the tilt of the wire caused by the
actuator. Imagine that a Cartesian coordinate system, with coordinates x and
z, is defined in the vertical plane of motion with its origin at the pivot and the
sensor located in the positive direction of the horizontal coordinate. Gravity (with
acceleration g) acts on the bead so that it would slide toward the origin should
h be positive. (a) Use Newton’s second law of motion to determine the equation
of motion of the bead in the case where it slides without friction. In particular,
under the assumption that the initial position of the bead has coordinates (L, 0)
and its initial velocity is zero, show that the equation of motion is the initial
value problem
x¨ +
h(t)
h(t)2 + L2 (gL + h
(t) ˙x + h(t)x)=0, x(0) = L, x˙ (0) = 0
with z(t) = h(t)x(t). Hint: To construct the model, consider the parametric curve
t → (x(t), z(t)) for the motion of the bead, considered as a particle, in the vertical
Cartesian plane. According to Newton, mass times acceleration equals the total
force. There are two forces: the force due to gravity, which acts in the downward
vertical direction with gravitational acceleration approximated by the constant g
and the force (electromagnetic in origin) holding the bead on the wire. The second
force is unknown but acts perpendicular to the wire. Consider the dot product of
both sides of Newton’s equation with a vector tangent to the straight wire. (b)
Write the equation of motion for the case where a (linear) frictional force acts
opposite to the direction of the particle velocity along the wire. (c) Consider the
frictionless case with the model derived in (a). Suppose the actuator has moved
the wire to a fixed nonzero tilt and the bead starts from rest at (L, 0). This
might be considered an approximation of the case where all admissible controls
are constant functions. Also suppose the preassigned desired final position is
(L/2, 0) and the final time is T = 1. The problem is to minimize the objective
(L/2 − x(1))2 + ˙x(1)2, which is a measure of how close the final position and
velocity have to their desired values. Show by pencil-and-paper computations
(or with the aid of an algebraic processor) that there are two global minima
in case 2L < 5g. Also determine the minimum value of the objective function.
What happens at 2L ≥ 5g? Is there an intuitive physical reason why the optimal
solution is not unique? Remark: Nonuniqueness signals pending difficulties for
optimization over more realistic sets of admissible controls. (d) The existence of
derivatives of the control h complicates the problem as it is not in the form of
the control problems considered in this chapter. One way to recast the control
system into a form that is discussed in this chapter is to augment the dynamical
equations—in a manner akin to the standard procedure for writing a second-order6.7 Control 381
ODE as a first-order system—so that the control is u = h:
x˙ = z,
z˙ = − h(t)
h(t)2 + L2 (gL + kx˙ + ux),
h˙ = k,
k˙ = u.
As before, suppose the bead starts at rest at the point with coordinates (L, 0).
The new objective is to move the bead to some specified rest position at some
preassigned time while minimizing the integral of the square of the control over
the fixed time interval. Using the necessary conditions and the maximum princi￾ple, determine a reduction of this optimal control problem to a boundary value
problem for the state/costate system. Hint: Add an end cost to the objective
function to be minimized that penalizes end states that are not close to the
desired end state. (e) Repeat part (d) with friction taken into account. (f) For
definiteness in the context of part (c), take g = 9, 8, L = 2, the desired rest
position L = 1, and the final time t = 1. Approximate the optimal control u.
Hint: A reasonable (but perhaps not the best) approach is to use part (d) or (e).
This leads to a shooting problem for a function f : R4 → R4. Instead of seeking a
zero of this function, minimize the scalar function g with the same domain given
by g(w) = f(w), f(w). There is a wealth of available software that might be
employed and much to learn by writing a code to solve this problem. (g) Pose
a variation of the optimization problem and solve it. (h) Consider a tilt table;
that is a plane initially parallel to the Earth with actuators designed to tilt the
table to an arbitrary plane that can be given as the range of a function over the
horizontal plane. Write the equations of motion of a particle sliding on the table
under the influence of gravity and solve an optimization problem. For example,
consider the particle with some initial position and velocity on the table. Design
a control that moves it to the origin with minimal total force applied by the
actuators.
Exercise 6.141. Recall the model control system (6.147) for the inverted pen￾dulum. (a) Set up and solve (perhaps with a numerical approximation) the time￾optimal control problem (see Ex. 6.150 part (g)) for moving the pendulum from
some initial nonvertical configuration to a specified new base-point location with
the pendulum in the stabilized vertical position. This is often called a swing-up
problem for the inverted pendulum. (b) Set up a control functional that assigns
a cost to the use of the actuator(s) that move the base point of the inverted
pendulum and minimize the cost of operation for a swing-up problem.
Exercise 6.142. Imagine n ≥ 1 masses m1, m2, m3,...mn moving along a hor￾izontal line (so that the gravitational force can be neglected). These masses are
at the initial positions 0 ≤ ξ1 < ξ2 < ξ3 < ··· < ξn < L where L is the
position of a rigid wall. They are connected by springs with spring constants
k1, k2, k3 ...,kn such that the first mass is connected to the second, the second
to the third, and so on with the nth spring connected to the rigid wall. Viscous
damping, perhaps actuated by dashpots, is considered with damping constants
μ1, μ2, μ3,...,μn. A control force directed along the line toward the wall with382 6. Applications
magnitude |u|, bounded by a fixed constant M > 0, is applied to the first mass.
At time t = 0 the configuration is in equilibrium. The objective is to determine a
control u that moves the first mass with minimal applied force to a given fraction
0 <r< 1 of the distance ξ2 − ξ1 in the direction of L in exactly time t = ff > 0
so that at this time all the masses have zero velocity. (a) Consider the case of
two masses. Write a model first-order system of ODEs for the motion of the
masses. Describe a viable prescription for solving the model ODE by hand. Use
an algebraic processor to find the exact solution for the motion for the special
case where damping vanishes, the masses, and spring constants are equal, the
initial positions of the masses and the wall are, respectively, 0, L/2, and L, the
initial velocities of the masses are zero, and the applied force is constant in the
direction of the wall with magnitude c. (b) (Numerics) Write a computer code for
approximating solutions of spring-mass systems and check it against the exact
solution in part (a). Consult a book on numerical methods if necessary or use a
blackbox ODE solver. For definiteness consider the case of two masses. Set them
to 0.5, the spring constants to 49.0, the wall position L = 0.2, and the applied
force magnitude c = 0.70. Also, set T = 1. Remark: MKS units were considered in
choosing numerical values. (c) Add damping. For definiteness use μ1 = μ2 = 5.0,
and report your approximation for the motions of the masses. (d) Do not impose
a uniform bound max0≤t≤T |u| on the magnitude of the control and assume the
optimization task is to move the first mass to position half way between the equi￾librium positions of the first two masses. For the case of two masses with system
parameters given in parts (b) and (c), write the control problem in Mayer form.
Write the control Hamiltonian and the costate system. Note that the end point
gradient (transversality) condition specializes to the costate corresponding to the
new state variable in the Mayer form by taking the constant value −1. Apply
the maximum principle and show that the optimal control is expressible as an
explicit function of the states and costates. Substitute for the optimal control into
the state-costate system. Show that the optimal states, costates, and control are
solutions of a boundary value problem where boundary conditions at t = 0 and
t = 1 are given by the requirements of the stated optimization problem. Hint: At
time t = 0 the masses are at equilibrium with the first mass at the origin and
the second at L/2. Determine positions and velocities of the masses at their final
state by assuming it is an equilibrium. Approximate the solution of the boundary
value problem by shooting; that is by guessing initial unknown costates, integrat￾ing the system to the final time t = 1, and checking if the final-time boundary
conditions are satisfied. This scenario is made more precise by defining a function
from R4 to itself that must be zero at a solution of the boundary value problem.
A zero can be approximated by Newton’s method. Report graphs of the posi￾tions of the masses, the optimal control, and the integral of the square of the
optimal control over time. (e) Consider the two mass problem for larger values
of the damping coefficients. Give a reason based on the underlying dynamics of
the state-costate system why the problem might become more challenging (ill
conditioned) for numerical approximations as the damping increases. (f) What
about more masses, different system parameters, different measurements of the
amount of control force, or cases with unequal masses or springs? (g) Impose a
uniform bound on the control. Is shooting a viable solution method for this case?
Discuss.6.7 Control 383
Exercise 6.143. In practice for industrial applications, 90% of control systems
are proportional-integral-derivative (PID) feedback controls. Suppose the signal
x is observable from a plant, which might have unknown system dynamics, and
some input to the plant can be influenced by an actuator modeled by the control
input u. A PID controller (in its simplest form) is a device that has three settings,
given by real numbers kP , kI and kD often called (input) gains. For input x the
feedback control is
u(t) = kP x(t) + kI
 t
0
x(s) ds + kdx˙ (t).
The gains are adjusted (tuned) to try to maintain the plant in a desired operating
condition. Many tuning heuristics have been proposed that are often successful
in practice; otherwise PID controllers would not be in common use. Of course
there is also an extensive literature where theory and practice have been well
studied. This exercise is not meant to substitute for practical tuning protocols.
General problem: When is there a useful way to apply optimal control to tune a
PID controller? (a) Suppose the plant is modeled by a harmonic oscillator (which
is often a good place to start) given by
x¨ + μx˙ + ω2
x = f(t),
where the forcing is some unknown function that might include unwelcome dis￾turbances. The desired operating condition is x(t) ≡ 0. In case x is the measured
output, the PID control system is
x¨ + μx˙ + ω2
x = f(t) + kP x(t) + kI
 t
0
x(s) ds + kdx˙ (t).
Actuators operate at a cost, which is assumed to be some known constant times
the magnitude of the vector of gains. Idea: Assume the gains are functions of
time and use optimal control to minimize some cost such as
 T
0
k2
P + k2
I + k2
D + x2 + ˙x2 ds.
For definiteness, let μ = 0, ω = 2, T = 20, and f(t) = exp(−2(t − 3)2). Deter￾mine the boundary value problem that follows from the maximum principle and
discuss how it might be solved. (b) (Numerics) Approximate the optimal control
vector (kP , kI , kD) for part (a). (c) To obtain constant values of the PID control
parameters choose the average values of the corresponding control functions and
test the resulting PID controller. (d) A robust controller would bring the system
toward its desired state for unexpected choices of the forcing function f. Idea:
Solve the optimal control problem many times for a wide (but well conceived)
class of forcing functions and use the resulting optimal controls to choose con￾stant values for the PID gains. Discuss a methodology, implement it numerically,
and test the result against some unused forcing function. (e) Choose a variation
on the theme of this problem, describe it in detail, and solve it. (f) Comment on
the viability of the suggested methodology.
Exercise 6.144. [Toward A Control Game] According to Newton, the position
γ of a particle with fixed mass m satisfies the differential equation mγ¨ = F(γ, t),384 6. Applications
where F is the sum of the forces on the particle given as a function of the position
of the particle and the current time t. Imagine a game played on the Cartesian
plane with a particle of mass m that enters at the origin with unit velocity
in the horizontal direction, where as usual the horizontal coordinate is x and
the vertical coordinate is y. A player controls the dimensionless interaction field
strengths a : R → R and b : R → R that determine the fields
X(x, y, t) := ka(t)
(x2 + (y + 2)2)3/2
 x
y + 2
,
Y (x, y, t) := kb(t)
(x2 + (y − 2)2)3/2
 x
y − 2

,
where k has units so that the fields have units of force when interacting with
the particle. The fields may be considered to be something like, but not exactly,
electric fields emanating from sources situated at the points with coordinates
(0, −2) and (0, 2), respectively. Generation of the interaction field strengths is
supposed to use a power source (for example, electricity) whose amount is directly
proportional to the root mean squares of the strengths. The outcome of a play
is tallied at time t = √5 seconds by assigning the score given by the sum of
two quantities: (1) the square of the difference between the final state of the
particle and the target state whose position in the plane has coordinates (2, 1)
and whose velocity vanishes, and (2) the power usage measured by the square of
the L2 norm of the strength protocol (a, b). The base version of the game is for
k = 1 and consists of one play by each player; the winner has the lowest score.
(a) Write the equation of motion using Newton’s law and convert to a first￾order control system with control given by the pair of scalar-valued functions
(a, b). Write the objective as an integral plus an end-time condition. Change
to Mayer form and show how the optimal control problem reduces to solving
a boundary value problem for a system of ordinary differential equations. (b)
Change the objective of the game to reaching the target in exactly time t = 1.
The necessary conditions are the same except that the terminal condition is
replaced by the condition that the Hamiltonian along an optimal state-costate
solution must be constant. Again show how the problem is reduced to solving a
boundary value problem. (c) (Numerics.) Compute an approximate solution of
the boundary value problem of part (a) using the shooting method; that is, choose
initial data for a costate equation and refine this choice (via Newton’s method
for example) until a desired tolerance is reached that provides your definition of
a sufficiently accurate approximate solution. This is called an indirect solution
method: Necessary (and perhaps sufficient) conditions are formulated, these are
discretized, and they are approximately numerically. The maximum principle
is usually employed. (d) Direct solution methods proceed in two main steps: (1)
transcribing the control problem via a discretization to obtain a finite-dimensional
optimization problem; and, (2) use of gradient descent, Newton’s method, or other
optimization procedures to approximate the solution of the approximate finite￾dimensional optimization problem. Formulate and implement a direct method to
approximate the optimal protocol for the base version of the game. (e) Remarks:
There is much to learn by working on this problem. Does a solution exist? Is it
unique? How robust is the solution; that is, will small perturbations of the control
result in small changes in the winning strategy? Why was the L2 objective chosen6.7 Control 385
for this problem? What would happen if no control costs were assessed but the
sizes of the admissible field strengths were uniformly bounded? What happens
if the field strengths are required to be within some given bounds? Many other
variations are worth exploring. (f) Optimal strategies exist for the base version of
the game. A successful player would have implemented an algorithm that closely
approximates one of them. First level versions of the game are not as easily solved.
For example, suppose there are n players. Each takes a turn and is scored as in
the base level game. But in this version, the coefficient k is a function of time
on the interval 0 ≤ t ≤ √5 determined by the other n − 1 players under the
following rules. The time interval is split into 32(n − 1) equal length segments,
each of the n − 1 players is allowed to choose one segment over which k is set to
their choice of a number in the interval [0, 1]; k is set to unity on the remaining
segments. The choices are secret (that is, each choice is known by exactly one
player: the player who made it). In a modern setting, this version of the game
could be played by submission of computer programs and each player’s list of k￾change choices. Once the mathematics has been fully understood for this game,
it would become an interesting game for engineers if they add to the scoring of
each play the CPU-time of running plays on a neutrally administered computer.
Request a game with several people and write a complete summary and critique
of the strategies that were employed. (g) Machine learning algorithms could be
tested by allowing AIs to be players.
6.7.5 Parameter Estimation and the Adjoint Method
A fundamental paradigm in applied science requires construction of a math￾ematical model of some process or observed phenomenon, identifying the
parameters in the model according to their known values or by experi￾mental or observational measurements, and validating the model with the
identified parameter values against the data and new data not used in the
identification process. The model is abandoned, revised, or reconstructed
according to an assessment of the validation process. Closely related are
problems in machine learning, which might involve, for example, construct￾ing the model from training data. In principle, similar ideas can be used in
both cases but machine learning problems often come with a large-scale set
of parameters. This has motivated many new ideas. A particularly beautiful
idea, the adjoint method, makes an important ingredient of the parameter
identification problem feasible in large-scale parameter spaces. It is briefly
described in this section. Of course, differential equations appear in math￾ematical models and differential equation theory is useful in implementing
parameter identification algorithms.
Continuing with the simple logistic growth model discussed in Sec. 6.7.4,
imagine using this model to predict population dynamics of fish in a lake
for which data has been collected that gives measured estimates (derived,
for example, from electroshocking and statistical inference) of total biomass
w (in some convenient unit) for six consecutive years (measured with the386 6. Applications
temporal parameter τ ):
data = (0.040, 0.123, 0.390, 1.043, 1.95, 2.55). (6.174)
The proposed model is
dw
dτ = αw(1 − w/K) − β, w(0) = w0. (6.175)
In this microcosm, the parameter identification problem is to determine α,
K, and β so that the model (best) fits the data.
With respect to the application, the constraints α > 0, K > 0, β ≥ 0
may be imposed. Of many possible definitions of fitness, weighted mean
square is often used:
F(α, K, β) = 
6
i=1
qi(w(i − 1) − datai)
2. (6.176)
The parameters appear on the right-hand side because w is a function of
time τ and these parameters. Also, q is some chosen vector of weights often
taken with components all equal to the absolute value of the reciprocal
of the largest element in the data vector. The problem is to minimize the
function F : R3 → R subject to the positivity constraints. In practice, the
fitness function is constructed with respect to the data. Perhaps parts of
the data are considered more reliable than others, or the data might vary
widely in scale.
Suppose the desired minimum is in the interior of the feasible set. Using
the smoothness of solutions of ODEs with respect to their parameters the
function F is continuously differentiable. From calculus, the first derivative
(its gradient) must vanish at the minimum. Thus, the desired minimum
must be a solution of the equations given by the components of the vector
equation DF(p) = 0. For most examples, the resulting system of scalar
equations is not linear and not solvable using pencil and paper.
A viable method often used to approximate minima of a function is gra￾dient descent; that is, following (approximately) the solution of the initial
value problem
p˙ = −DF(p), p(0) = p0,
where in the example p = (α, K, β)T . The solution of the latter differential
equation is approximated for various choices of the initial guess p0 (of
the values of the optimal set of parameters) with the expectation that at
least one of the corresponding solutions is in the stable manifold of a sink
that corresponds to the desired rest point (DF(p) = 0) corresponding to
the desired (global) minimum of F. A major issue is that F may have
many local minima. This problem is partially mitigated by choosing many6.7 Control 387
initial points. There are more sophisticated methods, but no (as yet) known
method for ensuring that the global minimum is obtained.
To use gradient descent in the present context, notice that partial deriva￾tives of F with respect to parameters involve partial derivatives of the solu￾tion of the original initial value problem (6.175). Theoretically, as done in
several sections of this book, the desired derivatives may be obtained by
solving appropriate variational equations, one for each parameter.
For completeness, recall that partial derivatives with respect to param￾eters are computed from variational initial value problems of the form
x˙ = f(x, λ), x(0) = x0,
∂
∂txλ(t, λ) = fx(x(t, λ), λ)xλ(t, λ) + fλ(x(t, λ), λ), xλ(0, λ)=0.
(6.177)
See Ex. (6.148) to explore this methodology.
Newton’s method is the premier idea, but not always practical, for solv￾ing nonlinear systems of equations (such as DF(p) = 0). Recall that for
one variable problems g(x) = 0, an initial guess x0 produces the point
(x0, g(x0)) on the graph of g. The tangent line to the graph at this point
usually crosses the x coordinate axis at a unique point
x1 := x0 − g(x0)/g
(x0).
Often x1 is a better approximation to the desired root than x0. Taking
x1 as the new guess and repeating the process often produces a sequence
of approximations that converge to the root. In fact, if g has a root, the
derivative of g does not vanish at this root, and x0 is sufficiently close to the
root, then Newton’s method produces a sequence that rapidly converges to
the root. This theorem is true for functions g : Rn → Rn where division by
the derivative is replaced by multiplication by the inverse of the derivative
of g and the nonzero condition is replaced by this derivative being invertible
at the root; in effect,
pk+1 = pk − Dg(pk)
−1g(pk).
A useful way to employ the method is to assume the kth element pk has
been computed. The next step is to compute g(pk) and Dg(pk) and use
them to solve for q in the linear system of equations Dg(pk)q = −g(pk).
The update is pk+1 := pk + q (see Ex. (6.147)).
For the parameter identification problem (and other optimization prob￾lems), the function g in the last paragraph is the gradient of some scalar￾valued function. Thus, the derivative of g is the second derivative of this
scalar-valued function. In this case, the second derivative is called the Hes￾sian; it is a (symmetric) matrix of second-order partial derivatives. Vari￾ational equations may be used to compute these second-order derivatives388 6. Applications
(see Ex. (6.151)). But, as should be evident, the process is computationally
expensive in cases where the required derivatives are not readily available.
In the simple example discussed here, the full state (total biomass) was
measured at several points in time and used in the estimation problem.
In many realistic systems, data accumulation is obtained using sensors
that might only measure the effects of certain components of the state.
Also, in realistic problems incompleteness of the model and noise inher￾ent in measurements must be taken into account. Also, the process may
change in various ways with time. System parameter estimates should be
updated based on both historical and current measurements. Imagine, for
example, an onboard system used for control where system parameters are
continuously updated. In the biomass example, system parameters might
be updated based on current field measurements looking back some number
of years in the past.
For small scale problems such as the biomass example, gradient descent
or Newton’s method using variational derivatives is viable. But, in large￾scale problems, computing variational derivatives (one for each parameter)
can become prohibitively expensive. This is where the adjoint method can
be useful. It is described here in an abstract form using ideas that originated
in control theory.
Consider a (perhaps time dependent) ODE initial value problem
x˙ = f(x, t, p), x(0) = ξ, (6.178)
where p is a vector of parameters and the solution is denoted by t →
φ(t, ξ, p). To maintain some generality, suppose that g is some scalar func￾tion of the state x and the parameters, T > 0 is some fixed time, and the
objective is to minimize the function F given by
F(p) =  T
0
g(φ(t, ξ, p), t, p) dt. (6.179)
A key ingredient in implementing most minimization procedures is the
derivative of F given by
DF(p) =  T
0
gp(φ(t, ξ, p), t, p) + gx(φ(t, ξ, p), t, p)φp(t, ξ, p) dt. (6.180)
Under the assumption that g and its partial derivatives are given explicitly,
the most expensive part of the computation of DF is the determination of
the function t → φp(t, ξ, p). Once this function—actually one such function
for each component of p—and t → φ(t, ξ, p) are known, the derivative
can be computed (approximately) by numerical integration, perhaps by an
application of the trapezoidal rule. Using the variational equations of the
form (6.177), the problem is theoretically solved.
An alternative approach is to view the minimization of F as a constrained
minimization problem where the constraint is implicit: φ must satisfy the6.7 Control 389
differential equation. Using the standard idea, the constraint is incorpo￾rated with the addition of a Lagrange multiplier and the minimization
problem is recast in a new form: Minimize
Ψ(p, λ) :=  T
0
g(φ(t, ξ, p), t, p) + λ(t) · (f(φ(t, ξ, p), t, p) − φt(t, ξ, p)) dt
(6.181)
over the parameters and vector-valued functions λ (that match the dimen￾sion of the state x) defined on the interval [0, T].
The minimum argument p of Ψ and F are the same when F has a mini￾mum that is known (or assumed) to satisfy the constraint. In fact, whenever
the constraint is enforced, the term with the Lagrange multiplier vanishes.
In this situation, freedom to choose the Lagrange multiplier is advanta￾geous. To see how, differentiate with respect to p and rearrange to obtain
Ψp(p, λ) :=  T
0
gp(φ(t, ξ, p), t, p) + λ(t) · fp(φ(t, ξ, p), t, p)
+ (gx(φ(t, ξ, p), t, p) + λ(t) · fx(φ(t, ξ, p), t, p))φp(t, ξ, p)
− λ(t) · φtp(t, ξ, p)) dt. (6.182)
Integration by parts applied to the last term in the integrand and using ∗
to denote transpose yields
Ψp(p, λ) :=  T
0
gp(φ(t, ξ, p), t, p) + λ(t) · fp(φ(t, ξ, p), t, p)
+ (gx(φ(t, ξ, p), t, p) + λ(t)
∗fx(φ(t, ξ, p), t, p) + λ
(t)
∗)φp(t, ξ, p) dt
+ λ(0)φp(0, ξ, p) − λ(T)φp(T, ξ, p). (6.183)
To take advantage of the freedom to choose λ, the obvious choice is
gx(φ(t, ξ, p), t, p) + λ(t)
∗fx(φ(t, ξ, p), t, p) + λ
(t)
∗ = 0, λ(T)=0.
(6.184)
The initial value problem for λ∗, which needs only to be solved once, is akin
to the adjoint equation in control theory; thus the name adjoint method.
With this choice, the need to compute variational equations with respect
to the parameters is eliminated. The adjoint differential equation is solved
backward from t = T to t = 0.
Of course, the desired derivative
Df(p) =  T
0
gp(φ(t, ξ, p), t, p) + λ(t) · fp(φ(t, ξ, p), t, p) dt (6.185)
is calculated by numerical integration once λ and φ are known. An efficient
way to do this is to solve the ODE
μ˙ = gp(φ(t, ξ, p), t, p) + λ(t) · fp(φ(t, ξ, p), t, p), μ(T)=0390 6. Applications
backward to obtain
Df(p) = −μ(0).
is calculated by numerical integration once λ and φ are known. The adjoint
method is a manifestation of back propagation in machine learning.
Exercise 6.145. (a) Find a and b such that the solution of the initial value
problem ˙x = ax + bt, x(0) = 0 minimizes the function
F(a, b) =  2
0
(x(t) − t
2
)
2
.
(b) In the context of part (a), replace t
2 by t
3 and find an approximate solution.
(c) In the context of part (b), write the adjoint equation and solve it.
Exercise 6.146. Let A be a symmetric matrix and consider the optimization
problem: Minimize Ax, x over all x such that |x| = 1. (a) Write the correspond￾ing ODE for gradient descent. (b) Show that the ODE in part (a) might have
infinitely many rest points, but there is an open and dense set of matrices on
which there are only a finite number of rest points. (c) In case there are a finite
number of rest points, determine the maximum number that might occur. (d) The
same questions may be asked after replacing Ax, x by H(x), where H : Rn → R
is a member of a class of functions with a fixed finite number of parameters. Are
there interesting classes of functions where the questions can be answered?
Exercise 6.147. Show that the steps given in the text to implement Newton’s
method are equivalent to the update formula pk+1 = pk − DF(pk)
−1f(pk). Why
is it better to use the suggested implementation?
Exercise 6.148. (Numerics) In gradient descent methodology, the existence of
local minima makes the problem of finding a global minimum a grand challenge;
but it does not invalidate the method. (a) Write a code to test gradient descent
for the model problem corresponding to the data (6.174). Hints: Solutions of
the gradient ODE may be approximated by (Euler’s method): at each step, the
new value of p is the old value minus some multiple (step size) of the gradient
evaluated at the old value. The method has been refined to perform well on
realistic applications. A good initial guess for p(0) and appropriate step sizes
are essential for success. The gradient may be normalized to have unit length.
Its negative points in the direction where F decreases but perhaps a very small
step in that direction is required to actually decrease F. Adjusting the step size
(perhaps after each successful step) according to the behavior of the objective
function can be beneficial. Another idea is line search: Compute with several step
sizes along the direction of the negative gradient at each step and choose among
them according to which new p produces the smallest value of the objective
function. (b) Imagine a model given by a system of many nonlinear ODEs with
many unknown system parameters to be fit using some measured data. Discuss,
with reference to your experience with part (a) the viability of gradient descent.6.7 Control 391
Exercise 6.149. Suppose observed data for a dynamical system is given. Is it
possible to determine a model for the system by optimization? A simple scenario is
to imagine the ability to set an initial condition, say (x, y) for which the system
produces the output t → (z(t), w(t)), where t measures time. The system is
consistent in the sense that an input always produces the same output. You
suspect the system is governed by a system of ODEs ˙x = f(x, y) and ˙y = g(x, y),
which you would like to know. An idea is to guess that
f(x, y) = a00 + a10x + a01y + a20x2 + a11xy + a01y2 + ···
with a similar expression for g. (a) Describe a method for estimating the unknown
coefficients. (b) Choose a dynamical system to produce data, act as if this system
is not known, and show (using numerical computations) that it can be reproduced
by the method in part (a).
Exercise 6.150. Suppose F : Rn → Rn has as many continuous derivatives
as desired. (a) Show that a point where F vanishes and DF is not singular is a
hyperbolic sink of the dynamical system
p˙ = −DF(p)
−1
F(p) (6.186)
whose form is inspired by Newton’s method. (b) Is the point in part (a) isolated?
(b) Discuss the relation between ODE (6.186) and Newton’s method. (c) Prove
that the (Euclidean) norm of F decreases along trajectories of the differential
equation. (d) Suppose f : Rn → R has as many continuous derivatives as desired
and is bounded below. Consider the problem of approximating its global minimum
by approximating zeros of its gradient, which would become the function F in
part (a). By part (c) the norm of the gradient deceases along trajectories of the
differential equation. Show that if the Hessian of f is positive definite, then f
decreases along trajectories. (e) Show that ODE (6.186) is affine scale invariant;
that is, whenever A is an invertible n × n matrix and b is an n vector, the ODE
is invariant (that is, it has the same form) with respect to the change of variables
p = Aq + b when the function f is also replaced by its transformation. Hint:
f must be considered to be a self map of Rn. Also, show that gradient descent
p˙ = − grad f(p) is not (in general) scale invariant. (f) In Newton type methods,
Jacobian matrices may become (nearly) singular during iteration. One way to
mitigate obvious problems caused by this possibility is to employ the Levenberg￾Marquardt principle: When DF(p) is singular or nearly singular modify it by
adding an appropriate multiple of the identity so that the sum is not singular
in the current step. Show that for a square matrix A there is a nonnegative
choice of λ such that λI + A is not singular. A possible implementation of this
idea to ODE (6.186), for the case where it is applied to find a minimum of the
scalar-valued function f, is to consider the control system
q˙ = (λI + Hess f(q))−1 grad f(q), q(0) = q0 (6.187)
and the problem of minimizing the final value f(q(T )) of the ODE over some
finite time interval [0, T ] with respect to the set of all (measurable) controls
λ : [0, T ] → [0, ∞]. As a simple check, show directly and by using the maximum
principle that for f(q) = q2 the optimal control is λ(t) ≡ 0 as it should be.
(g) Part (f) does not take into account the speed of convergence. One way to
capture it is to set a tolerance  > 0 and a target set given by all states q such
that grad f(q), grad f(q) = 
2. The objective becomes reaching the target in the
shortest time. Time-optimal control problems are not covered by the theorems
proved in this section. But, the required modifications are not difficult to employ:392 6. Applications
The objective function becomes the integral of the function r(t) ≡ 1 over the
time interval [0, T ] where T is defined to be the smallest time for which the
control system’s state is on the target. The costate and maximum principle are as
before, but the end costate condition is replaced by transversality conditions or a
further restriction on the Hamiltonian. In case the system starts on a hypersurface
and ends on a hypersurface in the state space, the costate at the initial time
(respectively, the final time) must be perpendicular to the tangent space of the
initial (respectively, final) hypersurface at the initial (respectively, final) state. In
case the initial and final states are specified states (as in the special case where
f is a scalar function of a scalar variable), the Hamiltonian at the optimal state￾costate-control triple is required to be identically zero. Show that for f(q) = 
2
the time-optimal control is again λ(t) ≡ 0. (h) Consider the control system
q˙ = −(λ grad f(q) + (1 − λ)(Hess f(q))−1 grad f(q)), q(0) = q0, (6.188)
where the control λ is scalar and satisfies the constraint 0 ≤ λ ≤ 1. For f a scalar
function of a scalar variable, show that the optimal control to minimize the time
required to reach f
(q) =  is bang-bang; that is the control may switch but
it is always either zero or one. Hint: The scalar problem can be solved directly.
Show that the result is compatible with the necessary conditions of the maximum
principle. Challenge: What happens in the non scalar case? (i) (Numerics) Choose
test examples F with known roots and apply the results of part (a). (j) Choose
test examples f with known minima and apply the results of part (a). (k) Apply
the methodology of part (a) to the parameter identification problem discussed in
this section. (l) Apply part (a) to the Rosenbrock test function given by f(x, y) =
(1−x)
2 + 100(y −x2)
2; it has a narrow valley leading to its global minimum. For
definiteness and comparison with other methods, use the initial value (−1.8, 2).
Check the number of steps required versus pure Newton’s method and gradient
descent.
Exercise 6.151. (a) Write out the variational initial value problem (akin to
Eq. (6.177)) for the second-order partial derivative xab(t) of the solution of the
initial value problem ˙x = f(x, a, b), x(0) = x0. (b) Apply the result for part
(a) to the initial value problem ˙x = (ax + 1)/b, x(0) = x0. Solve the resulting
variational initial value problem and check the result against a direct computation
of the required second-order partial derivative from the solution of the differential
equation.
Exercise 6.152. Suppose F : Rn → Rn is given and F(a) = 0; that is, a is a
root of F. (a) The differential equation ˙x = F(x) has a rest point at a. State
hypotheses such that the following statement is true: If the hypotheses are met,
then the solution of the differential equation with x(0) = x0 converges to a as
time goes to infinity. Discuss the viability of this idea for finding the desired
root. (b) Repeat part (a) for the differential equation ˙x = −DF(x)
−1F(x) from
Ex. (6.150).
Exercise 6.153. (Numerics) (a) Write a code to test Newton’s method for the
model problem corresponding to the data (6.174). Hints: A good initial guess
is essential for success. Damping, that is using xk+1 = xk − μDF(xk)
−1F(xk)
for some positive μ < 1, can be beneficial. (b) Imagine a model given by a sys￾tem of many nonlinear ODEs with many unknown system parameters to be fit6.7 Control 393
using some measured data. Discuss, with reference to your experience with part
(a), the viability of Newton’s method for optimization problems. (c) Make up a
simple test problem that requires minimizing a scalar function of two variables
where the answer is known exactly. Check the speed of convergence of gradi￾ent descent versus Newton’s method and report on your results. (d) A problem
with Newton’s method applied to optimization is that a Newton step may not
decrease the objective function. One clever way to mitigate this problem is called
the Levenberg-Marquardt method (see Ex. 6.150 or find a friendly reference).
Implement this method, and apply it to the parameter identification problem.
Exercise 6.154. Many methods have been developed that avoid computing
derivatives in optimization problems. (a) Find a friendly reference for the Nelder￾Mead method and apply it to the parameter identification problem. (b) Repeat
part (a) for a patten search method. (c) Repeat part (a) for a genetic algorithm.
(d) Repeat part (a) for stochastic gradient descent.
Exercise 6.155. Solve the problem posed in the text for the data (6.174) and
model (6.175).7
Hyperbolic Theory
This chapter is an introduction to the theory of hyperbolic structures in
differential equations. The basic idea that is discussed might be called the
principle of hyperbolic linearization: If the system matrix of the linearized
flow of a differential equation has no eigenvalue with zero real part, then
the nonlinear flow behaves locally like the linear flow. This idea has far￾reaching consequences that are the subject of many important and useful
mathematical results. Here we will discuss two fundamental theorems: the
center and stable manifold theorem for a rest point and the Hartman–
Grobman theorem.
7.1 Invariant Manifolds
The stable manifold theorem is a basic result in the theory of ordinary
differential equations. It and many closely related results, for example, the
center manifold theorem, form the foundation for analyzing the dynamical
behavior of a dynamical system in the vicinity of an invariant set. In this
section, we will consider some of the theory that is used to prove such
results, and we will prove the existence of invariant manifolds related to
the simplest example of an invariant set, namely, a rest point. Nevertheless,
the ideas that we will discuss can be used to prove much more general
theorems. In fact, some of the same ideas can be used to prove the existence
and properties of invariant manifolds for infinite-dimensional dynamical
systems.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
C. Chicone, Ordinary Differential Equations with Applications, Texts in
Applied Mathematics 34, https://doi.org/10.1007/978-3-031-51652-8 7
395396 7. Hyperbolic Theory
The concept of an invariant manifold for a rest point arises from the study
of linear systems. Recall that if A is the system matrix of a linear differential
equation on Rn, then the spectrum of A splits naturally—from the point of
view of stability theory—into three subsets: the eigenvalues with negative,
zero, and positive real parts. After a linear change of coordinates that
transforms A to its real Jordan normal form, we find that the differential
equation ˙u = Au decouples into a system
x˙ = Sx, y˙ = Uy, z˙ = Cz,
where (x, y, z) ∈ Rk × R × Rm with k +  + m = n, and where S, U,
and C are linear operators whose eigenvalues have all negative, positive,
and zero real parts, respectively. The subspace Rk ⊂ Rn (more precisely
Rk × {0}×{0}) is called the stable manifold of the rest point at the origin
for the original system ˙u = Au, the subspace R is called the unstable
manifold, and the subspace Rm is called the center manifold.
According to Theorem 3.1, there are constants K > 0, a > 0, and b > 0
such that if ξ ∈ Rk and ζ ∈ R, then
x(t, ξ) = etSξ ≤ Ke−atξ, t ≥ 0,
y(t, ζ) = etU ζ ≤ Kebtζ, t ≤ 0, (7.1)
where t → x(t, ξ) is the solution of the differential equation ˙x = Sx with
the initial condition x(0, ξ) = ξ, and y is defined similarly. Here,   is
an arbitrary norm on Rn. There are no such exponential estimates on the
center manifold.
An analysis of the dynamics on the center manifold, when it exists, is
often one of the main reasons for finding a center manifold in the first place.
In this regard, let us recall that the flow of a nonlinear system near a rest
point where the linearization has an eigenvalue with zero real part is not
determined by the linearized flow. For example, the linearization at a rest
point of a planar system might have a center, whereas the corresponding
rest point for the nonlinear system is a sink or a source. In this case, the
center manifold at the rest point is an open subset of the plane. As this
example shows, we can expect the most complicated (and most interesting)
dynamics near a nonhyperbolic rest point to occur on a corresponding cen￾ter manifold. If a center manifold has dimension less than the dimension
of the phase space, then the most important dynamics can be studied by
considering the restriction of the original system to a center manifold. To
illustrate, let us imagine a multidimensional system that has a rest point
with a codimension two stable manifold and a two-dimensional center man￾ifold. Then, as we will see, the orbits of the nonlinear system are all locally
attracted to the center manifold, and therefore the non-trivial dynamics
can be determined by studying a planar system. This “center manifold
reduction” to a lower dimensional system is one of the main applications
of the theory.7.1 Invariant Manifolds 397
The stable manifold theorem states that if the linear system ˙u = Au is
hyperbolic (that is, all eigenvalues of A have nonzero real parts), then the
nonlinear system ˙u = Au + H(u), where H : Rn → Rn with H(0) = 0 and
DH(0) = 0, has stable and unstable manifolds corresponding to the stable
and unstable manifolds of the linear system. These manifolds are invariant
sets for the nonlinear system that contain the rest point at the origin, and
they have the same dimensions as the corresponding linear subspaces. In
fact, the corresponding linear subspaces are their tangent spaces at the rest
point. Moreover, the flow of the nonlinear system restricted to the stable
and the unstable manifolds has exponential (hyperbolic) estimates similar
to the inequalities in display (7.1).
Several different methods are available to prove the existence of invariant
manifolds, each with its own technical and conceptual advantages and dis￾advantages. We will use the Lyapunov–Perron method where the basic idea
is to seek the invariant manifold as the graph of a function obtained as the
fixed point of an integral operator on a Banach space. A viable alternative
method is based on the “graph transform” (see [139] and [218]). The main
reason for using the Lyapunov–Perron method here is that its application
illustrates useful ODE techniques.
To prove the existence of an invariant manifold at a rest point it is not
necessary to assume the linearized system is hyperbolic. Instead, it suffices
to assume that the spectrum of the linearization has a spectral gap; that
is, the spectrum is separated into two vertical strips in the complex plane
such that the maximum of the real parts of the eigenvalues in the left-hand
strip is strictly less than the minimum of the real parts of the eigenvalues
in the right-hand strip. This hypothesis is exactly the condition required
to apply the Lyapunov–Perron method. The stable, unstable, and center
manifold theorems are easily obtained as corollary results.
The next theorem is the main result of this section. It states the exis￾tence of a smooth global invariant manifold at a rest point of a nonlinear
system, provided that the linearization of the system at the rest point has
a spectral gap and the nonlinear remainder terms are sufficiently small. Its
proof is long, due mainly to technical issues, but not difficult to under￾stand. There are two main ideas: (1) set up a contraction mapping in an
appropriate Banach space of continuous functions so that its fixed point
is a function whose graph is the desired invariant manifold and (2) use
the fiber contraction principle to show that the fixed point is continuously
differentiable.
Let C1(X, Y ) denote the set of all continuously differentiable functions
with domain X and range in Y . For a function f ∈ C1(X, Y ), its C1-norm
f1 is the sum of the supremum norm of f and the supremum norm of its
derivative Df.
Theorem 7.1. Suppose that a and b are real numbers with a<b, S :
Rk → Rk and U : R → R are linear transformations, each eigenvalue of398 7. Hyperbolic Theory
S has real part less than a, and each eigenvalue of U has real part greater
than b. If F ∈ C1(Rk × R, Rk) and G ∈ C1(Rk × R, R) are such that
F(0, 0) = 0, DF(0, 0) = 0, G(0, 0) = 0, DG(0, 0) = 0, and F1 and G1
are sufficiently small, then there is a unique function α ∈ C1(Rk, R) with
the following properties:
α(0) = 0, Dα(0) = 0, sup
ξ∈Rk
Dα(ξ) < ∞,
whose graph, namely, the set
W(0, 0) = {(x, y) ∈ Rk × R : y = α(x)},
is an invariant manifold for the system of differential equations given by
x˙ = Sx + F(x, y), y˙ = Uy + G(x, y). (7.2)
Moreover, if (ξ,α(ξ)) ∈ W(0, 0), then for each λ>a there is a constant
C > 0 such that the solution t → (x(t), y(t)) of the system (7.2) with initial
condition (ξ,α(ξ)) satisfies the exponential estimate
x(t) + y(t) ≤ Ceλtξ
for t ≥ 0.
Proof. Several Banach spaces and several different norms are employed.
Proofs that these linear spaces with associated norms are indeed Banach
spaces are left to the reader except for one perhaps unfamiliar example.
Let C0(RN , RM) denote the linear space of all continuous functions
f : RN → RM
and define the following Banach spaces: C0(RN , RM), the set of all functions
f ∈ C0(RN , RM) such that f(0) = 0 and
f0 = sup
ξ∈RN
f(ξ) < ∞;
C1(RN , RM), the set of all continuously differentiable functions
f ∈ C0(RN , RM)
such that f(0) = 0, Df(0) = 0, and
f1 = f0 + Df0 < ∞,
and E0(RN , RM), the set of all functions f ∈ C0(RN , RM) such that f(0) =
0 and
fE = sup{
f(ξ)
ξ : ξ ∈ RN , ξ 
= 0} < ∞.7.1 Invariant Manifolds 399
Also for f ∈ C0(RN , RM), let the Lipschitz constant of f be denoted by
Lip(f) := sup
ξ=η
f(ξ) − f(η)
ξ − η
whenever the indicated supremum is finite.
Proposition A: The space E0(RN , RM) with the E-norm is a Banach
space.
To prove the proposition, assume for the moment an implication: If
{fn}∞
n=1 is a sequence in E0(RN , RM) that converges in the E-norm to
a function f : RN → RM, then the sequence converges uniformly on com￾pact subsets of RN . The usual theorems on uniform convergence imply
that the limit function f is continuous on RN . To show it is also bounded,
use the definition of convergence. There is a positive integer n such that
f − fnE < 1. For this integer,
fE ≤ f − fnE + fnE < 1 + fnE .
To show that E0(RN , RM) is a Banach space, we must show that it is
complete. To this end, suppose that the above sequence is Cauchy. We will
show that the sequence converges to a function f : RN → RM with f(0) =
0. By the facts claimed above, we must then have that f ∈ E0(RN , RM),
as required.
Let us define a function f : RN → RM. First, set f(0) = 0. If ξ ∈ RN is
not the zero vector, let  > 0 be given and note that there is an integer J
such that
fm(ξ) − fn(ξ)
ξ < 
ξ
whenever m and n exceed J. Thus, the sequence {fn(ξ)}∞
n=1 is a Cauchy
sequence in RM, and hence it has a limit that we define to be f(ξ).
We claim that the sequence {fn}∞
n=1 converges to the function f in the
E-norm. To prove the claim, let  > 0 be given. There is an integer J, as
before, such that, if ξ 
= 0, then
fn(ξ) − fp(ξ)
ξ < 
2
whenever the integers n and p exceed J. It follows that if ξ ∈ RN , including
ξ = 0, then the inequality
fn(ξ) − fp(ξ) ≤ 
2 ξ
holds whenever n and p exceed J. Using this fact, we have the following
estimates:
fn(ξ) − f(ξ)≤fn(ξ) − fp(ξ) + fp(ξ) − f(ξ)
≤ 
2 ξ + fp(ξ) − f(ξ).400 7. Hyperbolic Theory
If we now pass to the limit as p → ∞, we find that, for all ξ ∈ RN ,
fn(ξ) − f(ξ)
ξ ≤ 
2 < 
whenever n exceeds J. Thus, we have that fn − fE <  whenever n
exceeds J, and therefore the sequence converges to f in the E-norm.
To finish the proof, we must show that convergence in the E-norm is
uniform on compact sets. To this end, suppose that {fn}∞
n=1 converges to
f in the E-norm, let K be a compact subset of RN , and let  > 0 be given.
Define r := supξ∈K ξ. There is an integer J such that if ξ 
= 0, then
fn(ξ) − f(ξ)
ξ < 
r + 1
whenever n exceeds J. Hence, as before, if ξ ∈ K, then
fn(ξ) − f(ξ) ≤ 
r + 1ξ ≤  r
r + 1 < 
whenever n exceeds J. It follows that the convergence is uniform on the
compact set K. This completes the proof of Proposition A.
For positive ρ and δ, let
B0
ρ(RN , RM) := {f ∈ E0(RN , RM) : Lip(f) ≤ ρ},
B1
δ (RN , RM) := {f ∈ C1(RN , RM) : f1 < δ}.
The set B0
ρ(RN , RM) is a closed (in fact, compact) subset of E0(RN , RM).
Hence, it is a complete metric space with respect to the metric given by
the E-norm. The other set B1
δ (RN , RM) is an open subset of C1(RN , RM ).
Fix ρ > 0. If δ > 0, F ∈ B1
δ (Rk × R, Rk), and α ∈ B0
ρ(Rk, R), then the
differential equation
x˙ = Sx + F(x, α(x)) (7.3)
has a continuous flow. In fact, for each ξ ∈ Rk, there is a continuous solution
t → x(t, ξ, α) such that x(0, ξ,α) = ξ. Moreover, (t, ξ, α) → x(t, ξ, α) is a
continuous function.
To compress notation, let
χ(t) := x(t, ξ, α)
and note that the function t → χ(t) is defined for all t ≥ 0.
By the hypotheses of the theorem, there is a constant K > 0 such that
for all ξ ∈ Rk, ν ∈ R, and t ≥ 0,
etSξ ≤ Keatξ, e−tU ν ≤ Ke−btν. (7.4)
A direct proof of these estimates under the spectral gap condition is similar
to the proof of Theorem 3.1. They can also be obtained as a corollary to7.1 Invariant Manifolds 401
this theorem. Hint: Apply Theorem 3.1 to new operators S +cI and U +cI,
where c is a real number chosen so that there is a spectral gap containing
the origin.
Using the inequality DF0 < δ, the mean value theorem, and the sum
of the norms on factors for the norm on the product space Rk × R,
F(x, y) = F(x, y) − F(0, 0) ≤ δ(x + y).
Also, after obtaining a similar estimate for α and combining these esti￾mates,
F(x, α(x)) ≤ δ(1 + ρ)x.
By an application of the variation of parameters formula given in Propo￾sition (4.1), the function χ satisfies the integral equation
χ(t) = etSξ +
 t
0
e(t−τ)SF(χ(τ ), α(χ(τ ))) dτ (7.5)
from which we obtain the estimate
χ(t) ≤ Keatξ +
 t
0
Kδ(1 + ρ)ea(t−τ)
χ(τ ) dτ.
Equivalently,
e−atχ(t) ≤ Kξ +
 t
0
Kδ(1 + ρ)e−aτ χ(τ ) dτ.
By an application of Gronwall’s inequality,
x(t, ξ) = χ(t) ≤ Kξe(Kδ(1+ρ)+a)t
. (7.6)
In particular, the solution t → x(t, ξ) is defined for all t ≥ 0.
For α ∈ B0
ρ(Rk, R) and G ∈ B1
δ (Rk × R, R), if the graph
Mα := {(x, y) ∈ Rk × R : y = α(x)}
is an invariant set for the system (7.2), then the function
t → y(t, ξ, α) := α(x(t, ξ, α))
is a solution of the differential equation
y˙ = Uy + G(x, y) (7.7)
with initial condition y(0, ξ,α) = α(ξ). Equivalently, by variation of param￾eters and with the notational definition γ(t) := y(t, ξ, α),
e−tU γ(t) − α(ξ) =  t
0
e−τUG(χ(τ ), α(χ(τ ))) dτ.402 7. Hyperbolic Theory
Note that
e−tU γ(t) ≤ Ke−btρχ(t) ≤ K2ρξe(Kδ(1+ρ)+a−b)t (7.8)
and by the theorem hypothesis a − b < 0. In view of these facts, if
0 <δ< b − a
K(1 + ρ)
,
then limt→∞ e−tU γ(t) = 0 and
α(ξ) = −
 ∞
0
e−τUG(χ(τ ), α(χ(τ ))) dτ. (7.9)
Conversely, if α ∈ B0
ρ(Rk, R) satisfies the integral equation (7.9), then
the graph of α is an invariant manifold. To prove this statement, consider a
point (ξ,α(ξ)) on the graph of α, and redefine χ(t, ξ) = x(t, ξ, α) and γ(t) =
α(χ(t, ξ)). The desired result asserts that γ is a solution of the differential
equation (7.7). Using the new notation and integral equation (7.9),
d
dt

e−tU γ(t)

= − d
dt  ∞
0
e−(t+τ)UG(χ(τ,χ(t, ξ)), α(χ(τ,χ(t, ξ)))) dτ
= − d
dt  ∞
0
e−(t+τ)UG(χ(τ + t, ξ), α(χ(τ + t, ξ))) dτ
= − d
dt  ∞
t
e−sUG(χ(s, ξ), α(χ(s, ξ))) ds
= e−tUG(χ(t, ξ), γ(t)).
In other words,
e−tU γ˙(t) − e−tU U γ(t) = e−tUG(χ(t, ξ), γ(t)).
Therefore γ is a solution of the differential equation (7.7), as required.
Proposition B: Suppose that ρ > 0. If δ > 0 is sufficiently small,
F ∈ B1
δ (Rk ×R, Rk), and G ∈ B1
δ (Rk ×R, R), then the Lyapunov–Perron
operator Λ defined by
Λ(α)(ξ) := −
 ∞
0
e−tUG(x(t, ξ, α), α(x(t, ξ, α))) dt
is a contraction on the complete metric space B0
ρ(Rk, R).
Let us first prove that, for sufficiently small δ > 0, the Lyapunov–Perron
operator Λ maps the metric space B0
ρ(Rk, R) into itself. To show that Λ(α)
is Lipschitz with Lip(Λ(α)) ≤ ρ, consider ξ,η ∈ Rk and note that
Λ(α)(ξ) − Λ(α)(η) ≤ K(1 + ρ)G1
 ∞
0
e−btx(t, ξ, α) − x(t, η, α) dt.7.1 Invariant Manifolds 403
Using integral equation (7.5),
x(t, ξ, α) − x(t, η, α)
≤ Keatξ − η +
 t
0
KF1(1 + ρ)ea(t−τ)
x(τ, ξ,α) − x(τ, η, α) dτ.
After multiplying both sides of this inequality by e−at and applying Gron￾wall’s inequality,
x(t, ξ, α) − x(t, η, α) ≤ Kξ − ηe(KF 1(1+ρ)+a)t
. (7.10)
Returning to the original estimate, substituting inequality (7.10), and by
carrying out the resulting integration,
Λ(α)(ξ) − Λ(α)(η) ≤ K2δ(1 + ρ)
b − a − Kδ(1 + ρ)
ξ − η. (7.11)
For sufficiently small F1 and G1 (that is, for δ > 0 sufficiently small),
Λ(α) is a Lipschitz continuous function with Lipschitz constant less than
ρ. In fact, it suffices to choose δ such that
0 <δ< min b − a
K(1 + ρ)
, (b − a)ρ
K(1 + ρ)(K + ρ)

.
If δ > 0 is less than the first element in the brackets, then the denominator
of the fraction in inequality (7.11) is positive. If δ > 0 is less than the
second element, then the fraction is less than ρ.
In case ξ = 0, the corresponding solution of the differential equation (7.3)
is x(t, 0, α) ≡ 0. Hence, Λ(α)(0) = 0.
By estimate (7.11) with η = 0,
sup
ξ=0
Λ(α)(ξ)
ξ ≤
K2δ(1 + ρ)
b − a − Kδ(1 + ρ) < ∞.
Thus, Λ(α)E is bounded. This completes the proof that Λ is a transfor￾mation of the complete metric space B0
ρ(Rk, R) into itself.
It remains to show that Λ is a contraction.
By definition, the norm on the product Rk × R is the sum of the
Euclidean norms on the factors. Thus, because G1 is finite and the Lipschitz404 7. Hyperbolic Theory
constant for all functions in the space B0
ρ(Rk, R) does not exceed ρ,
Λ(α)(ξ) − Λ(β)(ξ)
≤ K
 ∞
0
e−btG(x(t, ξ, α), α(x(t, ξ, α))) − G(x(t, ξ, β), β(x(t, ξ, β))) dt
≤ KG1
 ∞
0
e−bt(x(t, ξ, α) − x(t, ξ, β)
+ α(x(t, ξ, α)) − α(x(t, ξ, β)) + α(x(t, ξ, β)) − β(x(t, ξ, β)) dt
≤ KG1
 ∞
0
e−bt((1 + ρ)x(t, ξ, α) − x(t, ξ, β)
+ α − βE x(t, ξ, β)) dt. (7.12)
To estimate the terms in the integrand of the last integral in display (7.12),
use integral equation (7.5) to obtain the estimate
x(t, ξ, α) − x(t, ξ, β) ≤
K
 t
0
ea(t−τ)
F(x(τ, ξ,α), α(x(τ, ξ,α))) − F(x(τ, ξ, β), β(x(τ, ξ, β))) dτ.
Then, by proceeding exactly as in the derivation of the estimate (7.12),
x(t, ξ, α) − x(t, ξ, β)
≤ KF1
 t
0
ea(t−τ)
(1 + ρ)x(τ, ξ,α) − x(τ, ξ, β) dτ
+ KF1α − βE
 t
0
x(τ, ξ, β) dτ.
After inserting inequality (7.6), integrating the second integral, and multi￾plying both sides of the resulting inequality by e−at,
e−atx(t, ξ, α)−x(t, ξ, β) ≤  t
0
K(1+ρ)F1e−aτ x(τ, ξ,α)−x(τ, ξ, β) dτ
+
KF1ξ
δ(1 + ρ) α − βE

eKδ(1+ρ)t − 1

.
An application of Gronwall’s inequality followed by some algebraic manip￾ulations can be used to obtain the inequality
x(t, ξ, α) − x(t, ξ, β) ≤ K
1 + ρ
α − βE ξe(2Kδ(1+ρ)+a)t
. (7.13)
By using inequalities (7.13) and (7.6) in the main estimate (7.12), inte￾grating, and performing some obvious manipulations,
Λ(α) − Λ(β)E ≤
2K2δ
b − a − 2Kδ(1 + ρ)
α − βE .7.1 Invariant Manifolds 405
Moreover, if
0 <δ< min b − a
2K(1 + ρ)
, b − a
2K(K +1+ ρ)

,
then Λ has a contraction constant strictly less than one, as required.
Taking into account all the restrictions on δ, if
0 <δ< b − a
K
min  ρ
(1 + ρ)(K + ρ)
, 1
2(1 + ρ)
, 1
2(K +1+ ρ)

,
then the Lyapunov–Perron operator has a fixed point whose graph is a
Lipschitz continuous invariant manifold that passes through the origin.
This completes the proof of Proposition B.
We will use the fiber contraction principle to prove the smoothness of
the invariant manifold that is the graph of the function α obtained as the
fixed point of the Lyapunov–Perron operator (see [215] for a similar proof
for the case of diffeomorphisms). To this end, let us follow the prescription
outlined after the proof of the fiber contraction theorem (Theorem 1.263).
The space of “candidates for the derivatives of functions in B0
ρ(Rk, R)”
is, in the present case, the set F = C0(Rk, L(Rk, R)) of all bounded con￾tinuous functions Φ that map Rk into the linear maps from Rk into R with
Φ(0) = 0 and with the norm
ΦF := sup
ξ∈Rk
Φ(ξ),
where Φ(ξ) denotes the usual operator norm of the linear transformation
Φ(ξ). Also, let Fρ denote the closed ball in F with radius ρ, that is,
Fρ := {Φ ∈ F : Φ ≤ ρ},
where ρ > 0 is the number chosen in the first part of the proof.
Proposition C: Suppose that β ∈ B0
ρ(Rk, R), the function t → x(t, ξ, β)
is the solution of the differential equation (7.3) with parameter β and initial
condition x(0, ξ, β) = ξ, and Φ ∈ Fρ. If F1 and G1 are both sufficiently
small, then Ψ given by
Ψ(β, Φ)(ξ) := −
 ∞
0
e−tU [Gx(x(t, ξ, β), β(x(t, ξ, β)))W(t, ξ, β, Φ)
+ Gy(x(t, ξ, β), β(x(t, ξ, β)))Φ(x(t, ξ, β))W(t, ξ, β, Φ)] dt,
(7.14)
where t → W(t, ξ, β, Φ) is the solution of the initial value problem
w˙ = Sw + Fx(x(t, ξ, β), β(x(t, ξ, β)))w
+ Fy(x(t, ξ, β), β(x(t, ξ, β)))Φ(x(t, ξ, β))w,
w(0) = I, (7.15)406 7. Hyperbolic Theory
defines a function from B0
ρ × Fρ to Fρ such that (with K from the expo￾nential estimates (7.4))
W(t, ξ, β, Φ) ≤ Ke(KF 1(1+ρ)+a)t
. (7.16)
If, in addition, β is continuously differentiable and if Λ denotes the Lyapunov–
Perron operator, then DΛ(β) = Ψ(β, Dβ).
To prove Proposition C, start by establishing that W is continuous and
it satisfies the integral equation
W(t, ξ, β, Φ) := etS +
 t
0
e(t−s)S[Fx(x(s, ξ, β), β(x(s, ξ, β)))W(s, ξ, β, Φ)
+Fy(x(s, ξ, β), β(x(s, ξ, β)))Φ(x(s, ξ, β))W(s, ξ, β, Φ)] ds.
(7.17)
The integral equation is simply obtained by applying the variation of param￾eters formula to the differential equation (7.15); the continuity of W follows
from the continuity of the solutions of differential equations with respect to
parameters because the right-hand side of the differential equation is con￾tinuous in (t, ξ, β, Φ). This fact is not obvious. But, it can be proved from
the observation that the terms on the right-hand side of the differential
equation can all be rewritten as compositions of continuous functions. For
example, to show that
(t, ξ, β, Φ) → Φ(x(t, ξ, β))
is continuous, note that x is continuous and the function Rk × Fρ →
L(Rk, R) given by (ζ, Φ) → Φ(ζ) is continuous. The continuity of the last
function follows from the continuity of the elements of F and the estimate
Φ1(ζ) − Φ2(η)≤Φ1(ζ) − Φ1(η) + Φ1 − Φ2F .
We will show that the improper integral in the definition of Ψ is con￾vergent. Using the hypotheses of the theorem and estimating in the usual
manner,
Ψ(β, Φ)(ξ) ≤  ∞
0
Ke−btG1(1 + ρ)W(t, ξ, β, Φ) dt. (7.18)
An upper bound for W(t, ξ, β, Φ) can be obtained from the integral
equation (7.17). In fact, estimating once again in the usual manner,
W(t, ξ, β, Φ) ≤ Keat +
 t
0
KF1(1 + ρ)e(t−s)aW(s, ξ, β, Φ) ds.
After multiplying both sides of this inequality by e−at and then applying
Gronwall’s inequality,
W(t, ξ, β, Φ) ≤ Ke(KF 1(1+ρ)+a)t
. (7.19)7.1 Invariant Manifolds 407
If the inequality (7.19) is inserted into the estimate (7.18), with
F1 ≤ δ < b − a
K(1 + ρ)
,
and the resulting integral is evaluated, then
Ψ(β, Φ)(ξ) ≤ K2G1(1 + ρ)
b − a − Kδ(1 + ρ)
. (7.20)
Thus, the original integral converges. Moreover, if the quantity G1 is
sufficiently small—the upper bound
G1 ≤ δ ≤ ρ(b − a − Kδ(1 + ρ))
K2(1 + ρ)
will suffice—then
Ψ(β, Φ)(ξ) ≤ ρ.
Finally, it is easy to check that Ψ(β, Φ)(0) = 0. Therefore, Ψ(β, Φ) ∈ Fρ,
as required.
Continuing with the proof, if β is continuously differentiable, then the
solution t → x(t, ξ, β) of the differential equation (7.3) given by
x˙ = Sx + F(x, β(x))
is continuously differentiable by Theorem 1.277. Moreover, with Φ := Dβ
the matrix function W(t, ξ, β) := xξ(t, ξ, β) (which is the solution of the
first variational equation of the differential equation (7.3)) is the corre￾sponding solution of the integral equation (7.17). In this case, the inte￾grand of the integral expression for Λ(β)(ξ) is clearly a differentiable func￾tion of ξ with derivative exactly the integrand of the integral expression
for Ψ(β, Dβ)(ξ). As we have shown above, this integrand is bounded above
by an integrable function. Thus, differentiation under the integral sign is
justified, and in fact, DΛ(β) = Ψ(β, Dβ), as required. This completes the
proof of the proposition.
The next task is to prove
Proposition D: The function
Γ : B0
ρ(Rk, R
) × Fρ → B0
ρ(Rk, R
) × Fρ
given by (β, Φ) → (Λ(β), Ψ(β, Φ)), is a continuous fiber contraction.
The proof has two main steps: showing that Ψ is a contraction with
respect to its second argument and proving the continuity of Γ.
To prove Ψ is a contraction with respect to its second argument, let
β ∈ B0
ρ(Rk, R) and consider preliminary estimates analogous to those made408 7. Hyperbolic Theory
previously with corresponding definitions of W1 and W2 given by
Ψ(β,Φ1)(ξ) − Ψ(β, Φ2)(ξ)
≤K
 ∞
0
e−bt(GxW1 + GyΦ1W1 − GxW2 − GyΦ2W2) dt
≤K
 ∞
0
e−btG1((1 + ρ)W1 − W2 + W2Φ1 − Φ2F ) dt, (7.21)
where, for notational convenience, the arguments of some of the functions
in the integrands are suppressed. Proposition C provides a useful upper
bound for W2. Thus, the main task is to determine an upper bound for
W1 − W2 with Φ1 − Φ2 as one of its factors. The proof begins with
definition (7.17) and uses Gronwall’s inequality.
Using shorthand notation with the obvious meaning and estimating as
usual,
W1 − W2 ≤ K
 t
0
e(t−s)aFx(W1 − W2) + Fy(Φ1W1 − Φ2W2) ds
≤ KF1(1 + ρ)
 t
0
e(t−s)aW1 − W2 ds
+ KF1Φ1 − Φ2F
 t
0
e(t−s)aW2 ds. (7.22)
To bound the integral in the second term, insert the upper bound for W2
in proposition C and combine the exponentials to obtain
 t
0
e(t−s)aW2 ds ≤ Keat  t
0
e(KF 1(1+ρ)+a)s−as ds
= Keat  t
0
e(KF 1(1+ρ)+a)s−as ds
= eat
F1(1 + ρ)
(eKF 1(1+ρ)t − 1)
and, after some rearrangement,
e−atW1 − W2 ≤ K
1 + ρ
Φ1 − Φ2F (eKF 1(1+ρ)t − 1)
+ KF1(1 + ρ)
 t
0
e−asW1 − W2 ds. (7.23)
By an application of Gronwall’s inequality and more rearrangement,
W1 − W2 ≤ K
1 + ρ
Φ1 − Φ2F (e(2KF 1(1+ρ)+a)t − e(KF 1(1+ρ)+a)t
),
(7.24)7.1 Invariant Manifolds 409
which is the desired inequality for the difference of W1 and W2.
Returning to inequality (7.21), substituting Eqs. (7.16) and (7.24), and
performing some algebraic manipulations,
Ψ(β, Φ1)(ξ)−Ψ(β, Φ2)(ξ)
≤ K2G1
 ∞
0
e(2KF 1(1+ρ)+a−b)t dt Φ1 − Φ2F
≤
K2G1
b − a − 2KF1(1 + ρ)
Φ1 − Φ2F . (7.25)
Thus, if
0 <δ< b − a
2K(K +1+ ρ)
,
F1 ≤ δ, and G1 ≤ δ, then
0 <
K2G1
b − a − 2KF1(1 + ρ) < 1,
and therefore Γ is a fiber contraction.
Remark: As should be clear by now, the same triangle and Gronwall
estimates are repeated with slight variations several times. The spectral
gap hypothesis appears when convergence of an improper integral depends
on the exponential e−(b−a)t factor in its integrand. These, together with
the flexibility to make the norms of F and G as small as desired, are of
course the key ingredients of the proof technique.
To complete the proof of Proposition D, we must show that Γ is contin￾uous. As in the proof of Theorem 1.265, it suffices to show that the map Ψ
is continuous with respect to its first argument. This result follows from
Lemma D: Suppose that Φ ∈ Fρ, H : Rk × R → L(R, R), and K :
Rk × R → L(Rk, R). If H and K are bounded continuous functions, then
the functions Δ : B0
ρ(Rk, R) → Fρ, given by
Δ(β)(ξ) =  ∞
0
e−tUH(x(t, ξ, β), β(x(t, ξ, β)))Φ(x(t, ξ, β))W(t, ξ, β, Φ)) dt,
and Υ : B0
ρ(Rk, R) → Fρ, given by
Υ(β)(ξ) =  ∞
0
e−tUK(x(t, ξ, β), β(x(t, ξ, β)))W(t, ξ, β, Φ)) dt,
where W is defined in Proposition C, are continuous.
The proof of Lemma D is rather long; we will outline the ideas of the
proof for the function Δ.
It suffices to show that Δ is continuous at each point of its domain; that
is, for each α ∈ B0
ρ(Rk, R) and each  > 0, there is some δ > 0 such that
Δ(β) − Δ(α)F <  whenever β − αE < δ.410 7. Hyperbolic Theory
For notational convenience, let
h(t, ξ, β) := H(x(t, ξ, β), β(x(t, ξ, β))).
By using the definition of Δ, the hyperbolic estimates, and the triangle
inequality, the proof is reduced to showing that the supremum over ξ ∈ Rk
of each of the three integrals,
I1 := K
 ∞
0
e−btH0ΦF W(t, ξ, β) − W(t, ξ, α) dt,
I2 := K
 ∞
0
e−btH0Φ(x(t, ξ, β)) − Φ(x(t, ξ, α))W(t, ξ, α) dt,
I3 := K
 ∞
0
e−btH(t, ξ, β) − H(t, ξ, α)ΦF W(t, ξ, α) dt,
is less than /3 whenever β − αE is sufficiently small.
The inequality (7.19) implies the convergence of the integrals I1, I2, and
I3. Thus, there is some T > 0 such that the supremum over ξ ∈ Rk of each
corresponding integral over [T,∞] is less than /6 uniformly with respect
to β. With this estimate in hand, it suffices to show that the supremum
over ξ ∈ Rk of each corresponding integral over the compact interval [0, T]
is bounded above by /6 for sufficiently small β −αE . In other words, the
general problem is continuity with respect to a parameter for an integral
over a finite time interval.
Considering, for example, I1, the requirement is to show that the function
h : B0
ρ(Rk, R) → F given by
h(β)(ξ) :=  T
0
e−btW(t, ξ, β) dt
is continuous. From advanced calculus, a function of the form
β →
 T
0
ω(t, β) dt
is known to be continuous provided that the function ω is continuous. But
when formulated as a theorem, usually ω : R × Rm → Rn; that is, for
β a variable in a finite-dimensional space. The usual argument procedes
by showing the function is continuous at each point α ∈ Rm by using the
uniform continuity of ω on a compact set of the form [0, T] × B where B
is a closed ball in Rm with center at α. In our case, the set B0
ρ(Rk, R) is
not compact because it is a ball in an infinite-dimensional Banach space.
Construction of a proof that works in Banach spaces is only slightly more
complicated; it uses the compactness of [0, T]. The idea is simple: Pick
α ∈ B0
ρ(Rk, R) and  > 0, and, for each t ∈ [0, T], use the continuity of ω7.1 Invariant Manifolds 411
to find a product neighborhood {s : |s − t| < δt}×{β : β − αE < δt} in
R × Bρ(Rk, R) on which
ω(s, β) − ω(t, α) < 
T .
Because [0, T] is compact, there are finitely many such neighborhoods that
cover [0, T] × {α}. For a positive δ less than the minimum of the corre￾sponding {δt1 , δt2 ,...,δtN }, we have that
ω(t, β) − ω(t, α < /T
whenever β − αE < δ, as required.
To complete the proof of Proposition D, it suffices to show that the
function ω : [0, T] × B0
ρ(Rk, R) → F, given by ω(t, β)(ξ) = W(t, ξ, β), the
function p : [0, T]×B0
ρ(Rk, R) → F, given by p(t, β)(ξ) = Φ(x(t, ξ, β)), and
the function h : [0, T] × B0
ρ(Rk, R) → F, given by h(t, β)(ξ) = H(t, ξ, β),
are continuous. We have already indicated the idea for the proof of this
fact for ω and p (see the discussion following display (7.17)); the proof for
h is similar. This completes the outline of the proof of Lemma D and the
proof of Proposition D.
Continuing with the proof of the theorem, let us define (φ0, Φ0) = (0, 0) ∈
B0
ρ(Rk, R) × Fρ and note that Dφ0 = Φ0. Also, let us define recursively a
sequence {(φn, Φn)}∞
n=0 by
(φn+1, Φn+1) := Γ(φn, Φn) = (Λ(φn), Ψ(φn, Φn)).
If Dφn = Φn, then, by Proposition C, DΛ(φn) = Ψ(φn, Φn) and DΛ(φn) ∈
Fρ. Thus, Dφn+1 = DΛ(φn) = Ψ(φn, Φn)=Φn+1. Moreover, if α is the
fixed point of the Lyapunov–Perron operator, then by the fiber contraction
theorem and the fact that Fρ is a complete metric space, there is some
Φ∞ ∈ Fρ such that
limn→∞ φn = α, limn→∞ Φn = Φ∞.
The sequence {φn}∞
n=0 converges in E0(Rk, R) to α and its sequence
of derivatives converges uniformly to a continuous function—an element
of Fρ. By Theorem 1.264, α is continuously differentiable with derivative
Φ∞, provided that the convergence of the sequence {φn}∞
n=0 is uniform.
While the norm in E0(Rk, R) is not the uniform norm, the convergence
is uniform on compact subsets of Rk. As differentiability and continuity
are local properties, the uniform convergence on compact subsets of Rk of
the sequence {φn}∞
n=0 to α is sufficient to obtain the desired result: α is
continuously differentiable with derivative Φ∞.
For a direct proof that the function α is continuously differentiable, con￾sider ξ, h ∈ Rk and note that, by the fundamental theorem of calculus, if412 7. Hyperbolic Theory
n is a positive integer, then
φn(ξ + h) − φn(ξ) =  1
0
d
dtφn(ξ + th) dt =
 1
0
Φn(ξ + th)h dt.
If we pass to the limit as n → ∞ and use the uniform convergence of
{Φn}∞
n=0 to the continuous function Φ∞, then we have the identity
α(ξ + h) − α(ξ) =  1
0
Φ∞(ξ + th)h dt,
and consequently the estimate
α(ξ + h) − α(ξ) − Φ∞(ξ)h≤  1
0
Φ∞(ξ + th)h dt −
 1
0
Φ∞(ξ)h dt
≤ h
 1
0
Φ∞(ξ + th) − Φ∞(ξ) dt.
The Lebesgue dominated convergence theorem can be used to show that
the last integral converges to zero as h → 0. This proves that Dα = Φ∞,
as required. ✷
As a remark, note that for the existence and smoothness of the invariant
manifold in Theorem 7.1, both F1 and G1 are required to be less than
the minimum of the numbers
(b − a)ρ
K(1 + ρ)(K + ρ)
, b − a
2K(1 + ρ)
, b − a
2K(K +1+ ρ)
.
Of course, if K is given, then there is an optimal value of ρ, namely, the
value that makes the minimum of the three numbers as large as possible.
Theorem 7.1 requires that the nonlinear terms F and G in the differential
equation (7.2) have sufficiently small C1-norms over the entire product
space Rk × R. Clearly, we cannot expect a differential equation whose
linearization at a rest point has a spectral gap to also have globally small
corresponding nonlinear terms. To overcome this difficulty, we will use the
following observation: By restricting attention to a sufficiently small open
set containing the rest point, the C1-norm of the nonlinear terms can be
made as small as we like.
Suppose that the coordinates are already chosen so that the rest point is
at the origin and the differential equation is given in a product neighbor￾hood of the origin in the form
x˙ = Sx + f(x, y), y˙ = Uy + g(x, y), (7.26)
where f and g, together with all their first partial derivatives, vanish at
the origin.7.1 Invariant Manifolds 413
Let δ > 0 be given as in the proof of Theorem 7.1. Choose a ball Br at
the origin with radius r > 0 such that
sup
(x,y)∈Br
Df(x, y) <
δ
3
, sup
(x,y)∈Br
Dg(x, y) <
δ
3
.
Then, using the mean value theorem, we have that
sup
(x,y)∈Br
f(x, y) <
δr
3 , sup
(x,y)∈Br
g(x, y) <
δr
3 .
Moreover, there is a smooth “bump function” (cf. Exercise 7.29) γ : Rk ×
R → R, also called in this context a “cut-off function,” with the following
properties:
(i) γ(x, y) ≡ 1 for (x, y) ∈ Br/3;
(ii) the function γ vanishes on the complement of Br in Rk × R;
(iii) γ = 1 and Dγ ≤ 2/r.
With these constructions, it follows that
D(γ · f)≤Dγf + γDf <
2
r
δr
3

+
δ
3 = δ (7.27)
with the same upper estimate for D(γ · G).
If we define F(x, y) := γ(x, y)f(x, y) and G(x, y) := γ(x, y)g(x, y), then
the system
x˙ = Sx + F(x, y), y˙ = Uy + G(x, y)
has a global C1 invariant manifold. The subset of this manifold that is
contained in the ball Br/3 is an invariant manifold for the system (7.26).
If the rest point is hyperbolic, so that a < 0 < b in Theorem 7.1, then we
have proved the existence and uniqueness of a stable manifold at the rest
point. In particular, solutions starting on this invariant manifold converge
to the origin as t → ∞. To obtain the existence of an unstable manifold,
simply reverse the direction of the independent variable, t → −t, and apply
Theorem 7.1 to the resulting differential equation.
Of course, the local invariant manifolds that are produced in the manner
just described may very well be just small portions of the entire invariant
manifolds at the rest point. It’s just that one of the global invariant man￾ifolds may not be the graph of a function. If Ws
loc(0, 0) denotes the local
stable manifold for a rest point at the origin, and if φt denotes the flow
of the corresponding differential equation, then we can define the stable
manifold by
Ws(0, 0) := 	
t≤0
φt(Ws
loc(0, 0)).
It can be shown that Ws(0, 0) is an immersed disk. A similar statement
holds for the unstable manifold.414 7. Hyperbolic Theory
In case the rest point is not hyperbolic, consider the system
x˙ = Sx + f(x, y, z), y˙ = Uy + g(x, y, z), z˙ = Cz + h(x, y, z),
where a change of coordinates has been made such that in the new coordi￾nates (x, y, z) ∈ Rk × R × Rm the spectrum of S is in the left half-plane,
the spectrum of U is in the right half-plane, and the spectrum of C lies on
the imaginary axis. We may group the first and last equations so that the
system is expressed in the form:
y˙ = Uy + g(x, y, z), 
x˙
z˙

=

S 0
0 C

x
z

+

f(x, y, z)
h(x, y, z)

.
In this case, there is a spectral gap bounded by a = 0 and some b > 0. An
application of Theorem 7.1 produces a “center-stable manifold”—a mani￾fold Wcs(0, 0) given as the graph of a smooth function α : Rk × Rm → R.
Using a reversal in time, a similar grouping of the second and third equa￾tions, and a second application of Theorem 7.1, produces a center-unstable
manifold Wcu(0, 0) given as the graph of a smooth function ω : R ×Rm →
Rk. The intersection of these manifolds is denoted by Wc(0, 0) and is a
center manifold for the original system (that is, an invariant manifold that
is tangent to the eigenspace of the linear system corresponding to the eigen￾values with zero real parts). To prove this fact, we will show that Wc(0, 0)
is given, at least locally, as the graph of a function μ : Rm → Rk × R with
μ(0) = 0 and Dμ(0) = 0.
There is a technical issue that depends on the choice of the number ρ.
Recall that ρ > 0 was used in the proof of Theorem 7.1 as the bound on
the Lipschitz constants for the functions considered as candidates for fixed
points of the Lyapunov–Perron operator. In case 0 <ρ< 1, we will show (as
a corollary of Theorem 7.1) that there is a smooth global center manifold.
In case ρ > 1, we will show that there is a local center manifold. Of course,
in the proof of Theorem 7.1 we were free to choose ρ < 1 as long as we
were willing to take the C1-norms of the nonlinear terms sufficiently small,
perhaps smaller than is required to prove the existence of the center-stable
and the center-unstable manifolds.
Suppose that 0 <ρ< 1. If there is a smooth function ν : Rm → Rk with
ν(0) = 0 that satisfies the functional equation
ν(z) = ω(α(ν(z), z), z), (7.28)
then (it is easy to check that) Wc(0, 0) is (locally) the graph of the smooth
function ζ → (ν(ζ), α(ν(ζ), ζ)). In other words, the intersection of the
graphs of α and ω (near the origin at least) is exactly
{(x, y, z) : x = ν(z), y = α(ν(z), z)}.7.1 Invariant Manifolds 415
To solve the functional equation (7.28), consider the Banach space
E0(Rm, Rk) with the E-norm as defined in the proof of Theorem 7.1, the
subset B0
ρ(Rm, Rk) consisting of all elements of E0(Rm, Rk) whose Lipschitz
constants do not exceed ρ, and the operator Λ that is defined for functions
in B0
ρ(Rm, Rk) by
Λ(ν)(ζ) := ω(α(ν(ζ), ζ), ζ).
The symbol Λ previously denoted the Lyapunov–Perron operator. It is used
here because the proof that each of these operators has a smooth fixed
function is essentially the same.
Proposition 7.2. If the Lipschitz constants of α and ω do not exceed ρ <
1, then Λ has a continuously differentiable fixed point in B0
ρ(Rm, Rk).
Proof. If ν ∈ B0
ρ(Rm, Rk), then Λ(ν) is continuous on Rm. A proof that Λ
is a contraction on B0
ρ(Rm, Rk) starts with the inequality
Λ(ν)(ζ1) − Λ(ν)(ζ2) ≤ ρζ1 − ζ2. (7.29)
To prove it, utilize Lipschitz continuity of ω and the definition of Λ:
Λ(ν)(ζ1) − Λ(ν)(ζ2) ≤ Lip(ω)(α(ν(ζ1), ζ1), ζ1) − (α(ν(ζ2), ζ2), ζ2)
≤ Lip(ω)α(ν(ζ1), ζ1) − α(ν(ζ2), ζ2), ζ1 − ζ2
≤ Lip(ω) max{α(ν(ζ1), ζ1)−α(ν(ζ2), ζ2), ζ1−ζ2}.
By similar estimations for α and in turn for ν, the operative quantity
Λ(ν)(ζ1) − Λ(ν)(ζ2) is bounded above by
Lip(ω) max{Lip(α) max{Lip(ν)ζ1 − ζ2, ζ1 − ζ2}, ζ1 − ζ2}.
This proves the desired inequality because all Lipschitz constants in this
latter expression do not exceed ρ.
Inequality (7.29) implies Λ(ν)(ζ1) ≤ ρζ1; therefore, Lip(Λ(ν)) ≤ ρ
and Λ(ν)E < ∞. These statements imply that Λ maps the complete
metric space B0
ρ(Rm, Rk) into itself.
Because 0 <ρ< 1 and
Λ(ν1)(ζ) − Λ(ν2)(ζ) ≤ Lip(ω) Lip(α)ν1(ζ) − ν2(ζ) ≤ ρ2ν1 − ν2E ζ,
Λ is a contraction on B0
ρ(Rm, Rk). In particular, it has a globally attracting
fixed point ν ∈ B0
ρ(Rm, Rk).
The fiber contraction principle can be used to prove that ν is smooth.
In fact, the proof is analogous to the proof of smoothness in the invariant
manifold theorem 7.1 (see also the discussion after the fiber contraction
theorem (Theorem (1.263))).416 7. Hyperbolic Theory
Consider the set F = C0(Rm, L(Rm, Rk)) of all bounded continuous func￾tions Φ that map Rm into the bounded linear maps from Rm into Rk with
Φ(0) = 0 and with the norm
ΦF := sup
ξ∈Rk
Φ(ξ)
where, as before, Φ(ξ) denotes the operator norm of the transformation
Φ(ξ). Also, let Fρ denote the closed ball in F with radius ρ, that is,
Fρ := {Φ ∈ F : Φ ≤ ρ}.
For φ ∈ B0
ρ(Rm, Rk), Φ ∈ Fρ, and using Leibniz notation according to the
variable names assigned when functions were first defined, let
Ψ(φ, Φ)(ζ) := ωy(α(φ(ζ), ζ), ζ)[αx(α(φ(ζ), ζ), ζ)Φ(ζ) + αz(φ(ζ), ζ), ζ)]
+ ωz(α(φ(ζ), ζ), ζ).
It is easy to check that Ψ maps B0
ρ(Rm, Rk) × Fρ into Fρ. Moreover, if φ
is continuously differentiable, then DΛ(φ) = Ψ(φ, Dφ).
The transformation Γ : B0
ρ(Rm, Rk) × Fρ → B0
ρ(Rm, Rk) × Fρ given by
Γ(φ, Φ) = (Λ(φ), Ψ(φ, Φ)) is a fiber contraction. In fact, we have
Ψ(φ, Φ1)(ζ) − Ψ(φ, Φ2)(ζ) ≤ ρ2Φ1 − Φ2.
Also, let us define Φ∞ to be the unique fixed point of the map Φ → Ψ(ν, Φ)
where, recall, ν is the unique fixed point of Λ.
Let (φ0, Φ0) = (0, 0) ∈ B0
ρ(Rm, Rk) × Fρ and define recursively the
sequence {(φ0, Φ0)}∞
n=0 by
(φn+1, Φn+1) = Γ(φn, Φn).
It is easy to check that Φn = Dφn for each nonnegative integer n. By the
fiber contraction principle, we have that limn→∞ φn = ν and
limn→∞ Φn = Φ∞.
As before, by using the uniform convergence on compact subsets of Rm of
the sequence {φn}∞
n=0 and the uniform convergence of {Φn}∞
n=0, we can
conclude that ν is continuously differentiable with derivative Φ∞.
If ρ > 1, let us consider the map Δ : Rk × Rm → Rk defined by
Δ(x, z) := x − ω(α(x, z), z).
An application of the implicit function theorem at the origin produces a
local solution z → ν(z) that can be used as above to define a function
whose graph is a subset Wc
loc(0, 0) of Wc(0, 0). ✷7.1 Invariant Manifolds 417
Figure 7.1: Schematic phase portrait for system (7.30) modified with a cut￾off function that removes the nonlinearity outside of the indicated disk.
Note that only the horizontal axis is a global center manifold for the mod￾ified differential equation.
We have proved that a C1 differential equation has C1 local invariant
manifolds at a rest point whenever the linearization there has a spectral
gap. It should be reasonably clear that the methods of proof used in this
section, together with an induction argument, can be used to show that if
1 ≤ r < ∞, then a Cr differential equation has Cr local invariant manifolds
at a rest point. The case of C∞, or analytic, differential equations is more
difficult. For example, an analytic differential equation may not have a C∞
center manifold (see [119, p. 126]).
Let us note that (local) center manifolds may not be unique. For example,
the rest point at the origin for the planar differential equation
x˙ = x2, y˙ = −y (7.30)
has infinitely many center manifolds (see Exercise 7.4). This fact may seem
contrary to the uniqueness of the invariant manifolds proved in Theo￾rem 7.1. The apparent contradiction arises because only one of the local
center manifolds for the differential equation (7.30) is defined globally. More
precisely, if this differential equation is modified by a cut-off function, then
only one of the local center manifolds extends as the graph of a globally
defined function (see Figure 7.1). Indeed, in the unbounded region where
the cut-off function vanishes, the modified vector field is given by the lin￾earized equations at the rest point, and for this linear system the only
invariant one-dimensional manifold that is the graph of a function over the
horizontal axis is the horizontal axis itself.
The local stable and unstable manifolds are unique. The key observation
is that, unlike for the center manifold case, the linearization at a hyperbolic418 7. Hyperbolic Theory
rest point, which defines the modified vector field in the region where the
cut-off function vanishes, is such that local invariant manifolds for the orig￾inal system would extend globally for the modified vector field as graphs
of functions. Thus, the existence of more than one local stable or unstable
manifold would violate Theorem 7.1.
Once an invariant manifold is known to exist, for example, by applying
the theory of this section, practical applications often require representation
of the invariant manifold in coordinates (via a Taylor series perhaps with
remainder) and a reduction of the dynamics to this invariant manifold
(that is, a coordinate representation of the reduced differential equation
on the invariant manifold). In the usual case, where the invariant manifold
is known to be the graph of a function α, representation and reduction
are achieved by starting with the equation y = α(x). Invariance implies
y˙ = Dα(x) ˙x. In case the graph is known to pass through some specific
point, for example, when α(0) = 0, a series expansion about this point
with unknown coefficients may be substituted into the latter equation using
the known original differential equation to obtain an equation involving x
and the unknown series coefficients. Rearrangement produces a series set
to zero. All its coefficients must vanish. This produces equations for the
unknown coefficients. With the coefficients determined (usually up to some
finite order), the series representation is substituted for y to produce the
reduced differential equation on the invariant manifold: Dα(x) ˙x = ˙y, which
is a function of x alone, the local coordinate on the invariant manifold.
Usually the latter equation can be rearranged to the desired form ˙x = h(x),
where h is a function from a subset of the space of states x to this space
of states.
Exercise 7.3. Determine the reduced differential equation on the stable mani￾fold at the origin up to order three in the variable x for the system of first-order
differential equations
x˙ = y, y˙ = x + x2
.
Exercise 7.4. Show that the system (7.30) has infinitely many local center
manifolds.
Exercise 7.5. The center manifold is defined as the intersection of two mani￾folds and proved to be a manifold. Is the intersection of two manifolds automat￾ically a manifold? Prove a theorem or give a counterexample.
Exercise 7.6. (a) Prove that all solutions of the second-order scalar ODE ¨x +
x˙ + x3 = 0 converge to zero as t → ∞. (b) Use the invariant manifold theorem
to prove that there is a family of solutions that converge to zero exponentially
fast. (c) Use the invariant manifold theorem to prove that there are families of
solutions that converge to zero with asymptotic rates ±Ct−1/2, where C is a
positive constant. To prove the desired result, a lemma is needed for ODEs of the
form ˙x = −xp + f(x) that specifies conditions on f and p such that for p > 1 the
asymptotic behavior of the system for x > 0 is governed by the behavior of the
truncated ODE ˙x = −xp. The case p = 1 is covered by the invariant manifold
theorem. Formulate and prove an appropriate lemma.7.2 Applications of Invariant Manifolds 419
Exercise 7.7. Consider the first-order system
x˙ = x + xy,
y˙ = y + z + yz,
z˙ = −2z + xz.
(a) Determine a change of variables so that the transformed system has the form
of Eq. (7.2) and display the transformed system. (b) Determine the unstable
manifold for the transformed system up to order two in the transformed variables.
(c) Determine the equation for the unstable manifold up to order two in the
original coordinates. Hint: The required algebra might best be done using an
algebraic processor.
Exercise 7.8. Consider the system
x˙ = −x,
y˙ = −y + xz,
z˙ = z + xy.
(a) There is a rest point at the origin. Describe its stable, unstable, and center
manifolds. (b) Show that the stable manifold is given (at least locally) by a
graph where one variable is a function of the other two. (c) The function in (b)
can be expressed exactly using elementary functions. What is this expression?
Hint: Solve by eliminating a variable to reach a second-order ODE. With an
exact solution in hand, choose initial conditions so that the resulting reduction
of the general solution is on the stable manifold. (d) Research question: Which
choices of quadratic terms in the system allow for exact solutions in elementary
functions?
7.2 Applications of Invariant Manifolds
The most basic application of invariant manifold theory is the rigorous
proof that the phase portraits of rest points of nonlinear systems have
invariant manifolds akin to the (linear) invariant subspaces at the zero
solution of a constant coefficient homogeneous linear system. But the appli￾cations of invariant manifold theory go far beyond this fact. It turns out
that invariant sets (for example, periodic orbits, invariant tori, etc.) also
have associated invariant manifolds. It is even possible to have a system
(called a uniformly hyperbolic system or an Anosov system) where every
orbit has associated non-trivial stable and unstable manifolds. The exis￾tence of invariant manifolds provides an important part of the analysis
required to understand the dynamical behavior of a differential equation
near an invariant set, for example, a steady state.
Another important application of invariant manifold theory arises when
we are interested in the qualitative changes in the phase portrait of a fam￾ily of differential equations that depends on one or more parameters. For420 7. Hyperbolic Theory
example, let us imagine that the phase portrait of a family at some param￾eter value has a rest point (more generally, an invariant set) that is not
hyperbolic. In this case we expect that the qualitative dynamical behavior
of the system will change—a bifurcation will occur—when the parameter
is varied. Often, if there are qualitative changes, then they are confined to
changes on a center manifold. After all, the dynamics on stable and unsta￾ble manifolds is very simple: asymptotic attraction in forward or backward
time to the invariant manifold. This observation often allows the reduction
of a multidimensional problem to a much lower dimensional differential
equation restricted to the center manifold as we will now explain.
Let us consider a differential equation that depends on a parameter ν.
Moreover, let us assume that the differential equation has a rest point
whose position in space is a smooth function of ν near ν = 0. In this case,
there is a change of coordinates that fixes the rest point at the origin and
transforms the system to the form
x˙ = S(ν)x + F(x, y, z, ν),
y˙ = U(ν)y + G(x, y, z, ν),
z˙ = C(ν)z + H(x, y, z, ν),
where S, U, and C are matrices that depend on the parameter. More￾over, C(0) has eigenvalues with zero real parts, S(0) has eigenvalues with
negative real parts, and U(0) has eigenvalues with positive real parts.
There is a standard “trick” that is quite important. Let us introduce ν
as a new dependent variable; that is, let us consider the system
x˙ = S(ν)x + F(x, y, z, ν),
y˙ = U(ν)y + G(x, y, z, ν),
z˙ = C(ν)z + H(x, y, z, ν).
ν˙ = 0.
Also, note that if we expand the matrices S, U, and C in powers of ν at
ν = 0 to obtain, for example, S(ν) = S(0)+νS(ν), then the term νS(ν)x is
a nonlinear term with respect to our new differential equation, and therefore
it can be grouped together with F(x, y, z, ν) in the first equation. Hence, by
an obvious redefinition of the symbols, we lose no generality if we consider
the system in the form
x˙ = Sx + F(x, y, z, ν),
y˙ = Uy + G(x, y, z, ν),
z˙ = Cz + H(x, y, z, ν),
ν˙ = 0
where the matrices S, U, and C do not depend on ν. Moreover, by grouping
together the last two equations, let us view the system as having its center7.2 Applications of Invariant Manifolds 421
part augmented by one extra center direction corresponding to ν. If ν is a
vector of parameters, then there may be several new center directions.
By our general theorem, there is a center manifold given as the graph
of a function with components x = α(z,ν) and y = β(z,ν) defined on the
space with coordinates (z,ν). In particular, the center manifold depends
smoothly on the coordinate ν in some open ball containing ν = 0, and
therefore the restriction of the original differential equation to this invariant
center manifold—its center manifold reduction—depends smoothly on the
parameter ν and has the form
z˙ = Cz + H(α(z,ν), β(z,ν), z,ν). (7.31)
The “interesting” dynamics near the original rest point for ν near ν = 0
is determined by analyzing the family of differential equations (7.31). In
fact, this construction is one of the most important applications of center
manifold theory.
The qualitative behavior for center manifold reduced systems is the same
on all local center manifolds. Moreover, each bounded invariant set of the
original system, sufficiently close to the rest point under consideration,
is also an invariant set for each center manifold reduced system (see, for
example, [66]).
Exercise 7.9. (a) Determine a center manifold for the rest point at the origin
of the system
x˙ = −xy, y˙ = −y + x2 − 2y2
and a differential equation for the dynamics on this center manifold. (b) Show
that every solution of the system is attracted to the center manifold (see the
interesting article by A. J. Roberts [219]). (c) Determine the stability type of
the rest point at the origin. Hint: Look for the center manifold as a graph of a
function of the form
y = h(x) = −αx2 + βx3 + ··· .
Why does the expected h have h(0) = 0 and h
(0) = 0? The condition for
invariance is ˙y = h
(x) ˙x with y = h(x). Find the first few terms of the series
expansion for h, formulate a conjecture about the form of h, and then find h
explicitly. Once h is known, the dynamical equation for the flow on the center
manifold is given by ˙x = −xh(x). (Why?) (d) Find an explicit equation for the
unstable manifold of the saddle point at the origin for the system
x˙ = x − xy, y˙ = −y + x2 − 2y2
,
and find the differential equation that gives the dynamics on this invariant man￾ifold. (e) How does the phase portrait change as  passes through  = 0.
Exercise 7.10. Find the third-order Taylor series approximation of the (scalar)
center manifold reduced family at the origin, as in display (7.31), for the system
z˙ =  − z + w +
1
4
((1 + )z2 − 2wz − (1 − )w2
),
w˙ =  + z − w − 1
4
((1 + )z2 − (2 − 4)wz + (3 + )w2
).422 7. Hyperbolic Theory
Exercise 7.11. The system
p˙ = −1
5
p(3p5 − 5q4
p2 + 13q2
p − 3), q˙ = −1
5
q(p5 + q2
p − 1)
derived in Exercise 1.152 has a semi-hyperbolic rest point at (p, q) = (1, 0). Use
a center manifold reduction to determine the phase portrait in a neighborhood
of this rest point. Hint: Use the hint for part (c) of Exercise 7.9.
Exercise 7.12. Show that the three-dimensional system
x˙ = z(x − y) − zx3
,
y˙ = z(x + y) − zy3
,
z˙ = −z + 3/2(x2 + y2
)+3x(x − y)z + 3y(x + y)z − 3x4
z − 3y4
z
has an asymptotically stable periodic orbit.
Exercise 7.13. [SIR Model] A basic model for spread of disease is the Suscep￾tible, Infected, Recovered (SIR) model:
S˙ = −bSI,
˙
I = bSI − kI,
R˙ = kI,
S + I + R = 1,
where of course the variables and rate parameters are all assumed to be nonneg￾ative. (a) Describe the meaning of each equation in the disease context. (b) Show
that the intersection Ω of the hyperplane S + I + R = 1 with the positive octant
in the state space is positively invariant, and explain why all relevant dynamics
takes place in Ω. (c) Show that the ω-limit set of every point in Ω is a rest point,
and explain the relevance of these rest points in the disease context. (d) Does the
model allow an outcome where everyone in the population is infected? (e) For
real disease applications, the rate parameters are unknown and may change with
time. Research a case study, for example, the COVID-19 epidemic, and assess
the effectiveness of modeling via SIR or more sophisticated models. Discuss the
relevance of parameter identification, numerical approximation, the introduction
of probabilistic methods, and the difference between qualitative and predictive
models. What is the difference between verification and validation of a model.
7.3 The Hartman–Grobman Theorem
In the last section we proved the existence of stable and unstable manifolds
for hyperbolic rest points of differential equations using a classic idea that
is worth repeating: The existence of the desired object, for example, an
invariant manifold, is equivalent to the existence of a fixed point for a
properly defined map on a function space, and the hyperbolicity hypothesis
is used to prove that this map is a contraction. This same idea is used in
this section to prove the Hartman–Grobman theorem (Theorem 1.40). This
result provides a perfect setting for our exploration of various aspects of
hyperbolicity theory. See [12], [184], [209], [202], [216], and [62] for the
origins of this marvelous proof, and for the original proofs see [117] and
[129].7.3 The Hartman–Grobman Theorem 423
7.3.1 Diffeomorphisms
We will prove the Hartman–Grobman theorem for discrete dynamical sys￾tems, that is, dynamical systems defined by diffeomorphisms as follows:
Suppose that Ω ⊆ Rn and F : Ω → Ω is a diffeomorphism. The orbit
of ξ ∈ Ω is the set of all iterates of ξ under transformation by F. More
precisely, the orbit of ξ is the set {F(ξ) :  ∈ Z}, where F −1 denotes the
inverse of F, we define F0(ξ) = ξ and (by induction) F+1(ξ) := F(F(ξ))
for every integer . A fixed point of the dynamical system defined by F
(that is, a point ξ such that F(ξ) = ξ) is analogous to a rest point for the
dynamical system defined by a differential equation.
There is, of course, a very close connection between the dynamical sys￾tems defined by differential equations and those defined by diffeomor￾phisms. If, for example, ϕt is the flow of an autonomous differential equa￾tion, then for each fixed t ∈ R the time t map given by ξ → ϕt(ξ) is
a diffeomorphism on its domain that defines a dynamical system whose
orbits are all subsets of the orbits of the flow. Also, a Poincar´e map is a
diffeomorphism whose orbits correspond to features of the phase portrait of
its associated differential equation. In particular, a fixed point of a Poincar´e
map corresponds to a periodic orbit of the associated differential equation.
Let ϕt be the flow of the differential equation ˙x = f(x) and recall that
t → Dφt(ζ) is the solution of the variational initial value problem
W˙ = Df(ϕt(ζ))W, W(0) = I.
In particular, if ζ is a rest point, then the solution of the initial value
problem is
Dφt(ζ) = etDf(ζ)
.
Thus, if ζ is a hyperbolic rest point and t 
= 0, then the linear transfor￾mation Dφt(ζ) has no eigenvalues on the unit circle of the complex plane.
For this reason, a fixed point of a diffeomorphism is called hyperbolic if the
derivative of the diffeomorphism at the fixed point has no eigenvalue on
the unit circle.
The next theorem is a version of the Hartman–Grobman theorem for
diffeomorphisms. Informally, it states that the phase portrait near a hyper￾bolic fixed point is the same, up to a continuous change of coordinates, as
the phase portrait of the dynamical system induced by the derivative of
the diffeomorphism evaluated at the fixed point.
Theorem 7.14 (Hartman–Grobman Theorem). Let Ω be an open sub￾set of Rn. If ζ is a hyperbolic fixed point for the diffeomorphism F : Ω → Ω,424 7. Hyperbolic Theory
then there is an open set U ∈ Rn containing ζ and a homeomorphism H
with domain U such that
F(H(x)) = H(DF(ζ)(x − ζ))
whenever x ∈ U and both sides of the equation are defined.
The proof of Theorem 7.14 is based on the idea that the conjugating home￾omorphism is the solution of a functional equation. Sufficient conditions for
the appropriate functional equation to have a unique solution are given in
the following key lemma.
Lemma 7.15. Suppose that A : Rn → Rn is an invertible hyperbolic linear
transformation and p : Rn → Rn is a smooth function such that p(0) = 0.
If 0 <α< 1 and the C1-norm of the function p is sufficiently small, then
there is a unique continuous function h : Rn → Rn such that h ≤ α,
h(0) = 0, and
h(Ax) − Ah(x) = p(x + h(x)) (7.32)
for every x in Rn.
Proof. For h : Rn → Rn, define the linear operator L by
L(h)(x) = h(Ax) − Ah(x),
the (nonlinear) operator Φ by
Φ(h)(x) = p(x + h(x)) − p(x),
and recast equation (7.32) in the form
L(h)(x) = Φ(h)(x) + p(x). (7.33)
The operator L is invertible on the Banach space C(Rn), the space of
bounded continuous transformations of Rn with the supremum norm. To
prove this fact, let us use the hyperbolicity of the linear transformation
A to decompose the space Rn as the direct sum of the invariant linear
eigenspaces Es and Eu that correspond, respectively, to the subsets of the
spectrum of A that lie inside and outside of the unit circle. Of course, if
the fixed point is a sink or a source, then there is only one eigenspace.
By Corollary 5.46, there are adapted norms (both denoted by | |) on the
eigenspaces such that the sum of these norms is equivalent to the norm
on Rn, and in addition there is a number λ, with 0 <λ< 1, such that if
x = xs + xu ∈ Es ⊕ Eu, then
|Axs| ≤ λ|xs|, |A−1xu| ≤ λ|xu|.
Also, if h is a transformation of Rn, then h can be expressed uniquely as a
sum of functions h = hs + hu where hs : Rn → Es and hu : Rn → Eu.7.3 The Hartman–Grobman Theorem 425
Using the projections to Es and Eu, let us note that
L(h)(x)=[hs(Ax) − A(hs(x))] + [hu(Ax) − A(hu(x))].
Because the eigenspaces are invariant sets for A, it follows that the equation
L(h)(x) = p(x),
where p : Rn → Rn has a solution h if and only if the “operator equations”
Ls(hs)(x) := hs(Ax) − Ahs(x) = ps(x),
Lu(hu)(x) := hu(Ax) − Ahu(x) = pu(x)
both have solutions. In particular, to prove that L is invertible, it suffices
to prove that Ls and Lu are both invertible as operators on the respec￾tive spaces C0(Rn, Es) and C0(Rn, Eu) where C0(Rn, Es), respectively
C0(Rn, Eu), denotes the space of continuous bounded functions from Rn
to Es, respectively Eu, with the adapted norm.
Let us define two additional operators S and U by
S(hs)(x) := hs(Ax), U(hu)(x) := hu(Ax)
so that
Ls(hs)=(S − A)hs, Lu(hu)=(U − A)hu.
Because A is invertible, both of the operators S and U are invertible; for
example, we have that S−1(hs)(x) = hs(A−1x). Moreover, it is easy to
prove directly from the definition of the operator norm that these operators
and their inverses all have norm one. Thus, we have
S−1A≤S−1 A ≤ λ < 1,
and therefore the operator I − S−1A is invertible. In fact, its inverse is
given by
(I − S−1A)
−1 = I +∞
=1
(S−1A)

where the Neumann series is easily proved to be absolutely convergent by
comparison with the geometric series
∞
=0
λ = 1
1 − λ.
Because the operator Ls can be rewritten in the form
Ls = S − A = S(I − S−1A),426 7. Hyperbolic Theory
it is invertible with inverse
L−1
s = (I − S−1A)
−1S−1.
Moreover, we have the following norm estimate:
L−1
s  ≤ 1
1 − λ.
Similarly, for the operator Lu, we have that
Lu = U − A = A(A−1U − I) = −A(I − A−1U)
with
A−1U ≤ λ < 1.
Therefore, the inverse of Lu is given by
L−1
u = −(I − A−1U)
−1A−1,
and, in addition, we have the norm estimate
L−1
u  ≤ λ
1 − λ <
1
1 − λ.
Using the norm estimates for the inverses of the operators Ls and Lu, it
follows that L is invertible and
L−1 <
2
1 − λ.
Let us recast equation (7.33) in the form
h = L−1Φ(h) + L−1p
and note that the solutions of equation (7.33) are exactly the fixed points
of the operator T defined by
T(h) := L−1Φ(h) + L−1p.
Also, the set
C0
α := {h ∈ C0(Rn) : h ≤ α, h(0) = 0}
is a complete metric subspace of the Banach space C0(Rn). Thus, to com￾plete the proof of the lemma, it suffices to show the following proposition:
T : C0
α → C0
α and T is a contraction.7.3 The Hartman–Grobman Theorem 427
To prove the proposition, note that if h(0) = 0, then T(h)(0) = 0 and
T(h)≤L−1(Φ(h) + p)
≤
2
1 − λ
 sup
x∈Rn
|p(x + h(x)) − p(x)| + p

≤
2
1 − λ
 sup
x∈Rn
|Dp(x)| h + p

≤
2
1 − λ

p1h + p

≤
2
1 − λ(1 + α)p1,
where  1 denotes the C1 norm. Hence, if
p1 <
1
2
α
1 + α(1 − λ),
then T is a transformation of the space C0
α. Moreover, because
T(h1) − T(h2) = L−1(Φ(h1) − Φ(h2))
≤
2
1 − λ sup
x∈Rn
p(x + h1(x)) − p(x + h2(x))
≤
2
1 − λ p1h1 − h2,
the same restriction on the size of p1 ensures that T is a contraction. ✷
Let us prove Theorem 7.14.
Proof. Assume, without loss of generality, that ζ is the origin of Rn. Also,
define A := DF(0) and note that, because F is a diffeomorphism, A is an
invertible hyperbolic linear transformation.
Choose α ∈ R such that 0 <α< 1 and α < 1/A−1. If we define f(x) :=
F(x) − Ax, then f(0) = 0 and Df(0) = 0. Thus, using the continuity of f,
there is an open neighborhood V of the origin such that the C1-norm of
the restriction of f to V is less than α/3. This norm is defined as usual in
terms of C0-norms as follows:
f1 = f + Df.
By using an appropriate bump function, as in the derivation of the esti￾mate (7.27), there is a smooth function f ∗ defined on all of Rn such that
f = f ∗ on V and the C1-norm of f ∗ (with the supremum taken over Rn)
does not exceed three times the C1-norm of the restriction of f to V ; that
is, since f1 ≤ α/3, we have that f ∗1 < α.428 7. Hyperbolic Theory
Apply Lemma 7.15 with p = f ∗ and define a new continuous function
H : Rn → Rn by
H(x) = x + h(x). (7.34)
Using equation (7.32), it is easy to see that F∗(H(x)) = H(A(x)) for all
x ∈ Rn, where F∗(x) := f ∗(x) + Ax. This function H, restricted to a
suitably small neighborhood of the origin, is a candidate for the required
local homeomorphism. Indeed, to complete the proof of the theorem, we
will show that there is an open set U containing the origin and contained
in V such that the restriction of H to U is a homeomorphism.
To prove that H is injective, let us suppose that H(x) = H(y) for some
points x and y in Rn. Using the identities
H(Ax) = F∗(H(x)) = F∗(H(y)) = H(Ay),
we have that
H(A
x) = H(A
y) (7.35)
for every integer  > 0. The same equation holds for  < 0. Indeed, we have
the identity F∗ ◦ H ◦ A−1 = H. Hence, if H(x) = H(y), it follows that
F∗(H(A−1x)) = F∗(H(A−1y)).
We will show that F∗ is injective; therefore, H(A−1x) = H(A−1y). The
desired result follows from this equation.
To show that F∗ is injective, suppose that F∗(x) = F∗(y). Using the
definition of F∗ we have the identity
x − y = A−1(f ∗(y) − f ∗(x)).
Recall that f ∗ < α. By application of the mean value theorem to f ∗,
x − y ≤ αA−1x − y.
Because α < 1/A−1, it follows that x = y and F∗ is injective.
Using equation (7.35) and the definition of H,
A
x + h(A
x) = A
y + h(A
y)
and
A
x − A
y = h(A
x) − h(A
y) ≤ 2h.
In particular, the set
{A
(x − y) :  ∈ Z}
is bounded. But this is a contradiction unless x = y. In fact, because A is
a hyperbolic linear transformation on Rn, if z 
= 0, then either
lim
→∞ A
z = ∞, or lim
→−∞ A
z = ∞.
Thus, H is injective.7.3 The Hartman–Grobman Theorem 429
There is an open neighborhood U of the origin such that its closure U¯
is compact, contained in V , and H(U¯) ⊂ V . Because H is a continuous
injective function on the compact set U¯ ⊂ Rn, an elementary argument
using point set topology [187, p. 167] shows that H restricted to U¯ is a
homeomorphism onto its image. In particular, H has a continuous inverse
defined on H(U¯). This inverse restricted to H(U) is still continuous. Thus,
H restricted to U is a homeomorphism onto its image. Finally, by Brouwer’s
theorem on invariance of domain (see [188, p. 207]), H(U) is open. ✷
7.3.2 Differential Equations
This section contains a proof of the following version of the Hartman–
Grobman theorem for a hyperbolic rest point of an autonomous differential
equation.
Theorem 7.16. Suppose that ζ is a rest point of the differential equation
x˙ = f(x) on Rn with flow ϕt and ψt is the flow of the linearized system
x˙ = Df(ζ)(x − ζ). If ζ is a hyperbolic rest point, then there is an open
subset U of Rn such that ζ ∈ U and a homeomorphism G with domain
U such that G(ϕt(x)) = ψt(G(x)) whenever x ∈ U and both sides of the
equation are defined.
Recall that C(Rn) denotes the Banach space of bounded continuous
transformations of Rn with the supremum norm.
Lemma 7.17. If A : Rn → Rn is an invertible hyperbolic linear trans￾formation and F : Rn → Rn is a homeomorphism, then the operator
Φ : C(Rn) → C(Rn) given by
Φ(g)(x) = Ag(x) − g(F(x))
is a bounded linear transformation with a bounded inverse.
Proof. If g ∈ C(Rn), then clearly x → Φ(g)(x) is a continuous trans￾formation of Rn. Also, it is clear that Φ is a linear operator. The norm
estimate
|Φ(g)(x)|≤|Ag(x)| + |g(F(x))|≤A g + g
(where A is the operator norm of the linear transformation A and g is
the supremum norm of the function g) shows that Φ is a bounded linear
operator on C(Rn).
The proof that the operator Φ has a bounded inverse is similar to the
proof of Lemma 7.15. In fact, relative to the splitting Rn = Es ⊕ Eu, the
operator Φ is given by Φ = Φs + Φu where
Φs(gs) := A ◦ gs − gs ◦ F, Φu(gu) := A ◦ gu − gu ◦ F.430 7. Hyperbolic Theory
The important point is that the operators S and U defined by
S(gs) := gs ◦ F, U(gu) := gu ◦ F
and their inverses are all bounded, and they all have operator norm one.
The operators Φs and Φu are inverted using Neumann series, as in the
proof of Lemma 7.15. ✷
Return to Theorem 7.16.
Proof. Assume that ζ = 0 and define B := Df(0). Also, note that
ψt(x) = etBx
and define A := ψ1, the time one map of the flow ψt.
By constructing an appropriate bump function γ defined on a neighbor￾hood of the origin and with f ∗ := γf + (1 − γ)B, the differential equation
x˙ = f ∗(x) (7.36)
has a complete flow ϕ∗
t together with the following additional properties:
(i) The function f ∗ : Rn → Rn has a finite Lipschitz constant.
(ii) There is an open neighborhood V of the origin such that the time
one map F := ϕ∗
1 agrees with the time one map ϕ1 of the flow ϕ1
on V .
(iii) The function p(x) := F(x) − Ax has finite C1-norm that is suffi￾ciently small so that, by Lemma 7.15, there is a function h ∈ C(Rn)
with h(0) = 0, h < 1, and
h(Ax) − Ah(x) = p(x + h(x))
for all x ∈ Rn.
Let us prove first that there is a continuous map G : Rn → Rn such that
G(F(x)) = AG(x); that is, G conjugates the time one maps of the linear
and nonlinear flows. In fact, because p has finite C1-norm, it follows that
p ∈ C(Rn) and by Lemma 7.17, there is a unique g ∈ C(Rn) such that
Ag(x) − g(F(x)) = p(x).
By defining G(x) := x + g(x), we have that
G(F(x)) = AG(x). (7.37)
Use the “time one conjugacy” G to define G : Rn → Rn by
G(x) :=  1
0
ψ−s(G(ϕ∗
s(x))) ds.7.3 The Hartman–Grobman Theorem 431
This function will be proved to be the desired conjugacy between the linear
and the nonlinear flows; that is,
ψt(G(x)) = G(ϕ∗
t (x)). (7.38)
Using the linearity of ψt, the change of variable τ = s + t − 1, the flow
property, and equation (7.37),
ψ−t(G(ϕ∗
t (x))) =  1
0
ψ−t−s(G(ϕ∗
s+t(x))) ds
=
 t
t−1
ψ−τ (ψ−1(G(ϕ∗
1(ϕ∗
τ (x))) dτ
=
 t
t−1
ψ−τ (G(ϕ∗
τ (x))) dτ.
By splitting the last integral into the two parts
 0
t−1
ψ−τ (G(ϕ∗
τ (x))) dτ +
 t
0
ψ−τ (G(ϕ∗
τ (x))) dτ,
changing the variable in the first integral to σ := τ + 1, and using the flow
property together with equation (7.37),
ψ−t(G(ϕ∗
t (x))) =  1
t
ψ−σ+1(G(ϕ∗
−1+σ(x))) dσ +
 t
0
ψ−τ (G(ϕ∗
τ (x))) dτ
= G(x),
as required.
Recall equation (7.37) and note that with t = 1 in equation (7.38)
G(F(x)) = AG(x), G(F(x)) = AG(x).
By Lemma 7.17, the function G is unique among all continuous transfor￾mations of Rn of the form G(x) = x+g(x) that satisfy the same functional
equation, provided that g ∈ C(Rn). Thus, to prove the equality G = G, it
suffices to show that the function x → G(x) − x is in C(Rn).
Using the definition of G,
G(x) − x =
 1
0
ψ−s(G(ϕ∗
s(x))) ds − x
=
 1
0
ψ−s[G(ϕ∗
s(x)) − ψs(x)] ds
=
 1
0
ψ−s[G(ϕ∗
s(x)) − ϕ∗
s(x) + ϕ∗
s(x) − ψs(x)] ds.432 7. Hyperbolic Theory
By making the usual approximations,
|G(x) − x| ≤ eB(|G(ϕ∗
s(x)) − ϕ∗
s(x)| + |ϕ∗
s(x) − ψt(x)|)
≤ eB( g + sup
0≤s≤1
|ϕ∗
s(x) − ψs(x)|).
Also, for 0 ≤ s ≤ 1,
|ϕ∗
s(x) − ψs(x)| ≤  s
0
|f ∗(ϕ∗
t (x)) − Bψt(x)| dt
≤
 s
0
|f ∗(ϕ∗
t (x)) − f ∗(ψt(x))| + |f ∗(ψt(x)) − Bψt(x)| dt
≤
 s
0
Lip(f ∗)|ϕ∗
t (x) − ψt(x)| dt + |f ∗(ψt(x)) − Bψt(x)| dt.
By the definition of f ∗,
|f ∗(ψt(x)) − Bψt(x)| = γ(x)|f(etBx) − BetBx|.
Because γ is a bump function, there is some real number M0 > 0 such
that γ vanishes for |x| > M0. Moreover, the continuous function x →
f(etBx) − BetBx is uniformly bounded on the compact product set where
0 ≤ t ≤ 1 and |x| ≤ M0 by some positive number M. Thus,
|ϕ∗
s(x) − ψs(x)| ≤  s
0
Lip(f ∗)|ϕ∗
t (x) − ψt(x)| dt + M.
By Gronwall’s inequality,
sup
0≤s≤1
|ϕ∗
s(x) − ψs(x)| ≤ MeLip(f ∗)
,
and, as a result, the function x → G(x) − x is in C(Rn).
It remains to show that G = G is a homeomorphism when restricted to
some neighborhood of the origin. Using property (iii) given above and the
proof of the Hartman–Grobman theorem for diffeomorphisms, the function
h given in property (iii) can be used to define a continuous function H by
H(x) = x + h(x) so that
F(H(x)) = H(Ax).
Thus,
G(H(Ax)) = G(F(H(x))) = AG(H(x))
and, with K := G◦H, we have K(A(x)) = A(K(x)). Moreover, the function
K has the form
K(x) = x + h(x) + g(x + h(x))7.3 The Hartman–Grobman Theorem 433
where, by the construction of G and H, the function
α : x → h(x) + g(x + h(x))
is in C(Rn) and A(α(x)) − α(Ax) = 0. By Lemma 7.17, there is only one
function α in C(Rn) that solves this functional equation. It follows that
α(x) ≡ 0. Therefore, K is the identity function and G(H(x)) = x for all
x ∈ Rn. Because there is an open set U containing the origin such that the
restriction of H to U is a homeomorphism onto its image, we must have
that G restricted to H(U) is the inverse of H. In particular, G restricted
to H(U) is a homeomorphism onto U. ✷
Exercise 7.18. (a) Determine a formula for G as in Theorem 7.16 for the dif￾ferential equation ˙x = −x + x2 and the rest point x = 0. (b) Same as part (a)
but for the rest point x = 1. Hint: Consider the change of variables y = 1/x.
Exercise 7.19. Suppose A is an invertible linear transformation of Rn. Let L
denote the set of all Lipschitz functions mapping Rn to Rn and, for α ∈ L, let
Lip(α) denote the (least) Lipschitz constant for α. Prove: There is an  > 0 such
that if α ∈ L and Lip(α) < , then A + α : Rn → Rn is continuous and bijective.
Also, prove that the inverse map is Lipschitz, hence continuous. This result is a
version of the Lipschitz inverse function theorem.
Exercise 7.20. [Toral Automorphisms] Consider the torus T2 = R2/Z2, that
is, all equivalence classes of points in the plane where two points are equivalent
if their difference is in the integer lattice. A unimodular matrix, for example,
A := 2 1
1 1
,
induces a map on T2 called a toral automorphism. (a) Prove that A is a hyperbolic
linear map (spectrum off the unit circle). (b) Prove that the map induced by A on
T2 is invertible. (c) Determine all periodic points of the induced map. (d) Prove
that the induced map has a dense orbit. (e) Show that every orbit of the induced
map has a one-dimensional stable and a one-dimensional unstable manifold, the
sets defined as the points in T2 that are asymptotic to the given orbit under
forward, respectively backward, iteration. Hyperbolic toral automorphisms are
the prototypical examples of Anosov (uniformly hyperbolic) dynamical systems
and they enjoy many interesting dynamical properties; for example, they are
“chaotic maps” where the entire phase space is a “chaotic attractor.” Also, note
that toral automorphisms are examples of area preserving dynamical systems:
the measures of subsets of the phase space do not change under iteration by the
map. (The flow of a Hamiltonian system has the same property.) (f) Prove that
hyperbolic toral automorphisms are ergodic; that is, they are area preserving
maps such that every one of their invariant sets has measure zero or measure
one. Hint: See [150]. The first-order system ˙x = 1, ˙y = α induces a flow on the
torus (where x and y are viewed as angular variables modulo one). (g) Prove that
the flow of this system is measure preserving. (h) Prove that the flow is ergodic
if α is irrational.434 7. Hyperbolic Theory
7.3.3 Linearization via the Lie Derivative
A proof of the Hartman–Grobman theorem for differential equations is
given in this section that does not require the Hartman–Grobman theorem
for diffeomorphisms (see [62]). Ideas are introduced that have many other
applications.
A fundamental concept is the Lie derivative for vector fields.
Definition 7.21. Suppose that X and Y are vector fields on Rn and φt is
the flow of X. The Lie derivative of Y in the direction of X, denoted LXY ,
is the vector field given by
LXY (x) = d
dtDφ−t(φt(x))Y (φt(x))


t=0.
Also, we will use the following elementary result.
Proposition 7.22. If X, Y , and Z are vector fields, φt is the flow of X,
and LXY = Z, then
d
dtDφ−t(φt(x))Y (φt(x)) = Dφ−t(φt(x))Z(φt(x)).
Proof. Note that
d
dtDφ−t(φt(x))Y (φt(x)) = d
dsDφ−(t+s)(φ(t+s)(x))Y (φ(t+s)(x))


s=0.
Set y = φt(x). Using the cocycle property
Dφ−t−s(z) = Dφ−t(φ−s(z))Dφ−s(z),
where z ∈ Rn (see Exercise 2.18), and the identity φt+s(x) = φs(y), we
have that
d
dsDφ−(t+s)(φt+s(x))Y (φt+s(x))


s=0 = Dφ−t(y) d
dsDφ−s(φs(y))Y (φs(y))


s=0
= Dφ−t(φt(x))Z(φt(x)).
✷
A C1 vector field f on Rn such that f(0) = 0 is called locally topolog￾ically conjugate to its linearization A := Df(0) at the origin if there is a
homeomorphism G : U → V of neighborhoods of the origin such that the
flows of f and A are locally conjugated by G; that is,
G(etAx) = φt(G(x))
whenever x ∈ U, t ∈ Rn, and both sides of the conjugacy equation are
defined. Recall that a matrix is infinitesimally hyperbolic if all of its eigen￾values have nonzero real parts.7.3 The Hartman–Grobman Theorem 435
Theorem 7.23 (Hartman–Grobman Theorem). Let f be a C1 vector
field on Rn such that f(0) = 0. If the linearization A of f at the origin is
infinitesimally hyperbolic, then f is locally topologically conjugate to A at
the origin.
Proof. For each r > 0 there is a smooth bump function ρ : Rn → [0, 1]
with the following properties: ρ(x) ≡ 1 for |x| < r/2, ρ(x) ≡ 0 for |x| > r,
and |grad ρ(x)| < 4/r for x ∈ Rn. The vector field Y = A + ξ where
ξ(x) := ρ(x)(f(x) − Ax) is equal to f on the open ball of radius r/2 at
the origin. Thus, it suffices to prove that Y is locally conjugate to A at the
origin.
Suppose that ϕt is the flow of Y . We will seek a solution G = id + η of
the conjugacy equation
G(etAx) = ϕt(G(x)), (7.39)
where η : Rn → Rn is continuous and differentiable in the direction A. Let
us first note that equation (7.39) is equivalent to the equation
e−tAG(etAx) = e−tAϕt(G(x)). (7.40)
By substituting G = id + η and differentiating both sides of the resulting
equation with respect to t at t = 0, we obtain the infinitesimal conjugacy
equation
LAη = ξ ◦ (id +η), (7.41)
where
LAη := d
dt(e−tAη(etA))


t=0 (7.42)
is the Lie derivative of η in the direction of the vector field given by A. (If G
is a conjugacy, then the right-hand side of equation (7.40) is differentiable;
therefore, in this case, the Lie derivative of G in the direction A is defined.)
We will show that if r > 0 is sufficiently small, then the infinitesimal
conjugacy equation has a bounded continuous solution η : Rn → Rn (dif￾ferentiable along A) such that G := id +η is a homeomorphism of Rn whose
restriction to the ball of radius r/2 at the origin is a local conjugacy as in
equation (7.39).
Since A is infinitesimally hyperbolic, A = A+ ⊕A−, where A+ : Rn → Rn
is a linear map whose spectrum is in the left half of the complex plane
and A− : Rn → Rn is a linear map whose spectrum is in the right half
of the complex plane. Put E+ = Range(A+) and E− = Range(A−). By
Corollary 3.3, there are positive constants C and λ such that
|etAv+| ≤ Ce−λt|v+|, |e−tAv−| ≤ Ce−λt|v−| (7.43)
for t ≥ 0, v+ ∈ E+, and v− ∈ E−. The Banach space B of bounded (in the
supremum norm) continuous vector fields on Rn, which we identify with436 7. Hyperbolic Theory
bounded continuous functions from Rn to Rn, splits into the complementary
subspaces B+ and B− of vector fields with ranges, respectively, in E+ or E−.
In particular, a vector field η ∈ B has a unique representation η = η+ +η−,
where η+ ∈ B+ and η− ∈ B−.
The function Υ on B defined by
(Υη)(x) =  ∞
0
etAη+(e−tAx) dt −
 ∞
0
e−tAη−(etAx) dt (7.44)
is a bounded linear operator Υ : B→B. The boundedness of Υ follows
from the hyperbolic estimates (7.43) and the boundedness of the projections
P ± : Rn → E±; for instance,
|etAη+(e−tAx)| ≤ Ce−λt|P +η(e−tAx)|
≤ Ce−λtP +η. (7.45)
The continuity of the function x → (Υη)(x) is an immediate consequence
of the following lemma from advanced calculus—essentially the Weierstrass
M-test—and the estimates of the integrands as in display (7.45).
Lemma 7.24. Suppose that h : [0,∞) × Rn → Rm, given by (t, x) →
h(t, x), is continuous (respectively, the partial derivative hx is continuous).
If for each y ∈ Rn there is an open set S ⊂ Rn with compact closure S¯
and a function M : [0,∞) → R such that y ∈ S, the integral  ∞
0 M(t) dt
converges, and |h(t, x)| ≤ M(t) (respectively, |hx(t, x)| ≤ M(t) ) whenever
t ∈ [0,∞) and x is in S¯, then H : Rn → Rm given by H(x) =  ∞
0 h(t, x) dt
is continuous (respectively, H is continuously differentiable and DH(x) =
 ∞
0 hx(t, x) dt).
Using the definition of LA in display (7.42) and the fundamental theorem
of calculus, it is easy to prove that Υ is a right inverse for LA, that is,
LAΥ = idB. As a consequence, if
η = Υ(ξ ◦ (id +η)) =: F(η), (7.46)
then η is a solution of the infinitesimal conjugacy equation (7.41).
The function F defined in display (7.46) maps B into B. Also, if η1 and
η2 are in B, then (by the linearity of Υ and the mean value theorem applied
to the function ξ)
F(η1) − F(η2)≤Υ ξ ◦ (id +η1) − ξ ◦ (id +η2)
≤ Υ Dξ η1 − η2.
Using the definition of ξ and the properties of the bump function ρ, we
have that
Dξ ≤ sup
|x|≤r
Df(x) − A +
4
r
sup
|x|≤r
|f(x) − Ax|.7.3 The Hartman–Grobman Theorem 437
Because Df is continuous, there is some positive number r such that
Df(x)−A < 1/(10Υ) whenever |x| ≤ r. By Taylor’s theorem (applied
to the C1 function f) and the obvious estimate of the integral form of
the remainder, if |x| ≤ r, then |f(x) − Ax| < r/(10Υ). For the number
r > 0 just chosen, we have the estimate ΥDξ < 1/2; therefore, F is
a contraction on B. By the contraction mapping theorem applied to the
restriction of F on the closed subspace B0 of B consisting of those elements
that vanish at the origin, equation (7.46) has a unique solution η ∈ B0,
which also satisfies the infinitesimal conjugacy equation (7.41).
We will show that G := id +η is a local conjugacy. Apply Proposition 7.22
to the infinitesimal conjugacy equation (7.41) to obtain the identity
d
dt(e−tAη(etAx)) = e−tAξ(G(etAx)).
Using the definitions of G and Y , it follows immediately that
d
dt(e−tAG(etAx)) = −e−tAAG(etAx) + e−tAY (G(etAx))
and (by the product rule)
e−tA d
dtG(etAx) = e−tAY (G(etAx)).
Therefore, the function given by t → G(etAx) is the integral curve of Y
starting at the point G(x). But, by the definition of the flow ϕt of Y , this
integral curve is the function t → ϕt(G(x)). By uniqueness, G(etAx) =
ϕt(G(x)). Because Y is linear on the complement of a compact set, Gron￾wall’s inequality can be used to show that the flow of Y is complete. Hence,
the conjugacy equation holds for all t ∈ R.
It remains to show that the continuous function G : Rn → Rn given by
G(x) = x + η(x) is a homeomorphism. Since η is bounded on Rn, the map
G = id +η is surjective. To prove this fact, choose y ∈ Rn, note that the
equation G(x) = y has a solution of the form x = y + z if z = −η(y + z),
and apply Brouwer’s fixed point theorem to the map z → −η(y + z) on
the ball of radius η centered at the origin. (Using similar ideas, it is also
easy to prove that G is proper; that is, the inverse image under G of every
compact subset of Rn is compact.) We will show next that G is injective.
If x and y are in Rn and G(x) = G(y), then ϕt(G(x)) = ϕt(G(y)) and, by
the conjugacy relation, etAx + η(etAx) = etAy + η(etAy). By the linearity
of etA, we have that
|etA(x − y)| = |η(etAy) − η(etAx)| ≤ 2η.
But, if u is a nonzero vector in Rn, then the function t → |etAu| is
unbounded on R (see Exercise 7.28). Hence, x = y, as required. By Brouwer’s438 7. Hyperbolic Theory
theorem on invariance of domain, the bijective continuous map G is a home￾omorphism. (Brouwer’s theorem can be avoided by using instead the fol￾lowing elementary fact: A continuous, proper, bijective map from Rn to Rn
is a homeomorphism.) ✷
Exercise 7.25. Prove that LXX = 0.
Exercise 7.26. Prove that LXY = 0 if and only if the flows of X and Y
commute; that is, φt ◦ ψs = ψs ◦ φt. Hint: Differentiate ψs = φ−t ◦ ψs ◦ φt with
respect to s and evaluate at s = 0.
Exercise 7.27. Associate the vector field X, whose principal components are
(1, 0), with the x-coordinate of the usual Cartesian coordinates in the plane. Like￾wise associate Y with the y-coordinate. (a) Show that LXY = 0 and LY X = 0.
(b) Suppose f is a diffeomorphism from some neighborhood of the origin to itself
and define Z and W to be the vector fields corresponding to the new coordinates
ξ and η given by ξ = f1(x, y) and η = f2(x, y), where f has component functions
f1 and f2. Show that LZW = 0 and LW Z = 0.
Exercise 7.28. Suppose A : Rn → Rn is a linear map and u ∈ Rn. Show that
if A is infinitesimally hyperbolic and u = 0, then the function t → |etAu| is
unbounded for t ∈ R.
Exercise 7.29. Prove that for each r > 0 there is a C∞ function ρ : Rn → [0, 1]
with the following properties: ρ(x) ≡ 1 for |x| < r/2, ρ(x) ≡ 0 for |x| > r, and
|grad ρ(x)| < 4/r for x ∈ Rn.
In the classic paper [130], Philip Hartman shows that if a>b> 0 and
c 
= 0, then there is no C1 linearizing conjugacy at the origin for the
analytic differential equation
x˙ = ax, y˙ = (a − b)y + cxz, z˙ = −bz. (7.47)
On the other hand, he proves the following two results. (1) If a C2 vector
field has a rest point such that either all eigenvalues of its linearization
have negative real parts or all eigenvalues have positive real parts, then the
vector field is locally C1 conjugate to its linearization. (2) If a C2 planar
vector field has a hyperbolic rest point, then the vector field is locally C1
conjugate to its linearization. Hartman proves the analogs of these theorems
for diffeomorphisms and then derives the corresponding theorems for vector
fields as corollaries (cf. [62]). We also note that Shlomo Sternberg proved
that the analytic planar system
x˙ = −x, y˙ = −2y + x2 (7.48)
is not C2 linearizable. Thus, it should be clear that smooth linearization is
a delicate issue.7.3 The Hartman–Grobman Theorem 439
In general, the conjugacy obtained as in the proof of Theorem 7.23 is
not smooth. This fact is illustrated by linearizing the smooth scalar vector
field given by f(x) = −ax + ξ(x) where a > 0, ξ(0) = 0, and ξ	
(0) = 0.
Suppose that ξ vanishes outside a sufficiently small open subset of the origin
with radius r > 0 so that, as in the proof of Theorem 7.23, the linearizing
transformation is G = id +η and
η(x) =  ∞
0
e−atξ(eatx + η(eatx)) dt. (7.49)
Set Ξ := ξ ◦ (id +η) and R := r + η. By an application of the (reverse)
triangle inequality, |x + η(x)| ≥ r whenever |x| ≥ R. Hence, Ξ(x)=0
whenever |x| > R. For x 
= 0, the change of variable u := eat transforms
equation (7.49) into
η(x) = 1
a
 R/|x|
1
Ξ(ux)
u2 du.
Moreover, if x > 0, then (with w = ux)
η(x) = x
a
 R
x
Ξ(w)
w2 dw,
and if x < 0, then
η(x) = −x
a
 x
−R
Ξ(w)
w2 dw.
If η were continuously differentiable in a neighborhood of the origin, then
we would have the identity
η	
(x) = 1
a
 R
x
Ξ(w)
w2 dw − Ξ(x)
ax
for x > 0 and the identity
η	
(x) = −1
a
 x
−R
Ξ(w)
w2 dw − Ξ(x)
ax
for x < 0. Because the left-hand and right-hand derivatives agree at x = 0
and Ξ(0) = Ξ	
(0) = 0, it would follow that
 R
0
Ξ(w)
w2 dw = −
 0
−R
Ξ(w)
w2 dw.
But this equality is not true in general. For example, it is not true if ξ(x) =
ρ(x)x2 where ρ is a bump function as in the proof of Theorem 7.23. In this
case, the integrands are nonnegative and not identically zero.440 7. Hyperbolic Theory
There are at least two ways to avoid the difficulty just described (cf. [62]).
First, note that the operator LA, for the case Ax = −ax, has the formal
right inverse given by
(Υη)(x) := −
 ∞
0
eatη(e−atx) dt.
Thus, the formal conjugacy is G = id +η, where
η(x) = −
 ∞
0
eatξ(e−atx + η(e−atx)) dt.
In this case, no inconsistency arises from the assumption that η	
(0) exists,
and this method does produce a smooth conjugacy for the scalar vector
fields under consideration here.
Another idea that can be used to search for a smooth conjugacy is to
differentiate both sides of the desired conjugacy relation
etAG(x) = G(φt(x))
with respect to t at t = 0. Or, equivalently, we can seek a conjugacy G =
id +η such that
DG(G−1(y))f(G−1(y)) = Ay,
where y = x + η(x); that is, the push forward of f by G is the lineariza￾tion of f at the origin. In this case, it is easy to see that η determines a
linearizing transformation G if η is a (smooth) solution of the first-order
partial differential equation
Dη(x)f(x) + aη(x) = −ξ(x).
To determine η, let φt denote the flow of f, replace x by the integral
curve t → φt(x), and note that along this characteristic curve
d
dtη(φt(x)) + aη(φt(x)) = −ξ(φt(x))
(see Section 6.6.4). By variation of parameters, we have that
d
dt eatη(φt(x)) = −eatξ(φt(x));
and, after integration, we have the identity
eatη(φt(x)) = η(x) −
 t
0
easξ(φs(x)) ds.
Recall that a function h : Ω ⊆ Rk → Rn is called H¨older continuous on
Ω if there are positive constants c > 0 and 0 < ν ≤ 1 such that
|h(x) − h(y)| ≤ c|x − y|
ν7.3 The Hartman–Grobman Theorem 441
for all x, y ∈ Ω. The number ν is called the H¨older exponent. If η is con￾tinuously differentiable, η(0) = 0, Dη(0) = 0, and Dη is H¨older continuous
with exponent η, then (using Taylor’s theorem)
|η(x)| ≤  1
0
|Dη(sx)x| ds = c
1 + ν
|x|
1+ν.
By Corollary 4.17, if x is near x = 0 and  > 0 is given, then there is some
constant C > 0 such that |φt(x)| ≤ Ce(−a)t
. Combining these estimates,
it is easy to see that there is some constant K > 0 such that
|eatη(φt(x))| ≤ Ke((1+η)−ηa)t
.
Hence, if 0 <  < aη/(1 + η), then
limt→∞|eatη(φt(x))| = 0
and we would have
η(x) =  ∞
0
eatξ(φt(x)) dt; (7.50)
that is, the function η defined by the smooth linearizing transformation
G = id +η is given by the formula (7.50).
If for example ξ ∈ C2, then the function η defined in display (7.50) is C1
and G = id +η is a smooth linearizing transformation (see Exercise 7.35).
As a test of the validity of the methodology in this section, consider the
differential equation ˙x = −ax+x2, where a > 0. Its flow φt can be computed
explicitly (see Ex. (7.18)) and used to evaluate the integral (7.50) to obtain
the smooth near-identity linearizing transformation G : (−a, a) → R given
by
G(x) = x +
x2
a − x. (7.51)
A discussion of linearization would not be complete without mention of
Dulac–Poincar´e linearization. This methodology includes an algorithm for
producing linearizing transformations as an application of a far-reaching
theory, other aspects of which are addressed in Ch. 11.
Consider an n × n-matrix A, a homogeneous quadratic vector function
Q, and the differential equation
x˙ = Ax + Q(x) + N (x), (7.52)
where N is given by a convergent power series at the origin and consists
of terms all of which are at least third order in the components of the
variable x. Because the three terms defining the vector field corresponding
to the differential equation all vanish at x = 0, this value is a rest point. A
natural question: Is there a linearizing transformation? If it exists, is there
an algorithm to produce it as a power series in x at x = 0?442 7. Hyperbolic Theory
A key idea is to seek a function mapping Rn into itself that is a diffeomor￾phism on some open set containing x = 0 and transforms the differential
equation to the form
x˙ = Ax + C(x) + O(|x|
4), (7.53)
where C is a homogeneous cubic and the big-O notation is used to indicate
the presence of terms all of order at least four. Because the setting consists
of polynomials, a natural choice for the desired change of coordinates is
a transformation of the form x → x − H(x) where H is a homogeneous
quadratic akin to Q. Near the origin H is small compared to the linear
term. It is called a near-identity change of variables. An application of the
inverse function theorem implies that the transformation is indeed a diffeo￾morphism in some neighborhood of the origin. Moreover, its inverse has the
form x → x + H(x) + O(|x|
4). Using this latter observation, respecting the
orders of terms, and changing variables as usual, the differential equation
is recast in the form
y˙ = Ay + AH(y) − DH(y)Ay + Q(y) + C(y) + O(|x|
5).
Thus the desired form is obtained if H can be chosen such that
LAH(y) = DH(y)Ay − AH(y) = Q(y). (7.54)
Suppose for the moment that such an H exists. The same idea may be
applied to the differential equation (7.53) by considering transformations
of the form x → x − H(x) where H is a homogeneous cubic. Iteration
proceeds as long as the corresponding equation (7.54) is solvable. As might
be expected, solvability depends on the eigenvalues of A. A new concept
is required to state the solvability condition: The set of eigenvalues of a
matrix
{λ1, λ2, λ3,...,λn}
is resonant if there are nonnegative integers ai and 1 ≤ m ≤ n such that
λm = a1λ1 + a2λ2 + ··· + anλn and a1 + a2 + a3 + ··· + an ≥ 2. The
eigenvalues are called non-resonant if no resonance exists. This method
is the main idea used to prove a notable theorem: The differential equa￾tion (7.52) can be formally linearized (that is, with out regard to con￾vergence) exactly when its linear part A has non-resonant eigenvalues. A
perhaps more important result of the theory addresses the case where res￾onances are present. A monomial xm1 xm2 ··· xmn in the components of x
is called resonant if there is a relation among the eigenvalue of A of the
form λm = m1λ1 + m2λ2 + ··· + mnλn. A useful theorem states that there
is a transformation of system (7.52) to the form ˙x = Ax + R(x) where all
the monomials of R are resonant. Sometimes this system (which is said to
be in a normal form) is simpler to analyze, its form may reveal symmetries7.3 The Hartman–Grobman Theorem 443
not apparent in the original system, or it might produce a useful approx￾imation of solutions near the origin. Of course, all systems that can be
transformed to the same normal form are qualitatively the same in some
small neighborhood of the origin. In some cases, where linearization at a
rest point does not produce a hyperbolic system matrix, the local phase
portrait can be determined by examining the resonant nonlinear terms of
a normal form.
Exercise 7.30. Apply the Dulac–Poincar´e method to obtain a transformation
of ˙x = −x + x2 to the form ˙x = −x + bx4 + O(x5). Exhibit the transformation
and determine the value of b.
Exercise 7.31. (a) Discuss linearization of system (7.47) with respect to the
Dulac–Poincar´e method. (b) Discuss linearization of system (7.48) with respect
to the Dulac–Poincar´e method.
Exercise 7.32. Let a, b, c, α, β, and γ be real numbers. Consider the differential
equation
x˙ = y + ax2 + bxy + cy2
,
y˙ = αx2 + βxy + γy2
.
Show: There is a near-identity change of coordinates that transforms the system
into the Bogdanov–Takens normal form.
x˙ = y + ··· ,
y˙ = αx2 + (2a + β)xy + ··· ,
where third- and higher-order terms are indicated with + ··· . Notes: Local phase
portraits near the origin for all systems that can be transformed to this normal
form are the same. Behavior of all nearby systems (a hypothesis that can be made
rigorous) is captured by the two-parameter family
x˙ = y,
y˙ = λ1 + λ2y + x2 ± xy,
for which there is a well-developed theory: the Bogdanov–Takens bifurcation.
Exercise 7.33. Why are H¨older exponents restricted to the interval (0, 1]?
Exercise 7.34. Verify that the function G defined in display (7.51) is a lin￾earizing transformation (at the origin) for the differential equation ˙x = −ax+x2,
where a > 0.
Exercise 7.35. Suppose that a > 0,r > 0, and ξ : R → [0, ∞) is a C2 function
which vanishes in the complement of the interval (−r, r). Also, suppose that
ξ(0) = ξ
(0) = 0. Show that if r is sufficiently small and φt is the flow of the
differential equation ˙x = −ax + ξ(x), then η defined in display (7.50) is a C1
function.8
Continuation of Periodic Solutions
A fundamental engineering problem is to determine the response of a phys￾ical system to an applied force. In this chapter some mathematical ideas
are introduced that can be used to address a classic case of this problem
where the physical system is an oscillator that is modeled by a differential
equation with periodic orbits and the applied force is modeled as a small
periodic perturbation. Partial answers to several important questions will
be given. Which, if any, of the unperturbed periodic orbits persist under the
perturbation? Are the perturbed periodic orbits stable? Can the perturbed
periodic orbits be approximated by analytic formulas? Although we will
restrict most of our discussion to planar systems—the case of most prac￾tical value, many of the results of the theory presented here can be easily
generalized to multidimensional systems (see [213] for some results in the
spirit of this chapter). On the other hand, the multidimensional case will
be discussed in detail for systems with first integrals and in the exercises.
Continuation theory has a long history in applied science and math￾ematics; it is still an active area of mathematical research. Thus, there
is a mathematical and scientific literature on this topic that is far too
extensive to be reviewed here. Nevertheless, every student of the subject
should be aware of the classic books by Aleksandr A. Andronov, Aleksandr
A. Vitt, and Semen E. Khaiken [10]; Nikolai N. Bogoliubov and Yuri A.
Mitropolsky [30]; Chihiro Hayashi [133], Nikolai Minorsky [182], and James
J. Stoker [241], and the more recent works of Mikl´os Farkas [98], John Guck￾enheimer, and Philip Holmes [119]; Jack K. Hale [122]; Jirair K. Kevorkian
and Julian D. Cole [151]; James Murdock [189]; Ali H. Nayfey [194]; and
Stephen W. Wiggins [261].
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
C. Chicone, Ordinary Differential Equations with Applications, Texts in
Applied Mathematics 34, https://doi.org/10.1007/978-3-031-51652-8 8
445446 8. Continuation of Periodic Solutions
8.1 A Classic Example: van der Pol’s Oscillator
An important mathematical model in the history of our subject is known
as van der Pol’s equation
x¨ + (x2 − 1) ˙x + ω2x = a sin Ωt. (8.1)
After this differential equation was introduced by Lord Rayleigh [212] in
1883, it has been suggested as a model for many different physical phe￾nomena. For example, Balthasar van der Pol [249] investigated it more
extensively when he studied the equation in 1926 as a model of the voltage
in a triode circuit. Then, just two years later, van der Pol and Johannes van
der Mark [251] proposed the equation as a model for the human heartbeat.
In this introduction, we will use the differential equation (8.1) to illustrate
some of the ideas that will be explored more fully later in this chapter.
Let us observe some of the dynamical features of the van der Pol equa￾tion. If a = 0 and  = 0, then equation (8.1) is the familiar model of a linear
spring; that is, a spring with restoring force modeled by Hooke’s law. This
equation is often referred to as the spring equation or the harmonic oscilla￾tor. The term a sin Ωt represents a periodic external force with amplitude a,
period 2π/Ω, and frequency Ω. The term (x2−1) ˙x can be viewed as repre￾senting a nonlinear damping. The “direction” of this damping depends on
the state (x, x˙) of the system where x represents position and ˙x represents
velocity. In fact, the energy of the spring is given by
E := 1
2
x˙
2 +
1
2
ω2x2,
and has time derivative
E˙ = ax˙ sin Ωt − (x2 − 1) ˙x2.
Thus, the external forcing and the nonlinear damping cause energy fluctu￾ations. Energy due to the damping leaves the system while |x| > 1 and is
absorbed while |x| < 1.
Our subject is motivated by the following basic question: If the current
state of the system is known, what does the model predict about its future
states? Even though the van der Pol equation has been studied intensively,
we cannot give a complete answer to this question. Nevertheless, as we will
see, many useful predictions can be made. In particular, in this section we
will show how to determine the steady-state behavior of the system when
there is no external force and the damping is small.
Let us consider the unforced, weakly damped, scaled van der Pol equation
given by
x¨ + (x2 − 1) ˙x + x = 0. (8.2)
The corresponding unperturbed ( = 0) equation ¨x + x = 0 is explicitly
solvable. Indeed, the solution with initial state (x0, x˙ 0) is given by
t → x0 cost + ˙x0 sin t.8.1 A Classic Example: van der Pol’s Oscillator 447
y y
x
Perturbed orbit
Poincar´e section
P( )
(ξ, 0)
x
Figure 8.1: The left panel depicts the phase portrait for the harmonic oscil￾lator. The right panel shows a perturbed orbit with initial state (ξ, 0) on
the positive x-axis that returns to the positive x-axis at the point P(ξ, ).
In particular, all solutions of the unperturbed system, except for the solu￾tion corresponding to the rest point at (0, 0), are periodic with period 2π.
Hence, there is no problem predicting the future states of the unperturbed
system.
What happens when  = 0? Does the differential equation (8.2) have a
periodic solution? If it does, then can we find a “formula” that represents
the solution? Or, if this is not possible, how can we approximate the periodic
solution? Is the periodic solution stable? We will approach such questions
using the geometric interpretation of the differential equation as a system
in the phase plane; that is, as the equivalent first-order system given by
x˙ = −y,
y˙ = x − (x2 − 1)y. (8.3)
Here, the choice ˙x = y works just as well, but the minus sign ensures
that trajectories move in the positive sense of the usual orientation of the
Euclidean plane.
If  = 0, then all orbits of system 8.3, except the rest point at the origin,
are circles that intersect the positive x-axis as shown in the left panel of
Figure 8.1. To investigate the orbits of the system (8.3) for  = 0, we will
consider the Poincar´e map defined on the positive x-axis.
Let us note that if  = 0 is sufficiently small, then the orbit of the solution
of system (8.3) with initial condition (x(0), y(0)) = (ξ, 0) remains close to
the circle with radius ξ at least until it returns to the x-axis after a finite
time T (ξ, ) that depends on the initial point and the value of . More
precisely, if t → (x(t, ξ, ), y(t, ξ, )) is the solution of system (8.3) with
initial condition
x(0, ξ, ) = ξ, y(0, ξ, )=0,448 8. Continuation of Periodic Solutions
then, as long as  is sufficiently small, the trajectory of this solution will
return to the positive x-axis at the point with coordinate x(T (ξ, ), ξ, ).
The function (ξ, ) → P(ξ, ) given by
P(ξ, ) := x(T (ξ, ), ξ, ) (8.4)
is called the parametrized return map (see the right panel of Figure 8.1).
If P(ξ, ) = ξ, then t → (x(t, ξ, ), y(t, ξ, )) is a periodic solution of the
system (8.3) with period T (ξ, ). In other words, if ξ is a fixed point of
the map ξ → P(ξ, ) or a zero of the associated displacement function
δ(ξ, ) = x(T (ξ, ), ξ, ) − ξ, then (ξ, 0) is the initial point for a periodic
orbit of the perturbed system.
Because δ(ξ, 0) ≡ 0, it is natural to look for the root ξ implicitly as a
function β of  such that, for  = 0, the point ξ = β() is the initial point
of a periodic solution of system (8.3). More precisely, we seek a function β
defined on some neighborhood of  = 0 in R such that δ(β(), ) ≡ 0. The
obvious way to find an implicit solution is to apply the implicit function
theorem (Theorem 1.275).
In the present context, the displacement function is defined by δ : U ×
V → R where U and V are both open subsets of R. Moreover, we have that
δ(ξ, 0) ≡ 0. If there were some point (ξ, 0) such that δξ(ξ, 0) = 0, then by
the implicit function theorem there would be an implicit solution and our
problem would be solved. But it is clear that the hypothesis of the implicit
function theorem is not satisfied. In fact, because δ(ξ, 0) ≡ 0, we have that
δξ(ξ, 0) ≡ 0. As we will see, however, the implicit function theorem does
apply after a further reduction.
Let us use the Taylor series of δ at  = 0 to obtain the equation
δ(ξ, ) = δ(ξ, 0) + O(
2),
where the O(2) term denotes the remainder. This notation is used formally
in the following way: The statement f() = g() + O(2) means that there
are constants K > 0 and 0 > 0 such that the inequality
|f() − g()| < K2
holds for || < 0. The required reduction is accomplished by defining a
new function
Δ(ξ, ) := δ(ξ, 0) + O()
so that
δ(ξ, ) = (δ(ξ, 0) + O()) = Δ(ξ, ).
Clearly, if there is a function  → β() such that Δ(β(), ) ≡ 0, then
δ(β(), ) ≡ 0.
Even though the implicit function theorem does not apply to the dis￾placement function δ, it might well apply to the function Δ. At any rate,8.1 A Classic Example: van der Pol’s Oscillator 449
we have reduced the original search for a periodic solution of the per￾turbed van der Pol equation to the problem of finding implicit solutions
of the equation Δ(ξ, ) = 0. Thus, by the implicit function theorem, we
have the following proposition: If ξ > 0 is a simple zero of the function
ξ → Δ(ξ, 0), that is, Δ(ξ, 0) = 0 and Δξ(ξ, 0) = 0, or equivalently if
δ(ξ, 0) = 0 and δξ(ξ, 0) = 0, then an implicit solution ξ = β() exists. The
function ξ → δ(ξ, 0) is called the reduced displacement function, and a sim￾ple zero of the reduced bifurcation function (respectively the corresponding
unperturbed periodic orbit) is called a continuation point of periodic solu￾tions of the system (8.3) (respectively a continuable periodicorbit). Also, a
periodic orbit is said to persist if it is continuable. The ideas used to prove
our proposition recur in every continuation problem that we will consider;
their implementation constitutes the first part, called the reduction step,
in the solution of the continuation problem.
The second part of the continuation method is the identification step,
that is, the identification of the reduced displacement function in terms of
the original differential equation. For system (8.3), perhaps the most direct
route to the identification of the reduced displacement function is via a
change to polar coordinates. But, as an illustration of a general method, let
us work directly in the original variables and identify the reduced function
by solving a variational equation derived from system (8.3).
To carry out the identification step, apply the chain rule to compute the
partial derivative
δ(ξ, 0) = ˙x(T (ξ, 0), ξ, 0)T(ξ, 0) + x(T (ξ, 0), ξ, 0)
and evaluate at  = 0 to obtain the equality
x˙(T (ξ, 0), ξ, 0) = −y(0, ξ, 0) = 0.
In particular, the function ξ → x˙(T (ξ, 0), ξ, 0)T(ξ, 0) and all of its deriva￾tives vanish. Thus, to complete the identification step it suffices to deter￾mine the partial derivative x(T (ξ, 0), ξ, 0). To do this, let us compute the
partial derivative with respect to  at  = 0 of both sides of the differen￾tial equation (8.3) to obtain a variational equation. Also, let us compute
the partial derivative with respect to  of both sides of each of the ini￾tial conditions x(0, ξ, ) = ξ and y(0, ξ, ) = 0 to obtain the corresponding
(variational) initial value problem
x˙  = −y, y˙ = x − (x2 − 1)y, x(0, ξ, 0) = 0, y(0, ξ, 0) = 0 (8.5)
whose solution is t → (x(t, ξ, 0), y(t, ξ, 0)).
The variational initial value problem (8.5) is expressed in matrix form
by
W˙ = AW + b(t), W(0) = 0 (8.6)450 8. Continuation of Periodic Solutions
where
A =
0 −1
1 0 
, b(t) =  0
(1 − x2(t, ξ, 0))y(t, ξ, 0)
,
and this nonhomogeneous 2 × 2 linear system is readily solved by the vari￾ation of parameters formula (4.1). Indeed, let us recall that the principal
fundamental matrix solution at t = 0 of the associated homogeneous linear
system W˙ = AW is the 2 × 2 matrix function t → Φ(t) with Φ =˙ AΦ and
Φ(0) = I, and the solution t → W(t) of the initial value problem (8.6) is
given by
W(t) = Φ(t)W(0) + Φ(t)
 t
0
Φ−1(s)b(s) ds. (8.7)
Moreover, for the system (8.3), we have that W(0) = 0, T(ξ, 0) = 2π, and
Φ(t) = etA =
cost − sin t
sin t cost

.
It follows that
x(t, ξ, 0) = ξ cost, y(t, ξ, 0) = ξ sin t
and, in addition,

x(2π, ξ, 0)
y(2π, ξ, 0)
= Φ(2π)
 2π
0
Φ−1(s)b(s) ds
=
 2π
0 sin s[(1 − ξ2 cos2 s)ξ sin s] ds
 2π
0 cos s[(1 − ξ2 cos2 s)ξ sin s] ds
.
After an elementary integration, we have that
δ(ξ, 0) = π
4
ξ(4 − ξ2), ξ> 0, (8.8)
and therefore ξ = 2 is a simple zero of the reduced displacement function
ξ → δ(ξ, 0). Hence, the unperturbed periodic orbit with radius 2 persists.
But since ξ = 2 is the only zero of the displacement function, all other peri￾odic orbits of the unperturbed system are destroyed by the perturbation.
In particular, there is a function  → β() defined on some neighborhood
of  = 0 such that β(0) = 2, and for each  in the domain of β the corre￾sponding van der Pol system (8.3) has a periodic orbit with initial condition
(x(0), y(0)) = (β(), 0).
The theory we have just developed to analyze the existence of continua￾tions of periodic solutions of the van der Pol equation will be generalized in
the next two sections of this chapter. In Sections 8.3.6 and 8.3.7 we will dis￾cuss a method that can be used to obtain analytical approximations of the8.1 A Classic Example: van der Pol’s Oscillator 451
perturbed periodic orbit. For an analysis of the stability of the perturbed
periodic solution see Exercise 8.3.
Let us formalize what we have done so far by considering the weakly
linear system
u˙ = Au + g(u), u ∈ R2 (8.9)
where
u =
x
y

, A =
0 −1
1 0 
, g(u) = g1(u)
g2(u)

.
By repeating the steps of the argument made for system (8.3), it is easy to
prove the following theorem.
Theorem 8.1. A simple zero of the function B : (0,∞) → R given by
ξ →
 2π
0
g1(ξ cos s, ξ sin s) cos s + g2(ξ cos s, ξ sin s) sin s ds
is a continuation point of periodic solutions of the system (8.9). Moreover,
if ξ0 is a continuation point, then B(ξ0)=0.
Exercise 8.2. Apply Theorem 8.1 to find the continuation points of periodic
solutions for the system
x˙ = −y + p(x, y), y˙ = x + q(x, y),
where p and q are entire functions with series representations given by
p = pijxi
yj
, q = qijxi
yj
.
For example, give a complete analysis when p, q are quadratic polynomials and
again when p, q are cubic polynomials.
Exercise 8.3. [Stability] Prove that for sufficiently small  the stability of the
perturbed periodic solution passing near the continuation point (ξ, 0) is deter￾mined by the size of Pξ(ξ, ). In particular, show that Pξ(ξ, ) ≥ 0 and prove the
following statements: If Pξ(ξ, ) < 1, then the periodic solution is (asymptoti￾cally) stable, and if Pξ(ξ, ) > 1, then the periodic solution is (asymptotically)
unstable. Also, note that
P(ξ, ) = P(ξ, 0) + P(ξ, 0) + O(
2
),
and therefore
Pξ(ξ, ) − 1 = (δξ(ξ, 0) + O()).
If, for example,  > 0 is sufficiently small and δξ(ξ, 0) < 0, then the periodic
orbit is stable. Thus, if  is sufficiently small, then to determine the stability, it
suffices to compute the sign of the mixed partial derivative at the continuation
point ξ. Apply your results to determine the stability of the perturbed periodic
orbit for the van der Pol equation.452 8. Continuation of Periodic Solutions
Exercise 8.4. The period of the perturbed periodic orbit for the van der Pol
oscillator is given by the function
 → T (β(), ),
where T is the return-time function that appears in the definition of the Poincar´e
map (8.4) and β is the implicit solution of the corresponding displacement func￾tion. Determine the first two terms of the Taylor series at  = 0 of the period of
the perturbed periodic orbit. Hint: Use the identity
y(T (β(), ), β(), ) ≡ 0.
We will learn a more efficient method for computing the period of the perturbed
periodic orbit in Section 8.3.6 (see Exercise 8.69).
Exercise 8.5. [Sensitivity Analysis] Given an initial value problem,
x˙ = f(x, λ), x(0) = x0,
depending on a vector of parameters, a natural question arises: How sensitive is
the solution to changes in the parameters? (a) One possible measure of sensitivity
is given by an approximation of percent change. Write the solution as t → x(t, λi)
for one of the parameters and consider the change from λi to λi + h. Expand
in a Taylor series at λi and argue that xλi (t, λi)λi/(100x(t, λi)) is a reasonable
approximation of the percent change in the value of the solution caused by a one
percent change in the parameter. (b) Using the sensitivity measure in part (a),
consider the initial value problem ˙x = ax + bx2, x(0) = x0, where a and b are
positive parameters and t = T is some fixed time. Determine the ratio of the
sensitivities with respect to a and b at time T and show that the more sensitive
parameter depends on the choice of x0. Show that there is a critical value of x0
where the ratio is unity, and find this critical value. Also, specify which parameter
is more sensitive as the initial condition passes through the critical value.
8.1.1 Continuation Theory and Applied Mathematics
Continuation theory, also called regular perturbation theory, is very useful
in applied mathematics where we wish to make predictions from a differ￾ential equation model of a physical process. In most instances, our model
is a family of differential equations; that is, the model depends on param￾eters. If a member of the family—obtained by fixing the parameters—has
a dynamical feature (for example, a rest point, periodic orbit, or invari￾ant manifold) that is relevant to the analysis of our applied problem, then
there is a natural and fundamental question: Does this feature persist if we
change the parameter values? Continuation theory is a diverse collection of
tools that can be used to answer this question in some situations.
In the rest of this chapter, we will extend the continuation theory for peri￾odic solutions introduced in Section 8.1 to cover more complex problems.
But, as in the example provided by the van der Pol equation, we will always8.1 A Classic Example: van der Pol’s Oscillator 453
look for continuations of unperturbed periodic solutions in a family of dif￾ferential equations with a small parameter. We will see that the underlying
ideas for the general continuation analysis are the same as those introduced
in this section: Construct an appropriate displacement function; reduce to
a bifurcation function whose simple zeros correspond—by an application
of the implicit function theorem—to continuation points; and identify the
reduced bifurcation function in terms of the given differential equation.
Perhaps our analysis of the continuation of periodic solutions for the gen￾eral weakly nonlinear system provides initial evidence for the notion that
the proof of a general result such as Theorem 8.1 is often easy compared
with the task of applying the result to a realistic model. For our example,
where the perturbation term is a single harmonic, the bifurcation function
is a quadratic polynomial (formula (8.8)) and its roots are therefore easy to
determine. If, however, we consider a perturbation with several harmonics,
as, for example, in Exercise 8.2, then the problem of finding the number
and position of the persistent unperturbed periodic solutions becomes more
difficult. This illustrates a maxim that lies at the heart of many problems
in applied mathematics: The more realistic the model, the more difficult it
is to apply general theorems.
Maxim number two: General theorems are always too weak. If you work
hard and are fortunate, you might develop all of the ideas necessary to prove
a classic and beautiful theorem such as Theorem 8.1. You may then go to
your collaborator, a very good engineer, and proudly announce your result:
“If ... and  is sufficiently small, then there is a periodic solution.” But you
know what is coming! Your collaborator will say, “That’s interesting, but
how small do I have to make the perturbation so that I can be sure there is
a periodic orbit?” You are now invited to find a computable number 0 > 0
and a proof that periodic solutions exist at least for || < 0. If you succeed
in doing this for the model equation (8.2), then your collaborator will be
happy for a moment. But before long she comes back to you with a new
perturbation term in mind: “Does your method apply if we add ... ?”
When confronted with an applied problem, there is a natural tendency
for a mathematician to try to prove a theorem. Perhaps by now you feel that
your contribution to the applied project is not receiving enough credit. But
in fact your results are enormously valuable. Because you have answered
some basic questions, new questions can be asked. You have also provided
a way to understand why a periodic orbit exists. After proving a few more
theorems that apply to show the existence of periodic orbits for a few
more basic model equations, your understanding of periodic orbits begins
to coalesce into a theory that gives a conceptual framework, which can
be used by you, and others, to discuss the existence of periodic orbits in
systems that are too complex to analyze rigorously.
In general, the applied mathematician faces a highly non-trivial, perhaps
impossible, task when trying to rigorously verify the hypotheses of general
theorems for realistic models of physical systems. In fact, doing so might454 8. Continuation of Periodic Solutions
require the development of a new area of mathematics. Most often, we are
left to face the realization that rigorous results can only be obtained for
simplified models.
Do not be discouraged.
The analysis of a mathematical model, even a simple one, deepens our
understanding, sharpens our formulation of results, forces us to seek new
methods of analysis, and often reveals new phenomena. In addition, rigor￾ous results for simple models provide test cases that can be used to debug
implementations of numerical methods that we intend to use to obtain
predictions from more realistic models.
When we return as mathematicians to confront a realistic model of our
original physical problem (the understanding of which is the real object
of the game), it is not always clear how to continue doing mathematics.
Instead, we turn to computation and investigate numerical methods. Per￾haps we become experts in computer algebra, or we investigate computer
graphics in order to find useful visual representations of our data, and so on.
But when our simulations are implemented, we are happy to have knowl￾edge of the range of expected phenomena, we are happy to be able to test
our code on the simplified models we have rigorously analyzed, and we are
happy to verify numerically the hypotheses of a general theorem that we
have proved. All of this helps us gain confidence in our predictions.
By running our simulations, we find evidence for an answer to our origi￾nal physical question. But during the process, we might also see unexpected
results or we conceive new ideas to improve our simulations. These experi￾ences motivate us to find additional rigorous results. Thus, we are naturally
led back to questions in mathematics. And so it goes—a natural cycle that
will be repeated many times during our attempts to understand physical
phenomena.
Our technical skills will improve and our depth of understanding will
increase as we master more sophisticated mathematical methods and learn
from the experience of doing applied mathematics. The remainder of this
chapter is intended to help provide an example of an area of applicable
mathematics as well as the opportunity to gain some useful experience with
some types of differential equations that appear as mathematical models.
8.2 Autonomous Perturbations
The topic of this section is periodic solutions for autonomous differential
equations of the form
x˙ = f(x, ) x ∈ Rn, (8.10)
where  is a parameter and the unperturbed system ( = 0) given by
x˙ = f(x, 0) (8.11)
has at least one identifiable periodic solution.8.2 Autonomous Perturbations 455
8.2.1 Poincar´e’s Method of Continuation
Suppose the unperturbed system (8.11) has a periodic solution Γ. To employ
Poincare’s method suggested by example in Section 8.1, we must define a
displacement function. To do this, choose a point v ∈ Γ and a hypersurface
Σ1 (a submanifold of dimension n − 1) in Rn such that Σ1 contains v and
the tangent space to Γ at v is orthogonal to the tangent space of Σ1 at v.
In other words, the vector f(v, 0) is normal to the hypersurface at v. By an
application of the implicit function theorem, there is an open subset Σ ⊆ Σ1
with v ∈ Σ and some 0 > 0 such that for each σ ∈ Σ and  with || < 0
the solution of the corresponding differential equation (8.10) with initial
value σ returns to Σ1 at some finite positive time. More precisely, there is
a return-time function T : Σ×(−0, 0) → R and a (parametrized) Poincar´e
map P : Σ × (−0, 0) → Σ1. The subset Σ ⊆ Σ1 is called a Poincar´e sec￾tion. At the given periodic orbit, which has some period T > 0, the return
time T (v, 0) = T and the return point is P(v, 0) = v.
For the example in Section 8.1, the Poincar´e section is a line. Here,
by allowing the Poincar´e section Σ to be a manifold, we create a new
technical problem: What is the definition of displacement on a manifold?
There are at least two options. We could define Δ : Σ × (−0, 0) → Rn by
Δ(σ, ) := P(σ, ) − σ. Displacement is then a vector in Rn. Alternatively,
because Σ1 is an (n − 1)-dimensional manifold, the displacement function
δ : Σ × (−0, 0) → Rn−1 can be represented in local coordinates. Indeed,
choose a function σ : Rn−1 → Σ ⊆ Rn such that σ(0) = v and for each
ξ ∈ Rn−1 the vector ˙σ(ξ) is a nonzero tangent vector to Σ at σ(ξ). A
displacement function is then defined by
δ(ξ, ) := σ−1(P(σ(ξ), ) − σ(ξ)). (8.12)
It is the representation of the displacement function adopted in this section.
With the choice of local coordinates such that σ(0) = v on Γ, δ(0, 0) = 0.
The desired continuation of Γ (that is, the existence of a family of periodic
orbits containing Γ and parameterized by ) is achieved when there is some
parameter value 2 with |2|≤|1| and a function β : (−2, 2) → Rn−1
such that β(0) = 0 and δ(β(), ) ≡ 0.
The desired function β exists by an application of the implicit function
theorem when its main hypothesis is satisfied: For  = 0, the derivative
of the function ξ → δ(ξ, 0) is an invertible transformation of Rn−1. This
requirement can of course be interpreted in the context of the application.
Let t → u(t, ζ, ) be the solution of the differential equation ˙x = f(x, )
with the initial condition x(0) = ζ. Rewrite Eq. (8.12) in the form
σ(δ(ξ, )) = P(σ(ξ), ) − σ(ξ) = u(T (σ(ξ), ), σ(ξ), ) − σ(ξ).456 8. Continuation of Periodic Solutions
Set  = 0, differentiate with respect to the local variable ξ, evaluate at 0,
and let I denote the identity operator to obtain the equation
Dσ(0)δξ(0, 0)Dσ(0)−1 = f(v, 0)Tσ(v, 0) + uζ (T, v, 0) − I. (8.13)
Because the return-time map (a scalar-valued function) is restricted to the
Poincar´e section Σ, its derivative with respect to its space variable x is
represented by a vector tangent to Σ. Due to the orthogonality of Σ and
f(v, 0) at v, the first term on the right-hand side of Eq. (8.13) vanishes.
Thus, the derivative δξ(0, 0) is invertible if and only if uζ (T, v, 0) − I is
invertible as a map from the tangent space of the Poincar´e section at v to
itself. In finite-dimensional space, the invertibility is thus equivalent to the
statement that the operator uζ (T, v, 0) does not have an eigenvalue equal
to one. This is a basic result for continuation of periodic orbits:
Theorem 8.6. If the family of differential equations x˙ = f(x, ) parame￾terized by  has a periodic solution Γ for  = 0 and the derivative of the
return map on a Poincar´e section orthogonal at its intersection with Γ does
not have eigenvalue one, then Γ continues in the family.
A periodic orbit is called nondegenerate if it satisfies the hypothesis of
the theorem.
Important notes: Recall that uζ (T, v, 0)f(v) = f(v). Therefore, the num￾ber one is always an eigenvalue of the monodromy map on the ambient
tangent space at v. This eigenvalue is not considered for the restriction of
this map to a Poincar´e section. Also, the operator uζ (T, v, 0) is obtained
by solving a linear differential equation. Indeed, starting with the differen￾tial equation ˙x = f(x), where the parameter has been suppressed, and the
initial value problem
u˙(t, ζ, 0) = f(u(t, ζ, 0)), u(0,ζ, 0) = ζ,
the desired quantity t → uζ (t, ζ, 0) is the matrix solution of the homoge￾neous variational equation (also called the first variational equation) given
by
W˙ = Df(u(t, ζ, 0))W (8.14)
with initial condition W(0) = I. In other words, t → uζ (t, ζ, 0) is the prin￾cipal fundamental matrix solution of the system (8.14) at t = 0 and the
desired derivative uζ (T, v, 0) is just the value of the solution of the varia￾tional initial value problem at t = T. A periodic orbit is called hyperbolic if
the derivative of the Poincar´e map at v has no eigenvalue with modulus one.
This condition is stronger than necessary for a periodic orbit to be nonde￾generate. Floquet theory may be applied to the differential equation (8.14).
Finally, it should be clear that the nature of the family parameterized by
 does not play a role as long as the dependence on  is smooth.
Finding explicit examples of isolated periodic orbits in nonlinear differ￾ential equations in Rn for n > 2 is rare. Pencil-and-paper applications of8.2 Autonomous Perturbations 457
continuation in this general context are successful only in special cases. But,
the framework presented in this section is often applied when periodic orbits
and Floquet multipliers are approximated by numerical methods (see [92]
for an accessible example). Exception is encountered in celestial mechanics,
for example, in perturbations of the two-body problem (see Ex. (8.10)). In
this case there is an additional feature: the presence of a first integral. A
modification of the theory is required to analyze this case (see Ex. (8.9)).
Exercise 8.7. (a) Show that the system (in polar coordinates)
r˙ = r(1 − r2
)
3
, ˙
θ = 0
has an isolated periodic orbit (a limit cycle). (b) Is this periodic orbit nondegen￾erate? (c) Repeat parts (a) and (b) for the system
r˙ = r(1 − r2
), ˙
θ = 0.
Exercise 8.8. One-parameter families were considered in this section. What is
true for k-parameter families with k > 1?
Exercise 8.9. (a) Suppose A is an n×n matrix, u and v are nonzero n-vectors,
u and v are orthogonal and
Au = u, vT A = vT ,
where T denotes transpose. Prove that one is an eigenvalue of A with algebraic
multiplicity at least two. Also, give an example with algebraic multiplicity two and
geometric multiplicity one. (b) Suppose that ˙x = f(x, ) is a family of differential
equations that has a corresponding family I(x, ) of (nonconstant) first integrals
and ˙x = f(x, 0) has a periodic orbit. Prove: The derivative of the monodromy map
at a point on this periodic orbit has eigenvalue one with geometric multiplicity at
least two. Prove: If the eigenvalue one has geometric multiplicity two (in which
case the periodic orbit is called elementary or nondegenerate), then the periodic
orbit continues in the family. Hint: Work in local coordinates on an appropriate
Poincar´e section with a coordinate map that depends on the parameter.
Exercise 8.10. Recall the Kepler system (6.33). It is not necessary to review
its derivation except to note that r and θ are polar coordinates. (a) Specify a
first integral by inspection of the differential equations. (b) Specify a circular
period orbit. (c) Prove that your periodic orbit is elementary using the results of
Ex. (8.9).
8.2.2 Continuation of Periodic Orbits of Planar Systems
In this section, general results on continuation discussed in the last section
are specialized to the planar case where more is known. Again the topic is
continuation—also called persistence—of periodic solutions in the family
of differential equations (8.10), but with u ∈ R2, in case the unperturbed
differential equation (8.11) has an identifiable periodic solution.458 8. Continuation of Periodic Solutions
Two main cases are considered: The periodic solution of the unperturbed
differential equation is a limit cycle (recall Definition 1.190), or the periodic
solution is a member of a one-parameter family (forming an annulus) of
periodic solutions. In the latter case, the main problem is to determine
which, if any, of the periodic solutions forming the annulus persist.
Let Γ be a periodic solution of the unperturbed system (8.11) with period
T > 0. To apply Poincar´e’s continuation method the main issue is determi￾nation of the eigenvalues of the derivative of the Poincar´e return map on a
Poincar´e section orthogonal to the Γ at some point v. In the notation of the
previous section, where u denotes the solution of the family of differential
equations, then desired derivative is uζ (T, v, 0) restricted to the tangent
space of the Poincar´e section at v.
8.2.3 Diliberto’s Theorem
For notational convenience, we may also consider the more usual notation
for the flow ϕ of the unperturbed system and note that ϕt(ζ) := u(t, ζ, 0).
Also, let t → Φ(t) be the principal fundamental matrix solution of the
unperturbed first variational equation of the unperturbed differential equa￾tion at t = 0. The following (by now familiar) proposition in the new nota￾tion is simple but fundamental:
Φ(t)f(ζ) = f(ϕt(ζ)).
To prove it, note that Φ(0)f(ζ) = f(ζ) and
d
dtf(ϕt(ζ)) = Df(ϕt(ζ))f(ϕt(ζ)).
Thus, t → f(ϕt(ζ)) and t → Φ(t)f(ζ) are solutions of the same initial value
problem, and therefore they must be equal.
Specializing to the planar case, define f ⊥ = Rf where R is the rotation
matrix  0 −1
1 0 	 and note that f and f ⊥ are linearly independent at each
point of the plane at which f is nonzero (for example, at each point on
Γ). If f(ζ) = 0, then there are two real-valued functions t → a(t, ζ) and
t → b(t, ζ) such that
Φ(t)f ⊥(ζ) = a(t, ζ)f(ϕt(ζ)) + b(t, ζ)f ⊥(ϕt(ζ)). (8.15)
We will soon find useful formulas for a and b. Before we do so, let us
note that the fundamental matrix Φ(t) is represented as a linear trans￾formation from R2, with the basis {f(ζ), f ⊥(ζ)}, to R2, with the basis
{f(ϕt(ζ)), f ⊥(ϕt(ζ))}, by the matrix
Φ(t) = 
1 a(t, ζ)
0 b(t, ζ)

. (8.16)8.2 Autonomous Perturbations 459
As in equation (8.12), the local coordinate map σ (now specialized to σ :
R → Σ) is such that σ(0) = v and ˙σ(0) is a tangent vector at v ∈ Σ ⊆ R2.
There are real constants c1 and c2 such that
σ˙(0) = c1f(v) + c2f ⊥(v).
Using the return-time map T and displacement δ,
(δξ(0, 0) + 1)(c1f(v) + c2f ⊥(v))
= Tσ(0, 0)f(v) + Φ(T)(c1f(v) + c2f ⊥(v))
= Tσ(0, 0)f(v) + c1f(v) + c2a(T,v)f(v) + c2b(T,v)f ⊥(v).
Moreover, because Σ is transverse to Γ, we have c2 = 0. Using this fact and
the linear independence of f and f ⊥, it follows that
δξ(0, 0) = b(T,v) − 1, (8.17)
Tξ(0, 0) = −c2a(T,v) + c1δξ(0, 0)
= −c2a(T,v) + c1(b(T,v) − 1). (8.18)
Let us identify the quantities a(T,v) and b(T,v) geometrically. From
equation (8.17), it is clear that b(T,v) is the local representative of the
derivative of the Poincar´e map for the unperturbed system (8.11) at {v} =
Γ ∩ Σ. Let γ be a curve in R2 such that ˙γ(0) = −f ⊥(v) (for example, by
taking t → γ(t) to be the solution of the differential equation ˙u = −f ⊥(u)
with initial condition u(0) = v ), then c1 = 0, c2 = −1, and a(T,ζ) is the
derivative of the (local representative of the) return-time map for (8.11) on
Σ at v.
The Euclidean divergence and curl of the vector function f : R2 → R2
with f(x, y)=(f1(x, y), f2(x, y)) are defined as follows:
div f(x, y) := ∂f1
∂x (x, y) + ∂f2
∂y (x, y),
curl f(x, y) := ∂f2
∂x (x, y) − ∂f1
∂y (x, y).
Also, the scalar curvature function of the smooth curve t → (x(t), y(t)) is
given by
κ := x˙y¨ − y˙x¨
( ˙x2 + ˙y2)3/2 .
We will write κ(t, ζ) to denote the scalar curvature along the curve t →
ϕt(ζ) given by the phase flow ϕt of an autonomous planar differential equa￾tion.460 8. Continuation of Periodic Solutions
Theorem 8.11 (Diliberto’s Theorem). Let ϕt denote the flow of the
differential equation u˙ = f(u), u ∈ R2. If f(ζ) = 0, then the principal fun￾damental matrix solution t → Φ(t) at t = 0 of the homogeneous variational
equation
W˙ = Df(ϕt(ζ))W
is such that
Φ(t)f(ζ) = f(ϕt(ζ)),
Φ(t)f ⊥(ζ) = a(t, ζ)f(ϕt(ζ)) + b(t, ζ)f ⊥(ϕt(ζ)),
where
b(t, ζ) = |f(ζ)|
2
|f(ϕt(ζ))|
2 e
 t
0 div f(ϕs(ζ)) ds, (8.19)
a(t, ζ) =  t
0

2κ(s, ζ)|f(ϕs(ζ))| − curl f(ϕs(ζ))	
b(s, ζ) ds. (8.20)
The integral formulas (8.19) and (8.20) for a(t, ζ) and b(t, ζ) seem to
have been first obtained by Stephen P. Diliberto [84]. We note, however,
that his formula for a(t, ζ) incorrectly omits the factor 2 of the curvature
term.
Proof. By definition
t → a(t)f(ϕt(ζ)) + b(t)f ⊥(ϕt(ζ))
is the solution of the variational equation (8.14) with initial value f ⊥(ζ).
In particular, a(0) = 0, b(0) = 1, and
a(t)Df(ϕt(ζ))f(ϕt(ζ))
+ a
(t)f(ϕt(ζ)) + b(t)Df ⊥(ϕt(ζ))f(ϕt(ζ)) + b
(t)f ⊥(ϕt(ζ))
= a(t)Df(ϕt(ζ))f(ϕt(ζ)) + b(t)Df(ϕt(ζ))f ⊥(ϕt(ζ)). (8.21)
After taking the inner product with f ⊥(ϕt(ζ)) and suppressing the argu￾ments of various functions, we obtain the equation
b
|f|
2 = b

Df · f ⊥, f ⊥−Df ⊥ · f,f ⊥
	
.
Since f ⊥ = Rf, where R =  0 −1
1 0
	
, we have
Df ⊥ · f,f ⊥ = RDf · f, Rf = Df · f,f
and
b
|f|
2 = b(Df · f ⊥, f ⊥ + Df · f,f − 2Df · f,f).8.2 Autonomous Perturbations 461
By an easy (perhaps lengthy) computation, it follows that
b = b div f − b d
dt ln |f|
2.
The solution of this differential equation with the initial condition b(0) = 1
is exactly formula (8.19).
From equation (8.21), taking the inner product this time with f(ϕt(ζ)),
we obtain
a
|f|
2 = b(Df · f ⊥, f−Df ⊥ · f,f)
= b(f ⊥,(Df)
∗f−RDf · f,f)
= b(f ⊥,(Df)
∗f + f ⊥, Df · f)
= b(f ⊥, 2Df · f + f ⊥,((Df)
∗ − (Df))f),
(8.22)
where ∗ denotes the transpose. Also, by simple computations, we have
f ⊥, 2Df · f = 2κ|f|
3,
f ⊥,((Df)
∗ − (Df))f = −|f|
2 curl f
where the scalar curvature κ, the curl, and the other functions are eval￾uated on the curve t → ϕt(ζ). After substitution of these formulas into
equation (8.22), an integration yields formula (8.20). ✷
Recall that the periodic orbit Γ with period T is hyperbolic if the deriva￾tive of the Poincar´e map on Σ at v = Γ∩Σ has no eigenvalue with modulus
one. By our geometric identification, this derivative is just b(T,v). Using
the equality |f(ϕT (v))| = |f(v)| and Diliberto’s theorem, we have the iden￾tification
b(T,v) = e
 T
0 div f(ϕt(v)) dt.
Thus, the derivative of the Poincar´e map is independent of the choice of
section Σ. In addition, by a change of variables, it is easy to see that the
derivative does not depend on v ∈ Γ. These remarks give an alternate proof
of Proposition 5.49, which we restate here in a slightly different form.
Proposition 8.12. A periodic solution t → ϕt(ζ) of u˙ = f(u) with period
T is hyperbolic if and only if
 T
0
div f(ϕt(ζ)) dt = 0. (8.23)
Also, using Eq. (8.17) together with the implicit function theorem, we
have a theorem on persistence.
Theorem 8.13. A hyperbolic periodic solution of the differential equation
u˙ = f(u) persists for autonomous perturbations.462 8. Continuation of Periodic Solutions
Exercise 8.14. Prove: If the flow ϕt of the differential equation ˙x = f(x) has
the periodic orbit Γ with period T, then  T
0 div f(ϕt(ζ)) dt does not depend on
the choice of ζ ∈ Γ.
Exercise 8.15. With respect to Proposition 8.12, suppose that Γ is the periodic
orbit corresponding to the periodic solution t → ϕt(ζ). Show that the inequality

Γ
div f ds < 0
is not sufficient to prove that Γ is a stable limit cycle.
Exercise 8.16. Suppose that Γ is a hyperbolic periodic solution with period
T of the planar system ˙u = f(u) and ζ ∈ Γ. Using the notation of Diliberto’s
theorem, define
g(ϕt(ζ)) = 1
b(t, ζ)
 a(T,ζ)
b(T,ζ) − 1 + a(t, ζ)

f(ϕt(ζ)) + f ⊥(ϕt(ζ)).
Prove the following facts: (a) Φ(t)g(ζ) = b(t, ζ)g(ϕt(ζ)). (b) g(ϕT (ζ)) = g(ζ). (c)
The vector g is nowhere parallel to f. (d) The vector field determined by g on Γ
is invariant under the linearized flow.
Exercise 8.17. [Multidimensional Systems] Suppose that ˙u = f(u), u ∈ Rn
has a periodic orbit Γ cut transversely by an (n−1)-dimensional surface Σ ⊆ Rn.
Here, transversality means that f(v) is not tangent to Σ for v = Γ ∩ Σ. (a) Show
that the analogue of Theorem 8.13 is valid in this context. Hint: Although there
is no obvious substitute for Diliberto’s formulas, the ideas of this section apply.
Use the definition of hyperbolicity, that is, the derivative of the Poincar´e map on
Σ at v has its spectrum off the unit circle in the complex plane; and then proceed
abstractly by following the same argument presented for Theorem 8.13. (b) Is
the hyperbolicity hypothesis necessary when n > 2? Can you prove a stronger
result?
Exercise 8.18. Obtain equation (8.19) using Liouville’s formula (2.20). Warn￾ing: At first sight, in the context of equation (8.19), it might appear that the
fundamental matrix for system (8.14) is given by  1 a(t)
0 b(t)
 relative to the basis
{f(ϕt(ζ)), f ⊥(ϕt(ζ))}.
But, this matrix does not represent the fundamental matrix solution in any
fixed basis. Rather, it represents a transition from the initial basis given by
{f(ζ), f ⊥(ζ)} to the basis {f(ϕt(ζ)), f ⊥(ϕt(ζ))}.
Exercise 8.19. Let t → α(t) be a nonconstant solution of the scalar second￾order differential equation ¨x = f(x). (a) Show that the principal fundamental
matrix solution Φ(t) at t = 0 of the first variational equation along α is defined
by
Φ(t) = 	 α˙ (t) β(t)
f(α(t)) β˙(t)


where ˙α(0)β˙(0) − β(0)¨α(0) = 1, β is given by t → α˙ (t)γ(t), γ is an antiderivative
of ˙α(t)
−2 defined on the time intervals where ˙x does not vanish, and β, defined by8.2 Autonomous Perturbations 463
removing singularities, is smooth on the domain of α (cf. [71]). Also, compare this
result with the fundamental matrix solution given by Diliberto’s theorem. Hint:
Use Abel’s formula. Show that the second-order ODE is reversible. A planar
system ˙x = p(x, y), ˙y = q(x, y) is called reversible if t → (x(−t), −y(−t)) is
a solution whenever t → (x(t), y(t)) is a solution. For the second-order ODE,
t → x(T −t) is a solution whenever t → x(t) is a solution. Show that if ˙x has two
zeros, then x(t) is periodic. In case ˙x has no zeros, show that γ is an antiderivative
such that ˙α(0)β˙(0) − β(0)¨α(0) = 1. In case ˙x has exactly one zero at t = T and
T ≥ 0, show that the function γ is an antiderivative on the interval (−∞, T) (or
on the maximal interval of time less than T on which the solution x is defined)
such that ˙α(0)β˙(0) − β(0)¨α(0) = 1, and γ(T + s) := −γ(T − s) for s > 0. A
similar construction is made if T < 0. In case ˙x has two zeros, x is periodic and
γ is an antiderivative defined periodically on the whole line such that the initial
condition ˙α(0)β˙(0) − β(0)¨α(0) = 1 is satisfied.
Problem 8.20. Can Diliberto’s theorem be generalized to the case of vari￾ational equations for differential equations defined in Rn for n > 2? A solu￾tion of this exercise together with some examples would perhaps make a
useful research article.
8.2.4 Preparation Theorem and Persistence of Nonhyperbolic
Periodic Orbits
To determine the persistence of periodic orbits of the differential equa￾tion (8.11), our main hypothesis, δξ(0, 0) = 0, is equivalent to requiring
the unperturbed periodic solution to be hyperbolic. Let us consider the
continuation problem for nonhyperbolic periodic orbits.
If an unperturbed planar periodic orbit is not hyperbolic, then we cannot
determine an implicit solution of the equation δ(ξ, ) = 0 by a direct appli￾cation of the implicit function theorem. Instead, the main new tool for the
analysis is the (Weierstrass) preparation theorem. The following statement
is a special case of this important result (see [9], [32], and [65]).
Theorem 8.21 (Preparation Theorem). If δ : R × R → R, given by
(ξ, ) → δ(ξ, ), is analytic (or C∞) and
δ(0, 0) = ∂δ
∂ξ (0, 0) = ∂2δ
∂ξ2 (0, 0) = ··· = ∂n−1δ
∂ξn−1 (0, 0) = 0, ∂nδ
∂ξn (0, 0) = 0,
then there are n smooth functions ai : R → R defined near  = 0 and a
function U : R × R → R defined near (ξ, ) = (0, 0) such that ai(0) = 0,
i = 1,...,n, U(0, 0) = 0, and
δ(ξ, )=(a0() + a1()ξ + ··· + an−1()ξn−1 + ξn)U(ξ, ).
The name “preparation theorem” is used because the function δ, written
in the form given in the conclusion of the theorem, is prepared for a study464 8. Continuation of Periodic Solutions
of its zeros. Moreover, because U(0, 0) = 0 (such a function U is called a
unit in the algebra of functions defined in a neighborhood of the origin),
the zeros of the function δ(ξ, ) near (ξ, ) = (0, 0) are exactly the zeros of
the Weierstrass polynomial
a0() + a1()ξ + ··· + an−1()ξn−1 + ξn.
In particular, there are at most n zeros for each fixed  near  = 0.
For the case where δ is the displacement function associated with a peri￾odic orbit Γ, the multiplicity of Γ is defined to be the degree n of the
Weierstrass polynomial. If n = 1, then Γ is hyperbolic and exactly one
continuation point of periodic solutions exists for || = 0 sufficiently small.
It follows from the preparation theorem that if Γ has multiplicity n, then
there is some choice of the function g in the differential equation (8.10)
such that n families of periodic solutions bifurcate from Γ at  = 0. But,
for each specific perturbation, the actual number of continuations can only
be determined by analyzing the coefficients of the Weierstrass polynomial.
Exercise 8.22. Show that the system
x˙ = −y + x(x2 + y2 − 1)2
,
y˙ = x + y(x2 + y2 − 1)2 (8.24)
has a limit cycle with multiplicity 2.
As an illustration of the ideas just presented, let us analyze the contin￾uation problem for a periodic orbit Γ with multiplicity 2.
Using the displacement function δ associated with Γ, we have that
δ(0, 0) = δξ(0, 0) = 0, δξξ(0, 0) = 0,
and, by the preparation theorem,
δ(ξ, )=(a0() + a1()ξ + ξ2)U(ξ, ) (8.25)
where a0(0) = 0, a1(0) = 0, but U(0, 0) = 0. We will solve for ξ implicitly
with respect to . But, in anticipation of a bifurcation at  = 0, we cannot
expect to have a smooth continuation given by a function  → β() such
that β(0) = 0 and δ(β(), ) ≡ 0. More likely, there are implicit solutions
defined for  > 0 or  < 0, but not both. For this reason, we say there are N
positive branches at the bifurcation point (0, 0) if there is some 0 > 0 and
N continuous functions β1,...,βN , each defined for 0 ≤ <0 such that
for each j = 1,...,N, βj (0) = 0, and δ(βj (), ) ≡ 0. Negative branches are
defined analogously for −0 <  ≤ 0. Of course, the number and position
of the branches is determined by the roots of the Weierstrass polynomial.8.2 Autonomous Perturbations 465
With respect to the Weierstrass polynomial in display (8.25), we have
a0() = a01 + O(
2), a1() = O(),
and therefore the roots of this Weierstrass polynomial are given by
ξ = β() = −a1() ± 
−4a01 + O(2)
2 .
If  = 0 has fixed sign and a01 > 0, then there are no real branches. On
the other hand, if a01 < 0, then there are two real branches given by
β1() = √−a01 + O(), β2() = −√−a01 + O().
To identify the coefficient a01, compute the derivatives
δ(0, 0) = a01U(0, 0),
δξξ(0, 0) = 2U(0, 0),
and note that
a01 = 2δ(0, 0)/δξξ(0, 0). (8.26)
Of course δξξ(0, 0) is just the second derivative of the unperturbed Poincar´e
map ξ → σ−1P(σ(ξ), 0). A formula for the derivative δ(0, 0) will be com￾puted below.
As an example, consider applying the result in equation (8.26) to the
bifurcation of limit cycles for the system
x˙ = − y + x(x2 + y2 − 1)2,
y˙ =x + y(x2 + y2 − 1)2 + (x2 − 1)y. (8.27)
By a change to polar coordinates, we have the equivalent system
r˙ =r(r2 − 1)2 + r sin2 θ(r2 cos2 θ − 1),
˙
θ =1 +  cos θ sin θ(r2 cos2 θ − 1).
Note that, for r near r = 1, if  is sufficiently small, then we can treat θ as
a time-like variable and obtain the following differential equation for r:
dr
dθ = F(r, θ, ) := r(r2 − 1)2 + r sin2 θ(r2 cos2 θ − 1)
1 +  cos θ sin θ(r2 cos2 θ − 1) . (8.28)
Also, for each ξ near ξ = 1, define the function θ → r(θ, ξ, ) to be the
unique solution of the differential equation (8.28) with the initial condition
r(0, ξ, ) = ξ.
Note that the displacement function is given by δ(ξ, ) = r(2π, ξ, ) − ξ.
Thus, to compute the partial derivative δξ(ξ, ), it suffices to solve the
variational initial value problem
d
dθ rξ = Fr(r(θ, ξ, ), ξ, )rξ, rξ(0, ξ, )=1466 8. Continuation of Periodic Solutions
to obtain the useful formula
rξ(θ, ξ, ) = e
 θ
0 Fr(r(s,ξ,),ξ,) ds.
By Exercise 8.22, the point ξ = 1 corresponds to the unperturbed limit
cycle. Thus, if we view ξ as a coordinate on the positive x-axis, then
δ(1, 0) = r(2π, 1, 0) − 1 = 0. Moreover, we have
rξ(2π, ξ, 0) = e
 2π
0 (r2−1)(5r2−1) dθ,
and therefore δξ(1, 0) = 0. By taking one more derivative with respect to
ξ, note that
δξξ(1, 0) = rξξ(2π, 1, 0) =  2π
0
8rξ dθ = 16π
is positive. To compute δ(1, 0), solve the variational initial value problem
d
dθ r = sin2 θ(cos2 θ − 1), r(0, 1, 0) = 0
to obtain
δ(1, 0) = r(2π, 1, 0) = −
 2π
0
sin4 θ dθ < 0.
By our analysis of the Weierstrass polynomial, there are two branches of
periodic solutions for small  > 0. One branch consists of stable limit cycles;
the other branch consists of unstable limit cycles. We will outline a method
for proving this fact, but the details are left to the reader.
The stability of the perturbed limit cycles is determined by δξ(β(), ). In
fact, the orbit is unstable if δξ(β(), ) > 0 and stable if δξ(β(), ) < 0. To
prove this claim, recall that δ(ξ, ) = σ−1(P(σ(ξ), )) − ξ and the stability
type is determined by the derivative of the Poincar´e map. Since δξ(0, 0) =
0, the stability type for small  is determined by the sign of δξ(0, 0). If
δξ(0, 0) > 0 and a branch of continued periodic solutions exists for  > 0,
then the branch is unstable. If the branch exists for  < 0, then the branch
is stable. If δξ(0, 0) < 0 and the branch exists for  > 0, then it is stable,
whereas, if the branch exists for  < 0, then it is unstable.
A complete analysis for autonomous perturbations in case Γ is hyperbolic
and a viable approach to the problem when Γ has finite multiplicity have
been presented. What if Γ has infinite multiplicity?
To discuss this problem, the definition of infinite multiplicity must be
clarified. A possible meaning is the statement that δ(ξ, 0) ≡ 0. This is
not the same as requiring that all partial derivatives of the displacement
function δ with respect to ξ vanish at (ξ, ) = (0, 0); maybe δ is infinitely
flat but still δ(ξ, 0) = 0 for ξ = 0. One way to make progress is to assume δ
is real analytic (it will be if the differential equation (8.10) is real analytic).
In this case at least the two possible definitions are equivalent.8.2 Autonomous Perturbations 467
Exercise 8.23. Give an example of an infinitely flat limit cycle: The periodic
orbit is isolated but ∂kδ/∂ξk(0, 0) = 0 for k = 1, 2, 3,....
8.2.5 Continuation from an Annulus of Period Orbits
Suppose that δ(ξ, 0) ≡ 0 and consider the perturbation series
δ(ξ, ) = δ(ξ, 0) +
1
2! δ(ξ, 0)
2 + O(
3).
Note that
δ(ξ, ) = (δ(ξ, 0) + O()). (8.29)
Here, since δ(ξ, 0) ≡ 0, the periodic orbit Γ is contained in a period annulus;
that is, an annulus in the plane consisting entirely of periodic orbits of the
unperturbed differential equation (8.11) (see, for example, Figure 6.2).
Although we could consider continuations from the chosen periodic orbit
Γ, the problem perhaps becomes more interesting by modifying it to con￾sideration of all of the periodic orbits in the period annulus. The goal is
then to determine if any of them persist. This is the problem discussed
here.
In view of Eq. (8.29) and an application of the implicit function theo￾rem, the reduction step is easy: A simple zero of the function ξ → δ(ξ, 0)
is a continuation point of periodic solutions. Equivalently, if δ(ξ0, 0) = 0
and δξ(ξ0, 0) = 0, then the periodic solution Γξ0 of the unperturbed sys￾tem (8.11) with initial value σ(ξ0) ∈ Σ persists.
For the identification step, a useful formula is obtained for δ(ξ, 0) by
computing the partial derivative with respect to  in the equation
σ(δ(ξ, ) + ξ) = u(T (ξ, ), σ(ξ), )
to obtain the identity
δ(ξ, 0) ˙σ(ξ) = T(ξ, 0)f(σ(ξ)) + u(T (ξ, 0), σ(ξ), 0). (8.30)
The function t → u(t, σ(ξ), 0) is the solution of the inhomogeneous varia￾tional initial value problem
W˙ = Df(ϕt(σ(ξ)))W + g(ϕt(σ(ξ))), W(0) = 0, (8.31)
where the initial condition follows from the identity u(0, σ(ξ), 0) ≡ σ(ξ).
(Differential equation (8.31) is also called the second variational equation.)
By the variation of constants formula,
u(T(ξ, 0), σ(ξ), 0) = Φ(T(ξ, 0))  T(ξ,0)
0
Φ−1(s)g(ϕs(σ(ξ))) ds468 8. Continuation of Periodic Solutions
where Φ(t) denotes the principal fundamental matrix solution of the sys￾tem (8.14) at t = 0.
Let us use the identifications given in equations (8.19) and (8.20) by first
expressing the function g in the form
g(ϕt(σ(ξ))) = c1(t, σ(ξ))f(ϕt(σ(ξ))) + c2(t, σ(ξ))f ⊥(ϕt(σ(ξ)))
with
c1(t, σ(ξ)) = 1
|f(ϕt(σ(ξ)))|
2 g(ϕt(σ(ξ))), f(ϕt(σ(ξ))),
c2(t, σ(ξ)) = 1
|f(ϕt(σ(ξ)))|
2 g(ϕt(σ(ξ))), f ⊥(ϕt(σ(ξ)))
:= 1
|f(ϕt(σ(ξ)))|
2 f(ϕt(σ(ξ))) ∧ g(ϕt(σ(ξ))).
Also, note that the inverse of the matrix (8.16) represents the action of
the inverse of the principal fundamental matrix at t = 0 from the span of
{f, f ⊥} at u(t, σ(ξ), 0) to the span of {f, f ⊥} at σ(ξ). Likewise, the matrix
in equation (8.16) evaluated at T(ξ, 0) is the matrix representation of the
fundamental matrix with respect to the basis {f, f ⊥} at σ(ξ). Thus, we
have that
Φ(T(ξ, 0)) = 1 a(T(ξ, 0), σ(ξ))
0 b(T(ξ, 0), σ(ξ)) 
,
Φ−1(s)g(ϕs(σ(ξ))) = 1
b(s, σ(ξ)) b(s, σ(ξ)) −a(s, σ(ξ))
0 1  c1(s, σ(ξ))
c2(s, σ(ξ))
,
and
u(T(ξ, 0), ξ, 0) = (N (ξ) + a(T(ξ, 0), σ(ξ))M(ξ))f(σ(ξ))
+ b(T(ξ, 0), σ(ξ))M(ξ)f ⊥(σ(ξ)) (8.32)
where
M(ξ) :=  T(ξ,0)
0
1
b(t, σ(ξ))|f(ϕt(σ(ξ)))|
2 f(ϕt(σ(ξ))) ∧ g(ϕt(σ(ξ))) dt
= 1
|f(σ(ξ))|
2
×
 T(ξ,0)
0
e−  t
0 div f(ϕs(σ(ξ))) dsf(ϕt(σ(ξ))) ∧ g(ϕt(σ(ξ))) dt,
N (ξ) :=  T(ξ,0)
0
1
|f(ϕt(σ(ξ)))|
2 g(ϕt(σ(ξ))), f(ϕt(σ(ξ))) dt
−
 T(ξ,0)
0
a(t, σ(ξ))
b(t, σ(ξ))|f(ϕt(σ(ξ)))|
2 f(ϕt(σ(ξ))) ∧ g(ϕt(σ(ξ))) dt.8.2 Autonomous Perturbations 469
After taking the inner product of both sides of equation (8.30) with the
vector f ⊥(σ(ξ)), and using the formulas for M and N , the quantity δ(ξ, 0)
is seen to be given by
δ(ξ, 0) = b(T(ξ, 0), σ(ξ))|f(σ(ξ))|
2
σ˙(ξ), f ⊥(σ(ξ)) M(ξ). (8.33)
In this formula, σ, f ˙ ⊥ = 0 because Σ is transverse to the unperturbed
periodic solutions, and b(t, ζ) = 0 because |f| does not vanish along the
unperturbed periodic orbit.
The autonomous Poincar´e–Andronov–Melnikov function is defined by
M(ξ) :=  T(ξ,0)
0
e−  t
0 div f(ϕs(σ(ξ))) dsf(ϕt(σ(ξ))) ∧ g(ϕt(σ(ξ))) dt. (8.34)
Here, ξ → T(ξ, 0) is a local representation of the period function associated
with the period annulus of the differential equation (8.11); the number
T(ξ, 0) is the minimum period of the periodic orbit labeled by ξ, that is,
the orbit passing through the point in the plane with coordinates (ξ, 0).
It should be clear that values of the function M are independent of the
choice of Poincar´e section. In fact, as long as ξ is a smooth parameter for
the periodic solutions in our period annulus, the value of M at a particular
periodic solution is not altered by the choice of the parametrization.
Theorem 8.24. Suppose that the differential equation (8.11) has a period
annulus A whose periodic solutions are parametrized by a smooth function
σ : R → A given by ξ → σ(ξ). If ξ0 is a simple zero of the function
ξ → M(ξ) given by the formula (8.34) for the perturbed system (8.10),
then the periodic solution of the unperturbed system (8.11) passing through
ξ0 is continuable.
Proof. This result follows immediately from the formula (8.33) and the
persistence of the simple zeros of ξ → δ(ξ, 0). We only remark that, in
general, if α(ξ) = β(ξ)γ(ξ) with β(ξ) nonvanishing, then the simple zeros
of α and γ coincide. ✷
Exercise 8.25. Find the continuable periodic solutions of the perturbed har￾monic oscillator in each of the following systems:
1. weakly damped van der Pol equation:
x¨ + (x2 − 1) ˙x + ω2
x = 0;
2. nonlinear weakly damped van der Pol equation:
x¨ + (x2 − 1) ˙x + ω2
x − λx3 = 0;
3. modified van der Pol equation:
x¨ + (x2 + ˙x2 − 1) ˙x + x = 0.470 8. Continuation of Periodic Solutions
8.2.6 Periodic Orbits of Multidimensional Systems with First
Integrals
Consider a k-dimensional family of differential equations
x˙ = f(x, λ), x ∈ Rn, λ ∈ Rk (8.35)
with the family of solutions t → φ(t, x, λ) such that φ(0, x, λ) = x. The
topic of this section is continuation theory for periodic orbits of the family
member with λ = 0, the unperturbed differential equation, in the presence
of first integrals. In this setting, families of periodic orbits usually exist and
these families persist.
To be precise, some definitions are required. Recall that a first integral
H is a function H : Rn × Rk → R such that d/dtH(φ(t, x, λ), λ) ≡ 0. A set
of first integrals {H1, H2,...,Hm} is called independent on U × V ⊆ Rn ×
Rk if {grad H1(x, λ), grad H2(x, λ),..., grad Hm(x, λ)} is a set of linearly
independent vectors whenever (x, λ) ∈ U × V .
Throughout this section, let Γ denote a periodic orbit with period T of
the differential equation ˙x = f(x, 0).
Proposition 8.26. If λ0 > 0 and the family (8.35) has m independent
first integrals on U × {λ : |λ| < λ0} where U ⊆ Rn is an open neighborhood
of Γ, then the geometric multiplicity of the Floquet multiplier one for Γ is
at least m + 1.
Proof. Let the first integrals be H1, H2,...,Hm and suppose that Γ0 con￾tains the point v ∈ Rn. By the hypothesis, the set
{grad H1(v, 0), grad H2(v, 0),..., grad Hm(v, 0)}
consists of linearly independent vectors. By the definition of first integrals,
the nonzero vector f(v, 0) is perpendicular (with respect to the usual metric
in Rn) to the span of these gradients.
Choose vectors
v1, v2,...,vn−m−1
in Rn so that the gradients together with these vectors span the hyperplane
Σ0 at v that is perpendicular to f(v, 0). Let Σ be a subset of Σ0 that
contains v and is a Poincar´e section for the family (8.35) for all |λ| < λ1,
where λ1 > 0, and let P : Σ × {λ : |λ| < λ1} → Σ0 denote the associated
family of Poincar´e maps.
For each i ∈ {1, 2,...,m}, ξ ∈ Σ, and |λ| < λ1, the definition of first
integrals implies
Hi(P(σ, λ), λ) = Hi(σ, λ). (8.36)
By differentiation with respect to σ,
grad Hi(v, 0)Pσ(v, 0) = grad Hi(v, 0).8.2 Autonomous Perturbations 471
In other words, the number one is an eigenvalue of the transpose of the
linear map Pσ(v, 0) with geometric multiplicity at least m. Because the
transpose of a linear map has the same eigenvalues, with the same algebraic
and geometric multiplicities as the map, one is an eigenvalue (hence, a
Floquet multiplier) of Pσ(v, 0) with geometric multiplicity at least m. The
vector f(v, 0) is orthogonal to Σ. Thus, the Floquet multiplier one for Γ
has geometric multiplicity at least m + 1, as required.
Note: The relationship between the eigenvalues and eigenvectors of a
linear transformation and its transpose is made clear by examining them
in their Jordan canonical forms. ✷
In the presence of m independent first integrals, Γ is called nondegenerate
if the geometric multiplicity of its Floquet multiplier one is m + 1.
Proposition 8.27. If Γ is nondegenerate in the presence of m independent
first integrals, then Γ is contained in an m-parameter family Υ of periodic
orbits of x˙ = f(x, 0) and this family persists.
Proof. Define Λ : Rn−1 × Rk × Rm → Rn−1 by
Λ(σ, λ, μ)=(H1(σ, λ) − H1(v, 0) + μ1,...,Hm(σ, λ) − Hm(v, 0) + μm,
(P(σ, λ) − σ) · v1,...,(P(σ, λ) − σ) · vn−m−1).
Note that Λ(v, 0, 0) = 0 and, by the multiplicity hypothesis, Λσ(v, 0, 0) :
Rn−1 → Rn−1 is invertible. By the implicit function theorem, there is a
positive number b and a unique function
β : {λ : |λ| < b}×{μ : |μ| < b} → Σ
such that Λ(β(λ, μ), λ, μ) ≡ 0.
Using equation (8.36) and the implicit solution, for i ∈ {1, 2,...,m} it
follows that
Hi(P(β(λ, μ), λ), λ) = Hi(β(λ, μ), λ)
and
(P(β(λ, μ), λ) − β(λ, μ)) · vi = 0.
Using the independence of the first integrals and the inverse function the￾orem applied to the map
σ → (H1(σ, λ),...,Hm(σ, λ), σ · v1,...,σ · vm−n−1),
it follows that P(β(λ, μ), λ) = β(λ, μ) whenever |λ| and |μ| are smaller
than some sufficiently small positive number. In particular, for each fixed
λ, there is an m-dimensional family of periodic solutions. ✷
More can be proved: The family of periodic orbits μ → G(λ, μ), which
may be chosen such that G(0, 0) = Γ, is such that each periodic orbit472 8. Continuation of Periodic Solutions
G(λ, μ) is contained in an (n − m)-dimensional invariant manifold
M(λ) = {x ∈ Rn : H1(x, λ) = a1(λ, μ),
H2(x, λ) = a2(λ, μ),...,Hm(x, λ) = am(λ, μ)},
where for each i ∈ {1, 2,...,m}, ai : {λ : |λ| < a}×{μ : |μ| < a} → R, the
corresponding period function T (λ, μ) is such that T (0, 0) = T, and every
function in sight is as smooth as the family (8.35).
For a planar Hamiltonian system, the Hamiltonian is an independent
first integral at a periodic orbit and the multiplicity of the Floquet multi￾plier is exactly two. Thus, a periodic orbit of a planar Hamiltonian system
cannot be isolated; it must be contained in a one-parameter family of peri￾odic orbits. In the case of more than one degree of freedom (corresponding
to a four or higher even-dimensional system), it is necessary to check that
the multiplicity of the Floquet multiplier is m + 1. In fact, for Hamiltonian
systems, this multiplicity is often exceeded. The reason is simple to under￾stand. But, some additional theory of Hamiltonian systems is required to
prove a theorem that states the exact multiplicity.
Suppose that the functions H1, H2,...,Hm are independent first inte￾grals of the Hamiltonian system X1 with Hamiltonian H1. We have proved
that if X1 has a periodic orbit, then one is a Floquet multiplier with
geometric multiplicity at least m + 1. Consider the Hamiltonian systems
X2,...,Xm corresponding to the remaining integrals. Also, let φt
i denote
the flow of Xi. It might happen that, for some j, the flow of Xj commutes
with the flow of X1; that is, φt
1 ◦ φs
i = φs
i ◦ φt
1. In this case, at the point x0
on a periodic orbit of X1 with period T and by differentiation with respect
to s at x = 0, we have the identity DφT
1 (x0)Xj (x0) = Xj (x0). In other
words, Xj (x0) is an eigenvector corresponding to the Floquet multiplier
one. It turns out that if the flow of Xj (x0) commutes with the flow of each
of the vector fields X1, X2,...,Xm, then Xj (x0) is not in the span of the
eigenvectors corresponding to the mere existence of first integrals. Thus,
the multiplicity of the Floquet multiplier is at least m+ 2; and, in this case,
the results in this section do not apply.
We will need a few new ideas to see why Xj (x0) is independent of the
other eigenvectors with eigenvalue one.
For the Hamiltonian case, the dimension n must be even, say n = 2N. Let
angle brackets denote the usual inner product on RN and, for two vectors
v = (v1, v2) and w = (w1, w2) in Rn = RN ×RN , define the skew-symmetric
bilinear form ω : Rn × Rn → R by
ω(v, w) = v1, w2−v2, w1.
Also, note that ω is nondegenerate; that is, if ω(v, w) = 0 for all w, then
v = 0. A nondegenerate skew-symmetric bilinear form on a vector space
is called a symplectic form, an object that plays a fundamental role in
Hamiltonian mechanics.8.2 Autonomous Perturbations 473
In the setting of this section, note that
grad H1(v) = ω(X1, v)
for all v ∈ Rn. Suppose, in more generality, that a symplectic form is
assigned to each tangent space of an even-dimensional manifold M so that
the assignment varies smoothly over the manifold and consider a smooth
function H : M → R. The Hamiltonian vector field with Hamiltonian H is
defined to be the unique vector field XH on M such that
dH(Y )p = ω(XH, Y )p
for all vector fields Y on the manifold and all p ∈ M. In the general
theory of Hamiltonian systems, the symplectic form comes first; it is used
to define the Hamiltonian structure. In our simple case, the manifold is Rn
and the (usual) symplectic form is constant over the manifold. It produces
the Hamiltonian vector field (Hy, −Hx).
Returning to the Floquet multipliers, the goal is to show that the eigen￾vector Xj is not in the span of the eigenvectors corresponding to the
Floquet multipliers obtained by the existence of first integrals. Similar
to the construction in the proof of Proposition 8.26 where the vectors
v1, v2,...,vn−m−1 are defined, choose coordinates on the Poincar´e section
Σ (in a neighborhood of the point v ∈ Σ that lies on a periodic orbit of
X1) using the first integrals as the first m-coordinates. In other words, the
coordinate map is
x → (H1(x), H2(x),...,Hm(x), x · v1, x · v2,...,x · vn−m−1).
Let P denote the Poincar´e map. Because for each first integral Hi(P(σ)) =
Hi(σ), the first m-coordinates of P(σ) are simply σ1, σ2,...,σm. Hence,
the derivative of P has the block form
 I 0
A B 
where I is the m×m-identity and B : Rn−m−1 → Rn−m−1. The m Floquet
multipliers obtained by the existence of the first integrals correspond to
the diagonal elements of the m × m-identity. Note that an eigenvector
corresponding to one of these eigenvalues must have a nonzero element
among its first m-components. In fact, if the eigenvector has block form
(u, v), then it must satisfy the equation Au + Bv = v. If u = 0, then
Bv = v and the eigenvalue belongs to B instead.
The vector fields Xi and Xj (and the corresponding first integrals H1
and Hj ) are in involution if ω(Xi, Xj ) = 0.
Assume the following proposition is true.
Proposition 8.28. The Hamiltonian vector fields X and Y are in involu￾tion if and only if their flows commute.474 8. Continuation of Periodic Solutions
It has a useful corollary: In the setting of this section, if the Hamiltonian
vector field X1 has independent first integrals H1, H2,...,Hm and Hj is
in involution with each of these first integrals, then the Floquet multiplier
one has multiplicity at least m + 2. To prove this fact, suppose that Xj is
in involution with Xi for each i ∈ {1, 2,...,m}. Then,
grad Hj · Xi = ω(Xj , Xi)=0.
In other words, Xi does not have a component in the direction of the first
m-coordinates. This proves desired result.
It remains to prove Proposition 8.28. By employing more machinery (the
calculus of differential forms), this fact has an elegant proof (see, for exam￾ple, [2]), which holds for Hamiltonians on manifolds. We will simply give
the ingredients of a proof for the manifold Rn with the usual constant
symplectic form.
Suppose that H and K are Hamiltonians and their corresponding Hamil￾tonian vector fields are X = (Hy, −Hx) and Y = (Ky, −Kx). These vector
fields are in involution when
ω(X, Y ) = Hx, Ky−Hy, Kx = 0.
Define the Lie bracket [X, Y ] of X and Y by [X, Y ] = LXY (see Defini￾tion 7.21). A basic fact proved in Lemma 8.29 is that [X, Y ] = XY − Y X,
where, for instance, XY is the vector field obtained by taking the direc￾tional derivative of each component of Y in the direction X. Using this
result and the component notation, the x-component of [X, Y ] is
Hy, Kyx−Hx, Kyy − (Ky, Hyx−Kx, Hyy).
By differentiating the expression for ω(X, Y ) with respect to y, it follows
that the x-component vanishes. The y-component is seen to vanish simi￾larly.
The next lemma finishes the proof of Proposition 8.28.
Lemma 8.29. (1) On Rn, [X, Y ] = XY −Y X. (2) The flows of two vector
fields X and Y commute if and only if their Lie bracket vanishes.
Proof. Let φt denote the flow of the vector field X and ψt denote the flow
of Y .
By the definition of the Lie derivative
LXY (x) = d
dtDφ−t(φt(x))Y (φt(x))


t=0.
Define the vector field Z by
Z(φt(x)) = Dφ−t(φt(x))Y (φt(x)) (8.37)8.2 Autonomous Perturbations 475
and note that Dφt(x)Z(φt(x)) = Y (φt(x)). From the definition of a flow,
as usual,
d
dtDφt(x) = DX(φt(x))Dφt(x).
Using these formulas and differentiating with respect to t in equation (8.37),
we have that
Dφt(x) d
dtZ(φt(x)) = DY (φt(x))X(φt(x)) − DX(φt(x))Y (φt(x));
and, after evaluation at t = 0, the identity
LXY = DY (x)X(x) − DX(x)Y (x).
In other words, [X, Y ] = XY − Y X.
Suppose that the flows commute; that is, φt ◦ ψs = ψs ◦ φt. Equivalently,
ψs = φ−t ◦ ψs ◦ φt. By differentiating with respect to s and evaluating at
s = 0, it follows that
Dφ−t(φt(x))Y (φt(x)) = Y (x).
Hence [X, Y ] = LXY = 0.
Suppose that [X, Y ] = 0. In general, if d
dth(φt(x))|t=0 = 0 for all x, then
t → h(φt(x)) is a constant function; indeed,
0 =
d
dth(φt(φs(x)))|t=0 = d
dth(φs+t)|t=0 = d
dsh(φs).
By applying this fact in the definition of the Lie derivative and using the
hypothesis, the function t → Dφ−t(φt(x))Y (φt(x)) is constant. Hence,
Dφ−t(φt(x))Y (φt(x)) = Y (x).
Let γs := φ−t ◦ ψs ◦ φt. Its derivative with respect to s is
d
dsγs(x) = Dφ−t(ψs ◦ φt(x))Y (ψs ◦ φt(x))
= Dφ−t(φt ◦ γs(x)))Y (φt ◦ γs(x))
= Y (γs(x)).
By the uniqueness of solutions of initial value problems, γs = ψs. ✷
Exercise 8.30. Suppose that A and B are matrices. Use Lemma 8.29 to prove
that etAetB = et(A+B) if and only if AB = BA.
Exercise 8.31. Prove that if ˙x = f(x), for x ∈ Rn, has n independent first
integrals, then f = 0.476 8. Continuation of Periodic Solutions
Exercise 8.32. Show that periodic orbits of the uncoupled harmonic oscillators
model
H(q1, q2, p1, p2) = 1
2
(p2
1 + p2
2) + 1
2
(q2
1 + q2
2)
occur in four-parameter families. This suggests the existence of an independent
integral in involution with the Hamiltonian. Find this integral.
Exercise 8.33. Prove that the system
x˙ = −y + x(x2 + y2 − 1), y˙ = x + y(x2 + y2 − 1)
does not have a (nonconstant) first integral.
Exercise 8.34. (a) Prove that the system
x˙ = −y + xy, y˙ = x + y2
has a (nonconstant) first integral. (b) Prove that the system has a center at the
origin.
Exercise 8.35. Suppose that X is a vector field on Rn with flow φt and g :
Rn → R. Define f(x, t) = g(φt(x)) and prove that
ft(x, t) = ∇f(x, t)X(x).
Recall that ∇f is the gradient of f.
Exercise 8.36. [Completely Integrable Hamiltonians] A Hamiltonian system
with n degrees of freedom is called completely integrable if it has n independent
first integrals. (a) Prove that the Hamiltonian system with Hamiltonian H =
p2
1/2+1−cos q2 + (p2
2 +q2
2)/2 is completely integrable. (b) Prove that the system
q˙ = p,
p˙ = − sin q + x4
4 sin q,
x˙ = y,
y˙ = −x − x3 cos q (8.38)
has a (nonconstant) first integral. (d) Prove that system (8.38) is not completely
integrable. (cf. [119, Section 4.8]).
Exercise 8.37. [A Theorem of Lyapunov] Consider an ODE ˙x = f(x) defined
on Rn, n ≥ 2 with a rest point at the origin and a nondegenerate first integral.
(a) Show that the ODE may be written in the form ˙x = Ax + g(x) where A is
a constant n × n-matrix, g(0) = 0 and Dg(0) = 0. (b) Introduce a parameter
λ by changing coordinates to x = λy. Show that in these coordinates the ODE
for λ = 0 is ˙y = Ay + O(λ). (c) Suppose the eigenvalues of A are a pair of pure
imaginary (nonzero) numbers ±iω and all the rest are real or complex with the
property that division by iω is not an integer. Show that ˙y = Ay has a family of
periodic orbits with periodic 2π/ω. (d) With the hypothesis stated in part (c),
show that each periodic orbit in the family is nondegenerate and apply the results
of this section to conclude that the family of period orbits persists. (d) Return
to the original coordinates and prove Lyapunov’s theorem: Under the hypotheses
stated in this problem, the rest point at the origin has a family of periodic orbits
in its center manifold (compare [176]).8.3 Nonautonomous Perturbations 477
Exercise 8.38. Consider the second-order system
x¨ = −x(x2 + y2
), y¨ = −y(x2 + y2
).
(a) Show that the system has two first integrals. (b) Show that the system has a
two-parameter family of periodic solutions. (c) Are the circular periodic solutions
in part (b) nondegenerate? (d) Prove that noncircular periodic orbits exist. These
are difficult to analyze with pencil-and-paper for the simple reason that their
periods and time dependence is not easy to identity. Use a numerical approach
to gather evidence for degeneracy or nondegeneracy of these orbits.
8.3 Nonautonomous Perturbations
A classic problem from applied mathematics is the search for periodic solu￾tions of nonautonomous periodically perturbed systems of the form
u˙ = f(u) + g(u, t, ), u ∈ R2. (8.39)
More precisely, suppose that ω > 0, the unperturbed system ( = 0) has
a periodic solution Γ whose period is T = 2π/ω, and the function t →
g(u, t, ) for each fixed u is periodic with period
η := η() = n
m
2π
ω
+ k + O(
2), (8.40)
where n and m are relatively prime positive integers and k ∈ R is the
“detuning parameter.” In particular,
mη(0) = n
2π
ω (8.41)
and the periodic solution Γ is said to be in (m : n)-resonance with the
periodic perturbation function g at  = 0. Equation (8.41) is called a reso￾nance relation. If, as before, we let t → u(t, ζ, ) denote the solution of the
differential equation (8.39) with initial condition u(0,ζ,) = ζ in R2, then
we have that t → u(t, ζ, 0) defines a 2π/ω-periodic function for each ζ ∈ Γ.
The nonautonomous differential equation (8.39) is equivalent to the first￾order system
u˙ = f(u) + g(u, τ, ),
τ˙ = 1 (8.42)
in the extended phase plane. Because g is a periodic function of time, it is
customary to view τ as an angular variable modulo η(). This leads to the
very useful geometric interpretation of the system (8.42) as a differential
system on the phase cylinder R2 × T where
T := {e2πiτ/η() : τ ∈ R}.478 8. Continuation of Periodic Solutions
u(η( ), )
ζ
τ = 0 τ = η( )
Figure 8.2: The left panel depicts an orbit on an invariant cylinder start￾ing at ζ and returning to the Poincar´e section at τ = 0 for the Poincar´e
map (8.43). The right panel is a schematic depiction of the same orbit on
the torus formed by identifying the Poincar´e section at τ = 0 with the plane
at τ = η(). If the orbit were to close on the mth return to the section, it
would be an (m : 1) subharmonic.
There is an annular region A ⊆ R2 containing Γ and some 0 > 0 such
that Σ = A× {1} ⊆ R2 ×T is a Poincar´e section for the system (8.42) with
associated (parametrized) Poincar´e map P : Σ×(−0, 0) → R2 defined by
(ζ, 1, ) → u(η(),ζ,).
Because the set A × {1} is naturally identified with A, the Poincar´e map
can and will be viewed as the map P : A × (−0, 0) → R2 given by
(ζ,) → u(η(),ζ,). (8.43)
A fruitful goal is to look for (m : n)-subharmonic solutions of the per￾turbed system (8.39), that is, periodic solutions of the differential equa￾tion (8.39) with period mη(). They correspond to periodic points of period
m for the Poincar´e map. Actually, there is a finer classification of such solu￾tions that is often made as follows: A periodic solution is called a harmonic
if it closes at the first pass through the Poincar´e section after rotating once
in the T direction. Harmonics are associated with (1 : 1) resonance. A peri￾odic solution is called a subharmonic of order m if it closes at the mth pass,
m > 1, through the Poincar´e section after rotating once in the T direction.
The name “subharmonic” is used because the frequency 2π/(mη()) is a
submultiple of the frequency 2π/η() of the perturbation. Subharmonics
are associated with (m : 1) resonance with m > 1. A periodic solution is
called an (m, n)-ultrasubharmonic if it closes at the mth pass through the
Poincar´e section after rotating n times, n > 1, in the T direction. Ultrasub￾harmonics are associated with (m : n) resonance with n > 1. The geometry
of subharmonic orbits in the extended phase plane is depicted in Figure 8.2.
The key point derived from our geometric interpretation of the perturba￾tion problem is the following: A periodic point of period m for the Poincar´e8.3 Nonautonomous Perturbations 479
map is a periodic solution with period mη() for the system (8.39). To see
this, let ζ be a periodic point of period m so that
u(mη(),ζ,) = ζ.
Consider the solution t → u(t, ζ, ) of the system (8.39) and the function
given by
v(t) := u(t + mη(),ζ,),
and note that
v˙ = f(v) + g(v, t + mη(), ).
Using the periodicity of g, this last equation simplifies to yield
v˙ = f(v) + g(v, t, ),
and therefore t → v(t) is a solution of the differential equation (8.39). As
v(0) = ζ and u(0,ζ,) = ζ, the solutions t → u(t, ζ, ) and t → v(t) must be
the same; that is, u(t+mη(),ζ,) = u(t, ζ, ) and the function t → u(t, ζ, )
is mη()-periodic.
As before, let us define the (parametrized) displacement function δ :
A × (−0, 0) → R2 by
δ(ζ,) = u(mη(),ζ,) − ζ. (8.44)
Here there is no need for a local coordinate representation via a coordinate
chart: Points in the domain A × (0, 0) ⊂ R2 × R are already expressed in
local coordinates.
Clearly, if ζ ∈ Γ, where Γ is a resonant periodic solution of the differential
equation (8.11), then δ(ζ, 0) = 0; in effect,
δ(ζ, 0) = u(mη(0),ζ, 0) − ζ = u(m
n
m
2π
ω
,ζ, 0) − ζ = 0.
To see if Γ persists, we would like to apply the implicit function theorem
to the function δ at the point (ζ, 0) where δ(ζ, 0) = 0. Thus, we would like
to show that the linear map δζ (ζ, 0) : R2 → R2 is invertible. But, for a
point ζ that lies on a resonant periodic solution Γ, this map always has a
non-trivial kernel. In fact, we have that δζ (ζ, 0)f(ζ) ≡ 0 for ζ ∈ Γ. This
result is geometrically obvious. But to construct an analytic proof, let us
use the definition of the directional derivative and the group property of
the unperturbed flow to obtain the identity
δζ (ζ, 0)f(ζ) = d
dt δ(u(t, ζ, 0), 0)

t=0
= d
dt(u(2πn/ω, u(t, ζ, 0), 0) − u(t, ζ, 0))

t=0
= d
dt(u(2πn/ω + t, ζ, 0) − u(t, ζ, 0))

t=0
= f(u(2πn/ω, ζ, 0)) − f(ζ)=0.
(8.45)480 8. Continuation of Periodic Solutions
We have just proved that the kernel of the linear transformation δζ (ζ, 0)
contains the subspace generated by f(ζ). Here and hereafter we will let
[v] denote the subspace spanned by the enclosed vector. In particular, we
have [f(ζ)] ⊆ Kernel δζ (ζ, 0). The analysis to follow later in this chapter
falls naturally into two cases: [f(ζ)] = Kernel δζ (ζ, 0) and Kernel δζ (ζ, 0) =
R2. After a short section devoted to the continuation of periodic orbits
from unperturbed rest points where the kernel of the derivative of the
displacement can be trivial, we will develop some of the theory required to
determine the continuable periodic orbits in each of these two cases.
Exercise 8.39. [Multidimensional Oscillators] Suppose that the system ˙u =
f(u), for the vector case u ∈ Rn, has a T-periodic orbit Γ given by the solution
t → γ(t). (a) Show that the number one is a Floquet multiplier of the (periodic)
variational equation ˙w = Df(γ(t))w. (b) Prove that if the Floquet multiplier
one has algebraic multiplicity one and if g : Rn × R × R → Rn is a smooth
function given by (u, t, ) → g(u, t, ) such that the corresponding map given
by t → g(u, t, ) is T-periodic for each u and , then Γ persists in the family
u˙ = f(u) + g(u, t, ). (c) Is the same result true if the geometric multiplicity is
one?
8.3.1 Rest Points
Let us suppose that the unperturbed system
u˙ = f(u),
derived from the system (8.39) by setting  = 0, has a rest point u = ζ.
This point is a fixed point of the unperturbed Poincar´e map and a zero
of the unperturbed displacement function. In particular, the rest point
corresponds to a periodic solution of the artificially autonomous system
u˙ = f(u), τ˙ = 1, (8.46)
where τ is considered as an angular variable modulo η(0). To determine if
the corresponding periodic solution continues, we have the following theo￾rem.
Theorem 8.40. If ζ is a rest point for the unperturbed system u˙ = f(u)
derived from the system (8.39), and the Jacobian matrix Df(ζ) has no
eigenvalue of the form 2πN i/η where N is an integer, then the periodic
orbit with period η(0) for system (8.46) corresponding to ζ persists as an
η()-periodic solution of equation (8.39).
Proof. The partial derivative
δζ (ζ, 0) = uζ (η(0),ζ, 0) − I8.3 Nonautonomous Perturbations 481
is easily computed by solving the variation initial value problem
W˙ = Df(ζ)W, W(0) = I
to obtain
δζ (ζ, 0) = eηDf(ζ) − I.
The matrix δζ (ζ, 0) is invertible if and only if the number one is not an
eigenvalue of eηDf(ζ)
. Thus, the desired result follows from Theorem 5.7
and the implicit function theorem. ✷
Exercise 8.41. Describe the bifurcations of rest points that may occur in case
2πN i/η is an eigenvalue of Df(ζ) for some integer N.
8.3.2 Isochronous Period Annulus
If the coordinate neighborhood A ⊂ R2 containing the unperturbed peri￾odic orbit Γ is a period annulus A, it is possible that every periodic solution
in A has the same period, that is, the period annulus is isochronous. In
this case, if a resonance relation holds for one periodic solution in A, then
it holds for all of the periodic solutions in A. We will determine the con￾tinuable periodic solutions for an unperturbed system with an isochronous
period annulus.
Let us note that a period annulus for a linear system is necessarily
isochronous. Although there are nonlinear systems with isochronous period
annuli (just transform a linear system with a period annulus by a nonlinear
change of coordinates), they can be difficult to recognize.
Exercise 8.42. Prove that the following systems have isochronous period annuli.
1. ¨x + 1 − √1+2x = 0.
2. (Loud’s system) ˙x = −y + Bxy, ˙y = x + Dx2 + F y2 in case (D/B, F/B)
is one of the following:
(0, 1), (−1
2
, 2), (0, 1
4
), (−1
2
, 1
2
)
(see [49] and [167]).
Loud’s theorem states that every quadratic system with an isochronous period
annulus can be transformed by a linear change of coordinates to one of the four
systems mentioned above. An interesting unsolved pure mathematics problem is
to determine the number and positions of critical points for the period functions
of the period annuli of Loud’s system as the parameters B, D, and F are varied.
For example, there are some period functions with two critical points. It is not
known if this is the maximum number (see [49]).482 8. Continuation of Periodic Solutions
For the rest of this subsection, let us assume that the unperturbed sys￾tem (8.11) has an isochronous period annulus A where every periodic orbit
has period 2π/ω. In this case, δ(ζ, 0) ≡ 0 and δζ (ζ, 0) ≡ 0 for ζ ∈ A.
Because the perturbation series for the displacement function (see dis￾play (8.44)) has the form
δ(ζ,) = (δ(ζ, 0) + O()),
we have the following proposition: A simple zero ζ of the function ζ →
δ(ζ, 0) is an (ultra)subharmonic continuation point. In other words, there
is a number 0 > 0 and a continuous function β : (−0, 0) → R2 given
by  → β() such that β(0) = ζ and δ(β(), ) ≡ 0. Of course, β() is the
initial value of a subharmonic solution of the differential equation (8.39).
This result is the now familiar reduction step of our analysis.
To identify the function ζ → δ(ζ, 0), we simply compute this partial
derivative from the definition of the displacement (8.44) to obtain
δ(ζ, 0) = mη
(0)f(ζ) + u(mη(0), ξ, 0)
= mkf(ζ) + u(2πn/ω, ζ, 0).
As before, t → u(t, ζ, 0) is the solution of a variational initial value prob￾lem, namely,
W˙ = Df(ϕt(ζ))W + g(ϕt(ζ), t, 0), W(0) = 0
where ϕt is the flow of the unperturbed system. The solution of the ini￾tial value problem is obtained just as in the derivation of equation (8.32).
The only difference is the “nonautonomous” nature of g, but this does
not change any of the formal calculations. In fact, with the notation as in
equation (8.32), we obtain
u(2πn/ω, ζ, 0) = (N (ζ) + a(2πn/ω, ζ)M(ζ))f(ζ)
+ b(2πn/ω, ζ)M(ζ)f ⊥(ζ). (8.47)
By the geometric interpretation of the functions a and b given following
equation (8.18), these functions are readily reinterpreted in the present
context. In fact, since every orbit of our isochronous period annulus is not
hyperbolic, we must have b(2π/ω, ζ) = 1, and, since the period function is
constant, we also have a(2π/ω, ζ) = 0. Thus, we obtain the identity
δ(ζ, 0) = (mk + N (ζ))f(ζ) + M(ζ)f ⊥(ζ). (8.48)
Exercise 8.43. Show that b(n2π/ω, ζ) = bn(2π/ω, ζ) and
a(2πn/ω, ζ) = a(2π/ω, ζ)
n
−1
j=0
b
j
(2π/ω, ζ).8.3 Nonautonomous Perturbations 483
Theorem 8.44. Suppose the differential equation (8.39) is such that the
unperturbed system has an isochronous period annulus A with period 2π/ω
and the perturbation g(u, t, ) has period ν()=(n/m)2π/ω + k + O(2)
where n and m are relatively prime positive integers. If the bifurcation
function B : A → R2 given by ζ → (mk + N (ζ),M(ζ)) has a simple zero
ζ, then ζ is a continuation point of (m : n) (ultra)subharmonics for the
system (8.39).
Proof. The theorem follows from equation (8.48). Indeed, if
F(ζ) := f1(ζ) −f2(ζ)
f2(ζ) f1(ζ)

and B(ζ) := 
mk + N (ζ)
M(ζ)

,
then
B(ζ) = F(ζ) · B(ζ),
and the simple zeros of B coincide with the simple zeros of B. ✷
Theorem 8.44, specialized to the case where the unperturbed system is
linear, is slightly more general than Theorem 8.1. For example, suppose
that f(u) = Au where
A =
 0 −ω
ω 0

, ω> 0.
Since div f ≡ 0 and |f| is constant on orbits, b(t, ζ) ≡ 1. Also, let us note
that
2κ(t, ζ)|f(ϕt(ζ))| − curl f(ϕt(ζ)) = 2 1
|ζ|
ω|ζ| − 2ω = 0,
and therefore a(t, ζ) ≡ 0. (This is a good internal check that the formula
for a is correct!) Thus, in this special case,
N (ζ) =  n2π/ω
0
1
|f(ϕt(ζ))|
2 f(ϕt(ζ)), g(ϕt(ζ), t, 0) dt,
M(ζ) = 1
|f(ζ)|
2
 n2π/ω
0
f(ϕt(ζ)) ∧ g(ϕt(ζ), t, 0) dt.
More explicitly, we have that
N (ζ) = 1
ω|ζ|
2
 n2π/ω
0
xg2(x, y, t, 0) − yg1(x, y, t, 0) dt,
M(ζ) = − 1
ω|ζ|
2
 n2π/ω
0
xg1(x, y, t, 0) + yg2(x, y, t, 0) dt (8.49)
where
x := x(t, ζ) = ζ1 cos ωt − ζ2 sin ωt, y := y(t, ζ) = ζ1 sin ωt + ζ2 cos ωt.484 8. Continuation of Periodic Solutions
Let us consider the stability of the perturbed (ultra)subharmonics. Note
that the “perturbation series” for the Poincar´e map is given by
P(ζ,) = ζ + P(ζ, 0) + O(
2),
and P(ζ, 0) = δ(ζ, 0). Thus, the formula for the partial derivative of the
Poincar´e map with respect to  is given by equation (8.48) and
P(ζ,) = ζ1
ζ2

+ 

kmω −ζ2
ζ1

+
 n2π/ω
0 g1 cos ωt + g2 sin wt dt
 n2π/ω
0 g2 cos ωt − g1 sin wt dt
+ O(
2) (8.50)
where g1 and g2 are evaluated at (x, y, t, 0).
It should be clear that the stability of the perturbed (ultra)subharmonics
is determined by the eigenvalues of the matrix Pζ (ζ,), called the linearized
Poincar´e map evaluated at the fixed point of ζ → P(ζ,) correspond￾ing to the subharmonic. The subharmonic is stable if both eigenvalues lie
inside the unit circle in the complex plane. Of course, if the linearized
Poincar´e map is hyperbolic, then the local behavior near the periodic orbit
is determined—stability is just a special case of this more general fact.
It is not too difficult to show that if  > 0 is sufficiently small, then
the matrix Pζ (ζ,) evaluated at the perturbed fixed point has both of its
eigenvalues inside the unit circle in the complex plane provided that each
eigenvalue of the matrix Pζ(ζ, 0) has negative real part. For  < 0, each
eigenvalue of the matrix Pζ(ζ, 0) must have positive real part. Equivalently,
it suffices to have
det Pζ(ζ, 0) > 0,  tr Pζ(ζ, 0) < 0.
The proof of this fact contains a pleasant surprise.
The perturbation series for the Poincar´e map evaluated along the curve
 → (β(), ) has the form
Pζ (β(), ) = I + A + 
2B + O(
3)
where A = Pζ(β(0), 0). In particular, we have used the equality Pζζ (ζ, 0) =
0. The characteristic polynomial of the first-order approximation of this
matrix, namely, I + A, has coefficients that contain terms of second order
in . Thus, it appears that second-order terms in the perturbation series
are required for computing the eigenvalues to first order. Fortunately, there
is an unexpected cancellation, and the eigenvalues, for  > 0, are given by
1 + 
1
2

tr A ±


tr2 A − 4 det A
	
+ O(
2). (8.51)8.3 Nonautonomous Perturbations 485
Using formula (8.51), it is easy to show that if the eigenvalues of A have
nonzero real parts, then the first-order terms of the expansion determine
the stability. If A has an eigenvalue with zero real part, then higher-order
terms in the perturbation expansion must be considered (see [191]).
General formulas for the eigenvalues of Pζ(ζ, 0) can be obtained in terms
of certain partial derivatives of M and N . But, such formulas are usually
not useful. A better approach is to use the special properties of the system
under investigation.
Exercise 8.45. Prove the statements following equation (8.51) concerning the
eigenvalues of the matrix Pζ (ζ,).
8.3.3 The Forced van der Pol Oscillator
In this subsection we will outline, by formulating a series of exercises, some
applications of the continuation theory (developed so far in this chapter)
to the classic case of the van der Pol oscillator. Also, we mention briefly
some of the additional structures that can be studied using our first-order
methods.
Exercise 8.46. Find the (ultra)subharmonics for the periodically forced van
der Pol oscillator
x¨ + (x2 − 1) ˙x + x = a sin Ωt.
In particular, for fixed a = 0, find the regions in (Ω, ) space near the line  = 0
where (ultra)subharmonics exist.
The regions mentioned in Exercise 8.46 are called entrainment domains
or, in some of the electrical engineering literature, they are called synchro￾nization domains. We cannot determine the entire extent of the entrain￾ment domains because our first-order theory is only valid for sufficiently
small ||. Higher-order methods can be used to obtain more information
(see, for example, [181] and [125], and for some classic numerical experi￾ments [133]).
To use the formulas for M and N in display (8.49), let us consider the
first-order system in the phase plane given by
x˙ = −y, y˙ = x + (−(x2 − 1)y − a sin Ωt).
Also, let us consider curves in the (Ω, ) parameter space of the form  →
(Ω(), ) where
Ω() = m
n − k
m
n
	2
 + O(
2),
η()=2π n
m
+ k + O(
2).486 8. Continuation of Periodic Solutions
To complete Exercise 8.46, start by looking for harmonics; that is, look
for periodic solutions of the perturbed system with periods close to 2π for
Ω near Ω = 1. Set m = n = 1. To help debug the computations for this
example, first try the case k = 0 where k is the detuning, and show that
there is a harmonic at the point (ζ1, ζ2) provided that ζ2 = 0 and ζ1 is a
root of the equation ζ3
1 −4ζ1 + 4a = 0. This corresponds to perturbation in
the vertical direction in the parameter space. Show that the harmonic will
be stable if |ζ1| > 2 and that there is a unique (stable) harmonic in case
a = 1.
There is a very interesting difference between the (1 : 1) resonance and
the (m : n) resonance with m/n = 1. To glimpse into this structure, con￾sider the (m : n) resonance where m/n = 1 and use equation (8.50) to
compute the following first-order approximation of the associated Poincar´e
map:
ζ1 → ζ1 + 

− kmζ2 + nπζ1 − nπ
4 ζ1(ζ2
1 + ζ2
2 )
	
,
ζ2 → ζ2 + 

kmζ1 + nπζ2 − nπ
4 ζ2(ζ2
1 + ζ2
2 )
	
. (8.52)
This map preserves the origin. Thus, it is natural to study the map in polar
coordinates where it is represented to first order by
r → r + nπr
1 − r2
4

,
θ → θ + mk. (8.53)
Here the first-order formula in the rectangular coordinates goes over to
a formula in polar coordinates that contains higher-order terms in  that
have been deleted. Can we safely ignore these higher-order terms?
For the (1 : 1) resonance with k included as a parameter, a similar first￾order computation yields the map
ζ1 → ζ1 + 

− kζ2 + πζ1 − aπ − π
4
ζ1(ζ2
1 + ζ2
2 )
	
,
ζ2 → ζ2 + 

kζ1 + πζ2 − π
4
ζ2(ζ2
1 + ζ2
2 )
	
. (8.54)
Note that if this map preserves the origin, then a = 0. Thus, polar coor￾dinates are not the natural coordinates for studying this map. Instead, a
useful representation of the map is obtained by changing to the complex
coordinate z = ζ1 + iζ2 where the map is represented in the form
z → z + 

(π + ki)z − 1
4
πz2z¯ − aπ	
. (8.55)
We will that show the dynamics of the map defined in display (8.52) are
quite different from the dynamics of the map in display (8.54).8.3 Nonautonomous Perturbations 487
For the map (8.53), the circle r = 2 is an invariant set and every point
in the plane except the origin is attracted to this circle under iteration. On
the circle, the map gives a rational or an irrational rotation depending on
whether or not k is rational. In other words, an analysis of the dynamics
at this approximation suggests that there is an invariant torus in the phase
space of the differential equation and solutions of the differential equation
that do not start on the torus are attracted at an exponential rate to
this torus in positive time. Roughly speaking, such an invariant torus is
called normally hyperbolic; for the precise definition of normal hyperbolicity
see [99] and [139].
Solutions of the differential equation on the invariant torus may wind
around the torus, as in the case of irrational rotation, or they may be
attracted to a subharmonic on the torus as in the case of rational rotation.
There are general theorems that can be used to show that a normally
hyperbolic torus will persist with the addition of a small perturbation (see,
for example, [99], [122], and [139], and also [45] and [54]). Thus, we see that
there is a second possible type of entrainment. It is possible that solutions
are entrained to the torus when there are no periodic solutions on the torus.
In this case, corresponding to an irrational rotation, every solution on the
torus is dense; that is, every solution on the torus has the entire torus as
its omega limit set.
Exercise 8.47. View the circle as the set {eiθ : θ ∈ R} and define the (linear)
rotation on the circle through angle α as the map eiθ → ei(θ+α) for some fixed
k ∈ R. Prove the following classic result of Jacobi: If α is a rational multiple of
π, then every point on the circle is periodic under the rotation map. If α is an
irrational multiple of π, then the orbit of each point on the circle is dense in the
circle. In the irrational case, the solutions are called quasi-periodic.
For the forced van der Pol oscillator, we cannot determine the quasi￾periodicity of the flow by looking at the first-order approximation of the
Poincar´e map—the flow on the torus is nonlinear. There are actually three
possibilities. The nonlinear flow can have all its orbits dense, all its orbits
periodic, or it can have isolated periodic solutions. We have to be careful
here because the nonlinear Poincar´e map on the invariant torus is not, in
general, a rotation as defined above. Rather, it is likely to be conjugate to
a rotation by a nonlinear change of coordinates.
The Poincar´e map will have a stable subharmonic, or at least an isolated
subharmonic, on the invariant torus provided that the bifurcation function
has simple zeros on this torus. We will have more to say about this topic
below.
For the case m/n = 1, an examination of the map (8.53) shows that
a necessary condition for the existence of subharmonics on the invariant
torus near r = 1 is that k = 0. In the (m : n) entrainment domain in the488 8. Continuation of Periodic Solutions
(Ω, ) (frequency-amplitude) parameter space, the curves corresponding to
the subharmonics would have to be expressible as series
Ω = mω
n −
m
n
2 k
2π

ω2 +∞
j=2
Ωj 
j
(see equation (8.40)). But because k = 0, it follows that they are all of
the form Ω = mω/n + O(2). Thus, all such curves have the same tangent
line at  = 0, namely, the line given by Ω = mω/n. The portion of the
entrainment domain near  = 0 that is filled by such curves is called an
Arnold tongue.
For the map (8.54), there are fixed points corresponding to harmonics
but not necessarily an invariant torus. In case k = 0, there is a fixed point
only if ζ2 = 0 and
ζ3
1 − 4ζ1 + 4a = 0.
In case k = 0, the computations are more complicated. There are many
different ways to proceed. One effective method is “Gr¨obner basis reduc￾tion” (see [79]). Without going into the definition of a Gr¨obner basis for a
polynomial ideal, the reduction method is an algorithm that takes as input
a set of polynomials (with rational coefficients) and produces a new set of
polynomials with the same zero set. The reduced set is in a good normal
form for further study of its zero set. The output depends on the ordering
of the variables. In particular, the Gr¨obner basis is not unique.
For example, by using the MAPLE V command gbasis with the lexico￾graphic ordering of the variables ζ1 and ζ2, the equations
−kζ2 + πζ1 − aπ − π
4
ζ1

ζ2
1 + ζ2
2
	
= 0,
kζ1 + πζ2 − π
4
ζ2

ζ2
1 + ζ2
2
	
= 0,
can be reduced to
4k2ζ1 + 4kπζ2 + aπ2ζ2
2 = 0,
16k3aπ + 
16k4 + 16k2π2	
ζ2 + 8akπ3ζ2
2 + a2π4ζ3
2 = 0. (8.56)
By an inspection of equations (8.56), it is clear that there are either
one, two, or three fixed points in the Poincar´e section. If there is exactly
one solution, then (for sufficiently small  > 0), either it corresponds to
a stable harmonic that attracts the entire phase space, and, as a result,
there is no invariant torus, or, it corresponds to an unstable harmonic and
there is an invariant torus. The first-order approximation of the Poincar´e
map restricted to the invariant circle corresponding to this invariant torus
may be conjugate to either a rational or an irrational rotation. In case it is
rational, each point on the invariant circle is periodic. On the other hand,8.3 Nonautonomous Perturbations 489
if it is irrational, then each point has a dense orbit. Are these properties
present in the perturbed Poincar´e map?
If there are three harmonics, several different phase portraits are possible,
but generally the Poincar´e map has a sink, a source, and a saddle. The
“most likely” possibility in this case is to have the unstable separatrices
of the saddle attracted to the sink. In this case, the separatrices together
with the saddle and the sink form an invariant “circle” that corresponds
to an invariant torus for the flow of the differential equation. We may
ask if this set is a manifold. The answer is not obvious. For example, if
the linearization of the Poincar´e map at the sink happens to have complex
eigenvalues, then the separatrices will “roll up” at the sink and the invariant
“circle” will not be smooth. In our case, however, if  is small, then the
linearization of the Poincar´e map is near the identity, and therefore this
roll-up phenomenon does not occur. Does this mean the invariant circle is
smooth?
The case where there is an “invariant torus”—consisting of a saddle, its
unstable manifold, and a sink—is particularly interesting from the point
of view of applications. For example, a trajectory starting near the stable
manifold of the saddle will be “entrained” by the harmonic corresponding
to the saddle on perhaps a very long time scale. But, unless the orbit
stays on the stable manifold, a very unlikely possibility, it will eventually
leave the vicinity of the saddle along the unstable manifold. Ultimately, the
orbit will be entrained by the sink. But, because of the possibility of a long
sojourn time near the saddle, it is often not clear in practice, for example, in
a numerical experiment, when a trajectory has become entrained (phase
locked) to the input frequency with a definite phase. This phenomenon
might be the cause of some difficulties if we wish to control the response of
the oscillator.
Which regions in the (k, a) parameter space correspond to the existence
of three harmonics? Answer: the region of the parameter space where the
cubic polynomial (8.56) has three distinct real roots. To find this region,
let us first compute the discriminant locus of the polynomial, that is, the
set of points in the parameter space where the cubic polynomial has a
double root (see [32]). Of course, the discriminant locus is the zero set of
the discriminant of the polynomial. Equivalently, the discriminant locus is
given by the set of points in the parameter space where the polynomial and
its first derivative have a simultaneous solution. This set is also the zero
set of the resultant of the polynomial and its first derivative. In our case,
a computation shows that the discriminant locus of the cubic polynomial
in display (8.56) is the zero set of the polynomial
Δ(k, a) := 27π6a4−16π6a2−144π4a2k2+64π4k2+128π2k4+64k6. (8.57)
The discriminant locus is also the boundary of the region corresponding
to the existence of three real roots. This region is the bounded region
depicted in Figure 8.3.490 8. Continuation of Periodic Solutions
-1.5 -1 -0.5 0.5 1 1.5
-1
-0.5
0.5
1
k
a
Figure 8.3: Discriminant locus for the polynomial (8.56). The bounded
region corresponds to the existence of three harmonics for the periodically
forced van der Pol equation.
Exercise 8.48. The discriminant locus corresponds to an invariant curve for
the Hamiltonian system
k˙ = −∂Δ
∂a (k, a), a˙ = ∂Δ
∂k (k, a) (8.58)
with Hamiltonian Δ. Show that the invariant set consists of six trajectories and
five rest points (zeros of the vector field). The four rest points not at the origin
are all degenerate—the Jacobian matrix at each rest point has zero eigenvalues.
Study the local behavior of the discriminant locus at each of its singular points
to explain the corners in Figure 8.3. For example, show that
k0 = −
√3
3 π, a0 = −4
9
√
6
is a rest point and that the discriminant locus near this rest point is given by
a − a0 =
√2
π (k − k0) ± 2
3
31/4
π3/2 (k − k0)
3/2 + O((k − k0)
2
).
In particular, the tangents to the discriminant locus at the singular point coincide;
that is, the discriminant locus has a cusp at the singular point. To show this you
can just note that the discriminant locus is a quadratic in a2 and solve. A more
complicated but perhaps more instructive way to obtain the same result is to use
the theory of Newton polygons and Puiseux series (see, for example, [32]).
For each parameter value (k, a) in the unbounded region of the plane
bounded by the discriminant locus, the corresponding differential equation8.3 Nonautonomous Perturbations 491
has one subharmonic solution. We can determine the stability of this sub￾harmonic using the formulas given in the preceding section following for￾mula (8.50). In particular, there are curves in the parameter space starting
near each cusp of the discriminant locus that separates the regions corre￾sponding to stable and unstable harmonics. These curves are exactly the
curves in the (k, a) parameter space given by the parameter values where
the following conditions (see formula (8.51)) are met at some fixed point
of the first-order linearized Poincar´e map: The trace of the linearization of
the O() term of the first-order Poincar´e map vanishes and its determinant
is positive. We call these the PAH curves in honor of Poincar´e, Andronov,
and Hopf.
To determine the PAH curve, note first that the trace of the O() term
of the linearization (8.54) is given by π(2 − zz¯) and use fact if there is a
fixed point, then the O() term of the map (8.55) vanishes. Thus, (k, a) lies
on the PAH curve when the determinant is positive and the following two
equations have a simultaneous solution:
2 − zz¯ = 0,
(π + ki)z − 1
4
πz2z¯ − aπ = 0.
All three conditions are satisfied provided that (k, a) lies on one of the
curves given by
a2π2 = π2
2 + 2k2, |k| ≤ π
2
. (8.59)
The portion of the PAH curve in the region where k > 0 and a > 0 is
depicted in Figure 8.4. Note that the PAH curve does not pass through the
cusp on the discriminant locus; rather, it “stops” on the discriminant locus
at the point (k, a)=(π/2, 1). This suggests there are more bifurcations for
parameter values in the region corresponding to three harmonics—inside
the bounded region cut off by the discriminant locus. This is indeed the
case. A more detailed bifurcation diagram and references to the literature
on these bifurcations can be found in [119, p. 71] where the first-order
approximation is obtained by the method of averaging, a topic that will be
covered in Chapter 10.
Exercise 8.49. Compare and contrast our computation of the first-order approx￾imation of the Poincar´e map with the first-order approximation obtained by the
method of averaging, see Chapter 10 and [119], [261], or [227].
Exercise 8.50. Find the points where the PAH curve intersects the discrimi￾nant locus. Show that the determinant of the linearized Poincar´e map vanishes
at a fixed point of the first-order Poincar´e map exactly when the parameter value
defining the map is on the discriminant locus. Study the bifurcations at the point
(k, a)=(π/2, 1) on the hyperbola (8.59) to account for the end point of the PAH
curve. The set of (k, a) points where the determinant of the O() term of the492 8. Continuation of Periodic Solutions
linearized Poincar´e map vanishes at the fixed point is determined by finding the
parameters (k, a) where the equations
π2 − π2
zz¯ +
3
16π2
(zz¯)
2 + k2 = 0,
(π + ki)z − 1
4
πz2
z¯ − aπ = 0
have a simultaneous solution for z.
The eigenvalues of the linearized Poincar´e map are generally complex
conjugates λ(k, a) and λ¯(k, a); they lie on the unit circle in the complex
plane when (k, a) is on one of the PAH curves. In other words, the sta￾bility of a corresponding harmonic is not determined by the first-order
terms of the perturbation series at this point. If we consider a second curve
μ → (k(μ), a(μ)) that crosses one of the boundary curves at the parameter
value μ = 0, then we will see that the fixed point of the Poincar´e map
changes its stability as we cross from μ < 0 to μ > 0. For example, the
real parts of the eigenvalues of the linearized map may change from neg￾ative to positive values—the stability changes in this case from stable to
unstable. The bifurcation corresponding to this loss of stability is called
Hopf bifurcation. The theory for this bifurcation is quite subtle; it will be
discussed in detail in Chapter 11. But roughly speaking if the parameter
value μ > 0 is sufficiently small, then the Poincar´e map has an invariant
circle with “radius” approximately √μ and “center” approximately at the
unstable harmonic.
Exercise 8.51. Show (numerically) that the Hopf bifurcation occurs as described
in this section for the forced van der Pol oscillator. See Figure 8.4. For example,
fix k = 2 and  = .001, then compute phase portraits of the Poincar´e map for
several choices of a in the range a = 1.2 to a = 1.1.
Exercise 8.52. Determine the “phase portrait” of the Poincar´e map for the
forced van der Pol oscillator near (1 : 1) resonance for the case when the param￾eters (k, a) lie on the discriminant locus. In particular, determine the phase por￾trait in case (k, a) is a singular point of the discriminant locus. How does the
phase portrait change on a curve of parameter values that passes through the
discriminant locus?
Exercise 8.53. Code a numerical simulation of the Poincar´e map for the forced
van der Pol oscillator and verify that the first-order analysis of this section pre￾dicts the dynamical behavior of the iterated Poincar´e map.
Exercise 8.54. Consider the forced van der Pol oscillator near (1 : 1) resonance
for fixed input amplitude a, for example, a = 3
4 . Determine the value of the
detuning for which the amplitude of the response is maximum.8.3 Nonautonomous Perturbations 493
0.5 1 1.5 2 2.5 3
0.25
0.5
0.75
1
1.25
1.5
PAH Curve
Discriminant Locus
PAH Curve
k
a
Discriminant Locus
Figure 8.4: The left panel depicts the PAH curve in the region in the (k, a)
parameter space with k > 0 and a > 0 together with the discriminant locus.
The right panel is a blowup of the figure near the cusp on the discriminant
locus.
8.3.4 Regular Period Annulus and Lyapunov–Schmidt
Reduction
In this section we will discuss a continuation theory for periodic solutions
of the periodically perturbed oscillator
u˙ = f(u) + g(u, t, ), u ∈ R2, (8.60)
in case the unperturbed system has a resonant periodic orbit that is con￾tained in a nonisochronous period annulus.
Consider an unperturbed resonant periodic orbit Γ for system (8.60)
that is contained in a period annulus A, and recall that if A is isochronous,
then all of the orbits in A are resonant and the unperturbed displacement
function ζ → δ(ζ, 0), defined as in display (8.44) by
δ(ζ,) = u(mη(),ζ,) − ζ, (8.61)
vanishes identically. If A is not isochronous, then we expect that although
the unperturbed displacement function does vanish on Γ, it does not vanish
on nearby periodic orbits.
What happens when we attempt to apply the implicit function theorem?
For each z ∈ Γ we have δ(z, 0) = 0. If, in addition, the linear transformation
δζ (z, 0) : R2 → R2 were invertible, then z would be a continuation point.
But, as demonstrated by the result in display (8.45), this linear transfor￾mation is not invertible. In particular, all vectors tangent to Γ are in its
kernel.
In this section, we will consider the case where the kernel of the deriva￾tive of the displacement function at each point z ∈ Γ is exactly the one￾dimensional tangent space to Γ at z. In other words, we will assume that
Kernel δζ (ζ, 0) = [f(ζ)]. If this condition is met, then Γ, as well as the
corresponding invariant torus for the system494 8. Continuation of Periodic Solutions
u˙ = f(u), τ˙ = 1,
is called normally nondegenerate.
Before proceeding to the continuation analysis, let us consider a geomet￾rical interpretation of our assumption about the kernel of the derivative
of the displacement function. For this, we do not need to assume that the
periodic orbit Γ is contained in a period annulus. Instead, we may assume
more generally that there is a region R ⊆ R2 and an 0 > 0 such that the
displacement function δ : R×(−0, 0) → R2 is defined. Also, let us assume
that there is a curve Σ ⊆ R transverse to Γ—a Poincar´e section—such that
the return-time map T : Σ×(−0, 0) → R is defined. The following propo￾sition gives the geometrical conditions we seek.
Proposition 8.55. Suppose Γ is an (m : n) resonant unperturbed periodic
solution of the periodically perturbed oscillator (8.60); T : Σ × (−0, 0) →
R is the return time map defined on a Poincar´e section Σ with {v} =
Γ ∩ Σ; and R ⊆ R2 is a region containing Γ such that for some 0 > 0 the
displacement δ : R × (−0, 0) → R2 given in equation (8.61) is defined. If
Γ is contained in a period annulus A ⊆ R such that the differential T∗(v, 0)
of σ → T (σ, 0) at σ = v is nonsingular, then Kernel δζ (ζ, 0) = [f(ζ)] and
Range δζ (ζ, 0) = [f(ζ)] for each ζ ∈ Γ. If Γ is a hyperbolic limit cycle,
or if T∗(v, 0) is nonsingular, then Kernel δζ (ζ, 0) = [f(ζ)] for all ζ ∈ Γ.
Moreover, if Σ is orthogonal to Γ at v, then
Range δζ (ζ, 0) = [r1f(ζ) + r2f ⊥(ζ)]
for each ζ ∈ Γ where, for a and b as in Diliberto’s theorem (Theorem 8.11),
r1 = a(2πn/ω, v) = −
n
−1
j=0
bj (2π/ω, v)(T∗(ζ, 0)f ⊥(ζ)),
r2 = b(2πn/ω, v) − 1 = bn(2π/ω, v) − 1.
Proof. By equation (8.45), we have [f(ζ)] ⊆ Kernel δζ (ζ, 0). Consider the
vector field f ⊥ and the solution t → u⊥(t, ζ) of the initial value problem
u˙ = f ⊥(u), u(0) = ζ.
In other words u⊥(t, ζ) is the flow of f ⊥. We have
δζ (ζ, 0)f ⊥(ζ) = d
dt(u(mη(0), u⊥(t, ζ), 0) − u⊥(t, ζ))



t=0
= uζ (2πn/ω, ζ, 0)f ⊥(ζ) − f ⊥(ζ).
Here t → uζ (t, ζ, 0) is the principal fundamental matrix at t = 0 for the
variational equation (8.14). Thus, from equation (8.15) and Exercise 8.438.3 Nonautonomous Perturbations 495
we have
δζ (ζ, 0)f ⊥(ζ) =  n
−1
j=0
bj (2π/ω, v)

a(2π/ω, v)f(ζ)
+(bn(2π/ω, v) − 1)f ⊥(ζ). (8.62)
If Γ is contained in a period annulus, then b(2π/ω, v) = 1 and, by equa￾tion (8.18),
a(2π/ω, v) = − |f(ζ)|
2
σ˙(0), f ⊥(ζ)
T
(0)
where ˙σ(0) is the tangent vector to Σ at ζ determined by the parametriza￾tion of Σ. Thus,
δζ (ζ, 0)f ⊥(ζ) = −n |f(ζ)|
2
σ˙(0), f ⊥(ζ)
T
(0)f(ζ) = 0
and Kernel δζ (ζ, 0) = [f(ζ)]. Also, the range of δζ (ζ, 0) is [f(ζ)].
On the other hand, if Γ is a hyperbolic limit cycle, then b(2π/ω, v) = 1,
the coefficient of f ⊥(ζ) in equation (8.62) does not vanish, and f ⊥(ζ) ∈/
Kernel δζ (ζ, 0). Hence, in this case we have that Kernel δζ (ζ, 0) = [f(ζ)].
Moreover, if Σ is orthogonal to Γ at ζ, then
δζ (ζ, 0)f ⊥(ζ) = n
−1
j=0
bj (2π/ω, v)
 − T
(0)|f(ζ)|
2
σ˙(0), f ⊥(ζ)

f(ζ)
+ (bn(2π/ω, v) − 1)f ⊥(ζ).
✷
We say that a period annulus A is a regular period annulus if the dif￾ferential T∗(ζ, 0) of the return time, defined as in the previous theorem, is
nonsingular at every point of A. Let us note that the differential T∗(ζ, 0) is
nonsingular if and only if the corresponding period function for the period
annulus A has a nonvanishing derivative; that is, the period function is
regular.
Every resonant periodic orbit contained in a regular period annulus is
normally nondegenerate. Also, by Proposition 8.55, if Γ is a resonant peri￾odic orbit in A and ζ ∈ Γ, then both the kernel and range of the partial
derivative δζ (ζ, 0) are given by [f(ζ)]. In particular, if we restrict the lin￾ear map δζ (ζ, 0) to [f ⊥(ζ)], then the map δζ (ζ, 0) : [f ⊥(ζ)] → [f(ζ)] is an
isomorphism. We will use these facts in the analysis to follow.
Exercise 8.56. Prove that a linear map on a finite-dimensional vector space,
when restricted to a complement of its kernel, is an isomorphism onto its range.
What happens in an infinite-dimensional space?496 8. Continuation of Periodic Solutions
Let us reiterate a basic fact: The partial derivative δζ (ζ, 0) of the dis￾placement function, when viewed as a map on all of R2, has a non-trivial
kernel. Although this precludes a direct application of the implicit func￾tion theorem to solve the equation δ(ζ,) = 0 on R2 × R, we can use the
implicit function theorem to reduce our search for continuation points to
the problem of solving a related equation on a lower dimensional space.
This is accomplished by using an important technique called Lyapunov–
Schmidt reduction. This method is very general. In fact, it works for equa￾tions defined on Banach spaces when the linear map playing the role of our
derivative δζ (ζ, 0) is a Fredholm operator. We will give a brief introduc￾tion to these simple but powerful ideas in an abstract setting. As we will
demonstrate when we apply the method to our continuation problem, it is
very fruitful to keep the idea of the method firmly in mind, but it may not
be efficient to adapt all of the abstraction verbatim. Also, on a first reading
and for the applications to be made later in this section, it is sufficient to
consider only finite-dimensional real Banach spaces, that is, Rn with the
usual norm.
Let us suppose that B1 and B2 are Banach spaces and L(B1, B2) denotes
the bounded linear maps from B1 to B2. Also, with this notation, let us
recall that a map G : B1 → B2 is C1 if there is a continuous map L : B1 →
L(B1, B2) such that
limn→0
G(x + h) − G(x) − L(x) · h
h = 0
for each x ∈ B1. A map A ∈ L(B1, B2) is called Fredholm if it has a
finite-dimensional kernel and a closed range with a finite-dimensional com￾plement. The index of a Fredholm map is defined to be the difference of
the dimensions of its corange and kernel.
Suppose that X and Y are open subsets of the Banach spaces B and
E, respectively, and F : X × Y → B, given by (x, y) → F(x, y), is a C1
map such that F(0, 0) = 0. Since F is C1, the partial derivative Fx(0, 0)
is a bounded linear map on B. Let us assume in addition that Fx(0, 0) is
Fredholm with index zero.
If B is finite dimensional, as in our application where F is the displace￾ment function and B = R2, then every linear map is automatically Fred￾holm. Although we will use the hypothesis that our Fredholm map has index
zero to ensure that the final reduced bifurcation function is a map between
finite-dimensional spaces of the same dimension, the general Lyapunov–
Schmidt reduction technique does not require the Fredholm map to have
index zero.
Let K denote the kernel of the Fredholm map Fx(0, 0), and let R denote
its range. There are subspaces KC and RC such that B = K ⊕ KC and
B = R ⊕ RC. The complement RC exists by the Fredholm hypothesis.
The existence of a complement KC for the finite-dimensional kernel K in8.3 Nonautonomous Perturbations 497
an infinite-dimensional Banach space is a consequence of the Hahn–Banach
theorem (see [223, p. 105]). Indeed, choose a basis for the finite-dimensional
subspace, apply the Hahn–Banach theorem to extend the corresponding
dual basis functionals to the entire Banach space, use the extended func￾tionals to define a projection from the entire space to the finite-dimensional
subspace, and construct the desired complement as the range of the com￾plementary projection.
The complementary subspaces KC and RC are not unique. In fact, in
the applications, the correct choices for these spaces can be an important
issue. On the other hand, there are always complementary linear projections
P : B → R and Q : B → RC corresponding to the direct sum splitting
of B. Also, there is a product neighborhood of the origin in X of the form
U × V where U ⊆ K and V ⊆ KC.
Consider the map H : U ×V ×Y → R defined by (u, v, y) → PF(u+v, y).
Its partial derivative with respect to v at (0, 0, 0) is given by
PFx(0, 0)

KC : KC → R. (8.63)
The map P is the projection to the range of Fx(0, 0). Thus, PFx(0, 0)

KC =
Fx(0, 0)

KC . Note that in a finite-dimensional space the map (8.63) is an
isomorphism. The same result is true in an infinite-dimensional space under
the assumption that Fx(0, 0) is Fredholm. In effect, the open mapping the￾orem (see [223, p. 99]) states that a continuous bijective linear map of
Banach spaces is an isomorphism.
The main idea of the Lyapunov–Schmidt reduction results from the
observation that, by the implicit function theorem applied to the map H,
there are open sets U1 ⊆ U and Y1 ⊆ Y , and a C1 map h : U1 × Y1 → KC,
with h(0, 0) = 0 such that
PF(u + h(u, y), y) ≡ 0.
The (Lyapunov–Schmidt) reduced function F : U1×Y1 → RC associated
with F is defined by
(u, y) → QF(u + h(u, y), y).
Clearly, F(0, 0) = 0. If there is a continuous function y → β(y), with
β(y) ∈ U1 such that β(0) = 0 and F(β(y), y) ≡ 0, then
QF(β(y) + h(β(y), y), y) ≡ 0,
PF(β(y) + h(β(y), y), y) ≡ 0.
In particular, since P and Q are projections to complementary subspaces
of B, we must have
F(β(y) + h(β(y), y), y) ≡ 0,498 8. Continuation of Periodic Solutions
that is, y → β(y) + h(β(y), y) is an implicit solution of F(x, y) = 0 for x as
a function of y near (x, y) = (0, 0).
The implicit function theorem cannot be used directly to find u as an
implicit function of y for the reduced equation F(u, y) = 0. (If this were
possible, then we would have been able to solve the original equation
F(x, y) = 0 by an application of the implicit function theorem.) To prove
this fact, let us consider the partial derivative
Fu(0, 0) = QFx(0, 0)(I + hu(0, 0)) : K → RC.
Here r := Fx(0, 0)(I + hu(0, 0))u ∈ R, so Qr = 0. Thus, Fu(0, 0) is not
invertible; it is in fact the zero operator.
Although the implicit function theorem does not apply directly to the
reduced function, we may be able to apply it after a further reduction. For
example, in the applications to follow, we will have a situation where
F(u, 0) ≡ 0. (8.64)
In this case, under the assumption that F ∈ C2, let us apply Taylor’s the￾orem (Theorem 1.252) to obtain the representation F(u, y) = Fy(u, 0)y +
G(u, y)y where (u, y) → G(u, y) is the C1 function given by
G(u, y) =  1
0
(Fy(u, ty) − Fy(u, 0)) dt.
Thus, we also have
F(u, y)=(Fy(u, 0) + G(u, y))y
where G(u, 0) = 0 and
Gy(u, 0) =  1
0
tFyy(u, 0) dt = 1
2
Fyy(u, 0).
In particular, let us note that the simple zeros of the reduced bifurcation
function B : U1 → RC defined by
B(u) = Fy(u, 0)
are the same as the simple zeros of the function u → Fy(u, 0) + G(u, 0).
Thus, by another application of the implicit function theorem, it follows
that if the reduced bifurcation function B has a simple zero, then the
equation
Fy(u, 0) + G(u, y)=0
has an implicit solution. Therefore, the simple zeros of the reduced bifur￾cation function B are continuation points.8.3 Nonautonomous Perturbations 499
Let us now apply the Lyapunov–Schmidt reduction to our continua￾tion problem in case the resonant periodic orbit Γ is contained in a reg￾ular period annulus. For definiteness, let Γm/n := Γ denote the unper￾turbed periodic solution that is in (m : n) resonance with the periodic
perturbation, and recall from Proposition 8.55 that Kernel δζ (ζ, 0) = [f(ζ)]
and Range δζ (ζ, 0) = [f(ζ)]. According to the Lyapunov–Schmidt reduc￾tion, we should choose coordinates and projections relative to the split￾ting R2 = [f(ζ)] ⊕ [f ⊥(ζ)]. But, in keeping with the philosophy that the
Lyapunov–Schmidt reduction is merely a guide to the analysis, we will con￾sider instead a coordinate system that has the required splitting property
“infinitesimally”; that is, we will choose coordinates tangent to the sum￾mands of the splitting rather than coordinates on the subspaces themselves.
Let ϕt denote the flow of the system ˙u = f(u) and let ψt denote the flow
of the system ˙u = f ⊥(u). Of course, ϕt(ζ) = u(t, ζ, 0). Define
Υ(ρ, φ) = ϕφψρ(v),
where v ∈ Γm/n is viewed as arbitrary but fixed. Also, the subscripts on ϕ
and ψ denote the temporal parameter in the respective flows, not partial
derivatives. This should cause no confusion if the context is taken into
account.
The (ρ, φ)-coordinates are defined in some annulus containing Γm/n.
They have the property that for ρ fixed, φ → Υ(ρ, φ) is tangent to f,
whereas for φ fixed at φ = 0, the map ρ → Υ(ρ, φ) is tangent to f ⊥. More
precisely, we have that
Υρ(ρ, φ) = DΥ(ρ, φ) ∂
∂ρ = Dϕφ(ψρ(v))f ⊥(ψρ(v)),
Υφ(ρ, φ) = DΥ(ρ, φ) ∂
∂φ = f(Υ(ρ, φ))
where ∂/∂ρ (respectively ∂/∂φ) denotes the unit vector field tangent to the
ρ-axis (respectively the φ-axis) of the coordinate plane. Also, in the new
(local) coordinates, the displacement is given by
Δ(ρ, φ, ) := δ(Υ(ρ, φ), ) (8.65)
and we have that
Δ(0, φ, 0) = δ(ϕφ(v), 0) ≡ 0.
The first step of the Lyapunov–Schmidt reduction method is to find an
implicit solution of the map H given by H(ρ, φ, ) := P · Δ(ρ, φ, ) where
P := P(φ) is a projection onto the range [f(ϕφ(v))] of the linear map
δζ (ϕφ(v), 0). For definiteness, let ,  denote the usual inner product on R2
and define H by
(ρ, φ, ) → Δ(ρ, φ, ), f(ϕφ(v)).500 8. Continuation of Periodic Solutions
The partial derivative of H with respect to ρ—the direction complemen￾tary to the kernel—evaluated at (0, φ, 0) is given by Δρ(0, φ, 0), f(ϕφ(v)).
Using Diliberto’s theorem, Proposition 8.55, and equation (8.62), we have
Δρ(0, φ, 0) = δζ (ϕφ(v), 0)DΥ(0, φ) ∂
∂ρ
= δζ (ϕφ(v), 0)Dϕφ(v)f ⊥(v)
= δζ (ϕφ(v), 0)
a(φ)f(ϕφ(v)) + b(φ)f ⊥(ϕφ(v))	
= b(φ)δζ (ϕφ(v), 0)f ⊥(ϕφ(v))
= b(φ)a(2πn/ω)f(ϕφ(v)). (8.66)
Thus,
Hρ(0, φ, 0) = Δρ(0, φ, 0), f(ϕφ(v)) = b(φ)a(2πn/ω)|f(ϕφ(v))|
2 = 0,
and we can apply (as expected) the implicit function theorem to obtain an
implicit function (φ, ) → h(φ, ) such that h(φ, 0) = 0 and
Δ(h(φ, ), φ, ), f(ϕφ(v)) ≡ 0.
Also, because Δ(0, φ, 0) ≡ 0 and the implicit solution produced by an appli￾cation of the implicit function theorem is unique, we have that h(φ, 0) ≡ 0.
The second step of the Lyapunov–Schmidt reduction is to consider the
zeros of the reduced displacement function Δ given by 
(φ, ) → Q(φ)(Δ(h(φ, ), φ, )) = Δ(h(φ, ), φ, ), f ⊥(ϕφ(v))
where Q(φ) is the indicated linear projection onto the complement of the
range of the partial derivative δζ (ϕφ(v), 0). Here, as mentioned previously,
we can make a further reduction. In fact, because
Δ(h(φ, 0), φ, 0), f ⊥(ϕφ(v)) ≡ 0,
it follows that
Δ(  φ, ) = (Δ(φ, 0) + O()).
Let us define the bifurcation function B : R → R by
B(φ) := Δ(φ, 0).
By the general remarks following equation (8.64), the simple zeros of B are
(ultra)subharmonic continuation points. This ends the reduction phase of
our analysis.
We will identify the bifurcation function B geometrically and analyti￾cally. As we will see in a moment,
B(φ) = Δ(φ, 0) = Q(φ)P(ϕφ(v), 0) (8.67)8.3 Nonautonomous Perturbations 501
Figure 8.5: The top left panel depicts a resonant periodic orbit Γ viewed
as a manifold of fixed points for the Poincar´e map. The “twist” in the
tangential directions along Γ due to the changing periods of the periodic
orbits in the period annulus and the normal “push” directions due to the
perturbations as detected by the Melnikov function are also depicted. In
this illustration, the Melnikov function has two zeros. The top right panel
shows the local directions of twist and push near the continuation points
corresponding to these zeros. The bottom two panels depict the perturbed
fixed points of the Poincar´e map (subharmonics of the perturbed differential
equation) and their stability types as would be expected by inspection of
the directions of twist and push. The local phase portraits of the perturbed
periodic orbits are seen to be saddles and rotation points that alternate
in the direction of the unperturbed resonant orbit. The global depiction
of the stable and unstable manifolds of the saddle point only illustrates
one of many possibilities. Also, the rotation point is depicted as a center,
but of course it can be a source or sink, depending on the nature of the
perturbation.502 8. Continuation of Periodic Solutions
where P is the Poincar´e map. Also, let us note that if we take Q to be an
arbitrary projection to the complement of the range of δζ (ϕφ(v), 0), then
we will obtain an equivalent bifurcation function, that is, a bifurcation
function with the same simple zeros. In any case, the bifurcation function
is the projection onto the complement of the range of the partial derivative
of the Poincar´e map with respect to the bifurcation parameter.
To determine an analytic expression for the bifurcation function and to
show that the representation (8.67) is valid, start with the definitions of
B and Δ, and compute the derivative of  Δ(  φ, ) at  = 0 to obtain the
formula
B(φ) = Δρ(h(φ, 0), φ, 0)h(φ, 0) + Δ(h(φ, 0), φ, 0), f ⊥(ϕφ(v))
= Δρ(0, φ, 0)h(φ, 0) + Δ(0, φ, 0), f ⊥(ϕφ(v)).
By using equation (8.66) and the identity h(φ, 0) ≡ 0, it follows that
B(φ) = Δ(0, φ, 0), f ⊥(ϕφ(v)).
Here, Δρ(0, φ, 0)h(φ, 0) is viewed as the vector Δρ(0, φ, 0) multiplied by
the scalar h(φ, 0) . Strictly speaking, Δρ(0, φ, 0) is a linear transformation
R → R2 represented by a 2 × 1 matrix that we identify with a vector in
R2 and h(φ, 0) is a linear transformation R → R that we identify with a
scalar.
To find a formula for the partial derivative Δ(0, φ, 0), first use the defi￾nition δ(ζ,) = u(mη(),ζ,) − ζ and compute the partial derivative with
respect to  to obtain the equation
Δ(0, φ, 0) = mη
(0)f(ϕφ(v)) + u(2πn/ω, ϕφ(v), 0). (8.68)
Then, by equation (8.32), we have
u(2πn/ω, ζ, 0) = (N + aM)f(ζ) + bMf ⊥(ζ), (8.69)
and therefore
B(φ) = b(2πn/ω)|f(ϕφ(v))|
2M(φ).
Thus, φ is an (ultra)subharmonic continuation point if and only if φ is a
simple zero of the subharmonic Melnikov function
M(φ) :=  2πn/ω
0
e−  t
0 div f(ϕs+φ(v)) dsf(ϕt+φ(v)) ∧ g(ϕt+φ(v), t, 0) dt.
(8.70)
These arguments are formalized in the following theorem.
Theorem 8.57. If Γ is an (m : n) resonant unperturbed periodic solu￾tion of the differential equation (8.60) contained in a regular period annu￾lus, then the simple zeros of the bifurcation function φ → M(φ) defined
by (8.70) are the (ultra)subharmonic continuation points.8.3 Nonautonomous Perturbations 503
What is the real meaning of the Melnikov function? One answer to this
question is provided by the identification given by equation (8.67). The
partial derivative of the Poincar´e map in the direction  determines the
infinitesimal direction of drift for orbits of the perturbed Poincar´e map
near the point ϕφ(v). When the magnitude of the infinitesimal drift is zero,
then we expect a periodic orbit. The precise condition for this is given in
Theorem 8.57.
The stability type of the perturbed orbit is also determined by an exami￾nation of the direction of drift determined by the Melnikov function. In fact,
the resonant periodic orbit is fixed by the unperturbed Poincar´e map. By
the assumption that the resonant orbit is “normally nondegenerate,” the
drift of the unperturbed Poincar´e map is in opposite directions on opposite
sides of the resonant orbit. The sign of the Melnikov function determines
the drift in the direction of the complement to the range of the infinitesimal
displacement, a direction that is known to be transverse to the unperturbed
orbit. A plot of these directions at a continuable point suggests the stability
type as seen in Figure 8.5.
Exercise 8.58. Use the Lyapunov–Schmidt reduction to determine conditions
on the triplet of functions (g1, g2, g3) so that the system of equations,
1 − x2 − y2 − z2 + g1(x, y, z)=0,
1 − x2 − y2 − z2 + g2(x, y, z)=0,
xyz + g3(x, y, z)=0,
has solutions for small  = 0. What (additional) condition assures that roots
found in this way are simple?
Exercise 8.59. Consider the forced rotor given by
¨θ +  sin θ =  sin t.
The associated first-order system with ˙
θ = v can be considered as a differential
equation on the cylinder R × T, where T denotes the unit circle. In this interpre￾tation, all orbits of the unperturbed system are periodic. Moreover, the periodic
orbit Γ corresponding to v = 1 is (1 : 1) resonant. Show that Γ is normally non￾degenerate, and determine the continuation points on Γ. What can you say about
the (m : n) resonant periodic orbits? Change the time scale in the differential
equation to slow time τ = √t. What is the meaning of the continuable peri￾odic solutions relative to the transformed differential equation? The slow time
equation is a rapidly forced pendulum. Does it have subharmonics?
Exercise 8.60. Suppose that F : R3 × R → R3 is a function given in the form
F(u, ) = g(u) + h(u)
where g, h : R3 → R3 are smooth vector functions with
g(u)=(g1(u), g2(u), g3(u)), h(u)=(h1(u), h2(u), h3(u)).504 8. Continuation of Periodic Solutions
Prove the following theorem: If the slot functions g1 and g2 are identical and
v ∈ R3 is such that g(v) = 0 and the vectors grad g1(v) and grad g3(v) are linearly
independent, then there is a curve s → γ(s) in R3 such that γ(0) = 0, ˙γ(s) = 0,
and F(γ(s), 0) ≡ 0. If such a curve exists and s = 0 is a simple zero of the scalar
function given by s → h2(γ(s)) − h1(γ(s)), then there is a curve  → β() is R3
such that β(0) = v and F(β(), ) ≡ 0. Moreover, for each sufficiently small  = 0,
the point β() is a simple zero of the function u → F(u, ) (see [57]).
Exercise 8.61. Prove that the roots of a (monic) polynomial depend continu￾ously on its coefficients.
8.3.5 Limit Cycles–Entrainment–Resonance Zones
In Section 8.3.4 we considered the continuation of (ultra)subharmonics of
the differential equation
u˙ = f(u) + g(u, t, ), u ∈ R2 (8.71)
from a resonant unperturbed periodic orbit contained in a period annulus.
Here, we will consider continuation of (ultra)subharmonics from a resonant
unperturbed limit cycle.
If we view the differential equation (8.71) as an autonomous first-order
system on the phase cylinder R2×T, then the unperturbed differential equa￾tion has an invariant torus Γ × T. For the theory in this section, it is not
necessary to determine the fate of the invariant torus after perturbation.
In fact, if Γ is a hyperbolic limit cycle, then the corresponding invariant
torus is a normally hyperbolic invariant manifold. Roughly speaking, an
invariant manifold is attracting and normally hyperbolic if the linearized
flow for each orbit on the manifold contracts normal vectors at an exponen￾tial rate, and if the slowest such rate is faster than the fastest contraction
rate for a vector that is tangent to the manifold. There is a similar defini￾tion if the manifold is repelling or if it has both attracting and repelling
normal directions. In our case, if the limit cycle Γ is attracting, then its
normal contraction rate is exponential and its tangential contraction rate is
zero. Moreover, the invariant unperturbed torus corresponding to Γ inher￾its this behavior (see Exercise 8.63). Thus, this invariant torus is normally
hyperbolic. In this case, by a powerful, important theorem (see [224], [225]
for the Neimark–Sacker bifurcation and the generalizations [99] and [139]),
the normally hyperbolic torus persists after perturbation. The continuation
theory in this section describes the flow on this perturbed invariant torus.
Typically, there is an even number of (ultra)subharmonics that alternate
in stability around the perturbed torus.
By the above remarks, if our unperturbed system has a resonant, attract￾ing hyperbolic limit cycle, then after perturbation there is an attract￾ing invariant torus and nearby perturbed orbits are attracted to stable
(ultra)subharmonic orbits on this torus. In the engineering literature, this8.3 Nonautonomous Perturbations 505
phenomenon is called entrainment: As nearby orbits are attracted to the
perturbed invariant torus, their quasi-periods approach the periods of the
(ultra)subharmonics on the perturbed torus. In particular, the asymptotic
periods are entrained to a multiple of the period of the input perturbation.
For a perturbation of small amplitude, this entrained period is close to the
resonant period mη(0) as in equation (8.41). We will determine a bifur￾cation function whose simple zeros are the continuation points of these
(ultra)subharmonics.
For the remainder of this section let us consider the periodically per￾turbed oscillator (8.71) under the following assumptions:
(i) There is an unperturbed periodic orbit Γ in (m : n) resonance with
the periodic perturbation as in equation (8.41).
(ii) There is a region R ⊆ R2 with Γ ⊂ R such that the displacement
δ : R × (−0, 0) → R2 is defined for some 0 > 0.
(iii) As in Proposition 8.55, the periodic orbit Γ is a hyperbolic limit cycle,
or, alternatively, the differential of the return-time map σ → T (σ, 0)
at v = Γ ∩ Σ defined on some curve Σ transverse to Γ is nonsingular.
Let us also note that by the third hypothesis and Proposition 8.55, we have
that Kernel δζ (ζ, 0) = [f(ζ)] for ζ ∈ Γ, and therefore the invariant torus
Γ × T is normally nondegenerate.
The analysis required to obtain the bifurcation function in case the
unperturbed resonant periodic orbit is a limit cycle is analogous to the
analysis carried out in the last section for the case of a regular period
annulus. In particular, using the same notation as before, we can apply the
Lyapunov–Schmidt reduction to the displacement function represented in
the same (ρ, φ)-coordinates.
By the abstract theory of the Lyapunov–Schmidt reduction, ρ can be
defined implicitly as a function of (φ, ) when it is projected onto the range
of the infinitesimal displacement, that is, the partial derivative of the
displacement with respect to the space variable. It is easy and instructive
to verify this directly. In fact, with respect to the (ρ, φ)-coordinates, the
section Σ as in Proposition 8.55 is just an integral curve of f ⊥, and hence
it is orthogonal to Γ. Thus, let us first consider the map
(ρ, φ, ) → Δ(ρ, φ, ), r1f(ϕφ(v)) + r2f ⊥(ϕφ(v))
where r1 and r2 are defined in Proposition 8.55 and Δ is the local coor￾dinate representation defined in display (8.65) of the displacement. By
equation (8.66) and this proposition, its differential with respect to ρ at506 8. Continuation of Periodic Solutions
(ρ, ) = (0, 0) is given by
Δρ(0, φ, 0), r1f(ϕφ(v)) + r2f ⊥(ϕφ(v))
= δζ (ϕφ(v), 0)DΥ(0, φ)∂/∂ρ, r1f(ϕφ(v)) + r2f ⊥(ϕφ(v))
= b(φ)δζ (ϕφ(v), 0)f ⊥(ϕφ(v)), r1f(ϕφ(v)) + r2f ⊥(ϕφ(v))
= b(φ)r1f(ϕφ(v)) + r2f ⊥(ϕφ(v)), r1f(ϕφ(v)) + r2f ⊥(ϕφ(v))
= b(φ)|f(ϕφ(v))|
2(r2
1 + r2
2).
Also, let us note that during the course of the last computation we have
proved that
Δρ(0, φ, 0) = b(φ)(r1f(ϕφ(v)) + r2f ⊥(ϕφ(v))). (8.72)
By the assumptions for this section, we have r2
1 + r2
2 = 0, and therefore
Δρ(0, φ, 0) = 0. Thus, by an application of the implicit function theorem,
there is a function (φ, ) → h(φ, ) such that h(φ, 0) ≡ 0 and
Δ(h(φ, ), φ, ), r1f(ϕφ(v)) + r2f ⊥(ϕφ(v)) ≡ 0.
Recall that Γ × T is normally nondegenerate if
Kernel δζ (ϕφ(v), 0) = [f(φφ(v))].
Since this kernel is a one-dimensional subspace of the two-dimensional tan￾gent space of the Poincar´e section, the range of the infinitesimal displace￾ment must also be one-dimensional, and therefore r2
1 + r2
2 = 0. Of course,
this inequality also holds if either Γ is hyperbolic or the differential of the
return time is nonzero.
The reduced displacement function is just the projection of the dis￾placement onto a complement for the range of the infinitesimal displace￾ment. For definiteness, let us consider the reduced displacement function
(φ, ) → Δ(  φ, ) given by
Δ(  φ, ) = Δ(h(φ, ), φ, ), −r2f(ϕφ(v)) + r1f ⊥(ϕφ(v)).
Since Δ(h(φ, 0), φ, 0) ≡ 0, we have Δ(  φ, 0) ≡ 0 and
Δ(  φ, ) = (Δ(φ, 0) + O()).
If the bifurcation function B : R → R is defined by B(φ) := Δ(φ, 0), then
we have, as usual, the following proposition: The simple zeros of B are the
(ultra)subharmonic continuation points. This ends the reduction step.
The identification of the bifurcation function B is accomplished with
the aid of a simple computation. Indeed, using the definition of B, we have
that
B(φ) = Δρ(h(φ, 0), φ, 0)h(φ, 0) + Δ(h(φ, 0), φ, 0),
− r2f(ϕφ(v)) + r1f ⊥(ϕφ(v)). (8.73)8.3 Nonautonomous Perturbations 507
To simplify this expression, apply identity (8.72) to obtain the representa￾tion
B(φ) = Δ(h(φ, 0), φ, 0), −r2f(ϕφ(v)) + r1f ⊥(ϕφ(v)).
Also, let us note that, as in the last section, B(φ) = Q(φ)P(ϕφ(v), 0).
Using equations (8.68) and (8.40), substitute the solution (8.32) of the
nonhomogeneous variational equation for Δ to obtain the formula
B(φ) = mη
(0)f(ϕφ(v)) + 
N (φ) + a(2πn/ω)M(φ)
	
f(ϕφ(v))
+ b(2πn/ω)M(φ)f ⊥(ϕφ(v)), −r2f(ϕφ(v)) + r1f ⊥(ϕφ(v))
where, by Proposition 8.55,
r1 = a(2πn/ω, ϕφ(v)), r2 = b(2πn/ω, ϕφ(v)) − 1.
Hence, the bifurcation function is given by
B(φ) = 
(1 − b(2πn/ω, ϕφ(v)))(mk + N (φ))
+ a(2πn/ω, ϕφ(v))M(φ)
	
|f(ϕφ(v))|
2.
Define the subharmonic bifurcation function by
C(φ) := (1−b(2πn/ω, ϕφ(v)))(mk+N (φ))+a(2πn/ω, ϕφ(v))M(φ) (8.74)
where
b(t, ϕφ(v)) = |f(v)|
2
|f(ϕt+φ(v))|
2 e
 t
0 div f(ϕs+φ(v)) ds,
a(t, ϕφ(v)) =  t
0

2κ(s, ϕφ(v))|f(ϕs+φ(v))|
− curl f(ϕs+φ(v))	
b(s, ϕφ(v)) ds,
b(2πn/ω, ϕφ(v)) = bn(2π/ω, ϕφ(v)) = (e

Γ div f )
n,
a(2πn/ω, ϕφ(v)) =  n
−1
j=0
bj (2π/ω, ϕφ(v))
×
 2π/ω
0

2κ(t, ϕφ(v))|f(ϕt+φ(v))|
− curl f(ϕt+φ(v))	
b(t, ϕφ(v)) dt;
and
M(φ) =  2πn/ω
0
1
b(t, φ)|f(ϕt+φ(v))|
2 f(ϕt+φ(v)) ∧ g(ϕt+φ(v), t, 0) dt,
N (φ) =  2πn/ω
0
1
|f(ϕt+φ(v))|
2 g(ϕt+φ(v), t, 0), f(ϕt+φ(v)) dt
−
 2πn/ω
0
a(t, φ)
b(t, φ)|f(ϕt+φ(v))|
2 f(ϕt+φ(v)) ∧ g(ϕt+φ(v), t, 0) dt.508 8. Continuation of Periodic Solutions
Remark 1. The function φ → b(2πn/ω, ϕφ(v)) is constant, but the function
φ → a(2πn/ω, ϕφ(v)) may not be constant.
Theorem 8.62. If Γ is an (m : n) resonant unperturbed periodic solu￾tion of the periodically perturbed oscillator (8.71) such that Γ × T is a
normally nondegenerate unperturbed invariant torus for the system (8.42),
then the simple zeros of the subharmonic bifurcation function φ → C(φ) are
(ultra)subharmonic continuation points.
By inspection of the formula for the subharmonic bifurcation function
φ → C(φ), let us note that this function is periodic with period 2π/ω,
the period of the resonant limit cycle Γ. This simple observation leads
to an important application of Theorem 8.62, at least in the case where
Γ is hyperbolic. In fact, the theorem provides a partial answer to the
following question: What are the regions in the (η, ) parameter space cor￾responding to the existence of (m : n) (ultra)subharmonics of the sys￾tem (8.71)? For this application it is traditional to view these regions
in frequency–amplitude coordinates instead of period–amplitude coordi￾nates. Thus, let us define Ω = 2π/η. The subset Dm/n of all (Ω, ) in the
parameter space such that the corresponding system (8.71) has an (m : n)
(ultra)subharmonic is called the (m : n) entrainment domain. In effect,
relative to the geometric interpretation provided by the system (8.42), if
(Ω, ) is in Dm/n, then there is a solution of the corresponding system on
the perturbed invariant torus that wraps n times in the direction of φ, the
parameter on Γ, and m times in the direction of the “time” given by τ in
system (8.42).
Theorem 8.62 only applies for  sufficiently small. Thus, it cannot provide
an answer to the general question posed above. Nonetheless, this theorem
does give valuable insight into the geometry of entrainment domains near
 = 0. To see why this is so, note first that the frequency, in terms of
equation (8.40), is given by
Ω = m
n
ω − km2
2πn2 ω2 + O(
2), (8.75)
and the (ultra)subharmonics correspond to the simple zeros of the subhar￾monic bifurcation function C. Thus, using the definition of C we expect
(ultra)subharmonics to exist whenever the detuning parameter k satisfies
the equation
(1 − b(2πn/ω, ϕφ(v)))mk = −C(φ) (8.76)
for some φ ∈ R, where the new function C is defined by
C(φ) = (1 − b(2πn/ω, ϕφ(v)))N (φ) + a(2πn/ω, ϕφ(v))M(φ).
The existence of solutions for equation (8.76) depends on the maximum
and minimum values of the function C on the interval 0 ≤ φ < 2π/ω.
Let us denote these values by Cmin and Cmax. Also, let us assume that8.3 Nonautonomous Perturbations 509
the unperturbed resonant torus is attracting, that is, b(2πn/ω, ϕφ(v)) < 1.
Under these assumptions, we have the following result: If
Cmin < (b − 1)mk < Cmax,
then (m : n) (ultra)subharmonics exist for sufficiently small ||. In other
words, from equation (8.75), the lines in the frequency–amplitude space
given by
L1 := {(Ω, ):Ω= m
n
ω +
Cmin
(1 − b)
mω2
2πn2 },
L2 := {(Ω, ):Ω= m
n
ω +
Cmax
(1 − b)
mω2
2πn2 } (8.77)
are the tangent lines to the (m : n) entrainment domain at  = 0. The
shape of an entrainment domain (see, for example, Figure 8.6) suggested
to Vladimir Arnold the shape of a tongue. Thus, entrainment domains are
often referred to as Arnold tongues. They are also called resonance zones
or resonance horns.
If C(φ) ≡ 0, then the tangents L1 and L2 computed above coincide. In
this case the tongue has vertical tangents provided that it extends all the
way to the -axis.
If C has a simple zero, then the corresponding tongue is “open.” Also, in
the (Ω, )-coordinates the left boundary of the tongue corresponds to the φ
coordinate on Γ giving the minimum value of C, while the right boundary
corresponds to the maximum value of C. Thus, we see how the phase of
the entrained solution shifts as the detuning parameter is changed so that
(1 − b(2πn/ω))mk passes from the minimum to the maximum value of
C. Finally, if a boundary is crossed as the detuning k is varied, say the
boundary corresponding to the minimum of C, then it is clear that for k
sufficiently small there are no (ultra)subharmonics, for k at the minimum
of C there is a bifurcation point, and as k increases from this value, two
branches of subharmonics bifurcate. This scenario is very common (generic,
in fact) and is called a saddle-node bifurcation. It will be studied in detail
in Chapter 11.
Let us consider the family of differential equations
x˙ = −y + x(1 − x2 − y2),
y˙ = x + y(1 − x2 − y2) +  cos(Ωt) (8.78)
to illustrate some typical computations that are used to approximate the
boundaries of entrainment domains (see [43]).
The unperturbed member of the family (8.78) has the unit circle as an
attracting hyperbolic limit cycle with the corresponding solution starting
at (x, y) = (cos θ,sin θ) given by
x(t) = cos(t + θ), y(t) = sin(t + θ).510 8. Continuation of Periodic Solutions
Ω=1
Ω
L− L+
D1/1
Figure 8.6: A schematic depiction of the (1 : 1) entrainment domain D1/1
and its tangent lines L± := {(Ω, ) :  = ±2 (Ω − 1)} at (Ω, ) = (1, 0) for
the system (8.78).
If Ω() := m/n + Ω1, then the period of the forcing function is
2π
Ω() = 2πn
m − 2π
 n
m
2
Ω1 + O(
2).
Also, for this system a ≡ 0, and therefore
C(θ) = (1 − b(2πn))mk + (1 − b(2πn))N (θ, 2πn)
= 
1 − e−4πn	
mk + 
1 − e−4πn	  2πn
0
cos(t + θ) cos(Ω(0)t) dt.
Moreover, we have that
C(θ) = 
1 − e−4πn	  2πn
0
cos(t + θ) cos(m
n
t) dt
= 
1 − e−4πn	
×
πn cos θ, m
n = 1,
n2
m2−n2

sin θ + m+n
2n sin(2πm − θ) + m−n
2n sin(2πm + θ)
	
, m
n = 1
=

1 − e−4πn	
πn cos θ, m = n,
0, m = n.
Thus, for m = n; that is, for the (1 : 1) resonance, the tangents of the
entrainment domain at the resonant point (Ω, ) = (1, 0) are
 = ±2 (Ω − 1),
whereas, for the case m = n, the tangents have infinite slope.8.3 Nonautonomous Perturbations 511
The phase shift mentioned above is also easy to see in this example. The
phase angle is θ. Also, if we use the equality m = n and divide by a common
factor, then the equation
k + π cos θ = 0
has the same roots as the zeros of the subharmonic bifurcation function. In
particular, the detuning parameter k simply serves to translate the graph of
the function θ → π cos θ in the vertical direction. Thus, at the left boundary
of the tongue, k = π and the phase of the entrained solution will be near
θ = π, whereas at the right-hand boundary we have k = −π and the phase
will be near θ = 0.
Exercise 8.63. Suppose that Γ is a hyperbolic limit cycle of the planar system
u˙ = f(u). Show that the linearized flow on the limit cycle attracts or repels
normal vectors on the limit cycle at an exponential rate. Hint: The limit cycle
has a characteristic multiplier that is not unity. Alternatively use the function b
defined in Diliberto’s theorem. Also, show that normal vectors on the invariant
torus Γ × T for the system
u˙ = f(u), ψ˙ = 1
on the phase cylinder where ψ is an angular variable are attracted exponentially,
whereas tangent vectors have contraction rate zero.
Exercise 8.64. The theory of this chapter does not apply directly to determine
the subharmonic solutions of the system
x˙ = y − x(1 − x2 − y2
)
2 −  cos t,
y˙ = − x − y(1 − x2 − y2
)
2 +  sin t.
Why? Develop an extension of the continuation theory to cover this case and use
your extension to determine the subharmonics (see [41]).
8.3.6 Lindstedt Series and the Perihelion of Mercury
We have discussed in detail how to prove the existence of periodic solu￾tions of nonlinear differential equations by continuation. In this section, we
will consider a procedure invented by Anders Lindstedt in 1882 that can
be used to find useful series approximations for these periodic solutions.
Lindstedt’s method will be applied to the problem of the precession of the
perihelion of Mercury—the most famous verification of the general theory
of relativity—and in the next section it will be used to determine the widths
of entrainment domains for a forced van der Pol oscillator. The limitations
of Lindstedt’s method will also be briefly discussed.
Let us begin with the problem of the perihelion of Mercury. If a Cartesian
coordinate system is fixed at the Sun, then the osculating ellipse traced
out by the motion of Mercury is observed to precess. This means that512 8. Continuation of Periodic Solutions
Perihelion
Mercury
Sun
Figure 8.7: The point of closest approach to the Sun moves after each orbit
in the direction of the revolution of Mercury. (The orbit of Mercury is
nearly circular. The figure is not drawn to scale.)
the perihelion of Mercury—the point of closest approach to the Sun—
changes after each revolution, moving in the direction of the motion of
the planet (see Figure 8.7). In fact, the point of perihelion is observed to
advance by approximately 43 seconds of arc per century. No satisfactory
explanation of this phenomenon was known until after the introduction of
the general theory of relativity by Albert Einstein. In particular, in 1915
Einstein found that his theory indeed predicts a precession of 43 seconds
of arc per century—a stunning confirmation of his theory (see [240, Part
I]). Shortly thereafter, Karl Schwarzschild (1916) found a solution of the
gravitational field equations for a circularly symmetric body—the Sun—
and he gave a rigorous derivation of the same relativistic correction to the
Newtonian solution for the orbit of Mercury (see, for example, [259, p. 247]
for more history).
While the derivation of Schwarzschild’s solution of the perihelion problem
from the general theory of relativity is beyond the scope of this book (see
[155], [240], or [259] for readable accounts), it turns out that the reciprocal
ρ of the distance r from the center of the Sun to Mercury, as Mercury moves
on a geodesic with respect to the space-time metric produced by the Sun,
is closely approximated by a solution of the differential equation
d2ρ
dφ2 = γ − ρ + δρ2 (8.79)
where
γ := G0M(1 − G0(m+M)
ac2 )
G0(m + M)a(1 − e2)
, δ := 3G0M
c2 , (8.80)
M is the mass of the Sun, m is the mass of Mercury, G0 is the Newtonian
gravitational constant, a is the semimajor axis of the elliptical orbit of8.3 Nonautonomous Perturbations 513
Mercury, e is the eccentricity of the ellipse, and c is the speed of light
(see [155] and [259]). We will predict the precession of the perihelion of
Mercury from the differential equation (8.79).
Exercise 8.65. The constant γ in display (8.80) is usually given (see [259]) in
the form
γ := c2b2
G0M
where b := r2dθ/ds and s is the proper time along the orbit of Mercury. Show that
this formula agrees with the definition of γ in display (8.80). Hint: In relativity,
ds
dt = c

1 − v2
c2 ;
that is, the proper time is the arc length along the orbit with respect to the space￾time metric. Use the formula for the angular momentum given in Exercise 6.30.
Approximate the velocity v along the orbit by 2πa/T, where T is the orbital
period, and use Kepler’s third law, as in Exercise 6.30.
In view of the results in Section 6.2.2, especially the harmonic oscillator
model (6.35) for Kepler motion, the system (8.79) with α = 0 is exactly
the same as the model predicted from Newton’s theory. In fact, as we have
seen, this model predicts a fixed elliptical orbit for Mercury. We will see
that the perturbed orbit precesses.
The sizes of the parameters in equation (8.79) depend on the choice of
the units of measurement. Thus, it is not meaningful to say that α is a small
parameter. This basic problem is ubiquitous in applied mathematics. While
some authors do not worry about the units, there is only one correct way
to proceed: rescale the variables so that the new system is dimensionless.
For equation (8.79), if we define a new dependent variable η := ρ/γ, then
the differential equation is recast in the form
d2η
dφ2 + η =1+ η2 (8.81)
where the ratio  := δγ is dimensionless. To approximate , note that
 = 3GM/c2
a(1 − e2)
GM
G(m + M)

1 − G(m + M)
ac2

,
m is much smaller than M, and GM/(ac2) is small, to obtain
 ≈ 3GM
c2a(1 − e2)
, (8.82)
and use the physical constants
G0 = 6.668 × 10−11 m3
kg·sec2 , a = (387)(149, 598, 845)m,
c = 3 × 108 m
sec , mSun = (332700)(5.977) × 1024kg,
e = 0.206,
(8.83)514 8. Continuation of Periodic Solutions
reported in [108] to compute the approximation
 ≈ 7.973 × 10−8. (8.84)
The differential equation (8.81) has two rest points in the phase plane:
a center near the point with coordinates (1, 0), and a saddle near (1/, 0).
Moreover, the orbit corresponding to the perturbed motion of Mercury
corresponds to one of the periodic orbits surrounding the center (see Exer￾cise 8.66).
Exercise 8.66. Show that the phase portrait of the system (8.81) has exactly
two rest points: a saddle and a sink; approximate the positions of these rest
points with power series in , and show that the orbit of Mercury corresponds
to a periodic orbit. Note that it is not enough for this physical problem to prove
the result for “sufficiently small epsilon.” Rather, the value  = γδ must be
used! Hint: Initial conditions for the orbit of Mercury can be approximated from
the physical data. The level sets of the energy corresponding to the differential
equation (8.81) are invariant manifolds in the phase plane. In fact, one of them
forms the boundary of the period annulus.
How can we find a useful approximation of the perturbed periodic orbit
corresponding to the motion of Mercury? To answer this question, let us
view  as a parameter and observe that the differential equation (8.81) is
analytic. Thus, the periodic solution φ → η(φ, ) that we wish to approxi￾mate is given by an analytic function η of two variables. Also, this solution
is an analytic function of the initial conditions. Thus, the perturbed solu￾tion can be expanded as a convergent power series in ; at least this is true
if  is sufficiently small. We will come back to this problem in a moment.
For now, let us assume that there is a series expansion of the form
η(φ, ) = η0(φ) + η1(φ) + η2(φ)
2 + O(
3). (8.85)
A natural idea is to substitute the series (8.85) into the differential equa￾tion (8.81), and then try to solve for the unknown Taylor coefficients by
equating like powers of . In fact, if this is done, then (using dots to denote
derivatives with respect to φ) the order zero equation is
η¨0 + η0 = 1. (8.86)
Note that we have some freedom in the choice of initial data for the
solution of the differential equation (8.86). For example, if we consider the
system in the phase plane, then there is an interval on the η-axis that lies
to the right of the unperturbed rest point at (1, 0) and contains one of the
intersection points of our perturbed periodic orbit with the η-axis. In fact,
this interval can be chosen to be a Poincar´e section. Thus, we can suppose
that the desired periodic orbit corresponding to the solution φ → η(φ, )8.3 Nonautonomous Perturbations 515
starts at φ = 0 on this section at a point with coordinate 1 + b for some
b = b() > 0. In other words, for sufficiently small  > 0, we have the initial
conditions η(0, ) = 1+ b and ˙η(0, ) = 0. In particular, η0(0) = 1 + b,
η˙0(0) = 0, and the corresponding solution or the order zero differential
equation is
η0(φ)=1+ b cos φ.
Note that truncation at this order predicts elliptical motion for Mercury.
In fact, the zero order approximation is just the solution of the harmonic
oscillator model (6.35) of Kepler motion.
By using a trigonometric identity and some algebraic manipulation, the
first-order term in the series expansion of η is seen to be the solution of the
initial value problem
η¨1 + η1 =  1
β2 +
b2
2
	
+
2b
β2 cos φ +
b2
2
cos 2φ,
η1(0) = 0, η˙1(0) = 0, (8.87)
and, by an application of the variation of constants formula, the solution
of this initial value problem has the form
η1(φ) = c1 + c2φ sin φ + c3 cos 2φ
where c1, c2, and c3 are nonzero constants.
We now have a problem: The first-order approximation
η(φ) ≈ η0(φ) + η1(φ)
is not periodic. Indeed, because one Fourier mode of the forcing function in
the differential equation (8.87) is in resonance with the natural frequency of
the harmonic oscillator, the expression for η1(φ) contains the secular term
c2φ sin φ. Indeed, the function φ → c2φ sin φ is unbounded as φ → ∞.
The word “secular” means an event that occurs once in a century. The
inference is clear: Even if its coefficient is small, a secular term will eventu￾ally have arbitrarily large values. In particular, if there is a secular term in
an approximation with a finite number of terms, then the approximation
will not be periodic unless there is a fortuitous cancellation.
We started with a periodic function φ → η(φ, ), but the first-order term
in its series expansion in powers of the perturbation parameter  is not
periodic. How can this be?
As an example to illustrate the reason for the appearance of secular
terms, let us consider the harmonic oscillator with small detuning given by
u¨ + (1 + )
2u = 0
with the initial conditions u(0) = b and ˙u(0) = 0. For this example, we
have that
u(t, ) = b cos((1 + )t) = b cost − (btsin t) − 1
2
(bt2 cost)
2 + O(
3).516 8. Continuation of Periodic Solutions
Hence, even though the series represents a periodic function, every finite
order approximation obtained by truncation of the series is unbounded.
Clearly, these finite order approximations are not useful over long time
intervals. Also, note that the terms in this series expansion have the “wrong”
period. Whereas the solution is periodic with period 2π/(1+), the trigono￾metric terms on the right-hand side all have period 2π.
Lindstedt observed that secular terms appear in the series for a perturbed
periodic solution because the parameter-dependent frequency of the per￾turbed periodic orbit is not taken into account. He showed that the secular
terms can be eliminated if the solution and its frequency are simultaneously
expanded in powers of the perturbation parameter.
As an illustration of Lindstedt’s method, let us consider a perturbed
linear system of the form
u¨ + λ2u = f(u, u,  ˙ ) (8.88)
that has a family of periodic solutions t → u(t, ) with the initial conditions
u(0, ) = b and ˙u(0, ) = 0. In other words, the corresponding periodic orbits
in the phase plane all pass through the point with coordinates (b, 0). Also,
let us define the function ω given by  → ω() such that the frequency of
the periodic solution t → u(t, ) is ω().
Lindstedt introduces a new independent variable
τ = ω()t
so that the desired periodic solution t → u(t, ) is given by
u(t, ) = v(ω()t, )
where τ → v(τ, ) is the 2π-periodic solution of the initial value problem
ω2()v + λ2v = f(v, ω()v
, ), v(0, ) = b, v
(0, ) = 0 (8.89)
and v denotes the derivative of v with respect to τ .
Lindstedt’s computational method is the following: Write the 2π-periodic
function τ → v(τ, ) and the frequency  → ω() as series
v(τ, ) = v0(τ ) + v1(τ ) + v2(τ )
2 + ··· ,
ω() = λ + ω1 + ω2
2 + ··· ,
substitute these series into the differential equation (8.89), and then com￾pute the unknown coefficients recursively by equating the terms with like
powers of . Alternatively, the differential equations for the Taylor coeffi￾cients of v can be computed directly from the differential equation (8.89)
as variational equations.
To determine the order zero coefficient, set  = 0 in equation (8.89) to
see that v0 is the solution of the initial value problem
λ2(w + w)=0, w(0) = b, w
(0) = 0,8.3 Nonautonomous Perturbations 517
and therefore
v0(τ ) = b cos τ. (8.90)
Next, let us note that v1(τ ) = v(τ, 0). Hence, by differentiating both
sides of equation (8.89) with respect to  and evaluating at  = 0, the
function v1 is seen to be the solution of the initial value problem
λ2(w + w) = f(b cos τ, −λb sin τ, 0) + 2λω1b cos τ,
w(0) = 0, w
(0) = 0. (8.91)
Because the function τ → v(τ, ) is 2π-periodic independent of , so is the
function τ → v(τ, 0), and therefore the point (b, 0) is a continuation point
of periodic solutions in the phase plane for the (usual) first-order system
corresponding to the differential equation in display (8.91). By rescaling
and then applying Theorem 8.1 to this first-order system, it follows that
 2π
0
(f(b cos τ, −λb sin τ, 0) + 2λω1b cos τ ) sin τ dτ = 0.
Hence, the Fourier series for the function τ → f(b cos τ, −λb sin τ, 0), which
has the form
A0 + A1 cos τ + B1 sin τ + ∞
n=2
(An cos nτ + Bn sin nτ ),
must be such that B1 = 0. If we impose this condition and also choose
ω1 = A1/(2λb), then the forcing function on the right-hand side of the
linear system (8.91) has no resonant term. Thus, the corresponding solution
v1 contains no secular terms, and it is periodic with period 2π.
Using the second-order variational equation, Theorem 8.1, and an appro￾priate choice of ω2, all secular terms can be eliminated in the corresponding
linear system, and the function v2 is therefore periodic with period 2π. In
fact, this procedure can be repeated to determine all of the coefficients
of the Taylor series in  for the perturbed frequency ω() and the solu￾tion v. Moreover, it follows from our assumptions that the resulting series
converge.
The original periodic solution is represented by a series of form
u(t, ) = v(ω()t, ) = b cos(ω()t) + v1(ω()t) + O(
2) (8.92)
where v1 is determined above, and the frequency of the original periodic
solution is given by
ω() = λ +
A1
2λb  + O().
Let us note that because the series coefficients of the series (8.92) depend
on , the Lindstedt series expansion for u is not a Taylor series.518 8. Continuation of Periodic Solutions
If the Lindstedt procedure is carried out to some finite order—the only
possibility in most applied problems—then, to obtain an approximation
to the desired periodic solution, we must substitute a truncation of the
series for the frequency ω into a truncation of the Lindstedt series for the
periodic solution. This leads to the question “How well does the truncated
Lindstedt series approximate the original periodic solution?” The answer
for the case considered here is that the difference between the nth-order
truncation and the solution is O(n+1) on a time interval of length C/ for
some constant C > 0. See [189] for a careful treatment of order estimates
of this type.
For one-dimensional oscillators, the error estimate just mentioned for
Lindstedt series can be obtained from the associated Taylor series for the
same solution. The analysis for multidimensional differential equations is
more complicated. For example, for Hamiltonian perturbations of multi￾dimensional Hamiltonian systems, the Lindstedt series generally diverge!
This famous result of Poincar´e is very important in the history of mathe￾matics. The divergence of these series suggests that the underlying dynam￾ics must be very complex. In fact, this observation led Poincar´e to several
major results, for example, the discovery of chaotic dynamics in Hamilto￾nian dynamical systems (see [13], [23], [83], [164], [165], [180], [185], [186],
and [240]). On the other hand, Lindstedt series are useful for approximat￾ing the periodic solutions that are obtained as continuations of periodic
orbits of the type considered in this chapter. In fact, it is no accident that
Theorem 8.1 is used to obtain the Lindstedt series for the example ana￾lyzed above. The bifurcation functions (called the determining equations
in the context of Lindstedt series) can be used to obtain Lindstedt approx￾imations for the continued periodic solutions in each case that we have
discussed (see Exercise 8.68).
Let us return to the perihelion of Mercury.
To apply Lindstedt series to obtain an approximation for the precession
of perihelion, introduce new variables
v := η − 1, τ = ω()φ
into equation (8.81) so that
ω2()v + v = (1 + v)
2
(where v denotes dv/dτ ), and use equations (8.90) and (8.91) to show that
v0(τ ) = b cos τ and v1 is the solution of the initial value problem
w + w = 
1 +
b2
2
	
+ 2b(1 + ω1) cos τ +
b2
2
cos 2τ
with initial conditions w(0) = w
(0) = 0. Thus, following Lindstedt’s pro￾cedure, if ω1 := −1, then the secular terms are eliminated. In fact, in the
original variables we have
ρ(φ) = γ + bγ cos 
(1 − )φ
	
+ O(). (8.93)8.3 Nonautonomous Perturbations 519
Moreover, the lowest order truncation of the Lindstedt series (8.93) that
includes the relativistic correction yields the approximation
ρ(φ) ≈ γ + bγ cos 
(1 − )φ
	
. (8.94)
In view of equation (8.94), the distance r = 1/ρ of Mercury to the center
of the Sun is approximated by
r ≈ 1
γ

1 + b cos 
(1 − )φ
		 . (8.95)
Also, the perihelion for this elliptical orbit occurs when the argument of the
cosine is a multiple of 2π. Thus, if the orbit starts at perihelion at φ = 0,
then after one revolution it returns to perihelion when (1 − )φ = 2π, that
is, when φ has advanced by approximately
2π = 6πG0M
a(1 − e2)
radians from the unperturbed value φ = 2π.
Using the expression for Kepler’s third law in Exercise 6.30 and the
physical constants (8.83), the orbital period of Mercury is seen to be
T ≈ 7.596 × 106sec.
In other words, Mercury orbits the Sun approximately 414.9 times in a cen￾tury. Using the estimate for  in display (8.84), the orbital advance of the
perihelion per century is thus found to be 2.08 × 10−4 radians, or approxi￾mately 43 seconds of arc per century. (Can you imagine how Einstein must
have felt when he computed this number?)
Exercise 8.67. For the perturbed harmonic oscillator ¨u + u = u, the natural
frequency is “corrected” at first order in the perturbation parameter by ω() =
1 − . What is the first-order correction if the perturbation is u2 or u3? What
about un.
Exercise 8.68. Discuss the application of Lindstedt’s method to forced oscil￾lators. For example, find the first-order approximation for the solution(s) of the
forced oscillator
u¨ + u = (α cos(ωt) + bu3
).
Hint: Recall the theory in Section 8.3.2 for the continuation of periodic solutions
in an isochronous period annulus. In particular, recall that we expect to find
periodic solutions when the parameter ω is near a resonance, say ω()= 1+ ω1.
In this case, assume the value of the detuning ω1 is known, and look for solutions
(harmonics) with frequency ω. This search can be conducted within the geometry
of the stroboscopic Poincar´e map. Unlike the case of an autonomous perturbation;
here the frequency is known, but the initial position of the solution in the Poincar´e
section is not known. Rather, the initial position, the continuation curve, is a520 8. Continuation of Periodic Solutions
function of . This suggests the introduction of a new time variable τ = ω()t so
that we can look for periodic solutions with period 2π of the scaled differential
equation
ω2
()v + v = (α cos(τ ) − βu3
).
To apply the Lindstedt method, we must expand v(t, ) as a power series in  as
before, but, because the initial position of the periodic orbit is not known, we
must also expand the initial values v(0, ) and v
(0, ). The coefficients for these
series expansions of the initial data and the function v are to be determined by
equating coefficients. If
v(0, ) = ζ10 + ζ11 + O(
2
), v
(0, ) = ζ20 + ζ21 + O(
2
),
then the 2π-periodic zero order approximation is
v0(τ ) = ζ10 cos τ + ζ20 sin τ.
The values of ζ10 and ζ20 are determined at the next order. Compute the first￾order approximation, consider the condition required to make the approximation
2π-periodic, and compare your result with the bifurcation equations obtained at
the end of Section 8.3.2. Also, consider the form of the Lindstedt series in the
original time variable.
Exercise 8.69. Compute to at the least second order in the small parameter
the approximate period of the perturbed periodic orbit for the van der Pol oscil￾lator (8.3) (see [7] and [78]).
8.3.7 Entrainment Domains for van der Pol’s Oscillator
Consider the forced van der Pol oscillator in the form
x¨ + δ(x2 − 1) ˙x + x =  cos Ωt. (8.96)
We will use the formulas of Section 8.3.5 together with Lindstedt approx￾imations to estimate—because the unperturbed system is not explicitly
integrable—the widths of the entrainment domains for system (8.96).
For small δ, the second-order Lindstedt approximation for the solution
corresponding to the unperturbed limit cycle Γ is given by [253]
x(t) = 2 cos s + 3
4 sin s − 1
4 sin 3s
	
δ
+ 
− 1
8
cos s +
3
16 cos 3s − 5
96 cos 5s
	
δ2 + O(δ3) (8.97)
where s = (1−δ2/16+O(δ4))t, and the approximate period of the limit cycle
is τ := 2π(1 + δ2/16) + O(δ4). Moreover, these approximations are valid;
that is, the difference between the approximation and the exact solution is
bounded by a constant times δ3 on the time scale of one period of the limit
cycle Γ.8.3 Nonautonomous Perturbations 521
Recall the function C given in equation (8.76) and the formulas (8.77)
used to determine the width of the entrainment domains. To use these
formulas, let us approximate the extrema of the function C. This is accom￾plished by using the Lindstedt series (8.97) to approximate the phase plane
parameterization of Γ given by
θ → (x(t + θ), x˙(t + θ)).
If the resulting formulas are inserted into C and the terms of like order are
collected, then we obtain an approximation of the form
C(θ) ≈ c1(θ)δ + c2(θ)δ2.
This approximation vanishes unless m = n or m = 3n, a manifestation of
the resonances that appear in the approximation of the limit cycle as well
as the order of the approximation. At these resonances we have that
b(nτ )=1 − 2nπδ + 2n2π2δ2 + O(δ3).
Also, for m = n the function C is given by
C(θ) = −(n2π2 cos θ)δ +
1
8
n2π2(sin 3θ − 3 sin 5θ
+ sin θ + 8nπ cos θ + 4 sin θ cos 2θ + 6 sin θ cos 4θ)δ2 + O(δ3),
while for m = 3n it is given by
C(θ) = −1
8

n2π2 sin 3θ
	
δ2.
In order to approximate the extrema of C in case m = n, note that the
extrema of the function θ → C(θ)/δ at δ = 0 occur at θ = 0 and θ = π.
The perturbed extrema are then approximated using the series expansion
of the left-hand side of the equation C
(θ) = 0. In fact, for m = n we have
Cmin = −n2π2δ + n3π3δ2 + O(δ3), Cmax = n2π2δ − n3π3δ2 + O(δ3),
while for m = 3n we have
Cmin = −1
8
n2π2δ2 + O(δ3), Cmax = 1
8
n2π2δ2 + O(δ3).
By inserting these expressions into the formulas (8.77) for the tangent lines
of the entrainment domains at  = 0 we obtain for m = n the O(δ4)
approximation
 = ±

4 + 1
2
δ2	Ω − 
1 − 1
16
δ2		,
while for m = 3n we obtain the O(δ3) approximation
 = ±
32
3
δ−1 − 32nπ
3 +
4
3
δ − 4nπ
3
δ2	Ω − 3

1 − 1
16
δ2		.522 8. Continuation of Periodic Solutions
Of course, the accuracy of these computations can be improved and higher￾order resonances can be studied by starting with higher-order Lindstedt
approximations (see [7] and [78]). Also, the presence of the term containing
δ−1 in the slope of the tangent line for the (3 : 1) resonance indicates
that the entrainment domain has nearly vertical tangents for small δ, and
therefore this entrainment domain is very thin near the Ω-axis.
Exercise 8.70. Numerical values can be obtained from the approximation for￾mulas in this section. For example, if δ = 0.1 and (m : n) = (1 : 1), then the
tangents obtained from the Lindstedt series are approximately
 = ±4.005(Ω − 0.999).
Find the entrainment domain for this case using a numerical simulation of the van
der Pol oscillator, approximate the tangents to the entrainment domains using
the results of your simulation, and compare the results with the approximations
given by Lindstedt series (see [43]). Hint: Find the frequency ω of the unperturbed
van der Pol limit cycle using a numerical simulation. Set up a grid of (Ω, ) values
for Ω near ω and  near zero. Then, for each choice of these parameter values
set initial conditions near the intersection of the unperturbed limit cycle with
the x-axis, iterate the Poincar´e map several times, and test to see if the iterates
converge to a fixed point. If they do, assume that entrainment has occurred
and color the corresponding grid point. If no entrainment occurs, then leave the
corresponding grid point uncolored. The entrainment domain will emerge from
the display of the colored grid points.
8.4 Forced Oscillators
In this section we will apply our continuation theory to the oscillator
x¨ + h(x, x˙) ˙x + f(x) = g(t) (8.98)
where the function t → g(t), the external force, has period 2π/Ω. As usual,
we will consider the differential equation (8.98) as the first-order system
x˙ = y, y˙ = −f(x) + (g(t) − h(x, y)y), (8.99)
and we will assume that the unperturbed system
x˙ = y, y˙ = −f(x) (8.100)
has a period annulus A containing a resonant periodic solution Γm/n whose
period 2π/ω is in (m : n) resonance with the period of g. Also, we will
assume that the period function on A has a nonzero derivative at Γm/n.8.4 Forced Oscillators 523
Under the assumptions stated above, we have proved that the simple
zeros of the function φ → M(φ) given by
M(φ) =  n2π/ω
0
y(t + φ, ξ)

g(t) − h(x(t + φ, ξ), y(t + φ, ξ))y(t + φ), ξ)
	
dt
(8.101)
are the continuation points for (m : n) (ultra)subharmonics. Here φ may
be viewed as a coordinate on Γm/n and ξ is a point on Γm/n that defines an
origin for the coordinate φ. For simplicity, we choose ξ to lie on the x-axis.
Note that the integrand of the integral used to define M is periodic with
period n2π/ω. If we suppress the variable ξ and change the variable of
integration to s = t + φ, then
M(φ) =  φ+n2π/ω
φ

y(s)g(s − φ) − h(x(s), y(s))y2(s)
	
ds.
The function
θ →
 θ+n2π/ω
θ

y(s)g(s − φ) − h(x(s), y(s))y2(s)
	
ds
is constant for each fixed value of φ. Thus, we can represent the bifurcation
function in the following convenient form:
M(φ) =  n2π/ω
0

y(s)g(s − φ) − h(x(s), y(s))y2(s)
	
ds = I1(φ) + I2,
where
I1(φ) :=  n2π/ω
0
y(s)g(s − φ) ds, I2 :=  n2π/ω
0
h(x(s), y(s))y2(s) ds.
The function t → (x(−t), −y(−t)) is a solution of the unperturbed differ￾ential equation with initial value at the point (ξ, 0). Thus, by the uniqueness
of solutions, x is an even function and y is an odd function of time.
Because s → y(s) is an odd function, we have the Fourier series
y(s) = ∞
k=1
yk sin kωs, g(s) = g0 +∞
k=1
gc
k cos kΩs +∞
k=1
gs
k sin kΩs
where all coefficients are real. With these representations, it is easy to
compute
I1(φ) =∞
k=1
∞
=1
ykgs

 n2π/ω
0
sin(kωs) sin(Ω(s − φ)) ds
+∞
k=1
∞
=1
ykgc

 n2π/ω
0
sin(kωs) cos(Ω(s − φ)) ds.524 8. Continuation of Periodic Solutions
Moreover, taking into account the resonance relation Ω = mω/n and apply￾ing the change of variables θ = ωs/n, we have
I1(φ) = n
ω
∞
k=1
∞
=1
ykgs

 2π
0
sin(nkθ) sin(mθ − Ωφ)
+
n
ω
∞
k=1
∞
=1
ykgc

 2π
0
sin(nkθ) cos(mθ − Ωφ).
The integrals in the last formula vanish unless k = mj and  = nj for some
integer j > 0. Thus, we obtain a simplification that yields the formula
I1(φ) = nπ
ω
∞
j=1
ymjgs
nj cos(njΩφ) +∞
j=1
ymjgc
nj sin(njΩφ)
	
= nπ
ω
∞
j=1
ymjgs
nj cos(mjωφ) +∞
j=1
ymjgc
nj sin(mjωφ)
	
.
In particular, φ → I1(φ) is a 2π/(mω)-periodic function.
To simplify I2, let us note that the corresponding integrand is 2π/ω￾periodic, and therefore
I2 = n
 2π/ω
0
h(x(s), y(s))y2(s) ds.
We are interested in the simple zeros of M on the interval 0 ≤ φ < 2π/ω.
Let us note that the graph of I1(φ) over this interval repeats m times
since φ → I1(φ) is 2π/(mω)-periodic. The constant I2 simply translates
the graph of I1(φ) to the graph of M. Also, if I1(φ) − I2 has k zeros on
0 ≤ φ < 2π/(mω), then M has mk zeros.
Generally, a periodic function has an even number of zeros over one
period. Hence, generally, there is a nonnegative integer N such that k = 2N,
and M has an even number of zeros. Thus, we expect 2mN (ultra)subhar￾monics to continue from a given resonant periodic solution. It is important
to note that this number will be large if m is large. In this regard, let us
note that if the period of the unperturbed orbit Γm/n is large, then there
are resonances with m large.
In order for I1 to be nonzero, the forcing must contain some Fourier
modes that are the same as the modes present in the derivative y of the
unperturbed solution corresponding to the periodic orbit Γm/n. It is not
clear how to determine which modes are present in this unperturbed solu￾tion without solving the differential equation. But, because y is an odd
function, we might expect all odd modes to be present.
Under the assumption that I1 is not zero, M has zeros provided that I2
is not too large in absolute value. In effect, I2 serves to translate the graph
of the periodic function I1 in a vertical direction. This suggests that if the8.4 Forced Oscillators 525
damping is too large, then there will be no periodic solutions that continue
from the resonant unperturbed periodic orbits. In fact, there is a delicate
relationship between the amplitude of I1 and the magnitude of I2 that is
required to determine the global dynamics. The precise relationship that is
required must be obtained from each choice of the model equation.
Example 8.71. Consider the damped periodically forced oscillator
x¨ + αx + f(x) = β cos Ωt.
Whereas gc
1 = β, all other Fourier modes vanish. Thus, on a resonant
unperturbed orbit, if we use the notation of this section, we must have
n = 1 and j = 1. In fact, we have
I1(φ) = π
ω
ymβ sin(mωφ), I2 = α
 2π/ω
0
y2(s) ds,
M(φ) = π
ω
ymβ sin(mωφ) − α|y|
2
2
where the norm is the L2-norm. Note that M has simple zeros if and only
if
0 <
 πymβ
ω|y|
2
2α
−1
< 1.
In particular, if ym = 0 and if the ratio α/β is sufficiently small, then there
are 2m zeros.
To determine the number and positions of the continuable periodic orbits,
we must determine the resonant periodic orbits in the period annuli of the
unperturbed system (8.100); that is, we must determine the behavior of the
period function associated with the given period annulus. While the period
function must have a nonzero derivative at a resonant unperturbed periodic
orbit to apply our first-order continuation theory, it is sometimes possible
to determine if a resonant periodic orbit is continuable even in the case
where the period function vanishes at this orbit (see [41] and [213]). But,
the more basic problem of finding the critical points of a period function
is non-trivial even for system (8.100) (see the survey [220], and also [48]
and [49]).
Note that system (8.100) has all its rest points on the x-axis. If these rest
points are all nondegenerate (their linearizations have nonzero eigenvalues),
then the rest points will be either hyperbolic saddle points or centers. To
prove this fact, recall that system (8.100) has a first integral. Indeed, if
we view the differential equation ¨x + f(x) = 0 as a model equation for a
nonlinear spring, then we know that its total energy
H(x, y) = 1
2
y2 + F(x)526 8. Continuation of Periodic Solutions
where
F(x) :=  x
0
f(s) ds
is a first integral. Here the choice F(0) = 0 is arbitrary; the addition of a
constant to H just redefines the “potential energy.” Also, note that H is
constant on the trajectories of the differential equation (8.100).
Without loss of generality, suppose that system (8.100) has a rest point
at the origin. By our choice of the energy, H(0, 0) = 0. Also, since f(0) = 0,
we also have that Hx(0, 0) = 0. By the assumption that the rest point is
nondegenerate, we have Hxx(0, 0) = f
(0) = 0 and so
H(x, y) = 1
2
y2 + f
(0)
2
x2 + O(x3).
More generally, suppose H : Rn → R. We say that H has a singularity
at 0 ∈ Rn if H(0) = 0 and grad H(0) = 0. The singularity is called non￾degenerate if det(Hess H(0)) = 0 where Hess H is the n × n matrix with
components
∂2H
∂xi∂xj
(x1,...,xn), i = 1, . . . , n, j = 1, . . . , n.
Theorem 8.72 (Morse’s Lemma). If H : Rn → R has a nondegenerate
singularity at the origin, then there is a smooth function h : Rn → Rn such
that h(0) = 0, det Dh(0) = 0, and
H(h(x1,...,xn)) = n
i,j=1
∂2H
∂xi∂xj (0)xi
xj .
Informally, Morse’s lemma states that there is a nonlinear change of coor￾dinates at a nondegenerate singular point of a scalar-valued function that
transforms it into the quadratic form determined by its Hessian evaluated
at the singular point. See [178] for an elementary proof of Morse’s lemma.
The best proof uses more sophisticated machinery (see [2]). Also see Ex.
8.74 for an important corollary.
Exercise 8.73. Prove Morse’s lemma for n = 1. Hint: Reduce to the case where
the function H is given by H(x) = x2f(x) and f(0) > 0. Define g(x) = x
f(x)
and prove that there is a function h such that g(h(x)) = x. The desired change
of coordinates is given by x = h(z).
Exercise 8.74. Suppose that H : Rn → R has a nondegenerate singularity at
the origin. Show that Hess H(0) has n real eigenvalues
λ2
1,...,λ2
k, −λ2
k+1,..., −λ2
n8.4 Forced Oscillators 527
where λi = 0 for i = 1,...,n. The number n − k is called the index of the
singularity. Prove the following corollary of the Morse lemma: There is a change
of coordinates h : Rn → Rn such that
H(h(x1,...,xn)) = k
i=1
λ2
i x2
i − n
i=k+1
λ2
i x2
i .
For system (8.100), it follows from the Morse lemma that there is a
new coordinate system near each rest point such that the orbits of the
system (8.100) all lie on level curves of the conic y2 + f
(0)x2. There are
only two cases: If f
(0) > 0, then the origin is a center, and if f
(0) < 0,
then the origin is a hyperbolic saddle.
Each center is surrounded by a period annulus A. Let us suppose that
there are rest points on the boundary of A. In this case, there are either
one or two hyperbolic saddle points on the boundary; the remainder of the
boundary is composed of the stable and unstable manifolds of these saddle
points. Because there are rest points on the boundary of A, the corre￾sponding period function grows without bound as its argument approaches
the boundary of A. In particular, the period annulus contains an infinite
number of resonant periodic orbits, and among these there are orbits with
arbitrarily large periods. Also, the period function approaches 2π/
f
(0),
the period of the linearization of the system at the origin, as its argument
approaches the origin. Thus, there is at least one unperturbed periodic
solution with each preassigned period in the interval (2π/
f
(0),∞). Let
us also note that if the period function is not an increasing function, then
there may be more than one unperturbed orbit in A with the same period.
Also, if there is a rest point on the outer boundary of the period annulus,
then the frequency of the resonant periodic orbits approaches zero as the
resonant orbits approach the boundary.
Since the rational numbers are a dense subset of R and since the reso￾nance relation has the form n2π/ω = m2π/Ω, there are infinitely many res￾onant periodic solutions in each subannulus containing two periodic orbits
with different periods. In particular, a period annulus whose boundary con￾tains rest points has a subannulus with this property. Thus, it should be
clear that if the unperturbed system (8.100) has a period annulus contain￾ing periodic orbits with different periods, if the derivative of the period
function does not vanish on each resonant orbit in the annulus, and if the
damping is sufficiently small, then there will be a large number of perturbed
(ultra)subharmonics.
Are there infinitely many (ultra)subharmonics? Our analysis does not
answer this question. To see why, recall our main result: If the function
M in display (8.101) has simple zeros along an (m : n) resonant unper￾turbed periodic orbit, then for sufficiently small  there are 2mN per￾turbed (ultra)subharmonics where N is some positive integer. But, if we
consider an infinite number of resonant orbits, for example, a sequence of528 8. Continuation of Periodic Solutions
periodic orbits that approach the boundary of our period annulus, then
it might happen that the infinite number of requirements for  to be suf￾ficiently small cannot be satisfied simultaneously without taking  = 0.
For this reason, we cannot conclude that there is an  > 0 such that
the corresponding perturbed system has infinitely many periodic solutions
even if (ultra)subharmonics continue from all resonant unperturbed peri￾odic orbits. Thus, we are left with evidence that oscillators with an infinite
number of (ultra)subharmonics exist, but we have no proof.
If an oscillator has an infinite number of hyperbolic periodic orbits of
saddle type all contained in some compact subset of its extended phase
space, then we might expect the dynamical behavior of the oscillator in
a neighborhood of this set to be very complex: orbits in the neighbor￾hood might tend to follow a stable manifold, pass by a saddle point, follow
the motion on its unstable manifold, pass near another stable manifold,
and then repeat the process. Whatever the exact nature of such a flow,
it should be clear that we cannot hope to understand the dynamics of
oscillators without considering this possible behavior. It turns out that by
using some new ideas introduced in Chapter 9 we will be able to show that
some periodically perturbed oscillators do indeed have an infinite number
of (ultra)subharmonics and that their flows are “chaotic.”9
Homoclinic Orbits,
Melnikov’s Method, and Chaos
In the last chapter, we discussed the near resonance continuation theory for
periodic orbits of periodically perturbed oscillators. For the case where the
unperturbed oscillator has a regular period annulus, we found that there is
generally an infinite number of resonances at which a first-order perturba￾tion theory can be used to prove the existence of perturbed periodic orbits.
But, as mentioned previously, we cannot conclude from the results of our
analysis that the perturbed oscillator has infinitely many periodic orbits.
To do so would seem to require a condition that might be impossible to
satisfy. Indeed, the nonzero amplitude of the perturbation would have to be
made sufficiently small for each of an infinite sequence of continuations cor￾responding to an infinite sequence of resonant unperturbed periodic orbits
that approaches the boundary of a period annulus. The subject of this
chapter is a perturbation theory that is valid at the boundary of the period
annulus. When the theory is applied, the amplitude of the perturbation is
required to be sufficiently small only once.
Generally, the boundary of a period annulus for an unperturbed oscil￾lator consists of one or more saddle points connected by homoclinic or
heteroclinic orbits. Let us define a saddle connection to be an orbit whose
α- and ω-limit sets are hyperbolic saddle points. A saddle connection is
called a homoclinic orbit if its α- and ω-limit sets coincide. On the other
hand, the saddle connection is called a heteroclinic orbit if its α-limit set
is disjoint from its ω-limit set.
If the saddle points on the boundary of our period annulus are hyper￾bolic, then they persist along with their stable and unstable manifolds.
For simplicity, let us consider the case where there is just one hyperbolic
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
C. Chicone, Ordinary Differential Equations with Applications, Texts in
Applied Mathematics 34, https://doi.org/10.1007/978-3-031-51652-8 9
529530 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
Figure 9.1: A homoclinic loop.
Figure 9.2: Possible phase portraits of a planar system after perturbation
of a system with a homoclinic orbit.9. Homoclinic Orbits, Melnikov’s Method, and Chaos 531
Figure 9.3: A homoclinic loop bifurcation: A periodic orbit appears after
a perturbation that breaks a homoclinic loop.
saddle point p on the boundary of our period annulus such that this rest
point is “connected to itself” by a homoclinic orbit as in Figure 9.1. If the
perturbation is autonomous, then the portions of the perturbed stable and
unstable manifolds at p that form the homoclinic loop either coincide or
separate into one of the two configurations depicted in Figure 9.2. For a
periodic nonautonomous perturbation, we will consider the corresponding
(stroboscopic) Poincar´e map. The saddle point p is a fixed (or periodic)
point for the unperturbed Poincar´e map and the homoclinic orbit lies on
the invariant stable and unstable manifolds of p. After perturbation, the
perturbed stable and unstable manifolds can coincide, split, or cross. The
main problem addressed in this chapter is the determination of the relative
positions of the perturbed invariant manifolds for both the autonomous
and nonautonomous cases.
For autonomous perturbations, the splitting of saddle connections is
important because it is related to the existence of limit cycles. For example,
suppose that the perturbed configuration of stable and unstable manifolds
is as depicted in the right-hand panel of Figure 9.2. If the perturbation of
the rest point at the inner boundary of the unperturbed period annulus
is a source and the perturbation is sufficiently small, then no additional
rest points appear; and, by the Poincar´e-Bendixson theorem, there must
be at least one-periodic orbit “inside” the original homoclinic loop (see
Figure 9.3).
For the case of a periodic perturbation, the most interesting case occurs
when the perturbed stable and unstable manifolds of the Poincar´e map
cross. For the case of a homoclinic loop, a point of intersection is called a
transverse homoclinic point for the Poincar´e map if the stable and unstable
manifolds meet transversally, that is, the sum of their tangent spaces at
the crossing point is equal to the tangent space of the two-dimensional
Poincar´e section at this point. (There is an analogous concept for transverse
heteroclinic points.)532 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
Figure 9.4: Part of a homoclinic tangle for the stable and unstable mani￾folds of a saddle fixed point of a Poincar´e map.
If there is a transverse homoclinic point, then, by a remarkable theorem
called the Smale-Birkhoff theorem, there is a nearby “chaotic invariant set.”
A weak version of this theorem states that if there is a transverse homo￾clinic point, then the perturbed Poincar´e map has infinitely many unstable
periodic points in a small neighborhood of the unperturbed homoclinic
loop. But even more is true: There is a compact invariant set that contains
these periodic points and also infinitely many nonperiodic solutions that
“wander as randomly as a sequence of coin tosses” in the vicinity of the
boundary of the original period annulus [229]. Moreover, the trajectories
of solutions starting in this invariant set are “sensitively dependent” on
their initial conditions; that is, no matter how close we take their initial
conditions, the corresponding points on two different trajectories will be at
least half of the diameter of the invariant set apart at some finite future
time. The existence of such an invariant set is what we mean when we
say the system is chaotic (see, for example, the mathematical references
[119], [185], [218], [229], [231], and [261], as well as the general references
[15], [83], [121], and [164]).
Although the proof of the existence of “chaotic dynamics” in the presence
of a transverse homoclinic point requires several new ideas that we will
not discuss here, it is very easy to see why the existence of a transverse
homoclinic point must lead to complicated dynamics. Note first that the
forward iterates of a transverse homoclinic point, themselves all transverse
homoclinic points, must approach the corresponding saddle point along
its stable manifold. Because these points also lie on the unstable manifold9. Homoclinic Orbits, Melnikov’s Method, and Chaos 533
of the same saddle point, the unstable manifold must stretch and fold as
shown schematically in Figure 9.4. This homoclinic tangle is responsible
for the existence of a chaotic invariant set.
The chaotic invariant sets in the homoclinic tangle are similar to hyper￾bolic saddle points in the sense that these chaotic invariant sets have both
stable and unstable manifolds. Thus, roughly speaking, many solutions
of the corresponding differential equation (which have their initial points
near one of these chaotic invariant sets) will approach the chaotic invari￾ant set along the direction of the stable manifold on some long time scale,
but eventually these trajectories leave the vicinity of the chaotic invariant
set along the direction of the stable manifold. Such an orbit will exhibit
transient chaos. This is what usually happens if the differential equation
is not conservative. On the other hand, for Hamiltonian systems where
the dimension of the phase space is not more than four (for a mechani￾cal system this means that there are not more than two degrees of free￾dom), these “transient orbits” are often constrained to some neighborhood
of the original homoclinic loop. In this case, they continually revisit the
chaotic invariant sets obtained from the transverse homoclinic points and
they exhibit chaotic effects for all time. Finally, there are dissipative sys￾tems that contain “chaotic attractors,” compact chaotic invariant sets that
attract all nearby orbits. These chaotic sets are not necessarily associated
with transverse homoclinic points. Chaotic attractors are poorly under￾stood. For example, it is generally very difficult to prove the existence of
a chaotic attractor for a system of differential equations. On the other
hand, it is not at all difficult to “see” a chaotic attractor using numerical
simulations (see Exercise 9.1).
We will show how to detect the splitting of saddle connections by defin￾ing a function that determines the separation of the perturbed invariant
manifolds as a function of the bifurcation parameters. It turns out that the
appropriate function is the limit of the subharmonic Melnikov function as
the base points on the periodic orbits approach the boundary. For exam￾ple, for the case of the forced oscillator (8.99) where we have defined the
subharmonic Melnikov function M in display (8.101), the function we seek
is the limit of M as ξ approaches a point on the boundary that is not a rest
point. The limit function, again denoted by M, is again called the Melnikov
function. In fact, the Melnikov function for the differential equation (8.99)
is given by
M(φ) =  ∞
−∞
y(t + φ, ξ)

g(t) − h(x(t + φ, ξ), y(t + φ, ξ))y(t + φ, ξ)

dt
or, after the obvious change of variables, by
M(φ) =  ∞
−∞
y(s, ξ)

g(s − φ) − h(x(s, ξ), y(s, ξ))y(s, ξ)

ds534 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
where ξ is a base point on the boundary and the coordinate φ specifies
position on the boundary.
For an autonomous perturbation, the Melnikov function does not depend
on the initial point for the unperturbed solution on the boundary. In this
case the sign of the Melnikov function determines the direction in which the
invariant manifolds split. For a time periodic perturbation, the Melnikov
function does depend on the initial point on the boundary, and its sim￾ple zeros correspond to positions where the perturbed stable and unstable
manifolds intersect transversally for sufficiently small  = 0.
The derivation and analysis of the Melnikov function for autonomous per￾turbations is of course a special case of its derivation for nonautonomous
perturbations. Since the analysis for autonomous perturbations is concep￾tually simpler, we will give a detailed discussion of this case first.
Exercise 9.1. Write a report on numerical simulations of the Lorenz system
x˙ = σ(y − x), y˙ = ρx − y − xz, z˙ = −βz + xy
(see the original paper of Edward N. Lorenz [166] or any book on dynamical
systems theory). Start by setting the parameter values β = 8
3 , ρ = 28, and
σ = 10. Choose an initial condition in the first quadrant, for instance near the
unstable manifold of the saddle point at the origin, integrate forward in time,
and display the resulting approximate orbit using three-dimensional graphics.
The “Lorenz butterfly attractor” will appear. Also graph one of the observables,
say t → y(t), and compare the time series you obtain with the graph of a periodic
function. Choose a second initial condition near the initial condition you started
with and plot together the two simulated graphs of t → y(t). Note that these
graphs will stay close together for a while (as they must due to the smoothness
of solutions with respect to initial conditions), but eventually they will diverge.
In fact, after a transient, the evolution of the two solutions will appear to be
completely unrelated. For this reason, it is impossible to predict the position
of the state vector from the initial conditions over long time-periods with an
accuracy that is small compared with the diameter of the attractor; the evolution
of the system is extremely sensitive to changes in the initial conditions. This is
the signature of a chaotic flow.
Exercise 9.2. Consider the system
x˙ = y, y˙ = −ax + yz, z˙ = b − y2
(see [136]). (a) Suppose that b = 0. Show that there are no rest points and no
trapping region containing the origin. Note: A trapping region U is an open set
contained in some finite ball such that every solution that starts in U stays in
the closure of U for all forward time. (Thus, it is difficult to know where to
begin an analysis.) (b) Set a = 1 and b = 0. Describe the phase portrait in
words and drawings with evidence provided by rigorous analysis. In particular,
discuss the existence and nature of two-dimensional invariant manifolds. Prove
that there are no periodic orbits. Determine the forward ω-limit set of the point
(1/
√3, 1/
√3, 1/
√3). (c) Set a = 0 and b = 1. Describe the phase portrait in9.1 Autonomous Perturbations: Separatrix Splitting 535
words and drawings with evidence provided by rigorous analysis. In particular
describe the evolution of a typical orbit starting at (0, η, 0) in case η > 0, η = 0,
and η > 0. (d) Numerical simulation suggests complicated dynamics when a > 0
and b > 0. Formulation and proof of a descriptive theorem is a challenge. A useful
result would be to prove that there is a forward bounded orbit. At least this would
prove that there is a nontrivial omega-limit set. Write a (not necessarily rigorous)
explanation of the main feature of the dynamics, in contrast to the case a = 0,
that suggests the observed complications.
9.1 Autonomous Perturbations:
Separatrix Splitting
Consider the planar system
u˙ = f(u, λ), u ∈ R2, λ ∈ Rn (9.1)
and let ξ0 ∈ R2 be a regular point for the unperturbed system
u˙ = f(u, 0). (9.2)
As usual, let t → u(t, ξ, λ) denote the solution of the differential equa￾tion (9.1) such that u(0, ξ, λ) = ξ, define f ⊥(u) = Rf(u, 0) where
R := 0 −1
1 0 
,
and let t → Ψ(t, ξ) denote the flow of the orthogonal system ˙u = f ⊥(u).
Here, of course, t → Ψ(t, ξ0) is transverse to t → u(t, ξ0, 0) at ξ0.
Define
Σ := {Ψ(t, ξ0) : t ∈ R}, (9.3)
and suppose that we have devised some construction that produces two
families of solutions of the differential equation (9.1), each parametrized by
λ, whose members are all transverse to Σ such that at λ = 0 the corre￾sponding solutions coincide with the unperturbed solution t → u(t, ξ0, 0).
Our objective is to obtain some information about the rate of separation
of the solutions belonging to the two parametrized families of solutions. In
fact, we will obtain a general conclusion about this separation rate follow￾ing the presentation given by Stephen Schecter [228]. This result will then
be used to address the problem of breaking saddle connections.
Suppose that our construction produces two smooth functions ρi : Rn →
R, i = 1, 2, given by λ → ρi
(λ) such that ρi
(0) = 0 where ρi
(λ) gives the
point of intersection of the ith family with Σ. We desire information about
the separation of the two solution families of the differential equation (9.1)
given by
γi
(t, λ) := u(t, Ψ(ρi
(λ), ξ0), λ), i = 1, 2. (9.4)536 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
Let us view these families as “variations” of the unperturbed solution; that
is, γi is a family of solutions containing the unperturbed solution at λ = 0
γi
(t, 0) = u(t, ξ0, 0), i = 1, 2.
Also, γi has initial value on the transverse section Σ. In fact,
γi
(0, λ) = Ψ(ρi
(λ), ξ0).
The separation of the variations from the unperturbed solution is defined
by the function λ → ρ1(λ) − ρ2(λ); it measures a signed distance between
the points where our variations cross Σ. Of course, in a perturbation prob￾lem, it is unlikely that we will be given the functions ρ1 and ρ2 explicitly.
At best, we will be able to infer their existence (probably by an application
of the implicit function theorem). Nonetheless, for small λ, a good approx￾imation of the separation is given by the separation function sep : Rn → R
defined by
sep(λ) := Ψ(ρ1(λ), ξ0) − Ψ(ρ2(λ), ξ0), f ⊥(ξ0)
= f(ξ0, 0) ∧ (Ψ(ρ1(λ), ξ0) − Ψ(ρ2(λ), ξ0)).
Let us note that, sep(0) = 0. Also, sep(λ) = 0 if and only if the solutions
γ1(t, λ) and γ2(t, λ) are identical. This last fact follows because a solution
of an initial value problem is unique.
As usual, we can determine the local nature of S := {λ : sep(λ)=0}
provided that there is at least one j = 1,...,n such that sepλj (0) = 0. In
fact, if this condition holds, then (by the implicit function theorem) S is a
surface of dimension n − 1 passing through 0 ∈ Rn whose normal vector at
this point is just grad(sep)(0).
What have we done so far? In analogy with our continuation theory for
periodic solutions, we have defined a function akin to the displacement
function and we have reduced the study of its zero set to an application
of the implicit function theorem. Let us make this reduction useful by
identifying the partial derivatives of the separation function.
To identify the partial derivatives of the separation function using the
original differential equation (9.1), we expect to solve a variational equa￾tion. But to obtain a nontrivial variational equation we must have some
time dependence in the separation function. This requirement motivates
the definition of the time-dependent separation function S : R × Rn → R
given by
S(t, λ) := γ1(t, λ) − γ2(t, λ), f ⊥(ϕt(ξ0))
= f(ϕt(ξ0), 0) ∧ (γ1(t, λ) − γ2(t, λ))
where ϕt is the flow of the system (9.2). Since S(0, λ) = sep(λ), the main
idea—originally due to Melnikov—is to compute the desired partial deriva￾tives of the separation function sep from the corresponding partial deriva￾tives of the time-dependent separation function S.9.1 Autonomous Perturbations: Separatrix Splitting 537
Let us define two auxiliary functions
Si
(t, λ) := γi
(t, λ), f ⊥(ϕt(ξ0))
= f(ϕt(ξ0), 0) ∧ γi
(t, λ)
for i = 1, 2, and note that S(t, λ) = S1(t, λ) − S2(t, λ). To compute the
required partial derivatives, start with the formula
Si
λj (t, 0) = f(ϕt(ξ0), 0) ∧ γi
λj (t, 0), (9.5)
and use the solution t → γi
(t, λ) of the differential equation (9.1) to obtain
the variational equation
γ˙ i
λi (t, 0) = fu(ϕt(ξ0), 0)γi
λj (t, 0) + fλj (ϕt(ξ0), 0). (9.6)
Next, define A(t) := fu(ϕt(ξ0), 0) and use equation (9.5) to obtain the
differential equation
S˙ i
λj (t, 0) = fu(ϕt(ξ0), 0)f(ϕt(ξ0), 0) ∧ γi
λj (t, 0) + f(ϕt(ξ0), 0) ∧ γ˙ i
λj (t, 0)
= A(t)f(ϕt(ξ0), 0) ∧ γi
λj (t, 0) + f(ϕt(ξ0), 0) ∧ A(t)γi
λj (t, 0)
+ f(ϕt(ξ0), 0) ∧ fλj (ϕt(ξ0), 0). (9.7)
Formula (9.7) can be simplified by an application of the following easily
proved proposition from vector analysis: If A is a 2×2 matrix and v, w ∈ R2,
then
Av ∧ w + v ∧ Aw = (tr A)v ∧ w.
In fact, with the aid of this proposition, we have
S˙ i
λj (t, 0) = div f(ϕt(ξ0), 0)f(ϕt(ξ0), 0) ∧ γi
λj (t, 0)
+ f(ϕt(ξ0), 0) ∧ fλj (ϕt(ξ0), 0)
= div f(ϕt(ξ0), 0)Si
λj (t, 0) + f(ϕt(ξ0), 0) ∧ fλj (ϕt(ξ0), 0). (9.8)
The differential equation (9.8) is a linear variational equation for the
function t → Si
λj (t, 0). To solve it, let us assume that we know the behavior
of γ1(t, 0) as t → −∞ and the behavior of γ2(t, 0) as t → ∞. If we define
K(t) := e−  t
0 div f(ϕt(ξ0),0) ds
and integrate both sides of the differential equation
d
dt

K(t)Si
λj (t, 0)
= K(t)f(ϕt(ξ0)) ∧ fλj (ϕt(ξ0), 0),
then we obtain the identities
S1
λj (0, 0) = K(t)S1
λj (t, 0) +  0
t
K(s)f(ϕs(ξ0)) ∧ fλj (ϕs(ξ0), 0) ds,
−S2
λj (0, 0) = −K(t)S2
λj (t, 0) +  t
0
K(s)f(ϕs(ξ0)) ∧ fλj (ϕs(ξ0), 0) ds.538 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
Note that the right-hand side of each of these identities is constant with
respect to t. Using this fact, the desired partial derivative is given by
sepλj (0) = lim t→−∞ 
K(t)f(ϕt(ξ0)) ∧ γ1
λj (t, 0)
+
 0
t
K(s)f(ϕs(ξ0)) ∧ fλj (ϕs(ξ0), 0) ds
+ limt→∞ 
− K(t)f(ϕt(ξ0)) ∧ γ2
λj (t, 0)
+
 t
0
K(s)f(ϕs(ξ0)) ∧ fλj (ϕs(ξ0), 0) ds
. (9.9)
We reiterate that the indicated limits exist because the quantities in square
brackets are constants. Of course, the summands of each expression in
square brackets are not necessarily constants.
The representation (9.9) of the partial derivatives of the separation func￾tion is useful because it is general. We will return to the main topic of this
section and apply this result to the perturbation of saddle connections.
Suppose that ξ0 denotes a point on a saddle connection for system (9.2)
connecting the hyperbolic saddle points p0 and q0 (maybe p0 = q0); that
is,
lim t→−∞ ϕt(ξ0) = p0, limt→∞ ϕt(ξ0) = q0.
Also, let Σ denote the section at ξ0 defined in display (9.3). By the implicit
function theorem, if λ is sufficiently small, then there are perturbed hyper￾bolic saddle points
pλ = p0 + O(λ), qλ = q0 + O(λ)
for the system (9.1). Define t → γ1(t, λ) to be the solution of the sys￾tem (9.1) with initial condition on Σ (as in equation (9.4)) that lies on
the unstable manifold of pλ, and let t → γ2(t, λ) denote the corresponding
solution on the stable manifold of the hyperbolic saddle point qλ. By The￾orem 7.1, the stable and unstable manifolds γi
, i = 1, 2, intersect the fixed
curve Σ. To see this, add the equation λ˙ = 0 to the system (9.1) and use
the smoothness of the center stable manifold corresponding to each rest
point of the augmented system corresponding to the saddle points p0 and
q0.
We will outline a proof of the following proposition:
Proposition 9.3. If system (9.1) with λ = 0 has a saddle connection and
if γ1 and γ2, as in display (9.4), are defined to be solutions on the unstable
and stable manifolds of the perturbed saddle points, then
lim t→−∞ K(t)f(ϕt(ξ0)) ∧ γ1
λj (t, 0) = 0, (9.10)
limt→∞ K(t)f(ϕt(ξ0)) ∧ γ2
λj (t, 0) = 0. (9.11)9.1 Autonomous Perturbations: Separatrix Splitting 539
Moreover,
sepλj (0) =  ∞
−∞
e−  t
0 div f(ϕs(ξ0),0) dsf(ϕt(ξ0), 0) ∧ fλj (ϕt(ξ0), 0) dt. (9.12)
The important formula (9.12) for the partial derivatives of the separation
function with respect to the parameters was probably known to Poincar´e.
It was also discovered independently by several different authors (see, for
example, [175], [217], and [234]). In spite of this history, the integral is now
most often called the Melnikov integral.
Since sep(0) = 0, the Taylor series of the separation function at λ = 0 is
sep(λ) = 	n
j=1
λj sepλj (0) + O(|λ|
2). (9.13)
In particular, if n = 1 and  := λ1, then
sep() = (sep(0) + O()). (9.14)
Therefore, if sep(0) = 0 and if || is sufficiently small, then formula (9.12)
can be used to determine the sign of sep(), and therefore the splitting
direction of the perturbed stable and unstable manifolds relative to the
direction determined by f ⊥(ξ0).
An outline for a proof of the limit (9.11) will be given; a proof for the
limit (9.10) can be constructed similarly.
View the vector field f as a mapping f : R2 × Rn → R2. Since f(q0, 0) =
0 and since fu(q0, 0) : R2 → R2 is a nonsingular linear transformation
(it has no eigenvalue on the imaginary axis in the complex plane by the
hyperbolicity of q0), the implicit function theorem implies there is a map
q : Rn → R2 defined near λ = 0 such that q(0) = q0 and f(q(λ), λ) ≡ 0. By
the continuous dependence of the eigenvalues of a matrix on its coefficients,
we have that q(λ) is a hyperbolic saddle point for |λ| sufficiently small.
As mentioned above, the stable manifold of q(λ) varies smoothly with
λ by Theorem 7.1. In particular, the function (t, λ) → γ2(t, λ) depends
smoothly on t and λ, and limt→∞ γ2(t, λ) = q(λ). The matrix fu(q0, 0)
has two real eigenvalues −μ1 < 0 < μ2. Moreover, as t → ∞ the curve
t → γ2(t, 0) approaches the saddle point q0 tangent to the eigenspace cor￾responding to the eigenvalue −μ.
By an affine change of coordinates, if necessary, we may as well assume
that q0 is located at the origin and the unperturbed differential equation
u˙ = f(u, 0) has the form
x˙ = −μx + f1(x, y), y˙ = νy + f2(x, y) (9.15)
where both μ and ν are positive constants, and the functions f1 and f2
together with their first-order partial derivatives vanish at the origin. In540 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
these coordinates, the stable manifold of the hyperbolic saddle point at the
origin is given by the graph of a smooth function h : U → R where U ⊂ R
is an open interval containing the origin on the x-axis. Moreover, because
the stable manifold is tangent to the x-axis at the origin, we have h(0) = 0
and h
(0) = 0.
The estimate that we will use to compute the limit (9.11) is the content
of the following proposition.
Proposition 9.4. If |x0| is sufficiently small, then there is a positive con￾stant c such that the solution of the system (9.15) starting at (x0, h(x0))
satisfies the estimate
|x(t)| + |y(t)| ≤ ce−μt, t ≥ 0.
The next lemma (compare Theorem 7.1) will be used to prove Proposi￾tion 9.4.
Lemma 9.5. If t → x(t) is the solution of the initial value problem
x˙ = −μx + g(x), x(0) = x0,
where μ > 0 and g : R → R is a smooth function such that g(0) = 0 and
g
(0) = 0, then there are constants  > 0 and c > 0 such that |x(t)| ≤ ce−μt
for t ≥ 0 whenever |x0| < .
Proof. The function defined by
G(x) := 
x−2g(x), x = 0
0, x = 0
is continuous at x = 0. Thus, there is some constant C such that |G(x)| ≤ C
for sufficiently small |x|.
For x = 0, we have
− x˙
x2 − μ
x = −G(x).
If y := 1/x, then ˙y − μy = −G(x(t)) and
e−μty(t) = y(0) −
 t
0
e−μsG(x(s)) ds.
Thus, we have the estimate
|e−μty(t)|≥|y(0)| −  t
0
e−μs|G(x(s)| ds.
For sufficiently small |x0|,
|y(0)|≤|e−μty(t)| + C
 t
0
e−μs ds. (9.16)9.1 Autonomous Perturbations: Separatrix Splitting 541
To prove this last inequality, use the following simple proposition: if |x0|
is sufficiently small, then |x(t)| < |x0| for t ≥ 0. It is proved by observing
that the point x = 0 is an attracting rest point for our one-dimensional
differential equation. A weaker assumption would also be sufficient. For
example, it suffices to assume that for |x0| sufficiently small, the interval
(−|x0|, |x0|) is positively invariant. This follows immediately by considering
the direction of the vector field corresponding to our differential equation
at the end points of the appropriately chosen interval.
After an elementary integration, inequality (9.16) states that
|y(0)|≤|e−μty(t)| +
C
μ (1 − e−μt).
Moreover, because t ≥ 0, it follows that
|y(0)|≤|e−μty(t)| +
C
μ ,
and therefore
1
|x0|
− C
μ
≤
1
eμt|x(t)|
.
If |x0| > 0 is sufficiently small, then
1
|x0|
− C
μ
>
1
c
> 0
for some c > 0. Thus, we have that
|x(t)| ≤ ce−μt.
✷
Exercise 9.6. Under the same hypotheses as in Lemma 9.5, prove that
limt→∞ e
μtx(t)
exists and is not equal to zero (see [228]).
Let us prove Proposition 9.4.
Proof. Consider the change of coordinates for the system (9.15) given by
p = x, q = y − h(x).
In these coordinates, the saddle point stays fixed at the origin, but the sta￾ble manifold is transformed to the p-axis. The restriction of the transformed
differential equation to the p-axis is given by
p˙ = −μp + f1(p, h(p)). (9.17)542 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
If g(p) := f1(p, h(p)), then all the hypotheses of the lemma are satisfied,
and we conclude that there is some |p0| = 0 such that solutions of the
differential equation (9.17) satisfy |p(t)| ≤ ce−μt for some c > 0 and all
t ≥ 0 whenever |p(0)| < |p0|. In the original coordinates, the corresponding
solution on the stable manifold is given by x(t) = p(t), y(t) = h(x(t)).
Since y = h(x) is tangent to the x-axis at x = 0, there is a constant c1 > 0
such that |h(x)| < c1x2 for |x| sufficiently small. Thus, if the initial value
of the solution of the differential equation (9.15) on the stable manifold is
sufficiently close to the origin, then there is a positive constant c such that
|x(t)| + |y(t)| = |x(t)| + |h(x(t))|≤|x(t)|(1 + c1|x(t)|) ≤ ce−μt. ✷
To conclude our discussion of the limit (9.11), we must analyze the
asymptotic behavior of the functions K(t), f(ϕt(ξ0), 0), and γ2
λj (t, 0). Let
us note first that since f(u, 0) is Lipschitz, we have
||f(u, 0)|| = ||f(u, 0) − f(0, 0)|| ≤ L||u||
for some constant L > 0. By the proposition,
||f(ϕt(ξ0), 0)|| ≤ Lce−μt.
Likewise, using the smoothness of u → f(u, 0), we have
div f(u, 0) = tr fu(0, 0) + R(u)
where, for sufficiently small ||u||, there is a constant c2 > 0 such that the
remainder R satisfies ||R(u)|| ≤ c2||u||. Thus
K(t) = e−  t
0 ν−μ due−  t
0 R(u(s)) ds
≤ e(μ−ν)t
ec2
 t
0 ce−µs ds
≤ c3e(μ−ν)t
for some c3 > 0. It follows that
limt→∞ K(t)f(ϕt(ξ0), 0) = 0.
To complete the argument, we will show that |γ2
λj (t, 0)| is bounded. For
this, we use the smoothness of the stable manifold with respect to the
parameter λ. There is no loss of generality if we assume that the hyperbolic
saddle q(λ) remains at the origin with its stable manifold tangent to the
x-axis as in system (9.15). Indeed, this geometry can be achieved by a
parameter-dependent affine change of coordinates. More precisely, there is
a smooth function (x, λ) → h(x, λ) defined near (x, y) = (0, 0) such that
the stable manifold is the graph of the function x → h(x, λ). Of course, we9.1 Autonomous Perturbations: Separatrix Splitting 543
also have that h(0, λ) ≡ 0 and hx(0, λ) ≡ 0. Using this representation of
the stable manifold,
γ2(t, λ)=(x(t, λ), h(x(t, λ), λ))
where t → x(t, λ) is a solution of a differential equation
x˙ = −μx + g(x, λ)
similar to differential equation (9.17). After differentiation, we find that
γ2
λj (t, 0) = (xλj (t, 0), hx(x(t, 0), 0)xλj (t, 0) + hλj (x(t, 0), 0)).
By the smoothness of the function h, both hx(x, 0) and hλj (x, 0) are
bounded for x in some fixed but sufficiently small interval containing x = 0.
Thus, the boundedness of the function t → γ2
λj (t, 0) will be proved once we
show that t → xλj (t, 0) is bounded as t → ∞. To obtain this bound, note
that the function t → xλj (t, 0) is a solution of the variational equation
w˙ = −μw + gx(x(t, 0), 0)w + gλj (x(t, 0), 0). (9.18)
Because gx(0, 0) = 0 and the function g is smooth, we have the estimate
|gx(x, 0)| ≤ c1|x|
for some c1 > 0. In addition, since g(0, λ) ≡ 0, it follows that gλj (0, 0) = 0.
Also, by the smoothness of g, the partial derivative gλj is locally Lipschitz.
In fact, there is some c2 > 0 such that
|gλj (x, 0)| = |gλj (x, 0) − gλj (0, 0)| ≤ c2|x|.
With the obvious choice of notation, the differential equation (9.18) has
the form
w˙ = (−μ + α(t))w + β(t) (9.19)
and the solution
w(t) = e−μte
 t
0 α(s) ds
w(0) +  t
0
eμse−  s
0 α(τ) dτβ(s) ds
.
By Proposition 9.4, there is a constant c3 > 0 such that
|α(t)| ≤ c3e−μt, |β(t)| ≤ c3e−μt,
for t ≥ 0. Also, let us note that
 t
0
|α(s)| ds ≤
c3
μ (1 − e−μt) <
c3
μ .544 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
Thus, we obtain the following growth estimate for the solution of the dif￾ferential equation (9.19):
|w(t)| ≤ e−μtec3/μ|w(0)| + e−μtec3/μc3ec3/μt.
In particular, |w(t)| is bounded for t ≥ 0. This completes the proof.
As an application of our result on the splitting of separatrices, let us
consider the damped van der Pol oscillator
x¨ + (x2 − 1) ˙x + x − c2x3 = 0
where c > 0 and  is a small parameter. If, as usual, we define ˙x = y, then
the energy for the unperturbed system is given by
H(x, y) = 1
2
y2 +
1
2
x2 − 1
4
c2x4.
The unperturbed Hamiltonian system
x˙ = y, y˙ = −x + c2x3
has a pair of hyperbolic saddle points at (x, y)=(±1/c, 0) and a center at
the origin surrounded by a regular period annulus. The boundary of the
period annulus is a pair of heteroclinic orbits of the unperturbed system
that both lie on the curve with energy 1/(4c2).
The Melnikov integral has the form
M =
 ∞
−∞
y2(1 − x2) dt.
Using the equality ˙x/y = 1 and the energy relation, let us note that the
time parameter on the heteroclinic orbits is given by
t =
 x
0
 1
2c2 − σ2 +
c2
2 σ4−1/2 dσ.
After integration, this fact yields the solution
x(t) = 1
c tanh(t/√
2 ), y(t) = 1
c
√2
sech2(t/√
2 ),
and the formula
M = 1
2c2
 ∞
−∞
sech4(t/√
2 )(1 − 1
c2 tanh2(t/√
2 )) dt.
This elementary integral can be evaluated using the substitution u =
tanh(t/√2 ) to obtain the value
M = 2
√2
15c2

5 − 1
c2

.9.2 Periodic Perturbations: Transverse Homoclinic Points 545
If, for example, c2 < 1
5 , then M < 0 and both heteroclinic orbits break.
If in addition  > 0 is sufficiently small, then the system will have a limit
cycle surrounding the origin. (Why?)
Exercise 9.7. Discuss the splitting of saddle connections for the damped Duff￾ing equation
x¨ + x − x + c
2
x3 = 0.
Does the perturbed system have limit cycles?
Exercise 9.8. A heteroclinic orbit of a planar Hamiltonian system does not
persist under a general (autonomous) Hamiltonian perturbation. Prove that a
homoclinic loop of a planar Hamiltonian system persists under (autonomous)
Hamiltonian perturbation. Determine the fate of the heteroclinic orbits in the
phase plane for the mathematical pendulum when it is perturbed in the family
˙
θ = v, v˙ = − sin θ + 
as  varies in the closed unit interval. Repeat the exercise for the perturbed
pendulum system viewed as a family on the phase cylinder?
9.2 Periodic Perturbations:
Transverse Homoclinic Points
In this section we will consider periodic perturbations of a planar Hamil￾tonian oscillator
x˙ = Hy(x, y), y˙ = −Hx(x, y) (9.20)
whose phase portrait has a homoclinic loop as depicted in Figure 9.1. Our
main objective is to prove that if the Melnikov function defined on the
homoclinic loop has simple zeros, then the periodically perturbed oscillator
has transverse homoclinic points.
There are at least two reasons for the unnecessary restriction to unper￾turbed Hamiltonian systems. First, because Hamiltonian vector fields are
divergence free, the Liouville factor
e−  t
0 div f(ϕt(ξ0),0) ds
is constant. Therefore, the expression for the Melnikov integral is simplified
(see, for example, formula (9.9)). The second reason is the recognition that
for the most important applications of the theory, the unperturbed systems
are Hamiltonian.
To avoid writing the components of system (9.20), let us define the vector
ν = (x, y) and, with a slight abuse of notation, the function
f(ν) := (Hy(ν), −Hx(ν))546 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
so that differential equation (9.20) has vector form
ν˙ = f(ν).
Also, let us suppose that g : R2 × R × R → R2 is a function given by
(ν, t, ) → g(ν, t, ) that is 2π/Ω-periodic in t. The corresponding periodi￾cally perturbed oscillator is given in vector form by
ν˙ = f(ν) + g(ν, t, ),
and in component form by
x˙ = Hy(x, y) + g1(x, y, t, ),
y˙ = −Hx(x, y) + g2(x, y, t, ). (9.21)
Let us denote the flow of the unperturbed Hamiltonian system (9.20) by
ϕt, the homoclinic loop at the hyperbolic saddle point ν0 for the unper￾turbed system (9.20) by Γ, and the solution of the perturbed system (9.21)
by t → V (t, ν, ) where ν ∈ R2 and V (0, ν, ) ≡ ν. Also, as usual, let us
define the (stroboscopic) parametrized Poincar´e map P : R2 × R → R2 by
P(ν, ) := V (2π/Ω, ν, ).
Finally, the Melnikov function M : Γ → R for the perturbed oscilla￾tor (9.21) is defined by
M(ζ) :=  ∞
−∞
f(ϕt(ζ)) ∧ g(ϕt(ζ), t, 0) dt (9.22)
where, of course, f ∧ g := f1g2 − g1f2.
The main result of this section on the existence of transverse homoclinic
points is stated in the following theorem.
Theorem 9.9. If || is sufficiently small, then the parametrized Poincar´e
map for the system (9.21) has a hyperbolic saddle fixed point ν() such that
ν() = ν0 +O(). If ζ0 is a simple zero of the Melnikov function M defined
on Γ and || = 0 is sufficiently small, then the corresponding Poincar´e map
(at this value of ) has a transverse homoclinic point relative to the stable
and unstable manifolds of the hyperbolic fixed point ν(). If, on the other
hand, M has no zeros and || = 0 is sufficiently small, then the stable and
unstable manifolds of ν() do not intersect.
For the applications of Theorem 9.9, it is often convenient to work with a
local coordinate on the homoclinic loop Γ. In fact, if we choose some point
z on Γ, then the homoclinic orbit is parametrized by the corresponding
solution of the differential equation, for example, by  → ϕ−	(z). Thus, the
function M : R → R defined by
M() := M(ϕ−	(z)) =  ∞
−∞
f(ϕt−	(z)) ∧ g(ϕt−	(z), t, 0) dt (9.23)9.2 Periodic Perturbations: Transverse Homoclinic Points 547
is a local representation of the Melnikov function. Moreover, by the change
of variables σ := t − , we also have the useful identity
M() =  ∞
−∞
f(ϕσ(z)) ∧ g(ϕσ(z), σ + , 0) dσ. (9.24)
As an important example, let us consider the first-order system equiva￾lent to the periodically forced pendulum
¨θ + λ sin θ = a sin Ωt
on the phase cylinder, that is, the system
˙
θ = v,
v˙ = −λ sin θ + a sin Ωt (9.25)
where θ is an angular coordinate modulo 2π. The unperturbed phase cylin￾der system has a hyperbolic saddle point with coordinates (θ, v)=(π, 0)
and two corresponding homoclinic loops. Moreover, the unperturbed sys￾tem is Hamiltonian with respect to the total energy
H(θ, v) := 1
2
v2 − λ cos θ,
and both homoclinic loops lie on the energy surface in the phase cylinder
corresponding to the graph of the energy relation
v2 = 2λ(1 + cos θ).
Using the equality (1/v)dθ/dt = 1, the energy relation, and the identity
1 + cos θ = 2 cos2(θ/2), we have the unperturbed system restricted to the
upper homoclinic orbit given by the scalar differential equation
2
√
λ = 1
cos(θ/2)
dθ
dt .
If we impose the initial condition θ(0) = 0, then the initial value problem
has the elementary implicit solution
1
2
ln 1 + sin(θ/2)
1 − sin(θ/2)

= √
λ t,
or equivalently the solution
t → θ(t) = 2 arcsin(tanh(√
λ t)).
The corresponding solution of the pendulum equation
θ = 2 arcsin(tanh(√
λ t)) = 2 arctan(sinh(√
λ t)),
v = 2√
λ sech(√
λ t) (9.26)548 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
with the initial condition (θ, v) = (0, 2
√
λ ) is easily determined by substi￾tution of θ(t) into the energy relation or by differentiation of the function
t → θ(t) with respect to t.
In view of the solution (9.26) on the upper homoclinic loop, the Melnikov
function (9.24) for the periodically forced pendulum is given by
M() := 2a
√
λ
 ∞
−∞
sech(√
λ σ) sin(Ω(σ + )) dσ.
By using the trigonometric identity for the sine of the sum of two angles
and by observing that the function σ → sech(√
λ σ) sin(Ωσ) is odd, the
formula for M can be simplified to the identity
M()=2a
√
λ sin(Ω)
 ∞
−∞
sech(√
λ σ) cos(Ωσ) dσ
where the value of the improper integral is given by
 ∞
−∞
sech(√
λ σ) cos(Ωσ) dσ = π
√
λ
sech  πΩ
2
√
λ

. (9.27)
The function M has infinitely many simple zeros given by

 = mπ
Ω : m ∈ Z

.
Thus, by Theorem 9.9, the Poincar´e map for the system (9.25) has trans￾verse homoclinic points. (Treat yourself to an aesthetic experience. Find a
few quiet hours, sit alone, avoid all computer algebra systems, review the
elements of complex analysis, and then use the residue calculus to com￾pute the value of the improper integral (9.27). Pure as light, let Cauchy’s
theorem, a crown jewel of nineteenth-century mathematics, shine within.)
In preparation for the proof of Theorem 9.9, let us recast the differential
equation (9.21) as the first-order system on the phase cylinder R2×T given
by
x˙ = Hy(x, y) + G1(x, y, τ, ),
y˙ = −Hx(x, y) + G2(x, y, τ, ),
τ˙ = Ω (9.28)
where τ is an angular variable modulo 2π and
Gi(x, y, τ, ) := gi(x, y, τ /Ω, )
for i = 1, 2. Also, let us note that the corresponding vector form of sys￾tem (9.28) is
V˙ = f(V ) + G(V,τ, ),
τ˙ = Ω. (9.29)9.2 Periodic Perturbations: Transverse Homoclinic Points 549
Hyperbolic periodic oribit
τ
y
x
Homoclinic manifold
Figure 9.5: Phase portrait for the system (9.30) on the phase cylinder.
The homoclinic manifold is the cylinder over the homoclinic loop of the
corresponding planar Hamiltonian system.
The unperturbed system
x˙ = Hy(x, y),
y˙ = −Hx(x, y),
τ˙ = Ω (9.30)
has a two-dimensional homoclinic manifold S corresponding to the homo￾clinic loop of the corresponding planar Hamiltonian system as sketched in
Figure 9.5. Note that the original hyperbolic saddle point of the planar
Hamiltonian system corresponds to a hyperbolic periodic orbit γ of sys￾tem (9.30) that has two-dimensional stable and unstable manifolds, denoted
Ws(γ) and Wu(γ), respectively. Moreover, the homoclinic manifold is con￾tained in Ws(γ) ∪ Wu(γ).
To obtain a coordinate system on the homoclinic manifold, let us recall
that the local coordinate on the homoclinic loop is given by the function
 → ϕ−	(z) where z is fixed in Γ. The manifold S is parametrized in the
same manner. In fact, if p ∈ S, then there is a unique point (, τ ) ∈ R × T
such that
p = (ϕ−	(z), τ ).
In other words, the map
(, τ ) → (ϕ−	(z), τ )
is a global chart whose image covers the manifold S.550 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
We are interested in the fate of the homoclinic manifold for  = 0. The
first observation is that the periodic orbit γ is continuable for sufficiently
small || and its continuation is a hyperbolic periodic orbit γ() with a
two-dimensional stable manifold Ws(γ()) and a two-dimensional unstable
manifold Wu(γ()). The persistence of γ, and hence the first statement of
Theorem 9.9, follows easily from the results of Chapter 8. The existence of
the perturbed stable and unstable manifolds follows from results similar to
those in Chapter 7. In fact, the existence of the perturbed invariant mani￾folds can be proved from the existence of invariant manifolds for the hyper￾bolic fixed point of the perturbed Poincar´e map. The Hartman-Grobman
theorem for diffeomorphisms in Chapter 7 can be used to obtain the exis￾tence of continuous invariant manifolds at the hyperbolic fixed point of the
Poincar´e map corresponding to the hyperbolic saddle point ν0. The proof of
the smoothness of these invariant sets is analogous to the proof of smooth￾ness given in Chapter 7 for the invariant stable and unstable manifolds at
a hyperbolic rest point of a differential equation.
We will prove a version of Theorem 9.9 that takes into account the geom￾etry of the homoclinic manifold. The formulation of this result requires an
extension of the Melnikov function (9.23) to a function, also denoted by
the symbol M, that is defined on the homoclinic manifold S by
M(, τ ) :=  ∞
−∞
f(ϕt−	(z)) ∧ G(ϕt−	(z), Ωt + τ, 0) dt. (9.31)
The statement in Theorem 9.9 concerning the existence of a transverse
homoclinic point is an easy consequence of the following result.
Theorem 9.10. If there is a point in S with coordinates (, τ ) such that
M(, τ )=0, M	(, τ ) = 0,
and if || = 0 is sufficiently small, then the stable manifold Ws(γ()) and
the unstable manifold Wu(γ()) intersect transversally at a point in the
phase cylinder O() close to the point (ϕ−	(z), τ ).
A point p of transversal intersection of the stable and unstable manifolds
of the hyperbolic periodic orbit γ in the phase cylinder corresponds to a
point of transversal intersection of the stable and unstable manifolds of
the corresponding hyperbolic fixed point of the perturbed Poincar´e map.
In fact, the corresponding point of transversal intersection on the Poincar´e
section may be taken to be the first intersection of the orbit through p with
the Poincar´e section.
The proof of Theorem 9.10 will require some additional notation and two
lemmas.
Let us measure the splitting of the stable and unstable manifolds relative
to the unperturbed homoclinic manifold S. To be precise, note first that9.2 Periodic Perturbations: Transverse Homoclinic Points 551
L(p)
Wu(γ( ))
Splitting Distance
Ws(γ( ))
Figure 9.6: Perturbed stable and unstable manifolds. The splitting distance
is computed with respect to the lines in the direction of the normals to the
homoclinic manifold.
there is a natural choice for a normal vector at each point p := (ϕ−	(z), τ ) ∈
S, namely the vector
N(, τ )=(ϕ−	(z),τ,η(), 0)
with base point (ϕ−	(z), τ ) and the first component of its principal part is
given by
η() := DH(ϕ−	(z)) = (Hx(ϕ−	(z)), Hy(ϕ−	(z))).
Of course, the tangent space to S at the point p is generated by the two
vectors
(Hy(ϕ−	(z)), −Hx(ϕ−	(z)), 0), (0, 0, 1)
where the base point is suppressed and the last component is in R, the
tangent space to the circle T at the point with angle τ . Note that both of
these basis vectors are orthogonal to the vector N(, τ ) with respect to the
usual inner product of R3.
The unperturbed stable and unstable manifolds are transverse to the line
L(p) through the point p on S with direction vector N(, τ ). Thus, for a
small perturbation, the perturbed stable and unstable manifolds must also
intersect L(p) transversally (see Figure 9.6). The idea is to use the distance
between the intersection points of the perturbed invariant manifolds and552 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
the line L(p) as a measure of the distance between the perturbed mani￾folds at the point p ∈ S. But there is a problem: The perturbed invariant
manifolds might intersect the line more than once, perhaps even an infinite
number of times. Thus, it is not clear which intersection points to choose
in order to measure the distance at p between the perturbed stable and
unstable manifolds.
Suppose that ps() is a point on L(p)∩Ws(γ()), and pu() is a point on
L(p) ∩ Wu(γ()). Also, recall that the point p depends on the coordinates
 and τ . If, in components relative to the phase cylinder,
ps()=(zs(, τ, ), τ ), pu()=(zu(, τ, ), τ ),
then there are corresponding solutions of the perturbed system (9.28) given
by
t → (V s(t, zs(, τ, ), ), τ + Ωt), t → (V u(t, zu(, τ, ), ), τ + Ωt).
Of course, the solution corresponding to ps() is in the (invariant) stable
manifold Ws(γ()) and the solution corresponding to pu() is in the unsta￾ble manifold Wu(γ()). There is one choice for ps() among all points in
L(p) ∩ Ws(γ()) such that the corresponding solution
t → (V s(t, zs(, τ, ), ), τ + Ωt), (9.32)
does not intersect L(p) for all t > 0. Likewise, there is one choice for pu()
among all points in L(p) ∩ Wu(γ()) such that the corresponding solution
t → (V u(t, zu(, τ, ), ), τ + Ωt), (9.33)
does not intersect L(p) for all t < 0. In other words, these solutions are,
respectively, the “last” intersection point of the perturbed stable manifold
and the “first” intersection of the perturbed unstable manifold with the
line L(p). While it is intuitively clear that these special intersection points
exist, this fact can be proved (see, for example, [261, p. 495]). At any rate,
let us use these special intersection points to measure the distance between
the perturbed stable and unstable manifolds.
Lemma 9.11. If p ∈ S and || is sufficiently small, then the first com￾ponents of the solutions (9.32) and (9.33) corresponding to the last inter￾section point ps() and the first intersection point pu() on L(p) have the
following representations:
V s(t, zs(, τ, ), ) = ϕt−	(z) + rs(t) + O(
2), t ≥ 0,
V u(t, zu(, τ, ), ) = ϕt−	(z) + ru(t) + O(
2), t ≤ 0 (9.34)
where the functions rs : (0,∞) → R2 and ru : (−∞, 0) → R2 given by
rs(t) = V s
 (t, zs(, τ, 0), 0) and ru(t) = V u
 (t, zu(, τ, 0), 0) are bounded on
the indicated infinite time intervals.9.2 Periodic Perturbations: Transverse Homoclinic Points 553
Proof. We will prove the result for the solutions on the stable manifold;
the result for the unstable manifold is similar. Also, we will suppress the
variables  and τ by using the notation
V s(t, ) := V s(t, zs(, τ, ), ), V u(t, ) := V u(t, zu(, τ, ), ).
The basic estimate required to prove the lemma is obtained with an
application of Gronwall’s inequality (Lemma 2.1). Fix  and τ . Also, recall
that t → V s(t, ) is a solution of the differential equation
V˙ = F(V, t, ) := f(V ) + G(V, t, ),
and t → ϕt−	(z) is a solution of the differential equation V˙ = F(V, t, 0).
By integration, we have that
V s(t, ) − zs(, τ, ) =  t
0
F(V s(σ, ), σ, ) dσ,
ϕt−	(z) − ϕ−	(z) =  t
0
F(ϕσ−	(z), σ, ) dσ. (9.35)
Both solutions belong to the projection to the V -plane of a stable manifold
of a periodic orbit in the phase cylinder. Thus, both solutions for t ≥ 0 lie
in a compact subset K of the plane. By the smoothness of the function F,
there is a Lipschitz constant C1 > 0 such that
|F(V1, t, ) − F(V1, t, 0)| ≤ C(|V1 − V2| + ||)
for Vi, i = 1, 2 in K and || sufficiently small. Also, by the smoothness of
the stable manifold with respect to , if || is sufficiently small, then there
is a constant C2 > 0 such that
|zs(, τ, ) − ϕ−	(z)| ≤ C2. (9.36)
If we subtract the equations in display (9.35) and use the inequalities
just mentioned, then we obtain the estimate
|V s(t, ) − ϕt−	(z)| ≤ C2 + C1t + C1
 t
0
|V s(σ, ) − ϕσ−	(z)| dσ.
Hence, by an application of Gronwall’s inequality,
|V s(t, ) − ϕt−	(z)| ≤ (C2 + C1t)eC1t
. (9.37)
Recall that ν() denotes the perturbed hyperbolic saddle point and ν0
the hyperbolic saddle point for the planar Hamiltonian system. By a simple
application of the implicit function theorem, it follows that
ν() = ν0 + O().554 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
Since the solutions in the inequality (9.37) belong to the respective stable
manifolds of ν() and ν0, there is some constant C3 > 0 and some T > 0
such that if t>T, then
|V s(t, ) − ϕt−	(z)| ≤ C3. (9.38)
Therefore, if || is sufficiently small, then, by the Gronwall estimate (9.37)
for 0 ≤ t ≤ T and the estimate (9.38) for t>T, there is a constant C > 0
such that
|V s(t, ) − ϕt−	(z)| ≤ C (9.39)
for all t ≥ 0.
Because the solution V is a smooth function of the parameter , there is
a smooth remainder R such that
V s(t, ) = ϕt−	(z) + rs(t) + 
2R(t, ).
Thus, using the inequality (9.39), we have that
|rs(t) + R(t, )| = |V s(t, ) − ϕt−	(z)| ≤ C.
Finally, let us divide this estimate by  and then set  = 0 to obtain the
desired result: |rs(t)| ≤ C for t ≥ 0. ✷
Let us define the distance between the perturbed stable and unstable
manifolds at p = (ϕ	(z), τ ) to be
sep(, τ, ) := pu
 − ps
 , N(, τ )
|N(, τ )|
= zu(, τ, ) − zs(, τ, ), η()
|η()|
= DH(ϕ−	(z))(zu(, τ, ) − zs(, τ, ))
|η()| (9.40)
Because sep(, τ, 0) ≡ 0, we have the representation
sep(, τ, ) = sep(, τ, 0) +  sep(, τ, 0) + O(
2)
= (sep(, τ, 0) + O()). (9.41)
Also, by differentiation with respect to  in equation (9.40), it follows that
the leading-order coefficient of the separation function is given by
sep(, τ, 0) = M¯ (, τ )
|DH(ϕ−	(z))| (9.42)
where
M¯ (, τ ) := DH(ϕ−	(z))(V u
 (0, zu(, τ, 0), 0) − V s
 (0, zs(, τ, 0), 0)). (9.43)
In particular, up to a normalization, M¯ (, τ ) is the leading coefficient in
the expansion (9.41).9.2 Periodic Perturbations: Transverse Homoclinic Points 555
Lemma 9.12. The function M¯ defined in display (9.43) is equal to the
Melnikov function defined in display (9.31); that is, if a point on the homo￾clinic manifold is given in coordinates by (, τ ), then
M¯ (, τ ) = M(, τ ).
Proof. (The proof of this lemma is similar to the proof of Proposition 9.3.)
Define the time-dependent Melnikov function
m(t, , τ ) := DH(ϕt−	(z))(V u
 (t, 0) − V s
 (t, 0))
where  and τ are suppressed as in the proof of Lemma 9.11, and note that
m(0, , τ ) = M¯ (, τ ). Also, define two more auxiliary functions ms and mu
by
ms(t, , τ ) = DH(ϕt−	(z))V s
 (t, 0), mu(t, , τ ) = DH(ϕt−	(z))V u
 (t, 0)
so that m(t, , τ ) = mu(t, , τ ) − ms(t, , τ ). If m∗ denotes either mu or ms,
and likewise V ∗ denotes V s or V u, then
m˙ ∗(t, , τ ) = D2H(ϕt−	(z))[f(ϕt−	(z)), V ∗
 (t, 0)]
+ DH(ϕt−	(z))V˙ ∗
 (t, 0). (9.44)
Let us also recall that t → V ∗(t, ) is defined to be a solution of the sys￾tem (9.21); that is,
V˙ ∗ = f(V ∗) + G(V ∗, Ωt + τ, ). (9.45)
By differentiation of equation (9.45) with respect to  at  = 0 we obtain
the second variational equation
V˙ ∗
 = Df(ϕt−	(z))V ∗
 + G(ϕt−	(z), Ωt + τ, 0). (9.46)
Let us substitute the expression for V˙ ∗
 given by equation (9.46) into the
differential equation (9.44) and rearrange the terms to obtain
m˙ ∗(t, , τ ) = DH(ϕt−	(z))G(ϕt−	(z), Ωt + τ, 0) + B(t)V ∗
 (t, 0) (9.47)
where B(t) is the linear transformation of R2 given by
B(t)V := D2H(ϕt−	(z))[f(ϕt−	(z)), V ]
+ DH(ϕt−	(z))Df(ϕt−	(z))V. (9.48)
Also, by differentiating both sides of the identity
DH(ξ)f(ξ) ≡ 0
with respect to ξ ∈ R2, let us observe that
D2H(ξ)f(ξ) + DH(ξ)Df(ξ) ≡ 0.556 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
Thus, it follows that B(t) ≡ 0, and the differential equation (9.47) for m∗
reduces to
m˙ ∗(t, , τ ) = DH(ϕt−	(z))G(ϕt−	(z), Ωt + τ, 0). (9.49)
By integration of equation (9.49) separately for ms and mu, the following
formulas are obtained:
ms(t, , τ ) − ms(0, , τ ) =  t
0
DH(ϕσ−	(z))G(ϕσ−	(z), Ωσ + τ, 0) dσ,
mu(0, , τ ) − mu(t, , τ ) =  0
−t
DH(ϕσ−	(z))G(ϕσ−	(z), Ωσ + τ, 0) dσ.
(9.50)
In view of Lemma 9.34, the function t → V s
 (t, 0) is bounded. Also,
because DH vanishes at the hyperbolic saddle point ν0, we have that
limt→∞ DH(ϕt−	(z)) = 0,
and therefore
limt→∞ ms(t, , τ )=0.
It follows that the improper integral on the right-hand side of the first
equation in display (9.50) converges and
−ms(0, , τ ) =  ∞
0
DH(ϕt−	(z))G(ϕt−	(z), Ωt + τ, 0) dt.
Similarly, we have that
mu(0, , τ ) =  0
−∞
DH(ϕt−	(z))G(ϕt−	(z), Ωt + τ, 0) dt.
To complete the proof, simply note the equality
mu(0, , τ ) − ms(0, , τ ) = M¯ (, τ )
and that the sum of the integral representations of the quantities mu(0, , τ )
and −ms(0, , τ ) is just the Melnikov integral. ✷
As a consequence of Lemma 9.12 and the representation of the separation
function (9.41), we have now proved that
sep(, τ, ) = 
 M(, τ )
|DH(ϕ−	(z))|
+ O()

. (9.51)
In other words, the Melnikov function (properly normalized) is the leading￾order term in the series expansion of the separation function in powers of
the perturbation parameter. This is the key result of Melnikov theory.
Let us now prove Theorem 9.10.9.2 Periodic Perturbations: Transverse Homoclinic Points 557
Proof. For notational convenience, let us define
S(, τ, ) := M(, τ )
|η()| + O().
where η() = DH(ϕ−	(z)) so that formula (9.51) is recast in the form
sep(, τ, ) = S(, τ, ).
If M(0, τ0) = 0 and M	(0, τ0) = 0, then (0, τ0, 0) is a zero of S such that
S	(0, τ0, 0) = 0. Therefore, by the implicit function theorem, there is a
real-valued function h defined on some product neighborhood of  = 0 and
τ = τ0 such that h(0, τ0) = 0 and S(h(, τ ),τ, ) ≡ 0. Or, in other words,
using the definition of the separation function, our result implies that if ||
is sufficiently small, then the stable and unstable manifolds intersect at the
points given by
(V s(0, zs(h(, τ ), ), ), τ ) ≡ (V u(0, zu(h(, τ ), ), ), τ ), (9.52)
or equivalently at the points
(zs(h(, τ ), ), τ ) ≡ (zu(h(, τ ), ), τ ). (9.53)
To complete the proof we will show that if || = 0 is sufficiently small,
then the stable and unstable manifolds intersect transversally at the point
given in display (9.53).
Let us note that the curves in the phase cylinder given by
 → (zs(, τ0, ), τ0), τ → (zs(0,τ, ), τ )
both lie in Ws(γ()). Therefore, the vectors
(zs
	 (0, τ0, ), 0), (zs
τ (0, τ0, ), 1)
span the tangent space to Ws(γ()) at the intersection point with coordi￾nates (0, τ0). Indeed, since S	(0, τ0, 0) = 0, it follows from the definition of
the separation function and the continuity with respect to  that if || = 0
is sufficiently small, then zs
	 (0, τ0, ) = 0. Thus, the first tangent vector
is nonzero. Because the second component of the second tangent vector is
nonzero, the two vectors are linearly independent. Similarly, the vectors
(zu
	 (0, τ0, ), 0), (zu
τ (0, τ0, ), 1)
span the tangent space to the unstable manifold at the intersection point.
The stable and unstable manifolds meet transversally provided that three
of the four tangent vectors given above span R3. To determine a linearly
independent subset of these tangent vectors, we will use the definition of
the Melnikov function and Lemma 9.34.558 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
First, in view of the equalities
M(0, τ0)=0, zu(0, τ0, ) = zs(0, τ0, ),
and the definition of the Melnikov function, let us note that
∂
∂
DH(ϕ−	(z))(zu(, τ0, ) − zs(, τ0, ))
|η()|



	=	0
= 
M	(0, τ0)
|η(0)| + O()

and
∂
∂
DH(ϕ−	(z))(zu(, τ0, ) − zs(, τ0, ))
|η()|



	=	0
=
DH(ϕ−	0 (z))(zu
	 (0, τ0, ) − zs
	 (0, τ0, ))
|η(0)|

.
By combining the results of these computations, we have that
DH(ϕ−	0 (z))(zu
	 (0, τ0, ) − zs
	 (0, τ0, )) = (M	(0, τ0) + O()). (9.54)
Set t = 0 and τ = τ0 and differentiate both sides of both equations in
display (9.34) with respect to  at  = 0 to obtain the representations
zs
	 (0, τ0, ) = −f(ϕ−	(z)) + zs
	(0, τ0, 0) + O(
2),
zu
	 (0, τ0, ) = −f(ϕ−	(z)) + zu
	(0, τ0, 0) + O(
2).
Thus, by substitution into the equation (9.54), let us note that


DH(ϕ−	0 (z))(zu
	(0, τ0, 0) − zs
	(0, τ0, 0)) + O()

= (M	(0, τ0)+ O()). (9.55)
Also, since the determinant of a matrix is a multilinear form with respect
to the columns of the matrix, it follows by an easy computation using the
definition of the Hamiltonian vector field f that
det 
zu
	 (0, τ0, ), zs
	 (0, τ0, )

= 

det 
− f,zs
	
+ det 
zu
	, −f

+ O()

= (DH(ϕ−	0 (z)(zu
	(0, τ0, 0)
− zs
	(0, τ0, 0)) + O()). (9.56)
In view of the equations (9.55) and (9.56) we have that
det 
zu
	 (0, τ0, ), zs
	 (0, τ0, )

= M	(0, τ0) + O().
Therefore, the determinant is not zero, and the vectors
zu
	 (0, τ0, ), zs
	 (0, τ0, )
are linearly independent. Hence, due to the independence of these vectors,
the tangent vectors
(zu
	 (0, τ0, ), 0), (zs
	 (0, τ0, ), 0), (zs
τ (0, τ0, ), 1)9.3 Origins of ODE: Fluid Dynamics 559
are linearly independent, as required. As a result, if || = 0 is sufficiently
small, then the perturbed stable and unstable manifolds meet transversally
at the base point of these tangent vectors. ✷
Exercise 9.13. Discuss the existence of transverse homoclinic points for the
periodically perturbed Duffing oscillator
x¨ − x + x3 =  sin(Ωt).
Exercise 9.14. Discuss the existence of transverse homoclinic points for the
periodically perturbed damped pendulum
¨θ + ω2 sin θ = g(θ, t)
where
g(θ, t) := −λ ˙
θ + sin(Ωt).
How does the existence of transverse homoclinic points depend on the parame￾ters? What happens if the sinusoidal time periodic external force is replaced by
a smooth periodic function p(t). What happens if the viscous damping term is
replaced by −λ ˙
θ2?
Exercise 9.15. Discuss the existence of transverse homoclinic points for the
parametrically excited pendulum
¨θ + (ω2 +  cos(Ωt)) sin θ = 0.
Exercise 9.16. Discuss the existence of transverse homoclinic points for the
pendulum with “feedback control”
¨θ + sin θ + αθ − β = (−λ ˙
θ + γ cos(Ωt)
(see [264]). The “Melnikov analysis” of this system seems to require numerical
approximations of the Melnikov integral. Compute an approximation of the Mel￾nikov integral and find parameter values where your computations suggest the
existence of simple zeros. Plot some orbits of the stroboscopic Poincar´e map to
obtain an approximation of its phase portrait. Also find parameter values where a
numerical experiment suggests the corresponding dynamical system has sensitive
dependence on initial conditions.
Exercise 9.17. Using formula (9.31), prove that M(, τ ) = 0 if and only if
Mτ (, τ ) = 0. Note that a corollary of this result is the conclusion of Theorem 9.10
under the hypothesis that M(, τ ) = 0 and Mτ (, τ ) = 0.
9.3 Origins of ODE: Fluid Dynamics
The description of the motion of fluids is a central topic in practical scien￾tific research with a vast literature in physics, engineering, mathematics,560 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
and computation. The basic model is a system of partial differential equa￾tions of evolution type. Thus, as might be expected, many specializations of
this model lead to ordinary differential equations. In fact, some of the most
interesting and most important problems in ordinary differential equations
have their origin in fluid dynamics.
The purpose of this section is to briefly discuss the Euler and Navier￾Stokes model equations; to derive a system of ordinary differential equa￾tions, called the ABC system, that has been used to describe the steady
state motion of an ideal fluid in a certain ideal situation; and to discuss the
dynamics of the ABC system as an application of our analysis of perturbed
oscillators.
Caution: Treat this section as “a finger pointing at the moon.”
9.3.1 The Equations of Fluid Motion
Let us consider a fluid with constant density ρ confined to some region
R in space, and let us assume that the motion of the fluid is given by
the time-dependent velocity field u : R × R → R3 with (ξ, t) → u(ξ, t).
The position of a particle of the moving fluid is given by a smooth curve
t → γ(t) in R. Thus, the momentum of this fluid particle is ρu(γ(t), t),
and, according to Newton’s law, the motion of the particle is given by the
differential equation
ρ
d
dt(u(γ(t), t)) = F
where F denotes the sum of the forces. Although a fluid is always subjected
to the force of gravity and perhaps to other external body forces, let us
ignore these forces and consider only the constitutive force laws that model
the internal shear forces that are essential to our understanding of the
physical nature of fluids, just as Hooke’s law is the essential constitutive
force law for springs.
Internal fluid forces can be derived from more basic physical laws (see,
for example, [64] and [156]); however, let us simply note that the basic force
law is
F = μΔu − grad P
where μ is a constant related to the viscosity of the fluid, P is called the fluid
pressure, and the Laplacian operates on the velocity field componentwise.
Of course, the gradient and the Laplacian derivatives are with respect to the
space variables only. Let us also note that the viscosity term is a function of
the fluid velocity, but the pressure is a second unknown dependent variable
in the system. Thus, we will have to have two equations in the two unknown
functions u and P.
Using Newton’s law and the constitutive force law, the equation of motion
for a fluid is
ρ
∂u
∂t (ξ, t) + Du(ξ, t)u(ξ, t)

= μΔu(ξ, t) − grad P(ξ, t)9.3 Origins of ODE: Fluid Dynamics 561
where D denotes differentiation with respect to the space variables. In
fluid mechanics, if x, y, z are the Cartesian coordinates in R3 and ex, ey, ez
are the usual unit direction vectors (here the subscripts denote coordinate
directions, not partial derivatives), then the gradient operator
∇ := ∂
∂xex +
∂
∂y ey +
∂
∂z ez
is introduced and the advection term (Du)u is rewritten, using the usual
inner product, in the form u, ∇u, or more commonly as u · ∇u. Here, ∇
acts componentwise on the vector field u.
The fluid density must satisfy the continuity equation (6.98)
∂ρ
∂t + div(ρu)=0.
Thus, under our assumption that the density is constant (homogeneous
fluid), we must have that u is divergence free. This is equivalent to the
assumption that the fluid is incompressible.
Because our fluid is confined to a region of space, some boundary condi￾tions must be imposed. In fact, physical experiments show that the correct
boundary condition is u ≡ 0 on the boundary ∂R of the region R. To
demonstrate this fact yourself, consider cleaning a metal plate by using a
hose to spray it with water; for example, try cleaning a dirty automobile.
As the pressure of the water increases, the size of the particles of dirt that
can be removed decreases. But, it is very difficult to remove all the dirt
by spraying alone. This can be checked by polishing with a clean cloth. In
fact, the velocity of the spray decreases rapidly in the boundary layer near
the plate. Dirt particles with sufficiently small diameter are not subjected
to flow velocities that are high enough to dislodge them.
By introducing units of length, time, and velocity (that is, x → x/L,
t → t/T and u → u/U) and introducing the kinematic viscosity ν :=
μ/ρ, the system of equations for the velocity field and the pressure can be
rescaled to the dimensionless form of the Navier-Stokes equations for an
incompressible fluid in R given by
∂u
∂t + u · ∇u = 1
Re
Δu − grad p,
div u = 0,
u = 0 in ∂R (9.57)
where Re := LU/ν is the (dimensionless) Reynolds number. The existence
of this scaling is important: If two flows have the same Reynolds number,
then the flows have the same dynamics. For example, flow around a scaled
model of an airplane in a wind tunnel might be tested at the same Reynolds
number expected for the airplane under certain flight conditions. Perhaps
the same Reynolds number can be obtained by increasing the velocity in562 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
the wind tunnel to compensate for the smaller length scale of the model.
In principle, the behavior of the model is then exactly the same as the real
aircraft.
Euler’s equations for fluid motion can be viewed as an idealization of
the Navier-Stokes equations for a fluid with zero viscosity. These equations
have the form
∂u
∂t + u · ∇u = − grad p,
div u = 0,
u, η = 0 in ∂R (9.58)
where η is the outward unit normal vector field on ∂R. Note that the “no
slip” boundary condition for the Navier-Stokes equations is replaced by the
condition that there is no fluid passing through the boundary. The reason
for the physically unrealistic Euler boundary conditions is to ensure that
Euler’s partial differential equations are “well posed,” that is, they have
unique solutions depending continuously on initial conditions.
A naive expectation is that the limit of a family of solutions of the
Navier-Stokes equations as the Reynolds number increases without bound
is a solution of Euler’s equations. After all, the term Δu/Re would seem
to go to zero as Re → ∞. Note, however, the possibility that the second
derivatives of the velocity field are unbounded in the limit. For this and
other reasons, the limiting behavior of the Navier-Stokes equations for large
values of the Reynolds number is not yet completely understood. Thus, the
dynamical behavior of the family as the Reynolds number grows without
bound is a fruitful area of research.
Flow in A Pipe
As an example of the solution of a fluid flow problem, let us consider per￾haps the most basic example of the subject: flow in a round pipe.
If we choose cylindrical coordinates r, θ, z with the z-axis being the axis
of symmetry of a round pipe with radius a, then it seems natural to expect
that there are some flow regimes for which the velocity field has its only
nonzero component in the axial direction of the pipe; that is, the velocity
field has the form
u(r, θ, z, t) = (0, 0, uz(r, θ, z, t)) (9.59)
where the components of this vector field are taken with respect to the
basis vector fields er, eθ, ez that are defined in terms of the usual basis of
Euclidean space by
er := (cos θ,sin θ, 0), eθ := (− sin θ, cos θ, 0), ez := (0, 0, 1).
Let us express the Euler and the Navier-Stokes equations in cylindrical
coordinates. Recall that if f is a function and F = Frer + Fθeθ + Fzez is a9.3 Origins of ODE: Fluid Dynamics 563
vector field on Euclidean space, then in cylindrical coordinates,
∇f = ∂f
∂r er +
1
r
∂f
∂θ eθ + ∂f
∂z ez,
div f = 1
r
∂
∂r (rFr) + 1
r
∂Fθ
∂θ +
∂Fz
∂z ,
Δf = 1
r
∂
∂r

r
∂f
∂r

+
1
r2
∂2f
∂θ2 +
∂2f
∂z2 . (9.60)
To obtain the Navier-Stokes equations in cylindrical coordinates, consider
the unknown velocity field u = urer + uθeθ + uzez. Write this vector
field in the usual Cartesian components by using the definitions of the
direction fields given above, insert the result into the Navier-Stokes equa￾tions, and then compute the space derivatives using the operators given
in display (9.60). If we multiply the first two of the resulting component
equations—the equations in the directions ex and ey—by the matrix
 cos θ sin θ
− sin θ cos θ

,
then we obtain the equivalent system
∂ur
∂t + (u · ∇)ur − 1
r
u2
θ = 1
Re

Δur − 1
r2 (ur + 2∂uθ
∂θ )

− ∂p
∂r ,
∂uθ
∂t + (u · ∇)uθ +
1
r
uruθ = 1
Re

Δuθ − 1
r2 (uθ − 2
∂ur
∂θ )

− 1
r
∂p
∂θ ,
∂uz
∂t + (u · ∇)uz = 1
Re
Δuz − ∂p
∂z ,
div u = 0. (9.61)
The Euler equations in cylindrical coordinates for the fluid motion in
the pipe are obtained from system (9.61) by deleting the terms that are
divided by the Reynolds number. If the velocity field u has the form given in
equation (9.59), then u automatically satisfies the Euler boundary condition
at the wall of the pipe. Thus, the Euler equations for this velocity field u
and scaled pressure p reduce to the system
∂p
∂r = 0, ∂p
∂θ = 0, ∂uz
∂t + uz
∂uz
∂z = −∂p
∂z = 0, ∂uz
∂z = 0.
It follows that p must be a function of z only, and
∂uz
∂t = −∂p
∂z . (9.62)
If we now differentiate equation (9.62) with respect to z, then we see imme￾diately that ∂2p/∂z2 = 0. Therefore, p = p0 + p1z for some constants564 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
p0 and p1, and we must also have that uz = −p1t + u0 for some constant
u0. Let us note that if we were to impose the no slip boundary conditions
(which are not correct for Euler flow), then the only possible solution is
uz = 0 and p = p0. In particular, we cannot impose a nonzero initial fluid
velocity.
There are two cases for Euler flow (with the no penetration boundary
condition): If p1 = 0, then the pressure is constant in the pipe and the
velocity field is constant. This is called plug flow. If p1 = 0, then both
the pressure and the velocity become unbounded as time passes to infinity.
Both cases are not physically realistic. For example, the flow in the first
case does not satisfy the experimentally observed fact that the velocity of
the flow is larger in the center of the pipe than at its wall. Nonetheless,
because of its mathematical simplicity, plug flow is often used as a model.
For example, plug flow is often used to model flow in tubular reactors
studied in chemical engineering.
What about Navier-Stokes flow?
If we consider the same pipe, the same coordinate system, and the same
hypothesis about the direction of the velocity field, then the Navier-Stokes
equations reduce to
∂p
∂r = 0, ∂p
∂θ = 0, ∂uz
∂t + uz
∂uz
∂z = 1
Re
Δuz − ∂p
∂z = 0, ∂uz
∂z = 0,
with the no slip boundary condition at the wall of the pipe given by
uz(a, θ, z, t) ≡ 0.
This system of equations is already difficult to solve! Nevertheless, we can
obtain a solution if we make two additional assumptions: The velocity field
is in steady state and it is symmetric with respect to rotations about the
central axis of the pipe. With these assumptions, if we take into account
the equation ∂uz/∂z = 0, then it suffices to solve the single equation
1
Re
1
r
∂
∂r

r
∂uz
∂r

= pz.
Because pr = 0 and pθ = 0, we have that pz depends only on z while the
left-hand side of the last equation depends only on r. Thus, the functions
on both sides of the equation must have the same constant value, say c. If
this is the case, then p = cz + p0.
The remaining ordinary differential equation
ru
z (r) + u
z(r)=(cRe)r
with the initial condition uz(a) = 0 has the continuous solution
uz(r) = 1
4
cRe (r2 − a2).9.3 Origins of ODE: Fluid Dynamics 565
Thus, we have derived the result that the steady state velocity field u pre￾dicted by the Navier-Stokes equations is parabolic with respect to the radial
coordinate. This flow field is physically realistic, at least if the Reynolds
number is sufficiently small; it is called Poiseuille flow.
Exercise 9.18. Consider Poiseuille flow in a section of length L of an infinite
round pipe with radius a. If the pressure is p in at the inlet of the section and
the flow speed at the center of the pipe is v in, then determine the pressure at
the outlet. What happens in the limit as the Reynolds number grows without
bound? Compare with the prediction of plug flow.
Using the vector identity
1
2 grad(u · u) = u × curl u + u · ∇u
where · denotes the usual inner product on Euclidean space, let us rewrite
Euler’s equation in the form
ut − u × curl u = grad(−1
2
(u · u) − p).
With the definition α := −1
2 |u|
2 − p, we obtain Bernoulli’s form of Euler’s
equations
ut = u × curl u + grad α,
div u = 0,
u · η = 0 in ∂R. (9.63)
Potential Flow
Let us consider an important specialization of Bernoulli’s form of Euler’s
equations: potential flow in two space dimensions. The idea is the following.
Assume that the velocity field u is the gradient of a potential f so that u =
grad f. Substitution into system (9.63), using the identity curl(grad u)=0
and some rearrangement, gives the equations of motion
grad(∂f
∂t +
1
2
| grad f|
2 − p)=0, Δf = 0. (9.64)
As a result, we see immediately that the quantity
∂f
∂t +
1
2
| grad f|
2 − p
is constant with respect to the space variables. In particular, if u is a steady
state velocity field, then there is a constant c such that
p = c − 1
2
|u|
2; (9.65)566 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
that is, the pressure is a constant minus half the square of the velocity.
This is Bernoulli’s law.
In view of the second equation of system (9.64), the potential f is a
harmonic function. Therefore, by considering a hypothetical flow on a two￾dimensional plane with Cartesian coordinates (x, y) and velocity field u =
( ˙x, y˙), the potential f is locally the real part of a holomorphic function, say
h = f +iψ. Moreover, the pair f,ψ satisfies the Cauchy-Riemann equations
∂f
∂x = ∂ψ
∂y , ∂f
∂y = −∂ψ
∂x .
Thus, the assumption that u = grad f implies the fluid motions are solu￾tions of an ordinary differential equation that can be viewed in two different
ways: as the gradient system
x˙ = ∂f
∂x, y˙ = ∂f
∂y ;
or the Hamiltonian system
x˙ = ∂ψ
∂y , y˙ = −∂ψ
∂x . (9.66)
The function ψ, a Hamiltonian function for system (9.66), is called the
stream function. The orbits of system (9.66), called stream lines, all lie on
level sets of ψ. Let us also note that because the stream lines are orbits of
a gradient system, there are no periodic fluid motions in a region where the
function h is defined.
It should be clear that function theory can be used to study planar
potential flow. For example, if ψ is a harmonic function defined in a simply
connected region of the complex plane such that the boundary of the region
is a level set of ψ, then ψ is the imaginary part of a holomorphic function
defined in the region, and therefore ψ is the stream function of a steady
state flow. This fact can be used to find steady state solutions of Euler’s
equations in many regions of the complex plane.
As an example, let us start with plug flow in a pipe with radius a and
notice that every planar slice containing the axis of the pipe is invariant
under the flow. In fact, if we view the strip
S := {(x, y):0 <y< 2a}
as such a slice where we have taken x as the axial direction, then the
plug flow solution of Euler’s equations in S is given by the velocity field
u = (0, c) and the pressure p = p0 where c and p0 are constants. This is a
potential flow, with potential f(x, y) = cx, stream function ψ(x, y) = cy,
and complex potential h(x, y) = cz = c(x + iy).
Suppose that Q is an invertible holomorphic function defined on S and
that R is the image of S under Q, then w → h(Q−1(w)) for w ∈ R9.3 Origins of ODE: Fluid Dynamics 567
is a holomorphic function on R. Moreover, by writing h = f + iψ, it is
easy to see that w → ψ(Q−1(w)) is a stream function for a steady state
potential flow in R. In particular, stream lines of ψ map to stream lines of
w → ψ(Q−1(w)).
For example, let us note that w := Q(z) = √z has a holomorphic branch
defined on the strip S such that this holomorphic function maps S into the
region in the first quadrant of the complex plane bounded above by the
parabola {(σ, τ ) : στ = a}. In fact, Q−1(w) = w2 so that
x = σ2 − τ 2, y = 2στ.
The new “flow at a corner” has the complex potential h(Q−1(w)) = cw2 =
c(σ2 − τ 2 + 2iστ ). Thus, the velocity field is
u = (2cσ, −2cτ ).
The corresponding pressure is found from Bernoulli’s equation (9.65). In
fact, there is a constant p1 such that
p = p1 − 2c2(σ2 + τ 2). (9.67)
The stream lines for the flow at a corner are all parabolas.
The flow near a wall is essentially plug flow. In fact, if we consider, for
example, the flow field on a vertical line orthogonal to the σ-axis, say the
line with equation σ = σ0, then the velocity field near the wall, where
τ ≈ 0, is closely approximated by the constant vector field (2cσ0, 0). In
other words, the velocity profile is nearly linear.
Exercise 9.19. Consider the plug flow vector field u = (c, 0) defined in a hori￾zontal strip in the upper half-plane of width 2a. Find the push forward of u into
the first quadrant with respect to the map Q(z) = √z with inverse Q−1(w) = w2.
Is this vector field a solution of Euler’s equations at the corner? Explain.
A Boundary Layer Problem
We have just seen that planar steady state Euler flow has stream lines
that are (locally) orbits of both a Hamiltonian differential equation and a
gradient differential equation. Moreover, in our example of flow at a corner,
the velocity profile near the walls is linear. What about planar steady state
Navier-Stokes flow?
Let us again consider the physical problem of flow at a corner (see [174,
p. 222]). By physical reasoning, we might expect that the most prominent
difference between Euler flow and Navier-Stokes flow at a corner is pro￾duced near the walls at the corner. The stream lines of the Euler flow are
bent near the corner, but the velocity of the flow field does not approach
zero at the walls—the fluid in the Euler model moves as if it had zero vis￾cosity. For the Navier-Stokes flow, where the viscosity of the fluid is taken568 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
into account, the fluid velocity vanishes at the walls. On the other hand,
the Navier-Stokes flow far away from the corner would be expected to be
essentially the same as the Euler flow.
In our model, the fluid velocity field is assumed to be divergence free.
Because we are working in two space dimensions, this assumption implies
that there is a stream function; that is, the velocity field is Hamiltonian.
In fact, if the planar coordinates at the corner are renamed to x, y and
the velocity field u has components v, w so that the associated differential
equation for the fluid motion is
x˙ = v(x, y), y˙ = w(x, y),
then the orbits of this system correspond to solutions of the exact first-order
differential equation dy/dx = w/v. Recall that the differential equation is
exact if the corresponding differential one-form wdx − vdy is closed; that
is, if ∂w/∂y +∂v/∂x = 0. Thus, there is a (locally defined) function ψ(x, y)
such that ∂ψ/∂x = −w and ∂ψ/∂y = v; that is, ψ is a stream function for
the flow. This result is proved in elementary courses in differential equa￾tions; it is also a special case of Poincar´e’s lemma: If n > 0, then a closed
form on a simply connected region of Rn is exact.
Using the stream function for the Euler flow at the corner given by
(x, y) → 2cxy and some physical reasoning, we might guess that the stream
function for the corresponding Navier-Stokes flow is given by ψ(x, y) =
xg(y) for some function g to be determined. Of course, we are free to
assume our favorite form for this stream function. The problem is to show
that there is a corresponding solution of the Navier-Stokes equations and to
use this solution to predict the velocity profile for the flow near the corner.
For the stream function ψ(x, y) = xg(y), the velocity field is
(v(x, y), w(x, y)) = (xg
(y), −g(y)). (9.68)
Because the formula for the pressure for the Euler flow is given by equa￾tion (9.67) and the unknown function g depends only on the second space
variable, let us postulate that the pressure for the Navier-Stokes flow is
given by
p(x, y) = p0 − 2c2(x2 + G(y)) (9.69)
where p0 is a constant, and G is a function to be determined.
The steady state Navier-Stokes equations are
v
∂v
∂x + w
∂v
∂y = 1
Re
Δv − ∂p
∂x,
v
∂w
∂x + w
∂w
∂y = 1
Re
Δw − ∂p
∂y ,
∂v
∂x +
∂w
∂y = 0 (9.70)9.3 Origins of ODE: Fluid Dynamics 569
5 10 15 20 25 30 35 40
0.2
0.4
0.6
0.8
1
1.2
1.4
Figure 9.7: Plot of f
(t/) versus t for the solution of the Falkner-Skan
boundary value problem (9.72) with  = 1/10.
with the boundary condition that the velocity field (v(x, y), w(x, y)) van￾ishes at the wall.
If the velocity field (9.68) and the pressure (9.69) are inserted into the
Navier-Stokes equations (9.70), the system reduces to the equations
1
Re g + gg − (g
)
2 + 4c2 = 0, G = 1
2c2 (gg +
1
Re g) (9.71)
with the boundary conditions
g(0) = 0, g
(0) = 0.
We also have made the assumption that the velocity field (9.68) is the same
as the Euler velocity field (2cx, −2cy) far away from the wall. Ideally, we
must have 2cx ≈ xg
(y) for large y, that is,
limy→∞ g
(y)=2c.
We will be able to solve for the pressure and thus construct the desired
solution of the system (9.70) provided that there is a solution of the first
equation of system (9.71) with the specified initial and boundary condi￾tions. Let us rescale with g := 2cf and define  = 1/(cRe) to reduce our
quest for a solution of system (9.70) to finding a function f that solves the
boundary value problem
f + ff − (f
)
2 +1=0, f(0) = f
(0) = 0, f
(∞) = 1 (9.72)
for  > 0 a small parameter (see Exercises 1.12, 1.9, and 9.20). The ordinary
differential equation, essentially the Falkner-Skan equation (see [70], [97],
[132], and [154]), is typical of a class of equations that arise in “boundary
layer theory,” the origin of an important class of “singular perturbation
problems,” (see [149], [151], [189], [194], and [200]).570 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
Exercise 9.20. A proof of the existence of a solution of the boundary value
problem (9.72) is not trivial. This exercise outlines the main ingredients for a
geometric proof. Recall Exercise 1.9 and recast the boundary value problem as
follows: For  > 0 sufficiently small, determine the existence of a solution of the
system
x˙ = y, y˙ = z, z˙ = y2 − xz − 1 (9.73)
such that x(0) = 0, y(0) = 0, and limτ→∞ y(τ ) = 1. Equivalently, we may
consider the fast-time system
x˙ = y, y˙ = z, z˙ = y2 − xz − 1 (9.74)
and again seek a solution with x(0) = 0, y(0) = 0, and limt→∞ y(t) = 1. (a) For
system (9.74) with  = 0, show that the plane {(x, y, z) : y = 1} is invariant and
the open ray {(x, y, z) : x > 0, y = 1, z = 0} is invariant with basin of attraction
{(x, y, z) : x > 0, y = 1}. (b) For  > 0 and small, the qualitative structure of
part (a) persists. Show that the line {(x, y, z) : y = 1, z = 0} is invariant for
system (9.74) for all . (c) By part (a), the half-plane {(x, y, z) : x > 0, y = 1}
is the stable manifold of the open ray {(x, y, z) : x > 0, y = 1, z = 0}. This
qualitative structure will persist for sufficiently small . Justify this statement.
(d) This is the hard part. If  > 0, then the two-dimensional stable manifold
for the open ray {(x, y, z) : x > 0, y = 1, z = 0} intersects the plane {(x, y, z) :
y = 0} and the curve of intersection meets the line {(x, y, z) : x = 0, y = 0}
exactly once. Show that if this statement is true, then the original boundary
value problem (9.73) has a unique solution. (e) Reproduce Figure 9.7. Hint: Set
x(0) = 0 and y(0) = 0 in system (9.74) and use Newton’s method to find a zero
of the function ζ → y(40, ζ)−1, where t → y(t, ζ) is the second component of the
solution of the system with the initial value y(0, ζ) = 0. This numerical method
is called shooting. Note: There is nothing special about the number 40; it is just
a choice for a large experimental value of the fast time. (f) Support the claim in
part (d) by numerical experiments. In particular, graph an approximation of the
intersection curve of the stable manifold and the plane.
9.3.2 ABC Flows
The dynamics of a fluid that is predicted by Euler’s equations (9.58) depend
on the region that confines the flow and on the initial velocity field. In this
section, we will study the fluid dynamics of an ideal family of steady state
solutions that are periodic in the entire space relative to all three directions.
Let us seek a steady state solution u, a rest point of the infinite-dimensional
flow given by Euler’s equations in Bernoulli’s form (9.63), that is peri￾odic in each space variable with period 2π. If there is such a steady state,
then it exists on all of R3 so no additional boundary condition is nec￾essary. In effect, the usual boundary condition for Euler’s equations is
replaced by the periodicity requirements. For this reason our requirements
are called periodic boundary conditions. Also, if we like, we can view the
solution as a vector field on the (compact) three-dimensional torus T3
defined by considering each of the Cartesian coordinates of R3 modulo 2π.9.3 Origins of ODE: Fluid Dynamics 571
We will consider the special class of steady states given as solutions of the
system
u × curl u = 0, div u = 0. (9.75)
System (9.75) has many solutions, but certainly the most famous are the
velocity fields of the form
u = (A sin z + C cos y, B sin x + A cos z, C sin y + B cos x)
where A, B, and C are constants. These vector fields generate the ABC
flows (see [13], [17], [44], [51], [109], and [106]). The corresponding system
of ordinary differential equations
x˙ = A sin z + C cos y,
y˙ = B sin x + A cos z,
z˙ = C sin y + B cos x (9.76)
is a useful test example for the behavior of steady state Euler flow.
By rescaling the system and the time parameter, and by reordering the
variables if necessary, all the interesting cases for different parameter values
can be reduced to the consideration of parameters satisfying the inequalities
A = 1 ≥ B ≥ C ≥ 0. To obtain a perturbation problem, let us consider
the system with A = 1 > B = β>C =  where  is a small parameter.
Also, to simplify some formulas to follow, let us introduce a translation of
the first variable x → x + π/2. The ABC system that we will study then
has the form
x˙ = sin z +  cos y,
y˙ = β cos x + cos z,
z˙ = −β sin x +  sin y (9.77)
where 0 <β< 1, and  ≥ 0 is a small parameter.
Note that the subsystem
x˙ = sin z, z˙ = −β sin x (9.78)
of system (9.77) is Hamiltonian with respect to the Hamiltonian function
H(x, z) := β cos x + cos z. (9.79)
Of course, the function H is constant on orbits of system (9.78).
A typical phase portrait for system (9.78) is depicted in Figure 9.8. Inde￾pendent of the choice of β, there is a rest point at the origin surrounded
by a period annulus A whose outer boundary consists of two hyperbolic
saddle points with coordinates (±π, 0) together with the heteroclinic orbits
connecting these saddles (see Exercise 9.21). If we view the system on T3,
then these saddle points coincide, and the boundary of the period annulus
is just one saddle and a homoclinic orbit.572 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
-4 -2 0 2 4
-4
-2
0
2
4
Figure 9.8: Computer generated phase portrait for system (9.78) with β =
0.16.
Exercise 9.21. Prove the statements made in this section about the phase
portrait of system (9.78).
Each orbit Γh in A corresponds to a level set of H given by
Γh := {(x, z) : H(x, z) = h}
for some h in the range 1 − β<h< 1 + β. The boundary of the period
annulus corresponds to the level set with h = 1 − β.
On each orbit in the closure of the period annulus A for the unperturbed
system (9.78) we have that
y˙ = β cos x + cos z = h
for some “energy” h > 0. It follows that ˙y is positive everywhere in an
open neighborhood of the closure of A. Let us therefore view y as a time￾like variable for the perturbed system and consider the associated system
x = sin z
β cos x + cos z
+  cos y
β cos x + cos z
,
z = −β sin x
β cos x + cos z
+  sin y
β cos x + cos z (9.80)
where  denotes differentiation with respect to y. Of course, if we find a
solution
y → (x(y, ), z(y, )) (9.81)9.3 Origins of ODE: Fluid Dynamics 573
of system (9.80), then there are corresponding solutions
t → (x(y(t), ), y(t), z(y(t), )) (9.82)
of system (9.77) obtained by solving the equation
y˙ = β cos x(y, ) + cos z(y, ). (9.83)
Let us notice that system (9.80) with  = 0 is the same as system (9.78)
up to a reparametrization of the independent variable. Moreover, the unper￾turbed system (9.80) is a Hamiltonian system with respect to the Hamil￾tonian function (9.79). Finally, we have the following useful proposition: If
y → (x(y), z(y)) is the solution of the unperturbed system (9.80) with the
initial condition x(0) = 0, z(0) = z0, then
−x(−y) = x(y), z(−y) = z(y), (9.84)
that is, x is odd and z is even. To see this, consider the new functions u
and v defined by
(u(y), v(y)) = (−x(−y), z(−y))
and verify that the function y → ((u(y), v(y)) is a solution of the unper￾turbed system (9.80) with the initial condition u(0) = 0, v(0) = z0.
9.3.3 Chaotic ABC Flows
The unperturbed system (9.80) has heteroclinic cycles. For example, a cycle
is formed by the hyperbolic saddle points at (x, z)=(±π, 0) and their
connecting orbits. (Note that this cycle is also the boundary of a period
annulus.) Or, if we view the system on the phase cylinder obtained by
considering the variable x as an angle, then this cycle has only one saddle
point and it is connected by two distinct homoclinic orbits. In this section
we will see that for all but one value of the parameter β in the interval
0 <β< 1, the Melnikov function along these heteroclinic orbits has simple
zeros. Thus, for sufficiently small  > 0, system (9.80), and of course the
corresponding original ABC system, has a chaotic invariant set. This result
serves as an interesting application of our perturbation theory. It suggests
that “real” fluids have chaotic motions.
Let us recall that the unperturbed heteroclinic orbits lie on the set
{(x, z) : cos z + β cos x = 1 − β}, (9.85)
and let us consider the unperturbed solution y → (x(y), z(y)) starting
at the point (0, arccos(1 − 2β)). The Melnikov function is given (up to a
nonzero scalar multiple) by
M(φ) = 1
(1 − β)2
 ∞
−∞
(sin(z(y+φ)) sin y+β sin(x(y+φ)) cos y) dy. (9.86)574 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
This integral is easily transformed to the more useful representation
M(φ) = sin φ
(1 − β)2
 ∞
−∞
(β sin x(s) sin s − sin z(s) cos s) ds (9.87)
by first changing the independent variable in the integral (9.86) to s := y+θ
and then by using the sum formulas for sine and cosine together with the
facts that the function y → sin x(y) is odd and y → sin z(y) is even. If, in
addition, we apply integration by parts to obtain the formula
 ∞
−∞
sin z(s) cos s ds = β
1 − β
 ∞
−∞
cos z(s) sin x(s) sin s ds,
and substitute for cos z(s) from the energy relation in display (9.85), then
we have the identity
 ∞
−∞
sin z(s) cos s ds =
 ∞
−∞
β sin x(s) sin s ds
− β2
1 − β
 ∞
−∞
sin x(s) cos x(s) sin s ds.
Finally, by substitution of this identity into equation (9.87), we obtain the
following representation for the Melnikov function
M(φ) = β2 sin φ
(1 − β)3
 ∞
−∞
sin x(s) cos x(s) sin s ds. (9.88)
Of course it is now obvious that the Melnikov function will have infinitely
many simple zeros along the heteroclinic orbit provided that the integral
Is :=  ∞
−∞
sin x(s) cos x(s) sin s ds.
does not vanish.
To determine if Is is not zero, let us consider a method to evaluate
this improper integral. The first step is to find explicit formulas for the
unperturbed solution. Note that z(y) > 0 along the heteroclinic orbit.
Integrate the unperturbed differential equation
x
(y)
sin z(y) = 1
1 − β
on the interval (0, y), and use the energy relation to obtain the equation
y
1 − β =
 x(y)
0
1
1 − ((1 − β) − β cos s)2
ds
= 1
β
 x(y)
0
1
(1 + cos s)((2 − β(1 + cos s))
ds.9.3 Origins of ODE: Fluid Dynamics 575
The form of the last integrand suggests the substitution u = 1 + cos s,
which transforms the integral so that the last equality becomes
y
√β
1 − β = −
 1+cos x(τ)
2
1
u
(2 − u)(2 − βu)
du.
Using the indefinite integral
 1
u
(2 − u)(2 − βu)
du = −1
2
ln 4
(2 − u)(2 − βu) − 2(β + 1)u + 8
u

and a simple algebraic computation, we have the equality
cos x(y) = −(β − 1)e4cy + 2(3 − β)e2cy + β − 1
(β − 1)e4cy − 2(β + 1)e2cy + β − 1
where
c :=
√β
1 − β .
Also, by the trigonometric identity sin2 x + cos2 x = 1, we have that
sin x(y) = −4
1 − β ecy(ecy − 1)
(β − 1)e4cy − 2(β + 1)e2cy + β − 1
.
Define
F(w) := −4(1 − β)
−3/2 w(w2 − 1)((1 − β)w4 + 2(β − 3)w2 + 1 − β)
(w4 + 2 1+β
1−β w2 + 1)2
and note that
Is =
 ∞
−∞
F(ecy) sin y dy
=
 ∞
−∞
F(eζ
√β) sin((1 − β)ζ) dζ.
Also, note that the poles of the integrand of Is correspond to the zeros of the
denominator of F. To determine these zeros let us write the denominator
in the factored form
w4 + 21 + β
1 − β w2 +1=(w2 − u1)(w2 − u2)
where
u1 :=
√β − 1
√β + 1, u2 :=
√β + 1
√β − 1
.576 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
The poles corresponding to e2ζ
√β = u1 are
ζ = 1
2
√β

ln(−u1) + πi + 2kπi
, k ∈ Z,
where Z denotes the set of integers, and the poles corresponding to u2 are
ζ = 1
2
√β

ln(−u2) − πi + 2kπi
, k ∈ Z.
The locations of the poles suggest integration around the rectangle Γ in
the complex plane whose vertices are T, T + iπ/√β, −T + iπ/√β, and
−T. In fact, for sufficiently large T > 0, Γ encloses exactly two poles of the
integrand, namely,
ζ1 := 1
2
√β

ln(−u1) + πi
, ζ2 := 1
2
√β

ln(−u2) + πi
.
The function F defined above is odd. It also has the following property:
If w = 0, then F(1/w) = −F(w). Using these facts, the identity sin ζ =
(eiζ − e−iζ )/(2i), and a calculation, our integral can be recast in the form
Is = −i
 ∞
−∞
F

eζ
√β
ei(1−β)ζ dζ.
For notational convenience, define
K := K(β) = (1 − β)π
2
√β ,
and also consider the contour integral

Γ
F

eζ
√β
ei(1−β)ζ dζ.
The corresponding path integral along the upper edge of Γ is just e−2K
multiplied by the path integral along the lower edge. Also, by using the
usual estimates for the absolute value of an integral, it is easy to see that
the path integrals along the vertical edges of Γ approach zero as T increases
without bound. Thus, the real improper integral Is is given by
Is = −i

1 + e−2K−1

Γ
F

eζ
√β 
ei(1−β)ζ dζ
= 2π

1 + e−2K−1
(Res(ζ1) + Res(ζ2)) (9.89)
where the residues are computed relative to the function G given by
G(ζ) := F

eζ
√β 
ei(1−β)ζ .9.3 Origins of ODE: Fluid Dynamics 577
Define
F1(w) := (w2 − u1)
2F(w), F2(w) := (w2 − u2)
2F(w)
and compute the Laurent series of G at ζ1 and ζ2 to obtain the following
residues:
Res(ζ1) =ei(1−β)ζ1
4βu2
1
βeζ1
√βF
1

eζ1
√β 
− 
2
β − i(1 − β)

F1

eζ1
√β ,
Res(ζ2) =ei(1−β)ζ2
4βu2
2
βeζ2
√βF
2

eζ2
√β 
− 
2
β − i(1 − β)

F2

eζ2
√β .
To simplify the sum of the residues, let us define
A := cos 1 − β
2
√β ln(−u2)

, B := sin 1 − β
2
√β ln(−u2)

so that
ei(1−β)ζ1 = e−K(A−Bi), ei(1−β)ζ2 = e−K(A + Bi),
and let us note that since u1 = 1/u2, we have
eζ1
√βeζ2
√β = −1.
Also, note that the function F1 is odd, F
1 is even, and verify the following
identities:
F1(1/w) = − 1
w4u2
2
F2(w),
F
1(1/w) = 1
w2u2
2
F
2(w) − 4
w3u2
2
F2(w).
Finally, for notational convenience, define L := √−u2 .
Using the notation and the identities just mentioned, the residues are
given by
Res(ζ1) = e−K
4βu2
2

(A−Bi)(−iL
β F
2(iL) + 
2
β + i(1 − β)

F2(iL)

,
Res(ζ2) = e−K
4βu2
2

(A + Bi)(iL
β F
2(iL) − 
2
β − i(1 − β)

F2(iL)

.
Thus, in view of formula (9.89), we have
Is = πe−K
βu2
2(1 + e−2K)

B
β

− 2iF2(iL) − LF
2(iL)

+ A(1 − β)iF2(iL)

.
(9.90)578 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
-3 -2 -1 0 1 2 3
-3
-2
-1
0
1
2
3
Figure 9.9: Some orbits of the stroboscopic Poincar´e map for system (9.80)
with  = 0.01 and β = 0.1.
2.9 2.95 3 3.05 3.1
-0.1
0
0.1
0.2
Figure 9.10: Blowup of Figure (9.9) near the unperturbed hyperbolic saddle
point at (x, z)=(π, 0). Several orbits are depicted.9.3 Origins of ODE: Fluid Dynamics 579
The quantities
F
2(iL) = 4(1 + √β )2(β + 2√β ) − 1)
√1 − β (1 − √β )β3/2 ,
−iF2(iL)=41 + √β
1 − √β
1/2 (1 + √β )2
√1 − β (1 − √β )β
are real and −iF2(iL) is nonzero for 0 <β< 1. Also, if the identity
2 + L F
2(iL)
iF2(iL) = 1 − β
√β
is inserted into equation (9.90), then
Is = π(1 − β)e−K
βu2
2(1 + e−2K)
(−iF2(iL))
B−A
.
Remark 2. The computation of the Melnikov function for the ABC flow
given here follows the analysis in [44] where there are a few computational
errors that are repaired in the analysis of this section. In particular, the
final value of Is reported in [44] is not correct.
Clearly, Is = 0 if and only if A = B, or equivalently if
tan 1 − β
2
√β
ln 1 + √β
1 − √β

= 1.
The last equation has exactly one root for β in the open unit interval:
β ≈ 0.3. Thus, except at this one-parameter value, our computation proves
that the perturbed stable and unstable manifolds intersect transversally,
and, as a result, the corresponding perturbed flow is chaotic in a zone near
these manifolds.
The results of a simple numerical experiment with the dynamics of sys￾tem (9.80) are depicted in Figure 9.9 and Figure 9.10. These figures each
depict several orbits of the stroboscopic Poincar´e map—the independent
variable y is viewed as an angular variable modulo 2π. Figure 9.10 is a
blowup of a portion of Figure 9.9 near the vicinity of the unperturbed sad￾dle point at (x, z)=(π, 0). The results of this experiment suggest some of
the fine structure in the stochastic layer that forms after breaking the het￾eroclinic orbits of the unperturbed Poincar´e map. As predicted, the orbit
structure appears to be very complex (see [17]).
Exercise 9.22. Reproduce Figures 9.9 and 9.10. The value  = 0.01 was used
to obtain an easily reproducible picture. Note that our theory only predicts the
existence of chaotic invariant sets for sufficiently small . Probably  = 0.01 is too
big. Perform a series of numerical experiments to illustrate how the stochastic
layer changes as  changes for both smaller and larger values of .580 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
Figure 9.11: The figure depicts the attractor for the stroboscopic Poincar´e
map for the system ¨θ +μ ˙
θ + sin θ = −1/10 + 2 cos(2t) sin θ where, from left
to right, μ = 0.03, 0.0301, 0.1, 0.5, 0.56, and 0.65.
Exercise 9.23. Discuss the statement: “The ABC system is conservative.” Note
that system (9.80) is a perturbed Hamiltonian system with no damping. The
nature of chaotic invariant sets for dissipative systems can be quite different
from the chaotic invariant sets for Hamiltonian systems. In particular, dissipative
systems can have chaotic attractors. Roughly speaking, a chaotic attractor S is
a compact invariant set with a dense orbit such that S contains the ω-limit set
of every orbit in an open neighborhood of S. Although it is very difficult to
prove the existence of chaotic attractors, numerical evidence for their existence
is abundant. Consider the stroboscopic Poincar´e map on the phase cylinder for
the parametrically excited pendulum with damping and torque given by
¨θ + μ ˙
θ + sin θ = −τ + a cos(2t) sin θ.
Let the usual coordinates on the phase cylinder be (v, θ) where v := ˙
θ. It is
convenient to render the graphics in a new coordinate system on the cylinder
that flattens a portion of the cylinder into an annulus on the plane. For example,
in Figure 9.11 iterates of the Poincar´e map are plotted in the (x, y)-plane with
x = (2(4 − v))1/2 cos θ, y = (2(4 − v))1/2 sin θ
for the system
¨θ + μ ˙
θ + sin θ = − 1
10 + 2 cos(2t) sin θ
for six different values of μ. In each case a single orbit is depicted. The same
picture is obtained independent of the initial value for the iterations as long as
the first few iterations are not plotted. Thus, it appears that each depicted orbit9.3 Origins of ODE: Fluid Dynamics 581
is near an attractor. Reproduce Figure 9.11. Also, explore other regions of the
parameter space of the oscillator by performing numerical experiments. To learn
more about chaotic attractors, see, for example, [119], [218], and [261].
Periodic Orbits of ABC Flows
In the last section we proved that the ABC system has chaotic invariant sets
for some choices of the parameters. If such a set exists as a consequence
of the transversal intersection of stable and unstable manifolds near an
unperturbed heteroclinic cycle, then it follows from a general theory (one
which we have not presented here) that the chaotic invariant set contains
infinitely many periodic orbits. But, this result does not tell us if any of
the unperturbed periodic orbits in the various resonant tori are continu￾able. Although the rigorous determination of the continuable unperturbed
periodic orbits seems to be a difficult problem which is not yet completely
solved, we will use this problem as a vehicle to introduce some new tech￾niques.
Before we begin the continuation analysis, let us note that if we find a
continuable subharmonic of the unperturbed system (9.80), then there is
a corresponding family of periodic solutions of system (9.77). To see this,
let us suppose that the family of solutions (9.81) is a continuation of an
unperturbed periodic orbit with period 2πm for some positive integer m,
and let us consider the solutions of the equation (9.83) with the initial
condition y(0) = 0. Because the family (9.81) at  = 0 is a periodic orbit
of the unperturbed system (9.80), there is a number h such that
β cos x(y, 0) + cos z(y, 0) = h.
Thus, t → ht is a solution of equation (9.83). Since this solution is complete,
if  is sufficiently small, then the solution t → y(t, ) of system (9.83) such
that y(0, ) = 0 exists at least on the interval
0 ≤ t ≤
2πm
h .
Moreover, there is a positive function  → η() such that η(0) = 2πm/h
and
y(η(), )=2πm.
The corresponding vector function
t → (x(y(t, ), ), y(t, ), z(y(t, ), )) (9.91)
is a solution of system (9.77). Moreover, we have, for example, the equations
x(y(η(), ), ) = x(2πn, ) = x(0, ) = x(y(0, ), );
that is, the function s → x(y(s, ), ) is periodic with period η(). Of course,
the same is true for the function s → x(z(s, ), ), and it follows that, for582 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
each fixed small , the function (9.91) is a periodic solution of the ABC
system.
As we have seen, the unperturbed system (9.80) has a period annulus A
surrounding the origin whose boundary contains hyperbolic saddle points.
These saddle points are fixed points of the stroboscopic Poincar´e map that
persist under perturbation (see Exercise 9.24). In the perturbed system
their continuations are unstable periodic orbits, as are the corresponding
periodic orbits of the ABC system. This fact is important for proving the
hydrodynamic instability of the ABC systems (see [106]).
Exercise 9.24. Prove that the hyperbolic saddle points in the phase plane for
the unperturbed system (9.80) viewed as periodic orbits in the corresponding
phase cylinder persist as hyperbolic periodic orbits under perturbation in sys￾tem (9.80) and that these perturbed periodic orbits are hyperbolic saddle type
periodic orbits for the corresponding ABC system.
As the periodic orbits in A approach the outer boundary of this period
annulus, the corresponding periods increase without bound. Therefore, the
period annulus A is certainly not isochronous. We might expect this period
annulus to be regular. But, because this is not always the case, the per￾turbation analysis for this problem is complicated. Note, however, that the
continuation theory will apply if we can find a resonant unperturbed peri￾odic orbit Γ for the system (9.80) such that the derivative of the associated
period function does not vanish at Γ and simultaneously the associated
subharmonic Melnikov function (8.70) has simple zeros.
As discussed in the last section, periodic orbits in A are in one-to-one
correspondence with their energy h, with 1 − β<h< 1 + β. Also, let us
consider the corresponding period function h → T(h) where T(h) is the
minimum period of the periodic orbit denoted Γ(h). Because the perturba￾tion terms are periodic with period 2π, the periodic orbit Γ(h) is in (m : n)
resonance provided that
2πm = nT(h).
Let us fix h and assume for the moment that T
(h) = 0 so that the
required regularity assumption is satisfied, and let us consider the solution
y → (x(y), z(y)) of the unperturbed system with orbit Γ(h) and initial
condition (x(0), y(0)) = (0, arccos(h − β)). Because the divergence of the
unperturbed system vanishes, simple zeros of the function
M(φ) = 1
h2
 2πm
0
(sin(z(y + φ)) sin y + β sin(x(y + φ)) cos y) dy
correspond to continuation points. By an argument similar to the one used
to derive equation (9.88), it is easy to show that
M(φ) = β2
h3 sin φ
 2πm
0
sin x(s) cos x(s) sin s ds.9.3 Origins of ODE: Fluid Dynamics 583
Thus, using our continuation analysis, in particular Theorem 8.57, we have
proved the following proposition.
Proposition 9.25. Suppose that Γ(h) is an (m : n) resonant unperturbed
periodic orbit of system (9.80) with energy h in a period annulus with period
function T and s → (x(s), y(s)) is an unperturbed solution with orbit Γ(h).
If T
(h) = 0 and
I(h) :=  2πm
0
sin(2x(s)) sin s ds = 0, (9.92)
then there are 2m continuation points on Γ(h).
To apply Proposition 9.25 to prove that there are in fact 2m continuation
points on the orbit Γ(h) we must show that I(h) = 0 and T
(h) = 0.
But, even if we cannot do this rigorously, our analysis is still valuable. For
example, if we fix β, then we can use numerical approximations to graph the
functions I and T as an indication of the validity of the requirements. This
is probably a more reliable method than a direct search for the periodic
solutions by numerical integration of the perturbed differential equations.
There is no simple argument to show that I(h) = 0. In fact, for most res￾onances, I(h) vanishes and our first-order method fails. A precise statement
is the content of the next proposition.
Proposition 9.26. If n = 1, then I(h)=0.
Proof. To prove the proposition, use the periodicity of the integrand to
recast the integral as
I(h) =  πm
−πm
sin(2x(s)) sin s ds.
Then, by the change of variables s = mσ and the resonance relation we
have that
I(h) = m
 π
−π
sin 
2x
nT(h)
2π σ
 sin mσ ds.
The function
t → sin 
2x
T(h)
2π t

)
is odd and 2π-periodic. Thus, it can be represented by a (convergent)
Fourier sine series, say
	∞
ν=1
bν(h) sin νt.
If this series is evaluated at t = nσ and inserted into the integral, all but
one of the summands vanishes. The exceptional term is
bν(h)
 π
−π
sin nνσ sin mσ dσ584 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
with nν = m. Since m and n are relatively prime, this term can only be
nonzero if n = 1 and ν = m, as required. Moreover, I(h) = 0 if and only if
the Fourier coefficient bν(h) = 0. ✷
Exercise 9.27. Prove: If t → y(t) is an odd periodic function with period 2π/ω
and 2πn/ω = 2πm/Ω for relatively prime integers m and n with n > 1, then
 2πn/ω
0
y(t) sin Ωt dt = 0.
Although an antiderivative for the integrand of I(h) at an (m : 1) res￾onance cannot be expressed in elementary functions, this integral can be
evaluated using Jacobi elliptic functions. We will indicate the procedure for
doing this below. Unfortunately, the resulting value seems to be too com￾plex to yield a simple statement of precisely which of the (m : 1) resonances
are excited at first order. Therefore we will not give the full derivation here.
Instead, we will use this problem to introduce the Jacobi elliptic functions
and the Picard-Fuchs equation for the period function. For a partial result
on the existence of continuable periodic orbits see [44] and Exercise 9.28.
In fact, most of the (m : 1) resonances are excited.
Exercise 9.28. This exercise is a research project. For which (m : 1) resonances
of system (9.80) is the integral (9.92) not zero?
Let us now glimpse into the wonderful world of elliptic integrals, a gem
of nineteenth-century mathematics that remains a very useful tool in both
modern pure and applied mathematics (see [35] and [262]). Perhaps the best
way to approach the subject of special functions is to view it in analogy
with trigonometry. The trigonometric functions are so familiar that we tend
not to notice how they are used. Often, we operate with these functions
simply by using their properties—periodicity and trigonometric identities.
We do not consider their values, except at a few special values of their
arguments. The complete elliptic integrals and the Jacobi elliptic functions
that we will mention below can be treated in the same way. Of course, it
is clear why the trigonometric functions show up so often: Circles appear
everywhere in mathematics! The reason why elliptic functions show up so
often is deeper; perhaps after more familiarity with the subject this reason
will become apparent.
What are elliptic functions? For 0 ≤ φ ≤ π/2 and 0 ≤ k ≤ 1, define
u := u(φ, k) =  φ
0
1
1 − k2 sin2 θ
dθ.9.3 Origins of ODE: Fluid Dynamics 585
The Jacobi elliptic functions are functions of two variables defined as fol￾lows:
sn(u, k) := sin φ, cn(u, k) := cos φ, dn(u, k) := 1 − k2 sn2(u, k)
where the argument k it is called the elliptic modulus. The complete elliptic
integrals of the first and second kinds are defined, respectively, by
K(k) :=  π/2
0
1
1 − k2 sin2 θ
dθ, E(k) :=  π/2
0

1 − k2 sin2 θ dθ.
The domain of the Jacobi elliptic functions can be extended to the entire
complex plane where each of these functions is “doubly periodic”; for exam￾ple, sn has the periods 4K(k) and 2iK(
√1 − k2 ), and cn has the periods
4K(k) and 2K(k)+2iK(
√1 − k2 ). In fact, more generally, a doubly peri￾odic meromorphic function for which the ratio of its periods is not real is
called an elliptic function. By the definitions of the Jacobi elliptic functions,
we have the identities
sn2(u, k) + cn2(u, k)=1, dn2(u, k) + k2 sn2(u, k)=1.
These are just two simple examples of the many relations and identities
that are known.
Exercise 9.29. Consider the solution t → (x(t), y(t)) of the system of differen￾tial equations ˙x = −y, ˙y = x with the initial condition x(0) = 1 and y(0) = 0,
and define the sine and cosine functions by
(x(t), y(t)) = (cos t, sin t).
Prove the basic trigonometric identities and periodicity properties of the sine and
cosine using this definition. Also, prove that
θ =
 sin θ
0
1
√1 − s2 ds.
Suppose that 0 <k< 1 and consider the solution of the system of differential
equations
x˙ = yz, y˙ = −xz, z˙ = −k2
xy
with initial condition (x(0), y(0), z(0)) = (0, 1, 1). Show that this solution is given
by
(x(t), y(t), z(t)) = (sn(t, k), cn(t, k), dn(t, k)).
If this solution is taken as the definition of the Jacobi elliptic functions, then it
is possible to derive many of the most important properties of these functions
without too much difficulty (see [24, p. 137]).
Exercise 9.30. Consider the pendulum model given by ¨θ + λ sin θ = 0, define
the phase plane in the usual manner by defining a new variable v := ˙
θ, and note586 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
that there is a center at the origin of the phase plane. The period function for the
corresponding period annulus is not constant. Fill in the details of the following
derivation of a formula for this period function.
If the periodic orbit meets the θ-axis at θ = θ0, then the energy surface corre￾sponding to the periodic orbit is the graph of the relation
v2 = 2λ(cos θ − cos θ0).
Note that dθ/dt = v and consider the symmetries of the periodic orbit to deduce
that the period T of the orbit is given by
T = 4
√2λ
 θ0
0
1
√cos θ − cos θ0
dθ.
Use the identity cos θ = 1 − 2 sin2(θ/2) to rewrite both of the terms cos θ and
cos θ0 in the integrand, and then change variables in the integral using
sin φ = sin(θ/2)
sin(θ0/2)
to obtain the formula
T = 4
√λ
 π/2
0
1
1 − k2 sin2 φ
dφ = 4
√λ
K(k), k = sin(θ0/2).
Show that the limit of the period function as the periodic orbits approach the
origin is T(0) := 2π/√λ and that the period function grows without bound as
the periodic orbits approach the outer boundary of the period annulus. Suppose
that the bob of a physical pendulum is pulled out 15◦, 30◦, or 90◦ from the
downward vertical position and released from rest. Approximate the periods of
the corresponding periodic motions using a numerical integration or a careful
analysis of the series expansion of K in powers of k. What percent error is made
if these periods are approximated by T(0)? (Galileo is said to have deduced
that the period of the librational motion of a pendulum does not depend on its
amplitude. He made this deduction while sitting in a cathedral and observing a
chandelier swinging in the breeze blowing through an open window. Discuss his
theory in light of your approximations.)
How do the elliptic functions arise for the ABC flows? To answer this
question, let us consider the solution y → (x(y), z(y)) of the unperturbed
system (9.80) defined above with the initial condition
x(0) = 0, z(0) = arccos((h − β))
and note that the corresponding orbit Γ(h) meets the positive x-axis at the
point with coordinates
(arccos((h − 1)/β), 0).
The first equation of the unperturbed system (9.80) can be rewritten in
the form
1
sin z(y)
x
(y) = 1
h.9.3 Origins of ODE: Fluid Dynamics 587
If we restrict attention for the moment to the portion of Γ(h) in the first
quadrant with y > 0, then after integration, we have the identity
y
h =
 y
0
x
(τ )
sin z(τ )
dτ.
If we apply the change of variables s = x(τ ) followed by t = − cos s and
rearrange the integrand, then we have the identity
βy
h =
 − cos x(y)
c
1
√a − t
√b − t
√t − c
√t − d
dt
where
a := 1, b := 1 − h
β , c := −1, d := −1 + h
β
and a>b ≥ − cos x(y) >c>d. This integral can be evaluated using the
Jacobi elliptic functions (see [35, p. 112]) to obtain
βy
h = β sn−1(sin φ, k)
where
k2 = (1 + β)2 − h2
4β , sin φ =
 2β(1 − cos x(y))
(1 − h + β)(1 + h − β cos x(y)
1/2
.
It follows that
cos x(y) = 1 − A2 sn2(βy/h, k)
1 − B2 sn2(βy/h, k)
with
A2 := (1 − h + β)(1 + h)
2β , B2 := 1 − h + β
2 ,
and, using the trigonometric identity sin2 θ + cos2 θ = 1, we also have
sin x(y) = √
2
A2 − B2 sn(βy/h, k) dn(βy/h, k)
1 − B2 sn2(βy/h, k) .
Moreover, it is easy to see that the solution formulas for sin x(y) and
cos x(y) are valid for all y.
The function sn has real period 4K; and therefore, the period of Γ(h) is
given by
T = 4h
√β
K(k(h)) = 8C2 − k2 K(k)
where
C2 = (1 + β)2
4β .588 9. Homoclinic Orbits, Melnikov’s Method, and Chaos
Because dh/dk < 0, the critical points of T are in one-to-one correspon￾dence with the critical points of the period function viewed as a function
of the elliptic modulus k.
There is a beautiful approach to the study of the monotonicity properties
of T that is based on the following observation: The derivatives of the
complete elliptic integrals E and K can be expressed as linear combinations
(with function coefficients) of the same complete elliptic integrals. In fact,
we have
E
(k) = E(k) − K(k)
k , K
(k) = E(k) − (1 − k2)K(k)
k(1 − k2) .
Of course, this means that K and E can also be expressed in the same
manner. As a result, the three expressions for T(k), T
(k), and T(k) are
all linear combinations of the two functions E(k) and K(k). Thus, T, T
,
and T must be linearly dependent; that is, T satisfies a second-order
differential equation. In fact, T satisfies the Picard-Fuchs equation
C2 − k2
k(1 − k2)
T+
k4 + (1 − 3C2)k2 + C2
k2(1 − k2)2 T
+ (1 − 2C2)k2 + C2(2 − C2)
k(C2 − k2)(1 − k2)2 T = 0.
The function T is positive, 0 <k< 1, and 1 < C2 < ∞. By the Picard￾Fuchs equation, if T
(k) = 0, then the sign of T(k) is the same as the sign
of the expression
C := C2(C2 − 2) + (2C2 − 1)k2.
We also have the Taylor series expansion
T(k)=4πC + π(C2 − 2)
C k2 + O(k4).
These facts are the key ingredients required to prove the following two
propositions: 1) If C2 > 2, then T has no critical points. 2) T has at most
two critical points. The proofs are left as exercises.
Exercise 9.31. (a) Prove: If f and g are two functions such that f
, f, g
, and
g are all linear combinations (with function coefficients) of f and g, then every
linear combination T of f and g is a solution of a homogeneous second-order ODE.
(b) Find a second-order ODE satisfied by the function x → a sin x+b cos x where
a and b are constants. Prove that this function does not have a positive relative
minimum. (c) Repeat part (b) for the function x → x sin x + x cos x. (d) Repeat
part (b) for the function x → Jν(x) + J
ν(x) where Jν is the Bessel function
of the first kind of order ν and a is a constant. Hint: Recall Bessel’s equation
x2J
ν +xJ
ν+(x2−ν2)Jν = 0. (e) Formulate a general theorem that uses properties9.3 Origins of ODE: Fluid Dynamics 589
of the coefficients of the second-order ODE satisfied by T and the asymptotics
of T at the origin to imply that T is a monotone function. Apply your result to
prove that the function T : (0, 1) → R given by T(k)=2E(k) − (2 − k2)K(k) is
negative and monotone decreasing (see [49, page 290]).10
Averaging
This chapter is an introduction to the method of averaging—a far-reaching
and rich mathematical subject that has many important applications. Our
approach to the subject is through perturbation theory; for example, we will
discuss the existence of periodic orbits for periodically forced oscillators.
In addition, some ideas will be introduced that have implications beyond
the scope of this book.
We have already discussed in Section 6.2 applications of the method of
averaging to various perturbations of a Keplerian binary. While an under￾standing of these applications is not required as background for the math￾ematical theory in this chapter, a review of the Keplerian perturbation
problem in celestial mechanics is highly recommended as a wonderful way
to gain an appreciation for the subject at hand. For further study there are
many excellent mathematical treatments of the theory and applications of
the method of averaging (see, for example, [13], [15], [122], [165], [189], [227]
and [119], [151], [164], [194], [261]).
10.1 The Averaging Principle
Let us consider a family of differential equations given by
u˙ = f(u) + g(u, t, ), u ∈ Rn, (10.1)
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
C. Chicone, Ordinary Differential Equations with Applications, Texts in
Applied Mathematics 34, https://doi.org/10.1007/978-3-031-51652-8 10
591592 10. Averaging
where the perturbation term is periodic in time with period η > 0. Also,
let us suppose that the unperturbed system
u˙ = f(u), u ∈ Rn (10.2)
is completely integrable (see Exercise 8.36). In this case, a theorem of
Joseph Liouville states that the phase space for the unperturbed system is
foliated by invariant tori (see [13]). We will not prove this result; instead,
we will simply assume that a region of the phase space of our unperturbed
system is foliated by invariant tori. In the planar case, this is exactly the
assumption that the unperturbed system has a period annulus.
The method of averaging is applied after our system is transformed to
the standard form
˙
I = F(I,θ), ˙
θ = ω(I) + G(I,θ).
The new coordinates (I,θ), called action-angle variables, are always defined
under our integrability assumption on the unperturbed system. To illus￾trate this result, we will construct the action-angle variables for the har￾monic oscillator and outline the construction for a general planar Hamil￾tonian system; the Delaunay elements, defined for the Kepler problem in
Section 6.2, provide a more substantial example.
The harmonic oscillator is the Hamiltonian system
q˙ = p, p˙ = −ω2q, (10.3)
with Hamiltonian H : R × R → R, which represents the total energy, given
by
H(q, p) := 1
2
p2 +
1
2
ω2q2.
For this system, the entire punctured plane is a period annulus, the periodic
orbits correspond to regular level sets of H , and each level set is an ellipse.
Similarly, if a general one-degree-of-freedom Hamiltonian system
q˙ = ∂H
∂p , p˙ = −∂H
∂q (10.4)
with Hamiltonian H : R × R → R has a period annulus A, then each
periodic orbit in A is a subset of a regular level set of H .
In case A is a period annulus for the Hamiltonian system (10.4), let
O(q0, p0) denote the periodic orbit that passes through the point (q0, p0) ∈
A, and note that O(q0, p0) is a subset of the regular energy surface
{(q, p) ∈ R2 : H(q, p) = H(q0, p0)}.
The function I : A → R defined by
I(q0, p0) := 1
2π

O(q0,p0)
p dq (10.5)10.1 The Averaging Principle 593
is called the action variable for the Hamiltonian system on the period annu￾lus. Its value at (q0, p0) is the normalized area of the region in the phase
space enclosed by the periodic orbit O(q0, p0). (The action variable should
not be confused with the action integral of a mechanical system, which is
the integral of its Lagrangian over a motion.)
For the harmonic oscillator, the action variable at (q0, p0) = (0, 0) is 1/(2π)
multiplied by the area enclosed by the ellipse O(q0, p0) with equation
1
2
p2 +
1
2
ω2q2 = 1
2
a2
where a := (p2
0+ω2q2
0)1/2. The area of the ellipse (π times the product of the
lengths of its semimajor and semiminor axes) is a2/ω. Therefore, the action
variable for the harmonic oscillator is proportional to its Hamiltonian; in
fact,
I(q, p) = a2
2ω = 1
ω
H(q, p).
Since the Hamiltonian is constant on orbits, the action variable is a first
integral, that is, ˙
I = 0.
For our planar Hamiltonian system (10.4), let Σ be a Poincar´e section
in the period annulus A, and let T : A → R denote the period function;
it assigns to (q, p) ∈ A the period of the periodic orbit O(q, p). Also, let
t → (Q(t), P(t)) denote the solution corresponding to O(q, p) that has its
initial value on Σ. Since, by the definition of the action variable,
I(q, p) = 1
2π
 T(q,p)
0
P(s)Q˙(s) ds,
the function t → I(q(t), p(t)) is constant for every solution t → (q(t), p(t))
on O(q, p); that is, ˙
I = 0.
To define the angle variable, let us first define the time map τ : A → R;
it assigns to each point (q, p) ∈ A the minimum positive nonnegative
time required to reach (q, p) along the solution of the system that starts
at the intersection point of the orbit O(q, p) and the section Σ. With this
notation, the angular variable θ is defined by
θ(q, p) := 2π
T(q, p)
τ (q, p). (10.6)
For the harmonic oscillator, every periodic orbit has the same period
2π/ω. Moreover, if we take the section Σ to be the positive q-axis, then
q = a
ω cos(ωτ (q, p)), p = −a sin(ωτ (q, p)).
Since ˙q = p, we have that
−a sin(ωτ (q, p)) d
dt(τ (q, p)) = −a sin(ωτ (q, p)),594 10. Averaging
and therefore, ˙τ = 1. Hence, the angular variable satisfies the differential
equation ˙
θ = ω.
In the general case, the frequency of the periodic orbit may be a noncon￾stant function of the action variable, and the differential equation for the
angular variable has the form ˙
θ = ω(I). To prove this fact, note first that
if t → (q(t), p(t)) is a solution, then t → T(q(t), p(t)) is a constant function
whose value is the period of the corresponding periodic orbit. For nota￾tional convenience, let t → (Q(t), P(t)) denote the solution corresponding
to the same periodic orbit that has its initial value on the section Σ. Using
the definition of τ , we have the identities
Q(τ (q(t), p(t))) = q(t), P(τ (q(t), p(t))) = p(t).
In particular, t → (Q(τ (q(t), p(t))), P(τ (q(t), p(t)))) is a solution of the
original Hamiltonian system. More generally, suppose that the function
t → u(t) is a complete solution of the differential equation ˙u = f(u) and
t → u(γ(t)) is also a solution for some function γ : R → R. Clearly t →
u(t + γ(0)) is a solution of the differential equation with the same initial
condition as t → u(γ(t)). Hence, u(γ(t)) = u(t + γ(0)) for all t ∈ R, and by
differentiating both sides of this identity with respect to t, we have that
f(u(γ(t))) ˙γ(t) = f(u(t + γ(0))).
Since u(γ(t)) = u(t + γ(0)) and f does not vanish along the orbit, we
have the equation ˙γ = 1. By applying this result to the Hamiltonian sys￾tem, it follows that ˙τ = 1, and therefore ˙
θ = 2π/T(q, p). Clearly, T is
constant on each orbit O(q, p). Since the action variable increases as the
areas bounded by the periodic orbits in the period annulus increase, the
frequency ω(q, p) := 2π/T(q, p) can be viewed as a function of the action
variable and ˙
θ = ω(I), as required.
In Section 10.3, we will prove that the function
(q, p) → (I(q, p), θ(q, p))
defines a polar coordinate chart on an annular subset of A. It follows that
the change to action-angle variables is nonsingular, and the Hamiltonian
system in action-angle variables, that is, the system
˙
I = 0, ˙
θ = ω(I)
can be viewed as a system of differential equations on the phase cylinder,
which is the product of a line with coordinate I and a one-dimensional
torus with coordinate θ (see Section 1.7.6).
More generally, a multidimensional integrable system has an invariant
manifold that is topologically the cross product of a Cartesian space RM
and a torus TN . In this case, action-angle variables I and θ can be defined
in RM × TN such that the integrable system is given by
˙
I = 0, ˙
θ = ω(I) (10.7)10.1 The Averaging Principle 595
where I ∈ RM and θ ∈ TN are vector variables. This is the standard
form for an integrable system, the starting point for classical perturbation
theory.
The method of averaging is a powerful tool that is used to obtain and
analyze approximate solutions for perturbations of integrable systems, that
is, for systems of differential equations of the form:
˙
I = F(I,θ), ˙
θ = ω(I) + G(I,θ), (10.8)
where θ is a vector of angular variables defined modulo 2π, the functions
θ → F(I,θ) and θ → G(I,θ) are 2π-periodic, and || is considered to
be small. Poincar´e called the analysis of system (10.8) “the fundamental
problem of dynamical systems.”
In physical applications, mathematical models are rarely formulated
directly in action-angle variables. As illustrated by the analysis of the per￾turbed Kepler problem in Section 6.2, the transformation to action-angle
variables can be a formidable task. On the other hand, the benefits of
working with a system in the standard form (10.8) often justify the effort
to obtain the coordinate transformation.
An immediate benefit derived from the transformation to action-angle
variables is the simplicity of the geometry of the unperturbed dynamics of
system (10.7). In fact, the solution with the initial condition (I,θ)=(I0, θ0)
is given by I(t) ≡ I0 and θ(t) = ω(I0)t + θ0. Note that the action variables
specify a torus in the phase space, and the angle variables evolve linearly
on this torus.
Definition 10.1. Suppose that I0 is in RM. The N-dimensional invariant
torus
{(I,θ) ∈ RM × TN : I = I0}
for the system (10.7) is resonant if there is a nonzero integer vector K
of length N such that K, ω(I0) = 0 where   denotes the usual inner
product. In this case we also say that the frequencies, the components of
the vector ω(I0), are in resonance.
If an invariant torus for the system (10.7) is not resonant, then every
orbit on the torus is dense. In case N = 2, every orbit on a resonant torus
is periodic. Matters are not quite so simple for N > 2 where the existence
of a resonance relation does not necessarily mean that all orbits on the
corresponding invariant torus are periodic. This is just one indication that
the dynamics of systems with more than two frequencies is in general quite
different from the dynamics of systems with one frequency. But, in all
cases, the existence of resonant tori plays a central role in the analysis of
the perturbed dynamical system.
Some aspects of the near-resonance behavior of the planar case of sys￾tem (10.1) are discussed in detail in Chapter 8, especially the continuation
theory for resonant unperturbed periodic solutions. As we have seen, this596 10. Averaging
special case and the general multidimensional time-periodic system (10.1)
can be viewed as systems on a phase cylinder by the introduction of a new
angular variable so that the extended system is given by
u˙ = f(u) + g(u, τ, ), τ˙ = 1. (10.9)
If u ∈ R2 and the system ˙u = f(u) has a period annulus A, then it is
integrable. In this case, a subset of the three-dimensional phase space for
the extended system (10.9) at  = 0 is filled with invariant two-dimensional
tori corresponding to the periodic orbits in A. Thus, there is one action
variable, which has a constant value on each periodic orbit, and two angle
variables. One of the angular variables is τ ; the other is the angle variable
defined for the action-angle variables of the unperturbed planar system.
The basic idea that leads to the development of the method of averaging
arises from an inspection of system (10.8). In particular, since || is assumed
to be small and the time derivatives of the action variables are all propor￾tional to , the action variables will remain close to their constant unper￾turbed values (while undergoing small-amplitude high-frequency oscilla￾tions due to the relatively fast angular velocities) on a time interval whose
length is inversely proportional to ||. Thus, on this time interval, we would
expect to obtain a close approximation to the slow evolution of the action
variables (with their small-amplitude high-frequency oscillations removed)
by averaging the (slow) action variables over the (fast) angle variables.
In most applications, we are interested in the evolution of the action vari￾ables, not the angle variables. For example, let us note that the semimajor
axis of the orbit of a planet about a star is given by an action variable.
While a determination of the planet’s exact position in the sky requires
the specification of an action and an angle variable, a prediction of the
long-term evolution of the relative distance between the planet and the
star is probably more interesting (and more realistic) than a prediction of
the planet’s exact position relative to the star at some point in the distant
future.
The Averaging Principle. Let t → (I(t), θ(t)) be a solution of sys￾tem (10.8), and let t → J(t) denote the solution of the corresponding initial
value problem
J˙ = F¯(J), J(0) = I(0) (10.10)
(called the averaged system) where F¯ is the function defined by
F¯(J) := 1
(2π)N

TN
F(J, θ) dθ.
If  > 0 is small, then there are constants C > 0 and τ > 0 such that
|I(t) − J(t)| < C as long as 0 ≤ t < τ ; that is, the solution of the
averaged system is a useful approximation of the evolution of the action
variables of system (10.8).10.1 The Averaging Principle 597
The averaging principle has a long history, which is deeply rooted in
perturbation problems that arise in celestial mechanics (see, for example,
[227]); but, the averaging principle is not a theorem. Nevertheless, in a phys￾ical application, it might be reasonable to replace a mathematical model,
which is given in the form of the differential equation (10.8), with the cor￾responding averaged system (10.10), to use the averaged system to make a
prediction, and to then test the prediction against the results of a physical
experiment.
The next theorem (the averaging theorem) validates the averaging prin￾ciple under the hypothesis that there is exactly one angle variable. In this
case, there is a 2π-periodic change of variables for system (10.8) such that
the transformed differential equation decouples, and the first-order trun￾cation with respect to  of its action variables is exactly the averaged sys￾tem (10.10). The existence of such “averaging transformations” is the essen￾tial ingredient used in the proof of the averaging theorem and the central
issue of the subject.
Theorem 10.2 (Averaging Theorem). Suppose that system (10.8) is
defined on U × T where U is an open subset of RM.
(i) If there is some number λ such that ω(I) >λ> 0 for all I ∈ U,
then there is a bounded open ball B contained in U, a number 1 > 0,
and a smooth function k : B × T → B such that for each I ∈ B the
function θ → k(I,θ) is 2π-periodic, the function I → I + k(I,θ) is
invertible on B for 0 ≤ <1, and the change of coordinates given
by
L = I + k(I,θ) (10.11)
transforms the system (10.8) to the form
L˙ = F¯(L) + 
2F1(L, θ, ), ˙
θ = ω(L) + G1(L, θ, ), (10.12)
where
F¯(L) = 1
2π
 2π
0
F(L, θ) dθ
and both of the functions F1 and G1 are 2π-periodic with respect to
their second arguments.
(ii) If in addition T > 0, B0 is an open ball whose closure is contained
in the interior of B, and if for each I0 ∈ B the number τ (I0) denotes
the largest number less than or equal to T such that the solution of
the averaged system (10.10) with initial condition J(0) = I0 is in
the closure of B0 for 0 ≤ t ≤ τ (I0), then there are positive numbers
2 ≤ 1 and C such that for each I0 ∈ B0 and for 0 ≤ <2
all solutions t → (I(t), θ(t)) of the system (10.8) with initial value
I(0) = I0 are approximated by the solution t → J(t) of the averaged598 10. Averaging
system (10.10) with J(0) = I0 as follows:
|I(t) − J(t)| < C
on the time interval given by 0 ≤ t < τ (I0).
Proof. To prove statement (i), define a new function F on U × T given
by
F(L, θ) := F(L, θ) − F¯(L),
and let k denote the solution of the differential equation
∂k
∂θ (L, θ) = − 1
ω(L)
F(L, θ) (10.13)
with the initial condition k(L, 0) = 0; that is, k is given by
k(L, θ) = − 1
ω(L)
 θ
0
F˜(L, s) ds.
The function k is defined on U × T, and the function θ → k(L, θ) is
2π-periodic. To prove the periodicity, fix L, define the new function
k(θ) = k(L, θ + 2π) − k(L, θ),
and note that k(0) = 0 and k
(θ) ≡ 0. (Our definition of k is given using
the assumption that there is only one angle. The existence of a correspond￾ing 2π-periodic function is problematic when there are several angles (see
Exercise 10.4).)
We will show that the relation L = I + k(I,θ) defines a coordinate
transformation by proving that the corresponding function (I,θ) → (L, θ)
is invertible. To this end, consider the smooth function K : U ×U ×T×R →
RM given by
(I, L, θ, ) → I + k(I,θ) − L.
For each point ξ = (L, θ) in c (B)×T (where c  denotes the closure of the
set), we have that K(L, L, θ, 0) = 0 and the partial derivative KI (L, L, θ, 0)
is the identity transformation of RM. Therefore, by the implicit function
theorem, there is a product neighborhood Γξ ×γξ contained in (U ×T)×R
and containing the point (ξ, 0), and a smooth function Hξ : Γξ × γξ → U
such that
Hξ(L, θ, ) + k(Hξ(L, θ, ), θ) = L
for all ((L, θ), ) ∈ Γξ × γξ. In other words, the function L → Hξ(L, θ, ) is
a local inverse for the function I → I + k(I,θ). Moreover, if (I,(L, θ), ) ∈
U × Γξ × γξ is such that K(I, L, θ, ) = 0, then I = Hξ(L, θ, ).
Since c (B) × T is compact, there is a finite collection of the neighbor￾hoods Γξ×γξ that cover B×T. Let Γ denote the union of the corresponding10.1 The Averaging Principle 599
Γξ, and let γ denote the intersection of the corresponding intervals on the
real line. We have B ⊂ Γ, and there is some 0 such that γ contains the
closed interval [0, 0].
The function k has a global Lipschitz constant Lip(k) on the compact
set c (B) × T. Let us define 1 > 0 such that
1 < min  1
Lip(k)
, 0

.
If θ ∈ T and 0 ≤  ≤ 1, then the map I → I + k(I,θ) is injective. In fact,
if
I1 + k(I1, θ) = I2 + k(I2, θ),
then
|I1 − I2| = || Lip(k)|I1 − I2| < |I1 − I2|,
and therefore I1 = I2. It follows that there is a function H : Γ×T×[0, 1] →
B such that H is the “global” inverse; that is,
H(L, θ, ) + k(H(L, θ, ), θ) = L.
By the uniqueness of the smooth local inverses Hξ, the function H must
agree with each function Hξ on the intersection of their domains. Thus, H
is smooth and we have defined a coordinate transformation L := I+k(I,θ)
on B ×T×[0, 1]. Moreover, by expanding H in a Taylor series at  = 0 and
with the first-order remainder given by H, we see that I = L + H(I, θ, ).
It is also easy to check that θ → H(I, θ, ) is a 2π-periodic function.
Using the coordinate transformation, we have that
L˙ = ˙
I + 
∂k
∂I ˙
I + 
∂k
∂θ ˙
θ
= F(I,θ) + 
∂k
∂I (I,θ)(F(I,θ)) + 
∂k
∂θ (I,θ)(ω(I) + G(I,θ))
= (F(I,θ) + ∂k
∂θ (I,θ)ω(I)) + 
2α(I,θ),
where
α(I,θ) := ∂k
∂I (I,θ)F(I,θ) + ∂k
∂θ (I,θ)G(I,θ).
Using the inverse transformation and Taylor’s theorem, there is a function
F1 such that
L˙ = (F(L, θ) + ∂k
∂θ (L, θ)ω(L)) + 
2F1(L, θ, ). (10.14)
After the formula for the partial derivative of k (equation (10.13)) is inserted
into equation (10.14), the new differential equation is given by
L˙ = (F(L, θ) + F¯(L) − F(L, θ)) + 
2β(L, θ, ).600 10. Averaging
Thus, the coordinate transformation (10.11) applied to the system (10.8)
yields a new system of the form
L˙ = F¯(L) + 
2F1(L, θ, ),
˙
θ = ω(L) + G1(L, θ, ). (10.15)
This completes the proof of statement (i).
To prove the asymptotic estimate in statement (ii), consider the differ￾ential equation for L−J obtained by subtracting the averaged system from
the first differential equation of the system (10.15) and then integrate to
obtain
L(t) − J(t) = L(0) − J(0) + 
 t
0
F¯(L(s)) − F¯(J(s)) ds
+ 
2
 t
0
F1(L(s), θ(s), ) ds.
If Lip(F¯) > 0 is a Lipschitz constant for F¯, and if B is an upper bound
for the function
(L, θ, ) → |F1(L, θ, )|
on the compact space c (B) × T × [0, 1], then we have the estimate
|L(t) − J(t)|≤|L(0) − J(0)| +  Lip(F¯)
 t
0
|L(s) − J(s)| ds + 
2Bt (10.16)
as long as L(t) remains in c (B).
An application of the specific Gronwall lemma from Exercise 2.3 to the
inequality (10.16) yields the following estimate:
|L(t) − J(t)| ≤ 
|L(0) − J(0)| +  B
Lip(F¯)

e Lip(F¯)t
.
We also have that
L(0) = I(0) + k(I(0), θ(0)), I(0) = J(0).
Thus, if 0 ≤ t ≤ τ (I(0)) (where τ is defined in the statement of the
theorem), then there is a constant C0 such that
|L(t) − J(t)| ≤ C0 (10.17)
as long as L(t) remains in c (B).
Note that L(t) is in c (B) whenever |L(t)−J(t)| is less than the minimum
distance between the boundaries of B0 and B. If 0 < 2 < 1 is chosen so
that C0/2 is less than this distance, then the estimate (10.17) ensures that
L(t) is in c (B) on the time interval 0 ≤ t ≤ τ (I(0)).10.2 Averaging at Resonance 601
Finally, let C1 be an upper bound for the function
(L, θ) → |k(L, θ)|
and note that for t in the range specified above we have the inequality
|I(t) − J(t)|≤|I(t) − L(t)| + |L(t) − J(t)| ≤ C1 + C0.
Therefore, with C := C0 + C1, we have the required asymptotic estimate.
✷
Exercise 10.3. (a) Write van der Pol’s equation
x¨ + (x2 − 1) ˙x + ω2
x = 0
as a first-order system and transform to action-angle variables. Hint: Use ˙x = ωy.
(b) Compute the averaged equation and show that it has a hyperbolic rest point.
(c) Compare the results of part (b) and Section 8.1.
Exercise 10.4. Consider the PDE
ω1kθ(θ, φ) + ω2kφ(θ, φ) = g(θ, φ),
where g is 2π-periodic with respect to the angular variables θ and φ. This differ￾ential equation can be viewed as a PDE on the two-dimensional torus T2. Does
the PDE have a 2π-periodic solution? More precisely, find sufficient conditions
on g, ω1, and ω2 such that periodic solutions exist. Hint: Look for a solution k
represented as a Fourier series. Note that expressions of the form mω1 + nω2,
where m and n are integers, will appear in certain denominators. If there are
resonances (that is, integers m and n such that mω1 + nω2 = 0) or an infi￾nite sequence of “small denominators,” then the Fourier series for k, which is
determined by equating coefficients after substitution of the Fourier series in
the PDE, will not converge for most choices of g. On the other hand, conver￾gence is possible if the frequencies satisfy an appropriate Diophantine condition
|mω1 + nω2| ≥ C(|m| + |n|)
−α, where C > 0 and α > 0 are constants and the
inequality holds for every nonzero integer vector (m, n). There is a vast literature
on the problem of small divisors (see, for example, [165]).
10.2 Averaging at Resonance
In this section we will demonstrate a remarkable fact: Some of the most
important features of the dynamical behavior near a resonance of a generic
multidimensional oscillator are determined by an associated one-degree-of￾freedom oscillator that resembles a perturbed pendulum with torque. We
will also give some examples to show that the averaging principle is not
always applicable in multifrequency systems.602 10. Averaging
Let us consider the system (10.1) with u ∈ RM+N where the period
of the perturbation is η = 2π/Ω and where the unperturbed system has
an invariant set that is foliated by N-dimensional invariant tori. In this
case, if action-angle variables (I,ϕ) ∈ RM × TN are introduced, then the
differential equation is expressed in the form
˙
I = F(I, ϕ, t) + O(
2),
ϕ˙ = ω(I) + G(I, ϕ, t) + O(
2). (10.18)
Moreover, it is 2π-periodic in each component of the N-dimensional vector
of angles and 2π/Ω-periodic in time. By introducing an additional angular
variable τ , system (10.18) is equivalent to the autonomous system with M
action variables and N + 1 angle variables given by
˙
I = F(I, ϕ, τ /Ω) + O(
2),
ϕ˙ = ω(I) + G(I, ϕ, τ /Ω) + O(
2),
τ˙ = Ω. (10.19)
Let us suppose that there is a resonance relation
K, ω(I) = nΩ, (10.20)
where K is an integer vector of length M and n is an integer such that
the components of K and the integer n have no common factors. The
corresponding set
RK,n := {(I, ϕ, τ ) : K, ω(I) = nΩ},
which is generally a hypersurface in the phase space, is called a reso￾nance manifold. Our goal is to describe the perturbed dynamics of the sys￾tem (10.19) near this resonance manifold. To do this, we will use yet another
set of new coordinates that will be introduced informally and abstractly.
In practice, as we will demonstrate later, the appropriate new coordinates
are chosen using the ideas of the abstract construction; but, their precise
definition depends on special features of the system being studied.
The set
AK,n := {I : K, ω(I) = nΩ},
is the intersection of the resonance manifold with the “action space.” This
set is generally a manifold in RM, sometimes also called a resonance man￾ifold. To distinguish AK,n from RK,n, let us call AK,n the resonance layer
associated with the resonance relation (10.20).
A point in the action space is determined by its distance from the reso￾nance layer and by its projection to the resonance layer. In particular, there10.2 Averaging at Resonance 603
are local coordinates defined in a neighborhood of the resonance layer, or
at least near a portion of this manifold, given by
r = K, ω(I) − nΩ, z = A(I),
where r is a measure of the distance of the point with action variable I
to the resonance layer and the (M − 1)-dimensional vector z is the vector
coordinate of the projection, denoted by the smooth map A, of the point
I to the resonance layer AK,n.
For  = 0, let us define the new stretched distance-coordinate ρ = r/√
and new angular variables
ψ = K, ϕ − nτ, χ = B(ϕ, τ )
where the vector function B : TN+1 → TN+1 is chosen so that the transfor￾mation to the new angles is invertible. Of course, B must also be 2π-periodic
in each component of ϕ and in τ .
In the coordinates ρ, z, ψ, and χ, system (10.19) has the form
ρ˙ = √K, Dω(I)F(I, φ, τ /Ω) + O(),
z˙ = O(),
ψ˙ = √ρ + O(
3/2),
χ˙ = O(1), (10.21)
where I, φ, and τ are viewed as functions of ρ, z, ψ, and χ.
In system (10.21), ρ and ψ are slow variables, the M − 1 variables repre￾sented by the vector z are “super slow,” and χ is an N-dimensional vector
of fast variables. In keeping with the averaging principle, we will average
over the fast (angular) variables, although we have provided no theoretical
justification for doing so unless N = 1. The system of differential equations
obtained by averaging over the fast variables in system (10.21) is called the
partially averaged system at the resonance.
To leading order in μ := √, the partially averaged system at the reso￾nance is
ρ˙ = μK, Dω(I)F∗(I,ψ),
z˙ = 0,
ψ˙ = μρ, (10.22)
where F∗ is obtained by averaging the function
χ → F(I,φ(ψ, χ), τ (ψ, χ)/Ω).
Here we have used the names of the original variables for the corresponding
averaged variables even though this is a dangerous practice. The solutions604 10. Averaging
of the partially averaged equations are not the same as the solutions of the
original system.
The function F∗ is periodic in its second argument with period some
integer multiple of 2π. In particular, using Fourier series, there is a constant
vector c(I) and a vector-valued periodic function ψ → h(I,ψ) with zero
average such that
F∗(I,ψ) = c(I) + h(I,ψ).
Also, we can easily obtain the expansion, in powers of μ, of the function
Dω expressed in the new coordinates. In fact, because
I(r, z) = I(μρ, z) = I(0, z) + O(μ),
it follows that Dω(I) = Dω(I(0, z)) + O(μ).
Under the generic assumption
K, Dω(I(0, z))F∗(I(0, z), ψ) = 0
(that is, the vector field corresponding to the averaged system is transverse
to the resonance manifold), and, in view of the form of system (10.22) and
the above definitions, there are real-valued functions z → p(z) and (z,ψ) →
q(z,ψ) such that the function (z, φ) → p(z) + q(z, φ) is not identically zero
and the first-order approximation to the partially averaged system has the
form
ρ˙ = μ(p(z) + q(z,ψ)),
z˙ = 0,
ψ˙ = μρ. (10.23)
Finally, by defining a slow time variable s = μt and taking into account
that z is a constant of the motion, we will view system (10.23) as the
following parametrized family of differential equations with parameter z:
dρ
ds = p(z) + q(z,ψ), dψ
ds = ρ ; (10.24)
or, equivalently,
d2ψ
ds2 − q(z,ψ) = p(z), (10.25)
where the function ψ → q(z,ψ) is periodic with average zero. For instance,
q might be given by q(z,ψ) = −λ sin ψ for some λ > 0.
In accordance with the usual physical interpretation of the differential
equation (10.25), we have just obtained a wonderful result: Near a res￾onance, every oscillator behaves like a pendulum influenced by a constant
torque.
The precise nature of the dynamical behavior near the resonance depends
on the functions p and q in the differential equation (10.25) and the10.2 Averaging at Resonance 605
Figure 10.1: Phase portrait of pendulum with “large” constant torque. All
orbits pass through the resonant value of the action variable.
perturbation terms that appear in the higher-order approximations of the
partially averaged system. In particular, let us note that the coefficients
of the pendulum equation are functions of the super slow variables. Thus,
they vary slowly with the slow time. Although a rigorous description of the
motion predicted by the partially averaged system is highly non-trivial and
not yet completely understood, our result certainly provides a fundamen￾tal insight into the near-resonance dynamics of oscillators. Also, this result
provides a very good reason to study the dynamics of perturbed pendulum
models.
Consider the simple pendulum with constant torque (equation (10.25)
with p(z) = c and q(z,ψ) := −λ sin ψ) given by
ρ˙ = c − λ sin ψ, ψ˙ = ρ (10.26)
where λ > 0 and c ≥ 0. The phase space of this system is the cylinder
(ρ, ψ) ∈ R×T. Also, let us note that the circle given by the equation ρ = 0
would correspond to the resonance manifold in our original oscillator.
If c/λ > 1, then ˙ρ > 0, and it is clear that all trajectories pass through
the resonance manifold as depicted in Figure 10.1. If, on the other hand,
c/λ < 1, then there are two rest points on the resonance manifold, a saddle
and a sink, and the phase portrait will be as depicted in Figure 10.2. In
particular, some orbits still pass through the resonance manifold, but now
the periodic orbits surrounded by the homoclinic loop are captured into the
resonance. These orbits correspond to orbits for which an action variable
librates near its resonant value on a long time scale. In the pendulum
model, the libration goes on forever. On the other hand, if a pendulum
system is obtained by partial averaging at a resonance, then its coefficients
are expected to vary slowly with time. In particular, the ratio c/λ will606 10. Averaging
Figure 10.2: Phase portrait for pendulum with “small” constant torque. The
region bounded by the homoclinic orbit corresponds to the trajectories that
are captured into resonance. The corresponding action variable oscillates
around its resonant value.
change over time and perhaps reach a value that exceeds one. In this case,
the corresponding action variable can drift away from its resonance value.
If the averaging procedure is carried to the next higher order in μ, then
a typical perturbation that might appear in the pendulum model is a small
viscous friction. For example, the perturbed system might be
ρ˙ = c − μρ − λ sin ψ, ψ˙ = ρ. (10.27)
The phase portrait of this system on the phase cylinder for the case c/λ < 1
is depicted in Figure 10.3. Note that there is a “thin” set of trajectories,
some with their initial point far from the resonance manifold, that are
eventually captured into the resonance. Again, because the coefficients of
system (10.27) will generally vary slowly with time, it is easy to imagine the
following scenario will occur for the original system: An action variable of
our multidimensional oscillator evolves toward a resonance, it is captured
into the resonance and begins to librate about its resonant value. After
perhaps a long sojourn near the resonance, the action variable slowly drifts
away from its resonant value. Meanwhile, the same action variable for a
solution with a slightly different initial condition evolves toward the reso￾nant value, but the values of the action variable pass through the resonance
without oscillating about the resonant value (see Figure 10.4).
Although the dynamics of the differential equation (10.25) are similar to
the dynamics of the simple pendulum, there are generally several alternat￾ing saddles and centers along the resonance manifold. After a perturbation,
there can be several “thin” subsets of the phase space corresponding to tra￾jectories that are eventually captured into the resonance. Again trajectories10.2 Averaging at Resonance 607
Figure 10.3: Phase portrait for pendulum with “small” constant torque and
“small” viscous friction. A thin strip of trajectories are captured into the
resonance. The corresponding action variable with initial condition in the
strip moves toward its resonant value and then begins to oscillate around
its resonant value.
t
I0
I
Figure 10.4: Two schematic time signals of an action variable I are depicted
for orbits with slightly different initial conditions. One time trace passes
through the resonant value I = I0; the other is captured into the resonance
on a long time scale before it leaves the vicinity of its resonant value.608 10. Averaging
that are captured into a resonance will tend to remain near the resonance
manifold on a long time interval. But as the remaining super slow action
variables drift, the trajectory will often move into a region near the reso￾nance manifold where it will pass through the resonance. After it reaches
this region, the trajectory will eventually move away from the influence of
the resonance—at least for a while. To complicate matters further, the set
of resonance manifolds is dense in the action space (the rational numbers
are dense in the real line), and, for the case of at least three angle vari￾ables, resonance manifolds corresponding to different integer vectors can
intersect. Thus, there is a complex web, called the Arnold web, of reso￾nance manifolds that each influence the perturbed motion of nearby orbits.
Although the precise dynamics in the phase space and the corresponding
fluctuations of the action variables is usually very difficult to analyze, the
resonance capture mechanism, which is partly responsible for the complex￾ity of the motions in phase space for dissipative systems, is made reasonably
clear by our analysis.
The study of pendulum-like equations with slowly varying parameters is
the subject of hundreds of research articles. You should now see why there
is so much interest in such models. Perhaps the simplest case to analyze
is the pendulum with periodic forcing or with periodic changes in some of
its parameters. While we have not discussed all of the known dynamical
behavior associated with such models, we have discussed the possibility
that periodic orbits continue (Chapter 8) and chaotic invariant sets appear
(Chapter 9). This general subject area is certainly not closed; it remains a
fruitful area of mathematical research (see, for example, [126], [183]).
Exercise 10.5. Consider the pendulum model with slowly varying torque given
by
ρ˙ = a sin(√t) − λ sin ψ, ψ˙ = ρ,
where a, λ, and  are parameters. Identify the region in phase space correspond￾ing to the librational motions of the pendulum at the parameter value  = 0.
Determine (by numerical integration if necessary) the behavior in forward and
backward time of the corresponding solutions for the system with  > 0 that have
initial conditions in the librational region.
Exercise 10.6. Consider the phase modulated pendulum (remember that our
pendulum model (10.27) is only a special case of the type of equation that is
obtained by partial averaging) given by
ψ¨ + sin(ψ + a sin(t)) = 0.
What can you say about the dynamics?
Exercise 10.7. Show that resonance manifolds do not intersect in systems with
just two angle variables, but that they can intersect if there are three or more
angles.10.2 Averaging at Resonance 609
Because a trajectory can be captured into resonance, the averaging prin￾ciple is not generally valid for systems with more than one angular variable.
To see why, note that a solution of the averaged system might pass through
a resonance while the corresponding solution of the original system is cap￾tured into the resonance. If this occurs, then the norm of the difference
of the evolving action variables and the corresponding averaged variables,
given by |I(t) − J(t)|, may grow to a value that is O(1) in the perturba￾tion parameter as time evolves and the solution t → I(t) is trapped in the
resonance. In particular, this scenario would violate the expected estimate;
that is, |I(t) − J(t)| < C1.
A complete analysis for the dynamics of multifrequency systems is not
known. Thus, this is an area of much current research (see, for example, [13],
[15], [164], and [227]). One of the most important issues is to determine the
“diffusion rates” for the action variables to leave the vicinity of a resonance
and to arrive at a second resonance. The long-term stability of the models
of the motion of many-body systems, for example, our solar system, is
essentially bound up with this question. This is currently one of the great
unsolved problems in mathematics.
A concrete counterexample to the validity of averaging for the case of
two or more angles is provided by the system
˙
I1 = ,
˙
I2 =  cos(θ2 − θ1),
˙
θ1 = I1,
˙
θ2 = I2, (10.28)
introduced in [227] (see Exercise 10.8).
Exercise 10.8. Find the averaged system for the oscillator (10.28) and the
general analytical solution of the averaged system. Show that a solution of the
original system is given by
I1(t) = t + I0,
I2(t) = I1(t),
θ1(t) = 
1
2
t
2 + I0t + θ1(0),
θ2(t) = θ1(t).
For these solutions, show that the estimate expected from the averaging theorem
(Theorem 10.2) is not valid.
Let us note that system (10.28) has a resonance manifold given by the
resonance relation I2 −I1 = 0. As prescribed above in our partial averaging
procedure, consider new coordinates defined by
√ ρ = I2 − I1, z = I2, ψ = θ2 − θ1, χ = θ2,610 10. Averaging
and note that system (10.28), when expressed in these coordinates, is given
by
ρ˙ = √ (cos ψ − 1),
z˙ =  cos ψ,
ψ˙ = √ ρ,
χ˙ = z. (10.29)
Averaging over the fast angle χ in system (10.29) produces the partially
averaged system
ρ¯˙ = √ (cos ψ¯ − 1),
z¯˙ = 0,
¯˙
ψ = √ ρ. ¯ (10.30)
For each fixed ¯z, there is an orbit O1 whose ω-limit set is the rest point
(¯ρ, z, ¯ ψ¯) = (0, z, ¯ 0) and a second orbit O2 with this rest point as its α￾limit set. (Prove this!) The trajectories corresponding to the orbit O1 are
all captured into the resonance relative to the first-order approximation of
the partially averaged system, and the rest point is captured for all time.
But the action variable corresponding to ¯z, a super slow variable, drifts
from its initial value so that the trajectories corresponding to the orbit O1
eventually pass through the resonance. This example thus provides a clear
illustration of the mechanism that destroys the possibility that the averaged
system—not the partially averaged system—gives a good approximation to
the full system over a long time scale. In effect, the averaged system for
this example does not “feel the influence of the resonance.”
Exercise 10.9. The partially averaged system (10.30) is obtained by averaging
over just one angle. Thus, the averaging theorem ensures that under appropriate
restrictions the partially averaged system is a good approximation to the original
system. Formulate appropriate restrictions on the domain of definition of the
partially averaged system and determine an appropriate time scale for the validity
of averaging. Give a direct proof that your formulation is valid.
Exercise 10.10. In applied mathematics, often only the lowest order reso￾nances are considered; they seem to have the most influence on the dynamics.
As an example to illustrate why this observation might be justified, consider the
near-resonance dynamics of system (10.28) at a “high” order resonance given
by the resonance relation mI1 = nI2 where m and n are relatively prime, and
m = n. Show that there are integers k and  such that the matrix
R := m −n
k  
is unimodular. Next, define new angular coordinates by
φ1
φ2

= R
θ1
θ2

.10.2 Averaging at Resonance 611
Also, define new action variables by
√ ρ = mI1 − nI2, z = kI1 + I2.
Change to the new coordinates, find the partially averaged system, and show
that, in this approximation, all orbits pass through the resonance. Does this
mean that the averaging principle is valid for orbits starting near the higher￾order resonances in this example?
Let us consider the system (10.1) with u ∈ R2; that is, a planar peri￾odically perturbed oscillator. Furthermore, let us assume that action-angle
variables have been introduced as in system (10.18). In this case, the reso￾nance layer given by the resonance relation mω(I) = nΩ is (generically) a
point I = I0 in the one-dimensional action space.
To determine the partially averaged system at the resonance layer given
by I = I0, let ρ denote the scaled distance to the resonance layer; that is,
√ ρ = I − I0.
Also, let τ = Ωt, and introduce a new angular variable by
ψ = mφ − nτ.
Then, to first order in the perturbation parameter √, the differential equa￾tion in these new coordinates is given by the system
ρ˙ = √ F(I0, ψ/m + nτ /m, τ /Ω) + O(),
ψ˙ = √ mω
(I0)ρ + O(),
τ˙ = Ω
where ρ and ψ are slow variables, and τ (corresponding to the time variable
in our nonautonomous perturbation) is a fast angular variable.
By the averaging theorem, there is a change of coordinates such that the
transformed system, to leading order in μ = √, is given by
J˙ = μF¯(θ), ˙
θ = μmω
(I0)J, (10.31)
where
F¯(θ) := 1
2πm  2πm
0
F(I0, θ/m + nτ /m, τ /Ω) dτ.
Under the assumption that ω
(I0) = 0—in other words, under the assump￾tion that the unperturbed resonant periodic orbit corresponding to the
action variable I = I0 is normally nondegenerate—the averaged system for
 > 0 has a nondegenerate rest point at (J0, θ0) if and only if J0 = 0 and
the function F¯ has θ0 as a simple zero.
Note that the solution of the system
J˙ = μF¯(θ), ˙
θ = μmω
(I0)J, τ˙ = Ω612 10. Averaging
starting at the point (J, θ, τ ) = (0, θ0, 0) is a periodic orbit, and in addi￾tion if the rest point is hyperbolic, then this periodic orbit is hyperbolic.
We would like to conclude that there is a corresponding periodic orbit for
the original oscillator. This fact is implied by the following more general
theorem.
Theorem 10.11. Consider the system
˙
I = F(I,θ) + 
2F2(I, θ, ),
˙
θ = ω(I) + G(I, θ, ), (10.32)
where I ∈ RM and θ ∈ T; where F, F2, and G are 2π-periodic functions
of θ; and where there is some number c such that ω(I) >c> 0. If the
averaged system has a nondegenerate rest point and  is sufficiently small,
then system (10.32) has a periodic orbit. If in addition  > 0 and the rest
point is hyperbolic, then the periodic orbit has the same stability type as
the hyperbolic rest point; that is, the dimensions of the corresponding stable
and unstable manifolds are the same.
Proof. The averaged differential equation is given by J˙ = F¯(J) where
F¯ is the average of the function θ → F(I,θ). Let us suppose that J0 is a
nondegenerate rest point of the averaged system; that is, F¯(J0) = 0 and
the derivative DF¯(J0) is an invertible transformation.
By the averaging theorem, if  is sufficiently small, then there is a 2π￾periodic change of coordinates of the form J = I + L(I,θ), defined in
an open set containing {J0} × T, such that system (10.32) in these new
coordinates is given by
J˙ = F¯(J) + O(
2),
˙
θ = ω(J) + O(). (10.33)
Let t → (J(t, ξ, ), θ(t, ξ, )) denote the solution of the system (10.33)
such that J(0, ξ, ) = ξ and θ(0, ξ, ) = 0. By an application of the implicit
function theorem, there is a smooth function (ξ, ) → T(ξ, ) that is defined
in a neighborhood of (J, θ)=(J0, 0) such that T(J0, 0) = 2π/ω(J0) and
θ(T(ξ, ), ξ, ) ≡ 2π. Moreover, let us define a (parametrized) Poincar´e map,
with the same domain as the transit time map T, by
P(ξ, ) = J(T(ξ, ), ξ, ).
By expanding the function  → P(ξ, ) into a Taylor series at  = 0, we
obtain
P(ξ, ) = J(T(ξ, 0), ξ, 0) + (J˙(T(ξ, 0), ξ, 0)T(ξ, 0)
+ J(T(ξ, 0), ξ, 0)) + O(
2).10.2 Averaging at Resonance 613
Note that J(T(ξ, 0), ξ, 0) ≡ ξ and J˙(T(ξ, 0), ξ, 0) = 0. Moreover, the func￾tion t → J(t, ξ, 0) is the solution of the variational initial value problem
given by
W˙ = F¯(J(t, ξ, 0)), W(0) = 0.
Using the identities J(t, ξ, 0) ≡ ξ and T(ξ, 0) ≡ 2π/ω(ξ), it follows that
J(t, ξ, 0) = tF¯(ξ) and
P(ξ, ) = ξ +  2π
ω(ξ)
F¯(ξ) + O(
2). (10.34)
Consider the displacement function δ(ξ, ) := P(ξ, ) − ξ and note that
its zeros correspond to the fixed points of the Poincar´e map. Also, the
zeros of the displacement function are the same as the zeros of the reduced
displacement function defined by
Δ(ξ, ) := 2π
ω(ξ)
F¯(ξ) + O().
By easy computations, it follows that Δ(J0, 0) = 0 and
Δξ(J0, 0) = 2π
ω(J0)
DF¯(J0).
Hence, by an application of the implicit function theorem, there is a func￾tion  → β() defined on some interval containing  = 0 such that β(0) = J0
and such that for each  in the domain of β, the vector β() ∈ RM is a fixed
point of the Poincar´e map ξ → P(ξ, ). In particular, (J, θ)=(β(), 0) is
the initial condition for a periodic orbit of the system (10.33). Since the
original system (10.32) is obtained from system (10.33) by a (appropriately
periodic) change of coordinates, there are corresponding periodic orbits in
the original system.
Finally, to determine the stability type of the periodic orbit, we must
compute the derivative of the Poincar´e map with respect to the space vari￾able. Using the series expansion (10.34), if the derivative with respect to
ξ is evaluated at the initial point ξ = β() of the perturbed periodic orbit
and the result is expanded in a Taylor series at  = 0, the following formula
is obtained:
Pξ(β(), ) = I +  2π
ω(J0)
DF¯(J0) + O(), (10.35)
where, in deference to tradition, I in this formula is the identity map of
RM, not the variable I in the original differential equation.
Abstractly, the matrix equation (10.35) has the form
P − I = (A + R()),
where A is infinitesimally hyperbolic with, say, N eigenvalues with positive
real parts and M −N eigenvalues with negative real parts. If  is sufficiently614 10. Averaging
θ
J
Figure 10.5: Phase portrait of the first-order approximation of the partially
averaged system (10.36) in case F¯ = 0.
small, then the matrix A+R() has the same number of such eigenvalues. If
in addition  > 0, then the matrix (A+R()) has the same number of such
eigenvalues that are all as close to the origin in the complex plane as desired.
Since there are only a finite number of eigenvalues and the eigenvalues of
P are exactly the eigenvalues of the matrix (A + R()) shifted one unit
to the right in the complex plane, it follows that, for sufficiently small
positive , the matrix P has N eigenvalues outside the unit circle and
M − N eigenvalues inside the unit circle, as required. The proof that this
structure is preserved by the inverse of the averaging transformation and
is therefore inherited by the original system is left to the reader. ✷
The partially averaged system (10.31) obtained above is given more pre￾cisely by the system
J˙ = μF¯(θ) + O(μ2), ˙
θ = μmω
(I0)J + O(μ2), (10.36)
where the presence of perturbation terms is indicated by the order symbol.
Let us assume that ω
(I0) > 0 and consider some of the possible phase
portraits of this system. The phase portrait (of the phase plane) of the
first-order approximation of system (10.36) in case F¯ = 0 is depicted in
Figure 10.5. The J-axis, the intersection of the resonance manifold with
the (J, θ)-plane, consists entirely of rest points. A higher order analysis is
required to determine the dynamics of the perturbed system. The phase
portrait for the first-order approximation in case F¯ has fixed sign (taken
here to be positive) is shown in Figure 10.6. In this case all orbits pass
through the resonance. A typical phase portrait for the case where F¯ has
simple zeros is depicted in Figure 10.7. There are several regions corre￾sponding to librational motions where orbits are permanently captured into10.2 Averaging at Resonance 615
J
θ
Figure 10.6: Phase portrait of the first-order approximation of the partially
averaged system (10.36) in case F¯ is a positive function.
θ
J
Figure 10.7: Phase portrait of the first-order approximation of the partially
averaged system (10.36) in case F¯ has simple zeros.616 10. Averaging
Figure 10.8: Phase portrait of the stroboscopic Poincar´e map for the per￾turbed system (10.36). The left panel depicts entrainment, the right panel
depicts chaos.
resonance. Finally, in Figure 10.8, two possible phase portraits of the stro￾boscopic Poincar´e map of the perturbed system are illustrated. Whereas
the left panel corresponds to resonance capture—in the context of a period￾ically perturbed oscillator this would also be called entrainment—the right￾hand panel corresponds to transient chaos; that is, the chaotic invariant
set is of saddle type so that nearby orbits approach the chaotic set along a
stable manifold, they “feel” the chaos on some finite time scale, and they
eventually drift away along an unstable manifold.
Exercise 10.12. In Theorem 10.11, suppose that the rest point is nondegener￾ate but not hyperbolic. What can be said about the stability type of the corre￾sponding periodic orbit?
Exercise 10.13. Compare and contrast the continuation theory for periodic
orbits of planar periodically perturbed oscillators given in Chapter 8 and the
theory presented in this chapter.
Exercise 10.14. Consider the following modification of an example introduced
in [115] and [119], namely, the system
x˙ = y(1 − x2 − y2
) + [δx − x(x2 + y2
) + γx cos(Ωt)],
y˙ = −x(1 − x2 − y2
) + [δy − y(x2 + y2
)], (10.37)
where δ, γ, and Ω are positive constants and  is a small parameter.
Here the action-angle variables are trigonometric. Show that (I,θ) defined by
the transformation
x = √
2I sin θ, y = √
2I cos θ
are action-angle variables for the system (10.37). The square root is employed
to make the transformation have Jacobian equal to one. This is important in
Hamiltonian mechanics where it is desirable to have coordinate transformations
that respect the Hamiltonian structure—such transformations are called sym￾plectic or canonical. At any rate, to find continuable periodic orbits, consider the
(m : n) = (2 : 1) resonance. Partially average the system at this resonance and10.2 Averaging at Resonance 617
200 400 600 800 1000
t
-6
-4
-2
0
2
4
200 400 600 800 1000
t
-6
-4
-2
0
2
4
Figure 10.9: The figure depicts the response signal for v := ˙
θ versus time
for the system ˙
θ = v, ˙v = − sin θ − (m1 + m2v − B cos(Ω(t − t0)) sin θ)
with t0 = 0, Ω = 2, m1 = 10, m2 = 1, B = 32, and  = .001. The left
panel depicts an orbit that is captured into resonance; the initial condition
is (θ, ˙
θ) = (0, 3.940252). The right panel depicts the corresponding signal
for the orbit with initial condition (θ, ˙
θ)=0, 3.940253).
use Theorem 10.11 to conclude that the original system has periodic orbits for
small  > 0.
There are some interesting dynamics going on in this example. Try some numer￾ical experiments to approximate the phase portrait of the stroboscopic Poincar´e
map. What is the main feature of the dynamics? Can you see the subharmonic
solutions near the (2 : 1) resonance? In addition to the references given above,
look at [45].
Exercise 10.15. If a linear oscillator is periodically perturbed, then its
response is periodic with the same frequency as the perturbation. On the other
hand, the amplitude of the response depends on the frequency. In particular, the
amplitude is large if the input frequency is (nearly) resonant with the natural
frequency of the oscillator. A lot of important scientific work and a lot of engi￾neering have been accomplished under the impression that the above statements
are true when the first sentence begins with the phrase “If an oscillator is peri￾odically forced. . . .” By reading to this point in this book you are in a strong
position to challenge these statements when the word “linear” is left out. Prove
that the statements are true for linear oscillators and give examples to show that
nonlinear oscillators do not always behave so simply. Suppose that a nonlinear
oscillator, say ˙x = f(x), is periodically perturbed with a periodic perturbation of
frequency Ω and the function t → xi(t) is observed where xi is one of the compo￾nent functions of a solution t → (x1(t),...,xn(t)). Will the signal t → xi(t) retain
some “trace” of the periodic input? For example, consider the power spectrum
of this function, that is, the square of the absolute value of its Fourier transform.
Will the frequency Ω have a large amplitude in the power spectrum? Try some618 10. Averaging
numerical experiments. The previous question does not have a simple answer.
But questions of this type arise all the time in physics and engineering where
we are confronted with multivariable systems that are often far too complex to
be analyzed with analytic methods. Discuss the reasons why the study of simple
models might be valuable for understanding complex systems.
Exercise 10.16. Consider the system
˙
θ = v, v˙ = − sin θ − (m1 + m2v − B cos(Ω(t − t0)) sin θ),
a parametrically excited pendulum with damping and torque. Reproduce Fig￾ure 10.9 as an illustration of passage through resonance. Determine an approxi￾mate neighborhood of the point (θ, ˙
θ) = (0, 3.940252) corresponding to the initial
conditions for orbits that are captured into resonance. Can you automate a crite￾rion for “capture into resonance”? Explore other regions of the parameter space
by using numerical experiments.
10.3 Action-Angle Variables
To use the theory presented so far in this chapter we must be able to
express our oscillator in action-angle variables. In practice, the construction
of action-angle variables is a formidable task—recall the construction of
the Delaunay variables in Chapter 6. For linear oscillators, the appropriate
coordinate change can be constructed using polar coordinates, while the
construction of action-angle variables for the pendulum requires the use of
Jacobi elliptic functions. A general construction of action-angle variables
for planar oscillators is presented in this section. The construction uses
some of the ideas discussed in Chapter 8.
Let us consider a differential equation of the form
u˙ = f(u) + g(u, t), (10.38)
where the unperturbed system
u˙ = f(u) (10.39)
has a period annulus A. We will construct action-angle variables near a
periodic orbit Γ contained in A. The differential equation (10.39), expressed
in the new coordinates that we denote by I and ϑ, has the form
˙
I = 0, ϑ˙ = ω(I).
Interpreted geometrically, these new coordinates are related to polar coor￾dinates in that I is a radial variable and ϑ is an angular variable. In fact,
whereas I is constant on each periodic solution, ϑ changes linearly on each
periodic solution. In case the system (10.39) is Hamiltonian, the new coor￾dinates reduce to the usual action-angle variables on A.10.3 Action-Angle Variables 619
With reference to system (10.39), define the orthogonal system
u˙ = f ⊥(u), u ∈ X (10.40)
where, in oriented local coordinates, f ⊥(u) := Jf(u) with
J =
	0 −1
1 0 

.
We mention that J rotates vectors in the plane through a positive angle
of π/2 radians. The same symbol J is often used in this context with the
opposite sign.
Let ϕt denote the flow of the differential equation (10.39) and let ψt
denote the flow of the differential equation (10.40). Also, for vectors ξ1 and
ξ2 in R2, define ξ1 ∧ ξ2 := ξ1,Jξ2, where the brackets denote the usual
inner product in R2.
A periodic orbit Γ of (10.39) has an orientation determined by its time
parameterization. To specify an orientation, we define ε = ε(f) = 1 in case
for each ζ ∈ Γ the vector f ⊥(ζ) is the outer normal at ζ. If f ⊥(ζ) is the
inner normal, then ε := −1. Also, the orientation of the period annulus
A is defined to be the orientation inherited from its constituent periodic
solutions.
Choose a point ζ ∈ A and note that there is an open interval U ⊂ R
containing the origin such that the image of the map ρ → ψρ(ζ) for ρ ∈ U
is a section Σζ transverse to the orbits of system (10.39) in A. Also, define
Υ : U × R → A by
Υ(ρ, φ) = ϕφ(ψρ(ζ)). (10.41)
Clearly, Υ is smooth. In fact, Υ is a covering map, that is, a periodic coordi￾nate system on A. We will see below that Υ defines “flow box” coordinates:
coordinates that straighten out the flow in a neighborhood of the periodic
orbit containing the point ζ.
To construct the action-angle variables, let us begin by considering the
derivative of the map Υ defined in display (10.41). Diliberto’s theorem
(Theorem 8.11) states that if
b(t, ζ) := ||f(ζ)||2
||f(ϕt(ζ))||2 e
 t
0 div f(ϕs(v)) ds,
a(t, ζ) :=  t
0

2κ(s, ζ)||f(ϕs(ζ))|| − curl f(ϕs(ζ))
b(s, ζ) ds, (10.42)
where κ denotes the signed scalar curvature along the curve t → ϕt(ζ),
ζ ∈ A, then
DΥ(ρ, φ) ∂
∂φ = f(Υ(ρ, φ)),
DΥ(ρ, φ) ∂
∂ρ = b(φ, ψρ(v))f ⊥(Υ(ρ, φ)) + a(φ, ψρ(v))f(Υ(ρ, φ)).620 10. Averaging
In other words, the matrix representation of the derivative DΥ(ρ, φ) relative
to the ordered bases {∂/∂ρ, ∂/∂φ} and {f ⊥(Υ(ρ, φ)), f(Υ(ρ, φ))} is given by
DΥ(ρ, φ) = 	 b(φ, ψρ(v)) 0
a(φ, ψρ(v)) 1

.
Since b does not vanish for ζ ∈ A, it follows that Υ is a local diffeomorphism
and in fact Υ is a covering map onto its image.
To express the original system (10.38) in (ρ, φ)-coordinates, note first
that there are smooth functions (u, t) → p(u, t) and (u, t) → q(u, t) such
that
g(u, t) = p(u, t)f ⊥(u) + q(u, t)f(u) (10.43)
for all (u, t) ∈ A×R. Thus, to change system (10.38) to the new coordinates,
we simply solve for
j(u, t) ∂
∂ρ + k(u, t) ∂
∂φ
in the matrix equation
	 b 0
a 1

 	 j
k


=
	 p
1 + q 

to obtain
	 j
k


=
	  1
b p
1 + (q − a
b p)


.
It follows that system (10.39) in the new coordinates is given by
ρ˙ =  1
b(φ, ψρ(v))p(Υ(ρ, φ), t),
φ˙ =1+ 

q(Υ(ρ, φ), t) − a(φ, ψρ(v))
b(φ, ψρ(v)) p(Υ(ρ, φ), t)

. (10.44)
To compress notation, let us write (10.44) in the form
ρ˙ = Q(ρ, φ, t), φ˙ =1+ R(ρ, φ, t). (10.45)
Define a second change of coordinates by
ρ = β(I), φ = α(I)ϑ, (10.46)
where I → α(I) and I → β(I) are smooth functions to be specified below.
Here, since the coordinate transformation must be invertible, we need only
assume that α(I)β
(I) = 0. In the (I,ϑ)-coordinates, system (10.44) has
the form
˙
I =  1
β(I)
Q(β(I), α(I)ϑ, t),
ϑ˙ = φ˙ − ϑα
(I) ˙
I
α(I) (10.47)
= 1
α(I) + 
 1
α(I)
R(β(I), α(I)ϑ, t) − ϑ α
(I)
α(I)β(I)
Q(β(I), α(I)ϑ, t)

.10.3 Action-Angle Variables 621
To specify the functions α and β we require two auxiliary functions—
the period function and the area function. To define the period function,
recall that the image of the map ρ → ψρ(ζ) for ρ ∈ U is a section for
the unperturbed flow on the period annulus A. The period function on A
relative to this section is the map T : U → R that assigns to each ρ ∈ U
the minimum period of the solution of system (10.39) that passes through
the point φρ(ζ) ∈ A. In the “standard” case, A is an annulus whose inner
boundary is a rest point. In this case, we define the area function ζ → A(ζ);
it assigns to each ζ ∈ A the area enclosed by the unperturbed solution
through ζ.
The function β is defined to be the solution of the initial value problem
dρ
dI = ε
2π
T(ρ)
1
||f(ψρ(ζ))||2 , ρ(I0)=0, (10.48)
where in the standard case I0 = A(ζ)/(2π), and in the case where A has
a non-trivial inner boundary I0 = 0. The choice of initial condition for
the standard case agrees with tradition; a different choice of initial condi￾tion simply results in a constant translation of the “action” variable. The
function α is defined by
α(I) := −ε
T(β(I))
2π , (10.49)
where ε = ±1 according to the orientation of the period annulus A.
Using the definition T(I) := T(β(I)), the system (10.47) has the form
˙
I = ε
T(I)
2π ||f(ψρ(ζ))||2Q(β(I), α(I)ϑ, t),
ϑ˙ = −ε
2π
T(I) − ε 2π
T(I)
R(β(I), α(I)ϑ, t)
+ ϑT
(I)
2π ||f(ψρ(ζ))||2Q(β(I), α(I)ϑ, t)

. (10.50)
From equation (10.43), we have the identities
p = 1
||f||2 g, f ⊥ = 1
||f||2 f ∧ g, q = 1
||f||2 f,g.
In view of system (10.44), the system (10.50) can be rewritten in the form
˙
I = ε
T(I)
2π E(I,ϑ)f(Υ(β(I), α(I)ϑ)) ∧ g(Υ(β(I), α(I)ϑ), t),
ϑ˙ = −ε
2π
T(I)
− ε 2π
T(I)
||f(Υ(β(I), α(I)ϑ))||−2f,g +

ϑT
(I)
2π ||f(ψβ(I)(ζ))||2
− 2π
T(I)
a(α(I)ϑ, ψβ(I)(ζ))
||f(φβ(I)(ζ))||−2E(I,ϑ)f ∧ g

, (10.51)622 10. Averaging
where
E(I,ϑ) := e−  α(I)ϑ
0 div f(Υ(β(I),α(I)s)) ds.
Again, for notational convenience, let us write the first-order system (10.51)
in the compact form
˙
I = F(I, ϑ, t), ϑ˙ = ω(I) + G(I,ϑ). (10.52)
Note that both F and G are 2π-periodic in ϑ and 2π/Ω-periodic in t.
Thus, we have transformed the original perturbed system to action-angle
coordinates.
To prove that the action-angle coordinate transformation
u = Υ(β(I), α(I)ϑ) (10.53)
is canonical in case the unperturbed system is Hamiltonian, it suffices to
show the transformation is area preserving, that is, the Jacobian of the
transformation is unity. In fact, the Jacobian is
det 	−f2(u) f1(u)
f1(u) f2(u)

 
a(φ, ψρ(ζ)) 1  	 β
(I) 0
α
(I)ϑ α(I)


= ||f(u)||2
||f(ψρ(ζ))||2 b(φ, ψρ(ζ)).
But, if f is a Hamiltonian vector field, then div f = 0, and
b(φ, ψρ(ζ)) = ||f(ψρ(ζ))||2
||f(u)||2 ,
as required. Moreover, in case f is the Hamiltonian vector field for the
Hamiltonian H , we have f(u) = −J grad H(u). Recall that ρ = β(I) and
define h := H(ψρ(ζ)). Then,
dI
dh = ε
T(ρ(h))
2π .
Thus, the derivative of the action variable with respect to energy is the
normalized energy-period function, as it should be.11
Bifurcation
Consider the family of differential equations
u˙ = f(u, λ), u ∈ Rn, λ ∈ R. (11.1)
In case f(u0, λ0) = 0, the differential equation with parameter value λ = λ0
has a rest point at u0 and the linearized system at this point is given by
W˙ = fu(u0, λ0)W. (11.2)
If the eigenvalues of the linear transformation fu(u0, λ0) : Rn → Rn are
all nonzero, then the transformation is invertible, and by an application of
the implicit function theorem there is a curve λ → β(λ) in Rn such that
β(λ0) = u0 and f(β(λ), λ) ≡ 0. In other words, for each λ in the domain
of β, the point β(λ) ∈ Rn corresponds to a rest point for the member of
the family (11.1) at the parameter value λ.
Recall that if all eigenvalues of the linear transformation fu(u0, λ0) have
nonzero real parts, then the transformation is called infinitesimally hyper￾bolic and the rest point u0 is called hyperbolic. Also, in this case, since the
eigenvalues of Df(u, λ) depend continuously on u and the parameter λ, if
|λ−λ0| is sufficiently small, then the rest point u = β(λ) of the differential
equation (11.1) at the parameter value λ has the same stability type as the
rest point u0 = β(λ0). In particular, if the rest point u0 is hyperbolic, then
for sufficiently small λ the perturbed rest point β(λ) is also hyperbolic.
If fu(u0, λ0) is not infinitesimally hyperbolic, then there is at least one
eigenvalue with zero real part. The topology of the local phase portrait of
the corresponding differential equation (11.1) at this rest point may change
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
C. Chicone, Ordinary Differential Equations with Applications, Texts in
Applied Mathematics 34, https://doi.org/10.1007/978-3-031-51652-8 11
623624 11. Bifurcation
under perturbation; that is, the local phase portrait at u0 may change for
λ close but not equal to λ0. If it does, we say that a bifurcation occurs.
For example, the phase portrait for the differential equation with λ = λ0
may have no rest points or several rest points in the vicinity of the original
rest point.
Bifurcations are considered in this chapter for the special case where the
linear transformation fu(u0, λ0) has a simple zero eigenvalue; that is, a zero
eigenvalue with algebraic (and geometric) multiplicity one, or a pair of pure
imaginary complex conjugate eigenvalues each with algebraic multiplicity
one. In particular, some of the “generic” bifurcations that occur under this
condition are described.
Although only the loss of stability at a rest point of a differential equa￾tion will be discussed, the basic results presented here can be modified to
cover the case of the loss of stability of a fixed point of a map; and in
turn the modified theory can be applied to the Poincar´e map to obtain a
bifurcation theory for periodic orbits. The extension of bifurcation the￾ory from rest points to periodic orbits is only the beginning of a vast
subject that has been developed far beyond the scope of this book. For
example, the loss of stability of a general invariant manifold can be con￾sidered. On the other hand, bifurcation theory is by no means complete:
Many interesting problems are unresolved. (See the books [9] and [66] for
detailed and wide ranging results on bifurcations of planar vector fields,
and [12], [65], [112], [113], [119], [170], [234], [260], and [261] for more gen￾eral bifurcation theory.)
Exercise 11.1. Prove that the eigenvalues of an n × n matrix depend contin￾uously on the components of the matrix.
11.1 One-Dimensional State Space
In this section, some of the general concepts of bifurcation theory are
illustrated in their simplest form by an analysis of an important bifur￾cation associated with rest points of scalar differential equations, namely,
the saddle-node bifurcation. In addition, the theory is augmented with a
discussion of bifurcation problems in applied mathematics.
11.1.1 The Saddle-Node Bifurcation
Consider the family of differential equations
u˙ = λ − u2, u ∈ R, λ ∈ R (11.3)
and note that with f(u, λ) := λ − u2,
f(0, 0) = 0, fu(0, 0) = 0, fuu(0, 0) = −2, fλ(0, 0) = 1.11.1 One-Dimensional State Space 625
Moreover, rest points for members of this family are given by λ = u2.
There are no rest points for λ < 0, one rest point at λ = 0, and two rest
points for λ > 0. Moreover, the rest point at λ = 0 is called a saddle￾node, which is a rest point such that the system matrix of the linearization
of the corresponding differential equation at this point has a simple zero
eigenvalue. For λ > 0 the rest points are u = ±
√
λ. Via linearization, one
of them is stable, the other unstable. This family thus provides an example
of a saddle-node bifurcation (see Figure 1.6 for the bifurcation diagram).
Roughly speaking, the one-parameter family (11.1) has a saddle-node
bifurcation at (u0, λ0) if its bifurcation diagram near (u0, λ0), with the
point (u0, λ0) translated to the origin, is similar to Figure 1.6. The next
proposition states the precise conditions that the bifurcation diagram must
satisfy and sufficient conditions for a saddle-node bifurcation to occur at
(u, λ) = (0, 0) in case system (11.1) is a scalar differential equation; a
formal definition and a more general theorem on saddle-node bifurcation
(Theorem 11.13) will be formulated and proved in Section 11.2.
Proposition 11.2. Suppose that f : R × R → R is twice continuously
differentiable. If
f(0, 0) = 0, fu(0, 0) = 0, fuu(0, 0) = 0, fλ(0, 0) = 0,
then there is a saddle-node bifurcation at (u, λ) = (0, 0). In particular,
there is a number p0 > 0 and a unique smooth curve β in R × R given by
p → (p, γ(p)) for |p| < p0 such that each point in the range of β corresponds
to a rest point, and the range of β is quadratically tangent to R × {0}; that
is,
f(p, γ(p)) ≡ 0, γ(0) = γ
(0) = 0, γ(0) = 0.
Moreover, the stability type of the rest points corresponding to β changes
at p = 0; that is, p → fu(p, γ(p)) changes sign at p = 0. Also, γ(0) =
−fuu(0, 0)/fλ(0, 0).
Proof. Because fλ(0, 0) = 0, the implicit function theorem implies the
existence of a positive number p0 and a curve p → γ(p) such that γ(0) = 0
and f(p, γ(p)) ≡ 0 for |p| < p0. Since the derivative of the function p →
f(p, γ(p)) is zero,
fu(p, γ(p)) + fλ(p, γ(p))γ
(p)=0.
In particular
fu(0, 0) + fλ(0, 0)γ
(0) = 0,
and, in view of the hypotheses, γ
(0) = 0. Since the second derivative of
the function p → f(p, γ(p)) is also zero,
fuu(0, 0) + fλ(0, 0)γ(0) = 0.626 11. Bifurcation
By rearrangement of this equation and by the hypotheses of the proposition,
it follows that
γ(0) = −fuu(0, 0)
fλ(0, 0) = 0.
Finally, because the derivative of the map p → fu(p, γ(p)) at p = 0 is the
nonzero number fuu(0, 0), this map indeed changes sign at p = 0. ✷
11.1.2 A Normal Form
If f satisfies the hypotheses of Proposition 11.2, the preparation theorem
(Theorem 8.21) implies that it can be factored such that
f(u, λ)=(a0(u) + λ)U(u, λ),
a0(0) = 0, and U(0, 0) = 0. Thus, the flow of the differential equation
u˙ = f(u, λ) (11.4)
is topologically equivalent to the flow of the differential equation ˙u =
a0(u) + λ on some open neighborhood of the origin by the identity homeo￾morphism. Or, if you like, the two differential equations are equivalent by a
rescaling of time (see Proposition 1.126). The hypotheses fu(0, 0) = 0 and
fuu(0, 0) = 0 imply that a
0(0) = 0 and a
0 (0) = 0. As a result, the function
a is given by
a0(u) = 1
2
a
0 (0)u2 + O(u3).
By the Morse lemma (see Exercise 8.73), there is a change of coordinates
u = μ(y) with μ(0) = 0 that transforms the differential equation (11.4)
into the form
y˙ = 1
μ
(y)
(λ ± y2)
where, of course, μ
(y) = 0 because the change of coordinates is invertible.
By a final rescaling of time and, if necessary, a change in the sign of λ, we
obtain the equivalent differential equation
y˙ = λ − y2. (11.5)
The family (11.5) is a normal form for the saddle-node bifurcation:
Every one-parameter family of scalar differential equations that satisfies
the hypotheses of Proposition 11.2 at a point in the product of the phase
space and the parameter space can be (locally) transformed to this normal
form by a (nonlinear) change of coordinates and a rescaling of time. In this
context, the differential equation (11.5) is also called a versal deformation
or a universal unfolding of the saddle-node.11.1 One-Dimensional State Space 627
The reader may suspect that the use of such terms as “versal deforma￾tion” and “universal unfolding” is indicative of a rich and mature underly￾ing theory. This is indeed the case. Moreover, there are a number of excel￾lent books on this subject. For example, the book of Vladimir Arnold [12]
has a masterful exposition of the “big ideas” of bifurcation theory while the
books of Martin Golubitsky and David G. Schaeffer [112] and Golubitsky,
Ian Stewart, and Schaeffer [113] contain a more comprehensive study of the
subject (see also [65] and [66]).
In the next two sections we will explore some of the philosophy of bifur￾cation theory and discuss how bifurcation problems arise in applied math￾ematics.
11.1.3 Bifurcation in Applied Mathematics
Is bifurcation theory important in applied mathematics? To discuss this
question, let us suppose that we have a model of a physical system given
by a family of differential equations that depends on some parameters. We
will consider the process that might be used to identify these parameters
and the value of the resulting model for making physical predictions.
In a typical scenario, a model has “system parameters” and “control
parameters.” System parameters specify the measurements of intrinsic phys￾ical properties; control parameters correspond to adjustments that can be
made while maintaining the integrity of the physical system. By changing
control parameters in a mathematical model, predictions might be made
that avoid or help design expensive physical experiments or new phenomena
might be discovered.
Ideally, system parameters are identified by comparing predictions of
the model with experimental data. But, for a realistic model with several
system parameters, parameter identification will almost always require a
complicated analysis. In fact, parameter identification is itself a fascinat￾ing and important problem in applied mathematics that is not completely
solved. Usually, parameter identifications are not exact. Indeed, combining
an approximation algorithm with experimental data always leads to some
uncertainty in the parameter values.
Different but related problems arises when the model system of differen￾tial equations obtained from a parameter identification process contains a
degeneracy. For example, suppose it has a degenerate rest point for some
choice of the control parameters.
Have we just been unlucky? Can we adjust the parameters to avoid the
degeneracy? What does the appearance of a degenerate rest point tell us
about our original model?
Consider the case where there are no control parameters. If, for exam￾ple, our original model is given by the differential equation (11.5) and our
parameter identification process results in specifying the system parameter
value λ = 0 so that the corresponding differential equation has a degenerate628 11. Bifurcation
rest point, then it would seem that we have been very unlucky. Indeed, pre￾dictions from the model with λ = 0 would seem to be quite unreliable. By
an arbitrarily small change in the estimated value of the system parameter,
the model the model predicts two hyperbolic rest points or no rest points
at all. From a more general point of view, the value λ = 0 for the system
parameter produces a model that is not structurally stable. By small per￾turbations two structurally stable model systems are produced that happen
to not be qualitatively the same (see Exercise 11.3). Whereas in the scalar
model (11.5) analysis is transparent, analysis of such structural instabilities
in multiparameter or multidimensional models is much less clear.
Two vector fields defined on the same state space are called topologically
equivalent if there is a homeomorphism of the state space that maps all
orbits of the first vector field onto orbits of the second vector field and pre￾serves the direction of time along all the orbits (the time parameterization
of the orbits is ignored). Of course, if two vector fields are topologically
equivalent, then their phase portraits are qualitatively the same. A vector
field (and the corresponding differential equation) is called structurally sta￾ble if there is an open set of vector fields in the C1 topology that contains
the given vector field, and all vector fields in this open set are topologi￾cally equivalent to the given vector field. The idea is that the topological
type of a structurally stable vector field is not destroyed by a small smooth
perturbation (recall Exercise 1.112).
While it might seem reasonable to suspect that most models are struc￾tural stable (for instance, we might expect that the set of structurally
stable vector fields is open and dense in the C1 topology), this is not the
case. On the other hand, there is a rich mathematical theory of structural
stability. In particular, deep theorems in this subject state necessary and
sufficient conditions for a vector field to be structurally stable. An intro￾duction to these results is given in the book of Stephen Smale [230] and the
references therein. From the perspective of applied mathematics, the defini￾tion of structural instability is perhaps too restrictive. A system is deemed
unstable if its topological type is destroyed by an arbitrary C1 perturba￾tion. But in mathematical modeling the differential equations that arise
are not arbitrary. Rather, they are derived from physical laws. Thus, the
structural stability of a model with respect to its parameters—the subject
matter of bifurcation theory—is often a more important consideration than
the C1 structural stability of the model.
Consider next a model system that contains control parameters. For
example, suppose that the original differential equation is
u˙ = λ − au2,
a is a system parameter, and λ is a control parameter; and, the system
parameter is measured or identified to be not zero. In this case, the one￾parameter family of differential equations has a saddle-node at the control11.1 One-Dimensional State Space 629
parameter value λ = 0. The instability is unavoidable for all nearby choices
of the system parameter. This observation suggests the reason why bifur￾cation theory is important in the analysis of models given by families of
differential equations: While a nondegenerate member of a family may be
obtained by a small change of its parameter, all sufficiently small pertur￾bations of the family may contain members with a degeneracy. We will
discuss this essential fact in more detail in the next section.
Exercise 11.3. Consider the set S of all smooth functions defined on R and
endow S with the C1([0, 1]) topology; that is, the distance between two functions
f and g in S is
f − g = f − g0 + f − g
0
where the indicated C0-norm is just the usual supremum norm over the unit
interval. Also, let S denote the subset of S consisting of the functions f ∈ S that
satisfy the following properties: (i) f(0) = 0 and f(1) = 0. (ii) If a is in the open
interval (0, 1) and f(a) = 0, then f
(a) = 0. Prove that each element in S is
structurally stable relative to S. Also, prove that S is an open and dense subset
of S.
11.1.4 Families, Transversality, and Jets
A structurally unstable system might occur at some parameter value in
a family of differential equations. This possibility leads to the important
question: “Is such a degeneracy avoidable for some family obtained by an
arbitrarily small perturbation of the given family?”
To gain some insight, a geometric interpretation of the space of vector
fields as in Figure 11.1 is useful. Consider the space of all smooth vector
fields on Rn and the subset that has a nonhyperbolic rest point. To draw a
picture, suppose that vector fields are represented by points in Euclidean
three-dimensional space and degenerate vector fields are represented by the
points on a hypersurface D. Since the complement of the set D is dense, if
f is a point in D, then there are points in the complement of D that are
arbitrarily close to f. By analogy, if the geometric interpretation is faithful,
then there is an arbitrarily small C1 perturbation of the vector field f that
is nondegenerate; that is, the corresponding system has only hyperbolic
rest points. This is indeed the case if all vector fields are restricted to be
defined on a fixed compact domain.
Next, consider a one-parameter family of vector fields as a curve in the
space of all smooth vector fields, and suppose that this curve meets the
hypersurface D that represents the degenerate vector fields. If the curve
meets the surface so that its tangent vector at the intersection point is
not tangent to the surface—called a transversal intersection—then every
sufficiently small deformation of the curve will have a nonempty transver￾sal intersection with D. In other words, the degeneracy (depicted by the630 11. Bifurcation
Figure 11.1: Two families of vector fields, represented as curves, meet the
set of structurally unstable vector fields represented by hyperplanes. The
family in the left-hand illustration is tangent to the hyperplane. A small
perturbation produces a family consisting entirely of structurally stable
vector fields. In contrast, all sufficiently small perturbations of the family
depicted as the curve (which might be tangent to the hyperplane) in the
right-hand illustration have structurally unstable members.
intersection of the curve with the hypersurface) cannot be removed by per￾turbation of the curve. By analogy, if our original family of vector fields
meets a “surface” corresponding to a degenerate set in the space of all vec￾tor fields (which is infinite dimensional) and if the intersection of the curve
with this degenerate surface is transversal, then the degeneracy cannot be
removed by a small deformation of the family. This is one of the main rea￾sons why bifurcation theory is important in applied mathematics when we
are studying a model given by a family of differential equations.
The geometric picture gives the correct impression for structural instabil￾ities due to the nonhyperbolicity of rest points, the subject of this chapter.
Indeed, we will show how to make a precise interpretation of this geome￾try for scalar vector fields. There is, however, an important warning: Our
picture is misleading for some more complicated structural instabilities, a
topic that is beyond the scope of this book (see, for example, [216] and
[230]).
Let us identify the set of all scalar vector fields with the space of smooth
functions C∞(R, R). In view of Proposition 11.2, only a finite set of the
partial derivatives of a scalar family is required to determine the presence
of a saddle-node bifurcation. In fact, this observation is the starting point
for the construction of a finite-dimensional space, called the space of k-jets,
that corresponds to the ambient space in our geometric picture.
Although the “correct” definition of the space of k-jets requires the intro￾duction of vector bundles (see, for example, [3]), we will enjoy a brief
glimpse of this theory by considering the special case of the construction11.1 One-Dimensional State Space 631
Y
p X
g
g(p)
g(X)
M
Figure 11.2: The sum of the tangent space to g(X) at g(p) and the tangent
space to M at g(p) is the tangent space to Y at g(p). In this case, the map
g : X → Y is transverse to the submanifold M ⊂ Y .
for the space C∞(R, R) where everything is so simple that the general
definition of a vector bundle can be avoided.
Consider the space R × C∞(R, R) and let k denote a nonnegative inte￾ger. Two elements (x, f) and (y, g) in the product space are defined to be
equivalent if
(x, f(x), f
(x), f(x),...,f(k)
(x)) = (y, g(y), g
(y), g(y),...,g(k)
(y))
and equality is in the vector space Rk+2. The set of all equivalence classes
Jk(R, R) is called the space of k-jets.
The equivalence class containing (x, f) is denoted by the symbol [x, f].
Using it, define the natural projection πk of Jk(R, R) into R by πk([x, f]) =
x. The k-jet extension of f ∈ C∞(R, R) is the map jk(f) : R → Jk(R, R)
defined by
jk(f)(u)=[u, f].
Because πk(jk(f)(u)) ≡ u, the k-jet extension is called a section of the fiber
bundle with total space Jk(R, R), base R, and projection πk. The fiber over
the base point x ∈ R is the set {[x, f] : f ∈ C∞(R, R)}. Also, let us define
Zk to be the image of the zero section of Jk(R, R); that is, Zk is the image
of the map ζ : R → Jk(R, R) given by ζ(u)=[u, 0].
The k-jet bundle can be “realized” by a choice of local coordinates. In
fact, the usual choice for the local coordinates is determined by the map
Φk : Jk(R, R) → R × Rk+1 defined by
Φk([u, f]) = (u, f(u), f
(u),...,f k(u)).632 11. Bifurcation
It is easy to check that Φk is well-defined and that we have the commutative
diagram
Jk(R, R) Φk
−→ R × Rk+1
⏐
⏐
πk
⏐
⏐
π1
R identity
−→ R
where π1 is the projection onto the first factor of R×Rk+1. Thus, Jk(R, R)
is identified with R × Rk+1 as a smooth manifold. Also, the set Z is given
in the local coordinates by Z := R × {0}. The jet space is the desired
finite-dimensional space that incorporates all the data needed to consider
bifurcations that depend only on a finite number of partial derivatives of a
family of scalar vector fields.
Definition 11.4. Suppose that g : X → Y is a smooth map and M denotes
a submanifold of the manifold Y . The map g is transverse to M at a point
p ∈ X if either g(p) ∈ M, or g(p) ∈ M and the sum of the tangent space
of M at g(p) and the range of the derivative Dg(p) (both viewed as linear
subspaces of the tangent space of Y at p) is equal to the entire tangent
space of Y at g(p). The function g is transverse to the manifold M if it is
transverse to M at every point of X.
The Fig. 11.2 shows an example of transverse manifolds.
The next theorem—stated here with some informality—is a far-reaching
generalization of the implicit function theorem.
Theorem 11.5 (Thom’s Transversality Theorem). The set S of maps
whose k-jet extensions are transverse to a submanifold M of the space of
k-jets is a dense subset of the space of all sufficiently smooth maps; more￾over, S is a countable intersection of open dense sets. In addition, if M is
closed in the space of k-jets, then S is open.
To make Thom’s theorem precise, we would have to define topologies on
our function spaces. The usual Cr topology is induced by the norm defined
as the sum of the suprema of the absolute values of the partial derivatives
of a function up to order r. But, this topology is not defined on the space
Cr(R, R) because some of the functions in this space are unbounded or
have an unbounded partial derivative. To get around this problem, we can
restrict attention to functions defined on a compact domain in R, or we
can use one of the two useful topologies on Cr(R, R) called the weak and
the strong topology. Roughly speaking, if f is a function, α > 0, and K is a
compact subset of R, then a basic open set in the weak topology, also called
the compact open topology, is defined to be the set of functions g such that
the distance between f and g in the Cr-norm, relative to the compact set
K, is less than the positive number α. The strong topology is similar, but it
includes the neighborhoods defined by requiring that functions be close on
(infinite) families of compact subsets of their domains. The strong topology
is important because some of its open neighborhoods control the size of the11.1 One-Dimensional State Space 633
function and its partial derivatives “at infinity.” These topologies are the
same if the functions in Cr(R, R) are all restricted to a compact set. In
this case, the corresponding function space is the usual Banach space of Cr
functions defined on the compact set. The important observation is that
Thom’s theorem is valid for both the weak and strong topologies. (See the
book of Morris Hirsch [137] for a precise definition of these topologies and
a proof of Thom’s theorem.)
A set is called residual if it is the (countable) intersection of open and
dense subsets. By Baire’s theorem, every residual set in a complete metric
space is dense (see [223]). Also, a property that holds on a residual set is
called generic. It turns out that even though the weak and strong topologies
on C∞(R, R) are not metrizable, the set C∞(R, R) is a Baire space with
respect to these topologies; that is, with respect to these topologies, a
countable intersection of open and dense sets is dense. Using these notions,
Thom’s transversality theorem can be restated as follows: The property of
transversal intersection is generic.
As a simple example of an application of Thom’s theorem, consider the
transversality of the 0-jet extensions of functions in C∞(R, R) with the
image of the zero section. Note that by the definition of transversality the
0-jet extension of f ∈ C∞(R, R) is transversal to the image of the zero
section Z0 at u ∈ R if either j0(f)(u) = [u, 0], or j0(f)(u)=[u, 0] and the
image of the derivative of the 0-jet extension j0(f) at u plus the tangent
space to Z0 at [u, 0] is the tangent space to J0(R, R) at [u, 0]. The next
goal is to determine this transversality condition more explicitly and use
Thom’s theorem to state a fact about the genericity of vector fields with
hyperbolic rest points.
Differentiability of the map j0(f) and the properties of its derivative are
local properties that can be determined in the local coordinate representa￾tion of the jet bundle. In fact, with respect to the local coordinates men￾tioned above, the local representative of the map j0(f) is u → Φ0(j0(f)(u)).
In other words, the local representation of j0(f) is the map F : R → R×R
defined by u → (u, f(u)); and, in these coordinates, the range of the deriva￾tive of F is spanned by the vector (1, f
(u)).
The local representation of Z0 is given by the linear manifold Z0 :=
{(x, y) ∈ R × R : y = 0}. Hence, the tangent space of Z0 at each of its
points can be identified with Z0. Moreover, Z0, viewed as a subspace of
R × R, is spanned by the vector (1, 0).
The 0-jet extension of the function f is transverse to the zero section at
the point u ∈ R if and only if f
(u) = 0; it is transverse to the zero section
if it is transverse at every u ∈ R. In other words, the 0-jet extension of f is
transverse to the zero section if and only if all zeros of f are nondegenerate;
or equivalently if and only if all rest points of the corresponding differential
equation ˙u = f(u) are hyperbolic.
By Thom’s theorem, if f is in C∞(R, R), then there is an arbitrarily
small perturbation of f such that the corresponding differential equation634 11. Bifurcation
has only hyperbolic rest points. Moreover, the set of all scalar differential
equations with hyperbolic rest points is open.
The proof of Thom’s theorem is not trivial. But, for the simple case
that we are considering, part of Thom’s result is a corollary of the implicit
function theorem: If f has finitely many nondegenerate zeros, then every
sufficiently small perturbation of f has the same property.
To prove this fact, consider the Banach space C1(R, R) consisting of all
elements of C1(R, R) that are bounded in the C1-norm. Suppose that
f ∈ C1(R, R) has only nondegenerate zeros and consider the map ρ :
R × C1(R, R) → R given by (u, f) → f(u). This map is smooth (see Exer￾cise 11.8). Moreover, if ρ(u0, f0) = 0, then ρu(u0, f0) = f
(u0) = 0. Thus,
there is a map f → β(f) defined on a neighborhood U of f0 in C1(R, R)
with image in an open subset V ⊂ R such that β(f0) = u0 and f(β(f)) ≡ 0.
Moreover, if (u, f) ∈ V × U and f(u) = 0, then u = β(f). In other words,
every function in the neighborhood U has exactly one zero in V . Also, there
are open subsets U0 ⊆ U and V0 ⊆ V such that for each function f in U0
its derivative f
(u) = 0 whenever u ∈ V0. Hence, every function in U0 has
a unique nondegenerate zero in V0. If, in addition, the function f has only
finitely many zeros, then every perturbation of f has only nondegenerate
zeros.
Exercise 11.6. Consider the set of differential equations of the form ˙u = f(u)
on Rn that have a finite number of rest points. Show that the subset with hyper￾bolic rest points is open and dense in the C1 topology.
Jet spaces were used to analyze perturbations of scalar differential equa￾tions that have only hyperbolic rest points. The next objective is to show
that the conditions required for a saddle-node are the same as those for a
jet extension map to be transversal to the zero section of a jet bundle.
Consider the 1-jet extensions of smooth scalar maps and the image of
the zero section Z1 ⊂ J1(R, R). If j1(f)(u) ∈ Z1, then f has a saddle-node
at u; that is, f(u) = 0 and f
(u) = 0. Thus, to study the saddle-node
bifurcation, we must consider families of maps in C∞(R, R). In fact, we
will identify these families as elements of the space C∞(R × R, R) where a
typical element f is given by a function of two variables (u, λ) → f(u, λ).
Define a new jet bundle with total space J(1,0)(R × R, R) consisting of
all equivalence classes of triples (u, λ, f) ∈ R × R × C∞(R × R, R) where
the triples (v, δ, f) and (w, ν, g) are equivalent if
v = w, δ = ν, f(v, δ) = g(w, ν), fu(v, δ) = gu(w, ν),
and the bundle projection is given by [u, λ, f(u, λ), fu(u, λ)] → (u, λ). Our
somewhat nonstandard jet space J(1,0)(R×R, R) may be viewed as a space
of families of sections of the 1-jet bundle of functions in C∞(R, R).11.1 One-Dimensional State Space 635
The (1, 0)-jet extension of f ∈ C∞(R × R, R) is the map
j(1,0)(f) : R × R → J(1,0)(R × R, R)
given by j(1,0)(f)(u, λ)=[u, λ, f(u, λ), fu(u, λ)], and the image of the zero
section Z(1,0) is the set of all equivalence classes of triples of the form
(u, λ, 0).
The local representative of the (1, 0)-jet extension is given by
(u, λ) → (u, λ, f(u, λ), fu(u, λ)).
Also, the (1, 0)-jet extension is transverse to the zero section Z(1,0) at a
point (u, λ) where f(u, λ) = 0 and fu(u, λ) = 0 if the following conditions
are met: The vector space sum of
(i) the range of the derivative of the local representative of the (1, 0)-jet
extension; and
(ii) the tangent space of the local representation of Z(1,0) at (u, λ, 0, 0)
is equal to the entire space R4. In other words, these vector subspaces are
(i) the span of the vectors
(1, 0, fu(u, λ), fuu(u, λ)) and (0, 1, fλ(u, λ), fλu(u, λ)); and
(ii) the span of the vectors (1, 0, 0, 0) and (0, 1, 0, 0).
This transversality condition holds whenever
fλ(u, λ) = 0 and fuu(u, λ) = 0,
exactly the conditions for a nondegenerate saddle-node bifurcation.
Just as for the case of nondegenerate zeros, the subset of all families of
smooth maps that have a saddle-node bifurcation is dense, and this set can
be identified as the countable intersection of open and dense subsets of the
space C∞(R×R, R). Moreover, by using the implicit function theorem, it is
easy to prove that if a family has a saddle-node bifurcation at some point,
then a small perturbation of this family also has a saddle-node bifurcation
at a nearby point. Thus, by translating our criteria for saddle-node bifur￾cations into the geometric language of transversality, we have developed an
approach to showing that saddle-node bifurcations can be unavoidable in
all sufficiently small perturbations of some one-parameter families of maps;
and, as a result, we have a positive answer to the question “Is bifurcation
theory important?”
In the remainder of this chapter sufficient conditions are discussed for
nondegenerate bifurcations in one-parameter families. Transversality the￾ory could be applied in each case to show that, in an appropriate sense,
these bifurcations are generic.
The unavoidability of the saddle-node bifurcation in one-parameter fami￾lies has been established. A natural next question arises: “Are saddle-nodes636 11. Bifurcation
unavoidable in two-parameter families?” The answer is “yes.” In fact, noth￾ing new happens for the saddle-node bifurcation relative to multiparameter
families of maps. The reason is that the set corresponding to the saddle￾node has codimension one in an appropriate function space. On the other
hand, bifurcation theory in families with two or more parameters is gener￾ally much more difficult than the theory for one-parameter families because
global features of the dynamics must be taken into account (see, for exam￾ple, [65], [66], [119], and [261]).
Exercise 11.7. Formulate and prove a theorem based on the implicit function
theorem that can be used to show that a small perturbation of a family of maps
with a saddle-node bifurcation has a nearby saddle-node bifurcation.
Exercise 11.8. Prove: The map R × C1([a, b], R) → R given by (u, f) → f(u)
is smooth.
Exercise 11.9. Prove: There is a saddle-node bifurcation for some values of the
parameter λ in the family
u˙ = cos λ − u sin u.
Exercise 11.10. Draw the bifurcation diagram for the scalar family of differ￾ential equations
x˙ = λx − x2
.
The bifurcation at λ = 0 is called transcritical. Prove a proposition similar to
Proposition 11.2 for the existence of a transcritical bifurcation.
Exercise 11.11. (a) Draw the bifurcation diagram for the scalar family of dif￾ferential equations
x˙ = λx − x3
.
The bifurcation at λ = 0 is called the supercritical pitchfork. (b) Prove a propo￾sition similar to Proposition 11.2 for the existence of a supercritical pitchfork
bifurcation. (c) Compare and contrast the supercritical pitchfork bifurcation with
the subcritical pitchfork bifurcation whose normal form is
x˙ = λx + x3
.
Exercise 11.12. (a) Discuss bifurcations of rest point for
x˙ = z − x, y˙ = x − y, z˙ = f(y) − az
for the case f(y) = y/(1+y2) where a is the bifurcation parameter. (b) Formulate
and prove a theorem of the form: If f is such that . . . , then the system has a
. . . bifurcation.11.2 Saddle-Node Bifurcation via Lyapunov-Schmidt Reduction 637
11.2 Saddle-Node Bifurcation via
Lyapunov-Schmidt Reduction
In this section, saddle-node bifurcation is discussed for the n-dimensional
family of differential equations (11.1) given by
u˙ = f(u, λ), u ∈ Rn, λ ∈ R.
Sufficient conditions for saddle-node bifurcation, as should be clear from
mastery of the previous section, do not mention time-dependent solutions
differential equations in the family; rather, the hypotheses require only
properties of the function f : Rn × R → Rn that defines the family of
vector fields associated with the family of differential equations. In concert
with this fact, u0 ∈ Rn is a saddle-node for f : Rn × R → Rn at λ0 if
f(u0, λ0) = 0, the linear transformation fu(u0, λ0) : Rn → Rn has zero
as an eigenvalue with algebraic multiplicity one, and all other eigenvalues
have nonzero real parts. Also, a saddle-node bifurcation is said to occur
at a saddle-node u = u0 for the parameter value λ = λ0 if the following
conditions are met:
SNB1 There is a number p0 > 0 and a smooth curve p → β(p) in Rn × R
such that β(0) = (u0, λ0) and f(β(p)) ≡ 0 for |p| < p0.
SNB2 The curve β has a quadratic tangency with Rn × {λ0} at (u0, λ0).
More precisely, if the components of β are defined by
β(p)=(β1(p), β2(p)),
then β2(0) = λ0, β
2(0) = 0, and β
2 (0) = 0.
SNB3 If p = 0, then the matrix fu(β(p)) is infinitesimally hyperbolic. Also,
exactly one eigenvalue of the matrix crosses the imaginary axis with
nonzero speed as the parameter p passes through p = 0.
The next theorem, called the saddle-node bifurcation theorem, gives suf￾ficient generic conditions for a saddle-node bifurcation to occur.
Theorem 11.13. Suppose that f : Rn × R → Rn is a smooth function,
u = u0 is a saddle-node for f at λ = λ0, and the kernel of the linear
transformation fu(u0, λ0) : Rn → Rn is spanned by the nonzero vector
k ∈ Rn. If fλ(u0, λ0) ∈ Rn and fuu(u0, λ0)(k, k) ∈ Rn are both nonzero and
both not in the range of fu(u0, λ0), then there is a saddle-node bifurcation
at (u, λ)=(u0, λ0) (that is SNB1, SNB2, and SNB3 are met). Moreover,
among all C∞ one-parameter families that have a saddle-node, those that
undergo a saddle-node bifurcation form an open and dense subset.
Second derivatives that appear in the statement of Theorem 11.13 are
easily understood from the correct point of view. Indeed, suppose that
g : Rn → Rn is a smooth function given by u → g(u) and recall that its638 11. Bifurcation
(first) derivative Dg is a map from Rn into the linear transformations of
Rn; that is, Dg : Rn → L(Rn, Rn). For u, v, w ∈ Rn, the derivative of g
at u in the direction w is denoted by Dg(u)w. Assume that g ∈ C2. The
derivative of the map u → Dg(u)w for fixed w at u ∈ Rn in the direction
v is given by
d
dtDg(u + tv)w



t=0
= (D2g(u)w)v = D2g(u)(w, v).
Hence, to compute the second derivative D2g in case g ∈ C2, it suffices
to compute the first derivative of the map u → Dg(u)w. Also, recall from
calculus that the function (w, v) → D2g(u)(w, v) is bilinear and symmet￾ric. Linearity follows from the linearity of first derivatives; Symmetry is a
restatement of the equality of mixed partial derivatives.
Exercise 11.14. Suppose that f : R × R × R → R × R is given by
f(x, y, λ)=(λ − x2 + xy, −2y + x2 + y2
)
and u := (x, y). Show that f satisfies all the hypotheses of Theorem 11.13 at
(u, λ) = (0, 0). Draw the phase portrait of ˙u = f(u, λ) near (u, λ) = (0, 0) for
λ < 0, λ = 0, and λ > 0.
Exercise 11.15. Prove that if g ∈ C2, then D2g(u)(v, w) = D2g(u)(w, v).
The proof of Theorem 11.13 is straightforward:
Proof. Assume, with no loss of generality, that u = 0 is a saddle-node for
f at λ = 0. Also, assume that zero is an eigenvalue of the linearization
fu(0, 0) : Rn → Rn with algebraic multiplicity one, and the kernel K of this
linear transformation is one-dimensional, say K = [k].
Using the Lyapunov-Schmidt reduction and linear algebra, there is an
(n − 1)-dimensional complement K⊥ to K in Rn whose basis is
k⊥
2 ,...,k⊥
n .
Corresponding to these choices, there is a coordinate transformation Ψ :
R × Rn−1 → Rn given by
(p, q) → pk +n
i=2
qik⊥
i
where, in the usual coordinates of Rn−1, the point q is given by q =
(q2,...,qn). Likewise, the range R of fu(0, 0) is (n − 1)-dimensional with
a one-dimensional complement R⊥. Let Π : Rn → R and Π⊥ : Rn → R⊥
be corresponding complementary linear projections.11.2 Saddle-Node Bifurcation via Lyapunov-Schmidt Reduction 639
Consider the map  : R × Rn−1 × R → R given by
(p, q, λ) → Πf(Ψ(p, q), λ).
Because f(0, 0) = 0, (0, 0, 0) = 0. From equation (8.63) of the abstract
formulation of the Lyapunov-Schmidt reduction, q(0, 0, 0) is invertible as
a linear transformation Rn−1 → Rn−1. Thus there is a function h : R×R →
Rn−1 given by (p, λ) → h(p, λ) such that h(0, 0) = 0 and, for (p, λ) in a
sufficiently small neighborhood of the origin in Rn−1 × R,
Πf(Ψ(p, h(p, λ)), λ) ≡ 0. (11.6)
Though not logically necessary for the proof, it is instructive to check
the invertibility of the derivative directly. In fact,
q(0, 0, 0) = Πfu(0, 0)Ψq(0, 0).
But Ψq(0, 0) : Rn−1 → Rn is given by
Ψq(0, 0)q = n
i=2
qik⊥
i .
Hence, the range of Ψq is the complement of the Kernel fu(0, 0) previously
chosen. Also Ψq is an isomorphism onto its range. On the complement of
its kernel, fu(0, 0) is an isomorphism onto its range and Π is the identity
on this range. In other words, q(0, 0, 0) is an isomorphism.
Viewed geometrically, the function h defines a two-dimensional surface
in R × Rn−1 × R given by {(p, h(p, λ), λ):(p, λ) ∈ R × R} which lies in
the zero set of . In addition, the (Lyapunov-Schmidt) reduced function is
τ : R × R → R⊥ defined by
(p, λ) → Π⊥f(Ψ(p, h(p, λ)), λ).
Of course, if (p, λ) is a zero of τ , then f(Ψ(p, h(p, λ)), λ) = 0.
A direct check shows τ (0, 0) = 0. Suppose for the moment that τλ(0, 0) =
0. The implicit function theorem would imply that there is a unique curve
p → γ(p) in R such that γ(0) = 0 and τ (p, γ(p)) ≡ 0. Moreover, in this
case, it follows that
f(Ψ(p, h(p, γ(p))), γ(p)) ≡ 0.
In other words, the image of the function β defined by
p → (Ψ(p, h(p, γ(p)), γ(p))
is a curve in the zero set of f(u, λ) that passes through the point (u, λ) =
(0, 0).640 11. Bifurcation
To show SNB1, it suffices to prove the inequality τλ(0, 0) = 0. Note first
that
τλ(0, 0) = Π⊥(fu(0, 0)Ψq(0, 0)hλ(0, 0) + fλ(0, 0)).
Because Π⊥ projects to the complement of the range of fu(0, 0), the last
formula reduces to
τλ(0, 0) = Π⊥fλ(0, 0).
By hypothesis, fλ(0, 0) is a nonzero vector that is not in R; therefore,
τλ(0, 0) = 0, as required.
To prove SNB2, we will show that γ
(0) = 0 and γ(0) = 0. Note first
that the derivative of both sides of identity τ (p, γ(p)) ≡ 0 with respect to
p is given by
τp(p, γ(p)) + τλ(p, γ(p))γ
(p) ≡ 0. (11.7)
Moreover, for p = 0 the equality γ(0) = 0 implies
τp(0, 0) + τλ(0, 0)γ
(0) = 0.
Recall that τλ(0, 0) = 0. Using the definition of τ ,
τp(p, λ)=Π⊥fu(Ψ(p, h(p, λ)), λ)

Ψp(p, h(p, λ)) + Ψq(p, h(p, λ))hp(p, λ)

,
(11.8)
and, in particular,
τp(0, 0) = Π⊥fu(0, 0)
Ψp(0, 0) + Ψq(0, 0)hp(0, 0)
.
Because Π⊥ projects to the complement of the range of fu(0, 0), it follows
that τp(0, 0) = 0, and therefore γ
(0) = 0. This fact together with equation
(11.7) imply the equality
τpp(0, 0) + τλ(0, 0)γ(0) = 0.
Thus,
γ(0) = −τpp(0, 0)
τλ(0, 0) .
To prove the inequality τpp(0, 0) = 0, first use equation (11.8) and recall
that Π⊥ projects to the complement of the range of fu(0, 0) to obtain the
equality
τpp(0, 0) = Π⊥fuu(0, 0)
Ψp(0, 0) + Ψq(0, 0)hp(0, 0)2
,
where “the square” is shorthand for the argument of the bilinear form
fuu(0, 0) on Rn.
Differentiate both sides of identity (11.6) with respect to p at p = 0 to
obtain the equation
Πfu(0, 0)
Ψp(0, 0) + Ψq(0, 0)hp(0, 0)
= 0. (11.9)11.2 Saddle-Node Bifurcation via Lyapunov-Schmidt Reduction 641
Because Π projects to the range of fu(0, 0), equation (11.9) is equivalent
to the equation
fu(0, 0)(Ψp(0, 0) + Ψq(0, 0)hp(0, 0)) = 0,
and therefore the vector
Ψp(0, 0) + Ψq(0, 0)hp(0, 0)
is in the kernel K of fu(0, 0). Easy deductions from the definition of Ψ are
that Ψp(0, 0) = k ∈ K and Ψq(0, 0)hp(0, 0) ∈ K⊥. Thus, hp(0, 0) = 0, and
as a consequence τpp(0, 0) = 0 if and only if
fuu(0, 0)(k, k) = 0, fuu(0, 0)(k, k) ∈ R. (11.10)
This completes the proof of SNB2.
To prove SNB3, and complete the proof of the theorem, reconsider the
curve β of rest points given by p → (Ψ(p, h(p, γ(p)), γ(p)). We must show
that the matrix fu(β(p)) is invertible for small nonzero p ∈ R and a single
eigenvalue of fu(β(p)) passes through zero with nonzero speed at p = 0. In
other words, the rest points on the curve β are hyperbolic for p = 0, and
there is a generic change of stability at p = 0. Of course, the first condition
follows from the second.
To analyze the second condition, consider the eigenvalues of fu(β(p)). By
the hypothesis of the theorem, there is exactly one zero eigenvalue at p = 0.
Thus, there is a curve p → σ(p) in the complex plane such that σ(0) = 0 and
σ(p) is an eigenvalue of fu(β(p)). Also, there is a corresponding eigenvector
V (p) such that
fu(β(p))V (p) = σ(p)V (p), (11.11)
V (0) = k.
By differentiating both sides of the identity (11.11) with respect to p at
p = 0 and simplifying the result,
fuu(0, 0)(k, k) + fu(0, 0)V 
(0) = σ
(0)k.
Its projection is,
Π⊥fuu(k, k) = σ
(0)Π⊥k.
In view of inequality (11.10), Π⊥fuu(0, 0)(k, k) = 0; therefore, σ
(0) is a
nonzero real number. ✷
Exercise 11.16. Prove: With the notation as in the proof of Theorem 11.13,
if Π⊥k = 0 and n ≥ 2, then zero is an eigenvalue of fu(0, 0) with multiplicity at
least two.642 11. Bifurcation
Exercise 11.17. Suppose that A : Rn → Rn is a linear transformation with
exactly one zero eigenvalue. Show that there is a nonzero “left eigenvector” w ∈
Rn such that wTA = 0. Also, show that v is in the range of A if and only if
v, w	 = 0. Discuss how this exercise gives a method to verify the hypotheses of
Theorem 11.13.
Exercise 11.18. Verify the existence of a saddle-node bifurcation for the func￾tion f : R2 × R → R2 given by
f(x, y, λ)=(λ − x2
, −y).
Exercise 11.19. Determine the bifurcation diagram for the phase portrait of
the differential equation
xx¨ + ax˙
2 = b
where a and b are parameters.
Exercise 11.20. [Hamiltonian saddle-node] Suppose that
u˙ = f(u, λ), u ∈ R2 (11.12)
is a planar Hamiltonian family with parameter λ ∈ R. Prove that if f(u0, λ0)=0
and the corresponding linearization at u0 has a zero eigenvalue, then this eigen￾value has algebraic multiplicity two. In particular, a planar Hamiltonian system
cannot have a saddle-node. Define (u0, λ0) to be a Hamiltonian saddle-node at
λ0 if f(u0, λ0) = 0 and fu(u0, λ0) has a zero eigenvalue with geometric multiplic￾ity one. A Hamiltonian saddle-node bifurcation occurs if the following conditions
hold:
• There exist s0 > 0 and a smooth curve γ in R2×R such that γ(0) = (u0, λ0)
and f(γ(s)) ≡ 0 for |s| < s0.
• The curve of critical points γ is quadratically tangent to R2 × {λ0} at
(u0, λ0).
• The Lyapunov stability type of the rest points on the curve γ changes at
s = 0.
Prove the following proposition formulated by Jason Bender [26]: Suppose that the
origin in R2×R is a Hamiltonian saddle-node for (11.12) and k ∈ R2 is a nonzero
vector that spans the one-dimensional kernel of the linear transformation fu(0, 0).
If the two vectors fλ(0, 0) ∈ R2 and fuu(0, 0)(k, k) ∈ R2 are nonzero and not in
the range of fu(0, 0), then a Hamiltonian saddle-node bifurcation occurs at the
origin.
Reformulate the hypotheses of the proposition as conditions on the Hamilto￾nian for the family so that there is no mention of the vector k. Also, generalize
the proposition to Hamiltonian systems on R2n. See [180] for the corresponding
result for Poincar´e maps at periodic orbits of Hamiltonian systems.
Discuss the Hamiltonian saddle-node bifurcation for the following model of a
pendulum with feedback control
x˙ = y, y˙ = − sin x − αx + β
(see [264]).11.3 Poincar´e-Andronov-Hopf Bifurcation 643
Exercise 11.21. Consider the system of differential equations
T˙ = a − 3T + 3ce1/T , c˙ = 4 − c − ce1/T
It is a simplified model for a chemical concentration c in a stirred tank reactor with
input flow, output flow, and temperature T. The basic problem is to determine the
existence, number, and stability types of steady states as the control parameter
a > 0 varies and the physical requirements c > 0 and T > 0 are respected. (a)
Prove that a steady state exists for all values of the control parameter. (b) Prove
that a saddle-node bifurcation occurs for at least one critical value of the control.
Remark: Problems of the type introduced in this exercise for stirred tank reactors
often lead to accessible unanswered questions of physical significance.
11.3 Poincar´e-Andronov-Hopf Bifurcation
Consider the family of differential equations
u˙ = F(u, λ), u ∈ RN , λ ∈ RM (11.13)
where λ is a vector of parameters.
Definition 11.22. An ordered pair (u0, λ0) ∈ RN × RM consisting of a
parameter value λ0 and a rest point u0 for the corresponding member of
the family (11.13) is called a Hopf point if there is a curve C in RN × RM,
called an associated curve, that is given by  → (C1(), C2()) and satisfies
the following properties:
(i) C(0) = (u0, λ0) and F(C1(), C2()) ≡ 0.
(ii) The linear transformation given by the derivative Fu(C1(), C2()) :
RN → RN has a pair of nonzero complex conjugate eigenvalues α()±
β()i, each with algebraic (and geometric) multiplicity one. Also,
α(0) = 0, α
(0) = 0, and β(0) = 0.
(iii) Except for the eigenvalues ±β(0)i, all other eigenvalues of Fu(u0, λ0)
have nonzero real parts.
In other words, a one-parameter family of differential equations has a
Hopf point if a single pair of complex conjugate eigenvalues, associated
with the linearizations of a corresponding family of rest points, crosses the
imaginary axis in the complex plane with nonzero speed at the parameter
value of the bifurcation point, whereas all other eigenvalues have nonzero
real parts. We will show that if some additional generic assumptions are
met, then there are members of the family (11.13) that have a limit cycle
near the Hopf point.
Because the linear transformation given by the derivative Fu(u0, λ0) at
the Hopf point (u0, λ0) has exactly two eigenvalues on the imaginary axis,644 11. Bifurcation
0 0
Stable limit cycle
= 0
Stable sink Weak focus
Figure 11.3: Supercritical Hopf bifurcation: A limit cycle emerges from a
weak focus as the bifurcation parameter is increased.
the results in Chapter 7, especially equation (7.31), can be used to show that
there is a center manifold reduction for the family (11.13) that produces a
family of planar differential equations
u˙ = f(u, λ), u ∈ R2, λ ∈ RM, (11.14)
with a corresponding Hopf point. Moreover, there is a product neighbor￾hood U × V ⊂ RN × RM of the Hopf point (u0, λ0) such that if λ ∈ V and
the corresponding member of the family (11.13) has a bounded orbit in U,
then this same orbit is an invariant set for the corresponding member of
the planar family (11.14). Thus, it suffices to consider the bifurcation of
limit cycles from the Hopf point of this associated planar family.
There are important technical considerations related to the smoothness
and uniqueness of the planar family obtained by a center manifold reduc￾tion at a Hopf point. By the results in Chapter 7, if the family (11.13) is
C1, then the augmented family (obtained by adding a new equation cor￾responding to the parameters) has a local C1 center manifold. But this
result is not strong enough for the proof of the Hopf bifurcation theorem
given below. In fact, we will require the reduced planar system (11.14) to
be C4. Fortunately, the required smoothness can be proved. In fact, using
the fiber contraction principle as in Chapter 7, together with an induction
argument, it is possible to prove that if 0 <r< ∞ and the family (11.13)
is Cr, then the reduced planar system at the Hopf point is also Cr in a
neighborhood of the Hopf point. Also, whereas local center manifolds are
not necessarily unique, it turns out that all rest points, periodic orbits,
homoclinic orbits, and so on, that are sufficiently close to the original rest
point, are on every center manifold. Thus, the bifurcation phenomena that11.3 Poincar´e-Andronov-Hopf Bifurcation 645
are determined by reduction to a center manifold do not depend on the
choice of the local center manifold (see, for example, [66]).
Definition 11.23. (1) A set S ⊂ RN has radii (r1, r2) relative to a point
p if r1 ≥ 0 is the radius of the smallest RN -ball centered at p that contains
S and the distance from S to p is r2 ≥ 0.
(2) The planar family (11.14) has a supercritical Hopf bifurcation at a Hopf
point with associated curve  → (c1(), c2()) if there are three positive
numbers 0, K1, and K2 such that for each  in the open interval (0, 0) the
differential equation ˙u = f(u, c2()) has a hyperbolic limit cycle with radii
(K1
√ + O(), K2
√ + O())
relative to the rest point u = c1().
(3) The bifurcation is called subcritical if there is a similar limit cycle for
the systems with parameter values in the range −0 << 0.
(4) The family (11.13) has a supercritical (respectively, subcritical) Hopf
bifurcation at a Hopf point if the corresponding (center manifold) reduced
system (11.14) has a supercritical (respectively, subcritical) Hopf bifurca￾tion.
To avoid mentioning several similar cases, which can be treated with
essentially the same ideas, the following discussion is limited to Hopf points
such that the parametrized eigenvalues α±β i satisfy the additional assump￾tions
α
(0) > 0, β(0) > 0. (11.15)
In particular, attention is restricted to the supercritical Hopf bifurcation
as depicted in Figure 11.3.
Under the latter standing hypothesis (11.15), a rest point on the associ￾ated curve  → c() of the Hopf point is a stable hyperbolic focus for the
corresponding system (11.14) for  < 0 and an unstable hyperbolic focus for
 > 0. By introducing an additional hypothesis that implies “weak attrac￾tion” to the rest point u0 at the parameter value λ0, the main result of this
section is the existence of a stable limit cycle that “bifurcates from this
rest point” as  increases through  = 0.
Family (11.14) is transformed to a more convenient form by a local change
of coordinates and a reduction to one-parameter. After the translation v =
u − c1(), it becomes
v˙ = f(v + c1(), λ)
with f(0 + c1(), c2()) ≡ 0. Associated rest points in the new coordinates
remain at the origin for all values of the parameter . Thus, it suffices to
consider family (11.14) reduced to the form
u˙ = f(u, λ), u ∈ R2, λ ∈ R. (11.16)
It has a Hopf point at (u, λ) = (0, 0) ∈ R2 ×R and associated curve c given
by λ → (0, λ).646 11. Bifurcation
Proposition 11.24. If (u, λ) = (0, 0) ∈ R2×R is a Hopf point for the fam￾ily (11.16) with associated curve λ → (0, λ) and eigenvalues α(λ) ± β(λ)i,
then there is a smooth parameter-dependent linear change of coordinates of
the form u = L(λ)z that transforms the system matrix A(λ) := fu(0, λ) of
the linearization at the origin along the associated curve into the Jordan
normal form
α(λ) −β(λ)
β(λ) α(λ)
	
.
Proof. Suppose that w(λ) = u1(λ) + u2(λ)i is a (nonzero) eigenvector for
the eigenvalue α(λ) + β(λ)i. There is an eigenvector of the form

1
0

−

v1(λ)
v2(λ)

i.
To prove this fact, it suffices to find a family of complex numbers c(λ) +
d(λ)i such that
(c + d i)(u1 + u2 i) = 
1
0

−

v1
v2

i
for a family of numbers v1, v2 ∈ R where the minus sign is inserted to
determine a convenient orientation. Equivalently, it suffices to solve the
equation
cu1 − du2 =

1
0

,
which is expressed in matrix form as follows:
(u1, −u2)

 c
d

=

1
0

.
Because eigenvectors w and ¯w corresponding to the distinct eigenvalues
α ± β i are linearly independent and
(u1, −u2)
 1 1
−i i	
= (w, w¯),
det [u1, −u2] = 0. Therefore, there is a unique solution for the vector (c, d).
Using this fact, the eigenvalue equation is
A

 1
0
	
− i
v1
v2
	 
= (α + iβ)

 1
0
	
− i
v1
v2
	 
,
and its real and imaginary parts are
A
1
0
	
= α
1
0
	
+ β
v1
v2
	
, Av1
v2
	
= −β
1
0
	
+ α
v1
v2
	
. (11.17)11.3 Poincar´e-Andronov-Hopf Bifurcation 647
Hence, if
L := 1 v1
0 v2
	
,
then
AL = L
α −β
β α 	
.
Again, since the vectors u1 and u2 are linearly independent, so are the
following nonzero scalar multiples of these vectors
1
0
	
,
v1
v2
	
.
This completes the proof that the matrix L is invertible. Moreover, v1 and
v2 can be solved for explicitly. Indeed, using the equations (11.17),
(A − αI)
1
0
	
= β
v1
v2
	
.
With
A := a11 a12
a21 a22 	
,
the desired vectors are
v1 = a11 − α
β , v2 = a21
β .
Here β := β(λ) is not zero at λ = 0, so the functions λ → v1(λ) and λ →
v2(λ) are smooth. Finally, the change of coordinates v = L(λ)z transforms
the family of differential equations (11.16) to ˙z = L−1(λ)f(L(λ)z, λ), and
the linearization of the transformed equation at z = 0 is given by
α(λ) −β(λ)
β(λ) α(λ)
	
.
The matrix function λ → L−1(λ) is also smooth at the origin. In fact, it is
given by
L−1 = 1
v2
v2 −v1
0 1 	
where 1/v2(λ) = β(λ)/a21(λ). Division is permitted because a21(λ) =
0 implies the linearization has real eigenvalues, in contradiction to the
hypotheses. ✷
By Proposition 11.24, there is no loss of generality in assuming that
differential equation (11.16) has the form
x˙ = α(λ)x − β(λ)y + g(x, y, λ),
y˙ = β(λ)x + α(λ)y + h(x, y, λ) (11.18)648 11. Bifurcation
where (1) the functions g and h together with their first partial derivatives
with respect to the space variables vanish at the origin; (2) the real func￾tions λ → α(λ) and λ → β(λ) are such that α(0) = 0 (because the real part
of the linearization must vanish at λ = 0) and, by the standing assump￾tion, α
(0) > 0 (because the derivative of the real part does not vanish at
λ = 0); (3) by the assumption that β(0) > 0, the eigenvalues α(λ) ± iβ(λ)
are nonzero complex conjugates for |λ| sufficiently close to zero; and (4)
β(0) = 1, a normalization that is achieved by a reparametrization of time
in the family (11.18).
A periodic orbit of the family (11.18) will be proved to exist near the
origin of the coordinate system by applying the implicit function theorem
to find a zero of the associated displacement function defined along the
x-axis.
For the desired application of the implicit function theorem, the displace￾ment function must be proved to have a smooth extension to the origin.
While it is clear that the displacement has a continuous extension to the
origin—define its value at the rest point to be zero—it is not clear that
the extended displacement function is smooth. Indeed, recall the standard
proof that a return map exists near some point p on a Poincar´e section. It
is based on the implicit function theorem and requires the vector field to be
transverse to the Poincar´e section at p. But this condition is not satisfied
at the origin for members of the family (11.18) because the vector field
vanishes at this rest point.
Smoothness of the displacement function for the system (11.18) is proved
as an application of the blowup construction discussed in Section 1.7.6. By
changing to polar coordinates, the family (11.18) is transformed to
r˙ = α(λ)r + p(r, θ, λ), ˙
θ = β(λ) + q(r, θ, λ) (11.19)
where
p(r, θ, λ) := g(r cos θ, r sin θ, λ) cos θ + h(r cos θ, r sin θ, λ) sin θ,
q(r, θ, λ) := 1
r

h(r cos θ, r sin θ, λ) cos θ − g(r cos θ, r sin θ, λ) sin θ

.
Since the functions (x, y) → g(x, y, λ) and (x, y) → h(x, y, λ) and their
first partial derivatives vanish at the origin, the function q in system (11.19)
has a removable singularity at r = 0. Thus, this system is smooth. More￾over, by the change to polar coordinates, the rest point at the origin in
the plane has been blown up to the circle {0} × T on the phase cylinder
R × T. In our case, where β(λ) = 0, the rest point at the origin (for each
choice of the parameter λ) corresponds to the periodic orbit on the cylinder
given by the solution r(t) ≡ 0 and θ(t) = β(λ)t. A Poincar´e section on the
cylinder for these periodic orbits, for example, the line θ = 0, has a smooth
(parametrized) return map that is equivalent to the corresponding return
map on the x-axis for the family (11.18). Thus, if we blow down—that11.3 Poincar´e-Andronov-Hopf Bifurcation 649
is, project back to the plane—then the image of our transversal section
is a smooth section for the flow with a smooth return map and a smooth
return-time map. In particular, both maps are smooth at the origin. In
other words, the displacement function on the x-axis of the plane is conju￾gate (by the change of coordinates) to the smooth displacement function
defined on the line θ = 0 in the cylinder. Another way to view the situation
is to ignore the smoothness at the origin in the plane and work with the
blowup where smoothness is obvious; prove the existence of a stable period
orbit there that does not intersect the circle at r = 0; and blow down in
a neighborhood of this periodic orbit where the blow down is obviously
smooth.
On the phase cylinder, the bifurcation problem is of a different type:
bifurcation of periodic orbits from a periodic orbit rather than the bifur￾cation of periodic orbits from a rest point. Indeed, Hopf bifurcation on the
phase cylinder is analogous to bifurcation from a multiple limit cycle as
in our previous discussion following the Weierstrass preparation theorem
(Theorem 8.21) on page 463.
For the generic case, the limit cycle (given on the cylinder by the set
{(r, θ) : r = 0} for the family (11.19) at λ = 0) will be proved to have
multiplicity of three. But, unlike the general theory for bifurcation from
a multiple limit cycle with multiplicity three, the Hopf bifurcation has an
essential new feature revealed by the geometry of the blowup: The bifur￾cation is symmetric with respect to the set {(r, θ) : r = 0}. More precisely,
each member of the family (11.19) is invariant under the change of coordi￾nates given by
R = −r, Θ = θ − π. (11.20)
While this symmetry has many effects, it should at least be clear that if a
member of the family (11.19) has a periodic orbit that does not coincide
with the set {(r, θ) : r = 0}, then the system has two periodic orbits: one
in the upper half cylinder, and one in the lower half cylinder. Also, if the
set {(r, θ) : r = 0} is a limit cycle, then it cannot be semistable, that
is, attracting on one side and repelling on the other (see Exercise 11.25).
The geometry is similar to the geometry of the pitchfork bifurcation (see
Exercise 11.11 and Section 11.4).
The general theory of bifurcations with symmetry is an important topic
that is covered in detail in the excellent books [112] and [113].
Exercise 11.25. Prove: If the set Γ := {(r, θ) : r = 0} on the cylinder is a
limit cycle for the member of the family (11.19) at λ = 0, then this limit cycle is
not semistable. State conditions that imply Γ is a limit cycle and conditions that
imply it is a hyperbolic limit cycle.
By the hypotheses, the line {(r, θ) : θ = 0} is a transversal section for
the flow of system (11.19) on the phase cylinder for |r| sufficiently small.650 11. Bifurcation
Moreover, as mentioned above, there is a smooth displacement function
defined on this section. In fact, let t → (r(t, ξ, λ), θ(t, ξ, λ)) denote the
solution of the differential equation (11.19) with the initial condition
r(0, ξ, λ) = ξ, θ(0, ξ, λ)=0,
and note that (because β(0) = 1)
θ(2π, 0, 0) = 2π, ˙
θ(2π, 0, 0) = β(0) = 0.
By an application of the implicit function theorem, there is a product neigh￾borhood U0×V0 of the origin in R×R, and a function T : U0×V0 → R such
that T(0, 0) = 2π and θ(T(ξ, λ), ξ, λ) ≡ 2π. Thus, the desired displacement
function δ : U0 × V0 → R is defined by
δ(ξ, λ) := r(T(ξ, λ), ξ, λ) − ξ. (11.21)
The displacement function (11.21) is complicated by the presence of the
implicitly defined return-time function T, a difficulty that can be avoided
by yet another change of coordinates. Indeed, since T(0, 0) = 2π and ˙
θ(t, 0, 0) = β(0) = 0, it follows from the continuity of the functions T and ˙
θ and the implicit function theorem that there is a product neighborhood
U ×V of the origin with U ×V ⊆ U0 ×V0 such that for each (ξ, λ) ∈ U ×V
the function t → θ(t, ξ, λ) is invertible on some bounded time interval con￾taining T(ξ, λ) (see Exercise 11.32). Moreover, if the inverse function is
denoted by s → θ−1(s, ξ, λ), then the function ρ : R × U × V → R defined
by
ρ(s, ξ, λ) = r(θ−1(s, ξ, λ), ξ, λ)
is a solution of the initial value problem
dρ
ds = α(λ)ρ + p(ρ, s, λ)
β(λ) + q(ρ, s, λ) , ρ(0, ξ, λ) = ξ
and
ρ(2π, ξ, λ) = r(T(ξ, λ), ξ, λ).
By renaming the variables ρ and s to new variables r and θ, the displace￾ment function δ : U × V → R as defined in equation (11.21) with respect
to the original variable r is also given by the formula
δ(ξ, λ) = r(2π, ξ, λ) − ξ (11.22)
where θ → r(θ, ξ, λ) is the solution of the initial value problem
dr
dθ = α(λ)r + p(r, θ, λ)
β(λ) + q(r, θ, λ) , r(0, ξ, λ) = ξ. (11.23)
In particular, with respect to the differential equation (11.23), the “return
time” does not depend on the position ξ along the Poincar´e section or the
value of the parameter λ; rather, it has a constant value 2π.11.3 Poincar´e-Andronov-Hopf Bifurcation 651
Definition 11.26. Suppose that (u, λ) = (0, 0) ∈ R2 × R is a Hopf point
for the family (11.16). The corresponding rest point u = 0 is called a
weak attractor (respectively, a weak repeller) if the associated displacement
function (11.22) is such that δξξξ(0, 0) < 0 (respectively, δξξξ(0, 0) > 0). In
addition, the Hopf point (u, λ) = (0, 0) is said to have multiplicity one if
δξξξ(0, 0) = 0.
Theorem 11.27 (Hopf Bifurcation Theorem). If the family of differ￾ential equations (11.16) has a Hopf point at (u, λ) = (0, 0) ∈ R2 × R and
the corresponding rest point at the origin is a weak attractor (respectively,
a weak repeller), then there is a supercritical (respectively, subcritical) Hopf
bifurcation at this Hopf point.
Proof. Assume that the family (11.16) is C4. By Proposition 11.24, there
is a smooth change of coordinates that transforms the family (11.16) into
the family (11.18). Moreover, because β(0) = 0, the function
S(r, θ, λ) := α(λ)r + p(r, θ, λ)
β(λ) + q(r, θ, λ) ,
and therefore the family of differential equations
dr
dθ = S(r, θ, λ), (11.24)
is as smooth as the original differential equation (11.16); that is, it is at
least in class C4.
The associated displacement function δ defined in equation (11.22) is
given by the C4 function
δ(ξ, λ) := r(2π, ξ, λ) − ξ (11.25)
where θ → r(θ, ξ, λ) is the solution of the differential equation (11.24)
with initial condition r(0, ξ, λ) = ξ. Moreover, each function ξ → δ(ξ, λ) is
defined in a neighborhood of ξ = 0 in R.
Since δ(0, λ) ≡ 0, the displacement function is represented as a series,
δ(ξ, λ) = δ1(λ)ξ + δ2(λ)ξ2 + δ3(λ)ξ3 + O(ξ4),
whose first-order coefficient is given by
δ1(λ) = δξ(0, λ) = rξ(2π, 0, λ) − 1
where θ → rξ(θ, 0, λ) is the solution of the variational initial value problem
drξ
dθ = Sr(0, θ, λ)rξ = α(λ)
β(λ)
rξ, rξ(0, 0, λ)=1.
Hence, by solving the scalar first-order linear differential equation,
δ1(λ) = rξ(2π, 0, λ) − 1 = e2πα(λ)/β(λ) − 1.652 11. Bifurcation
Because α(0) = 0,
δ(ξ, 0) = ξ2
δ2(0) + δ3(0)ξ + O(ξ2)

.
Note that if δ2(0) = 0, then δ(ξ, 0) has constant sign for sufficiently small
|ξ| = 0; therefore, the trajectories of the corresponding system (11.19) at
λ = 0 do not spiral around the origin of its phase plane (draw a picture).
Equivalently, the periodic orbit {(r, θ) : r = 0} on the phase cylinder is
a semistable limit cycle. But using the assumptions that α(0) = 0 and
β(0) = 0 and Exercise 11.25, this qualitative behavior cannot occur: The
existence of a semistable limit cycle on the phase cylinder violates the
symmetry (11.20), which carries over to the differential equation (11.23).
In fact, if θ → r(θ, ξ, λ) is a solution of equation (11.23), then so is the
function θ → −r(θ + π, ξ, λ). For all of these equivalent reasons, δ2(0) = 0.
Consider the function Δ : R × R → R defined on the domain of the
displacement function by
Δ(ξ, λ) = δ1(λ) + δ2(λ)ξ + δ3(λ)ξ2 + O(ξ3),
and note that
Δ(0, 0) = e2πα(0)/β(0) − 1=0,
Δξ(0, 0) = δ2(0) = 0,
Δξξ(0, 0) = 2δ3(0) = δξξξ(0, 0)/3 = 0,
Δλ(0, 0) = 2πα
(0)/β(0) > 0.
By Proposition 11.2, Δ has a saddle-node bifurcation at ξ = 0 for the
parameter value λ = 0. In particular, there is a curve ξ → (ξ, γ(ξ)) in
R × R with γ(0) = 0, γ
(0) = 0, and γ(0) = 0 such that Δ(ξ, γ(ξ)) ≡ 0.
As a result,
δ(ξ, γ(ξ)) = ξΔ(ξ, γ(ξ)) ≡ 0.
In other words, there is a periodic solution of the corresponding member
of the family (11.18) that meets the Poincar´e section at the point with
coordinate ξ whenever λ = γ(ξ).
For the remainder of the proof, assume that δξξξ(0, 0) < 0; the case where
δξξξ(0, 0) > 0 is similar.
Proposition 11.2 implies the inequality
γ(0) = −Δξξ(0, 0)
Δλ(0, 0) = − β(0)
6πα
(0) δξξξ(0, 0) > 0.
Therefore the coefficient of the leading-order term of the series
λ = γ(ξ) = γ(0)
2 ξ2 + O(ξ3)11.3 Poincar´e-Andronov-Hopf Bifurcation 653
does not vanish. Hence, the position coordinate ξ > 0 corresponding to a
periodic solution is represented as follows by a power series in √
λ :
ξ =

−λ 12πα
(0)
β(0)δξξξ(0, 0)	1/2
+ O(λ), (11.26)
and the distance from the periodic orbit to the origin is of the form K2
√
k+
O(k) for some constant K2. Using the construction discussed following
display (11.21), the function S = S(r, θ, λ) can be restricted to a compact
subset of its domain with no loss of generality. By continuity, the magnitude
of the partial derivative Sr is bounded by some constant K > 0 on such a
compact set. Note that S(0, θ, λ) ≡ 0 and apply the mean value theorem
to S to obtain the inequality
|r(θ, ξ, λ)| = |ξ| +
 θ
0
|S(r, φ, λ)| dφ ≤ |ξ| + K
 θ
0
|r| dφ.
By an application of Gronwall’s inequality,
|r(θ, ξ, λ)| ≤ ξe2πK.
Thus, the periodic solution lies in a ball whose radius is K1
√
k + O(k) for
some constant K1, as required.
The proof is completed by showing that the periodic solution correspond￾ing to ξ given by the equation (11.26) is a stable hyperbolic limit cycle.
Consider the Poincar´e map defined by
P(ξ, λ) := δ(ξ, λ) + ξ = ξ(Δ(ξ, λ) + 1)
and note that
Pξ(ξ, λ) = ξΔξ(ξ, λ) + Δ(ξ, λ)+1.
At the periodic solution, λ = γ(ξ); therefore,
Pξ(ξ, γ(ξ)) = ξΔξ(ξ, γ(ξ)) + 1.
The inequality Δ(ξ, γ(ξ)) ≡ 0 implies that
Δξ(ξ, γ(ξ)) = −Δλ(ξ, γ(ξ))γ
(ξ).
The relations Δλ(0, 0) > 0, γ
(0) = 0, and γ(0) > 0 and the choice of ξ > 0
sufficiently small imply the inequalities γ
(ξ) > 0 and −Δλ(ξ, γ(ξ)) < 0.
As a consequence, Δξ(ξ, γ(ξ)) < 0 and 0 < Pξ(ξ, γ(ξ)) < 1. In other words,
the periodic solution is a hyperbolic stable limit cycle. ✷
While the presentation given in this section discusses the most important
ideas needed to understand the Hopf bifurcation, there are a few unresolved
issues. Note first that sufficient conditions for the Hopf bifurcation are654 11. Bifurcation
given only for a two-dimensional system obtained by restriction to a center
manifold, not for the original system of differential equations. In particular,
the definition of a weak attractor is only given for two-dimensional systems.
Also, we have not discussed an efficient method to determine the sign of
the third space-derivative of the displacement function, an essential step
for practical applications of the Hopf bifurcation theorem. For a resolution
of the first issue see [170]; the second issue is addressed in the next section.
Exercise 11.28. Consider the two systems
r˙ = λr ± r3
, ˙
θ =1+ ar2
,
where (r, θ) are polar coordinates. Show that the − sign system has a supercritical
Hopf bifurcation and the + sign system has a subcritical Hopf bifurcation. The
given systems are normal forms for the Hopf bifurcation.
Exercise 11.29. Show that the system
x˙ = λx − y + xy2
, y˙ = x + λy + y3
has a subcritical Hopf bifurcation. Hint: Change to polar coordinates and com￾pute (explicitly) the Poincar´e map defined on the positive x-axis. Recall that
Bernoulli’s equation ˙z = a(t)z + b(t)zn+1 is transformed to a linear equation by
the change of variables w = z−n.
Exercise 11.30. The system
x˙ = a − x − 4 xy
1 + x2 ,
y˙ = bx
1 − y
1 + x2

arises in a model of chemical reactions where the state variables x and y repre￾sent concentrations of two chemical species. In the physically relevant parameter
space both a and b are positive. What can you say about the typical behavior
of the chemical concentrations as (a, b) ranges through the first quadrant of the
parameter plane?
Exercise 11.31. The system
x˙ = 1 − z, y˙ = z − 1, z˙ = x − y + az2
is a simplified version of a water reservoir model (see [36] or [80]), where z > 0. (a)
Show that the system has oscillatory behavior for some choices of the parameter
a but no limit cycles. Hint: Reduce to a two-dimensional system with w = x − y
and consider Hopf bifurcation. (b) Are the oscillations stable? Discuss.
Exercise 11.32. Suppose that K ⊆ R and W ⊆ Rk are open sets, g : K ×W →
R is a smooth function, and T > 0. If [0, T] ⊂ K, 0 ∈ W, and gt(t, 0) = 0 for
all t ∈ K, then there are open product neighborhoods I × U ⊆ K × W and
J × V ⊆ R × Rk with [0, T] ⊂ I and 0 ∈ U and a smooth function h : J × V → R11.3 Poincar´e-Andronov-Hopf Bifurcation 655
such that h(g(t, u), u) = t whenever (t, u) ∈ I×U. Hint: Consider the function G :
K×W ×R → R given by G(t, w, s) = g(t, w)−s and note that G(t, w, g(t, w)) ≡ 0
and Gt(t, w, g(t, w)) = 0. Apply the implicit function theorem to obtain a function
h such that G(h(s, w), w, s) ≡ 0 and h(g(t, w), w) = t. The implicit function is
only locally defined but it is unique. Use the uniqueness to show that h is defined
globally on an appropriate product neighborhood.
11.3.1 Multiple Hopf Bifurcation
The hypothesis in the Hopf bifurcation theorem, which states that a Hopf
point has multiplicity one, raises at least two important questions related to
the stability type (attractor or repeller) of the bifurcated limit cycle: How
to check the sign of the third space-derivative δξξξ(0, 0) of the displacement
function? What happens if δξξξ(0, 0) = 0? The answers to these questions
are discussed in this section.
For the second question, recall the proof of the Hopf bifurcation theorem
where the condition δξξξ(0, 0) = 0 ensures that the series representation
of the displacement function has a nonzero coefficient at the lowest pos￾sible order. A Hopf point is called multiple and the corresponding Hopf
bifurcation is called a multiple Hopf bifurcation in case δξξξ(0, 0) = 0.
Multiple Hopf bifurcation is analyzed here for the case of a planar vector
field that depends on a vector of parameters. More precisely, suppose the
parameter vector λ is in RM for the corresponding family of differential
equations
u˙ = f(u, λ), u ∈ R2 (11.27)
with the following additional properties: (1) the function f is real analytic;
(2) the state u = 0, at the parameter value λ = λ∗, is a rest point for
the differential equation ˙u = f(u, λ∗); and (3) the eigenvalues of the linear
transformation fu(0, λ∗) are nonzero pure imaginary numbers. Under these
assumptions, the displacement function δ is represented by a convergent
power series of the form
δ(ξ, λ) = ∞
j=1
δj (λ)ξj . (11.28)
Definition 11.33. The rest point at u = 0, for the member of the fam￾ily (11.27) at the parameter value λ = λ∗, is called a weak focus of order k
if k is a positive integer such that
δ1(λ∗) = ··· = δ2k(λ∗)=0, δ2k+1(λ∗) = 0.
This result is not difficult to prove. A special case is proved in the course
of the proof of the Hopf bifurcation theorem: If δ1(λ∗) = ··· = δ2k−1(λ∗) =
0, then δ2k(λ∗) = 0. In fact, this is another manifestation of the symmetry
given in display (11.20).656 11. Bifurcation
The next proposition is a corollary of the Weierstrass preparation theo￾rem (Theorem 8.21).
Proposition 11.34. If the family (11.27) has a weak focus of order k at
u = 0 for the parameter value λ = λ∗, then at most k limit cycles appear
in a corresponding multiple Hopf bifurcation. More precisely, there is some
 > 0 and some ν > 0 such that u˙ = f(u, λ) has at most k limit cycles in
the open set {u ∈ R2 : |u| < ν} whenever |λ − λ∗| < .
While Proposition 11.34 states that at most k limit cycles appear in a
multiple Hopf bifurcation at a weak focus of order k, additional informa￾tion about the set of coefficients {δ2j+1(λ) : j = 0,...,k} is required to
determine precisely how many limit cycles appear. For example, to obtain
the maximum number k of limit cycles, it suffices to have these coefficients
be independent in the following sense: There is some δ > 0 such that for
each j ≤ k and each  > 0, if |λ0 − λ∗| < δ and
δ1(λ0) = δ2(λ0) = ··· = δ2j−1(λ0)=0, δ2j+1(λ0) = 0,
then there is a point λ1 such that |λ1 − λ0| <  and
δ1(λ1) = ··· = δ2j−3(λ1)=0, δ2j−1(λ1)δ2j+1(λ1) < 0.
The idea is that successive odd-order coefficients can be obtained with
opposite signs by making small changes in the parameter vector. The reason
why this condition is important will be made clear later.
Before discussing the multiple Hopf bifurcation in more detail, a prelimi￾nary task is the explicit computation of the coefficients of the displacement
function. Suppose that the family of differential equations (11.27) has the
form
x˙ = x − y + p(x, y), y˙ = x + y + q(x, y), (11.29)
where p and q together with their first-order partial derivatives vanish at
the origin, and the system parameters consist of  and coefficients of the
Taylor series representations of p and q at the origin. The parameter 
is named separately because it is the real part of the eigenvalues of the
linearization at the origin. For formal calculations, the parameter vector
might be infinite in case p and q are arbitrary analytic functions. But in
the cases of most interest here, these analytic functions are polynomials.
The task at hand is to show how to compute Taylor coefficients of the
displacement function at (x, y) = (0, 0) for the family member at  = 0.
As a convenient notation, which is consistent with the notation used in
the Hopf bifurcation theorem, let the displacement function at the origin
for the family (11.29) be the function given by (ξ, ) → δ(ξ, ), where the
additional parameters are suppressed.11.3 Poincar´e-Andronov-Hopf Bifurcation 657
Set  = 0 in system (11.29), change to polar coordinates, and consider
the initial value problem
dr
dθ = r2A(r, θ)
1 + rB(r, θ)
, r(0, ξ) = ξ (11.30)
(similar to the initial value problem (11.23)) where ξ is the coordinate on
the Poincar´e section given by a segment of the line {(r, θ) : θ = 0}.
The solution r of differential equation (11.30) is analytic and therefore
represented by a series of the form
r(θ, ξ) = ∞
j=1
rj (θ)ξj . (11.31)
As an important remark, note that only the first few terms of this series
are required to determine δξξξ(0, 0). Therefore, the analyticity of the fam￾ily (11.30) is not necessary to verify the nondegeneracy condition for Hopf
bifurcation.
In view of the initial condition for the solution of differential equa￾tion (11.31), it follows that r1(θ) ≡ 1 and rj (0) = 0 for all j ≥ 2. Hence, if
the series (11.31) is inserted into the differential equation (11.30) and like
powers of ξ are collected, then the sequence {rj (θ)}∞
j=2 of coefficients can
be found recursively.
Because r1(θ) ≡ 1, the displacement function has the representation
δ(ξ, 0) = r(2π, ξ) − ξ = r2(2π)ξ2 + r3(2π)ξ3 + O(ξ4).
Recall that δ2(0) = 0. Therefore, r2(2π) = 0 and in turn
δ(ξ, 0) = r3(2π)ξ3 + O(ξ4). (11.32)
This derivation proves the useful formula
δξξξ(0, 0) = 3!r3(2π). (11.33)
Indeed, using power series the coefficient r3(2π) can and will be com￾puted explicitly (see Exercise 11.36 and the desired expression (11.45)).
Formula (11.33) is available to determine the sign of the derivative δξξξ(0, 0)
that determines the stability type of the bifurcated limit cycle.
Exercise 11.35. In the context of Definition 11.33, show that if δ1(λ∗) = ··· =
δ2k−1(λ∗) = 0, then δ2k(λ∗) = 0.
Exercise 11.36. Find an expression for r3(2π) in terms of the Taylor coeffi￾cients of the functions p and q in equation (11.29). Hint: Only the coefficients of
order two and three are required.658 11. Bifurcation
The method proposed above for determining Taylor coefficients of the
displacement function has the advantage of conceptual simplicity; its dis￾advantage is the requirement that a differential equation be solved to com￾plete each step of the algorithm. There is a more computationally efficient
procedure—introduced by Lyapunov—that is purely algebraic. The main
idea is to recursively construct a polynomial Lyapunov function for sys￾tem (11.29) by taking into account terms in the Taylor expansion up to
each order.
For notational convenience write the series expansions of p and q in the
form
p(x, y) = ∞
j=2
pj (x, y), q(x, y) = ∞
j=2
qj (x, y),
where pj and qj are homogeneous polynomials of degree j for each j =
2,...,∞; let V denote the proposed Lyapunov function represented for￾mally as the series
V (x, y) = 1
2
(x2 + y2) +∞
j=3
Vj (x, y), (11.34)
where each Vj is a homogeneous polynomial of degree j; and let
X(x, y) := (−y + p(x, y)) ∂
∂x + (x + q(x, y)) ∂
∂y
denote the vector field associated with the system (11.29).
Exercise 11.37. Suppose that V , in display (11.34), is an analytic function.
Show that there is a neighborhood U of the origin such that V (x, y) > 0 for
(x, y) ∈ U \ (0, 0).
To determine the stability of the rest point at the origin for the system
of differential equations corresponding to the vector field X, let ϕt denote
the flow of X and define the Lie derivative of V in the direction of the
vector field X by
(LXV )(x, y) = d
dtV (ϕt(x, y))



t=0
= gradV (x, y) · X(x, y).
Recall the discussion of Lyapunov’s direct method in Section 1.6. Using
the language of Lie derivatives, if V (x, y) > 0 and LXV (x, y) ≤ 0 on some
punctured neighborhood of the origin, then V is called a Lyapunov function
for system (11.29) at (x, y) = (0, 0), and the main result is
Theorem 11.38. If V is a Lyapunov function at (x, y) = (0, 0) for the
system (11.29) at  = 0 and LXV (x, y) < 0 for each point (x, y) in some
punctured neighborhood of the origin, then the rest point at the origin is
asymptotically stable.11.3 Poincar´e-Andronov-Hopf Bifurcation 659
Define Hn to be the vector space of all homogeneous polynomials of
degree n in the variables x and y; consider the vector field on R2 given
by R(x, y)=(x, y, −y, x); and observe that if V is a function defined on
R2, then the Lie derivative LRV can be viewed as the action of the linear
differential operator LR, defined by
LR := −y
∂
∂x + x
∂
∂y , (11.35)
on V . Explicitly,
(LRV )(x, y) = −yVx(x, y) + xVy(x, y),
where the subscripts denote partial derivatives (see Exercise 11.39).
Exercise 11.39. Prove that Hn is a finite-dimensional vector space, compute
its dimension, and show that the operator LR is a linear transformation of this
vector space.
Using the definition of the Lie derivative,
LXV (x, y) =
x +∞
j=3
Vjx(x, y)
 − y +∞
j=2
pj (x, y)

+ 
y +∞
j=3
Vjy(x, y)
x +∞
j=2
qj (x, y)

.
By collecting terms on the right-hand side of this identity according to their
degrees,
LXV (x, y) = xp2(x, y) + yq2(x, y)+(LRV3)(x, y) + O((x2 + y2)
2).
There are two key facts: The polynomial xp2(x, y) +yq2(x, y) is in H3 and
Proposition 11.40. If n is an odd integer, then LR : Hn → Hn is a linear
isomorphism.
Assuming for the moment the validity of Proposition 11.40, there exists
an element V3 ∈ H3 such that
(LRV3)(x, y) = −xp2(x, y) − yq2(x, y).
With this choice of V3, the terms of order three in the expression for LXV
vanish, and
LXV (x, y) = xp3(x, y) + yq3(x, y) + V3x(x, y)p2(x, y) + V3y(x, y)q2(x, y)
+ (LRV4)(x, y) + O((x2 + y2)
5/2).660 11. Bifurcation
Proposition 11.41. If n is an even integer, say n = 2k, then the linear
transformation LR : Hn → Hn has a one-dimensional kernel generated by
(x2 + y2)k ∈ Hn. Also, the homogeneous polynomial (x2 + y2)k generates a
one-dimensional complement to the range of LR.
Assuming the validity of Proposition 11.41, there is a constant L4 and
V4 := L4(x2 + y2)2 in H4 such that
LXV (x, y) = V4 + O((x2 + y2)
5/2). (11.36)
Equation (11.36) is useful. Indeed, if L4 = 0, then the function
V (x, y) = 1
2
(x2 + y2) + V3(x, y) + V4(x, y) (11.37)
determines the stability of the rest point at the origin. More precisely, if
L4 < 0, then V is a Lyapunov function in some sufficiently small neigh￾borhood of the origin and the rest point is stable. If L4 > 0, then the rest
point is unstable (to see this fact just reverse the direction of time).
Remark 3. These stability results do not require the vector field X to be
analytic. Also, the formal computations with V written as a power series
are justified because the Lyapunov function (11.37) (which is used to apply
Theorem 11.38) turns out to be a polynomial.
In case L4 = 0, the same procedure used to obtain it produces a new V
such that the leading term of the expression for LXV is L6(x2 + y2)3, and
so on. Thus, Lyapunov’s method produces a useful stability theorem:
Theorem 11.42. If L2n = 0, for n = 2, 3, 4,...,N, but L2N+2 = 0, then
the stability of the rest point at the origin is determined: If L2N+2 < 0,
then the rest point is stable. If L2N+2 > 0, then the rest point is unstable.
The constant L2k is called the kth Lyapunov quantity. By Theorem 11.42
and the algorithm for computing these Lyapunov quantities, we have a
method for constructing Lyapunov functions at linear centers of planar
systems. At least this is true in case a finite number of steps produces
a nonzero Lyapunov quantity. The corresponding polynomial Lyapunov
function determines the stability type of the rest point.
What happens if all Lyapunov quantities vanish? This question is answered
by the Lyapunov center theorem [162]:
Theorem 11.43 (Lyapunov Center Theorem). If the vector field X
is analytic and L2n = 0 for each integer n ≥ 2, then the origin is a cen￾ter. Moreover, the formal series for V is convergent in a neighborhood of
the origin and it represents a function whose level sets are orbits of the
differential equation corresponding to X.11.3 Poincar´e-Andronov-Hopf Bifurcation 661
Exercise 11.44. Write a program using an algebraic processor that upon input
of system (11.29) and an integer N outputs L2n, n = 2,...,N. Use your program
to compute L4 for the system (11.29) in case the coefficients of p and q are
regarded as parameters. Hint: Look ahead to page 669.
Before turning to the proofs of Propositions 11.40 and 11.41 (starting
on page 663), the relation between Taylor coefficients of the displacement
function and Lyapunov quantities is established by
Proposition 11.45. Suppose that ξ → δ(ξ, 0) is the displacement function
for the system (11.29) at  = 0, and L2n for n ≥ 2, are the corresponding
Lyapunov quantities. If k is a positive integer and L2j = 0 for the integers
j = 1,...,k − 1, then
∂2k−1δ
∂ξ2k−1 (0, 0) = (2k − 1)!2πL2k.
In particular, δξξξ(0, 0) = 3!2πL4. (2)
Proof. The last statement of the theorem is proved here; the general proof
is left as an exercise.
Equation (11.33) states that δξξξ(0, 0) = 3!r3(2π). Thus, it suffices to
prove the equality r3(2π)=2πL4.
In polar coordinates, the polynomial
V (x, y) = 1
2
(x2 + y2) + V3(x, y) + V4(x, y)
in display (11.37) is given by
V := V (r cos θ, r sin θ) = 1
2
r2 + r3V3(cos θ,sin θ) + r4V4(cos θ,sin θ).
Define ρ = √
2V and let r := r(θ, ξ) denote the (positive) solution of
the initial value problem (11.30). Also, define vj (θ)=2Vj (cos θ,sin θ) for
j = 3, 4 to obtain the equality
ρ = (r2(1 + v3(θ)r + v4(θ)r2))1/2
= r(1 + v3(θ)
2
r + φ(θ)r2 + O(r3)) (11.38)
where φ(θ) = v4(θ)/2 − (v3(θ))2/8. Moreover, if r ≥ 0 is sufficiently small,
then ρ is represented by the power series indicated in display (11.38).
The displacement with the parameter vector suppressed is given by
δ(ξ) := ρ(2π, ξ) − ρ(0, ξ).662 11. Bifurcation
Use the initial condition r(0, ξ) = ξ together with equation (11.38), to
express the displacement in the form
δ(ξ) = r(2π, ξ)(1 + v3(2π)
2
r(2π, ξ) + φ(2π)r2(2π, ξ))
− ξ(1 + v3(0)
2 ξ + φ(0)ξ2) + O(ξ4) + O(r4(2π, ξ)).
Because vj (0) = vj (2π),
δ(ξ) = r(2π, ξ)(1 + v3(0)
2
r(2π, ξ) + φ(0)r2(2π, ξ))
− ξ(1 + v3(0)
2 ξ + φ(0)ξ2) + O(ξ4) + O(r4(2π, ξ)).
Using formula (11.31), namely,
r(2π, ξ) = ξ + r3(2π)ξ3 + O(ξ4)
and an algebraic computation,
δ(ξ) = r3(2π)ξ3 + O(ξ4). (11.39)
Previously obtained formulas and straightforward calculus imply
δ(ξ) = ρ(2π, ξ) − ρ(0, ξ)
=
 2π
0
dρ
dθ (θ, ξ) dθ =
 2π
0
1
ρ
dV
dθ dθ =
 2π
0
1
ρ
dV
dt
dt
dθ dθ
=
 2π
0
1
r(1 + 1
2 v3(θ)r + O(r2))(L4r4 + O(r6)) 1
1 + rB(r, θ)
dθ
=
 2π
0
L4r3 + O(r4) dθ
=
 2π
0
L4(ξ + r3(θ)ξ3 + O(ξ4))3 + O(ξ4) dθ
= 2πL4ξ3 + O(ξ4).
By equating the derived expression for δ and the equation in display (11.39),
we have proved that r3(2π)=2πL4, as required. ✷
Exercise 11.46. Our definition of the Lyapunov quantities depends on the
basis for the complement of the range of LR : Hn → Hn for each even integer
n. If a different basis is used, “Lyapunov quantities” can be defined in a simi￾lar manner. Describe how these quantities are related to the original Lyapunov
quantities.
Exercise 11.47. Describe all Hopf bifurcations for the following equations:11.3 Poincar´e-Andronov-Hopf Bifurcation 663
1. ˙x = y, ˙y = −x + 	y − ax2y where a ∈ R.
2. ˙x = 	x − y + p(x, y), ˙y = x + 	y + q(x, y) where p and q are homogeneous
quadratic polynomials.
3. ¨x + 	(x2 − 1) ˙x + x = 0 where 	 ∈ R.
4. ˙x = (x − βy)x + 	y, ˙y = (x2 − y)y where β, 	 ∈ R.
Propositions 11.40 and 11.41 can be proved in a variety of ways. The
combined proof given here uses some elementary ideas from Lie’s theory
of symmetry groups for differential equations (see [199]). The reader is
encouraged to construct a purely algebraic proof.
Proof. The operator LR : Hn → Hn, as in display (11.35), defines Lie dif￾ferentiation in the direction of the vector field given by R(x, y)=(x, y, −y, x).
Geometrically, R represents the infinitesimal (positive) rotation of the plane
centered at the origin. Its flow is the linear (positive) rotation given by
ϕt(x, y) = etA x
y
	
=
cost − sin t
sin t cost
	 x
y
	
(11.40)
where
A := 0 −1
1 0 	
.
For a smooth function f : R2 → R and z := (x, y) a point in its domain,
LRf(z) = d
dt(f(ϕt(z)))



t=0
.
A fundamental proposition in Lie’s theory is the following statement: If
h is an infinitesimally invariant function with respect to a vector field X
(that is, LXh = 0), then h is constant along integral curves of X. This
simple result depends on the group property of the flow of X.
To prove Lie’s proposition in the special case considered here, let h(t, z) :=
f(ϕt(z)) and note that
d
dth(t, z)



t=s
= limτ→0
1
τ
[h(s + τ,z) − h(s, z)]
= limτ→0
1
τ
[f(ϕτ (ϕs(z))) − f(ϕs(z))]
= LRf(ϕs(z)). (11.41)
If s → LRf(ϕs(z)) vanishes identically (that is, f is infinitesimally invari￾ant), then the function t → h(t, z) is a constant; therefore f(ϕt(z)) = f(z)
for each t ∈ R, as required.
For the special case where LR : Hn → Hn and H ∈ Hn there is a useful
corollary: The homogeneous polynomial H is in the kernel of LR if and
only if H is rotationally invariant.664 11. Bifurcation
If n is odd and H ∈ Hn is in the kernel of LR, then
H(x, y) = H(ϕπ(x, y)) = H(−x, −y)=(−1)nH(x, y) = −H(x, y)
and therefore H = 0. Because Hn is finite dimensional and LR is a linear
operator with a trivial kernel, it is invertible.
If n is even and H ∈ Hn is rotationally invariant, then
H(cos θ,sin θ) = H(1, 0), 0 ≤ θ < 2π.
Moreover, since H ∈ Hn is homogeneous,
H(r cos θ, r sin θ) = rnH(1, 0).
In other words, H(x, y) = H(1, 0)(x2 + y2)n/2. Thus, the kernel of LR
is one-dimensional and it is generated by the homogeneous polynomial
(x2 + y2)n/2.
The next step is to prove that the polynomial (x2 + y2)n/2 generates a
complement to the range of LR. Because the kernel of LR is one-dimensional,
its range has codimension one. Thus, it suffices to show that the nonzero
vector (x2 + y2)n/2 is not in the range.
If there is some H ∈ Hn such that LRH(x, y)=(x2+y2)n/2, then choose
z = (x, y) = 0 and note that LRH(ϕt(z)) = ||z||n = 0. By formula (11.41),
the function t → H(ϕt(z)) is the solution of the initial value problem
u˙ = ||z||n, u(0) = H(z),
which is given by H(ϕt(z)) = ||z||nt + H(z). Because t → H(ϕt(z)) is
2π-periodic, ||z|| = 0, in contradiction. ✷
All ingredients are in place to detect (multiple) Hopf bifurcation from a
weak focus of finite order in a family of differential equations of the form
x˙ = λ1x − y + p(x, y, λ), y˙ = x + λ1y + q(x, y, λ) (11.42)
where λ = (λ1,...,λN ) is a vector-valued parameter and the corresponding
parameterized vector field is sufficiently smooth. For simplicity, assume that
the coefficients of the Taylor series at the origin for the functions (x, y) →
p(x, y, λ) and (x, y) → q(x, y, λ) are polynomials in the components of λ.
Recall that Lyapunov quantities are computed at the parameter values
where λ1 = 0. As a convenient notation, let Λ = (0, λ2,...,λN ) be the
vector variable for points in this hypersurface of the parameter space so
that the Lyapunov quantities are functions of the variables λ2,...,λN .
There is a basic result: Suppose k is a positive integer and the vector
field corresponding to the family (11.42) is class C2k. If for some fixed Λ∗
in the hypersurface L2j (Λ∗) = 0 for j = 2,...,k −1 and L2k(Λ∗) = 0, then
by Proposition 11.34 at most k − 1 limit cycles appear near the origin of11.3 Poincar´e-Andronov-Hopf Bifurcation 665
the phase plane for the members of the family corresponding to parameter
values λ with |λ − Λ∗| sufficiently small.
The theory discussed so far does not apply, even for real analytic vector
field families with a rest point at the origin, when L2k(Λ∗) = 0 for each
integer k ≥ 2 because the bifurcation point does not have finite multiplicity.
In this case, the rest point at the origin is a center. There are several
beautiful ideas that appear in the analysis of bifurcations for this case (see,
for example, [25], [49], [221], and [268]). Perhaps a surprise, abstract algebra
plays a starring roles.
A general theory is being developed for real analytic families of planar
vector fields for which a starting point is encoded in
Proposition 11.48. The Lyapunov quantities, for an analytic family of
planar vector fields with a rest point at the origin whose linearization has
a system matrix with pure imaginary eigenvalues, are polynomials in the
Taylor coefficients of the vector field at the origin.
Before presenting a proof, some of the development of the general theory
is informally discussed.
Proposition 11.48 suggests that Lyapunov quantities can be computed
in a finite number of steps using polynomial algebra. In fact, this is a key
observation.
The displacement function for the family (11.42) may be represented
near the origin as a power series of the form
δ(ξ, λ) = δ1(λ)ξ +∞
j=2
δj (λ)ξj
where δ1(λ) = e2πλ1 −1. For the case of interest, there is a point in param￾eter space where λ1 = 0 and the rest point at the origin is an infinite-order
weak focus at λ = Λ∗. The main goal of the theory is to determine how
many zeros of the displacement function survive for nearby parameter val￾ues. These zeros of course correspond to periodic orbits.
Suppose that λ1 = 0 and every Taylor coefficient for the displacement
function, expanded at ξ = 0 and valid in a neighborhood of the parameter
value Λ∗, is a polynomial in the variables λ2 − λ∗
2, λ3 − λ∗
3,...,λN − λ∗
N .
In effect, this occurs for the differential equations
x˙ = −y + p(x, y, λ), y˙ = x + q(x, y, λ), (11.43)
which are analyzed first. At the final step, to be explained, λ1 is reintro￾duced.
The set of all polynomials with real coefficients, this finite set of variables,
and the usual notions of addition and multiplication, has the algebraic
structure of a commutative ring. A less obvious but essential fact is that
it is a Noetherian ring; that is, every chain of ideals I1 ⊆ I2 ⊆ I3 ⊆··· in666 11. Bifurcation
this ring terminates. This is a consequence of Hilbert’s basis theorem (see,
for example, [32]); it states that every ideal such in a polynomial ring is
finitely generated. By applying these results from abstract algebra to the
displacement function, the chain of ideals
(δ2) ⊆ (δ2, δ3) ⊆ (δ2, δ3, δ4) ⊆···
must terminate. More precisely, there is an ideal
(δ2, δ3, δ4,...,δK)
in the chain that contains all subsequent polynomials that appear in the
chain. In other words, there is an ideal generated by a finite initial segment
of Taylor coefficients of the displacement function with λ1 = 0 that contains
all of the subsequent Taylor coefficients. Hence, for each positive integer
J, there is a set of polynomials {μJk(λ) : k = 2,...,K} such that
δJ (λ) = 
K
k=2
μJk(λ)δk(λ). (11.44)
Using representation (11.44) and a formal calculation, the following series
expansion can be obtained for the displacement function:
δ(ξ, λ) = δ1(λ)ξ
+ δ2(λ)ξ2(1 + ∞
j=K+1
μj2(λ)ξj−2)
+ ··· + δK(λ)ξK(1 + ∞
j=K+1
μjK(λ)ξj−K).
While it is certainly not obvious that this formal rearrangement of the
Taylor series of the displacement function is convergent, this result has been
proved (see, for example, [49] and the references therein). By inspection of
this series, it is reasonable to expect and not too difficult to prove that if
|ξ| and |λ − λ∗| are sufficiently small, then the appearance of limit cycles
is determined by an analysis of the zero set of the function
B(ξ, λ) := δ1(λ)ξ + δ2(λ)ξ2 + ··· + δK(λ)ξK.
In particular, because B is a polynomial of degree K in the variable ξ, the
displacement function δ cannot have more than K “local” zeros.
By the symmetry with respect to ξ = 0, only the odd-order Taylor coeffi￾cients of the displacement function are important. Proposition 11.45 implies
that, for order greater or equal to three and up to constant multiples, these
coefficients are the corresponding Lyapunov quantities.11.3 Poincar´e-Andronov-Hopf Bifurcation 667
In case k is a positive integer and the initial segment of odd-order Taylor
coefficients (δ3, δ5, δ7,...,δ2k+1) generates the ideal of all Taylor coeffi￾cients, the bifurcation point at ξ = 0 and Λ∗ is said to have order k. Of
course, the reason for this definition is that at most k local limit cycles can
appear after perturbation from a bifurcation point of order k. Indeed, the
origin ξ = 0 accounts for one zero of the displacement function and each
limit cycle accounts for two zeros because such a limit cycle must cross
both the positive and the negative ξ-axis. Since the displacement function
has at most 2k + 1 zeros, there are at most k local limit cycles.
As mentioned previously, additional conditions must be satisfied to deter￾mine the exact number of limit cycles. For example, suppose that the func￾tion
B1(λ, ξ) := δ1(λ) + δ2(λ)ξ + ··· + δ2k+1(λ)ξ2k
is such that δ2k+1(Λ1) > 0 and δj (Λ1) = 0 for j = 1,..., 2k, where Λ1
is a particular value of Λ. This situation arises when L2j (Λ1) vanishes for
j = 1,...k and the value of the polynomial L2k+2 at Λ1 is positive. If there
is a vector parameter value Λ2 such that
δ2k+1(Λ2) > 0, δ2k−1(Λ2) < 0, δj (Λ2)=0
for j = 1,..., 2k − 1, and |Λ2 − Λ1| is sufficiently small, then the function
ξ → B1(ξ,Λ2) will have two zeros near ξ = 0, one positive zero and one
zero at the origin. By continuity, if |Λ3 − Λ2| is sufficiently small, then the
corresponding function at the parameter value Λ3 also has continuations
of these zeros. Moreover, if there is a choice of Λ3 in the required open
subset of the parameter space such that δ2k−1(Λ3) > 0, then B1(ξ,Λ3) has
three zeros, and so on. Well, almost. At the last step, δ1(λ) = e2πλ1 − 1 is
adjusted using nonzero values of λ1. This is, of course, the reintroduction
of λ1 mentioned previously.
To implement the theory, two main steps are required: (1) a finite set
of Taylor coefficients, say {δj : j = 2, 3, 4,..., 2k + 1}, must be computed,
and (2) the ideal generated by them (equivalently, the ideal generated by
the Lyapunov quantities up to the corresponding order) must be proved
to contain all subsequent Taylor coefficients (equivalently, Lyapunov quan￾tities). Step (1) is solved conceptually by the algorithm used to compute
Lyapunov quantities. But practically, polynomials involved can become so
large that their manipulation is a challenge. Step (2) seems to be a difficult
problem in general; it has been solved only for a few special cases.
The first and most famous result using steps (1) and (2) was proved by
Nikolai N. Bautin [25] for quadratic systems—that is, for
x˙ = x − y + p(x, y), y˙ = x + y + q(x, y)
where p and q are homogeneous quadratic polynomials and the parameters
are  and the coefficients of p and q. He showed that the ideal of all Taylor668 11. Bifurcation
coefficients is generated by (δ3, δ5, δ7). Thus, at most three limit cycles can
bifurcate from the origin. Moreover, it is possible to construct examples
where three limit cycles appear (see [25] and [268]). Research in this direc￾tion continues to improve and refine the basic ideas presented here (see, for
example, [110] and the references therein).
Counting the exact number of limit cycles of a polynomial system remains
a grand challenge. Indeed, this is the content of Hilbert’s 16th problem: Is
there a bound for the number of limit cycles of a polynomial system that
depends only on the degrees of the polynomials that define the system? This
problem is not solved, even for the case of quadratic systems. The best
result obtained so far is the following deep theorem of Yulij Il’yashenko [143]
and Jean Ecalle [89].
Theorem 11.49. A polynomial system has at most a finite number of limit
cycles.
(See the book of Il’yashenko [143] and the review [42] for a mathematical
history of the work on Hilbert’s problem, and see [211] for a complete
bibliography of quadratic systems theory.)
The remainder of this section is devoted to the promised proof of Propo￾sition 11.48, which states that Lyapunov quantities for an analytic system
are polynomials in the Taylor coefficients of the vector field corresponding
to the system, and to a description of an algorithm that can be used to
compute Lyapunov quantities.
Proof. Consider the vector field
X(x, y) := (−y
∂
∂x + x
∂
∂y ) + ∞
j=2
pj (x, y) ∂
∂x +∞
j=2
qj (x, y) ∂
∂y

where pj , qj ∈ Hj for each integer j ≥ 1. Also, let
V (x, y) := 1
2
(x2 + y2) +∞
j=3
Vj (x, y)11.3 Poincar´e-Andronov-Hopf Bifurcation 669
where Vj ∈ Hj for each j ≥ 1. The Lie derivative of V in the direction X
is given by
LXV = ∞
j=3
LRVj + x
∞
j=2
pj + y
∞
j=2
qj +∞
j=2
pj
∞
j=3
Vjx +∞
j=2
qj
∞
j=3
Vjy
= ∞
j=3
LRVj +∞
j=2
(xpj + yqj ) +∞
j=2

j−2
i=0
pj−iV(i+3)x + qj−iV(i+3)y

= LRV3 + xp2 + yq2 +∞
j=4

LRVj + xpj−1 + yqj−1
+
j−4
i=0
pj−i−2V(i+3)x + qj−i−2V(i+3)y

.
For each even integer j ≥ 2, let Πj : Hj → Hj denote the linear pro￾jection whose kernel is the range of the operator LR and whose range is a
one-dimensional complement to the range of the operator LR; that is, the
subspace of H2j generated by the vector (x2 + y2)j . Also, for each integer
j ≥ 4, define Hj ∈ Hj by
Hj := xpj−1 + yqj−1 +
j−4
i=0

pj−i−2V(i+3)x + qj−i−2V(i+3)y

so that
LXV = LRV3 + xp2 + yq2 +∞
j=4

LRVj + Hj

.
The following algorithm produces the Lyapunov quantities:
Input (k, p2,...,p2k−1, q2,...,q2k−1)
V3 := −L−1
R (xp2 + yq2)
For j from 4 to 2k do
If j is odd, then Vj := −L−1
R (xpj−1 + yqj−1 + Hj )
If j is even, then
Lj := Πj (Hj )/(x2 + y2)j/2
Vj := −L−1
R

Hj − Lj (x2 + y2)j/2
End for loop;
Output (L4, L6,...,L2k).
Remark 4. To implement the algorithm, good practice is to choose a basis
for each vector space Hj and represent the linear transformations Πj and
LR in this basis (see Exercise 11.51).
Remark 5. The value of L4 in case
pj (x, y) = 
j
i=0
aj−i,ixj−i
yi
, qj (x, y) = 
j
i=0
bj−i,ixj−i
yi670 11. Bifurcation
is given by
L4 = 1
8
(a20a11 + b21 + 3a30 − b02b11
+ 3b03 + 2b02a02 − 2a20b20 − b20b11 + a12 + a02a11). (11.45)
The sign of this quantity is the same as the sign of the third Taylor coef￾ficient of the displacement function. Thus, the sign of L4 can be used to
determine the stability of a weak focus as required in the Hopf bifurcation
theorem. Indeed, the formula for L4 is perhaps the most useful result in
practice for proving the existence of a limit cycle via Hopf bifurcation.
If k is a positive integer, then the Lyapunov quantity L2k is a polynomial
in the Taylor coefficients of p and q at the origin. To prove this fact, note
that
LRV2k + H2k − L2k(x2 + y2)
k = 0.
Moreover, the linear flow of the vector field R is given by
ϕt(x, y) = etA x
y
	
where
A =
0 −1
1 0 	
,
and, by Exercise 11.50, the projection Π2k is represented by
Π2kH(x, y) = 1
2π
 2π
0
H(ϕt(x, y)) dt. (11.46)
Since the rotationally invariant elements of H2k are in the complement
of the range of LR, the composition Π2kLR is equal to the zero operator,
and therefore
L2k(x2 + y2)
k = Π2kH2k(x, y).
In particular, the desired Lyapunov quantity is given by the integral
L2k = 1
2π
 2π
0
Hk(cost,sin t) dt.
Hence, by inspection of the algorithm for computing Lyapunov quantities,
L2k is a polynomial in the coefficients of the polynomials
p2,...,p2k−1, q2,...,q2k−1.
✷
Exercise 11.50. Demonstrate that the representation (11.46) is valid by show￾ing: a) Π2k is linear, b) Π2kH is rotationally invariant, and c) Π2k(x2 + y2)
k =
(x2 + y2)
k.11.3 Poincar´e-Andronov-Hopf Bifurcation 671
Exercise 11.51. Prove the following statements. The set B := {xn−i
yi | i =
0,...,n} is a basis for Hn and LR has the following (n + 1) × (n + 1) matrix
representation with respect to the given (ordered) basis:
LR =
⎛
⎜⎜⎜⎜⎝
0 1 0 00 ···
−n 0 2 00 ···
0 1 − n 0 30 ···
0 02 − n 0 4 ···
· · · · · ···
⎞
⎟⎟⎟⎟⎠ .
The kernel of LR on H2k, for k ≥ 2, is generated by the vector
K = (Bk,0, 0, Bk,1, 0, Bk,2,..., 0, Bk,k)
where the numbers
Bk,j = k!
j!(k − j)!
are the binomial coefficients, and
{(a1,...,a2k, 0) : (a1,...,a2k) ∈ R2k}
is a vector space complement of the kernel. The operator LR on H2k is represented
by the matrix (
1,...,
2k+1) partitioned by the indicated columns. The matrix
representation for LR, restricted to this complement of the kernel, is given by
(
1,...,
2k, 0). Consider V,H ∈ H2k and the associated matrix equation
(
1,...,
n, K)V = H,
where the matrix is partitioned by columns and H is represented in the basis B.
The matrix is invertible. If the solution V is given by the vector (a1,...,a2k, L),
then H is given by H = 
2k
j=1 aj 
j +LK where L is the corresponding Lyapunov
quantity. The projection Π2k is given by Π2k(H) := LK.
Exercise 11.52. Determine the stability of the rest point at the origin for the
system
x˙ = −y − x2 + xy, y˙ = x + 2xy.
Exercise 11.53. Discuss the Hopf bifurcation for the following systems:
1. ˙x = 	x − y − x2 + xy, y˙ = x + 	y + 2xy.
2. ˙x = 	x − y − x2 + 	xy, y˙ = x + 	y + 2y2.
3. ˙x = x(x − βy) + 	y, y˙ = y(x2 − y).
4. ˙x = λx − y − y(x2 + y2) − 2x(x2 + y2)
2,
y˙ = x + λy + x(x2 + y2) − 2y(x2 + y2)
2
Exercise 11.54. (a) Show that the system
u˙ = v, v˙ = −u − 1
√5
(2u2 + 3uv − 2v2
)
has a center at the origin. Hint: Use Exercise 1.140 and separate variables to find
a first integral. (b) Show by computation that the Lyapunov quantities L2, L4,
and L6 all vanish.672 11. Bifurcation
Exercise 11.55. Consider the quadratic system in Bautin normal form:
x˙ = λ1x − y − λ3x2 + (2λ2 + λ5)xy + λ6y2
,
y˙ = x + λ1y + λ2x2 + (2λ3 + λ4)xy − λ2y2
.
Find the corresponding Lyapunov quantities L2, L4, and L6. Construct a curve in
the parameter space with a supercritical Hopf bifurcation. Construct a quadratic
system with two limit cycles surrounding the origin and a quadratic system
with three limit cycles surrounding the origin. (If you need help, see [50] or
[204, p. 449].)
Exercise 11.56. The family
¨θ + sin θ − Ω cos θ sin θ = I ˙
θ,
where Ω and I are real parameters, is a simple model in dimensionless form of
a whirling pendulum with a feedback control. Discuss the existence of a Hopf
bifurcation for the rest point at the origin in the phase plane at the control
coefficient value I = 0. How does the existence of a Hopf bifurcation depend on
the rotation speed Ω? Draw the bifurcation diagram.
Exercise 11.57. Consider the following model for the dimensionless concen￾trations x and y of certain reacting chemicals
x˙ = a − x − 4xy
1 + x2 , y˙ = bx
1 − y
1 + x2

,
and the curve C in the first quadrant of the (a, b)-parameter space given by
b = 3a/5−25/a. Prove that a supercritical Hopf bifurcation occurs when a curve
in the parameter space crosses C from above. This exercise is taken from
[244, p. 256] where the derivation of the model and typical phase portraits are
described.
Exercise 11.58. Consider the Brusselator model
x˙ = a − (b + 1)x + x2
y, y˙ = bx − x2
y
for concentrations of chemical species x and y and parameters a and b. Prove
that, for some choices of the parameters, a stable limit cycle exists.
Exercise 11.59. Discuss Hopf bifurcation for the two-parameter family ˙x =
(ax − y)(1 − x2), y˙ = x + by(1 − x2).
Exercise 11.60. [Warmup for Ex. (11.61)] Suppose that a planar system ˙x =
f(x) has a rest point and the linearization at this point has system matrix A
with a pair of complex conjugate eigenvalues a ± bi. Construct a matrix B such
that B−1AB has the form  a −b
b a 
.
Exercise 11.61. [Normal Forms] The computation of Lyapunov quantities is
a special case of a procedure for simplifying vector fields near a rest point. To
describe the procedure, called reduction to normal form, suppose that ˙x = f(x)
is a smooth system on Rn with a rest point at the origin. Let A := Df(0) and
expand f in its Taylor series to order k at the origin to obtain the representation
f(x) = Ax +k
j=2
fj (x) + O(|x|
k+1)11.3 Poincar´e-Andronov-Hopf Bifurcation 673
where fj is the jth-order term of the expansion, whose components are homoge￾neous polynomials of degree j. To simplify the differential equation, first simplify
the linear terms to Jordan normal form via a linear transformation x = By.
(a) Show that the transformed system is given by
y˙ = B−1
ABy +k
j=2
B−1
fj (By) + O(|y|
k+1),
which is equivalent to the differential equation
x˙ = Jx +k
j=2
f 1
j (x) + O(|x|
k+1)
where J := B−1AB and f 1
j := B−1 ◦ fj ◦ B.
(b) To simplify the quadratic terms, consider a transformation of the form x =
y +h(y) where h has quadratic homogeneous polynomial components. Show that
this transformation is invertible in a neighborhood of the origin.
(c) Show that the change of variables in part (b) transforms the differential
equation to the form
y˙ = (I + Dh(y))−1
(J(y + h(y)) +k
j=2
f 1
j (y + h(y)) + O(|y|
k+1),
which gives (to second order)
y˙ = Jy + (Jh(y) − Dh(y)Jy) + f 1
2 (y) + O(|y|
3
).
(d) Note that in part (c) the previously simplified first-order term is not changed
by the coordinate transformation. Also, define Hj to be the transformations of
Rn whose components are homogeneous polynomials of degree j. Consider the
operator LJ (the Lie derivative in the direction J) where J is viewed as a vector
field. Prove that LJ : H2 → H2, LJ is linear, and (LJ h)(y) = Dh(y)Jy − Jh(y).
(e) Define R to be the range of LJ and choose a complement K of the linear
subspace R in H2. The element f 1
2 ∈ H2 can be represented in the form f 1
2 =
R1
2 + K1
2 where R1
2 ∈ R and K1
2 ∈ K. Solve the equation LJ h = R1
2 for h and
show that this choice leads to the transformed differential equation
y˙ = Jy + f 2
2 (y) +k
j=3
f 2
j (y) + O(|y|
k+1)
where f 2
2 ∈ K. At this point the original differential equation is in normal form
to order two.
(f) Show that the transformation to normal form can be continued to order k.
Hint: At each order j there is a transformation of the from x = y + h(y) where
h ∈ Hj such that after this transformation the terms of order less than j remain
unchanged and the new differential equation is in normal form to order j; that
is, the jth order term is in a space complementary to the range of LJ in Hj .
(g) Suppose that n = 2 and
A =
0 −1
1 0 
.674 11. Bifurcation
determine a normal form for an arbitrary planar system of the form ˙x = Ax+F(x)
where F(0) = DF(0) = 0 and F is smooth.
(h) Show that with appropriate choices of complementary subspaces the normal
form for part (g) is given by
r˙ = k
j=1
a2j+1r2j+1, ˙
θ =1+k
j=1
b2j r2j
.
(i) Repeat part (h) for the matrix
A =
α −β
β α 
to obtain the normal form for the Hopf bifurcation. With appropriate choices, its
polar coordinate representation is
r˙ = αr +k
j=1
a2j+1r2j+1, ˙
θ = β +k
j=1
b2j r2j
(cf. Exercise 11.28). Note: There is an extensive literature on normal forms (see,
for example, [66], [119], [190], [227], and [261]).
11.4 Dynamic Bifurcation
Dynamic bifurcation theory is introduced in this section, and an important
delay phenomenon associated with slow passage of a changing parameter
through a bifurcation point is discussed.
Qualitative changes in phase portraits for members of a family of differ￾ential equations such as
u˙ = f(u, λ), u ∈ Rn, λ ∈ R
for different values of the parameter λ is called static bifurcation theory.
A parameter is changed, but it does not change with time. While static
bifurcation theory might seem to give correct results when a parameter
is varied slowly with time—maybe the parameter is changed by moving
a control dial, new phenomena may occur that are not explained by this
theory. Some of them, which occur in systems of the form
λ˙ = g(u, λ, ), u˙ = f(u, λ, ),
where the dynamic parameter λ is viewed as a dependent variable and  is
a (small) parameter, are discussed as an introduction to the subject called
dynamic bifurcation theory.
As a preliminary step, consider a differential equation ˙u = X(u) on R2
with flow ϕt and two curves Σ and Σ in R2 that are both transverse to the11.4 Dynamic Bifurcation 675
vector field X (that is, X is never tangent to either curve). The differential
equation is said to induce a section map P : Σ → Σ if there is a smooth
function T : Σ → R such that P defined on Σ by P(σ) = ϕT(σ)(σ) has
values in Σ
.
Proposition 11.62. Suppose that
x˙ = f(x, y), y˙ = yg(x, y)
is a C1 differential equation on R2; a, b ∈ R with a<b; Σb := {(x, y) ∈
R2 : x = b}; and, for each δ > 0, Σδ
a := {(x, y) ∈ R2 : x = aand|y| < δ}.
If f(x, 0) > 0 for all x in the closed interval [a, b] and δ > 0 is sufficiently
small, then the differential equation induces a section map P from Σδ
a to
Σb such that, in the usual local coordinate on Σδ
a given by (a, η) → η,
P
(0) = e
 b
a
g(x,0)
f(x,0) dx.
Proof. Let t → (x(t, η), y(t, η)) denote the family of solutions of the dif￾ferential equation with the initial conditions
x(0, η) ≡ a, y(0, η) ≡ η.
Since the x-axis is invariant, y(t, 0) ≡ 0. Also, since f(x, 0) > 0 on the
interval [a, b], there is a number T0 > 0 such that x(T0, 0) = b and ˙x(T0, 0) >
0. By the implicit function theorem, there is some number δ > 0 and a
smooth function T : [−δ, δ] → R such that T(0) = T0 and x(T(η), η) ≡ b
for all η ∈ [−δ, δ]. Hence, the desired section map (in local coordinates)
exists and is given by
P(η) = y(T(η), η).
Moreover, by using the identity ˙y(t, 0) ≡ 0, the derivative of the section
map at η = 0 is given by
P
(0) = ˙y(T(0), 0)T
(0) + yη(T(0), 0) = yη(T(0), 0).
Note that
xη(0, η) ≡ 0, yη(0, η) ≡ 1
and t → (xη(t, η), yη(t, η)) is the solution of the variational initial value
problem
u˙ = fx(x(t, η), y(t, η))u + fy(x(t, η), y(t, η))v,
v˙ = y(t, η)gx(x(t, η), y(t, η))u
+ [y(t, η)gy(x(t, η), y(t, η)) + g(x(t, η), y(t, η))]v,
u(0) = 0,
v(0) = 1.676 11. Bifurcation
After evaluation at η = 0, the second equation decouples from the system,
and it follows immediately that
P
(0) = v(T(0)) = e
 T (0)
0 g(x(s,0),0) ds
where ˙x(t, 0) = f(x(t, 0), 0), x(0, 0) = 0, and x(T(0), 0) = b. By introducing
the new variable ξ = x(s, 0), we have dξ = f(ξ, 0) ds and
P
(0) = e
 b
a
g(ξ,0)
f(ξ,0) dξ,
as required. ✷
For real numbers  and a<b, consider a family of differential equations
on R2 of the form
x˙ = F(x, y, ), y˙ = yG(x, y, ) (11.47)
where  is a parameter and F(x, 0, 0) > 0 whenever x ∈ [a, b]. If || is
sufficiently small, then F(x, 0, ) > 0 whenever x ∈ [a, b]. Hence, using the
notation of Proposition 11.62, if  > 0 is sufficiently small, then
P
(0, a, b, ) = e
 b
a
G(ξ,0,)
F (ξ,0,) dξ.
Consider the orbit of system (11.47) starting at the point (a, η), for |η|
small, and note that if P
(0, a, b, ) < 1, then this orbit meets Σb at a point
(b, σ) where |σ| < |η|; that is, the orbit is attracted toward the x-axis.
On the other hand, if P
(0, a, b, ) > 1, then the orbit is repelled from the
x-axis. The critical value P
(0, a, b, ) = 1 occurs if and only if
 b
a
G(ξ, 0, )
F(ξ, 0, )
dξ = 0. (11.48)
For system (11.47) with  = 0, the x-axis is an invariant set consisting
entirely of rest points. Moreover, the system matrix of the linearized system
at the rest point p with coordinates (x, 0) has system matrix
0 0
0 G(x, 0, 0)	
.
In case G(x, 0, 0) < 0, the one-dimensional stable manifold of p is normal
to the x-axis, and solutions starting on this manifold are attracted to p
exponentially fast. On the other hand, when G(x, 0, 0) > 0, the rest point
p has a one-dimensional unstable manifold , and solutions are repelled. The
rest point is called a turning point if G(x, 0, 0) = 0.
Assume that a<c<b and (c, 0) is a turning point such that G(x, 0, 0) <
0 for x<c and G(x, 0, 0) > 0 for x>c. The proved results about the
derivative of the section map imply that if  > 0 is sufficiently small,11.4 Dynamic Bifurcation 677
then a solution starting near the x-axis at a point (x, y) with x<c will
be (rapidly) attracted to the vicinity of the x-axis, drift slowly along the
x-axis at least until its first component is larger than c, and eventually
be (rapidly) repelled from the vicinity of the x-axis. The next (somewhat
imprecise) theorem—no definition of the concept “leaves the vicinity” has
been given—identifies the point on the x-axis where the solution leaves.
Theorem 11.63. Suppose for system (11.47) that (1) a<c; (2) (c, 0) is a
turning point; (3) G(x, 0, 0) < 0 for x<c and G(x, 0, 0) > 0 for x>c, and
(4) η = 0 is such that the family of solutions t → (x(t, a, ), y(t, a, )) with
x(0, a, ) = a and y(0, a, ) = η is defined for sufficiently small ||. Then,
as  approaches zero from above, the point on the x-axis where the solution
leaves the vicinity of the x-axis approaches (b, 0), where b is the smallest
number larger than c that satisfies equation (11.48).
Proof. The solution will be repelled from the x-axis near (b, 0) whenever
the derivative of the section map P
(0, a, b, ) is positive. The sign of this
derivative is determined by the integral equation (11.48). If b satisfies the
equation, then the orbit is repelled from the vicinity of the x-axis after t
increases past the time T when x(T) = b. But, the derivative at such a
point can be made arbitrarily large for sufficiently small  > 0. ✷
To apply Theorem 11.63, recall the static supercritical pitchfork bifurca￾tion whose normal form is given by the family
y˙ = y(μ − y2)
parameterized by μ. Every member of this family has y = 0 as a rest point.
If μ < 0, then this rest point is stable; if μ > 0, it is unstable. Moreover, a
pair of stable rest points y = ±√μ exist for μ > 0.
Imagine that the parameter μ is slowly changed from a negative value to a
positive value. The steady state of the system would be expected to change
from the stable steady state y = 0, for negative values of the parameter μ,
to one of the new stable steady states y = ±√μ as soon as μ is positive.
To test the validity of this scenario, suppose that  > 0 is small and the
state of the system is governed by the differential equation
μ˙ = , y˙ = y(μ − y2) (11.49)
so that the “parameter” μ changes slowly with time. By an application of
formula (11.48), the solution starting at the point (−μ0, y0) is attracted
to the y-axis. It drifts along this invariant set—that is, it stays near the
“steady state” y = 0—until μ ≈ μ0. Then, it leaves the vicinity of this
invariant set and approaches one of the new stable “steady states.” Of
course, system (11.49) has no rest points. But, the μ-axis is an invariant
set for the dynamic bifurcation system; it corresponds to the rest points at
y = 0 for the “static system” ( = 0). Also, there are orbits of the dynamic
bifurcation system that lie near the parabola given by μ = y2.678 11. Bifurcation
-1
-0.5
0
0.5
1
1.5
2
2.5
-1.5 -1 -0.5 0 0.5 1 1.5 2
Figure 11.4: The solution in phase space (y versus μ) for system (11.49)
with  = 0.02 and initial conditions μ = −1 and y = 2.
In contrast to the static bifurcation scenario, the dynamic bifurcation is
delayed; a solution t → (μ(t), y(t)), which starts near the μ-axis at time
t = 0 and whose first component eventually increases through the critical
value μ = 0, does not immediately approach a stable steady state (that
is, one of the invariant sets near the graphs of the functions μ → ±√μ).
Rather, the solution remains near the “unstable” μ-axis until μ(t) exceeds
−μ(0), a value that can be much larger than the static bifurcation value
μ = 0 (see Figure 11.4).
The existence of a subcritical pitchfork bifurcation in a physical system
often signals the possibility of a dramatic change in its dynamics. To see
why, consider the corresponding normal form dynamic bifurcation system
μ˙ = , y˙ = y(μ + y2). (11.50)
A solution of system (11.50) starting near the negative μ-axis at (−μ0, y0) is
attracted toward the invariant μ-axis, and it remains nearby until μ ≈ μ0.
But, in contrast to the supercritical pitchfork bifurcation, when μ>μ0
the magnitude of this solution grows rapidly without being bounded by
the ghosts of the corresponding static bifurcation’s stable steady states.
Thus, such a solution would be expected to move far away from equilibrium
toward some distant attractor.11.4 Dynamic Bifurcation 679
20 40 60 80 100 120
-1.5
-1
-0.5
0
0.5
1
1.5
2
Figure 11.5: A plot of x versus t for system (11.51) with parameter values
a = 0.7, η = 0.1, and ω = 3 and initial conditions (x, y, u) = (1, 0, −0.5).
Two simple examples of slow passage through a bifurcation, where the
bifurcation parameter is a linear function of time, have just been discussed.
The main feature of these examples is the existence of two time scales: fast
attraction and slow passage. Indeed, recognition of the existence of different
time scales in a dynamical problem is often the key to understanding seem￾ingly exotic phenomena. A final example of this type models slow passage
through a subcritical pitchfork bifurcation coupled with an appropriate
nonlinear change in the bifurcation parameter, which can be viewed as an
explanation for “bursting,” a dynamical behavior observed, for example, in
the electrical behavior of neurons (see [148]).
The model system is
z˙ = (u + iω)z + 2z|z|
2 − z|z|
4,
u˙ = η(a − |z|
2) (11.51)
where (for notational convenience) z = x + iy is a complex state variable
but a, ω, and η are real parameters. The behavior of the state variable
x versus t for a typical solution of system (11.51) with the parameter η
small and 0 <a< 1 is depicted in Figure 11.5. Note that the state of
the system seems to alternate between periods of quiescence followed by
bursts of oscillations. To reveal the underlying mechanism that produces680 11. Bifurcation
-1.5 -1 -0.5 0 0.5 1 1.5 2
0
0.5
1
1.5
2
Figure 11.6: Plot (r versus u) of the limit cycle for system (11.52) with
parameters a = 0.7 and η = 0.1 together with the line r = 0 and the curve
u + 2r2 − r4 = 0.
this behavior, change to polar coordinates z = reiθ and note that the
angular variable decouples in the equivalent system
u˙ = η(a − r2),
r˙ = ur + 2r3 − r5,
˙
θ = ω. (11.52)
Hence, the dynamical behavior of system (11.51) can be described by ana￾lyzing the phase portrait of the two-dimensional subsystem for the ampli￾tude r and the “bifurcation variable” u.
System (11.52) resembles the normal form model (11.50) for the subcriti￾cal pitchfork bifurcation. At least a subcritical pitchfork bifurcation occurs
in the dynamics of the system
u˙ = ηa, r˙ = r(u + 2r2),
where η plays the role of  and u the role of μ. An analysis of the corre￾sponding static subcritical pitchfork bifurcation for system (11.52), that is,
bifurcations of rest points in the system
r˙ = ur + 2r3 − r5 (11.53)11.4 Dynamic Bifurcation 681
where u is the bifurcation parameter, reveals that for u > 0 there are two
stable steady states which are not associated with the pitchfork bifurcation;
they exist because of the additional term r5. Inclusion of the dynamic
bifurcation parameter, given by the first equation of system (11.52), ensures
the occurrence of the delay phenomenon described in this section. Solutions
are attracted to the u-axis, pass through the bifurcation value u = 0,
and then move rapidly away from this axis. But, for system (11.52), after
such a solution is repelled from the u-axis, it is attracted to the ghost
of the static stable steady states, that is, the curve u + 2r2 − r4 = 0.
In addition, in transit from the vicinity of the u-axis to the vicinity of
this curve, the u component of the velocity vector changes sign. Thus,
such a solution moves slowly near the curve u + 2r2 − r4 = 0 at least
until its first component is negative. After this, the solution eventually
leaves the vicinity of this curve and is rapidly attracted toward the u-axis.
This hysteresis effect accounts for the oscillatory behavior of the system as
depicted in Figure 11.6. Bursting phenomena in the system are now easy to
understand: The angle θ is changing with constant frequency corresponding
to the oscillatory nature of the system. At the same time, the amplitude of
the oscillation is changing on two time scales. The amplitude is almost zero
as a solution on the limit cycle moves “slowly” along the u-axis or along
the ghost of the static stable steady state curve given by u + 2r2 − r4 = 0.
The speed in this slow regime is directly proportional to the value of ηa.
On the other hand, the amplitude of the oscillation changes rapidly during
the intervals of time while the solution is attracted or repelled from the
u-axis.
Exercise 11.64. Predict the behavior of the solution of the system
μ˙ = 	, y˙ = y(sin(μ) + y2 − y4
)
with the initial condition (μ, y)=(−1, 1/2) under the assumption that 	 > 0 is
small.
Exercise 11.65. Find the general solution of the differential equation
y˙ = y(	t − y2
)
corresponding to system (11.49). Can you explain the behavior of the solution
depicted in Figure 11.4? Hint: The differential equation is a form of Bernoulli’s
equation.
Exercise 11.66. Study systems (11.51) and (11.52) analytically and numeri￾cally for various values of the parameters. How does the character of the bursting
change as a is increased from a = 0 to a = 1 (statically) for fixed values of η and
ω? (The answer is in [148].) Prove that the planar system, consisting of the first
two equations of system (11.52), has a limit cycle.
Exercise 11.67. Show that slow passage through a subcritical pitchfork bifur￾cation for system (11.52) corresponds to slow passage through a Hopf bifurcation
for system (11.51).682 11. Bifurcation
11.5 Global Continuation and the
Crandall-Rabinowitz Theorem
The topic of this section is illustrated by a classic example: Euler buckling
via the boundary value problem (BVP)
x¨ + λ sin x = 0, x(0) = 0, x(π) = 0 (11.54)
with real parameter λ.
To define an appropriate bifurcation function, recall the function spaces
L2 (defined on the interval [0, π]) of square integrable functions and H2
0
(defined on the same interval) consisting of functions that vanish at the
end points of the interval and have two derivatives in L2. The bifurcation
function F : H2
0 × R → L2 is given by
F(x, λ)=¨x + λ sin x. (11.55)
Its analysis begins with a few basic facts: For each λ, F(0, λ) = 0 and the
derivative Fx(x, λ) with respect to the function space variable x ∈ H2 is
the linear transformation from H2
0 to L2 defined by
Fx(x, λ)y = ¨y + λ cos(x)y.
In particular, at x = 0,
Fx(0, λ)y = ¨y + λy. (11.56)
For λ0 in the interval [0, 1), this linear transformation has a bounded
inverse, which can be computed explicitly. Thus, by the implicit function
theorem, there is a unique function β, mapping an interval containing λ0
into H2
0 such that F(β(λ), λ) = 0. In fact, this function must be β(λ) ≡ 0.
At λ = 1, the linear transformation Fx(0, 1) is not invertible; it has a
one-dimensional kernel. Thus, the curve in the domain of F starting at
(x, λ) = (0, 0) and parameterized by λ that is mapped to zero by this
function contains the image of β on the interval 0 ≤ λ < 1 and has its first
possible bifurcation at λ = 1. In other words, the solution x = 0 is unique
in some ball around the origin in H2
0 as long as 0 ≤ λ < 1. New solutions
might appear if λ is increased.
The BVP 11.54 is a simple model of a slender beam such that position
along its central axis is measured by a real coordinate t in the interval
0 ≤ t ≤ π, a traction force with amplitude λ (measured in appropriate
units) is applied at one end of the beam, and whose deflection is a function
x : [0, π] → R that solves the BVP. So far, the main prediction from the
model is that the beam does not bend as long as the applied force remains
small; that is 0 ≤ λ < 1. To determine what occurs at λ = 1 and for larger
values of the parameter is the subject of this section.11.5 Global Continuation and the Crandall-Rabinowitz Theorem 683
Buckling is a typical and classic bifurcation problem that differs from the
theory for rest points or periodic orbits in families of ordinary differential
equations because its bifurcation function F depends on two variables: the
first is in a function space and the second varies over an open set of real
numbers. But, the basic problem remains the same: Determine and describe
the set of points in the domain of F that map to zero. In this section, some
point in the zero set is known. The problem is to determine and describe (if
they exist) curves in the zero set that contain the given point. Such curves
are called bifurcation curves.
Bifurcation functions arising in applications, even those whose underlying
models are as simple as BVP (11.54), usually have bifurcation curves that
cannot be expressed by simple formulas using only elementary functions.
For this reason, approximations using numerical methods usually play an
insightful role. There is also a rich mathematical theory that lays a rigorous
foundation. Both aspects of the subject are discussed. A goal is to provide
theory sufficient to write a computer code that on input of a bifurcation
function and an initialization, outputs its bifurcation curves.
Exercise 11.68. (a) Compute the inverse of Fx(0, λ), as in Equation 11.56, for
λ in the interval [0, 1). (b) Prove the transformation Fx(0, 1) is not invertible.
Hint: Show its range is not all twice differentiable functions that vanish at 0
and π.
11.5.1 Finite-Dimensional Approximation
Let X and Z denote Banach spaces, consider the abstract bifurcation func￾tion F : X × R → Z, assume F(x0, λ0) = 0, and F is continuously differ￾entiable (to every desired order). The problem is to determine, if it exists,
a bifurcation curve Γ in X × R that contains (x0, λ0). More precisely, we
seek a piecewise continuous function γ : J ⊆ R → X × R such that J is an
open interval containing the origin, γ(0) = (c0, λ0), and F(γ(s)) = 0 for all
s ∈ J. The curve Γ is the image of γ.
To approximate bifurcation curves using a computer, the problem must
be appropriately discretized. A first step in this direction for an infinite￾dimensional problem is reduction to finite dimensions. In fact, in many
models encountered in applications, X and Z are infinite-dimensional func￾tion spaces. For Euler buckling, X = H2
0 and Z = L2 have the same Fourier
basis. In fact, due to zero Dirichlet boundary conditions, a Fourier basis
for both H2
0 and L2 is the infinite set of functions t → sin kt parameter￾ized by all positive integers k. Banach spaces do not necessarily have bases,
but for many applications they have identifiable countable bases. Often the
appropriate basis functions are suggested by the mathematical or physical
context of the model under consideration.684 11. Bifurcation
Suppose that X andZ are function spaces such that φi fori = 1, 2, 3,...,∞
is a basis for X and ψi for i = 1, 2, 3,...,∞ is a basis for Z. Then,
∞
i=1
ziφi = F(
∞
i=1
xiφi, λ)
whenever the infinite sums with real coefficients represent elements in X
and Z, respectively. Natural finite-dimensional approximations of F are the
elements of the sequence of functions fn : Rn × R → Rn defined by
fn(x1, x2, x3,...,xn)=(z1, z2, z3,...,zn). (11.57)
With careful attention to details and perhaps additional hypotheses, this
sequence of finite-dimensional bifurcation problems and their bifurcation
curves often converge in an appropriate sense to the original bifurcation
problem. If it does, some of its elements can be discretized and input to a
computer program whose purpose is to approximate bifurcation curves.
ForEuler bucking, theFourier sine basis, and the notation of display (11.57),
zj = −
n
k=1
k2δjkxk +
2λ
π
 π
0
sin(n
k=1
xk sin kt) sin jt dt. (11.58)
Exercise 11.69. (a) Prove formula (11.58). (b) For the Euler buckling bifurca￾tion function (11.55), F(0, 1) = 0. Is this true for its finite-dimensional approxi￾mations fn defined using formulas (11.57) and (11.58)?
Exercise 11.70. (a) Write the finite-dimensional approximate bifurcation
function f1 as defined by formulas (11.57) and (11.58). (b) Prove that f1 can
be simplified to the representation f1(x1, λ) = −x1 + 2λJ1(x1), where J1 is
the Bessel function of the first kind; which satisfies the initial value problem
y¨ + ty˙ + (t
2 − 1) = 0, y(0) = 0, and ˙y(0) = 1/2. Hint: Use known properties of
Bessel functions or use power series. (c) Recall Ex. (11.11). Prove that the family
defined by f1 has a pitchfork bifurcation at (x1, λ) = (0, 1). Hint: Use power
series.
11.5.2 Continuation
Using the notation of the previous section for the finite-dimensional bifurca￾tion function f, if f(c0, λ0) = 0 and the partial derivative fc(c0, λ0) : Rn →
Rn is invertible, then (by the implicit function theorem) there is a unique
bifurcation curve passing through (c0, λ0). For the Euler buckling exam￾ple, the corresponding derivative fc(0, λ) is invertible as long as 0 ≤ λ < 1.
Continuation algorithms, the subject of this section, are designed to approx￾imate the bifurcation curve and perhaps continue it through bifurcations.
Let J be the (maximal) open interval on which the bifurcation curve γ :
J → Rn ×R is defined so that f(γ(s)) = 0; c : J → Rn and λ : J → R such11.5 Global Continuation and the Crandall-Rabinowitz Theorem 685
that γ(s)=(c(s), λ(s)); and s0 ∈ J and c0 ∈ Rn such that γ(s0)=(c0, λ0).
Also let Γ denote the image of γ. It is called the bifurcation set.
Suppose that (c0, λ0) has been computed. A continuation algorithm is
designed to approximate the next point along the (discretized) curve after
an increment ds > 0 for the parameter s along the curve has been specified
and the velocity vector ˙γ(s0) is known. The algorithm to be described here
produces an approximation of the required velocity at the next point along
the curve so it is available for subsequent steps. But, in case the algorithm
is to be started at (c0, λ0), the velocity of the curve at this initial point
must be determined separately. Doing so is problem dependent (see, for
example, Ex. 11.75).
At a point on the bifurcation curve at parameter value s and velocity v,
Euler’s approximation of the curve at s + ds gives
γ(s + ds) ≈ γ(s) + dsv.
It is a natural approximate value, but it usually does not lie on the desired
zero set; that is, the value of the bifurcation function
f(γ(s) + dsv)
is not expected to be zero. To diminish the residual (which is the differ￾ence of this latter value from zero), a viable strategy is to use the Euler
approximation as the predictor in a predictor-corrector algorithm where
the corrector step is an iterative process that uses Euler’s approximation
as its initial guess to determine a nearby point whose residual has smaller
absolute value.
Because the domain and range of f : Rn × R → Rn are not of the
same dimension, the function f is not suitable for constructing an iteration
scheme. A natural and useful idea is to augment it to a new function that
maps Rn × R to Rn+1 and has the same set of zeros.
For a nonzero tangent vector v to the bifurcation curve at a point p in
the bifurcation set Γ, a key observation is that the line passing through p
in the direction of a component of v with maximum magnitude is a useful
(local) coordinate manifold for parameterizing Γ by projection onto this
line in a neighborhood of p. More precisely, let π : Rn × R → R be the
usual linear projection onto this line in Rn ×R . The direction of the line is
chosen because Γ is most closely approximated by the velocity component
in this direction. Of course, it also has a desired property: If v is not zero,
then π(v) is not zero.
Proposition 11.71. For the bifurcation function f : Rn×R → Rn, suppose
that f(c0, λ0)=0, fc(c0, λ0) : Rn → Rn is invertible, and v is tangent to
Γ at (c0, λ0). Define π : Rn × R → R to be linear projection onto the
line through c0 ∈ Rn in the direction of a component of v with maximal
magnitude and define g : Rn+1 × R → Rn × R by
g(x, ds)=(f(x), π(x) − π(p + dsv)). (11.59)686 11. Bifurcation
Then, there is a function σ : I ⊆ R → Rn+1 such that I is an open interval
containing the origin, σ(0) = (c0, λ0), and g(σ(ds), ds) ≡ 0. Moreover, if x
is sufficiently close to (c0, λ0), |ds| is sufficiently small, and g(x, ds)=0,
then x = σ(ds). In other words, ds → σ(ds) parameterizes Γ near (c0, λ0).
Proof. The proposition follows from the implicit function theorem applied
to g. Let p = (c0, λ0) and note that g(p, 0) = 0. For every u ∈ Rn+1 the
partial derivative of g with respect to x at (p, 0) is given by
gx(p, 0)u = (Df(p)u, π(u)).
To apply the implicit function theorem, it suffices to show that the linear
transformation defined by the partial derivative has a trivial kernel; that
is, if u = 0, then gx(p, 0)u = 0.
Let ω : R → Rn+1 be such that ω(0) = p and ˙ω(0) = u. To be compatible
with the definition of f on its domain Rn × R, which is naturally identified
with Rn+1, the function ω may be viewed in (c, λ) components as ω(t) =
(c(t), λ(t)). Likewise the vector u has components (ξ, ). Using this notation,
d
dtf(ω(t))


t=0 = Df(p)u = fc(p)ξ + fλ(p).
The linear transformation fc(p) is invertible, thus it has n-dimensional
range. This implies the range of Df(p) is at least n-dimensional. Thus its
kernel is at most one-dimensional. In fact, the kernel is one-dimensional and
spanned by v. To prove this fact, note that if u = v, γ may be substituted
for ω and then Df(p)v = 0 because f(γ(s)) ≡ 0. But by hypothesis, π(v) =
0. This implies gx(p, 0) has a trivial kernel and is therefore invertible, as
required. ✷
In view of Proposition 11.71, the desired (local) augmentation of f is
defined—using the already computed p = (c0, λ0), velocity v, projection π,
and fixing ds > 0 so that |ds| is small in Eq. (11.59)—by h : Rn+1 → Rn×R
according to the formula
h(x)=(f(x), π(x) − π(p + dsv)).
A zero q of this function (near p) is a point near Γ in the direction in which
s is changing most rapidly. It may be approximated by Newton’s method or
one of its variants. Once q is computed, the current point along the curve is
redefined using the old notation so that p = q and the new components of
p have the old names (c0, λ0). Likewise the current value of the parameter
s0 is updated to be the previous s0 + ds in preparation for the next step.
Before repeating the predictor-corrector process to take another step,
the new velocity vector (again denoted by the old name v) must be com￾puted at the new s0. Viewing the parameterized bifurcation curve as before,11.5 Global Continuation and the Crandall-Rabinowitz Theorem 687
γ(s)=(c(s), λ(s)) and
d
dsf(γ(s))


s=s0
= fc(c0, λ0)˙c(0) + fλ(c0, λ0)λ˙(0) = 0;
that is, tangent vectors at s = s0 (in particular the desired velocity v =
(˙c(0), λ˙(0))) must satisfy the vector equation
fc(c0, λ0)ξ + fλ(c0, λ0)τ = 0
for ξ ∈ Rn and τ ∈ R. In case fc(c0, λ0) is invertible at the new point on the
curve, take τ = 1 (or −1 in case the desired continuation is in the negative
direction along the curve) and solve for
ξ = −(fc(c0, λ0))−1fλ(c0, λ0).
In effect, this solution (for ds > 0) is obtained by viewing the parameter s
as approximate arc length along the curve and solving for the velocity in
the linear system
fc fλ
0 1 	  c˙
λ˙
	
=
0
1
	
evaluated at the current s and γ(s). There are potential snags: The com￾puted tangent vector (ξ, τ ) may point in the wrong direction along Γ or its
magnitude may not be changing continuously. A viable remedy is to nor￾malize the computed vector by dividing by its magnitude and checking its
direction via inner product of the result with the vector v computed in the
previous step. The normalized vector is multiplied by −1 when the inner
product is negative and left unchanged otherwise. At least this additional
procedure produces a viable new v.
Step-by-step continuation of the curve with perhaps different choices of
ds at each step may proceed as long as the partial derivative fc(γ(s)) at
the current s is invertible.
11.5.3 Bifurcation
The continuation procedure already described fails when a point γ(s) is
reached along Γ where the linear transformation defined by the partial
derivative of the bifurcation function with respect to its space variable,
fc(γ(s)), is singular. In this case, Γ could have an endpoint or a continuation
might be possible but requires a choice among many branches. In this
section the simplest (but often the most likely) possibilities are discussed.
The curve Γ may not branch at γ(s) and simply continue through the
singularity. In this case γ has a nonzero velocity vector at the singularity.
The next proposition states sufficient conditions for computing this velocity
vector. For simplicity and using a slight abuse of notation, let the point γ(s)
have components (c, λ) in Rn × R.688 11. Bifurcation
Proposition 11.72. Suppose that f : Rn × R → Rn. If the null space of
the linear transformation fc(c, λ) : Rn → Rn is one-dimensional, fλ(c, λ)
is not in the range of fc(c, λ), and ν is a unit vector in this null space; then
the linear system
fc(c, λ) fλ(c, λ)
ν 0
	  ξ
τ
	
=
0
1
	
(11.60)
has a unique solution for the variable vector (ξ, τ ).
Proof. To show that the system matrix is not singular, suppose that (ξ, τ )
is in its null space. There is an ortho-complement U to the one-dimensional
kernel of fc(c, λ) and this linear transformation restricted to U is an iso￾morphism onto its range. Thus every vector in the domain of fc(c, λ) can
be written as the sum of a scalar multiple of ν and some vector u in U. In
particular, there is a scalar a such that (ξ, τ )=(aν + u, q), and the image
of this point under the linear transformation is given by the two equations
fcu = −τfλ, νξ = 0.
By the assumption that fλ(c, λ) is not in the range of fc(c, λ), the vector
fλ(c, λ) is not zero. Thus, the first equation holds only if τ = 0. Because
fc(c, λ) restricted to U is an isomorphism, u = 0. The second equation
states that the usual inner product of ν and ξ vanishes. Since ξ = aν, the
scalar a must be zero; therefore (ξ, τ ) = (0, 0), in contradiction. ✷
Under the assumptions of the proposition (which must be checked in a
viable computer code) and the additional assumption that γ has a nonzero
velocity at s, a nonzero scalar multiple of this vector can be determined by
solving linear system (11.60). The desired velocity can then be determined
as discussed in the previous section.
A more sophisticated theory is required to determine if a velocity vector
exists. For the case, as in Proposition 11.72, where the singular partial
derivative has a one-dimensional kernel, the theory for determining a new
branch of the zero set is well studied. A version of the Crandall-Rabinowitz
theorem (see [72]) is basic and has a familiar setting.
Theorem 11.73. Suppose that X and Z are Banach spaces, f : X×R → Z
is twice continuously differentiable, and f(0, λ)=0 for all λ in some open
interval J ⊆ R. If for some λ0 ∈ J, the linear transformation fx(0, λ0) :
X → Z has a one-dimensional kernel generated by ν ∈ X, its range
has a one-dimensional complement, and fxλ(0, λ0)ν is not in the range
of fx(0, λ), then there is a local C1 curve s → (x(s), λ(s)) passing through
(0, λ0) at s = 0 such that on its domain f(x(s), λ(s)) = 0, x
(0) = ν and
λ
(0) = −a/(2b), where πRc is the projection onto the (one-dimensional)
complement of the range and
a := πRc fxx(0, λ0)(ν, ν), b = πRc fxλ(0, λ0)ν.11.5 Global Continuation and the Crandall-Rabinowitz Theorem 689
Proof. The original source [72] has a readable proof. Alternatively, the
ingredients for a proof are in this book. A proof strategy incorporates a
Lyapunov-Schmidt reduction for f at (0, λ0) (see Ex. 11.80). ✷
A simple corollary, useful in constructing the continuation algorithm,
addresses the case where a bifurcation set Γ exists and is the range of
γ : R → X × R where γ(s)=(ξ(s), s). Suppose that γ(s0)=(x0, s0)
and fx(x0, s0) has a one-dimensional kernel. Using the change of coor￾dinates y = x − ξ(s), the singular point (x0, s0) maps to (0, s0) and the
Crandall-Rabinowitz theorem may be applied to the new function g defined
by g(y, s) = f(y + ξ(s), s). Observe that
gy(0, s0) = fx(x0, s0).
Thus the (one-dimensional) kernel and co-range of gy(0, s0) are the same
as those for fx(x0, s0). Let ν generate the kernel as before and compute
a :=πRc gyy(0, s0)(ν, ν) = πRc fxx(x0, s0)(ν, ν),
b :=πRc gys(0, s0)ν = πRc (fxx(x0, s0)(ν, ξ
(s0) + fxs(x0, s0)ν). (11.61)
A tangent vector to the new branch under the (transversality) condition
that b = 0 is given by (ν, −a/(2b)) in the (y, s) coordinates. Transforming
this vector by the derivative of the change of coordinates (y, s) → (y +
ξ(s), s) at (0, s0) produces a tangent vector at (x0, s0) for the new branch
in the original coordinates:
(ν − a
2b
ξ
(s0), − a
2b
). (11.62)
11.5.4 Summary
Consider f : Rn+1 → Rn and suppose a curve Γ in the zero set of f is
being followed up to the point x0 ∈ Rn+1 where a unit tangent vector v to
Γ in the desired direction along the curve has been specified. The task is to
continue Γ in the direction of this tangent vector. As previously discussed,
a predicted point near Γ is obtained via Euler’s approximation using the
vector v, and this point is used as the initial guess in an iterative corrector
algorithm (usually a quasi-Newton method) that produces a point x1 taken
as the continuation along Γ. To proceed, a tangent vector to Γ at x1 must
be determined. In the generic case, the derivative Df(x1) of f at x1 has
maximal rank n and the implicit function theorem implies that Γ continues
uniquely through x1. The desired tangent vector is a basis vector u for the
one-dimensional kernel of this derivative taken to have its direction compat￾ible with the previously given tangent vector v, which may be determined
by ensuring the scalar product v · u is positive. In the least degenerate
case, the one always considered in practical code, the kernel of Df(x1) is
two-dimensional and there are two possibilities: (1) Γ continues uniquely690 11. Bifurcation
through x1 or (2) the zero set of f has at least two branches passing through
x1. The existence of a second branch is implied by the Crandall-Rabinowitz
theorem when its hypotheses are satisfied. Several possibilities are present
in case these assumptions are not satisfied. In principle, to determine tan￾gents to the original and perhaps other branches requires an analysis of
the second-order derivatives of f. A way to continue the curve in practi￾cal code in the degenerate situation is to simply approximate its tangent
vector using the previously computed point x0 and the newly computed
point x1 by x1 − x0. This idea can be refined to allow more accuracy by
saving several previously computed points, using them to determine, for
example, a polynomial approximation of the curve, and then computing
(and normalizing) the tangent vector of this curve at x1.
There is high-quality software available for computing bifurcation curves
(see, for example, [93]). Alternatively, understanding the discussion in this
chapter should be sufficient to write a viable computer code.
Exercise 11.74. Does the Crandall-Rabinowitz theorem apply to the function
f : R2 × R → R2 given by f(x, y, λ)=(xλ − x3, −y)? Discuss the details.
Exercise 11.75. (a) Derive formula (11.58). (b) For n = 1, show directly from
formula (11.58) that the first singular point along the (approximate) bifurcation
curve s → (0, s) occurs at s = 1. (c) For n = 1, prove that the (approximate)
bifurcation curve has a branch that emerges orthogonal to the λ axis. A unit
vector (with a choice of two directions) would serve as the initial velocity vector
for the desired branch of the bifurcation curve. (d) Generalize (b) to n > 1. (e)
Generalize (c) to n > 1.
Exercise 11.76. Consider the BVP ¨x + eλx = 0, x(0) = 0, and x(1) = 0. Is
there a bifurcation akin to Euler buckling. Analyze and explain.
Exercise 11.77. (a) Write a computer code for following bifurcation curves
and apply it to the Euler buckling problem. In particular, use it to make a graph
of the bifurcation curve through the first singular point (that is, λ = 1). (b)
Consider the normal form for the pitchfork bifurcation: ˙x = λx − x3. Use your
computer program to follow the curve of rest points starting at (x, λ) = (0, −1)
and by increasing λ. What happens at λ = 0? Set up your code to automatically
follow the smooth continuation if it exists and then to automatically follow a new
branch if one exists. Note: Output of computer code can be checked directly with
pencil and paper. Show this work. (c) Apply your computer code to the BVP in
Ex. 11.76.
Exercise 11.78. [Continuation via Arc Length] The implicit function theorem,
as usual, ensures the existence of a solution curve for f(x, λ) = 0 at a point
(ξ, 
) when f(ξ, 
) = 0 and the linear transformation fx(ξ, 
) is invertible. It
fails when fx(ξ, 
) is not invertible. As in this section, continuation may proceed
under additional conditions on f. A point of view slightly different from the one
taken in this section is called arc length (or pseudo-arc length continuation). The
idea is geometric. Suppose that f(x0, λ0) = 0 and T denotes the unit tangent
vector to the solution curve at (x0, λ0) in the chosen direction (increasing or
decreasing λ). Let s be a new real parameter. Construct the point p at distance s11.5 Global Continuation and the Crandall-Rabinowitz Theorem 691
in (x, λ)-space from (x0, λ0) in the direction of T and the hyperplane containing
p and perpendicular to T. For small s, the solution curve should intersect this
plane at some point q. Write an equation g(q, s) = 0 that q must satisfy for
the given s. Use it to define a new bifurcation equation F(x, λ, s) = 0 given
by F(x, λ, s)=(f(x, λ), g(x, λ, s)). (a) Determine conditions on f and prove that
they ensure the implicit function can be applied to F to extend the solution curve.
(b) Show that the arc length continuation method is successful when applied to
the bifurcation function f : R2 × (0, ∞) → R2 given by
f(x, y, λ)=(x + λ − (x + λ)
3
/3 − y, y2 − x + 1).
Exercise 11.79. Consider the three-parameter system
x˙ = α + x2
y − (1 + β)x − γ(x2 + y2
),
y˙ = x(β − xy) − γ(x2 + y2
),
which was constructed from a named system, the Brusselator, that is obtained
when γ = 0; it is a simple model of an oscillating chemical reaction. (a) Set γ = 0.
Show that the system has a rest point and that a limit cycle exists for some region
in the (α, β) parameter plane. Discuss what you can prove about the boundary of
the region on which limit cycles exist? (b) Terms with parameter γ, which have
no obvious physical significance, were added to confine interesting dynamics to
bounded regions of the phase plane. A complete analysis of the changing phase
portrait over the three-dimensional parameter space is a rich source of phase
plane analysis problems. For the remainder of this exercise, which is devoted to
numerical computations, two parameters are fixed: α = 1.25 and γ = 0.01. The
third, β, is the bifurcation parameter. For β0 = 0 discuss a method to approximate
a rest point (x0, y0) with approximate coordinates (1.12541, 2.22776). (c) Use a
continuation method to determine a bifurcation curve in (x, y, β) coordinates as
β is changed from β0 in both directions. In particular, discuss the termination
of this curve; that is, does it terminate in either direction at a finite value? Or,
does it seem to exist for all β. Make a graph of the (x, y) coordinates of the
bifurcation curve in the plane over some bounded region that captures its main
features and specify what your graph shows. (d) A limit cycle surrounds the rest
point with coordinates (x0, y0) at β = β0. To verify this fact, consider the line
segment from the origin in the phase plane to (x0, y0) and determine a fixed point
of the return map defined on this Poincar´e section. (e) Using a perhaps changing
Poincar´e section, suggested by the construction in (d), use a continuation method
to determine a bifurcation curve for the existence of the limit cycle as β is changed
from β0. Discuss termination(s) of this bifurcation curve. Draw graphs showing
the position of the changing rest point of part (b) and the fixed point on the
Poincar´e section as β is changed. Discuss in detail exactly what your graph shows.
(f) The changing phase portrait in a region containing the limit cycle considered
in part (e) reveals two interesting mechanisms related to the termination of this
limit cycle. What are they?
Exercise 11.80. Write a complete proof of the Crandall-Rabinowitz theorem.References
[1] Abraham, R. and J. E. Marsden (1978). Foundations of Mechanics,
2nd ed. Reading: The Benjamin/Cummings Pub. Co.
[2] Abraham, R., J. E. Marsden, and T. Ratiu (1988). Manifolds, Tensor
Analysis, and Applications, 2nd ed. New York: Springer-Verlag.
[3] Abraham, R. and J. Robbin (1967). Transversal Mappings and Flows.
New York: W. A. Benjamin, Inc.
[4] Acheson, D. (1997). From calculus to chaos : an introduction to
dynamics. Oxford : New York : Oxford University Press.
[5] Alekseev, V. M. (1961). An estimate for the perturbations of the
solutions of ordinary differential equations. Vestnik Moskov. Univ.,
Ser. I Mat. Meh. 2, 28–36 (in Russian).
[6] Ahlbrandt, C. D. and J. Ridenhour (2003). Floquet theory for time
scales and Putzer representations of matrix logarithms. J. Diff. Eqs.
Appl. (1)9, 77–92.
[7] Andersen, C. and J. Geer (1982). Power series expansions for fre￾quency and period of the limit cycle of the van der Pol equation.
SIAM J. Appl. Math. 42, 678–693.
[8] Andronov, A. A., E. A. Leontovich, I. I. Gordon, and A. G. Maier
(1973). Qualitative Theory of Second-Order Dynamic Systems. New
York: John Wiley & Sons.
© The Editor(s) (if applicable) and The Author(s), under exclusive license to
Springer Nature Switzerland AG 2024
C. Chicone, Ordinary Differential Equations with Applications, Texts in
Applied Mathematics 34, https://doi.org/10.1007/978-3-031-51652-8
693694 References
[9] Andronov, A. A., E. A. Leontovich, I. I. Gordon, and A. G. Maier
(1973). Theory of Bifurcations of Dynamic Systems on a Plane. New
York: John Wiley & Sons.
[10] Andronov, A. A., A. A. Vitt, and S. E. Khaiken (1966). Theory of
Oscillators. Oxford: Pergamon Press.
[11] Arnold, V. I. (1973). Ordinary Differential Equations. Cambridge:
M. I. T. Press.
[12] Arnold, V. I. (1982). Geometric Methods in the Theory of Ordinary
Differential Equations. New York: Springer-Verlag.
[13] Arnold, V. I. (1978). Mathematical Methods of Celestial Mechanics.
New York: Springer-Verlag.
[14] Arnold, V. I., ed. (1988). Dynamical Systems I. New York: Springer￾Verlag.
[15] Arnold, V. I., ed. (1988). Dynamical Systems III. New York: Springer￾Verlag.
[16] Arnold, V. I., ed. (1990). Huygens and Barrow, Newton and Hooke,
E. J. F. Primrose, trans. Basel: Birkh¨auser-Verlag.
[17] Arnold, V. I. and B. A. Khesin (1998). Topological Methods in Hydro￾dynamics. New York: Springer-Verlag.
[18] Arnold, V. I. and A. Avez, (1968) Ergodic Problems of Classi￾cal Mechanics. (The Mathematical Physics Monograph Series) New
York: W. A. Benjamin, Inc.
[19] Aronson D. G. and H. F. Weinberger (1978). Multidimensional non￾linear diffusion arising in population genetics. Adv. Math. 30, 33–76.
[20] Arscott F. M. (1964). Periodic Differential Equations. New York:
MacMillan.
[21] Art´es, J. C., F. Dumortier, and J. Llibre (2005). Qualitative Theory
of Planar Differential Systems. Book Manuscript.
[22] Ashbaugh, M., C. Chicone, and R. Cushman (1991). The twisting
tennis racket. J. of Dyn. and Diff. Eqs. 3, 67–85.
[23] Barrow-Green, J. (1997). Poincar´e and the Three Body Problem.
Providence: Amer. Math. Soc.
[24] Bates, L. M. and R. H. Cushman (1997). Global Aspects of Classical
Integrable Systems. Boston: Birkh¨auser-Verlag.References 695
[25] Bautin, N. N. (1954). On the number of limit cycles which appear
with the variation of coefficients from an equilibrium position of focus
or center type. Amer. Math. Soc. Transl. 100, 1–19.
[26] Bender, J. (1997). Chaotic dynamics in a planar Hamiltonian system.
Master’s Project. Columbia: University of Missouri.
[27] Benguria, R. D. and M. C. Depassier (1996). Variational characteri￾zation of the speed of propagation of fronts for the nonlinear diffusion
equation. Comm. Math. Phy. 175, 221–227.
[28] Bisplinghoff, R. L. and H. Ashley (1975). Principles of Aeroelasticity.
New York: Dover Publications, Inc.
[29] Bliss, G. A. (1925). Calculus of Variations. Carus Math. Monographs,
Mathematical Association of America.
[30] Bogoliubov, N. N. and Y. A. Mitropolsky (1961). Asymptotic Methods
in the Theory of Nonlinear Oscillations. Delhi: Hindustan Pub. Corp.
[31] Bressan, A and B. Piccoli (2007). Introduction to the Mathematical
Theory of Control, Amer. Inst. Math. Sci, Springfield MO.
[32] Brieskorn, E. and H. Kn¨orrer (1986). Plane Algebraic Curves. Basel:
Birkh¨auser-Verlag.
[33] Brunton, S.L., M. Budiˇi´c, E. Kaiser, and J. N. Kutz (2022). Modern
Koopman Theory for Dynamical Systems, SIAM Review 64, 229–
340.
[34] Burns, J.A. (2014) Introduction to the Calculus of Variations and
Control with Modern Applications, CRC press, New York.
[35] Byrd, P. F. and M. D. Friedman (1971). Handbook of Elliptic Integrals
for Engineers and Scientists, 2nd ed. Berlin: Springer-Verlag.
[36] Cannon Jr., R.H. (1967). Dynamics of Physical Systems, New York:
McGraw-Hill.
[37] Carath´eodory, C. (1954). Theory of Functions of a Complex Variable,
1, pp. 229–236. New York: Chelsea.
[38] Chen, M., X.–Y. Chen, and J. K. Hale (1992). Structural stability
for time-periodic one-dimensional parabolic equations. J. Diff. Eqs.
96(2), 355–418.
[39] Chicone, C. (1986). Limit cycles of a class of polynomial vector fields
in the plane. J. Diff. Eqs. 63(1), 68–87.696 References
[40] Chicone, C. (1987). The monotonicity of the period function for pla￾nar Hamiltonian vector fields. J. Diff. Eqs. 69, 310–321.
[41] Chicone, C. (1992). Bifurcation of nonlinear oscillations and fre￾quency entrainment near resonance. SIAM J. Math. Anal. 23(6),
1577–1608.
[42] Chicone, C. (1993). Review of Finiteness Theorems for Limit Cycles,
by Yu. Il’yashenko. Bull. Amer. Math. Soc. 28(1), 123–130.
[43] Chicone, C. (1994). Lyapunov–Schmidt reduction and Melnikov inte￾grals for bifurcation of periodic solutions in coupled oscillators. J.
Diff. Eqs. 112, 407–447.
[44] Chicone, C. (1995). A geometric approach to regular perturbation
theory with an application to hydrodynamics. Trans. Amer. Math.
Soc. 374(12), 4559–4598.
[45] Chicone, C. (1997). Invariant tori for periodically perturbed oscilla￾tors. Publ. Mat. 41, 57–83.
[46] Chicone, C. (2017) An Invitation to Applied Mathematics: Differen￾tial Equations, Modeling, and Computation, Elsevier, London.
[47] Chicone, C., J. Critser and J. Benson (2005). Exact solutions of the
Jacobs two parameter flux model and cryobiological applications. J.
Cryobiology, 50(3), 308–316.
[48] Chicone, C. and F. Dumortier (1993). Finiteness for critical points of
the period function for analytic vector fields on the plane. Nonlinear
Analy. 20(4), 315–335.
[49] Chicone, C. and M. Jacobs (1989). Bifurcation of critical periods for
plane vector fields. Trans. Amer. Math. Soc. 312, 433–486.
[50] Chicone, C. and M. Jacobs (1991). Bifurcation of limit cycles from
quadratic isochrones. J. Diff. Eqs. 91, 268–327.
[51] Chicone, C. and Yu. Latushkin (1997). The geodesic flow generates a
fast dynamo: an elementary proof. Proc. Amer. Math. Soc. 125(11),
3391–3396.
[52] Chicone, C. and Yu. Latushkin (1999). Evolution Semigroups in
Dynamical Systems and Differential Equations. Providence: Amer￾ican Mathematical Society.
[53] Chicone, C., S. J. Lombardo and D. G. Retzloff (2021). Model￾ing, approximation, and time optimal temperature control for binder
removal from ceramics, Disc. Cont. Dyn. Sys. Ser. B, In Press.References 697
[54] Chicone, C. and W. Liu (1999). On the continuation of an invariant
torus in a family with rapid oscillation. SIAM J. Math. Analy. 31,
386–415.
[55] Chicone, C. and W. Liu (2004). Asymptotic phase revisited. J. Diff.
Eqs. 204(1), 227–246.
[56] Chicone, C., B. Mashhoon, and D. G. Retzloff (1996). Gravitational
ionization: periodic orbits of binary systems perturbed by gravita￾tional radiation. Ann. Inst. H. Poincar´e. 64(1), 87–125.
[57] Chicone, C., B. Mashhoon, and D. G. Retzloff (1996). On the ioniza￾tion of a Keplerian binary system by periodic gravitational radiation.
J. Math Physics. 37, 3997–1416.
[58] Chicone, C., B. Mashhoon, and D. G. Retzloff (1997). Addendum: On
the ionization of a Keplerian binary system by periodic gravitational
radiation. J. Math Physics. 38(1), 554.
[59] Chicone, C., B. Mashhoon, and D. G. Retzloff (1997). Gravitational
ionization: a chaotic net in the Kepler system. Class. Quantum Grav.
14, 699–723.
[60] Chicone, C., B. Mashhoon, and D. G. Retzloff (1999). Chaos in the
Kepler system. Class. Quantum Grav. 16, 507–527.
[61] Chicone, C. and J. Sotomayor (1986). On a class of complete poly￾nomial vector fields in the plane. J. Diff. Eqs. 61(3), 398–418.
[62] Chicone, C. and R. Swanson (2000). Linearization via the Lie deriva￾tive. Electron. J. Diff. Eqns., Monograph 02 (http://ejde.math.swt.
edu).
[63] Chicone, C. and J. Tian (1982). On general properties of quadratic
systems. Proc. Amer. Math. Soc. 89(3), 167–178.
[64] Chorin, A. J. and J. E. Marsden (1990). A Mathematical Introduction
to Fluid Mechanics, 2nd ed. New York: Springer-Verlag.
[65] Chow, S. N. and J. K. Hale (1982). Methods of Bifurcation Theory.
New York: Springer-Verlag.
[66] Chow, S. N., C. Li, and D. Wang (1994). Normal Forms and Bifur￾cation of Planar Vector Fields. Cambridge: Cambridge University
Press.
[67] Coddington, E. A. and N. Levinson (1955). Theory of Ordinary Dif￾ferential Equations. New York: McGraw–Hill Book Co.698 References
[68] Colwell, P. (1992). Bessel functions and Kepler’s equation. Amer.
Math. Monthly. 99, 45–48.
[69] Colwell, P. (1993). Solving Kepler’s Equation over Three Centuries.
Richmond: Willmann-Bell, Inc.
[70] Coppel, W. A. (1960). On a differential equation of boundary-layer
theory. Phil. Trans. Roy. Soc. London, Ser. A. 253, 101–36.
[71] Corbera, M. and J. Llibre (2002). On symmetric periodic orbits of
the elliptic Sitnikov problem via the analytic continuation method.
Celestial Mechanics (Evanston 1999), 91–97, Contemporary Mathe￾matics, Vol. 292. Providence: American Mathematical Society.
[72] Crandall, M. G. and P. H. Rabinowitz (1971). Bifurcation from a
simple eigenvalue. J. Func. Analy, 8, 321–340.
[73] Cushman, R. (1983). Reduction, Brouwer’s Hamiltonian, and the
critical inclination, Celest. Mech. 31, 401–429. Corrections: (1984).
Celest. Mech. 33, 297.
[74] Cushman, R. (1984). Normal form for Hamiltonian vectorfields with
periodic flow. In Differential Geometric Methods in Mathematical
Physics. S. Sternberg Ed. Reidel, Dordrecht: Reidel, 125–145.
[75] Cushman, R. and J. C. van der Meer (1987). Orbiting dust under
radiation pressure. In Proceedings of the XVth International Confer￾ence on Differential Geometric Methods in Theoretical Physics. H.
Doebner and J. Henning Eds. Singapore: World Scientific, 403–414.
[76] Cushman, R. (1988). An analysis of the critical inclination problem
using singularity theory. Celest. Mech. 42, 39–51.
[77] Cushman, R. (1992). A survey of normalization techniques applied
to perturbed Keplerian systems. Dynamics reported. New series 1,
54–112.
[78] Dadfar, M., J. Geer, and C. Andersen (1984). Perturbation analysis
of the limit cycle of the free van der Pol equation. SIAM J. Appl.
Math. 44, 881–895.
[79] Davenport, J. H., Y. Siret, and E. Tournier (1993). Computer Algebra:
Systems and Algorithms for Algebraic Computation, 2nd ed. London:
Academic Press, Ltd.
[80] Danby, J.M.A. (1985). Computing Applications to Differential Equa￾tions, Reston: Reston Pub. Inc.
[81] Davis, H. T. (1960). Introduction to Nonlinear Differential and Inte￾gral Equations. New York: Dover Pub. Inc.References 699
[82] Devaney, R.L. (1986). An Introduction to Chaotic Dynamical Sys￾tems. Benjamin/Cummings: Menlo Park, CA.
[83] Diacu, F. and P. Holmes (1996). Celestial Encounters : The Origins
of Chaos and Stability. Princeton: Princeton University Press.
[84] Diliberto, S. P. (1950). On systems of ordinary differential equations.
In Contributions to the Theory of Nonlinear Oscillations, Annals of
Mathematics Studies, 20, pp. 1–38. Princeton: Princeton University
Press.
[85] Dumortier, F. (1977). Singularities of vector fields on the plane. J.
Diff. Eqs. 23(1), 53–106.
[86] Dumortier, F. (1978). Singularities of Vector Fields. Rio de Janeiro:
IMPA Monograph, No. 32.
[87] Dumortier, F. (2005). Asymptotic phase and invariant foliations near
periodic orbits. Proc. Amer. Math. Soc., In press.
[88] Dunford, N. and J. T. Schwartz (1958). Linear Operators. New York:
Interscience Publications, Inc.
[89] Ecalle, J. (1992). ´ Introduction aux fonctions analysables et preuve
constructive de la conjecture du Dulac. Actualities Math. Paris: Her￾mann.
[90] Elbialy, M. S. (2001). Local contractions of Banach spaces and spec￾tral gap conditions. J. Funct. Anal 182(1), 108–150.
[91] Elderling, J. (2013). Normally Hyperbolic Invariant Manifolds: The
Noncompact Case. Paris: Atlantis Press.
[92] Elsadek, A., G. El-Bouymuni and H. Taha (2017). Stability analysis
of longitudinal dynamics of hovering flapping MAVs/insects. AIAA
Atmospheric Flight Mechanics Conference, 9–13 2017, Grapevine,
Texas. https://doi.org/10.2514/6.2017-1635.
[93] Ermentrout, B. (2002). Simulating, Analyzing, and Animating
Dynamical Systems: A Guide to XPPAUT for Researchers and Stu￾dents. Software, Environments and Tools, Series No. 14), Philadel￾phia: SIAM.
[94] Evans, L. C. (1998). Partial Differential Equations. Providence:
American Mathematical Society.
[95] Evans, L. (1984) An Introduction to Mathematical Optimal Control
Theory (Version 0.2), Unpublished lecture notes, Univ. California￾Berkeley.700 References
[96] Ewing, G. M. (1969). Calculus of Variations with Applications. New
York: W. W. Norton & Company, Inc.
[97] Falkner, V. M. and S. W. Skan (1931). Solution of boundary layer
equations. Philos. Mag. 7(12), 865–896.
[98] Farkas, M. (1994). Periodic Motions. New York: Springer-Verlag.
[99] Fenichel, N. (1971). Persistence and smoothness of invariant mani￾folds for flows. Indiana U. Math. J. 21, 193–226.
[100] Fermi, E., S. Ulam, and J. Pasta (1974). Studies of nonlinear problems
I. In Nonlinear Wave Motion, Lect. Appl. Math., Amer. Math. Soc.
15, 143–155.
[101] Feynman, R. P., R. B. Leighton, and M. Sands (1964). The Feynman
Lectures on Physics I–III. Reading: Addison–Wesley Pub. Co.
[102] Fife, P. C. (1979). Mathematical Aspects of Reacting and Diffusing
Systems. Lect. Notes Biomath., 28. New York: Springer-Verlag.
[103] Flanders, H. (1963). Differential Forms with Applications to the Phys￾ical Sciences. New York: Dover Publications, Inc.
[104] Fletcher, C. A. J. (1982). Burgers’ equation: a model for all reasons.
In Numerical Solutions of Partial Differential Equations. J. Noye, ed.
Amsterdam: North–Holland Pub. Co.
[105] Frantz, M. (2006). Some graphical solutions of the Kepler problem,
Amer. Math. Monthly., 113(1), 47–56.
[106] Friedlander, S. A. Gilbert, and M. Vishik (1993). Hydrodynamic
instability and certain ABC flows. Geophys. Astrophys. Fluid Dyn.
73(1-4), 97–107.
[107] Friedrich, H. and D. Wintgen, (1989). The hydrogen atom in a uni￾form magnetic field—an example of chaos. Phys. Reports. 183(1),
37–79.
[108] Geyling, F. T. and H. R. Westerman (1971). Introduction to Orbital
Mechanics. Reading: Addison–Wesley Pub. Co.
[109] Ghil, M. and S. Childress (1987). Topics in Geophysical Fluid Dynam￾ics : Atmospheric Dynamics, Dynamo Theory, and Climate Dynam￾ics. New York: Springer-Verlag.
[110] Gin´ea, Luiz, F.S. Gouveia, and J. Torregrosa (2021). J of Diff. Eqs.
275, 309–331.
[111] Gleick, J. (1987). Chaos: Making a New Science. New York: Viking.References 701
[112] Golubitsky, M. and D. G. Schaeffer (1985). Singularities and Groups
in Bifurcation Theory, 1. New York: Springer-Verlag.
[113] Golubitsky, M., I. Stewart, and D.G. Schaeffer (1988). Singularities
and Groups in Bifurcation Theory, 2. New York: Springer-Verlag.
[114] Gonzales, E. A. (1969). Generic properties of polynomial vector fields
at infinity. Trans. Amer. Math. Soc. 143, 201–222.
[115] Greenspan, B and P. Holmes (1984). Repeated resonance and homo￾clinic bifurcation in a periodically forced family of oscillators. SIAM
J. Math. Analy. 15, 69–97.
[116] Grimshaw, R. (1990). Nonlinear Ordinary Differential Equations.
Oxford: Blackwell Scientific Pub.
[117] Grobman, D. (1959). Homeomorphisms of systems of differential
equations (Russian). Dokl. Akad. Nauk. 128, 880–881.
[118] Gr´obner, W. (1960). Die Lie-Reihen und Ihre Anwendungen. Berlin:
VEB Deutscher Verlag der Wissenschaften (in German).
[119] Guckenheimer, J. and P. Holmes (1986). Nonlinear Oscillations,
Dynamical Systems, and Bifurcations of Vector Fields, 2nd ed. New
York: Springer-Verlag.
[120] Guti´errez, C. (1995). A solution to the bidimensional global asymp￾totic stability conjecture. Ann. Inst. H. Poincar´e Anal. Non Lin´eaire.
12(6), 627–671.
[121] Gutzwiller, M. C. (1990). Chaos in Classical and Quantum Mechan￾ics. New York: Springer-Verlag.
[122] Hale, J. K. (1980). Ordinary Differential Equations, 2nd ed. Malabar:
R. E. Krieger Pub. Co.
[123] Hale, J. K., L.T. Magalh˜aes, and W. M. Oliva (1984). An Introduc￾tion to Infinite Dimensional Dynamical Systems—Geometric Theory.
New York: Springer-Verlag.
[124] Hale, J. K. (1988). Asymptotic Behavior of Dissipative Systems. Prov￾idence: American Mathematical Society.
[125] Hall, R. (1984). Resonance zones in two-parameter families of circle
homeomorphisms. SIAM J. Math. Analy. 15, 1075–1081.
[126] Haller, G. (1999). Chaos Near Resonance. New York: Springer-Verlag.
[127] Hahn, W. (1967). Stability of Motion. A. P. Baartz, trans. New York:
Springer-Verlag.702 References
[128] Harris, W. A., J. P. Fillmore, and D. R. Smith (2001). Matrix
exponentials—another approach. SIAM Rev. 43(4), 694–706.
[129] Hartman, P. (1960). A lemma in the theory of structural stability of
differential equations. Proc. Amer. Math. Soc. 11, 610–620.
[130] Hartman, P. (1960). On local homeomorphisms of Euclidean space.
Bol. Soc. Mat. Mexicana 5 (2), 220–241.
[131] Hartman, P. (1964). Ordinary Differential Equations. New York: John
Wiley & Sons, Inc.
[132] Hastings, S. P. and W. C. Troy (1988). Oscillating solutions of the
Falkner–Skan equation for positive β. J. Diff. Eqs. 71, 123–144.
[133] Hayashi, C. (1964). Nonlinear Oscillations in Physical Systems. New
York: McGraw–Hill Book Co.
[134] Henry, D. (1981). Geometric Theory of Semilinear Parabolic Equa￾tions. New York: Springer-Verlag.
[135] Howe, R. (1983). Very basic Lie theory. Amer. Math. Monthly. 90,
600–623; Correction (1984). 91, 247.
[136] Heidel, J. and F. Zhang (2007) Nonchaotic and Chaotic Behav￾ior in Three-Dimensional Quadratic Systems: Five-One Conservative
Cases, International Journal of Bifurcation and Chaos. 17 (60) 2049–
2072.
[137] Hirsch, M. (1976). Differential Topology. New York: Springer-Verlag.
[138] Hirsch, M. and C. Pugh (1970). Stable manifolds and hyperbolic sets.
In Global Analysis XIV, Amer. Math. Soc. 133–164.
[139] Hirsch, M., C. Pugh, and M. Shub (1977). Invariant Manifolds, Lec￾ture Notes in Math., 583. New York: Springer-Verlag.
[140] Hirsch, M. and S. Smale (1974). Differential Equations, Dynamical
Systems, and Linear Algebra. New York: Academic Press.
[141] Holmes, P. and J. Marsden (1981). A partial differential equation with
infinitely many periodic orbits and chaotic oscillations of a forced
beam. Arch. Rat. Mech. Analy. 76, 135–165.
[142] Hubbard, J. and B. Burke (2002). Vector calculus, linear algebra, and
differential forms. A unified approach, 2nd ed. Upper Saddle River,
NJ: Prentice Hall, Inc.
[143] Il’yashenko, Yu. S. (1991). Finiteness Theorems for Limit Cycles,
Transl. Math. Mono., 94. Providence: American Mathematical Soci￾ety.References 703
[144] Ince, I. L. (1956) Ordinary Differential Equations. New York: Dover.
[145] Iooss, G. and M. Adelmeyer (1992). Topics in Bifurcation Theory and
Applications. River Edge: World Scientific Pub. Company, Inc.
[146] Isaacson, E and H. Keller (1966). Analysis of Numerical Methods.
New York: John Wiley & Sons, Inc.
[147] Iserles, A. (2002). Expansions that grow on trees. Not. Amer. Math.
Soc. 49(4), 430–440.
[148] Izhikevich, E. M. (2001). Synchronization of elliptic bursters. SIAM
Rev. 43(2), 315–344.
[149] Jones, C. K. R. T. (1995). Geometric singular perturbation the￾ory. Dynamical systems (Montecatini Terme, 1994). Lecture Notes
in Math. 1609. Berlin: Springer-Verlag, 44–118.
[150] Katok, A. B and B. Hasselblatt (1995). Introduction to the Mod￾ern Theory of Dynamical Systems. Cambridge: Cambridge University
Press.
[151] Kevorkian, J. and J. D. Cole (1981). Perturbation Methods in Applied
Mathematics. New York: Springer-Verlag.
[152] Kovalevsky, J. (1967). Introduction to Celestial Mechanics, Astro￾physics and Space Science Library, 7. New York: Springer-Verlag.
[153] Kolmogorov, A. N., I. G. Petrovskii, and N. S. Piskunov (1991). A
study of the diffusion equation with increase in the amount of sub￾stance, and its applications to a biological problem. In Selected Works
of A. N. Kolmogorov, V. M. Tikhomirov, ed. Dordrecht: Kluwer Aca￾demic Pub. Group.
[154] Kundu, P. K. and I. M. Cohen (2004). Fluid Mechanics, 3rd ed.,
Amsterdam: Elsevier Academic Press.
[155] Landau, L. D. and E. M. Lifshitz (1951). Classical Theory of Fields.
Reading: Addison–Wesley Press.
[156] Landau L. D. and E. M. Lifshitz (1987). Fluid Mechanics, 2nd ed.,
J. B. Sykes and W. H. Reid, trans. New York: Pergamon Press.
[157] Lang, S. (1962). Introduction to Differentiable Manifolds. New York:
Interscience Pub.
[158] Lee, E. B. and L. Markus (1967). Foundations of Optimal Control
Theory. New York: Wiley.704 References
[159] Lefschetz, S. (1977). Differential Equations: Geometric Theory, 2nd
ed. New York: Dover Publications, Inc.
[160] Levi, M. (1988). Stability of the inverted pendulum—a topological
explanation. SIAM Rev. 30, 639–644.
[161] Levi, M., C. Hoppensteadt, and M. Miranker (1978). Dynamics of
the Josephson junction. Quart. Appl. Math. 36, 167–198.
[162] Liapounoff, M. A. (1947). Probl`eme g´en´eral de la stabilit´e du move￾ment. Princeton: Princeton University Press.
[163] Liberzon, D, (2012) Calculus of Variations and Optimal Control: A
Concise Introduction, Princeton Univ. Press: Princeton NJ.
[164] Lichtenberg, A. J. and M. A. Lieberman (1982). Regular and Stochas￾tic Motion. New York: Springer-Verlag.
[165] Lochak, P. and C. Meunier (1988). Multiphase Averaging for Classical
Systems, H. S. Dumas, trans. New York: Springer-Verlag.
[166] Lorenz, E. N. (1963). Deterministic non-periodic flow. J. Atmos. Sci.
20, 131–141.
[167] Loud, W. S. (1964). Behavior of the periods of solutions of certain
plane autonomous systems near centers. Contributions Diff. Eqs. 3,
21–36.
[168] Magnus, W. and S. Winkler (1979). Hill’s Equation. New York: Dover
Publications, Inc.
[169] Massey, W. S. (1967). Algebraic Topology: An Introduction. New
York: Springer-Verlag.
[170] Marsden, J. E. and M. McCracken (1976). The Hopf Bifurcation and
Its Applications. New York: Springer-Verlag.
[171] McCuskey, S. W. (1963). Introduction to Celestial Mechanics. Read￾ing: Addison–Wesley Pub. Co.
[172] McGehee, R. P. and B. B. Peckham (1994). Resonance surfaces for
forced oscillators. Experiment. Math. (3)3, 221–244.
[173] McGehee, R. P. and B. B. Peckham (1996). Arnold flames and reso￾nance surface folds. Internat. J. Bifur. Chaos Appl. Sci. Engrg. (2)6
(1996), 315–336.
[174] McLachlan, N. W. (1958). Ordinary Non-linear Differential Equa￾tions in Engineering and Physical Sciences, 2nd ed. New York:
Oxford University Press.References 705
[175] Melnikov, V. K. (1963). On the stability of the center for time periodic
perturbations. Trans. Moscow Math. Soc. 12, 1–57.
[176] Meyer, K. and D. Offin (2017). An Introduction to Hamiltonian
Mechanics and the N-body Problem. 3rd Ed., Cham: Springer Inter￾national Publishing AG.
[177] Miller, R. K and A. N. Michel (1982). Ordinary Differential Equa￾tions. New York: Academic Press.
[178] Milnor, J. (1963). Morse Theory. Princeton: Princeton University
Press.
[179] Milnor, J. (1978). Analytic proofs of the “hairy ball theorem” and the
Brouwer fixed-point theorem. Amer. Math. Monthly 85(7), 521–524.
[180] Meyer, K. R. and G. R. Hall (1992). Introduction to Hamiltonian
Dynamical Systems and the N-Body Problem. New York: Springer￾Verlag.
[181] Meyer, K. R. and D. S. Schmidt (1977). Entrainment domains. Funk￾cialaj Edvacioj. 20, 171–192.
[182] Minorsky, N. (1962). Nonlinear Oscillations. Princeton: D. Van Nos￾trand Company, Inc.
[183] Morozov, A. D. (1998). Quasi-Conservative Systems: Cycles, Reso￾nances and Chaos. Series A, 30, Singapore: World Scientific.
[184] Moser, J. (1969). On a theorem of Anosov. J. Diff. Eqns. 5, 411–440.
[185] Moser, J. (1973). Stable and Random Motions in Dynamical Systems,
Annals of Math. Studies, 77. Princeton: Princeton University Press.
[186] Moser, J. (1978/79). Is the solar system stable? Math. Intelligencer
1(2), 65–71.
[187] Munkres, J. R. (1975). Topology: A First Course. Englewood Cliffs:
Prentice–Hall, Inc.
[188] Munkres, J. R. (1984). Elements of Algebraic Topology. Reading:
Addison–Wesley Pub. Co.
[189] Murdock, J. A. (1991). Perturbations: Theory and Methods. New
York: Wiley–Interscience Pub.
[190] Murdock, J. A. (2003). Normal Forms and Unfoldings for Local
Dynamical Systems. New York: Springer-Verlag.
[191] Murdock, J. A. and C. Robinson (1980). Qualitative dynamics from
asymptotic expansions: Local theory. J. Diff. Eqns. 36, 425–441.706 References
[192] Murray, J. D. (1980). Mathematical Biology. New York: Springer￾Verlag.
[193] Mustafa, O. G. and Y. V. Rogovchenko (2005). Estimates for domains
of local invertibility of diffeomorphisms. Proc. Amer. Math. Soc., In
Press.
[194] Nayfeh, A. H. (1973). Perturbation Methods. New York: John Wiley
& Sons.
[195] Nemytskii, V. V. and V. V. Stepanov (1989). Qualitative Theory of
Differential Equations. New York: Dover Publications, Inc.
[196] B.R. Noack, K. Afonasiev, M. Morzy´nski, G. Tadmor and F. Thiele
(2003). A hierarchy of low-dimensional models for the transient and
post-transient cylinder wake, Fluid Mech 497, 335–363.
[197] Nolasko, E. et.al. (2021). Optimal control in chemical engineering:
past, present and future. Comp. Chem Eng., 155, p. 107528.
[198] Oden, J. T. (1979). Applied Functional Analysis: A First Course for
Students of Mechanics and Engineering Science. Englewood Cliffs:
Prentice–Hall, Inc.
[199] Olver, P. J. (1986). Applications of Lie Groups to Differential Equa￾tions. New York: Springer-Verlag.
[200] O’Malley, R. E. (1991). Singular Perturbation Methods for Ordinary
Differential Equations. New York: Springer-Verlag.
[201] Palais, R. P. (1997). The symmetries of solitons. Bull. Amer. Math.
Soc. 34, 339–403.
[202] Palis, J. and W. deMelo (1982). Geometric Theory of Dynamical Sys￾tems: An Introduction. New York: Springer-Verlag.
[203] Pao, C. V. (1992). Nonlinear Parabolic and Elliptic Equations. New
York: Plenum Press.
[204] Perko, L. (1996). Differential Equations and Dynamical Systems, 2nd
ed. New York: Springer-Verlag.
[205] Pesch, H. J. (1994), A practical guide to the solution of real-life opti￾mal control problems. Control and Cybernetics 23 (1/2), 7–60.
[206] Pesch, H. J. and M. Plial (2009), The maximum principle of opti￾mal control: A history of ingenious ideas and missed opportunities,
Control and Cybernetics 38 (4A), 973–995.References 707
[207] Petty, C. M. and J. V. Breakwell (1960). Satellite orbits about a
planet with rotational symmetry. J. Franklin Inst. 270(4), 259–282.
[208] Poincar´e, H. (1881). M´emoire sur les courbes d´efinies par une
´equation diff´erentielle. J. Math. Pures et Appl. 7(3), 375–422; (1882).
8, 251–296; (1885). 1(4), 167–244; (1886). 2, 151–217; all reprinted
(1928). Oeuvre, Tome I. Paris: Gauthier–Villar.
[209] Pugh, C. (1969). On a theorem of P. Hartman. Amer. J. Math. 91,
363–367.
[210] Putzer, E. J. (1966). Avoiding the Jordan Canonical Form in the Dis￾cussion of Linear Systems with Constant Coefficients, Amer. Math.
Month. 73(1) 2–7.
[211] Reyn, J. W. (1994). A Bibliography of the Qualitative Theory of
Quadratic Systems of Differential Equations in the Plane, 3rd ed.,
Tech. Report 94-02. Delft: TU Delft.
[212] Rayleigh, Lord (1883). On maintained vibrations. Phil. Mag. 15, 229.
[213] Rhouma, M. B. H. and C. Chicone (2000). On the continuation of
periodic orbits. Meth. Appl. Analy. 7, 85–104.
[214] Robbin, J. W. (1968). On the existence theorem for differential equa￾tions. Proc. Amer. Math. Soc. 19, 1005–1006.
[215] Robbin, J. W. (1971). Stable manifolds of semi-hyperbolic fixed
points. Illinois J. Math. 15(4), 595–609.
[216] Robbin, J. W. (1972). Topological conjugacy and structural stability
for discrete dynamical systems. Bull. Amer. Math. Soc. 78(6), 923–
952.
[217] Robbin, J. W. (1981). Algebraic Kupka–Smale theory. Lecture Notes
in Math. 898, 286–301.
[218] Robinson, C. (1995). Dynamical Systems: Stability, Symbolic Dynam￾ics, and Chaos. Boca Raton: CRC Press.
[219] Roberts, A. J. (1997). Low-dimensional modelling of dynamical sys￾tems. http://xxx.lanl.gov/abs/chao-dyn/9705010.
[220] Rothe, F. (1993). Remarks on periods of planar Hamiltonian systems.
SIAM J. Math. Anal. 24(1), 129–154.
[221] Roussarie, R. (1998). Bifurcation of Planar Vector Fields and
Hilbert’s Sixteenth Problem. Basel: Birkha¨auser-Verlag.708 References
[222] Rousseau, C. and B. Toni (1997). Local bifurcations of critical periods
in the reduced Kukles system. Can. J. Math. 49(2), 338–358.
[223] Rudin, W. (1966). Real and Complex Analysis. New York: McGraw–
Hill Book Co.
[224] Sacker, R. (1964) On invariant surfaces and bifurcation of periodic
solutions of ordinary differential equations, New York University,
IMM-NYU 333, October 1964.
[225] Sacker, R. (1969) A perturbation theorem for invariant manifolds and
holder continuity, J. Math. Mech. 18, 705–762.
[226] S´anchez, D. (2002). Ordinary Differential Equations: A Brief Eclectic
Tour. Washington DC: The Mathematical Association of America.
[227] Sanders, J. A. and F. Verhulst (1985). Averaging Methods in Nonlin￾ear Dynamical Systems. New York: Springer-Verlag.
[228] Schecter, S. (1990). Simultaneous equilibrium and heteroclinic bifur￾cation of planar vector fields via the Melnikov integral. Nonlinearity.
3, 79–99.
[229] Smale, S. (1965). Diffeomorphisms with many periodic points. In Dif￾ferential and Combinatorial Topology, S. S. Cairns, ed. Princeton:
Princeton Univ. Press 63–80.
[230] Smale, S. (1980). The Mathematics of Time: Essays on Dynam￾ical Systems, Economic Processes and Related Topics. New York:
Springer-Verlag.
[231] Smale, S. (1998). Finding a horseshoe on the beaches of Rio. Math.
Intelligencer. 20, 39–62.
[232] Smoller, J. (1980). Shock Waves and Reaction-Diffusion Equations.
New York: Springer-Verlag.
[233] Sontag, E. D. (1990). Mathematical Control Theory. New York:
Springer-Verlag.
[234] Sotomayor, J. (1974). Generic one-parameter families of vector fields
on two-dimensional manifolds. Publ. Math. IHES 43, 5–46.
[235] Sotomayor, J. (1979). Li¸c´oes de equa¸c´oes diferenciais ordin´arias. Rio
de Janerio: IMPA.
[236] Sotomayor, J. (1990). Inversion of smooth mappings. Z. Angew.
Math. Phys. 41, 306–310.
[237] Sotomayor, J. (1996). Private communication. Lleida Spain.References 709
[238] Spanier, E. H. (1981). Algebraic Topology, corrected reprint. New
York: Springer-Verlag.
[239] Spivak, M. (1965). Calculus on Manifolds. New York: W. A. Ben￾jamin, Inc.
[240] Sternberg, S. (1969). Celestial Mechanics, Parts I and II. New York:
W. A. Benjamin, Inc.
[241] Stoker, J. J. (1950). Nonlinear Vibrations. New York: John Wiley &
Sons.
[242] Strang, G. and G. J. Fix (1973). An Analysis of the Finite Element
Method. Englewood Cliffs: Prentice–Hall, Inc.
[243] Strauss, W. A. (1992). Partial Differential Equations: An Introduc￾tion. New York: John Wiley & Sons.
[244] Strogatz, S. (1994). Nonlinear Dynamics and Chaos. Reading:
Addison–Wesley Pub. Co.
[245] H. J. Sussmann and J.C. Willems, 300 Years of Optimal Control:
From the brachystochrone to the maximum principle, IEEE Contr.
Sys. Mag., 17(3), 32–44.
[246] Takens, F. (1974). Singularities of vector fields. Publ. Math. IHES
43, 47–100.
[247] Temam, R. (1988). Infinite-dimensional Dynamical Systems in
Mechanics and Physics. New York: Springer-Verlag.
[248] Trotter, H. F. (1959). On the product of semi-groups of operators.
Proc. Amer. Math. Soc. 10, 545–551.
[249] van der Pol, B. (1926). On relaxation oscillations. Phil. Mag. 2, 978–
992.
[250] van der Pol, B. and J. van der Mark (1927). Frequency demultiplica￾tion. Nature. 120, 363–364.
[251] van der Pol, B. and J. van der Mark (1929). The heartbeat considered
as a relaxation oscillation and an electrical model of the heart. Arch.
N´eerl. de Physiol. de L’homme et des Animaux. 14, 418–443.
[252] Varadarajan, V. S. (1974). Lie Groups, Lie Algebras and their Rep￾resentations, New York: Springer-Verlag.
[253] Verhulst, F. (1989). Nonlinear Differential Equations and Dynamical
Systems, New York: Springer-Verlag.710 References
[254] Vinograd, R. E. (1957). The inadequacy of the method of character￾istic exponents for the study of nonlinear equations. Mat. Sbornik.
41(83), 431–438.
[255] Watson, G. N. (1966). A Treatise on the Theory of Bessel Functions,
2nd ed. Cambridge: Cambridge University Press.
[256] Weber, T. A. (2011) Optimal Control Theory with Applica￾tions in Economics. The MIT Press, http://www.jstor.org/stable/
j.ctt5hhgc4.
[257] Weinberger, H. (1999). An example of blowup produced by equal
diffusions. J. Diff. Eqns. (1)154, 225–237.
[258] Weissert, T. (1997). The Genesis of Simulation in Dynamics: Pursu￾ing the Fermi–Ulam–Pasta Problem. New York: Springer-Verlag.
[259] Weyl, H. (1952). Space–Time–Matter, H. L. Brose, trans. New York:
Dover Publications, Inc.
[260] Wiggins, S. W. (1988). Global Bifurcations and Chaos. New York:
Springer-Verlag.
[261] Wiggins, S. W. (1990). Introduction to Applied Nonlinear Dynamical
Systems and Chaos. New York: Springer-Verlag.
[262] Whittaker, E. T. and G. N. Watson (1927). A Course of Modern
Analysis, 1996 reprint. Cambridge: Cambridge University Press.
[263] Winkel, R. (2000). A transfer principle in the real plane from non￾singular algebraic curves to polynomial vector fields. Geom. Dedicata.
79, 101–108.
[264] Yagasaki, K. (1994). Chaos in a pendulum with feedback control.
Nonlinear Dyn. 6, 125–142.
[265] Yakubovich, V. A. and V. M. Starzhinskii (1975). Linear Differential
Equations with Periodic Coefficients, D. Louvish, trans. New York:
John Wiley & Sons.
[266] Yorke, J. A. (1969). Periods of periodic solutions and the Lipschitz
constant. Proc. Amer. Math. Soc. 22, 509–512.
[267] Zeidler, E. (1991). Applied Functional Analysis: Applications to
Mathematical Physics. New York: Springer-Verlag.
[268] Zo ˙ l¸adek, H. (1994). Quadratic systems with center and their pertur￾bations. J. Diff. Eqs. 109(2), 223–273.Index
A
ABC system, 570–588
and elliptic functions, 584
and chaotic attractors, 580
and hydrodynamic instabil￾ity, 582
and Melnikov function, 574
as oscillator, 572
chaotic, 573–579
heteroclinic cycle for, 573
numerical experiment, 579
periodic orbits of, 581–588
Picard-Fuchs equation, 588
unperturbed phase portrait,
571
Acheson, D., 301
action
of Lagrangian, 245
action integral, 248
action variable, 593
action-angle variables, 278
and averaging, 592
and Diliberto’s theorem, 619
and time map, 593
for harmonic oscillator, 592
for Kepler system, 265–280
for planar system, 618–622
adjoint method, 385
adjoint representation, 181
aeroelasticity, 333
Alekseev’s formula, 193
analytic function, 3
Andronov, A., 445
angular coordinate system, 67
angular momentum, 265
annular region, 97
anomaly
eccentric, 276
mean, 276
arc, 32
Arnold
tongue, 307, 488, 509
web, 608
Arnold, V., 509, 627
ascending node, 268
associated curve, 643
asymptotic
period, 240
phase, 240
stability, 19
© The Editor(s) (if applicable) and The Author(s), under exclusive license to
Springer Nature Switzerland AG 2024
C. Chicone, Ordinary Differential Equations with Applications, Texts in
Applied Mathematics 34, https://doi.org/10.1007/978-3-031-51652-8
711712 Index
Vinograd’s system, 81
asymptotic stability, 19
autonomous differential equation,
6
change of coordinates, 58
first order, 7
geometry of, 8
orbit types, 11
perturbations of, 454–469
phase flow, 14
averaged system
at resonance, 603
definition, 596
diamagnetic Kepler, 292
for satellite of oblate planet,
285
forced oscillator, 611
with two angles, 609
averaging, 262, 284–292, 591–616
action-angle variables, 592
and celestial mechanics, 597
and diffusion, 609
and fundamental problem of
dynamical systems, 595
and integrable systems, 592
and periodic orbits, 612
and resonance, 595, 601
capture, 605
passage through, 605, 614
and slow evolution of action,
596
at resonance, 601
pendulum model, 604–608
counterexample, 609
diamagnetic Kepler problem,
286–291
entrainment, 616
general invalidity of, 609
origins of, 284
partial, 603
perturbed binary system, 284
planar forced oscillator, 611
principle, 596
resonance
layer, 602
resonant
torus, 595
theorem, 597
transformation, 598
transient chaos, 616
B
Baire space, 633
Baire’s theorem, 633
Baker–Campbell–Hausdorff formula,
179, 180
Banach space
calculus in, 113–140
integration in, 120–126
bang-bang control, 392
basin of attraction, 31, 39
Bautin’s theorem, 667
Bautin, N., 105, 667
Beltrami’s theorem, 249
Bender, J., 642
Bernoulli equation, 5
Bernoulli’s equation, 59, 654
Bessel equation, 191
Bessel function, 280
bifurcation, 13–23, 449–523, 623–
670
and loss of stability, 624
blue sky, 13
diagram, 13
function, 500, 506
identification, 449, 453, 467,
500, 506
reduced, 498
reduction, 449, 467
subharmonic, 507
Hopf, 643–670
order k, 667
Neimark–Sacker, 504
pitchfork, 14, 636, 677
saddle-node, 14, 509, 624–641,
652
Hamiltonian, 642
theory, 623–670
and applied mathematics,
627–629Index 713
and families, 629
and jets, 629–636
and transversality, 629–636
dynamic, 674
normal forms, 626, 672
one-dimensional state space,
624–636
static, 674
transcritical, 636
big O notation, 448
bilinear form
coercive, 327
continuous, 327
binary system
action-angle coordinates, 265
and celestial mechanics, 264
and fundamental problem of
dynamics, 278
angular momentum, 265
completely integrable, 278
Delaunay elements, 273
diamagnetic Kepler problem,
286–291
averaging, 291
equations of motion, 264–272
Euler angles, 268
harmonic oscillator model, 271
invariant tori, 271, 278
Kepler
ellipse, 271
equation, 279
third law, 272
osculating plane, 267
perturbed, 273
in Delaunay elements, 277
satellite of an oblate planet,
280–286
averaging, 284
critical inclination, 285
birational map, 62
blow up
in finite time, 3, 4
blowup
and desingularization, 75
at rest point, 75–81
of singularity, 75–77, 648
blue sky catastrophe, 13
Bogdanov-Takens normal form, 443
Bogoliubov, N., 445
Bolza form, 366
boundary layer, 567–570
singular perturbation, 569
boundary value problem
at boundary layer, 569
Dirichlet, 309
Neumann, 310
shooting, 90, 570
branch point, 509
Brouwer fixed point theorem, 85,
89, 103
Brusselator, 672
bump function, 259, 413
bundle map, 132
Burgers’s equation, 344
bursting, 679
C
Cr
function, 45
-norm, 115
calculus, 113–140
canonical coordinates, 286
Cauchy’s
theorem, 548
Cayley-Hamilton theorem, 174, 350
center, 19, 23
linear, 23
center manifold, 33, 396, 414
center-stable manifold, 414
central projection, 77
chain rule, 113
change of coordinates, 58
chaos
chaotic attractor, 533
informal definition, 532
theory, 529
Melnikov method, 529
transverse homoclinic points,
545–559
transient, 533714 Index
chaotic attractor, 580
Chapman–Kolmogorov identities,
157
characteristic
exponent, 212
multiplier, 211
characteristics
of PDE, 340
charged particle
and Lorentz force, 263
motion of, 263
relativistic motion, 263
Chebyshev polynomial, 344
Cherkas, L., 106
Christoffel symbols, 258
Clairaut’s relation, 258
cocycle, 157, 434
Cole, J., 445
compactification
at infinity, 77
polynomial vector field, 79
complete
elliptic integral, 585
forward orbit, 92
completely integrable, 476
complex
potential, 566
solution, 161
configuration space, 36, 252
conjugate, 434
connecting orbit, 96
conservation law, 192, 339, 344
constitutive laws, 261
continuable periodic orbit, 449
continuation
point, 449, 464, 467, 498
subharmonic, 482, 483, 500,
502, 505, 506, 508
theory, 445–528
and entrainment for van der
Pol, 520
Arnold tongue, 488, 509
autonomous perturbations,
454
entrainment, 504
for multidimensional oscil￾lators, 504
forced oscillators, 522
forced van der Pol, 485
from rest points, 480
isochronous period annulus,
481
limit cycles, 504
Lindstedt series, 511
Lyapunov–Schmidt reduc￾tion, 496–499
Melnikov function, 469, 502
nonautonomous perturba￾tions, 477
normal nondegeneracy, 494
perihelion of Mercury, 511
regular period annulus, 493
resonance zone, 504
unforced van der Pol, 446
continuity equation, 309, 561
continuous dependence
of solutions, 3
contraction
constant, 127
definition of, 127
fiber contraction, 132
mapping theorem, 127
principle, 127–138, 327
uniform, 128
control
bang-bang, 392
LTI system, 347
PID, 382
control Hamiltonian, 364
control theory, 346
controllability matrix, 350
convection, 309
coordinate
chart, 42, 45
map, 43
system, 45
coordinate system
angular, 67
polar, 68
coordinatesIndex 715
cylindrical, 68
polar, 66–71
spherical, 68, 78
covering map, 66
critical inclination, 285
critical point, 11
curl, 459
curve, 32
cut-off function, 413
cylindrical coordinates, 68
D
degrees of freedom, 36, 252
Delaunay elements, 265
desingularized vector field, 70
determining equations, 518
detuning parameter, 477, 508, 509,
511
diagonal set, 57
diamagnetic Kepler problem, 286–
291
diffeomorphism, 44
differentiable function, 44, 113
differential
1-form, 72
of function, 72
differential algebraic system, 111
differential equation
ABC, see ABC system
autonomous, 6
Bernoulli’s, 59
binary system, 264
Burgers’s, 344
charged particle, 263
continuity, 309, 561
coupled pendula, 292
diffusion, 309
Duffing’s, 333, 545
Euler’s
fluid motion, 562
rigid body, 40
Euler-Lagrange, 246
Falkner-Skan, 569
Fermi-Ulam-Pasta, 296
Fisher’s, 334
for fluids, see fluid dynamics
for projectile, 101
harmonic oscillator, 12
heat, 309
Hill’s, 225
inverted pendulum, 302
Lorenz, 534
Loud’s, 481
Mathieu, 221
Maxwell’s, 260
Navier-Stokes, 561
Newton’s, 29, 36, 260
nonautonomous, 6
order of, 6
ordinary, 1
pendulum, see pendulum
Picard-Fuchs, 588
quadratic, 92
reaction diffusion, 309
Riccati, 229
singular, 112
solution of, 2
spatial oscillator, 263
van der Pol, 2, 7, 106, 108,
446, 544, 601
variational, 88
Volterra–Lotka, 47
diffusion, 309
equation, 309
rates, 609
diffusivity constant, 309
Diliberto, S., 460
Diliberto’s theorem, 459
Diophantine condition, 601
dipole potential, 74
directional derivative, 246
Dirichlet problem, 309
discrete dynamical system, 423
discriminant locus, 489
displacement function, 84, 448, 479
reduced, 449
divergence
and Bendixson’s criterion, 103
and Diliberto’s theorem, 459
and limit cycles, 99, 106716 Index
and volume, 159
double-well potential, 39
drift velocity, 263
Duffing equation
forced, 215
Dulac
criterion, 105
function, 105, 106
Dulac function, 104
Dulac’s criterion, 103
Dulac–Poincar´e linearization, 441
dynamic bifurcation theory, 674
dynamics
one-dimensional, 85
quadratic family, 85
E
Ecalle’s theorem, 668
eccentric anomaly, 276
eigenvalue
definition of, 160
eigenvalues
and adapted norms, 235
and characteristic multipliers,
212, 221
and Floquet theory, 211
and fundamental matrix, 162
and generic bifurcation, 624
and Hopf bifurcation, 643
and hyperbolic fixed points,
423
and hyperbolicity, 23
and index of singularity, 526
and invariant manifolds, 396
and Lyapunov exponents, 223
and normal modes, 297
and perturbation series, 484
and spectrum, 312
and stability, 21, 23, 183, 187,
197, 337
of periodic orbits, 233, 613
of subharmonics, 484
of time-periodic systems, 213,
216
and stability for PDE, 311
continuity of, 624
of derivative of Poincar´e map,
233
of matrix exponential, 213
stability, 186
eigenvector
definition of, 160
Einstein, A., 512
elliptic functions
integrals, 584
Jacobi, 585
elliptic modulus, 585
elliptic sector, 82
energy method
for PDE, 315
energy surface, 37
regular, 37, 592
entrainment, 487, 504–511, 520–
522, 616
domain, 485, 508
equation
Wa˙zewski’s, 63
also, see differential equation
continuity, 160, 309
Duffing
forced, 215
Duffing’s, 333
Falkner-Skan, 569
Fisher’s, 334
functional, 414, 424
Hamilton-Jacobi, 339
Kepler’s, 276, 279
Korteweg-de Vries, 339
Lyapunov’s, 200
Newton’s, 317
spring, 446
van der Pol, 108, 446, 601
Vinograd, 81
equilibrium point, 11
ergodic, 433
ergodic dynamical system, 177
Euclidean norm
matrix, 109
Euclidean space, 72
Euler’s equationsIndex 717
fluid dynamics, 562
rigid body motion, 40
Euler-Lagrange equation, 246–256
and extremals, 247
and Hamilton’s principle, 248
and Lagrangian, 248
of free particle, 251
evaluation map, 118
evolution
family, 17, 157
group, 157
existence theory, 1–4, 140–150
by contraction, 143
by implicit function theorem,
141
maximal extension, 147
smoothness by fiber contrac￾tion, 145
exponential map, 167
extremal, 247
F
Falkner-Skan equation, 569
family of differential equations, 2
Farkas, M., 445
fast
time, 113
variable, 603
Feigenbaum number, 85
Fermi, E., 296
Fermi-Ulam-Pasta oscillator, 296–
300
experiments, 300
normal modes, 300
Feynman, R., 260
fiber
contraction, 132
contraction theorem, 132
of tangent bundle, 53
of trivial bundle, 132
Fife, P., 338
first integral, 38, 251, 343, 470
in involution, 473
first variational equation, 456
fixed point, 127, 423
globally attracting, 127
Floquet
exponent, 212
multiplier, 211
normal form, 210
theorem, 207
theory, 207–221
and limit cycles, 215
and matrix exponential, 207
and periodic solutions, 217
and resonance, 215
characteristic exponents, 212
characteristic multipliers, 211
Marcus–Yamabe example,
216
monodromy operator, 210
reduction to constant sys￾tem, 213
Floquet theory, 151
flow, 15
complete, 15
flow box theorem, 59
fluid dynamics, 559–588
ABC flow, see ABC system
Bernoulli’s
equation, 565
law, 566
boundary conditions, 561
continuity equation, 561
corner flow, 567
equations of motion, 560
Euler’s equations, 562
Falkner-Skan equation, 569
flow in a pipe, 562
Navier-Stokes equations, 560
plug flow, 564
Poiseuille flow, 565
potential flow, 565
Reynolds’s number, 561
stream function, 566
foliation, 243
forced oscillator, 522–528
formal solution, 314
formula
Lie–Trotter product, 177718 Index
Liouville’s, 158, 160
nonlinear variation of param￾eters, 193
variation of parameters, 189
forward orbit, 92, 99
Fr´echet differentiable, 113
Fredholm
index, 496
map, 496
frequency, 446
function
Bessel, 280
bifurcation, 500, 506
contraction, 127
differentiable, 44
displacement, 84
Dulac, 105
exponential, 167
Lipschitz, 143, 196
local representation, 44
Lyapunov, 94, 200, 658
Melnikov, 502, 533
period, 527
real analytic, 3, 45
regulated, 120
separation, 536
simple, 120
smooth, 1, 45
subharmonic bifurcation, 507
uniform contraction, 128
functional equation, 414, 424
fundamental
matrix, 156
set, 155
G
Gal¨erkin
approximation, 323, 344
method, 322
principle, 324
general linear group, 56, 167
generic, 167, 624, 633, 637
geodesic, 73, 257
geometric singular perturbation the￾ory, 111
Gershgorin circle theorem, 362
global stability, 30, 104
Golubitsky, M., 627
Gr¨obner basis, 488
gradient
in cylindrical coordinates, 74
like, 40
omega limit set of, 103
system, 40, 72
with respect to Riemannian
metric, 73
graph of function, 41
Gronwall
inequality, 152
specific Gronwall lemma, 153
Guckenheimer, J., 445
H
H¨older continuous, 440
Hale, J., 445
Hamilton-Jacobi equation, 339
Hamiltonian, 251
classical, 36
system, 36, 72, 74, 265, 490
completely integrable, 476
integral of, 38
Hamilton’s principle, 249
harmonic, 453
function, 566
solution, 478, 486
and entrainment, 489
harmonic oscillator, 12, 29, 36, 446
action-angle variables, 592
and motion of Mercury, 513
and motion of moon, 225
and secular perturbation, 515
model for binary system, 271
perturbed, 469
phase portrait of, 447
Hartman–Grobman theorem, 24,
422–433
for diffeomorphisms, 423
for differential equations, 429
statement of, 423
using Lie derivative, 434Index 719
Hayashi, C., 445
heat equation, 309
heteroclinic
cycle, 573
orbit, 57, 529
Hilbert basis theorem, 666
Hilbert space basis, 324
Hilbert’s 16th problem, 92, 668
Hill’s equation, 225–228
and trace of monodromy matrix,
227
characteristic multipliers of,
226
Lyapunov’s theorem on sta￾bility, 228
stability of zero solution, 226
Hill, G., 225
Hirsch, M., 633
Holmes, P., 445
homoclinic
manifold, 549
orbit, 57, 292, 529
tangle, 533
homogeneous linear system, 151
Hopf bifurcation, 492, 643–670
and Lyapunov quantities, 660
and polynomial ideals, 666
at weak focus, 655
finite order, 664
infinite order, 665
multiple, 655, 656
multiplicity one, 651
order k, 666
point, 643
subcritical, 645
supercritical, 645
theorem, 651
hyperbolic
fixed point, 423
linear transformation, 23
periodic orbit, 456, 461, 463
rest point, 23, 623
saddle, 23
sink, 23
source, 23
theory, 395–443
hyperbolic toral automorphism, 433
I
ideal
of Noetherian ring, 666
Il’yashenko’s theorem, 668
implicit function theorem, 139–140
and continuation, 448, 453
and existence of ODE, 141
and inverse function theorem,
61
and persistence, 461
and Poincar´e map, 84, 455
and regular level sets, 48
and separation function, 536
proof of, 139
statement of, 48
index
Fredholm, 496
of rest point, 109
of singularity, 527
of vector field, 109
inertia matrix, 40
inertial manifold, 322
infinite-dimensional ODE, 309–322
infinitesimal
displacement, 505
hyperbolicity, 23
invariant, 663
infinitesimally
hyperbolic, 230, 623
hyperbolic matrix, 186
inhomogeneous linear system, 189
initial
condition, 2
value problem, 2, 141
integral curve, 2
invariant
function, 343
infinitesimal, 663
manifold, 32–38, 47, 395–421
applications of, 419–421
linear, 47
set, 32720 Index
sphere, 57
submanifold, 54
invariant foliation, 243
inverse function theorem, 60
inverted pendulum, 381
isochronous period annulus, 481
isolated rest point, 57
isospectral, 181
J
Jacobi elliptic function, 585
Jacobi identity, 181
jet
extension, 631
space, 630, 631
theory of, 630–635
Jordan canonical form, 163, 173
Jordan curve theorem, 94
K
Kepler
equation, 276, 279
motion, see binary system
system, 265
Kevorkian, J., 445
Khaiken, S., 445
Kolmogorov, A., 338
Koopman operator, 175
Korteweg-de Vries equation, 339
L
Lagrange multiplier, 249
Lagrangian, 246, 292
Laplace transform, 171
Lax pair, 181
Legendre necessary condition, 248
Legendre transformation, 251
level set
regular, 48
Levenberg-Marquardt principle, 391
Levi, M., 301
Li´enard
system, 106
transformation, 106
Lie
algebra, 56, 180
bracket, 474
derivative, 434, 474, 475, 658,
659, 669
group, 56, 180
infinitesimal invariant, 663
Trotter formula, 177
limit cycle
and asymptotic period, 240
and asymptotic phase, 240,
243
definition of, 96
globally attracting, 99
hyperbolic, 461
infinitely flat, 467
multiple, 464, 466
semistable, 649
stability of, 239
time-periodic perturbation of,
458
uniqueness of, 99, 106
limit set, 92–105
alpha-, 92
compact, 93
connected, 93
invariance of, 93
omega-, 92
Lindstedt series, 511–519
and forced oscillators, 519
and perihelion of Mercury, 518
and period of van der Pol oscil￾lator, 520
divergence of, 518
truncation of, 518
Lindstedt, A., 511
line of nodes, 267
linear center, 23
linear system
Chapman–Kolmogorov iden￾tities, 157
constant coefficients, 160–182
and Jordan canonical form,
172
and matrix exponential, 166–
173Index 721
fundamental matrix, 162,
172
Lax pairs, 180
Lie Groups, 180
Lie–Trotter formula, 177
solutions of, 161
evolution family, 157
extension, 154
fundamental matrix, 156
fundamental set of solutions,
155
homogeneous, 151–182
inhomogeneous, 189
Liouville’s formula, 158, 160
matrix solution, 156
nonconstant coefficients
and matrix exponential, 174
on infinite-dimensional space,
175
principal fundamental matrix,
156
stability of, 183–187
and eigenvalues, 187
and hyperbolic estimates,
183
and infinitesimal hyperbol￾icity, 186
state transition matrix, 156
superposition, 155–159
linearity
of differential equations, 155
linearization, 20
Dulac–Poincar´e, 441
of a vector field, 21
linearized
stability, 22, 187, 189
Liouville’s formula, 158, 160, 462
Lipschitz function, 143, 196
and periods, 109
Lipschitz inverse function theorem,
433
little o notation, 158
Liu, W., 187
local
coordinate, 455
property, 22
versus global, 83
Lorentz force, 261
Lorenz system, 30, 94, 534
Loud’s
system, 481
theorem, 481
LTI control system, 347
Lyapunov
algorithm, 658, 669
center theorem, 660
direct method, 20, 25, 658
equation, 200
exponent, 222–225
function, 25, 30, 94, 200, 658
indirect method, 20
linearized stability theorem,
197, 200
quantities
are polynomials, 668
quantity, 660, 670
stability, 18, 25–29
stability index, 670
stability theorem, 26
stability theorem for Hill’s equa￾tion, 228
theorem, 476
Lyapunov–Perron
method, 397
operator, 402
Lyapunov–Schmidt reduction, 496–
499, 505
M
manifold, 43
abstract definition, 45
center, 395
invariant, 32, 395–421
invariant linear, 47
linear, 46
smooth, 41–49
stable, 33, 395
submanifold, 43
map
adjoint representation, 181722 Index
bundle, 132
contraction, 127
coordinate, 43
exponential, 167
fiber contraction, 132
Poincar´e, 84, 232
return, 83, 84
uniform contraction, 128
Marcus–Yamabe example, 216
mass matrix, 333
Mathieu equation, 221
matrix
infinitesimally hyperbolic, 186
solution of linear system, 156
Maxwell’s laws, 260
Mayer form, 366
mean anomaly, 276
mean value theorem, 118, 120, 125
Melnikov, V. K.
and separation function, 536
function, 533
autonomous, 469
for ABC flows, 573, 574
for periodically forced pen￾dulum, 548
homoclinic loop, 546
homoclinic points, 545
meaning of, 503
push detected, 501
subharmonic, 502
integral, 539, 544
method, 529
method of lines, 192
minimal surface area, 259
Minorsky, N., 445
Mitropolsky, Y., 445
momentum, 260
monodromy operator, 210
Morse’s lemma, 526
motion, 248
multiple Hopf bifurcation, 655
multiplicity
of limit cycle, 464
Murdock, J., 445
N
Navier-Stokes equations, 561
Nayfeh, A., 445
nearest neighbor coupling, 296
Neumann problem, 310
Neumann series, 192
Newton’s
equation, 317
law, 6, 260
polygon, 490
Newton’s method, 62, 387
Newtonian to Lagrangian
mechanics, 288
Noether’s theorem, 253
Noetherian ring, 665
nonautonomous differential equa￾tion, 6
nonautonomous ODE, 158
nonlinear system
stability of, 189–199
and linearization, 197
and Lipschitz condition, 196
and Lyapunov functions, 200
Poincar´e-Lyapunov theorem,
205
stability of periodic orbits, 231
nonlinear variation of parameters,
193
norm
Cr, 115
Euclidean, 3
matrix, 109
operator, 115
supremum, 115
normal form, 626, 672
normal modes, 294, 295
normally hyperbolic, 111
manifold, 504
torus, 487, 504
normally nondegenerate, 494, 505
O
ODE, see ordinary differential equa￾tion
omegaIndex 723
limit point, 92
limit set, 92
omega lemma, 118
one-dimensional dynamics, 85
one-parameter group, 16
one-way wave equation, 192
optimal control
time optimal, 391
orbit, 2
connecting, 96
heteroclinic, 57, 529
homoclinic, 529
saddle connection, 529
order k bifurcation, 667
order of differential equation, 6
ordinary differential equation, 1,
141
infinite dimensional, 309–322
oscillator
harmonic, 446
van der Pol, 2, 7, 106, 108,
446–451
osculating plane, 267
P
PAH curve, 491
Palais, R., 300
partial averaging, 603
partial differential equations, 307–
342
as infinite-dimensional ODE,
309–322
Burgers’s equation, 344
energy method, 315
first order, 339–342
characteristics, 339
fluids, see fluid dynamics
Fourier series, 314
Gal¨erkin approximation, 322–
334
heat equation, 309
linearization, 311
reaction diffusion, 307
reaction-diffusion-convection,
308
rest points of, 316
period function, 318
traveling wave, 334–338
Fisher’s model, 334
passage through resonance, 605
Pasta, J., 296
pendulum
and Galileo, 586
and Hamiltonian, 38
at resonance, 604
coupled, 292–295
beats, 295
normal modes, 294
small oscillation of, 294
inverted, 301–306, 381
stability of, 302
mathematical, 24
period function, 585, 586
periodically forced, 547
phase modulated, 608
whirling, 672
with oscillating support, 221
with torque, 99, 604
periastron
argument of, 276
perihelion of Mercury, 511
period
annulus
definition of, 270, 467
drawing of, 270
isochronous, 481
regular, 495
function, 469, 527
of periodic orbit, 16
periodic orbit
and time-periodic perturba￾tions, 477–504
asymptotic stability of, 233
asymptotically stable, 19
continuable, 449
continuation, see continuation
continuation of, 445–528
definition of, 10
existence by averaging, 612724 Index
hyperbolic, 461, 463, 464, 494,
495, 504, 508
limit cycle
and asymptotic period, 240
and asymptotic phase, 240,
243
asymptotic stability, 236
of inhomogeneous linear sys￾tem, 229
persistence of hyperbolic, 461
persistent, 449
stability
and eigenvalues of Poincar´e
map, 233
stable
definition of, 19
periodic solution, 82–105
perturbation
geometric, 111
regular, 110
singular, 111
perturbation theory, see continu￾ation
Petrovskii, I., 338
phase
curve, 2
cylinder, 71, 477
flow, 14
notations for, 16
locked, 489
plane, 447
portrait, 11
shift, 509
and Arnold tongue, 511
space, 11, 36, 252
physics
classical, 260–291
constitutive laws, 261
Picard-Fuchs equation, 588
PID control, 382
Piskunov, N., 338
pitchfork bifurcation, 14, 636
plug flow, 564
Poincar´e
and fundamental problem of
dynamics, 278
compactification, 77
geometric theory of, 83
index, 109
map, 83–90, 231, 232, 455,
478
and displacement function,
84
and period orbits, 84
example of, 85
linearized, 484
metric, 258
plane, 73, 258
section, 83, 232, 455, 478
sphere, 78
Poincar´e linearization, 441
Poincar´e, H., 77, 81, 83, 539
Poincar´e–Andronov–Melnikov
function, 469
Poincar´e–Bendixson theorem, 95,
102
Poincare
continuation, 455
Poiseuille flow, 565
polar coordinates, 66
removable singularity, 70
system of, 68
polynomial systems, 77
positive branches, 464
positively invariant, 95
preparation theorem, 463
principal fundamental matrix, 156,
214, 450
principle
averaging, 591, 596
contraction, 127–138
determinism, 3
Gal¨erkin, 324
Hamilton’s, 249
linearized stability, 22, 189,
200, 312
of superposition, 155
problem
critical inclination, 285Index 725
diamagnetic Kepler, 286
diffusion in multifrequency sys￾tems, 609
fundamental problem of dynam￾ics, 278
Hilbert’s 16th, 92, 668
initial value, 141
open problem, 57
periodic orbits of ABC flows,
584
structural stability of gradi￾ents, 57
Puiseux series, 490
punctured plane, 68
push forward
of vector field, 58
Q
quadratic family, 85
quadratic regulator, 371
quadratic system, 92
quadrature
reduced to, 37
quasi-periodic solution, 219, 487
R
radii
of set, 645
Rayleigh quotient, 250
Rayleigh, Lord, 446
reachable, 346
reaction-diffusion models, 307
real analytic, 3
rectification lemma, 59
recurrence relation, 297
reduced displacement function, 449
reduction, 255, 449
regular
level set, 48
point, 59
regular perturbation, 110
regular perturbation theory, 452
regulated
function, 120
integral of, 123
reparametrization
of time, 63–65
complete, 65
geometric interpretation, 63
rescaling, 64
and diamagnetic Kepler prob￾lem, 291
and small parameter, 282
binary system, 264
coupled pendula, 293
in Falkner-Skan equation, 569
in Fisher’s equation, 335
in Navier-Stokes equations, 561
inverted pendulum, 302
residual set, 633
resonance
(m : n), 477
and averaging, 595
capture into, 605
definition of, 595
layer, 602
manifold, 602
passage through, 605
relation, 477
zone, 509
resonant
definition of, 595
rest point, 11, 490
basin of attraction, 31
hyperbolic, 23
isolated, 57, 76
location of, 20
nondegenerate, 23
semi-hyperbolic, 82, 422
return
map, 84, 448
time map, 84, 455
reversible, 25, 463
Reynold’s number, 344
Reynolds’s number, 561
Riccati equation, 5, 165, 229
cross ratio, 166
Riemannian metric, 72
rigid body motion, 40, 107
Robbin, J., 141726 Index
Roberts, A., 421
roll up, 489
Routh-Hurwitz stability test, 362
S
saddle connection, 529
breaking of, 57, 535
separation function, 536
saddle-node, 625, 637
bifurcation, 14, 625, 637, 652
at entrainment boundary,
509
bifurcation theorem, 625, 637
Hamiltonian, 642
sanity check, 370
scalar curvature, 459
Schaeffer, D., 627
Schecter, S., 535
Schwarzschild, K., 512
second variational equation, 467
section, 83
secular term, 515
semi-flow, 316
semi-group, 316
semi-hyperbolic, 82, 422
semistable
limit cycle, 649
rest point, 14
sensitivity to parameters, 452
separation function, 536
time-dependent, 536
separatrix splitting
autonomous perturbations, 535–
545
nonautonomous perturbations,
545–559
shock waves, 344
shooting, 90, 570
simple function, 120
integral of, 121
simple zero, 449
singular
perturbation, 569
singular perturbation theory, 111
singularity
index of, 527
nondegenerate, 526
sink, 19, 337
SIR model, 422
slow
manifold, 113
time, 113, 503, 604
variable, 603
Smale, S., 628
Smale-Birkoff theorem, 532
small denominator, 601
smooth
function, 1, 45
manifold, 41
solution
of ODE, 2
stable, 18
unstable, 19
Sotomayor, J., 145
source, 19
space of k-jets, 630
spectral
gap, 397
mapping theorem, 213
radius, 234
spectrum, 312
spherical coordinates, 68, 78, 358
spring equation, 446
stability, 451
by linearization, 18–24
global, 30, 104
index, 670
Lyapunov’s method, 25–29
periodic orbit, 232
stable
eigenspace, 33
in the sense of Lyapunov, 19
manifold, 33, 489
steady state, 14
subharmonic, 484
subspace, 33
stable manifold, 34, 396
stable matrix, 362
state space, 10, 11, 37, 252
state transition matrix, 156Index 727
static bifurcation theory, 674
steady state, 11
stable, 14, 18
Stewart, I., 627
stiffness matrix, 330
stirred tank reactor, 643
Stoker, J., 445
straightening out theorem, 59
strain, 259
stream
function, 566
line, 566
strong solution
of PDE, 325
strong topology, 632
structural stability, 57, 628
and heteroclinic orbit, 57
of vector field, 57
open problem, 57
Sturm-Liouville theory, 250
subharmonic, 478
bifurcation function, 507
continuation point, 482, 500
stable, 484
submanifold, 43
open sets of, 44
superposition, 155
supremum norm, 115
surface of revolution, 258
symmetry, 255
symplectic form, 73, 472
and Hamiltonian systems, 74
synchronization domain, 485
system of differential equations, 2
T
tangent
bundle, 53
fiber of, 53
map, 53
space, 50–55
definition of, 51
geometric definition, 56
of invariant manifold, 50
of submanifold, 51
Taylor’s theorem, 126
tent map, 85
theorem
asymptotic stability, 23
averaging, 597
periodic orbits, 612
Baire’s, 633
Bautin’s, 667
Bautin’s Dulac function, 105
Beltrami’s, 249
Bendixson’s, 103
Brouwer fixed point, 85, 89,
103
Cauchy’s, 548
Cayley-Hamilton, 174, 350
chain rule, 113
continuous dependence, 3
contollability, 355
contraction mapping, 127
Diliberto’s, 459
Dulac’s, 103
Dulac’s criterion, 105
Ecalle’s, 668
existence and uniqueness, 3,
141, 143
extension, 3
fiber contraction, 132
Floquet’s, 207
flow box, 59
Gershgorin circle, 362
Gronwall’s, 152
Hartman–Grobman, 24, 423,
429, 435
Hilbert basis, 666
Hopf bifurcation, 651
Il’yashenko’s, 668
implicit function, 48, 139, 448
inverse function, 60
Jordan curve, 94
Lax-Milgram, 327
Lie’s, 663
Lie–Trotter, 177
Liouville’s, 158, 160, 462
Lipschitz inverse function, 433
Loud’s, 481728 Index
Lyapunov
center, 660
instability, 28
linearized stability, 200
on Hill’s equation, 228
stability, 26
mean value, 118, 120, 125
Morse’s lemma, 526
Noether’s, 253
omega lemma, 118
Poincar´e–Bendixson, 95, 102
saddle-node bifurcation, 625,
637
Smale-Birkoff, 532
specific Gronwall, 153
spectral mapping, 213
Taylor’s, 126
Thom’s transversality, 632–
633
transport, 160
uniform contraction, 128
Weierstrass preparation, 463,
656
Yorke’s, 109
thermalization, 300
Thom’s transversality theorem, 632–
633
time t map, 423
time map
and action-angle variables, 593
time-dependent
separation function, 536
time-dependent vector field, 158
time-optimal control, 391
topological conjugacy, 434
topological equivalence, 628
topology
strong, 632
weak, 632
toral automorphism, 433
torus
normally hyperbolic, 487, 504
trajectory, 2
transcritical bifurcation, 636
transient chaos, 533, 616
transport theorem, 160
transversal, 629
transversality
of map to submanifold, 632
of vector to manifold, 83
transverse homoclinic point, 531
trapping region, 534
traveling wave, 335
trivial bundle, 132
trivial solution, 183
Trotter product formula, 177
turning point, 676
two-body problem, see binary sys￾tem
U
Ulam, S., 296
ultrasubharmonic, 478
uniform contraction
mapping, 128
theorem, 128
unit
in algebra of functions, 464
unit sphere, 77
universal unfolding, 626
unperturbed system, 446
unstable rest point, 202
unstable steady state, 14
V
van der Mark, J., 446
van der Pol oscillator, 2, 446–451,
601
continuation theory, 448
Melnikov integral for, 544
periodic solution of, 447
weakly damped, 469
van der Pol, B., 446
variation of
curves, 246, 536
parameters, 189, 450
nonlinear, 193
variational equation, 88, 449, 456,
460, 467, 482, 494, 507
solution of planar, 459Index 729
vector field, 8
base point, 50
conservative, 201
principal part, 50
velocity profile, 567
Vinograd’s system
and asymptotic stability, 81
viscous damping, 39
Vitt, E., 445
Volterra integral equation, 192
Volterra–Lotka system, 47
W
Watt governor, 206
wave equation
one way, 192
weak
attractor, 651
multiplicity of, 651
focus, 655
repeller, 651
weak solution
of PDE, 325
weak topology, 632
Weierstrass
polynomial, 464
preparation theorem, 463, 656
Wiggins, S., 445
Wronskian, 226
Y
Yorke’s theorem, 109
Yorke, J., 109
Z
zero solution, 151
