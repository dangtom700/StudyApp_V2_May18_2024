Lecture Notes in Statistics 225
Gregory C. Reinsel
Raja P. Velu
Kun Chen
Multivariate 
Reduced-Rank 
Regression
Theory, Methods and Applications
Second EditionLecture Notes in Statistics
Volume 225
Series Editors
Peter Diggle, Department of Mathematics, Lancaster University, Lancaster, UK
Scott Zeger, Baltimore, MD, USA
Ursula Gather, Dortmund, Germany
Peter Bühlmann, Seminar for Statistics, ETH Zürich, Zürich, SwitzerlandLecture Notes in Statistics (LNS) includes research work on topics that are more
specialized than volumes in Springer Series in Statistics (SSS). The series editors
are currently Peter Bühlmann, Peter Diggle, Ursula Gather, and Scott Zeger. Peter
Bickel, Ingram Olkin, and Stephen Fienberg were editors of the series for many
years.Gregory C. Reinsel • Raja P. Velu • Kun Chen
Multivariate Reduced-Rank
Regression
Theory, Methods and Applications
Second EditonGregory C. Reinsel
Statistics
University of Wisconsin–Madison
Madison, WI, USA
Raja P. Velu
Managerial Statistics and Finance
Syracuse University
Syracuse, NY, USA
Kun Chen
Statistics
University of Connecticut
Storrs, CT, USA
ISSN 0930-0325 ISSN 2197-7186 (electronic)
Lecture Notes in Statistics
ISBN 978-1-0716-2791-4 ISBN 978-1-0716-2793-8 (eBook)
https://doi.org/10.1007/978-1-0716-2793-8
1st edition: © Springer Science+Business Media New York 1998
2nd edition: © Springer Science+Business Media, LLC, part of Springer Nature 2022
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse
of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional affiliations.
This Springer imprint is published by the registered company Springer Science+Business Media, LLC,
part of Springer Nature.
The registered company address is: 1 New York Plaza, New York, NY 10004, U.S.A.In memory of our teachers
Ted Anderson and Greg ReinselPreface to the Second Edition
The first version of the book was published in 1998. It covered the developments
in the field as of that period. The pioneering work of Ted Anderson has provided a
framework for rich extensions of the reduced-rank model not only in methodology
but also in applications. Since the publication of the book, the interest in big data
has grown exponentially and thus there is a need for models that are relevant for
the analysis of large dimensional data. The basic reduced-rank models described
in the book provide a natural way to handle large dimensional data without using
any a priori theory. These models have been mainly extended via various forms
of regularization techniques that found their way in applications varying from
natural science areas such as genetics to social science fields including economics
and finance. These techniques lead to further dimension-reduction via variable
selection beyond what is implied by the reduced-rank models. They also add a
great value in terms of elegant, simplified interpretations of the model coefficients
and in the context of time series data, provide more accurate forecasts. Many of
the developments have come not only from the field of statistics but also from
machine learning, economics, etc., and thus, the interest in these extended topics
comes from various areas of orientation. With emphasis on computational aspects,
many of the research papers are also usually supplemented with coded algorithms
for ready implementation.
Because of the specialized nature of the topics, the first edition of the book was
in the form of a research monograph. But the book has been received very well; the
latest Google search indicates that there are more than 560 citations of the book with
most of the citations in articles published in top journals in statistics, econometrics,
and machine learning. Several doctoral theses have followed as well, extending the
models discussed in the book. Given the significance of the big data methodology
and its applications and given the continued interest in the book, the coverage in the
revision is intended to be extensive and timely.
What Is New in This Revision?
The first edition of the book had nine chapters; it started with the multivariate
regression model, established the basics, and then introduced the reduced-rank
viiviii Preface to the Second Edition
aspect of the coefficient matrix, outlining its practical significance both in terms of
dimension-reduction and in terms of model interpretation. Two notable features of
this book are the theoretical developments in the context of models for chronological
data as well as panel data. The book covered applications in finance and in
macroeconomics where well-founded theories lead naturally to a reduced-rank
model that is core to the book. The book concluded with a chapter on alternate
models that were available at that time. Select methods were illustrated with
numerical examples. In this revision, while most of the book (the first eight chapters)
has remained intact, some sections are expanded as at least two chapters had resulted
in journal publications and hence required some changes from the knowledge gained
in the process of their publications. Many of the chapters are also updated with
recent applications and extensions in various fields.
We begin the revision with Chapter 9 from the first edition that provided brief
descriptions of alternate methods to dimension reduction; the content of this chapter
is now moved to Section 2.9 and is expanded to include some new related methods
such as envelope models. In the new Chapter 9, we discuss partially reduced-rank
regression with grouped responses. In multivariate regression setup, there have
been many strategies to perform predictor variable selection with predictor group
structure, while the response group structure is largely overlooked. To close this gap,
we introduce the partially reduced-rank regression model, where only a subset of
response variables in its relationship with predictor set has reduced-rank structure.
In many applications, this method can handle potential block structures both in the
response and in the predictor variables and dimension reduction on partial subsets
more naturally.
In the era of data explosion, reduced-rank estimation, as a general dimension
reduction strategy, has been cast into the celebrated regularized learning paradigm
and flourished as a key ingredient in modern multivariate learning with high￾dimensional, large-scale data. We add four new chapters on these topics: (a) high￾dimensional reduced-rank regression, (b) unbiased risk estimation in reduced-rank
regression, (c) generalized reduced-rank regression, and (d) sparse reduced-rank
regression. The materials in these chapters feature a unified high-dimensional multi￾variate learning framework, rigorous non-asymptotic analysis, thorough exposition
on computation and optimization, and a versatile set of real-world applications.
In Chapter 10, our focus is on high-dimensional reduced-rank regression. We
start with an overview of the regularized learning paradigm, illustrated by a
few fundamental sparse regression methods. This part acts as a bridge between
the classical regime of large T (sample size), small n (number of predictors or
responses), and the high-dimensional regime of large n, small T . With this paradigm
shift, we formulate the reduced-rank regression models under the framework of
singular value penalization. A prototype approach, adaptive nuclear norm penalized
regression, is discussed in detail, with a non-asymptotic analysis on rank selection
and prediction. We then show that sparse models and reduced-rank models can
be understood in a unified framework of integrative reduced-rank regression.
This method aims to recover group-specific reduced-rank structures from differentPreface to the Second Edition ix
predictor groups and thus can be regarded as a multi-view extension of the reduced￾rank regression with two sets of regressors covered in Chapter 3.
In Chapter 11, we study unbiased risk estimation of reduced-rank regression for
model assessment and selection. Under Stein’s unbiased risk estimation framework,
a foremost task in estimating the true prediction error (risk) of a reduced-rank
model is to obtain an accurate estimate of model complexity. This concept is more
broadly known as the model degrees of freedom, and a naive estimator is simply
the number of free parameters. However, for non-linear estimation procedures,
including reduced-rank regression, the exact correspondence between the model
complexity and the number of free parameters may not hold. In this chapter,
we provide a finite-sample unbiased estimator of the degrees of freedom for a
general class of reduced-rank estimators. The finding that the unbiased estimator
admits an explicit form as a function of the estimated singular values is quite
elegant. The results cover a significant gap in the literature and advocate the use
of the unbiased degrees of freedom in constructing information criterion for a more
accurate selection of reduced-rank models.
Chapter 12 introduces several generalized reduced-rank regression models to
handle complex data issues. Specifically, we discuss robust estimation and outlier
detection in reduced-rank models with possibly contaminated data, reduced-rank
matrix completion with incomplete data, and mixed-response with a mix of
continuous and discrete data reduced-rank regression. The effect of outliers on the
estimation and inference is illustrated with practical examples. In the end, we intro￾duce a simple yet effective framework that simultaneously accounts for mixed data,
potential outliers, and missing values. These methods go beyond the conventional
setup and greatly enhance the applicability of reduced-rank methodology in real￾world applications.
In Chapter 13, we consider the integration of the reduced-rank structure with the
sparsity structure in high-dimensional multivariate regression. It has been realized
that these two types of low-dimensional structures could very much complement
each other: while the reduced-rank structure exploits the dependence among vari￾ables to achieve parsimony, the sparseness helps in variable selection. In this chapter,
we first consider the situation where both, reduced-rank structure and column
sparsity in the coefficient matrix, are present. This leads to the sparse reduced-rank
regression for dimension reduction and predictor selection. We then consider a more
challenging situation where the sparsity is imposed on the (generalized) singular
value decomposition of the reduced-rank coefficient matrix, which leads to the co￾sparse factor regression for achieving factor-specific, two-way variable selection.
The focus of this revised edition is not only the expanded list of topics covered
with in-depth theory, but also the intuitions and insights that will illuminate further
developments and novel applications. We expand on the examples bringing in more
applications in genetics, environmental science etc. The illustrative examples from
economics and finance are augmented with new, novel applications. We also attempt
to provide the computational tools and resources necessary to apply the methods.
We have carefully selected examples to show the unique value of the reduced￾rank regression methods whether to uncovering latent relationships which are notx Preface to the Second Edition
readily observable with the usual methods or to large dimensional forecasting for
chronological data where the traditional methods break down. An R package called
rrpack with the new tools developed in the book is made publicly available.
Acknowledgments We want to thank Caleb McWhorter who has helped us
immensely in getting the book to its current form by his skilled knowledge in LATEX.
We also want to thank our doctoral students, Zhaoque Zhou for his help on Chapter 8
and Jianmin Chen who helped in carrying out several numerical examples. Jianmin
Chen, Xin Yang, and Bruce Jin all helped with proofreading Chapters 10–13. The
reviews of the proposal for the revision of the book were quite constructive, and we
want to thank the reviewers. Professor Guofu Zhou at Washington University at St.
Louis, read through Chapter 8 and offered valuable suggestions; we want to express
our appreciation. Raja Velu would like to thank Whitman School of Management
for the support toward the production of this book.
Special Recognition A big project like this is not possible without the support of
the family. We owe our gratitude to our spouses, Yasodha and Tingting, and our
children for their everlasting love and support.
Syracuse, NY, USA Raja Velu
Storrs, CT, USA Kun ChenPreface to the First Edition
In the area of multivariate analysis, there are two broad themes that have emerged
over time. The analysis typically involves exploring the variations in a set of
interrelated variables or investigating the simultaneous relationships between two
or more sets of variables. In either case, the themes involve explicit modeling of
the relationships or dimension-reduction of the sets of variables. The multivariate
regression methodology and its variants are the preferred tools for the parametric
modeling and descriptive tools such as principal components or canonical correla￾tions are the tools used for addressing the dimension-reduction issues. Both act as
complementary to each other and data analysts typically want to make use of these
tools for a thorough analysis of multivariate data. A technique that combines the
two broad themes in a natural fashion is the method of reduced-rank regression.
This method starts with the classical multivariate regression model framework but
recognizes the possibility for the reduction in the number of parameters through a
restriction on the rank of the regression coefficient matrix. This feature is attractive
because regression methods, whether they are in the context of a single response
variable or in the context of several response variables, are popular statistical tools.
The technique of reduced-rank regression and its encompassing features are the
primary focus of this book.
The book develops the method of reduced-rank regression starting from the
classical multivariate linear regression model. The major developments in the
area are presented along with the historical connections to concepts and topics
such as latent variables, errors-in-variables, and discriminant analysis that arise in
various science areas and a variety of statistical areas. Applications of reduced￾rank regression methods in fields such as economics, psychology, and biometrics
are reviewed. Illustrative examples are presented with details sufficient for readers
to construct their own applications. The computational algorithms are described so
that readers can potentially use them for their analysis, estimation, and inference.
The book is broadly organized into three areas: regression modeling for general
multi-response data, for multivariate time series data, and for longitudinal data.
Chapter 1 gives basic results pertaining to analysis for the classical multivariate
linear regression model. Chapters 2 and 3 contain the important developments and
xixii Preface to the First Edition
results for analysis and applications for the multivariate reduced-rank regression
model. In addition to giving the fundamental methods and technical results, we also
provide relationships between these methods and other familiar statistical topics
such as linear structural models, linear discriminant analysis, canonical correlation
analysis, and principal components. Practical numerical examples are also presented
to illustrate the methods. Together, Chapters 1 through 3 cover the basic theory and
applications related to modeling of multi-response data, such as multivariate cross￾sectional data that may occur commonly in business and economics.
Chapters 4 and 5 focus on modeling of multivariate time series data. The
fourth chapter deals with multivariate regression models where the errors are
autocorrelated, a situation that is commonly considered in econometrics and other
areas involving time series data. Chapter 5 is devoted to multiple autoregressive
time series modeling. In addition to natural extensions of the reduced-rank models
introduced in Chaps. 1 through 3 to the time series setting, we also elaborate on
nested reduced-rank models for multiple autoregressive time series. A particular
feature that may occur in time series is unit-root nonstationarity, and we show how
issues about nonstationarity can be investigated through reduced-rank regression
methods and associated canonical correlation methods. We also devote a section
to the related topic of cointegration among multiple nonstationary time series, the
associated modeling, estimation, and testing. Numerical examples using multivari￾ate time series data are provided to illustrate several of the main topics in Chaps. 4
and 5.
Chapter 6 deals with the analysis of balanced longitudinal data, especially
through use of the popular growth curve or generalized multivariate analysis of
variance (GMANOVA) model. Results for the basic growth curve model are first
developed and presented. Then we illustrate how the model can be modified and
extended to accommodate reduced-rank features in the multivariate regression
structure. As a result, we obtain an expanded collection of flexible models that can
be useful for more parsimonious modeling of growth curve data. The important
results on estimation and hypothesis testing are developed for these different forms
of growth curve models with reduced-rank features, and a numerical example is
presented to illustrate the methods. The use of reduced-rank growth curve methods
in linear discriminant analysis is also briefly discussed.
In Chapter 7, we consider seemingly unrelated regression (SUR) equations
models, which are widely used in econometrics and other fields. These models
are more general than the traditional multivariate regression models because they
allow for the values of the predictor variables associated with different response
variables to be different. After a brief review of estimation and inference aspects
of SUR models, we investigate the possibility of reduced rank for the coefficient
matrix pooled from the SUR equations. Although the predictor variables take on
different values for the different response variables, in practical applications the
predictors generally represent similar variables in nature. Therefore we may expect
certain commonality among the vectors of regression coefficients that arise from
the different equations. The reduced-rank regression approach also provides a type
of canonical analysis in the context of the SUR equations models. EstimationPreface to the First Edition xiii
and inference results are derived for the SUR model with reduced-rank structure,
including details on the computational algorithms, and an application in marketing
is presented for numerical illustration.
In Chapter 8, the use of reduced-rank methods in the area of financial economics
is discussed. It is seen that, in this setting, several applications of the reduced-rank
regression model arise in a fairly natural way from economic theories. The context in
which these models arise, particularly in the area of asset pricing, the particular form
of the models and some empirical results are briefly surveyed. This topic provides an
illustration of the use of reduced-rank models not mainly as an empirical statistical
modeling tool for dimension-reduction but more as arising somewhat naturally in
relation to certain economic theories.
The book concludes with a final chapter that highlights some other techniques
associated with recent developments in multivariate regression modeling, many of
which represent areas and topics for further research. This includes brief surveys
and discussion of other related multivariate regression modeling methodologies that
have similar parameter reduction objectives as reduced-rank regression, such as
multivariate ridge regression, partial least squares, joint continuum regression, and
other shrinkage and regularization techniques.
Because of the specialized nature of the topic of multivariate reduced-rank
regression, a certain level of background knowledge in mathematical and statistical
concepts is assumed. The reader will need to be familiar with basic matrix theory
and have some limited exposure to multivariate methods. As noted above, the
classical multivariate regression model is reviewed in Chapter 1 and many basic
estimation and hypothesis testing results pertaining to that model are derived and
presented. However, it is still anticipated that the reader will have some previous
experience with the fundamentals of univariate linear regression modeling.
We want to acknowledge the useful comments by reviewers of an initial draft
of this book, which were especially helpful in revising the earlier chapters. In
particular, they provided perspectives for the material in Chapter 2 on historical
developments of the topic. We are grateful to T. W. Anderson for his constructive
comments on many general aspects of the subject matter contained in the book. We
also want to thank Guofu Zhou for his helpful comments, in particular on material
in Chapter 8 devoted to applications in financial economics. The scanner data used
for illustration in Chapter 7 was provided by A. C. Nielsen and we want to thank
David Bohl for his assistance. Raja Velu would also like to thank the University of
Wisconsin-Whitewater for granting him a sabbatical during the 1996–97 academic
year and the University of Wisconsin-Madison for hosting him; the sabbatical was
partly devoted to writing this book.
Finally, we want to express our indebtedness to our wives, Sandy and Yasodha,
and our children, for their support and love which were essential to complete this
book.
Madison, WI, USA Gregory C. Reinsel
Syracuse, NY, USA Raja VeluContents
1 Multivariate Linear Regression .......................................... 1
1.1 Introduction .......................................................... 1
1.2 Multivariate Linear Regression Model and Least Squares
Estimator ............................................................. 3
1.3 Further Inference Properties in the Multivariate
Regression Model ................................................... 5
1.4 Prediction in the Multivariate Linear Regression Model .......... 10
1.5 Numerical Examples ................................................. 12
1.5.1 Biochemical Data ........................................... 12
1.5.2 Sales Performance Data .................................... 15
2 Reduced-Rank Regression Model ........................................ 19
2.1 The Basic Reduced-Rank Model and Background ................. 19
2.2 Some Examples of Application of the Reduced-Rank Model...... 24
2.3 Estimation of Parameters in the Reduced-Rank Model ............ 29
2.4 Relation to Principal Components and Canonical
Correlation Analysis ................................................. 39
2.4.1 Principal Components Analysis............................ 39
2.4.2 Application to Functional and Structural
Relationships Models....................................... 40
2.4.3 Canonical Correlation Analysis............................ 42
2.5 Asymptotic Distribution of Estimators in Reduced-Rank Model .. 45
2.6 Identification of Rank of the Regression Coefficient Matrix ....... 55
2.7 Reduced-Rank Inverse Regression for Estimating
Structural Dimension................................................. 59
2.8 Numerical Examples ................................................. 60
2.9 Alternate Procedures for Analysis of Multivariate
Regression Models ................................................... 64
xvxvi Contents
3 Reduced-Rank Regression Models with Two Sets of Regressors...... 75
3.1 Reduced-Rank Model of Anderson.................................. 75
3.2 Application to One-Way ANOVA and Linear
Discriminant Analysis................................................ 82
3.3 Numerical Example Using Chemometrics Data .................... 85
3.4 Both Regression Matrices of Lower Ranks: Model and
Its Applications....................................................... 90
3.5 Estimation and Inference for the Model............................. 92
3.5.1 Efficient Estimator.......................................... 93
3.5.2 An Alternative Estimator................................... 97
3.5.3 Asymptotic Inference....................................... 98
3.6 Identification of Ranks of Coefficient Matrices..................... 103
3.7 An Example on Ozone Data ......................................... 105
3.8 Conclusion ............................................................ 110
4 Reduced-Rank Regression Model With Autoregressive
Errors ....................................................................... 113
4.1 Introduction and the Model .......................................... 113
4.2 Example on the U.K. Economy: Basic Data and Their
Descriptions .......................................................... 115
4.3 Maximum Likelihood Estimators for the Model.................... 115
4.4 Computational Algorithms for Efficient Estimators ................ 121
4.5 Alternative Estimators and Their Properties ........................ 123
4.5.1 A Comparison Between Efficient and Other Estimators.. 125
4.6 Identification of Rank of the Regression Coefficient Matrix ....... 127
4.7 Inference for the Numerical Example ............................... 128
4.8 An Alternate Estimator with Kronecker Approximation ........... 131
4.8.1 Computational Results ..................................... 132
5 Multiple Time Series Modeling With Reduced Ranks ................. 135
5.1 Introduction and Time Series Models ............................... 135
5.2 Reduced-Rank Autoregressive Models.............................. 138
5.2.1 Estimation and Inference ................................... 138
5.2.2 Relationship to Canonical Analysis of Box and Tiao ..... 140
5.3 An Extended Reduced-Rank Autoregressive Model ............... 141
5.4 Nested Reduced-Rank Autoregressive Models ..................... 144
5.4.1 Specification of Ranks...................................... 144
5.4.2 A Canonical Form .......................................... 145
5.4.3 Maximum Likelihood Estimation.......................... 146
5.5 Numerical Example: U.S. Hog Data ................................ 148
5.6 Relationship Between Nonstationarity and Canonical Correlations 157
5.7 Cointegration for Nonstationary Series—Reduced Rank
in Long Term ......................................................... 161
5.7.1 LS and ML Estimation and Inference ..................... 164
5.7.2 Likelihood Ratio Test for the Number of
Cointegrating Relations .................................... 169Contents xvii
5.8 Unit Root and Cointegration Aspects for the U.S. Hog
Data Example......................................................... 170
6 The Growth Curve Model and Reduced-Rank Regression
Methods ..................................................................... 177
6.1 Introduction and the Growth Curve Model ......................... 177
6.2 Estimation of Parameters in the Growth Curve Model ............. 179
6.3 Likelihood Ratio Testing of Linear Hypotheses in Growth
Curve Model.......................................................... 184
6.4 An Extended Model for Growth Curve Data........................ 188
6.5 Modification of Basic Growth Curve Model to
Reduced-Rank Model ................................................ 191
6.6 Reduced-Rank Growth Curve Models .............................. 194
6.6.1 Extensions of the Reduced-Rank Growth Curve Model .. 199
6.7 Application to One-way ANOVA and Linear
Discriminant Analysis................................................ 202
6.8 A Numerical Example ............................................... 205
6.9 Some Recent Developments ......................................... 210
7 Seemingly Unrelated Regressions Models With Reduced Ranks ..... 213
7.1 Introduction and the Seemingly Unrelated Regressions Model .... 213
7.2 Relation of Growth Curve Model to the Seemingly
Unrelated Regressions Model ....................................... 216
7.3 Reduced-Rank Coefficient in Seemingly Unrelated
Regressions Model ................................................... 217
7.4 Maximum Likelihood Estimators for Reduced-Rank Model ...... 219
7.5 An Alternate Estimator and Its Properties .......................... 222
7.6 Identification of Rank of the Regression Coefficient Matrix ....... 225
7.7 A Numerical Illustration with Scanner Data ........................ 227
7.8 Some Recent Developments ......................................... 235
8 Applications of Reduced-Rank Regression in Financial
Economics ................................................................... 239
8.1 Introduction to Asset Pricing Models ............................... 239
8.2 Estimation and Testing in the Asset Pricing Model ................ 242
8.3 Additional Applications of Reduced-Rank Regression in Finance 248
8.4 Empirical Studies and Results on Asset Pricing Models ........... 249
8.5 Related Topics........................................................ 251
8.6 An Application ....................................................... 254
8.7 Cointegration and Pairs Trading ..................................... 256
9 Partially Reduced-Rank Regression with Grouped
Responses ................................................................... 259
9.1 Introduction: Partially Reduced-Rank Regression Model .......... 260
9.2 Estimation of Parameters ............................................ 261
9.3 Test for Rank and Inference Results................................. 264
9.4 Procedures for Identification of Subset Reduced-Rank Structure.. 266
9.5 Illustrative Examples................................................. 268
9.6 Discussion and Extensions........................................... 275xviii Contents
10 High-Dimensional Reduced-Rank Regression .......................... 279
10.1 Introduction........................................................... 279
10.2 Overview of High-Dimensional Regularized Regression .......... 280
10.3 Framework of Singular Value Regularization....................... 285
10.4 Reduced-Rank Regression via Adaptive Nuclear-Norm
Penalization........................................................... 289
10.4.1 Adaptive Nuclear Norm .................................... 290
10.4.2 Adaptive Nuclear-Norm Penalized Regression ........... 292
10.4.3 Theoretical Analysis........................................ 294
10.5 Integrative Reduced-Rank Regression: Bridging Sparse
and Low-Rank Models............................................... 298
10.5.1 Composite Nuclear-Norm Penalization ................... 300
10.5.2 Theoretical Analysis........................................ 301
10.6 Applications .......................................................... 305
10.6.1 Breast Cancer Data ......................................... 305
10.6.2 Longitudinal Studies of Aging ............................. 307
11 Unbiased Risk Estimation in Reduced-Rank Regression .............. 311
11.1 Introduction........................................................... 311
11.2 Degrees of Freedom .................................................. 312
11.3 Degrees of Freedom of Reduced-Rank Estimation ................. 314
11.4 Comparing Empirical and Exact Estimators ........................ 319
11.4.1 Simulation Setup ........................................... 319
11.4.2 Comparing Estimators of the Degrees of Freedom ....... 320
11.4.3 Performance on Estimating the Prediction Error.......... 322
11.4.4 Performance on Model Selection .......................... 323
11.5 Applications .......................................................... 325
11.5.1 Norwegian Paper Quality Data............................. 325
11.5.2 Arabidopsis Thaliana Data ................................. 326
12 Generalized Reduced-Rank Regression.................................. 329
12.1 Introduction........................................................... 329
12.2 Robust Reduced-Rank Regression .................................. 330
12.2.1 Non-Robustness of Reduced-Rank Regression ........... 330
12.2.2 Robustification with Sparse Mean Shift ................... 333
12.2.3 Theoretical Analysis........................................ 337
12.3 Reduced-Rank Estimation with Incomplete Data................... 340
12.3.1 Noiseless Matrix Completion .............................. 341
12.3.2 Stable Matrix Completion .................................. 344
12.3.3 Computation ................................................ 345
12.4 Generalized Reduced-Rank Regression with Mixed Outcomes ... 347
12.5 Applications .......................................................... 351
12.5.1 Arabidopsis Thaliana data.................................. 351
12.5.2 Longitudinal Studies of Aging ............................. 354Contents xix
13 Sparse Reduced-Rank Regression ........................................ 357
13.1 Introduction........................................................... 357
13.2 Sparse Reduced-Rank Regression ................................... 358
13.2.1 Sparse Reduced-Rank Regression for Predictor Selection 358
13.2.2 Computation ................................................ 361
13.2.3 Theoretical Analysis........................................ 362
13.3 Co-sparse Factor Regression......................................... 363
13.3.1 Model Formulation and Deflation Procedures ............ 363
13.3.2 Co-sparse Unit-Rank Estimation........................... 367
13.3.3 Theoretical Analysis........................................ 368
13.4 Applications .......................................................... 371
13.4.1 Yeast eQTL Mapping Analysis ............................ 371
13.4.2 Forecasting Macroeconomic and Financial Indices....... 374
Appendix .......................................................................... 379
References......................................................................... 385
Subject Index ..................................................................... 403
Reference Index .................................................................. 407About the Authors
Gregory C. Reinsel (now deceased) was Professor of Statistics at the University of
Wisconsin, Madison. He was a fellow of the American Statistical Association. He
is also author of the book Elements of Multivariate Time Series Analysis, Second
Edition, and coauthor, with G.E.P. Box and G.M. Jenkins, of the book Time Series
Analysis: Forecasting and Control, Third Edition.
Greg will remain the first author, in our gratitude.
Raja Velu taught business analytics and finance at Syracuse University. The first
version of the book was mainly based on his thesis written under the supervision
of Professor Reinsel and Professor Dean Wichern. He works in the big data models
area with interest in high-dimensional time series and forecasting applications. His
book, Algorithmic Trading and Quantitative Strategies, co-authored with practition￾ers from CITI and JP Morgan Chase, is published by Taylor and Francis. He has
also served in the tech industry with companies such as Yahoo!, IBM-Almaded, and
Microsoft Research. He was recently (2021–2022) a visiting researcher at Google
working with the Resource Efficiency Data Science team.
Kun Chen is an associate professor in the Department of Statistics at the University
of Connecticut. He is a Fellow of the American Statistical Association and an
Elected Member of the International Statistical Institute. The first version of the
book has had profound influence on his research since his PhD study at the
University of Iowa under the supervision of Professor Kung-Sik Chan. In recent
years, he has been pursuing a comprehensive investigation of theory, methodologies,
and computational approaches of reduced-rank modeling in large-scale statistical
learning. His related work has resulted in many publications in statistics, machine
learning, and scientific journals and the developed methods have been applied to
tackle consequential problems in various fields including public health, ecology,
and biological sciences.
xxiChapter 1
Multivariate Linear Regression
1.1 Introduction
Regression methods are perhaps the most widely used statistical tools in data
analysis. When several response variables are studied simultaneously, we are in
the sphere of multivariate regression. The usual description of the multivariate
regression model, which relates the set of m multiple responses to a set of n predictor
variables, assumes implicitly that the m × n regression coefficient matrix is of full
rank. It can then be demonstrated that the simultaneous estimation of the elements of
the coefficient matrix, by least squares or maximum likelihood estimation methods,
yields the same results as a set of m multiple regressions, where each of the m
individual response variables is regressed separately on the predictor variables.
Hence, the fact that the multiple responses are likely to be related is not involved in
the estimation of the regression coefficients as no information about the correlations
among the response variables is taken into account. Any new knowledge gained by
recognizing the multivariate nature of the problem and the fact that the responses
are related is not incorporated when estimating the regression parameters jointly.
There are two practical concerns regarding this general multivariate regression
model. First, the accurate estimation of all the regression coefficients may require
a relatively large number of observations, which might involve some practical
limitations. Second, even if the data are available, interpreting simultaneously
a large number of the regression coefficients can become unwieldy. Achieving
parsimony in the number of unknown parameters may be desirable both from the
estimation and interpretation points of view of multivariate regression analysis. We
will address these concerns by recognizing a big data feature that enters when
modeling via multivariate regression.
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8_1
12 1 Multivariate Linear Regression
The special feature that can enter the multivariate linear regression model is that
we admit the possibility that the rank of the regression coefficient matrix can be
deficient. This implies that there are linear restrictions on the coefficient matrix,
and these restrictions themselves are often not known a priori. Alternately, the
coefficient matrix can be written as a product of two component matrices of lower
dimension. Such a model is called a reduced-rank regression model, and the model
structure and estimation method for this model will be described in detail later.
It follows from this estimation method that the assumption of lower rank for the
regression coefficient matrix leads to estimation results which take into account the
interrelations among the multiple responses and which use the entire set of predictor
variables in a systematic fashion, compared to some ad hoc procedures which may
use only some portion of the predictor variables in an attempt to achieve parsimony.
The abovementioned features both have practical implications. When we model
with a large number of response and predictor variables, the implication in terms
of restrictions serves a useful purpose. Certain linear combinations of response
variables can, eventually, be ignored for the regression modeling purposes, since
these combinations will be found to be unrelated to the predictor variables. The
alternate implication indicates that only certain linear combinations of the predictor
variables need to be used in the regression model, since any remaining linear
combinations will be found to have no influence on the response variables given
the first set of linear combinations. Thus, the reduced-rank regression procedure
takes care of the dimension reduction aspect of multivariate regression model
building through the assumption of lower rank for the regression coefficient matrix.
This feature of reduced-rank regression and its variations are particularly useful in
modeling big data.
In the case of multivariate linear regression, where the dependence of response
variables among each other is utilized through the assumption of lower rank, the
connection between the regression coefficients and the descriptive tools of principal
components and canonical variates can be demonstrated. There is a correspondence
between the rank of the regression coefficient matrix and a specified number
of principal components or canonical correlations. Therefore, the use of these
descriptive tools in multivariate model building is explicitly demonstrated through
the reduced-rank assumption. Before we proceed with the consideration of various
aspects of reduced-rank models in detail in subsequent chapters, we present some
basic results concerning the classical full-rank multivariate linear regression model
in the remainder of this chapter. More detailed and in-depth presentations of results
for the full-rank multivariate linear regression model may be found in the books by
Anderson (1984b, Chap. 8), Izenman (2008, Chap. 6), Muirhead (1982, Chap. 10),
and Srivastava and Khatri (1979, Chaps. 5 and 6), among others.1.2 Multivariate Linear Regression Model and Least Squares Estimator 3
1.2 Multivariate Linear Regression Model and Least Squares
Estimator
We consider the general multivariate linear model,
Yk = CXk + k, k = 1,...,T, (1.1)
where Yk = (y1k,...,ymk) is an m × 1 vector of response variables, Xk =
(x1k,...,xnk) is an n × 1 vector of predictor variables, C is an m × n regression
coefficient matrix, and k = (1k,...,mk) is the m × 1 vector of random errors,
with mean vector E(k) = 0 and covariance matrix Cov(k) =  , an m × m
positive-definite matrix. The k are assumed to be independent for different k. We
shall assume that T vector observations are available and define the m×T and n×T
data matrices, respectively, as
Y = [Y1,...,YT ] and X = [X1,...,XT ]. (1.2)
To begin with, we assume that m + n ≤ T and that X is of full rank n = rank(X) <
T . This last condition is required to obtain a unique (least squares) solution to the
first order equations. These conditions will be relaxed in later chapters. We arrange
the error vectors k, k = 1,...,T , into an m×T matrix  = [1, 2,...,T ], similar
to the data matrices given by (1.2). We will also consider the errors arranged into an
mT × 1 vector e = (
(1)
,..., 
(m))
, where (j ) = (j1,...,j T ) is the vector of
errors corresponding to the j th response variable in the regression equation (1.1).
Then we have that
E(e) = 0, Cov(e) =  ⊗ IT , (1.3)
where A ⊗ B denotes the Kronecker product of two matrices A and B. That is, if A
is an m × n matrix and B is a p × q matrix, then the Kronecker product A ⊗ B is
the mp × nq matrix with block elements A ⊗ B = (aijB), where aij is the (i, j )th
element of A.
The unknown parameters in model (1.1) are the elements of the regression
coefficient matrix C and the error covariance matrix  . These can be estimated
by the method of least squares, or equivalently, the method of maximum likelihood
under the assumption of normality of the k. Before we present the details on the
estimation method, we rewrite the model (1.1) in terms of the complete data matrices
Y and X as
Y = CX + . (1.4)
The least squares criterion is given as
e
e = tr(
) = tr[(Y − CX)(Y − CX)

] = tr T
k=1
k 
k

, (1.5)4 1 Multivariate Linear Regression
where tr(A) denotes the trace of a square matrix A, that is, the sum of the diagonal
elements of A. The least squares (LS) estimate of C is the value C
 that minimizes
the criterion in (1.5). Using results on matrix derivatives (e.g., Rao 1973, p. 72),
in particular ∂tr(CZ)/∂C = Z
, we have the first order equations to obtain a
minimum as
∂tr(
)
∂C = 2[−YX + C(XX
)] = 0, (1.6)
which yields a unique solution for C as
C
 = YX
(XX
)
−1 =
 1
T
YX
  1
T
XX
−1
. (1.7)
If we let C
(j ) denote the j th row of the matrix C, it is directly seen from (1.7) that
the corresponding j th row of the LS estimate C
 is
C

(j ) = Y
(j )X
(XX
)
−1 =
 1
T
Y
(j )X
  1
T
XX
−1
, (1.8)
for j = 1,...,m, where Y
(j ) = (yj1,...,yj T ) denotes the j th row of the matrix
Y, that is, Y(j ) is the T ×1 vector of observations on the j th response variable. Thus,
it is noted here that Y
(j )X = T
k=1 yjkX
k refers to the cross products between the
j th response variable and the n predictor variables, summed over all T observations.
Hence, the rows of the matrix C are estimated in (1.7) by the least squares regression
of each response variable on the predictor variables and therefore the covariances
among the response variables do not enter into the estimation.
From the presentation of model (1.1), it follows that our focus here is on
the regression coefficient matrix C of the response variables Y on the predictor
variables X; the intercept term typically will not be explicitly represented in the
model. With a constant term explicitly included, the model would take the form
Yk = d + CXk + k, where d is an m × 1 vector of unknown parameters. For
convenience, in such a model, it will be assumed that the predictor variables have
been centered by subtraction of appropriate sample means, so that the resulting
data matrix X has zero means, i.e., X¯ = 1
T
T
k=1 Xk = 0. These mean-corrected
observations would typically be the basis of all the calculations that follow. Under
this setup, the basic results concerning the LS estimate of C would continue to
hold, while the LS estimate of the intercept term is simply d
 = Y¯ = 1
T
T
k=1 Yk.
For practical applications, an intercept is almost always needed in modeling, and in
terms of estimation of C and  the inclusion of a constant term is equivalent to
performing calculations with response variables Yk that have (also) been corrected
for the sample mean Y¯.
The maximum likelihood method of estimation, under the assumption that values
of the predictor variables Xk are known constant vectors and the assumption of1.3 Further Inference Properties in the Multivariate Regression Model 5
normality of the error terms k in addition to the properties given in (1.3), yields the
same results as least squares. The likelihood is equal to
f () = (2π )−mT /2 | |
−T /2 exp 	
−(1/2)tr 

−1
 
 , (1.9)
where  = Y − CX. For maximum likelihood (ML) estimation, the criterion that
must be minimized to estimate C is seen from (1.9) to be
tr 
−1 

= tr 	
−1/2  (Y − CX)(Y − CX)
 −1/2  
, (1.10)
which can also be expressed as e
(−1  ⊗IT )e. From (1.10), the first order equations
to obtain a minimum are ∂tr(−1  
)/∂C = 2−1  [−YX + C(XX
)] = 0, which
yields the same solution for C as given in (1.7). In the later chapters, we shall
consider a more general criterion,
tr(
) = tr[1/2(Y − CX)(Y − CX)

1/2], (1.11)
where  is a positive-definite matrix. The criteria (1.5) and (1.10) follow as special
cases of (1.11) by appropriate choices of the matrix . The ML estimator of the error
covariance matrix  is obtained from the least squares residuals  = Y − C
X as
 = 1
T (Y−C
X)(Y−C
X) ≡ 1
T S, where S = (Y−C
X)(Y−C
X) = T
k=1k
k
is the error sum of squares matrix, and k = Yk − CX k, k = 1,...,T , are the least
squares residual vectors. An unbiased estimator of  is obtained as
 = 1
T − n  = 1
T − n

T
k=1
k
k. (1.12)
1.3 Further Inference Properties in the Multivariate
Regression Model
Under the assumption that the matrix of predictor variables X is fixed and
nonstochastic, properties of the least squares estimator C
 in (1.7) can readily be
derived. In practice, the Xk in (1.1) may consist of observations on a stochastic
vector, in which case the (finite sample) distributional results to be presented for the
LS estimator C
 can be viewed as conditional on fixed observed values of the Xk.
First, C
 is an unbiased estimator of C since from (1.4)
E(C)  = E[YX
(XX
)
−1] = E[Y]X
(XX
)
−1 = CXX
(XX
)
−1 = C.
From (1.8), we find that
Cov(C
(i),C
(j )) = (XX
)
−1X Cov(Y(i), Y(j )) X
(XX
)
−1 = σij (XX
)
−16 1 Multivariate Linear Regression
for i, j = 1,...,m, where σij denotes the (i, j )th element of the covariance
matrix  , since Cov(Y(i), Y(j )) = σij IT . The distributional properties of C

follow easily from multivariate normality of the error terms k. Specifically, we
consider the distribution of vec(C

), where the “vec” operator transforms an n × m
matrix into an nm-dimensional column vector by stacking the columns of the matrix
below each other. Thus, for an m × n matrix C
, vec(C

) = (C

(1)
,...,C

(m))
,
where C

(j ) refers to the j th row of the matrix C
. A useful property that relates
the vec operation with the Kronecker product of matrices is that if A, B, and C
are matrices of appropriate dimensions so that the product ABC is defined, then
vec(ABC) = (C ⊗ A)vec(B) [see Neudecker (1969)]. Another useful property is
tr(ABCB
) = {vec(B)}
(C ⊗ A) vec(B).
Now notice that the model (1.4) may be expressed in the vector form as
y = vec(Y
) = vec(X
C
) + vec(
) = (Im ⊗ X
)vec(C
) + e, (1.13)
where y = vec(Y
) = (Y
(1)
,..., Y
(m))
, and e = vec(
) = (
(1)
,..., 
(m)) is
similar in form to y with Cov(e) =  ⊗IT . In addition, the least squares estimator
C
 in (1.7) can be written in vector form as
vec(C

) = vec
(XX
)
−1XY

= (Im ⊗ (XX
)
−1X) vec(Y
), (1.14)
where vec(Y
) is distributed as multivariate normal with mean vector (Im ⊗
X
)vec(C
) and covariance matrix  ⊗ IT . It then follows directly that vec(C

)
is also distributed as multivariate normal, with
E[vec(C

)] = (Im ⊗ (XX
)
−1X) E[vec(Y
)] = vec(C
),
and
Cov[vec(C

)] = (Im ⊗ (XX
)
−1X)Cov[vec(Y
)](Im ⊗ X
(XX
)
−1)
=  ⊗ (XX
)
−1.
We summarize the main results on the LS estimate C
 in the following.
Result 1.1 For the model (1.1) under the normality assumption on the k, the
least squares estimator C
 = YX
(XX
)−1 is the same as the maximum likelihood
estimator of C, and the distribution of the least squares estimator C
 is such that
vec(C

) ∼ N
vec(C
),  ⊗ (XX
)
−1
. (1.15)
Note, in particular, that this result implies that the j th row of C
, C

(j ), which is
the vector of least squares estimates of regression coefficients for the j th response
variable, has the distribution C
(j ) ∼ N (C(j ), σjj (XX
)−1).
The inference on the elements of the matrix C can be made using the result in
(1.15). In practice, because  is unknown, a reasonable estimator such as the1.3 Further Inference Properties in the Multivariate Regression Model 7
unbiased estimator in (1.12) is substituted for the covariance matrix in (1.15). All
the necessary calculations for the above linear least squares inference procedures
can be carried out using standard computer software, such as Statistical Analysis
System (SAS) or using programs based on the “R” language such as “rrpack.”
We shall also indicate the likelihood ratio procedure for testing certain simple
linear hypotheses regarding the regression coefficient matrix. Consider X partitioned
as X = [X
1, X
2]
 and corresponding C = [C1, C2], so that the model (1.4) is
written as Y = C1X1 + C2X2 + , where C1 is m × n1 and C2 is m × n2 with
n1 + n2 = n. It may be of interest to test the null hypothesis H0 : C2 = 0 against
the alternative C2 = 0. The null hypothesis implies that the predictor variables
X2 do not have any (additional) influence on the response variables Y, given the
impact of the variables X1. Using the likelihood ratio (LR) testing approach, it is
easy to see from (1.9) that the LR test statistic is λ = UT /2, where U = |S|/|S1|,
S = (Y − C
X)(Y − C
X) and S1 = (Y − C
1X1)(Y − C
1X1)
. The matrix S is the
residual sum of squares matrix from maximum likelihood (i.e., least squares) fitting
of the full model, while S1 is the residual sum of squares matrix obtained from
fitting the reduced model with C2 = 0 and C
1 = YX
1(X1X
1)−1. This LR statistic
result follows directly by noting that the value of the multivariate normal likelihood
function evaluated at the maximum likelihood estimates C
 and  = (1/T )S is
equal to a constant times |S|
−T /2, because tr(−1  
) = tr(T −1   ) = T m is
a constant when evaluated at the ML estimate. Thus, the likelihood ratio for testing
H0 : C2 = 0 is λ = |S1|
−T /2/|S|
−T /2 = UT /2. It has been shown (see Anderson
(1984b, Chapter 8)) that for moderate and large sample size T , the test statistic
M = −[T − n + (n2 − m − 1)/2] log(U ) (1.16)
is approximately distributed as chi-squared with n2m degrees of freedom (χ2
n2m),
and the hypothesis is rejected when M is greater than a constant determined by the
χ2
n2m distribution.
It is useful to express the LR statistic λ = UT /2 in an alternate equivalent form.
Write XX = A = [(Aij )], where Aij = XiX
j , i, j = 1, 2, and let A22.1 = A22 −
A21A−1
11 A12. Then the LS estimate of C1 under the reduced model with C2 = 0 can
be expressed as C
1 = YX
1(X1X
1)−1 = C
1 + C
2A21A−1
11 because of the full model
LS normal equations (1.6), YX
1 = C
1X1X
1 + C
2X2X
1. Hence,
Y − C
1X1 = (Y − C
X) + C
2 (X2 − A21A−1
11 X1),
where the two terms on the right are orthogonal since (Y − C
X)X = 0 because C

is a solution to the first order equations (1.6). It thus follows that
S1 = (Y − C
1X1)(Y − C
1X1)

= (Y − C
X)(Y − C
X)
 + C
2(A22 − A21A−1
11 A12)C

2
= S + C
2 A22.1 C

2, (1.17)8 1 Multivariate Linear Regression
and also note that A−1
22.1 is the n2 × n2 lower diagonal block of the matrix (XX
)−1
(e.g., see Rao (1973, p. 33)). Hence, we see that
U = |S|/|S1|=|S|/|S + H| = 1/|I + S−1H|,
where H = C
2A22.1C

2 is referred to as the hypothesis sum of squares matrix

(associated with the null hypothesis C2 = 0). Therefore, we have − log(U ) = m
i=1 log(1 + λ2
i ), where λ2
i are the eigenvalues of S−1H, and the LR statistic
M in (1.16) is directly expressible in terms of these eigenvalues. In addition, it
can be shown (e.g., see Reinsel (1997, Section A4.3)) that the λ2
i are related to
the sample partial canonical correlations ρi between Y and X2, eliminating X1, as
λ2
i = ρ2
i /(1 − ρ2
i ), i = 1,...,m. We summarize the above in the following result.
Result 1.2 For the model (1.1) under the normality assumption on the k, the
likelihood ratio for testing H0 : C2 = 0 is λ = UT /2, where U = |S|/|S1| =
1/|I + S−1H| with H = C
2A22.1C

2. Therefore, − log(U ) = m
i=1 log(1 + λ2
i ),
where λ2
i are the eigenvalues of S−1H. For large sample size T , the asymptotic
distribution of the test statistic M = −[T − n + (n2 − m − 1)/2] log(U ) is chi￾squared with n2m degrees of freedom.
Known Restrictions: Finally, we briefly consider testing the more general null
hypothesis of the form H0 : F1CG2 = 0, where F1 and G2 are known full￾rank matrices of appropriate dimensions m1 × m and n × n2, respectively. In
the hypothesis F1CG2 = 0, the matrix G2 allows for restrictions among the
coefficients in C across (subsets of) the predictor variables, whereas F1 provides
for restrictions in the coefficients of C across the different response variables. For
instance, the hypothesis C2 = 0 considered earlier is represented in this form with
G2 = [0
, I 
n2 ]
 and F1 = Im, and the hypothesis of the form F1CG2 = 0 with
the same matrix G2 and F1 = [Im1 , 0] would postulate that the predictor variables
X2 do not enter into the linear regression relationship only for the first m1 < m
components of the response variables Y. Using the LR approach, the likelihood ratio
is given by λ = UT /2, where U = |S|/|S1| and S1 = (Y − C
X)(Y − C
X) with C

denoting the ML estimator of C obtained subject to the constraint that F1CG2 = 0.
In fact, using Lagrange multiplier methods, it can be shown that the constrained ML
estimator under F1CG2 = 0 can be expressed as
C
 = C
 − SF
1(F1SF
1)
−1F1CG 2[G
2(XX
)
−1G2]
−1G
2(XX
)
−1. (1.18)
An alternate way to develop this last result is to consider matrices F = [F
1, F
2]

and G = [G1, G2] such that F1F
2 = 0 and G
1G2 = 0 (for convenience) and apply
the transformation to the model (1.4) as Y∗ = FY = F CGG−1X + F ≡ C∗X∗ +
∗, where C∗ = FCG, Y∗ = FY, and X∗ = G−1X. With C∗ partitioned into
submatrices C∗
ij , i, j = 1, 2, the hypothesis F1CG2 = 0 is equivalent to C∗
12 = 0.
Maximum likelihood estimation of the parameters C∗ in this transformed “canonical
form” model, subject to the restriction C∗
12 = 0, and transformation of the resulting1.3 Further Inference Properties in the Multivariate Regression Model 9
ML estimate C
∗ back to the parameters C of the original model as C
 = F −1C
∗G−1
then lead to the result in (1.18). Using either method, from (1.18), we therefore have
Y − C
X = (Y − C
X)
+ SF
1(F1SF
1)
−1F1CG 2[G
2(XX
)
−1G2]
−1G
2(XX
)
−1X,
where the two terms on the right are orthogonal. Following the above developments
as in (1.17), we thus readily find that
S1 = S + SF
1(F1SF
1)
−1(F1CG 2)[G
2(XX
)
−1G2]
−1
× (F1CG 2)

(F1SF
1)
−1F1S ≡ S + H, (1.19)
and U = 1/|I + S−1H| so that the LR test can be based on the eigenvalues of
S−1H.
The nonzero eigenvalues of the m × m matrix S−1H are the same as those
of the m1 × m1 matrix S−1 ∗ H∗, where S∗ = F1SF
1 and H∗ = F1H F
1 =
F1CG 2[G
2(XX
)−1G2]
−1(F1CG 2)
, so that U−1 = |I +S−1 ∗ H∗| = m1
i=1(1+λ2
i ),
where the λ2
i are the eigenvalues of S−1 ∗ H∗. Analogous to (1.16), the test statistic
used is M = −[T − n + 1
2 (n2 − m1 − 1)] log(U ), which has an approximate
χ2
n2m1 distribution under H0. Somewhat more accurate approximation of the null
distribution of log(U ) is available based on the F-distribution, and in some cases
(notably when n2 = 1 or 2 or m1 = 1 or 2), exact F-distribution results for U are
available (see Anderson (1984b, Chap. 8)). Values relating to the (exact) distribution
of the statistic U for various values of m1, n2, and T − n have been tabulated by
Schatzoff (1966), Pillai and Gupta (1969), and Lee (1972). However, for most of
our applications, we will simply use the large sample χ2
n2m1 approximation for the
distribution of the LR statistic M.
Concerning the exact distribution theory associated with the test procedure, under
normality of the k, it is established (e.g., Srivastava and Khatri (1979, Chap. 6)) that
S∗ has the W (F1F
1, T − n) distribution, the Wishart distribution with matrix
F1F
1 and T − n degrees of freedom, and H∗ is distributed as Wishart under
H0 with matrix F1F
1 and n2 degrees of freedom. The sum of squares matrices
S∗ and H∗ is also independently distributed, which follows from the well-known
fact that the residual sum of squares matrix S and the least squares estimator C
 are
independent, and H∗ is a function of C
 alone. Thus, in particular, in the special
case m1 = 1, we see that S−1 ∗ H∗ = H∗/S∗ is distributed as n2/(T − n) times the
F-distribution with n2 and T − n degrees of freedom.
Tests other than the LR procedure may also be considered for the multivariate
linear hypothesis F1CG2 = 0. The most notable of these is the test developed by
Roy (1953) based on the union-intersection principle of test construction. Roy’s test
rejects for large values of the test statistic which is equal to the largest eigenvalue of10 1 Multivariate Linear Regression
the matrix S−1 ∗ H∗. Charts and tables of the upper critical values for the largest root
distribution have been computed by Heck (1960) and Pillai (1967).
Notice that the above hypotheses concerning C can be viewed as an assumption
that the coefficient matrix C has a reduced rank, but of a particular known or
specified form (i.e., the matrices F1 and G2 in the constraint F1CG2 = 0 are
known). In the next chapter, we consider further investigating the possibility of a
reduced-rank structure for the regression coefficient matrix C, but where the form of
the reduced rank is not known. In particular, in such cases of reduced-rank structure
for C, linear constraints of the form F1C = 0 will hold but with the m1 × m matrix
F1 unknown (m1 < m). Notice from (1.18) that the appropriate ML estimator of C
under (known) constraints on C of such a form is given by
C
 = (Im − SF
1(F1SF
1)
−1F1)C
 ≡ PC,  (1.20)
where P = Im − SF
1(F1SF
1)−1F1 is an idempotent matrix of rank r = m −
m1. In Sections 2.3 and 2.6, we will find that the ML estimator C
 of C subject
to the constraint of having reduced rank r (but with the linear constraints F1C = 0
unknown) can be expressed in a similar form as in (1.20), but in terms of an estimate
F
1 whose rows are determined from examination of the eigenvectors of the matrix
S−1/2C(  XX
)C

S−1/2 associated with its m1 smallest eigenvalues.
Similarly, the constraints across the predictor variables, CG2 = 0, where the
matrix G2(n × n2) is unknown, have applications in Finance (see Chapter 8). For a
known G2, the ML estimator of C takes the form,
C
 = C
[In − G2(G
2(XX
)
−1G2)
−1G
2(XX
)
−1] ≡ CQ,  (1.21)
where Q = In −G2(G
2(XX
)−1G2)−1G
2(XX
)−1 is an idempotent matrix of rank
n − n2. The ML estimator C
 with reduced rank r can also be expressed in a form
similar to (1.21), but the columns of G2 are determined by the eigenvectors of the
matrix (XX
)1/2C

S−1C(XX  
)1/2 associated with its n2 smallest eigenvalues.
1.4 Prediction in the Multivariate Linear Regression Model
Another problem of interest in regard to the multivariate linear regression model
(1.1) is confidence regions for the mean responses corresponding to a fixed value X0
of the predictor variables. The mean vector of responses at X0 is CX0 and its esti￾mate is CX 0, where C
 = YX
(XX
)−1 is the LS estimator of C. Under the assump￾tion of normality of the k, noting that CX 0 = vec(X
0C

) = (Im ⊗ X
0)vec(C

), we
see that CX 0 is distributed as multivariate normal N (CX0, X
0(XX
)−1X0 ) from
Result 1.1. Since C
 is distributed independently of  = 1
T −n S with (T − n)
distributed as Wishart with matrix  and T −n degrees of freedom, it follows that1.4 Prediction in the Multivariate Linear Regression Model 11
T 2 = (CX 0 − CX0)
 −1
 (CX 0 − CX0)/{X
0(XX
)
−1X0} (1.22)
has Hotelling’s T 2-distribution with T − n degrees of freedom (Anderson 1984b,
Chap. 5). Thus, {(T − n − m + 1)/m(T − n)}T 2 has the F-distribution with m and
T − n − m + 1 degrees of freedom, so that a 100(1 − α)% confidence ellipsoid for
the true mean response CX0 at X0 is given by
(CX 0 − CX0)
 −1
 (CX 0 − CX0)
≤ {X
0(XX
)
−1X0}
 m(T − n)
T − n − m + 1
Fm,T −n−m+1(α)
, (1.23)
where Fm,T −n−m+1(α) is the upper 100αth percentile of the F-distribution with m
and T − n − m + 1 degrees of freedom. Because
(a
CX 0 − a
CX0)
2/(a
a) ≤ (CX 0 − CX0)

−1
 (CX 0 − CX0)
for every nonzero m × 1 vector a, by the Cauchy–Schwarz inequality, 100(1 −
α)% simultaneous confidence intervals for all linear combinations a
CX0 of the
individual mean responses are given by
a
CX 0 ± {(a
a)X
0(XX
)
−1X0}
1/2
 m(T − n)
T − n − m + 1
Fm,T −n−m+1(α)1/2
.
Notice that the mean response vector CX0 of interest represents a particular example
of the general parametric function F1CG2 discussed earlier in the hypothesis testing
context of Section 1.3, corresponding to F1 = Im, G2 = X0, and hence n2 = 1.
A related problem of interest is the prediction of values of a new response vector
Y0 = CX0 + 0 corresponding to the predictor variables X0, where it is assumed
that Y0 is independent of the values Y1,...,YT . The predictor of Y0 is given by
Y
0 = CX 0, and it follows similarly to the above result (1.23) that a 100(1 − α)%
prediction ellipsoid for Y0 is
(Y0 − CX 0)

−1
 (Y0 − CX 0)
≤ {1 + X
0(XX
)
−1X0}
 m(T − n)
T − n − m + 1
Fm,T −n−m+1(α)
. (1.24)
Another prediction problem that may be of interest on occasion is that of prediction
of a subset of the components of Y0 given that the remaining components have been
observed. This problem would also involve the covariance structure  of Y0, but
it will not be discussed explicitly here. Besides the various dimension reduction12 1 Multivariate Linear Regression
interpretations, one additional purpose of investigating the possibility of a reduced￾rank structure for C, as will be undertaken in the next chapter, is to obtain more
precise predictions of future response vectors as a result of more accurate estimation
of the regression coefficient matrix C in model (1.1). The advantage of reduced￾rank models in prediction will become apparent when dealing with a large number
of variables. This will be demonstrated in the next chapter.
1.5 Numerical Examples
In this section we present two examples to illustrate some of the methods and results
described in the earlier sections of this chapter. We consider the basic multivariate
regression analysis methods of the previous sections applied to these examples.
1.5.1 Biochemical Data
We consider biochemical data taken from a study by Smith et al. (1962). The data
involve biochemical measurements on several characteristics of urine specimens
of 17 men classified into two weight groups, overweight and underweight. Each
subject, except for one, contributed two morning samples of urine, and the data
considered in this example consist of the 33 individual samples (treated as indepen￾dent with no provision for the possible correlations within the two determinations
of each subject) for five response variables (m = 5), pigment creatinine (y1), and
concentrations of phosphate (y2), phosphorous (y3), creatinine (y4), and choline
(y5), and the concomitant or predictor variables, volume (x2), and specific gravity
(x3), in addition to the predictor variable for weight of the subject (x1). The
multivariate data set for this example is presented in Table A.1 of the Appendix.
We consider a multivariate linear regression model for the kth vector of responses
of the form
Yk = c0 + c1x1k + c2x2k + c3x3k + k ≡ CXk + k, k = 1,...,T, (1.25)
with T = 33, where Yk = (y1k, y2k, y3k, y4k, y5k) is a 5 × 1 vector, and Xk =
(1, x1k, x2k, x3k) is a 4 × 1 vector (hence n = 4). With Y and X denoting 5 × 33
and 4 × 33 data matrices, respectively, the least squares estimate of the regression
coefficient matrix is obtained as1.5 Numerical Examples 13
C
 = YX
(XX
)
−1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
15.2809 −2.9090 1.9631 0.2043
(4.3020) (1.0712) (0.6902) (1.1690)
1.4159 0.6044 −0.4816 0.2667
(0.8051) (0.2005) (0.1292) (0.2188)
2.0187 0.5768 −0.4245 −0.0401
(0.6665) (0.1659) (0.1069) (0.1811)
1.8717 0.6160 −0.5781 0.3518
(0.5792) (0.1442) (0.0929) (0.1574)
−0.8902 1.3798 −0.6289 2.8908
(7.0268) (1.7496) (1.1273) (1.9095)
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
The unbiased estimate of the 5 × 5 covariance matrix of the errors k (with
correlations shown above the diagonal) is given by
 = 1
33 − 4  =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
11.2154 −0.4225 −0.3076 −0.3261 −0.0780
−0.8867 0.3928 0.8951 0.6769 0.3418
−0.5344 0.2911 0.2692 0.6605 0.2891
−0.4924 0.1913 0.1545 0.2033 0.0343
−1.4285 1.1718 0.8203 0.0845 29.9219
⎤
⎥
⎥
⎥
⎥
⎥
⎦
,
where  = Y − C
X. The estimated standard deviations of the individual elements
of C
 are obtained as the square roots of the diagonal elements of  ⊗ (XX
)−1,
where
(XX
)
−1 =
⎡
⎢
⎢
⎣
1.6501 −0.0807 −0.1934 −0.3798
−0.0807 0.1023 −0.0146 −0.0243
−0.1934 −0.0146 0.0423 0.0424
−0.3798 −0.0243 0.0424 0.1219
⎤
⎥
⎥
⎦ .
These estimated standard deviations are evaluated and displayed in parentheses
below the corresponding estimates in C
. We find that many of the individual
estimates in C
 are greater than twice their estimated standard deviations.
We illustrate the LR test procedure by testing the hypothesis that the two
concomitant predictor variables x2 and x3 have no influence on the response
variables, that is, H0 : C2 ≡ [c2, c3] = 0. The matrix due to this null hypothesis
is obtained as14 1 Multivariate Linear Regression
H = C
2A22.1C

2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
129.365 −38.712 −28.064 −47.170 −106.563
−38.712 12.469 8.410 15.266 40.140
−28.064 8.410 6.088 10.248 23.225
−47.170 15.266 10.248 18.695 49.587
−106.563 40.140 23.225 49.587 164.779
⎤
⎥
⎥
⎥
⎥
⎥
⎦
,
where
A22.1 =

0.042472 0.042358
0.042358 0.121855 −1
=
 36.0395 −12.5278
−12.5278 12.5613 
and the error matrix is S =  = (33 − 4) . The nonzero eigenvalues of the
matrix S−1H = S−1C
2A22.1C

2 are the same as those of
C

2S−1C
2A22.1 =
 2.5587 −1.1269
−2.1683 1.4912 
and are found to equal λ2
1 = 3.67671 and λ2
2 = 0.37319. Hence, with T = 33,
n = 4, n2 = 2, and m = 5, the value of the test statistic in (1.16) is obtained as
M = [33 − 4 + (2 − 5 − 1)/2][log(1 + 3.6767) + log(1 + 0.3732)] = 50.2126
with n2m = 2 · 5 = 10 degrees of freedom. Since the critical value of the χ2
10
distribution at level α = 0.05 is 18.31, H0 is clearly rejected and the implication
is that the concomitant variables (x2, x3) have influence on (at least some of) the
response variables y1, y2, y3, y4, and y5. From informal examination of the estimates
in C
 and their estimated standard deviations, it is expected that the significance of
this hypothesis testing result is due mainly to the influence of the variable volume
(x2), with the variable x3 having much less contribution. (In fact, the value of the
test statistic in (1.16) for H0 : c3 = 0 yields the value M = 14.6389 with 5 degrees
of freedom and indicates only mild influence for x3 on the response variables.)
Similarly, we can test the hypothesis that the weight variable (x1) has no
influence on the response variables, H0 : c1 = 0. For the LR test of this hypothesis,
since n2 = 1, the single nonzero eigenvalue of S−1H can be obtained as the
scalar c
1S−1
c1/0.10231 = 0.08184/0.10231 = 0.79995, noting that 0.10231
represents the (2, 2)-element of the matrix (XX
)−1. So the value of the test statistic
in (1.16) is M = [33 − 4 + (1 − 5 − 1)/2] log(1 + 0.79995) = 15.5756 with 5
degrees of freedom. The null hypothesis H0 is rejected at the 0.05 level. Because
n2 = 1 for this H0, an “exact” test is available based on the F-distribution,
similar to the form presented in relation to (1.22) with X0 = (0, 1, 0, 0) so that
CX0 = c1, using the statistic 33−4−5+1
5(33−4) c
1−1
 c1/0.10231 = 3.9998 with m = 5
and T −n−m+1 = 33−4−5+1 = 25 degrees of freedom. A similar conclusion
is obtained as with the approximate chi-squared LR test statistic.1.5 Numerical Examples 15
Concluding, we also point out that the last three columns of the LS estimate C

(excluding the column of the constant terms) give some indications of a reduced￾rank structure for the 5 × 3 matrix [c1, c2, c3] of regression coefficients in model
(1.25). For example, notice the similarity of coefficient estimates among the second
through fourth rows of the last three columns of C
. This type of reduced-rank
structure and other forms will be examined formally in the subsequent chapters.
1.5.2 Sales Performance Data
We consider the data on sales staff given in Johnson and Wichern (2008). This data
was also analyzed by Cook and Su (2013) in the context of envelope model fitting.
The performance quality of a firm’s sales staff is measured by growth, profitability,
and new-account sales. These measures are then related to series of tests taken by
the staff for creativity, mechanical reasoning, abstract reasoning, and mathematical
ability. The number of observations is T = 50, the number of response variables,
m = 4, and the number of predictors, n = 3. The data is listed in Table A.2 in the
Appendix.
The multivariate regression model (1.4) is fit for this data. Observe that the
matrix “Y ” is of dimension 4 × 50 and “X” is of dimension 3 × 50; the coefficient
matrix, C, is of dimension 4 × 3. The predictor variables are, x1: sales growth,
x2: sales profitability, and x3: new account sales. The response variables are, y1:
score on creativity test, y2: score on mechanical reasoning test, y3: score on abstract
reasoning test, and y4: score on mathematics test.
The scatterplot of the variables is given in Fig. 1.1. It is interesting to note that all
three predictor variables are highly correlated among themselves and the response
variable, y4, is highly correlated with all predictors. The correlation matrix given
below in Table 1.1 confirms that.
We subtract the mean for each series and the demeaned series form the data
matrices Y and X given in the text. The regression coefficient matrix, C
, in (1.7) is
C
 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
−0.086 −0.027 0.753∗
(0.171) (0.107) (0.186)
0.062 0.213∗ −0.012
(0.138) (0.087) (0.150)
0.428∗ −0.248∗ 0.152
(0.077) (0.048) (0.084)
0.443∗ 0.610∗ 0.193
(0.191) (0.120) (0.208)
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
*Significant at 0.05 significant level.16 1 Multivariate Linear Regression
y1
5 10 15 20 10 30 50 90 105 120
5 10 15
5 10 15 20
r = 0.591
CI = 0.374, 0.746
y2
r = 0.147
CI = −0.137, 0.409
r = 0.386
CI = 0.121, 0.600
y3
6 8 10 12 14
10 20 30 40 50
r = 0.413
CI = 0.152, 0.620
r = 0.575
CI = 0.353, 0.735
r = 0.566
CI = 0.342, 0.730
y4
r = 0.572
CI = 0.349, 0.734
r = 0.708
CI = 0.535, 0.824
r = 0.674
CI = 0.488, 0.802
r = 0.927
CI = 0.875, 0.958
x1
85 95 105
90 100 110 120
r = 0.542
CI = 0.310, 0.712
r = 0.746
CI = 0.590, 0.848
r = 0.465
CI = 0.215, 0.658
r = 0.944
CI = 0.903, 0.968
r = 0.926
CI = 0.873, 0.958
x2
5 10
r = 0.700
CI = 0.524, 0.819
r = 0.637
CI = 0.437, 0.778
6 10 14
r = 0.641
CI = 0.442, 0.780
r = 0.853
CI = 0.753, 0.914
85 95 110
r = 0.884
CI = 0.803, 0.933
r = 0.843
CI = 0.737, 0.908
95 100 110
95 105 115
x3
Fig. 1.1 Scatterplot of sales performance
Table 1.1 Correlations
y1 y2 y3 y4 x1 x2 x3
y2 0.591
y3 0.147 0.386
y4 0.413 0.575 0.566
x1 0.572 0.708 0.674 0.927
x2 0.542 0.746 0.465 0.944 0.926
x3 0.700 0.637 0.641 0.853 0.884 0.843 1
Average 11.220 14.180 10.560 29.760 98.836 106.622 102.810
Std. Dev. 3.950 3.385 2.140 10.538 7.337 10.124 4.712
Although most of the elements of C
 appear to be significant, the matrix C
 could
be of lower rank. The unbiased estimate of the error covariance matrix is given as
Σ =
⎛
⎜
⎜
⎝
8.113 0.387 −0.701 −0.679
2.530 5.273 0.027 −0.693
−2.556 0.079 1.639 0.176
−6.140 −5.054 0.715 10.085
⎞
⎟
⎟
⎠ .
Here the entries above the main diagonal are correlations and the entries below are
covariances. Observe that the residuals in Fig. 1.2 exhibit some high correlations
indicating that some information from there can be useful for modeling the
relationships. This and other aspects will be explored in Chapter 2.1.5 Numerical Examples 17
Res1
−4 0 4
−6 −2 0 2 4
−6 0 4 8
−4 −2 0 2 4
r = 0.387
CI = 0.121, 0.600
Res2
r = −0.701
CI = −0.819, −0.525
r = 0.0268
CI = −0.253, 0.303
Res3
−3 −1 0 1 2 3
−6 −2 0 2 4 6 8
−6 0 4
r = −0.679
CI = −0.805, −0.494
r = −0.693
CI = −0.814, −0.514
−3 0 2
r = 0.176
CI = −0.108, 0.433
Res4
Fig. 1.2 Residual plotsChapter 2
Reduced-Rank Regression Model
2.1 The Basic Reduced-Rank Model and Background
The classical multivariate regression model presented in Chapter 1, as noted before,
does not make direct use of the fact that the response variables are likely to be
correlated. A more serious practical concern is that even for a moderate number of
variables whose interrelationships are to be investigated, the number of parameters
in the regression matrix can be large. For example, in a multivariate analysis of
economic variables (see Example 2.2), Gudmundsson (1977) uses m = 7 response
variables and n = 6 predictor variables, thus totaling 42 regression coefficient
parameters (excluding intercepts) to be estimated, in the classical regression setup.
But the number of vector data points available for estimation is only T = 36;
these are quarterly observations from 1948 to 1956 for the United Kingdom. Thus,
in many practical situations, especially in Big Data settings where the number
of variables can exceed the number of observations, there is a need to reduce
the number of parameters in model (1.1). We approach this problem through the
assumption of lower rank of the matrix C in model (1.1). More formally, in the
model Yk = CXk + k, we assume that
rank(C) = r ≤ min(m, n). (2.1)
The methodology and results that we will present for the multivariate regression
model under this assumption of reduced rank apply equally to both the cases of
dimension m ≤ n and m>n. However, for some of the initial discussion below, we
shall assume m ≤ n<T for convenience of notation.
The rank condition (2.1) has two related practical implications. First, with r<m
it implies that there are (m−r) linear restrictions on the regression coefficient matrix
C of the form
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8_2
1920 2 Reduced-Rank Regression Model

iC = 0, i = 1, 2, . . . , (m − r), (2.2)
and these restrictions themselves are often not known a priori in the sense that
1,...,m−r are unknown. Premultiplying (1.1) by 
i, we have

i Yk = 
i k. (2.3)
The linear combinations, 
iYk, i = 1, 2, . . . , (m − r), could be modeled without
any reference to the predictor variables Xk and depend only on the distribution of
the error term k. Otherwise, these linear combinations can be isolated and can be
investigated separately.
The second implication that is somewhat complementary to (2.2) is that with
assumption (2.1), C can be written as a product of two lower dimensional matrices
that are of full ranks. Specifically, C can be expressed as
C = AB, (2.4)
where A is of dimension m × r and B is of dimension r × n, but both have rank r.
Note that the r columns of the left matrix factor A in (2.4) can be viewed as a basis
for the column space of C, while the r rows of B form a basis for the row space.
The model (1.1) can then be written as
Yk = A(BXk) + k, k = 1,...,T, (2.5)
where BXk is of reduced dimension with only r components. A practical use of
(2.5) is that the r linear combinations of the predictor variables Xk are sufficient to
model the variation in the response variables Yk and there may not be a need for all
n linear combinations or otherwise for all n variables, as would be the case when no
single predictor variable can be discarded from the full-rank analysis. Hence, there
is a gain in simplicity and in interpretation through the reduced-rank regression
modeling, although from a practitioner point of view one would still need to have
measurements on all n predictor variables to perform the reduced-rank analysis
leading to the construction of the smaller number of relevant linear combinations.
In the later chapters (10–13), we discuss sparse methods that will address selecting
or discarding predictor variables in the reduced-rank regression setup.
We shall describe now how the reduced-rank model arises in various contexts
from different disciplines and this will also provide a historical background.
Anderson (1951) was the first to consider in detail the reduced-rank regression
problem, for the case where Xk, the set of predictor variables, is fixed. In an
application to economics, he called Yk the economic variables, but Xk is taken
to be the vector of noneconomic variables that can be manipulated. The linear
combinations 
Yk are called structural relations. These are stable relationships
among the macroeconomic variables that could not be manipulated. Estimating the
restrictions “” is thus important to understand the basic structure of the economy.
Izenman (1975) introduced the term “reduced-rank regression” and examined this2.1 The Basic Reduced-Rank Model and Background 21
model in detail. The reduced-rank regression model and its statistical properties
were examined further by Robinson (1973, 1974), Tso (1981), Davies and Tso
(1982), Zhou (1994), and Geweke (1996), among others.
Subsequent to but separate from the initial work of Anderson (1951), reduced￾rank regression concepts were considered in various contexts and under different
terminologies by several authors, as noted by van der Leeden (1990, Chapter 2–3).
Rao (1964) studied principal components and presented results that can be related
to reduced-rank regression, referring to the most useful linear combinations of
the predictor variables as principal components of instrumental variables. Fortier
(1966) considered reduced-rank modeling, which he referred to as simultaneous
linear prediction modeling, and van den Wollenberg (1977) discussed basically the
same procedure as an alternative to canonical correlation analysis, which is known
as redundancy analysis. The basic results in these works are essentially the same
and are related to a particular version of reduced-rank estimation, which we note
explicitly in Section 2.3. Keller and Wansbeek (1983), starting from an errors-in￾variable model form, demonstrated the formulation of a reduced-rank model both
in terms of linear restrictions on the regression coefficient matrix and in terms of
lower-dimensional component matrices, referred to as the principal relations and the
principal factors specifications, respectively. In the machine learning community,
the reduced-rank model falls in the category of multi-task learning.
An appealing physical interpretation of the reduced-rank model was offered
by Brillinger (1969). This is to regard a situation where the n component vector
Xk represents information which is to be used to send a message Yk having m
components (m ≤ n), but such a message can only be transmitted through r channels
(r ≤ m). Thus, BXk acts as a code and on receipt of the code, form the vector ABXk
by premultiplying the code, which it is hoped, would be as close as possible to the
desired Yk.
Aside from this attractiveness as a statistical technique for parameter reduction,
it is also known that a reduced-rank regression structure has an interpretation in
terms of underlying hypothetical constructs that drive the two sets of variables.
The concept of unobservable ‘latent’ variables has a long history in economics
and the social sciences [see Zellner (1970) and Goldberger (1972)]. These latent
variables are hypothetical constructs and the observable variables may appear as
both effects and causes of these latent variables. The latent models are extensively
used in sociology and in economics. To illustrate, let Yk = AY ∗
k + Uk, where Y ∗
k
is a scalar latent variable, A is a column vector, and Yk is the set of indicators of
the underlying hypothetical construct. Suppose Y ∗
k is determined by a set of causal
variables Xk through Y ∗
k = BXk + Vk, where B is a row vector. By combining the
two models, we have the reduced-form equation connecting the multiple causes to
the multiple indicators (denoted as MIMIC model),
Yk = ABXk + (AVk + Uk) = CXk + k. (2.6)
The above multivariate regression model falls in the reduced-rank framework and
it follows that the coefficient matrix is principally constrained to have rank one.22 2 Reduced-Rank Regression Model
Jöreskog and Goldberger (1975) describe the above model with an application in
sociology. Clearly, this situation can be extended to allow for r > 1 latent variables,
with the resulting model (2.6) having coefficient matrix of reduced rank r.
In the context of modeling macroeconomic time series {Yt}, Sargent and Sims
(1977) discuss the notion of “index” models, which fit into the above framework.
The indexes, X∗
t = BXt , which are smaller in number are constructed from a
large set of time series predictor variables Xt and are found to drive the vector
of observed response variables Yt . They are further interpreted through a theory
of macroeconomics. More recent developments of reduced-rank models applied to
vector autoregressive time series analysis, for example, where the predictor variables
Xt in (2.5) represent lagged values Yt−1 of the time series Yt , have been pursued by
Reinsel (1983), Velu, Reinsel and Wichern (1986), Ahn and Reinsel (1988, 1990),
and Johansen (1988, 1991), among others. In the area of financial economics, the
reduced-rank regression models naturally arise from a priori economic theory (see
Chapter 8; Velu and Zhou (1999)).
Models closely related to the reduced-rank regression structure in (2.5) have been
studied and applied in various other areas. For example, in the work preceding that
of Anderson’s, Tintner (1945, 1950) and Geary (1948) studied a similar problem
within an economic setting in which the (unknown) “systematic parts” Wk of the
Yk’s are not necessarily specified to be linear functions of a known set of predictor
variables Xk but were still assumed to satisfy a set of m − r linear relationships

iWk = 0. That is, the model considered was
Yk − μ = Wk + k, (2.7)
where Wk is unobservable and is taken to be the systematic part and k is the random
error. It is assumed that the systematic part {Wk} varies in a lower-dimensional linear
space of dimension r (< m), so that the Wk satisfy LWk = 0 for all k (assuming the
Wk are “centered” at zero) with L denoting an unknown (m−r)×m full-rank matrix
having rows 
i as in (2.3). Typically, analysis and estimation of the parameters L
in such a model require some special assumptions on the covariance structure 
of the error part k, such as  known or  = σ20, where σ2 is an unknown
scalar parameter, but 0 is a known m × m matrix. This problem falls within the
framework of models for functional relationships or structural relationships in the
systematic part {Wk}, that is, the relationships are represented by LWk = 0. The
terminology “functional” is commonly used when the Wk are taken to be fixed and
“structural” when the Wk are treated as random.
The above model can also be considered from the multivariate errors-in-variables
regression model viewpoint. To give a more explicit representation in this form,
write the (m − r) × m full-rank matrix L as L = [L1, L2], where L1 is (m − r) ×
(m − r) and assumed to be of full rank, and partition Yk, Wk, and k in a compatible
fashion as Yk = (Y 
1k, Y 
2k)
, Wk = (W
1k, W
2k)
, and k = (
1k, 
2k)
. Then we can
multiply the linear relation LWk = L1W1k + L2W2k = 0 on the left by L−1
1 to
obtain2.1 The Basic Reduced-Rank Model and Background 23
W1k = −L−1
1 L2W2k ≡ K2W2k,
where K2 = −L−1
1 L2. So we have the multivariate regression relation
Y1k − μ1 = W1k + 1k ≡ K2W2k + 1k, (2.8)
where the “independent” variables W2k in the regression model are not directly
observed but are observed with error, in that Y2k − μ2 = W2k + 2k is observed.
Analysis and estimation of the above models (2.7) and (2.8) have been studied
and described by many authors including Sprent (1966), Moran (1971), Gleser and
Watson (1973), Gleser (1981), Theobald (1975), Anderson (1976, 1984a), Bhargava
(1979), and Amemiya and Fuller (1984). A few specific estimation results for these
models will be presented briefly in relation to the topic of principal components
analysis which is discussed in Section 2.4.
The model (2.7) with the linear restrictions LWk = 0 can also be given a
“parametric” or factor analysis representation as Yk − μ = AFk + k [for example,
see Anderson (1984a)], where A is an m × r matrix of factor loadings of full rank
r<m (such that LA = 0) and Fk is an r×1 vector of unobservable common factors.
(Model (2.5) can be viewed as a special case of this form with Fk = BXk being
specified as r unknown linear combinations of the observable set of n predictor
variables Xk, and (2.5) has been advocated for use in the factor analysis context as
a special case of fixed effects factor analysis by van der Leeden (1990, Chapter 5).)
Typically, for the factor analysis model, it is assumed that the Fk are random
vectors with mean zero and covariance matrix Ir, and  = diag(σ2
1 ,...,σ2
m) is
diagonal. Interest in factor analysis modeling stems originally from its applications
in psychology and education, and early developments were by Thurstone (1947),
Reiersøl (1950), Lawley (1940, 1943, 1953), Rao (1955), Anderson and Rubin
(1956), and Jöreskog (1967, 1969).
When the error covariance matrix in model (2.7) is assumed to be unknown and
arbitrary, replicated vector observations are needed for estimation. For example, we
may have N ≥ 2 observations for each k with the model Ykj − μ = Wk + kj ,
j = 1,...,N, k = 1,...,T . This is a multivariate one-way ANOVA model, and
the analysis of such a model under the reduced-rank or linear restriction assumptions
for the (fixed effects) Wk has been considered by Anderson (1951, 1984a), Villegas
(1961, 1982), Theobald (1975), Healy (1980), and Campbell (1984), among others,
following earlier work by Fisher (1938) who considered a related problem in regard
to discriminant analysis. This one-way ANOVA model situation will be discussed
in more detail later in Section 3.2.
The above brief summary indicates that research and interest in applications of
models that directly involve reduced-rank regression concepts or that are closely
related have a long history and that the scope is quite broad. However, because
the focus of this book is directly on the (reduced-rank) multivariate regression
modeling, primarily, it will not be possible to explore all of the above discussed
related areas in detail. Articles by Anderson (1984a, 1991) provide an excellent24 2 Reduced-Rank Regression Model
summary of the developments and results in some of these areas, such as estimating
linear functional and structural relationships, factor analysis models, and the relation
to estimation of simultaneous equations models in econometrics. Interested readers
may refer to these articles for a more detailed and complete survey of developments
in these areas.
2.2 Some Examples of Application of the Reduced-Rank
Model
In this section we introduce and discuss some practical examples where reduced￾rank methods have been applied.
Example: 2.1 Davies and Tso (1982) have applied reduced-rank regression meth￾ods to study the experimental properties of hydrocarbon fuel mixtures in relating
response to composition. The responses are the engine test ratings and they are
related to the composition of hydrocarbon fuels, which is characterized by the
technique of gas–liquid chromatography (g.l.c.). The responses that are correlated
among themselves are known to be dependent on the same portion of g.l.c.
chromatograms. The precision of the g.l.c. determination of composition is quite
high, so that the resultant data vector of explanatory variables may be regarded as
deterministic, but the responses exhibit a wide variation. From the description of
the problem, it is possible that a multivariate regression model with rank constraints
might be appropriate. A small number of certain features of the chromatogram could
be used to predict fuel properties. Because the experimentation in this area is quite
expensive, Davies and Tso (1982) point out the usefulness of the model in reducing
the number of components.
A total of 30 vector observations, corresponding to different gasoline blends, are
divided into two data sets of 14 and 16 blends, respectively, for validation purposes.
On each blend, five gasoline distillation measurements y1,...,y5 were considered,
each representing percentage weight evaporated by a specified temperature, and ten
x variables were considered to represent the corresponding blend composition by
condensing the blend’s low-resolution chromatogram into a 10-component vector
by grouping of peak areas. Thus, the classical multivariate regression would call for
estimating fifty regression coefficient parameters (excluding intercepts) with thirty
vector observations. Using the mean square error criterion for model fit, 1
T m 
Y −
Y(r)
2, where Y(r) is the matrix of predicted values of the responses when the rank
of the regression coefficient matrix is taken to be r and is estimated by minimizing
the overall error sum of squares criterion, Davies and Tso demonstrate that r = 2
is fairly sufficient. In fact, in both data sets, over 95% of the variation in Yk is
explained by considering a model with r = 2 linear combinations of the Xk. Thus,
one may notice the substantial reduction in the number of parameters. Moreover,
the similarity in the two linear combination or two factor linear subspace of the Xk2.2 Some Examples of Application of the Reduced-Rank Model 25
Fig. 2.1 Prediction and fitting to distillation data for Set II gasolines
between the two data sets was fairly high, suggesting that the factors identified have
some physical significance.
Figure 2.1 shows the results, in terms of residual mean square (RMS) error, of
predicting the response measurements for gasolines in Set II from a knowledge of
their composition estimates from both Set I and Set II. The upper curve (MRI)
corresponds to predictions on Set II using coefficients of the linear combinations
(factors) and corresponding regression coefficients estimated from Set I. The second
curve (MRFI) corresponds to a fit for Set II obtained using coefficients of the
linear combinations (factors) of the Xk estimated from Set I but re-estimating the
regression coefficients of the Yk on these factors using Set II. The lower curve
(MRII) corresponds to the model estimated entirely using data from Set II.
Example: 2.2 The second example we discuss here is based on the work of Gud￾mundsson (1977), mentioned earlier in Section 2.1. Klein et al. (1961) constructed
a detailed econometric model of the United Kingdom. The data consist of quarterly
observations from 1948 to 1956 of 37 time series of response variables and 32 time
series of predictor variables, some of which may be lagged values of response
variables. This could be conceptually handled through the reduced-rank regression
model, but the computational burden would be enormous. Therefore, some grouping
of variables based on economic considerations was applied. But the focus of
Gudmundsson’s paper was to construct some linear combinations of variables,
representing various aspects of the general economic situation. Because most series
are likely to be related to the state of the economy in general, it is anticipated that
some useful linear combinations are possible.26 2 Reduced-Rank Regression Model
The following subset of time series variables was used in the reduced-rank
regression calculations. The dependent variables are
y1t Index of total industrial production
y2t Consumption of food, drinks, and tobacco at constant prices
y3t Price index of total consumption
y4t Total civilian labor force
y5t Index of weekly wage rates
y6t Index of volume of total imports
y7t Index of volume of total exports
and the predictor variables are x1t = y1t−1, x2t = y2t−1, x3t = y6t−1, x4t = y7t−1,
x5t = price index of total imports lagged by two time periods and x6t = t, the time
index.
The predictor variables are normalized before fitting the reduced-rank regression
model. Gudmundsson chose to estimate the linear combinations of predictor
variables, x∗
it = β
iXt , i = 1,...,r, as the coefficients βi which maximize the sum
of the squared sample correlation coefficients of x∗
i and the endogenous variables
y1,...,ym, subject to normalization and orthogonality conditions for the x∗
i . The
first three linear combinations were judged by Gudmundsson to be sufficient for use
in explaining all the endogenous variables, but no formal justification was provided
for this choice. This implicitly is assuming that the rank of the regression coefficient
matrix is three. The method used for estimating the βi was mildly criticized by
Theobald (1978). From the estimation results to be presented in Section 2.3, it
follows that these estimates can be viewed as ML estimates under normality and
the (possibly unrealistic) assumption that the covariance matrix of the error vector
t takes the form  = σ2Im.
Table 2.1 presents the information on the resulting (estimates of the) linear
combinations (X∗
t = BXt) and the sample correlations of the linear combinations
with all the response variables. It can be seen from the first part of the table that the
linear combinations are dominated by x6, the time trend variable, and by the variable
x1, the lagged index of industrial production. But from the correlations, it would
appear that the first linear combination, which accounts for most of the variation, is
fairly highly correlated with all the response variables. Such a combination can also
be taken as an index that represents the general economic situation. Gudmundsson
also compared the performance of the three linear combinations in predicting all
37 variables used in Klein et al. (1961). The comparisons were made using the
root mean square error criterion. The reduced-rank model performed better than the
econometric model for production variables, roughly comparable for consumption,
imports and exports, but not as good as the econometric model for prices and
employment series. This has to be expected because the three linear combinations
were constructed from a subset of the independent variables that did not have
representation by price or employment. But considering the magnitude of dimension
reduction, this is somewhat a small price to pay.2.2 Some Examples of Application of the Reduced-Rank Model 27
Table 2.1 Coefficients in the linear combinations X∗
t = BXt and sample correlations with
response variables Yt for econometric data example
Coefficients of independent variables in X∗
t = BXt
x1t x2t x3t x4t x5t x6t
x∗
1t 0.148 0.095 0.086 0.013 −0.058 0.720
x∗
2t −0.797 −0.193 −0.013 −0.226 0.302 0.936
x∗
3t −0.406 −0.271 −0.238 0.319 −0.271 0.767
Correlation coefficients of X∗
t with endogenous variables (Yt)
Sum of squared
y1t y2t y3t y4t y5t y6t y7t Corr. coefs.
x∗
1t 0.974 0.941 0.966 0.984 0.979 0.897 0.868 6.252
x∗
2t −0.083 −0.008 0.225 −0.042 0.138 −0.110 −0.142 0.110
x∗
3t 0.034 0.156 −0.065 0.015 0.016 −0.192 0.028 0.068
Example: 2.3 Glasbey (1992) used the reduced-rank model in a recent application
to demonstrate the relationship between measurements on solar radiation taken over
various sites in Scotland and the physical characteristics of the sites. The aim of
the analysis was to summarize the differences in solar radiation among sites and
if possible to relate them to other physical characteristics of the sites. The sites,
radiometric stations ten in number, were located on the range of Pentland Hills to the
south of Edinburgh (Fig. 2.2) and monthly averages of solar radiation measurements
on a log-scale for a period of 19 months form the 19 × 10 data matrix Y. The
log-scale was used to make variability over the seasons to be approximately the
same. Because of the proximity of these stations, one might expect these response
variables to be interrelated over sites, but any possible spatial correlations over the
sites will be ignored. The X matrix is formed with five site characteristics, latitude
(x1), longitude (x2), altitude (x3), and percentage sun loss in summer (x4) and
in winter (x5). Thus, the data matrix X is of dimension 5 × 10. This is a novel
application of the reduced-rank model and does not conform to the dimensional
details presented earlier (i.e., m = 19, n = 5, and T = 10 in this example, so
that m>T ). At the first stage after adjusting for monthly means, simple models
for the response variables at each site that account for most of the variability
in Y are constructed via the method of principal components and singular value
decomposition (SVD). Two terms or components were considered to be sufficient
in the singular value representation for Y on the basis that they account for a high
percent (85%) of the overall variation of the site values about the monthly mean over
all sites. Then these two summary vectors (1 × 10) of the responses were regressed
on X, the matrix of physical characteristics. The result can be shown to be in a form
of reduced-rank regression, where the rank of the matrix is assumed to be equal to
two. Over 80% of the variation in month-corrected Y is supposedly accounted by
the two linear combinations of X:28 2 Reduced-Rank Regression Model
Fig. 2.2 Microclimate survey area with locations of 10 radiometric stations (from Glasbey (1992))
x∗
1 = −319.3 + 5.7x1 + 0.2x2 − 0.0022x3 + 0.004x4 − 0.0076x5
(1.2) (0.6) (0.0013) (0.012) (0.0019)
x∗
2 = 160.2 + 2.6x1 + 4.8x2 − 0.0037x3 + 0.017x4 + 0.0019x5
(2.1) (1.1) (0.0024) (0.022) (0.0034),
where the numbers in parentheses are the estimated standard errors.
The first combination x∗
1 is primarily a latitude effect, with some negative
adjustments for altitude and percentage sun loss in winter. The second combination
x∗
2 is a longitude effect but adjusted for altitude. The more northerly sites (see2.3 Estimation of Parameters in the Reduced-Rank Model 29
Fig. 2.2) have positive values and the southern sites have negative values for the
regression coefficient on x∗
1 and western sites have the positive values on x∗
2 . In
general, Glasbey (1992) concludes that the sites at higher altitudes received less
radiation; winter sun loss reduced radiation levels but the summer loss is not known
to be effective.
Based on the above examples alone, it would appear that the reduced-rank
methods have strong potential for both theoretical study and applications. A few
other recent applications of reduced-rank regression include the following. In a
study by Ryan et al. (1992), reduced-rank methods were applied to the analysis of
response data (DNA, RNA, and protein) from a joint toxicity bioassay experiment
to determine the joint effects of toxic compounds that include copper and zinc on
the growth of larval fathead minnows. In the field of financial economics, several
asset pricing theories have been proposed for testing the efficiency of portfolios,
as described by Zhou (1991, 1995). These theories generally imply certain rank
restrictions on the regression coefficient matrix that relates asset returns to market
factors. Empirical verification using asset returns data on industry portfolios has
been made through tests for reduced-rank regression. Other recent applications will
be covered throughout the book.
2.3 Estimation of Parameters in the Reduced-Rank Model
The reduced-rank regression model described in Section 2.1 has the following
parameters to be estimated from a sample of observations: the matrix A which is of
dimension m×r, the matrix B which is of dimension r×n, and the m×m covariance
matrix  of the error term. Estimation of the constraint vectors 1,...,m−r will
be taken up following these. Before we proceed with the details, we note that in the
reduced-rank model with C = AB the component matrices A and B are determined
only up to nonsingular linear transformations. To obtain a particular unique set
of parameter values for A and B from the elements of C, certain normalization
conditions are typically imposed. These conditions, as we have chosen here, will
follow from the estimation criterion. To clearly exhibit the algebraic features of
the problem, we will initially formulate the criterion for determination of A and
B in terms of “population” quantities, and later we will demonstrate how the
maximum likelihood estimates from sample data are obtained through use of the
initial population results.
As we shall see shortly, reduced-rank estimation will be obtained as a certain
reduced-rank approximation of the full-rank least squares estimate of the regression
coefficient matrix. Therefore, for presentation of reduced-rank estimation, we first
need an important matrix result called the Householder–Young or Eckart–Young
theorem (Eckart and Young 1936), which presents the main tool for approximating
a full-rank matrix by a matrix of lower rank. The solution is related to the singular
value decomposition of the full-rank matrix. As a preliminary to this main theorem,
we state a needed result given in Rao (1973, p. 63).30 2 Reduced-Rank Regression Model
Lemma 2.1 Let A be an m × m symmetric matrix with eigenvalues λ1 ≥ ··· ≥
λm, and denote the corresponding normalized eigenvectors as P1,...,Pm. The
supremum of r
i=1 X
iAXi = tr(X
AX), with X = [X1,...,Xr], over all sets
of r ≤ m mutually orthonormal vectors X1,...,Xr is equal to r
i=1 λi and is
attained when the Xi = Pi, i = 1,...,r.
Proof Let P = [P1,...,Pm], so that P is an orthogonal matrix with P
AP =
 = diag(λ1,...,λm). Any set of r mutually orthonormal vectors X1,...,Xr can
be expressed as Xi = m
j=1 cijPj ≡ P ci, i = 1,...,r, where ci = (ci1,...,cim)
,
with X
iXi = c
iP
P ci = c
ici = 1 and X
iXl = c
iP
P cl = c
icl = 0, for i = l.
Then
r
i=1
X
iAXi = r
i=1
c
iP
AP ci = r
i=1
c
ici = r
i=1
m
j=1
λj c2
ij = m
j=1
λj

r
i=1
c2
ij
,
where the coefficient of each λj ,
r
i=1 c2
ij , is ≤ 1 and the sum of these coefficients
is r, since m
j=1
r
i=1 c2
ij = r
i=1 c
ici = r. To maximize the sum above, the
optimum choice is thus to make the coefficients of the λj equal to 1 for j = 1,...,r,
and hence 0 for j>r. This is attained only by the choice Xi = Pi, i = 1,...,r,
that is, cij = 1 for i = j and cij = 0 for i = j , where Pi is a normalized eigenvector
of A corresponding to the eigenvalue λi. The eigenvectors Pi are (essentially)
unique, of course, if the eigenvalues λi are distinct, but the choice of eigenvectors
will not be unique if some among the first r eigenvalues are not distinct.
Theorem 2.1 Let S be a matrix of order m × n and of rank m. The Euclidean
norm, tr[(S − P )(S − P )
], is minimum among matrices P of the same order as S
but of rank r(≤ m), when P = MM
S, where M is m × r and the columns of M
are the first r (normalized) eigenvectors of SS
, that is, the normalized eigenvectors
corresponding to the r largest eigenvalues of SS
.
Proof Let P = MN, where M is an m×r matrix and N is an r ×n matrix. Without
loss of generality, assume that M is orthonormal with M
M = Ir. Minimizing the
criterion given in the theorem over N, for a given M, yields N = (M
M)−1M
S =
M
S, by similar argument that led to the least squares solution in (1.7). Substituting
this in the criterion yields, after some simplification,
tr[(S − P )(S − P )
] = tr[SS
(Im − MM
)] = tr(SS
) − tr(M
SS
M). (2.9)
Minimizing the above quantity with respect to M is equivalent to maximizing
tr(M
SS
M) subject to M
M = Ir. (2.10)2.3 Estimation of Parameters in the Reduced-Rank Model 31
Based on the result in Lemma 2.1, it follows that the above goal is achieved
by choosing the columns of M to be the (orthonormal) eigenvectors of SS that
correspond to the first (largest) r eigenvalues.
Remark—Singular Value Decomposition The positive square roots of the eigen￾values of SS are referred to as the singular values of the matrix S. In general, an
m × n matrix S, of rank s, can be expressed in the singular value decomposition
as S = V U
, where  = diag(λ1,...,λs) with λ2
1 ≥ ··· ≥ λ2
s > 0 being
the nonzero eigenvalues of SS
, V = [V1,...,Vs] is an m × s matrix such that
V 
V = Is, and U = [U1,...,Us] is n × s such that U
U = Is. The columns
Vi are the normalized eigenvectors of SS corresponding to the eigenvalue λ2
i ,
and Ui = 1
λi
S
Vi, i = 1,...,s. For the situation in Theorem 2.1 above, from
the singular value decomposition of S, we see that the criterion to be minimized
over rank r approximations P = MN is tr[(V U − MN )(V U − MN)
] =
tr[( − V 
MNU )( − V 
MNU)
]. The result in Theorem 2.1 states that the
minimizing matrix factors M and N are given by M = [V1,...,Vr] ≡ V(r)
and N = M
S = V 
(r)S = V 
(r)V U = [λ1U1,...,λrUr]
 ≡ (r)U
(r), with
U(r) = [U1,...,Ur] and (r) = diag(λ1,...,λr). So P = MN = V(r)(r)U
(r)
yields the “optimum” rank r approximation to S = V U
, and the resulting
minimum value of the trace criterion is m
i=r+1 λ2
i .
We restate the multivariate regression model before we proceed with the esti￾mation of the relevant parameters. From (2.5), the reduced-rank regression model
is
Yk = ABXk + k, k = 1,...,T, (2.11)
where A is of dimension m×r, B of dimension r×n, and the vectors k are assumed
to be independent with mean zero and covariance matrix  , where  is positive
definite. Estimation of A and B in (2.11) is based on the following result (Brillinger
1981, Section 10.2), which essentially uses Theorem 2.1. For this “population”
result, the vector of predictor variables Xk in (2.11) will be treated as a stochastic
vector for ease of exposition.
Theorem 2.2 Suppose the (m+n)-dimensional random vector (Y 
k, X
k) has mean
vector 0 and covariance matrix with yx = 
xy = Cov(Yk, Xk) and xx =
Cov(Xk) nonsingular. Then, for any positive-definite matrix , an m × r matrix
A and r × n matrix B, for r ≤ min(m, n), which minimize
tr{E[1/2(Yk − ABXk)(Yk − ABXk)

1/2]}, (2.12)
are given by
A(r) = −1/2[V1,...,Vr] = −1/2V, B(r) = V 
1/2yx−1
xx , (2.13)32 2 Reduced-Rank Regression Model
where V = [V1,...,Vr] and Vj is the (normalized) eigenvector that corresponds to
the j th largest eigenvalue λ2
j of the matrix 1/2yx−1 xx xy1/2 (j = 1, 2,... ,r).
Proof Setting S = 1/2yx−1/2 xx and P = 1/2AB1/2 xx , the result follows easily
from Theorem 2.1 because the criterion (2.12) can be expressed as
tr{1/2(yy − ABxy − yxB
A + ABxxB
A
)1/2}
= tr{1/2(yy − yx−1
xx xy)1/2}
+ tr{1/2(yx−1/2 xx − AB1/2 xx )(yx−1/2 xx − AB1/2 xx )

1/2}, (2.12
)
where yy = Cov(Yk). Hence we see that minimization of this criterion, with
respect to A and B, is equivalent to minimization of the criterion considered in
Theorem 2.1. In terms of quantities used in Theorem 2.1, M = 1/2A(r) and N =
B(r)1/2 xx . At the minimum the criterion (2.12) has the value tr(yy) − r
j=1 λ2
j .
Several remarks follow regarding Theorem 2.2 [see Izenman (1975, 1980)]. As
mentioned before, nothing appears to be gained by estimating the equations jointly
in the multivariate regression model. But a true multivariate feature enters the
model when we assume that the coefficient matrix may not have full rank. Hence,
the display of superscripts for A and B in Theorem 2.2 is meant to denote the
prescribed rank r. We also observed earlier that the decomposition C = AB is
not unique because for any nonsingular r × r matrix P, C = AP −1P B, and hence
to determine A and B uniquely, we must impose some normalization conditions. For
the expressions in (2.13), the eigenvectors Vj are normalized to satisfy V 
jVj = 1,
and this is equivalent to normalization for A and B as follows:
B xxB = 2, A
A = Ir, (2.14)
where 2 = diag(λ2
1,...,λ2
r) and Ir is an r×r identity matrix. Thus, the number of
independent regression parameters in the reduced-rank model (2.11) is r(m+n−r)
compared to mn parameters in the full-rank model. Hence, a substantial reduction
in the number of parameters is possible.
The elements of the reduced-rank approximation of the matrix C are given as
C(r) = A(r)B(r) = −1/2
⎛
⎝r
j=1
VjV 
j
⎞
⎠ 1/2 yx −1
xx = P yx −1
xx , (2.15)
where P is an idempotent matrix for any , but need not be symmetric. Observe
that, analogous to (1.7), yx−1 xx is the usual full-rank (population) regression
coefficient matrix. When r = m,
m
j=1 VjV 
j = Im, and therefore, C(r) reduces
to the full-rank coefficient matrix.2.3 Estimation of Parameters in the Reduced-Rank Model 33
Rao (1979) has shown a stronger result that the solution for A and B in
Theorem 2.2 also simultaneously minimizes all the eigenvalues of the matrix in
(2.12) provided that  = −1
yy . This was proved using the Poincairé separation
theorem. Robinson (1974) has proved a similar result, which indicates that the
solutions of A and B in Theorem 2.2, when the corresponding sample quantities
are substituted and with  = −1  , are Gaussian estimates under the reduced￾rank model (2.11), that is, maximum likelihood estimates under the assumption
of normality of the k. In the remainder of this book, we may often refer to such
estimates derived from the multivariate normal likelihood as maximum likelihood
estimates, even when the distributional assumption of normality is not being made
explicitly. We shall sketch the details below to establish the maximum likelihood
estimates of the parameters in the reduced-rank regression model (2.11). Before we
proceed with this, we first state a needed result from Rao (1979, Theorem 2.3) which
provides, in some sense, a stronger form of the result in Theorem 2.1.
Lemma 2.2 Let S be an m × n matrix of rank m and P be an m × n matrix of rank
≤ r(≤ m). Then
λi(S − P ) ≥ λr+i(S) for any i,
where λi(S) denotes the ith largest singular value of the matrix S (as defined in the
remark following the proof of Theorem 2.1), and λr+i(S) is defined to be zero for
r + i>m. The equality is attained for all i if and only if P = V(r)(r)U
(r), where
S = V U represents the singular value decomposition of S as described in the
remark.
For ML estimation of model (2.11), let Y = [Y1,...,YT ] and X =
[X1,...,XT ], where T denotes the number of vector observations. Assuming
the k are independent and identically distributed (i.i.d.), following a multivariate
normal distribution with mean 0 and covariance matrix  , the log-likelihood,
apart from irrelevant constants, is
L(C,  ) =
T
2
	
log |−1
 | − tr(−1
 W )
, (2.16)
where |−1  | is the determinant of the matrix −1  and W = (1/T )(Y − CX)(Y −
CX)
. When  is unknown, the solution obtained by maximizing the above log￾likelihood is  = W. Hence, the concentrated log-likelihood is L(C,  ) =
−(T /2)(log |W| + m). Maximizing this expression (with the structure C = AB, of
course) is equivalent to minimizing |W| and hence minimizing |−1  W| since |−1  |
is a positive constant, where
 = (1/T )(YY − YX
(XX
)
−1XY
) ≡ (1/T )(Y − C
X)(Y − C
X)

and C
 = YX
(XX
)−1 is the (full-rank) LS estimate of C.34 2 Reduced-Rank Regression Model
Robinson (1974) has shown that the solution in Theorem 2.2 when sample
quantities are substituted, namely
A
(r) = −1/2[V
1,..., V
r], B
(r) = [V
1,..., V
r]
 1/2 yx −1
xx , (2.17)
where yx = (1/T )YX
, xx = (1/T )XX
, and V
j is the eigenvector that
corresponds to the j th largest eigenvalue λ2
j of 1/2yx−1 xx xy1/2, with the
choice  = −1  , minimizes simultaneously all the eigenvalues of −1  W and hence
minimizes |−1  W|. To establish this, write
W = 1
T (Y − C
X + (C
 − AB)X)(Y − C
X + (C
 − AB)X)

= 1
T (Y − C
X)(Y − C
X)
 +
1
T (C
 − AB)XX
(C
 − AB)
≡  + (C
 − AB) xx (C
 − AB)
.
Thus |−1  W|=|Im + −1  (C
 − AB)xx (C
 − AB)
| = m
i=1(1 + δ2
i ),
where δ2
i are the eigenvalues of the matrix −1  (C
 − AB)xx (C
 − AB)
. So
minimizing |−1  W| is equivalent to simultaneously minimizing all the eigenvalues
of −1/2  (C
 − AB)xx (C
 − AB)
−1/2  ≡ (S − P )(S − P )
, with S =
−1/2  C
1/2 xx and P = −1/2  AB1/2 xx . From the result of Lemma 2.2, we see
that the simultaneous minimization is achieved with P chosen as the rank r
approximation of S obtained through the singular value decomposition of S, that
is, P = V
(r)V

(r)S = V
(r)V

(r)−1/2  C
1/2 xx ≡ −1/2  A
(r)B
(r)1/2 xx . So C
(r) ≡
A
(r)B
(r) = 1/2  V
(r)V

(r)−1/2  C
, with A
(r) and B
(r) as given in (2.17), is the ML
estimate.
Now the correspondence between the maximum likelihood estimators and the
quantities A(r) and B(r) given in Theorem 2.2 is evident. It must be noted here that
 is the ML estimate of  obtained in the full-rank regression model, that is,
 = (1/T )(Y−C
X)(Y−C
X) with C
 = yx−1 xx equal to the full-rank estimate
of C as given in Eq. (1.7). The ML estimate of  under the reduced-rank structure
is given by  = (1/T )(Y − C
(r)X)(Y − C
(r)X)
, with C
(r) = A
(r)B
(r) obtained
from (2.17). From the developments above, we see that  can be represented as
 =  + (C
 − C
(r)) xx (C
 − C
(r))

=  + (Im − P )C
xx C
 (Im − P )
=  + 1/2  (Im − V
(r) V

(r)) R (I  m − V
(r) V

(r)) 1/2  , (2.18)
where P = 1/2  V
(r) V

(r)−1/2  , R
 = −1/2  yx −1 xx xy −1/2  , and V
(r) =
[V
1,..., V
r]. Note, also, that the sample version of the criterion in (2.12) and of the
term on the right-hand side of the corresponding identity in (2.12
) (with  = −1  )
is2.3 Estimation of Parameters in the Reduced-Rank Model 35
tr 
−1

1
T (Y − ABX)(Y − ABX)

 
= m + tr !
−1
 (C
 − AB)xx (C
 − AB)
"
.
So the ML estimates A
(r) and B
(r) can be viewed as providing the “optimum”
reduced-rank approximation of the full-rank least squares estimator C
 = yx−1 xx
corresponding to this criterion. The choice  = −1  in the criterion, which leads
to the ML estimates A
(r) and B
(r), provides an asymptotically efficient estimator
under the assumptions that the errors k are independent and normally distributed
as N (0,  ).
Remark—Equivalence of Two Estimators Let the solution (estimates) for the
component matrices A and B given by Theorem 2.2 when sample quantities are
substituted, with the choice  = −1
yy , where yy = (1/T )YY
, be denoted as
A
(r)
∗ and B
(r)
∗ , respectively. Then we mention that while this results in different
component estimates from the estimates A
(r) and B
(r) provided in (2.17) for the
choice  = −1  , it yields the same ML estimate for the overall coefficient matrix
C = AB, that is, C
(r) = A
(r)B
(r) ≡ A
(r)
∗ B
(r)
∗ . This connection in solutions between
the choices  = −1
yy and  = −1  will be discussed in more detail in Section 2.4.
It must be further observed that the (m−r) linear restrictions on C(r) = A(r)B(r)
are

j = V 
j 1/2, j = r + 1, . . . , m, (2.19)
because
V 
j 1/2 C(r) = V 
j 1/2 −1/2[V1,...,Vr]B(r) = 0
, j = r + 1, . . . , m.
Thus, the last (m − r) eigenvectors of the matrix given in Theorem 2.2 also
provide the solution to the complementary part of the problem. Recall that the
linear combination 
jYk = 
j k in (2.3) does not depend on the predictor variables
Xk. The necessary calculations for both aspects of the (population) problem can be
achieved through the computation of all eigenvalues and eigenvectors of the matrix
1/2yx−1 xx xy1/2, given in Theorem 2.2.
A comment worth noting here refers to the solution to the reduced-rank problem.
In the construction of Theorem 2.1, observe that the optimal matrix N was derived
in terms of M and then the optimal matrix M was determined. This is equivalent
to deriving the matrix B in terms of A and then determining the optimal matrix A.
Conversely, one could fix B and solve for A in terms of B first. This is the regression
step in the calculation procedure. Because
Yk = A(BXk) + k = AX∗
k + k,36 2 Reduced-Rank Regression Model
given X∗
k = BXk, A = yxB
(BxxB
)−1, the usual regression coefficient
matrix obtained by regressing Yk on X∗
k . If we substitute A in terms of B in
the criterion (2.12) given in Theorem 2.2, it reduces to minimizing tr{yy  −
(Bxx B
)−1 B xy  yxB
}. From this, it follows that the columns of 1/2 xx B
may be chosen as the eigenvectors corresponding to the r largest eigenvalues of
−1/2 xx xy  yx −1/2 xx . To see all this in a convenient way, notice that, in terms
of the notation of Theorem 2.1, the criterion in (2.12
) to be minimized, tr[(S −
P )(S −P )
], can also be expressed as tr[(S −P )
(S −P )] ≡ tr[(S −P
)(S −P
)
]
with P = N
M
. Thus, we may treat this as simply interchanging S with S in the
reduced-rank approximation problem. Hence, following the proof of Theorem 2.1,
the minimizing value of M
, for given N
, is obtained as M = (NN
)−1NS
.
With the correspondences S = 1/2yx−1/2 xx , M = 1/2A, and N = B1/2 xx
used in the proof of Theorem 2.2, this leads to the optimal value M ≡ A
1/2 =
(BxxB
)−1Bxy1/2, and hence A = yx B (Bxx B
)−1 as indicated above.
Then as in Theorem 2.1, the optimal value of N ≡ 1/2 xx B is determined as
the matrix whose columns are the eigenvalues of S
S = −1/2 xx xy yx −1/2 xx
corresponding to its r largest eigenvalues. Hence we see that this approach calls for
eigenvectors of S
S instead of SS as in the result of Theorem 2.1. The approach
presented here, of fixing B first, is appealing as various hypothesized values can be
set for B and also it is easier to calculate quantities needed for making inferences,
such as standard errors of the estimator of A, and so on, as in the full-rank setup.
The result presented in Theorem 2.2 and the above remark describe how an
explicit solution for the component matrices A and B in (2.12) can be obtained
through computation of eigenvalues and eigenvectors of either SS or S
S. For
the extended reduced-rank models to be considered in later chapters such an
explicit solution is not possible, and iterative procedures need to be used. Although
the reduced-rank model (2.11) is nonlinear in the parameter matrices A and B,
the structure can be regarded as bilinear, and thus certain simplifications in the
computations are possible. For the population problem, consider the first order
equations resulting from minimization of the criterion (2.12), that is, consider the
first partial derivatives of the criterion (2.12) with respect to A and B, set equal to
zero. These first order equations are readily found to be
yxB − ABxxB = 0 and A
yx − (A
A)Bxx = 0.
As observed previously, from the first equation above, we see that for given B,
the solution for A can be calculated as A = yx B (B xx B
)−1. In addition,
similarly, from the second equations above, we find that for given A, the solution
for B can be obtained as B = (A  A)−1 A yx −1 xx , or B = A  yx−1 xx
assuming that the normalization conditions in (2.14) for A are imposed. The
iterative procedure, similar to calculations involved in a method known as partial
least squares, calls for iterating between the two solutions (i.e., the solution for
A in terms of B and the solution for B in terms of A) and at each step of the
iteration imposing the normalization conditions (2.14). A similar iterative procedure2.3 Estimation of Parameters in the Reduced-Rank Model 37
was suggested by Lyttkens (1972) for the computation of canonical variates and
canonical correlations.
It might also be noted at this point that alternate normalizations could be
considered. One alternate normalization, with B partitioned as B = [B1, B2], where
B1 is r × r, is obtained by assuming that, since B is of rank r, its leading r × r
submatrix B1 is nonsingular (the components of Xk can always be arranged so that
this holds). Then we can write C = AB = (AB1)[Ir, B−1
1 B2] ≡ A0[Ir, B0] and
the component parameter matrices A0 and B0 are uniquely identified. (A similar
alternate normalization could be imposed by specifying the upper r × r submatrix
of A equal to Ir). The appeal of using partial least squares increases under either of
these alternate normalizations, since either normalization can be directly imposed
on the corresponding first order equations above. These notions also carry over
to the sample situation of constructing estimates A
(r) and B
(r), iteratively, by
using sample versions of the above partial least squares estimating equations. An
additional appeal in the sample setting is that it is very convenient to accommodate
extra zero constraints on the parameter elements of A and B, if desired, in the partial
least squares estimation whereas this is not possible in the eigenvalue–eigenvector
solutions as in (2.17).
Remark—Case of Special Covariance Structure The preceding results on ML
estimation apply to the typical case where the error covariance matrix  is
unknown and is a completely general positive-definite matrix. We may also briefly
consider the special case where the error covariance matrix in model (2.11) is
assumed to have the specified form  = σ20, where σ2 is an unknown scalar
parameter, but 0 is a known (specified) m × m matrix. Then from (2.16), the ML
estimates of A and B are obtained by minimizing
1
σ2 tr[−1
0 (Y − ABX)(Y − ABX)

]
= 1
σ2 tr[−1
0 (Y − C
X)(Y − C
X)

] +
T
σ2 tr 	
−1
0 (C
 − AB)xx (C
 − AB)

.
So from Theorem 2.1, it follows immediately that the ML solution is A
 = 1/2
0 V
,
B
 = V
 −1/2
0 C
, and so C
 = A
B
 = 1/2
0 V
V
 −1/2
0 C
, where V
 = [V
1,..., V
r]
and the V
j are the normalized eigenvectors of the matrix
−1/2
0 C
xx C

−1/2
0 ≡ −1/2
0 yx −1
xx xy −1/2
0
corresponding to the r largest eigenvalues λ2
1 > ··· > λ2
r. The corresponding ML
estimate of σ2 is given by38 2 Reduced-Rank Regression Model
σ2 = 1
T m
tr[(Y − A
B
X)

−1
0 (Y − A
B
X)]
≡ 1
m
#
tr[−1
0  ] + m
i=r+1
λ2
i
$
. (2.20)
In particular, assuming  = σ2Im, so 0 = Im, corresponds to minimizing the
sum of squares criterion tr[(Y − ABX)(Y − ABX)
]. This gives the reduced-rank
ML or least squares estimators as A
 = V
, B
 = V

C
, and C
 = V
V
 C
, with
σ2 = 1
T m tr[(Y − A
B
X)
(Y − A
B
X)] ≡ 1
m {tr[ ] + m
i=r+1
λ2
i }, where the V
j
are normalized eigenvectors of the matrix C
xxC
 ≡ yx−1 xx xy . This reduced￾rank LS estimator was used by Davies and Tso (1982) and Gudmundsson (1977), in
particular. The corresponding population reduced-rank LS result for random vectors
Y and X amounts to setting  = Im in the criterion (2.12) of Theorem 2.2, with
the problem reduced to finding A and B which minimize tr{E[(Y − ABX)(Y −
ABX)
]} ≡ E[(Y − ABX)
(Y − ABX)]. The optimal (LS) solution is A = V(r),
B = V 
(r)yx−1 xx , where the Vj are normalized eigenvectors of yx−1 xx xy . In
Section 2.1, we referenced the related works by Rao (1964), Fortier (1966), and
van den Wollenberg (1977) that used different terminologies, such as redundancy
analysis, regarding the reduced-rank problem, but the procedures discussed in these
papers can be recognized basically as being equivalent to the use of this reduced￾rank LS method. It was also noted by Fortier (1966) that the linear predictive factors,
that is, the transformed predictor variables BX = V 
yx−1 xx X, resulting from the
LS approach can be viewed simply as the principal components (to be discussed
in the next section) of the unrestricted LS prediction vector Y
 = yx−1 xx X, noting
that Cov(Y )  = yx−1 xx xy and the Vj in V 
yx−1 xx X ≡ V 
Y
are the eigenvectors
of this covariance matrix.
It should be mentioned, perhaps, that estimation results under the LS criterion are
sensitive to the choice of scaling of the response variables Yk, and in applications of
the LS method it is often suggested that the component variables yik be standardized
to have unit variances before applying the LS procedure. Of course, if Yk is
“standardized” in the more general way as −1/2
yy Yk so that the standardized vector
has the identity covariance matrix, then the LS procedure applied to −1/2
yy Yk is
equivalent to the canonical correlation analysis results for the original variables Yk.
Even when the form  = σ2Im does not hold, the reduced-rank LS estimator
may be useful relative to the reduced-rank ML estimator in situations where the
number of vector observations T is not large relative to the dimensions m and n of
the response and predictor vectors Yk and Xk, since then the estimator  may not
be very accurate or may not even be nonsingular (e.g., as in the case of Example 2.3
where T = 10 < m = 19).2.4 Relation to Principal Components and Canonical Correlation Analysis 39
2.4 Relation to Principal Components and Canonical
Correlation Analysis
2.4.1 Principal Components Analysis
Principal component analysis, originally developed by Hotelling (1933), is usu￾ally concerned with explaining the covariance structure of a large number of
interdependent variables through a small number of linear combinations of the
original variables. The two objectives of the analysis are data reduction and
interpretation which coincide with the objectives of the reduced-rank regression.
A major difference between the two approaches is that the principal components
are descriptive tools that do not usually rest on any modeling or distributional
assumptions.
Let the m × 1 random vector Y have the covariance matrix yy with eigenvalues
λ2
1 ≥ λ2
2 ≥ ··· ≥ λ2
m ≥ 0. Consider the linear combinations y∗
1 = 
1Y,... , y∗
m =

mY , normalized so that 
ii = 1, i = 1,...,m. These linear combinations have
Var(y∗
i ) = 
i yy i, i = 1, 2, . . . , m,
Cov(y∗
i , y∗
j ) = 
i yy j , i, j = 1, 2, . . . , m.
(2.21)
In a principal components analysis, the linear combinations y∗
i are chosen so that
they are not correlated but their variances are as large as possible. That is, y∗
1 = 
1Y
is chosen to have the largest variance among linear combinations with 
11 = 1,
y∗
2 = 
2Y is chosen to have the largest variance subject to 
22 = 1 and the
condition that Cov(y∗
2 , y∗
1 ) = 
2yy1 = 0, and so on. It can be easily shown
(e.g., from Lemma 2.1) that the 
is are chosen to be the (normalized) eigenvectors
that correspond to the eigenvalues λ2
i of the matrix yy , so that the normalized
eigenvectors satisfy 
iyyi = λ2
i ≡ Var(y∗
i ). Often, the first r(r < m) components
y∗
i , corresponding to the r largest eigenvalues, will contain nearly all the variation
that arises from all m variables, in that r
i=1 Var(y∗
i ) ≡ r
i=1 
iyy i represents
a very large proportion of the “total variance” m
i=1 Var(y∗
i ) ≡ tr(yy ). For
such cases, in studying variations of Y , attention can be directed on these r linear
combinations resulting in a reduction in dimension of the variables involved.
The principal component problem can be represented as a reduced-rank problem,
in the sense that a reduced-rank model can be written for this situation as
Yk = ABYk + k (2.22)
(with Xk ≡ Yk). From the solutions of Theorem 2.2, by setting  = Im, we
have A(r) = [V1,...,Vr] and B(r) = V  because yx = yy . The Vj s refer to
the eigenvectors of yy . Thus, a correspondence is easily established. The i in
(2.21) are the same as the Vi that result from the solution for the matrices A and
B in (2.22). If both approaches provide the same solutions, one might ask, then,40 2 Reduced-Rank Regression Model
what is the advantage of formulating the principal components analysis in terms
of reduced-rank regression model. The inclusion of the error term k appears to
be superfluous, but the model (2.22) could provide a useful framework to perform
the residual analysis to detect outliers, influential observations, and so on, for the
principal components method.
2.4.2 Application to Functional and Structural Relationships
Models
Consider further the linear functional and structural relationships model discussed
briefly in Section 2.1,
Yk − μ = Wk + k, k = 1,...,T, (2.23)
where the {Wk} are assumed to satisfy the linear relations LWk = 0 for all k, with L
an unknown (m − r) × m matrix of full rank and the normalization LL = Im−r is
assumed. In the functional case where the Wk are treated as fixed, with E(Yk −μ) =
Wk, it is assumed that T
k=1 Wk = 0, whereas in the structural case the Wk are
assumed to be random vectors with E(Wk) = 0 and Cov(Wk) =  = 
, where
 is an m × r full-rank matrix (r < m). In both cases, it is also assumed that
the error covariance matrix has the structure  = σ2Im, where σ2 is unknown.
Thus, in the structural case, the Yk have covariance matrix of the form Cov(Yk) =
 +  =  + σ2Im.
Let Y¯ = (1/T )T
k=1 Yk and yy = (1/T )T
k=1(Yk − Y )(Y ¯ k − Y )¯  denote
the sample mean vector and sample covariance matrix of the Yk. Also let λ2
1 >
λ2
2 > ··· > λ2
m > 0 denote the eigenvalues of yy with corresponding normalized
eigenvectors i, i = 1,...,m. Then y∗
ik = 
i Yk are the sample principal
components of the Yk. In the functional case, it has been shown originally by
Tintner (1945), also see Anderson (1984a), that an ML estimate of the unknown
constraints matrix L is given by the matrix L
 = L

2 ≡ [r+1,...,m]
 whose rows
correspond to the last (smallest) m − r principal components in the sample. We
also set L
1 = [1,...,r], corresponding to the first (largest) r sample principal
components. Then, the ML estimates of the unknown but fixed values Wk are given
by
Wk = L
1L

1(Yk − Y ), k ¯ = 1,...,T, (2.24)
with μ = Y¯, and σ2 = 1
m
m
i=r+1
λ2
i . Thus, in effect, under ML estimation the
variability described by the first r sample principal components (as measured by
λ2
1,...,λ2
r) is assigned to the variation in the mean values Wk and the remaining
variability, m
i=r+1
λ2
i , is assigned to the variance σ2 of the errors k. The above
estimator σ2 of σ2 is biased downward, however.2.4 Relation to Principal Components and Canonical Correlation Analysis 41
We verify these results by using the framework of the ML estimation results
established in Section 2.3 for the reduced-rank regression model. As discussed in
Section 2.1, the functional model Yk − μ = Wk + k, LWk = 0, can be written
equivalently as
Yk − μ = AFk + k, k = 1,...,T ; (2.25)
that is, Wk = AFk, where A is m × r such that LA = 0 and the Fk are r × 1 vectors
of unknown constants. Hence, in matrix form we have
Y = μ1 + W +  = μ1 + ABX +  ≡ μ1 + CX + , (2.26)
where X = IT , W = [W1,...,WT ] ≡ C = AB with B = [F1,...,FT ], and
1 = (1,..., 1) denotes a T × 1 vector of ones. For estimation, the condition
W1 = T
k=1 Wk = 0, equivalently, T
k=1 Fk = 0, is imposed. The unrestricted
LS estimators of the model are μ = Y¯, Wk = Yk − Y¯, k = 1,...,T , or
C
 = Y − Y¯ 1 (with  ≡ 0 since Yk − μ − Wk ≡ 0). Then notice that
C
xxC
 = 1
T (Y − Y¯ 1
)(Y − Y¯ 1
) = yy , with eigenvalues λ2
1 > λ2
2 > ··· >
λ2
m > 0 and corresponding normalized eigenvectors i, i = 1,...,m. Then with
L
1 = [1,...,r], according to the ML estimation results for the special case
 = σ2Im presented at the end of Section 2.3, we have that the ML estimates
are A
 = L
1, B
 = L

1C
 ≡ L

1(Y − Y¯ 1
), so that W = C
 = A
B
 = L
1L

1(Y − Y¯ 1
).
That is,
Wk = L
1L

1(Yk − Y ), k ¯ = 1,...,T,
as in (2.24), with μ = μ = Y¯. In addition, the ML estimate of σ2, from (2.20),
is given by σ2 = 1
m
m
i=r+1
λ2
i as stated above. (Actually, the above obtained
results follow from a slight extension of results for the basic model considered in
this chapter, which explicitly allows for a constant term. The general case of this
extension will be considered in Section 3.1.)
In the structural case of the model (2.23), L
 = L

2 is also an ML estimate of
L, whereas the ML estimate of σ2 is σ2 = 1
m−r
m
i=r+1
λ2
i . The ML estimate of
Cov(Wk) =  =  is given by
 = L
1D1L

1 − σ2L
1L

1 = L
1(D1 − σ2Im)L

1,
where D1 = diag(λ2
1,...,λ2
r). So the ML estimate of Cov(Yk) = yy is
yy =  + σ2Im ≡ L
1D1L

1 + σ2 L
2L

2,
noting that L
1L

1 + L
2L

2 = Im. These estimators were obtained by Lawley (1953)
and by Theobald (1975), also see Anderson (1984a).42 2 Reduced-Rank Regression Model
Thus, in the above linear relationship models, we see that the sample principal
components arise fundamentally in the ML solution. The first r principal compo￾nents are the basis for estimation of the systematic part Wk of Yk, e.g., Wk =
L
1L

1(Yk −Y )¯ in the functional case, and (Yk −Y )¯ −L
1L

1(Yk −Y )¯ = L
2L

2(Yk −Y )¯
can be considered as the estimate of the error. However, a distinction in this
application of principal components analysis is that the error in approximation of
Yk through its first r principal components L

1Yk is not necessarily small but should
have the attributes of the error term k, namely mean 0 and covariance matrix
with equal variances σ2. So in this analysis the remaining last (m − r) values
λ2
r+1,...,λ2
m are not necessarily “negligible” relative to λ2
1,...,λ2
r, as would be
hoped in some typical uses of principal components analysis. In the linear functional
or structural analysis, the representation of Yk by such a model form might be
considered to be good provided only that λ2
r+1,...,λ2
m are roughly equal (but not
necessarily small).
2.4.3 Canonical Correlation Analysis
Canonical correlation analysis was introduced by Hotelling (1935, 1936) as a
method of summarizing relationships between two sets of variables. The objective is
to find that linear combination of one set of variables which is most highly correlated
with any linear combination of a second set of variables. The technique has received
attention from theoretical statisticians as early as Bartlett (1938), but ter Braak
(1990) identifies that because of difficulty in interpretation of the canonical variates,
its use is not widespread in some disciplines. By exploring the connection between
the reduced-rank regression and canonical correlations, we provide an additional
method of interpreting the canonical variates. Because the role of any descriptive
tool such as canonical correlations is in better model building, the connection we
display below provides an avenue for interpreting and further using the canonical
correlations.
More formally, in canonical correlation analysis, for random vectors Xk and Yk
of dimensions n and m, respectively, the purpose is to obtain an r × n matrix G and
an r × m matrix H, so that the r-vector variates ξk = GXk and ωk = H Yk will
capture as much of the covariances between Xk and Yk as possible. The following
results from Izenman (1975) summarize how this is done.
Theorem 2.3 We assume the same conditions as given in Theorem 2.2 and further
we assume yy is nonsingular. Then the r × n matrix G and the r × m matrix H,
with H yyH = Ir, that minimize simultaneously all the eigenvalues of
E[(H Y − GX)(H Y − GX)
] (2.27)
are given by2.4 Relation to Principal Components and Canonical Correlation Analysis 43
G = V 
∗ −1/2
yy yx −1
xx , H = V 
∗ −1/2
yy ,
where V∗ = [V ∗
1 ,...,V ∗
r ] and V ∗
j is the (normalized) eigenvector corresponding to
the j th largest eigenvalue ρ2
j of the matrix
R∗ = −1/2
yy yx −1
xx xy −1/2
yy . (2.28)
Denoting by g
j and h
j the j th rows of the matrices G and H, respectively,
the j th pair of canonical variates is defined to be ξj = g
jXk and ωj = h
jYk,
j = 1, 2,...,m (m ≤ n). The correlation between ξj and ωj is the j th canonical
correlation coefficient, and hence, since Cov(ξj , ωj ) = V ∗
j R∗ V ∗
j = ρ2
j , Var(ξj ) =
V ∗
j R∗V ∗
j = ρ2
j , and Var(ωj ) = V ∗
j V ∗
j = 1, we have Corr(ξj , ωj ) = ρj and
the ρj s are the canonical correlations between Xk and Yk. The canonical variates
possess the property that ξ1 and ω1 have the largest correlation among all possible
linear combinations of Xk and Yk, ξ2 and ω2 have the largest possible correlation
among all linear combinations of Xk and Yk that are uncorrelated with ξ1 and ω1,
and so on.
We shall compare the results of Theorem 2.3 with those of Theorem 2.2. When
 = −1
yy , the matrix B(r)
∗ = V 
∗ 1/2 yx −1 xx from (2.13) is identical to G and
H = V 
∗ −1/2
yy is equal to A(r)− ∗ , a reflexive generalized inverse of A(r)
∗ = 1/2
yy V∗
Rao (1973, p. 26). The transformed variates ξ (r)
k = B(r)
∗ Xk and ω(r)
k = A(r)− ∗ Yk have
V 
∗R∗V∗ = 2
∗ ≡ diag(ρ2
1 ,...,ρ2
r ) as their cross-covariance matrix. The matrix R∗
is a multivariate version of the simple squared correlation coefficient between two
variables (m = n = 1) and also of the squared multiple correlation coefficient
between a single dependent variable (m = 1) and a number of predictor variables.
The model, with the choice of  = −1
yy , is referred to as the reduced-rank model
corresponding to the canonical variates situation by Izenman (1975). Although
a number of choices for  are possible, we want to comment on the choice of
 = −1  , which leads to maximum likelihood estimates (see Robinson (1974)
and Section 2.3). Because  = yy − yx−1 xx xy , the correspondence with
results for the choice  = −1
yy is fairly simple. In the first choice of  = −1
yy ,
the squared canonical correlations are indeed the eigenvalues of yx−1 xx xy with
respect to yy. For the choice of  = −1  , we compute eigenvalues, say λ2
j , of the
same matrix yx−1 xx xy but with respect to yy − yx−1 xx xy, that is, roots of
the determinantal equation |λ2
j (yy − yx−1 xx xy ) − yx−1 xx xy | = 0. Since the
ρ2
j for the first choice  = −1
yy satisfy |ρ2
j yy − yx−1 xx xy| = 0, algebraically
it follows that
ρ2
j = λ2
j /(1 + λ2
j ). (2.29)
The canonical variates can be recovered from the calculation of eigenvalues and
eigenvectors for the choice  = −1  through44 2 Reduced-Rank Regression Model
ξ (r) = B(r)Xk = V  −1/2  yx −1
xx Xk and ω(r) = A(r)−Yk = V  −1/2  Yk,
where V = [V1,...,Vr], λ2
j and Vj are the eigenvalues and (normalized)
eigenvectors, respectively, of the matrix −1/2  yx−1 xx xy−1/2  , and λj =
ρj /(1 − ρ2
j )1/2, where ρj is the j th largest canonical correlation. It must be noted
that the corresponding solutions from (2.13) for the choice  = −1  satisfy
A(r) = 1/2  V = 1/2  1/2  −1/2
yy V∗(Ir −2
∗)−1/2 =  −1/2
yy V∗(Ir −2
∗)−1/2
and B(r) = V  −1/2  yx −1 xx = (Ir − 2
∗)−1/2 V 
∗ −1/2
yy yx −1 xx because
V∗ = 1/2
yy −1/2  V (Ir − 2
∗)1/2 and so V = 1/2  −1/2
yy V∗ (Ir − 2
∗)−1/2, where
V∗ = [V ∗
1 ,...,V ∗
r ] and the V ∗
j are normalized eigenvectors of the matrix R∗ in
(2.28). Also notice then that
A(r) = −1/2
yy V∗(Ir − 2
∗)
−1/2
= (yy − yx −1
xx xy ) −1/2
yy V∗ (Ir − 2
∗)
−1/2
= 1/2
yy (Im − −1/2
yy yx−1
xx xy −1/2
yy ) V∗ (Ir − 2
∗)
−1/2
= 1/2
yy V∗ (Ir − 2
∗)
1/2
and B(r) = (Ir −2
∗)−1/2 V 
∗ −1/2
yy yx −1 xx , whereas the solutions of A and B for
the choice  = −1
yy are given by A(r)
∗ = 1/2
yy V∗ and B(r)
∗ = V 
∗ −1/2
yy yx −1 xx .
Thus, compared to these latter solutions, only the columns and rows of the solution
matrices A and B, respectively, are scaled differently for the choice  = −1  , but
the solution for the overall coefficient matrix remains the same with A(r)B(r) =
A(r)
∗ B(r)
∗ .
The connection described above between reduced-rank regression quantities in
Theorem 2.2 and canonical correlation analysis in the population also carries over
to the sample situation of ML estimation of the reduced-rank model. In particular,
let ρ2
j denote the sample squared canonical correlations between the Xk and Yk,
that is, the eigenvalues of R
∗ = −1/2
yy yx −1 xx xy −1/2
yy , where yy = 1
T YY
,
yx = 1
T YX
, and xx = 1
T XX
, and let V
∗
j denote the corresponding (normalized)
eigenvectors, j = 1,...,m. (Recall from the remark in Section 1.2 that, in practice,
in the regression analysis the predictor and response variables Xk and Yk will
typically be centered by subtraction of appropriate sample mean vectors before
the sample canonical correlation calculations above are performed.) Then the ML
estimators of A and B in the reduced-rank model can be expressed in an equivalent
form to (2.17) as
A
(r) =  −1/2
yy V
∗(Ir − 2
∗)
−1/2 = 1/2
yy V
∗(Ir − 2
∗)
1/2,
B
(r) = (Ir − 2
∗)
−1/2 V

∗ −1/2
yy yx −1
xx ,2.5 Asymptotic Distribution of Estimators in Reduced-Rank Model 45
with V
∗ = [V
∗
1 ,..., V
∗
r ], 2
∗ = diag(ρ2
1 ,..., ρ2
r ), and  = yy − yx−1 xx xy .
Through the linkage between the two procedures, we have demonstrated how
the descriptive tool of canonical correlation can also be used for modeling and
prediction purposes. Such descriptive tools are not easily available for extended
models considered in the book.
2.5 Asymptotic Distribution of Estimators in Reduced-Rank
Model
The small sample distribution of the reduced-rank estimators associated with the
canonical correlations, although available, is somewhat difficult to use. Therefore,
we focus on the large sample behavior of the estimators and we derive in this section
the asymptotic theory for the estimators of the parameters A and B in model (2.11).
The asymptotic results follow from Robinson (1973). From Theorem 2.2, for any
positive-definite , the population quantities can be expressed as
A = −1/2[V1,...,Vr] = −1/2V = [α1,...,αr], (2.30a)
B = V  1/2 yx −1
xx = [β1,...,βr]

. (2.30b)
The estimators of A and B, from (2.17), corresponding to these populations
quantities are
A
= −1/2[V
1,..., V
r] = −1/2V
 = [α1,...,αr], (2.31a)
B
 = V
 1/2 yx −1
xx = [β
1,..., β
r]

, (2.31b)
where V
j is the normalized eigenvector corresponding to the j th largest eigenvalue
λ2
j of 1/2 yx−1 xx xy 1/2, with yx = (1/T )YX and xx = (1/T )XX
. We
assume that xx = lim
T →∞
1
T
XX exists and is positive definite, and yx = Cxx . In
the following result, we assume  = −1  is known, for convenience. The argument
and asymptotic results for estimators A
 and B
 would be somewhat affected by the
use of  = −1  , where  = (1/T )(YY
−YX
(XX
)−1XY
), mainly because the
normalization restrictions on A
then depend on −1  . These effects will be discussed
subsequently. Recall that for the choice of  = −1  the estimators A
 and B
 in
(2.31) are the maximum likelihood estimators of A and B under the assumption of
normality of the error terms k (Robinson (1974, Theorem 1), and Section 2.3).
Our main result on the asymptotic distribution of A
 and B
 when  = −1  is
assumed to be known and the predictor variables are non-stochastic is contained in
the next theorem.46 2 Reduced-Rank Regression Model
Theorem 2.4 For the model (2.11) where the k satisfy the usual assumptions, let
(vec(A)
, vec(B)
) ∈ , a compact set defined by the normalization conditions
(2.14). Then, with A
, B
 and A, B given by (2.31) and (2.30), respectively, vec(A) 
and vec(B)  converge in probability to vec(A) and vec(B), respectively, as T → ∞.
Also, the vector variates T 1/2(αj − αj ) and T 1/2(β
j − βj ) (j = 1,...,r) have a
joint limiting distribution as T → ∞, which is singular multivariate normal with
null mean vectors and the following asymptotic covariance matrices:
E[T (αj − αj )(α − α)

] →
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
m
i=j=1
λ2
j + λ2
i
(λ2
j − λ2
i )2 αiα
i j = 
− λ2
j + λ2

(λ2
j − λ2
)2 αα
j j = ,
E[T (β
j − βj )(β
 − β)

] →
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
r
i=j=1
3λ2
j − λ2
i
(λ2
j − λ2
i )2 βiβ
i + −1
xx j = ,
− λ2
j + λ2

(λ2
j − λ2
)2 ββ
j j = ,
E[T (αj − αj )(β
 − β)

] →
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
2λ2
j
r
i=j=1
1
(λ2
j − λ2
i )2 αiβ
i j = ,
− 2λ2

(λ2
j − λ2
)2 αβ
j j = ,
where the λ2
j , j = 1,...,r, are the nonzero eigenvalues, assumed to be distinct, of
the matrix 1/2yx−1 xx xy1/2, with  = −1  .
Proof With the use of perturbation expansion of matrices (Izenman 1975), the
eigenvectors V
j of the matrix M = −1/2  yx −1 xx xy −1/2  can be expanded
around the eigenvectors Vj of N = −1/2  yx −1 xx xy −1/2  to give
V
j ∼ Vj +m
i=j
1
(λ2
j − λ2
i )
Vi[V 
j (M − N )Vi] (j = 1, . . . , r). (2.32)
Then since xx and yx converge in probability to xx and yx (Anderson 1971,
p. 195), M converges to N, and hence V
j converges in probability to Vj as T → ∞.
Consistency of A
and B
 then follows directly from the relations (2.30) and (2.31).
To establish asymptotic normality, using xy = xxC + T −1 T
k=1 Xk
k, we
can write
yx−1
xx xy − yx−1
xx xy = CUT + U
T C + U
T −1
xx UT + C(xx − xx )C
,2.5 Asymptotic Distribution of Estimators in Reduced-Rank Model 47
where UT = T −1 T
k=1 Xk
k. Hence, following Robinson (1973), from the
perturbation expansion in (2.32), we have (based on the first two terms, as predictors
are assumed to be non-stochastic)
T 1/2(V
j − Vj ) = T 1/2m
i=j
1
(λ2
j − λ2
i )
Vi[(V 
i −1/2  ⊗ V 
jVB)
+ (V 
j−1/2  ⊗ V 
i VB)] vec(UT ) + Op(T −1/2)
≡ T 1/2D
j vec(UT ) + Op(T −1/2) (j = 1, . . . , r), (2.33)
where we have used the relation vec(ABC) = (C ⊗ A) vec(B) and the fact that
−1/2  C = −1/2  AB = V B.
Define D = [D1,...,Dr], so that T 1/2 vec(V
 − V ) ∼ T 1/2D vec(UT ).
Then since T 1/2 vec(UT ) = vec(T −1/2 T
k=1 Xk
k) converges in distribution to
N (0,  ⊗ xx ) as T → ∞ (Anderson 1971, p. 200), the asymptotic distribution
of T 1/2 vec(V
 − V ) is N (0, G), where G = D
( ⊗ xx )D has blocks Gj =
D
j ( ⊗ xx )D, with
Gjj = m
i=j=1
λ2
j + λ2
i
(λ2
j − λ2
i )2 ViV 
i (j = 1, . . . , r),
Gj = − λ2
j + λ2

(λ2
j − λ2
)2 VV 
j (j = ),
if we note that V 
i Vj = 0 for i = j , V 
i Vi = 1, and BxxB = 2. The asymptotic
distribution of T 1/2 vec(A
− A) follows easily from the above and relation (2.31)
with T 1/2 vec(A
− A)  T 1/2(Ir ⊗ 1/2  ) vec(V
 − V ) converging in distribution
to N{0, (Ir ⊗ 1/2  )G(Ir ⊗ 1/2  )}. Hence, if we note the relation 1/2  Vi = αi, the
individual covariance terms that correspond to the above distribution are as given in
the theorem.
To evaluate the asymptotic distribution of T 1/2 vec(B
 − B
), observe that
T 1/2 vec(B
 − B
) = T 1/2 vec(−1
xx xy −1/2  V
 − −1
xx xy −1/2  V )
 T 1/2 vec{−1
xx xy −1/2  (V
 − V )}
+ T 1/2 vec{(−1
xx xy −1/2  − −1
xx xy −1/2  )V }
= T 1/2(Ir ⊗ −1
xx xy −1/2  ) vec(V
 − V )
+ T 1/2(V 
−1/2  ⊗ −1
xx ) vec
T −1
T
k=1
Xk
k
48 2 Reduced-Rank Regression Model
 {(Ir ⊗ B
V 
)D + (V 
−1/2  ⊗ −1
xx )}T 1/2 vec(UT ),
(2.34)
where we again have used the fact that −1/2  yx−1 xx = −1/2  C = V B. Hence
T 1/2 vec(B
 − B
) converges in distribution to N (0,H), where
H = (Ir ⊗ B
V 
) D ( ⊗ xx ) D (Ir ⊗ VB) + (Ir ⊗ −1
xx )
+ (Ir ⊗ B
V 
) D ( ⊗ xx )(−1/2  V ⊗ −1
xx )
+ (V 
−1/2  ⊗ −1
xx )( ⊗ xx ) D (Ir ⊗ V B), (2.35)
and we used the fact that V 
V = Ir.
The elements of the matrix H can be evaluated more explicitly as follows: the
(j, )th block of (Ir ⊗ B
V 
)D
( ⊗ xx )D(Ir ⊗ VB) is
B
V 
GjV B =
#(λ2
j + λ2
i )(λ2
j − λ2
i )−2βi β
i, j = 
−(λ2
j + λ2
)(λ2
j − λ2
)−2β β
j , j = ,
while the (j, )th block of (Ir ⊗ B
V 
)D
( ⊗ xx )(−1/2  V ⊗ −1 xx ) is
B
V 
D
j (1/2  V ⊗ In) =
#(λ2
j − λ2
i )−1βi β
i, j = 
(λ2
j − λ2
)−1β β
j , j = ,
where the sums are over i = 1, . . . , r, i = j . Combining terms and simplifying, we
obtain the results stated in the theorem.
Finally, from (2.33) and (2.34), asymptotic covariances between the elements of
T 1/2 vec(A
− A) and T 1/2 vec(B
 − B
) are given by the elements of the matrix
(Ir ⊗ 1/2  )D
( ⊗ xx )[D(Ir ⊗ VB) + (−1/2  V ⊗ −1
xx )].
The (j, )th block of this matrix can be determined explicitly in a manner similar
to the previous derivation for the blocks of the matrix H and can be shown to have
elements as given in the statement of the theorem.
Robinson (1974) develops the asymptotic theory for estimates of the coefficients
in a general multivariate regression model which includes as a special case the
reduced-rank regression model (2.5). The results presented here do not agree
with the distribution results of Robinson. The difference occurs in the distribution
of T 1/2 vec(B
 − B
). Robinson claims that T 1/2 vec(B
 − B
) has asymptotic
covariance matrix (Ir ⊗ −1 xx ). However, (Ir ⊗ −1 xx ) is the asymptotic covariance
matrix for the estimator B
 = V 
−1/2  yx−1 xx of B, which would be appropriate if
V , or equivalently A, were known. As can be seen from (2.34) and (2.35), the actual2.5 Asymptotic Distribution of Estimators in Reduced-Rank Model 49
asymptotic covariance matrix of the estimator B
 includes additional contributions
besides (Ir ⊗ −1 xx ), which correspond to the presence of the first term on the right￾hand side of (2.34).
When rank(C) = 1, that is, C = αβ
, the asymptotic distributions of the
estimators α and β
 implied by Theorem 2.4 agree with those of Robinson (1974).
The results agree in the rank one situation because of the asymptotic independence
of α, β
for this case; the first term on the right-hand side of (2.34) vanishes in this
case since (Ir ⊗ B
V 
)D = βV 
1D
1 = 0. This asymptotic independence does not
hold in general.
The asymptotic theory for the estimators A
 and B
 is useful for inferences
concerning whether certain components of Xk contribute in the formation of
explanatory indexes BXk and also whether certain components of Yk are influenced
by certain of these indices. Because of the connection between the component
matrices and the quantities involved in the canonical correlation analysis, inferences
regarding the latter quantities can also be made via Theorem 2.4. Anderson
(1999a,b) presents the asymptotic distribution of the canonical variates and also
of the reduced-rank regression estimators when predictors are stochastic and non￾stochastic. For the stochastic case, additional terms are needed for the asymptotic
distribution of α and β
in Theorem 2.4. As the details are quite involved, we do not
pursue them here.
An additional quantity of interest is the overall regression coefficient matrix C =
AB and the asymptotic distribution of the estimator C
 = A
B
 follows directly from
the results of Theorem 2.4 [see Stoica and Viberg (1996) and Lütkepohl (1993,
p. 199)]. It is easily observed, under conditions of Theorem 2.4, that C
is a consistent
estimator of C. The asymptotic distribution of C
 can be derived in the following
way. Based on the relation
T 1/2(C
 − C) = T 1/2(A
B
 − AB) = T 1/2(A
− A)B + T 1/2A(B
− B) + op(1),
it follows that
T 1/2 vec(C
 − C
) = T 1/2[(Im ⊗ B
), (A ⊗ In)]

vec(A
 − A
)
vec(B
 − B
)

+ op(1)
= T 1/2M

vec(A
 − A
)
vec(B
 − B
)

+ op(1),
where M = [(Im ⊗ B
), (A ⊗ In)]. Collecting the terms in Theorem 2.4 for the
variances and covariances in the limiting distribution of the components in the
vectors T 1/2(αj − αj ) and T 1/2(β
j − βj ), j = 1,...,r, we denote this covariance
matrix as
W = lim
T →∞ Cov 
T 1/2 vec(A
 − A
)
T 1/2 vec(B
 − B
)

=

W11 W12
W21 W22
,50 2 Reduced-Rank Regression Model
where W11 denotes the asymptotic covariance matrix of T 1/2 vec(A
 − A
), W12 is
the asymptotic covariance matrix between T 1/2 vec(A

−A
) and T 1/2 vec(B

−B
),
and W22 is the asymptotic covariance matrix of T 1/2 vec(B
 − B
); for example,
W22 = H, the expression in (2.35) from the proof of Theorem 2.4. It must be noted
that care is needed in identifying the elements of W11 and W12 because the results
in the proof of Theorem 2.4 refer to vec(A
− A) not vec(A
 − A
). The asymptotic
distribution of C
 now follows directly as
T 1/2 vec(C
 − C
) d
→ N (0,MWM
). (2.36)
When the rank of the matrix C is assumed to be full, the distribution in (2.36) is
equivalent to that in (1.15).
We want to demonstrate the equivalence between the asymptotic distribution of C

given in Anderson (1999a) and the distribution in (2.36). Here it should be observed
that the covariance matrix is W = B−
θ , which is a (reflexive) generalized inverse of
the asymptotic information matrix Bθ . Observer that
Bθ = M
(Σ−1  ⊗ Σ )M ≡ M
M =

Σ−1  ⊗ BΣB Σ−1  A ⊗ BΣ
A
Σ−1  ⊗ ΣB A
Σ−1  A ⊗ Σ
,
where M = (Σ−1/2  ⊗ Σ1/2  )M. Now, from a result in Searle (1971, p. 20), the
matrix quantity M(M
M)−M
= MB−
θ M
is invariant to the choice of generalized
inverse B−
θ of Bθ . Hence, it follows that MB−
θ M is also the same for all choices of
generalized inverse B−
θ . There are many convenient choices for B−
θ , but we consider
one given by Searle (1971, p. 27) for partitioned matrices. If we denote the blocks
of Bθ above as Fij , i, j = 1, 2, then this choice of B−
θ is given by
B−
θ =

F −1
11 + F −1
11 F12Q−F
12F −1
11 −F −1
11 F12Q−
−Q−F
12F −1
11 Q−

,
where Q = F22 − F
12F −1
11 F12 = {A
Σ−1  A}⊗[Σ − ΣxxB
{BΣxxB
}−1BΣxx ],
F −1
11 = Σ ⊗ {BΣxxB
}−1, and we can take a generalized inverse of Q as Q− =
{A
Σ−1  A}−1⊗Σ−1 xx . Therefore, the chosen generalized inverse of Bθ takes the form
B−
θ =

b11 b12
b21 b22
,
where
b11 = (Σ ⊗ {BΣxxB
}
−1) + A{A
Σ−1  A}
−1A ⊗ {BΣxxB
}
−1),
b12 = −A{A
Σ−1  A}
−1 ⊗ {BΣxxB
}
−1B,
b21 = −{A
Σ−1  A}
−1A ⊗ B
{BΣxxB
}
−1,
b22 = {A
Σ−1  A}
−1 ⊗ Σ−1 xx .2.5 Asymptotic Distribution of Estimators in Reduced-Rank Model 51
Thus, we obtain the asymptotic covariance matrix for the reduced-rank ML
estimator as
MB−
θ M = [Im ⊗ B
, A ⊗ In]Bθ [Im ⊗ B
, A ⊗ In]

= [Σ − A{A
Σ−1  A}
−1A
] ⊗ B
{BΣxxB
}
−1B
+ A{A
Σ−1  A}
−1A ⊗ Σ−1 xx .
Notice that this is the same result as given in (3.20) of the paper by Anderson
(1999a), apart from a reverse in the order of the Kronecker products which is due to
the convention used here for arranging the vector version of C by rows instead of
columns. Also this is an equivalent form of the asymptotic covariance matrix result
as given by Stoica and Viberg (1996).
The result in (2.36) for the reduced-rank estimator C
 can be compared with the
asymptotic covariance matrix of T 1/2 vec(C
 − C
) for the full-rank estimator C

(which ignores the reduced-rank structure). This covariance matrix is given by ⊗
−1 xx from Result 1.1 of Section 1.3. For the general rank r case, the covariance
matrix in (2.36) does not take a particularly convenient form for comparison with
that of the full-rank estimator. However, we consider the special case of r = 1
for which explicit results are easily obtained. For this case, we have already noted
that (Ir ⊗ B
V 
)D = 0 and so we see from (2.35) that W22 reduces to W22 =
H = −1 xx . It also follows that W12 = 0 and we can find that W11 = 1/2  (Im −
V1V 
1) 1/2  (β
xxβ)−1, with C = αβ
. Then using α = 1/2  V1, it follows that the
asymptotic covariance matrix in (2.36) for the rank one case simplifies to
MWM = {( − 1/2  V1 V 
11/2  ) ⊗ β(β xx β)−1 β
}
+ (1/2  V1 V 
1 1/2  ⊗ −1
xx )
= ( ⊗ −1
xx )
− {( − 1/2  V1 V 
1 1/2  ) ⊗ (−1
xx − β(β xx β)−1β
)}.
This expression can be directly compared with the corresponding covariance matrix
expression  ⊗ −1 xx of the full-rank estimator, and the reduction in covariance
matrix of the reduced-rank estimator is readily revealed for this special case.
One can also look at the effect of reduced-rank estimation on prediction. The
prediction error of the predictor Y
0 = CX 0 at X0 for full-rank LS estimation is
Y0 − CX 0 = 0 − (C
 − C)X0, so that the prediction error covariance matrix is
(1 + X
0(XX
)−1X0) . In the rank one case, using (2.36), the covariance matrix
of the prediction error Y0 −CX 0 = 0 −(C
−C)X0 ≡ 0 −(Im ⊗X
0) vec(C
 −C
)
using the reduced-rank estimator C
 = αβ
 is (approximately)52 2 Reduced-Rank Regression Model
 +
1
T (Im ⊗ X
0)MWM
(Im ⊗ X0)
= (1 + X
0(XX
)
−1X0) 
−

X
0(XX
)
−1X0 − (β
X0)2
(β
(XX
)β) 

 − 1/2  V1 V 
1 1/2  
.
So the prediction error covariance matrix is decreased (by the positive amount equal
to the second term on the right side above) by use of the reduced-rank estimator
relative to the full-rank LS estimator.
For the rank “r” case, the results, although are not explicit as in the rank one case,
can be shown to hold. The efficiency gained from the assumptions of low rank if it
is true can be demonstrated as follows: note that the asymptotic covariance matrix
of C
 can be written as
MWM = [Σ ⊗ Σ−1 xx ]−[(Σ − A(A
Σ−1  A)−1A
) ⊗ (Σ−1 xx − B
(BΣxxB
)
−1B)]
= [Σ ⊗ Σ−1 xx ] − )
(Σ − Σ1/2  V V 
Σ1/2  ) ⊗

Σ−1 xx −r
i=1
βiβ
i
β
iΣxxβi
* .
It is observed that the two matrices involved in the Kronecker product in the second
term are both positive definite. The covariance matrix of the prediction error at X =
X0 can be written as (approximately)
Σ +
1
T (Im ⊗ X
0)MWM
(Im ⊗ x0) = (1 + X
0(XX
)
−1X0)Σ
−
#
X
0(XX
)X0 −r
i=1
(β
iX0)2
β
i(XX
)βi
$!
Σ − Σ1/2  V V 
Σ1/2  "
.
It can be easily seen that the last two terms are indeed positive, and therefore there
is certainly a gain in using reduced-rank models.
Remark—Impact of Estimating Error Covariance Matrix We now discuss the
effect on asymptotic distribution results from the use of the estimator  = −1  in
construction of the estimators A
 and B
 in (2.31). Restricting consideration to the
rank one case, the perturbation methods applied in the proof of Theorem 2.4 can
also be used to obtain the asymptotic covariance matrix of estimators α and β
based
on −1  , subject to α
−1  α = 1. First, define M = −1/2  yx−1 xx xy−1/2  and
let V
1 be the normalized eigenvector of M corresponding to its largest eigenvalue
λ2
1. Similar to (2.32), we have the expansion of V
1 around V1 as2.5 Asymptotic Distribution of Estimators in Reduced-Rank Model 53
V
1 ∼ V1 +m
i=2
1
λ2
1
Vi[V 
1(M − N )Vi] ≡ V1 +
1
λ2
1
(Im − V1V 
1)(M − N )V1.
Then write M − N = (M − M) + (M − N ), where M − N is treated as in the proof
of Theorem 2.4, and
M − M = −1/2  yx −1
xx xy −1/2  − −1/2  yx −1
xx xy −1/2 
= (−1/2  − −1/2  ) yx −1
xx xy −1/2 
+ −1/2  yx −1
xx xy(−1/2  − −1/2  ) + Op(T −1).
Also notice that yx−1 xx xy = αβ
xxβα = λ2
1αα in the rank one case.
Because α = 1/2  V1 and C = αβ
, we have (Im−V1V 
1)−1/2  C = 0. Using this
and the approximation −1/2  −−1/2  = −−1/2  (1/2  −1/2  ) −1/2  +Op(T −1),
we obtain a representation analogous to (2.33) as
T 1/2(V
1 − V1) = 1
λ2
1
(Im − V1 V 
1)
!
−1/2  T 1/2 U
T β + λ2
1T 1/2 

−1/2  − −1/2  
α
"
= 1
λ2
1
	
(Im − V1V 
1) −1/2  ⊗ β

T 1/2 vec(UT )
− (Im − V1V 
1)−1/2  T 1/2 

1/2  − 1/2  
−1/2  α + Op(T −1/2).
The estimator of α = 1/2  V1 is α = 1/2  V
1, so that α − α = 1/2  (V
1 − V1) +
(1/2  − 1/2  )V1 + Op(T −1). Therefore, from above, we have
T 1/2(α − α) = 1
λ2
1
	
1/2  (Im − V1V 
1) −1/2  ⊗ β

T 1/2 vec(UT )
+ α α −1
 T 1/2 

1/2  − 1/2  
−1/2  α + Op(T −1/2)
= 1
λ2
1
	
1/2  (Im − V1V 
1) −1/2  ⊗ β

T 1/2 vec(UT )
+
1
2


α
−1
 ⊗ α α −1
 
T 1/2 vec( −  ), (2.37)
where we have used the relation (1/2  − 1/2  ) 1/2  + 1/2  (1/2  − 1/2  ) =
 −  + Op(T −1).
From Muirhead (1982, Chapter 3) and Schott (1997, Chapter 9), T 1/2 vec( −
 ) has asymptotic covariance matrix equal to 2D∗
m( ⊗  )D∗
m, where D∗
m =
Dm(DmD
m)−1D
m and Dm is the m2 × 1
2m(m + 1) duplication matrix such that
Dmvech( ) = vec( ). Here, vech( ) represents the 1
2m(m + 1) × 1 vector54 2 Reduced-Rank Regression Model
consisting of the distinct elements of  (i.e., those on and below the diagonal). In
addition, we know that T 1/2 vec(UT ) converges in distribution to N (0,  ⊗ xx),
and UT and  are asymptotically independent. These results can be used in (2.37)
to obtain the asymptotic covariance matrix of α as
T Cov(α − α) =

1
λ2
1
2
	
1/2  
Im − V1V 
1

−1/2  ⊗ β

( ⊗ xx )
×
	
−1/2  
Im − V1V 
1

1/2  ⊗ β

+
1
4


α −1
 ⊗ αα −1
 
× 2D∗
m( ⊗  ) D∗
m (−1
 α ⊗ −1
 αα
)
= 1
λ2
1
1/2  (Im − V1V 
1) 1/2  +
1
2
α α
,
since D∗
m(−1  α ⊗ −1  α) = −1  α ⊗ −1  α.
Next we consider the estimator β
 = V

1 −1/2  yx −1 xx = α −1  yx −1 xx .
From the approximation −1  α−−1  α = −1  (α−α)+(−1  −−1  )α+Op(T −1),
we obtain the representation
T 1/2(β
− β) = −1
xx xy {−1
 T 1/2(α − α) + T 1/2(−1
 − −1
 ) α}
+ T 1/2(−1
xx xy − −1
xx xy) −1
 α + Op(T −1/2)
= −1
xx xy {−1
 T 1/2(α − α) − −1
 T 1/2( −  )−1
 α}
+ T 1/2(−1
xx xy − −1
xx xy) −1
 α,
using the approximation −1  −−1  = −−1  ( − )−1  +Op(T −1). Thus,
from the approximation for T 1/2(α − α) given in (2.37), we have
T 1/2(β
− β) = −1
2 β α −1
 T 1/2( −  ) −1
 α
+ T 1/2(−1
xx xy − −1
xx xy) −1
 α + Op(T −1/2)
= −1
2 (α −1
 ⊗ β α −1
 )T 1/2 vec( −  )
+ (α −1
 ⊗ −1
xx ) T 1/2 vec(UT ). (2.38)
This representation yields the asymptotic covariance matrix for β
as2.6 Identification of Rank of the Regression Coefficient Matrix 55
T Cov(β
− β) = (α −1
 ⊗ −1
xx )( ⊗ xx )(−1
 α ⊗ −1
xx )
+
1
4 (α −1
 ⊗ β α −1
 ) 2D∗
m ( ⊗  ) D∗
m (−1
 α ⊗ −1
 α β
)
= −1
xx +
1
2 β β
.
Furthermore, the asymptotic cross-covariance matrix between α and β
 is also
obtained from the representations (2.37) and (2.38) as
T Cov(α − α, β
− β) = 1
λ2
1
[1/2  (Im − V1V 
1) −1/2  ⊗ β
]( ⊗ xx )
× (−1
 α ⊗ −1
xx ) − 1
4 (α −1
 ⊗ α α −1
 )
× 2D∗
m( ⊗  ) D∗
m (−1
 α ⊗ −1
 α β
)
= 1
λ2
1
[1/2  (Im − V1V 
1) −1/2  α ⊗ β
] −
1
2 α β = −1
2 α β
.
Therefore, we find that T 1/2{(α−α)
, (β
−β)
} has asymptotic covariance matrix
given by
W =

W11 W12
W21 W22
=

( − αα
)(β
xxβ)−1 + 1
2αα −1
2 aβ
−1
2βα −1 xx + 1
2ββ

.
Compared to the previous case where  = −1  is taken as known, the above
includes the additional term 1
2 δδ with δ = (α
, −β
)
. However, it is also readily
seen that the asymptotic covariance matrix, MWM
, where M = [Im ⊗ β, α ⊗ In],
of the overall regression coefficient matrix estimator C
 = αβ
 is not affected by
use of −1  instead of −1  , because Mδ = (α ⊗ β) − (α ⊗ β) = 0. Therefore,
whereas the asymptotic distribution of the individual component estimators α and β

is affected by use of −1  in place of −1  , that of the overall estimator C
 = αβ
 is
not changed.
2.6 Identification of Rank of the Regression Coefficient
Matrix
It is obviously important to be able to determine the rank of the coefficient matrix C
for it is a key element in the structure of the reduced-rank regression problem. The
asymptotic results are derived assuming that the rank condition is true. However,
the relationship between canonical correlation analysis and reduced-rank regression
allows one to check on the rank by testing if certain correlations are zero.56 2 Reduced-Rank Regression Model
Bartlett (1947) suggested T m
j=r+1 log(1 +λ2
j ) as the appropriate statistic for
testing the significance of the last (m − r) canonical correlations, where λj =
ρj /(1 − ρ2
j )1/2 and ρj is the j th largest sample canonical correlation between
the Yk and Xk. From the discussion presented in Section 2.4, it follows that if the
hypothesis that the last (m − r) population canonical correlations are zero is true,
then it is equivalent to assuming that rank(C) = r (strictly, that rank(C) ≤ r).
Bartlett’s statistic follows from the likelihood ratio method of test construction [see
Anderson (1951)], as we now indicate.
Under the likelihood ratio (LR) testing approach, it is easy to see, similar to
the results presented at the end of Section 1.3, that the LR test statistic for testing
rank(C) = r is λ = UT /2, where U = |S|/|S1|,
S = (Y − C
X)(Y − C
X)

, S1 = (Y − C
(r)X)(Y − C
(r)X)

,
S is the residual sum of squares matrix from fitting the full-rank regression model,
while S1 is the residual sum of squares matrix from fitting the model under the
hypothesis of the rank condition on C. It must be noted that C
 and C
(r) are related
as C
(r) = PC
, where P is an idempotent matrix of rank r. Specifically, recall from
Sections 2.3 and 2.4 that the ML estimates under the null hypothesis of rank(C) =
r are A
(r) = 1/2  V
, B
(r) = V
 −1/2  yx −1 xx , with C
(r) = A
(r)B
(r), where
V
 = [V
1,..., V
r] and the V
j are (normalized) eigenvectors of the matrix R
 =
−1/2  yx −1 xx xy −1/2  associated with the r largest eigenvaluesλ2
j , and  =
(1/T )S1 = (1/T )(Y − C
(r)X)(Y − C
(r)X)
. The unrestricted ML estimates are,
of course, C
 = yx −1 xx and  = (1/T )(Y − C
X)(Y − C
X) as derived in
Section 1.2. Notice that, since m
j=1 V
jV

j = Im, we have
C
 − C
(r) = (Im − 1/2  V
V
 −1/2  ) yx −1
xx
= 1/2 
⎡
⎣ m
j=r+1
V
jV

j
⎤
⎦ −1/2  yx −1
xx .
Therefore, we find that
1
T
S1 = 1
T (Y − C
X + (C
 − C
(r))X)(Y − C
X + (C
 − C
(r))X)

=  + 1/2 
⎡
⎣ m
j=r+1
V
jV

j
⎤
⎦ R

⎡
⎣ m
j=r+1
V
jV

j
⎤
⎦ 1/2 
=  + 1/2 
⎡
⎣ m
j=r+1
λ2
jV
jV

j
⎤
⎦ 1/2  , (2.39)2.6 Identification of Rank of the Regression Coefficient Matrix 57
where theλ2
j , j = r + 1,...,m, are the (m − r) smallest eigenvalues of R
. Hence,
using (2.39), the LR statistic is
λ =
|S1|
|S|
−T /2
=
|(1/T )S1|
| |
−T /2
=

| ||Im + m
j=r+1
λ2
jV
jV

j |
| |
−T /2
=
⎡
⎣ +m
j=r+1
(1 +λ2
j )
⎤
⎦
−T /2
.
Therefore, the criterion λ = UT /2 is such that
− 2 log(λ) = T m
j=r+1
log(1 +λ2
j ) = −T m
j=r+1
log(1 − ρ2
j ), (2.40)
since 1 + λ2
j = 1/(1 − ρ2
j ). This statistic, −2 log(λ), follows asymptotically
the χ2
(m−r)(n−r) distribution under the null hypothesis [see Anderson (1951,
Theorem 3)]. The test statistic is asymptotically equivalent to T m
j=r+1
λ2
j ≡
T tr{−1  (C
 − C
(r)) xx (C
 − C
(r))
}, which follows the above χ2 distribution
and this result was obtained independently by Hsu (1941). A simple correction
factor for the LR statistic in (2.40), to improve the approximation to the
χ2
(m−r)(n−r) distribution, is given by −2{[T − n + (n − m − 1)/2]/T }log(λ) =
−[T − n + (n − m − 1)/2]
m
j=r+1 log(1 − ρ2
j ). This is similar to the corrected
form used in (1.16) for the LR test in the classical full-rank model.
Thus, to specify the appropriate rank of the matrix C, we use the test statistic
M = −[T − (m + n + 1)/2]
m
j=r+1 log(1 − ρ2
j ), for r = 0, 1,...,m − 1, and
reject H0 : rank(C) = r when M is greater than an upper critical value determined
by the χ2
(m−r)(n−r) distribution. The smallest value of r for which H0 is not rejected
provides a reasonable choice for the rank. Additional guidance in selecting the
appropriate rank could be obtained by considering the LR test of H0 : rank(C) = r
versus the more refined alternative hypothesis H1 : rank(C) = r + 1. This test
statistic is given by −T log(1 − ρ2
r+1).
Alternative procedures for testing the rank of a matrix in more general settings
have been proposed by Cragg and Donald (1996) based on the LDU decomposition
of the unrestricted estimate of the coefficient matrix and by Kleibergen and Paap
(2006) using the singular value decomposition of the coefficient matrix. Recall that
the rank test proposed in (2.40) is based on the singular value decomposition of
Σ−1/2  CΣ 1/2  , a weighted coefficient matrix. The weights come from the covariance
matrix of vec(C

), which has a Kronecker product structure. In many applications
of reduced-rank models, such a structure is not possible; therefore, these tests can
be applied to more general models. Other useful tools for the specification of the
rank include the use of information-theoretic model selection criteria, such as the
AIC criterion of Akaike (1974) and the BIC criterion of Schwarz (1978), or cross-58 2 Reduced-Rank Regression Model
validation methods (Stone 1974) based on measures of the predictive performance
of models of various ranks.
The importance of estimating the rank correctly and its associated bias-variance
tradeoff are explored in detail later in Section 10.4.3 in the book. Anderson (2002)
considers the properties of the reduced-rank estimator when the rank is not specified
correctly. If the specified rank is less than the true rank, the estimator is likely to be
biased, and when it is higher, the estimator tends to have larger variance. Later in
the book, we demonstrate how a few outliers can distort the true rank.
Remark: Relation of LR Tests Between Known and Unknown Constraints We
briefly compare the results relating to the LR test for specified constraints on the
regression coefficient matrix C (see Section 1.3) with the above LR test results
for the rank of C, that is, for a specified number of constraints, but where these
constraints are not known a priori. From Section 1.3, the LR test of H0 : F1C = 0,
where F1 is a specified (known) (m − r) × m full-rank matrix, is obtained as M =
[(T −n)+ 1
2 (n−m1−1)]
m1
i=1 log(1+λ2
i ), with m1 = m−r, whereλ2
i are the m−r
eigenvalues of S−1 ∗ H∗ = (F1SF
1)−1F1C
XX
C

F
1 ≡ (F1F
1)−1F1C
xxC

F
1.
The asymptotic distribution of M under H0 is χ2
(m−r)n. In the case where F1 is
unknown, F1C = 0 (for some F1) is equivalent to the hypothesis that the rank
of C is r. As has been shown in this section, the LR test for this case is given
by M = [(T − n) + 1
2 (n − m − 1)]
m
j=r+1 log(1 + λ2
j ), where λ2
j are the
m − r smallest eigenvalues of R
 = −1/2  C
xx C
 −1/2  , and the asymptotic
distribution of M is χ2
(m−r)(n−r). In addition, the corresponding ML estimate of
the unknown constraints matrix F1 = [r+1,...,m]
 in F1C = 0 is given by
F
1 = [V
r+1,..., V
m]
 −1/2  , where V
j are the normalized eigenvectors of R

associated with the λ2
j . Hence we see a strong similarity in the form of the LR
test statistic results, where in the case of unknown constraints the LR statistic can
be viewed as taking the same form as in the known constraints case, but with
the constraints estimated by ML to have a particular form. (Note that if the ML
estimate F
1 is substituted for F1 in the expression S−1 ∗ H∗ for the case of known F1,
this expression reduces to [V
r+1,..., V
m]
 −1/2  C
xx C
 −1/2  [V
r+1,..., V
m].)
Because the constraints are estimated, the degrees of freedom of (m − r)n in the
known constraints case are adjusted to (m − r)(n − r) in the case of unknown
constraints, a decrease of (m − r)r. This reflects the increase in the number of
unknown (unconstrained) parameters from rn in the known constraints case to
r(m + n − r) in the unknown constraints case.
The correspondence between correlation and regression is known to be fully
exploited in the modeling of data in the case of multiple regression. The results
above demonstrate the usefulness of the canonical correlations as descriptive
statistics in aiding the multivariate regression modeling of large data sets. However,
we shall demonstrate in the future chapters that such a neat connection does not
always appear to hold in such an explicit way for more complex models, but
nevertheless canonical correlation methods can still be used as descriptive tools and
in model specification for these more complicated situations.2.7 Reduced-Rank Inverse Regression for Estimating Structural Dimension 59
2.7 Reduced-Rank Inverse Regression for Estimating
Structural Dimension
Bura and Cook (2001) consider a more general nonnormal distributional setting
in which study of the conditional distribution of Yk given Xk is of interest, and
the conditional distribution is assumed to depend on Xk only through the r linear
combinations X∗
k = BXk. The linear subspace generated by the rows of B is then
referred to as a dimension reduction subspace for the conditional distribution of
Yk given XK, and r is called the structural dimension. Our previous discussions
related to model (2.5) can be viewed as the more specialized situation that focuses
on the conditional mean or regression function E(Yk | Xk), which is assumed to
be a linear function of X∗
k = BXk. In the more general setting, if the dimension
reduction subspace can be identified, then simplifications occur in the study of the
conditional distribution of Yk given Xk, such as use of summary plots of Yk versus
BXk as sufficient graphical displays of all the necessary information.
Consider the standardized version of Xk, Zk = Σ−1/2 xx [Xk − E(Xk)]. Then Bura
and Cook (2001) show that examination of the inverse regression function E(ZkYk)
can be used to help determine the dimension and the basis vectors of the dimension
reduction subspace. They use parametric multivariate linear regression to represent
the inverse regression function as
E(Zk | Yk) = GFk, (2.41)
where Fk = [f1(Yk), f2(Yk), . . . , fq (Yk)]
 is q-dimensional, G is n×q, and the fi
are q arbitrary real-valued linearly independent known functions of Yk, e.g., such as
powers of the components of Yk. If q is chosen sufficiently large so that the linear
model gives a valid representation of E(Zk | Yk) and the distribution of Xk satisfies
a certain linearity condition, then the theory from dimension reduction subspaces
indicates that the coefficient matrix G in the inverse regression (2.41) will have
reduced rank r∗ ≤ r and that E(Zk | Yk) = GFk belongs to the linear subspace
generated by the r columns of Σ1/2 xx B
. This yields that r∗ = rank(G) provides a
lower bound on the dimension of the dimension reduction subspace and that G has
the reduced-rank factorization G = Σ1/2 xx B
D ≡ A∗D for an r × q matrix D.
Therefore, it follows that the usual reduced-rank estimation and testing pro￾cedures can be applied to the inverse regression model (2.41) to determine the
structural dimension r and the basis vectors (rows) of B. Specifically, define the data
matrices F = [F1,..., FT ] and Z = [Z1,...,ZT ], where Zk = Σ−1/2 xx (Xk − X),
k = 1,...,T . Then let
G = ZF
(FF
)
−1 ≡ Σzf Σ−1
ff and Σz|y = Σzz − Σzf Σ−1
ff Σf z
be the usual LS estimate of G and corresponding residual covariance matrix associ￾ated with the inverse linear regression model (2.41). Also, note that Σ1/2 xx Σzf = Σxf60 2 Reduced-Rank Regression Model
and that Σ1/2 xx Σz|yΣ1/2 xx ≡ Σx|y , the residual covariance matrix from the LS
regression of Xk on Fk. Then as in the usual procedure, we consider the eigenvalues
λ2
j and (normalized) eigenvector Vj of the matrix
R = Σ−1/2
z|y Σzf Σ−1
ff Σf z Σ−1/2
z|y ≡ Σ−1/2
x|y Σxf Σ−1
ff Σf x Σ−1/2
x|y . (2.42)
Using a slightly different motivation, Bura and Cook (2001) suggested use of the
“trace” statistic T min(n,q)
j=r+1 λ2
j to test the reduced-rank hypothesis that rank(G) =
r, which is asymptotically distributed as χ2
(n−r)(q−r). From previous discussions,
however, this is asymptotically equivalent to the Gaussian-based LR test statistic
T min(n,q)
j=r+1 log(1 +λ2
j ) = −T min(n,q)
j=r+1 log(1 − ρ2
j ). Furthermore, as analogous
to results in Section 2.3, the reduced rank estimate of the factor A∗ = Σ1/2 xx B in
G = A∗D is given by A∗ = Σ1/2
z|y V(r), and hence an estimate of B is provided by
B = Σ−1/2 xx Σ1/2
z|y V(r) ≡ Σ−1 xx Σ1/2
x|y V(r), where V(r) = [V1,..., Vr]. Thus, the
required information is provided through use of the usual reduced-rank procedures
applied to the inverse regression model.
2.8 Numerical Examples
We illustrate the reduced-rank regression methods developed in this chapter using
the example on biochemical data and the sales performance data that were examined
in Section 1.5.
Biochemical Data As indicated in the analysis presented in Section 1.5.1, these
data suggest that there may be a reduced-rank structure for the regression coefficient
matrix (excluding the constant term) of the predictor variables Xk = (x1k, x2k, x3k)
.
To examine this feature, the rank of the coefficient matrix first needs to be deter￾mined. The LR test statistic M = −[(T −1)−(m+n+1)/2]
m
j=r+1 log(1−ρ2
j )
with n = 3, discussed in relation to (2.40), is used for r = 0, 1, 2, to test the rank,
that is, to test H0 : rank(C) ≤ r, and the results are given in Table 2.2. It would
appear from these results that the possibility that the rank is either one or two could
be entertained. Only estimation results for the rank 2 situation will be presented in
detail here.
Table 2.2 Summary of results for LR tests on rank of coefficient matrix for biochemical data
Eigenvalues Canonical Rank M = LR Critical
λ2
j Correlations ρj r Statistic d.f. Value (5%)
4.121 0.897 56.89 15 24.99
0.519 0.584 1 11.98 8 15.51
0.018 0.132 2 0.49 3 7.812.8 Numerical Examples 61
To obtain the maximum likelihood estimates, we find the normalized eigen￾vectors of the matrix R
 = −1/2  yx −1 xx xy −1/2  associated with the two
largest eigenvalues, λ2
1 and λ2
2, in Table 2.2. (Note that in constructing the sample
covariance matrices xx and xy, the variables Yk and Xk are adjusted for sample
means.) The normalized eigenvectors are
V

1 = (−0.290, 0.234, 0.025, 0.906, 0.199)
and
V

2 = (0.425, 0.269, −0.841, 0.047, 0.194),
respectively. We then compute the ML estimates from A
(2) = 1/2  V
(2) and
B
(2) = V

(2)
−1/2  yx−1 xx , with V
(2) = [V
1, V
2]. Using the results of Theorem 2.4,
the asymptotic standard errors of the elements of these matrix estimates are also
obtained. These ML estimates and associated standard errors that follow from
results in Section 2.5 are given below as
A
 =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
−1.085 0.333 0.231 0.397 1.109
(0.265) (0.042) (0.040) (0.013) (0.435)
1.335 −0.126 −0.266 −0.092 0.875
(0.655) (0.120) (0.085) (0.049) (1.246)
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
B
 =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1.361 −1.326 1.190
(0.352) (0.222) (0.405)
−0.908 0.494 1.240
(0.302) (0.179) (0.337)
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
To appreciate the recovery of information by using only two linear combinations
BX k of Xk = (x1k, x2k, x3k) to predict Yk, the reduced-rank matrix C
 = A
B
 can
be compared with the full-rank LS estimate C
 given in Section 1.5. Notice again that
we are excluding the first column of intercepts from this comparison. The reduced￾rank estimate of the regression coefficient matrix is
C
 =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
−2.6893 2.0981 0.3649
0.5679 −0.5040 0.2400
0.5558 −0.4374 −0.0555
0.6248 −0.5726 0.3583
0.7142 −1.0379 2.4044
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.62 2 Reduced-Rank Regression Model
Most of the coefficients of C
 that were found to be statistically significant are
recovered in the reduced-rank estimate C
 fairly well. The (approximate) unbiased
estimate of the error covariance matrix from the reduced-rank analysis (with
correlations shown above the diagonal) is given as
 = 1
33 − 3   =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
10.8813 −0.4243 −0.3090 −0.3243 −0.0841
−0.8638 0.3808 0.8952 0.6743 0.3452
−0.5204 0.2820 0.2606 0.6592 0.2912
−0.4743 0.1845 0.1492 0.1966 0.0320
−1.5012 1.1527 0.8045 0.0768 29.2890
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
where the residuals are obtained from the LS regression of Yk on BX k with intercept
included. The diagonal elements of the above matrix, when compared to the entries
of the unbiased estimate of the error covariance matrix under the full-rank LS
regression (given in Section 1.5), indicate that there is little or no loss in fit after
discarding one linear combination of the predictor variables.
For comparison purposes, corresponding ML results when the rank of the
coefficient matrix is assumed to be one can be obtained by using as regressor only
the (single) linear combination of predictor variables formed from the first row of
B
. The diagonal elements of the resulting (approximate) unbiased estimate of the
error covariance matrix from the rank one fit are given by 11.5149, 0.3773, 0.2913,
0.1949, and 28.7673. These values are very close to the residual variance estimates
of both the rank 2 and full-rank models, suggesting that the rank one model could
also provide an acceptable fit. Note that the single index or predictive factor in the
estimated rank one model is roughly proportional to x∗
1 ≈ (x1 + x3) − x2. Although
a physical or scientific interpretation for this index is unclear, it does have statistical
predictive importance. For illustration, Fig. 2.3 displays scatter plots of each of the
response variables yi versus the first predictive index x∗
1 = β

1X, with estimated
regression line. It is seen that variables y2, y3, and y4 have quite strong relationship
with x∗
1 , whereas the relationship with x∗
1 is much weaker for y5 and is of opposite
sign for y1.
Sales Performance Data The squared canonical correlations between the perfor￾mance and the test scores of the sales staff are 0.9945, 0.8781, and 0.3836. The LR
test to test for the rank(C) ≤ r is M = −453
j=r+1 log(1 − ρ2
j ), which results
in values, 276.4, 73.5, and 7.2, respectively, for r = 0, 1, 2. The chi-square critical
values at 5% are 21.0, 12.6, and 5.99 for 12, 6, and 2 d.f. This clearly suggests
that the rank of C matrix can be taken as two. The normalized eigenvectors are the
columns of the matrix,2.8 Numerical Examples 63
Fig. 2.3 Scatter plots of response variables y1,...,y5 for biochemical data versus first predictive
variable index x∗
1
V
 =
⎛
⎜
⎜
⎝
0.483 0.260 −0.807 −0.220
0.340 −0.469 0.263 −0.771
0.437 0.731 0.518 −0.075
0.678 −0.421 0.109 0.593
⎞
⎟
⎟
⎠ .
The A
and B
 component matrices when rank(C) = 2 follow:64 2 Reduced-Rank Regression Model
A
(2) = Σ1/2  [V
1, V
2] =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.262 0.404
(0.041) (0.222)
0.253 −0.381
(0.033) (0.168)
0.144 0.508
(0.019) (0.094)
1.033 −0.986
(0.044) (0.213)
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
B
(2) =

V

1
V

2

Σ−1/2  C
 =
⎛
⎜
⎜
⎝
0.597 0.200 0.749
(0.061) (0.040) (0.067)
0.323 −0.449 0.442
(0.059) (0.038) (0.065)
⎞
⎟
⎟
⎠ .
The product C
(2) = A
(2)
B(2)
, the reduced rank estimate of C, is given as
C
(2) =
⎛
⎜
⎜
⎝
0.287 −0.129 0.375
0.029 0.222 0.022
0.250 −0.199 0.332
0.299 0.650 0.339
⎞
⎟
⎟
⎠ .
It must be noted that the significant elements of the C
 matrix are recovered well
with the rank two approximation. Note that the complementary version of the
reduced-rank problem is estimating the linear constraint “l” so that l

C = 0; such a
constraint implies l

Yk ∼ l

ek, which is uncorrelated to the predictors. Note that
l

2 = V

3Σ−1/2  = (−0.2697, 0.1552, 0.3065, −0.0124) and l

1 = V

4Σ−1/2  =
(−0.0189, −0.3368, −0.0534, 0.0949). In general, the reasoning-related variables
seem to contrast with other tests. The scatterplots of Y ∗
k =l

Yk versus the predictor
variables Xk are given in Fig. 2.4. The plots clearly indicate how these combinations
are not related to the predictors.
2.9 Alternate Procedures for Analysis of Multivariate
Regression Models
Although we mainly focus on the reduced-rank multivariate regression models
in this book, it is helpful to briefly survey and discuss other related multivariate
regression modeling methodologies that have similar parameter reduction objectives
as reduced-rank regression, such as multivariate ridge regression, partial least
squares, joint continuum regression, envelope models, and other shrinkage and
regularization techniques. Some of these procedures are designed particularly for2.9 Alternate Procedures for Analysis of Multivariate Regression Models 65
x1 x2 x3
y1
* y2
*
90 100 110 90 100 110 120 95 100 105 110 115
−2
−1
0
1
2
−2
−1
0
1
2
y* Fig. 2.4 Scatterplot of Y ∗ =l

Y versus the predictor variables X
situations where there is a very large number of responses or predictors relative to
the sample size. This situation where m, n > T is addressed in the later chapters in
the context of reduced-rank regression and the big data, as well.
Multivariate Ridge Regression and Other Shrinkage Estimates
Consider the standard multivariate linear regression model as in (1.4), Y =
CX + , where Y = [Y1,...,YT ] is m × T , X = [X1,...,XT ] is n × T ,
C = [C(1),...,C(m)]
 is m × n,  = [1,...,T ], and the k are mutually
independent with E(k) = 0 and Cov(k) =  . The ordinary LS estimator of
C is C
 = YX
(XX
)−1, which can also be expressed in vector form as in (1.14),
c = vec(C

) = (Im ⊗ (XX
)
−1X) vec(Y
). (2.43)
As discussed in the introduction, Sclove (1971), among others, has argued that
a deficiency of the LS estimator C
 is that it takes no account of  , the error
covariance matrix. To remedy this shortcoming and also to take advantage of
possible improvements in the estimation of the individual C(i) even for the case
where  = σ2Im by using (univariate) ridge regression for that case, Brown
and Zidek (1980, 1982), Haitovsky (1987), and others suggested multivariate ridge
regression estimators of c = vec(C
).
The ridge regression estimator of c proposed by Brown and Zidek (1980) is of
the form66 2 Reduced-Rank Regression Model
c∗(K) = (Im ⊗ XX + K ⊗ In)
−1(Im ⊗ X) vec(Y
)
= (Im ⊗ XX + K ⊗ In)
−1(Im ⊗ XX
)c, (2.44)
where K is the m × m ridge matrix. Brown and Zidek (1980) motivated (2.44) as
a Bayes estimator, with prior distribution for c such that c ∼ N (0, V ⊗ In), where
V is an m × m covariance matrix, with K = V −1 in (2.44) (and  is treated
as known). By letting the choice of the ridge matrix K depend on the data Y in
certain ways, adaptive multivariate ridge estimators are obtained. Mean square error
properties of these adaptive and empirical Bayes estimators are compared, and their
dominance over the ordinary LS estimator (i.e., K = 0) is considered by Brown and
Zidek (1980).
Haitovsky (1987) considered a slightly more general class of multivariate ridge
estimators, developed by using a more general prior distribution for the regression
coefficients c of the form c ∼ N (1m ⊗ ξ , V ⊗ ), where V > 0 is the m × m
covariance matrix, except for scale factor, between any two columns of C, and  >
0 is the n × n covariance matrix, except for scale factor, between any two rows (i.e.,
C
(i) and C
(j )) in C. This leads to multivariate ridge estimators of the form
c∗ = (Im ⊗ XX + Kw ⊗ −1)
−1(Im ⊗ X) vec(Y
)
= (Im ⊗ XX + Kw ⊗ −1)
−1(Im ⊗ XX
)c, (2.45)
where Kw = V −1W and W = Im − (1
mV −11m)−11m1
mV −1, and the error
covariance matrix  is assumed known. Estimation for the more realistic situation
where  (and the prior covariance matrices V and ) are unknown was also
discussed by Haitovsky (1987), using multivariate mixed-effects and covariance
components model ML estimation methods. (The situation of unknown covariance
matrix was also considered by Brown and Zidek (1982), for their class of ridge
estimators.) Brown and Zidek (1980) and Haitovsky (1987) both noted particular
multivariate shrinkage estimators, such as those of Efron and Morris (1972), result
as special cases of their classes of multivariate ridge estimators. These estimators
do not seem to have been used extensively in practical applications, although Brown
and Payne (1975) used a specialized form of the estimators effectively in “election
night forecasting.”
Various other forms of estimators of the regression coefficient matrix C that
“shrink” the usual LS estimator C
, in the spirit of Stein and Efron–Morris type
shrinkage, were considered by Bilodeau and Kariya (1989), Honda (1991), and
Konno (1991), among others. These authors examined the risk properties of the
proposed classes of shrinkage estimators, and for certain risk functions they
presented sufficient conditions for these alternate estimators to have improved risk
relative to the LS (minimax) estimator.2.9 Alternate Procedures for Analysis of Multivariate Regression Models 67
Canonical Analysis Multivariate Shrinkage Methods
Different shrinkage methods for the multivariate linear regression model based
on classical canonical correlation analysis between Yk and Xk were proposed
by van der Merwe and Zidek (1980) and Breiman and Friedman (1997). In
some respects, these methods may be viewed as generalizations of reduced-rank
regression modeling. Recall, as shown in Sections 2.3 and 2.4, that the ML reduced￾rank estimator of the regression coefficient matrix C, of rank r < min(m, n), can
be expressed in terms of the canonical correlation analysis between Yk and Xk.
Specifically, the ML reduced-rank estimator can be written as
C
(r) = A
(r)
∗ B
(r)
∗ = 1/2
yy V
(r)
∗ V
(r) ∗ −1/2
yy C,  (2.46)
where C
 = YX
(XX
)−1 ≡ yx−1 xx is the ordinary LS estimator of C,
V
(r)
∗ = [V
∗
1 ,..., V
∗
r ], and the V
∗
j are normalized eigenvectors of the matrix
−1/2
yy yx−1 xx xy−1/2
yy associated with its r largest eigenvalues ρ2
j , the squared
sample canonical correlations between the Yk and Xk. Letting V
∗ = [V
∗
1 ,..., V
∗
m],
an m × m orthogonal matrix, H = V

∗−1/2
yy , and ∗ = Diag(1,..., 1, 0,..., 0),
an m×m diagonal matrix with ones in the first r diagonals and zeros otherwise, the
ML reduced-rank estimator in (2.46) can be expressed in the form
C
(r) = H−1∗HC.  (2.47)
As we have discussed previously in Sections 1.2 and 2.4, in practical application, the
predictor variables Xk and response variables Yk will typically already be centered
and be expressed in terms of deviations from sample mean vectors before the
canonical correlation and least squares calculations are performed.
In the canonical analysis-based multivariate shrinkage methods, the estimator of
C is formed as in (2.47) but with the diagonal matrix ∗ replaced by a diagonal
matrix  of “shrinkage factors” {d
j }. For example, in the generalized cross￾validation (GCV)-based approach of Breiman and Friedman (1997), the diagonal
elements of  used are given by
d
j = (1 − p)(ρ2
j − p)
(1 − p)2ρ2
j + p2(1 − ρ2
j )
, j = 1, . . . , m, (2.48)
where p = n/T is the ratio of the number n of predictor variables to the sample
size T . In practice, the positive-part shrinkage rule is used in that d
j is replaced by 0
whenever d
j < 0 (i.e., ρ2
j < n/T ) in (2.48). The form of the d
j in (2.48) is derived
by Breiman and Friedman (1997) so as to minimize the generalized cross-validation
prediction mean square error. The diagonal elements d
j may also be determined
by a fully cross-validatory procedure. In the method proposed earlier by van der
Merwe and Zidek (1980), the diagonal elements used for the matrix  in place of
∗ in (2.47) are68 2 Reduced-Rank Regression Model
fj =

ρ2
j − n − m − 1
T

/ρ2
j

1 − n − m − 1
T

,
with the positive-part rule also employed. Breiman and Friedman (1997) also
presented and discussed generalizations of their procedure, which combine the
canonical analysis shrinkage methods with principal components regression and
(univariate) ridge regression, especially useful for situations involving underdeter￾mined (n>T ) or poorly determined (n<T but large relative to T , e.g., n ≈ T )
samples. An alternate shrinkage procedure can also be motivated by the extended
reduced-rank regression model in Chapter 3 and this procedure will be discussed
explicitly therein.
Partial Least Squares
Partial least squares was introduced by Wold (1975) as an iterative computational
algorithm for linear regression modeling, especially for situations where the number
n of predictor variables is large. The multiple response (m > 1) version of partial
least squares begins with a “canonical covariance” analysis, a sequential procedure
to construct predictor variables as linear combinations of the original n-dimensional
set of predictor variables Xk. At the first stage, the initial predictor variable x∗
1k =
b
1Xk is determined as the first “canonical covariance” variate of the Xk, the linear
combination of Xk that maximizes
{(T − 1) × sample covariance of b
Xk and 
Yk} ≡ b
XY

over unit vectors b (and over all associated unit vectors ). For given vector b, we
know that  = YX
b/(b
XY
YX
b)1/2 is the maximizing choice of unit vector for ,
so that we then need to maximize b
XY
YX
b with respect to unit vectors b. Thus,
the first linear combination vector b1 is the (normalized) eigenvector of XY
YX
corresponding to its largest eigenvalue. (In the univariate case, m = 1 and Y is
1 × T and so this reduces to b1 = XY
/(YX
XY
)1/2 with associated “eigenvalue”
equal to YX
XY
.)
At any subsequent stage j of the procedure, there are j predictor variables x∗
ik =
b
iXk, i = 1,...,j , determined and the next predictor variable x∗
j+1,k = b
j+1Xk,
that is, the next unit vector bj+1 needs to be found. It is to be chosen as the vector
b to maximize the sample covariance of b
Xk and 
Yk (with unit vector  also
chosen to maximize the quantity), that is, we maximize b
XY
YX
b, subject to bj+1
being a unit vector and x∗
j+1,k = b
j+1Xk being orthogonal to the previous set of
j predictor variables x∗
ik, i = 1,...,j (i.e., b
j+1XX
bi = 0, for i = 1,...,j ).
The number w of predictors chosen until the sequential process is stopped is a
regularization parameter of the procedure; its value is determined through cross￾validation methods in terms of prediction mean square error. The final estimated2.9 Alternate Procedures for Analysis of Multivariate Regression Models 69
regression equations are constructed by ordinary LS calculation of the response
vectors Yk on the first w “canonical covariance” component predictor variables,
X∗
k = (x∗
1k,...,x∗
wk) ≡ BX k, where B
 = [b1,...,bw]

, that is, the estimated
regression equations are Y
k = AX ∗
k ≡ A
BX k with
A
= YX∗(X∗X∗)
−1 ≡ YX
B

(B
XX
B

)
−1
and X∗ = B
X is w×T . In practice, the predictor variables Xk and response variables
Yk will typically already be expressed in terms of deviations from sample mean
vectors, as we have discussed initially in Section 1.2.
Discussion of the sequential computational algorithms for the construction of the
“canonical covariance” components x∗
jk = b
jXk required in the partial least squares
regression procedure and of various formulations and interpretations of the partial
least squares method is given by Wold (1984), Helland (1988, 1990), Hoskuldsson
(1988), Stone and Brooks (1990), Frank and Friedman (1993), and Garthwaite
(1994). In particular, for univariate partial least squares, Garthwaite (1994) gives
the interpretation of the linear combinations of predictor variables, x∗
jk = b
jXk,
as weighted averages of predictors, where each individual predictor holds the
residual information in an explanatory variable that is not contained in earlier linear
combination components (x∗
ik,i < j), and the quantity to be predicted is the 1 × T
vector of residuals obtained from regressing yk on the earlier components. Also, for
the univariate case, Helland (1988, 1990) and Stone and Brooks (1990) showed that
the linear combination vectors B
 = [b1,...,bw]
 are obtainable from nonsingular
(w × w) linear transformation of [XY
, (XX
)XY
,...,(XX
)w−1XY
]

.
Joint Continuum Regression
Continuum regression, proposed by Stone and Brooks (1990) for the univariate
response case and extended by Brooks and Stone (1994) to the multivariate case,
may be viewed as a generalization of the method of partial least squares regression
modeling and also of principal components regression. Like partial least squares, it
is a sequential construction method, involving cross-validatory choice of certain
control parameters. At a given stage j , j predictor variables x∗
ik = b
iXk, i =
1,...,j , have been determined and the next predictor x∗
j+1,k = b
j+1Xk, that is,
the next linear combination vector bj+1, is chosen to maximize a certain criterion
Tα (defined below) that expresses the potentiality of x∗
j+1,k for predicting a similarly
maximized linear combination of the response vector Yk, subject to bj+1 being
of unit length and {x∗
j+1,k} being orthogonal to {x∗
1k,...,x∗
jk}. The choice of the
number w of predictor variables x∗
1k,...,x∗
wk and of the parameter α involved in
the criterion Tα can be made on the basis of cross-validatory indices constructed
from LS regression of Yk on X∗
k = (x∗
1k,...,x∗
wk)
. As in partial least squares, the
predictor variables Xk and response variables Yk may already be expressed in terms
of deviations from sample mean vectors.70 2 Reduced-Rank Regression Model
For 0 ≤ α < 1, the form of the criterion Tα to determine the vector b at a given
stage is
Tα = max


=1
{(T − 1) × sample covariance of b
Xk and 
Yk}
2
× (b
XX
b){α/(1−α)}−1. (2.49)
The first factor in (2.49) is an indicator of the predictive ability of the linear
combination b
Xk for the Yk, and the second factor is a power of the sum of squares
of the b
Xk relevant to principal components analysis of the Xk. The maximization
(with respect to  for given vector b) built into the criterion (2.49) is straightforward,
with the factor in brackets in (2.49) equal to {b
XY
}2, leading to the maximizing
unit vector  = YX
b/(b
XY
YX
b)1/2. Thus, with γ = α/(1 − α), this gives the
criterion as
Tα = (b
XY
YX
b)(b
XX
b)γ −1.
Numerical details of the method for determination of the vector b maximizing Tα
(subject to the orthogonality constraints) are discussed by Stone and Brooks (1990)
and Brooks and Stone (1994). As with partial least squares, the final estimated
regression equations are obtained by ordinary LS calculation of the response vectors
Yk on the first w constructed predictor variables, X∗
k = (x∗
1k,...,x∗
wk)
.
Brooks and Stone (1994) note that as α → 1 (so that γ → ∞ and hence the
criterion tends to be dominated by the second factor (b
XX
b)γ −1), the predictors
constructed by this method become the principal components of X. For the case
α = 0, the criterion becomes T0 = b
XY
YX
b/b
XX
b. For this case, no more
than m predictor variables x∗
jk = b
jXk are constructible; when m ≤ n<T ,
it follows that the vectors bj are directly related to the eigenvectors V
j of the
matrix YX
(XX
)−1XY corresponding to the largest eigenvalues, with b
j equal to
a normalized version of V

jYX
(XX
)−1. Hence, this case is equivalent to reduced￾rank regression calculations with  taken to be of the form σ2Im, the “least
squares” method used by Davies and Tso (1982) as discussed in Section 2.3. In
addition, the choice α = 1/2 in the criterion gives simply T1/2 = (b
XY
YX
b) and
leads to a modification due to de Jong (1993) of a version of partial least squares
regression determination. We note that for the univariate case, m = 1, Sundberg
(1993) established a direct relation between first factor continuum regression (i.e.,
using only the first w = 1 predictor x∗
1k = b
1Xk) and ridge regression and discussed
its implications.
Envelope Regression
Recall the general multivariate linear model as presented in (1.1),
Y = CX + ,2.9 Alternate Procedures for Analysis of Multivariate Regression Models 71
where Y is an m × 1 vector of response variables, X is an n × 1 vector of predictor
variables, C is an m × n regression coefficient matrix, and  is the m × 1 vector of
random errors, with mean vector E() = 0 and covariance matrix Cov() =  ,
an m × m positive-definite matrix. In the sequel, let A be the subspace spanned
by the columns of the matrix A, i.e., A = span(A). We use either PA or PA to
denote the projection matrix onto span(A) and also use either QA or QA to denote
the projection onto the orthogonal complement of A, i.e., QA = QA = I − PA =
I − PA.
Envelope method is a paradigm for dimension reduction introduced by Cook
et al. (2010). In the context of multivariate linear regression, the main idea is to
distinguish the “inmaterial” information in Y that is irrelevant to the association
between Y and X. This leads to a reduction of the dimensionality of the problem and
could potentially improve efficiency in model estimation and prediction. Consider a
subspace A ⊆ Rm that satisfies the following two conditions:
(I) QAY | X ∼ QAY,
(II) PAY ⊥ QAY | X.
Condition (I) indicates that the distribution of QAY does not depend on X and thus
does not carry “direct” information on C, while condition (II) ensures that, given X,
QAY cannot provide any “indirect” information on C through an association with
PAY . Putting together these two conditions, we know that all the information on
how X impacts Y (the information on C) is contained solely in PAY , while QAY
contains only the so-called immaterial information.
We can now proceed to further characterize QAY under the multivariate linear
regression model. Let C = span(C). Condition (I) holds if and only if C ⊆ A and
condition (II) holds if and only if A is a reducing subspace of Σ ; that is, A must
be decomposed as
Σ = PAΣPA + QAΣQA.
The intersection of all the subspaces with the above properties is, by construction,
the smallest reducing subspace of Σ that contains A, which is called the Σ -
envelope of C, denoted as EΣ(C).
Let Γ ∈ Rm×u be an orthogonal basis of EΣ(C), where u ≤ m is the dimension
of the envelope. Since C ⊆ span(Γ ), we have that C = Γ ξ for some ξ ∈ Ru×n.
The multivariate linear regression model can then be expressed in its envelope form
as follows:
Y = ΓξX + ,  ∼ N (0, Σ ), (2.50)
Σ = ΓΩΓ  + Γ0Ω0Γ 
0,
where [Γ, Γ0] ∈ Rm×m is orthogonal, and both Ω = Γ 
ΣΓ and Ω0 = Γ 
0 Σ Γ0
are positive definite.72 2 Reduced-Rank Regression Model
The idea of envelopes can be incorporated into reduced-rank regression, giving
rise to a hybrid reduced-rank envelope model (Cook et al. 2015). The resulting
estimator is at least as efficient as either the reduced-rank regression estimator or the
ordinary envelope estimator. To formulate, recall that in reduced-rank regression,
we allow that rank(C) = r ≤ min(m, n), so the coefficient matrix C can be
parameterized as
C = AB, A ∈ Rm×r
, B ∈ Rr×n,
where A and B are both of rank r. We recognize that EΣ(C) = EΣ(A), and the
dimension of the envelope u is always greater than or equal to the rank r because
dim{EΣ(C)} ≥ dim{span(C)} = rank(C) = r. The reduced-rank envelope model
then specifies that
C = AB = Γ ξ = Γ ηB, Σ = ΓΩΓ  + Γ0Ω0Γ 
0,
where Ω ∈ Ru×u and Ω0 ∈ R(m−u)×(m−u) are positive-definite matrices, and
η ∈ Ru×r is of rank r. When the envelope dimension u = m, there is no immaterial
information to be reduced, so the reduced-rank envelope model degenerates to the
reduced-rank regression model. When the coefficient matrix C is full rank, the
reduced-rank envelope model degenerates to the ordinary envelope model. The main
difference between the two approaches is that the information of the component
matrix, Γ , comes from both the conditional mean and the variance terms in the
envelope modeling.
To see the potential gain of utilizing reduced-rank regression and envelopes, we
can compare different models by the number of free parameters in the coefficient
matrix C and the error covariance matrix Σ ; see Table 2.3. The total number
of free parameters is reduced by (m − r)(n − r) from the least squares method
to reduced-rank regression and is further reduced by (m − u)r from reduced-rank
regression to reduced-rank envelope regression. As a numerical example, Cook et al.
(2015) applied reduced-rank envelope regression on the salesman data (m = 4 and
n = 3) presented in Section 2.8 and selected a model with r = 2 and u = 4.
Compared to reduced-rank regression, the standard deviations of the elements in
the reduced-rank envelope estimator were 1% to 24% smaller, where 24% smaller
standard deviation corresponds to a doubling of the observations for reduced￾Table 2.3 Comparing number of free parameters in the coefficient matrix and the error covari￾ance matrix
Method Number of free parameters
Least squares mn + m(m + 1)/2
Reduced-rank regression (m + n − r)r + m(m + 1)/2
Envelope regression nu + m(m + 1)/2
Reduced-rank envelope regression (n + u − r)r + m(m + 1)/22.9 Alternate Procedures for Analysis of Multivariate Regression Models 73
rank regression to achieve the same performance as its reduced-rank envelope
counterpart.
Cook et al. (2015) showed that once we know the envelope, the coefficient matrix
C can be obtained as the rank-r reduced-rank regression estimator from regressing
Γ 
Y on X. In general, the estimation of the envelope models can be conducted via
maximum likelihood under the normality assumption on . Various optimization
algorithms have been developed, including the one-direction-at-a-time algorithm
(Cook et al. 2010) and the faster envelope coordinate descent algorithm (Cook and
Zhang 2018). We refer to Cook (2018) for details and more recent developments.
In conclusion, we hope that the reduced-rank and alternate dimension reduction
tools for multivariate regression, as surveyed above, will develop further interest and
find broader use in the future as large volumes of data are routinely being collected
for analysis in many areas of application. A systematic comparison of these methods
in terms of their performance in capturing various features of the data would be
valuable.Chapter 3
Reduced-Rank Regression Models with
Two Sets of Regressors
3.1 Reduced-Rank Model of Anderson
In Chapter 2, we have demonstrated the utility of the reduced-rank model for
analyzing data on a large number of variables. The interrelationship between the
dependent and independent variables can be explained parsimoniously through the
assumption of a lower rank for the regression coefficient matrix. The basic model
that was described in Chapter 2 assumes that the predictor variables are all grouped
into one set, and therefore, they are all subject to the same canonical calculations.
In this chapter we broaden the scope of the reduced-rank model by entertaining
the possibility that the predictor variables can be divided into two distinct sets with
separate reduced-rank structures. Such an extension will be shown to have some
interesting applications.
An extension of the basic model to include more than one set of regressors was
already suggested in the seminal work of Anderson (1951). Anderson considered
the following model where the set of dependent variables is influenced by two sets
of regressor variables, one set having a reduced-rank coefficient matrix and the other
having a full-rank coefficient matrix. Formally, let
Yk = CXk + DZk + k, k = 1, 2,...,T, (3.1)
where Xk is a vector of regressor variables, C is of reduced rank, and Zk is a vector
of additional variables that influence Yk, but the matrix D is of full rank.
This model corresponds to a latent structure as mentioned by Robinson (1974).
The set of regressor variables Xk supposedly influences the dependent variables Yk
through a few unobservable latent variables, while the set of regressor variables Zk
directly influences Yk. That is, one might suppose the model Yk = AY ∗
k +DZk+Uk,
where Y ∗
k is an r1 × 1 vector of unobservable latent variables which influence Yk,
with Y ∗
k determined in part through a set of causal variables Xk as Y ∗
k = BXk +
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8_3
7576 3 Reduced-Rank Regression Models with Two Sets of Regressors
Vk. Then we obtain the reduced-form model equation as Yk = ABXk + DZk +
(AVk + Uk) = CXk + DZk + k with C = AB of reduced rank. An alternative
interpretation of the model is that Zk may contain important variables, and hence
the rank of its coefficient matrix is likely to be full, but Xk contains a large number
of potential explanatory variables and possibly the dimension of Xk can be reduced.
One elementary example of model (3.1) is its use to explicitly allow for a constant
term in the reduced-rank model of Chapter 2 for the “true” predictor variables Xk,
which can be done formally by taking Zk to be the scalar 1 with D being the m × 1
vector of constants in model (3.1).
The formulation of the model by Anderson was in terms of constraints as in (2.2)
acting upon the reduced-rank coefficient matrix C. The focus was on estimation of
these constraints and the inferential aspects of these estimated constraints. Because
our focus here is on the component matrices A and B, we shall outline only their
associated inferential results. In model (3.1), the vector of response variables Yk is
of dimension m × 1, Xk is of dimension n1 × 1, the vector of additional predictor
variables Zk is of dimension n2 × 1, and the k are m × 1 independent random
vectors of errors with mean 0 and positive-definite covariance matrix  , for k =
1, 2,...,T . Assume that
rank(C) = r1 ≤ min(m, n1), (3.2)
and hence, as in (2.4), C = AB where A is an m × r1 matrix and B is an r1 × n1
matrix. Hence the model (3.1) can be written as
Yk = ABXk + DZk + k, k = 1,...,T. (3.3)
Define the data matrices as Y = [Y1,...,YT ], X = [X1,...,XT ], and Z =
[Z1,...,ZT ].
Estimation of A and B in (3.3) and estimation of D follow from the population
result given below. The result is similar to Theorem 2.2 in Chapter 2.
Theorem 3.1 Suppose the (m + n1 + n2)-dimensional random vector (Y 
, X
, Z
)
has mean 0 and covariance matrix with yx = 
xy = Cov(Y, X), yz = 
zy =
Cov(Y, Z), and 
xx xz
zx zz 
= Cov 
X
Z

is nonsingular. Then for any positive￾definite matrix , an m×r1 matrix A and an r1 ×n1 matrix B, for r1 ≤ min(m, n1),
which minimize
tr{E[1/2(Y − ABX − DZ)(Y − ABX − DZ)
1/2]} (3.4)
are given by
A(r1) = −1/2[V1,...,Vr1 ] = −1/2V (r1)
,
B(r1) = V (r1)
1/2yx.z−1
xx.z
(3.5)3.1 Reduced-Rank Model of Anderson 77
and D = yz−1 zz − C(r1)
xz−1 zz with C(r1) = A(r1)
B(r1)
, where V (r1) =
[V1,...,Vr1 ] and Vj is the (normalized) eigenvector that corresponds to the j th
largest eigenvalue λ2
1j of the matrix 1/2yx.z−1 xx.zxy.z1/2 (j = 1, 2,...,r1).
It must be observed that yx.z = yx − yz−1 zz zx , xy.z = 
yx.z, xx.z =
xx − xz−1 zz zx are the partial covariance matrices obtained by eliminating the
linear effects of Z from both Y and X.
Proof With the following transformation,
X∗ = X − xz−1
zz Z, D∗ = D + Cxz−1
zz , (3.6)
we can write
Y − CX − DZ = Y − CX∗ − D∗Z, (3.7)
where X∗ and Z are orthogonal, that is, Cov(X∗,Z) = 0. The criterion in (3.4),
which must then be observed, is the same as
tr{E[1/2(Y − ABX∗ − D∗Z)(Y − ABX∗ − D∗Z)
1/2]}.
Because X∗ and Z are orthogonal, the above criterion can be decomposed into two
parts:
tr{E[1/2(Y − ABX∗)(Y − ABX∗)

1/2]}
− (tr{E[1/2Y Y 
1/2]} − tr{E[1/2(Y − D∗Z)(Y − D∗Z)
1/2]}).
The first part is similar to (2.12) in Theorem 2.2, and therefore the solutions for
A and B follow directly. To see this clearly, observe that yx∗ = yx.z and
x∗x∗ = xx.z are the partial covariance matrices. As far as the determination of
D∗ is concerned, observe that the part where D∗ appears in the above criterion is
simply the least squares criterion and therefore D∗ = yz−1 zz . The optimal value
of the solution for D is recovered from (3.6).
We shall now indicate several useful remarks in relation to the result of Theo￾rem 3.1. As in the reduced-rank regression model (2.11) without the Zk variables,
there are some implied normalization conditions on the component matrices B and
A in the solution of Theorem 3.1 as follows:
Bxx.zB = 2
1, A
A = Ir1 . (3.8)
The solutions A and B in (3.5) are related to partial canonical vectors and λ2
1j are
related to partial canonical correlations. For the most direct relation, the appropriate
choice of  is the inverse of the partial covariance matrix of Y , adjusted for Z,
 = −1
yy.z, where yy.z = yy − yz−1 zz zy . Then, λ1j = ρ1j represent the78 3 Reduced-Rank Regression Models with Two Sets of Regressors
partial canonical correlations between Y and X after eliminating Z. Another choice
for  is  = −1
yy.(x,z) = Σ−1  , and for this choice, it can be demonstrated, similar
to the derivation in Section 2.4, that the associated eigenvalues λ2
1j in Theorem 3.1
are related to the partial canonical correlations ρ1j as
λ2
1j = ρ2
1j /(1 − ρ2
1j ). (3.9)
The maximum likelihood estimates are obtained by substituting the sample
quantities in the solutions of Theorem 3.1 and by the choice of  = −1  , where
 ≡ yy.(x,z) = yy − [yx , yz]

xx xz
zx zz −1 
xy
zy 
(3.10)
is the estimate of the error covariance matrix  based on full-rank regression. In
(3.10), the estimates are defined to be yy = 1
T YY
, yx = 1
T YX
, yz = 1
T YZ
,
xx = 1
T XX
, xz = 1
T XZ
, and zz = 1
T ZZ
, where Y, X, and Z are data
matrices. This yields the ML estimates
A
(r1) = 1/2  V ,  B
(r1) = V

−1/2  yx.z−1
xx.z (3.11)
and D = yz−1 zz − C
(r1)
xz−1 zz with C
(r1) = A
(r1)
B
(r1)
, where V
 =
[V
1,..., V
r1 ] and the V
j are the (normalized) eigenvectors of the matrix
R
 = −1/2  yx.z−1
xx.zxy.z−1/2  ≡ −1/2  C
xx.zC

−1/2 
associated with the r1 largest eigenvalues. In the above, yx.z = yx−yz−1 zz zx ,
xx.z = xx −xz−1 zz zx , and C
 = yx.z−1 xx.z is the full-rank LS estimate of C.
The above results on the form of ML estimates can be verified by similar
arguments as in Section 2.3 for the single regressor set model (2.11). Similar to
Section 2.3, the ML estimates of A, B, and D are obtained by minimizing |W|, or
equivalently, |−1  W|, where W = (1/T )(Y − ABX − DZ)(Y − ABX − DZ)
.
Now with C = AB, we can write
Y − CX − DZ = (Y − C
X − DZ) + (C
 − C)X + (D − D)Z
= (Y − C
X − DZ) + (C
 − C)(X − xz−1
zz Z)
+ {(D − D) + (C
 − C)xz−1
zz }Z,
where C
 and D are the unrestricted full-rank LS estimators, and the three terms
on the right-hand side of the last expression are orthogonal. So we obtain the
corresponding decomposition
W =  + (C
 − AB)xx.z(C
 − AB)3.1 Reduced-Rank Model of Anderson 79
+ (D∗ − D − Cxz−1
zz )zz(D∗ − D − Cxz−1
zz )

,
where D∗ = D + C
xz−1 zz ≡ yz−1 zz , with the last equality holding from the
normal equations for the unrestricted LS estimators C
 and D. For any C, the third
component in the above decomposition can be uniformly minimized, as the zero
matrix, by the choice of D = D∗ − Cxz−1 zz = yz−1 zz − Cxz−1 zz . Thus, the
ML estimation of minimizing |−1  W| is reduced to minimizing
|Im + −1/2  (C
 − AB)xx.z(C
 − AB)
−1/2  |
with respect to A and B. This is the same minimization problem as for ML
estimation in Section 2.3, and it again follows similarly from Lemma 2.2 that
all eigenvalues of the matrix −1/2  (C
 − AB)xx.z(C
 − AB)
−1/2  will be
simultaneously minimized by the choice of estimators given in (3.11), and hence the
above determinant will also be minimized. Thus, these are ML estimators of A and
B, and the corresponding ML estimator of D is as noted previously, D = yz−1 zz −
C
(r1)
xz−1 zz with C
(r1) = A
(r1)
B
(r1)
. The ML estimate of  under the reduced￾rank structure is given by  = (1/T )(Y − C
(r1)
X − DZ)(Y − C
(r1)
X − DZ)
.
Similar to the result in (2.18) of Section 2.3, the ML estimate  can be represented
as
 =  + (C
 − C
(r1)
)xx.z(C
 − C
(r1)
)

=  + (Im − P )C
xx.zC

(Im − P )
,
where P = 1/2  V
V

−1/2  , and C
 = yx.z−1 xx.z is the full-rank LS estimate of C.
Because of the connection displayed in (3.9), for example, the rank of the matrix
C can be identified by the number of nonzero partial canonical correlations between
Yk and Xk, eliminating Zk. More precisely, based on a sample of T observations, the
likelihood ratio test procedure for testing H0 : rank(C) ≤ r1 yields the test statistic
− 2 log(λ) = −T m
j=r1+1
log(1 − ρ2
1j ), (3.12)
where ρ1j are the sample partial canonical correlations between Yk and Xk,
eliminating Zk, that is, ρ2
1j are the eigenvalues of the matrix
R
1 = −1/2
yy.z yx.z−1
xx.zxy.z−1/2
yy.z .
The derivation of the form of the LR statistic in (3.12) is similar to the derivation
given in Section 2.6 for the reduced-rank model with one set of regressor variables.
The restricted ML estimators are C
(r1) and D as given above, and we let S1 = (Y −
C
(r1)
X−DZ)(Y−C
(r1)
X−DZ) denote the corresponding residual sum of squares80 3 Reduced-Rank Regression Models with Two Sets of Regressors
matrix under H0. Because D = yz−1 zz − C
xz−1 zz , where C
 = yx.z−1 xx.z and
D denote the full-rank LS estimators, it follows that
(C
 − C
(r1)
)X + (D − D)  Z = (C
 − C
(r1)
)(X − xz−1
zz Z),
and we also have that
C
 − C
(r1) = (Im − 1/2  V
V

−1/2  )C

= 1/2 
⎡
⎣ m
j=r1+1
V
jV

j
⎤
⎦ −1/2  yx.z−1
xx.z.
Therefore, similar to the result in (2.39) of Section 2.6, we find that
1
T S1 = 1
T S + 1
T [(C
 − C
(r1)
)X + (D − D)  Z][(C
 − C
(r1)
)X + (D − D)  Z]

=  + 1/2 
⎡
⎣ m
j=r1+1
V
jV

j
⎤
⎦ R
1
⎡
⎣ m
j=r1+1
V
jV

j
⎤
⎦ 1/2 
=  + 1/2 
⎡
⎣ m
j=r1+1
λ2
1jV
jV

j
⎤
⎦ 1/2  ,
where the λ2
1j , j = r1 + 1,...,m, are the (m − r1) smallest eigenvalues of the
matrix R
 = −1/2  yx.z−1 xx.zxy.z−1/2  . From results similar to those presented
in the discussion at the end of Section 2.4 [e.g., see (2.29)], we know that λ2
1j =
ρ2
1j /(1 − ρ2
1j ) and so 1 +λ2
1j = 1/(1 − ρ2
1j ). Therefore,
U−1 = |S1|/|S| =
,
,
,
1
T S1
,
,
, /| | = +m
j=r1+1
(1 +λ2
1j ) = +m
j=r1+1
(1 − ρ2
1j )
−1.
Thus, it follows that the LR criterion −2 log(λ), where λ = UT /2, is given as in
(3.12). Anderson (1951) has shown that under the null hypothesis that rank(C) ≤
r1, the asymptotic distribution of the LR statistic is such that −2 log(λ) ∼
χ2
(m−r1)(n1−r1)
. The LR statistic in (3.12) can be used to test the hypothesis H0 :
rank(C) ≤ r1 and can be considered for different values of r1 to obtain an
appropriate choice of the rank of C. As in Section 2.6, a correction factor is
commonly used for the LR statistic in (3.12), yielding the test statistic M =
−[T − n + (n1 − m − 1)/2]
m
j=r1+1 log(1 − ρ2
1j ), whose null distribution is more
closely approximated by the asymptotic χ2
(m−r1)(n1−r1) distribution than is that of
−2 log(λ) in (3.12).3.1 Reduced-Rank Model of Anderson 81
The model (3.1) written as
Yk = ABXk + DZk + k, k = 1,...,T, (3.13)
provides an alternative way of interpreting the estimation procedure. Given B,
therefore BXk, estimates of A and D are obtained by the usual multivariate least
squares regression of Yk on BXk and Zk. When these estimates are substituted in
the likelihood criterion as functions of B, the resulting quantity will directly lead
to the estimation of B, via the canonical vectors associated with the sample partial
canonical correlations between Yk and Xk after eliminating Zk. This interpretation
will be shown to be useful for a more general model that we consider later in this
chapter.
As in Section 2.3 for the single regressor reduced-rank model, the above
discussion leads to an alternate form of the population solution to the reduced￾rank problem for model (3.1) besides that given in Theorem 3.1. For given BX,
we can substitute the least squares values of A and D (in particular, A =
yx.zB
(Bxx.zB
)−1) from the regression of Y on BX and Z into the criterion
(3.4) or alternatively into its decomposition given in the proof of Theorem 3.1. This
criterion then simplifies to tr{yy.z−(Bxx.zB
)−1Bxy.zyx.zB
}, and similar
to the discussion at the end of Section 2.3, it follows that the solution for B is such
that the columns of 1/2 xx.zB can be chosen as the eigenvectors corresponding to the
r1 largest eigenvalues of −1/2 xx.z xy.zyx.z−1/2 xx.z .
Although the results presented in Theorem 3.1 and above provide an explicit
solution for the component matrices A, B, and for D, using the eigenvalues and
eigenvectors of appropriate matrices, an iterative partial least squares approach
similar to that discussed for the single regressor model in Section 2.3 can also be
useful and is computationally convenient. For the population version of the problem,
the first order equations that result from minimization of the criterion (3.4) are
yxB − ABxxB − DzxB = 0
A
yx − (A
A)Bxx − A
Dzx = 0
yz − ABxz − Dzz = 0.
For given A and B, the first step in the iterative process is to obtain D from the third
equation as D = (yz − ABxz)−1 zz . For given B (and D), the solution for A can
be calculated from the first equation as A = (yx − Dzx)B
(BxxB
)−1, while
for given A (and D), from the second equation, the solution for B can be calculated
as B = (A
A)−1A
(yx − Dzx ). Alternatively, the optimal expression for
D (from the third equation) can be substituted into the first equation, and the
solution for A is then obtained for given B as A = yx.zB
(Bxx.zB
)−1, and
similarly, substituting the optimal expression for D into the second equation yields
the solution of B for given A as B = (A
A)−1A
yx.z−1 xx.z. Thus, in effect, D
can be eliminated from the first two equations and the partial least squares iterations82 3 Reduced-Rank Regression Models with Two Sets of Regressors
can be performed in terms of A and B only, completely analogous to the procedure
in Section 2.3 but using partial covariance matrices. The solution for D can finally
be recovered as given above. At each step of the iterations, one can consider using
the normalization conditions (3.8) for A and B. This partial least squares procedure
can also be carried over directly to the sample situation with the appropriate sample
quantities being used to iteratively construct estimates A
(r1)
, B
(r1)
, and D, which
are given more explicitly in (3.11).
Recall the original motivation behind Anderson’s article. While A and B, the
components of C, are formed out of the largest partial canonical correlations and
the associated vectors, the constraints on C which were of Anderson’s focus can be
obtained via the (m − r1) smallest (possibly zero) partial canonical correlations and
the associated vectors. It may be clear by now that the reduced-rank model in (3.13)
of this chapter can be accommodated through essentially the same set of quantities
as defined for the reduced-rank model in Chapter 2, with the difference being that the
covariance matrices used in Chapter 2 are replaced by partial covariance matrices.
For this reason, we do not repeat the asymptotic theory for the estimators that
was presented in Section 2.5. The results of Theorem 2.4 in that section could be
restated here for the case  = −1  known by substituting the covariance matrices
with corresponding partial covariance matrices. It is assumed that the predictors are
non-stochastic as well. Results for the case of stochastic predictors follow from the
asymptotic distribution presented later in Section 3.5.1 for an extended model.
3.2 Application to One-Way ANOVA and Linear
Discriminant Analysis
We consider the reduced-rank model (3.1) applied to the multivariate one-way
ANOVA situation and exhibit its relation to linear discriminant analysis. Suppose
we have independent random samples from n multivariate normal distributions
N (μi,), denoted by Yi1,...,YiTi , i = 1, 2,...,n, and set T = T1 +···+ Tn.
It may be hypothesized that the n mean vectors μi lie in some (unknown) r￾dimensional linear subspace, with r ≤ min(n − 1, m). (We will assume n<m for
convenience of notation.) This is equivalent to the hypothesis that the m × (n − 1)
matrix (μ1 − μ, . . . , μn−1 − μ), or (μ1 − μn,...,μn−1 − μn), is of reduced rank
r, where μ = n
i=1 Tiμi/T .
Because Yij = (μi −μn)+μn+ij , j = 1, 2,...,Ti, i = 1, 2,...,n, the model
can be easily written in the reduced-rank model form (3.1), Yij = CXij +DZij +ij .
The correspondences are C = (μ1−μn,...,μn−1−μn), D = μn, Zij ≡ 1, and Xij
is an (n − 1) × 1 vector with 1 in the ith position and zeros elsewhere for i<n and
Xnj ≡ 0. The reduced-rank restrictions imply C = AB with B = (ξ1,...,ξn−1)
of dimension r × (n − 1), or μi − μn = Aξi, i = 1, 2,...,n − 1; equivalently,
L(μi−μn) = 0, i = 1, 2,...,n−1, where L = (
1,...,
m−r) is (m−r)×m with
row vectors 
i as in (2.1). In terms of data matrices, we have Y = CX + DZ + ,3.2 Application to One-Way ANOVA and Linear Discriminant Analysis 83
where Y = [Y11,...,Y1T1 ,...,Yn1,...,YnTn ], Z = (1,..., 1) ≡ 1
T , and X =
[X(1),..., X(n−1), 0] with X(i) = [0,..., 0, 1Ti, 0,..., 0]
 where 1n denotes an N￾dimensional vector of ones, so that X(i) is an (n − 1) × Ti matrix with ones in the
ith row and zeros elsewhere. From the LS estimation results of Sections 1.2, 1.3,
and 3.1, the full-rank or unrestricted LS estimates of C and D are given as
C
 = yx.z−1
xx.z = [Y¯
1 − Y¯
n,..., Y¯
n−1 − Y¯
n], (3.14a)
D = (yz − C
xz)−1
zz = Y¯
n, (3.14b)
where Y¯
i = Ti
j=1 Yij /Ti is the ith sample mean vector, i = 1, 2,...,n. The
corresponding ML estimate of the error covariance matrix  ≡  is given by
 = 1
T S with S = n
i=1
Ti
j=1(Yij − Y¯
i)(Yij − Y¯
i) being the usual “within￾group” sum of squares and cross-products matrix.
For the reduced-rank or restricted estimates of C and D, we need to con￾sider the eigenvectors associated with the r largest eigenvalues of the matrix
yx.z−1 xx.zxy.z ≡ C
xx.zC
 with respect to the (unrestricted) residual covariance
matrix estimate  . Observe that zz = 1 and xz = 1
T (T1,...,Tn−1)
, and hence
xx.z = xx − xz−1
zz zx = 1
T [diag(T1,...,Tn−1) − 1
T
NN
],
where N = (T1,...,Tn−1)
. Therefore, it follows that
C
xx.zC
 = 1
T
)
n−1
i=1
Ti(Y¯
i − Y¯
n)(Y¯
i − Y¯
n)
 − T (Y¯ − Y¯
n)(Y¯ − Y¯
n)

*
= 1
T
)
n
i=1
Ti(Y¯
i − Y )( ¯ Y¯
i − Y )¯ 
*
≡ 1
T
SB, (3.15)
where SB is the “between-group” sum of squares and cross-products matrix,
with Y¯ = n
i=1 TiY¯
i/T . Let V
k denote the normalized eigenvector of
−1/2  C
xx.zC

−1/2  = 1
T −1/2  SB−1/2  corresponding to the kth largest
eigenvalue λ2
k. Then V
∗
k = −1/2  V
k satisfies (λ2
k − 1
T SB)V
∗
k = 0 or
(λ2
kS − SB)V
∗
k = 0, with V
∗
k normalized so that V
∗
k V
∗
k = 1. Let
V
∗ = [V
∗
1 ,..., V
∗
r ] = −1/2  V
 with V
 = [V
1,..., V
r]. Then, from the results
of Section 3.1, the ML reduced-rank estimate of C is
C
 = A
(r)B
(r) = 1/2  V
V

−1/2  C
 = V
∗V

∗C.  (3.16)
The corresponding restricted estimate of D = μn is then84 3 Reduced-Rank Regression Models with Two Sets of Regressors
D ≡ μn = (yz − C
xz)−1
zz = Y¯ − C
 1
T
N
= Y¯ − V
∗V

∗
1
T
n−1
i=1
Ti(Y¯
i − Y¯
n) = Y¯ + V
∗V

∗(Y¯
n − Y ). ¯ (3.17)
From (3.16), it is now easy to see that the ML reduced-rank estimates μi of the
individual population means μi are related as μi −μn = V
∗V

∗(Y¯
i −Y¯
n), so that
using (3.17), the ML reduced-rank estimates of the μi are
μi = Y¯ + V
∗V

∗(Y¯
i − Y ), i ¯ = 1, . . . , n. (3.18)
The reduction in dimension for estimation of the μi −μn appears through the use of
only the r linear combinations V

∗(Y¯
i−Y¯
n) of the differences in sample mean vectors
to estimate the μi − μn under the reduced-rank restrictions. In addition, using the
results in Sections 2.3 and 3.1, the ML estimate of the error covariance matrix 
under the reduced-rank assumptions is given by
 =  + (Im − V
∗V

∗)
 1
T
SB

(Im − V
∗V

∗ ) (3.19)
and also note that Im − V
∗V

∗ = V
−V

− with V
− = [V
∗
r+1,..., V
∗
m].
Now we indicate the relation with linear discriminant analysis. The setting of
linear discriminant analysis is basically that of the one-way multivariate ANOVA
model. The objectives of discriminant analysis are to obtain representations of the
data that best separate the n groups or populations and based on this to construct a
procedure to classify a new vector observation to one of the groups with reasonable
accuracy. Discrimination is based on the separations among the m-dimensional
mean vectors μi, i = 1,...,n, of the n populations. If the mean vectors μi were to
lie in some r-dimensional linear subspace, r < min(n − 1, m), then only r linear
functions of the μi (linear discriminant functions) would be needed to describe the
separations among the n groups. The Fisher–Rao approach to discriminant analysis
in the sample is to construct linear combinations (linear discriminants) zij = a
Yij
which successively maximize the ratio
a
SBa
a
Sa = n
i=1
Ti(z¯i − ¯z)2/
n
i=1

Ti
j=1
(zij − ¯zi)
2. (3.20)
Note that this is the ratio of the between-group (i.e., between means) to the within￾group variation in the sample. Thus, the sample first linear discriminant function is
the linear combination a
1Yij that maximizes the ratio in (3.20), the sample second
linear discriminant function is the linear combination a
2Yij that maximizes the ratio
subject to a
2a1 = 0 (i.e., the sample covariance between a
1Yij and a
2Yij is
zero), and so on.3.3 Numerical Example Using Chemometrics Data 85
Now recall that the V
∗
k in the solution to the above one-way ANOVA model
reduced-rank estimation problem are the eigenvectors of the matrix S−1SB cor￾responding to its nonzero eigenvalues λ2
k, for k = 1,...,n − 1, that is, they
satisfy (λ2
kS − SB)V
∗
k = 0. From this, it is known that the first eigenvector V
∗
1 ,
for example, provides the linear combination zij = a
Yij which maximizes the
ratio (3.20), that is, V
∗
1 yields the sample first linear discriminant function in the
linear discriminant analysis of Fisher–Rao, and λ2
1 represents the maximum value
attained for this ratio. Then V
∗
2 gives the second linear discriminant function, in the
sense of providing the linear combination which gives the maximum of the above
ratio subject to the orthogonality condition V
∗
1 SV
∗
2 = 0. In this same fashion, the
collection of vectors V
∗
k provides the set of (n − 1) sample linear discriminant
functions. In linear discriminant analysis of n multivariate normal populations,
these discriminant functions are generally applied to the sample mean vectors Y¯
i,
to consider V
∗
k (Y¯
i − Y )¯ , in an attempt to provide information that best separates
or discriminates between the means of the n groups or populations. In practice,
it may be desirable to choose a subspace of rank r<n − 1 and use only the
corresponding first r linear discriminant functions that maximize the separation
of the group means. From the above discussion, these would be obtained as the
first r eigenvectors V
∗
1 ,..., V
∗
r , and they might contain nearly all the available
information for discrimination. We thus see that restriction to consideration of only
these r linear discriminant functions can be formalized by formulating the one￾way ANOVA model with a reduced-rank model structure for the population mean
vectors μi.
The above results on the ML estimates in the one-way ANOVA model under
the reduced-rank structure were obtained by Anderson (1951) and later by Healy
(1980), Campbell (1984), and Srivastava (1997) [also by Theobald (1975) with 
assumed known], while the related problem in discriminant analysis was considered
earlier by Fisher (1938). The connections between reduced-rank regression and
discriminant analysis were also discussed by Campbell (1984), and a more direct
formulation of the structure on the population means along the lines of (3.18), i.e.,
μi = μ + V∗ξi ≡ μ + V∗V 
∗(μi − μ) was considered there. The reduced￾rank linear discriminant analysis method forms the basis of recent extensions of
discriminant analysis that are useful in analyzing more complex pattern recognition
problems (Hastie and Tibshirani 1996) and in analyzing longitudinal data through
growth curve models (Albert and Kshirsagar 1993).
3.3 Numerical Example Using Chemometrics Data
In the example of this section, we apply the multivariate reduced-rank regression
methods of Chapter 2 and Section 3.1 to chemometrics data obtained from
simulation of a low-density polyethylene tubular reactor. The data are taken from
Skagerberg, MacGregor and Kiparissides (1992), who used partial least squares86 3 Reduced-Rank Regression Models with Two Sets of Regressors
(PLS) multivariate regression modeling discussed in Section 2.9 applied to these
data both for predicting properties of the produced polymer and for multivariate
process control. The data were also considered by Breiman and Friedman (1997) to
illustrate the relative performance of different multivariate prediction methods.
The data set consists of T = 56 multivariate observations, with m = 6 response
variables and n = 22 predictor (or “process”) variables. The response variables are
the following output properties of the polymer produced:
y1, number-average molecular weight
y2, weight-average molecular weight
y3, frequency of long chain branching
y4, frequency of short chain branching
y5, content of vinyl groups in the polymer chain
y6, content of vinylidene groups in the polymer chain
The process variable measurements employed consist of 20 different temperatures
measured at equal distances along the reactor (x1 − x20), complemented with the
wall temperature of the reactor (x21) and the feed rate of the solvent (x22) that also
acts as a chain transfer agent. The temperature measurements in the temperature
profile along the reactor are expected to be highly correlated, and therefore reduced￾rank methods may be especially useful to reduce the dimensionality and complexity
of the input or predictor set of data. For interpretational convenience, the response
variables y3, y5, and y6 were rescaled by the multiplicative factors 102, 103, and 102,
respectively, before performing the analysis presented here, so that all six response
variables would have variability of the same order of magnitude. The predictor
variable x21 was also rescaled by the factor 10−1.
As usual, both the response and predictor variables were first centered by
subtraction of appropriate sample means prior to the multivariate analysis, and
we let Y and X denote the resulting data matrices of dimensions 6 × 56 and
22 × 56, respectively. To obtain a preliminary “benchmark” model for comparison
with subsequent analyses, a multivariate regression model was fit by LS to the
vector of response variables using only the two predictor variables x21 and x22,
the wall temperature of the reactor and the solvent feed rate. The LS estimate of the
regression coefficient matrix (with estimated standard errors given in parentheses)
from this preliminary model is obtained as
C

1 = (X1X
1)
−1X1Y
=
⎡
⎢
⎢
⎢
⎢
⎢
⎣
0.1357 1.0956 −1.5618 1.4234 0.5741 0.5339
(0.0475) (0.1230) (0.1660) (0.2045) (0.0813) (0.0783)
5.8376 28.3353 −0.1305 0.1715 0.1065 0.0596
(0.1704) (0.4414) (0.5953) (0.7336) (0.2917) (0.2809)
⎤
⎥
⎥
⎥
⎥
⎥
⎦
,3.3 Numerical Example Using Chemometrics Data 87
where X1 is 2 × 56. The unbiased estimate of the covariance matrix of the errors k
from the multivariate regression model (with correlations shown above the diagonal
and the covariances below the diagonal) is given by
 = 1
56 − 3

=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.15323 0.79248 0.26439 0.88877 0.89995 0.90231
0.31458 1.02835 −0.10505 0.77472 0.75447 0.76567
0.14156 −0.14571 1.87093 0.10493 0.14821 0.13324
0.58637 1.32412 0.24190 2.84066 0.95115 0.95700
0.23610 0.51276 0.13587 1.07438 0.44916 0.95458
0.22792 0.50104 0.11760 1.04082 0.41282 0.41640
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
where  = Y − C
1X1. We also have tr( ) = 6.75873, and the determinant of the
ML error covariance matrix estimate,  = 1
56
, is | | = 0.25318 × 10−4.
Clearly, from the results above, we see that the response variables y1 and y2
are very highly related to x22 and only more mildly related to x21, whereas response
variables y3−y6 are more strongly related to x21 with little or no dependence on x22.
It is also found that the errors from the fitted regressions are very strongly positively
correlated among the variables y1 and y4 − y6, the errors for y2 are rather highly
positively correlated with each of y1 and y4−y6, and the errors for y3 show relatively
small correlations with the errors for all the other response variables. Thus, there is
a structure in the relationship between the response variables and the two predictors.
We now incorporate the 20 measurements of the temperature profile along the
reactor into the multivariate regression analysis. The LS estimation of the response
variables regressed on the entire set of 22 predictor variables (x1−x22) is performed
and yields an ML estimate  = 1
56 of the error covariance matrix with
| | = 0.97557 × 10−8, where  = Y − C
X. A requisite LR test of the hypothesis
that the regression coefficients for all temperature profile variables (x1 − x20) are
equal to zero for all response variables gives a test statistic value, from (1.16), equal
toM = −[56−23+(20−6−1)/2] log(0.97557×10−8/0.25318×10−4) = 310.53,
with 6 ∗ 20 = 120 degrees of freedom. This leads to clear rejection of the
null hypothesis and indicates that the set of 20 temperature variables is useful in
explaining variations of the response variables. In addition, the “unbiased” estimate
of the error covariance matrix,  = 1
56−23
, has diagonal elements equal to
0.03297, 0.38851, 0.45825, 0.21962, 0.05836, 0.03717, with trace equal to 1.19487.
Comparison to the corresponding values for the simple two predictor variable model
above indicates that the inclusion of the temperature variables has its most dramatic
effect in reduction of the error variances for response variables y4, y5, and y6. A
rather high proportion of the t-statistics for the individual regression coefficients
of the temperature variables in the LS fitted model are not “significant,” however,
partly due to the high level of correlations among many of the 20 temperature
profile variables. Because of this high degree of correlation among the temperature
measurements, it is obviously desirable to search for alternatives to the ordinary
least squares estimation results based on all 22 predictor variables. One common88 3 Reduced-Rank Regression Models with Two Sets of Regressors
approach to resolve the problem of dealing with a large number of predictor
variables might be to attempt to select a smaller subset of “key variables” as
predictors for the regression, by using various variable selection methods, principal
components methods, or other more ad hoc procedures. Unfortunately, this type of
approach could lead to discarding important information that relates the response
variables to the predictors, which might never be detected, and also to loss of
precision and quality of the regression model. Therefore, to avoid this possibility, we
consider the more systematic approach of using reduced-rank regression methods.
Because of the nature of the set of predictor variables, as a “natural” reduced-rank
model for these data we consider the model (3.13) of Section 3.1, Yk = ABXk +
DZk + k. Here, we take the “primary” or full-rank set of predictor variables as
Z = (1, x21, x22)
, the wall temperature and solvent feed rate variables, and the
reduced-rank set as X = (x1,...,x20)
, the 20 temperature measurements along
the reactor. To determine an appropriate rank for the coefficient matrix C = AB of
the variables Xk, we perform a partial canonical correlation analysis of Yk on Xk,
after eliminating the effects of Zk, and let ρ1j , j = 1,...,m, denote the sample
partial canonical correlations. The LR test statistic M = −[T − n + (n1 − m −
1)/2]
m
j=r1+1 log(1 − ρ2
1j ), with n = 23 and n1 = 20, discussed in Section 3.1 in
relation to (3.12), is used for r1 = 0, 1,..., 5, to test the rank of C, H0 : rank(C) ≤
r1. The results are presented in Table 3.1, and they strongly indicate that the rank of
C can be taken as equal to two.
We then obtain the ML estimates for the model with rank r1 = 2 by first finding
the normalized eigenvectors of the matrix
R
 = −1/2  yx.z−1
xx.zxy.z−1/2 
associated with its two largest eigenvalues, λ2
11 and λ2
12, in Table 3.1. These
normalized eigenvectors are found to be
V
1 = (0.15799, 0.26926, −0.37958, 0.57403, 0.46378, 0.46244)

,
V
2 = (0.13435, 0.03954, 0.81760, 0.43827, −0.21363, 0.27241)

.
Maximum likelihood estimates of the matrix factors A and B are then computed
as A
(2) = 1/2  V
(2) and B
(2) = V

(2)
−1/2  yx.z−1 xx.z, with V
(2) = [V
1, V
2],
Table 3.1 Summary of results for LR tests on rank of coefficient matrix for chemometrics data
Eigenvaluesλ2
1j
Partial canonical
correlations ρ1j Rank ≤ r1
M = LR
statistic d.f.
Critical value
(5%)
57.198 0.991 310.53 120 146.57
6.427 0.930 1 150.00 95 118.75
1.119 0.727 2 0.80 72 92.81
0.667 0.633 3 41.14 51 68.67
0.396 0.533 4 20.95 32 46.19
0.217 0.422 5 7.76 15 25.003.3 Numerical Example Using Chemometrics Data 89
and the ML estimate of the coefficient matrix D is obtained from D = (yz −
A
(2)
B
(2)
xz)−1 zz . The ML estimates for A and D are found to be
A
(2) =

0.039611 0.108042 −0.047137 0.196351 0.076314 0.074153
0.067271 0.037769 0.452448 0.229329 0.082096 0.091330 
and
D =
⎡
⎣
24.5251 18.9297 109.5563 97.6315 41.2813 40.7216
−0.3685 −0.2129 −1.2063 −1.0284 −0.3756 −0.3943
5.4768 27.6520 −0.8007 −1.4021 −0.4906 −0.5444
⎤
⎦ .
Essentially, all of the coefficient estimates in A
 and D are highly significant. On
examination of the two rows of the ML estimate B
(2)
, it is found that the two linear
combinations in the “transformed set of predictor variables” of reduced dimension
two, X∗
k = (x∗
1k, x∗
2k) = B
(2)
Xk, are roughly approximated by
x∗
1 ≈ −114x1 − 121x2 + 154x3 + 58x14 − 85x15
+ 124x16 − 245x17 − 108x18 + 191x19 + 52x20,
x∗
2 ≈ −175x2 + 93x3 − 42x11 − 63x13 + 94x14 − 59x15 + 49x19 − 108x20.
Thus, it is found that certain of the temperature variables, particularly those in
the range x5 − x10, do not play a prominent role in the modeling of the response
variables, although a relatively large number among the 20 temperature variables
do contribute substantially to the two linear combination predictor variables x∗
1
and x∗
2 . The total number of estimated coefficients in the regression model with
the reduced-rank of two imposed on C is 6 ∗ 3 + 6 ∗ 2 + 2 ∗ 20 − 4 = 66 as
compared to 6 ∗ 23 = 138 in the full-rank ordinary LS fitted model. The “average”
number of parameters per regression equation in the reduced-rank model is thus 11,
as opposed to 23 in the full-rank model. An “approximate unbiased” estimator of
the error covariance matrix  under the reduced-rank model may be constructed
as
 = 1
56 − 11
=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.03260 0.04556 0.05621 0.01363 0.01874 0.01023
0.04556 0.36888 0.05422 −0.01976 −0.00776 −0.00774
0.05621 0.05422 0.40822 0.11388 0.11901 0.05683
0.01363 −0.01976 0.11388 0.18082 0.04824 0.02197
0.01874 −0.00776 0.11901 0.04824 0.06057 0.02345
0.01023 −0.00774 0.05683 0.02197 0.02345 0.03232
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,90 3 Reduced-Rank Regression Models with Two Sets of Regressors
with tr( ) = 1.08341, where  = Y − A
B
X − DZ. The diagonal elements
of the estimate  above, when compared to the corresponding entries of the
unbiased estimate of  under full-rank LS as reported earlier, show substantial
reductions in the estimated error variances for the response variables y3, y4, and
y6 (of around 14% on average), slight reductions for y1 and y2 (around 3%
average), and only slight increase for y5 (around 4%). The reduced-rank model
(3.13) thus provides considerable simplification over the full-rank model fitted
by LS and generally provides a better fit of model as well. In the reduced-rank
model results, it might also be of interest and instructive to examine in some
detail the vectors j = −1/2  V
j , j = 3,..., 6, which provide estimates for
linear combinations y∗
jk = 
jYk of the response variables that, supposedly, are
little influenced by the set of temperature variables. For example, two such linear
combinations are estimated to be, approximately, y∗
6 ≈ 1.51y1 − 1.84y2 + 2.96y5
and y∗
5 ≈ −4.98y1 + 2.40y4 − 2.94y6. Interpretations for these linear combinations
require further knowledge of the subject matter which goes beyond the scope of this
illustrative example.
An additional application of the reduced-rank methods in the context of this
example is also suggested. One of the goals of the study by Skagerberg et al.
(1992) was to “monitor the performance of the process by means of multivariate
control charts” over time, to enable quick detection of changes early in the
process. In the case of a univariate measurement, this is typically accomplished
by use of statistical process control charts. For this multivariate chemometrics
data set, one might require as many as 22 charts for the predictor variables and
6 for the response variables. Because of the association established between the
response and the predictor variables in the reduced-rank model, involving only two
linear combinations of the temperature profile variables, a small number of linear
combinations only might be used to effectively monitor the process. Skagerberg
et al. (1992) suggested a similar idea based on linear combinations that result from
partial least squares regression methods.
3.4 Both Regression Matrices of Lower Ranks: Model and
Its Applications
The assumption that the coefficient matrix of the regressors Zk in (3.1) is of full
rank and cannot be collapsed may not hold in practice. We propose a model that
assumes that there is a natural division of regressor variables into two sets, and
each coefficient matrix can be of reduced rank. This model seems to have wider
applications not only for analyzing cross-sectional data where observations are
assumed to be independent but also for analyzing chronological (time series) data.
The usefulness of the extended model will be illustrated with an application in
meteorology where the total ozone measurements taken at five European stations are
related to temperature measurements taken at various pressure levels. This model3.4 Both Regression Matrices of Lower Ranks: Model and Its Applications 91
will be shown also to lead to some intuitively simpler models which cannot be
otherwise identified as applicable to this data set. Thus, the merit of reduced-rank
regression techniques will become more apparent.
The extended reduced-rank model is given as in (3.1) by
Yk = CXk + DZk + k, k = 1,...,T, (3.21)
with the assumption that
rank(C) = r1 ≤ min(m, n1)
rank(D) = r2 ≤ min(m, n2)
(3.22)
accommodating the possibility that the dimension reduction could be different for
each set. Hence we can write C = AB, where A is an m × r1 matrix and B is an
r1 × n1 matrix. Similarly, we can write D = EF where E is an m × r2 matrix and
F is an r2 × n2 matrix.
A situation that readily fits into this setup is given in the appendix of the paper
by Jöreskog and Goldberger (1975). Suppose there are two latent variables y∗
1 and
y∗
2 each determined by its own set of exogenous causes and its own disturbances so
that
y∗
1k = β
1Xk + V1k, y∗
2k = β
2Zk + V2k.
The observable indicators Yk are assumed to be related to the latent variables
linearly, giving Yk = α1y∗
1k+α2y∗
2k+Uk. Therefore the reduced-rank form equations
are
Yk = α1β
1Xk + α2β
2Zk + k = CXk + DZk + k,
where both C and D are of unit rank. This example can be formally expanded into
two blocks of latent variables each explained by their own causal set. This would
result in a higher dimensional example for the extended model.
With the factorization for C and D, we can write the model (3.21) as
Yk = ABXk + EFZk + k, k = 1,...,T. (3.23)
Following Brillinger’s interpretation, the n1 component vector Xk and the n2
component vector Zk that arise from two different sources might be regarded as
determinants of a “message” Yk having m components, but there are restrictions
on the number of channels through which the message can be transmitted. These
restrictions are of different orders for Xk and Zk. We assume that these vectors Xk
and Zk cannot be assembled at one place. This means that they cannot be collapsed
jointly but have to be condensed individually as and when they arise from their
sources. Thus BXk and F Zk act as codes from their sources of origin, and on receipt92 3 Reduced-Rank Regression Models with Two Sets of Regressors
they are premultiplied by A and E, respectively, and are summed to yield Yk, subject
to error k.
Estimation and inference aspects for the extended reduced-rank model (3.23)
were considered by Velu (1991). While the focus of that paper and the remainder of
this chapter is on the estimation of component matrices, the complementary feature
of linear constraints, which are of different dimensions and forms for the different
coefficient matrices, has been shown to be useful in multidimensional scaling.
Takane, Kiers and de Leeuw (1995) consider a further extension of model (3.23),
to allow for more than two sets of regressor variables and provide a convergent
alternating least squares algorithm to compute the least squares estimates of the
component matrices of the reduced-rank coefficient matrices.
A model similar to (3.23) can be used to represent the time lag dependency of a
multiple autoregressive time series {Yt}. For illustration, let Xt = Yt−1 and Zt =
Yt−2. Model (3.23) for the time series {Yt} can then be written as Yt = ABYt−1 +
EFYt−2 + t , which provides a framework for dimension reduction for multiple
time series data. An even more condensed model follows if, further, B = F when
rank(C) = rank(D) = r. Then Yt = A(BYt−1) + E(BYt−2) + t , and this model
has an interpretation in terms of index variables. In this case the lower-dimensional
Y ∗
t = BYt acts as an index vector driving the higher-dimensional time series Yt .
These dimension reducing aspects in a multiple autoregressive time series context
are discussed in Reinsel (1983), Velu, Reinsel and Wichern (1986), and Ahn and
Reinsel (1988). The reduced-rank model discussed in Section 3.1 would also be
sensible to apply in the multiple time series context. For example, we might consider
Yt = DYt−1 + ABYt−2 + t , where the coefficient matrix of the lag one variables
Yt−1 may be of full rank. An interpretation is that because of the closer ordering in
time to Yt , all variables in Yt−1 are needed for Yt , but some (linear combinations
of) variables in the more distant lagged term Yt−2 are not needed. This provides
an example of a “nested reduced-rank” autoregressive model, and a general class
of models of this type was considered by Ahn and Reinsel (1988). Autoregressive
models with reduced rank are discussed in further detail in Chapter 5.
3.5 Estimation and Inference for the Model
Before we present the efficient estimators of the coefficient matrices for model
(3.23), it is instructive to note that combining Xk and Zk through blocking, as in
the conventional reduced-rank model considered in Chapter 2, may not necessarily
produce a reduced-rank structure. The extended model (3.23) can be written as
Yk = C∗

Xk
Zk

+ k,
where C∗ = [C,D] is an m × (n1 + n2) matrix. From matrix theory, we have3.5 Estimation and Inference for the Model 93
min(m, r1 + r2) ≥ rank(C∗) ≥ max(r1, r2).
Treating the above model as a classical reduced-rank regression model, when r =
rank(C∗), would yield the factorization
C∗ = GH = [GH1, GH2],
where G is an m×r matrix and H is an r ×(n1 +n2) matrix with partitions H1 and
H2 of dimensions r × n1 and r × n2, respectively. Note that both submatrices H1
and H2 in the partitioned matrix H have (at most) rank r, but this formulation does
not yield the desired factorization. Even if the individual matrices H1 and H2 are of
lower ranks, when put in partitioned form, the partitioned matrix H may be of full
rank (i.e., r = m) and the model may have no particular reduced-rank structure at
all. When m is assumed to be greater than r1 + r2, the above blocking amounts to
overestimating the rank of each individual matrix by (r1 + r2) − max(r1, r2). There
is also a conceptual difference between the above model and the model in (3.23).
This is best illustrated through the simple case r = r1 + r2 which we now consider.
The “correct” factorization we are seeking under model (3.23) can be written as
[C,D]=[AB, EF]=[A, E]

B 0
0 F

,
where the orders of A, B, E, and F are defined as before. The factorization we
would arrive at if we worked through a “blocked” version of model (3.23) is
[C,D] = GH = [G1, G2]

H11 H12
H21 H22 
,
where G1 is an m × r1 matrix, G2 is an m × r2 matrix, H11 is an r1 × n1, H12 is
an r1 × n2, H21 is an r2 × n1, and H22 is an r2 × n2. Recall that the right-hand side
matrix in the factorization is used to form codes out of the regressors, and in the
original formulation of the model (3.23), each regressor set, Xk and Zk, has its own
codes. This feature is violated if we work with the blocked model above.
3.5.1 Efficient Estimator
To identify the individual components in the extended reduced-rank model (3.23),
e.g., A and B from C, it is necessary to impose certain normalization conditions
on them. Normalizations similar to those used in Chapter 2 for the reduced-rank
regression model with one set of regressor variables are chosen for this model. For
the sample situation, these normalizations are94 3 Reduced-Rank Regression Models with Two Sets of Regressors
A
A = Ir1 ; B( 1
T XX
)B = 2
r1
E
E = Ir2 ; F ( 1
T ZZ
)F = ∗2
r2 ,
(3.24)
where X = [X1, X2,...,XT ] and Z = [Z1, Z2,...,ZT ] are data matrices,  is
a positive-definite matrix, and 2
r1 and ∗2
r2 are diagonal matrices. The  matrix
can be specified by the analyst, whereas the diagonal elements of r1 and ∗
r2 are
dependent on C and D, respectively. Let θ = (α
, β
, γ 
, δ
) denote the vector of
unknown regression parameters in model (3.23), where α = vec(A
), β = vec(B
),
γ = vec(E
), and δ = vec(F
). To arrive at the “efficient” estimator of θ, consider
the criterion
ST (θ ) = 1
2T
tr{(Y − ABX − EFZ)(Y − ABX − EFZ)

}, (3.25)
where Y = [Y1, Y2,...,YT ] is the data matrix of the dependent variables. The
estimates are obtained by minimizing criterion (3.25) subject to the normalization
conditions (3.24). Criterion (3.25) is the likelihood criterion when the errors k are
assumed to be normal, and  = −1  where  = 1
T (Y − A
B
X − E
F
Z)(Y −
A
B
X − E
F
Z) is the maximum likelihood estimate of the error covariance matrix.
For this reason, the resulting estimators can be called efficient. However, for the
asymptotic theory, we shall initially take  = −1  as known in criterion (3.25),
for convenience. The asymptotic theory of the proposed estimator follows from
Robinson (1973) or Silvey (1959) and will be presented later.
Several procedures are available for computing the efficient estimators. The first
order equations that result from the criterion (3.25) refer only to the necessary
conditions for the minimum to occur and so do not guarantee the global minimum.
Therefore, we suggest various computational schemes to check on the optimality of
a solution found by an alternate method.
The first order equations resulting from (3.25) are
∂ST (θ )
∂A = − 1
T
BX(Y − ABX − EFZ)

 (3.26a)
∂ST (θ )
∂B = − 1
T
X(Y − ABX − EFZ)

A (3.26b)
∂ST (θ )
∂E = − 1
T
FZ(Y − ABX − EFZ)

 (3.26c)
∂ST (θ )
∂F = − 1
T
Z(Y − ABX − EFZ)

E (3.26d)
An iterative procedure that utilizes only the first order equations is the partial
least squares procedure [see Gabriel and Zamir (1979)]. The procedure is easy to
use when compared with other suggested procedures and is now described. To start
assume that the matrices B and F in model (3.23) are known. The estimates of A3.5 Estimation and Inference for the Model 95
and E are easily obtained by least squares regression of Yk on [BXk,FZk]. Thus,
starting with estimates B
 and F
 that satisfy the constraints (3.24), A
 and E
 can be
obtained by the usual multivariate regression procedure. The resulting A
and E
 are
then adjusted to satisfy the normalization constraints (3.24) before moving to the
next step to obtain new estimates B
 and F
 of B and F.
The first order equations ∂ST (θ )/∂B = 0 lead to the solution for B as
B
 = A


 1
T (Y − E
F
Z)X
  1
T
XX
−1
. (3.27)
Substituting this B
 into the first order equation ∂ST (θ )/∂F = 0, we arrive at the
equations
F

 1
T
ZZ

− QF R = P ,
where R = 1
T ZX
( 1
T XX
)−1 1
T XZ
, Q = (E

A)(  A

E)  , and
P = E


 1
T
YZ

− (E

A)  A


 1
T
YX
  1
T
XX
−1  1
T
XZ

.
We can readily solve for F
 by conveniently stacking the columns of F
, using the
“vec” operator and the Kronecker products, from
 1
T
ZZ ⊗ I

− (R ⊗ Q)
vec(F )  = vec(P ). (3.28)
Observe that with the regression estimates A
and E
, using (3.28), we can solve for
F
 and then use all of these estimates in (3.27) to solve for B
. The normalizations
implied by the conditions (3.24) can be carried out using the spectral decomposition
of matrices, e.g., Rao (1973, p. 39).
An alternative procedure that can be formulated is along the lines of the
modification of the Newton–Raphson method due to Aitchison and Silvey (1958).
The details of this procedure will be made clearer when the asymptotic inference
results are presented later, but the essential steps can be described as follows. Let
h(θ ) be an r∗ = r2
1 + r2
2 vector obtained by stacking the normalization conditions
(3.24) without duplication, and let the matrix of first partial derivatives of these
constraints be Hθ = (∂hj (θ )/∂θi). Note that Hθ is of dimension s × r∗ where
s = r1(m + n1) + r2(m + n2). The first order equations that are obtained by
minimizing the criterion ST (θ )−h(θ )
λ, where λ is a vector of Lagrange multipliers,
can be expanded through Taylor’s theorem to arrive at
 Bθ −Hθ
−H
θ 0
  √T (θ − θ )
√Tλ

=

−
√T∂ST (θ )/∂θ
0

(3.29)96 3 Reduced-Rank Regression Models with Two Sets of Regressors
In the above expression, ∂ST (θ )/∂θ is essentially (3.26) arranged in a vector
form and Bθ is the expected value of the matrix of second-order derivatives,
{∂2ST (θ )/∂θi∂θj }.
To obtain explicit expressions for these quantities, we write the matrix form of
model (3.23), Y = CX + DZ +  = ABX + EFZ + , in vector form as
y = vec(Y
) = (Im ⊗ X
)vec(C
) + (Im ⊗ Z
)vec(D
) + e
= (Im ⊗ X
B
)α + (Im ⊗ Z
F
)γ + e
= (A ⊗ X
)β + (E ⊗ Z
)δ + e, (3.30)
where e = vec(
), and we have used the relations vec(C
) = vec(B
A
) = (Im ⊗
B
)vec(A
) = (A ⊗ In1 )vec(B
) and similarly for vec(D
). Then the criterion in
(3.25) can be written as ST (θ ) = 1
2T e
( ⊗ IT )e with the error vector e as in (3.30),
and so we find that ∂ST (θ )/∂θ can be expressed as
∂ST (θ )
∂θ = 1
T
∂e
∂θ ( ⊗ IT )e = − 1
T
M

 ⊗ X
 ⊗ Z

e, (3.31)
where M = Diag[M
1, M
2] is block diagonal with the matrices M
1 and M
2 in its
main diagonals,
M1 = [Im ⊗ B
, A ⊗ In1 ] ≡ ∂vec(C
)
∂α , ∂vec(C
)
∂β

and
M2 = [Im ⊗ F
, E ⊗ In2 ] ≡ ∂vec(D
)
∂γ  , ∂vec(D
)
∂δ

.
Notice that the vector of partial derivatives ∂e/∂θ that is needed in (3.31) is easily
obtained from (3.30) as ∂e/∂θ = −[Im ⊗ X
B
, A ⊗ X
, Im ⊗ Z
F
, E ⊗ Z
]. It also
follows, noting that E(e) = 0, that the matrix Bθ can be written as
Bθ ≡ E
∂2ST (θ )
∂θ∂θ
 
= 1
T
E
∂e
∂θ ( ⊗ IT )
∂e
∂θ
 
= M 1
T
E

 ⊗ XX  ⊗ XZ
 ⊗ ZX  ⊗ ZZ

M
≡ M
∗M, (3.32)
where3.5 Estimation and Inference for the Model 97
∗ =

 ⊗ Σxx  ⊗ Σxz
 ⊗ Σzx  ⊗ Σzz 
.
The basic idea in this procedure is to solve for the unknown quantities using
Eq. (3.29), where in practice the sample version of Bθ (that is, the sample version
of ∗) is used in which the expected value matrices xx , xz, and zz are replaced
by their corresponding sample quantities xx = 1
T XX
, xz = 1
T XZ
, and zz = 1
T ZZ
, respectively. There are some essential differences between this procedure and
the partial least squares procedure. This procedure uses the second-order derivatives
of ST (θ ) in addition to the first-order derivatives. In addition, the estimates and
also the constraints are simultaneously iterated. Because of these differences, the
dimensions of the estimators are much larger. An added problem to this procedure
is that we cannot use the results on inverting the partitioned matrix, because the rank
of Bθ is the same as the rank of M (which equals r1(m+n1−r1)+r2(m+n2−r2))
and therefore Bθ is singular. But the singularity can be removed using a modification
suggested by Silvey (1959, p. 399); see also Neuenschwander and Flury (1997).
Observe that the matrix Hθ is of rank r2
1 + r2
2 and it can be demonstrated that Bθ +
HθH
θ is of full rank r1(m + n1) + r2(m + n2). Using this fact, we can rewrite the
Eq. (3.29) as

Bθ + HθH
θ −Hθ
−H
θ 0
  √T (θ − θ )
√Tλ

=

−
√T∂ST (θ )/∂θ
0

.
The iterated estimate of θ at the ith step is given as
θi
λi

=
θi−1
0

+
)
Bθi−1 + Hθi−1
H
θi−1 −Hθi−1
−H
θi−1 0
*−1

−∂ST (θi−1)/∂θ
0

(3.33)
and using a stopping rule based on either the successive differences between the θi or
the magnitudes of the Lagrange multipliersλi or ∂ST (θi−1)/∂θ, the estimates can be
calculated. In general we would expect this procedure to perform better numerically
than the partial least squares procedure, because it uses more information. However,
in practical applications, the size of the matrix Bθ can be quite large.
A third estimation procedure, which is not (statistically) efficient relative to the
above efficient estimator but intuitively appealing, is presented in the discussion
below as an alternative estimator.
3.5.2 An Alternative Estimator
The proposed alternative estimator is similar to the estimator of the parameters in
the model (3.3) of Section 3.1. It is inefficient but intuitive as mentioned earlier and98 3 Reduced-Rank Regression Models with Two Sets of Regressors
it is based on the full-rank coefficient estimates. Recall that in the model (3.1) or
(3.3), D was assumed to be full rank but C could be of reduced rank. It was noted
at the end of Section 3.1 that, in the population, solutions for the components A
and B of the matrix C ≡ AB under model (3.3) can be obtained as follows: the
columns of 1/2 xx.zB are chosen to be the eigenvectors corresponding to the largest
r1 eigenvalues of −1/2 xx.z xy.zyx.z−1/2 xx.z and A = yx.zB
(Bxx.zB
)−1. The
matrix D then is obtained as D = yz−1 zz − ABxz−1 zz . The r1 eigenvalues can
be related to partial canonical correlations between Yk and Xk eliminating Zk and
are used to estimate the rank of C (see Section 3.1). The corresponding estimates
in the sample are easily computable using the output from the standard documented
software packages as in the case of the reduced-rank model discussed in Chapter 2.
The alternative estimator is in the same spirit as the estimator in model (3.1) but
differs only in the estimation of the matrix C. In Theorem 3.1, the criterion was
decomposed into two parts:
tr{E[1/2(Y − ABX∗)(Y − ABX∗)

1/2]}
+ tr{E[1/2(Y − D∗Z)(Y − D∗Z)
1/2]}.
Here we modify the second part as tr{E[1/2(Y − DZ∗)(Y − DZ∗)
1/2]} where
D = EF and Z∗ = Z − zx−1 xx X, leaving the first part as it is in the procedure
of Section 3.1. This is equivalent to assuming that C is of full rank when estimating
the components of the matrix D. Therefore the columns of 1/2 zz.xF are given
by the eigenvectors of −1/2 zz.x zy.xyz.x−1/2 zz.x and E = yz.xF
(F zz.xF
)−1.
Stated differently, in each case (either C or D), in the alternative procedure we first
compute the full-rank estimator and then project it to a lower dimension (reduced
rank) individually, ignoring the simultaneous nature of restrictions involved in the
reduced-rank problem. The alternative estimators of the components A, B and E, F
are simply the corresponding sample versions of the above expressions.
Implicit in the solution is the fact that the matrices B and F are normalized with
respect to partial covariance matrices xx.z and zz.x respectively, rather than the
covariance matrices xx and zz as for the efficient estimator [see Eq. (3.8)]. For
different choices of normalizations, it is assumed that the true parameter belongs
to different regions of the parameter space. However, through the transformations
from one set of true parameters to the other using the spectral decomposition of
matrices, the estimators are made comparable to each other. Thus we provide yet
another procedure to arrive at estimates of the extended model (3.23).
3.5.3 Asymptotic Inference
The asymptotic distribution of the maximum likelihood estimator and the alternative
estimator can be derived using the theoretical results from Robinson (1973) or3.5 Estimation and Inference for the Model 99
Silvey (1959). Before we present the main results, we observe the following
points. It must be noted that with the choice of  = −1  , the normalization
conditions (3.24) imposed on θ vary with sample quantities. To apply Silvey (1959)
results directly for estimators of θ, the constraints are required to be fixed and
not vary with sample quantities. The modifications to account for the randomness
could be accommodated by considering the asymptotic theory of Silvey (1959)
for the combined set of ML estimators θ and  jointly, but explicit details
on the asymptotic distribution of the ML estimator θ would then become more
complicated. Hence, we do not consider results for this case explicitly. As in the
case of the asymptotic results of Theorem 2.4 for the single regressor model, we
use  = −1  as known instead of  = −1  , to obtain more convenient explicit
results on the asymptotic covariance matrix of the estimator of θ. It is expected
that the use of  = −1  instead of  = −1  will not have much impact on the
asymptotic distribution. Suppose we let (3.25) stand for the criterion with the choice
 = −1  and let the same criterion and the constraints in (3.24) for the choice of
fixed  = −1  be denoted as S∗
T (θ ).
Theorem 3.2 Assume the limits
lim
T →∞  1
T
ΣkXkX
k

= xx , lim
T →∞  1
T
ΣkZkZ
k

= zz,
and
lim
T →∞  1
T
ΣkXkZ
k

= xz exist almost surely and Σxx Σxz
Σzx Σzz 
and  are positive-definite. Let θ denote the estimator that absolutely minimizes
the criterion (3.25) subject to the normalizations (3.24), with the choice  = −1 
known. Then as T → ∞, θ → θ almost surely and √T (θ − θ ) has a limiting
multivariate normal distribution with null mean vector and singular covariance
matrix
W = (Bθ + HθH
θ )
−1Bθ (Bθ + HθH
θ )
−1, (3.34)
where Bθ = M
∗M is given in (3.32) and H
θ = ∂h(θ )/∂θ
.
Proof With assumptions stated in the theorem and with an assumption that the
parameter space  is compact and if the true parameter is taken to be an interior
point of , the almost sure convergence follows from Silvey (1959) results. To
prove the limiting distribution part of the theorem, we follow the standard steps.
The first-order equations are100 3 Reduced-Rank Regression Models with Two Sets of Regressors
∂S∗
T (θ )
∂θ − Hθλ = 0
h(θ ) = 0.
(3.35)
Let θ and λ be solutions of the above equations. Because the partial derivatives of
hi(θ ) are continuous,
Hθ = Hθ + o(1) (3.36a)
and
h(θ ) = [H
θ + o(1)](θ − θ ). (3.36b)
By Taylor’s theorem,
∂S∗
T (θ )
∂θ |θ = ∂S∗
T (θ )
∂θ +
#
∂2S∗
T (θ )
∂θ∂θ + o(1)
$
(θ − θ ). (3.37)
Observe that ∂2S∗
T (θ )
∂θ∂θ → Bθ almost surely. Substituting (3.36) and (3.37) in
Eq. (3.35) evaluated at θ, λ, we arrive at (3.29), which forms the basic components
of the asymptotic distribution of √T (θ − θ ).
If we let  = Y − ABX − EFZ, the first order quantities ∂S∗
T (θ )/∂θ can be
written compactly as in (3.31). Observe that the same quantities written in the matrix
form in (3.26) are
∂S∗
T (θ )
∂A = −B
 1
T
X


∂S∗
T (θ )
∂B = −  1
T
X

A
∂S∗
T (θ )
∂E = −F
 1
T
Z


∂S∗
T (θ )
∂F = −  1
T
Z

E.
The above can be reexpressed, as in (3.31), using the vec operator and writing
vec(X
) = ( ⊗ X)e and vec(Z
) = ( ⊗ Z)e, as
√
T
∂S∗
T (θ )
∂θ = −M
√
T
vec( 1
T X
)
vec( 1
T Z
) 
.
A simple interpretation for this relation is as3.5 Estimation and Inference for the Model 101
∂S∗
T (θ )/∂θ = (∂φ
/∂θ )(∂S∗
T (θ )/∂φ),
where φ = vec(C
, D
) = (vec(C
)
, vec(D
)
) denotes the vector of “unrestricted”
regression coefficient parameters, with ∂φ
/∂θ = M and
∂S∗
T (θ )/∂φ = −vec 1
T
X
,
1
T
Z


≡ −∗vec 1
T
X
, 1
T
Z

,
with ∗ = Diag[ ⊗ In1 ,  ⊗ In2 ]. From the multivariate linear regression theory,
we know that
√
T
vec( 1
T X
)
vec( 1
T Z
)
 d
→ N (0, ∗∗),
where
∗∗ =

Σ ⊗ Σxx Σ ⊗ Σxz
Σ ⊗ Σzx Σ ⊗ Σzz 
,
and, therefore, with  = −1  , √T ∂S∗
T (θ )
∂θ
d
→ N (0, Bθ ), where Bθ =
M
∗∗∗∗M = M
∗M. The asymptotic covariance matrix of θ is the relevant
partition (upper left block) of the following matrix:
 Bθ −Hθ
−H
θ 0
−1 
Bθ 0
0 0   Bθ −Hθ
−H
θ 0
−1
. (3.38)
The above expression follows from (3.29) and the upper left partitioned portion of
the resulting matrix product will yield the asymptotic covariance matrix of θ. In
order to arrive at the explicit analytical expression W, we can use Silvey (1959)
modification, where we replace Bθ by Bθ + HθH
θ in (3.38) in the matrices that
require inverses. The standard results for finding the inverse of a partitioned matrix
[e.g., see Rao (1973, p. 33)] will yield the asymptotic covariance matrix W.
For the numerical example that will be illustrated in the last section of this
chapter, we compute estimates using the partial least squares procedure and their
standard errors are calculated via the matrix W in the above theorem. Although
the quantity Hθ does not appear to be easily written in a compact form, it can be
computed using standard results on matrix differentiation of the quadratic functions
of the elements of A, B, E, and F involved in the normalization conditions (3.24).
The asymptotic distribution of the alternative estimator, which was motivated
by the model (3.1) of Anderson and which was discussed earlier, can also be
considered. A limiting multivariate normal distribution result can be established
for this estimator by following the same line of argument as in the proof of
Theorem 3.2, but we omit the details. A main feature of the results is that
the alternative estimator is asymptotically inefficient relative to the estimator of102 3 Reduced-Rank Regression Models with Two Sets of Regressors
Theorem 3.2, although the expression for the asymptotic covariance matrix of the
alternative estimator is somewhat cumbersome and so is not easily compared with
the asymptotic covariance matrix (3.34) in Theorem 3.2. The proposed alternative
estimator, it should be again emphasized, does not have any overall optimality
properties corresponding to the extended reduced-rank model (3.23), but is mainly
motivated by the model (3.3) in Section 3.1. The usefulness of this estimator is that
it provides initial values in the iterative procedures for the efficient estimator and
more importantly that the associated partial canonical correlations can be used to
specify (identify) the ranks of the coefficient matrices C and D in model (3.23).
This topic is elaborated on in the following section.
Comments For the reduced-rank model (2.11) with a single regressor set Xk
considered in Chapter 2, the asymptotic distribution results for the corresponding
reduced-rank estimators A
and B
 can be obtained as a special case of the approach
and results in Theorem 3.2 simply by eliminating aspects related to the second
regressor set Zk. In Theorem 2.4 of Section 2.5, the asymptotic results for the
reduced-rank estimators in the single regressor model (2.11) were stated in more
explicit detail in terms of eigenvalues and eigenvectors of relevant matrices, and the
estimators were obtained in more explicit noniterative form as compared to results
for the extended model (3.23). Nevertheless, the final asymptotic distribution results
are the same in both approaches. In addition, as in the single regressor model, the
results of Theorem 3.2 enable the asymptotic distribution of the overall coefficient
matrix estimators C
 = A
B
 and D = E
F
, obtained as functions of the reduced-rank
estimators in the extended model, to be recovered. Specifically, from the relation
T 1/2vec(C
 − C
) = T 1/2[Im ⊗ B
, A ⊗ In1 ]

vec(A
 − A
)
vec(B
 − B
)

+ op(1)
≡ T 1/2M1

vec(A
 − A
)
vec(B
 − B
)

+ op(1)
and a similar relation for T 1/2vec(D − D
) in terms of T 1/2vec(E
 − E
) and
T 1/2vec(F
 − F
), the asymptotic distribution result T 1/2vec(C
 − C
, D − D
) d
→
N (0,MWM
) readily follows, where M is defined below Eq. (3.31) and W is the
asymptotic covariance matrix of Theorem 3.2 as defined in (3.34). Also notice
that the covariance matrix W is singular, since the matrix Bθ = M
∗M is
singular. However, it can be verified that W is a (reflexive) generalized inverse
of Bθ (that is, the relations BθWBθ = Bθ and WBθW = W both hold), which
we denote as W = B−
θ . Therefore, the above asymptotic covariance matrix for
T 1/2vec(C
 − C
, D − D
) can be expressed as MB−
θ M = M{M
∗M}−M
.
This form can be compared to the asymptotic covariance matrix for the full-rank
least squares estimators C
 and D in the extended model, which ignore the reduced￾rank restrictions. The asymptotic covariance matrix of this estimator, T 1/2vec(C
 −
C
, D − D
), is given by ∗−1.3.6 Identification of Ranks of Coefficient Matrices 103
3.6 Identification of Ranks of Coefficient Matrices
It has been assumed up to this point that the ranks of the coefficient matrices C
and D in model (3.21) are known. Usually in practice these ranks are unknown a
priori and have to be estimated from the sample data. A formal testing procedure is
presented, but the properties of the procedure have not been fully investigated.
In the extended reduced-rank model (3.21) if the structure on the coefficient
matrix C is ignored, then the rank of D can be estimated from partial canonical
correlation analysis between Yk and Zk eliminating Xk (see Section 3.1). The rank
of D is taken to be the number of eigenvalues ρ2
2j of −1/2
yy.x yz.x−1 zz.xzy.x−1/2
yy.x
that are not significantly different from zero. The likelihood ratio criterion for
testing whether the number of corresponding population eigenvalues which are
zero is equal to m − r2 is given by the test statistic similar to (3.12), namely
−T m
j=r2+1 log(1 − ρ2
2j ), where the ρ2j are the sample partial canonical cor￾relations between Yk and Zk eliminating Xk. For large samples, this statistic is
approximately the same as T m
j=r2+1 ρ2
2j . These criteria for testing the hypothesis
that rank(D) = r2 against the alternative that D is of full rank are shown to be
asymptotically distributed as χ2 with (m − r2)(n2 − r2) degrees of freedom (e.g.,
Anderson (1951)). We can use similar criteria to test for the rank of C assuming that
D is full rank.
Convenient formal test procedures for the ranks when both coefficient matrices
are assumed to be rank deficient are difficult to formulate because of the simulta￾neous nature of the problem. Given X∗
k = BXk, model (3.23) is in the framework
of the model (3.3) studied in Section 3.1, and hence the rank of D is precisely
estimated by partial canonical correlation analysis between Yk and Zk eliminating
X∗
k . Similarly, partial canonical correlation analysis between Yk and Xk eliminating
Z∗
k = F Zk can be used to test for the rank of C, if the matrix F were given. In
these computations, it is assumed that B or F is known, but in practice these have to
be estimated by the corresponding sample quantities. Hence, objective criteria that
do not depend on the values of B or F are not immediately apparent. However, we
argue that, in the population, conditioning on the total information Xk is equivalent
to conditioning on the partial information X∗
k = BXk.
We can write model (3.23) as Yk = [A, 0]Vk + EFZk + k, where Vk =
[B
, B
1]

Xk and B1 is a matrix of dimension (n1 − r1) × n1. Assuming the
population version of the normalization conditions (3.24), we have BxxB = 2
r1
and we can complete the orthogonalization procedure by finding B1 such that
B1xxB
1 = 2
n1−r1 and BxxB
1 = 0. The additional variables B1Xk included in
Vk are intended to capture any information from Xk that exists beyond the linear
combinations BXk that are assumed to be relevant in the model (3.23). Hence,
we have a one-to-one transformation from Xk to Vk, and for a known [B
, B
1]

conditioning on Vk is equivalent to conditioning on Xk. Thus, in the population, the
rank of the matrix D = EF can be determined as the number of nonzero partial
canonical correlations between Yk and Zk given Xk as in the model of Section 3.1.
A more general population result that throws some light on relative efficiencies104 3 Reduced-Rank Regression Models with Two Sets of Regressors
of the two procedures, i.e., canonical correlation analysis between Yk and Zk
eliminating X∗
k = BXk versus eliminating Xk, in the sample is presented below. An
analogous sample version of the result indicates that using sample partial canonical
correlations, eliminating Xk, to test whether the hypothesis that rank(D) = r2 is
true, is less powerful than using those eliminating only BXk.
Lemma 3.1 Let the true model be Y = AX∗ + DZ +  where X∗ = BX. Also
let ρ∗2
j and ρ2
j (j = 1, 2, . . . , m) be the (population) squared partial canonical
correlations between Y and Z obtained by eliminating X∗ and X, respectively.
Suppose that rank(D) = r2; then (a) ρ∗2
j ≥ ρ2
j , for all j , and (b) ρ∗
j = ρj = 0,
j = r2 + 1,...,m.
Proof Assume without loss of generality that the covariance matrices zz, xx and
 are identity matrices. Then, since yz = [A, 0]xz + Dzz = [A, 0]xz + D,
yx = [A, 0]xx + Dzx = [A, 0] + Dzx, and yy = AA + DD + I +
[A, 0]xzD + Dzx [A, 0]

, we find that the relevant partial covariance matrices
needed in the partial canonical correlation analysis of Y and Z eliminating X are
given by zz.x = zz−zx−1 xx xz = I −zxxz, yz.x = yz−yx−1 xx xz =
D(I − zxxz), and yy.x = yy − yx−1 xx xy = I + D(I − zxxz)D
.
The partial covariance matrices when eliminating only X∗ can be obtained by
simply replacing X by X∗ in these expressions. Thus it follows that the squared
partial canonical correlations between Y and Z eliminating X are the roots of the
determinantal equation |D(I − zxxz)D − ρ2/(1 − ρ2)I | = 0. Similarly, the
squared partial canonical correlations between Y and Z eliminating X∗ are the roots
of the determinantal equation |D(I − zx∗x∗z)D − ρ∗2/(1 − ρ∗2)I | = 0. Since
I − zxxz ≥ 0, the number of nonzero eigenvalues (roots) in either equation is
equal to the rank of D.
To prove part (a) note that D(I − zx∗x∗z)D ≥ D(I − zxxz)D since the
matrices in parentheses are respectively the residual covariance matrices obtained by
regressing Z on X and Z on X∗, a subset of X. This implies that ρ∗2
j /(1 − ρ∗2
j ) ≥
ρ2
j /(1 − ρ2
j ) for all j , and hence, ρ∗2
j ≥ ρ2
j , j = 1, 2,...,m.
The above lemma indicates that in the population, the rank of D for the extended
model (3.23) can be determined using the partial canonical correlations similar to
methods for the model (3.3) of Section 3.1. But in the sample, an analogous version
of the above result implies that the procedure is not efficient. To emphasize that the
quantity ρ∗ depends on the choice of the matrix B, we write it as ρ∗(B). Under the
null hypothesis that rank(D) = r2, both statistics ψ
 = −T m
j=r2+1 log(1−ρ2
j ) and
ψ
∗ = −T m
j=r2+1 log(1−ρ∗2
j (B)) follow χ2 distributions with (m−r2)(n2 −r2)
degrees of freedom (when ψ
∗ uses the true value of B). Since ψ
∗ ≥ ψ
, which
follows from the result ρ∗2
j ≥ ρ2
j for all j = 1,...,m that is implied by a sample
analogue of the above lemma, use of (sample) partial canonical correlation analysis
with the statistic ψ
 to test the null hypothesis that rank(D) = r2 is less powerful
than with the statistic ψ
∗.3.7 An Example on Ozone Data 105
An alternative data-based method for estimating the ranks of the coefficient
matrices is the rank trace method given by Izenman (1980). This calls for examining
the residual covariance matrix for every possible rank combination and so it involves
a large amount of computational effort. Our limited experience suggests that the use
of partial canonical correlations, which represents a much easier computation, is a
good way to identify the ranks.
3.7 An Example on Ozone Data
Ozone is found in the atmosphere between about 5 km and 60 km above the
Earth’s surface and plays an important role in the life cycle on earth. It is well
known that stratospheric ozone is related to other meteorological variables, such
as stratospheric temperatures. Reinsel et al. (1981), for example, investigated the
correlation between total column ozone measured from Dobson ground stations
and measurements of atmospheric temperature at various pressure levels taken near
the Dobson stations. A typical pattern of correlations between total ozone and
temperature as a function of pressure level of the temperature reading was given
by Reinsel et al. (1981) for the data from Edmonton, Canada.
Temperature measurements taken at pressure levels of 300 mbar and below are
negatively correlated with total ozone measurements, and temperature measure￾ments taken at pressure levels of 200 mbar and above are positively correlated
with ozone. For the observations from Edmonton, a maximum negative correlation
between total ozone and temperature occurs in the middle troposphere at 500 mbar
( 5 km), and a maximum positive correlation occurs in the stratosphere at
100 mbar ( 16 km). However, these patterns are known to change somewhat with
location, especially with latitude.
We consider here the monthly averages of total ozone measurements taken
from January 1968 to December 1977 for five stations in Europe. The stations are
Arosa (Switzerland), Vigna Di Valle (Italy), Mont Louis (France), Hradec Kralove
(Czech Republic), and Hohenpeissenberg (Germany). The ozone observations,
which represent the time series Yt , are to be related to temperature measurements
taken at 10 different pressure levels. The temperature measurement at each pressure
level is an average of the temperature measurements at that pressure level taken
around the three stations Arosa, Vigna Di Valle, and Hohenpeissenberg. The series
are seasonally adjusted using regression on sinusoidal functions with periods 12 and
6 months, before the multivariate regression analysis is carried out.
The correlations, averaged over the five ozone stations, between the ozone
measurements and the temperature data for the 10 pressure levels are displayed in
Fig. 3.1. As can be seen from Fig. 3.1, the average correlations confirm the pattern
mentioned earlier. But, in general, negative correlations are of greater magnitude
than positive correlations. The zone between the troposphere and stratosphere
known as the tropopause provides a natural division between the temperature106 3 Reduced-Rank Regression Models with Two Sets of Regressors
Fig. 3.1 Average correlation between ozone and temperatures at European stations versus pres￾sure level (and average height)
measurements in influencing ozone measurements. The temperature measurements
in the troposphere, at the first five pressure levels (the pressure levels below 10 km),
form the first set of time series regressor variables Xt , and the temperature measure￾ments at the next five pressure levels (above 10 km) in the stratosphere form the
second set of regressors Zt . As mentioned earlier, all series are seasonally adjusted
prior to the multivariate analysis. The ozone series exhibit weak autocorrelations of
magnitude about 0.2 which have been ignored in the analysis reported here. Because
the ozone stations are relatively close to each other, and the temperature series
are averages representing the region, with substantial correlations in neighboring
pressure levels, some redundancy in the parameters can be expected and thus the
extended reduced-rank multivariate regression model may be appropriate.
A standard multivariate regression model Yt = CXt + DZt + t was initially
fitted to the data, and the least squares estimates of the coefficient matrices were
obtained as
C
 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
−0.99 1.28 −4.51 −5.09∗ 2.66
−0.95 −0.61 −3.70 0.48 −1.87
0.33 1.77 −1.51 −2.02 −3.35∗
0.59 0.65 −5.74 −7.29∗ 6.15∗
−1.12 2.20 −6.14∗ −5.53∗ 3.95∗
⎤
⎥
⎥
⎥
⎥
⎥
⎦3.7 An Example on Ozone Data 107
Table 3.2 Summary of results for reduced-rank regression models fitted to ozone-temperature data
Type of Rank Rank Number of BIC = log ||
regression model of C of D parameters, n∗ +(n∗ log T )/T
– – – 0 log |yy | = 22.3591
Full rank 5 5 50 22.4683
Two regressors 1 1 18 21.8511
2 1 25 21.8466
2 2 32 21.9704
3 2 37 22.0670
D =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
−0.75 −0.48 1.62 −2.55 4.62∗
0.89 −3.30 4.12∗ −4.14∗ 3.88∗
1.39 −1.45 2.93 −2.39 3.22∗
−3.30 1.11 1.13 −3.48 5.09∗
−1.76 −0.05 1.13 −0.65 3.39∗
⎤
⎥
⎥
⎥
⎥
⎥
⎦
.
The asterisks indicate that the individual coefficients have t-ratios greater than 2.
Taking that as a rule of thumb for their significance, it follows that the temperature
measurements in the last three pressure levels of both the troposphere and the
stratosphere have greater influence than the measurements in the first two pressure
levels when all are considered together. The mere nonsignificance of the elements
of the first two columns in both matrices C
 and D probably indicates that these
coefficient matrices may be of reduced rank.
As argued in Section 3.6, formal test procedures for the ranks when both C and
D are rank deficient are difficult to formulate, but we still may use partial canonical
correlations to identify the ranks as in the testing procedure for the model considered
in Section 3.1. Though this procedure of using partial canonical correlations is not
efficient, it is a good practical tool, as illustrated by this example. The sample partial
canonical correlations ρ1j between Yt and Xt eliminating Zt are found to be 0.71,
0.43, 0.34, 0.11, and 0.07. Using Bartlett’s test statistic (i.e., the asymptotically
equivalent form of the LR test statistic given in (3.12) of Section 3.1) to test the
hypothesis H0 : rank(C) = r1 versus H1 : rank(C) = 5 for r1 = 1, 2, the rejection
tail probabilities (p-values) corresponding to the two null hypotheses are found
to be 0.001 and 0.076. If the conventional rule of fixing the level of significance
as either 0.05 or 0.01 is followed, the hypothesis that rank(C) = 2 cannot be
rejected. Since this procedure is inefficient, the possibility that rank(C) = 3 (the
corresponding tail probability is 0.076) may also be entertained. When a similar
canonical correlation analysis between Yt and Zt eliminating Xt is conducted, the
partial canonical correlations are 0.53, 0.35, 0.28, 0.12, and 0.08. Using Bartlett’s
test statistic to test the hypothesis H0 : rank(D) = r2 versus H1 : rank(D) = 5 for
r2 = 1, 2, the rejection tail probabilities are found to be 0.04 and 0.24. Here again,
the possibility that the rank of D is either 1 or 2 can be entertained.108 3 Reduced-Rank Regression Models with Two Sets of Regressors
Comparison of various model fittings is based on the information criterion (BIC)
as discussed by Schwarz (1978), and a summary of the results is presented in
Table 3.2. Of the two-regressor-sets reduced-rank models, the model fitted with
ranks 2 and 1 for C and D, respectively, has the smallest BIC value, which suggests
that the partial canonical correlations still may be useful tools for identifying the
ranks of C and D. (If a stringent significance level of 0.01 had been used, the tests
based on partial canonical correlations could have concluded that the ranks are 2 and
1.) The model with ranks 1 and 1 also comes close to attaining the minimum BIC
value. The estimated components of the best fitting model, obtained using the partial
least squares computational approach, are as follows (the entries in parentheses are
the standard errors computed using Eq. (3.34) from the results in Theorem 3.2):
A
 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
8.28 5.86 4.59 8.48 8.55
(0.39) (0.43) (0.56) (0.63) (0.39)
1.03 0.21 −5.77 4.12 1.85
(1.16) (1.10) (0.49) (1.35) (1.13)
⎤
⎥
⎥
⎥
⎥
⎥
⎦
B
 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
−0.06 0.15 −0.61 −0.52 0.16
(0.13) (0.22) (0.24) (0.24) (0.17)
0.00 −0.38 −0.25 0.46 0.55
(0.12) (0.21) (0.23) (0.19) (0.14)
⎤
⎥
⎥
⎥
⎥
⎥
⎦
E
 =
 8.07 5.74 4.65 9.01 8.25
(0.84) (0.89) (0.85) (1.09) (0.84)

F
 =

−0.09 −0.13 0.32 −0.38 0.57
(0.14) (0.22) (0.22) (0.18) (0.13)

.
In this model the temperature measurements at the first two pressure levels
do not play a significant role, as can be seen from the standard errors of the
corresponding coefficients in both B
and F
. This aspect was also briefly noted in the
discussion of the full-rank coefficient estimates. The reduced-rank model also has
an interpretation that is consistent with the correlation pattern mentioned earlier.
Specifically, the first linear combination of Xt has large negative values for the
temperature measurements at 700 mbar and 500 mbar, and the linear combination of
Zt mainly consists of the last measurement in the stratosphere with a predominantly
positive influence.
The temperature series, it may be recalled, are averages taken over three stations
and thus are meant to represent the average temperature across the region, represen￾tative for all the stations. Because of the average nature of the temperature series
and the proximity of the Dobson ozone stations under study, it may be expected
that the most significant linear combinations of the temperature measurements will
have a similar influence on ozone in all five stations. This is broadly confirmed by3.7 An Example on Ozone Data 109
the first row elements of the component A
 and by the elements of the component
E
 in the model. The first linear combination of BX t and F Z t may have the same
influence on ozone measurements of all five stations. This is tested by fitting a model
where individual ranks of C and D are assumed to be unity and further constraining
the elements within A and within E to be equal. This is equivalent to a multiple
regression procedure where the collection of all ozone measurements (combined
over all stations) is regressed on the 10 temperature measurements, taking account
of contemporaneous correlations across the five stations. This will be called the
“constrained” model,
Yt = 1BXt + 1F Zt + t or Y = 1[B, F][X
,Z
]
 + ,
where 1 denotes an m × 1 vector of ones, and the following estimation results were
obtained:
B
 = 7.5
-
−0.06 0.14 −0.58∗ −0.52 0.20.
F
 = 7.0
-
−0.10 −0.12∗ 0.31∗ −0.38∗ 0.58∗
.
.
Notice that this constrained model can be viewed as a special case of a conventional
reduced-rank model of rank one in which the left factor in the regression coefficient
matrix is constrained to be equal to 1.
It is interesting to see that the estimated combinations of temperature measure￾ments using the constrained model are roughly proportional to the first rows of B

and F
. The merit of the reduced-rank regression techniques is apparent when we
consider that we might not have identified the constrained model structure from
the data by merely looking at the full-rank coefficient estimates C
 and D. As a
contrast to the reduced-rank model and the constrained model, a “simplified” model
can be fitted where a temperature measurement at one fixed pressure level from
the tropospheric region and one from the stratospheric region are selected as the
potential explanatory variables. The two particular levels may vary from station to
station, and we select the temperature level whose estimated regression coefficient
has the largest negative t-ratio in the troposphere and the level whose estimate has
the largest positive t-ratio in the stratosphere, in the full-rank regression analysis, as
potential variables.
The performance of the three models is compared in terms of residual variances
of individual series rather than using an overall measure such as BIC. The results
are given in Table 3.3. It is clear from Table 3.3 that the results for the reduced￾rank model and the constrained model are fairly close to those of the full-rank
model. This indicates that we do not lose too much information by reducing the
dimension of the temperature measurements. Even the simplified model performs
close to the other two models for the three stations Vigna Di Valle, Mont Louis, and
Hohenpeissenberg. The analysis can be further improved with the models that can
account for spatial correlations as well.110 3 Reduced-Rank Regression Models with Two Sets of Regressors
Table 3.3 Summary of the analysis of the ozone-temperature data residual variances
Residual variance
Total Full rank Reduced rank Simplified Constrained
Station variance model model model model
Arosa 259.08 99.84 100.67 114.13 104.04
Vigna Di Valle 159.97 73.35 80.27 84.09 85.74
Mont Louis 124.10 56.85 60.72 68.95 88.92
Hradec Kralove 323.94 139.59 147.35 182.14 158.76
Hohenpeissenberg 274.78 103.28 105.99 112.36 112.36
3.8 Conclusion
The suggestion is to first estimate the regression of Yk on a small number of
the “preferred” predictor variables, say Zk, which are expected to yield canonical
correlations that are a large portion of the total correlations due to all predictor
variables, in the usual ordinary least squares method. Then use the previous
canonical analysis multivariate shrinkage methods applied to the residuals, where
the methods can be represented in terms of partial canonical correlation analysis.
That is, suppose there exists a partition of the predictor variables as (X
k, Z
k)
,
where Xk is n1 × 1, and Zk is n2 × 1 with n2 “small”, such that Zk contains
predictor variables that are “known” or “expected” to give a large portion of
the correlation with Yk. Accordingly, write the multivariate regression model as
Yk = CXk + DZk + k. Then, in the alternate suggested procedure, obtain the
usual LS estimates in the regression of Yk on Zk only as D∗ = yz−1 zz , and
similarly perform the LS regression of Xk on Zk to obtain the LS coefficient matrix
D2 = xz−1 zz . Then consider the shrinkage estimator of the regression coefficient
matrix C of the form
C
 = H−1
1 1H1C,  (3.39)
where
C
 = yx.z−1
xx.z ≡ (Y − D∗Z)(X − D2Z)

{(X − D2Z)(X − D2Z)

}
−1
is the ordinary LS estimate of C, with the corresponding estimate of D given by D =
D∗ − C
D2. In the same spirit as above, the shrinkage involved for C
 is developed
and expressible in terms of the squared sample partial canonical correlations ρ2
1j
of the Yk and Xk, after adjusting for Zk. The rows of the matrix H1 in this case
correspond to the sample partial canonical vectors for the variates Yk, and 1 in
(3.39) is a diagonal matrix with elements d
1j of a similar form as in (2.48) but in
terms of the ρ2
1j , that is,3.8 Conclusion 111
d
1j = (1 − p1)(ρ2
1j − p1)
(1 − p1)2ρ2
1j + p2
1(1 − ρ2
1j )
, j = 1, . . . , m,
where p1 = n1/(T − n2). This form is derived based on consideration of the mean
squared error matrix of prediction of Yk using (X
k, Z
k)
, and bias correction of the
usual squared sample partial canonical correlation estimators.
Some Recent Developments In Chapter 2, we have mentioned that the reduced￾rank regression problem had been studied under other names such as “redundancy”
analysis. With the extended models presented in this chapter, the redundancy
analysis has been extended to “partial redundancy” analysis, taking into account
sparseness and other known constraints beyond what are implied by the reduced￾rank set-up, see Takane and Jung (2008). In the case of linear discriminant analysis
discussed in Section 3.2, one should observe that if “m” is large, the estimation
of the error covariance matrix Σ = 1
T S is not efficient unless some structure is
imposed. In practice, the so-called spiked structure where except for a few large
eigenvalues, others are equal and small, is commonly observed. Note that Fisher￾Rao discriminant functions are based on the eigenvectors of S−1SB, where SB is the
between group sum of squares. In addition to entertaining a spiked structure on “S”,
tuning parameter “γ ” is introduced so that the eigenvalues and eigenvectors can be
computed based on S−1(γ SB). This is shown to provide a competitive classification
scheme by Niu et al. (2018). Finally, the model (3.23) is extended to many subsets of
predictors and is shown to have interesting practical applications. The model known
as integrative reduced-rank regression is studied by Li et al. (2019) and is covered
in some detail in Chapter 11.Chapter 4
Reduced-Rank Regression Model With
Autoregressive Errors
4.1 Introduction and the Model
The classical multivariate regression methods are based on the assumptions that
(i) the regression coefficient matrix is of full rank and (ii) the error terms in the
model are independent. In Chapters 2 and 3, we have presented regression models
that describe the linear relationships between two or more large sets of variables
with a fewer number of parameters than those posited by the classical model. The
assumption (i) of full rank of the coefficient matrix was relaxed, and the possibility
of reduced rank for the coefficient matrix has produced a rich class of models. In this
chapter, we also relax the assumption (ii) that the errors are independent, to allow
for possible correlation in the errors that may be likely in the regression modeling of
time series data. For the ozone/temperature time series data considered in Chapter 3,
the assumption of independence of errors appears to hold.
When the data are in the form of time series, as we consider in this chapter,
the assumption of serial independence of errors is often not appropriate. In these
circumstances, it is important to account for serial correlation in the errors. Robinson
(1973) considered the reduced-rank model for time series data in the frequency
domain with a general structure for errors. In particular, assuming the errors follow
a general stationary process, Robinson studied the asymptotic properties of both the
least squares and other more efficient estimators of the regression coefficient matrix.
However, often the serially correlated disturbances will satisfy more restrictive
assumptions, and, in these circumstances, it would seem possible to exploit this
structure to obtain estimates that may have better small sample properties than the
estimators proposed by Robinson. (For a related argument, see Gallant and Goebel
(1976).) This approach was taken by Velu and Reinsel (1987), and in this chapter,
we present the relevant methodology and results.
The assumption we make on the errors is that they follow a (first order)
multivariate autoregressive process. We consider the estimation problem with the
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8_4
113114 4 Reduced-Rank Regression Model With Autoregressive Errors
autoregressive error assumption in the time domain. The asymptotic distribution
theory for the proposed estimator is derived, and computational algorithms for the
estimation procedure are also briefly discussed. Discussion of methods for the initial
specification and testing of the rank of the coefficient matrix is presented, and an
alternative estimator, which is motivated by the need for initial identification of the
rank of the model, is also suggested. As an example, we present an analysis of some
macroeconomic data of the United Kingdom that was previously considered by
Gudmundsson (1977). This has been discussed earlier in Example 2.2 of Section 2.2.
Consider the multivariate regression model for the vector time series {Yt},
Yt = CXt + Ut, t = 1, 2,...,T, (4.1)
where Yt is an m × 1 vector of response variables, Xt is an n × 1 vector of predictor
variables, and C is an m × n regression coefficient matrix. As in Chapter 2, we
assume that C is of reduced rank,
rank(C) = r ≤ min(m, n). (4.2)
We assume further that the error terms Ut satisfy the stochastic difference equation
Ut = Ut−1 + t, (4.3)
where U0 is taken to be fixed. Here the t are independently distributed random
vectors with mean zero and positive-definite covariance matrix  , and  = (φij )
is an m × m matrix of unknown parameters, with all eigenvalues of  less than
one in absolute value. Hendry (1971), Hatanaka (1976), and Reinsel (1979), among
others, have considered maximum likelihood estimation of the model parameters
for the specification (4.1) and (4.3). When the regression matrix C is assumed to
be of lower rank r, following the discussion in Chapter 2, write C = AB with
normalization conditions as in (2.14),
A
A = I, and BxxB = 2, (4.4)
where xx = lim
T →∞
1
T
T
t=1 XtX
t , and  is a specified positive-definite matrix.
The main focus in this chapter is on the estimation of A and B based on a sample
of T observations. Although we do not consider the complementary result of (4.2)
that there are (m − r) linear constraints (assuming m<n) in detail here, the
implications for the extended model (4.1)–(4.3) are the same as the model discussed
in Chapter 2. The linear combinations 
iYt , i = 1, 2, . . . , (m−r), such that 
iC = 0,
do not depend on the predictor variables Xt , but they are no longer white noise
processes.4.3 Maximum Likelihood Estimators for the Model 115
4.2 Example on the U.K. Economy: Basic Data and Their
Descriptions
To illustrate the procedures to be developed in this chapter, we consider the macroe￾conomic data of the United Kingdom that has been previously analyzed by Klein
et al. (1961) and Gudmundsson (1977) (see Section 2.2). Klein et al. (1961) present
a detailed description of an econometric model based on an extensive database
including the data used in our calculations. Quarterly observations, starting from the
first quarter of 1948 to the last quarter of 1956, are used in the following analysis.
The endogenous (response) variables are y1 = index of industrial production,
y2 = consumption of food, drinks, and tobacco at constant prices, y3 = total
unemployment, y4 = index of volume of total imports, and y5 = index of volume
of total exports, and the exogenous (predictor) variables are x1 = total civilian
labor force, x2 = index of weekly wage rates, x3 = price index of total imports,
x4 = price index of total exports, and x5 = price index of total consumption. The
relationship between these two sets of variables is taken to reflect the demand side
of the macrosystem of the economy of the United Kingdom. The first three series of
the set of endogenous variables are seasonally adjusted before the analysis is carried
out. Time series plots of the resulting data are given in Fig. 4.1. These data are also
presented in Table A.3 of the Appendix.
Initially, the least squares regression of Yt = (y1t, y2t, y3t, y4t, y5t) on Xt =
(x1t, x2t, x3t, x4t, x5t) was performed without taking into account any autocorrela￾tion structure on the errors. In order to get a rough view on the rank of the regression
coefficient matrix, canonical correlation analysis was also performed. The squared
sample canonical correlations between Yt and Xt are 0.95, 0.67, 0.29, 0.14, and
0.01, which suggest a possible deficiency in the rank of C.
However, the residuals resulting from the least squares regression indicated a
strong serial correlation in the errors. While commenting on the model of Klein
et al. (1961), Gudmundsson (1977, p. 51) had a similar view that some allowance
should be made for the serial correlation in the residuals of many of their equations.
Therefore, we consider incorporating an autoregressive errors structure in the
analysis.
4.3 Maximum Likelihood Estimators for the Model
We now consider an estimation procedure that will yield an efficient estimator, in
the sense that it has the same asymptotic distribution as the maximum likelihood
estimator under the normality assumption for the errors. From the model (4.1) and
(4.3), we have
Yt − Yt−1 = CXt − CXt−1 + t, (4.5)116 4 Reduced-Rank Regression Model With Autoregressive Errors
Fig. 4.1 Quarterly macroeconomic data for the U.K., 1948–19564.3 Maximum Likelihood Estimators for the Model 117
where the error term t in (4.5) satisfies the usual independence assumptions. Let
 = [1,...,T ], and note that (4.5) leads to
Y − Y−1 = CX − CX−1 + , (4.6)
where Y = [Y1,...,YT ] and X = [X1,...,XT ] are the m × T and n × T
data matrices of observations on the response and predictor variables, and Y−1 =
[Y0,...,YT −1] and X−1 = [X0,...,XT −1] are the lagged data matrices. Before
we examine the reduced-rank situation, we first briefly consider estimation of
parameters C and  in the full-rank version of the model, that is, without the rank
restriction on C. The full-rank estimator is useful, in particular, as a basis for the
alternative reduced-rank estimator that is discussed in Section 4.5 and that is used
in the identification procedure described in Section 4.6 to specify the rank of the
coefficient matrix C.
We consider minimization of the criterion ST (C, ) = 1
2T tr(
) = 1
2T e
( ⊗
IT )e, where  = Y−Y−1 −CX+CX−1, e = vec(
), and the choice  = −1 
is taken. From (4.6), we have
vec(Y∗) = X∗vec(C
) + vec(
),
where Y∗ = Y−Y−1 and X∗ = (Im ⊗ X
)−( ⊗ X
−1) is an mT ×mn matrix. It
follows from standard results for the linear regression model that, for given values
(estimates)  and  of  and  , the full-rank estimate of C is obtained as the
(estimated) generalized least squares (GLS) estimate, given by
vec(C

) = G−1X∗(−1
 ⊗ IT )vec(Y∗)/T , (4.7)
where G = X∗(−1  ⊗ IT )X∗/T , with Y∗ = Y − Y−1 and X∗ = (Im ⊗
X
) − ( ⊗ X
−1). The covariance matrix of the full-rank estimate is estimated
by Cov /[vec(C

)] = 1
T G−1. Conversely, for a given value (estimate) C
 of C, the
corresponding estimate of  to minimize the above criterion ST (C, ) is given by
 = UU
−1(U−1U
−1)
−1,
where U = Y−C
X = [U
1,..., U
T ], U−1 = [U
0,..., U
T −1], with U
t = Yt −CX t ,
t = 1,...,T , and  is estimated as  = 
/T , where  = U − U−1. The
estimation scheme is thus performed most conveniently by an iterative procedure
that alternates between estimation of C through (4.7), for given estimate , and
estimation of  as indicated for given estimate C
. The most common starting value
is the least squares estimate of C, given in (1.7), that ignores the AR(1) errors
structure.
Now for estimation of the reduced-rank model with C = AB, let θ =
(α
, β
, φ
)
, where α = vec(A
), β = vec(B
), and φ = vec(
). We again consider
minimization of the criterion118 4 Reduced-Rank Regression Model With Autoregressive Errors
S1T (θ ) = 1
2T
tr(
), (4.8)
subject to normalization conditions
A
A = Ir, B  1
T
XX

B = 2, (4.9)
where 2 is diagonal. If the autocorrelation structure implied by (4.3) is ignored by
setting  = 0, then one is led to the least squares estimator considered by Robinson
(1973). It has been shown that the resulting least squares estimator is consistent but
not efficient. To arrive at the asymptotic theory for the estimator that is obtained
by minimizing (4.8) subject to (4.9), we shall again use the results of Silvey (1959)
and Robinson (1972, 1973). For the normalization conditions (4.9), we shall be
particularly interested in the choice of  = −1  for the asymptotic results where
 is the sample covariance matrix of the residuals obtained from the maximum
likelihood estimation of the model (4.1) and (4.3) with the rank restriction on C.
Note because of this choice of , the normalization conditions vary with sample
quantities. As in Chapters 2 and 3, however, to provide more explicit results, the
asymptotic theory presented here is for the estimator with the choice of  = −1  ,
which is a population quantity. For the sake of brevity, we omit the details, but the
main result follows.
Theorem 4.1 Consider the model (4.1) and (4.3) under the stated assumptions,
with C = AB, where A and B satisfy the normalization conditions (4.4), and let
h(θ ) = 0 denote the r2 × 1 vector of normalization conditions (4.4) obtained by
stacking the conditions (4.4) without duplication. The first r(r + 1)/2 components
of h(θ ) are of the form α
i−1  αj − δij , i ≤ j , where αi denotes the ith column of
the matrix A and δij = 1 for i = j and 0 otherwise, and the remaining r(r − 1)/2
elements are β
ixxβj , i<j , where β
i denotes the ith row of B. Assume the limits
lim
T →∞
1
T

T
t=1
XtX
t = xx = x (0), lim
T →∞
1
T

T
t=2
Xt−1X
t = x (1)
exist almost surely, and define
G = (−1
 ⊗ x (0)) − (−1
  ⊗ x (1)

)
−(
−1
 ⊗ x (1)) + (
−1
  ⊗ x (0))
= Cov[vec(Xt 
t−1
 − Xt−1
t−1
 )].
Let θ denote the estimator that absolutely minimizes (4.8) subject to (4.9) for the
choice  = −1  . Then as T → ∞, θ → θ almost surely and √T (θ − θ ) has a
multivariate normal distribution with zero mean vector and covariance matrix given4.3 Maximum Likelihood Estimators for the Model 119
by the leading (r(m+n)+m2)-order square submatrix of the (r(m+n+r)+m2)-
order matrix
 Bθ −Hθ
−H
θ 0
−1 
Bθ 0
0 0   Bθ −Hθ
−H
θ 0
−1
, (4.10)
where
Bθ = M

G 0
0 −1  ⊗ u

M, M =

Im ⊗ B A ⊗ In 0
0 0 Im ⊗ Im

,
Hθ = {(∂hj (θ )/∂θi)} is the matrix of first derivatives of the constraints in (4.4) and
is of order r(m + n) + m2 × r2, and u = u +  , which can be obtained
from vec(u) = [Im2 −  ⊗ ]
−1vec( ). Specifically, it can be shown as in
Theorem 3.2 of Chapter 3 that the covariance matrix of this limiting distribution
is given by W = (Bθ + HθH
θ )−1Bθ (Bθ + HθH
θ )−1. We note that the choice
of normalization condition (4.4) guarantees the existence of the inverse matrix in
(4.10).
Proof We shall only briefly indicate the proof here because many of the details
follow along similar lines as Silvey (1959) and have been discussed in the proof of
Theorem 3.2 in Chapter 3. Initially, we consider estimates that minimize S∗
1T (θ ) −
h(θ )
λ, where λ is the Lagrangian multiplier and S∗
1T (θ ) denotes the criterion (4.8)
with the choice  = −1  . Let θ andλ be solutions of the first order equations. It can
be shown that {∂2S∗
1T (θ )/∂θ∂θ
} → Bθ almost surely. So following Silvey (1959),
we have the result below (similar to (3.29)).
 Bθ + o(1) −Hθ + o(1)
−H
θ + o(1) 0
  √T (θ − θ )
√Tλ

=

−
√T∂S∗
1T (θ )/∂θ
0

. (4.11)
Thus the asymptotic distribution of √T (θ − θ ) follows from the asymptotic
distribution of the quantity √T∂S∗
1T (θ )/∂θ. For the model considered, we can
express in vector form as
y∗ = vec(Y∗) = X∗vec(C
) + vec(
)
= X∗(Im ⊗ B
)α + e = X∗(A ⊗ In)β + e,
where e = vec(
) has covariance matrix  ⊗ IT . The vector AR(1) error part of
the model, U = U−1 + , can similarly be written as vec(U
) = vec(U
−1
) +
vec(
) = (Im ⊗ U
−1)φ + e. The criterion in (4.8) with  = −1  can also be
rewritten as S∗
1T (θ ) = 1
2T e
(−1  ⊗ IT )e, and we therefore find that120 4 Reduced-Rank Regression Model With Autoregressive Errors
∂S∗
1T (θ )
∂θ = 1
T
∂e
∂θ (−1
 ⊗ IT )e = − 1
T
⎡
⎣
(Im ⊗ B)X∗
(A ⊗ In)X∗
(Im ⊗ U−1)
⎤
⎦ (−1
 ⊗ IT )e
= − 1
T
⎡
⎣
Im ⊗ B 0
A ⊗ In 0
0 Im ⊗ Im
⎤
⎦

(−1  ⊗ X)e − (
−1  ⊗ X−1)e
(−1  ⊗ U−1)e 
,
where X∗ = (Im ⊗ X
) − ( ⊗ X
−1).
Hence, noting that (
−1  ⊗ X−1)e = (
−1  ⊗ In)vec(X−1
) and so on,
the vector of partial derivatives can be written as √T∂S∗
1T (θ )/∂θ = M
F
√T ZT ,
where ZT = vec{X−1
, X
, U−1
}/T , and
F =


−1  ⊗ In −(−1  ⊗ In) 0
0 0 −(−1  ⊗ Im)

.
It is known (e.g., Anderson (1971, Chapter 5)) that √T ZT
d
−→ N (0,V) as T → ∞,
where
V =
⎡
⎣
 ⊗ x (0)  ⊗ x (1) 0
 ⊗ x (1)  ⊗ x (0) 0
0 0  ⊗ u
⎤
⎦ .
It thus follows that √T∂S∗
1T (θ )/∂θ d
−→ N (0, Bθ ), where Bθ = M
F
VFM is as
defined, and also that {∂2S∗
1T (θ )/∂θ∂θ
} → E{ 1
T
∂e
∂θ (−1  ⊗ IT ) ∂e
∂θ} = Bθ . The
asymptotic theory of Theorem 4.1 thus follows from (4.11).
Note that rank(Bθ ) = rank(M) = r(m + n − r) + m2 < r(m + n) + m2 because
of the existence of r2 linearly independent restrictions between the columns. Hence,
the information matrix Bθ is singular, and this makes the computation of the
asymptotic covariances of the estimators in Theorem 4.1 more difficult. Similar to
the discussion in Section 3.5, Silvey (1959, p. 401) suggested alternatively that the
asymptotic covariance matrix of θ is given by the leading submatrix of

Bθ + HθH
θ −Hθ
−H
θ 0
−1 
Bθ 0
0 0  Bθ + HθH
θ −Hθ
−H
θ 0
−1
,
which can be shown to be equivalent to the matrix W given in Theorem 4.1.
The asymptotic theory for the estimators A
 and B
 is useful for inferences
concerning whether certain components of Xt contribute in the formation of the
explanatory indexes BXt and also whether certain components of Yt are influenced
by certain of these indexes. The overall regression coefficient matrix C = AB
is, of course, also of interest, and the asymptotic distribution of the estimator
C
 = A
B
 follows directly from results of Theorem 4.1. Based on the relation4.4 Computational Algorithms for Efficient Estimators 121
C
 − C = A
B
 − AB = (A
− A)B + A(B
 − B) + Op(T −1), it follows directly
that √T vec(C
 − C
) d
−→ N (0, M1W1M
1), where M1 = [Im ⊗ B
, A ⊗ In] and W1
denotes the asymptotic covariance matrix of √T {(α − α)
, (β
− β)
}
, that is, the
leading r(m + n)-order square submatrix of the matrix W in Theorem 4.1.
Remark–Asymptotic Independence of Estimators As will be discussed in more
detail in the next section, we note that the asymptotic covariance matrix W in
Theorem 4.1 is block diagonal of the form W = diag(W1, −1  ⊗ u), where
W1 = (Bδ + H1H
1)−1Bδ(Bδ + H1H
1)−1, with δ = (α
, β
)
, Bδ = M
1GM1,
and H1 = ∂h(θ )
/∂δ. This indicates that the estimates A
 and B
 of the regression
components of the model (4.1) and (4.3) are asymptotically independent of the
estimate  of the AR(1) error component of the model. In addition, it follows that
the asymptotic covariance matrix of √T vec(C
 − C
), where C
 = A
B
, can be
expressed more explicitly as
M1W1M
1 = M1(M
1GM1 + H1H
1)
−1M
1GM1(M
1GM1 + H1H
1)
−1M
1
≡ M1(M
1GM1)
−M
1.
4.4 Computational Algorithms for Efficient Estimators
We now briefly comment on the procedure for computing the efficient estimator.
An iterative procedure can be formulated along the lines of Aitchison and Silvey
(1958) modification of the Newton–Raphson method to account for constraints as
discussed in Section 3.5. It is based on a modification of the first order equations in
(4.11) to

Bθ + HθH
θ −Hθ
−H
θ 0
 θ − θ
λ

=

−∂S∗
1T (θ )/∂θ
0

, (4.12)
where explicitly
∂S∗
1T (θ )/∂θ = vec(∂S∗
1T /∂A
, ∂S∗
1T (θ )/∂B
, ∂S∗
1T (θ )/∂
),
with
∂S∗
1T (θ )/∂A = T −1(BX−1
−1
  − BX
−1
 ),
∂S∗
1T (θ )/∂B = T −1(X−1
−1
 A − X
−1
 A),
∂S∗
1T (θ )/∂ = −T −1U−1
−1
 ,122 4 Reduced-Rank Regression Model With Autoregressive Errors
U = Y − ABX, and  = U − U−1. Hence, at the ith step, the iterative scheme is
given by
 θi
λi

=
θi−1
0

−

Wi−1 Qi−1
Q
i−1 −Ir2
 )
∂S∗
1T (θ )/∂θ|θi−1
h(θ )|θi−1
*
, (4.13)
where

W Q
Q −Ir2

=

Bθ + HθH
θ −Hθ
−H
θ 0
−1
,
so that
W = (Bθ + HθH
θ )
−1Bθ (Bθ + HθH
θ )
−1,
and
Q = −(Bθ + HθH
θ )
−1Hθ .
The covariance matrix of θi is approximated by
(1/T )(Bθ + HθH
θ )
−1Bθ (Bθ + HθH
θ )
−1 = (1/T )W
evaluated at θi. The iterations are carried out until either the successive differences
between θi or the magnitudes of the Lagrangian multipliersλi or the magnitudes of
first order quantities ∂S∗
1T (θi−1)/∂θ are reasonably small.
However, as in the full-rank estimation discussed in Section 4.3, the calculations
in the iterative scheme for θ can be presented in a somewhat simpler form, in which
one alternates between estimation of A and B (α and β), for given estimate of ,
and estimation of  for given estimates of A and B. Let δ = (α
, β
) so that θ =
(δ
, φ
)
. Then note that the matrix Bθ is block diagonal, having the form Bθ =
diag(M
1GM1, −1  ⊗ u) ≡ diag(Bδ, Bφ), where M1 = [Im ⊗ B
, A ⊗ In]. In
addition, because the normalization constraints h(θ ) = 0 (h(δ) = 0) do not involve
, the matrix of first derivatives of h(θ ) with respect to θ can be partitioned as
Hθ = ∂h(θ )
/∂θ = [H
1, 0
]

, where H1 = ∂h(θ )
/∂δ. Therefore, upon rearranging
the terms, we see that the first order equations in (4.12) separate as

Bδ + H1H
1 −H1
−H
1 0
 δ − δ
λ

=

−∂S∗
1T (θ )/∂δ
0

, (4.14)
and Bφ(φ
 − φ) = −∂S∗
1T (θ )/∂φ. These last equations are equivalent to ( −
)u = T −1U
−1 ≡ T −1(U − U−1)U
−1. On substituting the sample estimate
u = T −1U−1U
−1 for u, we obtain the solution  = UU
−1(U−1U
−1)−1 for
the estimate of , where U = Y − C
X with C
 = A
B
. The iteration scheme in4.5 Alternative Estimators and Their Properties 123
(4.13) is then reduced in dimension, based on the uncoupled first order equations in
(4.14), by replacing θi by δi and making the additional appropriate simplifications.
Specifically, because

Bδ + H1H
1 −H1
−H
1 0
−1
=

W1 Q1
Q
1 −Ir2

,
where Q1 = −(M
1GM1 + H1H
1)−1H1, this leads to the iterative step to obtain the
estimates A
and B
 as follows:
δi = δi−1 − W1,i−1(∂S∗
1T (θ )/∂δ)|θi−1 − Q1,i−1h(δ)|δi−1
λi = −Q
1,i−1(∂S∗
1T (θ )/∂δ)|θi−1 + h(δ)|δi−1
.
The iterations thus alternate between the above equations to update the estimates of
A and B and the step given to update the estimate of . The covariance matrices
of the final estimates δ and φ
 are provided by 1
T W1 and 1
T W2, where W2 =  ⊗
−1 u ≡ B−1
φ .
4.5 Alternative Estimators and Their Properties
Initial specification of an appropriate rank of the regression coefficient matrix
is desirable before the iterative estimation procedure is employed and the initial
estimates of the component matrices are important for the convergence of iterative
methods. This dictates a need for an alternative preliminary estimator that is
computationally simpler and that may also be useful at the initial model specification
stages to identify an appropriate rank for the model. In practice, the full-rank
estimator may be computed first, and if there is a structure to the true parameter
matrix, we should be able to detect it by using the full-rank estimator directly. The
alternative initial estimator we propose below is essentially based on this simple
concept.
To motivate the alternative estimator, we first recall the solution to the reduced￾rank regression problem for the single regressor case under independent errors, as
presented in Chapter 2. The sample version of the criterion that led to the solution in
the single regressor case reduces to finding A and B by minimizing (see Eq. (2.12
))
tr[1/2(C
 − AB)xx (C
 − AB)
1/2]
= [vec{(C
 − AB)
}]
( ⊗ xx )vec{(C
 − AB)
}, (4.15)
where C
 = yx−1 xx is the full-rank least squares estimator. Using the Householder–
Young theorem, the problem is solved explicitly as in (2.17). This is not the case
with an AR(1) error model since the full-rank estimate of C, as displayed in (4.7)124 4 Reduced-Rank Regression Model With Autoregressive Errors
of Section 4.3, does not have an analytically simple form. For given  and  (≡
−1), minimization of the efficient criterion (4.8) reduces to the problem of finding
A and B such that
[vec{(C
 − AB)
}]
Gvec{(C
 − AB)
} (4.16)
is minimized, where C
 is the corresponding full-rank GLS estimate as described
in Section 4.3, vec(C

) = G−1X∗(−1  ⊗ IT )vec(Y∗)/T , and G = X∗(−1  ⊗
IT )X∗/T . Now comparison of this latter expression with (4.15) indicates that an
explicit solution via the Householder–Young Theorem is not possible unless the
matrix G can be written as the Kronecker product of two matrices of appropriate
dimensions. This can happen only under rather special circumstances, for instance,
when  = ρI . Hence, the solution is not simple as in the standard independent
error case.
Motivated by a desire to obtain initial estimates for the reduced-rank model (for
various choices of the rank r) that are relatively easy to compute in terms of the
full-rank estimator C
 of model (4.1)-(4.3), we consider an alternate preliminary
estimator. This estimator essentially is chosen to minimize (4.16), but with G
replaced by ( ⊗ XX
/T ), the weight matrix that appears in (4.15), the inefficient
least squares criterion. Note since this results in a criterion that is essentially the
least squares criterion (4.15) with a better estimate C
 of C in place of the least
squares estimate yx−1 xx , the natural choice of  is  = −1 u , where u =
(Y − C
X)(Y − C
X)
/T . This proposed estimation procedure; it should be kept in
mind, does not have any overall optimality properties corresponding to our model,
but is motivated by the standard model and computational convenience.
As in the single regressor case, we can obtain the alternative estimators A
∗
and B
∗ using the Householder–Young theorem, which satisfy the normalization
conditions A
∗−1 u A
∗ = Ir, B
∗(XX
/T )B
∗ = ∗2. From the results in Chap￾ter 2, it follows that the columns of 1/2 xx B
∗ are chosen to be the eigenvectors
corresponding to the first r eigenvalues of the matrix 1/2 xx C

−1 u C
1/2 xx , the
positive diagonal elements of ∗ are the square roots of the eigenvalues, and
A
∗ = C
xxB
∗∗−2. Equivalently, we can represent the alternate estimators by
taking A
∗ = 1/2 u V
(r) and B
∗ = V

(r)−1/2 u C
, where V
(r) = [V
1,..., V
r] and the
V
j are normalized eigenvectors of the matrix −1/2 u C
xxC

−1/2 u corresponding
to its r largest eigenvalues. Although this is primarily of interest, for preliminary
model specification, and used as an initial estimator, the asymptotic distribution of
the alternative estimator can also be developed along similar lines as in Theorem 4.1.
We shall not present details, but if we let α∗ = vec(A
∗), β
∗ = vec(B
∗),
δ∗ = (α∗, β
∗) denote the alternative estimator and δ∗ = (α∗, β∗)
, the parameter
values satisfying the corresponding population normalizations, then it can be shown
that √T (δ∗ − δ∗) has a limiting multivariate normal distribution with zero mean
vector and covariance matrix given by the leading r(m + n)-order square submatrix
of the r(m + n + r)-order matrix4.5 Alternative Estimators and Their Properties 125

M∗
1 (−1 u ⊗ xx )M∗
1 −H∗
1
−H∗
1 0
−1
×

M∗
1 (−1 u ⊗ xx )G−1(−1 u ⊗ xx )M∗
1 0
0 0 
×

M∗
1 (−1 u ⊗ xx )M∗
1 −H∗
1
−H∗
1 0
−1
,
where M∗
1 = [Im ⊗ B∗, A∗ ⊗ In] and H∗
1 = {(∂hj (δ∗)/∂δ∗
i )} is r(m + n) × r2.
4.5.1 A Comparison Between Efficient and Other Estimators
Though efficient estimators are desirable, the alternative estimator discussed above,
as well as the least squares estimator, is more easily computable and may also be
quite useful as initial estimators and for model specification purposes. We will
briefly consider their merits in terms of asymptotic efficiencies relative to the
efficient estimator.
To obtain relatively explicit results, we shall restrict attention to the rank 1
case only, so that C = αβ
, where α and β are column vectors that satisfy the
normalizations α
−1  α = 1, β
x (0)β = λ2. In addition, we assume for simplicity
that  is known and has the form  = ρI , where |ρ| < 1. In this case, we can show
that the asymptotic covariance matrix of the efficient estimator (α
, β

) is
W1 =
 1
a ( − αα
) 0
0 −1

, (4.17)
with
 = (1 + ρ2)x (0) − ρ(x (1) + x (1)

),
and
a = (1 + ρ2)λ2 − 2ρ(β
x (1)β) = β
β.
In the classical independent error case, ρ = 0, W1 reduces to the result given in
Robinson (1974), W1 = diag{(β
x (0)β)−1( − αα
), x (0)−1) as given in the
discussion following the proof of Theorem 2.4 in Section 2.5.
For the alternative estimator, note the decomposition is C = α∗β∗, where the
column vector α∗ satisfies the normalization condition α∗−1 u α∗ = 1. Because
of this change in the constraint on α∗, it is necessary to make a transformation
of the parameter estimator so that the alternative estimator will be comparable to126 4 Reduced-Rank Regression Model With Autoregressive Errors
the efficient estimator. This transformation is easy to obtain (since u = {1/(1 −
ρ2)} for the special case  = ρI ) as follows: β = {1/(1 − ρ2)1/2}β∗, α =
(1−ρ2)1/2α∗, and λ = λ∗/(1−ρ2)1/2. Hence, the asymptotic covariance matrix of
the alternative estimator when normalized in the same way as the efficient estimator
is
R =
) a∗
λ4 ( − αα
) 0
0 −1
*
, (4.18)
where
a∗ = β
x (0)−1x (0)β.
Comparison of the matrix W1 in (4.17) with R in (4.18) indicates that, asymptot￾ically, the alternative estimator of β is efficient, but the alternative estimator of
α is not. In fact, the inefficiency of the alternative estimator of α can be readily
established by verifying that a∗/λ4 ≥ 1/a through the Cauchy–Schwarz inequality.
This asymptotic comparison has an interesting interpretation that indicates that an
efficient estimator can be achieved indirectly by a two-step procedure in which the
efficient alternative estimator β
 is first obtained, and then an efficient estimate of
α is obtained by regression of Yt on the index β

Xt , taking into account the AR
error structure. We note that one special case in which the alternate estimator α
is efficient occurs when the Xt have first order autocovariance matrix of the form
x (1) = γ x (0), since then  = (1 + ρ2 − 2γρ)x (0) and hence a∗/λ4 = 1/a,
and so in particular α is efficient when the Xt are white noise (γ = 0).
Now we shall compare the least squares (LS) estimator with the alternative
estimator for the rank one case. From the asymptotic results of Robinson (1973)
and using a similar parameter transformation as in the case of the alternative
estimator, we obtain the covariance matrix for the least squares estimator under
the normalization α
−1  α = 1 as
S =
) a1
λ4(1−ρ2)
( − αα
) 0
0 1
(1−ρ2)
x (0)−1∗
x (0)x (0)−1
*
(4.19)
with
a1 = λ2 + 2
∞
j=1
(β
x (j )β)ρj ,
and
∗
x (0) = x (0) +∞
j=1
(x (j ) + x (j )
)ρj .4.6 Identification of Rank of the Regression Coefficient Matrix 127
It is not easy to analytically compare the above covariance matrix with R in (4.18)
in general. However, for illustration, suppose the Xt’s are first order autoregressive
with coefficient matrix equal to γ I . With these simplifying conditions, we have
shown before that the alternative estimator is efficient. However, the LS estimator
performs poorly. We shall particularly concentrate on the estimators of α and
compare the elements in the first diagonal block in S and R. Thus we compare
a1/λ4(1 − ρ2) with a∗/λ4, where a1 = λ2(1 + γρ)/(1 − γρ) and a∗ = λ2/(1 +
ρ2 − 2γρ). For γ = ρ = 0.8, the efficiency of the LS estimator of α relative
to the alternative estimator of α is 0.22. This simple comparison suggests that the
alternative estimator may perform substantially better than the LS estimator, and as
mentioned before, the alternative estimator is also computationally convenient.
4.6 Identification of Rank of the Regression Coefficient
Matrix
It is obviously important to be able to determine the rank of the coefficient matrix
C for it is a key determinant in the structure of the reduced-rank regression
problem. From the asymptotic distribution theory presented earlier, we do not learn
about the rank of C because the asymptotic results are derived assuming the rank
conditions are true. In the single regressor model with independent errors, the exact
relationship between canonical correlation analysis and reduced-rank regression
allows one to check on the rank by testing if certain canonical correlations are
zero (see Section 2.6). When the rank of C is r, under independence and normality
assumptions, the statistic suggested by Bartlett (1947), T m
j=r+1 log(1 + λ2
j ) ≡
−T m
j=r+1 log(1 − ρ2
j ), which is also the likelihood ratio statistic as discussed
in Section 2.6, has a limiting χ2
(m−r)(n−r) distribution (Hsu 1941). However, this
test procedure is not valid under the autoregressive error model of (4.3). We need
to develop other large sample quantities to test the hypothesis of interest in the
framework of a correlated error structure.
Likelihood ratio test procedures have been shown to be valid for rather general
autocorrelated error structures by Kohn (1979, Section 8). The likelihood ratio test
statistic takes the form
χ2 = T log(| |/| |), (4.20)
where  denotes the unrestricted maximum likelihood estimate of  under the
autoregressive error structure and  the restricted estimate based on the restricted
maximum likelihood estimate C
 = A
B
of C subject to the constraint that rank(C) =
r. Hence, this statistic can be used to test the null hypothesis that rank(C) = r, and
the statistic will follow a χ2-distribution with mn − {r(m + n) − r2} = (m −
r)(n − r) degrees of freedom, the number of constrained parameters, under the null
hypothesis. This statistic can be used to test the validity of the assumption of a128 4 Reduced-Rank Regression Model With Autoregressive Errors
reduced-rank structure of rank r for C, once the efficient estimates A
 and B
 have
been obtained as in Section 4.3 with C
 = A
B
. However, this procedure would not
be useful at the preliminary specification stage where we are interested in selecting
an appropriate rank for C prior to efficient estimation. This leads to consideration of
an “approximate” likelihood ratio procedure, as a rough guide to preliminary model
specification (e.g., see Robinson (1973, p. 159) for a related suggestion), in which
the alternative estimator of C derived under the assumption of AR(1) errors is used
to form the estimate of  in the statistic (4.20) as an approximation to the efficient
constrained estimator  , with the resulting statistic treated as having roughly a
chi-squared distribution with (m − r)(n − r) degrees of freedom under H0. Thus,
if C
∗ = A
∗B
∗ denotes the alternative estimator of C from Section 4.5, then we use
∗
 = ∗
∗/T in (4.20) in place of the restricted maximum likelihood estimate
 , where∗ = U∗ − ∗U∗
−1, ∗ = U∗U∗
−1(U∗
−1
U∗
−1)−1, and U∗ = Y − C
∗X is
obtained from the alternative estimate.
It should be emphasized that the main motivation (and advantage) of using this
approximate test, with the alternative estimator approximating the exact restricted
ML estimate, is that preliminary tests for models of various ranks can all be tested
simultaneously based on one set of eigenvalue–eigenvector calculations for the
alternative estimator. The validity of the model (rank) selected at this preliminary
stage would be checked further, of course, following efficient estimation of the
parameters A and B by use of the chi-squared likelihood ratio statistic (4.20) with
the efficient estimates A
, B
, and  . However, it should be noted that the use of the
preliminary approximate likelihood ratio test would tend to be “conservative,” in the
sense of yielding larger values for the test statistic than the exact procedure, since the
value |∗
 | based on the preliminary alternative estimator must be larger than that
of | | from the final restricted ML estimate. It is possible that a computationally
simple preliminary test that has an exact asymptotic chi-squared distribution may
be obtainable, and an investigation of this matter is a worthwhile topic for further
research.
4.7 Inference for the Numerical Example
The initial analysis of the macroeconomic data indicated that the serial correlations
must be taken into account for proper analysis. Thus, having tentatively identified
that the data set may be analyzed in the framework of the models described in this
chapter, we first compute the full-rank estimator under an autoregressive structure
for the errors. A first order autoregressive error model is considered for the purpose
of illustration, although one might argue toward a second-order model. The trace
of the residual covariance matrix  for the AR(1) model is 133.52, and the trace
under the ordinary least squares fit, i.e., the zero autocorrelation model, is 215.83,
which indicates that significant improvement occurs when an AR(1) model for the
errors is assumed. The full-rank estimate of the regression coefficient matrix under4.7 Inference for the Numerical Example 129
this assumption on the errors is given as follows:
C
 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
5.42∗ −0.90∗ −0.06 −0.42∗ 1.90∗
1.24∗ −0.10 0.13 −0.32∗ 0.56∗
−5.92 0.29 −0.37 1.21 −1.05
3.13∗ −0.51 −0.11 −0.13 1.38∗
9.46∗ −0.37 0.71∗ −0.86∗ 0.39
⎤
⎥
⎥
⎥
⎥
⎥
⎦
.
The starred entries indicate that the t-ratios of these elements are greater than
1.65. Although the elements of C
 are not too different from the elements of the
least squares estimate, there are some differences in the significance of individual
elements when autocorrelation in the errors is accounted for. The corresponding
estimate of the coefficient matrix  in the AR(1) error model (4.3) is given by
 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
−0.186 −0.323 −0.121 −0.147 0.170
−0.121 −0.450 −0.031 −0.106 0.082
−0.235 −0.324 0.853 0.596 0.123
0.151 0.393 −0.266 −0.081 −0.096
−0.946 0.801 −0.116 −0.275 0.266
⎤
⎥
⎥
⎥
⎥
⎥
⎦
with eigenvalues of  equal to 0.649, −0.508, 0.094, and 0.083 ± 0.240i. These
eigenvalues provide an additional indication that autocorrelation in the error term
Ut in the regression model (4.1) is substantial and the error term is stationary.
In order to initially specify the rank of the regression coefficient matrix C, the
approximate chi-squared likelihood ratio statistic (4.20), which is based on the full￾rank estimator C
 and the alternative estimators under the AR(1) error structure, was
computed for various values of the rank r, and the results are presented in Table 4.1.
It is seen from this table that the rank of the matrix C may be taken to be equal
to 2 when serial correlations are accounted for. For the purpose of illustration, we
take the rank to be equal to 2, and the efficient estimates of the component matrices
and their standard errors can be computed using the iterative algorithm given in
Section 4.4. These are presented below:
Table 4.1 Summary of computations of approximate chi-squared statistics used for specification
of rank of the model
r = rank of the Chi-squared
matrix C under H0 statistic d.f. Critical value (5%)
78.42 25 37.70
1 31.09 16 26.30
2 13.48 9 16.90
3 3.35 4 9.49
4 0.20 1 3.84130 4 Reduced-Rank Regression Model With Autoregressive Errors
A
 =
⎡
⎢
⎢
⎣
2.70 1.09 −1.69 2.54 2.84
(0.13) (0.07) (1.16) (0.34) (0.43)
0.21 0.31 0.15 −0.86 3.80
(0.43) (0.21) (1.28) (0.49) (0.50)
⎤
⎥
⎥
⎦ ,
B
 =
⎡
⎢
⎢
⎣
0.81 −0.13 0.08 −0.24 0.61
(0.41) (0.09) (0.07) (0.08) (0.16)
1.57 0.19 0.28 −0.24 −0.49
(0.47) (0.12) (0.08) (0.10) (0.19)
⎤
⎥
⎥
⎦ .
We compute the matrix C
 = A
B
 to see how well the full-rank estimate C
 is
reproduced by the product of the lower-rank matrices,
C
 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
2.51 −0.30 0.27 −0.70 1.54
1.38 −0.08 0.17 −0.34 0.51
−1.14 0.24 −0.09 0.37 −1.10
0.72 −0.49 −0.05 −0.41 1.97
8.26 0.37 1.30 −1.61 −0.12
⎤
⎥
⎥
⎥
⎥
⎥
⎦
.
It can be seen that the entries of the full-rank estimate C
 are fairly well
reproduced in C
, and this could certainly be improved by entertaining the possibility
that the rank of the matrix is 3. Alternatively, we can compare the trace of the
residual covariance matrix  , 145.42, obtained from estimation of the rank 2
model, with the trace of  , which we had earlier mentioned as 133.52. The
9% increase in the trace value has to be viewed in conjunction with the reduction
in the number of (functionally) independent regression parameters from 25 under
the full-rank model to 16 under the rank 2 model. Under the null hypothesis that
rank(C) = 2, we also compute the exact likelihood ratio statistic (4.20) using the
restricted ML estimate C
 to check the validity of the reduced-rank specification.
The computed value is χ2 = 6.92 with 9 degrees of freedom, which suggests
adequacy in the reduced-rank specification. Subsequent calculation of the restricted
ML estimate for a rank 1 model specification and of the associated likelihood ratio
statistic (4.20) led to rejection of a regression model of rank 1.
It is not our intention to give a detailed interpretation of the estimates of
the component matrices, but some observations are worth noting. The two linear
combinations of Xt , X∗
t = BX t , are the most useful linear combinations of the
exogenous variables for predicting the endogenous variables Yt , and these are
plotted in Fig. 4.2. Based on the standard errors, note that the index of weekly
wage rates does not appear to play any significant role in explaining the endogenous
variables considered here. The first linear combination is (approximately) x∗
1t =
0.61x5t − 0.24x4t + 0.81x1t , an index based on only the price index of total
consumption, the price index of total exports, and the labor force. This combination
is essentially a trend variable and affects all the endogenous variables except4.8 An Alternate Estimator with Kronecker Approximation 131
Fig. 4.2 First two exogenous “index” series for the U.K. economic data
unemployment. The second linear combination x∗
2t = 1.57x1t +0.28x3t −0.24x4t −
0.49x5t seems to be especially useful only to explain y5t , the index of volume
of total exports. Also, it may be observed that neither combination significantly
influences the unemployment series. To come up with a serious model of the U.K.
economy, we may have to expand the set of exogenous variables, and the nature of
that investigation would be much more detailed than that considered here.
4.8 An Alternate Estimator with Kronecker Approximation
In Section 4.5, we replaced the estimate of the covariance matrix of vec(C

) = G by

Σ−1 u ⊗ Σxx 
to obtain the alternate estimators of A and B. As shown earlier, the
Kronecker structure of separable covariance matrices leads to fast computation of
the parameters and may be especially useful for dimension reduction aspects of data
with a large number of variables. The suggested Kronecker structures in Section 4.5
are based on intuitive ideas. In this section, we describe the nearest Kronecker
product approximation of a matrix of general form suggested by Van Loan and
Pitsianis (1993). The algorithm is simple to use, and the solution preserves the
positive definiteness required for the covariance matrices.
Let G be a covariance matrix of dimension mn × mn. This could be a sum of
several Kronecker products or simply a matrix that is not in the form of a Kronecker
product. The solution to the approximation of G by G1 ⊗ G2, i.e., to minimize

G − G1 ⊗ G2
F = 
R(G) − (G1) ⊗ (G2)
F , (4.21)
where R(G) is a matrix that is rearranged from the columns and rows of
G so that 
G
F = 
R(G)
F . The rearrangement function is R(G) =
[vec
(G11), . . . vec
(Gm1), vec
(G21), . . . , vec
(Gmm)], where Gkl is the k,lth n×n132 4 Reduced-Rank Regression Model With Autoregressive Errors
block of G. The rearranged matrix R(G) is m2 × n2 and can be rectangular. Here

·
F denotes the Frobenius norm.
It is shown that minimizing (4.21) is essentially reduced to a rank one approxi￾mation of the rectangular matrix, R(G). By singular value decomposition of R(G),
U
R(G)V = Δ = diag(δ1,...,δq ), where U ∈ Rn2×n2
and V ∈ Rm2×m2
are
orthogonal matrices, and the singular values are arranged in descending order and
are positive; observe q = min{m2, n2}. The solution to (4.21) is given by
vec(G1) = 0
δ1 U1 and vec(G2) = 0
δ1 V1, (4.22)
where U1 and V1 are the first column of U and V matrices. Application of
this method in various statistical contexts is given in Genton (2007) and in
Guggenberger, Kleibergen and Mavroeidis (2022). Note this approximation is based
on the elements of G matrix, and therefore, we expect it to perform better than
other Kronecker structures suggested earlier. Genton (2007) defines an error index
to measure the error in the approximation as
KG(G1, G2) = 
G − G1 ⊗ G2
F

G
F
=
1q
i=2 δ2

i
q
i=1 δ2
i
. (4.23)
The index is bounded below by zero and above by √1 − 1/q.
4.8.1 Computational Results
In order to study the effectiveness of the Kronecker’s approximation of G in (4.21),
we compute the trace and the determinant of the error covariance matrix, Σ for
ranks r = 1, 2,..., 5. Recall that the reduced-rank estimators of “C” are obtained
from the projection of Γ 1/2
1 CΓ 1/2
2 to the lower dimension. Results we present below
are for the following alternatives; we assume C
 is the full rank GLS estimator of C
under AR(1) structure for the errors, as given in (4.7).
(a) Weight matrix, G, not in Kronecker form
(b) Weight matrices: Γ1 = Σ−1 u ; Γ2 = Σxx
(c) Weight matrices: Γ1 = G1; Γ2 = G2
Note that C
(r) = PC
, where the projection matrix P = Γ 1/2
1 V
rV

rΓ −1/2
1 and
V
(r) is the m × r matrix of eigenvectors corresponding to the first “r” eigenvalues
of Γ 1/2
1 CΓ 2C

Γ 1/2
! . The results under (a) are given in the last section. For (b) and
(c), we also compute results for r = 2.
The G-matrix is of dimension 25×25 and is not reproduced here. Now the results
of the Kronecker’s product: the G matrix is decomposed into G1 ⊗ G2 and they are
given below:4.8 An Alternate Estimator with Kronecker Approximation 133
G1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
6.61 0.20 0.30 −1.42 −0.79
0.20 25.59 −0.05 −0.92 −1.45
0.30 −0.05 0.22 0.53 −0.15
−1.42 −0.92 0.53 2.40 0.06
−0.79 −1.45 −0.15 0.06 0.98
⎤
⎥
⎥
⎥
⎥
⎥
⎦
, G2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
0.16 1.35 0.55 0.88 0.87
1.35 12.37 4.54 7.93 7.98
0.58 4.54 4.82 5.57 3.56
0.88 7.93 5.57 7.60 5.74
0.87 7.98 3.56 5.74 5.33
⎤
⎥
⎥
⎥
⎥
⎥
⎦
.
The matrices G1 and G2 are positive definite. The resulting rank(2) approximation
of C matrix from this decomposition is given below:
C
(2)
KR =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
4.37 −0.62 0.10 −0.55 1.70
1.81 −0.24 0.06 −0.23 0.63
−6.15 0.23 −0.55 0.89 −0.48
2.88 −0.65 −0.09 −0.32 1.81
8.73 −0.43 0.72 −1.24 0.98
⎤
⎥
⎥
⎥
⎥
⎥
⎦
.
The approximation performs well compared to the rank 2 efficient coefficient
matrix. The effectiveness of this method crucially depends on how well G is
approximated by the Kronecker product G1 ⊗ G2. The eigenvalues of G and the
eigenvalues of G1 ⊗ G2 are virtually indistinguishable (see Fig. 4.3); other refers
to the eigenvalues of Σ−1 u ⊗ Σxx . To make the comparisons more meaningful, we
can also compute the AIC criterion for various ranks. It can be shown that weighing
the matrix based on the Kronecker approximation yields uniformly better results.
Further details on this can be found in Velu and Herman (2017).
Fig. 4.3 Eigenvalue dataChapter 5
Multiple Time Series Modeling With
Reduced Ranks
5.1 Introduction and Time Series Models
There has been a growing interest in multiple time series modeling, particularly
through use of vector autoregressive moving average models. The subject has
found an appeal and has applications in various disciplines, including engineering,
physical sciences, business and economics, and the social sciences. In general,
multiple time series analysis is concerned with modeling and estimation of dynamic
relationships among m related time series y1t,...,ymt , based on observations on
these series over T equally spaced time points t = 1,...,T , and also between
these series and potential input or exogenous time series variables x1t,...,xnt ,
observed over or around the same time period. In this chapter, we shall explore the
use of certain reduced-rank modeling techniques for analysis of multiple time series
in practice. We first introduce a general model for multiple time series modeling
but will specialize to multivariate autoregressive (AR) models for more detailed
investigation.
Let Yt = (y1t,...,ymt) be an m × 1 multiple time series vector of response
variables, and let Xt = (x1t,...,xnt) be an n × 1 vector of input or predictor time
series variables. Let t denote an m × 1 white noise vector of errors, independently
distributed over time with E(t) = 0 and Cov(t) =  , a positive-definite matrix.
We consider the multivariate time series model
Yt =
p
∗
s=0
CsXt−s + t, (5.1)
where the Cs are m × n matrices of unknown parameters. In the general setting, the
“input” vectors Xt could include past (lagged) values of the response series Yt . Even
for moderate values of the dimensions m and n, the number of parameters that must
be estimated in model (5.1), when no constraints are imposed on the matrices Cs,
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8_5
135136 5 Multiple Time Series Modeling With Reduced Ranks
can become quite large. Because of the potential complexity of these models, it is
often useful to consider canonical analyses or other dimension reduction procedures
such as reduced-rank regression methods described in earlier chapters. This may
also lead to more efficient models due to the reduction in the number of unknown
parameters.
The multivariate time series model in (5.2) below, which places some structure
on the coefficient matrices Cs in (5.1), has some useful properties and is in the spirit
of reduced-rank regression models. We consider the model
Yt = 
p2
s=0

p1
u=0
AsBuXt−s−u + t, (5.2)
where the As are m × r and the Bu are r × n matrices, respectively, with 1 ≤ r ≤
min(m, n), and we shall assume m ≤ n for convention. Let L denote the lag operator
such that LYt = Yt−1, and define the m × r and r × n matrix polynomial operators,
of degrees p2 and p1, respectively, by
A(L) = 
p2
s=0
AsLs
, B(L) = 
p1
u=0
BuLu.
Model (5.2) can then be written as
Yt = A(L)B(L)Xt + t . (5.3)
Brillinger (1969) considered a generalization of (5.3) where A(L) and B(L) are
two-sided infinite order operators. He gives the interpretation already mentioned in
Section 2.1, namely, that the r ×1 series X∗
t = B(L)Xt represents a reduction of the
n × 1 input series Xt . The reduction might be chosen, in terms of the operator B(L)
and, eventually, the operator A(L), so that the information about Yt contained in the
series Xt is recovered as closely as possible by the factor A(L)X∗
t = A(L)B(L)Xt .
Observe that in the standard reduced-rank regression model considered in Chapter 2,
we have simply A(L) = A and B(L) = B.
The model (5.3) can accommodate many situations that may be useful in practice.
Suppose we partition Xt to explicitly allow for lagged values of the series Yt , as
Xt = (Z
t, Y 
t−1)
. Partitioning the matrices Bu in (5.2) conformably, with B(L) =
[B0(L), B1(L)], allows us to write (5.3) as
Yt − A(L)B1(L)Yt−1 = A(L)B0(L)Zt + t . (5.4)
The quantities Z∗
t = B0(L)Zt have, in certain situations, interpretations as dynamic
indices for the Zt , and consequently, a model such as (5.4) is sometimes referred to
as an index model (e.g., Sims 1981).
We now focus on the class of multivariate autoregressive (AR) models that results
as a special case of (5.2). When m = n and Xt contains only lagged values of Yt so5.1 Introduction and Time Series Models 137
that Xt = Yt−1, model (5.3) becomes
Yt = A(L)B(L)Yt−1 + t, (5.5)
and this model for Yt represents a p∗ = p1 + p2 + 1 order multivariate AR process.
We mention two particular cases of special interest. First, with p2 = 0 and p1 =
p − 1, we have
Yt = A0B(L)Yt−1 + t = A0B0Yt−1 +···+ A0Bp−1Yt−p + t
= A0BY ∗
t−1 + t ≡ CY ∗
t−1 + t, (5.6)
where B = [B0, B1,...,Bp−1], C = A0B, and Y ∗
t−1 = (Y 
t−1,...,Y 
t−p)
. Since
rank(C) = r ≤ m, model (5.6) has the same structure as the multivariate reduced￾rank regression model (2.5). Various aspects of this particular reduced-rank AR
model form were studied by Velu et al. (1986) and are also presented by Lütkepohl
(1993, Chapter 5). Second, with p1 = 0 and p2 = p − 1, we obtain
Yt = A0B0Yt−1 +···+ Ap−1B0Yt−p + t
= A0Y ∗
t−1 +···+ Ap−1Y ∗
t−p + t, (5.7)
where Y ∗
t = B0Yt is an r × 1 vector of linear combinations of Yt , r ≤ m. In
model (5.7), the Y ∗
t may be regarded as a lower-dimensional set of indices for the
Yt , whose p lagged values are sufficient to represent the past behavior of Yt . So this
model may be called an index model. Note that on multiplying (5.7) on the left by
B0, we find that the series Y ∗
t also has the structure of an AR process of order p.
Estimation of parameters and other aspects of model (5.7) have been considered by
Reinsel (1983).
The class of models stated in (5.2) is rather flexible and provides many options for
modeling multivariate time series data. The structure of coefficient matrices in (5.2)
in its various forms is within a reduced-rank framework. There are other forms and
extensions of reduced-rank model for multiple time series, which may not directly
follow from (5.2), that are found to also be of practical importance. For example, the
extended reduced-rank regression model (3.3) considered by Anderson (1951) and
the model (3.23) may have utility in multivariate time series modeling. Specifically,
consider the vector AR(p) model
Yt = 
p
j=1
jYt−j + t . (5.8)
Suppose that for some p1, 1 ≤ p1 < p, the matrix [p1+1,..., p] is of reduced
rank, while [1,..., p1 ] is of full rank. (For instance, a particular case occurs
with p = 2, where 2 is reduced rank but 1 is full rank.) Such models with
this structure fit into the framework of model (3.3), with the correspondences Zt =138 5 Multiple Time Series Modeling With Reduced Ranks
(Y 
t−1,...,Y 
t−p1 ) and Xt = (Y 
t−p1−1,...,Y 
t−p)
. This type of model might be
useful if dimension reduction is possible in representing the influence on Yt of more
distant past values, whereas the recent lagged values must be retained intact. In the
case where both matrices [1,..., p1 ] and [p1+1,..., p] are of reduced ranks,
we have an example of the model (3.23).
A more general reduced-rank setup for vector AR models was proposed by Ahn
and Reinsel (1988). In the vector AR(p) model (5.8), suppose it is assumed that
the matrices j have a particular reduced-rank structure, such that rank(j ) =
rj ≥ rank(j+1) = rj+1, j = 1,...,p − 1. The j then have the decomposition
j = AjBj , where Aj is m × rj and Bj is rj × m, and it is further assumed that
range(Aj ) ⊃ range(Aj+1). These models are referred to as nested reduced-rank
AR models. We will discuss this more general case further in Section 5.3 (also see
Reinsel (1997, Section 6.1) for more details on this topic).
5.2 Reduced-Rank Autoregressive Models
The dynamic relationships among components of the vector time series Yt can
be modeled in various ways. For example, see Brillinger (1969), Parzen (1969),
Akaike (1976), Sims (1981), Jenkins and Alavi (1981), Tiao and Box (1981),
Hannan and Kavalieris (1984), Tiao and Tsay (1989), and Lütkepohl (1993) for
various approaches. Doan et al. (1984) considered Bayesian methods applied to
time-varying coefficients vector autoregressive models, with a particular emphasis
on forecasting. Some considerations to the dimension reduction aspects in modeling
have been given by the authors above, as well as by Anderson (1963), Brillinger
(1981), Box and Tiao (1977), Priestley et al. (1974a,b), and others. In this section,
we consider the vector AR model (5.8) where the coefficient matrices j are
of reduced-rank structure similar to the standard reduced-rank regression model.
Because the vector AR model (5.8) is in the same framework as the multivariate
regression model (1.1), we can readily make use of earlier results from Chapters 1
and 2. Therefore, we will avoid unnecessary repetition of details and shall be brief
in our presentation.
5.2.1 Estimation and Inference
We consider the vector autoregressive model

(5.8) of order p, given by Yt = p
j=1 jYt−j + t . Defining (L) = I − 1L −···− pLp, where L denotes
the lag operator, we assume that det{(z)} = 0 for all complex numbers |z| ≤ 1.
The series Yt is thus assumed to be stationary. Set C = [1,..., p] and assume
that rank(C) = r<m, which yields the decomposition C = AB as in (2.4).
From model (5.6), the correspondence between the reduced-rank regression model
in Chapter 2 and this particular reduced-rank AR model follows by setting A = A0,5.2 Reduced-Rank Autoregressive Models 139
B = [B0, B1,...,Bp−1], and Xt = Y ∗
t−1 = (Y 
t−1,...,Y 
t−p) in (2.5). Estimation
of the component matrices A and B follows easily from Theorem 2.2 and subsequent
results presented in Section 2.3.
For the stationary vector AR(p) process {Yt} above, define the covariance matrix
at lag j as (j ) = E(Yt−jY 
t), and set ∗ = [(1)
, . . . , (p)
]
 = E(Y ∗
t−1Y 
t), and
p = E(Y ∗
t−1Y ∗
t−1) as the mp × mp matrix that has (i − j ) in the (i, j )th block.
As in Chapter 2, under the assumption of reduced rank, for any choice of positive￾definite matrix , the population coefficient matrices A and B in the reduced-rank
AR model can be expressed as
A = −1/2 [V1,...,Vr] = −1/2 V = [α1,...,αr], (5.9a)
B = V  1/2 
∗ −1
p = [β1,...,βr]

, (5.9b)
where the Vj are (normalized) eigenvectors of the matrix 1/2
∗−1
p ∗1/2 associ￾ated with the r largest eigenvalues λ2
j . This also implies that A and B in (5.9) satisfy
the normalizations BpB = 2 and A
A = Ir, where 2 = diag(λ2
1,...,λ2
r).
Based on a sample Y1,...,Yt , let p = (1/T )
t Y ∗
t−1Y ∗
t−1 and ∗ =
(1/T )
t Y ∗
t−1Y 
t . Then the (conditional) ML estimates of A and B corresponding
to the population quantities in (5.9) are given by
A
= 1/2  [V
1,..., V
r] = 1/2  V
 = [α1,...,αr], (5.10a)
B
 = V
 −1/2  
∗ −1
p = [β
1,..., β
r]

, (5.10b)
where V
 = [V
1,..., V
r] and V
j is the (normalized) eigenvector of the matrix
−1/2  
∗
−1
p ∗−1/2  corresponding to the j th largest eigenvalueλ2
j , and
 = (1/T )[YY − YY∗
−1(Y∗
−1Y∗
−1)
−1Y∗
−1Y
].
Here, Y = [Y1,...,Yt] and Y∗
−1 = [Y ∗
0 ,...,Y ∗
T −1] are the m×T and mp ×T data
matrices containing the observations on Yt and Y ∗
t−1, respectively, for t = 1,...,T .
It follows that the ML estimators A
 = [α1,...,αr] and B
 = [β
1,..., β
r]

above, with  = −1  taken as fixed, possess the same form of asymptotic
distribution as given in Theorem 2.4 for the multivariate reduced-rank regression
model. The proof of this result is essentially the same as given for Theorem 2.4,
and so to avoid repetition, we omit any further details. These asymptotic results
are useful in the calculation of approximate standard errors for the ML estimates
in (5.10) and will be used for the numerical example on the analysis of the U.S.
hog data that will be presented later in the chapter. It also follows that the same
asymptotic theory holds for the LR testing for the rank of the coefficient matrix C
in the reduced-rank autoregressive model as was presented in Section 2.6 for the
reduced-rank regression model (e.g., Kohn 1979). That is, under the null hypothesis
H0 : rank(C) ≤ r, the LR test statistic140 5 Multiple Time Series Modeling With Reduced Ranks
M = [T − mp + (mp − m − 1)/2] m
j=r+1
log(1 +λ2
j )
has an asymptotic χ2
(m−r)(mp−r) distribution.
5.2.2 Relationship to Canonical Analysis of Box and Tiao
In Section 2.4, the connection between reduced-rank regression and canonical
correlation analysis was demonstrated. In the multiple time series context, Box and
Tiao (1977) considered a canonical analysis of a vector time series from a prediction
point of view. Consider the m-variate autoregressive process {Yt} defined by (5.8),
with Cov(Yt) = (0) and Cov(t) =  , both positive definite. For an arbitrary
linear combination of Yt , ωt = 
Yt , we have
ωt = 
Yt = 
Y
t + 
t ≡ ωt + et,
where Y
t = 1Yt−1 +···+ pYt−p, ωt = 
Y
t , and et = 
t . Thus, we find that
the variance of ωt can be decomposed into the sum of a “predictor” variance and an
“error” variance as σ2
ω = σ2
ω +σ2
e . A quantity that measures the predictability of the
linear combination ωt , based on the past values of the vector series {Yt}, is the ratio
ρ2 = Var(ωt)
Var(ωt) = σ2
ω
σ2
ω
=  ∗(0) 
 (0)  , (5.11)
where ∗(0) = Cov(Y
t) = p
j=1 j(j ) = C∗. It follows that, for maxi￾mum predictability, the quantity ρ2 must be the largest eigenvalue of the matrix
(0)−1∗(0) and  is the corresponding eigenvector. Similarly, the eigenvector
corresponding to the smallest eigenvalue of (0)−1∗(0) gives the least predictable
linear combination of Yt . In general, there will be m eigenvalues 1 ≥ ρ2
1 ≥ ρ2
2 ≥
··· ≥ ρ2
m ≥ 0, and let 1,...,m denote the corresponding eigenvectors. Then
define the m × m matrix L that has 
i as its ith row, and consider the transformed
process Wt = L
Yt = (ω1t,...,ωmt)
. It follows that the transformation Wt =
L
Yt produces m new component series that are ordered from “most predictable”
to “least predictable.” The developments of this canonical analysis approach are
due to Box and Tiao (1977), and we will now indicate that they also correspond
to the classical canonical correlation analysis originally developed by Hotelling
(1935, 1936). Therefore, we see that this approach is directly related to the reduced￾rank AR modeling and the ML estimates A
 and B
 given in (5.10) because of
the previously established connection of reduced-rank regression with classical
canonical correlation analysis.5.3 An Extended Reduced-Rank Autoregressive Model 141
For the stationary vector AR(p) model (5.8), we know that
C = [1,..., p] = 
∗−1
p
and that ∗(0) = Cov(Y
t) = p
j=1 j(j ) = C∗ = 
∗−1
p ∗ (e.g., see Reinsel
1997, Sections 2.2 and 6.4). From (5.11), it follows that the choice of  that gives
the maximum predictability is then the eigenvector 1 satisfying
(0)
−1
∗−1
p ∗1 = ρ2
1 1, (5.12)
with ρ2
1 being the largest eigenvalue of (0)−1
∗−1
p ∗. Note that this is the
same as the largest eigenvalue of the matrix (0)−1/2
∗−1
p ∗(0)−1/2, which
is of the same form as the matrix in (2.28) of Section 2.4. Thus we see that
this is equivalent to a canonical correlation analysis between the vectors Yt and
Xt ≡ Y ∗
t−1 = (Y 
t−1,...,Y 
t−p)
. If h
iYt and g
iXt denote the ith canonical variables,
then it is easy to see that hi = i and gi = C
i, for i = 1,...,m. The ρi are the
canonical correlations between Yt and Xt . Hence, the canonical analysis based on
the predictability measure of (5.11) is the same as the classical canonical correlation
analysis for the stationary series.
It is to be noted that the number of nonzero eigenvalues associated with (5.12)
is the same as the rank of ∗, which in turn is equal to rank(C). So the preceding
canonical analysis techniques for vector AR processes considered by Box and Tiao
are related to the reduced-rank regression model (2.5). Specifically, from the results
in Section 2.4 and from (5.9), i = −1/2  Vi, i = 1,...,m, where Vi denotes
the (normalized) eigenvector in (5.9) for the choice of  = −1  and i is the
eigenvector corresponding to the ith largest root ρ2
i in (5.12), normalized such that

i(0)i = (1 − ρ2
i )−1. Hence, under a reduced-rank structure for the coefficient
matrix C in the vector AR(p) model, the component matrices A and B in (5.9)
can equivalently be expressed in terms of L(r) = [1,...,r] as A = L(r) and
B = L
(r)
∗−1
p . With m − r zero eigenvalues in (5.12), we see that 
iC = 0
for i = r + 1,...,m. Thus, there are m − r corresponding linear combinations
ωit = 
iYt ≡ 
it that form white noise series. It is useful to identify such
white noise series in the multiple time series modeling, because that will simplify
the structure of the model, and this can be accomplished through the canonical
correlation analysis and associated reduced-rank regression modeling.
5.3 An Extended Reduced-Rank Autoregressive Model
We discuss briefly the use of the extended reduced-rank regression model (3.3)
considered by Anderson (1951), as presented in Section 3.1, within the time series
framework of the vector autoregressive model (5.8). As noted in the discussion
following (5.8), for the vector AR(p) model in (5.8), if for some p1, 1 ≤ p1 < p,142 5 Multiple Time Series Modeling With Reduced Ranks
the matrix [p1+1,..., p] of AR coefficients for more distant lags is of reduced
rank r1 < m, but the matrix [1,..., p1 ] of AR coefficients for the recent past
is of full rank, then the resulting time series model is in the form of (3.3). Writing
C = [p1+1,..., p] and D = [1,..., p1 ], with Zt = (Y 
t−1,...,Y 
t−p1 ) and
Xt = (Y 
t−p1−1,...,Y 
t−p)
, the AR(p) model can be restated as
Yt = CXt + DZt + t, t = 1,...,T, (5.13)
where rank(C) = r1 < m. Hence, we have C = AB, where A is an m × r1 matrix
and B is an r1 × m(p − p1) matrix.
The ML estimation of the component parameter matrices A and B, and D
in (5.13) follows from the results developed in Section 3.1. Likewise, the cor￾responding population results, which we discuss first, follow from Theorem 3.1.
As in Section 5.2, denote ∗(p) = [(1)
, . . . , (p)
]
 = E(Y ∗
t−1Y 
t) and p =
E(Y ∗
t−1Y ∗
t−1). Then note that yz ≡ E(YtZ
t) = ∗(p1)
, zz ≡ E(ZtZ
t) =
p1 , xx ≡ E(XtX
t) = p−p1 , and denote yx ≡ E(YtX
t) = [(p1 +
1)
, . . . , (p)
] ≡ ∗∗(p − p1) and xz ≡ E(XtZ
t) = (p−p1,p1). From the
results of Theorem 3.1, and relation (3.5) in particular, it follows that the population
coefficient matrices in the extended reduced-rank vector AR model (5.13) can be
written as
A(r1) = 1/2  [V1,...,Vr1 ] = 1/2  V(r1), (5.14a)
B(r1) = V 
(r1)−1/2  yx.z−1
xx.z, (5.14b)
and D = (yz−A(r1)
B(r1)
xz)−1 zz = (∗(p1)
−A(r1)
B(r1)
(p−p1,p1))−1
p1 , where
the Vj are (normalized) eigenvectors of −1/2  yx.z−1 xx.zxy.z−1/2  associated
with the r1 largest eigenvalues λ2
1j , and
yx.z = yx − yz−1
zz zx = ∗∗(p − p1)
 − ∗(p1)

−1
p1 
(p−p1,p1),
xx.z = xx − xz−1
zz zx = p−p1 − (p−p1,p1)−1
p1 
(p−p1,p1)
are partial covariance matrices eliminating the linear effects of Zt from both Yt
and Xt . This implies that A(r1) and B(r1) in (5.14) satisfy the normalizations
B(r1)
xx.zB(r1)
= 2
1 and A(r1)
−1  A(r1) = Ir1 , where 2
1 = diag(λ2
11,...,λ2
1r1
).
Also recall that the rank of C in (5.13) is equal to the number of nonzero partial
canonical correlations [ρ1j = λ1j /(1 + λ2
1j )1/2] between Yt and the more distant
lagged values Xt = (Y 
t−p1−1,...,Y 
t−p)
, eliminating the influence of the closer
lagged values Zt = (Y 
t−1,...,Y 
t−p1 )
.
For the sample Y1,...,Yt , let
∗(p1)
 = 1
T

t
YtZ
t, p1 = 1
T

t
ZtZ
t, ∗∗(p − p1)
 = 1
T

t
YtX
t,5.3 An Extended Reduced-Rank Autoregressive Model 143
and so on, denote the sample covariance matrices corresponding to the population
quantities above, with yx.z = ∗∗(p − p1) − ∗(p1)−1
p1

(p−p1,p1) and
xx.z = p−p1 −(p−p1,p1)
−1
p1

(p−p1,p1) the associated sample partial covariance
matrices. Then it follows directly from the derivation in Section 3.1, leading
to (3.11), that the (conditional) ML estimates are
A
(r1) = 1/2  [V
1,..., V
r1 ] = 1/2  V
(r1), (5.15a)
B
(r1) = V

(r1) −1/2  yx.z −1
xx.z, (5.15b)
and D = (yz−A
(r1)
B
(r1)
xz)−1 zz = (∗(p1)
−A
(r1)
B
(r1)(p−p1,p1))−1
p1 , where
the V
j are (normalized) eigenvectors of −1/2  yx.z−1 xx.zxy.z−1/2  associated
with the r1 largest eigenvalues λ2
1j . As with the ML estimators for the regression
model (3.3) in Section 3.1, the asymptotic distribution theory for the ML estimators
A
(r1)
, B
(r1)
, and D in (5.15) above follows easily from the results in Theorem 2.4.
It also follows that LR testing for the appropriate rank of the coefficient matrix
C in the extended reduced-rank AR model (5.13) can be carried out in the same
manner as described in Section 3.1. In particular, for testing the null hypothesis
H0 : rank(C) ≤ r1, we can use the LR test statistic
M = [T − mp + (m(p − p1) − m − 1)/2] m
j=r1+1
log(1 +λ2
1j )
= −[T − mp + (m(p − p1) − m − 1)/2] m
j=r1+1
log(1 − ρ2
1j ),
which has an asymptotic χ2
(m−r1)(m(p−p1)−r1) null distribution, where ρ2
1j =
λ2
1j /(1 +λ2
1j ) are the squared sample partial canonical correlations between Yt and
Xt eliminating Zt .
To conclude, we observe that the extended reduced-rank vector autoregressive
model (5.13) has the feature that m − r1 linear combinations of Yt exist such that
their dependence on the past can be represented in terms of only the recent past
lags Zt = (Y 
t−1,...,Y 
t−p1 )
. This type of feature may be useful in modeling and
interpreting time series data with several component variables. The specification of
the appropriate reduced rank r1 and the corresponding ML estimation of the model
parameters can be performed using the sample partial canonical correlation analysis
and associated methods as highlighted above and as described in more detail in
Section 3.1.144 5 Multiple Time Series Modeling With Reduced Ranks
5.4 Nested Reduced-Rank Autoregressive Models
We now briefly discuss analysis of vector AR(p) models that have simplifying
nested reduced-rank structures in their coefficient matrices j . Specifically, we
consider the vector AR(p) model in (5.8), in which it is assumed that the
matrices j have a particular reduced-rank structure, such that rank(j ) = rj ≥
rank(j+1) = rj+1, j = 1, 2,...,p − 1. Then, the j can be represented in the
form j = AjBj , where Aj is m×rj and Bj is rj ×m, and we further assume that
range(Aj ) ⊃ range(Aj+1). Thus, we can write (5.8) as
Yt = 
p
j=1
AjBjYt−j + t . (5.16)
We refer to such models as nested reduced-rank AR models, and these models,
which generalize the earlier work by Velu et al. (1986), were studied by Ahn and
Reinsel (1988). These models can result in more parsimonious parameterization,
provide more details about structure possibly with simplifying features, and offer
more interesting and useful interpretations concerning the interrelations among the
m time series.
5.4.1 Specification of Ranks
To obtain information on the ranks of the matrices j in the nested reduced￾rank model, a partial canonical correlation analysis approach can be used. One
fundamental consequence of the model above is that there will exist at least m − rj
zero partial canonical correlations between Yt and Yt−j , given Yt−1,...,Yt−j+1.
This follows because we can find a (m−rj )×m matrix F
j , whose rows are linearly
independent, such that F
jAj = 0 and, hence, F
jAi = 0 for all i ≥ j because of the
nested structure of the Aj . Therefore,
F
j (Yt −
j−1
i=1
iYt−i) = F
j (Yt −
p
i=1
iYt−i) ≡ F
j t (5.17)
is independent of (Y 
t−1,...,Y 
t−j ) and consists of m − rj linear combinations of
(Y 
t ,...,Y 
t−j+1)
. Thus, m − rj zero partial canonical correlations between Yt and
Yt−j occur. Hence, performing a (partial) canonical correlation analysis for various
values of j = 1, 2,... will identify the rank structure of the nested reduced-rank
model, as well as the overall order p of the AR model.5.4 Nested Reduced-Rank Autoregressive Models 145
The sample test statistic that can be used to (tentatively) specify the ranks is
M(j, s) = −(T − j − jm − 1 − 1/2) m
i=(m−s)+1
log(1 − ρ2
i (j )) (5.18)
for s = 1, 2,...,m, where 1 ≥ ρ1(j ) ≥ ··· ≥ ρm(j ) ≥ 0 are the sample partial
canonical correlations between Yt and Yt−j , given Yt−1,...,Yt−j+1. Under the null
hypothesis that rank(j ) ≤ m−s within the nested reduced-rank model framework,
the statistic M(j, s) is asymptotically distributed as chi-squared with s2 degrees of
freedom (e.g., Tiao and Tsay 1989). Hence, if the value of the test statistic is not
“unusually large,” we would not reject the null hypothesis and might conclude that
j has reduced rank equal to the smallest value rj (≡ m − sj ) for which the test
does not reject the null hypothesis. Note, in particular, that when s = m the statistic
in (5.18) is the same as the LR test statistic for testing H0 : j = 0 in an AR(j )
model, since log(Uj ) = log{|Sj |/|Sj−1|} = m
i=1 log(1−ρ2
i (j )), where Sj denotes
the residual sum of squares matrix after LS estimation of the AR model of order j .
5.4.2 A Canonical Form
of the matrices j = AjBj that one can construct a nonsingular m × m matrix P
such that the transformed series Zt = P Yt will have the following simplified model
structure. The model for Zt is AR(p),
Zt = P Yt = 
p
i=1
P (AiBi)P −1P Yt−i + P t
= 
p
i=1
A∗
i B∗
i Zt−i + et ≡ 
p
i=1
∗
i Zt−i + et, (5.19)
where A∗
i = P Ai, B∗
i = BiP −1, ∗
i = A∗
i B∗
i , and et = P t . The rows of P
will consist of a basis of the rows of the various matrices F
j indicated in (5.17).
Thus, the matrix P can be chosen so that its last m − rj rows are orthogonal to the
columns of Aj , and, hence, A∗
i = [A∗
i1, 0
]
 has its last m − ri rows equal to zero.
This implies that ∗
i = A∗
i B∗
i also has its last m − ri rows being zero. Hence, the
“canonical form” of the model has
Zt = 
p
i=1

∗
i1
0

Zt−i + et,
with the number m − ri of zero rows for ∗
i = [∗
i1, 0
]
 increasing as i increases.
Therefore, the components zjt of Zt = (z1t, z2t,...,zmt) are represented in an146 5 Multiple Time Series Modeling With Reduced Ranks
AR(p) model by fewer and fewer past lagged variables Zt−i as j increases from 1
to m.
For the nested reduced-rank AR model with parameters j = AjBj , for any
rj × rj nonsingular matrix Q, we can write j = AjQ−1QBj = A¯jB¯j , where
A¯j = AjQ−1 and B¯j = QBj . Therefore, some normalization conditions on
the Aj and Bj are required to ensure a unique set of parameters. Assuming the
components of Yt have been arranged so that the upper rj × rj block matrix
of each Aj is full rank, this parameterization can be obtained by expressing the
j as j = AjBj = A1DjBj , where A1 is m × r1 and has certain elements
“normalized” to fixed values of ones and zeros, and Dj = [Irj , 0]
 is r1 × rj .
More specifically, the matrix A1 can always be formed to be lower triangular with
ones on the main diagonal, and when augmented with the last m − r1 columns of
the identity matrix, the inverse of the resulting matrix A can form the necessary
matrix P of the canonical transformation in (5.19). (See Ahn and Reinsel 1988
for further details concerning the normalization.) Thus, with these conventions,
we can write the model as Yt = A1
p
i=1 DiBiYt−i + t . In addition, if we set
#
0 = A−1 = P and define the m × m coefficient matrices #
i = [B
i, 0]

,
i = 1,...,p, the nested reduced-rank model can be written in an equivalent form
as #
0Yt = p
i=1 #
i Yt−i + #
0t , which is sometimes referred to as an “echelon
canonical form” for the vector AR(p) model (e.g., see Reinsel 1997, Sec. 3.1).
For example, consider the special case where the ranks of all i are equal,
r1 = ··· = rp ≡ r, and notice that this special nested model then reduces to
the reduced-rank AR model discussed in Section 5.2 where rank[1,..., p] = r.
The normalization for the matrix A1 discussed above in this case becomes A1 =
[Ir, A
10]

, where A10 is an (m − r) × r matrix. The augmented matrix A and the
matrix P = A−1 in the canonical form (5.19) can be taken as
A =
 Ir 0
A10 Im−r

and P = A−1 =
 Ir 0
−A10 Im−r

,
where the last m − r rows of P consist of the (linearly independent) vectors F
j
from (5.17) such that F
ji = 0 for all i = 1,...,p. Thus, for this special model,
the last m−r components zjt = F
jYt in the transformed canonical form (5.19) will
be white noise series.
5.4.3 Maximum Likelihood Estimation
We let β0 denote the a × 1 vector of unknown parameters in the matrix A1, where
it follows that a = p
j=1(m − rj )(rj − rj+1). Also, let βj = vec(B
j ), a mrj × 1
vector, for j = 1,...,p, and set θ = (β
0, β
1,...,β
p)
. The unknown parameters
θ are estimated by (conditional) maximum likelihood, using a Newton–Raphson
iterative procedure. The (conditional) log-likelihood for T observations Y1,...,Yt
(given Y0,...,Y1−p) is5.4 Nested Reduced-Rank Autoregressive Models 147
l(θ ,  ) = −(T /2)log | | − (1/2)

T
t=1

t−1
 t .
It can be shown that an approximate Newton–Raphson iterative procedure to obtain
the ML estimate of θ is given by
θ(i+1) = θ(i) − {∂2l/∂θ∂θ
}
−1
θ(i){∂l/∂θ}θ(i)
= θ(i) +
#
M


T
t=1
Ut−1
 U
t

M
$−1
θ(i) #
M

T
t=1
Ut−1
 t
$
θ(i)
,
where θ(i) denotes the estimate at the ith iteration, ∂
t /∂θ = M
Ut , Ut =
[Im ⊗ Y 
t−1,...,Im ⊗ Y 
t−p]

, and M is a specific matrix of dimension (a +
mp
j=1 rj ) × m2p whose elements are given functions of the parameters θ (but
not the observations).
It can be shown that the ML estimate θ has an asymptotic distribution such that
T 1/2(θ − θ ) converges in distribution to N (0, V −1) as T → ∞, where V =
M
E(Ut−1  U
t)M ≡ M
GM and G = E(Ut−1  U
t) has (i, j )th block element
equal to −1  ⊗ (i − j ). Thus for large T , the ML estimate θ is approximately
distributed as N (θ , T −1V
−1 t ), where Vt = M
(T −1 T
t=1 Ut−1  U
t)M. In addition,
the asymptotic distribution of the (reduced-rank) ML estimates j = A
jB
j of the
AR parameters j follows directly from the result above. It is determined from the
relations
j − j = A
1DjB
j − A1DjBj
= (A
1 − A1)DjBj + A1Dj (B
j − Bj ) + (A
1 − A1)Dj (B
j − Bj )
= (A
1 − A1)DjBj + A1Dj (B
j − Bj ) + Op(T −1).
From this, we find that
φ
− φ = M(θ − θ ) + Op(T −1),
where φ
 = vec{
1,..., 
p}, φ = vec{
1,..., 
p}, and M = ∂φ/∂θ
, in fact, so
that
T 1/2(φ
− φ) d
−→ N (0,MV −1M
) as T → ∞.
In particular, it is noted that the collection of resulting reduced-rank estimates j =
A
jB
j of the j has a smaller asymptotic covariance matrix than the corresponding
full-rank least squares estimates, since
MV −1M = M{M
GM}
−1M < G−1.148 5 Multiple Time Series Modeling With Reduced Ranks
Note that the preceding developments and estimation results have similarities to the
results presented in Section 3.5 concerning the estimation of multivariate reduced￾rank regression models with two sets of regressors, except that the reduced-rank
normalizations imposed on the vector of autoregressive parameters φ are now in
the more explicit form j = A1DjBj with constraints that certain elements of
A1 be fixed at zero or one. Also, it follows that LR tests of various hypotheses
concerning the ranks and other restrictions on the matrices j can be performed
in the usual manner based on the ratio of determinants of the residual covariance
matrix estimators,  = (1/T )T
t=1t
t , in the “full” and “restricted” models,
respectively. More details concerning the nested reduced-rank AR models are given
by Ahn and Reinsel (1988) and Reinsel (1997, Section 6.1).
5.5 Numerical Example: U.S. Hog Data
To illustrate the reduced-rank modeling of the previous sections, we consider the
U.S. hog, corn, and wage time series previously analyzed by several authors,
including Quenouille (1968, Chapter 8), Box and Tiao (1977), Reinsel (1983), Velu
et al. (1986), and Tiao and Tsay (1983, 1989). Annual observations from 1867 to
1948 on five variables, farm wage rate (FW), hog supply (HS), hog price (HP),
corn price (CP), and corn supply (CS), will be analyzed. These measurements were
logarithmically transformed and linearly coded by Quenouille, and, following Box
and Tiao, the wage rate and hog price time series were shifted backward by one
period. We consider this modified set of series as our basic set for analysis. The
wage rate was originally included in the set as a feasible trend variable to reflect the
general economic level. The other series are almost exclusively related to the hog
industry. After corrections for means and recording the data in hundreds, we denote
the basic 5-variate time series as Yt . Time series plots of the five series, unadjusted
for means, are shown in Fig. 5.1.
The series Yt was first subjected to a multivariate stepwise autoregressive analysis
to identify an appropriate order of an AR model. The LR chi-squared statistic
(Tiao and Box 1981; Reinsel 1997, Section 4.3) for testing the significance of the
coefficient matrix at lag j (i.e., testing H0 : j = 0 in the AR model of order
j ) is given by (5.18) with s = m. For the first three lags (orders) j = 1, 2, 3,
the LR statistic values are obtained as 448.51, 98.13, and 31.93, respectively. Note
that the upper 5% critical value for the χ2
25 distribution is 37.7. Thus the LR test
results suggest that an AR model of order 2 may be acceptable and that it fits much
better than, for example, an AR model of order 1. Using more elaborate initial
model specification techniques and criterion, Tiao and Tsay (1989) suggested an
ARMA(2,1) model for the series.
The AR(2) model Yt = 1Yt−1 + 2Yt−2 + t , when fitted without imposing
any rank or other constraints, gives (conditional) ML or LS estimates5.5 Numerical Example: U.S. Hog Data 149
Fig. 5.1 The annual U.S. hog industry data, 1867–1947150 5 Multiple Time Series Modeling With Reduced Ranks
1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
1.49∗ −0.09 −0.03 0.01 0.12
0.12 0.43∗ −0.08 0.11∗ 0.29∗
1.21∗ −0.42 0.34∗ 0.20 0.02
1.05∗ −0.83 −0.27 0.75∗ 0.49
0.00 0.28 0.12 −0.02 0.23
⎤
⎥
⎥
⎥
⎥
⎥
⎦
2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
−0.35∗ −0.17 −0.10 −0.02 0.02
−0.16 0.04 0.28∗ −0.22∗ −0.16∗
−0.47 0.14 −0.53∗ 0.31∗ 0.39∗
−0.25 −0.12 −0.42∗ 0.07 0.07
−0.39 0.34 0.37∗ −0.11 0.24
⎤
⎥
⎥
⎥
⎥
⎥
⎦
with residual ML covariance matrix estimate  such that | | = 0.1444 × 10−3.
The asterisks indicate that the individual coefficient estimates have t-ratios greater
than two in absolute value.
Two interesting points emerge from these estimation results. First, the wage
rate (y1t) is clearly nonstationary, and it is not influenced by any other variables;
hence, it may be regarded as exogenous. This is consistent with our initial remark
that the wage rate was included to reflect the general economic level. Second, the
corn supply (y5t) is influenced mainly by hog price (y3t) in the previous year.
Historically, the ratios between total feed consumption and hog production have not
changed much over the years. This is true regardless of improvements in feeding
efficiency; much of the corn produced is intended for hog feed, and so the price of
hogs influences the corn supply.
One can attempt to simplify the estimated full-rank AR(2) model by sequentially
setting to zero those coefficients whose estimates are quite small compared to their
standard errors and reestimating the remaining coefficients. However, in general,
this can be time-consuming and may not lead to any simplifications or improved
interpretation in the structure of the model for the data, while such a simplification in
structure may be uncovered by a reduced-rank analysis. Hence, we now investigate
the possibility of reduced-rank autoregression models for these data. As discussed
previously, the relationship between canonical correlation analysis and reduced-rank
regression allows one to check on the rank of the regression coefficient matrix by
testing if certain canonical correlations are zero.
The squared sample canonical correlations ρ2
i between Yt and Xt =
(Y 
t−1, Y 
t−2)
, which are the same as the (estimated) predictability ratios from (5.11),
are equal to 0.981, 0.866, 0.694, 0.193, and 0.084. We test the null hypothesis
H0 : rank(C) = r against the alternative H1 : rank(C) = 5 for r = 1, 2, 3, 4,
where C = [1, 2], using the LR test statistic M given in Section 5.2. The
LR test statistic values corresponding to the four null hypotheses are obtained
as M = 244.91, 104.07, 21.16, and 6.14, with 36, 24, 14, and 6 degrees of
freedom, and the associated tail probabilities are found to be 0.00, 0.00, 0.10, and
0.41, respectively. That is, rank(C) = 3 is consistent with the data. (Strictly, the
asymptotic chi-squared distribution theory for the LR statistics in this setting is5.5 Numerical Example: U.S. Hog Data 151
developed for the stationary vector AR case. There are indications that the series
Yt in this example is at least “partially nonstationary”; nevertheless, it is felt that
the use of the LR test procedures here is informative for rank determination.) The
(conditional) ML estimates of the component matrices obtained from (5.10) for the
AR(2) model of reduced rank r = 3 are
A
=
⎡
⎢
⎢
⎢
⎢
⎢
⎣
0.33 0.06 0.04
0.09 −0.15 −0.08
0.32 0.15 0.47
0.20 0.17 0.42
0.12 −0.28 −0.02
⎤
⎥
⎥
⎥
⎥
⎥
⎦
B
 =
⎡
⎣
4.07 0.60 0.13 −0.15 0.35 −1.13 −0.33 −0.03 −0.19 −0.09
1.88 −1.54 0.21 −0.98 −1.58 0.73 −1.03 −1.43 0.84 0.13
−0.60 −1.14 0.32 1.05 0.48 −0.47 0.76 −0.63 0.46 0.83
⎤
⎦
with ML error covariance matrix estimate  such that | | = 0.1954 × 10−3.
The standard errors of the ML estimates above can be computed using the
previously mentioned asymptotic results similar to those in Theorem 2.4. All the
elements in A
, except for the coefficient in the (5,3) position, are found to be
significant when “significance” is taken to be greater than twice the asymptotic
standard error. The estimated linear combinations
Y
∗
t−1 = B(Y  
t−1, Y 
t−2)

can be regarded as indices constructed from the past data values. For illustrative
purposes, we present these linear combinations with significant terms only, together
with standard errors. These are
y∗
1,t−1 = 4.07(±0.42)y1,t−1 − 1.13(±0.43)y1,t−2,
y∗
2,t−1 = 1.88(±0.42)y1,t−1 − 1.54(±0.51)y2,t−1 − 0.98(±0.25)y4,t−1
−1.58(±0.26)y5,t−1 − 1.03(±0.44)y2,t−2
−1.43(±0.22)y3,t−2 + 0.84(±0.23)y4,t−2,
y∗
3,t−1 = −1.14(±0.50)y2,t−1 + 1.05(±0.22)y4,t−1 − 0.63(±0.21)y3,t−2
+0.46(±0.22)y4,t−2 + 0.83(±0.28)y5,t−2.
The first linear combination mainly consists of farm wages at lag 1 and lag 2,
whereas the third linear combination does not involve farm wages.
The total number of functionally independent parameter estimates in A
and B
 is
only 36, because of the normalization conditions, compared with 50 in the full-rank
model. One could also consider a further simplification of the reduced-rank model
by use of the procedure that sets to zero those coefficients whose estimates are quite152 5 Multiple Time Series Modeling With Reduced Ranks
small compared to their standard errors and reestimates the remaining coefficients.
However, in this case, the explicit eigenvalue–eigenvector solution given by (5.10)
would no longer be applicable, and nonlinear iterative estimation procedures must
be employed, such as those described by Reinsel (1997, Sections 5.1 and 6.1). Such
a procedure was, in fact, applied to the above AR(2) reduced-rank model example
and resulted in a quite adequate model containing only 22 parameters, but we omit
further details.
In view of the discussion in Section 5.2, the above reduced-rank regression
analysis is easy to relate to the canonical analysis. As noted, the ML estimates A

and B
, specifically the eigenvectors V
i in (5.10), are related to the eigenvectors i
that are obtained using the sample version of (5.12) by
i = −1/2  V
i, i = 1, . . . , m.
Since i = −1  αi, the asymptotic distribution theory for the i follows easily from
that for A
, for i = 1,...,r, thus providing a useful inferential tool for the canonical
analysis of Box and Tiao (1977). The i can be used for the canonical analysis that
transforms the series Yt into 5 new series
Wt = L

Yt,
where L
 = [1,...,m], and the accompanying standard errors of the i can be
helpful for interpreting the extent of contribution of each original component yit in
the canonical variates. The matrix L
 is obtained as
L
 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
2.561 1.548 0.610 −0.627 −0.388
−2.286 3.747 0.180 0.805 1.627
2.219 0.372 −1.235 −1.176 −1.104
−0.480 0.813 1.366 −1.326 −0.646
0.869 −2.978 −1.115 0.634 1.647
⎤
⎥
⎥
⎥
⎥
⎥
⎦
,
with nearly all of the estimated coefficients being “significantly” nonzero. The
components of Wt are contemporaneously uncorrelated and are ordered from most
predictable to least predictable. Also recall from Section 5.2 that the components
ωit = 
iYt are normalized such that they have sample variance equal to (1 − ρ2
i )−1
(i.e., 
i
(0)i = (1 − ρ2
i )−1). The transformed series Wt are plotted in Fig. 5.2.
This figure indicates that the first two series are nearly nonstationary, and the last
two fluctuate randomly around constants and hence (on further examination) tend
to behave like white noise series, consistent with the reduced rank 3 structure.
The first linear combination ω1t = 
1Yt associated with the largest canonical
correlation, that is, the largest predictability ratio, appears to have contributions from
all the series but is dominated by farm wages. The nearly random (white noise)
components associated with the two smallest canonical correlations may be of
interest due to their relative stability over time. For instance, the sample variance of5.5 Numerical Example: U.S. Hog Data 153
Fig. 5.2 Transformed series in canonical analysis for annual U.S. hog industry data, 1867–1947154 5 Multiple Time Series Modeling With Reduced Ranks
the final white noise canonical component, ω5t /3.739, when normalized so that the
vector of coefficients of the yit has unit norm, is equal to only 0.078. It is instructive
to observe that this variance is very small compared to the average variance (equal
to 3.58) of the original component series yit , i = 1,..., 5.
To arrive at some additional meaningful interpretations involving the last two
(white noise) components, we look at certain linear combinations of ω4t and ω5t .
By fixing the coefficients of HSt (y2t) and HPt (y3t) to be equal to 1, we obtain the
linear combination
0.589ω4t − 0.175ω5t = −0.435y1t + y2t + y3t − 0.892y4t − 0.668y5t .
On taking antilogs, this implies, approximately, that the series can be related to a
Cobb–Douglas production function,
It = HPtHSt /{(CPtCSt)
0.78(FWt)
0.43},
and is a white noise series. (See Box and Tiao 1977 for similar interpretations.) The
sample variance of the (essentially) white noise index above, ω∗
t = (0.589ω4t −
0.175ω5t)/1.853, normalized in a similar way as ω5t /3.739, is 0.135. Hence, we
have obtained a quite stable contemporaneous linear relationship among the original
5 variables.
A nested reduced-rank AR model (5.16) can also be considered for these
data. The testing procedure (5.18) for investigation of the rank structure of a
possible nested reduced-rank model was used. The results of a partial canonical
correlation analysis of the data, in terms of the (squared) sample partial canonical
correlations ρ2
i (j ) for lags j = 1, 2, 3, and the associated test statistic values
M(j, s) from (5.18) are displayed in Table 5.1. From the statistics in this table,
we reconfirm that an AR model of order 2 may be appropriate, but we also find that
the hypothesis that rank(2) = 3 is clearly not rejected and that the hypothesis that
rank(1) = 4 is clearly not rejected. Hence, these results suggest an AR(2) model
with a nested reduced-rank structure such that rank(1) = 4 and rank(2) = 3,
that is, Yt = A1(B1Yt−1 + D2B2Yt−2) + t with A1 a 5 × 4 matrix, B1 a 4 × 5
matrix, B2 a 3 × 5 matrix, and D2 = [I3, 0]

. Subsequent estimation of this model,
by (conditional) ML methods, as outlined in a subsection of Section 5.3, resulted in
a fitted model with the following estimates:
A
1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
1.000 0.000 0.000 0.000
0.000 1.000 0.000 0.000
0.000 0.000 1.000 0.000
0.437 0.103 0.647 1.000
−0.701 1.765 0.430 −0.394
⎤
⎥
⎥
⎥
⎥
⎥
⎦5.5 Numerical Example: U.S. Hog Data 155
Table 5.1 Summary of partial canonical correlation analysis between Yt and Yt−j and associated
chi-squared test statistics M(j, s) from (5.18) for the U.S. hog data. (Test statistics are used to
determine the appropriate AR order and ranks r = m − s of coefficient matrices j )
M(j, s)
j Partial correlations s = 5 s = 4 s = 3 s = 2 s = 1
1 0.987, 0.876, 0.725, 0.413, 0.153 448.5 177.5 70.4 15.5 1.75
2 0.741, 0.561, 0.473, 0.140, 0.068 98.1 44.3 18.8 1.7 0.31
3 0.471, 0.386, 0.310, 0.073, 0.023 31.9 16.5 6.6 0.4 0.03
Critical value = χ2
s2 (.05) 37.7 26.3 16.9 9.5 3.84
B
1 =
⎡
⎢
⎢
⎣
1.482 −0.032 −0.022 0.012 0.116
0.152 0.332 −0.098 0.096 0.278
1.195 −0.449 0.337 0.209 0.020
−0.152 −0.919 −0.551 0.526 0.331
⎤
⎥
⎥
⎦
B
2 =
⎡
⎣
−0.362 −0.160 −0.097 −0.020 −0.007
−0.180 0.054 0.281 −0.200 −0.106
−0.442 0.120 −0.542 0.307 0.401
⎤
⎦
with ML error covariance matrix estimate  such that | | = 0.1584 × 10−3.
In the results above, the elements in A
1 equal to 0 and 1 are fixed for normalization
purposes.
Notice that when A
1 is augmented with the last column of the identity matrix,
then P equal to the inverse of this matrix provides the canonical transformation
Zt = P Yt in (5.19) for this nested reduced-rank model. From this, for example, we
find that the series
z5t = 0.529y1t − 1.805y2t − 0.685y3t + 0.394y4t + y5t
forms a white noise process, while the series
z4t = −0.437y1t − 0.103y2t − 0.647y3t + y4t
has dependence on the past of {Yt} that can be represented by only one lag (an
AR(1) scalar component in the terminology of Tiao and Tsay 1989). We note that the
estimated white noise series z5t under this nested model is almost the same (except
for a scale factor) as the estimated “least predictable” canonical series ω5t = 
5Yt
obtained from the earlier reduced-rank model and associated canonical correlation
analysis. Again, further simplification of the nested reduced-rank model above could
be considered by setting to zero those coefficients whose estimates are quite small
compared to their standard errors and reestimating the remaining coefficients, but
we will not go into any further details here.156 5 Multiple Time Series Modeling With Reduced Ranks
Table 5.2 Summary of various analyses of the U.S. hog data
ML residual variances
Nested
Full-rank Reduced-rank reduced rank Index model Univariate
Variable model model (r = 3) (r1, r2 = 4, 3) (r = 3) model
Wage rate 0.1332 0.1374 0.1336 0.1449 0.1456
Hog supply 0.0544 0.0566 0.0554 0.0710 0.1379
Hog price 0.3665 0.3729 0.3668 0.5632 0.8165
Corn price 0.9448 1.0916 0.9580 1.0776 1.4270
Corn supply 0.4623 0.5130 0.4853 0.4845 0.4594
| |(×10−3) 0.1444 0.1954 0.1584
n∗ = number of
parameters
50 36 42
AIC −7.577 −7.629 −7.687
The empirical results of several analyses of the hog data are summarized in
Table 5.2 in terms of the variances of the individual residual series, as well as the
determinant of the ML residual covariance matrix  and the AIC criterion,
AIC = log(| |) + 2n∗/T,
where n∗ denotes the number of estimated parameters in the model. Included
in Table 5.2 are results for both the full-rank and reduced-rank second-order
autoregressive models discussed earlier, as well as results from the nested reduced￾rank AR(2) model and a second-order index model of rank 3 considered by
Reinsel (1983). As mentioned before, various authors have analyzed the U.S.
hog data, and in particular, Tiao and Tsay (1983) used the hog data to illustrate
specification procedures for multivariate models. For comparative purposes, they
also fit univariate autoregressive moving average models. They observed that the
multivariate models clearly outperform the univariate models in terms of residual
variance, mainly for the hog supply, hog price, and corn price series. Table 5.2 shows
that the results for the reduced-rank models (especially the nested model) are fairly
close to those of the full-rank model when compared in terms of residual variances,
but the reduced-rank models involve far fewer estimated parameters, and results for
both these models are slightly better than those for the index model. Between the two
reduced-rank models, although the nested model involves a few more parameters,
the overall measures such as the AIC criterion indicate that it represents a somewhat
better model fit. The results clearly indicate that little useful information is lost by
reducing the dimension of the lagged series.5.6 Relationship Between Nonstationarity and Canonical Correlations 157
5.6 Relationship Between Nonstationarity and Canonical
Correlations
The reduced-rank models for vector autoregressive time series discussed in the
previous sections can be viewed generally in the framework of models related
to multivariate linear regression with constraints on the autoregressive coefficient
matrices. The associated canonical analysis of Box and Tiao (1977) has been seen
to be the same as the classical canonical correlation analysis originally introduced
by Hotelling (1935, 1936). Only a few concepts specific to time series analysis, such
as the notion of the degree of predictability of linear combinations based on the part
of the series {Yt}, have thus far entered into the presentation of the reduced-rank AR
models and canonical analysis. In this section, we briefly address one additional
issue unique to time series analysis. This is the aspect of nonstationarity of the
vector time series process {Yt} and the relation of nonstationarity to the canonical
analysis. Here we also demonstrate how the reduced-rank autoregressive model can
be formulated to test for non-stationarity.
Consider again the vector AR(p) model (5.8), which we express as (L)Yt = t ,
where (L) = I − 1L −···− pLp is the AR(p) operator. Recall that we have
assumed the condition that det{(z)} = 0 for all complex numbers |z| ≤ 1 (i.e.,
all roots of det{(z)} = 0 must lie outside the unit circle) and that this condition
ensures the stationarity of the process {Yt}. In particular, for the AR(1) process Yt =
Yt−1 + t [p = 1 in (5.8)], we have (L) = I − L, and the condition for
stationarity above is equivalent to the condition that all eigenvalues of the matrix 
lie inside the unit circle.
One interesting result in the work of Box and Tiao (1977) is the development
of the correspondence between nonstationarity in the vector AR(1) process, as
reflected by the roots of det{I − z} = 0 (equivalently, the eigenvalues of ) and
the (squared) canonical correlations between Yt and Yt−1 in the canonical analysis,
as reflected in the eigenvalues of the matrix
R∗ = (0)
−1/2(1)

(0)
−1(1)(0)
−1/2,
for example, see (5.12) and the discussion following it. To develop the correspon￾dence more easily, note that we can write the matrix R∗ as
R∗ = (0)
−1/2(0)
(0)
−1/2,
since  = (1)
(0)−1. The specific result to be established is that the number
of eigenvalues of  approaching the unit circle (the nonstationary boundary) is the
same as the number of canonical correlations between Yt and Yt−1 that approach
unity. This correspondence was presented as a result in the appendix of the paper by
Box and Tiao (1977). Velu et al. (1987) gave an alternative more direct proof of the
result, which we now discuss.
As a preliminary, we begin with a statement of the following useful lemma.158 5 Multiple Time Series Modeling With Reduced Ranks
Lemma 5.1 For any m × m complex-valued matrix , with ordered eigenvalues
|α1|≥···≥|αm|,
+
k
j=1
|αj | ≤ +
k
j=1
ρj , k = 1,...,m − 1,
and
+m
j=1
|αj | = +m
j=1
ρj ,
where ρ2
j is the j th largest eigenvalue of ∗ and ∗ denotes the conjugate
transpose of .
Proof See Marshall and Olkin (1979, p. 231).
We now give the main result as follows.
Theorem 5.1 Suppose Yt follows the stationary vector AR(1) model Yt = Yt−1 +
t , where Cov(t) =  is assumed to be positive definite and fixed. With respect
to variation of the matrix , a necessary and sufficient condition for d ≤ m
of the eigenvalues of (0)−1/2(0)
(0)−1/2 to tend to unity is that d of the
eigenvalues of  approach values on the unit circle.
Proof (Sufficiency) Note that the eigenvalues of  are the same as the eigenvalues
of  = (0)−1/2(0)1/2. Denote these eigenvalues by αj , j = 1,...,m.
The squared canonical correlations, denoted by ρ2
j , between Yt and Yt−1 (≡ Xt)
are the eigenvalues of (0)−1/2(0)
(0)−1/2 = ∗ (see Section 2.4 and
also (5.12)). Because det{I − z} = m
j=1(1 − αj z), from the stationarity
assumption, it follows that |αj | < 1, for j = 1,...,m. The canonical correlations
ρj all lie between zero and one. From Lemma 5.1 with k = d, we have
+
d
j=1
|αj | ≤ +
d
j=1
ρj ,
and when |αj | → 1, j = 1,...,d, then d
j=1 ρj → 1 and hence at least the first d
of the canonical correlations ρj → 1.
(Necessity) See Velu et al. (1987).
Box and Tiao (1977) developed the sufficiency part of the proof with consid￾erable detail, but in the presentation above a simple proof follows with the use
of Lemma 5.1. Some extensions of the results in Theorem 5.1 to higher order
AR(p) processes were presented by Velu et al. (1987), involving partial canonical
correlations. However, a complete correspondence between the number of roots αj
of det{(z)} = 0 that approach the unit circle and the number of various canonical5.6 Relationship Between Nonstationarity and Canonical Correlations 159
correlations and partial canonical correlations of the AR(p) process {Yt} has not
been established.
The main implication of Theorem 5.1 is that the existence of d canonical
correlations ρj close to one in the vector AR(1) process implies that the associated
canonical components will be (close to) nonstationary series, and these may
reflect the nonstationary trending or dynamic growth behavior in the multivariate
system, while the remaining r = m − d canonical variates possess stationary
behavior. Hence, this canonical analysis may be useful for exploring the nature of
nonstationarity of the multiple autoregressive time series. Specifically, the nature of
nonstationarity can be examined by considering the canonical correlations between
Yt and Yt−1 and the associated eigenvectors (canonical variates). This approach
to investigation of the nonstationary aspects of the vector AR process has been
explored by Bossaerts (1988) and Bewley and Yang (1995).
A particularly important instance of nonstationarity in the vector AR process is
the case where the only roots of det{(z)} = 0 on the unit circle are unity. This
corresponds typically to the situation where the nonstationary vector process Yt
becomes stationary upon first differencing, that is, Wt = (1 − L)Yt is stationary.
In this case, for the vector AR(1) model, an alternate canonical analysis is also
extremely useful to examine the nature of nonstationarity, in particular, the number
of unit roots in the system. The associated methodology is the basis for ML
estimation and LR testing procedures concerning processes with unit roots, and it
will be discussed and developed in much more detail in the next section. For the
present, we will establish a connection between the number of unit eigenvalues of
 in the vector AR(1) process and the number of zero canonical correlations in a
certain canonical analysis.
For this, we first note that the vector AR(1) model, Yt = Yt−1 + t , can be
expressed in an equivalent form as
Wt ≡ Yt − Yt−1 = −(I − )Yt−1 + t, (5.20)
which is referred to as an error-correction form. Notice that if  has eigenvalues
approaching unity, then I −  will have eigenvalues approaching zero (and will
become of reduced rank). The methodology of interest for exploring the number of
unit roots in the AR(1) model is motivated by the model form (5.20) and consists of
a canonical correlation analysis between Wt = Yt − Yt−1 and Yt−1 (≡ Xt). Under
stationarity of the process Yt , from (5.20), note that the covariance matrix between
Wt and Yt−1 ≡ Xt is equal to wx = −(I − )(0). Thus, the squared canonical
correlations between Wt and Yt−1 are the eigenvalues of
−1
wwwx−1
xx xw = −1
ww(I − )(0)(I − )

, (5.21)
where
ww = Cov(Wt) = 2(0) − (0) − (0) = (I − )(0) + (0)(I − )

.160 5 Multiple Time Series Modeling With Reduced Ranks
We now present the basic result.
Theorem 5.2 Suppose Yt follows the stationary vector AR(1) model, with error￾correction form given in (5.20), where Cov(t) =  is assumed to be positive
definite and fixed. With respect to variation of the matrix , the condition that d ≤ m
of the eigenvalues of −1 ww(I −)(0)(I −) (i.e., squared canonical correlations
between Wt and Yt−1) tend to zero implies that at least d of the eigenvalues of 
approach unity.
Proof Denote the squared canonical correlations between Wt and Yt−1, that is, the
eigenvalues of (5.21), by σ2
j , j = 1,...,m. Now notice that
(0)
−1/2ww(0)
−1/2 = (0)
−1/2(I − )(0)
1/2 + (0)
1/2(I − )

(0)
−1/2
≡ U + U
,
where U = (0)−1/2(I − )(0)1/2. In addition, then, the matrix in (5.21) can be
reexpressed as
(0)
−1/2{(0)
−1/2ww(0)
−1/2}
−1
× (0)
−1/2(I − )(0)(I − )

(0)
−1/2(0)
1/2
≡ (0)
−1/2{U + U
}
−1UU
(0)
1/2,
and this matrix has the same eigenvalues as the matrix
{U + U
}
−1/2UU
{U + U
}
−1/2.
Let |α∗
1 |≥···≥|α∗
m| denote the ordered eigenvalues of the matrix {U +U
}−1/2U.
Now suppose that d = m−r canonical correlations σj approach 0, so that σr+1 → 0
in particular. Then, by Lemma 5.1,
r+1
j=1 |α∗
j | ≤ r+1
j=1 σj → 0. This implies that
|α∗
r+1| → 0 (so also |α∗
j | → 0 for j = r + 1,...,m) and hence that at least d =
m−r eigenvalues of {U +U
}−1/2U approach 0. This, in turn, implies that at least d
eigenvalues of U approach 0. Since the eigenvalues of U = (0)−1/2(I−)(0)1/2
are the same as the eigenvalues of I − , and the latter eigenvalues are equal to
1 − αj , j = 1,...,m, where αj are the eigenvalues of , it follows that at least d
values (1 − αj ) → 0, equivalently, at least d eigenvalues αj → 1.
To consider the (partial) converse result, suppose that d = m − r eigenvalues αj
of  approach unity, and all other |αj | remain bounded in the limit by a constant
strictly less than one. Hence, d eigenvalues 1 − αj of I −  approach zero, and, in
addition, we will assume that the rank of I −  approaches r = m − d in the limit.
Then it will follow that at least d squared canonical correlations σ2
j (i.e., eigenvalues
of (5.21)) will tend to zero. Finally, then, we notice that the results of Theorems 5.1
and 5.2 combine to establish the following relation. Namely, if d ≤ m canonical5.7 Cointegration for Nonstationary Series—Reduced Rank in Long Term 161
correlations between Wt = Yt − Yt−1 and Yt−1 tend to zero, then at least d of the
canonical correlations between Yt and Yt−1 approach unity.
To discuss certain relationships concerning nonstationarity in more specific
detail, we consider further the particular case of a vector AR(1) model Yt =
Yt−1 + t in which d = m − r eigenvalues αi of  approach unity, and
all other |αi| remain bounded in the limit by a constant strictly less than one.
We will also assume that  approaches its limit in such a way that the rank of
I −  is r = m − d in the limit, that is, there are d linearly independent left
eigenvectors Q
1,...,Q
d associated with the unit eigenvalues of . Informally,
then, in a canonical correlation analysis between Wt = Yt − Yt−1 and Yt−1,
the d linear combinations w∗
1t = Q
1Wt,...,w∗
dt = Q
dWt tend to provide the
canonical variates having associated canonical correlations that approach zero. The
corresponding canonical variates for Yt−1 are −Q
i(I − )Yt−1, with the squared
canonical correlations
σ2
i = Corr2(Q
iWt, −Q
i(I − )Yt−1)
= (Q
i(I − )(0)(I − )
Qi)2
(Q
iwwQi)(Q
i(I − )(0)(I − )
Qi)
= Q
i(I − )(0)(I − )
Qi
Q
i(I − )(0)Qi + Q
i(0)(I − )
Qi
→ 0,
since Q
i(I − )(0)(I − )
Qi/[Q
i(I − )(0)Qi + Q
i(0)(I − )
Qi] ∼
(1 − αi)2Q
i(0)Qi/[2(1 − αi)Q
i(0)Qi] = (1 − αi)/2 → 0 in the limit.
5.7 Cointegration for Nonstationary Series—Reduced Rank
in Long Term
In Sections 5.2 and 5.3, we considered certain reduced-rank models for vector
autoregressive time series, and in Section 5.6, we presented a relationship between
nonstationarity of the process and canonical correlation analyses. In this section,
reduced-rank regression methods will be shown to be useful in recognizing and
explicitly modeling nonstationary aspects of a vector process. While the computa￾tional procedures of the reduced-rank methods used in modeling relationships in this
nonstationary setting are similar to those presented for other reduced-rank models in
previous chapters and sections, the asymptotic theory of the estimation and testing
procedures differs from the usual asymptotic normal theory because of the presence
of nonstationary characteristics of the series involved in the analysis.
Many time series in practice exhibit nonstationary behavior, often of a homoge￾neous nature, that is, drifting or trending behavior such that apart from a shifting
local level or local trend the series has homogeneous patterns with respect to shifts
over time. Often, in univariate integrated ARMA time series models, a nonstationary162 5 Multiple Time Series Modeling With Reduced Ranks
time series can be reduced to stationarity through differencing of the series. For the
vector AR model (5.8), (L)Yt = t , where (L) = I − 1L −···− pLp, the
condition for stationarity of the vector process Yt is that all roots of det{(z)} = 0
be greater than one in absolute value. To generalize this to nonstationary but
nonexplosive processes, one can consider a general form of the vector AR model
where some of the roots of det{(z)} = 0 are allowed to have absolute value equal
to one. More specifically, because of the prominent role of the differencing operator
(1 − L) in univariate models, we might only allow some roots to equal one (unit
roots), while the remaining roots are all greater than one in absolute value.
The nonstationary (unit-root) aspects of a vector process Yt become more
complicated in multivariate time series modeling compared to the univariate case,
due in part to the possibility of cointegration among the component series yit
of a nonstationary vector process Yt . For instance, the possibility exists for each
component series yit to be nonstationary with its first difference (1−L)yit stationary
(in which case yit is said to be integrated of order one), but such that certain
linear combinations zit = b
iYt of Yt will be stationary. In such circumstances,
the process Yt is said to be cointegrated with cointegrating vectors bi (e.g., Engle
and Granger 1987). An interpretation of cointegrated vector series Yt , particularly
related to economics, is that the individual components yit share some common
nonstationary components or “common trends,” and hence, they tend to have certain
similar movements in their long-term behavior. A related interpretation is that the
component series yit , although they may exhibit nonstationary behavior, satisfy
(approximately) a long-run equilibrium relation b
iYt ≈ 0 such that the process
zit = b
iYt , which represents the deviations from the equilibrium, exhibits stable
behavior and so is a stationary process. A specific nonstationary AR structure for
which cointegration occurs is the model where det{(z)} = 0 has d<m roots equal
to one and all other roots are greater than one in absolute value, and also the matrix
(1) = I − 1 −···− p has rank equal to r = m − d. Then, for such a process,
it can be established that r linearly independent vectors bi exist such that b
iYt is
stationary, and Yt is said to have cointegrating rank r. Properties of nonstationary
cointegrated systems have been investigated by Engle and Granger (1987), among
others, and basic properties and estimation of cointegrated vector AR models will
be considered in some detail in the remainder of this section. In particular, we will
illustrate the usefulness of reduced-rank estimation methods in the analysis of such
models.
Thus, we consider multivariate AR models for nonstationary processes {Yt}. We
focus on situations where the only “nonstationary roots” in the AR operator (L)
are roots equal to one (unit roots), and we assume there are d ≤ m such unit roots,
with all other roots of det{(z)} = 0 outside the unit circle. We note immediately
that this implies that det{(1)} = 0 so that the matrix (1) = I−p
j=1 j does not
have full rank. It is also assumed that rank{(1)} = r, with r = m−d, and that (1)
has (exactly) d zero eigenvalues. It is further noted that this last condition implies
that each component of the first differences Wt = Yt −Yt−1 will be stationary (rather
than any component of Yt being integrated of order higher than one). The AR(p)5.7 Cointegration for Nonstationary Series—Reduced Rank in Long Term 163
model, (L)Yt = Yt − p
j=1 jYt−j = t , can also be represented in the error￾correction form (Engle and Granger 1987) as ∗(L)(1 − L)Yt = −(1)Yt−1 + t ,
that is,
Wt = CYt−1 +
p
−1
j=1
∗
jWt−j + t, (5.22)
where Wt = (1 − L)Yt = Yt − Yt−1,
∗(L) = I −
p
−1
j=1
∗
jLj ,
with ∗
j = −p
i=j+1 i, and C = −(1) = −(I − p
j=1 j ).
The form (5.22) is obtained from direct matrix algebra, since (L) can always
be reexpressed as (L) = ∗(L)(1 − L) + (1)L with ∗(L) as defined. For
example, in the AR(2) model, we have
(L) = I − 1L − 2L2 = (I − IL) + (I − 1 − 2)L + 2(I − IL)L
= (I + 2L)(I − IL) + (I − 1 − 2)L ≡ ∗(L)(1 − L) + (1)L,
where ∗(L) = I + 2L ≡ I − ∗
1L with ∗
1 = −2. Under the assumptions
on (L), it can also be shown that ∗(L) is a stationary AR operator having all
roots of det{∗(z)} = 0 outside the unit circle. The error-correction form (5.22) is
particularly useful because the number of unit roots in the AR operator (L) can
conveniently be incorporated through the “error-correction” term CYt−1, so that the
nature of nonstationarity of the model is conveniently concentrated in the behavior
of the single coefficient matrix C in this form.
From the assumptions, the matrix I − (1) = p
j=1 j has d linearly
independent eigenvectors associated with its d unit eigenvalues, while its remaining
eigenvalues are less than one in absolute value. Let P and Q = P −1 be m × m
matrices such that Q(p
j=1 j )P = diag[Id , r] = J , where J is the Jordan
canonical form of p
j=1 j , so that QCP = J − I = diag[0, r − Ir]. Hence,
C = P (J − I )Q = P2(r − Ir)Q
2, where P = [P1, P2] and Q = [Q1, Q2], with
P1 and Q1 being m × d matrices, so that C is of reduced rank r<m. Therefore,
the error-correction form (5.22) can be written as
Wt = AQ
2Yt−1 +
p
−1
j=1
∗
jWt−j + t ≡ AZ2t−1 +
p
−1
j=1
∗
jWt−j + t, (5.22
)
where A = P2(r − Ir) is m × r of rank r and Z2t = Q
2Yt . We also define
Z1t = Q
1Yt , and let Zt = (Z
1t
, Z
2t) = QYt .164 5 Multiple Time Series Modeling With Reduced Ranks
It follows from the above that although Yt is nonstationary (with first differences
that are stationary), the r linear combinations Z2t = Q
2Yt are stationary (e.g., since
through (5.22) the variables Z2t are such that Z2t − rZ2t−1 is linearly expressible
in terms of the stationary series {Wt} and {t} only and r is stable). In this situation,
Yt is cointegrated of rank r, and the rows of Q
2 are cointegrating vectors. The
stationary linear combinations Z2t = Q
2Yt may be interpreted as long-term stable
equilibrium relations among the variables Yt , and the error-correction model (5.22),
as written in (5.22
), formulates that the changes Wt = Yt − Yt−1 in the variables Yt
depend on the deviations from the equilibrium relations in the previous time period
(as well as on previous changes Wt−j ). These deviations, Z2t−1, are thus viewed as
being useful explanatory variables for the next change in the process Yt , and in the
context of (5.22
), they are referred to as an error-correction mechanism.
Conversely, the d-dimensional series Z1t = Q
1Yt is purely nonstationary, with
the unit-root nonstationary behavior of the series Yt generated by Z1t . That is,
from Zt = (Z
1t, Z
2t) = QYt , we obtain the relation Yt = P Zt = P1Z1t +
P2Z2t . We then see that Yt is a linear combination of the d-dimensional purely
nonstationary component Z1t and the r-dimensional stationary component Z2t . The
purely nonstationary component Z1t may be viewed as the common nonstationary
component or the d “common trends” among the Yt , with the interpretation that the
nonstationary behavior in each of the m component series Yit of Yt is actually driven
by d(< m) common underlying stochastic trends (Z1t).
5.7.1 LS and ML Estimation and Inference
Least squares estimates C
, ∗
1,..., ∗
p−1 for the model (5.22) in error-correction
form can be obtained, and the limiting distribution theory for these estimators has
been derived by Ahn and Reinsel (1990). To describe the asymptotic results, note
that the model (5.22) can be expressed as
Wt = CPZt−1 +
p
−1
j=1
∗
jWt−j + t
= CP1Z1t−1 + CP2Z2t−1 +
p
−1
j=1
∗
jWt−j + t,
where Zt = QYt = (Z
1t, Z
2t)
, with Z1t = Q
1Yt purely nonstationary, whereas
Z2t = Q
2Yt is stationary, and note also that CP1 = 0. Because of the presence
of the nonstationary “regressor” variables (Z1t) in the model above, the least
squares and maximum likelihood estimation theory is non-standard compared
to the situation with stationary regressors. In addition, from (5.22), the process
ut = Q(Wt − CYt−1) = Zt − J Zt−1, where J = diag[Id , r], is equal to5.7 Cointegration for Nonstationary Series—Reduced Rank in Long Term 165
ut = Q(p−1
j=1 ∗
jWt−j + t) and, hence, ut is stationary. Therefore, ut has the
(stationary) infinite MA representation of the form ut = (L)at = ∞
j=0 j at−j ,
with at = Qt , and we let  = (1) = ∞
j=0 j .
Now let F = [C, ∗
1,..., ∗
p−1], Xt−1 = (Y 
t−1, W
t−1,...,W
t−p+1)
, and
X∗
t−1 = (Z
t−1, W
t−1,...,W
t−p+1) ≡ Q∗Xt−1, where Q∗ = diag[Q, Ik(p−1)],
and assume that observations Y1−p,...,Y0, Y1,...,Yt are available (for conve￾nience of notation), with Wt = Yt − Yt−1. Then the error-correction model (5.22)
can be written as
Wt = F Xt−1 + t = FP∗X∗
t−1 + t,
where P∗ = diag[P,Ik(p−1)] = Q∗−1. Thus, the least squares (LS) estimator of F
can be represented by
F
 =


T
t=1
WtX
t−1
 
T
t=1
Xt−1X
t−1
−1
=


T
t=1
WtX∗
t−1
 
T
t=1
X∗
t−1X∗
t−1
−1
Q∗,
so that
(F
− F )P∗ =


T
t=1
tX∗
t−1
 
T
t=1
X∗
t−1X∗
t−1
−1
. (5.23)
With U∗
t−1 = (Z
2t−1, W
t−1,...,W
t−p+1)
, which is stationary, and so X∗
t−1 =
(Z
1t−1, U∗
t−1)
, it has been shown that T −3/2 T
t=1 U∗
t−1Z
1t−1
p
−→ 0 as T → ∞.
Hence, it follows that the LS estimator for the model (5.22) has the representation
[T CP 1, T 1/2 (CP 2 − CP2, ∗
1 − ∗
1,..., ∗
p−1 − ∗
p−1)]
=

P


T −1atZ
1t−1
 
T −2Z1t−1Z
1t−1
−1
,


T −1/2tU∗
t−1
 
T −1U∗
t−1U∗
t−1
−1

+ op(1).
Then, the distribution theory (see Lemma 1 and Theorem 1 in Ahn and Reinsel
1990) for the least squares estimator is such that T CP 1
d
−→ PM, where166 5 Multiple Time Series Modeling With Reduced Ranks
M = 1/2 a
2 1
0
Bd (u) dBm(u)
  2 1
0
Bd (u)Bd (u) du −1
−1/2 a1 −1
11 ,
(5.24)
where a = QQ = Cov(at) with  = Cov(t), a1 = [Id , 0]a[Id , 0]
 is
the upper-left d × d block of the matrix a, and 11 is the upper-left d × d block
of the matrix , Bm(u) is a m-dimensional standard Brownian motion process,
and Bd (u) = −1/2 a1 [Id , 0]1/2 a Bm(u) is a d-dimensional standard Brownian
motion. It can be shown (Reinsel 1997, Section 6.3.8) that 11 is expressible more
explicitly as 11 = {Q
1∗(1)P1}−1. It also holds that T 1/2(CP 2 − CP2) and
T 1/2(∗
j − ∗
j )
, j = 1,...,p − 1, have a joint limiting multivariate normal
distribution as in stationary model situations, such that the “vec” of these terms
has limiting distribution N (0,  ⊗ −1
U∗ ), where U∗ = Cov(U∗
t ). Also, the LS
estimates ∗
1,..., ∗
p−1 have the same asymptotic distribution as in the “stationary
case,” where Q2 is known and one regresses Wt on the stationary variables Z2t−1 =
Q
2Yt−1 and Wt−1,...,Wt−p+1.
When maximum likelihood estimation of the parameter matrix C in (5.22) is
considered subject to the reduced-rank restriction that rank(C) = r, it is convenient
to express C as C = AB, where A and B are m × r and r × m full-rank matrices,
respectively. As usual, some normalization conditions on A and B are required for
uniqueness. One possibility is to have B normalized so that B = [Ir, B0], where
B0 is an r × (m − r) matrix of unknown parameters. The elements of the vector
Yt can always be arranged so that this normalization is possible. Implicit in this
arrangement of Yt is that if we write Yt = (Y 
1t, Y 
2t)
, where Y1t is r × 1 and Y2t is
d × 1, then there is no cointegration among the components of Y2t . It is emphasized
that the estimation of the model with the reduced-rank constraint imposed on
C is equivalent to the estimation of the AR model with d unit roots explicitly
imposed in the model. Hence, this is an alternative to (arbitrarily) differencing
each component of the series Yt prior to fitting a model in situations where the
individual components tend to exhibit nonstationary behavior. This explicit form of
modeling the nonstationarity may lead to a better understanding of the nature of
nonstationarity among the different component series and may also improve longer￾term forecasting over unconstrained model fits that do not explicitly incorporate
unit roots in the model. Hence, there may be many desirable reasons to formulate
and estimate the AR model with an appropriate number of unit roots explicitly
incorporated in the model, and it is found that a particularly convenient way to do
this is through the use of model (5.22) with the constraint rank(C) = r imposed.
Maximum likelihood estimation of C = AB = P2(r −Ir)Q
2 and the ∗
j under
the constraint that rank(C) = r, using an iterative Newton–Raphson procedure with
the normalization B = [Ir, B0], is presented by Ahn and Reinsel (1990), and the
limiting distribution theory for these estimators is derived. Specifically, note that the
model can be written as5.7 Cointegration for Nonstationary Series—Reduced Rank in Long Term 167
Wt = ABYt−1 +
p
−1
j=1
∗
jWt−j + t ≡ ABYt−1 + DW∗
t−1 + t, (5.25)
where D = [∗
1,..., ∗
p−1] and W∗
t−1 = (W
t−1,...,W
t−p+1)
. It is recognized
that this takes the same form as the extended reduced-rank model (3.3) considered
in Section 3.1. We define the vector of unknown parameters as θ = (β
0, α
)
, where
β0 = vec(B
0), and α = vec{[A, D]

} = vec{[A, ∗
1,..., ∗
p−1]

}. With b = r(m−
r) + rm + m2(p − 1) representing the number of unknown parameters in θ, define
the b × m matrices
Ut−1 = [(A ⊗ H
Yt−1)

, Im ⊗ U

t−1]

,
where U
t−1 = [(BYt−1)
, W
t−1,...,W
t−p+1]
 is stationary, and the matrix H =
[0, Im−r] is (m − r) × m such that Y2t = H
Yt is taken to be purely nonstationary
by assumption. Then, based on T observations Y1,...,Yt , the Gaussian estimator
of θ is obtained by the iterative approximate Newton–Raphson relations
θ(i+1) = θ(i) +
#

T
t=1
Ut−1−1
 U
t−1
$−1
θ(i) #

T
t=1
Ut−1−1
 t
$
θ(i)
, (5.26)
where θ(i) denotes the estimate at the ith iteration.
Concerning the asymptotic distribution theory of the Gaussian estimators under
the model where the unit roots are imposed, it is assumed that the iterations in (5.26)
are started from an initial consistent estimator θ(0) such that D∗(θ(0) − θ ) =
{T (β
(0)
0 − β0)
, T 1/2(α(0) − α)
} is Op(1), where D∗ = diag[T Ird , T 1/2I(b−rd)].
Then, it has been established (Ahn and Reinsel 1990) that
T (B
0 − B0) d
−→ (A
−1
 A)−1A
−1
 PMP −1
21 ,
where the distribution of M is specified in (5.24), and P21 is the d × d lower
submatrix of P1. For the remaining parameters, α, in the model, it follows that
T 1/2(α − α) =
⎡
⎣Im ⊗

1
T

T
t=1
U
t−1U

t−1
−1⎤
⎦
1
√T

T
t=1
(Im ⊗ U
t−1)t + op(1).
Hence, it is also shown that T 1/2(α−α) d
−→ N (0, ⊗−1
U
 ), where U
 = Cov(U
t),
similar to results in stationary situations.
The iterative procedure (5.26) to obtain the Gaussian estimator θ can readily
be modified to incorporate additional constraints, such as zero constraints or
nested reduced-rank constraints on the stationary parameters ∗
j in (5.22), beyond
the cointegrating constraint that rank(C) = r. However, when there are no168 5 Multiple Time Series Modeling With Reduced Ranks
constraints imposed other than rank(C) = r, the Gaussian reduced-rank estimator
in model (5.22) can also be obtained explicitly through the partial canonical
correlation analysis, based on the previous work as discussed in Section 3.1,
since the model (5.25) has the same form as (3.3). This approach was presented
by Johansen (1988, 1991). To describe the results, let Wt and Y
t−1 denote the
“adjusted” or residual vectors from the least squares regressions of Wt and Yt−1,
respectively, on the lagged values W∗
t−1 = (W
t−1,...,W
t−p+1)
, and let
Sww = 1
T

T
t=1
WtW
t , Swy = 1
T

T
t=1
WtY

t−1, Syy = 1
T

T
t=1
Y
t−1Y

t−1.
Then, the sample partial canonical correlations ρi between Wt and Yt−1, given
Wt−1,...,Wt−p+1, and the corresponding vectors V
∗
i are the solutions to
(ρ2
i Im − S−1/2
ww SwyS−1
yy SywS−1/2
ww )V
∗
i = 0, i = 1, . . . , m. (5.27)
The reduced-rank Gaussian estimator of C = AB can then be obtained from results
of Sections 2.3–2.4 and 3.1. It can be expressed explicitly as C
 = S1/2
wwV
∗V

∗S−1/2
ww C
,
where C
 = SwyS−1
yy is the full-rank LS estimator, and V
∗ = [V
∗
1 ,..., V
∗
r ] are the
(normalized) vectors corresponding to the r largest partial canonical correlations ρi,
i = 1,...,r. This form of the estimator provides the reduced-rank factorization as
C
 = (S1/2
wwV
∗)(V

∗S−1/2
ww C)  ≡ A
B, 
with A
 = S1/2
wwV
∗ satisfying the normalization A

S−1
wwA
 = Ir. The Gaussian
estimator for the other parameters ∗
1,..., ∗
p−1 can be obtained by ordinary least
squares regression of Wt − CY t−1 on the lagged variables Wt−1,...,Wt−p+1. As
an alternate form of the reduced-rank estimator, let V
∗∗
i = S−1/2
ww V
∗
i (1 − ρ2
i )−1/2,
i = 1,...,m, and define the m × r matrix V
∗∗ = S−1/2
ww V
∗(Ir − 2
∗)−1/2,
where ∗ = diag(ρ1,..., ρr). Then the reduced-rank estimator can be expressed
equivalently as C
 = V
∗∗V

∗∗C
, where
 = T −1
T
t=1
t
t = Sww − SwyS−1
yy Syw
is the error covariance matrix estimate of  from the full-rank LS estimation. The
vectors V
∗∗
i in V
∗∗ are normalized by V
∗∗
i V
∗∗
i = 1 (so that V

∗∗V
∗∗ = Ir).
Note from the discussion in Section 2.4 (e.g., see Theorem 2.3) that the
expression B
 = V

∗S−1/2
ww C
 shows that the estimated cointegrating vectors (rows
of B
) are obtained from the vectors that determine the first r canonical variates of
Yt−1 in the partial canonical correlation analysis. In addition, it is seen from the form
of the reduced-rank estimator C
 = S1/2
wwV
∗V

∗S−1/2
ww C
 given above that the Gaussian5.7 Cointegration for Nonstationary Series—Reduced Rank in Long Term 169
estimates of the “common trends” components Z1t = Q
1Yt can be obtained with
Q1 = S−1/2
ww [V
∗
m,..., V
∗
r+1],
so that the rows of Q
1 are the vectors V
∗
i S−1/2
ww , i = r +1,...,m, corresponding to
the d smallest partial canonical correlations, with the property that Q
1C
 = 0 since
Q
1A
 = [V
∗
m,..., V
∗
r+1]

V
∗ = 0. This method of estimation of the common “trend”
or “long-memory” components was explored by Gonzalo and Granger (1995).
5.7.2 Likelihood Ratio Test for the Number of Cointegrating
Relations
Within the context of model (5.22), it is necessary to specify or determine the rank r
of cointegration or the number d of unit roots in the model. Thus, it is also of interest
to test the hypothesis H0 : rank(C) ≤ r, which is equivalent to the hypothesis that
the number of unit roots in the AR model is greater than or equal to d (d = m − r),
against the general alternative that rank(C) = m. The likelihood ratio test for this
hypothesis is considered by Johansen (1988) and by Reinsel and Ahn (1992). The
likelihood ratio (LR) test statistic is given by
−T log(U ) = −T log(|S|/|S0|),
where S = T
t=1t
t denotes the residual sum of squares matrix in the full or
unconstrained model, while S0 is the residual sum of squares matrix obtained under
the reduced-rank restriction on C that rank(C) = r. It follows from work of
Anderson (1951) in the multivariate linear model with reduced-rank structure (see
Section 3.1) that the LR statistic can be expressed equivalently as
− T log(U ) = −T m
i=r+1
log(1 − ρ2
i ), (5.28)
where the ρi are the d = m − r smallest sample partial canonical correlations
between Wt = Yt − Yt−1 and Yt−1, given Wt−1,...,Wt−p+1. The limiting
distribution for the likelihood ratio statistic has been derived by Johansen (1988),
and by Reinsel and Ahn (1992) using the distribution theory for the full-rank LS
and Gaussian reduced-rank estimators. Specifically, it is given by
−T log(U ) d −→ tr#2 1
0
Bd (u) dBd (u)
 2 1
0
Bd (u)Bd (u) du−1170 5 Multiple Time Series Modeling With Reduced Ranks
×
2 1
0
Bd (u) dBd (u)
 , (5.29)
where Bd (u) is a d-dimensional standard Brownian motion process. Note that the
asymptotic distribution of the likelihood ratio test statistic under H0 depends only
on d and not on any parameters or the order p of the AR model.
Critical values of the asymptotic distribution of (5.29) have been obtained by
simulation by Johansen (1988) and Reinsel and Ahn (1992) and can be used in
the test of H0. Some other approaches to testing for unit roots in nonstationary
multivariate systems have been examined by Engle and Granger (1987), Stock and
Watson (1988), Fountis and Dickey (1989), Phillips and Ouliaris (1986), and Bewley
and Yang (1995). It is known that inclusion of a constant term in the estimation
of the nonstationary AR model affects the limiting distribution of the estimators
and test statistics. Modification of the asymptotic distribution theory for the LS and
Gaussian estimators and for LR testing, to allow for inclusion of constant terms
in the estimation, has been considered by Johansen (1991), Johansen and Juselius
(1990), and Reinsel and Ahn (1992), and critical values for the corresponding
limiting distribution of the LR test statistic have been obtained by Monte Carlo
simulation.
5.8 Unit Root and Cointegration Aspects for the U.S. Hog
Data Example
We illustrate the modeling and analysis procedures for nonstationary vector AR
processes discussed in the preceding section by further considering the annual U.S.
hog data from Section 5.5. The previous analyses for these data in Section 5.5
indicate that an AR(2) model may be adequate, and the basic time series plots
in Fig. 5.1 suggest nonstationarity in the component series. Hence, we investigate
the nature of the nonstationarity in terms of (the number of) unit roots in the
AR(2) model operator, equivalently, the number of cointegrating relations among
the components. As discussed, the AR(2) model can be expressed in the equivalent
error-correction model form (5.22),
Wt = CYt−1 + ∗
1Wt−1 + t,
where Wt = Yt − Yt−1, C = −(1) = −(I − 1 − 2), and ∗
1 = −2.
Within the context of a vector AR model, we consider LR tests for the number
of unit roots d or the rank of cointegration r = m − d. Thus, we perform the partial
canonical correlation analysis between Wt and Yt−1, given Wt−j , j = 1,...,k − 1.
For greater generality and to illustrate the effect of the choice of AR order k on
the LR test results, we provide results for various values of k = 1,..., 5. The
sample partial canonical correlations ρi are shown in Table 5.3, as well as the LR5.8 Unit Root and Cointegration Aspects for the U.S. Hog Data Example 171
Table 5.3 Summary of partial canonical correlation analysis between Wt = Yt − Yt−1 and Yt−1
and associated LR test statistics from (5.29) for the U.S. hog data. (Test statistics are used to
determine the appropriate rank r = m − d, the number of cointegrating relations, of matrix C)
LR Statistic
k Partial correlations r = 0 r = 1 r = 2 r = 3 r = 4
1 0.718, 0.708, 0.473, 0.340, 0.063 142.2 84.9 30.0 10.0 0.32
2 0.804, 0.576, 0.473, 0.354, 0.022 142.9 61.7 30.2 10.5 0.04
3 0.593, 0.530, 0.442, 0.369, 0.003 86.7 53.4 28.0 11.3 0.00
4 0.560, 0.490, 0.386, 0.344, 0.052 71.5 42.9 22.0 9.8 0.21
5 0.677, 0.569, 0.393, 0.372, 0.026 99.2 53.1 23.8 11.2 0.05
Critical values at 0.05% 71.1 49.4 31.7 18.0 8.16
test statistic values for H0 : rank(C) = r for r = 0,..., 4. Since an AR(p) model
of order p = 2 has been chosen as appropriate, we concentrate on the LR test
statistics for k = 2 in Table 5.3. By referring to the upper percentiles of the limiting
distribution of the LR test statistic (e.g., see Reinsel 1997, p. 205), we find that the
hypotheses of d ≥ 1 (r ≤ 4) or d ≥ 2 (r ≤ 3) are clearly not rejected, while d ≥ 4
(r ≤ 1) or d ≥ 5 (r ≤ 0) are strongly rejected. There is also moderate evidence
for rejection of d ≥ 3 (r ≤ 2), and so the LR testing procedures suggest that there
are d = 2 unit roots and r = 3 cointegrating vectors. (Note that, with the exception
of the value for r = 0 at k = 2, the LR test statistic values in Table 5.3 remain
relatively stable for values of k ≥ 2, which is consistent with an AR model of order
2.)
The full-rank LS estimates 1 and 2 of the AR(2) model were given previously
in Section 5.5, and from these, the LS estimates C
 = −(I − 1 − 2) and ∗
1 =
−2 in the error-correction form can easily be deduced. The Gaussian reduced-rank
estimates with r = rank(C) = 3 imposed, which incorporates the d = 2 unit roots
in the model, can be obtained from the partial canonical correlation results for AR
order k = 2 ≡ p. As described in the previous section, we obtain C
 = A
B
, with
A
= S1/2
ww V
∗ and B
 = V

∗ S−1/2
ww C
, as
C
 = A
B
 =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0.027 −0.180 0.023
−0.286 −0.162 −0.100
0.781 −0.252 −0.126
0.288 −1.013 0.463
−0.214 0.464 −0.109
⎞
⎟
⎟
⎟
⎟
⎟
⎠
×
⎛
⎝
0.639 0.367 −1.240 0.617 0.262
−0.747 1.671 0.609 0.073 −0.735
−0.214 1.465 0.613 −0.741 −0.812
⎞
⎠172 5 Multiple Time Series Modeling With Reduced Ranks
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0.147 −0.258 −0.130 −0.013 0.121
−0.041 −0.522 0.195 −0.115 0.125
0.715 −0.319 −1.199 0.557 0.492
0.843 −0.910 −0.691 −0.240 0.444
−0.460 0.537 0.482 −0.018 −0.309
⎞
⎟
⎟
⎟
⎟
⎟
⎠
with ML error covariance matrix estimate  such that | | = 0.1653 × 10−3.
There is close agreement between the reduced-rank estimate C
 and the (implied) LS
estimate C
, and the corresponding ML estimate (not shown) ∗
1 of ∗
1 is found to
be very close to the LS estimate ∗
1 = −2.
From the estimation results above, estimates of three cointegrating linear combi￾nations are given by
Z2t = (z3t, z4t, z5t)
 = −1
∗ BY t ≡ Q
2Yt,
which are normalized such that the variances of the corresponding adjusted series
Z
2t = −1
∗ B
Y
t have unit variances (i.e., −1
∗ BS yyB

−1
∗ = Ir). These three series
are displayed in Fig. 5.3 and give the rather clear appearance of being stationary.
Also shown in Fig. 5.3 are the two linear combinations
Z1t = (z1t, z2t)
 = Q
1Yt,
where Q1 = S−1/2
ww [V
∗
m,..., V
∗
r+1] and is given by
Q
1 =

3.237 −0.744 −0.228 −0.362 0.085
2.346 −1.096 −0.673 −1.017 −2.059
.
The series Z1t = Q
1Yt are normalized such that the variances of the adjusted
first differences of these series, Q
1Wt , have unit variances (i.e., Q
1SwwQ1 = Id ).
These linear combinations can be interpreted as the “common trend” series among
the five original series. Notice that the first of the common trend series consists
predominantly of the farm wage rates series (y1t).
From a model building point of view, we recall from Section 5.5 that a reduced
rank of 3 was also indicated for the lag 2 coefficient matrix 2, and so this would
also be true for the matrix ∗
1 in the error-correction form. Thus, for the final model
estimation results, we fit a combined reduced-rank AR(2) model in error-correction
form, Wt = CYt−1 + ∗
1Wt−1 + t , with the constraints that rank(C) = 3 and
rank(∗
1) = 3, the latter constraint as suggested from the LR test statistic results
of Table 5.1 in Section 5.5. Notice that there is no “nesting” of the two coefficient
matrices in this form of reduced-rank model, and hence, the model has the form of
the model (3.23) in Section 3.4. We write C = AB and ∗
1 = EF and use the
normalizations B = [I3, B0] and E = [I3, E
0]

. The final Gaussian reduced-rank5.8 Unit Root and Cointegration Aspects for the U.S. Hog Data Example 173
Fig. 5.3 Common trends and cointegrating series for the annual U.S. hog industry data, 1867–
1947174 5 Multiple Time Series Modeling With Reduced Ranks
estimates are obtained by Newton–Raphson iterative procedures, similar to those
described in Sections 5.3 and 5.7 and are given by
C
 = A
B
 =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0.136 −0.220 −0.118
−0.036 −0.555 0.184
0.714 −0.333 −1.196
0.849 −1.003 −0.734
−0.493 0.711 0.548
⎞
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎝
100 −1.6129 −0.3114
010 −0.1694 −0.4095
001 −1.3807 −0.4947
⎞
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0.136 −0.220 −0.118 −0.020 0.106
−0.036 −0.555 0.184 −0.103 0.147
0.714 −0.333 −1.196 0.557 0.506
0.849 −1.003 −0.734 −0.187 0.509
−0.493 0.711 0.548 −0.082 −0.409
⎞
⎟
⎟
⎟
⎟
⎟
⎠
∗
1 = E
F
 =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1 00
0 10
0 01
1.511 −0.986 0.072
−2.728 3.922 1.572
⎞
⎟
⎟
⎟
⎟
⎟
⎠
×
⎛
⎝
0.364 0.117 0.084 0.033 0.008
0.161 −0.005 −0.265 0.199 0.131
0.392 −0.060 0.557 −0.325 −0.434
⎞
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0.364 0.117 0.084 0.033 0.008
0.161 −0.005 −0.265 0.199 0.131
0.392 −0.060 0.557 −0.325 −0.434
0.419 0.178 0.428 −0.169 −0.148
0.256 −0.433 −0.392 0.179 −0.193
⎞
⎟
⎟
⎟
⎟
⎟
⎠
with ML error covariance matrix estimate  such that | | = 0.1696 × 10−3.
Because the ML estimates for ∗
1 follow the usual normal distribution theory
asymptotics, a LR test statistic for the validity of the constraint that rank(∗
1) = 3
has the usual asymptotic χ2
4 distribution. The value of this LR statistic is found to be
−(79−2·5−3/2)log(0.1653/0.1696) = 1.708, and so the reduced-rank restriction
on ∗
1 is quite reasonable, very similar to the LR test finding from Table 5.1 in
Section 5.5. Also notice that the final reduced-rank estimate C
 = A
B
is quite similar
to the reduced-rank estimate obtained when the rank of ∗
1 is not constrained.5.8 Unit Root and Cointegration Aspects for the U.S. Hog Data Example 175
Finally, it may be of some interest to relate and compare the decomposition of
Yt into trend and stationary components, associated with the above analysis in the
error-correction model form, to the canonical analysis associated with the reduced￾rank modeling previously considered in Section 5.5. The transformed vectors
of variates associated with the reduced-rank and error-correction form analyses,
respectively, are the canonical variates Wt = L

Yt and the common trend and
cointegrating variates Zt = (Z
1t, Z
2t) = QY t , where Q = [Q1, Q2]

. Hence,
these two sets of transformed variates are related by Wt = L

Q−1Zt , with the
matrix L

Q−1 given by
L

Q−1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1.1395 −0.2594 0.0794 0.3199 0.3702
−0.0097 −1.1913 0.9551 −0.0831 0.7225
0.0087 0.4240 0.8146 −0.7074 0.8143
0.0114 −0.0864 −0.5534 −0.3071 0.6034
0.0276 −0.0792 −0.0096 −0.6063 −0.4135
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
In the error-correction form analysis above, the original series Yt has been decom￾posed into a “purely nonstationary” component Z1t and a stationary component
Z2t . Of course, any other series formed as linear combinations of the elements
of Z2t will also be stationary, whereas linear combinations that include elements
of Z1t will be nonstationary. The canonical analysis of Box and Tiao (1977)
does not have exactly the same purpose as the analysis above, but there can
be some similarities as noted previously. In particular, in the canonical analysis,
components associated with canonical correlations close to one will tend to be
nonstationary, while those associated with the smaller canonical correlations will
tend to be stationary; canonical correlations very close to zero give rise to series
with (approximate) white noise behavior. From examination of the structure of the
matrix L

Q−1 in the relation Wt = L

Q−1Zt , it is found that the approximate white
noise canonical variate components ω4t = 
4Yt and ω5t = 
5Yt in Wt are related to
the components of Zt roughly as
ω4t ≈ −0.55z3t − 0.31z4t + 0.60z5t and ω5t ≈ −0.61z4t − 0.41z5t .
Thus, not surprisingly, we see that the (stationary) nearly white noise variates
in the canonical correlation analysis are essentially represented as certain linear
combinations of only the stationary components z3t , z4t , z5t resulting from the
cointegration-common trends analysis associated with the error-correction model
form. By contrast, in the linear relations that connect the nearly nonstationary
components, ω1t and ω2t , of the canonical correlation analysis with the transformed
variates Zt , it is seen that ω1t has a large contribution from the first common trend
component z1t and ω2t has a large contribution from z2t .
Also notice that the final error-correction form of the model with the additional
reduced-rank structure on ∗
1 imposed, that is, Wt = ABYt−1+EFWt−1+t , can be
rewritten in the original AR(2) form as Yt = (I + AB + EF )Yt−1 − EFYt−2 + t .176 5 Multiple Time Series Modeling With Reduced Ranks
Hence, in this model, any possible white noise structure could be revealed as a
linear combination 
Yt ≈ 
t for which the relations 
(I + AB + EF) ≈
0 and 
(EF ) ≈ 0 hold. However, although the analysis based on the error￾correction model form is not particularly designed to identify possible white noise
component structure, we see from the discussion in the preceding paragraph that
such white noise components (if present) would come from linear combinations of
the cointegrating stationary component vector Z2t = Q
2Yt in the error-correction
model analysis.Chapter 6
The Growth Curve Model and
Reduced-Rank Regression Methods
6.1 Introduction and the Growth Curve Model
One additional general model class that has aspects of reduced-rank regression,
especially in its mathematical structure, is that of the growth curve or generalized
multivariate ANOVA (GMANOVA) model. This type of model is often applied
in the analysis of longitudinal or repeated measures data arising in biomedical
and other areas. For these models, the components yjk of the m-dimensional
response vector Yk = (y1k,...,ymk) usually correspond to responses on a single
characteristic of a subject made over m distinct times or occasions, with interest in
studying features of the mean response over time, and a sample of T independent
subjects is available. Corresponding to each subject is an n-dimensional set of
time-invariant explanatory variables Xk, which may include factors for treatment
assignments held fixed over all occasions. In addition, it may be postulated that
the pattern of mean response over time for any subject can be represented by
some parametric function, e.g., polynomial function, of time whose coefficients are
specified to be linear functions of the time-invariant explanatory variables Xk. That
is, it is assumed that E(yjk) = a
jβk, where a
j is a known (specified) 1 × r vector
whose elements are functions of the j th time, for j = 1,...,m, and the coefficients
in βk are unknown linear functions of Xk. Let A = [a1,...,am]
 denote the known
m×r matrix, r < min(m, n), whose columns contain values of the functions of time
specified in the parametric form of the mean response function over time. Then the
model (2.5),
Yk = A(BXk) + k, k = 1,...,T, (6.1)
with unknown r × n matrix B, corresponds to this situation with βk ≡ BXk
interpretable as the coefficients in the mean response function over time for the
kth subject, represented as linear functions of the Xk. Another interpretation for the
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8_6
177178 6 The Growth Curve Model and Reduced-Rank Regression Methods
model is that in the standard multivariate regression model, Yk = CXk + k, the
rows of elements of the regression coefficient matrix C are specified to be linear
functions of a known basis functions of time, such as (orthogonal) polynomials up
to degree r − 1. That is, the j th row of C is specified as C
(j ) = a
jB, where a
j is a
known 1×r vector whose elements are functions of the j th time, for j = 1,...,m,
and B is an r × n matrix of unknown parameters. Thus C = AB, with A specified
(known), and hence, the model is again Yk = ABXk + k.
To briefly illustrate a situation more specifically, suppose the T individuals
are from n different experimental or treatment groups, and there are no other
explanatory variables. Then the n × T overall across-individual design matrix X is
simply a matrix of indicator variables as in the one-way ANOVA situation. Assume
Ti vector observations Yil are available for the ith group, for l = 1,...,Ti, with
T = T1 +···+Tn, and observations in each group are measured at the same m time
points, t1 < t2 < ··· < tm. In addition, we assume a model for the growth curves
over time for each group to be of the form of a polynomial of degree r − 1, so that
the mean value at the j th time point for the lth individual in the ith group is given
by
E(yjil) = β1i + β2itj + β3it
2
j +···+ βr it
r−1
j ≡ a
jβi,
with a
j = (1, tj ,...,tr−1
j ) and βi = (β1i, β2i,...,βr i)
. Then the basic
(polynomial) growth curve model for all individuals can be written in matrix form
as Y = ABX + , where Y = [Y11,...,Y1T1 ,...,Yn1,...,YnTn ],
A =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
1 t1 · · t
r−1
1
1 t2 · · t
r−1
2
· · ·· ·
· · ·· ·
1 tm · · tr−1 m
⎤
⎥
⎥
⎥
⎥
⎥
⎦
, B =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
β11 ··· β1n
β21 ··· β2n
· ··· ·
· ··· ·
βr1 ··· βrn
⎤
⎥
⎥
⎥
⎥
⎥
⎦
,
X =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
1
T1 0 0 · · 0
0 1
T2 0 · · 0
· · · ·· ·
· · · ·· ·
0 0 0 · · 1
Tn
⎤
⎥
⎥
⎥
⎥
⎥
⎦
,
and 1T denotes a T × 1 vector of ones.
Techniques for analysis of the basic growth curve model (6.1), and for various
extensions of this model, will be described in the remainder of this chapter. Analysis
for the growth curve model (6.1) is somewhat simpler than that of the general
reduced-rank model (2.5) of Chapter 2, in that the matrix A is specified as known in
the growth curve model, but the two cases still share some mathematical similarities.
Moreover, application of the general model (2.5), with A unknown or unspecified,
to the growth curve or longitudinal data setting might be useful or desirable in some6.2 Estimation of Parameters in the Growth Curve Model 179
situations, especially when m and n are relatively large. The interpretation could be
that the columns of A represent the (unknown) basis functions for the “parametric”
form of the mean response function over time, which are not explicitly specified.
The growth curve model was introduced by Potthoff and Roy (1964) and was also
studied extensively in its early stages by Rao (1965, 1966, 1967), Khatri (1966), and
Grizzle and Allen (1969).
In some uses of the model (6.1), the covariance matrix  of the errors might be
assumed to possess some special structure, rather than a general m × m covariance
matrix. A particular case results from the random effects model assumption for the
kth individual’s regression coefficients (e.g., Laird and Ware 1982; Reinsel 1985).
For this case, it is assumed that Yk = Aβk + uk, where βk is a random vector with
E(βk) = BXk and Cov(βk) = β, and the error vector uk has Cov(uk) = σ2Im.
Hence, with τk = βk −BXk, we have Yk = ABXk +Aτk +uk ≡ ABXk +k, where
k = Aτk + uk has covariance structure  = Cov(Aτk + uk) = AβA + σ2Im.
Sometimes in the above model, only a subset of βk is assumed to be random. Other
types of special covariance structure, including AR and MA time series structures,
are discussed by Jennrich and Schluchter (1986), Diggle (1988), Chi and Reinsel
(1989), Rochon and Helms (1989), and Jones (1986, 1990).
6.2 Estimation of Parameters in the Growth Curve Model
Some basic estimation results for the growth curve model written in matrix form,
Y = ABX + , will now be presented. Under normality and independence of the
errors k, we know from Section 2.3 that the ML estimate of B is the value that
minimizes |(Y − ABX)(Y − ABX)
| with respect to variation of B. It will be
established shortly that the ML estimate of B is given by
B
 = (A
−1
 A)−1A
−1
 YX
(XX
)
−1 ≡ (A
−1
 A)−1A
−1
 C,  (6.2)
where C
 = YX
(XX
)−1 and  = (1/T )(Y − C
X)(Y − C
X) ≡ (1/T )S. The
corresponding ML estimate of the general error covariance matrix  is thus given
by
 = 1
T (Y − AB
X)(Y − AB
X)
 =  + (C
 − AB)  xx (C
 − AB)  
=  + (Im − A(A
−1
 A)−1A
−1
 )C
xxC

× (Im − A(A
−1
 A)−1A
−1
 )

. (6.3)
For derivation of the ML estimate of B, obtained by minimizing |(Y−ABX)(Y−
ABX)
|, we first have the decomposition180 6 The Growth Curve Model and Reduced-Rank Regression Methods
(Y − ABX)(Y − ABX)
 = (Y − ABX)[I − X
(XX
)
−1X](Y − ABX)

+ (Y − ABX)X
(XX
)
−1X(Y − ABX)

= (Y − C
X)(Y − C
X)

+ (C
 − AB)XX
(C
 − AB)
= S + (C
 − AB)XX
(C
 − AB)
,
where S = Y[I − X
(XX
)−1X]Y = (Y − C
X)(Y − C
X)
. So we obtain
|(Y − ABX)(Y − ABX)

|=|S| |Im + S−1(C
 − AB)XX
(C
 − AB)
|
= |S| |In + XX
(C
 − AB)
S−1(C
 − AB)|
= |S| |In + xx (C
 − AB)
−1
 (C
 − AB)|,
where xx = (1/T )XX
. We can also write
(C
 − AB)
−1
 (C
 − AB) = C

[−1
 − −1
 A(A
−1
 A)−1A
−1
 ]C

+ (C
 − AB)
−1
 A(A
−1
 A)−1A
−1
 (C
 − AB)
= (C
 − AB)  
−1
 (C
 − AB) 
+ (B
− B)
(A
−1
 A)(B
− B)
≡  + (B
 − B)
(A
−1
 A)(B
− B),
where B
 = (A
−1  A)−1A
−1  C
, and  = (C
 − AB)  
−1  (C
 − AB)  . Thus we
find that
|(Y − ABX)(Y − ABX)

|=|S| |In + xx| |In + (In + xx)
−1xx
× (B
− B)
(A
−1
 A)(B
− B)|
= |S| |In + xx| |In + (−1
xx + )
−1
× (B
− B)
(A
−1
 A)(B
− B)|.
Let δ2
i ≥ 0, i = 1,...,n, be the eigenvalues of
(−1
xx + )
−1(B
− B)
(A
−1
 A)(B
− B),
so6.2 Estimation of Parameters in the Growth Curve Model 181
|(Y − ABX)(Y − ABX)

|=|S| |In + xx|
+n
i=1
(1 + δ2
i ) ≥ |S| |In + xx|
with equality (under variation of B) if and only if δ2
1 = ··· = δ2
n = 0, that is,
if and only if A(B
 − B) = 0. Since A is a full-rank matrix, this implies that the
value of B given by B
 above yields the minimum, so B
 = (A
−1  A)−1A
−1  C
 ≡
(A
S−1A)−1A
S−1C
 is the ML estimator of B.
We briefly indicate a few simplifications that occur in the estimation for the
special case mentioned in Section 6.1 where the across-individual design matrix
X is a matrix of indicator variables corresponding to n different treatment groups,
that is, X = diag(1
T1
, 1
T2
,..., 1
Tn
). Then we have XX = diag(T1,...,Tn) and
C
 = YX
(XX
)
−1 = [Y¯
1,..., Y¯
n] ≡ Y ,¯
where Y¯
i = Ti
j=1 Yij /Ti is the sample mean vector for the ith group, i = 1,...,n.
Hence, the ML estimator of B is B
 = (A
−1  A)−1A
−1  Y¯, where  = (1/T )S.
In addition,
S = n
i=1

Ti
j=1
(Yij − Y¯
i)(Yij − Y¯
i)

is the usual “within-group” error sum of squares matrix, and from (6.3), the ML
estimate of the error covariance matrix  is  = (1/T )S+(1/T )n
i=1 Ti(Y¯
i −
Aβ
i)(Y¯
i − Aβ
i)
, where β
i = (A
−1  A)−1A
−1  Y¯
i is the ith column of B
.
In the original study of the growth curve model (6.1) by Potthoff and Roy
(1964), the following reduction of the model was adopted for the analysis. For an
arbitrary m × m positive-definite matrix , the equation in (6.1) is multiplied by
(A
A)−1A
 to obtain
(A
A)−1A
Yk = BXk + (A
A)−1A
k.
This can be written as
Y ∗
k = BXk + ∗
k , k = 1,...,T, (6.4)
which is in the form of the classical multivariate regression model. Therefore,
from the classical regression framework, the LS estimator of B in (6.4) is B
 =
Y∗X
(XX
)−1 = (A
A)−1A
C
, where Y∗ = (A
A)−1A
Y. The optimum
choice for  is  = −1  , which will yield the best linear unbiased estimator of
B in model (6.1). With  unknown, we see from the previous derivations that
the ML estimator B
 in (6.2) has the same form as B
 using  = −1  . Notice also
that if the known matrix A was “normalized” so that the columns of V = 1/2A
are orthonormal, that is, A
A = V 
V = Ir, then the above estimator B
 takes the182 6 The Growth Curve Model and Reduced-Rank Regression Methods
similar form as in (2.17) of Section 2.3 for the reduced-rank model, B
 = V 
1/2C
,
but with the known matrix V = 1/2A in place of an estimated matrix V
. In
addition, for both the reduced-rank and the growth curve model, we have found
that the ML estimator is of the same form and is obtained with the choice  = −1  .
Finally, we mention that if the error covariance matrix  has a special “random
effects” structure such as  = AβA +σ2Im, as discussed in Section 6.1, then it
follows (e.g., see Reinsel 1985) that the ML estimator of B under this model reduces
to the LS estimator B
 = (A
A)−1A
YX
(XX
)−1.
To further interpret the ML estimator of B and examine its statistical properties,
we can also consider a transformation of the model (6.1) to a conditional model.
Let A1 = A(A
A)−1 and A2 be m × r and m × (m − r) matrices, respectively, of
full ranks such that A
1A = Ir and A
2A = 0, which also imply that A
2A1 = 0.
The choice of A2 as the column basis of Im − A(A
A)−1A will satisfy the above
conditions. Then let Y1k = A
1Yk and Y2k = A
2Yk, so that the original model (6.1)
can be transformed into

Y1k
Y2k

=

BXk
0

+

A
1k
A
2k

. (6.5)
The form (6.5) indicates that the information on B is contained within the
relationship between the variables Y1k and Xk and the transformed responses Y2k,
which are referred to as the set of covariates, have mean values that do not involve
B, but Y2k may be correlated with Y1k. The conditional model for Y1k, given Y2k,
which follows from (6.5), can be written as
Y1k = BXk + Y2k + ∗
1k, (6.6)
where  = Cov(Y1k, Y2k){Cov(Y2k)}−1 = (A
1A2)(A
2A2)−1 and the r￾dimensional error term ∗
1k has zero mean vector and covariance matrix ∗
1 =
(A
1A1) − (A
1A2)(A
2A2)−1(A
2A1) ≡ (A
−1  A)−1. The last
equality follows from a matrix result presented in Lemma 2b of Rao (1967). The
(conditional) model (6.6) is again in the form of the classical regression set up with
two sets of predictors. From the discussion in Sections 1.3 and 3.1, it follows that
the LS and ML estimators of B in (6.6) are B
 = y1x.y2−1
xx.y2 , where y1x.y2 and
xx.y2 are sample partial covariance matrices adjusted for Y2k. It will be established
below that this estimator can be expressed equivalently in the form in (6.2), namely
B
 = (A
−1  A)−1A
−1  C
 with C
 = yx−1 xx . Because the marginal distribution
of Y2k does not involve B, it follows that the ML estimator of B from the conditional
model (6.6) is also the ML estimator for the unconditional model (6.1), as has
already been established.
To verify the equivalence in the two forms of the estimator B
, we first note that
the LS estimator of  in (6.6) is obtained as
 = y1y2.x−1
y2y2.x = (A
1yy.xA2)(A
2yy.xA2)
−16.2 Estimation of Parameters in the Growth Curve Model 183
≡ (A
1A2)(A
2A2)
−1, (6.7)
where yy.x = yy − yx−1 xx xy ≡  . It then follows, from the normal
equations for LS estimation, that the LS estimator of B in (6.6), B
 = y1x.y2−1
xx.y2 ,
can be equivalently expressed as
B
 = (y1x − y2x )−1
xx = [A
1yx − (A
1A2)(A
2A2)
−1A
2yx ]−1
xx
= [A
1 − (A
1A2)(A
2A2)
−1A
2 ]−1
 yx−1
xx
= (A
1A1)(A
1−1
 A1)
−1A
1−1
 yx−1
xx ≡ (A
−1
 A)−1A
−1
 C,  (6.8)
where C
 = yx−1 xx , and the result in Lemma 2b of Rao (1967) has again been
used. Thus the LS estimator of B from the conditional model approach of (6.6) is
the ML estimator of B in model (6.1) and can be expressed in the equivalent form
as in (6.2).
We note from (6.8) that the ML estimator of B in model (6.1) can be expressed
as
B
 = A
1yx−1
xx − A 
2yx−1
xx
= (A
A)−1A
YX
(XX
)
−1 − A 
2YX
(XX
)
−1, (6.9)
with  = (A
1A2)(A
2A2)−1. In this form, B
 is interpretable as a
covariance adjusted estimator of the simple LS estimator for model (6.1),
B
 = (A
A)−1A
YX
(XX
)−1, where the adjustment involves use of the (mean￾zero) covariates Y2 = A
2Y through their (estimated) regression relationship with
Y1 = A
1Y = (A
A)−1A
Y. The interpretation and development of properties
of the estimator B
 through the covariance adjustment point of view is examined
in more detail by Rao (1967) and Grizzle and Allen (1969). This includes the
notion of constructing estimators of B through covariance adjustment as in (6.9),
but using only a subset of the covariates in Y2 = A
2Y, whose proper choice
would depend on knowledge of special structure for the covariance matrix  .
Specifically, covariates having zero (or very small) covariance with the response
variables Y1 = A
1Y and with the remaining covariates in Y2 = A
2Y would be
omitted in (6.9), the idea being that the use of “irrelevant” covariates in (6.9) would
result in an increase in variability of the estimator B
, and hence, such covariates
should be eliminated in the estimation of B.
Notice that an alternate way to view the growth curve model (6.1) is as having
the form of the standard multivariate regression model Yk = CXk + k, with the
regression coefficient matrix C ≡ AB constrained or specified to satisfy the linear
restriction A
2C = 0 (since A
2A = 0). From the result in (1.18) of Section 1.3 (with
F1 ≡ A
2 and G2 ≡ In), it follows that the constrained ML estimator of C under
A
2C = 0 can be written as184 6 The Growth Curve Model and Reduced-Rank Regression Methods
C
 = C
 − SA2(A
2SA2)
−1A
2C, 
where C
 = YX
(XX
)−1 is the usual LS estimator of C. Hence, because in the
growth curve model, C is represented in the form C = AB with A known, this
implies that the ML estimator of B can be obtained from C (  ≡ AB)  as
B
 = (A
A)−1A
C
 = (A
A)−1A
C
 − (A
A)−1A
SA2(A
2SA2)
−1A
2C, 
which is identical to the expression previously obtained in (6.9), recalling that A
1 =
(A
A)−1A
.
Under normality of the k, the ML estimator B
given by (6.2) or (6.8) can be seen
to be unbiased, i.e., E(B)  = B, using the property that C
 and  are independently
distributed and E(C)  = C ≡ AB. The independence property can also be used to
help establish that the covariance matrix of B
 is given by
Cov[vec(B)  ] =
T − n − 1
T − n − 1 − (m − r)(XX
)
−1 ⊗ (A
−1
 A)−1,
and an unbiased estimator of (A
−1  A)−1 is T
T −n−(m−r)(A
−1  A)−1. (For exam￾ple, see Rao (1967) and Grizzle and Allen (1969) for details.) Although the first
two moments of B
 are known, the complete distribution is not available in simple
form. Gleser and Olkin (1966, 1970) and Kabe (1975) obtained expressions for the
probability density of the ML estimator B
.
6.3 Likelihood Ratio Testing of Linear Hypotheses in Growth
Curve Model
We now present and derive the LR test statistic for H0 : F1BG2 = 0, where F1
and G2 are known full-rank matrices of dimensions r1 × r and n × n2, respectively,
with r1 ≤ r and n2 ≤ n. We also want to give the associated asymptotic distribution
theory for the LR statistic. These results can be obtained from testing results for the
classical multivariate linear regression model, as given in Section 1.3, applied to the
conditional model (6.6). The hypothesis H0 : F1BG2 = 0 for model (6.1) can be
expressed in terms of the conditional model (6.6) as
H0 : F1[B,]

G2
0

≡ F1DG∗
2 = 0,
where D = [B,] and G∗
2 = [G
2, 0
]

. Note that the conditional model (6.6)
can be written as Y1k = [B,]X∗
k + ∗
1k = DX∗
k + ∗
1k, where X∗
k = [X
k, Y 
2k]

,
which is in the framework of the classical multivariate regression model (1.1). So
H0 : F1DG∗
2 = 0 can be tested using the results given in Section 1.3.6.3 Likelihood Ratio Testing of Linear Hypotheses in Growth Curve Model 185
The unconstrained ML estimator of D is the LS estimator
D = Y1X∗(X∗X∗)
−1,
where X∗ = [X
, Y
2], which yields the estimators B
and  discussed in Section 6.2.
That is, D = [B,  ] where B
 and  are the (LS) estimators given in (6.7)
and (6.8) of Section 6.2. The constrained ML estimator of D, and hence of B,
under H0 : F1BG2 ≡ F1DG∗
2 = 0 can be obtained from the expression in
(1.18) of Section 1.3 with the appropriate changes in notation. The error sum
of square matrix from (unconstrained) LS estimation of the conditional model is
S∗
1 = Y1[I − X∗(X∗X∗)−1X∗]Y
1, where Y1 = A
1Y. However, notice that
Y1[I − X∗(X∗X∗)
−1X∗] = Y1 − DX∗ = A
1Y − B
X − Y2
= [A
1 − (A
1SA2)(A
2SA2)
−1A
2]Y − B
X
= (A
S−1A)−1A
S−1Y
− (A
S−1A)−1A
S−1YX
(XX
)
−1X
= (A
S−1A)−1A
S−1Y[I − X
(XX
)
−1X],
again using the result from Lemma 2b of Rao (1967) in the same way as in (6.8).
Hence, the error sum of squares matrix can be expressed as
S∗
1 = Y1[I − X∗(X∗X∗)
−1X∗]Y
1
= (A
S−1A)−1A
S−1Y[I − X
(XX
)
−1X]Y
A1
= (A
S−1A)−1A
S−1SA1 = (A
S−1A)−1, (6.10)
recalling that A
A1 = Ir. The hypothesis sum of squares matrix associated with
H0 : F1BG2 ≡ F1DG∗
2 = 0 is, from expression (1.19) of Section 1.3, given by
H = S∗
1F
1(F1S∗
1F
1)
−1(F1DG ∗
2)[G∗
2 (X∗X∗)
−1G∗
2]
−1
× (F1DG ∗
2)

(F1S∗
1F
1)
−1F1S∗
1
= S∗
1F
1(F1S∗
1F
1)
−1(F1BG 2)[G
2G2]
−1
× (F1BG 2)

(F1S∗
1F
1)
−1F1S∗
1 , (6.11)
where  = [In, 0](X∗X∗)−1[In, 0]
 is the leading n × n submatrix of (X∗X∗)−1,
from which it is known that
 = {XX − XY
2(Y2Y
2)
−1Y2X
}
−1 ≡ (1/T )−1
xx.y2 .186 6 The Growth Curve Model and Reduced-Rank Regression Methods
In addition, from a standard result on the inverse of a partitioned matrix (e.g., see
Rao 1973, p. 33), with E = Y2Y
2 −Y2X
(XX
)−1XY
2 ≡ A
2SA2, we also find that
 = (XX
)
−1 + (XX
)
−1XY
2E−1Y2X
(XX
)
−1
= (XX
)
−1 + (XX
)
−1XY
A2(A
2SA2)
−1A
2YX
(XX
)
−1
= (XX
)
−1 + (XX
)
−1XY
× [S−1 − S−1A(A
S−1A)−1A
S−1]YX
(XX
)
−1
= (XX
)
−1 + (XX
)
−1XY
S−1YX
(XX
)
−1
− B

(A
S−1A)B,  (6.12)
using the matrix result from Lemma 1 of Khatri (1966) that
A2(A
2SA2)
−1A
2 = S−1 − S−1A(A
S−1A)−1A
S−1
since A
2A = 0.
As in Section 1.3, the LR test of H0 is based on the ordered eigenvalues λ2
1 >
··· > λ2
r1 of the r1 × r1 matrix S−1 ∗ H∗, where
H∗ = F1H F
1 = (F1BG 2)[G
2G2]
−1(F1BG 2)
 (6.13)
and S∗ = F1S∗
1F
1 = F1(A
S−1A)−1F
1. The LR test statistic is
M = −[T − n − (m − r) +
1
2
(n2 − r1 − 1)] log(U )
≡ [T − n − (m − r) +
1
2
(n2 − r1 − 1)]
r1
i=1
log(1 +λ2
i ),
which is asymptotically distributed as χ2
r1n2 , where U−1 = |I + S−1 ∗ H∗| =
r1
i=1(1+λ2
i ). The distribution of S∗ is Wishart, W (F1∗
1F
1, T −n−(m−r)), and
under H0, the conditional distribution of H∗ is also Wishart with matrix F1∗
1F
1
and n2 degrees of freedom, where ∗
1 = (A
−1  A)−1. Since this conditional null
distribution for H∗ does not depend on X∗ (i.e., on Y2), it is also the unconditional
null distribution of H∗. The above form of the LR test of H0 : F1BG2 = 0 was first
derived by Khatri (1966). Some additional aspects of the testing procedure were
further examined by Grizzle and Allen (1969).
Recall that in the growth curve model (6.1), A represents the design matrix of
the within-individual model and X is the usual across-individual design matrix.
Various choices of the matrices F1 and G2 in H0 : F1BG2 = 0 lead to special
cases of tests of hypothesis of particular interest. These include testing for the
order of polynomial in the within-individual model and testing for homogeneity or6.3 Likelihood Ratio Testing of Linear Hypotheses in Growth Curve Model 187
similarity in regression coefficients across different treatment groups. For instance,
the within-individual model is often represented in terms of a polynomial regression
over time points, of degree r −1, and suppose we wish to test for lower degree r −2
versus r − 1. That is, we test for the significance of the highest order coefficient
of the within-individual model, over all individuals. This can be represented as
H0 : β
(r) = 0, where β
(r) denotes the rth row of B, and this is equivalent to
H0 : F1BG2 = 0 with F1 = (0,..., 0, 1) and G2 = In. For this, we see from
(6.13) that H∗ = (F1B)  −1(F1B)   = β

(r)−1β
(r), where β

(r) is the rth row of
B
, and S∗ = F1(A
S−1A)−1F
1 is the rth diagonal element of (A
S−1A)−1. Hence,
since r1 = 1, the LR test is based on the scalar H∗/S∗ = β

(r)−1β
(r)/S∗, which is
distributed as n/(T −n−(m−r)) times the F-distribution with n and T −n−(m−r)
degrees of freedom under H0.
For another example, suppose that the across-individual design matrix X cor￾responds to n distinct treatment groups as in a one-way multivariate ANOVA
model, and we wish to test the null hypothesis of homogeneity of the growth curve
parameters (or at least of a subset of these parameters) over the n groups. This is
equivalent to the condition that the n columns of B be equal (or at least the lower
r1 × n submatrix of B has equal columns). This hypothesis can be expressed in the
form F1BG2 = 0 with F1 = [0, Ir1 ] and
G2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
1 0 · · 0 0
0 1 · · 0 0
· ··· · ·
· ··· · ·
0 0 · · 0 1
−1 −1 ··−1 −1
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
of dimension n × (n − 1). For example, in the case with only n = 2 groups, we
have G2 = (1, −1) and G
2G2 = (1/T1) + (1/T2) + (1/T ){Y¯
1 − Y¯
2 − A(β
1 −
β
2)}
−1  {Y¯
1−Y¯
2−A(β
1−β
2)}, and with r1 = r, the test statistic for H0 : BG2 = 0
(i.e., β1 − β2 = 0, where B = [β1, β2]) is based on the single nonzero eigenvalue
of S−1 ∗ H∗, that is,
(BG 2)

(A
S−1A)(BG 2)[G
2G2]
−1 = 1
T (β
1 − β
2)

A
−1
 A(β
1 − β
2)/[G
2G2].
Also notice that a test of goodness-of-fit or adequacy of specification of the
within-individual model can be tested in the framework of the general multivariate
linear model Yk = CXk + k, with C specified to be of the form C = AB under the
growth curve model (6.1). The LR test can be carried out using the standard methods
discussed in Section 1.3 as a test of H0 : A
2C = 0 in the model Yk = CXk + k,
since this hypothesis is equivalent to the hypothesis that C has the form AB with
A specified (and A2 is the m × (m − r) matrix such that A
2A = 0). Thus, from
Section 1.3, we find that the hypothesis sum of squares matrix for H0 is188 6 The Growth Curve Model and Reduced-Rank Regression Methods
H∗ = A
2C(  XX
)C

A2 = A
2YX
(XX
)
−1XY
A2,
while the error sum of squares matrix is S∗ = A
2SA2, with S∗ + H∗ = A
2YY
A2.
The LR test procedure uses the statistic M = −[T −n+(n−(m−r)−1)/2] log(U ),
where U = |S∗|/|S∗ + H∗| = 1/|Im−r + S−1 ∗ H∗|, with M having an approximate
χ2
n(m−r) distribution under H0. The LR can also be expressed in a form that avoids
the explicit use of the matrix A2. Specifically, from (6.3), we see that the m × m
error sum of squares matrix under H0 is
S1 ≡ S + H = (Y − AB
X)(Y − AB
X)

= S + (Im − A(A
S−1A)−1A
S−1)YX
(XX
)
−1XY
× (Im − A(A
S−1A)−1A
S−1)

.
Therefore, it follows that
U = |S|/|S1| = 1/|Im + S−1H|
= 1/|Im + S−1(Im − A(A
S−1A)−1A
)S−1YX
(XX
)
−1XY
|,
so that U can be expressed in terms of the (m − r) nonzero eigenvalues of
the matrix S−1(Im − A(A
S−1A)−1A
)S−1YX
(XX
)−1XY
. It is also shown by
Kshirsagar and Smith (1995, p. 194–195) that U can be expressed equivalently as
U = {|S||A
S−1A|}/{|YY
||A
(YY
)−1A|}.
6.4 An Extended Model for Growth Curve Data
An extension of the basic growth curve (or generalized MANOVA) model (6.1) to
allow a mixture of growth curve and MANOVA components has been considered
by Chinchilli and Elswick (1985). In this extended model, the mean of the response
vector Yk is represented by the sum of two components, a growth curve portion
and a standard MANOVA portion. The model could be viewed as an analysis
of covariance model that adjusts the growth curve structure for the influence of
additional covariates for the response vector. Specifically, the model considered is
Yk = ABXk + DZk + k, k = 1,...,T, (6.14)
where the m × 1 response vector Yk depends on the n1 × 1 vector Xk in the same
way as in the basic growth curve model (6.1) and also on the n2 × 1 vector Zk of
additional covariates. However, the second set of covariates is not specified to enter
the model with growth curve structure but simply with the standard multivariate
linear model structure. This model may have appeal when it seems appropriate to6.4 An Extended Model for Growth Curve Data 189
express the effects of Xk in terms of growth curve structure over time, whereas it
may not be desirable to express the effects of the additional covariates Zk in this
manner. Notice that the form of model (6.14) is similar to the reduced-rank model
(3.3) of Section 3.1, but in the present growth curve model context, the matrix A
is known. As usual, the error terms k are assumed to follow a multivariate normal
distribution with zero mean vector and positive-definite covariance matrix  .
The maximum likelihood estimators of the unknown parameters B, D, and 
of model (6.14) are derived by Chinchilli and Elswick (1985). We can use some
of the tools that have been developed in Sections 3.1 and 6.2 to readily present
the results. Let C = AB and denote the LS estimates of C and D in the model
Yk = CXk + DZk + k by C
 = yx.z−1 xx.z and D = yz.x−1 zz.x , respectively.
The ML estimates of model (6.14) are obtained by minimizing |W| or, equivalently,
|−1  W|, where
W = (1/T )(Y − ABX − DZ)(Y − ABX − DZ)

and  = (1/T )(Y − C
X − DZ)(Y − C
X − DZ) ≡ yy.(x,z) as given in (3.10).
As shown in Section 3.1, we have the decomposition
W =  + (C
 − AB)xx.z(C
 − AB)
+ (D∗ − D − Cxz−1
zz )zz(D∗ − D − Cxz−1
zz )

,
where D∗ = D + C
xz−1 zz ≡ yz−1 zz . For any C, the last component above can
be uniformly minimized, as the zero matrix, by the choice of D = D∗ −Cxz−1 zz .
Thus, the ML estimation of minimizing |−1  W| is reduced to minimizing
|Im + −1
 (C
 − AB)xx.z(C
 − AB)
|
with respect to B. This is the same form of minimization problem as considered
at the beginning of Section 6.2 for the basic growth curve model. Hence, from the
same arguments as given there, it follows that B
 = (A
−1  A)−1A
−1  C
, with
C
 = yx.z−1 xx.z, is the ML estimator of B in model (6.14). The corresponding ML
estimator of D then is
D = D∗ − AB
xz−1
zz = yz−1
zz − AB
xz−1
zz
≡ D + (C
 − AB)  xz−1
zz . (6.15)
It follows from the above decomposition that the ML estimate of the error
covariance matrix is
 =  + (C
 − AB)  xx.z(C
 − AB)  
=  + (Im − A(A
−1
 A)−1A
−1
 )C
xx.zC
190 6 The Growth Curve Model and Reduced-Rank Regression Methods
× (Im − A(A
−1
 A)−1A
−1
 )

.
As can be observed from the above results, the main differences between ML
estimates of the parameters of the basic growth curve model (6.1) and ML estimates
of the parameters of the extended model (6.14) are that the estimate  of the error
covariance matrix in (6.14) is obtained from the LS regression of Yk on Xk and Zk
and the estimate of B involves the LS regression coefficient matrix C
 = yx.z−1 xx.z
that is obtained after adjusting for the effect of Zk on Yk and Xk.
It is also possible to develop the conditional model approach for (6.14) in a
similar way to model (6.6) for the basic growth curve model. Hypothesis testing
under the extended model (6.14) can be developed along the same lines as in
Section 6.3 for the model (6.1). In addition to the adjustments that were mentioned
at the end of the last paragraph, the LR tests will involve an adjustment in the error
degrees of freedom associated with the inclusion of the additional covariates Zk in
the model. We will not pursue the details for LR testing of linear hypotheses here.
However, we do note that there are two separate tests concerning the issue of model
adequacy that might be of interest for the extended model (6.14). The first type of
test of hypothesis is similar to that mentioned at the end of Section 6.3, namely,
a test of the adequacy of the growth curve feature (involving the term ABXk)
in the extended model (6.14) relative to a standard multivariate regression model
Yk = CXk+DZk+k. The second type of test of interest would be the adequacy of a
“combined” growth curve structure model of the form Yk = A(BXk+B∗Zk)+k ≡
A[B,B∗][X
k, Z
k]
 + k relative to the extended model (6.14) that is a “mixture” of
growth curve structure and standard multivariate regression.
Verbyla and Venables (1988) examined another extension of growth curve model
(6.1) that allows for the sum of two (or more) growth curve or time profile model
structures. For the sum of two component structures, the model has the form
Yk = ABXk + A∗B∗Zk + vk, where A and A∗ are known matrices specifying
the growth curve or time profile structures of the coefficients for the two sets
of regressors Xk and Zk. This type of model can accommodate features such as
allowance for different forms of growth profile for different sets of regressors (e.g.,
different forms for treatment design variables compared to measured covariates)
and time-varying covariates (i.e., different covariate values at the different response
times). An important special case occurs when the columns of A represent a subset
of or are “nested” within the column space of A∗, such as when a lower-degree
polynomial structure is specified for the coefficients of Xk than for those of Zk.
The extended model (6.14) of Chinchilli and Elswick (1985) can be seen to be a
particular case of this situation with A∗ = Im. The ML estimation of these sum
of profiles models in general requires iterative methods, although explicit closed￾form estimation results can be obtained in special cases such as the nested model
situation. We remark that the reduced-rank structures could also be considered for
the parameter matrices B and B∗ in the general sum of profiles setting. This could
be viewed as an extension of the model in Chapter 3 and Velu (1991). A special6.5 Modification of Basic Growth Curve Model to Reduced-Rank Model 191
case of the reduced-rank growth curve model setting for two sets of regressors, with
A = A∗ and B of reduced rank, will be considered later.
6.5 Modification of Basic Growth Curve Model to
Reduced-Rank Model
In order to relate the concepts in the growth curve model context to the reduced-rank
modeling of Chapter 2, we need to recall the role of the matrix components A and
B that appear in the basic growth curve model (6.1). As emphasized previously, the
m × r matrix A is known or specified in the growth curve context and is taken to be
the design matrix of the within-individual model. Because the elements of Yk consist
of repeated measurements of the same response variable over time, the columns of
A are specified as known functions of time, typically polynomial terms. The r × n
matrix B represents the regression coefficients associated with the known functions
of time. The n × T matrix X is the usual across-individual design matrix. When
the individuals are from n different groups and there are no additional explanatory
variables, then X is simply a matrix of indicator variables as in the one-way ANOVA
model.
To focus further on this specific situation, suppose there are n different exper￾imental or treatment groups and Ti vector observations Yij are available for the
ith group, for j = 1,...,Ti. With observations in each group assumed to
be measured at the same m time points, t1 < t2 < ··· < tm, the basic
(polynomial) growth curve model can be written as Y = ABX + , where Y =
[Y11,...,Y1T1 ,...,Yn1,...,YnTn ],
A =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
1 t1 · · t
r−1
1
1 t2 · · t
r−1
2
· · ·· ·
· · ·· ·
1 tm · · tr−1 m
⎤
⎥
⎥
⎥
⎥
⎥
⎦
, B =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
β11 ··· β1n
β21 ··· β2n
· ··· ·
· ··· ·
βr1 ··· βrn
⎤
⎥
⎥
⎥
⎥
⎥
⎦
,
X =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
1
T1 0 0 · · 0
0 1
T2 0 · · 0
· · · ·· ·
· · · ·· ·
0 0 0 · · 1
Tn
⎤
⎥
⎥
⎥
⎥
⎥
⎦
,
and 1T denotes a T × 1 vector of ones.
In Section 6.3, we indicated how various forms of hypotheses about the matrix
B might be of interest for the analysis of the growth curve model. The degree r − 1
of the polynomial and the number n of different treatment groups both play roles
in determining the number of parameters (= rn) to be estimated and compared in192 6 The Growth Curve Model and Reduced-Rank Regression Methods
the growth curve analysis. If the growth curves could be adequately expressed by a
lower-degree polynomial and if the number of distinct groupings could be reduced
on the basis of similarity of growth curve features, a more parsimonious model
would emerge. In practice, the choice of degree r −1 might be made initially on the
basis of plots of mean growth curves of each of the n groups and would subsequently
be more formally chosen on the basis of goodness-of-fit LR tests as discussed in
Section 6.3. However, in some situations, a relatively low degree polynomial may
not be adequate to represent the growth curve patterns over time. Equivalently, we
may not want to specify the mathematical form of the growth curve function over
time as a specific degree polynomial or any other specified function of time. For
such situations, in this section, we explore how the reduced-rank regression model
and methods could be applied to build a parsimonious model.
In the reduced-rank regression model framework, Y = ABX + , of Chapter 2,
both matrices A and B are unknown, and the rank restrictions on C = AB are meant
to imply certain consequences. When considered for use in the growth curve data
setting, the roles of the component matrices A and B cannot be viewed in exactly the
same way as the matrices A and B in the basic growth curve model (6.1). However,
certain similarities and correspondences do exist. For the reduced-rank model, the
m × r matrix A is unknown, but the r columns of A could be viewed as a (low￾dimensional) set of basis curves over time (with general unspecified forms) such
that each group’s growth curve over time can be represented adequately by some
linear combination of the basis set of curves. The coefficients in the ith column of
the r × n matrix B then correspond to the coefficients of the linear combination of
basis curves for the ith treatment group. The across-individual model could also be
parameterized in terms of an overall mean growth curve and n − 1 among-group
contrasts, with a corresponding design matrix X whose first row has all elements
equal to one. That is, for example, we can take
X =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
1
T1 1
T2 1
T3 · · 1
Tn−1 1
Tn
1
T 1 0 0 · · 0 −1
Tn
0 1
T2 0 · · 0 −1
Tn
· · · ·· · ·
· · · ·· · ·
0 0 0 · · 1
Tn−1 −1
Tn
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
with B = [β¯, β1 − β¯,..., βn−1 − β¯], where βi denotes the ith column of the
original matrix B, that is, the r × 1 vector of growth curve parameters for the ith
group, and β¯ = n−1 n
i=1 βi. Then in this parameterization, the first column of AB
gives the general mean growth curve over all groups, and the remaining columns
provide for various deviations or contrasts of the group means from the overall mean
growth curve. Thus, the methodology of the reduced-rank regression modeling of
Chapter 2 not only seeks for a basic set of curves over time, and the associated
coefficients, it also searches for possible reductions in the number of basic curves
required for adequate representation of all the groups’ growth curves. However, it6.5 Modification of Basic Growth Curve Model to Reduced-Rank Model 193
does not seem that the unknown matrix A in the reduced-rank model should be
viewed as a substitute for the known matrix A of polynomial terms in the basic
growth curve model. The use of the reduced-rank modeling methodology might be
considered when specification of the exact mathematical form of the growth curves,
as polynomials of a certain degree, is not desirable or adequate, as mentioned.
We now recall the fundamental results from Chapter 2 concerning estimation of
the reduced-rank regression model Yk = ABXk + k, k = 1,...,T , for application
to the growth curve data situation. The ML estimators of the component matrices A
and B are as given in Section 2.3,
A
= 1/2  V ,  B
 = V

−1/2  yx−1
xx , (6.16)
where V
 = [V
1,..., V
r] and V
j is the normalized eigenvector corresponding to
the j th largest eigenvalueλ2
j of the matrix −1/2  yx−1 xx xy−1/2  . The estimates
satisfy the normalizations A

−1  A
 = Ir and B
xxB
 = 2. The ML estimate of
the error covariance matrix  can be expressed as
 =  + (C
 − C)  xx (C
 − C)  
,
where C
 = A
B
 and C
 = yx−1 xx . A “goodness-of-fit” test for the adequacy of
the model with reduced rank r, that is, of H0 : rank(C) = r against the general
alternative of a full-rank regression matrix C is provided by the LR test statistic
given in Section 2.6,
M = −[T − n + (n − m − 1)/2] m
j=r+1
log(1 − ρ2
j ),
where ρ2
1 > ···≥ ρ2
m ≥ 0 are the ordered eigenvalues of the matrix
−1/2
yy yx−1
xx xy−1/2
yy .
The asymptotic distribution of the ML estimators A
 and B
 of the reduced-rank
component matrices is given in Theorem 2.4 of Section 2.5. For the growth curve
setting, one may also want to consider the extended version of the reduced-rank
model as discussed in Section 3.1, Yk = ABXk + DZk + k, where Zk represents
a portion or subset of the total set of explanatory variables that are not postulated to
enter the model with a growth curve or reduced-rank structure. As noted earlier,
this model is in the same spirit as the extended growth curve model discussed
in Section 6.4. For example, in the ANOVA situation with n treatment groups, it
may be desired to separate the overall mean response curve from the growth curve
structure and incorporate growth curve structure only for the remaining n−1 among￾group contrasts. This corresponds to a case with Zk ≡ 1 for all k, and Xk are
(n − 1)-dimensional vectors of group indicators.194 6 The Growth Curve Model and Reduced-Rank Regression Methods
6.6 Reduced-Rank Growth Curve Models
For longitudinal or “growth curve” data Yk, as we have seen in earlier parts of this
chapter, it is often appropriate to specify a growth curve model structure,
Yk = ABXk + k, k = 1,...,T,
where A is a known or specified m × r matrix and the r × n matrix B is unknown.
The matrix B plays the role of summarizing the mean structure of the response
variables over time (i.e., the growth curve structure), typically in the form of
regression coefficients of polynomial functions over time. For example, BXk can
be viewed as the mean vector of the r-dimensional summary of the response vector,
A
1Yk = (A
A)−1A
Yk, and the “complete” response mean vector can be expressed
in terms of this through E(Yk) = A(BXk). In this way, the mean vectors of the
Yk are specified to have a reduced-rank structure of a specified form. However, the
r × n regression coefficient matrix B, which summarizes the mean vector structure,
could itself also have reduced rank. Thus, in this section, we consider the possibility
of reduced-rank structure for B in the basic growth curve model and consider the
assumption that rank(B) = r1 < min(r, n). As in previous chapters, this assumption
of reduced rank yields the matrix decomposition as
B = B1B2,
where B1 and B2 are r×r1 and r1×n matrices, respectively, of unknown parameters.
Therefore, the reduced-rank growth curve model can be written as
Yk = AB1B2Xk + k, k = 1,...,T. (6.17)
The reduced rank of B implies that the vectors of growth curve parameters exist
in r1-dimensional space. The r1 columns of B1 provide a basis for this space; the
elements in the ith column of B2 provide the coefficients in the linear combination
of basis growth curve parameters for the ith treatment group, in the n-group
ANOVA situation, for example. The model also retains the usual reduced-dimension
interpretation that the r1×1 vector of linear combinations, X∗
k = B2Xk, is sufficient
to represent the linear relationships between Yk and Xk. The analysis of model
(6.17), including estimation of the unknown parameters B1, B2, and  , and
extensions and applications of this model, will be the focus of this section.
To derive the ML estimates of B1 and B2, we will again utilize the estimation
results of Sections 6.2 and 2.3. The ML estimates are obtained by minimizing |(Y−
AB1B2X)(Y − AB1B2X)
| with respect to B1 and B2, where A is a known matrix.
Following the derivation given in Section 6.2 for the ML estimate of B in the model
(6.1) where B is assumed to be of full rank, we obtain the relation6.6 Reduced-Rank Growth Curve Models 195
|(Y − AB1B2X)(Y − AB1B2X)

|
= |S||In + xx| |In + (−1
xx + )
−1(B
 − B)
(A
−1
 A)(B
− B)|
= |S| |In + xx|
× |Ir + (A
−1
 A)(B
− B1B2)(−1
xx + )
−1(B
− B1B2)

|, (6.18)
where the “full-rank” ML estimate of B is now denoted as
B
 = (A
−1
 A)−1A
−1
 C

with C
 = YX
(XX
)−1, and  = (C
 − AB)  
−1  (C
 − AB)  . Thus, ML estimation
for B1 and B2 is equivalent to minimizing the last term on the right side of
(6.18). Note that this term has the same form as the quantity involved in the
expressions following (2.17) in Section 2.3 and that the minimization is achieved
by simultaneous minimization, with respect to B1 and B2, of all the eigenvalues of
the matrix
(A
−1
 A)1/2(B
− B1B2)(−1
xx + )
−1(B
− B1B2)

(A
−1
 A)1/2, (6.19)
where  = (1/T )S. The matrix in (6.19) can be expressed as (S(r) − P(r))(S(r) −
P(r))
, where
S(r) = (A
−1
 A)1/2B(  −1
xx + )
−1/2
= (A
−1
 A)−1/2A
−1
 C(  −1
xx + )
−1/2
and P(r) = (A
−1  A)1/2B1B2(−1 xx + )−1/2. From the same arguments as in
Section 2.3, using the result of Lemma 2.2 in particular, the minimizing matrix P(r)
is chosen as the rank-r1 approximation of S(r) obtained through the singular value
decomposition of S(r) as P(r) = V
(r1)V

(r1)
S(r), where V
(r1) = [V
1,..., V
r1 ] and V
j
is the normalized eigenvector of the matrix
S(r)S
(r) = (A
−1
 A)−1/2A
−1
 C(  −1
xx +)
−1C

−1
 A(A
−1
 A)−1/2 (6.20)
corresponding to the j th largest eigenvalueλ2
j . The ML estimates of B1 and B2 can
then be written explicitly, in a similar form as (2.17) of Section 2.3, as
B
1 = (A
−1
 A)−1/2V
(r1), (6.21a)
B
2 = V

(r1)(A
−1
 A)1/2B
 = V

(r1)(A
−1
 A)−1/2A
−1
 C.  (6.21b)
The ML estimate of B = B1B2 under the reduced-rank assumption is therefore
given by196 6 The Growth Curve Model and Reduced-Rank Regression Methods
B
 = B
1B
2 = (A
−1
 A)−1/2V
(r1)V

(r1)(A
−1
 A)1/2B

= (A
−1
 A)−1/2V
(r1)V

(r1)(A
−1
 A)−1/2A
−1
 C. 
Since the V
j are normalized eigenvectors, implicit in the construction of the
estimators B
1 and B
2 are the usual normalizations
B
2(−1
xx + )
−1B

2 = 2 ≡ diag{λ2
1,...,λ2
r1 }, B

1(A
−1
 A)B
1 = Ir1 .
The ML estimator of  for model (6.17) can be written as
 = 1
T (Y − AB
1B
2X)(Y − AB
1B
2X)

=  + (C
 − AB
1B
2)xx (C
 − AB
1B
2)

=  + (Im − P )(C
xxC

)(Im − P )
,
where P = A(A
−1  A)−1/2V
(r1)V

(r1)
(A
−1  A)−1/2A
−1  .
It is easy to see that the results derived for the reduced-rank growth curve
model (6.17) simplify to the results for the standard reduced-rank regression model
discussed in Chapter 2. These are obtained by setting A = Im with r = m, so
that B
 ≡ C
 and  = 0. The matrix S(r)S
(r) that is involved in the reduced-rank
growth curve model simplifies to R
 = −1/2  yx−1 xx xy−1/2  , the matrix through
which the standard reduced-rank model calculations were made in Chapter 2. We
also readily see that the above expression for  simplifies to (2.18) when A = Im
for the standard reduced-rank model.
The same estimation results can be derived using the conditional model approach
as presented in Section 6.2. The conditional model (6.6) when B is assumed to be
of reduced rank can be written as
Y1k = B1B2Xk + Y2k + ∗
1k, (6.22)
which is in the form of the reduced-rank model (3.3) of Section 3.1. From (3.11), it
follows that the ML estimates of B1 and B2 from this model are given by
B
1 = (A
−1
 A)−1/2V ,  B
2 = V

(A
−1
 A)1/2y1x.y2−1
xx.y2 ,
recalling from (6.10) that ∗
1 = (A
−1  A)−1 is the “unrestricted” ML estimate of
the error covariance matrix ∗
1 = Cov(∗
1k) in model (6.6), and V
 = [V
1,..., V
r1 ],
where V
j is the normalized eigenvector of the matrix
(A
−1
 A)1/2y1x.y2−1
xx.y2xy1.y2 (A
−1
 A)1/2 (6.23)6.6 Reduced-Rank Growth Curve Models 197
corresponding to the j th largest eigenvalue. However, from (6.8), we know that
y1x.y2−1
xx.y2 ≡ B
 ≡ (A
−1  A)−1A
−1  C
, and it has also been established in
Section 6.3 that xx.y2 = (−1 xx + )−1 (noting the correspondence −1 xx +  ≡
T  = −1
xx.y2 from (6.12)). So we see that the matrix in (6.23) is identical to the
matrix S(r)S
(r) in (6.20) that is involved in ML estimation through the previous
direct approach. Hence, it immediately follows that the above expressions for B
1
and B
2, obtained from the conditional model approach, can be expressed in an
identical form as in (6.21). In addition, note that the “unrestricted” ML estimate
of the error covariance matrix ∗
1 in model (6.6) can be expressed in the form
∗
1 = (A
−1  A)−1 ≡ y1y1.y2 − y1x.y2−1
xx.y2xy1.y2 and that
y1y1.y2 = 1
T [A
1YY
A1 − A
1YY
A2(A
2YY
A2)
−1A
2YY
A1]
= 1
T (A
(YY
)
−1A)−1 ≡ (A
−1
yy A)−1,
with yy = 1
T YY
, using Lemma 1 of Khatri (1966). Hence, we see that
y1x.y2−1
xx.y2xy1.y2 = y1y1.y2 − (A
−1
 A)−1
= (A
−1
yy A)−1 − (A
−1
 A)−1,
so it immediately follows that another equivalent form to represent the matrix in
(6.23) is as (A
−1  A)1/2[(A
−1
yy A)−1 − (A
−1  A)−1](A
−1  A)1/2, which is
similar to the form given by Albert and Kshirsagar (1993) in related work.
The formulation of the reduced-rank growth curve model (6.17) in the condi￾tional model form (6.22) leads more directly to LR testing for the rank of the
matrix B. Because (6.22) is in the framework of the reduced-rank model (3.3) of
Section 3.1, a LR test of the rank of B can be obtained through the eigenvalues λ2
j
of the matrix in (6.23), which is equivalent to the matrix S(r)S
(r) in (6.20) as noted.
These eigenvalues are also directly related, in the usual way, to the sample partial
canonical correlations between Y1k and Xk after adjusting for the effects of Y2k.
Following the developments in Section 3.1, the LR test for H0 : rank(B) = r1 is
based on the test statistic
M = [T − n − (m − r) + (n − r − 1)/2] r
j=r1+1
log(1 +λ2
j ), (6.24)
whose null distribution is approximated by the χ2-distribution with (r −r1)(n−r1)
degrees of freedom.
Alternatively, the formulation of the reduced-rank growth curve model (6.17) in
the conditional model form (6.6) also leads directly to LR testing for the rank of B.
Because (6.6) is in the framework of the reduced-rank regression model considered198 6 The Growth Curve Model and Reduced-Rank Regression Methods
in Chapter 3, a LR test of the rank of B can be obtained through results derived
there. Specifically, a LR test is obtained using the eigenvalues λ2
j of the matrix in
(6.23), which is equivalent to the matrix S(r)S
(r) as noted. These eigenvalues are
also directly related, in the usual way, to the sample partial canonical correlations
between Y1k and Xk after adjusting for Y2k. Following the details of Anderson’s
model discussed in Chapter 3, we are led to the LR test for H0 : rank(B) ≤ r1 that
is based on the same test statistic M as in (6.24).
Fujikoshi (1974) derived the form of the LR statistic for testing H0 :
rank(FBH ) ≤ r1, where F and H are known full-rank matrices, using the
conditional model approach. The LR test obtained was of the form (6.14), with the
λ2
j being the eigenvalues of the matrix S−1/2 e ShS−1/2 e ≡ {F (A
Σ−1  A)−1F
}−1/2
(FBH)  {H
Σ−1
xx.y2H}−1(FBH)  
{F (A
Σ−1  A)−1F
}−1/2 and Σ−1
xx.y2 expressed in
the form [Σ−1 xx + Ψ]. The matrix S−1/2 e ShS−1/2 e reduces to that in (6.23), and the
preceding results are obtained as a special case when F = Ir and H = In.
Note that LR testing for the rank of the matrix C(= AB) in the standard reduced￾rank regression model, Yk = CXk + k = ABXk + k, ignoring the knowledge that
the matrix A is known in the growth curve model, still could serve as a valid initial
testing procedure, for convenience. Under the assumed reduced-rank growth curve
model (6.17), the additional rank deficiency of C = AB = AB1B2 is due to the
component B = B1B2, since A is specified to be of full rank r. Hence, LR testing
in the standard regression model (as summarized in Section 6.5) would be valid to
suggest a possible rank for the matrix B, initially. However, this procedure would
not be as efficient as the LR testing procedure discussed in the preceding paragraph,
since it does not take into account the known specified value of the component
matrix A in model (6.17).
We comment briefly on the relationship between the LR test for specified linear
constraints on the coefficient matrix B and the LR test results above for the rank
of B. The LR test for H0 : FB = 0, where F is a specified (r − r1) × r full-rank
matrix, is derived in Section 6.3 to have the same form as the statistic M in (6.24),
with r replaced by (r −r1), and the statistic is asymptotically distributed as χ2
(r−r1)n.
In this context, theλ2
j are the eigenvalues of the matrix
S−1
∗ H∗ ≡ {F (A
−1
 A)−1F
}
−1F (A
−1
 A)−1/2S(r)S
(r)(A
−1
 A)−1/2F
.
When B is of reduced rank r1 < r, this is equivalent to the condition related
to H0 above that B satisfies FB = 0 for an unknown (r − r1) × r full-rank
matrix F. The LR test statistic for the rank of B is given by (6.24) and has an
asymptotic χ2
(r−r1)(n−r1) distribution. Thus we see the similarity between the two
testing situations, with the matrix S(r)S
(r) playing a main role in both cases. Because
the constraints are unknown in the reduced-rank case and need to be estimated,
the degrees of freedom (r − r1)n in the known constraints case are decreased to
(r − r1)(n − r1) for the reduced-rank case.6.6 Reduced-Rank Growth Curve Models 199
6.6.1 Extensions of the Reduced-Rank Growth Curve Model
In the same spirit as the extended model (6.14) considered in Section 6.4, the
reduced-rank growth curve model (6.17) can be extended to allow for the effect
of some predictor variables on the response without growth curve structure. That is,
we consider the model
Yk = AB1B2Xk + DZk + k, k = 1,...,T, (6.25)
where all terms are similar to those in (6.14) and (6.17). Maximum likelihood
estimation of the component matrices B1 and B2, as well as D, can be easily carried
out along the same lines as the relevant calculations for model (6.17). The main
difference is that the basic calculations now will be performed after adjustment
for the additional predictor variables Zk, as discussed in Section 6.4. Hence, the
procedures are similar to the preceding ones except that various sample covariance
matrices are replaced by sample partial covariance matrices, adjusting for Zk. In
particular, ML estimation of B1 and B2, similar to the expressions in (6.21), is now
based on B
 = (A
−1  A)−1A
−1  C
 with C
 = yx.z−1 xx.z, and the matrix similar
to that in (6.19) now has −1 xx.z in place of −1 xx .
Another extension of the model (6.17) concerns the inclusion of the additional
predictor variables Zk in the growth curve structure, but without any reduced-rank
feature. This is equivalent to representing D in model (6.25) in the form D = AB∗,
so that the resulting model can be represented as
Yk = A B1B2Xk + AB∗Zk + k
≡ A[B1B2Xk + B∗Zk] + k, k = 1,...,T. (6.26)
This can be viewed as analogous to the reduced-rank model (3.3) of Section 3.1,
but where the reduced-rank and “standard” regression structures are in terms of
the growth curve parameters of the responses Yk rather than in terms of the
response vectors Yk themselves. A special case of model (6.26) was the focus
of work by Albert and Kshirsagar (1993). Their study was in the context of a
multivariate one-way ANOVA situation involving n treatment groups, with Zk ≡ 1
and Xk an (n − 1)-dimensional vector of indicators. They considered the model
of the form of (6.26) for the purpose of combining the features of growth curve
with the application of reduced-rank regression to linear discriminant analysis. In
Section 3.2, we discussed the application of the reduced-rank regression model for
data from a multivariate one-way ANOVA situation and indicated its relation to
linear discriminant analysis in the case of m-dimensional unstructured mean vectors.
The above model (6.26) can be applied to growth curve structures in a similar way,
with emphasis on discriminant analysis through use of the r-dimensional growth
curve parameter vectors of each group to summarize their mean structure.
We will first discuss the ML estimation of parameters in model (6.26) and
then examine in more detail the special case of the multivariate one-way ANOVA200 6 The Growth Curve Model and Reduced-Rank Regression Methods
situation. For model (6.26), let D∗ = D + Cxz−1 zz with C = AB = AB1B2 and
D = AB∗, so that D∗ = AB∗ + AB1B2xz−1 zz = A(B∗ + B1B2xz−1 zz ) ≡
AB∗∗. Also let D∗ = yz−1 zz and C
 = yx.z−1 xx.z denote the unrestricted LS
estimators of D∗ and C. For ML estimation of (6.26), we need to minimize |W|,
where W = (1/T )(Y−AB1B2X−AB∗Z)(Y−AB1B2X−AB∗Z)
. As in previous
cases, ML estimation and LR testing results for the extended reduced-rank growth
curve model (6.26) can be derived using the conditional model approach. Analogous
to (6.22), the conditional model corresponding to (6.26) can be written as
Y1k = B1B2Xk + B∗Zk + Y2k + ∗
1k
= B1B2Xk + ∗Y ∗
2k + ∗
1k, (6.27)
where ∗ = [B∗, ] and Y ∗
2k = [Z
k, Y 
2k]

. This model is in the form of the reduced￾rank model (3.3) of Section 3.1. From (3.11) and recalling from (6.10) that ∗
1 =
(A
−1  A)−1 is the “unrestricted” ML estimate of the error covariance matrix ∗
1 =
Cov(∗
1k) in the conditional models (6.6) and (6.27), it follows that the ML estimates
of B1 and B2 from this model are given by
B
1 = (A
−1
 A)−1/2V
∗
(r1), B
2 = V
∗
(r1)(A
−1
 A)1/2y1x.y∗
2 −1
xx.y∗
2
, (6.28)
where V
∗
(r1) = [V
∗
1 ,..., V
∗
r1 ] and V
∗
j is the normalized eigenvector of the matrix
(A
−1
 A)1/2y1x.y∗
2 −1
xx.y∗
2
xy1.y∗
2 (A
−1
 A)1/2
corresponding to the j th largest eigenvalue λ∗2
j . However, from previous results
associated with the reduced-rank growth curve model (6.17), it follows that
y1x∗.y2−1
x∗x∗.y2 = (A
−1  A)−1A
−1  G ≡ B
∗, with G = [C,  D], and that
x∗x∗.y2 = (−1
x∗x∗ +∗)−1, where X∗ = [X
,Z
]
 and ∗ = (G−AB
∗)
−1  (G−
AB
∗). From these results, it can readily be established that y1x.y∗
2 −1
xx.y∗
2
=
B
 ≡ (A
−1  A)−1A
−1  C
 and that xx.y∗
2 = (−1 xx.z + )−1, where  =
(C
−AB)  
−1  (C
−AB)  . So we find that, equivalently, the eigenvectors V
∗
j involved
in the ML estimates B
1 and B
2 in (6.28) are those obtained from the matrix
S∗
(r)S∗
(r) = (A
−1
 A)1/2B(  −1
xx.z + )
−1B

(A
−1
 A)1/2 (6.29)
with B
 ≡ (A
−1  A)−1A
−1  C
. Similar to the arguments following Eq. (6.23), the
matrix in (6.29) can be expressed equivalently in the form (A
−1  A)1/2[(A
−1
yy.zA)−1−
(A
−1  A)−1](A
−1  A)1/2.
The ML estimate of B = B1B2 under the reduced-rank assumption is thus given
by6.6 Reduced-Rank Growth Curve Models 201
B
 = B
1B
2 = (A
−1
 A)−1/2V
∗
(r1)V
∗
(r1)(A
−1
 A)1/2B

= (A
−1
 A)−1/2V
∗
(r1)V
∗
(r1)(A
−1
 A)−1/2A
−1
 C. 
In addition, from results in Section 3.1 for the model (3.3), we obtain the ML
estimate of ∗ = [B∗, ] from the conditional model (6.27) as ∗ = y1y∗
2 −1
y∗
2 y∗
2
−
B
xy∗
2 −1
y∗
2 y∗
2
. From this, we can show that the ML estimate of the component B∗ is
obtained as
B
∗ = B
∗∗ − B
1B
2{xz−1
zz − xy.z−1
yy.z(D∗ − AB
∗∗)}, (6.30)
where D∗ = yz−1 zz and B
∗∗ = (A
−1
yy.zA)−1A
−1
yy.zD∗. Since C
 =
yx.z−1 xx.z,  = yy.z − yx.z−1 xx.zxy.z, and C
xz−1 zz = D∗ − D, where
D = yz.x−1 zz.x , it follows that
(A
−1
 A)−1A
−1
 C
{xz−1
zz − xy.z−1
yy.z(D∗ − AB
∗∗)}
= (A
−1
 A)−1A
−1
 {D∗ − D + ( − yy.z)−1
yy.z(D∗ − AB
∗∗)}
= B
∗∗ − (A
−1
 A)−1A
−1
 D. 
Therefore, the ML estimator B
∗ in (6.30) can also be expressed in the equivalent
form
B
∗ = B
∗∗ + (A
−1
 A)−1/2V
∗
(r1)V
∗
(r1)(A
−1
 A)1/2(B
∗ − B
∗∗), (6.31)
where B
∗ = (A
−1  A)−1A
−1  D. Finally, the ML estimate of the error covariance
matrix  is
 = 1
T (Y − AB
1B
2X − AB
∗Z)(Y − AB
1B
2X − AB
∗Z)

=  +
1
T (C
X + DZ − AB
1B
2X − AB
∗Z)
× (C
X + DZ − AB
1B
2X − AB
∗Z)

=  + (C
 − AB
1B
2)xx.z(C
 − AB
1B
2)

+ (Im − AB
1B
2xy.z−1
yy.z)(D∗ − AB
∗∗)zz
× (D∗ − AB
∗∗)

(Im − AB
1B
2xy.z−1
yy.z)

.
The formulation of the extended reduced-rank growth curve model (6.26) in the
conditional model form (6.27) also leads directly to LR testing for the rank of the
matrix B. Because (6.27) is in the framework of the reduced-rank model (3.3) of202 6 The Growth Curve Model and Reduced-Rank Regression Methods
Section 3.1, a LR test of the rank of B can be obtained through the eigenvaluesλ∗2
j
of the matrix S∗
(r)S∗
(r) in (6.29). These eigenvalues are also directly related, in the
usual way, to the sample partial canonical correlations between Y1k and Xk after
adjusting for the effects of Y ∗
2k. Following the developments in Section 3.1, the LR
test for H0 : rank(B) = r1 is based on the test statistic
M = [T − n − (m − r) + (n1 − r − 1)/2] r
j=r1+1
log(1 +λ∗2
j ), (6.32)
whose null distribution is asymptotically the χ2-distribution with (r − r1)(n1 − r1)
degrees of freedom.
Note that the form of the LR statistic for this situation also follows from results
of Fujikoshi (1974) mentioned earlier. Because model (6.26) can be expressed as
Yk = A[B B∗][X
k Z
k]
 + k, so B = [B B∗]H with H = [In1 0]

, the LR
statistic can be obtained by using F = Ir and H = [In1 0]
 in the notation for the
work of Fujikoshi (1974) discussed earlier.
6.7 Application to One-way ANOVA and Linear
Discriminant Analysis
We now elaborate on the multivariate one-way ANOVA case and consider the
situation and details similar to those from Section 3.2. Suppose we have independent
random samples Yi1,...,YiTi , i = 1,...,n, from n multivariate normal distribu￾tions N (μi,), and set T = T1 +···+ Tn. We suppose the mean vectors have
the growth curve structure μi = Aβi, where A is the known m × r within-subject
design matrix and βi is the r ×1 vector of unknown growth curve parameters for the
ith group. It will be assumed that the βi lie in some r1-dimensional linear subspace
with r1 ≤ min(n − 1,r). This is equivalent to the condition that the matrix of
contrasts [β1 − βn,...,βn−1 − βn] is of reduced rank r1. As mentioned above, the
model (6.26) accommodates this special situation where the model is expressed as
Yij = A[BXi + B∗Zi] + ij , with B = B1B2, and the choices of B and B∗ as
B = [β1 − βn,...,βn−1 − βn] and B∗ = βn, and Zi ≡ 1. The LS estimates of
C = AB and D = AB∗ are as in (3.14), C
 = [Y¯
1−Y¯
n,..., Y¯
n−1−Y¯
n] and D = Y¯
n,
and the corresponding estimate of  is  = 1
T
n
i=1
Ti
j=1(Yij −Y¯
i)(Yij −Y¯
i)
.
The (full-rank) estimates of B and B∗ are obtained directly from the results of
Section 6.2 as
B
 = (A
−1
 A)−1A
−1
 C,  B
∗ = (A
−1
 A)−1A
−1
 D.  (6.33)
The calculations related to the reduced-rank growth curve aspect, that is, ML
estimation of the component matrices B1 and B2, follow from the preceding general6.7 Application to One-way ANOVA and Linear Discriminant Analysis 203
results. From (6.28) and (6.29), the ML estimates of the component matrices are
based on the normalized eigenvectors of
(A
−1
 A)−1/2A
−1
 C(  −1
xx.z + )
−1C

−1
 A(A
−1
 A)−1/2. (6.34)
Let V
∗
(r1) denote the r ×r1 matrix whose r1 columns are the normalized eigenvectors
of the matrix above corresponding to its r1 largest eigenvalues. Then it follows from
(6.28) that the ML estimates of the component matrices are
B
1 = (A
−1
 A)−1/2V
∗
(r1), B
2 = V
∗
(r1)(A
−1
 A)−1/2A
−1
 C.  (6.35)
We set V
∗∗
(r1) = (A
−1  A)1/2V
∗
(r1) so that it satisfies the normalization
V
∗∗
(r1)
(A
−1  A)−1V
∗∗
(r1) = Ir1 . Then we can express the ML estimates in (6.35) as
B
1 = (A
−1  A)−1V
∗∗
(r1) and B
2 = V
∗∗
(r1)
(A
−1  A)−1A
−1  C
. The corresponding
ML reduced-rank estimate of B is
B
 = B
1B
2 = (A
−1
 A)−1V
∗∗
(r1)V
∗∗
(r1)(A
−1
 A)−1A
−1
 C.  (6.36)
The ML estimate of B∗ = βn is, from (6.31), given by
β
n = B
∗∗ + (A
−1
 A)−1/2V
∗
(r1)V
∗
(r1)(A
−1
 A)1/2(B
∗ − B
∗∗)
= B
∗∗ + (A
−1
 A)−1V
∗∗
(r1)V
∗∗
(r1)(B
∗ − B
∗∗),
where B
∗ = (A
−1  A)−1A
−1  Y¯
n and B
∗∗ = (A
−1
yy.zA)−1A
−1
yy.zY¯, since
D∗ = yz−1 zz = Y¯, with yy.z = (1/T )n
i=1
Ti
j=1(Yij −Y )(Y ¯ ij −Y )¯ 
. Because
the ith column of C
 is Y¯
i − Y¯
n and the ML reduced-rank growth curve estimate is
B
 ≡ B
1B
2 = [β
1 − β
n,..., β
n−1 − β
n], we also have
(β
i − β
n) = (A
−1
 A)−1V
∗∗
(r1)V
∗∗
(r1)(A
−1
 A)−1A
−1
 (Y¯
i − Y¯
n). (6.37)
Combining the last two expressions, we have the ML reduced-rank estimate of βi
as
β
i = (A
−1
yy.zA)−1A
−1
yy.zY¯ + (A
−1
 A)−1V
∗∗
(r1)V
∗∗
(r1)
× {(A
−1
 A)−1A
−1
 Y¯
i − (A
−1
yy.zA)−1A
−1
yy.zY¯}. (6.38)
Thus, the βi − βn are estimated using only the r1 linear combinations
V
∗∗
(r1)
(A
−1  A)−1A
−1  (Y¯
i − Y¯
n) of the differences in the sample mean vectors.
Observe that when there is no growth curve structure, hence r = m and A = Im,
the result in (6.38) reduces to the result (3.18) in Section 3.2.
Note that the matrix in (6.34), whose eigenvectors and eigenvalues are needed,
involves the matrix C(  −1 xx.z + )−1C

, where204 6 The Growth Curve Model and Reduced-Rank Regression Methods
 = (C
 − AB)  
−1
 (C
 − AB) 
and (C
 − AB)  = (Im − P )C
 with P = A(A
−1  A)−1A
−1  . Now the matrix
C
xx.zC
 = 1
T SB from (3.15), where SB = n
i=1 Ti(Y¯
i − Y )( ¯ Y¯
i − Y )¯  is the
“between-group” sum of squares matrix. Therefore, using the following standard
matrix inversion result (Rao 1973, p. 33),
(−1
xx.z + )
−1 = xx.z − xx.z(C
 − AB)  
× {(C
 − AB)  xx.z(C
 − AB)   +  }
−1(C
 − AB)  xx.z,
it follows that the matrix involved in (6.34) can be expressed in a more explicit form
as
C(  −1
xx.z + )
−1C

= 1
T
SB − 1
T
SB(Im − P )
×

(Im − P )  1
T
SB

(Im − P ) +  −1
(Im − P )
1
T
SB.
Finally, we briefly discuss aspects of linear discriminant analysis within the
reduced-rank growth curve context for the multivariate one-way ANOVA growth
curve model, Yij = μi + ij ≡ Aβi + ij , j = 1,...,Ti, i = 1,...,n.
Under the reduced-rank assumptions above for the growth curve parameters βi, for
i = 1,...,n − 1, we have
βi = βn + (βi − βn) = βn + B1B(i)
2 and μi = Aβi = Aβn + AB1B(i)
2 ,
where B(i)
2 is the ith column of the matrix B2. Let B1∗ = [B1, B∗] be an r × r full￾rank augmentation of the matrix B1, normalized by B
1∗(A
−1  A)B1∗ = Ir, and set
L
∗ = [1,...,r]
 = B
1∗A
−1  . Then considering the r linear combinations
L
∗μi = B
1∗A
−1
 (Aβn + AB1B(i)
2 ) = L
∗Aβn +

Ir1
0

B(i)
2 , i = 1,...,n − 1,
we see that only the first r1 vectors 1,...,r1 yield different mean vectors among
the n groups. So these first r1 vectors represent the set of potentially useful
information for discrimination among the n groups and thus may be regarded as
an adequate set of linear discriminant functions. The ML estimate of these r1
discriminant vectors L = [1,...,r1 ]
 = B
1A
−1  is
L
 = B

1A
−1
 = V
∗∗
(r1)(A
−1
 A)−1A
−1
 ,6.8 A Numerical Example 205
where B
1 = (A
−1  A)−1V
∗∗
(r1) is the ML estimate of B1. Also notice that the
ML estimates of the corresponding linear combinations of mean vectors, L
μi, are
obtained using (6.38) as
L

μi = L

Aβ
i = V
∗∗
(r1)β
i
= V
∗∗
(r1)(A
−1
 A)−1A
−1
 Y¯
i ≡ V
∗∗
(r1)β
i, i = 1, . . . , n, (6.39)
where β
i = (A
−1  A)−1A
−1  Y¯
i is the “full-rank” ML estimate of the growth
curve parameters for the ith group. So the r1 column vectors of V
∗∗
(r1) may be
referred to as the discriminant vectors for the space of growth curve parameters (e.g.,
Albert and Kshirsagar 1993, p. 351). Thus for an individual with response vector
Yij , β
ij = (A
−1  A)−1A
−1  Yij can be regarded as a weighted LS estimate of its
individual growth curve parameters, and the ML estimates of its linear discriminants
are L

Yij = V
∗∗
(r1)
(A
−1  A)−1A
−1  Yij ≡ V
∗∗
(r1)
β
ij , r1 linear combinations of its
estimated growth curve parameters.
6.8 A Numerical Example
In this section, we present details for a numerical example on multivariate growth
curve and reduced-rank growth curve analysis for some bioassay data, to illustrate
some of the methods and results described in the previous sections of this chapter.
We consider the basic multivariate growth curve model analysis methods of the
previous sections applied to some bioassay data taken from a study by Volund
(1980). The data consist of measurements on blood sugar concentration of 36 rabbits
at 0, 1, 2, 3, 4, and 5 h after administration of an insulin dose. The data are presented
in Table A.4 of the Appendix. The 36 rabbits comprise four groups of nine animals
according to a 2×2 factorial with two types of insulin preparation, a “standard” and a
“test” preparation, crossed with two dose levels, 0.75 and 1.50 units per rabbit, based
on an assumed potency of 26 units per mg insulin. In fact, the “standard” preparation
was a preparation of MC porcine insulin, and the “test” was a preparation from
another batch of MC porcine insulin, so we will not expect any substantial “type of
treatment” effect.
Figure 6.1 shows the average blood sugar concentrations for each of the four
groups at each time point.
We consider the vector Y = (y1, y2,...,y5) of measurements at the times 1, 2,
3, 4, and 5 h as the m = 5 dimension response vector and use as predictor variables
the indicator variable for insulin type (x1 = ±1), the indicator variable for dose
level (x2 = ±1), the initial blood sugar concentration at time 0 minus 100 (x3 ≡
y0 − 100), and a term for “interaction” between dose level and initial concentration
(x4 = x2x3). Thus, the initial multivariate linear regression model that we consider
for the kth vector of responses is of the form206 6 The Growth Curve Model and Reduced-Rank Regression Methods
Fig. 6.1 Average blood sugar concentrations over time for each of four treatment groups. Different
groups indicated by symbols: 1, standard treatment at low dose; 2, standard treatment at high dose;
3, test treatment at low dose; 4, test treatment at high dose
Yk = c0 + c1x1k + c2x2k + c3x3k + c4x4k + k ≡ CXk + k, k = 1,...,T,
where C = [c0, c1,...,c4] and Xk = (1, x1k,...,x4k)
, and T = 36. With Y
and X denoting 5 × 36 data matrices, respectively, the least squares estimate C
 =
YX
(XX
)−1 of the regression coefficient matrix C is obtained as
C
 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
54.5405 −0.0500 −5.0428 0.7879 −0.0062
(1.4840) (1.4530) (1.4820) (0.2300) (0.2242)
59.0460 0.6836 −7.0757 0.8073 −0.1451
(1.5700) (1.5380) (1.5680) (0.2434) (0.2372)
69.8572 −0.6324 −8.1554 1.0494 0.0948
(1.9110) (1.8710) (1.9080) (0.2962) (0.2887)
85.1540 −1.7673 −4.4398 1.3019 0.6732
(2.1070) (2.0630) (2.1040) (0.3266) (0.3183)
95.9062 −1.6904 −0.5283 1.1409 0.6809
(1.8180) (1.7810) (1.8160) (0.2818) (0.2747)
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
with estimated standard deviations given in parentheses below the corresponding
estimate. As was anticipated, it is seen that the insulin type “treatment” effect
variable x1 is not a significant factor for these data, whereas the dose variable (x2)
and initial concentration (x3) are quite important as expected, and the interaction
(x4) of the dose variable with initial concentration is a relevant factor mainly for the6.8 A Numerical Example 207
later responses at times of 4 and 5 h. The ML estimate of the 5×5 covariance matrix
of the errors k (with correlations shown above the diagonal) is given by
 = 1
36 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
62.1769 0.8552 0.5617 0.3149 0.1569
56.2715 69.6356 0.7907 0.5422 0.3865
44.9707 66.9940 103.0968 0.8106 0.5045
27.8012 50.6523 92.1435 125.3396 0.7572
11.9579 31.1633 49.5009 81.9090 93.3698
⎤
⎥
⎥
⎥
⎥
⎥
⎦
,
where  = Y − C
X, with log(| |) = 17.7096.
We next allow for a growth curve model structure of the form of (6.1), Yk =
ABXk + k, to represent the pattern of mean blood sugar concentrations over
the times 1 through 5 h. With the columns of the within-subject design matrix A
specified as orthogonal polynomial terms, it is found that a polynomial model of
degree 3 (r = 4) is required for an adequate fit. The goodness-of-fit chi-squared
test statistic, discussed in Section 6.3, for adequacy of this model gives the value of
M = 6.22 with 5 degrees of freedom. For the matrix A as shown, we obtain the
ML estimate for the growth curve coefficient matrix B as follows:
A =
⎡
⎢
⎢
⎣
1 1 1 11
−2 −1 0 12
2 −1 −2 −1 2
−120 −2 1
⎤
⎥
⎥
⎦ ,
B
 =
⎡
⎢
⎢
⎣
72.9036 −0.6908 −5.0451 1.0176 0.2599
10.6675 −0.6092 0.9131 0.1137 0.1932
1.0211 −0.1128 0.9673 −0.0307 0.0220
−1.0303 0.3352 −0.0117 −0.0620 −0.0884
⎤
⎥
⎥
⎦ ,
where B
 = (A
−1  A)−1A
−1  C
, with ML error covariance matrix estimate 
such that log(| |) = 17.9010. It is found that the growth curve model reproduces
the mean blood sugar concentration curves over time, in the sense that the estimates
of mean concentration obtained from the growth curve model, for each group, are
quite close to the mean concentrations estimated from the LS regression results.
Next we explore the possibility of a reduced-rank regression model form, without
specification of any specific polynomial growth curve model structure. That is, as
suggested in Section 6.5, we consider models of the form of Chapter 2, Yk =
ABXk + k, where the multivariate regression coefficient matrix C = AB is
assumed to have reduced rank r and A and B are both unknown (unspecified)
matrices of parameters to be estimated (subject to normalization constraints, of
course). LR testing procedures for the rank of C, as described in Section 2.6,
indicate that a coefficient matrix of rank r = 2 will be adequate, with the LR statistic
for testing H0 : rank(C) ≤ 2 having the value M = 8.62 with (5 − 2)(5 − 2) = 9
degrees of freedom. The ML estimates of the component matrices A and B are208 6 The Growth Curve Model and Reduced-Rank Regression Methods
obtained as in (2.17) of Section 2.3 (also (6.16) in Section 6.5) and are given by
A
 =
 4.6510 5.0493 5.9560 7.2172 8.1490
−4.2176 −6.3067 −6.5796 −2.1946 1.3839 
,
B
 =

11.7598 −0.2109 −0.2252 0.1454 0.0656
0.0601 −0.2346 0.9531 −0.0078 0.0770 
,
with ML error covariance matrix estimate  having log(| |) = 17.9922.
Notice, first, that the two column vectors of the estimated matrix A
 can be viewed
as the two “basis” functions over time needed to represent the mean blood sugar
concentration curves over time for each of the groups, and these provide essentially
as good a representation as the third degree polynomial growth curve model. Also,
taking into account the scales of the variables xik, the first linear combination
x∗
1k = β

1Xk of predictor variables in X∗
k = BX k consists mainly of a constant term
and contributions from the initial concentration (x3) and interaction (x4) variables,
whereas the second linear combination x∗
2k = β

2Xk is dominantly the dose level
variable (x2). We also mention that although a rank 2 model provides an acceptable
fit, a rank 3 model produces an excellent fit with log(| |) = 17.7404.
We can also consider a reduced-rank structure within the growth curve model
framework, as detailed in Section 6.6. Thus, we consider a model in the form of
(6.17), Yk = A B1B2Xk + k, with the within-subject growth curve design matrix
A specified to have the third degree orthogonal polynomial terms as in the previous
full-rank growth curve model. The LR testing procedure for the rank associated
with this model again indicates that a rank 2 model would be adequate, with the
LR test statistic from Section 6.6 for a test of H0 : rank(B) ≤ 2 giving the value
M = 7.74 with (4 − 2)(5 − 2) = 6 degrees of freedom. The ML estimates of the
component growth curve coefficient matrices B1 and B2 are obtained from (6.21)
and are given as
B

1 =
 6.2299 0.8919 0.0813 −0.0785
−3.7281 1.3797 0.9835 −0.2211 
,
B
2 =

11.7553 −0.2105 −0.2389 0.1453 0.0644
0.1224 −0.2746 0.8787 −0.0094 0.0696 
,
with ML error covariance matrix estimate  having log(| |) = 18.1590.
Notice that the two estimated linear combinations of predictor variables that occur
in this model, x∗
ik = β

iXk, i = 1, 2, are very similar to those estimated for the
previous reduced-rank regression model with a rank of 2 (especially the first linear
combination x∗
1k). In addition, it is found that the factor AB
1 from the reduced-rank
growth curve model above is fairly similar to the estimated left coefficient matrix A

in the previous reduced-rank regression model (especially the first column), with6.8 A Numerical Example 209
(AB
1)
 =
 4.6871 5.0997 6.0674 7.1976 8.0977
−4.2995 −6.5335 −5.6950 −2.8896 0.7772 
.
Finally, we can also consider the “extended” versions of the reduced-rank
regression and reduced-rank growth curve models, as discussed in Sections 3.1
and 6.6, respectively. The extended reduced-rank regression model we considered is
Yk = C1X1,k+DZk+k, with C1 = AB, where Zk ≡ 1 corresponds to the constant
terms in the model and X1,k = (x1k, x2k, x3k, x4k) contains the remaining predictor
variables. LR testing procedures for the appropriate rank of C1, through canonical
correlation analysis calculations as described in Section 3.1, indicate that a rank of
r1 = 2 would be appropriate for C1, with a LR test statistic for H0 : rank(C1) ≤ 2
having the value M = 1.93 on (5 − 2)(4 − 2) = 6 degrees of freedom. The ML
estimates of the component matrices A and B, obtained from Eq. (3.11) as described
in Section 3.1, are given by
A
 =
 5.3545 5.7591 8.0997 9.2386 6.9661
−3.3313 −5.4329 −5.0752 −0.3182 2.5203 
,
B
 =

−0.2229 −0.4256 0.1489 0.0643
−0.2865 0.8867 0.0192 0.0917 
,
with D = (54.4812, 58.9653, 69.9291, 85.1844, 95.8072)
, and ML error covari￾ance matrix estimate  having log(| |) = 17.7738. Note that the ML estimate
D in this model is nearly identical to the first column (corresponding to the constant
terms) of the LS estimate C
 in the multivariate regression model. In addition, notice
that the estimated coefficients in B
, especially those in the first row, that determine
the two linear combinations of the predictor variables X1,k are fairly similar to the
corresponding values in the previous reduced-rank model, which did not separate
out the constant term from the remaining predictor variables in formulating the
reduced-rank structure. The current extended model does provide for a slightly
better fitting model, however.
The extended version of the reduced-rank growth curve model, Yk = ABX1,k +
AB∗Zk + k, with B = B1B2, was also considered. A rank of 2 was selected for
the growth curve coefficient matrix B (LR statistic for testing H0 : rank(B) ≤ 2
of M = 1.15 with 4 degrees of freedom). The ML estimates were obtained from
(6.28) as
B

1 =
 7.0023 0.7475 −0.4158 −0.5541
−2.8220 1.4915 0.8697 −0.3841 
,
B
2 =

−0.2312 −0.3852 0.1498 0.0684
−0.3156 0.8303 0.0099 0.0810 
,210 6 The Growth Curve Model and Reduced-Rank Regression Methods
Table 6.1 Summary of fitting results of several multivariate regression and growth curve models
estimated for the blood sugar concentrations data
Type of model
Rank of C
or B
Number of
parameters,
n∗ log | |
AIC =
log | | +
2n∗/T
BIC =
log | | +
n∗ log T/T
Regression,
full rank 5 25 17.7096 19.0985 20.1981
Regression,
reduced rank 2 16 17.9922 18.8811 19.5849
Regression,
extended
reduced rank 2 19 17.7738 18.8293 19.6651
Growth curve,
full rank 4 20 17.9010 19.0121 19.8918
Growth curve,
reduced rank 2 14 18.1590 18.9368 19.5526
Growth curve,
extended
reduced rank 2 16 17.9400 18.8289 19.5327
with B
∗ = (72.8995, 10.6707, 0.9950, −1.0610)
, and ML error covariance matrix
estimate  having log(| |) = 17.9400. Once again, the implied two linear
combinations of predictor variables determined under this model are similar to those
estimated in the previous reduced-rank models. In addition, the ML estimate B
∗ in
this model is very similar to the first column of the estimate B
 in the previous “full￾rank” growth curve model.
A summary of the fits of several multivariate regression and growth curve models
considered for the blood sugar concentration data is presented in Table 6.1. Given
in the table are, values of log | | and of the AIC and BIC information criteria for
each model, where AIC = log | | + 2n∗/T and BIC = log | | + n∗ log(T )/T ,
and n∗ is the number of estimated regression parameters in the model. From
this summary of results, it can be seen that the models with reduced ranks offer
desirable improvements (and simplifications) over the full-rank models. Thus, we
have illustrated that more parsimonious models can be developed in the growth
curve context through the use of reduced-rank regression methods.
6.9 Some Recent Developments
Unbalanced Data The models studied in this chapter all correspond to a single
response characteristic over “m” distinct times on “T ” subjects. It is possible to
collect several response variables, and these data may not be all available over the
same number of times; response variables may be measured at distinct frequency.6.9 Some Recent Developments 211
Hwang and Takane (2004) study a multivariate reduced-rank growth curve model in
such a setting for a parsimonious representation.
It is taken that for T independent subjects, Y (j )
k is a mj -dimensional j th response
vector, and X(j )
k is a nj -dimensional set of time-invariant explanatory variables. This
setup is general as data on response variables do not have to be collected at the same
time points and not necessarily at the same number of times. Thus, the model (6.1)
can be written in a general form,
Y (j )
k = A(j )B(j )X(j )
k + 
(j )
k , j = 1, 2,... ,J. (6.40)
It is important to note that Y (j )
k and X(j )
k are of varying dimensions, varying
over the j th response variables. As in model (6.17), a reduced-rank structure on
Bj = B(j )
1 B(j )
2 can be imposed for dimension reduction. It is also possible that
“J ” predictor sets, X(1)
k = ··· = X(J )
k = Xk, and if A(j )
s are known, the model
is the standard growth curve model. This case can be further simplified when all
known basis functions across the “J ” response variables, A(1) = ··· = A(J ).
The estimation and inference details are all equal and can be found in Hwang and
Takane (2004). In some practical applications, it is possible to consider additional
constraints on B1 and B2 matrices across the response variables.
GAMANOVA-MANOVA von Rosen and von Rosen (2017) consider an extension
of the growth curve model with two sets of regressors, one set representing
the growth curve part and the other representing the variance part. Imposing
column-wise nested reduced-rank structure, the authors demonstrate how it provides
interpretation in terms of latent variables.
A Mixture Model The traditional growth curve model specifies a set of r “basis”
functions of time, such as (orthogonal) polynomial terms up to degree r − 1,
to represent the form of mean response over time. The completely prescribed
parametric model then takes the form (6.1), Yk = ABXk +k, k = 1,...,T , where
A is a m × r matrix whose columns contain prescribed values of the set of “basis”
functions of time, and B is a r × n matrix of unknown parameters to be estimated,
with r<m.
Sometimes a rather large number of basis functions of a particular class might
need to be specified to obtain adequate representation for the mean response
function, e.g., high degree polynomial curves may be needed. Thus, it might not
be desired to specify an entire set of basis functions of time in advance, but to
let some functions be left unspecified of a general form to be determined. So,
as a generalization, we can consider a model that has a mixture of pre-specified
and unspecified basis functions. In particular, to motivate, we could partition A as
A = [A1, A2], where A1 is m × r1 and is prescribed, with A2 being m × r2 but
“general” and not prescribed. For instance, we could take r1 = 2 with the columns
of A1 corresponding to a constant term and linear term as the only pre-specified
functions. Then partitioning B compatible with the partition of A, the generalized
model is212 6 The Growth Curve Model and Reduced-Rank Regression Methods
Yk = A1B1Xk + A2B2Xk + k. (6.41)
This model can be modified or generalized further to allow for (at least partly)
different predictor variable effects for different parts of the model as
Yk = A1B1Xk + A2B2Zk + k, (6.42)
where Zk could represent a n2-dimensional vector of additional covariates (some
of which could be in common with Xk). Such a modification might be desired if
we wish to treat certain of the explanatory variables in a different manner than
others, e.g., the effects of treatment group indicator variables might be modeled
differently than effects of continuous measurement covariates. We see that the
generalized model is a mixture of a traditional growth curve model component,
A1B1Xk with A1 prescribed, and a reduced-rank regression component, A2B2Zk
with r2 ≤ min(m, n2), and A2 as well as B2 unknown. The two extremes of the
traditional growth curve model and the reduced-rank regression are included as
special cases of the more general model.
Interest will be in model specification/selection, estimation of the unknown
regression parameters B1, A2, and B2 (with the latter two parametric matrices
subject to some normalization conditions), and estimation of the error covariance
matrix Σ . The model could also be modified to allow for some forms of random
effects covariance structure, particularly for the prescribed component of the model
A1B1Xk, such that Cov(Yk) = A1Γ A
1 + σ2I, for example.
The models developed in this chapter have scope for further extension and
applications.
(Some of the results in this chapter have appeared in Reinsel and Velu 2003.)Chapter 7
Seemingly Unrelated Regressions Models
With Reduced Ranks
7.1 Introduction and the Seemingly Unrelated Regressions
Model
The classical multivariate linear regression model discussed in Chapter 1 can be
generalized by allowing the different response variables yik to have different input
or predictor variables Xik = (xi1k,...,xink) for different i, so that yik = X
ikC(i)+
ik, i = 1,...,m, and the errors ik are contemporaneously correlated across
the different response variables. Multivariate linear regression models of this form
were considered by Zellner (1962), who referred to them as seemingly unrelated
regression (SUR) equation models. In experimental design situations, the model is
also referred to as the multiple design multivariate linear model (e.g., Srivastava
1967; Roy et al. 1971). In the notation of Section 1.2, the linear regression model
for the T × 1 vector of values for the ith response variable is
Y(i) = X
iC(i) + (i), for i = 1, . . . , m, (7.1)
where Xi = [Xi1,...,XiT ] is n × T , while Y(i) = (yi1,...,yiT )
, (i) =
(i1,...,iT )
, and C(i) are the same as in the regression model of Chapter 1.
Stacking the m vector equations of (7.1) together, the SUR model can be expressed
in vector form as
y = Xc + e, (7.2)
where X = Diag(X
1, X
2,..., X
m), while the vectors c = (C
(1)
,...,C
(m))
, y =
(Y
(1)
,..., Y
(m))
, and e = (
(1)
,..., 
(m)) are the same as in Section 1.3, and
hence, Cov(e) =  =  ⊗ IT = [(σij IT )].
Before we proceed with discussion of statistical properties associated with
analysis of the SUR model, we want to motivate the usefulness of such models.
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8_7
213214 7 Seemingly Unrelated Regressions Models With Reduced Ranks
The dependence of the m equations in (7.1) through the contemporaneous corre￾lations of the error terms (i) is a special feature of the SUR model. As will be
demonstrated below, use of this additional information across the equations can
improve the statistical properties of the estimators of the regression coefficients.
The econometrics literature contains many applications of the SUR model (e.g., see
Srivastava and Giles 1987, p. 2–3). The first example used by Zellner (1962) is based
on data that appear in Boot and de Wit (1960). For two large corporations, General
Electric and Westinghouse, both from the same type of industry, data were taken
on the investment functions of the firms annually from 1935 through 1954. It seems
reasonable to assume that the error terms associated with the investment functions of
the two companies may be contemporaneously correlated because of the influence
of common market factors. Another example where the SUR specification is used
is in the context of explaining certain economic activities in different geographic
regions. Giles and Hampton (1984) estimated production functions for several
regions in New Zealand assuming the possibility of inter-regional dependency
through the correlations in the error terms. These examples suggest that the SUR
model is appropriate and useful for a wide range of applications. The least square
(LS) estimators of the C(i) in model (7.1) are similar to those in (1.8),
C
(i) = (XiX
i)
−1XiY(i), i = 1, . . . , m.
The LS estimators have the properties that E(C
(i)) = C(i) and
Cov(C
(i),C
(j )) = σij (XiX
i)
−1XiX
j (XjX
j )
−1, i, j = 1, . . . , m.
In particular, we have the usual result that Cov(C
(i)) = σii(XiX
i)−1. However,
unlike the previous classical multivariate linear model, in the SUR model, the LS
estimators are not the same as the generalized least squares (GLS) estimator, which
is given by
c = {X
(−1
 ⊗ IT )X}
−1X
(−1
 ⊗ IT )y. (7.3)
Hence, the GLS estimator for this model does depend on the covariance matrix 
and does not coincide with the individual LS estimators of the m regression models
in (7.1), unless X = Im ⊗ X
, that is, X1 = X2 = ··· = Xm, so that the m linear
regression models each involve the same design matrix X of input variables. The
GLS estimator in (7.3) has mean vector E(c) = c and covariance matrix given by
Cov(c) = {X
(−1
 ⊗ IT )X}
−1. (7.4)
The difficulty with implementing the GLS estimator in (7.3) is that the covariance
matrix  will typically be unknown. A two-step estimation procedure was
originally proposed (e.g., Zellner 1962) in which an estimate of  is obtained
from the residuals from the LS estimators C
(i) as  = T −1

, where  =7.1 Introduction and the Seemingly Unrelated Regressions Model 215
[(1),...,(m)]
 with (i) = Y(i) − X
iC
(i), i = 1,...,m, being the vectors of
residuals from the LS estimation of the individual linear regression models in (7.1).
Then, the GLS estimator of the form of (7.3) is obtained, but with  replaced by
the estimate  , that is,
c = {X
(−1
 ⊗ IT )X}
−1X
(−1
 ⊗ IT )y. (7.5)
The covariance matrix of the estimator c is then approximated by Cov /(c) =
{X
(−1  ⊗ IT )X}−1. The two-step procedure can also be iterated, with the current
GLS estimator c in (7.5) at any given stage used to form the estimate  , and
this will lead to the maximum likelihood estimators of c and  under the
normality assumption of the errors (i) in (7.1). In principle, the GLS estimator
c = (C

(1)
,...,C

(m)) in (7.3) offers gains in efficiency, in terms of reduced
variance, over the LS estimators. The largest gains in efficiency can be expected for
situations when the design matrices satisfy XiX
j ≈ 0, and the squared correlations
ρ2
ij = σ2
ij /(σiiσjj ) of the errors are large (e.g., see Zellner 1962, 1963; Revankar
1974). For illustration, consider the case with m = 2 equations. Then, from (7.4),
the covariance matrix of the GLS estimator can be written as
Cov(c) = Cov 
C
(1)
C
(2)

=

σ11X1X
1 σ12X1X
2
σ12X2X
1 σ22X2X
2
−1
,
where we denote −1  = [(σij )], with σ11 = σ22/(σ11σ22 − σ2
12), σ22 =
σ11/(σ11σ22 −σ2
12), and σ12 = −σ12/(σ11σ22 −σ2
12). In particular, using a standard
matrix inversion result, we have that
Cov(C
(1)) = {σ11X1X
1 − [(σ12)
2/σ22]X1X
2(X2X
2)
−1X2X
1}
−1
= σ11(1 − ρ2
12){X1X
1 − ρ2
12X1X
2(X2X
2)
−1X2X
1}
−1,
noting that 1/σ11 = (σ11σ22 − σ2
12)/σ22 = σ11(1 − ρ2
12) and (σ12)2/(σ11σ22) =
(σ12)2/(σ11σ22) = ρ2
12. Compared to the covariance matrix of the LS estimator,
Cov(C
(1)) = σ11(X1X
1)−1, we readily see that the largest reduction in variance
for the GLS estimator will occur when X1X
2 = 0 and ρ2
12 is large. In most
practical situations, the predictor variables (the design matrices) are likely to be
similar, and therefore, the condition X1X
2 = 0 is not generally possible except in
certain experimental conditions, where the design matrices can be so constructed.
In practice, for the GLS estimator in (7.5), the use of the estimated error covariance
matrix  will cause some increase in the covariance matrix of the resulting GLS
estimator above the ideal result in (7.4).216 7 Seemingly Unrelated Regressions Models With Reduced Ranks
7.2 Relation of Growth Curve Model to the Seemingly
Unrelated Regressions Model
It has been noted by Stanek III and Koch (1985) that the basic growth curve (or
GMANOVA) model considered in Chapter 6 can be interpreted as a special case of
a SUR model. Recognition of this relationship between the growth curve and SUR
models can be useful for better appreciation of estimation procedures under the two
models and for methods of extension to more flexible growth curve models. Thus,
we briefly discuss the relationship in this section. Consider from Section 6.2 the
basic growth curve model in matrix form, Y = ABX + , and let H = [A1, A2],
where A1 = A(A
A)−1 and A2 is m × (m − r) such that A
2A = 0. Then under the
transformation of the model as in (6.5), with Y∗ = H
Y = [Y∗
(1)
,..., Y∗
(m)]

, we
have
Y∗ = H
ABX + H
 = P
BX + ∗ =

BX
0

+ ∗, (7.6)
where P = H
A = [Ir, 0]
 and ∗ = H
 = [∗
(1)
,..., ∗
(m)]

. Set ∗ =
Cov(∗
k ) = H
H, and write the r × n matrix B of unknown growth curve
coefficients as B = [B(1),...,B(r)]

. The transformed growth curve model (7.6)
can then be expressed as
Y∗
(i) = X
B(i) + ∗
(i), i = 1, . . . , r, (7.7a)
and
Y∗
(i) = ∗
(i), i = r + 1, . . . , m. (7.7b)
The (transformed) growth curve model, expressed in the form (7.7), can be seen to
be a particular version of a SUR model (7.1). The model can be written in vector
form similar to (7.2) as
y∗ = Xb + e∗ = (P ⊗ X
)b + e∗, (7.8)
with X = (P
⊗X
) = [Ir ⊗X, 0]

, b = vec(B
) = (B
(1)
,...,B
(r))
, and Cov(e∗) =
∗ ⊗ IT . For this model, a two-stage GLS estimator can be constructed by using
an estimate of ∗ based on LS estimation of each of the m regression equations
separately, as discussed at the end of Section 7.1, with the common design matrix X.
The resulting estimator of ∗ is ∗ = (1/T )Y∗[IT −X
(XX
)−1X]Y∗ = H
H.
From the last relation and the nonsingularity of H, it follows that H∗−1H =
−1  . Thus, when GLS estimation is applied to model (7.8), with the estimate ∗,
we obtain
b = [(P ⊗ X)(∗−1 ⊗ IT )(P ⊗ X
)]
−1(P ⊗ X)(∗−1 ⊗ IT )y∗7.3 Reduced-Rank Coefficient in Seemingly Unrelated Regressions Model 217
= [(P∗−1P
)
−1P∗−1 ⊗ (XX
)
−1X]y∗.
In matrix form, using the relations P = A
H and H∗−1H = −1  , we find that
this GLS estimator can be written equivalently as
B
 = (P∗−1P
)
−1P∗−1Y∗X
(XX
)
−1
= (A
−1
 A)−1A
−1
 YX
(XX
)
−1. (7.9)
Notice that this last expression in (7.9) is identical to the ML estimator of B for the
growth curve model (6.1), as given in (6.2) of Section 6.2. Hence, we see that the
basic growth curve model can be transformed and reexpressed as a particular SUR
model (7.7) and that a corresponding two-step GLS estimation for the SUR model
yields the ML estimate for the growth curve model. Since SUR models can be of
much more general form than in (7.7), the connection displayed above between
growth curve models and SUR models suggests that, perhaps, more general and
flexible growth curve models could be formulated and analyzed through the methods
of SUR models.
7.3 Reduced-Rank Coefficient in Seemingly Unrelated
Regressions Model
Before we formally introduce the reduced-rank aspects for the SUR model (7.1), we
want to discuss the need and usefulness of such a feature particularly in modeling
large-scale data sets. From the previous discussion thus far, it can be seen that the
main idea is that the m regression equations in (7.1) are indeed related to each
other, and this relationship is captured through the correlations between the error
terms. The aspect of reduced rank that we will propose here captures additional
relationship in a more “structured” way. To expand on this, consider the investment
function example used by Zellner (1962). Denote It as the current gross investment
of the firm, Ct−1 as the firm’s beginning of the year capital stock, and Ft−1 as the
value of the firm’s outstanding shares at the beginning of the year. These variables
are measured for the two electric companies annually from 1935 through 1954.
Observe that each company has its own values of the predictor variables Ct−1 and
Ft−1, but they are conceptually the same type of variables. General Electric’s gross
investment, on the average, is about twice as large as that of Westinghouse. For these
data, the GLS estimates obtained from (7.5) are given as
General Electric : I1t = 0.139C1,t−1 + 0.038F1,t−1 − 27.7
Westinghouse : I2t = 0.064C2,t−1 + 0.058F2,t−1 − 1.3.218 7 Seemingly Unrelated Regressions Models With Reduced Ranks
The estimated correlation between the estimated residual in the two equations is
approximately 0.73. The standard errors of the regression coefficients from the
SUR model approach are generally smaller than the standard errors of the ordinary
LS coefficient estimates. Thus the SUR model approach provides more efficient
estimates. From the above estimated equations, we observe that although the
response variable It is of different magnitudes for the two companies, the regression
coefficients of the predictor variables are roughly of the same magnitude. This raises
the possibility that apart from the constant terms, the coefficients of certain of the
predictor variables, for example Ft−1, could be constrained to be the same, or more
generally, the matrix formed from the coefficients of each SUR equation could
be of reduced rank. The possible relations or constraints in regression coefficients
across equations are somewhat suggested even in the two equation examples, but
we might expect such features to be more prominent when the number of SUR
equations is larger. Thus the dependence among the SUR equations can be more
directly modeled through dependency of the regression coefficient vectors from the
different equations because the predictor variables are conceptually similar. While
this dependency can be estimated mainly through the available data on the observed
response and predictor variables, the dependency through correlations among the
error terms as formulated by Zellner (1962) might be thought of as representing
the unknown or unobservable common factors, possibly not accounted for by the
available data or the regression component of the model (7.1). The approach we
will take here has the features of the reduced-rank regression model first discussed
in Chapter 2. It provides an opportunity for dimension reduction as well as a way
to make interpretations of linear combinations of predictor variables as canonical
variables in a certain sense. These features will be emphasized as we develop the
model. As a starting point for the development of the reduced-rank model, we restate
the SUR model (7.1) as
Y(i) = X
iC(i) + (i), for i = 1, . . . , m.
Arranging the regression coefficient vectors C(i) in matrix form, we have C =
[C(1), C(2),...,C(m)]
 as an m × n matrix. As noted previously, the model (7.1)
becomes equivalent to the standard multivariate regression model when the predictor
variables are the same for all m equations, that is, X1 = X2 =···= Xm = X, with
the equations Y
(i) = C
(i)X + 
(i) then being stacked to yield the matrix equation
Y = CX +  as in (1.4), where Y = [Y(1),..., Y(m)]
 and  = [(1),..., (m)]

.
For the SUR equation model, we shall assume that the m×n matrix C is of reduced
rank. More formally, we assume that
rank(C) = r ≤ min(m, n). (7.10)
Recall that an implication of this assumption is that C can be written as C = AB,
where A is a full-rank matrix of dimension m × r and B is full rank of dimension
r × n. Then, because C(i) = B
a(i), where A = [a(1),...,a(m)], and each a(i) is of7.4 Maximum Likelihood Estimators for Reduced-Rank Model 219
dimension r × 1, we can write the reduced-rank version of the SUR model (7.1) as
Y(i) = X
iB
a(i) + (i), i = 1, . . . , m. (7.11)
From the discussion in Chapter 2, it may be recalled that when the matrices Xi
are the same, the linear combinations of X that are most useful for modeling the
relationships with the response variables Y are given by BX. Depending on the
dimensions involved, substantial reduction in parameterization is possible, and BX
can also be related to canonical variables. We see from (7.11) that such an idea
can be extended to the more general multivariate regression system (7.1), and
the coefficients (the matrix B) of a type of canonical variables are fixed to be
the same for all m seemingly unrelated data sets. The distinctions among these
regression equations are reflected in the coefficients a(i) that are allowed to differ
among the regression equations in (7.1). Thus the reduced-rank regression approach
to analyze the regression models in (7.1) provides a canonical method to link
these models through the structure of the coefficients in addition to taking into
account the correlations among the error terms. We will focus on the estimation
of the component matrices A and B and also on the subsequent estimation of
the matrix C(= AB). Similar to the previous reduced-rank model situations, we
need to impose normalization conditions as in (2.14) to uniquely identify the
parameters A and B. Because of the different matrices Xi that appear in model (7.1),
natural normalization conditions for B are not obvious. Recall that we earlier
argued that dimension reduction through reduced rank seemed sensible because
of the “similarity” of the predictor variable sets among the m different regression
equations. Hence, for the normalization condition, it appears reasonable to use the
average of the second moment matrices of the predictor variables as follows:
A
A = Ir and BxxB = 2
r, (7.12)
where  is a positive-definite matrix that will typically be specified as  = −1 
and
xx = lim
T →∞
1
mT
m
i=1
XiX
i.
Note that these conditions correspond to (2.14) when Xi = X for all i.
7.4 Maximum Likelihood Estimators for Reduced-Rank
Model
We now consider derivation of efficient estimators of the coefficient matrices A
and B in the reduced-rank SUR model under the normality assumption for the error220 7 Seemingly Unrelated Regressions Models With Reduced Ranks
terms in (7.1). Let θ = (α
, β
)
, where α = vec(A
) and β = vec(B
). The criterion
to be minimized can be stated as
ST (θ ) = 1
2T e
( ⊗ IT )e, (7.13)
where e = y−Xc, subject to the normalization conditions as in (7.12) that are based
on sample versions of the normalizing matrices. We take  = −1  , the inverse of
the error covariance matrix estimate based on LS residuals, and xx is replaced by
xx = 1
mT
m
i=1 XiX
i. It appears that it is not possible to obtain an explicit solution
for A and B simultaneously. Therefore, we consider the first order conditions for
minimization of (7.13) from which one can solve for either one of the component
matrices when the other is known.
Since C = AB, we have c = vec(C
) = (A ⊗ In)vec(B
) = (Im ⊗ B
)vec(A
),
and so e = y−X(A⊗In)β = y−X(Im ⊗B
)α. Therefore, the first order conditions
from (7.13) are given as
∂ST (θ )
∂α = − 1
T (Im ⊗ B)X
( ⊗ IT )[y − X(Im ⊗ B
)α], (7.14a)
∂ST (θ )
∂β = − 1
T (A ⊗ In)X
( ⊗ IT )[y − X(A ⊗ In)β]. (7.14b)
Introduce the notations X(B) = X(Im⊗B
) = Diag{X
1B
,..., X
mB
} and X(A) =
X(A ⊗ In). Then the solutions to (7.14) for α and β, each in terms of the other
parameter, can be expressed as
α = [X(B)
( ⊗ IT )X(B)]
−1X(B)
( ⊗ IT )y, (7.15a)
β
= [X(A)
( ⊗ IT )X(A)]
−1X(A)
( ⊗ IT )y. (7.15b)
It is easy to see that when X1 = ··· = Xm = X, X = (Im ⊗ X) and the
above equations reduce to the partial least squares equation results for the standard
multivariate regression model case as given in Section 2.3. That is, we then have
X(B) = (Im ⊗ X
B
) and X(A) = (A ⊗ X
), so that the solutions in (7.15)
reduce to α = [Im ⊗ (BXX
B
)−1BX]y and β
= [(A
A)−1A
 ⊗ (XX
)−1X]y,
equivalently, A
 = Y X
B
(BXX
B
)−1 and B
 = (A
A)−1A
YX
(XX
)−1 with
y = vec(Y
).
In the above expressions, we will take  = −1  , where  is obtained from the
LS residuals as defined in Section 7.1. As noted in previous chapters, the first order
equations in (7.14) refer only to necessary conditions for a minimum to occur and
so do not guarantee the global minimum. We suggest two computational schemes
for solving the first order equations to obtain the efficient estimator.
The partial least squares procedure is perhaps computationally the easiest to
implement. For a given estimate β
or B
, the equation in (7.15a) is used to computeα
or A
, and similarly, given the estimate α, Eq. (7.15b) is used to arrive at an estimate7.4 Maximum Likelihood Estimators for Reduced-Rank Model 221
β
. Once these are computed, we use the equations
A

−1
 A
= Ir and B
xxB
 = 2
r (7.16)
to normalize the estimates, with xx = 1
mT
m
i=1 XiX
i. To perform the nor￾malizations based on given unnormalized estimates A
 and B
, we first obtain
the normalized eigenvectors V1,...,Vr of A

−1  A
 with corresponding eigen￾values d1,...,dr and define the r × r matrices V = [V1,...,Vr] and D =
diag(d1,...,dr). Next we obtain the normalized eigenvectors U1,...,Ur of the
matrix D1/2V 
B
xxB

V D1/2, with corresponding eigenvalues λ2
1,...,λ2
r, and set
U = [U1,...,Ur]. Then the normalized versions of the estimates of A and B are
given by A
∗ = AV D  −1/2U and B
∗ = U
D1/2V 
B
, such that C
 ≡ A
B
 = A
∗B
∗,
and A
∗ and B
∗ satisfy the normalization conditions (7.16). The partial least squares
procedure then continues to iterate between the two solutions α and β
, and at each
step, the above normalizations are imposed.
The second computational procedure is essentially a Newton–Raphson method
based on the Taylor expansion of the first order equations. Thus, it is based on
the same relations as given in (3.29) of Section 3.5.1 and in (4.11) of Section 4.3.
Similar to the procedures given in Sections 3.5.1 and 4.4, the ith step of the iterative
scheme is given by
θi
λi

=
θi−1
0

−

Wi−1 Qi−1
Q
i−1 −Ir2
 )
∂ST (θ )/∂θ|θi−1
h(θ )|θi−1
*
, (7.17)
where
W = (Bθ + HθH
θ )
−1Bθ (Bθ + HθH
θ )
−1 (7.18)
and Q = −(Bθ + HθH
θ )−1Hθ , with Hθ = {∂hj (θ )/∂θi}, where h(θ ) = 0 denotes
the r2 ×1 vector of normalization condition (7.12). In (7.18), the matrix Bθ is Bθ =
limT →∞ ∂2ST (θ )/∂θ∂θ as defined below in Eq. (7.20) in Theorem 7.1. Notice also
that the vector of first partial derivatives in (7.17) can be expressed as
∂ST (θ )
∂θ = − 1
T
M
X
(−1
 ⊗ IT )e, (7.19)
where M = [(Im ⊗ B
), (A ⊗ In)]. The iterations are carried out until successive
changes in θi or ∂ST (θi)/∂θ are reasonably small.
We now state a result on the asymptotic properties of the efficient estimator.
For this, we take  = −1  as known in (7.13) and (7.12), for convenience
of presentation. Again, the theoretical results follow from the work of Robinson
(1973) and Silvey (1959), and the proof is essentially of the same lines as those
of Theorems 3.2 and 4.1. Therefore, details of the proof are omitted; we note
only that the main asymptotic distributional result needed in the proof is that222 7 Seemingly Unrelated Regressions Models With Reduced Ranks
√
1
T
X
(−1  ⊗ IT )e d
→ N (0,V) as T → ∞ where V is defined below in
Theorem 7.1 (e.g., see Srivastava and Giles 1987, p. 28).
Theorem 7.1 Consider the model (7.1) under the stated assumptions, with C =
AB, where A and B satisfy the normalization conditions (7.12), and let h(θ ) =
0 denote the r2 × 1 vector of normalization conditions obtained by stacking
the condition (7.12) without duplication. Assume the limits limT →∞ 1
T XiX
j =
limT →∞ 1
T
T
k=1 XikX
jk = ij , i, j = 1,...,m, exist almost surely and  is
a positive-definite matrix. Let θ be the estimator that minimizes the criterion (7.13)
subject to conditions (7.12) for the choice  = −1  . Then as T → ∞, θ → θ
almost surely and √T (θ − θ ) has a limiting multivariate normal distribution with
zero mean vector and (singular) covariance matrix given by the matrix W =
(Bθ + HθH
θ )−1Bθ (Bθ + HθH
θ )−1, as in (7.18), where
Bθ = lim
T →∞
∂2ST (θ )
∂θ∂θ = M
VM, M = [(Im ⊗ B
), (A ⊗ In)], (7.20)
and V = {(Vij )} is mn × mn, where Vij = σijij is an n × n matrix, σij denotes
the (i, j )th element of −1  , and Hθ = {∂hj (θ )/∂θi} is of dimension r(m + n) × r2.
The asymptotic covariance matrix W can be used to make inferences on the
parameters of the component matrices A and B based on the efficient estimators A

and B
. The inferential aspects associated with the overall reduced-rank regression
coefficient matrix estimator C
 = A
B
 are also of interest. Because C
 − C = (A
−
A)B+A(B
−B)+op(T −1/2), it follows as in previous cases that √T vec(C

−C
) d
→
N (0,MWM
), and MWM = M(M
VM)−M
. Inferences related to C
 can be
based on this asymptotic distribution result.
7.5 An Alternate Estimator and Its Properties
The computation of the ML estimator in the reduced-rank SUR model requires
iterative procedures, as noted in the previous section, and for good convergence
of the procedures, a reasonably accurate starting value is needed. Therefore, initial
estimates of the component matrices A and B should be chosen with some care.
In this section, we seek an alternate to the ML estimator that is computationally
simpler and that can be used to specify initial values for the component matrices
in the ML procedures. In addition, such an alternate estimator can be used in a
more computationally convenient way than the ML estimator to initially specify or
identify the appropriate rank of the coefficient matrix C through hypothesis testing
procedures. As in the approach discussed for the model considered in Chapter 4,
a full-rank estimator can be first computed, and it may be used to help reveal
any possible reduced-rank structure in the true coefficient matrix. Hence, we will7.5 An Alternate Estimator and Its Properties 223
construct an alternate estimator for the component matrices A and B based on the
full-rank GLS estimator of the regression coefficient matrix C for model (7.1).
The motivation for the alternate estimator in the reduced-rank SUR model (7.1)
is somewhat similar to the reasoning offered for the alternate estimator considered
in Section 4.5 for the reduced-rank regression model with autoregressive errors of
Chapter 4. Observe that the criterion (7.13), with the choice  = −1  , can be
written as
ST (θ ) = 1
2T (y − Xc)
(−1
 ⊗ IT )(y − Xc) +
1
2T (c − c)
X
(−1
 ⊗ IT )X(c − c),
(7.21)
where c = {X
(−1  ⊗ IT )X}−1X
(−1  ⊗ IT )y is the full-rank GLS estimator
in (7.5). Minimizing ST (θ ) with respect to A and B (with C = AB) is equivalent to
minimizing the second term above subject to the normalization conditions in (7.16).
As discussed in the previous section, the computational procedures to solve for
the efficient estimates of the component matrices are iterative. However, in the
special case of the classical multivariate regression setup, we have seen in Chapter 2
that the ML estimates of the component matrices can be calculated simultaneously
and explicitly through computation of certain eigenvectors and eigenvalues. In this
special case, this is possible because we then have X = (Im ⊗ X), a specific
Kronecker structure, and so the second term in (7.21) reduces to
1
2T (c − c)
(−1
 ⊗ XX
)(c − c) = 1
2T
tr[−1
 (C
 − AB)XX
(C
 − AB)
],
where c = vec(C

) and C
 = Y X
(XX
)−1. The Householder–Young theorem can
be used to arrive at explicit solutions for A and B, for example, as given in (2.17)
of Section 2.3. These computational simplifications are possible mainly because in
the criterion the matrix −1  ⊗ XX
, which acts as a “weights matrix” for the vector
c − c that indicates the distance between the (full-rank) GLS estimator and the true
parameters, is of Kronecker product form, whereas such simplifications do not occur
in the SUR model since the matrix X
(−1  ⊗ IT )X is of more complicated (not a
Kronecker) form.
For an alternate estimation procedure, we suggest that this latter matrix be
replaced in the criterion by a weight matrix of similar form to that which appears
in the classical multivariate regression setup. Specifically, in place of 1
T XX in the
classical case, we choose the moment matrix of the averages of the predictor variable
sets,
x¯ x¯ = 1
T
X¯ X¯  = 1
m2T
m
i=1
m
j=1
XiX
j ,
where X¯ = 1
m
m
i=1 Xi, and consider use of the following criterion to be minimized:224 7 Seemingly Unrelated Regressions Models With Reduced Ranks
1
2
(c − c)
(−1
 ⊗ x¯ x¯)(c − c) = 1
2
tr[−1
 (C
 − AB)x¯ x¯(C
 − AB)
]. (7.22)
It follows from the Householder–Young theorem and related results in Chapter 2
that the alternate estimates A
 and B
, which are obtained by minimizing (7.22) and
which satisfy similar normalization conditions as in (7.16) for the ML estimates,
can be computed in the following way. The columns of 1/2
x¯ x¯ B
 are chosen to
be the eigenvectors corresponding to the r largest eigenvalues of the matrix
1/2
x¯ x¯ C

−1  C
1/2
x¯ x¯ , and A
 = C
x¯ x¯B

−2, where  is a diagonal matrix with
positive diagonal elements equal to the square roots of the eigenvalues of the matrix.
Equivalently,
A
= 1/2  [V
1,..., V
r] and B
 = [V
1,..., V
r]

−1/2  C, 
where the V
j are normalized eigenvectors of the matrix −1/2  C
x¯ x¯C

−1/2 
corresponding to its r largest eigenvalues.
It is expected that this alternate procedure might result in estimators of A and B
that have fairly high efficiency in cases where the matrices of regressors Xi have
somewhat similar characteristics, in the sense that the terms 1
T XiX
j are moderately
similar for all i and j , since then the “weights matrix” 1
T X
(−1  ⊗ IT )X might
be well approximated by −1  ⊗ x¯ x¯. Otherwise, the alternate estimators A
 and
B
 might tend to not have good efficiency, but they still might have reasonable
use as initial estimators. We also suggest the following improvement to this initial
estimation procedure. Specifically, once the initial estimate B
 has been obtained as
indicated, the estimate of A is obtained by a (one-step) GLS estimation of the SUR
model (7.11) for the Y(i) on the constructed regressors (B
Xi)
, i = 1,...,m. That
is, we obtain the initial estimate of A using (7.15a), based on the initial estimate B
,
as
α = [X(B)  
(−1
 ⊗ IT )X(B)  ]
−1X(B)  
(−1
 ⊗ IT )y,
with X(B)  = Diag{X
1B

,..., X
mB

} and α = vec(A

) = (a
(1)
,...,a
(m))
.
Our main interest in developing the alternate estimator is to provide initial
values for the component matrices in computing the ML estimates and also for
use in preliminary specification of the rank, as will be discussed in Section 7.6.
However, the asymptotic distribution of the alternate estimator might be of some
interest for comparison with the efficient estimator. Because the details are similar
to Theorem 7.1 and to results from Chapter 4, we only briefly mention the final
result. We let θ = (α
, β

)
, where α = vec(A

) and β
= vec(B

) with the alternate
estimators A
 and B
 as given above. It can then be established that √T (θ − θ ) has
a limiting multivariate normal distribution with zero mean vector and covariance
matrix given by the leading r(m + n)-order square submatrix of the r(m + n + r)-
order matrix7.6 Identification of Rank of the Regression Coefficient Matrix 225

M
(−1  ⊗ x¯ x¯)M −Hθ
−H
θ 0
−1
×

M
(
−1
 ⊗
x¯ x¯)V −1(
−1
 ⊗
x¯ x¯)M 0
0 0 
×

M
(−1  ⊗ x¯ x¯)M −Hθ
−H
θ 0
−1
,
where M,V , and Hθ are as defined before in Theorem 7.1, and
x¯ x¯ = lim
T →∞
1
m2T
m
i=1
m
j=1
XiX
j = 1
m2
m
i=1
m
j=1
ij .
This covariance matrix is expressible more explicitly as
W∗ = (B∗
θ + HθH
θ )
−1M
(−1
 ⊗ x¯ x¯)V −1(−1
 ⊗ x¯ x¯)M(B∗
θ + HθH
θ )
−1,
where B∗
θ = M
(−1  ⊗ x¯ x¯)M. Note that the matrix V is the limiting form
of T times the covariance matrix in (7.4). Although the distinction among the
different matrices of predictor variables Xi is not fully accounted for in construction
of the component matrix estimates under the alternate procedure, the asymptotic
covariance matrix of the alternate estimator θ does reflect the distinctness of the Xi
through the matrix V .
7.6 Identification of Rank of the Regression Coefficient
Matrix
To carry out the computations related to estimating the component matrices A and
B through the maximum likelihood method as outlined in Section 7.4, the rank
of the regression coefficient matrix C needs to be determined. In the classical
multivariate linear regression model situation, where all Xi in model (7.1) are
equal, the specification of rank could be conveniently carried out because of the
correspondence between the number of nonzero canonical correlations and the rank
of the regression matrix. For this case, we have used the LR statistics
−[(T − n) +
1
2
(n − m − 1)] m
j=r+1
log(1 − ρ2
j ),
where the ρ2
j are the squared sample canonical correlations. This form of test does
not apply in the SUR model, since each response variable has its own set of predictor226 7 Seemingly Unrelated Regressions Models With Reduced Ranks
variables. We therefore need to develop some large sample procedures to test the
hypothesis of the rank of C.
Before we further consider the problem of testing for the rank of the matrix C,
we want to briefly comment on procedures for testing usual linear hypotheses for
coefficients in the SUR model (7.1). Typically, the model is considered in the form
given by (7.2), and the linear hypothesis constraints are stated as Rc = 0, where R
is a known matrix. A particular application of interest is testing for aggregation bias
as discussed by Zellner (1962). Recall that the model (7.1) may often refer to data
relating to several different micro-units. Simple aggregation of data from these units
into macro-unit data, and subsequent estimation of the aggregate model, will not
lead to any aggregation bias if the hypothesis H0 : C(1) = C(2) = ··· = C(m) (that
is, all regression coefficients are the same) holds, or equivalently, Rc = 0, where R
is an appropriately defined known (m − 1)n × mn matrix. The usual LR procedures
can be applied to test such hypotheses. The linear constraints imposed by a reduced￾rank condition are more general than the above equality of coefficients hypothesis,
however. If the above hypothesis holds, it implies that the rank of the matrix C is
one; the reverse implications are somewhat more general than the coefficient vectors
being identical (i.e., they are proportional). Examining the investment function
example described in Section 7.3 for aggregation bias, based on the LR test Zellner
(1962), concluded that the hypothesis of coefficient vector equality is rejected. Note
in our earlier discussion of this example that the coefficients of some predictor
variables, not necessarily all, might be constrained to be the same implying that
the regression coefficient matrix could be of lower rank. Thus, interest in testing
for the rank of the regression coefficient matrix may stem from other than purely
statistical modeling considerations. We now discuss procedures for testing the rank
of the matrix C of regression coefficients.
We can still consider the LR statistic for testing H0 : rank(C) ≤ r. In general
terms, this is given as
M = [(T − n) +
1
2
(n − m − 1)] log(|(r)
 |/| |), (7.23)
where  denotes the unrestricted ML estimate of  and (r)
 denotes the
restricted estimate based on the reduced-rank ML estimate C
 = A
B
 of C,
subject to the condition that rank(C) ≤ r. The statistic M asymptotically will
follow a χ2
(m−r)(n−r) distribution under the null hypothesis. The test statistic
could be computed for various values of r and used to test the validity of each
rank assumption. Because this procedure would require the somewhat involved
calculations for the ML estimates A
 and B
 to obtain (r)
 , for a range of possible
ranks r, at the initial specification stage, it would not be a convenient procedure
for selection of an appropriate rank for C, prior to computing the ML estimate
of C. Therefore, for initial specification of possible rank, we also suggest using
an alternate testing procedure based on the alternate estimator of C described in
Section 7.5. The numerical example presented in the next section indicates that the
alternate estimation procedure is somewhat useful for initial testing of rank, as well7.7 A Numerical Illustration with Scanner Data 227
as for obtaining initial values in iterative computation of the ML estimates of A and
B.
The alternate test statistic that we consider is thus M∗ = [(T − n) + 1
2 (n −
m − 1)] log(|(r)
 |/| |), where (r)
 is the error covariance matrix estimate
constructed, in the usual way, from the residual vectors (i) = Y(i) − X
iC
(r)
(i) ,
i = 1,...,m, where C
(r) = [C
(r)
(1),...,C
(r)
(m)]
 = A
(r)B
(r) and A
(r) and
B
(r) are the alternate estimates of A and B described in Section 7.5. Another
possible and convenient alternate test statistic that is motivated by the alternate
estimation procedure in Section 7.5 is of the form M∗ = [(T − n) + 1
2 (n −
m − 1)]
m
j=r+1 log(1 + λ2
j ), where the λ2
j are the eigenvalues of the matrix
−1/2
, C
x¯ x¯C

−1/2
, , x¯ x¯ = 1
m2T
m
i=1
m
j=1 XiX
j , and C
 is the full-rank GLS
estimator obtained from (7.5). Although this test statistic does not involve any
quantities that are canonical correlations, because of the analogue to the classical
multivariate linear model case we may interpret ρ2
j = λ2
j /(1 + λ2
j ) as “canonical
correlation” quantities. The standardized values ρ2
j might be easier to interpret as
descriptive tools than the values λ2
j . We thus suggest using the above statistic M∗
and the reference χ2
(m−r)(n−r) distribution to initially test the hypothesis of the rank
of C, for various values of r. This alternate “approximate” test procedure will tend
to be conservative because the determinant of the estimated error covariance matrix
obtained using the alternate estimator will be larger than that obtained using the ML
estimator of A and B.
7.7 A Numerical Illustration with Scanner Data
We illustrate the procedures for the seemingly unrelated regressions model devel￾oped in this chapter with an application in the area of marketing. Typically,
manufacturers and retailers have to make, on a continual basis, important marketing
decisions regarding the promotion and pricing of products. They make these
decisions based on elasticity estimates related to various promotional activities and
the price discounts. Now that scanner data of weekly sales for retail chains are
readily available, these elasticities can be easily computed. But these extensive data
do not guarantee reliable estimates for individual product brands and generally yield
a large set of estimates that may be difficult to fully comprehend. We attempt to
address these problems through the reduced-rank SUR models developed in this
chapter. The point-of-sale data are collected at the store level for all the brands in a
product category for a weekly time interval. In large cities, a retail chain typically
owns several stores. For our illustrative example, we aggregate the store level data
into data for the chain and compute the elasticity estimates because important
marketing negotiations related to trade discounts and promotions are usually made
between the manufacturers and the retail chains; the retail chains then decide on
marketing plans at the store level.228 7 Seemingly Unrelated Regressions Models With Reduced Ranks
The product whose data we consider in this analysis is a refrigerated product with
several brands; the data are for a large chain located in a highly populated city in the
northeastern United States. The weekly data span the period from mid-September
1988 to mid-June 1990, covering 93 weeks. In addition to the number of units sold,
for each brand, we consider data on regular price, actual price, whether promotion
was present and whether the brand was on display. With the information available,
we consider the following model, similar to the model advocated by Blattberg and
George (1991) and Blattberg et al. (1994), as appropriate for the store level data
analysis:
St = β0 + β1St−1 + β2Pt + β3P Dt + β4P Rt + β5Dt + β6DDt + β7CPDt + t,
(7.24)
where
St = logarithm of the number of units sold in tth week
Pt = regular price
P Dt = price discount = (regular price—actual price)/regular price
P Rt = indicator for promotions
Dt = indicator for display
DDt = deal decay code = k if deal is present in (k + 1)st week
CPDt = maximum price discount for competing brands in a chain
The “semi-log” model (7.24) has also been used by Blattberg and Wisniewski (1989)
and is known to fit better than log–log or linear models. While the regression
coefficients of a log–log model can be directly interpreted as elasticities, the
coefficients of a semi-log model can be converted into point elasticities.
There are some broad assertions from the marketing literature that can guide us to
interpret the coefficients of the above model. Three promotional variables, P Rt , Dt ,
and DDt , are included in the model. Although the promotion and display variables
had several categories, we use them as binary indicator variables because of a lack
of variation in the actual occurrence of these categories. The deal decay index is
supposed to reflect the waning effectiveness of promotions. It is postulated that
the first weeks of promotions are far more effective than later weeks. Displays are
expected to increase sales because the retail space available is limited and expensive;
they are usually reserved for high volume items. Two variables are included to
measure the price effect, the regular price, and price discount. It is believed that
deal elasticities are generally higher than price elasticities. The effect of competition
can be measured through their promotional and price cross-elasticities, but here we
chose only the competitors’ price discount as a variable that may play a significant
role. There is some discussion on deciding on which brands compete with each
other, but it generally is agreed that brands with similar prices tend to compete with
each other. Brand switching based on price discounts occurs mostly toward higher
priced brands; if the lower-tier brand promotes, it does not usually attract customers7.7 A Numerical Illustration with Scanner Data 229
from higher-tier brands. The lagged value of St is included in (7.24) to account for
serial correlation.
Because we consider the chain level data, the variables in (7.24) are all
aggregated over the stores within that chain. The response variable St at the chain
level is the logarithm of total sales aggregated over the stores, Pt is the average
price, P Dt is the average price discount, P Rt is the proportion of stores promoting,
Dt is the proportion of stores displaying, DDt is the deal decay code averaged over
stores, and CPDt is the average price discount offered by the competitors. To get
some understanding of the nature of the data, Fig. 7.1 displays time series plots of St ,
log sales for the m = 8 brands. For all but brand 6, we notice that the sales generally
peak during the summer weeks, especially around July 4th. In Fig. 7.2, we present
plots of the predictor variables for brand 1 only, to avoid repetition. The regular price
is a non-decreasing step function with relatively few jumps over the duration. The
plots show that price discounts and promotions are fairly frequent, and their peaks
tend to match peaks in log sales, whereas there was no display activity for brand 1
over this time period. Also, for most weeks, one or more competitors’ brands offer
price discounts.
All variables are adjusted by subtraction of sample means before we proceed
with the analysis. The ordinary LS estimates of the regression model (7.24) were
first computed for each of the m = 8 brands with n = 7 predictor variables,
based on T = 92 observations, because of the lag term included in the model.
The correlations of the residuals across equations are all positive and range from
0.1 to 0.6, indicating similarity of the sales of different brands over time in
this product market. This suggests the possibility that SUR equation estimates
might be desirable. Both the two-step and the ML estimates of the SUR model
were computed. The determinants of the residual covariance matrices are 0.02101,
0.01244, and 0.01141 (×10−8) for the LS, two-step SUR, and ML SUR estimation,
respectively. The (full-rank) ML estimate of the coefficient matrix C is presented in
Table 7.1, with expected signs of the coefficients for individual predictor variables
indicated in parentheses in the column headings; an asterisk indicates that the
coefficient estimate is significant at the 5% level.
For brands 1, 3, and 4, because of no activity in display area, the coefficients for
the display indicator Dt cannot be estimated, and they are taken to be “missing.”
Each type of predictor variable exhibits significance for one or more brands,
confirming the usefulness of including them in the model. The brands that have
nonsignificant coefficients for lagged sales tend to be influenced by promotions. All
significant coefficients have the correct expected signs except for the price discount
for brand 3. We are not able to explain this anomaly but observe that having only
one “unreliable” estimate among such a large number of estimates might be taken
as satisfactory (see Blattberg and George 1991, for related discussion).
To proceed with reduced-rank analysis under the SUR model framework, using
the initial alternate estimation procedure, we need to have the full-rank estimate
of C with no “missing” elements. Because the display variable is significant for
most brands and is potentially important from the marketing area point of view,
we want to entertain the possibility of accommodating this predictor variable in the230 7 Seemingly Unrelated Regressions Models With Reduced Ranks
Fig. 7.1 Weekly log sales of scanner data for 8 brands, September 1988–June 19907.7 A Numerical Illustration with Scanner Data 231
Fig. 7.2 Weekly predictor variable values for log sales of scanner data for brand 1, September
1988–June 1990232 7 Seemingly Unrelated Regressions Models With Reduced Ranks
Table 7.1 Full-rank ML estimates of coefficient matrix C in SUR model for the scanner data
Lag Price
Sales Price Disc Prom Disp DD CPD
Brand (+) (−) (+) (+) (+) (−) (−)
1 0.358* −0.795* 2.597* 0.213 – −0.161* −0.048
2 0.127* −0.816 0.617 0.587* 2.814* −0.104 0.304
3 0.041 −0.584* −1.786* 0.699* – 0.014 0.166
4 0.311* −2.736* 2.753* 0.173 – −0.924 0.150
5 0.244* −4.696* 1.817* 0.372* 5.646* −0.047 −0.058
6 0.040 −0.865* 2.367* 0.495* 7.403* −0.263* −0.642*
7 0.054 −1.973* 3.136* 0.211 3.111* −0.116* −0.164
8 0.292* −0.621 2.262 0.709 4.633 −2.140 −0.537*
model for all brands. While there are various options to fill in the missing estimates,
we need to be sure that the procedure does not unduly affect the estimation of
component matrices in the reduced-rank model. We have considered two options:
(1) substituting the average value of the available display coefficient estimates
for the missing values; (2) because brands tend to compete within a price tier,
substituting the available competitor’s display variable coefficient estimate for the
missing value. Specifically, brands 1 and 2 are in the top price tier, 3, 4, and 5 are in
the mid price tier, and brands 6, 7, and 8 are in the low price tier. Since the estimation
results were very similar using both options, we will follow option (2) and present
the corresponding analysis and results.
As suggested in Section 7.6, we use the results from Section 7.5 that are related
to the alternate estimator to obtain information for an initial specification of the
rank of C and initial estimates of the coefficient matrices A and B in the reduced￾rank SUR model. The nonzero eigenvaluesλ2
j of −1/2  C
x¯ x¯C

−1/2  are found to
be 2.740, 1.784, 0.691, 0.183, 0.118, 0.067, and 0.005. The corresponding squared
“canonical correlation” quantities ρ2
j are 0.733, 0.641, 0.409, 0.155, 0.106, 0.062,
and 0.005, which may provide a better feel for the possible rank. Formally, using
the approximate LR test procedure mentioned in Section 7.6 gives the impression
that the rank might need to be taken as large as r = 4 or r = 5. However,
because of the conservative nature of this test procedure and the desire to explore
a more parsimonious model, we further investigate the reduced-rank model with
specification of lower ranks.
Thus, we examine ML fitting of models of various ranks. The procedure followed
in calculation of the ML estimates for the model of each different rank is the partial
least squares procedure described in Section 7.4. The BIC criterion is used to select
a model of suitable rank, and the results for the BIC as well as the AIC criterion are
presented in Table 7.2. It appears from these results that a model of rank 2 might be
appropriate based on BIC, although the use of the AIC criterion would again require
the use of model with rank as high as 5.7.7 A Numerical Illustration with Scanner Data 233
Table 7.2 Summary results
on ML estimation of SUR
models of various ranks fitted
to the scanner data
n∗ = number
Rank | | × 108 of parameters AIC BIC
1 0.081145 14 −2.207 −1.823
2 0.031958 26 −2.878 −2.165
3 0.021850 36 −3.041 −2.054
4 0.014791 44 −3.257 −2.051
5 0.012370 50 −3.305 −1.935
6 0.011406 54 −3.299 −1.819
7 0.011406 56 −3.256 −1.721
The ML estimates of the component matrices B and A in the model of rank r = 2
are
B
 =

1.0226 −15.0856 18.5987 −0.1566 4.7584 −0.4678 −0.7757
0.5840 8.3231 −4.0433 5.0278 31.8771 −0.4947 −2.0161 
,
A
=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1184 0.0945
0.0770 0.1166
0.0525 0.0736
0.1956 0.0008
0.2095 0.0740
0.1207 0.1504
0.1658 0.0510
0.1273 0.1859
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
The resulting ML estimate C
 = A
B
 is displayed in Table 7.3. Comparing elements
of the rank 2 ML estimate of C with those of the full-rank estimate given in
Table 7.1, we can observe that most of the coefficient estimates that were found
to be significant under the full-rank estimation are well recovered within the rank
2 model specification, with the rank 2 model having less than one-half the number
of parameters as the full-rank model. Also note that the price discount coefficient
estimate for brand 3, which had the “incorrect” sign in the full-rank estimation, now
attains the correct sign in the model of rank 2.
From the ML estimate B
, it is interesting to note that the most important linear
combination of the predictor variables (ignoring coefficients with small magnitudes)
is St−1 − 15.1Pt + 18.6P Dt + 4.8Dt . This linear combination might be interpreted
to represent a market factor comprised of price and display variables. The price
discount variable P Dt receives a larger weight than the display variable Dt . From
the ML estimates in the first column of A
, we see that this linear combination has
the highest impact on sales for brands 4, 5, and 7, medium impact for brands 1,
6, and 8, and smallest impact for brands 2 and 3. The second predictive linear
combination includes a large contribution from the display variable Dt , and both the
promotion variable P Rt and the competitors’ price discount variable CPDt enter234 7 Seemingly Unrelated Regressions Models With Reduced Ranks
Table 7.3 ML estimates of coefficient matrix C = AB in reduced-rank SUR model of rank
r = 2 for the scanner data
Lag Price
Sales Price Disc Prom Disp DD CPD
Brand (+) (−) (+) (+) (+) (−) (−)
1 0.176 −0.998 1.819 0.458 3.584 −0.102 −0.283
2 0.146 −0.191 0.960 0.574 4.084 −0.094 −0.295
3 0.096 −0.179 0.679 0.362 2.596 −0.061 −0.189
4 0.205 −2.883 3.605 0.010 1.188 −0.096 −0.168
5 0.257 −2.544 3.597 0.339 3.356 −0.135 −0.312
6 0.211 −0.569 1.637 0.737 5.368 −0.131 −0.397
7 0.199 −2.077 2.878 0.230 2.412 −0.103 −0.231
8 0.238 −0.374 1.616 0.915 6.532 −0.152 −0.474
more substantially than in the first linear combination. This predictive combination
seems to have greater influence on brands 8, 6, and 2. To make more serious
interpretations of the reduced-rank estimation results for these data, it would be
necessary to delve into the extensive marketing literature. A goal here was to
illustrate how certain canonical and reduced-rank procedures can be used for the
SUR model setup. Further discussion related to interpretation of the estimation
results will not be given here.
As a summary of the analysis results, we mention that the values of the ML
residual variance estimates of log sales for the 8 brands under the full-rank SUR
model fit were 0.0389, 0.1311, 0.0518, 0.0622, 0.2337, 0.1717, 0.0637, and 0.0625,
whereas the corresponding values under the reduced-rank (r = 2) fit were 0.0520,
0.1318, 0.0620, 0.0674, 0.2670, 0.1720, 0.0593, and 0.0665. It is observed that
the magnitudes of the residual variance estimates under the rank 2 model are
comparable to those of the full-rank model. Taking into account the considerable
reduction in the number of parameters that need to be estimated for the rank 2
model, slight increases in residual variances may be a small price to pay. An
additional appeal of reduced-rank modeling of scanner data is that it provides a way
to summarize, for managers, the large number of regression coefficient (elasticity)
estimates through the “canonical quantities,” and these canonical quantities can also
be used to keep track of market changes over time. Thus, the interpretation of large
amounts of data can be simplified for managerial use. A version of the results in
the chapter is published by Velu and Richards (2008). In this chapter, we further
demonstrate how the quantities “a(i)’s” can be related to store level demographics.
In marketing studies,c(i)’s are used to group the stores and then common regression
coefficients are estimated by pooling. Note that these coefficients are of dimension
“n.” With the use of reduced-rank idea, first we isolate the common effect via B
in (7.11) and the individual store effect via r-dimensional a(i). Because r<n, the
clustering is easier to construct and interpret.7.8 Some Recent Developments 235
7.8 Some Recent Developments
The model developed and studied in this chapter has broader applications as the
recent literature would suggest. The basic structure of the model is specified in (7.1)
along with (7.10), which results in (7.11) in the formulation of seemingly unrelated
reduced-rank regression (SURERR). The basis of the formulation is that regression
coefficients from each unit’s regression can be pooled together via reduced-rank
model inducing some commonality among them. This is more general than simply
testing if the coefficients are equal over all units. Recall that the basic data matrix
of response variables is still (m × T) Y -matrix. Because of the structure in (7.1),
each row of Y -matrix is associated with (T × n) X matrix of predictors. These
predictors are also time-varying but take different values for different response units.
The reduced-rank constraint in (7.10) essentially pools the information from “m”
time series regression models in (7.1). If “m” and “T ” are fairly large, it is also
possible to consider “T ” cross-sectional regressions and pool the information across
units via reduced-rank constraint. More specifically, if we denote each column of the
data matrix Y as Yk, then
Yk = Zkβk + εk, k = 1, 2,...,T. (7.25)
Observe that Yk is a “m”-dimensional vector, Zk is a “m × n” dimensional matrix
of predictors, βk is a “n”-dimensional coefficient vector, to be estimated at each
time point. Although we can add some dependence structure to εk’, we assume
here that the εk’s are independent and follow, N (0,Σ); the variance–covariance
matrix captures contemporaneous correlations among the “m” units. Stacking the
“β” coefficient as C = [β1,...,βT ]

, a T × n matrix, we can impose that
rank(C) = r ≤ n. (7.26)
This results in the decomposition βk = B
ak, where B is a n × r matrix that binds
the estimates over time and “ak” is a r-dimensional vector that drives the model:
Yk = ZkB
ak + εk. (7.27)
We elaborate on this model form now. While the motivation for the formulation of
model (7.1) is given from economics point of view in the seminal paper by Zellner
(1962), the motivation for model (7.25) comes from financial economics and can be
directly related to the principal component analysis. If Yk denotes the returns of “m”
assets at the kth time unit, then
Yk = Afk + εk, (7.28)
where A is known as the factor loading matrix of order m × r and fk is a r￾dimensional factor where r<m. As shown in Section 2.4.1, the loading matrix and236 7 Seemingly Unrelated Regressions Models With Reduced Ranks
the factors all can be estimated through the eigenvectors of the covariance matrix of
Yk. As noted in Model (2.22),
Yk = ABYk + εk. (7.29)
Thus, the model (7.28) can be written in the form of the reduced-rank regression
model. The model (7.29) can be stated as a model with unknown factors; instead if
“fk” can be related to known predictors, Xk, then
Yk = ABXk + εk (7.30)
resulting in the classical reduced-rank model studied in detail in Chapter 2.
Interestingly in financial economics, the interest has been on relating the factor
loading matrix to asset characteristics that can be both static and dynamic. This
approach is outlined in Connor and Linton (2007) and Connor et al. (2012) and
further extended by Fan et al. (2016) as “projected principal component analysis.”
The discussion that follows next is based on the work of Kelly et al. (2019) that
introduces the concept of Instrumental Principal Component Analysis (IPCA). The
specification of the IPCA model is as follows. Let
Yk = Akfk + εk, Ak = ZkB + ε∗
k . (7.31)
Here Zk is (m × n) time-varying predictors, where each of its rows denotes the
asset-specific variables that change over time and B is m × r matrix that is fixed;
combining the two equations in (7.31), we have the model
Yk = ZkB
fk + εk, (7.32)
which is exactly in the form of (7.11), except that these are “T ” cross-sectional
regressions. Thus, the IPCA model can be estimated using the methodology given
in this chapter. The criterion to be minimized can be stated as
S(θ ) = 1
2T e
(IT ⊗ Γ )e
= 1
2T

T
k=1
(Yk − Zkβk)

Γ (Yk − Zkβk).
(7.33)
The above is possible mainly because of the Kronecker structure of the errors
and because Γ = Σ−1  goes with each equation in (7.27), unlike the SURE
model in (7.11), where the elements of Σ came from different equations in (7.1).
Observe that the GLS estimate of βk is β
k = (Z
kΓ Zk)−1Z
kΓ Yk and β
k ∼
N (βk, (Z
kΓ Zk)−1). The criterion in (7.33) is indeed equivalent to minimizing
(apart from the divisor in (7.33))7.8 Some Recent Developments 237
W = 
T
k=1
(β
k − B
ak)

(Z
kΓ Zk)(β
k − B
ak). (7.34)
The first order equation is
∂W
∂ak
= −B(Z
kΓ Zk)β
k + [BZ
kΓ ZkB
]ak, which leads
to (for a given “B”)
ak = [BZ
kΓ ZkB
]
−1(BZ
kΓ Zk)β
k. (7.35)
Substituting ak in (7.34) leads to
min W = max Tr )

T
1
-
B(Z
kΓ Zk)β
kβ

k(Z
kΓ Zk)B
. -B(Z
kΓ Zk)B
.−1
*
. (7.36)
Remark The above expression when Γ = I is equivalent to Kelly et al. (2019,
p 507, equation (8)). As noted in their paper, a closed-form solution is not possible
mainly because of the second term in (7.36). But if that term B(Z
kΓ Zk)B is
replaced by B(Z
Γ Z)B
, where Z is the average of Zk, then it can be shown that
B can be extracted from the first “r” eigenvectors of 1
T
T
1 (Z
kΓ Zk)β
kβ

k(Z
kΓ Zk)
with respect to Z
Γ Z.
To get the first derivative of W with respect to B
, we write the criterion in terms of
vec(B). Note that
Tr W = 
T
1
β
k(Z
kΓ Zk)β
k − 2

T
1
-
β
k(Z
kΓ Zk) ⊗ a
k
.
vec(B)
+ vec’(B)

T
1

Z
kΓ Zk ⊗ aka
k


vec(B).
We used the matrix result, tr(AZ
BZC) = vec
(Z)(CA ⊗ B
)vec(Z) in arriving at
the above expression. It can be seen for a given “ak,”
vec(B) =
)

T
1

Z
kΓ Zk ⊗ aka
k

*−1 )

T
1

Z
kΓ Zk ⊗ a
k

β
k
*
. (7.37)
The solutions to B and ak can be obtained by alternating (7.35) and (7.37) and
imposing normalization conditions. If we set A
 = (a1,...,aT ) as a r × T matrix,
then the following conditions are made to hold:
A
A
 = Λr and BB = Ir, (7.38)238 7 Seemingly Unrelated Regressions Models With Reduced Ranks
where Λr is a diagonal matrix. Iterative procedure based on the second derivatives
can also be derived as given in Theorem 7.1, and additional details can be found in
Luca et al. (2022) on the properties of the resulting estimators and for an application
on asset pricing models in the finance area. In fact an extended model considered
by Kelly et al. (2019) where the non-time varying intercept-like term is included
amounts to decomposing the m × n coefficient matrix, C = (β1,...,βm) = AB +
α1
n. This can also be addressed using the methodology given in this chapter.
Remark While the research on multivariate regression model we began with in
Chapter 1 has been extensively studied and extended, research on seemingly
unrelated regression models discussed in this chapter warrants further work. These
models have a number of practical applications. They can incorporate time-varying,
unit-varying, as well as fixed characteristics. Adding more variables to model (7.1)
or to (7.27) that are not part of the dimension reduction aspect requires further
study. Note that reduced-rank model of Anderson (Section 3.1) can be identified
through partial canonical correlations. Such elegant interpretations are not available
for the seemingly unrelated regression models. With the two formulations presented
here in this chapter, these models can easily be applied to study the panel data. For
example, the model with time-varying coefficients Yik = X
ikβk + μk + Uik can be
accommodated. Here “μk” unobservable time effect and βk can be assumed to be
random, time-varying but is the same across the cross-sectional units. For the SURE
models, variable selection methods such as LASSO need to be explored and studied.
Some ideas toward this direction are covered in later chapters.Chapter 8
Applications of Reduced-Rank
Regression in Financial Economics
8.1 Introduction to Asset Pricing Models
In previous chapters, we have developed reduced-rank regression models of various
forms, which have wide applications in a variety of contexts in the physical and
social sciences. In this chapter, we focus on the area of financial economics, where
several applications of the reduced-rank regression models arise in a fairly natural
way. Thus, the topics that will be examined in this chapter involve consideration
of the contexts in which financial models arise from economic theories, the form
of the models, and the empirical verification of these models through reduced-rank
regression methods. In earlier chapters, the application of reduced-rank regression
methods was mainly motivated from an empirical dimension reduction aspect,
whereas the use of reduced-rank models presented in this chapter results from
a priori economic theory. With the high volume of financial data that are now
routinely becoming available, these theories can be examined through empirical
tests of the models (see Section 7.8). We point out that, for the most part, only
the basic reduced-rank methods developed in Chapter 2 and the methods related
to cointegrated modeling discussed in Section 5.6 are needed for the presentations
considered here.
Of fundamental interest to financial economists is to examine the relationship
between the risk of a financial security and its return. While it is obvious that risky
assets can generally yield higher returns than risk-free assets, a quantification of
the tradeoff between risk and expected return was made through the development
of the capital asset pricing model (CAPM), for which the groundwork was laid
in the seminal work by Markowitz (1959). A central feature of the CAPM is that
the expected return is a linear function of the risk. The risk of an asset typically
is measured by the covariability between its return and that of an appropriately
defined “market” portfolio. Examples of expected return-risk model relationships
include the Sharpe (1964) and Lintner (1965) CAPM, the zero-beta CAPM of
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8_8
239240 8 Applications of Reduced-Rank Regression in Financial Economics
Black (1972), the arbitrage pricing theory (APT) due to Ross (1976), and the
intertemporal asset pricing model by Merton (1973). The economy-wide models
developed by Sharpe (1964), Lintner (1965), and Black (1972) are based on the
work by Markowitz (1959) that assumes that investors would hold a mean–variance
efficient portfolio. They showed that if investors have homogeneous expectations
and hold efficient portfolios, then the market portfolio will itself be a mean–variance
efficient portfolio. The CAPM equation (8.3), given below, is a direct implication of
the mean–variance efficiency of the market portfolio. The main difference between
the work of Sharpe (1964) and Lintner (1965) and the work of Black (1972) is that
the former assume the existence of a risk-free lending and borrowing rate and the
rates are the same, whereas the latter derived a more general version of the CAPM
in the absence of a risk-free asset. Initially, we focus on the work of Black (1972)
as it readily fits into the reduced-rank regression framework.
Gibbons (1982) developed the Black (1972) CAPM hypothesis through the
multivariate regression model. Suppose there are m assets whose returns are
observed over T time periods, and let rik denote the return on asset i in period
k. The model is specified as follows:
rik = αi + βixk + ik, i = 1, . . . , m, k = 1,...,T, (8.1)
where xk denotes the return on the market portfolio in period k. The coefficient βi
in (8.1) is the “risk” of the ith asset, equal to
βi = Cov(rik, xk)/Var(xk), i = 1, . . . , m,
and the ik are assumed to have zero mean and be uncorrelated over time, with
the random vector k = (1k,...,mk) having positive-definite m × m covariance
matrix  = Cov(k). The above Eqs. (8.1) can be written in the form of a
multivariate regression model, given in Chapter 1, and represent “a statistical
statement rather than one derived from financial theory.” With the zero mean
assumption on the errors in (8.1), we have
E(rik) = αi + βiE(xk), i = 1, . . . , m. (8.2)
The Black (1972) CAPM requires, for the mean–variance efficiency of the market
portfolio, that the following relationship holds between the expected returns and the
risks:
E(rik) = γ0 + βi[E(xk) − γ0], i = 1, . . . , m, (8.3)
where γ0 is the expected return on the “zero-beta” portfolio or any portfolio whose
return is not correlated with the return on the market portfolio. The zero-beta
portfolio is defined to be the portfolio whose returns have the minimum variance of
all portfolios whose returns are uncorrelated with the returns of the market portfolio.
Comparing (8.2) with (8.3), we see that (8.3) implies that the intercepts of the8.1 Introduction to Asset Pricing Models 241
model (8.1) must satisfy the constraints
αi = γ0(1 − βi), for all i = 1, . . . , m, (8.4)
which is used as a basis for empirical testing of the zero-beta CAPM model, that is,
testing the efficiency of the market portfolio. A similar formulation of the problem
of testing the efficiency of the market (or any specified) portfolio is obtained by
Gibbons et al. (1989), but based on the assumptions of the Sharpe (1964) and Lintner
(1965) CAPM. In this case, the regression uses excess returns (returns in excess of
the risk-free rate), and the null hypothesis is simply that all the alphas are zeros.
To connect the constraints (8.4) with a reduced-rank feature, we first observe that
the model (8.1) can be written in a slightly different form as
yik ≡ rik − xk = αi + (βi − 1)xk + ik.
We will then express this in the form of the multivariate regression model in (1.1).
Define Yk = (r1k − xk,...,rmk − xk) and Xk = (1, xk)
, where Yk is the m × 1
vector of “excess” returns on the m assets. Further, write the m×2 coefficient matrix
as
C =
 α1 α2 ··· αm
β1 − 1 β2 − 1 ··· βm − 1

≡ [α, β − 1m],
where 1m is an m × 1 vector of ones. Now the model can be restated as the
multivariate regression model
Yk = CXk + k, k = 1,...,T, (8.5)
where k is the m × 1 vector of random errors, with E(k) = 0 and Cov(k) =  .
Notice that under the constraints (8.4), the coefficient matrix C in (8.5) is expressible
as
C = [α, β − 1m]≡[γ0(1m − β), β − 1m]
= [β − 1m][−γ0, 1] = β∗γ 
, (8.6)
where γ  = [−γ0, 1] and β∗ = β − 1m. Thus, the matrix C is of reduced- rank one,
although there is some slight difference in the reduced-rank setup presented here
compared to the approach for the model discussed in Chapter 2. While the form
of (8.6) is similar to the form C = AB in (2.4), the right-hand factor γ  in this setup
(i.e., C = β∗γ 
) is structured in that a normalization is directly incorporated into
the second element of the vector γ . Recall, also, that the reduced-rank restriction on
C implies that there is one constraint on the matrix C that can be stated as
Cγ ∗ = 0, (8.7)242 8 Applications of Reduced-Rank Regression in Financial Economics
where γ ∗ = (1, γ0) is “structured” as well. A main difference between (8.7)
and (2.2) is that the constraints in (2.2) operate on the rows of the m × n matrix
C, whereas the above constraints operate on the columns of the m × 2 matrix C.
This difference in emphasis is due mainly to the convention adopted in Chapter 2,
assumed for convenience of exposition, that m ≤ n, while m>n = 2 in the present
situation. For either case, methods developed in Chapters 1 and 2 are useful here.
8.2 Estimation and Testing in the Asset Pricing Model
Gibbons (1982) suggested the following iterative procedure for estimation of the
coefficient matrix C in (8.5), subject to the constraints (8.4), which is very much in
the spirit of the partial least squares method. Let Y and X denote the m × T and
2 × T data matrices, respectively.
(i) In the first step, obtain the ordinary LS estimator C
 = YX
(XX
)−1 as given
in (1.7), which is not constrained by (8.4). This yields the LS estimates α and
β
 of α and β. Then obtain  = (1/T )(Y − C
X)(Y − C
X)
, the estimate
of the contemporaneous error covariance matrix. (To avoid singularity, m must
be greater than T , a condition that might not hold; but in the finance setting
because of availability of data on a large number m of securities, one might
typically have m>T .)
(ii) In step two, motivated by (8.4), compute the estimate γ0 = {α
−1  (1m −
β)  }/(1m − β)  
−1  (1m − β)  , a GLS estimate of γ0 obtained by regressing the
αi on the (1 − β
i), i = 1,...,m, with the weights matrix −1  .
(iii) Using (8.3) and the estimate γ0 from (ii), perform m individual “market model”
regressions of rik −γ0 on xk −γ0, without a constant term, to reestimate the βi.
(iv) With the new estimates β
i, iterate the steps in (ii) and (iii) that yield γ0 and the
new β
i until convergence.
In several studies in finance (see Gibbons et al. 1989), following Sharpe (1964)
and Lintner (1965), it is assumed that there exists a riskless asset that is usually
taken as the U.S. Treasury bill rate. For this choice, the value of γ0 is known and
is set equal to the rate of return on the riskless asset. Then testing for the (known)
constraints (8.4) can be done using the results in Chapter 1. For known γ0, therefore
known γ ∗, the constraints (8.7) can be tested through the LR statistic in (1.16)
and the approximate chi-squared distribution theory, or in this case that involves
only a single constraint vector, through Hotelling’s T 2-statistic given in (1.22). This
follows from the same reasoning as in (1.22), and with  = {T /(T − 2)} , the
appropriate test statistic is
T 2 = (γ ∗C

−1
 Cγ  ∗)/γ ∗(XX
)
−1γ ∗, (8.8)
which has the Hotelling’s T 2-distribution with T − 2 degrees of freedom. Thus,
F = {(T − m − 1)/m(T − 2)}T 2 has the F-distribution with m and T − m − 18.2 Estimation and Testing in the Asset Pricing Model 243
degrees of freedom. This is also known as the Wald test discussed by Gibbons et al.
(1989) to test H0 : α = 0. The test is stated as F =


T −m−1 m
 
1 + x2
s2
x

αΣ−1
εε α,
where α is the estimate of α in the regression of excess returns over excess market
returns. The interpretation of the noncentrality parameter under the alternative that
Cγ ∗ = 0 in terms of the financial context has been of some interest in the finance
literature. The constrained ML estimator of C under (8.7) is
C
 = C


I2 − γ ∗γ ∗(XX
)−1
γ ∗(XX
)−1γ ∗

, (8.9)
which follows as a special case of the form given in (1.18) with F1 = Im and
G2 = γ ∗. Note that an alternate direct derivation of the test statistic (8.8) can be
motivated as follows. With γ0 known, model (8.1) can be written in the equivalent
form as (rik −γ0) = α∗
i +βi(xk −γ0)+ik, where α∗
i = αi −γ0(1−βi) ≡ 0 under
conditions (8.4). It follows that if the excess asset returns rik − γ0 are regressed
on the excess market return xk − γ0, the resulting intercepts are expected to be
zero. The traditional multivariate T 2-test for the m intercepts to be zero yields the
statistic in (8.8), noting that Cγ  ∗ = α − γ0(1m − β)  ≡ α∗ is equal to the vector of
LS estimates of the intercepts α∗
i .
When a riskless asset is not assumed to exist, so that γ0 and hence γ ∗ are not
known, we can obtain the constrained ML estimates and test the CAPM hypothesis
of (8.4) using the reduced-rank regression methods developed in Chapter 2. Because
of the rank one structure, and the simple nature of both γ in (8.6) and γ ∗ in (8.7), we
can obtain the ML estimator of γ0 in two ways. From general results in Section 2.3,
the ML estimation criterion leads to minimizing (2.16), and the resulting solutions
are given in (2.17). It follows, using the notation of the problem discussed here, that
the ML estimates of the components β∗ and γ in C = β∗γ  are given by
β
∗ = 1/2  V
1, γ  = V

1−1/2  C,  (8.10)
where V
1 is the (normalized) eigenvector that corresponds to the largest eigenvalue
λ2
1 of the matrix −1/2  C
xxC

−1/2  . These estimates satisfy the normalization
β
∗−1  β
∗ = 1, while γ 
xxγ = λ2
1. Because of the disproportionate dimensions
of Yk and Xk involved (m is typically much larger than n = 2), we suggest that it
would be practical to compute the equivalent ML estimates of β∗ and γ based on
the eigenvector of the 2 × 2 matrix 1/2 xx C

−1  C
1/2 xx . Let V
∗
1 be the (normalized)
eigenvector that corresponds to the largest eigenvalue λ2
1 of the latter matrix. Then
it follows from discussion given toward the end of Section 2.3 that an equivalent
expression for the ML estimates is given by
β
∗ = C
1/2 xx V
∗
1 , γ  = V
∗
1 −1/2 xx , (8.11)
which satisfy the normalization γ 
xxγ = 1, while β
∗−1  β
∗ = λ2
1. By rescaling
the above ML estimate γ in (8.10) or (8.11) to have the form [−γ0, 1]

, we obtain244 8 Applications of Reduced-Rank Regression in Financial Economics
the ML estimate γ0 of γ0. Therefore, the iterative scheme suggested by Gibbons
(1982) to obtain estimators of γ0 and β can be replaced by a one-time eigenvalue
and eigenvector calculation to obtain the ML estimates.
If the focus of estimation is on γ0, this parameter can also be estimated through
estimation of the constraints vector γ ∗ in (8.7). As discussed in Section 2.3, the
ML estimate of the constraint corresponds to obtaining the eigenvector V
2 (or V
∗
2 )
associated with the smallest eigenvalueλ2
2 of the matrix −1/2  C
xxC

−1/2  (or of
the matrix 1/2 xx C

−1  C
1/2 xx ). This can also be motivated in this context through
the T 2-statistic given in (8.8) (e.g., see Anderson, 1951, for a similar argument). For
a given vector γ ∗, the constrained ML estimate of the error covariance matrix 
is easily obtained from the constrained ML estimate C
 given in (8.9) as
 =  +
1
γ ∗−1 xx γ ∗Cγ  ∗γ ∗C

. (8.12)
Since
| |=| ||Im + (γ ∗−1
xx γ ∗)
−1−1/2  Cγ  ∗γ ∗C

−1/2  |
= | |(1 + (γ ∗−1
xx γ ∗)
−1γ ∗C

−1
 Cγ  ∗),
minimizing | | with respect to the unknown constraint vector γ ∗ to arrive at the
ML estimate is equivalent to minimizing the statistic T 2. Therefore, it follows that
the estimate γ ∗ can be obtained through the eigenvector that corresponds to the
smallest eigenvalueλ2
2 of 1/2 xx C

−1  C
1/2 xx . Again, by appropriately rescaling the
components of γ ∗, we obtain the ML estimate γ0 of γ0. In either approach, the
likelihood ratio statistic for testing H0 : rank(C) = 1 is obtained from the results
of Section 2.6 as [T − (m + 3)/2] log(1 + λ2
2) and follows an asymptotic χ2
m−1
distribution under the null hypothesis.
Zhou (1991, 1995) formulated the testing of the asset pricing constraints in (8.4)
as a problem in reduced-rank regression and investigated small sample distributional
properties of the likelihood ratio test (a test essentially based on λ2
2) for testing
rank(C) = 1 versus rank(C) = 2. Shanken (1986) as well as Zhou (1991) derived
the ML estimate of the parameter γ0. The focus of Zhou’s studies, however, was on
testing for the rank of C rather than on estimates of γ0 and β. Zhou (1991) comments
that the accuracy of the ML estimates would be of interest to practitioners in the
finance area, but observes that the standard error of the zero-beta rate estimate γ0 is
not known. Because of the connections with reduced-rank estimation demonstrated
here, we mention that the large sample distribution of the ML estimator γ0 can be
derived using the results in Section 2.5. We state the result.
Result For the model (8.5) where εk’s satisfy the usual assumptions, let the
parameters belong to a compact set defined by the normalization constraints8.2 Estimation and Testing in the Asset Pricing Model 245
β∗Σ−1 εε β∗ = 1 and γ 
Σxxγ = λ2
1. Then √T (γ0 − γ0) ∼ N (0, σ2
γ0 ), where
σ2
γ0 = 1
λ2
1

1 + (μx − γ0)2
σ2
x

.
The proof follows from the discussion in Section 2.5 that the asymptotic variance
of γ is Σ−1 xx + 1
2γ γ 
. Because γ0 = −γ1/γ2 is totally differentiable, applying the
standard asymptotics (see Rao 1973, p. 387), the result follows. To simplify, we use
the condition γ 2
2 [σ2
x + (μx − γ0)2] = λ2
1 that results from the normalized condition
γ 
Σxxγ = λ2
1.
The asymptotic inference on γ0 can be made using the above result. It should be
noted that the asymptotic variance involves the nonzero eigenvalue, λ2
1. The expres￾sion for the asymptotic variance is exactly the same as the one given in Campbell
et al. (1997, Eqn 5.3.8.1, p. 202). It follows from the alternative formulation of the
ML estimation in (8.11) and the normalization condition β∗
Σ−1 εε β∗ = λ2
1. The
quantity (μx − γ0)/σx for a known risk-free rate γ0 is the Sharpe ratio.
It is easy to show using (8.5) with the constraint (8.4), a portfolio formed with
weight vector, w = (w1,...,wm) as w
Yt = w
CXt + w
εt , which results in the
model for portfolio return as rpt = γ0+(w
β)(xt−γ0)−at , where rpt = m
i=1 wirit
and at = w
εt . Thus the structure under Black-CAPM is preserved at the portfolio
level as well. To derive the result, observe that w
C = w
[γ0(1m − β), β − 1m] =
[γ0(1m − w
β), (w
β − 1)] and rearrange the terms.
The methods and results presented for the model (8.1) can easily be generalized
to the multi-beta CAPM, which is often considered in the finance literature. The
market model for the multi-beta CAPM can be written similar to (8.1) as
rik = αi +n−1
j=1
βij xjk + ik, (8.13)
where (x1k,...,xn−1,k) is a (n − 1) × 1 vector of returns on the (n − 1) reference
portfolios, and βi = (βi1,...,βi,n−1) is the (n − 1) × 1 vector of risk measures.
The constraints (8.4) generalize in the multi-beta CAPM to
αi = γ0
⎛
⎝1 −n−1
j=1
βij
⎞
⎠ ≡ γ0(1 − β
i1n−1), i = 1, . . . , m. (8.14)
Similar to the previous case, the model (8.13) can be reexpressed as
yik = rik − 1
n − 1
n−1
j=1
xjk = αi +n−1
j=1

βij − 1
n − 1

xjk + ik,
and hence, it can be written in the multivariate regression form as Yk = CXk + k,
with Xk = (1, x1k,...,xn−1,k)
, and the m × n coefficient matrix C will be of246 8 Applications of Reduced-Rank Regression in Financial Economics
reduced rank n−1 (n<m is assumed) under the constraints of (8.14). The reduced￾rank methods and results from Chapter 2, in particular, results in Section 2.5, readily
extend to this case. Some finite-sample testing procedures related to (8.14) are
discussed by Velu and Zhou (1999).
APT Model An alternate financial theory to the CAPM is the arbitrage pricing
theory (APT) proposed by Ross (1976). A basic difference between the two theories
stems from APT’s treatment of the correlations among the asset returns. The
APT assumes that returns on assets are generated by a number of industry-wide
and market-wide factors; returns on any two securities are likely to be correlated
if the returns are affected by the same factors. Although the CAPM allows for
correlations among assets, it does not specify the underlying factors that induce
the correlations. The APT where the factors are observable economic variables
can also be formulated as a multivariate regression model with nonlinear (bilinear)
parameter restrictions, that is, with reduced-rank structure. The details of the
model as multivariate regression with nonlinear restrictions were first presented
by McElroy and Burmeister (1988), and Bekker et al. (1996) further identified the
restrictions in the APT model as reduced-rank structure and discussed the estimation
of parameters, including the “undersized samples” problem when m>T . In the
APT, it is assumed that the returns on the m assets at the kth time period are
generated by a linear (observable) factor model, with n factors, as
rk = E(rk) + Bfk + k, k = 1,...,T, (8.15)
where rk is an m × 1 vector of returns, fk is an n × 1 vector of measured factors
representing macroeconomic “surprises,” and B is an m × n matrix with typical
element bij measuring the sensitivity of asset i to factor j . The random errors k
are assumed to follow the same assumptions as in model (8.5). The APT starts with
model (8.15) and explains the cross-sectional variation in the expected returns E(rk)
under the assumption that arbitrage profits are impossible without undertaking some
risk and without making some net investment. The fundamental result of APT is that
E(rk) is approximated by
E(rk) = δ0k1m + Bδ,
where δ0k is the (possibly time-varying) risk-free rate of return for period k, assumed
to be known at the beginning of period k, and δ is an n×1 vector of premiums earned
by an investor for assuming one unit of risk from each of the n factors. Combining
the above two models, we have
rk = δ0k1m + Bδ + Bfk + k. (8.16)
Now, we let Yk = rk − δ0k1m, Xk = [1, f 
k]

, and C = [Bδ,B] ≡ B[δ, In].
Then the model (8.16) is in the same framework as (8.5), Yk = CXk + k, with the
constraint on the coefficient matrix C given by8.2 Estimation and Testing in the Asset Pricing Model 247
C
 1
−δ

= 0.
Thus the m × (n + 1) coefficient matrix C has a rank deficiency of one. The vector
of risk premiums δ can be estimated from the eigenvector that corresponds to the
smallest eigenvalue of 1/2 xx C

−1  C
1/2 xx . By normalizing the first element of this
eigenvector as equal to −1, the remaining elements give the estimate δ of δ. The
associated estimator of B is then obtained as B
 = YX∗(X∗X∗)−1, where X∗ =
[X∗
1 ,..., X∗
T ] and the X∗
k = δ + fk ≡ [δ, In]Xk are formed based on the estimator
of δ.
McElroy and Burmeister (1988) suggested an iterative procedure to estimate the
parameters in (8.16). Geweke and Zhou (1996) provide a Bayesian framework for
estimating and making finite sampling inference of the above models. The posterior
distributions of the parameters are estimated via Gibbs sampler. Bekker et al. (1996)
noted that the problem could be formulated as a reduced-rank regression setup as
indicated above. Bekker et al. (1996) also derived the second-order properties of
the estimators of B and δ and discussed the case of undersized samples, that is,
when there are more assets than time periods or m>T . The asymptotic results
can be derived directly through the approach of Silvey (1959) as demonstrated in
Chapters 3 and 4. The case of undersized samples is important to consider in the
finance context because we may often have m>T , and then the unconstrained
estimate  of  will be singular. Two approaches were suggested for this case;
one was simply to set  = Im, which leads to least squares reduced-rank estimates
as discussed in Section 2.3. The second approach is to impose some factor analysis
structure on  , or grouping the assets in terms of industries, thereby reducing the
number of unknown parameters and resulting in a nonsingular estimate of  . The
unknown parameters in  can be estimated from  .
Note that in the model (8.16) above, it is assumed that the risk-free rate δ0k,
although time-varying, is known. It is shown that “δ” the risk premium parameters
associated with the factors “fk” are estimated as a constraint on the regression
coefficient matrix resulting from regressing Yk = rk − δ0k1m on Xk = [1, f 
k]

.
Shanken (1992) and Shanken and Zhou (2007) have studied the estimation of the
model rk = δ01m + Bδ + Bfk + εk, where δ0 is the zero-beta rate that is not known.
For a recent discussion of this model, see Gospondinov et al. (2019). We want to
observe that this model falls in the framework of Anderson’s model discussed in
Chapter 3. The (pricing) restriction resulting from the APT model, where “δ0” is
unknown, is
E(rk) = μr = δ01m + Bδ,
which can also be restated in terms of “α,” the intercept term in the regression mode,
Yk = CXk + εk, as
α = μr − Bμf = δ01m + B(δ − μf ).248 8 Applications of Reduced-Rank Regression in Financial Economics
Note the coefficient matrix “C” is associated with Xk = (1, f 
k) that includes the
intercept term as C = (α, B). The stated restrictions in this case can also be stated
as a constraint on the following expanded matrix:
Gγ = 
1m α B
⎛
⎝
−δ0
1
−(δ − μf )
⎞
⎠ = 0.
Using the results of Chapter 3 and Section 8.2 (see Gospondinov et al. 2019, p. 457),
with sample estimates substituted, the estimate of γ can be obtained from the
eigenvector associated with the smallest eigenvalue of Σ∗ 1
2 xx G
Σ−1GΣ∗ 1
2 xx , where
Σ∗
xx = AΣ−1 xx A with A = [0n, In]

. From the eigenvector, γ , with appropriate
rescaling, we can recover the estimates for δ0 and δ.
8.3 Additional Applications of Reduced-Rank Regression in
Finance
Additional applications of reduced-rank regression models occur in various areas of
finance. In these contexts, the models typically arise as latent variables models, as
discussed in Section 2.1, and they are described in Campbell et al. (1997, p. 446–
448). Hansen and Hodrick (1983) and Gibbons and Ferson (1985) investigated the
time-varying factor risk premiums; Campbell (1987) and Ferson and Harvey (1991)
studied the areas of stock returns, forward currency premiums, international equity
returns, and capital integration; Stambaugh (1988) tested the theory of the term
structure of interest rates. In all these studies, the models involved appear to have
the reduced-rank structure. Because of the diverse nature of the various topics above,
we refrain from discussing them except for the work of Gibbons and Ferson (1985),
which follows somewhat from the models considered earlier.
The Black (1972) CAPM in (8.3) and other related models suffer from some
methodological shortcomings; the expected returns E(rik) are assumed to be
constant over a sufficient period of time, and the market portfolio of risky assets, on
which the return xk is computed, must be observable. Gibbons and Ferson (1985)
developed tests of asset pricing models that do not require the identification of the
market portfolio or the assumption of constant risk premiums. These tests can be
shown to be related to testing the rank of a regression coefficient matrix. We present
only the simpler version of the model.
If γ0 is assumed to be known in (8.3), then the returns rik and xk can be adjusted
by subtraction of this riskless rate of return γ0, and this yields E(r∗
ik) = βiE(x∗
k ),
where r∗
ik = rik − γ0 is the excess return on asset i and x∗
k = xk − γ0 is the excess
return on the market portfolio. If the expected returns are changing over time and
depend on the financial information prescribed by an n-dimensional vector Xk at
time k − 1, a generalization of the above model can be written as8.4 Empirical Studies and Results on Asset Pricing Models 249
E(r∗
ik|Xk) = βiE(x∗
k |Xk), i = 1, . . . , m, (8.17)
where r∗
ik = rik − γ0k and x∗
k = xk − γ0k are excess returns. The above holds for all
assets, hence for i = 1 in particular. Therefore, if β1 = 0,
E(r∗
ik|Xk) = (βi/β1)E(r∗
1k|Xk), (8.18)
which does not involve the expected excess return on the market portfolio. Suppose
the observed excess returns are assumed to be a linear function of the financial
information Xk,
r∗
ik = δ
iXk + ik, (8.19)
where δi is an n × 1 vector of regression coefficients, and suppose the assumptions
on the ik hold as stated earlier. Using i = 1 as a reference asset and substituting
into both sides of (8.18) from (8.19) yield
δi = (βi/β1)δ1, i = 1. (8.20)
Observe that the choice of the reference asset is arbitrary, and the coefficients βi are
not known. The conditions (8.20) lead to a reduced-rank regression setup as follows.
The multivariate form of model (8.19) is given by
Yk = CXk + k, k = 1,...,T, (8.21)
where Yk = (r∗
1k,...,r∗
mk) is the m × 1 vector of excess returns and C =
[δ1,...,δm]
 is an m × n regression coefficient matrix. The conditions (8.20) imply
that C = δ1[1, (β2/β1), . . . , (βm/β1)] and hence that rank(C) = 1. It follows that
the number of independent parameters to be estimated is (m+n−1) instead of mn.
Gibbons and Ferson (1985) also present a more general version of model (8.17).
In this, the excess return on the market portfolio is replaced by excess returns on r
hedge portfolios, which are not necessarily observed or by r reference assets. This
leads to a multivariate regression model as in (8.21) and to the rank condition on
the coefficient matrix that rank(C) = r. Therefore, all the methods that have been
developed in Chapter 2 can be readily used in this setup.
8.4 Empirical Studies and Results on Asset Pricing Models
There exists an enormous amount of literature discussing empirical evidence on
the CAPM. The early evidence was largely supportive of the CAPM, but in the
late 1970s less favorable evidence for the CAPM began to emerge. Because of the
availability of voluminous financial data, the statistical tests of hypotheses related
to the CAPM and other procedures discussed here can be carried out readily. We250 8 Applications of Reduced-Rank Regression in Financial Economics
will briefly survey some empirical results that have been obtained related to models
discussed in this chapter. For additional empirical results, readers may refer to
Campbell et al. (1997).
Gibbons (1982) used the monthly stock returns (rk) provided by the Center
for Research in Security Prices (CRSP), and the CRSP equal-weighted index was
taken as the return on the market portfolio (xk). The time period of 1926–1975
was divided into ten equal five-year subperiods. The number of stock returns was
chosen to be m = 40. Within each subperiod, it is assumed that the parameters of
model (8.1) are stable. The general conclusion obtained from the study was that the
restrictions (8.4) implied by the CAPM were rejected at “reasonable significance
levels in five out of ten subperiods.” Some diagnostic procedures were suggested
to postulate alternative model specifications. Other studies also do not provide
conclusive results. Therefore, it is suggested that in order to analyze financial
data more thoroughly, new models need to be specified, or because the testing of
restrictions is based on large sample theory, the tests must be evaluated through
their small sample properties.
Zhou (1991) used the reduced-rank regression tools for estimation and studied
the small sample distributions of the estimators and associated test statistics. In
particular, Zhou (1991) obtained results on the small sample distribution of the
smallest eigenvalue test statistic λ2 described in Section 8.2. He also considered
the CRSP data, from 1926–1986, but instead of stock returns that may exhibit more
volatility, he used 12 value-weighted industry portfolios. The analysis was again
performed on a five-year subperiod basis. Out of 13 subperiods, the hypothesis (8.4)
was rejected for all except four. He concluded that “the rejection of efficiency is
most likely caused by the fundamentally inefficient behavior of the (market) index.”
The widely quoted paper by Gibbons et al. (1989) also considered empirical
analyses of similar data. Recall that these authors considered the multivariate
Hotelling’s T 2-statistic (8.8) for testing for portfolio efficiency in the models
discussed by Sharpe (1964) and Lintner (1965), where the risk-free rate of return,
γ0 in (8.4), is known. Although their focus was on studying the sensitivity of
the test statistic (8.8), they also considered empirical analyses of monthly returns.
The empirical examples that they presented illustrate the effect that different sets
of assets can have on the outcome of the test. First they reported their analysis
on portfolios that were sorted by their risks. Using monthly returns on m = 10
portfolios from January 1931 through December 1965, through the F-statistic
indicated in Section 8.2, they confirm that the efficiency of the market portfolio
cannot be rejected. For monthly returns data from 1926 through 1982, however,
they constructed one set of m = 12 industry portfolios and another set of m = 10
portfolios based on the relative market value or size of the firms. For industry-based
portfolios, the F-statistic rejected the efficiency hypothesis, but for the size-based
portfolios, it did not. Thus, the formation of portfolios could influence the outcome
of the test procedure. Their overall conclusion is that “the multivariate approach can
lead to more appropriate conclusions,” because in the case of the industry-based
analysis, all 12 univariate t-statistics failed to reject the efficiency hypothesis.8.5 Related Topics 251
Other empirical results reported by McElroy and Burmeister (1988), Gibbons
and Ferson (1985), and Zhou (1995) use the tests for the rank of the coefficient
matrix being equal to one in some form or another. McElroy and Burmeister (1988)
considered monthly returns from CSRP over January 1972 to December 1982 for a
sample of 70 firms. The predictor variables chosen for use in model (8.16) include
four macroeconomic “surprises” and the market return. The macro-factors include
indices constructed from government and corporate bonds, deflation and treasury
bills, and others. These authors focus more on the interpretation of the regression
coefficients of the model (8.16); tests for the rank of the resulting coefficient matrix
were not reported. Gibbons and Ferson (1985) considered daily returns from 1962
to 1980 for several major companies. While the excess returns on these companies
form the response variables, the two predictors used in model (8.19) are taken to be
an indicator variable for Monday, when mean stock returns are observed to be lower
typically than other days of the week, and a lagged return of a large portfolio on
common stocks. The null hypothesis (8.20) is rejected in their analysis. Zhou (1995)
applied reduced-rank methodology to examine the number of latent factors in the
U.S. equity market. For the 12 constructed industry portfolios, monthly data from
October 1941 to September 1986 was examined. Several predictors were included
in the model: returns on equal-weighted index, treasury bill rate, yield on Moody’s
BAA-rated bonds, and so on. The general conclusion reached from the study was
that the resulting regression matrix is of rank one consistent with the original CAPM
hypothesis.
8.5 Related Topics
Finite-Sample Results When there is a riskless asset, as discussed in Section 8.2,
Gibbons et al. (1989) provide an exact finite-sample test of the linear pricing
relationship under normality, and Zhou (1993) extends the test to allow for a more
general elliptical distribution for the asset returns. Because the zero-beta rate is
unobservable and is estimated, it is more difficult to develop a small sample test.
As mentioned in the last section, Zhou (1991) provides a finite-sample test for the
single beta case. For the multi-beta case, Shanken (1986) provides only bounds on
the exact distribution of the likelihood ratio test (LRT). The exact distribution of
the LRT for the multi-beta pricing models is given in Velu and Zhou (1999). It is
far more complex than the case of risk-free rate and is dependent monotonically
on a nuisance parameter. Yet, inferences can be made with a reasonable estimate
of the nuisance parameter. Based on the proposed tests, the efficacy of a portfolio
of two stock indices, the value-weighted and equal-weighted, is examined using
the returns data from January 1926 to December 1994. While most studies reject
efficiency of the single index, it is found that the efficacy of a portfolio of the two
indices cannot be rejected for all of the ten-year subperiods except for the first one.
For more complete details, interested readers can refer to Velu and Zhou (1999). The
finite-sample distribution is also studied by Beaulieu et al. (2013) using simulation-252 8 Applications of Reduced-Rank Regression in Financial Economics
based procedures for testing the mean–variance efficiency when the zero-beta rate
is unknown and for building confidence intervals for the zero-beta rate. These are
shown to be robust and can accommodate a correction for heteroskedasticity and
autocorrelations. They are also robust to distributions beyond multivariate normal
such as multivariate Student-t, normal mixtures, etc.
Shrinking Factor Dimension In this chapter, the models such as (8.13) or (8.22)
are formulated and are used to test pricing restrictions resulting from some a priori
theories. These theories commonly identify a select few factors (such as Fama–
French factors); in practice, a few hundred factors are considered as potential
factors that can affect the stock returns. The key questions then are: how many
factors are needed and how much value they add beyond the usual Fama–French
factors. This investigation neatly falls into the framework of the model in Chapter 2,
where the focus was on dimension reduction, whereas the models considered in
this chapter thus far have been on the complementary aspects of the problem, that
is, estimating and testing the constraints on the coefficient matrix. He et al. (2022)
consider the problem of shrinking the dimension of a large factor space in relation
to the returns via a reduced-rank model, but their formulation assumes a more
general weight matrix, and they use generalized method of moments (GMM) to
estimate the parameters. They follow a two-step approach, where the so-called basis
assets are used to estimate the model, to avoid issues related to large unbalanced
panel data, and the model then is tested on portfolios of assets. The Instrumented
Principal Component Analysis (IPCA) discussed in Section 7.8 is also a dimension
reduction technique, but the focus was on changing firm characteristics, but here the
focus is on time series factors (refer to Eqs. (7.28) and (7.30)). We begin with the
model (7.30) with a constant term:
Yk = α + ABfk + εk. (8.22)
Here B is a r × n full-rank matrix of factor coefficients. This can also be taken
as Anderson’s model (3.3) with D = α and Zk = 1. Note the factors “fk” are
observable.
He et al. (2022) consider GMM approach to estimate the model parameters
in (8.22). In the GMM framework, the model residuals “εk” can have general
heteroskedasticity and can represent other deviations from i.i.d. assumption. Using
Xk = (1, f 
k) as instruments, the moment conditions are
E[hk(α, C)] = 0, hk(α, C) = εk(α, C) ⊗ Xk, (8.23)
where
hT (α, C) = 1
T

T
k=1
hk(α, C) = VecX
ε
T

. (8.24)8.5 Related Topics 253
Here X and ε are the data matrices of predictors and the errors. The estimates of the
model (8.22) are obtained by Hansen (1982).
min
α,C
hT (α, C)
WT hT (α, C), (8.25)
where WT is a m(n + 1) × m(n + 1) positive-definite weighting matrix. The main
difference in the estimation approach in Chapter 2 and GMM is that the errors “εk”
can have a covariance structure beyond independent and identical distribution over
“k.” Because of the complexity of the structure of WT , there is no elegant solution
in terms of eigenvectors, etc., but the estimation can be solved using numerical
methods. If the weight matrix, WT , takes the Kronecker form, WT = (W1 ⊗ W2),
where W1 is “m × m” and W2 is “(n + 1) × (n + 1),” the solution follows the
steps used in Chapter 3. The normalization conditions may differ, but the resulting
solutions are equivalent.
We have mentioned elsewhere (see Chapter 4) the importance of having a
Kronecker structure for the weight matrix (see Section 4.5) that leads to elegant
one-step solution in terms of singular values and the associated eigenvectors of
the weighed coefficient matrix. In the case of the reduced-rank regression model,
note the weight matrix takes the Kronecker’s structure, Σ−1 εε ⊗ Σxx . As noted in
Section 4.5, the solution to the estimation of model parameters in (8.23) reduces to
finding α, A, and B by minimizing (see 3.4)
tr{E[Γ 1/2(Y − ABF − α1
T )(Y − ABF − α1
T )

Γ 1/2]},
where F is the “n × T ” matrix of factors in (8.23). With a simple transformation
with F∗ = F −F1
T and α∗ = α+CF, the above can be decomposed into two parts
as shown in Section 3.1. Apart from a term that does not involve any parameters,
the above is equivalent to minimizing,
tr{E[Γ 1/2(Y−ABF∗)(Y−ABF∗)Γ 1/2]}+tr{E[Γ 1/2(Y−α∗1
T )(Y−α∗1
T )

Γ 1/2]}.
Thus the solution to the reduced-rank part follows from results in Chapter 2. The
above can be shown to result in minimizing, W = (C
 − AB)(F∗F∗)(C
 − AB) +
(Y − α∗)(Y − α∗)
. The solution is elegantly derived due to the Kronecker structure
of the weight matrix, Σ−1 εε ⊗ F∗F∗.
To formulate the solution via GMM, observe that model (8.22) results in Y X =
C∗XX + εX
, where C∗ = [α : AB] and X = [1T , F
]. The covariance matrix
of Vec(εX
) can be general, accommodating heteroskedasticity, etc., but if it takes a
Kronecker form, W1 ⊗ W2, the minimizing criterion is
Vec
(Y X − C∗XX
)(W1 ⊗ W2)Vec(Y X − C∗XX
)
= tr{W1/2
1 (Y X − C∗XX
)W1/2
2 W1/2
2 (Y X − C∗XX
)

W1/2
1 }.254 8 Applications of Reduced-Rank Regression in Financial Economics
Write C∗ = [α : C] and note only “C” is restricted to reduced-rank condition. The
minimizing criterion is then
W∗ = W1/2
1 (Y − CF − α1
T )(X
W2X)(Y − CF − α1
T )W1/2
1
= W1/2
1 (Y − CF∗ − α∗1
T )(X
W2X)(Y − CF∗ − α∗1
T )

W1/2
1 .
(8.26)
Minimizing tr W∗ results in the following: for a given “C,”
α∗ = (Y − CF∗)(X
W2X)1T
1
T X
W2X1T
.
Substituting this back in (8.26) and setting C = AB, where A and B are of lower
rank of “r” matrices, and for a given B, it can be shown that
A
= Y X
W∗
2 XF∗
B
(BF∗X
W∗
2 XF∗
B
)
−1.
Observe that
W∗
2 = (X
W2X)1/2
)
In+1 − (X
W2X)1/21T 1
T (X
W2X)1/2
1
T X
W2X1T
*
(X
W2X)1/2
is an idempotent matrix of rank “n.” Inserting this back into W∗ results in minimiz￾ing (BF∗W∗
2 Y 
W1YW∗
2 B
)(BF∗W∗
2 F∗
B
)−1, which is in the same framework as
in Chapter 2. If we desire the normalization condition as BF∗W∗
2 F∗
B = Λr, a
diagonal matrix, then the rows of “B” matrix are exactly the “r” eigenvectors that
correspond to the “r” largest eigenvalues of BF∗W∗
2 Y 
W1YW∗
2 B
.
We want to conclude this section with some comments. When W2 = In+1, then
the results can be shown to be equivalent to the results for Anderson’s model in
Chapter 3. Inference on the coefficient matrices A and B and the intercept terms
α all can be drawn from the asymptotic results in Chapter 3 (see Theorem 3.2).
Because of specific choices of W1(= Σεε) and W2(= Σxx.z) used in the Anderson
model, stronger result such as minimizing simultaneously all the eigenvalues of W∗
is possible. It is not clear if such a result will hold in the general W1 ⊗ W2 case. The
topic is worth investigating.
8.6 An Application
We use the same data set that was studied in Shanken and Zhou (2007) but extend the
duration. The data contains monthly returns of equally weighted 25 portfolios sorted
by size and book to market from March 1964 to December 2021. In addition, we
have excess market return, SMB and HML factors as well. Thus, we have m = 25,8.6 An Application 255
T = 696, and the C matrix is of dimensions m×4, including the intercept terms. The
dimensions of C matrix being 25 × 4, its elements are too numerous to enumerate
here, and so we just present the descriptives of the estimated coefficients (see Table
8.1) of the following regression model:
rik = αi + β1ix1k + β2ix2k + β3ix3k + ik, i = 1, 2, . . . , m, k = 1, 2,...,T.
(8.27)
Here x1 is the market return-risk-free rate, x2 is SMB, small minus big return spread,
and x3 is HML, high minus low return spread.
The asset pricing restriction resulting from model (8.27) is as stated in Shanken
and Zhou (2007, Eqn 22),
H0 : α = α01m + γ1β1 + γ2β2 + γ3β3, (8.28)
where the γ ’s are called risk premia. This directly leads to the rank condition as
given in (8.14):
H0 : α = γ0(1m − B1n−1), (8.29)
where B is an m × (n − 1) regression coefficient matrix. Taken together with α,
C = [α, B] is of reduced rank (n−1). In this example, we can expect rank(C) = 3.
Extending the estimates as stated in (8.11) for rank 1 case to rank 3, where
C = β∗γ  with β
∗ = C
Σ1/2 xx V
∗, γ  = V
∗Σ1/2 xx . Here V
∗ = [V
∗
1 , V
∗
2 , V
∗
3 ], the
matrix of the three eigenvectors that correspond to the largest three eigenvalues
of Σ1/2 xx C

Σ−1 εε C
Σ1/2 xx . This approximation of C
 by C
 is fairly close with all
the coefficients of the predictors are recovered well. This can be seen from the
descriptive statistics of the coefficients of the reduced-rank matrix, C
, given in
Table 8.2 below.
To estimate γ0, the risk-free rate, we can follow the setup as given in (8.7). With
the C matrix of reduced rank, with a rank-deficit by one, we can estimate γ0 using
the relationship
Cγ ∗ = αγ ∗
1 + Bγ ∗
2 = 0, (8.30)
Table 8.1 Descriptives of
the coefficients in
model (8.27)
α β
1 β
2 β
3
Average 0.352 1.022 0.520 0.305
Std. dev 0.144 0.070 0.502 0.410
Table 8.2 Descriptives of
the coefficients of (8.27) with
rank constraint
α β
1 β
2 β
3
Average 0.422 1.018 0.529 0.302
Std. dev 0.027 0.085 0.502 0.411256 8 Applications of Reduced-Rank Regression in Financial Economics
where γ ∗ is the eigenvector that corresponds to the smallest eigenvalue of
Σ1/2 xx C
Σ−1 εε CΣ1/2 xx . From (8.29), the above equation can be written as
γ0γ ∗
1 (1m − B1n−1 + Bγ ∗
2 ) = 0. (8.31)
An alternative method is to simply use the rank 3 based estimator of α and B
 and
regress α on B
1n−1 to obtain γ0. Such an estimate will also take into account the
variation in α as well. For the data set under consideration, γ0 = 0.419, and the
average risk-free rate of return for the duration is 0.367. Thus the zero-beta return
is higher than the average risk-free rate.
8.7 Cointegration and Pairs Trading
While the focus of this Chapter has been on applications of reduced-rank regression
in the asset pricing area, there are other areas of finance where the models discussed
in this book have natural applications. One such area is pairs trading, an investment
strategy, which assumes that if the fundamental values of two stocks are the
same, then their prices move together and hence, they are cointegrated. These
are sometimes called as “Siamese twins”; examples include, Royal Dutch/Shell
and Unilever NV/PLC. In general there is some evidence that asset prices are
co-integrated. If the price vector, pt , is cointegrated with cointegrating rank, r, it
implies that there are r linear combinations of pt that are weakly dependent; thus
these assets can be taken as somewhat redundant and any deviation of their price,
from the linear combination of the other assets prices are temporary and can be
expected to revert. Here we illustrate the application only with two assets.
A model based cointegration approach to pairs trading is elegantly presented in
Tsay (2010, p. 313). It is assumed that pt = log(Pt) = log(Pricet) follows a random
walk, but the log prices, p1t and p2t are cointegrated. Assume pt follows a VAR(1)
model, pt = pt−1 + t and its error-correction form can be stated as,
pt − pt−1 = ( − I )pt−1 + t . (8.32)
If rank( − I )=1, then it can be written as
 − I = αβ
and wt = β
pt is called the cointegrated series. Writing the model (8.32) as
pt − pt−1 = αwt−1 + t .
Because wt is stationary, the temporary deviations from its mean can be exploited
to make buy and sell decisions between the two assets. The logic is that the price8.7 Cointegration and Pairs Trading 257
deviations are temporary and the processes will revert back to their means. For an
extended discussion of the cointegration applications, see Velu, Hardy and Nehren
(2020) and Farago and Hjalmarsson (2019). The discussion presented in Section 5.7
can be applied to more general settings that can use the information from other asset
characteristics such as volume, volatility, etc.Chapter 9
Partially Reduced-Rank Regression
with Grouped Responses
An important characteristic of high-dimensional data is the presence of intrinsic
group structures. The complex group structures in variables and in units of observa￾tions arise in many important scientific fields and they may carry useful information.
They reflect correlations among the variables within the groups and ignoring the
group structures will lead to inefficient use of available data. For example, in
genetics, the DNA markers are grouped into genes which are grouped into biological
pathways; these pathways are successfully used to detect rare occurrences of certain
health issues in individuals. In macroeconomics, it is postulated that the general
economy is driven by two factors or indices, monetary and real; each index has
its own set of associated variables. Thus the group structures can be of interest for
theoretical reasons as well as for uncovering new rare associations.
In a multivariate response-predictor setup, a widely adopted strategy is to per￾form predictor variable selection for each response variable. While it may address
the predictor group structure, the response group structure is overlooked. This is
best dealt with, in the multivariate regression set up that models the simultaneous
relationships with appropriate constraints on the regression coefficient matrix, that
can represent the block structure. Because in many practical applications, the
number of variables can be much greater than the number of observations available,
some dimension reductions are required to handle such data. The reduced-rank
regression model introduced in Chapter 2 can handle the block structures and the
dimension reduction more naturally. While the focus in Chapter 2 was on mostly on
the predictor variables selection, in this chapter, the partially reduced-rank model is
introduced, where only a subset of response variables may be related to a subset of
predictors.
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8_9
259260 9 Partially Reduced-Rank Regression with Grouped Responses
9.1 Introduction: Partially Reduced-Rank Regression Model
We consider the basic multivariate regression model that relates a set of m
response variables Yk = (y1k,...,ymk) to a set of n predictor variables Xk =
(x1k,...,xnk)
,
Yk = CXk + k, k = 1,...,T, (9.1)
where C is a m×n regression matrix. The m×1 vectors of errors k are assumed to
be distributed as i.i.d. multivariate normal with mean vector 0 and m×m nonsingular
covariance matrix Σ = cov(k). We assume that m ≥ n and m + n<T , but these
will be relaxed in later sections.
Recall that the main feature of the reduced-rank linear model is the reduced-rank
restriction of the form rank(C) = r < min(m, n), which yields the decomposition
C = AB, where A is m×r and B is r ×n. As noted in Chapter 2, one interpretation
provided by this decomposition is that the lower-dimensional set of r predictors
X∗
k = BXk contains all the relevant information in the original set of predictors Xk
for representing the variations in the response variables Yk. Thus the focus was on
mainly dimension reduction of the predictor set.
Standard reduced-rank regression methods may not be adequate to identify
more specialized structures in the coefficient matrix C, however. For example,
suppose the regression structure (as indicated above) of (9.1) were such that the
(m − 1)dimensional subset Y1k = (y1k,...,ym−1,k) has reduced-rank structure
of rank 1, E(Y1k) = α1β
1Xk, and E(ymk) = β
mXk, where β1 and βm are linearly
independent n×1 vectors. Then the model for Yk has reduced-rank structure of rank
2, of the form
E(Yk) =

α1β
1
β
m

Xk =

α1 0
0 1 β
1
β
m

Xk ≡ A∗B∗Xk.
Because A∗B∗ = 
A∗P −1

(P B∗) for any nonsingular 2 × 2 matrix P, standard
reduced-rank estimation of a rank 2 model might not reveal the more specialized
structure that actually exists. There might be considerable gains in estimation of
the model with the more specialized structures specified over estimation of the
general form of rank 2 model. Hence, it would be desirable to have a methodology
to identify the existence of such specialized structures, and to properly account for
it in efficient estimation of regression parameters. Thus, as a variant of the usual
reduced-rank model, we consider a somewhat more general situation as above in
which the reduced-rank coefficient structure occurs for (or is concentrated on) only
a subset of the response variables.
As illustrated, the usual reduced-rank specification may be somewhat restrictive,
since it requires that all response variables have regressions that are expressible in
terms of a lower-dimensional set of linear combinations of the predictor variables
in Xk. In practice, it may be that this feature does not hold for a (small) subset9.2 Estimation of Parameters 261
of the response variables, but for the remaining set of response variables it does,
possibly with a few relevant linear combinations of the predictor variables. In fact, in
such cases the usual reduced-rank procedure may not be very effective or efficient,
because the lower-rank feature of the submatrix of C may not be revealed, if the
requirement is that the reduced-rank specification be determined on the entire set of
response variables. For these cases, the reduced-rank structure should be imposed
on a partitioned (row-wise) submatrix of C for more proper modeling.
Initially, for estimation, it will be assumed that the components of Yk =
(Y 
1k, Y 
2k) have a known arrangement so that the first m1 components Y1k possess a
reduced-rank feature separate from the remaining components Y2k. We partition the
regression coefficient matrix C in (9.1) as
C =

C1
C2

, (9.2)
where C1 is m1 × n and C2 is m2 × n, with m1 + m2 = m, and assume the reduced￾rank feature that
rank(C1) = r1 < min(m1, n) (9.3)
and C2 is of full rank, and the rows of C2 are not linearly related to the rows of C1.
Thus, for now it is assumed that the subset of response variables that is of reduced
rank is known, but the more practical case where the subset is to be determined has
been the subject of much research recently and will be addressed later. Note these
assumptions imply rank(C) = r1 + m2(≤ n) in (9.1). We can write C1 = AB,
where A is a m1 × r1 matrix and B is a r1 × n matrix, both of full ranks, and note
that C has the “overall factorization” C = diag(A, Im2 )[B
, C
2]

. We consider the
estimation of A, B, and hence of C1 and C2, and other related inference procedures
for this “partially” reduced-rank model.
9.2 Estimation of Parameters
We now obtain the maximum likelihood (ML) estimators of parameters for the
model (9.1) with the (partially) reduced-rank restriction (9.3). We represent the
m × T data matrix Y as Y = [Y
1, Y
2]

, where Y1 and Y2 are m1 × T and
m2 × T matrices of values of the response vectors Y1k and Y2k, respectively. The
full-rank or least squares (LS) estimators of C1 and C2 are C
1 = Y1X
(XX
)−1 and
C
2 = Y2X
(XX
)−1, where X = [X1,...,Xn]. We partition the error vectors as
k = (
1k, 
2k) and partition the error covariance matrix Σ = cov(k) such that
Σ11 = cov(1k), Σ12 = cov(1k, 2k), and Σ22 = cov(2k).
We first note that the decomposition C1 = AB is not unique. Therefore we need
to impose some normalization conditions, chosen as follows:262 9 Partially Reduced-Rank Regression with Grouped Responses
BΣxxB = Λ2 and A
Σ−1
11 A = Ir1 , (9.4)
where Σxx = (1/T )XX
, Λ2 = diag 
λ2
1,...,λ2
r1

, and Ir1 denotes an r1 × r1
identity matrix. We select these because in the basic multivariate reduced-rank
regression model, the estimator of A and B can be related to eigenvectors associated
with the “r” largest canonical correlations (see Chapter 2). Thus the number
of free parameters in the regression coefficient matrix structure of the model
is r1 (m1 + n − r1) + m2n compared to mn parameters in the full-rank model.
Also noted that the number of free parameters in the usual reduced-rank model,
which considers only that the rank of the overall matrix C is r = r1 + m2, is
r (m + n − r) = r1 (m1 + n − r1) + m2 (m1 + n − r1). Hence if m1 is relatively
large, there can be a substantial reduction in the number of model parameters.
Assuming the k are i.i.d., following a multivariate normal distribution with mean
vector 0 and covariance matrix Σ, apart from irrelevant constants the log-likelihood
is
L(C1, C2, Σ) =
T
2
	
log
,
,
,
Σ−1
,
,
, − tr 

Σ−1W
 , (9.5)
where W = (1/T )(Y − CX) (Y − CX)
 and ,
,Σ−1
,
, is the determinant of the
matrix Σ−1. Maximizing (9.5) with respect to Σ yields Σ = W. Hence, the
concentrated log-likelihood is L(C1, C2, Σ)  = −(T /2)(log |W| + m). We can
proceed to directly derive the ML estimates of C1 = AB and C2 as values that
minimize |W|, but as an alternative we consider an argument based on a conditional
distribution approach for additional insight.
Consider maximizing the likelihood expressed in terms of the marginal distribu￾tion for Y1 and the conditional distribution for Y2 given Y1. The model for Y1 is of
course the reduced-rank model Y1k = C1Xk + 1k, k = 1,...,T , with parameters
C1 = AB and Σ11 = cov(1k), and the conditional model for Y2, given Y1, is
represented as
Y2k = C2Xk + Σ21Σ−1
11 (Y1k − C1Xk) + ∗
2k ≡ C∗
2Xk + D∗Y1k + ∗
2k (9.6)
with parameters C∗
2 = C2 − Σ21Σ−1
11 C1, D∗ = Σ21Σ−1
11 , and Σ∗
22 = cov(2k) =
Σ22 − Σ21Σ−1
11 Σ12, and cov(1k, 2k) = 0. The two sets of parameters {C1, Σ11}
and {C∗
2 , D∗, Σ∗
22} are functionally independent, so ML estimation can be per￾formed separately on them.
ML estimation for C1 and Σ11 is the same as ML estimation of the standard
reduced-rank model and is equivalent to simultaneously minimizing the eigen￾values of Σ−1/2
11 
C
1 − AB
Σxx 
C
1 − AB
Σ−1/2
11 , where C
 = -
C

1,C

2
. =
YX

XX
−1 is the (full-rank) LS estimate of C, and Σ11 is the upper-left block
of the corresponding estimate Σ = (1/T )(Y−C
X)(Y−C
X)
. From earlier results
in Chapter 2, this yields the ML estimators of A and B as9.2 Estimation of Parameters 263
A
= Σ1/2
11 V
(r1), B
 = V

(r1)Σ−1/2
11 C
1, (9.7)
where V
(r1) = -
V
1,..., V
r1
.
, and V
j is the (normalized) eigenvector that corre￾sponds to the j th largest eigenvalueλ2
j of the matrix R
1 = Σ−1/2
11 C
1ΣxxC

1Σ−1/2
11 .
The ML estimator of C1 is C
1 = A
B
, and the ML estimator of Σ11 under the
reduced-rank structure is given by Σ11 = (1/T )(Y1 − C
1X)(Y1 − C
1X)
.
For the conditional model (9.6), notice that the parameters C∗
2 and D∗ are full
rank by assumption, so ML estimation in the conditional model simply yields the
usual full-rank LS estimates. These can be expressed in a convenient form (e.g., see
Chapter 3) as
C
∗
2 = C
2 − Σ21Σ−1
11 C
1, D∗ = Σ21Σ−1
11 , (9.8)
with Σ∗
22 = Σ22−Σ21Σ−1
11 Σ12. The MLE of C2 = C∗
2 +D∗C1 can then be obtained
as
C
2 = C
∗
2 + D∗C
1 = C
2 − Σ21Σ−1
11 (C
1 − C
1). (9.9)
The MLEs of Σ21 and Σ22 can be obtained accordingly, from Σ21 = D∗Σ11 =
Σ21Σ−1
11 Σ11 and
Σ22 = Σ∗
22 + Σ21Σ−1
11 Σ12 = Σ22 − Σ21Σ−1
11 Σ12 + Σ21Σ−1
11 Σ11Σ−1
11 Σ12.
Thus the MLEs C
1 and C
2 can be obtained using matrix routines for eigenvector
(or singular value) decompositions in a non-iterative fashion. The ML estimation
strategy essentially corresponds to the usual reduced-rank estimation to obtain C
1
followed by “covariance-adjustment” estimation (9.9) to obtain C
2, where the LS
estimator C
2 is adjusted by the “covariates” Y1 − C
1X.
Note that C
1 − C
1 = (I − P
1)C
1 and C
2 − C
2 = Σ21Σ−1
11 (C
1 − C
1) =
Σ21Σ−1
11 (I − P
1)C
1, where P
1 = Σ1/2
11 V
(r1)V

(r1)
Σ−1/2
11 is an idempotent matrix
of rank r1. Hence, C
 − C
 = Q(I  − P
1)C
1, where Q = [Im1 , Σ−1
11 Σ12], and then
from basic results we get the maximum likelihood estimate,
Σ = Σ + (C
 − C)  Σxx (C
 − C)  
= Σ + Q(I  − P
1)C
1ΣxxC

1(I − P
1)

Q
= Σ + QΣ1/2
11 	
I − V
(r1)V

(r1)

R
1
	
I − V
(r1)V

(r1)

Σ1/2
11 Q
. (9.10)
Observe the equivalence of (9.10) to (2.18) in the context of the model studied in
Chapter 2.264 9 Partially Reduced-Rank Regression with Grouped Responses
9.3 Test for Rank and Inference Results
We consider the likelihood ratio (LR) test of the hypothesis H0 : rank(C1) ≤ r1.
The LR test statistic for testing rank(C1) = r1 is λ = UT /2, where U = |S|/|S1| ≡
|Σ|/|Σ|, S = (Y − C
X)(Y − C
X) is the residual sum of squares matrix from
fitting the full-rank model, while S1 = (Y − C
(r)X)(Y − C
(r)X) is the residual
sum of squares matrix from fitting the model under the rank condition on C1.
Here C
(r) denotes the estimate of C under rank condition (9.3). It is known that
|Σ|=|Σ11||Σ∗
22|=|Σ11||Σ∗
22| and |Σ|=|Σ11||Σ∗
22|, where Σ∗
22 ≡ Σ∗
22 since
reduced-rank estimation in the model for Y1 does not affect the LS estimation for the
conditional model (9.6). Therefore, we have U = |Σ11|/|Σ11|. It follows that the LR
testing procedure is the same as in the usual reduced-rank regression model for Y1
and does not involve the response variables Y2. Therefore, the criterion λ = UT /2
is such that (see Section 2.6, eqn (2.40))
− 2 log(λ) = T m1
j=r1+1
log 

1 +λ2
j

= −T m1
j=r1+1
log 

1 − ρ2
j

, (9.11)
where λ2
j , j = r1 + 1,...,m1 are the (m1 − r1) smallest eigenvalues of R
1, and
1 +λ2
j = 1/(1 − ρ2
j ), where the ρ2
j are the squared sample canonical correlations
between Y1k and Xk (adjusting for sample means if constant terms are allowed for).
Then (9.11) follows asymptotically the χ2
(m1−r1)(n−r1) distribution under the null
hypothesis (see Anderson (1951), Theorem 3). A simple correction factor for the LR
statistic in (9.11), to improve the approximation to the χ2
(m1−r1)(n−r1) distribution, is
given by
M = − 2 {[T − n + (n − m1 − 1)/2] /T }log(λ)
= − [T − n + (n − m1 − 1)/2] m1
j=r1+1
log 

1 − ρ2
j

.
This approximation is known to work well when T is large (see Anderson 1984b,
Chapter 8).
The alternative hypothesis in the above testing procedure is that the matrix C is
of full rank. There may be situations where r1 + m2 = r < min(m, n) so that the
matrix C would still have reduced rank r to begin with. We might want to test the
subset reduced-rank model assumptions of (9.2)–(9.3) against the alternative of the
usual reduced-rank model, rank(C) = r = r1 +m2. The form of the LR statistic for
this test can readily be developed, and
− 2 log(λ) = −T
⎡
⎣ m1
j=r1+1
log 

1 − ρ2
j

− m
j=r+1
log 

1 − ρ2
j

⎤
⎦ , (9.12)9.3 Test for Rank and Inference Results 265
where ρ2
j , j = r + 1,...,m, are the (m − r) ≡ (m1 − r1) smallest squared sample
canonical correlations between Yk and Xk, with −2 log(λ) distributed as χ2
m2(m1−r1)
asymptotically.
Concerning distributional properties of the ML estimators C
1 and C
2, the approx￾imate normal distribution and approximate covariance matrix of the reduced-rank
estimator C
1 follow directly from results by Anderson (1999a) (see Sections 2.3
and 2.5). In particular, the results from Anderson (1999a, p. 1147–1148) imply that
the large sample (as T → ∞) approximate covariance matrix of C
1 is given by
cov[vec(C
1)] = Σ11 ⊗ 
XX
−1 −

Σ11 − A


A
Σ−1
11 A
−1
A

⊗
	
XX
−1 − B 
B 
XX

B
−1 B

. (9.13)
For the general case, arguments similar to those by Ahn and Reinsel (1988)
(also see Sec. 3.5) establish that the asymptotic covariance matrix of the joint ML
estimator γ = [vec(C

1), vec(C

2)]
 is given by
cov (γ ) = M
	
M


Σ−1 ⊗ XX

M
−1
M
, (9.14)
where M = diag 
M1, Im2n

and M1 = ∂γ1/∂θ
, with γ1 = vec(C
1) ≡ vec(B
A
)
and θ = [vec(A
)
, vec(B
)
]

. It can be verified that this approach yields the same
result for cov[vec(C

1)] as the expression given in (9.13) and, furthermore, that we
obtain the asymptotic expression
cov[vec(C

2)] = Σ∗
22 ⊗ (XX
)
−1 + (Σ21Σ−1
11 ⊗ In)cov[vec(C

1)](Σ−1
11 Σ12 ⊗ In),
(9.15)
where cov[vec(C
1)] is given by the expression in (9.13), and Σ∗
22 = Σ22 −
Σ21Σ−1
11 Σ12. Consider, for instance, the extreme case in which r1 = rank(C1) is
taken as 0, so that C1 = 0 and estimation of C1 is not involved. Then, in (9.9),
C
2 = C
∗
2 and (9.15) collapses to cov[vec(C

2)] = Σ∗
22 ⊗ (XX
)−1. This can be
recognized as a familiar result in the context of “covariance-adjustment,” where in
this case the entire set of response variables in Y1k would be used as covariates
for Y2k since C1 = 0 implies that the complete vector of responses Y1k is entirely
unrelated to Xk.
For the covariance matrix of C
2, it may also be instructive to mention a more
direct argument. Since Σ is consistent for Σ, we can write the ML estimator C
2 as
C
2 = C
2 − Σ21Σ−1
11 (C
1 − C
1) = C
2 − Σ21Σ−1
11 (C
1 − C
1) + op


T −1/2

= (C
2 − Σ21Σ−1
11 C
1) + Σ21Σ−1
11 C
1 + op


T −1/2

= HC
 + Σ21Σ−1
11 C
1 + op


T −1/2

,266 9 Partially Reduced-Rank Regression with Grouped Responses
where H =
	
−Σ21Σ−1
11 , I 
is such that HΣH = Σ∗
22. Because (C
2−Σ21Σ−1
11 C
1)
and C
1 have zero covariance, it follows that the two terms in (9.3) that comprise
C
2 are uncorrelated (independent) asymptotically. So we obtain the asymptotic
covariance matrix of C
2 using (9.3) from cov[vec(C

2)] = cov[vec(C

H
)] +
cov[vec(C
1Σ−1
11 Σ12)], the result in (9.15).
Finally, we can compare the distributional properties of the ML estimators
C
1 and C
2 under the subset reduced-rank model assumptions of (9.2)–(9.3) with
those for ML estimators obtained under the usual or standard reduced-rank model
assumptions. As noted in 9.1, (9.2)–(9.3) imply that rank(C) = r1 + m2 = r
in (9.1) with an overall factorization as C = diag(A, Im2 )[B
, C
2]
 ≡ A∗B∗. When
r = min(m, n) there is no reduced rank overall and one would consider the usual
LS estimator C
 with cov[vec(C

)] = Σ ⊗(XX
)−1. When r < min(m, n), however,
one may still obtain the ML estimator of C for the usual reduced-rank model,
denoted as C = A∗B∗ with components C1 and C2. Using similar methods as
previously, it can be shown that the asymptotic covariance matrices of vec(C
1) and
vec(C
2) are of the same form as the ML estimators in (9.13) and (9.15) under the
subset reduced-rank model, but with (XX
)−1 − B
∗(B∗(XX
)B
∗)−1B∗ in place of
(XX
)−1−B
(B(XX
)B
)−1B. These results can thus be directly compared with the
results in (9.13) and (9.15) to indicate the increase in the covariance matrix relative
to the subset reduced-rank model.
9.4 Procedures for Identification of Subset Reduced-Rank
Structure
In practice, we typically would not know or be able to specify a priori the subset
of response variables for which (9.2)–(9.3) hold. The procedure of examining all
possible subsets of the response variables for tests of rank, although will result
in an optimal solution, involves tedious calculations. For each subset, we need
to calculate the canonical correlations or the eigenvalues of R
1. Even for modest
values of m, this approach leads to excessive calculations because it involves
2m − m − 1 subset calculations. For the example we illustrate below, m = 6 and
we need to consider fifty-seven sets of canonical correlations. As an alternative,
similar to the case of multiple regression, we suggest an exploratory procedure,
using “Stepwise Backward Elimination” of responsive variables, one in each step.
It must be observed that the two concepts are related as they both involve testing for
redundancy. Seber (1984, Section 10.1.6) discusses procedures for selecting the best
subset of the responsive variables in the case of multivariate regression. We briefly
describe them below.
Consider the regression model (9.1) and suppose we want to test the hypothesis
H0 : F C = 0, where F is a known full-rank matrix that provides for restrictions
in the coefficients of C across the different response variables. Assume that the9.4 Procedures for Identification of Subset Reduced-Rank Structure 267
dimension of F is m∗ ×m. The LR test of H0 : F C = 0 is based on the eigenvalues,
λ2
i , of S−1 ∗ H∗, where S∗ = FSF and H∗ = FC(XX ˜ 
)C˜
F and S = T Σ˜ . The
test statistic is M = −(T − n + 1
2 (n − m∗ − 1))log U which has an approximate
χ2
nm∗ distribution under H0, where U−1 = |I + S−1 ∗ H∗| = m∗
i=1(1 + λ2
i ). If we
want to eliminate the y-variables, we want to consider those y-variables that make
an insignificant contribution to the test of H0. We have observed elsewhere (see
Section 1.3) that the hypothesis that the rank of C is r is equivalent to the above H0,
where F is unknown.
The hypotheses H0 : F C = 0 can also be tested by considering a set of
sequential hypotheses as given in Hawkins (1976). The U statistic can be expressed
as U = m
i=1 t2
ii, where the t2
ii’s are independent and follow, in small samples, Beta
distributions. The quantity t2
ii measures the strength of the relationship between yi
and X after adjusting for y1,...,yi−1 and generally small values of t2
ii indicate a
significant relationship. The connection between Beta and the F-distribution yields
that
Fi(m∗, n − m∗ − i + 1) = n − m∗ − i + 1
m∗ ·
1 − t2
ii
t2
ii
(9.16)
statistics can be used to test the hypothesis H0. If the H0 is true, then each Fi is
nonsignificant. As Uk = k
i=1 t2
ii is the test statistic for H0 based on k-variables, a
criterion for finding the best subset of “k” variables is to find the subset that mini￾mizes Uk. To obtain the overall best subset, the calculation requires 2m −1 values of
Uk. This is not feasible if m is large and an alternative is to use a stepwise method.
Before we present the backward-elimination method observe that t
2
ii ≤ 1 and
t2
ii = ˜sii/s˜hii, where s˜ii = sii − s
i−1S−1
i−1si−1. The quantities sii is the ith diagonal
element of S and Si−1 is the leading (i−1)×(i−1) submatrix of S. Similarly, shii is
the ith diagonal element of S+H = S+SF
(F SF
)−1FC(XX ˜ 
)C˜
F
(F SF
)−1F S
and other items can be defined similarly. The decision on inclusion or exclusion of
a variable is based on examination of t2
ii. Observe that in the stepwise selection
procedure suggested in the literature includes variables for which t2
ii is small. In
Hawkins (1976), it is suggested that we start with a variable “i” for which t2
ii is
minimum. If this quantity is sufficiently small, then the ith variable is included in
step 1 and this variable is swept out of S and S + H.
We want to describe another procedure similar to above but that assumes that the
constraints (F-matrix) are not known. In (9.1) with m>n, however, there will exist
m−n linear combinations 
jYk such that 
jC = 0. When (9.2)–(9.3) holds, m1−r1
of the vectors j can take the form 
j = [
1j , 0
], with 1j such that 
1jC1 = 0. As
described in Chapter 2, the j can be estimated by j = Σ−1/2V
j , j = n+1,...,m,
where the V
j are normalized eigenvectors of the matrix R
 = Σ−1/2C
ΣxxC

Σ−1/2
associated with its m−n smallest (zero) eigenvalues. If a certain number (m1−r1 >
m1 − n) of the estimates j satisfy 
j ≈ [
1j , 0
], for a particular partition of the268 9 Partially Reduced-Rank Regression with Grouped Responses
response variables, then this feature can serve as a preliminary tool to identify the
existence and nature of the partially reduced-rank structure for C.
If desired, one can also systematically examine all possible subsets of the
response variables for each given dimension m1 < m, and compute the LR
statistics for tests of rank and other summary statistics for each subset case. An
information criterion such as AIC could then be used to select the “best” subset
of response variables that exhibits the most desirable (partially) reduced-rank
regression features. The procedure of examining all possible subsets of the response
variables for tests of rank and other features may not be practical or desirable
when the dimension m is relatively large. Thus we suggest and describe briefly
a stepwise “backward-elimination” procedure. In this, we start with all m(≥ n)
response variables included in Y1k ≡ Yk and carry out the LR test procedure for
reduced rank. Assume that r is identified as the rank of the overall coefficient matrix
C by the LR test at this stage, that is, r is the smallest value such that the hypothesis
H0 : rank(C) ≤ r is not rejected by the LR test procedure. Then we consider
each of the m distinct subsets of Yk, of the form (y1k,...,yi−1,k, yi+1,k,...,ymk)
,
obtained by excluding one response variable (the ith variable) at a time. For
each (m − 1)-dimensional subset we calculate the LR statistic for testing the
corresponding hypothesis that rank(C1) = r − 1. If yi
k is the response variable
that yields the smallest value of the LR statistic for testing rank(C1) = r − 1 and if
this value leads to not rejecting this null hypothesis, then the subset Y1k with variable
yi
k excluded is chosen. After excluding yi
k, we test if it is reasonable to reduce the
rank of C1 without discarding any more y-variables. At the next step we consider
each of the m − 1 distinct subsets of this Y1k obtained by excluding a remaining
response variable one at a time. For each (m − 2)-dimensional subset we calculate
the LR statistic for testing the corresponding hypothesis rank(C1) ≤ r −2. If yik is
the response variable that yields the smallest value of the LR test statistic and if this
leads to not rejecting this null hypothesis, then the new (m − 2)-dimensional subset
Y1k with variable yik excluded (in addition to the previously excluded variable yi
k)
is chosen. The stepwise procedure continues until no further response variables
can be “eliminated.” The performance of these procedures needs to be formally
evaluated.
9.5 Illustrative Examples
In this section we present two numerical examples to illustrate the (partially)
reduced-rank methods. The first example involves macroeconomic time series data
for which consideration of subset structure leads to further rank reduction. The
second example involves chemometrics data for which ignoring the subset structure
leads to no rank reduction.
UK Macroeconomic Data We now consider UK macroeconomic data presented in
Chapter 4. Recall that the relationships between endogenous (response) variables9.5 Illustrative Examples 269
and exogenous (prediction) can be taken to reflect the demand side of the macrosys￾tem of the UK economy. The quarterly observations are for the years 1948–1956
and after seasonal adjustment of some series; the plots are presented in Section 4.2.
From preliminary LS regression of each response variable yik on the predictor
variables xik, some moderate degree of autocorrelation is noted in the residuals,
particularly for the second (consumption of food, etc.) and third (total unemploy￾ment) response variables. Therefore, based on LS estimation of a model which
included a lagged response variable in these two equations, in the analysis described
below we use the “adjusted” response variables y∗
2k = y2k + 0.6y2,k−1 and y∗
3k =
y3k − 0.635y3,k−1 to account for autocorrelation. Because of these adjustments,
results presented here will differ from the numerical results in Section 4.7.
Now let Yk = (yik, y∗
2k, y∗
3k, y4k, y5k) and xk = (x1k,...,x5k)
. For convenience
of notation, let Y and X denote the 5 × 36 data matrices of response and predictor
variables, respectively, after adjustment by subtraction of overall sample means. The
least squares estimate of the 5 × 5 regression coefficient matrix C is
C
 = YX
(XX
)
−1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
5.5010∗ −0.8392∗ 0.3044 −0.8910∗ 2.0452∗
1.9134∗ −0.1506 0.2745∗ −0.5852∗ 0.9334∗
−2.7096 0.6824 −1.2594∗ 2.4762∗ −2.5368∗
4.4956∗ −0.5684 0.4521 −0.8408∗ 1.5797∗
8.9506∗ −0.1536 0.4797 −0.4363 −0.1680
⎤
⎥
⎥
⎥
⎥
⎥
⎦
,
where the entries with asterisk indicate that the usual t-ratios of these estimates are
greater than 1.65 in absolute value. The ML estimate Σ = (1/36)(Y − C
X)(Y −
C
X) of the 5 × 5 covariance matrix Σ of the errors k has diagonal elements
σjj , j = 1,..., 5, given by 10.0280, 2.7598, 69.5145, 25.1619, 34.1522, with
log(|Σ|) = 13.5484.
From the values and significance of the elements in C
, the regression coefficients
corresponding to certain of the response variables yik share strong similarities. For
example, possible similarities among y1k, y∗
2k, and y4k indicate that reduced rank
may be possible for the set of response variables and/or for some subset of these
variables. We perform LR tests of rank(C) ≤ r for r = 4, 3, 2, 1. For reference,
the squared sample canonical correlations between Yk and Xk (adjusting for sample
means) are ρ2
1 = 0.96921, ρ2
2 = 0.40003, ρ2
3 = 0.31975, ρ2
4 = 0.11723, and
ρ2
5 = 0.01675. For r = 3 the LR test gives a chi-squared test statistic value of
M = 4.176 with 4 degrees of freedom, so a hypothesis of reduced rank of three
or less for C is clearly not rejected. For r = 2, the value M = 15.543 with 9
degrees of freedom is obtained, which hints that a reduced rank of two could even
be possible. For the present, however, we adopt the more conservative conclusion
that rank(C) ≤ 3 with r = 3.
The ML estimates of the matrix factors A
 and B
 in C
 = A
B
 under the rank 3
model are obtained in the standard way, similar to (9.7). These estimates are270 9 Partially Reduced-Rank Regression with Grouped Responses
A
 =
⎡
⎣
2.144∗ 1.292∗ −0.574∗ 1.966∗ 2.230∗
0.907 −0.001 −3.829∗ 1.044 4.964∗
0.631 0.785∗ −7.272∗ −0.073 −2.049
⎤
⎦ ,
B
 =
⎡
⎣
1.845 −0.187 0.125 −0.303 0.669
0.875 0.015 0.090 −0.069 −0.171
−0.274 −0.060 0.117 −0.273 0.344
⎤
⎦ ,
and the corresponding ML estimate Σ of the error covariance matrix Σ has diagonal
elements σjj , j = 1,..., 5, given by 10.5321, 2.8166, 69.6403, 25.6354, 34.1685,
quite similar to the full-rank LS results, with log(|Σ|) = 13.6900. For reference,
ML estimates under the rank 2 model (r = 2) would merely consist of the first
two columns and first two rows of A
and B
, respectively. Notice that, in particular,
from the estimate A
 there may still be some strong similarities among its rows
of coefficients, except that the third row of estimates, corresponding to response
variable y∗
3 , does seem to show substantial differences from the remaining rows.
We examine the estimated vectors 
j = V

jΣ−1/2, j = 4, 5, associated with the
“near” zero eigenvalues λ2
j = ρ2
j /(1 − ρ2
j ) of R
 = Σ−1/2C
ΣxxC

Σ−1/2 (such
that 
jC
 ≈ 0), which are 
4 = [−0.3362, 0.4722, 0.0029, −0.0199, 0.0678]
and 
5 = [0.1334, 0.0432, 0.0109, −0.2004, 0.0262]. These vectors have the
feature that coefficients corresponding to the response variable y∗
3 are close to zero,
there are only moderate values corresponding to variable y5, and some relatively
large nonzero coefficients corresponding to variables y1, y∗
2 , and y4. Based on
the discussion on procedures to identify subset reduced-rank structure, the above
features are highly suggestive of a further reduced-rank structure for the coefficient
matrix associated with variables y1, y∗
2 , y4, and y5, with variable y∗
3 excluded. The
stepwise backward-elimination procedure discussed earlier also performs quite well
in revealing this feature in this example. In particular, at the first stage the LR
statistic M for testing rank(C1) ≤ 2 gives a relatively small and nonsignificant
value of 4.363 with 6 degrees of freedom, when the single response variable y∗
3k is
excluded from Yk, and this LR test statistic value is smaller than when any of the
other single response variables yik, i = 3 is excluded.
We rearrange the response variables with Y1k = (y1k, y∗
2k, y4k, y5k) and Y2k =
(y∗
3k), and let C1 denote the upper 4 × 5 submatrix of C corresponding to the
(rearranged) response variables in Y1k. As indicated, the LR test of H0 : rank(C1) ≤
2 gives the test statistic value of M = 4.363 with 6 degrees of freedom, whereas
the LR test of H0 : rank(C1) ≤ 1 gives M = 18.542 with 12 degrees of freedom.
The hypothesis of reduced rank of two for C1 is clearly acceptable, while reduced
rank of one might also be plausible but again the more conservative value r1 = 2 is
taken. ML estimates of the factors A
and B
in C
1 = A
B
for this subset reduced-rank
model under r1 = 2 are obtained from (9.7) and, in particular, the estimate A
is9.5 Illustrative Examples 271
A
 =

2.372∗ 1.429∗ 2.172∗ 2.475∗
0.391 −0.439 0.837 5.260∗

.
From these results we see that the coefficient estimates for the second predictive
factor (second column of A
) are generally small (and nonsignificant) for response
variables y1k, y∗
2k, and y4k, but much more substantial for the last response variable
y5k. This suggests an even further reduced-rank feature for the coefficient matrix
associated with y1, y∗
2 , and y4, with variable y5 excluded (in addition to the
previously exclude y∗
3 ). Thus we entertain continuation of the backward-elimination
procedure for reduced rank, leading to eliminating y5k and retaining only y1k, y∗
2k,
and y4k in Y1k, and identification of coefficient structure of rank one for this subset.
Finally, we rearrange the response variables with Y1k = (y1k, y∗
2k, y4k)
, Y2k =
(y∗
3k, y5k)
, and let C1 denote the upper 3 × 5 submatrix of C corresponding
to the (rearranged) response variables in Y1k. The squared sample canonical
correlations between Y1k and Xk (adjusting for sample means) are ρ2
1 = 0.96244,
ρ2
2 = 0.16460, and ρ2
3 = 0.02684. The LR test of H0 : rank(C1) ≤ 1
gives M = 6.315 with 8 degrees of freedom, so the hypothesis of reduced
rank of one for C1 is clearly acceptable. We obtain the ML estimate C
1 = A
B

under this rank one model, with A
 = [2.3808∗, 1.4350∗, 2.1792∗]
 and B
 =
[1.7153, −0.1984, 0.1738, −0.3951, 0.7226]. The associated reduced rank one
ML estimate C
1 rather accurately represents the corresponding LS estimates of C,
displayed previously as the first, third, and fourth rows of C
 in the original ordering
of response variables. The associated ML estimate of C2 (last two rows of C under
rearrangement) is obtained from (9.9) as
C
2 =

−2.4660 0.6212 −1.2741 2.4845 −2.4863
7.7977 0.1343 0.5454 −0.4755 −0.4024
.
The corresponding ML estimate Σ of the error covariance matrix Σ has diagonal
elements, corresponding to the original ordering of the response variables, of
10.6964, 2.8908, 69.5369, 26.3046, 34.6696, with log(|Σ|) = 13.7554, which
are close to values from Σ under (full-rank) LS estimation and from the earlier
“general” reduced rank 3 model for C. Taking into account the magnitudes of
variation of the five predictor variables and (approximate) standard errors of
elements of B
, the single predictive index variable x∗
1k = BX k in the above rank
one subset model is composed most prominently of strong contributions from x1k,
x4k, and x5k, with much lesser relative weight given to x2k and x3k. Graphical
illustration in support of the reduced-rank one model feature can be provided by
scatter plots of each of the response variables against the single predictive index
variable x∗
1k = BX k determined from the rank one modeling of the coefficient
matrix C1 for the variables y1, y∗
2 , and y4. Such illustration further confirms that
each of y1, y∗
2 , and y4 has a quite strong linear relationship with the single index
x∗
1 , whereas response variables y∗
3 and y5 (especially y∗
3 ) do not show as strong of272 9 Partially Reduced-Rank Regression with Grouped Responses
relationship with x∗
1 because they are also influenced by other factors within the set
of predictor variables. This sort of exploration needs further investigation.
Chemometrics Data We consider again the multivariate chemometrics data from
Skagerberg et al. (1992) that was analyzed in Chapter 3. The partial least squares
(PLS) multivariate regression modeling of the data was used both for predicting
properties of the produced polymer and for multivariate process control. The data
were also considered by Breiman and Friedman (1997) and Reinsel (1999) to
illustrate the relative performance of different multivariate prediction methods. In
Chapter 3, we used the data to illustrate the use of partial canonical correlation
methods in reduced-rank modeling.
Recall that the data set consists of T = 56 multivariate observations, with
m = 6 response variables and 22 “original” predictor (or process) variables. The
response variables are the following output properties of the polymer produced:
y1, number-average molecular weight; y2, weight-average molecular weight; y3,
frequency of long chain branching; y4, frequency of short chain branching; y5,
content of vinyl groups in the polymer chain; y6, content of vinylidene groups in the
polymer chain. The process variable measurements employed consist of the wall
temperature of the reactor (x1) and the feed rate of the solvent (x2) that also acts
as a chain transfer agent, complemented with 20 different temperatures measured
at equal distances along the reactor. For interpretational convenience, the response
variables y3, y5, and y6 were rescaled by the multiplicative factors 102, 103, and
102, respectively, for the analysis presented here, so that all six response variables
would have variability of the same order of magnitude. The predictor variable x1 was
also rescaled by the factor 10−1. The temperature measurements in the temperature
profile along the reactor are expected to be highly correlated and therefore methods
to reduce the dimensionality and complexity of the input or predictor set of data
may be especially useful. In Chapter 3, we used partial canonical correlation
analysis between the response variables and the 20 temperature measurements,
given x1 and x2, to exhibit that only the first two (partial) canonical variates of the
temperature measurements were necessary for adequate representation of the six
response variables y1 through y6. We denote these two (partial) canonical variates
of the temperature measurements as x3 and x4, and suppose that these two (partial
canonical) variables together with the two original variables x1 and x2 represent the
set of predictor variables available for modeling of the six response variables, for
purposes of illustration of the partially reduced-rank modeling methodology.
We consider a multivariate linear regression model for the kth vector of responses
of the form
Yk = D + CXk + k, k = 1,...,T, (9.17)
with T = 56, where Yk = (y1k,...,y6k) is a 6 × 1 vector, Xk = (x1k,...,x4k) is
a 4 × 1 vector (hence n = 4), and D allows for the 6 × 1 vector of constant terms
in the regression model. For convenience of notation, let Y and X denote the 6 × 56
and 4 × 56 data matrices of response and predictor variables, respectively, after9.5 Illustrative Examples 273
adjustment by subtraction of overall sample means. Then the least squares estimate
of the 6 × 4 regression coefficient matrix C is
C
 = YX
(XX
)
−1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
−0.3686 5.4768 0.0396 0.0678
(0.0415) (0.0779) (0.0030) (0.0089)
−0.2129 27.6520 0.1080 0.0378
(0.1396) (0.2621) (0.0101) (0.0301)
−1.2063 −0.8007 −0.0471 0.4525
(0.1469) (0.2757) (0.0106) (0.0316)
−1.0284 −1.4021 0.1964 0.2293
(0.0978) (0.1835) (0.0071) (0.0211)
−0.3756 −0.4906 0.0763 0.0821
(0.0566) (0.1062) (0.0041) (0.0122)
−0.3943 −0.5444 0.0742 0.0913
(0.0413) (0.0776) (0.0030) (0.0089)
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
with estimated standard errors of the individual elements of C
 displayed in
parentheses below the estimates, and
D = [24.525, 18.930, 109.556, 97.631, 41.281, 40.722]

.
The ML estimate Σ = (1/56)(Y − C
X)(Y − C
X) of the 6 × 6 covariance matrix
Σ of the errors k has diagonal elements σjj , j = 1,..., 6, given by 0.0262,
0.2964, 0.3280, 0.1453, 0.0487, 0.0260, with log(|Σ|) = −16.6529. Moderate
sample correlations, of the order of 0.5, are found between most of the pairs of
residual variablesjk, j = 1,..., 6, except for residuals2k for the second response
variable y2k which exhibit little correlation with residuals from response variables
y3k through y6k. By comparison with these (partial) correlations among the yjk after
adjustment for Xk, the original response variables y1k and y2k show a correlation of
about 0.985, variables y4k, y5k, and y6k form another strongly correlated group with
correlations of about 0.975, while y3k has moderate negative correlations with y4k,
y5k, and y6k of the order of −0.5.
From the values and significance of the elements in C
, the regression coefficients
corresponding to variables y4 through y6 share strong similarities, indicating that
rank one may be possible for this (sub)set of response variables. But with the
inclusion of response variables y1 through y3, the rank of C might be four, that
is, full rank. In fact, a LR test of rank(C) ≤ 3 versus rank(C) = 4 gives a
chi-squared test statistic value of M = 10.172 with 3 degrees of freedom, so274 9 Partially Reduced-Rank Regression with Grouped Responses
a hypothesis of reduced rank of three or less for C is rejected. Moreover, the
estimated vectors 
j = V

jΣ−1/2, j = 5, 6, associated with the zero eigenvalues
of R
 = Σ−1/2C
ΣxxC

Σ−1/2 are found to be

5 = [0.2398, −0.0542, 0.0143, −2.2035, 3.7356, 1.9501] and

6 = [−0.2420, 0.0432, 0.1300, 0.6732, 4.1355, −5.8899] .
These vectors have the feature that coefficients corresponding to the first three
response variables y1, y2, and y3 are close to zero, with relatively large nonzero
coefficients corresponding to variables y4, y5, and y6. Based on the related
discussion in Chapter 3 and earlier in this chapter, this feature is highly suggestive
of a reduced-rank structure for the coefficient matrix associated with variables y4
through y6. The stepwise backward-elimination procedure discussed in Section 9.4
also performs quite well in this example. In particular, at the first stage the LR
statistics M for testing rank(C1) ≤ 3 give extremely small and nonsignificant
values of 0.025, 0.024, and 0.243, each with 2 degrees of freedom, when the
single response variable y1k, y2k, or y3k, respectively, is excluded from Yk. As the
procedure continues, it leads to clearly retaining only the variables y4k, y5k, and y6k
in Y1k and identification of coefficient structure of rank one for this subset.
We now rearrange the response variables with Y1k = (y4k, y5k, y6k) and Y2k =
(y1k, y2k, y3k)
, and let C1 denote the upper 3 × 4 submatrix of C corresponding to
the (rearranged) response variables in Y1k. The LR test of H0 : rank(C1) ≤ 2 gives
the test statistic value of M = 0.032 with 2 degrees of freedom, and the LR test of
H0 : rank(C1) ≤ 1 gives the test statistic value of M = 2.285 with 6 degrees of
freedom, so the hypothesis of reduced rank of one for C1 is quite acceptable. The
hypothesis that C1 = 0 has a LR statistic value of M = 201.310 with 12 degrees of
freedom, so we should clearly retain the rank one hypothesis. We mention that the
squared sample canonical correlations between Y1k and Xk (adjusting for sample
means) are ρ2
1 = 0.97981, ρ2
2 = 0.04322, and ρ2
3 = 0.00063.
To obtain the ML estimate C
1 = A
B
 under the rank one hypothesis, we find the
normalized eigenvector of the matrix R
1 = Σ−1/2
11 C
1ΣxxC

1Σ−1/2
11 associated with
the largest eigenvalue λ2
1 ≡ ρ2
1 /(1 − ρ2
1 ) = 48.5232. This normalized eigenvector
is V
1 = [0.7747, 0.2362, 0.5866]

. So as in (9.7) we compute A
 = Σ1/2
11 V
1 and
B
 = V

1Σ−1/2
11 C
1 to obtain
A
= [0.3218, 0.1266, 0.1216]
 ,
B
 = [−3.2250, −4.4277, 0.6104, 0.7320] ,
so that the reduced-rank ML estimate of C1 is9.6 Discussion and Extensions 275
C
1 = A
 B
 =
⎡
⎣
−1.0376 −1.4245 0.1964 0.2355
−0.4082 −0.5604 0.0773 0.0926
−0.3920 −0.5382 0.0742 0.0890
⎤
⎦ .
This rank one estimate quite accurately recovers the corresponding LS estimates,
displayed previously as the last three rows of C
 in the original ordering of
response variables. The associated ML estimate of C2 (last three rows of C under
rearrangement) is
C
2 =
⎡
⎣
−0.3769 5.4591 0.0399 0.0697
−0.2131 27.6516 0.1081 0.0379
−1.2616 −0.9190 −0.0455 0.4700
⎤
⎦ .
The corresponding ML estimate Σ of the error covariance matrix Σ has diagonal
elements, corresponding to the original ordering of the response variables, of
0.0263, 0.2964, 0.3315, 0.1456, 0.0499, 0.0260, with log(|Σ|) = −16.6081, which
are very close to values from Σ under (full-rank) LS estimation. For graphical
illustration in support of the reduced-rank feature among the subset of response
variables y4, y5, and y6, Fig. 9.1 displays scatter plots of each of the response
variables against the single predictive index variable x∗
1k = BX k determined from
the rank one modeling of the coefficient matrix C1 for the variables y4, y5, and y6.
(The linear fits of each of y4, y5, and y6 with x∗
1 obtained from the reduced-rank
model are also indicated in the graphs.) This illustration confirms that each of y4,
y5, and y6 has a quite strong linear relationship with the single index x∗
1 , whereas
response variables y1, y2, and y3 do not show any particular relationship with x∗
1 ,
consistent with the modeling results.
9.6 Discussion and Extensions
The model considered in this chapter might be viewed as complementary to the
original reduced-rank model of Anderson (1951) (see Chapter 3) wherein for the
regression of Yk the set of predictor variables Xk was separated into a subset X1k
whose coefficient matrix was taken to be of reduced rank and another subset X2k
with full-rank coefficient matrix. This division into separate sets could be based
on knowledge of the subject matter. In the current model the role is reversed in
that the set of response variables is divided into one subset Y1k having reduced￾rank coefficient matrix in its regression on Xk and so being influenced by only a
small number of predictive variables constructed as linear combinations of Xk, and
another subset Y2k having full-rank coefficient matrix with separate predictors that
are linearly independent of those influencing Y1k.
An extended version of the original reduced-rank model was also considered in
Chapter 3 and in Velu (1991) where each subset X1k and X2k of the predictors has276 9 Partially Reduced-Rank Regression with Grouped Responses
Fig. 9.1 Response variables y1,...,y6 for the chemometrics data versus the single index
predictor variable x∗
1 = BX determined from the partially reduced-rank regression model
coefficient matrix of reduced rank and hence each subset can be represented by a
smaller number of linear combinations to describe the relationship with Yk. As an
analogous extension of the current “partially” reduced-rank model introduced in this
section, one could also easily envision situations where the two subsets Y1k and Y2k
of response variables each have separate reduced-rank structures, instead of only
Y1k having reduced rank. As a motivating example, as stated earlier, in an economic
system it might be postulated that a certain subset of the endogenous (response)
economic variables are influenced by only a few indices of the exogenous (predictor)
economic variables in the system, while another subset of the endogenous variables
is influenced only by a few different indices of the exogenous variables. Hence, each
subset is influenced by a small number of different (linearly independent) linear
combinations of Xk. Such a model adds flexibility to the current model and may9.6 Discussion and Extensions 277
be useful in various applications. It would be interesting to explore the statistical
procedures of ML estimation and LR testing for ranks associated with models
of this form. In particular, we anticipate that simultaneous ML estimation of the
two separate components of reduced-rank structure will involve a feature, which
we might refer to as “seemingly unrelated reduced-rank regression,” analogous to
the aspect that occurs in simultaneous estimation of “seemingly unrelated (full￾rank) regression” systems. Some of these ideas are currently studied under the
topic of sparse group Lasso (see Li et al. 2015) and an identification procedure of
possible groups is investigated in Luo and Chen (2020). Because this topic requires
introduction of sparseness concepts, we will defer the discussion on this to a later
chapter in the book.Chapter 10
High-Dimensional Reduced-Rank
Regression
10.1 Introduction
In various fields of science and engineering, massive amount of data are produced
and collected at unprecedented speed. High-dimensional statistical problems, with
the number of variables exceeding the sample size, are routinely encountered. Such
problems are non-trivial even in the simple case of multiple linear regression; with
high-dimensional data, conventional methods, including least squares method and
maximum likelihood estimation, may cease to work. Many challenges, such as noise
accumulation, spurious correlation, data quality issues, among others, are known
collectively as the “curse of dimensionality” (Donoho 2000). Fortunately, such high￾dimensional problems are not hopeless; a universal key is to summon the Occam’s
razor, the fundamental law of parsimony, by assuming that the underlying truth is
“simple.” That is, the true model lies in a low-dimensional “corner” of the high￾dimensional model space, in a proper sense. With this belief, various regularized
estimation techniques have been developed, making it possible to recover low￾dimensional signal from high-dimensional data.
The regularized learning paradigm has a long history. The well-known ridge
regression, which was introduced to overcome the deficiencies of multicollinearity
among predictors by shrinking the regression coefficients toward zero, dates back
to as early as Tikhonov (1943) and was independently developed in several related
disciplines (in the field of statistics, see, e.g., Hoerl and Kennard (1970), McDonald
and Galarneau (1975), and Brown and Zidek (1980)). With the rise of big data
and the prevalence of large-scale learning tasks, this paradigm has undergone an
exciting development in recent decades. In particular, various sparse models have
been developed and investigated (Tibshirani 1996; Fan and Lv 2010), and shrinkage￾or sparsity-inducing regularization has become a standard technique in training of
high-dimensional or over-parameterized statistical and machine learning models.
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8_10
279280 10 High-Dimensional Reduced-Rank Regression
The unprecedented data explosion opens the door for tackling many complicated
real-world problems through an integrative learning of the underlying data gener￾ation mechanism, which, by its nature, is often multivariate. Indeed, multivariate
regression and related techniques emerge as essential tools for uncovering entangled
relations between multiple outcomes and multiple features with ever growing
dimensionality and complexity. In these problems, the central task usually amounts
to recovering one or more large-scale coefficient matrices with some desired low￾dimensional structures. In the context of multivariate regression, for example,
various sparse estimation methods concern variable selection by pursuing entrywise
or blockwise sparse patterns in the coefficient matrix (Turlach et al. 2005; Kim et al.
2009; Peng et al. 2010; Rothman et al. 2010; Obozinski et al. 2011).
What truly distinguishes multivariate problems from their univariate counter￾parts, and also one of the most fascinating facets of the field of modern multivariate
learning, is that matrix-type objects have much more delicate structures than vector
objects because of the possible “interactions” between rows and columns. In
particular, the notion of “sparsity” for matrix-type objects can take many different
forms: while the complexity of a vector can be measured by its sparsity, the
complexity of a matrix can also be measured by its rank, which is the main subject of
this book. Intriguingly, although the low-rank structure in general does not translate
to sparsity on the entries of the matrix, it translates to sparsity on the singular values.
As such, reduced-rank regression is recognized as a regularized learning technique
and has been revived for high-dimensional multivariate learning (Fazel 2002; Yuan
et al. 2007; Bunea et al. 2011).
In this chapter, we start with an overview of the regularized learning paradigm
and a few fundamental sparse regression methods in 10.2. In Section 10.3, we cast
reduced-rank regression into the regularized learning paradigm and introduce its
path of revival to high-dimensional settings under the framework of singular value
penalization. Section 10.4 presents adaptive nuclear-norm penalized regression
(Chen et al. 2013), a prototype singular value penalization approach for simultane￾ous rank reduction and model estimation. In Section 10.5, we consider an integrative
multi-view regression model (Li et al. 2019), which is capable of estimating view￾specific low-rank structures from different views/groups of the predictors. The
framework nicely bridges sparse and reduced-rank models and reveals that they can
be analyzed in a unified framework. Two applications are discussed in Section 10.6.
10.2 Overview of High-Dimensional Regularized Regression
Throughout this chapter, for any vector X ∈ Rn, we denote by 
X
0, 
X
1, 
X
2,
and 
X
∞ the 0 norm, 1 norm, 2 norm, and ∞ norm, defined, respectively,
as 
X
0 = n
j=1 I (Xj = 0), where I (·) is the indicator function, 
X
1 = n
j=1 |Xj |, 
X
2 = (
n
j=1 X2
j )1/2, and 
X
∞ = maxn
j=1 |Xj |.10.2 Overview of High-Dimensional Regularized Regression 281
To illustrate the main ideas and results of regularized learning, consider the
fundamental multiple linear regression setup,
Y = X
β + , (10.1)
where Y = (y1, y2,...,yT) is the T ×1 vector of responses, X = [X1,...,XT ] the
n × T design matrix, β = (β1,...,βn) a vector of regression coefficients, and  =
(1, 2,...,T ) a vector of random errors. We assume the errors are independently
and identically distributed (i.i.d.) standard normal random variables with mean zero
and variance σ2.
In some practical applications, the number of predictors can be large, comparable
to or even exceeding the sample size this gives rise to the high-dimensional
regression problems. A parsimonious model can be achieved by imposing some
low-dimensional structure on β and employing regularization methods for its
estimation and inference. The general form of a regularized estimation criterion
can be expressed as
min
β∈Rn J (β; Y, X) + λρ(β), (10.2)
where J (C) is a loss function measuring how well the model fits the observed
data, ρ(·) is a penalty function or regularizer measuring the complexity of the
model as a function of the coefficient vector, and λ is a non-negative tuning
parameter for balancing model fitting and model complexity. For linear regression,
it is conventional and fundamental to consider the sum of squared error loss, i.e.,
J (β; Y, X) = 
Y − X
β
2
2/2.
The well-known ridge regression can be expressed as
min
β∈Rn
1
2

Y − X
β
2
2 + λ
β
2
2. (10.3)
This corresponds to using a squared 2 norm regularizer ρ(β) = 
β
2
2 = n
j=1 β2
j , which overcomes the deficiencies resulting from the multicollinearity
among predictors by shrinking the regression coefficients toward zero. The problem
admits a unique and explicit solution,
β
Ridge = (XX + λIn)
−1XY,
where In denotes the n×n identity matrix. As mentioned earlier, this idea dates back
to as early as 1943 in Tikhonov (1943). Besides inducing shrinkage estimation, ridge
regression, or 2 regularization in general, has a grouping effect such that highly
correlated variables tend to exhibit similar estimated coefficients.
In many applications, there is a need of performing variable selection, that is, to
identify a set of features/predictors that are the most relevant to the response. This
is based on the belief that the underlying true model is simple and only involves a282 10 High-Dimensional Reduced-Rank Regression
small set of predictors. In the above linear regression model, it amounts to assuming
that the true coefficient vector β is sparse. We use
S∗ = {j : βj = 0, j = 1,...,n}
to denote the active set with s∗ = |S∗| being its cardinality, the number of active
variables. In high-dimensional settings, although n can grow with T or even exceed
T , the number of active variables s∗ shall remain to be relatively small. The ridge
regression, while can greatly improve prediction, is not suited for the task of variable
selection because it does not produce exactly sparse solution. Various sparsity￾inducing regularization methods have been developed to enable variable selection,
that is, to recover the active set S∗. Below we provide an overview of the 0 and 1
norm penalized regression methods, which are the most fundamental.
The 0 norm regularized regression takes the form
min
β∈Rn
1
2

Y − X
β
2
2 + λ
β
0, (10.4)
where 
β
0 = n
j=1 I (βj = 0). As such, the method directly uses the model
dimension as the penalty; this can be immediately seen to be closely related to the
best subset selection approach. Foster and George (1994) and Barron et al. (1999)
studied the behavior of 0 regularization, and their main results, in the context
of linear regression, give the following prediction error bound: when choosing
λ = O(σ2 log(n)), the 0 norm penalized estimator β
0 satisfies
E
 1
T 
X
β
0 − X
β
2
2
 
 inf
S⊂{1,...,n}
 1
T 
(IT − PS)X
β
2
2 + σ2(1 + log n)|S|
T
 
,
where  means the inequality holds up to a multiplicative constant, IT is the T × T
identity matrix, and PS is the projection matrix onto the row space of the submatrix
of X with row indices in S. The first term quantifies the approximation error or bias
when using the predictors in S to build the regression model; the term vanishes when
S = S∗, leading to a simplified bound E{
X
β
0 − X
β
2
2/T }  (1 + log n)s∗/T .
This shows the rate of the prediction error and the price we have to pay for
searching over many subset models: it is not much because the prediction error
rate is only inflated by a logarithm factor, log n, compared to the oracle rate, the
rate that is achieved by the least squares estimation when we knew the active set
S∗ a priori. Unfortunately, despite the appealing risk behavior of 0 regularization,
its computation with a naive exhaustive search approach is infeasible even for a
moderate number of predictors, say, the value of n is around 50. The computational
burden of (10.4) stems from the combinatory nature of the subset selection problem
and the non-convexity and discontinuity of the 0 norm regularizer. Not until
recently, many optimization algorithm and strategies are been developed to more
efficiently solve the problem, typically in an approximate way (Bertsimas et al.
2016; Zhu et al. 2020).10.2 Overview of High-Dimensional Regularized Regression 283
The infeasibility of 0 norm regularization naturally leads to the consideration of
its convex relaxation. The 1 norm regularized regression, known as the Lasso (least
absolute shrinkage and selection operator) (Tibshirani 1996), takes the form
min
β∈Rn
1
2

Y − X
β
2
2 + λ
β
1. (10.5)
Due to the properties of the 1 penalty, Lasso not only is able to perform shrinkage
estimation of β toward zero, but also is capable of producing exactly sparse
solutions. To understand how Lasso works, it is intriguing to examine its geometry
and compare with that of the ridge regression in (10.3). First of all, these two
methods can both be equivalently expressed in a constrained form. Lasso can be
expressed as
min
β∈Rn 
Y − X
β
2
2 s.t. 
β
1 ≤ δ,
while ridge regression minimizes the same loss subject to a different constraint,

β
2 ≤ δ, i.e.,
min
β∈Rn 
Y − X
β
2
2 s.t. 
β
2 ≤ δ.
Figure 10.1 shows the geometry of Lasso and ridge regression when n = 2. The
key difference of the two methods is reflected on the shapes of their constrained
regions: the 1 ball 
β
1 ≤ δ forms a diamond centered at the origin, while the
2 ball 
β
2 ≤ δ forms a disk centered at the origin. Provided that X is of full row
rank, the contours of the loss 
Y−X
β
2
2 form ellipsoids centered at the least squares
solution β
LS. For either method, the solution is obtained when the elliptical contour
first hits the constrained region. Both methods perform shrinkage estimation, as
these estimators are necessarily closer to the origin than the least squares estimator
β
LS, unless β
LS is already in the constrained region. The Lasso estimator β
Lasso can
be obtained at one of the corners of the diamond with non-trivial probability, and
thus it is capable of producing exactly sparse solution. The ridge estimator β
Ridge,
on the other hand, has zero probability to be exactly sparse since its constrained
region is smooth without any corners.
In terms of computation, the solution paths of Lasso can be efficiently obtained
by several methods, including coordinatewise descent algorithms (Tseng 2001; Fu
1998), a modified version of least angle regression (Efron et al. 2004), and several
versions of path-following algorithms (Friedman et al. 2007). Here we sketch the
main idea of the coordinatewise descent algorithm for an illustration. For any given
λ > 0 and some initial value β(0)
, the algorithm solves (10.5) by minimizing
the objective function with respect to one coefficient at a time, with all the other
coefficients held fixed at their current values. With some algebra, it can be shown
that the one-dimensional problem takes the following general form:284 10 High-Dimensional Reduced-Rank Regression
−4 −2 −1 012 4 6
−2 2 4 6 β2
β1
βˆ
LS
−4 −2 −1 012 4 6
−2
2
4
6
β2
β1
βˆ
LS
Fig. 10.1 Geometry of the Lasso and the ridge regression methods
min
θ∈R
1
2
(y − θ )2 + λ|θ|,
which admits an explicit solution given by the soft-thresholding rule: θ =
sgn(y)(|y| − λ)+, where sgn(·) is the sign function and a+ takes the positive
part of a. The algorithm then updates one coefficient at a time as above and cycles
through all coefficients until convergence.
The theoretical properties of the Lasso have been thoroughly studied; a com￾prehensive treatment can be found in Bühlmann and van de Geer (2011). The
main theoretical concerns are on the prediction of the response, the estimation of
β, and the recovery of S∗, three closely related yet different tasks. Under certain
compatibility condition on the design matrix X, such as the restricted eigenvalue
(RE) condition (Bickel et al. 2009), the prediction error of Lasso is bounded as
E{
X
β
Lasso − X
β
2
2/T }  σ2(log n)s∗/T for a suitable choice of λ at the
order σ
0
log(n)T . What is remarkable about this result is that it shows that the
computationally efficient Lasso method can achieve the same prediction error rate
as the computationally infeasible 0 regularization method, albeit under additional
assumptions on the design matrix. Again, this implies an oracle property of the
Lasso: up to a log(n) factor, the mean squared prediction error is of the same order
as if one knew a priori that s∗ predictors are relevant and only used them to conduct
least squares regression. Moreover, the rate is essentially optimal up to the log(n)
factor, in a minimax sense.
As for the estimation accuracy of the Lasso, under the same restrictive eigenvalue
condition and choice of λ, we have that 
β
Lasso − β
2
2  (log n)s∗/T with
probability tending to 1. Therefore, the estimation consistency can be achieved even
when n grows polynomially in T , i.e., n = O(T a) for any a > 0, as long as s∗ stays
relatively small, i.e., s∗ = o(T / log(n)).
To ensure the success in variable selection or support recovery with the Lasso,
much stronger conditions are needed. Intuitively, the so-called β-min condition
requires that the non-zero coefficients are sufficiently large, so they can be hopefully
distinguished from zero, and the “irrepresentable” condition states that the relevant10.3 Framework of Singular Value Regularization 285
predictors cannot be too highly correlated and thus be too well represented by the
irrelevant predictors (Zhao and Yu 2006). The irrepresentable condition is rather
strong and implies the compatibility condition (Bühlmann and van de Geer 2011),
and the former is essentially a necessary condition for Lasso to achieve selection
consistency. Therefore, a general conclusion is that Lasso for variable selection
only works in a rather narrow range of problems, excluding many cases where the
predictors exhibit strong correlations.
Beyond the convex Lasso method, there also have been extensive work on non￾convex penalization and more structured regularization approaches with auxiliary
information on the design; we refer to, e.g., Bühlmann and van de Geer (2011), Fan
and Lv (2010), and Huang et al. (2012) for comprehensive reviews on these topics.
10.3 Framework of Singular Value Regularization
In multivariate regression problems, the most critical learning task usually amounts
to the recovery of one or more large-scale coefficient matrices with some desired
low-dimensional structures. To elaborate, recall the multivariate linear regression
framework presented in Section 1.2. Let Y ∈ Rm be a multivariate response variable
and X ∈ Rn a multivariate predictor variable. Observing T independent data pairs
(Xk, Yk), k = 1,...,T , the multivariate linear regression (MLR) model is again
stated as
Y = CX + , (10.6)
where Y = [Y1,...,YT ]=[Y(1),...,Y(m)]
 ∈ Rm×T is the response matrix,
X = [X1,...,XT ]=[X(1),...,X(n)]
 ∈ Rn×T the predictor matrix, C =
[C1,...,Cn]=[C(1),...,C(m)]
 ∈ Rm×n the unknown coefficient matrix, and
 = [1,..., T ]=[(1),..., (m)]
 ∈ Rm×T the random error matrix. Unless
otherwise noted, we assume ks are i.i.d. and each has mean zero and a positive￾definite covariance matrix  . An intercept term is avoided by either including
a vector of ones in X or by centering the responses and predictors. Without
any additional structural assumptions, the least squares estimation of C under
Model (10.6) is equivalent to perform m separate least squares for each response on
the set of predictors, which does not utilize any multivariate flavor of the problem
and may fail miserably in high-dimensional scenarios.
The reduced-rank regression (RRR), the main subject of this book, assumes that
the coefficient matrix C is rank-deficient, i.e.,
r(C) = r,
where r(·) denotes the rank of the enclosed matrix, and 0 ≤ r ≤ min(m, n). This
is equivalent to assuming that there exist two matrices A ∈ Rm×r and B ∈ Rr×n,
of full column rank and row rank, respectively, such that C = AB. The reduced-286 10 High-Dimensional Reduced-Rank Regression
rank structure mitigates the curses of dimensionality by dramatically reducing the
effective number of parameters when r is small. Further, this structure conveniently
captures dependencies between responses and induces an appealing latent factor
interpretation: the responses are linked to the predictors through r common latent
factors/variables as given by BX. Conceptually, a reduced-rank model searches for
the most relevant subspace of the predictors when it is believed that they contribute
to the prediction of the outcomes collectively rather than sparsely.
A natural question is then whether reduced-rank regression can be cast in
the regularized learning paradigm. In the context of multivariate regression in
which the main objective is to estimate the coefficient matrix C, the regularized
learning framework can be expressed as minimizing a penalized loss function of the
following form:
J (C; Y, X) + ρ(C; λ) (10.7)
with respect to C ∈ Rm×n, where J (C) is a loss function measuring the goodness￾of-fit, and ρ(·; λ) is a penalty function or regularizer measuring the complexity of
the enclosed matrix, in which λ is a non-negative tuning parameter for balancing
between model fitting and model complexity.
Before diving into regularized learning, we make a few remarks on choice of
the loss function. Thus far in the formulation of the reduced-rank regression, we
have been using a weighted 2 loss, J (C; Y, X) = tr{(Y − CX)
(Y − CX)}; see
Section 2.3. It is common to regard the weighting matrix  ∈ Rm×m as known (Yuan
et al. 2007; Izenman 2008). The choice of  is flexible and is usually based on a pilot
covariance estimate  . For example, it can be −1  when  is nonsingular, or a
regularized version ( + cIm)−1 for some c > 0 and Im being the m × m identity
matrix. When a reliable estimate of  is unavailable, a standard practice in finance
and econometric forecasting is to simplify  to a diagonal matrix, or equivalently,
to an identity matrix after robustly scaling the response variables. Although it
sounds intriguing to consider jointly estimating the reduced-rank mean structure
and the error covariance matrix, the problem is notoriously difficult with high￾dimensional data. Potential further improvement in regression coefficient estimation
is not warranted without strong technical or structural conditions. Therefore, for
ease of presentation, in this chapter, we take  as the m × m identity matrix and
mainly focus on the regularized least squares problem of the form
min
C
!

Y − CX
2
F + ρ(C; λ)"
,
where 
·
F denotes the Frobenius norm, i.e., 
Y−CX
2
F = tr{(Y−CX)
(Y−CX)}.
In general, reduced-rank structure of a matrix does not necessarily result in
sparsity of its entries. For example, a matrix of ones, which is apparently of unit
rank, has no zero entries at all. Rather, the reduced-rank structure directly translates
to sparsity of the singular values of the matrix because the rank of a matrix is the
same as the number of its nonzero singular values. To be precise, denote dj (C) as10.3 Framework of Singular Value Regularization 287
the j th largest singular value of the matrix C and d(C) = (d1(C), . . . , dq (C)) the
vector of singular values of C with q = min(m, n). Then, the rank of C can be
expressed as the 0 norm of the singular values
r(C) = 
d(C)
0 = 
q
j=1
I (dj (C) = 0), q = min(m, n).
Similarly, the nuclear norm of a matrix is defined as the 1 norm of its singular
values,

C
" = 
d(C)
1 = 
q
j=1
dj (C).
Both the matrix rank and the nuclear norm are complexity measures of a
matrix. Their connections and differences can be understood through considering
the fundamental low-rank matrix approximation problem. Write the singular value
decomposition of Y ∈ Rm×T as
Y = V DU
, D = diag{d(Y)}, (10.8)
where U and V are, respectively, T ×q and m×q orthonormal matrices and diag(·)
denotes a diagonal matrix with the enclosed vector or elements on its diagonal. The
following results characterize the solutions of the rank or nuclear-norm penalized
matrix approximation problems.
Result 10.1 For any λ ≥ 0 and Y ∈ Rm×T , let
Hλ(Y) = arg minC
{
Y − C
2
F + λ2r(C)},
Sλ(Y) = arg minC
{
Y − C
2
F + 2λ
C
"}.
The solutions are Hλ(Y) = V Hλ(D)U with
Hλ(D) = diag 3
dj (Y)I (dj (Y) > λ), j = 1,...,q4
, (10.9)
and Sλ(Y) = V Sλ(D)U with
Sλ(D) = diag 3
(dj (Y) − λ)+, j = 1,...,q4
. (10.10)
The singular value hard-thresholding estimator, Hλ(Y), eliminates any singular
value of Y that is below the threshold λ, while the singular value soft-thresholding
estimator Sλ(Y) eliminates any singular value that is below λ and also shrinks all
the other larger singular values of Y by λ toward zero. These estimators can be288 10 High-Dimensional Reduced-Rank Regression
considered as natural extensions of the hard/soft-thresholding estimators for scalars
and vectors (Donoho and Johnstone 1995; Cai et al. 2010).
With the above connection between rank deficiency and sparsity of singular
values, the reduced-rank regression can be recognized as a sparse regression model
under a broader concept of sparsity. Consequently, under the regularized learning
framework in (10.7), reduced-rank estimation can be achieved through penalizing
the singular values of either the coefficient matrix C or the regression component
CX; the latter may lead to desirable computational advantages (to be elaborated
later in Section 10.4). That is, we can explicitly set ρ(C; λ) = ρ(d(C); λ) or
ρ(C; λ) = ρ(d(CX); λ), where ρ(·; λ) is a sparsity-inducing penalty function. To
demonstrate, below we discuss the cases when ρ is the 0 or the 1 function.
Bunea et al. (2011) studied the rank-penalized regression criterion, in which the
penalty is set to be proportional to the rank, or equivalently, the 0 norm of the
singular values of the coefficient matrix,

Y − CX
2
F + λ2
d(C)
0. (10.11)
This work was among the first to cast reduced-rank regression as a penalized
regression and generalize it to high-dimensional settings. It is recognized that
minimizing (10.11) is equivalent to minimizing

Y − CX
2
F + λ2
d(CX)
0, (10.12)
owing to the fact that r(CX) = r(C) whenever the rank reduction is effective (Chen
et al. 2013). The rank-penalized regression problem admits an explicit solution.
Specifically, by Pythagoras theorem, the problem is the same as minimizing

C
LX − CX
2
F + λ2r(CX), where C
LX denotes the least squares fitted value
from minimizing 
Y − CX
2
F with respect to C. It then follows from Result 10.1
that the solution can be obtained through hard-thresholding the singular value
decomposition of C
LX using (10.9), i.e.,
C
(λ)
H = V
D−Hλ(D)  V

C
L, (10.13)
where V
D2V
 is the eigenvalue decomposition of (C
LX)(C
LX) = YPXY with
PX being the projection matrix onto the column space of X
, and A− denotes the
Moore–Penrose inverse (Moore 1920; Penrose 1955) of matrix A. Not surprisingly,
the set of rank-penalized estimators spans exactly the same solution path as the
corresponding rank-constrained formulation, which is minC 
Y − CX
2
F subject to
r(C) ≤ r, for 0 ≤ r ≤ q (see also Theorem 2.2).
Another popular approach under the framework of (10.7) is the nuclear-norm
penalized regression (NNP) (Yuan et al. 2007),
1
2

Y − CX
2
F + λ
d(C)
1. (10.14)10.4 Reduced-Rank Regression via Adaptive Nuclear-Norm Penalization 289
It is attractive as a convex relaxation of the rank-penalized criterion in (10.11). This
problem, however, generally does not have an explicit solution. Extensive research
has been devoted to developing efficient optimization methods (Yuan et al. 2007;
Toh and Yun 2010; Udell et al. 2016), one of which is the iterative singular value
soft-thresholding algorithm (Cai et al. 2010).
Alternatively, motivated by the equivalency between (10.11) and (10.12), one
could consider
1
2

Y − CX
2
F + λ
d(CX)
1, (10.15)
which, based on Pythagoras theorem and Result 10.1, leads to an explicit solution
through singular value soft-thresholding,
C
(λ)
S = V
D−Sλ(D)  V

C
L. (10.16)
In analogous to the 1 norm penalization on vectors, nuclear-norm penalization
encourages sparsity among the singular values of the coefficient matrix. This
results in a continuous solution path and achieves simultaneous rank reduction and
shrinkage estimation.
10.4 Reduced-Rank Regression via Adaptive Nuclear-Norm
Penalization
It is intriguing that the rank and nuclear-norm penalization approaches can be
viewed as 0 and 1 singular value penalization methods, respectively. Motivated
by these connections, we consider in detail the adaptive nuclear norm regularization
method, which is designed with a goal to close the gap between the 0 and 1
penalization schemes.
The adaptive nuclear norm of a matrix C ∈ Rm×n is defined as a weighted sum
of its singular values:

C
"w = 
q
j=1
wj dj (C), (10.17)
where wj s are non-negative weights. We first show that the adaptive nuclear norm is
non-convex when the weight of the singular value decreases with the singular value,
a condition needed for a meaningful regularization. Despite the non-convexity, the
adaptive nuclear-norm penalized estimator has a closed-form solution in matrix
approximation problems. We then present the adaptive nuclear-norm penalized
regression (ANN) method for conducting simultaneous dimension reduction and
coefficient estimation in high-dimensional multivariate regression.290 10 High-Dimensional Reduced-Rank Regression
10.4.1 Adaptive Nuclear Norm
The adaptive nuclear norm 
C
"w defined in (10.17) forms a rich class of penalty
functions indexed by the weights. Clearly, it includes the nuclear norm as a special
case with unit weights. Because the nuclear norm is convex and is a matrix norm, an
immediate question arises as to whether or not its extension with weights preserves
convexity, which is the case for the analogous Lasso and adaptive Lasso penalties
(Zou 2006). Convexity of the penalty function has important consequences in both
numerical optimization and statistical analysis. There are some advantages and
disadvantages: comparing to the non-convex counterparts, convex regularization in
general leads to more efficient computation but may need much stronger conditions
to achieve desired parameter estimation and model selection performance (Fan and
Lv 2010; Zhang and Zhang 2012).
Let us consider an example to demonstrate the convexity issue. If there exists
an index k such that wk < wk+1, then f (C) = 
C
"w as defined in (10.17) is
necessarily a non-convex function. Let C and Z be two m × m diagonal matrices
such that cjj = j , for j = 1,...,m, while Z equals C but with entries switched at
positions m − k + 1 and m − k for some 1 ≤ k ≤ m − 1 on the diagonal. It is then
easy to verify that
f (C) = f (Z) =
m
j=1
wj (m − j + 1),
f
C + Z
2

− f (C)
2 − f (Z)
2 =1
2
(wk + wk+1) − wk > 0.
Therefore, f (·) is non-convex.
More generally, the following result states that the convexity of (10.17) depends
on the ordering of the non-negative weights.
Result 10.2 Let f (C) = 
C
"w be defined in (10.17) for any matrix C ∈ Rm×n.
Then f (·) is convex in C if and only if w1 ≥···≥ wq ≥ 0.
Hence, for the adaptive nuclear norm (10.17) to be a convex function, the weights
must be non-decreasing with the singular values. However, for penalized estimation,
the opposite is desirable. We would and shall henceforth impose the order constraint
0 ≤ w1 ≤···≤ wq , q = min(m, n), (10.18)
which ensures that a larger singular value receives a lighter penalty to help reduce
the bias and a smaller singular value receives a heavier penalty to help promote
sparsity. The non-convexity of f (·) arises from the constraint (10.18), under which
f (·) is no longer a matrix norm. In fact, f (·) is neither convex nor concave under
constraint (10.18). Here is an example. Consider m = n = 2, and10.4 Reduced-Rank Regression via Adaptive Nuclear-Norm Penalization 291
C1 =

2 0
0 1
, C2 =

1 0
0 2
.
Let w1 = 1 and w2 = 2. It can be verified that f (C1) = f (C2) = f (−C2) = 4,
while f {(C1 +C2)/2} = 4.5 > {f (C1)+f (C2)}/2; also, f {(C1 −C2)/2} = 1.5 <
{f (C1) + f (−C2)}/2.
The preceding discussion in Section 10.3 on the connections between different
thresholding rules and penalty terms motivates us to consider the use of the adaptive
nuclear norm to bridge the gap between the 0 and 1 singular value penalties and
fine-tuning the bias–variance tradeoff. We begin with a result in the context of low￾rank matrix approximation; we are able to obtain, with complete characterization,
an explicit global minimizer of the least squares criterion penalized by a non-convex
adaptive nuclear norm.
Theorem 10.1 For any λ ≥ 0, 0 ≤ w1 ≤ ··· ≤ wq with q = min(m, n), and
Y ∈ Rm×n with its singular value decomposition given by Y = V DU
, a global
optimal solution to the optimization problem
min
C
1
2

Y − C
2
F + λ
C
"w 
(10.19)
is Sλw(Y), where
Sλw(Y) = V Sλw(D)U
,
Sλw(D) = diag{(dj (Y) − λwj )+, j = 1,...,q}. (10.20)
Further, if all the nonzero singular values of Y are distinct, then Sλw(Y) is the
unique optimal solution.
Proof We first prove that Sλw(Y) is indeed a global optimal solution to (10.19).
Write d = (d1,...,dq ) = d(C), which implies the entries of d are in non￾increasing order. Since the penalty term only depends on the singular values of
C, (10.19) can be equivalently written as
min
d:d1≥···≥dq≥0
⎧
⎨
⎩
min
C∈Rm×n,d(C)=d
⎛
⎝
1
2

Y − C
2
F + λ

q
j=1
wj dj
⎞
⎠
⎫
⎬
⎭ .
For the inner minimization problem, we have the inequality

Y − C
2
F = tr(YY
) − 2tr(YC
) + tr(CC
)
= 
q
j=1
d2
j (Y) − 2tr(YC
) +
q
j=1
d2
j292 10 High-Dimensional Reduced-Rank Regression
≥ 
q
j=1
d2
j (Y) − 2d(Y)

d +
q
j=1
d2
j .
The last inequality is due to von Neumann’s trace inequality (von Neumann 1937;
Mirsky 1975). Equality holds when C admits the singular value decomposition C =
V diag(d)U
, where V and U are defined in (10.8) as the left and right singular
matrices of Y. Then the optimization reduces to
min
d:d1≥···≥dq≥0
⎡
⎣

q
j=1
1
2
d2
j − (dj (Y) − λwj )dj +
1
2
d2
j (Y)
 ⎤
⎦ . (10.21)
This objective function is completely separable in the dj ’s and is minimized only
when dj = (dj (Y) − λwj )+. This is a feasible solution because {dj (Y)} is in
non-increasing order, while {wj } is in non-decreasing order. Therefore, Sλw(Y) =
V diag{(d(Y) − λw)+}U is a global optimal solution to (10.19). The uniqueness
follows by the equality condition for the von Neumann’s trace inequality when
Y has distinct nonzero singular values and the uniqueness of the strictly convex
optimization in (10.21). This concludes the proof.
The fact that a closed-form global minimizer can be found for the non-convex
matrix approximation problem (10.19) is rather surprising. As mentioned, the result
mainly stems from von Neumann’s trace inequality (Mirsky 1975). Following Zou
(2006), the weights can be set to be wj = d−γ
j (Y), for j = 1,...,q, where γ ≥ 0
is a pre-specified constant. In this way, the order constraint (10.18) is automatically
satisfied.
10.4.2 Adaptive Nuclear-Norm Penalized Regression
Predictive accuracy and computational efficiency are both pivotal in high￾dimensional regression problems. Motivated by criteria (10.11), (10.12), (10.14),
and (10.15), we consider the estimation of C by minimizing the following adaptive
nuclear-norm penalized regression (ANN) criterion:
1
2

Y − CX
2
F + λ
CX
"w, (10.22)
where the weights {wj } are required to be non-negative and non-decreasing. In
practice, a foremost task of using (10.22) is setting proper adaptive weights.
Following Zou (2006), this can be based on the least squares solution,
wj = d−γ
j (YPX) = d−γ
j (C
LX), (10.23)10.4 Reduced-Rank Regression via Adaptive Nuclear-Norm Penalization 293
where γ is a non-negative constant, and PX denotes the projection matrix onto the
row space of X, so that YPX = C
LX with C
L being a least squares estimator of the
coefficient matrix C.
The ANN method in (10.22) is built on two main ideas. First, the criterion focuses
on the fitted values CX and encourages sparsity among the singular values of CX
rather than those of C. This may yield a low-rank estimator for CX and hence for C
(Koltchinskii et al. 2011). A prominent advantage of this setup is that the problem
can then be solved explicitly and efficiently by utilizing the results established in
Theorem 10.1. Second, the adaptive penalization of the singular values allows a
flexible bias–variance tradeoff: a large singular value receives a small penalty to
control for possible bias, and a small singular value receives a large penalty to help
induce sparsity and hence reduce the rank. As such, the method builds a bridge
between the 0 and 1 singular value penalization methods and can be viewed as
analogous to the adaptive Lasso method (Tibshirani 1996; Zou 2006; Huang et al.
2008) developed for the case of univariate sparse regression.
The following result shows that the ANN criterion admits an explicit minimizer.
Result 10.3 A minimizer of (10.22), denoted by C
(λw)
S , is given by
C
(λw)
S = V
D−Sλw(D)  V

C
L, C
(λw)
S X = Sλw(C
LX) = V
Sλw(D)  U

,
(10.24)
where C
L is the least squares estimator of C, V
DU
 is the singular value
decomposition of C
LX, and Sλw(·) is defined in (10.20).
By Pythagoras theorem, minimizing criterion (10.22) is equivalent to minimizing

C
LX − CX
2
F /2 + λ
CX
"w with respect to C. The above result then follows
directly from Theorem 10.1. The proposed method first projects Y onto the row
space of X, i.e., YPX = C
LX; the estimator is then obtained as a low-rank
approximation of YPX by adaptively soft-thresholding its singular values. The
thresholding level is data-driven: the smaller an initially estimated singular value,
the larger its thresholding level.
The estimated rank from ANN corresponds to the smallest nonzero singular value
of YPX that exceeds its thresholding level, i.e.,r = max{r : dr(YPX) > λwr}. For
the choice of the weights in (10.23), the estimated rank is
r = max{r : dr(YPX)>λ1/(γ +1)
}, (10.25)
for 0 ≤ λ<dγ +1
1 (YPX) and r = 0 for λ ≥ dγ +1
1 (YPX). Therefore, the plausible
range of the tuning parameters is λ ∈ [0, dγ +1
1 (YPX)], with λ = 0 corresponding
to the least squares solution and λ = dγ +1
1 (YPX) to the null solution. To select
an optimal λ and hence an optimal solution, K-fold cross-validation method can be
used, based on predictive performance of the models (Stone 1974).294 10 High-Dimensional Reduced-Rank Regression
The ANN estimator C
(λw)
S in (10.24) and the 0 estimator C
(λ)
H in (10.13)
differ only in their estimated singular values for CX, but the difference can be
consequential. While the solution path of the 0 method is discontinuous and the
number of possible solutions equals the maximum rank, the proposed criterion
offers a more flexible bias–variance tradeoff in that the resulting solution path is
continuous and is guided by the data-driven weights. The two methods can both
be efficiently computed, in contrast to the computationally intensive 1-based NNP
method in (10.14). Moreover, compared to NNP that tends to overestimate the rank,
ANN may improve rank determination with the use of the adaptive weights.
10.4.3 Theoretical Analysis
We perform non-asymptotic analysis to evaluate the finite-sample performance of
the reduced-rank estimators in high-dimensional settings. In particular, we focus on
the rank estimation and prediction properties of the ANN method, which covers a
family of reduced-rank estimators based on singular value thresholding. The finite￾sample analysis would be particularly insightful in high-dimensional settings, i.e.,
when the number of observations is much smaller than the number of variables.
10.4.3.1 Setup and Assumptions
To understand the behaviors of ANN on rank estimation and prediction, we need
proper measures of the signal strength and the noise level. Recall that PX denotes
the projection matrix unto the column space of X (or the row space of X). From the
multivariate linear model in (10.6), it holds that
YPX = CX + PX.
This suggests that we can use the rth largest singular value of CX, i.e., dr(CX),
to measure the signal strength, and use the largest singular value of the projected
error matrix PX, i.e., d1(PX), to measure the noise level. To control for the noise
level, we make the following conventional yet fundamental assumption on the error
distribution.
Assumption 10.1 The entries of the error matrix  are independently and identi￾cally distributed N (0, σ2) random variables.
Result 10.4 Let rx denote the rank of X and suppose Assumption 10.1 holds. Then
for any t > 0, E(d1(PX)) ≤ σ (√rx + √m), and P (d1(PX) ≥ E{d1(PX)} +
σt) ≤ exp(−t2/2).10.4 Reduced-Rank Regression via Adaptive Nuclear-Norm Penalization 295
Assumption 10.1 ensures that the noise level d1(PX) is of order √rx +√m, and
this result remains valid for independent sub-Gaussian errors (Bunea et al. 2011).
Now let us consider the signal strength relative to the noise level. Intuitively, if
d1(PX) is much larger than the size of the signal, it is possible that part of the
low-rank regression components could be masked by the noise and may be lost
during the thresholding procedure; as such, the components that can be recovered
or distinguished from the noise may have a smaller rank than the true rank r. The
results below characterize the true target of the rank estimation in ANN, i.e., the
effective rank, and its relationship to the signal strength, the noise level, and the
adaptive weights.
Result 10.5 Suppose that there exists an index s ≤ r such that ds(CX)>(1 +
δ)λ1/(γ +1) and ds+1(CX) ≤ (1 − δ)λ1/(γ +1) for some δ ∈ (0, 1]. Then s is called
the effective rank. Moreover, the estimated rank by ANN satisfies that
P (r = s) ≥ 1 − P (d1(PX) ≥ δλ1/(γ +1)
).
Here r is the rank of true coefficient matrix C,r the estimated rank given in (10.25),
PX the projection matrix onto the row space of X,  the error matrix in model (10.6),
and γ the power parameter in the adaptive weights (10.23).
Proof By (10.25), r>s holds if and only if ds+1(YPX)>λ1/(γ +1)
, and r<s
holds if and only if ds(YPX) ≤ λ1/(γ +1)
. Then
P (r = s) = P
!
ds+1(YPX)>λ1/(γ +1) or ds(YPX) ≤ λ1/(γ +1)
"
.
Using Weyl’s inequalities on singular values (Franklin 2000) and observing that
YPX = CX + PX, we have d1(PX) ≥ ds+1(YPX) − ds+1(CX) and d1(PX) ≥
ds(CX) − ds(YPX). Hence, ds+1(YPX)>λ1/(γ +1)
, which implies d1(PX) ≥
λ1/(γ +1)
−ds+1(CX), and ds(YPX) ≤ λ1/(γ +1)
. This in turn implies that d1(PX) ≥
ds(CX) − λ1/(γ +1)
. It then follows that
P (r = s) ≤ P
	
d1(PX) ≥ min{λ1/(γ +1) − ds+1(CX), ds(CX) − λ1/(γ +1)
}

.
Note that min{λ1/(γ +1) − ds+1(CX), ds(CX) − λ1/(γ +1)
} ≥ δλ1/(γ +1)
. This
completes the proof.
With the above results, it is clear that we need to have a strong enough signal to
avoid the gap between the effective rank and the true rank, and we need to choose
an appropriate rate of the tuning parameter to ensure that the estimated rank equals
to the true rank with high probability. These considerations lead to the following
assumption on the signal strength.
Assumption 10.2 For any θ > 0, let λ = {(1 + θ )σ (√rx + √m)/δ}γ +1 with δ
defined in Result 10.5, and dr(CX) > 2λ1/(γ +1)
.296 10 High-Dimensional Reduced-Rank Regression
10.4.3.2 Rank Consistency and Prediction Error Bound
Rank determination is an important task in reduced-rank estimation. The rank of the
true coefficient matrix C, denoted as r, can be viewed as the number of effective
linear combinations of the predictors linked to the responses. Anderson (2002)
studied the effect of rank mis-specification on the properties of the estimators in
classical settings. Generally speaking, when the specified rank is less than the true
rank, biases occur; when the specified rank is greater, variances of the estimators
can be unnecessarily large. Hence, it is important to correctly estimate the rank.
Here, under the high-dimensional regime, we study the rank determination problem
and examine the effects of rank mis-specification through establishing the non￾asymptotic prediction error bounds of the reduced-rank estimators.
Theorem 10.2 Suppose Assumptions 10.1 and 10.2 hold. Let r be the estimated
rank given in (10.25), and let rx = r(X) be the rank of X. Then P (r = r) → 1 as
rx + m → ∞.
Proof When dr(CX) > 2λ1/(γ +1)
, we have dr(CX) > 2λ1/(γ +1) ≥ (1+δ)λ1/(γ +1)
and dr+1(CX) = 0 ≤ (1 − δ)λ1/(γ +1)
, for some 0 < δ ≤ 1. The effective rank
s defined in Result 10.5 equals the true rank, i.e., s = r, and min{λ1/(γ +1) −
dr+1(CX), dr(CX) − λ1/(γ +1)
} ≥ δλ1/(γ +1)
. It then follows from Result 10.4 that
P (r = r) ≥ 1 − P{d1(PX) ≥ δλ1/(γ +1)
}
= 1 − P{d1(PX) ≥ (1 + θ )σ (√rx + √m)}
≥ 1 − exp{−θ 2(rx + m)/2}
→ 1
as rx + m → ∞. This completes the proof.
Theorem 10.2 shows that ANN is able to identify the correct rank with
probability tending to 1 as rx + m goes to infinity. The consistency results can be
extended to the case of sub-Gaussian errors and can also be adapted to the case when
rx + m is bounded and the sample size T goes to infinity. The rank consistency of
the ANN estimator is thus valid for both classical and high-dimensional asymptotic
regimes.
The prediction performance of the ANN estimator is characterized in Theo￾rem 10.3 below. For simplicity, we write C
S for C
(λw)
S .
Theorem 10.3 Suppose Assumptions 10.1 and 10.2 hold. Then with probability at
least 1 − exp{−θ 2(rx + m)/2},

C
SX − CX
2
F  (1 + θ )2σ2(rx + m)r, (10.26)
where  means the inequality holds up to a multiplicative constant.10.4 Reduced-Rank Regression via Adaptive Nuclear-Norm Penalization 297
The proof starts with the so-called basic inequality, i.e., by the definition of C
S
in (10.24),

Y − C
SX
2
F + 2λ

q
j=1
wj dj (C
SX) ≤ 
Y − CX
2
F + 2λ

q
j=1
wj dj (CX).
That is, the objective function evaluated at the solution is no greater than that
evaluated at any other m × n matrix including the true coefficient matrix C. Noting
that

Y − C
SX
2
F = 
Y − CX
2
F + 
C
SX − CX
2
F + tr{(CX − C
SX)

},
we get

C
SX − CX
2
F ≤ 2tr{PX(C
SX − CX)

}
+ 2λ{

q
j=1
wj dj (CX) −
q
j=1
wj dj (C
SX)}.
The results can then be established by bounding the stochastic error part, i.e., the first
term, with concentration inequalities in Result 10.4, and bounding the difference
in penalty part, i.e., the second term, using matrix inequalities such as Weyl’s
inequality (Franklin 2000). The details can be found in Bunea et al. (2011) and
Chen et al. (2013).
The non-asymptotic bound in (10.26) shows that the prediction error is bounded
by d2
1 (PX)r up to some multiplicative constant, with probability 1−exp{−θ 2(rx +
m)/2}. The smaller the noise level or the true rank, the smaller the prediction error.
The bound is valid for any X and C. The estimation error bound of C
S can also
be readily derived from Theorem 10.3 in low-dimensional scenarios. For example,
if drx (X) ≥ c > 0 for some constant c, then under Assumptions 10.1 and 10.2,

C
S − C
2
F  c−2λ2/(γ +1)
r.
To better understand the effects of rank mis-specification in reduced-rank
estimation, we can apply similar techniques as above to examine the performance
of the rank restricted estimators obtained as
C
k = arg minC

Y − CX
2
F , s.t. r(C) ≤ k,
for k = 1 ..., min(rx , m). Bunea et al. (2011) showed that under Assumption 10.1,

C
kX − CX
2
F 
min

(rx ,m)
j=k+1
d2
j (CX) + σ2(rx + m)k,298 10 High-Dimensional Reduced-Rank Regression
with high probability (tending to one as rx + m → ∞). The first term,
min(rx ,m)
j=k+1 d2
j (CX), is the approximation error or the bias term, which is decreasing
in k and vanishes for k > r(CX). The second term, σ2(rx + m)k, results from
the stochastic error and can be regarded as the variance term, which increases in
k and is tied to the dimension of the parameter space. Hence, we can interpret
the error bound as the squared bias plus the variance in reduced-rank estimation
and understand the effects of rank specification from the perspectives of bias–
variance tradeoff. This is consistent with earlier work by, e.g., Anderson (2002).
Similar results regarding the ANN estimator can be found in Theorem 4 of Chen
et al. (2013). With proper choice of the tuning parameter, either the rank-penalized
regression or the adaptive nuclear-norm penalized regression can achieve the best
bias–variance tradeoff among their corresponding class of estimators.
The error bounds of ANN are comparable to those of the 0 rank-penalized
estimator from (10.11) and the 1 nuclear-norm penalized estimator from (10.14)
(Bunea et al. 2011; Rohde and Tsybakov 2011). The rate of convergence is
(rx + m)r, which is the optimal minimax rate for rank sparsity under suitable
regularity conditions (Rohde and Tsybakov 2011; Bunea et al. 2012). However,
the bounds for the nuclear-norm penalized estimator from (10.14) were obtained
with some additional assumptions on the design matrix X, and its tuning sequence
for achieving the smallest mean squared error usually does not lead to correct rank
recovery (Bunea et al. 2011). While both the rank-penalized estimator and the ANN
method are able to achieve correct rank recovery and minimal mean squared error
simultaneously, the latter possesses a continuous solution path produced by data￾driven adaptive penalization that may lead to improved empirical performance.
Other theoretical aspects of high-dimensional low-rank estimation problems
can be found in Negahban and Wainwright (2011), Rohde and Tsybakov (2011),
Koltchinskii et al. (2011), and Lu et al. (2012).
10.5 Integrative Reduced-Rank Regression: Bridging Sparse
and Low-Rank Models
Multi-view data, or measurements of several distinct yet interrelated sets of
characteristics pertaining to the same set of subjects, have become increasingly
common in various fields. In a human lung study, for example, segmental airway
tree measurements from CT-scanned images, patient behavioral data from ques￾tionnaires, gene expressions data, together with multiple pulmonary function test
results from spirometry, were all collected. Unveiling lung disease mechanisms then
amounts to linking the microscopic lung airway structures, the genetic information,
and the patient behaviors to the global measurements of lung functions. In an
Internet network analysis, the popularity and influence of a web page are related
to its layouts, images, texts, and hyperlinks as well as to the content of other
web pages that link back to it. In the Longitudinal Studies of Aging (LSOA)10.5 Integrative Reduced-Rank Regression: Bridging Sparse and Low-Rank. . . 299
(Stanziano et al. 2010), the interest is to predict current health conditions of patients
using historical information of their living conditions, household structures, habits,
activities, medical conditions, among others. The availability of such multi-view
data has made tackling many fundamental problems possible through an integrative
statistical learning paradigm, whose success owes to the utilization of information
from various lenses and angles simultaneously.
The aforementioned problems can all be cast under a multivariate regression
framework, in which both the responses and the predictors can be high-dimensional,
and in addition, the predictors admit some natural grouping structure. We use this
simple yet general framework for achieving integrative learning. To formulate,
suppose we observe Xk ∈ Rnk×T for k = 1,...,K, each consisting of T copies
of independent observations from a set of predictor/feature variables of dimension
nk, and also we observe data on m response variables Y ∈ Rm×T . Let X =
[X
1,..., X
K]
 ∈ Rn×T be the design matrix collecting all the predictor sets/groups,
with n = K
k=1 nk. Both n and m can be much larger than the sample size T .
Consider the multivariate linear regression model,
Y = CX +  = 
K
k=1
CkXk + , (10.27)
where C = [C1,...,CK] ∈ Rm×n is the unknown regression coefficient matrix
partitioned corresponding to the predictor groups, and  contains independent
random errors with zero mean. We denote q = min(n, m) and qk = min(nk, m).
For simplicity, we assume both the responses and the predictors are centered so
there is no intercept term. Again, the naive least squares estimation fails miserably
in high dimensions as it leverages neither the associations among the responses nor
the grouping of the predictors.
When the predictors exhibit a group structure as in model (10.27), a group
regularization approach, for example, the convex group Lasso method (Yuan and
Lin 2006), can be readily applied to promote groupwise predictor selection; a
comprehensive review of these sparse methods is provided by Huang et al. (2012).
On the other hand, for problems with multivariate response, reduced-rank regression
has been attractive, where a low-rank constraint on the parameter matrix induces an
interpretable latent factor formulation and conveniently enables information sharing
among the regression tasks.
In essence, to best predict the multivariate response, sparse methods search for
the most relevant subset or groups of predictors, while reduced-rank methods search
for the most relevant subspace of the predictors. However, neither class of methods
can fulfill the needs in the aforementioned multi-view problems. The predictors
within each group/view may be strongly correlated, each individual variable may
only have weak predictive power, and it is likely that only a few of the views are
useful for prediction. Indeed, in many studies, it is largely the collective effort of
some groups of predictors that drives the outcomes.300 10 High-Dimensional Reduced-Rank Regression
We consider an integrative reduced-rank regression (iRRR) model, where
the integration is in terms of the multi-view predictors. To be specific, under
model (10.27), the iRRR approach assumes that each set of predictors has its own
low-rank coefficient matrix, i.e., each Ck is possibly of low rank or even a zero
matrix, i.e., 0 ≤ rk ≤ qk where rk = r(Ck), for k = 1,...,K. As such, latent
features or relevant subspaces are extracted from each predictor set Xk under the
supervision of the multivariate response Y, and the sets of latent variables/subspaces
in turn jointly predict Y.
The iRRR model strikes a balance between flexibility and parsimony, as it nicely
bridges two seemingly quite different model classes: reduced-rank models and
sparse models. On the one hand, iRRR generalizes the two-set regressor model
discussed in Chapter 3 by allowing multiple sets of predictors, each of which can
correspond to a low-rank coefficient matrix. On the other hand, iRRR subsumes
sparse model setup by allowing the rank of Ck be zero, for any k = 1,...,K, i.e.,
the coefficient matrix of a predictor group could be entirely zero.
The groupwise low-rank structure in iRRR is distinctive from a globally low-rank
structure for C in classical reduced-rank regression models. The low rankness of
Ck’s does not necessarily imply that C is of low rank. Conversely, if C is of low rank,
i.e., r = r(C)  min(n, m), all we know for sure is that the rank of each Ck is upper
bounded by r. Nevertheless, we can first attempt an intuitive understanding of the
potential parsimony of iRRR in multi-view settings. The numbers of free parameters
in C (the naive degrees of freedom) for an iRRR model, a globally reduced-rank
model, and a group-sparse model are df1 = K
k=1(nk + m − rk)rk, df2 = (n +
m − r)r, and df3 = K
k=1 nkmI (rk = 0), respectively, where I (·) is an indicator
function. With high-dimensional multi-view data, consider the scenario that only a
few views/predictor groups impact the prediction in a collective way, i.e., rk’s are
mostly zero, and each nonzero rk could be much smaller than min(nk, m). Then df1
could be substantially smaller than both df2 and df3. For example, if r1 > 0, while
rk = 0 for any k > 1 (i.e., r = r1), we have df1 = (n1 + m − r1)r1, df2 = (n + m −
r1)r1, and df3 = n1m, respectively. Another example is when r = K
k=1 rk, e.g.,
Ck’s in model (10.27) have distinct column spaces. Since K
k=1(nk + m − rk)rk ≤
{m + K
k=1(nk − rk)}{K
k=1 rk} = (n + m − r)r, iRRR is more parsimonious than
the globally reduced-rank model.
10.5.1 Composite Nuclear-Norm Penalization
To recover the view-specific low-rank structure in the iRRR model, consider a
convex optimization approach via composite nuclear-norm penalization,
C
 ∈ arg min
C∈Rm×n
1
2

Y − CX
2
F + λ

K
k=1
wk
Ck
", (10.28)10.5 Integrative Reduced-Rank Regression: Bridging Sparse and Low-Rank. . . 301
where 
Ck
" = qk
j=1 dj (Ck) is the nuclear norm of Ck, wk’s are some pre￾specified weights, and λ is a tuning parameter controlling the amount of regular￾ization. The use of the weights is to adjust for the dimension and scale differences
of Xks. We choose
wk = d1(Xk){
√m + 0
r(Xk)}, (10.29)
based on a concentration inequality of the largest singular value of a Gaussian
matrix. This choice balances the penalization of different views and allows the use
of only a single tuning parameter to achieve desired statistical performance; see
Section 10.5.2 for additional details.
Through the composite nuclear-norm penalization, the iRRR approach can
achieve view selection and view-specific subspace selection simultaneously. This
shares the same spirit as the bi-level selection methods for univariate sparse regres￾sion (Breheny and Huang 2009; Chen et al. 2016). Moreover, iRRR seamlessly
bridges group-sparse and low-rank methods as its special cases:
• Case 1: Nuclear-norm penalized regression (NNP). When n1 = n and K =
1, (10.28) reduces to the NNP method as in (10.14), which learns a global low￾rank association structure.
• Case 2: Multi-task learning (MTL). When nk = 1 and n = K, (10.28) becomes
a special case of MTL (Caruana 1997), in which all the tasks are with the same
set of features and the same set of samples. MTL achieves integrative learning
by exploiting potential information sharing across the tasks, i.e., all the task
models share the same sparsity pattern of the features.
• Case 3: Lasso and group Lasso. When m = 1, (10.28) becomes a group Lasso
method, as 
Ck
" = 
Ck
2 when Ck ∈ Rnk . Further, when nk = 1 and n =
K, (10.28) reduces to a Lasso regression.
The convex optimization of (10.28) has no closed-form solution in general,
but one can solve it with an alternating direction method of multipliers (ADMM)
algorithm (Boyd et al. 2011). The details can be found in Li et al. (2019), with
extensions on handling incomplete data and non-Gaussian response; these topics
are also discussed later in Chapter 11.
10.5.2 Theoretical Analysis
We investigate the theoretical properties of the iRRR estimator from solving the
composite nuclear-norm penalization problem. In particular, we derive its non￾asymptotic performance bounds for estimation and prediction. The results recover
performance bounds of several related methods, including Lasso, group Lasso, and
NNP. We further show that iRRR is capable of substantially outperforming those
methods under realistic settings of multi-view learning.302 10 High-Dimensional Reduced-Rank Regression

We mainly consider the multi-view regression model in (10.27), Y = K
k=1 CkX + , and the iRRR estimator in (10.28) with the weights defined
in (10.29),
C
 ∈ arg min
C∈Rm×n
1
2

Y − CX
2
F + λ

K
k=1
d1(Xk)
!√m + 0
r(Xk)
"

Ck
".
Define Z = XX
/T , and Zk = XkX
k/T , for k = 1,...,K. We scale the rows of X
such that the diagonal elements of Z all equal to 1. Denote Λl(Z) as the lth largest
eigenvalue of Z, so that Λl(Z) = dl(X)2/T .
Theorem 10.4 Assume  has i.i.d. N (0, σ2) entries. Let λ = (1 + θ )σ, with θ > 0
arbitrary. Then with probability at least 1−K
k=1 exp[−θ 2{m+r(Xk)}/2], we have

C
X − CX
2
F ≤ 
BX − CX
2
F + 4λ

K
k=1
d1(Xk)
!√m + 0
r(Xk)
"

Bk
",
for any B = [B1,...,BK] ∈ Rm×n with Bk ∈ Rm×nk , k = 1,...,K.
Theorem 10.4 shows that C
balances the bias term 
BX−CX
2
F and the variance
term 4λ
K
k=1 d1(Xk){
√m + √r(Xk)}
Bk
" in prediction.
Now consider the estimation performance of C
. For the low-dimensional sce￾nario that dn(X) > 0, an oracle inequality for C
 can be readily obtained from
Theorem 10.4. We therefore mainly focus on the general high-dimensional scenario.
Motivated by Lounici et al. (2011), Negahban and Wainwright (2011), Koltchinskii
et al. (2011), among others, we impose a general restricted eigenvalue (RE)
condition for the multi-view regression model in (10.27). We say that X satisfies
RE condition over a restricted set C(r1,...,rK; δ) ⊂ Rm×n if there exists some
constant κ(X) > 0 such that
1
2T 
ΔX
2
F ≥ κ(X)
Δ
2
F , for all Δ ∈ C(r1,...,rK; δ).
Here each rk is an integer satisfying 1 ≤ rk ≤ min(nk, m) and δ is a tolerance
parameter.
To specify the restricted set C, we need some additional constructions. For
each Ck ∈ Rm×nk (k = 1,...,K), let Ck = VkDkU
k be its full singular value
decomposition, where Uk ∈ Rnk×nk , Vk ∈ Rm×m satisfy U
kUk = Ink and
V 
kVk = Im. For each r ∈ {1, 2, ··· , qk}, where qk = min(nk, m), let Ur
k , V r
k
be the submatrices of singular vectors associated with the top r singular values of
Ck. Define the following subspaces of Rm×nk :
A(Ur
k , V r
k ) = {Δk ∈ Rm×nk ; col(Δk) ⊂ V r
k ,row(Δk) ⊂ Ur
k },10.5 Integrative Reduced-Rank Regression: Bridging Sparse and Low-Rank. . . 303
B(Ur
k , V r
k ) = {Δk ∈ Rm×nk ; col(Δk) ⊥ V r
k ,row(Δk) ⊥ Ur
k },
where row(Δk) and col(Δk) denote the row space and column space of Δk,
respectively. We may adopt the shorthand notation Ar
k and Br
k when no confusion
arises. Let PBrk
k
denote the projection operator onto the subspace Brk
k , and define
Δ
k = PBrk
k
(Δk) and Δ
k = Δk − Δ
k . We now define the restricted set
C(r1,...,rK; δ) = 3
Δ ∈ Rm×n; 
Δ
F ≥ δ,

K
k=1
wk
Δ
k
" ≤ 
K
k=1
⎛
⎝3wk
Δ
k
" + 4wk

qk
j=rk+1
dj (Ck)
⎞
⎠
⎫
⎬
⎭ ,
(10.30)
where δ is a tolerance parameter and wk = d1(Xk)(√m + √r(Xk))/T , as defined
in (10.29).
We are now ready to present the main results on the estimation error of the
integrative reduced-rank estimator C
.
Theorem 10.5 Assume that  has i.i.d. N (0, σ2) entries. Suppose X satisfies the
RE condition with parameter κ(X) > 0 over the set C(r1,...,rK; δ). Let λ = 2(1+
θ )σ with θ > 0 arbitrary. Then with probability at least 1 − K
k=1 exp{−θ 2(m +
r(Xk))/2},

C
 − C
2
F  max#
δ2, σ2(1 + θ )2
K
k=1
Λ1(Zk)
κ(X)2
(
√m + √r(Xk))2rk
T ,
σ (1 + θ )
K
k=1
√Λ1(Zk)
κ(X)
(
√m + √r(Xk))(qk
j=rk+1 dj (Ck))
√T
$
.
On the right-hand side of the above upper bound, the first term is from
the tolerance parameter in the RE condition, which ensures that the condition
can possibly hold when the true model is not exactly low-rank (Negahban and
Wainwright 2011), i.e., when qk
j=rk+1 dj (Ck) = 0. The second term gives the
estimation error of recovering the desired view-specific reduced-rank structure, and
the third term gives the approximation error incurred due to approximating the true
model with the view-specific reduced-rank model. When the true model has exactly
the desired structure, i.e., r(Ck) = rk, it suffices to take δ = 0 and the upper bound
then yields the estimation error

C
 − C
2
F  σ2
K
k=1
(m + r(Xk))rk/T,304 10 High-Dimensional Reduced-Rank Regression
which holds with high probability in the high-dimensional setting that m+r(Xk) →
∞. In the classical setting of T → ∞ with fixed m and r(Xk), by choosing θ ∝
√log T , the rate becomes σ2 log(T )K
k=1 rk/T with probability approaching 1.
Intriguingly, the results in Theorem 10.5 can specialize into oracle inequalities
of several regularized estimation methods, including NNP, MTL, Lasso, and group
Lasso. This is because these models can all be viewed as special cases of the iRRR
model. As such, iRRR seamlessly bridges sparse models and low-rank methods and
provides a unified theory of the two types of regularization.
First, consider the NNP method defined in (10.14), which corresponds to the
special case of K = 1 and wk = 1 in iRRR. The restricted set in (10.30) becomes
C(r) = {Δ ∈ Rm×n; 
Δ
" ≤ 3
Δ

"},
where Δ = PBr (Δ) and Δ = Δ − Δ. Theorem 10.5 then implies that under the
RE condition with κ(X) > 0 over C(r), if we choose λ = 2σ (1 + θ )d1(X){
√m +
√r(X)}, then with probability at least 1 − exp{−θ 2(m + r(X))/2}, it holds that

C
 − C
2
F  σ2
κ(X)2
(
√m + √r(X))2r
T .
This bound recovers the known results on NNP, see, e.g., Negahban and Wainwright
(2011).
Next, consider the MTL setting, which corresponds to nk = 1 and n = K in
iRRR. Write C = [C1,...,Cn] ∈ Rm×n, and S = {j ; 
Cj 
2 = 0}. The restricted
set becomes
C(S) =
#
Δ = [Δ1,...,Δn] ∈ Rm×n;

k∈Sc

Δk
2 ≤ 3

k∈S

Δk
2
$
.
By choosing λ ∝ σ
√log n/m, Theorem 10.5 yields the high-probability bound

C
 − C
2
F  σ2
κ(X)2
(log n + m)|S|
T ,
where |S| is the cardinality of S. The same bound can be obtained from results in
Lounici et al. (2011) in a more general setting of MTL, or from results in Negahban
et al. (2012) on group Lasso by vectorizing the MTL problem here into a univariate
response regression.
Another example is Lasso, which corresponds to m = 1 and n = K in iRRR.
It is seen that the integrative model now reduces to a conventional linear regression
model with a univariate response, and correspondingly the composite nuclear norm10.6 Applications 305
degenerates to the 1 norm of the coefficient vector C ∈ Rn. Let S = {j ; cj = 0};
then the restricted set becomes
C(S) =
#
Δ = (Δ1,...,Δn)
 ∈ Rn;

k∈Sc
|δk| ≤ 3

k∈S
|δk|
$
.
Theorem 10.5 implies that by choosing λ ∝ σ
√log n,

C
 − C
2
2  σ2
κ(X)2
log n · |S|
T
holds with high probability. Again, this is a well-known result and has been
discussed in Section 10.2.
To see the potential advantage of iRRR over NNP or MTL, we make some
comparisons of their error rates based on Theorem 10.5. To convey the main
message, consider the case where nk = n1, r(Xk) = r(X1) for k = 1,...,K,
rk = r1 for k = 1,...,s, and rk = 0 for k = s + 1,...,K. The error rate is
σ2sr1(m+r(X1))/T , σ2r(m+r(X)))/T , for iRRR and NNP, respectively. As long
as sr1 = O(r), iRRR achieves a faster rate because that r(X1) ≤ r(X) always holds.
For comparing iRRR and MTL, we get that with probability 1−n−1, iRRR achieves
an error rate σ2(log n+m+r(X1))sr1/T (by choosing θ = 0
4 log n/(m + r(X1))),
while MTL achieves σ2(log n + m + 1)sn1/T . The two rates agree with each other
in the MTL setting when r(X1) = r1 = n1 = 1, and the former rate can be much
faster in the iRRR setting when, for example, r1  n1 and r(X1) = O(log(n)+m).
These theoretical results justify the superiority of iRRR over NNP and MTL for
analyzing multi-view data.
10.6 Applications
10.6.1 Breast Cancer Data
We consider a breast cancer data set consisting of gene expression measurements
and comparative genomic hybridization measurements for n = 89 subjects. The
data set is available in the R package PMA (Witten et al. 2009), and a detailed
description can be found in Chin et al. (2006).
Prior studies have demonstrated that certain types of cancer are characterized
by abnormal DNA copy-number changes (Pollack et al. 2002; Peng et al. 2010).
Thus it is of interest to examine the relationship between DNA copy-number
variations and gene expression profiles, for which multivariate regression methods
can be useful. Biologically, it makes sense to regress gene expression profiles on
copy-number variations because the amplification or deletion of the portion of
DNA corresponding to a given gene may result in a corresponding increase or
decrease in expression of that gene. The reverse approach is also meaningful, in306 10 High-Dimensional Reduced-Rank Regression
that the resulting predictive model may identify functionally relevant copy-number
variations. This approach has been shown to be promising in enhancing the limited
comparative genomic hybridization data analysis with the wealth of gene expression
data (Geng et al. 2011; Zhou et al. 2012). We have tried both approaches. Setting 1:
designating the copy-number variations of a chromosome as predictors and the gene
expression profiles of the same chromosome as responses, and Setting 2: reversing
the roles of the predictors and the responses. We find that in setting 1, none of the
methods provides an adequate fit to the data, and reduced-rank regression may even
fail to pick up any signals. The reduced-rank models give much better results under
setting 2. We thus report only the results for setting 2.
We focus the analysis on chromosome 21, for which n = 227 and m = 44.
Both the responses and the predictors are standardized. We compare the various
reduced-rank methods by the following cross-validation procedure. The data were
randomly split into a training set of size Ttr = 79 and a test set of size Tte = 10. All
model estimation was carried out using the training data, with the tuning parameters
selected by ten-fold cross validation. We used the test data to calibrate the predictive
performance of each estimator C
, specifically, by its mean squared prediction error

Yte −C
Xte
2
F /(mTte), where (Yte, Xte) denotes the test set. The random-splitting
process was repeated 100 times to yield the average mean squared prediction error
and the average rank estimate for each method; see the upper panel of Table 10.1.
As the number of predictors is much greater than the sample size, it is reasonable
to assume that only a subset of predictors is important. Therefore, a perhaps better
modeling strategy is the subset multivariate regression with a selected subset of
predictors. Several variable selection methods have been proposed in the context of
reduced-rank regression (Chen et al. 2012; Chen and Huang 2012; Bunea et al.
2012). We modified the preceding cross-validation procedure for comparing the
reduced-rank subset regression methods. The only modification was that for each
random split, we first applied the method of Chen et al. (2012) using the training set
to select a set of predictors, with which the reduced-rank methods were subsequently
carried out using the training set and calibrated using the test set. Since our main
goal here is to compare the various reduced-rank methods, we defer the description
of the predictor selection procedures (Chen et al. 2012) to Chapter 13.
The results are summarized in the lower panel of Table 10.1. We consider
the nuclear-norm penalized estimator C
N , the rank-penalized estimator C
H , the
adaptive nuclear-norm penalized estimator C
S (with weight parameter γ = 2), the
rank-penalized estimator with an additional ridge penalty C
H R, and the adaptive
nuclear-norm penalized estimator with an additional ridge penalty C
SR. For each
method, we report the average of mean squared prediction errors (MSPE) based on
100 replications of 79/10 splits of the data and the corresponding average of the rank
estimates (Rank). The standard errors are reported in parentheses.
Table 10.1 shows that the ANN estimator C
S enjoys slightly better predictive
performance than both C
H and C
N . The number of selected predictors in the
100 splits ranges from 71 to 102; hence, incorporating variable selection greatly
reduces the number of predictors and may potentially improve model interpretation.
However, in this example, reduced-rank estimation using a subset of predictors10.6 Applications 307
Table 10.1 Comparison of the model fits to the chromosome 21 data for various reduced-rank
methods
C
N C
H C
H R C
S C
SR
Full data
MSPE 0.71 (0.2) 0.69 (0.1) 0.68 (0.1) 0.68 (0.1) 0.68 (0.1)
Rank 6.2 (0.8) 1.0 (0.0) 1.0 (0.0) 1.0 (0.2) 1.8 (0.4)
Selected predictors
MSPE 0.85 (0.2) 0.90 (0.2) 0.82 (0.1) 0.85 (0.1) 0.84 (0.1)
Rank 4.6 (0.7) 1.3 (0.5) 1.9 (0.3) 2.1 (0.2) 2.4 (0.5)
results in higher mean squared prediction error than using all predictors, uniformly
for all methods, but more so for C
H than for other methods. The nuclear-norm
penalized estimator C
N generally yields a higher rank estimate than the other
methods. Incorporating ridge penalization improves the predictive performance of
the reduced-rank methods. Particularly, C
H R may substantially outperform its non￾robust counterpart C
H . Both C
N and C
H R can be computationally intensive for
large data sets, while other methods are much faster to compute.
10.6.2 Longitudinal Studies of Aging
The Longitudinal Studies of Aging (LSOA) (Stanziano et al. 2010) had been a
collaborative effort of the National Center for Health Statistics and the National
Institute on Aging. The study interviewed a large cohort of senior people (70 years
of age and over) in 1997–1998 (WAVE II) and 1999–2000 (WAVE III), respectively,
and measured their health conditions, living conditions, family situations, health
service utilizations, among others. Here our objective is to examine the predictive
relationship between health-related events in earlier years and health outcomes in
later years, which can be formulated as a multivariate regression problem.
There are T = 3988 subjects who participated in both WAVE II and WAVE III
interviews. After data pre-processing (Luo et al. 2018), n = 294 health risk and
behavior measurements in WAVE II are treated as predictors, and m = 41 health
outcomes in WAVE III are treated as responses. The response variables are binary
indicators, characterizing various cognitive, sensational, social, and life quality
outcomes, among others. Over 20% of the response data entries are missing. The
predictors are multi-view, including housing condition (X1 with n1 = 38), family
structure/status (X2 with n2 = 60), daily activity (X3 with n3 = 40), prior medical
condition (X4 with n4 = 114), and medical procedure since last interview (X5 with
n5 = 40). We thus apply the iRRR method to perform the regression analysis.
As a comparison, we also implement generalized reduced-rank regression (gRRR)
with binary responses (Luo et al. 2018) (to be elaborated in Section 12.4), and
both classical and sparse logistic regression methods using the R package glmnet,
denoted as glm and glmnet, respectively.308 10 High-Dimensional Reduced-Rank Regression
We use a random-splitting procedure to evaluate the performance of different
methods. More specifically, each time we randomly select Ttr = 3000 subjects as
training samples and the remaining Tte = 988 subjects as testing samples. For each
method, we use fivefold CV on the training samples to select tuning parameters and
apply the method to all the training data with the selected tuning parameters to yield
its coefficient estimate. The performance of each method is measured by the average
deviance between the observed true response values and the estimated probabilities,
defined as
Average Deviance = −2
Tte
k=1
m
j=1{yjk log pjk + (1 − yjk)log(1 − pjk)}δjk
Tte
k=1
m
j=1 δjk
,
where δjk is an indicator of whether yjk is observed. To measure the predictive
performance, we also calculate the Area Under the Curve (AUC) value of the
Receiver Operating Characteristic (ROC) curve for each outcome variable. This
procedure is repeated 100 times and the results are averaged.
In terms of the average deviance, iRRR and glmnet yield very similar results
(with mean 0.77 and standard deviation 0.01), and both substantially outperform
gRRR (with mean 0.83 and standard deviation 0.01) and glm (fails due to a few
singular outcomes). The out-of-sample AUCs for different response variables are
shown in Fig. 10.2. The response variables are sorted based on their missing rates
from large (over 70%) to small (about 13%). Again, the performance of iRRR is
comparable to that of glmnet. The iRRR tends to have a slight advantage over glmnet
for responses with high missing rates. This could be due to the fact that iRRR can
borrow information from other responses, while the univariate glmnet cannot.
To understand the impact of different views on prediction, we produce heatmaps
of the estimated coefficient matrices in Fig. 10.3 (glm is omitted due to its poor
performance). The predictors fall into 5 groups, namely, housing condition, family
status, daily activity, prior medical condition, and change in medical procedure
since last interview, from top to bottom separated by horizontal black lines.
For visualization purpose, we also sort the responses based on their grouping
structure (e.g., cognition, sensation, social behavior, and life quality). The estimates
from iRRR and glmnet show quite similar patterns: it appears that the family
structure/status group and the daily activity group have the most predictive power,
and the variables within these two groups contribute to the prediction in a collective
way. As for the other three views, iRRR yields heavily shrunk coefficient estimates,
while glmnet yields very sparse estimates. These agreements partly explain the
similarity of the two methods in their prediction performance. In contrast, the
gRRR method tries to learn a globally low-rank structure rather than a view-specific
structure; consequently, it yields a less parsimonious solution with less competitive
prediction performance.10.6 Applications 309
Response Variables (Ordered by Missing Rates)
1 6 11 16 21 26 31 36 41
Out-Sample AUC
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 Comparison of Out-Sample AUCs
iRRR
gRRR
glmnet
glm
Fig. 10.2 LSOA data analysis. The mean and standard deviation (error bar) of AUC for each
response variable over 100 random-splitting procedures. The responses, from left to right, are
ordered by missing rates from large to small (Li et al. 2019)
iRRR
Response
10 20 30 40
Predictor
50
100
150
200
250
gRRR
Response
10 20 30 40
Predictor
50
100
150
200
250
glmnet
Response
10 20 30 40
Predictor
50
100
150
200
250
Fig. 10.3 LSOA data analysis. The heat maps of the coefficient matrices estimated from different
methods (Li et al. 2019)Chapter 11
Unbiased Risk Estimation
in Reduced-Rank Regression
11.1 Introduction
Reduced-rank regression, similar to any regularized estimation technique, produces
a spectrum of low-dimensional models with varying complexities. Its successful
application in practice relies on accurate model assessment and selection, for which
the quality of a model is objectively measured by its true prediction error, that is, its
error for predicting a new observation. Apparently, the true prediction error depends
on the underlying true model and has to be estimated from the data. The celebrated
Stein’s unbiased risk estimation (SURE) framework (Stein 1981) provides a general
way to construct an unbiased estimator of the true prediction error of any “nearly
arbitrary” estimator (Donoho and Johnstone 1995). It turns out that the unbiased
estimator usually is a sum of two parts. The first part is the training error based on
the observed data, and if used alone, it inevitably underestimates the true prediction
error because the same data is being used to fit the model and assess its error. The
second term is the bias correction term or the term of optimism (Hastie et al. 2009),
and it is closely related to model complexity: the “harder” we fit the data (using
a more complex model), the stronger each observation affects its own prediction,
thereby increasing the optimism. This shows that a foremost task in estimating the
true prediction error for model selection and assessment is to obtain an accurate
estimate of the model complexity, or broadly known as the model degrees of
freedom.
Degrees of freedom is a widely used concept in statistics, and in a narrow sense, it
simply counts the number of free parameters in a model. As a general measure of the
complexity of a model fitting procedure (Hastie and Tibshirani 1990), the concept
was generalized and formalized under the aforementioned SURE framework (Stein
1981). An accurate estimation of the degrees of freedom is key to evaluating the true
prediction risk for model assessment (Mallows 1973; Efron 2004). In practice, with
the estimated degrees of freedom, one can construct information criterion such as
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8_11
311312 11 Unbiased Risk Estimation in Reduced-Rank Regression
Akaike Information Criterion (AIC) (Akaike 1974), Generalized Cross-Validation
(GCV) criterion (Golub et al. 1979), among others, for model selection.
In conventional multiple linear regression model (10.1) with the least squares
estimation, the degrees of freedom is the number of regression coefficients, n.
However, for general nonlinear estimation procedures, the exact correspondence
between the degrees of freedom and the number of free parameters may not hold
(Ye 1998). For example, with the best subset selection method (Hocking and Leslie
1967), we search for the best regression model with n0 ∈ {1, 2,...,n} predictors
that minimizes the residual sum of squares. The resulting model has n0 coefficients,
but the degrees of freedom could be much higher than n0 because intuitively the
search over all the subsets of size n0 increases estimation complexity (Hastie et al.
2009). From an alternative perspective, with the best subset selection, the optimal
n0-dimensional subspace that minimizes the residual sum of squares clearly depends
on the response; as such the estimation is highly nonlinear in the response, which
results in the loss of correspondence between the degrees of freedom and the number
of free parameters.
Similar arguments apply to reduced-rank regression. Instead of searching for
the best subset of the predictors, reduced-rank regression searches for the best r￾dimensional subspace of the predictors, which leads to increased model complexity.
Because the optimal rank-r subspace depends also on the response matrix Y, the
correspondence between the number of free parameters and the degrees of freedom
need not hold. Indeed, the number of free parameters in a m × n matrix of rank r,
(n+m−r)r, has been suggested (see Chapter 2) as a naive estimate of the degrees of
freedom of reduced-rank regression with the rank constraint r ≤ min(m, n). More
precisely, with an arbitrary and possibly rank-deficit predictor matrix X, the number
of free parameters equals to (rx + m − r)r, where rx = r(X) denotes the rank of
X. Henceforth, we refer to (rx + m − r)r as the naive estimator of the degrees of
freedom of a rank-r reduced-rank regression.
In this chapter, we present a finite-sample unbiased estimator of the degrees of
freedom for a general class of reduced-rank estimators (Mukherjee et al. 2015).
The finding that the unbiased estimator admits an explicit form as a function of the
estimated singular values is quite elegant. We demonstrate that using the unbiased
estimator leads to more accurate model evaluation and selection in reduced-rank
regression.
11.2 Degrees of Freedom
Stein (1981), in his theory of unbiased risk estimation, first introduced a formal
definition of the degrees of freedom of a statistical estimation procedure. Later Efron
(2004) showed that Stein’s treatment can be considered as a special case of a more
general notion under the Gaussian assumption. Assume that we have data of the
form Y ∈ RT , X ∈ Rn×T . Given X, the response originates from the model Y ∼
(μ, σ2I ), where μ ∈ RT is the true mean that can be a function of X, and σ2 is11.2 Degrees of Freedom 313
the common variance. Then for any estimation procedure m(·) with fitted values
μ = m(X,Y) ∈ RT , the degrees of freedom of m(·) is defined as
df(m) = 
T
i=1
cov(μi, Yi)/σ2. (11.1)
The rationale is that more complex models would provide better fit for the data,
and hence the covariance between observed and fitted pairs would be higher. This
expression is not directly observable except for certain simple cases, for example,
when m(X,Y) = SY is a linear smoother, with S ∈ RT ×T being a function of X
only. In that case, it is not difficult to see that df(m) = tr(S), which agrees with the
classical definition of the degrees of freedom (Hastie and Tibshirani 1990). More
progress can be made for the case when Y ∼ N (μ, σ2I ). It was proven that as long
as the partial derivative ∂μi/∂Yi exists almost everywhere for all i ∈ {1,...,T }, it
holds true that
cov(μi, Yi) = σ2E
∂μi
∂Yi

.
This leads to the following unbiased estimator of the degrees of freedom for the
fitting procedure m(·),
df(m) = 
T
i=1
∂μi
∂Yi
. (11.2)
Using the degrees of freedom definition in (11.1), Efron (2004) employed a
covariance penalty approach to prove that the CP-type statistics (Mallows 1973),
namely
CP(μ) = 1
T 
Y − μ
2
2 +
2df(m)
T σ2, (11.3)
provide an unbiased estimator of the true prediction error. This demonstrates the
important role played by the degrees of freedom in model assessment and gives
a principled way to select the optimal model without using more computationally
expensive methods such as cross validation (Efron 2004).
Many important works followed Stein (1981). Donoho and Johnstone (1995)
used the unbiased risk estimation framework to derive the degrees of freedom for
the soft-thresholding operator in wavelet shrinkage. Meyer and Woodroofe (2000)
employed this framework to derive the same for shape-restricted regression. Li and
Zhu (2008) used this approach to compute an unbiased estimator of the degrees of
freedom of penalized quantile regression. Zou et al. (2007) showed that the number
of nonzero coefficients provides an unbiased estimator of the degrees of freedom for
the Lasso. These results are of great practical importance, allowing one to use model314 11 Unbiased Risk Estimation in Reduced-Rank Regression
selection criteria such as CP for selecting the corresponding regularized estimators
without incurring extra computational cost.
However, beyond a few special cases, the unbiased estimator in (11.2) in general
may not admit an explicit analytical form. To overcome the analytical difficulty in
estimating the degrees of freedom for general statistical procedures, Ye (1998) and
Shen and Ye (2002) proposed the generalized degrees of freedom approach, where
they evaluated partial derivatives in (11.2) numerically, using data perturbation
techniques to compute an approximately unbiased estimator. Efron (2004) used a
parametric bootstrap to also arrive at an approximately unbiased estimator of (11.1).
In this method the objective is to directly estimate cov(μi, Yi) by drawing repeated
samples from the underlying distribution and fitting the model. In the absence of
such samples, this can be achieved by using a parametric bootstrap to simulate
data from a larger unbiased model and computing the covariance between the fitted
and observed values. Although these simulation approaches allow the estimation of
the exact degrees of freedom in many highly nonlinear modeling procedures, they
are computationally expensive, and the lack of any closed-form expression makes
investigation of the theoretical properties a difficult task.
11.3 Degrees of Freedom of Reduced-Rank Estimation
Estimating the degrees of freedom for reduced-rank regression is a challenging
problem because of the nonlinear nature of the estimator. Even though the reduced￾rank solution admits a closed-form, it is highly nonlinear and involves complex
matrix operations such as the singular value decomposition. Here we study the
degrees of freedom of a general class of reduced-rank estimators in the framework
of unbiased risk estimation. As it turns out, the unbiased estimator indeed admits an
explicit form.
Recall the multivariate linear regression model in (10.6). Let Y ∈ Rm×T be the
least squares fitted value matrix, with a singular value decomposition of the form
Y = YX
(XX
)
−X = V
m× ¯r D
r¯× ¯r
W
r¯×n
, (11.4)
where A− denotes the Moore-Penrose inverse (Moore 1920; Penrose 1955) of
matrix A. The Moore-Penrose inverse is well defined for an arbitrary set of
dimensions (m, n, T ) as well as for a potentially rank-deficient predictor matrix
X. In (11.4), V and W are orthogonal matrices that represent the left and right
singular vectors of Y and D = diag{di, i = 1,...,r¯}, with d1 ≥ ··· ≥ dr¯ > 0
being the nonzero singular values of Y. Without loss of generality we assume that
r(Y) = ¯r = min(rx , m), where rx denotes the rank of X. We denote the kth column
of W and V by wk and vk, respectively.
Using the Eckart–Young Theorem (Eckart and Young 1936) (see Theorem 2.1),
the fitted value matrix from the rank-r reduced-rank regression is given by the best11.3 Degrees of Freedom of Reduced-Rank Estimation 315
rank-r approximation of the least square fitted value matrix, which can be expressed
as
Y(r) =

r
k=1
vkv
k

Y = V (r)D(r)W(r)
, r = 1,...,r,¯ (11.5)
where A(r) denotes the first r columns of A. More generally, we consider a broad
class of reduced-rank estimators with fitted value matrix given by
Y(λ) = C(λ)  X = r¯
k=1
sk(dk, λ)dkvkw
k =

r¯
k=1
sk(dk, λ)vkv
k

Y, (11.6)
where each sk(dk, λ) ∈ [0, 1] is a function of dk and λ, and they satisfy s1(d1, λ) ≥
··· ≥ sr¯(dr¯, λ) ≥ 0. For simplicity, we write sk(dk, λ) = sk(λ) = sk. It is
immediately seen that both the reduced-rank regression estimator in (11.5) and the
adaptive nuclear-norm penalized estimator in (10.24) are special cases. This class
of estimators has the same set of singular vectors as the reduced-rank regression
estimator in (11.5), but may have different singular value estimates given by shrunk
or thresholded versions of the singular values of the least squares estimator. As
discussed in Section 10.4, they can be obtained from a non-convex singular value
penalization or thresholding operations and possess many desirable theoretical
properties.
We now apply the definition in (11.2) to derive an unbiased estimator of
the degrees of freedom for the above class of reduced-rank estimators. Hansen
(2018) showed that weak differentiability of reduced-rank estimator is sufficient
to guarantee that Stein’s Lemma applies here. We start by rewriting the multivariate
linear regression model (10.6) as a multiple linear regression model
vec(Y
)
mT ×1
= 
Im ⊗ X

mT ×mn
vec(C
) mn×1
+ vec(
)
mT ×1
,
where ⊗ denotes the Kronecker product between matrices, and vec(·) stands for the
column-wise vectorization operator on a matrix. This allows us to directly apply
results derived under the vector case such as those in (11.1) and (11.2).
To facilitate the calculation of the degrees of freedom, let XX = QS2Q be the
eigen decomposition of XX
, that is, Q ∈ Rn×rx , Q
Q = I , and S ∈ Rrx×rx is a
diagonal matrix with positive diagonal elements. Then, the Moore-Penrose inverse
of XX can be written as (XX
)− = QS−2Q
. Define
H = S−1Q
XY
.
It follows that H ∈ Rrx×m admits a singular value decomposition of the form
H = UDV 
, (11.7)316 11 Unbiased Risk Estimation in Reduced-Rank Regression
where U ∈ Rrx× ¯r, U
U = I , and V , D are defined in (11.4). The matrix H
shares the same set of singular values and right singular vectors with Y in (11.4),
as H
H = YY = YX
(XX
)−XY
. Moreover, H is full rank because Y is of rank
r¯ = min(rx , m). The matrix H plays a key role in deriving a simple form for the
degrees of freedom. In particular, this construction allows us to avoid singularities
arising from rx < m in the high-dimensional scenario.
Now, upon using the above construction, we can write Y = H
S−1Q
X and
Y(r) = H (r)
S−1Q
X, r = 1,...,r,¯
where H (r) = U(r)D(r)V (r)
. Then, applying definition (11.2) and using the
identities tr(AB) = tr(BA), vec(ABC) = (C ⊗ A)vec(B) and the chain rule
of differentiation, we obtain the following expression of the exact estimator of the
degrees of freedom,
df(r) = tr )
∂vec{Y(r)
}
∂vec(Y
)
*
= tr

Im ⊗ X
QS−1
 
∂vec{H (r)}
∂vec(H )   ∂vec(H )
∂vec(Y
)
 
= tr

Im ⊗ X
QS−1
 
∂vec{H (r)}
∂vec(H )  

Im ⊗ S−1Q
X

= tr 
∂vec{H (r)}
∂vec(H ) 
. (11.8)
Similarly, for the general class of reduced-rank estimators in (11.6), we have
Y(λ) =

r¯
k=1
sk(dk, λ)vkv
k

H
S−1Q
X = V D(λ)U  
S−1Q
X,
where D(λ)  = diag{sk(dk, λ)dk, k = 1,...,r¯}. Once again using familiar results
in matrix algebra we arrive at a simpler expression for the exact estimator of the
degrees of freedom
df(λ) = tr 
∂vec{H (λ)  }
∂vec(H ) 
, (11.9)
where H (λ)  = UD(λ)V  
.
It is now clear that the problem of estimating the degrees of freedom reduces
to the computation of the divergence measure of a low-rank approximation of the
matrix H with respect to H itself. This necessarily involves the derivatives of the
singular values and singular vectors with respect to the entries of the matrix, which
are not only highly nonlinear functions of the matrix, but are also discontinuous on11.3 Degrees of Freedom of Reduced-Rank Estimation 317
certain subsets of matrices (O’Neil 2005). Fortunately, there has been a considerable
amount of work on the smoothness and differentiability of the singular value
decomposition of a real matrix, see Magnus and Neudecker (1998), O’Neil (2005).
In particular, we refer to Theorem 1 in Mukherjee et al. (2015) on the derivatives of
the singular values and singular vectors of a matrix with respect to any entry of the
matrix.
Even with such powerful matrix algebra results, it is still not at all clear whether
the degrees of freedom estimators in (11.8) and (11.9) may admit explicit forms.
Examining the structure of the singular value decomposition sheds light on this. The
pairs of singular vectors (uk, vk) are orthogonal to each other, representing distinct
directions in Rrx×m without any redundancy. Intuitively, these directions themselves
are indistinguishable, and their relative importance in constituting the matrix H
are entirely revealed by the singular values. This suggests that the complexity of
reduced-rank estimation, as exhibited by the relative complexity of a low-rank
approximation H (r) or H (λ)  of H, may only depend on the singular values of the
matrix H and the mechanism of singular value shrinkage or thresholding. This is the
main intuition that motivated the findings for the explicit forms of (11.8) and (11.9),
which are summarized in the following theorems.
Theorem 11.1 Let Y be the least squares fitted value matrix as given in (11.4).
Let rx = r(X) and r¯ = r(Y) = min(rx , m). Suppose the singular values of Y
satisfy d1 > ··· > dr¯ > 0. Consider the reduced-rank estimator Y(r) in (11.5). An
unbiased estimator of the effective degrees of freedom is
df(r) =
⎧
⎪⎨
⎪⎩
max(rx , m)r +r
k=1
r¯
l=r+1
d2
k + d2
l
d2
k − d2
l
,r< r¯;
rxm, r = ¯r.
The above results are further generalized to the class of reduced-rank estimators
in (11.6). The weights sk(dk, λ) are treated as random quantities because they are
usually functions of the singular values.
Theorem 11.2 Let Y be the least squares fitted value matrix as given in (11.4). Let
r¯ = rank(Y) = min(rx , m) and suppose the singular values of Y satisfy d1 > ··· >
dr¯ > 0. Consider the reduced-rank estimator Y(λ) in (11.6), and let r = r(λ) =
max{k : sk(dk, λ) > 0.}. An unbiased estimator of the effective degrees of freedom is
df(λ) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
max(rx , m)r
k=1
sk +r
k=1
r¯
l=r+1
sk(d2
k + d2
l )
d2
k − d2
l
+r
k=1
r
l=k
d2
k (sk−sl)
d2
k −d2
l
+ r
k=1 dks
k, r < r¯;
max(rx , m)r
k=1
sk +r
k=1
r
l=k
d2
k (sk − sl)
d2
k − d2
l
+r
k=1
dks
k, r = ¯r,318 11 Unbiased Risk Estimation in Reduced-Rank Regression
where for simplicity we write sk = sk(dk, λ) and s
k = ∂sk(dk, λ)/∂dk, the partial
derivative of sk(dk, λ) with respect to dk.
The explicit formulae presented in the above theorems facilitate further explo￾ration of the behaviors and properties of the degrees of freedom. For example,
consider the unbiased estimator for reduced-rank regression in Theorem 11.1. It
is always true that
df(r) ≥ max(rx , m)r +r
k=1
r¯
l=r+1
d2
k + 0
d2
k − 0 = (rx + m − r)r, r = 1,...,r.¯
(11.10)
This suggests that the exact estimator is always greater than the naive estimator, the
number of free parameters (rx + m − r)r. Similar to the Lasso in multiple linear
regression problem (Tibshirani 1996; Zou et al. 2007), reduced-rank estimation
can be viewed as a latent factor selection procedure, in which we both construct
and search over as many as r¯ latent linear factors. Therefore, the increments in
the degrees of freedom as shown in (11.10) can be interpreted as the price we
have to pay for performing this latent factor selection. For the general methods
considered in Theorem 11.2, this inequality no longer holds, due to the shrinkage
effects induced by the weights, 0 ≤ sk ≤ 1. The reduction in the degrees of freedom
due to singular value shrinkage can offset the price paid for searching over the set of
latent variables. Therefore, similar to Lasso, adaptive singular value penalization can
provide effective control over the model complexity (Tibshirani and Taylor 2012;
Mukherjee et al. 2015).
Although the unbiased estimator and naive estimators are quite different, some
interesting connections can be made. They are close to each other when they are
evaluated at the true underlying rank, especially when the signal is strong relative to
the noise level. This phenomenon is also noted in the empirical studies. Suppose the
true model rank is r(C) = r. Intuitively, the r¯−r smallest singular values from least
squares may be close to zero and are not comparable to the r largest ones; using the
approximation dk ≈ 0, k = r + 1,...,r¯, we obtain df(r) ≈ (rx + m − r)r. A
more rigorous argument can be made under classical large sample setting where n
and m are fixed and T → ∞; under standard assumptions, consistency of the least
squares estimation can readily be established. Using techniques such as perturbation
expansion of matrices (Izenman 1975), the consistency of Y implies the consistency
of the estimated singular values. The first r estimated singular values converge to
their nonzero true counterparts while the rest converge to zero in probability. It
follows that
df(r) → (rx + m − r)r,
as T → ∞ in probability. An immediate implication is that for each r = 1,...,r¯,
if we assume that the true model is of rank r, then in a classical asymptotic sense,
the number of free parameters, (rx + m − r)r, is the correct degrees of freedom.11.4 Comparing Empirical and Exact Estimators 319
This clearly relates to the error degrees of freedom of the classical asymptotic
χ2 statistic from the likelihood ratio test of H0: r(C) = k (Izenman 1975), for
each k = 1,...,r¯. If n and m are allowed to diverge with sample size T , these
classical asymptotic results would break down as the convergence of nonzero
singular values fails (Bai and Silverstein 2009). In contrast, the unbiasedness
property of the exact estimators is non-asymptotic, and hence it is valid for any
given (n, m, T ). Furthermore, non-asymptotic prediction error bounds have been
developed for reduced-rank estimation methods as discussed in Section 10.4.3, and
the minimax convergence rate coincides with the number of free parameters (Rohde
and Tsybakov 2011; Bunea et al. 2011). These results do reveal the limitations,
the underlying assumptions and the asymptotic nature of the naive estimator and
provide further justification of the exact estimators.
The derived formulae also exhibit some interesting behaviors of rank reduction.
In essence, reduced-rank methods distinguish the signal from the noise by examin￾ing the estimated singular values from least squares estimation: the large singular
values more likely represent the signals while the small singular values mostly
correspond to the noise. By rank reduction, we aim to recover the signals exceeding
the noise level. Suppose that dk and dk+1 are close for some k = 1,...,r¯ − 1. It
can be argued that the true model rank is unlikely to be k, because the (k + 1)th and
the kth layers are hardly distinguishable. Indeed, this is reflected from the degrees
of freedom: for r = k, the formula includes a term (dk + dk+1)/(dk − dk+1),
which can be excessively large. On the other hand, there is no such term for
r = k + 1. Consequently, the unbiased estimator of the degrees of freedom may not
monotonically increase as the rank r increases, in contrast to the naive estimator. In
the above scenario, the estimates for r = k can even be larger than that of r = k+1.
This automatically reduces the chance of k being selected as the final rank.
We remark that several works, including Candès et al. (2013), Mukherjee et al.
(2015), and Yuan (2016), independently studied the degrees of freedom in low-rank
estimation under the SURE framework. A rigorous verification on the applicability
of Stein’s Lemma, however, came later in Hansen (2018). For a most recent
investigation of this problem, we refer to Mazumder and Weng (2020), where the
authors emphasized the verification of the regularity conditions for applying Stein’s
lemma on a group of low-rank matrix estimators. Moreover, for those estimators
for which Stein’s Lemma does not apply readily, the estimator of the degrees of
freedom is derived through a Gaussian convolution method.
11.4 Comparing Empirical and Exact Estimators
11.4.1 Simulation Setup
We investigate the properties of the degrees of freedom estimator using numerical
simulation. Two scenarios of model dimensions are considered. In Model I, we set320 11 Unbiased Risk Estimation in Reduced-Rank Regression
T = 50, n = 20, m = 15, rx = 20, r = 5. The design matrix X is constructed
by generating its T columns as i.i.d. samples from N (0, Σx ), where Σx = (σij )n×n
and σij = ρ|i−j | with some 0 <ρ< 1. In Model II, we set T = 40, n = 100,
m = 50, rx = 15, r = 5. The design matrix X is generated as X = Σ1/2 x X0, where
Σx is defined as above, X0 = X1X2, X1 ∈ Rn×rx , X2 ∈ Rrx×T , and all entries of
X1, X2 are i.i.d. N (0, 1). To control the singular structure of CX, the right singular
vectors of C are taken as the eigenvectors of Σx . The left singular vectors of C are
generated by first generating a matrix of standard normal random samples and then
orthogonalizing the matrix by QR decomposition. The first r singular values of C
are set to be nonzero values, and the difference between successive singular values
is fixed to be two. The data matrix Y is then generated by Y = CX + , where the
elements of  are i.i.d. samples from N (0, σ2). We define the signal to noise ratio
(SNR) as dr(CX)/

F and set σ2 accordingly.
Each simulated model is then characterized by the following parameters: T
(sample size), n (number of predictors), m (number of responses), rx (rank of
the design matrix), r (rank of the coefficient matrix), ρ (design correlation), and
SNR (signal to noise ratio). The experiment was replicated 500 times for each
ρ ∈ {0.3, 0.5, 0.9}, corresponding to low, moderate, and high correlations among
the predictors, and SNR ∈ {0.1, 0.2, 0.3}, corresponding to low, moderate, and high
signal strength approximately. Three estimation methods are considered, namely the
ordinary least squares method (OLS), the classical reduced-rank regression (RRR),
and the adaptive nuclear-norm penalized regression (ANN) with γ = 2.
11.4.2 Comparing Estimators of the Degrees of Freedom
We compare the unbiased estimator of the degrees of freedom with the naive one
given by the number of free parameters. Figures 11.1 and 11.2 show the results for
the simulation models I and II, respectively, for ρ = 0.5 and SNR = 0.3. (The
results are similar in other settings.) The dashed line with triangles in each figure
shows the naive degrees of freedom. The solid line connects a sequence of boxplots
covering a range of increasing rank values (for RRR) or decreasing penalty levels
(for ANN), each of which shows the empirical distribution of the exact estimator
of the degrees of freedom. For better illustration, very few outliers declared by the
boxplots are not shown. For the ANN method, the averaged estimated ranks are also
shown as a function of the penalty level (the dot-dashed line).
The unbiased estimator is indeed quite different from the naive estimator. For the
RRR method, the curve for the exact estimator is always above the one for the naive
estimator, and the increments represent the price paid for latent variable selection.
When the rank is smaller than the true rank, the two estimates are quite close to
each other. When the rank exceeds the true rank, their discrepancy is much larger,
so is the variability of the unbiased estimator. This is related to the fact that the
estimated singular values exceeding the true rank could be small and close to each11.4 Comparing Empirical and Exact Estimators 321
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
50 100 150 200 250 300
Rank
Degrees of freedom
(a)
1 5 9 14 19 24 29 34 39 44 49 54 59 64 69 74 79 84 89 94 99
50 100 150 200 250 300
Lambda index
Degrees of freedom
Estimated rank
0 5 10 15
(b)
Fig. 11.1 Comparing naive and exact degrees of freedom estimators using Model I. (a) Estimated
degrees of freedom in the reduced-rank regression method (RRR). (b) Estimated degrees of
freedom in the adaptive nuclear-norm penalization method (ANN)
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
100 200 300 400 500 600 700
Rank
Degrees of freedom
(a)
1 5 9 14 19 24 29 34 39 44 49 54 59 64 69 74 79 84 89 94 99
100 200 300 400 500 600 700
Lambda index
Degrees of freedom
Estimated rank
0 5 10 15
(b)
Fig. 11.2 Comparing naive and exact degrees of freedom estimators using Model II. (a) Estimated
degrees of freedom in the reduced-rank regression method (RRR). (b) Estimated degrees of
freedom in the adaptive nuclear-norm penalization method (ANN). The settings are the same as in
Fig. 11.1
other. For the ANN method, the curve with the exact estimator is mostly lower than
the one with the naive estimator, demonstrating that shrinkage estimation provides
effective control over the model complexity. Again, as expected, the variability of
the unbiased estimator becomes larger when the rank exceeds the true rank.322 11 Unbiased Risk Estimation in Reduced-Rank Regression
11.4.3 Performance on Estimating the Prediction Error
We examine the performance of the exact and naive estimators of the degrees of
freedom on the estimation of true prediction error. Based on the SURE theory, we
use the CP criterion in (11.3) to estimate the prediction error. For each replication i
(i = 1,...,g, g = 500), denote the simulated data as Y(i), the estimated regression
component by RRR as Y(i)(r) and the estimated degrees of freedom as df(i)(r), for
r = 1,..., min(rx , m). The average CP prediction error is then computed as,
R(r)  = 1
T mg

g
i=1
!

Y(i) − Y(i)(r)
2
F + 2σ2df(i)(r)"
.
In practice, the error variance σ2 is usually unknown and needs to be estimated.
However, since our main purpose is to examine the behavior of the estimators of the
degrees of freedom, we simply use the true σ2 value in the above expression.
To assess how well the CP formula resembles the true prediction error, we also
compute another estimate based on independently generated test data. Specifically,
in each replication i, we first estimate the model by RRR based on the training data,
and then evaluate its prediction performance on an independently generated test set
Y[i] of the same size. We compute
R(r) = 1
T mg

g
i=1
!

Y[i] − Y(i)(r)
2
F
"
,
and refer to it as the average true prediction error.
The average CP prediction error curves are computed for both the RRR and ANN
methods and for both the proposed and naive estimators of the degrees of freedom.
Figures 11.3 and 11.4 show the simulation results for ρ = 0.5 and SNR = 0.2. (The
results are similar in other settings.) In each panel, the solid line is the true prediction
error curve, the dotted line is the CP prediction error curve based on the unbiased
exact estimator of the degrees of freedom, and the dashed line is the CP prediction
error curve based on the naive estimator. For the ANN method, the dot-dashed line
shows the averaged estimated rank as a function of the penalty levels.
The average CP curve based on the proposed estimator mostly overlaps with the
true prediction error curve, demonstrating that the proposed estimator is indeed
unbiased. On the other hand, there is an apparent discrepancy between the true
prediction error curve and the one computed based on the naive estimator, especially
when the estimated rank is close to or larger than the true rank. Moreover, the curve
based on the naive degrees of freedom is minimized at a slightly different location
than those of the other two curves. These results suggest that the proposed degrees
of freedom estimator leads to an accurate assessment of the actual prediction risk,
and using the naive estimator may lead to suboptimal performance on both model
assessment and model selection.11.4 Comparing Empirical and Exact Estimators 323
l
l
l
l l
l l l l l l l l l l
2 4 6 8 10 12 14
15 20 25
Rank
Prediction error
(a)
l
l
l
l
l
l
l
l
l l l l l l l
l
l
l
l
l
l
l l l l l l l l l l l l l l l l l l l l l l
20 40 60 80 100
12 13 14 15 16 17
Lambda index
Prediction error
Estimated rank
4 6 8 10 12 14
(b)
Fig. 11.3 Comparing prediction error estimates using Model I. (a) Estimated prediction error
curves in the reduced-rank regression method (RRR). (b) Estimated prediction error curves in
the adaptive nuclear-norm penalization method (ANN)
l
l
l
l l
l
l l l l l l l l l
2 4 6 8 10 12 14
20 25 30 35 40 45
Rank
Prediction error
(a)
l
l
l
l
l
l
l
l
l l l l l l l l
l
l
l
l l l l l l l l l l l l l l l l l l l l l l l l l l
20 40 60 80 100
20 25 30 35
Lambda index
Prediction error
Estimated rank
2 4 6 8 10 12 14
(b)
Fig. 11.4 Comparing prediction error estimates using Model II. (a) Estimated prediction error
curves in the reduced-rank regression method (RRR). (b) Estimated prediction error curves in the
adaptive nuclear-norm penalization method (ANN). All the settings are the same as in Fig. 11.3
11.4.4 Performance on Model Selection
With the estimated degrees of freedom, information criterion can be constructed
to efficiently perform model selection. We can thus examine the performance of
various information criteria based on either the unbiased or naive estimators of
degrees of freedom. For an illustration, we choose the generalized cross-validation
criterion (GCV) (Golub et al. 1979), based on the predictive performance of models.324 11 Unbiased Risk Estimation in Reduced-Rank Regression
The results of using AIC, BIC, and CP are similar. For the RRR method, the GCV
criterion based on df (r) 8 is defined as
T m
Y − Y(r)
2
F
{T m − df(r)}2 . (11.11)
Note that GCV does not involve the unknown error variance so that its estimation
is not required. The model accuracy is measured by the average of the scaled
mean squared prediction errors (MSPE) from all replications, i.e., 100
CX −
C
X
2
F /(T m). To evaluate the rank determination performance, we report the
averaged rank estimates from all replications (Rank). All the above quantities
are similarly defined for the ANN method. The GCV criteria constructed from
the unbiased and naive degrees of freedom are denoted as GCV(e) and GCV(n),
respectively. The label “(e)” denotes the use of the exact degrees of freedom
estimator in computing the model selection criteria while “(n)” denotes the use of
the naive degrees of freedom estimator. The results for various SNR and design
correlation levels are summarized in Tables 11.1 and 11.2, with the standard errors
reported in the parenthesis.
Table 11.1 Model selection performance of GCV(e) and GCV(n) using Model I
RRR ANN
ρ SNR OLS GCV(e) GCV(n) GCV(e) GCV(n)
0.3 0.1 MSPE 1362.5
(299.1)
723.76 (179.1) 739.35 (184.8) 582.64 (131.3) 605.19 (145.9)
Rank 15 (0.0) 3.28 (0.7) 3.79 (0.6) 3.94 (0.7) 3.79 (0.9)
0.2 MSPE 348.2 (76.9) 193.09 (48.2) 200.86 (50.8) 171.55 (41.3) 175.47 (42.2)
Rank 15 (0.0) 4.37 (0.6) 4.82 (0.6) 4.89 (0.7) 4.56 (0.7)
0.3 MSPE 153.42 (34) 83.62 (20.4) 85.47 (22.6) 77.38 (18.5) 78.24 (19.1)
Rank 15 (0.0) 4.98 (0.3) 5.14 (0.4) 5.22 (0.5) 5.08 (0.4)
0.5 0.1 MSPE 1476.18
(338.5)
761.8 (196.9) 786.52 (217.6) 634.21 (149.6) 657.39 (172.2)
Rank 15 (0.0) 3.47 (0.6) 3.94 (0.6) 4.07 (0.7) 3.78 (0.8)
0.2 MSPE 376.4 (83.1) 206.88 (50.5) 215.47 (54.9) 186.04 (43.6) 189.95 (44.7)
Rank 15 (0.0) 4.32 (0.5) 4.81 (0.6) 4.88 (0.7) 4.54 (0.7)
0.3 MSPE 167.62
(36.5)
90.54 (21.9) 92.7 (23.1) 84.38 (19.7) 85.4 (20.7)
Rank 15 (0.0) 4.98 (0.3) 5.14 (0.4) 5.17 (0.4) 5.06 (0.4)
0.9 0.1 MSPE 491.84 (112) 242.91 (60.3) 251.62 (66.7) 214.49 (50.5) 220.9 (54.6)
Rank 15 (0.0) 3.67 (0.5) 4.1 (0.5) 4.19 (0.7) 3.82 (0.6)
0.2 MSPE 124.55
(27.3)
67.69 (17.1) 70.6 (18.2) 61.63 (14.6) 63.09 (15.2)
Rank 15 (0.0) 4.34 (0.5) 4.82 (0.6) 4.87 (0.7) 4.47 (0.6)
0.3 MSPE 55.69 (12.1) 29.77 (7.3) 30.64 (8) 27.93 (6.7) 28.4 (7.2)
Rank 15 (0.0) 4.96 (0.3) 5.15 (0.4) 5.22 (0.5) 5.06 (0.5)11.5 Applications 325
Table 11.2 Model selection performance of GCV(e) and GCV(n) using Model II
RRR ANN
ρ SNR OLS GCV(e) GCV(n) GCV(e) GCV(n)
0.3 0.1 MSPE 2631.71
(1320.4)
1015.06 (518.9) 1046.8 (552.2) 843.65 (392.5) 877.11 (423.6)
Rank 15 (0.0) 3.51 (0.6) 3.83 (0.5) 4.24 (0.7) 3.58 (0.7)
0.2 MSPE 687.88
(340.1)
305.14 (158.1) 303.31 (159.4) 261.08 (129.2) 272.84 (135.8)
Rank 15 (0.0) 4.57 (0.5) 4.92 (0.3) 5.18 (0.5) 4.57 (0.5)
0.3 MSPE 294.98
(142.9)
122.05 (60) 122.77 (60.3) 114.58 (55.1) 114.44 (55)
Rank 15 (0.0) 5 (0.0) 5.02 (0.1) 5.15 (0.4) 5 (0.0)
0.5 0.1 MSPE 2646.61
(1345.1)
1026.89 (532.9) 1048.34 (559) 849.66 (400.1) 886.8 (436.8)
Rank 15 (0.0) 3.49 (0.6) 3.8 (0.5) 4.23 (0.7) 3.55 (0.7)
0.2 MSPE 645.17
(318.2)
284.59 (143.3) 283.43 (143.8) 243.59 (117.2) 253.44 (120.8)
Rank 15 (0.0) 4.58 (0.5) 4.93 (0.3) 5.15 (0.4) 4.57 (0.5)
0.3 MSPE 292.16
(140.5)
121.35 (59.6) 121.56 (59.5) 113.78 (55.1) 113.47 (55)
Rank 15 (0.0) 5 (0.0) 5.01 (0.1) 5.18 (0.5) 5 (0.0)
0.9 0.1 MSPE 1843.64
(1106.5)
698.59 (417.6) 715.15 (434.5) 589.27 (325) 619.54 (359.3)
Rank 15 (0.0) 3.5 (0.6) 3.8 (0.5) 4.25 (0.8) 3.51 (0.7)
0.2 MSPE 449.34
(258.1)
198.09 (118.9) 195.7 (115.3) 169.6 (97.1) 177.1 (103.5)
Rank 15 (0.0) 4.6 (0.5) 4.91 (0.3) 5.18 (0.5) 4.58 (0.5)
0.3 MSPE 199.9
(107.6)
82.88 (45.4) 83.2 (45.5) 77.89 (41.6) 77.73 (41.5)
Overall, the GCV(e) outperforms the GCV(n) on model selection in both the
RRR and ANN methods. In most of the cases, the models selected by GCV(e) have
slightly better predictive performance than those by GCV(n), especially when the
SNR level is low to moderate and the correlation between predictors is moderate to
high. We note that as expected the ANN method performs slightly better than RRR
due to its adaptive shrinkage estimation.
11.5 Applications
11.5.1 Norwegian Paper Quality Data
We consider a data set from a paper quality experiment conducted at a paper￾making factory in Norway. The goal was to examine how the m = 13 response
variables of different characteristics of paper were influenced by n = 9 predictor326 11 Unbiased Risk Estimation in Reduced-Rank Regression
Table 11.3 Model selection performance of GCV(e) and GCV(n) in the Norwegian paper quality
data analysis
RRR ANN
l% OLS GCV(e) GCV(n) GCV(e) GCV(n)
50% MSPE 2.3 (1.2) 1.76 (0.9) 1.81 (1.0) 1.51 (0.8) 1.60 (0.9)
Rank 9 (0.0) 2.50 (1.4) 2.87 (1.0) 3.01 (1.2) 2.98 (1.4)
70% MSPE 1.29 (0.6) 1.11 (0.5) 1.12 (0.6) 1.01 (0.4) 1.05 (0.5)
Rank 9 (0.0) 2.67 (1.1) 2.97 (0.7) 3.22 (0.9) 3.07 (1.1)
90% MSPE 0.89 (0.3) 0.78 (0.2) 0.78 (0.3) 0.75 (0.2) 0.76 (0.2)
Rank 9 (0.0) 2.97 (0.2) 3.01 (0.1) 3.05 (0.3) 3.07 (0.4)
variables constructed from three experimental factors and their second-order terms
for capturing interaction and nonlinear effects. The sample size is T = 29. The data
were used in Aldrin (2000), Izenman (2008) and Bunea et al. (2012) to illustrate the
efficacy of various reduced-rank approaches. A detailed description of the data set
can be found in either Aldrin (2000) or Izenman (2008).
We randomly select a proportion, say, l% of the data as the training set to fit the
regression model and obtain C
, and then use the rest of the data as the testing set
to evaluate the prediction error defined as 
Yte − C
Xte
2
F /(T m). The rank in the
RRR method and the tuning parameter in the ANN method are selected by GCV
based on either the unbiased or the naive estimator of the degrees of freedom. This
process is repeated 100 times for each l% ∈ {50%, 70%, 90%}. Table 11.3 reports
the averaged prediction error and the averaged rank estimates of all runs, along with
their standard errors. It can be seen that the GCV based on the unbiased estimator of
the degrees of freedom performs slightly better in terms of model selection than that
based on the naive estimator. For both RRR and ANN, the rank is often estimated
to be three, which agrees with previous studies (Izenman 2008; Bunea et al. 2012).
The effectiveness and applicability of the unbiased exact estimator of the degrees of
freedom are well demonstrated by this data set.
11.5.2 Arabidopsis Thaliana Data
In this application, we use several different information criteria, including Mallow’s
CP, GCV, and BIC, each with either the unbiased or the naive estimator of the
degrees of freedom, for selecting a reduced-rank model in a genetic association
study (Wille et al. 2004).
This is a microarray experiment aimed at understanding the regulatory control
mechanisms between the isoprenoid gene network in the Arabidopsis thaliana plant,
more commonly known as thale cress or mouse-ear cress. Isoprenoids serve many
important biochemical functions in plants. To monitor the gene-expression levels,
118 GeneChip microarray experiments were carried out. The predictors consist of
39 genes from two isoprenoid bio-synthesis pathways, namely MVA and MEP, and
the responses consist of gene-expression of 795 genes from 56 metabolic pathways,11.5 Applications 327
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
Cp(e)
Cp(n)
GCV(e)
GCV(n)
BIC(e)
BIC(n)
OLS
1.0
1.5
2.0
2.5
3.0
3.5
Prediction error
l
l
ll
l
l
l
l
l
l
l
l
l
l
0
10
20
30
40
CP GCV BIC
Percentage of relative gain
Fig. 11.5 Prediction performance in Arabidopsis Thaliana data (Mukherjee et al. 2015)
many of which are downstream of the two pathways considered as predictors. Thus
some of the responses are expected to show significant associations to the predictor
genes. We select two downstream pathways, Carotenoid and Phytosterol, as our
responses. It has already been proven experimentally that the Carotenoid pathway
is strongly attached to the MEP pathway, whereas the Phytosterol pathway is
significantly related to the MVA pathway. See Wille et al. (2004) and the references
therein for a more detailed discussion. We have T = 118 observations on n = 39
predictors and m = 36 responses, all log-transformed to reduce the skewness. The
responses are standardized in order to make them comparable.
For the purpose of illustration, we split the data set randomly into training and
test sets of equal size. The model is built using the training samples and the test
set is used to evaluate the predictive performance. The performance measure is the
mean squared prediction error (MSPE) on the testing data 
Yte − Yte
2
F /(mT ) .
The entire process is repeated 100 times based on random splits.
The mean squared prediction errors for all methods are summarized in Fig. 11.5.
The left panel displays the boxplot of mean square prediction errors of each
method over 100 random splits for three model selection criteria. Again, the label
“(e)” denotes the use of the exact degrees of freedom estimator in computing
the model selection criteria while “(n)” denotes the use of the naive degrees of
freedom estimator. The right panel displays the boxplot of relative gain in prediction
error for each model selection method by using the exact degrees of freedom
estimator over the naive degrees of freedom estimator. For all three model selection
criteria considered, using the exact unbiased estimator tends to outperform using
the naive estimator in terms of prediction accuracy. The relative gain is almost
always positive, shown by the right panel of Fig. 11.5. Also, among the three model
selection criteria, the BIC appears to outperform the other criteria and tends to select
a very parsimonious model that may be easier to interpret; see Table 11.4.328 11 Unbiased Risk Estimation in Reduced-Rank Regression
Table 11.4 Prediction accuracy and rank selection performance for the competing methods on
the Arabidopsis thaliana data
CP(e) CP(n) GCV(e) GCV(n) BIC(e) BIC(n) OLS
Avg(MSPE) 2.20 2.24 2.19 2.28 1.30 1.39 2.59
Std(MSPE) 0.3 0.3 0.3 0.3 0.1 0.2 0.3
Mean(Rank) 8.76 9.71 8.68 10.50 1.09 1.48 –
Std(Rank) 1.2 0.8 1.3 1.0 0.4 0.8 –Chapter 12
Generalized Reduced-Rank Regression
12.1 Introduction
Our discussion on reduced-rank models thus far is mainly focused on classical
multivariate linear regression setup. This chapter further generalizes reduced-rank
models to deal with more complicated scenarios that are commonly encountered in
real-world applications. In particular, we want to discuss several related aspects that
will enhance the use of these models. These include, robust estimation and outlier
detection in reduced-rank models with possibly contaminated data, low-rank matrix
completion with incomplete data, and generalized reduced-rank models for a mix of
continuous and discrete responses. All these topics are discussed under the general
framework of high-dimensional multivariate regression.
Outliers and anomalies are bound to occur in many applications, especially
with large-scale data. For example, in finance, stock returns of specific sectors or
companies sometimes experience dramatic short-term disturbances that may or may
not be easily explained or captured by the underlying market factors. These are often
short-lived and isolated fluctuations and need to be adequately accounted for in
studying the overall market movement that is estimated by reduced-rank models (see
Chapter 8). In some applications, besides conducting robust estimation, the interest
could also lie in the detection of anomalies hidden behind some low-dimensional
“common” signal. For example, in motion detection via video surveillance, a
reduced-rank model component is designed to extract the common background of
the images/frames, and whatever remains is taken to capture the motion of object.
Motivated by these applications, we address a fundamental problem: how and to
what extent can outliers affect reduced-rank estimation (She and Chen 2017)? We
show that indeed, reduced-rank estimation is non-robust and can breakdown with
just one extreme (abnormal) observation. Several questions follow: How to identify
potential outliers? What tools can be used for model diagnostics in reduced-rank
estimation? How to robustify reduced-rank estimation?
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8_12
329330 12 Generalized Reduced-Rank Regression
Multivariate data are also generally prone to missing values. There is often a
need to either impute the missing values or conduct modeling with incomplete data.
A famous example is the Netflix problem. Movie ratings collected from Netflix
customers are organized as a matrix, in which each entry represents the rating
of a movie by a customer; the value is observed if the customer has watched
the movie and otherwise the entry is missing. Because each customer watches
only a limited number of movies, this matrix is largely incomplete. The problem
is to build a recommender system that predicts the missing entries, that is, to
recommend movies based on predicting the movie ratings a customer would give,
had the customer watched the movies. Another example is from the longitudinal
studies of aging (Stanziano et al. 2010) discussed in Section 10.6.2. Recall that a
national representative sample of several thousands of subjects who were seventy
years of age or over were interviewed and followed. Their health, functional status,
living arrangements, and health services utilization were measured as they aged.
It is of interest to examine the changes and the association between their past
and current health status. However, there were more than 20% missing values
in the outcomes across a range of assessments on health conditions. In these
applications, the fundamental problem is termed as “matrix completion” (Candès
and Recht 2009; Recht 2011). The question then is, how can we recover the missing
entries of a partially observed and possibly contaminated low-rank matrix? We
present the main theoretical results and computational algorithms developed for
matrix completion and make connections to reduced-rank regression modeling with
incomplete response.
In many real-world problems, the collected outcomes are often of mixed types
including continuous measurements, binary indicators, and counts, rendering the
classical multivariate linear regression framework not directly applicable. In the
aforementioned study on aging, for example, continuous measurements of health,
memory and sensation scores, dichotomous measurements of various medical con￾ditions are to be predicted by subject demographics, their medical history, lifestyle,
and social behavior. The mixed outcomes, regardless of their types, are often likely
to be interrelated, reflecting diverse yet related aspects of the underlying multivariate
data generating mechanism. As such, reduced-rank modeling is expected to be
beneficial. We introduce methods that utilize mixed outcomes in reduced-rank
estimation and a simple yet effective framework that simultaneously accounts for
outliers and missing values (Luo et al. 2018).
12.2 Robust Reduced-Rank Regression
12.2.1 Non-Robustness of Reduced-Rank Regression
We have seen that reduced-rank regression can be very effective in modeling
multivariate data. It substantially reduces the number of free parameters. However,12.2 Robust Reduced-Rank Regression 331
it is yet unclear how sensitive reduced-rank estimation is to the presence of
outliers, which is likely to occur commonly in practice. How robust is reduced-rank
regression? In other words, could the low-rank structure be masked or be distorted
by outliers? Here we attempt to provide some answers from both empirical and
theoretical perspectives. The contents in this chapter are mainly drawn from She
and Chen (2017).
We use a real data example in finance to show the non-robustness of reduced-rank
regression as well as the potential power of robustification. Consider the weekly
stock log-return (the difference in log price) data for nine of the ten largest (ranked
by Fortune magazine) American corporations in 2004 (Yuan et al. 2007), with
Yk ∈ R9 (k = 1,...,T ) and T = 52. Chevron was excluded due to a big drop
in its stock price in the 38th week of the year. The nine time series are shown in
Fig. 12.1. For the purpose of constructing market factors that drive general stock
movements, a reduced-rank vector autoregressive (VAR) model of order 1 can be
used (Yk = CYk−1+k, with C of reduced rank). By conditioning on the initial state
and assuming the normality of k, the conditional likelihood leads to a least squares
criterion, so the estimation of C can be formulated as a reduced-rank regression
problem. However, as shown in Fig. 12.1, several stock returns have experienced
abrupt changes. The autoregressive structure makes any outlier in the time series
also a leverage point in the covariates, which in this case are the lagged values.
Using the weekly log-returns in the first 26 weeks for training and those in the
last 26 weeks for validation, we can analyze the data with both the classical reduced￾rank regression and a robust reduced-rank regression approach (to be introduced
later). While both methods result in unit-rank models, the robust reduced-rank
regression automatically detects three outliers. These are the log-returns of Ford
at weeks 5 and 17 and the log-return of General Motors at week 5. These two weeks
correspond to major market disturbances attributed to the auto industry. By taking
the outlying samples into account, the robust approach has led to a more reliable
model. Table 12.1 displays the factor coefficients indicating how the stock returns
are related to the estimated factors, and the p-values for testing the associations
between the estimated factors and the individual stock return series using the data
in the last 26 weeks. The stock factor estimated robustly has positive influence over
all nine companies, and overall, it correlates with the series much better as judged
from the reported correlation coefficients and p-values. The out-of-sample mean
squared errors for the least squares, reduced-rank regression, and robust reduced￾rank regression are 9.97, 8.85, and 6.72, respectively, and the 40% trimmed mean
square errors are 5.44, 4.52, and 3.58, respectively. As such, the robustification of
reduced-rank estimation has resulted in about 20% improvement in prediction.
Theoretically, the non-robustness of reduced-rank regression can be conceptual￾ized using the notion of breakdown point from the robust statistics literature. Denote
the reduced-rank regression estimator of the coefficient matrix as C(r  ; X, Y),
C(r  ; X, Y) ∈ arg minC

Y − CX
2
F s.t. r(C) ≤ r, (12.1)332 12 Generalized Reduced-Rank Regression
Fig. 12.1 Stock returns example: scaled weekly log-returns of stocks in 2004. The dashed line
in each panel separates the series to two parts, i.e., the first 26 weeks for training and the last 26
weeks for testing. The log-returns of Ford at weeks 5 and 17 and the log-return of General Motors
at week 5 are captured as outliers by fitting robust reduced-rank regression, which are indicated by
the circles (She and Chen 2017)
Table 12.1 Stock returns example: factor coefficients showing how the stock returns load on the
estimated factors. Reported p-values are for testing the associations between the estimated factors
and the stock returns using the data in the last 26 weeks
Reduced-rank regression Robust reduced-rank regression
Coefficient p-value Coefficient p-value
Walmart 0.46 0.44 0.36 0.23
Exxon −0.15 0.32 0.14 0.84
General Motors 0.96 0.42 0.90 0.02
Ford 1.20 0.64 0.59 0.18
General Electric 0.24 0.67 0.32 0.06
Conoco Phillips −0.04 0.19 0.36 0.08
Citi Group 0.27 0.93 0.45 0.00
IBM 0.36 0.42 0.57 0.13
American International Group 0.19 0.01 0.58 0.00
to emphasize its dependence on the specified rank and the data. We define the finite￾sample breakdown point for reduced-rank regression, denoted as ∗(C)  , in the spirit
of Donoho and Huber (1983) as follows:
∗(C)  = 1
T
min !
k ∈ N ∪ {0} : sup Y∈Rm×T :
Y−Y
0≤k

C(r  ; X, Y)X
F = +∞"
,12.2 Robust Reduced-Rank Regression 333
where N is the set of natural numbers, and 
·
0 is the 0 norm of the enclosed matrix,
i.e., 
A
0 = 
vec (A)
0 = |{(i, j ) : aij = 0}| with |·| denoting the cardinality of the
enclosed set. To describe this quantity intuitively, ∗(C)  is the minimum proportion
of arbitrary outliers needed among the observations such that the model prediction
can be completely distorted (with infinite bias). For example, the sample mean has a
breakdown point of 1/T , because as long as one of the observations goes to infinity,
the sample mean goes to infinity as well. On the other hand, the sample median, as
a robust estimator for the population center, has a breakdown point of 1/2.
Result 12.1 Given any X ∈ Rn×T , Y ∈ Rm×T with X = 0 and r ≥ 1, let
C(r  ; X, Y) be a reduced-rank regression estimator which results from (12.1). Then
its finite-sample breakdown point is exactly 1/T .
This breakdown point analysis indicates that a single outlier is capable of
severely distorting reduced-rank estimation. The result still holds for a broad class
of the singular value penalized estimators (She and Chen 2017). In other words,
the popular means of reduced-rank regularization, although quite effective for
dimension reduction, is extremely sensitive to outliers. This conclusion suggests
that the conventional rank reduction methods are to be used with caution in real
data applications.
Although the above result confirms the non-robustness of reduced-rank estima￾tion, it does not provide any insight into the relationship between the magnitude of
the outliers and the amount of distortion in the parameter estimation or prediction.
Indeed, classical robust analysis falls in either deterministic worst-case studies or
large-T asymptotics with the model dimensions held fixed, which may not meet the
needs of modern large-scale reduced-rank modeling. Therefore, robust reduced-rank
estimation and a non-asymptotic robust analysis under a general high-dimensional
regression framework are very much needed.
12.2.2 Robustification with Sparse Mean Shift
As we have observed in the previous section, the robustification of reduced-rank
matrix estimation is non-trivial. A straightforward idea might be to use a robust loss
function  in place of the squared error loss in (12.1), leading to
min
C

T
k=1
(
(Yk − CXk)
2) s.t. r(C) ≤ r. (12.2)
However, such an estimator may result in non-trivial computational and theoretical
challenges. Even when  is the well-known Huber’s loss (Huber 1981), there may
not be a straightforward algorithm for solving (12.2), though non-convex robust loss
functions are known to be preferable in dealing with gross outliers with possible
high leverage values.334 12 Generalized Reduced-Rank Regression
To begin with, we consider a sparse mean-shift regression framework to robustify
reduced-rank regression. The framework was used for linear regression (She and
Owen 2011; Lee et al. 2012) and was later utilized for high-dimensional reduced￾rank estimation in She and Chen (2017). The multivariate mean-shift regression
model can be stated as follows,
Y = CX + S + , (12.3)
where C ∈ Rm×n is the coefficient matrix, S ∈ Rm×T describes the outlier effects
on Y, and  ∈ Rm×T is the random error matrix with i.i.d. columns following
N (0,Σ). Obviously, this leads to an over-parameterized model, so we must make
some structural assumptions on the unknown matrices (C, S) appropriately. Hence
we assume that C is of reduced rank and S is sparse with only a few nonzero entries
or nonzero columns of entries because outliers are expected to be rare, not consistent
with most of the data.
The robust reduced-rank regression problem is then formulated as
min
C,S 1
2

Y − CX − S
2
F + ρ(S; λ) 
s.t. r(C) ≤ r. (12.4)
Here, ρ(·; λ) is a sparsity-inducing penalty function with λ controlling the amount
of penalization. The following form of ρ can handle element-wise outliers
ρ(S; λ) = m
j=1

T
k=1
ρ(|sjk|; λ). (12.5)
This was used to illustrate the stock return analysis in Section 12.2.1. It is more
common in studies on robust statistics to assume outlying samples, or outlying
columns in (Y, X), which corresponds to imposing the penalty function
ρ(S; λ) = 
T
k=1
ρ(
sk
2; λ), (12.6)
where sk is the kth column vector of S. Unless otherwise specified, we mainly focus
on dealing with column-wise outliers. This covers distortions that occur at specific
samples or time points. The analyses to be presented here can handle element-wise
outliers as well after some simple modification.
Interestingly, there is a universal connection between the mean-shift robustifi￾cation and the conventional M-estimation with robust loss functions. As such, the
sparse mean-shift estimation approach indeed comes with a guarantee of robustness
and generalizes M-estimation to the multivariate reduced-rank setting.
To proceed, the following notations are needed. For A = [a1,...,aT ] ∈ Rm×T ,
let 
A
2,1 = T
k=1 
ak
2, which gives the sum of the column-wise 2 norms of A,12.2 Robust Reduced-Rank Regression 335
and let 
A
2,0 = T
k=1 I (
ak
2 = 0), which gives the number of nonzero columns
of A. Given an index set S ⊂ {1,...,T }, we denote 
k∈S 
ak
2 by 
AS
2,1.
It is well known that sparsity-inducing penalization is always associated with
certain thresholding operation. For example, the convex 1 penalty yields an
estimator that is obtained from a soft-thresholding rule. In fact, this coupling
framework is more general. Let us first formally define the threshold functions.
A thresholding function is a real-valued (odd, monotonic, and bounded) function
Θ(t; λ) defined for −∞ <t< ∞ and 0 ≤ λ < ∞ such that
(i) Θ(−t; λ) = −Θ(t; λ);
(ii) Θ(t; λ) ≤ Θ(t
; λ) for t ≤ t

;
(iii) lim
t→∞ Θ(t; λ) = ∞;
(iv) 0 ≤ Θ(t; λ) ≤ t for 0 ≤ t < ∞.
(12.7)
Given any Θ, Θ is defined for any matrix A = [a1,...,aT ] ∈ Rm×T in a column￾wise fashion as Θ(A  ; λ) = [Θ(a  1; λ), . . . , Θ(a  T ; λ)], where
Θ(a  ; λ) =

{Θ(
a
2; λ)/
a
2}a, a = 0;
0, a = 0. (12.8)
A penalty ρ can be constructed from an arbitrary thresholding function Θ(·; λ)
by
ρ(t; λ) − ρ(0; λ) = ρΘ(t; λ) + δ(t; λ), (12.9)
where ρΘ(t; λ) = 9 |t|
0 [sup{s : Θ(s; λ) ≤ u} − u] du, and δ(·; λ) is a non-negative
function satisfying δ{Θ(s; λ); λ} = 0, for all s ∈ R (She 2009). Many popularly
used convex and non-convex penalties are all covered by (12.9). For example,
the convex group-1 penalty λ
T
k=1 
ak
2 is associated with the soft-thresholding
ΘS(t; λ) = sgn(t)(|t| − λ)+. The group-0 penalty (λ2/2)
T
k=1 I (
ak
2 = 0) can
be obtained from (12.9) with the hard-thresholding ΘH (t; λ) = tI(|t| > λ), and
δ(t; λ) = (λ−|t|)2I (0 < |t| < λ)/2. This Θ-ρ coupling framework also covers the
bridge penalty p for 0 <p< 1, the smoothly clipped absolute deviation (SCAD)
penalty (Fan and Li 2001), the minimax concave penalty (Zhang 2010), and the
capped 1 (Zhang 2010) as particular cases.
Theorem 12.1
(a) Suppose Θ(·; λ) is an arbitrary thresholding rule satisfying (12.7), and let ρ
be any penalty associated with Θ through (12.9). Consider the group penalized
form of the robust reduced-rank regression criterion336 12 Generalized Reduced-Rank Regression
min
C,S #
1
2

Y − CX − S
2
F +
T
k=1
ρ(
sk
2; λ)$
s.t. r(C) ≤ r. (12.10)
For any fixed C, a globally optimal solution for S is S(C) = Θ(  Y−CX; λ). By
profiling out S with S(C), (12.10) can be expressed as an optimization problem
with respect to C only, and it is equivalent to the robust M-estimation problem
in (12.2), where the robust loss function is given by
(t; λ) =
2 |t|
0
ψ(u; λ) du, ψ(t; λ) = t − Θ(t; λ).
(b) Given % ∈ {0, 1,...,T }, consider the group-0 constrained form of the robust
reduced-rank regression criterion
min
C,S

Y − CX − S
2
F s.t. r(C) ≤ r, 
S
2,0 ≤ %. (12.11)
Similarly, after profiling out S, (12.11) can be expressed as an optimization
problem with respect to C only and is equivalent to the rank-constrained
trimmed least squares problem
min
C
T
−%
k=1
r(k) s.t. r(C) ≤ r, (12.12)
where r(1),...,r(T ) are the order statistics of the 2 norms of the residual
vectors rk = 
Yk − CXk
2, k = 1,...,T , satisfying r(1) ≤···≤ r(T ).
Theorem 12.1 connects sparse mean-shift estimation with a penalty ρ to robust
M-estimation with a loss function . The universal link between (12.2) and (12.10)
provides insight into the choice of the penalty form. It is easy to verify that the
1-norm penalty leads to Huber’s loss, which is known to be prone to masking and
swamping and may fail in the presence of even moderately leveraged outliers. To
handle gross outliers, redescending ψ function is often advocated. This amounts to
using non-convex penalties in (12.10). For example, Hampel’s three-part ψ (Hampel
et al. 2005) corresponds to the SCAD penalty (Fan and Li 2001), the skipped
mean ψ corresponds to the exact 0 penalty, and the rank-constrained least trimmed
squares can be rephrased as the 0-constrained form as in (12.11). The connection is
also valid in the case of element-wise outliers, with ρ and  applied in an element￾wise manner. Thus the framework provided here covers other widely discussed
methods in the literature.
An important feature of the mean-shift robustification is that it leaves the
original loss function intact. This approach not only provides a unified way to
robustify the estimation of low-rank matrix but also facilitates theoretical analysis
and computation of reduced-rank M-estimators in high dimensions.12.2 Robust Reduced-Rank Regression 337
Algorithm 1 Robust reduced-rank regression algorithm
Input X, Y, C(0)
, S(0)
; t = 0.
Repeat
(a) S(t+1) ← Θ(  Y − C(t)X; λ), according to (12.8);
(b) C(t+1) ← C(r  ; X, Y − S(t+1)
), according to (12.1);
t ← t + 1;
Until convergence, e.g., 
C(t+1) − C(t)
F /
C(t)
F + 
S(t+1) − S(t)
F /
S(t)
F < .
Now it is in order to consider the computational aspect of the problem. Compared
with the M-estimation approach, the mean-shift formulation (12.3) simplifies
computation and parameter tuning. Consider the penalized form of the robust
reduced-rank regression problem in (12.10), where ρ is constructed by (12.9). A
simple iterative algorithm is described in Algorithm 1, where the two matrices C
and S are alternatingly updated while the other held fixed until convergence.
Step (a) performs simple multivariate thresholding operations and Step (b)
conducts reduced-rank regression on the adjusted response matrix Y − S(t+1)
. It
is important to observe that there is no need to explicitly compute C(t) for updating
S in the iterative process. In fact, only C(t)X is needed, which depends on X through
PX, or I when n  T . The eigenvalue decomposition used in (12.1) has low
computational complexity because the rank values of practical interest are often
small. Algorithm 1 is simple to implement and is cost-effective. The model tuning
can be based on cross validation or some information criterion as discussed in
Chapter 10. More details on computation can be found in She and Chen (2017).
It is worth noting that the robust reduced-rank regression subsumes a special but
important case, Y = C + S + , which aims to decompose an observed matrix to
be the sum of a low-rank matrix C, a sparse matrix S, and a stochastic error matrix
. This problem is perhaps less challenging than its supervised counterpart, but has
wide applications in computer vision and machine learning (Wright et al. 2009;
Candès et al. 2011). Furthermore, the approach can be extended to reduced-rank
vector generalized linear models; see, e.g., Yee and Hastie (2003), She (2013), and
Luo et al. (2018). In these scenarios, directly robustifying the loss function becomes
even more complicated, while a sparse mean-shift term can always be introduced
without altering the form of the given loss function, so that many algorithms
designed for fitting generalized linear models can still be seamlessly utilized.
12.2.3 Theoretical Analysis
Theorem 12.1 provides some helpful intuition for the robust reduced-rank regres￾sion, but it might be inadequate from a theoretical perspective. For example, how
can one justify the need for robustification in estimating a matrix of low rank? Is
using redescending ψ functions still preferable in rank-deficient settings? We cannot338 12 Generalized Reduced-Rank Regression
assume, as in traditional robust analysis, an infinite sample size and a fixed number
of predictors or response variables, because n and/or m can be much larger than
T in modern applications. The key is then to conduct a non-asymptotic analysis to
“quantify” the robustness.
Assume that the mean-shift model is given by Y = CX + S + , where  has
i.i.d. N (0, σ2) entries, and consider the robust reduced-rank regression problem in
(12.4). We use r = r(C) to denote the rank of the true coefficient matrix, rx = r(X)
to denote the rank of X, and 
S
2,0 to denote the number of nonzero columns in S,
i.e., the number of outliers.
Denote Δ(C, S) = 
CX + S
2
F . Given an estimator (C,  S), we focus on its
prediction accuracy measured by
Δ(C
 − C,S − S) = 
(C
 − C)X + (S − S)
2
F . (12.13)
This predictive learning perspective is useful for evaluating the performance of an
estimator, and it requires no signal strength or parameter uniqueness assumptions.
The 2 prediction bound of Δ(C
 − C,S − S) is fundamental, in that such a
bound, together with additional compatibility conditions on the design, can lead
to estimation error bounds for C and S in different norms as well as selection
consistency properties (Ye and Zhang 2010; Lounici et al. 2011).
We present finite-sample oracle inequalities that apply to the problem of robust
reduced-rank regression in arbitrary dimensions. The following theorem concerns a
general penalty form ρ(S; λ) = T
k=1 ρ(
sk
2; λ). We assume that ρ(·; λ), with λ
as the threshold parameter, satisfies
ρ(0; λ) = 0, ρ(t; λ) ≥ ρH (t; λ), (12.14)
where ρH (t; λ) = (−t2/2+λ|t|)1|t|<λ+(λ2/2)1|t|≥λ. The latter inequality is natural
in view of (12.9), because a shrinkage estimator with λ as the threshold is always
bounded above by the hard-thresholding function ΘH (·, λ).
Theorem 12.2 Let λ = aσ (rx + log T )1/2 with a > 0 being a constant and let
(C,  S) be a global minimizer of (12.4). Then, for sufficiently large a, the following
inequality holds for any C
 ∈ Rm×n, S ∈ Rm×T satisfying r(C)  ≤ r:
E{Δ(C
 − C,S − S)}  Δ(C
 − C,S − S) + σ2(rx + m)r + ρ(S; λ) + σ2,
(12.15)
where  means the inequality holds up to a multiplicative constant.
The bound in (12.15) involves a bias term Δ(C
 − C,S − S), which ensures
applicability of robust reduced-rank regression to approximately sparse S. Similarly,
r may also deviate from the true rank r to some extent, as a benefit from the
bias-variance tradeoff. The second term σ2(rx + m)r is from the estimation of the
reduced-rank component, and the rest is due to estimation of the sparse component.
In particular, by setting r = r, C
 = C, and S = S in (12.15) and assuming ρ is12.2 Robust Reduced-Rank Regression 339
a bounded non-convex penalty satisfying ρ(t; λ)  λ2 for any t ∈ R, we obtain a
prediction error bound
E{Δ(C
 − C,S − S)}  σ2(rx + m)r + σ2(rx + log T )
S
2,0. (12.16)
Observe that the bias term vanishes. The first term coincides with the prediction
error rate for reduced-rank regression as obtained in Theorem 10.3, while the second
term quantifies the additional error from accommodating the outliers.
We remark that Theorem 12.2 does not place any requirement on the design
matrix X. Similar conclusions can be shown for the constrained form in (12.11)
and for any doubly penalized form with sparse penalization on S and singular value
penalization on C.
The potential benefit of applying a redescending ψ is revealed by Theorem 12.2.
As an example, for Huber’s ψ, which corresponds to the popular convex 1 penalty
due to Theorem 12.1, ρ(S; λ) on the right-hand side of (12.15) is unbounded,
while Hampel’s three-part ψ gives a finite rate as shown in (12.16). However,
convex methods are not hopeless. In some less challenging problems, where some
incoherence condition is satisfied by the augmented design matrix, Huber’s ψ can
achieve the same low error rate. Furthermore, it can be shown that in a minimax
sense, the error rate obtained in (12.16) is essentially optimal.
The non-asymptotic robustness analysis can also reveal the necessity of robust
estimation when outliers are present. Similar to Theorem 12.2, it can be shown that
the conventional reduced-rank regression, which sets S = 0, satisfies
E{Δ(C
 − C,S − S)}  inf
r(C)  ≤r

C
X − (CX + S)
2
F + σ2(rx + m)r + σ2.
(12.17)
Taking r = r and evaluating at the optimal C
 satisfying C
X = CX + SPCX and
r(C)  ≤ r, the error bound of reduced-rank regression is of order
σ2(rx + m)r + 
S(IT − PCX)
2
F . (12.18)
Because CX has low rank, IT − PCX is not null in general. Since notable outliers
that affect the projection subspace in performing rank reduction tend to occur in the
orthogonal complement of the range of CX, (12.18) can be arbitrarily large. This
result thus echoes the same non-robustness conclusion of reduced-rank regression
reached in Theorem 12.1 through the analysis of breakdown point.
One may argue that in the presence of outliers, applying the conventional
reduced-rank regression with a larger rank value may better control the size of the
bias term. To examine this, we can set C
 = C+SX
(XX
)− in (12.17), which yields
σ2rx 
S
2,0 + σ2m
S
2,0 + σ2(rx + m)r + 
S(I − PX)
2
F . (12.19)340 12 Generalized Reduced-Rank Regression
We used r(C)  ≤ r + 
S
2,0 to derive the about result. When n>T , we have that
PX = I , and so (12.19) offers an improvement over (12.18) by giving a finite error
rate of
σ2(rx + m)r + σ2(m + rx )
S
2,0.
However, the robust reduced-rank regression guarantees a consistently lower rate at
σ2(rx +m)r +σ2(m+log T )
S
2,0, because σ2rx 
S
2,0  σ2(log T )
S
2,0. The
performance gain can be large in big data applications, where the predictor matrix
can be of high dimensions and typically, multiple outliers are bound to be present.
We remark that although the above results are for the global minimizers of (12.4),
they remain valid under mild conditions for local solutions that can be achieved by
Algorithm 1 presented in Section 12.2.2. More recently, Tan et al. (2022) considered
robust sparse reduced-rank regression with heavy-tailed random noise via a convex
relaxation approach. Their approach minimizes the Huber’s loss function subject to
both the entrywise 1 norm and the nuclear-norm penalties on the coefficient matrix.
The resulting estimator is shown to be computationally tractable and demonstrates a
tradeoff between heavy-tailedness of the random noise and the estimation error rate
under a matrix-version restricted eigenvalue condition.
12.3 Reduced-Rank Estimation with Incomplete Data
Multivariate data are often prone to missing values, and in many problems there
is a genuine need to predict the missing values or conduct statistical analysis with
an incomplete data matrix. The associated problem, called the matrix completion,
has received a great deal of attention. The main question is, can we recover the
missing entries of a matrix with only a few observed values that may or may not
be contaminated with noise? From the first glance, this problem is ill-posed, in
that, recovering an arbitrary data matrix from a subset of its entries is in general
not possible. However, if the underlying matrix is assumed to have some special
structures such as reduced rank, it turns out that an accurate or even an exact
recovery is possible under certain conditions.
There have been many modern applications with matrix completion. As men￾tioned earlier, in collaborative filtering, matrix completion is used to build a Netflix
movie recommendation system from highly incomplete movie rating matrix; in
studies of aging on elderly subjects (Stanziano et al. 2010), matrix completion can
be utilized in reduced-rank regression to study longitudinal associations of past and
current health statues with partially observed health outcomes. Other applications in
the literature are found in system identification, global positioning, remote sensing,
computer vision, multi-task learning, among others.
To formulate the problem, let M ∈ RT1×T2 be the matrix we would like to know
as precisely as possible. However, we only observe a partial set of its entries mik,12.3 Reduced-Rank Estimation with Incomplete Data 341
(i, k) ∈ Ω, where
Ω = {(i, k); The (i, k)th entry of M is observed, i = 1,...,T1, k = 1,...,T2.}.
It is possible that these entries are contaminated by noise, deterministic or random.
To make the recovery somewhat possible, the matrix M (or its underlying signal)
is assumed to have a reduced-rank structure. The problem of matrix completion is
concerned with the computational methods and the theoretical underpinnings, such
as required conditions on the structure of M and the number of observations, for
recovering M accurately or exactly (in the noiseless case) with high probability.
In this section, we present the main methodologies and theoretical results on how
well one can recover low-rank matrices from a few entries. We first discuss the basic
noiseless case and then broaden the discussion to the more common situation that
the observed entries are corrupted with noise. We then connect matrix completion
to reduced-rank regression with incomplete data; the latter can be regarded as a
supervised extension of the former.
12.3.1 Noiseless Matrix Completion
In the noiseless case, the observed entries are precisely those of the unknown matrix
M ∈ RT1×T2 . Because M is assumed to be of low rank, i.e., r(M) = r 
min(T1, T2), we may consider the following optimization problem for its recovery,
min
C∈RT1×T2
r(C), s.t. PΩ (C) = PΩ (M). (12.20)
That is, we want to find a low-rank matrix that has the same values as M on the
observed entries. Unfortunately, this rank minimization problem is NP-hard, and
the known algorithms for exactly solving it requires computational complexity that
is at least exponential in the dimensions of the matrix (Recht et al. 2011).
An alternative is to consider nuclear-norm minimization,
min
C∈RT1×T2

C
", s.t. PΩ (C) = PΩ (M). (12.21)
This is the tightest convex relaxation of the above rank minimization problem in
(12.20), in the sense that the nuclear ball, {C ∈ RT1×T2 : 
C
" ≤ 1}, is the convex
hull of the set of rank one matrices with spectral norm bounded by one, that is,
{C ∈ RT1×T2 : r(C) = 1, d1(C) ≤ 1}. By (12.21), we aim to find the matrix
with minimal nuclear norm that has the same values as M on the observed entries.
This can be characterized as a semidefinite programming (SDP) problem, which
can be efficiently solved, for example, by interior point methods (Vandenberghe
and Boyd 1996). It is interesting to note that the situation here is analogous to the
combinatorial 0-minimization problem in sparse estimation briefly discussed in342 12 Generalized Reduced-Rank Regression
Section 10.2, for which 1-minimization is its tightest convex relaxation in the sense
that the 1 ball is the convex hull of the set of unit-norm vectors with at most one
nonzero entry.
Matrix completion is not always possible, even when rank of M is very small and
the sampling rate (the proportion of observed entries) is very high. First of all, the
sampling scheme, or the pattern of the observed entries, plays a role. For example,
the completion of M would be impossible if a row or a column is completely
unsampled. To avoid such (trivial) situations (with high probability), it is common
to impose the uniform sampling assumption that the observed entries are selected at
random without replacement. Another fundamental and subtle condition, concerns
the potential non-identifiability between the low-rank and the sparse structures. To
illustrate, consider the rank one matrix M = e1v
, where v ∈ RT2 and e1 is the first
vector in the canonical basis of RT1 :
M =
⎡
⎢
⎢
⎢
⎣
v1 v2 v3 ··· vT2−1 vT2
000 ··· 0 0
.
.
. .
.
. .
.
. .
.
. .
.
. .
.
.
000 ··· 0 0
⎤
⎥
⎥
⎥
⎦ .
Clearly, this low-rank matrix cannot be recovered if any entry of the first row is
missing. This type of instances, however, can occur with large probability under
uniform sampling.
Intuitively, there is no guarantee to complete a matrix, even it is of very
low rank, when some of its singular vectors are extremely sparse. From another
perspective, if a row (or column) is approximately orthogonal to the other rows (or
columns), to recover the matrix we would need to see all the entries in that row.
These observations motivate the concept of “coherence.” In particular, a geometric
incoherence assumption can be imposed to quantify the alignment between the
canonical basis of RT1 (respectively, RT2 ) and the column (respectively, row) space
of M (Candès and Recht 2009; Recht 2011). To define this, let U be a subspace of
RT of dimension r and PU be the orthogonal projection onto U. Then the coherence
of U is defined as
η(U) = T
r max
1≤i≤T

PU ei
2
2, (12.22)
where, with some abuse of notation, ei is the ith vector in the canonical basis of
RT . For any subspace, the coherence index η(U) can be as small as one, and it
is achieved, for example, when U is spanned by vectors whose entries all have
magnitude 1/
√T . On the other hand, η(U) can be as large as T/r, that is achieved
when the subspace contains a canonical basis vector. Intuitively, if a matrix has row
and column spaces with low coherence, each entry can be expected to provide about
the same amount of information, making its associated matrix completion problem
under uniform sampling relatively easy to achieve.12.3 Reduced-Rank Estimation with Incomplete Data 343
The assumptions and main results on matrix completion in the noiseless setting
are given below.
Assumption 12.1 We observe h entries of M with locations sampled uniformly at
random.
Assumption 12.2 The row and column spaces have coherences, as defined in
(12.22), bounded above by some positive η0.
Assumption 12.3 The matrix UV  has its maximum entry bounded by
η1
√r/(T1T2) in absolute value for some positive η1.
Theorem 12.3 Let M be an T1 × T2 matrix of rank r, with its singular value
decomposition given by M = UDV 
, where U ∈ RT1×r, D ∈ Rr×r, and V ∈
RT2×r. Without loss of generality, assume that T1 ≤ T2. Suppose Assumptions 12.1–
12.3 hold. Then, with probability at least 1−6 log(T2)(T1 +T2)2−2c −T 2−2c1/2
2 , for
some c > 1, the minimizer to problem (12.21) is unique and is exactly equal to M if
h  max{η2
1, η0}(log(T2))2r(T1 + T2), (12.23)
where the notation  means the inequality holds up to a multiplicative constant.
In Theorem 12.3, Assumption 12.1 specifies the uniform sampling scheme.
Assumptions 12.2 and 12.3 are regarding the geometric incoherence (Candès and
Recht 2009). Both η0 and η1 may depend on r, T1, or T2. Recall that the coherence
parameter η0 is bounded by 1 ≤ η0 ≤ T2/r. So, the bound confirms that it is
favorable to have low coherence. Moreover, we have that η2
1 ≤ η2
0r by the Cauchy￾Schwarz inequality. As shown in Candès and Recht (2009), both subspaces selected
from the uniform distribution and spaces constructed as the span of singular vectors
with bounded entries are not only incoherent with the standard basis but also obey
Assumption 12.2 with high probability for values of η1 at most of the order log(T1).
The bound in Theorem 12.3 is nearly optimal. This is a consequence of the
coupon collector’s problem, which, in this context, implies that at least T2 log T2
uniformly sampled entries are necessary to guarantee that at least one entry in every
row and column is observed with high probability. Moreover, a rank-r matrix has
degrees of freedom (T1 + T2 − r)r, which, intuitively, suggests that the number of
observations needed is at least O(T2r). Indeed, Candès and Tao (2010) showed that
the number of observations necessary for completion is of the order η0 log(T2)T2r,
when the entries are sampled uniformly at random. Therefore, the bound in (12.23)
is essentially optimal up to a logarithm factor of log(T2).
We remark that there have been many important findings on matrix completion,
and similar related results have been obtained by various authors. The results
presented here were mainly from the work by Recht (2011), which features weak
assumptions and simplicity in its analysis. In particular, Theorem 12.3 makes344 12 Generalized Reduced-Rank Regression
no assumption on the rank r, the aspect ratio T2/T1, and the condition number
d1(M)/dr(M). It thus has a scope to cover more practical applications.
12.3.2 Stable Matrix Completion
In real-world problems, we almost always observe the entries of the underlying
matrix of interest that are perturbed with some noise. In the Netflix problem, user’s
ratings are uncertain; in the longitudinal study of aging, the best we can hope for
is that the current health statue can provide reasonable prediction into the future. It
then arises the question that whether accurate matrix completion is still possible
from noisy samples, and if so, under what conditions. This problem has been
referred to as stable matrix completion in the literature.
Consider the following model to analyze the stable matrix completion problem,
yik = mik + ik, (i, k) ∈ Ω, (12.24)
where ik, (i, k) ∈ Ω are the error terms. In matrix form, the model can be expressed
as
PΩ (Y) = PΩ (M) + PΩ (),
where  = (ik) is an T1 × T2 error matrix.
To recover the unknown matrix M, we consider the following optimization
problem:
minC 
C
", s.t. 
PΩ (Y − C)
F ≤ δ. (12.25)
That is, we aim to find the matrix with minimal nuclear norm that is close to Y on the
observed entries as measured by the error in Frobenius norm. Similar to (12.21), this
can also be recognized as an SDP problem. We shall defer the detailed discussion
on the computation to Section 12.3.3. Let M be the solution of (12.25). The main
result of stable matrix completion is stated below.
Theorem 12.4 Suppose the conditions for ensuring noiseless recovery, as Assump￾tions 12.1–12.3 for establishing Theorem 12.3, hold. Assume 
PΩ ()
F ≤ δ for
some δ > 0. Let p = h/(T1T2) denote the fraction of observed entries. Then we
have

M − M
F  δ
1
(p + 2) min(T1, T2)
p
+ δ. (12.26)
Theorem 12.4 shows that when perfect noiseless recovery occurs, matrix comple￾tion is stable, implying that, accurate recovery can be achieved even with perturbed12.3 Reduced-Rank Estimation with Incomplete Data 345
entries. Besides the conditions under which noiseless recovery occurs, all we assume
is that the noise level is bounded as 
PΩ ()
F ≤ δ. When  is random, (12.26) holds
on the event 
PΩ ()
F ≤ δ, so a proper choice of δ based on the distribution of 
will lead to a high-probability error bound. For example, if the random error terms
{ik} are uncorrelated and each with mean zero and standard deviation σ (white
noise sequence), then δ2 ≤ σ2(h + √8h) with high probability. As expected, the
error of the matrix completion is proportional to δ; the smaller the noise level, the
smaller the error. Also, when the fraction of observed entries p is small, the error is
at most of the order δ
√min(T1, T2)/p.
12.3.3 Computation
The matrix completion problem in (12.25) can be equivalently expressed as a
regularized nuclear-norm minimization problem (Mazumder et al. 2010; Ma et al.
2011),
min
C

J (C) ≡ 1
2

PΩ (Y − C)
2
F + λ
C
"
 
. (12.27)
Let the solution be denoted as C(λ)  . Then it follows from standard duality result
(Boyd and Vandenberghe 2004) that for any δ < 
PΩ (Y)
F , we could use (12.27)
to solve (12.25) by searching for the value of λ(δ) that gives 
PΩ (Y−C(λ))  
F = δ.
We present a simple iterative algorithm to solve (12.27) (Mazumder et al. 2010),
which can be recognized to follow a Majorization-Minimization (MM) procedure
(Hunter and Lange 2000). The MM algorithm is an iterative procedure that works by
minimizing a sequence of properly chosen upper bounds or the so-called majorizers
of the objective function. As such, it is often able to solve a difficult optimization
problem by solving a sequence of much easier problems instead.
For the matrix completion problem, the MM algorithm starts with an initial
value C(0)
; a simple choice is the T1 × T2 zero matrix. At the tth iteration, with
the currently estimate C(t), the first step is to construct a majorization function
of the objective function J (C) at C(t), denoted as g(C;C(t)), that satisfies two
conditions: g(C(t);C(t)) = J (C(t)) and g(C;C(t)) ≥ J (C) for any C = C(t),
i.e., the majorization function takes the same value as the objective function at
the current estimate and otherwise is always greater than or equal to the objective
function. Here, a natural choice of the majorization function is
g(C;C(t)) = 1
2

Y(t) − C
2
F + λ
C
", (12.28)
where Y(t) = PΩ (Y) + PΩ⊥ (C(t)). That is, we simply fill the missing entries in Y
with the corresponding entries from the current estimate C(t). It is straightforward
to verify that the two conditions of being a majorization function are satisfied by the346 12 Generalized Reduced-Rank Regression
above g(C;C(t)):
g(C(t);C(t)) = 1
2

PΩ (Y) + PΩ⊥ (C(t)) − C(t)
2
F + λ
C(t)
" = J (C(t))
because C(t) = PΩ (C(t)) + PΩ⊥ (C(t)), and
g(C;C(t)) = 1
2

PΩ (Y) + PΩ⊥ (C(t)) − PΩ⊥ (C) − PΩ (C)
2
F + λ
C
"
= 1
2

PΩ (Y) − PΩ (C)
2
F +
1
2

PΩ⊥ (C(t)) − PΩ⊥ (C)
2
F + λ
C
"
≥ J (C),
because 
PΩ⊥ (C(t)) − PΩ⊥ (C)
2
F ≥ 0 for any C = C(t).
The next step is to update the estimate of C by minimizing the majorized
objective function in (12.28). This can be immediately seen as a nuclear-norm
penalized matrix approximation problem, with its solution given by the singular
value soft-thresholding operation according to Result 10.1. Specifically, Let the
singular value decomposition of Y(t) be given by V (t)D(t)U(t)
. Then
C(t+1) = arg minC g(C;C(t)) = Sλ(Y(t)) = V Sλ(D)U
,
where Sλ(D) = diag -
{dk(Y(t)) − λ}+, k = 1,..., min(T1, T2)
.
.
The above two steps, the majorization of the objective function at the current
estimate and the minimization of the resulting majorized objective function are
repeated, until reaching convergence. The procedure is summarized in Algorithm 2.
Due to its Majorization-Minimization structure, the above algorithm enjoys the
monotone descending property. It can be readily shown that the objective function
in (12.27) is monotonically decreasing along the iterations,
J (C(t+1)
) ≤ g(C(t+1)
;C(t)) ≤ g(C(t);C(t)) = J (C(t)).
The first inequality is due to the property of g that g(C;C(t)) ≥ J (C) for any
C = C(t), and the second inequality is due to the fact that C(t+1) is the minimizer
Algorithm 2 Matrix completion via iterative singular value thresholding
Input PΩ (Y), C(0)
, t = 0.
Repeat
(a) Y(t) ← PΩ (Y) + PΩ⊥ (C(t));
(b) C(t+1) ← Sλ(Y(t));
t ← t + 1;
Until convergence, e.g., 
C(t+1) − C(t)
F /
C(t)
F < .12.4 Generalized Reduced-Rank Regression with Mixed Outcomes 347
of g(C;C(t)). Given the convexity of the problem, this implies that the convergence
to a global optimum is guaranteed.
Algorithm 2 works well for problems of moderate size. However, for large
matrices, the computation could become burdensome mainly due to step (b),
because of the computational complexity involved in computing the singular value
decomposition of the matrix Y(t). Even the storage of the full matrix could become
an issue in some applications. For example, the movie rating matrix in the Netflix
problem is about of 400,000 by 20,000 and requires multiple gigabyte of storage
space.
There have been various strategies and algorithms to make matrix completion
more scalable. We briefly mention a few key ideas. First, because the percentage
of observed entries is usually quite small in large applications (1% in the Netflix
problem), sparse-matrix representations and the associated computational toolkit
can be utilized. Second, various strategies are considered to avoid the expensive full
singular value decomposition. In particular, if the solution is anticipated to be of low
rank, the rank constraint can be utilized to speed up the computation. For example,
in Algorithm 2, step (b) could be executed via a much more efficient reduced￾rank singular value decomposition. A different approach, maximum-margin matrix
factorization (MMMF), is originated from the result that for any rank r matrix
C ∈ RT1×T2 , it holds that

C
" = min
A∈RT1×r,B∈Rr×T2
!

A
2
F + 
B
2
F
"
.
Then resulting MMMF criterion solves
min
A,B 1
2

PΩ (Y − AB)
2
F + λ(
A
2
F + 
B
2
F )
 
. (12.29)
The problem is bi-convex in A and B and can be solved by alternating convex
search. We refer to Mazumder et al. (2010), Hastie et al. (2015), Davenport and
Romberg (2016), and Chi and Li (2019) for additional details and more recent
development.
12.4 Generalized Reduced-Rank Regression with Mixed
Outcomes
Thus far we have been focusing on various reduced-rank models developed for
continuous responses. In many applications, however, several types of outcomes,
e.g., continuous, binary, and count data, may all be collected. We refer to such a
collection of outcomes as mixed outcomes. Furthermore, such mixed data could
have missing values and outliers, as discussed in the previous sections. In general,
regardless of the measurement types, these outcomes are often likely to be related,348 12 Generalized Reduced-Rank Regression
representing diverse yet interrelated views of the underlying data generation
mechanism. Therefore, an integrative learning and joint predictive modeling of the
mixed outcomes could be preferable.
Here we present a mixed-outcome reduced-rank regression (mRRR) framework
to model mixed, incomplete, and potentially contaminated response data with high￾dimensional predictors. This provides a general, practical approach for analyzing
multivariate mixed outcomes from the exponential dispersion family, taking into
account missing values, outliers, effects of control variables, differential dispersion
of the outcomes, among others.
Let Y ∈ Rm×T be the complete response matrix consisting of T independent
samples from m outcome/response variables. Similar to the scenario of matrix
completion, we allow Y to be partially observed. Let
Ω = {(i, k); The (i, k)th entry of Y is observed, i = 1, . . . , m, k = 1,...,T.}
be an index set collecting all the entries corresponding to the observed outcomes.
Let Y = PΩ (Y) denote the projection of Y onto Ω, i.e., y˜ik = yik for any (i, k) ∈ Ω
and y˜ik = 0 otherwise.
We assume each yik, i.e., the kth observation on the ith outcome variable, for any
(i, k) ∈ Ω, follows a distribution in the exponential dispersion family (Jorgensen
1987). Specifically, the probability density function (pdf) of each yik takes the
following form,
f (yik; θik, φk) = exp yikθik − bi(θik)
ai(φi) + ci(yik; φi)
 
, (12.30)
where θik is the natural parameter of yik, φi is the dispersion parameter of the
ith outcome variable, and ai(·), bi(·), ci(·) are known functions determined by the
specific distribution of the ith outcome variable.
Here the m outcome variables are allowed to have different distributions within
the exponential dispersion family; Table 12.2 provides specifications of some most
common distributions in this family, including Normal, Bernoulli, and Poisson.
The dispersion parameter φi can either be known or unknown. For example, the
φi for Poisson distribution is known to be 1, but for Gaussian distribution φi
corresponds to its variance which is generally unknown. Let φ = (φ1,...,φm)
be the vector of the dispersion parameters, and we denote φu as a subvector of φ
consisting of all the unknown dispersion parameters. Without loss of generality, for
each outcome variable, we apply the canonical link function gi = (b
i)−1, so that
E(yik) = b
i(θik) = g−1
i (θik), where, with some abuse of notation, b
i(·) denotes the
derivative function of bi(·).
Let X ∈ Rn×T be the observed feature/predictor matrix. Also let Z ∈ R(nz+1)×T
be consisting of a vector of ones in its first row (corresponding to the intercept term)
and observations from nz control variables in the rest of its rows. The set of control
variables is application specific, and in general its dimension nz is much smaller
than the sample size T .12.4 Generalized Reduced-Rank Regression with Mixed Outcomes 349
Table 12.2 Some common distributions in the exponential dispersion family
Distribution Mean Variance θ φ a(φ) b(θ ) c(y; φ)
Bernoulli(p) p p(1 − p) log(p(1 − p)−1) 1 1 log(1 + eθ ) 0
Poisson(λ) λ λ log λ 1 1 eθ − log y!
Normal(μ, σ2) μ σ2 μ σ2 φ θ 2/2 −(y2φ−1 +
log 2πφ)/2
Gamma(α, β) α/β α/β2 −β/α 1/α φ − log(−θ ) log(ααyα−1/Γ (α))
The basis of the mRRR approach is the familiar generalized linear model
(GLM) (Yee and Hastie 2003; She 2013; Luo et al. 2018). Specifically, the natural
parameters in (12.30) are modeled as
θik = oik + B
(i)Z(i) + C
(i)X(i) + sik, (i, k) ∈ Ω. (12.31)
Here oik is a known offset term, which commonly arises, for example, in the
modeling of count data for adjusting the size of the population from which a count
is drawn. The coefficient vectors B(i) ∈ Rnz+1, i = 1,...,m, correspond to the
intercept and the control variables, while C(i) ∈ Rn, i = 1,...,m, correspond to
the predictors. The mean-shift term sik is to capture the potential outlying effect in
yik that cannot be explained by the control variables and the predictors.
Let Θ = [Θ(1),...,Θ(m)]
 = (θik) ∈ Rm×T be the natural parameter matrix,
C = [C(1),...,C(m)]
 ∈ Rm×n the coefficient matrix of the predictors, and B =
[B(1),...,B(m)]
 ∈ Rm×(pz+1) the coefficient matrix of the intercept and control
variables, whose first column, B0, gives the intercept vector. Let S = (sik) ∈ Rm×T
be the mean-shift parameter matrix. Then the model in (12.31) can be stated in
matrix form as
Θ = BZ + CX + S. (12.32)
With no additional assumptions on the parameter matrices, observe that the model is
over-parameterized when m or n are comparable or much larger than T . Moreover,
when the independence of yiks is assumed, the model reduces to a set of univariate
GLM analysis and thus does not possess any multivariate flavor. The key here, is
how to induce and take advantage of the interrelationships among the outcomes.
As such, the merit of the formulations in (12.30)–(12.32) lies in imposing some
suitable low-dimensional structures on the parameters. In particular, we assume C is
a reduced-rank matrix and S is a sparse matrix, analogous to the robust reduced-rank
regression discussed in Section 12.2. The reduced-rank assumption on C implies
that the multivariate outcomes are jointly associated with the predictors through a
few latent variables, while the sparsity assumption on S accommodates the scattered
outlying effects in the observed outcomes.
The mRRR is a rather “simple” model, in that the method does not explicitly
characterize the dependency among the outcomes and does not explicitly model the
missing data mechanism. These alternative proposals, though seemingly desirable,350 12 Generalized Reduced-Rank Regression
would not be easily applicable or generalizable in the simultaneous presence of out￾come heterogeneity, high dimensionality, data contamination, and incompleteness.
Nevertheless, we demonstrate that the reduced-rank approach could still be effective
in dealing with such complex data. It is also worth noting that when there are no
control variables and predictors, the cRRR model specializes to a robust principal
component analysis (PCA) with mixed and incomplete data. When the responses are
all from the same type of distribution, the method further simplifies to a generalized
robust PCA or to a generalized robust matrix completion method (Collins et al.
2002; Candès et al. 2011).
The regularized estimation problem for mRRR can be formulated as
min
C,B,S,φu
⎧
⎨
⎩− 
(i,k)∈Ω
i(C(i), B(i), sik, φi; Xk, Zk, yik) + ρ(S; λ)
⎫
⎬
⎭
s.t. r(C) ≤ r,
(12.33)
where
i(C(i), B(i), sik, φi; Xk, Zk, yik) = yikθik − bi(θik)
ai(φi) + ci(yik; φi)
with θik = oik +B
(i)Zk +C
(i)Xk +sik, and ρ is a sparsity-inducing penalty function
applied on S in either entry-wise or column-wise fashion. Comparing to the robust
reduced-rank regression criterion in (12.4), the above criterion replaces the sum of
squared error term with the negative log-likelihood function of the observed entries,
while the treatment of the incomplete data follows the same spirit as the matrix
completion problem in (12.27). Here the rank constraint can be replaced with a
singular value penalty, as discussed in Section 10.3. We remark that for modeling
the mixed outcomes, the inclusion of the dispersion parameters is necessary, which
ensures that all the outcomes can be brought to the same scale for joint analysis.
The optimization of (12.33) can be solved by combining the ideas of the
blockwise descent and the iterative thresholding used in Algorithm 1 (for robust
reduced-rank regression) and the Majorization-Minimization procedure used in
Algorithm 2 (for matrix completion). We shall briefly describe the main steps. At
the tth iteration, with fixed S, B and φu, a surrogate quadratic loss function with
respect to C is formulated and the resulting optimization problem becomes
min
C

C − C(t) − (Φ(t))
−1PΩ {Y − b
(Θ(t))}X

2
F s.t. r(C) ≤ r,
where Θ(t) = B(t)Z+C(t)X+S(t), Φ = diag{φ(t)
i , i = 1, . . . , m.}, and b
(Θ(t)) =
[b
1(Θ(t)
(1)
), . . . , b
m(Θ(t)
(m))]

. This is solved by the singular value hard-thresholding
operation, according to Result 10.1. When the rank constraint is replaced with
a singular value penalty, the solution is given by a corresponding singular value12.5 Applications 351
thresholding operation. With fixed C, B and φu, a similar surrogate loss function
with respect to S can be formulated and the problem becomes
min
S
1
2

S − S(t) − (Φ(t))
−1PΩ {Y − b
(Θ(t))}
2
F + ρ(S; λ),
which admits an explicit solution via a thresholding operation corresponding to the
choice of ρ; see Theorem 12.1 and the relevant discussions that follow. Finally, with
fixed C and S, the problem becomes a separable set of univariate GLM problems;
while the same majorization strategy can be applied to update B, many standard
gradient-based routines and implementations are also available. The algorithm
executes the above steps iteratively until convergence. With proper pre-scaling of
the predictor matrices, one can show that the objective function is guaranteed to
be non-increasing during the iterations (She 2013; Luo et al. 2018). Alternative
optimization methods are also available; see, e.g., Udell et al. (2016), in which a
number of local optimization methods were considered for a broad spectrum of
generalized low-rank models.
We remark that there exist many approaches that attempt to model the asso￾ciations among mixed outcomes in an explicit way, although primarily for low￾dimensional problems. To mention a few, Cox and Wermuth (1992) and Fitzmaurice
and Laird (1995) considered likelihood based methods by factorizing the joint
distribution as marginal and conditional distributions. Prentice and Zhao (1991) and
Zhao et al. (1992) used the generalized estimating equations (Liang and Zeger 1986)
to handle mixture of continuous and discrete outcomes. de Leon and Wu (2011)
considered a copula-based model for a bivariate mixed discrete and continuous
outcome. Indeed, a direct modeling of mixed outcomes is challenging due to a lack
of convenient multivariate distributions, not even to mention that a joint estimation
of both the mean and the covariance structure may become notoriously difficult
in high-dimensional settings. Another strategy, closely related to the reduced￾rank modeling, is to induce multivariate dependency through some shared latent
variables, conditioning on which the outcomes are then assumed to be independent
(Sammel et al. 1997; Dunson 2000; McCulloch 2008).
12.5 Applications
12.5.1 Arabidopsis Thaliana data
Recall the study on isoprenoid in Arabidopsis thaliana plants presented in Sec￾tion 11.5.2. Isoprenoids, constituting a large class of organic compounds, are
abundant and diverse in plants, and they serve many important biochemical
functions and have roles in respiration, photosynthesis and regulation of growth and
development in plants. A genetic association study was conducted to examine the
regulatory control mechanisms in the gene network for isoprenoid in Arabidopsis352 12 Generalized Reduced-Rank Regression
thaliana, a small flowering plant native to Eurasia and Africa. Specifically, T = 118
GeneChip microarray experiments were performed to monitor the gene expression
levels under various experimental conditions (Wille et al. 2004). To confirm the
existing experimental findings that there exist strong connections between two
isoprenoid biosynthesis pathways and some downstream pathways, we consider a
multivariate regression setup, with the expression levels of n = 39 genes from two
isoprenoid biosynthesis pathways as the predictors, and the expression levels of
m = 62 genes from four downstream pathways, namely plastoquinone, carotenoid,
phytosterol, and chlorophyll, as the responses.
Because of the small sample size relative to the number of unknown parameters,
the reduced-rank models can be handy. We apply both the classical reduced-rank
regression and the robust reduced-rank regression with group Lasso penalty to
deal with potential gross outliers. The fitted robust model has rank five, which
reduces the effective number of parameters by about 80% compared with the
least squares method. Interestingly, two samples, labeled 3 and 52, are identified
as outliers. The two outliers have a surprisingly big impact on both coefficient
estimation and model prediction: we find that 
C
 − C

F /
C

F ≈ 50% and

C
X − C
X
F /
C
X
F ≈ 26%, where C
 and C
 denote the robust and classical
reduced-rank regression estimates, respectively.
Figure 12.2 shows the detection paths by plotting the 2 norm of each column
in the estimates of the mean-shift matrix S for a sequence of values of the
tuning parameter λ. Sample 3 and sample 52 are captured as outliers, whose paths
are shown as a dotted line and a dashed line, respectively. These two detected
unusual samples are distinctive. Their outlying effects might be related to their
corresponding experimental conditions. In particular, sample 3 was the only sample
with Arabidopsis tissue culture in a baseline experiment. In addition, Fig. 12.2
reveals that sample 27 could be a potential outlier that merits further investigation.
The robust reduced-rank regression produces score variables or factors, con￾structed from the isoprenoid biosynthesis pathways in response to the 62 genes on
the four downstream pathways. Let X denote the design matrix after removing the
two detected outliers, and V
DU
 be the singular value decomposition of C
X. Then
Fig. 12.2 Arabidopsis thaliana data: outlier detection paths by the robust reduced-rank regression
(She and Chen 2017)12.5 Applications 353 −10 −5 0 5 10 Factor coefficient
1
2
3
4
5
6
7
89
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
3536
37
38
39
40
41
42
43
44
45
46
474849
50
515253
54
55
56
5758
59
60
61
62
Plastoquinone Carotenoid
Phytosterol
Chlorophyll
−10 −5 0 5 10
1
2
3
4
5
67
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
2728
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
4445
46
47
48
49
50
5152
53
5455
56
57
58
59
60
61
62
Plastoquinone
Carotenoid
Phytosterol
Chlorophyll
−10 −5 0 5 10
1
2
3
4
5
6
7
8
9
1011
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
4748
49
50
51
52
53
5455
56
57
58
59
60
61
62
Plastoquinone Carotenoid
Phytosterol
Chlorophyll
Fig. 12.3 Arabidopsis thaliana data: factor coefficients of the 62 response genes from plasto￾quinone, carotenoid, phytosterol, and chlorophyll pathways (She and Chen 2017)
Table 12.3 Arabidopsis thaliana data: percentage of genes on each response pathway that show
significance of a given factor, with the family-wise error rate controlled at level 0.01
Pathway Number of genes Factor 1 Factor 2 Factor 3
Carotenoid 11 55% 73% 9%
Phytosterol 25 20% 48% 32%
Chlorophyl 24 75% 21% 0%
U
 provides five orthogonal factors, and V
D gives the associated factor coefficients.
Figure 12.3 plots the coefficients of the first three leading factors for all the 62
response variables, which account for 86% of the variation explained by the five
factors. The panels from left to right correspond to the top three factors estimated by
the robust reduced-rank regression. For the sth factor (s = 1, 2, 3), two horizontal
lines are plotted at heights ±m−1/2ds(C
X), and three vertical lines separate the
genes into the four different pathways. As such, the genes located outside of the
two horizontal lines have relatively large coefficients/loadings in magnitude on the
corresponding factor. It is also informative to test the significance of the factors in
response to each of the 62 genes. The results are shown in Table 12.3. Plastoquinone
is excluded since it has only two genes and its behavior couples with that of
carotenoid most of the time. Even with the family-wise error rate controlled at
0.01, the factors obtained are overall predictive according to the percentages of
significance.
According to Fig. 12.3 and Table 12.3, the factors play very different roles in
different pathways. The genes that are correlated with the first factor are mainly
from carotenoid and chlorophyll, and almost all the coefficients there are negative.
It seems that the first factor interprets some joint characteristics of caroteniod and
chlorophyll. The second factor differentiates phytosterol genes from carotenoid
ones, and the third factor seems to mainly contribute to the phytosterol pathway.354 12 Generalized Reduced-Rank Regression
Therefore, by projecting the data onto a proper low-dimensional subspace in
a supervised and robust manner, distinct behaviors of the downstream pathways
and their potential subgroup structures can be revealed. More biological insights
could be gained by closely examining the experimental and background conditions.
Thus utilizing the tools developed in this chapter can lead to insights into complex
biological problems.
12.5.2 Longitudinal Studies of Aging
We use the LSOA data introduced in Section 10.6.2 to demonstrate the potential
power of integrative mixed-outcome reduced-rank modeling. Recall that a national
representative sample of several thousands of subjects who were at or over 70
years of age were interviewed and followed, and their health, functional status,
living arrangements, and health services utilization were measured as they moved
into and through their oldest ages. The main interest is to examine the changes
and the associations between their past and current health status. We have thus
considered a multivariate regression setup, by jointly regressing various health
measures collected during the period of 1999–2000 on the records collected during
the period of 1997–1998 from the same set of subjects. In Section 10.6.2, n = 294
past health risk and behavior measurements are treated as predictors and 41 later
binary health outcomes are treated as multivariate responses, and the analysis
includes T = 3988 subjects who participated the studies in both periods.
As a matter of fact, besides the 41 binary outcomes, the data set also includes
3 additional continuous outcome variables on self-rated health measures, namely
overall health status, memory status, and depression status. There may be strong
correlations among the large number of outcomes and the features. Therefore, it is
plausible that the outcomes are dependent on the features only through a few latent
factors, making dimension reduction and reduced-rank models applicable. We thus
apply mRRR with rank penalization to conduct an integrative analysis of the m = 44
continuous and binary outcomes. Since mRRR can deal with the missing values in
the outcomes, neither data removal nor imputation is needed. The gender and age
variables are used as controls and their corresponding coefficients are not penalized.
To demonstrate the efficacy of integrative learning, we mainly compare mRRR
with the univariate generalized linear models (uGLM). We use a random-splitting
procedure to evaluate the predictive performance. In each split, 75% of data are
randomly selected for training and the rest 25% data for testing. The prediction of
the continuous outcomes in each split is evaluated by mean squared errors (MSE),
while the prediction of the binary outcomes is evaluated by Area Under Curve
(AUC), based on testing data alone. The procedure is repeated 100 times.
We compute the average predictive measures from random splitting. The average
MSE for Gaussian outcomes are 0.69 (0.06) and 0.76 (0.07), and the average
AUC for binary outcomes are 0.77 (0.10) and 0.65 (0.11), for mRRR and uGLM,
respectively (the standard deviations are reported in the parenthesis). The uGLM12.5 Applications 355
Fig. 12.4 LSOA data: comparison of predictive performance of mRRR and uGLM. Use 75%
sample as training set (Luo et al. 2018)
approach is outperformed by mRRR by a large margin, indicating the strength of
low-rank estimation. Figure 12.4 provides a more detailed performance comparison
on the prediction of each individual outcome. The left panel displays the MSE
or AUC value for predicting each individual outcome. The right panel shows the
percentage of improvement by mRRR over uGLM. In the left penal, the left axis
shows MSE while the right axis shows AUC. In both panels, the solid vertical
line separates the Gaussian and the binary outcomes, while the dashed vertical
lines separate the binary outcomes to different categories. It can be seen that the
improvement by mRRR over uGLM is persistent across all the outcomes. The most
improvement appears to be in the categories of fundamental activity and extended
activity, where the percentage of improvement is over 20% for several outcomes.
Indeed, these outcomes tend to be moderately or highly correlated, making joint
estimation particularly beneficial. The mRRR also performs substantially better in
predicting the three continuous responses related to self-rated health. This can be
explained by the fact that two out of the three self-rated health measures are about
memory and depression, which are very relevant to the variables in the category of
cognition (Hammar and Ardal 2009).
We have also tried the regularized version of uGLM using elastic net (uGLM￾EN), whose average MSE for Gaussian outcomes and average AUC for binary
outcomes are 0.70 (0.06) and 0.75 (0.11), respectively. As such, the performance
of mRRR is only slightly better than that of uGLM-EN. This shows that shrinkage
and sparse estimation could also be quite effective in this application.
Next, we have tried the approach of fitting the 3 Gaussian responses and
the 41 binary responses separately. Using the aforementioned random-splitting
procedure with 75% samples for training, this approach yields almost identical
results compared to mRRR. That the benefit of joint modeling is not observed in
this case is partly due to the facts that the number of Gaussian responses is quite
small and the sample size is very large. We have then tried smaller sample sizes356 12 Generalized Reduced-Rank Regression
for training. From the random-splitting procedure with 25% data for training, the
average MSE for Gaussian outcomes are 0.78 (0.06) and 0.80 (0.06), and the average
AUC for binary outcomes are 0.75 (0.10) and 0.73 (0.10), for mRRR and gRRR,
respectively; with 10% data for training, the average MSE for Gaussian outcomes
are 0.86 (0.09) and 0.92 (0.09), and the average AUC for binary outcomes are 0.73
(0.11) and 0.70 (0.10), for mRRR and gRRR, respectively. Therefore, as the sample
size becomes smaller, while the performance of both methods deteriorates, the gain
of mRRR over gRRR becomes even more revealing.
The above analysis suggests that integrative modeling can be quite effective
especially when sample size is small or information from each individual response
is limited. It also suggests that a model that considers both reduced-rank and sparsity
structures may likely have superior performance; we will further discuss these types
of models in Chapter 13.Chapter 13
Sparse Reduced-Rank Regression
13.1 Introduction
Under the high-dimensional multivariate regression framework in Chapter 10,
researchers have considered several types of low-dimensional structural assump￾tions on the coefficient matrix C = [C1,...,Cn] ∈ Rm×n in (10.6). In particular, in
this book we have been focusing on the reduced-rank structure, where the rank of C
can be much smaller than its dimensions m and n. Another popular low-dimensional
structure is sparsity, where the entries of C or its components through certain matrix
decomposition (e.g., left and right singular matrices) are assumed to contain a large
number of zeros. It has been realized that these two types of low-dimensional
structures could complement each other. While the reduced-rank structure exploits
the dependence among variables to achieve parsimony, the sparseness helps in
variable selection. Indeed, the integration of the reduced-rank and the sparsity
structures has greatly advanced the applicability of high-dimensional multivariate
regression and facilitated scientific discoveries in a wide range of applications.
There are several possible patterns of sparsity in C that imply different response￾predictor associations and model interpretations. For example, if C is assumed to be
column-sparse, that is, if a number of columns in C are null vectors, it implies that
only a subset of the predictors are related to the response variables while the rest
that correspond to the zero columns in C are irrelevant. Together with the reduced￾rank structure on C, the resulting model assumes that the responses are associated
with a few latent factors and they are constructed as linear combinations of only
a subset of the predictors. Many novel applications are in the area of studying
regulatory relationships among genome-wide measurements (Vounou et al. 2010;
Ma et al. 2014; Uematsu et al. 2019). For instance, in a yeast expression quantitative
trait loci (eQTLs) mapping analysis, the goal is to understand how the eQTLs,
which are regions of the genome containing DNA sequence variants, influence the
expression level of genes in the yeast MAPK signaling pathways. The problem can
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8_13
357358 13 Sparse Reduced-Rank Regression
be formulated as a multivariate regression with the eQTLs as predictors and the
gene expression levels as responses. Extensive genetic and biochemical analysis
has revealed that there are a few functionally distinct signaling pathways of genes,
suggesting that the coefficient matrix is of reduced rank. Further, column sparsity in
the coefficient matrix arises from the postulate that only a relatively small number
of eQTLs regulates the gene expressions. On the other hand, row-sparsity of the
coefficient matrix could also arise, if only a subset of gene expressions gets regulated
by the eQTLs.
The sparsity on certain decomposition of C can lead to even more subtle and
more appealing model interpretation. Consider the situation that the right singular
matrix of C is assumed to be sparse; it implies that each latent factor is allowed to
be constructed (as a linear combination) from a different subset of predictors. If the
left singular matrix of C is assumed to be sparse, it implies that each response is
allowed to be associated with a different subset of the latent factors. Such flexibility
can be critical in many applications. In the aforementioned yeast eQTLs mapping
example, it is highly plausible that each signaling pathway involves only a subset of
genes, which are regulated by only a few genetic variants. This suggests that each
latent association between the eQTLs and the genes, as identified by each of the
unit-rank components, is sparse in both the responses and the predictors, and the
pattern of such sparsity is pathway specific. Moreover, it is known that the yeast
MAPK pathways regulate and interact with each other. Therefore, a joint sparse
reduced-rank regression analysis holds a great promise in decoding the complex
genetic structures through simultaneously searching for multiple distinct association
pathways between subsets of genes and subsets of genetic variants.
In this chapter, we first consider the situation where both, reduced-rank structure
and column sparsity in the coefficient matrix are present. This notably leads to
the sparse reduced-rank regression for dimension reduction and predictor selection
(Bunea et al. 2012; Chen and Huang 2012). When the row-sparsity is also present,
the model is extended to the two-way sparse reduced-rank regression that allows for
both predictor and response selection. We then consider the more challenging situ￾ation where sparsity is imposed on the (generalized) singular value decomposition
of the reduced-rank coefficient matrix. This leads to co-sparse factor regression to
achieve factor-specific, two-way variable selection (Chen et al. 2012, 2022; Mishra
et al. 2017).
13.2 Sparse Reduced-Rank Regression
13.2.1 Sparse Reduced-Rank Regression for Predictor
Selection
Consider the multivariate linear regression model as stated in (1.1) or (10.6). The
main objective is to estimate the unknown m × n dimensional coefficient matrix C
when the dimensions could be much larger than the sample size T . Let r = r(C)13.2 Sparse Reduced-Rank Regression 359
denote the rank of C, rx = r(X) denote the rank of X, and S∗ denote the index
set of the nonzero columns in C with s∗ = |S∗| being its cardinality. The sparse
reduced-rank regression assumes that the coefficient matrix is possibly of reduced￾rank with r ≤ min(rx , m), and it may have some zero columns, that is, s∗ ≤ n.
As such, this parsimonious model aims to achieve simultaneous rank reduction and
predictor selection. It is seen that in the unsupervised setting when X = I , the setup
is closely related to sparse principal component analysis (Zou et al. 2006; Witten
et al. 2009; Johnstone and Lu 2009).
It is helpful to first gain an intuitive understanding of the potential parsimony
offered by the simultaneous rank deficiency and column sparsity, through counting
the effective number of free parameters. The reduced-rank structure, with r ≤
min(m, rx ), can reduce the number of free parameters from mn to (rx + m − r)r.
The column sparsity, with s∗ ≤ n, can reduce the number of free parameters
to ms∗. While either structure offers certain control of the model complexity,
their combination, which gives a column-sparse reduced-rank structure, can further
reduce the effective number of free parameters to (s∗ + m − r)r. Because X can
always be reduced to an rx × T matrix with rx independent rows that span the same
row space of X, we can always reasonably assume that s∗ ≤ rx . Therefore, the
number (s∗ + m − r)r could be much smaller than either (rx + m − r)r or ms∗.
This further reduction is especially appealing in high-dimensional settings where
the control of complexity offered by either the reduced-rank or the column-sparse
structures alone may not be adequate.
Now consider the problem of estimating the coefficient matrix in the sparse
reduced-rank regression model. We focus on a two-step procedure consisting of
rank determination and rank-constrained sparse estimation. As shown in Bunea et al.
(2012), this procedure provides strong theoretical and computational guarantees
and is superior to several alternatives. These include the reverse approach of
first performing predictor selection by sparse regression followed by reduced-rank
regression with the selected set of predictors.
In the first step, we estimate the rank of the coefficient matrix by fitting a version
of the high-dimensional reduced-rank regression, for example, the rank selection
criterion (Bunea et al. 2011) and the adaptive nuclear-norm penalization method
(Chen et al. 2013) covered in Chapter 10. Either method ensures the consistent
estimation of the model rank under essentially the same signal to noise ratio
condition, as presented in Theorem 10.2. In what follows, we denote the estimated
rank asr.
In the second step, with the estimated rank r, we pursue column sparsity by
minimizing a group Lasso regularized regression criterion under the rank constraint
(Bunea et al. 2012),
C
 = arg minC
1
2

Y − CX
2
F + λ
C
2,1
 
s.t. r(C) ≤ r, (13.1)
where λ is a tuning parameter, 
C
2,1 is the column-wise group 2 norm of C, that
is, 
C
2,1 = n
k=1 
Ck
2, and the minimization is over all C ∈ Rm×n with rank360 13 Sparse Reduced-Rank Regression
less than or equal to r. Clearly, it must be noted that this approach subsumes both
reduced-rank regression and sparse regression as special cases. When λ = 0, we
get back to the classical reduced-rank regression setup; on the other hand, when
r = min(rx , m), the rank constraint becomes ineffective and we get back to the
column-sparse group Lasso regression setup. Therefore, the two different dimension
reduction strategies are jointly synthesized via the rank-constrained group Lasso
regression in (13.1).
To gain more insights, we may consider an equivalent form of (13.1) by writing
the coefficient matrix C in its reduced-rank decomposed form C = AB, where A ∈
Rm×r, B ∈ Rr×n, and A
A = Ir (Chen and Huang 2012; Bunea et al. 2012). Here
the orthogonality of A is imposed without any loss of generality, with a convenient
consequence that 
C
2,1 = 
B
2,1. The problem in (13.1) can then be equivalently
expressed as
(A,  B)  = arg min A,B 
F (A, B; λ) ≡ 1
2

Y − ABX
2
F + λ
B
2,1
 
, s.t. A
A = Ir,
(13.2)
where the minimization is taken over all orthogonal m ×r matrices A and allr × n
matrices B.
The solution to the problem in (13.2) is not unique. In fact, it is unique up to a
rotation by anyr ×r orthogonal matrix. More precisely, if (A,  B)  is a solution, then
(AQ 
, QB)  is also a solution for any Q ∈ Rr×r and Q
Q = Ir. One can verify
that the two sets of solutions provide exactly the same objective function value,
F (A,  B
; λ) = F (AQ 
, QB
; λ). Despite such non-uniqueness, the column-sparse
reduced-rank estimation still warrants meaningful predictor selection, owing to the
invariance property of the column sparsity structure, namely the column sparsity in
C = AB implies the same column sparsity in B and in any rotated QB. Therefore,
different solutions of (13.2) lead to the selection of exactly the same subset of
predictors.
Further, one may wonder the possibility of pursing the entrywise sparsity in B,
by, for example, replacing the group Lasso penalty λ
B
2,1 with the Lasso penalty
λ
B
1. This seems straightforward and could possibly enable factor-specific pre￾dictor selection. Unfortunately, the problem is more complicated. In the current
formulation, because B is determined only up to an orthogonal rotation, pursuing
any entrywise sparsity in B may not carry a clear meaning due to the lack of rotation
invariance. That is, an entrywise sparsity pattern in B can be easily destroyed or
distorted through rotation and thus does not transfer across arbitrary QB to allow
consistent variable selection. We will show in Section 13.3 that the pursuit of
entrywise sparsity in the decomposed components of C becomes meaningful when
additional identifiability condition is imposed; that is, a general sparsity pattern can
be sought on a pre-specified and “fully” identified decomposition of C.13.2 Sparse Reduced-Rank Regression 361
13.2.2 Computation
Solving directly the non-convex constrained optimization problem in (13.1) may
be difficult. It turns out that the alternative formulation in (13.2) facilitates the
optimization. Here we sketch a blockwise coordinate descent algorithm, in which A
and B are alternately updated until reaching convergence.
For fixed B, the problem becomes an orthogonal Procrustes problem (Gower and
Dijksterhuis 2004) with respect to the orthogonal matrix A, that is,
min
A 
Y − ABX
2
F , s.t. A
A = Ir. (13.3)
This problem admits an explicit solution: A = UV 
, where U ∈ Rm×r and V ∈
Rr×r are, respectively, the left and right singular matrix of YX
B
.
Now consider the optimization with respect to B for fixed A. Denote A⊥ ∈
Rm×(m−r) as the orthogonal complement matrix of A such that A⊥A⊥ = Im−r and
A
A⊥ = 0; that is, the combined m×m matrix [A, A⊥] satisfies [A, A⊥]

[A, A⊥] =
Im. Then it follows that 
Y − ABX
2
F = 
[A, A⊥]

(Y − ABX)
2
F = 
A
Y −
BX
2
F + 
A⊥Y
2
F . Therefore, for fixed A, the problem with respect to B becomes,
min
B
1
2

A
Y − BX
2
F + λ
B
2,1. (13.4)
This problem can be recognized as a multivariate group Lasso regression or a special
multi-task learning method as discussed in Section 10.5.1. It can be written as a
standard group Lasso problem because 
A
Y − BX
2
F = 
vec(A
Y) − (X ⊗
Ir)vec(B))
, where vec denotes the column-wise vectorization operator and ⊗
denotes the Kronecker product. This 1 problem can be solved efficiently by various
methods (Yuan and Lin 2006; Yang and Zou 2015).
The computational algorithm is summarized in Algorithm 3. It proceeds by
alternately updating A and B by solving (13.3) and (13.4), respectively. The objec￾tive function is monotonically descending along the iterations and the sequence
of iterates (A(t), B(t)) are uniformly bounded; in fact, given any λ > 0 and
Algorithm 3 Rank-constrained group Lasso regression
Input: 1 ≤ r ≤ min(m, rx ), λ ≥ 0, A(0) ∈ Rm×r with A(0)
A(0) = Ir, t = 0.
Repeat:
(a) B(t+1) ← minB{
A(t)
Y − BX
2
F /2 + λ
B
2,1};
(b)Perform singular value decomposition of YX
B(t+1)
, i.e.,
YX
B(t+1) = U(t+1)
D(t+1)
V (t+1)
;
(c) A(t+1) ← U(t+1)
V (t+1)
;
t ← t + 1;
Until: convergence, e.g., |F (A(t+1)
, B(t+1)
; λ) − F (A(t), B(t); λ|/|F (A(t), B(t); λ)| < .362 13 Sparse Reduced-Rank Regression
any initial orthogonal matrix A(0)
, Algorithm 3 is guaranteed to converge to a
stationary point of F (Bunea et al. 2012). This is typically the best we can hope
for by using blockwise coordinate descent to solve a non-convex problem (Tseng
2001). In practice, the rank and the initial value of A are obtained from reduced￾rank regression. The convergence is reached when the relative decrease of the
objective function in (13.2) or the relative change in the estimated coefficient matrix
C
 = A
B
 (as measured by, e.g., Frobenius norm) becomes smaller than a pre￾specified tolerance level. To obtain the final estimate, one can run Algorithm 3
to generate a series of candidate estimators for a grid of λ values with the pre￾determinedr. The optimal solution is then selected by cross validation or by a proper
information criterion (She 2017; She and Tran 2019).
13.2.3 Theoretical Analysis
We now examine the performance of the two-step sparse reduced-rank estimator.
The following assumptions are imposed (Bunea et al. 2012).
Assumption 13.1 The error matrix  has independent N (0, σ2) entries.
Assumption 13.2 dr(CX) > 2
√2σ (√rx + √m).
Assumption 13.3 log(
CX
F ) ≤ (
√2 − 1)2(rx + m)/4.
Let Σx = XX
/T . We say Σx ∈ Rn×n satisfies a restricted eigenvalue (RE)
condition RE(S, δS) for an index set S ⊆ {1,...,n} and a positive number δS, if
and only if tr
MΣxM
 ≥ δS

i∈S 
Mi
2
2 for all M = [M1,...,Mn] ∈ Rm×n
satisfying 
i∈S 
Mi
2 ≥ 2

i∈Sc 
Mi
2.
Assumption 13.4 Σx ∈ Rn×n satisfies the restricted eigenvalue condition
RE(S∗, δS∗ ), where S∗ = ∅ is the index set of the nonzero columns in the true
coefficient matrix C and d2
1 (Σx )/δS∗ is bounded with d2
1 (Σx ) being the largest
eigenvalue of Σx .
Assumptions 13.1–13.2 are essentially the same as Assumptions 10.1–10.2 in
Section 10.4.3 for establishing the rank consistency of reduced-rank regression.
Assumption 13.1 implies that the noise level, measured by d1(PX), is bounded by
2
√2σ (√rx + √m). Assumption 13.2 then requires that the signal level, measured
by dr(CX), is larger than the noise level, so that the correct rank can be recovered
through reduced-rank regression method with high probability. Assumption 13.3 is
needed to ensure that the error due to rank selection is negligible compared to the
rate mr+s∗r log(n). Assumption 13.4 is essential for high-dimensional setups when
n>T ; it is commonly adopted in the high-dimensional variable selection problems
(Bickel et al. 2009; Bühlmann and van de Geer 2011) and is a specialized case of
the general RE condition discussed in Section 10.5.2.13.3 Co-sparse Factor Regression 363
Theorem 13.1 Suppose Assumptions 13.1–13.4 hold. The two-step estimator C
,
with rank r selected by reduced-rank regression and the tuning parameter set as
λ = cσ:
d2
1 (Σx )rT log(en) for a large enough constant c, satisfies
E



C
X − CX
2
F

 (m + s∗ log(n))r.
The results imply that the two-step estimator C
 is column-sparse and rank
adaptive, in that neither the best number of predictors nor the correct rank has to be
specified prior to estimation. Further analysis reveals that this procedure achieves
the minimax optimal rate up to a log(n) factor.
It is interesting to compare the above established prediction error rate, (m +
s∗ log(n))r, to those of the reduced-rank regression estimator and the group Lasso
estimator, which are (m + rx )r and (m + log(n))s∗, respectively. Note that we
generally have r ≤ min(m, s∗) and s∗ ≤ rx . When the number of responses is much
larger than that of the effective dimension of the predictors, m ≥ rx , the rates of both
the reduced-rank regression and the sparse reduced-rank regression are dominated
by mr regardless of s∗. As such, the effect of pursuing column sparsity in C may
not be essential as comparing with rank reduction. On the other hand, when m<rx
and the rank r is small, the sparse reduced-rank regression can provide substantial
improvements over the other two methods.
It is worth mentioning some related follow-up work. She (2017) considered
selective reduced-rank regression with non-convex group-wise penalty forms that
are able to achieve a sharper error bound; a predictive information criterion was
also proposed for tuning the rank and the column support of the coefficient matrix.
Bayesian sparse reduced-rank regression has been developed in parallel; see, e.g.,
Zhu et al. (2014), Goh et al. (2017), and Chakraborty et al. (2019).
13.3 Co-sparse Factor Regression
13.3.1 Model Formulation and Deflation Procedures
As discussed in the previous section, the pursuit of entrywise sparsity in the
decomposed components of C may enable factor-specific variable selection, but
it is only meaningful on a pre-specified and “fully” identified decomposition. Also
recall that reduced-rank regression can be regarded as a latent factor model and thus
naturally connects to factor analysis and principal component analysis: all these
methods aim to identify certain low-dimensional subspace to represent Y, and their
main difference is that reduced-rank regression conducts a supervised search in the
row space of X while the other two methods are unsupervised.
Motivated by the connection to factor models, the following factorization of the
reduced-rank component CX is appealing in the sparsity pursuit,364 13 Sparse Reduced-Rank Regression
1
√T
CX = AD  1
√T
BX

, s.t. 1
√T
BX
  1
√T
BX

= A
A = Ir, (13.5)
where C = ADB with A = [α1,...,αr] ∈ Rm×r and B = [β1,...,βr]
 ∈
Rr×n, and D = diag{d1,...,dr} being a diagonal matrix of positive singular
values. Observe that this decomposition is not the conventional singular value
decomposition of C, because we require B(XX
/T )B = Ir rather than BB = Ir.
Let Σx = XX
/T , which is the sample-version covariance matrix of the predictors.
Then we refer to the decomposition in (13.5) as the Σx -orthogonal singular value
decomposition of C or simply singular value decomposition of C when no confusion
arises. For convenience, we denote
C[k] = dkαkβ
k, k = 1, . . . , r, (13.6)
where αk is the kth column of A, β
k is the kth row of B, and dk is the kth
diagonal element of D. We call C[k] as the kth layer Σx -orthogonal singular
value decomposition of C. As a matter of fact, (13.5) is almost the same as
the normalization adopted in the classical reduced-rank regression, c.f., (2.14) in
Section 2.3, except that here the singular values are separated out.
An important attribute of the Σx -orthogonal singular value decomposition is
that the r latent predictors/factors, i.e., (1/
√T )X
βk, k = 1,...,r, are required
to be orthogonal/uncorrelated to each other. This makes the reduced-rank regression
analysis in analogous to the unsupervised factor analysis, which also aims to identify
a set of uncorrelated factors to avoid redundancy and facilitate interpretation. Also,
compared to the decomposition adopted in the sparse reduced-rank regression
in (13.2), the additional constraint on B now makes both A and B identifiable.
Now it is in order to consider imposing sparsity on both A and B, giving arise
to the delicate “co-sparse reduced-rank” structure. There are several prominent
features in this formulation:
(1) It produces r latent predictors/factors, (1/
√T )X
βk, k = 1,...,r, each
of which is constructed as a linear combination of a subset of the original
predictors due to sparsity of each βk (factor-specific predictor selection);
(2) Each latent factor is allowed to be related to a subset of the response variables
due to sparsity of each αk (factor-specific response selection);
(3) The latent factors are uncorrelated with each other, due to the orthogonality of
(1/
√T )X
B
;
(4) The singular values dk reflect the relative strengths of the association between
the responses and the latent factors.
We stress that we desire the elements of the matrix factorization, the left and right
singular vectors, to be entrywisely sparse. This could be much stronger than just
requiring the coefficient matrix C itself to be sparse. A matrix with sparse singular
vectors could be sparse itself, but not vice versa in general. The appealing features of
such sparse factorization methods have been well demonstrated in various scientific
applications (Vounou et al. 2010; Chen et al. 2014; Ma et al. 2014).13.3 Co-sparse Factor Regression 365
Now consider the recovery of the Σx -orthogonal singular value decomposition
of C under the co-sparse factor model, specified by (10.6) and (13.5). The simulta￾neous presence of low-rank and co-sparsity structure makes the estimation problem
very challenging. A joint estimation of all sparse singular vectors may necessarily
involve identifiability constraints related to orthogonality (Uematsu et al. 2019),
leading to non-convex and non-smooth optimization that is often computationally
intensive or can be even intractable.
Motivated by the power method for computing singular value decomposition
(Golub and Van Loan 1996), we focus on deflation-based approaches to tackle
this problem; the main idea is to estimate the unit-rank components of CX one
by one, thus reducing the problem into a set of much simpler unit-rank problems.
Specifically, the hope is to divide the multi-rank factorization problem into a set of
unit-rank problems of the following form:
min
C∈Rm×n {L(C; Y, X) + ρ(C; λ)}, s.t. r(C) ≤ 1, (13.7)
where L(C; Y, X) = (1/2)
Y − CX
2
F is the sum of squares loss function, and
ρ(C; λ) = λ
C
1 is the 1 penalty term with tuning parameter λ ≥ 0. This
criterion targets the best sparse unit-rank approximation of Y in the row space of
X. Intriguingly, by writing the unit-rank matrix C as C = dαβ
, where d > 0,
α ∈ Rm, and β ∈ Rn, the problem can be equivalently expressed as a co-sparse
unit-rank regression (CURE),
min
d,α,β
3
L(dαβ
; Y, X) + λ
dαβ

1
4
,
s.t. d ≥ 0, T −1/2
X
β
2 = 1, 
α
2 = 1.
(13.8)
The penalty term is multiplicative in that 
C
1 = 
dαβ

1 = d
α
1
β
1, which
conveniently promotes the desired co-sparse structure. This is not really surprising,
because the sparsity of a unit-rank matrix directly leads to the sparsity in both its
left and right singular vectors. To be specific, if the (i, j )th entry of C = dαβ is
zero, it must be true that either the ith entry of α is zero or the j th entry of β is zero,
or both. Therefore, the equivalency between the above two formulations reveals that
CURE is capable of achieving a “fair” co-sparse regularization on both α and β with
a single penalty term.
We present two general deflation approaches to reach the desired simplification
in (13.7), or equivalently, (13.8).
The first approach is called “sequential pursuit” (Mishra et al. 2017; Zheng
et al. 2019). As its name suggests, this method extracts unit-rank factorization of
the coefficient matrix C in a sequential fashion, each time with the previously
estimated component removed from the current residual matrix. Specifically, the
method sequentially solves366 13 Sparse Reduced-Rank Regression
C
[k](λ) = arg minC
⎧
⎨
⎩
L(C; Y −
k−1
j=0
C
[j ]X, X) + ρ(C; λ)
⎫
⎬
⎭ , s.t. r(C) ≤ 1,
(13.9)
for k = 1,...r, where C
[0] = 0, and C
[k] = d
kαkβ

k is the selected unit-rank
solution (through tuning by cross-validation or some information criterion) in the
kth step.
The second approach is called parallel pursuit (Chen et al. 2012; Chen and Chan
2016; Chen et al. 2022). It requires an initial estimator of the coefficient matrix
C and its corresponding decomposition according to (13.5); some typical choices
in practice include the reduced-rank regression estimator and the 1-penalized
least squares estimator. The estimation of each rank one component C[k] can then
be isolated by removing from Y the initially estimated signals from the other
components. To be specific, let C
 = r
k=1 C
[k] = r
k=1 d
kαkβ

k be the initial
estimator of the true coefficient matrix C and its Σx -orthogonal singular value
decomposition. Then the parallel pursuit solves the problems
C
[k](λ) = arg minC
⎧
⎨
⎩
L(C; Y −r
j=k
C
[j ]X, X) + ρ(C; λ)
⎫
⎬
⎭ , s.t. r(C) ≤ 1,
(13.10)
for k = 1,...,r. Since these r problems are completely separated, they can be
implemented in parallel.
Both deflation approaches are intuitive and are commonly used in matrix
decomposition related problems (Witten et al. 2009; Lee et al. 2010; Chen et al.
2012; Allen et al. 2014). There are both advantages and disadvantages for using
these approaches. The parallel pursuit approach requires an initial estimator to
isolate the estimation of the unit-rank components, and refines the estimation of
each component around the vicinity of its initial estimator. As such, the performance
of the parallel pursuit relies on the quality of the initial estimator. The sequential
pursuit approach, on the other hand, keeps extracting unit-rank components from
the current residuals, subject to additional sparsity regularization. This is motivated
by the fact that the solution of the reduced-rank regression, which solves minC 
Y−
CX
2
F s.t. r(C) ≤ r, can be exactly recovered by sequentially fitting r unit-rank
regression problems with the residuals from the previous step, due to the celebrated
Eckart-Young Theorem (Eckart and Young 1936). While the sequential method does
not require an initial estimator, it may suffer from a noise accumulation effect,
because the estimation of each later component is impacted by the estimation of
all previous components.
We remark that both deflation methods give up the exact orthogonality require￾ment among different components, which make them much more computationally
scalable than any joint estimation approach. From a statistical estimation point of13.3 Co-sparse Factor Regression 367
view, it can be argued that the exact orthogonality is never necessary: although
what we aim to recover is a set of uncorrelated latent factors, the estimators of
these factors, which are based on finite data, do not necessarily have to be exactly
uncorrelated. As long as the estimators are consistent, we can achieve asymptotic
orthogonality.
13.3.2 Co-sparse Unit-Rank Estimation
It remains to solve the CURE problem in (13.7) or (13.8). The alternating convex
search (ACS) algorithm, or blockwise coordinate descent, is natural to be applied
here, in which the objective function is alternately optimized with respect to a
(overlapping) block of parameters, (d, α) or (d, β). Realizing that the objective
function is a function of (d, α) or (d, β) only through the products dα or dβ,
respectively, the parameter constraints can all be avoided in the blockwise routines.
First, consider the update of (d, β) with fixed α satisfying 
α
2 = 1. The relevant
constraints are d ≥ 0, T −1/2
X
β
2 = 1. Because the objective function is a
function of (d, β) through βˇ = dβ, the constraints are avoided when optimizing
with respect to βˇ. It essentially boils down to solving the following standard Lasso
problem,
min
βˇ
1
2

y − X(α)βˇ
2
2 + λ(α)
βˇ
1
 
, (13.11)
where y = vec(Y
), X(α) = α ⊗ X
, and λ(α) = λ
α
1. Once the solution of βˇ is
obtained, we standardize d = 
X
βˇ
2/
√T and β = β/d ˇ to satisfy the constraints
whenever βˇ = 0, otherwise we update d = 0 and terminate the algorithm. Similarly,
for fixed β satisfying T −1/2
X
β
2 = 1, the minimization of (13.8) with respect to
(d, α) becomes minimization with respect to αˇ = dα,
min
αˇ
1
2

y − X(β)αˇ 
2
2 + λ(β)
 ˇα
1
 
, (13.12)
where X(β) = Im ⊗ (X
β), and λ(β) = λ
β
1. Once αˇ is obtained, we standardize
d = 
ˇα
2 and α = ˇα/d whenever αˇ = 0; again, when αˇ = 0, we set d = 0
and terminate the algorithm. Mishra et al. (2017) showed that this ACS algorithm
is empirically stable and is guaranteed to converge to a coordinatewise minimum
point of (13.8).
To further speed up computation, stagewise learning techniques can be applied
to approximate the ACS solution paths; we refer to Zhao and Yu (2007), He et al.
(2018), and Chen et al. (2022) for more details.368 13 Sparse Reduced-Rank Regression
13.3.3 Theoretical Analysis
We now analyze the statistical properties of the deflation-based estimators of the
sparse SVD components, C
[k], k = 1,...,r, with tuning parameters λk, k =
1,...,r, respectively, from either the sequential pursuit in (13.9) or from the parallel
pursuit in (13.10).
For any m by n nonzero matrix Δ, denote by ΔS the corresponding matrix of
the same dimension that keeps the entries of Δ with indices in set S while sets the
others to be zero. We need the following assumptions.
Assumption 13.5 The random noise vectors are independently and identically
distributed as k ∼ N (0, Σ ), k = 1,...,T . Denote the j th diagonal entry of
Σ as σ2
j ; we assume σ2
max = max1≤j≤m σ2
j is bounded from above.
Assumption 13.6 For 1 ≤ j ≤ r, the gaps between the successive singular values
are positive, i.e., δj = dj − dj+1 > 0, where dj > 0 is the j th singular value of
T −1/2CX.
Assumption 13.7 There exists certain sparsity level s with a positive constant ρl
such that
inf
Δ
# 
ΔX
2
F
T (
ΔS
2
F ∨ 
ΔSc
s 
2
F )
: |S| ≤ s, 
ΔSc
1 ≤ 3
ΔS
1
$
≥ ρl,
where ΔSc
s is formed by keeping the s entries of ΔSc with largest absolute values
and setting the others to be zero.
Assumption 13.8 There exists certain sparsity level s with a positive constant φu
such that
sup
Δ
|S|
1/2
ΔXX

∞
T 
ΔS
F
: |S| ≤ s, 
ΔSc
1 ≤ 3
ΔS
1
 
≤ φu.
Assumption 13.5 assumes the random errors are Gaussian; the technical argu￾ment still applies as long as the tail probability bound of the noise decays
exponentially. Assumption 13.6 is imposed to ensure the identifiability of the r latent
factors (singular vectors), without which the targeted unit-rank matrices would not
be distinguishable. Similar assumptions can be found in Mishra et al. (2017), Zheng
et al. (2019), among others.
Assumption 13.7 is a matrix version of the restricted eigenvalue (RE) condition
proposed in Bickel et al. (2009), which is typically imposed in 1-penalization to
restrict the correlations between the columns of X within certain sparsity level,
thus guaranteeing the identifiability of the true regression coefficients. Here the
difference is that we consider the estimation of coefficient matrices instead of
vectors. Because the Frobenius norm of a matrix can be regarded as the 2-norm13.3 Co-sparse Factor Regression 369
of the stacked vector consisting of the columns of the matrix, Assumption 13.7
is equivalent to the RE condition in the univariate response setting. The integer s
here acts as a theoretical upper bound on the sparsity level of the true coefficient
matrices, the requirement of which will be shown to be different in the two deflation
approaches.
Similar to Assumption 13.7, Assumption 13.8 is a matrix version of the cone
invertibility factor (Ye and Zhang 2010) type of condition. It allows us to control
the entrywise estimation error by assuming that T −1
ΔXX

∞ can be bounded by
some multiple of the averaged error 
ΔS
F /|S|
1/2 when Δ is restricted in the cone.
In this way, the impact of the estimation error of the initial Lasso estimate can spread
out on the support instead of massing at one component.
Now we are ready to present the main results. Denote by sk = 
C[k]
0 for 1 ≤
k ≤ r and s0 = 
C
0. The following theorem characterizes the estimation accuracy
in different layers of the sequential pursuit.
Theorem 13.2 (Sequential Pursuit) Suppose Assumptions 13.5–13.8 hold with
the sparsity level s ≥ max1≤k≤r sk. Choose λ1 = 2σmax0
2α log(mn)T for some
constant α > 1 and λk = Πk−1
=1 (1 + η)λ1. The following results hold uniformly
over 1 ≤ k ≤ r with probability at least 1 − (mn)1−α,

Δk
2
F = 
C
[k] − C[k]
2
F = O(γ 2
k θ 2
k sk log(mn)/T ),
where θk = dk/δk, ηk = cθk with constant c = 24φuρ−1
l , and γk = Πk−1
=1 (1 + η)
with γ1 = 1.
Theorem 13.2 presents the estimation error bounds in terms of the squared
Frobenius norm for the co-sparse unit-rank matrix estimators in sequential pursuit
under mild and reasonable conditions. This is non-trivial because the subsequent
layers play the role of extra noises in the estimation of each unit-rank matrix. We
address this issue by showing that its impact is secondary due to the orthogonality
between different layers. The results hold uniformly over all true layers with a high
probability approaching to one in polynomial order of mn.
The regularization parameter λk reflects the minimum penalization level needed
to suppress the noise in the kth layer, which consists of two parts. One is from the
random noise accompanied with the original response variables, while the other is
due to the accumulation of the estimation errors from all previous layers. The latter
is of a larger magnitude than the former one such that the penalization level increases
almost exponentially with k. This may be inevitable in the sequential procedure
since the kth layer estimator is based on the residual response matrix after extracting
the previous layers. Hopefully, the estimation consistency is generally guaranteed
for all the significant unit-rank matrices as long as the true regression coefficient
matrix C is of sufficiently low rank.
When the singular values are well separated, for the first few layers, the
estimation accuracy is about O(sk log(mn)/T ), which is close to the optimal rate
O((su + sv)log(n ∨ m)/T ) for the estimation of C established in Ma et al. (2020).370 13 Sparse Reduced-Rank Regression
When the true rank is one, the main difference between the two rates lies in the
sparsity factors, where (su + sv) is a sum of the sparsity levels of the left and right
singular vectors, while the sparsity factor in the sequential pursuit is the product
of them. However, the optimal rate is typically attained through some non-convex
algorithms (Ma et al. 2020; Yu et al. 2020; Uematsu et al. 2019) that search for the
optimal solution in a neighborhood of C. In contrast, the CURE algorithm enjoys
better computational efficiency and stability.
Last but not least, in view of the correlation constraint on the design matrix X,
that is, s ≥ max1≤k≤r sk, the sequential pursuit is valid as long as the number
of nonzero entries in each layer C[k] is within the sparsity level s imposed in
Assumptions 13.7 and 13.8. It means that in practice, the sequential deflation
strategy can recover some complex association networks layer by layer, which is
not shared by either the parallel pursuit or other methods that directly target on
estimating the multi-rank coefficient matrix.
We then turn our attention to the statistical properties of the parallel pursuit. The
key point of this deflation strategy is to find a relatively accurate initial estimator
such that the signals from the other layers/components except the targeted one can
be approximately removed. The reduced-rank regression estimator may not be a
good choice under high dimensions, since it lacks sparsity constraints. Similar to
Uematsu et al. (2019), we mainly adopt the following Lasso initial estimator,
C
 = arg minC
1
2

Y − CX
2
F + λ0
C
1,
which can be efficiently solved by various algorithms (Friedman et al. 2007). Under
the same RE condition (Assumption 13.7) as the sequential pursuit on the design
matrix X with a tolerated sparsity level s ≥ s0, where s0 indicates the number of
nonzero entries in C, we can show that this Lasso initial estimator is consistent with
a convergence rate of O(s0 log(mn)/T ).
Based on C
, we can obtain the initial unit-rank estimates C
0
[k] for different
layers through a Σx -orthogonal SVD. However, these unit-rank estimates are not
guaranteed to be sparse even if the Lasso estimator C
 is a sparse one, which causes
additional difficulties in high dimensions. Thus, to facilitate the theoretical analysis,
our initial kth layer estimate C
[k] takes the s largest components of C
0
[k] in terms of
absolute values while sets the others to be zero, where s is the upper bound on the
sparsity level defined in Assumptions 13.7 and 13.8. These s-sparse initial estimates
C
[k] can maintain the estimation accuracy of the initial unit-rank estimates C
0
[k]
without imposing any assumption on signal strength, regardless of the thresholding
procedure. In practice, we can use the SVD estimates directly as the impact of fairly
small entries is negligible.
Theorem 13.3 (Parallel Pursuit) Suppose Assumptions 13.5–13.8 hold with the
sparsity level s ≥ max0≤k≤r sk. Choose λ0 = 2σmax0
2α log(mn)T with constant
α > 1 for the initial Lasso estimator C
, and λk = 2cψk
0
log(mn)T for some
positive constant c. The following results hold uniformly over 1 ≤ k ≤ r with
probability at least 1 − (mn)1−α,13.4 Applications 371

Δk
2
F = 
C
[k] − C[k]
2
F = O(ψ2
k sk log(mn)/T ),
where ψk = d1dc/(dk min[δk−1, δk]) with dc the largest singular value of C.
Based on the Lasso initial estimator, Theorem 13.3 demonstrates that the parallel
pursuit achieves accurate estimation of different layers and reduces the sparsity
factor in the convergence rates from s0 to sk, where the sparsity level s0 of the
entire regression coefficient matrix can be about r times of sk. Moreover, as there is
no accumulated noises in the parallel estimation of different layers, we do not see
an accumulation factor γk here such that the penalization parameters keep around a
uniform magnitude. The eigen-factor ψk plays a similar role as θk in the convergence
rates for sequential pursuit in Theorem 13.2, both of which can be bounded from
above under the low-rank and sparse structures. Thus, the estimation accuracy of all
the significant unit-rank matrices is about the same as that of the first layer in the
sequential pursuit, which reveals the potential superiority of the parallel pursuit.
In summary, the sequential and the parallel pursuits are very similar, while the
main difference lies in the technical requirement on the sparsity level s. As discussed
after Assumption 13.7, the integer s constrains the correlations between the columns
of X to ensure the identifiability of the true supports, thus can be regarded as fixed
for a given design matrix. Since the parallel pursuit requires not only the sparsity
level sk of each individual layer but also the overall sparsity level s0 to be no
larger than s, it puts a stricter constraint on correlations among the predictors. In
other words, when the true coefficient matrix C is not sufficiently sparse, the Lasso
initial estimator may not be accurate enough to facilitate the subsequent parallel
estimation, in which case the sequential pursuit could enjoy some advantage.
13.4 Applications
13.4.1 Yeast eQTL Mapping Analysis
In an expression quantitative trait loci (eQTLs) mapping analysis, the main objective
is to examine the association between the eQTLs, i.e., regions of the genome
containing DNA sequence variants, and the expression levels of the genes in certain
signaling pathways. Biochemical evidence often suggests that there exist a few
functionally distinct signaling pathways of genes, each of which may involve only a
subset of genes and correspondingly a subset of eQTLs. Therefore, the recovery of
such association structure can be formulated as a sparse factor regression problem,
with the gene expressions being the responses and the eQTLs being the predictors.
Here, we analyze the yeast eQTL data set described by Brem and Kruglyak (2005)
and Storey et al. (2005), to illustrate the power of sparse and low-rank approaches.
There are n = 3244 genetic markers and m = 54 genes that belong to the yeast
Mitogen-activated protein kinases (MAPKs) signaling pathway (Kanehisa et al.
2009), with data collected from T = 112 yeast samples.372 13 Sparse Reduced-Rank Regression
A marginal screening approach can be used to reduce the number of marker
locations, to alleviate the computational burden of having very high-dimensional
predictors. Specifically, the markers are combined into blocks such that markers
with the same block differed by at most one sample, and one representative marker
is chosen from each block. A marginal gene-marker association analysis is then
performed to identify markers that are associated with the expression levels of at
least two genes with a p-value less than 0.05, resulting in a total of n = 605 markers.
It is also worth trying the inclusion of all the n = 3244 markers for a joint regression
without marginal screening. Therefore, we try both approaches in this analysis.
We consider the classical reduced-rank regression (RRR), the sparse reduced￾rank regression (SRRR), the parallel co-sparse factor regression (PS-FAR) initial￾ized by RRR, and the sequential co-sparse factor regression (SS-FAR). For the co￾sparse unit-rank regression (CURE) procedure, we apply the stagewise estimation
technique to speed up the computation (Chen et al. 2022). Under either setting, with
or without marginal screening, we perform a random-splitting procedure to compare
the performance of different methods. To make the comparison fair, all the methods
have the same pre-specified rank, which is selected from reduced-rank regression
(RRR) via 10-fold cross validation. Specifically, each time the data set is randomly
split into 80% for model fitting and 20% for computing the out-of-sample mean
squared error (MSE) of the fitted model. Also recorded are the number of nonzero
entries in B
 (
B

0), the number of nonzero columns in B
 (
B

2,0), the number of
nonzero entries in A
 (
A

0), and the number of nonzero rows in A
 (
A

2,0). The
procedure is repeated 100 times, and to make a robust comparison we compute the
10% trimmed means and standard deviations of the above performance measures.
Table 13.1 reports the results for the setting with marginal screening. All the
methods that pursue sparse and low-rank structures outperform the benchmark
reduced-rank regression method in term of out-of-sample prediction performance;
the PS-FAR method performs the best, although the improvement is not substantial
compared to other close competitors. The results in the setting of n = 3244 without
marginal screening are reported in Table 13.2. The predictive performance of all
Table 13.1 Yeast eQTL mapping analysis: Results with marginal screening
Method 
B

0 
B

2,0 
A

0 
A

2,0 MSE
RRR 1815 (0) 605 (0) 162 (0) 54 (0) 0.34 (0.03)
SRRR 522.45 (18.28) 174.15 (6.09) 162 (0) 54 (0) 0.24 (0.02)
SS-FAR 67.42 (8.03) 64.49 (7.52) 31.64 (4.28) 19.06 (1.9) 0.23 (0.02)
PS-FAR 72.71 (8.84) 69.62 (8.17) 34.50 (4.68) 21.25 (1.92) 0.21 (0.02)
Table 13.2 Yeast eQTL mapping analysis: Results with full data (without marginal screening)
Method 
B

0 
B

2,0 
A

0 
A

2,0 MSE
RRR 9732 (1788.02) 3244 (0) 162 (29.76) 54 (0) 0.44 (0.04)
SRRR 1332.09 (359.29) 437.06 (44.94) 162 (29.76) 54 (0) 0.24 (0.02)
SS-FAR 57.56 (9.09) 56.46 (8.5) 24.18 (5.37) 15.65 (2.37) 0.24 (0.02)
PS-FAR 67.46 (13.01) 64 (8.35) 30.91 (7.85) 19.25 (2.7) 0.22 (0.02)13.4 Applications 373
Table 13.3 Yeast eQTL mapping analysis: Top genes in the estimated pathways. The first panel
is for the setting with marginal screening, and the second panel is for the setting with full data
Method Layer 1 Layer 2 Layer 3
With marginal screening (n = 605)
SS-FAR STE3, STE2, MFA2,
MFA1, CTT1, FUS1
CTT1, FUS1, MSN4,
GLO1, SLN1
FUS1, FAR1, STE2,
GPA1, FUS3, STE3,
TEC1, SLN1, STE12,
MFA2, CTT1
PS-FAR STE3, STE2, MFA2,
MFA1, CTT1, FUS1,
TEC1
CTT1, FUS1, MSN4,
STE3, GLO1, SLN1,
MFA2
FUS1, FAR1, STE2,
GPA1, FUS3, STE3,
TEC1, CTT1, SLN1,
STE12
Full data (n = 3244)
SS-FAR STE3, STE2, MFA2,
MFA1, CTT1, FUS1
FUS1, CTT1, FAR1,
SLN1, STE2, MFA2,
GPA1
TEC1
PS-FAR STE3, STE2, MFA2,
MFA1, CTT1, FUS1,
TEC1, WSC2
CTT1, FUS1, MSN4,
GLO1, MFA2, STE3,
SLN1
FUS1, FAR1, STE2,
STE3, GPA1, FUS3,
TEC1, CTT1
methods becomes slightly worse compared to the setting with marginal screening.
The performance of reduced-rank regression deteriorates the most since it lacks
the power of eliminating noise variables. The results suggest that in this particular
application the potential benefit of jointly considering all the markers is exceeded
by the loss due to high dimensionality. We see that the deflation methods are
able to perform competitively in such a high-dimensional problem with excellent
scalability.
Finally, we examine the genes selected in the estimated pathways (low-rank
components or layers) with the full data set. Since each A
k vector is normalized
to have unit 1 norm, we define top genes to be those with entries larger than 1/m
in magnitude. The selected top genes are summarized in Table 13.3. Figure 13.1
shows the scatterplots of the latent responses Y
A
k versus the latent predictors X
B
k
from the two deflation methods fitted on the full data. For PS-FAR, we report the
results from using initial estimator either from the reduced-rank regression (PS￾FAR(R)) or the Lasso (PS-FAR(L)). As explained in Gustin et al. (1998), pheromone
induces mating and nitrogen starvation induces filamentation are two pathways in
yeast cells. The identified genes STE2, STE3, and GPA1 are receptors required
for mating pheromone, which bind the cognate lipopeptide pheromones (MFA2,
MFA1, etc.). Besides, another top gene FUS3 is used to phosphorylate several
downstream targets, including FAR1 which mediates various responses required for
successful mating. Interestingly, our analysis identified an additional gene, TEC1,
that was not reported in previous analysis. TEC1, as the transcription factor specific
to the filamentation pathway, has been shown to have FUS3-dependent degradation
induced by pheromone signaling (Bao et al. 2004).374 13 Sparse Reduced-Rank Regression
−2
0
2
4
−0.8 −0.4 0.0 0.4 0.8
Latent predictor
Latent response
PS-FAR(L)
PS-FAR(R)
SS-FAR
(a)
−2
0
2
4
−0.8 −0.4 0.0 0.4 0.8
Latent predictor
Latent response
PS-FAR(L)
PS-FAR(R)
SS-FAR
(b)
−2
0
2
4
−0.8 −0.4 0.0 0.4 0.8
Latent predictor
Latent response
PS-FAR(L)
PS-FAR(R)
SS-FAR
(c)
Fig. 13.1 Yeast eQTL mapping analysis: Scatter plots of the estimated latent responses and latent
predictors (Chen et al. 2022). (a) First layer. (b) Second layer. (c) Third layer
13.4.2 Forecasting Macroeconomic and Financial Indices
Forecasting is an important task in many areas of science. It involves modeling
historical time series data and using the resulting models to gain insights into likely
interrelationships and drive future decision-making on public polices, investment
strategies, financial plans, among others. In many applications, especially in finance
and in economics, there is often a need to forecast with multiple interrelated time
series. The modeling of multiple time series is extensively covered in Chapter 5.
The dynamic relationships among the components of such time series suggest that
multivariate modeling and simultaneous forecasting could be preferable. Moreover,
when the number of time series is relatively high, dimension reduction techniques,
especially reduced-rank modeling and sparse regularization, may help to identify
parsimonious models with improved forecasting performance. For the effect of
reduced-rank estimation on prediction, it has been explicitly demonstrated how it
can result in the decrease of prediction error covariance matrix (see Section 2.5). It
is of great interest to examine how the two techniques, reduced-rank estimation and
sparse estimation, impact on the forecasting, either separately or jointly. We thus
consider an example from macroeconomics to compare the empirical forecasting
performance of various sparse and/or reduced-rank time series models.
We use monthly time series data from May 1973 to February 2010 on 18
macroeconomic/financial indices and rates in the U.S. This data was considered
earlier by Carriero et al. (2011). The series include unemployment rate (ur), PCE
price index (pcepi), core PCE price excluding food and energy (pcexfepi), non￾farm payroll employment (payrolls), weekly hours worked (weeklyhrs), new claims
for unemployment insurance (claims), nominal retail sales (retailsales), index of13.4 Applications 375
Fig. 13.2 Macroeconomics time series: Plots of the original data
consumer confidence (consconf), single-family housing starts (starts), industrial
production growth rate (ip), index of capacity utilization (cu), purchasing managers’
index of supplier delivery times (pmisupdeliv), purchasing managers’ index of new
orders (pmiorders), price of oil (poil), S&P 500 index of stock prices (sp500), yields
on 10-year treasury bonds (itb10y), federal funds rate (ffr), and real exchange rate
(realxr). Figure 13.2 displays the original data. As can be seen several series are non￾stationary and to make the analysis straightforward, for each series the stochastic
trend in the original data is removed by taking first difference, and the series is
then centered. To permit a meaningful comparison of different models and improve
illustration, we also replace some anomalous observations in each time series. To be
specific, any observation that is with a magnitude larger than 2.5 times the standard
deviation of the series is replaced with the mean of the series before that point. We
refer to Section 12.2 for the perspectives of robust estimation. Figure 13.3 shows the
data after differencing, centering, and anomaly replacement. These series no longer
seem to exhibit any apparent non-stationary behaviors.
With the processed data, we are interested in comparing the forecasting perfor￾mance of various univariate/multivariate autoregressive models (AR). We use the
data from May 1973 to December 2007 for model training, and each trained model
is then used to make one-step-ahead forecast with the rest of the data from January
2008 to February 2010. Univariate modeling suggests that in general each time
series can be adequately modeled by an autoregressive model with order 2 (AR(2)).
We thus focus on order-2 models in vector autoregressive (VAR) modeling as well.376 13 Sparse Reduced-Rank Regression
Fig. 13.3 Macroeconomics time series: Plots of the processed data
Table 13.4 Analysis of
macroeconomics time series:
Forecasting performance of
various
univariate/multivariate
autoregressive models
VAR(2)
AR(2) LS RRR GL SRRR
MSE 0.4423 0.4363 0.4326 0.4000 0.3990
TrimMSE 0.1823 0.1776 0.1737 0.1653 0.1656
Rank 18 8 18 8
% Nonzero 100% 100% 100% 77.8% 77.8%
A VAR model can be conveniently expressed in a multivariate linear regression
form; see Section 5.2. This allows us to adopt various reduced-rank and sparse
methods for model estimation. Specifically, to fit VAR(2) model, we consider least
squares method (LS), the penalized least squares method with column-wise group
Lasso penalty (GL), the reduced-rank regression (RRR), and the column-sparse
reduced-rank regression (SRRR). For each method, the forecasting performance is
measured by the mean squared errors (MSE) and the 10% trimmed mean squared
errors (TrimMSE) resulting from the one-step-ahead predictions with the testing
data. We also report the estimated rank and the percentage of nonzeros in the
estimated coefficient matrix (% Nonzero).
Table 13.4 reports the results from fitting the aforementioned models. Several
observations can be made. First, multivariate models in general forecast better
than univariate models. Comparing the univariate with the multivariate AR models,
the latter can utilize the potential associations among the multiple time series
to improve simultaneous forecasting. Second, with the VAR model setup, either13.4 Applications 377
the reduced-rank estimation or the sparse regularization may lead to improved
forecasting performance upon the least squares approach. In this particular example,
it appears that the group Lasso method results in more substantial improvement
in forecasting than the reduced-rank regression; the former produces a sparse
solution with about 22.2% of zeros in the estimated coefficient matrix, while the
latter substantially reduces the rank of the estimated coefficient matrix from 18 to
8. Last but not the least, the sparse reduced-rank regression model produces the
most parsimonious model and performs the best in forecasting among all models.
Although its forecasting performance is only slightly better than that of the group
Lasso approach, this is achieved by a much more parsimonious model with a sparse
and reduced-rank coefficient matrix. A cursory review of the estimation results
would indicate that unemployment rate, yields on 10-year treasury bonds, and
federal funds rate can be removed at both lags, and weekly hours worked and index
of capacity utilization can be taken out of lag 2. A systematic way to review the
sparseness results in the time series context is yet to be studied.
This example thus clearly demonstrates the strength of integrating sparsity
and reduced-rank structures for forecasting. More development on time series
forecasting and analysis via sparse and reduced-rank modeling can be found in,
for example, Gillard and Usevich (2018), Wang et al. (2022), Barratt et al. (2021),
and Dong et al. (2022).Appendix
See Tables A.1, A.2, A.3 and A.4.
Table A.1 Biochemical data
on urine samples of patients
from a study by Smith et al.
(1962)
y1 y2 y3 y4 y5 x1 x2 x3
17.60 1.50 1.50 1.88 7.50 0.98 2.05 2.40
13.40 1.65 1.32 2.24 7.10 0.98 1.60 3.20
20.30 0.90 0.89 1.28 2.30 1.15 4.80 1.70
22.30 1.75 1.50 2.24 4.00 1.28 2.30 3.00
18.50 1.20 1.03 1.84 2.00 1.28 2.15 2.70
12.10 1.90 1.87 2.40 16.80 1.22 2.15 2.50
10.10 2.30 2.08 2.68 0.90 1.22 1.90 2.80
14.70 2.35 2.55 3.00 2.00 1.30 1.75 2.40
14.40 2.50 2.38 2.84 3.80 1.30 1.55 2.70
18.10 1.50 1.20 2.60 14.50 1.35 2.20 3.10
16.90 1.40 1.15 1.72 8.00 1.35 3.05 3.20
23.70 1.65 1.58 1.60 4.90 1.35 2.75 2.00
18.00 1.60 1.68 2.00 3.60 1.35 2.10 2.30
14.80 2.45 2.15 3.12 12.00 1.40 1.70 3.10
15.60 1.65 1.42 2.56 5.20 1.40 2.35 2.80
16.20 1.65 1.62 2.04 10.20 1.41 1.85 2.10
17.50 1.05 1.56 1.48 9.60 1.41 2.65 1.50
14.10 2.70 2.77 2.56 6.90 1.62 3.05 2.60
22.50 0.85 1.65 1.20 3.50 1.62 4.30 1.60
17.00 0.70 0.97 1.24 1.90 2.05 3.50 1.80
12.50 0.80 0.80 0.64 0.70 2.05 4.75 1.00
21.50 1.80 1.77 2.60 8.30 2.30 1.95 3.30
13.00 2.20 1.85 3.84 13.00 2.30 1.60 3.50
(continued)
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8
379380 Appendix
Table A.1 (continued) y1 y2 y3 y4 y5 x1 x2 x3
13.00 3.55 3.18 3.48 18.30 2.15 2.40 3.30
12.00 3.65 2.40 3.00 14.50 2.15 2.70 3.40
22.80 0.55 1.00 1.14 3.30 2.30 4.75 1.60
18.40 1.05 1.17 1.36 4.90 2.30 4.90 2.80
8.70 4.25 3.62 3.84 19.50 2.62 1.15 2.50
9.40 3.85 3.36 5.12 1.30 2.62 0.97 2.80
15.00 2.45 2.38 2.40 20.00 2.55 3.25 2.70
12.90 1.70 1.74 2.48 1.00 2.55 3.10 2.30
12.10 1.80 2.00 2.24 5.00 2.70 2.45 2.50
11.50 2.25 2.25 3.12 5.10 2.70 2.20 3.40
Response variables are y1 = pigment creatinine, y2 = phos￾phate (mg/ml), y3 = phosphorous (mg/ml), y4 = creatinine
(mg/ml), y5 = choline (μg/ml); predictor variables are
x1 = weight (lbs/100), x2 = volume (ml/100), x3 = 100
(specific gravity −1)
Table A.2 Sales
performance data x1 x2 x3 y1 y2 y3 y4
93 96 97.8 9 12 9 20
88.8 91.8 96.8 7 10 10 15
95 100.3 99 8 12 9 26
101.3 103.8 106.8 13 14 12 29
102 107.8 103 10 15 12 32
95.8 97.5 99.3 10 14 11 21
95.5 99.5 99 9 12 9 25
110.8 122 115.3 18 20 15 51
102.8 108.3 103.8 10 17 13 31
106.8 120.5 102 14 18 11 39
103.3 109.8 104 12 17 12 32
99.5 111.8 100.3 10 18 8 31
103.5 112.5 107 16 17 11 34
99.5 105.5 102.3 8 10 11 34
100 107 102.8 13 10 8 34
81.5 93.5 95 7 9 5 16
101.3 105.3 102.8 11 12 11 32
103.3 110.8 103.5 11 14 11 35
95.3 104.3 103 5 14 13 30
99.5 105.3 106.3 17 17 11 27
88.5 95.3 95.8 10 12 7 15
99.3 115 104.3 5 11 11 42
87.5 92.5 95.8 9 9 7 16
105.3 114 105.3 12 15 12 37
(continued)Appendix 381
Table A.2 (continued) x1 x2 x3 y1 y2 y3 y4
107 121 109 16 19 12 39
93.3 102 97.8 10 15 7 23
106.8 118 107.3 14 16 12 39
106.8 120 104.8 10 16 11 49
92.3 90.8 99.8 8 10 13 17
106.3 121 104.5 9 17 11 44
106 119.5 110.5 18 15 10 43
88.3 92.8 96.8 13 11 8 10
96 103.3 100.5 7 15 11 27
94.3 94.5 99 10 12 11 19
106.5 121.5 110.5 18 17 10 42
106.5 115.5 107 8 13 14 47
92 99.5 103.5 18 16 8 18
102 99.8 103.3 13 12 14 28
108.3 122.3 108.5 15 19 12 41
106.8 119 106.8 14 20 12 37
102.5 109.3 103.8 9 17 13 32
92.5 102.5 99.3 13 15 6 23
102.8 113.8 106.8 17 20 10 32
83.3 87.3 96.3 1 5 9 15
94.8 101.8 99.8 7 16 11 24
103.5 112 110.8 18 13 12 37
89.5 96 97.3 7 15 11 14
84.3 89.8 94.3 8 8 8 9
104.3 109.5 106.5 14 12 12 36
106 118.5 105 12 16 11 39
Response variables are y1: score on creativity test,
y2: score on mechanical reasoning test, y3: score
on abstract reasoning test, y4: score on Mathemat￾ics test; predictor variables are x1: sales growth,
x2: sales profitability, x3: new account sales382 Appendix
Table A.3 Quarterly macroeconomic time series data for the UK, 1948–1956, from Klein et al.
(1961)
y1 y2 y3 y4 y5 x1 x2 x3 x4 x5
95.43 99.61 89.74 99.3 91.3 99.0 98.7 97.5 97.8 99.7
103.11 102.69 95.67 100.0 99.3 99.8 99.7 100.2 99.3 102.2
100.27 98.79 108.76 100.7 102.0 100.4 100.3 100.7 101.1 101.6
101.59 98.91 105.83 100.0 107.4 100.8 101.3 101.6 101.9 102.2
101.03 100.11 105.54 100.7 113.3 100.6 102.2 101.5 102.6 101.2
108.71 103.09 98.67 107.4 106.5 100.9 102.5 100.8 102.6 102.9
105.97 99.79 97.46 109.7 103.7 101.1 103.2 99.1 102.6 102.9
107.89 101.51 104.13 107.9 115.3 101.7 103.2 102.9 103.3 102.9
110.03 103.21 107.24 104.6 123.2 101.7 104.1 105.4 105.9 104.6
115.11 104.89 102.37 112.9 119.3 101.8 104.1 108.4 107.7 106.8
112.57 102.09 105.36 104.5 125.4 102.1 104.1 108.2 109.5 105.9
116.59 105.51 99.03 104.2 134.4 102.6 106.7 116.5 113.4 106.3
113.73 106.51 87.34 112.1 122.7 102.6 109.8 125.0 119.5 109.5
122.41 106.39 73.07 120.1 130.6 102.7 112.0 134.5 127.7 114.6
116.27 102.79 79.36 125.8 123.2 103.3 113.9 134.8 134.7 116.9
116.59 104.51 93.43 120.9 125.9 103.3 117.7 133.7 137.3 117.3
116.03 103.51 119.64 119.0 131.9 103.2 120.9 134.8 137.7 119.3
115.41 106.29 151.67 113.2 115.0 102.9 121.8 132.3 138.1 121.9
108.97 103.59 142.96 100.2 106.7 102.8 123.0 128.6 136.0 121.5
115.59 104.81 131.53 106.2 118.9 102.8 125.9 125.0 133.8 122.0
116.73 105.31 127.74 113.3 119.3 102.6 127.5 123.5 132.5 123.1
121.71 110.79 111.97 123.5 121.7 102.9 127.8 121.1 130.8 124.9
117.97 105.49 106.86 118.7 122.3 103.4 129.0 119.1 131.2 123.6
125.89 109.41 104.03 121.0 134.2 103.8 130.0 117.9 130.3 122.8
126.73 106.11 108.44 120.4 131.0 103.9 131.6 116.4 129.9 124.6
131.41 111.59 93.47 121.5 130.7 104.3 134.1 118.4 129.9 126.3
126.27 108.09 89.06 120.0 129.0 104.9 135.1 119.9 129.9 127.0
134.29 114.71 84.23 121.8 129.9 105.3 136.0 120.7 129.9 126.2
134.73 107.81 80.64 138.2 143.3 105.2 139.4 123.8 130.8 128.3
139.41 113.19 76.87 125.8 125.1 105.6 143.9 122.3 131.6 130.2
130.97 112.59 77.06 137.4 138.4 106.2 144.8 121.5 133.4 130.7
141.29 114.91 71.43 137.1 147.6 106.6 145.5 123.5 134.2 131.7
135.73 112.01 75.74 136.9 144.1 106.5 150.2 125.0 136.0 134.6
140.41 114.59 78.77 136.6 151.2 106.7 154.9 125.5 136.4 137.6
130.97 110.99 95.26 128.9 135.8 107.0 155.9 122.6 138.1 136.3
137.89 119.21 88.93 132.4 152.7 107.0 156.5 127.4 139.0 136.1
Response variables are y1 = industrial production, y2 = consumption, y3 = unemployment, y4 =
total imports, y5 = total exports; predictor variables are x1 = total labor force, x2 = weekly wage
rates, x3 = price index of imports, x4 = price index of exports, x5 = price index of consumptionAppendix 383
Table A.4 Blood sugar
concentrations (
y
0
−
y
5, in
mg/100 ml) of 36 rabbits at 0, 1, 2, 3, 4, and 5 hours after
administration of insulin
dose, with two types of
insulin preparation (standard, x1 = −1, and test, x1 = +1)
and two dose levels (0.75
units,
x
2 = −1, and 1.50
units,
x
2 = +1), from a study
by Volund (1980
)
y
0
y
1
y
2
y
3
y
4
y
5
x
1
x
2
96 37 31 33 35 41
1
1
90 47 48 55 68 89
1
1
99 49 55 64 74 97
1
1
95 33 37 43 63 92
1
1
107 62 62 85 110 117
1
1
81 40 43 45 49 55
1
1
95 49 56 63 68 88
1
1
105 53 57 69 103 106
1
1
97 50 53 59 82 96
1
1
97 54 57 66 80 89
1
−
1
105 66 83 95 97 100
1
−
1
105 49 54 56 70 90
1
−
1
106 79 92 95 99 100
1
−
1
92 46 51 57 73 91
1
−
1
91 61 64 71 80 90
1
−
1
101 51 63 91 95 96
1
−
1
87 53 55 57 78 89
1
−
1
94 57 70 81 94 96
1
−
1
98 48 55 71 91 96
−
1
1
98 41 43 61 89 101
−
1
1
103 60 56 61 76 97
−
1
1
99 36 43 57 89 102
−
1
1
97 44 51 58 85 105
−
1
1
95 41 45 49 59 78
−
1
1
109 65 62 72 93 104
−
1
1
91 57 60 61 67 83
−
1
1
99 43 48 52 61 86
−
1
1
102 51 56 81 97 103
−
1
−
1
96 57 55 72 85 89
−
1
−
1
111 84 83 91 101 102
−
1
−
1
105 57 67 83 100 103
−
1
−
1
105 57 61 70 90 98
−
1
−
1
98 55 67 88 94 95
−
1
−
1
98 69 72 89 98 98
−
1
−
1
90 53 61 78 94 95
−
1
−
1
100 60 63 67 77 104
−
1
−
1References
Ahn, S. K. and Reinsel, G. C. (1988) Nested reduced-rank autoregressive models for multiple time
series. Journal of the American Statistical Association, 83, 849–856.
Ahn, S. K. and Reinsel, G. C. (1990) Estimation for partially nonstationary multivariate autore￾gressive models. Journal of the American Statistical Association, 85, 813–823.
Aitchison, J. and Silvey, S. D. (1958) Maximum likelihood estimation of parameters subject to
restraints. Annals of Mathematical Statistics, 29, 813–828.
Akaike, H. (1974) A new look at the statistical model identification. IEEE Transactions on
Automatic Control, 19, 716–723.
Akaike, H. (1976) Canonical correlation analysis of time series and the use of an information
criterion. In Systems Identification: Advances and Case Studies (eds. R. K. Mehra and D. G.
Lainiotis), 27–96. New York: Academic Press.
Albert, J. M. and Kshirsagar, A. M. (1993) The reduced-rank growth curve model for discriminant
analysis of longitudinal data. Australian Journal of Statistics, 35, 345–357.
Aldrin, M. (2000) Multivariate prediction using softly shrunk reduced-rank regression. The
American Statistician, 54, 29–34.
Allen, G. I., Grosenick, L. and Taylor, J. (2014) A generalized least-square matrix decomposition.
Journal of the American Statistical Association, 109, 145–159.
Amemiya, Y. and Fuller, W. A. (1984) Estimation for the multivariate errors-in-variables model
with estimated error covariance matrix. The Annals of Statistics, 12, 497–509.
Anderson, T. W. (1951) Estimating linear restrictions on regression coefficients for multivariate
normal distributions. Annals of Mathematical Statistics, 22, 327–351.
Anderson, T. W. (1963) The use of factor analysis in the statistical analysis of multiple time series.
Psychometrika, 28, 1–25.
Anderson, T. W. (1971) The Statistical Analysis of Time Series. New York: Wiley.
Anderson, T. W. (1976) Estimation of linear functional relationships: Approximate distributions
and connections with simultaneous equations in econometrics (with discussion). Journal of the
Royal Statistical Society: Series B, 38, 1–36.
Anderson, T. W. (1991) Trygve Haavelmo and simultaneous equations models. Scandinavian
Journal of Statistics, 18, 1–19.
Anderson, T. W. (2002) Specification and misspecification in reduced rank regression. Sankhy A:
The Indian Journal of Statistics, 64, 193–205.
Anderson, T. W. and Rubin, H. (1956) Statistical inference in factor analysis. In Proceedings
of the Third Berkeley Symposium on Mathematical Statistics and Probability, Volume 5:
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8
385386 References
Contributions to Econometrics, Industrial Research, and Psychometry, 111–150. Berkeley,
Calif.: University of California Press.
Anderson, T. W. (1984a) Estimating linear statistical relationships. The Annals of Statistics, 12,
1–45.
Anderson, T. W. (1984b) An Introduction to Multivariate Statistical Analysis. New York: Wiley
second edn.
Anderson, T. W. (1999a) Asymptotic distribution of the reduced rank regression estimator under
general conditions. The Annals of Statistics, 27, 1141–1154.
Anderson, T. W. (1999b) Asymptotic theory for canonical correlation analysis. Journal of
Multivariate Analysis, 70, 1–29.
Bai, Z. D. and Silverstein, J. W. (2009) Spectral Analysis of Large Dimensional Random Matrices.
Springer, New York, 2 edn.
Bao, M. Z., Schwartz, M. A., Cantin, G. T., Yates III, J. R. and Madhani, H. D. (2004) Pheromone￾dependent destruction of the Tec1 transcription factor is required for MAP kinase signaling
specificity in yeast. Cell, 119, 991–1000.
Barratt, S., Dong, Y. and Boyd, S. (2011) Low-rank forecasting. ArXiv:2101.12414.
Barron, A., Birgé, L. and Massart, P. (1999) Risk bounds for model selection via penalization.
Probability Theory and Related Fields, 113, 301–413.
Bartlett, M. S. (1938) Further aspects of the theory of multiple regression. Proceedings of the
Cambridge Philosophical Society, 34, 33–40.
Bartlett, M. S. (1947) Multivariate analysis. Journal of the Royal Statistical Society: Series B, 9,
176–197.
Beaulieu, M. C., Dufour, J. M. and Khalaf, L. (2013) Identification-robust estimation and testing
of the zero-beta CAPM. The Review of Economic Studies, 80, 892–924.
Bekker, P., Dobbelstein, P. and Wansbeek, T. (1996) The APT model as reduced-rank regression.
Journal of Business & Economic Statistics, 14, 199–202.
Bertsimas, D., King, A. and Mazumder, R. (2016) Best subset selection via a modern optimization
lens. The Annals of Statistics, 44(2), 813–852.
Bewley, R. and Yang, M. (1995) Tests for cointegration based on canonical correlation analysis.
Journal of the American Statistical Association, 90, 990–996.
Bhargava, A. K. (1979) Estimation of a linear transformation and an associated distributional
problem. Journal of Statistical Planning and Inference, 3, 19–26.
Bickel, P. J., Ritov, Y. and Tsybakov, A. B. (2009) Simultaneous analysis of Lasso and Dantzig
selector. The Annals of Statistics, 37, 1705–1732.
Bilodeau, M. and Kariya, T. (1989) Minimax estimators in the normal MANOVA model. Journal
of Multivariate Analysis, 28, 260–270.
Black, F. (1972) Capital market equilibrium with restricted borrowing. Journal of Business, 45,
444–454.
Blattberg, R. C. and George, E. I. (1991) Shrinkage estimation of price and promotional elasticities:
Seemingly unrelated equations. Journal of the American Statistical Association, 86, 304–315.
Blattberg, R. C. and Wisniewski, K. J. (1989) Price-induced patterns of competition. Marketing
Science, 8, 291–309.
Blattberg, R. C., Kim, B. and Ye, J. (1994) Large scale databases: The new marketing challenge.
In The Marketing Information Revolution (eds. R. C. Blattberg, R. Glazer and J. L. J. L. Chap.),
173–203. Boston: Harvard Business School Press.
Boot, J. and de Wit, G. M. (1960) Investment demand: An empirical contribution to the aggregation
problem. International Economic Review, 1, 3–30.
Bossaerts, P. (1988) Common nonstationary components of asset prices. Journal of Economic
Dynamics and Control, 12, 347–364.
Box, G. E. P. and Tiao, G. C. (1977) A canonical analysis of multiple time series. Biometrika, 64,
355–365.
Boyd, S. and Vandenberghe, L. (2004) Convex Optimization. Cambridge University Press.References 387
Boyd, S., Parikh, N., Chu, E., Peleato, B. and Eckstein, J. (2011) Distributed optimization and
statistical learning via the alternating direction method of multipliers. Foundations and
Trends® in Machine Learning, 3, 1–122.
Breheny, P. and Huang, J. (2009) Penalized methods for bi-level variable selection. Statistics and
Its Interface, 2, 369–380.
Breiman, L. and Friedman, J. H. (1997) Predicting multivariate responses in multiple linear
regression. Journal of the Royal Statistical Society: Series B, 59, 3–54.
Brem, R. B. and Kruglyak, L. (2005) The landscape of genetic complexity across 5,700 gene
expression traits in yeast. Proceedings of the National Academy of Sciences of the United
States of America, 102, 1572–1577.
Brillinger, D. R. (1969) The canonical analysis of stationary time series. In Multivariate Analysis,
331–350. P.R. Krishnaiah, New York: Academic Press.
Brillinger, D. R. (1981) Time Series: Data Analysis and Theory. San Francisco: Holden–Day,
expanded edn.
Brooks, R. and Stone, M. (1994) Joint continuum regression for multiple predictands. Journal of
the American Statistical Association, 89, 1374–1377.
Brown, P. J. and Payne, C. (1975) Election night forecasting (with discussion). Journal of the
Royal Statistical Society: Series A, 138, 463–498.
Brown, P. J. and Zidek, J. V. (1980) Adaptive multivariate ridge regression. The Annals of Statistics,
8, 64–74.
Brown, P. J. and Zidek, J. V. (1982) Multivariate regression shrinkage estimators with unknown
covariance matrix. Scandinavian Journal of Statistics, 9, 209–215.
Bunea, F., She, Y. and Wegkamp, M. (2011) Optimal selection of reduced rank estimators of high￾dimensional matrices. The Annals of Statistics, 39, 1282–1309.
Bunea, F., She, Y. and Wegkamp, M. H. (2012) Joint variable and rank selection for parsimonious
estimation of high-dimensional matrices. The Annals of Statistics, 40, 2359–2388.
Bura, E. and Cook, R. D. (2001) Estimating the structural dimensions of regressions via parametric
inverse regression. Journal Royal Statistics Society: Series B, 63, 393–410.
Bühlmann, P. and van de Geer, S. (2011) Statistics for high-dimensional data. Springer Series in
Statistics. Springer, Heidelberg.
Cai, J.-F., Candés, E. J. and Shen, Z. (2010) A singular value thresholding algorithm for matrix
completion. SIAM Journal on Optimization, 20, 1956–1982.
Campbell, N. (1984) Canonical variate analysis—a general formulation. Australian Journal of
Statistics, 26, 86–96.
Campbell, J. (1987) Stock returns and the term structure. Journal of Financial Economics, 18,
373–399.
Campbell, J., Lo., A. and MacKinlay, A. C. (1997) The Econometrics of Financial Markets.
Princeton, NJ: Princeton University Press.
Candès, E. J. and Recht, B. (2009) Exact matrix completion via convex optimization. Found.
Comput. Math., 9, 717–772.
Candès, E. J. and Tao, T. (2010) The power of convex relaxation: Near-optimal matrix completion.
IEEE Transactions on Information Theory, 56, 2053–2080.
Candès, E. J., Li, X., Ma, Y. and Wright, J. (2011) Robust principal component analysis? Journal
of the ACM, 58, 1–37.
Candès, E. J., Sing-Long, C. A. and Trzasko, J. D. (2013) Unbiased risk estimates for singular value
thresholding and spectral estimators. IEEE Transactions on Signal Processing, 61, 4643–4657.
Carriero, A., Kapetanios, G. and Marcellino, M. (2011) Forecasting large datasets with Bayesian
reduced rank multivariate models. Journal of Applied Econometrics, 26, 735–761.
Caruana, R. (1997) Multitask learning. Machine Learning, 28, 41–75.
Chakraborty, A., Bhattacharya, A. and Mallick, B. K. (2019) Bayesian sparse multiple regression
for simultaneous rank reduction and variable selection. Biometrika, 107, 205–221.
Chen, K. and Chan, K.-S. (2016) A note on rank reduction and variable selection in multivariate
regression. Journal of Statistical Theory and Practice, 10, 100–120.388 References
Chen, K., Chan, K.-S. and Stenseth, N. C. (2012) Reduced rank stochastic regression with a sparse
singular value decomposition. Journal of the Royal Statistical Society: Series B, 74, 203–221.
Chen, K., Dong, H. and Chan, K.-S. (2013) Reduced rank regression via adaptive nuclear norm
penalization. Biometrika, 100, 901–920.
Chen, K., Chan, K.-S. and Stenseth, N. C. (2014) Source-sink reconstruction through regularized
multicomponent regression analysis–with application to assessing whether North Sea cod
larvae contributed to local fjord cod in Skagerrak. Journal of the American Statistical
Association, 109, 560–573.
Chen, K., Hoffman, E. A., Seetharaman, I., Lin, C.-L. and Chan, K.-S. (2016) Linking lung
airway structure to pulmonary function via composite bridge regression. The Annals of Applied
Statistics, 10, 1880–1906.
Chen, K., Dong, R., Xu, W., and Zheng, Z. (2022) Fast stagewise sparse factor regression. Journal
of Machine Learning Research, 23(271):1–45.
Chen, L. and Huang, J. Z. (2012) Sparse reduced-rank regression for simultaneous dimension
reduction and variable selection. Journal of the American Statistical Association, 107, 1533–
1545.
Chi, E. C. and Li, T. (2019) Matrix completion from a computational statistics perspective. WIREs
Computational Statistics, 11, e1469.
Chi, E. M. and Reinsel, G. C. (1989) Models for longitudinal data with random effects and AR(1)
errors. Journal of the American Statistical Association, 84, 452–459.
Chin, K., DeVries, S., Fridlyand, J., Spellman, P. T., Roydasgupta, R., Kuo, W.-L., Lapuk, A.,
Neve, R. M., Qian, Z. and Ryder, T. (2006) Genomic and transcriptional aberrations linked to
breast cancer pathophysiologies. Cancer Cell, 10, 529–541.
Chinchilli, V. M. and Elswick, R. K. (1985) A mixture of the MANOVA and GMANOVA models.
Communications in Statistics, Theory and Methods, 14, 3075–3089.
Collins, M., Dasgupta, S. and Schapire, R. E. (2002) A generalization of principal components
analysis to the exponential family. In Advances in Neural Information Processing Systems
(NeurIPS) 14, 617–624. Curran Associates, Inc.
Connor, G. and Linton, O. (2007) Semiparametric estimation of a characteristic-based factor model
of stock returns. Journal of Empirical Finance, 14, 694–717.
Connor, G., Hagmann, M. and Linton, O. (2012) Efficient semiparametric estimation of the Fama￾French model and extensions. Econometrica, 80, 713–754.
Cook, R, D. (2018) An Introduction to Envelopes: Dimension Reduction for Efficient Estimation in
Multivariate Statistics. Wiley.
Cook, R. D. and Su, Z. (2013) Scaled envelopes: scale-invariant and efficient estimation in
multivariate linear regression. Biometrika, 100, 993–954.
Cook, R. D. and Zhang, X. (2018) Fast envelope algorithms. Statistica Sinica, 28, 1179–1197.
Cook, R. D., Li, B. and Chiaromonte, F. (2010) Envelope models for parsimonious and efficient
multivariate linear regression. Statistica Sinica, 20, 927–960.
Cook, R. D., Forzani, L. and Zhang, X. (2015) Envelopes and reduced-rank regression. Biometrika,
102, 439–456.
Cox, D. R. and Wermuth, N. (1992) Response models for mixed binary and quantitative variables.
Biometrika, 79, 441–461.
Cragg, J. G. and Donald, S. G. (1996) On the asymptotic properties of LDU-based tests of the rank
of a matrix. Journal of the American Statistical Association, 91, 1301–1309.
Davenport, M. A. and Romberg, J. (2016) An overview of low-rank matrix recovery from
incomplete observations. IEEE Journal of Selected Topics in Signal Processing, 10, 608–622.
Davies, P. T. and Tso, M. K.-S. (1982) Procedures for reduced-rank regression. Journal of the
Royal Statistical Society: Series C, 31, 244–255.
de Jong, S. (1993) SIMPLS: An alternative approach to partial least squares regression. Chemo￾metrics and Intelligent Laboratory Systems, 18, 251–263.
de Leon, A. R. and Wu, B. (2011) Copula-based regression models for a bivariate mixed discrete
and continuous outcome. Statistics in Medicine, 30, 175–185.References 389
Diggle, P. J. (1988) An approach to the analysis of repeated measurements. Biometrics, 44, 959–
971.
Doan, T., Litterman, R. and Sims, C. (1984) Forecasting and conditional projection using realistic
prior distributions. Econometric Reviews, 3, 1–100.
Dong, Y., Qin, S. J. and Boyd, S. P. (2022) Extracting a low-dimensional predictable time series.
Optimization and Engineering, 23, 1189–1214.
Donoho, D. L. (2000) High-dimensional data analysis: The curses and blessings of dimensionality.
In American Mathematical Society Conference on Math Challenges of the 21st Century.
Donoho, D. L. and Huber, P. J. (1983) The notion of breakdown point. In A Festschrift for
Erich L. Lehmann, Wadsworth Statistics/Probability Series, 157–184. Belmont: Wadsworth
International.
Donoho, D. L. and Johnstone, I. M. (1995) Adapting to unknown smoothness via wavelet
shrinkage. Journal of the American Statistical Association, 90, 1200–1224.
Dunson, D. B. (2000) Bayesian latent variable models for clustered mixed outcomes. Journal of
the Royal Statistical Society: Series B, 62, 355–366.
Eckart, C. and Young, G. (1936) The approximation of one matrix by another of lower rank.
Psychometrika, 1, 211–218.
Efron, B. (2004) The estimation of prediction error: Covariance penalties and cross-validation.
Journal of the American Statistical Association, 99, 619–642.
Efron, B. and Morris, C. (1972) Empirical Bayes on vector observations: An extension of Stein’s
method. Biometrika, 59, 335–347.
Efron, B., Hastie, T. J., Johnstones, I. and Tibshirani, R. (2004) Least angle regression. The Annals
of Statistics, 32(2), 407–499.
Engle, R. F. and Granger, C. (1987) Co-integration and error correction: Representation, estima￾tion, and testing. Econometrica, 55, 251–276.
Fan, J. and Li, R. (2001) Variable selection via nonconcave penalized likelihood and its oracle
properties. Journal of the American Statistical Association, 96, 1348–1360.
Fan, J. and Lv, J. (2010) A selective overview of variable selection in high dimensional feature
space. Statistica Sinica, 20, 101–148.
Fan, J., Liao, Y. and Wang, W. (2016) Projected principal component analysis in factor models.
The Annals of Statistics, 44, 219–254.
Farago, A. and Hjalmarsson, E. (2019) Stock price co-movement and the foundations of pairs
trading. Journal of Financial and Quantitative Analysis. Cambridge University Press. 54(2),
629–665.
Fazel, M. (2002) Matrix rank minimization with applications. Ph.D. thesis, Stanford University.
Ferson, W. and Harvey, C. (1991) The variation of economic risk premiums. Journal of Political
Economy, 99, 385–415.
Fisher, R. A. (1938) The statistical utilization of multiple measurements. Annals of Eugenics, 8,
376–386.
Fitzmaurice, G. M. and Laird, N. M. (1995) Regression models for a bivariate discrete and
continuous outcome with clustering. Journal of the American Statistical Association, 90, 845–
852.
Fortier, J. J. (1966) Simultaneous linear prediction. Psychometrika, 31, 369–381.
Foster, D. and George, E. (1994) The risk inflation criterion for multiple regression. The Annals of
Statistics, 22(4), 1947–1975.
Fountis, N. G. and Dickey, D. A. (1989) Testing for a unit root nonstationarity in multivariate
autoregressive time series. The Annals of Statistics, 17, 419–428.
Frank, I. E. and Friedman, J. H. (1993) A statistical view of some chemometrics regression tools
(with discussion). Technometrics, 35, 109–148.
Franklin, J. N. (2000) Matrix Theory. Toronto: Dover Publications.
Friedman, J., Hastie, T. J., Höfling, H. and Tibshirani, R. (2007) Pathwise coordinate optimization.
The Annals of Applied Statistics, 2, 302–332.
Fu, W. J. (1998) Penalized regressions: The bridge versus the lasso. Journal of Computational and
Graphical Statistics, 7, 397–416.390 References
Fujikoshi, Y. (1974) The likelihood ratio tests for the dimensionality of regression coefficients.
Journal of Multivariate Analysis, 4, 327–340.
Gabriel, K. R. and Zamir, S. (1979) Lower rank approximation of matrices by least squares with
any choice of weights. Technometrics, 21, 489–498.
Gallant, A. R. and Goebel, J. J. (1976) Nonlinear regression with autocorrelated errors. Journal of
the American Statistical Association, 71, 961–967.
Garthwaite, P. H. (1994) An interpretation of partial least squares. Journal of the American
Statistical Association, 89, 122–127.
Geary, R. C. (1948) Studies in relations between economic time series. Journal of the Royal
Statistical Society: Series B, 10, 140–158.
Geng, H., Iqbal, J., Chan, W. C. and Ali, H. H. (2011) Virtual CGH: an integrative approach
to predict genetic abnormalities from gene expression microarray data applied in lymphoma.
BMC Medical Genomics, 4, 32.
Genton, M. C. (2007) Separable approximation of space-time covariance matrices. Environmetrics,
18, 681–694.
Geweke, J. (1996) Bayesian reduced rank regression in econometrics. Journal of Econometrics,
75, 121–146.
Geweke, J. and Zhou, G. (1996) Measuring the pricing error of the arbitrage pricing theory. Review
of Financial Studies, 9, 553–583.
Gibbons, M. R. (1982) Multivariate tests of financial models: A new approach. Journal of Financial
Economics, 10, 3–27.
Gibbons, M. R. and Ferson, W. (1985) Testing asset pricing models with changing expectations
and an unobservable market portfolio. Journal of Financial Economics, 14, 217–236.
Gibbons, M. R., Ross, S. A. and Shanken, J. (1989) A test of the efficiency of a given portfolio.
Econometrica, 57, 1121–1152.
Giles, D. and Hampton, P. (1984) Regional production relationships during the industrialization of
New Zealand, 1935–1948. Journal of Regional Science, 24, 519–533.
Gillard, J. and Usevich, K. (2018) Structured low-rank matrix completion for forecasting in time
series analysis. International Journal of Forecasting, 34, 582–597.
Glasbey, C. A. (1992) A reduced rank regression model for local variation in solar radiation.
Applied Statistics, 41, 381–387.
Gleser, L. J. (1981) Estimation in a multivariate “errors in variables” regression model: Large
sample results. The Annals of Statistics, 9, 24–44.
Gleser, L. J. and Olkin, I. (1966) A k-sample regression model with covariance. In Multivariate
Analysis, 59–72. New York: Academic Press.
Gleser, L. J. and Olkin, I. (1970) Linear models in multivariate analysis. In Essays in Probability
and Statistics (ed. R. C. Bose), 267–292. New York: Wiley.
Gleser, L. J. and Watson, G. S. (1973) Estimation of a linear transformation. Biometrika, 60,
525–534.
Goh, G., Dey, D. K. and Chen, K. (2017) Bayesian sparse reduced rank multivariate regression.
Journal of Multivariate Analysis, 157, 14–28.
Goldberger, A. S. (1972) Maximum likelihood estimation of regressions containing unobservable
independent variables. International Economic Review, 13, 1–15.
Golub, G. H. and Van Loan, C. F. (1996) Matrix Computations (3rd Ed.). Baltimore, MD, USA:
Johns Hopkins University Press.
Golub, G. H., Heath, M. and Wahba, G. (1979) Generalized cross-validation as a method for
choosing a good ridge parameter. Technometrics, 21, 215–223.
Gonzalo, J. and Granger, C. (1995) Estimation of common long-memory components in cointe￾grated systems. Journal of Business and Economic Statistics, 13, 27–35.
Gospondinov, N., Kan, R. and Robotti, C. (2019) Too good to be true? Fallacies in evaluating risk
factor models. Journal of Financial Economics, 132, 451–471.
Gower, J. C. and Dijksterhuis, G. (2004) Procrustes Problems. Oxford Statistical Science Series.
Oxford: Oxford University Press.References 391
Grizzle, J. E. and Allen, D. M. (1969) Analysis of growth and dose response curves. Biometrics,
25, 357–381.
Gudmundsson, G. (1977) Multivariate analysis of economic variables. Applied Statistics, 26, 48–
59.
Guggenberger, P., Kleibergen, F. and Mavroeidis, S. (2022) A test for kronecker product structure
covariance matrix. To appear in Journal of Econometrics.
Gustin, M. C., Albertyn, J., Alexander, M. and Davenport, K. (1998) MAP kinase pathways in
the yeast saccharomyces cerevisiae. Microbiology and Molecular Biology Reviews, 62, 1264–
1300.
Haitovsky, Y. (1987) On multivariate ridge regression. Biometrika, 74, 563–570.
Hammar, A. and Ardal, G. (2009) Cognitive functioning in major depression—a summary.
Frontiers in Human Neuroscience, 3, 26.
Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J. and Stahel, W. A. (2005) Robust Statistics–The
Approach Based on Influence Functions. New York: Wiley.
Hannan, E. J. and Kavalieris, L. (1984) A method for autoregressive-moving average estimation.
Biometrika, 71, 273–280.
Hansen, L. P. (1982) Large sample properties of the generalized method of moment estimators.
Econometrica, 50, 1029–1054.
Hansen, N. R. (2018) On Stein’s unbiased risk estimate for reduced rank estimators. Statistics &
Probability Letters, 135, 76–82.
Hansen, L. P. and Hodrick, R. (1983) Risk averse speculation in the forward foreign exchange
market: An econometric analysis of linear models. In Exchange Rates and International
Macroeconomics (ed. J. Frenkel), 113–152. Chicago: University of Chicago Press.
Hastie, T. J. and Tibshirani, R. (1990) Generalized Additive Models. Chapman and Hall, London.
Hastie, T. J. and Tibshirani, R. (1996) Discriminant analysis by Gaussian mixtures. Journal of the
Royal Statistical Society: Series B, 58, 155–176.
Hastie, T. J., Tibshirani, R. and Friedman, J. H. (2009) The Elements of Statistical Learning: Data
Mining, Inference, and Prediction. Springer, New York.
Hastie, T. J., Mazumder, R., Lee, J. D. and Zadeh, R. (2015) Matrix completion and low-rank SVD
via fast alternating least squares. Journal of Machine Learning Research, 16, 3367–3402.
Hatanaka, M. (1976) Several efficient two step estimators for the dynamic simultaneous equations
model with autoregressive disturbances. Journal of Econometrics, 4, 189–204.
Hawkins, D. M. (1976) The subset problem in multivariate analysis of variance. Journal of the
Royal Statistical Society: Series B 38, 132–139.
He, L., Chen, K., Xu, W., Zhou, J. and Wang, F. (2018) Boosted sparse and low-rank tensor
regression. In Advances in Neural Information Processing Systems (NeurIPS) 31, 1009–1018.
Curran Associates, Inc.
He, A., Huang, D., Li, J. and Zhou, G. (2022) Shrinking factor dimension: A reduced-rank
approach. To appear in Management Science.
Healy, J. D. (1980) Maximum likelihood estimation of a multivariate linear functional relationship.
Journal of Multivariate Analysis, 10, 243–251.
Heck, D. L. (1960) Charts of some upper percentage points of the distribution of the largest
characteristic root. Annals of Mathematical Statistics, 31, 625–642.
Helland, I. S. (1988) On the structure of partial least squares regression. Communications in
Statistics, Simulation and Computation, B 17, 581–607.
Helland, I. S. (1990) Partial least squares regression and statistical models. Scandinavian Journal
of Statistics, 17, 97–114.
Hendry, D. F. (1971) Maximum likelihood estimation of systems of simultaneous regression
equations with errors generated by a vector autoregressive process. International Economic
Review, 12, 257–272.
Hocking, R. R. and Leslie, R. N. (1967) Selection of the best subset in regression analysis.
Technometrics, 9, 531–540.
Hoerl, A. E. and Kennard, R. W. (1970) Ridge regression: Biased estimation for nonorthogonal
problems. Technometrics, 12, 55–67.392 References
Honda, T. (1991) Minimax estimators in the MANOVA model for arbitrary quadratic loss and
unknown covariance matrix. Journal of Multivariate Analysis, 36, 113–120.
Hoskuldsson, P. (1988) PLS regression methods. Journal of Chemometrics, 2, 211–228.
Hotelling, H. (1933) Analysis of a complex of statistical variables into principal components.
Journal of Education Psychology, 24, 417–441.
Hotelling, H. (1935) The most predictable criterion. Journal of Education Psychology, 26, 139–
142.
Hotelling, H. (1936) Relations between two sets of variates. Biometrika, 28, 321–377.
Hsu, P. L. (1941) On the limit distribution of roots of a determinantal equation. Journal of London
Mathematical Society, 16, 183–194.
Huang, J., Horowitz, J. L. and Ma, S. (2008) Asymptotic properties of bridge estimators in sparse
high-dimensional regression models. The Annals of Statistics, 36, 587–613.
Huang, J., Breheny, P. and Ma, S. (2012) A selective review of group selection in high dimensional
models. Statistical Science, 27, 481–499.
Huber, P. (1981) Robust Statistics. New York: John Wiley and Sons.
Hunter, D. R. and Lange, K. (2000) Quantile regression via an MM algorithm. Journal of
Computational and Graphical Statistics, 9, 60–77.
Hwang, H. and Takane, Y. (2004) A multivariate reduced-rank growth curve model with unbal￾anced data. Psychometrika, 69, 65–79.
Izenman, A. J. (1975) Reduced-rank regression for the multivariate linear model. Journal of
Multivariate Analysis, 5, 248–264.
Izenman, A. J. (1980) Assessing dimensionality in multivariate regression. In Handbook of
Statistics, 571–592. P.R. Krishnaiah, New York: North Holland.
Izenman, A. J. (2008) Modern Multivariate Statistical Techniques: Regression, Classification and
Manifold Learning. Springer, New York.
Jöreskog, K. G. (1967) Some contributions to maximum likelihood factor analysis. Psychometrika,
32, 443–482.
Jöreskog, K. G. (1969) A general approach to confirmatory maximum likelihood factor analysis.
Psychometrika, 34, 183–202.
Jöreskog, K. G. and Goldberger, A. S. (1975) Estimation of a model with multiple indicators and
multiple causes of a single latent variable. Journal of the American Statistical Association, 70,
631–639.
Jenkins, G. M. and Alavi, A. S. (1981) Some aspects of modeling and forecasting multivariate time
series. Journal of Time Series Analysis, 2, 1–47.
Jennrich, R. I. and Schluchter, M. D. (1986) Unbalanced repeated measures models with structural
covariance matrices. Biometrics, 42, 805–820.
Johansen, S. (1988) Statistical analysis of cointegration vectors. Journal of Economic Dynamics
and Control, 12, 231–254.
Johansen, S. (1991) Estimation and hypothesis testing of cointegration vectors in Gaussian vector
autoregressive models. Econometrica, 59, 1551–1580.
Johansen, S. and Juselius, K. (1990) Maximum likelihood estimation and inference on
cointegration–with applications to the demand for money. Oxford Bulletin of Economics and
Statistics, 52, 169–210.
Johnson, R. A. and Wichern, D. W. (2008) Applied Multivariate Statistical Analysis. Pearson, 6
edn.
Johnstone, I. M. and Lu, A. Y. (2009) On consistency and sparsity for principal components
analysis in high dimensions. Journal of the American Statistical Association, 104, 682–693.
Jones, R. H. (1986) Random effects and the Kalman filter. Proceedings of the Business and
Economic Statistics Section, American Statistical Association, 69–75.
Jones, R. H. (1990) Serial correlation or random subject effects? Communications in Statistics,
Simulation and Computation, 19, 1105–1123.
Jorgensen, B. (1987) Exponential dispersion models. Journal of the Royal Statistical Society:
Series B, 49, 127–162.References 393
Kabe, D. G. (1975) Some results for the GMANOVA model. Communications in Statistics, 4,
813–820.
Kanehisa, M., Goto, S., Furumichi, M., Tanabe, M. and Hirakawa, M. (2009) KEGG for
representation and analysis of molecular networks involving diseases and drugs. Nucleic Acids
Research, 38, D355–D360.
Keller, W. J. and Wansbeek, T. (1983) Multivariate methods for quantitative and qualitative data.
Journal of Econometrics, 22, 91–111.
Kelly, B. T., Pruitt, S. and Su, Y. (2019) Characteristics are covariances: A unified model of risk
and return. Journal of Financial Economics, 134, 501–524.
Khatri, C. G. (1966) A note on a MANOVA model applied to problems in growth curve. Annals of
the Institute of Statistical Mathematics, 18, 75–86.
Kim, S., Sohn, K.-A. A. and Xing, E. P. (2009) A multivariate regression approach to association
analysis of a quantitative trait network. Bioinformatics, 25, i204–i212.
Kleibergen, F. and Paap, R. (2006) Generalized reduced rank tests using the singular value
decomposition. Journal of Econometrics, 133, 97–126.
Klein, L. R., Ball, R. J., Hazlewood, A. and Vandome, P. (1961) An Econometric Model of the
United Kingdom. Oxford: Blackwell.
Kohn, R. (1979) Asymptotic estimation and hypothesis testing results for vector linear time series
models. Econometrica, 47, 1005–1030.
Koltchinskii, V., Lounici, K. and Tsybakov, A. B. (2011) Nuclear norm penalization and optimal
rates for noisy low rank matrix completion. The Annals of Statistics, 39, 2302–2329.
Konno, Y. (1991) On estimation of a matrix of normal means with unknown covariance matrix.
Journal of Multivariate Analysis, 36, 44–55.
Kshirsagar, A. M. and Smith, W. B. (1995) Growth Curves. New York: Marcel Dekker.
Lütkepohl, H. (1993) Introduction to Multiple Time Series Analysis. New York: Springer Verlag.
Laird, N. M. and Ware, J. H. (1982) Random-effects models for longitudinal data. Biometrics, 38,
963–974.
Lawley, D. N. (1940) The estimation of factor loadings by the method of maximum likelihood.
Proceedings of the Royal Society, A, 60, 64–82.
Lawley, D. N. (1943) The application of the maximum likelihood method to factor analysis. British
Journal of Psychology, 33, 172–175.
Lawley, D. N. (1953) A modified method of estimation in factor analysis and some large sample
results. In Uppsala Symposium on Psychological Factor Analysis, 35–42. Stockholm: Almqvist
and Wicksell.
Lee, Y. S. (1972) Some results on the distribution of Wilks’ likelihood-ratio criterion. Biometrika,
59, 649–664.
Lee, M., Shen, H., Huang, J. Z. and Marron, J. S. (2010) Biclustering via sparse singular value
decomposition. Biometrics, 66, 1087–1095.
Lee, Y., MacEachern, S. N. and Jung, Y. (2012) Regularization of case-specific parameters for
robustness and efficiency. Statistical Science, 27, 350–372.
Li, Y. and Zhu, J. (2008) l1-norm quantile regression. Journal of Computational and Graphical
Statistics, 17, 163–185.
Li, Y., Nan, B. and Zhu, J. (2015) Multivariate sparse group Lasso for the multivariate multiple
linear regression with an arbitrary group structure. Biometrics, 71, 354–363.
Li, G., Liu, X. and Chen, K. (2019) Integrative multi-view regression: Bridging group-sparse and
low-rank models. Biometrics, 75, 593–602.
Liang, K.-Y. and Zeger, S. L. (1986) Longitudinal data analysis using generalized linear models.
Biometrika, 73, 13–22.
Lintner, J. (1965) The valuation of risk assets and the selection of risky investment in stock
portfolios and capital budgets. Review of Economics and Statistics, 47, 13–37.
Lounici, K., Pontil, M., van de Geer, S. and Tsybakov, A. B. (2011) Oracle inequalities and optimal
inference under group sparsity. The Annals of Statistics, 39, 2164–2204.394 References
Lu, Z., Monteiro, R. D. C. and Yuan, M. (2012) Convex optimization methods for dimension reduc￾tion and coefficient estimation in multivariate linear regression. Mathematical Programming,
131, 163–194.
Luca, P., Velu, R., Wang, L. and Zhou, Z. (2022) Efficient modeling of cross-sectional returns via
reduced rank seemingly unrelated regressions SSRN 4083935
Luo, S. and Chen, Z. (2020) Feature selection by canonical correlation search in high-dimensional
multiresponse models with complex group structures. Journal of the American Statistical
Association, 115, 1227–1235.
Luo, C., Liang, J., Li, G., Wang, F., Zhang, C., Dey, D. K. and Chen, K. (2018) Leveraging mixed
and incomplete outcomes via reduced-rank modeling. Journal of Multivariate Analysis, 167,
378–394.
Lyttkens, E. (1972) Regression aspects of canonical correlation. Journal of Multivariate Analysis,
2, 418–439.
Ma, S., Goldfarb, D. and Chen, L. (2011) Fixed point and Bregman iterative methods for matrix
rank minimization. Mathematical Programming, 128, 321–353.
Ma, X., Xiao, L. and Wong, W. H. (2014) Learning regulatory programs by threshold SVD
regression. Proceedings of the National Academy of Sciences of the United States of America,
111, 15675–15680.
Ma, Z., Ma, Z. and Sun, T. (2020) Adaptive estimation in two-way sparse reduced-rank regression.
Statistica Sinica, 30, 2179–2201.
Magnus, J. R. and Neudecker, H. (1998) Matrix Differential Calculus with Applications in
Statistics and Econometrics. Wiley, New York.
Mallows, C. L. (1973) Some comments on Cp. Technometrics, 15, 661–675.
Markowitz, H. (1959) Portfolio Selection: Efficient Diversification of Investments. New York:
Wiley.
Marshall, A. S. and Olkin, I. (1979) Inequalities: Theory of Majorization and Its Applications.
New York: Academic Press.
Mazumder, R. and Weng, H. (2020) Computing the degrees of freedom of rank-regularized
estimators and cousins. Electronic Journal of Statistics, 14, 1348–1385.
Mazumder, R., Hastie, T. and Tibshirani, R. (2010) Spectral regularization algorithms for learning
large incomplete matrices. Journal of Machine Learning Research, 11, 2287–2322.
McCulloch, C. (2008) Joint modelling of mixed outcome types using latent variables. Statistical
Methods in Medical Research, 17, 53–73.
McDonald, G. C. and Galarneau, D. I. (1975) A Monte Carlo evaluation of some ridge-type
estimators. Journal of the American Statistical Association, 70, 407–416.
McElroy, M. B. and Burmeister, E. (1988) Arbitrage pricing theory as a restricted nonlinear
multivariate regression model: Iterated nonlinear seemingly unrelated regression models.
Journal of Business and Economic Statistics, 6, 29–42.
Merton, R. T. (1973) An intertemporal capital asset pricing model. Econometrica, 41, 867–887.
Meyer, M. and Woodroofe, M. (2000) On the degrees of freedom in shape-restricted regression.
The Annals of Statistics, 28, 1083–1104.
Mirsky, L. (1975) A trace inequality of John von Neumann. Monatschefte fur Mathematik, 79,
303–306.
Mishra, A., Dey, D. K. and Chen, K. (2017) Sequential co-sparse factor regression. Journal of
Computational and Graphical Statistics, 26, 814–825.
Moore, E. H. (1920) On the reciprocal of the general algebraic matrix. Bulletin of the American
Mathematical Society, 26, 394–395.
Moran, P. A. P. (1971) Estimating structural and functional relationships. Journal of Multivariate
Analysis, 1, 232–255.
Muirhead, R. J. (1982) Aspects of Multivariate Statistical Theory. New York: Wiley.
Mukherjee, A., Chen, K., Wang, N. and Zhu, J. (2015) On the degrees of freedom of reduced-rank
estimators in multivariate regression. Biometrika, 102, 457–477.
Negahban, S. and Wainwright, M. J. (2011) Estimation of (near) low-rank matrices with noise and
high-dimensional scaling. The Annals of Statistics, 39, 1069–1097.References 395
Negahban, S. N., Ravikumar, P., Wainwright, M. J. and Yu, B. (2012) A unified framework for
high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science,
27, 538–557.
Neudecker, H. (1969) Some theorems on matrix differentiation with special reference to Kronecker
matrix products. Journal of the American Statistical Association, 64, 953–963.
Neuenschwander, B. E. and Flury, B. D. (1997) A note on Silvey’s (1959) Theorem. Statistics and
Probability Letters, 36, 307–317.
Niu, Y. S., Hao, N. and Dong, B. (2018) A new reduced-rank linear discriminant analysis method
and its applications. Statistica Sinica, 28, 189–202.
O’Neil, K. (2005) Critical points of the singular value decomposition. SIAM Journal of Matrix
Analysis and Applications, 27, 459–473.
Obozinski, G., Wainwright, M. J. and Jordan, M. I. (2011) Support union recovery in high￾dimensional multivariate regression. The Annals of Statistics, 39, 1–47.
Parzen, E. M. (1969) Multiple time series modeling. In Multivariate Analysis, 389–409. P.R.
Krishnaiah, New York: Academic Press.
Peng, J., Zhu, J., Bergamaschi, A., Han, W., Noh, D.-Y., Pollack, J. R. and Wang, P. (2010)
Regularized multivariate regression for identifying master predictors with application to
integrative genomics study of breast cancer. The Annals of Applied Statistics, 4, 53.
Penrose, R. (1955) A generalized inverse for matrices. Proceedings of the Cambridge Philosophi￾cal Society, 51, 406–413.
Phillips, P. and Ouliaris, S. (1986) Testing for cointegration. Discussion Paper No. 809. Yale
University: Cowles Foundation for Research in Economics. https://www.sciencedirect.com/
science/article/abs/pii/0165188988900401#!
Pillai, K. C. S. (1967) Upper percentage points of the largest root of a matrix in multivariate
analysis. Biometrika, 54, 189–194.
Pillai, K. C. S. and Gupta, A. K. (1969) On the exact distribution of Wilks’ criterion. Biometrika,
56, 109–118.
Pollack, J. R., Sørlie, T., Perou, C. M., Rees, C. A., Jeffrey, S. S., Lonning, P. E., Tibshirani, R.,
Botstein, D., Børresen-Dale, A.-L. L. and Brown, P. O. (2002) Microarray analysis reveals a
major direct role of DNA copy number alteration in the transcriptional program of human breast
tumors. Proceedings of the National Academy of Sciences of the United States of America, 99,
12963–12968.
Potthoff, R. F. and Roy, S. N. (1964) A generalized multivariate analysis of variance model useful
especially for growth curve problems. Biometrika, 51, 313–326.
Prentice, R. L. and Zhao, L. P. (1991) Estimating equations for parameters in means and
covariances of multivariate discrete and continuous responses. Biometrics, 825–839.
Priestley, M. B., Rao, T. S. and Tong, H. (1974a) Applications of principal component analysis and
factor analysis in the identification of multivariable systems. IEEE Transactions on Automatic
Control, 19, 730–734.
Priestley, M. B., Rao, T. S. and Tong, H. (1974b) Identification of the structure of multivariable
stochastic systems. In Multivariate Analysis, 351–368. P.R. Krishnaiah, New York: Academic
Press.
Quenouille, M. H. (1968) The Analysis of Multiple Time Series. London: Griffin.
Rao, C. R. (1955) Estimation and tests of significance in factor analysis. Psychometrika, 20, 93–
111.
Rao, C. R. (1964) The use and interpretation of principal component analysis in applied research.
Sankhya: The Indian Journal of Statistics, Series A ¯ , 26, 319–358.
Rao, C. R. (1965) The theory of least squares when the parameters are stochastic and its application
to the analysis of growth curves. Biometrika, 52, 447–458.
Rao, C. R. (1966) Covariance adjustment and related problems in multivariate analysis. In
Multivariate Analysis, 321–328. P. R. Krishnaiah, New York: Academic Press.
Rao, C. R. (1967) Least squares theory using an estimated dispersion matrix and its application
to measurement of signals. In Proceedings of the 5th Berkeley Symposium on Mathematical
Statistics and Probability, 355–372. J. Neyman, Berkeley: University of California Press.396 References
Rao, C. R. (1973) Linear Statistical Inference and Its Applications. New York: Wiley, second edn.
Rao, C. R. (1979) Separation theorems for singular values of matrices and their applications in
multivariate analysis. Journal of Multivariate Analysis, 9, 362–377.
Recht, B. (2011) A simpler approach to matrix completion. Journal of Machine Learning Research,
12, 3413–3430.
Recht, B., Xu, W. and Hassibi, B. (2011) Null space conditions and thresholds for rank
minimization. Mathematical Programming, 127, 175–202.
Reiersøl, O. (1950) On the identifiability of parameters in Thurstone’s multiple factor analysis.
Psychometrika, 15, 121–149.
Reinsel, G. C. (1979) FIML estimation of the dynamic simultaneous equations model with ARMA
disturbances. Journal of Econometrics, 9, 263–281.
Reinsel, G. C. (1983) Some results on multivariate autoregressive index models. Biometrika, 70,
145–156.
Reinsel, G. C. (1985) Mean squared error properties of empirical Bayes estimators in a multivariate
random effects general linear model. Journal of the American Statistical Association, 80, 642–
650.
Reinsel, G. C. (1997) Elements of Multivariate Time Series Analysis. New York: Springer-Verlag,
second edn.
Reinsel, G. C. (1999) On multivariate linear regression shrinkage and reduced-rank procedures.
Journal of Statistical Planning and Inference, 81, 311–321.
Reinsel, G. C. and Ahn, S. K. (1992) Vector autoregressive models with unit roots and reduced rank
structure: Estimation, likelihood ratio test, and forecasting. Journal of Time Series Analysis,
13, 353–375.
Reinsel, G. C. and Velu, R. P. (2003) Reduced-rank growth curve models. Journal of Statistical
Planning and Inference, 114, 107–129.
Reinsel, G. C., Tiao, G. C., Wang, M. N., Lewis, R. and Nychka, D. (1981) Statistical analysis of
stratospheric ozone data for the detection of trends. Atmospheric Environment, 15, 1569–1577.
Revankar, N. S. (1974) Some finite sample results in the context of two seemingly unrelated
regression equations. Journal of the American Statistical Association, 69, 187–190.
Robinson, P. M. (1972) Non-linear regression for multiple time-series. Journal of Applied
Probability, 9, 758–768.
Robinson, P. M. (1973) Generalized canonical analysis for time series. Journal of Multivariate
Analysis, 3, 141–160.
Robinson, P. M. (1974) Identification, estimation and large-sample theory for regressions contain￾ing unobservable variables. International Economic Review, 15, 680–692.
Rochon, J. and Helms, R. W. (1989) Maximum likelihood estimation for incomplete repeated￾measures experiments under an ARMA covariance structure. Biometrics, 45, 207–218.
Rohde, A. and Tsybakov, A. (2011) Estimation of high-dimensional low-rank matrices. The Annals
of Statistics, 39, 887–930.
Ross, S. A. (1976) The arbitrage theory of capital asset pricing. Journal of Economic Theory, 13,
341–360.
Rothman, A. J., Levina, E. and Zhu, J. (2010) Sparse multivariate regression with covariance
estimation. Journal of Computational and Graphical Statistics, 19, 947–962.
Roy, S. N. (1953) On a heuristic method of test construction and its use in multivariate analysis.
Annals of Mathematical Statistics, 24, 220–238.
Roy, S. N., Gnanadesikan, R. and Srivastava, J. N. (1971) Analysis and Design of Certain
Quantitative Multiresponse Experiments. New York: Pergamon Press.
Ryan, D. A. J., Hubert, J. J., Carter, E. M., Sprague, J. B. and Parrott, J. (1992) A reduced-rank
multivariate regression approach to aquatic joint toxicity experiments. Biometrics, 48, 155–
162.
Sammel, M. D., Ryan, L. M. and Legler, J. M. (1997) Latent variable models for mixed discrete
and continuous outcomes. Journal of the Royal Statistical Society: Series B, 59, 667–678.References 397
Sargent, T. J. and Sims, C. A. (1977) Business cycle modeling without pretending to have too much
of a prior economic theory. In New Methods in Business Cycle Research, 45–110. Federal
Reserve Bank of Minneapolis, MN: C.A. Sims.
Schatzoff, M. (1966) Exact distribution of Wilks’ likelihood ratio criterion. Biometrika, 53, 347–
358.
Schott, J. R. (1997) Matrix Analysis for Statistics. New York: Wiley.
Schwarz, G. (1978) Estimating the dimension of a model. The Annals of Statistics, 6, 461–464.
Sclove, S. L. (1971) Improved estimation of parameters in multivariate regression. Sankhya: The ¯
Indian Journal of Statistics, 33, 61–66.
Searle, S. R. (1971) Linear Models. Wiley, New York.
Seber, G. A. F. (1984) Multivariate Observations. Wiley, New York.
Shanken, J. (1986) Testing portfolio efficiency when the zero-beta rate is unknown: A note. Journal
of Finance, 41, 269–276.
Shanken, J. (1992) On the estimation of beta-pricing models. Review of Financial Studies, 5, 1–33.
Shanken, J. and Zhou, G. (2007) Estimating and testing beta pricing models: alternative methods
and their performance in simulations. Journal of Financial Economics, 84, 40–86.
Sharpe, W. F. (1964) Capital asset prices: A theory of market equilibrium under conditions of risk.
Journal of Finance, 19, 425–442.
She, Y. (2009) Thresholding-based iterative selection procedures for model selection and shrink￾age. Electron. J. Statist., 3, 384–415.
She, Y. (2013) Reduced rank vector generalized linear models for feature extraction. Statistics and
Its Interface, 6, 197–209.
She, Y. (2017) Selective factor extraction in high dimensions. Biometrika, 104, 97–110.
She, Y. and Chen, K. (2017) Robust reduced-rank regression. Biometrika, 104, 633–647.
She, Y. and Owen, A. B. (2011) Outlier detection using nonconvex penalized regression. Journal
of the American Statistical Association, 106, 626–639.
She, Y. and Tran, H. (2019) On cross-validation for sparse reduced rank regression. Journal of the
Royal Statistical Society: Series B, 81, 145–161.
Shen, X. and Ye, J. (2002) Adaptive model selection. Journal of the American Statistical
Association, 97, 210–221.
Silvey, S. D. (1959) The Lagrangian multiplier test. Annals of Mathematical Statistics, 30, 389–
407.
Sims, C. A. (1981) An autoregressive index model for the U.S., 1948–1975. In Large-Scale Macro￾Econometric Models (eds. J. Kmenta and J. B. Ramsey), 283–327. Amsterdam: North Holland.
Skagerberg, B., MacGregor, J. and Kiparissides, C. (1992) Multivariate data analysis applied to
low-density polyethylene reactors. Chemometrics and Intelligent Laboratory Systems, 14, 341–
356.
Smith, H., Gnanadesikan, R. and Hughes, J. B. (1962) Multivariate analysis of variance
(MANOVA). Biometrics, 18, 22–41.
Sprent, P. (1966) A generalized least-squares approach to linear functional relationships (with
discussion). Journal of the Royal Statistical Society: Series B, 28, 278–297.
Srivastava, J. N. (1967) On the extension of Gauss–Markov theorem to complex multivariate linear
models. Annals of the Institute of Statistical Mathematics, 19, 417–437.
Srivastava, M. S. (1997) Reduced rank discrimination. Scandinavian Journal of Statistics, 24,
115–124.
Srivastava, V. K. and Giles, D. (1987) Seemingly Unrelated Regression Equations Models. New
York: Marcel Dekker.
Srivastava, M. S. and Khatri, C. G. (1979) An Introduction to Multivariate Statistics. New York:
North Holland.
Stambaugh, R. (1988) The information in forward rates: Implications for models of the term
structure. Journal of Financial Economics, 21, 41–70.
Stanek III, E. J. and Koch, G. G. (1985) The equivalence of parameter estimates from growth curve
models and seemingly unrelated regression models. The American Statistician, 39, 149–152.398 References
Stanziano, D. C., Whitehurst, M., Graham, P. and Roos, B. A. (2010) A review of selected
longitudinal studies on aging: Past findings and future directions. Journal of the American
Geriatrics Society, 58, 292–297.
Stein, C. M. (1981) Estimation of the mean of a multivariate normal distribution. The Annals of
Statistics, 9, 1135–1151.
Stock, J. H. and Watson, M. W. (1988) Testing for common trends. Journal of the American
Statistical Association, 83, 1097–1107.
Stoica, P. and Viberg, M. (1996) Maximum likelihood parameter and rank estimation in reduced￾rank multivariate linear regressions. IEEE Trans. Signal Processing, 44, 3069–3078.
Stone, M. (1974) Cross-validation and multinomial prediction. Biometrika, 61, 509–515.
Stone, M. and Brooks, R. J. (1990) Continuum regression: Cross-validated sequentially constructed
prediction embracing ordinary least squares, partial least squares and principal components
regression (with discussion). Journal of the Royal Statistical Society: Series B, 52, 237–269.
Storey, J. D., Akey, J. M. and Kruglyak, L. (2005) Multiple locus linkage analysis of genomewide
expression in yeast. PLoS Biology, 3, e267.
Sundberg, R. (1993) Continuum regression and ridge regression. Journal of the Royal Statistical
Society: Series B, 55, 653–659.
Takane, Y. and Jung, S. (2008) Regularized partial and/or constrained redundancy analysis.
Psychometrika, 73, 671–690.
Takane, Y., Kiers, H. A. and de Leeuw, J. (1995) Component analysis with different sets of
constraints on different dimensions. Psychometrika, 60, 259–280.
Tan, K. M., Sun, Q. and Witten, D. (2022) Sparse reduced rank Huber regression in high
dimensions. Journal of the American Statistical Association. In press.
ter Braak, C. J. F. (1990) Interpreting canonical correlation analysis through biplots of structure
correlations and weights. Psychometrika, 55, 519–531.
Theobald, C. M. (1975) An inequality with application to multivariate analysis. Biometrika, 62,
461–466.
Theobald, C. M. (1978) Letter to the editors. Applied Statistics, 27, 79.
Thurstone, L. L. (1947) Multiple-Factor Analysis. Chicago: University of Chicago Press.
Tiao, G. C. and Box, G. E. P. (1981) Modeling multiple time series with applications. Journal of
the American Statistical Association, 76, 802–816.
Tiao, G. C. and Tsay, R. S. (1983) Multiple time series modeling and extended sample cross￾correlations. Journal of Business and Economic Statistics, 1, 43–56.
Tiao, G. C. and Tsay, R. S. (1989) Model specification in multivariate time series (with discussion).
Journal of the Royal Statistical Society: Series B, 51, 157–213.
Tibshirani, R. (1996) Regression shrinkage and selection via the Lasso. Journal of the Royal
Statistical Society: Series B, 58, 267–288.
Tibshirani, R. J. and Taylor, J. (2012) Degrees of freedom in Lasso problems. The Annals of
Statistics, 40, 1198–1232.
Tikhonov, A. N. (1943) On the stability of inverse problems. Doklady Akademii Nauk SSSR, 39,
195–198.
Tintner, G. (1945) A note on rank, multicollinearity and multiple regression. Annals of
Mathematical Statistics, 16, 304–308.
Tintner, G. (1950) A test for linear relations between weighted regression coefficients. Journal of
the Royal Statistical Society: Series B, 12, 273–277.
Toh, K.-C. and Yun, S. (2010) An accelerated proximal gradient algorithm for nuclear norm
regularized least squares problems. Pacific Journal of Optimization, 6, 615–640.
Tsay, R. S. (2010) Analysis of financial time series, Third Edition, John wiley & sons.
Tseng, P. (2001) Convergence of a block coordinate descent method for nondifferentiable
minimization. Journal of Optimization Theory and Applications, 109, 475–494.
Tso, M. K.-S. (1981) Reduced-rank regression and canonical analysis. Journal of the Royal
Statistical Society: Series B, 43, 183–189.
Turlach, B. A., Venables, W. N. and Wright, S. J. (2005) Simultaneous variable selection.
Technometrics, 47, 349–363.References 399
Udell, M., Horn, C., Zadeh, R. and Boyd, S. (2016) Generalized low rank models. Foundations
Trends Machine Learning, 9, 1–118.
Uematsu, Y., Fan, Y., Chen, K., Lv, J. and Lin, W. (2019) SOFAR: Large-scale association network
learning. IEEE Transactions on Information Theory, 65, 4924–4939.
Vandenberghe, L. and Boyd, S. (1996) Semidefinite programming. SIAM Review, 38, 49–95.
van den Wollenberg, A. L. (1977) Redundancy analysis an alternative for canonical correlation
analysis. Psychometrika, 42, 207–219.
van der Leeden, R. (1990) Reduced Rank Regression With Structured Residuals. Leiden: DSWO
Press.
van der Merwe, A. and Zidek, J. V. (1980) Multivariate regression analysis and canonical variates.
Canadian Journal of Statistics, 8, 27–39.
Van Loan, C. and Pitsianis, N. P. (1993) Approximation with Kronecker products, in: M.S. Moonen.
G.H. Golub (Eds.), Linear Algebra for Large Scale and Real Time Applications. Kluwer
Publications. Dordrecht.
Velu, R. P. (1991) Reduced rank models with two sets of regressors. Journal of the Royal Statistical
Society: Series C, 40, 159–170.
Velu, R. and Herman, K. (2017) Separable covariance matrices and Kronecker approximation.
Procedia Computer Science, 108, 1019–1029.
Velu, R. P. and Reinsel, G. C. (1987) Reduced rank regression with autoregressive errors. Journal
of Econometrics, 35, 317–335.
Velu, R. and Richards, J. (2008) Seemingly unrelated reduced-rank regression model. Journal of
Statistical Planning and Inference, 138, 2837–2846.
Velu, R. P. and Zhou, G. (1999) Testing multi-beta asset pricing models. Journal of Empirical
Finance, 6, 219–241.
Velu, R. P., Reinsel, G. C. and Wichern, D. W. (1986) Reduced rank models for multiple time
series. Biometrika, 73, 105–118.
Velu, R. P., Wichern, D. W. and Reinsel, G. C. (1987) A note on nonstationarity and canonical
analysis of multiple time series models. Journal of Time Series Analysis, 8, 479–487.
Velu, R., Hardy, M. and Nehren, D. (2020) Algorithmic Trading and Quantitative Strategies.
Chapman and Hall/CRC.
Verbyla, A. P. and Venables, W. N. (1988) An extension of the growth curve model. Biometrika,
75, 129–138.
Villegas, C. (1961) Maximum likelihood estimation of a linear functional relationship. Annals of
Mathematical Statistics, 32, 1048–1062.
Villegas, C. (1982) Maximum likelihood and least squares estimation in linear and affine functional
models. The Annals of Statistics, 10, 256–265.
Volund, A. (1980) Multivariate bioassay. Biometrics, 36, 225–236.
von Neumann, J. (1937) Some matrix inequalities and metrization of matric-space. Tomsk
University Review, 1, 286–300.
von Rosen, T. and von Rosen, D. (2017) On estimation of some reduced-rank extended growth
curve models. Mathematical Methods of Statistics, 26, 299–310.
Vounou, M., Nichols, T. E. and Montana, G. (2010) Discovering genetic associations with
high-dimensional neuroimaging phenotypes: A sparse reduced-rank regression approach.
NeuroImage, 53, 1147–1159.
Wang, D., Zheng, Y., Lian, H. and Li, G. (2022) High-dimensional vector autoregressive time
series modeling via tensor decomposition. Journal of the American Statistical Association.
117, 1338–1356
Wille, A., Zimmermann, P., Vranova, E., Furholz, A., Laule, O., Bleuler, S., Hennig, L., Prelic, A.,
von Rohr, P., Thiele, L., Zitzler, E., Gruissem, W. and Buhlmann, P. (2004) Sparse graphical
Gaussian modeling of the isoprenoid gene network in Arabidopsis thaliana. Genome Biology,
5, R92.
Witten, D. M., Tibshirani, R. and Hastie, T. J. (2009) A penalized matrix decomposition, with
applications to sparse principal components and canonical correlation analysis. Biostatistics,
10, 515–534.400 References
Wold, H. (1975) Soft modeling by latent variables: The nonlinear iterative partial least squares
approach. In Perspectives in Probability and Statistics: Papers in Honour of M. S. Bartlett (ed.
J. Gani), 117–142. New York: Academic Press.
Wold, H. (1984) PLS regression. In Encyclopedia of Statistical Sciences, Eds. N. L. Johnson and
S. Kotz, vol. 6, 581–591. New York: Wiley.
Wright, J., Ganesh, A., Rao, S., Peng, Y. and Ma, Y. (2009) Robust principal component analysis:
Exact recovery of corrupted low-rank matrices via convex optimization. In Advances in Neural
Information Processing Systems (NeurIPS) 22, 2080–2088. Curran Associates, Inc.
Yang, Y. and Zou, H. (2015) A fast unified algorithm for solving group-lasso penalize learning
problems. Statistics and Computing, 25, 1129–1141.
Ye, J. (1998) On measuring and correcting the effects of data mining and model selection. Journal
of the American Statistical Association, 93, 120–131.
Ye, F. and Zhang, C.-H. (2010) Rate minimaxity of the Lasso and Dantzig selector for the lq loss
in lr balls. Journal of Machine Learning Research, 11, 3519–3540.
Yee, T. W. and Hastie, T. J. (2003) Reduced-rank vector generalized linear models. Statistical
Modelling, 3, 15–41.
Yu, M., Gupta, V. and Kolar, M. (2020) Recovery of simultaneous low rank and two-way sparse
coefficient matrices, a nonconvex approach. Electronic Journal of Statistics, 14, 413–457.
Yuan, M. (2016) Degrees of freedom in low rank matrix estimation. Science China Mathematics,
59, 2485–2502.
Yuan, M. and Lin, Y. (2006) Model selection and estimation in regression with grouped variables.
Journal of the Royal Statistical Society: Series B, 68, 49–67.
Yuan, M., Ekici, A., Lu, Z. and Monteiro, R. (2007) Dimension reduction and coefficient estimation
in multivariate linear regression. Journal of the Royal Statistical Society: Series B, 69, 329–346.
Zellner, A. (1962) An efficient method of estimating seemingly unrelated regressions and tests for
aggregation bias. Journal of the American Statistical Association, 57, 348–368.
Zellner, A. (1963) Estimators for seemingly unrelated regression equations: Some exact finite
sample results. Journal of the American Statistical Association, 58, 977–992.
Zellner, A. (1970) Estimation of regression relationships containing unobservable variables.
International Economic Review, 11, 441–454.
Zhang, T. (2010) Analysis of multi-stage convex relaxation for sparse regularization. Journal of
Machine Learning Research, 11, 1081–1107.
Zhang, C.-H. and Zhang, T. (2012) A general theory of concave regularization for high￾dimensional sparse estimation problems. Statistical Science, 27, 576–593.
Zhao, P. and Yu, B. (2006) On model selection consistency of lasso. Journal of Machine Learning
Research, 7, 2541–2563.
Zhao, P. and Yu, B. (2007) Stagewise Lasso. Journal of Machine Learning Research, 8, 2701–
2726.
Zhao, L. P., Prentice, R. L. and Self, S. G. (1992) Multivariate mean parameter estimation by using
a partly exponential model. Journal of the Royal Statistical Society: Series B, 805–811.
Zheng, Z., Bahadori, M. T., Liu, Y. and Lv, J. (2019) Scalable interpretable multi-response
regression via SEED. Journal of Machine Learning Research, 20, 1–34.
Zhou, G. (1991) Small sample tests of portfolio efficiency. Journal of Financial Economics, 30,
165–191.
Zhou, G. (1993) Asset pricing tests under alternative distributions. Journal of Finance, 48, 1927–
1942.
Zhou, G. (1994) Analytical GMM tests: Asset pricing with time-varying risk premiums. Review of
Financial Studies, 7, 687–709.
Zhou, G. (1995) Small sample rank tests with applications to asset pricing. Journal of Empirical
Finance, 2, 71–93.
Zhou, Y., Zhang, Q., Stephens, O., Heuck, C. J., Tian, E., Sawyer, J. R., Cartron-Mizeracki, M.-
A., Qu, P., Keller, J., Epstein, J., Barlogie, B. and Shaughnessy, J. D. (2012) Prediction of
cytogenetic abnormalities with gene expression profiles. Blood, 119, 148–150.References 401
Zhu, Junxian, Canhong Wen, Jin Zhu, Heping Zhang, and Xueqin Wang. (2020) A Polynomial
Algorithm for Best-Subset Selection Problem. Proceedings of the National Academy of
Sciences, 117(52), 33117–23.
Zhu, H., Khondker, Z., Lu, Z. and Ibrahim, J. G. (2014) Bayesian generalized low rank regression
models for neuroimaging phenotypes and genetic markers. Journal of the American Statistical
Association, 109, 977–990.
Zou, H. (2006) The adaptive Lasso and its oracle properties. Journal of the American Statistical
Association, 101, 1418–1429.
Zou, H., Hastie, T. J. and Tibshirani, R. (2006) Sparse principal component analysis. Journal of
Computational and Graphical Statistics, 15, 265–286.
Zou, H., Hastie, T. J. and Tibshirani, R. (2007) On the “degrees of freedom” of the Lasso. The
Annals of Statistics, 35, 2173–2192.Subject Index
A
Adaptive nuclear norm, 289–298
AIC, see Akaike’s information criterion (AIC)
Akaike’s information criterion (AIC), 57, 133,
156
ANOVA model, 23, 84, 191
discriminant analysis in, 82–85, 202–205
in growth curve, 177, 216–217
reduced-rank estimates in, 83
Arbitrage pricing theory (APT), 239–242
estimation in, 242
reduced rank in, 246
Asset pricing models, 239–248
APT, 239–242
CAPM, 239, 340
empirical studies on, 249–251
Asymptotic distribution
of LR test statistic, 7, 56, 88, 169–170
of LS estimator in cointegrated
autoregressive model, 164–169
of ML estimator in cointegrated
autoregressive model, 164–169
of reduced-rank ML estimator, 45–55, 99
in autoregressive model, 136
in regression model with autoregressive
error, 115–121
in seemingly unrelated regressions
model, 219–222
Autoregressive process, 136
canonical analysis for, 141–142
cointegration in, 169–170
for error term in regression, 113
extended reduced-rank model for, 141
nested reduced-rank model for, 144
nonstationary unit-root, 161, 162
reduced-rank model for, 144
stationarity condition, 157
B
Bayesian information criterion (BIC), 57, 108,
210, 232
BIC, see Bayesian information criterion (BIC)
Breakdown point, 331, 333, 339
Brownian motion process
in asymptotic distributions with
cointegrated processes, 164, 170
C
Canonical correlation analysis, 42–45
for autoregressive process, 141–143
measure of predictability in, 141
canonical variates in, 42, 140–141
in multivariate shrinkage methods, 67–68
relation to reduced-rank regression, 42–45
Canonical correlations, 42
partial, 8, 77–79, 82, 125, 168, 169
use for error-correction AR model, 162,
164
use for specification of rank, 144–145
Capital asset pricing model (CAPM), 239, 240
estimation in, 242–248
multi-beta, 245
reduced rank in, 243
testing for, 242–248
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8
403404 Subject Index
Cointegration, 161
error-correction AR model for, 160
likelihood ratio test for rank of, 169–170
rank of, 168
Common trends
in cointegrated autoregressive process, 162,
168
Composite nuclear norm, 300–301,
304
Coordinate descent, 361, 362, 367
Correlation, 43, 55
canonical, 42
partial, 68
Covariance adjustment
in growth curve model, 182
Covariance matrix
for error term, estimate of, 5
of generalized least squares estimator,
214
of least squares estimator, 5, 6
of ML estimator in growth curve model,
179
of reduced-rank ML estimator, 51, 78, 83,
84, 106, 200
D
Deflation, 363–367, 370, 373
Degrees of freedom, 314–319
Discriminant analysis, 82–85, 202–205
Duplication matrix, 53
E
Envelope models, 15, 64, 72, 73
Error-correction model
for autoregressive process, 159
estimation of parameters in, 151, 156
use of partial canonical correlation analysis
in, 169
Errors-in-variables, 21
Estimating zero-beta rate, 244, 251
Estimation
of cointegrated autoregressive models, 161,
162
of error covariance matrix, 5
generalized least squares, 214–215
of growth curve model, 179–180, 191–192,
194–195
least squares, 3
of reduced-rank autoregressive models,
137, 139, 142
of reduced-rank regression models, 136,
139, 142
of regression model with AR error,
115–116, 129
of seemingly unrelated regressions model,
213–214, 216–217
F
Factor analysis model, 23
G
Generalized cross validation (GCV), 312, 323
Generalized least squares
in regression model with autoregressive
error, 114
in seemingly unrelated regressions model,
213–215
Generalized linear models, 337, 349
Generalized method of moments (GMM), 252,
253
Geometric incoherence, 342, 343
Goodness-of-fit test
in growth curve model, 187, 192, 193
Group Lasso, 359–361, 363, 367, 369–370,
376
Group structures, 259, 299
Growth curve model, 177–212
conditional form of, 182, 197
covariance adjustment in, 182
extended, 188–191
goodness-of-fit test in, 187, 192, 193
likelihood ratio test for rank in, 184, 187
likelihood ratio test of linear hypothesis in,
184–188
maximum likelihood estimation in, 164,
189, 199
one-way ANOVA in, 178, 202–205
reduced-rank, 171, 191–193
relation to seemingly unrelated regressions
model, 213–215
special covariance structures in, 179
H
High-dimensional data, 279, 300
High-dimensional regression, 279–309
I
Incomplete data, 340–347
Index models, 22, 136
Information criterion, 306, 311
Instrumented principal component analysis
(IPCA), 236, 252Subject Index 405
J
Joint continuum regression, 70
K
Kronecker approximation, 131
Kronecker product, 3, 6
L
Lasso regression, 283, 285, 293, 301, 304
Latent variables, 21–22, 75, 91
Least squares estimate, 3–5
in cointegrated AR model, 162
properties of, 5
Likelihood, 4, 29, 127–128
Likelihood ratio (LR) test
for linear hypothesis, 7–9
for linear hypothesis in growth curve
model, 184–188
for rank in autoregressive model, 127, 137,
139
for rank of cointegration in autoregressive
model, 167
for rank of regression matrix, 55–58,
127–128, 244
Linear relationship models, 22, 40–42
functional, 22, 40–42
structural, 22, 40–42
M
Matrix completion, 329, 330, 340–348, 350
Maximum likelihood estimator
asymptotic distribution of, 49, 50, 52, 55,
80, 98, 99, 139
for extended reduced-rank regression
model, 68, 91–93
for reduced-rank autoregression model,
132, 138–141
for reduced-rank regression model, 44, 45
for reduced-rank regression model with AR
error, 115–121
for reduced-rank seemingly unrelated
regressions model, 213–215
Mixed outcomes, 347–351, 354
Model complexity, 311, 312, 318, 321
N
Newton–Raphson procedure, 95, 121, 146,
174, 221
Nuclear norm, 280, 287–298, 300–301, 306,
307, 315
O
Outlier detection, 329–333
P
Partial canonical correlations, 8, 68, 69, 77
for autoregressive process, 144–148
use for error-correction AR model, 160,
163
use for specification of rank, 102, 144–145
Partial least squares, 36, 64, 232, 242
Partially reduced-rank regression, 259–277
Perturbation expansion, 46, 47
Predictability measure
for autoregressive process, 141
Prediction
in multivariate linear model, 10–12, 65
Prediction error covariance, 51, 52, 374
Predictor selection, 358–360
Principal components, 21, 38–45
application to linear relationship models,
40–42
regression, 252
Procrustes problem, 361
Projected principal component analysis, 236
R
Reduced-rank model
in asset pricing, 239–248
autoregressive, 139–141, 143
growth curve, 177–179, 182
multivariate regression, 19, 21, 58, 59, 73
with AR error, 126
one-way ANOVA, 82–85, 202–205
seemingly unrelated regressions, 213–215
Redundancy analysis, 21, 38
Regularized learning, 279–286, 288
Response selection, 355, 363
Restricted eigenvalue condition, 288, 302
Ridge regression, 279, 281–283
for multivariate linear model, 280
Robust estimation, 329, 331, 332
S
Sales Performance Data, 15–17, 60, 62, 380
Seemingly unrelated regressions model,
213–215
generalized least squares estimator in, 214
ML estimator in reduced-rank, 222
reduced rank in, 213–238
relation to growth curve model, 216–217
relation to IPCA, 236–237406 Subject Index
Shrinkage estimation
for multivariate linear model, 280
using canonical correlation methods,
274
Shrinking factor dimension, 252–254
Singular value decomposition, 27, 285–289
Singular value regularization, 285–289
Sparsity, 357–360, 363, 365, 368–370,
377
Stein’s Lemma, 315, 319
Sum of squares matrix
error, 5, 7
hypothesis, 8, 9
in one-way ANOVA, 82–85
between-group, 83, 204
within-group, 83, 181
T
Time series forecasting, 374–377
U
Unbiased risk estimation, 311–328
Unit roots in AR process, 162, 169–176
relation to reduced rank in error-correction
model, 163
relation to unit canonical correlations, 168,
169
relation to zero canonical correlations, 159
V
Vec operator, 6Reference Index
A
Ahn and Reinsel, 22, 92, 137, 144, 146, 148,
164–165, 265, 355
Aitchison and Silvey, 95, 121, 385
Akaike, 57, 138, 312, 385
Albert and Kshirsagar, 85, 197, 199, 205, 385
Aldrin, 326, 385
Allen et al.366, 385
Amemiya and Fuller, 23, 385
Anderson, 2, 7, 9, 11, 20, 21, 23, 40, 41, 46,
49–51, 56, 57, 75, 80, 85, 103, 120,
137, 141, 169, 264, 265, 275, 296,
298, 385, 386
Anderson and Rubin, 23, 385
B
Bai and Silverstein, 319, 386
Bao et al., 373, 386
Barratt et al., 377, 386
Barron et al., 282, 386
Bartlett, 42, 56, 127, 386
Beaulieu et al., 251, 386
Bekker et al., 246, 247, 386
Bewley and Yang, 159, 386
Bhargava, 23, 386
Bickel et al.284, 362, 368, 386
Bilodeau and Kariya, 66, 386
Black, 239, 240, 248, 386
Blattberg and George, 228, 229, 386
Blattberg and Wisniewski, 228, 386
Blattberg et al., 228, 386
Boot and de Wit, 214, 386
Bossaerts, 159, 386
Box and Tiao, 141, 148, 152, 154, 157, 158,
175, 386
Boyd and Vandenberghe, 345, 386
Boyd et al., 301, 386
Breheny and Huang, 301, 387
Breiman and Friedman, 67, 68, 86, 272, 387
Brem and Kruglyak, 371, 387
Brillinger, 21, 31, 136, 138, 387
Brooks and Stone, 70, 387
Brown and Payne, 66, 387
Brown and Zidek, 65, 66, 279, 387
Bühlmann and van de Geer, 284, 285, 362, 387
Bunea et al., 280, 288, 295, 297, 298, 306, 319,
326, 358–360, 362, 387
Bura and Cook, 59, 60, 387
C
Cai et al., 288, 289, 387
Campbell, 23, 85, 248, 387
Campbell et al., 245, 248, 250, 387
Candès and Recht, 330, 342, 343, 387
Candès and Tao, 343, 387
Candès et al., 319, 337, 350, 387
Carriero et al., 374, 387
Caruana, 301, 387
Chakraborty et al., 363, 387
Chen and Chan, 366, 387
Chen and Huang, 306, 358, 360, 388
Chen et al., 277, 288, 297, 298, 301, 306, 358,
359, 364, 366, 367, 372, 374, 388
Chi and Li, 347, 388
Chi and Reinsel, 179, 388
Chin et al., 305, 388
© The Author(s), under exclusive license to Springer Science+Business Media,
LLC, part of Springer Nature 2022
G. C. Reinsel et al., Multivariate Reduced-Rank Regression, Lecture Notes
in Statistics 225, https://doi.org/10.1007/978-1-0716-2793-8
407408 Reference Index
Chinchilli and Elswick, 188–190, 388
Collins et al., 350, 388
Connor and Linton, 236, 388
Connor et al., 236, 388
Cook, 73, 388
Cook and Su, 15, 388
Cook and Zhang, 73, 388
Cook et al., 71–73, 388
Cox and Wermuth, 351, 388
Cragg and Donald, 57, 388
D
Davenport and Romberg, 347, 388
Davies and Tso, 21, 24, 38, 70, 388
de Jong, 70, 388
de Leon and Wu, 351, 388
Diggle, 179, 389
Doan et al., 138, 389
Dong et al., 377, 389
Donoho, 279, 389
Donoho and Huber, 332, 389
Donoho and Johnstone, 288, 311, 313, 388
Dunson, 351, 388
E
Eckart and Young, 29, 314, 366, 389
Efron, 311, 312, 314, 389
Efron and Morris, 66, 389
Efron et al., 283, 389
Engle and Granger, 162, 170, 389
F
Fan and Li, 335, 336, 389
Fan and Lv, 279, 285, 290, 389
Fan et al., 236, 389
Fazel, 280, 389
Ferson and Harvey, 248, 389
Fisher, 23, 85, 389
Fitzmaurice and Laird, 351, 389
Fortier, 21, 38, 389
Fountis and Dickey, 170, 389
Frank and Friedman, 69, 389
Franklin, 295, 297, 389
Friedman et al., 283, 370, 389
Fu, 283, 389
Fujikoshi, 1, 202, 389
G
Gabriel and Zamir, 94, 390
Gallant and Goebel, 113, 390
Garthwaite, 69, 390
Geary, 22, 390
Geng et al., 306, 390
Genton, 132, 390
Geweke, 21, 390
Geweke and Zhou, 247, 390
Gibbons, 240, 242, 244, 250, 390
Gibbons and Ferson, 248, 249, 251, 390
Gibbons et al., 241–248, 250, 251, 390
Giles and Hampton, 214, 390
Gillard and Usevich, 377, 390
Glasbey, 27–29, 390
Gleser, 23, 390
Gleser and Olkin, 184, 390
Gleser and Watson, 23, 390
Goh et al., 363, 390
Goldberger, 21, 390
Golub and Van Loan, 365, 390
Golub et al., 312, 323
Gonzalo and Granger, 169, 390
Gospondinov et al., 247, 250, 390
Gower and Dijksterhuis, 361, 390
Grizzle and Allen, 179, 183, 184, 186, 390
Gudmundsson, 19, 25, 38, 114, 115, 391
Guggenberger et al., 132, 391
Gustin et al., 373, 391
H
Haitovsky, 65, 66, 391
Hammar and Ardal, 355, 391
Hampel et al., 336, 391
Hannan and Kavalieris, 138, 391
Hansen, 253, 315, 319, 391
Hansen and Hodrick, 248, 391
Hastie and Tibshirani, 85, 311, 313, 391
Hastie et al., 311, 312, 347, 391
Hatanaka, 114, 391
Hawkins, 267, 391
He et al., 252, 367, 391
Healy, 23, 85, 391
Heck, 10, 391
Helland, 69, 391
Hendry, 114, 391
Hocking and Leslie, 312, 391
Hoerl and Kennard, 279, 391
Honda, 66, 391
Hoskuldsson, 69, 392
Hotelling, 39, 42, 140, 157, 392
Hsu, 57, 127, 392
Huang et al., 285, 293, 299, 392
Huber, 333, 392
Hunter and Lange, 345, 392
Hwang and Takane, 211, 392Reference Index 409
I
Izenman, 2, 20, 32, 42, 46, 105, 286, 318, 319,
326, 392
J
Jenkins and Alavi, 138, 392
Jennrich and Schluchter, 179, 392
Johansen, 22, 168–170, 392
Johansen and Juselius, 170, 392
Johnson and Wichern, 15, 392
Johnstone and Lu, 359, 392
Jones, 179, 392
Jöreskog, 23, 392
Jöreskog and Goldberger, 21, 91, 392
Jorgensen, 348, 392
K
Kabe, 184, 392
Kanehisa et al., 371, 393
Keller and Wansbeek, 21, 393
Kelly et al., 236, 237
Khatri, 179, 186, 197, 393
Kim et al., 280, 393
Kleibergen and Paap, 57, 393
Klein et al., 25, 26, 115, 382, 393
Kohn, 127, 139, 393
Koltchinskii et al., 293, 298, 302, 393
Konno, 66, 393
Kshirsagar and Smith, 188, 393
L
Laird and Ware, 179, 393
Lawley, 23, 41, 393
Lee, 9, 393
Lee et al., 334, 366, 393
Li and Zhu, 313, 393
Li et al., 111, 277, 280, 301, 309, 393
Liang and Zeger, 351, 393
Lintner, 239, 240, 242, 250, 393
Lounici et al., 302, 304, 338, 393
Lu et al., 298, 393
Luca et al., 238
Luo and Chen, 277
Luo et al., 307, 330, 337, 349, 351, 355, 394
Lütkepohl, 49, 137, 138, 394
Lyttkens, 37, 394
M
Ma et al., 345, 357, 364, 369, 394
Magnus and Neudecker, 317, 394
Mallows, 311, 313, 394
Markowitz, 239, 240, 394
Marshall and Olkin, 158, 394
Mazumder and Weng, 319, 394
Mazumder et al., 345, 347, 394
McCulloch, 351, 394
McDonald and Galarneau, 279, 394
McElroy and Burmeister, 246, 247, 251, 394
Merton, 340, 394
Meyer and Woodroofe, 313, 394
Mirsky, 292, 394
Mishra et al., 358, 365, 367, 368, 394
Moore, 288, 314, 394
Moran, 23, 394
Muirhead, 2, 53, 394
Mukherjee et al., 312, 317–319, 327, 394
N
Negahban and Wainwright, 298, 302–304, 394
Negahban et al., 304, 394
Neudecker, 6, 395
Neuenschwander and Flury, 97, 395
Niu et al., 111, 395
O
Obozinski et al., 280, 395
O’Neil, 317, 395
P
Parzen, 138, 395
Peng et al., 280, 305, 395
Penrose, 288, 314, 395
Phillips and Ouliaris, 170, 395
Pillai, 10, 395
Pillai and Gupta, 9, 395
Pollack et al., 305, 395
Potthoff and Roy, 179, 181, 395
Prentice and Zhao, 351, 395
Priestley et al., 138
Q
Quenouille, 148, 395
R
Rao, 4, 8, 21, 23, 29, 33, 38, 43, 95, 101, 179,
184–186, 204, 245, 395
Recht, 330, 341, 343, 396
Recht et al., 343, 396
Reiersøl, 23, 396410 Reference Index
Reinsel, 8, 22, 92, 114, 138, 141, 144, 146,
148, 152, 156, 166, 170, 171, 179,
182, 272, 396
Reinsel and Ahn, 169, 170, 396
Reinsel and Velu, 92, 113, 212, 396
Reinsel et al., 105, 114, 396
Revankar, 215, 396
Robinson, 21, 33, 34, 43, 45, 47–49, 75, 94, 99,
113, 118, 125, 126, 128, 221, 396
Rochon and Helms, 179, 396
Rohde and Tsybakov, 298, 319, 396
Ross, 239, 246, 396
Rothman et al., 280, 396
Roy, 9, 396
Roy et al., 213, 396
Ryan et al., 29, 396
S
Sammel et al., 351, 396
Sargent and Sims, 22, 396
Schatzoff, 9, 396
Schott, 53, 397
Schwarz, 57, 108, 397
Sclove, 65, 397
Searle, 50, 397
Seber, 266, 397
Shanken, 244, 247, 251, 397
Shanken and Zhou, 247, 255, 397
Sharpe, 239–242, 250, 397
She, 335, 337, 349, 351, 362, 363, 397
She and Chen, 329, 331–334, 337, 352, 353,
397
She and Owen, 334, 397
She and Tran, 362, 397
Shen and Ye, 314, 397
Silvey, 94, 95, 99, 101, 118–121, 221, 247, 397
Sims, 136, 138, 397
Skagerberg et al., 85, 90, 272, 397
Smith et al., 12, 379, 397
Sprent, 23, 397
Srivastava, 85, 215, 397
Srivastava and Giles, 214, 222, 397
Srivastava and Khatri, 2, 9, 397
Stambaugh, 248, 397
Stanek III and Koch, 216, 397
Stanziano et al., 299, 307, 330, 340, 397
Stein, 311–313, 397
Stock and Watson, 170, 398
Stoica and Viberg, 49, 51, 398
Stone, 58, 293, 398
Stone and Brooks, 69, 70, 398
Storey et al., 371, 398
Sundberg, 70, 398
T
Takane and Jung, 111, 398
Takane et al., 92, 398
Tan et al., 340, 398
ter Braak, 42, 398
Theobald, 23, 26, 41, 85, 398
Thurstone, 23, 398
Tiao and Box, 140, 150, 398
Tiao and Tsay, 138, 145, 148, 155, 156, 398
Tibshirani, 279, 283, 293, 311, 398
Tibshirani and Taylor, 318, 398
Tikhonov, 279, 281, 398
Tintner, 22, 40, 398
Toh and Yun, 289, 398
Tseng, 283, 362, 398
Tso, 21, 398
Turlach et al., 280, 398
U
Udell et al., 289, 351, 398
Uematsu et al., 365, 370, 398
V
van den Wollenberg, 21, 38, 399
van der Leeden, 21, 23, 399
van der Merwe and Zidek, 67, 399
Van Loan and Pitsianis, 131, 399
Vandenberghe and Boyd, 341, 398
Velu, 92, 190, 275, 399
Velu and Herman, 133, 399
Velu and Reinsel, 113, 399
Velu and Richards, 234, 399
Velu and Zhou, 22, 246, 251, 399
Velu et al., 22, 92, 137, 144, 148, 157, 158, 399
Verbyla and Venables, 190, 399
Villegas, 23, 399
Volund, 205, 399
von Neumann, 292, 399
von Rosen and von Rosen, 211, 399
Vounou et al., 357, 364, 399
W
Wang et al., 377, 399
Wille et al., 326, 352, 399
Witten et al., 305, 359, 366, 399
Wold, 68, 69, 399
Wright et al., 337, 400
Y
Yang and Zou, 361, 400
Ye, 312, 314, 400Reference Index 411
Ye and Zhang, 338, 369, 400
Yee and Hastie, 337, 349, 400
Yu et al., 370, 400
Yuan, 319, 400
Yuan and Lin, 299, 361, 400
Yuan et al., 280, 286, 289, 331, 400
Z
Zellner, 21, 213–215, 217, 218, 226, 235, 400
Zhang, 335, 400
Zhang and Zhang, 290, 400
Zhao and Yu, 285, 367, 400
Zhao et al., 351, 400
Zheng et al., 365, 368, 400
Zhou, 21, 29, 244, 250, 251, 254, 400
Zhou et al., 306, 400
Zhu et al., 363, 400
Zou, 290, 292, 293, 400
Zou et al., 313, 318, 359, 400
