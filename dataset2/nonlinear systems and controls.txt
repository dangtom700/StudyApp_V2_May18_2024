Jürgen Adamy
Nonlinear 
Systems and 
Controls
Second EditionNonlinear Systems and ControlsJürgen Adamy
Nonlinear Systems
and Controls
Second EditionJürgen Adamy
Fachgebiet Regelungsmethoden und
Intelligente Systeme
Technische Universität Darmstadt
Darmstadt, Germany
ISBN 978-3-662-68689-8 ISBN 978-3-662-68690-4 (eBook)
https://doi.org/10.1007/978-3-662-68690-4
© Springer-Verlag GmbH Germany, part of Springer Nature 2022, 2024
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material
is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, repro￾duction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic
adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does
not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective
laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give
a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that
may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and
institutional affiliations.
This Springer imprint is published by the registered company Springer-Verlag GmbH, DE, part of Springer Nature.
The registered company address is: Heidelberger Platz 3, 14197 Berlin, Germany
Paper in this product is recyclable.For Claudia, Ursula, and TimmPreface
Nonlinear dynamic systems and controls play an important role in science,
mathematics, economics, and engineering. The regulation of human body tem￾perature, the regulation of the money supply in an economy, and the autopilot
of an airplane are examples. A book about these systems can be written from
the point of view of the scientist, the mathematician, or the engineer. A book of
this kind must orient itself towards the objective of the discipline in question:
a book focused on the natural sciences would explain the dynamic behavior of
natural phenomena, such as planetary motion or the blood pressure regulation
in mammals. A mathematical approach would focus on theorems and their
proofs, and a book addressed to engineers would describe the theory and its
practical application, such as how to model and control nonlinear processes
and systems like power plants and rockets.
Karl Popper’s three-world theory illustrates these three perspectives: it is a
model of the scientific process that divides the world into the world of matter,
the world of the subjective ego, and the world of objective knowledge, as shown
in Figure P.1. The material world includes all kinds of matter including flora,
fauna, and the human body. The world of the subjective ego includes the
perceptions, thoughts, and ideas of an individual human being. In contrast,
the objective world consists of, among other things, scientific theories, works
of art, laws and agreements that are available to all or many people to examine
and change.
Scientists observe the material world and use the subjective ego to develop
a theory of how it works: they make discoveries. A discovery is published in
a scientific journal and is thus made part of the objective world. In this way,
a theory is broadly accessible and can be tested by other scientists and either
confirmed or falsified. In contrast to the classical natural sciences (physics,
chemistry, and biology), mathematics is a method, or we could also say a lan￾guage, with which natural and technological processes can be quantitatively
described. This is not possible using our everyday language. Mathematicians
are constantly expanding this language and exploring new ways of describing
facts using formulas and solving problems mathematically: they draft math￾viiPreface
C
C C
C
C
C
H
H
H
H
H H
∆
Ε
@B
@t
~ ~
~
Understanding
scientific theories
and
patenting inventions
Engineer
creating inventions
Building
of machinery
Natural scientist
making discoveries
Mathematician
Observation
of nature
Publishing theories
Fig. P.1: Karl Popper’s three-world ontology
ematical hypotheses and prove them. Engineers make use both of scientific
theories, especially physical theories, and mathematics, the language in which
these theories are formulated. They use this knowledge of the objective world
to create, within the world of their subjective ego, the idea for a new ma￾chine or a new technical process: they invent something new. By realizing
these inventions, they change the material world. They build skyscrapers, air￾planes, power plants, and robots. Construction plans and system descriptions
published in patents and journals become part of the objective world.
viiiPreface
Although these three disciplines are sharply delineated above, there are
a large number of fields in which they overlap. These include experimental
physics, in which scientists build special equipment for their research such
as particle accelerators, and control engineering, in which new mathemati￾cal theories, such as that of flat systems, are developed to solve engineering
problems.
This book is primarily oriented towards the engineer’s point of view. The
aim of the book is therefore to convey the knowledge of nonlinear systems and
controls that is important for an engineer, and to illustrate its applicability. In
doing so, the book describes the current state of the theory of these systems.
For this purpose, the theorems of this mathematical theory which have proved
useful in engineering practice have been selected. Since these theorems and
their proofs are available in the literature, the proofs have not been shown
here in the classical theorem-proof sequence: the emphasis in this book is on
teaching the theory and its application rather than proving of the theorems.
Literature Recommendations
To learn more about a particular field of research in detail, it is useful to
consult various books which address that field. In the case of the theory of
nonlinear systems and controls, for example, the works of H. K. Khalil, E.
D. Sontag, S. Sastry, H. Nijmeijer and A. van der Schaft, J.-J. E. Slotine and
W. Li, H. J. Marquez, and A. Isidori are recommended. C. M. Kellett and
P. Braun cover both linear and nonlinear control systems in their book. An
introduction to the subject is given in the compact book by G. Chen.
To understand the theory of nonlinear systems, the theory of linear systems
is useful or necessary in many cases. Describing this theory would require a
similarly extensive volume, so it cannot be addressed in detail in this book and
it is assumed that the reader is already familiar with it. Suitable works on this
topic have been written by G. F. Franklin, J. D. Powell, and A. Emami-Naeini,
as well as R. C. Dorf and R. H. Bishop, among others.
Acknowledgements
A large number of scholars have proofread the book for comprehensibility, er￾rors, and mistakes, and have contributed to its layout. For the first as well as
the second English edition, I would like to especially thank K. Olhofer-Karova
for patiently preparing the LATEX source code and Matlab simulations, and V.
Ansel for preparing the color images. My thanks go to the American transla￾tor H. Krehbiel for the linguistic revision of the text.
Jürgen Adamy, Technische Universität Darmstadt, 2023
ixContents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii
Literature Recommendations . .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .
Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1 Fundamentals of Nonlinear Systems . . . . . . . . . . . . . . . . . . . . . . . 1
1.1 System Description and System Behavior . . . . . . . . . . . . . . . . . . . 1
1.1.1 Linear and Nonlinear Systems. . . . . . . . . . . . . . . . . . . . . . . 1
1.1.2 System Description and Nonlinear Control Loops . . . . . . 2
1.1.3 Equilibrium Points of Nonlinear Systems . . . . . . . . . . . . . 5
1.1.4 Example: Satellite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.1.5 Equilibrium Points of Linear Systems . . . . . . . . . . . . . . . . 9
1.1.6 Stability and Asymptotic Stability . . . . . . . . . . . . . . . . . . . 11
1.1.7 Exponential Stability of Equilibrium Points . . . . . . . . . . . 18
1.1.8 Instability of Equilibrium Points . . . . . . . . . . . . . . . . . . . . 20
1.1.9 Stability in the Case of Variable Input Signals . . . . . . . . 25
1.1.10 Limit Cycles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
1.1.11 Sliding Modes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
1.1.12 Chaos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
1.1.13 Discrete-Time Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
1.2 Solution of Nonlinear Differential Equations . . . . . . . . . . . . . . . . 37
1.2.1 Existence of Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
1.2.2 Numerical Solution and Euler Method . . . . . . . . . . . . . . . 41
1.2.3 Accuracy of the Numerical Solution . . . . . . . . . . . . . . . . . . 43
1.2.4 The Modified Euler Method . . . . . . . . . . . . . . . . . . . . . . . . 44
xi
ix
ixContents
1.2.5 The Heun and Simpson Methods . . . . . . . . . . . . . . . . . . . . 45
1.2.6 The Runge-Kutta Methods . . . . . . . . . . . . . . . . . . . . . . . . . 47
1.2.7 Adaptation of the Step Size . . . . . . . . . . . . . . . . . . . . . . . . . 49
1.2.8 The Adams-Bashforth Methods . . . . . . . . . . . . . . . . . . . . . 51
1.2.9 The Adams-Moulton Predictor-Corrector Methods . . . . . 52
1.2.10 Stability of Numerical Integration Methods . . . . . . . . . . . 53
1.2.11 Stiff Systems and Their Solutions. . . . . . . . . . . . . . . . . . . . 56
1.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
2 Limit Cycles and Stability Criteria . . . . . . . . . . . . . . . . . . . . . . . . 69
2.1 The Describing Function Method . . . . . . . . . . . . . . . . . . . . . . . . . . 69
2.1.1 Idea behind the Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
2.1.2 Illustrative Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
2.1.3 Characteristic Curves and Their Describing Functions. . 75
2.1.4 Stability Analysis of Limit Cycles . . . . . . . . . . . . . . . . . . . 82
2.1.5 Example: Power-Assisted Steering System . . . . . . . . . . . . 85
2.2 Absolute Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
2.2.1 The Concept of Absolute Stability . . . . . . . . . . . . . . . . . . . 89
2.2.2 The Popov Criterion and Its Application . . . . . . . . . . . . . 91
2.2.3 The Aizerman and Kalman Conjectures . . . . . . . . . . . . . . 97
2.2.4 Example: Controlling a Ship . . . . . . . . . . . . . . . . . . . . . . . . 99
2.2.5 The Circle Criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
2.2.6 The Tsypkin Criterion for Discrete-Time Systems . . . . . 110
2.3 Lyapunov’s Stability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
2.3.1 The Concept and the Direct Method . . . . . . . . . . . . . . . . . 113
2.3.2 Illustrative Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
2.3.3 Quadratic Lyapunov Functions . . . . . . . . . . . . . . . . . . . . . . 119
2.3.4 Example: Mutualism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
2.3.5 The Direct Method for Discrete-Time Systems . . . . . . . . 125
2.3.6 The Indirect Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
2.3.7 Determining Exponential Stability . . . . . . . . . . . . . . . . . . . 127
2.3.8 Example: Underwater Glider . . . . . . . . . . . . . . . . . . . . . . . . 129
2.3.9 Catchment Regions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
2.3.10 LaSalle’s Invariance Principle . . . . . . . . . . . . . . . . . . . . . . . 137
2.3.11 Instability Criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
2.4 Passivity and Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
2.4.1 Passive Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
2.4.2 Stability of Passive Systems . . . . . . . . . . . . . . . . . . . . . . . . 146
2.4.3 Passivity of Connected Systems . . . . . . . . . . . . . . . . . . . . . 148
2.4.4 Passivity of Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . 151
2.4.5 Example: Transporting System for Material Webs . . . . . 155
2.4.6 Positive Real Transfer Functions . . . . . . . . . . . . . . . . . . . . 158
2.4.7 Equivalence of Positive Realness and Passivity . . . . . . . . 163
2.4.8 Lossless Hamiltonian Systems . . . . . . . . . . . . . . . . . . . . . . . 168
2.4.9 Example: Self-Balancing Vehicle . . . . . . . . . . . . . . . . . . . . . 175
xiiContents
2.4.10 Dissipative Hamiltonian Systems . . . . . . . . . . . . . . . . . . . . 179
2.4.11 Example: Separately Excited Direct-Current Machine . . 182
2.4.12 Linear Hamiltonian Systems . . . . . . . . . . . . . . . . . . . . . . . . 185
2.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
3 Controllability and Flatness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
3.1 Controllability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
3.1.1 Definition of Controllability. . . . . . . . . . . . . . . . . . . . . . . . . 205
3.1.2 Global and Local Controllability . . . . . . . . . . . . . . . . . . . . 212
3.1.3 Proving Controllability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
3.1.4 Example: Industrial Robot . . . . . . . . . . . . . . . . . . . . . . . . . 219
3.1.5 Small-Time Local Controllability of Driftless Systems . . 224
3.1.6 Example: Motor Vehicle with Trailer . . . . . . . . . . . . . . . . . 231
3.1.7 Omnidirectional Controllability . . . . . . . . . . . . . . . . . . . . . 233
3.1.8 Example: Steam Generator . . . . . . . . . . . . . . . . . . . . . . . . . 236
3.2 Flatness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
3.2.1 Basic Concept and Definition of Flatness . . . . . . . . . . . . . 239
3.2.2 The Lie-Bäcklund Transformation . . . . . . . . . . . . . . . . . . . 243
3.2.3 Example: VTOL Aircraft . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
3.2.4 Flatness and Controllability . . . . . . . . . . . . . . . . . . . . . . . . 250
3.2.5 Flat Outputs of Linear Systems . . . . . . . . . . . . . . . . . . . . . 251
3.2.6 Verification of Flatness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
3.3 Nonlinear State Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . 259
3.3.1 Transformations and Transformed System Equations . . . 259
3.3.2 Illustrative Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
3.3.3 Example: Park Transformation . . . . . . . . . . . . . . . . . . . . . . 266
3.3.4 Determining the Transformation Rule . . . . . . . . . . . . . . . . 273
3.3.5 Illustration Using Linear Systems. . . . . . . . . . . . . . . . . . . . 274
3.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
4 Nonlinear Control of Linear Systems . . . . . . . . . . . . . . . . . . . . . . 285
4.1 Control with Anti-Windup. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
4.1.1 The Windup Effect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
4.1.2 PID Controller with Anti-Windup Element . . . . . . . . . . . 287
4.1.3 Example: Direct-Current Motor . . . . . . . . . . . . . . . . . . . . . 288
4.1.4 A General Anti-Windup Method . . . . . . . . . . . . . . . . . . . . 291
4.1.5 Dimensioning the General Anti-Windup Controller . . . . 295
4.1.6 Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
4.2 Time-Optimal Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
4.2.1 Fundamentals and Fel'dbaum’s Theorem . . . . . . . . . . . . . 299
4.2.2 Computation of Time-Optimal Controls . . . . . . . . . . . . . . 301
4.2.3 Example 1/s2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
4.2.4 Time-Optimal Control of Low-Order Systems . . . . . . . . . 307
4.2.5 Example: Submarine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
4.2.6 Time-Optimal Pilot Control . . . . . . . . . . . . . . . . . . . . . . . . 313
xiiixiv Contents
4.3 Variable Structure Control Without Sliding Mode . . . . . . . . . . . 314
4.3.1 Fundamentals of Variable Structure Control . . . . . . . . . . 314
4.3.2 Piecewise Linear Control . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
4.3.3 Example: Ship-to-Shore Gantry Crane . . . . . . . . . . . . . . . 322
4.4 Saturation Controllers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
4.4.1 Basics and Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
4.4.2 Design in Multiple Steps . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
4.4.3 Example: Helicopter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
4.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
5 Nonlinear Control of Nonlinear Systems . . . . . . . . . . . . . . . . . . . 345
5.1 Gain-Scheduling Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
5.1.1 Mode of Operation and Design . . . . . . . . . . . . . . . . . . . . . . 345
5.1.2 Illustrative Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
5.1.3 Example: Solar Power Plant . . . . . . . . . . . . . . . . . . . . . . . . 354
5.2 Input-Output Linearization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
5.2.1 Basic Concept and Nonlinear Controller Canonical Form359
5.2.2 Nonlinear Controller and Linear Control Loop . . . . . . . . 364
5.2.3 Example: Magnetic Bearing. . . . . . . . . . . . . . . . . . . . . . . . . 367
5.2.4 Plants with Internal Dynamics . . . . . . . . . . . . . . . . . . . . . . 372
5.2.5 Design Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
5.2.6 Example: Lunar Module. . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
5.2.7 Input-Output Linearization of General SISO Systems . . 384
5.2.8 Relative Degree and Internal Dynamics of Linear
Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
5.2.9 Control Law for the Linear Case . . . . . . . . . . . . . . . . . . . . 395
5.2.10 Stability of Internal and Zero Dynamics . . . . . . . . . . . . . . 398
5.2.11 Input-Output Linearization of MIMO Systems . . . . . . . . 401
5.2.12 MIMO Control Loops in State-Space Representation . . . 405
5.2.13 Example: Combustion Engine . . . . . . . . . . . . . . . . . . . . . . . 410
5.3 Full-State Linearization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
5.3.1 Full-State Linearization of SISO Systems . . . . . . . . . . . . . 413
5.3.2 Example: Drilling Rig . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
5.3.3 Full-State Linearization of MIMO Systems. . . . . . . . . . . . 426
5.3.4 Flatness of Full-State Linearizable Systems . . . . . . . . . . . 430
5.3.5 Example: Rocket . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432
5.4 Feedforward and Feedback Control of Flat Systems . . . . . . . . . . 435
5.4.1 Fundamentals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
5.4.2 Feedforward Controls Using Fictitious Flat Outputs . . . 437
5.4.3 Flatness-Based Feedforward Control of Linear Systems . 441
5.4.4 Example: Propulsion-Based Aircraft Control . . . . . . . . . . 443
5.4.5 Flatness-Based Feedback Control of Nonlinear Systems . 448
5.4.6 Example: Pneumatic Motor . . . . . . . . . . . . . . . . . . . . . . . . . 453
5.4.7 Flat Inputs and Their Design . . . . . . . . . . . . . . . . . . . . . . . 458
5.4.8 Flat Inputs of Linear Systems . . . . . . . . . . . . . . . . . . . . . . . 462Contents xv
5.4.9 Example: Economic Market Model. . . . . . . . . . . . . . . . . . . 463
5.5 Control Lyapunov Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
5.5.1 Fundamentals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
5.5.2 Control Lyapunov Functions for Linear Systems . . . . . . . 467
5.5.3 Control Lyapunov Functions for Control-Affine Systems 468
5.5.4 Illustrative Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
5.5.5 Example: Power Plant with Grid Feed-In . . . . . . . . . . . . . 472
5.6 The Backstepping Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
5.6.1 Fundamentals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
5.6.2 Recursive Scheme for the Controller Design . . . . . . . . . . . 483
5.6.3 Illustrative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
5.6.4 Example: Fluid System with Chaotic Behavior . . . . . . . . 489
5.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
6 Nonlinear Control of Linear and Nonlinear Systems . . . . . . . 505
6.1 Model-Based Predictive Control . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
6.1.1 Basics and Functionality . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
6.1.2 Linear Model Predictive Control without Constraints . . 509
6.1.3 LMPC with Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
6.1.4 Example: Drainage System . . . . . . . . . . . . . . . . . . . . . . . . . 516
6.1.5 Nonlinear Model Predictive Control. . . . . . . . . . . . . . . . . . 521
6.1.6 Example: Evaporation Plant . . . . . . . . . . . . . . . . . . . . . . . . 526
6.2 Variable Structure Control with Sliding Mode . . . . . . . . . . . . . . . 530
6.2.1 Basics and Characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . 530
6.2.2 Design for Linear Plants. . . . . . . . . . . . . . . . . . . . . . . . . . . . 532
6.2.3 Dynamics in the Sliding Mode . . . . . . . . . . . . . . . . . . . . . . 534
6.2.4 Verification of Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . 535
6.2.5 Example: DC-to-DC Converter . . . . . . . . . . . . . . . . . . . . . . 536
6.2.6 Design for Nonlinear Plants. . . . . . . . . . . . . . . . . . . . . . . . . 542
6.2.7 Example: Optical Switch . . . . . . . . . . . . . . . . . . . . . . . . . . . 544
6.3 Passivity-Based Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547
6.3.1 Control of Passive Systems Using Static Controllers. . . . 547
6.3.2 Example: Damping of Seismic Building Vibrations . . . . . 550
6.3.3 Passivation of Non-Passive Linear Systems. . . . . . . . . . . . 558
6.3.4 Passivation of Non-Passive Control-Affine Systems . . . . . 563
6.3.5 Passivity-Based Control with IDA . . . . . . . . . . . . . . . . . . . 565
6.3.6 Example: Paper Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . 570
6.4 Fuzzy Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574
6.4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574
6.4.2 Fuzzification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 575
6.4.3 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577
6.4.4 Defuzzification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584
6.4.5 Fuzzy Systems and Fuzzy Controllers . . . . . . . . . . . . . . . . 585
6.4.6 Example: Distance Control for Automobiles. . . . . . . . . . . 588
6.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593xvi Contents
7 Observers for Nonlinear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . 603
7.1 Observability of Nonlinear Systems . . . . . . . . . . . . . . . . . . . . . . . . 603
7.1.1 Definition of Observability . . . . . . . . . . . . . . . . . . . . . . . . . . 603
7.1.2 Observability of Autonomous Systems. . . . . . . . . . . . . . . . 607
7.1.3 Example: Synchronous Generator. . . . . . . . . . . . . . . . . . . . 610
7.1.4 Observability of General Nonlinear Systems . . . . . . . . . . . 612
7.1.5 Nonlinear Observability Canonical Form. . . . . . . . . . . . . . 614
7.1.6 Observability of Control-Affine Systems . . . . . . . . . . . . . . 617
7.2 Canonical Forms and the Canonical Form Observer . . . . . . . . . . 620
7.3 Luenberger Observers for Nonlinear Control Loops. . . . . . . . . . . 624
7.4 Observer Design Using Linearization . . . . . . . . . . . . . . . . . . . . . . . 626
7.4.1 Basics and Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626
7.4.2 Control Loop with Observer . . . . . . . . . . . . . . . . . . . . . . . . 630
7.4.3 Example: Bioreactor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 631
7.5 The Extended Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635
7.5.1 Kalman Filter for Linear Systems . . . . . . . . . . . . . . . . . . . 635
7.5.2 The EKF for Nonlinear Systems. . . . . . . . . . . . . . . . . . . . . 637
7.5.3 Example: Jet Engine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 640
7.6 High-Gain Observer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643
7.6.1 Concept and Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643
7.6.2 High-Gain Observers in General Form. . . . . . . . . . . . . . . . 649
7.6.3 Example: Chemical Reactor . . . . . . . . . . . . . . . . . . . . . . . . 651
7.6.4 The Case of Control-Affine Systems. . . . . . . . . . . . . . . . . . 655
7.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 659
8 Solutions to the Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 663
A Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 695
A.1 Proof of the General Circle Criterion . . . . . . . . . . . . . . . . . . . . . . . 695
A.2 Parameters of the Container Crane Control . . . . . . . . . . . . . . . . . 699
B List of Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 701
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 707
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7331
Fundamentals of Nonlinear Systems
1.1 System Description and System Behavior
1.1.1 Linear and Nonlinear Systems
Only some of the processes and systems that are regularly encountered in
nature or industrial practice can be adequately described by linear models
and linear systems theory. A significant number of processes and systems are
nonlinear and must be represented by nonlinear models. Linear systems theory
is not generally applicable to nonlinear systems. Exceptions are nonlinear
systems, which can be sufficiently well approximated by a linear model.
Therefore, nonlinear systems and control loops require their own analy￾sis and design methods. Because nonlinear systems are mathematically much
more complex than linear systems, so far such methods have only been devel￾oped for certain classes of nonlinear systems and for specific applications. This
is quite different in the case of linear systems, for which an almost completely
elaborated systems theory exists with only a few unexplored areas [133, 152].
Figure 1.1 illustrates this different level of knowledge.
It is not entirely correct to speak of a nonlinear systems theory, since
the term actually refers to a collection of incoherent methods and theories.
Rather than a single theory, as shown schematically in Figure 1.1, there are
a number of theories for different classes of nonlinear systems and controllers
[77, 203, 232, 338, 402, 465]. Despite this diversity and depending on the focus,
it is common to subsume these theories under the terms nonlinear control or
nonlinear systems. One of the most important and most developed theories is
that of control-affine systems, which depend nonlinearly on the system states
but linearly on the input variables. The theory of such systems is almost
completely elaborated and is also of practical importance.
The most important great methods of nonlinear control engineering, which
are also most relevant to industrial practice, are the subject of this book. To
begin with, in this chapter, the system dynamics and the solution of non￾linear differential equations are addressed. This is followed by a description
© Springer-Verlag GmbH Germany, part of Springer Nature 2024
J. Adamy, Nonlinear Systems and Controls,
https://doi.org/10.1007/978-3-662-68690-4_1
12 Chapter 1. Fundamentals of Nonlinear Systems
Linear systems Nonlinear systems
Fig. 1.1: Level of knowledge regarding linear and nonlinear systems. Blue represents
the known areas; white represents the unexplored areas.
of the stability theory of nonlinear systems and feedback controls in Chap￾ter 2. Chapter 3 deals with the system property of controllability, which is the
prerequisite for any kind of feedback and feedforward control, which are also
known as open-loop and closed-loop control, respectively. It further describes
the flatness property of a system, which is related to its controllability, and
contains an explanation of state-space transformations and diffeomorphisms.
This fundamental knowledge is the starting point of a series of design
methods for nonlinear controllers of linear plants, i. e. of linear controlled
systems, as described in Chapter 4, and nonlinear plants, which are discussed
in Chapter 5. An important part of this chapter is the theory of control￾affine systems and their controls. Design methods for controllers that can be
used both in linear and in nonlinear plants follow in Chapter 6. Since most
nonlinear control methods require the state vector of the controlled system,
which is often not fully measurable, observers for nonlinear control loops are
dealt with in Chapter 7.
1.1.2 System Description and Nonlinear Control Loops
Nonlinear systems can generally be described by a vector differential equation
x˙ = f(x,u) (1.1)
and an output equation
y = g(x,u).
Here, x is the n - dimensional state vector, u is the m - dimensional actuator
or input variable vector , also called the control variable vector , f is the n -
dimensional system function vector, y is the r - dimensional output variable
vector , and g is the r - dimensional output function vector, or output vector
for short. In most cases, the output variable vector y does not depend on the
input variable vector u, i. e.
y = g(x).
If the output equation depends on u, this dependency is referred to as
feedthrough.1.1. System Description and System Behavior 3
In the case of linear systems
x˙ = Ax + Bu,
y = Cx + Du,
we call A ∈ IRn×n
the system matrix , B ∈ IRn×m the input matrix or the
input vector if m = 1, C ∈ IRr×n
the output matrix or the output vector if
r = 1, and D ∈ IRr×m the feedthrough or direct transmission matrix.
In contrast to linear systems, which are defined on the entire real coor￾dinate space IRn
, nonlinear systems, more precisely the functions f and g,
are sometimes defined only on a subset. This subset, the system’s domain of
definition, is the intersection of the functions f and g’s domains of definition.
It can be represented as the Cartesian product Dx,def × Du,def of the sets
Dx,def ⊆ IRn
of vectors x and Du,def ⊆ IRm of vectors u, where Dx,def and
Du,def are the sets on which the two functions f(x,u) and g(x,u) are both
defined. For simplicity, we will often only refer to the subspace Dx,def as the
system’s domain of definition. In the following, if no domain of definition is
explicitly mentioned, we will assume Dx,def × Du,def.
Sometimes, especially when a system possesses only a single-input and
single-output variable, this system can be described by an explicit higher
order differential equation
y
(n) = h(y, y, . . . , y ˙
(n−1), u1, . . . , um). (1.2)
By introducing the variables
x1 = y,
x2 = ˙y,
.
.
.
xn = y
(n−1)
,
equation (1.2) can also be represented as the vector differential equation
x˙ = f(x,u) =







x2
x3
.
.
.
xn
h(x1, . . . , xn, u1, . . . , um)







.
Systems with a single-input variable u and a single-output variable y
are referred to as SISO systems. Systems with multiple-input variables and
multiple-output variables are called MIMO systems. The system represented
by equation (1.1) is said to be autonomous if the function f does not depend
on a time-varying input variable u(t), or – which is essentially the same when
solving the differential equation – if f is not directly time-dependent, i. e.4 Chapter 1. Fundamentals of Nonlinear Systems
x˙ = f(x).
If a system is explicitly time-dependent, i. e.
x˙ = f(x,u, t)
holds and f is defined for t ∈ Dt,def ⊆ IR, we call it time-variant. If it is not,
i. e. when
x˙ = f(x,u) (1.3)
holds, it is called time-invariant. If the input signal u(t) of a system is equal
to zero or the input signal is missing, i. e. if
x˙ = f(x, t,u = 0) or x˙ = f(x, t)
holds, we call it a free system. A function x(t) that fulfills the differential
equation (1.3) for a given initial vector x(0) and a given input signal u(t) is
called solution or trajectory.
In control theory, three classes of nonlinear control systems are considered
to be most relevant. The first class consists of the linear controllers for non￾linear plants, as depicted in Figure 1.2. They are often associated with linear
controller design methods, since in this case the plant is linearizable and the
yref e Linear u y
controller
Nonlinear
plant
Fig. 1.2: Nonlinear plant with a linear controller
yref e u Linear y
plant
Nonlinear
controller
Fig. 1.3: Linear plant with a nonlinear controller
yref e u Nonlinear y
plant
Nonlinear
controller
Fig. 1.4: Nonlinear plant with a nonlinear controller1.1. System Description and System Behavior 5
linearized model is sufficiently accurate. Thus linear systems theory can be
applied. The reference variable and the control error are denoted by yref and
e, respectively. In the case of systems with multiple-input and multiple-output
variables, we use the bold letters e and yref.
The second class consists of nonlinear controllers for linear plants, as shown
in Figure 1.3. Often, simple nonlinear controllers are applied to linear plants
for technical reasons or because of their low cost. A common example is the
temperature control in electric irons using a bimetal. The bimetal has the
characteristic of a switch with hysteresis, i. e. it is a nonlinear system. Non￾linear controllers with a more complex design are also used to control linear
plants in order to achieve better control results compared to those that can
be achieved using linear controllers.
The third class consists of nonlinear controllers for nonlinear plants, as
illustrated in Figure 1.4. Nonlinear plants are often very complex in terms of
their behavior. For such cases, linear controllers are often not able to provide
the desired quality, and nonlinear controllers must be designed. For example, it
is sometimes possible to combine a nonlinear plant with a nonlinear controller
in such a way that the resulting control loop is linear. Due to the linearity, its
behavior is easy to comprehend.
1.1.3 Equilibrium Points of Nonlinear Systems
An equilibrium point is a state of a dynamic system which does not change
with time, i. e. it will remain constant for all future time. One of the central
objectives of control engineering is to drive the state variables of a plant to
an equilibrium point and to keep it there. For example, the aim is to steer
an aircraft to a certain altitude using an autopilot, to heat up the water in a
boiler to a desired temperature, or to roll a sheet of metal to a predetermined
thickness, and then to hold it there. Therefore to design a suitable controller,
an appropriate equilibrium point must first be found. This raises the question
of how to determine the equilibrium point of a nonlinear system. Before this
question is examined in more detail, the term equilibrium point should be
clearly defined.
Definition 1 (Equilibrium Point). Consider the system
x˙ = f(x,u).
A point xeq within the state space is called an equilibrium point if
x˙ = f(xeq, 0) = 0
holds.
In this definition, it was assumed that
u = 0.6 Chapter 1. Fundamentals of Nonlinear Systems
Of course, m - dimensional input variable vectors
u = c 6= 0
may exist, where c ∈ IRm is a constant vector, such that
x˙ = f(x,u = c) = 0 (1.4)
holds. Often, depending on c, systems have an infinite number of equilibrium
points. This is why, according to the above definition, we always refer to the
equilibrium points of the free system, i. e. x˙ = f(x,u = 0), when speaking of
an equilibrium point without further specification. The case of equation (1.4)
is covered by the above definition when the transformation
u = u˜ + c
is performed. Then we must analyze the system
x˙ = f(x,u˜ + c) = f˜(x,u˜)
which has an equilibrium point at xeq for u˜ = 0. This implies that
x˙ = f˜(xeq, 0) = 0
holds.
Determining an equilibrium point of a nonlinear system is often not an
easy task to accomplish. This is because we must solve the implicit equation
x˙ = f(xeq, 0) = 0
for xeq. One, zero, or multiple solutions may exist. A continuum of solutions
is also possible, or there may be none at all. This is illustrated in Figure 1.5
for a one-dimensional function f.
When calculating the equilibrium points xeq, the following three cases
may occur. In the first case, the implicit equation f(xeq, 0) = 0 is explicitly
solvable for xeq, i. e.
xeq = f
−1
(0)
holds.
In the second case, the implicit equation is transcendental. Here we must
resort to numerical methods, such as the multidimensional Newton’s method.
This can be difficult because we often face the problem of not knowing how
many equilibrium points exist, or even if there are any equilibrium points at
all.
The third case applies to many technical systems: we can often surmise an
equilibrium point xeq, either from intuition or from knowledge that is available
about the system. Substituting this estimate into the equation system allows
us to verify our assumption.1.1. System Description and System Behavior 7
Continuum of solutions
One solution Multiple solutions
No solution
f
f
f
f
x x
x x
xeq
xeq1 xeq2 xeq3
Fig. 1.5: Possible solutions of the equilibrium point equation f(xeq, 0) = 0
1.1.4 Example: Satellite
Let us consider the rotation of a satellite or a space probe that can be rotated
around its axes via control jets. Figure 1.6 shows such a satellite with its corre￾sponding body-fixed coordinate system (x, y, z) and a space-fixed coordinate
system (˜x, y, ˜ z˜).
For the angular momentum vector L of the satellite which rotates with
its corresponding body-fixed coordinate system and angular velocity vector
ω around the space-fixed coordinate system, using the inertia tensor J, the
relation
L = Jω
holds. The equation of motion is derived from
dL
dt = M, (1.5)
wherein the torque vector M contains the torques caused by the control jets.
For the derivative of the angular momentum L with respect to time, we obtain
dL
dt = Jω˙ + ω × (Jω), (1.6)
where the cross product ω × (Jω) results from the rotation of the body-fixed
coordinate system around the space-fixed coordinate system with velocity ω.
From equation (1.5) and equation (1.6), it follows that
Jω˙ = −ω × (Jω) + M. (1.7)
If the axes of the satellite’s body-fixed coordinate system are identical to
its principal axes of inertia, it holds that8 Chapter 1. Fundamentals of Nonlinear Systems
x
y
z
α
β
γ
ωz
ωx
ωy
z˜
x˜
y˜
Fig. 1.6: Satellite with a body-fixed coordinate system (x, y, z) and a space-fixed
coordinate system (˜x, y, ˜ z˜). The orientation of the satellite relative to the space￾fixed coordinate system is specified by the Euler angles α, β, γ.
J =


Jx 0 0
0 Jy 0
0 0 Jz

.
In this case, Euler’s rotation equations
Jxω˙ x = −(Jz − Jy)ωyωz + Mx,
Jyω˙ y = −(Jx − Jz)ωxωz + My,
Jzω˙ z = −(Jy − Jx)ωxωy + Mz
(1.8)
follow from equation (1.7) as the satellite’s equations of motion.
Let us now examine when ω˙ = 0 holds. The satellite has equilibrium points
only if at least two of the angular velocities ωx, ωy, and ωz are equal to zero.
We assume that the actuating variables Mx, My, and Mz are also equal to
zero. This results in three equilibrium sets, namely
ωeq1 =


ω1
0
0

, ωeq2 =


0
ω2
0

, ωeq3 =


0
0
ω3

.
Here the angular velocities ω1, ω2, ω3 ∈ IR may take arbitrary values. Obvi￾ously, an infinite number of equilibrium points exists. Note that1.1. System Description and System Behavior 9
ω1 = ω2 = ω3 = 0
is also possible. Thus ω = 0 is also an equilibrium point.
This example also illustrates that the term equilibrium point does not
necessarily imply the absence of movement, in this case of a rigid body. In￾stead, the term equilibrium point signifies that the states of the system under
consideration are not going to change with time.
Now we will describe the orientation of the satellite in the space-fixed
coordinate system. This is done by introducing the Euler angles α, β, and γ
[195, 221], which are depicted in Figure 1.6. By defining the vector
ϕ = [α β γ]
T
,
the motion equations of the satellite are given by

ϕ˙
ω˙

=

Λ(ϕ)ω
−J
−1
[ω × (Jω)]
+

0
J
−1

M, (1.9)
where the matrix
Λ(ϕ) =


1 sin(α) tan(β) cos(α) tan(β)
0 cos(α) − sin(α)
0 sin(α)/ cos(β) cos(α)/ cos(β)


transforms the angular velocity vector ω to the angular velocity vector ϕ˙ of
the space-fixed coordinate system. The transformation
ϕ˙ = Λ(ϕ)ω
is dependent on the Euler angles, i. e. on the vector ϕ. Obviously, the system
of equation (1.9) again possesses infinitely many equilibrium points

ϕeq
ωeq = 0

, ϕeq∈ IR3
,
if M = 0 holds for the actuator vector. In this case, however, the satellite
does not spin around any of its axes.
1.1.5 Equilibrium Points of Linear Systems
The linear systems
x˙ = Ax + Bu,
y = Cx + Du
are a particular case of the general system description
x˙ = f(x,u),
y = g(x,u)10 Chapter 1. Fundamentals of Nonlinear Systems
and will be considered briefly below to demonstrate an important difference
between them and nonlinear systems.
The equilibrium points of linear systems are easily determined by
x˙ = Axeq = 0
if u = 0 holds. The following cases may occur: if det(A) 6= 0 holds, only the
equilibrium point xeq = 0 exists. If det(A) = 0, the system A has zero-valued
eigenvalues and there is a linear subspace of vectors xeq, for which
Axeq = 0
holds. This means that a continuum of equilibrium points exists. A simple
example is the system
x˙ =

0 1
0 0
x +

0
1

u,
which consists of two integrators and is therefore described by the transfer
function 1/s2
that is shown in Figure 1.7. Obviously, all states
xeq =

a
0

, a ∈ IR,
are equilibrium points. This means that the x1-axis forms a continuum of
equilibrium points, as illustrated in Figure 1.7.
If u = c and det(A) = 0 holds, it is possible that no equilibrium point
exists. This could occur when
Ax = −Bc
is an overdetermined system of equations. Therefore, a linear system has either
one, zero, or a continuum of equilibrium points. The case of multiple isolated
equilibrium points, which is possible for nonlinear systems, does not occur in
linear systems.
u = 0 1 x2 x1
s
1
s
x2
x1
Continuum of
equilibrium points
Fig. 1.7: The system 1/s2
and its continuum of equilibrium points xeq = [a 0]T with
a ∈ IR1.1. System Description and System Behavior 11
1.1.6 Stability and Asymptotic Stability
An equilibrium point is said to be stable if all trajectories x(t) that begin in
a neighborhood of the equilibrium point converge to this point in the course
of time and remain there. In a weaker sense, the term stable is still used, even
if the trajectories do not terminate at the equilibrium point, but remain in a
neighborhood of the equilibrium point.
Let us consider a few examples of stable and unstable equilibrium points
below, in order to gain a preliminary impression and to develop a basic un￾derstanding of the different kinds of stability. The easiest way to achieve this
goal is by considering linear systems with the associated equilibrium point
xeq = 0. Since all solutions x(t) of the linear differential equation
x˙ = Ax
only involve terms given by
e
λit
and t
k
e
λj t with k ∈ {1, 2, 3, . . . },
all trajectories x(t) of the system obviously lead to the equilibrium point
xeq = 0 for t → ∞ if the inequality
Re {λi} < 0
holds for every eigenvalue λi of the system. In this case, both the equilibrium
point and the linear system are referred to as stable. Figure 1.8 shows the
trajectories of such a linear system.
However, if
Re {λi} > 0
holds for at least one of the eigenvalues of the system, the equilibrium point is
unstable. Then there are trajectories aiming away from the equilibrium point
and tending to infinity. Such a linear system is referred to as unstable. Figure
1.9 provides an example of the trajectories of an unstable system.
If a linear second-order system has a pair of complex conjugate eigenvalues
λ1/2 = ±j, the system is called a simple harmonic oscillator. Its trajectories
are shown in Figure 1.10. Obviously, the trajectories do not tend to the equi￾librium point xeq = 0 or to infinity. In this case, the equilibrium point xeq = 0
still possesses stability to a certain extent. A similar case is that of a second￾order system with one eigenvalue λ1 = 0 and one eigenvalue λ2 < 0, as shown
in Figure 1.11. Although all trajectories tend to an equilibrium point on the
x1-axis, none of the equilibrium points attracts all trajectories. On the other
hand, none of the trajectories tends to infinity. Therefore such systems are
also referred to as stable systems.
To develop a preliminary impression of stability conditions for nonlinear
systems, the system defined by12 Chapter 1. Fundamentals of Nonlinear Systems
x2
x1
Fig. 1.8: Trajectories of a stable lin￾ear system with eigenvalues λ1, λ2 with
Re {λ1} < 0, Re {λ2} < 0
x2
x1
Fig. 1.9: Trajectories of an unstable lin￾ear system with eigenvalues λ1, λ2 with
Re {λ1} > 0, Re {λ2} > 0
x2
x1
Fig. 1.10: Trajectories of an oscillator
with eigenvalues λ1/2 = ±j
x2
x1
Fig. 1.11: Trajectories of a system with
λ1 = 0 and λ2 < 0
x˙ 1 = x1(x2 − 1),
x˙ 2 = x2(x1 − 1) (1.10)
is regarded as an example. It possesses two equilibrium points at
xeq1 =

0
0

and xeq2 =

1
1

.
This demonstrates the previously mentioned difference regarding linear sys￾tems, because there are two isolated equilibrium points. This is not possible
for linear systems. They either have one equilibrium point at xeq = 0 or a
continuum of equilibrium points.
Figure 1.12 depicts the course of the trajectories of the system that is
described by equation (1.10) in the vicinity of the equilibrium points. Near
the origin, all trajectories tend toward the equilibrium point xeq1 = 0. This
equilibrium point can be referred to as being stable. The trajectories belonging1.1. System Description and System Behavior 13
-2
-1
0
1
2
-2 -1 0 1 2
State x1
State
x2
s
s
Fig. 1.12: Course of the trajectories of
the system that is described by equa￾tion (1.10).
x2
x1
x˜1
x˜2
xeq
Fig. 1.13: Transformation, more pre￾cisely translation, of an equilibrium
point xeq into the origin
to the equilibrium point xeq2 = [1 1]T
tend away from this point to infinity,
which means that this equilibrium point must be considered unstable.
This example demonstrates that nonlinear systems cannot generally be
classified as being stable or unstable, which is possible for linear systems.
Rather, we must consider the stability behavior of the system in the neigh￾borhood of an equilibrium point, i. e. the stability behavior of the equilibrium
point. If multiple equilibrium points exist, the term stability only refers to a
specifically considered equilibrium point and not to the entire system. In this
context there is a need for clarification of
(1) the behavior of trajectories in the neighborhood of an equilibrium point,
(2) the size of the region surrounding an equilibrium point, in which all tra￾jectories starting within the region tend to the equilibrium point, and
(3) the mathematical definition of the stability of an equilibrium point.
Before the above three issues are clarified, a simplification of the analysis
must be conducted. If an equilibrium point xeq has been determined for a
system, it can be shifted by an appropriate transformation
x = xeq + x˜
into the origin, i. e. to x˜ = 0. The system equations are then given by
x˜˙ = f(xeq+x˜,u) = f˜(x˜,u),
y = g(xeq+x˜,u) = g˜(x˜,u).
Since this transformation is always possible, we assume hereafter that the
equilibrium point of interest has been shifted to zero if it is not already at
this point. Figure 1.13 illustrates this transformation.14 Chapter 1. Fundamentals of Nonlinear Systems
In order to characterize the behavior of the trajectories of a system in the
neighborhood of an equilibrium point xeq = 0, we will first introduce the term
attractivity.
Definition 2 (Attractivity). Let a system
x˙ = f(x,u)
possess the equilibrium point xeq = 0. The equilibrium point xeq = 0 is called
locally attractive if a neighborhood U(0) exists around the equilibrium point
such that every initial vector
x(0) ∈ U(0)
leads to a trajectory x(t) of the free system, i. e. u = 0, which tends to the
equilibrium point xeq = 0 for t → ∞. If every trajectory of the free system
tends to zero for t → ∞, the equilibrium point is called globally attractive.
The set of initial points x(0) which lead to trajectories all tending to the
equilibrium point xeq = 0 is called the region of attraction.
Figure 1.14 illustrates the concept of attractivity. The attractivity of an
equilibrium point therefore ensures that every trajectory that begins at U(0)
tends to the equilibrium point. However, the term attractivity does not tell
us how far the trajectory departs from the equilibrium point xeq = 0. From a
practical point of view, this can be problematic. Basically, for real systems, we
would like to know which possibly dangerously large values the state x of the
system can take before converging to the equilibrium point. The subsequent
stability term is more detailed in this respect.
Definition 3 (Lyapunov Stability). Let a system
x˙ = f(x,u)
possess the equilibrium point xeq = 0. This equilibrium point is called locally
Lyapunov stable, or Lyapunov stable for short, if for every ε-neighborhood
Uε(0) = {x ∈ IRn
| |x| < ε}
a δ-neighborhood
Uδ(0) = {x ∈ IRn
| |x| < δ}
exists such that all trajectories x(t) of the free system which start within the
δ-neighborhood, i. e.
x(0) ∈ Uδ(0),
remain in the ε-neighborhood along their further course, i. e.
x(t) ∈ Uε(0) for all t > 0.
If, in addition, there is an ǫ-neighborhood Uε(0) for every δ-neighborhood
Uδ(0), the equilibrium point xeq = 0 is called globally Lyapunov stable.1.1. System Description and System Behavior 15
x2
x1
x0
x(t)
U(0)
Fig. 1.14: Attractive equilibrium
point
x2
x1
Uδ
Uε
Fig. 1.15: Illustration of the defi￾nition of Lyapunov stability
Figure 1.15 illustrates the above definition of stability by Lyapunov. Note
that the trajectories x(t) do not necessarily have to tend to the equilibrium
point xeq = 0 for an equilibrium point to be Lyapunov stable[1]. A specific
example of this case is the harmonic oscillator
x˙ =

0 1
−1 0
x,
whose trajectories x(t) we already saw in Figure 1.10 and whose equilibrium
point is globally Lyapunov stable.
As we know, an attractive equilibrium point is not necessarily Lyapunov
stable. This is illustrated by the system
x˙ 1 = x1

1 −
q
x
2
1 + x
2
2

−
x2
2
 
1 −
x1
p
x
2
1 + x
2
2
!
,
x˙ 2 = x2

1 −
q
x
2
1 + x
2
2

+
x1
2
 
1 −
x1
p
x
2
1 + x
2
2
!
.
(1.11)
It is not defined in x = 0, but can be continuously continued there by x˙ = 0.
This continued system possesses two equilibrium points
xeq1 =

0
0

and xeq2 =

1
0

,
neither of which is stable. However, in contrast to the equilibrium point xeq1,
the equilibrium point xeq2 is locally attractive.
Figure 1.16 illustrates this. All trajectories which begin in a neighbor￾hood of xeq1 tend away from this equilibrium point. On the other hand, all
trajectories with initial values
x(0) ∈ IRn
\{0}
[1] The terms stable and stable in the sense of Lyapunov are used as synonyms for
the term Lyapunov stable.16 Chapter 1. Fundamentals of Nonlinear Systems
-2
-1
0
1
2
-2 -1 0 1 2
State x1
State
x2
s s
Fig. 1.16: A system for which the equilibrium point x
T
eq2 = [1 0] is attractive but
not Lyapunov stable
converge to the attractive equilibrium point xeq2 for t → ∞. Notably, a special
trajectory xuc(t), which runs counterclockwise on the unit circle, also tends
to xeq2. In particular, we consider the case in which this trajectory, which is
given by
xuc(t) =




1 −
2
1 + z
2(t)
−
2z(t)
1 + z
2(t)




, z(t) = t
2
− sgn(x2(0)) ·
s
1 + x1(0)
1 − x1(0),
starts on the unit circle arbitrarily close to xeq2 with positive initial values
x1(0) and x2(0). Under such conditions, xuc(t) initially depart away from the
equilibrium point xeq2, and then, running on the unit circle, it tends to the
equilibrium point xeq2 for t → ∞. Since the trajectory first departs far from
xeq2 even if it starts arbitrarily close to xeq2 in the positive quadrant, it follows
that it is not possible for every ε-neighborhood Uǫ(xeq2) of xeq2 to define a
δ-neighborhood Uδ(xeq2), for which all trajectories that start within Uδ(xeq2)
remain in Uǫ(xeq2) along their further course. Therefore, the conditions of
Definition 3 are not fulfilled and therefore the equilibrium point xeq2 is not
Lyapunov stable.
For practical applications in control, equilibrium points which are both
Lyapunov stable and attractive are of central importance. This is because the
combination of these two properties ensures that a system can permanently
maintain a specific nominal state, i. e. an equilibrium point. Accordingly, the
term asymptotic stability is defined in
Definition 4 (Asymptotic Stability). If an equilibrium point xeq = 0 is
locally (globally) attractive and locally (globally) Lyapunov stable, it is called
locally (globally) asymptotically stable.1.1. System Description and System Behavior 17
-2
-1
0
1
2
-2 -1 0 1 2
U1
U2
State x1
State
x2
s
s
Fig. 1.17: Asymptotically stable equilibrium point xeq = 0 and a corresponding
catchment region U1 (blue). The neighborhood U2 is not a catchment region.
For an asymptotically stable equilibrium point xeq, the neighborhood
U(xeq) within which all starting trajectories tend to the equilibrium point
is also of interest. Not every neighborhood has this property, as illustrated
by the example of equation (1.10) and the accompanying Figure 1.17. In the
neighborhood U1 of the equilibrium point xeq = 0, all trajectories x(t) tend
to the origin. For the neighborhood U2, it is clearly visible that this is not the
case.
For such situations, we define the term catchment region as follows:
Definition 5 (Catchment Region). A neighborhood of an asymptotically
stable equilibrium point is called a catchment region if all trajectories that begin
within this neighborhood both remain within it and tend to the equilibrium point
in their further course.
Definition 6 (Region of Asymptotic Stability). The largest catchment
region is called the region of asymptotic stability.
The region of asymptotic stability is also often called the basin of the equilib￾rium point.
If there is only one globally asymptotically stable equilibrium point, then
the entire state space is the catchment region. From the engineering point
of view, this is the desirable case, and engineers attempt to achieve it in
control-loop design and other applications. In any case, the size of an equilib￾rium point’s region of asymptotic stability is of great importance in practical
applications.
So far, we have discussed the stability of equilibria. However, we are also in￾terested in the system’s stability behavior as a whole. In the case of a system
with a globally asymptotically stable equilibrium point, this is straightfor￾ward, and we can formulate18 Chapter 1. Fundamentals of Nonlinear Systems
Definition 7 (Asymptotic Stability of a System). A system with a glob￾ally asymptotically stable equilibrium point is called asymptotically stable.
However, the situation is more complex in the case of multiple equilibrium
points. This is why we will now introduce a more generalized definition of the
stability of a system:
Definition 8 (Stability of a System). Let us consider a system
x˙ = f(x,u).
We call the system stable if the following conditions hold for the free system:
(1) There is a bounded set S in which all trajectories remain during their
course.
(2) There is a bounded set H ⊃ S for every bounded set G ⊃ S such that all
trajectories beginning in H remain in G during their course.
(3) There is a set G ⊃ S for every set H.
This definition enables us to estimate the behavior of the system as a whole
and not just its behavior around its equilibrium points. Notice that Definition
8 provides us with a global definition of stability which is similar to Lyapunov’s
global definition of stability. This kind of stability of a system ensures that
there is no solution tending to infinity. However, it does not ensure that all
trajectories converge to a single equilibrium point. It is also possible that
trajectories do not run into an equilibrium point and therefore x˙(t) does not
become zero. The simple harmonic oscillator, for example, is a stable system.
If we want to ensure that the trajectories of a stable system all converge
to a bounded set, we have to formulate a stricter version of Definition 8. We
then obtain
Definition 9 (Strict Stability of a System). We call a stable system
x˙ = f(x,u) strictly stable if there is a bounded set S such that x(t → ∞) ∈ S
holds for all trajectories x(t) of the free system.
It is worth noting that the size of the set S is not specified in Definition 9.
It can be of any size. In practical applications, however, the size of the set S
does matter. In most cases, it should be as small as possible. As in Definition
8, the trajectories x(t) within the set S do not have to converge or tend to
stable equilibrium points. System (1.11), for example, is strictly stable even
though its two equilibrium points are unstable.
1.1.7 Exponential Stability of Equilibrium Points
The term asymptotically stable signifies that the trajectories x(t) of a system
for t→∞ tend to the corresponding equilibrium point. However, it does not
provide us with a measure of how quickly this happens. For example, let us
consider the linear asymptotically stable system1.1. System Description and System Behavior 19
x˙ = −λx, x(0) = x0, λ > 0.
The system’s solution is given by
x(t) = x0e
−λt
.
We can see that the solution of the system not only tends asymptotically to
zero but also decreases exponentially fast.
Accordingly, for all systems whose solutions decrease toward zero expo￾nentially or faster than exponentially, we can now define the term exponential
stability.
Definition 10 (Exponential Stability). If a system x˙ = f(x,u) pos￾sesses an asymptotically stable equilibrium point at x = 0, this equilibrium
point is called locally exponentially stable if positive constants m, α, and δ
exist such that
|x(t)| ≤ me−αt|x(0)| (1.12)
holds for the free system for all |x(0)| < δ and all t ≥ 0. If, additionally,
the above inequality is fulfilled for every initial vector x(0) of the system, the
equilibrium point is called globally exponentially stable.
In this context, we will point out that an exponentially stable equilibrium
point is always asymptotically stable, i. e.
exponentially stable ⇒ asymptotically stable.
Furthermore, the asymptotically stable equilibrium point xeq = 0 of a linear
system is exponentially stable, since all eigen-motions, i. e. the solutions of the
linear system for u = 0, decay exponentially. The largest possible constant α
in equation (1.12) is called the convergence rate of the system.
As an example, let us consider the system
x˙ = −x
3
, x(0) = x0,
and its equilibrium point xeq = 0. The equilibrium point is asymptotically
stable but not exponentially stable. We can deduce this from the system so￾lution
x(t) = p
x0
2x
2
0
t + 1
because there is a time t0 for every m, α > 0 so that the inequality
|x(t)| =
|x0|
p
2x
2
0
t + 1
> me−αt|x0|, t > t0, (1.13)20 Chapter 1. Fundamentals of Nonlinear Systems
is fulfilled. This is because equation (1.13) is equivalent to
1
m
e
αt >
q
2x
2
0
t + 1, t > t0.
Hence, the system’s equilibrium point xeq = 0 is not exponentially stable
because equation (1.12) from Definition 10 is not fulfilled.
We can determine that for linear systems, asymptotic stability entails
exponential stability. On the other hand, there are nonlinear systems with
asymptotically stable equilibrium points that are not exponentially stable.
1.1.8 Instability of Equilibrium Points
Intuitively, we expect that an equilibrium point which is not stable, or more
precisely formulated, which is not Lyapunov stable, should be called unstable.
Such an equilibrium point is characterized by at least one trajectory which
begins in an arbitrarily small neighborhood of the equilibrium point and then
departs from it. Accordingly, we can formulate
Definition 11 (Instability). An equilibrium point is called unstable if it is
not Lyapunov stable.
However, we must point out that unstable equilibrium points may well be
attractive, as illustrated by our example of system (1.11). Its two equilibrium
points are unstable; nevertheless, its equilibrium point
xeq2 =

1
0

is attractive. This means that all trajectories which start within an arbitrar￾ily small neighborhood around it finally converge to it. In their course, the
trajectories are bounded and never depart from the region that lies inside a
circle with a diameter greater than one after they have reached it. Thus no
trajectory tends to infinity.
However, systems exist with an equilibrium point for which trajectories
which start within an arbitrarily small neighborhood around it not only depart
from the equilibrium point, but also tend to infinity. From an engineering
perspective, such behavior is more dangerous for the functionality of a system
than the case of an unstable equilibrium point, for which no trajectory tends
to infinity. It is thus reasonable to introduce an instability definition that goes
beyond the concept given in Definition 11.
Definition 12 (Strict Instability). Let a system x˙ = f(x,u) possess an
equilibrium point at x = 0 for u = 0. This equilibrium point is called strictly
unstable if, for every neighborhood U of the equilibrium point, there is at least
one trajectory x(t) of the free system which begins at x(0) ∈ U and has the
limit1.1. System Description and System Behavior 21
lim
t→tinf
|x(t)| = ∞
for a time tinf ∈ (0, ∞].
Of course, any strictly unstable equilibrium point is also unstable, but not
vice versa. For example, the equilibrium points of system (1.11) are unstable,
but not strictly unstable. Examples of strictly unstable equilibrium points are
the equilibrium points xeq = 0 of linear systems with eigenvalues that possess
a positive real part.
As a further example, let us consider the system
x˙ 1 = x
2
1 − x
2
2
,
x˙ 2 = 2x1x2
(1.14)
with the initial values x1(0) = x10 and x2(0) = x20. It possesses the solution
x1(t) = x10 −
￾
x
2
10 + x
2
20
t
(x
2
10 + x
2
20)t
2 − 2x10t + 1
,
x2(t) = x20
(x
2
10 + x
2
20)t
2 − 2x10t + 1
(1.15)
and a single equilibrium point at x = 0. Its trajectories x(t) are in circular
shape and all tend to the equilibrium point xeq = 0. They are shown in
Figure 1.18.
Although all trajectories tend to the equilibrium point xeq = 0, the system
is unstable, since it does not fulfill the definition of Lyapunov stability that
was given in Definition 3. In addition, the equilibrium point is also strictly un￾stable, since all trajectories beginning on the positive x1-axis, i. e. trajectories
with initial values x10 > 0 and x20 = 0, are given by
-10
-5
0
5
10
-10 -5 0 5 10
State x1
State
x2
s
Fig. 1.18: Trajectories of the system given in equation (1.1422 Chapter 1. Fundamentals of Nonlinear Systems
x(t) =



x10
1 − x10t
0


. (1.16)
For
t → tinf =
1
x10
,
they tend to infinity. If we now consider the time progression of the state x1
for x10 > 0, as shown in Figure 1.19, we observe that the state x1(t) first
reaches infinity for t = 1/x10, and subsequently returns from infinity and
tends asymptotically to zero via the negative x1-axis for
t > tinf =
1
x10
.
If we include the point infinity in the progression of the trajectories, it follows
that every trajectory (1.16) ultimately tends to zero. Below we will examine
this behavior in more detail.
First we will consider the trajectory points
x(t) = 
0
x2(t)

for all trajectories. We compute them for the initial vector
x0 =

x10
x20 
=

1
x20 
with x20 > 0
and thereby obtain the trajectory points
xs =


0
x
2
20 + 1
x20

,
at which the trajectories x(t) intersect the x2-axis.
We note that the smaller the initial value x20 with
0 < x20 ≤ 1
becomes, i. e. the more closely it approaches the x1-axis, the larger the inter￾section point
xs2 =
x
2
20 + 1
x20
with the x2-axis becomes. This intersection point coincides with the diameter
d = xs21.1. System Description and System Behavior 23
tinf
x10
x1(t)
0 t
Fig. 1.19: Time course of the solution
x1(t) for x10 > 0 and x20 = 0
1
d
0 x20
Fig. 1.20: Diameter d of the trajectory
with dependence on x20
of the associated circular trajectory. Figure 1.20 illustrates this progression.
For the limit x20 → 0, the diameter and the intersection point of the associated
trajectory with the x2-axis become infinite. We can therefore envision this
trajectory as starting on the x1-axis with the initial values x10 = 1 and x20 = 0
and progressing along this axis to infinity. It follows the other trajectories in
their circular path, but passes through infinity from the positive real axis to
the negative real axis and then to the equilibrium point xeq = 0. Thus this
trajectory progresses along a circle of infinite radius. However, we have not
taken into account the fact that infinity does not belong to the set of real
numbers. This means that the trajectory x(t), t ∈ IR, is not defined for
t = tinf =
1
x0
in our example.
In the following, we will therefore be mathematically exact and utilize
a result from topology. We extend the real plane through a so-called one￾point compactification [56] by including infinity. This can be illustrated as a
stereographic projection which uniquely maps the real plane, which has been
extended to include infinity, onto a sphere. Here the sphere is tangent to the
plane that lies beneath it. Figure 1.21 shows this. From the north pole of
the sphere, we can draw a line to every point xp on the plane and obtain
an intersection point xc with the surface of the sphere. The point xc is a
mapping of the point xp lying on the plane. In this way, every point on the
plane can be bijectively mapped to a point on the surface of the sphere. The
north pole corresponds to infinity in the extended plane, while the south pole
corresponds to the origin of the extended plane. The trajectory that starts in
the plane on the x1-axis at x10 > 0 is projected into the north pole of the
sphere when it progresses to infinity. From there, it progresses further down
to the opposite side of the sphere to its south pole, which corresponds to the24 Chapter 1. Fundamentals of Nonlinear Systems
x1
x2
xc
xp
North pole
Fig. 1.21: Illustration of the one-point compactification of IR2
. The north pole cor￾responds to infinity in the real plane.
progression of the trajectory on the negative real axis to the equilibrium point
xeq = 0. In conclusion, all trajectories (1.15) tend to the equilibrium point
xeq = 0 for t → ∞. The equilibrium point is therefore globally attractive. As
noted previously, however, it is also strictly unstable.
Thus systems with attractive, but unstable, or even strictly unstable equi￾librium points also exist. We will conclude our descriptions of stability and
instability with a visual summary of their relations. For this purpose, Figure
1.22 illustrates the sets
Mus = set of unstable equilibrium points,
Msus= set of strictly unstable equilibrium points, Msus⊂Mus,
MLs = set of Lyapunov stable equilibrium points, MLs∩Mus=∅,
Mas = set of asymptotically stable equilibrium points, Mas⊂MLs,
Mes = set of exponentially stable equilibrium points, Mes⊂Mas,
Matt= set of attractive equilibrium points, Mas ⊂Matt, Matt∩Msus 6=∅.
At this point, it becomes clear once again that the term attractivity is unsuited
to describe the stability behavior of an equilibrium point.1.1. System Description and System Behavior 25
Lyapunov stable, MLs
Asymptotically stable, Mas
Exponentially stable, Mes
Unstable, Mus
Strictly unstable, Msus
Attractive, Matt
Fig. 1.22: The sets describing different types of stable, unstable, and attractive
equilibrium points and the relations among them
1.1.9 Stability in the Case of Variable Input Signals
So far we have dealt with the stability of the equilibrium point xeq = 0 of a
system
x˙ = f(x,u),
y = g(x,u)
for u = 0. In addition, we are also interested in how the state vector x(t)
and the output variable vector y(t), or more precisely their absolute values
|x(t)| and |y(t)|, change with dependence on the input signal u(t). From the
engineer’s practical point of view, it is desirable for bounded input signals to
produce bounded output signals and bounded state variables.
In simple words, we term a system input-to-state stable [430] if the course
of the state vector x(t) remains bounded for a bounded course of the input
variable vector u(t). For mathematical purposes, we will formulate this more
precisely in
Definition 13 (Input-to-State Stability). A system
x˙ = f(x,u)
is called input-to-state stable if, for all its initial state vectors x0 = x(0) and
for every bounded input function u(t),
(1) there exists a function β(|x0|, t) with β(0, t) = 0 for all t ≥ 0 and
β(|x0|, t → ∞) = 0 for all x0 ∈ IRn
, which is a strictly increasing continu￾ous function of its first argument |x0| and a strictly decreasing continuous
function of its second argument t, and26 Chapter 1. Fundamentals of Nonlinear Systems
(2) there exists a strictly increasing continuous function γ with γ(0) = 0
such that
|x(t)| ≤ β(|x0|, t) + γ(sup
t≥0
(|u(t)|))
holds for all t ≥ 0.
In the literature, input-to-state stability is usually abbreviated to ISS. The
above definition of stability ensures that the state vector x(t) takes values
that lie within a hypersphere with radius
β(|x0|, t) + γ(sup
t≥0
(|u(t)|)),
therefore remaining bounded for bounded input signals u(t) and bounded
initial values x0. If we set u = 0 in the defining equation of input-output
stability, we see that all trajectories x(t) of an input-to-state stable system
tend to zero for t → ∞. Consequently, an input-to-state stable system has
a globally asymptotically stable equilibrium point. However, in general, the
reverse conclusion does not hold.
Below we will determine the functions β and γ for a linear system
x˙ = Ax + Bu (1.17)
using the system’s solution [133]
x(t) = e
Atx0 +
Z
t
0
e
A(t−τ)Bu(τ)dτ. (1.18)
Based on this, we obtain an upper bound of the absolute value x(t) using
|x(t)| = |e
Atx0 +
Z
t
0
e
A(t−τ)Bu(τ)dτ| ≤ |e
Atx0| + |
Z
t
0
e
A(t−τ)Bu(τ)dτ|.
(1.19)
Firstly, the inequality
|e
Atx0| ≤ me−αt · |x0| = β(|x0|, t), for some m, α > 0 , (1.20)
holds if system (1.17) is asymptotically stable and is thus also exponentially
stable, since all asymptotically stable linear systems are exponentially stable.
Here m and α must be chosen appropriately. The function
β(|x0|, t) = me−αt · |x0|
is a strictly increasing continuous function of its first argument |x0| and a
strictly decreasing continuous function of its second argument t. We have
therefore found an appropriate function β according to Definition 13.1.1. System Description and System Behavior 27
Secondly, the following holds for the integral in equation (1.18)
|
Z
t
0
e
A(t−τ)Bu(τ)dτ| ≤ Z
t
0
|e
A(t−τ)Bu(τ)|dτ ≤
Z
t
0
me−α(t−τ)
|Bu(τ)|dτ.
(1.21)
With µ denoting the largest eigenvalue of the matrix B
T B, inequality
|Bu(t)| ≤ √
µ · sup
t≥0
(|u(t)|)
holds [40], hence
Z
t
0
me−α(t−τ)
|Bu(τ)|dτ ≤ m
√
µ sup
t≥0
(|u(t)|) ·
Z
t
0
e
α(τ−t)
dτ
= m
√
µ sup
t≥0
(|u(t)|)
1
α
(1 − e
−αt)
≤
m
√
µ
α
sup
t≥0
(|u(t)|) = γ(sup
t≥0
(|u(t)|)).
Using the bounds given in equations (1.19), (1.20), and (1.21), we obtain
inequality
|x(t)| ≤ me−αt · |x0|
| {z }
β(|x0|, t)
+
m
√
µ
α
sup
t≥0
(|u(t)|).
| {z }
γ(sup
t≥0
(|u(t)|))
Using Definition 13, from the asymptotic stability of a linear system (1.17),
i. e. α > 0, we can now conclude its input-to-state stability. Thus we arrive at
Theorem 1 (Input-to-State Stability of Linear Systems). A linear
system is input-to-state stable if and only if it is asymptotically stable.
As mentioned above, this equivalence does not generally apply to nonlinear
systems; additionally, the functions β and γ usually cannot be determined for
nonlinear systems. To illustrate the first, let us consider the system
x˙ = −x(1 − u).
For the input value u = 0, this system possesses a globally asymptotically
stable equilibrium point. For values u > 1, however, this system becomes
strictly unstable, which means that it is not input-to-state stable.
Analogously to the input-to-state stability, we can define the input-output
stability as follows [432, 433]:28 Chapter 1. Fundamentals of Nonlinear Systems
Definition 14 (Input-Output Stability). A system
x˙ = f(x,u),
y = g(x,u)
is called input-output stable if, for all its initial state vectors x0 = x(0) and for
every bounded input function u(t), there exist two functions β and γ according
to Definition 13, so that
|y(t)| ≤ β(|x0|, t) + γ(sup
t≥0
(|u(t)|))
holds for all t ≥ 0.
Again, the case of linear systems is simple. We obtain the only sufficient
Theorem 2 (Input-Output Stability of Linear Systems). An asymp￾totically stable linear system is input-output stable.
However, if a linear system has a coprime transfer function, the input-to￾state stable, input-output stable, and asymptotically stable properties are
equivalent to each other [427]. For the case of linear systems, the term Bounded
Input-Bounded Output stable (BIBO stable) is commonly used as a synonym
for input-output stability.
The simple example of the nonlinear system
x˙ = −x + u,
y =
1
x
illustrates that not every system with a globally asymptotically stable equi￾librium point xeq = 0 is also input-output stable. If the trajectory x(t) of this
example tends to the globally asymptotically stable equilibrium point xeq = 0,
the output variable y(t) becomes infinitely large.
1.1.10 Limit Cycles
In nonlinear systems, just as in linear systems, sustained oscillations may
occur. For these oscillations, the system states are repeated periodically and
the trajectory of a sustained oscillation is a closed curve. These oscillations
are called limit cycles.
As an example, Figure 1.23 illustrates the sustained oscillation of the Van
der Pol differential equation
x¨1 − (1 − x
2
1
) ˙x1 + x1 = 0.
The state-space model1.1. System Description and System Behavior 29
3
2
1
0 0
-1
-2
-3
5
-5
0 10 20 30 40 -5 0 5
Time t in s State x1
State
x1
State
x2
s
Fig. 1.23: The graph on the left shows the temporal variation of x1(t); on the right,
the trajectories x(t) and the limit cycle of the Van der Pol differential equation.
e u x1
u = −(1 − e
2
) ˙e x¨1 + x1 = u
Fig. 1.24: Van der Pol oscillator, represented as a control loop
x˙ 1 = x2,
x˙ 2 = −x1 + (1 − x
2
1
)x2
is equivalent to the differential equation above.
The Van der Pol differential equation describes situations such as the be￾havior of an electric oscillating circuit for radio stations consisting of a triode,
a capacitor, a coil, and a resistor. Here the term (1 − x
2
1
) acts as a nonlinear
attenuator. We can also represent a Van der Pol oscillator as a control loop
with a control error given by e = −x1 and a nonlinear characteristic curve
u = f(e, e˙) = −(1 − e
2
) ˙e
as the control law. The linear differential equation
x¨1 + x1 = u
is the plant, as shown in Figure 1.24. The purpose of the control loop, in this
case, is not to control the trajectory into an equilibrium point, but to main￾tain an oscillation. In this example, the limit cycle is produced deliberately.
Normally, however, limit cycles are undesired in control loops. Generally, the30 Chapter 1. Fundamentals of Nonlinear Systems
Stable limit cycle Semi-stable limit cycle Unstable limit cycle
Fig. 1.25: Limit cycles (black) and their stability behavior
purpose of a control loop is to keep the controlled variable constant and not
to generate oscillations.
Similar to equilibrium points, the trajectories either tend to a limit cycle
or away from it. The term stability can thus be applied to limit cycles. Three
cases can be distinguished. Firstly, there are asymptotically stable limit cycles,
toward which all trajectories in the closer vicinity converge. Secondly, there
are semi-stable limit cycles for which all trajectories on one side of the limit
cycle converge and the trajectories on the other side tend away from the
limit cycle. In the third case, all trajectories originating from the vicinity of
the limit cycle diverge, so it is called unstable. Figure 1.25 illustrates these
cases. In linear systems, neither stable, unstable, nor semi-stable limit cycles
can occur. In these cases only harmonic oscillators are possible, for which an
infinite number of closed trajectories exist. No other trajectories approach or
tend away from these trajectories.
Unstable and semi-stable limit cycles are of no practical significance, since
the trajectory always departs from the limit cycle when very small distur￾bances, which are always present in a real system, occur. However, what is
most relevant to control theory is the stable limit cycle. In general, as men￾tioned before, it is undesired. To detect limit cycles in control loops, we typ￾ically use the method of harmonic balance, which will be discussed in detail
in Section 2.1.
1.1.11 Sliding Modes
Apart from limit cycles, nonlinear systems exhibit further behavior which
does not exist in linear systems. Sliding modes belong to this category of
phenomena. They occur in systems with unsteady behavior, e. g. in systems
x˙ = f(x,u) with discontinuous functions f.
To explain the sliding mode phenomenon, we will consider the plant
x˙ =

0 1
−2 −3

x +

0
1

u,
y =

0.5 1
x,
which possesses the transfer function1.1. System Description and System Behavior 31
1
-1
u s + 0.5 y
(s + 1)(s + 2)
Fig. 1.26: Control loop with two-position controller
Output variable y(t) Control variable u(t)
1.5
1
1
0.5
0.5
0
0 -0.5
-0.5 -1
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
y
u
Time t in s Time t in s
Fig. 1.27: Temporal course of the output variable y(t) and control variable u(t)
G(s) = s + 0.5
(s + 1)(s + 2).
As a controller, we use a two-position controller
u =
(
1, y ≤ 0,
−1, y > 0,
which is sometimes termed a bang-bang controller. Figure 1.26 shows the cor￾responding control loop. Simulating the system for the initial values x1(0) = 1
and x2(0) = 1, we obtain the courses of the output variable y and the control
variable u which are plotted in Figure 1.27. Obviously, the output variable y
tends to zero. The control variable u, on the other hand, from a certain point
onward does not remain constant but switches back and forth between the
values u = 1 and u = −1 with a high frequency.
This behavior can be explained by considering the trajectories x(t) of
the system in the phase plane, as depicted in Figure 1.28, and analyzing the
control law
u =
(
1, x2 ≤ −0.5x1,
−1, x2 > −0.5x1.32 Chapter 1. Fundamentals of Nonlinear Systems
Trajectories x(t)
without sliding mode
with sliding mode
State variables x1(t) and x2(t)
State
x2
State x1
x1 and
x2
Time t in s
x1
x2
1
1
0.5
0.5
0
0
-0.5
-0.5
-1
1.5
-1 -0.5 0 0.5 1 0 2 4 6 8 10
Fig. 1.28: Trajectories x(t) and sliding mode on the (blue) switching line and time
courses of the states x1(t) and x2(t)
Obviously, the actuator signal takes the value of u = 1 below the line defined
by
x2 = −0.5x1 (1.22)
and takes the value u = −1 above this line.
The trajectories x(t) converge from both sides onto the straight line de￾scribed by equation (1.22). If a trajectory impinges onto the line, it briefly
switches to the other side and the actuator signal jumps from u = 1 to u = −1,
or vice versa. The trajectory then again impinges onto the line, briefly switches
sides, and the process starts all over again. This explains the previously ob￾served high-frequency switching of the actuator signal. The trajectory itself
slides along the switching line, accompanied by high-frequency switching of
the actuator signal, into the equilibrium point
xeq = 0.
The term sliding mode is derived from this sliding behavior.
The behavior described above has the disadvantage that the actuator,
which can be a valve or some other mechanical actuator, is heavily stressed
and quickly wears out. Therefore sliding modes are usually undesired and
should be avoided.
However, the sliding state also has an advantage. It can be shown that
the sliding of the trajectory x(t) into the equilibrium point is robust against
changes of the parameters of the plant. This means that the control system
that is in a sliding mode always has the same dynamic, even if the plant
changes. This behavior can therefore be exploited for the design of a certain
class of robust controllers, the sliding mode controllers. We will examine this
topic in Section 6.2.1.1. System Description and System Behavior 33
1.1.12 Chaos
Chaos occurs in biological, meteorological, economic, and technical systems
[50, 233, 451]. Specific examples of this are economic cycles, single-mode lasers,
micromechanical oscillators, and the development of populations in ecologi￾cal systems. The most important characteristic of a chaotic system is that
we cannot say exactly how its state variables will develop in future. This is
somewhat surprising, since chaotic systems can be described by ordinary dif￾ferential equations with a deterministic behavior. In this context, the term
deterministic excludes any kind of stochastic influences on the system.
Intuitively explained, chaotic behavior can be characterized by the follow￾ing three properties:
(1) The trajectories run aperiodically, i. e. they do not tend to limit cycles.
(2) The trajectories do not tend to an equilibrium point or to infinity.
(3) Arbitrarily close initial values or vectors lead to very different progressions
of trajectories.
As an example of a chaotic system, we will consider the double pendulum,
which is shown in Figure 1.29. The masses m1 and m2 are appended from
two pendulum rods of lengths l1 and l2, respectively. The joints allow for a
free rotation of the pendulum; that is, the rotation angles Θ1 and Θ2 are not
limited by stops. We will assume ideal conditions so that there is no friction
Θ1
Θ2
l1
l2
g
m1
m2
Fig. 1.29: Double pendulum with chaotic behavior34 Chapter 1. Fundamentals of Nonlinear Systems
or other energy losses. Due to the gravitational acceleration g = 9.81 m s−2
,
which acts upon the pendulum masses, the lossless double pendulum has a
Lyapunov stable, but not asymptotically stable, equilibrium point for the
angles
Θ1 = Θ2 = 0.
The system is described by the second-order differential equations [333]
Θ¨
1 =
g [sin(Θ2) cos(∆Θ) − µ sin(Θ1)] − sin(∆Θ)
h
l2Θ˙ 2
2 + l1Θ˙ 2
1
cos(∆Θ)
i
l1 [µ − cos2(∆Θ)] ,
Θ¨
2 =
µg [sin(Θ1) cos(∆Θ) − sin(Θ2)] + sin(∆Θ)
h
µl1Θ˙ 2
1 + l2Θ˙ 2
2
cos(∆Θ)
i
l2 [µ − cos2(∆Θ)]
with
∆Θ = Θ1 − Θ2,
µ = 1 +
m1
m2
.
For a simulation, shown in Figure 1.30, we choose m1 = m2 = 1 kg and
l1 = l2 = 1 m as the system’s parameter values. The chaotic behavior of the
double pendulum becomes evident if we consider two closely spaced initial
vectors




Θ1(0)
Θ˙
1(0)
Θ2(0)
Θ˙
2(0)




=




π/2
0
−π/2
0.01




and





Θ˜
1(0)
˜˙Θ1(0)
Θ˜
2(0)
˜˙Θ2(0)





=




π/2
0
−π/2
0.0101




Θ1,
˜Θ1 in rad
Time t in s
2
1
0
-1
-2
0 5 10 15 20 25 30 35 40 45 50
Fig. 1.30: Time courses of the angles Θ1 (black curve) and Θ˜1 (blue curve) of the
double pendulum with chaotic behavior1.1. System Description and System Behavior 35
and compare the two resulting progressions of the angles Θ1(t) and Θ˜
1(t). It
can be seen from Figure 1.30 that, after a while, the angles progress entirely
differently and without any recognizable pattern, even though their initial
conditions were nearly identical.
Technical systems very rarely exhibit chaotic behavior. This is because the
developers and users of technical systems are generally interested in designing
their devices, processes, or vehicles in such a way that they exhibit predictable
behavior. In Section 5.6, we will consider another chaotic technical system,
a fluid system, and how to control it in such a way that it has a globally
asymptotically stable equilibrium point and, consequently, no longer exhibits
chaotic behavior. It turns out that a chaotic system is no more difficult to
control than a conventional one. A detailed description of chaotic systems can
be found in [19, 20, 77, 92, 354, 436, 452].
1.1.13 Discrete-Time Systems
Apart from continuous-time systems, in control engineering and systems the￾ory, we also encounter systems for which the state variables and the signals of
the inputs and outputs are not observed or measured continuously over time,
but only at certain, meaning discrete instants k [143, 200, 201, 231, 259, 341].
Such discrete-time systems are often the result of sampling continuous-time
systems, which is typically the case with feedforward and feedback control sys￾tems in industrial plants where the control is implemented via a programmable
logic controller. The discrete-time sequence, however, can also be an inherent
property of the system. An example are radars that only detect an object
after a full revolution.
These discrete-time systems are described by difference equations
x(k + 1) = f(x(k),u(k)),
y(k) = g(x(k),u(k)) (1.23)
with k = 0, 1, 2, . . . Analogously to the continuous-time systems, x is the n -
dimensional state vector, u denotes the m - dimensional input variable vector,
and y is the r - dimensional output variable vector.
The equilibrium points xeq of a discrete-time system are defined by the
equation
xeq = f(xeq, 0).
In this, as in the continuous-time case, without loss of generality, u = 0 is
assumed.
The stability of an equilibrium point can be defined in a manner similar
to the continuous-time case. Assuming again that the equilibrium point lies
at x = 0, we can formulate the following definition:36 Chapter 1. Fundamentals of Nonlinear Systems
Definition 15 (Stability of Equilibrium Points of Discrete-Time Sys￾tems). Let a system
x(k + 1) = f(x(k),u(k))
possess the equilibrium point xeq = 0. This equilibrium point is called Lya￾punov stable if, for every ε-neighborhood
Uε(0) = {x ∈ IRn
| |x| < ε} ,
there exists a δ-neighborhood
Uδ(0) = {x ∈ IRn
| |x| < δ}
such that all sequences of states x(k) of the free system which begin in the
δ-neighborhood, i. e.
x(0) ∈ Uδ(0),
in their further progression, remain within the ε-neighborhood, i. e.
x(k) ∈ Uε(0) for k > 0.
If the condition
lim
k→∞
x(k) = 0
is additionally fulfilled for all x(0) ∈ Uδ(0), the equilibrium point is called
asymptotically stable.
Limit cycles can occur in discrete-time systems, just as in continuous-time
systems. This is the case if for a sequence of state vectors x(0), x(1), . . . , x(l),
the relation
x(0) = x(l)
is fulfilled for a constant input variable vector u(k). Since the limit cycle
x(0), x(1), . . . , x(l) = x(0)
in total comprises l + 1 states, this is referred to as a cycle or periodic orbit
of length l + 1.
Chaotic behavior is also possible. For example, chaos occurs in a discrete￾time system of order one if a cycle
x(0), x(1), x(2) = x(0),
i. e. a cycle of length three, exists [276]. Of course, the cycle itself is not chaotic.
However, as soon as the state departs marginally from the values x(0), x(1),
x(2) of the cycle, the course x(k) becomes chaotic. It is worth mentioning
that chaos can occur in discrete-time systems of order n = 1 or larger. In au￾tonomous continuous-time systems, however, chaos can only arise in systems
of order n = 3 or larger.1.2. Solution of Nonlinear Differential Equations 37
In most cases, it is impossible to model a sampled continuous-time nonlin￾ear system exactly using a difference equation (1.23). This is completely dif￾ferent from the situation with linear systems, where the discrete-time model
that results from the sampling of the continuous-time model can be calculated
from the continuous-time model in a simple and efficient way. This simple cal￾culation results from the knowledge of the solution of the linear differential
equation.
In the nonlinear case, we do not usually have the system solution available,
and therefore we cannot determine the discrete-time model, at least not ex￾actly. However, an approximation is possible. Using the numerical solutions of
nonlinear differential equations, it is possible to determine such approximate
models for sampled nonlinear systems. The focus of the next sections is on
solving nonlinear differential equations.
1.2 Solution of Nonlinear Differential Equations
1.2.1 Existence of Solutions
For linear systems and for their associated initial-value problems
x˙ = Ax + Bu
x(0) = x0,
we know the solution. It can be explicitly stated and, as we know from linear
control theory [133], it is given by
x(t) = e
Atx0 +
Z
t
0
e
A(t−τ)Bu(τ)dτ.
In nonlinear systems, the task of determining a solution becomes significantly
more difficult. In most cases, the solution cannot even be specified explicitly. If
it is nonetheless possible, we can attempt this by trial and error, transforming
or substituting variables, and integrating the differential equation. For some
differential equations, such as the Bernoulli and the d’Alembert differential
equations, solutions or solution methods are known. We will deal with the
most well-known methods in the exercises. An extensive collection of analyti￾cally solvable differential equations and their solutions can be found in [364].
However, there is no systematic general method for determining an analytical
solution. For this reason, we have to use numerical solution methods more of￾ten than not. Before we deal with these, we will clarify the conditions sufficient
for an ordinary differential equation to be solvable.
We will consider the non-autonomous differential equations
x˙ = f˜(x(t),u(t)). (1.24)38 Chapter 1. Fundamentals of Nonlinear Systems
The time-varying input function u(t) can also be interpreted, in its effect, as
a time-dependence of the function f, i. e.
f(x(t), t) = f˜(x(t),u(t)).
Equations of the (1.24) type are thus included in
x˙ = f(x(t), t), (1.25)
provided the input signal u(t) is known.
Determining a solution of the differential equation (1.25) requires that a
solution actually exists. Whether a solution exists is shown by [327]
Theorem 3 (Peano Existence Theorem). Let
x˙ = f(x, t), x ∈ Dx,def ⊆ IRn
, t ∈ Dt,def ⊆ IR,
x(t0) = x0,
be a given initial-value problem. If the function f is continuous in a neigh￾borhood of the point (x0, t0), the initial-value problem possesses at least one
solution x(t) in an open interval around t0.
Peano’s existence theorem guarantees the solvability of a differential equa￾tion. However, it does not guarantee the uniqueness of the solution. This
means that multiple solutions may well exist. To illustrate this, we can con￾sider the differential equation
x˙ =
√3
x
2 (1.26)
with the initial value
x(0) = 0 (1.27)
as an example. All functions
x =



 
1
3
t − a
!3
, t < 3a,
0, 3a ≤ t ≤ 3b,
 
1
3
t − b
!3
, t > 3b,
with arbitrary values a ≤ 0, b ≥ 0 fulfill the above initial-value problem (1.26),
(1.27). Hence infinitely many solutions exist, i. e. the solution is not unique.
If the Peano existence theorem is extended by the condition of Lipschitz
continuity of the function f, we arrive at the Picard-Lindelöf theorem[2]. This
theorem guarantees both the existence of a solution and its uniqueness. We
thus require Lipschitz continuity, which is defined by
[2] The Picard-Lindelöf theorem is also known as Picard’s existence theorem or the
Cauchy-Lipschitz theorem.1.2. Solution of Nonlinear Differential Equations 39
Definition 16 (Lipschitz Continuity). A function f(x) is called globally
Lipschitz continuous if the so-called Lipschitz condition
|f(x1) − f(x2)| ≤ L|x1 − x2|
is fulfilled for a real number L ≥ 0 for all x1, x2 ∈ Dx,def ⊆ IRn
. If a Lipschitz
condition is only fulfilled within a neighborhood of each point x ∈ Dx ⊆ Dx,def,
the function f(x) is called locally Lipschitz continuous.
Definition 17 (Lipschitz Continuity of Multivariate Functions). A
function f(x, t) is called globally Lipschitz continuous in x if the Lipschitz
condition
|f(x1, t) − f(x2, t)| ≤ L|x1 − x2|
is fulfilled for a real number L ≥ 0 for all t ∈ IR and all x1, x2 ∈ Dx,def ⊆ IRn
.
If a Lipschitz condition is only fulfilled within a neighborhood of each point
x ∈ Dx ⊆ Dx,def and for all t in an interval (tl
, tu) ⊆ IR, the function f(x, t)
is called locally Lipschitz continuous in x.
For example, the function
f(x) = x
2
is locally, but not globally, Lipschitz continuous. The following chain of de￾ductions holds:
globally Lipschitz continuous ⇒ locally Lipschitz continuous
⇒ locally continuous.
On the one hand, Lipschitz continuity is therefore a strict form of continuity.
On the other hand, it provides us with a measure, a maximum rate L, for
the change in the function values, which resembles the maximum value of a
function’s derivative. In particular, continuously differentiable functions are
locally Lipschitz continuous and continuously differentiable functions with a
bounded first derivative are globally Lipschitz continuous. However, not all
Lipschitz continuous functions are differentiable, but all differentiable globally
Lipschitz continuous functions have bounded derivatives.
Let us consider two examples. The first is the absolute value function
f(x) = |x|, which is globally Lipschitz continuous but not differentiable. The
second is the differential equation (1.26), whose right side is not globally
Lipschitz continuous and also not locally Lipschitz continuous around x = 0,
since its derivative
∂f(x)
∂x =
∂
√3
x
2
∂x =
2
3
1
√3 x40 Chapter 1. Fundamentals of Nonlinear Systems
becomes infinite at the point x = 0. Therefore, it is not possible to specify a
number L for any neighborhood of x = 0.
So equipped, we can formulate [327]
Theorem 4 (Picard-Lindelöf Local Existence Theorem). Let
x˙ = f(x, t), x ∈ Dx,def ⊆ IRn
, t ∈ Dt,def ⊆ IR,
x(t0) = x0
be a given initial-value problem. If the function f is locally Lipschitz contin￾uous in x and continuous in t in a neighborhood of the point (x0, t0), there
exists exactly one solution x(t) that is defined in an open interval around t0.
It is worth noting that this version of the Picard-Lindelöf existence theorem
guarantees the solution’s uniqueness but does not guarantee its existence for
all t ∈ IR. To see this, we will consider the example
x˙ = −
1
2x
,
x(0) = 1.
(1.28)
Obviously, the function
f(x) = −1/(2x)
is locally Lipschitz continuous in the neighborhood of the point
(x(0), t0) = (1, 0).
It is not globally Lipschitz continuous, since the derivative of f(x) becomes
infinite as x approaches zero. In this case, it is not possible to specify a constant
L that satisfies the Lipschitz condition. However, the conditions of the local
existence theorem are fulfilled. The only solution to the initial-value problem
(1.28) is
x(t) = √
1 − t.
We notice that no solution exists for values t > 1. The time t ∈ IR, for which
x(t) → ∞ (in this case t = 1) is called finite escape time. In contrast to some
nonlinear systems, unstable linear systems cannot run to infinity in finite time.
If we want to guarantee the existence of a solution for all values t ∈ IR, we
must resort to [264]
Theorem 5 (Picard-Lindelöf Global Existence Theorem). Let
x˙ = f(x, t), x ∈ Dx,def = IRn
, t ∈ Dt,def = IR,
x(t0) = x0
be a given initial-value problem. If the function f is globally Lipschitz contin￾uous in x and continuous in t, there exists exactly one solution x(t) for every
x0 ∈ IRn
, which is defined for all t ∈ IR.1.2. Solution of Nonlinear Differential Equations 41
A very well-known example of this global existence theorem is the linear
initial-value problem
x˙ = Ax,
x(0) = x0.
It is known that the unique solution is given by
x(t) = e
Atx0,
which is defined for all t ∈ IR. This result is consistent with our global existence
theorem, since the linear function
f(x) = Ax
is globally Lipschitz continuous, and thus a solution exists for all t ∈ IR.
1.2.2 Numerical Solution and Euler Method
We have already mentioned in the previous section that unlike linear differ￾ential equations, solving nonlinear differential equations with initial values
x(t0), i. e. initial-value problems
x˙(t) = f(x(t),u(t)),
x(t0) = x0,
is only numerically possible in most cases. For this purpose, a number of
numerical integration methods have been developed [172, 173, 423].
They are based on the following principle, which will be illustrated without
loss of generality for the case of a single-state variable x, i. e.
x˙(t) = f(x(t), u(t)). (1.29)
By integrating equation (1.29), we obtain the solution
x(t) = x(t0) + Z
t
t0
f (x(τ), u(τ)) dτ (1.30)
of the differential equation. Note that this equation is implicit with respect
to x(t), and therefore it is usually not analytically solvable. The integration
methods thus solve the integral in equation (1.30) numerically via approxima￾tions, which are more or less suitable. Accuracy and computational complexity
depend on the choice of the numerical integration method.
To obtain a numerical solution, we discretize equation (1.30). As shown in
Figure 1.31, the time-axis is divided by a set of equidistant points42 Chapter 1. Fundamentals of Nonlinear Systems
ti = t0 + h · i, i = 0, . . . , k,
where the sampling interval h is called the step size. Now it is possible to
write the solution of the differential equation (1.29) at the time instances ti
as a recursive formula
x(ti+1) = x(t0) + Z
ti
t0
f(x, u) dt
| {z }
x(ti)
+
t
Zi+1
ti
f(x, u) dt,
which is equivalent to
x(ti+1) = x(ti) +
t
Zi+1
ti
f(x, u) dt.
The points x(ti) are termed integration points.
The aim of the integration methods is to find a good approximation for
the integral. In the simplest case, as shown in Figure 1.32, the area
F =
t
Zi+1
ti
f(x, u)dt
between ti and ti+1 is approximated by a rectangle whose area is
...
h
t0 t1 t2 t3 t4 ti
Fig. 1.31: Time instances ti and step size h of the numerical solution
f
t
f(x(ti), u(ti))
ti ti+1
h · f(x(ti), u(ti))
Fapp
Fig. 1.32: Approximate solution of the integral1.2. Solution of Nonlinear Differential Equations 43
f
t0 t1 t2 t3 t4 t5 t
Fig. 1.33: Area calculation using the Eu￾ler method
x,
xˆ
t0 t1 t2 t3 t4 t5 t
x(t0) xˆ(t1) xˆ(t2)
xˆ(t3)
xˆ(t4)
xˆ(t5)
Exact solution x(t)
Fig. 1.34: Comparison between the exact
solution x(t) and its approximation xˆ(ti)
Fapp = h · f (x(ti), u(ti)).
This yields the value
xˆ(ti+1) = ˆx(ti) + h · f (ˆx(ti), u(ti))
as an approximation for x(ti+1). In the multidimensional case, using the ab￾breviations xˆ(ti) = xˆi and u(ti) = ui
,
xˆi+1 = xˆi + h · f(xˆi
,ui)
holds.
Here xˆ0 = x0 serves as the initial value for this recursion. The calculation
above is referred to as the Euler method. It is simple but inaccurate. Figure
1.33 illustrates the approximation that is yielded by this procedure.
1.2.3 Accuracy of the Numerical Solution
The approximation of the area under the curve of the differential equation’s
solution by a series of rectangles has a direct influence on the accuracy of the
solution, as illustrated in Figure 1.34. The error at step i, the cumulative error
εi = |x(ti) − xˆ(ti)| =
p
(x1(ti) − xˆ1(ti))2 + . . . + (xn(ti) − xˆn(ti))2,
is dependent on the overall number of previous steps and on the step size h.
Using the Euler method, the estimation
εk ≤ α · h
holds for the error εk after a fixed integration time T = k · h with k steps.
The method error εk therefore decreases for decreasing step sizes. In general,
α is an unknown constant.
For integration methods that are more accurate than the Euler method,
the error εk of the approximate solution can be decreased much more quickly
by reducing the step size h. In general, it holds that44 Chapter 1. Fundamentals of Nonlinear Systems
εn,
εr,
εtot
h h
∗
Total error εtot
Cumulative error εk
Rounding error εr
Fig. 1.35: The total error εtot of a numerical integration method is made up of the
method error εk and the rounding error εr.
εk ≤ α · h
q
.
The parameter q is called the order of accuracy or error order of an integration
method. The value of q defines how quickly the error εk converges to zero as
h tends to zero. Here a larger value of q indicates the greater accuracy of a
given integration method.
Regarding the precision of a numerical solution of differential equations,
it therefore seems appropriate to choose very small step sizes h. However, as
h is decreased, the number of required integration points increases, and with
it the computation time of the simulation.
There is another disadvantage to very small step sizes h. Although the
cumulative error εk decreases when the step size h is reduced, the computer’s
rounding error εr increases, as shown in Figure 1.35. Therefore an optimal
step size h
∗
exists for which the total error εtot is minimal. Inconveniently,
the optimal step size h
∗
generally cannot be determined.
1.2.4 The Modified Euler Method
The Euler method is very imprecise because of the chosen rectangular approx￾imation h · f (ˆxi
, ui). It can be improved by not choosing the value
f (ˆx(ti), u(ti))
at position xˆ(ti) = ˆxi as the height, but rather
f

xˆ

ti +
h
2

, u 
ti +
h
2

at position
xˆi+1/2 = ˆx(ti +
h
2
)1.2. Solution of Nonlinear Differential Equations 45
f
f (ˆxi, ui)
ti ti+1 t
Fapp
Fig. 1.36: Euler method
f
f(ˆxi+ 1
2
, ui+ 1
2
)
ti ti+1/2 ti+1 t
Fapp
Fig. 1.37: Modified Euler method
and
ui+1/2 = u(ti +
h
2
).
Figures 1.36 and 1.37 illustrate these conditions.
The integral of the modified Euler method, also known as the midpoint
method, is calculated as
Fapp = h · f
￾
xˆi+1/2, ui+1/2

.
In this way we obtain the following algorithm for the improved method in the
general case of multidimensional system equations:
(1) xˆi+1/2 = xˆi +
h
2
· f (xˆi
,ui),
(2) xˆi+1 = xˆi + h · f
￾
xˆi+1/2,ui+1/2

.
In contrast to the Euler method with the order of accuracy q = 1, the modified
Euler method has the order of accuracy q = 2.
1.2.5 The Heun and Simpson Methods
Further improvements in accuracy compared to the numerical methods de￾scribed above can be achieved if the integral
F =
t
Zi+1
ti
f(x(t), u(t))dt
is more accurately approximated by means of a trapezoid area. For this pur￾pose, the Heun method replaces the rectangle with a trapezoid to calculate
the approximation of the integr46 Chapter 1. Fundamentals of Nonlinear Systems
Fapp =
h
2
[f(xi
, ui) +f(xi+1, ui+1)] ,
as illustrated in Figure 1.38. Thus we obtain the recurrence relation
xˆi+1 = ˆxi +
h
2
[f (ˆxi
, ui) + f (ˆxi+1, ui+1)].
Here, however, the integration point xˆi+1 is also present on the right-hand
side of the equation. We could solve this implicit equation iteratively for xˆi+1.
Using the Heun method, however, we proceed differently and compute xˆi+1
approximately via a Euler step that is given by
xˆi+1 ≈ x˜i+1 = ˆxi + h · f (ˆxi
, ui).
In summary, we arrive at the Heun method for the general multidimensional
case:
(1) x˜i+1 = xˆi + h · f (xˆi
,ui),
(2) xˆi+1 = xˆi +
h
2
[f (xˆi
,ui) + f(x˜i+1,ui+1)].
Heun’s method is a so-called predictor-corrector method. The result of its
first equation yields the predictor with an approximation of the solution. This
is improved upon by the second equation, which is called the corrector. It is
worth noting that the Heun method, like the modified Euler method, has the
order of accuracy q = 2, and therefore does not improve upon the modified
Euler method in terms of accuracy.
If we use a parabola for the approximation of the area
F =
t
Zi+1
ti
f(x, u)dt
as the interpolation function for the function f on the interval [ti
, ti+1], the
accuracy can be further increased. By using the parabola, as depicted in Figure
1.39, we arrive at Simpson’s rule. To compute the parameters of the parabola,
an additional point at
ti+1/2 = ti +
h
2
,
which lies between the points (ti
, f(ˆxi
, ui)) and (ti+1, f(ˆxi+1, ui+1)) of the
Heun method, and the intermediate value
ui+1/2 = u(ti +
h
2
)
of the control signal are required. The integration of the parabola on the
interval [ti
, ti+1] then provides the estimate Fapp of the area to be integrated.
The resulting calculation rule is called the Simpson method and is sum￾marized as follows:1.2. Solution of Nonlinear Differential Equations 47
f
ti ti+1 t
c1t + c0
Fapp
Fig. 1.38: Heun method
ti ti+1/2 ti+1 t
f
f
c2t
2 + c1t + c0
Fapp
Fig. 1.39: Simpson method
(1) k1 = f (xˆi
,ui),
(2) k2 = f(xˆi +
h
2
k1,ui+1/2),
(3) k3 = f (xˆi − hk1 + 2hk2,ui+1),
(4) xˆi+1 = xi +
h
6
(k1 + 4k2 + k3).
It has the order of accuracy q = 3. The method is also known as the third-order
Runge-Kutta method.
1.2.6 The Runge-Kutta Methods
The Euler methods, the Heun method, and the Simpson method are special
cases of the Runge-Kutta methods. All of these calculation rules are called
one-step methods, since they determine a new integration point xˆi+1 based
only on a previous point xˆi
. The Runge-Kutta methods have the following
general form:
(1) k1 = f (xˆi
,ui),
(2) k2 = f (xˆi + hα21k1,u(ti + β2h)),
(3) k3 = f (xˆi + h (α31k1 + α32k2),u(ti + β3h)),
.
.
.
(m) km = f (xˆi + h (αm1k1 + . . . + αm,m−1km−1),u(ti + βmh)),
(m+1) xˆi+1 = xˆi + h ·
Xm
j=1
γjkj .48 Chapter 1. Fundamentals of Nonlinear Systems
Table 1.1: Orders of accuracy q of the Runge-Kutta methods
m 1 2 3 4 5 6 7 8 9
q 1 2 3 4 4 5 6 6 7
The corresponding orders of accuracy are listed in Table 1.1. With an increas￾ing number of approximation computations, i. e. the number m of stages, the
accuracy q of the method being used also increases. The special cases of the
Euler method, the modified Euler method, and the Heun and Simpson meth￾ods result from the parameters m, γ, β, and α, which are given in Table
1.2.
Table 1.2: Special cases of the Runge-Kutta methods
m γ β αij
Euler m = 1 γ1 = 1 − −
Modified m = 2 γ1 = 0 β2 =
1
2
α21 =
1
2
Euler γ2 = 1
Heun m = 2 γ1 =
1
2
β2 = 1 α21 = 1
γ2 =
1
2
Simpson m = 3 γ1 =
1
6
β2 =
1
2
α21 =
1
2
γ2 =
4
6
β3 = 1 α31 = −1
γ3 =
1
6
α32 = 2
Several variants exist for the same value of m ≥ 2, of which the classical
Runge-Kutta method with number of stages m = 4 is the most common. On
the one hand, this is because the computational effort is limited in this case.
On the other hand, it is because the order of accuracy q, as shown in Table 1.1,
does not increase when the number of stages increases from m = 4 to m = 5,
whereas the computational effort becomes greater. Also, the achievable ac￾curacy is high in proportion to the computational effort. Another advantage
is the simplicity of the Runge-Kutta equations; they are easy to remember.
A good mnemonic for the methods’ parameters provides the Butcher tableau
[23]. The most commonly used and best-known Runge-Kutta method is1.2. Solution of Nonlinear Differential Equations 49
(1) k1 = f (xˆi
,ui),
(2) k2 = f(xˆi +
h
2
k1,ui+1/2),
(3) k3 = f(xˆi +
h
2
k2,ui+1/2),
(4) k4 = f (xˆi + hk3,ui+1),
(5) xˆi+1 = xˆi +
h
6
(k1 + 2k2 + 2k3 + k4).
The method has an order of accuracy q = 4. Runge-Kutta methods of higher
orders can be found in [172].
Due to the advantageous characteristics mentioned above, the fourth-order
Runge-Kutta method and the Dormand-Prince 4/5 method [99, 172], an￾other very advantageous Runge-Kutta method, are the best-known integration
methods and are very frequently used.
1.2.7 Adaptation of the Step Size
So far, the step size h has been kept constant during the recursion. This is
not always appropriate. If the differential equation inhibits parts of strongly
differing dynamics, as illustrated in Figure 1.40, the integration methods that
we have discussed so far lead to inaccurate or computationally intensive so￾lutions. This is because we would have to choose a constant step size h to be
small enough to produce a good approximation of the course in the region of
the oscillations (t < 5s). In the regions that do not contain oscillations, i. e.
(t>5s), the simulation would require an unnecessarily large amount of steps,
since the step size is too small in that region. Therefore, during the simulation
x(t)
Time t in s
3
2
1
0
-1
0 2 4 6 8 10
Fig. 1.40: Differential equation with strongly differing dynamic components50 Chapter 1. Fundamentals of Nonlinear Systems
it is advisable to adapt h to the obtained progression of the solutions xˆ(t),
i. e. we choose small step sizes for fast dynamic progressions and large step
sizes for slow ones. With this step-size adaptation, the effort and duration of
the simulation can be reduced.
A simple way of adapting the step size is provided by the following algo￾rithm, where Φ denotes the function of the applied integration method, which
is used to compute an integration step with the approximation result xˆi
, and
ε is a prespecified error:
Step 1: Compute two recursion steps with h:
xˆi+1 = xˆi + hΦ(xˆi
,ui
, h),
xˆi+2 = xˆi+1 + hΦ(xˆi+1,ui+1, h).
Step 2: Compute a recursion step with 2h:
x˜i+2 = xˆi + 2hΦ(xˆi
,ui
, 2h).
Step 3: If
|xˆi+2 − x˜i+2| > ε,
set h
∗ = h/2 and i
∗ = i. If
|xˆi+2 − x˜i+2| ≤ 0.1ε,
set h
∗ = 2h and i
∗ = i + 2, otherwise set h
∗ = h and i
∗ = i + 2.
Set i = i
∗
, h = h
∗
, go to Step 1 and start from the beginning.
A much more effective step-size adaptation than the intuitive one men￾tioned above is as follows. We choose two one-step integration methods, i. e.
a method Γ with the order of accuracy q and a method Φ with q + 1. Let the
ε be a prespecified error. Then the following algorithm [108] applies:
Step 1: Compute
xˇi+1 = xˆi + hΓ(xˆi
,ui
, h),
x˜i+1 = xˆi + hΦ(xˆi
,ui
, h),
S =
q
s
h · ε
|xˇi+1 − x˜i+1|
.
Step 2: If S ≥ 1, set xˆi+1 = x˜i+1, h
∗ = h · min{2; S} and i
∗ = i + 1.
If S < 1, set h
∗ = h · max{0.5; S} and i
∗ = i.
Set i = i
∗
, h = h
∗
, go to Step 1 and start from the beginning.
Particularly for complex systems with dynamics that are difficult to assess, it
is advisable to always use an adaptive step size.1.2. Solution of Nonlinear Differential Equations 51
1.2.8 The Adams-Bashforth Methods
So far we have only dealt with one-step methods, i. e. methods for which only
one preceding value xˆi
is used to estimate the area F and thereby the value
xi+1. To approximate the integral
F =
t
Zi+1
ti
f(x(t), u(t))dt
even more precisely, it is useful to fit f using an interpolation polynomial
through a set of interpolation points
(ti−k, f(ˆxi−k, ui−k)) = (ti−k, fi−k),
.
.
.
(ti−1, f(ˆxi−1, ui−1)) = (ti−1, fi−1),
(ti
, f(ˆxi
, ui)) = (ti
, fi),
and possibly
(ti+1, f(ˆxi+1, ui+1)) = (ti+1, fi+1).
This latter interpolation point is precisely the one that we wish to calculate,
meaning it is not yet known. Figure 1.41 illustrates this. Since more than
one interpolation point is used for the area approximation, these methods are
called multi-step methods.
The case in which the point (ti+1, fi+1) is not used as an interpolation point
results in the Adams-Bashforth methods. Where four interpolation points are
used, i. e. a polynomial of order three, the Adams-Bashforth method is given
by
f
ti−2 ti−1 ti ti+1 t
fi−2
fi−1
fi
fi+1
Interpolation polynomial
Fapp
Fig. 1.41: Adams-Bashforth method52 Chapter 1. Fundamentals of Nonlinear Systems
xˆi+1 = xˆi +
h
24
￾
55fi − 59fi−1 + 37fi−2 − 9fi−3

with the order of accuracy q = 4. Note that the first three values xˆ1, xˆ2,
xˆ3 and f 1
, f 2
, f 3
, beginning from xˆ0 and f 0
, must be computed using a
one-step method.
An advantage of the Adams-Bashforth methods is that we only need to
calculate one new function value in each step. A disadvantage is that the
interpolation polynomial uses only the interpolation points
(ti−k, fi−k), . . . ,(ti
, fi)
but not
(ti+1, fi+1).
However, the approximation of F by Fapp is performed on the interval
[ti
, ti+1]. Now, the problem is that an interpolation polynomial outside its
interpolation interval, which is here [ti−k, ti
], tends to infinity for x → ∞.
Consequently, the interpolation polynomial deviates more and more from f(x)
with increasing stepsize and the approximation error is larger than desired.
To compensate for this disadvantage in the Adams-Bashforth methods, we
will improve upon them in the following section.
1.2.9 The Adams-Moulton Predictor-Corrector Methods
The Adams-Moulton methods apply an Adams-Bashforth method as a predic￾tor and improve the result by computing a correction term. This term is based
on an interpolation polynomial that includes the unknown interpolation point
(ti+1, fi+1)
in addition to the four interpolation points of the Adams-Bashforth method.
This yields the relation
xˆi+1 = ˆxi +
h
720

251 f(ˆxi+1, ui+1)
| {z }
fi+1
+646fi − 264fi−1 + 106fi−2 − 19fi−3

for an interpolation polynomial of order four. This equation is implicit in xˆi+1.
To determine xˆi+1, it is thus necessary to compute an iteration given by
xˆ
(l+1)
i+1 = ˆxi +
h
720

251f(ˆx
(l)
i+1, ui+1) + 646fi − 264fi−1 + 106fi−2 − 19fi−3

.
This has to be done until xˆ
(l)
i+1 no longer changes significantly. The chosen step
size h should be small enough that two iteration steps l = 1, 2 are sufficient.
In summary, in the multidimensional case this yields the Adams-Moulton
predictor-corrector method with the two recursion1.2. Solution of Nonlinear Differential Equations 53
(1) xˆ
(0)
i+1 = xˆi +
h
24
￾
55fi − 59fi−1 + 37fi−2 − 9fi−3

,
(2) xˆ
(l+1)
i+1 =xˆi+
h
720

251f(xˆ
(l)
i+1,ui+1)+646fi−264fi−1+106fi−2−19fi−3

and the order of accuracy q = 5. In this case, the first three points xˆi after
x0 must be computed again using a one-step method. It is advisable to use a
one-step method of the same order of accuracy q = 5 as the Adams-Moulton
multi-step method has.
The step size can also be adapted in the case of multi-step methods. How￾ever, this is more complex than for one-step methods [63].
1.2.10 Stability of Numerical Integration Methods
The accuracy of an integration method is significant, but so is its stability.
Below we will analyze the stability behavior of the integration methods we
have discussed. This requires answering the question of whether the error εn
of the numerical solution remains bounded or whether it exceeds all bounds
for an increasing number of steps and an increasing simulation duration. The
answer depends on the step size h, the differential equation x˙ = f(x,u)
itself, and the chosen integration method. If the error εn remains bounded,
the method is called stable.
Unfortunately the above question generally cannot be answered precisely
due to the nonlinearity of f. For the simple linear test case
x˙ = −λx with λ > 0 (1.31)
and the initial value
x(0) = x0,
however, the stability range of h can be determined. Since this is possible for
all the methods we have discussed, the methods can be compared based on
their stability behavior, which gives us a basic insight into the circumstances.
For the Euler method, we obtain
xˆi+1 = ˆxi + h(−λxˆi) = (1 − hλ)ˆxi
. (1.32)
Obviously, this difference equation is asymptotically stable if the inequality
|1 − hλ| < 1, i. e. 0 < hλ < 2,
is fulfilled. This result can also be obtained by determining the zero of the
system’s characteristic polynomial from equation (1.32),
P(z) = z − (1 − hλ),
which must lie within the interval [−1, 1] for the difference equation (1.32)
to be Lyapunov stable and within the interval (−1, 1) to be asymptoticall54 Chapter 1. Fundamentals of Nonlinear Systems
Table 1.3: Stability ranges
Euler hλ < 2
Modified Euler hλ < 2
Heun hλ < 2
Simpson hλ < 2.5359
Runge-Kutta (4th order) hλ < 2.7853
Adams-Bashforth (4th order) hλ < 0.3
Adams-Moulton (5th order) hλ < 1.8367
stable. For hλ < 2 the solution of the difference equation (1.32) thus tends
to the same value as the solution of the differential equation (1.31). The
progression of xˆi
, however, can deviate substantially from x(t).
In a way similar to the Euler method, stability for the test case of x˙ = −λx
can also be determined for other methods. In these methods, the respective
characteristic polynomial has an order greater than one. If its roots lie within
the unit circle, the method is stable. Taking this into account, we obtain the
stability ranges hλ that are provided in Table 1.3. Note that hλ > 0 must
hold.
The Adams-Bashforth method has a very small range of stability. This
can be explained by the fact that the interpolation on the interval [ti
, ti+1] is
computed via a polynomial that is based on the previous interpolation points
(ti−k, xi−k), . . . ,(ti
, xi), as mentioned previously. Interpolation polynomials
are only sufficiently exact between the interpolation points. Outside the inter￾polation range, they often differ significantly from the course that they should
approximate. This is why the computation of the integral becomes inaccurate.
To illustrate the stability behavior of the methods, Figure 1.42 plots dif￾ferent simulations which have been carried out for the test case
x˙ = −λx
with λ = 1 and the initial value x(0) = 1 for different step sizes h. The an￾alytical solution of this differential equation is
x(t) = e
−t
.
As we can see from Figure 1.42, stable step sizes h do not always lead to nu￾merically determined solutions with sufficiently small simulation errors. Figure
1.43 provides the relative error as a percentage for the example of x˙ = −x,
i. e.
ε
rel
n =
|εn|
|xn|
· 100% = |xn − xˆn|
|xn|
· 100%,1.2. Solution of Nonlinear Differential Equations 55
x(t)
x(t)
Time t in s Time t in s
2 2
1 1
0.5 0.5
0 0
-0.5 -0.5
-1 -1
0 2 4 6 8 10 0 2 4 6 8 10
e
−t
e
−t
h= 0.5
h= 1 h= 1
h= 2
h= 1.5
h= 3
h= 2.7853
h= 2.5
Fig. 1.42: Solution of x˙ = −x with the Euler method (left) and the fourth-order
Runge-Kutta method (right) for different step sizes h
0
20
40
60
80
100
120
10−10
10−8
10−6
10−4
10−2
100
102
104
0 0.2 0.4 0.6 0.8 1
10−2
10−1
100
Step size h
Step size h
Relative error in
%
Relative error in
%
1
1
2
2
3
3
4
4
5
5
6
6
7
7
Fig. 1.43: Relative error (in percentage points) in linear representation and in double
logarithmic representation for the following methods: 1. Euler, 2. modified Euler,
3. Simpson, 4. fourth-order Runge-Kutta, 5. Adams-Bashforth with the order of
accuracy q = 4, 6. Adams-Moulton with q = 5 (one iteration), 7. Adams-Moulton
with q = 5 (ten iterations)56 Chapter 1. Fundamentals of Nonlinear Systems
at the instant t = 10s for different step sizes h. The logarithmic representation
of the error shows the course of the error in more detail for small step sizes,
contrasted with the linear representation.
It becomes clear from Figure 1.43 that the classical fourth-order Runge￾Kutta method provides an approximate solution with a small error and rea￾sonable computational effort. Compared to other methods, this is an excel￾lent compromise between accuracy and effort; therefore this method is very
commonly used. For small step sizes, the Adams-Moulton predictor-corrector
method is even more precise than the fourth-order Runge-Kutta method.
1.2.11 Stiff Systems and Their Solutions
The one-step and multi-step methods that we have outlined so far are often
not suitable for systems
x˙ = f(x),
whose state variables xi have a strongly differing dynamic behavior, or contain
parts with very different dynamics. These systems are called stiff systems. A
simple linear example is the system that is defined by
x˙ 1 = −x1,
x˙ 2 = −100x2.
Obviously, a numerical solution requires us to choose a step size h which is
small enough to solve the second differential equation with sufficient accuracy.
However, this results in a very high number of simulation steps, since the
second differential equation requires a step size one hundred times smaller
than that of the first equation. Figure 1.44 illustrates this fact.
One possible solution for such stiff differential equations are one-step meth￾ods with adaptive step sizes, which were discussed in Section 1.2.7. In the case
x1, x2
Time t in s
1
0.8
0.6
0.4
0.2
0
0 0.5 1 1.5 2 2.5
x1
x2
Fig. 1.44: Example of a stiff system1.2. Solution of Nonlinear Differential Equations 57
of multi-step methods, specially developed integration methods are used which
are based on implicit recursive formulas and which exhibit particularly good
stability properties.
If we take the Euler method, for example, it is possible to use
F =
t
Zi+1
ti
f(x, u)dt ≈ hf(xi+1, ui+1)
instead of
F ≈ hf(xi
, ui).
This yields the approximation equation
xˆi+1 = ˆxi + hf(ˆxi+1, ui+1).
The implicit character of this equation requires the computation of multi￾ple iterations for each simulation step i. As in the correction formula of the
Adams-Moulton method, this yields an iteratively solvable equation
xˆ
(l+1)
i+1 = ˆxi + hf(ˆx
(l)
i+1, ui+1).
This method is called the implicit Euler method, which is also the simplest of
an entire class of integration methods for stiff differential equations: the Gear
methods. All these methods are implicit calculation rules. Table 1.4 lists the
Gear formulas up to the order of accuracy q = 4 for the multidimensional
case. Here M denotes the number of interpolation points. As initialization for
the corrector equations, the simple predictor
xˆ
0
i+1 = xˆi
Table 1.4: Gear formulas
M Gear formulas q
1 xˆ
(l+1)
i+1 = xˆi + hf
(l)
i+1 1
2 xˆ
(l+1)
i+1 =
1
3

4xˆi − xˆi−1 + 2hf
(l)
i+1
2
3 xˆ
(l+1)
i+1 =
1
11

18xˆi − 9xˆi−1 + 2xˆi−2 + 6hf
(l)
i+1
3
4 xˆ
(l+1)
i+1 =
1
25

48xˆi − 36xˆi−1 + 16xˆi−2 − 3xˆi−3 + 12hf
(l)
i+1
458 Chapter 1. Fundamentals of Nonlinear Systems
is used, and typically three iterations are computed.
An important characteristic of the Gear methods is their large range of
stability. Once again, let us select the test example
x˙ = −λx, λ > 0.
Its stability range is given by
hλ < ∞.
This stability range applies to all Gear methods. There are also adaptive step￾size variants of the Gear methods and other methods which can be used to
solve stiff differential equations [173, 224]. An overview of integration methods
for stiff differential equations can be found in [173].
1.3 Exercises
Exercise 1.1 For the system
x˙ 1 = x
3
1x
3
2 + x1u,
x˙ 2 = −x
2
1x2,
y = x2,
calculate the differential equation which is only dependent on the output vari￾able y and its time derivatives, as well as on the input variable u.
Exercise 1.2 Let us consider the system
x˙ 1 = x1(1 − x1 − x2),
x˙ 2 = x2(1 − x1 − 2x2).
Calculate the zero set of x˙ 1, i.e. the set of values x1 and x2 for which
x1(1 − x1 − x2) = 0
applies. Next, calculate the zero set of x˙ 2. Using the zero sets, determine the
system’s equilibrium points.
Exercise 1.3 Calculate the equilibrium points of the systems
(a) x˙ = x
2 + 1,
(b) x˙ 1 = (x2 − 1)2
sin(x1),
x˙ 2 = x
2
1
(1 − cos2
(x2)),
(c) x˙ 1 = x2,
x˙ 2 = −x1 +
x
3
1
6
− x2,
(d) x˙ 1 = −x1 + x2,
x˙ 2 = 0.1x1 − 2x2 − x
2
1 − 0.1x
3
1
,
(e) x˙ 1 = x2,
x˙ 2 = −x1 + x2(1 − 3x
2
1 − 2x
2
2
),1.3. Exercises 59
(f) x˙ 1 = −x1 + x2(1 + x1),
x˙ 2 = −x1(1 + x1),
(g) x˙ 1 = (x1 − x2)(x
2
1 + x
2
2 − 1),
x˙ 2 = (x1 + x2)(x
2
1 + x
2
2 − 1),
(h) x˙ 1 = x
2
1 + x
2
2 + x
2
3 − 1,
x˙ 2 = x1x2x3,
x˙ 3 = x1 − x
2
2
.
Exercise 1.4 Let us examine the contagion model of an infectious disease
such as measles with x1 representing the number of healthy, but potentially
vulnerable persons, x2 the number of infected patients, and x3 the number
of recovered patients who now have lifelong immunity. The epidemiological
model can be described by the equations
x˙ 1 = αN − rβ x2
N
x1 − µx1,
x˙ 2 = rβ x2
N
x1 − γx2 − µx2,
x˙ 3 = γx2 − µx3
with
α representing the birth rate,
N the population size, which we assume to be constant,
r the number of contacts between people per unit of time,
β the probability of infection per contact,
µ the death rate, and
γ the recovery rate.
(a) Determine the model’s equilibrium points.
(b) Which situation is represented by each of the equilibrium points?
Let us now examine the case of an incurable infectious disease such as AIDS.
In this case, no recovered patients exist and thus x3 is not included in our
calculation. Let us assume that the population size N = x1 + x2 remains
constant. We will use the simplified model
x˙ 1 = −rβ x2
N
x1,
x˙ 2 = rβ x2
N
x1.
(c) Reduce the system of differential equations to one differential equation
with x2 as the variable.
(d) Determine the equilibrium points.
(e) Calculate the solution to the differential equation. For this purpose, trans￾form the differential equation using x2 = z
−1
.
(f) Which equilibrium point does the system tend to if one or more persons
are contagious?
(g) What is the shortcoming of the model from (c), and how can it be reme￾died?60 Chapter 1. Fundamentals of Nonlinear Systems
Exercise 1.5 Let us examine the system
x˙ 1 = −x1 +
1
3
x
4
1x
2
2
, x10 = x1(0),
x˙ 2 = −x2, x20 = x(0),
whose solution is
x1(t) = x10
3
s
5e
2t
x
3
10x
2
20 + (5 − x
3
10x
2
20)e
5t
, x2(t) = x20e
−t
,
and whose only equilibrium point is xeq = 0.
(a) Show that the equilibrium point is not globally but locally attractive.
(b) Show that the equilibrium point is asymptotically stable.
(c) Show that the equilibrium point is not globally asymptotically stable.
(d) Determine the region of attraction GROA.
(e) Determine the region of asymptotic stability GRAS.
Exercise 1.6 Let us examine Chua’s circuit, which is shown in Figure 1.45.
(a) Create a mathematical model of the circuit. Use the capacitor voltages
uC1 and uC2 and the inductor current iL as the state variables. Begin by
calculating the nonlinear curve i(uC1), assuming that the diodes and the
operational amplifier are ideal components.
(b) What type of nonlinearity characterizes the circuit?
(c) Now, take
x1 =
uC1
u0
, x2 =
uC2
u0
, x3 =
R
u0
iL with u0 =
R2
R2 + R3
ud
L
R1
R2 R2
R3 R3
R4
R4
i
uC2 uC1
-ud ud
C2
R
C1
iL iR
iC2 iC1
Fig. 1.45: Chua’s circuit1.3. Exercises 61
as the new state variables and rescale the time using
t = RC2τ.
In addition, use the abbreviations
α =
C2
C1
, β =
R2C2
L
, a = −
R
R1
, b = R
R2 + R3
R2R3
−
R
R1
.
Based on this, calculate the normalized equations of the model.
(d) What type does this normalization take, and why do we perform it?
Exercise 1.7 Let us consider the system
x˙ 1 = α (−x1 + x2 − g(x1)),
x˙ 2 = x1 − x2 + x3,
x˙ 3 = −βx2,
with g(x1) =



bx1 + a − b, x1 ≥ 1,
ax1, |x1| < 1,
bx1 + b − a, x1 ≤ −1.
Here, a < b < 0 and a < −1, as well as α > 0 and β > 0 apply.
(a) Determine the equilibrium points of the system.
(b) Now
a = −8/7, b = −5/7, α = 44/5, and β = 15
apply. For each equilibrium point, determine whether it is asymptotically
stable, Lyapunov stable, or unstable.
(c) The system has no limit cycle, and no trajectory tends to infinity. What
do you conclude from this, taking into account the results from (b)?
Exercise 1.8 If linear systems are connected in series, it is possible to change
their order without changing anything in the overall transfer behavior. This
does not apply if nonlinear components are connected in series. Let us examine
two examples shown in Fig. 1.46. An oscillation of the type u(t) = A cos (ωt),
A > a, is applied at the inputs of both circuits at t = 0. What form do the
output signals y1 and y2 take? Make a sketch.
1
s
1
s
u1 u2
a a
-a -a
x1 y1 x2 y2
Fig. 1.46: Serial connections of a linear and a nonlinear system
Exercise 1.9 Consider the control loop shown in Figure 1.47.
(a) Draw the output signal y(t) corresponding to the input signal yref(t) shown
in Figure 1.48, and for the time interval 0 ≤ t ≤ 40 and x(0) = 0.
(b) How great is the steady-state error e∞ when yref = 3 and yref = −3?62 Chapter 1. Fundamentals of Nonlinear Systems
1
s
yref e x=y
1
-1 u
-1
1
Fig. 1.47: Loop with three-position control
yref
3
-3
10 20 t
Fig. 1.48: Input signal yref(t)
Exercise 1.10 We will now analyze the stability behavior of the control loop
shown in Figure 1.49.
(a) What are the eigenvalues of the plant?
(b) Calculate all the possible trajectories and sketch them on the x1x2-plane.
(c) Where do equilibrium points occur, and what are the stability properties
of each?
(d) How does the choice of the initial vector x(0) influence the stability be￾havior?
yref = 0 e u x1 =y
1
-1
Plant
x˙ 1 = x2
x˙ 2 = x1 + u
Fig. 1.49: Control loop with two-position control and unstable plant
Exercise 1.11 Let us examine the plant
x˙ 1 = x2,
x˙ 2 = u
with the output variable y = x1 and the proportional control u = −ky.
(a) For both k = 4 and k = 0.25, determine the state-space representation of
the corresponding two control loops.
(b) Specify the stability behavior of both control loops.
(c) Calculate the time solution x1(t) and x2(t) for k = 4 and k = 0.25 for the
initial values x1(0) = 0 and x2(0) ∈ IR. What shape do the trajectories
x(t) take for the two control loops? Draw them both.
(d) We will now apply the switching control law
u =
(
−4y, x1x2 ≥ 0,
−0.25y, x1x2 < 0.1.3. Exercises 63
Draw the trajectories of the control loop. What equilibrium points does
it have, and what is their stability behavior?
(e) We will now change the switching control law to the following:
u =
(
−4y, x1x2 < 0,
−0.25y, x1x2 ≥ 0.
Now draw the trajectories of the control loop again. What equilibrium
points does it have, and what is their stability behavior?
Exercise 1.12 Prove that a gradient system
x˙ = −

∂h(x)
∂x
T
has no limit cycles.
Exercise 1.13 Commercial fishing has a significant impact on the population
dynamics of fish species. This example deals with tuna and mackerel. Both
are prey of human beings; however, the mackerel is also the prey of tuna. The
population variable x1 of the mackerel can be described by
x˙ 1 = a1x1 − a2x1x2 − b1x1u1
[36, 400]. The change x˙ 1 in the population size x1 of the mackerel depends
on a1x1, where a1 is the growth coefficient, the number of predators x2, and
the catch rate u1 by human fishers. The increase x˙ 2 in the number of tuna
depends on the number x1 of their prey, but also on the competition −a3x2
between them and their catch rate u2 by human fishers. It is given by
x˙ 2 = −a3x2 + a4x1x2 − b2x2u2,
where a1, a2, a3, a4, b1, b2 > 0 and 0 ≤ u1 ≤ 1 and 0 ≤ u2 ≤ 1 apply.
(a) Determine the equilibrium points of the system for constant catch rates
u1 and u2.
(b) Linearize the system around the equilibrium point for which x1 6= 0,
x2 6= 0, and u1 = u2 = 0.5 apply, where ∆x1 and ∆x2 denote the varia￾tions in the mackerel and tuna populations.
Assume a1 > 0.5b1 in the following.
(c) What eigenvalues does the linearized model have?
(d) Assume that the catch rate of the mackerel is u1 = 0.5. Can the popu￾lations x1 and x2 around the equilibrium point from (b) be stabilized if
we use the control ∆u2 = −k∆x2 based only on the variation ∆x2 in the
tuna population?
Exercise 1.14 Let us now examine a distribution process for granular ma￾terial which is transported into two silos via a conveyer belt. The silos are
filled in turn using a swiveling tube. The identical silos each have volume V64 Chapter 1. Fundamentals of Nonlinear Systems
0.1V
V˙
out
V˙
out
V˙
in
Fig. 1.50: Distribution system for bulk material
and serve as buffer storage units for the subsequent removal and continued
transport of the granular material via two conveyer belts which carry it on
to processing plants. The volume flows V˙
1,out and V˙
2,out exiting the silos can
be regulated using adjustable unloaders. Figure 1.50 shows how this process
works.
A control mechanism is required to swivel the filling tube from the left￾hand to the right-hand silo and vice versa, ensuring that neither of the silos
is emptied entirely. The control mechanism in this example is such that the
tube swings out over the silo whose contents have fallen to 10% and pours the
material into that silo.1.3. Exercises 65
In the special case examined in the following, the constant volume flow V˙
in
of the material being transported amounts to 9/50 of a silo’s volume V per
hour; furthermore, the relation
10
9
V˙
in = V˙
1,out + V˙
2,out
holds. The two volume flows exiting the silos are constant and equal, i.e.
V˙
1,out = V˙
2,out = V˙
out applies. For safety reasons, the process is stopped when
one of the silos has contents lower than 0.1V .
(a) Determine the state-space model of the system with the control mecha￾nism described above. As state variables, use the filled volume V1 and V2
of the silos, which depend on the volume flow V˙
in being transported into
each silo, and as a switching variable z. If the swiveling tube fills the left
silo, z = 1 holds; if it fills the right one, then z = 0 applies.
(b) Calculate the time intervals ∆ti between switchovers of the swiveling tube
from one silo to the other. At time t = 0, V1 = 0.1V and V2 = 0.5V apply.
(c) Calculate the switching times ti at which the supplying mechanism swivels
from one silo to the other.
(d) Calculate the sum ttot of the times ∆ti of all switching intervals.
(e) Assess the control mechanism described here in terms of its applicability.
(f) What problem occurs if we simulate the system using an integration
method, e. g. a Runge-Kutta method?
Exercise 1.15 Let us examine the ordinary differential equation
x˙ = p(t)x + q(t)x
α
, α ∈ IR,
which is known as the Bernoulli differential equation.
(a) Execute the transformation z = x
1−α
.
(b) What type is the transformed Bernoulli differential equation?
Exercise 1.16 Determine the solution to the differential equation
x˙ = ax 
1 −
x
b

,
known as the logistic differential equation.
Exercise 1.17 Differential equations of the form
x˙ = f(x)g(t)
are called separable, since we can separate the variables x and t such that the
left side of the equation depends only on x and the right one only on t. Thus
we obtain
1
f(x)
dx = g(t)dt.
Determine the general solution of a separable differential equation.66 Chapter 1. Fundamentals of Nonlinear Systems
Exercise 1.18 Determine the analytical solutions x(t) of the following differ￾ential equations:
(a) x˙ = nx(n−1)/n, x(0) = 0,
(b) x˙ = −
2(1 + x
3
)
3x
2
, x(0) ∈ IR, hint: transform z = 1 + x
3
,
(c) x˙ = −x + x
2
, x(0) ∈ IR,
(d) x˙ = t
2
p
1 − x
2, x(0) ∈ [−1, 1], and
(e) x¨ =
p
1 + ˙x
2, x(0) ∈ IR, x˙(0) ∈ IR.
Exercise 1.19 Consider the differential equation
x˙ = − sgn(x)
p
|x|.
(a) Show that x(t) =



sgn(x0)
p
|x0| − 0.5t
2
, 0 ≤ t < 2
p
|x0|,
0, t ≥ 2
p
|x0|
for x0 ∈ IR\{0} is the only solution to the differential equation.
(b) Show that the equilibrium point xeq = 0 is globally asymptotically stable.
(c) What time teq does it take for the solutions x(t) to reach the equilibrium
point xeq = 0?
Exercise 1.20 The aim of this exercise is to determine the computing effort
of integration methods with different orders of accuracy qi
for the ordinary
differential equation x˙ = −x. For this purpose, take into account the functional
relationship between the relative error ε
rel = ˜αhq
and the step size h for the
various integration methods for the differential equation x˙ = −x in Figure
1.51, which shows a magnified area of Figure 1.43 in Section 1.2.10 on p. 55.
Display the results in table form.
(a) What is the relationship between the slopes and the order of accuracy qi
of each integration method? What are the slopes mi and the parameters
α˜i of the curves shown in the double logarithm plot in Figure 1.43 on
p. 55? To determine this, take only the approximately linear parts of the
curves into account.
(b) In each of the integration methods, what approximate step size h should
we choose for the example x˙ = −x such that the relative error is limited
to ε
rel = 10−6
? If necessary, extrapolate the curves shown in the graph.
(c) The time required to calculate the function f(x, u) normally takes up most
of the time required for the simulation of a common differential equation.
For the methods shown in Figure 1.43, estimate how often the function
f(x, u) must be calculated to determine the solution of the differential
equation x˙ = −x within the time interval [0, 10] if we wish to keep the
relative error below 10−6
.1.3. Exercises 67
10−10
10−8
10−6
10−4
10−2
100
102
104
10−2
10−1
Step size h
Relative error in
%
1
2
3
4
5
6 7
Fig. 1.51: Relative error in double logarithmic representation for the following meth￾ods: 1. Euler, 2. modified Euler, 3. Simpson, 4. fourth-order Runge-Kutta,
5. Adams-Bashforth with q = 4, 6. Adams-Moulton with q = 5 (one iteration),
7. Adams-Moulton with q = 5 (ten iterations)
(d) Why is it impossible, from a comparison of the integration methods’ orders
of accuracy q alone, to determine which method is the most suitable? What
is the most suitable one-step method? Based on the analysis above, which
integration method would you use for similar problems?
Exercise 1.21 Using the z-transformation, calculate the stability range of the
step size for the differential equation x˙ = −λx with λ > 0 for the following
integration methods:
(1) Simpson, (2) Runge-Kutta, 4th order,
(3) Adams-Moulton, 5th order, (4) Gear, 4th order.2
Limit Cycles and Stability Criteria
2.1 The Describing Function Method
2.1.1 Idea behind the Method
The describing function method, also called the method of harmonic balance,
is used to detect limit cycles in nonlinear control loops, which are structured
as depicted in Figure 2.1. It can also be used for nonlinear control loops that
have been transformed into such a structure. The control loop shown is re￾ferred to as the nonlinear standard control loop. It consists of a linear system,
represented here by its Laplace transfer function G(s), and a nonlinear char￾acteristic curve u = f(e), representing a controller or a plant’s nonlinearity,
for example.
The absence of a reference variable yref does not pose a serious problem,
since a constant reference variable can be shifted to zero via a transformation.
In any case, a limit cycle must also be ruled out for yref = 0. Thus it is
adequate to consider only the case yref = 0.
Nonlinear standard control loops are frequently encountered in practice,
either because nonlinear controllers are deliberately deployed, or because non￾linear characteristic curves are included as undesirable elements within the
control-loop structure, e. g. in the form of the limiting characteristic curve of
the actuator. Typical characteristic curves are illustrated in Figure 2.2.
e u y
Nonlinear characteristic Linear system
u = f(e) G(s)
Fig. 2.1: Nonlinear standard control loop
© Springer-Verlag GmbH Germany, part of Springer Nature 2024
J. Adamy, Nonlinear Systems and Controls,
https://doi.org/10.1007/978-3-662-68690-4_2
6970 Chapter 2. Limit Cycles and Stability Criteria
e
e
e
e
u
u
u
u
Two-position element Three-position element
Dead zone
(Insensitivity zone)
Limiting characteristic
(Saturation characteristic)
Fig. 2.2: Typical characteristic curves in control loops
A question that arises is whether limit cycles can occur in the above control
loop. To gradually approach the solution of this problem, we will first consider
the special case of a linear characteristic curve
u = f(e) = K · e.
In this case, the control loop takes the form shown in Figure 2.3. A permanent
oscillation, i. e. one that is self-sustaining within the control loop, arises if a
harmonic signal
e(t) = A · sin(ω0t) (2.1)
is fed into the control loop at the input of the linear characteristic and is
phase-shifted by π, i. e. 180◦
, and of amplitude A at the output of the linear
plant G(s), i. e.
y(t) = A · sin(ω0t − π) = −A · sin(ω0t);
because then this signal is fed into the control loop at the summation point,
the new signal e(t) = −y(t) corresponds to the old one (2.1), and the process
repeats. In this way, the loop generates a self-sustaining oscillation.
In the frequency domain, the above condition for a sustained oscillation is
formulated as
A · e
j(ω0t−π) = K · G(jω0) · A · e
jω0t
or
K · G(jω0) = −1. (2.2)2.1. The Describing Function Method 71
e u y
K G(s)
Fig. 2.3: Control loop with a linear controller
e
e
u
u
t t
u = f(e)
Fig. 2.4: Distortion of the input signal by the nonlinearity
This perpetuation of the sinusoidal oscillations at the input and the output
of the open loop is referred to as the state of harmonic balance.
From the above, we can now deduce the nonlinear situation as follows. If
we feed a sinusoidal signal
e(t) = A · sin(ω0t)
into the nonlinearity, we obtain a distorted sinusoidal signal at the output, as
illustrated in Figure 2.4.
The output signal u can be represented by a Fourier series as
u(t) = c0(A) +X∞
i=1
ci(A) · sin(i · ω0t + ϕi(A)).
If the nonlinearity is of the kind such that
c0(A) = 0 and ci ≪ c1, i = 2, 3, . . .
holds, i. e. the constant component is zero and the amplitudes of the harmonics
ci are small compared to the amplitude c1 of the fundamental wave, the output
u(t) of the controller can be approximated by
u(t) ≈ c1(A) · sin(ω0t + ϕ1(A)).
The condition c0(A)= 0 is fulfilled if the characteristic curve is point-symmetric
to the origin.
With the above approximation, we have linearized the nonlinearity f, and
the linear approximation has an amplitude-dependent gain. It is obtained from
the input and the output signal72 Chapter 2. Limit Cycles and Stability Criteria
e(t) = A · sin(ω0t) and u(t) = c1(A) · sin(ω0t + ϕ1(A))
or in phasor representations
e(t) = A · e
j(ω0t− π
2
)
and u(t) = c1(A) · e
j(ω0t− π
2 +ϕ1(A))
as
N(A) = u(t)
e(t)
=
c1(A)
A
· e
j ϕ1(A)
. (2.3)
This gain N(A) of the linearized characteristic element is called the describ￾ing function. Note that N(A) is not frequency-dependent. However, the gain
factor c1(A)/A and phase rotation ϕ1(A) do depend on the amplitude A of
the input signal.
The nonlinear characteristic element is now replaced by its linear approxi￾mation (2.3) in the control loop, as shown in Figure 2.5. For this linear control
loop the previously derived condition (2.2) for the state of harmonic balance,
i. e. a self-sustaining oscillation, is given by
N(A) · G(jω) = −1
or
G(jω) = −
1
N(A)
. (2.4)
Of course, the above equation only holds if the neglected harmonics occur￾ring at the frequencies 2ω0, 3ω0, 4ω0, . . . are sufficiently damped by the plant
G(s). This means that the transfer function G(s) must exhibit a sufficiently
strong low-pass behavior.
Summarizing the results above and generalizing them to the characteristic
curves
u = f(e, e˙),
we obtain the following heuristic.
e u y
N(A) G(jω)
Fig. 2.5: Linearized characteristic element and linear plant2.1. The Describing Function Method 73
Heuristic 1 (Harmonic Balance). Consider a nonlinear standard control
loop
Y (s) = G(s)U(s),
e = −y,
u = f(e, e˙).
Let the characteristic curve u = f(e, e˙) be point-symmetric with respect to the
origin, i. e. f(−e, −e˙) = −f(e, e˙) holds. Further, assume that the plant G(s)
possesses a sufficiently strong low-pass characteristic. If there exist values ω
and A, such that the equality
G(jω) = −
1
N(A)
is fulfilled, presumably a sustained oscillation occurs which has approximately
the frequency ω and the amplitude A.
The describing function N(A) is real-valued if the point-symmetric non￾linearity only depends on e. If it is also a function of e˙, N(A) usually has an
imaginary part as well.
Condition (2.4) can be evaluated graphically in the complex plane. To this
end, we will draw the Nyquist plot of G(jω) and the locus −1/N(A) of the de￾scribing function, which we will call nonlinear locus for short. If an intersection
point exists, equation (2.4) is fulfilled, and presumably a sustained oscillation
occurs. The frequency and amplitude of the presumed sustained oscillation
can also be approximately computed with the help of the intersection point.
2.1.2 Illustrative Example
To illustrate the method described above, we will consider the plant
G(s) = 9
s (s + 1) (s + 9),
which we control with a two-position element or relay characteristic
u = −b · sgn(y).
Figure 2.6 shows the corresponding control loop.
First we determine the describing function N(A) of the two-position con￾troller. For every sinusoidal signal input, its output function is a sequence of
square-wave signals. The associated Fourier series of the sequence of square￾wave signals is
u(t) = 4b
π

sin(ω0t) + sin(3ω0t)
3
+
sin(5ω0t)
5
+ . . .
.74 Chapter 2. Limit Cycles and Stability Criteria
e u y b
-b
9
s(s + 1)(s + 9)
Fig. 2.6: Control loop with two-position controller
Approximating
u(t) ≈
4b
π
sin(ω0t) = c1(A) · sin(ω0t),
we obtain
N(A) = c1(A)
A
=
4b
πA.
To detect a sustained oscillation, both sides of the equation of the harmonic
balance
G(jω) = −
1
N(A)
,
i. e.
9
jω(jω + 1)(jω + 9) = −
πA
4b
, (2.5)
are graphically illustrated. This is shown in Figure 2.7. Since an intersection
point of the Nyquist plot of G(jω) and the nonlinear locus −1/N(A) exists,
equation (2.5) is fulfilled for a value pair (ω, A), and we can conclude that a
sustained oscillation exists. Its amplitude and frequency are determined for
b = 1 from equation (2.5), i. e. from
−
4
πA = −
10
9
ω
2 + j
9ω − ω
3
9
,
to be
ω = 3 and A =
2
5π
≈ 0.127.
The approximative nature of the method becomes apparent when we compare
these values to a simulation, from which the values
ω = 2.5 and A = 0.195
are obtained.2.1. The Describing Function Method 75 Imaginary part
Real part
0.04
0.02
0
-0.02
-0.04
-0.4 -0.3 -0.2 -0.1 0 0.1
ω
−
1
N(A)
A → ∞
G(jω)
A = 0
Fig. 2.7: Nyquist plot of G(jω) and the nonlinear locus −1/N(A)
2.1.3 Characteristic Curves and Their Describing Functions
The method of harmonic balance is also applicable if the nonlinear character￾istic curve not only depends on e, but also on e˙. One of the most important
characteristic curves of this kind is the hysteresis curve
u =
(
b · sgn(e + a), e <˙ 0,
b · sgn(e − a), e >˙ 0
= b · sgn(e − a · sgn( ˙e))
shown in Figure 2.8. Hysteresis curves can be found in temperature-control
loops such as those in irons, among other applications. In these cases, the
heater is turned on using a bimetal. When this bimetal has heated up to
a sufficient temperature, it deforms in such a way that the contact opens,
turning off the heating current. After cooling down, the bimetal relaxes into
its original form, turns the heating current back on, and the cycle begins anew.
Backlash behavior is another frequently occurring nonlinearity which de￾pends both on e and on e˙. This behavior is described by the characteristic
curve shown in Figure 2.9. Backlash, as depicted in Figure 2.10, occurs in
mechanical systems as play between gear wheels, carriers, linkages, etc. Note
that, depending on the sign of e˙, the horizontal branches in Figure 2.9 are
passed through from both directions. The horizontal branches can occur for
every value of u. Backlash behavior can be modeled by
u˙ = me˙
h
H(me − u − ma)H( ˙e) + H(−me + u − ma)H(−e˙)
i
, (2.6)76 Chapter 2. Limit Cycles and Stability Criteria
u
−a a e
e <˙ 0
e >˙ 0
b
−b
Fig. 2.8: Hysteresis characteristic
curve
u
−a a e
e >˙ 0
e <˙ 0
Slope m
Fig. 2.9: Backlash characteristic curve
where the initial values u(0) and e(0) are restricted by
me(0) − ma ≤ u(0) ≤ me(0) + ma.
The parameter m is the backlash’s slope, the clearance is 2a, and
H(x) = (
0, x < 0,
1, x ≥ 0,
is the Heaviside function. For most systems, especially mechanical ones, the
slope is m = 1.
The describing functions N(A) of the hysteresis and backlash characteristic
curves are also determined by choosing a sine function as the input signal and
representing the output as a Fourier series. The describing function N(A) then
results again from the fundamental wave. The describing functions N(A) of
the hysteresis, backlash, and other important characteristic curves are given
in Table 2.1.
Many characteristic curves can be represented as an additive combination
of standard characteristic curves. This summation of characteristic curves
corresponds to a parallel connection of their function blocks. Examples are
step curves which are point-symmetric. They can be formed by adding three￾position characteristic curves. In this way, the step curve of an A/D converter
can be emulated, for example.
Since the relation
u = N1(A)e + N2(A)e = (N1(A) + N2(A)) e
holds, the total describing function Ntot(A) of k characteristic curves which
are connected in parallel, i. e.2.1. The Describing Function Method 77
a a
e
u
Fig. 2.10: Examples of systems with backlash behavior
u =
X
k
i=1
fi(e, e˙),
is given by the sum of the describing functions Ni(A) of the individual non￾linearities fi
, according to
Ntot(A) = X
k
i=1
Ni(A).
An important example of a composite characteristic curve Ntot(A) is a
piecewise linear continuous characteristic curve, i. e. a polygonal line. This
polygonal line can be constructed as a summation of dead zones or limiting
elements. In each of the 2k intervals
[±ai
, ±ai+1), i = 0, . . . , k,
of the polygon, there is one straight line with corresponding slope mi
. Further,
the polygon line is point-symmetric to the origin and consequently passes
through zero, so we obtain
u =



m1e, e ∈ [0, ±a1),
m2e ± (m1 − m2)a1, e ∈ [±a1, ±a2),
m3e ± (m1 − m2)a1 ± (m2 − m3)a2, e ∈ [±a2, ±a3),
.
.
.
mke ±
k
X−1
i=1
(mi − mi+1)ai
, e ∈ [±ak−1, ±∞).
We can construct the corresponding describing function of the polygon line by
using the describing function of a dead zone, which can be found in Table 2.1.78 Chapter 2. Limit Cycles and Stability Criteria
Table 2.1: Characteristic curves and their describing functions
Nonlinearity Describing Function N(A) and Nonlinear Locus −1/N(A)
Two-position
element
u
e
b
−b
Im
Re
A → ∞ A = 0
−
1
N(A)
N(A) = 4b
πA, A ≥ 0
Preload
u
e
b m
−b
Slope: m
Im
Re
A → ∞ A = 0
−
1
m
−
1
N(A)
N(A) = 4b
πA + m, A ≥ 0
Three-position
element
u
e
b
−b
a
−a
Im
Re
A→∞ A=a
√
2
A→a −
πa
2b
−
1
N(A)
N(A) = 4b
πAr
1−
 a
A
2
, A ≥ a2.1. The Describing Function Method 79
Table 2.1: Characteristic curves and their describing functions - continued
Nonlinearity Describing Function N(A) and Nonlinear Locus −1/N(A)
Dead zone
u
a e
−a
m
m
Slope: m
Im
Re
A → a A → ∞
−
1
m
−
1
N(A)
N(A) = m
 
1−
2
π
arcsin  a
A

−
2
π
a
A
r
1 −
 a
A
2
!
, A ≥ a
Saturation
u
e
b
−b
−a a
Slope: m =
b
a
Im
Re
A → ∞
−
1
m
A ≤ a
−
1
N(A)
N(A) =



m, 0 ≤ A ≤ a
2m
π
 
arcsin  a
A

+
a
A
r
1−
 a
A
2
!
, A > a
Power
u
e
u = e |e| ,
u = e
3
Im
Re
A → 0 A → ∞
−
1
N(A)
N(A) = 8A
3π
, for u = e |e| and A ≥ 0
N(A) = 3
4
A
2
, for u = e
3
and A ≥ 080 Chapter 2. Limit Cycles and Stability Criteria
Table 2.1: Characteristic curves and their describing functions - continued
Nonlinearity Describing Function N(A) and Nonlinear Locus −1/N(A)
Roots
u
e
u = sgn(e)
p
|e|,
u =
√3
e
Im
Re
A → ∞ A = 0
−
1
N(A)
N(A) = 1.11A
−1/2
, for u = sgn(e)
p
|e| and A ≥ 0
N(A) = 1.16A
−2/3
, for u =
√3
e and A ≥ 0
Dry friction
u
b
−b
e˙
Force of friction: b
u = −b sgn ( ˙e)
Im
Re
A → ∞
A = 0
−
1
N(A)
N(A) = −j
4 b
πA, A ≥ 0
Backlash
u
a e
−a
Slope: m
Im
Re
A → ∞
A → a
−
1
m
−
1
N(A)
N(A) = m
2
+
m
π

arcsin(α) + α
p
1−α2

− j
m
π
￾
1−α
2

with α = 1 −
2a
A
, A > 2.1. The Describing Function Method 81
Table 2.1: Characteristic curves and their describing functions - continued
Nonlinearity Describing Function N(A) and Nonlinear Locus −1/N(A)
Sharp hysteresis
u
e
b
−b
−a a
Im
Re
A → ∞ A = a
−
πa
4b
−
1
N(A)
N(A) = 4b
πAr
1 −
 a
A
2
− j
4ab
πA2
, A ≥ a
Elastic hysteresis
u
−a a
b
−b
e
m
Slope: m
Im
Re
A =
b + ma
a
A → ∞
−
1
N(A)
N(A) = m
π

µ

b + ma
mA 
+ µ

b − ma
mA  − j
4ba
πA2
,
µ(x) = arcsin(x) + x
p
1 − x
2, A ≥
b + ma
m
Three-position
element with
hysteresis
u
a e
−a
b
−b
c
−c
Im
Re
−
πa
4b
qa+c
a−c
−
πa
4b
A → ∞
A = a
−
1
N(A)
N(A)= 2b
πA r
1−
c
A
2
+
r
1−
a
A
2
!
− j
2b(a − c)
πA2
,
A ≥ a82 Chapter 2. Limit Cycles and Stability Criteria
For the polygonal line, we can then calculate
Ntot(A) = m1 +
k
X−1
i=1
(mi+1 − mi)

1 −
2
π
µ
ai
A

with
µ(x) = arcsin(x) + x
p
1 − x
2.
Here A ≥ ak−1 must hold.
2.1.4 Stability Analysis of Limit Cycles
Based on the Nyquist plot of G(jω) and the nonlinear locus −1/N(A), it is
not only possible to determine whether a limit cycle could exist; it is also
possible to deduce its stability behavior. Thus we can ascertain whether the
limit cycle is stable, semi-stable, or unstable.
For this purpose, we will assume that the limit cycle of amplitude Alc
has been determined. Then within a vicinity of the limit cycle, the behavior
of the control loop can be approximately described by the linear substitute
loop shown in Figure 2.3. In this case, the gain factor of the linear substitute
controller is
K = N(A).
If the amplitude is changed slightly by ∆A to
A = Alc + ∆A,
K also changes only slightly. With this change in the amplitude A, we have
left the limit cycle and have to answer the question of whether the trajectory
returns to the limit cycle or departs from it. This question can be answered
by examining the stability behavior of the linear substitute control loop (see
Figure 2.5) for changes in K = N(Alc + ∆A). Four cases are possible; see also
Figure 2.11.
For Case (a), let ∆A > 0 and the linear substitute control loop be unstable.
Thus the trajectory of the linear substitute control loop tends to infinity. Since
the linear substitute control loop is a good approximation of the nonlinear
control loop, we can conclude that the trajectory of the nonlinear loop departs
from the limit cycle.
For Case (b), let ∆A > 0 and the linear substitute control loop be stable,
so that the amplitude A decreases over time. Consequently, the trajectory of
the nonlinear control loop tends to the limit cycle.
For Case (c), let ∆A < 0 and the linear substitute control loop be unstable,
which is why the amplitude A increases over time and the trajectory of the
nonlinear control loop tends to the limit cycle.2.1. The Describing Function Method 83
∆A > 0
∆A = 0
Case (a): ∆A > 0 and unstable
linear substitute control loop
Case (b): ∆A > 0 and stable lin￾ear substitute control loop
∆A < 0
Case (c): ∆A < 0 and unstable
linear substitute control loop
Case (d): ∆A < 0 and stable lin￾ear substitute control loop
Fig. 2.11: Stability behavior for amplitude changes ∆A in the limit cycle
For Case (d), let ∆A < 0 and the linear substitute control loop be stable.
As a consequence, the trajectory of the nonlinear control loop departs from
the limit cycle.
Because both of the cases ∆A > 0 and ∆A < 0 occur in the control loop,
the stability of the limit cycle can be inferred from the following situations:
Situation 1: Cases (a) and (c) : semi-stable limit cycle,
Situation 2: Cases (a) and (d) : unstable limit cycle,
Situation 3: Cases (b) and (c) : stable limit cycle,
Situation 4: Cases (b) and (d) : semi-stable limit cycle.
Based on the simplified Nyquist criterion [130], we will decide whether the
linear substitute control loop becomes stable or unstable with a change in ∆A
and therefore a change in
K = N(Alc + ∆A).
In this way we can find out which of the above situations arises. From linear
systems theory, we know that the simplified Nyquist criterion is applicable to
an open loop with a transfer function K · G(s) that has exclusively poles λi
with Re {λi} < 0 and at most two poles at s = 0. If, in this case, the Nyquist
plot of G(jω) does not enclose or touch the critical point
−
1
K
= −
1
N(Alc + ∆A)
for 0 ≤ ω ≤ ∞, which is especially so when the Nyquist plot passes the
critical point on its right-hand side, the control loop is asymptotically stable.84 Chapter 2. Limit Cycles and Stability Criteria
G(jω)
−
1
N(A)
Situation 1: Cases (a) and (c) occur.
The limit cycle is semi-stable.
∆A<0 ∆A>0
−
1
N(A)
G(jω)
Situation 2: Cases (a) and (d) occur.
The limit cycle is unstable.
Alc
∆A>0 ∆A<0
−
1
N(A)
−1
N(Alc+∆A)
−1
N(Alc+∆A)
G(jω)
Situation 3: Cases (b) and (c) occur.
The limit cycle is stable.
−
1
N(A)
G(jω)
Situation 4: Cases (b) and (d) occur.
The limit cycle is semi-stable.
Fig. 2.12: Possible situations for the stability of limit cycles
Otherwise it is unstable. Thus, each of the four situations can be identified by
analyzing the Nyquist plot and the nonlinear locus, as shown in Figure 2.12.
For example, consider Situation 3 in Figure 2.12. For ∆A > 0, the Nyquist
plot of G(jω) passes by the point −1/N(Alc+∆A) on its right. Thus the linear
substitute control loop is stable. This situation corresponds to Case (b) from
Figure 2.11, i. e. the trajectory tends to the limit cycle from the outside. For
∆A < 0, on the other hand, the linear substitute control loop is unstable,
since the Nyquist plot of G(jω) passes by the point −1/N(Alc + ∆A) on its
left. This leads to Case (c) in Figure 2.11, and the trajectory tends to the
limit cycle from the inside. The limit cycle is therefore stable.
The calculations above are the basis for the following criterion for the
stability of limit cycles. Like the method of harmonic balance itself, it does not
provide a determination, only an indication of possible limit cycles’ stability
or instability.
Heuristic 2 (Stability of Limit Cycles). For a nonlinear standard control
loop with a plant G(s) having at most two poles at s = 0 and otherwise only
poles λi with Re {λi} < 0, a limit cycle is presumably2.1. The Describing Function Method 85
(1) stable if the nonlinear locus crosses the Nyquist plot from right to left at
the corresponding intersection point,
(2) semi-stable if the nonlinear locus is tangential to the Nyquist plot at the
corresponding intersection point,
(3) unstable if the nonlinear locus crosses the Nyquist plot from left to right
at the corresponding intersection point.
In this context, the directions left and right refer to the directions resulting
from moving along the Nyquist plot beginning at ω = 0.
We cannot normally observe unstable or semi-stable limit cycles in practi￾cal applications. Furthermore, an unstable or semi-stable limit cycle is often
not seriously critical for a control loop if the system has an asymptotically sta￾ble equilibrium point. This is the case because the trajectory departs from the
limit cycle, even for the smallest disturbances, and then, for example, tends
to the equilibrium point. Functions −1/N(A) with a nonlinear locus which,
as shown in Situation 2 of Figure 2.12, aim toward the origin usually do not
lead to stable limit cycles. This is due to the fact that the nonlinear locus
−1/N(A) crosses the Nyquist plot of G(jω) from left to right if the Nyquist
plot of G(jω) progresses in a clockwise rotating manner. The latter is usually
the case.
For the harmonic balance method, there are a number of extensions for
control loops with multiple characteristic curves, asymmetric characteristic
curves, and discrete controllers [88, 147, 319, 446].
2.1.5 Example: Power-Assisted Steering System
Now we will consider a power-assisted steering system for motor vehicles which
utilizes the principle of angle superposition [225, 243, 244]. In a vehicle’s steer￾ing system, a high steering transmission ratio is used to reduce the steering
wheel torque for the driver. A resulting disadvantage is the large steering
wheel angle. A motor-powered superposition gear therefore reduces the steer￾ing wheel angle to increase driving comfort. To do this, the superposition
gear generates a supplementary angle δ2 which is superposed onto the steer￾ing wheel angle δ1. Both together produce the output angle δy. Figure 2.13
shows the principle structure.
Steering systems with angle superposition are also used for active steering.
At low speeds, active steering reduces the steering wheel angle that the driver
must set for cornering. In this way, a driver can drive along narrow curves
with small movements of the steering wheel. At high speeds, the superposition
motor counter-steers. Hence large steering wheel rotations lead only to small
steering angles. This increases driving safety in this velocity range.
Normally the driver holds onto the steering wheel and specifies a steering
angle δ1. When the driver releases the steering wheel, this specification of the
steering angle disappears, and limit cycles can occur in the steering system,
which of course are undesired.86 Chapter 2. Limit Cycles and Stability Criteria
δ1
δ2, M
δy
Fig. 2.13: Power-assisted steering system which uses the principle of angle superpo￾sition
In order to detect possible limit cycles, we must examine the steering
control loop for this particular case where the driver has released the steering
wheel. It is shown in Figure 2.14. The servomotor with a current control
loop and differential drive constitutes the plant to be controlled, whose input
variable is the required torque value M of the servomotor. One output variable
of this plant is the output angle
δy = δ1 + δ2.
On the other hand, the steering wheel angle δ1 is also an output variable,
since the driver does not hold onto the steering wheel. The PD controller for
the supplementary angle δ2 has the parameters
KPD = 3000 N m rad−1
and T D = 0.02 s.
The servomotor including the current control and superposition gear is
described by the linear state-space model2.1. The Describing Function Method 87
KP
PD controller Motor with gearbox
Mc M
δ2
˙δ2
δy
δ1
KPD(δ2,ref − δ2
−T D
˙δ2)
δ˙ = Aδ + bM
δy = δ1 + δ2
δ2,ref
Fig. 2.14: Control loop of the steering system for a steering wheel that is not held
by the driver
Mc M −Mc
G(s)
Fig. 2.15: Steering system’s nonlinear standard control loop, which is derived by
transformation of the control loop depicted in Figure 2.14
δ˙ =




0 0 1 0
0 0 0 1
−67.3568 −67.3568 −11.3988 −11.3988
−24.1480 −24.1480 −4.0866 −4.0866




δ +




0
0
−4.0123 · 10−2
1.8977




M.
Here the state vector is given by
δ =

δ1 δ2
˙δ1
˙δ2
T
.
The torque M is limited to |M| ≤ 21 N m. Thus, if the PD controller
Mc =KPD(δ2,ref − δ2 − T D
˙δ2)
specifies torques Mc, which exceed these limits, they are limited by the satu￾ration characteristic
M =



Mmax, Mc > Mmax,
Mc, |Mc| ≤ Mmax,
−Mmax, Mc < −Mmax.
The steering wheel angle δ1 specifies the reference value δ2,ref of the sup￾plementary angle δ2 for a given vehicular velocity by using the factor KP = 1.5.
Since the driver is not holding onto the steering wheel, the superposition gear
impacts directly upon the steering wheel angle δ1, as shown in Figure 2.14.
The control loop in Figure 2.14 can be transformed into a nonlinear stan￾dard control loop, as shown in Figure 2.15. Taking into account the dynamics88 Chapter 2. Limit Cycles and Stability Criteria
30
25
20
15
10
5
0
-5
-50
-1500 -1000 -500 0
Real part
Imaginary part
G(jω)
−
1
N(A) A
ω
ω1 ω2
Fig. 2.16: Nyquist plot of G(jω) and nonlin￾ear locus −1/N(A)
of the current-controlled servomotors with a superposition gear, the transfer
function G(s) is obtained as
G(s) = −
Mc(s)
M(s)
=
113.9s
3 + 7181s
2 + 171200s + 965900
s
2(s
2 + 15.49s + 91.50) .
Applying the describing function method results in two intersecting points
of the Nyquist plot of G(jω) with the nonlinear locus −1/N(A) of the sat￾uration element’s describing function, as shown in Figure 2.16. We therefore
expect two limit cycles with frequencies close to ω1 and ω2. For the left-hand
intersecting point in Figure 2.16, the Nyquist plot of G(jω) is crossed from
right to left by the nonlinear locus curve. This limit cycle with frequency ω1 is
stable. The second limit cycle, on the other hand, is unstable and thus not of
practical significance, in contrast to the stable limit cycle, which is of interest.
The latter’s period length is determined by solving the equation of harmonic
balance G(jω) = −1/N(A), to yield
Tharm =
2π
ω1
= 1.97 s.
Because of the approximating nature of the harmonic balance, this value is
imprecise. By simulating the system as shown in Figure 2.17, the period length
can be determined to be Tsim = 2.42 s.
As can be seen from Figure 2.17, the oscillation has a high amplitude,
which is, of course, out of the question for a motor vehicle. The limit cycle
can be eliminated with an extension of the control system [244].2.2. Absolute Stability 89
δ1
δ2
δy
6
4
2
0
0
-2
-4
-6
50
-50
0
0
0.5
0.5
1
1
1.5
1.5
2
2
2.5
2.5
Time t in s
u in Nm δ1, δ2, δy in rad
Fig. 2.17: Time courses of the angles δ1, δ2, δy, and the actuating
variable u of the limit cycle
2.2 Absolute Stability
2.2.1 The Concept of Absolute Stability
In the previous section, we examined the nonlinear standard control loop,
which is illustrated again in Figure 2.18, for limit cycles. Of course, it is also
relevant to analyze the stability behavior in the absence of limit cycles. For
example, it is useful for practical purposes if we can determine for which
characteristic curves f the control loop has a globally asymptotically stable
equilibrium point.
We will address this question below, focusing our attention on character￾istic curves which lie in a sector bounded by two lines
u = K1e and u = K2e.
e u y
u = f(e) G(s)
Characteristic curve Plant
Fig. 2.18: Nonlinear standard control loop90 Chapter 2. Limit Cycles and Stability Criteria
Figure 2.19 plots this sector, for which we will use the short-hand notation
[K1, K2], as it is commonly utilized for intervals. Next, we will define a new
stability term [10] which is related to this sector [K1, K2].
Definition 18 (Absolute Stability). The nonlinear standard control loop
Y (s) = G(s)U(s),
e = −y,
u = f(e)
is called absolutely stable in a sector
[K1, K2]
if for every function
u = f(e)
which lies in this sector, is defined for all values e ∈ IR, and is piecewise
continuous, it possesses a globally asymptotically stable equilibrium point.
Here, Y (s) and U(s) denote the Laplace transform of the signals y(t) and
u(t), respectively.
It is already possible to initially estimate the size of the sector of absolute
stability[1]. Taking only linear characteristics into account, i. e. functions
u = f(e) = K · e,
we can determine the maximum range of parameters
(KH1, KH2) (2.7)
which result in an asymptotically stable linear control loop by using the Routh
criterion or the root locus method. The sector (2.7) is called the Hurwitz
sector [2]. Obviously, the maximum sector of absolute stability is always smaller
than or equal to the Hurwitz sector. Figure 2.20 illustrates the situation.
[1] The problem of finding conditions which guarantee that the nonlinear standard
control loop has a globally stable equilibrium point xeq is known as the Lur'e
problem. In this context, and when the nonlinearity has been shifted into the
feedback connection, the nonlinear standard control loop is usually called a Lur'e
system, which is shown in the figure below. A. I. Lur'e was the first to examine
the stability of this class of systems [290, 291].
v
G(s)
v=f(y)
y
[2] Only gains K ∈ (KH1, KH2) lead to an asymptotically stable control loop. The
non-included interval endpoints KH1 and KH2 lead to unstable or Lyapunov￾stable control loops. This is similar in the case of the largest sector of absolute
stability.2.2. Absolute Stability 91
u
e
u = K1 e
u = K2 e u = f(e)
Fig. 2.19: Sector of characteristic
curves bounded by u = K1e and u =
K2e
u
e
u=KH1 e
u=KH2 e
Hurwitz sector
Sector of absolute
stability
Fig. 2.20: Hurwitz sector and sector of
absolute stability
2.2.2 The Popov Criterion and Its Application
A criterion for showing absolute stability was developed by V. M. Popov in
1959 [278, 365, 366, 367, 368, 369, 370, 371, 372]. It is easy to use and allows
graphical interpretation. For its formulation, it is necessary to first define the
concept of strict marginal stability.
Definition 19 (Marginal Stability and Strict Marginal Stability). A
linear system with the transfer function G(s) is called marginally stable if it
has only poles pi with
Re {pi} ≤ 0,
where Re {pi} = 0 holds for at least one pole. We call a marginally stable
linear system strictly marginally stable if the linear control loop
Gε(s) = G(s)
1 + ε G(s)
is stable for every arbitrarily small ε > 0.
Viewed in graphical terms, the strict marginal stability of a system means
that the branches of the root locus, which begin on the imaginary axis, tend
to the left for increasing ε. This is illustrated in Figure 2.21 using the plant
G(s) = s + b
s
2 + a
, a > 0, b > 0,
as an example.
Popov’s criterion is formulated as follows.92 Chapter 2. Limit Cycles and Stability Criteria
Im
Re
Root locus
Fig. 2.21: Example of a root locus of a strictly marginally stable system
Theorem 6 (Popov Criterion). Let us consider the nonlinear standard
control loop
Y (s) = G(s)U(s),
e = −y,
u = f(e)
with the asymptotically stable or strictly marginally stable plant G(s). Further,
let the degree m of the numerator of G(s) be less than the degree n of the
denominator, and let the function u = f(e) be defined for all e ∈ IR, piecewise
continuous, and pass through the origin. Then the control loop defined above
is absolutely stable
(1) in the sector [0, K] in the case of an asymptotically stable G(s),
(2) in the sector [ε, K] with an arbitrarily small ε > 0 in the case of a strictly
marginally stable G(s)
if it is possible to find a real number q such that the Popov inequality
Re {(1 + q · jω) · G(jω)} > −
1
K
is fulfilled for all ω ≥ 0.
Two aspects of this criterion require some explanation. On the one hand, we
have to consider the applicability being limited to the sectors [0, K] and [ε, K]
and, on the other hand, the Popov inequality requires further discussion.
First we will elaborate on the sector of applicability. The distinction be￾tween the sectors [0, K] for stable plants G(s) and [ε, K] for strictly marginally
stable plants G(s) can be explained by the fact that a strictly marginally sta￾ble plant G(s) and a characteristic curve
u = 0 · e
result in a control loop which is not globally asymptotically stable. At the
least, an amplification of ε, i. e.2.2. Absolute Stability 93
u = ε · e
is required to stabilize the control system.
The distinction between the sectors [0, K] and [ε, K] also has an effect on
the characteristic curves u = f(e) being discussed. For instance, there are
characteristic curves which lie in [0, K] but not in [ε, K]. This is also the case
although ε > 0 is allowed to be arbitrarily small. For purposes of illustration,
we will look at two examples. The first example is depicted in Figure 2.22.
Here the characteristic curve u = f(e) tends to zero for e → ∞, but never
reaches this value. Obviously, there is no sector [ε, K] for which the line u = ε·e
does not intersect the characteristic curve. In the second example, which is
illustrated in Figure 2.23, the function
u = sgn(e)
p
|e|
tends to infinity for e → ∞, but it does so more weakly than any line u = ε · e,
so that there is always an intersection point. Thus in this example as well, the
characteristic curve u = sgn(e)
p
|e| will not lie in a sector [ε, K].
u
e
Fig. 2.22: Characteristic curve
which tends to zero for e → ∞
u
e
Fig. 2.23: Characteristic curve
which tends to ∞ for e → ∞
The limitation to sectors [0, K] instead of [K1, K2] is only seemingly re￾strictive. By rearranging the control loop from Figure 2.18, for which we will
consider the sector [K1, K2], K1 = 0 can be obtained. This is done by adding
two proportional elements with a gain of K1, as shown in Figure 2.24, to the
control loop. Obviously, the two proportional elements in effect cancel each
other out, so that the control loop remains unchanged. By combining the
two subsystems, we obtain the control loop depicted in Figure 2.25. For the
rearranged control loop, we have to look at the transformed sector
[K1 − K1, K2 − K1] = [0, K = K2 − K1].
Furthermore, unstable control loops can be stabilized by these transforma￾tions, so that the Popov criterion is also applicable to such plants.
Now that the Popov criterion’s area of applicability has been clarified, let
us consider its application. In particular, we would like to determine whether
it is possible to find a real number q such that the Popov inequality94 Chapter 2. Limit Cycles and Stability Criteria
f(e) G(s)
K1 K1
e u y
Fig. 2.24: Inserting a factor K1 into the nonlinear standard control loop
e u y
ˆf(e) = f(e) − K1e Gˆ(s) = G(s)
1 + K1G(s)
Fig. 2.25: A standard nonlinear control loop which is an equivalent version of the
control loop shown in Figure 2.24
Re {(1 + q · jω) · G(jω)} > −
1
K
is fulfilled for all ω ≥ 0 .
The solution of the Popov inequality can be illustrated in graph form. To
do this, we first need to rearrange the inequality to obtain
Re {G(jω)}
| {z }
X(ω)
−q · ω Im {G(jω)}
| {z }
Y (ω)
> −
1
K
.
This leads to the inequality
X(ω) − q · Y (ω) + 1
K
> 0, (2.8)
which is parameterized in terms of ω. The above inequality is fulfilled for all
points (X, Y ) which lie to the right of the line defined by
X − q · Y +
1
K
= 0.
The slope of this line is equal to 1/q, and its intersection with the real axis
is at −1/K. Figure 2.26 illustrates this. The line is referred to as the Popov
line.
We do not need to check for all values of X and Y to see if they satisfy
equation (2.8), since they depend on ω via2.2. Absolute Stability 95
X − qY > −
1
K
Slope 1
q
X
Y
−
1
K
Fig. 2.26: Sector (blue), in which X − qY > −1/K is fulfilled
X(ω) = Re {G(jω)},
Y (ω) = ω Im {G(jω)}.
(2.9)
The curve defined by equation (2.9) can be pictured in the coordinate plane
with X and Y as its coordinates. It is similar to a Nyquist plot if we set
G˜(jω) = X(ω) + jY (ω) = Re {G(jω)} + jω Im {G(jω)}.
The related locus is termed the Popov plot. It results from the Nyquist plot
of G(jω) when we multiply the imaginary part of G(jω) by ω. The graphic
interpretation of this is that the Nyquist plot of G(jω) is modified in the
direction of the imaginary axis. However, the real part is left unchanged, as
we see in Figure 2.27.
The possible values for X and Y are thus given by equation (2.9): they
are the values of the Popov plot. This result can be depicted in a graph and
utilized as follows. Popov’s inequality is fulfilled if the Popov plot lies on the
right-hand side of a line with the arbitrary slope 1/q which intersects the
X-axis, i. e. the real axis at −1/K.
With the above method, it is possible to solve the Popov inequality graph￾ically. Obviously, all lines to the left of the Popov plot fulfill the inequality.
From these, the value of −1/K and thus the sectors of absolute stability
[0, K] or [ε, K]
are directly visible. Clearly, we are interested in determining the largest pos￾sible sector. To do this, the Popov line is shifted to the Popov plot such that
it is its tangent, which results in the largest possible K = KPo, as shown in
Figure 2.28. The corresponding sector
[0, KPo) or [ε, KPo)96 Chapter 2. Limit Cycles and Stability Criteria
Popov
line
Popov plot
Arbitrary
slope 1
q
−
1
K
G(jω)
ω = 1
Re
Im
Fig. 2.27: The Nyquist plot of G(jω),
the Popov plot, a possible Popov line,
and a region (blue) where Popov’s in￾equality is fulfilled
Critical
Popov line
Popov plot
−
1
KPo
Re
Im
Fig. 2.28: The critical Popov line is
the tangent to the Popov plot which
produces the largest Popov sector.
is called the Popov sector . For the gain KPo of the Popov sector [0, KPo),
stability is not proven by the Popov criterion, since the Popov inequality
requires values Re{G(jω)} − qωIm {G(jω)} to be greater than, not greater
than or equal to −1/K. However, this distinction is not relevant in practice,
since, in any practical application, a safety distance from the critical value
KPo is always maintained.
It should be noted that the Popov criterion is of course also applicable if
the control loop takes the form
x˙ = Ax − bf(c
T x)
and its linear plant is controllable and observable.
Below we will consider two extensions of the Popov criterion. The Popov
criterion is also applicable if G(s) takes the form
G(s) = Gˆ(s) · e
−T s
and Gˆ(s) is asymptotically stable. In this case, however, only positive values
of q are allowed in the Popov inequality and the nonlinearity is required to be
continuous [277, 373].
In the case that the nonlinearity is time-variant, i. e. if u = f(e, t) holds,
the Popov criterion is also applicable. However, in this case, the following
additional restrictions apply [391]:
(1) 0 <
f(e, t)
e
< K, e 6= 0, and f(e, t) = 0 for all t ∈ IR,
(2) G(s) has no more than one pole at s = 0, and otherwise only poles si with
Re {si} < 0, and2.2. Absolute Stability 97
(3) q = 0.
The requirement q = 0 implies that the Popov line is vertical.
2.2.3 The Aizerman and Kalman Conjectures
For many nonlinear standard control loops, the application of the Popov crite￾rion is unnecessary. This is because for these systems, Aizerman’s conjecture
[9] holds. It states that the largest sector of absolute stability is identical to
the Hurwitz sector (2.7), which is much easier to determine. Even though the
conjecture does not hold for cases in general [93, 122], it is true for all first￾and second-order[3] plants with m < n. It is also true for all third-order plants
without zeros and only real poles [10, 37, 139]. If we restrict ourselves to sec￾tors [0, KH2) and [ε, KH2), i. e. if we only consider the nonnegative part of
the Hurwitz sector, Aizerman’s conjecture also holds for the following plants
[404, 478]:
(1) asymptotically stable third-order systems with
G(s) = b1s + b0
s
3 + a2s
2 + a1s + a0
or strictly marginally stable systems with
G(s) =
1
s
·
b1s + b0
s
2 + a1s + a0
,
(2) fourth-order systems without zeros, with at most one pole at s = 0, and
otherwise only real negative poles.
Similar to Aizerman’s conjecture, Kalman [217] guesses equality of the
Hurwitz sector and the sector of absolute stability if the nonlinearity’s deriva￾tive is bounded by
K1 <
∂f(e)
∂e < K2. (2.10)
Note that this restriction is stricter than Aizerman’s requirement of a sector
K1 <
f(e)
e
< K2, e 6= 0. (2.11)
In both cases above, we assume that f(0) = 0 holds. Thus, every function f
which fulfills inequality (2.10) satisfies inequality (2.11).
The Kalman conjecture is also false in general [255, 271]. However,
Kalman’s conjecture is valid in some cases not covered by the cases in which
Aizerman’s conjecture was proven to be true. For example, it holds for all
[3] There is a special case [10, 57, 380] in which Aizerman’s conjecture does not hold
for second-order systems, but it has no practical significance in engineering tasks.98 Chapter 2. Limit Cycles and Stability Criteria
linear systems of the third order or below with m < n; thus, for these, the
sector of absolute stability and the Hurwitz sector are identical if inequality
(2.10) holds [28, 270].
Furthermore, similar to the Kalman conjecture, we can easily calculate a
sector of absolute stability
[0, KH2) or [ε, KH2)
for nonlinear standard control loops which may have an integrator and a
delay. Here, the gain KH2 > 0 is again the upper bound of the Hurwitz sector
(KH1, KH2), which results when we use only linear characteristics u = Ke.
The gain KH2 is also known as the Nyquist value of a linear plant, since it
results from the Nyquist criterion and indicates the control loop’s gain where
the system’s behavior changes from stability to instability. We use the results
from [498] and obtain
Theorem 7 (Absolute Stability of Control Loops with Delay). Con￾sider a nonlinear standard control loop with a plant
G1(s) = be−T s
D(s)
or G2(s) = be−T s
sD(s)
where b > 0 and T ≥ 0, and a nonlinear controller
u = f(e) with f(0) = 0.
If
(1) the polynomial D(s) has only roots λi with
Re {λi} < 0 and |Im {λi}| < |Re {λi}|
and
(2) the nonlinearity’s derivative is bounded by
0 ≤
∂f(e)
∂e < KH2,
where KH2 is the Nyquist value of G1(s) or G2(s),
then the nonlinear standard control loop is absolutely stable in the sector
[0, KH2) for G1(s) and in the sector [ε, KH2) for G2(s).
At least for the plants above, it is possible to simply determine the largest
possible sector of absolute stability or its nonnegative part by calculating the
Hurwitz sector (KH1, KH2) or its subsector [0, KH2).
Next we will look at an example to complement the above considerations.
The plant is given by
G(s) = 1
(s
2 + 0.1s + 10)(s
2 + 0.2s + 20).2.2. Absolute Stability 99
Re
Im
0.6
0.2
-0.2
-0.6
-1
-0.2 -0.1 0 0.1 0.2
Popov
plot
−
1
KPo
G(jω)
−
1
KH2
Fig. 2.29: Example of a system for which the Hurwitz sector is larger than the Popov
sector
Figure 2.29 shows the corresponding Nyquist and Popov plots, so that the
Popov sector can be graphically calculated as [0, KPo ≈ 8.7). From the
Nyquist plot, it also becomes clear that the nonnegative Hurwitz sector with
the maximum slope KH2 is larger than the Popov sector, since it holds that
KPo ≈ 8.7 <
1012
45
= KH2.
However, the fact that the Popov sector is smaller than the Hurwitz sector
is not sufficient for us to conclude that the sector of absolute stability is
smaller than the Hurwitz sector. This is because the proof [431] of the Popov
criterion only shows its sufficiency, and we do not know if it delivers the largest
sector of absolute stability. Thus, Aizerman’s conjecture could still hold for
the above system. As mentioned previously, it does not hold for every system
[93, 122, 246, 362, 363]. However, this is difficult to prove in specific cases.
The following general rule deserves emphasis:
Popov sector ≤ largest sector of absolute stability ≤ Hurwitz sector.
So far, we have not addressed the question whether the Popov sector [0, KPo)
is always equal to the largest nonnegative sector of absolute stability [0, Kas).
It has been shown that this is not the case [281, 377].
2.2.4 Example: Controlling a Ship
As an example application, we will look at the control of the course of the
US Coast Guard ship Tampa, which is shown in Figure 2.30. The controlling
mechanism should be designed in such a way that the change in the angle of100 Chapter 2. Limit Cycles and Stability Criteria
ϑ
ϕ
Fig. 2.30: Ship Tampa
the course ϕ of 10◦
requires a maximum settling time of 50 s. In this case, the
control variable ϑ should not exceed ±10◦
. The yaw rate ϕ˙ is measured with
a gyroscope.
The Tampa’s transfer function G(s) which relates the rudder angle ϑ to
the yaw rate ϕ˙ has been experimentally determined [133] to be
G(s) = Ω(s)
Θ(s)
=
−0.0184(s + 0.0068)
(s + 0.2647)(s + 0.0063),
where Ω(s) and Θ(s) are the Laplace transforms of ω = ˙ϕ and ϑ, respectively.
The variable to be controlled is ϕ. For this reason, the yaw rate ϕ˙, which
is measured by the gyroscope, is integrated to obtain ϕ. A cascade control
with two proportional controllers for the yaw rate and the course angle are
used. The integral part of the plant ensures that no permanent control error
exists. To limit the rudder angle to ±10◦
, a saturation element is added in
front of the plant. The control loop is shown in Figure 2.31. The controls
are designed in such a way that a 10◦
change in the course corresponds to a
maximum control variable of 10◦
. In this case, the control loop is linear. For
larger changes in the course, the actuating variable saturates, i. e. the control
loop becomes nonlinear.
Now the question arises whether the control loop is also stable in nonlinear
cases. To investigate this using the Popov criterion, the control loop is first2.2. Absolute Stability 101
0.08 −12.5
10
10
G(s)
1
s
ϕref ϑˆ ϑ ϕ˙ ϕ
Rudder angle Course angle
P controller P controller Yaw rate
Fig. 2.31: Course control loop of the Tampa
1
10 s
10 0.0184(s + 0.0068)
(s + 0.2647)(s + 0.0063) 12.5(s + 0.08)
e u ϕ˙ ϕ
Fig. 2.32: Control loop of the ship in the form of a nonlinear standard control loop
u
e −ec ec
Fig. 2.33: Modified saturation characteristic curve
transformed into the nonlinear standard control loop. Figure 2.32 shows the
result of this transformation. Since the plant contains an integrator and is
strictly marginally stable, only sectors [ε, K] can be obtained using the Popov
criterion. The sector [ε, K > 1], however, does not include the saturation
characteristic curve, since its parallel to the abscissa always intersects any
arbitrary line with slope ε > 0. However, for small values of ε, this only
happens for large values of e. The saturation characteristic curve thus does
not lie in the sector [ε, K > 1], and the Popov criterion is hence inapplicable
in this case.
Because we wish to apply it anyway, we will resort to the following remedy.
We can note that it is not possible for arbitrarily large values of the control
error e to occur in a practically operated control loop. For this reason, the
saturation characteristic curve may be allowed to increase further beyond
some determinable value ec, which is never exceeded in a practically operated
system. This change in the characteristic curve, as depicted in Figure 2.33,102 Chapter 2. Limit Cycles and Stability Criteria
does not affect the behavior of the control loop. With this modification, the
characteristic curve lies in the Popov sector [ε, K > 1] and absolute stability
can be ensured.
It is worth noting that a reference input ϕref 6= 0 does not affect the
applicability of the Popov criterion. This is because the negated reference
input −ϕref can also be interpreted as the initial value ϕ(0) of the course
angle.
The corresponding Popov plot of the ship is depicted in Figure 2.34. It is
evident that a line can be shifted from the left such that it touches the Popov
plot and passes through the origin. The system is thus absolutely stable in
the sector [ε, ∞). Im
ϕ in degrees
Re Time t in s
ω = 0
ω → ∞
90◦
45◦
10◦
0.05
0
0
-0.05
-0.1
-0.15
-0.2
-0.25
100
80
60
40
20
-0.4 -0.2 0 0.2 0.4 0.6 0 50 100 150 200 250
Fig. 2.34: Popov plot of the ship Fig. 2.35: Course angle ϕ of the
ship
y in m
ϑ in degrees
x in m Time t in s
90◦
45◦
10◦
90◦
45◦
10◦
1600
1200
800
400
0
0
-2
-4
-6
-8
-10
0 500 1000 1500 2000 50 100 150 200 0 250
Fig. 2.36: View of the ship’s
course from the top
Fig. 2.37: Control signal, i. e.
rudder angle ϑ2.2. Absolute Stability 103
Figures 2.35, 2.36, and 2.37 show the simulated trajectories of the course
control for course angle changes of 10◦
, 45◦
, and 90◦
beginning at t = 10 s.
Here the velocity of the ship is assumed to be 10 m s−1
. Figure 2.37 illustrates
that, for a course change exceeding 10◦
, the control signal reaches saturation.
The ship’s rudder then has its maximum deflection. It also becomes clear from
these figures that, due to the saturation effect, large changes in the course are
carried out more slowly than small changes. This is reasonable, since large
curves should not be navigated as rapidly as small ones. Figure 2.36 shows
the curves that were navigated and the ship’s course changes from the top,
i. e. in the xy-plane.
2.2.5 The Circle Criterion
Like the Popov criterion, the circle criterion allows us to analyze nonlinear
standard control loops, such as the one shown in Figure 2.38, regarding abso￾lute stability. The nonlinear function
u = f(e, t)
can be time-varying and lies in a slope sector [K1, K2] with
0 ≤ K1 < K2,
i. e. it holds that
K1 ≤
f(e, t)
e
≤ K2, e 6= 0.
Consequently,
f(0, t) = 0
must be fulfilled. Figure 2.39 illustrates this case.
The circle criterion [330, 401, 402, 491, 492] confirms whether absolute sta￾bility of the standard nonlinear control loop is guaranteed in a sector [K1, K2].
Like the Popov criterion, however, the circle criterion is only a sufficient con￾dition. Thus, in general, we do not know whether we have determined the
maximum possible sector of absolute stability.
e u
u = f(e, t) G(s)
y
Fig. 2.38: Nonlinear standard control loop with a time-varying characteristic curve104 Chapter 2. Limit Cycles and Stability Criteria
e
u
K1e
K2e
f(e)
Fig. 2.39: Slope sector [K1, K2] of the circle criterion
Theorem 8 (Circle Criterion for Asymptotically Stable Systems).
Let the nonlinear standard control loop
Y (s) = G(s)U(s),
e = −y,
u = f(e, t), e ∈ IR, t ∈ IR,
be given whose open-loop transfer function G(s) is coprime, has only poles λi
with Re {λi} < 0, and has a degree m of the numerator which is less than
the degree n of the denominator. Let the function u = f(e, t) lie in the sector
[K1, K2] with 0 ≤ K1 < K2, and let f(0, t) = 0 hold for all t ∈ IR. If the
Nyquist plot of the frequency response G(jω) does not encompass, intersect,
or touch the circle D(K1, K2) which has its center on the real axis of the
complex plane and passes through the points
−
1
K1
and −
1
K2
,
then the control loop is absolutely stable in the sector [K1, K2].
The application of the circle criterion is simple. We need merely sketch
the Nyquist plot of G(jω) of the open loop for 0 ≤ ω < ∞ and the circle
D(K1, K2), as shown in Figure 2.40 and Figure 2.41. The nonlinear standard
control loop is stable for all circles which lie to the left of the Nyquist plot
of G(jω). Obviously, many circles fulfill this condition. The largest of these
circles intersects the real axis at −1/K and is of infinite diameter. In this case,
we obtain the sector [0, K]. Note that the circle is not allowed to intersect or
touch the Nyquist plot. Sectors of different circles must not be combined to
obtain a larger sector of absolute stability.2.2. Absolute Stability 105
Largest
possible
circle G(jω)
−
1
K
−
1
K1
−
1
K2
Im
Re
Fig. 2.40: Application of the circle cri￾terion
Popov line
G˜(jω)
G(jω)
Im
Re
Fig. 2.41: Comparison of the Popov
and circle criteria
The circle criterion is somewhat easier to apply than the Popov criterion,
since we only need the Nyquist plot of G(jω) and not the Popov plot. In
specific cases, however, the circle criterion yields a different sector of absolute
stability. Figure 2.41 illustrates such a case, where the Popov plot is denoted
with G˜(jω).
In the case of an unstable plant or sectors with negative K1, the sector
transformation we introduced for the Popov criterion in Section 2.2.2 is ap￾plied. In this way, we obtain an asymptotically stable plant or a transformed
sector with K1 ≥ 0 and the circle criterion is applicable again. As an alter￾native to this approach, the circle criterion can be formulated in such a way
[308, 402, 465] that the sector transformation is not required and the crite￾rion encompasses the cases mentioned above. Then the following criterion for
systems without poles on the imaginary axis holds.
Theorem 9 (Circle Criterion for Systems without Poles on the
Imaginary Axis). Let the nonlinear standard control loop
Y (s) = G(s)U(s),
e = −y,
u = f(e, t), e ∈ IR, t ∈ IR,
be given whose open-loop transfer function G(s) is coprime and has a degree
m of the numerator which is less than the degree n of the denominator; let
G(s) have ν ∈ {0, 1, . . . , n} poles λi with Re {λi} > 0 and no poles λi with
Re {λi} = 0. Let the function f(e, t) lie in the sector [K1, K2], let f(0, t) = 0
for all t ∈ IR, and let D(K1, K2) represent the circle in the complex plane
which passes through the points
−
1
K1
and −
1
K2106 Chapter 2. Limit Cycles and Stability Criteria
and which has its center on the real axis. In this case, the control loop is
absolutely stable in the sector [K1, K2]
(1) for 0 < K1 < K2 if the Nyquist plot of G(jω) for frequencies ω ranging
from −∞ to ∞ does not intersect or touch the circle D(K1, K2), and
orbits it ν times in a counterclockwise direction,
(2) for 0 = K1 < K2 and ν = 0 if the Nyquist plot is located to the right of
the vertical line crossing the real axis at −1/K2,
(3) for K1 < 0 < K2 and ν = 0 if the Nyquist plot lies inside the circle
D(K1, K2), and
(4) for K1 < K2 ≤ 0 if, after replacing G(jω) with −G(jω), and K1 and K2
with −K1 and −K2, respectively, either Condition (1) or (2) is fulfilled.
Figure 2.42 illustrates the four cases of the above theorem in graphic form.The
subplots are shown as the areas colored in blue through which the Nyquist
plot must not pass if absolute stability is to be proven. If, on the other hand,
−
1
K1
−
1
K1
−
1
K2
−
1
K2
−
1
K2
1
K1
1
K2
Im Im
Im Im
Re Re
Re Re
Case (1): 0<K1<K2 Case (2): 0=K1<K2
Case (3): K1 <0<K2 Case (4) without K2 = 0: K1 <K2<0
Fig. 2.42: To achieve absolute stability in the sector [K1, K2] using Theorem 9, the
Nyquist plot of G(jω) must lie entirely in the white areas.2.2. Absolute Stability 107
the Nyquist plot runs through a blue area, absolute stability of the control
loop cannot be proven.
Note that in the case of Condition (1), it is absolutely necessary to use
the Nyquist plot of G(jω) with −∞ < ω < ∞. In all other cases and in
Theorem 8, it is irrelevant whether the Nyquist plot is drawn for all frequencies
−∞ < ω < ∞, or only for nonnegative frequencies 0 ≤ ω < ∞. This is because
of the mirror symmetry of the Nyquist plot with −∞ < ω < ∞ with respect
to the real axis.
In the following, we will examine a specific example [213] for Case (1) of
Theorem 9. The plant is chosen as
G(s) = 2
(s − 1)(s + 2). (2.12)
It has a stable pole at s1 = −2 and an unstable pole at s2 = 1. Therefore,
ν = 1 holds. Figure 2.43 shows the corresponding Nyquist plot. The Nyquist
plot orbits the circle D(K1, K2) once in a counterclockwise direction, so we
can deduce that the nonlinear standard control loop is absolutely stable in
the sector [K1, K2].
The above circle criterion enables us to address unstable systems whose
poles lie in the open right half-plane of the complex numbers. However, it is
limited to systems which do not have poles on the imaginary axis[4]. It can
admittedly be extended to systems with poles on the imaginary axis [274].
However, this can be laborious. This is unsatisfactory because many systems
in practical applications have such poles. Therefore, it would be useful if we
were also able to apply the circle criterion to such systems in a simple way.
Im
Re
D
−
1
K1
−
1
K2
−1
ω= 0
ω <0
ω >0
ω=−∞
ω=∞
Fig. 2.43: Nyquist plot of G(jω) of system (2.12) and application of Case (1) of the
circle criterion. Here are K1 = 1.083 and K2 = 2.003.
[4] An illustrative example of this restriction is the system G(s) = 1/s4
. If poles on
the imaginary axis were allowed in Theorem 9, it would seem as if there were a
sector [ε, ∞) of absolute stability. However, there is no Hurwitz sector for this
system and, therefore, no sector of absolute stability.108 Chapter 2. Limit Cycles and Stability Criteria
To do this, we need the continuous angle variation, which we define as follows
[130, 483]:
Definition 20 (Continuous Angle Variation). Let the Nyquist plot of a
frequency response G(jω) for ω ∈ [0, ∞] and a point p on the negative real
axis of the complex plane be given. For each frequency ω, there is a phasor
pointing from the point p to the point G(jω). The continuous variation of this
phasor’s angle ϕ(ω) which results when G(jω) starts at ω = 0 and passes
through all frequencies up to ω = ∞ is called the continuous angle variation
Ω with respect to the point p.
In some cases the Nyquist plot is piecewise continuous, i. e. the Nyquist plot
has jump discontinuities between its continuous segments. If this is the case,
the continuous angle variation Ω consists of the sum of all continuous angle
variations of the segments.
Two examples of a continuous angle variation Ω are shown in Figure 2.44
and 2.45. For the first example
G(s) = 1
(s + 1)(s + 2)(s + 3),
we can easily read the continuous angle variation Ω = 0 from the Nyquist
plot. The second example
G(s) = 4s
2 + 3s + 1
s
2(s − 1)
Ω = 0
ω = ∞
ω = 0
v
p
Im
Re
G(s)= 1
(s+1)(s+2)(s+3)
Fig. 2.44: Nyquist plot of G(jω) for
ω ∈ [0, ∞] and a continuous angle
variation Ω = 0 of the phasor v
Ω = 2π
ω = ∞
ω = 0
v
p
ϕ(ω)
Im
Re
G(s)= 4s
2 + 3s + 1
s
2(s − 1)
Fig. 2.45: Nyquist plot of G(jω) for
ω ∈ [0, ∞] and a continuous angle
variation Ω = 2π of the phasor v2.2. Absolute Stability 109
requires a bit of calculation since we do not know the phasor’s angle ϕ(ω) of
G(jω) for ω = 0 in advance. Using
ϕ(ω) = arctan
Im {G(jω)}
Re {G(jω)} − p

= arctan
4
ω
3 − ω
7ω2 − 1 + pω2(1 + ω2)

,
we obtain ϕ(0) = 0. With this result, we can read a continuous angle variation
of Ω = 2π from the Nyquist plot. In general, Ω is easy to determine.
Using the continuous angle variation, we obtain the following general circle
criterion, the proof of which is given in Appendix A.1.
Theorem 10 (General Circle Criterion). Let the nonlinear standard
control loop
Y (s) = G(s)U(s),
e = −y,
u = f(e, t), e ∈ IR, t ∈ IR,
be given whose open-loop transfer function G(s) is coprime and has a degree
m of the numerator which is less then the degree n of the denominator; let
G(s) have ν poles λi with Re {λi} > 0 and µ poles λi with Re {λi} = 0. Let
the function f(e, t) lie in the sector [K1, K2], let f(0, t) = 0 for all t ∈ IR, and
let D(K1, K2) represent the circle in the complex plane which passes through
the points
−
1
K1
and −
1
K2
and which has its center on the real axis. In this case, the control loop is
absolutely stable in the sector [K1, K2]
(1) for 0 < K1 < K2 if the Nyquist plot of G(jω) does not intersect or touch
the circle D(K1, K2) and if the continuous angle variation of G(jω) with
respect to the point −1/K1 is
Ω = νπ + µ
π
2
, (2.13)
(2) for 0 = K1 < K2 and ν = µ = 0 if the Nyquist plot is located to the right
of the vertical line crossing the real axis at −1/K2,
(3) for K1 < 0 < K2 and ν = µ = 0 if the Nyquist plot lies inside the circle
D(K1, K2), and
(4) for K1 < K2 ≤ 0 if, after replacing G(jω) with −G(jω), and K1 and K2
with −K1 and −K2, respectively, either Condition (1) or (2) is fulfilled.
As an example, we look again at system (2.12) and its Nyquist plot, which
is shown in Figure 2.43. The circle D(K1, K2) is not intersected or touched by
the Nyquist plot of G(jω), and the continuous angle variation Ω with respect110 Chapter 2. Limit Cycles and Stability Criteria
to the point −1/K1 is Ω = π. Since ν = 1 and µ = 0 hold for system (2.12),
condition (2.13) is fulfilled. Thus, the control loop is absolutely stable in the
sector [K1 = 1.083, K2 = 2.003].
If K1 = K2, the sector collapses to a line, the characteristic curve is
reduced to a linear function u = f(e) = K1e, and the circle becomes a point
located at −1/K1. In this case, the circle criterion is equivalent to the Nyquist
criterion.
2.2.6 The Tsypkin Criterion for Discrete-Time Systems
When considering sampled control loops with a sampling period T and a z￾domain transfer function G(z), the concept of absolute stability can be defined
in a way analogous to the continuous-time case. Again, let us examine nonlin￾ear static characteristic curves f. To determine the absolute stability of such
discrete-time nonlinear control loops, as shown in Figure 2.46, Ya. Z. Tsypkin
derived theorems comparable to the Popov criterion [260, 455, 456, 503, 502].
It will become clear that their application is similarly simple. However, like the
Popov criterion, the Tsypkin criteria are only sufficient. Therefore, we cannot
determine with certainty the largest sector of absolute stability by applying
these criteria. First we will look at the following simple criterion.
Theorem 11 (Basic Tsypkin Criterion). Let the nonlinear standard con￾trol loop
Y (z) = G(z)U(z),
e = −y,
u = f(e)
with the plant G(z) be given. Let the function u = f(e) be defined for all values
e ∈ IR, piecewise continuous, pass through zero, and fulfill f(e → ∞) 6= 0.
Then the control loop is absolutely stable
(1) in the sector [0, K] for asymptotically stable transfer functions G(z) and
(2) in the sector [ε, K] with an arbitrarily small ε > 0 for transfer functions
G(z) with only one pole λi with λi = 1 and otherwise only poles λi with
|λi
| < 1
if the inequality
Re {G(z = e
jωT )} > −
1
K
is fulfilled for all 0 ≤ ωT ≤ π.
The criterion is still valid if the nonlinearity is time-varying [260]. It can be
simply represented geometrically. We need the Nyquist plot for G(z = e
jωT ),
as shown in Figure 2.47, and then we just need to move a vertical line from
the left to the right through the complex plane toward the Nyquist plot until2.2. Absolute Stability 111
e u y
u = f(e) G(z) =
bmz
m + . . . + b1z + b0
z
n + . . . + a1z + a0
Fig. 2.46: Discrete-time nonlinear standard control loop
the line reaches the plot but does not yet touch it. The intersection −1/K
with the real axis yields the sector [0, K] or [ε, K], of absolute stability. The
advantage of this criterion is its simple applicability. Its disadvantage is that,
in some cases, it does not provide a good estimate of the sector of absolute
stability. A criterion providing a better estimation is
Theorem 12 (Extended Tsypkin Criterion). Let the nonlinear standard
control loop
Y (z) = G(z)U(z),
e = −y,
u = f(e)
with an asymptotically stable plant G(z) be given. Let the function u = f(e)
be defined for all values e ∈ IR, piecewise continuous, pass through zero, and
be monotonous. Then the control loop is absolutely stable in the sector [0, K]
if there exists a real number q ≥ 0 for which the Tsypkin inequality
Re {
￾
1 + q(1 − e
−jωT )

· G(z = e
jωT )} > −
1
K
is fulfilled for all 0 ≤ ωT ≤ π.
Nyquist plot of G(e
jωT )
Critical
Tsypkin
line
−
1
K
Im
Re
Fig. 2.47: Application of the basic Tsypkin criterio112 Chapter 2. Limit Cycles and Stability Criteria
Here, Y (z) and U(z) denote the z-transforms of the signals y(t) and u(t),
respectively.
The above criterion is the discrete-time equivalent to the Popov criterion
with the difference that the characteristic curve u = f(e) is required to be
monotonous. This, however, is not a strong restriction, since almost all char￾acteristic curves which exist in practice fulfill this additional requirement.
Further, q ≥ 0 must hold.
The extended Tsypkin criterion can be calculated in graphic terms in a
fashion similar to the Popov criterion. The Tsypkin inequality
Re {G(e
jωT ) + q(1 − e
−jωT ) · G(e
jωT )} > −
1
K
is rearranged to
Re {G(e
jωT )}
| {z }
U(ω)
−q
￾
Re {e
−jωTG(e
jωT )} − Re {G(e
jωT )}

| {z }
V (ω)
> −
1
K
.
As in the case of the Popov criterion, this results in the parameterized in￾equality
U(ω) − qV (ω) + 1
K
> 0.
Again, an artificial frequency response, or more precisely the corresponding
locus curve, the Tsypkin plot, is defined by
G˜(e
jωT ) = U(ω) + jV (ω).
If the Tsypkin plot lies to the right-hand side, or below a line with slope 1/q
which intersects the real axis at −1/K, the control loop is absolutely stable.
Figure 2.48 illustrates this. Again, a line is moved until it touches the Tsypkin
Critical
Tsypkin
line
Tsypkin
plot
G˜(e
jωT )
Slope 1
q
−
1
K
G(jω)
Im
Re
Fig. 2.48: Application of the extended Tsypkin criterio2.3. Lyapunov’s Stability Theory 113
plot, resulting in the largest possible sector of absolute stability that can be
determined using the Tsypkin criterion.
For discrete-time systems, it is also possible to apply the circle criteria,
more precisely Theorem 8 and Theorem 9 [465, 504]. In this case, the Nyquist
plot of G(jω) of the continuous-time system must be replaced with the Nyquist
plot of G(z = e
jωT ), where −π ≤ ωT ≤ π holds. The variable ν again repre￾sents the number of unstable poles of the plant, i. e. the number of poles λi
with |λi
| > 1.
2.3 Lyapunov’s Stability Theory
2.3.1 The Concept and the Direct Method
In the previous sections, we dealt with methods for analyzing the stability of
nonlinear standard control loops, i. e. a limited class of systems. These include
the describing function method, the circle criterion, and the Popov criterion.
The methods mentioned above are important to control theory, since they
deal with types of control loops that frequently occur in practice.
Unfortunately, these methods of stability analysis are not applicable to
arbitrary nonlinear systems. A generic method for analyzing the stability of
nonlinear systems was introduced in 1892 by A. M. Lyapunov[5] [279, 292,
293, 355]. This method has been extended in many ways [26, 162, 170, 389]. In
principle, we can analyze whether an equilibrium point of a dynamical system
is stable or not using the Lyapunov method. It will become apparent, however,
that this is often not possible in practice. Hence the Lyapunov method does
not entirely solve the problem of how to analyze the stability of nonlinear
systems.
To illustrate the key principle behind the Lyapunov method, we will look
at different cases of possible stability behavior using the example of a ball
with friction influenced by the acceleration of gravity g. Figure 2.49 shows the
mechanical arrangements used for this purpose; only the left one exhibits a
stable equilibrium point.
The potential energy
Epot = mgy
of the ball with mass m is proportional to its height y. Each of the arrange￾ments in question forces the ball to move along a specific track, so that the
y-coordinate is a function f of x and z. For the potential energy, we therefore
obtain
Epot = mgf(x, z).
Obviously an equilibrium point is only stable if the potential energy has a
minimum at the equilibrium point, i. e. in this example, the function f must
have a minimum.
[5] The name is also sometimes spelled Liapunov.114 Chapter 2. Limit Cycles and Stability Criteria
x
y
z
g
Fig. 2.49: Stability and instability of a ball influenced by gravity
Fig. 2.50: Ball with energy source and force driving an upswing
The following example demonstrates that this alone, however, does not
suffice to ensure stability. Now imagine that the ball has an energy source
which causes it to swing up and move upwards, as illustrated in Figure 2.50.
Thus the equilibrium point is not stable, even though the potential energy
has a minimum. This is due to the fact that the system has an internal source
of energy.
In addition to the requirement that the potential energy has a minimum,
a further condition must obviously be satisfied to ensure stability of the equi￾librium point. We therefore require the potential energy along all trajectories
in the neighborhood of the equilibrium point to decrease or at least stay con￾stant.
Looking at the problem more closely, it seems possible to move away from
our starting point of a potential energy function and to generalize the above
idea. To assess the stability of an equilibrium point, it seems to be sufficient to
use an arbitrary function for which the following two conditions are fulfilled:
(1) The function must have a minimum at the equilibrium point.
(2) The function must decrease within a neighborhood of the equilibrium
point along all of its trajectories.
This is the basic concept behind the direct Lyapunov method, which is also
referred to as his second method. A. M. Lyapunov proved the following theorem
[170, 279, 389], which is essential to the stability analysis of dynamic systems.2.3. Lyapunov’s Stability Theory 115
Theorem 13 (Lyapunov’s Direct Method). Let the differential equation
x˙ = f(x) with the equilibrium point xeq = 0 possess a continuous and unique
solution for every initial state vector within a neighborhood U1(0) of the origin.
If a function V (x) exists which possesses continuous partial derivatives and
fulfills the conditions
(1) V (0) = 0,
(2) V (x) > 0 for x 6= 0,
(3) V˙ (x) < 0 for x 6= 0 (or V˙ (x) ≤ 0)
within a neighborhood U2(0) ⊆ U1(0), the equilibrium point xeq = 0 is asymp￾totically stable (or Lyapunov stable).
A function V which fulfills Conditions (1) and (2) is called positive definite
and has a minimum at x = 0. Condition (3) means that V decreases or stays
constant over time along all trajectories x(t) starting from U2(0). It is worth
noting, once again, that the assumption of an equilibrium point at x = 0 is
not a loss of generality, since every equilibrium point can be transformed to
x = 0.
Theorem 13 allows us to verify whether an equilibrium point is Lyapunov
stable or asymptotically stable. This depends on whether V˙ (x) ≤ 0 or V˙ (x) <
0 holds. Figure 2.51 illustrates a case in which V˙ (x) ≤ 0. Where V˙ (x) = 0
holds, it is possible that trajectories x(t) which do not tend to the equilibrium
point xeq = 0 fulfill the inequality V˙ (x) ≤ 0, i. e. the equilibrium point
is only Lyapunov stable. Figure 2.52, on the other hand, illustrates a case
of asymptotic stability. In this case the function V (x) decreases along all
trajectories x(t) except the trivial trajectory x(t) = 0, i. e. it holds for the
time derivative that V˙ (x) < 0 and x 6= 0.
If Conditions (2) and (3) are fulfilled for the entire state space, and addi￾tionally
V (x) → ∞ whenever |x| → ∞ (2.14)
x1
x2
V (x) = c
Fig. 2.51: Lyapunov stability
x2
x1
V (x) = c
Fig. 2.52: An example of asymptotic
stability116 Chapter 2. Limit Cycles and Stability Criteria
holds, the equilibrium point is globally asymptotically stable (or globally
Lyapunov stable). A function with the property (2.14) is called radially un￾bounded. Functions V (x) which satisfy the conditions of the Lyapunov stabil￾ity theorem are called Lyapunov functions in the case in which V˙ (x) ≤ 0 and
strict Lyapunov functions in the case in which V˙ (x) < 0. In practical use, the
time derivative of the Lyapunov functions V (x) is determined by the gradient
and the time derivative of the state to be
V˙ (x) = x˙
T
grad(V (x)) = Xn
i=1
x˙ i
∂V
∂xi
. (2.15)
The derivative of the state vector
x˙ = f(x)
can now be inserted into equation (2.15). Then we must check if V˙ (x) ≤ 0 or
if V˙ (x) < 0 holds for x 6= 0. This is illustrated by Figure 2.53. The solution of
the differential equation, which for nonlinear systems in many cases cannot be
computed analytically, is not required for us to apply Theorem 13. The name
direct method stems from the direct application of the differential equation
for the calculation of V˙ (x).
Next we will address the case in which we know a Lyapunov function V
for a system x˙ = f(x) for which we can only show that V˙ (x) ≤ 0. Therefore,
using Theorem 13, we can only prove Lyapunov stability and not asymptotic
stability. However, if no trajectory x(t) exists which starts at some initial
value x(0) and along which the derivative V˙ (x(t)) is continuously identical
to zero, the function V decreases along all trajectories that start within the
neighborhood of the equilibrium point xeq = 0. Thus the asymptotic stability
of the equilibrium point can also be proven for this case [29]. This is more
precisely formulated in
Theorem 14 (Barbashin and Krasovskii Theorem). Let the differen￾tial equation x˙ = f(x) with the equilibrium point xeq = 0 possess a contin￾uous and unique solution for every initial state vector within a neighborhood
U1(0) of the origin. Let a function V (x) exist which possesses continuous
partial derivatives and fulfills the following conditions within a neighborhood
U2(0) ⊆ U1(0):
(1) V (0) = 0,
(2) V (x) > 0 for x 6= 0,
(3) V˙ (x) ≤ 0.
(4) The set of state vectors x for which V˙ (x) = 0 holds does not include a
trajectory x(t), except x(t) = 0.
Then the equilibrium point xeq = 0 is asymptotically stable.
If the Lyapunov function of the above theorem is also radially unbounded and
U2(0) = IRn
, the equilibrium point is globally asymptotically stable.2.3. Lyapunov’s Stability Theory 117
•
grad(V(x))
x˙
x1
x2
V (x)
V (x)
Fig. 2.53: Illustration of the equation V˙ (x) = x˙
T
grad(V (x)) < 0
Seen in terms of graphics, Condition (4) means that no trajectory x(t)
may exist which progresses in such a way that V (x(t)) is always constant
along its course. Then the trajectory would run along a contour line of V and
therefore would never reach the equilibrium point xeq = 0. The trajectory runs
precisely along a contour line if and only if its direction x˙(t) is orthogonal to
the gradient (∂V /∂x)
T
. In this case,
V˙ (x(t)) = 
∂V
∂x
T
x˙(t) = 0 (2.16)
holds. Condition (4), however, does not rule out the existence of individual
points x of the trajectory for which equation (2.16) is fulfilled. It requires that
equation (2.16) does not hold for the entire trajectory x(t).
Theorem 14 is often useful in practice when it is only possible to find a
Lyapunov function with V˙ (x) ≤ 0. Condition (4) can be verified by determin￾ing the set of vectors x, for which V˙ (x) = 0 holds. These vectors are inserted
into x˙ = f(x). If the set contains a solution x(t) of the differential equation,
which is different from the equilibrium point xeq = 0, Condition (4) is not
fulfilled. In most cases, Condition (4) is fulfilled, since it only rarely happens
that a trajectory x(t) continuously runs along a contour line of the Lyapunov
function V .
The problem with applying the above stability theorems mainly consists
of finding a Lyapunov function V (x). For linear systems and for some special
cases of nonlinear systems, as discussed below, it is easy to find a Lyapunov
function based on intuition aided by graphs. Generally, however, it turns out118 Chapter 2. Limit Cycles and Stability Criteria
to be very difficult to determine a Lyapunov function. Although there are a
series of design methods for Lyapunov functions [30, 151, 164, 339, 356, 405]
such as Aizerman’s method, Schultz and Gibson’s method, and Ingwerson’s or
Zubow’s method, they are only applicable to special cases and in many cases
difficult to implement. The method that is most successful in many cases is
the determination of the energy function of the system and its usage as a
possible Lyapunov function. Ultimately, however, in the majority of cases it
is necessary to try out different approaches for V (x).
Due to this difficulty in finding a solution, we might suspect that there is no
Lyapunov function for some systems with an asymptotically stable equilibrium
point xeq = 0. However, this is generally not the case. An existence theorem,
the so-called converse Lyapunov theorem [300, 301, 311, 312, 486], is given by
Theorem 15 (Converse Lyapunov Theorem). If the system x˙=f(x)
possesses an asymptotically stable equilibrium point at x = 0, and the function
f is locally Lipschitz continuous in a neighborhood of x = 0, then a continu￾ously differentiable function V (x) exists with V (0) = 0, with V (x) > 0, and
V˙ (x) < 0 for x 6= 0.
If the condition of the theorem above is fulfilled, the existence of a Lya￾punov function is assured. The problem is finding it, as mentioned previously.
In the case of an equilibrium point xeq = 0 which is not asymptotically stable,
but only Lyapunov stable, only the existence of a time-dependent Lyapunov
function V (x, t) with V (0, t) = 0 as well as V (x, t) > 0 and
V˙ (x, t) = ∂V
∂x
x˙ +
∂V
∂t ≤ 0
for x 6= 0 is guaranteed [448].
2.3.2 Illustrative Example
Let us return once again to the system (1.10) on p. 12, i. e.
x˙ 1 = x1(x2 − 1),
x˙ 2 = x2(x1 − 1),
with the equilibrium points xeq1 = 0 and xeq2 = [1 1]T
. Its trajectories
are shown in Figure 2.54. A candidate for a Lyapunov function to prove the
stability of the equilibrium point xeq1 = 0 is
V (x) = x
2
1 + x
2
2
, (2.17)
since it holds that V (0) = 0 and V (x) > 0 otherwise. The contour lines of
the function V are circles. As we remember, the equilibrium point xeq2 is
unstable.2.3. Lyapunov’s Stability Theory 119 State
x2
State x1
2
1
0
-1
-2
-2 -1 0 1 2
s
s
Fig. 2.54: Trajectories x(t) and circular contour lines (blue) of the Lyapunov function
V (x) = x
2
1 + x
2
2
Now it is necessary to determine whether the function (2.17) decreases
along all system trajectories x(t) in the neighborhood of the equilibrium point.
For this purpose, we calculate
V˙ (x) = x˙
T
grad(V (x)) = 
x˙ 1 x˙ 2


2x1
2x2

= 2x
2
1
(x2 − 1) + 2x
2
2
(x1 − 1).
It holds that
V˙ (x) < 0 for x1 < 1 and x2 < 1,
so that the equilibrium point xeq = 0 is asymptotically stable and the function
(2.17) is a strict Lyapunov function. Figure 2.54 illustrates these results, with
the contour lines V (x) forming circles as mentioned earlier.
2.3.3 Quadratic Lyapunov Functions
Based on graphs supporting intuition, functions with circular or ellipsoidal
contour lines seem to be suitable candidates for Lyapunov functions for dif￾ferent systems. Their general form is determined by the positive definite
quadratic form
V (x) = x
T R x.
Figure 2.55 shows the contour lines of such a function.
Condition (1) of Theorem 13,
V (0) = 0,
is obviously fulfilled; if R is a positive definite matrix, Condition (2), i. e.120 Chapter 2. Limit Cycles and Stability Criteria
V (x) = const
x1
x2
Fig. 2.55: Contour lines of a positive definite quadratic function
V (x) > 0 for x 6= 0,
is also fulfilled. It remains necessary to verify Condition (3), i. e.
V˙ (x) ≤ 0 or V˙ (x) < 0,
for the system in question.
We will investigate the extent to which the approach of choosing quadratic
forms x
T R x for a Lyapunov function is effective using linear systems
x˙ = Ax. (2.18)
With
V (x) = x
T R x,
it holds for V˙ (x) that
V˙ (x) = x
T R x˙ + x˙
T R x.
Inserting the differential equation (2.18) yields
V˙ (x) = x
T RA x + x
T A
T R x
= x
T

RA + AT R

| {z }
−Q
x.
For asymptotic stability, we have to fulfill the inequality
V˙ (x) = −x
T Q x < 0.
Thus the matrix Q is required to be positive definite.
If the matrix equation2.3. Lyapunov’s Stability Theory 121
AT R + RA = −Q (2.19)
provides a positive definite matrix Q, V (x) = x
T R x is a strict Lyapunov
function and the system x˙ =Ax is asymptotically stable. It is also possible to
proceed inversely by specifying an arbitrary positive definite matrix Q and – if
system (2.18) is asymptotically stable – determining a positive definite matrix
R and thus a Lyapunov function. Equation (2.19) is called the Lyapunov
equation. The following theorem [258] holds.
Theorem 16 (Lyapunov Equation). The equilibrium point xeq = 0 of
the linear system x˙ = Ax is Lyapunov stable (asymptotically stable) if and
only if, for an arbitrary real-valued symmetric, positive semidefinite (positive
definite) matrix Q, a positive definite matrix R exists such that
A
T R + RA = −Q
holds. Then the function V = x
T R x is a (strict) Lyapunov function for the
system.
For stable linear systems, it is therefore always possible to find quadratic
Lyapunov functions. At least in this case, the approach of using quadratic
Lyapunov functions proves to be very suitable.
Theorem 16 is actually not significant in the stability analysis of linear
systems, which can be more easily determined using the eigenvalues of the
system. Its importance instead lies in the design theory of many nonlinear
control systems and also in the stability analysis of linearized nonlinear sys￾tems. We will address these topics later on.
2.3.4 Example: Mutualism
Let us consider a dynamic ecological system. In ecological systems there
are many and often very different dependencies between species. The best￾known are the predator-prey relationships, which are often modeled by Lotka￾Volterra equations. Interrelations between two species where both species
profit from the relationship can also be described by differential equations
that are closely related to the Lotka-Volterra equations. The type of cohabi￾tation that is beneficial for both species is called mutualism.
An example of this is the mutualism between the ocellaris clownfish (Am￾phiprion ocellaris) and the magnificent sea anemone (Heteractis magnifica),
which is depicted in Figure 2.56. On the one hand, the anemone protects
the clownfish against predators with its poisonous tentacles; on the other
hand, the fish also protects the anemone against predators such as filefish
(Monacanthidae). Another example is the mutualism between humans and
wheat. Mutualism of this kind is modeled by the equations
x˙ 1 = ax1 − cx2
1 + ex1x2,
x˙ 2 = bx2 − dx2
2 + fx1x2,
(2.20)122 Chapter 2. Limit Cycles and Stability Criteria
Fig. 2.56: Mutualism between ocellaris clownfish and anemone
in which x1 denotes the number of individuals of the first species and x2
that of the other species. The values a, b, c, d, e, and f are constant positive
parameters.
In equation (2.20), the terms x˙ 1 = ax1 and x˙ 2 = bx2 describe linear growth
laws, for which the population growth increases linearly depending on their
sizes. The term −cx2
1
, on the other hand, inhibits the growth of population
x1 for an increasing population size, because of rivalry for food within one
species, for example. This is also called intraspecific competition. The term
−dx2
2 has the same effect. The components ex1x2 and fx1x2 lead to a mutual
promotion of growth in both populations. These two components thus describe
the mutualism within the system.
The model (2.20) possesses the equilibrium points
xeq1 =

0
0

, xeq2 =

a/c
0

, xeq3 =

0
b/d
.
If the inequality ef < cd is additionally fulfilled, a fourth equilibrium point
occurs at
xeq4 =




be + ad
cd − ef
bc + af
cd − ef




,
which is created by mutualism. In the case where ef > cd, no equilibrium point
with positive coordinates exists, since the mutualism is much stronger than2.3. Lyapunov’s Stability Theory 123
the intraspecific competition and the populations x1 and x2 grow infinitely
large.
We will look at the following specific case
x˙ 1 = x1 − 10−3
x
2
1 + 0.5 · 10−3
x1x2,
x˙ 2 = x2 − 10−3
x
2
2 + 0.5 · 10−3
x1x2
(2.21)
with the equilibrium points
xeq1 =

0
0

, xeq2 =

1000
0

, xeq3 =

0
1000
, xeq4 =

2000
2000
.
Figure 2.57 depicts the courses of the trajectories in this ecological system. We
are especially interested in the equilibrium point xeq4 caused by mutualism.
To prove its stability, we will first transform xeq4 via
x = z + xeq4
into the origin, which yields
z˙1 = (z1 + 2000) − 10−3
(z1 + 2000)2 + 0.5 · 10−3
(z1 + 2000)(z2 + 2000),
z˙2 = (z2 + 2000) − 10−3
(z2 + 2000)2 + 0.5 · 10−3
(z1 + 2000)(z2 + 2000)
for the system (2.21) after transformation. The courses of the trajectories in
the transformed system are shown in Figure 2.58.
Now we choose
V (z) = z
2
1 + z
2
2
as potential Lyapunov function, and we arrive at
State x1
State
x2
0 1000 2000 3000
1000
2000
3000
0
s
s
s
s
Fig. 2.57: Trajectories of a mutualistic system. For a negative number of individuals
x1 and x2 (blue area), no trajectories occur in reality.124 Chapter 2. Limit Cycles and Stability Criteria
V(z)=r
2
State z1
State z2
-2000 -1000 0 1000 2000
-1000
1000
2000
-2000
0
s
s
s
s
Fig. 2.58: A circular catchment region with radius r = 1924 (blue) of the mutualistic
equilibrium point, which has been transformed to z = 0
V˙ (z) = 2z1z˙1 + 2z2z˙2
= − 4(z
2
1 + z
2
2
) − 2 · 10−3
(z
3
1 + z
3
2
) + 10−3
z1z2(z1 + z2) + 4z1z2.
(2.22)
In order to determine when V˙ (z) < 0 holds, we will define the polar
coordinates
z1 = r cos(ϕ),
z2 = r sin(ϕ).
Here r denotes the radius, ϕ represents the angle of the polar coordinates,
and r
2
further represents the level of a contour line of the Lyapunov function
V (z) = z
2
1 + z
2
2 = r
2
.
Thus it follows from equation (2.22) that
V˙ =−r
2
16000+5r cos(ϕ)+3r cos(3ϕ)+5r sin(ϕ)−8000 sin(2ϕ)−3r sin(3ϕ)
4000
,
which is obviously negative if
16000 − 8000 sin(2ϕ) + r [5 cos(ϕ) + 3 cos(3ϕ) + 5 sin(ϕ) − 3 sin(3ϕ)] > 0
(2.23)
holds. Equation (2.23) is equivalent to the condition
1
r
>
3 sin(3ϕ) − 3 cos(3ϕ) − 5 cos(ϕ) − 5 sin(ϕ)
8000(2 − sin(2ϕ)) ,2.3. Lyapunov’s Stability Theory 125
which is certainly fulfilled if
1
r
> max 
3 sin(3ϕ) − 3 cos(3ϕ) − 5 cos(ϕ) − 5 sin(ϕ)
8000(2 − sin(2ϕ)) 
≈
1
1924
holds.
For all values r < 1924, i. e. the ones within a circle of radius r = 1924, it
holds that V˙ (z) < 0. The equilibrium point xeq4 is therefore asymptotically
stable and the circle with r = 1924 around its center at xeq4 is a catchment
region, sometimes called a Lyapunov region, since the region is outlined by
a contour line from a Lyapunov function. However, as shown in Figure 2.58,
this Lyapunov region does not provide the maximum catchment region. The
latter would consist of the part of the state plane in Figure 2.57 where x1
and x2 are positive. Here the coordinate axes do not belong to the maximum
catchment region, since the trajectories which start here do not tend to the
non-mutualistic equilibrium points.
2.3.5 The Direct Method for Discrete-Time Systems
Similar to continuous-time systems, the direct Lyapunov method can also be
applied to discrete-time systems
xk+1 = f(xk).
The first two conditions of the Stability Theorem 13, V (0) = 0 and V (x) > 0
for all x 6= 0, hold unchanged. Only the third condition,
V˙ (x) < 0 for all x 6= 0,
which we are only calculating for the asymptotically stable case at this point,
has to be replaced by the condition
∆Vk = V (xk+1) − V (xk) < 0 for all xk 6= 0.
In the case of linear systems
xk+1 = Φxk
and with quadratic Lyapunov functions
V (xk) = x
T
k R xk,
we arrive at
∆Vk = x
T
k+1R xk+1 − x
T
k R xk
= x
T
k Φ
T R Φ xk − x
T
k R xk
= x
T
k
(Φ
T R Φ − R)xk < 0.126 Chapter 2. Limit Cycles and Stability Criteria
The above inequality is obviously fulfilled if the matrix Q in the equation
Φ
T R Φ − R = −Q (2.24)
is positive definite. Equation (2.24) is called the discrete Lyapunov equation. In
contrast to the Lyapunov equation for the continuous-time case, the Lyapunov
equation (2.24) depends on a quadratic function of the system matrix Φ.
2.3.6 The Indirect Method
The above results for continuous-time linear systems are the basis for the
stability analysis of nonlinear systems of the form
x˙ = Ax + g(x) (2.25)
with the associated equilibrium point xeq = 0. Here g(x) should tend faster
to x = 0 than |x| for |x| → 0. This is the case when
lim
|x|→0
g(x)
|x|
= 0 (2.26)
holds. For these systems, Lyapunov’s stability theory allows us to prove the fol￾lowing theorem [320], which is also referred to as Lyapunov’s indirect method,
Lyapunov’s first method, or Lyapunov’s linearization method.
Theorem 17 (Lyapunov’s Indirect Method of Stability). Let the sys￾tem
x˙ = Ax + g(x),
where g(x) is a continuous function, possess an equilibrium point at x = 0
and a continuous and unique solution for every initial state vector within a
neighborhood of x = 0. Let it further hold that
lim
|x|→0
g(x)
|x|
= 0.
If, in this case, the matrix A has only eigenvalues with a negative real part, the
equilibrium point xeq = 0 is asymptotically stable and a Lyapunov function
V (x) = x
T R x
will always exist for x˙ = Ax + g(x), whose matrix R results from
A
T R + RA = −Q
with an arbitrary positive definite matrix Q. If A has one or more eigenvalues
with a positive real part, the equilibrium point is unstable. If A does not have
any eigenvalues λi with a positive real part, but at least one with Re {λi} = 0,
the equilibrium point is stable or unstable depending on the form of g.2.3. Lyapunov’s Stability Theory 127
The class of nonlinear systems (2.25) is particularly interesting, because
most of the nonlinear systems
x˙ = f(x)
with an equilibrium point xeq = 0 can be represented by a Taylor polynomial
x˙ = f(x) = f(0)
| {z }
0
+
∂f
∂x




x=0
| {z }
A
x + g(x)
with residual g(x) and can be defined in terms of equation (2.25) in this way.
In these cases, equation (2.26) always holds for the residual g(x) of a function
f which is representable by a Taylor series. Thus using the above theorem,
we can deduce the stability behavior of the equilibrium point xeq = 0 of a
differential equation x˙ = f(x) by its corresponding linearization
x˙ = Ax with A =
∂f
∂x




x=0
around the equilibrium point xeq = 0. For this reason, the above theorem is
of exceptional importance, since it states that the stability behavior of a large
class of nonlinear systems can be easily determined based on the corresponding
linearized systems. However, it does not guarantee a large catchment region.
2.3.7 Determining Exponential Stability
To prove that a system
x˙ = f(x)
has an exponentially stable equilibrium point xeq = 0, positive constants m
and α must exist, so that the inequality
|x(t)| ≤ me−αt|x(0)| (2.27)
is fulfilled. We know this from Definition 10, p. 19. The solution x(t) of the
differential equation x˙ = f(x) tends to the equilibrium point xeq = 0 faster
than or equally as fast as the exponential function me−αt|x(0)|. For simplic￾ity’s sake, we will write x for x(t) and x0 for the initial value x(0) in the
following.
For nonlinear differential equations x˙ = f(x) it is generally very difficult,
if not impossible, to prove exponential stability using the defining equation
(2.27), since in most cases we cannot determine the solution x(t) of the differ￾ential equation x˙ = f(x) analytically. Also, in this case, Lyapunov functions
often allow us to prove stability. However, the conditions on the Lyapunov
function are stricter in the case of exponential stability than in the case of
asymptotic stability. The requirements that128 Chapter 2. Limit Cycles and Stability Criteria
V (0) = 0
and
V (x) > 0, x 6= 0,
from the Stability Theorem 13 on p. 115, are replaced by the stricter require￾ments
k1|x|
p ≤ V (x) ≤ k2|x|
p
, (2.28)
where k1, k2 and p are positive constants which remain to be determined.
Below we will assume that V is continuously differentiable. Now we will replace
the condition
V˙ (x) < 0, x 6= 0,
of Theorem 13 with the stricter condition
V˙ (x) ≤ −k3|x|
p
, (2.29)
where k3 is once again a positive constant which remains to be determined.
From inequality (2.29) and using inequality (2.28), it follows that
V˙ (x) ≤ −
k3
k2
V (x) = −aV (x), a =
k3
k2
. (2.30)
At this point, we will utilize the differentiable form of Grönwall’s Lemma[6]
.
It allows us to deduce the inequality
V (x) ≤ V (x0)e
−at (2.31)
from the differential inequality (2.30).
We will again apply inequality (2.28), which along with inequality (2.31)
yields the relationships
|x|
p ≤
V (x)
k1
≤
V (x0)
k1
e
−at ≤
k2
k1
|x0|
p
e
−at
[6] Grönwall’s Lemma states that if a nonnegative, continuously differentiable func￾tion η(t) fulfills the differential inequality
η˙(t) ≤ cη(t) + Ψ(t), t ∈ [0, T ∈ IR],
with a constant c ∈ IR and a nonnegative integrable function Ψ, the Grönwall
inequality
η(t) ≤

η(0) + Zt
0
Ψ(s)e
−csds

 e
ct
holds for all t ∈ [0, T ] [357]. Note that in the case discussed here, x and therefore
V are functions of time t, and it holds that Ψ(t) = 0.2.3. Lyapunov’s Stability Theory 129
and thus
|x| ≤ p
r
k2
k1
|x0|e
− a
p
t
= m|x0|e
−αt, m =
p
r
k2
k1
, α =
a
p
.
The defining expression (2.27) for exponential stability is therefore fulfilled by
the above inequality if condition (2.28) and condition (2.29) are fulfilled. This
leads to the following theorem.
Theorem 18 (Exponential Stability). Let the system x˙ = f(x) with the
equilibrium point xeq = 0 possess a continuous and unique solution for every
initial state vector within a neighborhood U1(0) of the origin. If a function
V (x) exists which is continuous in a neighborhood U2(0) ⊆ U1(0), possesses
continuous partial derivatives, and positive constants k1, k2, k3, and p exist
such that the inequalities
(1) k1|x|
p ≤ V (x) ≤ k2|x|
p
,
(2) V˙ (x) ≤ −k3|x|
p
are fulfilled at U2(0), then the equilibrium point xeq = 0 is exponentially
stable.
If the Conditions (1) and (2) of the theorem are satisfied in the entire state
space, the equilibrium point is globally exponentially stable.
2.3.8 Example: Underwater Glider
Underwater gliders are a type of autonomous underwater robot used for long￾term missions, such as to measure environmental data or salinity in oceans
and seas [178, 183, 263, 294]. Figure 2.59 illustrates a possible structure for
such a glider. They consume very little energy and can operate autonomously
for years. These gliders are about 2 m long or longer and have wings on the
sides of their hull, similar to sailplanes. Their principle of movement is also
similar to that of sailplanes. However, the cause of the glider’s movement is
different. It is based on a density change within the glider. To this end, for
example, a hydraulic liquid is pumped back and forth between an internal
swim bladder, i. e. one located inside the hull of the glider, and an external
one located outside the impermeable part of the hull at the stern.
For example, if the underwater glider is at the surface and the hydraulic
liquid is pumped from the external swim bladder to the internal one, the
volume of the glider is reduced due to the contraction of the external bladder,
and the specific density of the underwater vehicle becomes greater than that of
the surrounding water. Simultaneously, the center of gravity of the underwater
vehicle moves toward the bow and the glider begins to sink, bow first. The
center of gravity can be fine-tuned by a motor which changes the position
of the accumulator providing the energy supply for the glider. Due to the
wings, which have a similar effect as the wings of an aircraft, the glider does130 Chapter 2. Limit Cycles and Stability Criteria
v
γ
Internal
swim bladder
External
swim bladder
Movable
accumulator
Fig. 2.59: Structure of an underwater glider
not sink vertically; it glides on a slanting course into the depths, as shown
in Figure 2.60. After the glider reaches a prespecified depth, the hydraulic
liquid is pumped back into the external bladder at the stern. Consequently,
the volume of the glider increases while its weight remains constant and the
glider begins to rise. This takes place along a slanting upward path with a
gliding angle of γ. When the glider has reached the surface, radio frequency
signals can be transmitted and then the glider descends again. The resulting
motion as a function of time is a sawtooth-like course of movement. A typical
velocity for such a glider is approximately 1 km h−1
.
Below we will present a simplified model of the underwater glider [45, 46].
It describes the relative deviation
x1 =
v − vs
vs
of the glider’s velocity v from the stationary velocity vs and the deviation
x2 = γ − γs
of the gliding angle γ from the stationary gliding angle γs via
x˙ 1 = −
1
a
h
a (1 + x1)
2 + mg sin(x2 + γs)
i
, a = −mg sin(γs),
x˙ 2 =
1
a(1 + x1)
h
b (1 + x1)
2 − mg cos(x2 + γs)
i
, b = mg cos(γs),
(2.32)
and has an equilibrium point at xeq = 0. Here m is the mass of the glider and
g is the gravitational acceleration. For the deviation, it holds that
x1 ∈ (x1,min, x1,max) with x1,min > −1 and x2 ∈ [−π, π).2.3. Lyapunov’s Stability Theory 131
For the above system, the Lyapunov function
V (x) = 2
3
− (1 + x1) cos(x2) + 1
3
(1 + x1)
3
(2.33)
was found in [45]. See also [46]. Using this Lyapunov function, the exponential
stability of the equilibrium point xeq = 0 can be proven. For this purpose, we
will first consider Condition (1) of Theorem 18, namely
k1|x|
p ≤ V (x) ≤ k2|x|
p
,
where we choose p = 2. When we apply the Taylor series of the cosine function,
cos(α) = 1 −
α
2
2! +
α
4
4! −
α
6
6! +
α
8
8! − . . . ,
equation (2.33) becomes
Fig. 2.60: Sawtooth-like course of movement of an underwater glider132 Chapter 2. Limit Cycles and Stability Criteria
V (x) = 
1 +
x1
3

x
2
1 − (1 + x1)

−
x
2
2
2! +
x
4
2
4! −
x
6
2
6! +
x
8
2
8! − . . .
(2.34)
=

1 +
x1
3

x
2
1 +
1 + x1
2
x
2
2
1 −
x
2
2
3 · 4

+
2x
4
2
6! 
1 −
x
2
2
7 · 8

+ . . .
.
Within −π ≤ x2 < π, it holds for the elements of the above series that
1 −
x
2
2
3 · 4
> 0, 1 −
x
2
2
7 · 8
> 0, · · · ,
such that
V (x) ≥

1 +
x1
3

x
2
1 +
1 + x1
2

1 −
x
2
2
3 · 4

x
2
2 ≥ k1(x
2
1 + x
2
2
)
holds, where the positive constant k1 is given by
k1 = min 
1 +
x1,min
3
,
1 + x1,min
2

1 −
π
2
12 , x1,min > −1.
Furthermore, we obtain
V (x)=
1+
x1
3

x
2
1+
1+x1
2
x
2
2−
1+x1
4! x
4
2
1−
x
2
2
5 · 6

+
4!x
4
2
8! 
1−
x
2
2
9 · 10
+ . . .
by utilizing equation (2.34). Similar to the case above, it therefore follows for
−π ≤ x2 ≤ π that
V (x) ≤

1 +
x1
3

x
2
1 +
1 + x1
2
x
2
2 ≤ k2
￾
x
2
1 + x
2
2

with
k2 = max 
1 +
x1,max
3
,
1 + x1,max
2

.
The first condition of Theorem 18 is therefore fulfilled.
Next, the second condition of this theorem, namely
V˙ (x) ≤ −k3|x|
2
, (2.35)
is verified with p = 2. It holds that
V˙ (x) =∂V (x)
∂x
x˙
=
"
(1 + x1)
2 − cos(x2)
(1 + x1) sin(x2)
#T




−
1
a

a(1 + x1)
2 + mg sin(x2 + γs)

1
a(1 + x1)

b(1 + x1)
2 − mg cos(x2 + γs)





=
1
a

−a(1 + x1)
4 + a(1 + x1)
2
cos(x2) − mg(1 + x1)
2
sin(x2 + γs)
+ b(1+x1)
2
sin(x2)+ mg(cos(x2) sin(x2 + γs) − cos(x2 + γs) sin(x2))
| {z }
sin(γs)

2.3. Lyapunov’s Stability Theory 133
and with a = −mg sin(γs) and b = mg cos(γs) from equation (2.32), it follows
that
V˙ (x) = 1
a

− a(1 + x1)
4 + a(1 + x1)
2
cos(x2)
+ mg(1 + x1)
2
(cos(γs) sin(x2) − sin(x2 + γs))
| {z }
− sin(γs) cos(x2)
−a

=
1
a

−a(1 + x1)
4 + 2a(1 + x1)
2
cos(x2) − a

= −1 + 2(1 + x1)
2
cos(x2) − (1 + x1)
4
. (2.36)
In the following, we make use of
sin2
α
2

=
1
2
(1 − cos(α)) and cos(α) = 1 − 2 sin2
α
2

in equation (2.36) to obtain
V˙ (x) = − 1 + 2(1 + x1)
2 − (1 + x1)
4 − 4(1 + x1)
2
sin2
x2
2

= − [x1(x1 + 2)]2 − 4(1 + x1)
2
sin2
x2
2

. (2.37)
Obviously V˙ (x) < 0 holds for −π ≤ x2 < π; thus the equilibrium point is
asymptotically stable according to Lyapunov’s stability theorem on p. 115.
To prove exponential stability, however, the stricter condition (2.35) must be
met.
Taking into account that
sin2
x2
2

= sin2

|x2|
2

holds, we will first evaluate the Taylor series for −π ≤ x2 < π
sin 
|x2|
2

=
|x2|
2
−
1
3!
|x2|
3
2
3
+
1
5!
|x2|
5
2
5

1 −
|x2|
2
2
2 · 6 · 7

+ . . .
≥
|x2|
2
−
1
3!
|x2|
3
2
3
=
|x2|
2

1 −
|x2|
2
24 
≥
|x2|
2

1 −
π
2
24
and deduce that
sin2
x2
2

≥
1
4

1 −
π
2
242
x
2
2
.
Thus V˙ (x) from equation (2.37) can be estimated by134 Chapter 2. Limit Cycles and Stability Criteria
V˙ (x) ≤ −(2 + x1)
2
x
2
1 −

1 −
π
2
242
(1 + x1)
2
x
2
2
≤ − 
1 −
π
2
242
(1 + x1,min)
2
| {z }
k3
|x|
2
,
and the second condition for exponential stability, the inequality (2.35), is
also fulfilled. Therefore the glider always tends exponentially from an initial
deflection or a disturbance to the equilibrium point xeq = 0, i. e. it dives on
a path at a stationary velocity vs and stationary gliding angle γs.
2.3.9 Catchment Regions
So far the stability of an equilibrium point has been analyzed. In practice,
whether an equilibrium point is stable is not the only important question. We
are also interested in the largest region containing the equilibrium point for
which all trajectories beginning in this region tend to this point. This region,
the maximal catchment region, is also called the region of asymptotic stability
or basin of an equilibrium point. Figure 2.61 depicts this region for the system
(1.10) that was discussed in Section 1.1.6, p. 11 et seq., and Section 2.3.2,
p. 118. The curves, or for higher dimensions the hypersurfaces, which separate
the stable courses of trajectories from unstable ones are termed separatrix . In
Figure 2.61 the separatrix is defined by the two trajectories which tend to the
unstable equilibrium point
xeq2 =

1
1

.
State
x2
State x1
2
1
0
-1
-2
-2 -1 0 1 2
s
s
Fig. 2.61: Maximal catchment region (blue) of the exemplary system (1.10) on p. 122.3. Lyapunov’s Stability Theory 135
If the region of asymptotic stability of a locally asymptotically stable equi￾librium point is very small, the equilibrium point cannot be called asymp￾totically stable in practice. The proof of stability of an equilibrium point is
therefore not sufficient in practice. We must also consider the vicinity of the
equilibrium point, and the maximal catchment region must be large enough
to include the starting points of all trajectories that are of interest.
In general, the region of asymptotic stability of an equilibrium point can￾not be determined analytically. However, subsets of the region of asymptotic
stability can be determined if we have found a Lyapunov function. Such a
subset is a catchment region if all trajectories which start in this region never
leave it and tend to the equilibrium point xeq = 0. These catchment re￾gions are bounded by the contour lines of a Lyapunov function, for example.
As mentioned earlier, they are often referred to as Lyapunov sets, Lyapunov
regions, or contractive positively invariant sets. More specifically, we obtain
[462]
Theorem 19 (Catchment Region). If V (x) is a Lyapunov function for
the system x˙ = f(x) with the asymptotically stable equilibrium point xeq = 0,
the region
G = {x ∈ IRn
| V (x) < c}, c ∈ IR+,
is bounded, and
V˙ (x) < 0
holds everywhere in G \ {0}, then G is a catchment region of the equilibrium
point xeq = 0.
With no loss of generality, the open set G in Theorem 19 can be replaced by
a closed set
G = {x ∈ IRn
| V (x) ≤ c}.
Theorem 19 maintains its validity in this case.
As can be seen in Figure 2.53, the condition V˙ (x) < 0 ensures that V
decreases along all trajectories. Thus none of the trajectories can leave the
region G. Since V˙ (x) < 0 holds everywhere in this region and G is bounded,
they also tend to zero.
The condition that the region G must be bounded is essential. If it is not
fulfilled, G is not necessarily a catchment region, as shown in the following
example. We will examine the function
V (x) = x
2
1
1 + x
2
1
+ 2x
2
2
. (2.38)
This function is not radially unbounded, i. e. it does not hold that V (x) → ∞
for |x| → ∞, and thus there are also regions
G =

x ∈ IR2
| V (x) < c	136 Chapter 2. Limit Cycles and Stability Criteria
which are unbounded, i. e. the contour lines are not closed in every case; rather,
they tend to infinity for values c ≥ 1. The graph of the contour lines of this
function is shown in Figure 2.62.
There are systems with an equilibrium point xeq = 0 for which the function
(2.38) is a Lyapunov function V and V˙ (x) < 0 holds everywhere in IRn
\{0}.
Nevertheless, trajectories may tend to infinity along the contour lines of V (x)
with c > 1, even though along the contour lines it holds that V˙ (x) < 0. Figure
2.62 shows such a trajectory. Obviously, in this case, G is not a catchment
region of the equilibrium point xeq = 0.
Such a system is [44]
x˙ 1 = −x1 + 2x
3
1x
2
2
,
x˙ 2 = −x2.
(2.39)
Its trajectories are shown in Figure 2.63. For this system, which has a single
equilibrium point at xeq = 0, and the function (2.38), the inequality
V˙ (x) = −
2x
2
1 + 4x
2
2
￾
1 + 2x
2
1

(1 + x
2
1
)
2 < 0
holds for all x ∈ IR2
\{0}. Thus, the function (2.38) is a Lyapunov function
for system (2.39), and the equilibrium point xeq = 0 is asymptotically stable.
However, as we will soon see, it is not globally asymptotically stable. Note
that the latter cannot be verified by means of the Lyapunov function (2.38),
because it is not radially unbounded. Radial unboundedness, however, is a
necessary condition of a Lyapunov function V to prove global stability, as
required in equation (2.14) on p. 115.
The solutions of the system (2.39) are given by
x1(t) = x10e
t
p
x
2
10x
2
20 + (1 − x
2
10x
2
20) e
4t
,
x2(t) = x20e
−t
,
(2.40)
where x10 = x1(0) and x20 = x2(0). From (2.40), we can directly deduce that
the trajectory x(t) tend to the equilibrium point xeq = 0 for x
2
10x
2
20 < 1
and to infinity for x
2
10x
2
20 ≥ 1. The initial values which fulfill the equation
x
2
10x
2
20 = 1, i. e.
x20 = ±
1
x10
,
make up the separatrix, which represents the boundary between stable and
unstable trajectories. The trajectories that lie on the separatrix itself are
unstable and are given by
x1(t) = x10e
t
,
x2(t) = x20e
−t
2.3. Lyapunov’s Stability Theory 137
State x1
State
x2
-20 -10 0 10 20
-1.5
-1
-0.5
0
1
0.5
1.5
Fig. 2.62: The contour lines of the Lya￾punov function (2.38). The region with
closed contour lines is gray, an unstable
trajectory of the system (2.39) is shown
in red.
State x1
State
x2
-20 -10 0 10 20
-1.5
-1
-0.5
0
1
0.5
1.5
s
Fig. 2.63: Trajectories x(t) of the sys￾tem (2.39). The region of asymptotic
stability Gmax is shown in blue. All tra￾jectories outside of Gmax are unstable.
In the case where x
2
10x
2
20 > 1, all trajectories tend to infinity in finite time.
The denominator of equation (2.40), in this case, for the finite escape time
te =
1
4
ln 
x
2
10x
2
20
x
2
10x
2
20 − 1

becomes identical to zero[7] and therefore x1 becomes infinitely large.
The maximal catchment region, the region of asymptotic stability, is thus
given by
GRAS =

x ∈ IR2
| x
2
1x
2
2 < 1
	
.
Figure 2.63 not only illustrates the course of the trajectories x(t) but also the
region GRAS, whose boundary is the separatrix of the system. Figure 2.62 also
shows an unstable trajectory, which has its starting point at x10 = x20 = 1.5.
This illustrates the previously mentioned case in which V (x) decreases along
a trajectory, which nevertheless tends to infinity.
2.3.10 LaSalle’s Invariance Principle
In addition to the direct Lyapunov method, there is a more general and more
powerful method for analyzing the convergence of the solutions of a differential
[7] The solution (2.40) of the system (2.39) is only defined for the time interval
[−∞, te). For times t ≥ te no solution exists. This is because the right side of the
differential equation (2.39) is locally but not globally Lipschitz continuous. See
Theorems 4 and 5 on p. 40.138 Chapter 2. Limit Cycles and Stability Criteria
equation x˙ = f(x), which is LaSalle’s invariance principle [261]. In a certain
sense, it is a generalization of the direct method. With LaSalle’s invariance
principle, we are not restricted to the stability analysis of an equilibrium point;
it also allows for a simultaneous consideration of multiple equilibrium points
or the stability analysis of limit cycles.
The invariance principle is based on so-called invariant sets, which, similar
to catchment regions, are never left by trajectories which run within them.
The following two definitions describe this more precisely.
Definition 21 (Invariant Set). A set G is called invariant with respect to
a system x˙ = f(x) if x(t) ∈ G holds for all x(0) ∈ G and all −∞ < t < ∞.
Definition 22 (Positively Invariant Set). A set G is called positively
invariant with respect to a system x˙ = f(x) if x(t) ∈ G holds for all x(0) ∈ G
and t ≥ 0.
As specified in Definition 21, the trajectories whose courses lie within an
invariant set are inside this set for the past, t < 0, the present, t = 0, and the
future, t > 0. Trajectories thus never enter an invariant set; they start inside
it. In contrast to invariant sets, trajectories may enter positively invariant
sets for t ≤ 0, which they will no longer leave. Every invariant set is also a
positively invariant set. An invariant or positively invariant set for which all
trajectories inside it tend to an equilibrium point is a catchment region of the
equilibrium point, synonymously called a contractive positively invariant set,
as mentioned before. We are using the term catchment region here, since it is
shorter and more intuitive.
For example, if V is a Lyapunov function for an equilibrium point xeq = 0
of a system x˙ =f(x), it follows that
G = {x ∈ IRn
| V (x) < c}
is a positively invariant set.
Building upon the concept of invariant and positively invariant sets, we
can state [261]
Theorem 20 (LaSalle’s Invariance Principle). Let x˙ = f(x) be a sys￾tem with a compact positively invariant set Ω, and let V LaSalle(x) be a con￾tinuously differentiable function with
V˙
LaSalle(x) ≤ 0
for all x ∈ Ω. Further, let N denote the set of all points x ∈ Ω with
V˙
LaSalle(x) = 0
and let M denote the largest invariant set in N. In this case all solutions x(t)
that start within Ω tend to the set M for t → ∞.2.3. Lyapunov’s Stability Theory 139
•
V LaSalle
N
M
x(t)
Ω
x1
x2
Fig. 2.64: The function V LaSalle, the positively invariant set Ω, the set N, and the
invariant set M, to which the trajectory x(t) tends
Figure 2.64 illustrates the sets Ω, N, and M, as well as the LaSalle function
V LaSalle. The invariance principle does not require the function V LaSalle to be
positive definite and V LaSalle(0) = 0 to hold, as was the case for Lyapunov’s di￾rect method. However, the approach using a positive definite function V LaSalle
is reasonable, since it then holds that
Ω = {x ∈ IRn
| V LaSalle(x) ≤ c} (2.41)
is a positively invariant set of the system, assuming that the inequality
V˙
LaSalle(x) ≤ 0
holds. Note that the set (2.41) is closed, but not necessarily bounded. It is
compact only if it is closed and bounded. Since this property is required in
Theorem 20, we have to verify the boundedness of set (2.41) in the use of this
theorem.
A special case of the invariance principle occurs when only the solution
x(t) = 0 ∈ Ω
fulfills the equation
V˙
LaSalle(x) = 0,140 Chapter 2. Limit Cycles and Stability Criteria
i. e. when
N = {0}
holds, and everywhere else V˙
LaSalle(x) < 0 is fulfilled. Obviously, the trivial
solution x(t) = 0 of the differential equation in this case is also the largest
invariant set, i. e.
M = N = {0}.
Thus x = 0 is an asymptotically stable equilibrium point, since all trajectories
tend to zero. This special case equates to Lyapunov’s direct method, given in
Theorem 13 on p. 115, if we assume that V LaSalle is a positive definite function.
Barbashin and Krasovskii’s Theorem on p. 116 is also a special case of the
invariance principle, because M = {0} is stipulated in this theorem, although
it does not usually hold that N = {0}.
As a further example, we will examine the limit cycle of the system
x˙ 1 = −x2 + x1(1 − x
2
1 − x
2
2
),
x˙ 2 = x1 + x2(1 − x
2
1 − x
2
2
),
(2.42)
which is the unit circle defined by
x
2
1 + x
2
2 = 1. (2.43)
It is depicted in Figure 2.65. We can show that the unit circle is a limit cycle of
the system by inserting equation (2.43) into equation (2.42). The derivatives
of x1 and x2 on the unit circle are then
x˙ 1 = −x2,
x˙ 2 = x1.
(2.44)
We can prove that the unit circle is a closed trajectory by means of the normal
vectors
n =
∂(x
2
1 + x
2
2
)
∂x
= 2 
x1
x2

, x ∈

x ∈ IR2
| x
2
1 + x
2
2 = 1	
,
of the circle, i. e. the vectors which are perpendicular to the boundary of the
circle. Then, applying equation (2.44), we obtain
n
T x˙ = 0,
so that x˙ is perpendicular to n. Thus the circle forms a trajectory. This means
the unit circle is an invariant set for the example in question.
To determine the sets Ω, N, and M, we will choose the function
V LaSalle(x) = (x
2
1 + x
2
2 − 1)2
,
for which2.3. Lyapunov’s Stability Theory 141 State
x2
State x1
-2 -1 0 1 2
1
2
-2
-1
0
s
Fig. 2.65: Trajectories and limit cy￾cle of the system (2.42)
V LaSalle
0
−1
1
x2
x1
Fig. 2.66: Shape of the LaSalle function
V LaSalle(x) = (x
2
1 + x
2
2 − 1)2
V LaSalle(x) ≥ 0
holds, but not
V LaSalle(0) = 0.
Instead of the latter, all points of the unit circle satisfy the equation V LaSalle(x) =
0, i. e.
V LaSalle(x) = 0 for all x ∈ {x ∈ IR2
| x
2
1 + x
2
2 − 1 = 0}.
Furthermore, the function V LaSalle shown in Figure 2.66 has its minima on
the unit circle. For the time derivative, we obtain
V˙
LaSalle(x) = −4(x
2
1 + x
2
2
)(x
2
1 + x
2
2 − 1)2
.
On the unit circle and in x = 0, the derivative of the LaSalle function becomes
V˙
LaSalle(x) = 0.
We are leaving the point x = 0 out of our calculations. Taking this exception
into account, we obtain the sets
Ω = {x ∈ IR2
| V LaSalle(x) = (x
2
1 + x
2
2 − 1)2 ≤ 1 − ε}, 1 ≫ ε > 0,
N = {x ∈ IR2
| x
2
1 + x
2
2 = 1},
M = N,
where ε is an arbitrarily small positive number.
By means of Theorem 20 and the result above, we can calculate that all
trajectories x(t) which begin outside of N tend to the set M = N, i. e. the
unit circle, in its further progression. We have thus verified that the unit circle
is an asymptotically stable limit cycle.142 Chapter 2. Limit Cycles and Stability Criteria
2.3.11 Instability Criterion
All the methods of stability analysis we have described so far aim to prove
the stability of an equilibrium point. If stability cannot be verified, this might
be because the method of stability analysis is not suitable or because the
equilibrium point is unstable. Although the latter is a trivial reason, it is not
always an immediately obvious one.
In such a case, a useful approach is to investigate the instability of an
equilibrium point. This is possible by showing that, for a positive definite
function V (x),
V˙ (x) = x˙
T
grad(V (x)) > 0
holds around the equilibrium point xeq = 0, as illustrated in Figure 2.67. Here
we realize that the temporal change x˙ of the state vector and the gradient
of the function V both point away from the equilibrium point xeq = 0 in all
cases. The trajectories x(t) therefore tend away from the equilibrium point.
Accordingly, we can formulate the following theorem [479], which is a reversal
of Lyapunov’s stability theorem.
Theorem 21 (Instability). Let the differential equation x˙ = f(x) with
the equilibrium point xeq = 0 possess a continuous and unique solution for
every initial state vector within a neighborhood U1(0) of the origin. If a func￾tion V (x) exists which possesses continuous partial derivatives and fulfills the
conditions
•
grad(V(x))
x˙
x1
x2
V (x)
V (x)
Fig. 2.67: Illustration of the equation V˙ (x) = x˙
T
grad(V (x)) > 02.4. Passivity and Stability 143
(1) V (0) = 0,
(2) V (x) > 0 for x 6= 0,
(3) V˙ (x) > 0 for x 6= 0
in a neighborhood U2(0) ⊆ U1(0), then the equilibrium point xeq = 0 is un￾stable.
This theorem, however, cannot prove the instability of an equilibrium point if
in addition to the trajectories progressing away from it, there are trajectories
that tend to it. Theorems that allow for such an analysis can be found in
[170, 389]. These are not often used in practice, however.
2.4 Passivity and Stability
2.4.1 Passive Systems
Passive systems, as indicated by their name, do not have an internal energy
source such as a battery; they are only excitable via their inputs. Such sys￾tems frequently occur in practice. For example, mechanical systems without
propulsion or electrical circuits consisting only of resistors, capacitors, and
coils are always passive systems. In order to define the term passivity of a sys￾tem more precisely and at the same time provide an illustrative explanation,
let us consider a simple linear system consisting of a coil L and a resistor R,
as shown in Figure 2.68.
The energy balance of the system is of the form
Z
t
0
Ri2
R
(τ)dτ
| {z }
consumed
energy
+
1
2
Li2
L
(t) −
1
2
Li2
L
(0)
| {z }
stored
energy
=
Z
t
0
u(τ)i(τ)dτ
| {z }
supplied
energy
.
Since the consumed energy always has to be positive (or in the ideal but not
real case equal to zero), obviously the law
stored energy ≤ supplied energy
holds. If we introduce the storage function
S(t) = 1
2
Li2
L
(t)
for the stored energy, it holds with y = i that
S(t) − S(0) ≤
Z
t
0
u(τ)y(τ)dτ.144 Chapter 2. Limit Cycles and Stability Criteria
i
u
iL
L R
iR
(a)
u y = i
(b)
Fig. 2.68: RL circuit as a) a circuit diagram and b) a block diagram with input
variable u and output variable y = i
We can now generalize the above law, which states that the stored energy
always has to be smaller than or equal to the energy supplied, to arbitrary
systems including MIMO systems
x˙ = f(x,u),
y = g(x,u)
with the restriction that dim(y) = dim(u), i. e. it holds that
S(x(t)) − S(x(0)) ≤
Z
t
0
u(τ)
T y(τ) dτ, (2.45)
where S(x(t)) describes the total energy contained in the system. Since this
is positive, S is a function with S(x(t)) ≥ 0, where with no loss of generality
we can assume S(0) = 0. In this context, we use
Definition 23 (Positive and Negative Definite Functions). A function
v(x) with v(0) = 0 is called
(1) negative semidefinite if v(x) ≤ 0,
(2) negative definite if v(x) < 0 for all x 6= 0,
(3) positive semidefinite if v(x) ≥ 0,
(4) positive definite if v(x) > 0 for all x 6= 0.
The storage function S is therefore at least positive semidefinite. Lyapunov
functions, which we described in the previous section and, as we will soon
see, are closely related to storage functions, are positive definite. Below we
keep in mind that the variables depend on the time, and we will write them
in simplified form without explicitly referring to the time t, e. g. x instead of
x(t).
We can now drop the restriction which states that the function S must
describe the total energy of the system and regard S as a general positive
semidefinite function.
The inequality (2.45) can be transformed to2.4. Passivity and Stability 145
S˙(x) = ∂S(x)
∂x
x˙ ≤ u
T
y,
since equation (2.45) must hold for all t ≥ 0. Combining this with the above
results, we arrive at the following definition of passivity, strict passivity, and
losslessness.
Definition 24 (Passivity, Strict Passivity, and Losslessness). Let a
system be defined by
x˙ = f(x,u),
y = g(x,u)
with m = dim(u) = dim(y). If a continuously differentiable, positive semidefi￾nite function S(x) exists, and where required a positive definite function R(x),
such that for all x ∈ IRn
and u ∈ IRm
(1) S˙(x) ≤ u
T y holds, the system is called passive;
(2) S˙(x) + R(x) ≤ u
T y holds, the system is called strictly passive;
(3) S˙(x) = u
T
y holds, the system is called lossless.
It follows from the definition that strictly passive systems and lossless systems
are also always passive.
If we examine the special case of a static system
y = g(u),
the state vector x is not present. In this case, we can see that S(x) is inde￾pendent of x and therefore it is constant. Thus it holds that
S˙(x) = 0
and a static system, i. e. a characteristic curve or a characteristic diagram, is
passive if
0 ≤ u
T y
holds.
For the scalar case, this equation simplifies to
0 ≤ uy, (2.46)
from which it immediately follows that
sgn(u) = sgn(y).
All characteristic curves which satisfy equation (2.46) therefore lie within the
first and third quadrants and pass through the origin, as illustrated in Figure
2.69. The limiting element, the dead zone, and the three-position element are
passive, for example.146 Chapter 2. Limit Cycles and Stability Criteria
y
u
Fig. 2.69: Characteristic elements y = g(u), whose characteristic curves lie entirely
in the blue shaded sector, are passive.
2.4.2 Stability of Passive Systems
The closeness of the storage function S(x) in the definition of passivity to a
Lyapunov function of the passive system is obvious: inserting u = 0, which
we have required for an equilibrium point at x = 0, it follows from Definition
24 of passivity that
S˙(x) ≤ 0.
This is precisely the central requirement of Theorem 13, i. e. Lyapunov’s sta￾bility theorem on p. 115. However, there is a small but important difference:
the storage function S(x) from Definition 24 need only be positive semidefi￾nite, while a Lyapunov function is required to be positive definite. If we also
demand that the latter be fulfilled for the storage function, the conditions of
Theorem 13 are fulfilled, and we obtain
Theorem 22 (Stability of Passive Systems). A passive system with a
positive definite storage function S possesses an equilibrium point xeq = 0
which is Lyapunov stable.
If the storage function is radially unbounded, the equilibrium point is even
globally Lyapunov stable. If the storage function is not positive definite, its
stability is not guaranteed. A passive system can thus also be unstable. An
example of such a case is the system
x˙ =

−1 0
0 1 
x +

1
0

u,
y =

1 0 
x,
for which the state variable x2(t) is unobservable and unstable. A positive
semidefinite storage function for this system is
S(x) = 1
2
x
T

1 0
0 0 
x.2.4. Passivity and Stability 147
In this case, the passivity inequality
S˙(x) = x
T

1 0
0 0 
x˙ = x1x˙ 1 = −x
2
1 + ux1 ≤ uy = ux1
from Definition 24, which is equivalent to
−x
2
1 ≤ 0,
is fulfilled. The system is therefore passive.
We will now return to the general case. If, when using a positive definite
storage function, we require that the set
{x ∈ IRn
| S˙(x) = 0}
does not contain a trajectory x(t) except for x(t) = 0, this means that the
conditions from the Barbashin and Krasovskii stability theorem, i. e. Theorem
14 on p. 116, are fulfilled. In this case x = 0 is an asymptotically stable
equilibrium point, and we obtain
Theorem 23 (Asymptotic Stability of Passive Systems). A passive
system with a positive definite storage function S possesses an asymptotically
stable equilibrium point at x = 0 if no trajectory x(t) other than x(t) = 0 is
contained within the set {x ∈ IRn
| S˙(x) = 0}.
For strictly passive systems, similar to the passive case above, the inequal￾ity
S˙(x) + R(x) ≤ 0
holds with u = 0. Since the function R(x) is positive definite, by rearranging
the inequality to
S˙(x) ≤ −R(x), (2.47)
we can first deduce that S(x) is positive definite and not only positive semidef￾inite. This is because if S(x) were only positive semidefinite, points x 6= 0
with S(x) = 0 would exist. Such points are inevitably minima of a positive
semidefinite function. Consequently, the gradient of S(x) would have to fulfill
the equation

∂S(x)
∂x
T
= 0
for these points. The latter, however, is impossible, since because of equation
(2.47), i. e.
S˙(x) = ∂S(x)
∂x
x˙ ≤ −R(x) < 0 for x 6= 0, (2.48)148 Chapter 2. Limit Cycles and Stability Criteria
the derivative
∂S(x)
∂x
must be different from zero. Thus we can conclude that there is no minimum,
except for x = 0 with S(0) = 0. Therefore, it follows that S is positive definite.
Furthermore, because of the positive definiteness of R(x) and equation
(2.48), the inequality
S˙(x) < 0
is also fulfilled for x 6= 0. Thus the conditions of Lyapunov’s stability theorem
for asymptotic stability are fulfilled, the storage function S(x) is a Lyapunov
function, and we obtain
Theorem 24 (Asymptotic Stability of Strictly Passive Systems). A
strictly passive system possesses an asymptotically stable equilibrium point at
x = 0.
If the storage function is radially unbounded, the equilibrium point is globally
asymptotically stable. The term strict passivity, compared to passivity, con￾tains the property of asymptotic stability of the equilibrium point xeq = 0.
In contrast to this, passivity – assuming the existence of a positive definite
storage function – only ensures its Lyapunov stability.
2.4.3 Passivity of Connected Systems
When passive systems are connected, in many cases, a passive system again
results. We will first illustrate this by examining two strictly passive systems
which are connected in parallel. Figure 2.70 illustrates the parallel connection
of both strictly passive systems and their input and output variables.
For the strict passivity of the individual systems, it holds that
S˙
i(xi) + Ri(xi) ≤ u
T
i yi
, i = 1, 2. (2.49)
A strictly passive overall system requires the existence of a positive semidefi￾nite storage function S(x) and a positive definite function R(x) so that
S˙(x) + R(x) ≤ u
T y with x
T =

x
T
1 x
T
2

(2.50)
is fulfilled. With the connection scheme of Figure 2.70 and with the equations
S(x) = S1(x1) + S2(x2),
R(x) = R1(x1) + R2(x2),
y = y1 + y2
,
u = u1 = u2,
it follows from equation (2.50) that2.4. Passivity and Stability 149
u1
u2
u y
x˙ 1 = f 1
(x1, u1)
y1 = g1
(x1, u1)
y1
y2
x˙ 2 = f 2
(x2, u2)
y2 = g2
(x2, u2)
Fig. 2.70: Parallel connection of two passive systems
S˙
1(x1) + R1(x1) + S˙
2(x2) + R2(x2) ≤ u
T
y = u
T
1 y1 + u
T
2 y2
.
Provided that equation (2.49) is valid, this inequality is obviously fulfilled.
Since the statements above also hold for
R1(x1) = 0 and R2(x2) = 0,
i. e. the case of passivity, we can formulate
Theorem 25 (Parallel Connection of Passive Systems). Let there be
two (strictly) passive systems
x˙ 1 = f 1
(x1,u1), x˙ 2 = f 2
(x2,u2),
y1 = g1
(x1,u1), y2 = g2
(x2,u2).
Their parallel connection
x˙ 1 = f 1
(x1,u), u = u1 = u2,
x˙ 2 = f 2
(x2,u),
y = g1
(x1,u) + g2
(x2,u)
is a (strictly) passive system.
However, the series connection of two passive (or strictly passive) systems
does not always result in a passive (or strictly passive) system.
Of special interest and importance in control theory are control loops with
passive or strictly passive subsystems, as shown in Figure 2.71. In particular,
we also want to prove the strict passivity of the connected system in the case
in which the subsystems are strictly passive. We will choose the sum S(x) =
S1(x1) + S2(x2) of the storage functions S1 and S2 of the subsystems as the
storage function S of the overall system; for the function R, correspondingly,
R(x) = R1(x1) + R2(x2). Then equation (2.50) with
e1 = u1 − y2
,
e2 = u2 + y1
,
u =

u1
u2

, y =

y1
y2

(2.51)150 Chapter 2. Limit Cycles and Stability Criteria
x˙ 1 = f 1
(x1, e1)
y1 = g1
(x1, e1)
y1
x˙
y2 2 = f 2
(x2, e2)
y2 = g2
(x2, e2)
u1
u2
e1 e2
Fig. 2.71: A control loop consisting of two passive systems
takes the form
S˙
1(x1) + R1(x1) + S˙
2(x2) + R2(x2) ≤ u
T
y = u
T
1 y1 + u
T
2 y2
.
Using equation (2.51) once again, we can rearrange this to
S˙
1(x1)+R1(x1)+S˙
2(x2)+R2(x2)≤(e1+y2
)
T
y1+(e2−y1
)
T
y2 =e
T
1 y1+e
T
2 y2
.
(2.52)
Because of the strict passivity of the subsystems, i. e. because of
S˙
i(xi) + Ri(xi) ≤ e
T
i yi
, i = 1, 2,
the inequality (2.52) is fulfilled and the control loop is also strictly passive.
With the results above, we have also derived the passivity of the control loop
for the case of two passive subsystems, since for this derivation, we only have
to set R1(x) = 0 and R2(x) = 0.
Thus we obtain
Theorem 26 (Control Loop with Passive Subsystems). Let there be
two (strictly) passive systems
x˙ 1 = f 1
(x1, e1), x˙ 2 = f 2
(x2, e2),
y1 = g1
(x1, e1), y2 = g2
(x2, e2).
The control loop that is composed of these systems with e1 = u1 − y2
and
e2 = u2 + y1
, i. e.
x˙ 1 = f 1
(x1,u1 − y2
),
x˙ 2 = f 2
(x2,u2 + y1
),
y1 = g1
(x1,u1 − y2
),
y2 = g2
(x2,u2 + y1
)
is a (strictly) passive system.
Combined with the stability theorems from the previous section, the theo￾rem above provides a very useful tool for designing controllers. This is because,
according to the above theorem and Figure 2.71, combining a strictly passive2.4. Passivity and Stability 151
plant with a strictly passive controller leads to a control loop with a globally
asymptotically stable equilibrium point xeq = 0. This always requires that
the storage functions of the subsystems are radially unbounded and positive
definite.
In summary, passive systems with positive definite storage functions are
Lyapunov stable, while strictly passive systems are even asymptotically stable.
(More precisely: their equilibrium point xeq = 0 is Lyapunov stable.) Since
suitably connected passive systems are themselves passive, the connected sys￾tems also have a stable equilibrium point at x = 0. Thus, such connected
passive systems always yield stable systems. Showing that a system is pas￾sive, or even better strictly passive, is therefore very advantageous.
2.4.4 Passivity of Linear Systems
It is useful to investigate which conditions are necessary and sufficient for a
linear system to be passive or strictly passive. This is because, according to
the results obtained in the previous section, we can connect passive or strictly
passive linear plants to passive or strictly passive nonlinear controllers and
thus obtain a stable control loop.
For our analysis, we will consider the MIMO systems
x˙ = Ax + Bu, x ∈ IRn
, u ∈ IRm,
y = Cx + Du, y ∈ IRm,
(2.53)
with feedthrough Du. In the following, we will restrict ourselves to positive
definite storage functions, even though the definition of passivity only requires
positive semidefinite storage functions. But we have already established that
passive systems with positive semidefinite storage functions do not have to be
stable. It is therefore appropriate to rule out this case right from the start.
For linear systems that we want to examine for passivity and stability, we can
then assume a positive definite quadratic storage function
S(x) = 1
2
x
T R x
with no loss of generality, as shown in [448, 477]. This yields the passivity
inequality
S˙(x) = 1
2

x˙
T R x + x
T R x˙

≤ u
T y for all x ∈ IRn
, u ∈ IRm.
From this it follows with equation (2.53) that
1
2
x
T

AT R + RA
x + u
T B
T R x ≤ u
T Cx + u
T Du. (2.54)
It is possible to decompose every matrix M into a skew-symmetric com￾ponent and a symmetric component according to152 Chapter 2. Limit Cycles and Stability Criteria
M =
1
2

M − MT

| {z }
skew-symmetric
+
1
2

M + MT

| {z }
symmetric
.
For the skew-symmetric matrix, it holds that
z
T

M − MT

z = 0 for all z ∈ IRn
.
Thus we can formulate
u
T Du =
1
2
u
T

D + DT

u.
Equivalent to inequality (2.54), we arrive at the inequality
1
2

x
u
T
"
AT R + RA RB − C
T
B
T R − C −D − DT
# 
x
u

≤ 0,
which leads to
Theorem 27 (Passivity of Linear Systems). A linear system
x˙ = Ax + Bu, x ∈ IRn
, u ∈ IRm,
y = Cx + Du, y ∈ IRm,
with a positive definite storage function is passive if and only if a positive
definite n × n matrix R exists such that the (n + m) × (n + m) matrix
"
AT R + RA RB − C
T
B
T R − C −D − DT
#
(2.55)
is negative semidefinite.
The matrix above can now be represented by a matrix L and a matrix W
as
"
A
T R + RA RB − C
T
B
T R − C −D − DT
#
= −

L
T
WT


L W 
, (2.56)
where the matrix on the right-hand side of the equation above is always neg￾ative semidefinite[8]
.
Taking into account the symmetry of the matrix, the matrix equation
(2.56) leads to the three so-called Kalman-Yakubovich-Popov equations (KYP
equations)
[8] An initial and detailed treatment of this topic can be found in J. G. Willems [477]
and further information is provided in [163].2.4. Passivity and Stability 153
AT R + RA = −L
T L,
RB − C
T = −L
TW,
D + DT = WTW.
(2.57)
It is important to note that the requirement of a negative semidefinite
matrix (2.55), the fulfillment of equation (2.56), and thus (2.57) are equivalent.
This is because, by choosing L as a matrix of dimension (n + m) × n and W
as a matrix of dimension (n + m) × m, creating the matrix

L
T
WT


L W 
, (2.58)
it is possible to create any arbitrary positive semidefinite and also positive
definite symmetric (n + m) × (n + m) matrix.
This circumstance draws our attention to the dimensions of the matrices
L and W. Obviously we can vary their dimensions if we want to create a
positive semidefinite matrix (2.58). For example, either
L ∈ IRn×n
and W ∈ IRn×m
or
L ∈ IRm×n
and W ∈ IRm×m (2.59)
would fit together. Thus, the dimensions of the matrices L and W are not
predetermined. When choosing them, however, their compatibility with each
other and with the other matrices which occur in the Kalman-Yakubovich￾Popov equations (2.57) must be maintained. The choice of matrices (2.59),
which allows for low dimensions of the matrices, is sufficient for the solvability
of the KYP equations [477].
Using the above results, we formulate
Theorem 28 (Passivity and the KYP Equations). A linear system
x˙ = Ax + Bu, x ∈ IRn
, u ∈ IRm
y = Cx + Du, y ∈ IRm,
is passive with a positive definite storage function if and only if the Kalman￾Yakubovich-Popov equations
AT R + RA = −L
T L,
L
TW = C
T − RB,
WTW = D + DT
can be solved by a positive definite symmetric n × n matrix R, a matrix L,
and a matrix W.154 Chapter 2. Limit Cycles and Stability Criteria
In the case of strictly passive linear systems (2.53), we can proceed simi￾larly as in the case of passivity above. We will examine the defining inequality
S˙(x) + R(x) ≤ u
T y (2.60)
for strict passivity. Once again, we will choose a positive definite storage func￾tion
S(x) = 1
2
x
T R x (2.61)
with the n × n matrix R and also a quadratic function
R(x) = µ
1
2
x
T P x (2.62)
with a positive, real-valued constant µ and a positive definite matrix P . At
this point, we will note that choosing a general quadratic form (2.62) for the
function R(x) does not restrict the generality of our further deductions.
If we now insert the functions (2.61) and (2.62) into equation (2.60), we
obtain – similarly to the case of passivity – the inequality (2.60) for strict
passivity in the form of
1
2
x
T

AT R + RA + µP

x + u
T

B
T R − C

x −
1
2
u
T

D + DT

u ≤ 0
or, equivalently,
1
2

x
u
T
"
A
T R + RA + µP RB − C
T
B
T R − C −DT − D
# 
x
u

≤ 0. (2.63)
The linear system (2.53) is thus strictly passive if and only if positive definite
matrices R and µP exist such that the matrix of the quadratic form (2.63) is
negative semidefinite.
This is equivalently formulated as
Theorem 29 (Strict Passivity and the KYP Equations). A linear
system
x˙ = Ax + Bu, x ∈ IRn
, u ∈ IRm,
y = Cx + Du, y ∈ IRm,
is strictly passive if and only if the Kalman-Yakubovich-Popov equations
AT R + RA = −L
T L − µP ,
L
TW = C
T − RB,
WTW = D + DT
can be fulfilled by positive definite symmetric n × n matrices R and µP , a
matrix L, and a matrix W.2.4. Passivity and Stability 155
In cases without a feedthrough D, the Kalman-Yakubovich-Popov equa￾tions from Theorem 29 can be simplified, because, from D = 0, it follows that
W = 0. Furthermore, we can add the matrix L
T L, which is freely selectable
in this case, and the positive definite matrix µP , resulting in a positive definite
matrix Q, which yields
A
T R + RA = −Q,
C = B
T R.
In the case of Theorem 28, the matrix
Q = L
T L
only needs to be positive semidefinite. An explicit solution method for the
Kalman-Yakubovich-Popov equations can be found in [398].
2.4.5 Example: Transporting System for Material Webs
In the industrial production of paper, plastic foil, and textile fabrics, the
material webs are transported over a roller system, either during production
or subsequent treatment, such as coating. For this purpose, pairs of rollers
located between the dispenser and the rewinder press down onto the material
web, so that it is moved along by them. Figure 2.72 illustrates the basic
process.
The speed of the material web must be equal everywhere, i. e. the an￾gular velocity ωi of rollers with the same radius must be equal. From this
requirement, it follows as a further condition that the differences
Fi − Fi−1
between tractions Fi affecting the web at its contact points with the rollers
should be zero or at least not too large, since otherwise the material web will
tear or flutter [324, 488]. We now want to determine the torque equilibria at
the individual rollers. Here Mi
is the driving torque of the ith roller, Fi
is the
tension force which impacts the material web between the rollers i and i + 1,
r denotes the roller radius, J the moment of inertia of the roller elements,
and d · ωi
, with d being the friction coefficient, describes the sliding friction of
the gears. Simplifying, we assume that the radius of the rolls, including the
wound web, is the same.
For the dispenser, it holds that
Jω˙ 1 = rF1 − d · ω1 + M1. (2.64)
For the second and third roller elements, we obtain
Jω˙ i = r(Fi − Fi−1) − d · ωi + Mi
, i = 2, 3, (2.65)156 Chapter 2. Limit Cycles and Stability Criteria
ω1
ω2
ω3
ω4
F1
F2
F3
Fig. 2.72: Transporting system for material webs; the roller elements are constructed
in such a way that they all have the same inertia moment J.
and for the rewinder
Jω˙ 4 = −rF3 − d · ω4 + M4 (2.66)
in a similar way. Further, we must take into account that, between the rollers,
the material web acts like a spring, with c denoting the spring constant. Ac￾cording to Hooke’s law, it follows for the change in the tension between the
rollers i and i + 1 that
F˙
i = c(vi+1 − vi) = c r(ωi+1 − ωi), i = 1, . . . , 3 (2.67)
holds.
Using equations (2.64), (2.65), (2.66), and (2.67), we thus obtain seven
differential equations in total, which we subsume with the state vector
x =

ω1 F1 ω2 F2 ω3 F3 ω4
T
and the input variable vector
u =

M1 M2 M3 M4
T
in the linear system description
x˙ = Ax + Bu,
y =

x1 x3 x5 x7
T
=

ω1 ω2 ω3 ω4
T
(2.68)
with2.4. Passivity and Stability 157
A =



















−
d
J
r
J
0 0 0 0 0
−cr 0 cr 0 0 0 0
0 −
r
J
−
d
J
r
J
0 0 0
0 0 −cr 0 cr 0 0
0 0 0 −
r
J
−
d
J
r
J
0
0 0 0 0 −cr 0 cr
0 0 0 0 0 −
r
J
−
d
J



















, B =
1
J










1 0 0 0
0 0 0 0
0 1 0 0
0 0 0 0
0 0 1 0
0 0 0 0
0 0 0 1










.
To verify the system’s passivity, we will choose
S(x) = 1
2
x
T Rx, R =

















J 0 0 0 0 0 0
0
1
c
0 0 0 0 0
0 0 J 0 0 0 0
0 0 0
1
c
0 0 0
0 0 0 0 J 0 0
0 0 0 0 0
1
c
0
0 0 0 0 0 0 J

















(2.69)
as the storage function. For the temporal derivative of the storage function,
it holds that
S˙(x) = 1
2
x
T
(A
T R + RA)x + u
T B
T R x
= −d(x
2
1 + x
2
3 + x
2
5 + x
2
7
) + u1x1 + u2x3 + u3x5 + u4x7
= −d(x
2
1 + x
2
3 + x
2
5 + x
2
7
) + u1y1 + u2y2 + u3y3 + u4y4. (2.70)
From the passivity inequality
S˙(x) ≤ u
T y = u1y1 + u2y2 + u3y3 + u4y4,
inserting equation (2.70), we obtain the inequality
−d(x
2
1 + x
2
3 + x
2
5 + x
2
7
) ≤ 0,
which is obviously fulfilled. The system is thus passive.
This conclusion is also reached when verifying the Kalman-Yakubovich￾Popov equations from Theorem 29,
AT R + RA = −Q, (2.71)
C
T = RB, (2.72)158 Chapter 2. Limit Cycles and Stability Criteria
in the case that there is no feedthrough. Equation (2.71) is fulfilled with the
matrix R from equation (2.69), where
Q = 2










d 0 0 0 0 0 0
0 0 0 0 0 0 0
0 0 d 0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 d 0 0
0 0 0 0 0 0 0
0 0 0 0 0 0 d










is positive semidefinite. Equation (2.72) is also fulfilled, since
C = B
T R =




1 0 0 0 0 0 0
0 0 1 0 0 0 0
0 0 0 0 1 0 0
0 0 0 0 0 0 1




holds. This corresponds to the output matrix C of the system (2.68) in the
output equation y = Cx.
2.4.6 Positive Real Transfer Functions
If we wish to establish passivity conditions using the transfer function or the
transfer matrix
G(s) = C(sI − A)
−1B + D (2.73)
rather than the state-space representation, we use the concept of positive
realness [58, 307, 329, 445]. We will later see that the passivity of a linear state￾space model and the positive realness of its transfer function are equivalent
system properties. The positive realness is therefore the frequency-domain
analogon of passivity in the state space.
First we will define positive realness for SISO systems, and then for MIMO
systems.
Definition 25 (Positive Realness of Transfer Functions). A transfer
function
G(s) = k
(s − nm)· · ·(s − n1)
(s − pn)· · ·(s − p1)
is called positive real if, for all s ∈ C with Re {s} > 0, the inequality
Re {G(s)} ≥ 0
holds. It is called strictly positive real if G(s − ǫ) for some ǫ > 0 is positive
real.2.4. Passivity and Stability 159
Here C denotes the set of complex numbers. The term positive real[9],[10] is
derived from the fact that the real part Re {G(s)} of the transfer function
must always be positive or equal to zero.
Some properties of positive real transfer functions can be deduced from
the definition itself. First of all, we can easily conclude for
Re {s} > 0
and with s = σ + jω that
limσ→0
Re {G(σ + jω)} = Re {G(jω)} ≥ 0.
The Nyquist plot of G(jω) of positive real or strictly positive real systems
must therefore lie within the closed or open right half-plane of the complex
plane. Figure 2.73 illustrates this.
From the fact that the Nyquist plot of G(jω) must lie within the right
complex half-plane, for a positive real transfer function G(s) it follows that
n − m = 0 or n − m = 1
holds for its degree m of the nominator and its degree n of the denominator.
Otherwise the phase would be less than −90◦
.
Since the requirements of Definition 25 are generally difficult to verify, the
following necessary and sufficient criteria are useful. We start with [58]
Im
Re
Nyquist plot
ω = ∞ ω = 0
Fig. 2.73: Positive real transfer functions have a Nyquist plot in the closed right
half-plane.
[9] Both the term positive real and in particular the term strictly positive real are
defined differently and inconsistently in the literature [214, 228, 240, 288, 445].
Of the known definitions, we use here those from which it follows that passivity
and strict passivity are equivalent to positive realness and strict positive realness,
respectively.
[10] Closely related to the properties of passivity and positive realness is the property
of hyperstability, which was introduced by V.M. Popov [374, 375]. However, in
the literature the term passivity has prevailed over hyperstability. Detailed infor￾mation on hyperstability can be found in [16, 375].160 Chapter 2. Limit Cycles and Stability Criteria
Theorem 30 (Positive Real Transfer Functions I). A transfer function
G(s) is positive real if and only if
(1) G(s) only has poles pi with Re {pi} ≤ 0, and
(2) Re {G(jω)} ≥ 0 holds for every ω ≥ 0, for which jω is not a pole of G(s),
and
(3) possible poles jω0 are single poles and have nonnegative real residuals
Res
s=jω0
(G(s)) = lim
s→jω0
(s − jω0)G(s).
The conditions of the above sufficient and necessary theorem are also often
used as an alternative to Definition 25, to define the term positive realness
[58, 232]. Furthermore we can use [78, 473]
Theorem 31 (Positive Real Transfer Functions II). A coprime transfer
function
G(s) = N(s)
D(s)
is positive real if and only if
(1) the polynomial N(s) + D(s) only has zeros with a negative real part, and
(2) Re {G(jω)} ≥ 0 holds for every ω ≥ 0, for which jω is not a pole of G(s).
The property of positive realness at most ensures the Lyapunov stability
of a system and not asymptotic stability, since the real parts of the poles are
allowed to be equal to zero. On the other hand, strictly positive real transfer
functions only have poles with negative real parts. This property guarantees
asymptotic stability, which is extremely important in control loops. From a
practical point of view, strict positive realness is thus of much greater relevance
than positive realness.
For strictly positive real transfer functions the following holds [84, 85, 232]:
Theorem 32 (Strictly Positive Real Transfer Functions). A transfer
function G(s) is strictly positive real if and only if
(1) G(s) only has poles pi with Re {pi} < 0, and
(2) for every ω with 0 ≤ ω < ∞ the inequality Re {G(jω)} > 0 holds, and
(3) either
G(∞) > 0
or
G(∞) = 0
is fulfilled and, in the latter case,
limω→∞
ω
2Re {G(jω)} > 0
holds.2.4. Passivity and Stability 161
In addition to the above sufficient and necessary conditions for positive
or strict positive realness, the following necessary conditions [58], which we
partially described at the beginning of this section, also apply.
Theorem 33 (Necessary Conditions for Positive Realness). For a
(strictly) positive real transfer function G(s) it holds that
(1) only poles pi with Re {pi} ≤ 0 (Re {pi} < 0) exist,
(2) only zeros ni with Re {ni} ≤ 0 (Re {ni} < 0) exist,
(3) the difference n−m between the numerator degree m and the denominator
degree n is either zero or one,
(4) the Nyquist plot of G(jω) lies completely within the closed (open) right
half-plane of the complex plane.
The positive realness of a transfer matrix
G(s) =



G11(s) · · · G1m(s)
.
.
.
.
.
.
.
.
.
Gm1(s) · · · Gmm(s)



of the MIMO systems
Y (s) = G(s) · U(s)
is defined similarly to the positive realness of a transfer function in
Definition 26 (Positive Realness of Transfer Matrices). A transfer
matrix G(s) is called positive real if, for all s ∈ C with Re {s} > 0, the
inequality
Re {z
∗G(s)z} ≥ 0 for all z ∈ C
m (2.74)
holds. It is called strictly positive real if G(s−ǫ) is positive real for a constant
ǫ > 0.
Here z
∗
denotes the complex conjugate transpose of the vector z and C
m is
the m - dimensional space of complex numbers.
The defining equation (2.74) is equivalent to the requirement that the
matrix
G(s) + G
∗
(s) (2.75)
be positive semidefinite for all s ∈ C with Re {s} > 0. Here
G
∗
(s) = G
T
(s)
is the conjugate transpose, i. e. the hermitian transpose matrix of G(s), and
s is the complex conjugate of s.
For transfer matrices G(s), a necessary and sufficient criterion also exists
to determine positive realness [116, 232, 442]. This is162 Chapter 2. Limit Cycles and Stability Criteria
Theorem 34 (Positive Real Transfer Matrices). The transfer matrix
G(s) is positive real if and only if
(1) all transfer functions Gij (s) only have poles pi with Re {pi} ≤ 0, and
(2) for every ω ≥ 0, for which jω is not a pole of G(s),
G(jω) + G∗
(jω)
is a positive semidefinite matrix, and
(3) possible poles jω0 are single poles and the residual matrix
Res
s=jω0
(G(s)) = lim
s→jω0
(s − jω0)G(s)
is a hermitian and positive semidefinite matrix.
A quadratic n × n matrix M is hermitian if all its diagonal elements are real
and the relationship mik = mki holds for all symmetric off-diagonal elements
mik and mki. Here mki is the complex conjugate of mki. Thus a hermitian
matrix M takes the form
M =







m11 m12 m13 · · · m1n
m12 m22 m23 · · · m2n
m13 m23 m33 · · · m3n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
m1n m2n m3n · · · mnn







.
Note in this context that
G
∗
(jω) = G
T
(−jω)
holds.
Analogous to the above Theorem 34, there is a counterpart to strictly
positive real transfer matrices [83, 84, 118], namely
Theorem 35 (Strictly Positive Real Transfer Matrices). The m × m
transfer matrix G(s) is strictly positive real if and only if
(1) all transfer functions Gij (s) only have poles pi with Re {pi} < 0, and
(2) for every ω with 0 ≤ ω < ∞, the matrix
G(jω) + G
∗
(jω)
is positive definite, and
(3) the inequality
lim
|ω|→∞
ω
2(m−µ)
det(G(jω) + G
∗
(jω)) > 0, µ = rank (G(∞) + G
∗
(∞)),
is fulfilled.2.4. Passivity and Stability 163
If we have a look at the state-space representation belonging to the transfer
function (2.73), i. e.
x˙ = Ax + Bu,
y = Cx + Du,
(2.76)
it holds for the feedthrough matrix D that
D = G(∞) and DT = D∗ = G
∗
(∞).
The rank µ of the matrix G(∞) + G∗
(∞) from the theorem above can thus
also be determined via
µ = rank (D + DT
).
In the large majority of cases D = 0 is given, and thus it holds that µ = 0
in Condition (3) of Theorem 35, since only very few practical systems (2.76)
have a direct feedthrough of the control signal vector u to the output signal
vector y. Some further results if D 6= 0 can be found in [475].
2.4.7 Equivalence of Positive Realness and Passivity
The requirements of Theorems 34 and 35 for MIMO systems are generally
very difficult to verify. Based on the Kalman-Yakubovich-Popov equations
AT R + RA = −L
T L, (2.77)
L
TW = C
T − RB, (2.78)
WTW = D + DT
, (2.79)
we can derive criteria for positive or strict positive realness which are based
on the state-space representation. These are often easier to utilize than the
corresponding frequency-domain criteria. Our starting point is the defining
inequality (2.74) for positive real transfer functions, or the equivalent require￾ment that the matrix (2.75), i. e.
G(s) + G∗
(s) = G(s) + GT
(s), (2.80)
must be positive semidefinite for all s ∈ C with Re {s} > 0. We assume that
the system in question
x˙ = Ax + Bu,
y = Cx + Du
(2.81)
is controllable and observable. We then calculate its transfer matrix
G(s) = C(sI − A)
−1B + D. (2.82)164 Chapter 2. Limit Cycles and Stability Criteria
In this context, we recall linear system theory and the relationship be￾tween transfer matrix and state-space representation. A consequence of the
controllability and observability of a MIMO state-space model (2.81) with
degree n is that its transfer matrix (2.82) has the same degree and coprime
transfer functions Gij (s) as elements. Conversely, a state-space model (2.81)
is a controllable and observable realization of the transfer matrix (2.82) if and
only if all Gij (s) are coprime and the degree of this state-space model is equal
to the degree of the transfer matrix[11] [73].
We now insert equation (2.82) into equation (2.80). This yields the expres￾sion
G(s) + G
T
(s) = C(sI − A)
−1B + D + B
T
[(sI − A)
−1
]
T C
T + DT
for equation (2.80). By applying the KYP equations (2.78) and (2.79), i. e.
C = WT L + B
T R and D + DT = WTW,
the equation
G(s) + GT
(s) =
= (WTL+B
TR)(sI−A)
−1B+B
T
(sI−A
T
)
−1
(L
TW +RB)+WTW
= WT L(sI−A)
−1B+B
T
(sI−A
T
)
−1L
TW +WTW
+ B
T
h
R(sI−A)
−1+(sI−A
T
)
−1R
i
B
| {z }
F
(2.83)
follows. In the last summand F of the equation above, we now factor out the
matrix (sI − A)
−1
and its complex conjugate variant (sI − A
T
)
−1
. This last
summand F then takes the form
F = B
T
(sI − A
T
)
−1
h
(sI − A
T
)R + R(sI − A)
i
(sI − A)
−1B
= B
T
(sI − AT
)
−1
h
(s + s)R − AT R − RAi
(sI − A)
−1B
= B
T
(sI − AT
)
−1
h
2Re {s} · R + L
T L
i
(sI − A)
−1B, (2.84)
where the last conversion is based on the KYP equation (2.77). Equation
(2.84) is now inserted into equation (2.83), which yields
G(s) + G
T
(s) = h
WT + B
T
(sI − A
T
)
−1L
T
i 
W + L(sI − A)
−1B

+ 2Re {s} · B
T
(sI − AT
)
−1R(sI − A)
−1B.
[11] The degree n of a transfer matrix G(s) is defined as the degree of the lowest
common denominator of all minors of G(s). This denominator is called the char￾acteristic polynomial of the transfer matrix.2.4. Passivity and Stability 165
When the equation is more compactly formulated, i. e.
G(s) + GT
(s) = M∗M + 2Re {s} · N∗RN,
M = W + L(sI − A)
−1B,
N = (sI − A)
−1B,
we immediately recognize that G(s)+GT
(s) is positive semidefinite for all s ∈
C with Re {s} > 0. This is because the matrix M∗M is positive semidefinite
and so is the matrix N∗RN, since R is positive definite. The summation
M∗M + 2Re {s} · N∗RN
of both matrices is also positive semidefinite only, and not positive definite,
since the determinant
det (M∗M + 2Re {s} · N∗RN)
is a polynomial in s and can take the value zero. The KYP inequalities (2.77),
(2.78), and (2.79) are thus sufficient for the positive realness of the transfer
matrix (2.82). As shown in [17, 58] they are also necessary, so the following
theorem holds:
Theorem 36 (Positive Realness and the KYP Equations). Let there
be a controllable and observable system
x˙ = Ax + Bu, x ∈ IRn
, u ∈ IRm,
y = Cx + Du, y ∈ IRm,
with the associated transfer matrix
G(s) = C(sI − A)
−1B + D.
The transfer matrix G(s) is positive real if and only if the Kalman-Yakubovich￾Popov equations
AT R + RA = −L
T L,
L
TW = C
T − RB,
WTW = D + DT
are solvable by a positive definite symmetric n×n matrix R, a matrix L, and
a matrix W.
Further, we obtain the KYP equations for the strictly positive real case if
we combine the KYP equation
A
T R + RA = −L
T L (2.85)166 Chapter 2. Limit Cycles and Stability Criteria
of the above theorem with the requirement from Definition 26 of strict positive
realness, i. e. that
G(s − ǫ) = C[(s − ǫ)I − A]
−1B + D
= C[sI − (A + ǫI)]−1B + D
must be positive real. Accordingly, in equation (2.85) instead of the matrix A
we use the matrix A + ǫI and arrive at
(A + ǫI)
T R + R(A + ǫI) = −L
T L.
From this we obtain the equation
A
T R + RA = −L
T L − 2ǫR,
which we can represent with no loss of generality using
µ = 2ǫ > 0
as
A
T R + RA = −L
T L − µR.
From this follows [465]
Theorem 37 (Strict Positive Realness and the KYP Equations).
Let there be a controllable and observable system
x˙ = Ax + Bu, x ∈ IRn
, u ∈ IRm,
y = Cx + Du, y ∈ IRm,
with the associated transfer matrix
G(s) = C(sI − A)
−1B + D.
The transfer matrix G(s) is strictly positive real if and only if the Kalman￾Yakubovich-Popov equations
AT R + RA = −L
T L − µR,
L
TW = C
T − RB,
WTW = D + DT
are solvable by a positive definite symmetric n × n matrix R, a matrix L, a
matrix W, and a positive constant µ > 0.
By comparing Theorem 36 and Theorem 28, we can deduce the equivalence
of the terms passive and positive real. However, if we compare Theorem 37
and Theorem 29, equivalence between strictly passive systems and strictly2.4. Passivity and Stability 167
positive real transfer functions seems not to be present. This is because the
KYP equation
AT R + RA = −L
T L − µP (2.86)
from Theorem 29 and the KYP equation
A
T R + RA = −L
T L − µR (2.87)
from Theorem 37 differ. Obviously, equation (2.87) from Theorem 37 is more
restrictive, since in equation (2.86) from Theorem 29 we are not restricted in
our choice of the matrix P to the particular matrix R. However, it is possible
to state that in the case of a positive definite solution matrix R in equation
(2.87), if we choose
P = R,
equation (2.86) is also fulfilled. Thus we have so far only proven that systems
with strictly positive real transfer functions are strictly passive.
However, it is possible to show that the reverse is also true [297], because
in the case of positive definite solution matrices R and P for equation (2.86),
equation (2.87) is also fulfilled in every case. The system properties of strict
passivity and strict positive realness are thus equivalent to each other. They
describe the same circumstance: one is based on the system representation
in the state space, the other on the system representation in the frequency
domain.
Summarizing all of this, we can formulate [297]
Theorem 38 (Passivity and Positive Realness). A controllable and
observable system
x˙ = Ax + Bu,
y = Cx + Du
is (strictly) passive with a positive definite storage function if and only if its
transfer matrix is (strictly) positive real.
Via the theorems on positive realness, which are based on transfer func￾tions, we obtain an alternate way of evaluating whether a system is passive or
strictly passive. Here the criteria for SISO systems are easy to utilize, while
the criteria for MIMO systems are often very laborious.
All theorems that are based on the KYP equations involve searching for a
matrix R which solves them. Especially for the determination of strict passiv￾ity, a criterion would be useful which provides a statement on the strict pas￾sivity or strict positive realness of a system without requiring the search for a
matrix R. For the most common case, i. e. for systems without a feedthrough,
this is provided by [85]168 Chapter 2. Limit Cycles and Stability Criteria
Theorem 39 (Corless and Shorten’s Theorem). A transfer function
G(s) = C(sI − A)
−1B, C ∈ IRm×n
, B ∈ IRn×m,
is strictly positive real if and only if
(1) all poles of G(s) have negative real parts,
(2) the m × m matrix R = −CAB − (CAB)
T
is positive definite, and
(3) the matrix

A + ABR−1CA ABR−1B
T AT
−AT C
T R−1CA −AT − AT C
T R−1B
T AT

has 2m eigenvalues at zero, and no further eigenvalues on the imaginary
axis.
Since strict positive realness and strict passivity, according to Theorem 38, are
equivalent to each other for controllable and observable systems, Theorem 39 is
also a necessary and sufficient criterion for the strict passivity of a controllable
and observable system.
2.4.8 Lossless Hamiltonian Systems
A class of passive systems common in control engineering is the class of me￾chanical systems, in particular multi-body systems such as cranes, satellites,
and industrial robots. We can determine these systems’ equations of motion by
their kinetic and potential energies Ekin and Epot via the Lagrange equations
[184]
d
dt
∂L
∂q˙i
−
∂L
∂qi
= τi
, i = 1, . . . , k,
or the corresponding vectorial form
d
dt
∂L
∂q˙
−
∂L
∂q
= τ
T
. (2.88)
Here
L = Ekin − Epot (2.89)
is the Lagrange function; the variables qi written as vector
q =



q1
.
.
.
qk



denote generalized coordinates, i. e. lengths or angles, and the values τi written
as vector2.4. Passivity and Stability 169
c
m
z1, u
Fig. 2.74: Spring-mass system
τ =



τ1
.
.
.
τk



denote forces or torques.
For illustration purposes, we will look at a frictionless spring-mass system,
as depicted in Figure 2.74. By applying an external force u that extends the
spring, the mass m can be displaced by a stretch z1. The spring with its
associated stiffness c counteracts this action. For the kinetic and potential
energy, it holds that
Ekin =
1
2
mz˙
2
1
,
Epot =
1
2
cz2
1
,
and therefore
L =
1
2
￾
mz˙
2
1 − cz2
1

.
Inserting this into the Lagrange equation yields
1
2

d
dt 
∂(mz˙
2
1 − cz2
1
)
∂z˙1

−
∂(mz˙
2
1 − cz2
1
)
∂z1

= u.
From this, we obtain
mz¨1 + cz1 = u.
Using the state vector
z =

z1
z2

=

z1
z˙1

,
the state-space model of the system becomes
z˙ =


0 1
−
c
m
0

z +


0
1
m

u.
Aside from the Lagrange method, there is a further method from which
a description of the system can be derived and which is convenient in con￾trol theory. This is one in which the passivity of the system is immediatel170 Chapter 2. Limit Cycles and Stability Criteria
apparent. It is based on a generalized energy function, which is called the
Hamiltonian function.
We can derive this convenient system description by using the Lagrange
function as follows. First we will define the new coordinates
p =

∂L
∂q˙
T
. (2.90)
Our next step is to define the Hamiltonian function
H(q, p) = q˙
T
p − L(q, q˙). (2.91)
Now we differentiate equation (2.91) on both sides with respect to time and
arrive at
dH(q, p)
dt =
dq˙
T
p
dt −
dL(q, q˙)
dt ,
which yields
∂H
∂q
q˙ +
∂H
∂p
p˙ = q¨
T p + q˙
T p˙ −
∂L
∂q
q˙ −
∂L
∂q˙
q¨. (2.92)
Now we insert equation (2.90) into equation (2.92), which yields
∂H
∂q
q˙ +
∂H
∂p
p˙ = −
∂L
∂q
q˙ + q˙
T p˙. (2.93)
From the Lagrange equation (2.88) and using equation (2.90), it follows that
∂L
∂q
=
d
dt
∂L
∂q˙
− τ
T = p˙
T − τ
T
.
We insert this into equation (2.93) and arrive at
∂H
∂q
q˙ +
∂H
∂p
p˙ = (−p˙ + τ )
T
q˙ + q˙
T
p˙. (2.94)
By comparing both sides of the equation, we obtain the system equations,
which are known as the Hamiltonian canonical form
q˙ =

∂H
∂p
T
,
p˙ = −

∂H
∂q
T
+ τ .
(2.95)
They are also referred to as the Hamiltonian equations, which are 2k first-order
differential equations, while the corresponding equivalent Lagrange equations
are k second-order differential equations.2.4. Passivity and Stability 171
Below we will explain the meaning of the Hamiltonian function H. Here
we will take into account that, in conventional mechanical systems, the kinetic
energy takes the form
Ekin =
1
2
q˙
TM(q)q˙. (2.96)
For example, in the simple case of linear motion it holds that
Ekin =
1
2
mv2
.
In equation (2.96), the matrix M(q) is a generalized mass matrix whose el￾ements depend on the position q; an example is the joint angles qi of an
industrial robot. This generalized mass matrix is positive definite and thus
also regular. In contrast to the kinetic energy Ekin, the potential energy Epot
is only a function of the position q and not of the velocity q˙.
From equations (2.89) and (2.90) it follows with equation (2.96) that
p =

∂L
∂q˙
T
=

∂Ekin(q, q˙)
∂q˙
−
∂Epot(q)
∂q˙
T
= M(q)q˙. (2.97)
Inserting this into equation (2.91), along with equations (2.89), (2.96), and
(2.97), yields
H(q, p) = q˙
TM(q)q˙ − (Ekin(q, q˙) − Epot(q))
=
1
2
q˙
TM(q)q˙ + Epot(q)
and thus
H(q, p) = Ekin(q, p) + Epot(q).
The Hamiltonian function therefore corresponds to the total amount of energy
in the system.
We can now derive a further important property of the Hamiltonian func￾tion. For this purpose, we will take a further look at equation (2.94),
∂H
∂q
q˙ +
∂H
∂p
p˙
| {z }
dH
dt
= τ
T
q˙ −p˙
T
q˙ + q˙
T
p˙
| {z }
= 0
.
It immediately follows that
dH
dt = τ
T
q˙. (2.98)
The interpretation of this equation is useful for our understanding. Two cases
can be distinguished:172 Chapter 2. Limit Cycles and Stability Criteria
(1) No external forces τ influence the system. It therefore holds that
dH
dt = 0,
which means that H, and thus the total energy of the system, is constant.
This case describes lossless systems without an external energy input, such
as ideal pendulum clocks, meaning clocks without friction.
(2) An external force τ influences the system. In this case,
dH
dt = τ
T
q˙
defines the supplied power. An example of this is an industrial robot, for
which motors induce forces onto the joints.
What specific steps do we take to determine the Hamiltonian equations?
The following five steps describe the procedure:
Step 1: Write down the Lagrange function L = Ekin − Epot.
Step 2: Determine the generalized impulses
p =

∂L
∂q˙
T
= M(q)q˙.
Step 3: By inverting the equation from the previous step, determine the
generalized velocities
q˙ = M−1
(q)p.
Step 4: Write down the Hamiltonian function
H(q, p) = Ekin(q, p) + Epot(q).
Step 5: Form the Hamiltonian equations (2.95).
Determining the inverse function q˙ = M−1
(q)p of Step 3 can be especially
complicated and time-consuming. This is one of the reasons why the Hamilto￾nian equations are often more difficult to derive than the motion equations via
the Lagrange equations. Furthermore, the Hamiltonian equations are often of
a more complex form.
As an example, we will again examine the spring-mass system from Figure
2.74. With
q = z1 and p =
∂L
∂q˙
=
1
2
∂
￾
mz˙
2
1 − cz2
1

∂z˙1
= mz˙1,
the Hamiltonian function become2.4. Passivity and Stability 173
H = Ekin + Epot =
1
2
mz˙
2
1 +
1
2
cz2
1 =
1
2m
p
2 +
c
2
q
2
, (2.99)
which yields
q˙ =
∂H
∂p =
p
m
,
p˙ = −
∂H
∂q + u = −cq + u
(2.100)
for the Hamiltonian equations (2.95). In this example, the Hamiltonian equa￾tions have a simple form.
Now we will investigate the Hamiltonian equations (2.95) for passivity. We
subsume the coordinates q and p to a state vector
x =

q
p

so that we can formulate the Hamiltonian equations as
x˙ = JH

∂H
∂x
T
+ B · τ , J H =

0 I
−I 0

, B =

0
I

, (2.101)
where I is the k-dimensional identity matrix.
If we choose
y =

∂H
∂p
T
= q˙ (2.102)
as the output vector, set u = τ , and assume a storage function S(x), it holds
for the passivity inequality that
S˙(x) ≤ u
T
y = τ
T
q˙. (2.103)
This equation can obviously be solved if we choose
S(x) = H(x)
as the storage function, since, using equation (2.98), the identity
S˙ = H˙ = τ
T
q˙ (2.104)
follows. We know if H is a positive semidefinite function, i. e. it holds that
H(0) = 0 and H(x) ≥ 0 for x 6= 0, the system is passive. According to the
equal sign in equation (2.103) and equation (2.104), it is also lossless.
In our spring-mass example, H is a positive definite function, since
H(p, q) = 0 for [q p]
T= [z1 mz˙1]
T= 0174 Chapter 2. Limit Cycles and Stability Criteria
holds and H(p, q) is positive everywhere else, both of which we can see from
equation (2.99). Further, with equation (2.99) and equation (2.100) it holds
that
H˙ =
pp˙
m
+ cqq˙ =
up
m
= uq˙ = uy,
so that equation (2.103) is fulfilled, and thus the spring-mass system is passive.
In the following, we generalize the Hamiltonian equations (2.101) with the
output function (2.102) to
x˙ = J(x)

∂S
∂x
T
+ B(x) · u,
y = B
T
(x)

∂S
∂x
T
(2.105)
by considering the matrices J and B to be dependent on the state vector x.
Here arbitrary skew-symmetric matrices
J(x) = −J
T
(x)
are allowed. They describe the lossless energy exchange within the system. The
positive definite function S(x) we are now using instead of the Hamiltonian
function H(x) also constitutes a generalization. It may be a Hamiltonian
function, but could also be a differently obtained positive definite function.
However, we will restrict ourselves to positive definite functions S(x), since
only these functions, when used as storage functions, guarantee stability in
connection with passivity. The generalized system (2.105), which is described
by the generalized Hamiltonian equations, is called an input or port-controlled
Hamiltonian system (PCH system). The term port-controlled results from the
fact that we analyze systems with input variables u or τ .
The system (2.105) is also passive and lossless, and consequently stable if
S(x) is positive definite, since it holds that
S˙(x) = ∂S
∂x
x˙ =
∂S
∂x
J(x)

∂S
∂x
T
+
∂S
∂x
B(x) · u.
Based on the skew-symmetry of J, we obtain
∂S
∂x
J

∂S
∂x
T
= 0,
and thus
S˙(x) = ∂S
∂x
B(x) · u = u
T
y.
The passivity inequality (2.103) is therefore fulfilled, and the function S is the
storage function that we were searching for.2.4. Passivity and Stability 175
2.4.9 Example: Self-Balancing Vehicle
As an example of a PCH system, we will look at a self-balancing vehicle with
one axis and one or two wheels, as shown in Figure 2.75. Industrial applications
of such systems are the one-wheeled Honda U3-X [174], the Segway [220, 322],
and the Toyota Winglet [275], which both have two wheels. One or two motors
in the chassis drive the wheel (or wheels) with the generated torque M.
l
ω,M
vc
r
ϕ
S
z
x
Fig. 2.75: Self-balancing vehicle
To set up the dynamic equations in PCH form, we will first determine the
kinetic energy of the wheels
Ekin,w =
1
2
J wω
2 +
1
2
mwr
2ω
2 =
1
2
J w
r
2
x˙
2 +
1
2
mwx˙
2
,
where ω denotes the angular velocity of the wheels, which we consider to be
equal for both wheels. The velocity of the vehicle is given by x˙ = rω. The sum
of the inertia moment of the wheels is denoted as J w. Their mass and radius
are denoted by mw and r, respectively. The driving force that acts upon the
wheels is the torque M, which is generated by the motor.
Similarly, for the combined kinetic energy of the driver and chassis mounted
between the wheels, it holds that
Ekin,c =
1
2
Jcϕ˙
2 +
1
2
mc|vc|
2
, (2.106)
where ϕ is the angle between the vertical z-coordinate axis and the line passing
through the center of gravity S and the wheel axis, vc is the velocity of the176 Chapter 2. Limit Cycles and Stability Criteria
center of gravity S, Jc is the inertia moment of the driver and chassis with
respect to the wheel axis as the axis of rotation, and mc is the mass of the
driver and chassis.
For the velocity vc of the center of gravity S, it holds that
vc =

rω + lϕ˙ cos(ϕ)
−lϕ˙ sin(ϕ)

=

x˙ + lϕ˙ cos(ϕ)
−lϕ˙ sin(ϕ)

and thus for the kinetic energy (2.106) of the driver and chassis it follows that
Ekin,c =
1
2
Jcϕ˙
2 +
1
2
mc
￾
x˙
2 + 2lx˙ϕ˙ cos(ϕ) + l
2ϕ˙
2

.
Subsequently, we will determine the potential energy according to
Epot = mwgr + mcg(r + l cos(ϕ)) = (mw + mc)gr + mcgl cos(ϕ).
The constant term in the potential energy formula can be arbitrarily chosen
with no loss of generality, since the value at which the potential energy be￾comes zero can always be set arbitrarily. We choose mcgl as this value, so that
the minimum of the potential energy is zero. It lies at ϕ = π. Thus it holds
that
Epot = mcgl cos(ϕ) + mcgl.
Now we can write down the Lagrange function
L = Ekin,w + Ekin,c − Epot
from which, in the subsequent second step, we will be able to determine the
generalized impulses
p =

∂L
∂q˙
T
.
In the present case, we have
q =

x
ϕ

,
and thus
p =
 
∂L
∂q˙
!T
=






∂L
∂x˙
∂L
∂ϕ˙






=



mw + mc +
J w
r
2 mcl cos(ϕ)
mcl cos(ϕ) mcl
2 + Jc



| {z }
M(q)



x˙
ϕ˙



| {z }
q˙
(2.1072.4. Passivity and Stability 177
holds.
In the third step, we can determine from equation (2.107), i. e. from
p = M(q)q˙,
the vector
q˙ = M−1
(q)p,
which yields
q˙ =

x˙
ϕ˙

=
1
ab − c
2 cos2(ϕ)

b −c · cos(ϕ)
−c · cos(ϕ) a

p (2.108)
with the abbreviations
a = mw + mc +
J w
r
2
, b = mcl
2 + Jc, c = mcl.
The fourth step provides the Hamiltonian function
H(q, p) =Ekin,w + Ekin,c + Epot
=
1
2

mw +mc+
J w
r
2

x˙
2+
1
2
￾
mcl
2+Jc

ϕ˙
2+cx˙ϕ˙ cos(ϕ)+cg cos(ϕ)+cg
=
1
2
ax˙
2+
1
2
bϕ˙
2+cx˙ϕ˙ cos(ϕ)+cg cos(ϕ)+cg.
In this equation, regarding
[ ˙x ϕ˙]
T = q˙,
we insert equation (2.108), where
p = [p1 p2]
T
.
This results in
H(q, p) = ap2
2 + bp2
1 − 2cp1p2 cos(ϕ)
2 (ab − c
2 cos2(ϕ)) + cg cos(ϕ) + cg. (2.109)
The Hamiltonian function for x˙ = 0 and ϕ˙ = 0, i. e. p1 = 0 and p2 = 0 due to
equation (2.107), and for ϕ = π as well as x ∈ IR, possesses minima with
H([x π]
T
, 0) = 0.
The angle ϕ = π obviously does not appear in the real system; however, in
the mathematical system it provides stable equilibrium points for all states
x ∈ IR and p = 0. It is plausible that the equilibrium points at ϕ = 0, x ∈ IR,
and p = 0 are unstable178 Chapter 2. Limit Cycles and Stability Criteria
In the fifth step, we obtain the system dynamics in Hamiltonian form as
q˙ =

x˙
ϕ˙

=

∂H
∂p
T
=
1
ab − c
2 cos2(ϕ)
"
bp1 − cp2 cos(ϕ)
ap2 − cp1 cos(ϕ)
#
,
p˙ =

p˙1
p˙2

= −

∂H
∂q
T
+ τ = −






∂H
∂x
∂H
∂ϕ






+ τ
=





0
c · sin(ϕ)
"
c cos(ϕ)
￾
ap2
2 + bp2
1 − cp1p2 cos(ϕ)

− abp1p2
(ab − c
2 cos2(ϕ))2 + g
#





+ τ .
The external forces or torques τ result from the torque M, i. e.
τ =



M
r
−M


 ,
so that we finally arrive at the Hamiltonian equations




x˙
ϕ˙
p˙1
p˙2




=














bp1 − cp2 cos(ϕ)
ab − c
2 cos2(ϕ)
ap2 − cp1 cos(ϕ)
ab − c
2 cos2(ϕ)
0
c · sin(ϕ)
"
c cos(ϕ)
￾
ap2
2+bp2
1−cp1p2 cos(ϕ)

−abp1p2
(ab − c
2 cos2(ϕ))2 +g
#














+









0
0
1
r
−1









M.
(2.110)
For the output variables, we subsequently choose
y =

∂H
∂p
T
= q˙ =

x˙
ϕ˙

. (2.111)
It holds that H(0, 0) = 2cg, and otherwise H(q, p) ≥ 0. Because of the
inequality H(0, 0) 6= 0, the Hamiltonian function in the form from equation
(2.109) is not positive semidefinite. Therefore we cannot use it to establish
passivity for the vehicle. However, if we do not choose the angle ϕ as a state,
but rather the angle Θ = π−ϕ, we arrive at the modified Hamiltonian functi2.4. Passivity and Stability 179
Hˆ ([x Θ]
T
, p) = H([x ϕ = π − Θ]
T
, p)
=
ap2
2 + bp2
1 + 2cp1p2 cos(Θ)
2(ab − c
2 cos2(Θ)) + cg (1 − cos(Θ)),
which is positive semidefinite and with which we can finally prove the passivity
of the system (2.110), (2.111).
For the sake of comparison, from the Lagrange equation
d
dt
∂L
∂q˙
−
∂L
∂q
= τ
T
,
which is the common way of deducing a system’s dynamics, we can derive the
equations
x¨ =
c
a
￾
ϕ˙
2
sin(ϕ) − ϕ¨ cos(ϕ)

+
M
ar
,
ϕ¨ =
c
b
(g sin(ϕ) − x¨ cos(ϕ)) −
M
b
,
where the state vector is given by
[q1 q˙1 q2 q˙2]
T = [x x ϕ˙ ϕ˙]
T
.
They are not only easier to determine, but are also less complex than the
Hamiltonian equations (2.110). However, the passivity of the system cannot
be immediately determined from these equations of dynamics and equation
(2.111).
2.4.10 Dissipative Hamiltonian Systems
So far we have studied lossless Hamiltonian systems such as the spring-mass
system shown in Figure 2.74 on p. 169. If we wish to consider losses as well,
such as those due to friction, we must integrate an energy-consuming, i. e.
dissipative element into our considerations.
To this end, we will introduce a dissipation term D(x) into the PCH sys￾tem (2.105) which describes the system’s internal energy losses, thus obtaining
x˙ = (J(x) − D(x)) 
∂V
∂x
T
+ B(x) · u, J(x) = −J
T
(x),
y = B
T
(x)

∂V
∂x
T
.
(2.112)
Here D(x) is a symmetric and positive semidefinite matrix. Such systems
are termed port-controlled Hamiltonian systems with dissipation (PCHD sys￾tems). In this case V is no longer a Hamiltonian function in the conventiona180 Chapter 2. Limit Cycles and Stability Criteria
sense and does not necessarily have to represent the energy of the system.
However, V (x) should be a positive definite function. With
V˙ (x) = ∂V
∂x
x˙ =
∂V
∂x
J(x)

∂V
∂x
T
| {z }
= 0
−
∂V
∂x
D(x)

∂V
∂x
T
+
∂V
∂x
B(x) · u
we obtain the passivity inequality
V˙ (x) = u
T y −
∂V
∂x
D(x)

∂V
∂x
T
≤ u
T y, (2.113)
which is obviously fulfilled because of the positive semidefiniteness of D(x).
PCHD systems are thus always passive. For this reason, the system repre￾sentation (2.112) is also referred to as the passivity canonical form [253, 419].
Since the storage function V is assumed to be positive definite, and because
V˙ (x) ≤ 0
holds for u = 0, V is a Lyapunov function of the passive system. This is also
the reason why we no longer use the letter S but the letter V for the symbol
of the storage function. The latter typically represents Lyapunov functions.
Furthermore, if the dissipation matrix D(x) is positive definite, we can
fulfill the defining inequality for strict passivity
V˙ (x) + R(x) ≤ u
T y,
for example by choosing
R(x) = 1
2
∂V
∂x
D(x)

∂V
∂x
T
.
It subsequently holds that
V˙ (x) + R(x) = u
T y −
1
2
∂V
∂x
D(x)

∂V
∂x
T
≤ u
T y.
A PCHD system with a positive definite matrix D(x) is thus strictly passive.
According to Theorem 24 on p. 148, a strictly passive system possesses an
asymptotically stable equilibrium point at x = 0. This brings us to
Theorem 40 (Stability of PCHD Systems). A PCHD system
x˙ = (J(x) − D(x)) 
∂V
∂x
T
+ B(x) · u,
y = B
T
(x)

∂V
∂x
T
is strictly passive and has an asymptotically stable equilibrium point at x = 0
if D(x) is positive definite.2.4. Passivity and Stability 181
c
d
m
z1, u
Fig. 2.76: Spring-mass system with damping element
As an illustrative example, we will again consider the spring-mass system
and add a damping term to it as an energy consuming element, as shown in
Figure 2.76. For the equilibrium of forces, it holds that
mz¨1 = −cz1 − dz˙1 + u.
The equations of motion are therefore given by
z˙1 = z2,
z˙2 = −
c
m
z1 −
d
m
z2 +
u
m
.
(2.114)
For the PCHD representation, we again set
q = z1 and p = mz˙1 = mz2.
Using the Hamiltonian function as the storage function V , i. e.
V (q, p) = Ekin + Epot =
1
2
mz˙
2
1 +
1
2
cz2
1 =
1
2
p
2
m
+
1
2
cq2
,
and x = [q p]
T
, we can represent system (2.114) as
x˙ =

q˙
p˙

=





∂V
∂p
−
∂V
∂q





−


0
d
m
p

+

0
u

=
 0 1
−1 0 
−

0 0
0 d
 
∂V
∂x
!T
+

0
1

u
and thus the PCHD representation as

q˙
p˙

=
 0 1
−1 0 
−

0 0
0 d
 

cq
p
m

 +

0
1

u (2.115)
with the output value
y =
1
m
p.182 Chapter 2. Limit Cycles and Stability Criteria
From this example, it becomes clear that, because of the additional damp￾ing term, we can no longer derive the differential equations for PCHD sys￾tems from the Hamiltonian function or the storage function by using equation
(2.95), as we can for lossless Hamiltonian systems. This especially holds for
the dissipation matrix D.
Furthermore, the PCHD representation of a system is not unique. Rather,
there are many different PCHD representations whose forms depend on the
choice of the storage function V . It is not possible to prove asymptotic stability
of a strictly passive system’s equilibrium point xeq = 0 using every PCHD
representation, since the matrix D need not be positive definite for a strictly
passive system, as the above example illustrates. It depends on the choice
of the function V whether a PCHD representation has a positive or positive
semidefinite matrix D. In Section 2.4.12 we will continue with example (2.114)
and determine a PCHD representation with a positive definite matrix D.
It is worth noting that many physical systems can be modeled as PCHD
systems (2.112). In particular, passive mechanical and electrotechnical systems
can be represented in this form. However, it is not always easy to find a PCHD
representation for a strictly passive system.
2.4.11 Example: Separately Excited Direct-Current Machine
As a further example of a PCHD system, we will consider the separately
excited direct-current machine. Its stator consists of an electromagnet, at
which the stator voltage us is applied and through which the stator current
is flows. It generates the magnetic flux linkage
Ψs = Lsis,
where Ls is the inductance of the stator winding. Figure 2.77 shows the cor￾responding circuit diagram. Since the equation
usw = Ls
˙is = Ψ˙
s
holds for the voltage usw which is induced in the stator winding, we obtain
us = usw + Rsis = Ψ˙
s +
Rs
Ls
Ψs (2.116)
for the stator voltage us as the sum of the induced voltage usw and the voltage
at the ohmic stator resistor Rs.
The voltage induced in the rotor winding can be calculated using
urw = kωΨs.
It depends bilinearly on the angular velocity ω and the flux linkage Ψs via the
machine constant k. This yields the differential equation2.4. Passivity and Stability 183
is
Rs
Ls
Stator
us
ir
Lr Rr
Rotor
M kωΨ ur
s
Fig. 2.77: Separately excited direct-current motor
ur = Rrir + Lr
˙ir + kωΨs (2.117)
for the rotor current ir, where ur is the terminal voltage, Lr is the rotor
inductance, and Rr is the rotor resistance.
The machine generates the torque
MDCM = kirΨs,
so that we obtain
Jω˙ = kirΨs − d · ω − ML (2.118)
for the balance of torques, where d · ω is the sliding-friction torque with d
being the friction coefficient, ML denotes the load torque, and J is the inertia
moment of the rotor.
With the state vector x
T = [ir Ψs ω] and equations (2.116), (2.117), and
(2.118), the state-space model
x˙ =









−
Rr
Lr
x1 −
k
Lr
x2x3
−
Rs
Ls
x2
k
J
x1x2 −
d
J
x3









+









1
Lr
0 0
0 1 0
0 0 −
1
J


















ur
us
ML









,
y =

x1
1
Ls
x2 −x3
T
(2.119)
can be derived. By adding up the energies that are stored in the inductances
Eind =
1
2
Lri
2
r +
1
2
1
Ls
Ψ
2
s
and the rotation energy184 Chapter 2. Limit Cycles and Stability Criteria
Erot =
1
2
Jω2
,
we obtain
V (x) = Eind + Erot =
1
2
Lri
2
r +
1
2
1
Ls
Ψ
2
s +
1
2
Jω2
=
1
2
Lrx
2
1 +
1
2
1
Ls
x
2
2 +
1
2
Jx2
3
as a storage function which we can use to show that the separately excited
direct-current machine is passive. For the passivity inequality, we obtain
V˙ (x) = Lrx1x˙ 1 +
1
Ls
x2x˙ 2 + Jx3x˙ 3 ≤ u
T y,
where the input variable vector u is given by
u = [ur us ML]
T
.
From this, along with equation (2.119), it follows that
V˙ (x) = −Rrx
2
1 −
Rs
L2
s
x
2
2 − dx2
3 + x1ur +
1
Ls
x2us − x3ML
≤ urx1 +
1
Ls
usx2 − MLx3,
and from the above equation, we immediately arrive at
−Rrx
2
1 −
Rs
L2
s
x
2
2 − dx2
3 ≤ 0.
Since this inequality holds for all x 6= 0, the separately excited direct-current
machine is passive. It is even strictly passive, since we can define a positive
definite function R(x) in such a way that
V˙ (x) + R(x) ≤ u
T y
holds. For example, we can choose
R(x) = 1
2

Rrx
2
1 +
Rs
L2
s
x
2
2 + dx2
3

.
The passivity of the machine can also be directly deduced from its PCHD
representation. For this purpose, we rearrange equation (2.119). This yields
x˙ = (J(x) − D(x)) 
∂V (x)
∂x
T
+ Bu, (2.120)
where the matrices are2.4. Passivity and Stability 185
J(x) =








0 0 −
k
JLr
x2
0 0 0
k
JLr
x2 0 0








, D(x) =








Rr
L2
r
0 0
0 Rs 0
0 0
d
J
2








,
B(x) =








1
Lr
0 0
0 1 0
0 0 −
1
J








.
The dissipation matrix D describes the energy loss resulting from the ma￾chine’s ohmic resistances and friction. Since this matrix is positive definite,
according to Theorem 40 the system is strictly passive and its equilibrium
point xeq = 0 is asymptotically stable.
2.4.12 Linear Hamiltonian Systems
Considering the linear systems
x˙ = Ax + Bu,
y = Cx,
(2.121)
we can ask in which case such a system can be represented as a PCH system
x˙ = J

∂V
∂x
T
+ Bu,
y = B
T

∂V
∂x
T
(2.122)
with the associated storage function V (x). As a condition for the storage
function, by comparing equation (2.121) with equation (2.122), we obtain the
identity
Cx = B
T

∂V
∂x
T
. (2.123)
This means that the positive definite storage function V must be a quadratic
form
V (x) = 1
2
x
T Rx, R = RT
, (2.124)186 Chapter 2. Limit Cycles and Stability Criteria
with a positive definite matrix R. Thus equation (2.123) is equivalent to
C = B
T R. (2.125)
Furthermore, the system given in equation (2.121) must be lossless and
consequently passive to be representable in PCH form (2.122). This means
that V˙ = u
T y must hold, i. e.
V˙ (x) = ∂V
∂x
x˙ =
1
2
x
T

AT R + RA
x + u
TB
TR x = u
Ty = u
T B
TR x.
It follows that
ATR + RA = 0, (2.126)
which is equivalent to
V˙ (x) = 0 for u = 0.
This in turn is equivalent to the fact that the storage function V is a Lyapunov
function and that all the system’s trajectories run along the ellipsoidal contour
lines of the function V . The system (2.121) is thus Lyapunov stable, but not
asymptotically stable. Consequently, the system may not have more than one
eigenvalue at λi = 0, and otherwise has only complex conjugate eigenvalues
λi = jωi and λi+1 = −jωi with ωi ∈ IR\{0}.
For us to be able to represent the system (2.121) in PCH form (2.122), it
must further hold that
Ax = J

∂V
∂x
T
= JR x,
or
J = AR−1
. (2.127)
Since J is a skew-symmetric matrix and R is symmetric, it must hold that
AR−1 = −
￾
AR−1
T
= −R−1AT
and thus, after multiplying both sides by R, it follows that
A
T R + RA = 0.
This equation is identical to equation (2.126) and is therefore fulfilled, so that
we can choose equation (2.127) for J.
Equations (2.125) and (2.126) are the Kalman-Yakubovich-Popov equa￾tions from Theorem 28 on p. 153 for the case L = 0. They are the necessary
and sufficient conditions for the system (2.121) being lossless.
Summarizing the results above, we obtai2.4. Passivity and Stability 187
Theorem 41 (Linear PCH Systems). A linear system
x˙ = Ax + Bu,
y = Cx
is lossless and representable as a PCH system
x˙ = J

∂V
∂x
T
+ Bu,
y = B
T

∂V
∂x
T
with a positive definite storage function
V (x) = 1
2
x
T Rx
and J = AR−1
if and only if a positive definite matrix R exists such that
A
T R + RA = 0,
B
T R = C
holds.
Next we will examine PCHD systems
x˙ = (J − D)

∂V
∂x
T
+ Bu,
y = B
T

∂V
∂x
T
,
(2.128)
intending to analyze which conditions allow the linear system (2.121) to be
represented as a PCHD system with the associated storage function V . We
remind ourselves that the dissipation matrix D is positive semidefinite in this
case. This property of D will be needed later.
We will assume that equations (2.123), (2.124), and (2.125) hold again. If
the system (2.121) is to be a passive PCHD system with a positive definite
storage function (2.124), the passivity inequality
V˙ (x) = 1
2
x
T

A
T R + RA
x + u
T B
T Rx ≤ u
T
y = u
T B
T Rx
must hold, i. e. the equation
AT R + RA = −Q (2.129)
with the positive semidefinite matrix Q must be fulfilled.188 Chapter 2. Limit Cycles and Stability Criteria
Furthermore, if the inequality
V˙ (x) + R(x) ≤ u
T y,
i. e.
1
2
x
T

A
T R + RA
x + R(x) ≤ 0, (2.130)
is fulfilled with R(x) being a positive definite function, it follows that the
system (2.121) is strictly passive. Since R(x) is positive definite, the matrix
AT R + RA
must be negative definite in this case, so that equation (2.130) can be fulfilled.
Since we are free to choose R(x) as we wish, we will set
R(x) = −
1
4
x
T

AT R + RA
x =
1
4
x
T Qx > 0 for x 6= 0,
which for equation (2.130) yields
1
4
x
T

A
T R + RA
x ≤ 0.
This inequality is obviously fulfilled. Thus if the matrix Q from equation
(2.129) is positive definite, it follows that the system (2.121) is strictly passive.
According to Theorem 16 on p. 121, the storage function V (x) is thus a strict
Lyapunov function of the system (2.121) and the system is, in contrast to
PCH systems, asymptotically stable.
With the storage function (2.124) the differential equation of system
(2.128) takes the form
x˙ = (J − D) Rx + Bu.
Comparing this equation to (2.121) yields
A = (J − D) R. (2.131)
Similar to the previous case involving PCH systems, we will utilize the fact
that
J = AR−1 + D
is skew-symmetric. It must therefore hold that
AR−1 + D = −
￾
AR−1 + D
T
.
It follows tha2.4. Passivity and Stability 189
AR−1 + R−1AT = −2D.
After multiplying both sides of this equation by R and using the matrix
Q = 2RDR,
we finally arrive at the Lyapunov equation
AT R + RA = −Q. (2.132)
Since R is positive definite and D is positive semidefinite due to the system’s
passivity, it follows that Q is a positive semidefinite matrix. Equation (2.132)
is identical to equation (2.129), which is fulfilled because of the system’s pas￾sivity.
According to Theorem 16 on p. 121, the Lyapunov equation (2.129) with
the positive semidefinite matrix Q is only solvable if the matrix A does not
possess any eigenvalues with a positive real part, i. e. the system (2.121) must
be Lyapunov stable or asymptotically stable.
How do we obtain the matrices J and D? To do this, we will use equation
(2.131) and rearrange it to
J − D = AR−1
.
The matrix J is skew-symmetric and D is symmetric. We will take advantage
of this, since every matrix M can be decomposed into a skew-symmetric first
part and a symmetric second part using
M =
1
2

M − MT

+
1
2

M + MT

.
For the skew-symmetric part, we thus obtain
J =
1
2

AR−1 − R
−1A
T

and for the symmetric positive semidefinite part, we obtain
D = −
1
2

AR−1 + R−1AT

.
With these matrices, the system
x˙ = Ax + Bu,
y = Cx
(2.133)
can be represented in PCHD form (2.128) under the condition that equation
(2.125) and equation (2.132) hold. Again, these two equations are the condi￾tions of the Kalman-Yakubovich-Popov lemma, i. e. Theorem 28 on p. 153, or
Theorem 29 on p. 154. Since this is sufficient and necessary for passivity, we
can formulate190 Chapter 2. Limit Cycles and Stability Criteria
Theorem 42 (Linear PCHD Systems). A linear system
x˙ = Ax + Bu,
y = Cx
is (strictly) passive and can be represented as a PCHD system
x˙ = (J − D)

∂V
∂x
T
+ Bu,
y = B
T

∂V
∂x
T
with a positive definite storage function
V (x) = 1
2
x
T Rx (2.134)
and
J =
1
2

AR−1 − R−1AT

,
D = −
1
2

AR−1 + R−1AT

if and only if a positive definite matrix R and a positive semidefinite (positive
definite) matrix Q exist such that
A
T R + RA = −Q, (2.135)
B
T R = C
holds.
Similar to the case of PCH systems, it follows that a linear system (2.133)
can be represented as a PCHD system if and only if it is passive.
The theorem above once again highlights why the system representation
of PCHD systems is referred to as the passivity canonical form. This results
from the fact that passive linear systems can always be written in the passivity
canonical form, i. e. as PCHD systems. As already mentioned, it may well be
that there is a strictly passive system for which the storage function (2.134)
only leads to a positive semidefinite matrix Q in equation (2.135). However,
choosing a different appropriate storage function, or more precisely, a matrix
R, we can prove strict passivity.
For illustrative purposes, we will again consider the damped spring-mass
system (2.114), or (2.115) with
c = 1 N m−1
,
d = 1 kg s−1
,
m = 0.5 kg,2.4. Passivity and Stability 191
i. e.
z˙ =

0 1
−2 −2

| {z }
A
z +

0
2

|{z}
b
u. (2.136)
Accordingly, a PCHD system representation is given by
z˙ =
  0 1
−1 0 
| {z }
J
−

0 0
0 2 
| {z }
D
 ∂V
∂z
T
+

0
2

|{z}
b
u,
y = b
T R1z
(2.137)
with
V (z) = 1
2
z
T

2 0
0 1 
| {z }
R1
z. (2.138)
This PCHD representation differs from the one given in equation (2.115),
since we are not using the Hamiltonian coordinates
q = z1
and
p = mz2,
but the original coordinates z1 and z2. In this way, we do not have to perform a
change of coordinates, and we can use the same state vector z in both system
representations (2.136) and (2.137). The storage function (2.138), which is
also a Lyapunov function, leads – as already shown in equation (2.113) – to
the passivity inequality
V˙ (z) = uy −
∂V
∂z
D

∂V
∂z
T
≤ uy, (2.139)
i. e. to
−
∂V
∂z
D

∂V
∂z
T
≤ 0
with the positive semidefinite matrix D. The matrix
A
T R1 + R1A = −

0 0
0 4 
is also only negative semidefinite. Since D is only positive semidefinite and
not positive definite, in equation (2.139) the equality is fulfilled for some192 Chapter 2. Limit Cycles and Stability Criteria
specific cases. So with equation (2.139), we have only provided evidence for
the passivity of the system.
If, on the other hand, we choose
V (z) = 1
2
z
T

2 1
1 1 
| {z }
R2
z, (2.140)
as the storage function, we arrive at the PCHD representation
z˙ =
  0 1
−1 0 
| {z }
J
−

1 −1
−1 2 
| {z }
D
 ∂V
∂z
T
+

0
2

|{z}
b
u,
y = b
T R2z.
(2.141)
For strict passivity, the inequality
V˙ (z) + R(z) = −
∂V
∂z
D

∂V
∂z
T
+ R(z) + uy ≤ uy, (2.142)
must be satisfied, where R(z) must be a positive definite function. With the
derivative
∂V
∂z
= R2z,
equation (2.142) is equivalent to
−z
T R2DR2z + R(z) ≤ 0. (2.143)
Since the matrix D and thus the matrix R2DR2 are positive definite, we can
choose
R(z) = 1
2
z
T R2DR2z
as a positive definite function which fulfills equation (2.142) or, equivalently,
equation (2.143). Furthermore, it is also equivalent to the result given in equa￾tion (2.143), that the matrix
A
T R2 + R2A = −

4 2
2 2 
is negative definite. We have thus proven the strict passivity of the system
(2.141) with the associated storage function (2.140). At this point, we note
that different PCHD models of a system have different output variables and
that the output is irrelevant to stability.
In conclusion, we will note that there is an infinite number of PCHD
representations of the same system. Whether only the passivity of the system
can be proven based on the storage function V , or whether the strict passivity
can be proven as well, depends on whether the dissipation matrix D is positive
semidefinite or positive definite.2.5. Exercises 193
2.5 Exercises
Exercise 2.1 Calculate the describing functions for the four functions in Fig￾ure 2.78.
-a
u
e
b
-b
a
(a)
-a
u
e
m
a
(b)
-a
u
a e
m1
m2
(c) u
e
b
3b
a
2
5a
2
(d)
Fig. 2.78: Nonlinear functions; slopes are denoted by m, m1, and m2.
Exercise 2.2 Calculate the describing functions for the following two func￾tions:
(a) f(e) = k1e + k2e
3 + k3e
5
, (b) f(e) = Xn
i=1
kie
2i−1
.
Exercise 2.3 As in Exercises 1.6 and 1.7, we will now view Chua’s circuit,
here in the normalized form already calculated in Exercise 1.6 on p. 60 and
analyzed in Exercise 1.7 on p. 61:
x˙ =




−
44
5
44
5
0
1 −1 1
0 −15 0




x +


1
0
0

(−g(x1)), g(x1)= 1
7



−5x1− 3, x1 ≥ 1,
−8x1, |x1| < 1,
−5x1+ 3, x1 ≤ −1,
with the output variable y = x1.
(a) Determine the transfer function G(s) = Y (s)/U(s) with u = −g(x1).
(b) Represent Chua’s circuit as a nonlinear standard control loop.
(c) Determine the describing function of −g(x1). Note that g(−x1) = −g(x1).
(d) Demonstrate that the harmonic balance does not give any indication of a
limit cycle.194 Chapter 2. Limit Cycles and Stability Criteria
Exercise 2.4 Let us examine the nonlinear standard control loop shown in
Figure 2.79.
u
e
e
a
yref = 0 b u
G(s)
y
Fig. 2.79: Nonlinear standard control loop with a three-position element
The transfer function of the plant is
G =
4
s(s
2 + 4s + 4).
Using the describing function method, calculate how the parameters a and
b need to be adjusted so that the control loop does not go into a sustained
oscillation.
Exercise 2.5 Let us examine the control loop shown in Figure 2.80.
yref = 0
u
e
a
c
e
b
G(s)
u y
Fig. 2.80: Nonlinear standard control loop with a limited dead zone
(a) Calculate and sketch the describing function of the nonlinear element.
Let us now examine the special case a = 1, b = 3, c = 2.
(b) Assume the plant to be
G(s) = K
0.25s
3 + 0.375s
2 + s + 1
.
Approximately calculate the amplitudes of the two limit circles that can
occur and determine their frequency for K = 1.
(c) For which value K = Ks do we obtain a semi-stable limit circle? How
would you describe its stability behavior for varying input amplitudes A?
(d) What types of stability behavior do the limit cycles have for values K
which are smaller or greater than Ks?2.5. Exercises 195
Exercise 2.6 Now we will consider the blood-pressure regulation system in
humans and other mammals [91, 287, 384]. The blood pressure y is essentially
determined by the heart’s pumping force, which varies according to the pa￾tient’s physical and emotional state. To control it, the brain stem, or more
accurately, the medulla oblongata, generates a reference value of the blood
pressure yref. This is compared to the actual blood pressure y, which – as
shown in Figure 2.81 – is measured by so-called baroreceptors within the
artery walls, or more accurately in the aortic arch and in the carotid artery.
Baroreceptors are pressure-sensitive neurons whose readings are electri￾cally transmitted to the medulla oblongata via their axons. Here, the control
error yref − y is calculated and the control signal generated. The latter is
transmitted via the nervous system to the sinoatrial node of the heart. The
sinoatrial node in turn triggers the contraction, i.e. the pumping process, by
means of electrical signals. Figure 2.82 shows the simplified control loop in
block diagram form. It must be noted that the transmission of signals along
the nerve fibers takes place with a delay such that a time lag τ a exists. In ad￾dition, the neuronal activity is subjected to a sigmoidal saturation, as shown
in Figure 2.83, and which we will model here as
u˜ = f(˜e) = b · arctan(m(˜e − e˜c)) + ˜uc.
We will use the parameters b = 57.89, e˜c = 121, u˜c = 139, and m = 0.06 of a
patient with high blood pressure. The vascular system can be modeled using
a first-order transfer function with a time delay component
G(s) = e
−τes
1 + τν s
, τe = 2.98 s, τν = 5 s.
The measurement and the signal transmission – as mentioned above – cause
an additional time delay of
H(s) = e
−τas
, τa = 0.01 s.
The regulation can be approximated by a P controller with the amplification
factor k = 1.65.
(a) Convert the control loop shown in Figure 2.82 into a nonlinear standard
control loop with a point-symmetric sigmoidal curve u = f(e) which passes
through zero.
(b) Prove that the describing function N(A) of the sigmoidal saturation curve
u = b arctan(me) can be stated as
N(A) = 2b
mA2
p
m2A2 + 1 − 1

.
In doing so, use the definite integral
Zπ
0
1
1 + m2A2 sin2
(α)
dα =
π
√
m2A2 + 1
.196 Chapter 2. Limit Cycles and Stability Criteria
Medulla
oblongata Electrical signal
Deoxygenated blood
from upper body
Baroreceptors
Oxygenated
blood to upper body
Deoxygenated
blood to lungs
Oxygenated
blood from lungs
Oxygenated
blood to lower body
Deoxygenated
blood from lower body
Deoxygenated
blood to lungs
Sinoatrial
node
Fig. 2.81: The circulatory system of humans and mammals and its control2.5. Exercises 197
k
yref e˜
f(˜e)
u˜
G(s)
y
H(s)
Fig. 2.82: Simplified blood-pressure control loop
Neuronal activity
Blood pressure in mHg
60
100
140
180
220
50 100 150 200 250
Fig. 2.83: Neuronal activity, given as a dimensionless measure
(c) Determine whether a limit cycle can exist in this case, and if so, what its
frequency is.
(d) What statement can you make about the stability behavior of the limit
cycle?
Exercise 2.7 Now we will examine a nonlinear standard control loop with
the plant
G(s) = 0.1111
s
4 + 0.5s
3 + 10.06s
2 + 2.1s + 9
and an unspecified nonlinear element. Figure 2.84 shows the Nyquist plot
(dashed line) and the Popov plot (solid line).
(a) Using the Popov criterion, identify the maximum possible sector [0, KPo)
of absolute stability which can be determined using this criterion.
(b) In which sector can a linear curve lie so that asymptotic stability is en￾sured? What controller do we obtain in this case?
(c) Let us examine the nonlinearity
u = e
3
.
What maximum control errors e are permissible if absolute stability is to
be ensured?198 Chapter 2. Limit Cycles and Stability Criteria
Re
Im
0
-0.05
0.05
-0.04 -0.02 0 0.02 0.04
Fig. 2.84: Nyquist plot (dashed line) and Popov plot (solid line)
Exercise 2.8 For optimal functioning of a catalytic converter meant to reduce
the emissions from a combustion engine, the fuel must be entirely burnt;
otherwise carbon monoxide will result. Thus, oxygen sufficient for combustion
must be present. The amount of air necessary to ensure this without leaving
any oxygen behind is called the stoichiometric amount of air mst. A measure
of the completeness of the combustion is the ratio
λ =
mair
mst
between the amount of air mair available and the stoichiometric amount.
Where stoichiometric combustion occurs, the ratio is λ = 1. To ensure this,
a control system is used, the principal elements of which are shown in Figure
2.85.
The plant consists of a fuel injector with the duration v of the fuel injection
as the control signal, as well as the combustion process, which is characterized
by a time delay and a linear differential equation of the first order. Normally,
a PI controller is used for regulation. Figure 2.86 shows this control loop.
Because the oxygen sensor already includes the minus sign of the feedback, the
measured value is added to the reference value, instead of being subtracted
from it as is usual in a control loop. In our example, we have Te = 0.2 s,
Td = 0.5 s, and Ke = 25. In the control loop, all variables are shown as
deviations from the operating point λ = 1, so that the curve of the sensor
for this value passes through the origin and the reference value of the control
loop is zero. Within the linear region, the slope of the sensor’s curve u(λ) has
a minimum value of m = du/dλ = −10. The sensor curve is shown in Figure
2.87.2.5. Exercises 199
Controller
Throttle
Fuel injector
Crankshaft
Oxygen sensor
Catalytic converter
Fig. 2.85: Combustion engine
PI controller Engine
KP
1+T Is
T Is
Ke
1+T es
e
−T ds
∆u
v λ
λ
e
Fig. 2.86: Control loop
0.8 0.9 1 1.1 1.2 λ
1V
0.5V
u
Fig. 2.87: Curve of an oxygen sensor with output voltage u depending on λ200 Chapter 2. Limit Cycles and Stability Criteria
(a) Configure the PI controller KPI = KP(1+T Is)/(T Is)) such that the linear
part of the open control loop has the transfer function ce−T ds
/s.
(b) Determine the value c and thus also the gain KP of the PI controller such
that the control loop is absolutely stable in the sector [ε, 12) where ε > 0.
Exercise 2.9 Let us consider
G(s) = 1
(s + 1)(s + 2)(s + 3).
(a) Determine the largest sector of absolute stability that can be obtained
using the circle criterion.
(b) Determine the Popov sector.
(c) Determine the largest possible sector of absolute stability.
Exercise 2.10 Consider the plant
G(s) = 30
(s − 1)(s + 2)(s + 3).
(a) Sketch the Nyquist plot of G(jw) for ω ∈ [−∞, ∞]. To do this, use the
points G(j0), G(±j∞), and the points for which Im {G(jω)} = 0 applies.
In addition, use the points for which the imaginary part assumes its max￾ima and/or minima. Determine these extreme values approximately by
trying out various values ω.
(b) If we apply the circle criterion, where should the midpoint of the circle lie
in order to prove that a sector of absolute stability exists?
(c) Determine the midpoint c and radius r of the largest possible circle. How
large is the sector for which we can prove absolute stability?
(d) How large is the Hurwitz sector?
Exercise 2.11 Consider the plant
G(s) = s + 1
s
2 + 1
.
(a) Determine the continuous angle variation Ω of G(jω) in relation to a point
p on the negative real axis of the complex plane.
(b) Which sector of absolute stability can you determine using the circle cri￾terion if the midpoint of the circle is the point −1 on the real axis?
(c) Identify the Hurwitz sector.
Exercise 2.12 Let us examine the three plants
G1(s) = s + 1
s
2
, G2(s) = 2s
s
2 − 2s + 4
, and G3(s) = s
2 − 1
s
3
.
(a) Sketch the Nyquist plots. Use the circle criterion. Which sector of absolute
stability can you determine in each case?2.5. Exercises 201
(b) Determine the Hurwitz sector for each case.
Exercise 2.13 Let us examine the system
x˙ 1 = x2,
x˙ 2 = −2x1 − 2x2 − 4x
3
1
and the Lyapunov function
V (x) = 4x
2
1 + 4x
4
1 + 2x
2
2
.
Show that the equilibrium point xeq = 0 is Lyapunov stable.
Exercise 2.14 We will examine the system
x˙ 1 = x2 + x1
￾
x
2
1 + x
2
2

,
x˙ 2 = −x1 + x2
￾
x
2
1 + x
2
2

.
(a) Determine the equilibrium points of the system.
(b) Using the function V (x) = x
2
1+x
2
2
, analyze the stability of the equilibrium
point or points.
Exercise 2.15 Given the system
x¨ + dx˙
3 + kx = 0
with d, k > 0, demonstrate that
V (x) = 1
2
(kx2 + ˙x
2
)
is a Lyapunov function of the system’s equilibrium point. What stability prop￾erty can you prove using the function V (x)?
Exercise 2.16 We will now analyze the system
x˙ 1 = −x1 − x2,
x˙ 2 = x1 + (x
2
1 − 1)x2.
(a) Using Lyapunov’s indirect method, show that the origin is asymptotically
stable.
(b) Show that V (x) = x
T x is a Lyapunov function of the linearized system.
(c) What is the maximum catchment region we can calculate using the Lya￾punov function V (x) = x
T x?
Exercise 2.17 Let us examine the system
x˙ 1 = x2,
x2 = −x202 Chapter 2. Limit Cycles and Stability Criteria
(a) Show that the function
V (x) = 0.1(x
2
1 + x
2
2
)

2 + cos q
x
2
1 + x
2
2

is a Lyapunov function for the equilibrium point xeq = 0 and that this
equilibrium point is Lyapunov stable.
(b) What form do the level curves of the function V (x) take?
(c) Sketch the function V (x) for x1 ∈ [−50, 50] and x2 = 0.
(d) What shape do the trajectories x(t) of the system take, and what is a
system of this kind called?
Exercise 2.18 Consider the control loop shown in Figure 2.88 with the pa￾rameters
A =

−1 6
−1 −2

, b =

1
1

, k
T =

3 21
,
and the nonlinear controller u = e + e
3
.
yref = 0 e
e
u
u x
u = e + e
3
x˙ = Ax + bu
k
T
x
Fig. 2.88: Nonlinear control loop
(a) Formulate the differential equations of the closed control loop.
(b) Determine the equilibrium point xeq of the control loop.
(c) Using Lyapunov’s direct method, show that the equilibrium point of the
control loop is globally asymptotically stable. To do this, use the Lyapunov
function
V (x) = (x − xeq)
T
(x − xeq).
Exercise 2.19 Consider the system
x˙ 1 = (1 − x
2
1 − x
2
2 − x
2
3
)x1,
x˙ 2 = (1 − x
2
1 − x
2
2 − x
2
3
)x2,
x˙ 3 = (1 − x
2
1 − x
2
2 − x
2
3
)x3.
(a) Determine the system’s equilibrium points.
(b) Analyze the stability behavior of the equilibrium points.2.5. Exercises 203
Exercise 2.20 Let us examine the stability of the linear system
x˙ =
 
A0 +
X
k
i=1
piAi
!
x
with parameters pi which are either unknown or change only very slowly over
time. We will assume that the parameters pi are limited by the hyperrectangle
P = {pi ∈ IR | pi ∈ [pi,min, pi,max], i = 1, ..., k}.
Prove that the system for all p = [p1 p2 ... pk]
T ∈ P has a globally asymp￾totically stable equilibrium point at x = 0 with a Lyapunov function V (x) =
x
T Rx if
V˙ (x) < 0
applies at all 2
k
vertices pv,i of the hyperrectangle P.
Exercise 2.21 Let us examine the passivity of the satellite model (1.8) from
Section 1.1.4 on p. 8.
(a) Choose a suitable storage function S(ω), and show that the model is
passive and lossless where the input variable vector u = M and the
output variable vector y = ω.
(b) What property must the matrix K have in the control law u = −Kω
such that the closed control loop has a globally asymptotically stable
equilibrium point at ω = 0?
(c) Represent the satellite model as a PCH system. For this purpose, select
suitable new coordinates.
(d) Now choose a control law with the moment vector M as the control vari￾able vector such that a PCHD system with a globally asymptotically stable
equilibrium point at the origin results.
Exercise 2.22 Is a PID controller passive, strictly passive, or neither?
Exercise 2.23 If a system
x˙ = Ax + bu, y = c
T x
fulfills the Kalman-Yakubovich-Popov equations
AT R + RA = −Q, c
T = b
T R
(here Q is positive definite), then it is strictly passive. Using the Lyapunov
function V (x) = x
T Rx, show that in this case it is absolutely stable in the
sector [0, ∞].
Exercise 2.24 Consider the system
G(s) = s + 0.5
s
2 + s + 1
. (2.144)204 Chapter 2. Limit Cycles and Stability Criteria
(a) Is the system strictly positive real, only positive real, or neither?
(b) Is the system strictly passive, only passive, or lossless?
(c) For the general case, show that the nonlinear standard control loop is
strictly passive if the plant is strictly passive and the nonlinearity is pas￾sive. Which nonlinearities are possible in this case if we wish the control
loop to have a globally asymptotic equilibrium point?
(d) How large is the largest sector of absolute stability in the case of system
(2.144)?
Exercise 2.25 State the condition for the series connection of a system with
a strictly positive real transfer function G(s) and a PI controller
H(s) = P +
I
s
to be a passive system.
Exercise 2.26 Consider a mathematical pendulum, i.e. an ideal pendulum
with a massless rod of length l, point mass m, and no friction losses. It is
shown in Figure 2.89.
g
ϕ
m
l
Fig. 2.89: Pendulum
(a) Determine the Lagrangian function L.
(b) Formulate the Lagrangian equations and use them to calculate the equa￾tions of motion for the pendulum.
(c) Determine the Hamiltonian function.
(d) Determine the Hamiltonian equations.3
Controllability and Flatness
3.1 Controllability
3.1.1 Definition of Controllability
The objective of feedforward or feedback control is to influence a plant such
that a specified state xe is reached or – stated in a more far-reaching way
– a specified trajectory xref(t) is followed. To ensure that at least the first
objective is fulfilled, the plant
x˙ = f(x, u)
must be controllable. That means it must be possible to transform an arbitrary
state x0 to any other state xe via a suitable course of the actuating variable
u(t). More precisely, we define the term controllability by
Definition 27 (Controllability). Let a system
x˙ = f(x,u)
be defined for x ∈ Dx,def ⊆ IRn
and u ∈ Du,def ⊆ IRm. Let Dx ⊆ Dx,def be
a path-connected set and Du ⊆ Du,def. Then the system is called controllable
if any state x0 ∈ Dx can be transferred to any state xe ∈ Dx by an input
function u with function values u(t) ∈ Du along a trajectory x(t) ∈ Dx
within a finite time interval [0, T ].
Obviously, the controllability of a system is a fundamental requirement
for the realization and proper functioning of a feedforward or feedback con￾trol system. This is intuitively clear when thinking of an airplane that the
pilot or autopilot cannot maneuver into the desired flight altitude. For tech￾nical systems, the controllability is usually given, since the designer certainly
regards it as a major feature of the system and consequently considers it dur￾ing the design process. Therefore, if we find out that a technical system is not
© Springer-Verlag GmbH Germany, part of Springer Nature 2024
J. Adamy, Nonlinear Systems and Controls,
https://doi.org/10.1007/978-3-662-68690-4_3
205206 Chapter 3. Controllability and Flatness
controllable, the question arises whether the modeling has been performed
correctly.
An example of an uncontrollable system is
x˙ = u
2
,
since no initial value x0 can be transferred to a state xe if xe < x0, independent
of the course of the input function u(t).
The controllability of a system implies that we can reach any state from
any other state. However, it does not necessarily imply that we can do this
via any trajectory x(t). As a matter of fact, trajectories may exist which
a controllable system cannot execute regardless of the choice of the input
function u(t). To illustrate this circumstance, we will consider the system
x˙ 1 = u,
x˙ 2 = x1.
(3.1)
The direction [ ˙x1 x˙ 2]
T
in which a trajectory can move is limited by x˙ 2 = x1.
For positive values of x1, the state variable x2 always increases, whereas for
negative values, it always decreases. Figure 3.1 illustrates the possible direc￾tions of the trajectories. The figure also shows a realizable and an unrealizable
trajectory.
However, the above system is controllable. We can show this using the
controllability criterion [152, 98] for linear systems. A linear system
x˙ = Ax + Bu
is controllable if and only if the n × mn controllability matrix
Mcontr =

B AB A2B . . . An−1B

(3.2)
is of rank n. This is the case here, since with
A =

0 0
1 0 
and B =

1
0

it holds that
rank(Mcontr) = rank(
1 0
0 1 
) = 2.
The example of system (3.1) also illustrates that, even though controlla￾bility implies that any point x can be reached, it does not necessarily mean
that we can remain at this point. Here, such points are [x1 6= 0 x2] since it
holds that x˙ 2 = x1 6= 0.
Next we will discuss a stricter definition of controllability which gives us
new possibilities of influencing the system. First, let us consider the set of all
points xe = x(t) which the trajectories x(t) of a system reach for all times t
with 0 ≤ t ≤ T and all possible u(t) beginning at x0 = x(0). This set, which3.1. Controllability 207
x0
x0
x0
x0
xe
xe
x1
x2
x˙ 2 =x1
x˙ 1 =−u x˙ 1 =u
Unrealizable
trajectory
Realizable
trajectory
Fig. 3.1: Directions of realizable and unrealizable trajectories of the system (3.1)
we call the reachable set, is denoted by R(x0, T ). In short, R(x0, T ) is the set
of all states that can be reached from x0 within a finite time T or a shorter
period of time. The above definition of the set R(x0, T ) also implies that all
trajectories x(t) with 0 ≤ t ≤ T take their course within R(x0, T ). Using
R(x0, T ) we can define the term small-time local controllability (STLC):
Definition 28 (Small-Time Local Controllability). Let a system
x˙ = f(x,u)
be defined for x ∈ Dx,def ⊆ IRn
and u ∈ Du,def ⊂ IRm. Let Dx ⊆ Dx,def be
an open set and Du ⊆ Du,def a closed and bounded set. Then the system is
called small-time locally controllable if for every state x0 ∈ Dx, for u(t) ∈ Du
where 0 < t ≤ T , and for all 0 < T < ∞ the sets R(x0, T ) are neighborhoods
of x0.
Obviously, as stated above, Dx must be an open set if the system is to be
small-time locally controllable on this set, since the boundary points xb of a
closed set Dx do not possess a neighborhood R(xb, T ) ⊂ Dx.
According to the above definition, every set R(x0, T ) is a neighborhood
of x0. Small-time local controllability thus means that we can reach every
point xe located within an arbitrarily small neighborhood R(x0, T ) on a tra￾jectory whose course, beginning from x0, lies entirely within this neighbor￾hood R(x0, T ) in small time. This, however, does not imply that trajectories208 Chapter 3. Controllability and Flatness
beginning at x0 can aim in any arbitrary direction. Thus, small-time local
controllability does not mean that every trajectory is realizable.
The term local here must be interpreted such that a system can reach every
state in a neighborhood of any arbitrary other state on a trajectory which lies
within this neighborhood, i. e. a trajectory which is local. It does not refer to
the common distinction between local and global properties that is made in
control theory, e. g. local stability and global stability of an equilibrium point.
If a system is controllable but not small-time locally controllable, R(x0, T )
is not a neighborhood of x0, since in such a case x0 can also lie on the bound￾ary of R(x0, T ). This is exactly the case that was exemplified in equation
(3.1). Figures 3.2 and 3.3 illustrate the different situations for the cases in
which controllability and small-time local controllability exist. The blue high￾lighted areas mark the directions x˙ in which a trajectory x(t) can progress
starting at x0 for an appropriate choice of u(t). These blue areas of starting
directions change along the trajectory x(t) and are specific for each point of
the trajectory.
From the above consideration, we obtain
Theorem 43 (Controllability and Small-Time Local Controllabil￾ity). A system that is small-time locally controllable on a path-connected set
Dx is controllable on this set.
The requirement that Dx is a path-connected set is essential and usually
fulfilled. Small-time local controllability – in contrast to controllability – can
hold for a set that is not path-connected, since in this case it is not required
that any point xe ∈ Dx is reachable from any other point x0 ∈ Dx. It is pre￾cisely the latter that is an essential characteristic of controllability. The most
important difference between controllable and small-time locally controllable
systems is that there might be a long detour to arrive at xe from x0 for a
system that is only controllable. For a small-time locally controllable system,
R(x0, T ) x0
Fig. 3.2: A set R(x0, T ) of a controllable
system which can be reached within the
time T starting from the point x0. Here,
R(x0, T ) is not a neighborhood of x0.
R(x0, T )
x0
Fig. 3.3: A set R(x0, T ) where small￾time local controllability exists. The tra￾jectories, in general, cannot progress
from the point x0 in all directions.3.1. Controllability 209
on the other hand, this route is very short and lies within a neighborhood
close to x0 and xe.
For some systems, the controlling possibilities allow the realization of any
arbitrary trajectory x(t). For this, the trajectories must be able to progress
from x in all directions, i. e. the vector x˙ representing the direction of motion
must be omnidirectionally controllable by the actuating variable u. Accord￾ingly, we can now introduce the term omnidirectional controllability, which is
a stricter form of controllability than the two controllability definitions given
previously.
Definition 29 (Omnidirectional Controllability). Let a system
x˙ = f(x,u)
be defined for x ∈ Dx,def ⊆ IRn
and u ∈ Du,def ⊆ IRm. Let Dx ⊆ Dx,def be a
path-connected set and Du ⊆ Du,def. Then the system is called omnidirection￾ally controllable if any state x0 ∈ Dx can be transferred to any state xe ∈ Dx
by an input function u with function values u(t) ∈ Du along any arbitrary
defined trajectory x(t) ∈ Dx within a finite time interval [0, T < ∞].
A weak form of omnidirectional controllability holds if an arbitrarily de￾fined progression of a trajectory xref(τ) can be followed exactly, but only at a
slower velocity. This means that for the realizable progressions of trajectories
x(t), the relation
x(t) = xref(cτ) with 0 < c < 1
holds. This may occur if the temporal change x˙ of the trajectories x(t) is
limited, i. e. |x˙| < a, a > 0. For example, bounded input variables u cause
this.
For the case of omnidirectional controllability, the reachable set R(x0, T )
is also a neighborhood of the state x0, as was the case for the small-time
local controllability. In contrast to the latter, as illustrated in Figure 3.4, a
trajectory that starts at x0 can progress in every direction. Accordingly, any
arbitrary trajectory can be realized. From an engineering perspective, omnidi￾rectional controllability is thus a more valuable property than controllability
or small-time local controllability.
For linear systems
x˙ = Ax + Bu,
omnidirectional controllability is given if and only if the control matrix B
has rank n. This means that an equal number of input and state variables
exist. In this case only, the state vector’s temporal derivative x˙ and thus a
trajectory x(t) can be defined arbitrarily via Bu. In practice, however, linear
systems only rarely possess n input variables and are therefore only rarely
omnidirectionally controllable. This is different for nonlinear systems. Their210 Chapter 3. Controllability and Flatness
R(x0, T )
x0
Fig. 3.4: The set R(x0, T ) in the case
of omnidirectional controllability. The
trajectories can progress starting from
x0 in all directions.
Controllable
Small-time locally
controllable
Omnidirectionally
controllable
Fig. 3.5: The sets of controllable, small￾time locally, and omnidirectionally con￾trollable systems in case of path￾connected open sets Dx
complex dynamics are often accompanied by a larger number of input variables
when compared to the linear case. These are introduced by the designer to
allow the system to be more easily influenced, meaning that nonlinear systems
which are omnidirectionally controllable are more common.
If we compare Definitions 28 and 29, we immediately obtain
Theorem 44 (Small-Time Local and Omnidirectional Controllabil￾ity). A system that is omnidirectionally controllable on an open set Dx is also
small-time locally controllable on this set.
Figure 3.5 illustrates the relationship between the different kinds of con￾trollability, assuming that Dx is a path-connected open set. If Dx is not path￾connected, the set of small-time locally controllable systems is not a subset
of the controllable systems. This is because it is possible for a system to be
small-time locally controllable on a set that is not path-connected. However, it
is never controllable on a set of this kind, since not every point can be reached
from every other point. It is worth noting that this problem disappears if we
consider only path-connected sets. The latter should be the case in almost all
real-world applications.
Figure 3.6 shows an example of an omnidirectionally controllable system,
a ship with transverse thrusters in the bow and stern. Transverse thrusters are
tunnels that are mounted perpendicular to the direction of travel and contain
an internal propeller drive. Using the transverse thrusters, the ship can be
steered transversely to the normal direction of travel. The simultaneous usage
of the ship’s regular propulsion and transverse thrusters enables it to move
in any arbitrary direction, as illustrated in Figure 3.7. Furthermore, the ship
can be rotated on the spot, i. e. it can change its orientation. Examples are
drilling ships which must be exactly positioned over a drill hole, cruise ships,
large container ships, and ferries which must maneuver in harbors with an
accuracy of about one meter.3.1. Controllability 211
Fig. 3.6: Ship with transverse thrusters in the bow and stern, shown here in a service
ship which provides maintenance for submarine boreholes
Fig. 3.7: The ship’s transverse and rotational movement functions212 Chapter 3. Controllability and Flatness
x0 x0 x0
xe xe xe
Controllable Small-time locally
controllable
Omnidirectionally
controllable
Fig. 3.8: The shortest trajectories from x0 to xe for all three kinds of controllability
The shortest trajectories from one point x0 to a different point xe for the
cases of controllability, small-time local controllability, and omnidirectional
controllability are illustrated in Figure 3.8. This figure also illustrates the
meaning of the different kinds of controllability; for example, where a system
is to be controlled such that it follows a predefined trajectory. For controllable
systems, this is only possible with restrictions since their trajectories can
only move in specific directions. An improvement is provided by small-time
locally controllable systems which, inside a small neighborhood of a point
x(t), can change direction x˙ within a wider range than controllable systems
normally can. However, only omnidirectionally controllable systems allow any
predefined trajectory to be followed exactly.
An example of the abovementioned restrictions would be a motor vehicle
without a reverse gear, with position and orientation as state variables, such
as a motorbike. It is merely controllable, but not small-time locally control￾lable. This is why motorbikes cannot be turned around in a small space by
shunting. In contrast to this, motor vehicles with a reverse gear and tracked
vehicles are small-time locally controllable. However, neither is omnidirection￾ally controllable, since they cannot be moved orthogonally to their direction
of travel, in contrast to the ship discussed above.
In the next section, we will distinguish between locally and globally con￾trollable systems.
3.1.2 Global and Local Controllability
If they are controllable, linear systems are always controllable in the entire
state space IRn
. Therefore, the controllability is – similar to the stability
of the equilibrium point xeq = 0 – a global system characteristic of linear
systems. For nonlinear systems, this is not always the case. Nonlinear systems
exist which are only controllable on subsets Dx of their domain of definition
Dx,def, i. e. they are only locally controllable. Furthermore, there are also
nonlinear systems whose controllability characteristic varies, e. g. a system
which is small-time locally controllable on one subset and omnidirectionally3.1. Controllability 213
controllable on another. Consequently, we can define the terms global and local
controllability.
Definition 30 (Global Controllability). Let a system
x˙ = f(x,u)
be defined for x ∈ Dx,def ⊆ IRn
and for u ∈ Du,def ⊆ IRm. If the system
is (small-time locally or omnidirectionally) controllable on its domain of def￾inition Dx,def by means of u ∈ Du ⊆ Du,def, the system is called globally
(small-time locally or omnidirectionally) controllable by Du.
In this context, we recall that the set on which a system is controllable or
omnidirectional controllable must be path-connected.
Definition 31 (Local Controllability). Let a system
x˙ = f(x,u)
be defined for x ∈ Dx,def ⊆ IRn
and for u ∈ Du,def ⊆ IRm. If the system is
only (small-time locally or omnidirectionally) controllable on a proper subset
Dx of the system’s domain of definition Dx,def by means of u ∈ Du ⊆ Du,def,
it is called locally (small-time locally or omnidirectionally) controllable on Dx
by Du.
It should be noted that in practical applications, the set Du of admissible or
available control variable vectors u is often not identical to the set Du,def, on
which the system is defined. Since in some cases controllability depends not
only on the size of the set Dx but also on that of the set Du ⊆ Du,def, as in
the definitions above, it is useful to describe this dependence as controllable
on Dx by Du.
For illustration purposes, as an initial example, we will consider the system
x˙ =
u
x
with the domain of definition Dx,def = IR\{0}. It is directly apparent from
the differential equation that it is not possible to reach any negative endpoint
xe from any positive starting point x0, or vice versa. For this, the trajectory
would have to run through the point x = 0, for which the system is not
defined. This is also evident from the solution
x(t) = sgn(x0) ·
vuuutx
2
0 + 2 Z
t
0
u(τ)dτ, x0 6= 0, (3.3)
of the differential equation, because the sign of the solution x(t) can never
change.214 Chapter 3. Controllability and Flatness
Even though the system is not controllable on Dx = Dx,def, i. e. it is not
globally controllable, it is locally controllable on the two subsets D+
x = IR+
and D
−
x = IR−. The local controllability is directly apparent from its solution
(3.3). Furthermore, the system is globally small-time locally controllable.
Let us now consider a second example to further our understanding of
global and local controllability. In particular, we will consider the system
x˙ = xu (3.4)
which is defined on Dx,def = IR. Its system solution is
x(t) = x0e
Rt
0
u(τ)dτ
. (3.5)
Although the system is defined at x = 0, it is not controllable there. This is
because we cannot reach any point xe 6= 0 from x0 = 0. Furthermore, the
system solution (3.5) also shows that the sign of the solution x(t) can never
change. The system is thus not globally controllable. However, it is locally
controllable on D+
x = IR+ and D−
x = IR−.
The system (3.4) is also not globally small-time locally controllable, since
for the point x0 = 0 no reachable set
R(x0, T ) 6= {0},
i. e. no neighborhood R(x0, T ) of x0, exists. On the set D+
x = IR+ of positive
real numbers and on the set D−
x = IR− of negative real numbers, however,
the system (3.4) is locally small-time locally controllable
[1]
.
As a final example, we will consider the system
x˙ =

x2
0

+

0
1

u1 +

g(x2)
0

u2 with g(x2) = (
x
2
2
, x2 > 0,
0, x2 ≤ 0.
It is globally controllable, since the linear system component is controllable.
For x2 > 0, the system is also omnidirectionally controllable, since for this
case
g(x2) 6= 0
holds, and using u1 and u2, we can specify any arbitrary direction via x˙
and thus also any arbitrary trajectory x(t). Since, for x2 > 0, the system
is locally omnidirectionally controllable, according to Theorem 44 it is also
locally small-time locally controllable.
[1] If a system is small-time locally controllable only on a subset of its domain of
definition, then we have to call it locally small-time locally controllable. Although
this phrase is meaningful, it may seem confusing due to the double use of the term
locally. A non-confusing term would be locally small-time proximally controllable.
However, the term small-time proximally controllable is not established in the
literature.3.1. Controllability 215
3.1.3 Proving Controllability
Now that we have introduced and defined the different concepts of control￾lability, which are of fundamental importance for feedforward and feedback
control systems, the question arises how to verify the controllability of a spe￾cific system.
For linear systems, we know the answer: we check if the rank of the control￾lability matrix (3.2) equals n. For general nonlinear systems, no such easily
applicable criterion exists. This unsatisfactory situation arises because it is
necessary – with some exceptions – to know the solution of the system’s dif￾ferential equation to determine whether it is possible to transform any point
x0 into any other point xe by means of an input signal within finite time.
However, these solutions are rarely analytically determinable for nonlinear
systems.
For a special class of nonlinear systems, however, the controllability can
be examined. These are systems that are nonlinear in their state variables
xi
, but linear in their input variables ui
, i. e. they can be represented by the
differential equation
x˙ = a(x) + B(x) · u = a(x) +Xm
i=1
bi(x) · ui (3.6)
with x ∈ Dx,def ⊆ IRn
and u ∈ IRm. Such systems are called control-affine
systems[2]. In practice, this class of systems is of great importance because
many technological systems are control-affine.
We will now consider a particular control-affine system of the form







x˙ 1
x˙ 2
.
.
.
x˙ n−1
x˙ n







=







x2
x3
.
.
.
xn
α(x)







+







0
0
.
.
.
0
β(x)







u (3.7)
with
x =

x1 x2 · · · xn
T
,
which is referred to as the nonlinear controller canonical form. Figure 3.9
shows the associated block diagram.
Assuming that
β(x) 6= 0
holds for all x ∈ Dx ⊆ Dx,def, it follows that a new control or input variable
v can be used via
[2] Such systems are also termed input-linear systems, input-affine systems, or affine￾in-control systems in the literature.216 Chapter 3. Controllability and Flatness
α(x)
u
β(x)
1
s
1
s
1
x s n x3 x2 x1
Fig. 3.9: System in nonlinear controller canonical form
u =
− α(x) + v
β(x)
. (3.8)
With equation (3.8) the system (3.7) takes the form of a linear system







x˙ 1
x˙ 2
.
.
.
x˙ n−1
x˙ n







=







0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
0 0 0 · · · 0














x1
x2
.
.
.
xn−1
xn







+







0
0
.
.
.
0
1







v (3.9)
in linear controller canonical form. This is controllable, as quickly becomes
evident from its controllability matrix
Mcontr =

b Ab A2
b . . . An−1
b

=







0 0 · · · 0 1
0 0 · · · 1 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 1 · · · 0 0
1 0 · · · 0 0







which is of rank n. Since the linear system (3.9) is controllable, so is the
nonlinear system (3.7). It follows that a system that is given in nonlinear
controller canonical form with β(x) 6= 0 is controllable. This holds for all
systems that can be transformed into the nonlinear controller canonical form
using a bijective coordinate transformation. Such a transformation z = q(x)
assigns exactly one state z to each state x, and vice versa. Here it should
be at least once continuously differentiable. Such a transformation is referred3.1. Controllability 217
to as a diffeomorphism [425]. The proposition above holds because the con￾trollability is a system characteristic that is invariant to transformations by
diffeomorphisms. Therefore, the controllability of a system does not change if
the system is represented in different state coordinates.
For MIMO systems with n state variables and m input variables, the non￾linear controller canonical form of m subsystems is of the form


























x˙ 1
.
.
.
x˙ n1−1
x˙ n1
x˙ n1+1
.
.
.
x˙ n2−1
x˙ n2
.
.
.
x˙ nm−1+1
.
.
.
x˙ n−1
x˙ n


























=


























x2
.
.
.
xn1
α1(x)
xn1+2
.
.
.
xn2
α2(x)
.
.
.
xnm−1+2
.
.
.
xn
αm(x)


























+


























0 0 0 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 0
β1,1(x) β1,2(x) β1,3(x) · · · β1,m−1(x)β1,m(x)
0 0 0 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 0
0 β2,2(x) β2,3(x) · · · β2,m−1(x) β2,m(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 0
0 0 0 · · · 0 βm,m(x)



































u1
u2
u3
.
.
.
um−1
um









.
(3.10)
The individual subsystems[3] possess the nonlinear controller canonical form.
With the upper m×m triangular matrix
D(x) =





β1,1(x) β1,2(x) · · · β1,m(x)
0 β2,2(x) · · · β2,m(x)
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · βm,m(x),





,
similar to the SISO case, we can use a new input variable vector v via
u = D−1
(x) ·


−





α1(x)
α2(x)
.
.
.
αm(x)





+





v1
v2
.
.
.
vm







. (3.11)
We call the matrix D the decoupling matrix . Here it is assumed that βi,i(x) 6=
0 holds for all x ∈ Dx and i = 1, . . . , m, so that
det(D(x)) = β1,1(x) · β2,2(x) · . . . · βm,m(x) 6= 0
and consequently the inverse D−1
(x) exists.
[3] Notice that the degrees of the m subsystems are n1, n2−n1, n3−n2, . . . , n−nm−1.218 Chapter 3. Controllability and Flatness
Inserting equation (3.11) into the system’s representation (3.10), we arrive
at the specific linear controller canonical form


























x˙ 1
.
.
.
x˙ n1−1
x˙ n1
x˙ n1+1
.
.
.
x˙ n2−1
x˙ n2
.
.
.
x˙ nm−1+1
.
.
.
x˙ n−1
x˙ n


























=


























x2
.
.
.
xn1
0
xn1+2
.
.
.
xn2
0
.
.
.
xnm−1+2
.
.
.
xn
0


























+


























0 0 0 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 0
1 0 0 · · · 0 0
0 0 0 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 0
0 1 0 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 0
0 0 0 · · · 0 1



































v1
v2
v3
.
.
.
vm−1
vm









,
which is also referred to as the Brunovsky canonical form [60]. Analogous
to the SISO case, we can deduce that since the linear MIMO system in con￾troller canonical form is controllable, so is the corresponding system in nonlin￾ear controller canonical form. We can thus formulate the following important
theorem.
Theorem 45 (Controllability of Nonlinear Systems). A system
x˙ = f(x,u)
with x ∈ Dx,def ⊆ IRn
and u ∈ IRm, which is in nonlinear controller canonical
form, or which can be transformed into this form by means of a continuously
differentiable, bijective coordinate transformation, is controllable if the decou￾pling matrix D(x) fulfills
det (D(x)) 6= 0
for all x of a path-connected set Dx ⊆ Dx,def.
Of course, this theorem also includes the SISO case, in which
det (D(x)) = β(x)
holds.
The above theorem is of great importance because quite a few nonlinear
systems exist which can be transformed into the nonlinear controller canonical
form. The most important of these belong to the class of control-affine systems.
How these systems can be transformed into the nonlinear controller canonical
form, and how to control them, is discussed in Section 5.2.3.1. Controllability 219
3.1.4 Example: Industrial Robot
We will consider an industrial robot with three degrees of freedom, two rota￾tional degrees and one translational. The rotation angle Θ1 of a vertical arm
A1 with associated mass m1 constitutes the first degree of freedom of move￾ment with respect to the foundation, as shown in Figure 3.10. Attached to the
endpoint of the arm is a joint which realizes the second degree of freedom by
means of a rotation with angle Θ2. The third degree of freedom is obtained
through a shiftably mounted arm A2 within the rotary joint with associated
mass m2. The arm allows for a translational movement along the stretch r,
which defines the distance of the center of gravity of A2 to the rotary joint.
In order to derive the dynamic model of the robot, we first compute the
robot’s kinetic and potential energy, Ekin and Epot. The difference between
the two is the Lagrange function
L = Ekin − Epot, (3.12)
which we insert into the Lagrange equations [184]
Mi =
d
dt
∂L
∂Θ˙
i
−
∂L
∂Θi
, i = 1, 2,
F =
d
dt
∂L
∂r˙
−
∂L
∂r
(3.13)
and thereby determine the torques Mi and the force F. The torques Mi and
the force F are the robot’s control variables, and are proportional to the
currents of the electric motors which adjust the joints’ angles Θi and the
translation r.
The kinetic energy of the arm A1 is
Ekin,A1 =
1
2
JzΘ˙ 2
1
,
where Jz is the moment of inertia of arm A1 with respect to its longitudinal
axis z.
For arm A2, the kinetic energy is composed of its rotational energy Erot,A2
and the energy Etrans,A2
from the translational movement. With the angular
velocity vector ωA2
and the inertia matrix
J =


Jx˜ 0 0
0 Jy˜ 0
0 0 Jz˜


of arm A2, it holds that
Erot,A2 =
1
2
ω
T
A2
J ωA2
. (3.14)220 Chapter 3. Controllability and Flatness
g
r
y˜
x˜
z˜
Θ2
y
x
z
Θ1
Θ˙
1
Θ˙
1sin(Θ2)
Θ˙
1cos(Θ2)
Θ2
A2
A1
Fig. 3.10: Industrial robot with three degrees of freedom. The x, ˜ y, ˜ z˜-coordinate
system has its origin in the center of gravity of the robot arm A2.
With the unit vectors of the space-fixed coordinate system (x, y, z) and the
arm-coordinate system (˜x, y, ˜ z˜)
ex =


1
0
0

, ey =


0
1
0

 , ez =


0
0
1

 and ex˜ =


1
0
0

, ey˜ =


0
1
0

, ez˜ =


0
0
1

 ,
and the decomposition of the angular velocity shown in Figure 3.10
Θ˙
1ez = Θ˙
1 sin(Θ2)ex˜ + Θ˙
1 cos(Θ2)ey˜,
as well as the angular velocity Θ˙
2ez˜ of the arm A2 rotating around the z˜-joint
axis, it holds that
ωA2 = Θ˙
1 sin(Θ2)ex˜ + Θ˙
1 cos(Θ2)ey˜ + Θ˙
2ez˜. (3.15)3.1. Controllability 221
Inserting equation (3.15) into equation (3.14) yields
Erot,A2 =
1
2
h
(Jx˜ sin2
(Θ2) + Jy˜ cos2
(Θ2))Θ˙ 2
1 + Jz˜Θ˙ 2
2
i
.
For the computation of the translational energy Etrans,A2
, we require the
velocity vector vA2
of A2’s center of mass. It is composed of the radial velocity
r˙ex˜ as well as the tangential velocities Θ˙
2rey˜ and Θ˙
1r cos(Θ2)ez˜ and is given
by the equation
vA2 = ˙rex˜ + Θ˙
2rey˜ − Θ˙
1r cos(Θ2)ez˜.
We then obtain
Etrans,A2 =
1
2
m2 v
T
A2
vA2 =
1
2
m2
h
r˙
2 + r
2

Θ˙ 2
2 + Θ˙ 2
1
cos2
(Θ2)
i .
Furthermore, we have to determine the robot’s potential energy, for which
we obtain
Epot = m2 g r sin(Θ2).
It then follows according to equation (3.12) that
L = Ekin,A1 + Erot,A2 + Etrans,A2
| {z }
Ekin
−Epot
=
1
2
h￾
Jz + Jx˜ sin2
(Θ2) + (Jy˜ + m2r
2
) cos2
(Θ2)

Θ˙ 2
1 + (Jz˜ + m2r
2
)Θ˙ 2
2
+ m2r˙
2 − 2m2 g r sin(Θ2)
i
(3.16)
holds.
We now insert equation (3.16) into the Lagrange equations (3.13) and
obtain the expressions for the torques M1, M2, and the force F as
M1 =

Jz + Jx˜ sin2
(Θ2) + (Jy˜ + m2r
2
) cos2
(Θ2)

Θ¨
1
+ 2m2 r r˙ Θ˙
1 cos2
(Θ2) − (Jy˜ − Jx˜ + m2r
2
)Θ˙
1Θ˙
2 sin(2Θ2),
M2 = (Jz˜ + m2r
2
)Θ¨
2 + 2m2 r r˙ Θ˙
2
+
1
2
(Jy˜ − Jx˜ + m2r
2
)Θ˙ 2
1
sin(2Θ2) + m2 g r cos(Θ2),
F = m2r¨ − m2 r(Θ˙ 2
1
cos2
(Θ2) + Θ˙ 2
2
) + m2 g sin(Θ2).
(3.17)
We can reformulate equation (3.17) by using the state vector x, the input
variable vector u, and the output variable vector y,
x =

Θ1 Θ˙
1 Θ2 Θ˙
2 r r˙
T
, u =

M1 M2 F
T
, y =

Θ1 Θ2 r
T
,
yielding the dynamic mode222 Chapter 3. Controllability and Flatness
x˙ = a(x) + B(x) · u,
y = c(x)
(3.18)
with
a(x) =





















x2
−
2m2 x2 x5 x6 cos2
(x3) − (Jy˜ − Jx˜ + m2 x
2
5
)x2 x4 sin(2x3)
Jz + Jx˜ sin2
(x3) + (Jy˜ + m2 x
2
5
) cos2(x3)
x4
−
2m2 x4 x5 x6 +
1
2
(Jy˜ − Jx˜ + m2 x
2
5
)x
2
2
sin(2x3) + m2 g x5 cos(x3)
Jz˜ + m2 x
2
5
x6
x5(x
2
2
cos2
(x3) + x
2
4
) − g sin(x3)





















,
B(x) =















0 0 0
1
Jz + Jx˜ sin2
(x3) + (Jy˜ + m2x
2
5
) cos2(x3)
0 0
0 0 0
0
1
Jz˜ + m2x
2
5
0
0 0 0
0 0
1
m2















,
c(x) =


x1
x3
x5

.
The system (3.18) is in the nonlinear controller canonical form
x˙ =









x2
α1(x)
x4
α2(x)
x6
α3(x)









+









0 0 0
β1(x) 0 0
0 0 0
0 β2(x) 0
0 0 0
0 0 β3(x)









u,
y =

x1 x3 x5
T
(3.19)3.1. Controllability 223
and it holds that βi(x) 6= 0 for i = 1, 2, 3. The matrix
D(x) =



β1(x) 0 0
0 β2(x) 0
0 0 β3(x)



with the associated determinant
det(D(x)) = β1(x)β2(x)β3(x)
is different from zero, since βi(x) 6= 0 holds for i = 1, 2, 3. Thus the robot is
controllable.
The nonlinear controller canonical form allows for controller design which
is especially easy to handle, hence its name. For this design, we use the control
law
u = −



β
−1
1
(x) 0 0
0 β
−1
2
(x) 0
0 0 β
−1
3
(x)






α1(x) + a1,1x2 + a1,0x1 − V1 · yref,1
α2(x) + a2,1x4 + a2,0x3 − V2 · yref,2
α3(x) + a3,1x6 + a3,0x5 − V3 · yref,3


 =









2m2x2x5x6 cos2
(x3)−(J∆+m2x
2
5
)x2x4 sin(2x3)−
a1,1x2+a1,0x1−V1yref,1
β1(x)
2m2x4x5x6+
(J∆+m2x
2
5
)x
2
2
sin(2x3)
2
+m2gx5cos(x3)−
a2,1x4+a2,0x3−V2yref,2
β2(x)
−m2x5(x
2
2
cos2
(x3)+x
2
4
)+m2g sin(x3)−
a3,1x6+a3,0x5−V3yref,3
β3(x)









,
(3.20)
where yref,1, yref,2, and yref,3 are the reference variables and
J∆ = Jy˜ − Jx˜.
The coefficients ai,j and Vi are freely selectable. Inserted into equation (3.18)
or (3.19), the control law (3.20) yields a linear control loop with the system
equations
y¨1 + a1,1y˙1 + a1,0y1 = V1 · yref,1,
y¨2 + a2,1y˙2 + a2,0y2 = V2 · yref,2,
y¨3 + a3,1y˙3 + a3,0y3 = V3 · yref,3.
It is possible to design such linear control loops not only for the industrial
robot described here, but for all other nonlinear plants that can be represented
in nonlinear controller canonical form as well. We will utilize this property in
Section 5.2.224 Chapter 3. Controllability and Flatness
3.1.5 Small-Time Local Controllability of Driftless Systems
In the following, we will address control-affine systems (3.6) which have no
free dynamics a(x), sometimes called drift, i. e. systems characterized by
x˙ = B(x) · u.
Accordingly, these systems are called driftless systems. For the special case of
a driftless linear system
x˙ = Bu, (3.21)
controllability exists if and only if the control matrix B is of rank n, i. e. if
det(B) 6= 0
holds. On the one hand, this is plausible, since all n derivatives must be
manipulated to change a state x0 into a different state xe. On the other hand,
controllability also follows from the full rank of the controllability matrix of
the system, i. e. from
rank (Mcontr) = rank ￾ B AB A2B · · · A
n−1B

= rank (B) = n.
Additionally, the system is also omnidirectionally controllable.
Driftless control-affine systems, in contrast to the linear case described
above, can also be controllable if B(x) is not of rank n, i. e. if the number m
of input variables is smaller than the number n of states. This is not directly
plausible, since an n×m matrix B(x) with m < n does not span the entire
IRn
, and therefore the controllability matrix
Mcontr =

B(x) 0 · · · 0

is not of rank n for a fixed x. Accordingly, the corresponding driftless linear
system (3.21) with
rank(B) < n
would not be controllable.
At second glance, however, it becomes clear that the columns bi(x) of
a nonlinear system’s matrix B(x) change along a trajectory x(t), i. e. the
direction bi(x), in which the control input ui
is effective, varies with x. After
a movement from xa to xb, it is thus possible to steer the trajectory in a new,
previously impossible direction: B(xb) · u.
As an example, we will view the system


x˙ 1
x˙ 2
x˙ 3

 =


0
0
1


|{z}
b1
u1 +


cos(x3)
sin(x3)
0


| {z }
b2(x3)
u2, (3.223.1. Controllability 225
which describes a mobile robot with two individually driven wheels and a
freely movable stabilizing wheel, as shown in Figure 3.11. The control variable
u1 is the robot’s angular velocity and the control variable u2 is its translational
velocity in the direction of motion. The state variables x1 and x2 describe the
position of the robot and x3 denotes the angular orientation of the robot in
space.
Obviously, the direction on the xy-plane where the robot can move, i. e.
b2(x3), depends on its orientation x3. Figure 3.12 illustrates this dependency.
Choosing a suitable sequence of rotations u1 and longitudinal movements u2,
it is possible to reach any arbitrary point xe from any other arbitrary point
x0, although b1 and b2(x3) only span IR2
for a fixed x3. The red line in
Figure 3.12 shows such a trajectory. To generate this trajectory, we use the
control signal sequence

u1
u2

=

0
−1

,

u1
u2

=

1
0

,

u1
u2

=

0
1

to arrive at the sequence
x0 = x(0) → x(∆t) → x(2∆t) → x(3∆t) = xe
of the state vectors. According to the geometric illustration, it is plausible
that the system is controllable.
We will now consider the more general case of a system
x˙ = b1(x) · u1 + b2(x) · u2 (3.23)
with x ∈ IRn
. Similar to the case of the mobile robot above, we utilize the
sequence
x1
x2
x3
u1
u2
x
y
Fig. 3.11: Mobile robot with two individually driven wheels and a stabilizing wheel226 Chapter 3. Controllability and Flatness
x1
x1
x2
x2
x3
x3
x0
x(∆t)
x(2∆t)
xe
−π
0
π
1
0
−1
−1
0
1
Fig. 3.12: Directions of the vector b2(x3) for u2 = 1 and for the case in which the
robot is in the state [x1 x2 x3] = [0 0 R t
0
u1(τ )dτ ]. A possible trajectory is depicted
by the red line.
u(t) =



u
T = [ 1 0], 0 ≤ t < ∆t,
u
T = [ 0 1], ∆t ≤ t < 2∆t,
u
T = [−1 0], 2∆t ≤ t < 3∆t,
u
T = [ 0 −1], 3∆t ≤ t < 4∆t,
(3.24)
as the control signal. Next we examine the solution x(t) of the differential
equation (3.23) for the points in time 0, ∆t, 2∆t, 3∆t, and 4∆t, making the
abbreviations
x0 = x(0),
x1 = x(∆t),
x2 = x(2∆t),
x3 = x(3∆t),
x4 = x(4∆t) = xe.
To compute these values, we then approximate x(t) using the first elements
of a Taylor series
x((i + 1)∆t) ≈ x(i∆t) + ∆tx˙(i∆t) + 1
2
∆t2x¨(i∆t),3.1. Controllability 227
which can be more briefly formulated as
xi+1 ≈ xi + ∆tx˙ i +
1
2
∆t2x¨i
.
Using the differential equation (3.23) and the course of the control signal
(3.24), it follows that
x1 ≈ x0 + ∆t b1(x0) + 1
2
∆t2
∂b1
∂x




x0
· b1(x0), (3.25)
x2 ≈ x1 + ∆t b2(x1) + 1
2
∆t2
∂b2
∂x




x1
· b2(x1), (3.26)
x3 ≈ x2 − ∆t b1(x2) + 1
2
∆t2
∂b1
∂x




x2
· b1(x2), (3.27)
x4 ≈ x3 − ∆t b2(x3) + 1
2
∆t2
∂b2
∂x




x3
· b2(x3). (3.28)
In the following we will first insert equation (3.25) into equation (3.26) and
subsequently insert the result into equation (3.27). The result obtained from
this is in turn inserted into equation (3.28). We thus obtain x4 in dependence
on the starting point x0, so that we can deduce the direction which points
from x0 to x4 due to the control signal sequence (3.24).
As stated above, we will begin by inserting equation (3.25) into equation
(3.26) and obtain
x2 ≈ x0 + ∆t b1(x0) + 1
2
∆t2
∂b1
∂x




x0
· b1(x0) + ∆t b2(x0 + ∆tb1(x0) + . . .)
+
1
2
∆t2
∂b2
∂x




x0+∆tb1(x0)+...
· b2(x0 + ∆tb1(x0) + . . .). (3.29)
In the following, just as above, we will only include the terms up to the power
of two, i. e. ∆t2
, in all approximations, since we are only interested in small
values of ∆t. Again, we approximate
b2(x0 + ∆t b1(x0) + 1
2
∆t2
∂b1
∂x





x0
· b1(x0)) ≈ b2(x0) + ∆t
∂b2
∂x





x0
· b1(x0)
using the Taylor series and insert the result into equation (3.29), at which
point we can make the approximation
x1 = x0 + ∆tb1(x0) + . . . ≈ x0
for the differentiation point x0 + ∆tb1(x0) + . . . in the last term of equation
(3.29). It follows that228 Chapter 3. Controllability and Flatness
x2 ≈ x0 + ∆t[b1(x0) + b2(x0)]
+ ∆t2
"
1
2
∂b1
∂x





x0
· b1(x0) + ∂b2
∂x





x0
· b1(x0) + 1
2
∂b2
∂x





x0
· b2(x0)
#
.
We then apply this result in equation (3.27) and obtain
x3 ≈ x0+∆t b2(x0)+∆t2
"
∂b2
∂x





x0
· b1(x0) −
∂b1
∂x





x0
· b2(x0) +
1
2
∂b2
∂x





x0
· b2(x0)
#
.
Finally, inserting the result into equation (3.28) yields
x4 ≈ x0 + ∆t2
"
∂b2
∂x





x0
· b1(x0) −
∂b1
∂x





x0
· b2(x0)
#
. (3.30)
Based on equation (3.30) we can deduce that, at least for small time intervals
∆t, we can move from x0 in the direction
b3(x) = ∂b2
∂x





x0
· b1(x0) −
∂b1
∂x





x0
· b2(x0) = [b1, b2]
to reach x4 = xe by means of the control signal sequence (3.24). The short
form of the expression above is [b1, b2]. Such a differential operator
[f 1
, f 2
] = ∂f 2
(x)
∂x
f 1
(x) −
∂f 1
(x)
∂x
f 2
(x)
is termed the Lie bracket after M. S. Lie.
Therefore, in addition to the directions b1 and b2, it is also possible to
move in the direction b3 if the control signal sequence is chosen accordingly. If
one chooses control signal sequences which are different and longer than the
ones in equation (3.24), where ui ∈ {−1, 0, 1} holds, this results in additional
directions
[b1, [b1, b2]],
[b2, [b1, b2]],
[b1, [b1, [b1, b2]]],
.
.
.
[[b1, b2], [b1, [b1, [b1, b2]]]],
.
.
.
into which the system state x can proceed.
The considerations above (see also [80, 81, 179, 285, 338, 435]) bring us to3.1. Controllability 229
Theorem 46 (Chow’s Theorem). Let the system
x˙ =
Xm
i=1
bi(x) · ui
be defined for x ∈ Dx,def ⊆ IRn
. Let Dx ⊆ Dx,def be an open set, and assume
that ui ∈ [umin,i, umax,i] holds with 0 > umin,i ∈ IR and 0 < umax,i ∈ IR for
i = 1, . . . , m. Then, if n of the vectors
b1, . . . , bm, [b1, b2], . . . , [bm−1, bm], [b1, [b1, b2]], . . . , [[b1, b2], [b2, b3]], . . .
are linearly independent for all x ∈ Dx, the system is small-time locally con￾trollable.
Below we will discuss the details of the theorem above. The requirement
that positive as well as negative control values ui exist is essential. If ui
is only
either positive or negative, generally not all directions can be accessed. Note
once again that the controllability of the system is derived from the small-time
local controllability according to Theorem 43 on p. 208 for a path-connected
set Dx only. The small-time local controllability is more useful in control
applications than the controllability, since it allows for shorter trajectories
from x0 to xe.
When calculating a Lie bracket in the above theorem, we begin with the
simple brackets [bi
, bj ] and determine whether it is possible to derive a matrix
from them and the vectors bi with a determinant that is different from zero.
If this is not possible, higher, i. e. nested Lie brackets must be included. In
this case, all combinations of Lie brackets and vectors bi
, and combinations
of combinations, must be included.
Let us return to the mobile robot and its system description (3.22). Its
controllability is plausible due to its design. Since
M =

b1 b2 [b1, b2]

=


0 cos(x3) − sin(x3)
0 sin(x3) cos(x3)
1 0 0


holds additionally, and therefore for the determinant of this matrix it follows
that
det(M) = cos2
(x3) + sin2
(x3) = 1,
the conditions of Theorem 46 are fulfilled and the robot is also small-time
locally controllable.
It is easy to see that the robot is controllable. However, its small-time local
controllability is not apparent. If we consider a small neighborhood around a
point, such as x0 = 0, all directions in which the robot can move are severely
limited. Figure 3.13 shows an example.
Nevertheless, we can reach the point xe which lies outside the blue high￾lighted area in Figure 3.13 by executing the following steps. First, we will
proceed toward the state230 Chapter 3. Controllability and Flatness
x1
x2
xe
x0
Fig. 3.13: Directions on the x1x2-plane in which the robot can move if only small
changes in x3 around zero are included
x1
x2
x3
x0
xe
Fig. 3.14: Trajectory of the robot from
x0 to xe = x(4∆t)
x0, x(∆t)
x2
x1
x(3∆t)
x(2∆t) xe
Fig. 3.15: Trajectory of the robot
viewed from above
x(∆t) =


0
0
x3(∆t)


by means of u = [1 0]T
in time ∆t. This only requires a small change in x3.
In the second step, we use u = [0 1]T
and steer toward the point x(2∆t),
as shown in Figures 3.14 and 3.15. From point x(2∆t), we can maneuver in
approximately the same directions as we can from point x0 = 0 if, again, we
are in a small environment of x3 = 0. In the next step, we define the control
variable as u = [−2 0]T
, so that we only need to change x3, thus reaching3.1. Controllability 231
x(3∆t). From here, we move to xe = x(4∆t) using the control signal vector
u = [0 −1]T
.
By taking a zigzag trajectory, it is therefore possible to maneuver to any
arbitrary position. This explains why, for small-time locally controllable sys￾tems, it is possible to create trajectories which remain within an arbitrarily
small neighborhood of a point x0 and reach any point there.
3.1.6 Example: Motor Vehicle with Trailer
We will describe a higher-dimensional example of a driftless control-affine
system: a front-wheel drive motor vehicle with a trailer, as illustrated in Figure
3.16. The motor vehicle with a trailer is described by the model [31, 257]
x˙ =















cos(x3) cos(x4)
cos(x3) sin(x4)
0
1
l
sin(x3)
1
d
cos(x3) sin(x4 − x5)















| {z }
b1(x)
u1 +















0
0
1
0
0















|{z}
b2(x)
u2.
Here x1 and x2 denote the position of the vehicle, or, more precisely, the center
point of its rear axle. The state variable x3 is the steering angle of the vehicle’s
front wheels, x4 is the angle of the vehicle’s longitudinal axis to the x-axis of
the space-fixed coordinate system, and x5 is the angle of the trailer axle to
the x-axis. The velocity of the vehicle u1 and the velocity of the steering angle
of the vehicle’s wheel u2 are the control variables. Figure 3.16 illustrates this.
The distance between the vehicle’s front and rear axles, and the difference
between the center points of the trailer axle and the vehicle’s rear axle are
denoted by l and d, respectively. As a simplification, it is assumed that the
trailer hitch lies at the center point of the vehicle’s rear axle.
The admissible range of definition Dx of possible system states xi
is lim￾ited. On the one hand, there are limitations to the steering angle of the vehi￾cle’s wheel x3 which are given by
−
π
4
< x3 <
π
4
.
On the other hand, there are also limitations which arise due to the fact
that the trailer’s longitudinal axis may not form too great an angle Θ to the
vehicle’s longitudinal axis. In this case, we require
−
π
2
< Θ = x5 − x4 <
π
2
.232 Chapter 3. Controllability and Flatness
x1
x2
y
x
x5
d
l
u2
x3
x4
u1
Θ
Fig. 3.16: Schematic view from above of the motor vehicle with its trailer in the
space-fixed xy-coordinate system
This yields
Dx = {x ∈ IR5
| x1, x2 ∈ IR, |x3| <
π
4
, |x5 − x4| <
π
2
}
as the admissible domain of the state variables.
We will now examine the system’s controllability, and for this purpose we
need to compute the Lie brackets
[b1, b2] =












sin(x3) cos(x4)
sin(x3) sin(x4)
0
−
1
l
cos(x3)
1
d
sin(x3) sin(x4 − x5)












, [b1, [b1, b2]] =













−
1
l
sin(x4)
1
l
cos(x4)
0
0
1
ld cos(x4 − x5)













,
and3.1. Controllability 233
[b1, [b1, [b1, b2]]] =












−
1
l
2
sin(x3) cos(x4)
−
1
l
2
sin(x3) sin(x4)
0
0
l cos(x3) − d sin(x3) sin(x4 − x5)
l
2d
2












.
Along with the vectors b1(x) and b2(x), we generate the matrix
M =

b1 b2 [b1, b2] [b1, [b1, b2]] [b1, [b1, [b1, b2]]] 
and compute its determinant as
det(M) = −
cos(x3)
l
3d
2
.
Since the steering angle of the vehicle’s wheel x3 is limited to the interval
(−π/4, π/4), the determinant is different from zero. The system is therefore
small-time locally controllable.
3.1.7 Omnidirectional Controllability
The controllability of a control-affine system with an equal number of input
variables as state variables, i. e. m = n, can be examined in a simple fashion.
We will thus consider a system
x˙ = a(x) + B(x) · u (3.31)
with x ∈ Dx,def ⊆ IRn
and u ∈ IRm. If m = n and
det(B(x)) 6= 0
holds for all x of a set Dx ⊆ Dx,def, using the new input variable vector v,
we can express the control variable u as
u = −B
−1
(x) (a(x) − v)
and represent the system (3.31) as
x˙ = v.
Obviously, it is possible to specify any arbitrary trajectory x(t) such that
this trajectory can begin or continue in any arbitrary direction x˙ if and only
if B(x) is regular or
rank(B(x)) = n
holds in the case of an n×m matrix B(x) with m > n. Correspondingly, every
trajectory x(t) ∈ Dx between an initial point x0 ∈ Dx and every destination
point xe ∈ Dx is possible given a suitable choice of v(t) ∈ IRn
. Thus we obtain234 Chapter 3. Controllability and Flatness
Theorem 47 (Omnidirectional Controllability of Control-Affine Sys￾tems). A control-affine system
x˙ = a(x) + B(x) · u
with x ∈ Dx,def ⊆ IRn
and u ∈ IRm is omnidirectionally controllable if and
only if
rank(B(x)) = n
holds for all x of a path-connected set Dx ⊆ Dx,def.
Let us illustrate the difference between small-time local controllability and
omnidirectional controllability once again using two mobile robots. The first
is the robot powered by two wheels which was discussed in Section 3.1.5. It
is small-time locally, but not omnidirectionally controllable. The second is
an omnidirectional robot, which, as the name suggests, is omnidirectionally
controllable. It possesses three so-called Swedish wheels which not only allow
for a movement perpendicular to the wheel’s axle, but also along the wheel’s
axle itself. Figure 3.17 displays these wheels and the corresponding robot. The
system representation


x˙ 1
x˙ 2
x˙ 3

 =


0 cos(x3) − sin(x3)
0 sin(x3) cos(x3)
1 0 0


| {z }
B(x)


u1
u2
u3

 (3.32)
follows from the robot’s kinematics. Here x1 denotes its x-position and x2
denotes its y-position in space. The orientation of the robot, i. e. the angle
between its main axis and the x-axis of the space-fixed coordinate system, is
denoted by x3. The control variable u1 represents the angular velocity with
respect to the robot’s vertical axis, the control variable u2 is the velocity in
the longitudinal direction, and u3 is the velocity in the transverse direction.
Since
det(B(x)) = 1
holds, the robot is omnidirectionally controllable. This is also intuitively plau￾sible based on its design.
For the general nonlinear system
x˙ = f(x,u)
with x ∈ IRn
and n input variables ui
, i. e. u ∈ IRn
, we can derive a theorem
that is similar to Theorem 47. The existence of the inverse function
u = f
−1
(x, x˙) (3.33)
is sufficient to confirm the omnidirectional controllability of the system. Thus
it is possible to specify an arbitrary direction x˙ for every point x, and to3.1. Controllability 235
x
y
x1
x2
x3
u1
u3 u2
Fig. 3.17: Omnidirectional robot with three individually powered wheel axles and
Swedish wheels
deduce the required input variable vector u from equation (3.33), which is
interpreted as the control law. The requirement that the inverse function f
−1
exists, however, need not be fulfilled for the omnidirectional controllability in
every case, since, in some cases, x˙ can be generated not only by a single u,
but also by several different vectors u. In the latter case, however, an inverse
function would not exist.
Computing the inverse function f
−1
is often difficult or impossible, so
it would be useful if the verification of the omnidirectional controllability
could be simplified. This is achieved as follows. According to the fundamental
theorem on implicit functions [167], the inverse function f
−1
exists if the
Jacobian matrix
∂f(x,u)
∂u
(3.34)
is of rank n. This is also sufficient if dim(u) > n holds. This leads to the
general
Theorem 48 (Omnidirectional Controllability). A system
x˙ = f(x,u)
with x ∈ Dx,def ⊆ IRn
and u ∈ IRm is omnidirectionally controllable if
rank(∂f(x,u)
∂u
) = n
holds for all x of a path-connected set Dx ⊆ Dx,def and u ∈ IRm.
Alternatively, this theorem can also be proven by calculating the Taylor se￾ries expansion for f around the point (xp,up), and thereby representing the
system as236 Chapter 3. Controllability and Flatness
x˙ = f(xp,up)+
∂f
∂x





xp,up
·(x−xp)+
∂f
∂u





xp,up
·(u−up) +remainder. (3.35)
If the remainder is neglected, i. e. the system is linearized, it becomes evident
that any arbitrary x˙ can be specified via u if the Jacobian matrix (3.34) in
equation (3.35) is of rank n.
Theorem 48 is also valid if the number m of control variables is larger
than the number n of state variables. However, the case in which more control
variables than state variables are available does not occur often. The reason
for this is the redundancy of m − n control variables ui
in such cases.
An example of an omnidirectionally controllable system is the satellite
discussed in Section 1.1.4, if the state vector is only composed of its angular
velocity vector. The evaporation plant for syrup production which we will en￾counter in Section 6.1.6 is also omnidirectionally controllable. In the following
section, we will describe a further example of such a system.
3.1.8 Example: Steam Generator
Among other applications, steam generators are used in power plants to gen￾erate hot steam for a turbine. For this purpose, gas, oil, or pulverized coal
are burnt in a combustion chamber. Water flowing in a pipe system contained
in this combustion chamber is heated up. As illustrated in Figure 3.18, the
hot water ascends in the pipes and reaches the steam boiler. Here in the
boiler, steam rises up and is channeled into the turbine. Because colder water
has a higher specific gravity than hot water, colder water streams out of the
steam boiler into the lower pipe system in the combustion chamber. There
it is heated up anew and subsequently rises into the steam boiler again. The
fresh water is also fed into the steam boiler, either directly or via a preheater
which heats the water using the exhaust gas from the burner.
The steam thus generated is fed into a superheater which heats the steam
past the evaporation temperature so that the remaining liquid droplets are
dissolved. On the one hand, this serves to prevent damage to the turbine
blades by liquid droplets striking them. On the other hand, it increases the
degree of efficiency of the process. Downstream of the superheater, there is a
desuperheater. It compensates for the rise in temperature that occurs when
the load of the turbine decreases. For this purpose, demineralized water is
finely atomized and fed into the steam current. Using a valve, the injected
water can be controlled such that the desired steam temperature is reached
when the steam enters the turbine. Before this is done, the blend of steam and
water vapor is subsequently dried again in a second superheater, i. e. liquid
droplets are reheated so that they evaporate. After leaving the superheater
the amount of steam that is finally injected into the turbine can be controlled
by another valve.
The three state variables, i. e. the boiler pressure p in kg cm−2
, the mass
flow q of the steam at the entrance of the turbine in kg s−1
, and the water level3.1. Controllability 237
Hot steam outlet
Superheater
Superheater Hot steam inlet
Valve opening v
Mass flow q
to the turbine
Exhaust
Feedwater
m˙ w
Pressure p
Combustible supply m˙ s
Demineralized water
h
Boiler
Combustion chamber
with pipe system
Desuperheater
Fig. 3.18: Steam generator238 Chapter 3. Controllability and Flatness
h in cm can be influenced by three control variables. The latter are the mass
flow m˙ s of the combustible material in kg s−1
, the degree of opening v of the
valve in front of the turbine, and the amount m˙ w in kg s−1
of the feedwater
per second that is injected into the boiler.
As a specific application, we will examine the steam generator of a 200 MW
plant fired by coal, gas, or oil [383]. The plant is modeled by the equations



p˙
q˙
h˙


 =



−0.00193 q
√8 p + 0.00121 h
−0.785716 q
−0.000006 p
2 − 0.007328 q − 0.00914 h − 0.000082 h
2



+



0.014524 0 −0.000736
0 10√
p 0
0.002 0.463 0.00863






m˙ s
v
m˙ w


. (3.36)
The control variables m˙ s, v, and m˙ w are all positive. Therefore this system rep￾resentation is not appropriate to analyze the controllability, since the control
signals must have both positive and negative values. Without loss of general￾ity, we now transform the system to the operating point at 60 percent of the
maximum continuous rating using the new variables
x1 = p − pop, pop = 175.8 kg cm−2
,
x2 = q − qop, qop = 135.0 kg s−1
,
x3 = h − hop, hop = 64 cm,
u1 = ˙ms − m˙ s,op, m˙ s,op = 38.577 kg s−1
,
u2 = v − vop, vop = 0.8,
u3 = ˙mw − m˙ w,op, m˙ w,op = 190.961 kg s−1
.
Inserting this into equation (3.36) yields the transformed model
x˙ =



0.497185 − 0.00193√8
175.8 + x1(135 + x2) + 0.00121 x3
−106.072 + 8√
175.8 + x1 − 0.785716 x2
−0.0021096 x1− 6·10−6
x
2
1 − 0.007328 x2 − 0.019636 x3 − 0.000082 x
2
3



+



0.014524 0 −0.000736
0 10√
175.8 + x1 0
0.002 0.463 0.00863



u. (3.37)
The operating point is an equilibrium point if
u = 0.
The control variables u1, u2, and u3 can take on both positive and negative
values. For the determinant of B(x), we obtain3.2. Flatness 239
det(B(x)) = 0.001268√
175.8 + x1,
i. e. it is different from zero for all x1 > −175.8. Since p > 0 holds, only
values x1 > −175.8 are possible, the determinant is always positive, and the
mathematical model (3.37) of the steam generator is omnidirectionally con￾trollable. Since all control variables ui and states xi are limited in a real-world
system, the real system is only locally weakly omnidirectionally controllable
in practice.
3.2 Flatness
3.2.1 Basic Concept and Definition of Flatness
The term flatness signifies that we can rearrange the dynamic equations of a
system in such a way that all input variables and state variables of a system
can be represented by functions that depend only on the output variable
vector and its derivatives [124, 125, 127, 273]. The practical use of this system
property is obvious: if a system is flat, by specifying the trajectory of the
output variable vector it is possible to directly calculate the required course
of the input variables, i. e. the appropriate feedforward control.
As an example, we will consider the system of linear differential equations
x˙ 1 = x2,
x˙ 2 = −x1 − x2 + u,
y = x1
(3.38)
with the state variables x1, x2, the input variable u, and the output variable
y. We can describe the state variables as a function of the output variable and
its derivative as
x =

x1
x2

=

y
y˙

. (3.39)
The system of differential equations (3.38) can be reformulated as the differ￾ential equation
y¨ + ˙y + y = u. (3.40)
With this representation, we also know the relationship which explicitly de￾scribes u in dependence on y, y˙, and y¨.
If we know y(t), y˙(t), and y¨(t) or if we specify their progressions, it is
not only possible to compute the progression of u(t) associated with y(t)
using equation (3.40), but also to determine the associated state progressions
x1(t) and x2(t) using equation (3.39). Here the input variable u and the state
variables x1(t) and x2(t) can be directly determined from the output variable
y and its derivatives by evaluating a function. The output y is then called a
flat output and the system is called flat.240 Chapter 3. Controllability and Flatness
The above example of a flat system illustrates why the property of flatness
is useful. The result obtained for this specific example can be generalized to all
flat systems. As mentioned previously, this allows for a simple determination
of the control signal u(t), which generates a desired trajectory
(y(t), y˙(t), . . . , y(β)
(t)).
For the system characteristic of flatness, it is irrelevant whether the output
in question really exists or not. To verify flatness, we can come up with any
suitable system output. An output which does not exist in reality is called fic￾titious. If only fictitious flat outputs exist for the implementation of a control,
they must be converted to the real output or vice versa.
Based on the description above, we will define the term flatness [127]. In
doing so, we will only consider the systems
x˙ = f(x,u) (3.41)
with m ≤ n for which no control variable ui can be represented as a function
of other control variables uj6=i
. This is fulfilled if the Taylor series expansion
of f in equation (3.41),
x˙ = f(x0,u0) +
∂f
∂x





x0,u0
· (x − x0) +
∂f
∂u





x0,u0
· (u − u0) + remainder,
has a matrix
∂f
∂u





x0,u0
of rank m for all x0 and u0. We will now specify the concept of flatness more
precisely in
Definition 32 (Flatness). Let a system
x˙ = f(x,u)
be defined for x ∈ Dx,def ⊆ IRn
and u ∈ Du,def ⊆ IRm with m ≤ n and let
rank(∂f(x,u)
∂u
) = m
hold. The system is called flat if a real or fictitious output variable vector
y = h(x,u,u˙, . . . ,u
(α)
)
with a finite value for α ∈ IN exists, so that
(1) the state vector x can be represented as a function of y and a finite number
β of derivatives y
(i)
as
x = Ψ1(y, y˙, . . . , y
(β)
), (3.42)3.2. Flatness 241
(2) the input variable vector u can be represented as a function
u = Ψ2(y, y˙, . . . , y
(β+1)), (3.43)
and
(3) it holds for the input and output variable vector that
dim(y) = dim(u).
In this case, the output variable vector y is called flat output.
For SISO systems, it holds that y = h(x) and β = n − 1. On the other hand,
the value of β is not known a priori for MIMO systems [467]. Usually, not
only in SISO systems but in MIMO systems as well, the flat output y does
not directly depend on the control variable u or one of its derivatives u
(i)
. It
therefore often holds that y = h(x).
In general, we can distinguish between local and global flatness, depending
on whether Conditions (1), (2), and (3) of the above definition are only fulfilled
for a proper subset of the domain of definition of f(x,u), or whether they are
fulfilled for its entire domain of definition.
The term differentially flat is often used synonymously with the term flat.
This is because only the derivatives of y, but no integrals of y, are used to
determine u and x in the functions Ψ1 and Ψ2.
The Conditions (2) and (3) of Definition 32 are equivalent to stating that
the function y(t) does not fulfill any differential equation
ϕ(y, y˙, . . . , y
(γ)
) = 0, γ ∈ {0, 1, 2, . . . }. (3.44)
If, for example, dim(y) > dim(u), cases would exist for which equation (3.44)
would be fulfilled. The example
u = y1 + 2 ˙y1 + y2 + ˙y2
with dim(u) = 1 and dim(y) = 2 illustrates this. Obviously, for this case, we
can choose y2 and y˙2 such that
y2 + ˙y2 = 0,
which is a differential equation of the form (3.44), and u is determined only
from y1 and y˙1. If, on the other hand, the inequality dim(y) < dim(u) holds,
then not all ui are independent of each other. We can illustrate this by the
example
u1 = y1 + ˙y1,
u2 = y1,
from which
u1 = u2 + ˙u2242 Chapter 3. Controllability and Flatness
follows. As described earlier, the independence of the control variables ui
is
a reasonable assumption, since some of the variables ui would otherwise be
unnecessary to generate the output trajectory y(t). Assuming this indepen￾dence, equation (3.43) consists of m = dim(u) = dim(y) independent differ￾ential equations for y1, . . . , ym, which depend on the input variables ui
; thus,
these equations are not of the form (3.44).
If equation (3.44) is not fulfilled, the outputs yi are called differentially
independent. This property ensures that no output yi or any of its derivatives
is determined by one or more of the others. Consequently, there is no function
which describes a relationship between the variables
yi
, y˙i
, . . . , y
(β+1)
i
.
We are therefore completely free in choosing the output trajectory
(y, y˙, y¨, . . . , y
(β+1)),
and can also realize this trajectory using the control input from equation
(3.43).
It is most useful if a real output y is flat. On the one hand, in this case
the flatness can be easily verified using equation (3.42) and equation (3.43).
On the other hand, we can directly specify the required control (3.43) from
the trajectory (y, y˙, y¨, . . . , y
(β+1)).
Unfortunately, the real output is frequently not flat. In this case, it is
necessary to search for a fictitious flat output. Similar to the case of the
Lyapunov function, no practically applicable general method exists to find or
construct a fictitious flat output. Therefore we have to try out different outputs
to determine a flat output. It has proven useful to start by considering outputs
y which exhibit a derivative y
(k) which depends on the input variable vector
u as the first of the derivatives y
(i)
, i = 1, . . . , k, and has an order k which
is as high as possible. However, for a flat system there is not only one single
flat output, but an infinite number of flat outputs. All these outputs can be
converted into each other.
After determining a candidate y for a flat output, we can verify whether
it is truly flat. To do this, the function
y = h(x,u,u˙, . . . ,u
(α)
)
is derived multiple times, which yields
y˙ =
dh(x,u,u˙, . . . ,u
(α)
)
dt ,
y¨ =
d
2h(x,u,u˙, . . . ,u
(α)
)
dt2
,
.
.
.3.2. Flatness 243
until an algebraic system of equations for the variables x,u,u˙, . . . ,u
(α)
is ob￾tained which can be solved. From this system, we can determine the functions
x = Ψ1(y, y˙, . . . , y
(β)
),
u = Ψ2(y, y˙, . . . , y
(β+1))
of Definition 32, and thereby prove that the output is flat.
In certain cases, a flat system representation results directly from the
derivation of the physical equations. For example, this applies to mechanical
systems such as the industrial robot from Section 3.1.4 if they are modeled
using their kinetic and potential energies, Ekin and Epot, and the Lagrange
equations
d
dt
∂L
∂q˙i
−
∂L
∂qi
= τi
, i = 1, . . . , k,
or their vectorial form
d
dt
∂L
∂q˙
−
∂L
∂q
= τ
T
.
Here
L = Ekin − Epot
is the Lagrange function, the vector q
T = [q1 . . . qk] contains the generalized
coordinates, i. e. the distances or angles, and the vector τ
T = [τ1 . . . τk] con￾tains the forces and torques. From the Lagrange equations, we directly obtain
a flat system description with
y =

q1 · · · qk
T
,
u =

τ1 · · · τk
T
=

d
dt
∂L
∂q˙
−
∂L
∂q
T
= Ψ1(y, y˙, y¨),
x =

q1 · · · qk q˙1 · · · q˙k
T
=

y1 · · · yk y˙1 · · · y˙k
T
= Ψ2(y, y˙),
and the flat output y.
3.2.2 The Lie-Bäcklund Transformation
In the state-space representation
x˙ = f(x,u) (3.45)
of the system with the flat output
y = h(x,u,u˙, . . . ,u
(α)
), (3.46)244 Chapter 3. Controllability and Flatness
we use the state vector x and its time course x(t) to describe the system’s
behavior. In order to calculate the course of the state vector x(t) and the
output variable vector y(t), we need to solve the differential equation. As
mentioned, the output function h in equation (3.46) only depends on x in the
majority of cases, i. e. there is no feedthrough.
If, on the other hand, we use the flat system representation
x = Ψ1(y, y˙, . . . , y
(β)
), (3.47)
u = Ψ2(y, y˙, . . . , y
(β+1)) (3.48)
and the flat output variable vector y, we do not need to solve a differential
equation to determine x. The flat coordinates
zf =





y
y˙
.
.
.
y
(β)





are not subject to any dynamics since they are differentially independent
of each other, i. e. they do not have a mutual dependence in the form of a
differential equation (3.44). Rather, the time course of the m output variables
yi as well as all the associated derivatives
y
(k)
i
(t), k = 1, 2, . . . ,
can be chosen arbitrarily. They therefore represent a system without dynam￾ics, which is referred to as a trivial system. The two system descriptions (3.45),
(3.46) and (3.47), (3.48) are equivalent to each other. They can be bijectively
converted into each other via a transformation that is called the Lie-Bäcklund
transformation or Lie-Bäcklund isomorphism [18, 126]. The equations (3.47),
(3.48), and
u˙ = Ψ˙
2(y, y˙, . . . , y
(β+1)),
u¨ = Ψ¨2(y, y˙, . . . , y
(β+1)),
.
.
.
make up the Lie-Bäcklund transformations and the equations
y = h(x,u,u˙, . . . ,u
(α)
),
y˙ = h˙ (x,u,u˙, . . . ,u
(α)
),
y¨ = h¨(x,u,u˙, . . . ,u
(α)
),
.
.
.
the associated inverse transformation. Figure 3.19 illustrates both coordi￾nate spaces with their coordinate vectors x and zf which consists of y
(i)
,
i = 0, 1, . . . , β + 1, and the Lie-Bäcklund transformation.3.2. Flatness 245
x2
x(0) x(t) x(T )
x1
x ∈ IRn
y˙
y

y(t)
y˙(t)


y(0)
y˙(0)

y(T )
y˙(T )

zf =

y
y˙

∈IRm(β+1)
hf
h
Lie-Bäcklund transformation
Fig. 3.19: The Lie-Bäcklund transformation transforms the system representation
from the state-space coordinate system to the space of flat coordinates. For illustra￾tion purposes, the potential h of a so-called gradient system x˙ = f(x, u) with its
gradient grad(h) = f(x, 0) and the potential function hf of the trivial system in flat
system representation with grad(hf) = 0 is shown here.
In general, the Lie-Bäcklund transformation, even though it is bijective,
does not maintain the dimension of the original coordinate system, since it
holds that x ∈ IRn
, but zf ∈ IRm(β+1). The dimension of the input space IRm,
on the other hand, is not affected by the transformation.
As a simple example, we will look at system (3.38) once again. The Lie￾Bäcklund transformation is given here by
x = Ψ1(y, y˙) = 
y
y˙

,
u = Ψ2(y, y,˙ y¨) = ¨y + ˙y + y.
(3.49)
The inverse transformation is
y = x1,
y˙ = x2,
y¨ = −x1 − x2 + u.
(3.50)
If the transformation rule (3.49) is inserted into the system equations
(3.38), we obtain246 Chapter 3. Controllability and Flatness
x˙ 1 = x2 = ˙y,
x˙ 2 = −x1 − x2 + u = ¨y,
y = x1 = y.
Incorporating x˙ 1 = ˙y and x˙ 2 = ¨y from equation (3.50), the trivial relation
y˙ = ˙y,
y¨ = ¨y,
y = y
follows. This means that, as expected, we obtain a trivial system that can be
freely specified in terms of its variable y and the associated derivatives.
If we apply the inverse transformation equations (3.50) and insert them
into the trivial system, for which y, y˙, and y¨ are independent and can be freely
chosen, we get
y˙ = ˙x1 = x2,
y¨ = ˙x2 = −x1 − x2 + u,
y = x1,
which is the original system once again.
3.2.3 Example: VTOL Aircraft
As an example, we will consider a vertical take-off and landing (VTOL)
aircraft. Such an aircraft can hover as well as take off and land vertically
[129, 309]. For this purpose engines are mounted to the ends of the wings and
can be rotated into a vertical position for take-off and landing. For regular
flight, on the other hand, the thrusts are oriented horizontally. This allows
much higher flight speeds than is possible for helicopters. Such VTOL aircraft
include the Dornier Do 31, the Hawker Siddeley Harrier, and the Bell-Boeing
V-22 Osprey. The illustration of the AgustaWestland AW609, which is based
on the Bell-Boeing V-22 Osprey, is shown in Figure 3.20.
For vertical take-off and landing, the corresponding motion equations for
the horizontal position z1, as well as the vertical position z2 of the center
of gravity S, and the rotation ϕ in relation to the aircraft’s roll axis can be
derived from the balance of forces
mz¨1 = −(Fl + Fr) cos(α) sin(ϕ) + (Fl − Fr) sin(α) cos(ϕ),
mz¨2 = −mg + (Fl + Fr) cos(α) cos(ϕ) + (Fl − Fr) sin(α) sin(ϕ)
(3.51)
with the jet-engine forces denoted by Fl and Fr, and from the torque equation
Jϕ¨ = (Fl − Fr)(b cos(α) − a sin(α)). (3.52)
Here, m denotes the mass of the airplane, J is the inertia moment with respect
to the roll axis, α is the angle of inclination of an engine with respect to the3.2. Flatness 247
Fr
Fl
z1
ϕ
z2
u1
u2
α
a
b
S
g
Yaw axis
Pitch axis
Fig. 3.20: The AgustaWestland AW609, an example of a VTOL aircraft. The two
smaller diagrams illustrate the front view of the aircraft and the relevant parameters.
yaw axis, and a and b represent the distances of an engine to the center of
gravity S in the direction of the yaw and pitch axis, respectively. For the
gravitational acceleration, the common notation g is used.
For simplification, we will define the control variable u1 as the acceleration
in relation to the direction of the aircraft’s yaw axis, i. e.
u1 =
cos(α)
m
(Fl + Fr), (3.53)
and the control variable u2 as the angular acceleration in relation to the roll
axis, i. e.
u2 =
b cos(α) − a sin(α)
J
(Fl − Fr). (3.54)
For the sake of convenience, we will use the artificial length248 Chapter 3. Controllability and Flatness
ε =
J sin(α)
m(b cos(α) − a sin(α)) (3.55)
in the following calculations. It assumes low values due to the small angle α.
From equations (3.51) to (3.55), along with the state vector
x =

z˙1 z˙2 ϕ z ˙ 1 z2 ϕ
T
,
we obtain for the model of the VTOL aircraft
x˙ 1 = −u1 sin(x6) + εu2 cos(x6), (3.56)
x˙ 2 = u1 cos(x6) + εu2 sin(x6) − g, (3.57)
x˙ 3 = u2, (3.58)
x˙ 4 = x1, (3.59)
x˙ 5 = x2, (3.60)
x˙ 6 = x3. (3.61)
We now aim to identify a flat output variable vector y, and choose
y =

y1
y2

=

x4 − ε sin(x6)
x5 + ε cos(x6)

(3.62)
as a possible candidate.
According to the definition of flatness, the system equations (3.56) to (3.61)
must be reformulated such that the state vector x and the input variable
vector
u = [u1 u2]
T
are represented as functions
x = Ψ1(y, y˙, . . . , y
(β)
),
u = Ψ2(y, y˙, . . . , y
(β+1)).
To achieve this, we convert equation (3.62) into
x4 = y1 + ε sin(x6), (3.63)
x5 = y2 − ε cos(x6). (3.64)
The next task is the elimination of the state variable x6 in equations (3.63)
and (3.64). To do this, we compute the second derivative of equation (3.63)
which yields
y¨1 = ¨x4 − εx¨6 cos(x6) + εx˙
2
6
sin(x6).
In this equation, we incorporate equations (3.56), (3.58), (3.59), and (3.61) so
that
y¨1 = (εx˙
2
6 − u1) sin(x6) (3.65)3.2. Flatness 249
follows. Similarly, we obtain
y¨2 = −(εx˙
2
6 − u1) cos(x6) − g. (3.66)
Dividing equation (3.66) by equation (3.65) results in
x6 = − arctan 
y¨1
y¨2 + g
!
. (3.67)
By applying
arctan 
p
q
!
= arcsin 
p
p
p
2 + q
2
!
,
it follows from equation (3.63) and equation (3.67) that
x4 = y1 − ε
y¨1
p
y¨
2
1 + (¨y2 + g)
2
. (3.68)
In a comparable way, we obtain
x5 = y2 − ε
y¨2 + g
p
y¨
2
1 + (¨y2 + g)
2
(3.69)
from equation (3.64) and equation (3.67).
We have now represented x4, x5, and x6 in dependence on the output
variables y1 and y2, and their derivatives, to determine the function Ψ1. In
the next step, we will focus our attention on the state variables x1, x2, and
x3.
It holds that
x1 = ˙x4 = ˙y1 + ε
y¨1
...
y2
(¨y2 + g) −
...
y1
(¨y2 + g)
2
[¨y
2
1 + (¨y2 + g)
2]
3/2
(3.70)
and
x2 = ˙x5 = ˙y2 + ε
y¨1
...
y1
(¨y2 + g) − y¨
2
1
...
y2
[¨y
2
1 + (¨y2 + g)
2]
3/2
(3.71)
as well as
x3 = ˙x6 =
y¨1
...
y2 −
...
y1
(¨y2 + g)
y¨
2
1 + (¨y2 + g)
2
. (3.72)
Now that we have derived the function Ψ1 with
β = 3
from equation (3.67) to equation (3.72), we still have to prove the existence
of250 Chapter 3. Controllability and Flatness
u = Ψ2(y, y˙, . . . , y
(4))
to verify flatness. We begin with the control variable u1 and derive
u1 = εx˙
2
6 −
y¨1
sin(x6)
from equation (3.65). Inserting equations (3.67) and (3.72) into this equation
yields
u1 = ε
"
y¨1
...
y2 −
...
y1
(¨y2 + g)
y¨
2
1 + (¨y2 + g)
2
#2
+
q
y¨
2
1 + (¨y2 + g)
2.
For the control variable u2, with equation (3.58) and equation (3.72) we obtain
u2 = ˙x3 =
y¨1y
(4)
2 − y
(4)
1
(¨y2 + g)
y¨
2
1 + (¨y2 + g)
2
− 2
[¨y1
...
y2 −
...
y1
(¨y2 + g)][¨y1
...
y1 + (¨y2 + g)
...
y2
]
[¨y
2
1 + (¨y2 + g)
2]
2
,
so that we have also determined the function Ψ2. Thus, the VTOL aircraft
model (3.56) to (3.61) is flat.
3.2.4 Flatness and Controllability
Below we will examine a flat linear system that is given in controller canonical
form
x˙ =







0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
−a0 −a1 −a2 · · · −an−1







x +







0
0
.
.
.
0
1







u,
y =

b0 b1 · · · bm 0 · · · 0

x,
(3.73)
or has been transformed into this form. We can assume without loss of gen￾erality that the system is in this canonical form, since, like controllability,
flatness is invariant with respect to bijective state transformations, i. e. a sys￾tem characteristic which does not change.
The flatness of the system (3.73) can be quickly verified by introducing
the fictitious output
yf = x1.
This is because it holds that
x1 = yf
,
x2 = ˙x1 = ˙yf
,
x3 = ˙x2 = ¨yf
,
.
.
.
xn = ˙xn−1 = y
(n−1)
f
,3.2. Flatness 251
i. e.
x =
h
yf y˙f y¨f
· · · y
(n−1)
f
iT
= Ψ1(yf
, y˙f
, . . . , y
(n−1)
f
),
and
u = a0x1 + a1x2 + . . . + an−2xn−1 + an−1xn + ˙xn
= a0yf + a1y˙f + . . . + an−2y
(n−2)
f + an−1y
(n−1)
f + y
(n)
f
= Ψ2(yf
, y˙f
, . . . , y
(n)
f
).
Therefore, the fictitious output yf = x1 is flat.
Since every controllable linear system can be transformed into the con￾troller canonical form (3.73), a controllable linear system is always flat. On
the other hand, a flat system is also always controllable [127]. Both hold for
MIMO systems as well. Accordingly, we are able to formulate [273]
Theorem 49 (Controllability and Flatness of Linear Systems). A
linear system x˙ = Ax + Bu with x ∈ IRn
and u ∈ IRm is flat if and only if
it is controllable.
The above criterion is necessary and sufficient. A similar, merely sufficient
theorem [127] also exists for the case of nonlinear systems:
Theorem 50 (Controllability and Flatness of Nonlinear Systems).
A nonlinear system x˙ = f(x,u) with x ∈ Dx,def ⊆ IRn
and u ∈ Du,def ⊆ IRm
is controllable if it is flat.
There is a close relationship between controllability and flatness. The set
of flat systems is a subset of the controllable systems. In particular, in The￾orem 50 we now have a criterion to determine whether a nonlinear system is
controllable.
In summary, we can deduce that the controllability of a system allows us
to transform its state from any starting point to any end point if we choose
suitable control variables. Similarly, flatness allows us to generate every de￾sired output variable signal which possesses a derivative of sufficiently high
order using a suitable control variable. Both system properties allow for a
systematic manipulation of the system via a control variable.
3.2.5 Flat Outputs of Linear Systems
Although it is not possible to directly specify the set of flat outputs of a system
in general, it can easily be done for linear systems
x˙ = Ax + bu. (3.74)
This makes sense, since flatness is an important and useful property of linear
systems as well. As is the case for nonlinear systems, it allows us to design a
control system for a specified output variable signal yref(t).252 Chapter 3. Controllability and Flatness
In the previous section, we have already determined a flat output for the
special case of linear systems in controller canonical form. After multiplication
by an arbitrary constant different from zero, this output generates the set of
all flat outputs in controller canonical form. Consequently, a possible method
of determining the flat outputs of a linear system (3.74) is to transform it to
the controller canonical form. The flat outputs of the controller canonical form
are then transformed back to the original coordinates of the system (3.74).
Below, we will discuss an alternative possibility.
To determine the flat outputs
yf = c
T x
of a linear system in the general form (3.74), we will first derive the equation
x = Ψ1(yf
, y˙f
, . . . , y
(n−1)
f
). (3.75)
For this purpose, we will determine
yf = c
T x,
y˙f = c
T x˙ = c
TAx + c
T
bu,
y¨f = c
TAx˙ + c
T
bu˙ = c
TA
2x + c
TAbu + c
T
bu,˙
.
.
.
y
(n−1)
f = c
TA
n−2x˙ . . . =c
TA
n−1x+c
TA
n−2
bu+c
TA
n−3
bu˙ +. . .+ c
T
bu
(n−2)
.
To obtain a relationship consistent with equation (3.75), the above derivatives
must be independent of u, u, . . . , u ˙
(n−2), i. e. it must hold that
c
T
b = 0,
c
T Ab = 0,
c
T A
2
b = 0,
.
.
.
c
T A
n−2
b = 0.
(3.76)
It follows that







yf
y˙f
y¨f
.
.
.
y
(n−1)
f







=







c
T
c
T A
c
T A2
.
.
.
c
T An−1







| {z }
Mobs
x,3.2. Flatness 253
where Mobs is the observability matrix of the linear system (3.74) with the
output yf = c
T x. Assuming the regularity of Mobs, i. e. the observability of
the linear system with output yf
, it holds that
x = M−1
obs







yf
y˙f
y¨f
.
.
.
y
(n−1)
f







= Ψ1(yf
, y˙f
, . . . , y(n−1)).
We can therefore deduce that a system (3.74) with a flat output yf = c
T x is
observable.
Furthermore, a flat output yf must satisfy an equation of the form
u = Ψ2(yf
, y˙f
, . . . , y
(n)
f
).
The derivative
y
(n)
f = c
T Anx + c
T An−1
bu + c
T An−2
bu˙ + . . . + c
T
bu
(n−1)
,
which reduces to
y
(n)
f = c
T Anx + c
T An−1
bu (3.77)
due to equation (3.76), must therefore be dependent on u. This means that
it is required that
c
T An−1
b = α (3.78)
holds for an arbitrary α ∈ IR\{0}.
We combine equation (3.76) and equation (3.78) and arrive at
c
T

b Ab · · · An−2
b An−1
b

| {z }
Mcontr
=

0 0 · · · 0 α

.
Here Mcontr is the controllability matrix of system (3.74). If we assume
controllability, i. e. regularity of Mcontr, all flat outputs yf = c
T x are param￾eterized by
c
T =

0 · · · 0 α

M−1
contr.
Additionally, we can introduce new coordinates254 Chapter 3. Controllability and Flatness
z =





yf
y˙f
.
.
.
y
(n−1)
f





=





c
T
c
T A
.
.
.
c
T A
n−1





x = α







0 · · · 0 1
M−1
contr

0 · · · 0 1
M−1
contrA
.
.
.

0 · · · 0 1
M−1
contrAn−1






| {z }
T
x.
The coordinate transformation z = T x transforms system (3.74) into the
form
z˙ =







y˙f
y¨f
.
.
.
y
(n−1)
f
y
(n)
f







=







z2
z3
.
.
.
zn
c
T A
nx + c
T A
n−1
bu







=







z2
z3
.
.
.
zn
c
T A
nT
−1
z







+







0
0
.
.
.
0
α







u (3.79)
with the flat output
yf = c
T x = z1.
For this transformation, it is not necessary to explicitly compute the trans￾formation z = T x. The system equations (3.79) directly follow from
z˙i = y
(i)
f = zi+1, i = 1, . . . , n − 1,
and from equation (3.77).
Without loss of generality, we multiply equation (3.79) by the factor α
−1
on both sides, and rescale the coordinates zi to
z˜i = α
−1
zi
, i = 1, . . . , n.
In this way we obtain
z˜˙ =







0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
−a0 −a1 −a2 · · · −an−1







z˜ +







0
0
.
.
.
0
1







u,
yf = αz˜1
(3.80)
for the transformed system. Here the values for a0, . . . , an−1 are the coeffi￾cients of the characteristic polynomial P(s) = det(sI − A) of A. The form
in equation (3.80) is a special case of the controller canonical form, for which
the corresponding transfer function does not include zeros.
Every system in the controller canonical form equation (3.80) is thus flat
with yf = αz˜1 as the flat output. Conversely, any controllable linear system
(3.74) with a flat output3.2. Flatness 255
yf = c
T x = α[0 · · · 1]M−1
contrx
can be transformed into the controller canonical form (3.80). The output
yf = αz˜1 with an arbitrary α 6= 0 already describes all flat outputs. Equation
(3.80) can therefore be called the flat canonical form of a linear system.
3.2.6 Verification of Flatness
In the case of linear systems, flatness is easily verifiable, since, according to
Theorem 49, a controllable linear system is flat and vice versa. Therefore,
we only need to examine the controllability of a linear system in order to
verify its flatness. Verifying the flatness of nonlinear systems is not a simple
task as in the linear case. Nevertheless, a flatness criterion also exists for
nonlinear systems [273], although it is not easily applicable. Its conditions
require the solution of partial differential equations. Thus, the original problem
is transformed into a different one which is similarly difficult to solve. Flatness
can be systematically analyzed and evaluated with a reasonable effort for
particular classes of systems only.
Members of this class of systems are control-affine systems
x˙ = a(x) + B(x) · u
or, more generally, nonlinear systems
x˙ = f(x,u),
which are either in nonlinear controller canonical form or can be transformed
into this form using a bijective transformation which is continuously differen￾tiable. From the nonlinear controller canonical form


























x˙ 1
.
.
.
x˙ n1−1
x˙ n1
x˙ n1+1
.
.
.
x˙ n2−1
x˙ n2
.
.
.
x˙ nm−1+1
.
.
.
x˙ n−1
x˙ n


























=


























x2
.
.
.
xn1
α1(x)
xn1+2
.
.
.
xn2
α2(x)
.
.
.
xnm−1+2
.
.
.
xn
αm(x)


























+


























0 0 0 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 0
β1,1(x) β1,2(x)β1,3(x) · · · β1,m−1(x) β1,m(x)
0 0 0 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 0
0 β2,2(x)β2,3(x) · · · β2,m−1(x) β2,m(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 0
0 0 0 · · · 0 βm,m(x)



































u1
u2
u3
.
.
.
um−1
um









,
(3.81)256 Chapter 3. Controllability and Flatness
the flat output can be directly determined as
y =





x1
xn1+1
.
.
.
xnm−1+1





.
Its flatness can be verified as follows. With δi denoting the orders of the m
subsystems and
n1 = δ1,
n2 = δ1 + δ2,
n3 = δ1 + δ2 + δ3,
.
.
.
nm−1 = δ1 + . . . + δm−1,
n = δ1 + . . . + δm,
the derivatives of the output signals can be stated as
y1 = x1, y2 = xn1+1, . . . , ym = xnm−1+1,
y˙1 = ˙x1 = x2, y˙2 = xn1+2, . . . , y˙m = xnm−1+2,
y¨1 = ˙x2 = x3, y¨2 = xn1+3, . . . , y¨m = xnm−1+3,
.
.
.
.
.
.
.
.
.
y
(δ1−1)
1 = xn1
, y
(δ2−1)
2 = xn2
, . . . , y(δm−1)
m = xn,
and, with
δmax = max{δi
, i = 1, . . . , m},
this yields
x = [y1 y˙1 . . . y
(δ1−1)
1
. . . ym y˙m . . . y(δm−1)
m ]
T
= Ψ1(y, y˙, . . . , y
(δmax−1)).
(3.82)
If the decoupling matrix
D(x) =








β1,1(x) β1,2(x) β1,3(x) · · · β1,m(x)
0 β2,2(x) β2,3(x) · · · β2,m(x)
0 0 β3,3(x) · · · β3,m(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · βm,m(x)







3.2. Flatness 257
composed of the elements βi,j (x) used in equation (3.81) is regular for all x,
we obtain
u = D−1
(x)


−







α1(x)
α2(x)
.
.
.
αm(x)







+







y
(δ1)
1
y
(δ2)
2
.
.
.
y
(δm)
m









for the system’s input signal. With equation (3.82), this can also be written
as
u=D−1

Ψ1(y, y˙, . . . , y
(δmax−1))











y
(δ1)
1
y
(δ2)
2
.
.
.
y
(δm)
m








−








α1(Ψ1(y, y˙, . . . , y
(δmax−1)))
α2(Ψ1(y, y˙, . . . , y
(δmax−1)))
.
.
.
αm(Ψ1(y, y˙, . . . , y
(δmax−1)))










= Ψ2(y, y˙, . . . , y
(δmax)
).
(3.83)
Based on Definition 32 and using equations (3.82) and (3.83), we have shown
that y is a flat output. Now we can formulate the following sufficient crite￾rion for MIMO systems. For SISO systems, it is not only sufficient but also
necessary [420].
Theorem 51 (Flatness of the Controller Canonical Form). A system
x˙ = f(x,u)
with x ∈ Dx,def ⊆ IRn
and u ∈ Du,def ⊆ IRm (u ∈ Du,def ⊆ IR) is flat if
(if and only if ) it is available in nonlinear controller canonical form, or if it
can be transformed into this form by means of a continuously differentiable,
bijective coordinate transformation, and if the decoupling matrix D(x) fulfills
det (D(x)) 6= 0
for all x ∈ IRn
.
Since there are quite a few systems which can be transformed into the
nonlinear controller canonical form, the above theorem is of great importance.
This is all the more true since there is no general, practically applicable crite￾rion for proving flatness. However, a necessary condition [127, 310, 390, 420]
exists. It is based on lines lying in a hypersurface. With this condition, it is at
least possible to show that a system is not flat. The following theorem which
allows such a verification is also known as the ruled manifold criterion.258 Chapter 3. Controllability and Flatness
Theorem 52 (Necessary Condition for Flatness). Let a system be given
by
x˙ = f(x,u), x ∈ Dx,def ⊆ IRn
, u ∈ Du,def ⊆ IRm,
and let the underdetermined system resulting from the elimination of u from
this system be given by
h(x, x˙) = 0
with dim(h) = n − m. If the system x˙ = f(x,u) is flat, a real vector a 6= 0
with a ∈ IRn
exists such that the algebraic equation
h(p, q + λa) = 0
is fulfilled for all real numbers λ and all p, q ∈ IRn
, for which h(p, q) = 0
holds.
We will consider the example
x˙ 1 = u, (3.84)
x˙ 2 = −x1 + u
3
. (3.85)
Inserting equation (3.84) into equation (3.85) yields the underdetermined sys￾tem of equations
h(x, x˙) = −x1 + ˙x
3
1 − x˙ 2 = 0
and the algebraic equation
h(p, q + λa) = −p1 + (q1 + λa1)
3 − (q2 + λa2) = 0
with a = [a1 a2]
T
, p = [p1 p2]
T
, and q = [q1 q2]
T
. Multiplying out the
brackets, we arrive at
a
3
1λ
3 + 3a
2
1
q1λ
2 + (3a1q
2
1 − a2)λ + q
3
1 − p1 − q2 = 0. (3.86)
We then insert
h(p, q) = q
3
1 − p1 − q2 = 0
into equation (3.86) and obtain the equation
a
3
1λ
3 + 3a
2
1
q1λ
2 + (3a1q
2
1 − a2)λ = 0
as a necessary condition for the flatness of the system, which must be fulfilled
for all λ ∈ IR. This is only possible if
a1 = a2 = 0
holds. Obviously, no a 6= 0 exists for which the condition of Theorem 52 is
fulfilled. Consequently, the system (3.84), (3.85) is not flat.
We will conclude with the autonomous systems3.3. Nonlinear State Transformations 259
x˙ = f(x),
i. e. systems which do not explicitly depend on an input variable vector u(t).
Assuming that we have found a flat output y for such a system, we obtain
x = Ψ1(y, y˙, . . . , y
(β)
)
according to the definition of flatness: Definition 32 on p. 240. If this equation
is inserted into the differential equation x˙ = f(x), it follows that
∂Ψ1
∂y
y˙ +
∂Ψ1
∂y˙
y¨ + . . . +
∂Ψ1
∂y(β)
y
(β+1) = f(Ψ1(y, y˙, . . . , y
(β)
)).
This equation, however, contradicts the requirement (3.44) of differential in￾dependence of a flat output on p. 241, i. e. the requirement that no differential
equation
ϕ(y, y˙, . . . , y
(γ)
) = 0
exists. Thus we obtain
Theorem 53 (Nonexistent Flatness of Autonomous Systems). An
autonomous system x˙ = f(x) is not flat.
This theorem can also be directly deduced from the fact that no flat output y
can exist for autonomous systems, because Requirement (3) of Definition 32,
dim(y) = dim(u), is not fulfilled.
We will apply flatness theory to the design of feedforward and feedback
control systems in Section 5.4. For supplementary literature on flatness theory,
we refer the interested reader to [1, 72, 127, 128, 129, 273, 420, 485], among
others.
3.3 Nonlinear State Transformations
3.3.1 Transformations and Transformed System Equations
In linear systems theory, it is often useful to transform the representation of
a system
x˙ = Ax + Bu (3.87)
into a different one. This is done via a coordinate transformation
x = T z (3.88)
where the vector z represents the new coordinates. The matrix T is a regular
n × n matrix. The new system is then represented by
z˙ = T
−1AT z + T
−1Bu. (3.89)260 Chapter 3. Controllability and Flatness
Often the transformation matrix T is chosen such that the new system matrix
T
−1AT is diagonal or takes the form of a companion matrix
T
−1AT =





0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
−a0 −a1 −a2 · · · −an−1





.
These representations are useful for the controller design or for directly iden￾tifying system properties.
Similarly, for nonlinear systems
x˙ = f(x,u), (3.90)
the transformations
z = q(x) or x = q
−1
(z) (3.91)
can be useful as well if it is possible to transform the system description into
a suitable form this way for purposes such as controller design. Furthermore,
as discussed in the previous section, to verify the controllability of a control￾affine system, it is very helpful to be able to transform a system representation
into the controller canonical form.
Usually, transformations (3.91) are required to be continuously differen￾tiable and bijective. The latter means that only a single z can be assigned to
every x and vice versa. As already mentioned, such a continuously differen￾tiable transformation is called diffeomorphism. The transformed representa￾tion results from inserting the transformation equation (3.91) into the system
description (3.90). This yields
dq
−1
(z)
dt = f(q
−1
(z),u),
from which
∂q
−1
(z)
∂z
· z˙ = f(q
−1
(z),u)
and, finally, the transformed system representation
z˙ =

∂q
−1
(z)
∂z
−1
f(q
−1
(z),u) = fˆ(z,u) (3.92)
follows. Similarly, for the retransformation of the z-coordinates to the x￾coordinates, it holds that
x˙ =

∂q(x)
∂x
−1
fˆ(q(x),u) = f(x,u). (3.93)3.3. Nonlinear State Transformations 261
To calculate the inverse of the Jacobian matrix of q
−1
used in equation
(3.92), we apply the rule of derivation for inverse functions, obtaining

∂q
−1
(z)
∂z
−1
=
∂q(x)
∂x




x=q−1(z)
.
In many cases, the application of this identity simplifies the computation of
the transformed system in equation (3.92).
The calculations above make it clear why the transformation functions
q(x) and q
−1
(z) have to be continuously differentiable. Clearly, if q(x) and
q
−1
(z) are continuously differentiable, the elements of the matrices

∂q
−1
(z)
∂z
−1
and ∂q(x)
∂x
are continuous. If this is not the case, the right side of the differential equa￾tions (3.92) and (3.93) is discontinuous as a result of the transformation. This,
however, would be an unreasonable situation, since the left side of the differ￾ential equations z˙ or x˙ is the time derivative of a – logically – differentiable
function, which cannot jump, i. e. is continuous. In such a case, the trans￾formed differential equation, in contrast to the original differential equation,
would not be solvable.
For a linear system (3.87) and a linear transformation (3.88), it is easy to
show that the transformed system representation (3.89) follows from equation
(3.92). This is because the equations
x = q
−1
(z) = T z
and
∂q
−1
(z)
∂z
= T
hold.
Next we will derive an important theorem on diffeomorphisms, i. e. bijec￾tive and continuously differentiable coordinate transformations q(x). It holds
that
q
−1
(q(x)) = x
and for the derivative of this identity
∂q
−1
(z)
∂z
·
∂q(x)
∂x
= I.
Here I is the identity matrix. Accordingly, the Jacobian matrix ∂q(x)/∂x of
a diffeomorphism q(x) must be regular.
The regularity of the Jacobian matrix, however, is not only necessary for a
diffeomorphism; it is also sufficient. If the Jacobian matrix of q(x) is regular262 Chapter 3. Controllability and Flatness
at a point x0, according to the implicit function theorem [167], a continuously
differentiable inverse function q
−1
exists in a neighborhood of x0 for the con￾tinuously differentiable transformation function q. If the Jacobian matrix is
not only regular at a point x0 but for an open set, there is at least a subset
where the inverse function q
−1
exists for all points in this set. If not only a
diffeomorphism q but also its inverse q
−1
bijectively maps IRn
into IRn
, we
call it global. If this is not the case, we call the diffeomorphism local. Summa￾rizing the results above, we can use the following theorem to verify whether
a mapping z = q(x) is a local diffeomorphism or not.
Theorem 54 (Local Diffeomorphism). Let a function z = q(x) be given
which is defined on a set Dx,def ⊆ IRn
and maps Dx,def into IRn
. If and only
if
(1) q is continuously differentiable on a set Dx ⊆ Dx,def and
(2) the inequality det(∂q(x)
∂x
) 6= 0 holds for all x ∈ Dx,
then a set S ⊆ Dx exists where z = q(x) is a local diffeomorphism.
The above theorem guarantees the existence of the diffeomorphism only for
a set S – which generally has to be identified – and not for the domain of
definition Dx,def itself. An illustrative example [481] is
z = q(x) = "
e
x1 cos(x2)
e
x1
sin(x2)
#
, Dx,def = IR2
.
The determinant
det(∂q(x)
∂x
) = det("
e
x1 cos(x2) −e
x1
sin(x2)
e
x1
sin(x2) e
x1
cos(x2)
#
) = e
2x1
is unequal to zero for all x ∈ IR2
. Nevertheless, q is not a global diffeomor￾phism because z = q(x) is not a one-to-one and onto, i. e. not a bijective,
mapping for all x ∈ IR2
. For example, the two vectors
xa = 0 and xb =

0
2π

both result in
q(xa) = q(xb) = 
1
0

.
In some cases, we are interested in having a global diffeomorphism to map
IRn
into IRn
and vice versa. The best, but mostly very laborious way to verify
whether a mapping q is a global diffeomorphism is calculating its inverse q
−1
.
The following theorem [154, 245, 394, 481], Hadamard’s global inverse function
theorem, provides us with a less laborious way, but does not yield the inverse.3.3. Nonlinear State Transformations 263
Theorem 55 (Global Diffeomorphism). A function z = q(x) which
maps IRn
into IRn
is a global diffeomorphism if and only if
(1) q is continuously differentiable everywhere on IRn
,
(2) the inequality det(∂q(x)
∂x
) 6= 0 holds for all x ∈ IRn
, and
(3) the limit |q(x)| → ∞ results whenever |x| → ∞.
In general, the transformation equations may also be dependent on the
input variable vector u and possibly on some temporal derivatives u
(j)
,
j = 1, . . . , i, i. e. they can be stated as
z = q(x,u,u˙, . . . ,u
(i)
)
or
x = q
−1
(z,u,u˙, . . . ,u
(i)
). (3.94)
If we now transform the system
x˙ = f(x,u)
to z-coordinates, we obtain
dq
−1
(z,u, . . . ,u
(i)
)
dt =
∂q
−1
(z,u, . . . ,u
(i)
)
∂z
·z˙+
X
i
j=0
∂q
−1
(z,u, . . . ,u
(i)
)
∂u(j)
· u
(j+1)
= f(q
−1
(z,u, . . . ,u
(i)
),u)
after inserting equation (3.94) into the system equation x˙ = f(x,u). The
transformed system equation follows as
z˙ =

∂q
−1
(z,u,. . . ,u
(i)
)
∂z
−1
·

f(q
−1
(z,u,. . . ,u
(i)
),u)−
X
i
j=0
∂q
−1
(z,u,. . . ,u
(i)
)
∂u(j)
u
(j+1)
.
(3.95)
The equation for the retransformation can be derived in a comparable fashion.
It generally requires some effort to determine a transformation
z =q(x)
or
z =q(x,u,u˙, . . . ,u
(i)
)
which induces a change of coordinates resulting in the desired system rep￾resentation. This is already the case for linear systems when diagonalizing a
system, for example. To do this, the eigenvectors which make up the columns
of the transformation matrix T must be determined. For the nonlinear case,
the effort required is often much greater. We will discuss this problem in
Section 3.3.4.264 Chapter 3. Controllability and Flatness
3.3.2 Illustrative Example
Next we will consider a nonlinear example. Let a system be described by
x˙ =
1
x
. (3.96)
This differential equation is not defined for x = 0. We will therefore exclude
the point x = 0 from our considerations in the following. We select
z = q(x) = e
sgn(x)x
2
/2
(3.97)
as the transformation in order to simplify the representation of the differential
equation and to calculate its solution. The retransformation is given by
x = q
−1
(z) = sgn ￾
ln ￾
z
2
 

ln ￾
z
2


1/2
. (3.98)
The transformation rules (3.97) and (3.98) are bijective, continuously differ￾entiable, and q maps the space of real numbers IR to the interval (0, ∞).
Figure 3.21 shows the graph of q(x). We are now able to calculate the deriva￾tive
∂q−1
(z)
∂z =


ln ￾
z
2


−1/2
z
and, consistent with equation (3.92), we obtain
z˙ =
 

ln ￾
z
2


−1/2
z
!−1
·
 
1
sgn (ln (z
2))|ln (z
2)|
1/2
!
for the transformed system. Simplifying the latter, we arrive at
8
6
4
2
e
sgn(x)x
2
/2
-1 0 1 x
q(x)
Fig. 3.21: Graph of the function z =3.3. Nonlinear State Transformations 265
z˙ = z sgn ￾
ln ￾
z
2
 =
(
z, z > 1,
−z, 0 < z < 1.
(3.99)
By means of the transformation (3.97), we have managed to transform the
nonlinear system description (3.96) into one, or, more precisely, into two linear
system descriptions. Note that the two system descriptions (3.96) and (3.99)
are equivalent, since the transformation is bijective.
Finally, we solve the differential equation (3.96) by first determining the
solution of equation (3.99). Its solution is
z(t) = (
z0e
t
, z > 1,
z0e
−t
, 0 < z < 1,
with an initial value z0 = z(0). After retransformation according to equation
(3.98), we obtain the solution of the nonlinear system (3.96) as
x(t) = q
−1
(z(t)) =



sgn ￾
ln ￾
z
2
0
e
2t
 ·


ln ￾
z
2
0
e
2t


1/2
, z > 1,
sgn ￾
ln ￾
z
2
0
e
−2t
 ·


ln ￾
z
2
0
e
−2t


1/2
, 0 < z < 1.
(3.100)
Now we replace the initial value z0 with the initial value x0 = x(0), obtaining
x(0) = sgn ￾
ln ￾
z
2
0
 ·

ln ￾
z
2
0


1/2
,
from which
sgn(x0) = sgn ￾
ln ￾
z
2
0

results. Using equation (3.97), the relation
x
2
0 = ln ￾
z
2
0

, z0 > 1,
−x
2
0 = ln ￾
z
2
0

, 0 < z0 < 1,
between the initial values x0 and z0 follows. Inserting this result into equation
(3.100) yields
x(t) =



sgn (x0) ·

x
2
0 + 2t


1/2
, x0 > 0,
sgn (x0) ·

−x
2
0 − 2t


1/2
, x0 < 0,
= sgn (x0) ·
￾
x
2
0 + 2t
1/2
, x 6= 0,
as the solution of the differential equation x˙ = 1/x. We will calculate this
solution again in Exercise 3.16, using a simpler transformation equation than
266 Chapter 3. Controllability and Flatness
3.3.3 Example: Park Transformation
One of the most important nonlinear transformations used in electrical engi￾neering is the Park transformation. It allows us to simplify the model equations
of synchronous machines and induction machines (asynchronous machines)
significantly. In the following, we will consider induction machines, for which
the rotors are provided with a three-phase current of adjustable frequency ∆ω
via slip rings and a current converter. These are called doubly fed induction
machines. They are often used as generators for wind turbines. In this applica￾tion, they have a set of advantages compared to other generator designs: they
only require a small current converter, which merely converts the rotor current
which is fed into the rotor coils via the slip rings. The rotor voltage not only
allows us to control the effective power but also the reactive power. It is also
an advantage that the stator voltage usabc can be set to be the line voltage.
Furthermore, the frequency of the stator currents can remain constant and
consistent with the line frequency, even for varying angular velocities of the
rotor blade due to varying wind velocities. A disadvantage, however, is the
slip rings, since they wear out and require maintenance. Figure 3.22 illustrates
the structure of a wind turbine with a doubly fed induction generator.
=
~ =
~
Rotor
Slip rings
Gear box
Pm + Pe
DFIM
P m
P m
Pe Pe
usabc
urabc
Converter
Fig. 3.22: Wind turbine with a doubly fed induction machine (DFIM). The graph
also shows the energy flow associated with the absorbed wind power Pm and the
power of the rotor circuit Pe, where losses are assumed to be zero. The vectors usabc
and urabc correspond to the stator and rotor voltages of the phases a, b, c.3.3. Nonlinear State Transformations 267
First we will derive the model of the doubly fed induction machine [247,
254]. Figure 3.23 shows the stator and rotor coils, as well as their positions
relative to each other. The rotor of the induction generator, powered by the
turbine, turns with the mechanical frequency
ωm = Θ. ˙
Here the geometric angle Θ denotes the angle between the rotor coil in phase
ar and the stator coil in phase as, as shown in Figure 3.23. The indices r and s
stand for the rotor and stator, respectively. The rotor currents ira, irb, and irc
of frequency ∆ω are fed into the rotor coils via the slip rings and the current
converter. For a machine with p pole pairs, the magnetic field of the rotor
revolves with the frequency
∆ω
p
= ˙ϕ
around it. Here ϕ is the rotation angle of the rotor’s magnetic field. With
respect to the stator, the magnetic field therefore rotates at a frequency of
ωs = ωm +
∆ω
p
.
The rotor field induces a magnetic field in the stator coils that also rotates at
ωs = β˙ = Θ˙ + ˙ϕ,
isa
isb
isc
ira irb
irc
usa
usb
usc
ura
urb
urc
Θ
ωm
Rotor
Stator
Fig. 3.23: Rotor and stator coils and their relative positions268 Chapter 3. Controllability and Flatness
where β = Θ + ϕ is the rotation angle of the stator field. The frequency of
the stator field β˙ therefore corresponds to the sum of the mechanical rota￾tion frequency Θ˙ and the rotational frequency ϕ˙ of the rotor field. Since the
induction machine has p pole pairs, a voltage of frequency
ωelectric = p · ωs
is produced at the stator terminals. The relative difference
s =
ωs − ωm
ωs
=
ϕ˙
ωs
(3.101)
between the frequency ωs of the stator field and the mechanical frequency ωm
of the rotor is called slip. Using the current converter, we can adjust the rotor
frequency ∆ω. Thus we are able to regulate the generator frequency
ωelectric = p · ωs = p · ωm + ∆ω
via the rotor frequency ∆ω such that it is identical to the line frequency even
if the wind turbine has a varying rotational frequency ωm.
For the stator voltages of the phases as, bs, cs, we obtain



usa
usb
usc



| {z }
usabc
=



Ψ˙
sa
Ψ˙
sb
Ψ˙
sc



| {z }
Ψ˙
sabc
+



Rs 0 0
0 Rs 0
0 0 Rs



| {z }
Rs



isa
isb
isc



| {z }
isabc
, (3.102)
where Ψsa is the magnetic flux linkage, Ψ˙
sa is the induced voltage of the coil
of phase as, isa is the current that flows through the coil, and Rs is its ohmic
resistance. Similarly, for the rotor voltages, we obtain



ura
urb
urc



| {z }
urabc
=



Ψ˙
ra
Ψ˙
rb
Ψ˙
rc



| {z }
Ψ˙
rabc
+



Rr 0 0
0 Rr 0
0 0 Rr



| {z }
Rr



ira
irb
irc



| {z }
irabc
(3.103)
with the resistance Rr of each rotor coil, and the corresponding voltages,
currents, and flux linkages.
The stator and the rotor coils are coupled via the magnetic fluxes Ψrabc and
Ψsabc. Since the rotor rotates with frequency ωm = Θ˙
, the mutual inductance
of the rotor coils and stator coils changes according to the rotor angle Θ.
For the flux linkage of the stator coil of phase as, we obtain
Ψsa=Lssisa+Lms(isb+isc)+Lmsr
ira cos(Θ)+irb cos(Θ+
2π
3
)+irc cos(Θ−
2π
3
)

,
(3.104)3.3. Nonlinear State Transformations 269
while for the rotor coil of phase ar
Ψra=Lsrira+Lmr(irb+irc)+Lmsr
isa cos(Θ)+isb cos(Θ−
2π
3
)+isc cos(Θ+
2π
3
)

(3.105)
holds.
Here Lss is the self-inductance of the stator coils, Lsr is that of the rotor
coils, Lms is the mutual inductance of two stator coils, while Lmr is that of
two rotor coils and Lmsr is the mutual inductance of the stator coil of phase
as with respect to the rotor coil of phase ar at Θ = 0.
Using
isa + isb + isc = 0, ira + irb + irc = 0,
the abbreviations
Ls = Lss − Lms, Lr = Lsr − Lmr,
and equations (3.104) and (3.105), the equations
Ψsa = Lsisa + Lmsr 
ira cos(Θ) + irb cos(Θ +
2π
3
) + irc cos(Θ −
2π
3
)

,
Ψra = Lrira + Lmsr 
isa cos(Θ) + isb cos(Θ −
2π
3
) + isc cos(Θ +
2π
3
)

follow. Including phases b and c as well, we obtain








Ψsa
Ψsb
Ψsc








=








Ls 0 0
0 Ls 0
0 0 Ls
















isa
isb
isc








+Lmsr








cos(Θ) cos(Θ +
2π
3
) cos(Θ −
2π
3
)
cos(Θ −
2π
3
) cos(Θ) cos(Θ +
2π
3
)
cos(Θ +
2π
3
) cos(Θ −
2π
3
) cos(Θ)
















ira
irb
irc








|{z}
Ψsabc
| {z }
Ls
|{z}
isabc
| {z }
Γs(Θ)
|{z}
irabc
(3.106)
for the flux linkages of the stator, and








Ψra
Ψrb
Ψrc








=








Lr 0 0
0 Lr 0
0 0 Lr
















ira
irb
irc








+Lmsr








cos(Θ) cos(Θ −
2π
3
) cos(Θ +
2π
3
)
cos(Θ +
2π
3
) cos(Θ) cos(Θ −
2π
3
)
cos(Θ −
2π
3
) cos(Θ +
2π
3
) cos(Θ)
















isa
isb
isc








|{z}
Ψrabc
| {z }
Lr
|{z}
irabc
| {z }
Γr(Θ)
|{z}
isabc
(3.107)270 Chapter 3. Controllability and Flatness
for the flux linkage of the rotor. In vectorial form, we arrive at

Ψ˙
sabc
Ψ˙
rabc 
=

usabc
urabc 
−

Rs 0
0 Rr
  isabc
irabc 
(3.108)
and

Ψsabc
Ψrabc 
=

Ls 0
0 Lr
  isabc
irabc 
+ Lmsr 
0 Γs(Θ)
Γr(Θ) 0
  isabc
irabc 
(3.109)
from equations (3.102), (3.103), (3.106), and (3.107).
In the above formula, we need to know the angle Θ of the rotor position.
It can be calculated from the mechanical equation of motion
Θ¨ =
M
J
, (3.110)
where M is the rotor shaft’s effective torque, i. e. the difference between the
drive torque and the machine torque, and J is the rotation components’ mo￾ment of inertia. Combined, equations (3.108), (3.109), and (3.110) make up
the model of the induction generator.
In equation (3.109), the mutual inductances LmsrΓs(Θ) and LmsrΓr(Θ)
depend on the rotor angle Θ(t) and are therefore time-dependent. In order to
eliminate the flux linkage in equation (3.108) via equation (3.109), it would
be necessary to calculate the time derivative of equation (3.109), and thus
the time derivatives of the mutual inductances LmsrΓs(Θ) and LmsrΓr(Θ) as
well. This would involve laborious calculations and a very complex system
description. Therefore, the system variables are transformed to a coordinate
space in which the mutual inductances are constant by applying the Park
transformation [254]. The latter is a nonlinear state transformation. The re￾sulting coordinate system, which is referred to as the dq0-coordinate system,
rotates around the stator-fixed abc-coordinate system with synchronous fre￾quency ωs. The q-axis precedes the d-axis by 90◦
. Below only the electric and
magnetic state variables u, i, and Ψ will be transformed, not the mechanical
quantities.
The transformation equations can be stated as

usdq0
urdq0 
=

T(β) 0
0 T(ϕ)
 usabc
urabc 
,

isdq0
irdq0 
=

T(β) 0
0 T(ϕ)
  isabc
irabc 
,

Ψsdq0
Ψrdq0 
=

T(β) 0
0 T(ϕ)
 Ψsabc
Ψrabc 
(3.111)
with3.3. Nonlinear State Transformations 271
T(α) = 2
3









cos(α) cos(α −
2π
3
) cos(α +
2π
3
)
− sin(α) − sin(α −
2π
3
) − sin(α +
2π
3
)
1
2
1
2
1
2









,
T
−1
(α) =







cos(α) − sin(α) 1
cos(α −
2π
3
) − sin(α −
2π
3
) 1
cos(α +
2π
3
) − sin(α +
2π
3
) 1







.
The transformation (3.111) depends on the angles β(t) and ϕ(t), and therefore
also on the state variable
Θ = β − ϕ.
It is thus a transformation consistent with equation (3.91), i. e. a diffeomor￾phism which nonlinearly depends on the state of the system.
By inserting the transformation equations (3.111) into the system equation
(3.108), we arrive at
d
dt T
−1
(β) 0
0 T
−1
(ϕ)
  Ψsdq0
Ψrdq0  =

T
−1
(β) 0
0 T
−1
(ϕ)
 usdq0
urdq0 
−

Rs 0
0 Rr
 T
−1
(β) 0
0 T
−1
(ϕ)
  isdq0
irdq0 
,
from which, with T RsT
−1 = Rs and T RrT
−1 = Rr,




Ψ˙
sdq0
Ψ˙
rdq0




+




T(β) 0
0 T(ϕ)








d T
−1
(β)
dt 0
0
d T
−1
(ϕ)
dt








Ψsdq0
Ψrdq0




=




usdq0
urdq0




−




Rs 0
0 Rr








isdq0
irdq0




(3.112)
follows. Using
d T
−1
(α)
dt
= ˙α







− sin(α) − cos(α) 0
− sin(α −
2π
3
) − cos(α −
2π
3
) 0
− sin(α +
2π
3
) − cos(α +
2π
3
) 0







,
we obtain272 Chapter 3. Controllability and Flatness
T(α) ·
d T
−1
(α)
dt
= ˙α


0 −1 0
1 0 0
0 0 0

.
| {z }
K
With β˙ = ωs and ϕ˙ = sωs, equation (3.112) leads to

Ψ˙
sdq0
Ψ˙
rdq0 
+

ωsK 0
0 sωsK
  Ψsdq0
Ψrdq0 
=

usdq0
urdq0 
−

Rs 0
0 Rr
  isdq0
irdq0 
. (3.113)
This is independent of the angle Θ.
The algebraic equation (3.109) still has to be transformed via equation
(3.111), a calculation which yields

Ψsdq0
Ψrdq0
=

Ls 0
0 Lr
isdq0
irdq0
+Lmsr
T(β) 0
0 T(ϕ)
 0 Γs(Θ)
Γr(Θ) 0
T
−1
(β) 0
0 T
−1
(ϕ)
isdq0
irdq0
.
(3.114)
We now determine

T(β) 0
0 T (ϕ)
  0 Γs(Θ)
Γr(Θ) 0
  T
−1
(β) 0
0 T
−1
(ϕ)

=
3
2

0 U
U 0

with
U =


1 0 0
0 1 0
0 0 0

.
Using this result, and the abbreviation Lm = 3Lmsr/2, we then simplify equa￾tion (3.114) in order to get the transformed equation

Ψsdq0
Ψrdq0 
=

Ls 0
0 Lr
  isdq0
irdq0 
+ Lm

0 U
U 0
  isdq0
irdq0 
, (3.115)
i. e. an equation independent of the mechanical rotation angle Θ. The equa￾tions (3.113) and (3.115) represent the electrical system’s equations for the
doubly fed induction machine.
Having arrived at the transformed system equations (3.113) and (3.115),
we have actually reached our goal of making the mutual inductivities of the
original system equations (3.108) and (3.109) independent of the rotation
angle Θ by applying the Park transformation (3.111). In addition, we will
now insert equation (3.115) into equation (3.113) to eliminate the magnetic
flux linkage. With
KU = K,
we achieve

Ls LmU
LmU Lr
  ˙isdq0
˙irdq0 
+

ωsKLs + Rs ωsLmK
sωsLmK sωsKLr + Rr
  isdq0
irdq0 
=

usdq0
urdq0 3.3. Nonlinear State Transformations 273
for the electrical equations of the doubly fed induction machine, which are
independent of the flux linkage. For the mechanical equation of the machine,
equation (3.110), we obtain
Θ¨ = −sω˙ s =
M
J
or s˙ = −
Θ¨
ωs
= −
M
ωsJ
,
assuming a constant stator frequency ωs in equation (3.101). Similarly, for
synchronous machines, the Park transformation allows for a simplification of
the system equations, i. e. an independence of the mutual inductances relative
to the rotor position.
3.3.4 Determining the Transformation Rule
So far, we have assumed that the transformation rules
z = q(x) = p
−1
(x) and x = q
−1
(z) = p(z) (3.116)
which transform a system
x˙ = f(x,u) (3.117)
into the desired form
z˙ = fˆ(z,u) (3.118)
and back again are known. However, generally this is not the case. In fact the
desired system representation (3.118) is usually given, and we need to deter￾mine the corresponding transformation equation, the diffeomorphism (3.116).
For this purpose, we first insert equation (3.116) into equation (3.117)
which allows us to obtain
z˙ =

∂p(z)
∂z
−1
· f(p(z),u). (3.119)
Next, we equate (3.119) with the desired form (3.118) of the system, yielding
the conditional equation
∂p(z)
∂z
· fˆ(z,u) − f(p(z),u) = 0 (3.120)
for the diffeomorphism x = p(z) which we have been seeking.
This first-order partial differential equation, however, is only analytically
solvable in a few cases, which largely restricts its practical use. Nevertheless,
this result is helpful, since it demonstrates the difficulty of finding suitable
nonlinear state transformations. Later, in Section 5.2, we will see that, at
least for the class of control-affine systems
x˙ = a(x) + B(x) · u,
these transformations can often be determined.274 Chapter 3. Controllability and Flatness
3.3.5 Illustration Using Linear Systems
As a simple example of an application of equation (3.120), we will take a linear
system
x˙ = Ax + b · u, (3.121)
which is to be transformed into the desired form
z˙ = Azˆ + ˆb · u. (3.122)
Using equation (3.120), the required transformation rule is determined from
∂p(z)
∂z
(Azˆ + ˆb · u) − (Ap(z) + b · u) = 0.
It follows that
∂p(z)
∂z
Azˆ − Ap(z) + 
∂p(z)
∂z
ˆb − b

u = 0.
Since the above equation must hold for all z ∈ IRn
and u ∈ IR, we obtain
∂p(z)
∂z
Azˆ − Ap(z) = 0,
∂p(z)
∂z
ˆb − b = 0
(3.123)
from the previous equation. Our approach to solving this partial differential
equation utilizes the linear transformation (3.88), i. e.
x = T z = p(z) (3.124)
with the n×n matrix T which was previously used in Section 3.3.1. Inserting
this transformation into equation (3.123) yields
∂T z
∂z
Azˆ − AT z = 0 ⇔ T Aˆ − AT = 0, (3.125)
∂T z
∂z
ˆb − b = 0 ⇔ Tˆb − b = 0.
We note that, according to Theorem 55, the Jacobian matrix
∂p(z)
∂z
= T
must be regular. The known result from linear systems theory follows as
Aˆ = T
−1AT, (3.126)
ˆb = T
−1
b.3.3. Nonlinear State Transformations 275
From equation (3.126) we can conclude that the matrix Aˆ of the desired
system (3.122) must possess the same eigenvalues as the system matrix A. If
this is not the case, no diffeomorphism (3.124) exists to transform the system
(3.121) into the form of equation (3.122).
It remains to be determined how the matrix T can be computed from A
and Aˆ, or, more precisely, from equation (3.125),
T Aˆ − AT = 0. (3.127)
First, note that the matrix T is regular if and only if A and Aˆ can be
transformed to the same Jordan canonical form J [258], meaning that the
matrices A and Aˆ must have the same eigenvalues with the same associated
algebraic and geometric multiplicities[1]. The matrices V and Vˆ transform
the matrices A and Aˆ into the Jordan canonical form J according to
V
−1AV = J,
Vˆ
−1
AˆVˆ = J.
The solution of equation (3.127) is [144, 258]
T = V KVˆ
−1
, (3.128)
where K is a block diagonal matrix. Let the matrices A and Aˆ possess the
eigenvalues λ1, . . . , λk. Each eigenvalue λi
is of algebraic multiplicity ri
, and
of geometric multiplicity si
. Then the associated Jordan canonical form [155]
is consistent with the block diagonal matrix
J =






















J1,1 · · · 0 0 · · · 0 · · · 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · ·J1,s1 0 · · · 0 · · · 0 · · · 0
0 · · · 0 J2,1 · · · 0 · · · 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 0 · · ·J2,s2
· · · 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 0 · · · 0 · · ·Jk,1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 0 · · · 0 · · · 0 · · ·Jk,sk






















, where Ji,j =







λi 1 0 · · · 0 0
0 λi 1 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · λi 1
0 0 0 · · · 0 λi







,
and where the matrices Ji,j are pij × pij matrices. It holds that
[1] The algebraic multiplicity ri of an eigenvalue λi of matrix X determines the
number of linear factors (s − λi) of the characteristic polynomial of X. The
number of linearly independent eigenvectors of an eigenvalue λi corresponds to
its geometric multiplicity si.276 Chapter 3. Controllability and Flatness
1 ≤ pij ≤ ri and pi1 + pi2 + . . . + pisi = ri
.
The matrix K in equation (3.128) is of the particular block diagonal form
K =






















K1,1· · · 0 0 · · · 0 · · · 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · ·K1,s1 0 · · · 0 · · · 0 · · · 0
0 · · · 0 K2,1· · · 0 · · · 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 0 · · ·K2,s2
· · · 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 0 · · · 0 · · ·Kk,1· · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 0 · · · 0 · · · 0 · · ·Kk,sk






















with the submatrices
Ki,j =








k1(i, j) k2(i, j) k3(i, j) · · · kpij (i, j)
0 k1(i, j) k2(i, j) · · · kpij−1(i, j)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · k2(i, j)
0 0 0 · · · k1(i, j)








.
The submatrices Ki,j are upper pij × pij triangular Toeplitz matrices with
pij arbitrary parameters kl(i, j), where l = 1, . . . , pij . Because of the freely
selectable parameters, the solution (3.128) is not unique. Hence, an infinite
number of transformations exist which transform the system x˙ = Ax + bu
into a form consistent with z˙ = Azˆ + ˆbu.
Where A and Aˆ possess exactly n different eigenvalues, the Jordan canon￾ical form is diagonal. This means the matrix K is a diagonal matrix and the
solution (3.128) of the conditional equation (3.127) can be easily determined
from T. In this case, the matrices V and Vˆ are made up of the eigenvectors
of the matrices A and Aˆ.
Based on this example of a linear system, it seems plausible that for non￾linear systems as well, the choice of the transformed system, i. e. the system
z˙ = fˆ(z,u) into which we wish to transform the original system x˙ = f(x,u),
is severely limited. (In the linear case described above, this limitation is due
to the requirement of identical eigenvalues and Aˆ and A having an identi￾cal Jordan canonical form.) Consequently, a diffeomorphism does not exist
for any choice of z˙ = fˆ(z,u); and, even if it did exist, determining it could
become very involved.3.4. Exercises 277
3.4 Exercises
Exercise 3.1 Analyze to what extent the system x˙ = xu is controllable.
Exercise 3.2 Explain why the system
x˙ =


x2
α(x)
x3

 +


0
β(x)
0

u
is not controllable.
Exercise 3.3 Let us examine the service ship from Section 3.1.1, p. 211, in
the Cartesian coordinate system, shown in Figure 3.24.
(a) Create the state-space model of the ship with the position coordinates x1
and x2 and the course angle x3 as state variables. The input variables
are the ship speeds v1 and v2 in the longitudinal and sideways directions,
respectively, and the change in time v3 of the course angle x3.
(b) Show that the model from (a) is omnidirectionally controllable and flat.
(c) The speed vector v =

v1 v2 v3
T
results from the rotational speeds of the
transverse thrusters u2 and u3 in the bow and stern and the rotational
speed u1 of the ship’s propeller. This relationship can be modeled by
Mv˙ + Dv = u, u =

u1 u2 u3
T
. (3.129)
Determine the overall model with [x
T
v
T
]
T
as the state vector resulting
from the model from (a) above and equation (3.129). Examine whether
x1
x2
v2
v1
x3, v3
Fig. 3.24: State and input variables of the service ship model278 Chapter 3. Controllability and Flatness
the overall model is omnidirectionally controllable, only controllable, or
neither of the above. The matrix M is regular.
(d) Give a flat representation of the overall system.
Exercise 3.4 Given the differentiable vector functions g1
(x), g2
(x), g3
(x),
and g4
(x), prove
(a) [g1
, g1
] = 0,
(b) [g1
, g2
] = −[g2
, g1
] (antisymmetry),
(c) [g1
, g2
] = −[−g1
, g2
] = −[g1
, −g2
],
(d) [g1
, c1g2 + c2g3
] = c1[g1
, g2
] + c2[g1
, g3
], c1, c2 ∈ IR,
(e) [[g1
, g2
], [g3
, g4
]] = [g1
, [g2
, [g3
, g4
]]] − [g2
, [g1
, [g3
, g4
]]]; use the Jacobi
identity [g1
, [g2
, g3
]] + [g3
, [g1
, g2
]] + [g2
, [g3
, g1
]] = 0,
(f) [g1
, [g2
, [g2
, g1
]]] = [g2
, [g1
, [g2
, g1
]]],
(g) [λ(x)g1
, µ(x)g2
] =λ(x)µ(x)[g1
, g2
] + λ(x)

∂µ(x)
∂x
g1
(x)

g2
(x)
− µ(x)

∂λ(x)
∂x
g2
(x)

g1
with λ(x) and µ(x) being differentiable scalar functions.
Exercise 3.5 A polymer electrolyte membrane (PEM) fuel cell has two elec￾trodes which are separated by a polymer membrane; see Figure 3.25. At the
anode, the hydrogen molecules H2 being supplied split into two protons H+,
setting two electrons free. The protons diffuse through the membrane and at
the cathode they encounter the oxygen molecules O2 being supplied to that
point, where they bind together into water molecules H2O. In the course of
this, each water molecule binds two electrons. Before this reaction takes place,
the electrons have passed as an external current i through the load and the
line from the anode to the cathode. This process can be modeled [328] as be￾low using the relative gas pressures x1, x2, and x3 of the hydrogen, the oxygen,
and the steam as state variables, along with the volume flows u1 and u2 of
the hydrogen and the oxygen being supplied and the electric current i = u3
as input variables:
x˙ 1 = a(p − x1)u1 − 2ac(p − x1)u3,
x˙ 2 = b(p − x2)u2 − bc(p − x2)u3,
x˙ 3 = −bx3u2 + 2bc(p − x3)u3.
The parameters a, b, c, and p are constants. Determine whether the model of
the fuel cell is
(a) locally omnidirectionally controllable,
(b) globally omnidirectionally controllable on IR3
,
(c) locally small-time locally controllable,
(d) globally small-time locally controllable on IR3
.3.4. Exercises 279
i
O2
H2
H2O
Anode Cathode
Fig. 3.25: Polymer electrolyte membrane fuel cell
Exercise 3.6 Let us consider the system
x˙ 1 = f1(x1) + h1(x1)x2
x˙ 2 = f2(x1, x2) + h2(x1, x2)x3
x˙ 3 = f3(x1, x2, x3) + h3(x1, x2, x3)u.
(a) Identify the diffeomorphism which transforms the system into the nonlin￾ear controller canonical form.
(b) Identify the vector b(z) of the transformed system z˙ = a(z) + b(z)u, i.e.
of the nonlinear controller canonical form for the case in question.
(c) Is the system always controllable?
Exercise 3.7 Let us consider the linear system
x˙ =

0 1
−1 0
x +

0
1

u.280 Chapter 3. Controllability and Flatness
(a) Identify a flat output and the corresponding flat system representation.
(b) Determine the input signal u(t) of a feedforward control for the system
such that y = 1 − e
−t − te−t
applies.
Exercise 3.8 Determine a flat output and the flat system description for the
following systems:
(a) the satellite model (1.8) on p. 8 in Section 1.1.4,
(b) the robot model (3.22) on p. 224 in Section 3.1.5, and
(c) the robot model (3.32) on p. 234 in Section 3.1.7.
Exercise 3.9 Let us examine the separately excited direct-current motor
(2.119) on p. 183 in Section 2.4.11. We will assume that the load torque ML
is constant. In this case the motor has only the two input variables ur and us.
(a) Identify a flat output.
(b) Determine the flat system representation.
Exercise 3.10 Let us examine an active suspension system of the type used
in automobiles, among other applications. Because the suspension and shock￾absorbing components of an automobile’s wheels have an identical construc￾tion, it is sufficient to view the suspension mechanism of only one wheel, as
shown in Figure 3.26. Its state-space model is given by
x˙ 1 = x2,
x˙ 2 = −
1
mc
[Fs(x1, x3) + Fd(x2, x4)] + 1
mc
u,
x˙ 3 = x4,
x˙ 4 = −
ct
ms
x3 +
1
ms
[Fs(x1, x3) + Fd(x2, x4)] −
1
ms
u +
ct
ms
xd
with the spring and damping forces as follows:
Fs(x1, x3) = cs(x1 − x3) + cnls(x1 − x3)
3
,
Fd(x2, x4) = d(x2 − x4) + dnl(x2 − x4)
2
sgn(x2 − x4).
Here x1 and x3 are the spring travels and x2 and x4 are the corresponding
speeds. In addition, mc is one-fourth of the mass of the automobile without
the mass of the mounting rod and wheel; ms is the mass of the mounting rod
including the wheel; cs, cnls, and ct are the spring constants; and d and dnl
are the damping coefficients. An additional force u introduced into the system
serves as the control variable. Assume that the unevenness of the road, which
is represented by xd, is measured by a radar sensor, and xd is therefore known.
(a) Demonstrate that y = mcx1 + msx3 is a flat output and give the flat
system description.
(b) Is the system controllable and if so, why?
(c) Now introduce the flat output and its derivatives as the new coordinates
z
T =

y y˙ y¨ ˙˙˙y

.3.4. Exercises 281
Fs F d
mc
u
x3
x1
xd
ct
ms
d cs
Fig. 3.26: Active suspension of a wheel
Formulate z = t(x) and calculate z˙ = t˙(x). Here, replace x with the new
coordinates z. Which system representation z˙ = f(z) results, and what
is it called?
Exercise 3.11 Determine the inverse function of
z = q(x) = 
e
x1 cos(x2)
e
x1
sin(x2)

.
Exercise 3.12 Let the differential equation
x˙ = x − x
2
with x0 = x(0) be given.
(a) Identify the transformation x = p(z) which transforms the above differ￾ential equation into the linear differential equation z˙ = −z + 1, z0 = z(0).
(b) State the solution of the differential equation x˙ = x − x
2
for x0 = x(0).
Exercise 3.13 Let us examine the system
x˙ 1 = sin(x2),
x˙ 2 = −x1 + u.282 Chapter 3. Controllability and Flatness
(a) Calculate the inverse x = q
−1
(z) of the diffeomorphism z = q(x) where
z1 = x1 and z2 = sin(x2).
(b) Determine the transformed system description z˙ = f(z, u).
(c) What form does z˙ = f(z, u) take?
Exercise 3.14 Determine whether the following mappings are global or local
diffeomorphisms, or not diffeomorphisms at all. If there is a local diffeomor￾phism, identify the sets Mx and Mz which are bijectively mapped onto each
other. In each case, determine the inverse mapping if possible.
(a) x = z
3
,
(b) x = T z + a, T ∈ IRn×n
, a ∈ IRn
,
(c) x=
"
z1 cos(z2)
z1 sin(z2)
#
(polar coordinates),
(d) x=
"
z
3
1 + z2
z
3
2 − z1
#
,
(e) x=
"
e
−z1z2
e
z1z2
#
,
(f) x=



(R + z1 cos(z2)) cos(z3)
(R + z1 cos(z2)) sin(z3)
z1 sin(z2)



with 0<z1 < R (torus coordinates).
Figure 3.27 shows the polar and torus coordinates.
x2 x3
x2
z1
z1
P P
z2
z2
z3
R
x1
x1
Fig. 3.27: Polar coordinates (left) and torus coordinates (right)3.4. Exercises 283
Exercise 3.15 Let us examine the system (1.11) on p. 15 from Section 1.1.6.
(a) Transform the system (1.11) into polar coordinates using
x1 = z1 cos(z2),
x2 = z1 sin(z2).
(b) Using the system description in polar coordinates, describe the system’s
equilibrium points and trajectories.
Exercise 3.16 Let us consider the differential equation
x˙ =
1
x
. (3.130)
from Section 3.3.2, p. 264 et seq., once again.
(a) Transform the differential equation (3.130) using z = sgn(x)x
2
. What is
the result?
(b) Determine the differential equation’s solution.
(c) Compare the approach here with that of Section 3.3.2 on p. 264. What is
your finding?
(d) What type is the differential equation (3.130)?4
Nonlinear Control of Linear Systems
4.1 Control with Anti-Windup
4.1.1 The Windup Effect
Every real actuator of a control loop has a limitation in terms of the control
variable u because its maximum control power is finite. Consequently, every
real technical system possesses input constraints. This is illustrated in Fig￾ure 4.1, which shows a control loop with a plant G(s), a controller K(s), and
a limiting element. In this case, the complete control element consists of the
controller and the limiting element. The limitation of the control variable is
described by the saturation characteristic
u = sat(uc) =



umax, uc > umax,
uc, umin ≤ uc ≤ umax,
umin, uc < umin.
Examples of such limitations of the control variable uc are a servomotor’s
maximum torque, a ship’s maximum rudder angle, and the maximum aperture
of a valve. Often the saturation characteristic curve has symmetric limiting
points, i. e.
umin = −umax
yref e uc u y
umin
umax
K(s) G(s)
Fig. 4.1: Control loop with a linear plant, a linear controller K(s), and a limitation
of the control variable uc
© Springer-Verlag GmbH Germany, part of Springer Nature 2024
J. Adamy, Nonlinear Systems and Controls,
https://doi.org/10.1007/978-3-662-68690-4_4
285286 Chapter 4. Nonlinear Control of Linear Systems
holds.
If the control variable uc exceeds the limitations, the control loop is no
longer linear. In many cases, this is disadvantageous to its stability and control
behavior. We will examine this below.
As an important case in practice, we will consider the general structure of
a PID controller,
K(s) = KP

1 +
1
T Is
+ T Ds

= KPD(s) + KI
s
, KI =
KP
T I
,
in the control loop of Figure 4.1. The first term on the right side of the above
equation, KPD(s), can represent a P, PD, or D controller. Where only an I
controller exists, KPD(s) is not present.
Where u = uc, i. e. in the non-saturated case, the control loop is obviously
linear. In the saturated case, on the other hand,
u = umax or u = umin
holds, i. e. a constant control variable u acts upon the plant G(s).
Because of the constant control variable u in the case of saturation, no con￾trol is possible. In practical terms, the control loop is interrupted. Figure 4.2
illustrates this. Obviously, the interruption causes the integrator to continu￾ally integrate the control error e such that the integrator’s content increases
until the control error changes its sign. This process is called windup. Sum￾ming up the control error e is not only useless in this case; it is also harmful,
since a value of
uc < umin or uc > umax
cannot act upon the plant. The harm would be caused by the high value of
the integrator, which would only go down bit by bit after the control error e
changed its sign. Thus, the high value of the integrator prevents the needed
change in the control variable’s sign for a certain time. The control loop is
therefore interrupted until the integrator value is reduced such that the control
yref e
u = umax
uc ≥ umax
y
umax
umin
KI
s
G(s)
KPD(s)
Interruption
Fig. 4.2: Control loop in the saturated case4.1. Control with Anti-Windup 287
variable uc lies within the saturation limits. At this point, a feedback control
becomes possible again.
The windup behavior negatively affects the control behavior and it can
lead to increased overshooting or even to instability. In simplified form, the
situation can be interpreted as follows: if the control variable is in saturation
such that the control loop is interrupted, the integration part of the controller
is an unstable element of the open loop. Obviously, this is not a desirable
situation.
4.1.2 PID Controller with Anti-Windup Element
It is possible to avoid the windup described above by means of an anti-windup
structure, which ensures that the controller’s output variable uc does not
exceed the limits umin and umax. Figure 4.3 shows a PID controller with anti￾windup. The characteristic curve of the anti-windup element is a dead zone
described by
v =



m(uc − umax), uc > umax,
0, umin ≤ uc ≤ umax,
m(uc − umin), uc < umin,
with a very large positive value m ≫ 1 for its slope.
The dead zone creates a negative feedback from the control output to the
input of the integral component. If the controller’s output value uc is within
the limits of the control variable umin and umax, the feedback is not active,
i. e. the controller behaves like a normal PID controller. If uc exceeds one of
the limits umin or umax, the dead zone acts on the integral component as a
strong negative feedback. The negative feedback instantaneously reduces the
integral component to such a small value that uc does not exceed the control
variable limitation. Since the slope m is large but finite, the limits umin and
v
e u
m
uc
umax
umax
umin
umin
KI
s
KPD(s)
Anti-windup element
Fig. 4.3: PID controller with anti-windup element288 Chapter 4. Nonlinear Control of Linear Systems
v
e u
m
uc
umax
umin
KI
s
KPD(s)
Anti-windup element
Fig. 4.4: Alternative implementation of the anti-windup structure shown in Fig￾ure 4.3
umax can be marginally exceeded by the control variable uc. In practice, this
does not matter.
An implementation equivalent to the anti-windup structure in Figure 4.3
which does not require a dead zone is shown in Figure 4.4. The advantage of
this anti-windup structure is that only one nonlinear characteristic curve is
required. When there is a difference between uc and u, the integrator state is
reduced by the feedback m(uc − u). Since m ≫ 1 holds, this happens rapidly.
The transfer behavior between u and uc is given by
Gaw =
Uc(s)
U(s)
=
m · KI
s + m · KI
if KPD(s) is neglected. As is easily apparent, the reaction time of the anti￾windup element can be varied by m.
Apart from the classical anti-windup method for PID controllers discussed
above, a series of alternative methods and associated extensions also exist. An
overview can be found in [359].
4.1.3 Example: Direct-Current Motor
As an example, we will take the direct-current motor from Figure 4.5, which
acts as a servomotor that adjusts a rotation angle ϕload. In such servomotors,
anti-windup systems are often used in the controller.
For the armature voltage u, we obtain the equation
u = Ri + L˙i + k1ω (4.1)
with the armature current i and the voltage k1ω induced in the armature
winding. Here, R is the resistance, L is the inductance, ϕ is the rotation4.1. Control with Anti-Windup 289
i
u
R L
k1ω M
ω, ϕ
ϕload
Gearbox Load
r
Fig. 4.5: Direct-current motor with gearbox and load
angle, and ω = ˙ϕ the armature’s angular velocity. The generated torque can
be computed from
M = k2i
which is equal to
M = Jω˙ + k3ω,
where k3ω is the friction term proportional to the velocity. It follows that
k2i = Jω˙ + k3ω. (4.2)
With the armature’s inertia moment Ja, and the load’s inertia moment Jload,
which is converted via the gearbox with the transmission ratio r into the
inertia moment belonging to the armature shaft, we obtain the total inertia
moment
J = Ja +
1
r
2
Jload.
The values k1, k2, and k3 are motor parameters.
From equations (4.1) and (4.2), along with the Laplace transformed vari￾ables U(s), I(s), Φ(s) of u, i, ϕ, we calculate
U(s) = RI(s) + LI(s) · s + k1Φ(s) · s,
I(s) = J
k2
Φ(s) · s
2 +
k3
k2
Φ(s) · s
if all initial values are identical to zero. Combining the last two equations
yields the transfer function
Φ(s)
U(s)
=
1
s

LJ
k2
s
2 +

RJ
k2
+
Lk3
k2

s +

Rk3
k2
+ k1
.
If we also take into account that ϕ = rϕload, it follows that
G(s) = Φload(s)
U(s)
=
k2
rLJ
s

s
2 +

R
L
+
k3
J

s +

Rk3
LJ
+
k1k2
LJ .290 Chapter 4. Nonlinear Control of Linear Systems
Inserting the motor and load parameters
R = 8.9 Ω, L = 0.1 H, J = 0.1 Nm s2
rad−1
, r = 10,
k1 = 1.7775 V s rad−1
, k2 = 4 Nm A−1
, k3 = 0.1 Nm s rad−1
yields
G(s) = 40
s(s
2 + 90s + 800).
The armature voltage u, the control variable, is subject to the symmetric
limitations
−100 V ≤ u ≤ 100 V .
We now use a PI controller with
H(s) = KP + KI
·
1
s
and the parameters KP = 90 and KI = 150.
The simulations in Figure 4.6 show a small step response of ϕload = 0 rad to
ϕload = 1 rad = 57.3
◦ with a linear regulation, for which the control variable
does not saturate, and a large response of ϕload = 0 rad to ϕload = 10 rad =
573.0
◦ with a nonlinear regulation. In the latter case of a PI controller with
no anti-windup element for which the control variable saturates, the negative
Time t in s
ϕload in rad
u in V
Linear
Without anti-windup element
With anti-windup element
0
0
5
10
15
20
50
100
-50
-100
0
0
1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
Fig. 4.6: Rotation angle ϕload and armature voltage u in the linear case, the case
with an anti-windup element, and the case without an anti-windup element, in which
the controller is in saturation4.1. Control with Anti-Windup 291
effect of the windup of the I controller, i. e. the oscillation of the angle ϕload,
is obvious. The positive effect of the anti-windup element, which removes
the oscillation occurring due to the unnecessary windup of the integrator, is
equally evident.
4.1.4 A General Anti-Windup Method
Not all controllers are PID controllers. So the question arises which anti￾windup procedures could be used for a more general controller
Uc(s) = H(s) · E(s).
If the controller transfer function H(s) has unstable poles or poles with a
real part equal to zero, windup of the controller states and thus the controller
output uc occurs for limited control variables. Figure 4.7 shows the structure of
this more general control loop. For such controllers H(s), various anti-windup
methods exist [103, 142, 153, 222, 239, 424, 443, 444, 457, 489].
A very plausible anti-windup structure for general cases is obtained if we
take a small detour, as we will discuss in the next few pages. While doing
so, we will not only obtain a general anti-windup method for arbitrary linear
controllers but a simple method for handling saturated control variables of
state-space control loops with an observer as well [185].
Now we will examine a state-control loop with an observer, also known
as a control observer , as shown in Figure 4.8. The limitation of the control
variable is part of the plant and the pre-filter Gpf acts as compensation for
the steady-state error. In the linear operation mode, meaning
umin ≤ uc ≤ umax, (4.3)
the estimated state vector x˜ corresponds to the plant’s state vector x after
a certain settling time. Note that if the condition (4.3) for uc is not fulfilled,
the control loop is nonlinear.
Below we will first address the linear case. The linear state-control loop
with an observer has 2n eigenvalues. As we know from the theory of linear
observers and from the separation principle [98], these are the n eigenvalues
of the plant which is controlled by
yref e uc
umax u y
umin
H(s) G(s)
Fig. 4.7: Control loop with a saturation characteristic curve and an arbitrary linear
controller H(s)292 Chapter 4. Nonlinear Control of Linear Systems
yref
uk
uc y
x˜
k
T
Gpf
x˜˙ = Ax˜ + buc
+ l(y − c
T x˜)
x˙ = Ax + bu
y = c
T
x
Plant
Observer
Fig. 4.8: State-control loop with an observer
u = −k
T x,
i. e. the eigenvalues of the matrix
Aˆ = A − bkT
,
as well as the n eigenvalues of the observer matrix
F = A − lcT
.
The characteristic polynomial of the control loop with a control observer is
thus represented by
P(s) = det ￾
sI − A + bkT

| {z }
δ(s)
det ￾
sI − A + lcT

| {z }
∆(s)
,
which means it consists of the control loop’s characteristic polynomial δ(s)
and the observer’s characteristic polynomial ∆(s). Here, due to the separa￾tion principle, the control loop’s and the observer’s eigenvalues can be speci￾fied independently by the state-feedback gain vector k and the observer gain
vector l, respectively.
If the control variable uc violates the limitation, the control variable uc and
the plant’s input signal u differ. In this case, the input variables of the plant
and the observer also differ, as can be seen in Figure 4.8. Estimation errors
e = x − x˜ follow and the control performance deteriorates. This problem can
be easily solved by also limiting the observer’s input variable uc using the
same control variable limitation. This means the plant and observer always
have the same control variable. Figure 4.9 displays the modified structure.
Estimation errors resulting from the violation of the control variable li￾mitation no longer occur using the additional saturation element. The simp4.1. Control with Anti-Windup 293
yref uc u
uk
y
x˜
k
T
Gpf
x˜˙ = Ax˜ + bu
+ l(y − c
T
x˜)
x˙ = Ax + bu
y = c
T
x
Plant
Observer
Fig. 4.9: Control loop with an additional saturation characteristic curve to avoid the
negative effects of the control variable limitation
structure shown in Figure 4.9 also provides good control performance in many
cases if the control loop is temporarily operated in saturation. This particu￾larly holds for stable plants.
To find the anti-windup structure for the control loop in Figure 4.7 which
we are attempting to identify, in the first step we will describe the observer’s
behavior
x˜˙ =
￾
A − lcT

x˜ + bu + ly = Fx˜ + bu + ly
using transfer functions. Utilizing the Laplace transformed variables X˜(s),
U(s), and Y (s), we obtain
sX˜(s) − x˜(0) = F · X˜(s) + b · U(s) + l · Y (s).
Assuming
x˜(0) = 0,
it follows that
X˜(s) = (sI − F)
−1
b · U(s) + (sI − F)
−1
l · Y (s).
Along with the state controller
Uk(s) = k
TX˜(s),
we obtain
Uk(s) =k
T
(sI − F)
−1
b · U(s) + k
T
(sI − F)
−1
l · Y (s).
Next we will calculate the transfer function294 Chapter 4. Nonlinear Control of Linear Systems
yref uc u
uk
y
Gpf(s) G(s)=c
T
(sI−A)
−1
b
H2(s)=
N2(s)
∆(s)
H1(s)=
N1(s)
∆(s)
Plant
Anti-windup
element
Fig. 4.10: State-control loop with a controller and an observer, represented by trans￾fer functions and anti-windup element
H1(s) = Uk(s)
U(s)
= k
T
(sI − F)
−1
b =
N1(s)
∆(s)
and
H2(s) = Uk(s)
Y (s)
= k
T
(sI − F)
−1
l =
N2(s)
∆(s)
,
which describe the relationship between uk and u or y, respectively. The trans￾fer functions H1(s) and H2(s) have the same denominator:
∆(s) = det(sI − F).
The control loop with an observer can thus be represented by transfer
functions, as shown in Figure 4.10. Now we will interpret the additional sat￾uration characteristic curve u = sat(uc) as a kind of anti-windup element for
the transfer functions H1(s) and H2(s), which represent the controller. This
is because the additional saturation element prevents the undesired effects of
the control variable limitation.
It should be noted that the degree n of the denominators of H1(s) and H2(s)
is identical to the order of A, since we have fully reconstructed the state vec￾tor x. For a system with a reduced-order observer, the denominators of H1(s)
and H2(s) have a lower system order consistent with that of the observer.
We have also generalized the structure of the control loop in Figure 4.10 by
replacing the pre-filter Gpf with a general transfer function Gpf(s). The func￾tionality of the anti-windup element and the state controller with an observer
remains unaffected by this generalization.
In our second step, we will again examine the classical control loop with
the plant and the controller4.1. Control with Anti-Windup 295
G(s) = N(s)
D(s)
and H(s) = Nc(s)
Dc(s)
,
respectively, seeking its anti-windup element. This control loop, which is
shown in Figures 4.7 and 4.11, has the transfer function
Gloop(s) = Nc(s)N(s)
Dc(s)D(s) + Nc(s)N(s)
for the linear, i. e. the unsaturated case. The control loop from Figure 4.11 can
be rearranged so that it is consistent with the structure shown in Figure 4.12.
At this point, the polynomial ∆(s) is inserted in addition. This means that the
structure of Figure 4.12, except for the anti-windup element, complies with
that of the state-control loop with an observer, as shown in Figure 4.10. The
anti-windup element can now be integrated into the control loop, as shown in
Figure 4.12. Figure 4.13 depicts the resulting control loop.
In this way, after the process we went through to avoid saturation effects for
the state controller with an observer, we have obtained an anti-windup element
for the nonlinear standard control loop with an arbitrary linear controller
H(s). This control-loop structure, which is shown in Figure 4.13, thus provides
a solution to the anti-windup problem of the nonlinear standard control loop
from Figures 4.7 and 4.11.
4.1.5 Dimensioning the General Anti-Windup Controller
The question of choosing the polynomial ∆(s) which we posed in the previous
section still remains to be answered. In resolving this issue, we realize once
again that the following three control-loop structures are identical for the
linear case:
(1) the state controller with an observer and anti-windup element from Fig￾ure 4.9,
(2) the standard control loop from Figure 4.11,
(3) the standard control loop with an anti-windup element from Figure 4.13.
For the characteristic polynomial of the control loop with an observer, we
have already established that
δ(s) · ∆(s) = det(sI − Aˆ) det (sI − F). (4.4)
The characteristic polynomial of the standard control loop from Figure 4.11
and that of the control loop shown in Figure 4.13 are the same and can be
stated as
P(s) = Dc(s)D(s) + Nc(s)N(s). (4.5)
For the control-loop structures from Figure 4.10 and Figure 4.13 to be
identical, their characteristic polynomials (4.4) and (4.5) must be identical as
well. Therefore, the equation296 Chapter 4. Nonlinear Control of Linear Systems
PSfrag
yref e uc y
H(s) = Nc(s)
Dc(s)
G(s) = N(s)
D(s)
Plant
Fig. 4.11: Control loop with a control variable limitation and a general linear con￾troller H(s)
yref uc y
Dc(s)−∆(s)
∆(s)
G(s) = N(s)
D(s)
Nc(s)
∆(s)
Nc(s)
∆(s)
Plant
Fig. 4.12: Control-loop structure equivalent to the structure shown in Figure 4.11
yref uc u y
Dc(s)−∆(s)
∆(s)
G(s) = N(s)
D(s)
Nc(s)
∆(s)
Nc(s)
∆(s)
Plant
Anti-windup
element
Fig. 4.13: Control loop with saturation, a general controller, and an anti-windup
element4.1. Control with Anti-Windup 297
P(s) = Dc(s)D(s) + Nc(s)N(s) = δ(s) · ∆(s)
has to hold. All roots of the polynomial ∆(s) being sought must therefore be
roots of the characteristic polynomial P(s) of the standard control loop shown
in Figure 4.11. Note that the order of P(s) is n + k, where n is the order of
the plant and k is the order of the controller. The polynomial δ(s) is of degree
n and the polynomial ∆(s) is of degree k.
Where n = k, ∆(s) represents the characteristic polynomial of a full-state
observer for all states xi of the plant. Where k < n, the polynomial ∆(s) has
fewer than n roots and represents the characteristic polynomial of a reduced￾order observer. The case in which k < n is the usual case, since the controller
H(s) has the same order n as the plant in special cases only.
Note that the state-control loop with an observer does not have to be
designed in order to determine the desired characteristic polynomial ∆(s). The
equivalence between the state-control loop with an observer and the standard
control loop only serves to explain how the anti-windup is designed.
The results are summarized in the following theorem.
Theorem 56 (General Anti-Windup Structure). Let the standard con￾trol loop shown below
yref e uc y
H(s)= Nc(s)
Dc(s)
G(s) = N(s)
D(s)
consist of the controller Nc(s)/Dc(s) of order k and the plant N(s)/D(s) of
order n. The control loop shown below
yref uc u y
Dc(s)−∆(s)
∆(s)
G(s) = N(s)
D(s)
Nc(s)
∆(s)
Nc(s)
∆(s)
displays the same linear control behavior as the standard control loop, and has
an anti-windup element in addition. The k roots of the polynomial ∆(s) are
chosen such that they correspond to k roots of the control loop’s characteristic
polynomial
P(s) = Dc(s)D(s) + Nc(s)N(s).298 Chapter 4. Nonlinear Control of Linear Systems
When applying the above theorem, the k roots of the polynomial ∆(s) can
be freely specified, as long as each one corresponds to one of the n + k zeros
of the polynomial P(s). A suitable strategy is to choose them in a way that
yields good control behavior. Unfortunately, no rule exists for choosing them.
Instead, we are dependent on trial and error based on simulations to verify
the behavior.
In infrequent specific cases, the choice of the k zeros of ∆(s) may be diffi￾cult. This is the case when the polynomial P(s) has only complex conjugate
roots and the degree k of the polynomial ∆(s) is odd. Obviously, ∆(s) then
possesses a real root. A real root, however, does not exist in the set of complex
roots of P(s). This problem is solved by choosing the complex conjugate pair
of roots of P(s) with the largest damping coefficient d. Associated with this
pair is the polynomial
s
2 + 2dω0s + ω
2
0
.
We approximate this polynomial via
(s + ω0)
2
and choose ω0 as a real zero of ∆(s). The remaining k − 1 roots of the poly￾nomial ∆(s), whose number is even, are chosen in a way consistent with the
rule given in Theorem 56 from the set of complex conjugate roots of the
characteristic polynomial P(s).
4.1.6 Stability
Finally, we will discuss the stability of a control loop with an anti-windup
element. For a constant reference variable yref, this can be addressed by con￾verting the control loop into a nonlinear standard control loop with
yref(t) = 0,
as shown in Figure 4.14. In this case, the transfer function G˜(s) subsumes the
e u y
G˜(s)
umax
umin
Fig. 4.14: Nonlinear standard control loop
controller and plant transfer functions. Once represented in the form of the
nonlinear standard control loop, the circle criterion, among other methods,
can be used for stability analysis.4.2. Time-Optimal Control 299
4.2 Time-Optimal Control
4.2.1 Fundamentals and Fel'dbaum’s Theorem
In practice, the design of nonlinear controllers is often based on heuristics.
This is particularly true for the design of controllers which consist of elements
with nonlinear characteristic curves. Examples are many two-position con￾trollers and frequently controllers with an anti-windup element. Controllers
of this kind are designed based on intuition, prior knowledge, and assump￾tions about the plant and its control-loop behavior with a selected controller.
After designing such nonlinear controls, simulations are typically conducted
to verify the control performance. In addition, one of the methods discussed in
Chapter 2 should be applied to ensure stability. Consequently, this approach
can be divided into three steps: heuristic controller design, stability analysis
of the control loop, and simulation.
The reasons for a heuristic approach of this kind may be the fact that for
many problems analytic design methods do not exist or that the design prob￾lems are very complex. In contrast, a further possible reason may be that the
design problem is very simple and the implementation must be inexpensive.
For example, a temperature controller for electric irons or coffee machines can
be designed using an element with a hysteresis characteristic curve, e. g. a
bimetal.
Such an approach does not succeed if the control performance requirements
are high or the plant is extremely complex. These cases require appropriate
controller design methods which aim to achieve better control performance
than would be possible with a linear controller, or even to achieve the optimal
control performance with respect to a given quality measure. One class of
optimal feedforward and feedback controllers are time-optimal controllers. As
indicated by the name, this class of controllers provides a feedforward or
feedback compensation from an initial state x0 to the final state xe = 0
within the shortest possible time te.
As plants, we will select the linear SISO systems
x˙ = Ax + bu. (4.6)
The starting point for the controller design is the requirement that the sys￾tem’s trajectory x(t) is steered to the equilibrium state xeq = 0 in minimum
time te. That means the performance index
J = te (4.7)
must be minimized by an appropriately chosen control signal u(t). In this
context, we need to remember that the control signal is limited by
−umax ≤ u ≤ umax. (4.8)
Consequently, the following optimization problem must be solved: find the
feedforward control function u(t) for the system (4.6) with a control variable300 Chapter 4. Nonlinear Control of Linear Systems
limitation (4.8), such that the performance index (4.7) is minimized for a given
initial displacement x0. Closely related to this problem is that of determining
the time-optimal feedback control law u(x).
The above problems can be solved using Pontryagin’s maximum principle
[186, 237, 286, 453]. In general, the time-optimal feedback controls u(x) are
very difficult to determine and extremely complex to implement. That is the
reason why time-optimal feedback controllers in industrial practice, with few
exceptions [269, 353, 471], are found infrequently. Time-optimal feedforward
controllers u(t), on the other hand, are often easier to derive and implement.
They are used in various applications, such as [25, 35, 59, 79, 86, 136, 476].
We will not discuss the maximum principle, since most of the practically rele￾vant cases, particularly time-optimal feedforward controllers, can be computed
without it.
The course of the control variable u(t) of a time-optimal control is a very
simple one, since u(t) merely jumps from −umax to umax and vice versa, as
depicted in Figure 4.15. In this way, a series of switching operations between
−umax and umax is performed. The difficulty in designing a time-optimal con￾trol is the determination of the switching times
t1, t2, t3, . . . , te
for a feedforward control, and the determination of the feedback control law
u(x) for a feedback control. The feedback control law u(x) depends on the
state vector x, unlike the feedforward control sequence u(t) which is only a
function of time.
However, an important special case exists for which the switching times
are relatively easy to determine. These are plants which possess only real
eigenvalues. Without applying the maximum principle, time-optimal control
functions u(t) can then be deduced [112, 113, 114] using
Theorem 57 (Fel'dbaum’s Theorem). If the controllable system
x˙ = Ax + bu
of order n has only real eigenvalues, the course of the time-optimal control
function u(t) consists of a maximum of n switching intervals, for which u(t)
alternates between −umax and umax.
As illustrated in Figure 4.15 for a fourth-order system with only real eigen￾values, switching between −umax and umax occurs a maximum of three times,
i. e. we have four switching intervals. For a system with complex conjugate
eigenvalues, the number of switching intervals can be greater than n.
Note that the time-optimal control sequence does not exist for all x ∈ IRn
where plants are unstable. This is because, for limited control variables u,
not all initial states x(0) can be brought to the equilibrium point xeq = 0.
The limited control power is not sufficient in this case. On the other hand,4.2. Time-Optimal Control 301
0 t
t0 = 0 t1 t2 t3 te =t4
umax
u
−umax
Fig. 4.15: Time course of the control variable of a time-optimal feedforward or feed￾back controller
for controllable plants which do not have any eigenvalues with positive real
parts, time-optimal feedforward control sequences u(t) and feedback control
laws u(x) exist for all x ∈ IRn
.
4.2.2 Computation of Time-Optimal Controls
For systems with only real eigenvalues, the switching times ti can be deter￾mined as follows. For a linear system’s differential equation
x˙ = Ax + bu,
the solution
x(t) = e
Atx0 +
Z
t
0
e
A(t−τ)
bu(τ) dτ (4.9)
is well-known. In the first switching interval, the sign of the trajectory of u(t)
equals
α = 1 or α = −1.
Thus,
u(t) = (−1)i−1α · umax for t ∈ [ti−1, ti)
holds for i = 1, . . . , n. At present, we will leave the question open whether
α = 1 or α = −1 and go ahead calculating the solution of the system by
inserting the switching sequence u(t) into equation (4.9). We obtain
x(te = tn) = e
Atn x0 +
Xn
i=1
Z
ti
ti−1
e
A(tn−τ)
b(−1)i−1α · umax dτ
and with the final state
x(te = tn) = 0302 Chapter 4. Nonlinear Control of Linear Systems
the equation
0 = e
Atn x0 + α · umaxXn
i=1
Z
ti
ti−1
e
A(tn−τ)
b(−1)i−1
dτ. (4.10)
Taking into account
e
A(tn−τ) = e Atn e
−Aτ
,
equation (4.10) multiplied by e
−Atn results in
x0 = −α · umaxXn
i=1
(−1)i−1
Z
ti
ti−1
e
−Aτ
b dτ.
| {z }
w(ti) − w(ti−1)
(4.11)
Here, w(τ) is the antiderivative vector of e
−Aτ
b. Using equation (4.11), we
obtain
−
x0
αumax
= [w(t1)−w(t0)]−[w(t2)−w(t1)]+. . .+ (−1)n−1
[w(tn)−w(tn−1)].
This equation leads to the nonlinear system of equations
w(t1) − w(t2) + w(t3) − . . . +
1
2
(−1)n−1w(tn) = 1
2
w(0) −
x0
2αumax
(4.12)
with n equations and n unknowns, i. e. t1, t2, . . . , tn for a starting time t0 = 0.
In principle, α is also unknown. Since it is not known a priori whether
umax or −umax holds for the first switching interval, both values α = 1 and
α = −1 are tried. The correspondent signals u(t) are shown in Figure 4.16.
For one of the two cases the system of equations has a solution, whereas for
the other, it does not. The system of equations is only analytically solvable
u u
umax umax
−umax −umax
t t
α= 1
α=−1
Fig. 4.16: The first switching interval of a time-optimal control sequence begins with
umax or −umax.4.2. Time-Optimal Control 303
for lower-order systems. Otherwise, it is transcendental and must be solved
numerically.
The time-optimal switching times of systems x˙ = Ax + bu with complex
conjugate eigenvalues which satisfy the system of equations (4.12) can also
be computed. However, we do not know whether n switching intervals are
sufficient. If no solution is found, we need to try n+ 1, n+ 2, . . . intervals. This
can lead to additional solutions which do not provide time-optimal switching
times.
4.2.3 Example 1/s2
As a classic example [213], we will consider the plant 1/s2
. It occurs in sit￾uations such as when a mass is accelerated. The corresponding state-space
representation is
x˙ =

0 1
0 0
x +

0
1

u,
and we will assume that the control variable is symmetrically limited by
−umax ≤ u ≤ umax.
Thus, u is the acceleration, x2 is the velocity, and x1 is the distance traveled.
Figure 4.17 shows the corresponding block diagram.
According to Fel'dbaum’s theorem, the time-optimal control u(t) has a
maximum of two switching intervals, meaning one switching operation be￾tween −umax and umax. The switching times t1 and t2 are determined from
w(t1) −
1
2
w(t2) = 1
2
w(0) −
x0
2αumax
(4.13)
with
w(τ) = Z
e
−Aτ
dτ · b and x0 =

x10
x20
=

x1(0)
x2(0)
.
First, using the inverse Laplace transformation L
−1
the state-transition ma￾trix
e
−At = L
−1

(sI + A)
−1
	
= L
−1
(
s 1
0 s
−1
)
=

1 −t
0 1
u x2 x1
1
s
1
s
Fig. 4.17: Block diagram of the double integrator304 Chapter 4. Nonlinear Control of Linear Systems
is derived. Using the latter, we calculate
w(τ) = Z
e
−Aτ
dτ · b =
Z 
−τ
1

dτ =
"
−
1
2
τ
2 + C1
τ + C2
#
. (4.14)
Furthermore, equation (4.14) substituted into equation (4.13) yields
−2t
2
1 + t
2
2 = −2
x10
αumax
,
2t1 − t2 = −
x20
αumax
.
This nonlinear system of equations is easily solved. The resulting solutions
are
t1 = −
x20
αumax
±
s
1
2

x20
αumax 2
−
x10
αumax
, (4.15)
t2 = −
x20
αumax
± 2
s
1
2

x20
αumax 2
−
x10
αumax
. (4.16)
The question remains whether α = 1 or α = −1 holds, i. e. whether the
control sequence begins with umax or with −umax. To find this out, we will
examine equation (4.15) more closely. Obviously
t1 = −
x20
αumax
±
s
1
2

x20
umax 2
−
x10
αumax
≥ 0 (4.17)
must hold. Our first task is to determine which values x10 and x20 lead to a
value t1 ≥ 0 if α = 1 holds. In this case, equation (4.17) multiplied by umax
is of the form
−x20 ±
r
1
2
x
2
20 − umaxx10 ≥ 0. (4.18)
A requirement is that the term in the root is positive or equal to zero, i. e.
x10 ≤
1
2umax
x
2
20 (4.19)
must hold. Furthermore, we need only consider cases in which the sign in
front of the root in equation (4.18) is positive, since the associated solution
set also contains all solutions which result from a negative sign. As a further
condition, we therefore obtain
x20 ≤
r
1
2
x
2
20 − umaxx10 . (4.20)4.2. Time-Optimal Control 305
Two cases must be distinguished for this inequality. We will start with the
case x20 ≤ 0. In this case, equation (4.20) is obviously always fulfilled as long
as equation (4.19) holds. In the other case, x20 > 0 holds. We now square
both sides and obtain
x
2
20 ≤
1
2
x
2
20 − umaxx10 for x20 > 0
or rather
x10 ≤ −
1
2umax
x
2
20 for x20 > 0. (4.21)
The set of initial values
x0 =

x10 x20T
for which equation (4.15) is solvable with α = 1 is thus given by the inequal￾ities (4.19) for x20 ≤ 0 and (4.21). It is bounded by branches of the two
parabolas defined by the equal sign in these inequalities. Figure 4.18 illus￾trates this region in blue. For α = −1, a similar analysis leads to the result
that u = −umax holds for the region above the branches of the parabolas, i. e.
the white area in Figure 4.18.
In summary, the regions of the state space for which α = 1 or α = −1
holds, meaning u = umax or u = −umax also holds, are separated by
x10 =



x
2
20
2umax
, x20 ≤ 0,
−
x
2
20
2umax
, x20 > 0.
x10
x20
S(x2)=−
x2|x2|
2umax
−umax,
umax,
α = −1
α = 1
Fig. 4.18: The phase plane is separated into two halves by the switching curve
x1 = S(x2), which is shown in blue.306 Chapter 4. Nonlinear Control of Linear Systems
Together, these parabola branches form the switching curve
S(x2) = x1 = −
x2|x2|
2umax
.
Below the switching curve S(x2), u = umax holds, whereas above it u = −umax
holds.
It should be noticed that the minus sign in front of the roots in equa￾tions (4.15) and (4.16) can be omitted, as this leads to negative or irrelevant
switching times. With the results above, the switching times ti are computed
as
t1 = −
x20
αumax
+
s
1
2

x20
umax 2
−
x10
αumax
,
t2 = −
x20
αumax
+ 2s
1
2

x20
umax 2
−
x10
αumax
with
α =
(
1, x10 < S(x20) or x10 = S(x20) > 0,
−1, x10 > S(x20) or x10 = S(x20) < 0.
Using these equations, we are able to calculate the time-optimal feedforward
control.
The time-optimal feedback control can now also be determined. Above the
switching curve S(x2) the actuator signal −umax is used and below it umax,
i. e. the control law is
u(x) = (
umax, x1 − S(x2) < 0,
−umax, x1 − S(x2) > 0.
The latter is equivalent to
u =
(
umax, sgn(x1 − S(x2)) < 0,
−umax, sgn(x1 − S(x2)) > 0
= −umax · sgn(x1 − S(x2)) = umax · sgn(S(x2) − x1).
The time-optimal control law of the plant 1/s2
can eventually be represented
as
u = umax · sgn
−
x2|x2|
2umax
− x1

. (4.22)
The control law (4.22) provides the value u = 0 on the switching curve,
i. e. for x1 = S(x2). To be correct, in this case, u = −umax for x1 < 0 and
u = umax for x1 > 0 should hold. In practice, however, this is irrelevant since4.2. Time-Optimal Control 307
u
S(x2)
umax
-umax
1
s
1
s
x20 x10
x2 x1
Fig. 4.19: Time-optimal feedback control for the plant 1/s2
the trajectory can never run precisely along the switching curve due to noise.
The corresponding controller is shown in Figure 4.19.
The control law above and further time-optimal control laws for second￾order plants with real eigenvalues can be relatively easily derived by calcu￾lating all trajectories generated by u = umax and u = −umax in the phase
plane. Parts of these trajectories obviously form the set of all trajectories of
the time-optimally controlled system. In particular, the switching curve S is
identical to the parts of the two trajectories which pass through the origin
x = 0 for u = umax or u = −umax, respectively. Therefore, to derive the time￾optimal control law, it is sufficient to compute these two kinds of trajectories
and to formulate the switching curve S from parts of these trajectories based
on geometrical considerations in the phase plane.
4.2.4 Time-Optimal Control of Low-Order Systems
In the following, we will address second- and third-order systems with the
control variable limitation
|u| ≤ umax
and their associated time-optimal control laws. First, let us turn to plants
x˙ =

0 1
0 −a

x +

0
1

u, (4.23)
which have one eigenvalue at zero and one eigenvalue at λ = −a < 0. Note
that all controllable plants with the above eigenvalue configuration can be
transformed into a form consistent with (4.23), i. e. the controller canonical
form.
The time-optimal feedback control law is obtained as
u = umax sgn(S(x2) − x1)308 Chapter 4. Nonlinear Control of Linear Systems
PSfrag
u
S(x2)
umax
-umax
1
s + a
1
s
x20 x10
x2 x1
Fig. 4.20: Time-optimal feedback control system for the plant 1/(s(s + a))
with the switching curve
S(x2) = −
1
a
x2 +
umax
a
2
sgn(x2) ln 
1 +
a|x2|
umax 
.
Figure 4.20 shows the corresponding block diagram.
The second case we will address is stable second-order plants with real non￾zero eigenvalues λ1 < λ2 < 0. Here we assume that the system description is
in, or can be transformed into, a form consistent with
x˙ =

λ1 0
0 λ2

x +

λ1
λ2

u. (4.24)
Then the time-optimal feedback control law is stated as
u = umax sgn(S(x2) − x1)
with
S(x2) = umax sgn(x2)
"
1 +
|x2|
umax λ1/λ2
− 1
#
.
The time-optimal control law can also be computed for third-order systems
with two eigenvalues at zero and a negative eigenvalue λ = −a if they are
stated in controller canonical form, or can be transformed into this form [24].
However, here it is useful to transform the controller canonical form
x˜˙ =


0 1 0
0 0 1
0 0 −a

x˜ +


0
0
1

u
by means of4.2. Time-Optimal Control 309
x˜ =
1
a
3


1 0 1
0 a −a
0 0 a
2

x
into the form
x˙ =


0 a 0
0 0 0
0 0 −a

x +


−a
a
a

u. (4.25)
For the time-optimal control of the system (4.25), we obtain [24]
u = umax sgn(S(x1, x2) − x3) (4.26)
with
S(x1, x2) = umaxd
h
e
c
· (2 − e
√
b
) − 1
i
,
d = sgn 
x1 + x2 +
x2|x2|
2umax 
,
b =
x
2
2
2u2
max
+
d
umax
(x1 + x2),
c =
d · x2
umax
+
√
b.
If the plant was originally not in one of the corresponding state-space
representations (4.23), (4.24), or (4.25) but was transformed into such a form,
the corresponding control law must be transformed back to the plant’s original
coordinates. The following section shows an example.
The time-optimal feedback control laws for some third-order systems can
be found in [141, 395] and for certain fourth-order systems in [396]. In gen￾eral, for higher-order plants no closed-form expressions for the control law
can be derived. However, for such stable plants with exclusively real eigenval￾ues, a control law is obtained for which a transcendental nonlinear system of
equations must be solved to determine u [24]. In practice, apart from a few
exceptions with a low system order [24, 336, 399, 414, 474], the calculation and
implementation of a time-optimal feedback control law are no longer possible
for systems with complex conjugate eigenvalues.
4.2.5 Example: Submarine
Submarines can dive both dynamically and statically. For dynamic diving,
the depth rudder is adjusted during travel such that a downward force is gen￾erated. This allows the submarine to dive into deeper waters, even though
it is not heavier than the water it displaces. On the other hand, for static
diving, ocean water is inserted into ballast tanks so that the submarine be￾comes heavier and sinks. When the submarine rises, the water is expelled with310 Chapter 4. Nonlinear Control of Linear Systems
h, g
Ballast tank
Fig. 4.21: Submarine
compressed air from the tanks shown in Figure 4.21. In the following, we will
design a time-optimal depth control for static diving.
We assume that the submarine is balanced such that it floats in a certain
depth h and has an associated mass m. Starting from the ocean surface, the
depth h is measured by means of the water pressure. If additional water of
mass ∆m is inserted into or ejected from the ballast tank, a vertical force
F = g · ∆m
is generated which acts upon the submarine. Subsequently, we assume that
∆m ≪ m. The ballast water mass accelerates the submarine with a total mass
of m + ∆m as follows:
h¨ =
g · ∆m
m + ∆m
≈
g
m
· ∆m, (4.27)
so that the submarine rises or sinks statically. For ∆m < 0 it rises, whereas
for ∆m > 0 it sinks.
The ballast water mass ∆mref to be inserted or expelled from the sub￾marine is stated as a reference value to a secondary controller which is sub￾ordinated to the time-optimal controller and is assumed to be known. This
secondary controller can be described by the differential equation
(∆m)
· + a∆m = a∆mref. (4.28)
Inserting equation (4.27) into equation (4.28) yields4.2. Time-Optimal Control 311
˙˙˙h + ah¨ =
a · g
m
· ∆mref.
In the next calculation, we use
u˜ =
g
m
· ∆mref
as the control variable for the depth controller, which is the primary controller
and is yet to be designed. For the state vector, we choose
x˜ =


h
h˙
h¨

.
The parameters a = 0.005 and m = 1962 t, as well as the units of the vari￾ables, are chosen so as to model a Swedish submarine by the manufacturer
Kockumation AB [166]; we then obtain
x˜˙ =


0 1 0
0 0 1
0 0 −0.005

x˜ +


0
0
0.005

u˜
with the diving depth as the output variable y = ˜x1. Furthermore, we set
u˜max = 0.005 to be consistent with [166].
To obtain the time-optimal control law (4.26) as the depth control, we
transform the control variable u˜ via u˜ = 200u, and thus obtain the system
description
x˜˙ =


0 1 0
0 0 1
0 0 −0.005

x˜ +


0
0
1

u, (4.29)
y = ˜x1
with the input constraints
|u| ≤ umax = 2.5 · 10−5
.
We can now apply the control law (4.26) for the plant (4.29). However, we
need to use the transformed state variable vector x in equation (4.26), i. e.
x = a
3


1 0 1
0 a −a
0 0 a
2


−1
· x˜ =


a
3
0 −a
0 a
2
a
0 0 a

 · x˜,
to compute u. Inserting the transformation equation above into equation
(4.26) yields
u = umax sgn(S˜(˜x1, x˜2, x˜3) − ax˜3)312 Chapter 4. Nonlinear Control of Linear Systems
as the time-optimal control law for the submarine at the original coordinates
x˜, where
S˜(˜x1, x˜2, x˜3) = umaxd
h
e
c
· (2 − e
√
b
) − 1
i
with
d = sgn 
ax˜1 + ˜x2 +
(ax˜2 + ˜x3)|ax˜2 + ˜x3|
2umax 
,
b =
a
2
(ax˜2 + ˜x3)
2
2u
2
max
+
da2
(ax˜1 + ˜x2)
umax
,
c =
ad(ax˜2 + ˜x3)
umax
+
√
b.
As an example, we will describe how the submarine surfaces from a depth
of 100 meters, so the initial vector is:
x˜(0) =


100
0
0

 .
Figure 4.22 shows the progressions of the depth x˜1(t) and the control variable
u˜(t) which the time-optimal control generates.
100
50
0
0
-0.005
0.005
0
0
200
200
400
400
600
600
800
800
1000
1000
1200
1200
1400
1400
1600
1600
Control variable ˜u Diving depth ˜x1 in m
Time t in s
Linear controller
Time-optimal controller
Fig. 4.22: Time courses of the diving depth x˜1(t) = h(t) and the control variables
u˜(t) of the submarine’s linear (black) and time-optimal (blue) controllers4.2. Time-Optimal Control 313
As a comparison, the trajectories for a very good linear controller,
u˜ = 200u = −

4.164 · 10−5
2.128 · 10−2
2.592
x˜,
are also shown. This controller was designed to adhere to the control variable
limitation while exhibiting a short settling time and no overshooting. The
substantially shorter settling time of the time-optimal controller is evident.
However, its disadvantages are the discontinuous course of the control variable
and its high amplitude, which leads to increased energy consumption.
4.2.6 Time-Optimal Pilot Control
For the second-order and third-order plants with real eigenvalues we are view￾ing, determining the time-optimal feedback control law u(x) is relatively sim￾ple. For higher-order plants, analytically identifying time-optimal feedback
control laws is no longer feasible. As mentioned previously, the design and
implementation become impossible or extremely complex. Time-optimal feed￾forward controllers, on the other hand, are usually determinable and can be
implemented in practice. In particular, their application is appropriate in sys￾tems which progress from one point x0 to a point xe again and again, i. e.
systems which repeatedly take the same trajectory, as shown in Figure 4.23.
To eliminate disturbances, a linear feedback controller can be used addition￾ally, as shown in Figure 4.24. This feedback controller corrects the feedforward
control, which is called a pilot control or pre-control in this context.
Experience has shown that existing algorithms for the calculation of the
switching times for time-optimal feedforward controllers [109, 158, 159] exhibit
numerical problems for higher-order plants with complex eigenvalues. In such
cases, time-optimal solutions often cannot be determined. A solution to this
problem is the calculation of the feedforward control sequence of discrete￾time systems. In this approach, the continuous-time system is transformed
into a discrete-time system. The time-optimal feedforward control sequences
for discrete-time linear systems can be relatively easily calculated using linear
t1
t2
t3
t4
x1
x2
x0
xe
Fig. 4.23: Time-optimal trajectory
with switching times t1,. . . ,t4
yref u y
Controller Plant
Time-optimal
pilot control
Fig. 4.24: Control loop with time-optimal
pilot control314 Chapter 4. Nonlinear Control of Linear Systems
programming methods [33, 49, 74, 82, 227, 236, 295, 347, 409]. For this pur￾pose, the shorter the chosen sampling time, the better the step-optimal feed￾forward control sequence approximates the time-optimal feedforward control
function of the continuous-time system.
Comparing time-optimal controllers with linear controllers leads to the
following dilemma: time-optimal controllers have a short settling time, but
they are in general difficult or even impossible to design and implement. In
comparison, linear controllers are slow but simple to design and implement.
Evidently, it is impossible to achieve both very good regulation by the con￾troller and simplicity of design and implementation. However, this dilemma
can be resolved, because a trade-off exists between these two extremes. This is
the subject of the following section, which addresses variable structure control
systems.
4.3 Variable Structure Control Without Sliding Mode
4.3.1 Fundamentals of Variable Structure Control
Variable structure controllers can be divided into different classes. An impor￾tant class consists of the parameter- and structure-switching controllers. For
these, switching between different controllers is mostly performed in depen￾dence on the state vector x. As plants, we will consider the linear systems
x˙ = Ax + bu
where the control variable u is limited according to
−umax ≤ u ≤ umax.
Figure 4.25 shows the general structure of a control loop such as this. A
detailed description of switching control systems can be found in [280, 403].
In switching controllers, we can distinguish between two types of dynamic
behavior, which are illustrated using an example in the following. We will
examine switching between two P controllers and a second-order plant, as
shown in Figure 4.26. The switching strategy works as follows: a switching
curve
s(x) = r
T x = 0
separates the state space. On the right side the controller with a gain of k1 is
activated, while on the left side the controller with a gain of k2 is activated.
The control law takes the form
u =
(
k1e, s(x) ≥ 0,
k2e, s(x) < 0.
Depending on the choice of k1 and k2, different trajectories can be generated.
We can now distinguish between two general cases. In the first, the trajectories4.3. Variable Structure Control Without Sliding Mode 315
x
Controller 1
Controller 2
Controller l
Selection
strategy
.
.
.
x˙ =Ax+bu
Fig. 4.25: Variable structure control system
e
u y
x1 x
x2
k1
k1
k2
k2
1
s
2+a1s+a0
Switching strategy
Fig. 4.26: Control loop that switches between two P controllers with gains k1 and
k2
switch from one region into the other and continue their course there. This is
illustrated in the graph on the left of Figure 4.27.
In the second case, the trajectories x(t) tend to the switching curve s(x)
from both sides, as shown in the upper left quadrant of the right-hand graph
in Figure 4.27. Once it has reached the switching curve, the trajectory repeat￾edly switches from one side to the other. While maintaining this continuous
switching behavior, it eventually reaches the equilibrium point. This repeated
switching, which we already discussed in Section 1.1.11, is termed sliding
mode. Sliding mode controllers make up a special class of variable structure316 Chapter 4. Nonlinear Control of Linear Systems
x1 x1
x2 x2
k1
k1
k2
k2
s s
Fig. 4.27: Different trajectories for switching controllers: on the left without and on
the right with a sliding mode. The blue line represents the switching curve s(x) = 0.
controllers and are not addressed here, but will be discussed in detail in Sec￾tion 6.2.
In terms of their design complexity and control performance, variable
structure control loops without a sliding mode, which are the subject of this
section, lie between linear and time-optimal control loops. An example is given
in Figure 4.28 which shows typical step responses. The quality loss in the lin￾ear control loop compared to the time-optimal control loop is caused by the
insufficient utilization of the available control variable −umax ≤ u ≤ umax in
the linear case. This is illustrated in Figure 4.29. Because of the linearity, the
control variable u is correspondingly lower for small displacements x or con￾trol errors e than for large displacements. If, in contrast, we use high values
of the control variable in the case of low values of the control error e or small
displacements x, a more rapid compensation becomes possible.
The basic idea of switching or variable structure controllers without sliding
mode is to make better use of the controlled variable within the admissible
range −umax ≤ u ≤ umax than can be done using linear controllers. For this
purpose, several linear controllers are used. We switch between them by using
a selection strategy which normally depends on the system’s state vector x.
This is done in such a way that successively more effective controllers, i. e.
controllers making the control loop faster and faster, are used during the
control process. Figure 4.25 shows the corresponding control loop.
For variable structure controls, linear systems are mostly considered to be
controlled systems. The individual controllers can be either linear or nonlin￾ear. However, they are usually linear. In any case, because of the switching
operation, the resulting control loop is nonlinear.
The performance of a parameter or structure switching control is defined
by three factors: the number l of controllers, the controllers’ type and design,
and the switching strategy. Provided that the controllers and the selection4.3. Variable Structure Control Without Sliding Mode 317
t
y Time-optimal
Linear
Variable structure
Fig. 4.28: Comparison of the step re￾sponses of linear, variable structure,
and time-optimal control systems
u
t
umax
−umax
Time-optimal
Linear
Fig. 4.29: Typical control variable sig￾nals of linear and time-optimal con￾trollers
Controller
Selection parameter
u=k(x, p) x˙ = Ax + bu
p=p(x)
x
p
u
Fig. 4.30: Soft variable structure controller
strategy are adequately chosen, the achievable control performance increases
for an increasing number of controllers used in maximally close sequence.
The switching need not be based on a switching line, as discussed for the
simple example given above. It can also follow other types of curves or, for
higher-dimensional spaces, other types of surfaces. For example, in the next
section, we will discuss a controller with a selection strategy that switches
based on ellipsoids.
As mentioned previously, the achievable control performance of a switching
controller without a sliding mode increases with the number of controllers
used. So an obvious approach is to use as many controllers as possible and
ultimately an infinite number. Then the controller change does not occur due
to switching between the controllers; instead it is due to a continuous change
of the control parameters in the control law
u = k(x, p),
where the variation of the control law depends on a selection parameter
p = p(x),
which is continuous in x. These controllers, which are shown in Figure 4.30,
are called soft variable structure controllers [6, 61, 132, 192]. They can be seen318 Chapter 4. Nonlinear Control of Linear Systems
as a systematic evolution and improvement of switching controllers. The term
soft results from the continuous control variable trajectory, which does not
jump as it does in the case of switching controllers, which are discontinuous.
A sliding mode is impossible in this kind of controllers due to the continuous
control variable.
4.3.2 Piecewise Linear Control
In the following, we will describe a switching controller without a sliding mode
for linear plants with a control variable limitation [235, 480]
x˙ = Ax + bu,
|u| ≤ umax,
the general structure of which is shown in Figure 4.25. A set of l controllers
u = −k
T
1 x,
u = −k
T
2 x,
.
.
.
u = −k
T
l x,
is used, designed such that the feedback control system
x˙ =

A − bkT
i

x = Aˆix, i = 1, . . . , l, (4.30)
becomes faster with the increasing index i, i. e. the settling time of the cor￾responding linear subcontrol loop decreases for increasing index values i. For
example, this can be achieved by calculating each ki as the parameter vector
of a linear-quadratic controller based on the performance integral
J =
Z∞
0

pix
T P x +
1
pi
ru2

dt.
Here the matrix P is positive definite or positive semidefinite. The factor
pi must be chosen so that the control loop (4.30) utilizes increasingly faster
controllers with higher control variable values for increasing pi
.
A second simple possibility exists for selecting the controller vectors ki
.
We can specify them using an eigenvalue placement such that the eigenvalues
λji, j = 1, . . . , n, of the closed control loop (4.30) take on lower and lower real
parts for increasing index values i. Among other methods, we can do this by
shifting the eigenvalues λji on the rays
λj(i+1) = αi
· λji, αi > 1,4.3. Variable Structure Control Without Sliding Mode 319
Re
Im
λ11
λ12
λ13
λ14
λ21
λ22
λ23 λ24
λ34 λ33 λ32 λ31
Fig. 4.31: Eigenvalues λji of the parameter switching control loop
further and further to the left in the negative half-plane for increasing values
i. Other eigenvalue progressions are also possible, as shown in Figure 4.31.
For each of the l control loops (4.30), we determine a catchment region
Gi =

x ∈ IRn
| x
T Rix < ci
	
,
where Ri results from the Lyapunov equation
Aˆ
T
i Ri + RiAˆi = −Qi
.
The matrices Qi must be chosen as suitable positive definite matrices.
The scaling factors ci determine the expansion of the catchment regions
Gi
. Using the Lagrange multiplier method to maximize a region Gi so that
its boundary is tangent to the hyperplanes
|u| = |k
T
i x| = umax,
as shown in Figure 4.32, we can calculate the scaling factor ci
, obtaining
ci =
u
2
max
k
T
i R
−1
i ki
.
This ensures that no trajectory x(t) beginning in Gi violates any control
variable limitation
|u| = |k
T
i x| ≤ umax,
since Gi
is a catchment region, and thus the trajectory x(t) does not leave it.
The next step is verifying whether all the catchment regions Gi are nested
within each other, i. e. if
Gl ⊂ Gl−1 ⊂ . . . ⊂ G2 ⊂ G1320 Chapter 4. Nonlinear Control of Linear Systems
x1
x2
−k
T
i x < −umax
−k
T
i x > umax
Gi
|k
T
i x| < umax
Fig. 4.32: Catchment regions Gi and
the control variable limitation
−k
T
1 x = umax
k
T
1x=umax
k
T
2x = umax
−k
T
2x=umax
x1
x2
G1
G2
G3
G4
G5
Fig. 4.33: A series of nested catchment
regions Gi
holds. Figure 4.33 illustrates this nesting.
If the nesting condition is fulfilled, each trajectory moves from a larger
region Gi
into the smaller region Gi+1. Since the switching is always to a
better, meaning a faster controller ki+1, the compensation becomes more rapid
for each region.
The nesting of the regions Gi
is simple to verify, since the requirement
Gi+1 ⊂ Gi
is fulfilled if and only if
x
T Ri x
ci
<
x
T Ri+1x
ci+1
or
0 < x
T

Ri+1
ci+1
−
Ri
ci

x
holds. The latter is fulfilled if the matrix
Si =
Ri+1
ci+1
−
Ri
ci
(4.31)
is positive definite. The nesting of all regions Gi therefore exists if all matrices
Si
, for i = 1, . . . , l − 1, are positive definite. If the nesting condition is not
fulfilled for a specified set of controller vectors ki and catchment regions Gi
,
new vectors ki and regions Gi with greater distances between them are chosen
such that condition (4.31) holds for all i = 1, . . . , l − 1.
The control law is given by
u = −k
T
i x4.3. Variable Structure Control Without Sliding Mode 321
with
i =



1 if x ∈ G1\G2,
2 if x ∈ G2\G3,
.
.
.
.
.
.
l − 1 if x ∈ Gl−1\Gl
,
l if x ∈ Gl
.
Here the set differences Gi\Gi+1 make up the zones between two regions Gi
.
A controller ki
is always active if the state vector x lies within the associated
ringlike zone Gi\Gi+1. Since, for a progressing compensation, the trajectory
x(t) subsequently enters smaller regions Gi and faster controllers ki are acti￾vated, the compensation compared to a classical linear state controller such as
u = −k
T
1 x becomes increasingly rapid. Figure 4.33 illustrates this procedure.
In summary, the following design scheme results:
Step 1: For the plant
x˙ = Ax + bu with |u| ≤ umax,
we select a family of l controllers
u = −k
T
i x, i = 1, . . . , l.
Step 2: For every subcontrol loop
x˙ =

A − bkT
i

x = Aˆix, i = 1, . . . , l,
we determine a catchment region
Gi =
(
x ∈ IRn
| x
T Rix < ci =
u
2
max
k
T
i R
−1
i ki
)
from
Aˆ
T
i Ri + RiAˆ = −Qi
, i = 1, . . . , l
with an arbitrary positive definite matrix Qi
.
Step 3: We verify whether the matrices
Si =
Ri+1
ci+1
−
Ri
ci
are positive definite for all i = 1, . . . , l − 1. If this is not the case
for an index i, a new design iteration must be started by returning
to Step 1.322 Chapter 4. Nonlinear Control of Linear Systems
If instead of a finite number l of controllers ki we select an infinite number
with infinitely many catchment regions Gi
, a soft variable structure control re￾sults [6, 192, 234, 267]. The regions Gi are then infinitesimally densely nested,
that is, without any space in between. This means the settling time can be
further improved and becomes nearly time-optimal. Furthermore, the control
signal is continuous, since the controller parameters are continuously changed
in dependence on the system’s state, and, thus, there is no switching between
any controllers.
4.3.3 Example: Ship-to-Shore Gantry Crane
As an example, we will consider a ship-to-shore gantry crane [303], as is used
for loading and unloading container ships. Here, the container is suspended
from a system of ropes under the crane’s trolley, which transports the con￾tainer from the dock to the ship, or vice-versa, as shown in Figure 4.34.
To achieve the fastest possible dispatching of the ship in the harbor, the
cranes should load, and unload, the maximum possible number of containers
per hour. This requires a fast back and forth movement of the trolley, which
can lead to oscillations of the containers. Evidently, these are undesired, since
they render the accurate placement of the containers impossible. A suitable
controller enables the trolley to move at high speed. Moreover, it prevents the
container from oscillating over its target position of placement.
The dynamics of the crane equipment are described by the model
x˙ =













0 1 0 0 0
0 0
−βδ
αγ − β
2
0
γ
αγ − β
2
0 0 0 1 0
0 0
−αδ
αγ − β
2
0
β
αγ − β
2
0 0 0 0 −ωs













x +






0
0
0
0
K






u.
Here, the position s of the trolley, its velocity s˙, the container’s swing angle
ϕ, the angular velocity ϕ˙, and the force F with which the trolley is accelerated
make up the elements of the state vector
x = [s s ϕ ˙ ϕ F ˙ ]
T
.
The current of the trolley’s electric motor is the input variable u. For the
parameters of the crane, we use the abbreviations
α = mt + ml
,
β = ml(f − e),4.3. Variable Structure Control Without Sliding Mode 323
s
ϕ
Fig. 4.34: Ship-to-shore gantry crane
γ = ml(f − e)
2 + J,
δ = ml
· g ·

e +
b
d − b
f +
(d − b)d
4l
2 − (d − b)
2
f

,
f =
bl
d − b
s
1 −
1
4

d − b
l
2
.
Here mt is the mass of the trolley, ml
is the mass of the container and its
mount, l is the rope length, b is the distances between the container’s rope
attachments, d is the distances between the trolley’s rope attachments, e is
the vertical distance of the container’s rope attachment to its center of gravity,
J is the container’s inertia moment, and g = 9.81m s−2
is the gravitational324 Chapter 4. Nonlinear Control of Linear Systems
Trolley
g l
d
b
e
Fig. 4.35: Trolley and container
acceleration, as shown in Figure 4.35. The constants K and ωs are parameters
of the electric motor.
In the following, we will consider a design with
mt = 32 t, ml = 18.8 t, J = 20.5 t m2
,
d = 3 m, b = 2 m, e = 1.6 m,
l = 15 m, ωs = 0.91 s−1
, K = 0.94 kN s−1A
−1
,
and a control variable limitation of
umax = 100 A.
With the above parameter specifications, we arrive at
x˙ =






0 1 0 0 0
0 0 −12.4949 0 0.0312
0 0 0 1 0
0 0 −1.1895 0 0.0011
0 0 0 0 −0.9100






x +






0
0
0
0
0.94






u (4.32)
for the model. The set of possible initial displacements x0 = x(0) is
X0 = {x0∈IR5
| |s| ≤ 20 m, |s˙| ≤ 3 m s−1
, |ϕ| ≤ 5
◦
, |ϕ˙| ≤ 2
◦
s
−1
, F = 0 kN}.
Note that, for clarity, the angle ϕ and the angular velocity ϕ˙ are given in de￾grees and degrees per second, respectively. In the differential equation (4.32),
on the other hand, radians and radians per second must be used.4.3. Variable Structure Control Without Sliding Mode 325
Next we will select ten state controllers with vectors ki
for the design of
the switching controller consisting of these subcontrollers ki and the selection
strategy. Then we determine ten adequate catchment regions
Gi =

x ∈ IR5
| x
T Rix < ci
	
.
The largest of these regions, G1, is specified such that it contains the initial
set X0. The parameters of the subcontrollers ki and the matrices Ri can be
found in Appendix A.2. The eigenvalues of the subcontrol loops
x˙ = (A − bkT
i
)x
are shown in Figure 4.36. As an initial displacement, we will take
x0 =






s
s˙
ϕ
ϕ˙
F






=






20
0
0
0
0






.
The simulation of the state variables x1 = s, x3 = ϕ, x5 = F, and the control
variable u is shown in Figure 4.37. As a comparison, the graphs also show
the time courses for the linear state controller with gain k1 and the time￾optimal feedforward controller. As expected, the control signal u(t) of the
switching controller, i. e. the motor current, contains discontinuities. These
can be tolerated in this case, since the force F, which results from the motor
current u and which drives the crane, has a continuous progression.
Re Re
λ1i
λ1i
λ2i
λ2i
λ3i
λ4
λ4i i
λ5i
λ5i
1 1
0.5 0.5
0 0
-0.5 -0.5
-1 -1
-5 -4 -3 -2 -1 0 -0.8 -0.6 -0.4 -0.2 0
Im
Im
Fig. 4.36: The left plot shows all eigenvalues of the ten subcontrol loops. The right
plot shows a detailed view of the area of interest on the left plot so as to better
display the values taken on by λ1i and λ2i as well as λ4i and λ5i.326 Chapter 4. Nonlinear Control of Linear Systems
Switching feedback control
Linear feedback control
Time-optimal feedforward control
Time t in s
20
10
10
0
0
0
0
-10
100
100
-100
-100
0
0
0
0
5
5
5
5
10
10
10
10
15
15
15
15
20
20
20
20
25
25
25
25
30
30
30
30
35
35
35
35
Current
u in A Force
x5 in kN Angle
x3 in degrees Position
x1 in m
Fig. 4.37: Time courses of the position x1, the oscillation angle x3, the driving force
x5, and the control variable u, i. e. the motor current
The simulation illustrates that the regulation for the switching controller is
substantially more rapid than that of a good linear controller. The linear state￾feedback gain vector k1, which is also the initial gain vector of the piecewise
linear controller, has been determined using parameter optimization such that,
on the one hand, the initial set X0 lies within G1 and, on the other hand, the
regulation progresses as rapidly as possible. A further improvement in the
settling time can be achieved by using more than ten controllers. However,
for an increasing number of controllers, the design complexity also increases.
Note that the improvement in the control variable utilization compared to the
linear controller comes at the cost of a higher control energy consumption.4.4. Saturation Controllers 327
4.4 Saturation Controllers
4.4.1 Basics and Stability
Again we will consider a linear plant
x˙ = Ax + bu
with the input constraints
|u| ≤ umax. (4.33)
As a controller, we will use a state controller u = −k
T x which is allowed to
violate the limitation (4.33). We thus obtain the control law
u = sat(−k
T x) =



umax, −k
T x > umax,
−k
T x, −umax ≤ −k
T x ≤ umax,
−umax, −k
T x < −umax.
(4.34)
Figure 4.38 shows the structure of the resulting control loop. In the following,
it will become apparent that this simple nonlinear controller structure not only
ensures the stability of the control loop but can also achieve fast regulation
for certain plants.
Here a set X0 of possible initial displacements x(0) is given. All trajec￾tories beginning in X0 need to be stabilized so that they finally tend to the
equilibrium point xeq = 0. For unstable plants, this stabilization is not possi￾ble for every set X0. The reason for this is the control variable limitation. If
umax is too low or X0 is too large, the available maximal control variable umax
does not suffice for large initial displacements x(0) to stabilize the trajectory
x(t) which starts at x(0).
In order to ensure stability, the set X0 is assumed to lie within an ellipsoid￾shaped catchment region
G =

x ∈ IRn
| x
T R x < c	
,
x
u
x˙ = Ax + bu
k
T
x
Fig. 4.38: Control loop with saturation controller u = − sat(k
T
x)328 Chapter 4. Nonlinear Control of Linear Systems
i. e. within a region which no trajectory of the control loop leaves. The matrix
R that defines this region will be determined in the course of the stability
analysis.
For the stability analysis, we will now transform the control law (4.34) so
that it takes on the form
u = − sat(k
T x) = −h
T x − p(k
T x − h
T x) (4.35)
with
p =
sat(k
T x) − h
T x
k
T
x − h
T
x
. (4.36)
Here h is configured in such a way that the catchment region G is a subset of
the set
Zh = {x ∈ IRn
| |h
T x| ≤ umax}.
The two hyperplanes
−h
T x = ±umax
can be tangent to the ellipse G, but must not intersect it. Figure 4.39 illustrates
this.
The state-feedback gain k is chosen such that it leads to regulation which
is significantly more rapid than the one we would obtain if we were to use h
instead of k. Its control signal u(t) is correspondingly larger and the set
Zk =
n
x ∈ IRn
| |k
T x| ≤ umaxo
intersects the catchment region G, as shown in Figure 4.40.
We will now address all state vectors x ∈ G. These will be regulated to
the equilibrium point xeq = 0 using the control law (4.34) or, equivalently,
(4.35). This yields different cases; we will first view the case for which
x ∈ Zk and x ∈ G
hold, i. e. the state x is in the dark blue shaded set shown in Figure 4.40.
Obviously, for such vectors x the inequalities
|h
T x| ≤ umax and |k
T x| ≤ umax
are fulfilled, so that with sat(k
T
x) = k
T
x we obtain the factor
p =
k
T x − h
T x
k
T x − h
T x
= 1 (4.37)
from equation (4.36). The control law (4.35) is linear for these values of x, so
that4.4. Saturation Controllers 329
x1
x2
Zh
h
T
x= umax
−h
T
x= umax
G
Fig. 4.39: Region G, set Zh, and the
hyperplanes |h
T
x| = umax
x1
x2
Zk
−k
T
x=umax
k
T
x=umax
G
Fig. 4.40: Region G, set Zk, and the
hyperplanes |k
T
x| = umax
u = −k
T x
applies.
As the next vector x to be adjusted, we will select an arbitrary vector x
which lies within G but not within Zk. This set G\Zk is highlighted in light
blue in Figure 4.40. For the states x ∈ G\Zk, the inequality
|k
T x| > umax
holds, i. e. the control is in saturation. Therefore, the control law
u = − sat(k
T x) = −umax sgn(k
T x) for x ∈ G\Zk
follows. Using
k
T x = |k
T x|sgn(k
T x),
h
T x = |h
T x|sgn(h
T x),
this yields
p =
umax sgn(k
T x) − |h
T x|sgn(h
T x)
|k
T x|sgn(k
T x) − |h
T x|sgn(h
T x)
for equation (4.36). We now multiply both the numerator and denominator
by sgn(k
T x) and arrive at
p =
umax − |h
T x|sgn(h
T x) sgn(k
T x)
|k
T x| − |h
T x|sgn(h
T x) sgn(k
T x)
=
umax ± |h
T x|
|k
T x| ± |h
T x|330 Chapter 4. Nonlinear Control of Linear Systems
for x ∈ G\Zk.
From this result, and with
|h
T x| ≤ umax < |k
T x| for x ∈ G\Zk,
we obtain
0 ≤ p < 1 (4.38)
for all x ∈ G\Zk. With equation (4.38) and equation (4.37), we finally obtain
0 ≤ p ≤ 1
for all x ∈ G.
To guarantee the stability of the control loop
x˙ = Ax − b sat(k
T x) = h
A − bhT − pb

k
T − h
T
i x
for all x ∈ G and to specify G as a catchment region, we choose
V (x) = x
T R x
as a Lyapunov function. For the time derivative of V , we obtain
V˙ (x) = x
T

A−bhT−pb

k
T−h
T
T
R+R

A−bhT −pb

k
T −h
T

x < 0.
(4.39)
Obviously, V˙ (x) is a linear function of p, so that V˙ (x) takes on its maxi￾mum value, either in p = 0, or in p = 1 for p ∈ [0, 1]. The requirement (4.39)
is thus fulfilled for all p ∈ [0, 1] if
x
T

A − bhT
T
R + R

A − bhT


x < 0
holds for p = 0, and
x
T

A − bkT
T
R + R

A − bkT


x < 0
holds for p = 1. Using the results above, we can now derive the following
stability theorem for the state control with saturation [191].
Theorem 58 (Hu and Lin’s Stability Theorem). The control loop
x˙ = Ax − b sat(k
T x)
possesses the asymptotically stable equilibrium point xeq = 0 with the catch￾ment region4.4. Saturation Controllers 331
G = {x ∈ IRn
| x
T R x < c}
if a vector h exists, so that
G ⊆ {x ∈ IRn
| |h
T x| ≤ umax} (4.40)
holds, and the matrices

A − bhT
T
R + R

A − bhT

= −Qh,

A − bkT
T
R + R

A − bkT

= −Qk
are negative definite.
Note that the vector h does not influence the control law u = − sat(k
T x).
However, it is of fundamental importance for the stability statement of The￾orem 58. Requirement (4.40) is equivalent to the inequality
c · h
T R
−1h ≤ u
2
max.
This inequality can be easily verified by inserting h and R−1
.
4.4.2 Design in Multiple Steps
Before describing the design of the saturation controller, we will first review
the tasks involved in its design. The control loop is stated as
x˙ = Ax − b sat(k
T x)
where a state-feedback gain k is sought such that all initial displacements
x(0) ∈ X0 are regulated to the equilibrium point xeq = 0. Thus stability is
ensured. The set of possible initial displacements X0 is often a hypercube
X0 = {x ∈ IRn
| −αi ≤ xi ≤ βi
, i = 1, . . . , n}.
Let us now consider the design procedure, which can be divided into six steps:
Step 1: Determine a vector h such that
x˙ = (A − bhT
)x
is asymptotically stable.
Step 2: Choose a positive definite matrix Qh and determine the matrix
R of the catchment region G = {x ∈ IRn
| x
T R x < c} from the
Lyapunov equation

A − bhT
T
R + R

A − bhT

= −Qh.332 Chapter 4. Nonlinear Control of Linear Systems
Step 3: Set
c =
u
2
max
h
T R−1h
so that the two hyperplanes |h
T x| = umax are tangent to the catch￾ment region G.
Step 4: Using the vertices xv,i of X0, verify whether X0 ⊆ G holds, i. e.
examine whether
x
T
v,iR xv,i < c
holds for all i = 1, . . . , 2
n
. If this is the case, proceed with Step
5. Otherwise, go back to Step 1 and choose a controller h with a
smaller control-variable amplitude, or go to Step 2 and vary Qh.
Step 5: Choose a state-feedback gain k and examine whether the matrix

A − bkT
T
R + R

A − bkT

= −Qk
is negative definite. If this is not the case, repeat Step 5.
Step 6: Simulate the control loop
x˙ = Ax − b sat(k
T x).
If the control behavior is satisfactory, finish the design. If it is not,
go to Step 1 or Step 5, and repeat the design from there.
The disadvantage of the above design procedure lies in its heuristic com￾ponents. The vectors h and k, as well as the matrix Qh must be chosen based
on guesswork. There is also no guarantee at the outset that X0 ⊆ G will hold.
Instead, different design steps must potentially be repeated multiple times,
and the procedure is based on trial and error.
The design procedure consisting of the six steps described above can also
be done on a computer and then run automatically. Furthermore, a one-step
design is possible [191] if all design requirements are summarized into a system
of linear matrix inequalities [55]. The disadvantages of the six-step design
procedure as described above do not occur in this case. Saturation controllers
can also be used for MIMO plants [191].
4.4.3 Example: Helicopter
As an example, we will control a rescue helicopter using an autopilot. Heli￾copters of this kind are used by the coast guards of various countries. In a
maritime rescue, as shown in Figure 4.41, the aim is for the helicopter to hover
in a stationary position over the surface of the water. In situations such as
gusty winds resulting in position displacements, the controller of the autopilot4.4. Saturation Controllers 333
brings the helicopter back to the stationary target position. Aiding the pilot
in this way is of great importance for emergency missions in stormy weather.
Below we will focus on the horizontal position displacement x4 of the
helicopter, measured in meters, relative to its rescue position. As part of the
autopilot function, we aim to compensate for this position deviation to zero
by using a state controller with saturation. Furthermore, we aim to correct
displacements caused by gusty winds or other forces to zero as quickly as
possible. The position change can be described by the model [98]
COAST 
GUARD
1404
x2
x4
u
Fig. 4.41: Emergency rescue helicopter334 Chapter 4. Nonlinear Control of Linear Systems
x˙ =




−0.415 0 −0.0111 0
1 0 0 0
−1.43 9.8 −0.0198 0
0 0 1 0




x +




6.27
0
9.8
0




u,
y =

0 0 0 1
x,
where x1 is the pitch angle rate, x2 is the pitch angle, x3 is the horizontal
velocity, and x4 is the position, or as mentioned above, the position deviation.
The control variable u is the adjustable rotor angle, measured in radians. The
rotor angle is limited by
−0.11 ≤ u ≤ 0.11.
For the set of possible disturbances, or initial position displacements, we will
select
X0 =

x ∈ IR4
| |x1| ≤ 0.1, |x2| ≤ 0.2, |x3| ≤ 1, |x4| ≤ 10	
.
For a saturation controller
u = − sat(k
T
x)
which corrects all disturbances of the helicopter’s position within X0, taking
the control variable limitation
|u| ≤ 0.11
into account, we determine
k
T =

0.16226 0.20860 0.0089414 0.0020975
.
Figure 4.42 shows the time courses of the correction for the disturbance dis￾placement
x0 =




0.1
0.2
1
10




.
The time courses of a linear controller
u = −klin
T x
with
klin
T =

1.7719 2.8852 0.15954 0.038056
and the time-optimal feedforward controller are also shown for comparison.
The substantially more rapid regulation by the saturation controller compared
to the linear controller is plainly evident. However, this comes at the cost of
a higher control signal velocity u˙.4.5. Exercises 335
20
15
10
5
-5
0
0
-0.2
-0.1
0.1
0.2
0
0
2
2
4
4
6
6
8
8
10
10
12
12
14
14
16
16
18
18
20
20
Control variable
u in rad Position
y in m
Time t in s
Linear feedback control
Saturation feedback control
Time-optimal feedforward control
Fig. 4.42: Time courses of the position y and the control variable u of the helicopter
4.5 Exercises
Exercise 4.1 Let us examine the control loop with a PID controller and
anti-windup from Figure 4.43. The PID controller’s transfer function is
GPID(s) = NPID(s)
s
= GPD(s) + KI
s
.
(a) Convert the control loop from Figure 4.43 such that it takes the same
form as the control loop from Figure 4.44 and thus makes possible the
application of the Popov criterion and the circle criterion. In doing this,
determine the transfer functions Q(s), R(s), and G(s)R(s) with depen￾dence on m, KI
, N(s), D(s), and NPID(s).
Let us now examine the case of the plant
G(s) = 1
s
and PI controller
GPI(s) = KPs + KI
s
, KP = 2, KI = 1.
(b) Calculate the transfer function G(s)GPI(s) of the open loop in the lin￾ear case, i. e. without anti-windup, its poles, and the transfer function
G(s)R(s) of the open loop with anti-windup and m = 1000.336 Chapter 4. Nonlinear Control of Linear Systems
yref
GPD(s)
KI
s
m
v u
v
u
y
G(s) = N(s)
D(s)
u = sat(v)
Plant
Fig. 4.43: Control loop with PID controller and anti-windup
yref
Q(s) G(s) = N(s)
D(s)
R(s)
v y
Fig. 4.44: Control loop
(c) Are the Popov criterion and the circle criterion applicable?
(d) Draw the Nyquist plots and the Popov plots of G(s)GPI(s) and G(s)R(s).
(e) If possible, identify the sectors of absolute stability for the control loop
without anti-windup and the one with anti-windup, such that the global
asymptotic stability of the origin is ensured.
(f) Calculate the Hurwitz sectors for the control loop with and without anti￾windup. Is Aizerman’s conjecture correct in these cases?
Exercise 4.2 Show that in the formulas (4.15) and (4.16) of the optimal
switching times of the double integrator on p. 304, only a positive sign in
front of the square root yields the correct switching times.
Exercise 4.3 Calculate the time-optimal control feedback law for the plant
x˙ =

0 1
0 0
x +

0
1

u, |u| ≤ umax,
using the following steps:
(a) Determine the trajectories of the plant for the two control values u = umax
and u = −umax.4.5. Exercises 337
(b) Determine the trajectories from (a) which pass through x = 0. The parts
of these two trajectories which move in the direction of and towards x = 0
make up the switching curve S(x) = x1.
(c) Using the trajectories from (b), determine the time-optimal feedback con￾trol law u(x) for all x ∈ IR2
.
Exercise 4.4 For an unstable linear system
x˙ = Ax + bu, x ∈ IRn
,
with a control variable limitation |u| ≤ umax, no controller u(x) exists with
which all initial values x(0) ∈ IRn
can be transitioned to the equilibrium point
xeq = 0, i.e. the system cannot be stabilized for all x(0) ∈ IRn
. However, this
is possible for a subset of IRn
. The largest of these sets is called the null￾controllable region Xnull. Determine the null-controllable region Xnull for the
unstable first-order system
x˙ = λx + bu, |u| ≤ umax, λ > 0.
Exercise 4.5 Determine the switching time t1 of the time-optimal feedforward
controls and time-optimal feedback control laws u(x) for the plants
(a) x˙ = u, |u| ≤ umax,
(b) x˙ = λx + bu, |u| ≤ umax, λ ∈ IR \ {0}, b > 0.
Exercise 4.6 Determine the time-optimal feedback control law for the system
x˙ =

λ 1
0 λ

x +

0
λ

u, λ < 0, |u| ≤ umax.
In doing so, take into account that the switching curve is identical to sections
of the system’s trajectories for which u = umax and u = −umax apply and
which pass through the origin x = 0.
Exercise 4.7 The time-optimal feedback control of the triple integrator [96,
111, 141]
x˙ =


0 1 0
0 0 1
0 0 0

x +


0
0
1

u
with the control variable limitation |u| ≤ 1 is given by
u = − sgn(a|a| + b
3
),
a = x1 +
1
3
x
3
3 + x2x3 sgn(x2 +
1
2
x3|x3|),
b = x2 +
1
2
x
2
3
sgn(x2 +
1
2
x3|x3|).
State the control law for the case |u| ≤ umax.338 Chapter 4. Nonlinear Control of Linear Systems
Exercise 4.8 Nuclear fusion reactors make possible the controlled fusion of
deuterium and tritium nuclei. The heat that results can be used to produce
electricity. The Tokamak is one design for a plant in which an electricity￾conducting plasma with a temperature of up to 150 million degrees Celsius is
generated in a torus-shaped chamber. This plasma is shaped and held within
the chamber by means of magnetic fields of up to 10 T such that it does not
touch the walls of the chamber. If this were not the case, the plasma would
cool and the nuclear fusion would stop. The magnetic fields are generated
by means of active inductors, i.e. inductors to which an external variable
voltage ua is applied and through which an electrical current ia flows, and
passive inductors in which an electrical current iv is induced by the magnetic
fields. These passive inductors have no external electricity supply. Using these
inductors and their magnetic fields, the plasma, or more accurately, its center
line, can be positioned within the torus; see Figure 4.45. The vertical shift z
of the midpoint of the plasma can be represented by the simple model [407]




˙ia
˙iv




=
1
1 − kav




−
Raa
Laa
kav
Rvv
Lav
kav
Raa
Lav
−
Rvv(kav − Mvp)
Lvv(1 − Mvp)








ia
iv




| {z }
i
+




1
Laa(1 − kav)
−
kav
Lav(1 − kav)




ua
z=
1
ip

−
Lap
App
−
Lvp
App 
i, i =

ia iv
T
.
(4.41)
Here ip is the plasma flow. In the case of the joint European torus (JET), we
have the following parameters:
Raa = 35.0 · 10−3 Ω (Resistance of the active inductors),
Rvv = 2.56 · 10−3 Ω (Resistance of the passive inductors),
Laa = 42.5 · 10−3 H (Self-inductance of the active inductors),
Lav = 0.432 · 10−3 H (Mutual inductance of the active and passive
inductors),
Lvv = 0.012 · 10−3 H (Self-inductance of the passive inductors),
Lap = 115 · 10−6 Hm−1
(Mutual inductance between active inductors and
plasma per unit of length),
Lvp= 3.2 · 10−6 Hm−1
(Mutual inductance between passive inductors and
plasma per unit of length),
App = 0.5 · 10−6 Hm−2
.
In addition, the following apply:4.5. Exercises 339
Active inductor Passive inductor
Fig. 4.45: Tokamak-type nuclear fusion reactor
kav =
L
2
av
LaaLvv
and Mvp =
AppLvv
L2
vp
.
For the maximum inductor voltage ua, the limitation
|ua| ≤ 10 kV
applies, while the plasma current is ip = 400 · 103 A.
(a) Determine the model (4.41) of the JET and its eigenvalues λ1 and λ2
numerically.
(b) Transform the model of the JET calculated in (a) into the form
x˙ =

λ1 0
0 λ2

x +

λ1
λ2

u, λ1 > 0, λ2 < 0, (4.42)
with u = ua. State the transformation i = T x.
(c) For a general system (4.42) with |u| ≤ umax, determine the time-optimal
feedback control. For this purpose, determine the trajectories of the sys￾tem for the control signal values −umax and umax, and put together the
switching curve of the time-optimal control using sections of the trajecto￾ries that pass through the equilibrium point xeq = 0.340 Chapter 4. Nonlinear Control of Linear Systems
(d) Calculate the region G ⊆ IR2
in x-coordinates for which the JET can be
stably controlled to zero, taking into account the control variable limita￾tion |u| ≤ umax = 10 kV.
Exercise 4.9 Let the plant
x˙ =

0 1
0 0
x +

0
1

u (4.43)
with the control variable limitation |u| ≤ umax be given.
(a) Determine the time-optimal feedback control law uto(x) which calculates
the correct actuator value uto(x) for all x ∈ IR2
.
(b) In addition, determine a linear state controller u = −k
T x for the plant
(4.43) such that both eigenvalues of the control loop are λ1,2 = −1.
(c) Explain why the function
V (x) = x
T x
is a Lyapunov function of the linear control loop with the controller from
(b), and why it ensures asymptotic stability of the equilibrium point
xeq=0.
(d) Examine the controller
u(x) = (
−k
T x, if x
T x ≤ 0.01,
uto(x), if x
T x > 0.01,
(4.44)
which starts the control process with a time-optimal controller and then
switches to a linear state controller in the final phase. Such a controller
with two different modes is called a dual-mode controller. In the state
plane, draw the areas in which u = umax, u = −umax, and u = −k
T x
apply.
(e) Why is the equilibrium point xeq = 0 of the control loop (4.43), (4.44)
globally asymptotically stable?
(f) What advantage does the control loop with the control law (4.44) have
compared to the time-optimal control loop?
Exercise 4.10 Let us examine the discrete-time first-order system
x(k + 1) = ax(k) + bu(k), 0 < a < 1, b > 0,
with the control variable limitation |u| ≤ umax.
(a) Calculate x(k) in dependence on the initial value x(0) and the input values
u(0), u(1), . . . , u(k − 1).
(b) Demonstrate that
x(k)a
−k = x(0) +X
k
i=1
a
−i
bu(i − 1)
applies.4.5. Exercises 341
(c) Determine the minimum number N of steps k with which the system,
starting at x(0) ∈ IR, can be brought to the origin, i.e. x(N) = 0.
(d) Now determine the control sequence u(0), u(1), . . . , u(N − 1) with the
optimal number of steps which brings the system from x(0) to the origin
in the minimum number of steps N.
Exercise 4.11 Let us examine the switching system
x˙ = Aix with Ai =
(
A1, 2i∆t ≤ t < (2i + 1)∆t, i = 0, 1, 2, . . . ,
A2, (2i + 1)∆t ≤ t < (2i + 2)∆t, i = 0, 1, 2, . . .
(a) Show that if A1 + A2 only has stable eigenvalues and a sufficiently small
value of ∆t is selected, this is sufficient for the solution x(t) for all initial
values x0 = x(0) ∈ IRn
to tend to zero for t → ∞ .
(b) State a sufficient and necessary criterion for global asymptotic stability of
the switching system above.
(c) Determine whether the switching system for
A1 =

−1 −4
0 −1

and A2 =

−1 0
9 −1

is stable if in a first case study ∆t = ln(1.4) and in a second ∆t = ln(3)
apply.
Exercise 4.12 Let us consider a linear system
x˙ = Ax + bu, x ∈ IRn
, (4.45)
and a piecewise constant control
u = ui
for ti−1 ≤ t < ti with i = 1, . . . , N, N ∈ IN and t0 = 0, (4.46)
with the values ui each being constant.
(a) Let N = 2. Calculate the values u1 and u2 for
A=

0 −1
1 0
, b=

0
1

, x(0)=
−2
−4

, x(t2)=
0
0

, N=2, t1=
π
2
, and t2=
3
2
π.
(b) Repeat the calculation of u1 and u2 for the switching times t1 = π and
t2 = 2π. What result do you obtain?
(c) Derive a necessary and sufficient criterion for the existence of a control
(4.46) of the general system (4.45) which is capable of driving any initial
state x(0) into any other state x(tN ) in time tN .
Exercise 4.13 Below we will attempt to design a piecewise linear control.
Let us take a controllable and observable linear plant which is given without
restriction of generality in the controller canonical form342 Chapter 4. Nonlinear Control of Linear Systems
x˙ =







0 1 · · · 0
0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 1
−a0 −a1 · · · −an−1







x +







0
0
.
.
.
0
1







u
and has a parameter-dependent state controller
u = −k
T
(q)x with k(q) = D−1
(q)aˆ − a
with the diagonal matrix
D(q) = diag(q
n
, qn−1
, . . . , q), q ∈ (0, 1],
and the vectors
a =

a0 a1 · · · an−1

and aˆ =

aˆ0 aˆ1 · · · aˆn−1

.
Here the values aˆi represent the coefficients of the characteristic polynomial
of the closed control loop when q = 1. The control variable u is limited by
|u| ≤ umax.
(a) Demonstrate that the closed control loop
x˙ = (A − bkT
(q))x (4.47)
has the system matrix
Aˆ(q) = 1
q
D(q)Aˆ1D−1
(q), Aˆ1 = Aˆ(1) = A − bkT
(1).
(b) Show that for the eigenvalues λi(q) of Aˆ(q)
λi(q) = λ1i
q
, λ1i = λi(1),
holds.
(c) Demonstrate that each set
G(q) = 
x ∈ IRn
| e(q)x
TD−1
(q)RD−1
(q)x ≤ 1
	
(4.48)
is a catchment region of the equilibrium point xeq = 0 of the control loop
(4.47) if the matrix
Aˆ
T
1 R + RAˆ1 = −Q
is negative definite. Here R ∈ IRn×n
is a positive definite matrix and e(q)
is a positive function with which the size of the regions G(q) can be varied.4.5. Exercises 343
(d) Show that each of the areas G(q) is tangent to the corresponding hyper￾planes
|k
T
(q)x| = umax
if
e(q) = k
T
(q)D(q)R−1D(q)k(q)
u
2
max
applies.
(e) Parameter-dependent convex areas
G(q) = {x ∈ IRn
| g(x, q) ≤ 1}
are nested within one another without touching if
∂g(x, q)
∂q < 0
applies for all x ∈ IRn
. Demonstrate that the areas (4.48) are nested
within one another if
∂e(q)
∂q ≤ 0
applies and the matrix S in the equation
NR + RN = −S, N = diag(−n, −(n − 1), . . . , 1),
is positive definite.
(f) Create a procedure by which the above results can be used to design a
piecewise linear control in accordance with Section 4.3.2, p. 318 et seq.
(g) Based on the above results, determine a soft variable structure controller.
The latter consists of the control law u(x, q) and the equation to calculate
q. Note that, in this case, the catchment regions G(q) are infinitesimally
densely nested and the parameters of the controller vector k(q) are con￾tinuously varied. State the three requisite design equations.
(h) Explain why V (x) = q is a Lyapunov function of this soft variable struc￾ture control.
Exercise 4.14 What characteristic does the control law
u = − sat(h
T x) for x ∈ G
in Hu and Lin’s theorem on p. 330 have for k = h?
Exercise 4.15 Let us consider the linear plant
x˙ =

0 1
−1 0
x +

0
1

u
with the control variable limitation
|u| ≤ umax = 6.
Design a saturation controller based on Hu and Lin’s theorem on p. 330.
Proceed as follows:344 Chapter 4. Nonlinear Control of Linear Systems
(a) What eigenvalues does the plant have?
(b) Determine a state controller u = −h
T x such that the linear control loop
x = (A − bhT
)x has the eigenvalues s1,2 = −1.
(c) For the diagonal matrix Qh = diag(2, 2), determine the matrix R from
the Lyapunov equation
(A − bhT
)
T R + R(A − bhT
) = −Qh
.
(d) Determine the catchment region
G = {x ∈ IR2
| x
T Rx < c}
such that it is tangent to the hyperplanes of the control variable limitation
|u| = |h
T x| = umax.
(e) Select the controller u = −k
T x such that the eigenvalues of the linear
control loop x˙ = (A − bkT
)x are λ1,2 = −2.
(f) Is the equilibrium point xeq = 0 of the nonlinear control loop
x˙ = (A − b sat(k
T x))
asymptotically stable for all x ∈ G? Give reasons for your answer.5
Nonlinear Control of Nonlinear Systems
5.1 Gain-Scheduling Control
5.1.1 Mode of Operation and Design
Gain-scheduling methods allow for the design of relatively simple field-proven
controllers for nonlinear plants based on linear systems theory. For this reason,
gain-scheduling controls are widespread. Typical applications are flight con￾trol systems, controllers in the chemical processing industry, and automotive
engine controls.
For this class of controllers, the plant is linearized for different operating
points. This results in a series of linear submodels, for each of which a linear
controller is designed. In rare cases, nonlinear controllers are chosen. During
operation, the controller whose associated plant model corresponds best to
the current situation is activated or, alternatively, one suitably interpolates
between the controllers. Based on the linearized plant, it is possible to apply
the design methods of linear systems theory to nonlinear plants. This can
greatly simplify the design of control laws for nonlinear systems, which is
certainly one reason for the popularity of this design method.
The starting point of gain scheduling is the nonlinear plant
x˙ = f(x,u, d), x ∈ IRn
, u ∈ IRm, d ∈ IRs
,
y = g(x), y ∈ IRr
.
The vector d describes possible external factors or disturbances. The system’s
equilibrium points xeq are determined from the equation
x˙ = f(xeq,ueq, deq) = 0.
They depend on the input variable vector ueq, and possibly on external factors
deq, such as disturbances. These equilibrium points or operating points of the
system can commonly be represented as functions of a parameter vector ρ in
the form
© Springer-Verlag GmbH Germany, part of Springer Nature 2024
J. Adamy, Nonlinear Systems and Controls,
https://doi.org/10.1007/978-3-662-68690-4_5
345346 Chapter 5. Nonlinear Control of Nonlinear Systems
ueq = ueq(ρ),
xeq = xeq(ρ),
deq = deq(ρ),
and
yeq = g(xeq(ρ)) = yeq(ρ).
In the following, the dimension of the parameter vector ρ is indicated by l.
From the set
E = {ueq(ρ), xeq(ρ), deq(ρ), yeq(ρ), ρ ∈ M ⊂ IRl
} ⊂ IRm+n+s+r
of equilibrium points, a subset of p equilibrium points (or operating points)
(ueq(ρi
), xeq(ρi
), deq(ρi
), yeq(ρi
)), i = 1, ..., p,
is selected. Around each of these, a linearized model
∆x˙ i = A(ρi
)∆xi + B(ρi
)∆ui + S(ρi
)∆di
,
∆yi = C(ρi
)∆xi
is derived, where
∆xi = x − xeq(ρi
),
∆ui = u − ueq(ρi
),
∆di = d − deq(ρi
),
∆yi = y − yeq(ρi
),
and
A(ρi
) = ∂f(x,ueq(ρi
), deq(ρi
))
∂x




x=xeq(ρi
)
,
B(ρi
) = ∂f(xeq(ρi
),u, deq(ρi
))
∂u




u=ueq(ρi
)
,
S(ρi
) = ∂f(xeq(ρi
),ueq(ρi
), d)
∂d




d=deq(ρi
)
,
C(ρi
) = ∂g(x)
∂x




x=xeq(ρi
)
hold. Below, operating points are indicated by ρi
for simplicity instead of
using the operating point
(ueq(ρi
), xeq(ρi
), deq(ρi
), yeq(ρi
))5.1. Gain-Scheduling Control 347
x
u
1
2
3
4
5
6
Validity region of the
linearized model 5
Set of all equilibrium points (ueq(ρ), xeq(ρ))
(ueq(ρ3), xeq(ρ3))
Fig. 5.1: Validity regions of the parameterized linearization family
itself. The set of linearized models is called the parameterized linearization
family. These p linearized models have only a limited validity range around
the respective operating points. The regions of validity should be adjacent to
each other without gaps and should overlap, as illustrated in Figure 5.1. The
parameterized linearization family often does not cover the entire space of the
input and state variables with its validity ranges, so that this model family no
longer reflects the behavior of the nonlinear system sufficiently well outside
these regions.
For each of the p linearized models with the associated parameter vector
ρi
, a controller
z˙ i = hi(zi
, ∆di
, ∆xi
, ∆yi
, ∆yref,i),
∆ui = ki(zi
, ∆di
, ∆xi
, ∆yi
, ∆yref,i)
is designed whose optional dynamic is determined by a differential equation
with controller states zi
. Here,
∆yref,i = yref − yref,eq(ρi
)
is the deviation of the reference value vector yref from the operating point
yref,eq(ρi
). Figure 5.2 shows the i-th of these p control loops, which consists of
the i-th subcontroller and the plant which is linearized at the operating point
ρi
. The subcontrollers can be of various types, such as linear state controllers,
PID controllers, etc.
After designing the p controllers, we combine them into an overall con￾troller. This combining can be performed in different ways. It can be done, for
example, so that only one controller is activated. A more suitable method is
the interpolation between two or more controllers. Ideally, the selection and
the interpolation depend on the parameter vector ρ(t). However, the vector ρPSfrag 348 Chapter 5. Nonlinear Control of Nonlinear Systems
∆yref,i ∆ui
∆xi
∆yi
∆yi z˙i = hi(zi, ..., ∆yref,i)
∆ui = ki(zi, ..., ∆yref,i)
∆x˙ i =A(ρi
)∆xi+B(ρi
)∆ui+S(ρi
)∆di
∆yi = C(ρi
)∆xi
∆di ∆di
Fig. 5.2: The i-th control loop of a gain-scheduling controller
cannot be determined or measured for every application. It is then replaced
by a vector β(t) which is referred to as the scheduling vector , whose elements
βk(t) are composed of known quantities such as
(1) the state variables xk,
(2) the output variables yk,
(3) the control variables uk,
(4) the controller states zk, or
(5) the reference values yref,k.
A requirement is for
l = dim(ρ) = dim(β)
to hold and for the steady-state case
βs = β(t → ∞) = ρ (5.1)
to exist.
The simplest way to generate the control signal u is to activate a single
controller i with the control signal
u = ueq(ρi
) + ∆ui
. (5.2)
The controller i is determined by
i = arg min
k
|β − ρk
|. (5.3)
Thus, the controller i whose parameter vector ρi
is closest to the current value
of the scheduling vector β is active.
The control law (5.2) is simple, but has the disadvantage of switching
between controllers by means of the selection law (5.3). This results in unde￾sired jumps in the control signal which most of the actuators cannot perform
or withstand. It is therefore sensible to use a control law which interpolates
between the controllers, or more precisely, between their control variables.
The actuating variable u is then composed of the values ueq(ρi
) and ∆ui
.
This is achieved by interpolating between the controllers i in whose validity
region the scheduling vector β lies.5.1. Gain-Scheduling Control 349
The interpolation between the control variables ueq(ρi
) + ∆ui of the indi￾vidual controllers is carried out based on factors such as the weighted arith￾metic mean, which then results in the equation of the gain-scheduling con￾troller
u =
P
p
i=1
µi(β) · (ueq(ρi
) + ∆ui)
P
p
i=1
µi(β)
. (5.4)
The weight functions µi satisfy
0 ≤ µi(β) ≤ 1,
are equal to one at the operating point
β = ρi
,
and continuously decrease to zero or approximately to zero at the boundary
of the validity region which belongs to this operating point. One function with
this property is the multidimensional Gaussian function
µi(β) = e
−|Σ(β−ρi
)|
2
= e
−(β−ρi
)
T ΣT Σ(β−ρi
)
. (5.5)
The l × l matrix Σ can be a diagonal matrix such as
Σ =






1
√
2σ1
· · · 0
.
.
.
.
.
.
.
.
.
0 · · ·
1
√
2σl






.
The diagonal matrix gives each coordinate direction of the scheduling vector
β its own scaling with 1/(2σ
2
j
). Thus, in addition to circles as shown in Figure
5.1, ellipses can also be formed as contour lines of the function (5.5). In this
case, the axes of the ellipses are Cartesian oriented. If we use non-diagonal
positive definite matrices ΣT Σ, we can orient the ellipses arbitrarily in space.
Normally, the functions µi(β) are designed using an appropriate choice of
the parameters σi
, i = 1, . . . , l, such that each function µi(β) is nearly zero
for β = ρj
if j 6= i and equal to one if j = i. Then, for β = ρi
, equation (5.4)
reduces to
u ≈
µi(ρi
) · (ueq(ρi
) + ∆ui)
µi(ρi
)
= ueq(ρi
) + ∆ui (5.6)350 Chapter 5. Nonlinear Control of Nonlinear Systems
0 0.5 1 1.5 2 2.5 3 3.5 4
1
0.5
0
β
u
i = 1 i = 2
i = 3
Fig. 5.3: The interpolation u of the controller output values ueq(ρi) + ∆ui based on
the weighted arithmetic mean is shown as the blue curve. The black curves depict
the functions µi(β) · (ueq(ρi) + ∆ui) for i = 1, . . . , 3.
such that only the associated controller ∆ui becomes active at the operating
point ρi
. Between the operating points, a weighted average of the individual
controller output variables ueq(ρi
) + ∆ui
is used. Figure 5.3 illustrates this.
The interpolation law (5.4) has the often favorable property that, if a
vector β lies outside the valid operating regions in question, the controller
of the operating points closest to β remains active. This can be seen from
equation (5.4), because for vectors β of this type,
µi(β) ≪ 1
holds for all i. Thus the highest of these values µi(β), meaning the one with
the smallest Euclidean distance |β − ρi
|, has the greatest effect on equation
(5.4). In this case, equation (5.6) applies as well. Figure 5.3 illustrates this
property.
Overall, the design of a gain-scheduling control can be implemented in four
steps:
Step 1: Calculate the parameterized linearization family.
Step 2: Design the linear subcontrollers using linear design methods.
Step 3: Define the selection or interpolation law for the subcontrollers,
which depends on the scheduling parameters.
Step 4: Simulate the entire control loop and check stability and control
quality.
Step 4 in particular requires attention, as stability generally cannot be
ensured using analytical means. The stability of each of the linear subcontrol
loops is not sufficient in this case, since the entire system is nonlinear.5.1. Gain-Scheduling Control 351
Additional and supplementary material on gain-scheduling control can be
found in [21, 97, 107, 265, 468, 499]. An overview can be found in [266, 393,
472], among other sources.
5.1.2 Illustrative Example
Let us consider the simple example
x˙ = −x
3 + u.
The system has the equilibrium point
xeq =
√3 ueq.
Choosing
ρ = ueq
results in the parameterized linearization family
∆x˙ = A∆x + b∆u =
∂f(x, ueq)
∂x




xeq
· ∆x +
∂f(xeq, u)
∂u




ueq
· ∆u
= −3x
2
eq∆x + ∆u = −3
3
q
u
2
eq∆x + ∆u.
Furthermore, five operating points with
ρ1 = ueq1 = −2, xeq1 = −
√3
2 ≈ −1.26,
ρ2 = ueq2 = −1, xeq2 = −
√3
1 = −1,
ρ3 = ueq3 = 0, xeq3 =
√3
0 = 0,
ρ4 = ueq4 = 1, xeq4 =
√3
1 = 1,
ρ5 = ueq5 = 2, xeq5 =
√3
2 ≈ 1.26,
and
∆ui = u − ueq,i and ∆xi = x − xeq,i
are selected. Thus we obtain five linearized models with
ρ1 = ueq1 = −2 and ∆x˙ 1 = −4.76∆x1 + ∆u1,
ρ2 = ueq2 = −1 and ∆x˙ 2 = −3.00∆x2 + ∆u2,
ρ3 = ueq3 = 0 and ∆x˙ 3 = ∆u3,
ρ4 = ueq4 = 1 and ∆x˙ 4 = −3.00∆x4 + ∆u4,
ρ5 = ueq5 = 2 and ∆x˙ 5 = −4.76∆x5 + ∆u5.
For each model, a proportional controller with the control error
∆ei = ∆yref,i − ∆xi
and352 Chapter 5. Nonlinear Control of Nonlinear Systems
∆ui = ki∆ei
is designed, so that the corresponding control loops have the eigenvalues
λ1 = −12, λ2 = −4.5, λ3 = −1.5, λ4 = −4.5, λ5 = −12.
The subcontrollers ∆ui = ki∆ei have the parameters
k1 = 7.24, k2 = 1.50, k3 = 1.50, k4 = 1.50, k5 = 7.24.
The switching rule from equations (5.2) and (5.3) is applied here and results
in the overall control law
u = ueq(ρi) + ∆ui = ueq,i + ∆ui = ueq,i + ki
· ∆ei (5.7)
with
i = arg min
k=1,...,5
|β − ueq,k|. (5.8)
If we select
β = ρ = u
as the scheduling parameter in the control law (5.7), (5.8), the control law is
implicit in u. This is unfavorable, because in this case computing u is a com￾plex and time-consuming task. A more suitable option using the equilibrium
equation
x˙ = −x
3 + u = 0 ⇔ u = x
3
,
results in choosing
β = x
3
.
Obviously in this case, condition (5.1), i. e. βs = ρ for the steady-state case,
is fulfilled.
Figure 5.4 shows the entire control loop. We have
yref,eq,i = xeq,i
and
∆ei = ∆yref,i − ∆xi = yref − x = ∆e.
Figure 5.5 shows the simulation of the control-loop behavior for a step in the
reference variable from
yref = 0
to
yref = 1.5
at time t = 0. Since we are only using P controllers, there is a permanent
control error. The discontinuities in the course of u are caused by a change in
the index i, meaning a change in ueq,i and ∆ui according to
u = ueq,i + ∆ui
.5.1. Gain-Scheduling Control 353
yref ∆e u
x
x
i
∆yref,i =
yref − yref,eq,i
u = ueq,i + ki∆e x˙ = −x
3 + u
arg min
k
|x
3−ueq,k|
∆xi = x −
√3 ueq,i
Fig. 5.4: Block diagram of the entire control loop
0
0.5
1.5
1
1
2
3
4
5
0
0
0.5
0.5
1
1
1.5
1.5
2
2
2.5
2.5
Time t
u
x
State variable x
Reference variable yref
Fig. 5.5: Simulation of state variable x and actuating variable u from the gain￾scheduling control of the plant x˙ = −x
3 + u
As mentioned above, this is often undesirable, because not every actuator can
perform such jumps. But the abrupt change in the course of the state vari￾able x is also undesirable if we consider a flight controller, for example. In
many cases, a smooth progression of actuating variables and state variables is
preferable. In the next section, we will look at an example in which interpola￾tion takes place between the controllers so that no jumps occur in the control
signal.354 Chapter 5. Nonlinear Control of Nonlinear Systems
5.1.3 Example: Solar Power Plant
Below we will describe the solar power plant Acurex with a parabolic trough
collector, located in Almería in the south of Spain [67, 209, 392]. On an area
of 2674 m2
, the power plant has 480 solar collectors, which for a solar radia￾tion of 900 Wm−2
generate an electrical power of 1.2 MW. Figure 5.6 shows
the fundamental structure of such a system. Parabolic troughs focus the sun’s
rays via their mirrored surface in a translucent pipe in which oil circulates.
The concentrated solar radiation heats the oil, which then flows into an insu￾lated storage tank with a volume of 140 m3
. From here, the oil is transported
into a steam generator using a pump in which the heat of the oil is used to
generate steam. Finally, this steam is used to operate a turbine and an electric
generator. After leaving the steam generator, the oil is pumped back into the
Oil
Te
Sun ray
q, To
Tank
To the
turbine
Fig. 5.6: Solar power station5.1. Gain-Scheduling Control 355
collector array’s pipes at a rate of 2 to 10 ls−1
. The oil is therefore constantly
circulating throughout the plant.
The oil’s outlet temperature To is a measure of the collected energy. The
dynamic behavior of the outlet temperature when leaving the collector field
can be approximately described by the power balance
CT˙
o = η0SJ − qpcp(To − Te). (5.9)
Here, Te is the temperature of the oil when entering the collector field. It is
either constant or varies only very slowly and slightly. The solar radiation
intensity is taken into account by the variable J, where 0 ≤ J ≤ 1 kWm−2
holds.
Unlike the intensity J of the solar radiation, the volume flow q of oil can be
set between 2 and 10 ls−1
by adjusting the pump power. Furthermore, η0 is
the optical efficiency factor of the collectors, S is the effective collector area,
C the heat capacity of the oil, and pcp a system constant. For the Acurex
plant in Almería, the parameter set
η0S = 1322 m2
, C = 2267 kJ ◦C
−1
, pcp = 1.924 kJ ◦C
−1
l
−1
applies.
The oil temperature To must not exceed the temperature of 305◦C, and
in particular the difference between the inlet temperature Te and exit tem￾perature To must not be higher than 100◦C. Normal values for this difference
are around 70◦C. If the temperature difference To − Te is too great, the oil
pressure in the pipes of the collector field rises too high. Leaks can be the
result.
Therefore, our aim is to maintain the temperature difference
∆T = To − Te
at the constant value of 70◦C. This is done using a control system which
affects the temperature To and thus ∆T by controlling the volume flow q of
the oil in equation (5.9) or in the alternative equation
C(∆T )˙ + CT˙
e = η0SJ − qpcp∆T. (5.10)
Since the inlet temperature Te is approximately constant or varies only very
slowly, i. e. T˙
e ≈ 0, we obtain the approximation
C(∆T )˙ = η0SJ − qpcp∆T (5.11)
for equation (5.10).
To be in conformity with the rules of common denomination, we then
specify the state ∆T of system (5.11) as x and its control variable q as u. We
thus obtain
x˙ = aJ − bxu (5.12)356 Chapter 5. Nonlinear Control of Nonlinear Systems
with
a = η0SC−1 = 0.5831◦Cm2
kJ−1
and
b = pcpC
−1 = 0.8487 · 10−3
l
−1
.
The intensity J of the solar radiation, from a control engineering point of
view, is a disturbance variable.
The equilibrium points xeq of system (5.10) are given by
xeq =
aJeq
bueq
. (5.13)
The temperature difference xeq in the steady state is therefore proportional
to the measurable intensity J of the solar radiation. In the case of a constant
temperature difference xeq = 70◦C, we obtain
ueq = 9.82Jeq
by using equation (5.13).
In a manner similar to [209], we wish to control the system (5.12) by means
of gain-scheduling. For this purpose, we will first determine the parameterized
linearization family
∆x˙ =
∂aJ
∂J




J=Jeq
· ∆J −
∂bxueq
∂x




x=xeq
· ∆x −
∂bxequ
∂u




u=ueq
· ∆u (5.14)
of the system, where the variables
∆J = J − Jeq, ∆x = x − xeq, ∆u = u − ueq
denote the deviations from the values at the equilibrium points.
The linearized model
∆x˙ = a∆J − bueq∆x −
aJeq
ueq
∆u
follows from equation (5.13) and equation (5.14), where the parameter vector
ρ is
ρ = [ueq Jeq]
T
.
From the possible ranges of parameters,
2 ls−1 ≤ ueq ≤ 10 ls−1
and 0 kWm−2 ≤ Jeq ≤ 1 kWm−2
,
we select six operating points
ρi =

ueq,i
Jeq,i
∈
 4
0.5

,

6
0.5

,

8
0.5

,

4
0.8

,

6
0.8

,

8
0.8
5.1. Gain-Scheduling Control 357
4 5 6
1 2 3
2 4 6 8 10
0.2
1
J in kW m
−2
u in l s−1
∆T > 100◦C
∆T = 70◦C
Fig. 5.7: Operating points (1, 2, 3, 4, 5, and 6) and validity regions of the linear
submodels in which the respective Gaussian function of an operating point dominates
as illustrated in Figure 5.7. The region above the straight line
Jeq = 0.1455ueq
yielded by equation (5.13) includes temperature differences of ∆T > 100◦C.
This region, in which the control loop must not be operated, is shown in blue
in Figure 5.7. The desired steady-state conditions with xref = ∆T = 70◦C,
which depend on the intensity J of the solar radiation, are represented by a
straight line that is consistent with
Jeq = 0.1019ueq.
We thus obtain
i = 1 : ∆x˙ 1 = 0.5831 · ∆J1 − 3.395 · 10−3
· ∆x1 − 72.89 · 10−3
· ∆u1,
i = 2 : ∆x˙ 2 = 0.5831 · ∆J2 − 5.092 · 10−3
· ∆x2 − 48.60 · 10−3
· ∆u2,
i = 3 : ∆x˙ 3 = 0.5831 · ∆J3 − 6.790 · 10−3
· ∆x3 − 36.44 · 10−3
· ∆u3,
i = 4 : ∆x˙ 4 = 0.5831 · ∆J4 − 3.395 · 10−3
· ∆x4 − 116.6 · 10−3
· ∆u4,
i = 5 : ∆x˙ 5 = 0.5831 · ∆J5 − 5.092 · 10−3
· ∆x5 − 77.75 · 10−3
· ∆u5,
i = 6 : ∆x˙ 6 = 0.5831 · ∆J6 − 6.790 · 10−3
· ∆x6 − 58.31 · 10−3
· ∆u6
for the linear submodels at the i-th operating point. We then design a PI
controller
∆ui = Ki (xref − x) + Kiωi
Z
t
0
(xref − x) dτ358 Chapter 5. Nonlinear Control of Nonlinear Systems
for each of the submodels with the respective controller coefficients
K1 = −1.3254, K4 = −0.8284, ω1 = 0.0248, ω4 = 0.0248,
K2 = −1.9532, K5 = −1.2207, ω2 = 0.0253, ω5 = 0.0253,
K3 = −2.5577, K6 = −1.5985, ω3 = 0.0257, ω6 = 0.0257.
The controllers are dimensioned in such a way that the control loops they
form with the respective linear submodel always have eigenvalues at
λ1 = −0.04 and λ2 = −0.06.
To take into account the limitation of the pump power, the PI controllers have
anti-windup elements.
Taking into account equation (5.13), we will use
β =



u =
aJ
bx
J


 6= ρ
as the scheduling vector, where β(t → ∞) = ρ holds. The control variables
ui = ueq,i + ∆ui
are averaged using the weighted arithmetic mean (5.4) to obtain the gain￾scheduling controller
u =
P
6
i=1
e
−|Σ(β−ρi
)|
2
ui
P
6
i=1
e−|Σ(β−ρi
)|
2
,
where we choose
Σ =

2 0
0 0.3

.
We then simulate the gain-scheduling control loop of the solar power plant
for the assumed daily solar radiation curve shown in Figure 5.8. The progres￾sions of the pump flow u and the temperature difference x = ∆T between
the oil’s outlet temperature and the inlet temperature are also shown in Fig￾ure 5.8. As can be seen, this does not violate the constraints of the pump flow
rate, 2 ls−1 ≤ u ≤ 10 ls−1
. The temperature difference x is also kept at the
desired value of x = 70◦C. However, the temperature difference falls below
70◦C if the solar radiation J is less than 0.2 kWm−2
, since in this case the
heating of the oil by the sun is too low to produce a temperature difference
of 70◦C with a minimum oil flow of 2 ls−1
.5.2. Input-Output Linearization 359
0
0
0
1
1
1
2
2
2
3
3
3
4
4
4
5
5
5
6
6
6
7
7
7
8
8
8
9
9
9
10
10
10
1
0.5
0
0
0
80
60
40
20
10
5
Time t in hours
x in
◦ C
u in l s
−1
J in kW m
−2 Fig. 5.8: Intensity J of the solar radiation, volume flow u = q of the pump, and
temperature difference x = ∆T of the oil between the inlet and outlet of the collector
field
5.2 Input-Output Linearization
5.2.1 Basic Concept and Nonlinear Controller Canonical Form
In this section, we will describe a method which enables us to design controllers
for nonlinear plants directly, not utilizing a linear approximation of the non￾linear plant. The basic concept is to design a nonlinear controller so that it
completely compensates for the nonlinearity of the plant, thereby creating a
linear control loop. This has given rise to the terms feedback linearization or
exact linearization for this approach.
The methods of feedback linearization can be divided into two categories.
On the one hand, these are procedures that bring about a linearization of
the system behavior between input and output. We call this input-output lin￾earization or, more precisely, input-output exact linearization[1]. In this case,
[1] The modifier exact emphasizes that we have a linearization of the system every￾where within a set of the input and output variables and not only an approximate
linearization around an operating point.360 Chapter 5. Nonlinear Control of Nonlinear Systems
the input-output behavior is linear, but the state-space model of the control
loop may still contain nonlinearities. On the other hand, these are procedures
that linearize the entire state-space model. These procedures are called input￾state linearization, state-space exact linearization or full-state linearization.
We will first deal with input-output linearization.
As plants, we will consider SISO systems of the form
x˙ = a(x) + b(x) · u,
y = c(x),
(5.15)
i. e. systems which are nonlinear in x but linear in u. Therefore, as previ￾ously mentioned, they are called input-linear systems, input-affine systems,
or control-affine systems. We will use the latter designation. The assumption
of a linearly acting control variable u is not a major restriction because most
of the technical systems are linear in the control variable u. The system order
is
n = dim(x).
For the time being, we will limit ourselves to SISO systems in order to explain
the basic concept behind the procedure as clearly as possible. Later we will
also look at MIMO systems.
If the controlled system is now represented as a particular form of equation
(5.15), namely the nonlinear controller canonical form





x˙ 1
.
.
.
x˙ n−1
x˙ n





=





x2
.
.
.
xn
α(x)





+





0
.
.
.
0
β(x)





u,
y = x1,
(5.16)
the design objective of an overall linear control loop as described above can
be easily achieved. As the control law, we will select
u = −
α(x) + Pn
i=1
ai−1xi
β(x)
+
V
β(x)
yref (5.17)
with the reference variable yref as well as the freely selectable coefficients
ai−1, i = 1, . . . , n, and V , yielding
x˙ =







0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
−a0 −a1 −a2 · · · −an−1







x +







0
0
.
.
.
0
V







yref,
y = x15.2. Input-Output Linearization 361
for the closed loop, which is a linear system in controller canonical form[2]
.
Its eigenvalues can be freely specified via the n coefficients ai
. Throughout
these calculations, we assume that β(x) 6= 0 holds. This is usually the case
in technical applications, since otherwise the control variable (5.17) would be
infinitely large and the plant would not be controllable.
Naturally, if we model real systems using their original state variables, only
few will be in the nonlinear controller canonical form (5.16). But if we succeed
in bijectively transforming a system (5.15) into this form, we can apply the
control law above and obtain a linear control loop. This is the basic concept
of feedback linearization.
We will now attempt to identify the bijective state coordinate transforma￾tion which transforms the system (5.15) into the nonlinear controller canonical
form (5.16). For this purpose, we will use the Lie derivative, which is defined
as the gradient of a scalar function h(x) multiplied by a vector field f(x), i. e.
Lf h(x) = ∂h(x)
∂x
f(x) = gradT
(h(x)) · f(x).
The Lie derivative can be illustrated geometrically: if it is positive, the vector
field f points in the direction of increasing values of the function h. If it
is negative, it is the other way round and the vector field f points towards
decreasing function values h(x). The latter, for example, is the case for a
Lyapunov function V for an asymptotically stable equilibrium point xeq = 0
of a system x˙ = f(x). We remember that in this case
V˙ (x) = ∂V (x)
∂x
x˙ =
∂V (x)
∂x
f(x) = Lf V (x) < 0
holds. However, the situations above do not play a role in the input-output
linearization, and we use the Lie derivative here to simplify the calculations
procedures.
For the transformation we are looking for, we need, in particular, the Lie
derivatives
Lac(x) = ∂c(x)
∂x
a(x) and Lbc(x) = ∂c(x)
∂x
b(x).
We start by calculating the time derivative of the output variable y and obtain
y˙ =
dc(x)
dt =
∂c(x)
∂x1
x˙ 1 + . . . +
∂c(x)
∂xn
x˙ n =
∂c(x)
∂x
x˙.
Substituting
[2] The term nonlinear controller canonical form indicates that the controller design
is simple if the plant is in this canonical form. This is similar to the case of linear
systems, where the term controller canonical form also indicates that the design
of a linear state controller by means of pole placement is especially simple.362 Chapter 5. Nonlinear Control of Nonlinear Systems
x˙ = a(x) + b(x) · u
into the equation above yields
y˙ =
∂c(x)
∂x
a(x) + ∂c(x)
∂x
b(x) · u
or, using Lie derivatives,
y˙ = Lac(x) + Lbc(x) · u.
For most technical systems, in the above equation
Lbc(x) = ∂c(x)
∂x
b(x) = 0 (5.18)
holds so that we obtain
y˙ = Lac(x).
Systems for which this does not apply will be addressed later.
Linear systems in the controller canonical form
x˙ = Ax + bu =







0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
−a0 −a1 −a2 · · · −an−1







x +







0
0
.
.
.
0
1







u,
y = c
T x =

b0 b1 · · · bm 0 · · · 0

x
with m < n−1 provide an example of systems consistent with equation (5.18).
For these systems, the equation
Lbc(x) = ∂c
T x
∂x
b(x) = c
T
b = 0
obviously holds.
Next, the second time derivative y¨ is to be determined. Starting with
y˙ = Lac(x), we obtain the expression
y¨ =
dLac(x)
dt =
∂Lac(x)
∂x
x˙ =
∂Lac(x)
∂x
a(x)
| {z }
LaLac(x)
+
∂Lac(x)
∂x
b(x)
| {z }
LbLac(x)
·u.
For the first term in the latter equation, the abbreviation
LaLac(x) = L
2
a
c(x)
is used, since the Lie derivative La has been applied twice in a row. For the
second term, the identity5.2. Input-Output Linearization 363
LbLa(x) = ∂Lac(x)
∂x
b(x) = 0
often applies once again, yielding
y¨ = L
2
ac(x).
When the higher derivatives are also calculated, we finally obtain
y = c(x),
y˙ = Lac(x),
y¨ = L
2
a
c(x),
.
.
.
y
(δ−1) = L
δ−1
a
c(x),
y
(δ) = L
δ
a
c(x) + LbL
δ−1
a
c(x) · u.
(5.19)
In this sequence of time derivatives, the equation
LbL
i
a
c(x) = ∂Li
a
c(x)
∂x
b(x) = 0
holds for all indices
i = 0, . . . , δ − 2.
The Lie derivative LbL
δ−1
a
c(x) is not equal to zero only for indices i ≥ δ − 1.
We refer to δ as the difference degree or mostly as the relative degree of the
system. The relative degree indicates the degree of the output variable y’s
derivative at which it directly depends on the control variable u for the first
time. For linear systems, the relative degree is equal to the difference
δ = n − m
between the denominator degree n and the numerator degree m of the transfer
function. For δ = n the numerator polynomial has the order zero, i. e. the
numerator is a constant.
We will first examine the case
δ = n,
i. e. the case in which the system order n = dim(x) is equal to the relative
degree δ. Then the new state coordinates
z =







z1
z2
z3
.
.
.
zn







=







y
y˙
y¨
.
.
.
y
(n−1)







=







c(x)
Lac(x)
L
2
a
c(x)
.
.
.
L
n−1
a
c(x)







= t(x) (5.20)364 Chapter 5. Nonlinear Control of Nonlinear Systems
can be introduced. If the function t(x) is continuously differentiable and the
inverse function t
−1
exists, i. e. if
t
−1
(t(x)) = x
holds, and furthermore if t
−1
is continuously differentiable, the mapping
t : IRn → IRn
is referred to as a diffeomorphism, as previously described in Section 3.3.1,
p. 260 et seq. It forms the required bijective coordinate transformation and
transforms the system
x˙ = a(x) + b(x) · u,
y = c(x)
by differentiating the transformation equation (5.20), i. e.
z˙ = t˙(x) = ∂t(x)
∂x
x˙ =







Lac(x)
L
2
ac(x)
.
.
.
L
n−1
a
c(x)
L
n
ac(x) + LbL
n−1
a c(x) · u







,
and replacing x by x = t
−1
(z) into the nonlinear controller canonical form





z˙1
.
.
.
z˙n−1
z˙n





=





z2
.
.
.
zn
L
n
a
c(x)





+





0
.
.
.
0
LbL
n−1
a
c(x)





u=





z2
.
.
.
zn
L
n
a
c(t
−1
(z))





+





0
.
.
.
0
LbL
n−1
a
c(t
−1
(z))





u,
y = z1.
(5.21)
We have thus achieved our goal and in the following section can use the system
in nonlinear controller canonical form for purposes of controller design.
5.2.2 Nonlinear Controller and Linear Control Loop
We already know the control law (5.17) for the nonlinear controller canonical
form which yields a linear control loop. Using
α(x) = L
n
a
c(x) and β(x) = LbL
n−1
a
c(x),
we can write it as
u(x, yref) = −r(x) + v(x) · yref (5.22)5.2. Input-Output Linearization 365
yref u x y
v(x) x˙ = a(x) + b(x) · u c(x)
r(x)
Fig. 5.9: Control loop with linear dynamics between input yref and output y
with
r(x) = L
n
a
c(x) + k
T
z
LbL
n−1
a c(x)
, k
T = [a0 a1 · · · an−1]
as a controller and
v(x) = V
LbL
n−1
a c(x)
as a pre-filter. Figure 5.9 illustrates the corresponding block diagram. The
variable yref acts as the reference variable of the control loop. Obviously the
inequality LbL
n−1
a
c(x) 6= 0 must be fulfilled for all states x in which the
control is to be executed. If LbL
n−1
a c(x) = 0, according to equation (5.21)
the control variable would have no effect on the controlled system. The gen￾eral controller given by equation (5.22) is equal to the special control law
(5.17) from our initial calculation in which the controlled system is a priori
in nonlinear controller canonical form.
We can also interpret the control law
u = −
L
n
a
c(x) + k
T
z
LbL
n−1
a c(x)
+
V
LbL
n−1
a c(x)
yref (5.23)
as a state-dependent input variable transformation if we replace the input
variable u with the new input variable yref.
Inserted in the nonlinear controller canonical form (5.21), the control law
(5.22) results in a linear control loop in controller canonical form







z˙1
z˙2
.
.
.
z˙n−1
z˙n







=







0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
−a0 −a1 −a2 · · · −an−1














z1
z2
.
.
.
zn−1
zn







+







0
0
.
.
.
0
V







yref,
y = z1.
(5.24)366 Chapter 5. Nonlinear Control of Nonlinear Systems
We are free to choose the n coefficients ai
in the vector k so that we can
impose any desired eigenvalue configuration and thus any linear dynamics on
the control loop. The value V is also freely selectable. In practice, however,
the limitation of the control variable, |u| ≤ umax, limits our choice of the
parameters ai and V .
Using the transformations (5.20) and (5.23), we were successful in trans￾forming the nonlinear system representation into a linear one. For the differ￾ential equation of the output variable y, we thus obtain
y
(n) + an−1y
(n−1) + . . . + a1y˙ + a0y = V · yref
from equation (5.24). As already indicated, the term input-output exact lin￾earization and its shortened version input-output linearization have their ori￾gin in the linear behavior between the input variable yref and the output
variable y.
To make the control law (5.23) independent of z, we insert
z = t(x) =





c(x)
Lac(x)
.
.
.
L
n−1
a
c(x)





into equation (5.23) and arrive at the nonlinear Ackermann formula
u = −
L
n
a
c(x) + an−1L
n−1
a
c(x) + . . . + a1Lac(x) + a0c(x)
LbL
n−1
a c(x)
+
V · yref
LbL
n−1
a c(x)
as the control law, which only depends on the original state x.
We can summarize all these results in the following theorem.
Theorem 59 (Input-Output Linearization at the Maximum Rela￾tive Degree). Let the plant
x˙ = a(x) + b(x) · u, x ∈ Dx,def ⊆ IRn
, u ∈ IR,
y = c(x),
with the relative degree δ = n and the control law
u = −r(x) + v(x) · yref
be given. If the controller has the form
r(x) = L
n
a
c(x) + an−1L
n−1
a
c(x) + . . . + a1Lac(x) + a0c(x)
LbL
n−1
a c(x)
,5.2. Input-Output Linearization 367
and the pre-filter is given by
v(x) = V
LbL
n−1
a c(x)
, V ∈ IR,
with
LbL
n−1
a
c(x) 6= 0,
then the control loop has linear dynamic behavior described by
y
(n) + an−1y
(n−1) + . . . + a1y˙ + a0y = V · yref.
We refer to the procedure in Theorem 59 as input-output linearization at
the maximum relative degree, since it is tailored to the highest possible value
δ = n. The case of δ < n is discussed in Section 5.2.4.
The following fact should be emphasized again: for input-output lineariza￾tion to be possible on the system’s domain of definition Dx,def, the condition
LbL
n−1
a
c(x) 6= 0
must hold for all x ∈ Dx,def. If this is the case, we designate the relative
degree as well-defined. In the opposite case it is termed not well-defined and
input-output linearization is only possible on a proper subset Dx ⊂ Dx,def.
Depending on whether the relative degree is well-defined or not, the system
is globally or locally linearizable.
From the above it follows that a control-affine system with a relative degree
δ = n is always controllable (as long as β(x) = LbL
n−1
a
c(x) 6= 0 holds),
since it can be bijectively transformed into the nonlinear controller canonical
form (5.21). The controllability of such systems has already been addressed in
Theorem 45 on p. 218. For controllable and observable plants, it further holds
that the inequality δ ≤ n is fulfilled. However, in the case of uncontrollable
and unobservable systems, LbL
n−1
a
c(x) = 0 may be true and thus the input
signal u has no influence on the output signal y. If we calculate δ in this case,
we obtain δ = ∞. We can only refer to δ as a relative degree if δ ≤ n holds.
Literature on input-output linearization can also be found in [203, 232,
308, 338, 402].
5.2.3 Example: Magnetic Bearing
We will now describe an active magnetic bearing [316, 454] as used in electric
generators and motors, pumps, and turbines. It enables frictionless bearing of
the machine shaft and thus saves energy. The schematic diagram of a magnetic
bearing and one of its pairs of electromagnets are shown in Figure 5.10. The
electromagnets hold the shaft in position h = 0.
The magnetic fluxes Φ1 and Φ2, generated by the currents i1 and i2 in the
electromagnets, generate the forces F1 and F2 which act on the shaft. These
forces are adjusted in such a way that the bearing is mounted without contact.368 Chapter 5. Nonlinear Control of Nonlinear Systems
Electromagnet
R1
F1
R2
F2
h
i1
u1
i2
u2
Engine shaft
Fig. 5.10: Schematic diagram of an active magnetic bearing with one degree of free￾dom in the direction h. Further degrees of freedom not addressed here are produced
by additional electromagnets.
To save electrical energy, the electromagnets can be alternately switched on
and off. This operating mode is called a zero-bias or a low-bias operation.
The forces acting on the shaft are given by
Fi =
1
µ0A
Φ
2
i
, i = 1, 2. (5.25)
They variably depend on the magnetic flux Φi of the respective electromagnets
i. Here, µ0 is the magnetic field constant and A the surface of one pole of the
electromagnets. The magnetic flux
Φi = Φ0 + φi
, i = 1, 2, (5.26)
consists of two components, where Φ0 ≪ φi holds. The flux component Φ0
is constant, i. e. it makes up the bias. The flux components φi
include the
components which change over time. Neglecting the coil resistances R1 and
R2, we obtain for the relationship between the voltage ui of the electromagnet
i and the flux Φi5.2. Input-Output Linearization 369
Φ˙
i = φ˙
i =
ui
N
, i = 1, 2. (5.27)
In this case, ui
is the voltage applied to the i-th electromagnet and N is the
number of the coil’s turns. We now define
φ = φ1 − φ2 (5.28)
and switch the voltages of the two electromagnets on and off according to
u1 =
(
v for φ < 0,
0 for φ ≥ 0
(5.29)
and
u2 =
(
0 for φ < 0,
−v for φ ≥ 0.
(5.30)
The voltage v is variable and makes up the control signal
u = φ˙ =
v
N
of the controller which is yet to be designed.
With equation (5.25), the total force acting on the shaft can be stated as
Ftot = F1 − F2 =
1
µ0A
(Φ
2
1 − Φ
2
2
).
For the difference Φ
2
1 − Φ
2
2
arising from equations (5.26), (5.27), and (5.28),
and taking into account switching strategies (5.29) and (5.30), the relation
Φ
2
1 − Φ
2
2 = 2Φ¯
0φ + φ|φ|
with
Φ¯
0 = Φ0 + min {φ1(0), φ2(0)}
follows after an intermediate calculation [454].
Thus, for the acceleration ¨h of the driving shaft with mass m, we obtain
h¨ =
1
µ0Am(2Φ¯
0φ + φ|φ|).
We define x1 = h, x2 = h˙
, and x3 = φ as state variables of the system, and
the voltage u = v/N as the control variable. We then obtain the control-affine
state-space model
x˙ =


x2
α1x3 + α2x3|x3|
0


| {z }
a(x)
+


0
0
1


|{z}
b(x)
u370 Chapter 5. Nonlinear Control of Nonlinear Systems
with x = [x1 x2 x3]
T
as the state vector and u = φ˙ as the control variable.
The parameters are given by
α1 =
2Φ¯
0
µ0Am and α2 =
1
µ0Am.
The output variable y of the magnetic bearing is the displacement h, i. e.
y = c(x) = x1.
We can now design a controller using input-output linearization according
to Theorem 59. We will take into account the fact that the function
f(x) = x|x|
is differentiable and that its derivative is
f
′
(x) = 2|x|.
We can now proceed in the following steps:
Step 1: The Lie derivatives L
i
a
c(x) are
i = 0 : L
0
a
c(x) = c(x) = x1,
i = 1 : Lac(x) = ∂c(x)
∂x
a(x) = x2,
i = 2 : L
2
a
c(x) = ∂Lac(x)
∂x
a(x) = α1x3 + α2x3|x3|,
i = 3 : L
3
a
c(x) = ∂L2
a
c(x)
∂x
a(x) = 0.
Step 2: With the results from Step 1, we are able to calculate the terms
i = 0 : Lbc(x) = ∂x1
∂x
b(x) = 0,
i = 1 : LbLac(x) = ∂x2
∂x
b(x) = 0,
i = 2 : LbL
2
a
c(x) = ∂(α1x3 + α2x3|x3|)
∂x
b(x) = α1 + 2α2|x3|.
Thus for i = 2 the term LbL
i
a
c(x) is unequal to zero and δ = 3 = n
follows.
Step 3: It holds that
LbL
2
ac(x) = α1 + 2α2|x3| 6= 0
for all x ∈ IR3
, because α1 > 0, α2 > 0, and |x3| ≥ 0.5.2. Input-Output Linearization 371
Step 4: Since the relative degree is δ = 3, the transfer function G(s) of the
control loop also takes an order of three, i. e.
G(s) = V
s
3 + a2s
2 + a1s + a0
.
We now select three real poles s1 = s2 = s3 = −λ with λ > 0 and,
choosing V = λ
3
, we obtain the transfer function
G(s) = λ
3
s
3 + 3λs2 + 3λ2s + λ3
.
Step 5: Including the results of Steps 1, 2, and 4 yields the controller
r(x) = L
3
ac(x) + a2L
2
ac(x) + a1Lac(x) + a0c(x)
LbL2
a
c(x)
=
3λ(α1x3 + α2x3|x3|) + 3λ
2x2 + λ
3x1
α1 + 2α2|x3|
.
Step 6: For the pre-filter, we obtain
v(x) = V
LbL2
a
c(x)
=
λ
3
α1 + 2α2|x3|
.
The control loop thus calculated is shown in Figure 5.11. The dynamics of
the control loop, especially the control variable amplitude, and the required
actuator energy can be influenced by means of the triple pole at −λ, which
has not yet been specified. This makes it possible to keep the control value
u within prescribed limits, such as −1 ≤ u ≤ 1. However, ensuring that the
limits of the control variable are not exceeded is a difficult task in case of
exact linearization [165, 223, 302, 352].
v(x)=
λ
3
α1+2α2|x3|
x˙ =


x2
α1x3 + α2x3|x3|
0

+


0
0
1

u
y = x1
r(x)=
3λ(α1x3+α2x3|x3|)+3λ
2
x2+λ
3
x1
α1 + 2α2|x3|
yref u y
r(x) x
Fig. 5.11: Control of the active magnetic bearing using input-output linearization372 Chapter 5. Nonlinear Control of Nonlinear Systems
5.2.4 Plants with Internal Dynamics
Below we will address the case in which
δ < n,
i. e. the case in which the relative degree δ is lower than the system order
n. We recall that the sequence of derivatives (5.19) without dependence on
the control signal u stops at y
(δ−1). In this case as well, a nonlinear bijective
transformation rule which is continuously differentiable, a diffeomorphism
z = t(x), (5.31)
can be found to arrive at a system representation which is favorable to con￾troller design. Based on this, we can again design a control loop with linear
input-output behavior.
Since δ < n holds, only the first δ components t1, . . . , tδ of the diffeomor￾phism (5.31) can be used for the new state variables z1, . . . , zδ in the same
form as in the case in which δ = n holds. It holds that
z =












y
y˙
.
.
.
y
(δ−1)
zδ+1
.
.
.
zn












=












z1
z2
.
.
.
zδ
zδ+1
.
.
.
zn












= t(x) =












c(x)
Lac(x)
.
.
.
L
δ−1
a
c(x)
tδ+1(x)
.
.
.
tn(x)












. (5.32)
A comparison with the case δ = n in equation (5.20) demonstrates the iden￾tical procedure for the first δ elements of z. The functions tδ+1, . . . , tn can
be chosen arbitrarily, as long as it is certain that t is a diffeomorphism. For
this reason, the following requirements must be met: that t is continuously
differentiable, that the inverse function
x = t
−1
(z)
exists, and that this inverse function is also continuously differentiable. From
Theorem 54 on p. 262, we know that all the above requirements are fulfilled
and that t is a diffeomorphism if the Jacobian matrix ∂t(x)/∂x is regular.
Then, if we transform the original system description
x˙ = a(x) + b(x) · u,
y = c(x)
using the diffeomorphism (5.32), the result is5.2. Input-Output Linearization 373
z˙ =














z˙1
z˙2
.
.
.
z˙δ−1
z˙δ
z˙δ+1
.
.
.
z˙n














=
d
dt














c(x)
Lac(x)
.
.
.
L
δ−2
a
c(x)
L
δ−1
a c(x)
tδ+1(x)
.
.
.
tn(x)














=














Lac(x)
L
2
ac(x)
.
.
.
L
δ−1
a
c(x)
L
δ
ac(x) + LbL
δ−1
a c(x) · u
˙tδ+1(x)
.
.
.
˙tn(x)














.
Utilizing equation (5.32) once again leads to
z˙ =














z˙1
z˙2
.
.
.
z˙δ−1
z˙δ
z˙δ+1
.
.
.
z˙n














=














z2
z3
.
.
.
zδ
L
δ
ac(x) + LbL
δ−1
a c(x) · u
˙tδ+1(x)
.
.
.
˙tn(x)














, (5.33)
while
y = z1
still holds. In contrast to the case where
δ = n,
only the first δ rows of equation (5.33) are in controller canonical form.
Let us take a closer look at the functions tδ+1, . . . , tn. Using the transfor￾mation
x = t
−1
(z)
for i = δ + 1, . . . , n, the derivatives
˙ti(x) = ∂ti(x)
∂x
x˙ =
∂ti(x)
∂x
￾
a(x) + b(x) · u

= Lati(x) + Lbti(x) · u (5.34)
= ˆqi(x, u) = ˆqi(t
−1
(z), u) = qi(z, u)
can be determined. If the functions ti have been appropriately chosen or have
been calculated using the method specified in [388], so that
Lbti(x) = ∂ti(x)
∂x
b(x) = 0 (5.35374 Chapter 5. Nonlinear Control of Nonlinear Systems
holds, equation (5.34) can be simplified. The dependency on u is now dropped,
and
˙ti(x) = Lati(x) = ˆqi(x) = qi(z) (5.36)
applies. The elimination of the dependence on u simplifies the representation
of the transformed system greatly, since the internal dynamics now no longer
depend on u.
Equation (5.35) is a partial differential equation whose solution ti(x) usu￾ally exists, but is difficult to determine in many cases. In the following, and
for reasons of simplification as described above, we will assume that ti
fulfills
equation (5.35) for all i = δ + 1, . . . , n, and insert the functions ˙ti
from equa￾tion (5.36) into equation (5.33). We then obtain the so-called Byrnes-Isidori
canonical form












z˙1
.
.
.
z˙δ−1
z˙δ
z˙δ+1
.
.
.
z˙n












=












z2
.
.
.
zδ
α(z)
qδ+1(z)
.
.
.
qn(z)












+












0
.
.
.
0
β(z)
0
.
.
.
0












u,
y = z1
(5.37)
external dynamics
internal dynamics
output variable
as the system description, where
α(z) = L
δ
a
c(t
−1
(z)) and β(z) = LbL
δ−1
a
c(t
−1
(z))
hold.
As shown in equation (5.37), the system dynamics can be divided into an
external and an internal part. The external dynamics of the system, describing
the changes in the states z1, . . . , zδ, can be linearized by transforming the input
variable u into the new input variable yref, as in the case in which δ = n. Based
on equation (5.33) or equation (5.37), we can determine this transformation
as
u = −
L
δ
ac(x) + k
T
z
LbL
δ−1
a c(x)
+
V
LbL
δ−1
a c(x)
· yref (5.38)
with
k
T = [a0 · · · aδ−1 0 · · · 0].
If this is inserted into equation (5.37), the dynamic equations of the trans￾formed system are obtained as follows:5.2. Input-Output Linearization 375







z˙1
z˙2
.
.
.
z˙δ−1
z˙δ







=







0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
−a0 −a1 −a2 · · · −aδ−1














z1
z2
.
.
.
zδ−1
zδ







+







0
0
.
.
.
0
V







yref,



external
dynamics
(5.39)



z˙δ+1
.
.
.
z˙n


=



qδ+1(z)
.
.
.
qn(z)


,



internal
dynamics (5.40)
y=z1.
After applying the transformation (5.38) of the input variable, the external
dynamics are independent of the state variables zδ+1, . . . , zn of the internal
dynamics. In particular, the state variables of the internal dynamics have no
influence on the output variable y. As a result, the internal dynamics (5.40)
are not observable. The term internal symbolizes this property. This is why
the control loop as a whole loses its observability due to the linearization. In
practice, however, the latter is of little importance.
Obviously, the procedure above does not linearize the internal dynam￾ics. Fortunately, as already stated, it does not influence the output variable
y = z1 or the controlled external dynamics. The internal dynamics possess
the input variables z1, . . . , zδ and no output variable. We can interpret the in￾ternal dynamics as an independent system with the associated state variables
zδ+1, . . . , zn. This is illustrated by Figure 5.12.
Provided that
LbL
δ−1
a
c(t
−1
(z)) 6= 0
holds, the external dynamics (5.37) are always controllable because they are
present in nonlinear controller canonical form. We know this fact from The￾orem 45 on p. 218. In this case, again, we can interpret the transformation
equation (5.38) as a control law which has a state controller in the first sum￾mand and a pre-filter for the reference variable yref in the second. To make
equation (5.38) independent of the artificial state vector z, we can formulate
k
T
z = aδ−1L
δ−1
a c(x) + . . . + a1Lac(x) + a0c(x)
using
z1 = c(x), z2 = Lac(x), . . . , zδ = L
δ−1
a c(x),
which follows from equation (5.32). Thus we obtain
u = −
L
δ
ac(x) + an−1L
δ−1
a c(x) + . . . + a1Lac(x) + a0c(x)
LbL
δ−1
a c(x)
+
V · yref
LbL
δ−1
a c(x)376 Chapter 5. Nonlinear Control of Nonlinear Systems
as the control law.
The controlled external dynamics, meaning the control loop (5.39) itself,
have the same structure as in the case in which δ = n, i. e. as shown in Figure
5.9. Similar to the case in which δ = n, the output variable y is given by a
linear differential equation
y
(δ) + aδ−1y
(δ−1) + . . . + a1y˙ + a0y = V · yref. (5.41)
However, it only has the relative degree δ < n. We can also see from equation
(5.41) that the lower the relative degree δ, the more directly the input variable
u affects the output variable y.
At this point, the additional significance of the relative degree δ to the
behavior of the control loop becomes clear: it determines the order of the
linear differential equation of the control loop.
Internal dynamics do not affect the dynamics of the linear control loop, but
they do affect the dynamics of the overall system. It is obvious that unstable
internal dynamics[3] lead to an unstable overall system. Therefore, we need to
analyze the internal dynamics’ stability, which is a prerequisite if we are to
obtain a stable control loop.
Control loop with controlled external dynamics
Internal dynamics
yref y

z1 · · · zδ
T





z˙1
z˙2
.
.
.
z˙δ





=





0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
−a0 −a1 −a2 · · · −aδ−1










z1
z2
.
.
.
zδ





+





0
0
.
.
.
V





yref
y = z1



z˙δ+1
.
.
.
z˙n


 =



qδ+1(z1, . . . , zn)
.
.
.
qn(z1, . . . , zn)



Fig. 5.12: Block diagram of the control loop with controlled external dynamics and
unobservable internal dynamics
[3] The internal dynamics may have several equilibria, not merely a single one. In
this case the equilibria which are relevant to practical operation must be stable.
In this context, see also Definition 8 on p. 18, and the explanations of stability
and internal dynamics in Section 5.2.10, p. 398, for a deeper understanding.5.2. Input-Output Linearization 377
Again, we can summarize these results in a theorem.
Theorem 60 (Input-Output Linearization with a Reduced Relative
Degree). Let the plant
x˙ = a(x) + b(x) · u, x ∈ Dx,def ⊆ IRn
, u ∈ IR,
y = c(x)
with the relative degree δ < n and the control law
u = −r(x) + v(x) · yref
be given. If the controller has the form
r(x) = L
δ
a
c(x) + aδ−1L
δ−1
a
c(x) + . . . + a1Lac(x) + a0c(x)
LbL
δ−1
a c(x)
and the pre-filter is given by
v(x) = V
LbL
δ−1
a c(x)
, V ∈ IR,
with
LbL
δ−1
a
c(x) 6= 0,
then the control loop has linear dynamic behavior described by
y
(δ) + aδ−1y
(δ−1) + . . . + a1y˙ + a0y = V · yref.
In addition, the control loop has the internal dynamics



z˙δ+1
.
.
.
z˙n


 =



qδ+1(z)
.
.
.
qn(z)



with
z = t(x).
The internal dynamics do not influence the output y. If both the internal and
the linear external dynamics of the control loop have a globally asymptotically
stable equilibrium point at the origin and the internal dynamics with its input
variable vector [z1 · · · zδ] are input-to-state stable, then the equilibrium point
zeq = 0 of the whole system is globally asymptotically stable.
Theorem 59 and Theorem 60 are very similar and differ only in that inter￾nal dynamics are included in Theorem 60. We will discuss the stability of an
input-output linearized system in more detail in Section 5.2.10. It is important
to note that although the control loop has unobservable internal dynamics,378 Chapter 5. Nonlinear Control of Nonlinear Systems
the controlled system is normally observable. Therefore, the state vector x
required for the control can be estimated using an observer.
In the linear transfer behavior of the control loop
G(s) = V
s
δ + aδ−1s
δ−1 + . . . + a1s + a0
the coefficients
a0, a1, . . . , aδ−1
and the factor V of the pre-filter v(x) are arbitrarily selectable for both cases,
δ = n as well as δ < n. The dynamics of the transfer function G(s) can also be
freely defined using the parameters a0, . . . , aδ−1 and V . In practice, however,
the limitation of the control variable given by the inequality
|u| ≤ umax
again limits the available flexibility.
5.2.5 Design Procedure
The following general procedure, which applies to δ = n as well as δ < n,
describes how to calculate the controller and the pre-filter. We have already
followed a similar procedure for the special case of the magnetic bearing in
Section 5.2.3.
Step 1: Determine the Lie derivatives
L
i
a
c(x), i = 0, . . . , n = dim(x).
Step 2: Determine the Lie derivatives
LbL
i
a
c(x) = ∂Li
a
c(x)
∂x
b(x) (5.42)
with i = 0, 1, 2, . . . in ascending order. The lowest index i for which
the above term (5.42) is not equal to zero yields the relative degree
δ = i + 1.
The corresponding term in equation (5.42) reads as
LbL
δ−1
a c(x) = ∂Lδ−1
a
c(x)
∂x
b(x).
Step 3: For all occurring x, the inequality
LbL
δ−1
a c(x) 6= 0
must hold, i. e. the relative degree δ must be well-defined.5.2. Input-Output Linearization 379
Step 4: Select the linear dynamics of the control loop
G(s) = V
s
δ + aδ−1s
δ−1 + . . . + a1s + a0
by setting the freely selectable parameters V and a0, a1, . . . , aδ−1.
Step 5: Utilize the results from Steps 1, 2, and 4 to determine the controller
r(x) = L
δ
ac(x) + aδ−1L
δ−1
a c(x) + . . . + a0c(x)
LbL
δ−1
a c(x)
.
Step 6: The pre-filter is determined according to
v(x) = V
LbL
δ−1
a c(x)
.
Step 7: For δ < n, determine the internal dynamics and check them for
stability.
Checking the internal dynamics for stability can be complicated, since this
dynamic component is principally nonlinear.
5.2.6 Example: Lunar Module
In the following, we will look at an example with internal dynamics: a lander
[318] such as the lunar module Eagle of the Apollo 11 mission [22, 343], the
Phoenix Mars probe, and the lunar lander Altair from NASA’s Constellation
Program. The landing module of a potentially manned Mars mission might
have a similar design. Figure 5.13 shows the lunar module Eagle, which we
will consider in the following.
The main purpose of propulsion control for a landing module of the type
above is to ensure a soft landing. For this purpose, the engine’s vertically
acting thrust force is used to decelerate the gravity-induced approach toward
the lunar surface.
The following equation
mh¨ = −vm˙ − mg (5.43)
describes the balance of forces, where m is the mass of the lunar module
including the fuel supply, h denotes the altitude, v is the speed of the gas
ejected from the engine, and g = 1.62 m s−2
is the gravitational acceleration
of the moon. The term −vm˙ represents the thrust of the engine, where m <˙ 0
applies because the fuel mass decreases during landing. The Eagle had a thrust
of 4.56 . . . 45.04 kN during the landing phase. The engine’s exit velocity was
v = 3050 m s−1
.
It should also be noted that the fuel mass makes up a substantial part of
the total mass m. The total weight of the Eagle was m = 15264 kg, of which380 Chapter 5. Nonlinear Control of Nonlinear Systems
g
v
h
Fig. 5.13: Lunar module Eagle of the Apollo 11 mission
8165 kg of fuel was available for the landing phase alone. The decrease in the
mass m due to the consumption of the fuel supply is thus an essential aspect
of the equation of the shuttle’s motion (5.43).
Utilizing the state variables
x1 = h, x2 = h, x ˙
3 = m,
the control variable
u = −m, ˙5.2. Input-Output Linearization 381
and equation (5.43), we arrive at the model





x˙ 1
x˙ 2
x˙ 3





=





x2
−g
0





| {z }
a(x)
+





0
v
x3
−1





| {z }
b(x)
u, (5.44)
y = c(x) = x1
for the Eagle’s landing phase. Note that u = −m˙ is always positive.
Consistent with the design procedure from Section 5.2.5, we determine
L
0
a
c(x) = c(x) = x1,
L
1
ac(x) = ∂c(x)
∂x
a(x) = x2,
L
2
a
c(x) = ∂Lac(x)
∂x
a(x) = −g.
Further, we can compute
Lbc(x) = ∂c(x)
∂x
b(x) = 0,
LbLac(x) = ∂Lac(x)
∂x
b(x) = v
x3
.
Since LbLac(x) 6= 0, the system has the relative degree δ = 2. Because the
system order is n = 3 > δ, the system has internal dynamics. The mass
x3 = m of the shuttle is always positive. Thus,
LbLac(x) = v
x3
> 0
holds and the relative degree is well-defined. We can therefore design a con￾troller using input-output linearization. We must also ensure the stability of
the internal dynamics to guarantee the stability of the control system as a
whole.
The diffeomorphism can be written as
z = t(x) =




c(x)
Lac(x)
t3(x)




=




x1
x2
t3(x)




, (5.45)
where the function t3(x) is yet to be selected. This is done in such a way that
the partial differential equation (5.35) is fulfilled, i. e. in this case382 Chapter 5. Nonlinear Control of Nonlinear Systems
Lbt3(x) = ∂t3(x)
∂x
b(x) = 0.
We remember: if we do it this way, the internal dynamics do not depend on
the input variable u. Specifically, we obtain
Lbt3(x) = ∂t3(x)
∂x2
·
v
x3
−
∂t3(x)
∂x3
= 0. (5.46)
This partial differential equation has the solution
t3(x) = x3 · e
x2/v
,
which can easily be checked by inserting it into equation (5.46). So the diffeo￾morphism (5.45) is of the form
z = t(x) =




x1
x2
x3e
x2/v




or x = t
−1
(z) =




z1
z2
z3e
−z2/v




. (5.47)
Using equation (5.44) and equation (5.47), we obtain the system descrip￾tion with external and internal dynamics as





z˙1
z˙δ=2
z˙3





=





Lac(x)
L
2
ac(x) + LbLac(x)u
˙t3(x2, x3)





=





x2
−g +
v
x3
u
−v
−1
gx3e
x2/v





=





z2
−g +
v
x3
u
−v
−1
gz3





. (5.48)
external dynamics
internal dynamics
We can now design the control law according to Theorem 60 using
u = −
L
2
ac(x) + a1Lac(x) + a0c(x)
LbLac(x)
= −
−g + a1x2 + a0x1
v
x3 = −
−g + a1z2 + a0z1
v
x3. (5.49)
A reference variable yref is not required here, since the control adjusts the
state variables x1 = z1 and x2 = z2 to the equilibrium point
"
x1,eq
x2,eq #
= 0.
Therefore, the pre-filter v(x) is not necessary.5.2. Input-Output Linearization 383
With equations (5.47), (5.48), and (5.49) we can now represent the entire
control system by




z˙1
z˙2
z˙3




=




z2
−a1z2 − a0z1
−v
−1
gz3




, (5.50)
y = z1.
As can be seen from equation (5.50), the internal and external dynamics are
decoupled. The internal dynamics especially cause no problems because they
are stable. It is also apparent that the internal dynamics are not observable,
since z3 does not affect y = z1.
We will now transform the above controlled system description back to the
original coordinates x, and, with equation (5.50) and diffeomorphism (5.47),
we thus arrive at the representation




x˙ 1
x˙ 2
x˙ 3




=




x2
−a1x2 − a0x1
v
−1
x3(a1x2 + a0x1 − g)




of the control loop. We can also obtain this result by inserting the control law
(5.49) into the system description (5.44). The above equation shows that the
internal dynamics do not influence the controlled external dynamics. However,
we need the internal state variable x3 in the control law (5.49). This does not
pose a problem, since x3 = m holds and m is measurable.
As a concrete numerical example, we will continue with the lunar module
Eagle from the Apollo 11 mission and simulate the approach and landing phase
on the moon using the designed controller (5.49). We select the coefficients
a0 = 0.02 and a1 = 1.1, so that the eigenvalues of the controlled external
dynamics are
λ1 = −0.0185 and λ2 = −1.0815.
So we can describe the controlled lunar module by




x˙ 1
x˙ 2
x˙ 3




=




x2
−1.1x2 − 0.02x1
x3(3.61 · 10−4
x2 + 6.56 · 10−6
x1 − 5.31 · 10−4
)




with g = 1.62 m s−2
and v = 3050 m s−1
.
We begin the approach and landing phase [22] at an altitude of x1(0) =
2450 m and a sinking velocity of x2(0) = −45 m s−1
. The fuel supply for the
landing is still 1633 kg at this point. The total mass is x3(0) = 8732 kg. We
assume that the movement of the lunar module in this phase is approximately384 Chapter 5. Nonlinear Control of Nonlinear Systems
Time t in s
x1 in m
x2 in m s
−1
x3 in t
u in kg s
−1
3000
2000
1000
5
0
0
-20
-40
-60
4
7
7
8
9
6
6
3
0
0
0
0
50
50
50
50
100
100
100
100
150
150
150
150
200
200
200
200
250
250
250
250
300
300
300
300
Weight of the lunar module without fuel for landing
Fig. 5.14: Height x1, velocity x2, total mass x3, and control variable u
vertical to the lunar surface[4]. The time courses for the height x1, the speed
x2, and the remaining total mass x3 are shown in Figure 5.14.
5.2.7 Input-Output Linearization of General SISO Systems
We can extend input-output linearization, discussed in previous sections for
control-affine systems, to general nonlinear SISO systems
x˙ = f(x, u),
y = g(x).
(5.51)
Similar to the control-affine case, we will utilize the Lie derivative of g with
respect to f, i. e.
Lf g(x) = ∂g(x)
∂x
f(x, u)
[4] In reality, the Eagle lunar module’s trajectory was not completely vertical during
the final phase of the landing due to the orbit of curved descent around a part of
the moon.5.2. Input-Output Linearization 385
and the multiple Lie derivative
L
i
f
g(x) =
∂Li−1
f
g(x)
∂x
f(x, u).
We can now calculate
y˙ =
∂g(x)
∂x
x˙ =
∂g(x)
∂x
f(x, u) = Lf g(x) (5.52)
and
y¨ =
∂Lf g(x)
∂x
f(x, u) + ∂Lf g(x)
∂u u.˙
If
∂Lf g(x)
∂u u˙ = 0 (5.53)
holds, obviously Lf g(x) and thus y˙ in equation (5.52) do not depend on u.
We assume that equation (5.53) is valid so that we obtain
y¨ = L
2
f
g(x)
next. We can now compute
˙˙˙y =
∂L2
f
g(x)
∂x
f(x, u) +
∂L2
f
g(x)
∂u u.˙
If
∂L2
f
g(x)
∂u u˙ = 0
holds, which is similar to the case described above, the double Lie derivative
L
2
f
g(x) is not dependent on u either. We then continue calculating further
derivatives y
(i)
as long as
∂Li−1
f
g(x)
∂u = 0
holds, until we arrive at two equations of the form
y
(δ) =
∂Lδ−1
f
g(x)
∂x
f(x, u)
| {z }
L
δ
f
g(x)
+
∂Lδ−1
f
g(x)
∂u | {z }
0
u˙ = L
δ
f
g(x) = ϕ(x, u),
y
(δ+1) =
∂Lδ
f
g(x)
∂x
f(x, u) +
∂Lδ
f
g(x)
∂u | {z }
6= 0
u.˙
(5.54)386 Chapter 5. Nonlinear Control of Nonlinear Systems
Since now
∂Lδ
f
g(x)
∂u 6= 0 (5.55)
holds in equation (5.54), the function L
δ
f
g(x) depends on u. Since all deriva￾tives y
(i) = L
i
f
g(x) with i = 0, . . . , δ − 1 are independent of u, we obtain
y = g(x),
y˙ = Lf g(x),
.
.
.
y
(δ−1) = L
δ−1
f
g(x),
y
(δ) = L
δ
f
g(x) = ϕ(x, u).
(5.56)
As in the case of control-affine systems, the index δ is referred to as the
relative degree. Note that the δth Lie derivative L
δ
f
g(x) is used in equation
(5.55). Compared to this, for a control-affine system
x˙ = a(x) + b(x) · u, y = c(x),
the Lie derivative LbL
δ−1
a
c(x) is used to determine the relative degree δ.
In equation (5.56), we can replace the control variable u with a new control
variable
v = ϕ(x, u) (5.57)
so that we obtain the relation
y
(δ) = v.
This procedure assumes that the implicit equation (5.57) is uniquely solvable
for u; more precisely, that the function ϕ is bijective, meaning the inverse
function u = ϕ
−1
(x, v) exists. Here it will often be the case that the inverse
function ϕ
−1
can only be determined numerically.
If we select the control law with the new control variable v such that
v = −aδ−1y
(δ−1) − . . . − a1y˙ − a0y + V · yref (5.58)
holds, where yref is the reference variable of the controlled system, we obtain
the linear control-loop dynamics according to
y
(δ) + aδ−1y
(δ−1) + . . . + a1y˙ + a0y = V · yref.
Again, the coefficients ai are freely selectable. The input-output exact lin￾earization of general nonlinear systems has thus been achieved.5.2. Input-Output Linearization 387
Let us now transform system (5.51) into the so-called generalized Byrnes￾Isidori canonical form using a suitable diffeomorphism t(x). As in the control￾affine case, we choose
z = t(x) =



















g(x)
Lf g(x)
L
2
f
g(x)
.
.
.
L
δ−1
f
g(x)
tδ+1(x)
.
.
.
tn(x)



















.
By deriving z = t(x), we then calculate
z˙ =




















Lf g(x)
L
2
f
g(x)
.
.
.
L
δ−1
f
g(x)
L
δ
f
g(x)
˙tδ+1(x)
.
.
.
˙tn(x)




















=




















z2
z3
.
.
.
zδ
ϕˆ(z, u)
Lf tδ+1(x)
.
.
.
Lf tn(x)




















,



external
dynamics



internal
dynamics
y = z1,
where ϕˆ(z, u) = ϕ(t
−1
(z), u) holds, and then note that
˙ti(x) = Lf ti(x) = ∂ti(x)
∂x
f(x, u), i = δ + 1, . . . , n, (5.59)
depends on the control variable u. The dependence of the functions ˙ti on the
control variable u is similar to the control-affine case. However, this depen￾dence is not as simple to eliminate as in control-affine systems, where only
equation (5.35) on p. 373 needs to be solved. For equation (5.59) to be inde￾pendent of a non-constant control variable u, the partial differential equation
∂ ˙ti(x)
∂u =
∂
∂u 
∂ti(x)
∂x
f(x, u)

=
∂ti(x)
∂x
·
∂f(x, u)
∂u = 0388 Chapter 5. Nonlinear Control of Nonlinear Systems
must hold for all i = δ + 1, . . . , n. So we need to find n − δ functions ti such
that
Xn
k=1
∂ti(x)
∂xk
·
∂fk(x, u)
∂u = 0, i = δ + 1, . . . , n, (5.60)
holds.
When choosing or determining functions ti as solutions of equation (5.60),
we have to keep in mind that a diffeomorphism z = t(x) must result, i. e.
according to Theorem 54, p. 262,
det 
∂t(x)
∂x

6= 0
must hold. If we succeed in doing all the above, we then derive z = t(x) with
respect to time and obtain a similar transformed system representation as in
the control-affine case in equation (5.37). This is the generalized Byrnes-Isidori
canonical form
z˙ =




















Lf g(x)
L
2
f
g(x)
.
.
.
L
δ−1
f
g(x)
L
δ
f
g(x)
˙tδ+1(x)
.
.
.
˙tn(x)




















=




















z2
z3
.
.
.
zδ
ϕˆ(z, u)
qδ+1(z)
.
.
.
qn(z)




















,



external
dynamics



internal
dynamics
y = z1.
(5.61)
Here, the nonlinear part of the external dynamics is
z˙δ = ˆϕ(z, u) = v
and the internal dynamics are
z˙i = qi(z) = ˙ti(x) = ˙ti(t
−1
(z)), i = δ + 1, . . . , n.
In summarizing, using
u = ˆϕ
−1
(z, v) or u = ϕ
−1
(x, v)
and the control law (5.58), we can control the external dynamics of the trans￾formed system (5.61) and impose any linear dynamics on the control loop.5.2. Input-Output Linearization 389
5.2.8 Relative Degree and Internal Dynamics of Linear Systems
In the following, simply to deepen our understanding of the input-output lin￾earization method, we will apply input-output linearization to the controllable
and observable linear systems
x˙ = Ax + b · u,
y = c
T x.
(5.62)
Without any restriction of generality, we can assume that the system (5.62) is
present in controller canonical form or has been transformed into that form,
i. e. in the following we can assume that the system’s description is given by
A =










0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
−a0 −a1 −a2 · · · −an−1










, b =










0
0
.
.
.
0
1










,
c
T =
h
b0 b1 · · · bm 0 · · · 0
i
, bm 6= 0.
(5.63)
The associated transfer function is
G(s) = bms
m + . . . + b1s + b0
s
n + an−1s
n−1 + . . . + a1s + a0
. (5.64)
First, we will calculate the relative degree δ. In the derivatives
y
(i) = L
i
ac(x) + LbL
i−1
a c(x) · u = c
T A
ix + c
T A
i−1
b · u,
we use the identity
LbL
i−1
a c(x) = c
T Ai−1
b = 0 (5.65)
for i = 1, . . . , δ − 1. The inequality
c
T A
i
b 6= 0 (5.66)
is fulfilled only for i ≥ δ − 1. Using the Leverrier-Faddeev-Souriau-Frame
equation [135, 216]
(sI − A)
−1 =
1
D(s)
nX−1
k=0 X
k
l=0
an−k+lAl
!
s
n−1−k
,
where an = 1 and
D(s) = det(sI − A) = s
n + an−1s + . . . + a1s + a0390 Chapter 5. Nonlinear Control of Nonlinear Systems
is the characteristic polynomial of A, we obtain the expression
G(s) = c
T
(sI − A)
−1
b =
1
D(s)
nX−1
k=0 X
k
l=0
an−k+l c
T Al
b
!
s
n−1−k
for the transfer function of the system. Since c
T Al
b = 0 holds for l < δ − 1,
the transfer function can be written as
G(s) = 1
D(s)
nX−1
k=δ−1
 X
k
l=δ−1
an−k+l c
T A
l
b
!
s
n−1−k
.
Using
bn−1−k =
X
k
l=δ−1
an−k+l c
T Al
b, k = δ − 1, . . . , n − 1 and an = 1,
the transfer function finally becomes
G(s) = bn−δs
n−δ + bn−δ−1s
n−δ−1 + . . . + b1s + b0
s
n + an−1s
n−1 + . . . + a1s + a0
. (5.67)
If we compare equation (5.67) to equation (5.64), we see that the relative
degree of a linear system is
δ = n − m.
This means that the relative degree, also referred to as the difference degree,
is equal to the difference between the denominator degree and the numerator
degree.
The diffeomorphism (5.32) and the new coordinates z, which allow us to
formulate the system (5.62) in Byrnes-Isidori canonical form (5.37), are given
by
z =
















z1
z2
.
.
.
zδ
zδ+1
.
.
.
zn
















=
















c(x)
Lac(x)
.
.
.
L
δ−1
a c(x)
tδ+1(x)
.
.
.
tn(x)
















=
















c
T x
c
T Ax
.
.
.
c
T A
δ−1x
x1
.
.
.
xn−δ
















= t(x) (5.68)
in this case. The functions
tδ+i(x) = xi
, i = 1, . . . , n − δ,5.2. Input-Output Linearization 391
satisfy the partial differential equations (5.35) from Section 5.2.4 on p. 373,
which here take the form
∂tδ+i(x)
∂x
b(x) =
∂xi
∂x
b = 0, i = 1, . . . , n − δ.
At this point, recall that these differential equations ensure the independence
of the internal dynamics from the control variable u.
Now our aim is to transform the system (5.62) using the diffeomorphism
(5.68), which is linear here, i. e.
t(x) = T x = z, (5.69)
so that it is represented in z-coordinates and the internal dynamics become
visible. To this end, we will consider the upper δ elements of the vector in
equation (5.68), for which
c
T =
h
b0 b1 · · · bm 0 · · · 0
i
,
c
T A =
h
0 b0 b1 · · · bm 0 · · · 0
i
,
c
T A2 =
h
0 0 b0 b1 · · · bm 0 · · · 0
i
, (5.70)
.
.
.
c
T Aδ−1 =
h
0 · · · 0 b0 b1 · · · bm
i
holds. For the n × n matrix T of the transformation equation (5.69), we thus
obtain
T =




























b0 b1 b2 · · · bm−1 bm 0 0 · · · 0 0
0 b0 b1 · · · bm−2 bm−1 bm 0 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · · · · · · · · · · · · · · · · 0 0
0 0 0 · · · · · · · · · · · · · · · · · · bm 0
0 0 0 · · · · · · · · · · · · · · · · · · bm−1 bm
1 0 0 · · · 0 0 0 0 · · · 0 0
0 1 0 · · · 0 0 0 0 · · · 0 0
0 0 1 · · · 0 0 0 0 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1 0 0 0 · · · 0 0




























,



δ rows



m=n−δ
rows
| {z }
m = n − δ columns
| {z }
δ columns392 Chapter 5. Nonlinear Control of Nonlinear Systems
which is identical to the observability matrix
Mobs =










c
T
c
TA
c
TA2
.
.
.
c
TAn−1










consisting of the n row vectors (5.70) if δ = n. The Matrix T can be divided
into two parts according to
T =


M
Z

, M ∈ IRδ×n
, Z ∈ IR(n−δ)×n
,
where M is equal to the first δ rows of T and Z matches the last n − δ rows.
The matrix T is regular and so is the transformation, because
det(T) = det


M
Z

 = (−1)n−δ
det


Z
M

 = (−1)n−δ
b
δ
m
holds, since the swapping of M and Z results in a lower triangular matrix
whose determinant is equal to the product of its diagonal elements [40].
To obtain the transformed system description, we will first calculate the
derivative of the transformation equation (5.69)
z˙ = Tx˙ =
















c
T A x
c
T A
2x
.
.
.
c
T Aδx + c
T Aδ−1
b · u
x˙ 1
.
.
.
x˙ n−δ
















=
















z2
z3
.
.
.
c
T AδT
−1
z + c
T Aδ−1
b · u
x2
.
.
.
xn−δ+1
















,
y = c
T x = z1
(5.71)
based on equation (5.68) as well as equations (5.62) and (5.63). Furthermore,
according to equation (5.68), the identities
x1 = zδ+1, x2 = zδ+2, x3 = zδ+3, . . . , xn−δ = zn (5.72)
apply, and, using5.2. Input-Output Linearization 393
c
T x = b0 x1 + b1 x2 + . . . + bn−δ xn−δ+1 = z1
and m = n − δ, we obtain the relation
xn−δ+1 =
z1 − b0x1 − b1x2 − . . . − bn−δ−1xn−δ
bn−δ
=
z1 − b0zδ+1 − b1zδ+2 − . . . − bm−1zn
bm
.
(5.73)
If we insert equations (5.72) and (5.73) into equation (5.71), the system de￾scription eventually yields





















z˙1
.
.
.
z˙δ−1
z˙δ
z˙δ+1
.
.
.
z˙n−1
z˙n





















=





















z2
.
.
.
zδ
c
T AδT
−1
z + c
T Aδ−1
b · u
zδ+2
.
.
.
zn
−
b0
bm
zδ+1 −
b1
bm
zδ+2 − . . . −
bm−1
bm
zn +
1
bm
z1





















,



external
dynamics



internal
dynamics
y = z1. (5.74)
This is the Byrnes-Isidori canonical form of a linear system (5.62).
Considering only the internal dynamics












z˙δ+1
z˙δ+2
.
.
.
z˙n−1
z˙n












=












0 1 0 · · · 0 0
0 0 1 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 1
−
b0
bm
−
b1
bm
−
b2
bm
· · · −
bm−2
bm
−
bm−1
bm
























zδ+1
zδ+2
.
.
.
zn−1
zn












+












0
0
.
.
.
0
1
bm












z1,
the state variable z1 acts as the input variable. As we can immediately see
in the companion matrix above, the internal system dynamics possess the
characteristic polynomial P(s)/bm with
P(s) = bms
m + bm−1s
m−1 + . . . + b1s + b0.
The eigenvalues of the internal dynamics are therefore identical to the zeros
of the linear system. Of course, the transformed system (5.74) as a whole has
the same eigenvalues as the original system (5.62).
In summary, we can formulate394 Chapter 5. Nonlinear Control of Nonlinear Systems
Theorem 61 (Relative Degree and Internal Dynamics of Linear Sys￾tems). A controllable and observable linear system with the transfer function
G(s) = bms
m + bm−1s
m−1 + . . . + b1s + b0
s
n + an−1s
n−1 + . . . + a1s + a0
has the relative degree δ = n − m. The eigenvalues of the internal dynamics
are identical to the zeros of the transfer function.
We will now address the exceptional case of an uncontrollable and unob￾servable system. If equation (5.66) is not fulfilled for any i = 1, . . . , n − 1, i. e.
if c
T A
n−1
b = 0, then δ > n applies; more precisely, δ = ∞. To determine
that a system such as this is uncontrollable and unobservable, we calculate
[c
T
b cTAb cTA2
b · · · c
TAn−1
b] = c
TMcontr = (Mobsb)
T = 0
T
using equation (5.65). The expression above can only be identical to zero if
both the controllability matrix
Mcontr =[b Ab · · · An−1
b]
and the observability matrix
Mobs =










c
T
c
TA
c
TA
2
.
.
.
c
TAn−1










are singular. Therefore, the system is neither controllable nor observable if
δ = ∞.
In the following we will turn to a further issue and draw an analogy to
nonlinear systems. In linear and nonlinear systems, the internal dynamics can
be represented by
z˙int = q(zext, zint)
with
zext = [z1 · · · zδ]
T
and zint = [zδ+1 · · · zn]
T
.
Inserting the expression
u = −
c
T AδT
−1
z
c
T Aδ−1
b
into equation (5.74) and setting the initial values of zext(t) to zext(0) = 0
yields5.2. Input-Output Linearization 395
zext(t) = 0 for t ≥ 0,
and, correspondingly, y(t) = z1 = 0. The internal dynamics are now unaffected
by the state vector zext, or more precisely the state z1, which result from the
external dynamics. The variable z1 can be interpreted as the input variable
of the internal dynamics. Thus,
z˙int = q(0, zint), (5.75)
and in the case of linear systems the internal dynamics’ behavior are only
determined by the zeros of the system. This special case of internal dynamics
is referred to as zero dynamics, which is also defined for nonlinear systems by
(5.75). The zero dynamics of nonlinear systems thus have a similar meaning
as the zeros in linear systems.
If the zero dynamics (5.75), or more precisely their equilibrium point of
interest, zint,eq = 0, are asymptotically stable, the nonlinear system is often
described as minimum phase[5]. This is analogous to minimum-phase linear
systems, which are designated as minimum phase if they have only poles and
zeros si with Re {si} < 0 [199, 342]. Where the equilibrium point zint,eq of the
zero dynamics is not asymptotically stable, only Lyapunov stable, the system
is designated as weakly minimum phase. Since the zero dynamics of nonlinear
systems are normally nonlinear, more than one equilibrium point may exist.
Consequently, discussing its stability can be exceptionally complicated in this
case.
5.2.9 Control Law for the Linear Case
In addition to the previous section, we will revisit the input-output lineariza￾tion method for the linear plant (5.62), (5.63) and determine its control law.
From a practical point of view, of course, this is not necessary, since a linear
system does not require linearization. But this gives us two further interesting
insights into the method.
We obtain
u = −
c
T AδT
−1
z + k
T
z
c
T A
δ−1
b
+
V
c
T A
δ−1
b
yref, k
T = [ˆa0 · · · aˆδ−1 0 · · · 0] (5.76)
[5] The term minimum phase was introduced by H.W. Bode [52] and is mainly used
in communications engineering [342]. Its definition is not consistent in the litera￾ture. Among other fields, it is used in filter design, for which the lowest possible
phase shift is important. The common usage of the term as a stability description
for internal or zero dynamics seems to be misleading, because the stability of a
nonlinear system does not correspond to a phase response, which a nonlinear sys￾tem does not have at all. In addition, nonlinear zero dynamics can have several
equilibrium points with different stability behavior. Therefore, in this book the
terms asymptotic stability and Lyapunov stable are used in connection with the
internal dynamics and zero dynamics, because they are more appropriate. See
also [496].396 Chapter 5. Nonlinear Control of Nonlinear Systems
for the control law of the input-output linearization. Inserting this into the
Byrnes-Isidori canonical form (5.74) of a linear plant yields





















z˙1
.
.
.
z˙δ−1
z˙δ
z˙δ+1
.
.
.
z˙n−1
z˙n





















=





















0 1 0 · · · 0 0 0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1 0 0 0 · · · 0
−aˆ0 −aˆ1 −aˆ2 · · · −aˆδ−1 0 0 0 · · · 0
0 0 0 · · · 0 0 1 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 0 0 0 · · · 1
1
bm
0 0 · · · 0 −
b0
bm
−
b1
bm
−
b2
bm
· · · −
bm−1
bm





















| {z }
Aˆ





















z1
.
.
.
zδ−1
zδ
zδ+1
.
.
.
zn−1
zn





















+





















0
.
.
.
0
V
0
.
.
.
0
0





















| {z }
ˆb
yref,
y =
h
1 0 · · · 0
i
| {z }
ˆc
T
z
(5.77)
as equations for the control loop. This illustrates again that only the first δ
equations are relevant for the input-output behavior.
Next we will determine the transfer function
G(s) = Y (s)
W(s)
= cˆ
T
(sI − Aˆ)
−1ˆb.
The system matrix Aˆ and thus sI − Aˆ are block lower triangular matrices.
Therefore, we can easily calculate (sI − Aˆ)
−1
, because for the inverse of a
block lower triangular matrix
X =


X11 0
X21 X22

,
we obtain [40]
X−1 =


X−1
11 0
−X−1
22 X21X−1
11 X−1
22

.
Next we will partition Aˆ, ˆb, and cˆ
T
correspondingly, resulting in
Aˆ =


Aˆext 0
Aˆv Aˆint

, ˆb =


ˆbext
0

, and cˆ
T =
h
cˆext 0
i
, (5.78)5.2. Input-Output Linearization 397
where Aˆext ∈ IRδ×δ
, Aˆv ∈ IRm×δ
, Aˆint ∈ IRm×m, and cˆext, ˆbext ∈ IRδ
hold.
Note that cˆ
T
has only one non-zero element cˆ1 = 1, so it is not necessary
to include the bottom row of Aˆ in equation (5.78) in the calculation of the
transfer function. We thus arrive at
G(s) = cˆ
T
(sI − Aˆ)
−1ˆb = cˆ
T
ext(sI − Aˆext)
−1ˆbext =
=
h
1 0 · · · 0
i


sI −







0 1 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
−aˆ0 −aˆ1 −aˆ2 · · · −aˆδ−1









−1 






0
.
.
.
0
V







=
V
s
δ + ˆaδ−1s
δ−1 + . . . + ˆa1s + ˆa0
. (5.79)
The above result is not surprising, since the transfer behavior of a control loop
with a relative degree δ resulting from an input-output linearization is always
of the form (5.79).
Once again it is clear that the controller (5.76) leads to a reduction in the
system order from the order n of the plant (5.74) to the order δ of the transfer
function (5.79). However, this is only the case if the system matrix A, the input
vector b, and the output vector c are identical to the model parameters Am,
bm, and cm of the plant in the control law (5.76). If the model parameters
Am, bm, and cm deviate from the true system parameters A, b, and c, the
control law (5.76) no longer compensates for the eigenmovements
c
T AδT
−1
z
of the external dynamics in equation (5.74). That is, after inserting the control
law (5.76), which now takes the form
u = −
cm
T Am
δT
−1
z + k
T
z
cm
T Am
δ−1bm
+
V
cm
T Am
δ−1bm
yref,
into equation (5.74), we no longer obtain the differential equation
z˙δ = k
T
z + V · yref = −aˆδ−1zδ − aˆδ−2zδ−1 − . . . − aˆ1z2 − aˆ0z1 + V · yref;
instead we obtain
z˙δ =
"
c
T AδT
−1 −
c
T A
δ−1
b
cm
T Am
δ−1bm
(cm
T Am
δT
−1 + k
T
)
#
z +
c
T A
δ−1
bV · yref
cm
T Am
δ−1bm
.
Now the derivative z˙δ depends on the entire state vector z and thus on the
states zδ+1, . . . , zn of the internal dynamics. As a result, the system matrix
Aˆ of the control loop no longer exhibits the shape of a block lower triangular398 Chapter 5. Nonlinear Control of Nonlinear Systems
matrix as in equation (5.77); rather it is fully populated and the transfer func￾tion is of order n and not of order δ as in the case of exact modeling. So if the
modeling is imprecise, an increase in the system order of the transfer function
(5.79) occurs and the states of the internal dynamics now also influence the
input-output behavior. The resulting problem is obvious. For example, weakly
damped or slow eigenmovements caused by the imprecise linearization may
now disturb the originally planned transfer behavior (5.77). In the nonlin￾ear case, these robustness problems are similar. However, they are even more
challenging, because if the model deviates from the real controlled system, the
control loop’s input-output behavior is no longer linear.
Note that input-output linearization requires precise modeling if the
planned linear transfer behavior between the reference variable yref and the
output variable y is actually to be achieved.
A further insight emerges: it is now clear that the controller (5.76) of the
input-output linearization functions as a compensation controller, because
poles are canceled against zeros. This means that the control loop is no longer
observable. The unobservability is related to the fact that the transfer function
(5.79) has an order reduced by the value n − δ compared to the state-space
representation of the plant (5.77). From linear systems theory, we know that
a state-space model of order n has a transfer function with an order of less
than n if and only if it is not controllable or not observable, or both. Based
on the observability matrix
Mobs =










cˆ
T
cˆ
T Aˆ
cˆ
T Aˆ
2
.
.
.
cˆ
T Aˆ
n−1










=










cˆ
T
ext 0
cˆ
T
extAˆext 0
cˆ
T
extAˆ
2
ext 0
.
.
.
.
.
.
cˆ
T
extAˆ
n−1
ext 0










of the control loop (5.77), we can show its unobservability, because the last
n − δ columns of the observability matrix are zero vectors. Therefore, Mobs
is not regular. However, the regularity of Mobs is necessary and sufficient for
the observability of the control loop. Note that the plant (5.62) is generally
observable.
5.2.10 Stability of Internal and Zero Dynamics
In the previous sections, we have only briefly touched on the importance of the
initial dynamics for the control loop’s stability. If we take a closer look, various
questions arise: what kind of stability is required for the internal dynamics?
Is input-to-state stability a necessary prerequisite? What is the significance
of zero dynamics in this context?
First of all, we must emphasize that the internal dynamics5.2. Input-Output Linearization 399





z˙δ+1
.
.
.
z˙n





=





qδ+1(z1, . . . , zδ, zδ+1, . . . , zn)
.
.
.
qn(z1, . . . , zδ, zδ+1, . . . , zn)





are generally nonlinear. In the following, we will formulate it more concisely
as
z˙int = q(zext, zint), (5.80)
using
zext =





z1
.
.
.
zδ





and zint =





zδ+1
.
.
.
zn





.
Due to its nonlinearity, it might have more than one equilibrium point. There￾fore, in general it is sometimes misleading to refer to the stability of the inter￾nal dynamics; it is more accurate to refer to the stability of their equilibrium
points. In consequence, we must distinguish between local and global stability
or we must examine the system’s stability as defined in Definition 8 or 9 on
p. 18 as a whole. The controlled external dynamics of the input-output lin￾earized system, on the other hand, are linear. Thus, generally speaking, there
is only one asymptotically stable equilibrium point at zero.
Since the stability analysis of an equilibrium point is always based on
vanishing – or at least constant – input values, the input values of the internal
dynamics
z1, . . . , zδ
must be set at zero for this investigation. We thus obtain the zero dynamics
z˙int = q(0, zint)
alone; that is, we can analyze the stability of the equilibrium point or points of
the internal dynamics using the zero dynamics. The simplest situation is that
involving a single globally asymptotically stable equilibrium point. If, on the
other hand, several equilibrium points exist, at most local stability is possible.
In the latter case, it must be ensured that this local stability is sufficient for
control-loop operation.
A single globally asymptotically stable equilibrium point of the internal
dynamics is a necessary condition for the global asymptotical stability of the
equilibrium point of the control loop. But is this, along with a globally asymp￾totically stable equilibrium point of the external dynamics, also sufficient to
ensure the global asymptotic stability of the equilibrium of the control loop
consisting of external and internal dynamics? This question is entirely jus￾tified because the external dynamics influence the internal dynamics via the400 Chapter 5. Nonlinear Control of Nonlinear Systems
variables z1, . . . , zδ. Let us denote the external dynamics of the input-output
linearized control loop as
z˙ext = Aˆextzext + ˆbext · yref,
which, along with the internal dynamics (5.80), yields a composite system of
the form
z˙ext = Aˆextzext + ˆbext · yref,
o
external dynamics
z˙int = q(zext, zint)
o
internal dynamics
(5.81)
with the associated equilibrium point
zeq =


zext,eq
zint,eq

 = 0.
We can assume, without loss of generality, that the equilibrium point or, where
several equilibrium points exist, one of the equilibrium points of the internal
dynamics lies in zint = 0 or was transformed there.
System (5.81) is a cascaded system, whose stability was analyzed in several
works [66, 115, 351, 410, 427, 429, 464]. Using the results of [428, 429], and
[464], we obtain
Theorem 62 (Stability of Input-Output Linearized Systems). The
equilibrium point zeq = 0 of an input-output linearized system
z˙ =


z˙ext
z˙int

 =


Aˆextzext
q(zext, zint)

 +


ˆbext
0

yref
whose matrix Aˆext we assume to possess only eigenvalues with negative real
parts is
(1) asymptotically stable if and only if the equilibrium point zint,eq = 0 of the
zero dynamics z˙int = q(0, zint) is asymptotically stable and all functions
qi have bounded derivatives in a neighborhood of the origin,
(2) globally asymptotically stable if the equilibrium point zint,eq = 0 of the
zero dynamics z˙int = q(0, zint) is globally asymptotically stable and the
internal dynamics z˙int = q(zext, zint) with its input variable vector zext
are input-to-state stable,
(3) globally exponentially stable if and only if the equilibrium point zint,eq = 0
of the zero dynamics z˙int = q(0, zint) is globally exponentially stable and
all functions qi have bounded derivatives.
With the result above, we have established that the stability of the equilibrium
point zeq = 0 of the input-output linearized system is essentially determined
by the zero dynamics. Notice that the requirement for bounded derivatives of
the functions qi
in Statement (1) is satisfied if the functions qi are continuously
differentiable in a neighborhood of the origin.5.2. Input-Output Linearization 401
5.2.11 Input-Output Linearization of MIMO Systems
In the following our aim is to apply input-output linearization to MIMO sys￾tems
x˙ = a(x) +Xm
i=1
bi(x)ui
,
y = c(x)
(5.82)
with n state variables xj , m input variables ui
, and the same number of m
output variables yi
.
Requiring that the input and output variables both amount to m in num￾ber appears to be a restriction. If we wish to include systems with more output
than input variables, this is definitely the case. To add additional input vari￾ables involves a constructive extension of the system and leads to additional
costs. Therefore, this option is usually not considered. If, on the other hand,
we wish to address systems with more inputs than outputs, we can adapt the
dimension of the output variable vector y to that of the input variable vector
u by adding further output variables. Such additional output variables yi can
be state variables xj , which do not occur in the output vector c(x). One could
counter that additional output variables imply that they require measurement
and thus mean additional effort. However, all state variables must be mea￾sured in any case in order to implement a controller based on input-output
linearization.
As in the SISO case, we aim to design a controller and a pre-filter which
linearize the dynamic behavior occurring between the input and output vari￾ables. For this, we will begin with a single output variable yi = ci(x) of the
system and differentiate it according to
yi = ci(x),
y˙i = Laci(x) +Xm
k=1
Lbk
ci(x)
| {z }
0
uk,
y¨i = L
2
a
ci(x) +Xm
k=1
Lbk Laci(x)
| {z }
0
uk,
.
.
.
y
(δi−1)
i = L
δi−1
a ci(x) +Xm
k=1
Lbk L
δi−2
a ci(x)
| {z }
0
uk,
y
(δi)
i = L
δi
a ci(x) +Xm
k=1
Lbk L
δi−1
a ci(x)
| {z }
6= 0 for at
least one k
uk.
(5.83)402 Chapter 5. Nonlinear Control of Nonlinear Systems
In this sequence of derivatives, all Lie derivatives Lbk L
i
a
ci(x) with respect to
the vectors bk, k = 1, . . . , m, are equal to zero. Only in the derivative y
(δi)
i
, at
least one of these Lie derivatives is unequal to zero. Therefore, in a manner
similar to the SISO case, we will define the relative degree δi belonging to the
output value yi
, such that
Lbk L
l−1
a
ci(x) = 0, l = 1, . . . , δi − 1
and
Lbk L
δi−1
a ci(x) 6= 0 (5.84)
hold for at least one k ∈ {1, . . . , m}.
Now we will perform this computation for all output variables yi
, . . . , ym,
and thus determine the corresponding relative degrees δi
, . . . , δm as well as
the m equations
y
(δ1)
1 = L
δ1
a
c1(x) +Xm
k=1
Lbk L
δ1−1
a
c1(x)uk,
y
(δ2)
2 = L
δ2
a c2(x) +Xm
k=1
Lbk L
δ2−1
a c2(x)uk,
.
.
.
y
(δm)
m = L
δm
a
cm(x) +Xm
k=1
Lbk L
δm−1
a
cm(x)uk.
(5.85)
The equations (5.85) can be written more concisely in vectorial form as
˚y = ˚c(x) + D(x) · u (5.86)
with the m - dimensional vectors
˚y =










y
(δ1)
y
(δ2)
.
.
.
y
(δm)










, ˚c(x) =










L
δ1
a
c1(x)
L
δ2
a
c2(x)
.
.
.
L
δm
a
cm(x)










and the m × m decoupling matrix
D(x) =










Lb1 L
δ1−1
a c1(x) Lb2 L
δ1−1
a c1(x) · · · Lbm L
δ1−1
a c1(x)
Lb1 L
δ2−1
a c2(x) Lb2 L
δ2−1
a c2(x) · · · Lbm L
δ2−1
a c2(x)
.
.
.
.
.
.
.
.
.
.
.
.
Lb1 L
δm−1
a cm(x) Lb2 L
δm−1
a cm(x) · · · LbmL
δm−1
a cm(x)










.5.2. Input-Output Linearization 403
The circles on the vectors ˚y and ˚c symbolize the multiple derivatives y
(δi)
and
L
δi
a ci(x) of the vector elements.
The relationship (5.86) is the key equation for the exact input-output
linearization of MIMO systems. Obviously, if the decoupling matrix D(x) is
regular, we can introduce a new input variable vector v ∈ IRm with
u = −D−1
(x) · (˚c(x) − v), (5.87)
which, if inserted into equation (5.86), not only linearizes the input-output
behavior of the resulting system to
˚y = v,
i. e.
y
(δ1)
1 = v1,
y
(δ2)
2 = v2,
.
.
.
y
(δm)
m = vm;
(5.88)
it also completely decouples the effect of the input variables vi on the deriva￾tives y
(δi)
i
and thus the output variables yi
. This means that each input vari￾able vi
influences only one output variable yi
.
From equation (5.88), the output variables yi are obtained by the δi-fold
integration of the new control variables vi
. Each of the m equations thus
represents an integrator chain of length δi
. If we select the actuating variables
vi as control laws
vi = −ai,δi−1
y
(δi−1)
i − ai,δi−2
y
(δi−2)
i − . . . − ai,0yi + Vi
· yref,i, i = 1, . . . , m
(5.89)
with the reference variables yref,i, the result is control loops, to which any
dynamics
y
(δi)
i + ai,δi−1
y
(δi−1)
i + . . . + ai,1y˙i + ai,0yi = Vi
· yref,i, i = 1, . . . , m
can be assigned by freely choosing the coefficients ai,j . This corresponds to
the SISO case.
If we also insert the relations obtained from equation (5.83)
yi = ci(x), y˙i = Laci(x), . . . , y
(δi−1)
i = L
δi−1
a ci(x)
into equation (5.89) and subsequently equation (5.87), we obtain the control
law404 Chapter 5. Nonlinear Control of Nonlinear Systems
u=−D−1
(x)











L
δ1
a
c1(x) + . . . + a1,1Lac1(x) + a1,0c1(x) − V1 · yref,1
L
δ2
a
c2(x) + . . . + a2,1Lac2(x) + a2,0c2(x) − V2 · yref,2
.
.
.
L
δm
a cm(x) + . . . + am,1Lacm(x) + am,0cm(x) − Vm · yref,m











,
(5.90)
which is dependent on the state vector x and is structured analogously to the
control laws from Theorems 59 on p. 366 and 60 on p. 377 for the SISO case.
As we can see from the derivation above, we can perform input-output
linearization without transforming the system into the nonlinear controller
canonical form via a diffeomorphism. However, in this case, information about
the remaining system behavior will be lacking, i. e. information about the
unobservable internal dynamics which are described by the transformed state
variables.
A prerequisite for the procedure above is the regularity of the decoupling
matrix D(x). Only in this case is complete decoupling between input variables
vi or yref,i and output variables yi possible. As we will see, internal dynamics
can also occur. Whether such dynamics are present or not depends once again
on the relative degree of the MIMO system. We will define it in
Definition 33 (Vector Relative Degree of MIMO Systems). Let there
be a system
x˙ = a(x) +Xm
i=1
bi(x)ui
,
y = c(x)
with x ∈ Dx,def ⊆ IRn
and y ∈ IRm. Then
δ =
h
δ1 δ2 · · · δm
i
is called the vector relative degree of the system if the following applies:
(1) Lbk L
l−1
a ci(x) = 0, i, k = 1, . . . , m, l = 1, . . . , δi − 1.
(2) The m×m decoupling matrix D(x) is regular.
The fulfillment of requirement (5.84) that, for each i = 1, . . . , m, at least one
k ∈ {1, . . . , m} with
Lbk L
δi−1
a
ci(x) 6= 05.2. Input-Output Linearization 405
exists is ensured by the regularity of the matrix D(x).
The sum of the values δi
,
δ = δ1 + δ2 + . . . + δm,
is referred to as the total relative degree of a MIMO system. If the decoupling
matrix D(x) is regular for all x ∈ IRn
, the relative degree, as in the SISO
case, is well-defined. The following holds [459]:
Theorem 63 (Maximum Total Relative Degree). For the total relative
degree δ of a controllable and observable control-affine MIMO system with the
system order n, the relation
δ = δ1 + δ2 + . . . + δm ≤ n
holds.
In the SISO case, the total relative degree is equal to the relative degree. For
the exceptional situation where a system is uncontrollable and unobservable,
we obtain
δ = ∞,
as in the SISO case. This means we cannot perform input-output linearization.
5.2.12 MIMO Control Loops in State-Space Representation
We will transform the control-affine MIMO system (5.82) into the Byrnes￾Isidori canonical form and thus obtain the state description of the transformed
system. For this purpose, we will use the diffeomorphism
z =


































y1
y˙1
.
.
.
y
(δ1−1)
1
.
.
.
ym
y˙m
.
.
.
y
(δm−1)
m
zδ+1
zδ+2
.
.
.
zn


































= t(x) =


































c1(x)
Lac1(x)
.
.
.
L
δ1−1
a c1(x)
.
.
.
cm(x)
Lacm(x)
.
.
.
L
δm−1
a cm(x)
tδ+1(x)
tδ+2(x)
.
.
.
tn(x)


































. (5.91)406 Chapter 5. Nonlinear Control of Nonlinear Systems
The functions
tδ+1, . . . , tn
must be selected in such a way that the mapping
z = t(x)
is bijective and both t and t
−1
are continuously differentiable, i. e. t(x) is a
diffeomorphism. Where
δ = n,
the system has a maximum total relative degree and the functions tδ+1, . . . , tn
are omitted. In this case, the system has no internal dynamics. Because the
functions tδ+1, . . . , tn need not be determined in this case, the calculation of
the diffeomorphism is simplified considerably.
We will analyze the general case with internal dynamics, meaning δ < n,
and determine the functions ti(x). But first we will derive the transformed sys￾tem description, which will lead to a determination method for the functions
ti(x). The procedure is similar to that used in the SISO case.
Accordingly, we will transform the original system description (5.82) by
differentiating the diffeomorphism (5.91) with respect to time, which results
in
z˙ =
dt(x)
dt =





































Lac1(x)
L
2
a
c1(x)
.
.
.
L
δ1
a c1(x) +Xm
k=1
Lbk L
δ1−1
a c1(x)uk
.
.
.
Lacm(x)
L
2
acm(x)
.
.
.
L
δm
a cm(x) +Xm
k=1
Lbk L
δm−1
a cm(x)uk
˙tδ+1(x)
.
.
.
˙tn(x)





































.
From this, the Byrnes-Isidori canonical form5.2. Input-Output Linearization 407









































z˙1
z˙2
.
.
.
z˙δ1−1
z˙δ1
.
.
.
z˙δ1+...+δm−1+1
z˙δ1+...+δm−1+2
.
.
.
z˙δ−1
z˙δ
z˙δ+1
.
.
.
z˙n









































=









































z2
z3
.
.
.
zδ1
L
δ1
a c1(x) +Xm
k=1
Lbk L
δ1−1
a c1(x)uk
.
.
.
zδ1+...+δm−1+2
zδ1+...+δm−1+3
.
.
.
zδ
L
δm
a
cm(x) +Xm
k=1
Lbk L
δm−1
a
cm(x)uk
˙tδ+1(x)
.
.
.
˙tn(x)









































(5.92)
follows. At this point, recall that the total relative degree δ in the equation
above satisfies the relation
δ = δ1 + δ2 + . . . + δm.
The output variable vector in the transformed system from equation (5.92)
above is given by
y =










z1
zδ1+1
zδ1+δ2+1
.
.
.
zδ1+δ2+...+δm−1+1










.
For the derivatives ˙ti(x), we obtain
˙ti(x) = ∂ti(x)
∂x
· x˙ = Lati(x) +Xm
k=1
Lbk
ti(x)uk, i = δ + 1, . . . , n. (5.93)
In contrast to the SISO case, which we discussed in Section 5.2.4, these n − δ
equations generally cannot be made independent of all actuating variables uk,408 Chapter 5. Nonlinear Control of Nonlinear Systems
k = 1, . . . , m, by a suitable choice of functions ti(x). This means that the
m(n − δ) partial differential equations
Lbk
ti(x) = ∂ti(x)
∂x
bk(x) = 0, i = δ + 1, . . . , n, k = 1, . . . , m, (5.94)
in many cases cannot all be fulfilled. The reason for this problem is that there
are m(n−δ) coupled partial differential equations for n−δ functions ti
instead
of n − δ single partial differential equations in the SISO case.
Inserting
x = t
−1
(z) (5.95)
into equation (5.92), using
ϕi(z,u) = L
δi
a ci(t
−1
(z)) +Xm
k=1
Lbk L
δi−1
a ci(t
−1
(z))uk, i = 1, . . . , m, (5.96)
as an abbreviation, and inserting diffeomorphism (5.95) into equation (5.93),
which results in
qi(z,u) = ˙ti(x) = Lati(t
−1
(z)) +Xm
k=1
Lbk
ti(t
−1
(z))uk, i = δ + 1, . . . , n,
yields a transformation of the system to the Byrnes-Isidori canonical form





































z˙1
z˙2
.
.
.
z˙δ1−1
z˙δ1
.
.
.
z˙δ1+...+δm−1+1
z˙δ1+...+δm−1+2
.
.
.
z˙δ−1
z˙δ
z˙δ+1
.
.
.
z˙n





































=





































z2
z3
.
.
.
zδ1
ϕ1(z,u)
.
.
.
zδ1+...+δm−1+2
zδ1+...+δm−1+3
.
.
.
zδ
ϕm(z,u)
qδ+1(z,u)
.
.
.
qn(z,u)





































.



external dynamics



internal dynamics
(5.97)5.2. Input-Output Linearization 409
The functions ϕi(z,u) in equation (5.96) are identical to the derivatives
y
(δi)
i
in equation (5.85). From equation (5.85), we have already derived the
control law (5.90) for the input-output linearization which we will also apply
in this case. It linearizes the m nonlinear differential equations in equation
(5.97), i. e.
z˙δ1 = ϕ1(z,u) = L
δ1
a
c1(x) +Xm
k=1
Lbk L
δ1−1
a
c1(x)uk,
z˙δ1+δ2 = ϕ2(z,u) = L
δ2
a
c2(x) +Xm
k=1
Lbk L
δ2−1
a
c2(x)uk,
.
.
.
z˙δ1+...+δm = ϕm(z,u) = L
δm
a cm(x) +Xm
k=1
Lbk L
δm−1
a cm(x)uk.
Using the diffeomorphism (5.91) in addition to the control law (5.90), we
obtain the state-space model of the input-output linearized and completely
decoupled control loop





































z˙1
z˙2
.
.
.
z˙δ1−1
z˙δ1
.
.
.
z˙δ1+...+δm−1+1
z˙δ1+...+δm−1+2
.
.
.
z˙δ−1
z˙δ
z˙δ+1
.
.
.
z˙n





































=





































z2
z3
.
.
.
zδ1
−a1,0z1 − a1,1z2 − . . . − a1,δ1−1zδ1 + V1yref,1
.
.
.
zδ1+...+δm−1+2
zδ1+...+δm−1+3
.
.
.
zδ
−am,0zδ1+...+δm−1+1 − . . . − am,δm−1zδ + Vmyref,m
qδ+1(z,u) = ˆqδ+1(z, yref)
.
.
.
qn(z,u) = ˆqδ+1(z, yref)





































with the reference variable vector410 Chapter 5. Nonlinear Control of Nonlinear Systems
yref =





yref,1
.
.
.
yref,m





.
As expected, the external dynamics of this control loop are stated in the
linear controller canonical form of a MIMO system. The last n − δ differential
equations
z˙int = qˆ(z, yref)
which make up the internal dynamics are usually nonlinear, as in the SISO
case. If we satisfy condition (5.94), all qˆi are independent of yref.
In the case of a total relative degree
δ = n
there are no internal dynamics. For
δ < n,
we must also include the internal dynamics in our stability analysis.
5.2.13 Example: Combustion Engine
We will use a combustion engine as an example [361, 376]. Our objective is to
design a control for the idle speed. The available control variables u1 and u2
are measures of the ignition timing and a variable representing the position of
the throttle valve, respectively. The latter controls the air-mass flow necessary
for combustion, as shown schematically in Figure 5.15. With the rotational
speed x1, a measure x2 for the air-mass flow, and the air pressure x3 in the
intake manifold as state variables, as well as the disturbance variable d, the
model can be stated as
x˙ = a(x) + B(x) · u + s · d,
y = c(x)
with
a(x) =




a0 + a1x1 + a2x
2
1 + a3x2
a4x1x2 + a5x
2
1x3 + a6x
2
1x
2
3
a7x1x3 + a8x1x
2
3




, B(x) =




1 0
0 0
0 b(x3)




,
s =




−ad
0
0




, c(x) =


x1
x2

 ,5.2. Input-Output Linearization 411
x2
x1
x3
u1
u2
Fig. 5.15: Combustion engine with variables u1 and u2 representing the ignition
timing and the throttle position as control variables, as well as the rotational speed
x1, a variable x2 representing the air-mass flow, and the air pressure x3 in the intake
manifold as state variables
and the continuous function
b(x3) =



a9, x3 ≤ 0.5Pa,
2a9
Pa
q
x3Pa − x
2
3
, x3 > 0.5Pa.
Here Pa is the atmospheric air pressure. The disturbance d includes all dis￾turbances as they may be caused by the power consumption of the servo
hydraulics, the lighting, the radio system, the air conditioning system, etc.
We can determine
y˙1 = Lac1(x) + Lb1
c1(x) · u1 + Lb2
c1(x) · u2 + Lsc1(x) · d
= ˙x1 = a0 + a1x1 + a2x
2
1 + a3x2 + u1 − add
with412 Chapter 5. Nonlinear Control of Nonlinear Systems
Lac1(x) = a0 + a1x1 + a2x
2
1 + a3x2,
Lb1
c1(x) = 1,
Lb2
c1(x) = 0,
Lsc1(x) = −ad,
in which we treat the disturbance d as an input variable. Since y˙1 depends on
u1, δ1 = 1 follows.
Further, it holds that
y˙2 = Lac2(x) + Lb1
c2(x) · u1 + Lb2
c2(x) · u2 + Lsc2(x) · d = Lac2(x)
= ˙x2 = a4x1x2 + a5x
2
1x3 + a6x
2
1x
2
3
and
y¨2 = L
2
a
c2(x) + Lb1 Lac2(x) · u1 + Lb2 Lac2(x) · u2 + LsLac2(x) · d
with
L
2
a
c2(x) = x
3
1x3(a5 + 2a6x3)(a7 + a8x3) + a4x
2
1
[a4x2 + x1x3(a5 + a6x3)]
+ (a0 + a1x1 + a2x
2
1 + a3x2)[a4x2 + 2x1x3(a5 + a6x3)],
Lb1 Lac2(x) = a4x2 + 2a5x1x3 + 2a6x1x
2
3
,
Lb2 Lac2(x) = b(x3)(a5x
2
1 + 2a6x
2
1x3),
and
LsLac2(x) = −ad(a4x2 + 2a5x1x3 + 2a6x1x
2
3
).
This means the relative degree of the second state variable is δ2 = 2 and the
vector relative degree becomes
δ = [1 2].
With these results, we obtain


y˙1
y¨2

 =


Lac1(x)
L
2
a
c2(x)

 +


Lb1
c1(x) 0
Lb1 Lac2(x) Lb2 Lac2(x)

u +


−ad
LsLac2(x)

d.
Using the control law (5.87), which takes the form
u = −
1
Lb1
c1(x)Lb2 Lac2(x)
·


Lb2 Lac2(x) 0
−Lb1 Lac2(x) Lb1
c1(x)




Lac1(x) − v1
L
2
a
c2(x) − v2


(5.98)5.3. Full-State Linearization 413
in this case, and the two new input variables v1 and v2 yields
y˙1 = v1 − add,
y¨2 = v2 − ad(a4x2 + 2a5x1x3 + 2a6x1x
2
3
)d.
(5.99)
Here, formula (5.99) deviates from that of equation (5.88) due to the additional
disturbance d. Using
v1 = −a1,0y1 + V1yref,1 = −a1,0x1 + V1yref,1,
v2 = −a2,1y˙2 − a2,0y2 + V2yref,2 = −a2,1x˙ 2 − a2,0x2 + V2yref,2
= −a2,1(a4x1x2 + a5x
2
1x3 + a6x
2
1x
2
3
) − a2,0x2 + V2yref,2
in the control law (5.98), we can impose any desired input-output dynamics
y˙1 + a1,0y1 = V1yref,1 − add,
y¨2 + a2,1y˙2 + a2,0y2 = V2yref,2 − ad(a4x2 + 2a5x1x3 + 2a6x1x
2
3
)d
on the system using the freely selectable parameters a1,0, a2,1, and a2,0. The
disturbance d affects both differential equations.
5.3 Full-State Linearization
5.3.1 Full-State Linearization of SISO Systems
We will begin by briefly recapitulating the basic concept of input-output lin￾earization from Section 5.2: by means of a suitable state-space transformation
and a state-dependent control variable transformation, which we can also in￾terpret as a controller, we had linearized the system behavior between input
and output. In general, however, we were not able to do this for the system’s
behavior as a whole; the internal dynamics, if present, remain nonlinear. For
the input-output linearization of SISO systems, we started with the system
x˙ = a(x) + b(x) · u,
y = c(x)
(5.100)
with the relative degree δ and calculated the derivatives
y˙ = Lac(x) + Lbc(x)
| {z }
0
·u,
y¨ = L
2
a
c(x) + LbLac(x)
| {z }
0
·u,
.
.
.
y
(δ−1) = L
δ−1
a c(x) + LbL
δ−2
a c(x)
| {z }
0
·u,
y
(δ) = L
δ
ac(x) + LbL
δ−1
a c(x)
| {z }
6= 0
·u,
(5.101)414 Chapter 5. Nonlinear Control of Nonlinear Systems
which enables us to linearize the input-output behavior. If the relative degree
is δ = n, we can use the diffeomorphism
z = t(x) =










c(x)
Lac(x)
L
2
a
c(x)
.
.
.
L
n−1
a
c(x)










(5.102)
to transform the system description into the nonlinear controller canonical
form
z˙ =







z2
.
.
.
zn
α(x) + β(x) · u







.
If
β(x) = LbL
δ−1
a
c(x) 6= 0
holds, the system can be transformed via the new input variable v and the
control variable transformation
u =
− α(x) + v
β(x)
into the Brunovsky canonical form
z˙ = Az + b · v,
where
A =










0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
0 0 0 · · · 0










, b =










0
0
.
.
.
0
1










.
Its block diagram is shown in Figure 5.16. The output variable y can be stated
as
y = c
T
z, c
T =
h
1 0 · · · 0
i
.
If the above transformations are feasible, which is the case if δ = n and
β(x) 6= 0, we can thus describe the nonlinear system using an equivalent
linear state-space model. The following definition summarizes this.5.3. Full-State Linearization 415
...
v zn zn−1 zn−2 z3 z2 z1
1
s
1
s
1
s
1
s
1
s
Fig. 5.16: Block diagram of the Brunovsky canonical form
Definition 34 (Full-State Linearizability). A system
x˙ = a(x) + b(x) · u, x ∈ Dx,def ⊆ IRn
, u ∈ IR,
is called full-state linearizable if a diffeomorphism z = t(x) and an input
transformation u = p(x, v) exist, so that the system can be transformed into
the Brunovsky canonical form.
If a system has the relative degree δ = n, it is full-state linearizable. How￾ever, if the relative order δ is less than the system order n, the transformation
to the Brunovsky canonical form and the linearization in general cannot be
performed completely for all states. This results in internal dynamics, and
these are usually nonlinear. Here the question arises whether we can achieve
full-state linearization with a different choice of the output variable y = c(x).
In this case, we must select c(x) such that δ = n holds, and we thus obtain a
diffeomorphism (5.102), i. e. an input-output linearization, which consequently
has no internal dynamics and therefore no nonlinear dynamic components.
The question of whether such a choice of y is possible is especially relevant to
systems without an output variable y = c(x): can we achieve linearization as
described above by choosing a suitable output variable y?
Undoubtedly, we can achieve this goal if we are able to find an artificial
output variable
y = λ(x)
so that the system has the relative degree δ = n. This is equivalent to the
requirements
Lbλ(x) = 0,
LbLaλ(x) = 0,
.
.
.
LbL
n−2
a λ(x) = 0,
(5.103)
and
LbL
n−1
a λ(x) 6= 0, (5.104)
as is clear if we consider the sequence of derivatives in equation (5.101).
Assuming that equations (5.103) and (5.104) are fulfilled, we can refer to
y = λ(x) as the linearizing output. This output does not necessarily have to
be of practical importance. To determine it, we must solve equations (5.103).416 Chapter 5. Nonlinear Control of Nonlinear Systems
These are homogeneous linear partial differential equations of higher order;
an example is
LbL
2
aλ(x) = ∂
∂x

∂
∂x

∂λ(x)
∂x
a(x)

a(x)

b(x) = 0.
We can simplify the differential equations (5.103) by converting them into
homogeneous linear partial differential equations of the first-order.
For this purpose, we need the Lie bracket
[f, g] = ∂g(x)
∂x
f(x) −
∂f(x)
∂x
g(x)
of two vector functions f(x) and g(x). The Lie bracket is a special derivative
operator for vector functions, which we have already encountered in Section
3.1.5 on p. 228. If we nest Lie brackets, we can abbreviate this by writing
adi
f g = [f, adi−1
f
g] with ad0
f g = g for i = 1, 2, . . . .
This leads to
ad0
f g = g,
adf g = [f, g],
ad2
f g = [f, [f, g]],
ad3
f g = [f, [f, [f, g]]],
.
.
.
The ad operator will make our calculations below much simpler. In addition
to this operator and the Lie bracket, we also need the identity
[6]
Ladf gh(x) = L[f,g]h(x) = LfLgh(x) − LgLf h(x) (5.105)
of a function h(x).
So equipped, we can achieve our goal to transform equation (5.103) into a
system of homogeneous linear partial differential equations of the first order.
Using the identity (5.105) in the form
Lada bλ(x) = LaLbλ(x) − LbLaλ(x),
and
[6] This identity is sometimes called the Jacobi identity [422]. However, the common
form of the Jacobi identity is
[f, [g,h] + [h, [f, g]] + [g, [h, f]]] = 0.
It can be derived from the identity (5.105).5.3. Full-State Linearization 417
Lbλ(x) = 0,
LbLaλ(x) = 0,
which results from equation (5.103), we obtain
Lada bλ(x) = 0.
Again using the identity (5.105), we then compute
Lad2
a bλ(x) = L[a,[a,b]]λ(x) = LaL[a,b]λ(x) − L[a,b]Laλ(x)
= La(LaLbλ(x) − LbLaλ(x)) − (LaLbLaλ(x) − LbLaLaλ(x))
= L
2
aLbλ(x) − 2LaLbLaλ(x) + LbL
2
aλ(x).
Inserting
Lbλ(x) = 0,
LbLaλ(x) = 0,
LbL
2
aλ(x) = 0
into the latter, we then obtain
Lad2
a bλ(x) = 0.
This calculation can be continued with
Lad3
a bλ(x), Lad4
a bλ(x), . . .
As an equivalent condition to equation (5.103), we then obtain the desired
system of homogeneous first-order linear partial differential equations
Lbλ(x) = ∂λ(x)
∂x
b = 0,
Lada bλ(x) = ∂λ(x)
∂x
[a, b] = 0,
Lad2
a bλ(x) = ∂λ(x)
∂x
[a, [a, b]] = 0,
.
.
.
Ladn−2
a b
λ(x) = ∂λ(x)
∂x
[a, [a, . . . [a, b] . . .]] = 0
(5.106)
and
Ladn−1
a b
λ(x) = (−1)n−1LbL
n−1
a λ(x) 6= 0 (5.107)
for equation (5.104). We have shown so far that a solution of equations (5.103),
(5.104) always fulfills equations (5.106), (5.107) as well. The reverse conclusion418 Chapter 5. Nonlinear Control of Nonlinear Systems
is also correct [203, 422, 465]. Thus, equations (5.103), (5.104) and equations
(5.106), (5.107) are equivalent.
For us to solve the partial differential equations (5.106), it is very useful if
they are linear and of the first order. In practice, however, these differential
equations are often not simple to solve. In addition, their solutions are often
very complex and extremely lengthy when the system (5.100) is of higher
order.
We can examine the solvability of the above system of partial differential
equations (5.106) as well as the fulfillment of condition (5.107) and the asso￾ciated full-state linearizability [197, 198, 203, 206, 232], i. e. the existence of a
linearizing output with the relative degree δ = n, using
Theorem 64 (Full-State Linearizability of SISO Systems). A con￾trol-affine system x˙ = a(x) + b(x) · u with x ∈ Dx,def ⊆ IRn
and u ∈ IR is
full-state linearizable if and only if
(1) rank(h
b adab ad2
a
b · · · adn−1
a
b
i
) = n holds and
(2) all Lie brackets
[adi
a
b, adk
a
b] with i, k ∈ {0, 1, . . . , n − 2}
are linear combinations of the vectors
b, ada b, . . . , adn−2
a
b.
The set of vector functions {b, ada b, ad2
a b, . . . , adn−2
a b} is termed involutive
if the functions satisfy Condition (2) of the theorem. Note that the conditions
of Theorem 64 do not depend on an artificial output y = λ(x).
Theorem 64 provides a statement about the full-state linearizability of the
system
x˙ = a(x) + b(x) · u,
but it is not constructive. That is, it does not provide us with a procedure for
determining a linearizing output y = λ(x). Summarizing our results above, we
obtain the following theorem which enables us to determine a linearizing out￾put and the diffeomorphism needed to transform a system into the Brunovsky
canonical form.
Theorem 65 (Full-State Linearization of SISO Systems). A full-state
linearizable system x˙ = a(x) + b(x) · u with x ∈ Dx,def ⊆ IRn
and u ∈ IR is
transformed into the Brunovsky canonical form by the diffeomorphism
z = t(x) =










λ(x)
Laλ(x)
L
2
aλ(x)
.
.
.
L
n−1
a λ(x)









5.3. Full-State Linearization 419
and the input variable transformation
u =
v − L
n
aλ(x)
LbL
n−1
a λ(x)
with the new input variable v if the artificial output y = λ(x) fulfills the partial
differential equations
Lbλ(x) = 0,
Lada bλ(x) = 0,
Lad2
a bλ(x) = 0,
.
.
.
Ladn−2
a b
λ(x) = 0
and the inequality
LbL
n−1
a λ(x) 6= 0.
Note that the requirement stated in equation (5.107) and the last equation in
Theorem 65 are equivalent. Furthermore, the conditions of Theorem 65 can
always be fulfilled, since it presupposes full-state linearizability.
As mentioned above, verifying the full-state linearizability of higher order
systems using Theorem 64 can be very complex and often futile. In particular,
Condition (2) from Theorem 64, the involution condition, restricts the number
of full-state linearizable systems and is often time-consuming to verify.
If we have succeeded in transforming a full-state linearizable system into
the Brunovsky canonical form, we can immediately design a linear control
for the transformed system. This is especially simple because the Brunovsky
canonical form consists of a chain of n integrators.
5.3.2 Example: Drilling Rig
Deep boreholes in the earth’s crust are mainly used to access oil, gas, and
geothermal sources. The drilling technique is similar in all cases. The rotary
drilling technique uses a drilling rig, at the lower end of which a revolving drill
bit is installed as shown in Figure 5.17. A drilling fluid is pumped through the
interior of the drill pipe to the drill bit. This liquid cools and lubricates the drill
bit. At the same time, the liquid streams up into the borehole transporting
the milled rock to the surface. It is flushed up in the annular gap between
the drill pipes and the borehole wall. The borehole wall is covered with steel
pipes so it does not collapse. The drill pipe is turned by a carrier rod by
means of a rotary table driven by an electric motor. The rotary table has a
large flywheel mass in order to ensure that the speed of the drill pipe is as420 Chapter 5. Nonlinear Control of Nonlinear Systems
constant as possible. From time to time, extensions approximately 10 m in
length must be added to the line of drill rods.
Such boreholes typically reach depths between 3000 m and 5000 m. At
these drilling depths, the drill string will twist. Therefore, the angle Θ1 at
the top of the drill pipe and the angle Θ2 at its end can differ considerably.
For example, at a depth of 3500 m, the drill pipe will twist 360 degrees about
twenty times. Since the drill pipe acts as a torsion spring, torsional vibrations
of the drill head can occur. These vibrations are triggered by stick-slip friction
Fig. 5.17: Drilling rig5.3. Full-State Linearization 421
on the drill bit and are undesirable because they harm the bit and the drill
pipe and, in the worst case, can cause parts of the drill string to shear off.
Therefore, vibrations of this kind are suppressed by applying a control system
[2, 69, 412].
We can approximately model the drill pipe with two flywheels connected
by a torsion spring, as shown in Figure 5.18. The flywheels’ movements are
viscously damped by the drilling fluid, where d1 and d2 are the damping
constants. Here Mdrive is the driving torque at the rig. The load torque Mload
at the bit is caused by the stick-slip friction on the rock during the milling
process. The torque caused by the friction can be modeled by
Mfr(Θ˙
2) = 2
π
Mc(k1Θ˙
2e
−k2|Θ˙
2| + arctan(k3Θ˙
2))
with Mc = 0.5 kN m, k1 = 9.5 s rad−1
, k2 = 2.2 s rad−1
, and k3 = 35.0 s rad−1
.
Furthermore, an unknown, time-dependent torque Md(t) is added to the fric￾tion torque to formulate the load torque
Mload = Mfr(Θ˙
2) + Md(t).
We view the torque Md(t) as a disturbance which depends on the type of
rock, the temperature of the drill bit, etc. Figure 5.19 shows the curve of the
stick-slip friction Mfr(Θ˙
2).
The equations of motion can be stated as
J1Θ¨
1 = −c(Θ1 − Θ2) − d1Θ˙
1 + Mdrive,
J2Θ¨
2 = c(Θ1 − Θ2) − d2Θ˙
2 − Mfr(Θ˙
2) − Md(t),
(5.108)
where J1 summarizes the moments of inertia of the rotary table, the electric
motor, and its gear, and J2 summarizes the moment of inertia of the lower
drill pipe and that of the bit. The term
c(Θ1 − Θ2)
describes the spring force, where c = 473 N m rad−1
is the spring constant.
The viscous friction forces
−d1Θ˙
1 and − d2Θ˙
2
have the damping constants d1 = 425 N m s rad−1
and d2 = 50 N m s rad−1
,
respectively. The moments of inertia take on the values J1 = 2122 kg m2
and
J2 = 374 kg m2
.
With
x =
h
Θ˙
1 Θ1− Θ2 Θ˙
2
iT
as the state vector and the driving torque u = Mdrive as the input variable,
we obtain the model422 Chapter 5. Nonlinear Control of Nonlinear Systems
d1
d2
Θ2, −Mload
Θ1, Mdrive
J1
J2
c
Fig. 5.18: Simplified and abstracted model of the drill pipe with two concentrated
masses, their moments of inertia J1 and J2, and a torsion spring with spring constant
c
Θ˙
2
Mfr
Fig. 5.19: Curve of the stick-slip friction Mfr(Θ˙
2)5.3. Full-State Linearization 423
x˙ = a(x) + b · u + s · Md (5.109)
from equation (5.108), where the disturbance is denoted by Md(t) and
a(x) =




−a2x1 − a1x2
x1 − x3
a3x2 − a4x3 −a5x3e
−k2|x3| − a6 arctan(k3x3)
| {z }




,
− sMfr(x3)
b =




b
0
0




, s =




0
0
−s




.
The system’s parameters are
a1 =
c
J1
, a2 =
d1
J1
, a3 =
c
J2
, a4 =
d2
J2
,
a5 =
2Mck1
πJ2
, a6 =
2Mc
πJ2
, b =
1
J1
, s =
1
J2
.
To determine whether the system (5.109) is full-state linearizable, we will
apply Theorem 64 and begin by calculating
ada b = [a, b] = ∂b
∂x
a −
∂a
∂x
b
= −




−a2 −a1 0
1 0 −1
0 a3 −a4 − sM′
fr(x3)








b
0
0




=




a2b
−b
0




,
ad2
a
b = [a, [a, b]] = ∂[a, b]
∂x
a −
∂a
∂x
[a, b]
= −




−a2 −a1 0
1 0 −1
0 a3 −a4 − sM′
fr(x3)








a2b
−b
0




=




a
2
2
b − a1b
−a2b
a3b




,
where
M′
fr(x3) = ∂Mfr(x3)
∂x3
holds. This yields
rank([b ada b ad2
a
b]) = rank


b




1 a2 a
2
2 − a1
0 −1 −a2
0 0 a3l






= 3,424 Chapter 5. Nonlinear Control of Nonlinear Systems
and Condition (1) of Theorem 64 is fulfilled. Since the vectors b, ada b, and
ad2
a b are constant, all Lie brackets result in
[adi
a b, adk
a b] =




0
0
0




for i, k ∈ {0, 1},
i. e. they are trivial linear combinations of the vectors b and ada b. Thus
Condition (2) of Theorem 64 is also fulfilled and the system can be full-state
linearized.
It is interesting to note that in the case of a disappearing spring constant
c, i. e. where a3 = 0, the drilling system is no longer full-state linearizable.
In this unrealistic case, the spring is omitted and the connection between the
flywheels is broken. Then the control variable u has no influence on the state
x3, which is why the plant is not controllable and therefore is not full-state
linearizable.
We can use the partial differential equations
Lbλ(x) = ∂λ
∂x
b =
∂λ
∂x1
b = 0,
Lada bλ(x) = ∂λ
∂x
ada b =
∂λ
∂x1
a2b −
∂λ
∂x2
b = 0
(5.110)
from Theorem 65 to determine the linearizing output y = λ(x). In this case
the task is straightforward, because
y = λ(x) = x3
obviously satisfies the differential equations (5.110).
Thus, the diffeomorphism
z = t(x) =




λ(x)
Laλ(x)
L
2
aλ(x)




which transforms the system (5.109) into the Brunovsky canonical form can
be formulated as
z = t(x) =




x3
a3x2 − a4x3 − sMfr(x3)
a3(x1 − x3) − (a4 + sM′
fr(x3))(a3x2 − a4x3 − sMfr(x3))




.
The related retransformation is5.3. Full-State Linearization 425
x = t
−1
(z) =







1
a3
[z3 + (a4 + sM′
fr(z1)) z2] + z1
1
a3
[z2 + a4z1 + sMfr(z1)]
z1







.
By differentiating the equation z = t(x) with respect to time, we obtain the
transformed system representation, whose general form is already known to
us from Section 3.3.1 on p. 260, supplemented by the perturbation Md(t), as
z˙ =
∂t(x)
∂x
x˙ =
∂t(x)
∂x




x=t−1(z)
·
￾
a(t
−1
(z)) + b(t
−1
(z)) · u + s(t
−1
(z)) · Md

,
where
∂t(x)
∂x




x=t−1(z)
=




0 0 1
0 a3 −(a4 + sM′
fr(z1))
a3 −a3(a4+sM′
fr(z1)) −a3+(a4+sM′
fr(z1))2−sM′′
fr(z1)z2




is the Jacobi matrix of the diffeomorphism t(x). Here the abbreviation
M′′
fr(z1) = ∂
2M′
fr(z1)
∂z2
1
is used. Thus, we obtain
z˙ =




z2
z3
−(a1a4 + a2a3)z1 − (a1 + a2a4 + a3)z2 − (a2 + a4)z3 − h(z)




+




0
0
ba3




u
+
∂t(x)
∂x





x=t−1(z)
· s(t
−1
(z)) · Md (5.111)
for the transformed system, where
h(z) = a1sMfr(z1) + (z3 + a2z2)sM′
fr(z1) + sz2
2M′′
fr(z1).
Using the new input variable v and applying
u =
(a1a4 + a2a3)z1 + (a1 + a2a4 + a3)z2 + (a2 + a4)z3 + h(z)
ba3
+
v
ba3
,
we can transform system (5.111) into the Brunovsky canonical form. So we
obtai426 Chapter 5. Nonlinear Control of Nonlinear Systems
z˙ =




0 1 0
0 0 1
0 0 0




z +




0
0
1




v + s




−1
a4 + sM′
fr(z1)
a3−(a4+sM′
fr(z1))2+sM′′
fr(z1)z2




Md.
The nonlinear term represents the disturbance.
5.3.3 Full-State Linearization of MIMO Systems
We can extend Definition 34 of full-state linearizability to control-affine MIMO
systems
x˙ = a(x) +Xm
i=1
bi(x) · ui
. (5.112)
As in the SISO case, system (5.112) is referred to as full-state linearizable if
a diffeomorphism z = t(x) and its input variable transformation u = p(x, v)
exist, so that the system can be transformed into the MIMO Brunovsky canon￾ical form
x˙ =







A1 0 · · · 0
0 A2 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · Am







x +







e1 0 · · · 0
0 e2 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · em







v
with the δi × δi matrices Ai and the δi - dimensional vectors ei of the form
Ai =










0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
0 0 0 · · · 0










and ei =










0
0
.
.
.
0
1










.
Then [δ1 · · · δm] forms the vector relative degree of the system. The total
relative degree is δ = δ1 + . . . + δm = n.
Similarly to the SISO case, the following applies with regard to lineariz￾ability [197, 203, 206, 402]:
Theorem 66 (Full-State Linearizability of MIMO Systems). A control￾affine system
x˙ = a(x) +Xm
i=1
bi(x) · ui
with x ∈ Dx,def ⊆ IRn
, ui ∈ IR, i = 1, . . . , m, and a matrix M0 =
h
b1 · · · bm
i
of rank m can be full-state linearized if and only if5.3. Full-State Linearization 427
(1) all matrices
M1 =
h
b1 · · · bm ada b1 · · · ada bm
i
,
.
.
.
Mn−2 =
h
b1 · · · bm ada b1 · · · ada bm · · · ad n−2
a b1 · · · ad n−2
a bm
i
are of constant rank,
(2) the matrix
Mn−1 =
h
b1 · · · bm ada b1 · · · ada bm · · · ad n−1
a
b1 · · · ad n−1
a
bm
i
is of rank n, and
(3) all sets
n
b1, . . . , bm
o
,
n
b1, . . . , bm, adab1, . . . , adabm
o
,
.
.
.
n
b1, . . . , bm, ada b1, . . . , adabm, . . . , ad n−2
a
b1, . . . , ad n−2
a
bm
o
are involutive.
Recall that a set
{h1(x), h2(x), . . . , hm(x)}
of vector functions hk(x) is referred to as involutive if the Lie brackets
[hi
, hj ] , i, j ∈ {1, . . . , m}
of all possible combinations (i, j) can be represented as linear combinations
h
hi
, hj
i
=
Xm
k=1
αk(x)hk(x)
of the functions hk(x). The coefficients αk(x) are scalar functions which must
be appropriately determined.
As in Theorem 64, which deals with the SISO case, Theorem 66 is not
constructive. However, we can once again derive a theorem in a way similar
to the SISO case, one which allows us to calculate the linearizing outputs
λ1(x), . . . , λm(x) and the associated diffeomorphism. This is428 Chapter 5. Nonlinear Control of Nonlinear Systems
Theorem 67 (Full-State Linearization of MIMO Systems). A full￾state linearizable system
x˙ = a(x) +Xm
i=1
bi(x) · ui
with x ∈ Dx,def ⊆ IRn
and ui ∈ IR, i = 1, . . . , m, is transformed into the
Brunovsky canonical form by means of the diffeomorphism
z = t(x) =























λ1(x)
Laλ1(x)
.
.
.
L
δ1−1
a λ1(x)
.
.
.
λm(x)
Laλm(x)
.
.
.
L
δm−1
a λm(x)























with δ = δ1 + δ2 + . . . + δm = n, and the input variable transformation
u = −D−1
(x) · (˚c(x) − v)
with the new input variable vector v and the associated decoupling matrix D,
stated as
D(x) =







Lb1 L
δ1−1
a λ1(x) · · · LbmL
δ1−1
a λ1(x)
Lb1 L
δ2−1
a λ2(x) · · · LbmL
δ2−1
a λ2(x)
.
.
.
.
.
.
.
.
.
Lb1 L
δm−1
a λm(x) · · · LbmL
δm−1
a λm(x)







, ˚c(x) =







L
δ1
a λ1(x)
L
δ2
a λ2(x)
.
.
.
L
δm
a λm(x)







,
if the artificial output variable vector λ = [λ1 · · · λm]
T fulfills the partial dif￾ferential equations
Lbiλj = 0,
Lada biλj = 0,
Lad2
a bi
λj = 0,
.
.
.
L
ad
δj−2
a bi
λj = 0
for all i = 1, . . . , m and all j = 1, . . . , m, and the matrix D(x) is regular.5.3. Full-State Linearization 429
Since the control-affine system in the above theorem is assumed to be full￾state linearizable, δ = n holds for the vector relative degree [δ1 · · · δm] of the
system. To carry out a full-state linearization, the numbers δ1, . . . , δm must
be calculated in advance to determine the linearizing output variable vector
λ = [λ1 · · · λm]
T. This is possible due to the following [272, 305]:
Theorem 68 (Vector Relative Degree). A full-state linearizable system
x˙ = a(x) +Xm
i=1
bi(x) · ui
with x ∈ Dx,def ⊆ IRn
and a matrix M0 =
h
b1 · · · bm
i
of rank m possesses
an artificial output variable vector λ(x) with the vector relative degree
δ =
h
δ1 · · · δm
i
with δ1 + . . . + δm = n
where a value δi, i = 1, . . . , m, is equal to the number of values rk ≥ i,
k = 0, . . . , n − 1, and
r0 = rank(M0) = m,
r1 = rank(M1) − rank(M0),
.
.
.
rn−1 = rank(Mn−1) − rank(Mn−2)
holds.
Note that it is not necessary to know the artificial output variable vector
λ(x) when applying Theorem 68. A direct result of the theorem above is the
sequence
δ1 ≥ δ2 ≥ · · · ≥ δm.
To perform a full-state linearization of a specific system, we follow the proce￾dure below:
Step 1: Using Theorem 66, check whether the system is full-state lineariz￾able.
Step 2: Based on Theorem 68, determine the vector relative degree δ.
Step 3: Solve the partial differential equations from Theorem 67 and obtain
the linearizing output variable vector λ.
Step 4: By means of the output variable vector λ, determine the diffeomor￾phism z = t(x) from Theorem 67 and the input variable transfor￾mation u = −D−1
(x) · (˚c(x) − v).
Step 3 is the most difficult and complex step, because the partial differential430 Chapter 5. Nonlinear Control of Nonlinear Systems
equations can often be solved only with great effort, as is the case for SISO
systems.
Theorems 64 and 66 provide us with criteria we can use to determine
whether a control-affine system can be full-state linearized and thus trans￾formed into nonlinear controller canonical form. From Theorem 45, p. 218,
we know that systems in nonlinear controller canonical form are always con￾trollable. Therefore, Theorems 64 and 66 also enable us to determine the
controllability of a control-affine system.
5.3.4 Flatness of Full-State Linearizable Systems
From Theorem 51 on p. 257, we know that SISO systems x˙ = f(x, u) are
flat if and only if they are in nonlinear controller canonical form, or if they
are transformable into such a form, and if β(x) 6= 0 holds. In this case, a flat
or fictitious flat output of the nonlinear controller canonical form is y = x1.
Furthermore, we know from Section 5.3.1 that control-affine systems
x˙ = a(x) + b(x) · u (5.113)
can be full-state linearized if and only if they can be transformed into the
nonlinear controller canonical form and β(x) 6= 0 applies. Thus we obtain
Theorem 69 (Flatness of Full-State Linearizable SISO Systems). A
system
x˙ = a(x) + b(x) · u, x ∈ Dx,def ⊆ IRn
, u ∈ IR,
is flat if and only if it is full-state linearizable. Furthermore, if the system has
the relative degree n for an output y, it follows that y is a flat output.
Therefore the conditions from Theorem 64 on p. 418, which are sufficient and
necessary for the full-state linearizability of control-affine SISO systems, are
also sufficient and necessary for the flatness of these systems. At this point,
we remember that the properties of flatness and state linearizability may not
hold for all values x ∈ Dx,def and, in such cases, are only locally valid.
The flat system representation of a control-affine SISO system with the
output variable y = c(x) and relative degree δ = n can be derived using the
diffeomorphism
z = t(x) =







c(x)
Lac(x)
.
.
.
L
n−1
a
c(x)







=







y
y˙
.
.
.
y
(n−1)







. (5.114)
Thus, the inverse function provides the first equation5.3. Full-State Linearization 431
x = t
−1
(z) = t
−1

y, y, . . . , y ˙
(n−1)
= Ψ1

y, y, . . . , y ˙
(n−1) (5.115)
of the flat system representation.
For the calculation of u as a function of y and its derivatives, we will now
transform the system (5.113) by differentiating the diffeomorphism (5.114)
into the form
z˙ =










y˙
y¨
.
.
.
y
(n−1)
y
(n)










=










z2
z3
.
.
.
zn
L
n
ac(x) + LbL
n−1
a c(x) · u










.
This results in
u =
y
(n) − L
n
a
c(x)
LbL
n−1
a c(x)
=
y
(n) − L
n
ac
￾
t
−1
￾
y, y, . . . , y ˙
(n−1)
LbL
n−1
a c
￾
t
−1
￾
y, y, . . . , y ˙
(n−1)
= Ψ2

y, y, . . . , y ˙
(n)

.
(5.116)
Therefore, with equations (5.115) and (5.116), we have obtained the flat sys￾tem representation.
In the case of control-affine SISO systems, the property of flatness does not
reveal any significant new findings. This is because the control-affine system
is flat if it can be full-state linearized and vice versa. In particular, as shown
above, the flat system representation can be determined directly from the
solution of the full-state linearization problem.
However, in the case of control-affine MIMO systems, flatness and state
linearizability are no longer equivalent to each other, as is the case for SISO
systems. This is because the following theorem, which we can readily derive
from the nonlinear controller canonical form and Theorem 51 on p. 257, applies
merely sufficiently.
Theorem 70 (Flatness of Full-State Linearizable MIMO Systems).
A system
x˙ = a(x) +Xm
i=1
bi(x) · ui
, x ∈ Dx,def ⊆ IRn
, ui ∈ IR, i = 1, . . . , m,
is flat if it is full-state linearizable. If the system has the total relative degree
n for an output y, then y is a flat output.
Consequently, it may be that a control-affine MIMO system cannot be full￾state linearized, but is flat neverthel432 Chapter 5. Nonlinear Control of Nonlinear Systems
5.3.5 Example: Rocket
A rocket [129, 420] is an example of a flat system which is not full-state
linearizable. As shown in Figure 5.20, the rocket is placed in a fixed reference
coordinate system with specified x- and z-coordinates for the rocket’s center
of gravity S. Its mass is referred to as m and the acceleration due to gravity
is referred to as g. The rocket has a swiveling liquid-fuel propulsion whose
thrust we can regulate. As a result, we have propulsive forces Fx and Fz in
the directions x and z. We will now calculate the accelerations
x¨ =
Fx
m
,
z¨ =
Fz
m
− g,
taking into account the earth’s gravitational force.
The balance-of-torques equation can be formulated as
JΘ¨ = Fzd sin(Θ) − Fxd cos(Θ),
where d is the lever arm acted on by the forces Fx and Fz, meaning d is the
distance between the forces’ point of application and the center of gravity S,
and Θ is the flight angle.
With the state vector
h
x1 x2 x3 x4 x5 x6
i
=

x
g
x˙
g
z
g
z˙
g
Θ Θ˙

,
the input variables
u1 =
Fx
mg
, u2 =
Fz
mg
− 1,
and the parameter
ε =
J
mgd,
we obtain the rocket model
x˙ =













x2
0
x4
0
x6
sin (x5)
ε













| {z }
a
+













0
1
0
0
0
−
cos (x5)
ε













| {z }
b1
u1 +













0
0
0
1
0
sin (x5)
ε













| {z }
b2
u2. (5.117)5.3. Full-State Linearization 433
g
z
x
Fz
Fx
d
S
Θ
Fig. 5.20: Rocket with propulsion forces Fx and Fz and inclination angle Θ
To analyze the full-state linearizability of the system, we will apply Theo￾rem 66 and show that its third condition is not fulfilled. To this end, we first
determine the set of input vectors b1, b2 and the two Lie brackets
[a, b1] = ada b1 and [a, b2] = ada b2,
i. e.
n
b1, b2, ada b1, ada b2
o
=


















0
1
0
0
0
−
cos(x5)
ε















,















0
0
0
1
0
sin(x5)
ε















,















−1
0
0
0
cos(x5)
ε
x6 sin(x5)
ε















,















0
0
−1
0
−
sin(x5)
ε
x6 cos(x5)
ε


















.
(5.118)434 Chapter 5. Nonlinear Control of Nonlinear Systems
Now we will determine whether the set (5.118) is involutive. For this, we will
first calculate the Lie bracket
h
ada b1, b2
i
=














0
0
0
0
0
cos2
(x5) − sin2
(x5)
ε
2














. (5.119)
For the set (5.118) to be involutive, i. e. for Condition (3) of Theorem 66 to be
fulfilled, among other factors, the vector (5.119) must be a linear combination
h
adab1, b2
i
= α1(x)·b1(x)+α2(x)·b2(x)+α3(x)·ada b1(x)+α4(x)·ada b2(x)
(5.120)
of the vectors of the set (5.118). Obviously, αi(x) = 0 must hold for all αi
so that the first five elements of vector (5.119) are equal to zero. As a result,
the sixth element of vector (5.119) should also be zero, which is not the case.
Therefore, we cannot fulfill equation (5.120) and consequently vector (5.119)
is not a linear combination of the vectors of set (5.118); thus, this set is not
involutive. This means that Condition (3) of Theorem 66 is not fulfilled and
the rocket model cannot be full-state linearized.
But it is flat, as already mentioned. We can prove this using the output
y =
"
y1
y2
#
=
"
x1 + ε sin(x5)
x3 + ε cos(x5)
#
. (5.121)
We can now calculate
y¨1 = sin(x5) ·

u1 sin(x5) + (1 + u2) cos(x5) − εx˙
2
5

, (5.122)
y¨2 = cos(x5) ·

u1 sin(x5) + (1 + u2) cos(x5) − εx˙
2
5

− 1. (5.123)
Equation (5.122) and (5.123) yield
y¨1
y¨2 + 1
= tan(x5)
or
x5 = arctan 
y¨1
y¨2 + 1
= Ψ1,5 (y, y˙, y¨). (5.124)
Inserting equation (5.124) into equation (5.121) results in5.4. Feedforward and Feedback Control of Flat Systems 435
x1 = y1 − q
εy¨1
y¨
2
1 + (¨y2 + 1)2
= Ψ1,1 (y, y˙, y¨), (5.125)
x3 = y2 −
ε(¨y2 + 1)
q
y¨
2
1 + (¨y2 + 1)2
= Ψ1,3 (y, y˙, y¨). (5.126)
We have thus determined three elements of the function Ψ1. The missing
coordinate representations of the flat system representation
x2 = ˙x1 = Ψ1,2 (y, y˙, y¨,˙˙˙ y ),
x4 = ˙x3 = Ψ1,4 (y, y˙, y¨,˙˙˙ y ),
x6 = ˙x5 = Ψ1,6 (y, y˙, y¨,˙˙˙ y )
are obtained by differentiating equations (5.124), (5.125), and (5.126).
To calculate the function u = Ψ2(y, y˙, . . . , y
(4)) of the flat system repre￾sentation, we will use equation (5.122) and the relation
x¨5 =
1
ε
sin(x5) −
1
ε
cos(x5)u1 +
1
ε
sin(x5)u2,
which results from equation (5.117), yielding the linear system of equations



y¨1
sin(x5)
+ εx˙
2
5 − cos(x5)
εx¨5 − sin(x5)


 =


sin(x5) cos(x5)
− cos(x5) sin(x5)




u1
u2

.
From this, we can determine the control vector as follows:


u1
u2

 =


sin(x5) − cos(x5)
cos(x5) sin(x5)





y¨1
sin(x5)
+ εx˙
2
5 − cos(x5)
εx¨5 − sin(x5)


.
Inserting equation (5.124) yields the function u = Ψ2(y, y˙, . . . , y
(4)) which
we are looking for. Thus, we have proven that the system is flat.
5.4 Feedforward and Feedback Control of Flat Systems
5.4.1 Fundamentals
Based on the statements about flatness in Section 3.2 on p. 239 et seq., we
will aim to design feedforward and feedback controllers for flat systems. The
design of a feedforward controller for a flat system
x˙ = f(x,u),
y = g(x)436 Chapter 5. Nonlinear Control of Nonlinear Systems
is obvious and immediately feasible if the real output
y = g(x) = h(x)
is flat; from now on, we will write y = h(x) for the output function if y is a
flat output and otherwise we will formulate it as y = g(x). If y is flat, the
flat system representation
x = Ψ1(y, y˙, . . . , y
(β)
),
u = Ψ2(y, y˙, . . . , y
(β+1)) (5.127)
provides us with the feedforward control law (5.127) in dependence on the out￾put variable vector y and its derivatives. Figure 5.21 shows the corresponding
block diagram, where the reference variable is yref.
If a reference output signal yref(t) is predefined, i. e. a signal yref(t) and
its derivatives
y˙ ref(t), . . . , y
(β+1)
ref (t)
in equation (5.127) are predetermined, and an exact model which is unaffected
by a disturbance exists, the identity y(t) = yref(t) holds for the system’s
output signal. Note that not all reference variable signals yref(t) are suitable.
In fact, they must be differentiable at least β+1 times. Thus, reference variable
steps are not allowed.
In practice, it will rarely or never be the case that an exact model exists
and that there is an absence of disturbances. This means the output value y(t)
will deviate from the reference value yref(t). To solve this problem, a feedback
control is usually superimposed on the feedforward control, as shown in Figure
5.22.
The feedback control eliminates disturbances affecting the plant and also
compensates for the effects of discrepancies between the real process and the
model on which the flat system representation is based. Feedback control is
particularly important in the case of unstable plants. In the simplest case,
PID controllers can be used. When complex nonlinearities occur, the use of
nonlinear controllers such as gain-scheduling controllers may be necessary.
So far, we have only examined the case of a real flat output y, in which the
design of a flat feedforward controller is obvious. However, if the real output
y = g(x) is not flat, and we must use a fictitious flat output yf = h(x) for
the control, the design of a controller of this type is usually more complex.
This is discussed in the following section.
u = Ψ2(yref, y˙
ref, . . . , y
(β+1)
ref )
x˙ = f(x, u)
y = h(x)
yref u y
Fig. 5.21: Feedforward control of a flat system5.4. Feedforward and Feedback Control of Flat Systems 437
Feedback
controller
x˙ = f(x, u)
y = h(x)
Feedforward
controller
uf =Ψ2(yref, . . .)
yref
e y
uf
uc u
Fig. 5.22: Flatness-based feedforward and feedback control of a flat system
5.4.2 Feedforward Controls Using Fictitious Flat Outputs
Usually, for a flatness-based control, we use the reference signal yref(t) of the
real output variable y(t) as an input variable. If the real output y is not
flat, we must convert it into a fictitious flat output yf or vice versa. In the
following, we assume this situation and that we know a fictitious flat output
yf
.
By inserting the flat system equation
x = Ψ1(yf
, y˙f
, . . . , y
(β)
f
) (5.128)
into the equation y = g(x) for the system’s output, we obtain the desired
conversion rule between yf
and y:
y = g

Ψ1

yf
, y˙f
, . . . , y
(β)
f

= Γ

yf
, y˙f
, . . . , y
(γ)
f

,
(5.129)
with γ ≤ β. The identity γ = n − δ holds in the SISO case, where δ denotes
the relative degree of the system [169]. We can use the above equation (5.129),
which is an implicit differential equation, in different ways. The obvious pos￾sibility is to specify the real output signal y(t) and to solve the differential
equation (5.129) to calculate the progression of the fictitious flat output y
(γ)
f
.
This is usually a doable task if we can convert the differential equation (5.129)
such that y
(γ)
f
is explicitly given. If this is not possible, the differential equa￾tion is implicit in y
(γ)
f
, and calculating the fictitious flat output yf by solving
the differential equation is often tricky.
In general, we can choose any set of initial values
yf(0), y˙f(0), . . . , y
(γ−1)
f
(0).
For example, we can set all these values at zero and then use the progression
yf(t) of the flat output to calculate the control variable438 Chapter 5. Nonlinear Control of Nonlinear Systems
u = Ψ2

yf
, y˙f
, . . . , y
(β+1)
f

.
If we wish to specify an initial vector x(0) in addition to the signal y(t), we
must select the initial values
yf(0), y˙f(0), . . . , y
(β)
f
(0)
so that the relation
x(0) = Ψ1

yf(0), y˙f(0), . . . , y
(β)
f
(0)
(5.130)
resulting from equation (5.128) is fulfilled. In the SISO case, β = n−1 applies.
Therefore, we have n values
yf(0), . . . , y
(n−1)
f
(0),
i. e. the same number as states xi(0). Furthermore, a flat SISO-system is full￾state linearizable using equation (5.128), which is the diffeomorphism required
for this purpose. This means that the inverse function Ψ
−1
1
always exists in
the SISO case. The desired initial values
yf(0), y˙f(0), . . . , y
(n−1)
f
(0)
then result from the equation







yf(0)
y˙f(0)
.
.
.
y
(n−1)
f
(0)







= Ψ
−1
1
(x(0)).
In the MIMO case, there may be more initial values y
(j)
f,i (0), i = 1, . . . , r and
j = 0, . . . , β than initial states xi(0). In this case, the system of equations
(5.130) may be underdetermined.
Let us now examine how we can determine the fictitious flat output signal
yf(t) and its derivatives
y˙f(t), . . . , y
(γ)
f
(t)
from a given real output signal y(t) using equation (5.129). As already stated,
equation (5.129) is a differential equation in implicit form, i. e. the highest
order derivative y
(γ)
f
is not explicitly given. However, if equation (5.129) can
be transformed into the explicit form
y
(γ)
f = Γ
−1

yf
, y˙f
, . . . , y
(γ−1)
f
, y

, (5.131)5.4. Feedforward and Feedback Control of Flat Systems 439
we can solve this differential equation of order γ using standard numerical
methods, such as the Runge-Kutta method, after the equation has been trans￾formed into a first-order system of differential equations
z˙ =










z˙1
z˙2
.
.
.
z˙γ−1
z˙γ










=










z2
z3
.
.
.
zγ
Γ
−1
(z1, . . . , zγ, y)










, z =










yf
y˙f
y¨f
.
.
.
y
(γ−1)
f










.
By specifying the real output signal y(t), we can then determine the pro￾gression of the fictitious flat output yf(t). Subsequently inserting yf
into the
equation
u = Ψ2

yf
, y˙f
, . . . , y
(β+1)
f

(5.132)
yields the desired feedforward controller. If the differential equation (5.129) is
unstable, the calculation of the control variable u using equation (5.132) can
lead to problems. Very high values yf
(t) and associated numerical difficulties
can occur for the time interval [0, T ] in question. Furthermore, very high and
possibly unrealizable control variable values u can result as well.
If equation (5.129) is not explicitly solvable for y
(γ)
f
, the calculation of
yf
(t) for a given signal y(t) is more complex [131, 423]. However, the solution
of the differential equation (5.129) and thus the calculation of the progression
of the flat output yf
(t) from the specification of the real output y need only
be done once offline.
The flatness-based feedforward control has the structure shown in Figure
5.23. Here the feedforward controller is used to pilot the output signal y so
that it follows a reference output signal yf,ref or yref. The feedback control is
Feedback
controller
x˙ = f(x, u)
y = g(x)
Feedforward
controller
uf=Ψ2(yf,ref, . . .)
Conversion
y yref = Γ(yf,ref)
f,ref
yref e y
uf
uc u
Fig. 5.23: Flatness-based feedforward and conventional feedback controller of a sys￾tem with a fictitious flat output yf440 Chapter 5. Nonlinear Control of Nonlinear Systems
used to eliminate residual minor deviations from the reference output signal
resulting from causes such as model inaccuracies or external disturbances.
If we do not succeed in solving the implicit differential equation (5.129),
or if it is unstable and presents us with problems, we can proceed as follows.
Instead of a real signal y(t), we can plan for a progression of the fictitious flat
output yf
(t), namely yf,ref(t). This is inserted into the implicit differential
equation (5.129), and we then calculate the resulting reference course of the
real output yref(t). If it meets our requirements and possible constraints
yi,min ≤ yi ≤ yi,max, i = 1, . . . , r = dim(y), (5.133)
we have found a feasible solution to the control problem. If it does not meet
them, we must vary the reference progression of the fictitious output yf,ref(t)
until yref(t) is in the desired form and fulfills the constraints (5.133).
When selecting a fictitious output signal yf,ref(t), the initial value yf,ref(0)
and the final value yf,ref(T ) must correspond to the specified initial value
yref(0) and the specified final value yref(T ), i. e.
yref(0) = Γ(yf,ref(0), 0, . . . , 0), (5.134)
yref(T ) = Γ(yf,ref(T ), 0, . . . , 0) (5.135)
must apply. The elements of the derivatives y
(j)
f,ref(0) and y
(j)
f,ref(T ) are set to
zero for j = 1, . . . , γ. For the calculation of yf,ref(0) and yf,ref(T ), the implicit
equations (5.134) and (5.135) may have to be solved numerically if necessary.
Note that the reference signal yref(t) which is to be specified in equation
(5.129) must be differentiable at least β − γ + 1 times. This condition results
from the feedforward control law (5.132), in which the derivative y
(β+1)
f
is
required. Utilizing equation (5.131), we can represent it as
y
(β+1)
f =
∂
β−γ+1y
(γ)
f
∂tβ−γ+1 =
∂
β−γ+1Γ
−1
(yf
, y˙f
, . . . , y
(γ−1)
f
, y)
∂tβ−γ+1 .
This shows that the (β − γ + 1)-fold differentiation resulting from application
of the chain rule leads to the derivative y
(β−γ+1)
.
The differential equation (5.129) has an interesting significance in the con￾text of input-output linearization: in accordance with [169], we obtain
Theorem 71 (Real and Fictitious Flat Outputs). Let
x˙ = f(x, u),
y = g(x)
with x ∈ Dx,def ⊆ IRn
and u ∈ Du,def ⊆ IR be a system with the relative
degree δ, and let
yf = h(x)5.4. Feedforward and Feedback Control of Flat Systems 441
be a fictitious flat output. In this case the differential equation
y = g

Ψ1

yf
, y˙f
, . . . , y
(n−1)
f
 = Γ

yf
, y˙f
, . . . , y
(n−δ)
f

represents the internal dynamics of the system.
Note that in the theorem above, the internal dynamics are not represented
by a system of differential equations of order n − δ; they are represented by
a scalar differential equation. Here, the system’s output variable y formally
acts as an input variable. The order of the differential equation is n − δ, not
β = n−1. If we set y = 0, the differential equation describes the zero dynamics
of the system.
The time-consuming computations for cases in which the real output is
not flat and must be converted into a fictitious flat output make it clear how
advantageous real flat outputs are.
5.4.3 Flatness-Based Feedforward Control of Linear Systems
Linear systems which are controllable are flat and vice versa. This has already
been explained in Section 3.2.4, Theorem 49 on p. 251. In the following, we
will assume that a controllable linear system is present in controller canonical
form
x˙ =










0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
−a0 −a1 −a2 · · · −an−1










x +










0
0
.
.
.
0
1










u,
y =
h
b0 b1 · · · bm 0 · · · 0
i
x
(5.136)
or has been transformed into it. As we know from Section 3.2.5, p. 251 et seq.,
the real output y is only flat if
y = b0x1
holds. All flat outputs yf of the system can be stated as
yf = αx1
with α ∈ IR\{0}. Usually, yf = x1 is used as a flat output.
It is possible to use the controller canonical form (5.136) to directly derive
the flat system representation. The relation442 Chapter 5. Nonlinear Control of Nonlinear Systems
yf = x1,
y˙f = ˙x1 = x2,
y¨f = ˙x2 = x3,
.
.
.
y
(n−1)
f = ˙xn−1 = xn
applies; thus, for the first equation of the flat system representation, we obtain
x = Ψ1

yf
, y˙f
, . . . , y
(n−1)
f

=







yf
y˙f
.
.
.
y
(n−1)
f







.
From the controller canonical form (5.136), the second flat system equation
also directly follows as
u = a0x1 + a1x2 + . . . + an−1xn + ˙xn,
i. e.
u = a0yf + a1y˙f + . . . + an−1y
(n−1)
f + y
(n)
f
. (5.137)
The differential equation describing the relation between the real and fic￾titious output can also be simply determined as
y = b0x1 + b1x2 + . . . + bmxm+1
= b0yf + b1y˙f + . . . + bmy
(m)
f
.
(5.138)
Therefore, γ = m holds. Equation (5.138) is an example of the statement from
Theorem 71 in the previous section, namely that equation (5.138) describes
the internal dynamics of the system. In the case of linear systems, the inter￾nal dynamics are determined by the coefficients bi and thus by the zeros of
the numerator polynomial of the corresponding transfer function. We already
noted this connection in Section 5.2.8 on p. 389 et seq.
For the design of a flat feedforward control for the system (5.136), as in
the case of nonlinear systems, different scenarios exist. The simplest scenario
is the one in which
y = b0yf = b0x1
applies. The appropriate feedforward control law is given by equation (5.137).
Where5.4. Feedforward and Feedback Control of Flat Systems 443
y 6= b0yf
holds, it is important for the transfer function of system (5.136) to have only
zeros with a negative real part, because in this case the differential equation
(5.138) has a stable solution. The fictitious flat output yf required by the
feedforward control law is determined from the differential equation (5.138)
by specifying the real output signal y(t) as the reference signal yref(t) and by
solving the differential equation (5.138).
For the critical case in which y 6= yf holds and the system (5.136) pos￾sesses zeros with a nonnegative real part, the differential equation (5.138) has
unstable solutions. In this case, its solution yf can only be used for a given
reference signal yref(t) in the feedforward control law (5.137) if the signal yf(t)
does not assume values which are excessively high.
If the latter occurs, as in the nonlinear case, it is a valid procedure to plan a
fictitious reference output signal yf,ref(t). In this case, the initial value yf,ref(0)
and the final value yf,ref(T ) at time T are selected so that these two values
at least correspond to the reference values yref(0) and yref(T ). Suppose that,
both at time t = 0 and t = T , all derivatives of yf,ref and yref are identical to
zero. This is typical of a change from one operating point to another. From
equation (5.138) it then follows that
yf,ref(0) = 1
b0
yref(0)
and
yf,ref(T ) = 1
b0
yref(T )
as well as from equation (5.137)
u(0) = a0yf,ref(0) = a0
b0
yref(0),
u(T ) = a0yf,ref(T ) = a0
b0
yref(T )
for the required control variables at the operating points.
5.4.4 Example: Propulsion-Based Aircraft Control
In rare but dramatic cases, the control hydraulics of an aircraft may fail. As a
consequence, the elevator, rudder, and ailerons can no longer be operated by
the pilots. In this case the rudders are freely mobile, free of torque, and there￾fore in an approximately neutral position. A similar situation occurs when the
elevator, rudder, ailerons, and their trim tabs become stuck or damaged.
Examples of such emergencies are the flight of a Lockheed L-1011 TriStar
passenger aircraft [317] and an Airbus A300 cargo aircraft [194] whose hy￾draulic systems and flight control failed due to the hit of a surface-to-air444 Chapter 5. Nonlinear Control of Nonlinear Systems
Θ
ϕ
Fl
Fr
Pitch axis
Yaw axis
Roll axis
Fig. 5.24: An equal variation of the engine thrusts Fr and Fl causes a torque around
the pitch axis and, hence, a change of the pitch angle Θ. The thrust of the inner
engines of a Boeing 747-200 will act on the aircraft with a lever arm of 4.45 m and the
outer engines act with just 1.63 m below the pitch axis. Similarly, different variations
of the engine thrusts Fr and Fl cause a change in the yaw angle ϕ. The center of
gravity S of the aircraft lies at the intersection of its three axes.
missile. Both flights became emergencies and it was only possible to land the
aircraft due to the pilots’ considerable flight experience. To prevent the im￾pending crash, the pilots used the propulsion engines as the only remaining
control variables. A change in engine thrust can influence the direction of
flight, especially the angle of ascent ζ, i. e. the altitude of the aircraft. Fig￾ure 5.24 and Figure 5.25 illustrate how the angle of ascent can be influenced,
based on the fact that the engines are usually below the center of gravity of
the aircraft S and a torque around S can thus be generated. Furthermore, the
yaw angle ϕ can be influenced by different engine thrusts Fr and Fl on the
starboard and port sides, making a curved flight possible.
There have been a number of similar occurrences [62], not all of which have
ended as well as the ones mentioned above. A McDonnell Douglas DC-10-10
was destroyed in an emergency landing and 111 people died [331]; a Boeing
747-100SR crashed into a mountain as a result of hydraulic failure and 520
people were killed [8]. As a response to these accidents and the fact that an
aircraft with several engines can be controlled by means of the engines’ thrusts,
various controllers have been developed which support pilots in controlling the
aircraft in emergencies such as this [14, 64, 176, 210, 211, 340].
As a concrete example, we will examine the failure of the control hydraulics
in a Boeing 747-200 and design a flatness-based feedforward control of the an￾gle of ascent using the four engines. We will limit ourselves to the longitudinal
movement of the airplane shown in Figure 5.25, where we are interested in the
velocity deviation x1 from the intended flight velocity v0, the descent velocity
x2 = w, the pitch rate x3 = Θ˙
, and the pitch angle x4 = Θ. We can influence5.4. Feedforward and Feedback Control of Flat Systems 445
Θ
−ζ
w
v0
α
Horizon
Fig. 5.25: Aircraft at its velocity v0, its descent velocity w in the direction of the
aircraft’s yaw axis, its pitch angle Θ, and its angle of attack α. The angle of ascent
is ζ = Θ − α which, in this case, is negative as the plane sinks.
these variables by changing the thrust x5 of all four engines combined, which
is regulated by a secondary control. Its reference value x5,ref is used as control
variable u. As a linearized model of longitudinal motion [340] we will use
x˙ =










−1.08·10−2
0.106 −8.42 −9.77 3.91·10−6
−0.155 −0.634 85.3 −1.01 −1.77·10−7
7.16·10−4 −5.83·10−3 −0.504 7.95·10−4
7·10−8
0 0 1 0 0
0 0 0 0 −0.667










x +










0
0
0
0
0.667










u,
(5.139)
which is valid for an altitude range of 0 m to 3000 m and a speed range of v =
70 . . . 97 m s−1
. Here we are assuming an operating point of v0 = 84.7 m s−1
.
The output value y is the angle of ascent ζ, i. e. the difference between the
pitch angle Θ and the angle of attack α:
y = Θ − α ≈ Θ − sin(α) = Θ −
w
v0
=
h
0 −v
−1
0
0 1 0 i
x. (5.140)
The eigenvalues of the system are
λ1 = −0.6670,
λ2/3 = −0.5718 ± j0.7084,
λ4/5 = −0.0026 ± j0.1265.
The eigenvalue λ1 reflects the dynamics of the underlying thrust control. The
complex conjugate eigenvalues λ2/3 cause damped fast oscillations of the angle
of attack α. On the other hand, λ4/5 cause slow, very weakly damped oscil￾lations, so-called phugoids, where the angle of attack remains nearly constant446 Chapter 5. Nonlinear Control of Nonlinear Systems
and the angle of ascent ζ and speed v oscillate. Both types of oscillation are
of course undesirable.
Our goal is to adjust the angle of ascent by means of an autopilot which
uses a flat feedforward control. For this purpose we must determine a flat
output of the system. From Theorem 49 on p. 251, we know that a linear
system is flat if it is controllable. In this case, we can transform the system
into the controller canonical form, whose first state variable xccf,1 constitutes
a flat output.
Therefore, we will first determine the controllability matrix
Mcontr =










0 2.6080 · 10−6 −2.1733·10−6
1.5620·10−6 −1.1786·10−6
0 −1.1806·10−7
3.7320·10−6 −6.5746·10−6
6.0408·10−6
0 4.6690·10−8 −5.2118·10−8
2.3763·10−8
1.3576·10−8
0 0 4.6690·10−8 −5.2118·10−8
2.3763·10−8
0.667 −0.44489 0.29674 −0.19793 0.13202










whose determinant
det(Mcontr) = −1.8623 · 10−27
is very low. This means that the aircraft is very difficult to steer using the
engines alone, which is hardly surprising.
With
xccf = T
−1x, T
−1 =










h
0 · · · 0 1i
M−1
contr
h
0 · · · 0 1i
M−1
contrA
.
.
.
.
.
.
h
0 · · · 0 1i
M−1
contrAn−1










,
we can transform system (5.139), (5.140) into the controller canonical form
x˙ ccf =










0 1 0 0 0
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1
−0.008843 −0.02837 −0.5901 −1.617 −1.816










xccf +










0
0
0
0
1










u,
y = 10−9
h
3.825 32.53 5.159 1.394 0 i
xccf.
(5.141)5.4. Feedforward and Feedback Control of Flat Systems 447
We know that
yf = xccf,1
is a flat output, while y is not.
Next we will use the relation between y and yf
in the case of linear sys￾tems in controller canonical form, which was generally established in equation
(5.138), to obtain the relation
y = b0yf + b1y˙f + . . . + bmy
(m)
f
= 10−9
(3.825yf + 32.53 ˙yf + 5.159¨yf + 1.394˙˙˙yf),
(5.142)
where γ = m = 3. This also follows directly from the identities
yf = xccf,1, y˙f = xccf,2, . . . , y
(m)
f = xccf,m+1,
resulting from the first m + 1 differential equations of the controller canonical
form (5.141) after insertion into
y =
h
b0 b1 · · · bm 0 · · · 0
i
xccf.
Since the differential equation (5.142) has the eigenvalues
λ1 = −0.1198 and λ2/3 = −1.7905 ± j4.4386,
i. e. it is stable, yf can be utilized for the flat control without the described
problems. With equation (5.137) and by calculating yf(t) using equation
(5.142) for a given signal y(t), we obtain the control variable
u = a0yf + a1y˙f + a2y¨f + a3˙˙˙yf + a4y
(4)
f + y
(5)
f
= 0.008843yf + 0.02837 ˙yf + 0.5901¨yf + 1.617 ˙˙˙yf + 1.816y
(4)
f + y
(5)
f
.
(5.143)
In this way, we can generate a control variable signal u(t) for the feedforward
controller by inserting
yf(t) = yf,ref(t)
into equation (5.143).
In the following, the angle of ascent should increase continuously from the
value ζ = y = 0◦
to −1
◦
, so that the aircraft sinks constantly in the end with
ζ = −1
◦ = −0.01745 rad. For this purpose, we will choose
yref(t) =



−0.008725 
1 − cos( π
300
t)
!
, 0 ≤ t ≤ 300 s,
−0.01745, t > 300 s,448 Chapter 5. Nonlinear Control of Nonlinear Systems
-0.02
-0.015
-0.01
-0.005
-60
-40
-20
20
0
0
0
0
50
50
100
100
150
150
200
200
250
250
300
300
350
350
400
400
450
450
500
500
Control variable
u in kN Angle of ascent
y in rad
Time t in s
Fig. 5.26: Time courses of the angle of ascent y and the engine thrust u of the Boeing
747-200 for a transition of the angle of ascent from y = 0 to y = −1
◦ = −0.01745 rad
as the reference signal of the angle of ascent y = ζ. From equation (5.142) we
can also calculate the flat output yf using the initial values
yf(0) = 0, y˙f(0) = 0, y¨f(0) = 0 and ˙˙˙yf(0) = 0.
Figure 5.26 shows only the real output, i. e. the angle of ascent, since the flat
output in this case has almost the same progression. Note that the reference
signal yref(t), as required in Section 5.4.2, can be differentiated twice, meaning
β − γ + 1 times with β = 4 and γ = 3. Its second derivative is not continuous,
since this is not necessary here.
5.4.5 Flatness-Based Feedback Control of Nonlinear Systems
Theorem 69 on p. 430 states that a full-state linearizable control-affine SISO
system is flat and vice versa, i. e. a flat SISO system is also full-state lineariz￾able. We can extend this statement to general nonlinear SISO systems
x˙ = f(x, u) (5.144)
with x ∈ Dx,def ⊆ IRn
. First of all, we will assume that the system (5.144)
can be full-state linearized and that
y = h(x) (5.145)5.4. Feedforward and Feedback Control of Flat Systems 449
is a linearizing output. This means that the system (5.144), (5.145) has the
relative degree δ = n. For the linearization, we will again use the Lie derivative
Lf h(x) = ∂h(x)
∂x
f(x, u)
and compute
y = h(x),
y˙ = Lf h(x),
y¨ = L
2
f h(x),
.
.
.
y
(n−1) = L
n−1
f
h(x),
y
(n) = L
n
f h(x) = ϕ(x, u),
(5.146)
where all derivatives
y
(i) = L
i
f h(x), i = 1, . . . , n − 1, (5.147)
of the linearizable system (5.144), (5.145) are independent of the control vari￾able u. Only
y
(n) = L
n
f h(x) = ϕ(x, u) (5.148)
depends on u. In this way, using the new state variables
z =







z1
z2
.
.
.
zn







=







y
y˙
.
.
.
y
(n−1)







=







h(x)
Lf h(x)
.
.
.
L
n−1
f
h(x)







= t(x),
we obtain the so-called generalized controller canonical form [123]
z˙ =







z2
.
.
.
zn
ϕ(x, u) = ϕ(t
−1
(z), u)







, (5.149)
which is a generalization of the nonlinear controller canonical form. Note that
if we introduce a new control variable
v = ϕ(x, u),450 Chapter 5. Nonlinear Control of Nonlinear Systems
the linearized system representation in Brunovsky canonical form is obtained
from equation (5.149).
Since we assumed that the system is full-state linearizable, the function ϕ
can be inverted, and
u = ϕ
−1
(x, y(n)
) = ϕ
−1

t
−1
(z), y(n)

= Ψ2(y, y, . . . , y ˙
(n)
) (5.150)
applies with x = t
−1
(z) and v = y
(n)
. Furthermore, we know the expression
x = t
−1
(z) = Ψ1

y, y, . . . , y ˙
(n−1)
(5.151)
from equation (5.115) on p. 431. Equation (5.150) and equation (5.151) rep￾resent the flat system description of the system (5.144), (5.145). As a result,
any full-state linearizable system (5.144) is flat.
Now we will take the opposite approach and show that any flat system
(5.144), (5.145) can be state linearized. To this end, let us consider the first
equation
x = Ψ1

y, y, . . . , y ˙
(n−1)
.
This does not depend on the input signal u or one of its derivatives u
(i)
,
which means that all derivatives y
(i)
, i = 1, . . . , n − 1, fulfill equation (5.147).
Obviously, the identity
z = Ψ
−1
1
(x) = t(x)
holds, where
z =







y
y˙
.
.
.
y
(n−1)







. (5.152)
From the second equation of a flat system representation,
u = Ψ2

y, y, . . . , y ˙
(n)

, (5.153)
we can further conclude that y
(n)
is dependent on the input signal u, since
the derivatives y
(i)
, i, . . . , n − 1, in equation (5.146) are not. Thus, equation
(5.153) can be transformed into an equation conforming to equation (5.148).
As result, we obtain
y
(n) = L
n
f h(x) = ϕ(x, u) = ϕ(t
−1
(z), u)
= ˆϕ(z, u) = ˜ϕ(y, y, . . . , y ˙
(n−1), u).
(5.154)5.4. Feedforward and Feedback Control of Flat Systems 451
With equation (5.152) and equation (5.154), we then obtain the generalized
controller canonical form
z˙ =







z2
.
.
.
zn
ϕˆ(z, u)







,
y = z1
(5.155)
as a system representation equivalent to the flat system representation.
Because we assumed the system to be flat, equation (5.154) can be solved
for u, yielding
u = ˜ϕ
−1
(y, y, . . . , y ˙
(n)
) = Ψ2(y, y, . . . , y ˙
(n)
).
This also ensures that ϕ and ϕ˜ are invertible as well, from which the full-state
linearizability of system (5.155) follows. Consequently, system (5.144) is full￾state linearizable. So, as a generalization of Theorem 69, we can derive [420]
Theorem 72 (Flatness and Linearizability of SISO Systems). A non￾linear system
x˙ = f(x, u), x ∈ Dx,def ⊆ IRn
, u ∈ IR,
is flat if and only if it is full-state linearizable. If the system has the relative
degree n with respect to an output y, then y is a flat output.
In addition, the transformation which follows from the above is important to
the flatness-based controller design we aim to create. It can be stated as
Theorem 73 (Transformation into Brunovsky Canonical Form). For
a flat system
x˙ = f(x, u), x ∈ Dx,def ⊆ IRn
, u ∈ IR,
the flat output y, and the flat system representation
x = Ψ1(y, y, . . . , y ˙
(n−1)),
u = Ψ2(y, y, . . . , y ˙
(n)
),
the diffeomorphism which transforms the system into the generalized controller
canonical form







z˙1
.
.
.
z˙n−1
z˙n







=







z2
.
.
.
zn
ϕ(z, u)






452 Chapter 5. Nonlinear Control of Nonlinear Systems
is
z = Ψ
−1
1
(x)
with z = [y, y, . . . , y ˙
(n−1)]
T
. Furthermore, the state-dependent mapping
v = ϕ(z, u) = Ψ
−1
2
(y, y, . . . , y ˙
(n−1), u)
of the input variable u transforms the system into Brunovsky canonical form.
With the theorem above, we have also performed input-output linearization.
This is because
z˙n = y
(n) = v (5.156)
holds, so that the output variable y depends on the artificial input variable v
via n integrators. From
u = Ψ2(y, y, . . . , y ˙
(n)
) = Ψ2(y, y, . . . , y ˙
(n−1), v),
we can obtain the real input or control variable u.
Theorem 73 now enables us to design a flatness-based feedback control
system with respect to an output variable signal yref(t) which we can select
from a wide range. We can now insert the control law
v = y
(n)
ref −
nX−1
i=0
ai

y
(i) − y
(i)
ref
(5.157)
into the state equation (5.156) of the Brunovsky canonical form. Thus, after
inserting equation (5.157) into equation (5.156), the linear system dynamics
y
(n) − y
(n)
ref + an−1

y
(n−1) − y
(n−1)
ref 
+ . . . + a1 ( ˙y − y˙ref) + a0 (y − yref) = 0
(5.158)
are obtained.
Provided that we choose all roots of the characteristic polynomial
P(s) = s
n + an−1s + . . . + a1s + a0
exclusively with negative real parts by selecting suitable coefficients ai
, the
differential equation (5.158) is stable. Thus, we obtain
lim
t→∞ 
y
(i)
(t) − y
(i)
ref(t)

= 0, i = 0, . . . , n.
This means that the approximation
y
(i)
(t) ≈ y
(i)
ref(t), i = 0, . . . , n
applies such that the output variable y(t) follows the reference variable yref(t).5.4. Feedforward and Feedback Control of Flat Systems 453
z=





y
y˙
.
.
.
y
(n−1)





=Ψ
−1
1
(x)
x˙ = f(x, u)
y = h(x)
v=y
(n)
ref −
Xn−1
i=0
ai

y
(i)−y
(i)
ref
u=Ψ2(y, . . . ,y
(n−1)
,v)
Reference Controller Exact linearization Plant
u y
x
y
(n)
ref
v





yref
y˙ref
.
.
.
y
(n−1)
ref





Fig. 5.27: Flatness-based feedback control system
The complete control law consists of the two equations
u = Ψ2(y, y, . . . , y ˙
(n−1), v), (5.159)
v = y
(n)
ref −
nX−1
i=0
ai

y
(i) − y
(i)
ref
. (5.160)
Figure 5.27 shows the resulting control-loop. The diffeomorphism x = Ψ1(z)
and the transformation (5.159) linearizes the plant according to equation
(5.156), and the control law (5.160) generates the linear dynamics (5.158). The
coefficients ai are freely selectable with the restriction that they must result
in an asymptotically stable control loop. When specifying the time courses
yref(t), y˙ref(t), . . . , y
(n)
ref (t),
yref(t) must be differentiable at least n times.
As an alternative to the flatness-based feedforward control with superim￾posed feedback control, the flatness-based feedback control above is a second
way to use the flatness of a system for the feedback control with respect to a
given desired signal yref(t).
The flatness-based feedback control can also be implemented for MIMO
plants. However, the design may be much more complicated in these cases.
This is especially true if the controlled system is flat but not full-state lin￾earizable. Information on this topic is provided in [1, 420, 485].
5.4.6 Example: Pneumatic Motor
We will examine a pneumatic motor used for the feed of a drill or for moving
workpieces to grinding machines. Figure 5.28 shows the basic structure of such
a motor. Air is blown through a hose or a pipe into a cylinder via a compressor.
Due to the pressure P in the cylinder, a piston then moves distance s, and
with it a load mass is shifted. This compresses a spring which brings the piston
back to its initial position when the pressure in the cylinder drops. The spring454 Chapter 5. Nonlinear Control of Nonlinear Systems
s
P
Fig. 5.28: Pneumatic motor
possesses the spring constant c, the piston the surface area A, and the moving
parts, which are load, piston, and linkage, possess the mass m.
The equation of motion results from the spring force cs as well as from the
force P A acting on the piston and can be stated as
ms¨ = P A − cs. (5.161)
The pressure P in turn can be determined by means of the ideal gas equation
1
n
P V = RT (5.162)
if we assume isothermal behavior. Here, V = As is the cylinder volume, n is
the number of gas molecules in moles, R is the ideal gas constant, and T is
the temperature of the gas, which is assumed to be constant.
The next step is to differentiate the ideal gas equation (5.162) with respect
to time, which yields
P V˙
n
+
PV˙
n
−
P V n˙
n2
= 0
or
P˙ =
P
V

V n˙
n
− V˙

. (5.163)
With the air volume flow
q =
V n˙
n
,
which acts as the input variable u, and V = As, we obtain
P˙ =
P
s
 q
A
− s˙

(5.164)
from equation (5.163). If we set
x1 = s, x2 = ˙s, x3 = P, and u = q5.4. Feedforward and Feedback Control of Flat Systems 455
as the state variables and the input variable, equations (5.161) and (5.164)
provide the model of the pneumatic motor







x˙ 1
x˙ 2
x˙ 3







| {z }
x˙
=







x2
−
c
m
x1 +
A
m
x3
−
x2x3
x1







| {z }
a(x)
+







0
0
x3
Ax1







| {z }
b(x)
u. (5.165)
As the output variable of the system, we will select the load position
y = c(x) = x1 = s.
The system is full-state linearizable and y = x1 is a linearizing output.
This is easily verifiable, because the derivatives of y are
y = x1,
y˙ = ˙x1 = x2,
y¨ = ˙x2 = −
c
m
x1 +
A
m
x3,
˙˙˙y = −
c
m
x˙ 1 +
A
m
x˙ 3 = −
c
m
x2 −
Ax2x3
mx1
+
x3
mx1
u.
(5.166)
Thus, the relative degree δ = 3 equals the system order and the system is
full-state linearizable. As we recall from Theorem 69 and Theorem 72, this
means that y = x1 is a flat output.
Since y is a flat output, the flat system representation can also be quickly
derived from equation (5.166). This is because







x1
x2
x3







=







y
y˙
m
A
 
y¨ +
c
m
y
!







= Ψ1 (y, y,˙ y¨)
and
u = A
my˙˙˙y + my˙y¨ + 2cyy˙
cy + my¨
= Ψ2 (y, y,˙ y, ¨ ˙˙˙y ) (5.167)
hold.
We can use equation (5.167) as a flat feedforward control law. An ad￾vantageous alternative to the feedforward control is a flatness-based feedback
control, as described in the previous section and illustrated in Figure 5.27.
For this purpose, we need the inverse function456 Chapter 5. Nonlinear Control of Nonlinear Systems
z=






y
y˙
y¨






=






x1
x2
−
c
m
x1+
A
m
x3






x˙ =a(x)+b(x)u
y=c(x)=x1
v= ˙˙˙yref−
X2
i=0
ai

y
(i)−y
(i)
ref
u=A
my ˙˙˙y +my˙y¨+2cyy˙
cy+my¨
Reference Controller Exact linearization Plant
u y
x
˙˙˙yref
v





yref
y˙ref
y¨ref





Fig. 5.29: Flatness-based feedback control of the pneumatic motor
z =




y
y˙
y¨




= Ψ
−1
1
(x) =






x1
x2
−
c
m
x1 +
A
m
x3






.
Recall that, according to Theorem 73, the mapping Ψ
−1
1
is the diffeomorphism
which transforms the system into the nonlinear controller canonical form.
Furthermore, we need the controller
v = ˙˙˙yref −
X
2
i=0
ai

y
(i) − yref
(i)

.
Figure 5.29 shows the structure of the flatness-based feedback controller for
the pneumatic motor.
As a concrete numerical example, we can take
c = 20 N cm−1
, A = 50 cm2
, m = 1 kg.
The coefficients of the controller are selected as
a0 = 1000, a1 = 300, a2 = 30,
so that the closed linear control loop has the threefold eigenvalue
λ = −10.
We will now move the pneumatic motor’s piston from
y = x1 = s = 1 cm
to5.4. Feedforward and Feedback Control of Flat Systems 457
10
10
0
0
1
1
1
1
2
2
3
3
4
4
5
5
6
6
Time t in s
Position
y in cm Position
y in cm
Target trajectory yref and output y at m = 1kg
Output y at m = 0.5 kg
Output y at m = 1.5 kg
Fig. 5.30: Time courses of the position y = x1 for the flat feedforward control in the
upper diagram, and for the flatness-based feedback control in the lower diagram
y = 10 cm.
To do so, we specify
yref(t) = (
−1.406t
7 + 9.844t
6 − 23.630t
5 + 19.694t
4 + 1, 0 ≤ t ≤ 2,
10, 2 < t,
with the reference values
yref(0) = 1 cm, yref(2) = 10 cm,
y˙ref(0) = 0 cm s−1
, y˙ref(2) = 0 cm s−1
,
y¨ref(0) = 0 cm s−2
, y¨ref(2) = 0 cm s−2
,
˙˙˙yref(0) = 0 cm s−3
, ˙˙˙yref(2) = 0 cm s−3
.
Figure 5.30 shows the time course of the load position y = s of the piston for
the nominal case m = 1 kg as well as for the cases m = 0.5 kg and m = 1.5 kg.
In the upper diagram, the time course of the position y(t) is shown for the flat
feedforward control, and the lower diagram displays the course y(t) for the
flatness-based feedback control. As expected, unlike the feedforword control,
the feedback control compensates for the changes in the load mass m.458 Chapter 5. Nonlinear Control of Nonlinear Systems
5.4.7 Flat Inputs and Their Design
A prerequisite for the flatness-based design of a feedforward controller is a flat
output. The ideal case is one in which the real output of the system is flat.
If the real output is not flat, or if the system itself is not flat, it may make
sense to design the input of the system in such a way that the measurable
real output becomes flat. Thus the output is made flat by the creation of a
suitable input. An input such as this is termed flat input [137, 138, 337, 466].
In general, the design engineer of the system to be controlled will have to
select and implement the actuators required to integrate the flat input into
the system. Since she or he must be informed of such an input by the control
engineer, cooperation between the two is necessary at an early stage of the
design.
We will proceed by designing flat inputs for the autonomous nonlinear
SISO systems
x˙ = a(x),
y = c(x),
(5.168)
i. e. vectors b(x), to obtain a flat control-affine system
x˙ = a(x) + b(x) · u,
y = c(x)
with y as a flat output. For this purpose, we will transform the system using
z = q(x, u, . . . , u(n−2)) =











y
y˙
y¨
.
.
.
y
(n−1)











=










c(x)
Lac(x) + Lbc(x)u
L
2
a
c(x) + LbLac(x)u + LaLbc(x)u + L
2
b
c(x)u
2 + Lbc(x) ˙u
.
.
.
L
n−1
a c(x) +LbL
n−2
a c(x)u + . . . +L
n−1
b
c(x)u
n−1 +Lbc(x)u
(n−2)










.
(5.169)
For the time being, we will assume that the terms fulfill
LbL
i
a
c(x) 6= 0, i = 0, . . . , n − 2.5.4. Feedforward and Feedback Control of Flat Systems 459
By differentiating the transformation equation above, we obtain










z˙1
z˙2
.
.
.
z˙n−1
z˙n










=










y˙
y¨
.
.
.
y
(n−1)
y
(n)










=










Lac(x)
L
2
ac(x)
.
.
.
L
n−1
a
c(x)
L
n
ac(x)










+










Lbc(x)
LbLac(x)
.
.
.
LbL
n−2
a
c(x)
LbL
n−1
a c(x)










u+










0
LaLbc(x)
.
.
.
L
n−2
a Lbc(x)
L
n−1
a Lbc(x)










u+. . .
+










0
Lbc(x) ˙u
.
.
.
Lbc(x)u
(n−2)
Lbc(x)u
(n−1)










,
(5.170)
y = z1.
We will design the input vector b(x) in such a way that
Lbc(x) = ∂c(x)
∂x
b(x) = 0,
LbLac(x) = ∂Lac(x)
∂x
b(x) = 0,
.
.
.
LbL
n−2
a c(x) = ∂Ln−2
a c(x)
∂x
b(x) = 0
(5.171)
and
LbL
n−1
a
c(x) = ∂Ln−1
a
c(x)
∂x
b(x) = e(x) (5.172)
hold, where e(x) 6= 0 is a freely selectable function. Combining equations
(5.171) and (5.172), we obtain
Q(x)b(x) =







0
.
.
.
0
e(x)







with Q(x) =













∂c(x)
∂x
∂Lac(x)
∂x
.
.
.
∂Ln−1
a c(x)
∂x













. (5.173)460 Chapter 5. Nonlinear Control of Nonlinear Systems
This choice of b(x) considerably simplifies system (5.170), because all terms
containing
LbL
i
ac(x) = 0, i = 0, . . . , n − 2,
are now omitted, and in this case only, the system description
z˙ =










z2
z3
.
.
.
zn
L
n
ac(x)










+










0
0
.
.
.
0
LbL
n−1
a c(x)










u,
y = z1
applies. The system is now in nonlinear controller canonical form and has the
relative degree δ = n.
Furthermore, the transformation equation (5.169) takes the simpler form
z = q(x, u, u, . . . , u ˙
(n−2)) = t(x) =







c(x)
Lac(x)
.
.
.
L
n−1
a
c(x)







,
which we encountered in connection with input-output linearized systems with
the maximum relative degree δ = n discussed in Section 5.2.1, p. 358 et seq.,
which do not depend on the derivatives
u, . . . , u ˙
(n−2)
or powers
u
2
, . . . , un−1
.
As we learned from Theorem 51 on p. 257, SISO systems are flat if and
only if they are in controller canonical form or can be transformed into it. For
this canonical form,
y = z1
is a flat output. Furthermore, a system (5.168) is bijectively transformable
into nonlinear controller canonical form if and only if the Jacobian matrix
Q(x) = ∂t(x)
∂x
of the transformation z = t(x) is regular. We know this from Theorem 54 on
p. 262.5.4. Feedforward and Feedback Control of Flat Systems 461
Based on the analysis above, a system is flat if the matrix Q(x) is regular
and equation (5.173) applies. It then follows from equation (5.173) that the
input vector
b(x) = Q−1
(x)







0
.
.
.
0
e(x)







(5.174)
creates a flat input u and thus a flat output y = c(x). With the arbitrarily
selectable function e(x) 6= 0, equation (5.174) also generates all possible flat
inputs, since firstly Q(x) is regular and secondly b(x) fulfills equations (5.171)
and (5.172), which lead to the nonlinear controller canonical form and thus
ensure the flatness.
This is summarized in [466]
Theorem 74 (Flat Inputs of Control-Affine Systems). If an autono￾mous system
x˙ = a(x), x ∈ Dx,def ⊆ IRn
,
y = c(x)
is extended to a system
x˙ = a(x) + b(x) · u,
y = c(x)
with an input u, this input is flat and y is a flat output if and only if the
matrix
Q(x) =













∂c(x)
∂x
∂Lac(x)
∂x
.
.
.
∂Ln−1
a c(x)
∂x













is regular and the input vector fulfills the equation
b(x) = Q
−1
(x)







0
.
.
.
0
e(x)







for an arbitrary function e(x) 6= 0.462 Chapter 5. Nonlinear Control of Nonlinear Systems
Note that the matrix Q(x) is the observability matrix of a control-affine
system with the relative degree n which will be discussed again in Chapter 7.
For designing flat inputs of MIMO systems, see [337, 467].
5.4.8 Flat Inputs of Linear Systems
Now we will apply the results of the previous section to the special case of
linear systems
x˙ = Ax,
y = c
T x.
(5.175)
We are interested in determining when the system (5.175) has flat inputs
and what form they take. Theorem 74 provides us with the answers to these
questions. Firstly, the matrix
Q(x) =













∂c(x)
∂x
∂Lac(x)
∂x
.
.
.
∂Ln−1
a c(x)
∂x













=







c
T
c
T A
.
.
.
c
T A
n−1







, c(x) = c
T x,
which is the observability matrix of a linear system (5.175), must be regular.
This means the system (5.175) has flat inputs if and only if it is observable.
Secondly, the input vector b of the extended system
x˙ = Ax + b · u,
y = c
T x
must take the form
b = Q−1







0
.
.
.
0
e







, e ∈ IR\{0},
so that the output y of the linear system (5.175) is flat. We remember that
the constant e can be selected arbitrarily.
As in the nonlinear case, we can design inputs which enable particularly
efficient feedforward and feedback control of the system. It is also possible to
analyze existing inputs in terms of their suitability for flat control.5.4. Feedforward and Feedback Control of Flat Systems 463
5.4.9 Example: Economic Market Model
An example of an economic market is a simple model [34, 160, 268] of the de￾pendence on the price P of a product on the supply quantity Q and the quan￾tity Qd demanded by customers. We can assume that the change in price is
proportional to the difference between demand and supply quantity, i. e. that
P˙ = λ (Qd(P) − Q), λ > 0, (5.176)
holds, where λ is a constant of proportionality. If the demand Qd is greater
than the supply Q, the price P increases.
Furthermore, the manufacturer responds to the possible market price P,
or, to put it more precisely, to the difference between the market price P and
the manufacturer’s supply price Ps. This can be modeled by
Q˙ = µ (P − Ps(Q)), µ > 0. (5.177)
This means that if the market price P is higher than the supply price Ps
and increases, more will be produced. As in equation (5.176), the relationship
between Q˙ and P is linear where µ is a constant of proportionality.
We can now assume that the demand Qd
Qd(P) = γ − κP, γ, κ > 0 (5.178)
depends linearly on the current market price P via a constant κ. A low price
therefore leads to high demand.
The supply price Ps depends on the supply quantity Q via
Ps(Q) = δQ2 + β, δ, β > 0, (5.179)
for example.
If we insert equation (5.178) and equation (5.179) into the differential
equations (5.176) and (5.177), respectively, and select
x1 = P and x2 = Q
as state variables, we obtain


x˙ 1
x˙ 2

 =


a0 − a1x1 − a2x2
a3x1 − a4 − a5x
2
2

 = a(x),
where
a0 = λγ, a1 = λκ, a2 = λ, a3 = µ, a4 = µβ, a5 = µδ.
Our goal now is to influence the market price x1 = P to increase it. To do
this, we must identify an input with which we can exert influence on x1, i. e.
a flat input. Here, the price464 Chapter 5. Nonlinear Control of Nonlinear Systems
y = x1 = c(x)
is the system’s output of interest.
Applying Theorem 74, we determine
∂c(x)
∂x
= [1 0] ,
∂Lac(x)
∂x
=
∂(a0 − a1x1 − a2x2)
∂x
= [−a1 −a2]
and hence
Q(x) =




∂c(x)
∂x
∂Lac(x)
∂x




=


1 0
−a1 −a2

.
With
b(x) = Q
−1
(x)



0
e(x)


 =



1 0
−
a1
a2
−
1
a2






0
e(x)


 =



0
−
e(x)
a2



and by choosing
e(x) = −a2,
we obtain the input vector
b(x) =


0
1


in the system description
x˙ = a(x) + b(x) · u.
In this way, the control variable u from the market model


x˙ 1
x˙ 2

 =


a0 − a1x1 − a2x2
a3x1 − a4 − a5x
2
2

 +


0
1

u,
y = x1
(5.180)
becomes a flat input and y = x1 is a flat output.
If we now wish to influence the current market price y = x1 = P, we can
use the flat system representation and a reference trajectory yref(t) to specify
the control variable
u = Ψ2 (yref, y˙ref, y¨ref)
= a4 − a3yref + a5

a0 − a1yref − y˙ref
a2
2
−
a1y˙ref + ¨yref
a2
.5.5. Control Lyapunov Functions 465
From equation (5.180) it is clear that u affects the change in the supply
quantity x˙ 2 = Q˙
. The above result is therefore simple to interpret, because
the control variable u can directly influence the supply quantity x2 = Q. The
control variable u thus means either an increase in production or a product
shortage.
5.5 Control Lyapunov Functions
5.5.1 Fundamentals
The direct Lyapunov method can be used for stability analysis as well as for
controller synthesis. A controller synthesis method of this kind can be based
on so-called control Lyapunov functions V (x), often abbreviated as CLF. The
basic concept of control Lyapunov functions is to use the control variable
vector u of a plant
x˙ = f(x,u)
which has the equilibrium point
xeq = 0
such that the derivative
V˙ (x) = f
T
(x,u) · grad(V (x))
of a radially unbounded Lyapunov function V (x) is minimized and the in￾equality
V˙ (x) < 0
applies to all states x 6= 0. For example, the control variable u is determined
so that the infimum, i. e. the lower limit of V˙ (x), satisfies the inequality
inf
u
n
V˙ (x) = f
T
(x,u) · grad(V (x))o
< 0 (5.181)
for all x 6= 0. The control variable u(x) thus determined constitutes a control
law based on the Lyapunov stability theorem on p. 115, and yields a control
loop
x˙ = f(x,u(x))
with the globally asymptotically stable equilibrium point xeq = 0. Note that
it is also possible to aim for local stability only.
To ensure the stability of the control loop, it is not necessary to select
the function u(x) as the control law which generates the infimum of V˙ (x).
In fact, a control law u(x) which yields V˙ (x) < 0 for all x is sufficient. The
choice of the controller u(x) which leads to the infimal V˙ has the advantage
that V (x) decreases rapidly along the trajectories x(t) and the compensation
is therefore rapid.466 Chapter 5. Nonlinear Control of Nonlinear Systems
The term control Lyapunov function is derived from the fact that, using
a Lyapunov function, we define a controller u(x) which ensures stability. Al￾though the procedure above ensures the stability of the control loop being
designed, it does not allow a direct, calculable influence on the control perfor￾mance, i. e. the settling time. However, it is difficult or impossible to predict
whether our choice of a Lyapunov function will positively or negatively affect
the control performance. Namely, the faster the Lyapunov function decreases,
the smaller the settling time becomes. Therefore, the choice of the Lyapunov
function is critical for the control performance.
For time-invariant nonlinear systems, we will define the term control Lya￾punov function as follows.
Definition 35 (Control Lyapunov Function). Let
x˙ = f(x,u), x ∈ IRn
, u ∈ IRm,
be a system with the equilibrium point xeq = 0 for u = 0. A continuously
differentiable function V (x) is called a control Lyapunov function for this
system if it fulfills the following conditions:
(1) V (0) = 0,
(2) V (x) > 0 for all x 6= 0,
(3) V (x) → ∞ for |x| → ∞.
(4) A control law u(x) exists such that V˙ (x) < 0 applies to all x 6= 0.
If we apply the Barbashin and Krasovskii theorem, i. e. Theorem 14 on p.
116, we can define a control Lyapunov function somewhat less restrictively.
We thus arrive at
Definition 36 (Extended Control Lyapunov Function). Let
x˙ = f(x,u), x ∈ IRn
, u ∈ IRm,
denote a system with the equilibrium point xeq = 0 for u = 0. A continuously
differentiable function V (x) is called an extended control Lyapunov function
for this system if it satisfies the following conditions:
(1) V (0) = 0,
(2) V (x) > 0 for all x 6= 0,
(3) V (x) → ∞ for |x| → ∞.
(4) A control law u(x) exists such that V˙ (x) ≤ 0 applies to all x ∈ IR.
(5) The set of states x of the system controlled by u(x), to which V˙ (x) = 0
applies, contains no trajectory x(t) except x(t) = 0.
Definition 36 makes the requirement V˙ (x) < 0 from Definition 35 less restric￾tive. Now we can also address the case in which
V˙ (x) ≤ 05.5. Control Lyapunov Functions 467
holds, as long as, according to Condition (5), V˙ (x(t)) = 0 does not apply
along any trajectory x(t) of the controlled system.
Based on the Lyapunov and the Barbashin and Krasovskii theorems, The￾orem 13 on p. 115 and Theorem 14 on p. 116, respectively, and the definitions
above, we can directly derive
Theorem 75 (Existence of a Control Law). Let there be a system
x˙ = f(x,u), x ∈ IRn
, u ∈ IRm,
with the equilibrium point xeq = 0 for u = 0. If in this case a control Lyapunov
function or an extended control Lyapunov function V (x) exists for the system,
a controller u(x) can always be found such that the control loop’s equilibrium
point xeq = 0 is globally asymptotically stable.
If Condition (3) of Definitions 35 and 36 is not fulfilled, only local asymptotic
stability can be ensured.
As with the direct method in the case of stability analysis, the main dif￾ficulty with the above theorem on controller synthesis is finding a suitable
Lyapunov function V (x). Unfortunately, there is no general procedure for the
design of a Lyapunov function or a control Lyapunov function. In many cases,
we must rely on our intuition to find a control Lyapunov function or we must
test different possible functions V (x) to find out whether one of them fulfills
the conditions of Definition 35 or 36.
5.5.2 Control Lyapunov Functions for Linear Systems
As a simple example to illustrate the method of control Lyapunov functions,
we will examine the linear system
x˙ = Ax + b · u, |u| ≤ umax,
which we assume to be Lyapunov stable, and
V (x) = x
T R x
as a possible control Lyapunov function which is positive definite and radially
unbounded and therefore fulfills Conditions (1), (2), and (3) from Definition
35. Its derivative is
V˙ (x) = x
T

AT R + RA
x + 2b
T R x · u.
Obviously, V (x) = x
T R x is a control Lyapunov function if the matrix
A
T R + RA
is negative definite or negative semidefinite, since the control law468 Chapter 5. Nonlinear Control of Nonlinear Systems
u = −umax sgn 
b
T R x
(5.182)
yields
2b
T R x · u = −2umax sgn 
b
T R x
b
T R x = −2umax|b
T R x|,
meaning that Condition (4) from Definition 35
V˙ (x) = f
T
(x, u) grad(V (x))
= x
T

AT R + RA
x − 2umax|b
T R x| < 0
is fulfilled.
As we have seen, a control Lyapunov function is simple to determine for
a Lyapunov stable linear system. However, the control law (5.182) has the
disadvantage of switching between −umax and +umax. Only a few actuators
can provide and endure such switching behavior. Furthermore, undesirable
sliding modes can occur.
5.5.3 Control Lyapunov Functions for Control-Affine Systems
After this simple example, we will turn to a more general case. The plants in
question are now the control-affine systems
x˙ = a(x) + B(x) · u. (5.183)
In this case Condition (4) from Definition 35 takes the form
a
T
(x) · Vx(x) + u
T B
T
(x) · Vx(x) < 0, (5.184)
where we have abbreviated the gradient of the control Lyapunov function
V (x) as
Vx(x) = grad(V (x)) = 
∂V
∂x
T
.
From inequality (5.184) we can now derive
Theorem 76 (Control Lyapunov Function for Control-Affine Sys￾tems). For a system
x˙ = a(x) + B(x) · u
a radially unbounded, positive definite function V (x) is a control Lyapunov
function if one of the conditions
(1) B
T
(x)Vx(x) 6= 0 for all x 6= 0 or
(2) a
T
(x)Vx(x) < 0 for all x 6= 0 with B
T
(x)Vx(x)=0,
a
T
(x)Vx(x) = 0 for x = 0
is fulfilled.5.5. Control Lyapunov Functions 469
Condition (2) from the theorem above is particularly satisfied if
a
T
(x)Vx(x) < 0 for x ∈ IRn
\ {0},
a
T
(x)Vx(x) = 0 for x = 0
(5.185)
hold. In this significant case, the function V is already a Lyapunov function
for the free system x˙ = a(x).
In the event that there is a known control Lyapunov function V (x) for the
plant (5.183), we use E. D. Sontag’s control law, which globally asymptotically
stabilizes the equilibrium point xeq = 0 of the system (5.183) [426, 431]. It is
formulated in
Theorem 77 (Sontag’s Control Law). Let V (x) be a control Lyapunov
function for the system
x˙ = a(x) + B(x) · u.
In this case the control loop consisting of this system and the controller
u(x) =



−ks(x)B
T
(x)Vx(x) for B
T
(x)Vx(x) 6= 0,
0 for B
T
(x)Vx(x) = 0
with
ks(x) =
a
T
(x)Vx(x) + q
(aT (x)Vx(x))2 + h
2(x)
h(x)
,
h(x) = Vx
T
(x)B(x)B
T
(x)Vx(x),
has the globally asymptotically stable equilibrium point xeq=0.
By inserting Sontag’s control law into inequality (5.184), it can be proven that
the inequality is satisfied, and thus the control loop is globally asymptotically
stable. For systems (5.183) with bounded control inputs, a modified formula
can be found in [283].
Further we will discuss two control laws for cases in which the control
Lyapunov function fulfills condition (5.185) and thus Condition (2) from The￾orem 76. In this case, the autonomous part x˙ = a(x) of system (5.183) has a
globally asymptotically stable equilibrium point xeq = 0. We can rewrite the
system (5.183) as
x˙ = a(x) +Xm
i=1
bi(x) · ui
,
where the vectors bi(x) are the column vectors of the matrix B(x). Let us
assume that we have succeeded in identifying a control Lyapunov function for
the system above. We then obtain470 Chapter 5. Nonlinear Control of Nonlinear Systems
V˙ (x) = a
T
(x)Vx(x)
| {z }
< 0
+
Xm
i=1
Vx
T
(x)bi(x) · ui
. (5.186)
It is reasonable to select all ui
in equation (5.186) in such a way that V˙ (x)
takes on the lowest possible negative values so that the compensation for a
trajectory x(t) into the equilibrium point xeq = 0 happens rapidly. In the
case of control variable limitations |ui
| ≤ ui,max, we can achieve this goal if
we use the control law
ui = −ui,max sgn(Vx
T
(x)bi(x)), i = 1, . . . , m,
because then
V˙ (x) = a
T
(x)Vx(x) −
Xm
i=1
ui,max

Vx
T
(x)bi(x)

 < 0
and inequality (5.181) hold.
However, this switching control law has a discontinuous control variable
signal and possibly undesirable sliding modes. Both are drawbacks which have
already occurred with the control law (5.182).
The disadvantages above can be avoided if the control variable is selected
as
ui = − sat(Vx
T
(x)bi(x)), sat(v) =



vmax, v > vmax,
v, |v| ≤ vmax,
−vmax, v < −vmax,
by means of a saturation function. As a result,
V˙ (x) = a
T
(x)Vx(x) −
Xm
i=1
Vx
T
(x)bi(x) sat(Vx
T
(x)bi(x)) < 0
holds and in this case V also decreases rapidly along any trajectory.
5.5.4 Illustrative Example
As an example, we will analyze the system
x˙ =



−x1

1 − e
−x
2
1−x
2
2

+ x2e
−x
2
1−x
2
2
−x1e
−x
2
1−x
2
2 − x2

1 − e
−x
2
1−x
2
2




| {z }
a(x)
+



1 0
0 1



| {z }
B(x)
u
with the single equilibrium point xeq = 0. Figure 5.31 shows the trajectories
of the system for u = 0. A possible control Lyapunov function is5.5. Control Lyapunov Functions 471
V (x) = 1
2
￾
x
2
1 + x
2
2

.
We obtain
Vx
T
(x) = [x1 x2]
for the gradient and hence
B
T
Vx(x) = "
x1
x2
#
.
Since B
T
Vx(x) 6= 0 is valid for all x 6= 0, according to Theorem 76 the
function V is a control Lyapunov function. This is also directly apparent from
V˙ (x) = −
￾
x
2
1 + x
2
2


1 − e
−x
2
1−x
2
2

+ x1u1 + x2u2.
From Theorem 77, we obtain
ks(x) = −

1 − e
−x
2
1−x
2
2

+
q￾
1 − e
−x
2
1−x
2
2
2
+ 1,
and Sontag’s control law takes the form
u(x) = 
1 − e
−x
2
1−x
2
2

−
q￾
1 − e
−x
2
1−x
2
2
2
+ 1 
x.
Figure 5.32 shows the trajectories of the controlled system and Figure 5.33
the time courses representing x1 of the controlled system and the uncontrolled
one, which both start at an initial value of x(0) = [0.2 0.2]T
. Compared to
the markedly oscillating plant, the control loop no longer has a tendency to
oscillate.
State x1
State
x2
-2 -1 0 1 2
-2
-1
0
1
2
s
Fig. 5.31: Trajectories of the uncon￾trolled system
State x1
State
x2
-2 -1 0 1 2
-2
-1
0
1
2
s
Fig. 5.32: Trajectories of the control
472 Chapter 5. Nonlinear Control of Nonlinear Systems
0.2
0
-0.2
0 2 4 6 8 10 12 14 16 18 20
State
x1
Time t
Controlled
Uncontrolled
Fig. 5.33: Time courses of state variable x1 in the controlled case and the uncon￾trolled one
5.5.5 Example: Power Plant with Grid Feed-In
Below we will describe a power plant generating electricity and connected
to the power supply grid via a long high-voltage transmission line; we will
examine its behavior in the case of a short circuit [11, 15, 254]. The power
supply grid is assumed to be rigid, i. e. its frequency is constant and the
stability of the grid is not affected by the power plant in question. For example,
hydropower plants in remote regions are connected to the power grid by means
of such long high-voltage transmission lines, as shown in Figure 5.34.
The transmission line is connected in series with an adjustable capacitive
reactance XC [150, 284, 437]. The sum of the transient inductive reactance
of the generator, the inductive reactance of the transformer, and the reac￾tance of the transmission line is denoted as XL. Further, E¯ = Eejδ is the
generator voltage, more precisely the pole-wheel voltage, and V¯ = V ej0
is the
voltage of the rigid network. Figure 5.35 shows the equivalent circuit diagram
of the system. The power angle δ consists of the load angle of the synchronous
generator and the phase shift resulting from the transmission line.
The active power transmitted via the line is
P =
E · V
XL − XC
sin(δ). (5.187)
Now the significance of the additional capacitor in the system also becomes
clear. It increases the transportable power P by reducing the denominator
XL − XC in equation (5.187).
The capacity has a further function. Since it is adjustably designed ac￾cording to
XC = XC0 + ∆XC , (5.188)
it can be used to correct system faults as quickly as possible after a trans￾mission line malfunction such as a short circuit. With equation (5.188), we
obtain5.5. Control Lyapunov Functions 473
Fig. 5.34: A hydropower plant connected via a long transmission line to the rigid
grid
G
E¯ =E · e
jδ XL XC V¯ =V · e
j0
Rigid grid
Fig. 5.35: Generator with long transmission line to the power grid
P =
E · V
XL − XC0
(1 + u) sin(δ) (5.189)
with
u =
∆XC
XL − XC0 − ∆XC
for equation (5.187).
In stationary operation, the relation474 Chapter 5. Nonlinear Control of Nonlinear Systems
ωe = pωm (5.190)
applies to the mechanical angular velocity ωm of the pole wheel of the syn￾chronous machine with the pole pair number p and to the frequency ωe of the
pole-wheel voltage E sin (ωet + δ).
If temporal changes in the angle δ occur because of a short circuit or for
other reasons, the frequency of the pole-wheel voltage E sin (ωet + δ) and thus
the mechanical frequency ωm of the pole wheel changes according to
ωe + ˙δ = p(ωm + ∆ωm).
Inserting equation (5.190), we obtain as the change in the mechanical fre￾quency of the pole wheel
∆ωm =
˙δ
p
.
We will now examine the system’s power balance. The turbine power PT,
from which we substract the damping power loss
D∆ωm =
D
p
˙δ
and the power accelerating or decelerating the rotor
J(ωm + ∆ωm)(∆ωm)˙ ≈ Jωm(∆ωm)˙ = Jωe
p
2
¨δ,
is equal to the electrical power P from equation (5.189) generated by the
synchronous generator and transported via the transmission line. Here D is
a damping constant and J is the moment of inertia of all the turbine’s and
generator’s rotating parts. Thus,
PT − D∆ωm − Jωm(∆ωm)˙ = PE(1 + u) sin(δ), PE =
E · V
XL − XC0
,
applies to the power balance, from which
PT −
D
p
˙δ −
Jωe
p
2
¨δ = PE(1 + u) sin(δ)
and
¨δ =
p
2
Jωe

PT −
D
p
˙δ − PE(1 + u) sin(δ)

(5.191)
follow.
The stationary operating point5.5. Control Lyapunov Functions 475
δeq = arcsin 
PT
PE

of the system is obtained from equation (5.191) if we set ¨δ = 0 and ˙δ = 0, as
well as u = 0. The angle δ typically varies within narrow bounds around the
operating point δeq. Only in the case of larger disturbances can higher values
δ ∈ [−π, π] occur. Changes in the angle δ are undesirable because, according to
equation (5.189), the transmitted electrical power P fluctuates. Furthermore,
large changes in the angle δ can cause the synchronous generator to be out
of step, which means the pole wheel no longer runs synchronously with the
stator’s rotating magnetic field.
We will define


x1
x2

 =


δ − δeq
˙δ


as the state vector, and furthermore the abbreviations
a1 =
p
2PT
Jωe
, a2 =
pD
Jωe
, and a3 =
p
2PE
Jωe
.
So from equation (5.191), we obtain the model of the power plant connected
to a rigid grid as


x˙ 1
x˙ 2

 =


x2
a1 − a2x2 − a3(1 + u) sin(x1 + δeq)

 . (5.192)
Here, u is the system’s control variable. System (5.192) has the equilibrium
points
xeq,2i =


±2iπ
0

, xeq,(2i+1) =


±(2i + 1)π − 2δeq
0

 for i = 0, 1, 2, . . .
for u = 0. Since the equilibrium points and the associated trajectories of
indices i > 0 repeat periodically due to the sine function in equation (5.192),
taking into account the equilibrium points
xeq0 =


0
0

 and xeq1 =


π − 2δeq
0


is sufficient. The stability of the equilibrium point xeq0 and the instability of
the equilibrium point xeq1 can be simply proven using Lyapunov’s indirect
method (Theorem 17 on p. 126), using the linearized model of system (5.192).
The aim is to quickly compensate for disturbances that occur because of
line faults or other reasons, which corresponds to x 6= 0. That is, we wish to476 Chapter 5. Nonlinear Control of Nonlinear Systems
quickly eliminate deviations from the equilibrium point xeq0 by means of a
controller.
To derive a control law for system (5.192), here we can employ the control
Lyapunov function
V (x) = 1
2
x
2
2 − a1x1 + a3(cos(δeq) − cos(δeq + x1)). (5.193)
The condition V (0) = 0 is satisfied, and V (x) > 0 is valid within a certain
neighborhood of the equilibrium point xeq0 = 0. However, the control Lya￾punov function does not fulfill the condition V (x) > 0 for higher values of
x1 > 0. From a practical point of view, however, this is not necessary either,
because no higher values of x1 = δ − δeq occur. Figure 5.36 illustrates the
graph of the function V (x) depending on the state variable x1. Here we will
restrict ourselves to the case in which x2 = 0, because only for the component
dependent on x1, the course of V (x) is not directly evident. By contrast, the
term 0.5x
2
2
takes the form of a parabola.
With equation (5.192) and equation (5.193), we obtain
V˙ (x) = −a2x
2
2 − u · a3x2 sin(δeq + x1) (5.194)
for the derivative of the control Lyapunov function. We choose u in equation
(5.194) as
u = kx2 sin(δeq + x1), (5.195)
so that
V˙ (x) = −a2x
2
2 − a3kx2
2
sin2
(δeq + x1) ≤ 0
is fulfilled. Only the inequality V˙ (x) ≤ 0 and not the stricter condition V˙ (x) <
0 holds here, because V˙ (x) = 0 holds for the set {x | x1 ∈ IR, x2 = 0}.
However, since only the state vector x = 0 in this set satisfies the differential
x1 = δ − δeq in rad
V
80
40
0
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2
Fig. 5.36: Graph of the control Lyapunov function V (x) for x2 = 0 with δeq =
1.05 rad = 60.41◦
, a1 = 43.196, and a3 = 49.6765.5. Control Lyapunov Functions 477
equation (5.192), V˙ (x) is not identical to zero along any trajectory x(t) except
for the trivial case in which x(t) = 0. According to Definition 36, the function
V (x) is an extended control Lyapunov function, meaning that the stability of
the equilibrium point xeq = 0 is ensured. The value k > 0 is a freely selectable
parameter.
With its original coordinates δ and ˙δ, using x1 = δ − δeq and x2 = ˙δ, the
control law (5.195) takes the form
u = k
˙δ sin(δ). (5.196)
We will now present an example with PT = 540 MW, PE = 621 MVA,
D = 4.6889MWs, ωe = 2π · 50 s−1
, p = 1, and J = 39792 kg m2
, which results
in
a1 =
PT
Jωe
= 43.196 s−2
, a2 =
D
Jωe
= 0.375 s−1
, a3 =
PE
Jωe
= 49.676 s−2
.
A three-phase short circuit from t = 4 s to t = 4.06 s can be viewed as a test
case. In this time interval, the transmitted power is
P = PE sin(δ) = 0.
For the controller constant, we will select the value k = 0.075.
Figure 5.37 shows the time courses of the angle δ = x1 + δeq and the
trajectories [δ(t)
˙δ(t)]T
for system (5.192), which is controlled by applying
control law (5.195), or for system (5.191), which is controlled by applying
control law (5.196), and for the free system. It is evident that both the free
and the controlled system return to the equilibrium point, i. e. the operating
point δeq = 1.05 rad = 60.41◦
, after the short circuit ends. The control system
ensures that, in contrast to the free system, in this system the angle δ and
thus the transmitted power P = P E sin(δ) barely oscillate.
Time t in s
δ in rad
Power angle δ in rad
˙δ in rad s
−1
1.8
1.6
1.4
1.2
1
1
0.8
0.6
3
2
0
-1
-2
-3
0 5 10 15 20 0.6 0.8 1 1.2 1.4 1.6 1.8
s
Fig. 5.37: Time courses of the power angle δ = x1+δeq and the associated trajectories
[δ(t)
˙δ(t)]T
for the uncontrolled system (black) and the control loop (blue)478 Chapter 5. Nonlinear Control of Nonlinear Systems
5.6 The Backstepping Method
5.6.1 Fundamentals
The backstepping procedure enables us to determine controllers and Lyapunov
functions for nonlinear plants possessing the form
x˙ 1 = f 1
(x1) + h1(x1) · x2,
x˙ 2 = f2(x1, x2) + h2(x1, x2) · x3,
x˙ 3 = f3(x1, x2, x3) + h3(x1, x2, x3) · x4,
.
.
.
x˙ k = fk(x1, x2, . . . , xk) + hk(x1, x2, . . . , xk) · u
where x1 ∈ IRn
, x2, . . . , xk, u ∈ IR holds. The form of the above systems is
called the strict feedback form. Such systems constitute a subclass of control￾affine systems. Figure 5.38 illustrates the system structure of the plant.
If x1 is scalar, the output y = x1 is flat, and consequently such systems in
strict feedback form are flat. To prove this, we replace x1 with y in all equations
above and then insert the first equation into the second by eliminating x2. In
the second equation x3 now only appears in dependence on y, y˙, and y¨. When
this is solved for x3, we can insert the result into the third equation and so
on, until we obtain both the state vector
x
T =
h
x
T
1 x2 · · · xk
i
and the input variable u as functions of y, y, . . . , y ˙
(k)
. According to Theorem
50 on p. 251, the strict feedback form’s controllability follows from its flatness
in the SISO case.
As it is relatively simple to control control-affine systems using feedback
linearization, the question arises: why would another method, such as the
System component k System component 2 System component 1
u
xk
x1
x1
x2
x2
x3
x1
1
s
1
s
1
s
I
xk−1
f 1
h1
f2
h2
fk
hk
.
.
.
Fig. 5.38: Structure of a system in strict feedback form5.6. The Backstepping Method 479
backstepping method or the approaches based on control Lyapunov functions
discussed in the previous section, be useful? One answer is that feedback lin￾earization depends on an exact model of the plant. If there is a discrepancy
between the plant and the model, we cannot achieve exact linearization. In
this case, the control loop would be nonlinear and the feedback lineariza￾tion method would fail in its objective. A second disadvantage of feedback
linearization is that this method sometimes linearizes useful nonlinearities in
the plant, involving an unnecessarily high actuating energy. The backstepping
method and control Lyapunov functions both enable us to make suitable non￾linearities in the system usable for the control and also to render the control
robust against inaccuracies in the plant model [251].
For the determination of the controller u(x) and the Lyapunov function
V (x), we will begin our discussion with the special case
x˙ 1 = f(x1) + h(x1) · x2, (5.197)
x˙ 2 = u. (5.198)
Figure 5.39 shows the structure of the system. The state variable x2 is now
taken as the input variable of system (5.197). A continuously differentiable
control law
x2 = α(x1) with α(0) = 0 (5.199)
is assumed to be known, so that x1 = 0 is an asymptotically stable equilibrium
point in the α-controlled system. Here, x2 is obviously not the real control
variable. Rather, the state x2 is used temporarily as a virtual control variable
to derive the actual control law u(x1, x2).
Furthermore, a Lyapunov function V (x1) is assumed to be known for
system (5.197), which is controlled by the virtual control law (5.199); this
means the inequality
V˙ (x1) = ∂V
∂x1
(f(x1) + h(x1) · α(x1)) < 0
u x2 x˙ 1 1 x1
s
h(x1) I
f(x1)
1
s
Fig. 5.39: Structure of the system x˙ 1 = f(x1) + h(x1) · x2 with x˙ 2 = uPSfrag 480 Chapter 5. Nonlinear Control of Nonlinear Systems
u x2 x˙ 1 x1
α(x1)
1
s
h(x1) I
f(x1)+h(x1)α(x1)
1
s
Fig. 5.40: System with a virtual control law α(x1), which is equivalent to the system
from Figure 5.39
is fulfilled. To identify a Lyapunov function V (x1) of this kind and a controller
α(x1), we can use the method of control Lyapunov functions from the previous
section, among other methods.
The system consisting of equations (5.197) and (5.198) can be represented
in the form
x˙ 1 = f(x1) + h(x1) · α(x1) + h(x1) (x2 − α(x1)), (5.200)
x˙ 2 = u. (5.201)
The associated block diagram is shown in Figure 5.40. Note that the system
representations (5.200) and (5.197) are equivalent.
Using the transformation
z = x2 − α(x1),
the system equations (5.200) and (5.201) are converted into another equivalent
form yielding
x˙ 1 = f(x1) + h(x1)α(x1) + h(x1) · z,
z˙ = u − α˙(x1),
(5.202)
whose structure is shown in Figure 5.41. The transformation shifts the virtual
control law α(x1) preceding the integrator, which lends this method the name
backstepping or integrator backstepping.
We will now specify a Lyapunov function for the entire system (5.202)
using the Lyapunov function V (x1) and the additional term 0.5z
2
, which
yields
V tot(x1, x2) = V (x1) + 1
2
z
2 = V (x1) + 1
2
(x2 − α(x1))2
.
For the system (5.202), the derivative of this potential Lyapunov function is
given byPSfrag 5.6. The Backstepping Method 481
u z x˙ 1 x1
α˙(x1)
1
s
h(x1) I
f(x1)+h(x1)α(x1)
1
s
Fig. 5.41: System structure of Figure 5.40 where the virtual control law is shifted
backward to the input of the integrator
V˙
tot(x1, x2)= ∂V
∂x1
x˙ 1+zz˙
=
∂V
∂x1
(f(x1)+h(x1)α(x1))
| {z }
< 0
+
∂V
∂x1
h(x1)z+z (u − α˙(x1)).
We note that the Lyapunov function’s derivative V˙
tot depends on u.
We will now select
u = ˙α(x1) −
∂V
∂x1
h(x1) − k · z, k > 0, (5.203)
as the control law such that
V˙
tot(x1, x2) = ∂V
∂x1
(f(x1) + h(x1)α(x1)) − kz2 < 0
holds for all x1. Further, with
x˙ 1 = f(x1) + h(x1)x2,
we next obtain
u = ˙α(x1) −
∂V
∂x1
h(x1) − k · z
=
∂α
∂x1
x˙ 1 −
∂V
∂x1
h(x1) − k (x2 − α(x1))
=
∂α
∂x1
(f(x1) + h(x1)x2) −
∂V
∂x1
h(x1) − k (x2 − α(x1))
from equation (5.203). Here, k is a freely selectable positive parameter which
influences the dynamics of the control loop. High values of k lead to a fast
decrease in V tot and thus usually to a faster control.
With V tot, we have identified a Lyapunov function, which happens to be
a control Lyapunov function, and with the function u defined above a control
law for the plant (5.197), (5.198) has been identified as well. We can summarize
the above results in482 Chapter 5. Nonlinear Control of Nonlinear Systems
Theorem 78 (Simple Backstepping). Let there be a system
x˙ 1 = f(x1) + h(x1) · x2, (5.204)
x˙ 2 = u. (5.205)
We will assume that a virtual control law x2 = α(x1) with α(0) = 0 is known
for the subsystem (5.204) such that the virtual control loop has an asymptoti￾cally stable equilibrium point x1eq = 0. If the latter is the case and a Lyapunov
function V (x1) is known for the virtual control loop, then the control law
u =
∂α(x1)
∂x1
( f(x1) + h(x1) x2) −
∂V
∂x1
h(x1) − k (x2 − α(x1))
with an arbitrary k > 0 stabilizes the system’s equilibrium point

x
T
1eq x2eqT
= 0
asymptotically, and
V tot(x1, x2) = V (x1) + 1
2
(x2 − α(x1))2
is a Lyapunov function for the entire control loop.
If V (x1) → ∞ also applies to |x1| → ∞ in the theorem above, the equilibrium
point of the control system is globally asymptotically stable.
The result above is only moderately useful in the form shown above, be￾cause for the theorem to be applied it is necessary to find the control law
α(x1) and the Lyapunov function V (x1) for the subsystem (5.204), which
constitutes the essential part of the system (5.204), (5.205). Thus, we would
certainly find a controller for this system even without the theorem. The re￾sult, however, assuming V and α are known, provides the starting point for a
valuable result to be derived.
For this purpose, we will now deal with the system
x˙ 1 = f 1
(x1) + h1(x1) · x2,
x˙ 2 = f2(x1, x2) + h2(x1, x2) · u.
(5.206)
By applying
u =
u2 − f2(x1, x2)
h2(x1, x2)
, h2(x1, x2) 6= 0,
we transform it to
x˙ 1 = f 1
(x1) + h1(x1) · x2,
x˙ 2 = u2.
For this transformed system, the control law u2(x1, x2) and the Lyapunov
function V (x1) are known from Theorem 78. Therefore the control law u for
the system (5.206) can also be directly specified by using5.6. The Backstepping Method 483
Theorem 79 (Backstepping). Let there be a system
x˙ 1 = f 1
(x1) + h1(x1) · x2, (5.207)
x˙ 2 = f2(x1, x2) + h2(x1, x2) · u.
We will assume that a virtual control law x2 = α(x1) with α(0) = 0 is known
for the subsystem (5.207) such that the virtual control loop has an asymptoti￾cally stable equilibrium point x1eq = 0. If the latter is the case and a Lyapunov
function V (x1) is known for the virtual control loop, then the control law
u =
1
h2(x1, x2)
·

∂α(x1)
∂x1
(f 1
(x1) + h1(x1)x2)
−
∂V
∂x1
h1(x1) − k (x2 − α(x1)) − f2(x1, x2)

with an arbitrary k > 0 stabilizes the system’s equilibrium point
[x
T
1eq x2eq]
T = 0
asymptotically, and
V tot(x1, x2) = V (x1) + 1
2
(x2 − α(x1))2
is a Lyapunov function for the entire control loop.
5.6.2 Recursive Scheme for the Controller Design
Based on the theorems above, for the systems described previously which take
the form
x˙ 1 = f 1
(x1) + h1(x1) · x2,
x˙ 2 = f2(x1, x2) + h2(x1, x2) · x3,
.
.
.
x˙ k = fk(x1, . . . , xk) + hk(x1, . . . , xk) · u,
we can now determine control laws u(x) and Lyapunov functions V (x). For
this purpose, we will proceed with the following recursive design process in
steps:
Step 1: Consider the subsystem
x˙ 1 = f 1
(x1) + h1(x1) · x2,
x˙ 2 = f2(x1, x2) + h2(x1, x2) · x3,
which we will call T1. Design a virtual controller x3(x1, x2) and
a Lyapunov function V1(x1, x2) using Theorem 79 or, if possible,
Theorem 78.484 Chapter 5. Nonlinear Control of Nonlinear Systems
Step 2: Summarize the subsystem T1 to formulate a differential equation
x˜˙
1 =


x˙ 1
x˙ 2

 =


f 1
(x1) + h1(x1) · x2
f2(x1, x2) + h2(x1, x2) · x3

,
and then formulate subsystem T2:
x˜˙
1 =


f 1
(x1) + h1(x1) · x2
f2(x1, x2)

 +


0
h2(x1, x2)

x3,
x˙ 3 = f3(x1, x2, x3) + h3(x1, x2, x3) · x4.
This subsystem T2 consists of the first three differential equations of
the total system. Its form corresponds to what we saw in Theorem
79. Since a virtual controller x3(x1, x2) and a Lyapunov function
V1(x1, x2) are known from the first step, another virtual control
law x4(x1, x2, x3) and a Lyapunov function V2(x1, x2, x3) for T2
can be derived using Theorem 79.
Step 3: Summarize subsystem T2 as a differential equation
xˆ˙
1 =




x˙ 1
x2
x3




=




f 1
(x1) + h1(x1) · x2
f2(x1, x2) + h2(x1, x2) · x3
f3(x1, x2, x3) + h3(x1, x2, x3) · x4




and formulate subsystem T3:
xˆ˙
1 =




f 1
(x1) + h1(x1)x2
f2(x1, x2) + h2(x1, x2)x3
f3(x1, x2, x3)




+




0
0
h3(x1, x2, x3)




x4,
x˙ 4 = f4(x1, x2, x3, x4) + h4(x1, x2, x3, x4) · x5.
Subsystem T3 corresponds to the system in Theorem 79. Also,
a virtual controller x4(x1, x2, x3) and a Lyapunov function
V2(x1, x2, x3)are known from Step 2, so that a control law
x5(x1, x2, x3, x4) and a Lyapunov function V3(x1, x2, x3, x4) can be
derived for T3 using Theorem 79.
Step 4: ...
.
.
.
Step k − 1: ...5.6. The Backstepping Method 485
Steps 2 and 3 are almost identical; Step 4 and all subsequent steps not
listed here also correspond to their predecessors, so that it is possible to ad￾vance recursively from the subsystem T1 to the subsystem T2, from there to
T3, then to T4, and so on. The sequence below illustrates this:
x˙ 1 = f 1
(x1) + h1(x1) · x2
x˙ 2 = f2(x1, x2) + h2(x1, x2) · x3
x˙ 3 = f3(x1, . . . , x3) + h3(x1, . . . , x3) · x4
x˙ 4 = f4(x1, . . . , x4) + h4(x1, . . . , x4) · x5
.
.
.
x˙ k = fk(x1, . . . , xk) + hk(x1, . . . , xk) · u .
)
T1



T2



T3
. . .



Tk−1
Ultimately, after k−1 steps, we have derived the control law u we were seeking,
and also a Lyapunov function of the entire control loop.
The advantage of the backstepping method is its systematic design tech￾nique, which leads to a stable and robust control loop. Its disadvantage is that
the control performance can only be predicted or influenced to a limited ex￾tent. Extensions of and additions to the backstepping procedure can be found
in [251, 252, 411, 500].
5.6.3 Illustrative Examples
As a preliminary example, we will examine the plant
x˙ 1 = x
2
1 − x1 + x2, (5.208)
x˙ 2 = u. (5.209)
For the design of a controller u(x1, x2) and a Lyapunov function V tot(x1, x2),
we will apply Theorem 78. First, a controller
x2 = α(x1) = −x
2
1 − x1
is designed for the subsystem (5.208). Clearly, this controller stabilizes the
subsystem (5.208), because inserting it into equation (5.208) yields
x˙ 1 = −2x1.
Furthermore,
V (x1) = 1
2
x
2
1
is a Lyapunov function of the virtual control loop which proves that the loop
is globally asymptotically stable, because
V˙ (x1) = x1x˙ 1 = −2x
2
1 < 0486 Chapter 5. Nonlinear Control of Nonlinear Systems
State x1
State
x2
-5 5
8
4
4
0
0
-4
-4
-8
2
0
0
0
-2
-20
20
40
60
x1 x2
Vtot
s
Fig. 5.42: Trajectories of the plant and Lyapunov function Vtot from the first example
holds for all points x1 6= 0. From Theorem 78, we now derive the control law
u =
∂α
∂x1
￾
x
2
1 − x1 + x2

−
∂V
∂x1
− k(x2 − α(x1))
for the entire system consisting of equations (5.208) and (5.209), where k > 0
holds. With
∂α
∂x1
=
∂(−x
2
1 − x1)
∂x1
= −(2x1 + 1)
and
∂V
∂x1
= x1,
the control law
u = −(2x1 + 1)(x
2
1 − x1 + x2) − 2x1 − x
2
1 − x2
= −x1 − 2x
3
1 − 2x2 − 2x1x2
(5.210)
is obtained for k = 1. For the entire control loop (5.208), (5.209), (5.210), the
Lyapunov function is
V tot(x1, x2) = V (x1) + 1
2
(x2 − α(x1))2 =
1
2
x
2
1 +
1
2
￾
x1 + x
2
1 + x2
2
.
Figure 5.42 shows the trajectories of the plant and the form of the Lyapunov
function Vtot.
In the second example, we will begin with the controlled system from the
first example and add an integrator to it, so th5.6. The Backstepping Method 487
x˙ 1 = x
2
1 − x1 + x2, (5.211)
x˙ 2 = x3, (5.212)
x˙ 3 = u. (5.213)
For the design of a control law u(x1, x2, x3), we can again apply Theorem 78.
To design the controller, we first split the system denoted by (5.211), (5.212),
(5.213) into two subsystems, i. e. into the subsystem T1 consisting of equations
(5.211) and (5.212), and the total system T2 consisting of equations (5.211),
(5.212), (5.213), such that we obtain


x˙ 1
x˙ 2

 =


x
2
1 − x1 + x2
0


| {z }
f(x1, x2)
+


0
1


|{z}
h(x1, x2)
· x3,
x˙ 3 = u.



T1



T2
For subsystem T1, from equation (5.210) in the first example, we know
that a controller exists, namely
x3 = α(x1, x2) = −x1 − 2x
3
1 − 2x2 − 2x1x2,
as well as a Lyapunov function
V (x1, x2) = 1
2
x
2
1 +
1
2
(x1 + x
2
1 + x2)
2
.
Thus Theorem 78 immediately yields the control law
u =
∂α(x1, x2)
∂[x1 x2]
T
( f(x1, x2) + h(x1, x2) · x3)
−
∂V (x1, x2)
∂[x1 x2]
T
· h(x1, x2) − k (x3 − α(x1, x2))
=
∂α
∂x1
￾
x
2
1 − x1 + x2

+
∂α
∂x2
x3 −
∂V
∂x2
− k(x3 − α(x1, x2))
for the total system T2 with its system equations (5.211), (5.212), (5.213). For
k = 1 and
∂α(x1, x2)
∂x1
= −6x
2
1 − 2x2 − 1,
∂α(x1, x2)
∂x2
= −2x1 − 2,
∂V (x1, x2)
∂x2
= x2 + x1 + x
2
1
,
the control law, of which we see a simulation in Figure 5.43, can now be
formulated a488 Chapter 5. Nonlinear Control of Nonlinear Systems
u = −x1 − 2x
2
1 + 4x
3
1 − 6x
4
1 − 4x2 − 8x
2
1x2 − 2x
2
2 − 3x3 − 2x1x3.
A simulation of the control loop is shown in Figure 5.43, where the vector of
the initial values is
x0 = [1 1.5 1]T
.
For the Lyapunov function of the entire control loop, we obtain
V tot(x1, x2, x3) = V (x1, x2) + 1
2
(x3 − α(x1, x2))2
=
1
2
x
2
1 +
1
2
￾
x1 + x
2
1 + x2
2
+
1
2
￾
x3 + x1 + 2x
3
1 + 2x2 + 2x1x2
2
.
0
0
0
0
1
2
3
8
6
4
2
2
-2
-2
-4
-6
10
-10
-20
-30
0
0
0
0
1
1
1
1
2
2
2
2
3
3
3
3
4
4
4
4
5
5
5
5
6
6
6
6
7
7
7
7
x1 x2 x3 u
Time t
Controlled
Uncontrolled
Fig. 5.43: Time courses of the state variables x1, x2, and x3 and the control variable
u from the second examp5.6. The Backstepping Method 489
On the one hand, we recognize the integrating character of the plant: x3 is
constant, x2 increases linearly, and x1 increases at a greater than quadratic
rate. Thus, the system is unstable. As can be seen from the simulation, and
as expected from the design, the control stabilizes the system. In addition, we
have achieved high control performance.
5.6.4 Example: Fluid System with Chaotic Behavior
We will design a backstepping controller for an experimental fluid system
[87, 416, 470] consisting of a water-filled toroidal tube. The lower half of
this torus is electrically heated using a filament. The filament is wrapped
tightly around the torus and surrounded by insulating material so that the
lower half of the torus is uniformly heated. The upper half of the torus is
surrounded by a ring-shaped cooling jacket through which cooling water flows.
The temperature of the torus wall of the entire upper half is thus maintained
at a constant level. Figure 5.44 shows the configuration. The torus has a radius
of r1 = 38 cm and the radius of the torus tube is r2 = 1.5 cm.
Sensors are mounted at positions A, B, C, and D to measure the temper￾ature of the water contained in the torus at these points. At a low heating
power, meaning less than 190 W, there is a continuous flow of water inside
the torus. The water flows clockwise or counterclockwise, depending on the
A B
C
D
2r2 r1
Fig. 5.44: Fluid system with chaotic behavior. The inner toroidal tube contains
water. The upper half of the torus is cooled, the lower half heated.490 Chapter 5. Nonlinear Control of Nonlinear Systems
initial values. If the heating power is now increased to 600 W, for example,
there is no more continuous water flow within the torus. Under this condition,
the water flow continually changes its direction. Sometimes the water flows
clockwise, and sometimes it flows counterclockwise. The changes in direction
of the water flow are chaotic, not predictable.
The emergence of this chaotic behavior can be explained as follows. If the
speed of the water flow is reduced by a small irregularity, the water remains
longer in the heating or cooling area of the torus. This increases the temper￾ature difference between the cold water in the upper half of the torus and the
hot water in the lower half. As a result, the flow velocity increases due to the
increased temperature difference. This leads to a more rapid mixing of the
hot water from the lower half with the cold water from the upper half. Thus,
the temperature difference between the cold and hot water decreases again,
causing a decrease in the flow velocity as well and, in some cases, changes in
its direction.
The above behavior leads to oscillations and finally to the chaotic behavior
mentioned above. It can be described by the Lorenz equations
z˙1 = p(z2 − z1),
z˙2 = −z1z3 − z2, (5.214)
z˙3 = z1z2 − z3 − R.
Here, the variable z1 is a measure of the mean flow velocity, z2 is a measure
of the temperature difference T B − T A between points A and B, and z3 is a
measure of the temperature difference T C − T D. Furthermore, p > 0 is the
Prandtl number[7] and R is the Rayleigh number[8]
.
The Rayleigh number R is proportional to the heating power of the fila￾ment, so that we can control the system via
R = R0 + u.
The parameter R0 is proportional to a constant and u to a superimposed,
variable heating power.
For the control variable value u = 0, the system has the three equilibrium
points
zeq1 =




p
R0 − 1
p
R0 − 1
−1




, zeq2 =




−
p
R0 − 1
−
p
R0 − 1
−1




, zeq3 =




0
0
−R0




.
[7] The Prandtl number p = ν/a is a constant dimensionless characteristic number
describing fluids and is equal to the quotient of kinematic viscosity ν and thermal
diffusivity a.
[8] The temperature-dependent dimensionless Rayleigh number is an indicator of the
type of heat transfer in a fluid. Below a critical Rayleigh number, the heat transfer
in the fluid occurs primarily by heat conduction; above this value it occurs by
convection. The Rayleigh number indicates the stability of thermal fluid layers.5.6. The Backstepping Method 491
We will now control the system (5.214) such that zeq1 is an asymptotically
stable equilibrium point. For this purpose, the first step is to transform the
coordinates of the system so that the equilibrium point zeq1 is shifted to the
origin. This is done by means of the coordinate transformation x = z − zeq1,
i. e.




x1
x2
x3




=




z1 −
p
R0 − 1
z2 −
p
R0 − 1
z3 + 1




.
p
The transformed system has the equilibrium point xeq = 0, and with β =
R0 − 1 it takes the form
x˙ 1 = −px1 + px2, (5.215)
x˙ 2 = x1 − x2 − (x1 + β)x3, (5.216)
x˙ 3 = x1x2 + β(x1 + x2) − x3 − u. (5.217)
The system is in strict feedback form, so we can directly apply the backstep￾ping design theorems.
We will begin with the selection of the virtual control law
x2 = α1(x1) = 0,
which stabilizes subsystem (5.215). As a Lyapunov function, we choose
V1(x1) = 1
2
x
2
1
.
With x2 = α1(x1) = 0 and equation (5.215), the inequality
V˙
1(x1) = −px2
1 < 0
follows. We will now apply Theorem 79 to the subsystem (5.215), (5.216), i. e.
x˙ 1 = −px1
| {z }
f1(x1)
+ p
|{z}
h1(x1)
· x2,
x˙ 2 = x1 − x2
| {z }
f2(x1, x2)
−(x1 + β)
| {z }
h2(x1, x2)
x3,
yielding
x3 = α2(x1, x2) = −
1
x1 + β
(−px1 − k1x2 − x1 + x2)
as a virtual control law. We will now set k1 = 1, resulting in
x3 = α2(x1, x2) = (1 + p)x1
x1 + β
,492 Chapter 5. Nonlinear Control of Nonlinear Systems
and
V2(x1, x2) = 1
2
(x
2
1 + x
2
2
)
for the Lyapunov function.
In the next step, Theorem 79 is applied again, but now the focus is on the
complete system consisting of equations (5.215), (5.216), (5.217).
With the terms used in Theorem 79, we can formulate the system descrip￾tion


x˙ 1
x˙ 2

 =


−px1 + px2
x1 − x2


| {z }
f 1
(x1, x2)
+


0
−(x1 + β)


| {z }
h1(x1, x2)
x3,
x˙ 3 = x1x2 + β(x1 + x2) − x3
| {z }
f2(x1, x2, x3)
−1
|{z}
h2(x1, x2, x3)
· u.
After doing this, and including
∂α2
∂[x1 x2]
T
=

(1 + p)β
(x1 + β)
2
0

as well as
∂V2
∂[x1 x2]
T
= [x1 x2],
we obtain the expression
u = −x3 + k2x3 − k2
(1 + p)x1
x1 + β
+ βx1 −
p(1 + p)β(x2 − x1)
(x1 + β)
2
for the control law. With k2 = 1, the control law can finally be written as
u = βx1 −
(1 + p)x1
x1 + β
−
βp(1 + p)(x2 − x1)
(x1 + β)
2
.
Now we will simulate both the uncontrolled and the controlled process for
the parameters
p = 10 and β = 6.
The initial values are
x1 = 5, x2 = 5, and x3 = 5.
The resulting time courses for the flow velocity x1 are shown in Figure 5.45,
in which we can clearly see that the control suppresses the chaotic behavior
and compensates the process to the stationary value.5.7. Exercises 493
10
-5
-10
-15
-20
-25
0 5 10 15 20 25 0
6
5 5
4
3
2
1
0
0
-1
-2
0.5 1 1.5 2 2.5
Time t in min Time t in min State
x1
State
x1
Fig. 5.45: Time courses of the deviation x1 of the flow velocity from the equilibrium
point of the chaotic process (black) and the control loop (blue). The diagram on the
right shows an enlarged view of the control process.
5.7 Exercises
Exercise 5.1 Let us examine the longitudinal dynamics of a motor vehicle
with the intention of designing a speed control using gain scheduling [332].
We obtain the motor vehicle’s equation of motion based on the forces acting
on it, which are shown in Figure 5.46. They are the air resistance
Fd =
1
2
ρcdAv2
,
where ρ is the air density, cd
is the drag coefficient of the vehicle, A is its front
surface, and v its velocity. The rolling resistance is
Fr = crmg cos(ϕ),
where cr is the rolling resistance coefficient, m is the mass of the vehicle, g the
gravitational acceleration, and ϕ the angle of the road’s slope. Furthermore,
we have to take into account the proportional gravitational force Fs acting
on the vehicle due to the slope. In addition, the driving force or the braking
force Fa act on the vehicle via the gas pedal or the brake.
(a) Formulate the state-space model for the vehicle, with x = v being simulta￾neously the state variable and the output variable. The angle of the slope
ϕ is a disturbance variable. The acceleration generated by the engine or
brake is represented by the control variable u = Fa/m.
(b) Calculate the parameterized linearization family with the velocity x as
the scheduling parameter β. In doing this and in the steps that follow, set
ϕ = 0.494 Chapter 5. Nonlinear Control of Nonlinear Systems
ρ = 1.2 kgm−3
g = 9.81 ms−2
m = 1440 kg
cr = 0.015
A = 2 m2
cd = 0.3
Fs
m
Fr
Fd
Fa
ϕ
g
v
Fig. 5.46: Car on ascending road and average car parameters
(c) Now use the parameters from Figure 5.46 and determine three linearized
models for the operating points xop1 = 10 ms−1
, xop2 = 25 ms−1
, and
xop3 = 40 ms−1
. For each of the three linearized models, calculate a PI
controller
GPI(s) = P +
I
s
such that all eigenvalues of each of the linear control loops are exactly ten
times larger than the time constants of the linearized models in question.
(d) Identify the control law u(x) based on the weighted average, using Gauss
functions. Select the parameters σi of the Gauss functions such that σi
in
each case equals half the distance from one operating point xop,i to the
next.
Exercise 5.2 Let us examine a spring-mass system with variable mass m(t),
as shown in Figure 5.47. The state-space representation is given by the fol￾lowing equations:
x˙ 1 = x2,
x˙ 2 = −
c
m(t)
x1 +
1
m(t)
u,
y = x1.5.7. Exercises 495
c
m(t) F
x1
Fig. 5.47: Spring-mass system
Here, c = 30 Nm−1
is the spring constant and u is the force being applied ex￾ternally to the mass m(t). Assume that we know the mass m(t), which varies
within the interval [0.1 kg, 1.1 kg]. The system is linear, but time-varying. Sys￾tems of this kind are called linear parameter-varying systems (LPV systems).
(a) For m = 0.1 kg and m = 1 kg, calculate the eigenvalues of the system.
(b) Design a gain-scheduling controller based on the weighted average. As
subcontrollers, select PID controllers with the transfer functions
H(s) = KP,i
TI,iTD,is
2 + TI,is + 1
TI,is
.
Use the mass m(t) as a scheduling parameter β; as operating points, select
m1 = 0.2 kg, m2 = 0.4 kg, m3 = 0.6 kg, m4 = 0.8 kg, and m5 = 1 kg. The
transfer function of the closed subcontrol loops should have the denom￾inator (s + 10)3
in all five cases. As weighting functions, select triangles
and ramps whose sum is one.
(c) Design a single PID controller whose control parameters depend continu￾ously on the mass m(t) such that the control loop’s transfer function has
the denominator (s + 10)3
.
Exercise 5.3 Let us examine a model to calculate the human heartrate [449]:
x˙ 1 = −
1
ε
(x
3
1 + x1x2 + x3), 0 < ε ≪ 1,
x˙ 2 = −2x1 − 2x2,
x˙ 3 = −x2 − 1 + u,
y = x3.
Here, ε is a parameter, e. g. ε = 0.2, x1 represents the current length of the
fibers of the heart muscle, x2 the tension of the fibers, and x3 the electric
potential. The latter is measurable and thus x3 is also the output variable.
The system also includes an input variable u with which the potential can be
controlled using a pacemaker. As it pumps, the heart muscle contracts during
the systolic phase, i.e. x1(t) decreases, and blood is pumped into the arteries.
Subsequently, during the diastolic phase, the muscle relaxes, x1(t) increases,
and the chambers of the heart take in fresh blood.
(a) Determine the relative degree of the system.
(b) Conduct an input-output linearization. To do so, identify the required dif￾feomorphism. If internal dynamics occur, select the diffeomorphism such496 Chapter 5. Nonlinear Control of Nonlinear Systems
that it is independent of u. Determine the transformed system and the
zero dynamics.
(c) Formulate the controller and the prefilter such that the control loop has
arbitrary linear dynamics.
Exercise 5.4 Assume that f(x) and g(x) are differentiable vector functions,
µ(x) and λ(x) are differentiable scalar functions, and c is a real number. Prove
the following:
(a) Lf c = 0,
(b) Lµf λ(x) = µ(x)Lf λ(x),
(c) Lf+gµ(x) = Lf µ(x) + Lgµ(x),
(d) Lf (µ(x) + λ(x)) = Lf µ(x) + Lf λ(x),
(e) Lf (µ(x)λ(x)) = µ(x)Lf λ(x) + λ(x)Lf µ(x),
(f) L[f,g]µ(x) = LfLgµ(x) − LgLf µ(x).
Exercise 5.5 Let the bilinear system[9]
x˙ = Ax + uBx,
y = c
T x
with the relative degree δ = n be given.
(a) Conduct an input-output linearization. Determine the controller r(x) and
the prefilter v(x).
(b) What disadvantage does the control law u(x) have in this case?
(c) Identify the diffeomorphism z = t(x) which transforms the system into
the nonlinear controller canonical form.
(d) State the transformed differential equation z˙ = f(z, u), y = g(z) of the
plant.
Exercise 5.6 In automated arc welding, an electric arc is used to melt both
the welding electrode and the part being worked on. As the electrode continues
to move, the molten material cools and bonds the two parts together. During
this process, the welding electrode is progressively consumed. To maintain
the electric arc and thus continue the welding process, the electrode requires
continual replenishment so that the length of the electric arc remains con￾stant. A shielding gas protects the welding process from being influenced by
the environment. Figure 5.48 shows the process. The welding process can be
described by the equations [450]
x˙ = a(x) + b(x) · u
y = c(x)
with
[9] A system is called bilinear if it is linear both in the state x and the control
variable u, but is nonlinear overall.5.7. Exercises 497
Wire feeder
Shielding
gas supply
Cooling water
Welding electrode
Gas shield
Electric arc
Fig. 5.48: Arc welding process
a(x) =


−k0x1
k1x1 + k2x
2
1
(l − x2) − ve

 , b(x) =


k0
0

 , c(x) = x2.
The electrical direct current in the electric arc will be represented by x1 and
the length of the electric arc by x2. Here, due to the application of the solder,
the length of the electric arc x2 is somewhat smaller than the distance l
between the tip of the electrode and the part being worked on. The control
signal u is the reference current, i. e. the input signal, of an underlying current
control. The process constants are the positive parameters k0, k1, k2, and the
wire feed speed ve of the electrode. Below, our aim will be to design a control
by means of input-output linearization.
(a) Determine the relative degree δ of the system.
(b) Determine the controller and the prefilter which linearize the system.
(c) Can the control be built as designed?498 Chapter 5. Nonlinear Control of Nonlinear Systems
Exercise 5.7 Let us consider an input-output linearized system with the
controlled external dynamics
x˙ 1 = −x1 (5.218)
and the internal dynamics
x˙ 2 = −x2 + x
2
2x1. (5.219)
(a) Show that the controlled external dynamics and the zero dynamics both
have a single equilibrium point, each of which lies at the origin and is
globally asymptotically stable.
(b) Show that the equilibrium point
[x1 x2]
T = 0
of the composite system (5.218), (5.219) is not globally asymptotically
stable.
Exercise 5.8 Let us extend the system
x˙ = f(x, u), x ∈ IRn−1
,
y = g(x)
(5.220)
by an integrator
u˙ = v.
This process is called dynamic extension[10]. Use v as the new input variable
and u as a state variable.
(a) Which system description does this yield?
(b) What is the lowest possible relative degree δ that the extended system
can have?
(c) Conduct an input-output linearization of the extended system. Assume
that the relative degree δ is maximum. State the transformed system
description, i. e. the control loop’s differential equation, and the control
law v(x, u).
(d) State the control law u(x) for the system (5.220).
Exercise 5.9 Prove the Jacobi identity
[f, [g, h]] + [h, [f, g]] + [g, [h, f]] = 0.
To do this, use equation (5.105) on p. 416.
[10] The case described in this exercise is the simplest possible dynamic extension. In
general, a dynamic system
η˙ = ρ(x, η) + q(x, η)v,
u = r(x, η) + s(x, η)v
is used to generate a dynamic extension [203].5.7. Exercises 499
Exercise 5.10 Let a system in the strict feedback form
x˙ 1 = f1(x1) + h1(x1) · x2,
x˙ 2 = f2(x1, x2) + h2(x1, x2) · x3,
x˙ 3 = f3(x1, x2, x3) + h3(x1, x2, x3) · x4,
.
.
.
x˙ n = fn(x1, x2, . . . , xn) + hn(x1, x2, . . . , xn) · u
with hi(x1, . . . , xi) 6= 0 for i = 1, . . . , n be given.
(a) Determine the relative degree for each case y = xi
, i = 1, . . . , n.
(b) For y = x2, determine the internal dynamics of the system.
Exercise 5.11 Let us examine a macroeconomic model with the interest rate
x1, the investment demand x2, and the price exponent x3, which is a measure
of the inflation rate [487]. The model takes the form
x˙ 1 = x1(x2 − a) + x3,
x˙ 2 = −x
2
1 − bx2 + 1 + u,
x˙ 3 = −x1 − cx3.
Here, the increase x˙ 2 in the investment demand can be influenced by the input
variable u. The constants a > 0, b > 0, and c > 0 are the system parameters.
(a) Demonstrate that the model for x1 6= 0 is full-state linearizable.
(b) Identify a linearizing output y.
(c) Transform the system description into the Brunovsky canonical form with
the new input variable v, and determine the corresponding transforma￾tions.
(d) Formulate the output variable y as a function dependent only on the input
variable v of the transformed system.
(e) State a feedforward control law v(t) such that
y(t) =



0, t < 0,
sin2
(t), 0 ≤ t ≤
π
2
,
1,
π
2
< t.
Exercise 5.12 Let us examine the airship shown in Figure 5.49, which is
also called a blimp and is often used for advertising purposes. We will limit
ourselves to its dynamics in the horizontal plane, as shown in Figure 5.50.
A blimp is typically powered by two main engines which allow it to move
forward and backward. An additional engine is mounted at the back to enable
the blimp to turn around its main axis and to fly in a curved trajectory. Larger
blimps have additional tail units at the fin, while these are usually not present
on smaller unmanned airships. Here, we will model the dynamics of a blimp
without tail units.500 Chapter 5. Nonlinear Control of Nonlinear Systems
Fig. 5.49: Airship
y
x
u1
u3
b
ϕ
2a
u2
Fig. 5.50: Variables and parameters of the blimp’s dynamics in the horizontal plane
Based on the horizontal positions x1 = x and x3 = y, the course angle
x5 = ϕ, the propulsive thrusts u1 and u2 of the right and left main engines,
and the propulsive thrust u3 of the back engine, we obtain the dynamic model
x˙ 1 = x2,
x˙ 2 =
(u1 + u2) cos(x5) + u3 sin(x5)
m
,
x˙ 3 = x4,
x˙ 4 =
(u1 + u2) sin(x5) − u3 cos(x5)
m
,
x˙ 5 = x6,
x˙ 6 =
a(u1 − u2) + bu3
J
,
y =




x1
x3
x5




.5.7. Exercises 501
Here, m is the mass of the blimp, J is its moment of inertia around the yaw
axis, a is the lever arm of the right and the left main engine in relation to the
yaw axis, and b is the lever arm of the back engine in relation to the yaw axis.
(a) Determine the vectorial relative degree δ of the blimp.
(b) Is the vectorial relative degree δ well-defined?
(c) Determine the feedback control law u(x) which completely decouples the
input and output variables and stabilizes and linearizes the control loop.
Exercise 5.13 Show that the system
x˙ =










1
x4
x
2
5
x5
0










u1 +










0
0
0
0
1










u2
is globally controllable, but not full-state linearizable.
Exercise 5.14 Show that a driftless system x˙ = B(x) · u is not full-state
linearizable, except in the special case in which it is omnidirectionally con￾trollable.
Exercise 5.15 Is the model (5.44), p. 381, of the lunar module Eagle full-state
linearizable?
Exercise 5.16 Let the plant
x˙ 1 =
√3 x2,
x˙ 2 = −
3
q
x
2
2
· x3,
x˙ 3 = x1x3 + u
with the flat output y = x1 be given.
(a) State the flat system representation.
(b) Draw a picture of the structure of the flatness-based feedback control
loop. This closed control loop should have three eigenvalues at s = −5.
Formulate all the necessary equations in detail.
Exercise 5.17 The increase in a species is dependent on its population and,
where resources are unlimited, it follows the simple law of growth
x˙ = λx,
where λ is the growth coefficient. Thanks to progress in agriculture, medicine,
science, engineering, etc., it has been possible for human beings to increase
this growth coefficient λ and to make it dependent on x2, the level of human502 Chapter 5. Nonlinear Control of Nonlinear Systems
knowledge. In a good approximation, the following applies to the growth x˙ 1
of the human population x1 on Earth [238]:
x˙ 1 = α(βx2 − x1)x1.
Here, βx2 is the maximum world population which receives a sufficient food
supply based on a certain level of knowledge x2 and the corresponding tech￾nologies. The level of human knowledge x2 increases with time, along with
the increasing human population x1. Here,
x˙ 2 = 0.04093x1x2
applies. For a good approximation starting at x1(0) = 108
humans in 500
B.C., we will set x2(0) = 0.01. In addition, α = 1 and β = 1 apply.
(a) If the population x1 is the output of this growth model, identify a flat
input of the system such that y = x1 is a flat output and the output
vector b(x) is not dependent on x. In doing so, take into account that
x1 > 0 applies.
(b) State the flat system representation.
(c) What quantity can we influence using the flat input u?
(d) Calculate the control u(y, y,˙ y¨) such that y = x1 remains constant.
(e) What statement can you make about the practical applicability of the flat
input if you take the result from (c) into account?
Let us now assume that the growth rate of the human population on Earth
can be directly influenced by means of a control variable u such that
x˙ 1 = uα(βx2 − x1)x1
applies. In practice, this can be achieved by means of improved education,
improved social security, protection from unemployment, old age pension
schemes, birth control, etc. Here, α = 1 and β = 1 still apply.
(f) Calculate a flat output yf .
(g) Determine the flat system representation.
(h) How should we select the flat control u in this case if we wish the human
population on Earth
y = x1 = x1,∞
to remain constant? In this case, what is the curve representing human
knowledge x2 over time?
Exercise 5.18 Is the model (5.44) of the lunar module Eagle, see Section
5.2.6 on p. 381, flat?
Exercise 5.19 Let us examine a general nonlinear system
x˙ = f(x, u),
y = g(x).
(5.221)5.7. Exercises 503
Show that the dynamically extended system
x˙ = f(x, u),
u
(k) = v, k ∈ IN,
y = g(x)
is flat and that y is a flat output if the system (5.221) is flat and has the flat
output y.
Exercise 5.20 Like healthy tissue, cancerous tumors require a supply of blood
in order to grow. The blood transports the nutrients and oxygen necessary for
cell growth. If the supply of blood to the tumor is stopped by preventing the
growth of blood vessels to the tumor, it can no longer grow. A medication with
this effect prevents the new growth of blood vessels, or vascular angiogenesis,
around the tumor and causes atrophy of already existing blood vessels. This
starves the tumor, causing it to regress. This process can be described by the
following model [101, 171]:
x˙ 1 = −λx1 ln 
x1
x2

,
x˙ 2 = bx1 − d · x2
3
q
x
2
1 − µx2x3,
x˙ 3 = −ax3 + u,
y = x1.
Here, x1 is the tumor volume, x2 is the volume of the endothelium, i.e. the
innermost cell layer of the blood vessels of the tumor, and x3 is the concen￾tration of the medication in the body. The input variable u represents the
medication dose per day and per kilogram of the patient’s body weight. The
parameters λ, b, d, µ, and a are positive constants.
(a) State the transformation z = t(x) which transforms the model into the
nonlinear controller canonical form.
(b) Determine the inverse transformation x = t
−1
(z). To do this, use the flat
system representation with y = x1 as a flat output.
(c) State the flat feedforward control u(y, y,˙ y, ¨ ˙˙˙y ).
(d) Our aim is to dose the medication to decrease the tumor volume y accord￾ing to
y = e
−αt, α > 1.5a.
In this case, what is the flat feedforward control law u(t)?
(e) Sketch the curve of u(t).
Exercise 5.21 Let us consider a system which has a real flat output. What
is the difference between a flatness-based feedback control and a control of a
flat system with input-output linearization, and what is identical between the
two?504 Chapter 5. Nonlinear Control of Nonlinear Systems
Exercise 5.22 Show that the function V (x) = x
T Rx with a positive definite
matrix R is a control Lyapunov function for an omnidirectionally controllable
system x˙ = a(x) + B(x) · u.
Exercise 5.23 Let us examine the bilinear system
x˙ = Ax + uNx.
(a) What condition must be fulfilled such that the equilibrium point xeq = 0
is globally asymptotically stable for a constant input variable u = uc?
Now we will suppose that the control variable u is limited by |u| ≤ umax.
(b) Assume that A has only eigenvalues with a negative real part. In this case,
what condition is necessary for V (x) = x
T Rx to be a control Lyapunov
function for all x ∈ IRn
?
(c) What is the control law u(x) that minimizes V˙ (x)?
(d) What practical disadvantage does the control law have, and how can this
be remedied?
Exercise 5.24 Let us examine a system in the nonlinear controller canonical
form
x˙ =







x2
.
.
.
xn
f(x)







+







0
.
.
.
0
h(x)







u,
which is a special form of the strict feedback form.
(a) Using the backstepping method, design a controller u(x). In doing so, use
a P controller α(x1) as the virtual controller and
V1(x1) = 1
2
x
2
1
as the Lyapunov function in the first design step.
(b) State the differential equation of the control loop.
(c) How do the system descriptions of the above control loop and the con￾troller differ from the descriptions we would obtain in the case of a feed￾back linearization?6
Nonlinear Control of Linear and Nonlinear
Systems
6.1 Model-Based Predictive Control
6.1.1 Basics and Functionality
Among all advanced control methods, model-based predictive controls (MPCs),
or model predictive controls for short, are the most commonly used in industry
[68, 120, 157, 193, 296, 378, 381]. In the processing industry, especially in re￾fineries, chemical plants, cement mills, and paper and steel production, MPCs
are well-established standard procedures. MPCs are nonlinear control meth￾ods which are generally suitable to linear plants with limited control and state
variables, and to nonlinear plants as well. The reasons for their widespread
and successful use are the good explainability of their basic principle and their
applicability to complex high-order dynamical systems.
The operating principle of MPC is essentially based on the properties of
mathematical process models. Process models generally have two tasks in con￾trol engineering. Firstly, a model should provide a deeper understanding of
the process and its mode of operation. This knowledge is used for the design
of controllers such as state controllers, time-optimal controllers, and variable
structure controllers, among others. Secondly, a model allows for the predic￾tion of future behavior. It is just this possibility of predicting the future which
model predictive controls make use of. They calculate the model’s output vari￾able signals for different control input sequences online and then select the
best one. The optimized control variable is used to control the actual process.
Figure 6.1 illustrates the procedure.
Both continuous and discrete process models can be used for model pre￾dictive control. In most cases, however, discrete or discretized models
x(k + 1) = f(x(k),u(k)),
y(k) = g(x(k),u(k))
are employed, because in the case of continuous models, the optimization of
the control variable signal is much more complex.
© Springer-Verlag GmbH Germany, part of Springer Nature 2024
J. Adamy, Nonlinear Systems and Controls,
https://doi.org/10.1007/978-3-662-68690-4_6
505506 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
Current state variables
Predicted of the process
output signal
Reference signal
To the plant
Future
control variable signal
Optimal
control variable
Model
Optimizer
Fig. 6.1: Structure of a model predictive controller
We will begin with a known control variable u(k −1) at time instant k −1
and start with the optimization from this point on. The optimization varies
the future control variable signal u(k + i) from the time k onward for a finite
number i = 0, . . . , nc − 1 of control variable steps in such a way that a given
performance index J becomes minimal. The value nc is called the control
horizon. Commonly, the quadratic performance indices
J =
Xnp
i=1
|Q0
(y(k + i) − yref(k + i))|
2 +
Xnp
i=1
|R0u(k + i − 1)|
2
(6.1)
and
J =
Xnp
i=1
|Q0
(y(k + i) − yref(k + i))|
2 +
Xnc
i=1
|R1(u(k + i − 1) − u(k + i − 2))|
2
(6.2)
calculated over np time steps are used. The value np is called the prediction
horizon since the future behavior is predicted for np time steps. The prediction
horizon is always greater than the control horizon nc or both are equal, i. e.
np ≥ nc.
The matrices Q0
, R0, and R1 must be positive definite. They are often chosen
as diagonal matrices. In the above performance indices the difference between
the output variable sequence y(k + i) and a nominal or reference sequence
yref(k+i) is evaluated in the first term of the summation. In the second term,
the squared control variables, or control variable differences, are added and
weighted by the positive definite matrix R0 or R1. When minimizing J, this
second term ensures that the control variables u(k + i) in equation (6.1) and6.1. Model-Based Predictive Control 507
Past Future
Prediction y(k + i)
y, u
Reference
yref(k + i)
Control horizon nc
Prediction horizon np
u(k + i)
k k + nc k + np Time step
Fig. 6.2: Basic procedure of a model predictive control (MPC)
control variable differences in equation (6.2) do not assume values which are
too high, either to conserve energy or for other reasons.
The prediction of y(k + i) is made for np time steps in the performance
index J. The prediction horizon np is greater than or equal to the control
horizon nc, as mentioned above. To ensure for the case np > nc that control
variables u(k + i) are available for the time range
i ≥ nc,
and to allow us to predict y(k + i), all control variables beyond the control
horizon are constantly set at u(k+nc−1). Figure 6.2 illustrates the procedure.
After the optimization has determined the control sequence uopt(k + i)
with i = 0, . . . , nc − 1, only the first value of this sequence, i. e.
uopt(k) = u(k),
is applied to the real controlled system. This means the entire sequence
uopt(k + i) is not used for the control. Immediately after the activation of
u(k), the prediction horizon and optimization process described above are
shifted into the future by one step and a new optimal control variable sequence
is calculated. This procedure is repeated after each optimization. With each
repetition, the horizon is moved one time step further. Therefore, we refer to
this as a moving horizon. The moving horizon and the repetitive optimization
of the control sequence u(k + i) allow the MPC to react to disturbances and
compensate for them.
We can compare the operating mode of the MPC to the behavior of a chess
player – a frequently used simile. The player thinks through different sequences
of moves in his mind, considering three, four, or more moves in advance. In
reality, he then plays the first step of the combination that seems optimal to
him. After his opponent’s move, which may be regarded as a disturbance, he
selects his next, newly optimized move, and so on.508 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
The new prediction after each time step occurs based on the process model
and the control variable signals calculated by the optimization. However, the
control variables u(k + i) alone are not sufficient for the prediction. Rather,
the state x(k) at the beginning of the prediction period must also be known.
The reason for this is that the performance index J can only be calculated
using both the sequence of the model’s state vectors x(k+i), i = 1, 2, . . . np, in
order to calculate the sequence y(k+i), and the control sequence u(k+i−1).
In the best case, we can measure the plant’s state vector xpl(k) and calculate,
starting with
x(k) = xpl(k),
the model’s state vectors
x(k + i) = f(x(k + i − 1),u(k + i − 1)), i = 1, 2, . . . , np,
and
y(k + i) = g(x(k + i),u(k + i)), i = 1, 2, . . . , np,
by iteration. If the measurement is not possible, xpl(k) must be estimated as
in Figure 6.3 by x˜(k) using an observer based on the historical sequences
u(k − 1),u(k − 2), . . . ,
y(k − 1), y(k − 2), . . . ,
x˜(k)
ypl(k)
y(k+i) yref(k+i)
uopt(k)
u(k+i)
Plant
Observer
Model
Optimizer
Fig. 6.3: Structure of an MPC with observer, model, and optimizer, where ypl(k) is
the plant’s output variable vector and x˜(k) is its estimated state vector6.1. Model-Based Predictive Control 509
and
ypl(k − 1), ypl(k − 2), . . . ,
where ypl is the plant’s output variable vector. Combining all elements of an
MPC yields the structure that is shown in Figure 6.3.
6.1.2 Linear Model Predictive Control without Constraints
The MPCs most frequently used in industrial practice are based on linear
process models, which usually possess constraints on the control variables and
possibly on the state and output variables. MPCs of this kind are called linear
model predictive controls, abbreviated LMPC. In this section, we will address
the simple case of linear plants without constraints, which is not relevant to
practical applications. However, the study of MPCs for these systems allows us
firstly to gain a deeper understanding of the operation of MPC, and secondly
it provides us with some basics for the MPC of linear systems with constraints.
The latter is of greater interest in practice.
When using the quadratic performance index (6.1) or (6.2), the result is
a linear controller, as we will see in the following. It is well known that linear
controllers can also be designed using other, simpler methods. The true benefit
of an LMPC is therefore only obtained if constraints must be included in the
optimization problem. In this case, which we will discuss in the next section,
the result is a nonlinear controller. However, the MPC is still referred to as
an LMPC due to the linear process model.
Our starting point is the linear discrete-time model
x(k + 1) = Ax(k) + Bu(k),
y(k) = Cx(k)
(6.3)
with x ∈ IRn
, u ∈ IRm, and y ∈ IRr without constraints on u, x, or y, i. e.
the linear case. The control variable vector u(k) is composed of the previous
control variable vector u(k − 1) and the stepwise increment ∆u(k) at step k
according to
u(k)=u(k − 1) + ∆u(k) (6.4)
For the system description (6.3) above, we thus obtain
x(k + 1) = Ax(k) + Bu(k − 1) + B∆u(k),
y(k) = Cx(k).
(6.5)
We will now perform the calculation for all output variable vectors y(k+i)
from step k+1 up to step k+np, i. e. up to the prediction horizon np. Equation
(6.4) and equation (6.5) yield510 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
y(k + 1) = CAx(k) + CBu(k − 1) + CB∆u(k),
y(k + 2) = CA2x(k) + C(A + I)Bu(k − 1) + C(A + I)B∆u(k)
+ CB∆u(k + 1),
.
.
. (6.6)
y(k + i) = CAix(k) + C
￾
A
i−1 + . . . + A + I

Bu(k − 1)
+
X
i
j=1
C
￾
A
i−j + . . . + A + I

B∆u(k + j − 1),
.
.
.
y(k + nc) = CAncx(k) + C
￾
Anc−1 + . . . + A + I

Bu(k − 1)
+
Xnc
j=1
C
￾
Anc−j + . . . + A + I

B∆u(k + j − 1)
for the steps up to the control horizon nc. Beyond the control horizon, i. e.
i > nc, the control variable no longer changes, i. e.
∆u(k + i − 1) = 0
holds. Then we obtain
y(k + nc + 1) = CAnc+1x(k) + C (Anc + . . . + A + I) Bu(k − 1)
+
Xnc
j=1
C
￾
Anc+1−j + . . . + A + I

B∆u(k + j − 1),
.
.
. (6.7)
y(k + np) = CAnp x(k) + C
￾
A
np−1 + . . . + A + I

Bu(k − 1)
+
Xnc
j=1
C
￾
Anp−j + . . . + A + I

B∆u(k + j − 1).
The vectors y(k + 1) to y(k + np) are collected in the rnp × 1 vector
y¯(k + 1) =







y(k + 1)
y(k + 2)
.
.
.
y(k + np)







and the control variable changes ∆u(k) to ∆u(k + nc − 1) are lined up in the
mnc × 16.1. Model-Based Predictive Control 511
∆u¯(k) =







∆u(k)
∆u(k + 1)
.
.
.
∆u(k + nc − 1)







.
Note that the vector ∆u¯(k) generally has a lower dimension than the vector
y¯(k + 1) because nc ≤ np holds.
Combining equation (6.6) and equation (6.7) results in
y¯(k + 1) = F x(k) + Gu(k − 1) + H∆u¯(k), (6.8)
where
F =










CA
CA2
CA3
.
.
.
CAnp










, G=










CB
C(A + I)B
C(A
2 + A + I)B
.
.
.
C(Anp−1 + . . . + I)B










,
H=




















CB 0 . . . 0
C(A + I)B CB · · · 0
C(A
2 + A + I)B C(A + I)B · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
C(Anc−1+. . .+I)B C(Anc−2+. . .+I)B · · · CB
C(A
nc +. . .+I)B C(A
nc−1+. . .+I)B · · · C(A + I)B
.
.
.
.
.
.
.
.
.
.
.
.
C(Anp−1+. . .+I)B C(Anp−2+. . .+I)B · · · C(Anp−nc+. . .+I)B




















with F ∈ IRrnp×n
, G ∈ IRrnp×m, and H ∈ IRrnp×mnc hold. The component
g(k) = F x(k) + Gu(k − 1)
in equation (6.8) is determined by steps 0, . . . , k−1 of the control, which have
already been performed. In contrast to the term g(k), which is constant, the
term H∆u¯(k) in equation (6.8) contains the control variable sequence to be
optimized, i. e.
∆u(k), . . . , ∆u(k + nc − 1).
Taking this into account, we write equation (6.8) as512 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
y¯(k + 1) = g(k) + H∆u¯(k). (6.9)
The performance index
J(∆u¯(k)) = (y¯(k + 1) − y¯ref(k + 1))T Q (y¯(k + 1) − y¯ref(k + 1)) (6.10)
+ ∆u¯
T
(k)R∆u¯(k)
now needs to be minimized, where Q is a positive definite rnp×rnp matrix
and R is a positive definite mnc×mnc matrix. The rnp - dimensional vector
y¯ref(k + 1) =







yref(k + 1)
yref(k + 2)
.
.
.
yref(k + np)







contains the progression of the reference variable yref(k+i). Inserting equation
(6.9) into the performance index (6.10) and introducing the abbreviation
e(k) = g(k) − y¯ref(k + 1)
yields the quadratic form
J(∆u¯(k)) = (e(k) + H∆u¯(k))T Q (e(k) + H∆u¯(k)) + ∆u¯
T
(k)R∆u¯(k)
= ∆u¯
T
(k)

HT QH + R

∆u¯(k)
+ 2∆u¯
T
(k)HT Qe(k) + e
T
(k)Qe(k), (6.11)
which is positive definite. The necessary condition for a minimum of J, which
is
∂J(∆u¯(k))
∂∆u¯(k)
= 0, (6.12)
is also sufficient, because the performance index (6.11) is a positive definite
form and thus is a convex function. For equation (6.12), we obtain

HT QH + R

∆u¯(k) + HT Qe(k) = 0
in the present case. This linear equation results in the control variables
∆u¯(k) = −

HT QH + R
−1
HT Qe(k).
However, as mentioned earlier, only the first control variable ∆u(k) in the
vector6.1. Model-Based Predictive Control 513
PSfrag
−K
G
F
1
1−z−1
I
z
−1
I
x(k+1)=Ax(k)+Bu(k)
y(k) = Cx(k)
−y¯ref(k+1) e(k) ∆u(k) u(k) y(k)
u(k−1)
x(k)
Fig. 6.4: MPC for a linear plant without limitations
∆u¯(k) =










∆u(k)
∆u(k + 1)
∆u(k + 2)
.
.
.
∆u(k + nc − 1)










is used for control. We will therefore address only the first element
∆u(k) = −Ke(k)
of this vector, where
K =
h
I 0 · · · 0
i

HT QH + R
−1
HT Q

,
I ∈ IRm×m is the identity matrix. The control law is linear, has a dynamic,
and can be summarized as
u(k) = u(k − 1) + ∆u(k) = u(k − 1) − Ke(k),
e(k) = F x(k) + Gu(k − 1) − y¯ref(k + 1).
Figure 6.4 shows the corresponding structure of the linear MPC without
constraints. An observer was not used. However, it can easily be inserted into
the control loop to determine the state vector x(k) required for control.
6.1.3 LMPC with Constraints
As mentioned previously, linear MPCs without constraints have no great prac￾tical relevance and have been described here mainly to provide an easy-to￾understand introduction to the topic, and to explain how they work in prin￾ciple.514 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
In industrial practice, linear MPCs are used when the variables to be
optimized are subject to restrictions in the control variable ui(k), i. e.







umin,1
umin,2
.
.
.
umin,m







| {z }
umin
≤







u1(k)
u2(k)
.
.
.
um(k)







| {z }
u(k)
≤







umax,1
umax,2
.
.
.
umax,m







| {z }
umax
. (6.13)
These constraints of the control variables can also be represented as functions
of the changes in the control variables, so that equation (6.13) takes the form
umin ≤ u(k) =u(k − 1) +∆u(k) ≤ umax,
umin ≤ u(k + 1) =u(k − 1) +∆u(k) + ∆u(k + 1) ≤ umax,
umin ≤ u(k + 2) =u(k − 1) +∆u(k) + ∆u(k + 1) + ∆u(k + 2) ≤ umax,
.
.
.
umin ≤ u(k + nc − 1) =u(k − 1) +∆u(k) + . . . + ∆u(k + nc − 1)≤ umax.
(6.14)
We define
u¯min =





umin
.
.
.
umin





and u¯max =





umax
.
.
.
umax





as vectors of length mnc. The inequalities (6.14) can then be represented in
the matrix form
u¯min ≤ E · u(k − 1) + D · ∆u¯(k) ≤ u¯max (6.15)
with the mnc × m matrix
E =





I
.
.
.
I





and the lower triangular matrix of dimensions mnc × mnc, given by
D =







I 0 0 · · · 0
I I 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
I I I · · · I







.6.1. Model-Based Predictive Control 515
The matrices I are identity matrices of dimension m × m.
Constraints on the control variables’ rate of change, i. e. constraints of the
type
∆u¯min ≤ ∆u¯(k) ≤ ∆u¯max, (6.16)
are also often relevant. The vectors ∆u¯min and ∆u¯max are similar in structure
to the vectors u¯min and u¯max, and are of the same dimensions.
In addition to control variables, the state variables xi and output variables
yi can also be subject to constraints such as







ymin,1
ymin,2
.
.
.
ymin,r







| {z }
ymin
≤







y1(k)
y2(k)
.
.
.
yr(k)







| {z }
y(k)
≤







ymax,1
ymax,2
.
.
.
ymax,r







| {z }
ymax
.
The constraints on the output variables can be converted into constraints on
the control variable changes. With
y¯min ≤ y¯(k + 1) ≤ y¯max
and with equation (6.9), we obtain
y¯min ≤ g(k) + H∆u¯(k) ≤ y¯max. (6.17)
Here, the vector y¯min consists of np vectors ymin according to
y¯min =





ymin
.
.
.
ymin





∈ IRrnp×1
,
and for y¯max the same applies. Constraints on xi are of a similar form.
In summary, the inequalities (6.15), (6.16), and (6.17) can be represented
as
W∆u¯(k) ≤ w(k, k − 1) (6.18)
with
W =












−D
D
−I
I
−H
H












and w(k, k − 1) =












−u¯min + Eu(k − 1)
u¯max − Eu(k − 1)
−∆u¯min
∆u¯max
−y¯min + g(k)
y¯max − g(k)












.516 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
This results in the optimization problem
min
∆u¯(k)
J(∆u¯(k))
with the linear constraints (6.18).
We can now use the positive definite quadratic performance index (6.1)
or (6.2) once again. In this case, we have a convex quadratic optimization
problem with linear constraints, which has a unique minimum. Generally, this
minimum cannot be determined analytically, only numerically. However, there
are convergent standard procedures in quadratic programming which can be
used to calculate the minimum [41, 68, 296]. At this point, the disadvantages
of conventional model predictive control become clear: the control law cannot
be explicitly specified and the control variable’s calculation is time-consuming.
Explicit MPC offers a way to mitigate this disadvantage [12, 156, 230, 256,
298]. Such an MPC solves the optimization problem offline. For this purpose,
control variable vectors uopt,i are calculated for a number of state vectors
xi and setpoints yref,i and summarized in a table. By interpolation between
these values, a static controller
uopt = h(x, yref)
is obtained, which enables real-time control. However, the effort required by
this method increases greatly as the system order increases, so that it is only
applicable to systems with a low system order. Another way to mitigate the
real-time problem is to use software and hardware specifically tailored to the
problem [3]. Nonetheless, in many cases the advantages of MPCs, which are
their high control performance, good comprehensibility, and simple adjusta￾bility, outweigh their disadvantages.
6.1.4 Example: Drainage System
For the drainage of areas with high groundwater levels, such as those found in
the Netherlands or the American Everglades, sewer systems are used. Particu￾larly during or after heavy rainfall, water must be pumped from the drainage
sewers into neighboring rivers or the sea. Otherwise, excessive water levels
would endanger dikes, towns, and agricultural land. However, low water lev￾els in the canals are also problematic, as they can cause an embankment slide.
Moreover, large fluctuations in the water level can cause loosening of the soil
in the sewer walls due to pump effects, leading to damage of the sewers.
Therefore, water level in the sewers should be neither too low nor too high
but should remain within a specified tolerance range. For this purpose, control
systems are used to adjust the amount of water pumped out of the sewers.
Figure 6.5 shows the structure of such a drainage system. In some cases where
the facilities are near the sea, water is pumped into additional storage canals
or reservoirs during high tide. At low tide, the water then flows into the sea
without additional pumping.6.1. Model-Based Predictive Control 517
The water level h in the sewer system is dependent on the rainwater flowing
into the sewers and the amount of water that has been pumped out according
to the relation
h(k + 1) = h(k) + T
A
(qrain(k) − qp(k − kd)). (6.19)
Here, qrain indicates the amount of water per second flowing into the canals
due to rainfall or additional groundwater, and qp indicates the amount of
water pumped out per second. The area A comprises the total area of the
sewers, T is the time span in seconds between the times k and k + 1, and kd
is the number of time steps after which the control variable qp acts on the
water level in the sewers.
As an example we will describe the drainage system of the Delfland in the
Netherlands [348]. Its parameter values are
Fig. 6.5: Drainage system518 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
A = 7300000 m2 = 7.3 km2
, T = 900 s, kd = 1.
The water level h and pump volume flow qp are limited by
−0.55 m ≤ h ≤ −0.30 m
and
0 m3
s
−1 ≤ qp ≤ 75 m3
s
−1
.
The water level h in the sewers is indicated with respect to the sea level, i. e.
for h = 0, the water level in the sewers is equal to sea level. In the case of the
Delfland, it normally lies about half a meter below sea level. In our example,
we will set its reference value at href = −0.40 m.
The system’s state variables given by equation (6.19) are the water level
x1(k) = h(k),
and the delayed control signal
x2(k) = qp(k − 1),
which is delayed here by one time step, i. e. kd = 1. With the parameter values
above and the current control variable
u(k) = qp(k)
from equation (6.19), we obtain the state-space model
x1(k + 1) = x1(k) − 1.23 · 10−4
x2(k) + 1.23 · 10−4
qrain(k),
x2(k + 1) = u(k),
y(k) = x1(k).
Here, qrain acts as a disturbance.
The aim of a control is now to compensate for disturbances qrain due to
heavy precipitation in such a way that the water level x1 is brought to the
desired value x1 = −0.4. During the regulation process, the state variable x1
must not exceed the constraints
−0.55 ≤ x1 ≤ −0.30
and the control variable u must not exceed the constraints
0 ≤ u ≤ 75.
It is evident that the above control task is well-suited to model predictive
control. Firstly, the time T = 900 s between two controller actions is suffi￾ciently long for the computation, and secondly, the limitations of y = x1 and
u can very well be included in a model predictive control.6.1. Model-Based Predictive Control 519
-0.3
-0.35
-0.4
80
60
40
20
0
0
0
0
5
5
10
10
15
15
20
20
20
25
25
30
30
35
35
40
40
40
45
45
50
50
60 80 100 120 140 160 180 200
r = 0
r = 0
r = 0.0001
r = 0.0001
r = 0.000002
r = 0.000002
Time t in hours
Sampling step k
Water level
y in m Volume flow
u in
m
3
s
−1 Fig. 6.6: Progression of the output variable y and progression of the control variable
u depending on t = kT
For the design of the MPC, we now select the prediction horizon and the
control horizon as
np = 97 and nc = 2,
respectively. The long prediction horizon of 24h 15min allows the inclusion
of predicted rainfall based on a weather forecast, among other factors. This
amount of rain can be taken into account in the controller via qrain.
For the performance index, we will choose
J = (y¯(k + 1) − y¯ref(k + 1))T
(y¯(k + 1) − y¯ref(k + 1)) + r∆u¯
T
(k)∆u¯(k),
(6.20)
where the reference value of the water level is set constant at yref = href =
−0.4 m, i. e.
y¯ref(i) = [−0.4 m −0.4 m · · · −0.4 m ]
T
holds for all i = 0, 1, . . .
As a practical example, let us take an elevated water level of x1(0) =
−0.3 m following bad weather conditions. The second initial value is x2(0) = 0.
Figure 6.6 shows the courses of the control variable u and the water level y.
Three different controllers have been used, resulting from different weighting520 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
factors r in the performance index (6.20). As expected, the model predictive
control is fastest for r = 0 and becomes slower for r = 0.000002 and r =
0.0001.
The control variable u’s rate of change also depends on r. If abrupt startups
and shutdowns of the pump volume flow u are of no consequence, r = 0 can
be selected. If we would rather make less abrupt changes ∆u in the pump
volume flow u, we should select a larger value r. Alternatively, a limitation of
∆u can be introduced.
Figure 6.7 shows the contour lines of the performance index, which is
minimized during the control process. The contour lines take the shape of
1
1
1
1
1
1
1
5
5
5
5
5
5
10
10
10
10
10
20
20
20
20
30
30
30
40
40
40
50
50
60
60
60
70
80
80
90
100
100
110
120
1
1
1
5
5
5
10
10
10
10
20
20
20
30
30
30
40
40
40
50
50
60
60
70
70
80
90
90
100
100
110
110
120
120
130
130
140
150
160
170
180
190
200
210
220
1
1
1
1
5
5
5
5
5
10
10
10
10
10
20
20
20
20
30
30
30
40
40
40
50
50
60
60
70
80
80
90
100
100
110
110
120
130
140
150
160
170
180
1
1
1
1
1
1
5
5
5
5
5
5
10
10
10
10
20
20
20
20
30
30
30
30
40
40
50
50
60
60
70
70
80
80
100
100
110
110
80 80
80 80
40 40
40 40
0 0
0 0
-40 -40
-40 -40
-80 -80
-80 -80
80 80
80 80
40 40
40 40
0 0
0 0
-40 -40
-40 -40
-80 -80
-80 -80
∆u(1)
∆u(2)
∆u(6)
∆u(5)
∆u(11)
∆u(10)
∆u(41)
∆u(40)
Sampling step k = 1 Sampling step k = 5
Sampling step k = 10 Sampling step k = 40
Fig. 6.7: Illustration of the optimization problems to be solved in the time steps
k = 1, k = 5, k = 10, and k = 40 for the case r = 0.000002. The contour lines of
J and the admissible optimization region (blue) are displayed. A cross marks the
minimum in each case.6.1. Model-Based Predictive Control 521
very elongated ellipses. Furthermore, the graph shows the admissible region
for the optimization variables ∆u(k) and ∆u(k + 1) as resulting from the
constraints. The shapes of the contour lines of the performance index and
the admissible region depend on the time step k, i. e. they change during the
control process.
6.1.5 Nonlinear Model Predictive Control
For nonlinear plants
x˙ = f(x,u),
y = g(x)
(6.21)
or
x(k + 1) = f(x(k),u(k)),
y(k) = g(x(k)),
(6.22)
MPC can be used in a similar way as for linear plants. However, the imple￾mentation and calculation are usually much more complicated and complex
than in the linear case. This is because, due to the nonlinear system dynamics
(6.22), the optimization problem no longer takes a simple form. In particu￾lar, it is generally no longer convex. The nonlinearity causes a complicated
dependence of the performance index J on the control values u(k + i) to be
optimized. Such a nonconvex optimization problem often has several local
minima, and there are no solution procedures which reliably converge to the
global minimum. Despite the more complex numerics, the basic procedure is
roughly the same as for the LMPC.
Numerical optimization methods are used to solve the constrained opti￾mization problem [13, 94, 241]. One of the main problems is often solving the
optimization problem in real time. It is particularly time-consuming to deter￾mine the solution of system (6.21) for each control variable and performance
index calculation. Usually this is done by means of an integration procedure,
such as the Runge-Kutta procedure in the continuous-time case (6.21). In the
case of discrete systems, the system solution is calculated directly using the
recursion equation (6.22).
Because of the real-time problem, MPC is mostly used in control systems
with slow dynamics, such as in the processing industry. Another problem of
nonlinear MPC, abbreviated to NMPC, is ensuring the stability of the control
loop. Due to the nonlinearity of the plant and the fact that the control law
cannot be explicitly specified, stability generally cannot be guaranteed.
Below we will take a closer look at the stability problem. The starting point
of our analysis is system (6.22), where the input variable vector u is limited
according to u ∈ W ⊂ IRm, and the state vector x is limited according to
x ∈ X ⊂ IRn
. Let us assume that np = nc holds. We will use the function522 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
J(k) =
k+
Xnc−1
i=k
Q(x(i),u(i))
as the performance index to be minimized.
The function Q can have quadratic terms similar to those in equation (6.1).
Of course, other forms are also possible. In any case,
Q(x,u) > 0
must hold for all x 6= 0 and u 6= 0. Furthermore, the controlled system must
have an equilibrium point at
x(k) = 0
for u(k) = 0, and
Q(0, 0) = 0
must apply. Due to this requirement,
J(k) = 0
also applies to x(k) = 0 and to u(i) = 0, i = k, k + 1, . . .
Note that the performance index J indirectly depends on the state vector
x only, since the control variable vector u depends on x via the NMPC. Thus,
the performance index J seems to be a candidate for a Lyapunov function of
the predictively controlled system, because at the origin x = 0 it is identical
to zero and everywhere else it is positive. As a third condition for a Lyapunov
function (see Section 2.3.5, p. 125), J must fulfill the requirement
J(k) − J(k − 1) < 0, (6.23)
i. e. J(k) must decrease in the course of each control process.
Condition (6.23) is also fulfilled if J(k) is only slightly smaller than J(k−1).
In this case, however, the control occurs very slowly. Scokaert, Mayne, and
Rawlings [408] have thus formulated condition (6.23) more strictly as
J(k) − J(k − 1) < −µQ(x(k − 1),u(k − 1)), (6.24)
where µ ∈ (0, 1) holds.
For values of the parameter µ close to zero, the optimization procedure
will find a solution
u(k), . . . ,u(k + nc − 1)
more easily than for greater values of µ, because there are more solutions
in this case. The price to be paid for this is often a small decrease in the
performance index J. Consequently, the compensation is slow again.
Choosing µ close to one causes a greater decrease in J from one step k − 1
to the next. However, now the solution space is more limited, so that the
calculation of a solution becomes more complicated and can take longer.6.1. Model-Based Predictive Control 523
During the execution of the MPC, it must be constantly verified whether
inequality (6.24) is fulfilled. Once a corresponding control sequence u(k), . . . ,
u(k + nc − 1) has been identified using the optimization procedure, two cases
are possible. In the first, optimization time is still available and can be used to
improve the result. In the second, the available optimization time is used up
and the control variable u(k) is applied to the plant. The optimization then
starts over again with ˜k = k + 1 in an attempt to identify a new combination
of values u(
˜k), . . . ,u(
˜k +nc −1). Conveniently, the old set of control variables
u(k), . . . ,u(k + nc − 1) can be used as the starting point of the optimization.
Based on the above arguments, the performance index J seems to be suit￾able as a potential Lyapunov function, but we can only determine its decrease
in the general case along a trajectory, i. e. the fulfillment of inequality (6.23),
numerically during a control process. However, Lyapunov’s stability theorem
requires the verification of condition (6.23) in a neighborhood U(0) of the
equilibrium point xeq = 0, i. e. for all x ∈ U(0) \ {0} and not only for individ￾ual trajectories, to prove stability. As long as we can only verify the decrease
in J for individual trajectories and not for the entire neighborhood of x = 0,
the above procedure is not sufficient for the stability analysis.
The aforementioned problem can be illustrated by the following explana￾tions. Let us analyze a nonlinear model predictive control loop of order two
which has an equilibrium point at x = 0, and a circular stable limit cycle with
its center point at x = 0. We will start the control procedure by means of the
predictive control at a point x(0) outside the limit cycle. The trajectory x(k)
of the control loop tends asymptotically from the point x(0) to the limit cycle.
This means the performance index J decreases constantly, so that inequality
(6.23) or (6.24) is fulfilled. The trajectory x(k) does not tend to the equilib￾rium point xeq = 0 despite the decrease in J. Figure 6.8 illustrates this. In
conclusion, it is important to note that, in the general case, we cannot deter￾x(0)
x1
x2
x(k)
Limit cycle
Contour line of J
Fig. 6.8: Example of the continuous decrease in the performance index J along a
trajectory where the trajectory x(k) does not tend to zero524 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
mine whether the performance index J is a Lyapunov function. The decrease
in J during a compensation operation is merely a practice-oriented condition
and not sufficient to prove the stability of the model predictive control system.
Nonetheless, stability can be guaranteed if we succeed in proving that
inequality (6.23) holds for all x in a neighborhood of xeq = 0. In specific cases,
based on assumptions regarding the plant and the minimization problem,
this is possible and has yielded some basic results for the stability of MPCs
[51, 161, 205, 282, 315, 382]. A prerequisite for these is an infinite control
horizon nc or one of the following requirements.
To ensure that the equilibrium point xeq = 0 is actually reached, we
require that
x(k + nc) = 0 (6.25)
holds for every time step k in addition to the decrease in J, i. e. that stability is
given. Equation (6.25) is an additional constraint on the task of optimization.
In order to fulfill it, the control horizon nc must be sufficiently large. This can
require very large values of nc, meaning the amount nc of control values
u(k), . . . ,u(k + nc − 1)
to be optimized also becomes very large.
The latter problem is reduced if, instead of the constraint (6.25), we require
x(k + nc) ∈ U(0),
where U(0) is a neighborhood of the equilibrium point xeq = 0. The set U(0)
can be a tolerance range whose state values x ∈ U(0) are accepted as target
values of the control because their deviation from x = 0 is sufficiently small.
The neighborhood U(0) can also be a region in which a controller u2(x) is
used instead of the predictive controller u1(x), as illustrated in Figure 6.9.
x0
u1
x1
x2
u2
Switching
to u2
Predictive
control u1
Catchment
region
Conventional
feedback control u2
Fig. 6.9: Predictive dual-mode control system6.1. Model-Based Predictive Control 525
In this case, the controller u2(x) is designed in such a way that it leads to
an asymptotically stable regulation to the equilibrium point xeq = 0 for all
x ∈ U(0). The neighborhood U(0) is designed as a catchment region, i. e. no
trajectory x(k) leaves U(0) after entering this region. A compensation for an
initial value x0 6∈ U(0) starts with the predictive control u1(x) and switches
to the second controller u2(x) as soon as x(k) ∈ U(0) applies.
For example, the conventional control law u2(x) can be designed linearly,
based on a linearization of the plant around x = 0. Controllers of this kind
with two control laws u1 and u2 are called dual-mode controllers. We can
summarize the above in the following dual-mode MPC algorithm proposed by
Scokaert, Mayne, and Rawlings [408]:
Step 1: Choose µ ∈ (0, 1).
Step 2: Let k = 0. If x(0) ∈ U(0), then set u(0) = u2(x(0)). Otherwise,
find a control variable sequence
u(0),u(1), . . . ,u(nc − 1)
by optimizing J and an associated state variable sequence
x(0), x(1), . . . , x(nc),
so that
u(i) ∈ W for all i = 0, . . . , nc − 1,
x(i) ∈ X for all i = 0, . . . , nc, x(nc) ∈ U(0)
hold. As the active control variable value, use the vector u(0).
Step 3: Let k 6= 0. If x(k) ∈ U(0), then set u(k) = u2(x(k)). Otherwise,
find a control variable sequence
u(k), . . . ,u(k + nc − 1)
by optimizing J and an associated state variable sequence
x(k), . . . , x(k + nc),
so that
u(i) ∈ W for all i = k, . . . , k + nc − 1,
x(i) ∈ X for all i = k, . . . , k + nc, x(k + nc) ∈ U(0),
and
J(k) − J(k − 1) < −µQ(x(k − 1),u(k − 1))
holds. When optimizing J, the control variable sequence of the
previous step is selected as the starting point. Use u(k) as the
active control value. Repeat Step 3.526 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
At this point, it must be mentioned that, in practice, stability analysis is of￾ten neglected for nonlinear MPC in industry. In order to be able to make a
statement at all for an NMPC, stability is sometimes evaluated by simulating
the control loop for selected cases or operating points. However, this does not
provide a proof of stability. Thus, in addition to the extensive computational
effort [121], the main disadvantage of real-world MPCs is that stability either
cannot be rigorously proven, or this can be done only with great effort or only
for special cases [51, 161, 205, 282, 315, 382].
6.1.6 Example: Evaporation Plant
As an example, let us view an evaporation plant as it is used for purposes such
as the production of syrup in sugar factories [335, 458]. As shown in Figure
6.10, the system consists of an evaporator, a separator, and a condenser. The
input material, raw juice from sugar beets, is fed into the evaporator which is
designed as a heat exchanger. The evaporator is heated with steam of pressure
Ps. The raw material heated in the evaporator at pressure Pe leaves the
Evaporator Separator Condenser
Evaporated
material
Steam
Water
Input
material
Product with u2 = qp and x2 = Kp
Concentrate
Condensate
x1 h
x3
Pe
u1
Ps
Steam
u3
qc
Cooled
water
Fig. 6.10: Evaporation plant with evaporator, separator, and condenser6.1. Model-Based Predictive Control 527
evaporator as a mixture of steam and liquid and then enters the separator.
In the separator, the steam is separated and fed into a condenser. Cooled by
water that flows into the condenser at rate qc, the steam is condensed and
extracted from the facility. The concentrate collected in the separator has a
level of h. A part of this concentrate with concentration Kp is now removed as
a product from the process at volume rate qp. The much larger part, however,
is mixed with the input material and returned to the evaporator.
The system can be described by a third-order nonlinear model, for which
we will assume that the flow rate and the concentration of the input material
are constant. Here,
x1 = h, x2 = Kp, x3 = Pe
are the state variables, and
u1 = Ps, u2 = qp, u3 = qc
are the control variables. We will measure x1 in m, x2 in %, x3 and u1 in kPa,
and u2 and u3 in kg min−1
. The model has the form
x˙ 1 = a1x3 + a2x2 − b1u1 − b2u2 − k1,
x˙ 2 = −a3x2u2 + k2,
x˙ 3 = −a4x3 − a5x2 + b3u1 −
a6x3 + b4
b5u3 + k3
u3 + k4.
(6.26)
The parameters of the system are
a1 = 0.00751, b1 = 0.00192, k1 = 0.01061,
a2 = 0.00418, b2 = 0.05, k2 = 2.5,
a3 = 0.05, b3 = 0.00959, k3 = 6.84,
a4 = 0.03755, b4 = 0.1866, k4 = 2.5531,
a5 = 0.02091, b5 = 0.14,
a6 = 0.00315.
The state variables are the output variables
y1 = x1, y2 = x2, y3 = x3
of the process. Both the state variables and the control variables are subject
to constraints of the form
0 m ≤ x1 ≤ 2 m,
0 % ≤ x2 ≤ 50 %,
0 kPa ≤ x3 ≤ 100 kPa,
0 kPa ≤ u1 ≤ 400 kPa,
0 kg min−1 ≤ u2 ≤ 4 kg min−1
,
0 kg min−1 ≤ u3 ≤ 400 kg min−1
.
(6.27)528 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
The equilibrium point xeq of system (6.26) can be specified via the control
variables
u1eq =
1
b1

a1x3eq + a2x2eq − k1 −
b2k2
a3x2eq 
,
u2eq =
k2
a3x2eq
,
u3eq =
k3(−a4x3eq − a5x2eq + k4 + b3u1eq)
(a6 + a4b5)x3eq + a5b5x2eq − b5k4 + b4 − b3b5u1eq
.
In the following, we will select
xeq =
h
1 15 70iT
, (6.28)
which yields
ueq =
h
214.13 3.33 65.40iT
.
To make the design of a model predictive control as simple as possible, we
discretize the model and use a discrete-time MPC. The duration of a control￾value step is T = 1 min. For the prediction horizon and the control horizon,
we will choose np = nc = 5, and for the performance index
J(k) =
k
X
+4
i=k
(x(i) − xeq)
T Q (x(i) − xeq) + r (u(i) − ueq)
T
(u(i) − ueq).
Here, the weighting matrix is
Q =




100 0 0
0 1 0
0 0 10




and r = 0.1. The performance index with the associated constraints (6.27)
is converted using a barrier function so that an unconstrained performance
index
J˜(k)=(
J(k) if all x(i),u(i), i = 1, . . . , 4, fulfill inequality (6.27),
∞ if one x(i),u(i), i = 1, . . . , 4, does not fulfill inequality (6.27),
is to be minimized. Due to the Cartesian constraints, the performance index
J˜ can be efficiently optimized with the optimization method developed by
Hooke and Jeeves [189, 406]. The numerical solution of the system equation
(6.26) is carried out using the Euler method with a step size of 2.4 s.6.1. Model-Based Predictive Control 529
1
0.8
0.6
0.4
0.2
25
20
20
15
70
65
60
60
55
50
270
260
250
240
230
220
210
4
3.5
3
2.5
2
40
0
0
0
0
0
0
0
20
20
20
20
20
20
40
40
40
40
40
40
60
60
60
60
60
60
80
80
80
80
80
80
100
100
100
100
100
100
Sampling step k, time t in min Sampling step k, time t in min
Level
x1 in m Concentration
x2 in % Pressure
x3 in kPa
Pressure
u1 in kPa Flow rate
u2 in kg min
−1 Flow rate
u3 in kg min
−1
Fig. 6.11: Evaporation plant employing model predictive control with sampling time
T = 1 min
A stability analysis for this NMPC would be complicated and very labo￾rious, so we will proceed without it. Theoretically this is unsatisfactory, but
in practice, as mentioned, this is often done.
Let us take the initial state
x(0) = h
1 25 50iT
to be regulated to the operating point (6.28). Figure 6.11 shows the variation
in time of the control variables u1, u2, and u3 and the controlled variables y1,
y2, and y3. In addition to the good compensation result, it is also evident that
all constraints (6.27) for x1, x2, x3, u1, u2, and u3 are fulfilled. This especially530 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
holds for the flow rates u2 and u3, of which u2 is held at its maximum value
u2 = 4 kg min−1
for 4min and u3 is held at its minimum value u3 = 0 kg min−1
for 7min.
6.2 Variable Structure Control with Sliding Mode
6.2.1 Basics and Characteristics
Sliding mode controllers[1] switch back and forth between two control laws
depending on the state vector. This results in sliding modes which have the
advantage that the control system is robust with regard to parameter fluctu￾ations of the plant. A disadvantage of conventional sliding mode controllers is
the high-frequency switching of the actuator. This often leads to premature
wear. Sliding mode controllers were originally introduced for the control of
linear systems by Emeljanov [104, 105, 106] and were further developed by
Utkin [460, 461] and Itkis [204]. Improvements and applications for nonlinear
plants were later described by other researchers [134, 196, 415, 421, 482, 501].
In the following, we will first consider linear plants, since these systems can
be used to describe the basic principles of sliding mode control in a simple
way. Subsequently, we will address nonlinear plants in Section 6.2.6.
In order to illustrate the principle of sliding mode control, we will take the
plant
x˙ 1 = x2,
x˙ 2 = a · u, a > 0,
(6.29)
with the controller
u = −umax sgn (x2 + mx1). (6.30)
Here, umax and m are constants of the controller. The control law (6.30)
separates the state space into two regions via the switching line
x2 = −mx1. (6.31)
Below the switching line, we have
u = umax,
while above it,
u = −umax
holds. Figure 6.12 shows the switching line (6.31) and the trajectories of the
plant for the control variable values u = ±umax, which are parabolas.
[1] In the literature, sliding mode controls are often referred to as variable structure
controls. However, as described in Section 4.3, variable structure controls comprise
a much larger class of controls.6.2. Variable Structure Control with Sliding Mode 531
PSfrag
State x1
State
x2
2
1
0
-2
-1
-10 -5 5 0 10
Fig. 6.12: System trajectories for actu￾ator signals u = −umax (solid line) and
u = umax (dashed line) as well as the
switching line, shown in blue
State x1
State
x2
2
1
0
-2
-1
-10 -5 5 0 10
Fig. 6.13: Trajectories of the control
loop with sliding mode shown in blue.
Above the switching line, u = −umax
holds, while below it, u = umax holds.
The trajectories x(t) = [x1(t) x2(t)]T
of the control loop consist of parts
of the parabolas separated by the switching line. To determine them, we must
calculate the solutions
x2 = x2(0) + Z
t
0
aumax dτ = aumaxt + x2(0), (6.32)
x1 = x1(0) + Z
t
0
x2(τ) dτ =
1
2
aumaxt
2 + x2(0)t + x1(0) (6.33)
of the differential equations (6.29) for u = umax. From equation (6.32), we
obtain
t =
x2 − x2(0)
aumax
.
Inserting this into equation (6.33) results in
x1 =
x
2
2
2aumax
−
x
2
2
(0)
2aumax
+ x1(0).
This equation describes the parabolas along which the trajectories run. Their
branches are symmetrical with regard to the x1-axis, i. e. their apexes lie on
the x1-axis. In the case of u = −umax, we obtain the parabolas
x1 = −
x
2
2
2aumax
+
x
2
2
(0)
2aumax
+ x1(0).
The control law (6.30) causes the trajectories to run toward the switching
line x2 = −mx1 from both sides. Figure 6.13 illustrates this for the value532 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
m = 0.06. After reaching the switching line, the trajectories on both sides
cause a continual switching between −umax and umax. As a consequence, the
trajectory x(t) slides on the switching line into the equilibrium point xeq = 0.
The sliding dynamics are described by the switching line (6.31) itself, which
specifies them as
x2 = ˙x1 = −mx1.
Theoretically, the trajectory changes at infinite rapidity with infinitesi￾mally small deflection from one side of the switching line to the other. Thus,
this switching between −umax and umax occurs at infinitely high frequency re￾ferred to as chattering. Of course, in practice, the switching frequency is not
infinitely high; it depends on the maximum speed of the actuator. As previ￾ously mentioned, chattering is a serious disadvantage to sliding mode control,
because mechanical actuators wear out quickly.
However, sliding mode control has a significant advantage: the control loop
is robust with regard to plant variations in the sliding mode. This means that
the control-loop dynamics are always the same, even if the plant changes. In
the previous example, the dynamics are
x˙ 1 = −mx1,
x˙ 2 = ¨x1 = −mx˙ 1 = m2
x1 = −mx2
in the sliding mode. These dynamics are therefore independent of a, i. e. the
parameter of the plant (6.29).
6.2.2 Design for Linear Plants
The control process of a sliding mode control can be divided into three phases:
(1) the arrival phase, in which the trajectories approach the switching line or
switching hyperplane and reach it in finite time;
(2) the sliding mode phase, in which the trajectory slides on the switching
hyperplane into the equilibrium point; and
(3) the equilibrium point xeq = 0, at which the system remains.
For a globally stable equilibrium point xeq = 0, it must be ensured that all
trajectories tend toward the switching hyperplane in finite time and then move
to the equilibrium point xeq = 0 in the sliding phase.
Let us examine the linear plants
x˙ = Ax + bu
with a switching plane
s(x) = 0
and the associated control law6.2. Variable Structure Control with Sliding Mode 533
u(x) = (
u+(x) for s(x) > 0,
u−(x) for s(x) < 0.
In most cases, the switching function
s(x) = r
T x
is used, i. e. a switching hyperplane r
T x = 0.
First, the accessibility of the switching hyperplane should be ensured for
all trajectories of the state space. A necessary condition for this is that the
control-loop trajectories tend toward the switching hyperplane from both
sides, as shown in Figure 6.13. This means that
s <˙ 0 for s(x) > 0
and
s >˙ 0 for s(x) < 0
must hold. If both conditions are combined, we obtain
ss <˙ 0
as a condition for a sliding mode. Here
s˙ = gradT
(s(x)) · x˙, gradT
(s(x)) = ∂s(x)
∂x
,
holds, and if s(x) = r
T x, it follows that s˙ = r
T x˙. Unfortunately, the condition
ss <˙ 0 does not ensure that the trajectories reach the switching hyperplane
s(x) = 0 in finite time for every conceivable case. The condition is therefore
necessary, but not sufficient for a functional sliding mode control system.
There are different approaches to ensure the accessibility of the switching
hyperplane for all trajectories in finite time. A very common approach is that
developed by Gao and Hung [145]. Here, the decrease s˙(x) in the switching
function along the trajectories x(t) is preset to
s˙(x) = −q sgn(s(x)) − ks(x), (6.34)
where q and k are positive constants. Obviously, in the case of equation (6.34),
the switching hyperplane s fulfills the necessary condition
ss˙ = −q|s| − k · s
2 < 0.
Because s has a decrease rate of s <˙ −q (or an increase rate of s > q ˙ ) even
for very small values of |s| due to the choice of s˙ in equation (6.34), the
trajectories x(t) reach the switching hyperplane in finite time.
Using
s˙(x) = gradT
(s(x)) · x˙ = gradT
(s(x)) · (Ax + bu)534 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
the control law is obtained from equation (6.34) as
u(x) = −
gradT
(s(x)) · Ax + q sgn(s(x)) + ks(x)
gradT
(s(x)) · b
.
For the frequent case of a linear switching hyperplane
s(x) = r
T x = 0,
we obtain the control law
u(x) = −
r
T Ax + q sgn(r
T x) + kr
T x
r
T b
.
The arbitrary positive parameters q and k allow for the creation of favorable
control dynamics.
Note that sliding mode controllers can also be designed for systems with
multiple input variables, i. e. for systems
x˙ = Ax + Bu;
see [196, 397, 460].
6.2.3 Dynamics in the Sliding Mode
When trajectories x(t) reach the switching plane and the sliding mode begins,
the question arises: what dynamics does the control loop have in the sliding
mode? When considering this question, the discontinuity of the differential
equation
x˙ = Ax + bu, u(x) = (
u+(x) for s(x) > 0,
u−(x) for s(x) < 0
(6.35)
of the closed control loop on the switching hyperplane
s(x) = r
T x = 0
poses a problem.
It is evident that the differential equation is not defined on the switching
hyperplane, i. e. the existence and uniqueness of its solution is not ensured on
the hyperplane. This problem results from the switching between u+(x) and
u−(x), which causes a discontinuous control signal u(t). However, the right
side of the differential equation (6.35) must be continuous, since the left side is
continuous due to the differentiation of x. There are various methods [196] to
solve this problem, such as the Filippov method [119]. But this issue does not6.2. Variable Structure Control with Sliding Mode 535
matter in practical applications, since the trajectory cannot run accurately
on the switching hyperplane due to noise. Therefore, we do not need to take
this circumstance into account below.
We can determine the system dynamics in the sliding mode by employing
the following frequently-used method. The plant is in or has been transformed
into the controller canonical form
x˙ 1 = x2,
.
.
.
x˙ n−1 = xn,
x˙ n = −a0x1 − a1x2 − . . . − an−1xn + u.
(6.36)
Furthermore, the equation of the switching hyperplane is
r1x1 + . . . + rn−1xn−1 + rnxn = 0. (6.37)
Provided that rn 6= 0 holds, we can choose rn = 1 without loss of generality.
Thus, we obtain
xn = −r1x1 − . . . − rn−1xn−1. (6.38)
Substituting the state variable xn in the controller canonical form (6.36)
of the plant by using equation (6.38) yields the differential equation system
x˙ 1 = x2,
.
.
.
x˙ n−2 = xn−1,
x˙ n−1 = −r1x1 − . . . − rn−1xn−1
(6.39)
which describes the dynamics of the control loop in the sliding mode. Note
that in this case the state variable xn is given by the algebraic equation (6.38)
and the equation for x˙ n is therefore omitted. The differential equations (6.39)
are no longer dependent on the parameters ai of the plant. This means that
the control loop (6.39) is robust with regard to parameter fluctuations of the
plant. It is also noteworthy that the system order has been reduced by one
degree to n − 1 and the coefficients ri of the switching hyperplane (6.37) are
also the characteristic polynomial coefficients of the linear dynamics (6.39) in
the sliding mode.
6.2.4 Verification of Robustness
The main advantage of sliding mode controllers is their robustness with re￾gard to variations ∆A in the plant parameters or external disturbances d(t)
appearing in the system description
x˙ = (A + ∆A) x + bu + d.536 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
This means that the closed-loop dynamics depend only on the parameters ri
of the switching hyperplane s(x) = r
T x in the sliding mode, as we saw in
equation (6.39). The dynamics are independent of ∆A and d.
If both the following conditions are met [100], the robustness described
above will indeed result:
(1) There is a vector p, so ∆A = b · p
T
applies.
(2) There is an α(t), so d(t) = b · α(t) applies.
If, for example, the system is given in the controller canonical form
x˙ =










0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
−a0 −a1 −a2 · · · −an−1










x +










0
0
.
.
.
0
1










u,
the condition ∆A = b · p
T
can obviously be fulfilled if only the coefficients ai
vary.
6.2.5 Example: DC-to-DC Converter
A DC-to-DC converter is used to convert a DC voltage into a lower or higher
voltage. At the same time, it should operate with minimum losses. As an
example we will describe a buck converter [441], i. e. a device that converts
a voltage uin into a lower voltage uout. Figure 6.14 shows a converter of this
type including the corresponding controller.
The circuit functions by setting the output voltage uout based on a constant
reference voltage uref, so that
βuout =
R2
R1 + R2
uout = uref (6.40)
applies. For this purpose a power MOSFET, marked T in Figure 6.14, is used
as a switch, and is turned on or off by the controller. A switching variable z
indicates the state of the transistor T . If z = 1, the transistor conducts. If
z = 0, the transistor is disabled. The transistor is switched using a pulse width
modulation (PWM). This causes a continuous current flow iR, alternately fed
by iL and iC, so that the voltage βuout over the resistor R2 is equal to the
reference voltage uref, i. e. equation (6.40) is fulfilled.
The derivative u˙ out of the voltage across the load resistor RL and the
smoothing capacitor C is given by
u˙ out =
1
C
iC =
1
C
(iL − iR) = 1
C
Z
uinz − uout
L
dt −
uout
RG

, (6.41)6.2. Variable Structure Control with Sliding Mode 537
uref
uout
uin
x1
x2 βuout
Controller
−
β
C
R1
R2
RL
Current
measurement
C
L
T
iL iR
iC
Fig. 6.14: DC-to-DC converter
where
RG =
(R1 + R2)RL
R1 + R2 + RL
.
From equation (6.41), we obtain
u¨out = −
1
LC uout −
1
RGC
u˙ out +
uinz
LC . (6.42)
We will now define
x1 = βuout − uref, (6.43)
x2 = ˙x1 = βu˙ out
as state variables. Equations (6.42) and (6.43) result in the state-space model


x˙ 1
x˙ 2

 =


0 1
−
1
LC −
1
RGC




x1
x2

 +


0
1
LC

u, (6.44)
with the control variable u being
u = βuinz − uref. (6.45)
The system (6.44), (6.45) is ideal for the application of a sliding mode con￾trol. This is because, on the one hand, the digital control variable z already538 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
specifies a switching operation. On the other hand, the power transistor tol￾erates the high-frequency switching which occurs in sliding modes. The aim
of the control is to reach the operating point
x1 = βuout − uref = 0, (6.46)
i. e. uout = uref/β.
The system (6.44) possesses one asymptotically stable equilibrium point
xeq1 for z = 1 and one asymptotically stable equilibrium point xeq2 for z = 0.
For z = 1, this equilibrium point is
xeq1 =


βuin − uref
0

,
which, consistent with equation (6.43), results in an output voltage of uout =
uin. For z = 0, the equilibrium point is
xeq2 =


−uref
0

,
which, according to equation (6.43), results in an output voltage uout = 0.
Note that the desired operating point (6.46) of the control loop is not identical
to either of the two equilibrium points of the plant.
Let us now consider the converter’s trajectories x(t). For z = 1, they can
be calculated using equation (6.44) and are shown in Figure 6.15. For z = 0,
the trajectories can also be obtained as solutions of equation (6.44), as long
as iL > 0 holds. If the energy stored in the coil is consumed, iL = 0 holds.
Then the diode blocks and the capacitor C discharges according to
u˙ out +
1
RGC
uout = 0,
from which
uout = uout(0)e
−t/(RGC)
follows. For the trajectories x(t), with equation (6.43) we obtain
x1(t) = βuout(0)e
−t/(RGC) − uref, (6.47)
x2(t) = −βuout(0)
RGC
e
−t/(RGC)
. (6.48)
Inserting equation (6.47) into equation (6.48) results in
x2 = −
1
RGC
x1 −
uref
RGC
. (6.49)
For z = 0, the trajectories x(t) are composed of two parts. As long as iL > 06.2. Variable Structure Control with Sliding Mode 539
0
State x1
State
x2
z = 1
0
s
Fig. 6.15: Equilibrium point xeq1 and
trajectories for z = 1
0
State x1
State
x2
z = 0
iL = 0
0
s
Fig. 6.16: Equilibrium point xeq2 and
trajectories for z = 0
holds, the trajectories result from the solutions of equation (6.44). If iL = 0
holds, the trajectories move on the straight line (6.49), and on this line they
tend to the equilibrium point xeq2, according to equations (6.47) and (6.48).
Figure 6.16 shows the trajectories in question.
A switching line
s(x) = r1x1 + x2 = 0, r1 > 0, (6.50)
is now selected to separate the state space into two regions. In one region
z = 0 applies, and in the other z = 1, according to
z =
(
0 for s(x) > 0,
1 for s(x) < 0.
Figure 6.17 shows both regions and the trajectories in question. These tra￾jectories are composed of the trajectories shown in Figures 6.15 and 6.16. A
sliding mode occurs on the switching line.
Next, the necessary condition for a sliding mode, ss <˙ 0, will be used to
determine the parameter r1 of the switching line (6.50). From
ss˙ = s(x) · gradT
(s(x)) · x˙ < 0
and the system equations (6.44) and (6.45), we now obtain
ss˙ = s(x)
r1 −
1
RGC

x2 −
1
LC x1 +
βuinz − uref
LC 
< 0. (6.51)
If we set540 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
0
State x1
State
x2
z = 0
z = 1
s(x)= 0
0
s
Fig. 6.17: Trajectories, switching line (blue), and sliding mode of the DC-to-DC
converter
r1 =
1
RGC
,
the condition of existence (6.51) is independent of x2 = βu˙ out. With equation
(6.43), this results in
ss˙ = s(x)
β
LC (uinz − uout) < 0. (6.52)
From this condition, and in the case of s(x) < 0 and z = 1, we obtain
uin > uout. (6.53)
For s(x) > 0 and z = 0, and with condition (6.52),
uout > 0 (6.54)
follows. Conversely, if equations (6.53) and (6.54) are fulfilled, so is condition
(6.52), which is necessary for the existence of the sliding mode. Furthermore,
the trajectories of the control system also tend to the sliding mode in finite
time, since
s˙ =
β
LC (uinz − uout)
generates a constant decrease rate of
s˙ = −uout
β
LC < 0 for s(x) > 0 and z = 0
or a constant increase rate of6.2. Variable Structure Control with Sliding Mode 541
s˙ = (uin − uout)
β
LC > 0 for s(x) < 0 and z = 1.
The dynamics of the controlled converter in sliding mode are calculated
using
s(x) = r1x1 + x2 = 0 and x2 = ˙x1,
yielding
x1(t) = x1(0)e
−r1t
.
As we can see, the dynamics in the sliding mode are independent of changes in
the load resistance RL and fluctuations in the input voltage, i. e. the battery
voltage uin.
Let us examine a concrete implementation with the following parameters:
RL = 6 Ω, uin = 24 V,
L = 110 µH, uout = 12 V,
C = 100 µF, uref = 3.3 V.
Hence,
β =
uref
uout
= 0.275.
From
R2 =
βR1
1 − β
and R1 = 870 Ω it follows that R2 = 330 Ω. The time constant 1/r1 in sliding
mode is 0.6 ms.
Figure 6.18 shows the output voltage in the event of a sudden decrease
of 3 V in the input voltage uin. As expected, the output voltage uout does
not change. In real devices a voltage drop of several 10 mV can be observed.
Figure 6.19 shows the turning-on process. The output value of uout = 12 V is
reached after a time of 5/r1 = 3 ms.
12.5
12
11.5
11
27
25
23
21
19
0 0.0025 0.005 0.0075 0.01
Time t in s
Output voltage
uout in V
Input voltage
uin in V
Fig. 6.18: Control of the output voltage
uout after a drop in the input voltage uin
14
12
10
8
6
4
2
0
0 0.0025 0.005 0.0075 0.01
Time t in s
Output voltage
uout in V
Fig. 6.19: Time course of the out￾put voltage uout after the converter
is turned on542 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
6.2.6 Design for Nonlinear Plants
In certain cases, the design of sliding mode controllers for nonlinear plants
is no more difficult than for linear plants. Among other types of plants, this
applies to the control-affine plants
x˙ = a(x) + b(x) · u,
y = c(x)
(6.55)
with a relative degree of δ = n. The design is similar for δ < n.
We can convert the system representation above into the nonlinear con￾troller canonical form as described in Section 5.2.1, p. 358 et seq., for controller
design by means of feedback linearization. For this purpose, we will use the
Lie derivative
Lf h(x) = ∂h(x)
∂x
f(x) = gradT
(h(x)) · f(x).
First, we will compute the derivatives
y = c(x),
y˙ =
∂c(x)
∂x
x˙ = Lac(x),
y¨ =
∂Lac(x)
∂x
x˙ = L
2
a
c(x),
.
.
.
y
(n−1) =
∂Ln−2
a
c(x)
∂x
x˙ = L
n−1
a
c(x),
y
(n) =
∂Ln−1
a
c(x)
∂x
x˙ = L
n
a
c(x) + LbL
n−1
a
c(x) · u.
By using the new state coordinates
z1 = y, z2 = ˙y, . . . , zn = y
(n−1)
,
and the resulting transformation z = t(x), we obtain the representation of
the system (6.55) in nonlinear controller canonical form







z˙1
.
.
.
z˙n−1
z˙n







=







z2
.
.
.
zn
L
n
a
c(x) + LbL
n−1
a
c(x) · u







=







z2
.
.
.
zn
α(z)







| {z }
a˜(z)
+







0
.
.
.
0
β(z)







| {z }
˜b(z)
u (6.56)6.2. Variable Structure Control with Sliding Mode 543
with
α(z) = L
n
ac(t
−1
(z)) and β(z) = LbL
n−1
a c(t
−1
(z)).
The design of a sliding mode controller with switching hypersurface
s(z) = 0
is similar to the linear case. We will again use the approach
s˙(z) = −q sgn(s(z)) − ks(z) (6.57)
developed by Gao and Hung. Recall that this ensures the trajectories z(t) will
reach the switching plane in finite time. Again,
q > 0 and k > 0
apply.
Using
s˙(z) = gradT
(s(z)) · z˙
and equation (6.56) as well as equation (6.57) yields the control law
u(z) = −
gradT
(s(z)) · a˜(z) + q sgn(s(z)) + ks(z)
gradT
(s(z)) · ˜b(z)
.
In the case of a switching hyperplane s(z) = r
T
z = 0, this simplifies to
u(z) = −
r
T a˜(z) + q sgn(r
T z) + k r
T z
r
T ˜b(z)
. (6.58)
If we set r
T = [r1 r2 · · · rn−1 1], for equation (6.58) we obtain
u(z) = −
r1z2 + . . . + rn−1zn + k r
T z + α(z) + q sgn(r
T z)
β(z)
. (6.59)
Inserted into the plant (6.56), this results in the control-loop dynamics







z˙1
.
.
.
z˙n−1
z˙n







=







z2
.
.
.
zn
−kr1z1 − (r1 + kr2)z2 − . . . − (rn−1 + k)zn







−







0
.
.
.
0
q sgn(r
T
z)







.
This control loop is free of the nonlinearities of the plant. Further information
on topics such as sliding mode control for nonlinear MIMO systems can be
found in [89, 145, 360, 418].544 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
6.2.7 Example: Optical Switch
In the following, we will design a sliding mode controller for an optical switch.
This is implemented as a microelectromechanical system, abbreviated MEMS,
and serves to switch the light path between different optical fibers [54, 349]. In
this way it is possible to transmit an optical signal from one optical waveguide
to another and back again, without conversion to an electrical signal.
Figure 6.20 shows the structure of this kind of optical switch. The comb
drive converts an electrical voltage v, which serves as the switch’s input signal,
into a translational change of the position x1 of a rod. A mirror is attached to
the rod, enabling it to be pushed into and out of the optical path. The model
x˙ 1 = x2,
x˙ 2 = −
c
m
x1 −
1
m
(d1x1 + d0) · x2 +
ke
m
u + d
(6.60)
of the optical switch is in nonlinear controller canonical form. Here, u =
v
2
holds, while y = x1 represents the system’s output variable, and d is a
mechanical, time-dependent disturbance. The available voltage v is limited
by 0 ≤ v ≤ 35 V.
The parameters are the moving mass m = 2.35 · 10−9
kg, the spring
constant of the suspension c = 0.6 N m−1
, the damping coefficients d1 =
0.0363 kg s−1m−1
and d0 = 5.45 · 10−7
kg s−1
, and the converter constant
ke = 17.8 · 10−9 N V−2
of the comb drive. The values of the damping coeffi￾cients d0 and d1 must be regarded as uncertain estimates. In the following,
Laser
Comb drive Damping Mirror
x1
Fig. 6.20: Optical switch in microelectromechanical design6.2. Variable Structure Control with Sliding Mode 545
we will use the abbreviations
a0 =
c
m
= 2.55 · 108 N m−1
kg−1
,
a1 =
d1
m
= 1.54 · 107
s
−1m−1
,
a2 =
d0
m
= 232 s−1
,
b =
ke
m
= 7.57 N kg−1V
−2
.
Now, our aim is to be able to adjust different positions x1eq of the mirror.
These target positions x1eq are equal to the first element of the equilibrium
points
xeq =




b
a0
ueq
0




, (6.61)
where ueq is the control variable. By using equation (6.61) and the transfor￾mation rule
x1 = x1eq + ∆x1 =
b
a0
ueq + ∆x1
with the new variable ∆x1, we obtain
∆x˙ 1 = x2,
x˙ 2 = −(a0 + a1 x2)(x1eq + ∆x1) − a2 x2 + b u + d
(6.62)
for the model (6.60).
Now we will take the linear switching line
s(x) = r · ∆x1 + x2 = 0
with the parameter r yet to be determined. With the approach developed by
Gao and Hung, we obtain
s˙(x) = −q sgn(s(x)) − k s(x)
from equation (6.57). If we equate this with
s˙(x) = gradT
(s(x)) · (a(x) + b(x)u),
the control law, which we had already derived in equation (6.59) for the general
case, takes the form
u(x) = −
rx2−(a0 + a1x2)(x1eq + ∆x1)−a2x2+q sgn(s(x))+ks(x)
b
(6.63)546 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
for the present example. The parameters q and k are constants which need to
be suitably chosen. Inserting the control law (6.63) into the system dynamics
(6.62), the resulting expression for the control loop is
∆x˙ 1 = x2,
x˙ 2 = −rx2 − q sgn(s(x)) − ks(x) + d.
Using s(x) = r∆x1 + x2, it becomes
∆x˙ 1 = x2,
x˙ 2 = −kr∆x1 − (r + k)x2 − q sgn(r∆x1 + x2) + d.
(6.64)
If the nonlinear part is ignored, i. e. q = 0, this control loop has linear
dynamics with the characteristic polynomial
P(s) = s
2 + (r + k)s + rk.
The polynomial has the two zeros s1 = −r and s2 = −k, which are identical
to the two eigenvalues of the linear part of the control loop’s system dynamics
(6.64).
In the case of the sliding mode,
s(x) = r∆x1 + x2 = 0
holds. Then we have the reduced system dynamics
∆x˙ 1 = −r∆x1
and the algebraic equation
x2 = −r∆x1
for the second state variable.
We can simulate the control loop (6.64) for the parameters k = 20000,
r = 20000, and q = 1000 for different damping values
d1 = dnom = 0.0363,
d1 = 0,
d1 = 10dnom,
d1 = 100dnom.
Here, we will take an initial displacement of x1(0) = 25 µm and an initial
velocity of x2(0) = 0 µm s−1
. The first element x1eq of the equilibrium point
xeq is at 10 µm. Thus, ∆x1(0) = 15 µm. For the disturbance, we have assumed
d = 0. Figure 6.21 shows the progressions of the deviation ∆x1 for the four
different damping values. In all four cases, good control behavior becomes
apparent. The control loop is in sliding mode for curves in blue, and is still6.3. Passivity-Based Control 547
0
10
5
15
0 100 200 300 400
∆x1 in
µ
m
Time t in µs
d1 = dnom
d1 = 0
d1 = 10 dnom
d1 = 100 dnom
Fig. 6.21: Compensation of the displacement ∆x1 = 15 µm to the equilibrium point
∆x1 = 0 µm for different damping values d1. For blue curve components the system
is in sliding mode, for black curve components it is not.
moving toward the switching line for curves in black. The progressions for the
values d1 = dnom, d1 = 10dnom, and d1 = 0 are almost identical.
Note that a sliding mode control is only robust if it is in sliding mode.
If the plant’s parameters vary so that the sliding mode no longer exists or
can no longer be achieved, the control loop is no longer robust in the way we
have described. This is illustrated by the case of d1 ≫ dnom. In this case, the
sliding mode which generates robustness is reached very late.
6.3 Passivity-Based Control
6.3.1 Control of Passive Systems Using Static Controllers
In Section 2.4, we described passive systems and stated that their equilibrium
point xeq = 0, provided that the systems have a positive definite storage
function, is Lyapunov stable. In particular, strictly passive systems have an
asymptotically stable equilibrium point at x = 0. We are therefore interested
in designing a control loop so that it is strictly passive.
First, we will look at a very simple passivity-based control loop. It is based
on a strictly passive plant
x˙ = f(x, e2),
y2 = g(x)
and a passive static controller
y1 = h(e1, t),548 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
y1 = h(e1, t)
y1
x˙ = f(x, e
y2 2)
y2 = g(x)
u1
u2
e1 e2
Controller Plant
Fig. 6.22: Control loop with strictly passive plant and passive static controller
which may be time-variant. Along with
e1 = u1 − y2
,
e2 = u2 + y1
,
they result in the control loop shown in Figure 6.22. Remember Section 2.4.1,
p. 143 et seq., where we found that the static controller
y1 = h(e1, t),
which may be time-variant, is passive if
0 ≤ e
T
1 y1
(6.65)
applies. In the SISO case, 0 ≤ e1y1, this requirement means that the charac￾teristic curve must lie in the first and the third quadrants. Such characteristics
include two-position and three-position elements, among others.
In the following, it is assumed that S(x) is the plant’s storage function,
and R(x) is a positive definite function, so that the inequality
S˙(x) + R(x) ≤ e
T
2 y2
(6.66)
for the strict passivity of the plant is fulfilled. We will also use S(x) as a
storage function of the control loop and the function R(x) to prove the strict
passivity of the control loop. With the composite input variable vector
u =


u1
u2

 =


e1 + y2
e2 − y1


and the output variable vector
y =


y1
y2


of the control loop, we obtain the inequality6.3. Passivity-Based Control 549
S˙(x) + R(x) ≤ u
T y = (e1 + y2
)
T y1 + (e2 − y1
)
T y2 = e
T
1 y1 + e
T
2 y2
(6.67)
of strict passivity. Assuming that equation (6.65) applies, if the passivity in￾equality (6.66) of the plant is fulfilled, the passivity inequality (6.67) of the
control loop is also. The control loop is therefore strictly passive and thus has
an asymptotically stable equilibrium point. This is stated in
Theorem 80 (Control of Strictly Passive Plants Using Passive Static
Controllers). Let a passive static controller and a strictly passive plant, which
are given as
y1 = h(e1, t) and x˙ = f(x, e2),
y2 = g(x)
with x ∈ IRn
, e1, e2, y1
, y2 ∈ IRm, and
e1 = u1 − y2
and e2 = u2 + y1
with u1, u2 ∈ IRm, form the control loop
x˙ = f(x,u2 + y1
),
y1 = h(u1 − y2
, t),
y2 = g(x).
This control loop possesses an asymptotically stable equilibrium point at x = 0
for u = [u
T
1 u
T
2
]
T = 0. If the plant’s storage function S(x) is radially
unbounded, the equilibrium point is globally asymptotically stable.
Let us now consider the case in which the plant is only passive. Even then,
an asymptotically stable control loop can be created using a passive static
controller. However, additional conditions must be imposed in this case. For
this purpose, we need the property of zero-state detectability, which we define
in
Definition 37 (Zero-State Detectability). A system
x˙ = f(x,u), x ∈ IRn
, u ∈ IRm,
y = g(x,u), y ∈ IRr
,
is called zero-state detectable if
lim
t→∞
x(t) = 0
results for the case that u(t) = 0 and y(t) = 0 hold for all t ≥ 0.
Now we will start again from equation (6.67), which must be valid for the
control loop in this case as well. However, the controlled system is only passive
here, i. e.550 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
S˙(x) ≤ e
T
2 y2
applies instead of equation (6.66). Therefore,
e
T
1 y1 = e
T
1 h(e1, t) > 0 for all e1 6= 0
must be valid for the fulfillment of equation (6.67). Furthermore, the limit
lim
t→∞
x(t) = 0 for all t ≥ 0
must follow from e2(t) = 0 and y2
(t) = 0, i. e., for this case, the plant’s
trajectories tend to zero and remain there. The latter is equivalent to the
controlled system being zero-state detectable. If this were not the case, limit
cycles x(t) with S˙(x) = 0 could exist which cannot be observed based on the
input variable vector u(t) and the output variable vector y(t).
Similar to Theorem 80[2], and with the statements above, we can now
formulate the subsequent theorem [65].
Theorem 81 (Control of Passive Plants Using Passive Static Con￾trollers). Let the control loop from Theorem 80 be given, with the difference
that the plant is passive and the passive static controller satisfies e1h(e1, t) > 0
for e1 6= 0. If the plant has a positive definite storage function and is zero￾state detectable, then the control loop has an asymptotically stable equilibrium
point at x = 0 for u = 0. If the storage function is radially unbounded, the
equilibrium point is globally asymptotically stable.
We will now describe an example of the application of Theorem 81.
6.3.2 Example: Damping of Seismic Building Vibrations
Earthquakes trigger vibrations in buildings with steel girders, which can dam￾age them and even cause them to collapse. These building vibrations can be
attenuated using various damping methods. A possible approach is utilizing
viscous fluid dampers that are mounted on one or more floors to compen￾sate for seismic vibrations [434, 438, 439]. Figure 6.23 illustrates the case of a
building vibrating due to an earthquake.
The dynamics of the building are those of a multi-mass oscillator. The walls
act both as springs and dampers. We can therefore model two superimposed
floors using their masses mi and mi+1, connected by a spring with the spring
constant ci and a damper with the damping constant di
[350].
The building is set to vibration by the seismically induced acceleration
x¨g of the building’s foundation. We can model this acceleration as acting
immediately and with equal strength on all floors – except the foundation –
[2] In [65], Theorem 81 is only proven for time-invariant characteristics h(e1). How￾ever, the argument therein remains valid even with time-variant characteristics
h(e1, t).6.3. Passivity-Based Control 551
x¨g
m1, c1, d1
m2, c2, d2
m3, c3, d3
m4, c4, d4
m5, c5, d5
x1
x2
x3
x4
x5
Fig. 6.23: A building with viscous fluid dampers on each floor. The parameters mi,
ci, di, i = 1, . . . , 5, indicate the masses, spring constants, and damping constants of
the individual stories.
with the sign reversed. The damping forces F1, . . . , F5 of the viscous fluid
dampers counteract it. This results in the dynamics for the movement of the
floor on the first story,
m1x¨1 = −d1x˙ 1 − d2( ˙x1 − x˙ 2) − c1x1 − c2(x1 − x2) + F1 + m1x¨g,
and for those on the second, third, and fourth stories,552 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
m2x¨2 =−d2( ˙x2−x˙ 1) − d3( ˙x2−x˙ 3) − c2(x2−x1) − c3(x2−x3) + F2 + m2x¨g,
m3x¨3 =−d3( ˙x3−x˙ 2) − d4( ˙x3−x˙ 4) − c3(x3−x2) − c4(x3−x4) + F3 + m3x¨g,
m4x¨4 =−d4( ˙x4−x˙ 3) − d5( ˙x4−x˙ 5) − c4(x4−x3) − c5(x4−x5) + F4 + m4x¨g,
as well as for the dynamics of the fifth and highest story,
m5x¨5 = −d5( ˙x5 − x˙ 4) − c5(x5 − x4) + F5 + m5x¨g.
In summary, for the aforementioned five equations of motion in matrix nota￾tion, we obtain
Mx¨ = −Dx˙ − Cx + F + M1x¨g (6.68)
with
M =










m1 0 0 0 0
0 m2 0 0 0
0 0 m3 0 0
0 0 0 m4 0
0 0 0 0 m5










, D =










d1 + d2 −d2 0 0 0
−d2 d2 + d3 −d3 0 0
0 −d3 d3 + d4 −d4 0
0 0 −d4 d4 + d5 −d5
0 0 0 −d5 d5










,
F =










F1
F2
F3
F4
F5










, 1 =










1
1
1
1
1










, C =










c1 + c2 −c2 0 0 0
−c2 c2 + c3 −c3 0 0
0 −c3 c3 + c4 −c4 0
0 0 −c4 c4 + c5 −c5
0 0 0 −c5 c5










.
We can now extend the position vector x = [x1 x2 x3 x4 x5]
T
by the
derivatives x˙ 1, . . . , x˙ 5 and obtain the ten-dimensional state vector


x
x˙

.
In this way, from the five second-order differential equations (6.68), we obtain
the ten first-order differential equations


x˙
x¨

 =


0 I
−M−1C −M−1D




x
x˙

 +


0
M−1

F +


0
1

x¨g. (6.69)
As a five-dimensional output variable vector, we will use6.3. Passivity-Based Control 553
y = x˙. (6.70)
System (6.69), (6.70) is passive, which we can prove with the storage func￾tion
S(x, x˙) = 1
2
x˙
TMx˙ +
1
2
x
T Cx (6.71)
by inserting it into the passivity inequality
S˙(x, x˙) ≤ (F + M1x¨g)
T
| {z }
u
T
y,
where the force vector F and the disturbance M1x¨g act as input variable
vectors. Thus, inequality
S˙(x, x˙) = 1
2
(x¨
TMx˙ + x˙
TMx¨ + x˙
T Cx + x
T Cx˙)
= −x˙
TDx˙ + (F + M1x¨g)
T
y ≤ (F + M1x¨g)
T
y
follows, which is fulfilled due to the positive definiteness of the matrix D.
Thus, system (6.69), (6.70) is passive. Furthermore, it is also zero-state de￾tectable, because it is asymptotically stable, i. e. it holds that
lim
t→∞


x(t)
x˙(t)

 = 0,
which we obtain if F(t)=0, x¨g = 0, and y(t)=0 hold for all t≥0.
In this example, viscous fluid dampers are mounted on each story of the
building. They consist of a hydraulic cylinder and a moving piston. If the
piston moves, it generates a force
F˜ = ˜d · |v|
α
sgn (−v)
due to viscous friction, which counteracts the movement of the piston and
depends on its speed v, as shown in Figure 6.24. Here, ˜d is the damping
coefficient and α is a design constant with 0 < α ≤ 1 [190].
The viscous fluid dampers exert the forces
F˜
1 = ˜d1|x˙ 1|
α
sgn (−x˙ 1),
F˜
2 = ˜d2|x˙ 2 − x˙ 1|
α
sgn ( ˙x1 − x˙ 2),
F˜
3 = ˜d3|x˙ 3 − x˙ 2|
α
sgn ( ˙x2 − x˙ 3),
F˜
4 = ˜d4|x˙ 4 − x˙ 3|
α
sgn ( ˙x3 − x˙ 4),
F˜
5 = ˜d5|x˙ 5 − x˙ 4|
α
sgn ( ˙x4 − x˙ 5)554 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
0
0
1
1
−1
−1
α = 0.25
α = 0.75
α = 0.5
α = 1
v in m s −1
˜F in N
Fig. 6.24: Curve of the damping force F˜ = d˜· |v|
α
sgn (−v) for different values α and
˜d = 1
on the masses mi
. Figures 6.25 and 6.26 illustrate these forces. For the total
forces acting on the masses mi
, we obtain
F(x˙)=












F1
F2
F3
F4
F5












=












F˜
1−F˜
2
F˜
2−F˜
3
F˜
3−F˜
4
F˜
4−F˜
5
F˜
5












=












1 −1 0 0 0
0 1 −1 0 0
0 0 1 −1 0
0 0 0 1 −1
0 0 0 0 1
























˜d1|x˙ 1|
α
sgn (−x˙ 1)
d˜
2|x˙ 2 − x˙ 1|
α
sgn ( ˙x1 − x˙ 2)
˜d3|x˙ 3 − x˙ 2|
α
sgn ( ˙x2 − x˙ 3)
d˜
4|x˙ 4 − x˙ 3|
α
sgn ( ˙x3 − x˙ 4)
˜d5|x˙ 5 − x˙ 4|
α
sgn ( ˙x4 − x˙ 5)












.
(6.72)
This nonlinearity is passive because it satisfies the inequality (6.65), i. e.
0 < e
T
1 y1 = y
T
1 e1,
for all e1 6= 0. To prove this, we use
e1 = −x˙ and y1 = F(x˙) = F(−e1)
and obtain
y
T
1 e1 = F
T
(−e1)e1 = −F
T
(x˙)x˙.
From the latter, the passivity inequality6.3. Passivity-Based Control 555
c1 c2 c5
m1 m2 m5
d1 d2 d5
d˜1
˜d2
˜d5
x1 x2 − x1
Fig. 6.25: Model of a building based on springs, masses, and dampers
m1 m2 m5
F˜1 F˜1 F˜2 F˜2 F˜3 F˜5
Fig. 6.26: Diagram of the damping forces F˜i
F (−e1) Mx¨=−Dx˙ −Cx+F +M1x¨g
e1
y1 =F x˙
1x¨g
Fig. 6.27: Representation of the building model with viscous fluid dampers, shown
as a control-loop structure
y
T
1e1 =−F
T
(x˙)x˙ =











− ˜d1|x˙ 1|
α
sgn (−x˙ 1)
− ˜d2|x˙ 2 − x˙ 1|
α
sgn ( ˙x1 − x˙ 2)
− ˜d3|x˙ 3 − x˙ 2|
α
sgn ( ˙x2 − x˙ 3)
− ˜d4|x˙ 4 − x˙ 3|
α
sgn ( ˙x3 − x˙ 4)
− ˜d5|x˙ 5 − x˙ 4|
α
sgn ( ˙x4 − x˙ 5)











T










1 0 0 0 0
−1 1 0 0 0
0 −1 1 0 0
0 0 −1 1 0
0 0 0 −1 1






















x˙ 1
x˙ 2
x˙ 3
x˙ 4
x˙ 5











= ˜d1|x˙ 1|
α+1 +
X
5
i=2
˜di
|x˙ i − x˙ i−1|
α+1 > 0
results, which holds for all e1 = −x˙ 6= 0.
We can represent the model (6.68) of the building and the model (6.72)
of the viscous fluid dampers as a control-loop structure, as shown in Figure
6.27. This is a special case of the control-loop structure shown in Figure 6.22.556 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
However, this control-loop structure is inherent to the system and not
realized by a technical implementation, since we do not have to measure the
velocity vector x˙. Rather, the viscous fluid dampers intrinsically make use
of these velocities through the movement of their pistons with a velocity of
x˙ i
. From a mechanical point of view, the viscous fluid dampers are damping
elements connected in parallel with the building dampers. Nevertheless, the
interpretation of the overall dynamics as a feedback structure is useful, because
in this way we can prove the stability using Theorem 81.
Since the plant is a zero-state detectable passive system with the radially
unbounded, positive definite storage function (6.71) and the controller is a
passive nonlinearity which fulfills the condition of Theorem 81, we can con￾clude that the equilibrium point xeq = 0 of the undisturbed control loop is
globally asymptotically stable. Of course, we were aware of this previously
due to our understanding of the mechanical design.
In the following, we will describe a concrete application. In this case, the
building has five similar stories with the masses mi
, spring constants ci
, and
damping coefficients di
[350], which are in detail
m1 = 215.2 · 103
kg, c1 = 147 · 106 N m−1
, d1 = 113.8 · 103 N s m−1
,
m2 = 209.2 · 103
kg, c2 = 133 · 106 N m−1
, d2 = 92.4 · 103 N s m−1
,
m3 = 207.0 · 103
kg, c3 = 99 · 106 N m−1
, d3 = 81.0 · 103 N s m−1
,
m4 = 204.8 · 103
kg, c4 = 89 · 106 N m−1
, d4 = 72.8 · 103 N s m−1
,
m5 = 266.1 · 103
kg, c5 = 84 · 106 N m−1
, d5 = 68.7 · 103 N s m−1
.
The damping coefficients of the viscous fluid dampers are
˜d1 = 6506 · 103 N s m−1
,
˜d2 = 4343 · 103 N s m−1
,
˜d3 = 3455 · 103 N s m−1
,
˜d4 = 2914 · 103 N s m−1
,
˜d5 = 2648 · 103 N s m−1
,
and their parameter α is
α = 0.5125.
An example is the 1995 earthquake in K¯obe, Japan [140]. The quake lasted
about 20 s and reached a strength of 6.9 on the moment-magnitude scale, de￾stroying 170000 buildings and killing more than 6500 people. The elevated
highway collapsed along a length of 5 km. The horizontal acceleration x¨g
caused by the K¯obe quake and the shift xg resulting from the double inte￾gration of x¨g are both shown in red in Figure 6.28. The graph also shows6.3. Passivity-Based Control 557
Time t in s Time t in s
2
2
2
2
2
−2
−2
−2
−2
−2
50
50
50
50
50
−50
−50
−50
−50
−50
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
10
10 10
10
10
10
10
10
10
10
10
10
10
20 20
20
20
20
20
20
20
20
20
20
20
−10
30 30
30
30
30
30
30
30
30
30
30
30
0.5
−0.5
x1 in m
x2 in m
x3 in m
x4 in m
x5 in m
xg in m
¨x1 in m s
−2
¨x2 in m s
−2
¨x3 in m s
−2
¨x4 in m s
−2
¨x5 in m s
−2
¨xg in m s
−2
Fig. 6.28: The left column shows the shift xg (red) in the foundation caused by
the earthquake in K¯obe and the shift xi of the floors. The right column shows the
acceleration x¨g (red) of the foundation and the acceleration x¨i of the floors. The
blue curves show the building vibrations without viscous fluid dampers, while the
black curves show the building vibrations with these dampers.558 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
the accelerations x¨i acting on the respective stories i of our model building
and which, multiplied by the respective masses mi
, result in the acting forces.
Also shown are the displacements xi of the stories i, both with and with￾out viscous fluid dampers. With these dampers, the load on the building is
greatly reduced, so that the effect of the quake on the building is significantly
decreased.
Because this method effectively reduces vibrations, additional damping
devices, such as the viscous fluid dampers described above, are often used for
large buildings, bridges, and tall towers located in earthquake-prone areas.
6.3.3 Passivation of Non-Passive Linear Systems
For non-passive systems, the controller design methods from Section 6.3.1
cannot be applied at this stage. A useful procedure in this case is to make the
non-passive system passive by extending it with additional external circuitry,
such as a system connected in parallel, or feedback via an additional system.
After this passivation of the system, we can apply the theorems for passive
systems and the appropriate controller design procedures.
Let us first address the controllable linear SISO systems
x˙ = Ax + b · u,
y = c
T x
(6.73)
before turning to the general case of control-affine MIMO systems in the
following section.
Now our aim is to derive conditions under which a linear system can be
made passive using a static state feedback, i. e. a controller
u = −r
T x + V · yref
with the reference variable yref.
For this purpose we will use Theorem 28 from p. 153, the theorem on pas￾sivity and the Kalman-Yakubovich-Popov equations. If a SISO system (6.73)
is passive, the KYP equations
A
T R + RA = −Q, (6.74)
c
T = b
T R (6.75)
are valid, where R is a positive definite matrix and Q is a positive semidefinite
matrix.
We now multiply equation (6.75) by vector b and obtain
c
T
b = b
T Rb > 0 (6.76)
as a necessary condition for the passivity of the system (6.73). We will also
perform the same steps as for an exact input-output linearization, deriving
the output variable y according to the equations6.3. Passivity-Based Control 559
y = c
T x,
y˙ = c
T x˙ = c
T Ax | {z }
Lac(x)
+ c
T
b
|{z}
Lbc(x)
· u,
y¨ = . . .
.
.
.
The first Lie derivative
Lbc(x) = c
T
b
is already unequal to zero according to equation (6.76) if the system is passive.
Consequently, a passive system (6.73) must have a relative degree of δ = 1
due to Lbc(x) > 0.
We will derive a further necessary condition for the passivity of the system
(6.73). For this purpose, we will transform system (6.73) using the diffeomor￾phism
z = t(x) =







c
T x
t
T
2 x
.
.
.
t
T
n x







= T x (6.77)
with suitable linearly independent vectors ti
, i = 2, . . . , n. This will give us
z˙ =







c
T
t
T
2
.
.
.
t
T
n







x˙ =







c
T Ax + c
T
b · u
t
T
2 Ax + t
T
2 b · u
.
.
.
t
T
nAx + t
T
n
b · u







,
y = z1.
(6.78)
To eliminate the dependence of the terms
t
T
i Ax + t
T
i
b u, i = 2, . . . , n,
from equation (6.78) on the control value u, we now select the vectors ti
,
i = 2, . . . , n, so that





t
T
2
.
.
.
t
T
n





b = 0
applies. This means that all ti are perpendicular to b. Since according to
equation (6.76), the inequality c
T
b 6= 0 applies. If now the vector b is not560 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
a linear combination of the vectors ti
, i = 2, . . . , n, the vector c is also not
a linear combination of the vectors ti
, i = 2, . . . , n, alone. Consequently, the
vectors c and ti
, i = 2, . . . , n, form a base of IRn
and the matrix T is regular.
This results in
z˙ =








c
T AT −1
z + c
T
b · u
t2AT −1
z
.
.
.
tnAT −1
z








,
y = z1.
(6.79)
external dynamics
internal dynamics
To shorten this, we can define the vector
zint = [z2 · · · zn]
T
of internal states and obtain
z˙1 = Γ11z1 + Γ
T
12zint + c
T
b · u, } external dynamics
z˙int = Γ21z1 + Γ22zint, } internal dynamics
y = z1,
(6.80)
where
z =


z1
zint

,
h
Γ11 Γ
T
12 i
=c
T AT −1
, and h
Γ21 Γ22 i
=





t2AT −1
.
.
.
tnAT −1





hold.
Besides condition (6.75), equation (6.74) is the second requirement to be
fulfilled. It is equivalent to saying that the equilibrium point xeq = 0 of the
system (6.73) is Lyapunov stable. Because the linear transformation (6.77) is
regular, equation (6.74) tells us that the transformed system (6.80) must also
be Lyapunov stable. In particular, its internal dynamics must be Lyapunov
stable, i. e. the matrix Γ22 may only have eigenvalues with a nonpositive real
part. This is the second necessary condition we were attempting to identify.
Thus, we obtain
Theorem 82 (Passivity of Linear Systems). A passive system
x˙ = Ax + b · u,
y = c
T x
with a positive definite storage function has6.3. Passivity-Based Control 561
(1) a relative degree of δ = 1 and
(2) internal dynamics which are Lyapunov stable.
For the internal dynamics, or equivalently the zero dynamics, to be Lya￾punov stable, according to Theorem 16 from p. 121, it is sufficient and neces￾sary for a Lyapunov function
V (zint) = zint
T Rz˜ int
to exist. This means that a positive definite matrix R˜ exists, so that the
matrix Q˜ from the Lyapunov equation
Γ
T
22R˜ + RΓ˜ 22 = −Q˜ (6.81)
is negative semidefinite. This condition will be required later.
At this point recall Section 5.2.8, p. 389 et seq., in which we defined zero
dynamics as a special case of internal dynamics. Zero dynamics are derived
from internal dynamics when we set the input variables of the internal dynam￾ics, which are the external dynamics’ state variables, to zero. For the stability
analysis using Condition (2) of the previous theorem, we can therefore also
use the system’s zero dynamics and require that they be Lyapunov stable.
The two necessary conditions of the previous theorem are equivalent to
Conditions (2) and (3) of Theorem 33, p. 161. The requirement is that a
relative degree δ = 1 be equal to a difference between the denominator degree
n and the numerator degree m of the transfer function G(s), i. e. δ = n−m = 1.
The requirement that the zero dynamics be Lyapunov stable is identical to
the requirement that the real parts of all their eigenvalues be nonpositive with
at most one at zero.
If we now use static feedback, i. e. a controller
u = −r
T x + V · yref (6.82)
with the reference variable or new input variable yref, to make a non-passive
system (6.73) passive, we have to consider the following: based on equation
(6.80), it is immediately apparent that the relative degree and stability of the
internal dynamics cannot be changed by feedback (6.82). A relative degree of
δ = 1 and Lyapunov stable internal dynamics are thus necessary conditions
for the passivability of the system (6.73).
Furthermore, these conditions are also sufficient, as shown below. We first
notice that we can always stabilize the external dynamics in equation (6.80)
using the control law (6.82). In addition, we can always passivate the system
(6.80) by means of a suitable choice of the control law (6.82) as well. A suitable
choice is
u = −
1
c
T b

Γ11z1 + Γ
T
12zint + 2Γ
T
21Rz˜ int − yref
. (6.83)
We will prove this below. Inserting the control law (6.83) into the system
representation (6.80) leads to the equations562 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
z˙1 = −2Γ
T
21Rz˜ int + yref, } external dynamics
z˙int = Γ22zint + Γ21z1, } internal dynamics
y = z1
of the closed loop.
Its passivity can be verified by inserting the storage function
S(z1, η) = 1
2
z
2
1 + zint
T Rz˜ int
into the passivity inequality S˙(z1, zint) ≤ yref y, i. e.
S˙(z1, zint) = z1z˙1 + z˙int
T Rz˜ int + zint
T R˜z˙int
= zint
T
(Γ
T
22R˜ + RΓ˜ 22)zint + yrefz1 ≤ yref y.
Obviously, this inequality is fulfilled because y = z1 and
zint
T
(Γ
T
22R˜ + RΓ˜ 22)zint ≤ 0
apply. The latter holds because the zero dynamics z˙int = Γ22zint are assumed
to be Lyapunov stable. We stated this previously in equation (6.81). With
equation (6.83), we can always specify a static control law
u = −r˜
T
z + V · yref = −r
T x + V · yref, r
T = r˜
T T,
which passivates the system (6.80) and its original representation (6.73).
Thus we arrive at
Theorem 83 (Passivability of Linear Systems). A non-passive system
x˙ = Ax + b · u,
y = c
T x
can be converted into a passive control loop with a quadratic positive definite
storage function by a static controller
u = −r
T x + V · yref
if and only if the system
(1) has a relative degree of δ = 1 and
(2) its internal dynamics are Lyapunov stable.
We have already established that we cannot change the relative degree or
stability of the internal dynamics using static feedback. However, the stability
of the external dynamics can certainly be influenced by such feedback. For
example, we can stabilize an unstable system. The opposite situation occurs
when we use a parallel arrangement with an additional system instead of6.3. Passivity-Based Control 563
the feedback. In this case we cannot influence the external dynamics, i. e.
the locations of the poles of the system to be passivated. However, we can
manipulate its relative degree and internal dynamics, i. e. its numerator degree
and the location of the zeros, respectively. This becomes plausible if we take
a system
G1(s) = 1
D(s)
which is non-passive but stable as an example, and combine it with a system
G2(s) = N(s)
D(s)
to obtain
G1(s) + G2(s) = 1
D(s)
+
N(s)
D(s)
=
1 + N(s)
D(s)
by connecting it in parallel. Obviously, using the numerator N(s), we can
change the numerator degree and thus the relative degree and the zeros. Con￾sequently, the internal dynamics are changed as well.
In the following section, we will extend the results from this section to
MIMO control-affine systems.
6.3.4 Passivation of Non-Passive Control-Affine Systems
Let us now examine control-affine systems
x˙ = a(x) + B(x) · u,
y = c(x).
(6.84)
As in the preceding section, we will first clarify the conditions that must
be fulfilled for these systems to be passive before we determine how such
a system can be passivated. Similar to Theorem 82, the following theorem
applies [58, 65].
Theorem 84 (Passivity of Control-Affine Systems). A passive system
x˙ = a(x) + B(x) · u,
y = c(x)
with a two times continuously differentiable, positive definite storage function
has
(1) the vector relative degree δ = [1 · · · 1] and
(2) internal dynamics with an equilibrium point which is Lyapunov stable.564 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
In the following, we will address non-passive MIMO systems (6.84). Be￾cause we cannot change the relative degree of such a system using a static
controller and a passive system (6.84) must have a relative degree
δ = [1 · · · 1],
from the outset, we can limit our analysis to systems with this relative degree.
We intend to make these systems passive using the static controller
u(x) = −r(x) + V (x) · yref = −D−1
(x)
h
r˜(x) − V˜ · yrefi
, (6.85)
D(x) =







Lb1
c1(x) Lb2
c1(x) · · · Lbm c1(x)
Lb1
c2(x) Lb2
c2(x) · · · Lbm c2(x)
.
.
.
.
.
.
.
.
.
.
.
.
Lb1
cm(x) Lb2
cm(x) · · · Lbm cm(x)







,
r˜(x) =







Lac1(x) + a1,0 c1(x)
Lac2(x) + a2,0 c2(x)
.
.
.
Lacm(x) + am,0 cm(x)







, V˜ =







V˜
1 0 · · · 0
0 V˜
2 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · V˜m







,
as previously derived in Section 5.2.11, p. 401 et seq.
In the SISO case, the m × m decoupling matrix D(x) with its elements
dij = Lbj
ci(x)
is identical to Lbc(x), i. e. the denominator of the control law (6.85). Also,
recall
Lbc(x) 6= 0
must always apply to this denominator term. Accordingly,
det(D(x)) 6= 0
must hold in the MIMO case.
We obtain from [65]
Theorem 85 (Passivability of Control-Affine Systems). A non-passive
system
x˙ = a(x) + B(x) · u,
y = c(x)
with a regular decoupling matrix D(x) can be converted into a passive con￾trol loop with a two times continuously differentiable, positive definite storage
function using a static controller6.3. Passivity-Based Control 565
u(x) = −D−1
(x)[r˜(x) − V˜ · yref]
if and only if the system
(1) has a vector relative degree δ = [1 · · · 1] and
(2) the internal dynamics have an equilibrium point which is Lyapunov stable.
Theorems 84 and 85 are generalizations of Theorems 82 and 83 from the
preceding section. Since the systems we have addressed in Theorems 82 and
83 are linear, their Conditions (1) and (2) have global validity. This means
that if they are fulfilled in any subset of IRn
, they are fulfilled in the whole
IRn
. In contrast, it is possible that the Conditions (1) and (2) of Theorems 84
and 85 are only fulfilled in a proper subset of IRn
. Therefore, it may be that
the passivity and passivability of system (6.84) are only local properties.
6.3.5 Passivity-Based Control with IDA
Passivity-based control with an interconnection and damping assignment
(IDA) has the goal of providing a controller u(x) for a possibly, but not
necessarily, passive system
x˙ = a(x) + B(x) · u,
y = c(x)
(6.86)
so that the control loop takes the shape of a PCHD system
x˙ = (J(x) − D(x)) 
∂V (x)
∂x
T
+ B(x) · yref,
yPCHD = B
T
(x)

∂V (x)
∂x
T
(6.87)
with the reference vector yref
[3] [344, 345, 346]. As we know from Section
2.4.10 on p. 179 et seq., a PCHD system is always passive. Furthermore, it is
strictly passive if D(x) is positive definite. Our task is to design a controller
u(x) that will generate a passive control loop (6.87) in interconnection with
the plant (6.86) with good damping and an asymptotically stable equilibrium
point. However, the output variables y and yPCHD of the two systems are
generally no longer identical.
We assume that the equilibrium point which is of interest to us has been
transformed into the origin x = 0 or is already there. Now, we obtain for the
derivative of the positive definite function V (x)
[3] Note that the matrix D(x) in equation (6.87) is the damping matrix of a PCHD
system and not the decoupling matrix D(x). Both matrices are designated D in
the literature. We will retain these designations, as there is no risk of confusion
in the following.566 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
V˙ (x)= ∂V (x)
∂x
(J(x) − D(x))
∂V (x)
∂x
T
= −
∂V (x)
∂x
D(x)

∂V (x)
∂x
T
for yref = 0, where V˙ (x) < 0 holds if D(x) is positive definite. In the latter
case, the function V (x) is a strict Lyapunov function. Thus, the origin is
an asymptotically stable equilibrium point. If D(x) is positive semidefinite,
only V˙ (x) ≤ 0 holds. Then Barbashin and Krasovskii’s theorem on p. 116 or
Theorem 23 on p. 147 can be used to ensure asymptotic stability. Although the
following can be performed for the latter, we restrict ourselves, for simplicity,
to the case of positive definite matrices D(x) since it is the most common.
Now, we will attempt to identify a control law u(x) such that the control
loop with plant (6.86) as a controlled system is in PCHD form (6.87). For this
purpose, as a first step, we split the control variable
u = u1 + u2.
In the second step, we equate equation (6.86) and equation (6.87), obtaining
B(x) · u1 + B(x) · u2 = (J(x) − D(x)) 
∂V
∂x
T
− a(x) + B(x) · yref.
We then compensate for the influence of B(x) · yref by setting
u1 = yref.
This results in the equation
B(x) · u2 = (J(x) − D(x)) 
∂V
∂x
T
− a(x)
| {z }
v
, (6.88)
which we will examine in more detail below.
To this end, let us first identify the input variable vector u2 which satisfies
this equation. Here, we will note that equation (6.88) is usually an overdeter￾mined linear system of equations
B(x) · u2 = v (6.89)
for a constant x with respect to u2. In this system of equations, u2 is an m -
dimensional vector, v is an n - dimensional vector, and B is an n× m matrix.
The system of equations (6.89) can only be solved if the vector v is a linear
combination of the B columns. We can verify whether the latter is fulfilled
or not by constructing n − m linearly independent row vectors b
⊥
i which are
perpendicular to all m columns bj of the matrix B, i. e.
b
⊥
i bj = 0 for all i = 1, . . . , n − m and j = 1, . . . , m6.3. Passivity-Based Control 567
must hold. Together, the vectors (b
⊥
i
)
T
, i = 1, . . . , n − m, and the vectors
bj , j = 1, . . . , m, form a basis of IRn
. In matrix notation, we obtain
B
⊥B = 0,
where
B
⊥ =





b
⊥
1
.
.
.
b
⊥
n−m





.
The (n − m) × n matrix B
⊥ is called the annihilator matrix and its rank is
n − m.
If
B
⊥v = 0
holds, i. e. if v is perpendicular to all vectors b
⊥
i
, then v is a linear combination
of the columns of the matrix B. This means that equation (6.89) is solvable.
The control-loop equation (6.88) can thus be fulfilled if and only if
B
⊥(x)
"
(J(x) − D(x)) 
∂V
∂x
T
− a(x)
#
= 0 (6.90)
holds. In solving the previous equation, we have the choice between two differ￾ent variables and approaches to aim at a control loop with an asymptotically
stable equilibrium point:
(1) We can select a storage function V (x) with its minimum at zero. Then the
matrices J(x) and D(x) are to be determined, with the skew symmetry
of J(x), i. e. J(x) = −J
T
(x), and the positive definiteness of D(x) as
properties to be satisfied. In this case, equation (6.90) is an algebraic
equation with the variables J(x) and D(x).
(2) We can choose a skew-symmetric matrix J(x) and a positive definite
matrix D(x). Then we have to determine the storage function V (x) so
that equation (6.90) is fulfilled. The constraints V (0) = 0 and V (x) > 0
must be complied with. In this case, equation (6.90) is a linear partial
differential equation with respect to V (x).
If we succeed in fulfilling the key equation (6.90), the system of equations
(6.88) has a solution. Now we can determine the m - dimensional vector u2 by
selecting m equations with linearly independent row vectors b
T
i
of the n × m
matrix B(x) from
B(x) · u2 = v (6.91)568 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
in the following manner. We will denote the indices of these linearly indepen￾dent m row vectors b
T
i
as i1, i2, . . . , im, where i1, i2, . . . , im ∈ {1, 2, . . . , n}
applies. To select the corresponding equations
b
T
i1
u2 = vi1
,
.
.
.
b
T
im
u2 = vim
from the system of equations (6.91), we now construct the m × n matrix
K =





e
T
i1
.
.
.
e
T
im





,
where the n - dimensional vector eik
is the unit vector of the ik-th coordinate
direction of IRn
. This vector has zeros everywhere except in its ik-th element,
which is equal to one.
Now we multiply equation (6.91) by the matrix K from the left, selecting
the m linearly independent equations
KB(x) · u2 = Kv.
This yields
u2 = (KB(x))−1Kv (6.92)
and the control law we have been attempting to identify:
u = u2 + u1 = (KB(x))−1K
"
(J(x) − D(x)) 
∂V
∂x
T
− a(x)
#
+ yref.
(6.93)
Alternatively, we can also derive the control law (6.93) from the pseudoin￾verse of the matrix B(x). The pseudoinverse [40]
M+ =

MTM
−1
MT
of an n × m matrix M is used to approximately solve an overdetermined
system of the equations
M z = h, M ∈ IRn×m and z ∈ IRm, h ∈ IRn
. (6.94)
Its approximate solution is given by
zopt = M+h6.3. Passivity-Based Control 569
and yields
M zopt ≈ h.
This approximation attains the smallest value of the square error
|Mz − h|
2 = (Mz − h)
T
(Mz − h). (6.95)
If, however, the condition
M⊥h = 0
is also fulfilled, the vector h is a linear combination of the M columns, and
the system of equations (6.94) can be solved exactly. Hence, the error (6.95)
is identical to zero for z = zopt and the solution is no longer approximate; it
is exact, yielding
M zopt = h with zopt = M+h.
With the approach using the pseudoinverse of B(x), we can calculate
u2 =

B
T
(x)B(x)
−1
B
T
(x)v
as an alternative to equation (6.92). This means we can represent the control
law (6.93) as
u =

B
T
(x)B(x)
−1
B
T
(x)
"
(J(x) − D(x)) 
∂V
∂x
T
− a(x)
#
+ yref
(6.96)
if condition (6.90) is fulfilled.
Now we will address the case in which
y = yPCHD = B
T
(x)

∂V
∂x
T
is required. As mentioned earlier, this identity is generally not achievable. This
is because the control law (6.96) is static in nature, and from Theorem 85 in
the previous section, we know that we can make systems (6.86) passive using
a static control law if and only if the system has the vector relative degree
δ = [1 · · · 1]
and the internal dynamics have a Lyapunov stable equilibrium point. This
will not always be the case.
However, since we are using a state control (6.96) to control the entire state
vector and are mainly interested in the stability of the control loop, we can
also classify the passivity with respect to an arbitrary output variable vector.
Then it is irrelevant whether y is regarded as the output variable vector or
yPCHD. Thus the above methodology provides a useful tool for controlling
both control-affine and linear systems.570 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
6.3.6 Example: Paper Machine
As an example, we will describe a paper machine whose basic structure is
shown in Figure 6.29. From the stock preparation system (not shown here),
a water-pulp suspension containing about 1% pulp is fed into a mixing tank
via a valve which can be used to control the inflow qf
. The water stream qb,
generated during the manufacturing process and still containing pulp particles,
is also fed into this mixing tank via a pump. The suspension mixed in this way
is then fed into the headbox consisting of a long narrow nozzle gap through
which the water-pulp suspension is sprayed onto a rotating mesh wire which
functions as a sieve. This part of the machine is called the forming section. At
this point, the water precipitates due to gravity. If necessary, it can also be
suctioned off using a vacuum suction box. After dewatering, the precipitated
water is collected in a container below the mesh; from there it is pumped
back into the mixing tank. The pulp web formed on the mesh is then fed
into a press section. At this point, the pulp content is approximately 20%.
Next the pulp web is squeezed between two circulating press felts and thus
further dewatered. As no further water can be removed from the pulp web
by additional squeezing at the end of the press section, the pulp web is now
dewatered by drying. For this purpose, the pulp web is guided meanderingly
into the dryer section over steam-heated cylinders. After the dryer section,
the paper web is finished and wound on a reel spool.
In a paper machine, there is a multitude of controllers. In the following,
we will examine both the control of the water level
hh = hh,op + ∆hh
qf
qb
Headbox
Pump
Reel
spool
Dryer section
Press section
Forming section
Filter
hm
hh
Mixing tank
Valve
Fig. 6.29: Paper machine6.3. Passivity-Based Control 571
and the pulp concentration
ch = ch,op + ∆ch
of the suspension in the headbox. Here, ∆hh and ∆ch are the deviations from
the operating point, whose elements are marked by the index op. These two
variables, combined in the output variable vector
y
T = [∆hh ∆ch],
also make up two of the state variables in the vector
x
T = [∆hm ∆hh ∆cm ∆ch],
where
hm = hm,op + ∆hm
is the filling level and
cm = cm,op + ∆cm
is the pulp concentration in the mixing tank. The variables ∆hm and ∆cm are
once again the deviations from the operating point. The deviations ∆qf and
∆qb from the operating points of the two adjustable inflows
qf = qf,op + ∆qf and qb = qb,op + ∆qb
of the stock preparation system and the backflow serve as control variables.
This means that the control variable vector is given by
u
T = [∆qf ∆qb] .
The concentrations of the pulp in the backflow and in the suspension coming
from the stock preparation system, cb + ∆cb and cf + ∆cf
, fluctuate and act
as disturbance variables. Here, the fluctuations are ∆cb and ∆cf
.
The model [484] of a paper machine producing capacitor paper is given by
x˙ =







−1.93 0 0 0
0.394 −0.426 0 0
0 0 −0.63 0
0.82 −0.784 0.413 −0.426







| {z }
A
x
| {z }
a(x)
+







1.274 1.274
0 0
1.34 − 0.327x3 −0.65 − 0.327x3
0 0







| {z }
B(x)
u +







0
0
0.203∆cf + 0.406∆cb
0







| {z }
d(∆cf
, ∆cb)
,572 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
an equation which is bilinear in the states xi and the input variables ui
. The
vector d(∆cf
, ∆cb) represents the disturbances. Our goal will be to design a
passivity-based control with IDA for the aforementioned system to compen￾sate for the disturbances ∆cf and ∆cb, so that the control loop takes the
form
x˙ = (J(x) − D(x)) 
∂V
∂x
T
+ d(∆cf
, ∆cb)
with the reference vector yref = 0. As we know from the previous section, this
is possible if and only if equation (6.90) in the form
B
⊥(x)
"
(J(x) − D(x)) 
∂V
∂x
T
#
= B
⊥(x)a(x) (6.97)
is fulfilled. For the system in question, we will select
V (x) = 1
2
x
T x.
As we can immediately see, the row vectors of the matrix
B
⊥ =


0 1 0 0
0 0 0 1


are perpendicular to all column vectors of the matrix B(x). We use these
results in equation (6.97) and obtain


0 1 0 0
0 0 0 1

(J(x) − D(x)) x =


0.3940 −0.4260 0 0
0.8200 −0.7840 0.4130 −0.4260

x.
(6.98)
The matrix
J(x) − D(x) =







∗ ∗ ∗ ∗
0.3940 −0.4260 0 0
∗ ∗ ∗ ∗
0.8200 −0.7840 0.4130 −0.4260







has a number of arbitrarily selectable entries to fulfill the key equation (6.98),
which are marked here with asterisks. We will now choose
J − D =







−1.8322 0.0633 −1.7247 −0.8113
0.3940 −0.4260 0 0
−0.0039 0.7013 −0.8526 −0.1169
0.8200 −0.7840 0.4130 −0.4260







= M,6.3. Passivity-Based Control 573
and since each matrix M is composed of a skew-symmetric and a symmetric
component according to
M =
1
2
(M − MT
)
| {z }
skew-symmetric
+
1
2
(M + MT
)
| {z }
symmetric
,
we obtain the skew-symmetric matrix
J =
1
2
(M − MT
) =







0 −0.1654 −0.8604 −0.8156
0.1654 0 −0.3507 0.3920
0.8604 0.3507 0 −0.2650
0.8156 −0.3920 0.2650 0







.
The symmetric component D takes the form
D = −
1
2
(M + MT
) =







1.8322 −0.2287 0.8643 −0.0043
−0.2287 0.4260 −0.3507 0.3920
0.8643 −0.3507 0.8526 −0.1480
−0.0043 0.3920 −0.1480 0.4260







,
which is positive definite. We can now use equation (6.96) to define the control
law
u = (B
T
(x)B(x))−1B
T
(x)(J − D − A)x =


u1
u2


with
u1 = 0.0231x1 + 0.3686x2 − 0.5540x3 − 0.2667x4 + 0.0126x1x3
+ 0.0082x2x3 − 0.1046x3x4 − 0.2225x
2
3
,
u2 = 0.0537x1 − 0.3190x2 − 0.7997x3 − 0.3701x4 − 0.0126x1x3
− 0.0082x2x3 + 0.1046x3x4 + 0.2225x
2
3
.
Using this passivity-based control, we have obtained a linear control loop
with the eigenvalues
λ1 = −0.7918, λ2 = −0.8355, λ3/4 = −0.9548 ± j0.1803,
whose settling time is approximately half as long as that of the plant with its
eigenvalues
λ1/2 = −0.426, λ3 = −0.63, λ4 = −1.93.574 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
6.4 Fuzzy Control
6.4.1 Introduction
Simple nonlinear controllers, such as two- and three-position controllers, are
often designed based on human experience. A good example of this is a simple
temperature control as in a heater, as shown in Figure 6.30. Two simple rules
describe the controller behavior of this control loop:
(1) If the control error e is negative, then turn off the heater.
(2) If the control error e is positive, then turn on the heater.
If the control error e is described by a Boolean variable ϑ with
ϑ = 0 for e ≤ 0
and
ϑ = 1 for e > 0
and the status of the heater is described by z = 0 (heater off) and z = 1
(heater on), then the above rules can also be described by the simple Boolean
logic expression
z = ϑ.
The above control has some advantageous features: it is simple and inex￾pensive to implement. No process model is required for its design. The design
is based on rules that are easy to formulate and result from human experience.
The advantages of this kind of design are so great that the question arises
whether we can create a general design methodology for controllers with the
same advantages. A design methodology such as this would obviously be very
useful, because it could be used to design controllers that would be simple
to implement based on human empirical knowledge, i. e. rules which can be
verbally formulated. Ultimately, this means that we can mathematically im￾itate human behavior or causal knowledge using a computer to simulate the
behavior of a process operator or for other purposes.
Fuzzy logic, developed by L. A. Zadeh in 1965 [490] (see also [70, 75, 102,
207, 249, 321, 490]), is a generalization of Boolean logic and makes it possible
to apply the above procedure. The human behavior to be simulated by fuzzy
logic must be describable by "if-then rules", such as the following:
a
s + a
Two -position
controller Heater
Tref e T
Fig. 6.30: Control with a two-position controller6.4. Fuzzy Control 575
If the boiler temperature T is very high or the boiler pressure P is very
high, then set the valve opening V of the gas supply close to zero.
or
If the boiler temperature T is high and the boiler pressure P is low,
then set the valve opening V of the gas supply to approximately half￾open.
This raises the question of how such verbally formulated rules can be math￾ematized using fuzzy logic. We can answer this question by proceeding with
the following five steps:
Step 1: We mathematize terms such as very high or low or close to zero.
Step 2: Logic operations such as AND or OR are defined by mathematical
operators.
Step 3: The "if-then conclusion" is mathematized.
Step 4: The results of all conclusions, meaning of all rules, are combined.
Step 5: The result of the logical operation must be converted into a techni￾cally applicable numerical value.
Step 1 is referred to as fuzzification, Steps 2, 3, and 4 as inference, and Step
5 is defuzzification. They are discussed in detail in the next three sections.
6.4.2 Fuzzification
Let us review the following rule: if the boiler temperature T is high and the
boiler pressure P is low, then set the valve opening of the gas supply to approx￾imately half-open. In a rule of this type, variable quantities such as the boiler
temperature, the boiler pressure, and the valve opening are called linguistic
variables. Terms and expressions such as high, low, and approximately half￾open are referred to as linguistic values. Unlike numerical values, linguistic
values do not describe a single value, but a whole range of values.
In classical logic and classical set theory, a value range of this type can be
stated as
Mhigh = {T ∈ IR | 300 ◦C ≤ T ≤ 500 ◦C},
for example. A temperature value T is either part of this quantity or not.
Figure 6.31 shows the set Mhigh , where a membership function µ is used as a
measure of the set membership. In this example, the membership function µ
takes the form
µ(T ) = (
0 for T 6∈ Mhigh ,
1 for T ∈ Mhigh .
However, the above sharp separation into value ranges to which a temper￾ature either belongs or does not belong has little in common with the human576 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
µ(T )
1
300 500 T in ◦C
Fig. 6.31: Example of a discontinuous
membership function of the set Mhigh
µ(T )
1
300 500 T in ◦C
Fig. 6.32: Example of a continuous
membership function of the set Mhigh
concept of the term high. For example, to a human being it would seem non￾sensical that a temperature T = 299.99 ◦C would be assigned the membership
value µ(299.99 ◦C) = 0, whereas T = 300.01 ◦C is assigned the membership
value µ(300.01 ◦C) = 1.
In many cases, humans see membership in a set as a gradual shift and
not as an abrupt transition. Figure 6.32 shows an example of a continuous
membership function. A continuous membership function µ makes it possible
to have membership values µ(T ) with
0 ≤ µ(T ) ≤ 1,
which correspond to a linguistic value such as high. For example, T = 300 ◦C
is classified as high with a value of only µ(300 ◦C) = 0.45.
The assignment to a set is no longer false or true, meaning µ = 0 or µ = 1,
respectively; rather, the membership function µ has an infinite number of
values between zero and one. An assignment such as this is called fuzzy. In
this case, the term fuzzy set is used, which is defined as follows.
Definition 38 (Fuzzy Set). A set
F = {(x, µ(x)) | x ∈ G and µ(x) ∈ [0, 1]}
of pairs (x, µ(x)) is called a fuzzy set in the universe G. The mapping
µ : G → [0, 1]
is called the membership function of F. It assigns the degree of membership
µ(x) to each element x of the universe G.
Linking a variable x with a degree of membership µ(x) to a linguistic value
is called the fuzzification of x. The universe G is often identical to the set of
real numbers IR.
The range of values for a linguistic variable which are of interest, such as
the temperature T in the example above, is generally divided into several lin￾guistic values, as shown in Table 6.1. A fuzzy set and a membership function6.4. Fuzzy Control 577
Table 6.1: Examples of linguistic values
Linguistic variable Linguistic values
temperature T zero low high very high
µ(T )
1
100 200 300 400 500 600 T in ◦C
µzero µlow µhigh µvery high
Fig. 6.33: Examples of membership functions
belong to each of the individual linguistic values. Figure 6.33 illustrates a pos￾sible selection made by a human and based on his or her intuitive assessment
of the circumstances. Triangular, trapezoidal, and ramp-shaped functions are
often used as membership functions. Furthermore, other functions such as the
Gaussian function and the cosine-square function are also used.
If-then rules are often represented in the standardized form:
If x1 = LV1,j and . . . and xn = LVn,l, then y = LVp.
Here, x1, x2, . . . , xn are the n input variables of the rule, LVi,1, . . . , LVi,q are
the q linguistic values of the input variables xi
, the linguistic variable y is the
output variable of the rule, and LV1, . . . , LVr are the r linguistic values of the
output variable y. The linguistic values LVi,j are numerically defined by the
associated membership functions µi,j . The expression xi = LVi,k is therefore
synonymous with the specification of a membership value of xi to LVi,k, i. e.
µi,k(xi).
6.4.3 Inference
In the next step, the AND or OR operations in the rules are to be implemented
using mathematical operators, i. e. fuzzy operators. First, we will again con￾sider the classical logic approach, which only includes the membership values
zero and one. The AND and OR operations are known from Boolean logic and
are shown in Tables 6.2 and 6.3. Obviously, we can implement the Boolean
AND operation using the minimum operator
µy = min{µa, µb}
and the Boolean OR operation using the maximum operator578 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
Table 6.2: Boolean AND
µa µb µy
0 0 0
0 1 0
1 0 0
1 1 1
µy = µa ∧ µb
Table 6.3: Boolean OR
µa µb µy
0 0 0
0 1 1
1 0 1
1 1 1
µy = µa ∨ µb
µy = max{µa, µb}.
In the fuzzy logic approach, these logical operations are now adopted in
such a manner that all intermediate values are allowed for membership, in
addition to the values 0 and 1. Accordingly, the fuzzy AND and the fuzzy OR
are defined as follows:
Definition 39 (Fuzzy AND and Fuzzy OR). The fuzzy AND operation
of two membership values µa, µb ∈ [0, 1] is defined by
µa∧b = min{µa, µb}
while the fuzzy OR operation is defined by
µa∨b = max{µa, µb}.
With the current results, the "if" part of a rule, called the condition,
antecedent, or the premise, can be converted into a mathematical expression.
For example, consider the "if" part of the rule
If the boiler temperature T is high and the boiler pressure P is low,
then set the valve opening V of the gas supply to approximately half￾open.
From this "if" part, we obtain
µhigh∧low (T, P) = min{µhigh(T ), µlow (P)}.
This is illustrated in the following example. Inserting the temperature
T = 400 ◦C and the pressure P = 1 bar into the rule above, the values
µhigh (T = 400 ◦C) = 1
and
µlow (P = 1 bar) = 0.6
are obtained from the membership functions in Figure 6.34. For this single6.4. Fuzzy Control 579
µ
0.5
µhigh
1
300 400 500 T in ◦C
µ
µlow
1
1 2 3
0.5
P in bar
Fig. 6.34: Membership functions of temperature high and pressure low
point (400 ◦C, 1 bar), we obtain
µhigh∧low (T = 400 ◦C, P = 1 bar) = min{1, 0.6} = 0.6
as the result of the fuzzy AND operation. In the general case, the mathematical
evaluation of the "if" part of a rule, called the antecedent aggregation or the
aggregation for short, is given by
µagg(x1, . . . , xn) = min{µLV 1,k(x1). . . , µLV n,l(xn)}
if it has only fuzzy AND operations.
Note that there are a number of operators, besides the minimum and
the maximum operators, which can be used to implement the fuzzy AND
operation and the fuzzy OR operation, respectively [75, 102, 207]. In industrial
practice, the minimum and maximum operators are most frequently used.
Other common fuzzy operators are the bounded product
max{0, µa(x) + µb(x) − 1}
and the algebraic product
µa(x) · µb(x)
for the fuzzy AND, and the bounded sum
min{1, µa(x) + µb(x)}
and the algebraic sum
µa(x) + µb(x) − µa(x)µb(x)
for the fuzzy OR [304]. In contrast to the minimum and maximum opera￾tors, the algebraic product and the algebraic sum have the advantage of being
differentiable. The differentiability of the operators is important if the pa￾rameters of the membership function are to be optimized using a gradient
method.580 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
The next step is to determine how the "then" part of a rule, which is
also called the conclusion, can be described mathematically. The method of
obtaining the conclusion from the antecedent is called the implication. Here,
the "if" part is linked to the "then" part by a fuzzy operator.
To determine what effect the "if" part should have on the "then" part, it
makes sense to assume that the truth value of the conclusion should not be
greater than the truth value of the antecedent. If this were not the case, false
conclusions could be drawn. We can illustrate this with the following rule: if
the tomato is red, then it is ripe. Let us now assume that the tomato is green,
i. e. the antecedent has the truth value µagg = 0. It would clearly be wrong to
deduce that the tomato is nevertheless ripe, i. e. that the conclusion that the
tomato is ripe has a truth value of µ = 1.
A simple way to implement the above requirement is as follows: we can
limit the "then" part, i. e. its membership function
µLVp
(y),
to the result of the "if" part
µagg(x1, . . . , xn).
This is easily possible using the minimum operator. The fuzzy implication is
thus implemented using the minimum operator. For a rule k, this results in
µk(x1, . . . , xn, y) = min{µagg,k(x1, . . . , xn), µLVp
(y)}.
Note that, instead of the minimum operator, other operators can also be used
for the fuzzy implication. For example, the algebraic product
µagg,k(x1, . . . , xn) · µLVp
(y)
is commonly used.
For an example, let us return to the following rule:
If the boiler temperature T is high and the boiler pressure P is low,
then set the valve opening V of the gas supply to approximately half￾open.
Converted into fuzzy logic using the above results, we obtain
µk(T, P, V ) = min{µagg(T, P), µhalf (V )}
with
µagg(T, P) = min{µhigh (T ), µlow (P)}.
This is shown in graphical form in Figure 6.35.
For all possible value combinations (T, P, V ), there would be a characteris￾tic four-dimensional diagram which cannot be graphically depicted. Assuming
two constant values T, P, e. g. T = 400 ◦C and P = 1 bar, the graph can be6.4. Fuzzy Control 581
µ µ µ
µhigh
µk
µagg
T in P in bar ◦C V in%
µlow
µhalf
1
1 1 1
0.5 0.5 0.5
300 500 2 3 50 100
Fig. 6.35: Evaluation of a rule with the result µk (membership function highlighted
in blue)
reduced to two dimensions, as shown in Figure 6.35. The example illustrates
the limitation of µk to the result of the antecedent evaluation. The result µk
is the membership function µhalf truncated to µagg.
In general, the description of a person’s behavior will contain several rules.
The next step is therefore to determine how the results of several rules are to
be linked. This combination is called accumulation[4]
.
The entirety of all rules, the rule base, can be arranged either in a table or
in a matrix. Table 6.4 and Table 6.5 show examples. The tabular form is par￾ticularly suitable for rule bases with many input variables. The arrangement
as a matrix is similar to the Karnaugh map from Boolean logic. It is easier to
keep track with the matrix than with the tabular form. However, the matrix
form is only suitable for rule bases with few input variables xi
. An advan￾tage of the matrix form is the fact that the rule base is automatically free of
contradictions. This is because each field within the matrix represents exactly
one antecedent. No antecedent can occur several times and therefore, no con￾tradictory rules can occur due to different conclusions. Another advantage is
that if no conclusion exists for an antecedent, this is immediately apparent.
Table 6.4: Rule base in tabular form
Rule 1: If x1 = LV1,i and . . . and xn = LVn,j , then y = LVs.
Rule 2: If x1 = LV1,k and . . . and xn = LVn,l , then y = LVu.
.
.
.
Rule m: If x1 = LV1,q and . . . and xn = LVn,r, then y = LVw.
[4] In the literature, the term rule aggregation or briefly aggregation is used some￾times instead of the term accumulation. Here we adhere to the International
Standard IEC 61131-7, which standardizes the terms of fuzzy systems, and use
the term accumulation. This means there is no confusion between antecedent
aggregation and rule aggregation.582 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
Table 6.5: Rule base in matrix form. No rules exist for empty fields.
x2
LV2,1 LV2,2 LV2,3 LV2,4
LV3,1 LV1 LV3 LV1 LV1
x1 = LV1,1 x3 LV3,2 LV1 LV2
LV3,3 LV2 LV3
x2
LV2,1 LV2,2 LV2,3 LV2,4
LV3,1 LV1 LV3 LV1 LV1
x1 = LV1,2 x3 LV3,2 LV1 LV3 LV1 LV1
LV3,3 LV3 LV1 LV2 LV2
x2
LV2,1 LV2,2 LV2,3 LV2,4
LV3,1 LV1 LV3 LV1 LV3
x1 = LV1,3 x3 LV3,2 LV2 LV1 LV2
LV3,3 LV1 LV3 LV2 LV1
The accumulation can take place in different ways. The most common op￾tion is the union of all rule results. This is synonymous with the OR operation,
i. e. all rules from the rule base are combined using the fuzzy OR operation.
As explained above, the fuzzy OR operation can be realized using the
maximum operator. The accumulation result is then
µres(x1, . . . , xn, y) = max{µ1(x1, . . . , xn, y), . . . . . . , µm(x1, . . . , xn, y)}.
(6.99)
Again, this result can be illustrated graphically. For this purpose, consider the
following two rules:
If the boiler temperature T is high and the boiler pressure P is low,
then set the valve opening V of the gas supply to approximately half￾open,
and6.4. Fuzzy Control 583
Rule 1
Rule 2
Aggregation Accumulation
µ µ µ
µ µ µ
µres
µhigh
µlow
µlow µhalf
µvery high µclosed
µres
1
1
300
300
500
500
50
50
50
100
100
100
2
2
3
3
1
1
1
1 1
1 1
0.5
0.5
0.5
0.5
0.5
0.5
0.5
P in bar
P in bar
V in %
V in %
V in %
T in ◦C
T in ◦C
Fig. 6.36: Evaluation of a rule base using aggregation and accumulation
If the boiler temperature T is very high and the boiler pressure P is
low, then set the valve opening V of the gas supply to almost closed.
These rules can then be evaluated for a situation (T, P) with the cor￾responding membership functions. Here, (T, P) = (470 ◦C, 3 bar) is selected.
Figure 6.36 shows the resulting evaluation. It illustrates how the union µres
of the accumulation is performed using the OR operator from the fuzzy sets
or the membership values of the two rule outcomes, which are shown as blue
areas.
The result of the accumulation is a superimposition of the results obtained
from the individual rules. For a given situation (x1, . . . , xn), the accumulation
result (6.99) is a function of y. The evaluation of all rules by aggregation,
implication, and accumulation is called inference.584 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
µres
1
0.5
y
COA
yres
Fig. 6.37: Accumulation of all rule results and center of area (COA) of the resulting
fuzzy set. The value yres indicates the y coordinate of the COA.
Due to the superimposition of many rule results, the fuzzy set µres takes
the shape of a mountain range outlined by a polygonal line, as shown in
Figure 6.37. This fuzzy set is generally not usable as the final result of the
overall evaluation of the rules. Imagine, for example, that the valve opening is
described by the fuzzy set µres. However, we need a numerical value instead of
this fuzzy set for the valve opening in practical applications. We will calculate
this numerical value in the next section.
6.4.4 Defuzzification
As stated above, the result µres of the inference evaluation cannot be used
directly. In practice, we generally need a single numerical value, so we have
to determine a numerical value from the fuzzy set µres which is representative
of the result of the evaluation. This numerical value is commonly intended
to represent a compromise or an average of all rules. Its calculation is called
defuzzification.
The center of area, or more precisely its y coordinate
yres =
Z∞
−∞
y · µres(x1, . . . , xn, y) dy
Z∞
−∞
µres(x1, . . . , xn, y) dy
is such a value. It is the final result of a fuzzy logic, i. e. the output value of
the overall rule base evaluation. Figure 6.37 shows an example. The described
method of defuzzification is the best-known and is known as the center-of-area
method or COA method.
However, determining the center of area is complicated, since the mem￾bership function µres takes the shape of a polygonal line. The calculation of
the center of area yres can be simplified considerably by choosing singletons
instead of trapezoids or triangles for the membership functions of the linguis￾tic values LVi of y. Singletons µLVi
(y) only take the function value one at one
point ys; everywhere else they take the function value zero, i. e.6.4. Fuzzy Control 585
µres
ys
1
y
Fig. 6.38: Singleton
µres
V
µclosed µhalf µopen
µres,1
µres,2
µres,3
1
50% 62.5% 100%
Fig. 6.39: Defuzzification with singletons
µLVi
(y) = (
1 for y = ys,
0 otherwise.
Figure 6.38 shows an example of a singleton.
The result of the accumulation then takes on the form illustrated in Fig￾ure 6.39. In this case, the individual results of the accumulation no longer
overlap to form a polygonal line. Now the membership function µres can be
decomposed into
µres = µres,1 + µres,2 + µres,3 + . . .
The membership functions µres,i are the accumulated results of all rules that
affect the singleton i.
If all m output membership functions are singletons, we can replace the
formula for the center of area, which is now no longer applicable, with a
weighted average. This yields
yres =
Xm
i=1
ys,i · µres,i(x1, . . . , xn)
Xm
i=1
µres,i(x1, . . . , xn)
,
which is much easier to calculate than the center of area. This variant of
the COA method is called the center-of-singletons method, abbreviated as the
COS method. The example of a valve position shown in Figure 6.39 with the
three membership functions closed, half, and open illustrates this defuzzifica￾tion method. The result of the defuzzification is
yres =
50 · 0.75 + 100 · 0.25
0.75 + 0.25
% = 62.5%
with µres,1 = 0, µres,2 = 0.75, and µres,3 = 0.25.
6.4.5 Fuzzy Systems and Fuzzy Controllers
By means of fuzzification, aggregation, implication, accumulation, and de￾fuzzification, we can transform linguistically formulated rules into a mathe￾matical function when using the results of the previous sections. Figure 6.40
illustrates this.586 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
If ..., then ...
.
.
.
If ..., then ...
.
.
.
If ..., then ...
Fuzzy logic
µ1(x1, . . . , xn) = min{. . .}
.
.
.
µres(x1, . . . , xn, y) = max{. . .}
yres =
R
y · µres(y)dy
R
µres(y)dy
Fig. 6.40: Conversion of rules into equations using fuzzy logic
The function
yres = f(x1, . . . , xn)
is composed several times, with the composition resulting from the following
subfunctions and their associated calculation steps:
Fuzzification: determination of all membership values µLVi,j (xi),
Aggregation: µagg,k(x1, . . . , xn) = min{µLV1,i (x1), . . . , µLVn,l (xn)},
k = 1, . . . , m,
Implication: µk(x1, . . . , xn, y) = min{µagg,k(x1, . . . , xn), µLVp
(y)},
k = 1, . . . , m,
Accumulation: µres(x1, . . . , xn, y) = max{µ1(x1, . . . , xn, y), . . . ,
µm(x1, . . . , xn, y)},
Defuzzification: yres =
Z∞
−∞
y · µres(x1, . . . , xn, y) dy
Z∞
−∞
µres(x1, . . . , xn, y) dy
.
When calculating yres for a situation (x1, . . . , xn), we go through the steps
above. The variables x1, . . . , xn can be interpreted as input variables and y as
the output variable of a system, as shown in Figure 6.41.
Fuzzy system
y = f(x1, . . . , xn)
x1
xn
y
Fig. 6.41: Fuzzy system6.4. Fuzzy Control 587
Fuzzy controller Process
Fig. 6.42: Control loop with fuzzy controller
The possible applications of fuzzy systems are numerous. This is due to
the fact that many human behaviors can be described by a mathematical
function using fuzzy logic. Important areas of application are open- and closed￾loop controls, as mentioned above. This branch of fuzzy logic is called fuzzy
control [110, 177, 358, 497]. Figure 6.42 shows a control loop equipped with
a fuzzy controller.
In many applications, human behavior can be reproduced in this way.
Examples are the control of sewage plants [90, 326], ABS braking systems
[117, 313, 314], the control of railway and subway trains [212, 447, 469], the
control of paper machines [4, 5, 413], alarm systems for anesthesia [463], the
autofocus control for cameras [95, 188, 262], washing machine controllers [7,
27, 379, 440], and others [70, 215, 226, 447, 463].
When designing fuzzy controllers, we proceed as follows:
Step 1: Determine the linguistic variables xi
, yi
, which make up the input
and output variables of the fuzzy controller.
Step 2: Assign linguistic values LVi,j to each linguistic variable xi
, yi
.
Step 3: Select the membership functions µi,j for LVi,j .
Step 4: Determine the rules.
In most cases, the fuzzy controller is used to imitate a person who performs
or could perform the control. This could be the process operator of a chemical
or biotechnological process, or the driver of a subway train, for example.
It is rare for us to be able to determine a feedforward or feedback controller
with the desired behavior in a first design using the method above. This is
because the first design will not be able to reproduce human behavior accu￾rately enough. Consequently, the design is repeated with modified parameters
and rules until satisfactory behavior is achieved. This procedure is shown in
Figure 6.43. The design is based on trial and error and will require multiple
optimizations.
Note that the fuzzy controller is a static controller. It is nonlinear, but
without dynamics. Its nonlinearity entails the problem that the stability of
the control loop is either not verifiable at all or only with great effort. This
is one of the reasons why fuzzy controllers are mainly used in cases where588 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
Design of the
fuzzy controller
Testing in
simulation or practice
Improvement of
rules, membership
functions, etc.
Desired
behavior achieved?
End
No
Yes
Fig. 6.43: Procedure for designing a fuzzy controller
classical controllers and the associated stability assurance of the control loop
do not achieve satisfactory results.
6.4.6 Example: Distance Control for Automobiles
As an example application of a fuzzy controller, we will look at a distance
control for automobiles, which is described in similar form in [149, 187]. The
aim is to determine the speed v or acceleration a of an automobile so that the
distance xrel to a vehicle in front of the car takes a safe value, e. g. xref = 0.5v
with xref in meters and v in kilometers per hour. Figure 6.44 illustrates the
situation.
The following variables are relevant for distance control:
vlead : velocity of the vehicle in front,
v : speed of the vehicle being controlled,
vrel = vlead − v : relative velocity,
xrel : distance,
∆xrel = xrel − xref : distance deviation,
aref : desired acceleration of the vehicle being controlled,
a : acceleration of the vehicle being controlled.
Besides the speed v, the variables vrel and xrel can also be measured and thus
used as control inputs. The latter are measured by radar or laser. The car’s
movement can be described using a double integrator with the acceleration6.4. Fuzzy Control 589
xrel
v vlead
Fig. 6.44: Distance control
as the input variable and the distance as the output variable. In addition,
there is a subordinate acceleration control loop, which can be represented by
two first-order transfer elements. Therefore, the transfer function of the open
control loop is
G(s) = 1
s
2(1 + T1s)(1 + T2s)
with T1 = 0.074 s and T2 = 0.14 s. This results in the control loop shown in
Figure 6.45.
For the distance controller, it makes sense to imitate human behavior
using a fuzzy controller. Humans also use ∆xrel, vrel, and v as indicators
that they need to reduce or increase the acceleration aref of their vehicle.
In principle, a distance control of this kind could also be performed using a
linear controller. However, a linear controller does not guarantee sufficiently
safe driving behavior if the vehicles in question change lanes or encounter
slow vehicles after rounding a bend. In contrast, the fuzzy distance controller
shown in Figure 6.45 achieves this.
In the first design step, linguistic values and membership functions of
∆xrel, v, vrel, and aref are defined. The membership functions of Figure 6.46
Vehicle being controlled
Acceleration
control loop
Fuzzy
distance
control
xref
∆xrel aref a v
vlead
1
vrel xrel
(1+T1s)(1+T2s)
1
s
1
s
Fig. 6.45: Distance-control loop590 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
Difference ∆xrel of distance and nominal distance in m
Actual velocity v in m s
−1
Relative velocity vrel in m s
−1
Membership Membership Membership
ts sts ok stl tl
slow fast
s ss ef sf f
1
1
1
0.5
0.5
0.5
0
0
0
-25 -20 -15
-10
-10
-5
-5
0
0
0
5
5
5
10
10
10
15
15
20
20
25
25
30 35 40 45 50
Fig. 6.46: Membership functions of the input variables
1.5
1
0.5
0
-7 -6 -5 -4 -3 -2 -1 0 1 2 3
dg d ds z as ag
Target acceleration aref in m s
−2
Membership
Fig. 6.47: Membership functions of the output variable
are used for the input variables, whereas for the output variable the member￾ship functions are provided in Figure 6.47.
In the next step, the rules are defined using the matrix form in Table 6.6.
Here, the linguistic values are appropriately abbreviated. The matrix entries
indicate linguistic values of the acceleration aref.
The minimum operator is used for both aggregation and implication, while
the maximum operator is used for accumulation. For defuzzification, we will
apply the COS method. The above specifications have the advantage that they6.4. Fuzzy Control 591
Table 6.6: Fuzzy rules for distance control with the input variables v, ∆xrel, and
vrel, and the output variable aref
∆xrel
v =slow
ts sts ok stl tl
s dg d d d d
ss d d d ds as
vrel ef ds ds z as ag
sf ds ds as ag ag
f z z as ag ag
∆xrel
v =fast
ts sts ok stl tl
s d d d d z
ss d d d ds as
vrel ef ds ds z as ag
sf ds ds as ag ag
f z z as ag ag
Abbr. Meaning
s slower
ss slightly slower
ef equally fast
sf slightly faster
f faster
ts too small
sts slightly too small
ok ok
stl slightly too large
tl too large
dg decelerate greatly
d decelerate
ds decelerate slightly
z zero
as accelerate slightly
ag accelerate greatly
are cost-effective and very simple to implement using a fixed-point processor,
for example. Particularly the use of membership functions other than single￾tons or a defuzzification method other than the COS method would make the
evaluation much more complex and costly. In the present case, there would
also be no advantages, since the control behavior is very similar for both
variants.
In total, 50 rules are obtained. The fuzzy controller is a static controller
whose family of characteristics is four-dimensional. For the vehicle speeds
v = 10 m s−1
, i. e. for slow, and v = 30 m s−1
, i. e. for fast, the corresponding
family of characteristics are shown graphically in Figure 6.48. Five essential
regions can be identified in this nonlinear controller. In region 1, the controller
behavior is approximately the same as that of a linear controller. This region
addresses the stationary state with vrel ≈ 0 m s−1
and the correct distance
xrel ≈ xref, i. e. ∆xrel ≈ 0. In region 2, the vehicle can catch up with the
vehicle in front by accelerating at the maximum rate, as the distance is large
and the vehicle in front is moving away. In region 3, maximum deceleration
is required because the distance is much too small. In region 4, the approach592 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
∆xrel in m ∆xrel in m vrel in m s
−1
vrel in m s
−1
aref in m s
−2
aref in m s
−2
v = 10 m s−1
v = 30 m s−1
2 2
5 5
4 4
3
3
2 2
1 1
0 0
-2 -2
-4 -4
-6 -6
20 20
0
0
0
0
-20 -20
-10 -10
10 10
Fig. 6.48: Family of characteristics of the fuzzy distance controller at the velocities
v= 10 m s−1
and v= 30 m s−1
of the vehicle being controlled
is at a negative relative speed vrel from a large distance. The velocity is not
changed in this case. Thus, aref = 0 m s−2
applies until the vehicle has caught
up with the vehicle in front. In region 5, the vehicle in front is driving away at
a distance which is too small. Therefore, it is not accelerated or decelerated
until the correct distance is reached.
From the family of characteristics, it is also evident that it is only in the
relatively small regions 4 and 5 that neither acceleration nor deceleration takes
place. As a consequence, a distance control system is constantly accelerating
or braking in dense traffic. In practice, a distance control is combined with
cruise control, so that at low traffic density the speed control is performed
by the cruise control. The deceleration and acceleration processes are then
omitted.
We use for an example the same traffic situation as in [148]. Figure 6.49
shows the acceleration aref, the velocities v and vlead, the distance xrel, and
the target distance xref between two vehicles during a drive on the highway
of approximately 8 min in very dense traffic. The effective control behavior is
clearly visible in all ranges. The vehicle at the back which is being controlled
almost exactly maintains the distance prescribed by law. This also applies to
the incident at t = 200 s, when a vehicle with a distance of about 18.5 m and
a speed of 38 m s−1 ≈ 137 km h−1 ≈ 85 mph enters the lane in front of the
vehicle being controlled. The control restores the correct distance within 10 s.
Also noteworthy is the period between seconds 320 and 350, during which
the traffic comes to a complete halt. Here, too, the control system succeeds in
maintaining a safe distance which amounts to only centimeters when the vehi￾cles are stationary. In practice, the nominal distance is limited to a minimum
value of xmin = 2 m, for example. The equation xref = 0.5v is then replaced
by xref = 0.5v + xmin.6.5. Exercises 593
Time t in s
Acceleration in m s
−2 Velocity in m s
−1 Distance in m
Target distance
Actual distance
Front vehicle
Controlled vehicle
Stop and go Flowing traffic Stop and go
80
60
40
40
20
20
0
0
0
30
2
-2
-4
0
0
0
50
50
50
100
100
100
150
150
150
200
200
200
250
250
250
300
300
300
350
350
350
400
400
400
450
450
450
500
500
500
10
Fig. 6.49: Measurements during a simulated highway drive
6.5 Exercises
Exercise 6.1 Our aim is to design a model predictive control (MPC1) for the
discrete-time system
x(k + 1) = x(k) + u(k),
y(k) = x(k).
(6.100)
(a) Calculate the matrices F, G, and H for the case in which the prediction
and control horizons np = nc = 1 are equally long.
(b) Formulate the quadratic performance index J with the values Q = 1 and
R = 1 from equation (6.11) on p. 512 in dependence on the control variable
∆u(k) and the control error e(k).
(c) Calculate the control law u(k) dependent solely on the previous output
variables y(k − i) and the reference variable yref(k + 1).594 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
(d) Calculate the difference equation of the closed control loop with yref as
the input variable and y as the output variable.
(e) Calculate the eigenvalues of the control loop.
(f) Calculate the output variable sequence y(k), k = 0, . . . , 18, for the step
in the reference value from yref(0)= 0 to yref(k)= 1, k ≥ 1, and the initial
values y(0) = 0 and y(−1) = 0.
Exercise 6.2 Design a model predictive control (MPC2) for the plant (6.100)
with the predictive and reference horizon np = nc = 2.
(a) First, solve Exercise 6.1, parts (a) through (f), but now for Q = I, R = I,
and np = nc = 2.
(b) Determine the time step k3% at which the model predictive control
(MPC2) deviates by 3% or less from the reference value yref(k) = 1.
(c) Compare the time step k3% of this control with that of the control from
Exercise 6.1 (MPC1). How much faster is the control compared to that in
Exercise 6.1?
Exercise 6.3 Let us consider the system
x˙ =


0 1
0 0

 x +


0
1

u, |u| ≤ umax = 1.
(a) Design a sliding mode controller u(x) which has the switching line
s(x) = rx1 + x2 = 0.
Calculate the switching line s(x) such that it approximates the switching
curve S(x2) = x1 of the time-optimal controller, with the switching line
s(x) = 0 intersecting the switching curve S(x2) = x1 at x1 = 2. State
u(x) and s(x).
(b) Draw the switching line and the trajectories of the control loop.
(c) On the switching line, where does a sliding mode occur and where does it
not occur?
(d) Demonstrate that the sliding mode can be achieved in finite time.
(e) What are the dynamics of the control loop in the sliding mode?
(f) Identify and sketch the progression of x1(t) and x2(t) for the initial vector
x
T
(0) = h
−5 5i
. (6.101)
(g) For the initial vector (6.101), calculate the switching times of the time￾optimal control and draw the time-optimal progressions of x1(t) and x2(t).
(h) Given the initial vector
x
T
(0) = h
x10 x20 =−x10i
,
what is the maximum number k of switching intervals required to attain
the sliding mode?6.5. Exercises 595
Exercise 6.4 For a plant
x˙ = a(x) + b(x)u
calculate a sliding mode controller u(x) using Gao and Hung’s method.
Exercise 6.5 A motor vehicle’s brakes have an anti-lock braking system which
prevents the wheels from locking as a result of overly abrupt braking, thus
also preventing the driver from losing control over the vehicle. The measure
for locking is the slip
λ =
v − rω
v
of the wheel in question. It is the difference, in reference to the velocity v of
the vehicle, between v and the circumferential velocity rω of the wheel. Here,
r is the radius of the wheel and ω is its angular velocity. The wheel has the
moment of inertia J in relation to its rotational axis. The friction Ff between
the wheel and the road is obtained from the weight mg (where m amounts
to a quarter of the vehicle’s mass and g is the gravitational acceleration) and
the friction coefficient µ(λ), yielding
Ff = mgµ(λ).
The wheel is braked by the braking torque Mb. Figure 6.50 shows the process.
The friction coefficient can vary depending on the slip λ between µ = 0 for
Ff
mg
Mb
v
ω
r
Fig. 6.50: Wheel and the forces acting on it596 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
µ
1
Adhesion Sliding
0.2 0.4 0.6 0.8 1 λ
Dry road surface
Wet road surface
Fig. 6.51: Curve of the friction coefficient µ(λ) in varying road conditions
λ = 0 and µ ≈ 1 for λ > 0 [202]. Figure 6.51 shows a typical curve. If the road
surface is dry and we have normal driving conditions, the friction coefficient
is µ ≈ 0.015.
(a) Formulate the wheel’s equations of motion and, in doing so, choose the
state variables
x1 = v and x2 = λ,
and
u = Mb
as the control variable.
(b) Using Gao and Hung’s method and the switching line
s(x) = x2 − x2,ref = 0,
calculate a sliding mode controller. Here, x2,ref is the reference slip value.
(c) Determine x1(t) and x2(t) in the sliding mode and the time teq at which
x1(t) reaches zero.
(d) Assume that the friction coefficient µ(x2) is dependent on the material
used to pave the road, i.e. it is unknown. Which values does the braking
distance depend on?
(e) Select the most effective possible value x2,ref.
Exercise 6.6 Let us examine a three-zone oven in which the material being
heated passes through the three zones, which are separately operated. These
zones may be operated at different temperatures. Figure 6.52 shows the basic
design. The model of the oven describes the changes in temperature ∆Ti
,
i = 1, 2, 3, surrounding the operating point Ti
, i = 1, 2, 3. Here, the changes
in temperature ∆Ti are the output variables yi
. They depend on the changes
ui
in the heating levels at the operating point via the model in the Laplace
domain6.5. Exercises 597
u1 u2
u3
T1
T2
T3
Fig. 6.52: Three-zone oven
Y (s) = G(s)U(s)
with the transfer matrix
G(s) = c










1
1 + 5s
0.5
1 + 6s
0.2
1 + 6s
0.5
1 + 6s
1
1 + 5s
0.5
1 + 6s
0.2
1 + 6s
0.5
1 + 6s
1
1 + 5s










.
The parameter c depends on the mass and its specific thermal capacity.
(a) Show that the model of the three-zone oven is strictly positive real.
(b) For each of the three zones, a P controller is used which does not take
into account the coupling of the zones, i. e.
u =




P1 0 0
0 P2 0
0 0 P3




e, e = −y.
In addition, the control variables ui
, i = 1, 2, 3, are each subject to an
actuator saturation
ui =



umax, ui > umax,
u˜i
, umin < ui < umax,
umin, ui < umin,598 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
where umax > 0 and umin < 0. Is the point at which ∆Ti = 0, i = 1, 2, 3,
applies, an asymptotically stable equilibrium point of the control loop?
Exercise 6.7 We will examine the so-called Netushil backlash model [334]
u˙ = D(e −
u
m
), (6.102)
where D(x) is the dead zone nonlinearity
D(x) =



µ(x + a), x ≤ −a,
0, |x| < a,
µ(x − a), x ≥ a.
Here µ ≫ 1 is a very large value which tends to infinity in the ideal but
unrealistic case.
(a) Demonstrate that the backlash model (2.6) on p. 75 has the same dynamic
behavior as the Netushil backlash model (6.102).
(b) Show that the backlash nonlinearity is passive.
Exercise 6.8 Let us consider the nonlinear controller
u = h(x, e),
x˙ = e
(6.103)
with the input variable e and the output variable u where the nonlinear func￾tion h has the properties
h(0, 0) = 0
and
0 ≤ x[h(x, e) − h(0, e)] for all x, e ∈ IR,
0 ≤ e[h(x, e) − h(x, 0)] for all x, e ∈ IR.
(a) Which geometric properties does the characteristic diagram u = h(x, e)
have? For this purpose, examine the function h(x, e), or more precisely its
signs, in the four quadrants of the xe-plane.
(b) Show that the two properties
S(0) = 0 and S(x) ≥ 0 for x 6= 0
are fulfilled for the storage function
S(x) = Zx
0
h(˜x, 0)dx. ˜
(c) Prove that the characteristic diagram controller (6.103) is passive.6.5. Exercises 599
Exercise 6.9 Given the system
x˙ 1 = x
2
1 + u,
x˙ 2 = −x1 + x2,
y = x1,
why can’t the system be converted into a passive control loop by means of a
static controller u(x)?
Exercise 6.10 Use Theorem 80 on p. 549 to prove that the building with
viscous fluid dampers from Section 6.3.2, p. 550 et seq., has a globally asymp￾totically stable equilibrium point.
Exercise 6.11 We would like to design a control with IDA for the hydropower
plant with grid feed-in from Section 5.5.5, p. 472 et seq.
(a) State the model
x˙ =


x2
a1 − a2x2 − a3(1 + u) sin(x1 + δeq)


from equation (5.192) in both the nonlinear controller canonical form and
in the PCHD form. To do the latter, use the storage function
V (x) = 1
2
x
2
2 − a1x1 + a3(cos(δeq) − cos(x1 + δeq)).
(b) Now, using an IDA passivity-based control (IDA PBC), we will attempt
to design the control loop such that it has the dynamics
x˙ = (J CL − DCL)
∂VCL(x)
∂x
with
VCL(x) = V (x) + 1
2
kx2
1
.
What form must the matrix J CL − DCL take for a controller design of
this kind to be possible?
(c) Calculate the IDA control law based on the matrix
J CL − DCL =


0 1
−1 −a2−d

 , d > 0.
Exercise 6.12 Let us examine Chen’s chaotic system [76]
x˙ 1 = −35x1 + 35x2,
x˙ 2 = −7x1 + 28x2 − x1x3 + u,
x˙ 3 = x1x2 − 3x3,
y = x2.600 Chapter 6. Nonlinear Control of Linear and Nonlinear Systems
(a) What are the equilibrium points of the free system?
(b) State the system in PCHD form with the storage function
V1(x) = 1
10
x
2
1 +
1
2
x
2
2 +
1
2
x
2
3
.
(c) Is the equilibrium point xeq = 0 stable? Give reasons for your answer.
(d) Convert the system into the Byrnes-Isidori canonical form and state the
diffeomorphism z = t(x) which is required for this transformation.
(e) Can we use a static controller u(z) to design a passive control loop for the
system in question, and if so, why?
(f) State the control law u(z) such that the control loop in PCHD form with
V2(z) = 1
2
(z
2
1 + z
2
2 + z
2
3
)
is strictly passive.
(g) State the equations of the control loop in the original coordinates x.
(h) Is the equilibrium point xeq = 0 of the control loop globally asymptoti￾cally stable and if so, why?
Exercise 6.13 Show that De Morgan’s laws
µa ∧ µb = ¯µa ∨ µ¯b
µa ∨ µb = ¯µa ∧ µ¯b
are valid even in fuzzy logic for the fuzzy operators min and max. In doing
this, use the generally applicable equations
max{x1, x2} =
x1 + x2 + |x1 − x2|
2
,
min{x1, x2} =
x1 + x2 − |x1 − x2|
2
.
Recall that the negation of a fuzzy set is defined by µ¯ = 1 − µ.
Exercise 6.14 In Boolean logic, a ∨ a¯ = 1 applies. This is the law of the
excluded middle, also known as tertium non datur . Determine whether this
rule applies in fuzzy logic as well if we use
(a) the maximum operator,
(b) the algebraic sum, or
(c) the bounded sum
as fuzzy OR operators.
Exercise 6.15 Let us examine the distance control for automobiles from
Section 6.4.6 on p. 588 et seq. We will assume that the dynamics of the
acceleration control loop are so fast in comparison to the vehicle dynamics
that they can be neglected. In this exercise, we would like to examine the
distance control only for velocities of v ≥ 8.¯3 ms−1 = 10 kmh−1 ≈ 6.2 mph.
In this case, it is sufficient to use only the laws for v = fast in Table 6.6 on
p. 591 for the fuzzy distance controller.6.5. Exercises 601
(a) Based on the statements made in Exercise 6.8, determine if the subsidiary
system
x˙ rel = vrel,
aref = f(xrel, vrel),
which includes the fuzzy distance controller, is passive.
(b) Neglecting the acceleration control loop, i. e. a = aref, show that the dis￾tance control loop described in this problem has an asymptotically stable
equilibrium point at [xrel vrel] = 0
T
.
Exercise 6.16 To mimic human behavior in selecting the gear of a motor
vehicle, a control system for an automatic transmission is to be developed
using fuzzy logic [71, 417]. The output variable of the fuzzy logic is the up￾shifting rotational speed nup, which can take values between 1500 rpm and
5500 rpm. The downshifting rotational speed is given by ndown = 0.4nup. The
gear selection is dependent on the gas-pedal position P, the torque caused by
the driving resistance M, and the driver’s driving style F. The gas-pedal po￾sition P varies between 0% and 100% and can be assigned the three linguistic
values small, large, or kickdown. The vehicle’s driving resistance torque M
is the difference between the motor torque Mmot and the torque Ma which
actually accelerates the vehicle. The latter depends on the slope of the road,
the air resistance, etc. For us to compare it to the motor torque Mmot, it is
calculated back to the gearbox’s input. This yields
M = Mmot − Ma = Mmot −
v˙
R
rm
with the vehicle’s acceleration v˙, the wheel radius r, the vehicle mass m, and
the transmission ratio R of the gear. The driving resistance torque M can
vary between −200 Nm and 200 Nm and will be assigned the linguistic values
negative, small, or positive.
The designation of the driver’s driving style F as either defensive or ag￾gressive can be calculated over the time interval [t, t + T ] using the number
and degree of acceleration and braking processes by means of [306]
F =
1
T
t
Z
+T
t
|v˙|
4
R
dt.
Here, F varies between F = 0 for a very defensive driving style and F = 10 for
very aggressive drivers. The driving style F is allocated one of the linguistic
values, defensive or aggressive.
Design a fuzzy control with the upshifting rotational speed nup as the out￾put variable and the gas-pedal position P, the vehicle’s driving resistance
torque M, and the driving style F as the input variables. For the input
variables, use triangles and ramps as membership functions. For the output
variable, which is the upshifting rotational speed nup, choose nine singletons
between the values 1500 rpm and 5500 rpm.7
Observers for Nonlinear Systems
7.1 Observability of Nonlinear Systems
7.1.1 Definition of Observability
The following question arises in the case of nonlinear control loops with state
controllers: how can we determine the plant’s state variables xi
if it is either
technically impossible to measure them or not desired due to cost or other
reasons? This situation is comparable to the one we know from the case of
linear systems, in which the state variables xi are often not measurable or the
measurement would be too expensive.
In the linear case, observers are used to solve this problem by estimating
the state variables xi
. Figure 7.1 shows a state control loop of this kind with
an observer. In particular, the separation principle applies in this case, i. e.
the dynamics of the observer and those of the control loop can be specified
independently of each other via the observer matrix L and the controller
matrix K, respectively.
Observers can also be used for nonlinear systems. However, this is not as
straightforward as in the linear case. This is because the design of nonlinear
observers can be complicated and the stability of nonlinear control systems
with observers is often impossible or very difficult to verify. One reason for this
is that the separation principle does not normally apply to nonlinear control
loops with observers. However, the structure of a nonlinear control loop with
an observer, as shown in Figure 7.2, equals that of the linear one.
The type of nonlinear observer to be selected depends on the characteris￾tics of the plant. The following sections describe some of the common types.
Supplementary literature can be found in [42, 47, 107, 180, 229]. However,
before an observer can be designed, it must be ensured that the nonlinear sys￾tem is observable [47, 146, 180]. In practice, this step is sometimes neglected,
since it is often laborious to examine a nonlinear system’s observability. Fur￾thermore, most of the real-world systems, particularly technical systems, are
observable.
© Springer-Verlag GmbH Germany, part of Springer Nature 2024
J. Adamy, Nonlinear Systems and Controls,
https://doi.org/10.1007/978-3-662-68690-4_7
603604 Chapter 7. Observers for Nonlinear Systems
v
y − y˜
x(0)
x˜(0)
yref
Plant
u
x˜
y
u=yref−K x˜
x˙ = A x + B u
y = C x
x˜˙ = Ax˜ + Bu + v
Controller
Observer
y˜
L
Fig. 7.1: Linear state control loop with observer
yref u
x(0)
x˜(0)
x˜
y
u=K(yref, x˜)
x˙ = f(x, u)
y = g(x, u)
Observer
Fig. 7.2: Nonlinear state control loop with observer
We want to be thorough, so we will first define the concept of observability.
In this context, we will make a distinction between observability and weak
observability[1]
.
Definition 40 (Observability). Let a system
x˙ = f(x,u) with x(t0) = x0,
y = g(x,u)
[1] There are different concepts of observability. In addition, identical concepts are
sometimes referred to by different names [47, 182]. Here we will only describe the
two concepts which are particularly meaningful from a practical point of view in
control engineering.7.1. Observability of Nonlinear Systems 605
be defined for x ∈ Dx,def ⊆ IRn
and u ∈ Du,def ⊆ IRm, and let y ∈ IRr
. If all
initial vectors x0 ∈ Dx ⊆ Dx,def are uniquely determinable from the knowledge
of u(t) and y(t) in a finite time interval [t0, t1] for all u ∈ Du ⊆ Du,def, then
the system is called observable.
Similarly, although with fewer system requirements, we can define the
concept of weak observability[2]
.
Definition 41 (Weak Observability). Let a system
x˙ = f(x,u) with x(t0) = x0,
y = g(x,u)
be defined for x ∈ Dx,def ⊆ IRn
and u ∈ Du,def ⊆ IRm, and let y ∈ IRr
. If all
initial vectors x0 ∈ Dx ⊆ Dx,def within a neighborhood
U = {x0 ∈ IRn
| |x0 − xp| < ρ}
of a point xp ∈ Dx known to us are uniquely determinable from the knowledge
of u(t) and y(t) in a finite time interval [t0, t1] for all u ∈ Du ⊆ Du,def, and
if this is possible for all xp ∈ Dx, then the system is called weakly observable.
As with the system properties stability and controllability, a distinction has
to be made between global and local observability. In this context, globally
observable means that the system property applies to the system’s domain of
definition Dx,def × Du,def; locally observable[3] means that it only applies to a
proper subset Dx × Du of the domain. Since the observability of a nonlinear
system may depend on both the state x and the input variable vector u, we
will highlight this dependence by the notion observable on Dx for Du.
In many cases, weakly observable systems are also observable. This is al￾ways the case for linear systems. A weakly observable but not observable
example is the nonlinear autonomous system
x˙ = −
1
x
, (7.1)
y = x
2
,
where the domain of definition is Dx,def = IR\{0}. Obviously, it is not possible
to determine the initial value x0 from the knowledge of y(t) uniquely, because
the equation y = x
2
has the two solutions
x1 = −
√
y and x2 =
√
y.
[2] The concept of weak observability results from the analysis by R. Hermann and
A. J. Krener [182].
[3] Local observability is often defined differently in the literature [182]. In some of
these cases, however, the observability is not local in the common sense of the
word but refers to a stronger property than observability.606 Chapter 7. Observers for Nonlinear Systems
Therefore, system (7.1) is not observable. However, it is weakly observable
and even globally weakly observable, because for all values x0 from a suitably
selected neighborhood
U = {x0 ∈ Dx,def | |x0 − xp| < ρ}
of each point xp ∈ Dx,def the value x0 can be determined uniquely from
y = x
2
. On the one hand, we obtain
x0 = −
p
y(t0) for xp < 0,
and on the other
x0 =
p
y(t0) for xp > 0.
However, determining x0 requires knowledge of xp.
As another example, let us examine the system
x˙ 1 = α(x2), α(x2) = (
0, x2 ≤ 0,
x
3
2
, x2 > 0,
x˙ 2 = u,
y = x1,
whose domain of definition is Dx,def = IR2
. Using
y˙ = ˙x1 = α(x2)
for x2 > 0, we obtain
x1 = y,
x2 =
p3
y˙
for the vectors x of the set
Dx,part1 = {x ∈ IR2
| x1 ∈ IR, x2 > 0}.
So we can conclude that the system is observable for all x ∈ Dx,part1. In
contrast to this result, we obtain
x1 = y,
x2 = x2(0) + Z
t
0
u(τ)dτ
for Dx,part2 = Dx,def \Dx,part1. We can immediately conclude that the system
is not observable for all x ∈ Dx,part2, since we cannot calculate x2(t) using7.1. Observability of Nonlinear Systems 607
u(t) and y(t) due to the unknown initial value x2(0). Therefore, the system is
only locally observable.
As an additional example, we will now describe a locally weakly observable
system. It is given by
x˙ 1 = β(x2), β(x2) = (
0, x2 < 0,
sin2
(x2), x2 ≥ 0,
x˙ 2 = u,
y = x1.
This system is weakly observable, not observable, for the subset
Dx,part1 = {x ∈ IR2
| x1 ∈ IR, π
2
i ≤ x2 <
π
2
(i + 1), i = 0, 1, 2, . . .},
since the equation
y˙ = ˙x1 = sin2
(x2)
has an infinite number of solutions x2 for a value y˙ but can be solved uniquely
in each interval [
π
2
i,
π
2
(i + 1)), i = 0, 1, 2, . . . For the set
Dx,part2 = {x ∈ IR2
| x1 ∈ IR, x2 < 0},
we have
x1 = y,
x2 = x2(0) + Z
t
0
u(τ)dτ
with the unknown initial value x2(0). From this we can immediately see that
the system is only locally weakly observable for x ∈ Dx,part2.
For nonlinear systems, a distinction has to be made between observability
and weak observability, according to the definitions above. In contrast, linear
systems are always observable if they are weakly observable and vice versa,
which is equivalent to saying that they are always globally observable. For
nonlinear systems it is possible that their observability depends on the input
variable vector u. The observability of a linear system, however, is always
completely independent of the input variable vector u.
7.1.2 Observability of Autonomous Systems
First of all, let us view the autonomous systems
x˙ = f(x),
y = g(x),608 Chapter 7. Observers for Nonlinear Systems
i. e. systems which are time-invariant and are not dependent on an input
variable vector u. In order to develop an observability criterion for them, we
use the Lie derivative
Lf g(x) = ∂g(x)
∂x
f(x)
and the multiple Lie derivative
L
k
f
g(x) = LfL
k−1
f
g(x) =
∂Lk−1
f
g(x)
∂x
f(x)
for the determination of
y = L
0
f
g(x) = g(x),
y˙ = L
1
f
g(x) = ∂g(x)
∂x
f(x),
.
.
.
y
(n−1) = L
n−1
f
g(x) = LfL
n−2
f
g(x).
The above Lie derivatives are now combined in the vector
q(x) =





L
0
f
g(x)
.
.
.
L
n−1
f
g(x)





.
Using the vector
z =







y
y˙
.
.
.
y
(n−1)







with the new state variable z1, . . . , zn, we obtain
z = q(x).
If the inverse function q
−1
(z) exists, x can be determined using the knowl￾edge of z1 = y, z2 = ˙y, . . . , zn = y
(n−1). The knowledge of y(t) within an
interval [t0, t1] therefore leads to the knowledge of the state vector x(t0). We
can summarize this in
Theorem 86 (Observability of Autonomous Systems). A system
x˙ = f(x),
y = g(x)
defined on Dx,def ⊆ IRn
is observable if the inverse function q
−1
(z) to deter￾mine all x ∈ Dx ⊆ Dx,def exists.7.1. Observability of Nonlinear Systems 609
The condition of Theorem 86 is only sufficient, since it is possible that a
nonlinear system is observable even though the inverse function q
−1
(z) does
not exist. An example of such a system can be found in Exercise 7.6.
For many nonlinear systems, the inverse function q
−1
is either impossible
or very difficult to determine. We will therefore derive a criterion for weak
observability which is easier to apply. For this purpose, we will expand q in a
Taylor series around a point xp:
z = q(xp) + ∂q(x)
∂x




x=xp
· (x − xp) + remainder.
Neglecting the remainder yields the equation
z − q(xp) ≈
∂q(x)
∂x




x=xp
· (x − xp).
From this equation, it is obvious that the state x can be reconstructed based
on the knowledge of z − q(xp) if the Jacobian matrix
Q(xp) = ∂q(x)
∂x




x=xp
=







∂L0
f
g(x)
∂x
.
.
.
∂Ln−1
f
g(x)
∂x







x=xp
is of rank n. This is equivalent to the fact that the system of linear equations
z − q(xp) = Q(xp)(x − xp)
can be solved uniquely for x. In this case, within a neighborhood
U = {x ∈ IRn
| |x − xp| < ρ, ρ > 0}
of xp we can determine whether the system in question is observable. This
yields the following theorem [42] on weak observability.
Theorem 87 (Weak Observability of Autonomous Systems). A sys￾tem
x˙ = f(x),
y = g(x)
which is defined on Dx,def ⊆ IRn
is weakly observable if
rank(∂q(x)
∂x
) = rank(







∂L0
f
g(x)
∂x
.
.
.
∂Ln−1
f
g(x)
∂x







) = n
holds for all x ∈ Dx ⊆ Dx,def.610 Chapter 7. Observers for Nonlinear Systems
The matrix
Q(x) = ∂q(x)
∂x
is also called the observability matrix . The application of the criterion above
can be somewhat complicated, since the matrix Q depends on x. However, de￾termining the rank of Q is usually easier than determining the inverse function
q
−1
in Theorem 86.
Theorem 87 can also be derived directly from the transformation equation
z = q(x). According to the implicit function theorem, the inverse function
q
−1
exists in a neighborhood of a point x if the Jacobian matrix Q(x) has
rank n at that point.
To illustrate Theorem 87, let us take the linear systems
x˙ = Ax,
y = c
T x.
With g(x) = c
T x, we obtain
q(x) =









L
0
f
g(x)
L
1
f
g(x)
L
2
f
g(x)
.
.
.
L
n−1
f
g(x)









=









c
T x
c
T Ax
c
T A
2x
.
.
.
c
T An−1x









and hence the Jacobian matrix
Q(x) = ∂q(x)
∂x
=







c
T
c
T A
c
T A
2
.
.
.
c
T A
n−1







. (7.2)
If the matrix in equation (7.2), i. e. the observability matrix, has rank n, the
linear system is observable. For linear systems, this requirement is not only
sufficient; it can be shown [218] that it is also necessary. This is the well-known
observability criterion for linear systems, which is obviously a special case of
the above criterion for the observability of nonlinear systems.
7.1.3 Example: Synchronous Generator
Let us examine a synchronous generator [325] as shown in Figure 7.3. The
load angle x1, which is a measure of the stability of the generator and whose
sine is proportional to the generated power, is one of the state variables and is
also the output variable. The other state variables are the frequency deviation7.1. Observability of Nonlinear Systems 611
S
N
S
N
Direction of rotation
x1
Fig. 7.3: Synchronous generator with magnetic field shown in blue and load angle x1
x2 of the rotor with respect to the power grid and the field flux linkage x3
of the magnetic field. The flux linkage x3 is either impossible or very difficult
to measure. Therefore, we will estimate it using an observer, for example to
perform a load angle control. The prerequisite for this is the observability of
the system, which we will analyze subsequently.
The state-space model of the generator is given by
x˙ 1 = x2,
x˙ 2 = b1 − a1x2 − a2x3 sin(x1) −
b2
2
sin(2x1),
x˙ 3 = −c1x3 + c2 cos(x1) + c3,
y = x1.
Here, a1, a2, b1, b2, c1, c2, and c3 are all constant machine parameters. We
can now determine
y˙ = ˙x1 = x2,
y¨ = ˙x2 = b1 − a1x2 − a2x3 sin(x1) −
b2
2
sin(2x1)
and thus arrive at612 Chapter 7. Observers for Nonlinear Systems


z1
z2
z3

 =


y
y˙
y¨

 =



x1
x2
b1 − a1x2 − a2x3 sin(x1) −
b2
2
sin(2x1)


 = q(x).
For the relevant region of state vectors
Dx = {x ∈ IR3
| x1 ∈ (0, π), x2 ∈ IR, x3 ∈ IR},
according to Theorem 86, the synchronous machine is observable. This is
because the mapping
z = q(x)
is invertible, and the inverse mapping is


x1
x2
x3

 =




z1
z2
b1 − a1z2 − 0.5b2 sin(2z1) − z3
a2 sin(z1)




for all x ∈ Dx.
7.1.4 Observability of General Nonlinear Systems
Next we will examine the nonlinear systems
x˙ = f(x,u),
y = g(x,u)
with an input variable vector u. As in the case of autonomous systems, we
will again calculate the n − 1 total time derivatives
y˙ =
∂g
∂x
f(x,u) + ∂g
∂u
u˙ = h1(x,u,u˙),
y¨ =
∂h1
∂x
f(x,u) + ∂h1
∂u
u˙ +
∂h1
∂u˙
u¨ = h2(x,u,u˙,u¨),
˙˙˙y =
∂h2
∂x
f(x,u) + ∂h2
∂u
u˙ +
∂h2
∂u˙
u¨ +
∂h2
∂u¨
...
u = h3(x,u,u˙,u¨,
...
u),
.
.
.
y
(n−1) =
∂hn−2
∂x
f(x,u) +
nX−1
i=1
∂hn−2
∂u(i−1) u
(i) = hn−1(x,u,u˙, . . . ,u
(n−1)).
Similar to the observability of autonomous systems, we now define7.1. Observability of Nonlinear Systems 613
z =







y
y˙
y¨
.
.
.
y
(n−1)







=







g(x,u)
h1(x,u,u˙)
h2(x,u,u˙,u¨)
.
.
.
hn−1(x,u,u˙, . . . ,u
(n−1))







= q(x,u,u˙, . . . ,u
(n−1)).
Again, the existence of the inverse function of
z = q(x,u,u˙, . . . ,u
(n−1))
is essential for the observability of the system. From the calculations above,
we obtain
Theorem 88 (Observability of Nonlinear Systems). A system
x˙ = f(x,u),
y = g(x,u)
which is defined on Dx,def ⊆ IRn
and for all u(t) ∈ C
n−1
m is observable if the
inverse function q
−1
(z,u,u˙, . . . ,u
(n−1)) to determine all x ∈ Dx ⊆ Dx,def
exists for all u(t) ∈ Cu ⊆ C
n−1
m .
Here, C
n−1
m is the space of (n − 1) - times continuously differentiable m -
dimensional vector functions. If q is a global diffeomorphism, the system is
globally observable.
In Section 7.1.1, we noted that the observability of a nonlinear system, in
contrast to that of a linear system, can depend on the input variable vector
u. Theorem 88 substantiates this statement, because the mapping z = q(x)
and its solvability for x, i.e. the calculation of its inverse, can also depend on
u. The simple example
x˙ 1 = −x2 · u,
x˙ 2 = −x1 − x2,
y = x1
illustrates this fact. For u = 0 the system is not observable, while for u 6= 0 it
is.
The above theorem is difficult to apply in practice. In simple cases only,
it is possible to determine the inverse function
x = q
−1
(z,u,u˙, . . . ,u
(n−1)).
Again, it may be easier to verify weak observability than to verify observ￾ability. Analogous to the case of autonomous systems, we can derive614 Chapter 7. Observers for Nonlinear Systems
Theorem 89 (Weak Observability of Nonlinear Systems). A system
x˙ = f(x,u),
y = g(x,u)
which is defined on Dx,def ⊆ IRn
and for all u(t) ∈ C
n−1
m is weakly observable
if the condition
rank(∂q(x,u,u˙, . . . ,u
(n−1))
∂x
) = rank(


















∂g(x,u)
∂x
∂h1(x,u,u˙)
∂x
∂h2(x,u,u˙,u¨)
∂x
.
.
.
∂hn−1(x,u,u˙, . . . ,u
(n−1))
∂x


















) = n
is fulfilled for all x ∈ Dx ⊆ Dx,def and u(t) ∈ Cu ⊆ C
n−1
m .
In contrast to Theorem 88, Theorem 89 can be applied at least for single
points, if the analytical verification of the rank condition fails, by calculating
the rank or the determinant of the observability matrix
Q(x,u,u˙, . . . ,u
(n−1)) = ∂q(x,u,u˙, . . . ,u
(n−1))
∂x
at given grid points (x,u,u˙, . . . ,u
(n−1)).
7.1.5 Nonlinear Observability Canonical Form
Let us consider an observable system
x˙ = f(x,u), (7.3)
y = g(x,u).
We assume that we can determine the mapping
z = q(x,u,u˙, . . . ,u
(n−1)) (7.4)
and the inverse mapping
x = q
−1
(z,u,u˙, . . . ,u
(n−1)). (7.5)
In this case, it is possible to convert the system representation (7.3) into the
representation7.1. Observability of Nonlinear Systems 615








z˙1
z˙2
.
.
.
z˙n−1
z˙n








=








y˙
y¨
.
.
.
y
(n−1)
y
(n)








=








z2
z3
.
.
.
zn
hn

q
−1
(z,u,u˙, . . . ,u
(n−1)),u,u˙, . . . ,u
(n)









,
y = z1
by means of the mapping (7.5) and
y
(n) = hn(x,u,u˙, . . . ,u
(n)
) = ∂hn−1
∂x
f(x,u) +Xn
i=1
∂hn−1
∂u(i−1)u
(i)
.
Finally, by abbreviating
ϕ

z,u,u˙, . . . ,u
(n)

= hn

q
−1
(z,u,u˙, . . . ,u
(n−1)),u,u˙, . . . ,u
(n)

,
we obtain
z˙ =






z2
.
.
.
zn
ϕ

z,u,u˙, . . . ,u
(n)







, (7.6)
y = z1.
The system representation (7.6) is referred to as the nonlinear observability
canonical form [493]. Figure 7.4 shows the corresponding block diagram. If a
system is represented in this form, the observability can be inferred directly
from the system equations. This is because all states zi affect the output
variable y = z1 via the integrator chain in equation (7.6). Furthermore, all
state values zi can be determined from the output variable y and its derivatives
y
(i)
. Hence, a system that is represented in nonlinear observability canonical
form is always observable. Furthermore, any globally observable system can
be transformed into the observability canonical form.
As an example, let us take the system
x˙ 1 = −x2 + u,
x˙ 2 = −x3,
x˙ 3 = −x
3
1
,
y = x1.
(7.7)
In this case, we obtain
q(x, u, u˙) =


z1
z2
z3

 =


y
y˙
y¨

 =


x1
−x2 + u
x3 + ˙u

616 Chapter 7. Observers for Nonlinear Systems
ϕ(z, u, u˙ , . . . , u
(n)
)
1
s
1
s
1
s
1
s z˙n zn zn−1 z3 z2 y = z1
. . .
. . .
u u˙ u
(n)
Fig. 7.4: System in nonlinear observability canonical form
for the mapping (7.4), while its inverse mapping is given by
q
−1
(z, u, u˙) =


x1
x2
x3

 =


z1
−z2 + u
z3 − u˙

. (7.8)
Since the inverse mapping q
−1
exists for all x ∈ IR3
, system (7.7) is observable.
With the derivatives
z˙1 = z2,
z˙2 = z3,
z˙3 = ˙˙˙y = ˙x3 + ¨u = −x
3
1 + ¨u,
we obtain the nonlinear observability canonical form
z˙ =


z2
z3
−z
3
1 + ¨u

. (7.9)
This result can also be obtained more formally by using the differential
equation (3.95) of the transformed system that we derived in Section 3.3.1 on
p. 263. To refresh our memory, this differential equation is derived once again
below. Inserting the diffeomorphism
x = q
−1
(z,u, . . . ,u
(n−1))
into the system equation (7.3) yields7.1. Observability of Nonlinear Systems 617
dq
−1
(z,u, . . . ,u
(n−1))
dt =
∂q
−1
(z,u, . . . ,u
(n−1))
∂z
·z˙
+
nX−1
j=0
∂q
−1
(z,u, . . . ,u
(n−1))
∂u(j)
· u
(j+1)
= f(q
−1
(z,u, . . . ,u
(n−1)),u),
from which
z˙ =

∂q
−1
(z,u, . . . ,u
(n−1))
∂z
−1
·

f(q
−1
(z,u, . . . ,u
(n−1)),u) −
nX−1
j=0
∂q
−1
(z,u, . . . ,u
(n−1))
∂u(j)
· u
(j+1)

follows. After inserting diffeomorphism (7.8) from our exemplary system, we
obtain the transformed differential equation
z˙ =


1 0 0
0 −1 0
0 0 1






z2
−z3 + ˙u
−z
3
1

 −


0
1
0

u˙ −


0
0
−1

u¨

 =


z2
z3
−z
3
1 + ¨u

,
and thus equation (7.9) follows once again.
7.1.6 Observability of Control-Affine Systems
We will now address the control-affine systems
x˙ = a(x) + b(x) · u,
y = c(x).
(7.10)
If a system of this kind has a relative degree of n, it can be transformed into
nonlinear controller canonical form





z˙1
.
.
.
z˙n−1
z˙n





=





z2
.
.
.
zn
L
n
a
c(t
−1
(z)) + LbL
n−1
a
c(t
−1
(z))u





y = z1
(7.11)
using the diffeomorphism
z = q(x) = t(x) =





c(x)
Lac(x)
.
.
.
L
n−1
a c(x)





, (7.12)618 Chapter 7. Observers for Nonlinear Systems
as we know from Section 5.2.1, p. 359 et seq. Since the nonlinear controller
canonical form is identical to the nonlinear observability canonical form in the
case of control-affine systems (7.11), we obtain
Theorem 90 (Observability of Control-Affine Systems). A system
x˙ = a(x) + b(x) · u, x ∈ Dx,def ⊆ IRn
,
y = c(x)
is weakly observable if its relative degree is n.
Note that the theorem above only guarantees weak observability and not
observability. This is because the mapping (7.12) may be a diffeomorphism
only in a subset of the region of interest Dx and thus does not guarantee
bijectivity everywhere in Dx. However, if the diffeomorphism holds everywhere
in the region Dx or is even global, the system is observable.
Theorem 90 is only sufficient, but not necessary. The reverse conclusion,
that a control-affine system with a relative degree δ < n is not observable,
generally does not apply. As an example, let us take the system
x˙ = Ax + bu,
y = c
T x
with
A =

0 −1
1 0 
, b =

1
0

, c
T =

1 0 
.
It is of relative degree δ = 1 and the regular observability matrix is given by
Mobs =

c
T
c
T A

=

1 0
0 −1

.
Thus, this is an example of an observable system with a relative degree of
δ < n.
Now we will examine the case with a relative degree less than n. As in the
more general case x˙ = f(x,u), y = g(x,u), which we discussed in Section
7.1.4, we can calculate
y = c(x),
y˙ = Lac(x) + Lbc(x)u,
y¨ = L
2
ac(x) + LbLac(x)u + LaLbc(x)u + L
2
b
c(x)u
2 + Lbc(x) ˙u,
.
.
.
y
(n−1) = L
n−1
a c(x) + LbL
n−2
a c(x)u + . . . + L
n−1
b
c(x)u
(n−1) + Lbc(x)u
(n−2)
.
In this way, the transformation equation7.1. Observability of Nonlinear Systems 619
z =





y
y˙
.
.
.
y
(n−1)





=





c(x)
Lac(x) + Lbc(x)u
.
.
.
L
(n−1)
a
c(x) + . . . + Lbc(x)u
(n−2)





= q(x, u, u, . . . , u ˙
(n−2))
is obtained. As discussed in Section 5.2.4, p. 372 et seq., the terms LbL
k−1
a
c(x)
are identical to zero for all k < δ.
We can examine the observability and the weak observability of system
(7.10) using Theorems 88 and 89, respectively. In using the latter, we have to
verify whether the Jacobian matrix
∂q
∂x
=





















∂c(x)
∂x
∂Lac(x)
∂x
+
∂Lbc(x)
∂x
u
∂L2
a
c(x)
∂x
+
∂LbLac(x)
∂x
u +
∂LaLbc(x)
∂x
u +
∂L2
b
c(x)
∂x
u
2 +
∂Lbc(x)
∂x
u˙
.
.
.
∂Ln−1
a
c(x)
∂x
+
∂LbL
n−2
a
c(x)
∂x
u + . . . +
∂Lbc(x)
∂x
u
(n−2)





















has rank n. Note that the rank condition of Theorem 89 again depends on
u. Evidently, there may be input signals u, so that the rank condition is not
fulfilled and the system is not weakly observable. However, no statement about
weak observability can be made using Theorem 89 if the rank condition is not
fulfilled. This is because Theorem 89 provides a sufficient condition only.
Let us view the simple example
x˙ 1 = −x2 + x2 · u,
x˙ 2 = −x1,
y = x1.
(7.13)
Here,
∂q
∂x
=





∂c(x)
∂x
∂Lac(x)
∂x
+
∂Lbc(x)
∂x
u





=





1 0
0 u − 1





.
For u 6= 1, we obtain
rank ( ∂q
∂x
) = 2.620 Chapter 7. Observers for Nonlinear Systems
Therefore the system is weakly observable. If u = 1, it follows that ∂q/∂x
only has rank one. In fact, system (7.13) is not observable for u = 1, since in
this case
x˙ 1 = 0,
x˙ 2 = −x1,
y = x1
(7.14)
holds and, for this system, we cannot determine the value of x2 from the
knowledge of y and u. This can also be shown by applying the necessary and
sufficient observability criterion for linear systems, because the observability
matrix from equation (7.2) for system (7.14),

c
T
c
TA

=

1 0
0 0 
,
only has rank one.
7.2 Canonical Forms and the Canonical Form Observer
We have already noted the usefulness of system representations in special
coordinates, meaning canonical forms, for various tasks or concepts such as
controllability, controller design using feedback linearization, and observabil￾ity. For example, these coordinates and the canonical forms enable us to easily
assess the controllability and observability of a system.
The best known canonical forms are the controller canonical form
x˙ =







0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
−a0 −a1 −a2 · · · −an−1







x +







0
0
.
.
.
0
1







u,
y =

b0 b1 · · · bm 0 · · · 0

x,
and the observer canonical form
x˙ =







0 0 0 · · · 0 −a0
1 0 0 · · · 0 −a1
0 1 0 · · · 0 −a2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1 −an−1







x +










b0
.
.
.
bm
0
.
.
.
0










u,
y = xn7.2. Canonical Forms and the Canonical Form Observer 621
of linear systems[4]. The term controller canonical form is derived from the
fact that the design of a state controller
u = −k
T x
is particularly simple for systems in this representation. In the case of the
observer canonical form, the name has a similar origin, since this form makes
the design of a Luenberger observer easily possible.
For nonlinear systems as well, the names of the canonical forms were coined
because of these possibilities. For example, when designing a controller using
feedback linearization, we used the nonlinear controller canonical form in Sec￾tion 5.2.1, p. 359 et seq. Table 7.1 gives us an overview of some nonlinear
canonical forms used in control theory.
In the case of the nonlinear observer canonical form [248, 494, 495]
x˙ =







− a1(xn, u, u, . . . , u ˙
(n)
)
x1 − a2(xn, u, u, . . . , u ˙
(n−1))
.
.
.
xn−2 − an−1(xn, u, u,˙ u¨)
xn−1 − an(xn, u, u˙)







= Ax − a(xn,u
0...n),
y = g(xn), and xn = g
−1
(y)
(7.15)
with
A =







0 0 · · · 0 0
1 0 · · · 0 0
0 1 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 1 0







, u
0...n =

u u˙ · · · u
(n)
T
,
a(xn,u
0...n) = 
a1(xn,u
0...n) · · · an(xn,u
0...n)
T
,
we are able to easily design an observer which is termed a canonical form
observer or, synonymously, a normal form observer. The dynamics of this
type of an observer are given by
x˜˙ = Ax˜ − a(g
−1
(y),u
0···n
) + l(g
−1
(y) − c
T x˜),
c
T =

0 · · · 0 1
,
consist of a model of system (7.15) and the observer gain l. These are similar
[4] The nomenclature is not consistent in the literature. Although the one used here is
the most common, the terms controllable canonical form, control canonical form,
and controllability canonical form are used synonymously. Similarly, the term ob￾servable canonical form is used instead of observer canonical form. Furthermore,
the term normal form is used synonymously with the term canonical form.622 Chapter 7. Observers for Nonlinear Systems
Table 7.1: Canonical forms
1 Nonlinear controller canonical form
x˙ =







x2
x3
.
.
.
xn
α(x)







+







0
0
.
.
.
0
β(x)







u, y = g(x)
Characteristics : controllable if β(x) 6= 0, control-affine
Usage : designing controllers, verifying controllability
Equivalence : to the nonlinear observability canonical form in the
case of control-affine systems if y = x1
Special cases : linear controller canonical form, Brunovsky canonical
form
2 Generalized controller canonical form
x˙ =







x2
x3
.
.
.
xn
f(x, u, u, . . . , u ˙
(n)
)







, y = g(x)
Usage : designing controllers
Special cases : (nonlinear) controller canonical form
3 Nonlinear observability canonical form
x˙ =







x2
x3
.
.
.
xn
f(x, u, u, . . . , u ˙
(n)
)







, y = x1
Characteristics : observable
Usage : verifying observability, designing high-gain observers
Equivalence : to the nonlinear controller canonical form in the case of
control-affine systems and to the generalized controller
canonical form if g(x) = x1
Special cases : Brunovsky canonical form7.2. Canonical Forms and the Canonical Form Observer 623
Table 7.1: Canonical forms - continued
4 Brunovsky canonical form
x˙ =







x2
x3
.
.
.
xn
0







+







0
0
.
.
.
0
1







u, Characteristics : controllable, observable if y = x1
5 Byrnes-Isidori and generalized Byrnes-Isidori canonical form
x˙ =














x2
x3
.
.
.
xδ
α(x)
qδ+1(x)
.
.
.
qn(x)














+














0
0
.
.
.
0
β(x)
0
.
.
.
0














u, y = x1; x˙ =














x2
x3
.
.
.
xδ
ϕ(x, u)
qδ+1(x)
.
.
.
qn(x)














, y = x1
Usage : designing controllers
6 Nonlinear controllability canonical form
x˙ =





− a1(xn)
x1 − a2(xn)
.
.
.
xn−1 − an(xn)





+





1
0
.
.
.
0





u, y = g(x)
Characteristics : controllable Usage : verifying controllability
7 Nonlinear observer canonical form
x˙ =







− a1(xn, u, u, . . . , u ˙
(n)
)
x1 − a2(xn, u, u, . . . , u ˙
(n−1))
.
.
.
xn−2 − an−1(xn, u, u,˙ u¨)
xn−1 − an(xn, u, u˙)







, y = g(xn) and xn = g
−1
(y)
Characteristics : observable
Usage : designing canonical form observers
Special cases : linear observer canonical form624 Chapter 7. Observers for Nonlinear Systems
to a linear observer. The constant vector
l
T =

l1 · · · ln

is freely selectable. Note that we have used the system’s output value y instead
of the observer’s output value y˜ for calculating the observer’s dynamics. This
is possible because we can measure the output variable y.
In order to calculate the estimation error
e = x − x˜,
we will first determine its time derivative
e˙ = x˙ − x˜˙
= Ax − a(g
−1
(y),u
0···n
) − Ax˜ + a(g
−1
(y),u
0···n
) − l(g
−1
(y) − c
T x˜)
= Ax − Ax˜ − l(c
T x − c
T x˜).
The terms a(g
−1
(y),u
0···n
) are identical in the system dynamics and the ob￾server dynamics above, so they cancel each other out. Thus we obtain the
linear error dynamics
e˙ = (A − lcT
)e.
This approach to designing a nonlinear observer is known as exact error lin￾earization. The linear dynamics and the fact that we are free to choose the
eigenvalues of A − lcT makes the canonical form observer so effective. How￾ever, normally we have to transform a system representation into the observer
canonical form to design an observer of this type. This is generally a laborious
procedure, since we have to solve some partial differential equations to obtain
the appropriate diffeomorphism [43, 385, 386, 494].
7.3 Luenberger Observers for Nonlinear Control Loops
Luenberger’s observer theory was developed for linear systems. It can be ex￾tended to nonlinear systems for some special cases. For example, a Luenberger
observer can be used [289] in the case of a control loop with a linear plant
x˙ = Ax + Bu,
y = Cx
and a nonlinear controller
u = h(x, yref),
i. e. a nonlinear control loop
x˙ = Ax + Bh(x, yref).7.3. Luenberger Observers for Nonlinear Control Loops 625
As in the linear case, the observer
x˜˙ = Ax˜ + Bu
| {z }
Model
+ L(y − Cx˜)
| {z }
Feedback system
.
consists of a system model and a feedback system which acts like a controller.
The task of the feedback system
L (y − Cx˜)
is to asymptotically reduce the estimation error
e = x − x˜,
which is the difference between the system state x and the observer state x˜,
to zero. Figure 7.5 shows the structure of the control system with an observer
and the nonlinear control law u = h(x˜, yref).
v
x˜(0)
x(0)
yref
h(x˜, yref)
u
y
y˜
y − y˜
x˙ = A x + B u
y = C x
x˜
x˜˙ = A x˜+B u+v
y˜ = C x˜
L
Observer
Controller Plant
Fig. 7.5: Structure of the system with linear plant, nonlinear controller, and linear
observer
In the case of a constant reference vector yref, it is possible to transform the
control system in such a way that yref = 0 applies with no loss of generality.
Thus, a controller of the form
u = h(x˜)
is assumed below.
In order to ensure the stability of the control system in question, we can
use the following theorem, which was proven by D. G. Luenberger [289].626 Chapter 7. Observers for Nonlinear Systems
Theorem 91 (Luenberger Observer for Nonlinear Control Loops).
Let there be a control loop
x˙ = Ax + Bu, y = Cx,
u = h(x)
with a globally (locally) asymptotically stable equilibrium point xeq = 0. Fur￾ther, let the vector function h fulfill the Lipschitz condition
|h(x1) − h(x2)| ≤ k · |x1 − x2|
for all x1, x2 ∈ IRn
(for all x1, x2 within a neighborhood U ⊂IRn
of xeq = 0)
and for a real number k > 0. By inserting an asymptotically stable, linear
observer into the control loop, we obtain the overall system
x˙ = Ax + Bu, y = Cx,
u = h(x˜),
x˜˙ = Ax˜ + Bu + L(y − Cx˜).
This overall system has a globally (locally) asymptotically stable equilibrium
point at 
x
T x˜
T

= 0
T
.
In this context, recall Chapter 1, p. 39, where we defined Lipschitz continuity
and called functions fulfilling the Lipschitz condition Lipschitz continuous. For
differentiable functions, it holds that they are globally Lipschitz continuous if
and only if their derivatives are bounded.
Typical applications of Theorem 91 include soft variable structure controls
and the saturation controls which we discussed in Chapter 4.
7.4 Observer Design Using Linearization
7.4.1 Basics and Design
In this section, we will continue to address the Luenberger observer as we aim
to leverage its general concept to develop an observer for nonlinear systems.
If we consider a linear system
x˙ = Ax + Bu,
y = Cx
with the Luenberger observer
x˜˙ = Ax˜ + Bu + L(y − Cx˜),
the idea of applying this scheme to the nonlinear systems7.4. Observer Design Using Linearization 627
x˙ = f(x,u),
y = g(x)
seems reasonable. The observer is then given by
x˜˙ = f(x˜,u) + L(y − g(x˜)). (7.16)
For the observer error
e = x − x˜,
we obtain the differential equation
e˙ = f(x˜ + e,u) − f(x˜,u) − L(y − g(x˜)).
In contrast to the linear case, a nonlinear differential equation for the estima￾tion error is obtained. Replacing
y = g(x) = g(x˜ + e)
in this equation results in
e˙ = f(x˜ + e,u) − f(x˜,u) − L(g(x˜ + e) − g(x˜)). (7.17)
For the estimation error e to decay asymptotically, the design task is to
manipulate the nonlinear system equation (7.17) using L in such a way that
an asymptotically stable equilibrium point at e = 0 results.
The simplest way to design the observer matrix L is to linearize equation
(7.17). A linearization around a fixed point (x˜p,up) is performed by applying
the Taylor series expansion, resulting in
f(x˜p + ∆x˜ + e,u) = f(x˜p,u) + ∂f
∂x˜




x˜=x˜p
u=up
| {z }
A
·(∆x˜ + e) + . . .
Simultaneously, the relation
f(x˜p + ∆x˜,u) = f(x˜p,u) + A · ∆x˜ + . . .
holds. For the output vector function g, we obtain the similar results
g(x˜p + ∆x˜ + e) = g(x˜p) + ∂g
∂x˜




x˜=x˜p
u=up
| {z }
C
·(∆x˜ + e) + . . .
and628 Chapter 7. Observers for Nonlinear Systems
g(x˜p + ∆x˜) = g(x˜p) + C · ∆x˜ + . . .
We will neglect the terms of the Taylor series after the first derivative. Insert￾ing this reduced Taylor series into the observer equation (7.17) yields
e˙ ≈ (A − LC) · e. (7.18)
The latter is obviously the estimation equation of the linear observer. Note
that equation (7.18) is only an approximation, due to the omission of higher
derivatives in the Taylor series.
If we design the matrix A−LC of the observer so that it is asymptotically
stable, the estimation error e will also converge to zero for t → ∞. The
situation is similar to that of linear systems with linear observers. However,
due to the above approximation, this statement of stability only applies within
a neighborhood of unknown size surrounding the linearization point x˜p.
To achieve better, more general results, it is possible to represent the
linearization described above as a function of x˜ and u. Then the system
matrix
A(x˜,u) = ∂f(x˜,u)
∂x˜
and the output matrix
C(x˜) = ∂g(x˜)
∂x˜
are no longer constant; they are dependent on the operating point.
Furthermore, the observer matrix L is now selected as a function of x˜ and
u. Thus, instead of the linear differential equation
e˙ ≈ (A − LC) · e (7.19)
for the estimation error e, a nonlinear, more precise equation
e˙ ≈ (A(x˜,u) − L(x˜,u)C(x˜)) · e, (7.20)
dependent on x˜ and u, results and incorporates a continuum of operating
points. In contrast to a linear observer with its error dynamics (7.19), a non￾linear observer with its error equation (7.20) can better reproduce the plant’s
dynamics, and thus achieve a lower error e than the linear observer.
The aim now is to design the system matrix
F(x˜,u) = A(x˜,u) − L(x˜,u)C(x˜)
of the estimation error equation to be independent of x˜ and u, which requires
a suitable choice of L(x˜,u). Moreover, this should be done in such a way that
all eigenvalues of F have negative real parts. In this case,
e˙ = F e
applies, and the estimation error e decays asymptotically.7.4. Observer Design Using Linearization 629
To obtain a constant matrix F of this kind, we can attempt to calculate
L(x˜,u) for a given constant matrix F from
L(x˜,u) = (A(x˜,u) − F) C
−1
(x˜).
However, this is only possible for an invertible matrix C(x˜). A prerequisite
for the invertibility of C is that
C(x˜) = ∂g(x˜)
∂x˜
is a quadratic matrix. This in turn requires that g(x˜) is an n-dimensional
function, i. e. there must be as many output variables as there are states. In
this case, we are able to determine the state vector by means of
x = g
−1
(y)
and consequently do not need an observer. However, the existence of the same
number of states and output variables is rather rare in practice; therefore it
is usually not relevant.
For this reason, we will use an alternative method, albeit not one which
generally leads to a constant matrix F: the characteristic polynomial
P(s) = det(sI − F(x˜,u))
= det(sI − A(x˜,u) + L(x˜,u)C(x˜))
=
Yn
i=1
(s − λi)
is calculated and our requirement is that the real parts of all eigenvalues λi
are negative. Here it must be ensured that the eigenvalues λi and thus the
coefficients of P(s) are independent of both x˜ and u.
To ensure that all Re {λi} < 0, the observer matrix L(x˜,u) must be
designed accordingly. We succeed in this if we choose the matrix L(x˜,u) in
such a way that the characteristic polynomial
P(s) = s
n + an−1(L, x˜,u)s
n−1 + . . . + a0(L, x˜,u) (7.21)
is identical to the freely selectable polynomial
Pˆ(s) = s
n + ˆan−1s
n−1 + . . . + ˆa0 (7.22)
which has only zeros with negative real parts and constant coefficients ai
. By
equating the coefficients of the polynomials (7.21) and (7.22), we obtain
an−1(L, x˜,u) = ˆan−1,
an−2(L, x˜,u) = ˆan−2,
.
.
.
a0(L, x˜,u) = ˆa0.630 Chapter 7. Observers for Nonlinear Systems
This is a system with n nonlinear equations and n · r unknowns. The latter
are the elements of the n × r matrix L. Remember that n is the system order
and r is the dimension of the output variable vector y.
The solution of the above system of equations ensures that the eigenval￾ues of F are constant and correspond to the zeros of the freely selectable
polynomial Pˆ(s). However, this does not guarantee fulfillment of the original
requirement that the system matrix
F(x˜,u) = A(x˜,u) − L(x˜,u)C(x˜) (7.23)
of the linearized observer equation is constant, i. e. independent of x˜ and u.
Rather, the dependence of F on x˜ and u must be verified by inserting the
matrix L(x˜,u) into equation (7.23). If it turns out that F is not independent
of x˜ and u, the observer
x˜˙ = f(x˜,u) + L(x˜,u) · (y − g(x˜))
can still be used if the elements of F do not vary widely. In the latter case,
it is essential to assess the stability and performance by means of simulations
or other methods.
7.4.2 Control Loop with Observer
Inserting the observer developed above into a nonlinear control loop results
in a similar structure as in the case of the Luenberger observer for the linear
control loop. This is shown in Figure 7.6. However, L is not constant in the
nonlinear case; rather, it is a function of x˜ and u.
The corresponding equations for the control loop with the observer are
x˙ = f(x,u),
y = g(x),
)
plant
u = h(x˜, yref),
	
controller
x˜˙ = f(x˜,u) + L(x˜,u) · (y − g(x˜)).
	
observer
(7.24)
As an alternative to this notation, the equations for the overall system can
also be formulated in terms of the estimation error
e = x − x˜.
In this case, we obtain the differential equations of the control loop with the
approximate observer dynamics
x˙ = f(x,u),
y = g(x),
u = h(x − e, yref),
e˙ ≈ F(x − e,u)e.7.4. Observer Design Using Linearization 631
yref
v = L(x˜, u)(y − y˜)
u x
x˜ x˜
y
y − y˜
y˜
x(0)
x˜(0)
Plant
Observer
L(x˜, u)
h(x˜, yref) g(x)
g(x˜)
x˙ = f(x, u)
x˜˙ = f(x˜, u) + v
Fig. 7.6: Structure of the plant with observer and controller h(x˜, yref)
In contrast to the control loop’s description (7.24), this notation is only suit￾able for analysis and not for implementation, since the estimation error e
cannot be calculated with x being unknown.
But the equations of the latter system allow for a plausibility analysis with
regard to stability. Since F is approximately constant and has eigenvalues with
a negative real part,
e → 0 for t → ∞
applies. This means the control law for large values t becomes
u = h(x − e, yref) ≈ h(x, yref).
If the control loop is stable without an observer, it is plausible to assume,
although it is not proven, that the control loop with an observer is also stable.
However, to ensure that the control loop is stable, it is necessary to provide a
rigorous proof of stability using the Lyapunov method, for example.
7.4.3 Example: Bioreactor
Let us view a bioreactor [175] as an example. Bioreactors are used for the
production of vitamins and medicines. In the first step, a cell culture is culti￾vated and used for the production of the target substance. We will model the
growth phase of the cell culture as follows.632 Chapter 7. Observers for Nonlinear Systems
Cell culture
Pump
Mixer
Glucose
u
x1 x2
Fig. 7.7: Bioreactor
The reactor has a constant volume. Glucose is pumped into the bioreactor
as a substrate. The cell culture reproduces by consuming the substrate and
thereby increases its biomass. A mixer ensures even blending. The mixture
is finally removed from the reactor for further processing steps. Figure 7.7
illustrates the process.
The biomass concentration x1 of the cell culture, measured in g l−1
, in￾creases proportionally to its population according to
x˙ 1 = µ(x2) · x1.
In other words, it obeys a classic law of growth. The growth constant µ de￾pends on the concentration x2 of the growth substrate, i. e. the glucose, ac￾cording to the growth kinetics
µ(x2) = µ0 · x2
k1 + x2 + k2x
2
2
with the associated growth rate µ0 = 1 h−1
and the two affinity constants
k1 = 0.03 g l−1
and k2 = 0.5 l g−1
. The substrate concentration x2 in the
cell culture is measured in g l−1
.
The inflow relative to the reactor’s volume is the input variable u and
measured in h
−1
. It dilutes the biomass by the value −x1 · u. This results in
a change in the biomass concentration over time according to7.4. Observer Design Using Linearization 633
x˙ 1 = µ(x2) · x1 − x1 · u.
The substrate mass in the reactor is consumed by the biomass and therefore
decreases proportionally to the cell culture amount x1. The substrate inflow
increases the substrate concentration x2 in the reactor proportionally to the
inflow u and the difference between the concentration K of glucose in the
inflow and the concentration x2 in the reactor. Hence,
x˙ 2 = −
1
α
µ(x2) · x1 + (K − x2) · u
holds, where α = 0.5 is the yield coefficient of the bioreactor and K = 10 g l−1
is the glucose feed concentration. In summary, the state-space model
x˙ = a(x) + b(x) · u =
"
µ(x2) · x1
−
1
α
µ(x2) · x1
#
+

−x1
K − x2

u,
y = g(x) = 
1 0
x
results.
In the following, we will design an observer using linearization and the
state-dependent observer matrix L(x˜, u). Our goal is to estimate the concen￾trations x1 and x2. For the observer design, the Jacobian matrices
A(x˜, u) = ∂f(x˜, u)
∂x˜
and C(x˜) = ∂g(x˜)
∂x˜
must first be computed for the linearization of the system. This results in
A(x˜, u) =



µ(˜x2) − u µ′
(˜x2)˜x1
−
1
α
µ(˜x2) −
1
α
µ
′
(˜x2)˜x1 − u


, µ′
(˜x2) = ∂µ(˜x2)
∂x˜2
,
and
C(x˜) = 
1 0
.
In the next design step, the characteristic polynomial of the observer ma￾trix is calculated as
P(s) = det(sI − [A(x˜, u) − L(x˜, u)C(x˜)]
| {z }
observer matrix F(x˜, u)
),
where
L(x˜, u) = 
l1(x˜, u)
l2(x˜, u)

.
For the sake of clarity, we will use the abbreviations634 Chapter 7. Observers for Nonlinear Systems
l1 = l1(x˜, u), l2 = l2(x˜, u),
µ = µ(˜x2), µ′ = µ
′
(˜x2) = ∂µ(˜x2)
∂x˜2
.
This yields
P(s) = s
2 +

l1 − µ +
µ
′
α
x˜1 + 2u

s + µ
′
x˜1

l2 +
l1 + u
α

+ l1u − µu + u
2
for the characteristic polynomial of F.
Comparing the coefficients with those of the polynomial we intend to ob￾tain,
Pˆ(s) = s
2 + ˆa1s + ˆa0,
yields
aˆ1 = l1 − µ +
µ
′
α
x˜1 + 2u ,
aˆ0 = µ
′
x˜1

l2 +
l1 + u
α

+ l1u − µu + u
2
.
Solving this system of equations, we obtain
l1 = ˆa1 + µ −
µ
′
α
x˜1 − 2u ,
l2 =
aˆ0 − aˆ1u + u
2
µ′x˜1
−
aˆ1 + µ − α
−1µ
′x˜1 − 2u
α
.
We can now calculate the matrix F of the observer’s dynamics
e˙ ≈ F(x˜, u)e
by inserting the matrix L(x˜, u) into equation (7.23). This leads to
F(x˜, u) = A(x˜, u) − L(x˜, u) C(x˜)
=





−aˆ1 +
µ
′
α
x˜1 + u µ′
x˜1
−
aˆ0 − aˆ1u + u
2
µ′x˜1
+
aˆ1 − α
−1µ
′x˜1 − 2u
α
−
µ
′
α
x˜1 − u





.
Note that although F depends on x˜ and u, the characteristic polynomial
P(s) = Pˆ(s) = s
2 + ˆa1s + ˆa0
of the matrix F does not depend on x˜ and u. We will design the observer
with aˆ0 = 100 and aˆ1 = 20, so that the eigenvalues of F(x˜, u), i. e. the zeros
of the characteristic polynomial P, are s1/2 = −10.7.5. The Extended Kalman Filter 635
Time t in hours
5
4
3
0.1
0
0
0
1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
State variable
Estimated state variable
x1 in gl
−1
x2 in gl
−1 Fig. 7.8: Time courses of actual and estimated biomass concentration x1 and sub￾strate concentration x2
The diagrams in Figure 7.8 show the progression of the biomass and sub￾strate concentration in the bioreactor for the initial state
x(0) = [4.0 0.02]T
g l−1
.
As a comparison, the system states of the observer are given, starting from
x˜(0) = [3.0 0.05]T
g l−1
.
For an input variable value of u = 0.5, the system states approach the final
values x1 = 4.94 g l−1
and x2 = 0.03 g l−1
. The difference between the ac￾tual system states of the reactor and those observed are rapidly and steadily
eliminated.
7.5 The Extended Kalman Filter
7.5.1 Kalman Filter for Linear Systems
In practice, the extended Kalman filter, abbreviated EKF, is the most com￾monly used type of observer for nonlinear systems. It is based on a linearized
representation of the nonlinear system dynamics. To explain how it works, we
will first briefly describe the Kalman filter [133, 219] for linear systems.
The system to be observed,636 Chapter 7. Observers for Nonlinear Systems
x˙ = Ax + Bu + µ,
y = Cx + ρ,
is disturbed by two zero-mean, normally distributed, white-noise processes µ
and ρ which are assumed to be uncorrelated. Here µ is the process noise and
ρ is the measurement noise. The covariance matrices Q and S of the noise
processes are given by
cov{µ(t1), µ(t2)} = Q · δ(t1 − t2),
cov{ρ(t1), ρ(t2)} = S · δ(t1 − t2),
where δ is the Dirac delta distribution. The Kalman filter
x˜˙ = (A − LC) x˜ + Bu + Ly
with the filter matrix L yields an estimate x˜ of the state variable vector x.
Figure 7.9 depicts the system being observed and the structure of the Kalman
filter, which is identical to the structure of the Luenberger observer, as shown
in Figure 7.1, p. 604.
In fact, according to linear systems theory [133, 181], Kalman filters and
Luenberger observers are identical in their equations. They differ only in the
calculation of the matrix L. To design a Luenberger observer, appropriate
eigenvalues of the matrix A − LC must be chosen via L. In contrast to this
approach, the Kalman matrix L is designed so that the influence of the process
noise µ and the measurement noise ρ on the estimation error e is minimal.
For this purpose, we will use the performance index
J =
Xn
i=1
E

e
2
i
	
,
which is based on the estimation error
e = [e1 · · · en]
T = x − x˜
and the expected values
E

e
2
i
	
= lim
T→∞
1
2T
Z
T
−T
e
2
i
(t)dt.
Minimizing J in dependence on L leads to the matrix
L = P CTS
−1
(7.25)
which we were attempting to determine. The matrix P is yielded by the
algebraic Riccati equation
AP + P AT − P CTS
−1CP = −Q. (7.26)7.5. The Extended Kalman Filter 637
x(0)
x˜(0)
u
Kalman filter
y
v y˜
x˜
y − y˜
L
x˙ = Ax + Bu + µ
y = Cx + ρ
x˜˙ = Ax˜ + Bu + v
y˜ = Cx˜
Fig. 7.9: Structure of the linear Kalman filter
The matrices S and Q are generally unknown and are often assumed to be
identity matrices. In many cases, however, only subsequent, iterative testing
or optimization by trial and error using other matrices S and Q leads to a
satisfactory design result.
7.5.2 The EKF for Nonlinear Systems
In the case of a nonlinear system
x˙ = f(x,u) + µ,
y = g(x) + ρ,
(7.27)
we will utilize the estimation equation of the linear Kalman filter
x˜˙ = f(x˜,u) + L(y − g(x˜))
once again. We used this equation, which equals the equation (7.16) of the
Luenberger observer in Section 7.4.1, to design an observer using linearization.
However, the choice of a constant matrix L was disadvantageous, because the
observer will only perform well in the neighborhood of a given operating point.
The same applies here. Therefore, here we select a time-dependent observer
matrix L so that we can adapt it to the nonlinearity of the system which
depends on the trajectories x(t) or x˜(t). Thus, the observer equation takes
the form
x˜˙ = f(x˜,u) + L(t)(y − g(x˜)). (7.28)638 Chapter 7. Observers for Nonlinear Systems
This observer is referred to as the extended Kalman filter.
The design of L(t) is based on the well-known design equation (7.25) of
the linear Kalman filter
L(t) = P (t)C
T
(t)S
−1
,
which is now time-dependent. The matrix P (t) is again calculated from the
Riccati equation (7.26)
A(t)P (t) + P (t)AT
(t) − P (t)C
T
(t)S
−1C(t)P (t) = −Q,
which is now time-dependent too. The matrices A(t) and C(t) are the result
of linearizations, i. e. Taylor series expansions which are terminated after the
first derivative. So we obtain
A(t) = ∂f
∂x




x˜(t)
and C(t) = ∂g
∂x




x˜(t)
at the current estimated point x˜(t).
In practice, the algebraic Riccati equation above is solved using the Riccati
differential equation
P˙(t) = A(t)P (t) + P (t)AT
(t) + Q − P (t)C
T
(t)S
−1C(t)P (t) (7.29)
as in the linear case. The covariance matrix
P (0) = cov{x0 − x˜0, x0 − x˜0} = E{(x0 − x˜0)(x0 − x˜0)
T
}
of the initial estimation error
e0 = x0 − x˜0, with x0 = x(0), x˜0 = x˜(0)
is used as the initial value for the matrix P (t) we are attempting to determine.
Note that the Riccati differential equation (7.29) is not solved offline to
determine the stationary value of P . This would not be possible because A(t)
and C(t) vary continuously. Thus, the estimation equation (7.28) and the
Riccati differential equation (7.29) are solved simultaneously; the Jacobian
matrices A(t) and C(t) must also be calculated continuously. This dynamic
adjustment becomes apparent in the block diagram of the extended Kalman
filter in Figure 7.10.
The stability and accuracy of the estimation generally cannot be guaran￾teed for the extended Kalman filter [32, 39, 53]; they have to be evaluated
using simulations. The downside of this is that no general statement can be
made and the stability and the accuracy of the estimation only hold for the
specific simulations which have been carried out.
Designing the Kalman filter, i. e. choosing suitable values of S and Q,
involves trial and error and requires experience. In summary, for the system
(7.27) being observed, the equations of the extended Kalman filter are given
by7.5. The Extended Kalman Filter 639
x(0)
x˜(0)
y
y˜
y − y˜
x˜
x˜
L
L
P
v
C
A
x˙ = f(x, u) + µ
y = g(x) + ρ
P˙ =AP + P AT + Q
−P CT S
−1C P
A =
∂f
∂x




x˜,u
C =
∂g
∂x




x˜
L = P CT S
−1
x˜˙ = f(x˜, u) + v
y˜ = g(x˜)
u
u
System
Model
Extended Kalman filter
Fig. 7.10: System being observed and extended Kalman filter
x˜˙ = f(x˜,u) + L(y − g(x˜)),
A =
∂f
∂x




x˜(t),u(t)
,
C =
∂g
∂x




x˜(t)
,
P˙ = AP + P AT + Q − P CTS
−1CP ,
L = P CTS
−1
.
(7.30)
The complete structure of the extended Kalman filter and the mutual depen￾dencies of the individual equations are shown in Figure 7.10. Here the bold
arrows represent the forwarding of matrices.640 Chapter 7. Observers for Nonlinear Systems
With the exception of very simple cases, practical implementation requires
solving the Riccati differential equation using a numerical integration method
[323], such as the Runge-Kutta method. In principle, this also applies to the
estimation equation. Attempting to solve the Riccati differential equation nu￾merically can cause difficulties.
7.5.3 Example: Jet Engine
Consider a jet engine such as those used in commercial aircraft. The most
common type is the turbofan or fanjet. As shown in Figure 7.11, it has an
internal and an external airflow, which are also termed core and bypass stream.
The core stream is compressed by a compressor consisting of multiple blade
wheels which are consecutively arranged. The example shown in Figure 7.11
depicts five blade wheels. The compressed air is then mixed with kerosene
and this mixture is ignited behind the compressor. The resulting combustion
gases drive a gas turbine, which in this case has three blade wheels. It in turn
drives the compressor and the fan, which consists of the large blade wheel
located in front of the compressor and is connected to the gas turbine via a
common shaft. The fan sucks in air and generates the core stream and the
bypass stream, which is bypassed around the compressor and the turbine. The
bypass stream amplifies the thrust generated by the core stream such that it
causes approximately 80% of the engine thrust in commercial aircraft engines.
Unstable flow conditions can occur in the compressor due to a stall or
fluctuations in thrust. In the worst case of this scenario, the engine may be
damaged due to a flame leak or a reversal of the mass flow, among other
reasons. This can be avoided by using a controller, for example.
Thrust
Bypass
stream
Core
stream
Combustion
chamber
Φ
Fan
Fig. 7.11: Jet engine7.5. The Extended Kalman Filter 641
The conditions in the compressor are described by the Moore-Greitzer
equations [250, 299]
Φ˙ = −Ψ + Ψco + 1 +
3
2
Φ −
1
2
Φ
3 − 3ΦR,
Ψ˙ =
1
β
2
(Φ − γ
√
Ψ + 1),
R˙ = σR(1 − Φ
2 − R).
(7.31)
Here, Φ represents the mass flow through the compressor, Ψ is the pressure
rise, R ≥ 0 is a measure of the stall, and β, σ, and Ψco are constants. The
variable γ acts as the control variable u. It can represent bleed air, which is
taken from the core or bypass stream and is fed into the compressor in such
a way that neither a flow stall nor a reversal of the direction of the mass flow
occurs.
The aim is to stabilize the engine, meaning its compressor, at the equilib￾rium point
Req = 0, Φeq = 1 and Ψeq = Ψco + 2 (7.32)
at
γ = 2/
p
Ψco + 2.
This can be done with a nonlinear state controller. However, all three state
variables Φ, Ψ, and R are required for a controller of this type. The pressure
increase Ψ is measurable, but the quantities Φ and R are not. Therefore, they
must be estimated using an observer.
Before we design an extended Kalman filter as an observer, we will trans￾form system (7.31) so that the equilibrium point (7.32) is at the origin. For
this purpose, we will introduce the new state variables
x1 = Φ − 1,
x2 = Ψ − Ψco − 2,
x3 = R
and thus obtain the transformed system equations


x˙ 1
x˙ 2
x˙ 3

 = f(x, u) =





−x2 −
3
2
x
2
1 −
1
2
x
3
1 − 3x1x3 − 3x3
1
β
2

x1 − u
p
x2 + Ψco + 2 + 2
−σx2
3 − σx3(2x1 + x
2
1
)





(7.33)
using the control variable u = γ. The output variable y is a measure of the
pressure rise Ψ and is given by
y = x2 = Ψ − Ψco − 2.
At the equilibrium point xeq = 0, the control variable is642 Chapter 7. Observers for Nonlinear Systems
ueq =
2
√
Ψco + 2
.
We will now linearize system (7.33) around the estimated state x˜, resulting
in
A(t)= ∂f
∂x




x˜,u
=







−3˜x1 −
3
2
x˜
2
1 − 3˜x3 −1 −3˜x1 − 3
β
−2 − β
−2u
2
p
(˜x2 + Ψco + 2)
0
−2σx˜3(1 + ˜x1) 0 −2σx˜3 − σ(2˜x1 + ˜x
2
1
)







and
C(t) = ∂g
∂x




x˜
=
∂x2
∂x




x˜
= [0 1 0].
The linearized system matrix A(t), the linearized output vector C(t), and the
nonlinear system dynamics f(x, u) are now inserted into equations (7.30) of
the extended Kalman filter. For the observer, we thus obtain
x˜˙ = f(x˜, u) + L(y − x˜2),
P˙ = AP + P AT + Q − P CTS
−1CP ,
L = P CTS
−1
.
Note that C is a 1 × 3 matrix, i. e. a row vector, and L is a 3 × 1 matrix, i. e.
a column vector.
For the covariance matrices Q and S, with S being scalar in this case, we
select
Q =


0.45 1.15 0.88
1.15 8.65 1.77
0.88 1.77 3.04

, S = 0.12.
The system parameters are
Ψco = 0.72, σ = 4, and β = 0.71.
Figure 7.12 shows the progression of the original state variables Φ, Ψ, and R
for the initial state vector
[Φ(0) Ψ(0) R(0)] = [1.5 1.2 0.5]
of the plant and for the initial state vector
[Φ˜(0) Ψ˜(0) R˜(0)] = [0.5 0.2 0]
of the Kalman filter with the constant control variable
u = ueq = 2/
√
2.72.
At t = 0, the matrix P (t) is the identity matrix.7.6. High-Gain Observer 643
1.5
0.5
4
2
1
0
0
0.4
0.2
0
0
0
1
1
1
2
2
2
3
3
3
4
4
4
5
5
5
6
6
6
7
7
7
8
8
8
9
9
9
10
10
10
State variable
Estimated state variable
Pressure rise
Ψ Stall
R Mass flow
Φ
Time t in s
Fig. 7.12: Time courses of the real and estimated mass flow Φ, pressure increase Ψ,
and stall R of the flow
7.6 High-Gain Observer
7.6.1 Concept and Design
In order to explain the operating principle of a high-gain observer, we will
first assume that the nonlinear SISO system
x˙ = f(x, u),
y = g(x)
is formulated in the nonlinear observability canonical form
z˙ =







z2
z3
.
.
.
zn
ϕ(z, u, u, . . . , u ˙
(n−1))







,
y = z1,
(7.34)
or has been transformed into this form by means of644 Chapter 7. Observers for Nonlinear Systems
z = q(x, u, u, . . . , u ˙
(n−1)).
In the state-space model (7.34), the function ϕ describes the nonlinearities of
the system.
The system description (7.34) is often also given in the form
z˙ = Az + bϕ(z, u, u, . . . , u ˙
(n−1)),
y = c
T
z,
(7.35)
where
A =







0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
0 0 0 · · · 0







, b =







0
0
.
.
.
0
1







, c
T =

1 0 · · · 0

. (7.36)
The linear system description
z˙ = Az + b · uˆ
with the matrix A and the vector b from equation (7.36) is, as we already
know, referred to as the Brunovsky canonical form. It consists of n integrators
arranged in series. Since the output variable is
y = z1 = c
T
z,
the nonlinear observability canonical form is always observable, as is directly
apparent from the integration chain.
Much like the Luenberger observer for linear systems, the high-gain ob￾server for the above system is set to
z˜˙ = Az˜ + bϕ(z˜, u, u, . . . , u ˙
(n−1)) + ℓ(ε)
￾
y − c
T
z˜

(7.37)
with the estimation vector z˜ or, using y˜ = c
T
z˜, to
z˜˙ = Az˜ + bϕ(z˜, u, u, . . . , u ˙
(n−1)) + ℓ(ε)(y − y˜). (7.38)
Here,
ℓ(ε) =





ε
−1
0 · · · 0
0 ε
−2
· · · 0
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · ε
−n










l1
l2
.
.
.
ln





= D−1
(ε) · l
is the observer gain, which can be adjusted by its parameter ǫ > 0. The matrix
D−1
(ε) is a diagonal matrix which has the values ε
−i > 0 as elements dii. In
contrast to ℓ(ε), the vecto7.6. High-Gain Observer 645
x(0)
x˜(0)
y
System
u
y − y˜
y˜
z˜
Observer
ℓ(ε)
v = ℓ(ε)
￾
y − y˜

z˙ = Az + bϕ(z, u)
y = c
T
z
z˜˙ = Az˜+bϕ(z˜, u)+v
y˜ = c
T
z˜
Fig. 7.13: Structure of the high-gain observer using the abbreviated notation
ϕ(z˜, u) = ϕ(z˜, u, u, . . . , u ˙
(n−1) )
l = [l1 l2 · · · ln]
T
is constant but can be freely selected to influence the observer’s dynamic
behavior. The structure of the high-gain observer for system (7.35) is shown
in Figure 7.13.
Based on the estimation error
e = z − z˜,
the description of system (7.35), and observer (7.38), the error dynamics of
the observer are given by
e˙ =
￾
A−ℓ(ε)c
T

e +b

ϕ(z, u, u, . . . , u ˙
(n−1))−ϕ(z˜,u,u, . . . ,u ˙
(n−1))

= (A−D−1
(ε)lcT)e +b

ϕ(z,u,u, . . . ,u ˙
(n−1))−ϕ(z˜,u,u, . . . ,u ˙
(n−1))

. (7.39)
If we first assume that
ϕ(z˜, u, u, . . . , u ˙
(n−1)) = ϕ(z, u, u, . . . , u ˙
(n−1))
holds, the error dynamics are linear, i. e.
e˙ =
￾
A − D−1
(ε)lcT

e.
To calculate the eigenvalues of the observer’s system mat646 Chapter 7. Observers for Nonlinear Systems
F˜(ε) = A − D−1
(ε)lcT =







−l1ε
−1
1 · · · 0
−l2ε
−2
0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
−ln−1ε
−(n−1) 0 · · · 1
−lnε
−n
0 · · · 0







,
we determine its characteristic polynomial
P(s) = s
n +
l1
ε
s
n−1 +
l2
ε
2
s
n−2 + . . . +
ln−1
ε
n−1
s +
ln
ε
n
=

s −
λ1
ε
 s −
λ2
ε

. . . 
s −
λn
ε

.
Depending on ε, the eigenvalues
λ˜
i =
λi
ε
of F˜(ε) thus move along rays which emanate from the origin of the complex
plane. The real parts of the eigenvalues λ˜
i become smaller with decreasing ε,
which means that they shift more and more to the left side of the complex
plane. Eventually, the limit
limε→0
Re {λ˜
i} = limε→0
Re {λi}
ε
= −∞
applies, provided that all eigenvalues λi are chosen such that Re {λi} < 0
holds. Figure 7.14 illustrates this dependence of the eigenvalues λ˜
i on ε. For
the case in question,
ϕ(z˜, u, u, . . . , u ˙
(n−1)) = ϕ(z, u, u, . . . , u ˙
(n−1)),
the estimation error e converges to zero due to the linear error dynamics. The
convergence becomes more rapid the smaller we choose the parameter ε.
In the following, we will analyze the case
ϕ(z˜, u, u, . . . , u ˙
(n−1)) 6= ϕ(z, u, u, . . . , u ˙
(n−1)),
starting with a transformation of the estimation error e and selecting
e=





e1
e2
.
.
.
en





=





eˆ1
ε
−1
eˆ2
.
.
.
ε
−(n−1) eˆn





=





1 0 · · · 0
0 ε
−1
· · · 0
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · ε
−(n−1)





eˆ = εD−1
(ε)eˆ
as the transformation rule. Applying this, we obtain7.6. High-Gain Observer 647
Re
Im
ε → ∞
ε → 0
λ˜1
λ˜2
Fig. 7.14: Course of the eigenvalues depending on ǫ
εD−1
(ε)eˆ˙ =
￾
A − D−1
(ε)lcT

· εD−1
(ε)eˆ
+ b

ϕ(z, u, u, . . . , u ˙
(n−1)) − ϕ(z˜, u, u, . . . , u ˙
(n−1))

for the error equation (7.39) of the observer. After multiplication by D(ε),
this results in
εeˆ˙ =
￾
εD(ε)AD−1
(ε) − lcT
· εD−1
(ε)

eˆ (7.40)
+ D(ε)b

ϕ(z, u, u, . . . , u ˙
(n−1)) − ϕ(z˜, u, u, . . . , u ˙
(n−1))

.
Because
εD(ε)AD−1
(ε) = ε





ε 0 · · · 0
0 ε
2
· · · 0
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · ε
n










0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0










ε
−1
0 · · · 0
0 ε
−2
· · · 0
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · ε
−n





= A,
lcT
· εD−1
(ε) = ε





l1 0 · · · 0
l2 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
ln 0 · · · 0










ε
−1
0 · · · 0
0 ε
−2
· · · 0
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · ε
−n





= lcT
,
and
D(ε)b = ε
n
b
hold, we obtain the transformed error dynamics
εeˆ˙ = (A − lcT
)eˆ+ε
n
b

ϕ(z, u, u, . . . , u ˙
(n−1))−ϕ(z˜, u, u, . . . , u ˙
(n−1))

(7.4648 Chapter 7. Observers for Nonlinear Systems
for equation (7.40).
Now, the time t is rescaled according to
τ = ε
−1
· t,
resulting in a new time variable denoted by τ. This yields
eˆ˙ =
deˆ
dt =
deˆ
εdτ
and
deˆ
dτ = (A − lcT
)eˆ+ ε
n
b

ϕ(z, u, u, . . . , u ˙
(n−1)) − ϕ(z˜, u, u, . . . , u ˙
(n−1))

for equation (7.41). From this equation, it is directly apparent that the smaller
ε is, the less important the nonlinear term
ε
n
b

ϕ(z, u, u, . . . , u ˙
(n−1)) − ϕ(z˜, u, u, . . . , u ˙
(n−1))

becomes. In contrast to this, the linear part of the error dynamics of the
high-gain observer
deˆ
dτ = (A − lcT
)eˆ
has a constant system matrix
F = A − lcT =







−l1 1 0 · · · 0
−l2 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
−ln−1 0 0 · · · 1
−ln 0 0 · · · 0







,
whose eigenvalues λi are arbitrarily selectable via l1, . . . , ln, and is dominant.
If a sufficiently small value of ε is selected, the observer will be asymptotically
stable, and the estimation error eˆ and thus
e = z − z˜
both converge to zero. Consequently, the estimated value z˜ will converge to
the system state z. The term high-gain observer results from the fact that for
small values of ε, the elements of the observer gain
ℓ(ε) = D−1
(ε)l
assume large values. In this context, we note that the eigenvalues λ˜
i = λi/ε of
the high-gain observer with the system matrix F˜(ε) are larger by a factor of
1/ε than the eigenvalues of the transformed observer with the system matrix
F.7.6. High-Gain Observer 649
7.6.2 High-Gain Observers in General Form
In the preceding section, we explained the design of high-gain observers for
systems which were already formulated in or were transformed into nonlinear
observability canonical form. In practice, however, the system description will
rarely be in this canonical form. For us to design a high-gain observer, we
must transform the system
x˙ = f(x, u),
y = g(x)
(7.42)
into the observability canonical form (7.34) using mapping (7.4) from Sec￾tion 7.1.5, p. 614, i. e. by means of the diffeomorphism
z = q(x, u, u, . . . , u ˙
(n−1)). (7.43)
This cannot always be easily done. Note that state transformations of the
above type were already discussed in Section 3.3.1, p. 259 et seq.
Let us assume that we have succeeded in designing a high-gain observer
(7.38) for the system in canonical form, as described in the previous section.
One way to obtain the original state vector x˜ being observed is to formulate
the observer in such a way that its states are consistent with the original
coordinates x of system (7.42). To this end, the diffeomorphism (7.43) must
be inserted into the observer equation (7.38), i. e. into
z˜˙ = Az˜ + bϕ(z˜, u, u, . . . , u ˙
(n−1)) + ℓ(ε)(y − y˜). (7.44)
Using the diffeomorphism
z˜ = q(x˜, u, u, . . . , u ˙
(n−1)), (7.45)
we obtain
z˜˙ =
dq(x˜, u, u, . . . , u ˙
(n−1))
dt =
∂q
∂x˜
x˜˙ +
nX−1
i=0
∂q
∂u(i)
u
(i+1)
, (7.46)
where the vector x˜ denotes the estimate of the vector x. Inserting equation
(7.46) into equation (7.44) yields
∂q
∂x˜
x˜˙ +
nX−1
i=0
∂q
∂u(i)
u
(i+1) = Az˜ + bϕ(z˜, u, u, . . . , u ˙
(n−1)) + ℓ(ε)(y − y˜),
from which
x˜˙ =

∂q
∂x˜
−1
 
Az˜ + bϕ(z˜, u, u, . . . , u ˙
(n−1)) −
nX−1
i=0
∂q
∂u(i)
u
(i+1)!
+

∂q
∂x˜
−1
ℓ(ε)(y − y˜)
(7.47)650 Chapter 7. Observers for Nonlinear Systems
follows. Transforming the plant model
x˜˙ = f(x˜, u)
of the observer using the diffeomorphism (7.45) leads to

∂q
∂x˜
−1
 
Az˜ + bϕ(z˜, u, u, . . . , u ˙
(n−1)) −
nX−1
i=0
∂q
∂u(i)
u
(i+1)!
= f(x˜, u).
Inserting this into equation (7.47), we obtain the general equation
x˜˙ = f(x˜, u) + 
∂q(x˜, u, u, . . . , u ˙
(n−1))
∂x˜
−1
ℓ(ε)(y − y˜) (7.48)
for the high-gain observer. Note that
y˜ = c
T
z˜ = g(x˜)
holds.
The observer not only requires the measured variable y and the control
variable u; it also requires the derivatives u, . . . , u ˙
(n−1) which are also argu￾ments of the vector function q. Figure 7.15 shows the structure of the observer.
The design of an observer according to equation (7.44) or equation (7.48)
normally requires calculating the transformation
x(0)
x˜(0)
u y
y˜
v
Observer
System
v =
 
∂q
∂x˜
!−1
ℓ(ε)(y − y˜)
x˙ = f(x, u)
y = g(x)
x˜ = f(x˜, u) + v
y˜ = g(x˜)
x˜
Fig. 7.15: General high-gain observer using the abbreviated notation q for
q(x˜, u, u, . . . , u ˙
(n−1))7.6. High-Gain Observer 651
z = q(x, u, u, . . . , u ˙
(n−1))
or the corresponding inverse transformation q
−1
. In practice, this can be a
very challenging task.
In principle, there are two ways to implement a high-gain observer. On the
one hand, the observer equation (7.44), the input variable u, and the output
variable y can be used to determine the estimated state z˜ of the transformed
system (7.35). The actual estimation vector x˜ is calculated by applying the
inverse transformation formula
x˜ = q
−1
(z˜, u, u, . . . , u ˙
(n−1)).
On the other hand, the observer can also be realized in the original coor￾dinates x˜ using the general equation (7.48) of the high-gain observer. In any
case, as mentioned earlier, the crux of the method lies in the handling of the
function q. This is because either the inverse function q
−1
or the inverse of
the Jacobian matrix ∂q/∂x˜ must be determined. Both are often difficult.
7.6.3 Example: Chemical Reactor
An example is a stirred tank reactor [107, 229]. This reactor is continuously
supplied with substance A at a volume rate of qin. Substance A has the input
concentration c0 and the input temperature T0. In the reactor, the substance
is catalytically decomposed, releasing heat. For this reason, the reactor is
equipped with a cooling jacket through which cooling water flows at the tem￾perature T c. In the reactor, an agitator ensures that mixing is homogeneous
at all times. The decomposition products finally leave the reactor at a volume
rate qout. In the reactor, the mixture has the temperature T , and the concen￾tration of substance A in the mixture is denoted by c. Figure 7.16 illustrates
the process.
The reactor can be modeled by a second-order nonlinear system. The state
variables are defined as the normalized quantities
x1 =
c0 − c
c0
and x2 =
T − T0
T0
.
In contrast to the normalized temperature x2, the state variable x1 cannot be
measured. The input or control variable u is the normalized difference between
the coolant temperature T c and the input temperature T0, i. e.
u =
T c − T0
T0
.
The model equations are
x˙ = f(x, u),
y = x2
(7.49)652 Chapter 7. Observers for Nonlinear Systems
qout
T, c
qin, c0, T0
Outflowing
cooling water
Inflowing
cooling water
with T c
Fig. 7.16: Chemical reactor
with
f(x, u) = "
−ax1 + k(1 − x1)e
−α/(1+x2)
−βx2 + kd(1 − x1)e
−α/(1+x2) + bu #
and the parameters
c0 = 0.848 mol l−1
, T0 = 308.5 K,
k = 1.05 · 1014 min−1
, α = 34.2583,
a = 0.2674 min−1
, β = 1.815 min−1
,
d = 0.4682, b = 1.5476 min−1
.
The time unit is assumed to be one minute.
The first step is determining the diffeomorphism
z = q

x, u, u, . . . , u ˙
(n−1)
,
which transforms system (7.49) into the nonlinear observability canonical
form. This yields7.6. High-Gain Observer 653
z =

z1
z2

=

y
y˙

=

x2
−βx2 + kd(1 − x1)e
−α/(1+x2) + bu
= q(x, u). (7.50)
From this, the relation
x =

x1
x2

=
"
1 −
1
kd(z2 + βz1 − bu)e
α/(1+z1)
z1
#
= q
−1
(z, u) (7.51)
follows for q
−1
. Note that q
−1
in equation (7.51) is defined for all
z1 = x2 6= −1.
Because T > 0 and thus x2 > −1, the system can be considered observable
for all relevant values of x and z. To calculate the nonlinear observability
canonical form, equation (7.51) is inserted into equation (7.49), resulting in

z˙1
z˙2

=

z2
ϕ(z, u, u˙)

,
y = c
T
z, c
T = [1 0],
with
ϕ(z, u, u˙) = α · z2(z2 + β · z1 − bu)
(1 + z1)
2
−

a + β + ke−α/(1+z1)

z2
− k(β · z1 − bu − ad)e
−α/(1+z1) − a(β · z1 − bu) + bu.˙
In this way, according to equations (7.44) and (7.36), it is possible to
directly determine the high-gain observer for the chemical reactor as
z˜˙ =

z˜2
ϕ(z˜, u, u˙)

+ ℓ(ε)(y − y˜). (7.52)
By retransforming the observer dynamics (7.52) via equation (7.50) or by
calculating them using the general equation (7.48), we obtain the observer
dynamics in the original coordinates
x˜˙ = f(x˜, u) + 
∂q
∂x˜
−1
ℓ(ε)(y − c
T
q(x˜, u))
= f(x˜, u) + 
r11 r12
1 0 
ℓ(ε)(y − y˜)
using the acronyms
r11 = −
β(1 + ˜x2)
2
e
α/(1+˜x2) + kdα(˜x1 − 1)
kd(1 + ˜x2)
2
,654 Chapter 7. Observers for Nonlinear Systems
r12 = −
e
α/(1+˜x2)
kd .
The observer is dimensioned using
ε= 0.1, λ1 =−1.69, and λ2 =−2.6.
We assume the initial state vector
x
T
(0) = [0.504 0.02]
for the simulation shown in Figure 7.17. Since we can measure the temperature
x2, we will set x˜2(0) = x2(0) = 0.02. In contrast to x2, we cannot measure
x1. Having no accurate knowledge of the reactor’s initial values, we will select
x˜1(0) = 0 which yields
x˜
T
(0) = [0 0.02].
The input value is
u(t) = −0.01.
Figure 7.17 shows the concentration x1, the temperature x2, and the estimated
values x˜1 and x˜2. The estimated state variable x˜1 reaches the actual value of
the state variable x1 after about 25 seconds. As is to be expected, there is
almost no difference in the progression between the measured value x2 and
the estimated value x˜2 after this time period.
1
0.8
0.6
0.4
0
0
0.3
0.2
0.2
0.1
0
0
2
2
4
4
6
6
8
8
10
10
12
12
State variable
Estimated state variable
Temperature
x2 Concentration
x1
Time t in min
Fig. 7.17: Progressions of the observed and true states of the chemical reactor, i. e.
the normalized concentration x1 and the normalized temperature x27.6. High-Gain Observer 655
7.6.4 The Case of Control-Affine Systems
For the design of high-gain observers, we had previously assumed that the
plant is in the nonlinear observability canonical form
x˙ =







x2
x3
.
.
.
xn
ϕ(x, u, u, . . . , u ˙
(n−1))







,
y = x1
(7.53)
or can be transformed into this canonical form. As has been mentioned sev￾eral times, this will only rarely be the case. Fortunately, many control-affine
systems
x˙ = a(x) + b(x) · u,
y = c(x)
can be bijectively transformed into the form of equation (7.53) by means of a
transformation
z = q(x) = t(x). (7.54)
This transformation was already discussed in Section 5.2, p. 359 et seq. and
in Section 7.1.6, p. 617 et seq. For the sake of clarity and convenience, we will
allow ourselves to repeat the derivation of the transformation equation (7.54)
in the following. Note that the transformation t in Section 5.2 is a special case
of the transformation q in Section 7.1.4, p. 612.
The mapping t is determined by means of the Lie derivative
Lac(x) = ∂c(x)
∂x
a(x)
and the multiple Lie derivatives
L
k
ac(x) = LaL
k−1
a c(x)
=
∂Lk−1
a c(x)
∂x
a(x).
Now, the output function
y = c(x)
will be differentiated multiple times with respect to time t, yielding the equa￾tions656 Chapter 7. Observers for Nonlinear Systems
y =c(x),
y˙ =
∂c (x)
∂x
x˙ =
∂c (x)
∂x
a(x) + ∂c (x)
∂x
b(x)u = Lac(x) + Lbc(x)
| {z }
= 0
u,
y¨ =
∂Lac(x)
∂x
x˙ =
∂Lac(x)
∂x
a(x)+ ∂Lac(x)
∂x
b(x)u=L
2
a
c(x)+LbLac(x)
| {z }
= 0
u,
.
.
.
y
(δ−1)=
∂Lδ−2
a
c(x)
∂x
x˙ = L
δ−1
a
c(x) + LbL
δ−2
a
c(x)
| {z }
= 0
u,
y
(δ)=
∂Lδ−1
a
c(x)
∂x
x˙ = L
δ
ac(x) + LbL
δ−1
a c(x)
| {z }
6= 0
u.
In the process above, the output value y is differentiated until the term which
is multiplied by the control variable u becomes
LbL
δ−1
a
c(x) 6= 0
for the first time. As mentioned previously, the above sequence of derivatives
corresponds to that calculated in controller design using input-output lin￾earization, as described in Section 5.2. The order δ of the derivative, at which
LbL
δ−1
a
c(x) 6= 0 holds for the first time, is called the relative degree δ.
In the following, we will only address systems in which the relative degree
δ is equal to the system order n, i. e. δ = n. The sequence of derivatives
discussed previously takes the form
y = c(x),
y˙ = Lac(x),
y¨ = L
2
ac(x),
.
.
.
y
(n−1) = L
n−1
a c(x),
y
(n) = L
n
ac(x) + LbL
n−1
a c(x) · u .
(7.55)
New state variables
z =







z1
z2
z3
.
.
.
zn







=







y
y˙
y¨
.
.
.
y
(n−1)







=







c(x)
Lac(x)
L
2
ac(x)
.
.
.
L
n−1
a
c(x)







= t(x) (7.56)7.6. High-Gain Observer 657
are now defined, and the mapping t(x) we wish to calculate is directly ob￾tained. Along with equations (7.55) and (7.56), the system description we are
attempting to determine follows as





z˙1
.
.
.
z˙n−1
z˙n





=





z2
.
.
.
zn
L
n
ac(x) + LbL
n−1
a c(x) · u





,
y = z1 .
Now the original state x in the last row must be replaced by z using
x = t
−1
(z).
So we finally obtain





z˙1
.
.
.
z˙n−1
z˙n





=





z2
.
.
.
zn
ϕ(z, u)





,
y = z1
(7.57)
with
ϕ(z, u) = L
n
ac(t
−1
(z)) + LbL
n−1
a c(t
−1
(z)) · u.
We can formulate equation (7.57) as
z˙ = Az + b · ϕ(z, u),
y = c
T
z,
where
A =







0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 1
0 0 0 · · · 0







, b =







0
0
.
.
.
0
1







, c
T =

1 0 · · · 0

.
The associated high-gain observer
z˜˙ = Az˜ + bϕ(z˜, u) + ℓ(ε)
￾
y − c
T
z˜

(7.58)
can be transformed using
z˜ = t(x˜) (7.59)
so that it is formulated in the coordinates of the original syste658 Chapter 7. Observers for Nonlinear Systems
x˙ = a(x) + b(x) · u,
y = c(x).
(7.60)
Inserting equation (7.59) into equation (7.58), we obtain
dt(x˜)
dt = A t(x˜) + b ϕ(t(x˜), u) + ℓ(ε)
￾
y − c
T
t(x˜)

and, consequently,
∂t(x˜)
∂x˜
x˜˙ = A t(x˜) + b ϕ(t(x˜), u) + ℓ(ε) (y − c(x˜))
for the estimation vector x˜ in the original coordinates. In the last equation,
we multiply by the inverse of the Jacobian matrix ∂t(x)/∂x from the left.
Thus, we obtain the equation of the high-gain observer
x˜˙ = a(x˜) + b(x˜) · u +

∂t(x˜)
∂x˜
−1
ℓ(ε) (y − y˜) (7.61)
in the original coordinates, where y˜ = c(x˜) applies. Figure 7.18 shows the
structure of the high-gain observer for the original system (7.60). Equation
(7.61) also immediately follows from the general equation (7.48) of the high￾gain observer.
Finally, note that high-gain observers can also be designed if the relative
degree δ is smaller than the system order n, i. e. if δ < n holds. For more on
this topic, see [208, 387].
u
x(0)
x˜(0)
System
y
y˜
v
x˙ = a(x) + b(x) · u
y = c(x)
x˜˙ = a(x˜) + b(x˜) · u + v
y˜ = c(x˜)
v=

∂t(x˜)
∂x˜
−1
ℓ(ε)(y−y˜)
x˜ Observer
Fig. 7.18: Structure of the high-gain observer for control-affine system7.7. Exercises 659
7.7 Exercises
Exercise 7.1 Show that a linear system
x˙ = Ax + bu,
y = c
T x
with the relative degree δ = n is always observable.
Exercise 7.2 Let us examine the satellite model (1.8) from p. 8. Let
Jx < Jy and Jx < Jz
apply. For the torques, we will assume
Mx = My = Mz = 0.
Due to a malfunction in the satellite’s sensor system, we can only measure ωx.
In which cases is it possible to prove that the satellite is weakly observable
using Theorem 87 on p. 609?
Exercise 7.3 The dynamics of the glucose and insulin concentrations in dia￾betes patients can be described in a simplified way by the model [38, 242]
x˙ 1 = 0.028x1 − x2(x1 + 110),
x˙ 2 = −0.025x2 + 0.00013x3,
x˙ 3 = 0.093(x3 − 1.5) + 0.0083u,
y = x1.
Here, x1 is the glucose concentration in the blood plasma in milligrams per
deciliter (mg/dL), x2 is the insulin concentration in the tissue fluids in milli￾units per deciliter (mU/dL), x3 is the insulin concentration in the blood
plasma in mU/dL, and u is the intravenously injected amount of insulin in
mU/min.
(a) Determine whether the diabetes model is weakly observable or even ob￾servable.
(b) Is the type of observability found in (a) globally valid for x ∈ Dx,def = IR3
or only locally valid?
Exercise 7.4 Show that the strict feedback form
x˙ 1 = f1(x1) + h1(x1)x2,
x˙ 2 = f2(x1, x2) + h2(x1, x2)x3,
x˙ 3 = f3(x1, x2, x3) + h3(x1, x2, x3)x4,
.
.
.
x˙ n = fn(x1, . . . , xn) + hn(x1, . . . , xn)u,
y = x1
is observable and state the sufficient conditions under which this applies.660 Chapter 7. Observers for Nonlinear Systems
Exercise 7.5 Demonstrate that a system
x˙ 1 = x2 + b1(x1)u
x˙ 2 = x3 + b2(x1, x2)u
x˙ 3 = x4 + b3(x1, x2, x3)u
.
.
.
x˙ n−1 = xn + bn−1(x1, . . . , xn−1)u
x˙ n = α(x) + β(x)u
y = x1
is observable for all x ∈ IRn
and u ∈ C
n−1
.
Exercise 7.6 Let us examine the system [48]
x˙ 1 = 1,
x˙ 2 = x
2
1
,
y = x2
with x ∈ Dx = IR2
.
(a) Show that with the diffeomorphism
z =

y
y˙

= q(x)
the observability of the system via Theorem 86 on p. 608 is not provable
for all x ∈ Dx,def = IR2
.
(b) Demonstrate that with the addition of y¨, the state vector x for all x ∈ IR2
can be calculated from the knowledge of y and y¨, and that the system is
therefore globally observable.
(c) Which property of Theorem 86 on p. 608 becomes clearly evident with
the above results?
Exercise 7.7 Let us examine the system
x˙ 1 = x2 − x1 + u,
x˙ 2 = −x
3
1
,
y = x1.
(a) Determine the diffeomorphism
x = q(z)
which transforms the system into the nonlinear observer canonical form.
(b) Formulate the system in the nonlinear observer canonical form.7.7. Exercises 661
(c) Design a canonical form observer in z-coordinates whose error dynamics
have the eigenvalues s1,2 = −10.
(d) Transform the canonical form observer into the original coordinates x.
Exercise 7.8 Show that a system in nonlinear controllability canonical form
and y = xn can be transformed into the nonlinear controller canonical form
with β(x) = 1.
Exercise 7.9 Cable-operated elevators in very tall buildings involve a number
of problems, such as cable elongation and vertical oscillations. These problems
can be avoided by using cableless elevators which are operated by an electrical
linear motor. The elevator cage runs along tracks without touching them; these
tracks are part of the linear motor and also serve to maintain the contactless
distancing of the elevator cage from the tracks. Here, the distance x1 between
the elevator cage and the track is maintained using electromagnets mounted
in the elevator cage. Figure 7.19 shows the general design.
The system’s state variables are the distance x1, the derivative x2 = ˙x1 of
the distance, and the magnetic flux density x3 of the magnets. The simplified
x1
Fig. 7.19: Cableless elevator662 Chapter 7. Observers for Nonlinear Systems
system dynamics are given by the differential equations
x˙ 1 = x2,
x˙ 2 = −a

x3
x1
2
,
x˙ 3 = −cx3 + d
x2x3
x
2
1
+ bu,
y = x1.
Here, the inductor voltage u is the input variable. The constants a, b, c, and
d are parameters dependent on the construction.
(a) To calculate an observer
x˜˙ = f(x˜, u) + L(x˜, u) · (y − y˜)
using linearization, first determine the characteristic polynomial P(s, x˜, u)
of the matrix
F(x˜, u) = A(x˜, u) − L(x˜, u)C(x˜)
of the error dynamics.
(b) Which conditions must the matrix L(x˜, u) fulfill for the characteristic
polynomial P(s, x˜, u) of the matrix F(x˜, u) to be dependent on neither x˜
or u?
Exercise 7.10 We will now attempt to design a high-gain observer for the
pneumatic engine
x˙ =







x2
−
c
m
x1 +
A
m
x3
−
x2x3
x1







+







0
0
x3
Ax1







u,
y = x1
which we encountered in equation (5.165) in Section 5.4.6, p. 455.
(a) Show that the system is globally observable for all x ∈ IR3
.
(b) Transform the system into the nonlinear observability canonical form.
(c) Calculate a high-gain observer in the original coordinates x˜.8
Solutions to the Exercises
In this chapter the solutions to the exercises are presented. Due to the extent
of the calculations, the solutions to the exercises do not contain any calculation
steps, but only the final results. For the same reason, where a proof is to be
given or an assumption is to be verified, the proof or the solution method is
not included here.
1.1 yy¨ − y˙
2 + 2 ˙y
2
y
3 = 2 ˙yyu
1.2 Equilibrium points: xeq1 = 0, xeq2 =

1 0T
, xeq3 =

0 0.5
T
1.3 (a) The system has no equilibrium points.
The system has the set of equilibrium points E ={x ∈ IR2
| x1=0, x2 ∈ IR}
and the infinite number of equilibrium points x
T
eq,k =kπ
1 ±1

, k∈Z.
(b)
xeq1=

0 0T
, xeq2 =
√
6 0T
, xeq3 =

−
√
6 0T
(c)
xeq1=[0 0]T
, xeq2=
h
−5−
√
6 − 5−
√
6
iT
, xeq3 =
h
−5+√
6 − 5+√
6
iT
(d)
xeq=

0 0T
(e) xeq =

0 0T
(f)
The system has the equilibrium point xeq1 = 0 and the set E = {x∈IR2
|
x
2
1 + x
2
2 = 1} of equilibrium points.
(g)
xeq1,2 =

1
2
(
√
5 − 1) ±
r
1
2
(
√
5 − 1) 0 T
, xeq3,4 =

0 0 ±1
T
(h)
1.4 (a)
xeq1 =







N
α
µ
0
0







, xeq2 =









N
γ + µ
rβ
N
rβα − µ(γ + µ)
rβ(γ + µ)
N γ rβα − µ(γ + µ)
µrβ(γ + µ)









© Springer-Verlag GmbH Germany, part of Springer Nature 2024
J. Adamy, Nonlinear Systems and Controls,
https://doi.org/10.1007/978-3-662-68690-4_8
663664 Chapter 8. Solutions to the Exercises
The equilibrium point xeq1 is reached if no pathogens are present in the
population. The equilibrium point xeq2 represents the case of illness.
(b)
x˙ 2 =
rβ
N
(c) (N−x2)x2 xeq1=

0 N
T
, xeq2 =

N 0
T
(d)
x2(t)= x2(0)N
(N −x2(0))e
−rβt + x2(0)
(e) lim
t→∞
(f) x2(t) = N
At the end of an incurable infectious illness such as AIDS, the entire
population will be infected in the case of the simplified model. This is not
realistic, because in reality non-infected newborns are born and infected
patients die. These factors must be taken into account in the model for
the situation in reality to be described more accurately.
(g)
1.5 (d) GROA =

x ∈ IR2
| x
3
10x
2
20 6= 5	
(e) GRAS =

x ∈ IR2
| x
3
10x
2
20 <5
	
1.6 (a)
i(uC1)=



m2uC1−
ud
R3
, uC1≥u0,
m1uC1, |uC1|<u0,
m2uC1+
ud
R3
, uC1≤−u0,
u0 =
R2
R2+R3
ud,
with m1 =−
1
R1
,
m2 =−
1
R1
+
1
R2
+
1
R3
,
u˙ C1 =−
uC1
RC1
+
uC2
RC1
−
1
C1
i(uC1), u˙ C2=
uC1
RC2
−
uC2
RC2
+
iL
C2
,
˙iL =−
uC2
L
(b) Point-symmetric polygon line with bends at two points
(c) x˙ 1 = α (−x1 + x2 − g(x1))
x˙ 2 = x1 − x2 + x3
x˙ 3 = −βx2
with g(x1) =



bx1 + a − b, x1 ≥ 1,
ax1, |x1| < 1,
bx1 + b − a, x1 ≤ −1.
(d) The state variables x1, x2, and x3 become dimensionless as a result of the
normalization. Thus the equations become generally applicable and no
longer apply only to the electric circuit; in this form they can be applied
to other projects as well, such as mechanical ones.
1.7 (a)
xeq1 =0, xeq2 =

b − a
b + 1
0
a − b
b + 1 T
, xeq3=

a − b
b + 1
0
b − a
b + 1T
(b) The system, which is the normalized version of Chua’s circuit, consists of
three subsystems, each with linear dynamics. Each of these subsystems
has one of the three equilibrium points. The characteristic polynomials
for the subsystems are
Pa(s) = s
3 −
9
35
s
2 +
173
35
s −
132
7
for xeq1 and
Pb(s) = s
3 +
123
35
s
2 +
61
7
s +
264
7
for xeq2 and xeq3.665
Using the Routh criterion, we determine that both polynomials and thus
all three equilibrium points are unstable.
(c) The system is chaotic.
1.8 The output signal y1 is a square wave; y2 is a triangular wave.
1.9 (a) The output signal is shown in Figure 8.1 below.
10 20
3
2
1
-1
-2
-3
y(t)
t
Fig. 8.1: Output signal y(t) from solution 1.9
(b) The remaining control error for yref = 3 is e∞ = 1; for yref = −3 it is
e∞ = −1.
1.10 (a) λ1/2 = ±1
(b) The paths of the trajectories shown in Figure 8.2 are described by
x2 = ±
p
(x1 + 1)2 + c1, c1 = x
2
2
(0) − (x1(0) + 1)2
, for x1 < 0,
x2 = ±
p
(x1 − 1)2 + c2, c2 = x
2
2
(0) − (x1(0) − 1)2
, for x1 > 0.
-1.5
-1
-0.5
0
1.5
1
0.5
-2 -1.5 -1 -0.5 0 0.5 1 1.5 -2
State x1
State
x2
Fig. 8.2: Trajectories of the control loop; red trajectories for u = 1 and blue ones
for u = −1666 Chapter 8. Solutions to the Exercises
Because x˙ 1 = x2 applies, all trajectories in the upper half-plane tend to
the right and those in the lower half-plane to the left.
(c) There are three equilibrium points in the system. The equilibrium point
xeq1 =0 is Lyapunov stable, but not asymptotically stable; xeq2 =

−1 0T
and xeq3 =

1 0T
are strictly unstable.
(d) Stable trajectories exist only for the initial vectors
x(0) ∈ {x ∈ IR | |x1| + |x2| ≤ 1}.
1.11 (a)
k= 4: 
x˙ 1
x˙ 2

=

0 1
−4 0 x1
x2

, and k= 0.25: 
x˙ 1
x˙ 2

=

0 1
−0.25 0 x1
x2

(b) The eigenvalues λ1,2 = ±j
√
k of the two control loops are λ1,2 = ±j2 and
λ1,2 = ±j0.5. Thus the equilibrium point xeq = 0 is Lyapunov stable in
both cases, but not asymptotically stable.
(c)
x(t) =


x2(0)
√
k
sin(√
kt)
x2(0) cos(√
kt)

 for k = 4 and k = 0.25
The trajectories are ellipses
x
2
1
x
2
2
(0)k−1
+
x2
x
2
2
(0) = 1
with semiaxes a = x20/
√
k and b = x20, as shown in Figure 8.3.
(d) There is a single equilibrium point at x = 0, which is asymptotically
stable. Figure 8.4 shows an example trajectory.
(e) There is a single equilibrium point at x = 0, which is strictly unstable.
An example trajectory is shown in Figure 8.5.
k = 0.25 k = 4
x2 x2
x1 x1
Fig. 8.3: Trajectories of the two subsystems667
x2
k = 0.25 k = 4
k = 4 k = 0.25
x1
Fig. 8.4: An example trajectory of the
stable control loop
x2
x1
k = 4 k = 0.25
k = 0.25 k = 4
Fig. 8.5: An example trajectory of the
unstable control loop
1.13 (a)
x
T
eq1 =

a3+b2u2
a4
a1−b1u1
a2

, xeq2 =0,
and for the case in which a1 = b1u1 : x
T
eq3 = [x1 ∈IR 0]
(b) (∆x)˙ = A∆x + B∆u,
A=





0 −
a2
a4

a3+
b2
2

a4
a2

a1−
b1
2

0





, B =





−
b1
a4

a3+
b2
2

0
0 −
b2
a2

a1−
b1
2






(c) λ1,2 = ±j
p
(a1 − 0.5b1)(a3 + 0.5b2) (d) Yes
1.14 (a)
V˙
1 =

z −
5
9

V˙
in,
V˙
2 =

4
9
− z

V˙
in,
z =
(
1 if V1 = 0.1, or V˙
1 > 0 and V2 > 0.1,
0 if V2 = 0.1, or V˙
2 > 0 and V1 > 0.1.
(b)
∆ti = 4
4
5
i−1
, i= 1, 2, . . .
(c)
ti= 20"
1−

4
5
i
#
(d)
ttot =
X∞
i=0
∆ti = 20
(e) The more the time approaches the time ttot = 20 at which both silos reach
a volume of V1 = V2 = 0.1V , the shorter the duration ∆ti of the switching
intervals become. This means that the switching frequency increases and
eventually becomes infinitely high for t → ttot. The control obviously can￾not cope with this, because each element of the control system, including
the swiveling crane, has a limited switching speed. For practical reasons
as well, high-frequency switching back and forth is undesirable in this
situation. The effect of the increasingly shorter switching intervals whose
duration ∆ti becomes infinitely small for t → ttot is referred to as Zeno668 Chapter 8. Solutions to the Exercises
behavior, named after the Greek philosopher Zeno and his paradox of the
race between Achilles and the tortoise.
(f) If we simulate a system exhibiting Zeno behavior, an integration procedure
with a fixed step size will of necessity become inexact, and one with a
step-size adaptation will never attain a time t ≥ ttot, because the step
size becomes infinitely small for t → ttot.
1.15 (a) z˙ = (1 − α)(p(t)z + q(t)) (b) Linear differential equation
1.16 x(t) = bx(0)
x(0) + (b − x(0))e−at (logistic function)
1.17 x(t) = h
−1

h(x(0)) + Z
t
0
g(τ)dτ
, h(x) = Z
1
f(x)
dx
1.18 (a) x(t) = t
n
(b) x(t) = p3
(x
3(0) + 1)e−2t − 1
(c) x(t) = x(0)
x(0) + (1 − x(0))e
t
(d) x(t) = sin(1
3
t
3 + arcsin(x(0)))
(e) x(t) = x(0) + cosh(t + arsinh( ˙x(0))) − cosh(arsinh( ˙x(0))
1.19 (c) teq = 2p
|x0|
1.20 (a) We obtain mi = qi
, because lg(ε
rel) = q lg(h) + lg(˜α) follows from
ε
rel = ˜αhq
. See also Table 8.1.
(b) hi =
mi
q
ε
rel/α˜i
, see also Table 8.1. (c) Ni =ni
· 10/hi
, see also Table 8.1.
(d) The selection of the integration method does not depend only on its or￾der of accuracy. It also depends on the calculation effort, i. e. on the time
required for the simulation. In terms of both accuracy and the time re￾quired, useful approaches are the 4th-order Runge-Kutta method and the
5th-order Adams-Moulton procedure.
Table 8.1: Parameters for the Euler method (1), modified Euler method (2), Simp￾son method (3), Runge-Kutta method (4), Adams-Bashforth method (5), Adams￾Moulton method with one iteration (6), and the Adams-Moulton method with 10
iterations (7).
Number of the procedure i 1 2 3 4 5 6 7
Order of accuracy qi 1 2 3 4 4 5 5
Slope of the error curve mi 1 2 3 4 4 5 5
Constant α˜i 480 190 46 10 460 120 20
Step size hi for ε = 10−6
2 ·10−9
7 ·10−5
3 ·10−3
2 ·10−2
7 ·10−3
2 ·10−2
3 ·10−2
Function calculations ni per step 1 2 3 4 1 2 11
Total function calculations Ni 4.8 ·109
2.8 ·105
1.1 · 104
2.2 ·103
1.5 ·103
8.3 · 102
3.2 ·103
1.21 See Table 1.3 on p. 54. In the case of the Gear method, the stability
range of the step size h is hλ < ∞.669
2.1 (a),(b) See Table 2.1, p. 78 et seq.
(c)
N(A) = m2 +
2
π
(m1 − m2)

arcsin a
A

+
a
A
r
1 −
a
2
A2

, A > a
(d)
N(A) = 4b
πA
X
k
i=1
s
1 −

2i − 1
2A
a
2
,
2k − 1
2
a < A < 2k + 1
2
a, k ∈ IN
2.2 (a)
N(A)=k1 +
3k2
4
A
2 +
5k3
8
A
4
(b)
N(A)=Xn
i=1
ki

A
2
2(i−1)
2i−1
i−1

2.3 (a) G(s) = s
2 + s + 15
s
3 + 9.8s
2 + 15s + 132
(b) Chua’s circuit is shown in Figure 8.6.
e
−g(e) −G(s)
x1
x1
Fig. 8.6: Chua’s circuit represented as a nonlinear standard control loop
(c)
N(A) = 5
7
+
6
7π
"
arcsin 
1
A

+
1
A
r
1 −
1
A2
#
, A > a = 1
(d) The nonlinear locus −1/N(A) for A = a = 1 starts at −7/8 on the
negative real axis and follows the negative real axis for A → ∞ to −7/5.
The Nyquist plot of −G(jω) intersects the real axis in several places; the
furthest to the left is at
Re {G(j3.6196)} = −0.5268.
This means no point of intersection exists between the plots of −G(jω)
and −1/N(A), and thus there is probably no limit cycle either.
2.4 b < 2πa
2.5 (a)
N(A)=



0, A≤a,
m

1−
2
π
arcsin
a
A

−
2
π
·
a
A
r
1−
a
2
A2

, a<A≤b,
2m
π

arcsin
b
A

−arcsin
a
A

+
b
A
r
1−
b
2
A2
−
a
A
r
1−
a
2
A2

, b<A,670 Chapter 8. Solutions to the Exercises
Im
Re
A = ∞ A = b
A = a −
1
N(b)
Fig. 8.7: Nonlinear locus
with m = c/(b − a). Figure 8.7 shows the nonlinear locus −1/N(A). Its
turning point is −1/N(b), where
N(b) = m

1 −
2
π
arcsin a
b

−
2
π
a
b
r
1 −
a
2
b
2


is reached at A = b.
(b) ω1 = ω2 = 2, A1 = 2.475, A2 = 4.505 (c) Ks = 0.8568
(d) For K > 0.8568 we obtain a stable and an unstable limit cycle; for K <
0.8568 there is no limit cycle.
2.6 (a)
e f(e) u
k G(s)H(s)
−
Fig. 8.8: Blood-pressure control loop
f(e) = b arctan(me)
k G(s)H(s) = k
e
−(τe+τa)s
1 + τνs
=
1.65e
−2.99s
1 + 5s
(b) N(A) = 2b
πAZπ
0
arctan(mA sin(α)) sin(α)dα=
2b
πAZπ
0
mA cos2
(α)
1+m2A2 sin2
(α)
dα
=
2bm
π
Zπ
0
1
1+m2A2 sin2
(α)
dα−
2b
πmA2
Zπ
0

1−
1
1+m2A2 sin2
(α)

dα
=
2b
mA2
p
m2A2 + 1 − 1

(c) One single limit cycle exists with ω = 0.2π, i. e. f = 0.1 Hz.
(d) The limit cycle is stable.
2.7 (a) The nonlinear curve can lie in the sector [0, 45.5).
(b) A linear curve (proportional control) can lie in the sector (−81, 140.5).
(c) |e| < 6.74671
2.8 (a) T I=Te = 0.2 s, KP =
c
125
(b) c==
π
12
, KP ==
π
1500
2.9 (a) [0, 27.94] (b) [0, 60)
(c) Aizerman’s conjecture is correct in this case and leads to the sector
(−6, 60).
2.10 (a) The Nyquist plot of G(jω) is shown in Figure 8.9.
(b) Circle with its midpoint on the real axis between −3 and −5
(c) Midpoint c = −4.39, radius r = 0.23, sector [0.22, 0.24]
(d) Hurwitz sector: (0.2, 0.3) Im
Re
-5 -4.393 -3 -0.8946
-0.2315
0.2315
-0.3474
0.3474
Fig. 8.9: Nyquist plot
2.11 (a) Ω = π
(b)
[K1, K2] = " √
8
√
7 + √
8
+ ǫ,
√
8
√
8 −
√
7
− ǫ
#
with arbitrarily small ǫ > 0
(c) Hurwitz sector: (0, ∞)
2.12 Solution for G1(s): (a) The plant has two poles at s = 0, i. e. µ = 2. Thus
we use the general circle criterion, Theorem 10. The continuous angle
variation is Ω = π. The sector of absolute stability [K1, K2] is dependent
on the circle selected, as we can see from Figure 8.10. (b) Hurwitz sector:
(0, ∞)
Solution for G2(s): (a) Using Theorem 9, we obtain (1, ∞) as a sector of ab￾solute stability, because ν = 2 (2 unstable poles) and the circular Nyquist
plot of G2(jω) encircles each circle D (1/(1 − ǫ), 1/ε), 0.5 ≥ ε > 0, twice
counterclockwise. See Figure 8.11. (b) Hurwitz sector: K ∈ (1, ∞)
Solution for G3(s): (a) The general circle criterion, Theorem 10, must be ap￾plied, because there are three poles s1,2,3 = 0. No sector of absolute sta￾bility can be identified using the criterion, because the continuous angle
variation for all circles with K1 > 0, K2 > 0 takes the value Ω = π/2. See
Figure 8.12. However, to prove absolute stability, Ω = νπ + µπ/2 = 3π/2
would have to apply. See Figure 8.12. (b) There is no Hurwitz sector, i. e.
the plant cannot be stabilized by a P controller. That is why there is no
sector of absolute stability at all.672 Chapter 8. Solutions to the Exercises
ω=−∞
ω=∞
ω= 0
ω= 0
Re
Im
G1(jω)
Fig. 8.10: Nyquist plot of
G1(jω)
Re
Im
1
-1
-1 -0.5
ω=−2
ω=2
G2(jω)
ω= 1±
√
5
ω=−1±
√
5
ω=−∞
ω=0
ω=∞
Fig. 8.11: Nyquist plot
of G2(jω)
Im
Re
-
1
K
ω=−∞
ω=∞
ω=0
ω=0 G3(jω)
Fig. 8.12: Nyquist
plot of G3(jω)
2.14 (a) xeq = 0 (b) The equilibrium point xeq = 0 is unstable.
2.15 The only equilibrium point, which is [xeq x˙ eq]
T = 0, is globally Lyapunov
stable. Thus the system itself is globally Lyapunov stable.
2.16 (c) G = {x ∈ IR2
| x
2
1 + x
2
2 < 4}.
2.17 (b) The contour lines of V (x) are circles.
(c) Figure 8.13 shows the plot of the Lyapunov function V (x) for x2 = 0.
800
600
400
200
0
V (x1)
x1
-50 0 50
Fig. 8.13: Plot of the rotationally symmetric Lyapunov function V (x)
(d) The trajectories are circles, because the system is a harmonic oscillator.
2.18 (a)
x˙ =

−4 − 15
−4 − 23
x −

1
1

(3x1 + 21x2)
3
(b) There is a single equilibrium point at xeq = [0, 0]T
.
2.19 (a) Equilibrium points: xeq1 =0 and all xeq∈

x∈IR3
| x
2
1+x
2
2+x
2
3 = 1	
(b) 1. With V (x)=x
Tx, V˙ (x)>0 applies for all x∈

x∈IR3
| x
2
1+x
2
2+x
2
3 <1
	
.
This means that xeq1 = 0 is unstable (instability theorem, p. 142).673
2. With
V LaSalle(x)= (x
2
1+x
2
2+x
2
3−1)2
,
V˙
LaSalle(x) = 0 applies for
x∈M =N =

x∈IR3
| x
2
1+x
2
2+x
2
3 = 1	
.
In addition, V˙
LaSalle(x) < 0 applies for all x ∈ IR3
\{M ∪ {0}}. This
means that all equilibrium points xeq,i ∈ M are Lyapunov stable and all
trajectories with the exception of x(t) = 0 tends to M.
2.21 (a) S(ω) = 0.5
￾
Jxω
2
x + Jyω
2
y + Jzω
2
z

(b) The controller matrix K must be positive definite.
(c) As new coordinates, select the angular momentum values Lx = Jxωx,
Ly =Jyωy, and Lz =Jzωz. Use the storage function
H(L) = 1
2
 
L
2
x
Jx
+
L
2
y
Jy
+
L
2
z
Jz
!
= S(ω).
The PCH model is
L˙ = J(L)
∂H(L)
∂L
+ M with J(L) =


0 −Lz Ly
Lz 0 −Lx
−Ly Lx 0

.
(d) The control law is M(L) = −D
∂H(L)
∂L
, with D being positive definite.
2.22 A PID controller is passive, but not strictly passive.
2.24 (a) strictly positive real (b) strictly passive
(c) All characteristic curves in the sector [0, ∞] result in a control loop with
an asymptotically stable equilibrium point.
(d) Based on Aizerman’s conjecture, the largest sector of absolute stability
corresponds to the Hurwitz sector, which is (−1, ∞] in the case of the
system considered.
2.25 P · Re {G(jω)} ≥ − I
ω
Im {G(jω)}
2.26 (a) L = Ekin − Epot =
1
2
ml2ϕ˙
2 − mgl(1 − cos(ϕ))
(b) d
dt
∂L
∂ϕ˙
−
∂L
∂ϕ = mlϕ¨ + mlg sin(ϕ) = 0 ⇒ lϕ¨ + g sin(ϕ) = 0
(c) H(q, p) = p
2
2ml2
+ mgl(1 − cos(q)) with q = ϕ and p = ml2ϕ˙
(d) q˙ =
p
ml2
, p˙ = −mglsin(q)
3.1 The solution to the differential equation x˙ = xu is
x(t) = x(0)e
R t
0
u(τ)dτ
674 Chapter 8. Solutions to the Exercises
Depending on the sign of the initial value x(0), x(t) is either positive for
all t ∈ IR or it is negative. Thus the system is not globally controllable.
However, it is locally controllable for x(0) > 0 on IR+ and for x(0) < 0 on
IR−. It is not controllable if x(0) = 0.
3.2 The system is not controllable, because the state variable x3 cannot be
influenced by the control variable u.
3.3 (a) 

x˙ 1
x˙ 2
x˙ 3

 =


cos(x3) − sin(x3) 0
sin(x3) cos(x3) 0
0 0 1




v1
v2
v3

 = A(x) · v
(c) The overall model

x˙
v˙

=

A(x)v
−M−1Dv
+

0
M−1

u
is not omnidirectionally controllable. It is controllable, however, because
it is flat.
(d) A flat output is y = x. The flat system description is

x
v

=

y
A−1
(y)y˙

, A
−1
(y) =


cos(y3) sin(y3) 0
− sin(y3) cos(y3) 0
0 0 1

 ,
u = MA−1
(y)


y¨1 + ˙y2y˙3
y¨2 − y˙1y˙3
y¨3

 + DA−1
(y)y˙.
3.5 (a) The fuel cell model is locally omnidirectionally controllable for
x ∈

x ∈ IR3
| x1 6= p and x2 6= p and x3 6=
2
3
p

.
(b) The fuel cell model is not globally omnidirectionally controllable.
(c) The fuel cell model is locally small-time locally controllable for
x ∈

x ∈ IR3
| x1 6= p and x2 6= p
	
.
(d) The fuel cell model is not globally small-time locally controllable.
3.6 (a)
z =


y
y˙
y¨

=


x1
f1(x1) + h1(x1)x2
µ(x1, x2) + h1(x1)h2(x1, x2)x3


µ(x1, x2)=[f1(x1)+h1(x1)x2]

∂f1(x1)
∂x1
+ x2
∂h1(x1)
∂x1

+h1(x1)f2(x1, x2)
(b) b(z)=[0 0 h1(x1)h2(x1, x2)h3(x1, x2, x3)]T
with x1 =z1, x2 =
z2 − f1(z1)
h1(z1)
, x3 =
z3 − µ(z1, x2)
h1(z1)h2(z1, x2)675
(c) The system is controllable if the inequalities h1(x1) 6= 0, h2(x1, x2) 6= 0,
and h3(x1, x2, x3)6= 0 hold.
3.7 (a) Flat output: y = x1; x =

y y˙
T
, u = ¨y + y (b) u = 1 − 2te−t
3.8 (a)
y=


ωx
ωy
ωz

, u =


Mx
My
Mz

=


Jxy˙1 + (Jz − Jy)y2y3
Jyy˙2 + (Jx − Jz)y1y3
Jzy˙3 + (Jy − Jx)y1y2


(b)
y =

x1
x2

, x=




y1
y2
arctan(y˙2
y˙1
)




, u=




y˙1y¨2 − y˙2y¨1
y˙
2
1 + ˙y
2
2
q
y˙
2
1 + ˙y
2
2




(c)
y=


x1
x2
x3

, u=


0 0 1
cos(y3) sin(y3) 0
− sin(y3) cos(y3) 0

y˙
3.9 (a) Flat output: y=

y1 y2
T
=

x1 x3
T
(b) Flat system representation:
x=




y1
Jy˙2 + dy2 + ML
ky1
y2




, u =

ur
us

,
ur =Lry˙1 + Rry1 + y2
Jy˙2 + dy2 + ML
y1
,
us =
J(y1y¨2 − y˙1y˙2) + d(y1y˙2 − y˙1y2) − MLy˙1
ky2
1
+
Rs
Ls
Jy˙2 + dy2 + ML
ky1
3.10 (a) x1 =
1
mc
y +
ms
mcct
y¨ −
ms
mc
xd, x2 =
1
mc
y˙ +
ms
mcct
˙˙˙y −
ms
mc
x˙ d,
x3 = −
1
ct
y¨ + xd, x4 = −
1
ct
˙˙˙y + ˙xd,
u =
m
ct
y
(4) + ¨y + Fs(x1, x3) + Fd(x2, x4) − msx¨d
(b) The system is controllable, because flat systems are controllable.
(c) The transformation z = t(x) and the transformed system z˙ = t˙(x) with
x = t
−1
(z), which is in the nonlinear controller canonical form, are
z = t(x) =




mcx1 + msx3
mcx2 + msx4
−ctx3 + ctxd
−ctx4 + ctx˙ d




, z˙ =




z2
z3
z4
α(z)




+




0
0
0
β




u
with676 Chapter 8. Solutions to the Exercises
α(z)=−
ct
ms
h
z3+Fˆ
s(z)+Fˆ
d(z)−msx¨d
i
, β =
ct
ms
,
Fˆ
s(z)=csg(z)+cnlsg
3
(z), g(z)= z1
mc
+
ms+mc
mc

z3
ct
−xd

,
Fˆ
d(z)=d·h(z)+dnlh
2
(z) sgn(h(z)), h(z)= z2
mc
+
ms+mc
mc

z4
ct
−x˙ d

.
3.11
x = q
−1
(z) =



ln q
z
2
1 + z
2
2

arctan ￾ z2
z1




3.12 (a) x = p(z) = z
−1
(b) x(t) = x0
x0 + (1 − x0)e−t
3.13 (a)
x=

z1
arcsin(z2)

=q
−1
(z)
(b)
z˙ =
"
z2
−z1
q
1 − z
2
2
#
+
"
q
0
1 − z
2
2
#
u
(c) The transformed system description z˙ = f(z, u) takes the nonlinear con￾troller canonical form.
3.14 (a) Local diffeomorphism: z =
√3 x, Mz = IR, Mx = IR; although the
diffeomorphism bijectively maps IRn
into IRn
it is not a global diffeomorphism,
since z =
√3 x is not continuously differentiable at x = 0.
(b) Global diffeomorphism if T ∈ IRn×n
is regular;
z = T
−1x − T
−1a, Mz = IRn
, Mx = IRn
.
(c) Local diffeomorphism:
z =




q
x
2
1 + x
2
2
κ(x2) arccos ￾ x1
p
x
2
1 + x
2
2





, κ(x2) = (
1, x2 ≥ 0,
−1, x2 < 0,
Mz = {z ∈ IR | z1 6= 0, z2 ∈ (−π, π]}, Mx = IR2
\ {0}.
(d) Global diffeomorphism; the inverse is not analytically determinable.
(e) Not a diffeomorphism
(f) Local diffeomorphism if z1 6= 0 and z1 < R;
z=












s
x
2
3+
q
x
2
1+x
2
2−R
2
κ(x3) arccos p
x
2
1 + x
2
2 − R
q
x
2
3+
￾p
x
2
1 + x
2
2 −R
2
!
κ(x2) arccos x1
p
x
2
1 + x
2
2













, κ(xi)=(
1, xi ≥ 0,
−1, xi <677
Mz =

z ∈ IR3
| 0 < z1 < R, −π < z2 ≤ π, −π < z3 ≤ π
	
,
Mx =
(
x ∈ IR3
| 0 < x2
3 +
q
x
2
1 + x
2
2 − R
2
< R2
)
.
3.15 (a) z˙1 = z1(1 − z1), z˙2 = sin2
(0.5z2)
(b) The system has two equilibrium points at zeq1 = 0 and zeq2 = [1 0]T
.
Both are unstable, because all values z1 with 1 > z1 > 0 in a neighborhood
of zeq1 lead to z˙1 > 0 and, at zeq2, arbitrarily small values z2 > 0 lead to
z˙2 > 0. In addition, the radius z1 is constant for z1 = 1, because z˙1 = 0.
This yields a circular trajectory if z˙2 > 0. This trajectory tends to the
equilibrium point zeq2. From outside, i.e. z1 > 1, all trajectories tend to
the unit circle, because z˙1 < 0 applies here. From within, for z1 < 1 all
trajectories also tend to the unit circle, because z˙1 > 0 applies.
3.16 (a) z˙ = 2 sgn(z) (b) x(t) = sgn(x(0))p
2t + x
2(0)
(c) Both diffeomorphisms transform the nonlinear differential equation into
a linear differential equation – but into two different ones. So it may be
that there are several diffeomorphisms linearizing a differential equation.
(d) It is both a Bernoulli differential equation and a separable one.
4.1 (a)
Q(s) = NPID(s)
s + mKI
, R(s) = NPID(s)N(s) − mKID(s)
(s + mKI)N(s)
,
G(s)R(s) = NPID(s)N(s) − mKID(s)
(s + mKI)D(s)
(b) G(s)GPI(s) = 2s + 1
s
2
, G(s)R(s) = −998s + 1
s(s + 1000)
(c) Both criteria are applicable, provided that excessively high values of the
saturation block’s input variable cannot occur. This is always the case in
real world applications. In this context, see the remarks concerning the
modified saturation characteristic in Section 2.2.4 on p. 99 et seq.
(d) G(jω)GPI(jω)=−
1
ω2
−j
2
ω
, G(jω)R(jω)=−
998·103+1
ω2+106
+j
998ω
2−103
ω(ω2+106)
(e) For the control loop without anti-windup, i. e. the plant’s transfer function
is G(s)GPI(s), we can identify no sector of absolute stability which proves
stability, because the Popov criterion returns no result and the general
circle criterion yields no circles whose corresponding sectors of absolute
stability contain the saturation curve u = sat(v). This can be seen in
Figure 8.14 for the circle criterion and in the Figure 8.16 for the Popov
criterion.
In the case of the control loop with anti-windup, i. e. the plant’s transfer
function is G(s)R(s), there is a circle with an infinite radius which inter￾sects the point −1/k2 = −1/1.0020. This yields the sector [ǫ, 1.0020] of678 Chapter 8. Solutions to the Exercises
absolute stability. Figure 8.15 illustrates this. The Popov criterion yields
the same result, as can be seen in Figure 8.17.
ω=∞
ω= 0
Re
Im
Ω = π
5
-5
-5
G(jω)GPI(jω)
Fig. 8.14: Nyquist plot of G(s)GPI(s)
Im
Re
ω=∞
−0.9980
G(jω)R(jω)
-1
ω= 0
Ω =
π
2
Fig. 8.15: Nyquist plot of G(s)R(s)
Re
Im
-2
ω= 0 ω=∞
Fig. 8.16: Popov plot of G(s)GPI(s)
Im
Re
ω → ∞ ⇒ 0 + j998
−0.9980−j0.001 -1
ω=∞
ω= 0
Fig. 8.17: Popov plot of G(s)R(s)
(f) The Hurwitz sectors for the control loop with and without anti-windup
are [ǫ, 1000/998 ≈ 1.0020] and [ǫ, ∞), respectively, where 0 < ǫ ≪ 1. Aiz￾erman’s conjecture is true in both cases.
4.3 (a) x1(t) = α
2umax
￾
x
2
2
(t) − x
2
2
(0)
, α = ±1, x1(0) = 0
(b) x1(t) = x
2
2
(t)
2umax
for x2 < 0 and x1(t) = −
x
2
2
(t)
2umax
for x2 > 0
(c)
u(x) =



umax, x1 < S(x2),
−umax, x1 > S(x2), S(x2) = −
x2|x2|
2umax
− sgn (x2)umax, x1 = S(x2),
4.4 Xnull =

x ∈ IR | |x| <
umax|b|
λ
679
4.5 (a) t1 =
|x(0)|
umax
, u(x) = −umax sgn(x)
(b) t1 = −
1
λ
ln 
1 −
λ|x(0)|
umaxb

, u(x) = −umax sgn(x);
for λ < 0 all initial values x(0) ∈ IR can be compensated to zero, for λ > 0
only initial values x(0) restricted by |x(0)| < umaxb/λ can be compensated
to zero.
4.6 u=−umax sgn(S(x2)−x1), S(x2)=−
x2
λ
+
x2+umax sgn(x2)
λ
ln
1+
|x2|
umax
4.7 u=−umax sgn
aˆ|aˆ|+
ˆb
3
umax

,
aˆ=x1+
x
3
3
3umax
2
+
x2x3
umax
sgn
x2+
x3|x3|
2umax

,
ˆb=x2+
x
2
3
2umax
sgn
x2+
x3|x3|
2umax
4.8 (a) 
˙ia
˙iv

=

−1.2988 3.4199
46.7567 178.7697 ia
iv

+

37.1085
−1335.9062
ua,
z =

−575 · 10−6 − 16 · 10−6


ia
iv

Eigenvalues: λ1 = 179.6534, λ2 = −2.1825
(b)
i =

−0.1389 −28.4326
−7.3468 7.3468
x
(c) u=umax sgn(S(x2) − x1), S(x2)=umax sgn(x2)
"

1 +
|x2|
umax
λ1/λ2
− 1
#
(d) G =

x ∈ IR2
| |x1| < umax, x2 ∈ IR	
4.9 (a)
uto(x) =



umax, x1 < S(x2),
−umax, x1 > S(x2),
sgn (x1)umax, x1 = S(x2),
S(x2) = −
x2|x2|
2umax
= umaxh
sgn(S(x2) − x1) + sgn(x1)

1 − |sgn(S(x2) − x1)|
i
(b) u = −

1 2
x
(c) Using Barbashin and Krasovskii’s theorem on p. 116, we obtain
V (0) = 0, V (x) = x
2
1 + x
2
2 > 0 for x 6= 0, and V˙ (x) = −4x
2
2 ≤ 0.
The set of states for which V˙ (x) = 0 applies (except for x(t) = 0) is
M = {x ∈ IR2
| x2 = 0, x1 ∈ IR\{0}}. No trajectory x(t) of the linear
control loop lies within M, because a trajectory of this kind would have
to take the form680 Chapter 8. Solutions to the Exercises
xM (t) = 
x1(t)
0

but would not fulfill the differential equation
x˙ =

0 1
−1 −2

x
of the linear control loop.
(d) The disc with radius 0.1, shown in Figure 8.18, indicates the region in
which the linear controller u = −k
T x is activated. Outside the disc, the
time-optimal controller regulates the plant.
x2
u=−umax
-0.1
u=−k
T
x
0.1
Switching curve
x1 = S(x2)
x1
u=umax
Fig. 8.18: The dual-mode controller illustrated in the phase plane
(e) All trajectories of the time-optimal control loop which start outside the
disc tend to it. After entering the disc, the linear controller is active such
that all these trajectories tend to the origin. The reason for this is that
the disc is a catchment region. Thus all trajectories are asymptotically
stable.
(f) In practice, all control loops have output and state noise. Thus, when
the equilibrium point is reached and afterwards, the time-optimal control
switches continually back and forth between umax and −umax. The linear
controller in the area around the equilibrium point makes it possible to
avoid this switching.
4.10 (a) x(k)=a
k
x(0) +X
k−1
i=0
a
k−i−1
bu(i)
(c) N= ceil￾ ln(bumax) − ln(|x(0)|(1−a)+bumax)
ln(a)

(d)
u(i) =



−umax sgn(x(0)), 0 ≤ i ≤ N − 2
umax sgn (x(0))a − a
N
1 − a
−
a
N
b
x(0), i = N − 1
0, i ≥ 681
4.11 (b) The switching system is globally asymptotically stable if and only if
the matrix e
A2∆te
A1∆t has only eigenvalues λi with |λi
| < 1.
(c) For ∆t = ln(1.4) the system is stable; for ∆t = ln(3) it is unstable.
4.12 (a) u1 = 2, u2 = −1
(b) No values u1, u2 exist for the case in which t1 = π, t2 = 2π.
(c) In order for there to be a control sequence u1, . . . , un which transforms one
arbitrary state to another, it is necessary and sufficient for the matrix
M =


Z
t1
0
e
−Aτ
bdτ Z
t2
t1
e
−Aτ
bdτ · · · Z
tN
tN−1
e
−Aτ
bdτ


to have rank n.
4.13 (f) Design the procedure of a piecewise linear control:
Step 1: Choose eigenvalues λ1i and thus the coefficients aˆi of the control
loop’s characteristic polynomial.
Step 2: Determine the matrix R1 from
Aˆ
T
1 R1 + R1Aˆ1 = −Q
for a positive definite matrix Q.
Step 3: Check whether all catchment regions
Gi =

x ∈ IRn
| e(q)x
T D−1
(q)R1D−1
(q)x ≤ 1
	
are nested within one another. This is the case if
∂e(q)
∂q ≤ 0
and
NR1 + R1N = −S
is negative definite.
Step 4: Select l controllers ki = k(qi), i = 1, 2, . . . , l.
(g) Control law:
u = −k
T
(q)x = −
￾
D−1
(q)aˆ − a

x
Implicit equation for calculating q:
e(q)x
T D−1
(q)R1D−1
(q)x = 1
Calculate R1 by satisfying
Aˆ
T
1 R1 + R1Aˆ1 = −Q, NR1 + R1N = −S, and ∂e(q)
∂q ≤ 0
simultaneously.
(h) For the parameter q, the following applies:
1. For x → 0, the volume of G tends to 0; thus q does as well. Therefore
q(x = 0) = 0 applies.
2. For all x ∈ IRn
\ {0}, q > 0682 Chapter 8. Solutions to the Exercises
3. The selected parameter q decreases as the size of the catchment regions
Gi becomes smaller. Because all Gi are nested within one another
and all trajectories of larger catchment regions tend to smaller ones,
q decreases along each trajectory. Thus q <˙ 0 applies.
From 1, 2, and 3 above, it follows that q is a Lyapunov function.
4.14 The catchment region G lies between the hyperplanes |h
T x| = umax.
Consequently, the asymptotically stable regulation with u = − sat(h
T x) is
linear for x ∈ G.
4.15 (a) λ1,2 = ±j (b) h
T = [0 2] (c) R =

3 1
1 1
(d) G = {x ∈ IR2
| x
T Rx < 6} (e) k
T = [3 4]
(f) In accordance with Hu and Lin’s theorem, the equilibrium point xeq = 0
is asymptotically stable for x ∈ G, because the matrices Qh
and Qk
are
both positive definite.
5.1 (a) x˙ = −
1
2
ρcdA
m
x
2 − g(cr cos(ϕ) + sin(ϕ)) + u, y = x
(b) (∆x)˙ = −
ρcdA
m
β∆x + ∆u = −0.0005β∆x + ∆u, β = x, ∆y = ∆x
(c) xop1 = 10 ms−1
: uop1 = 0.17215, (∆x)˙ = −0.005∆x + ∆u
xop2 = 25 ms−1
: uop2 = 0.3034, (∆x)˙ = −0.0125∆x + ∆u
xop3 = 40 ms−1
: uop3 = 0.54715, (∆x)˙ = −0.02∆x + ∆u
P1 =0.095, I1 = 0.0025, P2 = 0.2375, I2 = 0.015625, P3 = 0.38, I3 = 0.04
(d)
u =
P
3
i=1
(uop,i + ∆ui)e
−(∆x2
i
)/(2σ
2
i
)
P
3
i=1
e
−(∆x2
i
)/(2σ
2
i
)
, ∆xi =x − xop,i, σ1 =σ2 =σ3 = 7.5
∆ui = Pi∆ei + Ii
Z
t
0
∆eidτ = Pie + Ii
Z
t
0
edτ
∆ei = ∆xref − ∆xi = (xref − xop,i) − (x − xop,i) = xref − x = e
5.2 (a) m = 0.1 kg: s1,2 = ±j
√
300, and m = 1 kg: s1,2 = ±j
√
30
(b) u=
X
5
i=1
di(m(t))ui
, ui=KPi

y−yref+
1
TIi
Z
t
0
(y−yref)dτ + TDi
d(y−yref)
dt 
Table 8.2 shows the parameters of the subcontrollers and Figure 8.19 the
weighting functions.
(c) u = KP(m)

y − yref +
1
TI(m)
Z
t
0
(y − yref)dτ + TD(m)
d(y − yref)
dt

683
KP(m)= 300m−c, TI(m)= 300m−c
1000m
, TD(m)= 30m
300m−c
, c= 30 Nm−1
Table 8.2: Parameters of the subcontrollers
Controller 1 2 3 4 5
mi 0.2 kg 0.4 kg 0.6 kg 0.8 kg 1 kg
KPi 30 90 150 210 270
TIi 0.15 0.225 0.25 0.2625 0.27
TDi 0.2 2/15 0.12 4/35 1/9
d1
1
d2 d3 d4 d5
0.2 0.4 0.6 0.8 1 m in kg
di
Fig. 8.19: Weighting functions
5.3 (a) δ = 1
(b)
z =


x3
x1
x2

, z˙ =



−z3 − 1
−
1
ε
(z
3
2 + z2z3 + z1)
−2z2 − 2z3


 +


1
0
0

u, z˙zero=
"
−
1
ε
(z
3
2 + z2z3)
−2z2 − 2z3
#
(c) u = x2 + 1 − a0x3 + V yref, y˙ + a0y = V yref
5.5 (a) u(x)=−
c
TAnx+an−1c
TAn−1x+. . .+a1c
TAx+a0c
Tx
c
TA
n−1Bx
+
V yref
c
TA
n−1Bx
(b) If, during regulation, the trajectory x(t) approaches the equilibrium point
xeq = 0, the term c
T A
n−1Bx becomes increasingly small and tends to￾wards zero. As a consequence, the control signal u becomes greater and
greater and tends to infinity. This is, of course, not feasible.
(c)
z =







y
y˙
.
.
.
y
(n−2)
y
(n−1)







=







c
T
c
T A
.
.
.
c
T A
n−2
c
T An−1







x = T x684 Chapter 8. Solutions to the Exercises
(d)
z˙ =







z2
z3
.
.
.
zn
c
T AnT
−1
z







+







0
0
.
.
.
0
c
T An−1BT −1
z







u, y = c
T T
−1
z
5.6 (a) δ = 2
(b) u(x)=−
L
2
a
c(x) + a1Lac(x) + a0c(x)
LbLac(x)
+
V yref
LbLac(x)
, y = c(x) = x2
Lac(x)=k1x1 + k2x
2
1
(l − x2) − ve
L
2
a
c(x)=−k0 [k1 + 2k2x1(l − x2)] x1 − k2

k1x1 + k2x
2
1
(l − x2) − ve

x
2
1
LbLac(x)=k0 [k1 + 2k2x1(l − x2)]
(c) The control can be implemented if LbLac(x)=k0[k1+2k2(l−x2)x1] 6= 0 is
fulfilled. The latter is the case, because k0 > 0, k1 > 0, k2 > 0, l > x2 > 0,
and x1 > 0.
5.8 (a)x˜˙ =a(x˜)+b(x˜)v, y=c(x˜)=g(x), x˜=

x
u

, a(x˜)=
f(x, u)
0

, b(x˜)=
0
1

(b) δ ≥ 2
(c)
z˙ =







y˙
y¨
.
.
.
y
(n−1)
y
(n)







=







0 1 0 . . . 0
0 0 0 . . . 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 . . . 1
−a0 −a1 −a2 . . . −an−1







z +







0
0
.
.
.
0
1







yref, y = c(x˜),
v = −
L
n
ac(x˜) + an−1L
n−1
a c(x˜) + ... + a1Lac(x˜) + a0c(x˜)
LbL
n−1
a c(x˜)
+
V yref
LbL
n−1
a c(x˜)
(d) u =
Z
t
0
vdτ
5.10 (a) δ = n + 1 − i for y = xi
, i = 1, . . . , n
(b) Internal dynamics: z˙n = f1(zn) + h(zn) · z1 with z1 = x2 and zn = x1
5.11 (b) A linearizing output is y = x3.
(c)
z =


y
y˙
y¨

 =


x3
−x1 − cx3
x1(a + c − x2) + (c
2 − 1)x3


u =
[x1(x2 − a) + x3] (a + c − x2) + x1(x
2
1 + bx2 − c
2
) − (c
3 − c)x3
x1
+
v
x1
(d) ˙˙˙y = v ⇒ y =
Z
t
0
Zτ
0
Zτ˜
0
vdτ dˆ τ dτ ˜685
(e)
v= ˙˙˙y =



0, t < 0,
−8 sin(t) cos(t), 0 ≤ t ≤
π
2
,
0,
π
2
< t
5.12 (a) δ =

2 2 2
(b) Since det(D(x)) = −2a/(Jm2
) 6= 0 holds, the vectorial relative degree δ
is well-defined.
(c)
u(x) = −D−1
(x)


a1,1x2 + a1,0x1 − V1yref,1
a2,1x4 + a2,0x3 − V2yref,2
a3,1x6 + a3,0x5 − V3yref,3


D−1
(x) = m
2a


a cos(x5) − b sin(x5) b cos(x5) + a sin(x5) J/m
b sin(x5) + a cos(x5) a sin(x5) − b cos(x5) −J/m
2a sin(x5) −2a cos(x5) 0


5.15 No, because rank(
b adab ad2
a b

) 6= 3. See Theorem 64 on p. 418.
5.16 (a) x1 = y, x2 = ˙y
3
, x3 = −3¨y, u = 3(yy¨ − ˙˙˙y )
(b) P(s) = (s+5)3 = s
3+15s
2+75s+125 ⇒ a2 = 15, a1 = 75, a0 = 125
Ψ1
−1
(x)=





y
y˙
y¨





=





x1
√3 x2
−
1
3
x3





, Ψ2(y, y,˙ ˙˙˙y ) = 3(yy¨ − ˙˙˙y ) = 3(yy¨ − v)
Figure 8.20 shows the block diagram of the control.
v
x
y
˙˙˙yref


yref
y˙ref
y¨ref


v= ˙˙˙yref −
X2
i=0
ai

y
(i)−y
(i)
ref
u = Ψ2(y, y, v ¨ )
x˙ = f(x, u)
y = x1
z=


y
y˙
y¨

= Ψ1
−1
(x)
Fig. 8.20: Flatness-based feedback control
5.17 (a)
x˙ =

(x2 − x1)x1
0.04093x1x2

+

0
1

u, y = x1
(b) x1 = y, x2 =
y˙ + y
2
y
, u =
yy¨ − y˙
2
y
2
+ ˙y − 0.04093( ˙y + y
2
)686 Chapter 8. Solutions to the Exercises
(c) The flat input can be used to influence the growth of human knowledge.
Good education and research funding, for example, can do this.
(d) u = −0.04093y
2
(e) Using the flat input, the population x1 can only be kept constant if human
knowledge and human technological possibilities x2 are held constant, i. e.
if they stagnate. This can be seen directly from x˙ 1 = (x2 − x1)x1 = 0.
(f) The flat output is yf = x2.
(g) x1 =
1
0.04093
·
y˙f
yf
, x2 = yf , u = 0.04903
y¨f yf − y˙
2
f
(0.04903y
2
f − y˙f ) ˙yf
(h) If the population size x1(t) = x1,∞ is kept constant by u = 0, then
yf (t) = x2(t) = x2(0)e
0.04093x1,∞t
represents the increase in human knowledge. The population can thus be
kept constant while the amount of knowledge continues to grow exponen￾tially.
5.18 No, because it is not full-state linearizable (Theorem 69, on p. 430).
5.20 (a)







z1
z2
z3







=







y
y˙
y¨







=








x1
− λx1 ln￾x1
x2

λx1

λ

ln
x1
x2

+1
ln
x1
x2

+b
x1
x2
− dx2/3
1 −µx3









=t(x)
(b)
x = Ψ1(y, y,˙ y¨)=







y
yey/˙ (λy)
1
µ

be−y/˙ (λy) − dy2/3 −
y˙
y
−
y¨
λy +
y˙
2
λy2








= t
−1
(z)
(c) u =
b
λµe
−y/˙ (λy)

λa +
y˙
2
y
2
−
y¨
y

−
d
µ
y
2/3

a +
2
3
y˙
y

+
1
µ
y˙
y

y˙
y
−
2
λ
y˙
2
y
2
+
3
λ
y¨
y
+
a
λ
y˙
y
− a

−
1
µy 
ay¨
λ
+ ¨y +
˙˙˙y
λ

(d) u(t) = 
2dα
3µ
−
da
µ

e
−2αt/3 +
a
µ

beα/λ + α

(e) Figure 8.21 shows the medication u(t).
5.21 Both controls are based on a full-state linearization which converts the
nonlinear system into the Brunovsky canonical form. In this aspect they are
identical. Furthermore, both use the output variable y and its derivatives for
control. The difference between the two is the use of the reference variabl687
u
u(0)
u(∞)
t
u(0) = 2dα
3µ
−
da
µ
+
abeα/λ
µ
+
αa
µ
u(∞) = abeα/λ + αa
µ
Fig. 8.21: Progression of the medication u
yref. In the case of a full-state linearization, the control law depends on a
reference signal yref(t), which may be constant. In the case of the flatness￾based feedback control, the control law depends not only on a reference signal
yref(t) but also on its derivatives y˙ref(t), . . . , y
(n)
ref (t). In this context, see also
Fig. 5.9 on p. 365 and Fig. 5.27 on p. 453.
5.23 (a) The matrix A + ucN must possess only eigenvalues with a negative
real part.
(b) The matrix AT R + RA must be negative definite.
(c) u = −umax sgn(x
T
(NT R + RN)x)
(d) The controller from (c) switches perpetually with a high frequency be￾tween −umax and umax. This also happens near the equilibrium point
xeq = 0. Of course, this is undesirable. We can avoid the high-frequency
switching using the modified control law
u = − sat(µx
T
(NR + RN)x).
The control parameter µ requires adjustment according to the application
in question. It determines for which values of x the control variable u is
smaller than umax.
5.24 (a)
u = −
f(x) + Pn
i=1
ai−1xi
h(x)
, ai
, i = 1, ..., n − 1, freely selectable
(b)
x˙ =







0 1 0 . . . 0 0
0 0 1 . . . 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 . . . 0 1
−a0 −a1 −a2 . . . −an−2 −an−1







x
(c) The two control loops are identical, with the difference that no reference
variable yref is present in the backstepping control. However, its control
law can be easily completed by adding V yref/h(x).688 Chapter 8. Solutions to the Exercises
6.1 (a) F = 1, G = 1, H = 1
(b) J(∆u(k)) = 2∆u2
(k) + 2∆u(k)e(k) + e
2
(k)
(c) u(k) = −0.5y(k − 1) + 0.5yref(k + 1)
(d) y(k + 1) = y(k) − 0.5y(k − 1) + 0.5yref(k + 1)
(e) z1/2 = 0.5 ± j0.5
(f) k 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
y(k) 0
1
2
1
5
4
5
4
9
8
1
15
16
15
16
31
32 1
65
64
65
64
129
128 1
255
256
255
256
511
512 1
6.2 (a) F =

1
1

, G=

1
2

, H =

1 0
2 1
,
J(∆u¯(k)) = ∆u¯
T
(k)

6 2
2 2
∆u¯(k) + 2∆u¯
T
(k)

1 2
0 1
e(k) + e
T
(k)e(k),
u(k) = −
1
4

y(k) + y(k − 1) − yref(k + 1) − yref(k + 2)
,
y(k + 1) = 1
4

3y(k) − y(k − 1) + yref(k + 1) + yref(k + 2)
,
z1/2 =
3
8
± j
1
8
√
7
k 0 1 2 3 4 5 6 7 8
y(k) 0 0.5 0.875 1.0313 1.0547 1.0332 1.0112 1.0001 0.9973
k 9 10 11 12 13 14 15 16 . . . 18
y(k) 0.9979 0.9991 0.9999 1.0001 1.0001 1.0001 1.0000 1.0000
(b) MPC2: k3% = 6
(c) MPC1: k3% = 10; the MPC2 is faster than the MPC1 by a factor of 1.6.
6.3 (a) s(x) = x1 + x2, u(x) = − sgn(s(x))
(b) Figure 8.22 shows the trajectories of the control loop.
(c) The sliding mode occurs between the points x
T =

−1 1
and x
T = 
1 −1

on the switching line, and nowhere else.
(e) x1(t) = −x2(t) = x1(0)e
−t
(f)
x1(t)=



−5 + 5t − 0.5t
2
, 0 ≤ t < 8,
59 − 11t + 0.5t
2
, 8 ≤ t < 12,
−e
−(t−12)
, 12 ≤ t,
x2(t)=



5 − t, 0 ≤ t < 8,
−11 + t, 8 ≤ t < 12,
e
−(t−12)
, 12 ≤ t;
see Figure 8.23.
(g) t1 = 5 + r
15
2
≈ 7.7386, t2 = 5 + 2r
15
2
≈ 10.4772; see Figure 8.24.
(h) k = ceil 
|x1(0)| − 1
2
689
4
2
0
-2
-4
-4 -2
State
x2
s(x)= 0 u=−umax
u=umax
State x1
0 2 4
Fig. 8.22: Approximation of the time-optimal controller’s switching curve using a
switching line. On this switching line a sliding mode occurs, which is shown in red.
8
6
4
2
0
-6
-4
-2
0 5 10 15 20
t
x1, x2
x1(t)
x2(t)
Sliding
mode
Fig. 8.23: Time courses of the sliding
mode control loop’s state variables
8
6
4
2
0
-6
-4
-2
0 5 10 15 20
t
x1, x2
x1(t)
x2(t)
Fig. 8.24: Time courses of the time￾optimal control loop’s state variables
6.4 u =
−q sgn(s(x)) − ks(x) − gradT
(s(x)) · a(x)
gradT
(s(x)) · b(x)
6.5 (a) x˙ 1 = −gµ(x2), x˙ 2 = −
gµ(x2)
x1

1 − x2 +
mr2
J

+
r
Jx1
u
(b) u(x) = −
Jx1
r

q sgn(x2−x2,ref)+k(x2−x2,ref)

+
Jgµ(x2)
r

1−x2+
mr2
J
690 Chapter 8. Solutions to the Exercises
(c)
x1(t) =



x1(0) − gµ(x2,ref)t, t < teq =
x1(0)
gµ(x2,ref)
,
0, t ≥ teq =
x1(0)
gµ(x2,ref)
,
x2(t) = x2,ref
(d) The slip x2 = λ is constant in the sliding mode, more precisely x2(t) =
x2,ref. The vehicle’s velocity x1(t) = x1(0)−gµ(x2,ref)t in the sliding mode
during braking depends on the friction coefficient. The latter is a function
of the road surfacing. Thus, the braking distance 0.5x1(0)/(gµ(x2,ref))
depends on the velocity when braking begins, and on the road surface.
(e) x2,ref = λref ≈ 0.16
6.6 (b)The model of the oven is strictly positive real, i.e. strictly passive. Since
it is linear, it possesses a radially unbounded storage function. The sat￾uration curve combined with the P controller is passive. Thus, according
to Theorem 80 on p. 549, the control loop has a globally asymptotically
stable equilibrium point at ∆Ti = 0, i = 1, 2, 3.
6.8 (a) The characteristic diagram passes through the origin. Furthermore,
the following inequalities apply:
x ≤ 0, e ≥ 0
h(x, e) ≤ h(0, e) ≥ 0
h(x, e) ≥ h(x, 0) ≤ 0
x ≥ 0, e ≥ 0
h(x, e) ≥ h(0, e) ≥ 0
h(x, e) ≥ h(x, 0) ≥ 0
x ≤ 0, e ≤ 0
h(x, e) ≤ h(0, e) ≤ 0
h(x, e) ≤ h(x, 0) ≤ 0
x ≥ 0, e ≤ 0
h(x, e) ≥ h(0, e) ≤ 0
h(x, e) ≤ h(x, 0) ≥ 0
6.9 The system has the relative degree δ = 1, but the internal dynamics are
unstable. Thus it is impossible to generate a passive control loop using a static
controller u(x). See Theorem 85 on p. 564.
6.10 This exercise is out of the ordinary, because the task is not solvable. (It
is the only unsolvable exercise in this book.) It is meant to illustrate that you
may be faced with mathematical problems and tasks in engineering which
are not solvable. You may recognize this immediately with some problems,
but not with others. Examples are the derivation of a general criterion for
the controllability of nonlinear systems or the construction of a perpetual
motion machine. In the example considered here, the plant (6.69), (6.70) is not
strictly passive because there is no positive definite matrix R which satisfies
the Kalman-Yakubovich-Popov equations691
AT R + RA = −µP,
RB = C
T
of Theorem 29 on p. 154 such that P is positive definite.
6.11 (a) Nonlinear controller canonical form x˙ = a(x) + b(x)u :
x˙ =

x2
a1 − a2x2 − a3 sin(x1 + δeq)

+

0
−a3 sin(x1 + δeq)

u
PCHD form:
x˙ = (J − D)
∂V
∂x
+ b(x)u, J =

0 1
−1 0
,
b(x)=
0
−a3 sin(x1+δeq)

D=

0 0
0 a2

.
(b) J CL−DCL =

0 1
∗ ∗
, ∗ = arbitrary, but such that J CL−DCL is negative
definite.
(c) u =
kx1 + dx2
a3 sin(x1 + δeq)
6.12 (a) xeq1 = 0, x
T
eq2 =
√
63 √
63 21
, x
T
eq3 =

−
√
63 −
√
63 21
(b)
x˙ = (J−D)
∂V1
∂x
+bu, J =


0 35 0
−35 0 −x1
0 x1 0

, D=


175 0 0
0 −28 0
0 0 3

, b=


0
1
0


(c) The equilibrium point xeq = 0 is unstable, because the linearized sys￾tem has an unstable eigenvalue (indirect Lyapunov method, Theorem 17,
p. 126).
(d) The Byrnes-Isidori canonical form and diffeomorphism t(x) are
z˙ =


28z1 − 7z2 − z2z3
35z1 − 35z2
z1z2 − 3z3

 +


1
0
0

u, z = t(x) =


x2
x1
x3

.
(e) According to Theorem 85 on p. 564 we can construct a passive control
loop using a static controller, because the relative degree is δ = 1 and the
internal dynamics
z˙int =

z˙2
z˙3

=

−35 0
0 −3
 z2
z3

+

35
z2

z1
are stable.
(f) The control law is u(z) = −28z1 − kz1 − 28z2 + yref and the control loop
in PCHD form z˙ = (Jcl − Dcl)
∂V2(z)
∂x
+ bclu, where692 Chapter 8. Solutions to the Exercises
Jcl =


0 −35 −z2
35 0 0
z2 0 0

 , Dcl =


k 0 0
0 35 0
0 0 3

, bcl =


1
0
0

.
(g) x˙ = (Jorg − Dorg)
∂V2(x)
∂x
+ byref,
Jorg =


0 35 0
−35 0 −x1
0 x1 0

, Dorg =


35 0 0
0 k 0
0 0 3

, b =


0
1
0


(h) The equilibrium point xeq1 = 0 is globally asymptotically stable, because
the constant matrix Dorg is positive definite and V2 is a Lyapunov function
with V (x) → ∞ for |x| → ∞.
6.14 The bounded sum satisfies a ∨ a¯ = 1, but the maximum operator and
the algebraic sum do not.
6.15 (a) The subsidiary system is passive.
6.16 The membership functions and the rule base are shown in Figure 8.25
and Table 8.3, respectively. Probably your solution is slightly different due to
different possible solutions.
µ µ
µ Driving style µ Gas-pedal position
M in Nm
P
Driving resistance torque
1 1
1 1
0
0 10 F 0 100%
defensive aggressive small large kick￾down
negative small positive
-200 0 200 1500 n 5500 up in rpm
Fig. 8.25: Membership functions of the input variables and the output variable nup
7.2 The weak observability can be proven for all ω ∈ IR3
, except for the cases
(1) Jz = Jy, (2) ωx = 0, and (3) ωy = ωz = 0.
7.3 (a)The system is control-affine and has the relative degree δ = 3. It is thus
weakly observable (see Theorem 90 on p. 618). Furthermore, the system
is even observable since the inverse q
−1
(z) of the function z = q(x) exists
with the exception of x1 = z1 = −110.693
Table 8.3: Rule base of the gear selection
Gas-pedal position P
small large kickdown
Driving
resistance
M
positive 2500 3000 5500
small 2000 2500 5000
negative 1500 2000 4500
Driving style = defensive
Gas-pedal position P
small large kickdown
Driving
resistance
M
positive 4500 5500 5500
small 4000 5000 5000
negative 3500 4500 4500
Driving style = aggressive
(b) For x1 = −110, x˙ 1 = 0.028x1 applies. In this case, x2 and x3 do not act
on y = x1. Thus the system is not observable at this point, i.e. it is only
locally observable and only locally weakly observable.
7.4 h1(x1) 6= 0, h2(x1, x2) 6= 0, . . . , hn−1(x1, . . . , xn−1) 6= 0
7.6 (a) With the inverse function q
−1
yielding

x1
x2

=

±
p
y˙
y

of z =

y
y˙

=

x2
x
2
1

= q(x),
not all x ∈ IR2
can be uniquely determined.
(b) We use y¨ in addition. Now, with the function
q˜
−1
(y, y¨) = 
0.5¨y
y

=

x1
x2

= x
and if y and y¨ are known, all x ∈ IR2
can uniquely determined.
(c) The example shows that Theorem 86 on p. 608 is only sufficient. A non￾linear system can thus be observable although no formula
x = q
−1
(y, y, . . . , y ˙
(n−1))
exists with which all x ∈ Dx are uniquely determinable.
7.7 (a) x1 = z2, x2 = z1 (b) z˙1 = −z
3
2
, z˙2 = z1 − z2 + u, y = z2
(c)
z˜˙ =

0 0
1 0
z˜ −

y
3
y − u

+

l1
l2

(y − z˜2), l1 = 100, l2 = 20
(d)
x˜˙ =

0 1
0 0
x˜ −

y − u
y
3

+

l2
l1

(y − x˜1), y = x1
7.9 (a) P(s)=s
3+

l1+c − d
x2
x
2
1

s
2+

l2+l1

c−d
x2
x
2
1

+
2ax2
3
x
3
1

d
x1
−1
s
+
2ax3
x
2
1

−l3−2d
x2x3
x
3
1
+l1d
x3
x
2
1
+

c−d
x2
x
2
1
  l2x
2
1
2ax3
−
x3
x1

(b) P(s) is independent of x and u, i.e. P(s) = s
3 + ˆa2s
2 + ˆa1s + ˆa0, if the
following apply:694 Chapter 8. Solutions to the Exercises
l1 = ˆa2 − c + d
x2
x
2
1
,
l2 = ˆa1 +
2ax2
3
x
3
1

1 −
d
x1

− l1

c − d
x2
x
2
1

,
l3 = −
aˆ0x
2
1
2ax3
+ d
x3
x
2
1

l1 −
2x2
x1

+

l2x
2
1
2ax3
−
x3
x1
 c − d
x2
x
2
1

.
7.10 (b)
z˙ =




z2
z3
−
2cz1z2 + mz2z3
mz1




+




0
0
cz1 + mz3
mAz1




u, y = z1
(c)
x˜˙ =






x˜2
−
c
m
x˜1 +
A
m
x˜3
−
x˜2x˜3
x˜1






+






0
0
x˜3
Ax˜1






u +






l1ε
−1
l2ε
−2
cl1ε
−1 + ml3ε
−3
A






(y − y˜)A
Appendix
A.1 Proof of the General Circle Criterion
Below we will prove the general circle criterion, Theorem 10, p. 109.
According to Theorem 80, p. 549, the equilibrium point xeq = 0 of the
nonlinear standard control loop
Y (s) = G(s)U(s),
e = −y,
u = f(e, t)
(A.1)
with the open-loop transfer function G(s) and the nonlinear characteristic
f(e, t) is globally asymptotically stable if f(e, t) lies in the sector [0, ∞], i. e.
the nonlinearity is passive, and G(s) is strictly positive real. In the following,
we will restrict ourselves to the sector [K1, K2] with 0 < K1 < K2, since all
other cases are identical to those of the circle criterion without poles on the
imaginary axis. For the nonlinearity,
K1 ≤
f(e, t)
e
≤ K2
holds. We will transform the sector [K1, K2] into sector [0, ∞]. For this pur￾pose, we will convert the control loop (A.1) in the following three steps.
Step 1: We add a negative feedback with gain K1 to the plant G(s) for
Y (s) = G(s)(U(s) − K1Y (s)) ⇔ G1(s) = Y (s)
U(s)
=
G(s)
1 + K1G(s)
to apply. So that the control loop is not changed in its dynamics,
we compensate this feedback by a gain K1 connected in parallel to
the nonlinearity
u = f(e, t) + K1y = f(e, t) − K1e, e = −y. (A.2)
© Springer-Verlag GmbH Germany, part of Springer Nature 2024
J. Adamy, Nonlinear Systems and Controls,
https://doi.org/10.1007/978-3-662-68690-4
695696 Appendix A. Appendix
Step 2: We extend the transfer function of Step 1 by the gain K2 − K1 and
obtain
G2(s) = Y (s)
U2(s)
=
(K2 − K1)G(s)
1 + K1G(s)
.
Again, the control loop’s dynamics must not change. Therefore, we
multiply the nonlinearity (A.2) by 1/(K2 − K1) which results in
u2 =
f(e, t) − K1e
K2 − K1
and U2(s) = U(s)
K2 − K1
.
Step 3: Now we add a parallel branch with gain 1 between the input of the
transfer function G2(s) and its output. So we obtain
G3(s) = G2(s) + 1 = 1 + K2G(s)
1 + K1G(s)
. (A.3)
This parallel branch is compensated by a positive feedback from the
output of the nonlinearity of Step 2 (which is the input of G3(s)) to
its input (which was the negated output of G3(s) in Step 2). This
results in the modified nonlinearity
u2 =
f(z, t) − K1z
K2 − K1
, z = u2 + e, e = −y
⇔ (K2 − K1)u2 =
f(z, t) − K1z
z
(u2 + e)
⇔ K2u2 −
f(z, t)
z
u2 =
f(z, t)
z
e − K1e
⇔ u2 =
f(z, t)
z
− K1
K2 −
f(z, t)
z
e = ˜f(e, t), z = u2 + e. (A.4)
The modified nonlinearity (A.4) transforms the sector [K1, K2] into
the sector [0, ∞]. We can easily see this if we insert f(z, t) = K1z
and f(z, t) = K2z into equation (A.4).
Figure A.1 illustrates these mutually canceling additions to the control loop
(A.1). Consequently, the resulting control loop
Y (s) = G(s)U(s),
e = −y,
u = ˜f(e, t)
has the same dynamic behavior as the control loop (A.1).A.1. Proof of the General Circle Criterion 697
f(e, t)
y
y
z y 1
K2 − K1
K2 − K1 G(s)
e
e
e
u2
u2
K1 K1
f(e, t) G(s)
u
m
m
˜f(e, t)
1 + K2G(s)
1 + K1G(s)
Fig. A.1: Transformation of sector [K1, K2] into sector [0, ∞]
Thus, we can conclude that the nonlinear standard control loop is abso￾lutely stable in the sector [K1, K2], 0 < K1 < K2, if the transfer function
(A.3) is strictly positive real. According to Theorem 32 on p. 160, the latter
is fulfilled if
Re (
1 + K2G(jω)
1 + K1G(jω)
)
> 0 (A.5)
holds for all ω ≥ 0, and if G3(s) is asymptotically stable, which is the case if
G(s)
1 + K1G(s)
(A.6)
is asymptotically stable, and if
G3(∞) > 0 (A.7)698 Appendix A. Appendix
holds. Equation (A.7) is fulfilled, since we have assumed in Theorem 10 that
the degree of the numerator of G(s) is lower than the degree of its denomina￾tor.
Using
G(jω) = a(ω) + jb(ω),
equation (A.5) can be reformulated as
Re 
1 + K2a(ω) + jK2b(ω)
1 + K1a(ω) + jK1b(ω)

=
(1 + K1a(ω)) (1 + K2a(ω)) + K1K2b
2
(ω)
(1 + K1a(ω))2 + K2
1
b
2(ω)
> 0 for all ω ≥ 0.
(A.8)
The latter is fulfilled if
(1 + K1a(ω)) (1 + K2a(ω)) + K1K2b
2
(ω) > 0
holds for all ω ≥ 0. From this, we obtain the inequality
(a(ω) + c)
2 + b
2
(ω) > r2
for all ω ≥ 0,
which describes the exterior of a disc with center
c =
1
2

1
K1
+
1
K2

and radius
r =
1
2

1
K1
−
1
K2

.
The circle lies symmetrically on the real axis, passing through the points
−1/K1 and −1/K2. Inequality (A.8) and thus inequality (A.5) are fulfilled if
and only if the Nyquist plot of G(jω) neither intersects nor touches the circle.
So far, we have followed the proofs of the circle criterion published in the
literature [168, 232, 308, 402, 422, 465]. Now our task is to ensure that equation
(A.6) is satisfied. In the literature, equation (A.6) is verified using the well￾known Nyquist criterion [98, 133, 465]. In contrast, we will use a different form
of the Nyquist criterion [130, 483], based on the continuous angle variation Ω,
to handle systems which may also have poles on the imaginary axis. For the
sake of convenience, we will repeat this Nyquist criterion within our proof:
Theorem 92 (Nyquist criterion). The feedback control loop
Y (s) = G(s)U(s)
U(s) = −KY (s), K ∈ IR,
with the coprime open-loop transfer function KG(s) whose numerator degree
is less than its denominator degree is asymptotically stable if and only if the
continuous angle variation with respect to the point −1/K + j0 is
Ω = νπ + µ
π
2
.A.2. Parameters of the Container Crane Control 699
Condition (1) of the general circle criterion results from this Nyquist criterion
and the conditions above. Conditions (2), (3), and (4) are identical to the
circle criterion for systems without poles on the imaginary axis. ⊓⊔
A.2 Parameters of the Container Crane Control
Subcontrollers u = −k
T
i x of the crane control described in Section 4.3.3:
k
T
1 =

1.9245 16.8960 −67.7493 −120.7999 0.3651 
,
k
T
2 =

5.3581 39.3339 287.3235 −181.1081 0.8983 
,
k
T
3 =

7.4166 51.0577 325.7613 −306.2776 1.1650 
,
k
T
4 =

11.7940 72.2878 138.0207 −535.5579 1.6982 
,
k
T
5 =

17.3572 94.5922 −118.3572 −678.5956 2.2315 
,
k
T
6 =

23.9695 117.0157 −417.4605 −708.2384 2.7648 
,
k
T
7 =

31.4013 138.7914 −730.7397 −602.6913 3.2980 
,
k
T
8 =

49.8302 188.8935 −1005.7853 −405.2965 4.2313 
,
k
T
9 =

72.0268 241.9377 −1118.9194 54.1054 5.1645 
,
k
T
10 =

97.0202 297.4944 −954.7146 787.7226 6.0977 
.
Matrices Ri of the catchment regions Gi = {x ∈ IR5

 x
T Rix ≤ 1}:
R1=






4.1107·10−4
3.3656·10−3 −4.0166·10−3 −2.4248·10−2
6.6442·10−5
3.3656·10−3
3.1180·10−2 −2.8011·10−2 −2.3607·10−1
6.2148·10−4
−4.0166·10−3 −2.8011·10−2
4.1307 1.7240·10−1 −2.7836·10−3
−2.4248·10−2 −2.3607·10−1
1.7240·10−1
5.4049 −2.9691·10−3
6.6442·10−5
6.2148·10−4 −2.7836·10−3 −2.9691·10−3
1.4478·10−5






,
R2=






3.7979·10−3
2.6079·10−2
1.8755·10−1 −2.2657·10−1
5.6711·10−4
2.6079·10−2
1.8712·10−1
1.2913 −1.6330 4.0165·10−3
1.8755·10−1
1.2913 2.8190·10 −1.1024·10 1.9722·10−2
−2.2657·10−1 −1.6330 −1.1024·10 3.0710·10 −2.7863·10−2
5.6711·10−4
4.0165·10−3
1.9722·10−2 −2.7863·10−2
9.5314·10−5






,
R3 =






7.0254·10−3
4.5741·10−2
3.1248·10−1 −4.1397·10−1
1.0098·10−3
4.5741·10−2
3.0763·10−1
2.0342 −2.7809 6.7269·10−3
3.1248·10−1
2.0342 3.3884·10 −1.8433·10 3.6004·10−2
−4.1397·10−1 −2.7809 −1.8433·10 4.2920·10 −5.3480·10−2
1.0098·10−3
6.7269·10−3
3.6004·10−2 −5.3480·10−2
1.5701·10−4






,700 Appendix A. Appendix
R4 =






1.8355·10−2
1.0811·10−1
4.7125·10−1 −9.4012·10−1
2.4504·10−3
1.0811·10−1
6.5416·10−1
2.8488 −5.5883 1.4831·10−2
4.7125·10−1
2.8488 3.8658·10 −2.5478·10 5.1777·10−2
−9.4012·10−1 −5.5883 −2.5478·10 7.2404·10 −1.1723·10−1
2.4504·10−3
1.4831·10−2
5.1777·10−2 −1.1723·10−1
3.5057·10−4






,
R5 =






5.2217·10−2
2.7366·10−1
4.0373·10−1 −2.3095 6.2049·10−3
2.7366·10−1
1.4687 2.4757 −1.2133·10 3.3474·10−2
4.0373·10−1
2.4757 4.2570·10 −2.1409·10 3.7964·10−2
−2.3095 −1.2133·10 −2.1409·10 1.3541·102 −2.6235·10−1
6.2049·10−3
3.3474·10−2
3.7964·10−2 −2.6235·10−1
7.8876·10−4






,
R6 =






1.1949·10−1
5.6923·10−1 −5.1097·10−1 −4.2345 1.3063·10−2
5.6923·10−1
2.7826 −1.6116 −2.0252·10 6.4387·10−2
−5.1097·10−1 −1.6116 6.7149·10 9.3161 −6.8998·10−2
−4.2345 −2.0252·10 9.3161 2.0442·102 −4.4193·10−1
1.3063·10−2
6.4387·10−2 −6.8998·10−2 −4.4193·10−1
1.5430·10−3






,
R7 =






2.3786·10−1
1.0378 −2.8921 −5.9433 2.4280·10−2
1.0378 4.6715 −1.1067·10 −2.6154·10 1.1058·10−1
−2.8921 −1.1067·10 1.5204·102
5.1281·10 −3.2462·10−1
−5.9433 −2.6154·10 5.1281·10 2.5478·102 −5.6212·10−1
2.4280·10−2
1.1058·10−1 −3.2462·10−1 −5.6212·10−1
2.7258·10−3






,
R8 =






5.8841·10−1
2.2140 −8.4857 −7.0109 4.9746·10−2
2.2140 8.6981 −2.9546·10 −2.7779·10 1.9994·10−1
−8.4857 −2.9546·10 3.2425·102
6.6341·10 −7.8731·10−1
−7.0109 −2.7779·10 6.6341·10 3.0685·102 −5.3622·10−1
4.9746·10−2
1.9994·10−1 −7.8731·10−1 −5.3622·10−1
4.8279·10−3






,
R9 =






1.0563 3.5328 −1.3691·10 −2.9545 7.6710·10−2
3.5328 1.2558·10 −4.2423·10 −1.2953·10 2.8163·10−1
−1.3691·10 −4.2423·10 5.5357·102 −1.1448·10 −1.1622
−2.9545 −1.2953·10 −1.1448·10 4.6012·102 −9.4034·10−2
7.6710·10−2
2.8163·10−1 −1.1622 −9.4034·10−2
6.7514·10−3






,
R10 =






1.6062 4.8038 −1.3282·10 6.2810 9.7536·10−2
4.8038 1.5756·10 −3.3740·10 1.2281·10 3.3568·10−1
−1.3282·10 −3.3740·10 9.2668·102 −1.0862·102 −1.1734
6.2810 1.2281·10 −1.0862·102
1.0134·103
6.5813·10−1
9.7536·10−2
3.3568·10−1 −1.1734 6.5813·10−1
7.9645·10−3






.B
List of Symbols
IR : Set of real numbers
IR+ : Set of positive real numbers
IR− : Set of negative real numbers
IRn
: Real coordinate space of dimension n
Dx,def : A function’s or a free system’s domain
of definition
Dx : Subset of Dx,def, i. e. Dx ⊆ Dx,def
Du,def : Subspace of a non-free system’s do￾main of definition Dx,def × Du,def
Du : Subset of Du,def, i. e. Du ⊆ Du,def
Dt,def : Subspace of a system’s domain of def￾inition Dx,def × Dt,def
C : Set of complex numbers
C
n
: Complex coordinate space of dimen￾sion n
IN : Set of natural numbers
IN0 : Set of natural numbers and zero
Z : Set of integers
C
k
: Set of k-times continuously differen￾tiable functions
C
k
m : Set of k-times continuously differen￾tiable, m-dimensional vector functions
T1 ⇔ T2 : T1 is equivalent to T2.
a ≈ b : a is approximately equal to b.
a ≪ b : a is much smaller than b.
a ≫ b : a is much greater than b.
x → a : x tends to a.
j : Imaginary unit j =
√
−1
z : Complex numbers are marked by an
underscore in certain cases
© Springer-Verlag GmbH Germany, part of Springer Nature 2024
J. Adamy, Nonlinear Systems and Controls,
https://doi.org/10.1007/978-3-662-68690-4
701702 Appendix B. List of Symbols
z : Complex conjugate number of
z = a + jb, i. e. z = a − jb
Re {a + jb} = a : Real part of a complex number
Im {a + jb} = b : Imaginary part of a complex number
[a, b] : Closed interval
[a, b) and (a, b] : Half-open interval
(a, b) : Open interval
|x| : Absolute value of a real or complex
number x
a, b, . . . : Vectors are represented by bold lower
case letters
a
T
: Transposed vector of a
a
∗
: Conjugate transpose vector a
1 : Vector of ones, 1 = [1 1 · · · 1]T
0 : Vector of zeros, 0 = [0 0 · · · 0]T
dim(x) : Dimension of a vector x, i. e. the num￾ber of the vector’s elements xi
kxk : Euclidean norm of a vector x, kxk =
(x
2
1 + . . . + x
2
n
)
1/2
|x| : Absolute value of a vector x, identical
to kxk
a × b : Cross product of vectors a and b
A, B, . . . : Matrices and some vectors of physical
quantities, e. g. the torque vector M,
are represented by bold capital letters.
I : Identity matrix
A−1
: Inverse matrix of A
A
T
: Transposed matrix of A
A : Conjugate matrix A of a matrix A,
i. e. each element aij is replaced by its
complex conjugate aij
A
∗
: Conjugate transpose A of a complex
matrix A, also called an adjoint ma￾trix
rank(A) : Rank of a matrix A; the rank equals
the number of linearly independent
columns of A.
det(A) : Determinant of a matrix A
diag(a11, a22, . . . , ann) : n × n diagonal matrix with the ele￾ments a11, a22, . . . , ann
{a, b, c, . . . } : Set with the elements a, b, c, . . .
a ∈ M : a is an element of the set M.
a /∈ M : a is not an element of the set M.703
M1 × M2 : Cartesian product M1 × M2 ∈ IRn+m
of the sets M1 ∈ IRn
and M2 ∈ IRm
M1 = M2 : Set M1 equals set M2.
M1 ∪ M2 : Union of set M1 and set M2
M1 ∩ M2 : Intersection of set M1 and set M2
M1 ⊂ M2 : M1 is a proper (also called strict) sub￾set of the set M2, i. e. M1 6= M2 holds.
M1 ⊆ M2 : M1 is a subset of the set M2, i. e. M1 =
M2 is possible.
M1 \ M2 : Set M1 without set M2, also called the
set difference
U(x) : Neighborhood of a point x; if it is un￾ambiguous which point x is meant,
only the designation U is used.
Uδ(x) : Neighborhood with
Uδ(x) = {x ∈ IRn
| |x| < δ}
inf{M} : Infimum of a set M
sup{M} : Supremum of a set M
min{M} : Minimum of a set M
max{M} : Maximum of a set M
f
−1
(z) : Inverse function of z = f(x), i. e. x =
f
−1
(z)
min f(x) : Minimum of a function f
min
i
xi
: Minimum of the values xi
arg min
i
xi
: The index i that identifies the mini￾mum xi
sat(u) =



umax, u > umax
u, umin ≤ u ≤ umax
umin, u < umin
: Saturation function
sgn(x) =



1, x > 0
0, x = 0
−1, x < 0
: Signum function
H(x) = (
0, x < 0
1, x ≥ 0
: Heaviside function[1], step function
ceil(x) : Next higher whole number of a real
number x, e. g. ceil(3.34) = 4
ln(x) : Natural logarithm, logarithm to the
base e
[1] There are different ways of defining the Heaviside function at x = 0. Here we use
H(0) = 1. Furthermore, H(0) = 0 and H(0) = 0.5 are in use.704 Appendix B. List of Symbols
lg(x) : Common logarithm, logarithm to the
base 10
arcsin(x) : Inverse of the sine function
arccos(x) : Inverse of the cosine function
arctan(x) : Inverse of the tangent function
arsinh(x) : Area hyperbolic sine, inverse hyper￾bolic sine
arcosh(x) : Area hyperbolic cosine, inverse hyper￾bolic cosine
X(s) = L{x(t)} : Laplace transform
x(t) = L
−1
{X(s)} : Inverse Laplace transform
G(s) : Laplace transfer function
G(z) : z-transfer function
f
′
(x) =
df(x)
dx : Derivative of the function f concern￾ing the variable x
df(x)
dx





x=a
or
df(x)
dx





a
: Derivative of f at the point x = a
x˙ =
dx
dt : Time derivative of x
x, ¨ x˙˙˙ : Two - and threefold derivative of x
with respect to time t
x
(i)
: i-th derivative of x with respect to
time t
df(x)
dt =
Xn
i=1
∂f(x)
∂xi
·
dxi
dt =
∂f(x)
∂x
· x˙ : Total derivative, i. e. derivation of all
variables with respect to t
∂f(x)
∂xi
: Partial derivative of a function f with
several variables, i. e. derivative with
respect to the variable xi
∂f(x)
∂x
=

∂f(x)
∂x1
∂f(x)
∂x2
· · ·
∂f(x)
∂xn

: Derivative of a function f with respect
to the vector x
grad(f(x)) =







∂f(x)
∂x1
.
.
.
∂f(x)
∂xn







=
 
∂f(x)
∂x
!T
: Gradient, i. e. directional derivative, of
a function f (the gradient specifies a
direction in space and is therefore a
column vector)705
∂f(x)
∂x
=











∂f1(x)
∂x1
∂f1(x)
∂x2
· · ·
∂f1(x)
∂xn
∂f2(x)
∂x1
∂f2(x)
∂x2
· · ·
∂f2(x)
∂xn
.
.
.
.
.
.
.
.
.
.
.
.
∂fn(x)
∂x1
∂fn(x)
∂x2
· · ·
∂fn(x)
∂xn











: Derivative of a vector function with re￾spect to a vector x; the result is the
Jacobian matrix.
Lf h(x) = ∂h(x)
∂x
f(x) : Lie derivative of h with respect to f
L
i
f h(x) = LfL
i−1
f
h(x) : i-th Lie derivative of h with respect to
f
˚c(x) =





L
δ1
a
c1(x)
L
δ2
a c2(x)
.
.
.
L
δm
a
cm(x)





: m-dimensional vector consisting of Lie
derivatives
[f, g] = ∂g(x)
∂x
f(x) −
∂f
∂x
g(x) : Lie bracket of the vector functions f
and g
adi
f g = [f, adi−1
f
g] : i-times nested Lie bracket
E{x(t)} = lim
T→∞
1
2T Z
T
−T
x(t) dt : Expected value, i. e. time average of a
signal x(t)
cov{x1(t), x2(t)} : Covariance matrix of two signals x1(t)
and x2(t)References
1. Abdallah, M. B., Ayadi, M., and Benrejeb, M. (2009): Flatness-Based Control
of MIMO Linear Systems. Proceedings of the 6th IEEE Multi-Conference on
Systems, Signals and Devices, Djerba, 1–6.
2. Abdulgalil, F., and Siguerdidjane, H. (2004): Input-state linearisation tech￾nique and nonlinear control in oil well drillstrings. 6th IFAC Symposium on
Nonlinear Control Systems (NOLCOS), Stuttgart. In IFAC Proceedings Vol￾umes 37(13):1425–1430.
3. Abughalieh, K. M., and Alawneh, S. G. (2019): A Survey of Parallel Imple￾mentations for Model Predictive Control. IEEE Access 7:34348–34360.
4. Adamy, J. (1997): Adaptation of Cross-Direction Basis-Weight Control in Pa￾per Machines Using Fuzzy Decision Logic. International Journal of Approxi￾mate Reasoning 16(1):25–42.
5. Adamy, J. (1997): Method for actuator identification during the transverse
profile control of a continuous material web. U. S. Patent 5,691,908.
6. Adamy, J., and Flemming, A. (2004): Soft variable-structure controls: a survey.
Automatica 40(11):1821–1844.
7. Ahmed, T., and Toki, A. (2016): A Review on Washing Machine Using Fuzzy
Logic Controller. International Journal of Emerging Trends in Engineering
Research 4(7):64–67.
8. Aircraft Accident Investigation Commission, Japanese Ministry of Transport
(1987): Japan Air Lines Co., Ltd, Boeing 747 SR-100, JA8119, Gunma Pre￾fecture, Japan, August 12, 1985. Aircraft Accident Investigation Report.
9. Azerman, M. A. (1949): Ob odno probleme, kasawes ustoqivosti ”
v
bol~xom“ dinamiqeskih sistem. Uspehi Matematiqeskih Nauk 4(4):187–
188. (Aizerman, M. A. (1949): On a problem concerning the stability "in the
large" of dynamical systems. Uspehi Matematicheskih Nauk 4(4):187–188, in
Russian).
10. Aizerman, M. A., and Gantmacher, F. R. (1964): Absolute Stability of Regu￾lator Systems, Holden-Day.
11. Alberto, L. F. C., and Bretas, N. G. (1998): Damping Estimation for Multi￾Swing Transient Stability Analysis: the OMIB case. Proceedings of the IEEE
International Conference on Power System Technology (POWERCON’98)
(2):1383–1387, Beijing.
© Springer-Verlag GmbH Germany, part of Springer Nature 2024
J. Adamy, Nonlinear Systems and Controls,
https://doi.org/10.1007/978-3-662-68690-4
707708 References
12. Alessio, A., and Bemporad, A. (2009): A Survey on Explicit Model Predictive
Control. In Magni, L., Raimondo, D. M., and Allgöwer, F. (Eds.): Nonlinear
Model Predicitve Control, 345–369, Springer.
13. Allgöwer, F., and Zheng, A. (Eds.) (2000): Nonlinear Model Predictive Con￾trol, Birkhäuser.
14. Alwi, H., and Edwards, C. (2009): Propulsion Control of a Large Civil Aircraft
using On-line Control Allocation. IEEE Proceedings of the American Control
Conference, St. Louis, 4581–4586.
15. Ametani, A., Nagaoka, N., Baba, Y., Ohno, T., and Koichi, Y. (2016): Power
System Transients: Theory and Applications, Taylor & Francis.
16. Anderson, B. D. O. (1968): A Simplified Viewpoint of Hyperstability. IEEE
Transactions on Automatic Control 13(3):292–294.
17. Anderson, B. D. O., and Vongpanitlerd, S. (2006): Network Analysis and Syn￾thesis: A Modern Systems Theory Approach, Dover Publications.
18. Anderson, R. L., and Ibragimov, N. H. (1979) Lie-Bäcklund Transformations
in Applications, SIAM, Philadelphia.
19. Andrievskii, B. R., and Fradkov, A. L. (2003): Control of Chaos: Methods and
Applications I. Methods. Automation and Remote Control 64(5):673–713.
20. Andrievskii, B. R., and Fradkov, A. L. (2003): Control of Chaos: Methods and
Applications II. Applications. Automation and Remote Control 65(4):505–533.
21. Apkarian, P., and Adams, R. J. (1998): Advanced Gain-Scheduling Techniques
for Uncertain Systems. IEEE Transactions on Control Systems Technology
6(1):21–32.
22. Apollo Program Office - MAO (1969): Apollo 11 [AS-506] Mission. NASA,
Mission Operation Report No. M-932-69-11.
23. Atkinson, K., Han, W., and Stewart, D. (2009): Numerical Solutions of Ordi￾nary Differential Equations, Wiley & Sons.
24. Athans, M., and Falb, P. L. (1966): Optimal Control, McGraw-Hill.
25. Auernig, J. W., and Troger, H. (1987): Time optimal control of overhead cranes
with hoisting of the load. Automatica 23(4):437–447.
26. Bacciotti, A., and Rosier, L. (2010): Liapunov Functions and Stability in Con￾trol Theory, Springer.
27. Badami, V. V., and Dausch, M. E. (1994): Fuzzy logic control method for re￾ducing water consumption in a machine for washing articles. U. S. Patent
5,284,523.
28. Barabanov, N. E. (1988): On the Kalman problem. Siberian Mathematical
Journal 29(3):333–341.
29. Barbaxin, E. A. i Krasovski, N. N. (1952): Ob ustoqivosti dvieni v
elom. Doklad Akademii Nauk SSSR 86(3):453–456 (Barbashin, E. A., and
Krasovskii, N. N. (1952): On the stability of motion in the large. Doklady of
Akademii Nauk SSSR 86(3):453–456, in Russian).
30. Barnett, S., and Storey, C. (1970): Matrix Methods in Stability Theory, Barnes
& Noble.
31. Barraquand, J., and Latombe, J.-C. (1993): Nonholonomic Multibody Mobile
Robots: Controllability and Motion Planning in the Presence of Obstacles.
Algorithmica 10(2-3-4):121–155.
32. Barrau, A., and Bonnabel, S. (2017): The Invariant Extended Kalman Filter as
a Stable Observer. IEEE Transactions on Automatic Control 62(4):1797–1812.
33. Bashein, G. (1971): A Simplex Algorithm for On-Line Computation of Time
Optimal Controls. IEEE Transactions on Automatic Control 16(5):479–482.References 709
34. Beckmann, M. J., and Ryder, H. E. (1969): Simultaneous Price and Quantity
Adjustment in a Single Market. Econometrica 37(3):470–484.
35. Ben-Asher, J., Burns, J. A., and Cliff, E. M. (1992): Time-optimal slewing of
flexible spacecraft. Journal of Guidance, Control and Dynamics 15(2):360–367.
36. Benz, K., Rech, C., and Mercorelli, P. (2019): Sustainable Management of
Marine Fish Stocks by Means of Sliding Mode Control. IEEE Proceedings
of the Federated Conference on Computer Science and Information Systems,
Leipzig, 907–910.
37. Bergen, A. R., and Williams, I. J. (1962): Verification of Aizerman’s Conjecture
for a Class of Third-Order Systems. IRE Transactions on Automatic Control
7(3):42–46.
38. Bergman, R. N., Phillips, L. S., and Cobelli, C. (1981): Physiologic Evalua￾tion of Factors Controlling Glucose Tolerance in Man: Measurement of Insulin
Sensitivity and Beta-cell Glucose Sensitivity from the Response to Intravenous
Glucose. The Journal of Clinical Investigation 68(6):1456–1467.
39. Bernard, P., Andrieu, V., and Astolfi, D. (2022): Observer design for
continuous-time dynamical systems. Annual Reviews in Control 53:224–248.
40. Bernstein, D. S. (2018): Scalar, Vector, and Matrix Mathematics: Theory,
Facts, and Formulas, Princeton University Press.
41. Bertsekas, D. P. (1999): Nonlinear Programming, Athena Scientific.
42. Besançon, G. (Ed.) (2007): Nonlinear Observers and Applications, Springer.
43. Bestle, D., and Zeitz, M. (1983): Canonical form observer design for non-linear
time-variable systems. International Journal of Control 38(2):419–431.
44. Bhatia, N. P., and Szegö, G. P. (2002): Stability Theory of Dynamical Systems,
Springer.
45. Bhatta, P. (2006): Nonlinear Stability and Control of Gliding Vehicles, PhD
thesis, Princeton University.
46. Bhatta, P., and Leonard, N. E. (2008): Nonlinear gliding stability and control
for vehicles with hydrodynamic forcing. Automatica 44(5):1240–1250.
47. Birk, J. (1992): Rechnergestützte Analyse and Lösung nichtlinearer Beobach￾tungsaufgaben. VDI Fortschritt-Berichte, Reihe 8, Nr. 294, VDI-Verlag, (Com￾puter aided analysis and solution of nonlinear observation tasks; in German).
48. Birk, J., and Zeitz, M. (1992): Computer-aided analysis of nonlinear observa￾tion problems. 2nd IFAC Symposium on Nonlinear Control Systems (NOL￾COS), Bordeaux. In IFAC Proceedings Volumes 25(3):257–262.
49. Blanchini, F. (1992): Minimum-Time Control for Uncertain Discrete-Time
Linear Systems. Proceedings of the IEEE Conference on Decision and Control,
Tucson (3):2629–2634.
50. Blümel, R., and Reinhardt, W. P. (1997): Chaos in Atomic Physics, Cambridge
University Press.
51. Boccia, A., Grüne, L., and Wortmann, K. (2014): Stability and feasibility of
state constrained MPC without stabilizing terminal constraints. System &
Control Letters 72:14–21
52. Bode, H.W. (1945): Network Analysis and Feedback Amplifier Design, D. Van
Nostrand.
53. Bonnabel, S., and Slotine, J.-J. E. (2014): A Contraction Theory-Based Analy￾sis of the Stability of the Deterministic Extended Kalman Filter. IEEE Trans￾actions on Automatic Control 60(2):565–569.710 References
54. Borovic, B., Hong, C., Liu, A. Q., Xie, L., and Lewis, F. L. (2004): Control of a
MEMS Optical Switch. Proceedings of the 43rd IEEE Conference on Decision
and Control, Atlantis (3):3039–3044.
55. Boyd, S., El Ghaoui, L., Feron, E., and Balakrishnan, V. (1994): Linear Matrix
Inequalities in System and Control Theory, SIAM.
56. Bradley, T.-D., Bryson, T., and Terilla, J. (2020): Topology: A Categorical
Approach. MIT Press.
57. Bragin, V. O., Vagaitsev, V. I., Kuznetsov, N. V., and Leonov, G. A. (2011):
Algorithms for Finding Hidden Oscillations in Nonlinear Systems. The Aizer￾man and Kalman Conjectures and Chua’s Circuits. Journal of Computer and
Systems Sciences International 50(4):511–543.
58. Brogliato, B., Lozano. R., Maschke, B., and Egeland, O. (2007): Dissipative
Systems Analysis and Control, Springer
59. Brown, C. J., and Ma, J. T. (1968): Time-Optimal Control of a Moving-Coil
Linear Actuator. IBM Journal of Research and Development 12(5):372–379.
60. Brunovský, P. (1970): A Classification of Linear Controllable Systems. Kyber￾netika 6(3):173–188.
61. Buhl, M., and Lohmann, B. (2009): Control with Exponentially Decaying Lya￾punov Functions and Its Use for Systems with Input Saturation. IEEE Pro￾ceedings of the 10th European Control Conference, Budapest, 3148–3153.
62. Burcham, F.W., Trindel, A., Fullerton, C. G., and Webb, L. D. (1996): Devel￾opment and Flight Evaluation of an Emergency Digital Flight Control System
Using Only Engine Thrust on an F-15 Airplane. NASA Technical Paper 3627.
63. Burden, R. L., Faires, J. D., and Burden, A.M. (2015): Numerical Analysis,
Cengage Learning.
64. Burken, J. J., Burcham, F.W., Maine, T. A., Feather, J., Goldthorpe, S., and
Kahler, J. A. (1996): Flight Test of a Propulsion-Based Emergency Control
System on the MD-11 Airplane With Emphasis on the Lateral Axis. NASA
Technical Memorandum 4746.
65. Byrnes, C. I., Isidori, A., and Willems, J. C. (1991): Passivity, Feedback Equiv￾alence, and the Global Stabilization of Minimum Phase Nonlinear Systems.
IEEE Transactions on Automatic Control 36(11):1228–1240.
66. Byrnes, C. I., and Isidori, A. (1991): Asymptotic Stabilization of Mini￾mum Phase Nonlinear Systems. IEEE Transactions on Automatic Control
36(10):1122–1137.
67. Camacho, E. F., Berenguel, M., and Rubio, F. R. (1994): Application of a gain
scheduling generalized predictive controller to a solar power plant. Control
Engineering Practice 2(2):227–238.
68. Camacho, E. F., and Bordons, C. (2007): Model Predictive Control, Springer.
69. Canudas-de-Wit, C., Rubio, F. R., and Corchero, M. A. (2008): D-OSKIL: A
New Mechanism for Controlling Stick-Slip Oscillations in Oil Well Drillstrings.
IEEE Transactions on Control Systems Technology 16(6):1177–1191.
70. Carter, J., Chiclana, F., Khuman, A. S., and Chen, T. (Eds.) (2021): Fuzzy
Logic: Recent Applications and Developments, Springer.
71. Casavola, A., Prodi, G., and Rocca, G. (2010): Efficient Gear Shifting Strate￾gies for Green Driving Policies. IEEE Proceedings of the American Control
Conference, Baltimore, 4331–4336.
72. Charlet, B., Lévine, J., and Marino, R. (1991): Sufficient conditions for dy￾namic state feedback linearization. SIAM Journal on Control and Optimiza￾tion 29(1):38–57.References 711
73. Chen, C.-T. (2014): Linear System Theory and Design, Oxford University
Press.
74. Chen, D., Bako, L., and Lecoeuche, S. (2012): The minimum-time prob￾lem for discrete-time linear systems: a non-smooth optimization approach.
Proceedings of the IEEE International Conference on Control Applications,
Dubrovnik, 196–201.
75. Chen, G., and Pham, T. T. (2000): Introduction to Fuzzy Sets, Fuzzy Logic,
and Fuzzy Control Systems, Taylor & Francis.
76. Chen, G., and Ueta, T. (1999): Yet another chaotic attractor. International
Journal of Bifurcation and Chaos 9(7):1465–1466.
77. Chen, G. (2023): Nonlinear Systems: Stability, Dynamics and Control, World
Scientific.
78. Chen, M. Z. Q., and Smith, M. C. (2009): A Note on Tests for Positive-Real
Functions. IEEE Transactions on Automatic Control 54(2):390–393.
79. Cheng, P., Vasić, M., García, O, Oliver, J. Á., Alou, P., and Cobos, J. A.
(2014): Minimum Time Control for Multiphase Buck Converter: Analysis and
Application. IEEE Transactions on Power Electronics 29(2):958–967.
80. Chern, S. S. und Shokurov, V. V. (Eds.) (2002): The Collected Papers of Wei￾Liang Chow, World Scientific.
81. Chow, W.-L. (1940): Über Systeme von linearen partiellen Differentialgle￾ichungen erster Ordnung. Mathematische Annalen 117(1):98–105, (On systems
of linear partial differential equations of first order; in German).
82. Consolini, L., and Piazzi, A. (2009): Generalized bang-bang control for feed￾forward constrained regulation. Automatica 45(10):2234–2243.
83. Corless, M., and Shorten, R. (2009): A correct characterization of strict posi￾tive realness for MIMO systems. IEEE Proceedings of the American Control
Conference, St. Louis, 469–475.
84. Corless, M., and Shorten, R. (2010): On the Characterization of Strict Pos￾itive Realness for General Matrix Transfer Functions. IEEE Transactions on
Automatic Control 55(8):1899–1904.
85. Corless, M., and Shorten, R. (2011): On a class of generalized eigenvalue prob￾lems and equivalent eigenvalue problems that arise in systems and control
theory. Automatica 47(3):431–442.
86. Cragon, S. D. (1991): Time-Optimal Control Application for an Exoatmo￾spheric Interceptor. Proceedings of the IEEE Telesystems Conference (1):31–
36.
87. Creveling, H. F., De Paz, J. F., Baladi, J. Y., and Schoenhals, R. J. (1975):
Stability characteristics of a single-phase free convection loop. Journal of Fluid
Mechanics 67(1):65–84.
88. Davison, E., and Constantinescu, D. (1971): A Describing Function Technique
for Multiple Nonlinearities in a Single-Loop Feedback System. IEEE Transac￾tions on Automatic Control 16(1):56–60.
89. DeCarlo, R. A., Zak, S. H., and Matthews, G. P. (1988): Variable Structure
Control of Nonlinear Multivariable Systems: A Tutorial. Proceedings of the
IEEE 76(3):212–232.
90. Demirci, Y, Pekel, L. C., Altinten, A., and Alpbaz, M. (2015): Application of
fuzzy control on the electrocoagulation process to treat textile wastewater.
Environmental Technology 36(24):3243–3252.712 References
91. de Paor, A., and Ringwood, J. (2006): A Simple Soft Limiter Describing Func￾tion for Biomedical Applications. IEEE Transactions on Biomedical Engineer￾ing 53(7):1233–1240.
92. Devaney, R. L. (2003): An Introduction to Chaotic Dynamical Systems, West￾view Press Inc.
93. Dewey, A. G., and Jury, E. I. (1965): A Note on Aizerman’s Conjecture. IEEE
Transactions on Automatic Control 10(4):482–483.
94. Diehl, M., Findeisen, R., Schwarzkopf, S., Uslu, I., Allgöwer, F., Bock, H. G.,
Gilles, E. D., and Schlöder, J. P. (2002): An Efficient Algorithm for Nonlinear
Model Predictive Control of Large-Scale Systems - Part I: Description of the
Method. Automatisierungstechnik 50(12):557–567.
95. Di Martino, F., and Sessa, S. (2020): Fuzzy Transforms for Image Processing
and Data Analysis, Springer.
96. Dodds, S. J. (2015): Feedback Control: Linear, Nonlinear and Robust Tech￾niques and Design with Industrial Applications, Springer.
97. Döring, D. (2003): Ein Beitrag zur Synthese von Gain-Schedulingreglern unter
Nutzung normierter Gaußscher Radial-Basisfunktionen, PhD thesis, Univer￾sität Magdeburg, (A Contribution to the Synthesis of Gain-Scheduling Con￾trollers Using Normalized Gaussian Radial Basis Functions; in German).
98. Dorf, R. C., and Bishop, R. H. (2016): Modern Control Systems, Prentice Hall.
99. Dormand, J. R., and Prince, P. J. (1980): A family of embedded Runge-Kutta
formulae. Journal of Computational and Applied Mathematics 6(1):19–26.
100. Draženóvic, B. (1969): The invariance conditions in variable structure systems.
Automatica 5(3):287–295.
101. Drexler, D., Sápi, J., Szeles, A., Harmati, I., Kovács, A., and Kovács, L. (2012):
Flat control of tumor growth with angiogenic inhibition. Proceedings of the
7th IEEE International Symposium on Applied Computational Intelligence
and Informatics, Timişoara, 179–183.
102. Driankov, D., Hellendoorn, H., and Reinfrank, M. (1996): An Introduction to
Fuzzy Control, Springer.
103. Edwards, C., and Postlethwaite, I. (1998): Anti-windup and bumpless-transfer
schemes. Automatica 34(2):199–210.
104. Emel~nov, S. V. (1967): Sistemy avtomatiqeskogo upravleni s peremen￾no strukturo, Nauka (Russian original publication of [105]).
105. Emel’yanov, S. V. (1967): Variable-Structure Control Systems, Nauka.
106. Emel’yanov, S. V. (2007): Theory of variable-structure control systems: In￾ception and initial development. Computational Mathematics and Modeling
18(4):321–331.
107. Engell, S. (Ed.) (1995): Entwurf nichtlinearer Regelungen, R. Oldenbourg,
(Nonlinear control design; in German).
108. Engeln-Müllges, G., and Uhlig, F. (1996): Numerical Algorithms with C,
Springer.
109. Fadden, E. J., and Gilbert, E. G. (1964): Computational Aspects of the Time￾Optimal Control Problem. In Balakrishnan, A.V., and Neustadt, L.W. (Eds.):
Computing Methods in Optimization Problems: 167–192, Academic Press.
110. Farinwata, S. S., Filev, D. P., and Langari, R. (2000): Fuzzy Control: Synthesis
and Analysis, Wiley & Sons.
111. Fel~dbaum, A. A. (1955): O sinteze optimal~nyh sistem s pomow~
fazovogo prostranstva. Avtomatika i telemehanika 16(2):129–149References 713
(Fel'dbaum, A. A. (1955): On the synthesis of optimal systems with the aid of
the phase space. Automatica y Telemehanika 16(2):129–149, in Russian).
112. Fel~dbaum, A. A. (1959): Vyqislitel~nye ustrostva v avtomatiqeskih
sistemah, Fizmatgiz, Moskva (Russian original publication of [113]).
113. Feldbaum, A. A. (1962): Rechengeräte in automatischen Systemen, R. Olden￾bourg, (Computers in Automatic Systems; in German).
114. Fel'dbaum, A. A. (1965): Optimal Control Systems, Academic Press.
115. Ferfera, A., and Hammami, M. A. (1995): Growth Conditions for Global Sta￾bilization of Cascade Nonlinear Systems. 3rd IFAC Conference on System
Structure and Control, Nantes. In IFAC Proceedings Volumes 28(8):471–474.
116. Fernández-Anaya, G., Martínez-García, J. G., and Ku˘cera, V. (2006): Char￾acterizing families of positive real matrices by matrix substitution on scalar
rational functions. System & Control Letters 55(11):871–878.
117. Fernández, J. P., Vargas, M. A., García, J. M. V., Carrillo, J. A. C, and Aguilar,
J. J. C. (2021): Coevolutionary Optimization of a Fuzzy Logic Controller for
Antilock Braking Systems Under Changing Road Conditions. IEEE Transac￾tions on Vehicular Technology 70(2):1255–1268.
118. Ferrante, A., Lanzon, A., and Brogliato, B. (2020): A Direct Proof of the
Equivalence of Side Conditions of Strictly Positive Real Transfer Functions.
IEEE Transactions on Automatic Control 65(1):450–452.
119. Filippov, A. F. (1988): Differential Equations with Discontinuous Righthand
Sides, Kluwer Academic Publishers.
120. Findeisen, R., Imsland, L., Allgöwer, F., and Foss, B. A. (2003): State and
Output Feedback Nonlinear Model Predictive Control: An Overview. Euro￾pean Journal of Control 9(2-3):190–206.
121. Findeisen, R., and Allgöwer, F. (2004): Computational delay in nonlinear
model predictive control. Proceedings of the 7th IFAC International Sympo￾sium on Advanced Control of Chemical Processes (ADCHEM), Hong Kong.
In IFAC Proceedings Volumes 37(1):427–432.
122. Fitts, R. (1966): Two Counterexamples to Aizerman’s Conjecture. IEEE
Transactions on Automatic Control 11(3):553–556.
123. Fliess, M. (1990): Generalized Controller Canonical Form for Linear and Non￾linear Dynamics. IEEE Transactions on Automatic Control, 35(9):994–1001.
124. Fliess, M., Lévine, J., Martin, P., and Rouchon, P. (1992): Sur les systèmes non
linéaires différentiellement plats. Comptes Rendus de l’Académie des Science,
Serie I-315:619–624, (On differentially flat nonlinear systems; in French).
125. Fliess, M., Lévine, J., Martin, P., and Rouchon, P. (1992): On differentially
flat nonlinear systems. 2nd IFAC Symposium on Nonlinear Control Systems
(NOLCOS), Bordeaux. In IFAC Proceedings Volumes 25(13):159–163.
126. Fliess, M., Lévine, J., Martin, P., and Rouchon, P. (1994): Nonlinear con￾trol and Lie-Bäcklund transformations: Towards a new differential geometric
standpoint. Proceedings of the 33rd IEEE Conference on Decision and Con￾trol, Lake Buena Vista, 1:339–344.
127. Fliess, M., Lévine, J., Martin, P., and Rouchon, P. (1995): Flatness and de￾fect of non-linear systems: introductory theory and examples. International
Journal of Control 61(6):1327–1361.
128. Fliess, M., Lévine, J., Martin, P., Ollivier, F., and Rouchon, P. (1997): Con￾trolling Nonlinear Systems by Flatness. In Byrnes, C. I., Datta, B. N., Gilliam,
D. S., and Martin, C. F. (Eds.): Systems and Control in the Twenty-First Cen￾tury, 137–154, Birkhäuser.714 References
129. Fliess, M., Lévine, J., Martin, P., and Rouchon, P. (1995): A Lie-Bäcklund
Approach to Equivalence and Flatness of Nonlinear Systems. IEEE Trans￾actions on Automatic Control 44(5):922–937.
130. Föllinger, O. (2022): Regelungstechnik, VDE-Verlag, (Control Engineering; in
German).
131. Fox, L., and Mayers, D. F. (1987): Numerical Solution of Ordinary Differential
Equations for Scientists and Engineers, Chapman and Hall.
132. Franke, D. (1982): Ausschöpfung von Stellgrößenbeschränkungen mittels wei￾cher strukturvariabler Regelung. Regelungstechnik 30(10):348–355, (Exhaust￾ing bounds on control by means of soft variable structure control; in German).
133. Franklin, G. F., Powell, J. D., Emami-Naeini, A. (2018): Feedback Control of
Dynamic Systems, Prentice-Hall.
134. Fridman, L., Moreno, J., and Iriarte, R. (2011): Sliding Modes after the First
Decade of the 21st Century, Springer.
135. Friedland, B. (2005): Control System Design: An Introduction to State-Space
Methods, Dover Publications.
136. Frik, M. (1966): Zeitoptimale Steuerung des Einstellvorganges bei Kreiselkom￾passen. Archive of Applied Mechanics 35(4):262–268, (Time-optimal control
of the adjustment process of gyrocompasses; in German).
137. Fritzsche, K., and Röbenack, K.(2018): On the Computation of Differentially
Flat Inputs. IEEE Proceedings of the 22nd International Conference on System
Theory, Control, and Computing (ICSTCC), Sinaia, 12–19.
138. Fritzsche, K., and Röbenack, K.(2020): On a generalized flat input definition
and physical realizability. 21st IFAC World Congress, Berlin. In IFAC Papers￾Online 53(2):5994–5999.
139. Fujii, K, and Shoji, K. (1972): Verification of the Aizerman and/or Kalman
Conjecture. IEEE Transactions on Automatic Control 17(3):406–408.
140. Fukushima, Y., Irikura, K., Uetake, T., and Matsumoto, H. (2000): Character￾istics of Observed Peak Amplitude for Strong Ground Motion from the 1995
Hyogoken Nanbu (Kobe) Earthquake. Bulletin of the Seismological Society of
America 90(3):545–565.
141. Fuller, A (1974): Simplification of Some Time-Optimal Switching Functions.
IEEE Transactions on Automatic Control 19(1):65–65.
142. Galeani, S., Tarbouriech, S., Turner, M., and Zaccarian, L. (2009): A Tutorial
on Modern Anti-Windup Design. IEEE Proceedings of the 10th European
Control Conference, Budapest, 306–323.
143. Galor, O. (2007): Discrete Dynamical Systems, Springer.
144. Gantmacher, F. R. (1984): The Theory of Matrices, American Mathematical
Society.
145. Gao, W., and Hung, J. C. (1993): Variable Structure Control of Nonlinear Sys￾tems: A New Approach. IEEE Transactions on Industrial Electronics 40(1):45–
55.
146. Gauthier, J. P., and Bornard, G. (1981): Observability for any u(t) of a Class of
Nonlinear Systems. IEEE Transactions on Automatic Control 26(4):922–926.
147. Gelb, A., and Vander Velde, W. E. (1968): Multiple-Input Describing Func￾tions and Nonlinear System Design, McGraw-Hill.
148. Germann, S., and Isermann, R. (1995): Nonlinear distance and cruise control
for passenger cars. IEEE Proceedings of the American Control Conference,
Washington, 3081–3085.References 715
149. Germann, S. (1997): Modellbildung und modellgestützte Regelung der
Fahrzeuglängsdynamik. VDI Fortschritt-Berichte, Reihe 12, Nr. 309, VDI￾Verlag, (Modeling and Model-Based Control of Vehicle Longitudinal Dynam￾ics; in German).
150. Ghandhari, M., Andersson, G., Pavella, M., and Ernst, D. (2001): A control
strategy for controllable series capacitor in electric power systems. Automatica
37(10):1575–1583.
151. Giesl, P., and Hafstein, S. (2015): Review on computational methods for Lya￾punov functions. Discrete and Continuous Dynamical Systems 20(8):2291–
2331.
152. Glad, T., and Ljung, L. (2000): Control Theory: Multivariable and Nonlinear
Methods, Taylor & Francis.
153. Glattfelder, A. H., and Schaufelberger, W. (2003): Control Systems with Input
and Output Constraints, Springer.
154. Gordon, W. B. (1972): On the Diffeomorphisms of Euclidean Spaces. The
American Mathematical Monthly 79(7):755–759.
155. Gowers, T., Barrow-Green, J., and Leader, I. (Eds.) (2008): The Princeton
Companion to Mathematics, Princeton University Press.
156. Grancharova, A., and Johansen, T. A. (2012): Explicit Nonlinear Model Pre￾dictive Control, Springer.
157. Grimble, M J., and Majecki, P. (2020): Nonlinear Industrial Control Systems,
Springer.
158. Grognard, F., Sepulchre, R., and de Dona, J. (2001): Dynamical Systems
that Compute Time-Optimal Switchings. 5th IFAC Symposium on Nonlinear
Control Systems (NOLCOS), St. Petersburg. In IFAC Proceedings Volumes
34(6):1189–1194.
159. Grognard, F., and Sepulchre, R.. (2001): Global analysis of a continuous-time
flow which computes time-optimal switchings. Proceedings of the 40th IEEE
Conference on Decision and Control, Orlando, 3826–3831.
160. Grossman, H. I. (1974): The Nature of Quantities in Market Disequilibrium.
The American Economic Review 64(3):509–514.
161. Grüne, L., and Pannek, J. (2011): Nonlinear Model Predictive Control,
Springer.
162. Gruyitch, L., Richard, J.-P., Borne, P., and Gentina, J.-C. (2004): Stability
Domains, Chapman & Hall.
163. Gupta, S., and Joshi, S. M. (1995): State Space Characterization and Robust
Stabilization of Dissipative LTI Systems. IEEE Proceedings of the American
Control Conference, Seattle, 3616–3619.
164. Gurel, O., and Lapidus, L. (1969): A Guide to the Generation of Liapunov
Functions. Industrial and Engineering Chemistry 61(3):30–41.
165. Gußner, T., Jost, M., and Adamy, J. (2012): Controller design for a class of
nonlinear systems with input saturation using convex optimization. Systems
& Control Letters 61(1):258–265.
166. Gutman, P.-O., and Hagander, P. (1985): A New Design of Constrained
Controllers for Linear Systems. IEEE Transactions on Automatic Control
30(1):22–33.
167. Guzman, A. (2003): Derivatives and Integrals of Multivariable Functions,
Birkhäuser.
168. Haddad, W. M., and Chellaboina, V. (2008): Nonlinear Dynamical Systems
and Control: A Lyapunov-Based Approach, Princeton University Press.716 References
169. Hagenmeyer, V., and Zeitz, M. (2004): Internal dynamics of flat nonlinear
SISO systems with respect to a non-flat output. Systems & Control Letters
52(3-4):323–327.
170. Hahn, W. (1967): Stability of Motion, Springer.
171. Hahnfeldt, P., Panigrahy, D., Folkman, J., and Hlatky, L. (1999): Tumor
Development under Angiogenic Signaling: A Dynamical Theory of Tumor
Growth, Treatment Response, and Postvascular Dormancy. Cancer Research
59(19):4770–4775.
172. Hairer, E., Nørsett, S. P., and Wanner, G. (1993): Solving Ordinary Differential
Equations I: Nonstiff Problems, Springer.
173. Hairer, E., and Wanner, G. (2004): Solving Ordinary Differential Equations
II: Stiff and Differential-Algebraic Problems, Springer.
174. Hamaya, K., Waita, H., Gomi, H., and Shin, J. (2014): Inverted Pendulum
type vehicle. U. S. Patent 8,763,733 B2.
175. Hangos, K. M., Bokor, J., and Szederkényi, G. (2004): Analysis and Control
of Nonlinear Process Systems, Springer.
176. Härefors, M., and Bates, D. G. (2003): Integrated propulsion based flight con￾trol system design for a civil transport aircraft. International Journal of Turbo
and Jet Engines 20(2):95–114.
177. Harris, J. (2005): Fuzzy Logic Applications in Engineering Science, Springer.
178. Haxel, J. H., Matsumoto, H., Meining, C., Kalbach, G., Lau, T.-K., Dziak,
R. P., and Stalin, S. (2019): Ocean sound levels in the northeast Pacific
recorded from an autonomous underwater glider. PLoS One 14(11):1–20.
179. Haynes, G.W., and Hermes, H. (1970): Nonlinear Controllability via Lie The￾ory. SIAM Journal on Control 8(4):450–460.
180. Hedrick, J. K., and Misawa, E. A. (1989): Nonlinear Observers – A State￾of-the-Art Survey. Transactions of the ASME, Journal of Dynamic Systems,
Measurement, and Control 111(3):344–352.
181. Hendricks, E., Jannerup, O., and Sørensen, P. H. (2008): Linear Systems Con￾trol: Deterministic and Stochastic Methods, Springer.
182. Hermann, R., and Krener, A. J. (1977): Nonlinear Controllability and Observ￾ability. IEEE Transactions on Automatic Control 22(5):728–740.
183. He, R., Woods, W., Zambon, J. B., and Xue, Z. (2016): Monitoring the Gulf
Stream and shelf environment in the South Atlantic Bight through integrated
autonomous underwater glider observations and data assimilative ocean model
predictions. IEEE Proceedings of Oceans 2016, Shanghai, 1-4.
184. Hibbeler, R. C. (2016): Engineering Mechanics: Dynamics, Pearson.
185. Hippe, P. (2006): Windup in Control, Springer.
186. Hocking, L. M. (1991): Optimal Control, Oxford University Press.
187. Holzmann, H., Germann, S., Halfmann, C., and Isermann, R. (1998): Intelli￾gent fuzzy distance and cruise control for passenger cars. Journal of Intelligent
and Fuzzy Systems 6(3):315–327.
188. Honma, H. (1996): Automatic focusing system with response control using
fuzzy interference. U. S. Patent 5,566,380.
189. Hooke, R., and Jeeves, T. A. (1961): „Direct Search“ Solution of Numerical
and Statistical Problems. Journal of the Association of Computing Machinery
(JACM) 8(2):212–229.
190. Hou, C. (2008): Fluid Dynamics and Behavior of Nonlinear Viscous Fluid
Dampers. Journal of Structural Engineering 134(1):56–63.References 717
191. Hu, T., and Lin, Z. (2001): Control Systems with Actuator Saturation: Anal￾ysis and Design, Birkhäuser.
192. Hu, T., and Lin, Z. (2002): On Improving the Performance with Bounded Con￾tinuous Feedback Laws. IEEE Transactions on Automatic Control 47(9):1570–
1575.
193. Huang, S., Tan, K. K., and Lee, T. H. (2001): Applied Predictive Control,
Springer.
194. Hughes, D., and Dornheim, M. A. (2003): No Flight Controls. Aviation Week
& Space Technology 159(23):42–43.
195. Hughes, P. C. (2004): Spacecraft Attitude Dynamics, Dover Publications.
196. Hung, J. Y., Gao, W., and Hung, J. C. (1993): Variable Structure Control: A
Survey. IEEE Transactions on Industrial Electronics 40(1):2–22.
197. Hunt, L. R., Su, R., Meyer, G., and Lin, Z. (1983): Design for multi-input non￾linear systems. In Brockett, R.W., Millman, R. S., and Sussman, H. J. (Eds.):
Differential Geometric Control Theory. Proceedings of the Conference Held
at Michigan University, Volume 27 of Progress in Mathematics, Birkhäuser,
268–297.
198. Hunt, L. R., Su, R., and Meyer, G. (1983): Global Transformations of Nonlin￾ear Sytems. IEEE Transactions of Automatic Control 28(1):24–31.
199. Ilchmann, A., and Wirth, F. (2013): On Minimum Phase. Automatisierungs￾technik 61(12):805–817.
200. Isermann, R. (1989): Digital Control Systems: Fundamentals, Deterministic
Control, Springer.
201. Isermann, R. (1991): Digital Control Systems: Stochastic Control, Multivari￾able Control, Adaptive Control, Applications, Springer.
202. Isermann, R. (2022): Automotive Control: Modeling and Control of Vehicles,
Springer.
203. Isidori, A. (1995): Nonlinear Control Systems, Springer.
204. Itkis, U. (1976): Control Systems of Variable Structure, Wiley & Sons.
205. Jadbabaie, A., and Hauser, (2005): On the Stability of Receding Horizon Con￾trol with a General Terminal Cost. IEEE Transactions on Automatic Control
50(5):674–678.
206. Jakubczyk, B., and Respondek, W. (1980): On linearization of control systems.
Bulletin de L’Académie Polonaise des Sciences, Série des Sciences Mathéma￾tiques 28(9-10):517–522.
207. Jin, Y. (2003): Advanced Fuzzy Systems Design and Applications, Physica￾Verlag.
208. Jo, N. H., and Seo, J. H. (2000): Input Output Linearization Approach to State
Observer Design for Nonlinear System. IEEE Transactions on Automatic Con￾trol 45(12):2388–2393.
209. Johansen, T. A., Hunt, K. J., and Petersen, I. (2000): Gain-scheduled control
of a solar power plant. Control Engineering Practice 8(9):1011–1022.
210. Jonckheere, E. A., and Yu, G.-R. (1998): H∞ longitudinal control of crippled
trijet aircraft with throttles only. Control Engineering Practice 6(5):601–613.
211. Jonckheere, E. A., and Yu, G.-R. (1999): Propulsion Control of Crippled Air￾craft by H∞ Model Matching. IEEE Transaction on Control Systems Tech￾nology 7(2):142–159.
212. Jordan, L. B. (1995): Locomotive traction control system using fuzzy logic.
U. S. Patent 5,424,948.718 References
213. Joshi, S. M. (1989): Out of Control. IEEE Control Systems Magazine 9(5):50.
214. Joshi, S.M., and Gupta, S. (1996): On a Class of Marginally Stable Positive￾Real Systems. IEEE Transactions on Automatic Control 41(1):152–155.
215. Kahraman, C. (Ed.) (2006): Fuzzy Applications in Industrial Engineering,
Springer.
216. Kailath, T. (1980): Linear Systems, Prentice Hall.
217. Kalman, R. E. (1957): Physical and Mathematical Mechanisms of Instability in
Nonlinear Automatic Control Systems. Transactions of the American Society
of Mechanical Engineers (ASME) 79(3):553–563.
218. Kalman, R. E. (1960): On the General Theory of Control Systems. 1st Inter￾national IFAC Congress on Automatic and Remote Control, Moscow. In IFAC
Proceedings Volumes 1(1):491–502.
219. Kalman, R. E., and Bucy, R. S. (1961): New Results in Linear Filtering and
Prediction Theory. Transactions of the ASME, Journal of Basic Engineering
83(1):95–108.
220. Kamen, D. L., Arling, R.W., Field, J. D., Morell, J. B., Pompa, J. B., and
Heinzmann, R. K. (2002): Personal Transporter. U. S. Patent 6,796,396 B2.
221. Kane, T. R., Likins, P.W., and Levinson, D. A. (1983): Spacecraft Dynamics,
McGraw Hill.
222. Kapila, V., and Grigoriadis, K. M. (2002): Actuator Saturation Control, Mar￾cel Dekker.
223. Kapoor, N., and Daoutidis, P. (2000): Stabilization of nonlinear processes with
input constraints. Computers and Chemical Engineering 24:9–21.
224. Kaps, P., and Rentrop, P. (1979): Generalized Runge-Kutta methods of order
four with stepsize control for stiff ordinary differential equations. Numerische
Mathematik 33(1):55–68.
225. Karnopp, D. (2001): Motorized power steering system, German Patent
DE 40 31 316 C2.
226. Kaur, R., and Singh, A. (2019): Fuzzy logic: an overview of different appli￾cation areas. Advances and Applications in Mathematical Sciences 18(8):677–
689.
227. Keerthi, S., and Gilbert, E. (1987): Computation of Minimum-Time Feed￾back Control Laws for Discrete-Time Systems with State-Control Constraints.
IEEE Transactions on Automatic Control 32(5):432–435.
228. Kelkar, A. G., and Joshi, S. M. (1996): Control of Nonlinear Multibody Flexible
Space Structures, Springer.
229. Keller, H. (1986): Entwurf nichtlinearer Beobachter mittels Normalformen.
VDI Fortschritt-Berichte, Reihe 8, Nr. 124, VDI-Verlag, (Design of Nonlinear
Observers Using Normal Forms; in German).
230. Kellett, C. M., and Braun, P. (2023): Introduction to Nonlinear Control: Sta￾bility, Control Design, and Estimation, Princeton University Press.
231. Kelley, W. G., and Peterson, A. C. (2001): Difference Equations: An Introduc￾tion with Applications, Academic Press.
232. Khalil, H. K. (2002): Nonlinear Systems, Prentice-Hall.
233. Kiel, L. D., and Elliott, W. E. (Eds.) (1997): Chaos Theory in the Social Sci￾ences: Foundations and Applications, University of Michigan Press.
234. Kiendl, H., and Schneider, G. (1972): Synthese nichtlinearer Regler für die
Regelstrecke const/s2
aufgrund ineinandergeschachtelter abgeschlossener Ge￾biete beschränkter Stellgröße. Regelungstechnik und Prozeß-Datenverarbeit-References 719
ung 20(7):289–296, (Synthesis of nonlinear controllers for the controlled
system const/s2
due to nested closed sets of limited actuator variable; in
German).
235. Kiendl, H. (1972): Suboptimale Regler mit abschnittweise linearer Struktur,
Springer, (Suboptimal Controllers with Sectional Linear Structure; in Ger￾man).
236. Kim, M.-H., and Engell, S. (1994): Speed-up of Linear Programming for Time￾Optimal Control. IEEE Proceedings of the American Control Conference, Bal￾timore, 3:2667–2670.
237. Kirk, D. E. (2004): Optimal Control Theory: An Introduction, Dover Publica￾tions.
238. Korotayev, A. (2005): A Compact Macromodel of World System Evolution.
Journal of World-Systems Research 11(1):79–93.
239. Kothare, M. V., Campo, P. J., Morari, M., and Nett, C. N. (1994): A unified
framework for the study of anti-windup designs. Automatica 30(12):1869–
1883.
240. Kottenstette, N., and Antsaklis, P. J. (2010): Relationships Between Positive
Real, Passive Dissipative, & Positive Systems. IEEE Proceedings of the Amer￾ican Control Conference, Baltimore, 409–416.
241. Kouvaritakis, B., and Cannon, M. (Eds.) (2001): Nonlinear Predictive Control:
theory and practice, The Institution of Engineering and Technology.
242. Kovács, L., György, A., Szalay, P., Benyó, B., Benyó, Z., Hann, C. E., and
Chase, J. G. (2010): Investigating the Applicability of qALPV Modeling to
ICU Models for Glycaemic Control. IEEE Proceedings of the UKACC Inter￾national Conference on Control, Coventry, 1–6.
243. Krämer, W. (2006): Grenzzyklen in einem Servolenksystem. Arbeitsberichte
der FH Ingolstadt, Nr. 12, (Limit cycles in a servo steering systems; in Ger￾man).
244. Krämer, W. (2007): Vermeidung von Grenzzyklen in einem Servolenksystem.
In Mechatronik 2007 - Innovative Produktentwicklung: Maschinenbau, Elek￾trotechnik, Informationstechnik, VDI-Berichte (2007), Nr. 1971, S. 811–818,
VDI-Verlag, (Avoiding limit cycles in a servo steering system; in German).
245. Krantz, S. G., and Parks, H. R. (2013): The Implicit Function Theorem.
Birkhäuser.
246. Krasovski, N. N. (1952): Teoremy ob ustoqivosti dvieni, oprdele￾myh sistemo dvuh uravneni. Prikladna matematika i mahanika
16(5):547–554 (Krasovskii, N. N. (1952): Theorems on stability of motions
based on a system of two equations. Prikladnaia matematika i mekhanika
16(5):547–554, in Russian).
247. Krause, P. C., Wasynczuk, O., and Sudhoff, S. D. (2013): Analysis of Electric
Machinery and Drive Systems, Wiley & Sons.
248. Krener, A. J., and Isidori, A. (1983): Linearization by output injection and
nonlinear observers. Systems & Control Letters 3(1):47–52.
249. Kroll, A. (2013): Computational Intelligence, R. Oldenbourg.
250. Krstić, M., Fontaine, D., Kokotović, P. V., and Paduano, J. D. (1998): Useful
Nonlinearities and Global Stabilization of Bifurcations in a Model of Jet En￾gine Surge and Stall. IEEE Transactions on Automatic Control 43(12):1739–
1745.
251. Krstić, M., Kanellakopoulos, I., and Kokotović, P. V. (1995): Nonlinear and
Adaptive Control Design, Wiley & Sons.720 References
252. Krstić, M., and Smyshlyaev, A. (2008): Boundary Control of PDEs: A Course
on Backstepping Designs, SIAM.
253. Kugi, A. (2001): Non-linear Control Based on Physical Models, Springer.
254. Kundur, P. (1994): Power System Stability and Control, McGraw-Hill.
255. Kuznetsov, N. V., Kuznetsova, O. A., Koznov, D., Mokaev, R. N., and An￾drievsky, B. (2018): Counterexamples to the Kalman Conjectures. 5th IFAC
Conference on Analysis and Control of Chaotic Systems (CHAOS), Eindhoven.
In IFAC PapersOnLine 51(33):138–143.
256. Kvasnica, M., Jones, C. N., Pejcic, I., Holaza, J., Korda, M., and Bakaráč,
P. (2019): Real-Time Implementation of Explicit Model Predictive Control.
In Raković, S. V., and Levine, W. S. (Eds.): Handbook of Model Predictive
Control, 387–412, Birkhäuser.
257. Lafferriere, G., and Sussmann, H. (1991): Motion planning for controllable
systems without drift. Proceedings of the IEEE International Conference on
Robotics and Automation, Sacramento, 1148–1153.
258. Lancaster, P., and Tismenetsky, M. (1985): The Theory of Matrices, Academic
Press.
259. Landau, I. D., and Zito, G. (2006): Digital Control Systems, Springer.
260. Larsen, M., and Kokotović, P. V. (2001): A brief look at the Tsypkin criterion:
from analysis to design. International Journal of Adaptive Control and Signal
Processing 15(2):121–128.
261. LaSalle, J. P. (1960): Some Extensions of Liapunov’s Second Method. IRE
Transactions on Circuit Theory 7(4):520–527.
262. Lee, Y., Jang, S.-I., Chung, K., Lee, D., Kim, W., and Lee, C.-W. (1994):
A Fuzzy-Control Processor for Automatic Focusing. IEEE Transactions on
Consumer Electronics 40(2):138–144.
263. Lee, J.-H., Hyeon, J.-W., Jung, S.-K., Lee, Y.-K., and Ko, S.-H.(2020): Obser￾vations of Temperature and Salinity Mesoscale Variability off the East Coast
of Korea using an Underwater Glider: Comparison with Ship CTD Survey
Data. Journal of Coastal Research 95(Special Issue):1167–1171.
264. Lein, M. (2022): A Mathematical Journey Through Differential Equations Of
Physics, World Scientific.
265. Leith, D. J., and Leithead, W. E. (1998): Gain-scheduled controller design: an
analytic framework directly incorporating non-equilibrium plant dynamics.
International Journal of Control 70(2):249–269.
266. Leith, D. J., and Leithead, W. E. (2000): Survey of gain-scheduling analysis
and design. International Journal of Control 73(11):1001–1025.
267. Lens, H., Adamy, J., and Domont-Yankulova, D. (2011): A fast nonlinear con￾trol method for linear systems with input saturation. Automatica 47(4):857–
860.
268. Lentz, W. (1993): Neuere Entwicklungen in der Theorie dynamischer Systeme
and ihre Bedeutung für die Agrarökonomie. Duncker & Humblot, (Recent
developments in the theory of dynamic systems and their relevance for agri￾cultural economics; in German).
269. Leonhard, W. (1976): Zeitoptimale Scherenregelung. Archiv für Elektrotechnik
58(1):61–67, (Time-optimal shear control; in German).
270. Leonov, G. A., Burkin I. M., and Shepeljavyi, A. I. (1996): Frequency Methods
in Oscillation Theory, Kluwer Academic Publishers.References 721
271. Leonov, G. A., Bragin, V. O., and Kuznetsov, N. V. (2010): Algorithm for
constructing counterexamples to the Kalman problem. Doklady Mathemat￾ics 82(1):540–542.
272. Lévine, J. (1997): Static and dynamic feedback linearization. In Fossard, A. J.,
and Normand-Cyrot, D. (Eds.): Nonlinear Systems - Vol. 3, Control: 93–126,
Chapman & Hall.
273. Lévine, J. (2009): Analysis and Control of Nonlinear Systems - A Flatness￾based Approach, Springer.
274. Levine, W. S. (2011): The Control Handbook, Taylor & Francis.
275. Li, A., and Ando, K. (2013): Measuring the Acceptability of Self-Balancing
Two-Wheeled Personal Mobility Vehicles. Journal of the Eastern Asia Society
for Transportation Studies 10:444–453.
276. Li, T.-Y., and Yorke, J. A. (1975): Period Three Implies Chaos. The American
Mathematical Monthly 82(10):985–992.
277. Li, X.-J. (1963): On the absolute stability of systems with time lags. Chinese
Mathematics 3(4):609–626.
278. Liao, X., and Yu, P. (2008): Absolute Stability of Nonlinear Control Systems,
Springer.
279. Liapounoff, A.M. (1907): Problème générale de la stabilité du mouvement.
Translation of the original publication [292] by M. É. Davaux. Annales de la
Faculté des Sciences de l’Université de Toulouse, 2
e
série, 9:203–474.
280. Liberzon, D. (2003): Switching in Systems and Control, Birkhäuser.
281. Liberzon, M. R. (2006): Essays on the Absolute Stability Theory. Automation
and Remote Control 67(10):1610–1644.
282. Limon, D., Alamo, T., Salas, F., and Camacho, E. F. (2006): On the Stabil￾ity of Constrained MPC without Terminal Constraint. IEEE Transactions on
Automatic Control 51(5):832–836.
283. Lin, Y., and Sontag, E. D. (1991): A universal formula for stabilization with
bounded controls. System & Control Letters 16(6):393–397.
284. Llamas, A., De La Ree Lopez, J., Mili, L., Phadke, A. G., and Thorp, J. S.
(1995): Clarifications of the BCU Method for Transient Stability Analysis.
IEEE Transactions on Power Systems 10(1):210–219.
285. Lobry, C. (1970): Contrôlabilité des systèmes non linéaires. SIAM Journal on
Control 8(4):573–605, (Controllability of nonlinear systems; in French).
286. Locatelli, A. (2017): Optimal Control of a Double Integrator: A Primer on
Maximum Principle, Springer.
287. Loughman, M., de Paor, A., Ringwood, J. V. (2016): The Effect of Blood
Pressure Pulsatility on the Baroreflex Gain. IEEE Proceedings of the 27th
Irish Signals and Systems Conference, Londonderry, 1–6.
288. Lozano-Leal, R., and Joshi, S. M. (1990): Strictly Positive Real Transfer Func￾tions Revisited. IEEE Transactions on Automatic Control 35(11):1243–1245.
289. Luenberger, D. G. (1966): Observers for Multivariable Systems. IEEE Trans￾actions on Automatic Control 11(2):190–197.
290. Lur~e, A. I. (1951): Nekotorye nelinenye zadaqi teorii avtomatiqes￾kogo regulirovani, Gosudarstvenoe izdatel~stvo tehniko-teoretiqesko
literatury, Moskva (Russian original publication of [291]).
291. Lur'e, A. I. (1957): Some Non-linear Problems in the Theory of Automatic
Control, Her Majesty’s Stationery Office, London.722 References
292. Lpunov, A. M. (1892): Obwa zadaqa ob ustoqivosti dvieni, Raz￾sudenie, Har~kov (Ljapunov, A. M (1892): The general problem of the
stability of motion. PhD thesis, University of Kharkiv; in Russian).
293. Lyapunov, A. M. (1992): The general problem of the stability of motion. Trans￾lation of the French version [279] by A. T. Fuller. International Journal of
Control 55(3):531–534.
294. Ma, W., Wang, Y., Yang, S., Wang, S., and Xue, Z. (2018): Observation of
Internal Solitary Waves Using an Underwater Glider in the Northern South
China Sea. Journal of Coastal Research 34(5):1188–1195.
295. Ma, Z., and Zou, S. (2021): Optimal Control Theory, Springer.
296. Maciejowski, J. M. (2002): Predictive Control with Constraints, Pearson.
297. Madeira, D., and Adamy, J. (2016): On the Equivalence between Strict Pos￾itive Realness and Strict Passivity of Linear Systems. IEEE Transactions on
Automatic Control 61(10):3091–3095.
298. Maeder, U., Cagienard, R., and Morari, M. (2007): Explicit Model Predic￾itve Control. In Tarbouriech, S., Garcia, G., and Glattfelder, A. H. (Eds.):
Advanced Strategies in Control Systems with Input and Output Constraints,
237–271, Springer.
299. Maggiore, M., and Passino, K. M. (2003): A Separation Principle for Non￾UCO Systems: The Jet Engine Stall and Surge Example. IEEE Transactions
on Automatic Control 48(7):1264–1269.
300. Malkin, I. G. (1952): Teori ustoqivosti dvieni, Izdatel~stvo Nau￾ka. (Russian original publication of [301]).
301. Malkin, I. G. (1958): Theory of stability of motion, University of Michigan
Library.
302. Mancilla Aguilar, J. L., García, R. A., and D’Attellis, C. E. (1996): Exact lin￾earization of nonlinear systems: trajectory tracking with bounded controls and
state constraints. International Journal of Control 65(3):455–467.
303. Margono, B. S. (2004): Optimierung von Bewegungsabläufen mit schwingungs￾freien Endpositionen zur Verkürzung der Arbeitszyklen von Container￾Schnellumschlaganlagen, PhD thesis, Universität Duisburg-Essen, (Optimiza￾tion of motion sequences with oscillation-free end positions for shortening the
operating cycles of container handling systems; in German).
304. Mizumoto, M., and Tanaka, K. (1981): Fuzzy Sets and Their Operations.
Information and Control 48(1):30–48.
305. Marino, R. (1986): On the largest feedback linearizable subsystem. Systems
& Control Letters 6(1):345–351.
306. Maron, C. (1992): Automatische Gangwahl bei PKW – Automatikgetrieben
mit Hilfe von Fuzzy-Logik. VDE-Fachtagung Technische Anwendungen von
Fuzzy-Systemen, Dortmund, 278–287, (Automatic gear selection in automatic
transmission of passenger cars using fuzzy logic. VDE Symposium Technical
Applications of Fuzzy Systems; in German).
307. Marquez, H. J., and Damaren, G. J. (1996): Analysis and Synthesis of Strictly
Positive Real Transfer Functions, Journal of the Franklin Institute 333
B(2):245–256.
308. Marquez, H. J. (2003): Nonlinear Control Systems, Wiley & Sons.
309. Martin, P., Devasia, S., and Paden, B. (1996): A Different Look at Output
Tracking: Control of a VTOL Aircraft. Automatica 32(1):101–107.
310. Martin, P., Murray, R.M., and Rouchon, P. (2009): Flatness based design.
Control Systems, Robotics and Automation XIII, 65–89.References 723
311. Massera, J. L. (1956): Contributions to Stability Theory. Annals of Mathemat￾ics 64(1):182–206.
312. Massera, J. L. (1958): Erratum: Contributions to Stability Theory. Annals of
Mathematics 68(1):202.
313. Matsumoto, N., Kuraoka, H., Ohoka, N., and Ohba, M. (1991): Servo Control
System. U. S. Patent 5,001,640.
314. Maurer, G. F. (1995): A Fuzzy Logic Controller for an ABS Braking System.
IEEE Transactions on Fuzzy Systems 3(4):381–388.
315. Mayne, D. Q., Rawlings, J. B., Rao, C. V., and Scokaert, P. O. M. (2000):
Constrained model predictive control: Stability and optimality. Automatica
36(6):789–814.
316. Mazenc, F., de Queiroz, M. S., Malisoff, M., and Gao, F. (2006): Further Re￾sults on Active Magnetic Bearing Control with Input Saturation. IEEE Trans￾actions on Control Systems Technology 14(5):914–919.
317. McMahan, J. (1978): Flight 1080. Air Line Pilot 47(7):6–10.
318. Meditch, J. S. (1964): On the Problem of Optimal Thrust Programming For a
Lunar Soft Landing. IEEE Transactions on Automatic Control 9(4):477–484.
319. Mees, A. I. (1984): Describing Functions: Ten Years On. IMA Journal of Ap￾plied Mathematics 32(1-3):221–233.
320. Michel, A. N., Hou, L., and Liu, D. (2015): Stability of Dynamical Systems,
Birkhäuser.
321. Michels, K., Klawonn, F., Kruse, R., and Nürnberger, A. (2006): Fuzzy Con￾trol: Fundamentals, Stability and Design of Fuzzy Controllers, Springer.
322. Miller, S., Kennedy, J. F., Molino, J., Emo, A., Rousseau, G., Tan, G., and
Do, A. (2010): Operating Characteristics of the Segway Human Transporter.
Technical Report of the U. S. Department of Transportation, No. FHWA-HRT￾10-025.
323. Mohammadi, F., and Hosseini, M. M. (2011): A comparative study of numeri￾cal methods for solving quadratic Riccati differential equations. Journal of the
Franklin Institute 348(2):156–164.
324. Mokhtari, F., Sicard, P., and Lechevin, N. (2007): Stabilizing Winding Systems
by Injection Damping Control on Control Based on Controlled Hamiltonian
Systems. IEEE Proceedings of the International Electric Machines and Drives
Conference (IEMDC), Antalya, 95–100.
325. Mukhopadhyay, B. K., and Malik, O. P. (1972): Optimal control of synchro￾nous-machine excitation by quasilinearisation techniques. IEE Proceedings -
Electric Power Applications 119(1):91–98.
326. Müller, A., Marsili-Libelli, S., Aivasidis, A., Lloyd, T., Kroner, S., and Wan￾drey, C. (1997): Fuzzy control of disturbances in a wastewater treatment pro￾cess. Water Research 31(12):3157–3167.
327. Murray, F. J., and Miller, K. S. (2007): Existence Theorems for Ordinary Dif￾ferential Equations, Dover Publications.
328. Na, W. K., Gou, B, and Diong, B. (2007): Nonlinear Control of PEM Fuel
Cells by Exact Linearization. IEEE Transactions on Industry Applications
43(6):1426–1433.
329. Narendra, K. S., and Taylor, J. H. (1973): Frequency Domain Criteria for Ab￾solute Stability, Academic Press.
330. Narendra, K. S., and Goldwyn, R. M. (1964): A Geometrical Criterion for the
Stability of Certain Nonlinear Nonautonomous Systems. IEEE Transactions
on Circuit Theory 11(3):406–408.724 References
331. National Transportation Safety Board (1989): Aircraft Accident Report -
United Airlines Flight 232, McDonnell Douglas DC-10-10, Sioux Gateway
Airport, Sioux City, Iowa, July 19, 1989. Report No. NTSB/AAR-90/06.
332. Naus, G. J. L. (2010): Model-based control for automotive applications, PhD
Thesis, University of Technology Eindhoven.
333. Nayfeh, A. H., and Mook, D. T. (1995): Nonlinear Oscillations, Wiley & Sons.
334. Netushil, A. (Ed.) (1978): Theory of Automatic Control, MIR.
335. Newell, R. B., and Lee, P. L. (1989): Applied Process Control: A Case Study,
Prentice-Hall.
336. Newman, W. S. (1990): Robust Near Time-Optimal Control. IEEE Transac￾tions on Automatic Control 35(7):841–844.
337. Nicolau, F., Respondek, W., and Barbot, J.-P. (2020): Flat Inputs: Theory and
Applications. SIAM Journal on Control and Optimization 58(6):3293–3321.
338. Nijmeijer, H., and van der Schaft, A. (2016): Nonlinear Dynamical Control
Systems, Springer.
339. Nikravesh, S. K. Y. (2013): Nonlinear Systems Stability Analysis: Lyapunov￾Based Approach, Taylor & Francis.
340. Ochi, Y. (2005): Flight control system design for propulsion-controlled aircraft.
Proceedings of the Institution of Mechanical Engineers - Part G: Journal of
Aerospace Engineering 219(4):329–340.
341. Ogata, K. (1987): Discrete-Time Control Systems, Prentice-Hall.
342. Oppenheim, A. V., and Schafer, R.W. (2010): Discrete-Time Signal Process￾ing, Pearson.
343. Orloff, R.W., and Harland, D.M. (2006): Apollo, Springer.
344. Ortega, R., and Garcia-Canseco, E. (2004): Interconnection and Damping As￾signment Passivity-Based Control: A Survey. European Journal of Control
10(5):432–450.
345. Ortega, R., van der Schaft, A., Maschke, B., and Escobar, G. (2002): Inter￾connection and damping assignment passivity-based control of port-controlled
Hamiltonian systems. Automatica 38(4):585–596.
346. Ortega, R., van der Schaft, A., Mareels, J., and Maschke, B. (2001): Putting
Energy Back in Control. IEEE Control Systems Magazine 21(2):18–33.
347. Outrata J, V. (1975): On the Minimum Time Problem in Linear Discrete Sys￾tems with the Discrete Set of Admissible Controls. Kybernetika 11(5):368–374.
348. van Overloop, P.-J., Weijs, S., and Dijkstra, S. (2008): Multiple model pre￾dictive control on a drainage canal system. Control Engineering Practice
16(5):531–540.
349. Owusu, K. O., Lewis, F. L., Borovic, B., and Liu, A. Q. (2006): Nonlinear Con￾trol of a MEMS Optical Switch. Proceedings of the IEEE Conference on De￾cision and Control, San Diego, 597–602.
350. Palacios-Quiñonero, F., Rubió-Massegú, J., Rossell, J. M., and Karimi, H. R.
(2012): Optimal passive-damping design using a decentralized velocity￾feedback H∞ approach. Modelling, Identification and Control 33(3):87–97.
351. Panteley, E., Loría, A., and Sokolov, A. (1999): Global Uniform Asymptotic
Stability of Cascaded Non-autonomous Non-linear Systems: Application to
Stabilisation of a Diesel Engine. European Journal of Control 5(1):107–115.
352. Pappas, G. J., Lygeros, J., and Godbole, D. N. (1995): Stabilization and Track￾ing of Feedback Linearizable Systems under Input Constraints. Proceedings of
the 34th IEEE Conference on Decision and Control, New Orleans, 596–601.References 725
353. Park, M.-H., and Won, C.-Y. (1991): Time Optimal Control for Induction
Motor Servo System. IEEE Transactions on Power Electronics 6(3):514–524.
354. Parker, T. S., and Chua, L. O. (1989): Practical Numerical Algorithms for
Chaotic Systems, Springer.
355. Parks, P. C. (1992): A. M. Lyapunov’s stability theory – 100 years on. IMA
Journal of Mathematical Control & Information 9(4):275–303.
356. Parks, P. C., and Hahn, V. (1993): Stability Theory, Prentice Hall.
357. Pavliotis, G. A., and Stuart, A.M. (2000): Multiscale Methods, Springer.
358. Peckol, J. K. (2021): Introduction to Fuzzy Logic, Wiley.
359. Peng, Y., Vrancic, D., and Hanus, R. (1996): Anti-Windup, Bumpless, and
Conditioned Transfer Techniques for PID Controllers. IEEE Control Systems
Magazine 16(4):48–57.
360. Perruquetti, W., and Barbot, J. P. (Eds.) (2002): Sliding Mode Control in
Engineering, Marcel Dekker Inc.
361. Pfiffner R., and Guzella, L. (1999): Feedback Linearization Idle-Speed Control:
Design and Experiments. Kybernetika 35(4):441–458.
362. Pliss, V. A. (1958): Nekotorye problemy teorii ustoiqivosti v elom,
Izdatel~stvo Leningradskogo universiteta (Pliss, V. A. (1958): Certain
problems in the theory of stability of motion in the whole. Leningrad Univer￾sity Press; in Russian).
363. Pliss, V. A. (1965): Certain problems in the theory of stability of motion in the
whole. English translation of the original publication [362], NASA Technical
Translation F-280.
364. Polyanin, A. D., and Zaitsev, V. F. (2018): Handbook of Ordinary Differential
Equations: Exact Solutions, Methods, and Problems, Chapman & Hall.
365. Popov, V. M. (1959): Criterii de stabilitate pentru sistemele neliniare de reglare
automată, bazate pe utilizarea transformatei Laplace. Studii şi Cercetări de
Energetică 9(1):119–135, (Stability criteria for nonlinear systems with auto￾matic control based on the use of the laplace transformation; in Romanian).
366. Popov, V. M. (1959): Criterii suficiente de stabilitate asimptotică in mare pen￾tru sistemele automate neliniare cu mai multe organe de execuţie. Studii şi
Cercetări de Energetică 9(4):647–680, (Sufficient criteria for asymptotic sta￾bility of general nonlinear systems with multiple actuating variables; in Ro￾manian).
367. Popov, V. M. (1960): Criterion of Quality for Non-linear Controlled Systems.
1st International IFAC Congress on Automatic and Remote Control, Moscow.
In IFAC Proceedings Volumes 1(1):183–187.
368. Popov, V.M. (1960): Noi criterii de stabilitate pentru sistemele automate
neliniare. Studii şi Cercetări de Energetică 10(1):159–174, (New stability cri￾teria for nonlinear control systems; in Romanian).
369. Popov, V. M. (1960): Nouveaux critériums de stabilité pour les systèmes au￾tomatiques nonlinéaires. French translation of the original publication [368].
Revue d’Électrotechnique et Énergétique 5(1):73–88.
370. Popov, V. M. (1960): Noi criterii grafice pentru stabilitatea stării staţionare a
sistemelor automate neliniare. Studii şi Cercetări de Energetică 10(3):601–612
(Romanian original publication of [371]).
371. Popov, V. M. (1961): New graphical criteria for the stability of the steady
state of nonlinear control systems. Revue d’Électrotechnique et Énergétique
6(1):25–34.726 References
372. Popov, V. M. (1961): Absolute Stability of Nonlinear Systems of Automatic
Control. Automation and Remote Control 22(8):857–875.
373. Popov, V.M., and Halanay, A. (1962): On the Stability of Nonlinear Au￾tomatic Control Systems with Lagging Argument. Automation and Remote
Control 23(7):783–786.
374. Popov, V. M. (1963): The Solution of a New Stability Problem for Controlled
Systems. Automation and Remote Control 24(1):1–23.
375. Popov, V. M. (1973): Hyperstability of Control Systems, Springer.
376. Powell, B. K., and Cook, J. A. (1987): Nonlinear Low Frequency Phenomeno￾logical Engine Modeling and Analysis. IEEE Proceedings of the American
Control Conference, Minneapolis, 332–340.
377. Pyatnitskii, E. S. (1973): Existence of Absolutely Stable Systems for Which
the Popov Criterion Fails. Automation and Remote Control 34(1):22–29.
378. Quin, S. J., and Badgwell, T. A. (2003): A survey of industrial model predictive
control technology. Control Engineering Practice 11(7):733–764.
379. Raja, K., and Ramathilagam, S. (2021): Washing machine using fuzzy logic
controller to provide wash quality. Soft Computing 25(15):9957–9965.
380. Răsvan, V., and Popescu, D. (2012): On the Problem of Aizerman, IEEE
Proceedings of the 16th International Conference on System Theory, Control
and Computing (ICSTCC), Sinaia, 1–6.
381. Rawlings, J. B. (2000): Tutorial Overview of Model Predictive Control. IEEE
Control Systems Magazine 20(3):38–52.
382. Rawlings, J. B., and Mayne, D. Q. (2009): Model Predictive Control: Theory
and Design, Nob Hill Publishing.
383. Ray, K. S., and Majumder, D. D. (1985): Fuzzy Logic Control of a Nonlinear
Multivariable Steam Generating Unit Using Decoupling Theory. IEEE Trans￾actions on Systems, Man, and Cybernetics 15(4):539–558.
384. Ringwood, J. V., and Malpas, S. C. (2001): Slow oscillations in blood pressure
via a nonlinear feedback model. American Journal of Physicol Regulatory
Integrative and Comperative Physiology 280(4):R1105–R1115.
385. Röbenack, K. (2004): An Approximation of Normal Form Observer Design:
Convergence and Computation. 6th IFAC Symposium on Nonlinear Control
Systems (NOLCOS), Stuttgart. In IFAC Proceedings Volumes 37(13):1353–
1358.
386. Röbenack, K. (2007): Direct approximation of observer error linearization for
nonlinear forced systems. IMA Journal of Mathematical Control and Informa￾tion 24(4):551–566.
387. Röbenack, K. (2007): Observer Design for a Class of Nonlinear Systems with
Non-Full Relative Degree. Nonlinear Dynamics and Systems Theory 7(4):399–
408.
388. Röbenack, K. (2017): On the Direct Computation of the Byrnes-Isidori Normal
Form. Proceedings in Applied Mathematics and Mechanics 17(1):811–812.
389. Rouche, N., Habets, P., and Laloy, M. (1977): Stability Theory by Liapunov’s
Direct Method, Springer.
390. Rouchon, P. (1994): Necessary Condition and Genericity of Dynamic Feed￾back Linearization. Journal of Mathematical Systems, Estimation, and Con￾trol 4(2):1–14.
391. Rozenvasser, E. N. (1963): The Absolute Stability of Nonlinear Systems. Au￾tomation and Remote Control 24(3):283–291.References 727
392. Rubio, F. R., Camacho, E. F., and Berenguel, M. (2006): Control de Campos
de Colectores Solares. Revista iberoamericana de automática e informática
industrial 3(4):26–45, (Control of solar collector fields; in Spanish).
393. Rugh, W. J., and Shamma, J. S. (2000): Research on gain scheduling. Auto￾matica 36(10):1401–1425.
394. Ruzhansky, M., and Sugimoto, M. (2015): On global inversion of homogeneous
maps. Bulletin of Mathematical Sciences 5(1):13–18.
395. Ryan, E. P. (1974): Time-optimal feedback control laws for certain third-order
relay control systems. International Journal of Control 20(6):881–911.
396. Ryan, E. P. (1977): Time-optimal feedback control of certain fourth-order sys￾tems. International Journal of Control 26(5):675–688.
397. Sabanovic, A., Fridman, L. M., and Spurgeon, S. K. (Eds.) (2004): Variable
Structure Systems: From Principles to Implementation, The Institution of
Engineering and Technology (IET).
398. Sadegh, N, Finney, J. D., and Heck, B. S. (1997): An explicit method for com￾puting the positive real lemma matrices. International Journal of Robust and
Nonlinear Control 7(12):1057–1069.
399. Sage, A. P., and White, C. C. (1977): Optimum Systems Control, Prentice
Hall.
400. Sager, S., Bock, H. G., Diehl M., Reinelt, G., and Schloder, J. P. (2006): Nu￾merical Methods for Optimal Control with Binary Control Functions Applied
to a Lotka-Volterra Type Fishing Problem. In Seeger A. (Ed.): Recent Ad￾vances in Optimization, Springer.
401. Sandberg, I.W. (1964): A Frequency-Domain Condition for the Stability of
Feedback Systems Containing a Single Time-Varying Nonlinear Element. The
Bell System Technical Journal 43(4):1601–1608.
402. Sastry, S. (1999): Nonlinear Systems: Analysis, Stability and Control, Springer.
403. Savkin, A. V., and Evans, R. J. (2002): Hybrid Dynamical Systems: Controller
and Sensor Switching Problems, Birkhäuser.
404. Schmidt, G., and Preusche, G. (1967): Popovs Stabilitätssatz als Mittel zur
teilweisen Bestätigung von Aizermans Vermutung. Regelungstechnik 15(1):20–
25, (Popov’s stability theorem as a means of partially confirming Aizerman’s
conjecture; in German).
405. Schwartz, C. A., and Yan, A. (1995): Systematic Construction of Lyapunov
Functions for Nonlinear Systems in Critical Cases. Proceedings of the 34th
IEEE Conference on Decision and Control, New Orleans, 3779–3784.
406. Schwefel, H.-P. (1995): Evolution and Optimum Seeking, Wiley.
407. Scibile, L. (1997): Non-linear Control of the Plasma Vertical Position in a
Tokamak, PhD Thesis, University of Oxford.
408. Scokaert, P. O. M., Mayne, D. Q., and Rawlings, J. B. (1999): Suboptimal Mod￾el Predictive Control (Feasibility Implies Stability). IEEE Transactions on
Automatic Control 44(3):648–654.
409. Scott, M. (1986): Time/fuel optimal control of constrained linear discrete sys￾tems. Automatica 22(6):711–715.
410. Seibert, P., and Suarez, R. (1990): Global stabilization of nonlinear cascade
systems. Systems & Control Letters 14(4):347–352.
411. Sepulchre, R., Janković, M., and Kokotović, P. V. (1997): Constructive Nonlin￾ear Control, Springer.728 References
412. Serrarens, A. F. A., van de Molengraft, M. J. G., Kok, J. J., and van den Steen,
L. (1998): H∞ Control for Suppressing Stick-Slip in Oil Well Drillstrings. IEEE
Control Systems Magazine 18(2):19–30.
413. Sharma, S. C., Sharma, C., Kumar, C., Rajawat, P. S., and Mohindra, M.
(2008): Fuzzy approach to control moisture content on a paper machine. Paper
Technology 49(4):33–38.
414. Shen, Z., Huang, P., and Andersson, S. B. (2013): Calculating switching times
for the time-optimal control of single-input, single-output second-order sys￾tems. Automatica 49(5):1340–1347.
415. Shtessel, Y., Edwards, C., Fridman, L., and Levant, A. (2014): Sliding Mode
Control and Observation, Birkhäuser.
416. Singer, J., Wang, Y.-Z., and Bau, H. H. (1991): Controlling a Chaotic System.
Physical Review Letters 66(9):1123–1125.
417. Singh, D. S., Kumari, S., Abhishek, V., and Sathyaraj, R. (2017): Optimal
Gear Indication for Green Environment. International Journal of Engineering
Research 6(5):271–274.
418. Sira-Ramírez,H.(1989): Nonlinear Variable Structure Systems in Sliding Mode:
The General Case. IEEE Transactions on Automatic Control 34(11):1186–
1188.
419. Sira-Ramírez, H. (1998): A general canonical form for feedback passivity of
non-linear systems. International Journal of Control 71(5):891–905.
420. Sira-Ramírez, H., and Agrawal, S. K. (2004): Differentially Flat Systems, Mar￾cel Dekker.
421. Sira-Ramírez, H. (2015): Sliding Mode Control, Birkhäuser.
422. Slotine, J.-J. E., and Li, W. (1991): Applied Nonlinear Control, Prentice-Hall.
423. Soetaert, K., Cash, J., and Mazzia, F. (2012): Solving Differential Equations
in R, Springer.
424. Sofrony, J. (2009): Anti-windup Compensation of Input Constrained Systems:
Synthesis using Riccati Equations, VDM Verlag.
425. Sommer, R. (1980): Control design for multivariable nonlinear time-varying
systems. International Journal of Control 31(5):883–891.
426. Sontag, E. D. (1989): A ’universal’ construction of Artstein’s theorem on non￾linear stabilization. Systems & Control Letters 13(2):117–123.
427. Sontag, E. D. (1989): Smooth Stabilization Implies Coprime Factorization.
IEEE Transactions on Automatic Control 34(4):435–443.
428. Sontag, E. D. (1989): Remarks on Stabilization and Input-to-State Stability.
IEEE Proceedings on the 28th Conference on Decision and Control, Tampa,
1376–1378.
429. Sontag, E. D. (1990): Further Facts about Input to State Stabilization. IEEE
Transactions on Automatic Control 35(4):473–476.
430. Sontag, E. D., and Wang, Y. (1995): On characterizations of the input-to-state
stability property. Systems & Control Letters 24(5):351–359.
431. Sontag, E. D. (1998): Mathematical Control Theory, Springer.
432. Sontag, E. D., and Wang, Y. (1999): Notions of input to output stability.
Systems & Control Letters 38(4-5):235–248.
433. Sontag, E. D. (2008): Input to State Stability: Basic Concepts and Results.
In Nistri, P., and Stefani, G. (Eds.): Nonlinear and Optimal Control Theory,
Springer.
434. Spencer, B. F., and Nagarajaiah, S. (2003): State of the Art of Structural
Control. Journal of Structural Engineering 129(7):845–856.References 729
435. Stefan, P. (1973): Two Proofs of Chow’s Theorem. In Mayne, D. Q., and Brock￾ett, R.W. (Eds.): Geometric Methods in System Theory, NATO Advanced
Study Institutes Series 3:159–164.
436. Strogatz, S. H. (2001): Nonlinear Dynamics and Chaos, Westview Press Inc.
437. Sun, Y., and Peng, J. (2002): A New Lyapunov Function for Transient Sta￾bility Analysis of Power Systems with Emergency Control. Proceedings of the
IEEE International Conference on Power System Technology (PowerCon 2002)
3:1540–1544, Kunming.
438. Symans, M. D., and Constantinou, M. C. (1997): Experimental Testing and
Analytical Modeling of Semi-Active Fluid Dampers for Seismic Protection.
Journal of Intelligent Material Systems and Structures 8(8):644–657.
439. Symans, M. D., and Constantinou, M. C. (1999): Semi-active control systems
for seismic protection of structures: a state-of-the-art review. Engineering
Structures 21(6):469–487.
440. Takagi, H. (1992): Application of neural networks and fuzzy logic to con￾sumer products. Proceedings of the IEEE International Conference on Indus￾trial Electronics, Control, Instrumentation, and Automation 3:1629–1633, San
Diego.
441. Tan, S.-C., Lai, Y.M., Cheung, M. K. H., and Tse, C. K. (2005): On the Practi￾cal Design of a Sliding Mode Voltage Controlled Buck Converter. IEEE Trans￾actions on Power Electronics 20(2):425–437.
442. Tao, G., and Ioannou, P. A. (1988): Strictly Positive Real Matrices and
the Lefschetz-Kalman-Yakubovich Lemma. IEEE Transactions on Automatic
Control 33(12):1183–1185.
443. Tarbouriech, S., and Turner, M. (2009): Anti-windup design: an overview of
some recent advances and open problems. IET Control Theory and Applica￾tions 3(1):1–19.
444. Tarbouriech, S., Garcia, G., Gomes da Silva, J. M., and Queinnec, J. (2011):
Stability and Stabilization of Linear Systems with Saturating Actuators,
Springer.
445. Taylor, J. H. (1974): Strictly Positive-Real Functions and the Lefschetz￾Kalman-Yakubovich (LKY) Lemma. IEEE Transactions on Circuits and Sys￾tems 21(2):310–311.
446. Taylor, J. H. (1999): Describing Functions. In Webster, J. G. (Ed.): Wiley En￾cyclopedia of Electrical and Electronics Engineering, Wiley-Interscience.
447. Terano, T., Asai, K., and Sugeno, M. (1994): Applied Fuzzy Systems, Aca￾demic Press.
448. Terrell, W. J. (2009): Stability and Stabilization: An Introduction, Princeton
University Press.
449. Thanom, W., and Loh, R. N. K. (2012): Observer-Based Nonlinear Feedback
Controls for Heartbeat ECG Tracking Systems. Intelligent Control and Au￾tomation 3(3):251–261.
450. Thomsen, J. S. (2005): Feedback Linearization Based Arc Length Control for
Gas Metal Arc Welding. IEEE Proceedings of the American Control Confer￾ence, Portland, 3568–3573.
451. Thompson, J. M. T., and Bishop, S. R. (Eds.) (1994): Nonlinearity and Chaos
in Engineering Dynamics, Wiley & Sons.
452. Thompson, J. M. T., and Stewart, H. B. (Eds.) (2002): Nonlinear Dynamics
and Chaos, Wiley & Sons.730 References
453. Tolle, H. (1975): Optimization Methods, Springer.
454. Tsiotras, P., and Wilson, B. C. (2003): Zero- and Low- Bias Control Designs for
Active Magnetic Bearings. IEEE Transactions on Control Systems Technology
11(6):889–904.
455. Tsypkin, Ya. S. (1964): A criterion for absolute stability of automatic pulse sys￾tems with monotonic characteristics of the nonlinear element. Soviet Physics￾Doklady 9(4):262–266.
456. Tsypkin, Ya. S. (1964): Frequency Criteria for the Absolute Stability of Nonlin￾ear Sampled-Data Systems. Automation and Remote Control 25(3):261–267.
457. Turner, M. C., and Bates, D. G. (Eds.) (2007): Mathematical Methods for
Robust and Nonlinear Control, Springer.
458. Tyagunov, A. A. (2004): High-Performance Model Predictive Control for Pro￾cess Industry, PhD thesis, Technische Universität Eindhoven.
459. Unbehauen R. (1998): Systemtheorie 2: Mehrdimensionale, adaptive und
nichtlineare Systeme. R. Oldenbourg, (System Theory 2: Multidimensional,
Adaptive, and Nonlinear Systems; in German).
460. Utkin, V. I. (1992): Sliding Modes in Control and Optimization, Springer.
461. Utkin, V. I. (1977): Variable Structure Systems with Sliding Modes. IEEE
Transactions on Automatic Control 22(2):212–222.
462. Valmorbida, G., and Anderson, J. (2017): Region of attraction estimation
using invariant sets and rational Lyapunov functions. Automatica 75(1):37–
45.
463. Verbruggen, H. B., and Babuška, R. (Eds.) (1999): Fuzzy Logic Control: Ad￾vances in Applications, World Scientific.
464. Vidyasagar, M. (1980): Decomposition Techniques for Large-Scale Systems
with Nonadditive Interactions: Stability and Stabilization. IEEE Transactions
on Automatic Control 25(4):773–779.
465. Vidyasagar, M. (2002): Nonlinear Systems Analysis, Society for Industrial &
Applied Mathematics.
466. Waldherr, S., and Zeitz, M. (2008): Conditions for the existence of a flat input.
International Journal of Control 81(3):439–443.
467. Waldherr, S., and Zeitz, M. (2010): Flat inputs in the MIMO case. 8th IFAC
Symposium on Nonlinear Control Systems (NOLCOS), Bologna. In IFAC Pro￾ceedings Volumes 43(14):695–700.
468. Wang, J., and Rugh, W. (1987): Feedback Linearization Families for Nonlinear
Systems. IEEE Transactions on Automatic Control 32(10):935–940.
469. Wang, X., Li, S., Su, S., and Tang, T. (2019): Robust Fuzzy Predictive Con￾trol for Automatic Train Regulation in High-Frequency Metro Lines. IEEE
Transactions on Fuzzy Systems 27(6):1295–1308.
470. Wang, Y., Singer, J., and Bau, H. H. (1992): Controlling chaos in a thermal
convection loop. Journal of Fluid Mechanics 237:479–498.
471. Weerasooriya, S., Low, T. S., and Huang, Y. H. (1994): Adaptive Time Op￾timal Control of a Disk Drive Actuator. IEEE Transactions on Magnetics
30(6):4224–4226.
472. Wei, G., Wang, Z., Li, W., and Ma, L. (2014): A Survey on Gain-Scheduled
Control and Filtering for Parameter-Varying Systems. Discrete Dynamics in
Nature and Society, Volume 2014 (April):1–10.
473. Weinberg, L., and Slepian, P. (1960): Positive real matrices. Indiana University
Mathematics Journal 9(1):71–83.References 731
474. Wen, J., and Desrochers, A. A. (1985): A minimum time control algorithm for
linear and nonlinear systems. Proceedings of the 24th IEEE Conference on
Decision and Control, Fort Lauderdale, 1441–1446.
475. Wen, J. T. (1988): Time Domain and Frequency Domain Conditions for Strict
Positive Realness. IEEE Transactions on Automatic Control 33(10):988–922.
476. Wichner, W., Wilharm, H., and Mahr, J. (1997): Crane with a travel drive
for the horizontal movement of a load hanging on a rope. German Patent DE
44 05 525 C2.
477. Willems, J. C. (1972): Dissipative dynamical systems: Part II: Linear Systems
with quadratic supply rates. Archive for Rational Mechanics and Analysis
45(5):352–393.
478. Willems, J. L. (1968): Further comments on a paper by G. Schmidt and G.
Preusche: „Popovs Stabilitätssatz als Mittel zur teilweisen Bestätigung von
Aizermans Vermutung“. Regelungstechnik 16(2):74–75.
479. Willems, J. L. (1970): Stability Theory of Dynamical Systems, Thomas Nelson
and Sons.
480. Wredenhagen, G. F., and Bélanger, P. R. (1994): Piecewise-linear LQ control
for systems with input constraints. Automatica 30(3):403–416.
481. Wu, F. F., and Desoer, C. A (1972): Global Inverse Function Theorem. IEEE
Transactions on Circuit Theory 19(2):199–201.
482. Yan, X.-G., Spurgeon, S. K., and Edwards, C. (2017): Variable Structure Con￾trol of Complex Systems, Springer.
483. Yeung, K. S. (1985): A Reformulation of Nyquist’s Criterion. IEEE Transac￾tions on Education 28(1):58–60.
484. Ying, Y., Rao, M., and Sun, Y. (1992): Bilinear control strategy for paper￾making process. Chemical Engineering Communications 111(1):13–28.
485. Yong, S, Z., Paden, B., and Frazzoli, E. (2015): Computational Methods for
MIMO Flat Linear Systems: Flat Output Characterization, Test and Tracking
Control. IEEE Proceedings of the American Control Conference, Chicago,
3898–3904.
486. Yoshizawa, T. (1966): Stability Theory by Liapunov’s Second Method, The
Mathematical Society of Japan.
487. Yu, H., Cai, G., and Li, Y. (2012): Dynamic analysis and control of a new
hyperchaotic finance system. Nonlinear Dynamics 67(3):2171–2182.
488. Yulin, X. (2009): Modeling and LPV Control of Web Winding System with
Sinusoidal Tension Disturbance. IEEE Proceedings of the Chinese Control
Conference (CCDC), Guilin, 3815–3820.
489. Zaccarian, L., and Teel, A. R. (2011): Modern Anti-Windup Synthesis: Control
Augmentation for Actuator Saturation, Princeton University Press.
490. Zadeh, L. A. (1965): Fuzzy Sets. Information and Control 8(3):338–353.
491. Zames, G. (1966): On the Input-Output Stability of Time-Varying Nonlinear
Feedback Systems-Part I: Conditions Derived Using Concepts of Loop Gain,
Conicity, and Positivity. IEEE Transactions on Automatic Control 11(2):228–
238.
492. Zames, G. (1966): On the Input-Output Stability of Time-Varying Nonlin￾ear Feedback Systems-Part II: Conditions Involving Circles in the Frequency
Plane and Sector Nonlinearities. IEEE Transactions on Automatic Control
11(3):465–476.
493. Zeitz, M. (1983): Controllability canonical (phase-variable) form for non-linear
time-variable systems. International Journal of Control 37(6):1449–1457.732 References
494. Zeitz, M. (1987): The extended Luenberger observer for nonlinear systems.
Systems & Control Letters 9(2):149–156.
495. Zeitz, M. (1989): Canonical Forms for Nonlinear Systems. IFAC Symposium
on Nonlinear Control Systems Design (NOLCOS), Capri. In IFAC Proceedings
Volumes 22(3):33–38.
496. Zeitz, M. (2014): Minimalphasigkeit – keine relevante Eigenschaft für die
Regelungstechnik! Automatisierungstechnik 62(1):3–10, (The property to be
minimum phase is not a relevant property for control engineering; in German).
497. Zhang, H., and Liu, D. (2006): Fuzzy Modeling and Fuzzy Control, Birkhäuser.
498. Zhang, J., Tugal, H., Carrasco, J., and Heath, W. P. (2018): Absolute Stability
of Systems With Integrator and/or Time Delay via Off-Axis Circle Criterion.
IEEE Control Systems Letters 2(3):411–416.
499. Zhao, Z.-Y., Tomizuka, M., and Isaka, S. (1993): Fuzzy Gain Scheduling of PID
Controllers. IEEE Transactions on System, Man & Cybernetics 23(5):1392–
1398.
500. Zhou, J., and Wen, C. (2008): Adaptive Backstepping Control of Uncer￾tain Systems: Nonsmooth Nonlinearities, Interactions of Time-Variations,
Springer.
501. Zinober, A. S. I. (Ed.) (1994): Variable Structure and Lyapunov Control,
Springer.
502. Zypkin, Ja. S. (1963): Die absolute Stabilität nichtlinearer Impulsregelsysteme.
Regelungstechnik 11(4):145–148, (On the Global Stability of Nonlinear Auto￾matic Sampled-Data Systems; in German).
503. Cypkin, . Z. (1977): Osnovy teorii avtomatiqeskih sistem, Nauka (Rus￾sian original publication of [504])
504. Zypkin, Ja. S. (1981): Grundlagen der Theorie automatischer Systeme, R. Old￾enbourg, (Fundamentals of the Theory of Automatic Control Systems; in Ger￾man).Index
A
ABS braking system, 587
absolute stability, see stability
acceleration control, 601
acceleration control loop, 589
accumulation, 581, 585, 590
Ackermann formula, nonlinear, 366
active suspension system, 280
Adams-Bashforth method, 51, 55,
67, 668
Adams-Moulton method, 52, 55, 67,
668
adaptation of the step size, 49
affine-in-control systems, see
control-affine system
aggregation, 579, 581, 585, 590
antecedent, 581
rule, 581
AIDS, 59, 664
aircraft control, 246, 443
airship, 499
Aizerman conjecture, 97, 673
algebraic product, 579
algebraic sum, 579
annihilator matrix, 567
antecedent, 578
antecedent aggregation, 579, 581
anti-lock braking system, 595
anti-windup, 285–298, 299, 335,
358, 677
general structure, 297
Apollo mission, 379
arc welding, 496
asymptotically stable, see stability
attractivity, 14, 15–24
global, 14, 16, 24, 60
local, 14, 15, 16
autofocus control, 587
automatic transmission, 601
automobile, see motor vehicle
autonomous system, see system
autonomous underwater robot,
129–134
autopilot, 5, 205, 332, 446
B
backlash, 75, 80, 598
backstepping, 478–492, 504, 687
simple, 482
bang-bang controller, see
two-position controller
Barbashin and Krasovskii stability
theorem, 116, 140, 147, 466,
566, 679
baroreceptor, 195
barrier function, 528
© Springer-Verlag GmbH Germany, part of Springer Nature 2024
J. Adamy, Nonlinear Systems and Controls,
https://doi.org/10.1007/978-3-662-68690-4
733734 Index
basin, see region of asymptotic
stability
bearing, magnetic, 367
Bernoulli differential equation, 37,
65, 677
BIBO stable, 28
bijective coordinate transformation,
216, 257
bilinear system, 496, 504, 572
bimetal, 5, 75, 299
bioreactor, 631
birth control, 502
bleed air, 641
blimp, 499
blood-pressure regulation system,
195
Boolean
AND operation, 577
logic, 574, 577, 581, 600
OR operation, 577
variable, 574
bounded
product, 579
sum, 579, 600, 692
bounded input-bounded output
stable, see BIBO stable
Brunovsky canonical form, 218, 414,
418, 424, 426, 450, 451, 499,
623, 644, 686
bulk material distribution, 64
Butcher tableau, 48
Byrnes-Isidori canonical form, 374,
390, 405, 600, 623
generalized, 387, 388, 623
of a linear system, 393, 396
C
cable-operated elevator, 661
cableless elevator, 661
cancer, 503
canonical form
Brunovsky, 218, 414, 418, 424,
426, 450, 451, 499, 623, 644,
686
Byrnes-Isidori, 374, 390, 405, 600,
623
generalized controller, 449, 451,
622
Hamiltonian, 170
linear controller, 218, 250, 251,
254, 255, 260, 307, 341, 362,
389, 441, 535, 536, 620
linear observer, 620, 623
listing of, 622–623
nonlinear controllability, 623, 661
nonlinear controller, 215, 223, 255,
257, 279, 359–367, 404, 414,
430, 456, 460, 496, 503, 504,
542, 617, 622, 661, 675, 676
nonlinear observability, 614, 615,
622, 643, 649, 652, 662
nonlinear observer, 621, 623, 660
passivity, 180, 190
canonical form observer, 621, 661
Cartesian product, 3
catalytic converter, 198
catchment region, 17, 124, 134, 201,
319, 325, 330, 342, 525, 680, 682
ellipsoid-shaped, 327
nested, 320, 681
cell culture, 631
center of area, 584
center of singletons, 585
chaos, 33, 36, 489, 599, 665
characteristic curve
backlash, 75, 80, 598
dead zone, 70, 77, 79, 145, 287, 598
dry friction, 80
hysteresis, 75, 81, 299
limiting, saturation, 69, 70, 79,
285, 293, 318
polygonal line, 77
power, 79
preload, 78
roots, 80
three-position element, 70, 78, 145,
194, 548
three-position element with
hysteresis, 81Index 735
two-position element, 70, 73, 78,
548
characteristic polynomial, 53, 254,
275, 292, 295, 342, 390, 452,
535, 546, 629, 633, 646, 662
chattering, see high-frequency
switching
chemical plant, 505
chemical reactor, 651
Chen’s chaotic system, 599
Chow’s Theorem, 229
Chua’s circuit, 60, 193, 664, 669
circle criterion, 103–110, 200, 298,
335, 677
for asymptotically stable systems,
104
for systems without poles on the
imaginary axis, 105
general, 109, 336, 671, 695
circulatory system, 196
closed-loop control, see feedback
control
clownfish, 121
COA method, 584
combustion engine, 198, 410
compact set, 139
compactification, see one-point
compactification
companion matrix, 260, 393
compressor, 453, 640
conclusion of a fuzzy rule, see fuzzy
rule
condenser, 526
condition of a fuzzy rule, see fuzzy
rule
container crane, 322
container ship, 210, 322
continuous angle variation, 108, 200,
671, 698
contractive positively invariant set,
135, 138
control-affine system, 1, 231, 234,
255, 260, 360, 468, 478, 563,
655
control canonical form, 621
control horizon, 506, 519, 524, 528
control Lyapunov function,
465–477, 479, 481, 504
control observer, 291
control variable constraints, see
control variable limitation
control variable limitation, 70, 77,
79, 87, 100, 145, 285, 287, 291,
293, 299, 311, 318, 324, 327,
333, 343, 378, 470, 515, 518, 597
control variable vector, see input
variable vector
controllability, 205–239, 250, 260,
367, 478, 620
criterion for linear systems, 206
global, 212, 278, 605
local, 212, 278, 605
local weakly omnidirectional, 239
matrix, 206, 253, 394, 446
of control-affine systems, 260, 367,
430
of the strict feedback form, 478
omnidirectional, 209–224,
233–239, 277, 278, 501, 504,
674
small-time local, 208–214, 224,
229–233, 234, 278, 674
weak form of omnidirectional, 209
controllability canonical form, 621
nonlinear, 623, 661
controllable canonical form, 621
controllable linear system, 251, 254,
255, 394, 441
controllable on Dx by Du, 213
controller canonical form
generalized, 449, 451, 622
linear, 218, 250, 251, 254, 255, 260,
307, 341, 362, 389, 441, 535,
536, 620
nonlinear, 215, 223, 255, 257, 279,
360, 359–367, 404, 414, 430,
456, 460, 496, 503, 504, 542,
617, 622, 661, 675, 676
convergence rate, 19
converse Lyapunov theorem, 118736 Index
coordinate transformation, 480, 491
bijective, 216, 257
linear, 259
Corless and Shorten’s theorem, 168
COS method, 585, 590
covariance matrix, 636, 638, 642
crane, 168, 322, 699
cruise control, 592
D
d’Alembert differential equation, 37
damping, 181, 280, 298, 421, 474,
544, 550, 565
DC-to-DC converter, 536
De Morgan’s laws, 600
dead zone, 70, 77, 79, 145, 287, 598
decoupling matrix, 217, 256, 402,
404, 428, 564
Definition
1 (Equilibrium Point), 5
2 (Attractivity), 14
3 (Lyapunov Stability), 14
4 (Asymptotic Stability), 16
5 (Catchment Region), 17
6 (Region of Asymptotic
Stability), 17
7 (Asymptotic Stability of a
System), 18
8 (Stability of a System), 18
9 (Strict Stability of a System), 18
10 (Exponential Stability), 19
11 (Instability), 20
12 (Strict Instability), 20
13 (Input-to-State Stability), 25
14 (Input-Output Stability), 28
15 (Stability of Equilibrium Points
of Discrete-Time Systems), 36
16 (Lipschitz Continuity), 39
17 (Lipschitz Continuity of
Multivariate Functions), 39
18 (Absolute Stability), 90
19 (Marginal Stability and Strict
Marginal Stability), 91
20 (Continuous Angle Variation),
108
21 (Invariant Set), 138
22 (Positively Invariant Set), 138
23 (Positive and Negative Definite
Functions), 144
24 (Passivity, Strict Passivity, and
Losslessness), 145
25 (Positive Realness of Transfer
Functions), 158
26 (Positive Realness of Transfer
Matrices), 161
27 (Controllability), 205
29 (Omnidirectional Control￾lability), 209
28 (Small-Time Local
Controllability), 207
30 (Global Controllability), 213
31 (Local Controllability), 213
32 (Flatness), 240
33 (Vector Relative Degree of
MIMO Systems), 404
34 (Full-State Linearizability), 415
36 (Extended Control Lyapunov
Function), 466
35 (Control Lyapunov Function),
466
37 (Zero-State Detectability), 549
38 (Fuzzy Set), 576
39 (Fuzzy AND and Fuzzy OR),
578
40 (Observability), 604
41 (Weak Observability), 605
defuzzification, 575, 584, 585, 586,
590
degree of membership, 576
Delfland, 517
depth control, 310
describing function, 72, 88, 193–197
collection, 78–81
method, 69, 113
of a sigmoidal saturation, 195
of added functions, 76
DFIM, see doubly fed induction
machineIndex 737
diabetes, 659
diffeomorphism, 217, 260–263, 273,
282, 364, 372, 382, 390, 405,
415, 418, 424, 429, 430, 451,
456, 495, 559, 600, 617, 624,
652, 676, 691
global, 263, 282, 676
local, 262, 282, 676
difference degree, 363, 390
difference equation, 35, 37, 53, 594
differential equation
Bernoulli, 37, 65, 677
d’Alembert, 37
logistic, 65
separable, 65, 677
differentially independent, 242, 244,
259
direct-current machine, separately
excited, 182
direct-current motor, 288
direct transmission matrix, see
feedthrough matrix
discrete-time system, 35, 37, 110,
125, 313, 505, 509, 518, 528
distance control, 588, 600
distribution process, 63
domain of definition of a system, 3
double pendulum, 33
doubly fed induction machine, 266
drainage system, 516
drift, 224
driftless system, 224, 231, 501
drilling rig, 419
drilling ship, 210
dry friction, 80
dual-mode controller, 340, 525, 680
dual-mode MPC, 525
dynamic extension, 498
E
Eagle, lunar module, 379–384, 501,
502
earthquake, 556
ecological system, 121
economic market model, 463
eigenvalue
of algebraic multiplicity, 275
of geometric multiplicity, 275
eigenvalue placement, 318
EKF, see extended Kalman filter
elastic hysteresis, 81
electric arc, 496
electromagnet, 367
elevator, 661
epidemiological model, 59
equilibrium point, 5, 138
asymptotically stable, 16, 19, 24,
61, 85, 118, 121, 126, 135, 140,
147, 180, 330, 399, 400, 479,
482, 483, 525, 547, 598, 601,
626, 627
attractive, 24
exponentially stable, 24, 127, 129
globally asymptotically stable, 16,
27, 35, 89, 116, 148, 151, 203,
400, 465, 467, 469, 626, 673,
690, 692
globally exponentially stable, 400
globally Lyapunov stable, 14, 116,
146, 672
globally stable, 146
locally asymptotically stable, 135,
626
locally Lyapunov stable, 14
Lyapunov stable, 14, 24, 61, 146,
666
of linear system, 9
of nonlinear system, 5
strictly unstable, 20, 24, 666
unstable, 11–13, 20, 20–24, 27, 61,
118, 126, 134, 143, 177, 665,
666, 677
error order, see order of accuracy
estimation error, 292, 624–631, 636,
645–648
Euler method, 41, 47, 53, 55, 57, 67,
528, 668
implicit, 57
modified, 45, 55, 67, 668738 Index
Euler’s rotation equations, 8
evaporation plant, 526
Everglades, 516
exact error linearization, 624
exact linearization, see feedback
linearization
example:
a motor vehicle with trailer, 231
bioreactor, 631
chemical reactor, 651
combustion engine, 410
controlling a ship, 99
damping of seismic building
vibrations, 550
DC-to-DC converter, 536
direct-current motor, 288
distance control, 588
double pendulum, 33
drainage system, 516
drilling rig, 419
economic market model, 463
evaporation plant, 526
fluid system with chaotic behavior,
489
helicopter, 332
industrial robot, 219
jet engine, 640
lunar module, 379
magnetic bearing, 367
mutualism, 121
optical switch, 544
paper machine, 570
Park transformation, 266
pneumatic motor, 453
power plant, 472
power-assisted steering system, 85
propulsion-based aircraft control,
443
rocket, 432
satellite, 7
self-balancing vehicle, 175
separately excited direct-current
machine, 182
ship-to-shore gantry crane, 322
solar power plant, 354
steam generator, 236
submarine, 309
synchronous generator, 610
transporting system for material
webs, 155
underwater glider, 129
VTOL aircraft, 246
excluded middle, 600
exponentially stable, see stability
extended Kalman filter, 637–640,
641
external dynamics, 374, 382, 388,
400, 410, 498, 560–562
of linear systems, 393–397, 560
of MIMO systems, 408
F
feedback control, 2, 35, 205, 215,
299, 300, 318, 457, 587
feedback linearization, 359, 370, 384,
389, 478, 504, 542, 620
feedforward control, 2, 35, 205, 215,
299, 300, 442, 587
feedthrough, 2–3, 151, 155, 158, 163,
167, 244
feedthrough matrix, 3, 163
Fel'dbaum’s theorem, 300, 303
fictitious flat output, see flat output
Filippov method, 534
finite escape time, 40, 137
fishing, 63
flat canonical form, 255
flat coordinates, 244, 245
flat input, 458–462, 502, 686
vector, 459–462
flat output, 239–249, 259, 436, 446,
461, 464
fictitious, 240, 434, 437, 439, 443
of linear systems, 251
of the nonlinear controller
canonical form, 256
real, 240, 436, 458
flatness, 239–259, 451, 461
global, 241Index 739
local, 241
of full-state linearizable MIMO
systems, 431
of full-state linearizable SISO
systems, 430
of full-state linearizable systems,
430
flatness and controllability, 250
flatness-based
feedback control, 448–453, 455
feedforward control, 439–443,
444, 453
flight control, 332, 443
flight controller, 353
fluid system, 489
Fourier series, 71, 73, 76
free system, 4, 6, 14, 18, 36, 469,
477, 600
fuel cell, 278
full-state linearizability, 415
of MIMO systems, 426, 433
of SISO systems, 418, 430, 451
full-state linearization, 360, 413,
448, 686
of human heartrate model, 495
of MIMO systems, 426–429
of SISO systems, 413–418
fuzzification, 575, 576, 585
fuzzy AND, 578, 579
fuzzy control, 574, 587, 601
fuzzy controller, 585, 587, 588, 591
fuzzy logic, 574, 578, 580, 584, 586,
600, 601
fuzzy OR, 578, 579
fuzzy rule
conclusion of, 580
condition of, 578
fuzzy set, 576, 584, 600
fuzzy system, 585, 586
G
gain-scheduling control, 345–358,
436, 493, 495
gantry crane, 322
Gao and Hung approach, 533, 543,
545, 595
gas turbine, 640
Gaussian function, 349, 357, 577
Gear method, 57, 67, 668
gear selection, 601
generalized Byrnes-Isidori canonical
form, see Byrnes-Isidori
canonical form
generalized controller canonical form,
see controller canonical form
generator, 266, 472, 610
globally asymptotically stable, see
stability
globally attractive, see attractivity
globally exponentially stable, see
stability
Grönwall’s Lemma, 128
gradient system, 63, 245
growth kinetics, 632
H
Hadamard’s global inverse function
theorem, 262
Hamiltonian canonical form, 170
Hamiltonian equations, 170,
171–179, 204
Hamiltonian function, 170, 171–182,
204
Hamiltonian system, 168–192
dissipative, 179
linear, 185
lossless, 168
harmonic balance, see describing
function method
harmonic oscillator, 15, 30, 672
simple, 11, 18
Heaviside function, 76
helicopter, 332
hermitian matrix, 162
Heun method, 45, 47
high-frequency switching, 32, 530,
538, 667, 687
high-gain observer, 622, 643–662740 Index
for control-affine systems, 655
Hooke and Jeeves optimization
method, 528
Hu and Lin’s stability theorem, 330,
343, 682
human heartrate, model of, 495
human knowledge, model of, 502
Hurwitz sector, 90, 97, 99, 200, 671,
673
hydraulic system, 129, 443, 553
hydropower plant, 599
hyperstability, 159
hysteresis, 5, 75–81, 299
elastic, 81
sharp, 81
I
IDA PBC, see passivity-based
control with IDA
implication, 580, 585, 590
implicit equation, 6, 46, 386, 438, 681
implicit Euler method, 57
implicit function, 235, 262
implicit function theorem, 235, 262,
610
induction machine, doubly fed, 266
industrial robot, 168, 171, 219, 243
infectious disease, 59
inference, 575, 577, 583
infimum, 465
input-affine system, see control-affine
system
input constraints, see control
variable limitation
input-linear system, see
control-affine system
input matrix, 3
input-output linearization, 359–413,
413, 440, 452, 495, 496
at the maximum relative degree,
366
of general systems, 384
of linear systems, 389
of MIMO systems, 401–410
with a reduced relative degree, 377
input-output stability, 28
input-state linearization, see
full-state linearization
input-to-state stability, 25
input variable vector, 2, 25, 35, 156,
217, 221, 233, 241, 263, 345,
403, 428, 521, 548, 566, 607, 613
input vector, 3, 397, 433, 459–462
insensitivity zone, see dead zone
instability, 20–24, 98, 142, 287, 475,
672
of equilibrium points, 20
of limit cycles, 85
theorem of, 142
insulin, 659
integration method, 41–58, 66, 640
stability of, 53
integration time, 43
integrator backstepping, see
backstepping
interconnection and damping
assignment, see passivity-based
control with IDA
internal dynamics, 372–413, 441,
442, 498, 499, 560–569, 684,
690, 691
of linear systems, 389, 393, 560
of MIMO systems, 408
interpolation function, 46
invariant set, 135, 138–141
contractive positively, 135, 138
positively, 138
inverse function, 172, 234, 261, 364,
372, 430, 438, 608–613, 651
inverse transformation, 244–246, 503,
651
involution condition, 419
involutive, 418, 427, 434
J
Jacobi identity, 278, 416, 498
Jacobian matrix, 235, 261, 274, 372,
460, 609, 610, 633, 638, 651, 658Index 741
jet engine, 246, 640
Jordan canonical form, 275
K
Kalman conjecture, 97
Kalman filter
extended, 635–642
linear, 635
Kalman-Yakubovich-Popov
equations (KYP equations),
152–189, 203, 558
Kalman-Yakubovich-Popov lemma,
189
Karnaugh map, 581
KYP equations, see
Kalman-Yakubovich-Popov
equations
L
Lagrange equation, 168–179, 219,
221, 243
Lagrange function, 168–176, 219, 243
Laplace transfer function, 69
LaSalle’s invariance principle, 138
Leverrier-Faddejew-Souriau-Frame
equation, 389
Lie bracket, 228, 232, 416, 418, 424,
427, 434
formulae, 278
nested, 416
Lie derivative, 361, 370, 384, 402,
449, 542, 559, 608, 655
formulae, 496
Lie-Bäcklund transformation, 243
limit cycle, 28–30, 33, 36, 61, 63,
70–88, 138, 140, 193, 194, 197,
523, 550, 669, 670
semi-stable, 30, 82
stable, 30, 82, 88
unstable, 30, 82, 88
limitation of state variables, see
state variable limitation
limitation of the control variable, see
control variable limitation
limiting characteristic curve, see
control variable limitation
limiting element, see control variable
limitation
linear matrix inequality, 332
linear parameter-varying system, 495
linear-quadratic controller, 318
linearizability, 451
linearization family, parameterized,
347–356, 493
linearizing output, 415–430, 449,
455, 499, 684
linearizing output variable vector,
see linearizing output
linguistic value, 575–577, 584, 587,
589, 601
linguistic variable, 575–577, 587
Lipschitz condition, 39, 626
Lipschitz continuous, 39, 118, 137,
626
listing of canonical forms, 622–623
LMPC with constraints, 513
LMPC without constraints, 509
load angle, 472, 610
locally attractive, see attractivity
logic
Boolean, 574, 577, 581, 600
classical, 575, 577
logistic differential equation, 65
logistic function, 668
Lorenz equations, 490
losslessness, 145, 204
Lotka-Volterra equations, 121
LPV system, see linear
parameter-varying system
Luenberger observer, 621, 624–637,
644
for nonlinear control loops, 626
lunar module Eagle, 379–384, 501,
502
Lur'e problem, 90
Lur'e system, 90
Lyapunov
converse theorem, 118
direct method, 113–126, 138, 467742 Index
equation, 121, 189, 319, 331, 344,
561
equation, discrete, 126
first method, see indirect method
function, 116–138, 144, 146, 180,
186, 201, 203, 242, 330, 340,
343, 478–492, 504, 522, 561, 682
function, quadratic, 119–126
function, strict, 116, 119, 121, 566
indirect method, 126, 201, 475
linearization method, see indirect
method
region, 125, 135
second method, 114
set, 135
Lyapunov stable, see stability
M
macroeconomic model, 499
magnetic bearing, 367, 378
marginal stability, 91, 97, 101
mathematical pendulum, 204
maximum operator, 579, 582, 590,
600
maximum principle, 300
measles, 59
medulla oblongata, 195
membership function, 575, 576–592,
601, 692
method error, 43
microelectromechanical system
(MEMS), 544
midpoint method, see modified Euler
method
MIMO system, 3, 151, 161, 167, 217,
218, 241, 257, 332, 401, 405,
426, 431, 453, 543, 564
full-state linearization of, 426–429
minimum operator, 577–581
minimum phase, 395
mobile robot, 225–231, 234
model predictive control (MPC),
505–530
linear (LMPC), 509, 593, 688
nonlinear (NMPC), 521
stability of, 521
with dual-mode, 525
modified Euler method, 44, 46
Moore-Greitzer equation, 641
motor vehicle, 85, 212, 231, 280, 493,
588, 595
with trailer, 231
motorbike, 212
moving horizon, 507
MPC, see model predictive control
multi-body system, 168
multi-step method, 51
mutualism, 121
N
NASA, 379
negative definite, 144, 188, 192, 331,
467, 681, 687
negative semidefinite, 144, 152, 191,
561
nerve fiber, 195
nesting condition, 320
Netushil backlash model, 598
neuronal activity, 195
nonlinear controller canonical form,
see controller canonical form
nonlinear locus, 73, 74–88, 669, 670
nonlinear observability canonical
form, see observability
canonical form
nonlinear observer canonical form,
see observer canonical form
nonlinear standard control loop, see
standard control loop
normal form, see canonical form
normal form observer, see canonical
form observer
nuclear fusion reactor, 338
null-controllable region, 337
numerical optimization method, 521
numerical solution of differential
equations, 37, 41–57, 528
Nyquist criterion, 83, 98, 110, 698Index 743
Nyquist plot, 73, 82–88, 95, 99,
104–113, 159, 197, 200, 336,
669, 671, 678
Nyquist value, 98
O
observability, 603–620, 660
global, 605
local, 605
of autonomous systems, 607–612
of control-affine systems, 375, 383,
617–620
of general nonlinear systems, 612
of linear systems, 253, 394, 398,
610
weak, 605, 614
observability canonical form,
nonlinear, 614, 615, 622, 643,
649, 652, 662
observability matrix, 253, 392, 394,
398, 610, 614
of control-affine systems, 462
of linear systems, 462, 618
observable on Dx for Du, 605
observer, 378, 508
by Luenberger, 624, 626, 630, 636,
644
design using linearization,
626–635, 637, 662
for control-affine systems, 378,
655–662
for linear systems, 291–297
high-gain, 622, 643–662
linear, 291, 603, 624–628
reduced-order, 294, 297
observer canonical form
linear, 620, 623
nonlinear, 621, 623, 660
observer error, see estimation error
omnidirectionally controllable, see
controllability
one-point compactification, 23
one-step method, 47–57, 67
open-loop control, see feedforward
control
optical switch, 544
optical waveguide, 544
order of accuracy, 44–58, 668
output matrix, 3, 158, 628
output variable vector, 2, 35, 221,
239–248, 407, 428, 436, 509,
548, 552, 569, 630
linearizing, 415–430, 449, 455,
499, 684
output vector, 3, 397, 401, 627, 642
output, flat, see flat output
oven, 596
P
pacemaker, 495
paper machine, 570, 587
parabolic trough, 354
parameter optimization, 326
Park transformation, 266–273
partial differential equation, 255,
273, 374, 381, 387, 391, 408,
416–430, 567, 624
passivability
of control-affine systems, 563, 564
of linear systems, 562
passivation, 558, 563
by feedback, 558
by parallel connection, 558
passive control loop, 547, 562–565,
599–601, 691
passive static controller, 547–549
passive system, 143–151, 190, 204,
547–565
passivity, 143–151
of connected systems, 148–151
of linear systems, 151, 560
of static systems, 145
of the satellite model, 203
strict, 145–155, 167, 180,
184–190, 203, 547, 548, 600,
673, 690
passivity-based control, 547–573744 Index
with IDA, 565, 572, 599
passivity canonical form, 180, 190
passivity inequality, 147, 151, 157,
173, 180, 184, 187, 549, 553, 562
PCH system, 174–189, 203
PCHD system, 179–192, 203, 565,
599, 600, 691
P controller, 100, 195, 286, 314, 351,
352, 504, 597, 671, 690
PD controller, 86, 286
pendulum, 204
performance index, 506–530, 593, 636
performance integral, 318
periodic orbit, 36
phugoid, 445
PI controller, 198, 204, 290, 335, 357,
494
with anti-windup, 290, 358
PID controller, 203, 286, 347, 495,
673
with anti-windup, 287, 335
piecewise linear control, 318, 341, 681
pilot control, 313
pneumatic engine, 662
pneumatic motor, 453
polar coordinates, 124, 282
polygonal line, 77, 584
polymer electrolyte membrane
(PEM), 278
Pontryagin, maximum principle by,
300
Popov
criterion, 91–103, 103–113, 197,
335, 336
inequality, 92–97
line, 94–97, 105
plot, 95–105, 198
sector, 96–99, 200
population model, 502
port-controlled Hamiltonian system,
see PCH system
positive definite
function, 115, 139, 140, 144, 467,
468
matrix, 119, 121, 126, 152–155,
162–168, 180, 185–192, 203,
318–322, 331, 343, 504, 506,
512, 553, 558, 561, 567, 573,
673, 681, 682, 692
quadratic performance index, 516
storage function, 146–153, 167,
185–190, 547–550, 560–565
positive definite matrix, 349
positive real transfer function,
158–167
positive realness, 158–168
positive semidefinite
function, 144, 147, 173
matrix, 153, 162–165, 180, 558
storage function, 144–151
positively invariant set, 138
power plant, 354, 472
power supply grid, 472
power, characteristic curve, 79
power-assisted steering system, 85
Prandtl number, 490
pre-control, see pilot control
pre-filter, 291, 294, 365, 375, 382, 401
predator-prey relationship, 63, 121
prediction horizon, 506, 519, 528
predictor-corrector method, 46, 52,
56
preload, 78
premise, 578–582
proportional controller, see P
controller
pseudoinverse, 568
pulse width modulation (PWM), 536
Q
quadratic form, 119, 154, 185, 512
quadratic optimization problem, 516
quadratic programming, 516
R
radially unbounded
Lyapunov function, 116, 135,
146–151, 465–468Index 745
storage function, 549
Rayleigh number, 490
reachable set, 207, 209, 214
real flat output, see flat output
recursive formula, 42
reduced-order observer, 294, 297
refinery, 505
region
of asymptotic stability, 17, 60,
134–137
of attraction, 14, 60
relative degree, 363, 364–462,
496–501, 542, 559–569, 617,
656, 658–659, 690, 692
total, 406–409, 426, 431
vector, 404, 412, 429, 565, 569
well-defined, 367, 378, 381, 405,
501, 685
relay characteristic, 73
retransformation, 260, 263, 424
Riccati differential equation, 638
Riccati equation, algebraic, 638
robot
autonomous underwater, 129–134
industrial, 168, 171, 219, 243
mobile, 225–231, 234, 280
robustness, 32, 398, 479, 485, 530,
532, 535, 547
rocket, 432
root locus method, 90
rounding error, 44
Routh criterion, 90, 665
rule aggregation, 581
rule base, 581, 582–585
in matrix form, 581, 590
in table form, 581
ruled manifold criterion, 257
Runge-Kutta method, 47, 55, 67,
439, 521, 640, 668
S
satellite, 7, 168, 203, 236, 280, 659
saturation, see characteristic curve
saturation controller, 327–334, 343,
626
scheduling vector, 348
Scokaert, Mayne, and Rawlings
algorithm, 525
sector of absolute stability, 90,
95–99, 103, 110–113, 200, 204,
671–673, 677
seismic building vibrations, 550
self-balancing vehicle, 175
separable differential equation, 65,
677
separately excited direct-current
motor, 280
separation principle, 292, 603
separator, 526
separatrix, 134, 136
servomotor, 86–88, 285, 288
sewage plant, 587
sewer systems, 516
sharp hysteresis, 81
ship, 99–102, 210–212, 277, 285, 322
ship-to-shore gantry crane, 322
sigmoidal saturation, 195
silo, 63
simple harmonic oscillator, 11, 18
Simpson method, 45, 46–56, 67, 668
simulation error, 54
singleton, 584, 591, 601
SISO system, 3, 167, 217, 241, 257,
299, 360, 406, 437, 451, 458,
548, 558, 643
flatness of, 430
full-state linearization of, 413–418
general nonlinear, 384
input-output linearization of, 384
positive realness of, 158
skew-symmetric matrix, 151, 174,
186, 189, 567, 573
sliding mode, 30, 315, 468, 470,
530–547, 594, 689, 690
sliding mode control, 32, 530–547,
596
robust, 547746 Index
small-time locally controllable, see
controllability
soft variable structure control, 317,
322, 343
solar power plant, 354
Sontag’s control law, 469, 471
speed control, 493
spring-mass system, 169–174, 181,
190, 494, 555
stability, 11–20
absolute, 89–113, 197, 200–204,
336, 671, 673, 677
asymptotic, 16, 17–24, 53, 85, 115,
116, 118, 121, 126, 135, 140,
147, 180, 330, 400, 465, 479,
482, 483, 525, 626, 627
exponential, 18, 19, 24, 26, 127,
129, 134, 400
global asymptotic, 16, 27, 35, 89,
116, 151, 341, 399, 400, 467,
469, 485, 626
global exponential, 19, 129, 400
global Lyapunov, 116, 146, 672
input-output, 28
input-to-state, 25–27, 377, 400
local asymptotic, 135, 626
Lyapunov, 14, 15–24, 36, 61, 115,
116–121, 146, 186, 201, 395,
547, 561–569, 666
marginal, 91, 101
of cascaded systems, 400
of control loops with anti-windup
element, 298
of control loops with saturation
controller, 330
of discrete-time systems, 36, 125
of fuzzy systems, 587
of input-output linearized systems,
400
of integration methods, 53
of limit cycles, 84
of linear systems, 11
of model predictive controls, 521
of passive systems, 146
of systems, 18
of zero dynamics, 398–400
strict marginal, 91, 92, 97, 101
stability theorem
by Barbashin and Krasovskii, 116,
140, 147, 466, 566, 679
by Hu and Lin, 330, 343, 682
by Luenberger, 626
by Lyapunov, 115, 126, 146, 465,
523
by Popov, 92
by Tsypkin, 110
stable, see stability
standard control loop
discrete-time nonlinear, 111
nonlinear, 69–113, 194, 195, 197,
204, 295, 298, 695
state control with saturation, 330
state controller
linear, 291–295, 321, 327, 340, 344,
347, 505, 603, 621
nonlinear, 375, 641
parameter-dependent, 342
state-space exact linearization, see
full-state linearization
state variable limitation, 515, 518,
527
steam generator, 236
steering system, 85
steering wheel, 85
step-optimal feedforward control, 313
step size, 42–58, 66, 528, 668
stick-slip friction, 420
stiff system, 56–58
stirred tank reactor, 651
storage function, 143, 144–192, 203,
548–568, 598–601, 673
positive definite, 146–153, 167,
185–190, 547–550, 560–565
positive semidefinite, 144–151
strict feedback form, 478, 491, 499,
504, 659
strict instability, 20, 21–24, 666
strict Lyapunov function, 116, 119,
121, 566Index 747
strictly marginally stable, see
stability
strictly passive, see passivity
strictly positive real, 158, 160–168,
204, 597, 673
strictly stable system, 18
submarine, 309–313
subway train, 587
sugar factory, 526
Swedish wheels, 234
switching
control, 62, 314–322, 325, 470
control without a sliding mode,
318
curve, 306, 314, 337, 339, 594, 680,
689
function, 533
hyperplane, 532–535
hypersurface, 543
interval, 65, 300, 302, 594, 667
line, 32, 317, 530–547, 594, 596,
689
synchronous generator, 472–475, 610
system
autonomous, 3, 259, 458, 461, 605,
607–615
control-affine, 360, 468, 478, 655
discrete, 505
discrete-time, 35, 110, 125, 313,
509, 518
domain of definition, 3
driftless, 224
free, 4, 6, 14, 18, 36, 469, 477, 600
passive, 143, 547
stiff, 56
strictly stable, 18
trivial, 244
system matrix, 3, 126, 260, 275, 342,
396, 628–630, 642–648
T
table of canonical forms, 622–623
Taylor polynomial, 127
Taylor series, 127, 131, 226, 227, 235,
240, 609, 627, 638
temperature control, 5, 75, 299, 574
tertium non datur, 600
Theorem
1 (Input-to-State Stability of
Linear Systems), 27
2 (Input-Output Stability of
Linear Systems), 28
3 (Peano Existence Theorem), 38
4 (Picard-Lindelöf Local Existence
Theorem), 40
5 (Picard-Lindelöf Global
Existence Theorem), 40
6 (Popov Criterion), 92
7 (Absolute Stability of Control
Loops with Delay), 98
8 (Circle Criterion for
Asymptotically Stable
Systems), 104
9 (Circle Criterion for Systems
without Poles on the Imaginary
Axis), 105
10 (General Circle Criterion), 109
11 (Basic Tsypkin Criterion), 110
12 (Extended Tsypkin Criterion),
111
13 (Lyapunov’s Direct Method),
115
14 (Barbashin and Krasovskii
Theorem), 116
15 (Converse Lyapunov Theorem),
118
16 (Lyapunov Equation), 121
17 (Lyapunov’s Indirect Method of
Stability), 126
18 (Exponential Stability), 129
19 (Catchment Region), 135
20 (LaSalle’s Invariance Principle),
138
21 (Instability), 142
22 (Stability of Passive Systems),
146
23 (Asymptotic Stability of
Passive Systems), 147748 Index
24 (Asymptotic Stability of
Strictly Passive Systems), 148
25 (Parallel Connection of Passive
Systems), 149
26 (Control Loop with Passive
Subsystems), 150
27 (Passivity of Linear Systems),
152
28 (Passivity and the KYP
Equations), 153
29 (Strict Passivity and the KYP
Equations), 154
30 (Positive Real Transfer
Functions I), 160
31 (Positive Real Transfer
Functions II), 160
32 (Strictly Positive Real Transfer
Functions), 160
33 (Necessary Conditions for
Positive Realness), 161
34 (Positive Real Transfer
Matrices), 162
35 (Strictly Positive Real Transfer
Matrices), 162
36 (Positive Realness and the
KYP Equations), 165
37 (Strict Positive Realness and
the KYP Equations), 166
38 (Passivity and Positive
Realness), 167
39 (Corless and Shorten’s
Theorem), 168
40 (Stability of PCHD Systems),
180
41 (Linear PCH Systems), 187
42 (Linear PCHD Systems), 190
43 (Controllability and Small-Time
Local Controllability), 208
44 (Small-Time Local and Omni￾directional Controllability), 210
45 (Controllability of Nonlinear
Systems), 218
46 (Chow’s Theorem), 229
47 (Omnidirectional Controllabi￾lity of Control-Affine Systems),
234
48(Omnidirectional Controllabi￾lity), 235
49 (Controllability and Flatness of
Linear Systems), 251
50 (Controllability and Flatness of
Nonlinear Systems), 251
51 (Flatness of the Controller
Canonical Form), 257
52 (Necessary Condition for
Flatness), 258
53 (Nonexistent Flatness of
Autonomous Systems), 259
54 (Local Diffeomorphism), 262
55 (Global Diffeomorphism), 263
56 (General Anti-Windup
Structure), 297
57 (Fel'dbaum’s Theorem), 300
58 (Hu and Lin’s Stability
Theorem), 330
59 (Input-Output Linearization at
the Maximum Relative
Degree), 366
60 (Input-Output Linearization
with a Reduced Relative
Degree), 377
61 (Relative Degree and Internal
Dynamics of Linear Systems),
394
62 (Stability of Input-Output
Linearized Systems), 400
63 (Maximum Total Relative
Degree), 405
64 (Full-State Linearization of
SISO Systems), 418
65 (Full-State Linearization of
SISO Systems), 418
66 (Full-State Linearization of
MIMO Systems), 426
67 (Full-State Linearization of
MIMO Systems), 428
68 (Vector Relative Degree), 429Index 749
69 (Flatness of Full-State
Linearizable SISO Systems),
430
70 (Flatness of Full-State
Linearizable MIMO Systems),
431
71 (Real and Fictitious Flat
Outputs), 440
72 (Flatness and Linearizability
of SISO Systems), 451
73 (Transformation into
Brunovsky Canonical Form),
451
74 (Flat Inputs of Control-Affine
Systems), 461
75 (Existence of a Control Law),
467
76 (Control Lyapunov Function for
Control-Affine Systems), 468
77 (Sontag’s Control Law), 469
78 (Simple Backstepping), 482
79 (Backstepping), 483
80 (Control of Strictly Passive
Plants Using Passive Static
Controllers), 549
81 (Control of Passive Plants
Using Passive Static
Controllers), 550
82 (Passivity of Linear Systems),
560
83 (Passivability of Linear
Systems), 562
84 (Passivity of Control-Affine
Systems), 563
85 (Passivability of Control-Affine
Systems), 564
86 (Observability of Autonomous
Systems), 608
87 (Weak Observability of
Autonomous Systems), 609
88 (Observability of Nonlinear
Systems), 613
89 (Weak Observability of
Nonlinear Systems), 614
90 (Observability of Control-Affine
Systems), 618
91 (Luenberger Observer for
Nonlinear Control Loops), 626
92 (Nyquist criterion), 698
three-position controller, 574
three-position element, 70, 78, 145,
194, 548
with hysteresis, 81
time-invariant, 4, 466, 550, 608
time-optimal
control, 299–314, 316, 334, 336,
339, 505, 594, 680, 689
control of a submarine, 309
control of the triple integrator, 337
control sequence, 301, 302
feedforward control, 300, 306, 313,
325, 335
pilot control, 313
settling time, 322
time-variant, 4, 548, 550
nonlinearity, 96
Tokamak, 338
toroidal tube, water-filled, 489
torus coordinates, 282
total relative degree, 405–409, 426,
431
tracked vehicle, 212
trajectory, 4
transformation equation, 245, 246,
260, 263, 270, 273, 311, 364,
372, 375, 391, 459, 545, 610,
618, 643, 646, 649, 655
Lie-Bäcklund, 243
transformation rule, see
transformation equation
transporting system for material
webs, 155
transverse thruster, 210, 277
trivial system, 244–246
Tsypkin, 110–113
basic criterion, 110
extended criterion, 111
inequality, 112
plot, 112750 Index
tumor, 503
turbofan, 640
two-position controller, 31, 73, 299,
574
two-position element, 70, 73, 78, 548
U
underwater glider, 129
unstable, 11, 20–24, 105–113,
134–143, 287, 291, 300, 327,
436–439, 443, 489, 562, 671,
681, 691
control loop, 82, 90, 93, 667
equilibrium point, 11–13, 20,
20–24, 27, 61, 118, 126, 134,
142, 177, 665, 666, 677
flow conditions, 640
internal dynamics, 376
limit cycle, 30, 82, 88
linear system, 12, 40, 337
passive system, 146
strictly, 20, 24
V
Van der Pol differential equation, 28
variable structure control, 505, 626
soft, 317, 322, 343
with sliding mode, 530–547
without sliding mode, 314–326
vector relative degree, 404, 412, 429,
565, 569
vessel, see ship
virtual controller, 479–492, 504
viscous fluid damper, 550–558
viscous friction, 421, 553
VTOL aircraft, 246
W
washing machine controller, 587
weakly minimum phase, 395
weighted arithmetic mean, 349, 358
welding, 496
well-defined, see relative degree
white noise, 636
wind turbine, 266
windup effect, 285–291
Z
Zeno behavior, 668
zero dynamics, 395, 398–400, 441,
496, 498, 561
zero-state detectability, 549, 553, 556
