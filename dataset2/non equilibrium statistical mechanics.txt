Non-Equilibrium Statistical 
Mechanics
Statistical mechanics provides a framework for relating the properties of macroscopic systems 
(large collections of atoms, such as in a solid) to the microscopic properties of its parts. However, 
what happens when macroscopic systems are not in thermal equilibrium, where time is not only a 
relevant variable, but also essential?
That is the province of nonequilibrium statistical mechanics – there are many ways for systems 
to be out of equilibrium! The subject is governed by fewer general principles than equilibrium 
statistical mechanics and consists of a number of different approaches for describing nonequi￾librium systems. 
Financial markets are analyzed using methods of nonequilibrium statistical physics, such as the 
Fokker-Planck equation. Any system of sufficient complexity can be analyzed using the meth￾ods of nonequilibrium statistical mechanics. The Boltzmann equation is used frequently in the 
analysis of systems out of thermal equilibrium, from electron transport in semiconductors to 
modeling the early Universe following the Big Bang. 
This book provides an accessible yet very thorough introduction to nonequilibrium statistical 
mechanics, building on the author’s years of teaching experience. Covering a broad range of 
advanced, extension topics, it can be used to support advanced courses on statistical mechanics, 
or as a supplementary text for core courses in this field.
Key Features:
• Features a clear, accessible writing style which enables the author to take a sophisticated ap￾proach to the subject, but in a way that is suitable for advanced undergraduate students and
above.
• Presents foundations of probability theory and stochastic processes and treats principles and
basic methods of kinetic theory and time correlation functions.
• Accompanied by separate volumes on thermodynamics and equilibrium statistical mechan￾ics, which can be used in conjunction with this book
James H. Luscombe received a PhD in Physics from the University of Chicago in 1983. After 
post-doctoral positions at the University of Toronto and Iowa State University, he joined the Re￾search Laboratory of Texas Instruments, where he worked on the development of nanoelectronic 
devices. In 1994, he joined the Naval Postgraduate School in Monterey, CA, where he is a profes￾sor of Physics. He was Chair of the Department of Physics from 2003 – 2009. He teaches a wide 
variety of topics, including general relativity, statistical mechanics, mathematical methods, and 
quantum computation. He has published more than 60 research articles, and has given more than 
100 conference presentations; he holds 2 patents.Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com Non-Equilibrium Statistical 
Mechanics
James H. LuscombeFirst edition published 2025
by CRC Press
2385 NW Executive Center Drive, Suite 320, Boca Raton FL 33431
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC
© 2025 Taylor & Francis Group, LLC 
Reasonable efforts have been made to publish reliable data and information, but the author and publisher cannot as￾sume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have 
attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders 
if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please 
write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or 
utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including pho￾tocopying, microfilming, and recording, or in any information storage or retrieval system, without written permission 
from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.com or contact the 
Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. For works that are not 
available on CCC please contact mpkbookspermissions@tandf.co.uk
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are used only for 
identification and explanation without intent to infringe.
ISBN: 978-1-138-54295-2 (hbk)
ISBN: 978-1-032-84342-1 (pbk) 
ISBN: 978-1-003-51229-5 (ebk)
DOI: 10.1201/9781003512295
Typeset in Nimbus Roman 
by KnowledgeWorks Global Ltd. 
Publisher’s note: This book has been prepared from camera-ready copy provided by the authors.Contents
Preface xi
CHAPTER 1 ■ Irreversibility, entropy, and fluctuations 1
1.1 ENTROPY AND IRREVERSIBILITY, CLAUSIUS INEQUALITY 1
1.2 EQUILIBRIUM, GLOBAL AND LOCAL 2
1.3 BALANCE EQUATIONS: FLUXES AND SOURCES 4
1.4 HYDRODYNAMIC CONSERVATION LAWS 4
1.4.1 Mass conservation, convective derivative 4
1.4.2 Momentum balance, stress tensor 6
1.4.3 Energy conservation, heat flux 7
1.5 ENTROPY SOURCES: FLUXES AND THERMO-FORCES 8
1.6 LINEAR FORCE-FLUX RELATIONS, KINETIC COEFFICIENTS 9
1.7 FORCES AND FLUXES LINKED THOUGH FLUCTUATIONS 11
1.8 THERMOELECTRICITY, KELVIN RELATION 14
1.9 STEADY STATES AND MINIMUM ENTROPY PRODUCTION 15
CHAPTER 2 ■ Fluctuations as stochastic processes 19
2.1 EINSTEIN FLUCTUATION THEORY 19
2.1.1 Characteristic functions and moments 21
2.1.2 Force-fluctuation correlations 22
2.2 MICROSCOPIC REVERSIBILITY AND ONSAGER RECIPROCITY 23
2.3 STOCHASTIC PROCESSES: ADDING TIME TO PROBABILITY 24
2.3.1 The consistency conditions 25
2.3.2 Stochastic calculus 26
2.4 STATIONARY, INDEPENDENT, AND MARKOV PROCESSES 27
2.4.1 Stationary processes: No unique origin of time 27
2.4.2 Independent processes: Present is independent of the past 27
2.4.3 Markov processes: Present depends on the immediate past 28
2.5 ERGODIC AND SPECTRAL PROPERTIES 29
2.5.1 Ergodicity 29
vvi ■ Contents
2.5.2 Spectral analysis, Wiener-Khinchin theorem 30
2.6 THERMAL NOISE, NYQUIST THEOREM 32
2.7 THE RANDOM WALK IN ONE DIMENSION, DIFFUSION 34
2.7.1 Asymptotic form 35
2.7.2 Passing to the continuous limit, Einstein diffusion equation 36
2.7.3 Wiener-Levy process 37
2.8 THE MASTER EQUATION 37
2.8.1 Derivation 37
2.8.2 Detailed balance 39
2.8.3 Matrix form 39
2.8.4 Eigenfunction expansion 40
2.9 GAUSSIAN PROCESSES 41
2.9.1 Gaussian distributions and cumulants 42
2.9.2 Multivariate Gaussian distributions 43
2.9.3 Many-time cumulants 44
2.9.4 Gaussian stochastic processes, Doob’s theorem 45
CHAPTER 3 ■ Brownian motion and stochastic dynamics 61
3.1 LANGEVIN EQUATION, EINSTEIN RELATION 62
3.1.1 Thermal noise 62
3.1.2 Brownian motion of free particles; the random force 64
3.1.3 Uniform external force; drift speed 66
3.2 LANGEVIN EQUATION AS A STOCHASTIC PROCESS 66
3.3 FOKKER-PLANCK EQUATION FOR ONE RANDOM VARIABLE 68
3.3.1 Derivation 68
3.3.2 Free Brownian particles in spatially homogeneous systems 70
3.3.3 Ornstein-Uhlenbeck process 72
3.4 BROWNIAN PARTICLES IN EXTERNAL FORCE FIELDS 72
3.4.1 The stationary solution 74
3.4.2 Passage over potential barriers: The Kramers escape problem 74
3.4.3 The field-free case with spatially dependent initial conditions 76
3.4.4 Strong damping regime, Smoluchowski equation 80
3.4.5 Uniform field 81
CHAPTER 4 ■ Kinetic theory: Boltzmann’s approach to irreversibility 91
4.1 FROM MECHANICS TO STATISTICAL MECHANICS 91
4.2 REDUCED PROBABILITY DISTRIBUTIONS 94
4.3 DYNAMICS OF REDUCED DISTRIBUTIONS: THE HIERARCHY 95
4.4 HYDRODYNAMICS AND THE HIERARCHY 97Contents ■ vii
4.4.1 Densities macroscopic and microscopic 97
4.4.2 Balance equations from the hierarchy 99
4.4.3 Microscopic balance equations 103
4.4.4 Hydrodynamics: Adding constitutive relations to conservation laws 104
4.4.5 Linearized hydrodynamics, normal modes 107
4.5 THE BOLTZMANN EQUATION, MOLECULAR CHAOS 109
4.5.1 Derivation 109
4.5.2 Comments 112
4.6 BOLTZMANN’S H-THEOREM, CONNECTION WITH ENTROPY 114
4.6.1 Proof of the H-theorem 114
4.6.2 Equilibrium, collisional invariants, Maxwell-Boltzmann distribution 115
4.6.3 Connection with the second law, Gibbs and Boltzmann entropies 118
4.6.4 Coarse graining and loss of information 120
4.6.5 The reversibility paradox, not 121
4.7 COLLISION FREQUENCY, MEAN FREE PATH 122
4.8 NORMAL SOLUTIONS OF THE BOLTZMANN EQUATION 123
4.8.1 Hilbert’s theorem, hydrodynamic regime 123
4.8.2 Chapman-Enskog method 127
4.9 CHAPMAN-ENSKOG THEORY OF TRANSPORT COEFFICIENTS 131
4.9.1 The heat and momentum fluxes associated with f(1) 131
4.9.2 Calculating A and B 132
CHAPTER 5 ■ Weakly coupled systems: Landau-Vlasov theories 145
5.1 HOMOGENEOUS WEAK COUPLING: THE LANDAU EQUATION 145
5.2 CONNECTION WITH THE FOKKER-PLANCK EQUATION 148
5.3 INHOMOGENEOUS PLASMAS: THE VLASOV EQUATION 149
5.4 LANDAU DAMPING 151
5.4.1 The linearized Vlasov equation 152
5.4.2 Initial value problem 153
CHAPTER 6 ■ Dissipation, fluctuations, and correlations 159
6.1 FLUCTUATIONS AND THEIR CORRELATIONS 159
6.2 LINEAR RESPONSE THEORY 162
6.2.1 Response functions: General derivations, classical and quantum 163
6.2.2 The generalized susceptibility χ(ω) and its analytic properties 165
6.2.3 Identification of χ′′(ω) with dissipation 166
6.2.4 Explicit formulae for the quantum response 168
6.2.5 Brownian motion of a harmonically bound classical particle 169
6.2.6 The relaxation function 170viii ■ Contents
6.3 FLUCTUATION-DISSIPATION THEOREM 171
6.4 GREEN-KUBO THEORY OF TRANSPORT COEFFICIENTS 172
6.4.1 Mechanical transport processes 172
6.4.2 Thermal transport processes 174
6.4.3 Einstein relation 178
6.4.4 Comments 179
6.5 GENERALIZED LANGEVIN EQUATION 179
6.5.1 Derivation 179
6.5.2 Recovering the Langevin equation for Brownian motion 182
6.6 MEMORY FUNCTIONS 183
APPENDIX A ■ Statistical mechanics 189
A.1 CLASSICAL ENSEMBLES: PROBABILITY DENSITY FUNCTIONS 189
A.1.1 Classical dynamics of many particle systems; Γ-space 189
A.1.2 The transition to statistical mechanics 190
A.1.3 Canonical ensemble, closed systems 191
A.1.4 Grand canonical ensemble, open systems 192
A.2 QUANTUM ENSEMBLES: PROBABILITY DENSITY OPERATORS 193
A.2.1 Quantum indeterminacy 193
A.2.2 Many-particle wave functions 194
A.2.3 The density operator 194
A.2.4 Canonical ensemble 196
A.2.5 Grand canonical ensemble 197
A.3 ENTROPY 197
A.4 THE WEYL CORRESPONDENCE PRINCIPLE 198
APPENDIX B ■ Probability theory 201
B.1 EVENTS, SAMPLE SPACE, AND PROBABILITY 201
B.2 RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS 202
B.2.1 Probability distributions on discrete sample spaces; joint
distributions 203
B.2.2 Probability densities on continuous sample spaces; joint densities 203
B.2.3 Moments of distributions 204
APPENDIX C ■ Elastic collisions 205
APPENDIX D ■ Integral equations and resolvents 209Contents ■ ix
APPENDIX E ■ Dynamical representations in quantum mechanics 213
APPENDIX F ■ Causality and analyticity 217
Bibliography 223
Index 235Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com Preface
S TATISTICAL physics—a core component of university curricula—is taught at three levels: ther￾modynamics, equilibrium statistical mechanics, and nonequilibrium statistical mechanics.1
• Thermodynamics is a science of matter that remarkably presupposes no knowledge of the micro￾scopic constituents of matter—the strength and the weakness of that theory. It introduces absolute
temperature T and entropy S as properties of the equilibrium state characterized by the values of
state variables,
2,3 measurable quantities such as pressure P and volume V. A precise definition of
equilibrium is elusive; basically, a system is in equilibrium when none of its macroscopic prop￾erties appear to be changing. No system is truly quiescent, however; it’s a matter of time scales.
Feynman[3, p1] defined equilibrium in temporal terms: “. . . the state where all the fast things have
happened, but the slow things have not,” a working definition we’ll see exemplified in this book. I
have written a companion volume on thermodynamics, [2], which I refer to when specific results
from that subject are required. What we can say about macroscopic systems is what we can mea￾sure. For any system there are only a handful of state variables, yet there are an enormous number
of microscopic components. What we can say therefore represents a projection of huge numbers of
microscopic degrees of freedom onto a small number of macro variables. It could almost be said
that thermodynamics is true by definition—a phenomenological codification of our experience of
the macroscopic world. That the laws of thermodynamics apply to disparate physical systems and
can be expressed mathematically compels the search for a unifying set of microscopic principles.
• Statistical mechanics relates the equilibrium properties of macroscopic systems to the nature of
their microscopic components. All information about a system’s components and their interactions
is contained in the system Hamiltonian, H. A central construct is the partition function Z, which
for N identical particles is4
Z(β,X) ≡ 1
N!

dΓ e−βH(p,q;X) = 1
N!

dE Ω(E)e−βE , (1)
where: β ≡ (kT)−1, with k Boltzmann’s constant;5 dΓ ≡ dN pdN q/h3N is a dimensionless
volume element in N-particle phase space,6 with h Planck’s constant; and X denotes external
1Continually distinguishing equilibrium from nonequilibrium statistical mechanics becomes tedious. Henceforth, the
unadorned term statistical mechanics will be used to mean the equilibrium theory. 2Italic font is used in this book for three reasons: for emphasis, for words and phrases not from the English language,
and when an important term is first used, engendering an index entry. 3State variables (the term is due to Gibbs[1, p2]) represent measurable quantities on macroscopic systems in thermal
equilibrium, the state of which is specified by a list of the values of its state variables. The generic term system indicates
that little (yet macroscopic) part of the universe we consider to be of interest[2, p7]. A correlative of system, not-system, is
known as the environment. Macroscopic is another term that eludes precise definition. Even systems indiscernible to human
senses (not ordinarily considered macroscopic) contain enormous numbers of atoms, and it’s in that sense we use the term
macroscopic: when the number of microscopic constituents is sufficiently large that we’re unable to track the dynamics of
individual particles. 4Equation (1) is classical. Appendix A reviews the foundations of statistical mechanics, classical and quantum. 5In this book, Boltzmann’s constant is denoted k and not kB. 6Familiarity with phase space is assumed, of which we make frequent use. Nolte[4] is a historical review of phase space.
xixii ■ Preface
parameters in the Hamiltonian.7 The quantity Ω(E) is the density of states function, such that
Ω(E)dE is the number of energy levels in the range [E,E + dE]. The Boltzmann factor e−βE
is proportional to the probability that the system has energy E. The partition function therefore
specifies the total number of energy states available to the system at temperature T. The internal
energy of a system interacting with its environment is not fixed, but fluctuates about a mean value
⟨H⟩ caused by energy transfers with the environment. Thermal equilibrium is not the quiescent
place of thermodynamics: Measurable quantities fluctuate in time, and fluctuations motivate the use
of probability and ensembles. I have written a companion volume on statistical mechanics, [5], to
which I refer when specific results from that subject are required.
• Nonequilibrium—other than equilibrium—describes many physical systems because there are
many ways not to be in equilibrium.8 Statistical mechanics is a general theory of a special (yet
ubiquitous) state of matter, thermal equilibrium; it’s built on analytical mechanics (applies to any
mechanical system) and the theory of probability (large numbers of microscopic components of
macroscopic systems). It’s a powerful theory: All thermodynamic quantities can be calculated from
a single formula, Eq. (1). In contrast, there is no comparably general theory of nonequilibrium sys￾tems, because there is no one way to be out of equilibrium. Nonequilibrium statistical mechanics, the
goal of which is to predict the spatiotemporal behavior of macroscopic systems given the nature of
their microscopic components, consists of a collection of different theoretical approaches. What dis￾parate nonequilibrium systems have in common is an irreversible evolution to thermal equilibrium,
the state of maximum entropy consistent with macroscopic constraints,9 and entropy is created in
irreversible processes (those that can’t be reversed in time). Time and irreversibility therefore play
central roles in nonequilibrium theories. Strictly speaking, time plays no role in thermodynamics
and statistical mechanics, although it sneaks in implicitly. Thermodynamics describes the timeless
state of thermal equilibrium, yet the direction of time appears implicitly through the existence of
irreversible processes. Thermodynamics, normally taught as a theory of transitions between equi￾librium states through reversible processes,10 contains the seeds of an extension to nonequilibrium
systems through the study of irreversible processes—see Chapter 1. Time appears implicitly in sta￾tistical mechanics through the existence of fluctuations (forcing us to interpret measurements as
average quantities), yet time is obviated (as it must be in an equilibrium theory) when time averages
are replaced with ensemble averages. Time-dependent properties of fluctuations—the cornerstone
of nonequilibrium statistical mechanics—are covered starting in Chapter 2.
Theories of macroscopic systems can be classified by the extent to which they include time and
fluctuations; see Table 0.1. Running through statistical physics is a four-way division: microscopic
and macroscopic levels of description of equilibrium and nonequilibrium systems. (One can add
subdivisions into classical and quantum treatments.) Thermodynamics is a macroscopic theory of
equilibrium that largely ignores fluctuations.11 Statistical mechanics is a microscopic theory based
on the existence of fluctuations but not on their dynamical properties. With nonequilibrium statis￾tical mechanics we have a theory in which time and fluctuations play central roles; it supersedes
thermodynamics and statistical mechanics.12
7External parameters are set by the environment (or by the experimenter), such as magnetic field strength. Thermody￾namics distinguishes intensive from extensive variables; see [2, p5] and Section 1.2. Equilibrium is achieved when a system’s
intensive variables associated with conserved quantities become equal to their environmental counterparts[2, p45]. 8Defining a subject in terms of a negative—what it’s not—leaves open the question of what it is. 9Gibbs showed that thermal equilibrium is characterized by two equivalent criteria: entropy is a maximum for fixed
energy, and energy is a minimum for fixed entropy[2, p53]. 10The terms reversible and irreversible are defined in footnote 2 of Chapter 1. 11Fluctuations can be included in thermodynamics through the device of virtual variations, conceivable variations of
state brought about by conceptually relaxing the isolation of systems from their environment[2, p45][5, p24]. Treating fluc￾tuations as virtual processes is similar to the use of virtual displacements in classical mechanics, “mathematical experiments”
consistent with existing constraints, but occurring at a fixed time. 12If you’ll permit the digression, an analogy with relativity theory presents itself. General relativity is a theory of space￾time that supersedes special relativity and Newtonian gravitation. Special relativity is in essence a “boundary condition” onPreface ■ xiii
Table 0.1 Time and fluctuations in theories of macroscopic systems
Thermodynamics Equilibrium Nonequilibrium
statistical mechanics statistical mechanics
Role of time Implicit. Implicit. Explicit.
Equilibrium theory Equilibrium theory Goal of the theory
based on time-invariant based on instantaneous is to determine the
state variables. Direction fluctuations in macroscopic time evolution of
of time established by quantities, motivating the macroscopic systems
irreversible processes use of ensembles and toward thermal
associated with entropy probability. Time averages equilibrium.
creation. replaced with ensemble
averages.
Treatment of Implicit. Explicit. Explicit.
fluctuations Modeled as virtual Equal-time correlations Time-dependent
variations of state[2, p45]. of spatially separated fluctuations modeled
fluctuations, but no as stochastic
dynamical properties. processes.
Level of Macroscopic. Microscopic. Microscopic and
description Based on a handful of Information about system macroscopic.
state variables. constituents and their Macroscopic models
interactions enters the of transport involving
theory through the hydrodynamics and
Hamiltonian. Statistical local thermodynamics.
mechanics reproduces Microscopic models
the laws of involving kinetic
thermodynamics. theory and time￾correlation functions.
The book is organized in a way that emulates the passage from thermodynamics to statistical me￾chanics, from macroscopic to microscopic. We begin with the macroscopic theory of irreversibility
based on local thermodynamics and hydrodynamic conservation laws obtained from phenomeno￾logical balance equations. Later (in Chapter 4) we show that balance equations and hydrodynamics
follow from first principles (the Liouville equation). A balance equation for entropy is derived,
allowing us to identify the fluxes and thermodynamic forces contributing to entropy creation in
irreversible processes. Linear force-flux constitutive relations introduce proportionality factors—
kinetic coefficients—which satisfy an experimentally verified symmetry, the Onsager reciprocity
conditions. Chapter 2, on the statistical theory of fluctuations, shows that reciprocity is a conse￾quence of the time-reversal invariance of the microscopic dynamics underlying fluctuations and the
stationarity of equilibrium averages. We can treat fluctuations as dynamical variables, and classify
them by their symmetries under time reversal, yet we have no first-principles way of modeling
their time dependence. We use stochastic processes for that purpose which bring time into prob￾abilistic descriptions. Appendix B is a review of probability as needed in statistical mechanics;
general relativity,[6, p24] the theory of spacetime that applies for vanishing gravity. Only in that sense does general relativity
need special relativity, to handle a limiting case; general relativity is the more comprehensive theory. In the same way, the
more comprehensive theory of nonequilibrium statistical mechanics relies on statistical mechanics to handle the special case
of thermal equilibrium. In this sense, statistical mechanics is a boundary condition on nonequilibrium statistical mechanics.xiv ■ Preface
Chapter 2 introduces stochastic processes, mathematics required in nonequilibrium statistical me￾chanics. Stationary and Markov processes are defined along with random walks, the master equa￾tion, and Gaussian processes. Also covered are thermal noise and Nyquist’s theorem which provide
a segue into the Langevin equation developed in Chapter 3 on Brownian motion and stochastic dy￾namics. The connection we find (with the Langevin equation) between dissipation and the strength
of fluctuations is an instance of a more general result derived in Chapter 6, the fluctuation-dissipation
theorem. The Fokker-Planck equation is derived. Whereas the Langevin equation supplies expres￾sions for nonequilibrium averages of observable quantities, the Fokker-Planck equation is a differ￾ential equation for nonequilibrium probability distributions; the two are closely related. Chapter 4
is devoted to kinetic theory, a more microscopic approach to irreversibility than stochastic meth￾ods. No book on the subject is complete without an extensive treatment of the Boltzmann equation
and the H-theorem. There isn’t a well-defined boundary between kinetic theory and nonequilibrium
statistical mechanics—kinetic theory is more concerned with the time-dependent distribution func￾tion and nonequilibrium statistical mechanics tends to emphasize the relation between irreversible
processes and spontaneous fluctuations; practitioners of both subjects need to be conversant in the
ways of the other. Chapter 5, which touches on the kinetic theory of plasmas, could logically have
been included in Chapter 4, yet that chapter is long enough. Plasma kinetic theory is an active field
of research that we make no attempt to review; it’s basic principles, however, the Landau-Vlasov
equations, are readily developed with material already covered in this book. We treat Landau damp￾ing, a fascinating, experimentally verified, example of irreversibility without dissipation. Chapter 6
concludes with more modern developments in nonequilibrium statistical mechanics that emphasize
time correlation functions—linear response theory, fluctuation-dissipation theorems, Green-Kubo
theory of transport coefficients, and the generalized Langevin equation. Nonequilibrium statistical
mechanics rests on a three-legged stool of stochastic methods, kinetic theory, and time correlation
functions.
Six appendices provide foundations for material used in the book. Appendix A is a review of
statistical mechanics, classical and quantum (the subject can be succinctly summarized), as well
as the topics of von Neumann entropy, the quantum generalization of Gibbs entropy, and the Weyl
correspondence principle. Appendix B reviews elementary probability theory. Appendix C reviews
two-body elastic collisions, essential in formulating the Boltzmann equation, and Appendix D re￾views integral equations, crucial in finding so-called normal solutions of the Boltzmann equation.
Appendix E is a review of the dynamical “pictures” of quantum mechanics (Schrodinger, Heisen- ¨
berg, and interaction representations). Appendix F reviews the mathematics of causality: Kramers￾Kronig relations, Titchmarsh’s theorem, and Fourier transforms of causal functions.
Physics curricula include courses on thermodynamics at the undergraduate level. Statistical me￾chanics is taught at the advanced undergraduate/beginning graduate level and is a requirement of
graduate physics programs. Nonequilibrium statistical mechanics is an advanced topic. A nodding
acquaintance with thermodynamics and statistical mechanics is thereby presumed (which inter alia
presumes a knowledge of elementary probability theory) as well as the standard topics of physics
curricula: analytical mechanics, quantum mechanics, electrodynamics, and mathematical methods.
Some exposure to tensor analysis and dyadic notation would be useful. SI units are used throughout.
To reduce the complexity of equations, multiple integrals (encountered frequently) are denoted with
a single integral sign, e.g.,  dvdr ≡  ···  dvxdvydvzdxdydz. One integral sign to rule them all.
I thank the editorial staff at CRC Press, in particular Dr. Danny Kielty and Rebecca Davies, for
their forbearance of numerous blown deadlines. I thank my family, for they have seen me too of￾ten buried in a computer. My wife Lisa I thank for her encouragement and countless conversations
on how not to mangle the English language. Finally, to students, try to remember that science is a
“work in progress”; more is unknown than known.
James Luscombe
Monterey, CaliforniaCHAPTER 1
Irreversibility, entropy, and
fluctuations
WE start with the macroscopic theory of irreversibility based on thermodynamics and hy￾drodynamics; in this way the issues to be addressed by microscopic theories are estab￾lished. Thermodynamics—the science of thermal equilibrium—contains the seeds of an extension
to nonequilibrium systems (as we show), nonequilibrium or irreversible thermodynamics.
1
1.1 ENTROPY AND IRREVERSIBILITY, CLAUSIUS INEQUALITY
The Clausius inequality for closed systems[2, p35], dS ≥ dQ/T (equality for reversible heat trans￾fers),2 shows that3 dQ/T does not account for all contributions to dS. Clausius[8, p363] termed the
difference
(dS − dQ/T) ≥ 0 (1.1)
the noncompensated transformation (a term we avoid), entropy changes not “compensated” by heat
transfers. The message is that entropy is created in irreversible processes.
4 Thermodynamics as
traditionally taught treats transitions between equilibrium states through reversible processes, with
(1.1) satisfied as an equality. Nonequilibrium thermodynamics studies entropy creation associated
1Either name has issues. Nonequilibrium thermodynamics is an oxymoron (if, as is traditional, thermodynamics is
considered a theory of the equilibrium state) and irreversible thermodynamics is redundant—irreversibility is the central
message of thermodynamics[2, p138]. The name notwithstanding, de Groot and Mazur[7] is the standard text on the subject. 2Thermodynamics distinguishes closed from open systems and reversible from irreversible processes. Closed systems
allow exchanges of energy with the environment but have a fixed mass; open systems allow exchanges of matter[2, p9].
Isolated systems exchange neither energy nor matter. The usual expression dS = dQ/T pertains to reversible transforma￾tions of closed systems; there is an additional contribution to the entropy of open systems associated with transfers of matter
[see Eq. (1.33) and Exercise 1.7]. A process is reversible if it can be exactly reversed through infinitesimal changes in the
environment; system and environment are restored to their original conditions[2, p14]. In irreversible processes, the system
can’t be restored to its original state without producing changes in the environment. The concept of reversibility is prob￾lematic because it envisions the reversal of processes in time. The description of equilibrium states doesn’t involve time, yet
reversible processes are conceived as proceeding through a succession of equilibrium states linked by infinitesimal reversible
transformations. Real changes of state occur at finite rates and thus intermediate states can’t rigorously be considered equi￾librium states. Strictly speaking, reversible processes are idealizations that don’t exist; practically speaking, there is some
small yet finite rate at which processes occur such that disequilibrium has no observable consequences within experimental
uncertainties. All processes are irreversible, some processes can be idealized as reversible. 3We dispense with the customary practice of distinguishing exact from inexact differentials; see [2, p6]. 4I. Prigogine (1977 Nobel Prize in Chemistry), referred to the second law of thermodynamics as the Principle of the Cre￾ation of Entropy[9, p32]. The Clausius inequality derives from Carnot’s theorem, that heat-engine efficiencies are bounded
by those of reversible engines[2, p28]. A violation of Carnot’s theorem is a violation of the Clausius version of the second
law[2, p29]. The Clausius inequality is, therefore, another version of the second law, of which there are many equivalent
ways of stating[2, p139]. Entropy is conserved in reversible processes; it’s created in irreversible processes.
DOI: 10.1201/9781003512295-1 12 ■ Non-Equilibrium Statistical Mechanics
with irreversible processes.5 The state variables of thermodynamics such as temperature T are, in
irreversible thermodynamics, replaced with fields that vary in space and time,6 such as temper￾ature, T(r, t). Nonuniformities in these fields engender irreversible processes; we seek to relate
irreversibility to inhomogeneities, including and especially those created by fluctuations.
We want irreversible thermodynamics to be a local theory.7 We seek partial differential equations
for the spatiotemporal variations of its fields, akin to Maxwell’s equations or fluid dynamics. For
that purpose, we rely on balance equations that describe the changes in time in the amount of a
quantity in a region of space; see Section 1.3. A central achievement of the theory is the derivation
of a balance equation for entropy (see Section 1.5), showing it can change for two reasons and
two reasons only: flows, described by an entropy flux vector JS, Eq. (1.33), and entropy sources,
σS ≥ 0, Eq. (1.34), the rate per volume at which entropy is produced in irreversible processes.
1.2 EQUILIBRIUM, GLOBAL AND LOCAL
A fundamental distinction in thermodynamics is between intensive and extensive quantities. Inten￾sive quantities have the same values throughout a system—pressure P, temperature T, or chemical
potential8 µ—and are independent of the size of the system. Extensive quantities are proportional
to the number of particles in the system N: entropy9 S, internal energy U, or magnetization10 M.
Denote the extensive variables of a system other than entropy {Xi}m
i=1; S is a function of these
quantities,11 S = S(X1,...,Xm). The first law of thermodynamics for a system of n chemical
species,12
dS = 1
T dU +
P
T dV − 1
T
n
k=1
µkdNk, (1.2)
shows that S = S(U, V, N1,...,Nn), where Nk is the number of particles of species k with µk
the chemical potential of the kth-species. An important point is that intensive variables occur in the
theory as partial derivatives of extensive variables,13 e.g., T −1 = (∂S/∂U)V,{Nk}.
5It’s said that thermodynamics should be called thermostatics because of the timeless nature of equilibrium. There
would be merit to that idea if thermodynamics consisted solely of the study of reversible processes. The second law, which
recognizes the existence of irreversibility, prescribes a time order to the states of naturally occurring processes, namely those
in which entropy increases. Time as a variable doesn’t occur in thermodynamics, yet the direction of time does! The term
thermostatics obscures the past-and-future relation of states in irreversible processes. 6The mathematical “arena” of thermodynamics is state space, a mathematical space of the values of state variables,
which by definition are their equilibrium values[2, p5]. A state of equilibrium is represented by a point in this space. Nonequi￾librium thermodynamics is a field theory with its variables defined on space and time. 7Nonlocal, nonequilibrium thermodynamics is an active field of research beyond the intended scope of this book.
8Chemical potential is the energy per particle required to add particles to the system under prescribed conditions (con￾stant S, V or constant T,P). See [5, p14]. 9Entropy is extensive; it scales with the size of the system. Nonextensive entropies lead to a puzzle known as the Gibbs
paradox, the resolution of which is that entropy must be extensive[2, p113]. In axiomatic formulations of thermodynamics,
the extensivity of entropy is taken as a postulate[10, p25]. 10In thermodynamics, M is the total dipole moment of the system. In electromagnetic theory, M is the magnetization
density, magnetic moment per volume. In thermodynamics, M is an extensive quantity; in electromagnetism M is intensive. 11The zeroth law of thermodynamics implies the existence of functional relations among state variables, equations of
state[2, p12], of which the entropy function can be considered, S = S(X1,...,Xm). In axiomatic formulations, entropy
as a function of extensive variables is taken as a postulate[10, p24]. 12Equation (1.2) is variously referred to as the entropy form of the first law, or the combined first and second laws of
thermodynamics. I prefer simply the first law, the first law written in terms of exact differentials. Equation (1.2), with the
µdN terms, is sometimes referred to as the Gibbs equation. The first law in the form of Eq. (1.2) is referred to as the
entropy representation,[2, p53] where S = S(U, V, {Nk}) is the dependent variable. Writing it in the equivalent form
dU = TdS − PdV +
k µkdNk, with U = U(S, V, {Nk}) dependent, is the energy representation. Thermodynamics
can be developed in either way; nonequilibrium thermodynamics mainly uses the entropy representation. 13For this reason it’s sometimes said that extensive variables are the primary variables of thermodynamics. Through the
use of Legendre transformations, intensive variables can be taken as independent variables in thermodynamics. See Chapter
4 of [2], Thermodynamic potentials: The four ways to say energy.Irreversibility, entropy, and fluctuations ■ 3
Extensive quantities are additive over subsystems.14 Entropy is the sum of subsystem entropies
S(α)
, S = 
α S(α)
. Subsystem entropies are functions of subsystem extensive variables, S(α) =
S(α)
(U(α)
, V (α)
, N(α)
1 ,...,N(α) n ), with 
α U(α) = U,

α V (α) = V , and 
α N(α)
k = Nk,
k = 1,...,n. The first law for a subsystem is [same form as Eq. (1.2)]
dS(α) = 1
T(α) dU(α) +
P(α)
T(α) dV (α) − 1
T(α)
n
k=1
µ(α)
k dN(α)
k , (1.3)
where (T(α)
)−1 = 
∂S(α)
/∂U(α)

V (α),{N(α)
k }, etc. The subsystems of a system in global thermo￾dynamic equilibrium are mutually in equilibrium (zeroth law of thermodynamics), with equality of
subsystem intensive variables,15 T(α) = T, P(α) = P, and µ(α)
k = µk, k = 1,...,n. In global
equilibrium, there is no need for superscripts on the local intensive variables in Eq. (1.3).
For systems not rigorously in global equilibrium, we assume that sufficiently small vol￾umes dV (α) (subsystems) can be found over which variations in state variables are negligi￾ble, with Eq. (1.3) holding for each subsystem, the assumption of local thermodynamic equilib￾rium.
16 For systems divided into local-equilibrium subsystems, local entropy functions S(α) =
S(U(α)
, V (α)
, N(α)
k ) (or local equations of state) have the same functional form of its variables as
the global entropy function, S = S(U, V, Nk), or equation of state.
17
Thermodynamics establishes that entropy is created in irreversible processes (Clausius inequal￾ity), but nothing in that theory can tell us the rate of entropy creation. Nonequilibrium thermody￾namics specifies a rate by dividing Eq. (1.3) by dt:
T ∂S
∂t ≡ ∂U
∂t + P ∂V
∂t −n
k=1
µk
∂Nk
∂t , (1.4)
where we’ve erased subsystem labels with the understanding that all quantities are local. Equation
(1.4) defines the rate of change of entropy in terms of the variations of the quantities on the right
side; it does not emerge as a result of the underlying microscopic dynamics. Such an approach can
be justified only a posteriori, by the validity of the conclusions derived from it. Where do we get the
time derivatives on the right of Eq. (1.4)? We can’t invoke thermodynamics again. Fortunately for
us, the extensive quantities (U, V, Nk) are associated with microscopic properties of matter. Their
rates of change are described by hydrodynamic conservation laws (see Section 1.4).
Equation (1.4) can be rewritten in terms of specific, per-mass quantities. In that way, the PdV
term is expressed as a variation in density ρ, a local system property. Let s ≡ S/M, u ≡ U/M, and
ρ−1 ≡ V /M denote specific entropy, internal energy, and volume, with M the total system mass.
Thus, ρs and ρu are per-volume densities of entropy and internal energy. The mass of species k, Mk,
has Nk = Mk/mk = M ck/mk particles, with mk the molecular mass and ck ≡ Mk/M the mass
fraction. The specific chemical work18 of the kth-species, µkdNk/M = (µk/mk)dck ≡ µkdck,
where µk = µk/mk denotes the kth specific chemical potential. Thus, we can write Eq. (1.4)
T ∂s
∂t = ∂u
∂t + P ∂
∂tρ−1 −n
k=1
µk
∂ck
∂t . (1.5)
14Additivity over subsystems requires that the thermodynamic limit exists of N → ∞, V → ∞ with N/V fixed[5,
p103]. The concepts of extensive and intensive don’t apply to systems so small that surface effects dominate[11, p10]. 15The general theory of thermodynamic equilibrium requires equality between subsystem-intensive variables associated
with (conjugate to) conserved quantities—energy, volume, and particle number[2, p44]. 16As noted in the Preface, nonequilibrium statistical physics is a collection of approaches to modeling nonequilibrium
phenomena. With the picture of local equilibrium, we’ve specified a class of nonequilibrium systems to which macroscopic
reasoning can be applied. 17This follows from the use of continuous functions in nonequilibrium thermodynamics to represent state variables.
18The small amount of chemical work (if it’s not heat, it’s work[2, p10]) µkdNk is the energy to change the number of
particles of species k by dNk, holding S, V, and all other mole numbers Nk fixed, with µk = (∂U/∂Nk)
S,V,Nk [2, p39].4 ■ Non-Equilibrium Statistical Mechanics
1.3 BALANCE EQUATIONS: FLUXES AND SOURCES
A balance equation for quantity ψ is a relation among integrals,
d
dt

V
ρψdr = −

S
Jψ · dS +

V
σψdr (1.6)
where ρψ is the volume density of ψ, V is a volume bounded by surface S having outward-pointing
surface element dS, Jψ is the flux of ψ through S, and σψ is a source, the rate per volume at which
ψ is created or destroyed in V . Equation (1.6) indicates that changes in the amount of ψ in V are
accounted for by 1) flows through S and 2) the creation or destruction of ψ in V by means other
than flow; there are no other possibilities.
19 Balance equations do not presume thermal equilibrium.
By applying the divergence theorem, we have the local form of balance equations,
∂
∂tρψ + ∇ · Jψ = σψ. (1.7)
If σψ = 0, ψ is said to be conserved; in that case, the only way ψ in V can change is to be
transported through S. When ψ is carried by a fluid, convective transport, Jψ = ρψv, where v is the
fluid velocity. In diffusive transport, fluxes are proportional to gradients in system parameters Qψ,
Jψ ∝ ∇Qψ; see examples in Section 1.6. We now derive balance equations for mass, momentum,
and energy;20 readers uninterested in the details could skip to Section 1.5.
1.4 HYDRODYNAMIC CONSERVATION LAWS
1.4.1 Mass conservation, convective derivative
The balance equation for a mass of chemical species k convectively transported is, from Eq. (1.6),
d
dt

V
ρkdr = −

S
ρkvk · dS +r
j=1

V
νkjJjdr, (1.8)
where ρk is the mass density of species k, vk is its velocity, Jj is the rate at which the jth-chemical
reaction occurs,21 and νkjJj is the rate at which species k is produced in reaction j. The quantity
νkj is the stoichiometric coefficient measured in units of mass density of species k in reaction j.
Example: Consider the chemical reaction 2H2 + O2 −→ 2H2O. This is a balanced chemical equa￾tion: The number of atoms of each element among the reactants (substances undergoing the reac￾tion) is the same as the number of atoms among the products (substances created by the reaction).
Chemical reactions can be written 
i ν0
i Xi = 0, where Xi is the chemical symbol of the i
th species
and ν0
i is the bare stoichiometric coefficient, counted as positive for products and negative for re￾actants: ν0
H2 = −2, ν0
O2 = −1, ν0
H2O = 2. The quantities νkj in Eq. (1.8) are expressed in units of
the density of species k appearing in reaction j; thus νkj ∝ mkν0
kj , with mk the molecular mass.
In the reaction, two H2 molecules combine with an O2 molecule to produce two H2O molecules.
In terms of mass,22 2mH2 + mO2 = 2mH2O. Whatever is the unit of density adopted in Eq. (1.8)
[grams/cc or kg/(cubic mile)], we can assign (using the molecular mass of the species involved)
νH2 = −1
9 , νO2 = −8
9 , and νH2O = 1. For every density unit of H2O produced, 1
9 of a density unit
of H2 disappears, along with 8
9 of a density unit of O2.
19We’ve introduced balance equations phenomenologically, per our experience of the macroscopic world. We show in
Chapter 4 that balance equations follow from first principles of physics. 20Balance equations for mass and energy-momentum comprise five scalar equations. In relativity theory, mass, energy,
and momentum are interrelated such that there are four equations expressing conservation of energy-momentum[6, p181]. 21No confusion should arise between J for the rate of a chemical reaction and the symbol J for current density. 22Mass changes due to energies released in chemical reactions (E = mc2) are negligibly small.Irreversibility, entropy, and fluctuations ■ 5
Equation (1.8) implies the local balance equation for each species k = 1,...,n,
∂ρk
∂t + ∇ · ρkvk = r
j=1
νkjJj . (1.9)
Because mass is conserved in chemical reactions, for each j = 1,...,r,
n
k=1
νkj = 0 (1.10)
(in the aforementioned example, 1 − 1
9 − 8
9 = 0). Summing Eq. (1.9) over k and using Eq. (1.10),
we have the continuity equation for total mass conservation:
∂ρ
∂t + ∇ · ρv = 0, (1.11)
where ρ ≡ 
k ρk is the total density and v ≡ 
k ρkvk/ρ is the center-of-mass velocity.
Formulas involving convected quantities can be compactly expressed when the total time deriva￾tive is written as the convective derivative (see [12, p73]),
Dw ≡ ∂
∂t + w · ∇. (1.12)
Consider the spatiotemporal change of a vector field A(r, t). For small (dt, dr), A(r+dr, t+dt) ≈
A(r, t)+dt(∂A/∂t)+(dr · ∇) A; by forming the difference quotient, we have in the limit dt → 0,
∂A/∂t + (w · ∇) A ≡ DwA, with w ≡ dr/dt. In terms of this operator, the continuity equation
has the form
Dvρ = −ρ∇ · v. (1.13)
If the local divergence ∇ · v < 0 (> 0), fluid is being compressed (expanded) as it flows; if
∇ · v = 0, the fluid is incompressible. Sometimes it’s easier to write the continuity equation as
Dv (1/ρ) = 1
ρ
∇ · v. (1.14)
An important concept is diffusion flow,
23 Jd
k ≡ ρk (vk − v), the flux of a species relative to the
center of mass velocity. Diffusion flows contribute to the entropy of open systems; see Eq. (1.33).
Note that 
k Jd
k = 0; the total mass flux in the center of mass frame is zero (implying that only
(n − 1) of the diffusion flows are independent). For k = 1,...,n,
Dvρk = −ρk∇ · v − ∇ · Jd
k +r
j=1
νkjJj . (1.15)
Summing Eq. (1.15) over k results in Eq. (1.13). Equation (1.15) simplifies with the mass fractions
ck ≡ ρk/ρ:
ρDvck = −∇ · Jd
k +r
j=1
νkjJj . (1.16)
The mass fractions sum to unity, 
k ck = 1; the sum of Eq. (1.16) over k vanishes. Only (n − 1)
of the mass fractions are independent.
23Not the same as the gradient-driven, diffusive transport defined on page 4, J ∝ ∇Q.6 ■ Non-Equilibrium Statistical Mechanics
1.4.2 Momentum balance, stress tensor
The balance equation for momentum is
d
dt

V
ρvdr
   rate of change of
momentum in V
= −

S
ρvv · dS
   momentum convected
through S
+

S
T · dS
   momentum produced by
internal forces at S
+ n
k=1

V
ρkFkdr
   momentum produced
by external forces
. (1.17)
The first term on the right accounts for the flow of momentum through S;
24 the other terms represent
momentum sources—forces—those coupling to S and those coupling to V (Fk is an external force
per mass coupling to species k). The stress tensor T in Eq. (1.17) represents the stresses25,26 associ￾ated with forces acting at surfaces, Tij ≡ limδSj→0(δFi/δSj ), the i
th-component of force acting on
the jth-component of surface area when surface area is represented as an outwardly oriented vector.
Equation (1.17) written in terms of vector components is:

V
∂ρvi
∂t dr = −

S
ρviv · dS +
3
j=1

S
TijdSj +n
k=1

V
ρkFk,idr. (i = 1, 2, 3)
The form of T depends on the nature of the internal forces of the system. For systems with
no internal torques (assumed here), T is symmetric,27 Tij = Tji. A pressure P acting normally at
surfaces, regardless of the orientation of the surface, is termed the hydrostatic pressure, with Tij =
−P δij . Shear stresses are modeled with the viscous stress tensor Π, with Tij = −P δij + Πij . The
most general second-rank tensor such that in uniform rotation there is no friction due to viscosity
has the form[15, p48] (we denote components of the gradient vector, ∇i ≡ ∂/∂xi)
Πij = η

∇jvi + ∇ivj − 2
3 δij∇ · v

+ ζδij∇ · v, (1.18)
where η > 0,ζ > 0 are phenomenological viscosity coefficients. The total stress tensor is written
T = −PI + Π, (1.19)
where I is the unit tensor with elements Iij = δij . We return to Π in Section 4.4.4.
From Eq. (1.17) we have the equation of local momentum balance,
∂
∂t(ρv) + ∇ · (ρvv) ≡ ρDvv = ∇ · T +n
k=1
ρkFk = −∇P + ∇ · Π +n
k=1
ρkFk, (1.20)
where the divergence of a second-rank tensor—a vector—has components,
[∇ · T]
i ≡ 
j
∇jTij . (1.21)
Using Eq. (1.18), Eq. (1.20) implies Newton’s second law for fluids, the Navier-Stokes equation,
ρDvv = −∇P + η∇2v + 
ζ + 1
3 η

∇(∇ · v) +n
k=1
ρkFk. (1.22)
24We’ve used dyadic notation (see [13, Chapter 11] or Morse and Feshbach[14, pp54–92]), a notation for tensors due to
Gibbs involving the juxtaposition of vectors with (vv)ij ≡ vivj . 25Stress has the dimension of force per area with SI units of Pascals (Pa).
26Sometimes ρvv is included in the stress tensor[15, p47]. The stress tensor is used in fluid mechanics and the theory
of elasticity. The negative of the stress tensor, the pressure tensor P ≡ −T is used in kinetic theory (see Chapter 4). Bold
Roman notation T refers to tensors as basis-independent objects. Just as we distinguish vectors A (bold italic font) from
their basis-dependent components Ai, we must distinguish tensors T from their components in a particular basis, Tij . 27Internal angular momentum is conserved in systems with symmetric stress tensors (see Exercise 4.19) and hydrody￾namics normally makes use of symmetric stress tensors. There are materials, however, requiring an antisymmetric part of the
stress tensor, such as nematic liquid crystals[16, p155]. Second-rank tensors can always be decomposed into symmetric and
antisymmetric parts, Tij = 1
2 (Tij + Tji) + 1
2 (Tij − Tji).Irreversibility, entropy, and fluctuations ■ 7
1.4.3 Energy conservation, heat flux
We develop balance equations for the center-of-mass kinetic energy and the potential energy asso￾ciated with external forces, energies we refer to as mechanical. We then consider internal energy,
the energies of thermal agitation and molecular interactions.28
Mechanical energy
A work-energy theorem follows by projecting the momentum balance equation (1.20) onto the
center-of-mass velocity v and using the results of Exercise 1.1:
v ·
 ∂
∂t(ρv) + ∇ · (ρvv)

= ∂
∂t
 1
2 ρv2
+ ∇ ·  1
2 ρv2v

= v ·

∇ · T +n
k=1
ρkFk

. (1.23)
Kinetic energy is not conserved: The source term on the right of Eq. (1.23) is the rate of work done
by internal and external forces.29 For future reference, the scalar v ·∇· T can be written
v · [∇ · T] ≡ 
3
ij=1
vi∇jTij = 
3
ij=1
∇j (viTij ) − 
3
ij=1
Tij∇jvi ≡ ∇ · [v · T] − T:∇v, (1.24)
where the double-dot notation A:B ≡ 
ij AijBij indicates a contraction of second-rank tensors
to produce a scalar, a generalization of the dot-product notation of vectors A·B (first-rank tensors).
Assume conservative external forces (per mass) derivable from potential functions, Fk =
−∇Φk (so that ρkΦk is an energy density), with ∂Φk/∂t = 0. The total potential energy asso￾ciated with external forces ρΦ ≡ n
k=1 ρkΦk satisfies the equation of motion30 [use Eq. (1.9)],
∂
∂t (ρΦ) = n
k=1
Φk
∂ρk
∂t = −
n
k=1
Φk∇ · (ρkvk) +r
j=1
Jj
✚✚✚✚✚❃0
n
k=1
Φkνkj . (1.25)
The last term vanishes if potential energy is conserved in chemical reactions, 
k Φkνkj = 0 (as￾sumed). Using ∇ · (Φkρkvk)=Φk∇ · (ρkvk) + ρkvk · ∇Φk, we have from Eq. (1.25)
∂
∂t (ρΦ) + ∇ · n
k=1
Φkρkvk

= −
n
k=1
ρkvk · Fk = −
n
k=1
Jd
k · Fk − v ·
n
k=1
ρkFk. (1.26)
The loss of potential energy associated with the work done by diffusion flows against external
forces, −
k Jd
k · Fk, appears as a gain in the internal energy of the system; see Eq. (1.31). The
loss of potential energy associated with the power expended by external forces, −v ·
n
k=1 ρkFk,
appears in Eq. (1.23) as a kinetic energy gain. The source terms in Eq. (1.26) represent conversions
of potential energy into internal energy and center-of-mass kinetic energy.
Adding Eqs. (1.23) and (1.26), we have a balance equation for mechanical energy,
∂
∂t
 1
2 ρv2 + ρΦ

= − ∇ · 
1
2 ρv2v +n
k=1
Φkρkvk − v · T

−n
k=1
Jd
k · Fk − T:∇v
≡ − ∇ · Jmech + σmech,
28Don’t be misled—all energies here are mechanical in origin; the distinction with internal energy is traditional in ther￾modynamics. 29The divergence of T is a force density. 30The term equation of motion is used generically to indicate the time derivative of a quantity (a common practice in
theoretical physics); it’s not necessarily intended as its meaning in classical mechanics.8 ■ Non-Equilibrium Statistical Mechanics
with
Jmech = 1
2 ρv2v +n
k=1
Φkρkvk − v · T
σmech = −
n
k=1
Jd
k · Fk − T:∇v. (1.27)
Internal energy
Mechanical energy is not conserved:31 σmech ̸= 0. We have to account for internal energy, the kinetic
energy of velocities relative to the center-of-mass velocity (known as thermal motions) and the
potential energy of intermolecular forces. Working with specific quantities, define the total energy
density ρe as the sum of mechanical and internal energy densities,32 ρe ≡ ρ
 1
2 v2 +Φ+ u

. We
“want” energy conservation, ∂(ρe)/∂t + ∇ · JE = 0, where JE is to be determined. If mechanical
energy isn’t conserved, neither is internal energy, implying a balance equation,
∂ρu
∂t + ∇ · JU = σU , (1.28)
where JU and σU are to be determined. Combine these definitions,
∂ρe
∂t + ∇ · JE = ∂ρu
∂t +
∂
∂tρ
 1
2 v2 + Φ
+ ∇ · JE = σU + σmech + ∇ · (JE − Jmech − JU ).
Total energy conservation is achieved theoretically by choosing
σU = −σmech JE = Jmech + JU . (1.29)
The rate at which mechanical energy is lost is balanced by the rate at which internal energy is
gained. That leaves JU undetermined. Allowing for convection of internal energy, take
JU ≡ ρuv + JQ, (1.30)
which passes the buck to JQ, the heat flux.
33 We then have a balance equation for internal energy
∂ρu
∂t + ∇ · (ρuv) ≡ ρDvu = −∇ · JQ − P∇ · v + Π:∇v +n
k=1
Jd
k · Fk. (1.31)
1.5 ENTROPY SOURCES: FLUXES AND THERMO-FORCES
We’re now in a position to assemble a balance equation for entropy. Using Eqs. (P1.1) and (P1.2),
∂(ρs)
∂t + ∇ · (ρsv) = ρDvs = ρ
T

T Dvs

= 1
T

ρDvu + ρP Dvρ−1 − ρ
n
k=1
µkDvck

(1.32)
= 1
T

− ∇ · JQ + Π:∇v +n
k=1
Jd
k · Fk +n
k=1
µk∇ · Jd
k +r
j=1
JjAj

,
where we’ve used Eqs. (1.31), (1.14), and (1.16), and where Aj ≡ −n
k=1 µkνkj denotes the
chemical affinity.
34 Affinities play the role of driving forces in chemical reactions[2, p89]. In equi￾librium, Aj = 0. Note the absence of pressure P in the final equality of Eq. (1.32): Hydrostatic
pressure (which is isotropic) makes no contribution to entropy creation.
35
31Thermodynamics pertains to mechanically nonconservative systems[2, p9]. 32No confusion should arise between e as the total energy per mass and the common notation for electrical charge. 33Equation (1.30) is phenomenological; convection has been put in “by hand” with no way of specifying JQ. We show
in Section 4.4.2 that internal energy convection occurs from first principles and we derive a microscopic expression for JQ. 34Chemical affinities have the dimension energy density. The term affinity is used in different ways in physics and
chemistry—its use is not unambiguous. We use the term as introduced by De Donder and Van Rysselberghe[17] to rep￾resent Clausius’s noncompensated heat associated with irreversible chemical reactions. 35Cosmology students take note. The Friedmann equations predict a reversible, isentropic expansion of the universe[6,
p351]. Pressure contributes to gravitation through the equivalent mass density P/c2, but not to entropy creation.Irreversibility, entropy, and fluctuations ■ 9
Using the identities
∇ · JQ
T

= 1
T ∇ · JQ + JQ · ∇ 1
T

∇ · 
µk
Jd
k
T

=µk
T ∇ · Jd
k + Jd
k · ∇µk
T

,
Eq. (1.32) can be put into the form of a balance equation, ∂ (ρs) /∂t + ∇ · JS = σS, with
JS ≡ ρsv +
1
T JQ − 1
T
n
k=1
µkJd
k (1.33)
σS ≡ 1
T
r
j=1
JjAj + JQ · ∇ 1
T

+n
k=1
Jd
k ·

Fk
T − ∇
µk
T
 +
1
T ∇v:Π. (1.34)
Equation (1.34), an explicit formula for entropy sources, is a significant achievement. In thermody￾namics, heat is the difference between changes in internal energy and the work done on or by the
system,[2, p8] Q ≡ ∆U − W. The formulas for JS and σS are manifestations of that reasoning—
energy and work are associated with hydrodynamic conservation laws.
Of the terms in the entropy flux JS, we have entropy convection ρsv, entropy transport from
heat flows JQ/T, and entropy transport from diffusion flows µkJd
k/T. The source σS is a sum
of four terms, each of which is a product of two factors: a rate at which processes occur, generally
referred to as fluxes, and a term associated with inhomogeneities, known as thermodynamic forces.
36
In each of the four terms in σS, there is a coupling of forces and fluxes of equal tensor rank.37 We
have in Eq. (1.34): 1) products of scalars (zeroth-rank tensors), r
j=1 JjAj ; 2) scalar products of
vectors (first-rank tensors); and 3) a contraction of second-rank tensors ∇v:Π (the gradient of a
vector is a second-rank tensor). The entropy source is bilinear38 in fluxes J (whether scalar, vector,
or tensor) and forces F (whether scalar, vector, or tensor), the form of which we can write
σS = 
α
Jα ◦ Fα, (1.35)
where α indicates scalar, vector, tensor, and ◦ denotes the operation required to produce a scalar
from the composition J ◦ F, whatever the tensor character of J and F.
1.6 LINEAR FORCE-FLUX RELATIONS, KINETIC COEFFICIENTS
The form of Eq. (1.35) seemingly implies a connection between causes of nonequilibrium phe￾nomena, forces F, and their associated effects, fluxes J, but that’s misleading because they’re not
independent physical effects (as we show in Section 1.7). Temperature gradients are produced by
placing different system boundaries in contact with heat reservoirs at different temperatures; gradi￾ents in chemical potentials are maintained through concentration differences; shear flows are main￾tained through suitable boundary conditions. Such influences set up the system’s response in the
form of fluxes. The distinction between cause and effect disappears, however, when it’s realized
36This description applies to the chemical affinities Aj , which are gradients of thermodynamic potentials (U, H, F, G,
see [2, p55]) with respect to reaction coordinates ξj ; for example, Aj = −(∂G/∂ξj )T ,P [2, p89]. Nonzero chemical
affinities are a measure of the extent to which reactions are not in chemical equilibrium. Thermodynamic forces have nothing
to do with Newtonian forces. Thermodynamic forces are also referred to as affinities—another use of that term. 37By Curie’s theorem (which according to one author, Curie neither stated nor proved[18, p35]), forces of a given tensor
rank do not give rise to fluxes of another tensor rank in isotropic media. See de Groot and Mazur[7, p58]. 38A bilinear form is a function f(u, v) linear in both arguments: f(u+v, w) = f(u, w)+f(v, w) and f(u, v+w) =
f(u, v) + f(u, w), and, for scalar λ, f(λu, v) = λf(u, v) = f(u, λv). See [6, p89] or [13, p342].10 ■ Non-Equilibrium Statistical Mechanics
that temperature differences (for example) in the interior of a system, away from boundaries, are
maintained by heat currents. Forces set up fluxes and fluxes induce forces.39 For the theory to have
predictive power, it must be augmented with constitutive relations between fluxes and forces.
Equation (1.34) has limitations apart from the need for constitutive relations to obtain closure.
It was derived by assembling balance equations for the quantities on the right of Eq. (1.5) (first law
of thermodynamics), where each brings with it a flux for the associated quantity. Balance equations
for mass, momentum, and energy [Eqs. (1.11), (1.20), and (1.31)], were derived assuming convec￾tion to be the mechanism responsible for establishing fluxes. Equation (1.34) therefore applies to
isotropic multi-component fluids, but not to solids or liquid crystals which are said to have broken
symmetries.
40 Additional slow modes emerge in such systems that must be taken into account, e.g.,
long-wavelength spin waves in magnets or phonons in crystalline solids. All slow processes must be
included in models of nonequilibrium phenomena.41 Balance equations for mechanical quantities
are not limited to systems in equilibrium. The balance equation for entropy, however, is valid only
for macroscopic systems slightly out of equilibrium; Eq. (1.5) holds for changes slow compared
with the time scale over which local equilibrium is established.
Diffusive transport is caused by the spontaneous movement in some quantity from regions of
higher concentration to regions of lower concentration. Under a wide range of experimental con￾ditions, diffusive flows are described by linear relations between fluxes and gradients in system
properties Qψ,
Jψ = − (transport coefficient) ∇Qψ,
with the transport coefficient as the proportionality factor. Examples include: Fourier’s law, a re￾lation between heat flux JQ and temperature gradient, JQ = −κ∇T, where κ is the thermal con￾ductivity; Fick’s law, a relation between particle flux Jn and concentration gradient, Jn = −D∇n,
where D is the diffusion coefficient and n is the number density, number of particles per volume;
and Ohm’s law, a relation between charge flux, the current density Jq, and electric field, the neg￾ative gradient of the electrostatic potential ϕ, Jq = −σ∇ϕ, where σ is the electrical conductivity.
Gradient relations extend to tensor quantities. In fluids, elements of the viscous stress tensor Πij , the
flux of the i
th-component of momentum through the jth-component of surface area, are proportional
to gradients of the velocity field, Πij = −η∂vi/∂xj , with η the viscosity coefficient, Newton’s law
of viscosity.
42 These relations are summarized in Table 1.1.
Table 1.1 Basic gradient-driven transport processes
Flux Force Transport relation Name
Heat, JQ ∇T JQ = −κ∇T Fourier’s law
Particle, Jn ∇n Jn = −D∇n Fick’s law
Charge, Jq ∇ϕ Jq = −σ∇ϕ Ohm’s law
(Momentum)x through (δS)y, Πxy
∂vx
∂y Πxy = −η
∂vx
∂y Newton’s law of viscosity
39Ohm’s law provides an example where an electric current (flux) I = ∆V /R flows between the nodes of a circuit when
a voltage difference (force) ∆V is maintained between them, where R is the resistance. A current I, however, maintained
between the same nodes establishes a voltage difference ∆V = IR. Which description is “right”? 40We’re referring to continuous symmetries, such as translational or rotational, where an infinitesimal parameter—
rotation angle, for example—can be continuously varied, as opposed to discrete symmetries such as parity or time reversal. 41Recall Feynman’s definition: equilibrium is the state where the fast things have happened but the slow things not.
42A Newtonian fluid is one in which viscous stresses are linearly correlated with the local strain rate[12, p146].Irreversibility, entropy, and fluctuations ■ 11
A complication can and does occur where a process characterized by the gradient in one quan￾tity causes the occurrence of other gradient-driven processes. For example, when a temperature
gradient is established, not only is a flow of heat set up but also an electric field (thermoelectricity).
The strength of the field is proportional to the temperature gradient, E = P∇T, where P is the
thermopower of the material.43 In electrically neutral systems, a region depleted of charge carriers
sets up an electric field to oppose the motion of charge.44 The voltage difference ∆V between two
points held at a temperature difference ∆T is ∆V = P ∆T, the Seebeck effect. Charges arriving
from a region of higher temperature carry “excess” energy relative to the environment at the colder
location. The rate of energy deposition, Q˙ = ΠI, is proportional to the current I (the Peltier effect),
with Π the Peltier coefficient. The Peltier effect is said to be conjugate to the Seebeck effect. In the
Seebeck effect, charges in motion (because of ∆T) create a voltage difference ∆V ; in the Peltier
effect, charges in motion (because of ∆V ) create heat. Another conjugate pair is the coupling of
particle diffusion and heat conduction. In the Soret effect (thermophoresis), a concentration gradient
is established as a result of a temperature gradient. In the conjugate Dufour effect, a temperature
gradient arises from a concentration gradient.
Conjugate effects can be modeled by adding appropriate terms to the transport relations in Table
1.1. One could add a temperature gradient to Fick’s law, Jn = −D1∇n − D2∇T, to produce
a model in which particle diffusion occurs because of a concentration gradient and a temperature
gradient. One could add a concentration gradient to Fourier’s law, JQ = −κ1∇T − κ2∇n, for a
model in which heat flow occurs because of a temperature gradient and a concentration gradient.
The point is, the same forces (∇T and ∇n) can give rise to different fluxes.
To enable the treatment of irreversible phenomena of arbitrary complexity, we allow that any
force can contribute to any flux, which we express as a system of linear equations,45
Ji = n
k=1
LikFk, (i = 1,...,n) (1.36)
where Ji, Fk are the Cartesian components of fluxes and forces, with the phenomenological terms
Lik the kinetic coefficients. The diagonal terms Lii are related to basic transport coefficients and the
off-diagonal terms Lik, k ̸= i, are connected to conjugate phenomena. Combining Eq. (1.36) with
the “component” form of Eq. (1.35),
σS = 
i
JiFi = 
i

k
LikFiFk ≥ 0. (1.37)
The square matrix of kinetic coefficients [Lik] must therefore be positive definite (from the physics),
which constrains the diagonal terms to be non-negative, Lii ≥ 0, and the off-diagonal terms to
satisfy the inequality LiiLkk ≥ 1
4 (Lik + Lki)
2
. There is no constraint on the sign of the off￾diagonal terms. We’ll see that the matrix [Lik] is symmetric.
1.7 FORCES AND FLUXES LINKED THOUGH FLUCTUATIONS
We now come arguably to the most important section of this chapter. Onsager developed a
general theory of linear irreversible processes based on what they have in common, entropy
creation[19][20]. A fundamental result of that theory is, for appropriately defined forces and fluxes,
43The thermopower is often denoted with the symbol S. To avoid confusion with entropy, we’ve used the symbol P. 44Charge carriers are much less massive than the ions they leave behind.
45The restriction to linear relations between fluxes and forces specifies a class of phenomena, those described by linear
irreversible thermodynamics. Extensions to nonlinear processes is an active field of research.12 ■ Non-Equilibrium Statistical Mechanics
the kinetic coefficients are symmetric, the Onsager reciprocal relations,
46
Lik = Lki. (i, k = 1,...,n) (1.38)
These relations are a kind of Newton’s third law, that if flux Ji is influenced by force Fk, then
flux Jk is influenced by force Fi with equal strength, Lik = Lki. This simplifies the analysis of
coupled irreversible processes.47 The reciprocal relations have been verified experimentally[22][23]
and could be considered another law of thermodynamics at the macroscopic level.48 We show in
Chapter 2 that Eq. (1.38) follows from basic ideas of statistical mechanics.
To explain “appropriate definition,” we must examine fluctuations because forces and fluxes are
linked through fluctuations.
49 Denote equilibrium values of extensive quantities X0
i , and let
αi ≡ Xi − X0
i (i = 1,...,n) (1.39)
represent the instantaneous deviation50 of Xi from X0
i . Entropy is maximized in equilibrium:
S0 ≡ S(X0
1 ,...,X0
n) is the maximum value S can have subject to macroscopic constraints[2, p53].
Associated with a prescribed set of fluctuations {α1,...,αn} is an entropy fluctuation ∆S ≡ S−S0
which, to lowest order in small quantities, is found from the expression51
∆S(α1,...,αn) ≈ 1
2
n
jk=1
∂2S0
∂Xj∂Xk
αjαk ≡ −1
2
n
jk=1
gjkαjαk, (1.40)
where the “metric” gjk = −∂2S0/∂Xj∂Xk is symmetric, gjk = gkj . Fluctuations about equilib￾rium result in momentary decreases in entropy (the stability condition of thermodynamics[5, p26]),
and thus the matrix [gjk] is positive definite.52 The maximum value S0 is associated with the state
of greatest disorder consistent with macroscopic constraints; fluctuations are momentary creations
of more ordered (less disordered) system configurations, implying ∆S < 0. Fluctuations create
nonequilibrium configurations from which the system returns to the state of maximum entropy.53
Entropy creation is associated with irreversibility; irreversibility is implied by entropy creation.
A rate of entropy change associated with fluctuations can be found by dividing Eq. (1.40) by dt,
d
dt
∆S(α1,...,αn) = 
k
∂∆S
∂αk
d
dt
αk = −

kj
gkjαj
d
dt
αk, (1.41)
where the first equality follows from the chain rule with fluctuations treated as differentiable dy￾namical variables; the second equality follows from Eq. (1.40) and the symmetry of gjk. The mid￾dle expression in Eq. (1.41) for d∆S/dt has the form of the middle expression in Eq. (1.37) for σS.
That invites us, after we get the dimensions right,54 to identify the rate Jk as a time derivative of αk
and the force Fk as the partial derivative of ∆S with respect to αk:
46Onsager received the 1968 Nobel Prize in Chemistry for discovery of the reciprocal relations. Equation (1.38) applies
in the absence of external magnetic fields. We show in Section 2.2 that the reciprocal relations are modified for systems in a
magnetic field B, Lik(B) = Lki(−B), a result due to Casimir[21]. 47The number of independent kinetic coefficients is reduced from n2 to 1
2n(n + 1). 48The reciprocal relations are sometimes referred to as the “fourth law of thermodynamics.”
49Kinetic coefficients connect forces and fluxes phenomenologically in Eq. (1.36); at the microscopic level, forces and
fluxes are linked through fluctuations. The connection with fluctuations is the link with nonequilibrium statistical mechanics. 50The state variables of thermodynamics are by definition their time-independent equilibrium values[2, p5]. Here we’re
allowing dynamic deviations from equilibrium. The n fluctuations in Eq. (1.39) are all measured at the same time. It’s
traditional to denote fluctuations as in Eq. (1.39) with αi. The notation δXi ≡ Xi − X0
i is also prevalent. 51First-order terms in the Taylor series of Eq. (1.40) vanish; see [2, p44]. Fluctuations in conserved extensive quantities
(energy, volume, particle number) occur with no change in entropy to first order in small quantities. 52Entropy is a concave function[5, p321]. Because of the minus sign in Eq. (1.40), the matrix [gjk] is positive definite. 53We return to this idea in Chapter 6 with the fluctuation-dissipation theorem. 54The quantity σS is the rate of entropy creation per volume, σS = (1/V )d∆S/dt.Irreversibility, entropy, and fluctuations ■ 13
Jk ≡ 1
V
dαk
dt (1.42)
Fk ≡ ∂∆S
∂αk
= −

j
gkjαj . (1.43)
We take Eqs. (1.42) and (1.43) as provisional definitions of Jk, Fk—there is wiggle room involving
multiplicative factors so that the product JkFk has the correct dimension.55 Equation (1.42) is the
Onsager regression hypothesis, that the rate of irreversible processes is set by the time rate of change
of fluctuations.
The entropy source σS is a locally intensive quantity, the rate of entropy creation per volume.
The term dαk/dt in Eq. (1.41) specifies an extensive quantity (fluctuations as defined here are
extensive).56 We must divide by V in Eq. (1.42) so that the product JkFk has the dimension of σS.
Consider the dimensions of the terms in Eq. (1.34). In the rate of entropy production associated
with viscous stresses, ∇v:Π, ∇v has the dimension of rate, (time)−1, and Π has the dimension of
pressure, or energy density.57 The same is true of chemical reactions: Ji is a rate and the chemical
affinity Ai is an energy density. The use of the term flux is therefore misleading (if it’s actually
a rate). Only for vector processes does the “flux” have the traditional dimension of flux. Through
dimensional analysis (square brackets denote dimension) we have, using Eqs. (1.42) and (1.43),
[JkFk] =  1
V
dXk
dt
∂S
∂Xk

= Rate × entropy density.
One might wonder how thermodynamic forces associated with gradients arise, as in Eq. (1.34).
Consider that if we “borrow” a length factor from V = L3,
[JkFk] =  1
L2
dXk
dt
× ∇  ∂S
∂Xk
 = Rate × entropy density,
except now the flux term is a flux in the traditional sense.
Consider an entropy current defined in terms of traditional fluxes,
JS ≡ 
k
FkJk, (1.44)
where Fk = ∂S/∂Xk is an intensive parameter (start with dS = 
k FkdXk), and Jk is a current
density associated with extensive parameter Xk. (One should check that JS has the dimensions of
entropy per area per time.) From Eq. (1.44),
∇ · JS = 
k
Jk · ∇Fk +
k
Fk∇ · Jk. (1.45)
Extensive variables (other than entropy) are conserved and satisfy continuity equations,
∂xk
∂t + ∇ · Jk = 0, (1.46)
where xk ≡ Xk/V is a per-volume density (as opposed to per mass). Combining Eqs. (1.46) and
(1.45),

k
Fk
∂xk
∂t + ∇ · JS = 
k
Jk · ∇Fk. (1.47)
Equation (1.47) is in the form of an entropy balance equation, implying the right side is an entropy
source expressed in terms of gradients of intensive thermodynamic parameters.
55To quote C. Kittel[24, p163]: “It is rarely a trivial problem to find the correct choice of forces and fluxes applicable to
the Onsager relation.” 56The other equation (1.43) for the thermodynamic force Fk specifies an intensive quantity. 57When divided by T in Eq. (1.34), Π/T has the dimension of entropy density.14 ■ Non-Equilibrium Statistical Mechanics
1.8 THERMOELECTRICITY, KELVIN RELATION
The simplest application of nonequilibrium thermodynamics is the thermoelectric effect, where
a temperature difference ∆T produces a heat current in addition to an electric current, which in
turn establishes a potential difference ∆V. Numerous other applications could be covered (see de
Groot and Mazur[7] or Kreuzer[25]). Our purpose is not to provide an exhaustive coverage of irre￾versible thermodynamics, but rather to establish a general framework for macroscopic descriptions
of nonequilibrium phenomena.
The first question to ask is: What are the forces and the fluxes? Ignoring viscous effects and
chemical reactions, we have from Eq. (1.34),
σS = JQ · ∇  1
T

+ JE ·
F
T − ∇
 µ
T
 ≡ JQ · XQ + JE · XE, (1.48)
where JE is a “provisional” electric current density. We need to be mindful of units here. The units
of JQ are W m−2. The units of what we’ve called the “electric” current, JE, are those of mass flux,
kg m−2 s−1. The units of XE in Eq. (1.48) are force per mass per Kelvin, N kg−1 K−1 (F is the
Coulomb force per mass). Thus, JE ·XE has units of entropy per volume per time, which we want.
To remove confusion over units, redefine JE and XE such that JE · XE = J
E · X
E, where we
multiply and divide by the unit of charge so that J
E has the usual units of A m−2 and
X
E = E
T − 1
e
∇
 µ
T

≡ 1
T E (1.49)
has units of V m−1 K−1, where E is the electric field, e is the magnitude of the electron charge, and
µ is the chemical potential in Joules. The quantity E in Eq. (1.49) is an effective electric field.
From Eq. (1.36), we write, grouping terms into vectors,
J
E =L11X
E + L12XQ = L11
T E − L12
T2 ∇T (1.50)
JQ =L21X
E + L22XQ = L21
T E − L22
T2 ∇T, (1.51)
where the kinetic coefficients are such that L12 = L21 (Onsager reciprocity).58 The quantity L11
has units of S K m−1, L12 and L21 have units of A K m−1, and L22 has units of W K m−1. How to
“dig out” the transport coefficients from these relations?
For ∇T = 0 in Eq. (1.50), we identify L11 = T σ, where σ > 0 is the electrical conductivity.
Also for ∇T = 0, we have the ratio of the magnitudes of the currents,
JQ
J

E
≡ Π = L21
L11
, (1.52)
where Π is the Peltier coefficient, which can be of either sign.59
For ∇T ̸= 0, no current flows in an open circuit. Setting J
E = 0 in Eq. (1.50), we have
E = (L12/ (T L11)) ∇T. Combining with Eq. (1.51),
JQ = − 1
T2

L22 − L21L12
L11 
∇T ≡ −κ∇T.
We therefore have another connection between kinetic coefficients and a transport coefficient,
κ = 1
T2

L22 − L21L12
L11 
.
58We see from Eqs. (1.50) and (1.51) that the same forces give rise to different fluxes, as noted in Section 1.6. 59Charge carriers can be of either sign.Irreversibility, entropy, and fluctuations ■ 15
We have three transport coefficients Π, σ, κ involving the three independent kinetic coefficients,
L11, L12, L22. The voltage difference ∆V is obtained by integrating E, with the result that
∆V =
 2
1
E · dr = L12
T L11  2
1
∇T · dr = L12
T L11
∆T,
where we’ve used Eq. (1.50) with J
E = 0. The linear relation between ∆V and ∆T is the Seebeck
effect, P ≡ ∆V /∆T. Thus,
P = 1
T
L12
L11
= 1
T
Π, (1.53)
where we’ve used Eq. (1.52). Equation (1.53) is the Kelvin relation,
60 which has been verified
experimentally[22]. Note how the phenomenological kinetic coefficients have been used as a “boot￾strap” to provide a testable prediction, independent of the Lij .
1.9 STEADY STATES AND MINIMUM ENTROPY PRODUCTION
Equilibrium is a time-invariant state of maximum entropy. In formulating statistical mechanics, we
recognize that measurements of equilibrium systems fluctuate from the random motions of micro￾scopic components. That leads to the use of ensembles classified by the types of interactions they
permit with the environment[5, Chapter 4]. The canonical ensemble is composed of systems al￾lowing energy exchanges with the environment; the grand canonical ensemble allows exchanges of
matter and energy. Equilibrium can be characterized as the state in which there is no net flow of
matter or energy between system and environment. A system without flows is a state of zero entropy
production, σS = 0; thus, zero entropy production is another way to characterize equilibrium (a
state of time-invariant entropy).
With nonequilibrium systems, we encounter a new type of time-invariant state, steady or sta￾tionary states, in which flows of heat, particles, or electricity occur at constant rates across system
boundaries. Under such conditions, the state of the system at any point is unchanging in time. The
system is time invariant, yet not in equilibrium because of the dissipative processes associated with
σS ̸= 0. Nonequilibrium thermodynamics is thus more comprehensive than traditional thermody￾namics; equilibrium is the steady state in which fluxes from the environment approach zero.
Steady states are characterized by an extremum principle, that (in steady state) the rate of en￾tropy production has the minimum value it can have consistent with prescribed boundary condi￾tions, a result established by Prigogine[26, p76] and de Groot and Mazur[7, Chapter 5]; see the
review of Jaynes[27]. Starting from Eq. (1.37), the general expression for entropy production,
σS = 
i JiFi = 
ik LikFiFk, and form the derivative with respect to a given force (holding
the others fixed),
∂σS
∂Fl

Fj
Lik constant
↓
= 
ik
Lik (Fiδk,l + Fkδi,l)
indices
↓
= 
i
(Lil + Lli) Fi
reciprocity
↓
= 2
i
LliFi
Eq. (1.36)
↓
= 2Jl.
Thus, Jl = 0 (a stationary state) is equivalent to (∂σS/∂Fl)Fj = 0, implying σS has a minimum
value61 with respect to variations in Fl (with the remaining forces held fixed). If another flux Jm is
zero, as well as Jl, the value of σS is at a smaller minimum. As the number of flows which are zero is
increased (for fixed forces), the minimum value of σS becomes progressively smaller. The limiting
60Kelvin derived Eq. (1.53) in 1854 using a now-discredited theory. He got the right answer using incorrect reasoning.
Boltzmann attempted without success to justify Kelvin’s approach. The correct derivation had to wait for Onsager. 61Because σS is a positive definite quadratic form, the extremum condition refers to a minimum, not a maximum.16 ■ Non-Equilibrium Statistical Mechanics
case of all flows zero corresponds to thermodynamic equilibrium where σS has its smallest possible
value, namely σS = 0. This theorem applies only if the kinetic coefficients are constant (implying
gradients in thermodynamic parameters are sufficiently small) and that Onsager reciprocity holds.
Consider a system featuring two flow processes, a flow of matter (which could be charged
species) Jm, and a flow of energy, Jth. From Eq. (1.36), let
Jm = L11Fm + L12Fth
Jth = L21Fm + L22Fth, (1.54)
where Fm and Fth are the thermodynamic forces associated with fluxes Jm and Jth. Using Eq. (1.37),
we find, invoking L12 = L21,
σS = L11F2
m + 2L12FmFth + L22F2
th, (1.55)
and thus
∂
∂Fm
σS = 2(L11Fm + L12Fth)=2Jm, (1.56)
where we’ve used Eq. (1.54) in the final equality. Minimum entropy production, ∂σS/∂Fm = 0, is
associated with steady state, Jm = 0, and vice versa. The state of no particle current is, for a fixed
temperature gradient, the state of minimum entropy production.
SUMMARY
Systems out of thermodynamic equilibrium evolve irreversibly toward equilibrium and entropy is
created in irreversible processes. We considered the macroscopic theory of irreversibility based on
thermodynamic and hydrodynamic principles, which, in spite of the many equations in this chapter,
is based on four fundamental equations: (1.1), (1.36), (1.37), and (1.38). The macroscopic theory
reveals the issues to be explained by microscopic approaches to nonequilibrium systems.
• Entropy is created but not destroyed in irreversible processes [the Clausius inequality, (1.1)],
implying the existence of a non-negative entropy source (rate of entropy creation per volume),
σS ≥ 0, with equality for reversible processes. Entropy production is minimized in steady￾state processes.
• The assumption of local equilibrium, which, together with macroscopic conservation laws,
leads to an expression for σS in the form σS = 
k JkFk, Eq. (1.37), where the terms Jk are
referred to as fluxes (but are often simply rates) and the terms Fk are known as thermodynamic
forces. Establishing the form of σS constitutes the heavy lifting in this chapter.
• Linear relations between forces and fluxes, Ji = 
j LijFj , Eq. (1.36), with the phenomeno￾logical terms Lij known as kinetic coefficients. Equation (1.36) recognizes that any force can
contribute to any flux. Said differently, the same forces can give rise to different fluxes.
• The Onsager reciprocal relations, the experimentally verified symmetry of kinetic coefficients
Lij = Lji, Eq. (1.38), follow from basic ideas of statistical mechanics (see Chapter 2) and
form a gateway into the study of time-dependent fluctuations. We identified fluxes as the
time rate of change of fluctuations, the Onsager regression hypothesis. That forces and fluxes
are linked through fluctuations, Eqs. (1.42), (1.43), implies the fundamental conclusion that
dissipation is associated with fluctuations, of which we’ll see in examples in Chapters 2, 3
and we’ll study systematically in Chapter 6.Irreversibility, entropy, and fluctuations ■ 17
EXERCISES
1.1 a. Show that the convective derivative, Eq. (1.12), satisfies the product rule of calculus (for
any differentiable functions ϕ, ψ, and for any velocity w)
Dw(ϕψ) = ϕDwψ + ψDwϕ.
b. Show for the special case of the center-of-mass velocity v, for any function ϕ, that
ρDvϕ = ∂
∂t(ρϕ) + ∇ · (ϕρv). (P1.1)
Use the continuity equation; ϕ may be a function or the component of a vector or tensor.
1.2 Derive Eq. (1.15).
1.3 Show that [∇ · (ρvv)]i = ρ(v · ∇)vi + vi∇ · ρv. See page 6 for the meaning of dyadic
notation and the divergence of a second-rank tensor.
1.4 Fill in the steps leading to the Navier-Stokes equation, (1.22).
1.5 Derive Eq. (1.31).
1.6 We defined a time rate of change of entropy by dividing the first law of thermodynamics by
dt, which we expressed in Eq. (1.5) as a relation among partial time derivatives. Show that
the same form holds as a relation among convective derivatives,
T Dvs = Dvu + P Dvρ−1 −n
k=1
µkDvck, (P1.2)
where all quantities are specific quantities. Thus, Eq. (1.4) remains valid for a mass element
followed with the center of mass motion. The specific entropy is a function of the variables
describing the macroscopic state of the system, s = s(u, ρ−1, ck). Show using standard ther￾modynamic identities (see [2, p42]) that
T∇s = ∇u + P∇ρ−1 −n
k=1
µk∇ck.
Note that for any function, df = ∇f · dr.
1.7 Using the balance equation for entropy ∂(ρs)/∂t + ∇ · JS = σS, show that we have a
generalization of the Clausius inequality for open systems, dS/dt ≥ −  JS · dS.
1.8 Referring to Section 1.8, derive expressions for the Onsager coefficients in terms of the trans￾port coefficients. Show that κ > 0. A: L11 = T σ, L12 = T σΠ, L22 = T2κ + T σΠ.
1.9 Isentropic flow is one for which entropy is conserved, σS = 0.
a. Use Eq. (1.34) to argue that isentropic flow implies no heat flow, no diffusion flows, no
chemical reactions, and neglect of viscous forces. There is only convection of entropy, as
per Eq. (1.33).
b. For isentropic flow show that
Dvρs = −ρs∇ · v.
Argue that isentropic flow (the entropy of a small volume of fluid doesn’t change as it
flows) is also incompressible flow.18 ■ Non-Equilibrium Statistical Mechanics
c. Enthalpy, H ≡ U + P V , is the heat added at constant pressure[2, p57]. Define a specific
enthalpy h ≡ H/M, so that h = u + P/ρ.
i. Show that the first law of thermodynamics can be written dh = Tds + (1/ρ)dP. For
isentropic flow, ∇h = ∇(P/ρ).
ii. For isentropic flows, show, assuming Fk = −∇Φk, that
Dvv = −∇

h + (1/ρ)

k
ρkΦk

. (P1.3)
d. Derive, or otherwise verify, the vector identity
v ×∇× v = 1
2
∇v2 − (v · ∇) v.
e. Show in the case of isentropic flow that
∂v
∂t = −∇

h + (1/ρ)

k
ρkΦk +
1
2
v2

+ v ×∇× v. (P1.4)
f. Finally, from Eq. (P1.4), show that
∂
∂t (∇ × v) = ∇ × [v × (∇ × v)] .
Thus, if ∇ × v = 0 at an instant of time, the velocity field remains curl-free for all times.CHAPTER 2
Fluctuations as stochastic
processes
T HE Onsager reciprocal relations are a consequence of the time-dependent properties of fluctu￾ations. Although the ensemble-based framework of statistical mechanics is predicated on the
existence of fluctuations, nowhere are their dynamical properties used in establishing that theory.
We know that spatial correlations exist among fluctuations (from elastic scattering experiments[5,
Section 6.6]). To prove the experimentally verified Onsager relations, and to secure a foothold
in nonequilibrium statistical mechanics, we must consider temporal correlations. Fluctuations are
modeled as stochastic processes—time sequences of random events—a topic not typically included
in science curricula. In this chapter, we develop stochastic processes as a mathematical tool (Sec￾tions 2.3, 2.4, and 2.5)—a necessary foundation for nonequilibrium statistical physics1—as well as
associated physical ideas. We begin with the simplest theory of fluctuations, that due to Einstein.
2.1 EINSTEIN FLUCTUATION THEORY
The transition from thermodynamics to statistical mechanics requires a generalization of our con￾cepts of equilibrium and measurement. In thermodynamics, state variables have fixed values that
persist unchanged in time. Microscopically, however, the local density and other quantities fluctuate
due to the erratic, random motions of a system’s myriad components.2 Let Xˆ denote a measurable
quantity. In thermodynamics, the equilibrium value X0 is obtained in a single measurement. In sta￾tistical mechanics, we interpret the equilibrium value as the time-invariant mean of a large number
of measurements3 of Xˆ. One can envision a series of measurements made at many times on the
same system, from which we calculate time averages,4 X. Or, one could envision measurements
made at the same time on a collection of macroscopically identical systems termed an ensemble, so
as to sample the effects of varied microscopic initial conditions, in which case we speak of ensemble
averages, ⟨X⟩. Statistical mechanics is concerned with ensemble averages.5
1Statistical mechanics requires probability theory: combinatorics, random variables, probability distributions, limit theo￾rems, characteristic functions, and cumulants. Nonequilibrium statistical mechanics requires in addition stochastic processes. 2Motion in classical mechanics is a deterministic unfolding of initial conditions, quantities that we have no ability to
control or measure in macroscopic systems. We have no alternative to treating fluctuations as random processes. 3The law of large numbers (a theorem in the theory of probability) guarantees the existence of the mean[5, p74]. 4The existence of time averages is guaranteed by Birkhoff ’s theorem,[28][5, p48] which pertains to time averages cal￾culated from the microscopic dynamics of the system. It’s one of two theorems on which the construction of statistical
mechanics relies (the other is Liouville’s theorem). 5Calculating time averages is impossible, which would require us to calculate the motion of large numbers of interacting
particles, a task beyond our ken (for practical as well as fundamental reasons). The ergodic hypothesis equates time averages
with ensemble averages. Statistical mechanics based on this assumption is highly successful, i.e., it works.
DOI: 10.1201/9781003512295-2 1920 ■ Non-Equilibrium Statistical Mechanics
A measurement of Xˆ on a randomly selected member of the ensemble will in general return a
value different from the mean. Can we deduce the probability P(X)dX that the result of measure￾ment lies in the range [X, X + dX]? That sounds impossibly general, yet we’re not dealing with an
arbitrary ensemble, but one composed of equilibrium systems. The Boltzmann entropy formula is a
bridge between microscopic and macroscopic,
S = k ln W, (2.1)
allowing us to calculate S given W, the number of ways a macrostate (specified by state variables)
can be realized from the microstates of a system (specified by initial conditions). Inverting Eq. (2.1),
W = exp(S/k), we have the number of ways by which a state of entropy S can be achieved. With
S0 the equilibrium entropy, by writing6 S = S0 + ∆S, W = exp(S0/k) exp(∆S/k). Dividing by
an appropriate constant, we have the probability7 of an entropy fluctuation ∆S,
P(∆S) = Ke∆S/k, (2.2)
where K is a normalization factor (∆S < 0 in fluctuations; see Section 1.7).
How are entropy fluctuations related to fluctuations in observable properties? From Eq. (1.40)
(reproduced here), fluctuations αi ≡ Xi − X0
i in a system’s extensive variables are associated with
entropy fluctuations ∆S at lowest order in small quantities through
∆S(α1,...,αn) ≈ −1
2
n
j,k=1
gjkαjαk, (1.40)
where gjk = − 
∂2S/∂Xj∂Xk
0 and where the number of variables n is left unspecified.
By combining Eq. (1.40) with Eq. (2.2), we have the joint probability density (see Appendix B),
the probability that the value of each fluctuation lies in the range [αi, αi + dαi],
P(α1,...,αn) = K exp 
− 1
2k
αT Gα
, (2.3)
where we’ve “packaged” fluctuations into a vector8 αT ≡ (α1,...,αn) with T transpose and the
n × n symmetric matrix G ≡ [gij ]. We require that the probability be normalized:9
 ∞
−∞
···  ∞
−∞
P(α1,...,αn)dα1 ··· dαn = 1. (2.4)
Combining Eq. (2.3) with Eq. (2.4), we find K = √
det G/ (2πk)
n/2 (see Exercise 2.1). We’ll refer
to Eq. (2.3) as the Einstein probability distribution. Mathematically it has the form of a multivariate
Gaussian or normal distribution (see Section 2.9).10 Keep in mind that the Einstein distribution is
approximate; it’s based on the second-order Taylor expansion of ∆S(α1,...,αn).
6We’re “pushing” thermodynamics to tell us something about nonequilibrium systems. Entropy is a property of the
equilibrium state; we’re extending it to small deviations from equilibrium, with |∆S|/S0 ≪ 1. 7The probability of an event is proportional to the number of ways it can occur; see Appendix B. Equation (2.2) was
first applied to the study of fluctuations by Einstein in 1910. See Pais[29, Chapter 4] for a review of Einstein’s contributions
to statistical physics beyond his 1905 investigations of Brownian motion. 8Representing fluctuations as a vector is for notational convenience and shouldn’t be taken literally. The argument of the
exponential in Eq. (2.3) must be dimensionless, which is made so by the metric G. 9Equation (2.3) assumes small fluctuations (so that truncating the Taylor expansion in Eq. (1.40) is accurate), yet we’re
allowing −∞ < αi < ∞ in Eq. (2.4). P(α1,...,αn) is presumed sharply peaked about P(0,..., 0) so that extending
the limits of integration introduces negligible error. It’s not strictly necessary to normalize probability distributions, although
highly convenient. At a minimum we require that probabilities be normalizable, whether or not we actually normalize. 10The misleading adjective “normal” was introduced by Karl Pearson, one of the founders of mathematical statistics,
who later regretted it[30].Fluctuations as stochastic processes ■ 21
Example. Consider a system in thermal contact with a heat reservoir such that U and V fluctuate.
The matrix G for this system is
G = −


∂2S
∂U2
∂2S
∂U∂V
∂2S
∂V ∂U
∂2S
∂V 2

 =


1
T2CV
− 1
CV T
 α
βT
− P
T

− 1
CV T
 α
βT
− P
T
 1
βT T V +
1
CV
 α
βT
− P
T
2

 , (2.5)
where CV is the constant-volume heat capacity, and α, βT are the thermal expansivity and isother￾mal compressibility[2, p19]. You are asked in Exercise 2.2 to evaluate the derivatives in Eq. (2.5).
Note that det G = 1/(βT T3V CV ). From thermodynamics, βT and CV are always positive, the
conditions of mechanical and thermal stability[2, p48].
2.1.1 Characteristic functions and moments
It turns out that Fourier transforms of probability distributions, known as characteristic functions,
often have a more direct relation to physical quantities of interest than the distributions themselves.11
The characteristic function of an n-variate probability density P(α1,...,αn) is defined
Φ(ω1,...,ωn) ≡
 ∞
−∞
···  ∞
−∞
exp(iωT α)P(α1,...,αn)dnα ≡ ⟨exp(iωT α)⟩, (2.6)
where ω ≡ (ω1,...,ωn)T is a vector of transform variables ωi (one for each αi), dnα ≡
dα1 ··· dαn, and the angular brackets denote an average. It’s often easier to find Φ(ω) than P(α),
in which case P(α) is obtained through inverse Fourier transformation. Approximations are also
easier to develop for Φ(ω) than for P(α) (see Section 2.9.1). Note that Φ(ω = 0) = 1 is the normal￾ization integral on P(α). General theorems on characteristic functions are covered in Cram´er[31,
Chapter 10] or in [5, p75].
One use of characteristic functions is to find the moments of probability distributions (see Ap￾pendix B).12 The moment ⟨αk1
1 ··· αkn
n ⟩ is obtained from derivatives of Φ(ω1,...,ωn),
⟨αk1
1 ··· αkn
n ⟩ ≡  ∞
−∞
···  ∞
−∞
αk1
1 ··· αkn
n P(α1,...,αn)dnα = (−i)k ∂kΦ (ω1,...,ωn)
∂k1 ω1 ··· ∂kn ωn




ω=0
,
(2.7)
where k ≡ k1 + ··· + kn. Thus Φ must be differentiable at ω = 0 for moments to exist.13 If one
knew all the moments, but not P(α) itself, one could in principle reconstruct Φ(ω),
14 from which
one could infer P(α). For k =1+1=2, Eq. (2.7) generates the covariance matrix
⟨αrαm⟩ = − ∂2
∂ωr∂ωm
Φ(ω1,...,ωn)




ω=0
. (2.8)
11The term characteristic function is typically unfamiliar to science students. Such students are, however, quite familiar
with the role of integral transforms in simplifying the analysis of physical problems; probability theory is no different. 12Characteristic functions are also called moment generating functions, and moments are also called correlation func￾tions. We presume familiarity with bracket notation ⟨⟩ as indicating averages. 13The Cauchy distribution P(α) = (1/π)1/(1 + α2) is an example of a normalized probability distribution for which
Φ(ω)=e−|ω| exists, but is not differentiable at ω = 0[5, p73]. The Cauchy distribution has no moments higher than the
zeroth. Consider that for the second moment,  ∞
−∞
x2
1+x2 dx =  ∞
−∞

1 − 1
1+x2

dx.
14One must check that certain relations among the moments hold; see [32].22 ■ Non-Equilibrium Statistical Mechanics
The characteristic function associated with the Einstein distribution is, combining Eq. (2.3) with
Eq. (2.6),
Φ(ω) = K
 ∞
−∞
···  ∞
−∞
exp 
− 1
2k
αT Gα + iωT α

dnα = exp 
−k
2
ωT G−1ω

, (2.9)
where G−1 is the matrix inverse of G; see Exercise 2.5. Combining Eqs. (2.9) and (2.7), we find
⟨αr⟩ = 0
⟨αrαm⟩ = k

G−1
rm . (2.10)
Positive fluctuations are as equally likely as negative, hence ⟨αr⟩ = 0 (more generally there can be
symmetry-breaking mechanisms so that ⟨αr⟩ ̸= 0). The second result is highly useful.
Example. From the matrix G in Eq. (2.5),
G−1 =


CV T2 + βT V T3
 α
βT
− P
T
2
βT V T2
 α
βT
− P
T

βT V T2
 α
βT
− P
T

βT V T

= 1
k
 ⟨(∆U)
2
⟩ ⟨∆U∆V ⟩
⟨∆V ∆U⟩ ⟨(∆V )
2
⟩

(2.11)
where we’ve used Eq. (2.10) in the second equality. Note for the ideal gas15 that α/βT = P/T.
The Einstein distribution follows from representing ∆S with the second term in its Taylor series
in fluctuations of extensive quantities. It can be shown that Eq. (2.10) is rigorously obtained even
when the truncation of ∆S implied by Eq. (1.40) is not made[33]. The higher-order moments cal￾culated from the Einstein distribution are incorrect, however.16 Fortunately the first two moments
are most frequently used in physical applications.
2.1.2 Force-fluctuation correlations
The thermodynamic force Fk conjugate to fluctuation αk is, from Eq. (1.43), Fk = ∂∆S/∂αk =
−
j gkjαj , or in vector form, F = −Gα, where F = (F1,...,Fn)T . Thus, α = −G−1F, or in
component form,
αi = −

k

G−1
ik Fk, (2.12)
implying
∂αi
∂Fl
= − 
G−1
il = −1
k
⟨αiαl⟩,
where we’ve used Eq. (2.10). For the correlation of fluctuations and forces,
⟨αjFl⟩ = −

m
glm⟨αjαm⟩ = −k

m
Glm 
G−1
mj = −kδlj , (2.13)
where we’ve used Eqs. (1.43) and (2.10). Even though each fluctuation αi has instantaneous projec￾tions along thermodynamic forces [see Eq. (2.12)], on average there is a nonzero correlation17 only
among conjugate force-fluctuation pairs, Eq. (2.13), and they’re anticorrelated (negative correla￾tion). Equation (2.13) is important in the Onsager theory of irreversible processes; it can be derived
directly, without the use of G; see Exercise 2.10.
15For the ideal gas, internal energy is independent of volume (Joule’s law[2, p21]); U and V are uncorrelated. 16To quote H.B. Callen[10, p280]: “The moments of fluctuating extensive parameters were first computed by Einstein in
1910. Einstein’s method was an approximate one, which happens to give precisely the correct results for the second moments
but which gives inexact results for the higher moments.” 17Quantities having zero covariance are said to be uncorrelated.Fluctuations as stochastic processes ■ 23
2.2 MICROSCOPIC REVERSIBILITY AND ONSAGER RECIPROCITY
Onsager reciprocity follows from the time-reversal properties of fluctuations18 and the stationarity
of equilibrium averages. Remarkably, kinetic coefficients—associated with irreversibility—have a
fundamental symmetry as a result of microscopic reversibility. Why microscopic theories feature
time reversibility yet macroscopically there is a unique direction of time is a perennial question.19
The governing theory of irreversibility is at root the second law of thermodynamics, which cannot
be reduced to microdynamics.20
Consider fluctuation αi at time t and fluctuation αj at time t + τ with τ > 0 and form their
product αi(t)αj (t + τ ). Define the time average of the product,
αi(t)αj (t + τ ) ≡ lim
T→∞
1
T
 T
0
αi(t)αj (t + τ )dt. (2.14)
By Birkhoff’s theorem[28] the limit exists when the time dependence of the integrand is generated
by Hamilton’s equations of motion;21 limT→∞(1/T)
 0
−T also exists. The limit is independent of
the origin of time: limT→∞(1/T)
 T +t0
t0 αi(t)αj (t + τ )dt is independent22 of t0. A proof of On￾sager reciprocity requires us to distinguish fluctuations that are even and odd in the velocities of
particles. Following Casimir[21], we refer to fluctuations even in velocities as α-variables and those
odd in velocities as β-variables. We discuss α-variables first.
Fluctuations even under time inversion (t → −t) have the property
αi(t)αj (t + τ ) = αi(t)αj (t − τ ). (2.15)
The correlation of αi(t) and αj (t + τ ) is the same as that of αi(t) and αj (t − τ ).
23 It’s immaterial
whether or not αj occurs before αi; only the relative time between events is significant. By shifting
the origin of time t → t + τ on the right side of Eq. (2.15),
αi(t)αj (t + τ ) = αi(t + τ )αj (t). (2.16)
Equation (2.16) is Onsager reciprocity in disguised form, a “time-influence” symmetry of fluctua￾tions, that if αi(t) is correlated with (influences) αj (t + τ ) then αj (t) influences αi(t + τ ) in the
same way. It’s a consequence of the time-reversal invariance of the microscopic dynamics and the
stationarity of equilibrium averages (no unique origin in time).
At this point subtract αi(t)αj (t) from both sides of Eq. (2.16) and divide by τ :
1
τ
αi(t) [αj (t + τ ) − αj (t)] = 1
τ
[αi(t + τ ) − αi(t)] αj (t).
18Under t → t′ = −t, p → p′ = −p, q → q′ = q, and H(p′
, q′
) = H(−p, q) = H(p, q); Hamilton’s equations
of motion are therefore time-reversal invariant. The same is true at the quantum level. For every solution of the Schrodinger ¨
equation ψ(t) (having a self-adjoint Hamiltonian), there is a solution ψ∗(−t)[5, p34]; the system is dynamically reversible. 19The discovery in 1964 of CP violations in weak decays (1980 Nobel Prize in Physics) implies that time reversal
symmetry is broken for sub-nuclear processes involving weak interactions. At the super macroscopic level, we have an
expanding universe, which provides a sense for the direction of time. 20The second law is the explicit recognition in the pantheon of physical laws of the existence of irreversibility. Unless
we seek to derive the second law from something more fundamental, we don’t have to account for irreversibility; it simply
is. We tend to think microscopic laws are more “real” than macroscopic; we’re accustomed to breaking things apart in the
attempt to say that the whole can explained by the sum of its parts, and sometimes it works that way, but not always. Unless
and until a more fundamental understanding emerges, laws have their domains of validity. The second law is not inherent in
microscopic laws of motion; it’s a separate principle that stands on its own. “More is different,” to quote P.W. Anderson[34].
That makes nonequilibrium statistical mechanics interesting but challenging. 21See [5, p48]. Birkhoff’s theorem also applies to fluctuations treated as random variables; see Section 2.5.
22If limT→∞(1/2T)
 T
−T |f(t)|
2dt exists, then so does limT→∞(1/2T)
 T +t0
−T +t0
|f(t)|
2dt; see Wiener[35, p38].
23Consider a movie of a kicker kicking a ball and a blocker jumping up to grab the ball. The blocker grabs the ball after
it’s been kicked. Now run the movie backwards. In the reversed sense of time, the ball leaves the blocker’s hands before it
arrives at the kicker’s foot.24 ■ Non-Equilibrium Statistical Mechanics
Take the limit τ → 0 (see Section 2.3.2 for the conditions under which that’s allowed),
αi(t) ˙αj (t) = α˙ i(t)αj (t). (2.17)
In Eq. (2.17), use Eq. (1.42) which equates fluxes with time derivatives of fluctuations (Onsager
regression hypothesis), and combine with Eq. (1.36), the linear force-flux relation:

m
LjmαiFm = 
n
LinαjFn.
Equate time averages with ensemble averages (ergodic hypothesis), αiFm = ⟨αiFm⟩ = −kδim
[see Eq. (2.13)] and we arrive at the reciprocal relations, Lji = Lij .
In systems subject to external magnetic fields, under time reversal24 B → −B. The Lorentz
force qv × B is therefore invariant under time reversal. The Onsager relations must be modified
to indicate that fluctuations on the left side of Eq. (2.16) are functions of B but those on the (time￾reversed) right side are functions of −B. Equation (1.38) is therefore written Lij (B) = Lji(−B).
Are there other velocity-dependent forces? In a reference frame rotating uniformly with angular
velocity Ω, particles experience the Coriolis force 2mv ×Ω. Under time reversal, Ω → −Ω. Thus,
fluctuations on the left side of Eq. (2.16) are functions of Ω and those on the right are functions of
−Ω. The Onsager relations are therefore Lij (B, Ω) = Lji(−B, −Ω).
Equation (2.15), and everything subsequent to it, holds for fluctuations even under time reversal.
It’s traditional to denote fluctuations odd under time reversal as βi. The analogs of Eq. (2.15) for
these variables are αi(t)βj (t + τ ) = −αi(t)βj (t − τ ) and βi(t)βj (t + τ ) = βi(t)βj (t − τ ). The
Onsager relations in their most general form can be written
Lij (B, Ω) = ϵiϵjLji(−B, −Ω),
with ϵi, ϵj the signatures of fluctuations under time reversal, ϵ = 1 if even and ϵ = −1 if odd.25
2.3 STOCHASTIC PROCESSES: ADDING TIME TO PROBABILITY
We’ve classified fluctuations according to their time-reversal properties, yet we have no way of
calculating them from first principles. We turn to stochastic processes, the branch of probability
theory that adds time to probabilistic descriptions.26 In what follows, we consider the case where a
single quantity can fluctuate,27 such as internal energy, where α ≡ U − ⟨U⟩ is a random variable.28
Definition. A stochastic process is a family of random variables {α(t):t ∈ T} where T is a set of
real numbers, usually interpreted as time. For every t ∈ T, a random number α(t) is observed. If T
is denumerable, it’s a discrete-time process; if T is not countable, it’s a continuous-time process.
Stochastic processes are families of random variables indexed by time, a definition too general to
provide a way of constructing them. Instead, they’re specified by a hierarchy of joint probability
densities obeying prescribed rules. A stochastic process is its hierarchy of joint probabilities.
24Under time reversal, currents that establish B are reversed, implying the reversal of B.
25Equation (2.16) should therefore be written αi(t)αj (t + τ) = ϵiϵjαi(t + τ)αj (t). 26There is no clear-cut reason to use the term stochastic over probabilistic. Stochastic is often followed by the word
process; stochastic processes emulate data that’s collected sequentially in time. To quote J.L. Doob[36], “A stochastic process
is the mathematical abstraction of an empirical process whose development is governed by probabilistic laws.” 27A stochastic process involves a single random variable sampled at various times. Such processes can be defined for all
other fluctuating quantities in a system, and correlations between different stochastic processes can be investigated. 28Random variables assign a number to every outcome of an experiment; see Appendix B.Fluctuations as stochastic processes ■ 25
2.3.1 The consistency conditions
We found in Section 2.1 the joint probability density for n random variables (α1,...,αn) represent￾ing fluctuations in n different physical quantities, all measured at the same time.29 Here we consider
fluctuations in the same quantity observed at n different times, (t1,...,tn). The joint probability
density for an n-time stochastic process is a function of 2n variables, the n times (t1,...,tn) and
the n random variables30 (α(t1),...,α(tn)), indicated notationally as a function of the times ti and
the values of random variables yi = α(ti),
Pn (y1, t1; y2, t2; ... ; yn, tn) = Prob (α(t1) = y1; α(t2) = y2; ... ; α(tn) = yn).
The subscript n on Pn indexes a collection of joint probabilities which, to comprise a stochastic
process, must possess certain properties known as the consistency conditions:
31
(a) Pn ≥ 0 (they are probabilities);
(b) The symmetry condition, that Pn is invariant under the interchange32 of pairs (yi, ti) and (yj , tj )
for i, j ∈ 1,...,n. There is no loss of generality therefore in ordering the times t1 < ··· < tn;
(c)  dynPn (y1, t1; y2, t2; ... ; yn, tn) = Pn−1 (y1, t1; y2, t2; ... ; yn−1, tn−1), the condition that
higher-order distributions imply the lower-order ones;
(d)  dy1P1(y1, t1)=1.
Note that integrations are over the y-variables only. The quantity Pn is defined for t1 ̸ t2 ̸
··· ̸= tn. If two times are identical, such as tn = tn−1, we have the stochastic process
Pn−1(y1, t1; ... ; yn−1, tn−1). No contradiction occurs if we define
Pn(y1, t1; ... ; yn−1, tn−1; yn, tn = tn−1) = δ(yn − yn−1)Pn−1(y1, t1; ... ; yn−1, tn−1). (2.18)
Functions {Pn} satisfying the consistency conditions completely specify stochastic processes.33
We require conditional probabilities. Let P1|1(y2, t2|y1, t1) denote the probability that α(t2) =
y2, given that α(t1) = y1. Conditional probabilities specify subensembles, that of the realizations of
α(t) having a value between [y1, y1 + dy1] at time t1, the fraction P1|1(y2, t2|y1, t1) has the value
between [y2, y2 + dy2] at t2. More generally,
Pl|k

yk+1, tk+1; ... ; yk+l,tk+l|y1, t1; ... ; yk, tk

≡ Pk+l

y1, t1; ... ; yk, tk; yk+1, tk+1; ... ; yk+l, tk+l

Pk

y1, t1; ... ; yk, tk
 , (2.19)
i.e., Pl|k is the probability that l samplings of α(t) have the values indicated, given that k samplings
are known to have produced the values indicated. Conditional probabilities satisfy the requirements:
(a) Pn|v ≥ 0;
(b)  dy1 ··· dynPn|v = 1;
29The “prescribed set” of fluctuations introduced in Section 1.7 were all observed at the same time, “instantaneous.” 30The values α(tj ) would be known if α(t) were an ordinary function of t; as it is, however, α(t) is a random variable. 31Compare with similar requirements on probability in Appendix B. We’re being cavalier in not always distinguishing
probabilities from probability densities (ditto with joint probabilities and conditional probabilities). It should be clear from
the context what is meant. We sum over discrete probabilities and integrate over probability densities. 32Joint probabilities are in the form of a series of “and” statements, the probability that α(t1) = y1, and α(t2) = y2,
and so on. The order in which one makes these statements is immaterial. 33A theorem due to Kolmogorov[37, p37] states that any functions satisfying the consistency conditions (and only the
consistency conditions) define a stochastic process. Specifying the joint probabilities Pn (that satisfy the consistency condi￾tions) is a way of specifying stochastic processes that bears a direct relation to the physics of the problem.26 ■ Non-Equilibrium Statistical Mechanics
(c)
Pn(y1, t1; ... ; yn, tn) =
··· 
dyn+1 ··· dyn+v

Pn|v(y1, t1; ... ; yn, tn|yn+1, tn+1; ... ; yn+v, tn+v)
× Pv(yn+1, tn+1; ... ; yn+v, tn+v)

.
2.3.2 Stochastic calculus
The Onsager regression hypothesis relies on the ability to differentiate fluctuations, and the recip￾rocal conditions rely on the ability to integrate fluctuations. Can calculus be applied to stochastic
processes? The definite integral of a stochastic process y(t) over a ≤ t ≤ b is defined in the usual
way as the limit of approximating sums:
 b
a
y(t)dt ≡ limn
k=1
y(t
′
k) (tk − tk−1), (2.20)
where the limit is over subdivisions of [a, b], a = t0 < t1 < ··· < tn = b, with tk−1 ≤ t
′
k ≤ tk,
as the maximum length of the sub-intervals (tk − tk−1) tends to zero. The integral  ∞
−∞ y(t)dt is
the limit of Eq. (2.20) as a → −∞ and b → ∞. Although Eq. (2.20) is the customary definition
of Riemann integral, it presents us with a question specific to this case: Does an infinite sum of
random variables possess a limit? A finite sum of random variables is itself a random variable. The
question becomes, can we speak of the convergence of a sequence of random variables?34 There
are several ways by which convergence of random variables is defined; see Parzen[38, Chapter 10].
We adopt the following: A sequence of random variables Z1,...,Zn is said to converge in mean
square to the random variable Z if35 limn→∞⟨|Z − Zn|
2
⟩ = 0. A necessary and sufficient condition
(Lo`eve[39, p472]) for the approximating sums in Eq. (2.20) to have a limit (in mean square) is that
the correlation function ⟨y(s)y(t)⟩ be integrable over (a ≤ s ≤ b, a ≤ t ≤ b). Assuming the
integrability of ⟨y(s)y(t)⟩, the linear operations of integration and calculating expectation values
commute,
  b
a
y(t)dt

=
 b
a
⟨y(t)⟩dt.
The derivative of a random process y(t) having a finite second moment is defined as the limit of
the difference quotient,
y′
(t) ≡ lim
h→0
1
h [y(t + h) − y(t)] ,
where the limit is taken as convergence in mean square. The limit exists (Lo`eve[39, p470]) if the
correlation function K(s, t) ≡ 
(y(s) − ⟨y(s)⟩) (y(t) − ⟨y(t)⟩)
 has continuous mixed second
partial derivatives ∂2K(s, t)/∂s∂t. A process y(t) is said to be differentiable in mean square if
y′
(t) exists as a limit in mean square. Under these conditions, the operations of differentiation and
expectation values commute,

y′
(t)

= d
dt
⟨y(t)⟩. (2.21)
Thus, the question of whether the rules of calculus can be applied to stochastic processes reduces
to the integrability and differentiability of the correlation function ⟨y(t)y(s)⟩ as a function of (t, s).
For future reference, a stochastic differential equation[40] (as applied to dynamical systems) is
one subject to stochastic driving forces. The solution of a stochastic differential equation is itself
34It helps not to get hung up on the word random. One might think that a truly random variable would occasionally have
large, ostensibly infinite, values. Random variables refer to unpredictable outcomes of experiments (see Appendix B), and
experiments on physical systems will not produce indefinitely large values of measured quantities. 35The convergence criterion implies that ⟨|Z − Zn|
2⟩ < ϵ for n>N(ϵ), for every positive number ϵ.Fluctuations as stochastic processes ■ 27
a stochastic process. If an input function I(t) is a stochastic process, then the solution X(t) of a
linear differential equation a0(t)X(n)(t)+a1(t)X(n−1)(t)+···+an(t)X(t) = I(t) is a stochastic
process, where the coefficients ak(t) are non-random functions of time.
2.4 STATIONARY, INDEPENDENT, AND MARKOV PROCESSES
The definitions given in Section 2.3 pertain to any stochastic process and comprise a level of gen￾erality at which little else can be accomplished. To make progress, ideas motivated by physical
conditions must be brought to bear. In this section, three classes of stochastic processes are defined.
An additional two will be introduced in later sections (Gaussian and differential).
2.4.1 Stationary processes: No unique origin of time
Definition. A stochastic process is stationary in the strict sense if, for all n and τ ,
Pn(y1, t1; y2, t2; ... ; yn, tn) = Pn(y1, t1 + τ ; y2, t2 + τ ; ... ; yn, tn + τ ). (2.22)
Stationary processes have no unique origin in time and are therefore a proxy for the dynamics of
fluctuations; their statistical properties remain the same if t1, t2,...,tn are each shifted by τ . Thus,
P1 is independent of time, P1(y1, t1) = P1(y1), and P2(y1, t1; y2, t2) is a function of the single
time t1 − t2 = τ , P2(y1, t + τ ; y2, t). The mean of a stationary process is therefore a constant
⟨y(t)⟩ =

dyyP1(y, t) = c = constant, (2.23)
and the autocorrelation function is a function only of the relative time between events
R(τ ) ≡ ⟨y(t + τ )y(t)⟩ =
  dy1dy2y1y2P2(y1, t + τ ; y2, t). (2.24)
Clearly R(0) > 0. From Exercise 2.12, R(τ ) = R(−τ ) and |R(τ )| ≤ R(0) for all τ . For every
system there is a time τc, the correlation time, such that R(τ ) τ≫τc −→ ⟨y(t)⟩⟨y(t + τ )⟩, i.e., y(t)
and y(t + τ ) become uncorrelated for τ ≫ τc. Do stationary processes have time derivatives? The
function R(τ ) has a continuous second derivative for any τ if it has a continuous second derivative at
τ = 0. It may happen that a process satisfies Eqs. (2.23) and (2.24) but Eq. (2.22) cannot be checked
for all n. Processes are termed stationary in the wide sense if, for all τ , Eq. (2.22) is satisfied for
n = 1, 2. Theorems that might be easy to show for wide-sense stationarity often become difficult
for strict-sense stationarity.36
For a process to be stationary, it must have lasted forever and it must last forever, −∞ <τ< ∞.
Physical processes can’t truly be stationary, therefore, which are of limited duration. Stationarity can
be invoked to a high degree of accuracy for systems that last longer than dynamical phenomena of
interest, such as in thermal equilibrium.37
2.4.2 Independent processes: Present is independent of the past
Suppose in a coin-flipping experiment one has obtained a long string of heads, HHH ··· H. What
are the odds of obtaining H in the next flip? It might seem that the chances of tails would go up—one
just “has” to obtain tails at some point. Unless the coins are correlated in an unusual way, the odds
of obtaining heads or tails on the next flip are the same; the outcome of the next flip is independent
of (has no memory of) the history of the process. Many problems in the theory of probability are
based on independent events. If only physics were so simple.
36Strict-sense stationarity implies wide-sense stationarity but not conversely.
37Equilibrium is the state where “all the ‘fast’ things have happened and all the ‘slow’ things not”[3, p1].28 ■ Non-Equilibrium Statistical Mechanics
Definition. An independent stochastic process is one for which either statement holds (for all k):
Pk

y1, t1; ... ; yk, tk

= 
k
l=1
P1

yl, tl
 (2.25)
P1|k−1

yk, tk|y1, t1; ... ; yk−1, tk−1

= P1

yk, tk

.
For independent processes, a knowledge of P1 is a complete solution of the problem. Because
P1|k−1 is replaced with P1, the outcome of the next trial is independent of the system’s past. Inde￾pendent processes are sometimes referred to as purely random processes.
2.4.3 Markov processes: Present depends on the immediate past
Flipping coins is a discrete-time process for which there is obviously a time interval between flips.
One could flip a coin and wait a week before flipping the next; independent events are uncorrelated
and are not affected by the time between them. Processes exist, however, where y(t1 + τ ) is not
independent of y(t1) for all τ ≥ 0, where y(t2) and y(t1) are correlated if |t2 −t1| is small enough.
Time correlations do not exist in independent processes. We need a richer framework; we must be
able to incorporate memory of past events.
The simplest way to do that is with a Markov process, in which y(tn) depends only on the most
recent past observation y(tn−1) and not on observations made at earlier times.38
Definition. In a Markov process, Pn|v>1 is replaced with Pn|1,
Pn|v(y1, t1; ... ; yn, tn|yn+1, tn+1; ... ; yn+v, tn+v)
Markov
approximation −→ Pn|1(y1, t1; ... ; yn, tn|yn+1, tn+1),
for t1 > ··· > tn > tn+1 > ··· > tn+v.
The history recorded in Pn|v>1 is therefore not fully retained. If the times (t1,...,tn) occur in
discrete multiples of a time unit, tn = n∆, a Markov process is referred to as a Markov chain.
Markov processes are determined by P2. One can show, by iterating the definition of conditional
probability, that for t1 > ··· > tn,
Pn(y1, t1; ... ; yn, tn) = P1|n−1(y1, t1|y2, t2; ... ; yn, tn)P1|n−2(y2, t2|y3, t3; ... ; yn, tn)···
P1|1(yn−1, tn−1|yn, tn)P1(yn, tn). (2.26)
Apply the Markov approximation to the conditional probabilities in Eq. (2.26) (compare with Eq.
(2.25)):
Pn(y1, t1; ... ; yn, tn)
Markov
↓
=
n
−1
i=1
P1|1(yi, ti|yi+1, ti+1)P1(yn, tn). (2.27)
All members of the hierarchy therefore follow from knowledge of P1|1 and P1. But, because
P1|1(yi, ti|yi+1, ti+1) = P2(yi, ti; yi+1, ti+1)
P1(yi+1, ti+1) = P2(yi, ti; yi+1, ti+1)
 dyiP2(yi, ti; yi+1, ti+1)
,
one could derive the hierarchy from knowledge of just P2.
One can’t choose P1|1 arbitrarily, however; it’s constrained by a consistency relation we now
derive. From Eq. (2.27), we have for n = 3, where t1 > t2 > t3,
P3(y1, t1; y2, t2; y3, t3) = P1|1(y1, t1|y2, t2)P1|1(y2, t2|y3, t3)P1(y3, t3). (2.28)
38The mathematician A.A. Markov studied (in 1907) the alternation between vowels and consonants in a piece of Russian
literature. He found that the occurrence of a vowel or a consonant depended strongly on whether the immediately preceding
letter was a vowel or a consonant, but only weakly on the character of earlier letters.Fluctuations as stochastic processes ■ 29
Integrating Eq. (2.28) over y2, we find
P1|1(y1, t1|y3, t3) = 
dy2P1|1(y1, t1|y2, t2)P1,1(y2, t2|y3, t3). (2.29)
Equation (2.29) is the Smoluchowski-Chapman-Kolmogorov equation (SCK),39 a basic equation of
Markov processes.40 It has an interpretation reminiscent of quantum physics: For t1 > t2 > t3, the
probability of α(t1) = y1 given that α(t3) = y3 can be found by considering the process as a sum
over compound processes consisting of the probability of α(t2) = y2 given α(t3) = y3 multiplied
by the probability of α(t1) = y1 given α(t2) = y2. Given this (appealing) interpretation, it’s
easy to forget that the SCK equation has been derived within the Markov approximation.41,42 The
conditional probability P1|1(y1, t1|y2, t2) is referred to as a transition probability: the probability
the system will be in state y1 at time t1 when it was in state y2 at time t2. Said differently, the system
makes a transition from y2 to y1 in the time interval (t1 − t2) with probability P1|1.
Independent processes are specified by P1 and Markov processes are specified by P2. One could
go on and consider processes specified by P3. Markov processes, however, find such prevalent
use that higher-order processes are simply referred to as non-Markovian. Wide-sense stationary
processes are also specified by P2. Processes that are stationary and Markov are of great interest in
modeling fluctuations.
2.5 ERGODIC AND SPECTRAL PROPERTIES
2.5.1 Ergodicity
In proving Onsager reciprocity we considered time correlation functions defined as time averages,
y(t)y(t + τ ) ≡ limT→∞(1/T)
 T
0 y(t)y(t + τ )dt. In completing the derivation, we equated time
averages with ensemble averages (see page 24); we invoked ergodicity.43 Fluctuations were treated
as dynamical variables in applying Birkhoff’s theorem;44 we did not treat fluctuations as stochas￾tic processes. Birkhoff’s theorem, however, applies to stationary stochastic processes.45 Stationary
processes must exhibit ergodicity to serve as models of fluctuations. For {y(ti), i = 1, 2,... } a
strictly stationary stochastic process, Birkhoff and others have shown that for a function g for which
⟨g(y(t))⟩ exists (and is independent of t; see Eq. (2.23)), the time average (1/T)
T
i=1 g(y(ti))
converges as T → ∞ and is equal to the ensemble average when certain supplementary conditions
are met.46 Ergodicity for strictly stationary processes is beyond the scope of this book.47 Processes
stationary in the wide sense, however, obey an ergodic theorem that’s easier to digest.
39Equation (2.29) was derived by Smoluchowski in 1906, Chapman in 1916, and Kolmogorov in 1931. It’s frequently
referred to as the Chapman-Kolmogorov equation, and infrequently as the Smoluchowski equation. 40One could start with the SCK equation as the definition of Markov process.
41The Markovian character of the SCK equation is exhibited by the fact that the probability of the transition y2 → y1 is
not affected by the previous transition y3 → y2. 42We note there are non-Markovian processes satisfying the SCK equation[41, p203]. 43Ergodic is an adjective, ergodicity is its noun form. The word ergodic was coined by Boltzmann as a combination of
the Greek words ϵργoν (ergon—“work”) and oδoς´ (odos—“path”). 44Birkhoff’s theorem has two parts. It’s primarily concerned with the existence of time averages; a corollary provides the
conditions under which time averages equal phase averages. 45What stationary processes and Hamiltonian dynamics have in common are measure-preserving transformations. The
natural motion in phase space (see [5, p33]) imposed by Hamilton’s equations of motion is a one-to-one measure-preserving
mapping of phase space onto itself at different times, basically the content of Liouville’s theorem. (The time evolution in
phase space is equivalent to a mapping of the canonical variables at time t into canonical variables at time t+dt—a canonical
transformation—and the Jacobian of canonical transformations is unity[5, p331].) Stationary processes are invariant under
time translations, another type of measure-preserving transformation. 46Such as “metric transitivity.” Although Birkhoff’s theorem provides necessary and sufficient conditions for the equality
of time and phase averages, namely metric transitivity, proving whether that condition holds is difficult. To quote M. Kac:
“As is well known, metric transitivity plays an important role in ergodic theory; however, it is almost impossible to decide
what Hamiltonians give rise to metrically transitive transformations”[42, p67]. 47See Doob[36, Chapters 10, 11] and Lo`eve[39, Chapters 30–32].30 ■ Non-Equilibrium Statistical Mechanics
Consider the time average of a process y(t) over the interval (−T,T),
yT ≡ 1
2T
 T
−T
y(t)dt. (2.30)
A sequence of time averages is ergodic48 if limT→∞ 
⟨(yT )2⟩−⟨yT ⟩2 = 0, if the variances of
time averages tend to zero as T → ∞. From Eq. (2.30),
⟨(yT )
2⟩ = 1
4T2
 T
−T
dt
′
 T
−T
dt⟨y(t)y(t
′
)⟩. (2.31)
For stationary processes, ⟨y(t)y(t
′
)⟩ ≡ R(|τ |) is a function only of τ ≡ t
′ − t (see Exercise 2.12).
Change variables: Let θ ≡ 1
2 (t
′ + t) and τ = t
′ − t, an area-preserving transformation. Thus,
⟨(yT )
2⟩ = 1
4T2
 2T
−2T
R(|τ |) (2T − |τ |) dτ = 1
T
 2T
0
R(τ )

1 − τ
2T

dτ. (2.32)
A wide-sense stationary process is ergodic if and only if (Slutsky’s theorem[43, Chapter 13]),
lim
T→∞
1
T
 2T
0

1 − τ
2T
 
R(τ ) − c2
dτ = 0, (2.33)
where ⟨yT ⟩ = c; see Eq. (2.23). Wide-sense stationary processes are ergodic if the autocorrelation
function R(τ ) decays sufficiently rapidly that Eq. (2.33) is satisfied.
2.5.2 Spectral analysis, Wiener-Khinchin theorem
Periodic functions f(x) of period p (f invariant under x → x+p) have Fourier series representations
f(x) = ∞
n=−∞ cne2inπx/p, where it’s known how to find the coefficients cn. By allowing p
to vary, and by letting p → ∞, square-integrable functions49 f(x) have integral representations
f(x) = (1/2π)
 ∞
−∞ F(α)eiαxdα, where F(α) =  ∞
−∞ f(x′
)e−iαx′
dx′ is the Fourier transform of
f(x). These considerations suggest that stationary stochastic processes have Fourier representations
(statistical properties invariant under t → t + τ for τ → ∞).
A problem with that idea is that stochastic processes do not have well-defined limits as t → ±∞.
A rigorous mathematical theory that avoids the problem, harmonic analysis, is beyond the level of
this book.50 To ensure the convergence of Fourier integrals, introduce a truncated stochastic process:
yT (t) ≡

y(t) |t| < T
0 |t| ≥ T,
where ultimately we’ll let T → ∞. The Fourier transform of yT (t) is defined
yT (ω) =  ∞
−∞
yT (t)eiωtdt =
 T
−T
y(t)eiωtdt,
with the inverse relation
yT (t) = 1
2π
 ∞
−∞
yT (ω)e−iωtdω. (2.34)
Equation (2.34) shows that yT (ω) is the spectral content of yT (t) at frequency ω.
48Ergodicity for wide-sense stationary processes is referred to as ergodic in the mean or mean-ergodic.
49That is,  ∞
−∞ |f(x)|
2dx < ∞, implying that f(x) → 0 as x → ±∞.
50See Wiener[35] and the article by S.O. Rice, Mathematical Analysis of Random Noise, in Wax[44, pp133–294].Fluctuations as stochastic processes ■ 31
Let’s look at the spectral content of the second moment. Start with the time average
y2 = lim
T→∞
1
2T
 T
−T
y2(t)dt = lim
T→∞
1
2T
 T
−T
y2
T (t)dt. (2.35)
By substituting two copies of Eq. (2.34),
 T
−T
y2
T (t)dt = 1
4π2
 ∞
−∞
dωyT (ω)
 ∞
−∞
dω′
yT (ω′
)
 T
−T
dte−it(ω+ω′
)
= 1
4π2
 ∞
−∞
 ∞
−∞
dωdω′
yT (ω)yT (ω′
) 2
ω + ω′ sin[(ω + ω′
)T].
Because we want the limit T → ∞ in Eq. (2.35), we can, by the product law of limits, use the
representation of the Dirac delta function[13, p105]
lim
T→∞
sin[(ω + ω′
)T]
ω + ω′ = πδ(ω + ω′
)
to infer that
y2 = 1
2π lim
T→∞
1
2T
 ∞
−∞
dω |yT (ω)|
2 = 1
2π
 ∞
−∞
S(ω)dω, (2.36)
where the spectral density or power spectrum is defined
S(ω) ≡ lim
T→∞
1
2T |yT (ω)|
2 . (2.37)
We see from Eq. (2.36) that S(ω)dω is the spectral contribution to y2 between ω and ω + dω. Note
that S(ω) = S(−ω) and S(ω) ≥ 0. If y(t) is a fluctuating current, S(ω) has SI units A2/Hz; if y(t)
is a fluctuating voltage, S(ω) has SI units V2/Hz.
Equation (2.36) can be generalized to the autocorrelation function of a stationary process:
R(τ ) = limT→∞
1
2T
 T
−T
y(t + τ )y(t)dt = lim
T→∞
1
2T
 T
−T
yT (t + τ )yT (t)dt. (2.38)
Substituting two copies of Eq. (2.34) and repeating the steps above, we find
R(τ ) = 1
2π
 ∞
−∞
S(ω)e−iωτdω = 1
2π
 ∞
−∞
S(ω) cos ωτdω = 1
π
 ∞
0
S(ω) cos ωτdω. (2.39)
Equation (2.39) is the Wiener-Khinchin theorem, that the autocorrelation function of a wide-sense
stationary process has a spectral decomposition given by the power spectrum of that process. The
inverse Fourier transform (valid when all questions of convergence make sense),
S(ω) =  ∞
−∞
R(τ )eiωτdτ =
 ∞
−∞
R(τ ) cos ωτdτ = 2  ∞
0
R(τ ) cos ωτdτ, (2.40)
is also called the Wiener-Khinchin theorem, in which case the spectral density of a wide-sense
stationary process is the Fourier transform of its autocorrelation function.
Suppose R(τ ) = cδ(τ ), implying S(ω) = c for all ω. A spectrum having equal intensity at all
frequencies (an infinite amount of energy) is termed white. A white spectrum can arise only if the
process y(t) is so random as to be uncorrelated with itself over any time, no matter how small. Such
an example is pathological because it implies that ⟨y2(t)⟩ is unbounded [see Eq. (2.36)], contradict￾ing the assumption of stationarity. In real situations, R(τ ) will not be so sharply peaked as a delta
function. At the other extreme, suppose R(τ ) = R(0) cos ω0τ , implying that the spectral density
is sharply peaked, S(ω) = πR(0) [δ(ω + ω0) + δ(ω − ω0)]. This example is also unphysical: For
ergodic stationary processes, R(τ ) must decay sufficiently rapidly that Eq. (2.33) is satisfied.32 ■ Non-Equilibrium Statistical Mechanics
2.6 THERMAL NOISE, NYQUIST THEOREM
Random thermal motions of charge carriers in electrical circuits generate thermal noise or Johnson
noise.
51 Consider the RLC circuit depicted in Fig. 2.1, where the system is in thermal equilibrium
V (t) L
R
C
Figure 2.1 Thermal agitations of charge carriers produce a fluctuating voltage V (t).
at temperature T. Thermal agitations of charge carriers produce fluctuating currents I(t), which,
through the usual circuit law, are associated with fluctuating voltages V (t),
V (t) = LdI
dt + RI +
1
C

I(t)dt. (2.41)
Fluctuations are stationary, implying we can bring to bear the tools of spectral analysis. Introduce
the Fourier transforms, V (t) = (1/2π)
 ∞
−∞ eiωtV(ω)dω and I(t) = (1/2π)
 ∞
−∞ eiωtI
(ω)dω.
Using Eq. (2.41), we find the relation between V(ω), I
(ω):
V(ω)=(R +i[ωL − 1/(ωC)]) I
(ω) ≡ Z(ω)I
(ω), (2.42)
where Z(ω) is the circuit impedance at frequency ω. The mean energy stored in the inductor asso￾ciated with the fluctuating current is, from the equipartition theorem,52
1
2
LI2

= 1
2
kT. (2.43)
Equation (2.43) must be consistent with the circuit equation (2.42). Let’s see what that implies.
From Eq. (2.36), we have the time average53
I2 = 1
2π lim
T→∞
1
2T
 ∞
−∞
dω



I

T (ω)



2
= 1
2π lim
T→∞
1
2T
 ∞
−∞
dω



VT (ω)



2
|Z(ω)|
2
= 1
2π
 ∞
−∞
dω
S(ω)
R2 + [ωL − 1/(ωC)]2 , (2.44)
where S(ω) ≡ limT→∞(1/2T)|VT (ω)|
2 is the spectral density [see Eq. (2.37)]. Invoking ergodic￾ity, equate ⟨I2⟩ from Eq. (2.43) with I2 from Eq. (2.44). We find
kT
L = 1
2πR2
 ∞
−∞
dω
S(ω)
1+(L/(ωR))2 [ω2 − ω2
0]
2 ,
51Shot noise, another kind of noise, originates from the discreteness of electrical charge, from fluctuations in the number
of charge carriers. Thermal noise arises from voltage fluctuations produced by randomness in charge motions. 52The equipartition theorem of classical statistical mechanics[5, p97] holds that energy equipartition (see [5, p91]) applies
not just to kinetic energy but to any quadratic degree of freedom, any degree of freedom that adds a quadratic term to the
Hamiltonian or energy function, e.g., p2/(2m) or 1
2mω2x2. To apply the theorem in this example, the energy of the charge
carriers must consist of the term 1
2 LI2 plus any other terms involving generalized coordinates and momenta of these particles
not involving I. The equipartition theorem breaks down at low temperatures where quantum effects manifest. 53T in Eq. (2.44) is the observation time, not the temperature.Fluctuations as stochastic processes ■ 33
where ω0 ≡ 1/
√
LC is the resonant frequency. Assume that L is so large54 that the integrand
is sharply peaked about ω = ω0, i.e., we have a tuned circuit. Under these conditions, we can
approximate S(ω) ≈ S(ω0) so that, with y = (2L/R)(ω − ω0),
kT
L ≈ S(ω0)
2πR2
 R
2L
  ∞
−∞
dy
1 + y2 = S(ω0)
4LR ,
presenting us with a formula for the spectral density:
S(ω)=4RkT. (2.45)
We’ve erased the subscript from ω0 in Eq. (2.45) because the resonant frequency can be freely
varied by varying circuit parameters. Equation (2.45) is Nyquist’s theorem55[45], published in 1928
concurrently with the discovery and measurement of thermal noise by Johnson[46]. It tells us the
noise power, the second moment of the fluctuating voltage ⟨V 2⟩ = 4RkT ∆f over the frequency
bandwidth ∆f that measurements are made.56 Nyquist’s theorem can be tested by using it to infer
the value of Boltzmann’s constant from noise measurements (found to differ by less than 1% from its
value obtained by other means[47]). The strength of thermal noise is proportional to the resistance
and the absolute temperature. Sensitive electronic equipment, such as radio telescopes, are cooled
to cryogenic temperatures to reduce thermal noise.
Our derivation of Eq. (2.45) differs from Nyquist’s, which involved an ingenious analysis of
a transmission line terminated at both ends with resistance R (schematically shown in Fig. 2.2),
with the entire system in equilibrium at temperature T. The transmission line is chosen so that its
V (t)
R R
V (t)
Figure 2.2 Idealized transmission line impedance matched to R, which act as black bodies.
characteristic impedance matches that of the resistor. As a consequence, electrodynamic modes on
the transmission line are absorbed by the terminating resistors without reflection. The resistors then
act as black bodies, objects that reflect nothing[2, p68] and emit energy modes distributed with the
Planck spectrum. The mean energy of a mode of frequency ω is ℏω/(eβℏω −1), where β = (kT)−1.
There’s more to the argument, but basically we can replace kT −→ ℏω/ 
eβℏω − 1

, and thus
S(ω)=4R ℏω
eβℏω − 1 (2.46)
is the Nyquist formula corrected for quantum effects. Thermal noise is therefore not white noise,
as one would infer from Eq. (2.45), which we now see applies for ω ≪ kT/ℏ (terahertz at room
temperature). The breakdown of equipartition averts the ultraviolet catastrophe.
54We want ω0(L/R) ≫ 1 (or L ≫ CR2) so that many oscillations of the circuit occur within the time L/R. 55Not to be confused with the Nyquist-Shannon sampling theorem for digital signal processing, also from 1928.
56For ∆f = N Hz, at room temperature kT∆f ≈ 4.1N × 10−21 W.34 ■ Non-Equilibrium Statistical Mechanics
2.7 THE RANDOM WALK IN ONE DIMENSION, DIFFUSION
We consider the random walk of a mythical drunkard who emerges from a drinking establishment
sufficiently inebriated so as to take steps with equal probability to the right or left on a level street (a
symmetric random walk). This problem is a stripped-down model of Brownian motion (see Chap￾ter 3), where a large particle (larger than molecular sizes), suspended in a fluid, undergoes erratic
motions from random collisions with fluid molecules. The random walk can be treated with more
traditional methods of probability theory (the binomial distribution, see Exercise 2.22); here we
analyze it as a Markov stochastic process.
Consider a one-dimensional lattice of points at positions x = ma, m = 0, ±1, ±2, ··· . At any
time the “walker” is at one of these sites. At times t = nτ , n = 0, 1, 2,... , the particle jumps to a
neighboring site to the right or left with equal probability. With the process started at x = 0 at t = 0,
we seek the conditional probabilities P1|1(m, n|0, 0). The form of the SCK equation appropriate to
a discrete state space57 is
P1|1(m, n|0, 0) = 
k
P1|1(m, n|k, n − 1)P1|1(k, n − 1|0, 0), (2.47)
where the sum on k is over the intermediate states accessible to the system between time steps n−1
and n, in this case k = m ± 1. To save writing, let P1|1(m, n|0, 0) ≡ p(m, n), in which case Eq.
(2.47) implies
p(m, n) = 1
2
p(m − 1, n − 1) + 1
2
p(m + 1, n − 1), (2.48)
where we’ve used the transition probability P1|1(m, n|k, n − 1) = 1
2 δk,m−1 + 1
2 δk,m+1. Note there
is no time derivative on the left of Eq. (2.48); we have a stochastic process, a succession in time
(labeled by n) of the probabilities of finding the particle at site m.
Equation (2.48) can be handled with Fourier methods. Define the Fourier transform of p(m, n),
where the transform variable is conjugate to the spatial index m,
p(q, n) ≡ ∞
m=−∞
p(m, n)e−iqm, (−π<q<π) (2.49)
where q is a dimensionless parameter restricted to (−π, π), and where we’ve extended the spatial
size of the system to ±∞. The inverse of Eq. (2.49) is
p(m, n) = 1
2π
 π
−π
eiqmp(q, n)dq. (2.50)
By applying Eq. (2.49) to Eq. (2.48), we find
p(q, n) = p(q, n − 1) cos q, (n = 1, 2,...)
an equation which is easily iterated:
p(q, n) = cosn(q)p(q, 0). (n = 0, 1,...) (2.51)
Combining Eq. (2.51) with Eq. (2.50), the solution of Eq. (2.48) has the integral representation
p(m, n) = 1
2π
 π
−π
eiqm (cos q)
n p(q, 0)dq.
57State space is the set of values of a random variable, in this case the probability of the walker at site m.Fluctuations as stochastic processes ■ 35
With p(m, 0) = δm,0, p(q, 0) = 1. The solution of Eq. (2.48) associated with the initial condi￾tion p(m, 0) = δm,0 has the representation
p(m, n) = 1
2π
 π
−π
eiqm (cos q)
n dq. (2.52)
For n = 1, we find that p(m, 1) = 1
2 (δm,−1 + δm,1), as it must for the given initial condition.
Equation (2.52) can be evaluated for any n, but developing a formula for general n isn’t illuminating.
We need to look at its large-n behavior.58
Probabilities are real. If eiqm is replaced with its real part, cos qm, Eq. (2.52) would be a Fourier
cosine transform, which applies to even functions. It behooves us to examine the parity properties
of the integrand. With a bit of analysis (see Exercise 2.28) one can show that p(m, n)=0 for n+m
an odd number, whereas for n + m an even number p(m, n) ̸= 0, with
p(m, n) = 
(1/π)
 π/2
−π/2(cos q)n cos(mq)dq (n + m = even)
0. (n + m = odd) (2.53)
That p(m, n)=0 for m + n an odd number implies that events with m + n odd are impossible to
achieve starting from the prescribed initial condition; see Exercise 2.29.
Note that p(m, n) is even in m, p(m, n) = p(−m, n) (symmetric random walk). The net dis￾placement is thus zero:
⟨m⟩ ≡ ∞
m=−∞
mp(m, n)=0.
The variance is more difficult. Reach for the trick that should be familiar from statistical mechanics:
Generate the sum you want by taking derivatives, m2 cos(qm) = −∂2 cos(qm)/∂q2,
⟨m2⟩ = ∞
m=−∞
m2p(m, n) = − 1
π
∞
m=−∞
 π/2
−π/2
dq (cos q)
n ∂2
∂q2 cos(mq). (2.54)
Now reach for the even older trick of integrating by parts, twice:
− 1
π
 π/2
−π/2
cosn q
∂2
∂q2 cos(mq)dq = n
π
 π/2
−π/2

n cosn q − (n − 1) cosn−2 q

cos(mq)dq
= n2p(m, n) − n(n − 1)p(m, n − 2). (2.55)
Combining Eqs. (2.55) and (2.54) (and using the normalization on p(m, n), see Exercise 2.27), we
have the characteristic result of a random walk,
⟨m2⟩ = n. (2.56)
The standard deviation of the process grows like √n.
2.7.1 Asymptotic form
Instead of attempting to evaluate Eq. (2.53) as n → ∞ for all m, let’s restrict ourselves to cases of
physical interest, |m| ≲ √n. Scale the “wavenumber” q in units of 1/
√n; let q ≡ y/√n. With that
substitution, Eq. (2.53) is equivalent to
p(m, n) = 1
π
√n
 π
√n/2
−π
√n/2
cosn
 y
√n

cos(my/√n)dy.
58The time τ represents an average time between collisions of the Brownian-particle with fluid molecules, a time short
relative to the times over which macroscopic observations are made.36 ■ Non-Equilibrium Statistical Mechanics
For large n, the major contribution to the integral occurs for |y| ≪ √n, for which
cosn
 y
√n

≈

1 − y2
2n
n n→∞ −→ exp(−y2/2),
where we’ve used the Euler definition of the exponential function. Thus, we have the asymptotic
form for n → ∞, m/√n = finite ( ∞
−∞ e−βx2
cos(bx)dx = π/β exp[−b2/(4β)])
p(m, n) ∼ 1
π
√n
 ∞
−∞
e−y2/2 cos(my/√n)dy =
 2
πn exp(−m2/(2n)). (2.57)
2.7.2 Passing to the continuous limit, Einstein diffusion equation
We can use Eq. (2.57) to pass from a description involving the probability distribution p(m, n) on
lattice points x = ma at discrete times t = nτ to a continuous description involving positions x
and times t. Under the substitutions m = x/a and n = t/τ , we seek the probability density p(x, t)
such that p(x, t)∆x is the probability the walker is in an interval ∆x around x at time t, where
∆x = a∆m, with 1 ≪ ∆m ≪ √n, so that
p(x, t)∆x ≈ 
m≤m1≤m+∆m
p(m1, t/τ ) ≈
2τ
πt
1
a
exp 
− x2τ
2a2t

a∆m, (2.58)
because p(m, n) varies slowly over the interval ∆m ≪ √n. Define the diffusion coefficient59
D ≡ a2/(2τ ), (2.59)
so that, from Eq. (2.58), p(x, t) ≈ (1/
√
πDt)e−x2/(4Dt)
. Let’s tentatively adopt that form of p(x, t)
to apply for all x and t, such that it’s properly normalized. As can be shown (see Exercise 2.31),
p(x, t) = 1
√
4πDt exp 
− x2
4Dt
(2.60)
is a Gaussian such that  ∞
−∞ p(x, t)dx = 1 for all times and moreover that p(x, 0) = δ(x).
Equation (2.60), which is strictly valid for t → ∞ and x/√
Dt = finite, is commonly taken as
a model to apply for all x and t for systems having the initial condition p(x, 0) = δ(x). As one can
show, p(x, t) in Eq. (2.60) is a solution of the diffusion equation,
60
∂
∂tp(x, t) = D ∂2
∂x2 p(x, t) (2.61)
(which one would find from the continuity equation together with Fick’s law). Thus p(x, t), derived
from the asymptotic form of the random-walk discrete distribution, Eq. (2.57), satisfies a partial
differential equation, a result we’ll see again in Chapter 3 (Fokker-Planck equation). We could have
developed Eq. (2.61) directly from the stochastic process. Subtract p(m, n − 1) from Eq. (2.48),
1
τ (p(m, n) − p(m, n − 1)) = a2
2τ
1
a2 [p(m − 1, n − 1) + p(m + 1, n − 1) − 2p(m, n − 1)] ,
in which we recognize the finite-difference approximations of the derivatives in Eq. (2.61). In this
way we see how naturally D = a2/(2τ ) arises as the diffusion coefficient. If this analysis were
repeated for a random walk on a d-dimensional integer lattice, one would find D = a2/(2dτ ).
59The dimensions of D are length2 per time. Committing that to memory will hold you in good stead. 60The diffusion equation is the prototype parabolic partial differential equation. That we’ve arrived at a time-irreversible
differential equation is a consequence of starting from a Markov process which assume one-way flows of time.Fluctuations as stochastic processes ■ 37
From Eq. (2.60) we find the second moment of the displacement, the Einstein diffusion equation,
⟨x2⟩t ≡
 ∞
−∞
x2p(x, t)dx = 2Dt, (2.62)
the continuum analog of Eq. (2.56), a result obtained in 1905.61 Equation (2.59) could be considered
microscopic in character—we’ve related the phenomenological transport coefficient D to model
parameters, yet we haven’t related a and τ to mechanical properties of the system. We’ll do that
in Chapter 3. Einstein’s famous result (another!), Eq. (2.62), is valid at long times but requires
modification at short times; see Section 3.2. Equation (2.62) is an instance of a more general result
derived in Chapter 6, the Green-Kubo theory of transport coefficients.
2.7.3 Wiener-Levy process
The Wiener-Levy process is a continuous-time Markov process for diffusion (the limit of the random
walk for continuous time and infinitesimally small step size) specified by the probabilities62
P1(y, t) = 1
√
4πDt exp 
−y2/(4Dt)
 (t ≥ 0)
(2.63)
P1|1(y2, t2|y1, t1) = 1
4πD(t2 − t1)
exp 
−(y2 − y1)
2/(4D(t2 − t1))
. (t2 > t1 > 0)
One can verify that ⟨y2⟩ = 2Dt using P1 and that the SCK equation is satisfied by the Wiener-Levy
form of P1|1(y2, t2|y1, t1). We return to the Wiener-Levy process in Section 3.2.
2.8 THE MASTER EQUATION
The master equation [see Eq. (2.66)] is a form of the SCK equation that finds wide use.63 Depending
on the nature of the state space, it’s either a set of differential-difference equations or an integro￾differential equation64 for time-dependent probabilities.
2.8.1 Derivation
Consider a small time interval between events t1 − t2 ≡ ∆t > 0. Write P1|1(y1, t1|y2, t2) =
P1|1(y1, t2 + ∆t|y2, t2) in the form
P1|1(y1, t2 + ∆t|y2, t2) = (1 − a0(y2)∆t) δ(y1 − y2)+∆tWt2 (y1|y2), (2.64)
where Wt2 (y1|y2) (a positive quantity) is a transition rate, the probability per unit time of a transi￾tion from state y2 at t2 to state y1 at t2+∆t and (1 − a0(y2)∆t) is the probability of no transition in
the time ∆t. In this form, P1,1 smoothly approaches a Dirac delta function as ∆t → 0+. Integrating
Eq. (2.64) over y1 (using the rules on page 25), we find a0(y2) =  Wt2 (y′
|y2)dy′
, the rate of all
transitions from state y2 at time t2 (y′ is a dummy variable). Combining Eq. (2.64) with the SCK
equation (see Exercise 2.35), we find in the limit ∆t → 0 (y ≡ y1, t ≡ t1)
∂
∂tP1|1(y, t|y3, t3) = 
dy2Wt(y|y2)P1|1(y2, t|y3, t3) − P1|1(y, t|y3, t3)

dy′
Wt(y′
|y). (2.65)
61Furth[ ¨ 48] is a valuable collection of Einstein’s articles on Brownian motion translated into English. 62Markov processes are specified by P1 and P1|1; see Section 2.4.3. 63The term master equation first appeared in 1940 in a statistical analysis of cosmic-ray showers[49]. The name arose
from the role of a general equation from which other results are derived. 64In an integro-differential equation, the unknown function appears in integrals and derivatives of the function[50].38 ■ Non-Equilibrium Statistical Mechanics
Equation (2.65) is the differential form of the SCK equation. It can be simplified by multiplying by
P1(y3, t3) and integrating over y3. We find (dropping the subscript on Wt for simplicity65)
∂
∂tP1(y, t) =  
W(y|y′
)P1(y′
, t)    gain
− W(y′
|y)P1(y, t)    loss

dy′
. (2.66)
The rate at which P1(y, t) changes is a balance between rates of gain processes, in which transitions
occur into the state y from all other states y′
, and loss processes in which transitions to all other states
y′ occur from y. The master equation finds wide use as it’s often a simple matter to come up with
approximate expressions for the transition rates W(y|y′
). For a discrete state space labeled by index
n, the master equation has the form
∂
∂tP(n, t) = 
n′̸=n
[W(n|n′
)P(n′
, t) − W(n′
|n)P(n, t)] . (2.67)
The sum is restricted to n′ ̸ n; the contribution of n′ = n formally drops out of the summation.
Example. The transition rate for the symmetric random walk is (α is a rate)
W(n|n′
) = α (δn′,n+1 + δn′,n−1), (2.68)
and thus
∂
∂tP(n, t) = α [P(n + 1, t) + P(n − 1, t) − 2P(n, t)] . (2.69)
The right side of Eq. (2.69) is the finite-difference approximation to the diffusion equation, (2.61).
Example. A sample of radioactive material has n0 active, identical nuclei at time t = 0. Let n′
(t)
denote the number of active nuclei at time t > 0, those that have not decayed. Assume a Markov
process, that the probability distribution at time t depends on the state of the system at time t
′ < t,
but not on its entire past history. Let α denote the probability per unit time that a surviving nucleus
decays, with the transition probabilities for n′ ̸ n given by
P1|1(n|n′
) =



0 n>n′
n′
α∆t n = n′ − 1
O(∆t)2. n<n′ − 1
The first requirement is that no new radioactive nuclei are created in the process (for example,
n = 5 and n′ = 3), and by the second requirement ∆t must be sufficiently small that the number
of radioactive nuclei changes by one (effectively enforced by the third requirement). The transition
rates are thus summarized by the formula W(n|n′
) = αn′
δn,n′−1, implying from Eq. (2.67) that
∂
∂tP(n, t) = α [(n + 1)P(n + 1, t) − nP(n, t)] (n = 0, 1, 2,...) (2.70)
subject to the initial condition P(n, 0) = δn,n0 . The solution to this system of equations is developed
in Exercises 2.38 and 2.39.
65Time-independent transition rates are referred to as temporally homogeneous.Fluctuations as stochastic processes ■ 39
2.8.2 Detailed balance
From Eq. (2.67), the master equation has a stationary solution Peq(n) when the following holds66

n′
W(n|n′
)Peq(n′
) = 
n′
W(n′
|n)

Peq(n). (2.71)
Equation (2.71) is a relation among the transition rates W(n|n′
); the coefficients Peq(n) are known
from statistical mechanics. Equation (2.71) indicates that in equilibrium the sum of all transitions
(per unit time) n′ → n is balanced by the sum of transitions n → n′ to all other states. A stronger
(yet simpler) condition is the requirement that detailed balance holds for each pair (n, n′
),
W(n|n′
)Peq(n′
) = W(n′
|n)Peq(n), (2.72)
that in equilibrium the rate of the transition n′ → n is the same as that for the reverse transition
n → n′
, a dynamic characterization of equilibrium. If Eq. (2.72) holds, so does Eq. (2.71).
Detailed balance does not have to be imposed as a separate requirement on transition rates, it’s
built in from the time-reversal symmetry of microscopic motions (just as with the Onsager recip￾rocal relations). A key point is that the states n, n′ in Eq. (2.72) represent macroscopic quantities,
functions of the microscopic coordinates of the system point in Γ-space (see Appendix A).67 Let
Γn (Γn′ ) denote the region of Γ-space associated with state n (n′
). A part (subset) of Γn, denoted
Γn→n′ , flows in time t to a subset of Γn′ , effecting the transition n → n′
. The transition probability
is the ratio of two phase-space volumes,
P1|1(n′
|n) = Vol(Γn→n′ )
Vol(Γn) = Vol(Γn→n′ )
Peq(n)
P1|1(n|n′
) = Vol(Γn′→n)
Vol(Γn′ ) = Vol(Γn′→n)
Peq(n′
) , (2.73)
where Vol denotes the 6N-dimensional volume of a region in Γ-space, and where we’ve equated
Peq(n) = Vol(Γn), a step that fixes a measure on phase space but does not affect the conclusion
we reach. The natural motion in phase space generated by Hamiltonian dynamics is a one-to-one,
measure-preserving mapping of Γ-space onto itself at different instants of time. A phase point at
time t1 uniquely determines its position in Γ-space at time t2, regardless of whether t2 > t1
or t2 < t1. Because of these properties, Vol(Γn→n′ ) = Vol(Γn′→n); the transition n′ → n is
the time-reversal of the transition n → n′
. From the two relations in Eq. (2.73), we infer that
P1|1(n′
|n)Peq(n) = P1|1(n|n′
)Peq(n′
), and then, with P1|1(n|n′
) = W(n|n′
)∆t, we have detailed
balance, Eq. (2.72). This argument is due to Wigner[52].68 A fuller derivation is given by de Groot
and Mazur.[7, pp92–100] We’ve relied on classical mechanics; the quantum version is discussed in
van Kampen[51, pp452–458].
2.8.3 Matrix form
The master equation can be written in matrix form by introducing the transition matrix W having
elements,69
Wn,n′ ≡ W(n|n′
) − δn,n′

n′′
W(n′′|n). (2.74)
66We assume only one stationary solution, the equilibrium probability distribution. Systems with multiple stationary
solutions exist, a topic beyond the intended level of this book. See van Kampen[51, p101]. 67Many micro-states are associated with macroscopically specified states, the basic message of Boltzmann entropy.
68E.P. Wigner received the 1963 Nobel Prize in Physics for the use of symmetry principles in fundamental physics.
69The transition rates W(n|n′
) are positive, as are the off-diagonal elements of the transition matrix Wn,n′ , n ̸= n′
,
but the diagonal elements of the transition matrix Wn,n are negative. Note that Wn,n′ need not be symmetric.40 ■ Non-Equilibrium Statistical Mechanics
In that way, the master equation (2.67) is equivalent to
∂
∂tP(n, t) = 
n′
Wn,n′P(n′
, t). (2.75)
Example. Using the transition rates associated with the random walk, Eq. (2.68), we have from Eq.
(2.74),
Wn,n′ = α (δn′,n−1 + δn′,n+1 − 2δn,n′ ).
For the decay process with W(n|n′
) = αn′
δn,n′−1, Eq. (2.70), we have
Wn,n′ = α [(n + 1)δn′,n+1 − nδn′,n] .
2.8.4 Eigenfunction expansion
By considering P(n, t) (n = 1, 2,...) as the elements of a column vector, P(t), the master equation
in (2.75) can be written ∂P/∂t = W P(t), which has the formal solution,
P(t)=etW P(0). (2.76)
The matrix exponential function is defined by the infinite series,70 exp(tW) ≡ ∞
n=0(t
n/n!)Wn,
where Wn is the nth power of W. From Eq. (2.74), Wn,n′ ≥ 0 for n ̸= n′
, which is a necessary and
sufficient condition for eW t to be non-negative[53, p172]. Thus, P(t) in Eq. (2.76) is positive and
bounded71 for all t ≥ 0.
Equation (2.75) is analogous to the Schrodinger equation ¨ ∂ψ/∂t = −(i/ℏ)Hψ, where H is
a Hermitian operator. For H ̸= H(t), ψ(t) = exp (−(it/ℏ)H) ψ(0). To handle this expression
one considers the associated eigenproblem, Hϕn = λnϕn (n = 0, 1,...). The eigenfunctions of a
Hermitian operator form a complete orthonormal set of functions (see [13, p66]). Thus, one can rep￾resent the initial condition as an infinite linear combination of eigenfunctions, ψ(0) = ∞
n=0 cnϕn,
where the expansion coefficients are found from the inner product cn = ⟨ϕn|ψ(0)⟩. The solution
has the form ψ(t) = ∞
n=0 cne−(it/ℏ)λn ϕn (see Exercise 2.41).
We can’t directly apply this familiar reasoning to the master equation because the transition
matrix W need not be symmetric. When combined with detailed balance, however, there is a way
forward. Note that the sum of the elements in any column of W vanishes (use Eq. (2.74)),

n
Wn,n′ = 0. (for each n′
) (2.77)
Equation (2.77) expresses conversation of probability; by combining Eq. (2.77) with Eq. (2.75),
the total probability 
n P(n, t) is fixed in time.72 Equation (2.77) informs us that W has a left
eigenvector ψ ≡ (1, 1,..., 1) with zero eigenvalue, ψ · W = 0, implying that W has a right
eigenvector with zero eigenvalue (in principle there could be more than one; see Exercise 2.42). The
detailed balance condition, Eq. (2.72), expressed in terms of transition matrices is
Wn,n′Peq(n′
) = Wn′,nPeq(n). (2.78)
70Equation (2.76) is a formal solution because, even though etW formally solves Eq. (2.75), one can’t sum the infinite
series (that etW represents) to obtain a useful closed-form expression (other than denoting it the exponential). In Eq. (2.76),
W is independent of time. There are specialized methods (for obtaining formal solutions) when W = W(t). 71The infinite series for eW t exists for all matrices W for fixed values of t, and for all t for any fixed W[53, p166]. 72The hallmark of conserved quantities is that associated with a change in time is a flow in space (see Section 1.3). We’ll
see in Section 3.3 that probability flows in the state space of random variables.Fluctuations as stochastic processes ■ 41
From Eq. (2.75), and using Eqs. (2.78), (2.77),
∂
∂tPeq(n) = 
n′
Wn,n′Peq(n′
) = 
n′
Wn′,nPeq(n)=0.
Thus, W has a right eigenvector of zero eigenvalue, the equilibrium probability distribution.
The transition matrix W is similar to a symmetric matrix, V . Define the matrix elements
Vn,n′ ≡ 1
Peq(n)
Wn,n′

Peq(n′
) (2.79)
for which Vn,n′ = Vn′,n by the detailed balance condition, Eq. (2.78). Equation (2.79) is a simi￾larity transformation,[13, p27] V = S−1W S, with Sij = Peq(i)δij . The eigenvalues of W are
therefore real and, as it turns out, nonpositive (see Exercise 2.45), an expression of the irreversibility
of the evolution of the nonequilibrium probability distribution into its equilibrium form.73
We can express the eigenproblem for W in the form

n′
Wn,n′ϕm(n′
) = −λmϕm(n), (2.80)
where the λm are positive with ϕm the associated eigenfunction. Combining Eqs. (2.79), (2.80),

n′
1
Peq(n)
Wn,n′

Peq(n′
)
  
Vn,n′
1
Peq(n′
)
ϕm(n′
)
  
ϕm(n′
)
= −λm
1
Peq(n)
ϕm(n)
  
ϕm(n)
. (2.81)
Equation (2.80) is thus equivalent to V ϕm = −λmϕm, where the eigenfunctions form a complete
orthonormal set (V is real and symmetric). We therefore have an orthonormality relation for the
eigenfunctions of W with respect to a weighting function,
⟨ϕm|ϕm′ ⟩ ≡ 
n
1
Peq(n)
(ϕm(n))T ϕm′ (n) = δm,m′ . (2.82)
With Eq. (2.82) established, the initial condition in Eq. (2.76) can be represented as an eigen￾function expansion, P(0) = 
m cmϕm = 
m⟨ϕm|P(0)⟩ϕm. Note that c0 = 1 (where ϕ0 = Peq).
We therefore have the solution of the master equation,
P(n, t) = Peq(n) + 
m>0
cmϕm(n)e−λmt
. (2.83)
We see that the nonequilibrium probability distribution approaches the equilibrium distribution as
t → ∞. If we order the eigenvalues λ1 < λ2 < ··· , λ1 controls the approach to equilibrium.
2.9 GAUSSIAN PROCESSES
A Gaussian processis a type of stochastic process occurring with sufficient regularity in applications
as to warrant separate treatment. A Gaussian process is an extension of the properties of Gaussian
distributions to stochastic processes.74 We found in Section 2.1 a multivariate Gaussian distribution
and its characteristic function, Eqs. (2.3) and (2.9). Here we develop properties of Gaussians not
covered in Section 2.1 and then we introduce Gaussian processes.
73The master equation presumes a one-way flow of time between transitions, ∆t > 0. 74Gaussian distributions occur generically in statistical physics. As an example, the probability PN (m) that, in a one￾dimensional random walk of N steps, the walker is found m steps to the right of the starting point is given by the binomial
distribution; see Exercise 2.22. The limit N → ∞ of the binomial distribution is precisely the Gaussian, the DeMoivre-42 ■ Non-Equilibrium Statistical Mechanics
2.9.1 Gaussian distributions and cumulants
A Gaussian function is the exponential of a concave quadratic function, P(x) ≡ K exp(−ax2+bx),
a > 0 and −∞ <x< ∞. Using  ∞
−∞ e−ax2+bxdx = π/aeb2/(4a) (a > 0), choose K = a/πe−b2/(4a) so that  ∞
−∞ P(x)dx = 1. The constants a, b are conventionally parameterized in
terms of the mean µ ≡ b/(2a) and the variance σ2 ≡ 1/(2a), implying
P(x) = 1
√
2πσ2 exp 
−(x − µ)2
2σ2

. (2.84)
One can show that ⟨x⟩ ≡  ∞
−∞ xP(x)dx = µ is the mean and ⟨x2⟩ ≡  ∞
−∞ x2P(x)dx = σ2 + µ2,
and thus σ2 = ⟨x2⟩−⟨x⟩2 is the variance. Gaussians have the property that their Fourier transforms
are Gaussian.75 As one can show,
Φ(ω) ≡
 ∞
−∞
P(x)eiωxdx = 1
√
2πσ2
 ∞
−∞
eiωxe−(x−µ)2/(2σ2)
dx = exp 
iωµ − ω2σ2/2

.
(2.85)
As discussed in Section 2.1, one role of the characteristic function Φ is to provide a gener￾ating function for the moments of the distribution; moments are found from derivatives of Φ.
For a single-variable probability density P(α), Φ(ω) = ⟨eiωα⟩ = ∞
n=0(iω)n⟨αn⟩/n!, and thus
⟨αn⟩ = (−i)n∂nΦ(ω)/∂ωn|ω=0. Another role of Φ (not treated in Section 2.1.1) is to provide the
cumulant generating function C(ω), defined as
C(ω) ≡ ln Φ(ω) = ln ∞
n=0
(iω)n
n! ⟨αn⟩

≡ ∞
n=1
Cn
n!
(iω)
n, (2.86)
where the expansion coefficient Cn, the nth-order cumulant, is found by taking derivatives,
Cn = (−i)n ∂n
∂ωn C(ω)




ω=0
= (−i)n ∂n
∂ωn ln Φ(ω)




ω=0
. (2.87)
Note that C0 = 0; Φ(0) = 1 for a normalized probability distribution. The cumulant expansion
therefore starts at n = 1 in Eq. (2.86). Cumulants are functions of moments. Using the power series
ln(1 + z) = z − 1
2 z2 + 1
3 z3 −··· , one can show76 for the first four cumulants,
C1 = ⟨α⟩
C2 = ⟨α2⟩−⟨α⟩
2
C3 = ⟨α3⟩ − 3⟨α2⟩⟨α⟩ + 2⟨α⟩
3
C4 = ⟨α4⟩ − 4⟨α3⟩⟨α⟩ − 3⟨α2⟩
2 + 12⟨α2⟩⟨α⟩
2 − 6⟨α⟩
4. (2.88)
Laplace theorem; see Cram´er[31, pp198–203], Sinai[54, p30], or [5, p72]. Gaussians also occur in the central limit theo￾rem, a general theorem of probability theory; see [5, p75] or Papoulis[43, p218]. Consider n ≫ 1 independent random
variables ∆x1,..., ∆xn representing the step lengths in a random walk having zero expectation values and variances
σ2
i = ⟨(∆xi)2⟩. The step lengths are uncorrelated (statistically independent), ⟨∆xi∆xj ⟩ = ⟨∆xi⟩⟨∆xj ⟩ = 0. Let
xn ≡ n
i=1 ∆xi be the sum of the step lengths, and let s2
n ≡ n
i=1 σ2
i be the sum of the individual variances. Define
a new random variable yn ≡ xn/sn. The central limit theorem states that, as n → ∞, the probability distribution for yn
approaches a Gaussian with unity variance, Pn(yn) n→∞ −→ P(y) = (1/
√2π) exp(−y2/2). 75It’s sometimes said, erroneously, that Gaussians are the only function having the property of being its own Fourier
transform. A Gaussian multiplied by a Hermite polynomial, for example, is its own Fourier transform, up to a multiplicative
constant. Considering the Fourier transform as a linear transformation in a Hilbert space of square integrable functions, one
is seeking the eigenfunctions of the Fourier transform operator, the number of which is unlimited. 76Put aside concerns over the existence of moments or the convergence of series. You’re formally matching powers of
(iω) between the two expansions in Eq. (2.86), a procedure that relies on the uniqueness of power series. See [13, p124].Fluctuations as stochastic processes ■ 43
Explicit expressions for the first 10 cumulants are listed in [55]. The nth cumulant77 is a function of
moments of order k ≤ n (conversely, the nth moment is a function of cumulants of order k ≤ n; see
Exercise 2.48). The first two cumulants C1, C2 are the mean and the variance. Cumulants beyond
second order are related to higher-order fluctuation moments. As one can show,
C3 = ⟨(α − ⟨α⟩)
3
⟩
C4 = ⟨(α − ⟨α⟩)
4
⟩ − 3⟨(α − ⟨α⟩)
2
⟩. (2.89)
Because fluctuations are typically small in macroscopic systems (exception: critical phenomena),
an approximation scheme is to set cumulants beyond a certain order to zero.78
The connection Φ(ω)=eC(ω) between the cumulant function C(ω) and the characteristic func￾tion Φ(ω) generalizes the relation Z(β)=e−βF in statistical mechanics between the free energy
and the partition function; see Eq. (A.15).79 A key property of cumulants is that they can be found
for any probability distribution: Whether equilibrium or nonequilibrium, the meaning of the av￾erage symbols ⟨⟩ in Eq. (2.88) has not been specified. In the statistical mechanics of interacting
particles, one develops a cumulant expansion for the free energy using a probability function based
on noninteracting particles[5, Section 6.3]. Cumulants also play a role in theories of real-space
renormalization[5, p280].
Suppose that P(α) = δ(α − α0), i.e., the event α = α0 occurs with certainty and with no vari￾ance. The associated cumulants all vanish except for C1. From Φ(ω) =  ∞
−∞ eiωαδ(α − α0)dα =
eiωα0 , C(ω) = ln Φ(ω)=iωα0, implying from Eq. (2.86) that C1 = α0 and Cn>1 = 0. The mo￾ments in this case factorize trivially, ⟨αn⟩ = ⟨α⟩n, and thus, from Eq. (2.88), C2 = C3 = C4 = 0.
We’ve made this foray into cumulants to show that cumulants associated with Gaussian distribu￾tions vanish beyond second order, Cn>2 = 0. From Eq. (2.85), C(ω) = ln Φ(ω)=iωµ − ω2σ2/2,
implying from Eq. (2.86) that C1 = µ, C2 = σ2, and Cn>2 = 0. Gaussians are unique in this
regard;80 one could define a Gaussian by the requirement that Cn>2 = 0. Gaussian processes are
the simplest generalization of deterministic processes (wherein the same outcome occurs with no
variance). Gaussians have one more parameter (than deterministic processes) describing the width
of the distribution such that Cn>2 = 0. The vanishing of higher-order cumulants implies that the
moments factorize in a characteristic way. From Eq. (2.88), we have for Gaussian statistics
C3 =0 =⇒ ⟨α3⟩ = 3⟨α2⟩⟨α⟩ − 2⟨α⟩
3 = 3σ2µ + µ3
C4 =0 =⇒ ⟨α4⟩ = 3⟨α2⟩
2 − 2⟨α⟩
4 = 3σ4 + 6σ2µ2 + µ4. (2.90)
2.9.2 Multivariate Gaussian distributions
Multivariate Gaussians are generalizations of the single-variable form to n random variables:
P(α1,...,αn) ≡ K exp 
− 1
2
n
ij=1
Aijαiαj +n
i=1
Biαi

, (2.91)
77Cumulants are sometimes written ⟨αn⟩c to signify the nth cumulant average. The average of an exponential,
Φ(ω) = ⟨eiωα⟩, is the exponential of a particular kind of average, Φ(ω) = exp C(ω) = exp ∞
n=1(iω)n⟨αn⟩c/n!

=
exp 
iω⟨α⟩ + 1
2 (iω)2(⟨α2⟩−⟨α⟩2) + 1
3! (iω)3(⟨α3⟩ − 3⟨α2⟩⟨α⟩ + 2⟨α⟩3) + ··· 
.
78Cumulants provide a systematic way to introduce approximations to Φ(ω), with the associated approximate probability
distribution P(α) found from inverse Fourier transformation[5, p76]. 79The partition function is the Laplace transform of the density of states function,[5, p86] Z(β) =  ∞
0 e−βEΩ(E)dE.
First derivatives of ln Z(β) generate thermodynamic quantities, e.g., P = kT ∂ ln Z/∂V , and second derivatives generate
measures of fluctuations such as the heat capacity, ∂2 ln Z/∂β2 = kT2CV . The characteristic function is the Fourier
transform of a probability distribution, equilibrium or not, Φ(ω) =  ∞
−∞ eiωαP(α)dα. Derivatives of ln Φ(ω) (of any
order) generate cumulants (of that order) [see Eq. (2.87)], which are functions of the moments of the distribution. 80The only smooth distribution having finitely many nonzero cumulants is the Gaussian[56].44 ■ Non-Equilibrium Statistical Mechanics
where Aij are elements of a real symmetric matrix and the quantities Bi are symmetry-breaking
terms. Equation (2.91) can be written in vector notation81
P(α) = K exp 
− 1
2
αT Aα + BT α

. (2.92)
If the random variables α1,...,αn are statistically independent, A is diagonal (Exercise 2.50). As
one can show, using the results of Exercise 2.5,82

exp 
− 1
2
αT Aα + BT α

dnα = (2π)n/2
√
det A exp 1
2
BT A−1B

.
Thus, take K = √
det A exp 
−1
2BT A−1B
/(2π)n/2. An equivalent way of writing Eq. (2.91)
(analogous to Eq. (2.84)) is, if we identify Bi = 
j Aijµj where µi ≡ ⟨αi⟩ (see Exercise 2.51),
P(α1,...,αn) =
√
det A
(2π)n/2 exp 
− 1
2
n
ij=1
(αi − µi)Aij (αj − µj )

. (2.93)
Combining Eq. (2.92) with Eq. (2.6), we have the characteristic function (a generalization of
Eq. (2.9))
Φ(ω1,...,ωn) = exp 
iωT µ − 1
2
ωT A−1ω

. (2.94)
The characteristic function of a multivariate Gaussian is therefore a multivariate Gaussian, implying
that cumulants of multivariate Gaussians vanish beyond second order (multivariate cumulants are
defined in Eq. (2.98)). All Gaussian distributions, multivariate or not, have the same property of
their cumulants. By combining Eq. (2.94) with Eq. (2.8) we find the generalization of Eq. (2.10)
⟨(αi − µi)(αj − µj )⟩ = −∂2Φ(ω1,...,ωn)
∂ωi∂ωj




ω=0
= (A−1)ij . (2.95)
2.9.3 Many-time cumulants
We defined in Eq. (2.6) the characteristic function Φ(ω1,...,ωn) of a multivariate probabil￾ity distribution P(α1,...,αn), where n random variables α1,...,αn are measured at the same
time. In Section 2.3 we defined stochastic processes characterized by a joint probability density
Pn(α1, t1; ··· ; αn, tn) that a single random variable α has the values αi ≡ α(ti) at the n times
t1,...,tn. We now define the many-time characteristic function of an n-time stochastic process
Φn(ω1, t1; ... ; ωn, tn) ≡

Pn(α1, t1; ... ; αn, tn)ei(ω1α1+···+ωnαn)
dnα, (2.96)
i.e., there’s a transform variable ωi associated with each αi = α(ti). The general n-time correlation
function is found by differentiating Eq. (2.96) (compare with Eq. (2.7)), where k ≡ k1 + ··· + kn,
⟨αk1 (t1)··· αkn (tn)⟩ =

αk1
1 ··· αkn
n Pn(α1, t1; ... ; αn, tn)dnα
= (−i)k ∂k
∂k1 ω1 ··· ∂kn ωn
Φn(ω1, t1; ... ; ωntn)




ω=0
. (2.97)
81Multivariate Gaussians are often defined in the form P(α) ∝ exp(− 1
2αT Σ−1α) with a matrix inverse, Σ−1. 82We’re using a single integral sign to indicate a multiple integration (a common practice in advanced work)—integrate
over the variables specified by the differential volume element using whatever limits of integration are appropriate.Fluctuations as stochastic processes ■ 45
We also define the many-time cumulant (compare with Eq. (2.87))
Cn(t1,...,tn) ≡ (−i)n ∂n
∂ω1 ··· ∂ωn
ln Φn(ω1, t1; ··· ; ωn, tn)




ω=0
. (2.98)
Using Eq. (2.98), we can write down the first few many-time cumulants:
C1(t1) = ⟨α(t1)⟩
C2(t1, t2) = ⟨α(t1)α(t2)⟩−⟨α(t1)⟩⟨α(t2)⟩
C3(t1, t2, t3) = ⟨α(t1)α(t2)α(t3)⟩−⟨α(t1)⟩⟨α(t2)α(t3)⟩
− ⟨α(t2)⟩⟨α(t3)α(t1)⟩−⟨α(t3)⟩⟨α(t1)α(t2)⟩
+ 2⟨α(t1)⟩⟨α(t2)⟩⟨α(t3)⟩. (2.99)
Cumulants have the property of vanishing (for Cn>1) when the random variables involved are statis￾tically independent; see Kubo[57] or [5, p77]. Nonzero cumulants probe many-particle correlations.
The vanishing of cumulants in time probes the time over which random variables remain correlated.
Example. Consider the construction of the third cumulant. Start by subtracting from the aver￾age ⟨α1α2α3⟩ the contribution when α1, α2, α3 are each statistically independent of the oth￾ers, ⟨α1⟩⟨α2⟩⟨α3⟩. Next subtract the value obtained when α1 is independent of α2 and α3,
⟨α1⟩(⟨α2α3⟩−⟨α2⟩⟨α3⟩). Repeat for α2 independent of α3 and α1, and α3 independent of α1
and α2. The final result is the expression for C3 shown in Eq. (2.99).
2.9.4 Gaussian stochastic processes, Doob’s theorem
A stochastic process is Gaussian if its hierarchy of joint probabilities consists of multivariate Gaus￾sians, i.e., for n = 1, 2,... ,
Pn(y1, t1; ··· ; yn, tn) =
√
det A
(2π)n/2 exp 
− 1
2
n
ij=1
(yi − µi)Aij (yj − µj )

, (2.100)
where yi ≡ y(ti) and µi = ⟨y(ti)⟩ is the expectation value of y(t) at time ti, and A is a positive￾definite n×n matrix. The many-time characteristic function follows by combining Eqs. (2.100) and
(2.96),
Φn(ω1, t1; ··· ; ωn, tn) = exp 
i
n
j=1
µjωj − 1
2
n
jk=1
(A−1)jkωjωk

. (2.101)
From Eq. (2.97) combined with Eq. (2.101), we find the generalization of Eq. (2.95) to include time,
(A−1)ij =

(yi − µi)(yj − µj )

≡

(y(ti) − ⟨y(ti)⟩)(y(tj ) − ⟨y(tj )⟩)

. (2.102)
We should check that the requirements on hierarchies listed on page 25 are satisfied by the
sequence in Eq. (2.100). The first two requirements are manifestly satisfied (Pn ≥ 0 and the
symmetry condition) and the last is satisfied by construction (normalization on P1). The con￾sistency condition  dynPn(y1,...,yn) = Pn−1(y1,...,yn−1) requires a few steps, however.
Use the fact that Pn(y1,...,yn) is the inverse Fourier transform of the associated characteristic46 ■ Non-Equilibrium Statistical Mechanics
function, Pn(y1,...,yn)=1/(2π)n  dω1 ··· dωne−i(ω1y1+···ωnyn)
Φn(ω1,...,ωn). Then,

dynPn(y1,...,yn) = 1
(2π)n

dω1 ··· dωne−i(ω1y1+···+ωn−1yn−1)
Φn(ω1,...,ωn)
×

dyne−iωnyn
= 1
(2π)n−1

dω1 ··· dωne−i(ω1y1+···+ωn−1yn−1)
δ(ωn)Φn(ω1,...,ωn)
= 1
(2π)n−1

dω1 ··· dωn−1e−i(ω1y1+···+ωn−1yn−1)
Φn−1(ω1,...,ωn−1)
= Pn−1(y1,...,yn−1).
In the first line we interchanged the order of integration and in the second we used the repre￾sentation of the Dirac delta function, δ(x)=1/(2π)
 ∞
−∞ dke−ikx. One might wonder where
properties specific to Gaussians were used in this derivation. We used the minimal property that
Φn(ω1,...,ωn−1, ωn = 0) = Φn−1(ω1,...,ωn−1), as can be seen from Eq. (2.101).83
2.9.4.1 Stationary Gaussian processes
Gaussian processes are specified by the covariance matrix A−1, which we illustrate with the sim￾plest example of finding the form of bivariate stationary Gaussian processes, P2(y1, t1; y2, t2),
where for convenience we consider a process y(t) with zero expectation value ⟨y(ti)⟩ = 0 (oth￾erwise define a new variable αi ≡ yi − µi.) In that case, from Eq. (2.102),
A−1 =
 ⟨y2(t1)⟩ ⟨y(t1)y(t2)⟩
⟨y(t1)y(t2)⟩ ⟨y2(t2)⟩

≡
 C C12
C12 C

, (2.103)
where we’ve used that for stationary processes ⟨y2(t1)⟩ = ⟨y(t2)2⟩ = ⟨y2⟩ = C, a constant (Eq.
(2.23)); the autocorrelation function C12 is a function of the time difference t1 − t2, Eq. (2.24).
From the form of A−1 in Eq. (2.103), we find for A
A = 1
C2 − C2
12
 C −C12
−C12 C

= 1
C(1 − (C12/C)2)
 1 −C12/C
−C12/C 1

≡ 1
C(1 − Γ2
12)
 1 −Γ12
−Γ12 1

, (2.104)
where
Γ12 ≡ ⟨y(t1)y(t2)⟩
⟨y2⟩ (2.105)
is the normalized autocorrelation function. With the matrix A determined, we have from Eq. (2.100)
P2(y1, t1; y2, t2) = 1
2πC1 − Γ2
12
exp 
− 1
2C(1 − Γ2
12)

y2
1 + y2
2 − 2Γ12y1y2
 
. (2.106)
P2 is stationary through the stationarity of the correlation function ⟨y(t1)y(t2)⟩ = Γ(t1 − t2). If
Γ12 = 0, i.e., if y(t1), y(t2) are uncorrelated, P2 factors into a product of independent Gaussians.
With P2 established, we can find P1 by integrating  P2(y1, t1; y2, t2)dy2 = P1(y1, t1),
P1(y, t) = 1
√
2πC exp 
− y2
2C

(2.107)
83One might also wonder why we’ve taken the diversion through the characteristic function; why not simply integrate
over Pn directly? Such an approach is more difficult and requires a certain dexterity with determinants, but the end result is
the same. The characteristic function doesn’t have a multiplicative factor involving determinants, a great simplification.Fluctuations as stochastic processes ■ 47
(the same as we’d find from Eq. (2.100) for n = 1). We see that, indeed, P1(y, t) = P1(y) for
stationary processes (see Section 2.4.1). We can also find the transition probability associated with
a stationary Gaussian process. From Eq. (2.19),
P1|1(y1, t1|y2, t2) = P2(y1, t1; y2, t2)
P1(y2, t2) = 1
2πC(1 − Γ2
12)
exp 
− 1
2C(1 − Γ2
12)
(y1 − Γ12y2)
2

.
(2.108)
We use this expression in Chapter 3.
2.9.4.2 Doob’s theorem
Does the transition probability in Eq. (2.108) represent a Markov process? It would appear so—the
probability of the transition y2 → y1 does not depend on the past history of the system. Markov
processes satisfy the SCK equation (Section 2.4.3). Let’s check if, using Eq. (2.108), whether
P1|1(y1, t1|y3, t3) =  dy2P1|1(y1, t1|y2, t2)P1|1(y2, t2|y3, t3). Setting C = 1 for convenience,
we find
1
2π
(1 − Γ2
12)(1 − Γ2
23)

dy2 exp 
− 1
2(1 − Γ2
12)
(y1 − Γ12y2)
2

(2.109)
× exp 
− 1
2(1 − Γ2
23)
(y2 − Γ23y3)
2

= 1
2π(1 − Γ2
12Γ2
23)
exp 
− 1
2(1 − Γ2
12Γ2
23)
(y1 − Γ12Γ23y3)
2
 ?
= P1|1(y1, t1|y3, t3).
Comparing the result of Eq. (2.109) with the form on the right side of Eq. (2.108) (with C=1), the
SCK equation is satisfied if the autocorrelation functions have the composition property
Γ13 = Γ12Γ23. (2.110)
Equation (2.110) is a functional equation, an equation where the unknowns are functions.84 Based
on the stationarity of Γij ≡ ⟨y(ti)y(tj )⟩ ≡ ϕ(|ti − tj |), and the ordering of the times t1 > t2 > t3,
Eq. (2.110) is equivalent to the functional equation ϕ(x + y) = ϕ(x)ϕ(y), the solution of which is
ϕ(x) = ax = ex ln a for some base a. We require that, for a positive constant c,
Γ(τ ) = exp(−c|τ |) (c ≥ 0) (2.111)
because Γ(τ ) ≤ Γ(0) = 1 and Γ(τ ) = Γ(−τ ) (see Exercise 2.12). Equation (2.111) is known as
Doob’s theorem,
85 that the autocorrelation function of a stationary Markov Gaussian process has
the exponential form Γ(τ ) = exp(−c|τ |).
SUMMARY
An introduction to the statistical theory of fluctuations was presented, an approach that follows from
the classic works of Einstein and Onsager and comprises a necessary foundation for the study of
nonequilibrium systems.
• The Einstein theory of fluctuations (Section 2.1) draws on the Boltzmann entropy formula
to establish the probability P(∆S) ∝ e∆S/k that fluctuations about equilibrium are associ￾ated with entropy fluctuations ∆S, where ∆S < 0. Entropy fluctuations, in turn, are related,
84A useful reference on functional equations is Aczel[58]. 85There are many Doob theorems in the literature. We’re referring to Theorem 1.1 in [59], reprinted in Wax[44, pp319–
337]. Wax is an invaluable resource to students of nonequilibrium statistical physics.48 ■ Non-Equilibrium Statistical Mechanics
through the first law of thermodynamics, to fluctuations αi ≡ Xi −X0
i in the extensive quan￾tities of the system Xi, where X0
i denotes the equilibrium value. With the Einstein theory,
one can calculate static properties of fluctuations, such as their average magnitudes and co￾variances. It provides, in the form of a multivariate Gaussian function, the joint probability
density P(α1,...,αn) that n fluctuating extensive quantities have the instantaneous values
α1,...,αn. We found a nonzero correlation between fluctuations and thermodynamic forces,
⟨αjFl⟩ = −kδjl, a key result in the Onsager theory of irreversible processes. Through scatter￾ing experiments we know that long-range spatial correlations can develop among fluctuations
(such as in critical opalescence). The time-dependent properties of fluctuations are not used
in setting up statistical mechanics but are crucial in nonequilibrium statistical mechanics.
• Fluctuations are dynamical processes, and by recognizing that we’re led to consider the corre￾lation of fluctuations in time. We defined time correlation functions in terms of time averages,
αi(t)αj (t + τ ) ≡ limT→∞(1/T)
 T
0 αi(t)αj (t + τ )dt, Eq. (2.14). In statistical mechanics,
time averages are equated with ensemble averages (ergodic hypothesis) to such an extent that
only infrequently does one encounter time averages. The ergodic hypothesis does not apply
to systems out of equilibrium and consequently one often encounters quantities defined as
time averages in nonequilibrium statistical mechanics. In our initial look at time correlation
functions (Section 2.2), we treated fluctuations as dynamical quantities subject to Hamilton’s
equations of motion. This was done so we could invoke Birkhoff’s theorem on the properties
of time averages and so that we could invoke the time-reversal symmetry of the microscopic
equations of motion. As a result, we arrived at Eq. (2.16), αi(t)αj (t + τ ) = αi(t + τ )αj (t),
a kind of “time-influence” symmetry of fluctuations, that if αi(t) is correlated with (influ￾ences) αj (t + τ ) with τ > 0, then αj (t) influences αi(t + τ ) in the same way. Equation
(2.16) is a consequence of the time-reversal invariance of the microscopic dynamics under￾lying fluctuations and the time-translational invariance of equilibrium averages. Even though
we can’t calculate the time dependence of αi(t) from first principles, Eq. (2.16) is a fun￾damental property of fluctuations about the equilibrium state. And, as discussed in Section
2.2, Eq. (2.16) is Onsager reciprocity in disguised form. Remarkably, kinetic coefficients, in￾troduced as phenomenological parameters in the description of irreversible processes, obey
the experimentally verified reciprocal relations from the time-reversibility of the microscopic
motions of system components.
• Although we treated fluctuations as dynamical quantities in Section 2.2 and classified them
by their time-reversal properties, we have no way of modeling their time dependence from
first principles. For that purpose we turned to stochastic processes (Section 2.3), the branch
of probability theory devoted to time-dependent random processes. A stochastic process is a
family of random variables {α(ti)} indexed by time. Stochastic processes are specified by
a hierarchy of joint probability densities meeting prescribed rules. Whereas in Section 2.1
we found the probability density for n variables α1,...,αn representing fluctuations in n
different physical quantities, all measured at the same time, in Section 2.3 we considered a
random variable α representing the same quantity observed at n different times, (t1,...,tn).
The joint probability density for an n-time stochastic process is a function of 2n variables,
the n times (t1,...,tn) and the n random variables (α(t1),...,α(tn)), indicated notationally
Pn(y1, t1; y2, t2; ... ; yn, tn) as a function of the times ti and the values of random variables
yi = α(ti). Note the subscript on Pn; the set of functions Pn for n = 1, 2,... comprises
a hierarchy of joint probabilities describing the stochastic process in successively more de￾tail. Using joint probability densities, one can calculate various correlation functions; e.g.,
⟨y(t1)y(t2)⟩ =   y1y2P2(y1, t1; y2, t2)dy1dy2. There are three main classes of stochastic
processes used in nonequilibrium statistical physics: independent, stationary, and Markov.Fluctuations as stochastic processes ■ 49
• A stochastic process is stationary if, for all n and τ ,
Pn(y1, t1; y2, t2; ... ; yn, tn) = Pn(y1, t1 + τ ; y2, t2 + τ ; ... ; yn, tn + τ ), (2.22)
i.e., stationary processes have no unique origin in time. Stationarity implies that P1 is in￾dependent of time, P1(y1, t1) = P1(y1), and that P2(y1, t1; y2, t2) is a function of the
single time τ ≡ t1 − t2, P2(y1, t + τ ; y2, t). As a consequence, the mean of station￾ary processes is a constant, ⟨y(t)⟩ =  dyyP1(y, t) = constant, and the autocorrelation
function R(τ ) ≡ ⟨y(t + τ )y(t)⟩ =   dy1dy2y1y2P2(y1, t + τ ; y2, t) has the properties
R(τ ) = R(−τ ) and |R(τ )| ≤ R(0). Moreover, R(τ ) factorizes for τ ≫ τc, where τc is a
system-dependent characteristic time, R(τ ) τ≫τc −→ ⟨y(t)⟩⟨y(t + τ )⟩, i.e., the process becomes
uncorrelated for τ ≫ τc. To be useful as dynamical models of fluctuations, stationary pro￾cesses must be ergodic (Section 2.5), that time averages yT ≡ (1/2T)
 T
−T y(t)dt approach
the ensemble mean as T → ∞, yT −→ ⟨yT ⟩. This requirement is met when R(τ ) decays
sufficiently rapidly in time; see Eq. (2.33). We derived the Wiener-Khinchin theorem, that for
stationary processes the spectral density (defined in Eq. (2.37)) is the Fourier transform of
the autocorrelation function. As an illustration of an ergodic stationary process, we analyzed
in Section 2.6 the noise generated in conductors by the random thermal agitations of charge
carriers and derived the experimentally verified Nyquist theorem relating the noise power
spectral density to the resistance and the absolute temperature.
• The conditional probability Pn|v(y1, t1; ... ; yn, tn|yn+1, tn+1; ... ; yn+vtn+v) is the proba￾bility that n samplings of α(t) have the values indicated, given that v samplings are known to
have produced the values indicated for t1 > ··· > tn > tn+1 > ··· > tn+v; see Eq. (2.19).
The Markov approximation consists of replacing Pn|v with Pn|1,
Pn|v(y1, t1; ... ; yn, tn|yn+1, tn+1; ... ; yn+v, tn+v)
Markov
approximation −→ Pn|1(y1, t1; ... ; yn, tn|yn+1, tn+1).
In a Markov process the probability of the current state depends only on the most recent past
observation, and not on observations made at earlier times; the “present” step is conditioned
by memory only of the most recent past. The probability P1|1 is well suited to physical de￾scriptions in a way similar to quantum mechanics: P1|1(y1, t1|y2, t2) is the probability that
α(t1) = y1 given that α(t2 < t1) = y2 is known to have occurred. P1|1(y1, t1|y2, t2) is
referred to as a transition probability; the system makes a transition from y2 to y1 in the time
interval (t1 − t2) with probability P1|1. The basic equation of Markov processes is the SCK
equation
P1|1(y1, t1|y3, t3) = 
dy2P1|1(y1, t1|y2, t2)P1,1(y2, t2|y3, t3). (2.29)
For t1 > t2 > t3, the probability of α(t1) = y1 given that α(t3) = y3 can be found by
considering the process as a sum over compound processes consisting of the probability of
α(t2) = y2 given α(t3) = y3 multiplied by the probability of α(t1) = y1 given α(t2) = y2.
• We considered the symmetric random walk problem in one dimension as an illustration of a
Markov process (Section 2.7). A particle situated at one of the discrete positions x = ma,
m = 0, ±1, ±2, ··· , makes a transition (at a discrete time t = nτ , n = 0, 1, 2,...) to one
of the neighboring sites m ± 1 with equal probability. The goal is to calculate the conditional
probability p(m, n) that a “walker” started at location m = 0 at time n = 0 is found at
x = ma at time t = nτ . This problem can be treated with traditional methods of probability
and combinatorics (see Exercise 2.22). We treated this problem as a stochastic process using
the discrete form of the SCK equation; see Eq. (2.47). There are many connections between
random walks and diffusion. We derived the Einstein result that in diffusive motion the mean￾square displacement ⟨x2⟩ ∝ t.50 ■ Non-Equilibrium Statistical Mechanics
• The master equation is an equivalent form of the SCK equation (and hence applies to Markov
processes) that finds wide use in physics and chemistry. For continuous-time processes, the
master equation is an integro-differential equation for the nonequilibrium probability distri￾bution,
∂
∂tP(y, t) =  
W(y|y′
)P(y′
, t) − W(y′
|y)P(y, t)

dy′
, (2.66)
where W(y|y′
) ≥ 0 is a rate, the transition probability per unit time from state y′ to state y.
The characteristic feature of the master equation is its “gain-loss form,” that the rate at which
P(y, t) changes is a balance between rates of gain processes in which transitions occur into
the state y from all other states y′ and loss processes in which transitions to all other states y′
occur from y.
For a discrete state space labeled by index n, the master equation has the form
∂
∂tP(n, t) = 
n′̸=n
[W(n|n′
)P(n′
, t) − W(n′
|n)P(n, t)] ≡ 
n′
Wn,n′P(n′
, t),
where Wn,n′ is an element of the transition matrix (see Eq. (2.74)). The detailed balance con￾dition, W(n|n′
)Peq(n′
) = W(n′
|n)Peq(n) (or Wn,n′Peq(n′
) = Wn′,nPeq(n)), is a dynamic
characterization of equilibrium, where for any pair of states n, n′
, the rate of the transition
n′ → n is the same as that of the reverse transition n → n′
. Detailed balance does not
have to be imposed as a requirement on transition rates; it follows from fundamental physics,
the time-reversal invariance of microscopic motions (just as with the Onsager relations). The
transition matrix elements have the property 
n Wn,n′ = 0 for each n′
, implying that the
total probability 
n P(n, t) is conserved in time. It also implies, when combined with de￾tailed balance, that the equilibrium probability distribution Peq(n) is an eigenvector of W
having zero eigenvalue, 
n′ Wn,n′Peq(n′
)=0. It can be shown that the nonzero eigenval￾ues of the transition matrix are negative (see Exercise 2.45), implying that the nonequilibrium
probability distribution evolves in time to the equilibrium distribution (see Eq. (2.83)).
• Gaussian processes (Section 2.9) are extensions of Gaussian probability distributions to in￾clude stochastic variables and time-dependent parameters. Gaussians occur generically in sta￾tistical physics through the central limit theorem, and they occur in nonequilibrium processes
as well. The theory of cumulants was introduced as a way of parameterizing Fourier trans￾forms (characteristic functions) of probability distributions primarily because Gaussians have
the unique property that their associated cumulants vanish except for the first two. Gaussian
processes are the simplest generalization of deterministic processes wherein the same out￾come occurs with no variance; they have one more parameter (than deterministic processes)
describing the width of the distribution in such a way that higher-order cumulants vanish.
Gaussian processes are specified by the covariance matrix, Eq. (2.102), the n × n matrix of
time correlation functions among the random variables of an n-time stochastic process. We
outlined the proof of Doob’s theorem that the autocorrelation function of a stationary Markov
Gaussian process decays exponentially in time.
EXERCISES
2.1 In the statistical theory of fluctuations we encounter multiple integrals of the form
I ≡
 ∞
−∞
···  ∞
−∞
exp 
− n
ij=1
Aijxixj

dx1 ··· dxn, (P2.1)
where Aij are elements of an n×n symmetric, positive-definite matrix A. The purpose of this
exercise is to guide you through one approach to evaluating this integral. The strategy is toFluctuations as stochastic processes ■ 51
change variables such that the quadratic form86 
ij Aijxixj ≡ xT Ax is simplified, where
T denotes transpose and xT = (x1,...,xn). Make a linear transformation xi = 
k Likyk,
or in matrix notation x = Ly, such that xT Ax = yT y. We require yT LT ALy = yT y, or
LT AL = I, where I is the n × n identity matrix. From the rules of determinants (det LT =
det L), (det L)
2 det A = 1, implying det L = 1/
√
det A (det L exists if det A ̸= 0). The
volume element in Eq. (P2.1) transforms as dx1 ··· dxn = det Ldy1 ··· dyn (det L is the
Jacobian). Show that
I = πn/2
√
det A. (P2.2)
Hint:  ∞
−∞ e−y2
dy = √π.
2.2 Evaluate the derivatives in Eq. (2.5). Refer to [2, p19] for definitions of α, βT , CV .
a. Show that  ∂2S
∂U2

V
=
 ∂
∂U  1
T

V
= − 1
T2CV
.
b. Show that
∂2S
∂U∂V =
 ∂
∂U P
T

V
= 1
T
∂P
∂U 
V
− P
T2
 ∂T
∂U 
V
= − 1
CV T
P
T − α
βT

.
Use the handy trick ∂P
∂U 
V
=
∂P
∂T 
V
 ∂T
∂U 
V
.
c. Show that  ∂2S
∂V 2

U
=
 ∂
∂V P
T

U
= 1
T
∂P
∂V 
U
− P
T2
 ∂T
∂V 
U
. Use the cyclic
relation[2, p18] or perhaps your skill with Jacobian determinants[2, p49] to show
∂P
∂V 
U
= ∂(P, U)
∂(V,U) = ∂(P, U)
∂(V,T)
∂(V,T)
∂(V,U) = 1
CV
∂(P, U)
∂(V,T) = −
 1
βT V + α
βT CV
∂U
∂V 
T

.
In the same way, show that
 ∂T
∂V 
U
= ∂(T,U)
∂(V,U) = ∂(T,U)
∂(T,V )
∂(T,V )
∂(V,U) = − 1
CV
∂U
∂V 
T
.
Show we have the intermediate result
 ∂2S
∂V 2

U
= −
 1
βT T V +
1
CV T
∂U
∂V 
T
 α
βT
− P
T
 .
It remains to evaluate the derivative (∂U/∂V )T , which can be found from an analysis of the first
law of thermodynamics together with a Maxwell relation,[2, p20]
∂U
∂V 
T
= T α
βT
− P.
Conclude that
 ∂2S
∂V 2

U
= −

1
βT T V +
1
CV
 α
βT
− P
T
2

.
86A quadratic form is a homogeneous quadratic polynomial in any number of variables. In three variables, ax2 + by2 +
cz2 + dxy + exz + fyz is a quadratic form for constants (a, ··· , f). A quadratic form in n variables can be generated by
an n × n symmetric matrix. A positive-definite quadratic form is positive for any nonzero values of its variables.52 ■ Non-Equilibrium Statistical Mechanics
2.3 Let {Xi}n
i=1 be a set of statistically independent random variables (see Appendix B), each
with its own characteristic function ΦXi (ω), and let Y be a random variable resulting from a
linear combination Y ≡ n
i=1 aiXi, where the ai are constants. Show that the characteristic
function associated with Y is the product of characteristic functions for the Xi,
ΦY (ω) = n
i=1
ΦXi (ω).
Hint: ΦY (ω) = ⟨eiωY ⟩ = ⟨e
iω
n
i=1 aiXi
⟩ = ⟨eiωa1X1 ··· eiωanXn ⟩ and the joint probability
distribution P(X1,...,Xn) for independent random variables factorizes.
2.4 Verify Eq. (2.7).
2.5 Guided exercise: Consider the following integral, the form of which we have in Eq. (2.9) (a
multivariate Gaussian integral, see Section 2.9.2),
I ≡
 ∞
−∞
···  ∞
−∞
exp 
−arT Gr + iωT r

dx1 ··· dxn, (P2.3)
where a > 0, r ≡ (x1,...,xn)T , G is a real symmetric n×n matrix, and ω = (ω1,...,ωn)T
is a constant vector. As always, the strategy is to make a suitable change in variables.87 Be￾cause G is real and symmetric, there exists an orthogonal matrix L such that LT GL = Λ,
where Λ is a diagonal matrix with the eigenvalues of G as its entries[13, p41]. Introduce a
new vector y such that r = Ly. The argument of the exponential in Eq. (P2.3) transforms as
−arT Gr + iωT r → −ayT Λy + iωT Ly = n
k=1

−aλky2
k + i 
ωT L

k yk

,
where λk are the eigenvalues of G. Because L is orthogonal, dnx = dny. Thus, Eq. (P2.3)
is equivalent to
I = n
k=1
 ∞
−∞
exp 
−aλky2
k + i 
ωT L

k yk

dyk. (P2.4)
Using standard methods of integration, show that Eq. (P2.4) is equivalent to
I = n
k=1
 π
aλk
exp 
−(ωT L)2
k
4aλk

= (π/a)n/2
n
k=1 λk
exp 
−n
k=1
(ωT L)
2
k/(4aλk)

. (P2.5)
Show that
ωT LΛ−1LT ω = 
k
(ωT L)2
k
λk
. (P2.6)
Hint: First show that 
LT ω

i = 
ωT L

i
. Then show that Λ−1 = LT G−1L, and hence
I = (π/a)n/2
√
det G exp 
− 1
4a
ωT G−1ω

. (P2.7)
The determinant of a matrix is the product of its eigenvalues[13, p38].
2.6 Show the results in Eq. (2.10). Hint: The inverse of a symmetric matrix is symmetric.
2.7 Derive the inverse matrix G−1 in Eq. (2.11) starting from the matrix G in Eq. (2.5).
87Note the difference between the approach here and that of Exercise 2.1.Fluctuations as stochastic processes ■ 53
2.8 Show from the example where U and V are allowed to fluctuate (page 22), that
⟨(∆U)
2
⟩ = CV kT2 + ⟨(∆V )
2
⟩
∂U
∂V 2
T
⟨∆U∆V ⟩ = ⟨(∆V )
2
⟩
∂U
∂V 
T
.
From the first formula we see that ⟨(∆U)
2
⟩ is not independent of ⟨(∆V )
2
⟩, which is reflected
in the nonzero correlation ⟨∆U∆V ⟩ in the second formula.
2.9 Show that ⟨FiFj ⟩ = kgij . Hint: Combine two copies of Eq. (1.43) with Eq. (2.10).
2.10 Equation (2.13) (required in the proof of the Onsager relations) can be derived directly, with￾out using the matrix G.
a. Show that Fj = k
∂ ln P
∂αj
. Use Eq. (2.2) together with Eq. (1.43).
b. Show that ⟨αiFj ⟩ = k
 ∞
−∞ ···  ∞
−∞ αi
∂P
∂αj
dnα = −kδij . Consider separately the cases
j ̸= i and j = i. Integrate by parts.
2.11 Fill in the steps between Eq. (2.17) and the Onsager relations, Eq. (1.38).
2.12 For a stationary stochastic process, show that:
a. The autocorrelation function, Eq. (2.24), has the property R(τ ) = R(−τ ). Hint: Use the
symmetry condition under interchange of arguments on page 25.
b. |R(τ )| ≤ R(0). Hint: Show that
0 ≤ ⟨(y(τ ) ± y(0))2⟩ = ⟨y2(τ )⟩ + ⟨y2(0)⟩ ± 2⟨y(τ )y(0)⟩.
2.13 Show that the joint probability densities Pk in Eq. (2.25) for independent processes satisfy
the consistency conditions listed on page 25.
2.14 Derive Eq. (2.26).
2.15 Show in the Markov approximation that
P3(y1, t1; y2, t2; y3, t3) = P2(y1, t1; y2, t2)P2(y2, t2; y3, t3)
P1(y2, t2) .
2.16 Fill in the steps between Eqs. (2.28) and (2.29), the SCK equation.
2.17 Show by multiplying Eq. (2.29) by P1(y3, t3) and integrating over y3 (using standard rules for
joint and conditional probability densities), one is led to an identity, P1(y1, t1) = P1(y1, t1).
Thus one might expect the SCK equation to be a general relation because it reduces to an
identity, yet it’s derived using the Markov approximation. There are non-Markovian processes
for which their transition probabilities satisfy the SCK equation[41, p203].
2.18 Fill in the steps between Eq. (2.31) and Eq. (2.32).54 ■ Non-Equilibrium Statistical Mechanics
2.19 Suppose the autocorrelation function decays exponentially, R(τ ) = R(0)e−γ|τ|
, where γ > 0
is a constant. Use the Wiener-Khinchin theorem to derive the associated spectral density. A:
S(ω) = R(0) 2γ
γ2 + ω2 .
2.20 For a real-valued function f(x), show that its Fourier transform F(ω) is such that F∗(ω) =
F(−ω), where F∗ is the complex conjugate of F. Hint: Let f(x) =  ∞
−∞ eiωxF(ω)dω.
2.21 What is the root-mean-square of voltage fluctuations at room temperature in a 100 Ω resistor
with a measurement bandwidth of 1 MHz? A: 1.29 µV.
2.22 The one-dimensional random walk can be treated with the binomial distribution (see [5, p70];
see also Chapter 1 of Reif[60]). Consider a walker that takes nr (nl) steps to the right (left),
for a total of N = nr + nl steps. We seek the probability PN (m) that in N steps the walker
is at site m = nr − nl given that it was initially at the origin. For the symmetric random walk
show that
PN (m) = N!
 1
2 (N + m)

!
 1
2 (N − m)

!
1
2N . (P2.8)
Note that (N + m) and (N − m) are even integers (as can be shown). Show that Eq. (P2.8)
solves Eq. (2.48) when we identify PN (m) ≡ p(m, n), i.e., N steps imply n time steps.
2.23 Show from Eq. (2.49) that the Fourier transform satisfies the relation p(q + Q, n) = p(q, n)
for Q = 2πt, t = ±1, ±2,... , i.e., p(q, n) is periodic in q with period 2π. That’s why q in
p(q, n) is restricted to the interval (−π, π), the familiar Brillouin zone of one-dimensional
lattices—nothing new to be learned by extending q outside of the first Brillouin zone.
2.24 Show that Eq. (2.50) is the inverse of Eq. (2.49). First show that  π
−π ei(k−m)
dq = 2πδk,m
for integer (k,m).
2.25 Verify by direct substitution that the integral representation of p(m, n) in Eq. (2.52) satisfies
Eq. (2.48).
2.26 Use Eq. (2.52) to find p(m, 2). A: p(m, 2) = 1
4 (δm,−2 + 2δm,0 + δm,2). Does this make
sense given that p(m, 0) = δm,0?
2.27 Show that the normalization on p(m, n),
∞
 m=−∞ p(m, n), is preserved by Eq. (2.52). Hint: ∞
m=−∞ eiqm = 2πδ(q). See [13, p97].
2.28 Substantiate the claims made in Eq. (2.53). The following strategy may help. Define a func￾tion f(q) ≡ (cos q)
n cos mq. Show that
f(q)=(−1)n+mf(π − q).
If n + m is even (odd), then f(q) is even (odd) about q = π/2.
2.29 From Eq. (2.53), p(m, n)=0 for m+n an odd number. Give an explanation why p(±1, 2) =
0, i.e., why is this probability zero, given that p(m, 0) = δm,0.
2.30 Generalize the symmetric random walk of Section 2.7 to a biased random walk where at each
site there is a constant probability α (β) of a step to the right (left), where α + β = 1.Fluctuations as stochastic processes ■ 55
a. Show, using the appropriate generalization of Eq. (2.48), that Eq. (2.52) generalizes to
p(m, n) = 1
2π
 π
−π
eiqm 
αe−iq + βeiqn dq (P2.9)
where p(m, n) ≡ P1|1(m, n|0, 0) with the initial condition p(m, 0) = δm,0. Show from
Eq. (P2.9) that ∞
m=−∞ p(m, n)=1, for all n. Hint: ∞
m=−∞ eiqm = 2πδ(q). Show
that p(m, n) = p(−m, n) only if α = β. Thus, ⟨m⟩ ̸= 0 when α ̸= β.
b. Apply the binomial theorem, (x + y)N = N
k=0 N
k

xN−kyk,[5, p62] to Eq. (P2.9) to
derive the generalization of Eq. (P2.8),
p(m, n) = n!
 1
2 (n − m)

!
 1
2 (n + m)

!
α(n+m)/2β(n−m)/2.
c. Using Eq. (P2.9), show that
mp(m, n) = n [αp(m − 1, n − 1) − βp(m + 1, n − 1)] .
Hint: meiqm = −i∂/∂q(eiqm). Then show that in an n-step ensemble,
⟨m⟩n = n(α − β), (P2.10)
i.e., there is a constant drift characterized by the difference (α − β). With m = x/a and
n = t/τ , argue that Eq. (P2.10) provides an estimate of the drift speed (a/τ )(α − β).
d. Show that
m2p(m, n) = np(m, n)+n(n−1) 
α2p(m − 2, n − 2) − 2αβp(m, n − 2) + β2p(m + 2, n − 2)
.
Hint: m2eiqm = −∂2/∂q2(eiqm). Then show that
⟨m2⟩n = n + n(n − 1) (α − β)
2 . (P2.11)
Thus there is a drift component to the spread of the distribution normally associated with
diffusion. Equation (P2.11) generalizes Eq. (2.56).
e. Show that ⟨m2⟩n − ⟨m⟩2
n = 4αβn. The variance grows linearly with n.
2.31 Show that p(x, t) in Eq. (2.60) is: 1) normalized,  ∞
−∞ p(x, t)=1 for all times; and 2) reduces
to the Dirac function, limt→0 p(x, t) = δ(x). Hint: Show that δϵ(x) ≡ (1/
√πϵ)e−x2/ϵ is a
sequence of functions such that limϵ→0
 δϵ(x)f(x) = f(0) for any smooth function f(x).
See [13, p89].
2.32 Show that p(x, t) in Eq. (2.60) is the solution of the diffusion equation, (2.61).
2.33 Show how Eq. (2.62) follows from Eq. (2.56) using the substitutions x = ma, t = nτ , and
D = a2/(2τ ).
2.34 The Wiener-Levy process is specified by the probability distributions in Eq. (2.63).
a. Verify that ⟨y2⟩ = 2Dt using P1. Gaussian integrals are highly useful (remember where
they are for future reference):
 ∞
−∞
x2ne−ax2
dx =
π
a
1 · 3 · 5 ···(2n − 1)
(2a)n (n ≥ 1)
 ∞
−∞
x2n+1e−ax2
dx = 0. (n ≥ 0)56 ■ Non-Equilibrium Statistical Mechanics
b. Show that the SCK equation is satisfied by the Wiener-Levy form of P1|1(y2, t2|y1, t1).
2.35 Derive Eqs. (2.65) and (2.66). Hint: To derive Eq. (2.65), substitute just one copy of Eq. (2.64)
in the right side of Eq. (2.29). The time t1 on the left side is t2 + ∆t.
2.36 Derive the master equations (2.34) and (2.70) from the transition rates given in the examples
on page 38. Hint: Use Eq. (2.67).
2.37 By introducing the Fourier transform P(q, t) ≡ ∞
n=−∞ e−iqnP(n, t), show that Eq. (2.34)
is equivalent to
∂
∂tP(q, t) = −4α sin2(q/2)P(q, t). (P2.12)
Equation (P2.12) can be integrated to obtain a closed-form expression for P(q, t), from which
P(n, t) can be found through inverse Fourier transformation.
2.38 a. Because of a special trick, useful information can be extracted from Eq. (2.70) without
having to obtain an explicit solution. Multiply Eq. (2.70) by n, and sum. Show that
∂
∂t∞
n=0
nP(n, t)

= −α
∞
n=0
nP(n, t),
and thus
∂
∂t⟨n(t)⟩ = −α⟨n(t)⟩.
The average number of surviving nuclei decays exponentially, ⟨n(t)⟩ = n0e−αt, implying
that the probability of a nucleus surviving to time t is p(t)=e−αt.
b. The solution of Eq. (2.70) can be derived with elementary methods of probability the￾ory. Let p denote the probability of a single nucleus surviving to time t. Argue that the
probability of n nuclei surviving to time t is
P(n, t) = n0
n

pn(1 − p)
n0−n,
where n0
n

is the binomial coefficient (see for example [5, p62]).
c. Show that
P(n, t) = n0
n

e−nαt 
1 − e−αtn0−n (n = 0, 1,...,n0) (P2.13)
solves the master equation, (2.70), subject to the initial condition, P(n, 0) = δn,n0 .
2.39 The solution to Eq. (2.70) can be found using another method, that of generating functions,
G(z, t). The form of the generating function for a particular problem depends on the range
of the stochastic variables and on the nature of the state space.88 We’ve already used the
Fourier transforms of probability distributions (a type of generating function), such as Eq.
(2.6) for continuous random variables or Eq. (2.49) for discrete random variables having
the range (−∞,∞), p(q, n) = ∞
m=−∞ e−iqmp(m, n). For random variables restricted to
positive discrete values (as in Eq. (2.70)), one might try a discrete Laplace transform

g(s, t) ≡ ∞
n=0 e−nsP(n, t), or more generally the generating function
G(z, t) ≡ ∞
n=0
znP(n, t), (P2.14)
88The use of generating functions is therefore an art; there are no definite rules to guide.Fluctuations as stochastic processes ■ 57
where the nature of the transform variable z emerges from the problem at hand.89 Note that
G(1, t)=1 (for normalized distributions) and G(0, t) = P(0, t), for all t. The quantity
G(z, 0) = ∞
n=0 znP(n, 0) is determined by the initial condition P(n, 0).
a. Show by differentiating Eq. (P2.14) and using the master equation (2.70) that
∂
∂tG(z, t) = α
∞
n=0
zn [(n + 1)P(n + 1, t) − nP(n, t)]
= α

∂
∂z
∞
n=0
zn+1P(n + 1, t) − z
∂
∂z
∞
n=0
znP(n, t)

= α
 ∂
∂z [G(z, t) − P(0, t)] − z
∂
∂z G(z, t)

= α(1 − z) ∂
∂z G(z, t). (P2.15)
Equation (P2.15) is a partial differential equation for the generating function associated
with the master equation (2.70). Note from Eq. (P2.15) that ∂G(1, t)/∂t = 0 (normaliza￾tion on P(n, t) remains fixed).
b. Equation (P2.15) can be put in a more convenient form through a change of variables. Let
τ ≡ αt and ξ ≡ − ln(1 − z) (so that z = 1 − e−ξ). Show that Eq. (P2.15) can be written
∂
∂τ G(z, t) = ∂
∂ξ G(z, t). (P2.16)
c. Show that Eq. (P2.16) is satisfied by any differentiable function f of a single variable,
G(z, t) = f(τ + ξ). Show for the initial condition P(n, 0) = δn,n0 that G(z, 0) = zn0 .
Argue that f(ξ) = 
1 − e−ξ
n0
, and therefore that
G(z, t) = 
1 − (1 − z)e−αtn0
. (P2.17)
Verify by direct substitution that Eq. (P2.17) solves Eq. (P2.15).
d. Show that Eq. (P2.17) is equivalent to the sum
G(z, t) = n0
k=0
n0
k
 
1 − e−αtn0−k 
ze−αtk
and thus, comparing with Eq. (P2.14),
P(k, t) = n0
k
 
1 − e−αtn0−k
e−kαt, (k = 0, 1,...,n0) (P2.18)
in agreement with Eq. (P2.13). Hint: Use the binomial theorem. The binomial coefficients
have the property that N
k

= 0 for k>N, and thus the generating function in this case is
a finite series. We’ve relied on the uniqueness of power series in arriving at Eq. (P2.18).
89A generating function is a way of encoding information about the family of probabilities P(n, t), n = 0, 1, 2,... , by
treating them as the expansion coefficients of a power series (which represents an analytic function). The generating function
for Legendre polynomials, for example (see [13, p153]), G(x, y) ≡ ∞
n=0 Pn(x)yn, has the closed-form expression
G(x, y) = (1 − 2xy + y2)−1/2 (but not all generating functions are known in closed form). To quote G. Polya, “A
generating function is a device somewhat similar to a bag. Instead of carrying many little objects detachedly, which could be
embarrassing, we put them all in a little bag, and then we have only one object to carry, the bag.”[61, p101]. Mathematically,
the transform variable z need not have a physical interpretation, although it often does in applications.58 ■ Non-Equilibrium Statistical Mechanics
2.40 Show that Eq. (2.75) is equivalent to Eq. (2.67) when the general form of the transition matrix
Eq. (2.74) is used.
2.41 a. Show from the infinite series representation of the matrix exponential (see page 40) that
(d/dt)eW t = WeW t = eW tW.
b. Show that Eq. (2.76) formally solves Eq. (2.75).
c. Show that if the eigenfunctions and eigenvalues of an operator A are known, Aϕn =
λnϕn, n = 1, 2,... , then, for each n, eAϕn = eλn ϕn.
2.42 The distinction between left and right eigenvectors is often a topic to which students of the
physical sciences are not exposed (hence this exercise).
a. Show that Eq. (2.77) follows from the definition of transition matrix, Eq. (2.74). Show
that Eq. (2.77) implies the existence of a left eigenvector (row vector) ψ = (1, 1,..., 1)
having zero eigenvalue, where in multiplying from the left, ψ · W = 0.
b. Consider a square matrix M that has a right eigenvector (column vector) ϕ associated
with eigenvalue λ, M · ϕ = λϕ. Show, for any right eigenvector ϕ of M, there is a left
eigenvector ϕT of MT having the same eigenvalue, where T denotes transpose. (The char￾acteristic polynomial of M is the same as for MT ; the determinant of a matrix equals that
of its transpose.) For symmetric matrices, therefore (M = MT ), for any right eigenvector
ϕ of M, there is a left eigenvector ϕT of M having the same eigenvalue.
c. Now show, for a non-symmetric square matrix M, if it has a right eigenvector M ·ϕ = λϕ
and a left eigenvector ψ · M = µψ, that if ψ · ϕ ̸= 0, then µ = λ, but ϕ is not necessarily
the same as ψT . Note that ψ and ϕ are left and right eigenvectors of the same matrix M
(not MT ). There could be more than one right eigenvector ϕ having the same eigenvalue
as a left eigenvector ψ, as long as ψ · ϕ ̸= 0.
2.43 Show that Eq. (2.78) (detailed balance in terms of transition matrices) follows from Eq. (2.72)
and Eq. (2.74).
2.44 Show that the matrix elements defined in Eq. (2.79) are symmetric, Vn,n′ = Vn′,n.
2.45 In this exercise we show the eigenvalues of the matrix V in Eq. (2.79) are nonpositive. We do
so by constructing the quadratic form associated with V . A quadratic form Q is a polynomial
of degree two in a set of real variables (y1, y2,...),
Q(y1, y2,...) ≡ 
n,m
Vn,mynym, (P2.19)
where Vn,m are the elements of a symmetric matrix.
a. Start by showing from Eqs. (2.79) and (2.74) that
Vn,m =

Peq(m)
Peq(n)

W(n|m) − δn,m
k
W(k|n)

,
where W(n|m) is the probability per unit time of the transition from state m → n.
b. Show that the diagonal elements are negative,
Vn,n = −
k̸=n
W(k|n),Fluctuations as stochastic processes ■ 59
and the off-diagonal elements are positive
Vn,m =

Peq(m)
Peq(n)
W(n|m). (m ̸= n)
Combine these results with Eq. (P2.19) to show that
Q = −

n
y2
n

k̸=n
W(k|n) +
n

m̸=n

Peq(m)
Peq(n)
ynymW(n|m). (P2.20)
c. Show that Eq. (P2.20) is equivalent to
Q = −1
2

n,m
W(n|m)Peq(m)


yn
Peq(n)
− 
ym
Peq(m)
2
, (P2.21)
and thus that Q is negative for any sets of variables (y1, y2,...). Note there is no contri￾bution for m = n, so the sum is restricted to m ̸= n. The simplest way to proceed is to
show that Eq. (P2.21) implies Eq. (P2.20). Expand the quadratic term in Eq. (P2.21) and
make use of detailed balance in the form of Eq. (2.72).
d. In a basis of eigenfunctions, V is diagonal with its eigenvalues on the diagonal. In that
case, Q (from Eq. (P2.19)) has the form Q = 
n λny2
n. We conclude that the eigenvalues
are negative semidefinite, λn ≤ 0. (If you’re wondering whether one could have some
positive eigenvalues and still have the sum be negative, other methods of analysis show
that the real parts of the eigenvalues are nonpositive).
2.46 Derive Eq. (2.83) for the form of the solution to the master equation.
2.47 Derive Eq. (2.85). Along the way you’ll need to reckon the fact that, for real constant α,
 ∞
−∞
e−x2
dx =
 ∞
−∞
e−(x+iα)2
dx,
i.e., in this case, the integral with a complex offset is independent of offset. Show this follows
from Cauchy’s integral theorem, that the closed contour integral in the complex plane of an
entire function is zero, e.g.,  e−z2
dz = 0. Consider a rectangle in the complex plane with
vertices at z = −R, z = R, z = R + iα, z = −R + iα and let R → ∞. See [13, p246].
2.48 Show that the relation between cumulants and moments in Eq. (2.88) can be inverted:
⟨α⟩ = C1
⟨α2⟩ = C2 + C2
1
⟨α3⟩ = C3 + 3C2C1 + C3
1
⟨α4⟩ = C4 + 4C3C1 + 3C2
2 + 6C2C2
1 + C4
1 .
Thus, the nth moment is a function of cumulants of order k ≤ n.
2.49 Show that the expressions for C3, C4 in Eq. (2.89) follow from the results in Eq. (2.88).
2.50 a. Show that, if the matrix A in the multivariate Gaussian Eq. (2.91) is diagonal, the ran￾dom variables are statistically independent. Hint: Consider a joint probability distribution
obtained from the product of single-variable Gaussians:
P(α1,...,αn) = n
k=1
1
2πσ2
k
exp 
− α2
k
2σ2
k

.60 ■ Non-Equilibrium Statistical Mechanics
Conversely, one can always diagonalize a real symmetric matrix (spectral theorem). One
can therefore find linear combinations of correlated Gaussian variables that are statisti￾cally independent.
b. Show the closely related statement that linear combinations of independent Gaussian vari￾ables are Gaussian. A Gaussian variable is one that is distributed with a Gaussian prob￾ability distribution. That is, let {Xi}n
i=1 be a set of independent Gaussian variables and
form a linear combination Y ≡ n
i=1 aiXi, where the ai are constants. Show that Y is a
Gaussian variable. Hint: This is easily done using characteristic functions.
2.51 Show that Eq. (2.93) is equivalent to Eq. (2.91) when the choice for K is made such that the
distributions are normalized to unity. Hint: You should find that B = Aµ (Bi = 
j Aijµj ).
Show that µT Aµ = BT A−1B. For an invertible square matrix A,

A−1T = 
AT −1
,
and in this example A is symmetric. The inverse of a symmetric matrix is symmetric.
2.52 Derive Eq. (2.95).
2.53 Show that Eq. (2.107) follows by integrating Eq. (2.106).
2.54 Perform the integration indicated in Eq. (2.109).CHAPTER 3
Brownian motion and
stochastic dynamics
BROWNIAN motion is a central problem in statistical physics, the perpetual irregular mo￾tion exhibited by colloidal1 or Brownian or B-particles, shown in Fig. 3.1. Incessant erratic
Figure 3.1 Brownian motion, the positions of a small particle suspended in water recorded
at regular time intervals. From [62, p116]. The length scale is 3 µm.
motions are maintained by collisions with the molecules of the suspension medium, impacts oc￾curring at such high rates2 that B-particle motion can only be modeled as a stochastic process. We
1A colloid is a mixture in which dispersed particles of size ≈ 1µm (large relative to molecular dimensions) are sus￾pended in a substance such as a liquid, aerosol, or gel. A colloid has a dispersed phase of suspended particles and a host
phase, the suspension medium. 2See Exercise 3.1.
DOI: 10.1201/9781003512295-3 6162 ■ Non-Equilibrium Statistical Mechanics
can’t expect to follow the trajectories of B-particles as in classical mechanics; the very concept of
path breaks down in Brownian motion,3 reminiscent of quantum mechanics. Two approaches are in
wide use, the Langevin equation and the Fokker-Planck equation, the subjects of this chapter.
3.1 LANGEVIN EQUATION, EINSTEIN RELATION
3.1.1 Thermal noise
We begin with the electrical analog of Brownian motion.4 Consider the RL circuit in Fig. 3.2 where
V (t) ⟨V 2⟩ = 4kTR∆f L
R
Figure 3.2 RL series circuit showing the fluctuating voltage V (t) associated with R.
the indicated voltage source V (t) is a fluctuating voltage with ⟨V ⟩ = 0 and second moment given
by Nyquist’s theorem, i.e., V (t) is the thermal noise associated with resistor R. Dropping the ca￾pacitance in Eq. (2.41), the fluctuating current satisfies the differential equation
dI
dt +
1
τc
I = 1
LV (t), (3.1)
where τc ≡ L/R is a characteristic time. Equation (3.1) is an instance of a Langevin equation, a
stochastic differential equation modeling a dynamical system subject to a random driving force.
The formal solution5 of Eq. (3.1) is developed in Exercise 3.2, from which one finds ⟨I(t)⟩ = 0.
Using Eq. (P3.1), the current autocorrelation is connected to that of the voltage through the relation,
⟨I(t)I(t + τ )⟩I0 = I2
0 e−(2t+τ)/τc +
1
L2 e−(2t+τ)/τc
 t
0
 t+τ
0
dt
′
dt
′′e(t′
+t′′)/τc ⟨V (t
′
)V (t
′′⟩I0 ,
(3.2)
where ⟨⟩I0 denotes an average over the subensemble having I = I0 at t = 0 and where cross terms
have been dropped (⟨V (t)⟩I0 = 0). Voltage fluctuations are stationary and hence ⟨V (t
′
)V (t
′′)⟩I0
is a function of the relative time, K(t
′′ − t
′
). The substitutions s ≡ t
′′ − t
′ and u ≡ t
′′ + t
′
suggest themselves as a means of simplifying the double integral in Eq. (3.2). The change of
variables is straightforward, but what are the limits of integration? Figure 3.3 shows the geome￾try of the coordinate transformation where a rectangle of area t(t + τ ) in the t
′′-t
′ plane is ro￾tated 45◦ into a rectangle in the u-s plane of area 2t(t + τ ) (the Jacobian equals 1
2 ). We find, for
J ≡  t
0
 t+τ
0 dt
′
dt
′′ exp ((t
′ + t
′′)/τc) K(t
′′ − t
′
), that
2J =
 0
−t
K(s)ds
 2t+s
−s
eu/τc du+
 τ
0
K(s)ds
 2t+s
s
eu/τc du+
 t+τ
τ
K(s)ds
 2(t+τ)−s
s
eu/τc du.
(3.3)
3The small black circles in Fig. 3.1 indicate the position of the suspended particle recorded every 30 seconds, published
in 1913 by Jean Perrin, who received the 1926 Nobel Prize in Physics for confirming the atomic picture of matter. The
straight lines connecting the dots in Fig. 3.1 do not represent the particle’s path. To quote Perrin, “If, in fact, one were to
mark second by second, each of the straight line segments would be replaced by a polygon path of 30 sides.” The path is
jagged at all length scales accessible to observation. Even today, Perrin’s Atoms[62] is worth reading. 4It’s well known that electrical networks can represent mechanical problems.
5We say formal because we don’t know the functional form of V (t), only its statistical properties.Brownian motion and stochastic dynamics ■ 63
s
u
−t
t
2t + τ
t + τ
τ
t + τ
τc
−τc
Figure 3.3 Domain of integration in the (u, s) plane corresponding to that in the (t
′′, t′
)
plane associated with the double integral in Eq. (3.2). Crosshatching indicates the physi￾cally relevant region |s| ≲ τc ≪ τ. The rectangle has area 2t(t + τ ).
As a check, if K(s) and eu/τc are replaced with unity in Eq. (3.3), the net effect of the integrals is
to generate the area 2t(t + τ ).
Equation (3.3) is as far as we can take an exact calculation without the explicit form of K(s).
Whatever its form, K(s) is appreciable only for |s| ≲ τc ≪ τ , the crosshatched region in Fig. 3.3.
We can therefore ignore the third double integral on the right of Eq. (3.3). Performing the remaining
integrations over u and approximating e|s|/τc ≈ 1, we find
J ≈ τc
2

e2t/τc − 1
  ∞
−∞
K(s)ds, (3.4)
where we’ve extended the limits of integration on s to ±∞ (where K(s) is negligibly small). Com￾bining Eq. (3.4) with Eq. (3.2),
⟨I(t)I(t + τ )⟩I0 = I2
0 e−(2t+τ)/τc + τc
2L2 e−τ/τc

1 − e−2t/τc

C, (3.5)
where C ≡  ∞
−∞ K(s)ds is the area under the curve. Equation (3.5) is not exact; it’s an asymptotic
result for t, τ ≫ τc. The fine details of the short-time behavior have been smoothed over.
For short times, fluctuations are determined by initial values, but at long times initial conditions
are forgotten. With t = 0 in Eq. (3.5), ⟨I(0)I(τ )⟩I0 = I2
0 e−τ/τc ; τc is the time scale over which
current fluctuations become uncorrelated. For τ = 0 and t → ∞,
⟨I2(t)⟩ t→∞ −→ τc
2L2 C
equipartition
↓
= kT
L , (3.6)
where we’ve equated ⟨I2(∞)⟩ with its equilibrium value. At long times, current fluctuations are
driven by voltage fluctuations. Estimating C ≈ 2τc⟨V 2⟩ in Eq. (3.6), we have ⟨I2⟩∼⟨V 2⟩/R2,
“Ohm’s law.” Equation (3.6) implies a connection between R and the strength of fluctuations:
R = 1
2kT  ∞
−∞
K(s)ds. (3.7)
Equation (3.7) is an instance of a general result, the fluctuation-dissipation theorem, which relates
dissipation6 (associated with entropy creation) to the temporal properties of fluctuations. Wherever
there is damping there are fluctuations, and conversely. Indeed, dissipation is essential to maintain￾ing thermal equilibrium. We return to this fundamental result in Chapter 6.
6Dissipated energy is energy diverted by the production of entropy into a form not available for work[5, p14].64 ■ Non-Equilibrium Statistical Mechanics
3.1.2 Brownian motion of free particles; the random force
Modeling Brownian motion requires us to give up on exact descriptions of motion in favor of
stochastic treatments. With that said, consider how one might approach the problem. A particle
of mass M (with M much larger than molecular masses, see Exercise 3.14), injected into a fluid
with velocity7 v0, experiences, by the laws of hydrodynamics, a velocity-dependent drag force
Fd = −αv, with α a friction coefficient. By Stokes’s law, α = 6πRν for a sphere of radius R
moving slowly in a fluid of viscosity ν[15, p66]. From Newton’s second law, thereore,
M dv
dt = −αv, (macroscopic) (3.8)
implying v(t) = v0 exp(−αt/M) for t ≥ 0; τc ≡ M/α is a characteristic time for dissipation of
kinetic energy (the relaxation time). This naive application of classical dynamics fails to capture an
essential part of the physics: The model predicts the velocity decaying to zero, yet Brownian motion
is incessant, and, from statistical mechanics, ⟨v2⟩ = kT /M (equipartition theorem[5, p97]). How
to do better? Clearly, take into account more of the interactions with the suspension medium. Yet
we’re unable to model the detailed motions of fluid molecules, whose impacts with the B-particle
are frequent and of irregular strength and direction. An approach introduced in 1908 by P. Langevin
obviates that problem by postulating a random force F(t) representing the effects of collisions.8 We
append Newton’s law of motion:
M dv
dt = −αv  systematic
force
+ F(t)  random
force
. (microscopic) (3.9)
In the parlance of the Langevin equation, one speaks of systematic forces, those affecting the average
motion and the random force.9 B-particles not subject to systematic forces other than the drag force
are known as free Brownian particles.
10 The motion of free B-particles is inertial for short times
(t ≪ M/α); see Eq. (3.18).
In that it models effects of microscopic properties, we can say the Langevin equation is micro￾scopic. We can’t say anything, however, about the random force other than specifying its statistical
properties. With the systematic force governing the average force experienced by B-particles, the
average of the random force must be zero over a subensemble specified by the initial condition
v = v0,
⟨F(t)⟩v0 = 0. (3.10)
Equation (3.10) embodies the key assumption that the random force is independent of macroscopic
initial conditions.
11 Equation (3.10) implies, from Eq. (3.9), that the average velocity decays to
zero,
⟨v(t)⟩v0 = v0 exp(−t/τc). (3.11)
Whatever the initial momentum of the B-particle, it’s transferred to the molecules of the suspension
medium, and the average velocity decays to zero.
More information is contained in the second moment, ⟨v2(t)⟩v0 , the average of a positive quan￾tity. The random force is stationary, and thus ⟨F(t1)F(t2)⟩ = K(t1 − t2). We expect there to be
7We restrict ourselves to one-dimensional descriptions, appropriate for isotropic systems. The generalization to three
dimensions is not difficult; see Exercise 3.6. We ignore the effects of gravity here. See Section 3.4.5. 8An English translation of Langevin’s 1908 paper appears in [63]. Our treatment follows that of Uhlenbeck and
Ornstein[64], reprinted in Wax[44, pp93–111]. 9The drag force is a dissipative systematic force supplied by the suspension medium, a source of irreversibility present
in any treatment of Brownian motion; it follows from hydrodynamics and is macroscopic in character. 10Analogous to free fall in gravitational physics, motion with no acceleration other than that provided by gravity[6, p188]. 11Said differently, the properties of the surrounding medium are unaffected by the motion of the B-particle; the suspen￾sion medium is a stationary environment. A justification of this key assumption is given in Section 6.5.Brownian motion and stochastic dynamics ■ 65
almost no correlation between impacts at different times,12 and thus K(τ ) is expected to be sharply
peaked about τ = 0. Without doing more math, we have from Eq. (3.5) under the substitutions
I → v, L → M, and R → α,
⟨v(t)v(t + τ )⟩v0 = v2
0e−(2t+τ)/τc + τc
2M2 e−τ/τc

1 − e−2t/τc

C, (3.12)
with the analog of Eq. (3.7), α = 1/(2kT)
 ∞
−∞ K(s)ds ≡ C/(2kT). What matters in the con￾nection between dissipation and the strength of fluctuations is the total area under the curve of the
correlation function K(s). One typically takes K(t−t
′
) as a delta function (delta-correlated noise),
⟨F(t)F(t
′
)⟩ = Γδ(t − t
′
) (3.13)
with
Γ=2kT α. (3.14)
Setting τ = 0 in Eq. (3.12), we find
⟨v2(t)⟩v0 = ⟨v2⟩eq + 
v2
0 − ⟨v2⟩eq
e−2t/τc , (3.15)
where ⟨v2⟩eq = kT /M, the thermal speed. Particles with v0 less than (greater than) kT /M
approach that value from below (above). For v2
0 = ⟨v2⟩eq, ⟨v2(t)⟩ is time invariant; systems in
equilibrium stay in equilibrium.13
To find ⟨x2(t)⟩, one could integrate the velocity, x(t) ∼  v(t)dt, square the result, and take the
average. A more direct way is to develop an equation of motion for ⟨x2(t)⟩. Multiply Eq. (3.9) by
x(t) (the instantaneous position) and use d(xx˙)/dt = xx¨+ ˙x2, xx˙ = 1
2 dx2/dt, and that ⟨xF(t)⟩ =
0. We find:
d2
dt2 ⟨x2(t)⟩ +
1
τc
d
dt
⟨x2(t)⟩ = 2⟨v2(t)⟩. (3.16)
The solution of Eq. (3.16) when Eq. (3.15) for ⟨v2(t)⟩v0 is substituted on the right side, is
⟨x2(t)⟩v0 = v2
0τ 2
c

1 − e−t/τc
2
+ ⟨v2⟩eqτ 2
c

2 t
τc
−

1 − e−t/τc
 3 − e−t/τc

, (3.17)
for the initial conditions ⟨x2⟩|t=0 = 0 and d⟨x2⟩/dt|t=0 = 0. For short times, Eq. (3.17) implies
⟨x2(t)⟩v0 = v2
0t
2 + O(t/τc)
3, t ≪ τc (3.18)
that B-particles behave initially as free particles moving at constant speed, and for long times
⟨x2(t)⟩v0 ∼ 2⟨v2⟩eqτct ≡ 2Dt, t ≫ τc (3.19)
that B-particles behave like diffusing particles executing a random walk.
The Langevin equation leads to a picture of Brownian motion as inertial at short times and
diffusive at long times. In diffusion, ⟨x2⟩ = 2Dt [Eq. (2.62)], where D = a2/(2τ ) [Eq. (2.59)]
involves the random-walk parameters a and τ . From Eq. (3.19) we identify a diffusion coefficient
for Brownian motion, D = ⟨v2⟩eqτc = kT /α. The “jump length” a (from the random walk) can be
estimated from Brownian-motion parameters, a ≈ τc
2⟨v2⟩eq. The formula
D = kT
α
(3.20)
is known as Einstein’s relation. Perrin combined Eq. (3.20) with his measurements of ⟨x2(t)⟩ for
particles executing Brownian motion to infer reasonably good values of the Boltzmann constant.14
12We expect, as solutions of the Langevin equation, that v(t) and v(t + ∆t) should differ infinitesimally for small ∆t,
and hence there must be almost no correlation between F(t) and F(t + ∆t). 13Thus, thermal equilibrium of B-particles at temperature T is achieved through numerous collisions with the particles
of the suspension medium that are themselves in equilibrium at temperature T. 14See Exercise 3.13. One compares the measured values of ⟨x2(t)⟩/(2t) with Eq. (3.20), limt→∞⟨x2(t)⟩/(2t) =
D = kT /α. Clearly one needs an independent measurement of the friction coefficient, α.66 ■ Non-Equilibrium Statistical Mechanics
3.1.3 Uniform external force; drift speed
Consider a B-particle of charge q in a uniform15 electric field E. Adding the Coulomb force as
another systematic force,
M dv
dt = qE − αv + F(t), (3.21)
where v is the velocity in the direction selected by E. In steady state16 d⟨v(t)⟩/dt = 0 and thus
0 = qE − α⟨v⟩d, (3.22)
where ⟨v⟩d, the drift speed, is the terminal speed attained by charged particles in a resistive medium.
Experimentally, ⟨v⟩d is linearly proportional to the electric field, ⟨v⟩d = µE (for E not too large),
where µ, the electrical mobility, is a material-specific parameter. From Eq. (3.22), µ = q/α; mo￾bility is inversely related to friction. With α = q/µ in Eq. (3.20), we have the Einstein relation for
charged particles
D = (kT /q) µ. (3.23)
Equation (3.23) shows a universal connection between transport coefficients,17 a result to be ex￾plained by nonequilibrium statistical mechanics; see Chapter 6. Equation (3.23) specifies a voltage
(energy per charge), the thermal voltage ≈ 26 mV at room temperature.
3.2 LANGEVIN EQUATION AS A STOCHASTIC PROCESS
The Langevin equation finds wide use in applications and is a successful theory by any measure.18
There’s a point of mathematical consistency, however, that should be addressed: Solutions of the
Langevin equation are continuous but not differentiable.
19 That creates a logical predicament. The
Langevin equation, involving the derivative dv/dt, is used to find a solution v(t) not having a
derivative! Physical theories can often be justified only a posteriori, by the validity of the conclu￾sions derived from them. Given the success of the Langevin equation, we shouldn’t be surprised if
another mathematical formulation were to be found that circumvents this problem.
Doob did that by interpreting the Langevin equation as a stochastic process. Start by multiplying
Eq. (3.9) by dt:
dv(t) = −βv(t)dt + A(t)dt, (dt > 0) (3.24)
where β ≡ α/M and A(t) ≡ F(t)/M. Equation (3.24) indicates that a small change in velocity
is produced by the frictional force acting over the next dt seconds (β has dimension time−1) and
by a random increment A(t)dt, which occurs with a distribution of values, positive and negative.
There are two times scales: time intervals over which variations in velocity v(t) are small, yet
which, over the same interval, A(t) undergoes numerous fluctuations, i.e., A(t) varies extremely
rapidly compared to variations in v(t).
20 Doob interpreted A(t)dt as a stochastic process, writing
A(t)dt ≡ dB(t), where the notation dB(t) for a differential stochastic process must be explained.
15Uniform force fields are a convenient idealization. We take up spatially dependent forces in Section 3.4. 16Steady state, where time derivatives vanish, is not necessarily the same as equilibrium. See Section 1.9. 17Historically, the Einstein relation provided the first clear way to experimentally verify our picture of Brownian motion
as due to thermal agitations of molecules, a picture which may seem obvious today, but was not in 1905. The history of
Brownian motion is recounted in Mazo[65]. 18Coffey[66] summarizes applications of Brownian motion theory. See also Risken[67]. 19See Doob[59], reprinted in Wax[44, pp319–337]. In the Einstein theory, the derivative v = dx/dt doesn’t exist if
⟨x2⟩ = 2Dt holds for t → 0, an issue known to Einstein; see Exercise 3.11. With the Langevin equation, ⟨x2⟩ ∝ t2 at
short times, Eq. (P3.8), implying the existence of dx/dt. The variance of the velocity, however, is ∝ |t| for short times
(Equation (1.1.9) in [59]), implying that the derivative dv/dt doesn’t exist (contrary to what was assumed at the outset). 20The time between successive collisions of the B-particle with molecules of the suspension medium is truly microscopic,
of order 10−18 sec; see Exercise 3.1. See [68] for observations of the velocity of a Brownian particle at µs time scales.Brownian motion and stochastic dynamics ■ 67
Doob wrote the Langevin equation as a relation among differentials (one in which the existence
of the limit dv/dt is not assumed),
dv(t) = −βv(t)dt + dB(t). (dt > 0) (3.25)
The formal solution of Eq. (3.25) is
v(t)=e−βt 
v0 +
 t
0
eβt′
dB(t
′
)

. (3.26)
The integral in Eq. (3.26), a Stieltjes integral (not typically included in science curricula), is the
limit of approximating sums,21
 t
0
eβydB(y) = limn→∞
n
−1
m=0
eβm∆t
 (m+1)∆t
m∆t
dB = limn→∞
n
−1
m=0
eβm∆t [B((m + 1)∆t) − B(m∆t)] ,
where we’ve divided (0, t) into subintervals, with n∆t = t. Thus,
B((m + 1)∆t) − B(m∆t) =  (m+1)∆t
m∆t
dB =
 (m+1)∆t
m∆t
A(t
′
)dt
′
.
The difference B((m + 1)∆t) − B(m∆t) is the change in velocity of the B-particle over the time
interval m∆t → (m + 1)∆t; dB(t) indicates the change in velocity for small ∆t at time t.
Doob took B(t) to be a process with independent increments (also called a differential process),
one in which, for t1 < ··· < tn, the differences {B(t2) − B(t1),...,B(tn) − B(tn−1)} are
a family of mutually independent random variables[36, Chapter 8]. Such a process is Markov:22
B(ti+1) − B(ti) is independent of B(t) for t<ti. The process is assumed stationary in the sense
that the probability distribution of the differences B(t + s) − B(s) is independent of s. As such, the
first moment is constant, taken to be zero,
⟨B(t) − B(s)⟩ = 0. (3.27)
On physical grounds we take ⟨B(t)⟩ = 0. The variance of processes with these properties (stationary
independent increments and ⟨B(t)⟩ = 0), is proportional to time (see Exercise 3.15),
⟨[B(t) − B(s)]2⟩ = γ|t − s|, (3.28)
where γ is a positive constant determined by the physics of the problem. The Wiener-Levy process
(see Section 2.7.3) meets these requirements, implying a Gaussian distribution of B(t)−B(0) with
zero mean and variance γt.
For f(t) a continuous function, the integral  f(t)dB(t) is well defined even though B(t) is not
of bounded variation.23 For any function f(t) continuous on [a, b], we have from Eq. (3.25),
 b
a
f(t)dv(t) = −β
 b
a
f(t)v(t)dt +
 b
a
f(t)dB(t). (3.29)
21The Stieltjes integral is a generalization of the Riemann integral. For [a, b] an interval of the real line with a ≡
x0 ≤ x1 ≤ ··· ≤ xn ≤ xn+1 ≡ b, and for f(x) bounded on [a, b], the Riemann integral is the limit of the sums
Rn ≡ f(ξ0)(x1 − a) + f(ξ1)(x2 − x1) + ··· + f(ξn)(b − xn), xr ≤ ξr ≤ xr+1, denoted  b
a f(x)dx. For f and g
bounded functions on [a, b], form the sum Sn ≡ f(ξ0)[g(x1)−g(a)]+f(ξ1)[g(x2)−g(x1)]+···+f(ξn)[g(b)−g(xn)],
with ξr chosen as previously. The limit, the Stieltjes integral, is denoted  b
x=a f(x)dg(x), or simply  b
a fdg. Nothing else
in this book relies on the properties of Stieltjes integrals. 22The converse is not true, however. Markov processes are not necessarily independent-increment stochastic processes.
Processes with independent increments form a subset of Markov processes. 23The existence of the Stieltjes integral  fdg requires that g(x) be bounded; see [69, p26]. Why that requirement can
be relaxed for stochastic processes is treated in Paley and Wiener[70, pp151–157].68 ■ Non-Equilibrium Statistical Mechanics
Taking f(t)=eβt (an integrating factor; see Exercise 3.16), Eq. (3.29) reproduces Eq. (3.26) when
we set a = 0, b = t, and integrate by parts.24 Equation (3.26) is then the complete solution of the
Langevin equation,25 a conclusion reached without positing the existence of dv/dt. Doob showed
further (Theorem 3 in [59]) that only when the differential process B(t) − B(0) has a Gaussian
distribution of zero mean and variance (2βkT t/M) are 1) the solutions v(t) continuous and 2)
only then is the Maxwell-Boltzmann distribution (see Section 4.6.2) for v(t) attained as t → ∞ for
any initial condition v(0). Through Doob’s analysis we gain a deeper insight into the nature of the
random force. The upshot, however, is that we can use the Langevin equation as originally stated in
Eq. (3.9) knowing that a rigorous approach leads to a solution, Eq. (3.26), the form of which is also
obtained through the naive assumption that dv/dt exists.
3.3 FOKKER-PLANCK EQUATION FOR ONE RANDOM VARIABLE
The Langevin equation supplies expressions for quantities such as ⟨v(t)⟩v0 , how the average speed
of a B-particle evolves in a subensemble specified by initial condition v0. One might wonder if,
instead of solving a differential equation for observables like ⟨v(t)⟩v0 , the same time-dependent
average could be calculated as an expectation value with respect to a time-dependent probability,26
⟨v(t)⟩v0,t0 =
 ∞
−∞
vP(v, t|v0, t0)dv. (3.30)
The quantity P(v, t|v0, t0)dv we seek is a transition probability,
27 the probability the velocity lies
in [v, v + dv] at time t, given that it had the value v0 at time t0 < t. As we show, such a function is
obtained as the solution to a differential equation, the Fokker-Planck equation, (3.36).28 Assuming
no unique origin in time, P(v, t|v0, t0) depends only on the difference τ ≡ t − t0. We erase the
label t0 in Eq. (3.30) and change notation, P(v, t|v0, t0) → P(v, τ |v0). For simplicity we treat
P(v, τ |v0) as a Markov process,29 that the probability does not depend on the entire past history of
the particle, only on v = v0 at τ = 0. We assume a stationary Markov process.
3.3.1 Derivation
We’d like an equation analogous to (3.30) to hold for any ensemble average, not just for the velocity.
Let P(y, τ |y0) be the conditional probability of a random variable y and let f(y) be a smooth
function30 such that f(y) → 0 as y → ±∞. We want an equation like (3.30) to hold for any f,
⟨f(τ )⟩y0 =

f(y)P(y, τ |y0)dy. (3.31)
Note the generality—any random variable y and any function f(y). To isolate P(y, τ |y0), one might
try differentiating with respect to the parameter τ . As we show, a partial differential equation for
24Integration by parts is defined for Stieltjes integrals,  b
a gdf = [f g]
b
a −  b
a fdg.
25If Eq. (3.25) is true, it implies Eq. (3.26); conversely if v(t) is given by Eq. (3.26), it satisfies Eq. (3.25). 26The Langevin equation is the equation of motion for observables in the presence of a random force. One solves the
differential equation and then takes the ensemble average, where the average is over measurements made on an ensemble of
systems having the same initial condition. With the Fokker-Planck approach, ensemble averages are calculated with respect
to the transition probability P(v, t|v0, t0) describing the same ensemble. 27We’ve erased the subscripts from P1|1(v, t|v0, t0) in Section 2.4.3. 28The Fokker-Planck equation was introduced by A.D. Fokker in 1913 and M. Planck in 1917. It was independently
derived by Kolmogorov in 1931, where it’s known as the Kolmogorov forward equation. It’s also known as the Einstein￾Fokker-Planck equation, which appeared without fanfare in Einstein’s 1905 article on Brownian motion. 29Fokker-Planck equations for non-Markovian processes exist in the literature (see for example Adelman[71]), which
pertain to research problems beyond the scope of this book. 30Smooth function is generally a code word for “all derivatives exist.”Brownian motion and stochastic dynamics ■ 69
P(y, τ |y0) can be developed from the construct (x ≡ y0 to save writing)

dyf(y) ∂
∂τ P(y, τ |x) = lim ∆τ→0
1
∆τ

dyf(y) [P(y, τ + ∆τ |x) − P(y, τ |x)] (3.32)
= lim
∆τ→0
1
∆τ

dyf(y)

dzP(y, ∆τ |z)P(z, τ |x)

− P(y, τ |x)

,
where we’ve used the SCK equation, the hallmark of Markov processes. Expand f(y) on the right
side Eq. (3.32) to second order in a Taylor series, f(y) ≈ f(z)+(y − z)f′
(z) + 1
2 (y − z)2f′′(z)
and interchange the order of integration:

dyf(y) ∂
∂τ P(y, τ |x) = lim ∆τ→0
1
∆τ
 
dzf(z)P(z, τ |x)

dyP(y, ∆τ |z)
+

dzf′
(z)P(z, τ |x)

dy(y − z)P(y, ∆τ |z)
+
1
2

dzf′′(z)P(z, τ |x)

dy(y − z)
2P(y, ∆τ |z)
−

dzf(z)P(z, τ |x)

. (3.33)
The first and last terms in Eq. (3.33) cancel (by the rules for conditional probabilities, see page 25).
Equation (3.33) involves a family of integrals of the form  dy(y−z)nP(y, ∆τ |z) (n = 1, 2,...),31
known as transition moments, which (keep in mind) we need evaluate only for small ∆τ . We assume
for small ∆τ , 
dy(y − z)P(y, ∆τ |z) ≡ M1(z)∆τ + O (∆τ )
2

dy(y − z)
2P(y, ∆τ |z) ≡ M2(z)∆τ + O (∆τ )
2

dy(y − z)
nP(y, ∆τ |z) ≡ Mn(z)(∆τ )
2. (n ≥ 3) (3.34)
All such integrals vanish as ∆τ → 0 (for n ≥ 1);32 the question is how they vanish for small ∆τ .
It’s borne out in physical theories that transition moments for n ≥ 3 vanish at least as fast as (∆τ )2
(an assumption to be checked at the end of a calculation—see Exercise 3.25). Combine Eqs. (3.34)
and (3.33), take the limit ∆τ → 0, and integrate by parts,

dyf(y) ∂
∂τ P(y, τ |x) = 
dzf(z)

− ∂
∂z [M1(z)P(z, τ |x)] + 1
2
∂2
∂z2 [M2(z)P(z, τ |x)]
,
which, with a trivial change of variables, leads to

dyf(y)
∂P(y, τ |x)
∂τ +
∂
∂y [M1(y)P(y, τ |x)] − 1
2
∂2
∂y2 [M2(y)P(y, τ |x]

= 0. (3.35)
Because Eq. (3.35) holds for any function f(y), it implies the Fokker-Planck equation33,34
∂
∂τ P(y, τ |x) = − ∂
∂y [M1(y)P(y, τ |x)] + 1
2
∂2
∂y2 [M2(y)P(y, τ |x)] . (3.36)
31We keep transition moments for n = 1, 2 in Eq. (3.33) because we have limited the expansion of f(y) to two term in
it’s Taylor series, which suffices for many physical applications. More terms could be included if need be. 32Lim∆τ→0P(y, ∆τ|z) = δ(y −z); lim∆τ→0
 dy(y −z)nP(y, ∆τ|z) =  dy(y −z)nδ(y −z)=0 for n ≥ 1. 33 
The passage from Eq. (3.35) to (3.36) is a familiar step in theoretical physics. It relies on a theorem that if
η(x)ϕ(x)dx = 0 holds for all reasonable functions η(x), then ϕ(x)=0 identically[72, p185]. 34The Fokker-Planck equation has an extensive literature. Books have been written about it; see Risken[67] for example.70 ■ Non-Equilibrium Statistical Mechanics
Equation (3.36) involves the transition probability P(y, τ |x), but the same form applies to
P1(y, t) =  dxP(y, t − t0|x)P1(x, t0). Equation (3.36) is equivalent to:
∂
∂τ P1(y, τ ) = − ∂
∂y [M1(y)P1(y, τ )] + 1
2
∂2
∂y2 [M2(y)P1(y, τ )] . (3.37)
Thus there are two versions of the Fokker-Planck equation: Eq. (3.36), involving the transition
probability P(y, τ |x) or Eq. (3.37) involving the probability P1(y, t). Either is referred to as the
Fokker-Planck equation; either can be used depending on the problem.
The Fokker-Planck equation is a partial differential equation for time-dependent probabilities. It
provides for the time rate of change of probability in terms of derivatives with respect to y; it models
the flow of probability in the space of random variables.
35 In that regard, one might wonder if there’s
a connection with the master equation, which also provides for a time derivative of probability. Such
is indeed the case; see Exercise 3.21. Equation (3.37) has the form of a continuity equation for
probability (erasing the subscript on P1(y, τ )),
∂
∂τ P(y, τ ) + ∂
∂y J(y, τ )=0, (3.38)
where the probability current density is
J(y, τ ) = M1(y)P(y, τ ) − 1
2
∂
∂y [M2(y)P(y, τ )] . (3.39)
There are two (and only two) types of probability flows: drift (or convection), associated with M1,
and diffusion, associated with M2. Whereas M2 > 0 [see Eq. (3.34)], M1 can be of either sign;
M1 probes the asymmetry of the transition probability; M1 ̸= 0 only if P(y, ∆τ |z) is not an even
function of y about z for small ∆t.
3.3.2 Free Brownian particles in spatially homogeneous systems
Equation (3.37) becomes an effective differential equation for determining P1(y, t) only when the
transition moments have been specified. For free Brownian motion, where the random variable is
the scalar velocity of the B-particle, we find the transition moments from the associated Langevin
equation. The first moment involves the average deviation of v from its initial value v0 in the next
∆t seconds (from Eq. (3.34)),36
M1∆t =

(v − v0)P(v, ∆t|v0)dv = ⟨v(∆t)⟩v0 − v0 = −v
α
M
∆t + O(∆t)
2,
implying for ∆t → 0,
M1 = − α
M v, (3.40)
where we’ve used Eq. (3.11). M1 < 0 because the average speed ⟨v(t)⟩v0 < v0 as a result of
collisions. One can show that (see Exercise 3.22)
M2 = 2kTα
M2 . (3.41)
Thus we have the Fokker-Planck equation for free Brownian motion,
∂
∂tP(v, t) = α
M
∂
∂v [vP(v, t)] + kTα
M2
∂2
∂v2 P(v, t). (3.42)
35It applies to processes where the jump length (see Exercise 3.21) is small enough that the methods of calculus can be
applied. This is justified for Brownian motion where the mass of the B-particle is much larger than molecular masses. 36The first transition moment is sometimes written M1∆t = ⟨∆v(∆t)⟩, which can be misleading: ⟨∆v⟩ vanishes in an
equilibrium ensemble average. The transition moments refer to nonequilibrium ensemble averages at short times.Brownian motion and stochastic dynamics ■ 71
Let’s find the stationary solution, which we expect to be the equilibrium probability distribution.
With ∂P/∂t = 0, we see from Eq. (3.38) that J = constant. The “boundary condition” J → 0 as
v → ∞ implies J = 0 for all v in steady state. Combining Eqs. (3.40), (3.41) with J = 0 in Eq.
(3.39), the Fokker-Planck theory leads us to a differential equation for the stationary solution Pst(v),

v +
kT
M
∂
∂v 
Pst(v)=0, (3.43)
which is solved by elementary means (K is a normalization constant),
Pst(v) = Ke−Mv2/(2kT) =
 M
2πkT e−Mv2/(2kT)
, (3.44)
none other than the Maxwell-Boltzmann distribution.37 At long times, the B-particle has a distribu￾tion of speeds characteristic of thermal equilibrium at absolute temperature T.
The non-stationary solution is more difficult because we have to build in the initial condition.
For that purpose work with the transition probability, for which P(y, 0|x) = δ(y − x). Write the
equation in terms of a dimensionless time τ ≡ βt, where β ≡ α/M and x ≡ v0:
∂
∂τ P(v, τ |x) = ∂
∂v [vP(v, τ |x)] + kT
M
∂2
∂v2 P(v, τ |x). (3.45)
There is no one way to solve differential equations. With that said, we Fourier transform with respect
to v, P(q, τ |x) ≡  ∞
−∞ P(v, τ |x)e−iqvdv. Multiply Eq. (3.45) by e−iqv and integrate over v, which,
using the results of Exercise 3.23, is equivalent to the first-order partial differential equation,38
 ∂
∂τ + q
∂
∂q 
P(q, τ |x) = −kT
M q2P(q, τ |x), (3.46)
which can be solved by the method of characteristics.39 The general solution to Eq. (3.46) is of the
form (as can be verified)
P(q, τ |x) = F(qe−τ )e−kT q2/(2M)
, (3.47)
where F is an arbitrary differentiable function. Choose F so that the initial condition is met. With
P(v, 0|x) = δ(v − x), P(q, 0|x)=e−iqx, implying F(q)=e−iqxekT q2/(2M)
. Thus,
P(q, τ |x) = exp 
−iqxe−τ − (kT/2M)q2 
1 − e−2τ  , (3.48)
a Gaussian in the variable q (see Section 2.9.1). To find P(v, τ |x), we must evaluate the inverse
Fourier transform, a calculation we’ve already done in Exercise 2.5. We write down the final result
for the transition probability associated with Brownian motion (τ ≡ αt/M):
P(v, τ |v0) = 
M
2πkT(1 − e−2τ )
exp 
− M
2kT
(v − v0e−τ )2
1 − e−2τ

. (3.49)
It reproduces the correct the initial condition as τ → 0, limτ→0 P(v, τ |v0) = δ(v − v0) (see
Exercise 3.27). As τ → ∞ initial conditions are forgotten, and we recover the Maxwell-Boltzmann
distribution, Eq. (3.44). We examine the three-dimensional version of this formula in Eq. (3.90).
37The Maxwell-Boltzmann distribution is derived in [2, p105], in [5, p127], and in Section 4.6.2. 38That is, introducing an integral transform has reduced the order of the Fokker-Planck equation from two to one.
39Discussed in books on partial differential equations; see Sneddon[73] or Courant and Hilbert[74]. See also the last page
of Wang and Uhlenbeck[75], reprinted in Wax[44, pp113–132].72 ■ Non-Equilibrium Statistical Mechanics
3.3.3 Ornstein-Uhlenbeck process
Equation (3.49), the solution of the Fokker-Planck equation for free B-particles, has the form of the
transition probability in stationary Gaussian processes; see Eq. (2.108). Doob’s theorem (see Section
2.9.4.2), which applies to stationary Gaussian Markov processes,40 on the exponential decay of time
correlation functions, is satisfied. The stochastic process having Eq. (3.49) as its transition proba￾bility and P1 in the Gaussian form of Eq. (2.107) is known as the Ornstein-Uhlenbeck process.
41
It’s associated with a Fokker-Planck equation having a linear drift coefficient M1 = −γv (with
γ constant) and diffusion coefficient, D = kT γ/M. The Ornstein-Uhlenbeck process is the only
process that is Markov, Gaussian, and stationary,42 an equivalent way of stating Doob’s theorem.
3.4 BROWNIAN PARTICLES IN EXTERNAL FORCE FIELDS
The Fokker-Planck equation derived in Section 3.3.1 pertains to the probability density P1(y, t)
of a single random variable y, which we used in Section 3.3.2 to represent the velocity v of a
free Brownian particle. More generally, a particle’s position r, its vector velocity v, or both can
be taken as random variables. Spatial variables come into play for particles in force fields and for
spatially dependent initial conditions. In this section, we derive the Fokker-Planck equation for the
probability density P1(r, v, t) that at time t the B-particle is “at” the point in phase space (r, v).
We start by developing an integral equation43 for P1(r, v, t). To save writing, denote the condi￾tional probability density P1|1(r, v, t|r − ∆r, v − ∆v, t − ∆t) ≡ ψ∆t(r − ∆r, v − ∆v; ∆r, ∆v),
the probability that the particle at (r − ∆r, v − ∆v) makes the transition (∆r, ∆v) in the next ∆t
seconds. From the rules for conditional probabilities (see Section 2.3.1),
P1(r, v, t) = 
d(∆r)d(∆v)ψ∆t(r−∆r, v−∆v; ∆r, ∆v)P1(r−∆r, v−∆v, t−∆t), (3.50)
where d(∆v) ≡ d(∆v1)d(∆v2)d(∆v3), etc. Assume ∆t is sufficiently small that ∆r = v∆t is
accurate. We can enforce that connection with a Dirac delta function,
ψ∆t(r − ∆r, v − ∆v; ∆r, ∆v) ≡ δ(∆r − v∆t)ϕ∆t(r − ∆r, v − ∆v; ∆v), (3.51)
where ϕ∆t is a transition probability in velocity space. Combining Eq. (3.51) with Eq. (3.50) and
integrating over ∆r, we have
P(r + v∆t, v, t + ∆t) = 
d(∆v)ϕ∆t(r, v − ∆v; ∆v)P(r, v − ∆v, t), (3.52)
where we’ve erased the subscript on P1 and we’ve let r → r + v∆t and t → t + ∆t. Assume that
ϕ∆t is sharply peaked about ∆v = 0 (impacts with fluid molecules produce small changes in the
velocity of the more massive B-particle). Expand the quantities in Eq. (3.52) in Taylor series,
P(r + v∆t, v, t + ∆t) = P(r, v, t)+∆t

v ·
∂P
∂r
+
∂P
∂t 
+ O(∆v2, ∆v∆t, ∆t
2)
ϕ∆t(r, v − ∆v; ∆v) = ϕ∆t(r, v; ∆v) − ∆v ·
∂ϕ∆t
∂v
+
1
2
∂2ϕ∆t
∂v∂v :∆v∆v + O(∆v3)
P(r, v − ∆v, t) = P(r, v, t) − ∆v ·
∂P
∂v
+
1
2
∂2P
∂v∂v :∆v∆v + O(∆v3), (3.53)
40The Fokker-Planck equation was derived under the assumption of stationary Markov processes.
41After [64], reprinted in Wax[44, pp93–111]. 42There is one other, trivial, process having these properties—independent stochastic processes.
43In integral equations, the unknown function is inside an integral. See [13, Chapter 10] or Appendix D.Brownian motion and stochastic dynamics ■ 73
where we expand the terms inside the integral to second order and we’ve used dyadic notation for
tensor contraction (see Section 1.4.3). Use the expansions in Eq. (3.53) in Eq. (3.52) and work
consistently to second order in ∆v on the right side and to first order in ∆t on the left. We find
∂P
∂t + v ·
∂P
∂r

∆t ≈

d(∆v)

−∆v ·
∂(P ϕ∆t)
∂v
+
1
2
∂2(P ϕ∆t)
∂v∂v :∆v∆v

, (3.54)
where, for conditional probabilities,  d(∆v)ϕ∆t(v, r; ∆v)=1 (so that the zeroth order term
cancels across the equation). Equation (3.54) implies, in the limit ∆t → 0,
 ∂
∂t + v · ∇r

P(r, v, t) = − ∂
∂v ·

P(r, v, t)
⟨∆v⟩∆t
∆t

+
1
2
∂2
∂v∂v :

P(r, v, t)
⟨∆v∆v⟩∆t
∆t

,
(3.55)
where ⟨∆v⟩∆t ≡  d(∆v)∆vϕ∆t(r, v; ∆v), etc. Equation (3.55) is the generalization of Eq.
(3.37) to include a vector velocity, a spatial dependence r, and transition moments having a tensor
character (compare with Eq. (3.34)). It becomes an effective differential equation for determining
P(r, v, t) when the transition moments have been specified.
To do that, return to the Langevin equation in the form of Eq. (3.25),
∆v = −(βv − K)∆t + ∆B, (3.56)
where we include an acceleration field K(r). Inverting Eq. (3.56), ∆B = ∆v + (βv − K)∆t.
By the Doob theorem mentioned on page 68, ∆B has a Gaussian distribution of known mean and
variance, implying
ϕ∆t(r, v; ∆v) = 1
(2πσ2
0∆t)3/2 exp 
−[∆v + (βv − K(r))∆t]
2/(2σ2
0∆t)

, (3.57)
where σ2
0 ≡ 2βkT /M. It’s straightforward to show that
⟨∆v⟩∆t = − (βv − K) ∆t
⟨∆vi∆vj ⟩∆t = 2βkT
M δij∆t + O(∆t)
2. (3.58)
Combining the expressions in Eq. (3.58) with Eq. (3.55) and taking the limit ∆t → 0, we have44
d
dt
P(r, v, t) =  ∂
∂t + v · ∇r + K(r) · ∇v

P(r, v, t)
= β∇v ·

v +
kT
M ∇v

P(r, v, t) ≡ ΩP(r, v, t) (3.59)
where ∇r is the gradient operator in position space with ∇v that in velocity space, and where Ω is
the Fokker-Planck operator,
Ω ≡ β∇v ·

v +
kT
M ∇v

. (3.60)
The terms on the right side of Eq. (3.59) are associated with Brownian motion and those on the left
comprise the total time derivative, dP(r, v, t)/dt. Note that the left side changes sign under t → −t
but the right side does not—the hallmark of irreversible evolution equations. Equation (3.59) cannot
be solved in closed form, but it can be solved in special cases.
44Equation (3.59) is known as the Kramers equation or the Klein-Kramers equation; it was derived by O. Klein in 1922
and independently by Kramers[76] in 1940. The nomenclature is not standardized and can be confusing. The distinction
made by some authors seems to be that the name Fokker-Planck equation is reserved for free B-particles.74 ■ Non-Equilibrium Statistical Mechanics
3.4.1 The stationary solution
Referring to Eq. (3.59), the stationary solution depends on r and v, call it Pst(r, v). When the force
field is derivable from a potential energy function, K(r) = −(1/M)∇ψ(r), Pst(r, v) has the
Boltzmann form,
Pst(r, v)=(constant) e−ψ(r)/(kT)
e−Mv2/(2kT)
, (3.61)
see Exercise 3.29. Note that the stationary solution is independent of the friction parameter β.
3.4.2 Passage over potential barriers: The Kramers escape problem
The stationary solution comes into play in the problem considered by Kramers[76] of the passage
of particles over potential barriers as a consequence of Brownian motion (see Fig. 3.4). Particles in
x
ψ(x)
x0
ψ0
0
Figure 3.4 Potential energy function ψ(x) with a smooth barrier ψ0.
the potential well are subject to the force associated with the negative gradient of ψ(x), but also to
the friction and random force supplied by the environment. Some particles in the well escape over
the barrier as a result of Brownian motion. At what rate do particles arrive at the top of the barrier?
This model has applications in the study of chemical as well as nuclear reactions.
The barrier is assumed large relative to thermal energies, ψ0 ≫ kT, implying that quasi￾stationary conditions prevail in which time derivatives can be neglected. Near the bottom of the
well the probability distribution is given by Eq. (3.61). The equilibrium distribution, however, in
that it’s associated with zero current, can’t be valid near the potential barrier where particles escape
at a nonzero rate. We seek a steady-state solution valid in the vicinity of the barrier that supports a
nonzero current. The shape of ψ(x) at the top of the barrier can be fit to a downward facing parabola,
ψ(x) ≈ ψ0 − 1
2Mω2
b (x − x0)2 for x ≈ x0. Here ωb is a parameter with dimension frequency set by
the curvature at x = x0, ω2
b = (−1/M)ψ′′|x=x0 . No oscillations occur near the barrier.
We modify the stationary solution Eq. (3.61) by introducing an unknown multiplicative function
F(x, v) and where we use the form of the barrier ψ(x) = ψ0 − 1
2Mω2
b (x − x0)2,
P(x, v) = CF(x, v)e−ψ0/(kT) exp  M
2kT

ω2
b (x − x0)
2 − v2

, (3.62)
where C is a constant. The one-dimensional version of Eq. (3.59) without the time derivative (we
seek a stationary solution) and with K(x) = ω2
b (x − x0) is

v
∂
∂x + ω2
b (x − x0) ∂
∂v 
P = β ∂
∂v 
v +
kT
M
∂
∂v 
P. (3.63)Brownian motion and stochastic dynamics ■ 75
The function F is found by substituting Eq. (3.62) in Eq. (3.63); we find it must satisfy the differ￾ential equation
v
∂F
∂x + 
βv + ω2
b (x − x0)
 ∂F
∂v = β kT
M
∂2F
∂v2 . (3.64)
Let’s attempt a solution of Eq. (3.64) by assuming F to be a function of a single variable, F(x, v) =
f[v−α(x−x0)] ≡ f(z), where α is to be determined. With this substitution, Eq. (3.64) is equivalent
to

(β − α)v + ω2
b (x − x0)

f′ = βkT
M f′′. (3.65)
For this approach to work, the terms in square brackets must be a function of z only. Try something
linear; let
(β − α)v + ω2
b (x − x0) = λz = λ(v − α(x − x0)), (3.66)
where λ is to be determined. Equation (3.66) is satisfied with λ = β − α and λα = −ω2
b , implying
the quadratic equation λ2 − βλ − ω2
b = 0 having roots λ± = 1
2 (β ± β2 + 4ω2
b ). Note that
λ+ + λ− = β and λ+λ− = −ω2
b . For either λ = λ±, Eq. (3.65) is equivalent to
λzf′
(z) = βkT
M f′′(z), (3.67)
which is readily integrated (A and B are constants),
f(z) = A + B
 z
0
dy exp  g
2
y2

, (3.68)
where g ≡ Mλ/(βkT).
The root λ+ is positive and λ− is negative. Which should we choose, or do we work with both?
What are the boundary conditions on f(z)? Referring to Fig. 3.4, we expect F(x, v) to vanish as
x → ∞ for v < 0. With z ≡ v − α(x − x0), if α is positive, we have the boundary condition
f(z) → 0 as z → −∞. To have α > 0, choose the negative root, λ = λ− = −ω2
b /λ+, implying
α = λ+ and g = −Mω2
b /(βkTλ+). Setting A = 0 in Eq. (3.68) and setting the lower limit to −∞,
f(z) = B
 z
−∞
exp 
− Mω2
b
2βkTλ+
y2

dy. (3.69)
For the other boundary condition, we expect that to the left of the barrier (see Fig. 3.4) F(x, v) → 1
as x → −∞; thus f(z) → 1 as z → ∞. This boundary condition determines B (set z = ∞ in Eq.
(3.69)), implying B = Mω2
b /(2πβkTλ+). The solution of Eq. (3.67) meeting these boundary
conditions is
f(z) = 
Mω2
b
2πβkTλ+
 z
−∞
exp 
− Mω2
b
2βkTλ+
y2

dy. (3.70)
Combining Eq. (3.70) with Eq. (3.62), we have a stationary solution valid for x ≈ x0,
P(x, v) = C
 Mω2
b
2πβkTλ+
e
−ψ0/(kT ) exp  M
2kT (ω2
b (x − x0)
2 − v2
)
  z
−∞
dy exp 
− Mω2
b
2βkTλ+
y2

,
(3.71)
where z = v−λ+(x−x0), with λ+ = 1
2 (β+
β2 + 4ω2
b ). To evaluate the constant C, consider the
stationary solution Eq. (3.61) in the vicinity of the potential well at x ≈ 0, where ψ(x) = 1
2Mω2
0x2,
with ω0 characterizing the curvature, ω0 = (1/M)ψ′′|x=0. Thus for x ≈ 0,
P(x, v) = C exp 
− M
2kT (v2 + ω2
0x2)

. (3.72)
By normalizing, with  ∞
−∞
 ∞
−∞ P(x, v)dxdv = 1, we find C = Mω0/(2πkT).76 ■ Non-Equilibrium Statistical Mechanics
The net diffusion current at the top of the barrier, x = x0, is found from
J =
 ∞
−∞
P(x = x0, v)vdv. (3.73)
Using Eq. (3.71),
J = C

Mω2
b
2πβkTλ+
e−ψ0/(kT)
 ∞
−∞
dv ve−Mv2/(2kT)
 v
−∞
exp 
− Mω2
b
2βkTλ+
y2

dy. (3.74)
After integrating by parts, we have
J = C

Mω2
b
2πβkTλ+
e−ψ0/(kT)
kT
M
  ∞
−∞
exp 
−Mλ+
2βkT v2

= ω0ωb
π(β + β2 + 4ω2
0)
e−ψ0/(kT)
. (3.75)
Equation (3.75) is an example of a thermally activated process, with activation energy ψ0.
3.4.3 The field-free case with spatially dependent initial conditions
Free Brownian motion is an important case that we better get right. With K = 0 in Eq. (3.59),
 ∂
∂t + v · ∇r

P(r, v, t) = β∇v ·

v +
kT
M ∇v

P(r, v, t). (3.76)
As shown in Exercise 3.30, probability is conserved in free Brownian motion (satisfies a continuity
equation) and the drag force is reproduced by the solution, an important consistency check, as it was
assumed at the outset. Under the banner of free Brownian motion, we consider separately the cases
of whether or not a spatial dependence is required.
3.4.3.1 Field free, no spatial dependence
When P is independent of position, the field-free Fokker-Planck equation has the form
∂
∂tP(v, t) = β∇v ·

v +
kT
M ∇v

P(v, t) ≡ ΩP(v, t), (3.77)
which as a class of problems (time dependence generated by an operator) would suggest finding
the eigenvalues and eigenfunctions of Ω. The operator Ω, however, is not self-adjoint (see Exercise
3.31), which 1) complicates the analysis and 2) reflects the irreversible nature of the time evolution
of nonequilibrium systems. Consider that in the matrix representation of Ω, its matrix elements
Ωm,n ≡ ⟨ϕm|Ωϕn⟩ (in an orthonormal basis45) are not symmetric; we have in general Ωm,n = 
Ω†
n,m∗ and if Ω† ̸= Ω, its matrix elements cannot be symmetric or Hermitian. We see from Eq.
(3.77) that Ω involves a divergence of probability currents which represent transitions in velocity
space. As shown in Exercise 3.21, the Fokker-Planck equation is equivalent to the master equation,
where we encounter a similar issue that the transition matrix Wn,n′ need not be symmetric (see
Section 2.8); in a nonequilibrium system the rate of the transition n → n′ need not be the same as
for n′ → n. We found, however, a similarity transformation of the transition matrix utilizing the
45Sometimes students forget that matrix elements of operators are defined with respect to orthonormal bases. There’s no
loss of generality; any basis is associated with an orthonormal basis through the Gram-Schmidt process[13, p12].Brownian motion and stochastic dynamics ■ 77
detailed balance condition, Eq. (2.79). Does detailed balance pertain to the Fokker-Planck equation;
is there a regime in which all transitions are balanced? Yes, when the probability current J = 0,
velocity transitions in one direction are balanced by those in the opposite, a type of detailed balance
condition. The Maxwellian function ϕ(v) ≡ exp(−Mv2/(2kT)) is associated with zero current:

v +
kT
M ∇v

ϕ(v)=0.
These considerations suggest that to put Ω in self-adjoint form, we apply the same transformation
as in Eq. (2.81) using ϕ(v) as the stationary distribution.
Thus, starting with the eigenvalue problem associated with Ω,
ΩF(v) = λF(v), (3.78)
introduce the similarity transformation

1
ϕ(v)
Ω
ϕ(v)
  F(v)
ϕ(v)

= λ

F(v)
ϕ(v)

, (3.79)
which we symbolize
ΩF(v) = λF(v). (3.80)
It’s straightforward to show that
Ω = β
kT
M ∇2
v − M
4kT v2 +
3
2

, (3.81)
which is self-adjoint.
With Ω as in Eq. (3.81), Eq. (3.80) has the form of the time-independent Schrodinger equation ¨
for the three-dimensional isotropic harmonic oscillator (see Exercise 3.33), the solutions of which
are well known. Using Eq. (P3.20) for F(v), with F(v) = ϕ(v)F(v), the eigenfunctions of Ω
are labeled by three non-negative integers n1, n2, n3,
Fn1,n2,n3 (v) = An1,n2,n3 exp 
−Mv2/(2kT)

Hn1 (v1/α)Hn2 (v2/α)Hn3 (v3/α), (3.82)
where α = 2kT /M, An1,n2,n3 is a constant, and the functions Hn are Hermite polynomials.
The associated eigenvalues λn1,n2,n3 = −β(n1 + n2 + n3). For simplicity we refer to the integers
n1, n2, n3 collectively as n, and we write Eq. (3.82) in compressed form
Fn(v) = An exp 
−Mv2/(2kT)

Hn

v/
2kT /M
. (3.83)
Equation (3.83) is the same as Eq. (3.82). Hermite polynomials have the orthogonality property46
 ∞
−∞
e−x2
Hn(x)Hm(x)dx = √π2nn!δn,m. (3.84)
When we choose
An1,n2,n3 =

2n1+n2+n3 n1!n2!n3! (2πkT /M)
3/2
−1/2
,
the eigenfunctions Fn have the orthonormality relation,47

d3v
1
ϕ(v)
Fn(v)Fm(v) = δn,m. (3.85)
Compare Eq. (3.85) with Eq. (2.82).
46Dennery and Krzywicki[77] is a good source on orthogonal polynomials, including the Hermite polynomials. We’re
using the “physicist” Hermite polynomials Hn(x), and not that of the “probabilists,” Hen(x). 47The functions Fn already have a Maxwellian built into them. To match up to the orthogonality condition Eq. (3.84),
we have to divide by ϕ(v).78 ■ Non-Equilibrium Statistical Mechanics
Armed with the eigenvalues and eigenfunctions of Ω, we have the solution of the Fokker-Planck
equation (3.77):
P(v, t) = 
n
CnFn(v) exp(λnt), (3.86)
where the expansion coefficients are obtained from the initial condition
Cn =

d3v
1
ϕ(v)
Fn(v)P(v, 0). (3.87)
For a particle known to have velocity v0 at time t = 0, P(v, 0) = δ(v − v0), implying that
Cn = Fn(v0)/ϕ(v0). In that case, we can write Eq. (3.86) as
P(v, t|v0) = 1
ϕ(v0)

n
Fn(v0)Fn(v) exp(λnt). (3.88)
The series in Eq. (3.88) can be summed using a property of Hermite polynomials, Mehler’s formula,
that for |z| < 1,
48
∞
n=0
z
2
n 1
n!
Hn(x)Hn(y) = 1
√1 − z2 exp 2xyz − z2(x2 + y2)
1 − z2

. (3.89)
Using Eq. (3.89) in Eq. (3.88), we find
P(v, t|v0) =  M
2πkT(1 − e−2βt)
3/2
exp 
− M
2kT(1 − e−2βt)

v − v0e−βt2

, (3.90)
a three-dimensional version of the Ornstein-Uhlenbeck process (compare with Eq. (3.49)). We pre￾viously found the Ornstein-Uhlenbeck transition probability using the method of characteristics
(see Section 3.3.2), a way of solving first-order partial differential equations. Here we’ve solved the
problem by finding the eigenvalues and eigenfunctions of the Fokker-Planck operator.
3.4.3.2 Field free, spatially dependent
A spatial dependence of the nonequilibrium probability distribution can occur when a particle’s
initial position is specified as well as its initial velocity. The theoretical treatment of such systems
builds on our previous analysis. Start with a Fourier transform with respect to spatial variables,
P(k, v, t) ≡

d3x exp(ik · r)P(r, v, t). (3.91)
Using Eq. (3.76) (the relevant Fokker-Planck equation), it’s straightforward to show that
∂
∂tP(k, v, t) = (Ω + ik · v) P(k, v, t). (3.92)
Thus we have an equation like Eq. (3.77) but with a modified operator, Ω → Ω′ =Ω+ik · v. We
write the associated eigenvalue problem
(Ω + ik · v) F′ = λ′
F′
, (3.93)
48Mehler’s formula is not a standard topic, even though it was published in 1866 (Szego[78] cites the original reference).
Rainville[79, p197] supplies a derivation. It’s listed without attribution as Theorem 53 in Titchmarsh[80, p77]. Mehler’s
formula works in summing Eq. (3.88) because the eigenvalue spectrum of Ω is equally spaced, like the harmonic oscillator.
Indeed, it’s used to find the quantum-mechanical propagator for the harmonic oscillator (see Sakurai[81, p119]).Brownian motion and stochastic dynamics ■ 79
where the primes distinguish the eigenfunctions and eigenvalues from those in Eq. (3.78). Perform
the same transformation on Eq. (3.93) as with Eq. (3.79). We find

Ω+i  k · v

f = λ′
f, (3.94)
where f(v) ≡ F′
(v)/
ϕ(v) and Ω is given in Eq. (3.81). Equation (3.94) is equivalent to

kT
M ∇2
v − M
4kT v2 +
3
2 + iβ−1k · v

f = β−1λ′
f. (3.95)
Let v2 → v2 − 4iDk · v − 4D2k2 + 4iDk · v + 4D2k2. Using Eq. (3.20), we have the identity
− M
4kT v2 = − M
4kT (v − 2iDk)
2 − iβ−1k · v − β−1Dk2.
Equation (3.95) is equivalent to

kT
M ∇2
v − M
4kT (v − 2iDk)
2 +
3
2

f = β−1 
λ′ + Dk2
f. (3.96)
Comparing Eq. (3.96) with Eq. (P3.16), we see that if F(v) is a solution of Eq. (P3.16) having
eigenvalue λ, then f(v) = F(v − 2iDk) is a solution of Eq. (3.96) with λ′ = λ − Dk2. With
F′
(v) = ϕ(v)f(v) and F(v) = F(v)/
ϕ(v), we have that
F′
n(v) = 
ϕ(v)
ϕ(v − 2iDk)
Fn(v − 2iDk) (3.97)
is an eigenfunction of Ω+ik · v with eigenvalue λ′
n = λn − Dk2. The orthonormality condition is
the same as Eq. (3.85),

d3v
1
ϕ(v)
F′
n(v)F′
m(v) = δn,m.
We therefore have the solution to Eq. (3.92),
P(k, v, t) = 
n
C′
nF′
n(v) exp(λ′
nt), (3.98)
where the coefficients C′
n are found from initial conditions, just as in Eq. (3.87). (The right side of
Eq. (3.98) should indicate a dependence on k, but it doesn’t. F′
n, C′
n, and λ′
n each depend on k.)
Consider a B-particle that at time t = 0 is at r = 0 with velocity v = v0, i.e., P(r, v, 0) =
δ(r)δ(v − v0), implying P(k, v, 0) = δ(v − v0), in turn implying C′
n = F′
n(v0)/ϕ(v0). We can
therefore write Eq. (3.98) as
P(k, v, t|v0) = 1
ϕ(v0)

n
F′
n(v0)F′
n(v) exp(λ′
nt), (3.99)
the analog of Eq. (3.88). Equation (3.99) can be summed using Eq. (3.89). We find
P(k, v, t|v0) = P(v, t|v0)e−Dk2t exp 
β−1 tanh(βt/2) 
ik · (v0 + v)+2Dk2 , (3.100)
where P(v, t|v0) is given in Eq. (3.90). To find P(r, v, t|v0) we have to evaluate the inverse Fourier
transform, but the Fourier transform of a Gaussian is another Gaussian. We find
P(r, v, t|v0) = P(v, t|v0) 1
(2πσ2)3/2 exp 
− 1
2σ2

r − β−1 tanh(βt/2)(v + v0)
2

(3.101)
where σ2 ≡ 2D 
t − 2β−1 tanh(βt/2)
.80 ■ Non-Equilibrium Statistical Mechanics
3.4.4 Strong damping regime, Smoluchowski equation
Consider a system such that β|v|≫|v˙|. The friction in such a system is sufficiently strong that
any accelerations produced by forces—random and external—are rapidly damped, resulting in B￾particles having small accelerations. Under the approximation v˙ = 0, the Langevin equation reduces
to
dr
dt = 1
β (K(r) + A(t)),
where A(t) is the fluctuating acceleration imposed by the medium. What is the form of the Fokker￾Planck equation in this case? Because we’re in a regime of small time variations in v, we expect
the velocity distribution to rapidly thermalize, even though the position distribution will remain far
from equilibrium for a longer time. We seek therefore a Fokker-Planck equation for probability
distributions depending only on (r, t).
We start with the Fokker-Planck equation (3.59) written in the form (restricting our analysis to
one dimension),
ΩP(x, v, t) ≡ ∂
∂v 
v +
kT
M
∂
∂v 
P(x, v, t)
= β−1
 ∂
∂t + v
∂
∂x + K ∂
∂v 
P(x, v, t) ≡ β−1LP(x, v, t). (3.102)
where the first equality defines the operator Ω (with βΩ ≡ Ω in Eq. (3.60)), and the last equality
defines the operator L. We seek the small β−1 form of the Fokker-Planck equation.49 We assume
that an expansion in powers of β−1 can be developed:
P(x, v, t) = P(0)(x, v, t) + β−1P(1)(x, v, t) + β−2P(2)(x, v, t) + ··· . (3.103)
Combining Eq. (3.103) with Eq. (3.102), we find the typical perturbative scheme where the term in
the expansion at each order in β−1 is found from the terms preceding it,
ΩP(0)(x, v, t)=0
ΩP(i+1)(x, v, t) = LP(i)
(x, v, t). (i = 0, 1,...) (3.104)
One might try to solve these equations by finding the Green function associated with Ω; such an
approach, however, is complicated by the fact that Ω has an eigenvector corresponding to zero
eigenvalue. One has to ensure at each order that LP(i) is orthogonal to the null space of Ω ([13,
p259]). This program is illustrated in Exercise 3.36. We find to first order in β−1 (see Eq. (P3.29)),
∂
∂tP(x, t) = −β−1 ∂
∂x [KP(x, t)] + D ∂2
∂x2 P(x, t), (3.105)
where P(x, t) ≡  P(x, v, t)dv.
Equation (3.105) is the Smoluchowski equation (derived in 1915), a generalized diffusion equa￾tion that includes a drift term associated with external forces. Note that it has the form of a continuity
equation, implying the probability current density50
J = β−1KP − D ∂P
∂x . (3.106)
There are convective as well as diffusive contributions to the probability current.
49We can’t simply let β → ∞ in Eq. (3.102) and be done with it because the solution P(x, v, t) also depends on β. 50Equation (3.106) becomes a vector relation in three dimensions, J = β−1KP − D∇P.Brownian motion and stochastic dynamics ■ 81
3.4.5 Uniform field
For K = g = constant, we have from Eq. (3.59)
∂P
∂t + v · ∇rP + g · ∇vP = β∇v ·

v +
kT
M ∇v

P. (3.107)
In allowing for the effects of gravity, the acceleration in a suspension medium must take buoyancy
into account: g = (1 − ρ0/ρ) g0 where ρ, ρ0 are the mass densities of B-particles and the surround￾ing fluid, g0 is the free-space gravitational acceleration at the earth’s surface, and the coordinate
system has been chosen so that the z-axis is directed upwards, opposite the direction of gravity. For
definiteness, take ρ>ρ0 so that g = −gzˆ and the terminal velocity vT = g/β (see Section 3.1.3)
is directed downward.
Because g is a constant vector, it can be placed on the right side of the equation,
 ∂
∂t + v · ∇r

P(r, v, t) = β∇v ·

v − vT +
kT
M ∇v

P(r, v, t). (3.108)
Equation (3.108) suggests working in a reference frame falling with the terminal velocity, because
in that frame the effect of gravity on Brownian motion is eliminated.51 This allows us to infer, for
example, that Einstein’s formula for the mean-square displacement (in three dimensions) generalizes
to ⟨[∆r − vT t]
2
⟩ = 6Dt. Although beautiful physics, it’s not terribly practical: A B-particle in a
container cannot fall indefinitely.
To illustrate the effects of gravity, let’s apply the Smoluchowski equation, which in a high￾friction environment pertains to the phenomenon of sedimentation. With K = −gzˆ, Eq. (3.105)
has the form
∂
∂tP(z, t)=(g/β) ∂
∂z P(z, t) + D ∂2
∂z2 P(z, t) (3.109)
Diffusion in the (x, y) plane takes place just as in the field-free case, and thus we restrict our atten￾tion to the z-direction. Equation (3.109) must be supplemented with appropriate boundary condi￾tions. Suppose the B-particle is initially at z = z0, implying that limt→0 P(z, t) = δ(z − z0). Let
the bottom of the container be the plane at z = 0, a boundary at which the normal component of the
probability current must vanish, implying from Eq. (3.106) the mixed boundary condition52
∂
∂z P(z, t)



z=0
= −Mg
kT P(0, t). (3.110)
The solution to Eq. (3.109) with these boundary and initial conditions has been given by
Chandrasekhar[82, p58], to which we refer.
SUMMARY
We introduced two methods for the dynamics of stochastic processes, the Langevin and Fokker￾Planck equations, using Brownian motion as a theme.
• The Langevin equation is Newton’s second law of motion involving two types of forces,
the systematic force that affects the average motion of particles and the random force F(t)
representing interactions with microscopic degrees of freedom. We’re unable to specify the
form of F(t) other than listing its statistical properties. The ensemble average vanishes
51Under r → r′ = r − vT t (Galilean transformation), v → v′ = v − vT . In the “primed” frame, falling with the
terminal velocity, Brownian motion is the same as in the field-free case. This is close to Einstein’s equivalence principle
that one cannot distinguish the effect of a uniform gravitational field from an accelerated frame in the absence of gravity[6,
p187]. 52We’re using the term mixed boundary condition as it occurs in Sturm-Liouville theory; see [13, p58].82 ■ Non-Equilibrium Statistical Mechanics
⟨F(t)⟩ = 0 and the second moment ⟨F(t)F(t
′
)⟩ is determined by demanding concordance
with the equipartition theorem. For the random force, we expect almost no correlation be￾tween impacts at different times; a common strategy is to take delta-correlated noise with
⟨F(t)F(t
′
)⟩ = Γδ(t − t
′
) with the strength Γ determined so that the equipartition theorem is
satisfied.
• The motion of Brownian particles is inertial at short times and diffusive at long times.
• There is a connection between a system’s resistive or damping process and the extent to which
fluctuations are correlated [Eq. (3.7)]. Wherever there is damping there are fluctuations, and
vice-versa.
• The Einstein relations, Eqs. (3.20) and (3.23), relate transport coefficients to temperature and
other system parameters.
• The Fokker-Planck equation (either Eq. (3.36) or Eq. (3.37)) is a partial differential equation
for time-dependent probabilities, either P1(y, t) or the transition probability P1|1(y, t|y0, t0).
The stochastic process describing the velocity of a Brownian particle, Eq. (3.49), the Ornstein￾Uhlenbeck process, is the only process that is simultaneously Markov, Gaussian, and station￾ary. Brownian motion in force fields was treated in Section 3.4.
EXERCISES
3.1 Chandrasekhar, in Stochastic Problems in Physics and Astronomy,
53 states, “Under normal
conditions, in a liquid, a Brownian particle will suffer about 1021 collisions per second . . . .”
Estimate the number of collisions per second suffered by a spherical B-particle of radius
1 µm suspended in water at room temperature. Do the calculation two ways.
a. Assume that water molecules of mass m have the thermal speed kT /m.
b. That formula, however, pertains to gases, and liquids are not gases. The average inter￾particle separation in water (a condensed phase) is a ≈ 3 × 10−10 m (obtained from
the density). Molecular motions in liquids are diffusive. An estimate for the speed of a
diffusing particle can be had from Eq. (2.59), (2D/a)=(a/τ ) ≈ v. The self-diffusion
coefficient of water at room temperature is ≈ 2 × 10−9 m2/s, implying v ≈ 10 m/s. This
estimate is an order of magnitude smaller than the first.
Using the lower estimate of the speed, a 1µm particle undergoes ≈ 1018 collisions per second.
Chandrasekhar didn’t specify a particle size.
3.2 a. Show that the following expression solves Eq. (3.1) for t ≥ 0, where V (t) is assumed to
be known,
I(t) = I0e−t/τc +
1
L
 t
0
e−(t−t′
)/τc V (t
′
)dt
′
, (P3.1)
and where I0 is the current at an arbitrarily chosen instant of time (stationary processes
have no unique origin in time). Use the Leibniz integral rule.
b. Define a time average I ≡ limT→∞(1/T)
 T
0 I(t)dt. Show that I = 0.
c. Equation (P3.1) holds for any initial condition. Positive values of I0 are as equally likely
as negative, and hence ⟨I0⟩ = 0. Show that ⟨I(t)⟩ = 0. Hint: ⟨V (t)⟩ = 0 for any time t.
53See Chandrasekhar[82], reprinted in Wax[44, pp3–91]. Chandrasekhar received the 1983 Nobel Prize in Physics.Brownian motion and stochastic dynamics ■ 83
3.3 Derive Eq. (3.3).
3.4 Suppose in Eq. (3.5) that I2
0 is replaced with its value obtained from the equipartition theorem.
In that case, what is the time evolution of ⟨I2(t)⟩? Systems in equilibrium stay in equilibrium.
3.5 Derive Eq. (3.11) from Eq. (3.9). Hint: Take the ensemble average of Eq. (3.9); use Eqs.
(2.21) and (3.10).
3.6 We formulated the Langevin equation in terms of scalar quantities, but it also applies to
vectors. Treat the velocity v of a B-particle as a random variable subject to the stochastic
differential equation
M dv
dt = −αv + F, (P3.2)
where F is the random force considered a vector quantity. Equation (P3.2) implies three
scalar equations, Mdvi/dt = −αvi + Fi, i = 1, 2, 3.
a. Show that the solution of Eq. (P3.2) with initial condition v(0) = v0 is
v(t)=e−(α/M)t

v0 +
1
M
 t
0
dt
′
e(α/M)t′
F(t
′
)

, (P3.3)
the vector analog of Eq. (P3.1). The subensemble average ⟨F⟩v0 = 0 for all v0, implying
⟨Fi⟩v0 = 0, i = 1, 2, 3. Thus, ⟨v(t)⟩v0 = v0 exp(−αt/M), the analog of Eq. (3.11).
b. Use Eq. (P3.3) to show that
⟨vi(t1)vj (t2)⟩v0
= e−(α/M)(t1+t2)

v0iv0j +
1
M2
 t1
0
 t2
0
dt
′
dt
′′e(α/M)(t′
+t′′)
⟨Fi(t
′
)Fj (t
′′)⟩v0

.
(P3.4)
Because the random force is independent of initial conditions, no harm would be done in
erasing the subscript v0 on ⟨Fi(t
′
)Fj (t
′′)⟩v0 , but let’s leave it there. Note that the formula
for ⟨vi(t1)vj (t2)⟩v0 depends on t1 and t2 separately (instead of on their difference) be￾cause the values of vi(t1) and vj (t2) depend on when the velocity was v0, i.e., there is a
preferred origin in time associated with the initial condition.
c. Equation (P3.4) indicates an average computed over a subensemble of systems having
the same initial condition v = v0. The full ensemble average is found by averaging over
initial conditions. This is sometimes indicated as a double average ⟨⟨ ⟩v0 ⟩, a notation we
won’t adopt; we simply let ⟨ ⟩ denote the full ensemble average. In an average over all
possible initial conditions, ⟨v0iv0j ⟩ is zero for i ̸ j but nonzero when i = j,
⟨v0iv0j ⟩ = 1
3
δij ⟨v2
0⟩,
where we’ve used isotropy, ⟨v2
0x⟩ = ⟨v2
0y⟩ = ⟨v2
0z⟩. We also have the generalization of
Eq. (3.13),
⟨Fi(t
′
)Fj (t
′′)⟩ = δijΓδ(t
′ − t
′′),
where the strength Γ is to be determined. Show from Eq. (P3.4) that
⟨vi(t1)vj (t2)⟩ = δij 
e−(α/M)(t1+t2)
1
3
⟨v2
0⟩ − Γ
2αM 
+
Γ
2αM e−(α/M)|t1−t2|

.
(P3.5)84 ■ Non-Equilibrium Statistical Mechanics
Stationarity requires that Γ/(2αM) = ⟨v2
0⟩/3, implying Γ=2αkT, just as in the scalar
case, Eq. (3.14). The same result is obtained by setting t1 = t2 ≡ t and letting t → ∞,
limt→∞⟨v2
i (t)⟩ = Γ
2αM = ⟨v2
i ⟩ = kT
M ,
which is another form of the stationarity requirement. For the full ensemble average, we
have a nice formula
⟨vi(t1)vj (t2)⟩ = δij
kT
M e−(α/M)|t1−t2|
.
3.7 What are the dimensions of Γ, the strength of the random force autocorrelation function?
Hint: What is the dimension of the Dirac delta function in Eq. (3.13)?
3.8 Derive Eq. (3.16). Note that the derivation requires the operations of taking a derivative and
forming expectation values to commute; see Section 2.3.2.
3.9 Derive Eq. (3.17).
3.10 Derive Eq. (3.18). Hint: Expand all terms in Eq. (3.17) to O(t
3).
3.11 Calculate the variance of displacements in Brownian motion from the Langevin equation.
a. Find the average displacement of a B-particle with initial speed v0. Integrate Eq. (3.11)
to show
⟨x(t) − x0⟩v0 = v0τc

1 − e−t/τc

. (P3.6)
Recognize that you’re invoking Eq. (2.21), d⟨x(t)⟩/dt = ⟨v⟩. Equation (P3.6) can be
interpreted as the average distance traveled in time t with velocity ⟨v(t)⟩v0 = v0e−t/τc .
Note that ⟨x(t) − x0⟩v0 ∼ v0τc for t ≫ τc.
b. Now average over v0 (see discussion in Exercise 3.6). Show that ⟨x(t) − x0⟩ = 0.
c. Average Eq. (3.17) over v0.
i. Show that the variance is
⟨(x(t) − x0)
2⟩ = 2kTM
α2
 t
τc
− 1+e−t/τc

. (P3.7)
Hint: ⟨v2
0⟩ = kT /M. (We’re using the scalar version of the Langevin equation.)
ii. Show from Eq. (P3.7) that
⟨(x(t) − x0)
2⟩ ∼



kT
M t
2 t ≪ τc
2kT
α
t. t ≫ τc
(P3.8)
The variance of the diffusion process (obtained from the solution of the diffu￾sion equation (2.61)), which grows linearly with time (as we see from Eq. (2.62),
⟨x2(t)⟩ = 2Dt, but also from the Gaussian form of Eq. (2.60)), is recovered in
the theory of Brownian motion at long times (as we see from Eq. (P3.8)) when
the Einstein relation Eq. (3.20) is invoked. Einstein was aware that his result
(⟨x2(t)⟩ = 2Dt) could not apply for short times. His reasoning is characteristically
insightful:[48, p34] “The reason is that we have implicitly assumed in our develop￾ment that the events during the time t are to be looked upon as phenomena indepen￾dent of the events in the time immediately preceding. But this assumption becomes
harder to justify the smaller the time t is chosen.”Brownian motion and stochastic dynamics ■ 85
3.12 Show that Eq. (3.20) yields the correct dimensions for the diffusion coefficient.
3.13 In an experiment on the Brownian motion of a particle of radius 0.4 µm in a water-glycerine
solution having viscosity ν = 0.0278 Poise at a temperature of 18.8 ◦C, the observed x￾component of the displacement in a 10 second interval was ⟨x2(t)⟩ = 3.3 × 10−12 m2. A
Poise is a non-SI unit of viscosity (named after Poiseuille), 1 Poise = 0.1 Pascal-second. Use
these data to estimate the value of Boltzmann’s constant. Hint: Use the Stokes formula.
3.14 In one of Perrin’s experiments, Brownian motion of mastic grains (a type of plant resin)
of radius 0.52 µm and mass 6.5 × 10−16 kg was observed in water of viscosity ν = 0.01
Poise[62, p123]. Calculate the relaxation time τc ≡ M/α, where α is the friction coefficient.
Use the Stokes formula. A: 6.6 × 10−9 s. Show that the ratio M/m of the mass of the B￾particle to the mass of a water molecule is (in this case) ≈ 2 × 1010, underscoring that
Brownian motion is the cumulative effect of random weak impulses.
3.15 Consider a stochastic process {X(t), t ≥ 0} that has stationary independent increments (see
Section 3.2) such that ⟨X(t)⟩ = 0 for t ≥ 0. This process has several useful properties.
a. Because ⟨X(t)⟩ = 0, the second moment ⟨[X(t) − X(0)]2⟩ ≡ f(t) is the variance. Show
that
f(t1 + t2) = f(t1) + f(t2). (P3.9)
Hint: f(t1 + t2) = ⟨[X(t1 + t2) − X(t1) + X(t1) − X(0)]2
⟩; use the statistical inde￾pendence of increments and invoke stationarity. Equation (P3.9) is Cauchy’s functional
equation (see Aczel[58]), the only solution of which is a linear function f(x) = αx for
constant α (proved by Cauchy in 1821). The variance therefore grows linearly with time,
f(t) = αt, where α > 0 is a constant determined by the physics of the problem. For
example, when X(t) refers to the displacement of a B-particle, we infer from Eq. (2.62)
that α = 2D.
b. Show that the covariance
K(s, t) ≡ ⟨X(s)X(t)⟩ = α min(s, t), (P3.10)
where min denotes the minimum of s and t.
Hint: ⟨X(s)X(t)⟩ = ⟨X(s) [X(t) − X(s) + X(s)]⟩.
3.16 Consider the differential form dv + βvdt in Eq. (3.25) (Doob’s form of the Langevin equa￾tion). Is it exact? (See [2, p6].) Show that f(t)=eβt is an integrating factor.
3.17 Show that Eq. (3.29) with the choice f(t)=eβt implies Eq. (3.26).
3.18 Derive Eq. (3.35).
3.19 Derive Eq. (3.37) from Eq. (3.36).
3.20 The Fokker-Planck equation associated with the Wiener-Levy process is the diffusion equa￾tion without drift. Calculate the transition moments for this process; see Eq. (3.34). A:
M2n+1 =0(n = 0, 1,...), M2 = 2D, M2n,n≥2 = 0.
3.21 Guided exercise: In this exercise, we show how the Fokker-Planck equation follows from
the master equation, and then discuss some implications. From Eq. (2.66), reproduced here,
erasing the subscript on P1(y, t), we have the master equation
∂
∂tP(y, t) =  ∞
−∞
dx [W(y|x)P(x, t) − W(x|y)P(y, t)] , (2.66)
where the transition rate W(y|x) is the probability per unit time of the transition x → y.86 ■ Non-Equilibrium Statistical Mechanics
a. The jump length ξ ≡ y − x is the change in a random variable x in the transition x → y.
Make the change in variable x = y − ξ and show that the master equation has the form
∂
∂tP(y, t) =  ∞
−∞
dξ [W(y|y − ξ)P(y − ξ, t) − W(y − ξ|y)P(y, t)] .
b. Define a function of two arguments g(a, b) ≡ W(a + b|b). Show that
∂
∂tP(y, t) =  ∞
−∞
dξ [g(ξ,y − ξ)P(y − ξ, t) − g(ξ,y)P(y, t)] ,
where in the second integral we’ve let ξ → −ξ.
c. Let u ≡ y − ξ. Develop g(ξ, u)P(u, t) in a power series in u about u = y (ξ = 0),
g(ξ, u)P(u, t) = g(ξ,y)P(y, t) + ∞
n=1
1
n!
(−ξ)
n
 ∂n
∂un [g(ξ, u)P(u, t)]




u=y

.
Show that
∂
∂tP(y, t) = ∞
n=1
(−1)n
n!
∂n
∂yn [αn(y)P(y, t)] , (P3.11)
where
αn(y) ≡
 ∞
−∞
ξng(ξ,y)dξ =
 ∞
−∞
dx(x − y)
nW(x|y) (P3.12)
is the nth moment of the transition rate, the jump moment.
Comment: Equation (P3.11) is the Kramers-Moyal expansion; it appeared in Kramers[76]
and was considerably improved upon by Moyal[83]. It provides an infinite-order differential
operator to represent the time rate of change of the probability (as opposed to the integral
operator of the master equation). It’s formally equivalent to the master equation, and therefore
not any easier to work with, but it suggests an approximation. If we truncate the expansion
after n = 2, we recover the Fokker-Planck equation, (3.37). Obtaining the Fokker-Planck
equation this way has been criticized by van Kampen[84][51]; a power series should have
some small parameter to guarantee convergence and simply truncating the series is not a
systematic approximation procedure. If we’re to ignore the jump moments for higher n, the
transition rate functions W(x|y) should be sharply peaked for x ≈ y, the requirement of
small jump lengths. Clearly there are mathematical issues in passing from an integral to a
differential operator. One must assume that certain partial derivatives exist as well as the
convergence of the expansion. R.F. Pawula[85][86] showed that if the αn defined by Eq.
(P3.12) exist for all n, and if αn = 0 for some even n, then αn = 0 for all n ≥ 3. By
this theorem, it’s logically inconsistent to retain more than two terms in the Kramers-Moyal
expansion unless all terms are retained. In some ways, the Fokker-Planck equation is the best
we can do in approximating the master equation. Pawula’s theorem is discussed in Risken[67,
Section 4.3].
3.22 Derive Eq. (3.41). Hint: M2 = lim∆t→0(1/∆t)⟨(v −v0)2⟩v0 . Use Eq. (3.11), Eq. (3.12) with
τ = 0, and delta-correlated noise, Eq. (3.13), with Γ=2kTα.
3.23 Show for a smooth function f(x) such that f(x) → 0 as x → ±∞,  ∞
−∞
∂
∂x [xf(x)] e−iqxdx = −q
∂
∂q  ∞
−∞
f(x)e−iqxdx
 ∞
−∞
∂2f(x)
∂x2 e−iqxdx = −q2
 ∞
−∞
f(x)e−iqxdx.
Assume all “integrated parts” vanish as x → ±∞.Brownian motion and stochastic dynamics ■ 87
3.24 Verify that the form of P(q, τ ) given in Eq. (3.47) solves the differential equation (3.46) for
any function F.
3.25 Using the transition probability for Brownian motion, Eq. (3.49), verify that the transition
moments M1, M2 from
Mn = lim
∆t→0
1
∆t
 ∞
−∞
(v − v0)
nP(v, ∆t|v0)dv
are as given in Eqs. (3.40) and (3.41), and then show that to leading order M3 = O(∆t)5/2
for small ∆t, and hence that M3 = 0 in the limit. Hint: Hermite polynomials54 have the
integral representation[88, p338]
 ∞
−∞
xne−(x−β)2
dx = (2i)−n√πHn(iβ),
where H1(z)=2z,H2(z)=4z2 − 2, H3(z)=8z3 − 12z, etc.
3.26 Fill in the steps between Eq. (3.48) and Eq. (3.49).
3.27 Show that the limit τ → 0 in Eq. (3.49) is a delta function. See Exercise 2.31.
3.28 Verify that with the proper substitutions Eq. (3.49) has precisely the form of a transition
probability for stationary Gaussian processes, Eq. (2.108).
3.29 Find the stationary solution of the Fokker-Planck equation when K(r) = −(1/M)∇rψ(r).
a. Try the separation of variables technique. Let Pst(r, v) = f(r)g(v). Show that Eq. (3.59)
reduces to
v ·
 1
f ∇rf

+ K ·
1
g
∇vg

= 1
g
β∇v ·

vg +
kT
M ∇vg

. (P3.13)
b. The terms in square brackets in Eq. (P3.13) generalize Eq. (3.43), which we obtained in
finding the stationary solution of the single-variable Fokker-Planck equation, (3.42). Show
that these terms vanish when g(v) = exp(−Mv2/(2kT)). Hint: ∇vv2 = 2v.
c. With g(v) so determined, f(r) must be chosen so that the left side vanishes. Show that
this requires
v ·

1
f ∇rf − M
kT K

= 0. (P3.14)
If K(r) = −(1/M)∇rψ(r), show that the terms in square brackets vanish if f(r) =
exp(−ψ(r)/kT).
Thus Pst(r, v) ∝ exp(−ψ(r)/kT) exp(−Mv2/2kT) is the stationary solution of the
Fokker-Planck equation, (3.59).
3.30 Consider the field-free Fokker-Planck equation for P(r, v, t)
∂P
∂t + v · ∇rP = β∇v ·

v +
kT
M ∇v

P (P3.15)
54Hermite polynomials are often listed in quantum mechanics texts. Or see Abramowitz and Stegun[87, p775].88 ■ Non-Equilibrium Statistical Mechanics
in an unbounded system. The boundary conditions are that P vanishes for large r and v.
Define spatially dependent probability and current densities
ρ(r, t) ≡

d3vP(r, v, t)
J(r, t) ≡

d3vvP(r, v, t)
where the integrals are over all velocities.
a. Show that these definitions imply a continuity equation
∂ρ
∂t + ∇r · J = 0.
Hint: Use the divergence theorem along with the boundary condition.
b. With the average speed defined
⟨v⟩t ≡

d3xd3vvP(r, v, t),
show that the drag force law is obtained for the average,
d
dt
⟨v⟩ = −β⟨v⟩.
Hint: Use the Leibniz integral rule, d⟨v⟩/dt =  d3xd3v (∂P/∂t). Recognize, using Eq.
(P3.15), that one term is a spatial integration over a gradient and vanishes because of
the boundary condition. For the second term, use integration by parts together with the
boundary condition.
3.31 Find the adjoint of the Fokker-Planck operator in Eq. (3.60). Any operator Ω has associated
with it a new operator Ω† (its adjoint) such that ⟨Ω†f|g⟩ = ⟨f|Ωg⟩ for any functions f,g
obeying appropriate boundary conditions (see [13, p56]), which in this case we require to
decay sufficiently rapidly as v → ±∞ that integrated parts vanish.
A: For Ω = ∇v · (v + (kT /M)∇v), Ω† = −v · ∇v + (kT /M)∇2
v.
3.32 Derive the form of Ω in Eq. (3.81), given its definition in Eq. (3.79). Don’t forget to let Ω act
on an unknown function.
3.33 We stated that with Ω given in Eq. (3.81), the eigenvalue problem ΩF = λF has the form
of the time-independent Schrodinger equation for the isotropic three-dimensional harmonic ¨
oscillator. Let’s flesh that out.
a. In Cartesian coordinates the eigenvalue problem has the separable form

kT
M
 ∂2
∂v2
x
+
∂2
∂v2
y
+
∂2
∂v2
z

− M
4kT

v2
x + v2
y + v2
z

+
3
2

F = 1
β (λx + λy + λz) F . 
(P3.16)
where λ = λx + λy + λz. Show that, with F = f(vx)f(vy)f(vz), an isotropic solution
requires that we need solve only a one-dimensional eigenvalue problem for f = f(v),
kT
M f′′ − M
4kT v2f =
λ
β − 1
2

f. (P3.17)Brownian motion and stochastic dynamics ■ 89
Introduce a dimensionless velocity y = v/α, where the velocity scale α is to be deter￾mined. Show that with α = 2kT /M, Eq. (P3.17) reduces to
f′′ − y2f = 2 λ
β − 1
2

f. (P3.18)
b. With the substitution f(y) = exp(−y2/2)H(y), show that Eq. (P3.18) reduces to
H′′ − 2yH′ − 2 (λ/β) H = 0. (P3.19)
Equation (P3.19) has polynomial solutions55 when −2(λ/β)=2n for n = 0, 1, 2,... , the
Hermite polynomials, Hn(y). We therefore have an equally spaced eigenvalue spectrum,
λ = −nβ. There is no “zero point” velocity here; this is a classical equation. Solutions of
ΩF = λF occur in the form of a Gaussian multiplied by products of Hermite polynomials,
Fn1,n2,n3 = An1,n2,n3 exp 
−v2/(2α2)

Hn1 (v1/α)Hn2 (v2/α)Hn3 (v3/α), (P3.20)
where λ = −β(n1 + n2 + n3) and An1,n2,n3 is a constant.
3.34 Show that Eq. (3.86) solves the field-free Fokker-Planck equation (3.77). Derive Eq. (3.87)
for the expansion coefficient.
3.35 Derive Eq. (3.90).
3.36 Consider the strong-friction expansion of P(x, v, t) in Eq. (3.103) for the Fokker-Planck
equation ΩP = β−1LP, where Ω, L are defined in Eq. (3.102).
a. At zeroth order, ΩP(0)(x, v, t)=0 (see Eq. (3.104)) so that P(0) is an eigenvector of Ω
associated with zero eigenvalue. Show that
P(0)(x, v, t) = ϕ(x, t) exp(−Mv2/(2kT)), (P3.21)
where ϕ(x, t) is an arbitrary (dimensionless) function of (x, t). As we show next, ϕ =
ϕ(x), so that P(0) = P(0)(x, v) is stationary.
b. At first order in β−1 (the first order in which K appears), ΩP(1)(x, v, t) = LP(0)(x, v, t).
Show that
ΩP(1)(x, v, t) =  ∂
∂t + v
∂
∂x − M
kT Kv
ϕ(x, t) exp(−Mv2/(2kT)). (P3.22)
One might consider solving Eq. (P3.22) for P(1) by finding the Green function for Ω (it
could happen). Ω, however, has an eigenvector with zero eigenvalue, which complicates
the analysis. When this happens, a solution for P(1) exists if and only if the right side of
Eq. (P3.22), LP(0), is orthogonal to the left eigenvector associated with zero eigenvalue
(see [13, p260]), in this case a constant. Show that this requirement, met in this case by
 ∞
−∞ LP(0)dv = 0, implies the solubility condition
∂
∂tϕ(x, t)=0. (P3.23)
A solution for P(1)(x, v, t) exists only if Eq. (P3.23) is obeyed, implying ϕ = ϕ(x) or
that P(0) is stationary. The stationary solution is independent of β.
55When λ does not meet the eigenvalue condition λ = −βn, the differential equation has an irregular singularity at
infinity, and such solutions must be discarded as unphysical. See for example [13, p130].90 ■ Non-Equilibrium Statistical Mechanics
c. With the time derivative absent from Eq. (P3.22), verify that
P(1)(x, v, t) = 
v
 M
kT Kϕ − ∂ϕ
∂x
+ f(x, t)

exp(−Mv2/(2kT)), (P3.24)
where f(x, t) is another unknown function (that must have the dimension time−1).
d. Working to one order higher, P(2) exists when  ∞
−∞ LP(1)dv = 0. Show that the associ￾ated solubility condition is
∂
∂tf(x, t) = − ∂
∂x (Kϕ) + kT
M
∂2ϕ
∂x2 . (P3.25)
e. Assemble the pieces and show that to first order in β−1,
P(x, v, t) = exp(−Mv2/(2kT)) 
ϕ(x) + β−1
Mv
kT Kϕ − v
∂ϕ
∂x + f(x, t)
+O(β−2).
(P3.26)
Integrate P(x, v, t) over v to find a probability distribution in (x, t):
P(x, t) =  ∞
−∞
P(x, v, t)dv =
2πkT
M

ϕ(x) + β−1f(x, t)

+ O(β−2). (P3.27)
f. Use Eq. (P3.25) and Eq. (P3.27) to show that
∂
∂tP(x, t) = 2πkT
M β−1

− ∂
∂x (Kϕ) + kT
M
∂2ϕ
∂x2

. (P3.28)
g. Recognize from Eq. (P3.27) that to lowest order ϕ(x) = M/(2πkT)P(x, t)+O(β−1).
Show that
∂
∂tP(x, t) = −β−1 ∂
∂x [KP(x, t)] + D ∂2
∂x2 P(x, t), (P3.29)
where we’ve used an Einstein relation.CHAPTER 4
Kinetic theory: Boltzmann’s
approach to irreversibility
Acommon feature of macroscopic systems is their irreversible evolution toward thermal equi￾librium, no matter how they’re prepared in nonequilibrium states. Irreversibility is built-in
to Markov processes.1 Stochastic models provide phenomenological frameworks for describing the
approach to equilibrium—and therein lies their justification—but they don’t predict irreversibility,
they’re guided by it. Of the laws of physics, only the second law of thermodynamics speaks explic￾itly to irreversibility, wherein entropy is created in irreversible processes. Entropy, however, is not
a microscopic property of matter, rendering microscopic treatments of irreversibility a challenging
problem in theoretical physics.2 We noted in Section 2.2 that the second law is not inherent in time￾reversal-invariant equations of motion, but did anyone ever try? Ludwig Boltzmann sought, in the
1870s, to derive the second law as a consequence of microscopic physics. We’ll identify the point in
his analysis where time asymmetry is introduced through a non-mechanical, statistical assumption.
Although he didn’t succeed in deriving entropy from mechanics, his discoveries are widely used
in modeling nonequilibrium systems. We develop Boltzmann’s approach in this chapter, known as
kinetic theory. Kinetic theory has a rich history; see Brush[89].
4.1 FROM MECHANICS TO STATISTICAL MECHANICS
Phase space for many-particle systems—Γ-space—is a mathematical space representing all possible
mechanical states of a system, with each state associated with a point of the space. The time develop￾ment of N particles in three spatial dimensions is governed by 6N first-order differential equations,
Hamilton’s equations of motion, Eq. (A.1). States are specified at an instant of time by 3N gener￾alized coordinates {qi}N
i=1 and 3N canonical momenta {pi}N
i=1; Γ-space is 6N-dimensional. Each
state is associated with a vector3 Γ ≡ (q1, p1,..., qN , pN ), its system point or its phase point, or,
in the older literature, its phase. The evolution of the system can be visualized as a trajectory in
Γ-space, Γ(t), the phase trajectory.
1Markov processes satisfy the SCK equation, (2.29), which presumes a one-way flow of time, t1 > t2 > t3. The master
equation follows from the SCK equation when transition probabilities are written as in Eq. (2.64), and the Fokker-Planck
equation follows from the master equation (Kramers-Moyal expansion), or directly from the SCK equation, Eq. (3.32). 2Like temperature, one can’t speak of the entropy of a single particle. What mechanical or electrodynamical property of
matter is associated with entropy? Entropy is a property of equilibrium macroscopic systems as a whole. 3Γ-space is the direct product of the space of canonical momenta (of dimension 3N) and the space of generalized
coordinates (of dimension 3N). The direct product of vector spaces U and V is a new vector space, U × V ≡ {(u, v)|u ∈
U, v ∈ V }, with vector addition and scalar multiplication defined component wise. The dimension of U × V is the sum of
the dimensions of U and V .
DOI: 10.1201/9781003512295-4 9192 ■ Non-Equilibrium Statistical Mechanics
Precise knowledge of the system requires a precise specification of 6N microscopic quantities,
quantities we lack the ability to control. Therein lies the transition from mechanics to statistical
mechanics. To circumvent the fundamental problem of uncertainty in initial conditions, the concept
of ensembles is introduced, collections of macroscopically identical systems; ensemble averages are
averages over initial conditions consistent with constraints. Ensembles are represented by treating
the point located by Γ as a random variable. Introduce a probability density ρ(Γ, t) with ρ(Γ, t)dΓ
the probability at time t the phase point lies within dΓ ≡ dp1 ... dpN dq1 ... dqN about Γ. From
ρ(Γ, t), ensemble averages can be calculated. For A(Γ) a Γ-space function, it’s ensemble average
is found from4
⟨A⟩t ≡

Γ
A(Γ)ρ(Γ, t)dΓ, (4.1)
where unit normalization has been assumed, 
Γ ρ(Γ, t)dΓ = 1.
How to find ρ(Γ, t)? By Liouville’s theorem,5 ρ(Γ, t) is a constant of the motion:
d
dt
ρ(Γ, t)
Liouville
theorem
↓
= 0
math
↓
= ∂ρ
∂t +
3N
k=1
 ∂ρ
∂pk
p˙k + ∂ρ
∂qk
q˙k

Hamilton
equations
↓
= ∂ρ
∂t +
3N
k=1

− ∂ρ
∂pk
∂H
∂qk
+ ∂ρ
∂qk
∂H
∂pk

(4.2)
≡ ∂ρ
∂t + iΛρ,
where Λ is the Liouville operator (a linear operator),
Λ(·) ≡ i

3N
k=1
∂H
∂qk
∂(·)
∂pk
− ∂H
∂pk
∂(·)
∂qk

=i[H,(·)]P , (4.3)
with (·) a placeholder for the Γ-space function that Λ acts on and [H,(·)]P the Poisson bracket,
see Eq. (A.3). The factor of i (unit imaginary number) is included in the definition of Λ to make it
Hermitian, Λ† = Λ; see [5, p335]. The Liouville operator can be written compactly as
Λ=i
N
n=1

(∇nH) · ∂
∂pn
−
 ∂H
∂pn

· ∇n

, (4.4)
where ∇n ≡ ∇rn . Thus we have Liouville’s equation, the fundamental equation of statistical
mechanics,6 on par with the status of Schrodinger’s equation in quantum mechanics, ¨
i
∂
∂tρ(Γ, t)=Λρ(Γ, t). (4.5)
It’s solution defines a unitary time evolution operator U(t) ≡ e−iΛt (see Exercise 6.1),
ρ(Γ, t) = U(t)ρ(Γ, 0) = exp (−iΛt) ρ(Γ, 0), (4.6)
where ρ(Γ, 0) is the state of the ensemble at t = 0. States of mechanical systems are specified by
points in Γ-space; states of statistical mechanical systems are specified by weightings on Γ-space,
ρ(Γ, 0) with  dΓρ(Γ, 0) = 1.
4Note that A(Γ) carries no time dependence; all time dependence is in the probability density. Equation (4.1) indicates
to sample A(Γ) at every point of Γ-space weighted by the probability ρ(Γ, t) that the system point is at Γ at time t.
This prescription is analogous to the Schrodinger picture of quantum mechanics where operators are fixed in time and the ¨
probability density is time dependent. See Appendix E. 5Liouville’s theorem is a fundamental theorem of analytical mechanics. Its proof, omitted here (see [5, p47]), relies on
the unit Jacobian of canonical transformations ([5, p331]) and that trajectories in Γ-space never cross ([5, p32]). 6In equilibrium, we have the stationarity condition, Λρeq(Γ)=0; see [5, p47].Kinetic theory: Boltzmann’s approach to irreversibility ■ 93
The eigenvalues of Λ are real (Hermiticity), with its eigenfunctions a complete orthogonal set
(Sturm-Liouville theory[13, p66]), implying for an arbitrary initial condition ρ(Γ, 0) that the tem￾poral behavior of ρ(Γ, t) is oscillatory with no decay to a unique state. There is no irreversible
decay inherent in the fundamental equations of motion, qualitatively distinct from what we find in
the solutions of the master equation (2.83) or the Fokker-Planck equation (3.49).
Let’s find ρ(Γ, t) for free particles. Consider N identical noninteracting particles of mass m
confined to a cubical box of edge length a. The Hamiltonian H = (2m)−1 N
n=1 p2
n implies
ΛN = −i

N
n=1
∂H
∂pn
· ∂
∂rn
= −i

N
n=1
vn · ∂
∂rn
, (free particles) (4.7)
where vn ≡ pn/m. The velocities vn are constants of the motion (free particles). Moreover, ΛN (in
this case) is a sum of one-body operators, those that act on one particle only. To find the eigenvalues
and eigenfunctions of ΛN it suffices (in this case) to solve the one-body eigenproblem,
−iv · ∇ψ(r) = λψ(r). (4.8)
Equation (4.8) has solutions ψk(r) = Aeik·r, where A is a constant, with eigenvalues λk = v · k.
For periodic boundary conditions, k = (2π/a)(n1, n2, n3) for integer ni. By normalizing to unity,
A = a−3/2. The eigenfunctions and eigenvalues of ΛN in Eq. (4.7) have the form
ψ{k}(r1,..., rN ) = 1
a3N/2 exp 
i

N
n=1
kn · rn

λ{k} = 
N
n=1
vn · kn, (4.9)
where {k} denotes the collection of wave vectors, {k} ≡ (k1,..., kN ). There are 3N distinct
eigenvalues implied by the notation {k}. An arbitrary initial condition ρ(Γ, 0) can be expressed as
a linear combination of eigenfunctions (complete set)
ρ(Γ, 0) = 
{k}
A{k}ψ{k}(r1,..., rN ), (4.10)
where the expansion coefficients A{k} are functions of the momenta (see Exercise 4.4),
A{k}(p1,..., pN ) = 
dr1 ··· drN ψ∗
{k}(r1,..., rN )ρ(Γ, 0). (4.11)
Using Eq. (4.6), we have for free particles,
ρ(Γ, t) = 1
a3N/2

{k}
A{k}(p1,..., pN ) exp 
i

N
n=1
kn · (rn − vnt)

, (4.12)
a function of 6N constants of the motion, p1,..., pN and r1 − v1t, . . . , rN − vN t (see Section
A.1.2). The sum in Eq. (4.12) does not have a long-time limit.7
7Convergent trigonometric sums 
n an exp(iωnt) represent almost periodic functions (see Bohr[90]) which nearly
repeat as time progresses and do not approach a limit ast → ∞. (If the ωn are integers, the sum represents a periodic function
of period 2π.) If the ωn are close together, one could try to approximate the sum by an integral,  a(ω) exp(iωt)dω. For
any reasonable function a(ω), the integral decays (irreversibly!) to zero as t → ∞. Every discrete sum is almost periodic
and every integral converges to zero as t → ∞. Modeling irreversibility is tricky.94 ■ Non-Equilibrium Statistical Mechanics
4.2 REDUCED PROBABILITY DISTRIBUTIONS
The solution of the Liouville equation is a joint probability density: For the N-body system to
occupy state Γ at time t, particle 1 is in state r1, p1 ≡ 1, particle 2 is in state r2, p2 ≡ 2, and so
on, at time t. To save writing, let d1 ≡ dr1dp1, d2 ≡ dr2dp2,... denote volume elements for
each particle. Thus, ρ(1,...,N, t)d1 ··· dN is the probability at time t particle 1 is in d1 about
state 1, particle 2 is in d2 about state 2, and so on. It turns out that the N-body distribution contains
far more information than is necessary to calculate measurable quantities. For that reason, reduced
distributions fs are introduced for fewer particles (among the set of N particles)8
fs(1, . . . , s, t) ≡

ρ(1, . . . , s, s + 1,...,N, t)d(s + 1)··· dN, (4.13)
where  fsd1 ··· ds = 1 for all s. Thus, f1 (for example) is the probability of finding particle 1 in
state 1, irrespective of the states of particles 2,...,N. The reduced distributions are quite useful.
Yet if we can’t solve the Liouville equation for ρ(Γ, t), we can’t calculate them from Eq. (4.13).
The strategy for calculating these functions is treated in Section 4.3.
To see how reduced distributions arise, consider the average velocity of particle 1. From Eqs.
(4.1) and (4.13),
⟨v1⟩t =

dΓv1ρ(Γ, t) = 
dr1dv1v1d2 ··· dN ρ(1,...,N, t) = 
dr1dv1v1f1(r1, v1, t).
(4.14)
By this device, the phase-space variables of the other particles have been formally eliminated.9
Sometimes we want the l-particle velocity distribution where spatial coordinates are eliminated,
ϕl(v1,..., vl, t) ≡

dr1 ··· drlfl(r1,..., rl, v1,..., vl, t), (4.15)
in which case Eq. (4.14) can be written ⟨v1⟩t =  dv1v1ϕ1(v1, t). Sometimes we want the spatial
distribution of l particles obtained by eliminating velocities,
nl(r1,..., rl, t) ≡

dv1 ··· dvlfl(r1,..., rl, v1,..., vl, t). (4.16)
The reduced distributions fs pertain to subsystems of s enumerated particles, with fsd1 ··· ds
the probability of finding the subsystem in the volumes d1 ··· ds about the Γ-space points(1,...,s).
A related quantity is the s-tuple distribution,
10 defined with a combinatoric factor,
Fs(1, . . . , s, t) ≡ N!
(N − s)!fs(1, . . . , s, t). (4.17)
Whereas fs pertains to s labeled particles (1,...,s), Fs is the probability density of finding particles
at points (1,...,s) without regard for which particles are counted among the s, with normalization
 Fsd1 ··· ds = N!/(N − s)!; Fs is a phase-space number density function for s-tuples.
8Reduced distribution functions were introduced in statistical mechanics by Yvon[91]. See Green[92] and Yvon[93].
Equation (4.13) holds for 1 ≤ s ≤ N − 1; for s = N, fN ≡ ρ. The reader may notice an analogy with the hierarchy of
joint probabilities introduced in Section 2.3. In that section, probabilities refer to events occurring at different times. 9We’ve worked with phase space spanned by (r, v) in Eq. (4.14). When all particles have the same mass (a typical
case), velocities are easier to work with than momenta. Be mindful of the dimensions of distribution functions. 10The notation for reduced and s-tuple distributions is not standardized; beware. The quantity N!/(N − s)! is not the
binomial coefficient N
s

(the number of ways of choosing s objects from N without regard to order); it counts the number
of distinct ways of choosing s from N>s objects. See Exercise 4.6. Combinatorics are reviewed in [5, Chapter 3].Kinetic theory: Boltzmann’s approach to irreversibility ■ 95
4.3 DYNAMICS OF REDUCED DISTRIBUTIONS: THE HIERARCHY
To calculate reduced probability densities, we ask what their evolution equations are. It often hap￾pens that the same question is addressed independently by different researchers. As we now show,
the reduced distribution functions satisfy a hierarchy of coupled dynamical equations known as the
BBGKY hierarchy (or simply the hierarchy), after Bogoliubov (published in 1946, English transla￾tion in [94]), Born and Green,[95][96] Kirkwood,[97][98] and Yvon[91].
Consider N identical, structureless particles of mass m interacting through a two-body potential
Φij ≡ Φ(|ri − rj |), with Hamiltonian
H(r1,..., rN , p1,..., pN ) = 1
2m

N
i=1
p2
i +
N
i=1

N
j>i
Φij . (4.18)
The potential energy term is written so that we never encounter a self-interaction, Φii. There are N
terms in the first sum in Eq. (4.18) and 1
2N(N − 1) terms in the second. For convenience, we work
with a modified Liouville operator, L ≡ −iΛ. Referring to Eq. (4.3),
LN = − 1
m

N
k=1
pk · ∂
∂rk
+
N
k=1

N
i=1

N
j>i
∂Φij
∂rk
· ∂
∂pk
. (4.19)
The first term in Eq. (4.19), the noninteracting part of the operator, corresponds to Eq. (4.7); the re￾maining terms contain the effects of interactions. Of the terms in the interaction part of the operator,
the only nonzero contributions are for k = i and k = j (Φ is a two-body interaction),
∂Φij
∂rk
· ∂
∂pk
= δk,i
∂Φij
∂ri
· ∂
∂pi
+ δk,j
∂Φij
∂rj
· ∂
∂pj
.
But ∂Φij/∂ri = −∂Φij/∂rj from calculus (but which embodies Newton’s third law), implying
∂Φij
∂rk
· ∂
∂pk
= ∂Φij
∂ri
·

δk,i
∂
∂pi
− δk,j
∂
∂pj

.
Thus,
LN = −

N
i=1
Ki +
N
i=1

N
j>i
Oij , (4.20)
where Ki is a kinetic energy operator and Oij contains the effects of interactions,
Ki ≡ 1
m
pi · ∂
∂ri
Oij ≡ ∂Φij
∂ri
·
 ∂
∂pi
− ∂
∂pj

≡ −Fij ·
 ∂
∂pi
− ∂
∂pj

, (4.21)
with Fij = −∂Φij/∂ri the (internal) force experienced at ri due to a particle at rj . The potential
energy operator is symmetric, Oij = Oji. The kinetic energy operator K is associated with spatial
gradients but the potential energy operator O is associated with momentum gradients.
To find an equation of motion for s<N particles, decompose LN into operators containing
only the coordinates of the s particles, Ls, and those associated with the remaining (N −s) particles,
LN = Ls + LN−s, (4.22)
where (set N = s in Eq. (4.20))
Ls = −
s
i=1
Ki +s
i=1
s
j>i
Oij , (4.23)96 ■ Non-Equilibrium Statistical Mechanics
and
LN−s ≡ − 
N
j=s+1
Kj +s
i=1

N
j=s+1
Oij + 
N
i=s+1

N
j>i
Oij . (4.24)
There are two types of interactions in LN−s, those between the sets of s and (N − s) particles and
those among the (N − s) particles. There are 1
2 s(s + 1) terms in Ls and 1
2 [N(N + 1) − s(s + 1)]
terms in LN−s.
We seek an equation of motion for fs. From Eq. (4.5) with Λ=iL, we have, using Eq. (4.22),
 ∂
∂t − Ls

ρ = LN−sρ =

− 
N
j=s+1
Kj +s
i=1

N
j=s+1
Oij + 
N
i=s+1

N
j>i
Oij

ρ. (4.25)
Integrate Eq. (4.25) over the Γ-space variables (s + 1,...,N), leaving us with
 ∂
∂t − Ls

fs = −
 s
i=1

N
j=s+1
Fij ·
 ∂
∂pi
− ∂
∂pj

ρ d(s + 1)··· dN. (4.26)
The first and the third terms in square brackets in Eq. (4.25) vanish upon eliminating s + 1,...,N,
when it’s assumed that ρ vanishes at boundaries. Likewise the derivatives with respect to pj in Eq.
(4.26) contribute surface terms and vanish. We’re left with
 ∂
∂t − Ls

fs = −
s
i=1
∂
∂pi
·
 
N
j=s+1
Fijρ d(s + 1)··· dN
= −(N − s)
s
i=1
∂
∂pi
·

Fi,s+1fs+1 d(s + 1), (4.27)
where the last step follows by symmetry (ρ is a totally symmetric function of its arguments).
The set of equations in (4.27) for s = 1,...,N, the BBGKY hierarchy, is not any easier to solve
than the Liouville equation. To determine fs requires that we know fs+1, which in turn requires
that we know fs+2 and so on. It might appear that little has been accomplished by this exercise.
The virtue of the hierarchy is that it provides a means of introducing approximations so that the
hierarchy is truncated at a given order.11
Note that the reduced distributions fs are not constants of the motion along the phase trajectories
of s<N particles (the right side of Eq. (4.27) is nonzero) because of their interactions with the
remaining (N − s) particles. Consider the first equation of the hierarchy,
 ∂
∂t − L1

f1(r1, p1, t) = −(N − 1) ∂
∂p1
·

F12f2(r1, p1, r2, p2)d2. (4.28)
The rate of change of f1(t)is affected by the force F12 exerted on 1 by 2, weighted by the probability
of finding the pair at phase points 1 and 2. If the net force exerted on 1 is aligned with p1, the change
in f1(1, t) is negative as the particle gains speed (make sure you understand that). When we single
out for consideration a group of s<N particles, fs is not a constant of the motion; Liouville’s
theorem holds when Newton’s third law can be applied between all particles.
It’s often easier to work with the hierarchy cast in terms of s-tuple distributions. Multiply Eq.
(4.27) for s = 1 by N and that for s = 2 by N(N − 1) to find12
 ∂
∂t − L1

F1(1) = − ∂
∂p1
·

F12F2(1, 2)d2
 ∂
∂t − L2

F2(1, 2) = − ∂
∂p1
·

F13F3(1, 2, 3)d3 − ∂
∂p2
·

F23F3(1, 2, 3)d3. (4.29)
11Theoretical physics is the art of approximation.
12Apologies for notation: Bold Fij is the internal force on particle i from particle j; unbold Fs is an s-tuple distribution.Kinetic theory: Boltzmann’s approach to irreversibility ■ 97
4.4 HYDRODYNAMICS AND THE HIERARCHY
Liouville’s equation is not suited for practical calculations; working in 6N-dimensions is impossible
for N ≈ 1023. Maxwell and Boltzmann worked with one-particle probability densities f1(r, p, t),
and Boltzmann in particular derived an evolution equation for f1(r, p, t). Before taking up that all￾important topic (see Section 4.5), we stop to consider the question of how nonequilibrium statistical
mechanics connects microscopic with macroscopic. We had to address the analogous question in
statistical mechanics, which must reproduce the laws of thermodynamics—the macroscopic theory
of equilibrium—a demand met by the simple step of relating the parameter β in the stationary distri￾bution ρ(E) ≡ Z−1e−βEΩ(E) to the absolute temperature,13 β = (kT)−1 (see [5, p92]). There is
no comparably simple way in nonequilibrium theory of connecting microscopic with macroscopic
because of the variety of phenomena and the complexity of evolution processes; the connection must
be established in stages. In the sense that β = (kT)−1 ensures the second law of thermodynamics is
reproduced by statistical mechanics, which macroscopic phenomena must the nonequilibrium the￾ory get right? In analogy with quantum mechanics, what is the “hydrogen atom” of nonequilibrium
statistical mechanics? The equations of hydrodynamics embody local conservation of mass, mo￾mentum, and energy—physical principles the theory must obviously retain. Kinetic theory is tasked
with deriving the equations of hydrodynamics from first principles and of calculating transport co￾efficients in terms of molecular parameters.
In this section, we show, first, that energy conservation is contained in the first two equations of
the hierarchy, without having to know the three-particle distribution. We derive balance equations
for mass, momentum, and energy from the hierarchy, implying the equations of hydrodynamics
have a microscopic basis.14 We thereby obtain expressions for the pressure tensor and heat flux
(introduced in Chapter 1) in terms of molecular variables. Finally, we find the long-wavelength,
low-frequency dynamical modes of simple fluids from the equations of hydrodynamics.15
4.4.1 Densities macroscopic and microscopic
The basic fields of hydrodynamics are densities: mass, momentum, and energy. There are two uses
of the term density, however, depending on the level of description. Macroscopically, matter is
seen as a continuum, whereas microscopically (nanometer length scales) it appears as a collection
of discrete particles moving under the influence of mutual interactions. The natural mathematical
representation of physical quantities in the first case is with continuous functions (classical fields),
with their spatiotemporal behavior determined by partial differential equations. At the microscopic
level, systems are described in terms of many particle dynamics. Fundamentally motion is governed
by quantum mechanics, but classical mechanics is often a good approximation.
Classical microscopic densities are possible because (classically) one can say either there is a
particle at position r or there is not. We define a microscopic mass density function through the use
of Dirac delta functions,
ρˆ(r) ≡ m
N
α=1
δ(r − rα). (4.30)
Here r is a parameter, an arbitrary position in the system, rα is the canonical position variable of
the αth-particle, and the “hat” on ρˆ indicates microscopic. Equation (4.30) specifies an irregularly
fluctuating quantity and is a function only in the sense that delta functions are functions.16 Yet it
defines a density:  ρˆ(r)dr = Nm.
13And if we redefine the partition function to include a factor of N! for identical particles. 14Balance equations were introduced in Chapter 1 phenomenologically, based on what seems obvious from experience. 15A simple fluid is a fluid with only a single chemical component.
16The Dirac delta function δ(x) is a generalized function such that  ∞
−∞ δ(x)F(x) = F(0), for F a smooth function.
No ordinary function has this property; δ(x) has meaning only “inside” integrals. See Lighthill[99] or [13, p66].98 ■ Non-Equilibrium Statistical Mechanics
Let’s evaluate the ensemble average of ρˆ at (r, t) [see Eq. (4.1)]:
ρ(r, t) ≡ ⟨ρˆ(r)⟩t = m
N
α=1

dr1dv1 ··· drαdvαδ(r − rα)··· drN dvN ρ(1,...,N, t)
= m
N
α=1

drαdvαδ(r − rα)f1(rα, vα, t) = m
N
α=1

dvαf1(r, vα, t)
= Nm 
dvf1(r, v, t) = m

dvF1(r, v, t), (4.31)
where we’ve used that vα is a dummy variable and Eq. (4.17).17 Apart from the factor of mass,
ρ(r, t) is the zeroth velocity moment of F1(r, v, t). The macroscopic density ρ(r, t) is associated
with the same total mass as the microscopic density ρˆ(r),
 drρ(r, t) = Nm; ρ(r, t) “smoothes
out” the delta functions of ρˆ.
A local, microscopic momentum density (or mass flux) is defined similarly:
gˆ(r) ≡ m
α
vαδ(r − rα). (4.32)
Repeating analogous steps, the ensemble average of gˆ at (r, t), is
g(r, t) ≡ ⟨gˆ(r)⟩t = m

vF1(r, v, t)dv. (4.33)
The momentum density g(r, t) is the first velocity moment of F1(r, v, t). From the mass density
ρ(r, t), a velocity field u(r, t), the local velocity, can be inferred from the mass flux,
g(r, t) = ρ(r, t) 1
ρ(r, t)
g(r, t) = ρ(r, t)
 vF1(r, v, t)dv
 F1(r, v, t)dv ≡ ρ(r, t)u(r, t). (4.34)
Thus, u(r, t) is the mean velocity at (r, t).
18
Likewise we define the local energy density,19
εˆ(r) ≡ 
α

1
2mv2
α + 1
2

β̸ α
Φ(rα, rβ)

δ(r − rα), (4.35)
with ensemble average at (r, t),
ε(r, t) ≡ ⟨εˆ(r)⟩t = 1
2
m

dvv2F1(r, v, t) + 1
2

dvdr′
dv′
F2(r, v, r′
, v′
, t)Φ(r, r′
)
≡ εK(r, t) + εΦ(r, t). (4.36)
The kinetic energy density εK is the second velocity moment of F1; the potential energy density εΦ,
although not a moment, results from a spatial weighting of F2 by the two-body potential Φ(r, r′
).
The prescription just developed, of a correspondence through ensemble averages between micro￾scopic functions (ρ, ˆ gˆ, εˆ) and macroscopic densities (ρ, g, ε), works for mechanical quantities asso￾ciated with one or more particles. The correspondence is a mapping between phase space and physi￾cal space. Not every macroscopic quantity can be so represented, however—macroscopic quantities
exist that cannot be expressed as the average of a dynamical function weighted by a distribution
function. Entropy is not attached to the dynamics of single particles; it’s associated with the system
as a whole. The densities of mass, momentum, and energy, however, suffice to establish balance
equations for those quantities.
17These manipulations rely on the symmetry of joint probabilities; see Section 2.3.1. 18The quantity F1(r, v, t)/[
 F1(r, v, t)dv] defines a local (r-dependent) velocity probability distribution. 19Note that εˆ(r) is the total energy density; we’ll soon define the internal energy used in thermodynamics. A microscopic
form of the internal energy density is given in Chapter 6.Kinetic theory: Boltzmann’s approach to irreversibility ■ 99
4.4.2 Balance equations from the hierarchy
Balance equations for mass, momentum, and energy follow from moments of the first two equations
of the hierarchy.
4.4.2.1 Mass conservation
Equations (4.31) and (4.33) imply that the continuity equation holds at every (r, t),
∂
∂tρ(r, t) + ∇ · g(r, t) ≡ m

dv
 ∂
∂t + v · ∇
F1(r, v, t)=0, (4.37)
where the second equality follows from the zeroth velocity moment of the first equation of the
hierarchy when distribution functions are assumed to vanish at far-off surfaces in velocity space.
Using Eq. (P4.1), one sees that mass conservation holds in the presence of external forces.
4.4.2.2 Momentum balance, pressure tensor P
With mass conservation implied by the zeroth moment, let’s take the first velocity moment of the
first equation of the hierarchy. Operate on Eq. (P4.1) with  vdv:

v
 ∂
∂t + v · ∇r +
F
m
· ∇v

F1(r, v, t)dv (4.38)
= 1
m

dr′
dv′
[∇rΦ(r, r′
) · ∇vF2(r, v, r′
, v′
, t)]vdv.
This equation simplifies with the identities derived in Exercise 4.12; we find
∂
∂tg(r, t) + m

v (v · ∇r) F1(r, v, t)dv
+

dvdr′
dv′
F2(r, v, r′
, v′
, t)∇rΦ(r, r′
) = 1
m
ρ(r, t)F(r). (4.39)
On the left of Eq. (4.39) we have the rate of change of momentum density ∂g/∂t and on the right
the external force density, ρ(r, t)F(r)/m. To interpret the other terms, we note that momentum is
conserved in the absence of external forces and thus we expect Eq. (4.39) for F = 0 to be in the
form of a balance equation,
∂
∂t gi(r, t) +
3
j=1
∂
∂rj
Pij (r, t)=0, (i = 1, 2, 3) (4.40)
with Pij elements of a momentum flux tensor P, also known as the pressure tensor.
20 Let’s see how
that comes about. Readers uninterested in the details should jump to Eq. (4.49).
The vector field I(r) ≡  dvdr′
dv′
F2(r, v, r′
, v′
, t)∇rΦ(r, r′
) is the force density at r aris￾ing from interactions with other system particles. Such a force would vanish if F2 were spherically
symmetric in the separation ∆r ≡ r′ −r [if Φ is spherically symmetric,21 Φ(r, r′
) = Φ(|r′ −r|)].
The quantity I(r) is thus a measure of inhomogeneities in the system. To bring that out, we work
with a generalization of I involving an integration over a new spatial variable r1,
I(r) = 
dr′
h(r, r′
)∇rΦ(r, r′
) = 
dr′
dr1h(r1, r′
)δ(r1 − r)∇r1Φ(r1, r′
), (4.41)
20The dimension of P is, equivalently, pressure, energy density, or momentum flux (see Exercise 4.14). The stress tensor
T introduced in Eq. (1.17) can be used synonymously with P; there’s a minus sign difference in definition, T = −P. 21The dominant part of most intermolecular interactions is spherically symmetric.100 ■ Non-Equilibrium Statistical Mechanics
where h(r, r′
) ≡  dvdv′
F2(r, v, r′
, v′
, t). As a step we’ll use more than once, change variables
r′ → r1, r1 → r′
, use ∇rΦ(|r − r′
|) = −∇r′Φ(|r − r′
|), and symmetry22 h(r1, r′
) = h(r′
, r1)
to write
I(r) = 1
2

dr1dr′
h(r1, r′
) [δ(r1 − r) − δ(r′ − r)] ∇r1Φ(r1, r′
). (4.42)
In this form, I(r) is a measure of anisotropy in the internal force field.23 Is there a way to handle
the difference of two delta functions? Delta functions, although highly singular, nevertheless have
derivatives when defined “inside” integrals (as in Eq. (4.42)).24 For that reason, delta functions can
be treated as analytic functions possessing Taylor series:
δ(r − r1) = δ(r − r′
) − (r1 − r′
) · ∂
∂r
δ(r − r′
) + 1
2 (r1 − r′
) (r1 − r′
): ∂2
∂r2 δ(r − r′
) −··· ,
where we’ve used dyadic notation and that (∂/∂x)δ(x−y) = −(∂/∂y)δ(x−y) (the delta function
is even; see [13, p106]). Thus, for the terms in square brackets in Eq. (4.42),
δ(r − r1) − δ(r − r′
) = − (r1 − r′
) · ∂
∂r [Drδ(r − r′
)] , (4.43)
where Dr ≡ 1−(1/2) (r1 − r′
)·∇r +···+ (1/n!) [− (r1 − r′
) · ∇r]
n−1 +··· . Equation (4.43)
is due to Irving and Kirkwood.[100] Other approaches utilize Fourier transforms[101]. Combine
Eq. (4.43) with Eq. (4.42) and take Dr = 1 for simplicity,25
I(r) = 1
2

dr1dvdr′
dv′
F2(r1, v, r′
, v′
, t) [− (r1 − r′
) · ∇rδ(r − r′
)] ∇r1Φ(r1, r′
). (4.44)
From Eq. (4.40), we expect I to be in the form of the divergence of a second-rank tensor,
I ≡ ∇ · PΦ (Φ indicates the contribution of intermolecular forces), with i
th vector component
Ii = 
∇ · PΦ
i ≡ 
j ∂P Φ
ij /∂rj ; see Eq. (1.21). We infer the tensor elements from Eq. (4.44) after
a few steps,
P Φ
ij (r, t) = −1
2

dvdr′
dv′
F2(r, v, r′
, v′
, t)[∇rΦ(r, r′
)]i(r − r′
)j . (4.45)
PΦ(r, t) is symmetric (P Φ
ij = P Φ
ji ) when Φ(r, r′
) is spherically symmetric; see Exercise 4.15.
The remaining term in Eq. (4.39) is already in the form of the divergence of a tensor:
m

v(v · ∇)F1(r, v, t)dv = 
i
∂
∂ri

mvviF1(r, v, t)dv ≡ ∇ · PK,
with
P K
ij (r, t) = m

vivjF1(r, v, t)dv, (4.46)
where K indicates the kinetic contribution to P. The manifestly symmetric tensor field PK(r, t) is
associated with the flux of particle momentum, averaged over the one-particle distribution.
22A general property of joint phase-space probability distributions, F2(r1, v1, r2, v2, t) = F2(r2, v2, r1, v1, t). 23We can also write the integral, with r′ = r +∆r, I(r) = 1
2
 d(∆r)∇rΦ(|∆r|) [h(r, r + ∆r) − h(r, r − ∆r)].
24The nth-derivative of δ(x), δ(n)(x), has the property  ∞
−∞ δ(n)(x)F(x)dx = (−1)nF(n)(0) for any smooth func￾tion F(x); see Lighthill[99, p19]. 25Expressions featuring the full expansion with Dr ̸= 1 are given in Irving and Kirkwood[100]. Massignon[102, p32]
avoids these expansions and provides a more careful presentation on the use of delta function techniques.Kinetic theory: Boltzmann’s approach to irreversibility ■ 101
We now separate the contributions to PK into those from the mean velocity u(r, t) and from
“random” velocities measured relative to the mean—thermal velocities.
26 Using dyadic notation,
add and subtract u,
PK(r, t) = m

vvF1(r, v, t)dv = m

(v − u + u)(v − u + u)F1(r, v, t)dv
= ρ(r, t)u(r, t)u(r, t) + m

(v − u)(v − u)F1(r, v, t)dv
≡ ρ(r, t)u(r, t)u(r, t) + P K(r, t), (4.47)
with
PK
ij (r, t) = m

(v − u)i(v − u)jF1(r, v, t)dv. (4.48)
We define the pressure tensor P without the convection term ρuu, P ≡ P K + PΦ.
Equation (4.39) is therefore a momentum balance equation [see Eq. (1.20) noting that T = −P],
∂g
∂t + ∇ · (ρuu + P) = 1
m
ρF. (4.49)
The term ∇ · ρuu represents the rate of change in local momentum due to the mean motion of the
system; ∇ · P represents the rate of momentum change by means other than average flow, from
random motions about the mean velocity (P K) or internal forces (PΦ).
A stress system is hydrostatic if an element of area always has a stress normal to itself for any
surface orientation. The hydrostatic pressure, a scalar field, is defined as a third of the trace of P,
P(r, t) ≡ 1
3
Tr P(r, t) =m
3

(v − u)
2F1(r, v, t)dv
− 1
6

dvdr′
dv′
F2(r, v, r′
, v′
, t)∇rΦ(r, r′
) · (r − r′
). (4.50)
There’s a kinetic contribution to the pressure (present in all systems, significant in gases) and a
contribution from forces associated with two-body interactions (important in liquids).27
4.4.2.3 Energy balance total and internal, heat flux JQ
Operate on Eq. (P4.1) with 1
2m  dvv2 (second velocity moment) to derive a balance equation for
kinetic energy,
∂
∂t εK + ∇ · JK = −

dvdr′
dv′
F2(r, v, r′
, v′
, t)v · ∇rΦ(r, r′
) + 1
m
ρF · u, (4.51)
where we have the kinetic energy flux (third velocity moment of F1),
JK(r, t) ≡ m
2

vv2F1(r, v, t)dv. (4.52)
Equation (4.51) is a work-energy theorem: The rate at which kinetic energy changes is balanced by
the rate at which forces do work, internal and external.
Now operate on Eq. (4.29) (the second equation of the hierarchy) with 1
2
 dvdr′
dv′
Φ(r, r′
):
∂
∂t εΦ(r, t) + 1
2

dvdr′
dv′
Φ(r, r′
)

v · ∂
∂r
+ v′ · ∂
∂r′

F2(r, v, r′
, v′
, t)=0. (4.53)
26What we’re calling thermal velocities, astronomers refer to as peculiar velocities. 27Equation (4.50) justifies the reasoning adopted by van der Waals that pressure is lowered by attractive interatomic
forces; see [5, p191]. Conversely, pressure is increased by repulsive interactions, such as degeneracy pressure; see [5, p149].102 ■ Non-Equilibrium Statistical Mechanics
Contributions from O12 in L2 and the source terms on the right of Eq. (4.29) vanish in arriving
at Eq. (4.53); all velocity derivatives lead to vanishing surface terms when integrated over dvdv′
.
To simplify Eq. (4.53), use the identity Φ∂F2/∂r = ∂(F2Φ)/∂r − F2∂Φ/∂r. We then have the
balance equation for potential energy:
∂
∂t εΦ + ∇ · J Φ = 1
2

dvdr′
dv′
F2(r, v, r′
, v′
, t) [v · ∇r + v′ · ∇r′ ] Φ(r, r′
), (4.54)
where the potential energy flux is
J Φ(r, t) ≡ 1
2

dvdr′
dv′
F2(r, v, r′
, v′
, t)Φ(r, r′
)v. (4.55)
By adding Eqs. (4.51) and (4.54) and integrating over space, one can show that

dr
 ∂
∂t

εK + εΦ
+ ∇ · 
JK + J Φ

= 1
m

drρu · F, (4.56)
implying local energy balance (see Exercise 4.16),
∂
∂t ε + ∇ · Jε = 1
m
ρu · F, (4.57)
where ε ≡ εK + εΦ and Jε ≡ JK + J Φ. In the absence of external forces, energy conservation is
an exact consequence of the first two equations of the hierarchy (without having to know F3).
Internal energy, the province of thermodynamics, is a component of the total energy. It can be
isolated by separating the contributions to εK, JK, J Φ into those from the mean motion and from
thermal velocities (as we did with PK in Eq. (4.47)). It can be shown that:
εK = 1
2
ρu2 +
m
2

(v − u)
2F1(r, v, t)dv ≡ 1
2
ρu2 + εK;
JK = u · P K + u
1
2
ρu2 + εK

+
m
2

(v − u)(v − u)
2F1(r, v, t)dv
≡ u · P K + u
1
2
ρu2 + εK

+ JK;
J Φ = uεΦ +
1
2

dvdr′
dv′
(v − u)Φ(r, r′
)F2(r, v, r′
v′
, t) ≡ uεΦ + JΦ. (4.58)
Internal energy, denoted ε, is the sum of the average thermal kinetic energy and potential energy,28
ε≡ εK + εΦ = ε − 1
2
ρu2. (4.59)
The flux of internal energy, traditionally called the heat flux [see Eq. (1.30)], is defined
JQ ≡ JK + JΦ. (4.60)
By combining Eqs. (4.58)–(4.60) with Eq. (4.57), we have a rewrite of the energy balance equation,
∂
∂t 1
2
ρu2 + ε

+ ∇ · 
JQ + u · P K + u
1
2
ρu2 + ε
 = 1
m
ρu · F. (4.61)
The total energy flux has contributions from heat, work, and convected energy.
28In Chapter 1 we denoted the internal energy density ρu. We use ε here to avoid confusion with the average speed u.Kinetic theory: Boltzmann’s approach to irreversibility ■ 103
A balance equation for 1
2 ρu2 follows by projecting the momentum balance equation onto u,
u · ∂
∂t(ρu) + u ·∇· (P + ρuu) = 1
m
ρu · F, (4.62)
which is equivalent to [see Exercise 4.18]
∂
∂t 1
2
ρu2

+ ∇ · 1
2
ρu2u + u · P

= P:∇u +
1
m
ρu · F. (4.63)
By subtracting Eq. (4.63) from Eq. (4.61), we arrive at a balance equation for internal energy,
∂ε
∂t + ∇ · 
uε+ JQ − u · PΦ
= −P:∇u. (4.64)
Note that 1) internal energy does not couple to external forces and 2) convection of internal en￾ergy occurs naturally in the theory and we have a microscopic expression for the heat flux [see Eq.
(4.60)], physical effects introduced phenomenologically in Chapter 1.
29 We see explicitly that inter￾nal energy is not conserved; the source term −P:∇u represents a conversion of internal into kinetic
energy, it’s the work done by the fluid against pressure forces.
4.4.3 Microscopic balance equations
Balance equations for (ρ, g, ε) result from moments of the hierarchy, equations in the form of rela￾tions among ensemble averages. Yet, conservation of mass, momentum, and energy hold for each
member of an ensemble, implying that balance equations don’t require ensemble averaging to be
true. In this section, we show that the balance equations for (ρ, g, ε) follow from averages of balance
equations for the microscopic densities (ˆρ, gˆ, εˆ). These results will be used in Chapter 6.
Differentiate Eq. (4.30) with respect to time and Eq. (4.32) with respect to space, leaving us with
(vα ≡ r˙α),30
∂ρˆ
∂t = m
α
(−r˙α · ∇r)δ(r − rα) = −∇ · gˆ. (4.65)
The time rate of change of ρˆ is balanced by the negative divergence of gˆ. The ensemble average of
Eq. (4.65) reproduces Eq. (4.37); ⟨ρˆ(r)⟩t = ρ(r, t) and ⟨gˆ(r)⟩t = g(r, t).
Microscopic momentum balance follows by differentiating Eq. (4.32) with respect to time:
∂gˆi
∂t = m
α
[vα,i (−vα · ∇)+ ˙vα,i] δ(r − rα) ≡ −
j
∂
∂xj
PˆK
ij + ˆfi, (4.66)
where PˆK
ij (r) ≡ m
α vα,ivα,j δ(r − rα) are microscopic components of PK [⟨PˆK
ij (r)⟩t =
P K
ij (r, t)] and ˆfi(r) ≡ m
α v˙α,iδ(r − rα) are components of a microscopic force density, fˆ.
In vector form,
∂gˆ
∂t + ∇ · PˆK = fˆ. (4.67)
In identifying fˆ with force, we’ve used Newton’s second law wherein acceleration is associated
with force when observed from inertial reference frames.31 The force could be external or internal
in origin, however; let’s write fˆ = fˆint + fˆext.
29Internal energy is motivated in thermodynamics as the kinetic energy of thermal agitations and the potential energy
associated with interatomic forces, a picture that emerges here from microscopic dynamics. Heat flux was defined in Chapter
1 as “whatever’s left over” in accounting for different types of energies, with no analytic way of characterizing it other than
giving it a name. Here have an expression for JQ(r, t) starting from first principles (Liouville’s equation). 30To show that (∂/∂t)δ(r − rα) = −(vα · ∇r)δ(r − rα), note that δ(r) ≡ δ(x)δ(y)δ(z); see [13, p261]. 31We’ve implicitly used inertial frames in our development. A careful discussion of inertial frames is given in [6, p4].104 ■ Non-Equilibrium Statistical Mechanics
For internal forces derivable from two-body interactions Φ(rα, rβ) (β ̸= α in these sums),
ˆfint
i (r) ≡ 
α,β
δ(r − rα) [−∇rα Φ(rα, rβ)]i = 1
2

α,β
[δ(rα − r) − δ(rβ − r)] [−∇rα Φ(rα, rβ)]i
= −1
2

α,β
[−∇rα Φ(rα, rβ)]i (rα − rβ) · ∂
∂r
δ(r − rβ) ≡ −
j
∂
∂rj
PˆΦ
ij = −

∇ · Pˆ Φ
i
,
where the second equality follows as the discrete version of Eq. (4.42) (through similar steps), the
third uses Eq. (4.43) with Dr = 1, and PˆΦ
ij are microscopic components of PΦ [⟨PˆΦ
ij (r)⟩t =
P Φ
ij (r, t)],
PˆΦ
ij (r) ≡ −1
2

α,β
[∇rα Φ(rα, rβ)]i (rα − rβ)j δ(r − rα). (4.68)
Thus, fˆint = −∇ · Pˆ Φ, implying microscopic momentum balance,
∂gˆ
∂t + ∇ · Pˆ = fˆext, (4.69)
where Pˆ ≡ PˆK + Pˆ Φ and fˆext(r) ≡ 
α Fext(rα)δ(r − rα) is the microscopic force density
associated with external forces; ⟨fˆext(r)⟩t = (1/m)ρ(r, t)Fext(r).
Microscopic energy balance follows by differentiating Eq. (4.35) with respect to time,
∂
∂t εˆ(r) =
α

1
2
mv2
α +
1
2

β̸=α
Φ(rα, rβ)

(−vα · ∇) δ(r − rα)
+
α
δ(r − rα)

mvα · v˙α +
1
2

β̸=α
(vα + vβ) · ∇rα Φ(rα, rβ)

. (4.70)
Through familiar steps, Eq. (4.70) has the form of a balance equation,
∂εˆ
∂t + ∇ · Jˆ
ε = ˆw, (4.71)
where Jˆ
ε is the total energy flux and wˆ is the power per volume expended by external forces,
Jˆ
ε(r) ≡

α

vα
1
2
mv2
α +
1
2

β̸=α
Φ(rα, rβ)

+
1
4

β̸=α
(vα + vβ) · ∇rα Φ(rα, rβ) (rα − rβ)

δ(r − rα)
wˆ(r) ≡

α
vα · Fext(rα)δ(r − rα). (4.72)
One can show[103, p200] that ⟨Jˆ
ε(r)⟩t = Jε(r, t) and ⟨wˆ(r)⟩t = (1/m)ρ(r, t)u(r, t) · Fext(r).
4.4.4 Hydrodynamics: Adding constitutive relations to conservation laws
The equations of hydrodynamics in their most basic form are the balance equations for mass, mo￾mentum, and internal energy. We collect them here,
∂
∂tρ + ∇ · (ρu)=0
∂
∂tρu + ∇ · (ρuu + P)=0
∂
∂t ε+ ∇ · (uε+ J) = −P:∇u, (4.73)Kinetic theory: Boltzmann’s approach to irreversibility ■ 105
where J includes all contributions to the heat flux. These equations are incomplete, however. To
be practical, they must be supplemented with constitutive relations between the fluxes (P, J) and
the fields (ρ,u, ε). We have definitions of (P, J) involving the one and two-particle distributions
but such expressions are formal unless we can calculate the reduced distributions (the job of kinetic
theory). The traditional way of closing the equations of hydrodynamics is to parameterize P and J.
In equilibrium, spatially uniform32 and time independent, u = 0, J = 0, and P is diagonal,
Pij = P δij , with P the hydrostatic pressure. For out-of-equilibrium systems, J ̸= 0 and P has
off-diagonal elements, effects associated with deviations from uniformity. Write P so that the con￾tributions of inhomogeneities add to the equilibrium form,
Pij = P δij + Πij , (4.74)
where the tensor elements Πij model momentum fluxes mediated by inhomogeneities. Deviations
from uniformity are, in a first approximation, represented by linear functions of gradients in the
local fields33 (ρ,u, T). We take Πij to be a linear combination of velocity gradients,
Πij ≡ 
l,m
Alm
ij ∇lum, (4.75)
where Alm
ij are expansion coefficients. Any second-rank tensor B can be written as a sum of sym￾metric traceless, antisymmetric, and diagonal tensors:
Bij = 1
2

Bij + Bji − 2
3 δij Tr B

+ 1
2 (Bij − Bji) + 1
3 δij Tr B. (4.76)
Because we require P to be symmetric (conservation of angular momentum, Exercise 4.19), the
simplest form of the expansion in Eq. (4.75) is that of Eq. (4.76),
Πij = −η

∇iuj + ∇jui − 2
3 δij∇ · u

− ζδij∇ · u (4.77)
where η and ζ are the coefficients of shear and bulk viscosity.34 Likewise we represent heat flux in
terms of gradients,
J = −λ∇ρ − κ∇T, (4.78)
where κ is the thermal conductivity and λ has no standard name. We show presently that λ = 0.
For simple fluids the entropy source has the form35
σS = − 1
T2 J · ∇T − 1
T
Π:∇u; (4.79)
Combining Eqs. (4.77)–(4.79), we find
σS = λ
T2 ∇ρ ·∇T +
κ
T2 |∇T|
2 + η
2T

ij

∇jui + ∇iuj − 2
3 δij∇ · u
2
+
3ζ
T (∇ · u)
2 . (4.80)
Thus, the inequality σS ≥ 0 implies κ ≥ 0, η ≥ 0, ζ ≥ 0. These sign requirements are as general
as the thermodynamic stability conditions, CV ≥ 0 and βT ≥ 0; see [2, p48] or [5, p26]. Because
∇ρ · ∇T can be of either sign, we set λ = 0 to have a theory manifestly consistent with the second
law of thermodynamics.36 The hydrodynamics of simple fluids involves three transport coefficients,
32By uniform we mean isotropic and homogeneous—all directions in space equivalent with no preferred spatial location.
These concepts are distinct: Isotropy does not necessarily imply homogeneity nor does homogeneity necessarily imply
isotropy; see [6, p45]. Homogeneity is implied if the system is isotropic at every point in space. 33The equations in (4.73) have (ρ, u,ε) as the basic fields. We’ll use thermodynamics to relate T to ε. 34Equation (4.77) is the same as Eq. (1.18) up to a minus sign; the sign difference between stress and pressure tensors.
35Equation (4.79) differs from Eq. (1.34) in the sign of Π; compare Eqs. (1.19) and (4.74) noting that T = −P. 36In relativistic fluid dynamics, λ ̸= 0 and is not independent of κ; see Landau and Lifshitz[15, p506].106 ■ Non-Equilibrium Statistical Mechanics
(κ, η, ζ). Note that, because the precise forms of the pressure tensor and heat flux have been sub￾sumed into the symbols P and J, the equations of hydrodynamics obtained from the conservation
laws in (4.73) hold independently of the microscopic forms of P and J. See Exercise 4.32.
Combining the parameterized expressions for P, J with the equations in (4.73), we arrive at a
form of the hydrodynamic equations (see Exercise 4.20),
∂ρui
∂t = −

j
∇j (ρuiuj ) − ∇iP + η∇2ui + (ζ + 1
3 η)∇i(∇ · u) (4.81)
∂ε
∂t = −∇ · (uε) − P∇ · u + κ∇2T + η

ij
[∇iuj + ∇jui] ∇iuj + (ζ − 2
3 η)(∇ · u)
2,
two equations in the five unknowns ρ,u, P, ε, T  (three equations including the continuity equation).
The number of variables can be reduced assuming local equilibrium with the concomitant local
equations of state P(r) = P[T(r), ρ(r)], ε(r) = ε[T(r), ρ(r)] having the same functional form as
in global equilibrium, P = P(T,ρ), ε = ε(T,ρ). That leaves ρ,u, T as the basic variables.37 We
now further assume that temporal variations of P, εoccur through the variations of38 T(r, t), ρ(r, t),
e.g., P(r, t) = P[T(r, t), ρ(r, t)]. With these assumptions,
∇P = (∂P/∂T)ρ ∇T + (∂P/∂ρ)T ∇ρ, (4.82)
where the thermodynamic derivatives have known connections with measurable quantities; see Ex￾ercise 4.22. For variations of internal energy, express εas the product of mass density ρ and specific
internal energy: let39 ε→ ρ(ρ−1ε) ≡ ρu. We require the space and time derivatives of u,
 ∇
∂t

u =
 ∂u
∂T 
ρ
 ∇
∂t

T +
∂u
∂ρ
T
 ∇
∂t

ρ, (4.83)
where ∂t ≡ ∂/∂t. It’s straightforward to show that (see Exercise 4.22)
ρ2
∂u
∂ρ
T
= −T
∂P
∂T 
ρ
+ P
 ∂u
∂T 
ρ
= cv, (4.84)
where cv is the specific heat at constant volume, the heat capacity CV per mass.
Combining these expressions with Eq. (4.81), we find, including the continuity equation,
∂tρ = − u · ∇ρ − ρ∇ · u
∂tui = − (u · ∇)ui − 1
ρ
 ∂P
∂T 
ρ
∇iT +
∂P
∂ρ 
T
∇iρ

+
1
ρ

η∇2ui + 
ζ + 1
3 η

∇i (∇ · u)

∂tT = − u · ∇T − T
ρcv
∂P
∂T 
ρ
∇ · u
+
1
ρcv

κ∇2T + η

ij
[∇iuj + ∇jui] ∇iuj + (ζ − 2
3 η)(∇ · u)
2

. (4.85)
We now have five equations in the five fields ρ(r, t),u(r, t), T(r, t). These are the most general
hydrodynamic equations40 consistent with the assumption of local thermodynamic equilibrium and
our parameterizations of P and J.
37State variables are not independent of each other; much of the mathematical apparatus of thermodynamics is devoted
to exposing their interrelations. We’re free to assume the existence of functional relations among state variables[2, p12]. 38This seemingly strong assumption is justified in the normal solutions of the Boltzmann equation; see Section 4.8 39Apologies for notation. In Section 1.2 we denoted the specific internal energy u (a common notation) and in this chapter
we used u for the mean local velocity (also a common notation). Here we indicate specific internal energy u. 40One might ask: What is hydrodynamics? Is it the set of equations in (4.73), or those in (4.85)? Practically speaking,
hydrodynamics = conservation laws + constitutive relations, the equations in (4.85).Kinetic theory: Boltzmann’s approach to irreversibility ■ 107
4.4.5 Linearized hydrodynamics, normal modes
The equations in (4.85) are quite complicated and have a rich variety of solutions because of
their nonlinear character. Our purpose is not to provide a detailed account of fluid dynamics (see
Batchelor[12] or Landau and Lifshitz[15]), but to develop the essential features that contribute to
our understanding of kinetic theory. There is a regime in which these equations simplify, where ρ
and T deviate slightly from their equilibrium values. We indicate this with the scheme
ρ(r, t) = ρ + δρ(r, t)
u(r, t)=0+ δu(r, t)
T(r, t) = T + δT(r, t). (4.86)
Small deviations (δρ, δT) are expected to generate small velocity variations δu around the equilib￾rium value of zero, and vice versa. Substituting (4.86) in (4.85) and keeping terms linear in small
quantities, we have41
∂tδρ = − ρ∇ · δu
∂tδu = − 1
ρ
 ∂P
∂T 
ρ
∇δT +
∂P
∂ρ 
T
∇δρ
+
1
ρ

η∇2δu + 
ζ + 1
3 η

∇ (∇ · δu)

∂tδT = − T
ρcv
∂P
∂T 
ρ
∇ · δu +
κ
ρcv
∇2δT. (4.87)
These are the equations of linearized hydrodynamics.
42 A key result is that they contain all the
transport coefficients of the nonlinear theory. The problem of determining transport coefficients is
therefore well-posed for linear systems.
We seek normal-mode solutions of these equations which (as with any normal-mode analysis)
can be used to represent arbitrary small amplitude deviations from equilibrium. For simplicity we
consider a system infinite in spatial extent to avoid the complication of boundary conditions. We
seek solutions in the form
δρ(r, t) = ρk exp(ik · r + ωt)
δu(r, t) = uk exp(ik · r + ωt)
δT(r, t) = Tk exp(ik · r + ωt), (4.88)
where we allow that ω can be complex-valued and there’s no restriction on k (infinite system).
The amplitudes ρk,uk, Tk are Fourier transforms of δρ, δu, δT. Substituting (4.88) in (4.87) with
∂t → ω and ∇ → ik, we have a system of linear homogeneous equations,
ωρk + iρk · uk = 0
iBkρk + 
ω + ϕk2
uk + χk(k · uk)+iAkTk = 0
iCk · uk + 
ω + σk2
Tk = 0, (4.89)
where A, B, C denote equilibrium properties and ϕ, χ, σ denote nonequilibrium properties,
A ≡ 1
ρ
∂P
∂T 
ρ
B ≡ 1
ρ
∂P
∂ρ 
T
C ≡ T
ρcv
∂P
∂T 
ρ
ϕ ≡ η
ρ
χ ≡ 1
ρ

ζ + 1
3 η
 σ ≡ κ
ρcv
.
41The term κ/(ρcv) in the temperature equation is known as the thermal diffusivity, sometimes denoted DT . The ratio
η/ρ is known as the kinematic viscosity, with η itself the dynamic viscosity. Note that κ/(ρcv) and η/ρ have the same
dimensions as the diffusion coefficient, length2/ time. 42We note that another fundamental theory of physics featuring nonlinear field equations, the general theory of relativity,
has a linearized regime with numerous physical consequences, such as gravitational waves[6, Chapter 18].108 ■ Non-Equilibrium Statistical Mechanics
Do the equations of linearized hydrodynamics admit solutions in the form of (4.88)? Not for
arbitrary ω. For a square system of linear homogeneous equations to have nontrivial solutions, the
determinant of the coefficient matrix must vanish, a requirement that imposes a connection between
ω and k, the dispersion relations, ωi = ωi(k), i = 1,..., 5. A full 5 × 5 coefficient matrix is
implied by the equations in (4.89). Those equations simplify when the coordinate system is oriented
with one axis parallel to k. We’re free to do that without loss of generality; such transformations
do not affect eigenvalues. Let k = kxˆ with ky = kz = 0. In that case, we seek the characteristic
polynomial associated with










ω iρk 000
iBk ω + (ϕ + χ)k2 0 0iAk
0 0 ω + ϕk2 0 0
00 0 ω + ϕk2 0
0 iCk 0 0 ω + σk2










= 0. (4.90)
Expanding the determinant, the allowed frequencies are the roots of the quintic function in ω,

ω + ϕk22 
ω3 + ω2k2(ϕ + χ + σ) + ω[σ(ϕ + χ)k4 + k2(Bρ + AC)] + ρBσk4 = 0.
Clearly there is a doubly degenerate root (which we refer to as ω3, ω4),
ω3 = ω4 = −ϕk2 = −(η/ρ)k2. (4.91)
The remaining allowed frequencies are found from the roots of the cubic function in curly braces.
That (arduous) path can be avoided by observing that all roots coalesce to zero as k → 0. Because
we’re interested in the slowest processes, we want only the small-k form of ω(k). Rather than seek
exact expressions, let’s try to find approximate solutions,
ωi(k) = aik + bik2 + O(k3). (i = 1, 2, 5) (4.92)
The unknown coefficients (ai, bi) are determined by matching powers of ωnkm for n + m ≤ 4
when Eq. (4.92) is substituted in the left side of
(ω−ω1)(ω−ω2)(ω−ω5) = ω3+ω2k2(ϕ+χ+σ)+ω[σ(ϕ+χ)k4+k2(Bρ+AC)]+ρBσk4. (4.93)
By this procedure (see Exercise 4.23) we find the long-wavelength forms shown in Table 4.1. The
Table 4.1 Frequencies of long-wavelength hydrodynamic modes
Dispersion relation Mode
ω1,2 = ±icsk − Γk2 + O(k3) Sound
ω3,4 = −η
ρ
k2 Shear
ω5 = − κ
ρcp
k2 + O(k3) Thermal
complex roots ω1,2 describe propagation in two opposite directions (along and against k, the longi￾tudinal direction), at the speed of sound43
cs ≡
∂P
∂ρ 
T
cp
cv
, (4.94)
43Equation (4.94) is equivalent to cs =
(∂P/∂ρ)S, the adiabatic sound speed (see [2, p24]). There is no dissipation
(constant entropy) associated with undamped sound propagation (set transport coefficients to zero in Eq. (4.80)).Kinetic theory: Boltzmann’s approach to irreversibility ■ 109
with damping coefficient
Γ ≡ 1
2ρ

4
3
η + ζ + κ
 1
cv
− 1
cp
 . (4.95)
The sound attenuation rate Γ > 0 has contributions from the viscosity η, the bulk viscosity ζ, and
the heat conductivity κ. The roots ω3,4,5 are purely damped excitations.
4.5 THE BOLTZMANN EQUATION, MOLECULAR CHAOS
The issue with the hierarchy isn’t lack of computational resources, as it is with Liouville’s equation,
it’s that it isn’t closed. The simplest possibility would be for the hierarchy to be truncated at s = 1.
A closed evolution equation for the one-particle distribution f1 is important enough to get its own
name, a kinetic equation,
44 which has the form45
 ∂
∂t − L1

f1(r, p, t) =  ∂
∂t + (p/m) · ∇    free
streaming

f1(r, p, t) ≡ Dvf1(r, p, t) = J[f1]    collision
operator
, (4.96)
where Dv is the convective derivative (v = p/m). The operator L1 = −(p/m) · ∇ appears in
the first equation of the hierarchy [see Eq. (4.29)] and is associated with the motion of independent
particles. The collision operator J[f1] (a functional46) is associated with the rate of change of f1 due
to particle interactions, referred to as collisions. The right side of Eq. (4.96) effectively represents
the rest of the hierarchy; it therefore can’t be found as an exact consequence of the equations of
motion, it must be devised from approximations that capture the physics of particle trajectories
modified by interactions. An extreme approximation to Eq. (4.96), neglecting J, would apply to
systems of such low density that encounters between molecules are so rare as to be negligible.
Although there are gases like that,47 we consider the kinetic equation established by Boltzmann, the
Boltzmann transport equation, that treats the more representative problem of particle interactions
through collisions.48
4.5.1 Derivation
Assume that 1) the intermolecular potential is short ranged—there is a distance r0 beyond which
the potential rapidly approaches zero—and 2) the system is sufficiently dilute that the average dis￾tance between particles far exceeds r0. For the average number density n ≡ N/V , the diluteness
criterion49 is nr3
0 ≪ 1. Under these conditions particles move with constant velocities v until they
encounter other particles closely enough that they deflect over short time intervals τc ≈ r0/v (the
microscopic collision time, see Exercise 4.25) after which free motion resumes with velocities v′
.
Simultaneous encounters of three or more particles occur so infrequently as to be negligible. Two￾body encounters can be seen as discrete events confined to a small volume r3
0, occurring over a short
time τc, in which changes in free-state velocities occur almost abruptly, v → v′
; see Fig. 4.1.
44Although the Fokker-Planck equation (3.37) is a closed equation in the one-particle distribution, it’s a linear differential
equation. Kinetic equations are closed nonlinear equations for f1; see Section 4.5.2.5. 45Equation (4.96) ignores the effects of external forces on the streaming (left) side of the equation; see Exercise 4.7.
46Functionals map functions to numbers, often by way of integrals, J[y] ≡  F(y(x))dx where F is known. In quantum
mechanics we encounter linear functionals—bras in Dirac notation; in kinetic theory functionals are often nonlinear. 47A Knudsen gas is one in which the mean free path exceeds the size of the container, a state of matter important in
nanometer-scale systems[104]. 48As one soon discovers, Boltzmann’s written output is voluminous; English translations of select articles are available
in Brush[89]. A valuable resource is Lectures on Gas Theory[105], described by M. Kac[42, p261] as “one of the greatest
books in the history of exact sciences.” Other references are [106, Chapter 3] and [107, Chapter 4]. The unadorned term
“Boltzmann equation” is ambiguous—it could refer to the Boltzmann entropy formula or the Boltzmann factor, exp(−βE),
or to a kinetic equation—hence the more precise term Boltzmann transport equation. 49The same criterion appears in the equilibrium theory of dilute gases, in the virial expansion[5, Section 6.2].110 ■ Non-Equilibrium Statistical Mechanics
interaction
region
v′
1
v′
v1
v time →
v1
v
v′
1
v′ time →
inverse collision
Figure 4.1 Two-body collisions mediated by short-range forces. Left: Direct collision.
Right: Inverse collision (role of initial and final states interchanged).
Under these assumptions, the dynamics of dilute-gas particles reduce to a series of two-body
encounters, and two-body problems (for motion in central force fields) reduce to the dynamics of
single particles (see Appendix C). Because before and after-collision velocities pertain to free par￾ticles, and because all gas particles have the same mass, for particles of velocities v, v1 interacting
to produce velocities v′
, v′
1 we have the conservation laws,
v + v1 = v′ + v′
1 v2 + v2
1 = v′2 + v′2
1 . (4.97)
The four scalar equations implied by (4.97) are not sufficient to determine the six scalar quantities
comprising the after-collision velocities; a knowledge of the force law is required for a unique
solution.50 Expressions for v′
, v′
1 that solve the equations in (4.97) can be written
v′ = 1
2 (v + v1 − gϵˆ) v′
1 = 1
2 (v + v1 + gϵˆ), (4.98)
where (for now) ϵˆ is an arbitrary unit vector51 and g ≡ |g| denotes the magnitude of the pre-collision
relative velocity,
g ≡ v1 − v. (4.99)
The effect of an elastic collision is to rigidly rotate the relative velocity (see Fig. 4.2, use Eq. (4.98)),
g′ ≡ v′
1 − v′ = gϵˆ. (4.100)
Thus, ϵˆ is aligned with g′
; to determine ϵˆ we must solve for the dynamical motion using the force
law between particles. The calculation of the scattering angle is reviewed in Appendix C.
scattering center
g
g′ = gϵˆ
θ g
gϵˆ
θ
Figure 4.2 Elastic scattering. The relative velocity g prior to the collision is rotated through
the scattering angle θ to the relative velocity after the collision g′
, with |g′
| = |g|.
The Boltzmann equation, an equation of motion for f(r, v, t), the number density at phase
point52 (r, v) at time t, can be derived by analyzing how the number of particles in state (r, v)
50Only in one dimension do conservation laws determine motion. In d-dimensions, there are d + 1 scalar equations in
(4.97), yet there are 2d scalar unknowns in the after-collision velocities; d +1=2d only for d = 1. 51The arbitrariness of ϵˆ underscores the indeterminism of the conservation laws in specifying the motion. 52As noted in footnote 9 of this chapter, we often work with a phase space spanned by r, v. We also drop the subscript
on f1. It’s conceptually simpler in the derivation of the Boltzmann equation to let f(r, v, t)drdv denote the number ofKinetic theory: Boltzmann’s approach to irreversibility ■ 111
changes in time, from flows at velocity v or from changes in v through collisions. The particles that
flow to (r, v) are those that ∆t seconds ago53 were in state (r − v∆t, v). The change in particle
number at (r, v) by means of streaming is therefore54
[f(r − v∆t, v, t) − f(r, v, t)] drdv ≈ −∆t(v · ∇f) drdv. (4.101)
Dividing by ∆t (to give us a rate), we see that the streaming term in Eq. (4.96) (with p = mv)
represents a balance between gains and losses from state (r, v) by means of flow.
Let’s apply gain-loss reasoning to the collision term, the new wrinkle in the Boltzmann equa￾tion.55 Keeping our emphasis on the rates of change of particle numbers in (r, v), we write
drdv
 ∂
∂t + v · ∇
f(r, v, t)=(G − L) drdv, (4.102)
where G and L are gain and loss rates per µ-space volume at (r, v). Collisions affect the rate of
change of f(r, v, t) in two ways: 1) among particles with velocity v, some undergo collisions,
resulting in a loss from state (r, v), and 2) among particles with velocities v′
1, v′
, some undergo
collisions such that one of the particles has final velocity v, a gain. We symbolize the loss collision
(v1, v) → (v′
1, v′
) and the gain collision (v′
1, v′
) → (v1, v); see Fig. 4.1. The (real-space) volume
containing particles of impact parameters between b and b + db (see Fig. C.6) and velocity56 g =
|v1−v| that will scatter in ∆t seconds is 2πbdbg∆t. Thus the number of particles about to encounter
a scatterer is 2πbdbg∆tf(r, v1, t)dv1 and there are f(r, v, t)drdv scatterers within dvdr of(r, v).
The total loss rate from collisions is therefore
Ldrdv =
 
dv12πbdbgf(r, v1, t)f(r, v, t)

drdv. (4.103)
The gain term is handled similarly. In the inverse collision (v′
1, v′
) → (v1, v), the roles of initial
and final states are interchanged from the direct collision (see Fig. 4.1).57 Applying the reasoning
leading to Eq. (4.103) to the inverse collision, we have the total gain rate from collisions,
Gdrdv =
 
dv′
dv′
12πbdbg′
f(r, v′
1, t)f(r, v′
, t)

dr
=
 
dv12πbdbgf(r, v′
1, t)f(r, v′
, t)

drdv, (4.104)
where we’ve applied the integral invariant of Poincar´e,[108, p73] dv′
1dv′ = dv1dv and we’ve set
g′ = g (elastic scattering).
Combining Eqs. (4.102)–(4.104), we arrive at the Boltzmann transport equation,58
 ∂
∂t + v · ∇
f(r, v, t) = 
dv12πbdbg [f(r, v′
1, t)f(r, v′
, t) − f(r, v1, t)f(r, v, t)] . (4.105)
It can be written equivalently in terms of the cross section σ(Ω) for scattering into the differential
solid angle dΩ about Ω (see Eq. (C.6)),
 ∂
∂t + v · ∇
f(r, v, t) = 
dv1dΩσ(Ω)|v1 − v| [f(r, v′
1, t)f(r, v′
, t) − f(r, v1, t)f(r, v, t)] .
(4.106)
particles within drdv of (r, v) at time t, rather than probability. It’s a matter of normalization—probability densities or
particle densities. By “at” (r, v), we mean within drdv of (r, v). 53We leave the time interval ∆t unspecified for now. We’ll see that it must satisfy inequality (4.107). 54The particles in state (r, v) at time t flow to (r + v∆t, v) in the next ∆t seconds. 55The master equation (2.66) also has the gain-loss form.
56Figure C.6 is in the reference frame of the particle with velocity v; see also Fig. C.5. 57The inverse collision is not the time-reversed collision, which in our notation would be (−v′
, −v′
) → (−v, −v). In
the older literature, the inverse collision is called the restituting collision. 58In an external force field F there is an additional term on the left side, (F/m) · ∂/∂v; see Exercise 4.7.112 ■ Non-Equilibrium Statistical Mechanics
The collision operator J[f] is the expression on the right side of Eq. (4.106). Boltzmann’s equation
is a nonlinear integro-differential equation. The variables (r, v, t) are specified on the left side of
the equation, we’re summing over v1, and (v′
1, v′
) are determined by (v1, v) through the scattering
cross section (see Appendix C). We cannot in the space of this book consider applications (see
Cercignani[109]). It’s used in disparate fields of research: astrophysics,[110] neutron transport,[111]
traffic flow,[112] and charge transport in solids,[113] to name just a few. It’s one of the fundamental
equations of nonequilibrium statistical mechanics. Despite its history, it remains far from fully
understood.59 We offer some remarks.
4.5.2 Comments
4.5.2.1 Derivation
Our derivation, although physically motivated, is ad hoc and not part of a systematic scheme.
It would be illuminating if the Boltzmann equation could be derived from the first equation of
the BBGKY hierarchy, where the assumptions underlying such a derivation would be expected
to present themselves explicitly. Attempts have been made by Bogoliubov,[94] Kirkwood,[98]
Grad,[115] and Prigogine[116, Section 2.5]. We lack the space to analyze these approaches; see
Liboff[117].
4.5.2.2 Dilute gases
The Boltzmann equation applies to dilute gases. The diluteness criterion nr3
0 ≪ 1 is sufficiently
embedded in the concepts underlying the derivation, that relaxing this assumption would require a
major revamping of the ideas involved. It ensures that collisions are localized events in space and
time; it allows us to conceive of particle interactions as a succession of binary collisions and to
calculate the velocities (v′
1, v′
) from the solution of a two-body problem. The extension to dense
gases has occupied the attention of many researchers, and we can only refer to reviews: Cohen[118],
Dorfman and van Beijeren[119], Mazenko and Yip[120], and Brush[121].
4.5.2.3 Irreversibility, molecular chaos assumption
The Boltzmann equation is not time-reversal symmetric. The left side of the equation reverses sign
under t → −t and the right side does not.60,61 Thus, if f(r, v, t) is a solution of the Boltzmann
equation, f(r, −v, −t) is not, in contrast to solutions of the Liouville equation (see Exercise 4.3).
Although we see in the finished product that the Boltzmann equation is not time-reversal symmetric,
where in its derivation did we depart from the laws of mechanics?
In loss collisions (v1, v) → (v′
1, v′
), v1 can be chosen independently of v; the colliding parti￾cles are uncorrelated. In gain collisions, the velocities of the colliding particles are constrained to
satisfy the conservation law v′ + v′
1 = v1 + v; one can’t freely vary v′
1 and v′
. Collisions cre￾ate correlations. Yet we have in the gain term Eq. (4.104) a product of one-particle distribution
functions befitting uncorrelated particles.62 It’s at this point we’ve departed from the laws of me￾chanics: v′
1, v′ follow deterministically from the force law, yet we’re treating them as statistically
59See the articles in [114] on such basic questions as the mathematical existence of solutions to the Boltzmann equation. 60We found the same in the Fokker-Planck equation (3.59) with Ω a linear operator. 61The scattering cross section is the same for direct, inverse, and time-reversed collisions. Equality of cross sections for
the direct and time-reverse collisions follows from the time-reversal invariance of the equations of motion; the conservation
laws in Eq. (4.97) are the same if velocities are replaced with their time-reversed versions. Equality of cross sections for the
inverse and direct collisions follows because one can read the conservation laws from left to right, or from right to left. The
direct, inverse, and time-reverse collisions are kinematically equivalent. 62The number of pairs of particles located simultaneously at different phase points is specified by the two-body distribu￾tion function f2(r, v, r, v′
, t). In setting f2(r, v, r, v′
, t) = f1(r, v, t)f1(r, v′
, t), correlations are ignored.Kinetic theory: Boltzmann’s approach to irreversibility ■ 113
independent.63 Boltzmann referred to uncorrelated after-collision particles the “molecular disor￾dered state,”[105, pp40–42] known today as the molecular chaos assumption,
64 a statistical rather
than a mechanical assumption.65 Boltzmann therefore did not derive irreversibility from time￾symmetric physics, it was introduced into the theory as an artifact of ignoring correlations. Despite
its extra-mechanical nature, Boltzmann’s equation makes predictions in good agreement with ex￾perimental results. There’s no question that the Boltzmann equation captures the physics of dilute
gases.
4.5.2.4 Markovian character, spatial inhomogeneity
The Boltzmann equation has irreversibility in common with Markov processes; is it Markovian?
That requires we say something about the time ∆t (upon which no constraints have been placed
up to now). It must be shorter than the relaxation time τr, the time over which f(r, v, t) changes
appreciably, and it must be longer than the collision time τc, the time to turn g into gϵˆ (see Fig. 4.2),
τc ≪ ∆t ≪ τr. (4.107)
The Boltzmann equation can’t apply to times shorter than the microscopic time τc. The diluteness
criterion (underlying the theory) implies a clear separation in time scales: nr3
0 ≪ 1 =⇒ τc/
τr ≪ 1. The Boltzmann equation is not defined in the opposite limit τr ≪ τc.
When, in analyzing the collision (v1, v) → (v′
1, v′
), we counted the number of particles of
velocity v1 about to collide with particles of velocity v (at r), we assumed that f(r, v1, t) does not
change much during ∆t (because ∆t ≪ τr) and hence we used f(r, v1, t) evaluated at the same
time as the number of scatterers, f(r, v, t)drdv. If ∆t ≈ τr, we should have used the number of
particles at the phase point (r, v1) at an earlier time t − τ , where τ is the time it takes for particles
with velocity v1 to reach the scatterer. In that case, the rate of change of f(r, v, t) would depend
on its instantaneous value as well as its previous history—a non-Markovian process. The condition
∆t ≪ τr ensures that the rate of change of f(r, v, t) depends only on the time t, and in that sense
the Boltzmann equation is Markovian.
These considerations apply spatially as well, that the distribution function not vary appreciably
over the range of distances traveled by particles in time ∆t. Consider that 2πbdbg∆t represents
the “dr” associated with f(r, v1, t)dv1 to determine the number of particles about to encounter
a scatterer. If f(r, v1, t) varied appreciably over the distance g∆t, we would have to evaluate the
distribution function at the point where the colliding particle comes from, resulting in a collision
operator delocalized in space, in essence “spatially non-Markovian.” The Boltzmann equation is
local in space and time: All distribution functions are evaluated at the same (r, t).
4.5.2.5 Nonlinearity
The Boltzmann equation is nonlinear; the collision term scales quadratically, J[λf] = λ2J[f]
(where λ is a constant) and the streaming terms scale linearly. As opposed to the hierarchy, where
the time rate of change of f1 depends on the two-particle distribution function f2, and so on, through
the molecular chaos assumption the entire hierarchy has been replaced by the collision operator to
produce a single equation nonlinear in f1. Kinetic equations are necessarily nonlinear; collisions
between particles are treated in terms of products of one-particle distributions.
We note that J[f] as a nonlinear operator can be represented in terms of a bilinear operator66
Q(f,g) ≡ 1
2

dv1dΩσ(Ω)|v1 − v|(f′
1g′ + f′
g′
1 − f1g − fg1), (4.108)
63For repulsive interactions (hard core potentials), particles push each other away. The probability of finding two particles
close together (post-collision) is smaller than the product of probabilities of finding a single particle anywhere in the system. 64Also called the Stosszahlansatz (introduced by the Ehrenfests[122]), the “assumption about the number of collisions.” 65To be clear, the molecular chaos assumption is a violation of mechanics.
66The operator Q(f, g) is bilinear: Q(λf, g) = λQ(f, g) and Q(a + b, g) = Q(a, g) + Q(b, g); see Exercise 4.27.114 ■ Non-Equilibrium Statistical Mechanics
where f′
1 ≡ f(r, v′
1, t), f′ ≡ f(r, v′
, t), f1 ≡ f(r, v1, t), f ≡ f(r, v, t); likewise for g. For g = f,
Q(f,f) = J[f]. Clearly, Q(f,g) = Q(g, f). We’ll use the bilinear form of J in Section 4.8.
4.6 BOLTZMANN’S H-THEOREM, CONNECTION WITH ENTROPY
In 1872 Boltzmann published, in the same article, two celebrated results: the kinetic equation for
f(r, v, t) and that a quantity traditionally called67,68
H(t) ≡

drdvf(r, v, t) ln [f(r, v, t)] (4.109)
never increases as time progresses, the so-called H-theorem,
d
dt
H(t) ≤ 0. (4.110)
The H-theorem has far-reaching consequences.69
4.6.1 Proof of the H-theorem
Because the normalization is fixed [(d/dt)
 f(r, v, t)drdv = 
(∂f /∂t)drdv = 0], we have from
Eq. (4.109),
d
dt
H(t) =  ∂f
∂t (1 + ln f) drdv =
 ∂f
∂t ln fdrdv. (4.111)
Let’s write the Boltzmann equation in the form
∂
∂tf(r, v, t) = − (v · ∇ + a · ∇v) f(r, v, t) + J[f], (4.112)
where we’ve included in the streaming term the acceleration (a ≡ F/m) caused by external forces
(see Exercise 4.7). Equation (4.111) can then be written schematically
d
dt
H(t) = dH
dt

streaming
+
dH
dt

collisions
. (4.113)
The streaming term vanishes, (dH/dt)streaming = 0 (see Exercise 4.28), and thus H(t) changes only
from collisions, d
dt
H(t) = 
drdvJ[f] ln f. (4.114)
The theorem follows if we can show the integrand in Eq. (4.114) is never positive.
We do that by making use of the symmetry properties of J[f]. Consider an integral of the
product of J[f(v)] with an arbitrary function of v, ψ(v). That operation specifies a linear operator
on ψ associated with J[f],
L(ψ) ≡

dvJ[f(v)]ψ(v) = 
dvdv1dΩσ(Ω)|v1 − v| [f(v′
1)f(v′
) − f(v1)f(v)] ψ(v).
(4.115)
67Boltzmann used E (for entropy) in 1872, but changed it to H in 1895, which has become standard. It’s said that H
is intended as the capital Greek letter η (used by Gibbs and others to symbolize entropy); see Brush[89, p182]. In 1937, S.
Chapman[123] wrote (on Boltzmann changing E to H): “This use of H must have seemed mysterious to many generations
of students, and it would be interesting to know whether any reader can account for its use or give an earlier instance of it.” 68One should be wary of equations featuring logarithms of dimensional quantities. Transcendental functions can only be
functions of dimensionless variables. The quantity f, whether particle or probability density, is not dimensionless. The only
way such equations can be valid is if there are other terms in the equation (perhaps not displayed) that lead to logarithms of
dimensionless quantities. A constant can be added to Eq. (4.109) that fixes this issue; see Eq. (4.130). 69R.C. Tolman devoted an entire chapter of The Principles of Statistical Mechanics[124, Chapter 6] to the H-theorem,
referring to it as “among the greatest achievements of physical science.” Strong praise.Kinetic theory: Boltzmann’s approach to irreversibility ■ 115
The interchange of dummy variables v ↔ v1 doesn’t alter the value of the integral, which we
indicate as L(ψ) = L(ψ1), where ψ1 ≡ ψ(v1). The change of dummy variables v, v1 → v′
, v′
1
effects the inverse transformation of the dependent variables v′
, v′
1 → v, v1 (use dv′
1dv′ = dv1dv,
g′ = g, and the invariance of the cross section; see footnote 61 on page 112) leading to L(ψ′
) =
−L(ψ), where ψ′ ≡ ψ(v′
). Under v′ ↔ v′
1, L(ψ′
) = L(ψ′
1). By linearity, therefore,
L(ψ) = 1
4L(ψ + ψ1 − ψ′ − ψ′
1). (4.116)
Equation (4.116) expresses a basic symmetry of J[f], one that we’ll use again, several times.
Combining Eqs. (4.116) and (4.114) and using ψ = ln f, we find
d
dt
H(t) = 1
4

drdvdv1dΩσ(Ω)|v1 − v| ln  ff1
f′
f′
1

(f′
f′
1 − ff1) ≤ 0, (4.117)
the non-positivity of which follows from the inequality (x − y) ln (y/x) ≤ 0 for positive x, y.
The integrand is therefore nonpositive for any distribution functions (which are individually posi￾tive). Solutions of the Boltzmann equation have the property that, regardless of initial conditions,
f(r, v, t) evolves in such a way that H(t) never increases as time progresses.
4.6.2 Equilibrium, collisional invariants, Maxwell-Boltzmann distribution
Although H(t) can’t increase, it also can’t decrease indefinitely; it’s bounded from below.70 A
bounded monotonic function must approach a limit, the time-invariant state dH/dt = 0, which we
identify with equilibrium. From Eq. (4.117) we see that dH/dt = 0 is achieved if and only if
f(r, v′
, t)f(r, v′
1, t) = f(r, v, t)f(r, v1, t). (4.118)
The state of dH/dt = 0 is therefore one in which all collisions are in detailed balance, in which
loss and gain collisions (v, v1) → (v′
, v′
1) and (v′
, v′
1) → (v, v1) occur with equal frequency, for
all velocities. Equilibrium is thus a special dynamical state.71 Note that Eq. (4.118) holds for any
(r, t); the Boltzmann equation is local in (r, t) (see Section 4.5.2.4).
To find the distribution feq satisfying Eq. (4.118), note that, as a consequence of Eqs. (4.115)
and (4.116), and the conservation laws Eq. (4.97), we have, for any f,

dvJ[f]



1
v
v2



= 0. (4.119)
Quantities ψ(v) such that ψ1 + ψ = ψ′
1 + ψ′ (those for which  dvJ[f]ψ(v)=0) are said to be
collisional invariants or summational invariants, quantities preserved in elastic collisions—particle
number, momentum, and kinetic energy. A key result in kinetic theory, on which much depends, and
a proof of which we omit, is that (1, v, v2) are the only linearly independent collisional invariants.
72
For future use, we denote the collisional invariants ψi, i = 1, 2, 3, 4, 5, for (1, vx, vy, vz, v2).
70For H(t) not to have a lower bound (H(t) → −∞), the integral in Eq. (4.109) would have to diverge, an assumption
that leads to a contradiction. Although ln f → −∞ as v → ∞ (f → 0), the integral  1
2mv2f(v)dv representing the
kinetic energy in the gas must exist on physical grounds. For the integral for H to diverge, − ln f must tend to ∞ faster
than v2, implying that f tends to zero more rapidly than exp(−v2), for which the integral for H exists, a contradiction!
Assuming it diverges implies that it converges. A finite minimum of H is consistent with finite kinetic energy. 71Detailed balance is a microscopic, dynamical characterization of equilibrium; see Sections 2.8.2 and 3.4.3.1 for de￾tailed balance in the master equation and the Fokker-Planck equation. Compare with the picture of equilibrium provided by
traditional thermodynamics, the timeless state in which a system’s macroscopic properties appear not to be changing[2, p4]. 72Jeans[125, p111] wrote that it’s obvious these are the only collisional invariants, yet a proof is not trivial. See
Sommerfeld[126, p307] or Harris[127, p57]. The invariants ψi may be multiplied by arbitrary functions of (r, t); the Boltz￾mann equation is local in (r, t). It’s obvious these are the only collisional invariants in the sense that assuming the opposite
leads to unphysical conclusions. Besides the trivial invariant ψ1 = 1, the four remaining invariants impose four constraints
on the after-collision velocities (v′
, v′
1), leaving the unit vector ϵˆ unspecified (see Section 4.5.1). If there were additional
independent invariants, after-collision velocities could be determined without knowing ϵˆ; no need for F = ma.116 ■ Non-Equilibrium Statistical Mechanics
4.6.2.1 Spatially uniform systems
The state specified by Eq. (4.118) is equivalent to ln f′
eq + ln f′
eq,1 = ln feq + ln feq,1. Thus, ln feq
is a collisional invariant, implying it’s a linear combination of the fundamental invariants,
ln feq(v) = A + B · v + Cv2, (4.120)
where the coefficients A, B, C (five quantities) are constants (for spatially uniform systems).73
Equation (4.120) implies an equivalent parameterization as a Gaussian74 (see Exercise 4.33),
feq(v) = K exp 
− (mβ/2) (v − u)
2 
. (4.121)
This expression, the Maxwell-Boltzmann velocity distribution,75 shows that gas in equilibrium,
a quiescent system macroscopically, is, microscopically, a collection of atoms having a range of
speeds at every point r, the distribution of which has been verified experimentally. The five param￾eters K,u, β in Eq. (4.121) are determined by demanding that feq(v) be consistent with known
macroscopic information. The quantity K is found from particle number,
N =

drdvfeq(v) = V

dvfeq(v) =⇒ K = n
mβ
2π
3/2
, (4.122)
with n ≡ N/V ; u is the average velocity,
N⟨v⟩ =

drdv vfeq(v) = Nu =⇒ u = ⟨v⟩, (4.123)
and β is related to the average energy76
1
2
mN⟨(v − ⟨v⟩)
2⟩ = 1
2
m

drdv(v − ⟨v⟩)
2feq(v) = 3
2
N
β = 3
2
NkT =⇒ β = 1
kT , (4.124)
where we’ve invoked 3
2NkT as the energy of an N-particle ideal gas77 at absolute temperature
T[5, p21]. Note that temperature (a thermodynamic quantity) has been introduced in Eq. (4.124) as
proportional to the distribution of velocities relative to the mean velocity.78 Temperature is thus a
measure of the width of the equilibrium velocity distribution,79
kT = 1
3
m⟨(v − u)
2⟩. (4.125)
If the distribution is sharply peaked about the mean, temperature is small; a wide distribution implies
high temperature. The velocity u is not particularly relevant here because it can be transformed away
73Boltzmann gave a proof of Eq. (4.120) in [105, p139]. A simpler proof was devised by Gronwall[128] in 1915. 74Gaussians are completely specified by their first three moments (when normalization is counted as the zeroth moment);
see Section 2.9.1. These line up with the three parameters K, u, β. 75Maxwell derived the form of Eq. (4.121) in 1860 for a gas at rest. His argument, although of historical interest, was not
mathematically rigorous. Boltzmann put the distribution on a much firmer footing using the H-theorem. There is no one way
to derive the Maxwell-Boltzmann distribution; see [5, Section 4.1.2.7], Getting to Boltzmann: A discussion. We attempted in
[5, Section 4.1.2] a probabilistic derivation based on the central limit theorem.
76Entirely translational for a dilute gas of atoms with short-ranged interactions undergoing elastic collisions.
77One could object we’re using the equipartition theorem of statistical mechanics (see [5, p97]) and hence we’re assuming
the Maxwell-Boltzmann distribution, the form of which we’re trying to establish from kinetic theory. One can take as exper￾imental input that the heat capacity CV of noble gases (which approximate ideal gases) is 3
2 Nk and hence U = 3
2 NkT. 78Does the temperature of a container of gas increase if it’s placed on a train moving at constant speed? If that were the
case, we’d have a way of detecting absolute motion which the theory of relativity says you can’t do (see [6, Chapter 1]). 79In thermodynamics, equilibrium is attained when the intensive variables of a system (associated with conserved quan￾tities) are equal to their environmental counterparts, i.e., temperature is set by the environment[2, p45]. Equation (4.125) is
an intrinsic definition of absolute temperature in terms of the statistical properties of a system in thermal equilibrium.Kinetic theory: Boltzmann’s approach to irreversibility ■ 117
(Galilean transformation) by choosing a reference frame in which u = 0. Temperature is a Galilean
invariant, as we see from Eq. (4.125). Note that when the Maxwell-Boltzmann distribution is used
in Eq. (4.50), we recover the ideal gas equation of state, P = nkT.
Equation (4.118) is equivalent to J[feq]=0. The Maxwell-Boltzmann distribution is invariant
under collisions;
80 equilibrium is a special dynamical mode in which all collisions are in detailed
balance, a state the H-theorem guarantees is always attained. Because the streaming terms in the
Boltzmann equation vanish for uniform systems, J[feq]=0 implies ∂feq/∂t = 0. Establishing
and maintaining equilibrium is effected by collisions, and collisions require interparticle forces.
A collection of noninteracting particles would never come to equilibrium; irreversibility requires
interactions. Ideal systems are a convenient fiction not occurring in nature.81
4.6.2.2 Inhomogeneous systems, local Maxwellian
For inhomogeneous systems, the streaming terms in the Boltzmann equation can be nonzero even
when J[f]=0. Systems can satisfy detailed balance—a condition in velocity space—at each point
of configuration space.82 The general Boltzmann equation for systems in detailed balance is
 ∂
∂t + v · ∂
∂r
+ a · ∂
∂v

f(r, v, t)=0. (4.126)
Let’s try a stationary solution of Eq. (4.126) using our favorite method, separation of variables.
Generalize Eq. (4.121) to f(r, v) = K(r)e−(mβ/2)(v−u)2
, with all r-dependence carried by K(r).
Assuming conservative forces with ma = −∇ϕ(r), Eq. (4.126) implies (under this assumption)
v · [∇K(r) + βK(r)∇ϕ(r)] − βK(r)u · ∇ϕ(r)=0. (4.127)
The only way Eq. (4.127) can be satisfied with the form K(r) = K0e−βϕ(r) (see Eq. (3.61)) is
if u = 0 (see Uhlenbeck and Ford[107, p86]). This isn’t a serious restriction; there is no global
average velocity in an inhomogeneous system. We can define a local density,
n(r) ≡

dvf(r, v) = K0e−βϕ(r)

dv e− 1
2 mβv2
= K0
 2π
mβ 3/2
e−βϕ(r) ≡ n0e−βϕ(r)
,
(4.128)
where n0 is the density where ϕ = 0. Thus, K0 = n0(mβ/2π)3/2 (compare with Eq. (4.122)).
Equation (4.128) is the barometric formula.
For the most general solution of J[f]=0, return to Eq. (4.120) and let A, B, C be arbitrary
functions of (r, t). In that case we have the generalization of Eq. (4.121),
fLM(r, v, t) ≡ n(r, t)
mβ(r, t)
2π
3/2
exp 
−1
2
mβ(r, t) (v − u(r, t))2

. (4.129)
Although the form of Eq. (4.129) solves Eq. (4.126) (J[f]=0), the functions n(r, t),u(r, t), β(r, t)
must be found from the streaming side of the Boltzmann equation, (4.126). Equation (4.129) with
80H(t) changes only through collisions and dH/dt = 0 in equilibrium. 81Systems composed of noninteracting components (ideal systems) are used in statistical mechanics to approximate
the equilibrium properties of real systems[5, Chapter 5]. Equilibrium is specified by the values of state variables and not
by how they have come to have their values. Exact differentials, widely used in thermodynamics, are predicated on the
history independence of equilibrium[2, p5]. Interactions are required for systems to come to equilibrium (no matter how
long equilibration takes); once in equilibrium, however, most systems have regimes in which interactions can be neglected.
For example, all gases (in equilibrium!) behave ideally at sufficiently low pressure[129]. 82Single-particle phase space (µ-space) is the direct product of configuration space and velocity space; Eq. (4.118) can be
satisfied at every point of r-space. The collision operator acts in v-space only; one can achieve detailed balance in v-space
(J[f]=0) while evolution remains in r-space. Irreversibility occurs in v-space (collisions); streaming is reversible.118 ■ Non-Equilibrium Statistical Mechanics
space and time-dependent parameters n,u, β is the local Maxwellian,
83 which provides a theoret￾ical foundation for the concept of local thermodynamic equilibrium. We return to inhomogeneous
systems and local Maxwellians in Section 4.8 and in Chapter 6.
4.6.3 Connection with the second law, Gibbs and Boltzmann entropies
The H-theorem shows that associated with solutions of the Boltzmann equation is a quantity H(t)
that decreases monotonically to a lower bound, the state of the gas in which dH/dt = 0. We
therefore have, in the framework of the Boltzmann equation, a model of nonequilibrium behavior
in which a dilute gas irreversibly comes to equilibrium regardless of initial state, in agreement with
observational experience. Even though we know something about the mathematical properties of
H(t), we don’t know what it represents. Entropy suggests itself,84 which increases until equilibrium
is achieved, when it has the maximum value it can have subject to constraints. Is the value of H in
the state dH/dt = 0, call it H∞, proportional to entropy? Is −kH∞ = S? We’ll see how that
equality can be arranged, but first we must address the issue raised in footnote 68 on page 114.
Referring to Eq. (4.109), redefine H so that the argument of the logarithm is dimensionless, and
let’s do that using the equilibrium distribution for spatially uniform systems,
H∞ ≡

drdvfeq(v) ln 
feq(v)/f


, (4.130)
where f
is a constant (to be determined) having the dimensions of feq. Using Eq. (4.121),
H∞ = V K 
dv e− 1
2 mβv2
ln 
(K/f
)e− 1
2 mβv2 
= −N

3
2 + ln 
f
V
N
 2π
mβ 3/2 , (4.131)
where we’ve integrated over r to produce the volume V . How to choose f
? Appeal to experiment!
The Sackur-Tetrode equation is an experimentally tested formula for the entropy of an ideal
gas,[2, p118]
S = N k 
5
2 + ln  V
Nλ3
T
 = N k ln 
e5/2 V
Nλ3
T

, (4.132)
where the thermal wavelength λT ≡ h/√
2πmkT. Note Planck’s constant in λT ; the entropy of
even the simplest macroscopic system, the ideal gas, cannot be calculated using classical physics.85
Classical theories miss the mark in two ways: the Heisenberg uncertainty principle which forces a
discretization of µ-space into cells of size86 h3, and the recognition that permutations of identical
83When n, u, β are constants, as in Eq. (4.121), it’s referred to as a global Maxwellian or an absolute Maxwellian. 84Entropy and H(t) evolve through irreversible processes; they have that in common. Boltzmann wrote in 1872:[89,
p263] “H must approach a minimum value and remain constant thereafter, and the corresponding final value of f will be
the Maxwell distribution. Since H is closely related to the thermodynamic entropy in the final equilibrium state, our result is
equivalent to a proof that the entropy must always increase or remain constant, and thus provides a microscopic interpretation
of the second law of thermodynamics.” Class assignment: Was Boltzmann justified in making this claim? 85In 1982, Feynman[130] quipped: “Nature isn’t classical, dammit, . . . .” Although true, one might think that thermo￾dynamics is far from the quantum realm. Not so with entropy. It can be shown (see [2, Section 3.7]) that for entropy to
be extensive, there must be an intrinsic quantity associated with material particles having the dimension of action, which
experiment shows is Planck’s constant[2, Section 7.4]. We noted in [2] the many instances in which thermodynamics antic￾ipates quantum physics (see the index of [2] thermodynamics, anticipates quantum physics). One can’t scratch the surface
of thermodynamics without finding quantum mechanics lurking beneath. Equation (4.132) shows there are two independent
lengths in an ideal gas: the average distance between particles (V /N)1/3 and the thermal wavelength λT , the average de
Broglie wavelength of the gas particles[5, p174]. Particles behave independently when V /N ≫ λ3
T ; when V /N ≪ λ3
T
they interact strongly and their quantum natures manifest. Quantum aspects of matter feature in calculations of entropy, but
not in calculations of internal energy and other quantities obtained in statistical mechanics from logarithmic derivatives, e.g.,
U = −∂ ln Z/∂β. 86For the Sackur-Tetrode equation to agree with experiment, we must use h, not ℏ nor 1
2 ℏ; see [2, p110]. The Sackur￾Tetrode formula (circa 1912) is to our knowledge the first instance in the logical development of physics of associatingKinetic theory: Boltzmann’s approach to irreversibility ■ 119
particles are not distinct states.87 Single-particle theories lead to a factor of 3
2 in expressions for
entropy. If the permutation symmetry of collections of identical particles is not taken into account,
one will not obtain the experimentally verified factor of 5
2 in Eq. (4.132).88
Equating (4.131) (multiplied by −k) with Eq. (4.132) implies a value of f
so that −kH∞ = S,
f
= em3/h3. (4.133)
The factor of e in f
(the indistinguishability factor) changes 3
2 in Eq. (4.131) to 5
2 . The factor of m3
occurs because we’re working with a µ-space spanned by r, v; it would be unity if spanned by r, p.
Choosing f
as in Eq. (4.133),89 we have a connection between entropy and the velocity distribution
of gas particles,90
S = −k

drdvfeq(v) ln  h3
em3 feq(v)

. (4.134)
We note that 75 years after the H-theorem, information was discovered as a generalization of
entropy having the same “P-log P” form as the H-function, where the same symbol is used,
H ≡ −K 
i pi ln pi (K is a positive constant and the pi are known probabilities).91
Entropy calculated from the one-particle distribution is known as the Boltzmann entropy,
92
SB ≡ −k

f1 ln f1drdv ≡ −kHB, (4.135)
with HB the long-time value of Eq. (4.109). Gibbs introduced a similar, yet different quantity, the
Gibbs entropy, calculated from the N-body distribution, fN (r1, v1,..., rN , vN ),
93
SG ≡ −k

fN ln fN dr1dv1 ··· drN dvN ≡ −kHG, (4.136)
where HG denotes a Gibbs H-function. With SG = −k⟨ln fN ⟩ as the statistical definition of en￾tropy, the laws of thermodynamics are reproduced in the canonical ensemble (see [5, pp92–96]),
Planck’s constant with material particles. Before that point in the history of physics, Planck’s constant was associated only
with photons. The thermal wavelength presages the de Broglie wavelength. 87Noted by Gibbs in 1902[131, p187]. 88The difference between 5
2 and 3
2 might seem small, but it’s huge. In a change of entropy ∆S = Nk, the number of
states accessible to the system is increased by a multiplicative factor of eN .
89Although f
 = em3/h3 ensures that the ideal-gas entropy is correctly reproduced by −kH∞, it’s a kludge designed
to get the right answer from a single-particle theory. Single-particle theories treat the gas as N particles in six-dimensional
µ-space, rather than, as is required, a system of identical particles in 6N-dimensional Γ-space; see [2, Section 7.6]. 90In thermodynamics, entropy is a function of state variables, S = Nkf(U/N, V /N). In statistical mechanics, entropy
is found from the partition function, S = k∂(T ln Z)/∂T, where Z is a function of the system Hamiltonian. The Hamil￾tonian is microscopic in that it contains all information about system components and their interactions, yet the view of the
system afforded by Z (the number of energy states available in equilibrium at temperature T) is static. Equation (4.134) is
a connection between entropy and a dynamical property of system components, the distribution of velocities in equilibrium.
We were led to Eq. (4.134) through the H-theorem, a dynamical approach. Note that S → ∞ if h → 0; S is finite because
h ̸= 0. Entropy is another quantity saved from the ultraviolet catastrophe by the introduction of Planck’s constant. 91C. Shannon published a landmark paper in 1948, “A Mathematical Theory of Communication,”[132] which was soon
thereafter reprinted in book form[133]. We’re not trying to imply that information was copied from the H-theorem; Shannon
proposed a set of axioms to be satisfied by any definition of information and then showed that information (also called the
Shannon entropy) H = −K 
i pi ln pi is the only function meeting those requirements. See [2, Chapter 12]. Information
applies to any system for which its probability distribution is known; entropy applies to systems for which the probability
distribution characterizes thermal equilibrium. Entropy is information, but information isn’t necessarily entropy. 92A second use of the term Boltzmann entropy (besides S = k ln W). In deriving the Boltzmann equation we took f to
be a number density such that  f(r, v)drdv = N. SB is therefore extensive; see Eq. (4.131). If, however, f is taken to
be a probability density with  f(r, v)drdv = 1, we should define SB = −Nk  f ln fdrdv.
93Gibbs did not explicitly write down Eq. (4.136); he introduced an equivalent quantity, η ≡  ηeηdp1 ··· dqN , the av￾erage of η, where eη = P[131, Chapter 11]. Gibbs entropy is not always included in statistical physics texts. See Feynman[3,
p28] or Kittel[24, p54]. Entropy, as we see from Eq. (4.136), is not the mean value of a mechanical quantity, but rather is a
measure of the uncertainty in the systems of the ensemble.120 ■ Non-Equilibrium Statistical Mechanics
an important consistency requirement.94 If the Gibbs definition preserves the structure of thermo￾dynamic relations in statistical mechanics, and the Boltzmann definition reproduces the entropy of
the ideal gas, there must be a relation between them. E.T. Jaynes[134] studied the two entropies
(see also [135]) and derived the inequality HB ≤ HG, with equality for independent particles.95
The customary entropy formula attributed to Boltzmann,96 S = k ln W (which applies to the mi￾crocanonical ensemble), is consistent with the Gibbs formula S = −k⟨ln fN ⟩ (canonical ensemble)
and is derivable from it; the formula for SG reduces to k ln W in the microcanonical ensemble.97
The Gibbs formula makes use of N-particle phase space (Γ-space), as does a correct calculation of
W for identical particles[2, p112].
We’ve been led into this discussion of entropy—a property of equilibrium systems—because
of its possible connection with the long-time value of H(t). The Gibbs entropy has the property
of being a constant of the motion for N-particle systems, a conclusion that follows from general
principles of classical mechanics98 or from direct calculation (see Exercise 4.40). Thus, SG exhibits
a required property of state variables, time invariance. Of course SB is also time invariant, but that’s
because J[feq]=0 is a property of the Boltzmann equation (which pertains to dilute gases and is
based on the molecular chaos assumption); SG remains constant under the microscopic dynamics
of systems of interacting particles of any density. Although the H-theorem shows the tendency of
dilute gases to irreversibly come to equilibrium with the Maxwell velocity distribution, that does
not constitute a proof of the second law of thermodynamics, which applies to all physical systems,
not just the ideal gas.99 It’s not enough to have a bounded, monotonic mathematical function H(t);
it must have a relation with experimentally measured quantities.
4.6.4 Coarse graining and loss of information
Of broader interest is that although the Gibbs H-function remains constant under the time-reversal￾invariant microdynamics, the Boltzmann H-function evolves irreversibly under the Boltzmann
equation. This qualitative difference illustrates the effects of coarse graining, the replacement
94Setting up statistical mechanics is guided methodologically by the requirement that it be the simplest mechanical model
consistent with the laws of thermodynamics[5, Section 2.5]. This demand is met by identifying macroscopically measurable
quantities with appropriate ensemble averages, as in U = ⟨H⟩ where H is the Hamiltonian. 95To quote Jaynes:[134] “. . . the Gibbs formula gives the correct entropy, as defined in phenomenological thermodynam￾ics, while the Boltzmann H expression is correct only in the case of an ideal gas.” Jaynes showed that SG satisfies the
Clausius relation ∆SG =  dQ/T (but not SB) for general systems (those having interacting components). The quantity
SB is the entropy of an ideal system of the same density and temperature as the actual system. The difference between SB
and SG is not negligible for systems in which interparticle forces have observable effects on thermodynamic properties.
Interactions of course affect the equation of state and the free energy; see [5, Sections 6.1–6.3]. 96There is debate whether Boltzmann ever wrote the formula attributed to him explicitly in the form S = k ln W
(apparently due to Planck). The closest he came to writing S = k ln W seems to be in [105, p74]. 97Whereas S = k ln W applies to isolated systems of fixed (U, V, N) (microcanonical ensemble), SG applies to closed
systems in thermal contact with their surroundings, of fixed (T,V,N) (canonical ensemble). Thus SG is more general than
k ln W, yet it reduces to that expression in the microcanonical ensemble. There, each member of the ensemble has a number
of states Ω, all of which are equally likely with probability Pr = 1/Ω, the principle of equal a priori probabilities[5, p116].
Using the Gibbs formula for discrete states, SG = −k
Ω
r=1 [(1/Ω) ln(1/Ω)] = k ln Ω. 98Classical motion can be described as the continuous unfolding of an infinitesimal canonical transformation generated
by the Hamiltonian[5, p334]. An arbitrary phase-space function at time t = 0, A(p(0), q(0)), is mapped (under the natural
motion in phase space) into the same function of the transformed variables A(p(t), q(t)) at time t, and the Jacobian of
canonical transformations is unity (see [5, p332]). Thus, SG(t1) = SG(t2) whether t1 > t2 or t1 < t2. 99By the way, what is the second law of thermodynamics? There is no one, all-encompassing way of stating the second
law, which has diverse, interrelated implications (see [2, Chapter 9]). Students (and others) often leap to “entropy” as the
second law. Entropy is best viewed as a consequence of the second law, with the second law itself the recognition of irre￾versibility. It’s remarkable how much physics follows from the observation that heat does not spontaneously flow from cold
to hot, the simplest form of the second law. As noted by P.W. Bridgman[136] (1946 Nobel Prize in Physics), “There have
been nearly as many formulations of the second law as there have been discussions of it.”Kinetic theory: Boltzmann’s approach to irreversibility ■ 121
of N-body dynamics with the effective dynamics of fewer degrees of freedom.100 In this case,
the one-particle distribution is obtained by eliminating almost all degrees of freedom of the gas,
f1(r1, v1) ≡  dr2dv2 ··· drN dvN fN (r1, v1,..., rN , vN ). In 1894, E.P. Culverwell[137] asked:
“Will someone say exactly what the H-theorem proves?” A reply can be given when the connec￾tion between entropy and information is brought out. Entropy is a measure of the microstates of a
system consistent with the macrostate specified by state variables (other than entropy)[2, p171]. En￾tropy represents the missing information about a system that can’t be accounted for in macroscopic
descriptions. Entropy as missing information, what could potentially be known about a system, is
an appealing idea, but can it be made precise? Can information be quantified in a way suitable for
physical theories? As the term is used in everyday life, information connotes a subjective expe￾rience, yet we’re suggesting a connection between something nominally qualitative (information)
with something physical (entropy).101 Is information physical? In 1961, R. Landauer[138] published
a seminal article, “Irreversibility and Heat Generation in the Computing Process,” proposing a link
between information and the thermodynamics of its manifestation in physical systems. Landauer’s
thesis, in brief, is that indeed, information is physical, that information is stored in physical media,
is manipulated by physical means, and is subject to the laws of physics.102 Landauer’s principle is
that entropy increases when information is destroyed (as in clearing a memory register), an exper￾imentally verified prediction[140][141][142]. We can now respond to Culverwell’s question. The
H-theorem tells us there is a loss of information in a dilute gas passing from nonequilibrium (low
probability) configurations to the state of equilibrium (the most probable state of macroscopic sys￾tems; see [5, pp363–365]). There is no proof that coarse-grained entropy increases in time, just as
there is no proof of the second law, yet Landauer’s principle is a useful way to think about it.103
4.6.5 The reversibility paradox, not
An objection to the H-theorem was raised soon after it was published (Loschmidt’s paradox or the
reversibility paradox), that it should not be possible to derive irreversible phenomena from time￾symmetric physics. Consider in the evolution of a system from time t0 to t1 to t2 (t0 < t1 < t2)
there is an associated decrease in H with time. Suppose it would be possible at time t1 to reverse the
velocity of every particle, vi → −vi. For systems governed by time-reversal-invariant dynamics,
for every state of the system with particles having velocities {vi}, there is a time-reversed state with
velocities {−vi}, and thus the value of H would be unchanged by this process. Before the time
reversal at t = t1, ∂H/∂t < 0, but after ∂H/∂t > 0, a contradiction.
The paradox resolves itself when the flaw in the argument is noticed: The dynamics described by
the H-theorem is not time-reversal invariant! Pre-collision velocities of dilute-gas particles are un￾correlated, but after-collision particles become correlated through the conservation laws, Eq. (4.97),
100Coarse graining is also used in statistical mechanics. In Ginzburg-Landau theory, a system of spins on a lattice {σi}
is replaced with a continuous spin-density field S(r)[5, p295]. A field description constitutes a coarse graining, a view of
the system from sufficiently large distances that it appears continuous. The renormalization group studies how the effective
interactions between coarse-grained degrees of freedom change over progressively larger length scales[5, Chapter 8]. 101The ability to quantify information is why information theory is so widely used in scientific research. Information (as
the term is used in information theory) is similar to entropy, with their commonality being missing information. (Information
is thus missing information. Got it?) The definition of information H = −K 
i pi ln pi takes into account the element of
surprise one experiences in learning of events whose occurrence was not previously certain. Which statement conveys more
information: 1) The sun rose this morning; 2) The sun exploded this morning. There’s no news in the first statement; the
sun rises everyday with high probability. The second statement, however, is real news: It’s unexpected (low probability). The
more likely an event, the less is the information conveyed in learning of its occurrence. 102Landauer wrote in 1999:[139] “Information is inevitably inscribed in a physical medium. It is not an abstract entity. It
can be denoted by a hole in a punched card, by the orientation of a nuclear spin, or by the pulses transmitted by a neuron.
The quaint notion that information has an existence independent of its physical manifestation is still seriously advocated.
This concept, very likely, has its roots in the fact that we were aware of mental information long before we realized that it,
too, utilized real physical degrees of freedom.” 103Landauer’s principle is not independent of the second law of thermodynamics[2, p184].122 ■ Non-Equilibrium Statistical Mechanics
a fact ignored in deriving Boltzmann’s equation (molecular chaos assumption). If the velocities of
all particles are reversed, collisions occur in the reverse sense, but the colliding particles would not
be uncorrelated (if the dynamics is time symmetric), and the value of ∂H/∂t found previously no
longer applies.
4.7 COLLISION FREQUENCY, MEAN FREE PATH
We can infer the rate at which particles collide using the arguments leading to the collision operator.
Divide Eq. (4.103) (the total rate of loss collisions104) by f(r, v, t) to yield the per-particle rate at
(r, t) that particles of velocity v undergo collisions, ν(r, v, t) ≡  dv1dΩσ(Ω)|v1 − v|f(r, v1, t).
This result is too general; let’s use f0(v) = n(m/2πkT)3/2 exp(−mv2/2kT), the equilibrium
distribution for spatially uniform systems: ν(v) =  dv1dΩσ(Ω)|v1 −v|f0(v1). Treat gas particles
as hard spheres that undergo elastic scattering when their surfaces come into contact,105 when their
centers are separated by the diameter D. The total scattering cross section is the area blocked out
by a sphere of radius D, σT = πD2. Take σT outside the integral; for hard spheres, therefore,
ν(v) = σT
 dv1|v1 − v|f0(v1). Evaluating the integral (see Exercise 4.41), we find
ν(v) = ν0

e−ξ2
+ (ξ−1 + 2ξ)
 ξ
0
e−t2
dt

, (4.137)
where ξ ≡ v/vmp, with vmp ≡ 2kT /m the most probable speed of gas particles,[5, p129] and
ν0 ≡ nσT
2kT/(mπ). The collision frequency of a particle moving through an equilibrium gas
thus depends on its speed. For particles at rest (for small x,
 x
0 exp(−t
2)dt = x − 1
3x3 + ···),
limv→0 ν(v)=2ν0 = nσT v, where v ≡ 8kT/(mπ) is the mean speed of particles in an equilib￾rium gas. As v increases, ν(v) increases monotonically, becoming linear, ν(v) v→∞∼ nσT v.
The inverse of ν(v) provides the time between successive collisions for particles of speed v. The
distance between successive collisions, l(v) ≡ v/ν(v), ranges from zero for the slowest atoms to
the value for the fastest
l ≡ limv→∞ l(v)=1/(nσT ). (4.138)
The limiting value l(∞) is called the mean free path.
106 Note that it’s independent of particle mass.
Example. Atomic diameters range from 50 to 500 pm (picometers);107 take D = 0.1 nm as
representative. At standard temperature and pressure108 n = 2.65 × 1025 m−3, implying from
Eq. (4.138) a mean free path l = 1.2 × 10−6 m. The interatomic separation can be estimated
from d ≈ n−1/3 = 3.35 × 10−9 m. The average distance between collisions is thus 360 times
the average distance between atoms—a dilute gas. Taking D to be the range of the interaction,
nD3 = 2.65 × 10−5, satisfying the diluteness criterion nr3
0 ≪ 1. For 107 Pa pressure, the molar
volume is 0.227 liters (assuming an ideal gas), with a mean free path 1.2 × 10−8 m. In this case,
d = 7.2 × 10−10 m, implying l = 17d. At higher pressures, the mean free path becomes on the
order of the interatomic separation, making the molecular chaos assumption difficult to justify.
104For every direct collision, there is a loss scattering event. 105This is our first use of the hard-sphere approximation. Expositions of kinetic theory often treat gas particles as hard
spheres from the outset. 106Paths taken by rigid molecules between successive collisions are called free paths. (For non-rigid molecules, scattering
events have no definite beginning and end, rendering the concept of free path problematic.) Equation (4.138) is widely used
for the mean free path, but there are other expressions in the literature; see Chapman and Cowling[106, p88]. A heuristic
derivation of Eq. (4.138) consists of finding the length l of a tube of cross-sectional area A in a gas of density n such that for
molecules traveling parallel to the tube one is guaranteed to encounter a scatterer, when lAnσT = A. 107Atomic radii are widely tabulated. See for example Slater[143]. 108From 22.7 liters per mole of an ideal gas at standard pressure and temperature, a good number to remember.Kinetic theory: Boltzmann’s approach to irreversibility ■ 123
4.8 NORMAL SOLUTIONS OF THE BOLTZMANN EQUATION
Hydrodynamic conservation laws, as found from balance equations for mass, momentum, and en￾ergy [see Eq. (4.73)], are incomplete without constitutive relations for the momentum and heat
fluxes. To obtain a closed set of equations for the hydrodynamic fields density ρ(r, t), velocity
u(r, t), and temperature T(r, t) [see Eq. (4.85)], it’s assumed that: 1) fluxes can be parameter￾ized in terms of local gradients [see Eqs. (4.77), (4.78)], and 2) local equations of state such as
P(r) = P[T(r), ρ(r)] extend to nonequilibrium states such that P(r, t) = P [T(r, t), ρ(r, t)] [see
Eqs. (4.82), (4.83)]. These ideas are (as we show) validated in normal solutions of the Boltzmann
equation in which spatiotemporal variations occur through a functional dependence on hydrody￾namic fields,
f(r, v, t) = f({ρi(r, t)}, v), (4.139)
where {ρi} is the set of the conserved moments of f associated with the collisional invariants ψi(v)
(see Exercise 4.43),
ρi(r, t) ≡ m

ψi(v)f(r, v, t)dv. (4.140)
From Eqs. (4.31), (4.34), (4.36) and (4.58), ρ1 = ρ, ρ2,3,4 = ρux,y,z, and ρ5 = ρu2 + 2εK. As an
example, the local Maxwellian Eq. (4.129) is a normal solution. With normal solutions, not only are
P, J determined by macroscopic fields, so are averages of any function of the velocity. Under what
conditions do normal solutions occur?
4.8.1 Hilbert’s theorem, hydrodynamic regime
Consider Boltzmann’s equation as an initial-value problem: One solves for f(t > 0) given f(t = 0).
It would be unrealistic to expect that we could ever have such a detailed knowledge of nonequilib￾rium systems as to know f(t = 0) precisely. We noted in Section 2.1.1 that if one knew all the
moments of a probability distribution, but not the distribution itself, one could in principle con￾struct the distribution from the moments. That would require knowledge of an infinite collection
of moments—also unrealistic. A result of Hilbert is then remarkable that if f (as a solution of the
Boltzmann equation) has a power series representation, f is uniquely determined for t > 0 by the
values at t = 0 of only five moments, the conserved moments ρi(r, 0). We outline the proof.
4.8.1.1 Time scales in the Boltzmann equation
Flows and collisions, the processes by which distribution functions change in the Boltzmann equa￾tion, occur at different rates. From Eq. (4.106) we see that [J[f]] = [nσvf¯ ] ≡ [f /τr] (square
brackets denote dimension), where τr = l/v¯ (relaxation time) is the time scale associated with the
collision operator (l is the mean free path, v¯ is the mean speed). For v¯ = 103 m s−1 and l = 10−6
m, τr = 10−9 s. For the collision time τc = D/v¯ (D is the range of the inter-particle potential), the
time for a scattering event to occur, we have for v¯ = 103 m s−1 and D = 10−10 m, τc = 10−13
s. Thus, τc ≪ τr, as in (4.107). In uniform systems, ∂f /∂t = 0 when J[f]=0 (when detailed
balance is achieved). In nonuniform systems, the streaming terms can be nonzero when J[f]=0;
detailed balance—a condition in velocity space—can be satisfied at each point of position space.
We can infer a time scale associated with the spatial inhomogeneities that drive flows, τh ≡ Lh/v¯,
where Lh can be estimated from the gradient, L−1
h ≡ |∇f|/f. There is no typical value of Lh,
which depends on experimental conditions, but it can be considered macroscopic. For Lh = 10−2
m and v¯ = 103 m s−1, τh = 10−5 s.
Systems featuring a clear separation of length scales D ≪ l ≪ Lh, implying the separation of
time scales τc ≪ τr ≪ τh, are said to be in the hydrodynamic regime. To compare time scales, let
δ ≡ τr/τh. With τr/τh = (l/v¯)/(Lh/v¯) = l/Lh, we have δ = l/Lh ≡ Kn, the Knudsen number.
Systems with Kn ≪ 1 behave as a continuum, with many collisions in time τh, σnvτ¯ h ≫ 1, the124 ■ Non-Equilibrium Statistical Mechanics
collision-dominated regime. Solutions of the Boltzmann equation that we develop (and the equations
of hydrodynamics) are valid for small Knudsen numbers. Boltzmann’s equation can then be written
in scaled form,
∂f
∂t + v · ∇f ≡ Dvf = 1
δ
Q(f,f), (δ ≪ 1) (4.141)
with t dimensionless, Dv the convective derivative, and Q the bilinear form of J, Eq. (4.108).
4.8.1.2 The Hilbert expansion
Hilbert posited a power series solution of the Boltzmann equation [in the form of (4.141)],109
f(r, v, t) = ∞
n=0
f(n)
(r, v, t)δn. (4.142)
Combining Eqs. (4.142) and (4.141), we find by matching powers110 of δ, the system of equations,
Q(f(0), f(0))=0
2Q(f(1), f(0)) = Dvf(0)
2Q(f(n)
, f(0)) = Dvf(n−1) −
n
−1
m=1
Q(f(m)
, f(n−m)
). (n > 1) (4.143)
Thus, when 1) Boltzmann’s equation can be written as in (4.141) and 2) the series in Eq. (4.142)
exists, the nonlinear integro-differential equation for f can be replaced with a homogeneous integral
equation for f(0) together with an infinite system of linear integral equations of the second kind for
f(n≥1). Appendix D is a review of integral equations.
The leading term f(0) is the solution of J[f(0)]=0, which by the H-theorem is uniquely given
by a local Maxwellian, Eq. (4.129), which we write
f(0)(r, v, t) = n(0) 
m/(2πkT(0))
3/2
exp 
−m/(2kT(0))(v − u(0))
2

. (4.144)
The fields n(0)(r, t),u(0)(r, t), T(0)(r, t) (left unspecified momentarily) are the moments of f(0):



n(0)
n(0)u(0)
3n(0)kT(0)/m


 =

f(0)(r, v, t)



1
v
(v − u(0))2



dv. (4.145)
The quantity f(0) is not necessarily associated with an actual state of the system; (n(0),u(0), T(0)),
the moments of f(0), are not necessarily equal to (n,u, T), the moments of f. As we’ll show,
consistency requires that (n(0),u(0), T(0)) satisfy the Euler equations of fluid dynamics.111
4.8.1.3 The linearized collision operator
The left side of each equation in (4.143) for n ≥ 1 involves 2Q(f(n), f(0)). Suppose f = f(0) + h.
Because of bilinearity, Q(f(0)+h, f(0)) = Q(f(0), f(0))+Q(h, f(0)) = Q(h, f(0)), which specifies
109Published in 1912 as the last chapter of Hilbert’s treatise on integral equations[144] and reprinted in [145]. There is an
English translation in Brush[121, pp89–101]. In Hilbert’s treatment, there is no physical basis for δ; it’s a formal parameter
only. Hilbert took the form of Boltzmann’s equation in (4.141) as an ansatz without physical motivation. 110Equating coefficients of like powers of δ on the two sides of the equation follows from the uniqueness of power series. 111The Euler equations are the equations of hydrodynamics for ideal fluids, which we take to be the five equations in
(4.85) with η = ζ = κ = 0. There are no dissipative effects in the Euler equations. Some authors refer to the Euler equation
(singular) as the Navier-Stokes equation (1.22) with η = ζ = 0; see Landau and Lifshitz[15, p3].Kinetic theory: Boltzmann’s approach to irreversibility ■ 125
a linear operator on h (f(0) is fixed), L, the linearized collision operator. Using Eq. (4.108) and the
detailed balance condition Eq. (4.118), we find:
L(h(v)) ≡ 2Q(h, f(0)) (4.146)
=

dv1dΩσ(Ω)|v1 − v|f(0)(v)f(0)(v1)
  h
f(0) ′
1
+
 h
f(0) ′
−
 h
f(0) 
1
− h
f(0) 
,
where the subscript 1 indicates evaluated at v1 and the prime indicates post-collision quantities (see
Fig. 4.1).112 We therefore have the linearized Boltzmann equation (see Exercise 4.46),
∂h
∂t + v · ∇h = L(h), (4.147)
simpler than Boltzmann’s equation in that it’s linear, yet it’s still an integro-differential equation for
which there is no standard mathematical theory.
By combining Eq. (4.146) with the result of Exercise 4.29, we have for functions k(v) and h(v),

dv

f(0)(v)
−1
k∗(v)L(h(v)) = −1
4

dvdv1dΩσ(Ω)|v1 − v|f(0)(v)f(0)(v1)
×
  k
f(0) ′
1
+
 k
f(0) ′
−
 k
f(0) 
1
− k
f(0) ∗
×
  h
f(0) ′
1
+
 h
f(0) ′
−
 h
f(0) 
1
− h
f(0) 
. (4.148)
We now use Dirac notation and treat functions f(v) as elements |f⟩ of a Hilbert space of square￾integrable functions with inner product defined with respect to the weighting function113 (f(0))−1,
⟨k|h⟩ ≡ 
dv

f(0)(v)
−1
k∗(v)h(v). (4.149)
Thus, L is Hermitian, ⟨k|L|h⟩ = ⟨h|L|k⟩∗, and negative semi-definite, ⟨h|L|h⟩ ≤ 0. For L|ϕn⟩ =
λn|ϕn⟩, the eigenvalues are real and nonpositive,114 λn ≤ 0. The case of λn = 0 occurs if and only
if h = f(0)(v)ψi(v), where ψi is one of the five collisional invariants (see Section 4.6.2). The zero
eigenvalue is precisely five-fold degenerate.
4.8.1.4 Existence of terms in the expansion
The next equation in (4.143), L(f(1)) = Dvf(0), has a solution for f(1) if and only if the solvability
conditions are satisfied, that the source term Dvf(0) have no projection onto the null space of L (the
subspace spanned by zero-eigenvalue eigenfunctions),115
⟨f(0)ψi|Dvf(0)⟩ =

dvψi(v)Dvf(0)(r, v, t)=0. (i = 1,..., 5) (4.150)
112Note that the factor of f(0)(v) could come outside the integral in Eq. (4.146), a step we’ll take later. 113A Hilbert space is a complete inner product space[13, p43]; an inner product must be specified, and inner products can
be defined with weighting functions[13, Chapter 2]. We encountered the same form of inner product in our treatments of the
master equation transition matrix and the Fokker-Planck equation, Eqs. (2.82) and (3.85). 114The master equation matrix and the Fokker-Planck operator have nonpositive eigenvalues; see Exercises 2.45 and 3.33. 115For solutions of linear inhomogeneous equations to exist, the source terms must have no projection onto the null
space of the operator. This is true of differential equations ([72, p356], [13, p259]), systems of algebraic equations,[72, p6]
and integral equations (see Appendix D). The linearized collision operator L can be written L = −νI + K, with ν the
collision frequency, I the identity, and K a symmetric kernel, the Hilbert decomposition (establishing this result occupies
approximately 25% of Hilbert’s article).[146, p197][147, p58][127, p61]. Thus, L(g)=0 is represented by a homogeneous
Fredholm equation of the second kind, allowing us to make use of associated theorems for the solutions of such equations.126 ■ Non-Equilibrium Statistical Mechanics
The five equations implied by (4.150) are related to the five Euler equations of fluid dynamics; the
moments of f(0) listed in Eq. (4.145) satisfy the Euler equations. For L(f(n+1)) in Eq. (4.143), we
have the associated solvability conditions

dvψi(v)

Dvf(n) − n
m=1
Q(f(m)
, f(n+1−m)
)

= 0.
For collisional invariants ψj ,
 ψj (v)Q(f(m)
, f(n+1−m)
)dv vanishes identically for all (n, m) (see
Exercise 4.29). For the terms in the Hilbert expansion to exist116 we therefore require, for n ≥ 0, 
dvψi(v)Dvf(n)
(r, v, t)=0. (i = 1,..., 5) (4.151)
With the particular solutions to Eq. (4.143) denoted ¯f(n) (assumed known), we have the general
solutions
f(n)
(r, v, t) = ¯f(n)
(r, v, t) +
5
j=1
γ(n)
j (r, t)f(0)(r, v, t)ψj (v), (4.152)
where the γ(n)
j (r, t) are expansion coefficients. These are uniquely determined by the Fredholm
condition (see the Fredholm theorem on page 210 or [72, p116]),

¯f(n)
(r, v, t)ψi(v)dv = 0. (i = 1,..., 5) (4.153)
By combining Eq. (4.152) with the solvability conditions Eq. (4.151), and when Eq. (4.153) is
invoked, we find the equations determining the γ(n)
j ,
∂
∂t
5
j=1
aij (r, t)γ(n)
j (r, t)

+
3
k=1
∂
∂xk

5
j=1
bijk(r, t)γ(n)
j (r, t) + c
(n)
ik (r, t)

= 0, (4.154)
where
 aij (r, t)
bijk(r, t)

≡

dvf(0)(r, v, t)ψi(v)ψj (v)
 1
vk

c
(n)
ik (r, t) ≡

dv ¯f(n)
(r, v, t)ψi(v)vk. (4.155)
The quantities aij , bijk are known; the c
(n)
ik presume the particular solutions ¯f(n). We have for each
n, five inhomogeneous first-order hyperbolic partial differential equations in the five unknowns
γ(n)
j (r, t). Solutions to these equations are determined when the initial data γ(n)
j (r, 0) are specified.
By combining Eq. (4.142) with Eq. (4.140):
ρi(r, t) = m∞
n=0

f(n)
(r, v, t)ψi(v)dv

δn ≡ ∞
n=0
ρ
(n)
i (r, t)δn. (4.156)
The expansion of the distribution function therefore implies associated expansions for the moments.
Using Eqs. (4.152), (4.153), (4.155), we find
ρ
(n)
i (r, t) = m
5
j=1
aij (r, t)γ(n)
j (r, t). (4.157)
Solutions for f(n)
(r, v, t) in Eq. (4.152) are therefore determined by specifying ρ
(n)
i (r, 0) as initial
data. That’s not the content of Hilbert’s theorem, however; it’s the starting point.
116To speak of the existence of the Hilbert expansion (not the existence of its terms f(n)), the issue of convergence should
be investigated, of which not much is known; see [146, Section 11.4].Kinetic theory: Boltzmann’s approach to irreversibility ■ 127
4.8.1.5 Hilbert’s result and discussion
It would appear that every time a new term f(n) is added to the expansion f = 
k f(k)
δk, five new
arbitrary functions ρ
(n)
i (r, 0) would have to be found. Hilbert showed, to the contrary, that arbitrary
functions appear in the expression for f(r, v, t) in such a way that it contains only five arbitrary
functions. For this he relied on the uniqueness of power series, that no matter how you’ve found the
solution to a problem in the form of a power series, once you have it, it’s the series.
Consider that we’re free to choose117 initial data such that
ρ
(0)
i (r, 0) ≡ m

dvf(0)(r, v, 0)ψi(v) = ρi(r, 0)
ρ
(n)
i (r, 0) ≡ m

dvf(n)
(r, v, 0)ψi(v)=0. (n ≥ 1) (4.158)
The conserved moments of f(0) are equated to those of f, with moments of f(n≥1) defined to be
zero. We therefore have Hilbert’s result of a power series solution for f depending only on the initial
data ρi(r, 0).
118
Hilbert’s theorem raises questions, physical and mathematical. Why does the solution involve
only five moments of f, whereas an exact solution would require the full specification of f at t = 0?
The answer is that Boltzmann’s equation doesn’t make sense for very short times. In the regime
δ ≪ 1 (collision frequency far exceeds the rate of flow processes), local equilibrium is established
so rapidly that the ρi basically don’t change from their initial values. By writing Eq. (4.141) in the
form δDvf = J[f], we see that it reduces to J[f]=0 if the limit δ → 0 is naively taken, the
solution of which is a Gaussian (local Maxwellian) parameterized by the five moments associated
with collisional invariants (see Section 4.6.2.1). Gaussians have higher moments, they’re just not
independent of the lower moments, the normalization, the mean, and the variance.
Hilbert’s analysis assumes the limit δ → 0 can be taken, of which there are issues. Physically,
τr → 0 for arbitrarily large densities, a regime not in the scope of Boltzmann’s equation based on
τr ≫ τc. We therefore have a singular perturbation (δ = 0 is fundamentally different from the
“neighboring theory” for δ ≪ 1) which can’t be handled with the usual perturbation methods (see
Bender and Orszag[148]). In Hilbert’s theory the moments ρ
(0)
i must satisfy the Euler equations.
It tries, therefore, to construct the correct moments ρi through a perturbation theory based on ideal
fluids. Singular-perturbative phenomena in fluid dynamics (see for example Van Dyke[149]) involve
nonanalytic deviations from ideal-fluid behavior, where a perturbation δ can’t be expanded around
δ = 0. The hope of perturbation theory is that the more the terms included in Eq. (4.156), the better
one approximates the fields ρi, a possibility precluded by Eq. (4.158) in which the moments of f(0)
are equated with those of f in one fell swoop. The reliance of Hilbert’s method on expansions for
the conserved moments is therefore problematic. We won’t dwell further on these issues. The merit
of Hilbert’s treatment is that it suggests the possibility of normal solutions.
4.8.2 Chapman-Enskog method
Between 1911 and 1917, S. Chapman and D. Enskog independently published articles on deriving
the equations of hydrodynamics from Boltzmann’s equation.119 Their method is similar to, yet gen￾eralizes Hilbert’s treatment in a way that leads to testable predictions for transport coefficients.120
117Guessing is a valid method of problem solving. 118We still have the particular solutions in Eq. (4.152). One might wonder what the problem is that we’ve solved. We have
a solution f(r, v, t) of the Boltzmann equation that’s non-negative, vanishes for |v|→∞, and is finite and continuous for
all times t. 119Brush[121] contains reprints of Chapman’s articles and an English translation of Enskog’s doctoral dissertation. Chap￾man and Cowling[106, Chapter 7] present a general treatment of the Enskog method. 120Grad[115][150][151] examined the Hilbert and Chapman-Enskog methods in detail and concluded they should both
ultimately lead to identical results.128 ■ Non-Equilibrium Statistical Mechanics
Normal solutions are assumed from the outset, f(r, v, t) = f(v, {ρi(r, t)}), implying from Eq.
(4.141),

5
i=1
∂f
∂ρi
∂ρi
∂t + v · ∇f = 1
δ
Q(f,f). (δ ≪ 1) (4.159)
A key step is to recognize that for the conserved moments (see Exercise 4.43),
∂ρi
∂t = −m∇ · 
ψi(v)vf(v, {ρk(r, t)})dv ≡ Di({ρk(r, t)}). (4.160)
The time derivatives of macroscopic quantities are expressed in terms of spatial derivatives! We
therefore have Boltzmann’s equation for normal solutions (in which time is formally eliminated),

5
i=1
∂f
∂ρi
Di({ρj}) + v · ∇f = 1
δ
Q(f,f). (δ ≪ 1) (4.161)
As with Hilbert, expand the distribution function, Eq. (4.142). We don’t, however, expand the
conserved moments to avoid nonanalyticities associated with singular perturbations. To enforce the
no-expansion treatment of these moments, the conditions in Eq. (4.158) are assumed,
ρi(r, t) = m

ψi(v)f(0)(r, v, t)dv
0 = m

ψi(v)f(n>0)(r, v, t)dv. (4.162)
We then introduce expansions of the time-derivative functions,
Di({ρj}) = ∞
n=0
D(n)
i ({ρj})δn, (4.163)
where
D(n)
i ({ρj}) ≡ −m∇ · 
ψi(v)vf(n)
(v, {ρj (r, t)})dv. (4.164)
By combining the expansions for f and Di, Eqs. (4.142) and (4.163), with Eq. (4.161), we find the
generalization of Hilbert’s scheme (see Eq. (4.143); L is the linear collision operator),
J[f(0)]=0
L(f(1)) = 
5
i=1
∂f(0)
∂ρi
D(0)
i ({ρj}) + v · ∇f(0) (4.165)
L(f(n)
) =
n
−1
k=0

5
i=1

∂f(k)
∂ρi
D(n−k−1)
i ({ρj}) + v · ∇f(n−1)
−
n
−1
k=1
Q(f(k)
, f(n−k)
). (n > 1)
The solvability conditions must be satisfied. We require, for n ≥ 1 and j = 1,..., 5,
m

dvψj (v)
n
−1
k=0

5
i=1

∂f(k)
∂ρi
D(n−k−1)
i ({ρj}) + v · ∇f(n−1)
−
n
−1
k=1
Q(f(k)
, f(n−k)
)

= 0.
The integral associated with the third term vanishes identically (see Section 4.8.1.4). For the first
term, the time derivatives D(l)
i involve only (r, t) and can come outside the integral. Thus,
m

dvψj (v)
n
−1
k=0

5
i=1
∂f(k)
∂ρi
D(n−k−1)
i =
n
−1
k=0

5
i=1
D(n−k−1)
i
∂
∂ρi
m

dvψj (v)f(k)
(r, v, t)
=
n
−1
k=0

5
i=1
D(n−k−1)
i
∂
∂ρi
ρj (r, t)δk,0 =
n
−1
k=0

5
i=1
D(n−k−1)
i δi,j δk,0 = D(n−1)
j ,Kinetic theory: Boltzmann’s approach to irreversibility ■ 129
where we’ve used Eq. (4.162). For the solvability conditions to hold, we must have
D(n−1)
j = −m∇ · 
dvψj (v)vf(n−1)(r, v, t),
precisely Eq. (4.164). The solvability conditions are identities in the Chapman-Enskog theory.
The solution of the first equation in (4.165) is the local Maxwellian f(0), Eq. (4.144), with
moments given in Eq. (4.145) but with superscripts erased, (n,u, T), because of Eq. (4.162). The
momentum and heat fluxes associated with f(0) are P = nkTI and J = 0; there are no dissipative
effects at lowest order in the Chapman-Enskog expansion.121
Rewrite the second equation in (4.165),
1
f(0) L(f(1)) = 
5
i=1
∂ ln f(0)
∂ρi

D(0)
i + v · ∇ρi

, (4.166)
which follows because f(0) depends on r only through {ρi(r, t)}. To find the derivatives with re￾spect to ρi we must change variables, {ρi} → (n,u, T); f(0) is a function of the fields (n,u, T).
122
From Eq. (4.144) (where l = x, y, z),
∂ ln f(0)
∂n = 1
n, ∂ ln f(0)
∂ul
= m
kT (v − u)l, ∂ ln f(0)
∂T = 1
T
 m
2kT (v − u)
2 − 3
2

,
and from Eq. (4.162) with ψi = {1, vx, vy, vz, v2},
123
ρ1 = mn ≡ ρ =⇒ n = ρ1/m
ρ2,3,4 = mnux,y,z =⇒ ux,y,z = ρ2,3,4/ρ1
ρ5 = 3nkT + mnu2 =⇒ T = m
3k
ρ5
ρ1
− 1
ρ2
1

l=2,3,4
ρ2
l

. (4.167)
Using the chain rule, we find the required derivatives:
∂ ln f(0)
∂ρ1
= 1
ρ

5
2 − 1
2
m
kT v2 +
1
6
 m
kT
2
u2(v − u)
2

;
∂ ln f(0)
∂ρl
= 1
ρ
 m
kT v − 1
3
 m
kT
2
u(v − u)
2

l
;
∂ ln f(0)
∂ρ5
= 1
ρ

−1
2
m
kT +
1
6
 m
kT
2
(v − u)
2

. (4.168)
We also need D(0)
i + v · ∇ρi. From Eqs. (4.164) and (4.167):124
D(0)
1 + v · ∇ρ1 = −∇ · (ρu) + v · ∇ρ;
D(0)
l + v · ∇ρl = − [∇ · (ρuu)]l − ∇l(nkT) + v · ∇(ρul);
D(0)
5 + v · ∇ρ5 = −∇ · 
u(5nkT + ρu2)

+ v · ∇(3nkT + ρu2). (4.169)
121What’s in a name? In kinetic theory, the first equation in Eq. (4.165) for f(0) is confusingly referred to as the first
Chapman-Enskog approximation, with the second equation, for f(1), the second approximation. This nomenclature differs
from other branches of physics where f(0) would be referred to as the zeroth approximation, with f(1) first order. 122The equations of hydrodynamics are also given in terms of (n, u, T) or (ρ ≡ mn, u, T); see Eq. (4.85). 123Some authors take ψ5 = 1
2 v2. Scalings of ψi are immaterial: (∂f /∂ρi)Di is invariant under ψi → λψi. 124Set P = nkT and ε = 3
2nkT to recover the Euler equations. See Exercise 4.49.130 ■ Non-Equilibrium Statistical Mechanics
Finally, using Eqs. (4.168) and (4.169), we find from Eq. (4.166), after considerable algebra,125
1
f(0) L(f(1)) = 1
T
my2
2kT − 5
2

y · ∇T +
m
kT
◦
yy:D, (4.170)
where: y ≡ v − u; ◦
w indicates a traceless (or non-divergent) tensor, ◦
w ≡ w −  1
3

i wii
I
(notation of Chapman and Cowling[106, p15]); and D is the rate-of-strain tensor with elements
Dij ≡ 1
2 (∇iuj + ∇jui). Terms proportional to the density gradient ∇n vanish identically.
The structure of L suggests a solution in the form f(1)(v) = f(0)(v)Φ(v); see Eq. (4.146). With
that substitution, we have from the linearized Boltzmann equation (4.147),
 ∂
∂t + v · ∇
Φ(r, v, t) = 1
f(0) L(f(0)Φ) ≡ nσ2kT
m
I(Φ). (4.171)
We’ve introduced nσ2kT /m = v/l = τ −1 r to make I(Φ) dimensionless. With the dimensionless
variable ξ ≡ m/(2kT)y, f(0)(v)dv → (n/π3/2) exp(−ξ2)dξ. The quantity σ (with dimen￾sion area) can be found from the total cross section, σ ≡  dΩσ(Ω). Let σ ≡ σ(Ω)/σ denote a
dimensionless cross section. Also, let |v1 − v| → (2kT /m)g, where g is dimensionless. Thus,
I(Φ(v)) ≡ 1
nσ m
2kT 
dv1dΩσ(Ω)|v1 − v|f(0)(v1) [Φ(v′
1) + Φ(v′
) − Φ(v1) − Φ(v)]
= 1
π3/2

dξ1e−ξ2
1 dΩσ(Ω)g [Φ(ξ′
1) + Φ(ξ′
) − Φ(ξ1) − Φ(ξ)] . (4.172)
By expressing the right side of Eq. (4.170) in terms of dimensionless velocities, we find
nσI(Φ) = 
ξ2 − 5
2

ξ · ∇ ln T + 2 ◦
ξξ:D , (4.173)
where D ≡ m/(2kT)D is the rate of strain tensor expressed in terms of a dimensionless velocity.
Note that nσ = l
−1, the inverse mean free path. If one pulled out the length scale Lh associated
with gradients on the right side of Eq. (4.173), we would have the Knudsen number δ = l/Lh. We
can (if we want) set l = 1 and work in length units of the mean free path.
We can guess the form of Φ by noting that the left side of Eq. (4.173) is linear in Φ but the right
side is linear in spatial derivatives. We can therefore build Φ out of a scalar product of ∇T with a
vector A(ξ) and the contraction of D with a second-rank tensor,126 B(ξ). Let’s try
Φ = lA · ∇ ln T + 2lB:D + a1 + a · v + a5v2, (4.174)
where we’ve included the complementary solution. The dimensionless quantities A, B are solutions
of:
I(A) = 
ξ2 − 5
2

ξ; I(B) = ◦
ξξ. (4.175)
The solvability conditions for these integral equations are satisfied identically; see Exercise 4.50.
The inhomogeneous terms in Eq. (4.175) involve ξ, implying A, B are functions of ξ. We can write
A(ξ) = A(ξ)ξ, (4.176)
where A is a scalar function. We see that B is traceless, 
i I(Bii)=0; it’s also symmetric,
I(Bij − Bji)=0. Thus we can write, where B is a scalar function,
B(ξ) = B(ξ)
◦
ξξ. (4.177)
125Group together terms associated with gradients of (n, u, T), a task Harris[127, p90] refers to as “gruesome.” 126With A and B, we’re using the notation of Chapman and Cowling[106, p123], the standard reference on this material.Kinetic theory: Boltzmann’s approach to irreversibility ■ 131
The quantities (a1, a, a5) in Eq. (4.174) are uniquely determined by the Fredholm requirements,

f(0)(v)ψi(v)

A · ∇ ln T + 2B:D + a1 + a · v + a5v2

dv = 0. (4.178)
With ψi(v) = (1, v, v2), Eq. (4.178) implies the three equations

f(0)(v)

a1 + a5v2
dv = 0

f(0)(v)
 1
T A(y)∇T + a

y2dv = 0

f(0)(v)

a1 + a5v2
v2dv = 0.
From the first and third equations, a1 = a5 = 0, while the middle indicates a is proportional to
∇T, implying a · v can be absorbed into A · ∇T. We can take a = 0 if we impose the constraint

dvf(0)(v)A(y)y2 = 0, (4.179)
an equation that plays an important role.
Thus, assuming Eq. (4.179), we have the form of f(1),
f(1)(r, v, t) = lf(0)(r, v, t)

A · ∇ ln T(r, t)+2B:D (r, t)

. (4.180)
We’d be done if we could account for A and B. In practice, no attempt is made at directly solving
the equations in (4.175) because (as we’ll see) only certain integrals of A, B are of interest, inte￾grals that turn out to be easier to evaluate than finding A, B themselves. Clearly f(1) is a normal
solution, a function of the hydrodynamic fields n(r, t),u(r, t), T(r, t). Although normal solutions
are assumed in the Chapman-Enskog method, we see that f(1) involves gradients of T(r, t) and
u(r, t). As we show, f(1) leads to expressions for J and P having the form assumed in Section
4.4.4. Thus, f(0) is associated with the Euler equations and f(1) with the Navier-Stokes equations.
At the next order, f(2) leads to the Burnett equations[152]. Higher-order terms may not give reliable
improvements over the Navier-Stokes equations because the Chapman-Enskog expansion does not
always converge[153]. The expansion is thought to be asymptotic to solutions of the Boltzmann
equation[151] and thus truncating at low order may give accurate results. The interpretation of the
Burnett equations remains a research topic[154]. We won’t dwell further on the existence of nor￾mal solutions. Normal solutions are expected for small Knudsen numbers, when the time between
successive collisions is small compared to the time scale of variations in macroscopic properties.
4.9 CHAPMAN-ENSKOG THEORY OF TRANSPORT COEFFICIENTS
Before seeking A and B in Eq. (4.180), we develop expressions for the fluxes J and P assuming
A and B are known.
4.9.1 The heat and momentum fluxes associated with f(1)
Using Eq. (4.180) for f(1) in Eq. (4.58), we have the heat flux,
J = m
2

dvf(1)(v)yy2 = m
2σπ3/2
2kT
m
3/2 
dξe−ξ2
ξξ2A(ξ)ξ · ∇ ln T
= k
3σπ3/2
2kT
m
∇T

dξe−ξ2
A(ξ)ξ4 = k
3σπ3/2
2kT
m
∇T

dξe−ξ2
A(ξ)

ξ4 − 5
2
ξ2

,132 ■ Non-Equilibrium Statistical Mechanics
where we’ve used the result of Exercise 4.51 and Eq. (4.179). The last step was not taken randomly.
Using Eqs. (4.175) and (4.176), we have
J =
 k
3σπ3/2
2kT
m

dξe−ξ2
A(ξ) · I(A)

∇T.
Thus, J is in the form of Fourier’s law, J = −κ∇T, with the thermal conductivity
κ = − k
3σπ3/2
2kT
m

dξe−ξ2
A(ξ) · I(A) ≡ k
3σπ3/2
2kT
m [A, A] . (4.181)
The integral represented by brackets [A, A] is non-negative (see Exercise 4.52) and thus κ ≥ 0.
With Eq. (4.180) in Eq. (4.48) we have the momentum flux (where we restore dimensions to D),
P = m

dvf(0)(v)yyΦ(v) = 2m
σπ3/2
2kT
m

dξe−ξ2
ξξB(ξ)
◦
ξξ:D
= 2m
5σπ3/2
2kT
m
◦
D

dξe−ξ2
B(ξ)
 ◦
ξξ:
◦
ξξ
=
 2m
5σπ3/2
2kT
m

dξe−ξ2
B:I(B)
 ◦
D,
where we’ve used an integral theorem from Chapman and Cowling[106, p22] and Eqs. (4.175),
(4.177). Thus P = −2η
◦
D, Newton’s law of viscosity,127 with transport coefficient
η = − m
5σπ3/2
2kT
m

dξe−ξ2
B:I(B) ≡ m
5σπ3/2
2kT
m [B, B] . (4.182)
The dimensionless integral represented by [B, B] is non-negative and hence η ≥ 0.
4.9.2 Calculating A and B
We’ve succeeded in relating transport coefficients to molecular properties through the integrals
[A, A] and [B, B], each involving the linear collision operator which depends on particle inter￾actions through the scattering cross section. It’s traditional in kinetic theory to examine various
models of particle interactions (see Chapman and Cowling[106, Chapter 10]), a task we’re largely
going to sidestep. We mention one case, however, that of Maxwell molecules, particles assumed to
interact with a repulsive force varying with the inverse fifth power of the intermolecular separation.
Many aspects of the Boltzmann equation simplify in this case, and for that reason calculations are
made assuming Maxwell molecules even if they have little physical significance.
In particular, the eigenproblem Iψ = λψ associated with the linear collision operator I can be
solved exactly for Maxwell molecules[155, Chapter 4]. We know in general that I is Hermitian with
real and nonpositive eigenvalues, with different eigenfunctions orthogonal to each other, and that
there are five eigenfunctions corresponding to zero eigenvalue, a consequence of the five collisional
invariants. It’s also isotropic in velocity space (I commutes with the rotation operator defined on that
space) implying its eigenfunctions have the form ψrlm(v) = Rrl(v)Ylm(θ, ϕ), where the spherical
harmonics Ylm are functions of the polar angles (θ, ϕ) of v with respect to an arbitrary direction.
The eigenvalues λrl are independent of m and are at least (2l + 1)-fold degenerate (the spherical
harmonics are defined for l = 0, 1, 2,... and m = 0, ±1,..., ±l). In our application to Maxwell
molecules, the index r takes integer values,128 r = 0, 1, 2,... .
127“Pulling” a factor of two out of Eq. (4.77) allows us to express the rate-of-strain tensor D as symmetric and traceless. 128The spectrum of I is discrete with no accumulation points for purely repulsive power-law potentials proportional to
r−n with n > 2[156]. It’s assumed the spectrum remains discrete for more general intermolecular forces that are not strictly
repulsive, such as the Lennard-Jones potential.Kinetic theory: Boltzmann’s approach to irreversibility ■ 133
The “radial functions” Rrl for Maxwell molecules have the form,[155, Chapter 4]
Rrl(ξ) = ξl
L(l+1/2) r (ξ2), (4.183)
where L(α) n denotes an associated Laguerre polynomial129 of degree n = 0, 1, 2,... , defined by
L(α) n (x) ≡ n
m=0
(−x)m
m!(n − m)!
Γ(n + α + 1)
Γ(m + α + 1), (4.184)
where α > −1 is a real number and Γ(x) is the Γ-function (see [13, Appendix C]). Some special
cases are L(α)
0 (x)=1 and L(α)
1 (x) = α + 1 − x. The set of functions {e−x/2xα/2L(α) n (x)}|∞
n=0 is
complete on the interval 0 ≤ x < ∞ (Szego[78, p104]) and have the orthogonality property130
 ∞
0
e−xxαL(α) n (x)L(α) m (x)dx = 1
n!
Γ(n + α + 1)δn,m. (4.185)
4.9.2.1 Formal evaluation of A and κ
Thus we have explicit expressions for the eigenfunctions of the linear collision operator associated
with Maxwell molecules—a restricted class of particles admittedly, yet which have the important
property of being a complete orthogonal set. A basis set is a basis set, which can be used to represent
A, B for general intermolecular potentials. We start by assuming that A(ξ) can be expanded in a
convergent series of the form
A(ξ) = ∞
m=0
amL(α) m (ξ2),
where α is to be determined. As usual, the expansion coefficients are isolated utilizing orthogonality.
We find
an = 2n!
Γ(n + α + 1)  ∞
0
dξe−ξ2
ξ2α+1L(α) n (ξ2)A(ξ).
Of course, we don’t know A(ξ), but we did impose a constraint on it in Eq. (4.179),
 dξe−ξ2
ξ2A(ξ)=0, equivalent to  ∞
0 dξe−ξ2
ξ4A(ξ)=0 by isotropy. By comparing with the
orthogonality condition, we see that a0 = 0 (L(α)
0 = 1) if we take α = 3
2 , a choice we adopt.131
Thus,
A = A(ξ)ξ = ∞
m=1
amL(3/2) m (ξ2)ξ ≡ ∞
m=1
ama(m)
. (4.186)
We see the expansion in associated Laguerre polynomials has an immediate payoff: From Eq.
(4.175), I(A)=(ξ2 − 5
2 )ξ = −L(3/2)
1 (ξ2)ξ = −a(1). Using the bracket introduced in Eq. (4.181),
consider
αq ≡ [a(q)
, A]=4π
 ∞
0
dξe−ξ2
ξ4L(3/2) q (ξ2)L(3/2)
1 (ξ2) = 15
4 π3/2δq,1. (4.187)
With Eq. (4.187), we have [A, A] = (15/4)π3/2a1. Because [A, A] ≥ 0 (Exercise 4.52), we infer
that a1 ≥ 0. By combining Eq. (4.186) with the definition of αq in Eq. (4.187), we have
αq = ∞
m=1
am[a(q)
, a(m)
] ≡ ∞
m=1
amaqm, (q = 1, 2,...) (4.188)
129In kinetic theory these are often referred to as Sonine polynomials, an obsolete reference to associated Laguerre poly￾nomials; see Whittaker and Watson[157, p352]. 130An equivalent orthogonality expression follows from the substitution x = y2 in Eq. (4.185), in which case we have
 ∞
0 dye−y2
y2α+1L(α) n (y2)L(α) m (y2) = Γ(n + α + 1)δn,m/(2n!).
131By taking α = 3
2 , the constraint in Eq. (4.179) is equivalent to a0 = 0.134 ■ Non-Equilibrium Statistical Mechanics
where aqm = amq; see Exercise 4.52. The a(m) are known and thus the amq can be considered
known. Equation (4.188) comprises an infinite set of equations for the infinite collection of coeffi￾cients132 am. Solutions to infinite sets of equations (when they exist) can be found through a method
of successive approximations. Define the kth approximant, A(k) ≡ k
m=1 a(k) m a(m)
, where

k
m=1
a(k) m amq = αq. (q = 1, 2,...,k) (4.189)
This scheme follows from neglecting terms in these expansions with m>k. It’s assumed that
a(k) m → am and A(k) → A as k → ∞. Using Eq. (4.187), we have from Cramer’s rule
a(k) m = 15
4 π3/2 ∆(k)
1m
∆(k) , (m = 1, 2,...,k) (4.190)
where ∆(k) denotes the determinant of the square symmetric matrix [amq] (m, q = 1, 2,...,k), and
∆1m is the cofactor of a1m in ∆(k)
. From Eq. (4.181), κ ∝ [A, A] ∝ a1, and thus
κ = 5k
4σ
2kT
m
a1 = 5k
4σ
2kT
m
lim
k→∞ ∆(k)
11
∆(k)

. (4.191)
4.9.2.2 Formal evaluation of B and η
Because B = B(ξ)
◦
ξξ (see Eq. (4.177)), we assume it can be expanded as
B = ∞
m=1
bmb(m)
, (4.192)
where
b(m) ≡ L(5/2)
m−1 (ξ2)
◦
ξξ. (4.193)
The reason for α = 5/2 will become apparent, and the shift in indices is for convenience, so that
we don’t end up referring to the (0, 0) element of a matrix. Define, using the bracket in Eq. (4.182),
βq ≡ [b(q)
, B] = −

dξe−ξ2
b(q)
:I(B) = −

dξe−ξ2
L(5/2)
q−1 (ξ2)
◦
ξξ:
◦
ξξ
= −8π
3
 ∞
0
dξe−ξ2
ξ6L(5/2)
q−1 (ξ2)L(5/2)
0 (ξ2) = −5
2
π3/2δq,1,
where we’ve used Eq. (4.175) and ◦
ξξ:
◦
ξξ = 2
3 ξ4 (Chapman and Cowling[106, p18]). We infer that
b1 ≤ 0 because [B, B] ≥ 0. And, because η ∝ [B, B] ∝ |b1|, we have, by the reasoning leading to
Eq. (4.191),
η = m
2σ
2kT
m |b1| = m
2σ
2kT
m
lim
k→∞




∆(k)
11
∆(k)




, (4.194)
where ∆(k) denotes the determinant of the square symmetric matrix bmq ≡ [b(m)
, b(q)
] (m, q =
1, 2,...,k) and ∆(k)
11 is the cofactor of b11 in ∆(k)
.
Explicit expressions for (κ, η) for various molecular models are given in Chapman and
Cowling[106, Chapter 10]. Comparisons with experiment would require the actual form of the in￾termolecular potential. One way by which that information can be inferred is through the second
virial coefficient[5, Section 6.2]. Detailed calculations made this way are in excellent agreement
with experiment[158, Section 8.4].
132It’s obvious but worth stating that to know A we require the coefficients am.Kinetic theory: Boltzmann’s approach to irreversibility ■ 135
SUMMARY
An introduction to kinetic theory was presented, a more microscopic approach to irreversibility than
stochastic methods.
• We started with Liouville’s theorem on the phase-space distribution function ρ(Γ, t),
where Γ ≡ (q1, p1,..., qN , pN ) denotes a vector in Γ-space (N-particle phase space),
with ρ(Γ, t)dΓ the probability at time t that the phase point lies within dΓ ≡
dp1 ... dpN dq1 ... dqN about Γ. Nonequilibrium ensemble averages are constructed from
ρ(Γ, t), ⟨A⟩t ≡  A(Γ)ρ(Γ, t)dΓ, where A(Γ) is a Γ-space function. By Liouville’s the￾orem, ρ(Γ, t) is a constant of the motion, dρ(Γ, t)/dt = 0, implying Liouville’s equation,
i∂ρ(Γ, t)/∂t = Λρ(Γ, t), where Λ is the Liouville operator, Eq. (4.4). ρ(Γ, t) contains far
more information than necessary to calculate measurable quantities; for that reason reduced
probabilities are introduced, fs(1,...,s;t) ≡  ρ(1, . . . , s, s + 1,...,N;t)d(s + 1)··· dN.
The quantity f1 for example is the probability of finding particle 1 in state 1, irrespective of
the states of particles 2,...,N. Macroscopic properties can be calculated from f1 and f2 (see
Section 4.4). Yet if we can’t solve Liouville’s equation for ρ(Γ, t), we can’t calculate reduced
distributions from their definition. The reduced distribution functions satisfy a hierarchy of
coupled dynamical equations, the BBGKY hierarchy, Eq. (4.27), where to determine fs re￾quires that we know fs+1, which in turn requires that we know fs+2, and so on. The challenge
is to devise approximations so that the hierarchy is truncated at a given order.
• Boltzmann derived a closed evolution equation for f1, one of the fundamental equations of
nonequilibrium statistical mechanics. Before taking that up in Section 4.5, we examined in
detail the equations of hydrodynamics in Section 4.4, foundational material for constructing
solutions of the Boltzmann equation in Section 4.8. The basic fields of hydrodynamics are
densities: mass, ρ(r, t) = m  dvf1(r, v, t); momentum, g(r, t) = m  dvvf1(r, v, t) =
ρ(r, t)u(r, t), with u(r, t) the mean velocity at (r, t); and energy, ε(r, t) = εK(r, t) +
εΦ(r, t), with the kinetic energy density εK(r, t) = 1
2m  dvv2f1(r, v, t) and εΦ(r, t) =
1
2
 dvdr′
dv′
f2(r, v, r′
, v′
, t)Φ(r, r′
) the potential energy density, where Φ(r, r′
) is the in￾termolecular potential energy function. The quantities (ρ, g, εK) represent the zeroth, first,
and second velocity moments of the one-particle probability distribution f1; εΦ results from a
weighting of the two-particle distribution f2 by Φ(r, r′
). These relations follow from ensem￾ble averages of microscopic expressions for mass, momentum, and energy density, (ˆρ, gˆ, εˆ);
see Section 4.4.1. The correspondence between ensemble averages of microscopic functions
(ˆρ, gˆ, εˆ) and macroscopic densities (ρ, g, ε) works for mechanical quantities associated with
one or more particles. Not every macroscopic quantity can be so represented. Entropy is not
attached to the dynamics of single particles; it’s a property of the system as a whole.
Balance equations for mass, momentum, and energy (introduced phenomenologically in
Chapter 1) can be derived from moments of the first two equations of the hierarchy. The
continuity equation results from the zeroth velocity moment of the first equation of the hier￾archy and the momentum balance equation is derived from its first velocity moment, thereby
providing us with a microscopic expression for the pressure tensor P. There are two contribu￾tions to P: kinetic, PK(r, t) ≡ m  dvf1(r, v, t)vv, and potential (when we set Dr = 1 in
Eq. (4.43)), PΦ(r, t) ≡ −1
2
 dvdr′
dv′
f2(r, v, r′
, v′
, t)∇rΦ(r, r′
)(r − r′
), where we’ve
used dyadic notation. We separate the contributions to PK into those from the local mean
velocity u(r, t) and random velocities measured relative to the mean, PK = ρuu + P K,
with P K(r, t) ≡ m 
(v − u)(v − u)f1(r, v, t)dv. The pressure tensor P ≡ P K + PΦ.
The hydrostatic pressure P(r, t) ≡ 1
3 Tr P(r, t), a scalar field, has two contributions,
P(r, t) = m
3
 dv(v−u)2f1(r, v, t)− 1
6
 dvdr′
dv′
f2(r, v, r′
, v′
, t)∇rΦ(r, r′
)·(r−r′
),
kinetic, present in all systems but significant in gases, and from internal forces associated with
two-body interactions, significant in liquids.136 ■ Non-Equilibrium Statistical Mechanics
The energy balance equation (4.57) is derived from the first and second equations of
the hierarchy, in a way that requires no knowledge of f3. Energy conservation is an ex￾act consequence of the first two equations of the hierarchy. There are contributions to
the energy flux Jε associated with kinetic and potential energies, Jε ≡ JK + J Φ,
with JK(r, t) = m
2
 dvvv2f1(r, v, t) (third velocity moment of f1), and J Φ(r, t) =
1
2
 dvdr′
dv′
f2(r, v, r′
, v′
, t)Φ(r, r′
)v. Internal energy, the province of thermodynamics,
can be isolated from the total energy by separating the contributions to εK, JK, J Φ into
those from the mean motion and velocities specified relative to the mean (as with PK). In
Eq. (4.58) it’s shown that: εK = 1
2 ρu2 + εK where εK(r, t) ≡ m
2
 dvf1(r, v, t)(v − u)2;
JK = u · P K + u( 1
2 ρu2 + εK) + JK with JK(r, t) ≡ m
2
 dvf1(r, v, t)(v − u)(v − u)2;
and J Φ = uεΦ + JΦ with JΦ(r, t) ≡ 1
2
 dvdr′
dv′
f2(r, v, r′
, v′
, t)(v − u)Φ(r, r′
). In￾ternal energy ε ≡ εK + εΦ is the average random kinetic energy and the average potential
energy. Internal energy flux (heat flux) JQ ≡ JK + JΦ. A balance equation for the mean
kinetic energy follows by projecting the momentum balance equation onto u, Eq. (4.63). By
subtracting that equation from the energy balance equation, we arrive at a balance equation
for internal energy, Eq. (4.64).
• Balance equations for ρ, g, ε follow from moments of the equations of the hierarchy in the
form of relations among ensemble averages. Yet conservation of mass, momentum, and en￾ergy hold for every member of an ensemble, implying that balance equations don’t require
ensemble averaging to be true. It’s shown in Section 4.4.3 that the balance equations for ρ, g, ε
follow from ensemble averages of balance equations for ρ, ˆ gˆ, εˆ.
• The equations of hydrodynamics in their most basic form are the balance equations for mass,
momentum, and internal energy; see Eq. (4.73). These equations must be supplemented with
constitutive relations between the fluxes (P, J) and the fields (ρ,u, ε). For systems in ther￾mal equilibrium, spatially uniform and time independent, u = 0, J = 0, and P is diagonal,
Pij = P δij , with P the hydrostatic pressure. For nonequilibrium systems, J ̸= 0 and P
has off-diagonal elements, effects associated with deviations from uniformity. Write P so
that nonequilibrium contributions add to the equilibrium form, Pij = P δij + Πij , where the
tensor elements Πij model momentum fluxes mediated by inhomogeneities. Deviations from
uniformity are, in a first approximation, represented by linear functions of gradients in local
fields. We take Πij to be a linear combination of velocity gradients, Πij = 
lm Alm
ij ∇lum.
The simplest form of Πij , Eq. (4.77), involves two parameters (η, ζ), the coefficients of shear
and bulk viscosity. In the same way, heat flux is modeled as proportional to temperature gradi￾ents J = −κ∇T, with κ the thermal conductivity. The non-negativity of the entropy source,
σS ≥ 0, implies κ ≥ 0, η ≥ 0, ζ ≥ 0; see Eq. (4.80). Combining the parameterized expres￾sions for P, J with the equations in (4.73), we arrive at a form of the hydrodynamic equations
in (4.81), two equations (three with the continuity equation) in five unknowns, (ρ,u, P, ε, T  ).
The number of independent variables can be reduced by assuming local thermodynamic equi￾librium, that local equations of state P(r) = P[T(r), ρ(r)], ε(r) = ε[T(r), ρ(r)] have the
same functional form as in global equilibrium, P = P(T,ρ), ε = ε(T,ρ), leaving (ρ,u, T)
as independent variables. We assume that temporal variations of P, ε occur through varia￾tions of T(r, t), ρ(r, t), e.g., P(r, t) = P[T(r, t), ρ(r, t)] (an assumption justified in Section
4.8). We arrive in (4.85) at a closed set of nonlinear equations among the five hydrodynamic
fields ρ(r, t),u(r, t), T(r, t), the most general hydrodynamic equations consistent with the
assumptions of local thermodynamic equilibrium and our parameterizations of P and J. We
found normal-mode solutions of the linearized version of these equations, (4.87), with the
long-wavelength form of dispersion relations shown in Table 4.1.
• The Boltzmann equation, (4.106), is a nonlinear integro-differential equation for the one￾body distribution f(r, v, t) in which change occurs in two ways: streaming and collisions.Kinetic theory: Boltzmann’s approach to irreversibility ■ 137
In the absence of intermolecular forces, f would be unchanged along particle trajectories,
df(r, v, t)/dt ≡ (∂/∂t+v ·∇r +a·∇v)f = 0, with a ≡ v˙. The Boltzmann equation adds
a term specifying the rate at which f changes due to particle interactions, df /dt = J[f],
where the collision operator J[f] is a nonlinear functional of f; see the right side of Eq.
(4.106). As J[f] in essence represents the rest of the hierarchy, it can’t be found as an exact
consequence of the equations of motion; it must be devised from approximations that capture
the physics of particle trajectories modified by interactions (collisions).
The derivation of J[f] relies on 1) short-range forces (there is a length r0 beyond which the
intermolecular potential rapidly approaches zero) and 2) dilute systems (average inter-particle
separation far exceeds r0). The diluteness criterion is nr3
0 ≪ 1, with n ≡ N/V . Under
these conditions, particles move with constant velocities v until they encounter other particles
closely enough that they deflect over the collision time τc ≡ r0/v, after which free motion
resumes with velocities v′
. The time τc is microscopic, with τc ≈ 10−13s representative.
Two-body encounters can be seen as discrete events confined to a small volume r3
0, in which
changes in free-state velocities occur almost abruptly, v → v′
, over the short time τc.
The Boltzmann equation is not time-reversal symmetric. The streaming terms reverse sign
under t → −t but the collision term does not. If f(r, v, t) is a solution of the Boltzmann
equation, f(r, −v, −t) is not (in contrast to solutions of the Liouville equation). Irreversibil￾ity is associated with the collision operator. Where in its derivation did we depart from the
laws of mechanics? Collisions effect change in two ways: 1) among particles with velocity
v, some undergo collisions, resulting in a loss from (r, v); 2) among particles with velocities
v′
1, v′
, some undergo collisions such that one of the particles has velocity v, a gain. In loss
collisions (v1, v) → (v′
1, v′
), v1 can be chosen independently of v; the colliding particles are
uncorrelated. In gain collisions (v′
1, v′
) → (v1, v), the velocities of the colliding particles are
constrained to conserve momentum, v′ + v′
1 = v1 + v; we can’t freely vary v′
1 and v′
. It’s
here we’ve departed from the laws of mechanics: v′
1, v′ follow deterministically from force
laws, yet Boltzmann assumed they’re statistically independent, the so-called molecular chaos
assumption. Despite its extra-mechanical nature, the Boltzmann equation makes predictions
about dilute gases in good agreement with experiment.
The Boltzmann equation is nonlinear; the collision term scales quadratically, J[λf] = λ2J[f]
with the streaming terms linear. The nonlinear operator J[f] can be represented in terms of a
bilinear operator Q(f,g) such that J[f] = Q(f,f); see Eq. (4.108). This form of the collision
operator is used in more advanced work; Sections 4.8, 4.9.
• Boltzmann’s H-theorem, an important consequence of the Boltzmann equation, states that
the quantity H(t) ≡  drdvf(r, v, t) ln[f(r, v, t)] never increases in time, dH(t)/dt ≤ 0,
when f(r, v, t) is a solution of Boltzmann’s equation. The proof relies on the symmetries of
J[f]; see Section 4.6.1. Regardless of initial conditions, solutions of the Boltzmann equation
evolve in a way that H(t) never increases. In that regard, −H(t) behaves as a proxy for
entropy S, and for the ideal gas the equality limt→∞ H(t) = −S/k can be arranged. More
generally, H(t) reflects the loss of information associated with nonequilibrium initial states as
the system equilibrates. The connection between H(t) and the second law of thermodynamics
is discussed in Sections 4.6.3 and 4.6.4.
Although H(t) can’t increase in time, it also can’t decrease indefinitely—it’s bounded
from below; see Section 4.6.2. It approaches a limit, the time-invariant state dH/dt = 0,
which we identify with equilibrium, a state achieved if and only if detailed balance holds,
f(r, v′
, t)f(r, v′
1, t) = f(r, v, t)f(r, v1, t), Eq. (4.118), where v′ + v′
1 = v + v1; see
Fig. 4.1. Equilibrium is a special dynamical state that occurs when gain and loss colli￾sions (v′
, v′
1) → (v, v1) and (v, v1) → (v′
, v′
1) occur with equal frequency for all ve￾locities v. Taking the logarithm of the detailed balance condition, we have for the equilibrium138 ■ Non-Equilibrium Statistical Mechanics
distribution ln f′
eq + ln f′
eq,1 = ln feq + ln feq,1. Quantities ψ such that ψ′ + ψ′
1 = ψ + ψ1 are
termed collisional invariants, quantities preserved in elastic collisions, such as particle num￾ber, momentum, and kinetic energy. A key result, on which much of the theory depends,
is that ψi, i = 1, 2, 3, 4, 5 for (1, vx, vy, vz, v2) are the only linearly independent colli￾sional invariants. The logarithm of the equilibrium distribution can therefore be expressed
as a linear combination of fundamental invariants, ln feq(v) = A + B · v + Cv2, where
A, B, C are constants for spatially uniform systems. It has an equivalent parameterization
as a Gaussian, feq(v) = K exp(−(mβ/2)(v − u)2), the Maxwell-Boltzmann velocity dis￾tribution, Eq. (4.121). The constants can be related to known macroscopic information, the
zeroth, first, and second velocity moments; feq(v) = n (mβ/2π)
3/2 exp[−1
2mβ (v − u)
2
],
with β = (1/kT). For inhomogeneous systems, the streaming terms can be nonzero even
though detailed balance has been achieved. Systems can satisfy detailed balance, a con￾dition in velocity space, at each point in configuration space. The most general solution
of J[f]=0 is to let the quantities A, B, C (constants for uniform systems) be arbitrary
functions of (r, t). In that case, we have the generalization of the velocity distribution,
f(r, v, t) = n(r, t) (mβ(r, t)/2π)
3/2 exp[−1
2mβ(r, t) (v − u(r, t))2
], Eq. (4.129), known
as a local Maxwellian function.
• We derived an expression for the collision rate of particles in an equilibrium gas, Eq. (4.137).
An asymptotic form of this expression yields the customary formula for the mean free path,
Eq. (4.138), l = (nσT )−1, with σT the total scattering cross section.
• As noted in Section 4.4.4, the hydrodynamic conservation laws are incomplete without con￾stitutive relations for the momentum and heat fluxes. We developed such relations by pa￾rameterizing local gradients with transport coefficients. To pass from the equations of hy￾drodynamics in (4.81) to their form in (4.85), we had to assume that local equations of
state such as P(r) = P[T(r), ρ(r)] extend to nonequilibrium systems such that P(r, t) =
P [T(r, t), ρ(r, t)]. In so-called normal solutions of the Boltzmann equation (see Section 4.8),
spatiotemporal variations occur through a functional dependence on hydrodynamic fields,
f(r, v, t) = f(v, {ρi(r, t)}), where ρi(r, t) ≡ m  ψi(v)f(r, v, t)dv denote the conserved
moments of f associated with the collisional invariants ψi(v). Normal solutions provide mi￾croscopic justification for the assumptions made in Section 4.4.4 that would otherwise be
considered phenomenological. We presented in Section 4.8 Hilbert’s theorem on the exis￾tence of normal solutions and the Chapman-Enskog method for constructing them. Normal
solutions are expected for small Knudsen numbers, the collision-dominated hydrodynamic
regime wherein the time between successive collisions is small compared to the time scale of
variations in macroscopic properties. In this way, we have a self-consistent construction of ki￾netic theory that relies on the properties of solutions to the Boltzmann equation. Kinetic theory
must be able to derive the equations of hydrodynamics from first principles (which embody
local conservation of mass, momentum, and energy), yet the construction of hydrodynamics
would be phenomenological in introducing transport coefficients as unknown parameters. We
outlined the Chapman-Enskog method for calculating transport coefficients in Section 4.9.
EXERCISES
4.1 Does the Liouville operator (see Eq. (4.4)) obey the product rule of differential calculus? For
functions f(r, p), g(r, p), is Λ(fg) = fΛg + gΛf? For simplicity, let N = 1.
4.2 The N-body distribution ρ(Γ, t) satisfies the Liouville equation ∂ρ(Γ, t)/∂t = −iΛρ(Γ, t),
where Λ is the Liouville operator. Show that phase-space functions A(Γ) satisfy a closely
related dynamical equation, dA(Γ)/dt = iΛA(Γ) (assuming ∂A/∂t = 0). The equation of
motion for ρ(Γ, t) is special in that it derives from Liouville’s theorem. The time dependenceKinetic theory: Boltzmann’s approach to irreversibility ■ 139
of A(Γ) is implicit through the evolution of Γ, A(Γ(t)), where Γ(t) is the image of Γ(t = 0)
under the natural motion in phase space. That is, Γ(t = 0), signifying a set of variables sat￾isfying Hamilton’s equations of motion at time t = 0, is mapped into Γ(t), a set of variables
satisfying Hamilton’s equations at time t.
4.3 Show that the solution of the Liouville equation is time-reversal symmetric, i.e., if ρ(Γ, t)
is a solution to the Liouville equation, then so is the time-reversed function ρ(Γ, τ ), where
τ ≡ −t and Γ is the time-reversed version of phase-space vector Γ. Hint: Λ = −Λ.
4.4 a. Derive Eq. (4.11). First show the orthonormality of the eigenfunctions in Eq. (4.9),

dr1 ... drN ψ∗
{k′}(r1,..., rN )ψ{k}(r1,..., rN ) = δ{k},{k′} ≡ δk1,k′
1 ...δkN ,k′
N .
b. Derive Eq. (4.12).
4.5 Show that the reduced distribution functions are normalized to unity,  fsd1 ··· ds = 1. What
is the dimension of fs? What are the dimensions of ϕl and nl in Eqs. (4.15) and (4.16)?
4.6 Among three particles there are 3!/1! distinct arrangements of two particles. Enumerate the
possibilities. The s-tuple distribution counts all distinct arrangements of s particles out of N.
4.7 Add a term to the Hamiltonian in Eq. (4.18) representing the effects of external forces,
N
i=1 ϕ(ri), where ϕ(r) is a one-body field coupling to every particle (such as electrostatic
potential energy), with F(r) = −∇ϕ(r) the associated force at position r.
a. Derive the Liouville operator LN = −iΛN . A: Add −N
n=1 F(rn)·∂/∂pn to Eq. (4.20).
b. Write down expressions for L1 and L2. Remember where you put these for future use.
L1 = − p
m · ∂
∂r − F(r) · ∂
∂p
L2 = −
p1
m · ∂
∂r1
+ F(r1) · ∂
∂p1

−
p2
m · ∂
∂r2
+ F(r2) · ∂
∂p2

+ O12.
4.8 Show that the interaction operator in Eq. (4.21) is symmetric, Oij = Oji.
4.9 Show that Ls has 1
2 s(s + 1) terms and LN−s has 1
2 [N(N + 1) − s(s + 1)] terms.
4.10 Show that Eq. (4.28) is dimensionally correct.
4.11 Write down the first equation of the hierarchy for a system on which an external force field
F(r) acts. Answer:
 ∂
∂t + v · ∇r +
F
m
· ∇v

F1(r, v, t) = 1
m

∇rΦ(r, r′
) · ∇vF2(r, v, r′
, v′
, t)dr′
dv′
.
(P4.1)
Verify that this equation is dimensionally correct.
4.12 In this exercise we derive two identities that follow from integration by parts, and are better
relegated to an exercise than devoting space to them in the main text. Assume integrated parts
vanish at infinitely distant surfaces in velocity space.140 ■ Non-Equilibrium Statistical Mechanics
a. For a constant vector A and f(r, v, t) the one-particle distribution, show that

dv [A · ∇vf(r, v, t)] v = − 1
m
ρ(r, t)A,
where ρ(r, t) is defined in Eq. (4.31).
b. Show that

dr′
dv′
dv[∇rΦ(r, r′
) · ∇vF2(r, v, r′
, v′
, t)]v =
−

dr′
dv′
dv[∇rΦ(r, r′
)]F2(r, v, r′
, v′
, t),
where Φ is the interparticle potential and F2 is the two-body distribution (properties of
those functions are not used in the derivation other than F2 vanishing as v → ∞).
4.13 Derive Eq. (4.39) from Eq. (4.38). Make use of the results derived in the previous exercise.
4.14 Show the dimensional equivalence of force per area, energy density, and momentum flux.
4.15 For spherically symmetric interatomic potentials Φ(|r − r′
|), show that the potential energy
part of the pressure tensor PΦ is symmetric, i.e., P Φ
ij = P Φ
ji . See Eq. (4.45). Hint: Show that
∂
∂ri
Φ(|r − r′
|) = (ri − r′
i)
|r − r′
|
Φ′
,
where Φ′ ≡ dΦ/d(|r − r′
|).
4.16 Derive Eq. (4.56). You should find that the right sides of Eqs. (4.51) and (4.54) cancel. If an
integral vanishes for all possible regions of integration, the integrand vanishes everywhere.
This reasoning doesn’t hold for definite integrals, e.g.,  1
−1 sin xdx = 0 does not imply that
sin x vanishes identically on [−1, 1]. If however 
E f(x)dx = 0 for any measurable set E,
then f = 0.
4.17 Derive the results in Eq. (4.58).
4.18 In this exercise we work through the ingredients that go into the derivation of Eq. (4.63).
a. Using calculus, verify the identity
u · ∂
∂t(ρu) = 1
2
∂
∂t(ρu2) + 1
2
u2 ∂ρ
∂t .
Combine with the continuity equation to show that
u · ∂
∂t(ρu) = 1
2
∂
∂t(ρu2) − 1
2
u2∇ · (ρu).
b. From Eq. (1.24), u·∇· P = ∇ ·(u · P) − P:∇u and u· [∇ · ρuu] = ∇ ·(u · ρuu) −
ρuu:∇u, where the contraction of second-rank tensors A:B ≡ 
ij AijBij . Show that:
i. u · ρuu = ρu2u;
ii. ρuu:∇u = 1
2 ρu · ∇u2;
iii. ∇ · 
ρu2u

= u2∇ · (ρu) + ρu · ∇u2.
c. Use these results to show that Eq. (4.62) implies Eq.(4.63).Kinetic theory: Boltzmann’s approach to irreversibility ■ 141
4.19 In Section 4.4.3, balance equations for mass, momentum, and energy were derived, but not
for angular momentum. Here we establish that orbital angular momentum is conserved when
the pressure tensor is symmetric. We define an orbital angular momentum density tensor as
the moment of the linear momentum density gˆ(r) (see Eq. (4.32)),
Lˆij (r) ≡ xigˆj (r) − xjgˆi(r) = m
α
[xivα,j − xjvα,i] δ(r − rα), (P4.2)
where xi are the coordinates of r. Note that Lˆij = −Lˆji; angular momentum is described by
an antisymmetric tensor, a property it inherits from the vector cross product, L = r × p.
a. Show in the absence of external forces that
∂
∂tLˆij = −

k
∂
∂xk
Mˆkij + Pˆij − Pˆji,
where the angular momentum flux tensor Mˆkij is the moment of the linear momentum
flux (pressure tensor),
Mˆkij ≡ xiPˆkj − xjPˆki,
where Pˆ = Pˆ K + Pˆ Φ is the full pressure tensor; see Eq. (4.69). Note that Mˆkij , the
elements of a third-rank tensor, are antisymmetric in the second and third indices. Orbital
angular momentum is conserved when Pˆ is symmetric.
b. Show that Mˆkij has the dimension of angular momentum flux.
4.20 Show that the divergence of the pressure tensor [see Eqs. (4.74) and (4.77)] has the form
[∇ · P]i = ∇iP − (ζ + 1
3 η)∇i(∇ · u) − η∇2ui,
where P is the hydrostatic pressure.
4.21 Derive Eq. (4.80). Hint: Symmetrize the velocity gradient in the contraction with the “η” part
of the tensor; let ∇jui → 1
2 (∇jui + ∇iuj ). The pressure tensor is symmetric, and we have
a sum over the indices (i, j). Add and subtract 2
3 δij∇ · u.
4.22 The thermal expansivity α and isothermal compressibility βT measure fractional changes in
V that occur with changes in T and P: α ≡ (1/V )(∂V /∂T)P and βT ≡ −(1/V )(∂V /∂P)T
(see [2, p19]). The quantity α can be of either sign, and βT is always positive.
a. Show that the derivatives required in Eq. (4.82) are given by
∂P
∂T 
ρ
= α
βT
∂P
∂ρ 
T
= 1
ρβT
.
b. In thermodynamics, the derivative (∂U/∂V )T (which is not easily measured), can be
related to measurable quantities through the relation (see [2, p20])
∂U
∂V 
T
= T
∂P
∂T 
V
− P.
Derive the related expression in terms of specific quantities in Eq. (4.83). Hint: Show that
(where M is the total mass)
∂U
∂V 
T
=

∂(U/M)
∂(V /M)

T
≡
 ∂u
∂ρ−1

T
= −ρ2
∂u
∂ρ
T
.142 ■ Non-Equilibrium Statistical Mechanics
4.23 Calculate the entries in Table 4.1 by the strategy of substituting Eq. (4.92) in Eq. (4.93).
a. Show by matching powers of ωnkm at third order (ω0k3, ωk2, ω2k) that a1a2a5 = 0,
a1a2 + a2a5 + a5a1 = Bρ + AC, and a1 + a2 + a5 = 0.
b. Show at fourth order (ω0k4, ωk3, ω2k2) that we have a1b2a5+a2b5a1+a5b1a2 = −ρBσ,
a1b2 + b1a2 + a2b5 + b2a5 + a5b1 + b5a1 = 0, and b1 + b2 + b5 = −(ϕ + χ + σ).
c. We don’t expect the thermal mode to have a propagating component. Show that setting
a5 = 0 implies a2 = −a1 and b2 = b1, what we expect of longitudinal sound modes
propagating in opposite directions with the same speed and the same damping coefficient.
d. Show that Bρ + AC = (∂P/∂ρ)T (cp/cv), where cp, cv are the specific heat capacities at
constant pressure and volume. Show from thermodynamics that α2T /(ρβT ) = cp − cv.
See [2, p20] and use the results of Exercise 4.22.
e. Show that b5 = −κ/(ρcp). Note the factor of cp; that’s not a misprint. Show that
b1 = − 1
2ρ

4
3
η + ζ + κ
 1
cv
− 1
cp
 .
From thermodynamics cp > cv (see [2, p20]); b1 is always negative.
4.24 Show that kinetic equations have the form [see Eq. (4.96)]
 ∂
∂t + p
m · ∂
∂r
+ F · ∂
∂p

f1(r, p, t) = J[f1]. (P4.3)
There are two sources of change to the momentum dependence of f1(r, p, t)—that generated
by external forces and that due to particle collisions. Compare Eq. (P4.3) with Eq. (3.59), the
Fokker-Planck equation for a Brownian particle in an external force field.
4.25 Estimate the collision time τc = r0/v for dilute gases at room temperature. For the interaction
range, use the Lennard-Jones length parameter for noble gases, r0 ≈ 3×10−10 m,[159, p398]
and for the speed, the rms speed in gases, 3kT /m. Contrast this time with an estimate from
the uncertainty principle, τc = ℏ/(kT).
4.26 Show that the velocities v′
1, v′
2 in Eq. (4.98) solve Eq. (4.97) when g is given in Eq. (4.99).
4.27 a. Show that the bilinear form of the collision operator Q(f,g), Eq. (4.108), is indeed a
bilinear form. See footnote 38 on page 9.
b. Show that J[f + g] = J[f] + J[g]+2Q(f,g). This indicates that J is nonlinear.
4.28 Show that (dH/dt)streaming = 0. See Eq. (4.113). Integrate by parts and assume f vanishes
at the boundaries of µ-space; f = 0 outside the region occupied by the gas and f → 0 at an
infinitely distant surface in velocity space.
4.29 Consider the integral over v of the product of the bilinear form of the collision operator
Q(f,g) with an arbitrary function of v, ψ(v), I ≡  dvQ(f,g)ψ(v). Show that
I = −1
8

dvdv1dΩσ(Ω)|v1 − v| [f(v′
1)g(v′
) + f(v′
)g(v′
1) − f(v1)g(v) − f(v)g(v1)]
× [ψ(v′
) + ψ(v′
1) − ψ(v) − ψ(v1)] .
Use the same steps leading to Eq. (4.116).Kinetic theory: Boltzmann’s approach to irreversibility ■ 143
4.30 Fill in the details leading to the H-theorem inequality (4.117), and to Eq. (4.119).
4.31 Consider the function f(x, y) ≡ (x − y) ln(y/x), where (x, y) are positive. Show for y =
x + δ, where |δ| ≪ x, that f ≈ −δ2/x. Note that f vanishes independently of the sign of δ.
4.32 What is the form of the hydrodynamic conservation laws associated with the Boltzmann equa￾tion? Take velocity moments of the Boltzmann equation and use Eq. (4.119) together with
any results from Section 4.4.2. You should find the same conservation laws as in Eqs. (4.37),
(4.49), and (4.57), except they include only the kinetic parts of the pressure tensor and the
energy flux. Long-range internal forces are ignored in the Boltzmann equation, with elastic
scattering modeled by the Boltzmann collision operator. Because the precise forms of P, J
were left unspecified, hydrodynamic equations derived from the Boltzmann equation are the
same as those in Section 4.4.4.
4.33 Find expressions for the quantities K,u, β in Eq. (4.121) in terms of A, B, C in Eq. (4.120).
A: K = exp[A − B2/(4C)],u = −(1/2C)B, β = (−2/m)C.
4.34 Show explicitly that the Maxwell-Boltzmann distribution in Eq. (4.121) satisfies the detailed
balance condition, Eq. (4.118), for any value of u. Hint: Use Eq. (4.97). Does the local
Maxwellian, Eq. (4.129), satisfy Eq. (4.118)?
4.35 What is the dimension of the parameter K in Eq. (4.122)?
4.36 Find the kinetic pressure tensor and heat flux vector associated with the Maxwell-Boltzmann
distribution, Eq. (4.121). A: PK
ij = nkT δij and JK = 0.
4.37 Derive Eq. (4.127).
4.38 Evaluate the integral in Eq. (4.131).
4.39 Show that the Sackur-Tetrode formula for the entropy of an ideal gas, Eq. (4.132), can be
written in the general form expected from thermodynamics, S = Nkf(U/N, V /N).
4.40 We stated that the Gibbs entropy is a constant of the motion as a consequence of general
principles of classical dynamics. Use the Liouville equation to verify explicitly from Eq.
(4.136) that
d
dt
SG = 0.
Hint: The Liouville operator is self-adjoint, Λ† = Λ.
4.41 Derive Eq. (4.137) starting from ν(v) = σT
 dv1|v1 − v|f0(v1). Change variables
with u ≡ v1 − v. Let v define the polar axis of the coordinate system. Use the inte￾gral  ∞
0 x2e−x2
sinh(γx)=(γ/4) + (√π/8)(2 + γ2) exp(γ2/4) erf(γ/2) (Gradshteyn
and Ryzhik[88, p365]), where the error function erf(x) ≡ (2/
√π)
 x
0 e−t2
dt. Note that
limx→∞ erf(x)=1.
4.42 Estimate the collision frequency of an atom in an argon gas in thermal equilibrium, under
temperature and pressure conditions of your choice. Compare your answer with estimates of
the collision frequency in Brownian motion; see Exercise 3.1.
4.43 Show that the moments of the solution of Boltzmann’s equation associated with the collisional
invariants satisfy conservation laws. Starting from Eq. (4.140), show that
∂ρi
∂t + m∇ · 
ψi(v)vf(r, v, t)dv = 0.
Thus m  vψi(v)f(r, v, t)dv is a current associated with the invariants.144 ■ Non-Equilibrium Statistical Mechanics
4.44 Derive the system of equations in (4.143).
4.45 Derive the linearized collision operator, Eq. (4.146).
4.46 Derive the linearized Boltzmann equation, Eq. (4.147). Start with the Boltzmann equation
[(∂/∂t) + v · ∇]f = Q(f,f) and let f = f(0) + h, where Q(f(0), f(0))=0; show its
equivalence with
∂h
∂t + v · ∇h = L(h) + Q(h, h),
where L is defined in Eq. (4.146). Equation (4.147) follows by ignoring the nonlinear term
Q(h, h). It’s obvious but worth noting that Q(f,g) has no projection on the null space of L.
4.47 Derive Eq. (4.148).
4.48 Derive Eq. (4.166).
4.49 Using results from Section 4.8.2, show that the Euler equations are obtained from the lowest
order time derivatives D(0)
j , j = 1,... 5. Show that (with i = x, y, z ≡ 2, 3, 4):
∂ρ
∂t = D(0)
1 ;
∂ρui
∂t = D(0)
i ;
∂ε
∂t = 1
2
D(0)
5 −
i
uiD(0)
i +
1
2
u2D(0)
1 .
Let P = nkT and ε= 3
2nkT.
4.50 Show that the solvability conditions associated with Eq. (4.175) are satisfied. Show that

dvf(0)(v)ψi(v)
my2
2kT − 5
2

yj = 0,

dvf(0)(v)ψi(v)

ykyl − 1
3
y2δk,l
= 0.
4.51 For F = F(y) a scalar function of the vector y, and for D a constant vector, show that

F(y)y (y · D) dy = 1
3
D

F(y)y2dy.
Hint: By symmetry,  y2
xF(y)dy =  y2
yF(y)dy =  y2
zF(y)dy.
4.52 Verify that the bracket defined in Eq. (4.181) is never negative. It’s best to show this for two
scalar functions F, G; the vector case then follows. Let
[F, G] ≡ − 
f(0)(v)F(v)I (G) dv
= −

dvdv1dΩσ(Ω)|v1 − v|f(0)(v)f(0)(v1)F(v) [G(v′
1) + G(v′
) − G(v1) − G(v)] ,
where I is defined in Eq. (4.172). Bracket notations have long been used in kinetic theory to
symbolize complicated expressions, going back to Enskog’s thesis at least. Using the argu￾ments leading to Eq. (4.116), show that
[F, G] =1
4

dvdv1dΩσ(Ω)|v1 − v|f(0)(v1)f(0)(v)
× [F(v′
1) + F(v′
) − F(v1) − F(v)] [G(v′
1) + G(v′
) − G(v1) − G(v)] .
Thus, [F, F] ≥ 0. Note that [F, G]=[G, F].CHAPTER 5
Weakly coupled systems:
Landau-Vlasov theories
T HE Boltzmann equation applies to dilute gases having short-ranged interactions. Particle colli￾sions in such systems are well localized in space and time, allowing us to model the dynamics
as a succession of binary scattering events caused primarily by the repulsive part of the intermolec￾ular potential. Many gases have potentials featuring repulsive interactions at short distances, such
as the Lennard-Jones potential (see [5, p181]). Here we consider a different type of system, weakly
coupled, in which the interaction potential is everywhere small relative to kinetic energies.1 Plasmas
approximate weakly coupled gases (and the plasma state is the most abundant form of visible matter
in the universe). In this chapter, we consider the application of kinetic theory to plasmas. We start
with the Landau equation[160], the kinetic equation for homogeneous weakly coupled gases.
5.1 HOMOGENEOUS WEAK COUPLING: THE LANDAU EQUATION
We can interpret σ(Ω)|v1−v| in the Boltzmann equation as the rate w at which particles of velocities
(v, v1) transition to (v′
, v′
1), and thus, for spatially homogeneous systems (suppressing r, t),
∂
∂tf(v) = 
dv1dΩw(v, v1; v′
, v′
1) [f(v′
1)f(v′
) − f(v1)f(v)] .
Writing the transition rate in the form w(v, v1; v′
, v′
1) is misleading, however. The four arguments
(v, v1, v′
, v′
1) are not independent; there’s a constraint among them, implying w is a function of
three vector variables. To show that, introduce the velocity deviation vector ∆ with v′ = v + ∆
and v′
1 = v1 − ∆ (ensuring momentum conservation, v′ + v′
1 = v + v1). From Eq. (4.98),
∆ = 1
2 (g − gϵˆ), where g = v1 − v, g = |g|, and ϵˆ is a unit vector in the scattering direction;
see Fig. 4.2. Consider the volume-preserving (unity Jacobian) transformation, v → 1
2 (v + v′
),
v1 → 1
2 (v1 + v′
1), v′ → v′ − v, and v′
1 → v′
1 − v1. Under this change of variables,
w(v, v1; v′
, v′
1) → w  1
2 (v + v′
), 1
2 (v1 + v′
1); v′ − v, v′
1 − v1

= w(v + 1
2∆, v1 − 1
2∆; ∆,−∆)
≡ w(v + 1
2∆, v1 − 1
2∆; ∆).
From detailed balance, w is invariant under the interchange of incident and scattered particles, im￾plying w(v + 1
2∆, v1 − 1
2∆; ∆) = w(v + 1
2∆, v1 − 1
2∆; −∆); thus, w is an even function in the
variable ∆ to the right of the semicolon. Boltzmann equation’s can then be written
∂
∂tf(v) = 
dv1dΩw(v + 1
2∆, v1 − 1
2∆; ∆) [f(v1 − ∆)f(v + ∆) − f(v1)f(v)] . (5.1)
1The terms comes from statistical mechanics where subsystems in thermal contact are said to be weakly coupled if their
energy of interaction is small compared to subsystem energies[5, p84].
DOI: 10.1201/9781003512295-5 145146 ■ Non-Equilibrium Statistical Mechanics
Boltzmann’s equation in this form is well suited to the analysis of weakly coupled systems.
Small interaction energies imply that the deviations ∆ are small compared to particle velocities,
|∆|≪|v|, |v1|. We can then approximate the functions in Eq. (5.1) with their Taylor expansions,
w(v +
1
2
∆, v1 − 1
2
∆; ∆) = w(v, v1; ∆) + 1
2
∆ · [∇v − ∇v1 ] w(v, v1; ∆) + O(∆2),
f(v + ∆) = f(v) + ∆ · ∇vf(v) + 1
2
∆∆:∇v∇vf(v) + O(∆3)
f(v1 − ∆) = f(v1) − ∆ · ∇v1 f(v1) + 1
2
∆∆:∇v1∇v1 f(v1) + O(∆3).
Substitute these expressions in Eq. (5.1) and keep terms through second order; we omit the algebra.
The first-order terms vanish and the second-order terms simplify by integrating by parts with respect
to v1. We find:
∂
∂tf(v) = 1
2

a,b

dv1dΩ∆a∆b

w(v, v1; ∆)

∂2f(v)
∂va∂vb
f(v1) − ∂f(v)
∂va
∂f(v1)
∂v1,b 
+
∂w(v, v1; ∆)
∂va

∂f(v)
∂vb
f(v1) − f(v)
∂f(v1)
∂v1,b  ,
where (a, b)=1, 2, 3. This expression can be written compactly:
∂f(v)
∂t = ∇v ·

dv1G · (∇v − ∇v1 ) f(v)f(v1) (5.2)
with the symmetric tensor
G ≡ 1
2

dΩw(v, v1; ∆)∆∆. (5.3)
As we’ll show, G can be related to the intermolecular potential.
We first develop an expression for ∆. The difference between the asymptotes v and v′ can be
calculated from the force exerted on the particle over its trajectory,
∆ = v′ − v = 1
m
 ∞
−∞
Fdt = − 1
m
 ∞
−∞
∇Φ (|r(t) − r1(t)|) dt, (5.4)
where Φ is the energy of interaction between the particle at its current position r(t) and the other
particle at position r1(t) [instantaneous interactions assumed, with Φ(r) = Φ(|r|)]. By definition,
r1(t) − r(t) = r1(0) − r(0) +  t
0 [v1(t
′
) − v(t
′
)]dt
′ ≡ a +  t
0 g(t
′
)dt
′
, where a is a constant and
g(t) = v1(t)−v(t) is the time-dependent generalization of g = v1 −v. Because ∆ is small (weak
coupling), we approximate the argument of Φ in Eq. (5.4) as
r1(t) − r(t) ≈ a + gt ≡ rt. (5.5)
With Eq. (5.5), we’ve approximated the actual, slightly curved trajectory (small deviations ∆) with
one of constant velocity. We introduce the Fourier transform of the potential, Φ(r) =  dkeik·rΦk,
which places the r-dependence in an exponential where it’s easy to analyze. With Φ(r) = Φ(r) a
central potential, Φk is real and a function of the magnitude of k, Φk = Φk. From Eq. (5.4),
∆ = − i
m

dt

dkΦkkeik·rt . (5.6)
We now turn to G in Eq. (5.3). Referring to Fig. C.6, which shows the geometry of two-body
collisions in the reference frame of an incident particle, we have w(v, v1; ∆)dΩ ≡ gσ(Ω) =
2πgbdb, where b is the impact parameter. Combining Eq. (5.6) with Eq. (5.3),
G = − π
m2
 ∞
0
gbdb
 ∞
−∞
dt1
 ∞
−∞
dt2

dk1

dk2k1k2Φk1Φk2 exp (i[k1 · rt1 + k2 · rt2 ]).Weakly coupled systems: Landau-Vlasov theories ■ 147
The exponential can be written exp(i[k1 ·rt1+k2 ·rt2 ]) = exp(i(k1+k2)·rt1 ) exp(ik2 ·g(t2−t1)).
From the scattering geometry, 2π
 ∞
0 bdb
 ∞
−∞ dt1g ≡  drt1 . Dropping the subscript from rt1 and
defining τ ≡ t2 − t1,
G = − 1
2m2

dr
 ∞
−∞
dτ

dk1dk2k1k2Φk1Φk2 exp(i[k1 + k2] · r) exp(ik2 · gτ ).
The integrations are now straightforward. We find:
G = 8π4
m2

dkΦ2
kδ(k · g)kk. (5.7)
Thus we have expressed G is a function of the Fourier components of the interaction potential, Φk.
Note that only modes with k orthogonal to g contribute.
Using Eq. (5.7), Eq. (5.2) can be written,
∂f(v1)
∂t = 8π4
m2

dv2

dkΦ2
kk · ∇v1 δ(k · g)k · (∇v1 − ∇v2 ) f(v1)f(v2). (5.8)
This can be put in symmetric form by replacing ∇v1 with ∇v1 − ∇v2 . For any function F(v)
vanishing sufficiently fast at infinity,  dv∇vF(v)=0. Thus,
∂f(v1)
∂t = 8π4
m2

dv2

dkΦ2
kk · ∇12δ(k · g)k · ∇12f(v1)f(v2), (5.9)
where ∇12 ≡ ∇v1 −∇v2 . Equation (5.9) is one form of the Landau equation[161][162][163][164].
Its solutions satisfy the H-theorem, with Maxwellians as stationary solutions (see Exercise 5.1).
x
y
z
g k
ϕ
θ
Figure 5.1 Coordinate system for evaluating the integral in Eq. (5.7) with the z-axis of k￾space aligned with g.
The Landau equation can be put in a more useful form. First, evaluate Eq. (5.7) in a frame with
the z-axis aligned with g; see Fig. 5.1. In that way, the nine elements of G can be found as cases of:
Gx x
y y
z z  = 8π4
m2
 ∞
0
k4Φ2
kdk
 π
0
δ(kg cos θ) sin θdθ
 2π
0
dϕ



sin θ cos ϕ
sin θ sin ϕ
cos θ



×



sin θ cos ϕ
sin θ sin ϕ
cos θ


 .
In this frame, only Gxx, Gyy are nonzero,
G =
 B
gm2
 

100
010
000

 , (5.10)148 ■ Non-Equilibrium Statistical Mechanics
where
B ≡ 8π5
 ∞
0
k3Φ2
kdk. (5.11)
Note that B ≥ 0. We can now find G in any frame because in this one frame it can be written as
a tensor equation, G = 
B/gm2
(I − IzIz), with I the unit tensor and Iz ≡ g/g. In an arbitrary
frame, therefore,2
Gab =
 B
gm2
 δab − gagb
g2

. (5.12)
We therefore have another expression for the Landau equation,
∂f(v1)
∂t = B
m2

ab

dv2 (∇12)a
1
g3

g2δab − gagb

(∇12)b f(v1)f(v2). (5.13)
Equations (5.9) and (5.13) are equivalent, yet (5.13) has a property not obvious in (5.9), that
details of the interaction potential appear only in B. The mechanics of the collision operator is
the same for all homogeneous weakly coupled gases. What distinguishes one from another, B,
amounts to setting the time scale. This feature is a limiting behavior associated with weak interac￾tions. Charges influence each other, weakly, at large separations because of the long-range nature of
electrical forces. The Landau kinetic equation3 describes the evolution of f(v, t) by means of weak
long-range interactions—“collisions.” When applied to the Coulomb potential, however, it runs into
difficulties. The Fourier transform of r−1, Φk ∼ k−2 (see Exercise 5.4), and thus from Eq. (5.11),
B ∼
 ∞
0
k3 ×
1
k4 dk = ln(∞) − ln(0). (5.14)
We have a logarithmic divergence at both limits.4 The divergence at k → ∞ is due to our neglect of
a hard core and that at k → 0 is due to the long range of the Coulomb force, which we see is “too
long.” If the potential weren’t purely Coulomb and featured cutoff parameters, we’d have a finite
theory, B ∼  kmax
kmin dk/k = ln(kmax/kmin), and plasmas do involve such parameters. A system of
mobile charges screens electric fields, implying the potential can’t be that of a bare point charge. The
potential is approximately Coulomb up to the screening length, the Debye length (appropriate for
the weak coupling regime), rD ≡ ϵ0kT/(ne2), with n the number density and e the magnitude of
the electronic charge (SI units). Thus, kmin ∼ r−1
D . That potential energies be small compared with
kinetic implies a distance below which the weak coupling assumption breaks down, the Landau
length, rmin ≡ e2/(4πϵ0kT), and thus kmax ∼ r−1
min. Consistency requires the system be sufficiently
dilute so that rD > rmin. These considerations beg the question of the potential energy environment
in plasmas.
5.2 CONNECTION WITH THE FOKKER-PLANCK EQUATION
In Chapter 3 we derived the Fokker-Planck equation for the probability distribution of a colloidal
particle subject to small molecular impacts (Brownian motion), and, as we’ve now seen, the Landau
equation emerges as the limiting case of the Boltzmann equation for weak interactions. Are the
two related, the Landau and Fokker-Planck equations? The Landau equation can indeed be derived
(in a nontrivial calculation) starting from the Fokker-Planck equation for an inverse-square force
law[166]. Here we show simply that the Landau equation has the Fokker-Planck form.
2By the principal of covariance, a tensor equation true in one frame is true in all frames; see [6, p84]. 3The Landau equation, like the Boltzmann equation, is a nonlinear integro-differential equation.
4Divergences occur frequently in modern theories of physics; the renormalization method in quantum field theory was
devised to handle them. Dirac wrote that the laws of nature “. . . do not govern the world as it appears in our mental picture
in any very direct way, but instead they control a substratum of which we cannot form a mental picture without introducing
irrelevancies”[165, pvii].Weakly coupled systems: Landau-Vlasov theories ■ 149
From Eq. (5.2), we can write the Landau equation
∂
∂tf(v, t) = 
i
∂
∂vi

− Ai +
j
∂
∂vj
Bij
f(v, t), (5.15)
where
Ai ≡

dv1


j
Gij
∂
∂v1j
f(v1, t) + f(v1, t)

j
∂
∂vj
Gij


Bij ≡

dv1Gijf(v1, t). (5.16)
Equation (5.15) has the form of the Fokker-Planck equation [see Eq. (3.37)] generalized to three
dimensions with a friction vector Ai and a diffusion tensor Bij . Note that these quantities change
as f(v, t) changes, whereas they’re constant in the Fokker-Planck equation. That there is a such
a connection is satisfying theoretically—the Fokker-Planck equation emerges from the theory of
random processes and the Boltzmann equation is an “almost” dynamical theory.5
5.3 INHOMOGENEOUS PLASMAS: THE VLASOV EQUATION
A plasma is an ionized gas, which we take to be electrically neutral.6 Charges repel like charges and
attract those of the opposite sign, creating clouds of opposite charge surrounding the particles of the
plasma. We can use statistical mechanics to model the effect. Assume a fixed charge Q > 0 held
in place at the origin, surrounded by a plasma. Denote the local density of negative charges n(r)
and that of positive charges p(r). Poisson’s equation connects charge density with the electrostatic
potential φ(r),
∇2φ(r) = − e
ϵ0
[p(r) − n(r)] − Q
ϵ0
δ(r). (e > 0) (5.17)
Statistical physics provides a relation between the average charge density and the potential, the
barometric formula, Eq. (4.128), p(r) = n exp(−eφ(r)/kT) and n(r) = n exp(eφ(r)/kT), with
n the density where φ = 0, which by neutrality is the same for positive and negative charges. We
therefore have the Poisson-Boltzmann equation (which we note is nonlinear),7
∇2φ(r) = 2e
ϵ0
n sinh(eφ(r)/kT) − Q
ϵ0
δ(r). (5.18)
Invoking the weak coupling assumption (sinh x ≈ x for x ≪ 1), Eq. (5.18) can be linearized,
∇2φ(r) = 2
r2
D
φ(r) − Q
ϵ0
δ(r), (5.19)
where8 rD = ϵ0kT/(e2n). The spherically symmetric solution of Eq. (5.19) is (see Exercise 5.7)
φ(r) = Q
4πϵ0r exp(−
√
2r/rD). (5.20)
Clearly φ → 0 for r ≫ rD. The average charge surrounding Q is −Q for r ≫ rD; Exercise 5.8.
5The molecular chaos assumption cannot be derived from the laws of mechanics.
6In the simplest situation, electrons and positive ions are created together in the process of forming a plasma and we
have charge neutrality. Nonneutral plasmas are possible, and comprise an active field of research; see Davidson[167]. 7Equation (5.18) is equivalent to the Debye-Huckel equation of ionic-solution theory (derived 1923); see Friedman[ ¨ 168]. 8The Debye length rD is traditionally defined without the factor of two. There are other, slightly different definitions of
rD depending on the geometry of the system.150 ■ Non-Equilibrium Statistical Mechanics
Debye shielding, exemplified by Eq. (5.20), provides a natural interpretation of a cutoff pa￾rameter rendering the Landau theory finite. The charge Q could be from a foreign object in the
plasma or it could be an electron or ion of the plasma. We can think of each electron as sur￾rounded by positive charges, with each ion surrounded by electrons. Charges separated by more
than a few Debye lengths don’t influence each other because of screening. To take into account
this characteristic feature of plasmas (screening), Landau’s theory must be extended to inhomoge￾neous systems. This is done by adding a flow term to the equation and by including spatial depen￾dence in the collision integral. Both distribution functions must be evaluated at the same location
[see Eq. (5.9)], but to preserve the symmetry between particles 1 and 2 we make the replacement
f(v1, t)f(v2, t) →  dr2δ(r1 − r2)f(r1, v1, t)f(r2, v2, t). Thus, as a first step, we have
 ∂
∂t + v1 · ∇r1

f(r1, v1, t) = 8π4
m2

dv2

dr2

dkδ(r1 − r2)Φ2
k (5.21)
× k · ∇12δ(k · g)k · ∇12f(r1, v1, t)f(r2, v2, t).
Equation (5.21) is incomplete, however. The Boltzmann equation was derived assuming free
paths between collisions, but charged particles are not free; they’re subject to electromagnetic forces.
Accelerated motion can be included in the Boltzmann equation for accelerations caused by known
external forces; see Eq. (P4.3). For plasmas we must develop a model of internally generated, local
forces. Noting that in homogeneous plasmas the net internal field is zero, we define a local potential
energy associated with deviations from uniformity,9
U(r, t) ≡

Φ(r − r′
)dr′

[f(r′
, v′
, t) − ϕ(v′
, t)] dv′
, (5.22)
where ϕ(v, t) is the solution of the Landau equation.10 The negative gradient of U(r, t) specifies a
local force which acts as if it were produced by an external field. Adding this force to Eq. (5.21),
 ∂
∂t+v1 · ∇r1

f(r1, v1, t) − 1
m
∇r1U(r1, t) · ∇v1 f(r1, v1, t) (5.23)
= 8π4
m2

dv2

dr2

dkδ(r1 − r2)Φ2
kk · ∇12δ(k · g)k · ∇12f(r1, v1, t)f(r2, v2, t).
Equation (5.23) coupled with Eq. (5.22) is the Vlasov-Landau equation.
The interaction potential Φ(r) occurs linearly in U(r) and quadratically in the collision integral
through Φ2
k. In the weak coupling approximation, it’s meaningful to neglect collisions in comparison
with the effects of long-range interactions:
 ∂
∂t + v · ∇r

f(r, v, t) − 1
m
∇rU(r, t) · ∇vf(r, v, t)=0. (5.24)
Equation (5.24), coupled with Eq. (5.22), is the Vlasov equation or the collisionless Boltzmann
equation.
11 Because ϕ(v, t) in Eq. (5.22) (solution of the Landau equation) is independent of posi￾tion, it adds a term to U(r) proportional to Φk=0, the gradient of which is zero. Equation (5.24) can
9“Deviations from uniformity” has been a guiding principle at several points, in deriving the off-diagonal part of the
pressure tensor and in the Chapman-Enskog theory. 10The distribution functions f(r, v, t) and ϕ(v, t) have the same dimensions; see Exercise 5.3. 11The Vlasov equation was developed in 1938[169] (see also Vlasov[170]) as a way of modeling plasma oscillations (a
collective effect known since the 1920s through the work of Tonks and Langmuir[171]), a phenomenon the Boltzmann equa￾tion based on short-range interactions cannot describe. The collisionless Boltzmann equation has been applied to problems
of galactic dynamics starting with the 1915 work of Jeans[172] and is a fundamental equation in that subject.Weakly coupled systems: Landau-Vlasov theories ■ 151
then be written with ϕ(v, t) omitted,12
 ∂
∂t +v1 ·∇r1

f(r1, v1, t)− 1
m

∇r1

dr2dv2Φ(r1 − r2)f(r2, v2, t)

·∇v1 f(r1, v1, t)=0.
(5.25)
For definiteness, consider a charge-neutral system where electrons are able to move through
a uniform background of positive charge.13 Assume particles interact microscopically through the
Coulomb potential; the effective potential energy environment is obtained from Eq. (5.22). Thus,
U(r, t) = − e2
4πϵ0

dr′ 1
|r − r′
|

dv′ [f(r′
, v′
, t) − ϕ(v′
, t)]
  
n(r′
, t) − n
≡ e
4πϵ0

dr′ 1
|r − r′
|
ρ(r′
, t), (5.26)
with n(r, t) the density of electrons at (r, t), ne the charge density of the positive background, and
ρ(r, t) = e[n − n(r, t)] the net local charge density. It’s straightforward to show that
∇2U(r, t) = − e
ϵ0
ρ(r, t). (5.27)
The Vlasov equation consists of equations (5.24) and the Poisson equation (5.27). The Vlasov￾Landau equations are self-consistent: Charged particles respond to the same potential they generate.
5.4 LANDAU DAMPING
The Vlasov equation is time-reversal invariant [as readily seen from Eq. (5.25)] and therefore cannot
describe the approach to equilibrium. Said differently, it does not have a definite direction of time.
It’s surprising, then, that it predicts a type of damping, Landau damping, derived in 1946[173]
and observed in 1964[174], in which a longitudinal space charge wave is damped in collisionless
plasmas. Plasma physics is outside the scope of this book,14 yet an example of damping without
dissipation (without entropy production) is within its purview. We outline the derivation.
Consider the simplest problem of electrons in a uniform positive-charge background of density
n in an unmagnetized, neutral, collisionless plasma. The local force on electrons, −eE(r, t), is
supplied by an electric field, with e > 0. The Vlasov “equation” consists of two coupled equations:
 ∂
∂t + v · ∇
f(r, v, t) − e
m
E(r, t) · ∇vf(r, v, t)=0, (5.28)
for the electron distribution function f(r, v, t), where E(r, t) = −∇φ(r, t) with
∇2φ(r, t) = − e
ϵ0

n −

f(r, v, t)dv

(5.29)
governing the scalar potential.
12We can drop ϕ(v, t) from the dynamical equation, but not from Eq. (5.22). 13Because ions are so much more massive than electrons, an approximation is to ignore the motion and spatial distribution
of positive ions, a model referred to as the “jellium” approximation. In semiconductors, ions—dopants attached to lattice
sites—are indeed immobile, with the exception of electromigration, the gradual motion of ions under high-current conditions. 14Landau damping is not restricted to plasma physics. It has been applied to the collisionless motions of stars in galaxies
moving through the gravitational field produced by other stars; see Lynden-Bell[175]. See also Binney and Tremaine[176].152 ■ Non-Equilibrium Statistical Mechanics
5.4.1 The linearized Vlasov equation
The Vlasov equation is nonlinear. Even though the collision term has been dropped, the self￾consistent potential renders the problem nonlinear. To make progress, linearize. Write the distri￾bution function
f(r, v, t) = f0(v) + f1(r, v, t), (5.30)
where f1(r, v, t) describes small, local, time-dependent deviations from a spatially uniform time￾independent distribution f0(v), with  f0(v)dv = n, and |f1| ≪ f0. With Eq. (5.30), Eqs. (5.28)
and (5.29) become, to first order in small quantities,
 ∂
∂t + v · ∇
f1(r, v, t) + e
m
∇φ(r, t) · ∇vf0(v)=0 (5.31)
∇2φ(r, t) = e
ϵ0

f1(r, v, t)dv. (5.32)
We can, in the usual way, seek plane-wave solutions for small-amplitude waves, f1(r, v, t) =
f1(v) exp[i(k · r − ωt)] and φ(r, t) = φ0 exp[i(k · r − ωt)], propagating in the k-direction with
phase speed vϕ ≡ ω/k. With these substitutions, Eqs. (5.31) and (5.32) become
(−iω + ik · v)f1(v)+i e
m
φ0k · ∇vf0(v)=0
−k2φ0 = e
ϵ0

f1(v)dv.
These equations can be combined,

k2 +
e2
mϵ0
 k · ∇vf0(v)
ω − k · v
dv

φ0 = 0.
The terms in square brackets must vanish, leaving us with a dispersion relation, the connection
ω = ω(k) between frequency and wavelength,
1 +
e2
mϵ0k2
 k · ∇vf0(v)
ω − k · v
dv = 0. (5.33)
We must find ω so that Eq. (5.33) is satisfied for given k and f0(v). There’s an obvious problem,
however—the integral isn’t defined! There is some v for which ω = k · v (and we integrate over all
v). Note that v is a particle velocity and (ω, k) are wave parameters. The singularity occurs when
electron velocities match the phase velocity of the wave.
To circumvent this problem, Fourier analyze with respect to spatial variables only, leaving the
time dependence intact, f1(r, v, t) = f1(v, t) exp(ik·r), E(r, t) = E(t) exp(ik·r), and φ(r, t) =
φ(t) exp(ik · r), in which case Eqs. (5.31) and (5.32) reduce to15
 ∂
∂t + ik · v

f1(v, t)=i e2
mϵ0k2 k · ∇vf0(v)

f1(v′
, t)dv′ (5.34)
φ(t) = − e
ϵ0k2

f1(v, t)dv. (5.35)
The electric field amplitude E(t) = −ikφ(t). Note that E is parallel to k, which specifies the
longitudinal direction.16 At this point, decompose v into vectors parallel and perpendicular to k,
15All fields should be labeled with an index k; ignored here to ease the notation. 16These waves are termed “electrostatic” because E = −∇φ implies k × E = 0. Longitudinal E-fields should be
contrasted with the transverse fields of electromagnetic waves in unbounded free space. One also finds longitudinal fields in
waveguides.Weakly coupled systems: Landau-Vlasov theories ■ 153
v = u + v⊥ so that k · v = ku. In this notation, the perturbed electron density
δn(t) ≡

f1(v, t)dv =

f1(u, v⊥, t)dudv⊥ ≡
 ∞
−∞
g(u, t)du. (5.36)
The quantity g(u, t) is the perturbed distribution as a function of longitudinal speed (transverse
velocities have been integrated out). Integrate Eq. (5.34) over v⊥, for which we find
 ∂
∂t + iku
g(u, t) = −ikη(u)
 ∞
−∞
g(u′
, t)du′
, (5.37)
with
η(u) ≡ − e2
mϵ0k2
d
du

f0(u, v⊥)dv⊥ ≡ − e2
mϵ0k2
d
du
g0(u) ≡ − e2
mϵ0k2 g′
0(u). (5.38)
Note that  ∞
−∞ g0(u)du = n. Equation (5.37) is the linearized Vlasov equation, a linear integro￾differential equation.
5.4.2 Initial value problem
Landau analyzed Eq. (5.37) as an initial value problem for which the perturbation f1 is prescribed at
time t = 0 and found for later times. This strategy underlies not Fourier transforming with respect
to time; instead one works with Laplace transforms which naturally incorporate initial conditions.17
The Laplace transform L, an integral transform converting a function of a real variable g(t) to a
function G(z) of a complex frequency variable z, is defined
G(z) = L[g](z) ≡
 ∞
0
e−ztg(t)dt. (5.39)
Note that L restricts t to positive values, thereby introducing a direction of time to the Vlasov
equation.18 The Laplace transform of derivatives is related to the transform of the function:
L[g′
](z) =  ∞
0
e−zt dg
dt
dt = −g(t = 0) + zG(z).
The inverse transform is defined
g(t) = L−1[G](t) ≡ 1
2πi
lim
T→∞  γ+iT
γ−iT
eztG(z)dz = 
j
(residues)j , (5.40)
where the integration path is the line Re (z) = γ in the complex z-plane, with γ greater than the
real part of all singularities of G(z), closed with an infinite semicircle (the Bromwich contour[13,
p244]), where the last equality follows from the Cauchy residue theorem.
Take the Laplace transform of Eq. (5.37); we find
−g(u, t = 0) + (z + iku) G(u, z) = −ikη(u)
 ∞
−∞
du′
G(u′
, z),
where G(u, z) ≡ L[g(u, t)]. Thus, G(u, z) satisfies the integral equation
G(u, z) = 1
z + iku 
g(u, t = 0) − ikη(u)
 ∞
−∞
du′
G(u′
, z)

. (5.41)
17Fourier transforming with respect to time assumes the future is the same as the past, which should apply to a time￾reversal invariant theory; that’s what makes Landau damping subtle, a transfer of energy between wave and particle. 18Case[177] presents a more comprehensive analysis based on the double-sided Laplace transform.154 ■ Non-Equilibrium Statistical Mechanics
Define
Ψ(z) ≡
 ∞
−∞
G(u, z)du. (5.42)
Integrate Eq. (5.41) over all u, in which case we find
Ψ(z) = 
1+ik
 ∞
−∞
du
η(u)
z + iku−1  ∞
−∞
du
g(u, t = 0)
z + iku ≡ 1
ε(k, z)
 ∞
−∞
du
g(u, t = 0)
z + iku , (5.43)
where ε(k, z) is the plasma dielectric function. The quantity Ψ is the Laplace transform of the
perturbed electron density, L[δn] = Ψ(z), and thus from Eq. (5.35), L[φ] = −(e/(ϵ0k2))Ψ(z), so
that
φ(t) = − e
ϵ0k2L−1[Ψ] = − e
ϵ0k2
1
2πi

C
ezt




 g(u, t = 0)
z + iku du
1+ik

du
η(u)
z + iku




dz, (5.44)
where C denotes the Bromwich contour. By substituting Ψ(z) back into Eq. (5.41), we have
G(u, z) = 1
z + iku [g(u, t = 0) − ikη(u)Ψ(z)] . (5.45)
In principle we’ve solved the problem: We have the Laplace transforms of g(u, t) and φ(t).
What remains is to find the inverse transforms. That turns out to be a fairly involved task, which for
our purposes we can forgo. At the end of the day, we’ll have an expression for φ in the form
φ(t) = − e
ϵ0k2

j
ezj t Res (Ψ)zj , (5.46)
where zj denotes locations of the poles of Ψ(z) with Res the residue. The potential has oscillations
determined by the imaginary parts of zj , with damping or growth controlled by the real parts.19
Referring to Eq. (5.44), poles of Ψ are either poles of the numerator [
 dug(u, t = 0)/(z + iku)]
or zeros of the denominator, ε(k, z). The numerator has no poles for physical choices of initial
perturbation g(u, t = 0). That leaves the task of finding the zeros of ε(k, z), which are independent
of initial conditions. Allowed dynamical modes are determined from solutions of the dispersion
relation,
ε(k, z)=1 − i
e2
mϵ0k
 ∞
−∞
(d/du)g0(u)
zj + iku du = 0. (5.47)
Equation (5.47) reduces to Eq. (5.33) under the substitution zj → −iω.
Our goal is not to find the most general solution, but rather its long-time form governed by the
smallest damping rate Γ (so that decaying fields vary as e−Γt
). That rate is given by the expression20
Γ = − πω3
p
2nk2 g′
0(u)




u=vϕ
, (5.48)
where ωp ≡ ne2/(mϵ0) is the plasma frequency. The criterion for damped oscillations is that
the slope of g0(u) evaluated at the wave speed is negative, g′
0(vϕ) < 0, implying there are more
slower particles than faster particles for speeds near the phase speed. Charged particles interact
strongly with longitudinal electrostatic waves when the speeds of particles moving in the direction
19The reader may note that the Laplace transform in Eq. (5.39) requires Re (z) > 0 and yet damped modes require
Re (zj ) < 0. Analytic continuation is used liberally in the derivation of Eq. (5.46). An exposition is given in Wu[178]. 20See for example Schmidt[179, p210]. This expression holds for wavelengths larger than the Debye length, krD ≪ 1.
One can’t have well-developed oscillations in plasmas with wavelengths smaller than the Debye length; see Exercise 5.13.Weakly coupled systems: Landau-Vlasov theories ■ 155
of wave propagation match the phase speed of the wave. Particles moving somewhat faster than the
wave lose energy to the field (and contribute to a growing field mode); particles moving slower gain
energy from the field and contribute to Landau damping.21 Which mechanism prevails depends on
the relative number of slower and faster particles for u ≈ vϕ.
We therefore have an example of irreversibility not associated with dissipation. For solutions of
the Vlasov equation, Boltzmann’s H-function remains unchanged, dH/dt = 0; see Exercise 5.11.
Moreover, stationary solutions of the Vlasov equation are not unique; see Exercise 5.12. Processes
such as Landau damping are an example of phase mixing[176, p269]. To avoid misunderstandings,
in this book the term irreversible refers solely to dissipative processes.
SUMMARY
Kinetic theory can be applied to plasmas as well as ordinary gases. Plasma kinetic theory is an
active field of research that we do not attempt to review (see Swanson[181]). We established its
basic principles—the Landau-Vlasov equations—based on material already developed in this book.
• Starting from the Boltzmann equation, Landau derived the kinetic equation for uniform,
weakly coupled gases in which the intermolecular potential is small compared to kT. Plasmas
(ionized gases) approximate weakly coupled gases. Charged particles influence each other at
large separations because of the long-range nature of Coulomb forces, yet such interactions
are weak. For dilute plasmas, close encounters are rare; weak, long-range “collisions” are
described by the Landau equation. The Landau equation can be written in the form of the
Fokker-Planck equation; both equations describe systems governed by weak collisions. For
all the approximations made in its derivation, the Landau equation can be derived from the
BBGKY hierarchy in the limit of a weak potential; see Bogoliubov[94, p97]. Solutions of the
Landau equation satisfy the Boltzmann H-theorem dH/dt ≤ 0, with the Maxwell-Boltzmann
distribution the stationary solution.
• The Landau equation encounters difficulties when applied to the bare Coulomb potential, not
so much because of its neglect of close collisions, but because of the “too-long” nature of the
Coulomb force. These issues can be overcome by parameterizing the potential with long and
short-distance cutoffs. Mobile charges of opposite kinds screen electric fields, implying the
intermolecular potential can be approximately Coulomb only up to the screening length, in
the simplest case the Debye length.
• To take into account screening—a characteristic feature of plasmas—Landau’s equation must
be extended to spatially nonuniform systems. The Vlasov-Landau “equation” [two coupled
equations, (5.22) and (5.23)] takes into account inhomogeneities through the development of
a self-consistent potential field, wherein charges respond to the same potential they generate.
A simpler version, valid within the weak-coupling approximation, known as the Vlasov or
the collisionless Boltzmann equation, ignores collisions and models the system as particles
streaming in the self-consistent potential, Eqs. (5.22) and (5.24).
• Landau damping is a damping mechanism of longitudinal space charge waves in collisionless
plasmas not involving dissipation. It occurs as a result of energy exchange between the wave
and charged particles moving with the speed of the wave. The Vlasov equation is time-reversal
invariant and cannot describe the approach to equilibrium; for its solutions dH/dt = 0, yet
it leads to an experimentally confirmed prediction of damping. Besides its intrinsic interest,
we’ve presented Landau damping as an example of irreversibility without dissipation, an ex￾ception to the theme of this book that entropy production is associated with inhomogeneities.
21Chen[180, Section 7.5] has a nice discussion of the physics of Landau damping.156 ■ Non-Equilibrium Statistical Mechanics
EXERCISES
5.1 Show that solutions of the Landau equation satisfy the H-theorem. Define the H-function,
not as in Eq. (4.109) with a spatial integration, but only with an integration over velocity,
H(t) ≡  dvf(v, t) ln[f(v, t)] (Landau’s equation applies to homogeneous systems).
a. Using Landau’s equation in the form of Eq. (5.2), show that
d
dt
H(t) = 
dv1 [ln f(v1, t) + 1] ∂f(v1, t)
∂t (P5.1)
=

dv1 [ln f(v1, t) + 1] ∇v1 ·

dv2G · ∇12f(v1, t)f(v2, t).
b. Integrate by parts on v1 to show that
d
dt
H(t) = −
  dv1dv2∇v1 ln f(v1, t) · G · ∇12f(v1, t)f(v2, t). (P5.2)
c. By interchanging the dummy variables v1, v2 in the integrand of Eq. (P5.2), show that
d
dt
H(t) = −1
2
  dv1dv2 [∇v1 ln f(v1, t) − ∇v2 ln f(v2, t)]·G·∇12f(v1, t)f(v2, t).
d. Define F(v1, v2) ≡ ∇v1 ln[f(v1, t)] − ∇v2 ln[f(v2, t)]. Show that
d
dt
H(t) = −1
2
  dv1dv2f(v1, t)f(v2, t)F(v1, v2) · G · F(v1, v2).
The H-theorem follows if we can show the integrand is never negative.
e. Show that
F · G · F =
 B
m2g3
 
g2F2 − (g · F)
2
.
Use the Schwartz inequality to conclude that F · G · F ≥ 0, proving dH/dt ≤ 0. The
H-theorem had better be satisfied by solutions of the Landau equation which is a special
case of the Boltzmann equation. Still, it’s a reassuring result; there are a limited number
of kinetic equations for which the H-theorem can be explicitly proven.
f. The time-invariant state dH/dt = 0 is achieved if and only if F · G · F = 0 implying
G · F = 0 (G is symmetric). As one can show from Eq. (5.12), G · g = 0. Stationary
solutions are therefore such that G · (F + βg)=0 where β is a constant.
i. Show, because G is arbitrary, stationary solutions are such that
∇v1 ln f(v1) − ∇v2 ln f(v2) = −β(v1 − v2).
Note that a constant vector u could be added and subtracted on the right side.
ii. Show that stationary solutions are Maxwellians f(v) = K exp 
−(β/2)(v − u)2
.
5.2 Derive Eq. (5.10).
5.3 Show that Eq. (5.13) is dimensionally correct if f has dimension (length)−3 × (velocity)−3.Weakly coupled systems: Landau-Vlasov theories ■ 157
5.4 What is the Fourier transform Φk [defined near Eq. (5.5)] of Φ(r) = r−1? A straightforward
attack on the problem might not prove satisfactory. One approach is to evaluate the integral
(do this),
 exp(ik · r)
k2 + a2 dk = 2π2
r
e−ar. (a > 0)
In the limit a → 0, Φk = (2π2k2)−1. (Another way to arrive at this result is to take the
Fourier transform of ∇2(1/r) = −4πδ(r).)
5.5 Show from Eq. (5.11) that B diverges for power-law potentials Φ(r) ∝ r−n for n > 1.
5.6 Show that the Landau length rmin = 1.67 × 10−5/T m, with T in Kelvin.
5.7 To solve Eq. (5.19) it’s best to keep r > 0 and determine the solution through boundary
conditions. At small distances we expect φ(r) to approach the form of the potential of a point
charge, φ(r) r→0 ∼ Q/(4πϵ0r). Let φ(r) = Q/(4πϵ0r)f(r). Develop a spherically symmetric
solution for f subject to the boundary conditions limr→0 f(r)=1 and limr→∞ f(r)=0.
5.8 Calculate the charge qind induced by the test charge Q at r = 0 in a plasma.
a. From Eq. (5.19) define (for r > 0) the induced charge density ρind(r) ≡ −(2ϵ0/r2
D)φ(r).
Show that
ρind(r) = − Q
2πr2
Dr
e−√2r/rD .
b. Calculate the induced charge in a ball of radius R, qind(R) ≡  R
0 4πr2ρind(r)dr. Show
that
qind(R) = −Q

1 − (1 + √
2R/rD)e−√2R/rD

.
Clearly qind → −Q for R ≫ rD and qind → 0 as R → 0.
5.9 Show that the Vlasov equation in the form of Eq. (5.25) follows from the first equa￾tion of the BBGKY hierarchy using the product approximation F2(r1, v1, r2, v2, t) =
F1(r1, v1, t)F1(r2, v2, t); see Eq. (4.29). The Vlasov equation is therefore formally the Li￾ouville equation of a single particle in an external field, yet that field is self-consistently
produced by the particles themselves. The Vlasov equation is nonlinear because of self￾consistency.
5.10 Derive Eq. (5.27).
5.11 a. Show, using the H-function for inhomogeneous systems, Eq. (4.109), that dH/dt = 0 for
solutions of the Vlasov equation (5.24). See Exercise 4.28.
b. Show directly that the Vlasov equation is time-reversal invariant. It does not have a definite
direction of time.
5.12 Consider stationary solutions of the Vlasov equation, solutions of
v · ∇f(r, v) − 1
m
∇U · ∇vf(r, v)=0.
a. Show this partial differential equation is solved by any function of the form
f(r, v) = W
1
2
mv2 + U(r)

.158 ■ Non-Equilibrium Statistical Mechanics
Stationary solutions are therefore not unique; any function W will do. This is reminiscent
of the wave equation, the solutions of which in one dimension are superpositions of any
functions f(x − vt) and g(x + vt). The parameter v is specified in the wave equation; the
only parameter entering the Vlasov equation is the mass (and not temperature).
b. Try separating variables. Let f(r, v) = ψ(r)g(mv2/2). Show you’re led to the interme￾diate result
v · [∇ ln ψ − (g′
/g)∇U]=0.
This simplifies if g′
/g = θ, a constant, implying g(x)=eθx and ψ(r)=eθU(r)
. One
could set θ = −1/(kT), but nothing forces that conclusion.
5.13 Suppose the time-invariant, spatially uniform velocity distribution f0(v) is the Maxwell￾Boltzmann distribution (see Eq. (4.121)), f0(v) = n(mβ/(2π))3/2 exp(−mβv2/2).
a. Find g0(u), the time-invariant, uniform distribution as a function of the longitudinal speed
u, with v2 = u2 + v2
⊥. A: g0(u) = n
mβ/(2π) exp(−mβu2/2).
b. Verify that  ∞
−∞ g0(u)du = n.
c. Show that the Landau damping rate in this case is given by
Γ = π
8
ω4
p
k3 (mβ)
3/2 exp[−mβω2
p/(2k2)].
Set u = ωp/k, where ωp is the plasma frequency.
d. Show that this expression can be rewritten in terms of the Debye length rD with
Γ = √π ωp
(2krD)3/2 exp 
− 1
2(krD)2

For the damping rate to be small compared to the oscillation frequency Γ/ωp ≪ 1 (and
thus to have well-developed oscillations), we require krD ≪ 1.CHAPTER 6
Dissipation, fluctuations,
and correlations
DISSIPATION1 is associated with fluctuations. We learned in Chapter 1 that entropy sources
involve fluxes and thermodynamic forces, quantities that are linked through fluctuations; see
Eqs. (1.42) and (1.43). Dissipation, associated with entropy creation, is therefore associated with
fluctuations. We see this in Nyquist’s theorem Eq. (2.45), S(ω)=4RkT, where the spectral density
of voltage fluctuations is related to the circuit resistance R (associated with dissipating electrical
energy). We find the same using the Langevin equation [see Eq. (3.7)], that R is related to temporal
correlations of voltage fluctuations. In the Einstein relation Eq. (3.20), D = kT /α, the diffusion
coefficient associated with Brownian motion (fluctuating position of the particle) is inversely related
to the friction parameter (associated with dissipating a colloidal particle’s kinetic energy by the drag
force). In this chapter, we study the connection between dissipation, fluctuations, and correlations.
6.1 FLUCTUATIONS AND THEIR CORRELATIONS
Although we’ve used correlation functions in this book, we need to distinguish different kinds of cor￾relations and thus we start with a review. Consider a dynamical function, Y (Γ) (see Section 4.1). Its
time dependence is implicit through Γ, Y (t) ≡ Y (Γ(t)), where Γ(t) is the image of Γ(t = 0) under
the canonical motion.2 To indicate time dependence explicitly, we write Y (Γ(t)) = eiΛt
Y (Γ(0))
with Λ the Liouville operator; see Exercise 4.2. In classical mechanics, time evolution is deter￾ministic: to every Y (t = 0) there is a unique Y (t ̸= 0).
3 We have no control over microscopic
initial conditions, however, and thus, in statistical mechanics, Γ is treated as a random variable
associated with a large collection of macroscopically identical systems (the ensemble). In thermal
equilibrium—a macroscopic steady state—the ensemble average of Y , ⟨Y ⟩, is independent of time,
⟨Y ⟩ =

dΓPeq(Γ)Y (Γ) = 
dΓ(0)Peq(Γ)eiΛt
Y (Γ(0))
=

dΓ(0) 
e−iΛt
Peq(Γ)

Y (Γ(0)) = 
dΓ(0)Peq(Γ)Y (Γ(0)), (6.1)
1Dissipated energy is energy that, as a result of entropy creation, is diverted into a form not available for work[2, p59].
Energy isn’t lost, it’s channeled into microscopic degrees of freedom, the form of energy known as heat[2, p10]. 2Every point of Γ-space evolves in time to a unique point under the action of Hamilton’s equations of motion, a one-to￾one mapping of Γ-space onto itself, the natural motion of phase space. See [5, p33]. 3Quantum time evolution is deterministic in that |ψ⟩t=0 uniquely implies |ψ⟩t̸=0. It’s the act of measurement that isn’t
deterministic and which adds another layer of uncertainty treated in quantum statistical mechanics; see Appendix A.
DOI: 10.1201/9781003512295-6 159160 ■ Non-Equilibrium Statistical Mechanics
where dΓ = dΓ(0) (unity Jacobian of canonical transformations[5, p332]), the adjoint of the time
evolution operator eiΛt is e−iΛt (see Exercise 6.1), and we’ve used stationarity ΛPeq(Γ)=0 (see [5,
p47]). A measurement of Y on a randomly selected member of the ensemble will in general return
a value different from ⟨Y ⟩. Let y(t) denote the fluctuation,
y(t) ≡ Y (t) − ⟨Y ⟩. (6.2)
Equation (6.2) underscores the stochastic nature of fluctuations: Y (t) doesn’t have a definite value—
it’s a random variable in the ensemble framework of statistical mechanics.
The only accessible information about stochastic variables is in their moments. The first moment
vanishes identically, ⟨y(t)⟩ = 0. For higher moments, start with the average of the square of y(t):
C0
yy ≡ ⟨y(t)y(t)⟩
0, (6.3)
where ⟨⟩0 denotes equilibrium average. Moments of this type are related to measurable quantities;
see [5, Chapter 4]. For example, the isothermal magnetic susceptibility χT = ⟨(∆M)2⟩0
T /(kT),
the constant volume heat capacity CV = ⟨(∆U)2⟩0
V /(kT2), and the isothermal compressibility
βT = (V /N2kT)⟨(∆N)2⟩0
T measure fluctuations in magnetization, internal energy, and particle
number.4 We can generalize Eq. (6.3) to include cross correlations,
C0
yz ≡ ⟨y(t)z(t)⟩
0. (6.4)
We found using fluctuation theory that ⟨∆U∆V ⟩0 = kβT V T2 (α/βT − P/T) in a system where
U and V can fluctuate [see Eq. (2.11); α is the thermal expansivity]. Time does not play a role in
correlations of this type. Equilibrium fluctuations are stationary and so are their correlations; see
Exercise 6.1. Another generalization is to the correlation of local fluctuations at different points x
of physical space, a correlation function,
5 which in equilibrium is a function only of the spatial
separation r,
C0
yz(r) ≡ ⟨y(x, t)z(x + r, t)⟩
0. (6.5)
Such correlations are independent of time (equilibrium) and position x (homogeneity); we can
therefore set (x, t)=0 in Eq. (6.5). An example is the correlation of spatially separated
spins on a lattice, ⟨σ0σr⟩0, where the location of the origin is immaterial.6 Correlations of
this type contribute to the elastic-scattering structure factor associated with local fluctuations
S(q) ≡  dreiq·r⟨y(0)y(r)⟩0, the scattering efficiency in the direction of the wavevector transfer
q ≡ kf −ki; see [5, Section 6.6]. Equation (6.5) reduces to Eq. (6.4) as r → 0; C0
yz(r = 0) = C0
yz.
We now consider the nonequilibrium counterpart,
Cyz(t) ≡ ⟨y(t)z(t)⟩, (6.6)
where ⟨⟩ denotes nonequilibrium average. This quantity is time dependent because the probability
distribution is not stationary. We can see that as follows (using the result of Exercise 6.1):
Cyz(t) = 
dΓy(Γ)z(Γ)P(Γ, t) = 
dΓy(Γ)z(Γ)e−iΛt
P(Γ, 0)
=

dΓeiΛt [y(Γ)z(Γ)] P(Γ, 0) = 
dΓy(Γ(t))z(Γ(t))P(Γ, 0). (6.7)
Thus there are two ways of expressing Cyz(t): one in which averages are taken with respect to
a time-dependent probability distribution P(Γ, t) or one in which averages are taken with respect
4The quantities χT and CV are obtained in the canonical ensemble, with βT in the grand canonical ensemble. 5The set of correlations C0
yz are elements of a correlation matrix of fluctuations in quantities (y, z). 6The origin can’t be “too close” to boundaries where surface effects introduce inhomogeneities, a problem avoided by
considering the lattice infinite in extent.Dissipation, fluctuations, and correlations ■ 161
to the probability of initial conditions, P(Γ, 0). These equivalent expressions mirror the passage
in quantum mechanics between the Schrodinger and Heisenberg representations (see ¨ Appendix E),
where, in the former, the state of the system7 is time dependent but observables are not, and in
the latter, the state of the system is fixed but observables carry time dependence. The dynamics
of fluctuations presented here differs from that introduced in Chapter 3. The stochastic nature of
Γ(0), inherent in the ensemble framework of statistical mechanics, induces the stochastic character
of Γ(t) through a deterministic dynamics; in the Fokker-Planck equation, dynamics is prescribed in
terms of transition probabilities. The two ways of expressing Cyz(t) in Eq. (6.7) show we can view
its time dependence as due either to the time evolution of P(Γ, t) or to the propagation of y(t)z(t)
in time for fixed P(Γ, 0).
Equation (6.6) can be generalized to local fluctuations,
Cyz(t; r, x) ≡ ⟨y(t; x)z(t; x + r)⟩. (6.8)
Equation (6.8) specifies the equal-time correlation function. It depends on x (and t and r); nonequi￾librium states are in general nonhomogeneous. Equation (6.8) suggests the next generalization, the
correlation between fluctuation y at point x at time t and fluctuation z at point x + r at time t + τ ,
Cyz(τ, t; r, x) ≡ ⟨y(t; x)z(t + τ ; x + r)⟩. (6.9)
Equation (6.9) defines the two-time correlation function, a function of two time variables and two
spatial variables. For τ = 0, Eq. (6.9) reduces to Eq. (6.8), Cyz(τ = 0, t; r, x) = Cyz(t; r, x). Two￾time correlations cannot be expressed in the Schrodinger representation. Consider, starting from the ¨
Heisenberg picture,
Cyz(τ, t; r, x) = 
dΓ
eiΛt
y(Γ; x)
 
eiΛ(t+τ)
z(Γ; x + r)

P(Γ, 0)
=

dΓeiΛt 
y(Γ; x)eiΛτ z(Γ; x + r)

P(Γ, 0)
=

dΓy(Γ; x)

eiΛτ z(Γ; x + r)

e−iΛt
P(Γ, 0)
=

dΓy(Γ; x)

eiΛτ z(Γ; x + r)

P(Γ, t). (6.10)
Two-time correlations can either be expressed in Heisenberg form or in a generalized Schrodinger ¨
representation involving the average of a τ -dependent dynamical quantity with respect to a t￾dependent distribution. The Heisenberg representation is used preferentially.
If the two-time correlation function were evaluated with respect to an equilibrium distribution,
the resulting correlation function would depend on τ and r but not on t and x (from stationarity and
homogeneity of the equilibrium state). Using Eq. (6.10),
C0
yz(τ ; r) ≡ ⟨y(t; x)z(t + τ ; x + r)⟩
0 =

dΓy(Γ; 0) 
eiΛτ z(Γ; r)

Peq(Γ)
= ⟨y(0; 0)z(τ ; r)⟩
0. (6.11)
Thus we can set (t, x)=0 in Eq. (6.9) in the case of an equilibrium distribution. Equation (6.11)
generalizes C0
yz(r) in Eq. (6.5) to the correlation of local fluctuation y(0; 0) with another z(τ ; r)
displaced in space and time. Correlation functions of this type are called time-dependent correlation
functions or simply time correlation functions and play a significant role in our subsequent analysis.
For z = y we have the autocorrelation function; see Eq. (2.24).
7The state of a statistical mechanical system is specified by a probability distribution; see Section 4.1.162 ■ Non-Equilibrium Statistical Mechanics
The spectral content of correlation functions is related to experimentally accessible infor￾mation, that of scattering structure factors. Scattering, of electromagnetic radiation or neutrons,
occurs from inhomogeneities in system properties, notably density fluctuations (whether mass,
charge, current, magnetization, etc.). In inelastic scattering, the dynamic structure factor de￾scribes the scattering intensity associated with the wavevector q and frequency ω of scattered
radiation.8 Van Hove[183] showed that the dynamic structure factor is proportional to the spec￾tral density of time correlation functions, C0
yz(ω; q) =  ∞
−∞ dτ
 dreiωτ+iq·rC0
yz(τ ; r) =
 ∞
−∞ dτ
 dreiωτ+iq·r⟨y(0; 0)z(τ ; r)⟩0. Thus we learn about the dynamical properties of corre￾lations through the measured dynamic structure factor. Inelastic scattering has been extensively
studied on systems near their critical points. We note that, just as scaling theories enable the study
of equilibrium critical phenomena (see [5, Chapter 8]), so too is the study of dynamic critical phe￾nomena facilitated by scaling hypotheses, a topic we lack the space to cover.9
So far we’ve made minimal use of quantum mechanics. Statistical mechanics is not just the
application of classical or quantum mechanics to large systems. Rather it seeks to connect macro￾scopic observations with the microscopics of systems governed by Hamiltonian dynamics, whether
in classical or quantum formulation; the nature of the micro-dynamics is often immaterial. There
are (obviously) systems, however, where quantum treatments are necessary. Fortunately, correlation
functions are easily extended to meet the requirements of quantum mechanics. Ensemble averages
are found from the density operator ρˆ, ⟨A⟩ = Tr ˆρAˆ, where Aˆ is the operator associated with A; see
Eq. (A.39). A distinctly quantum issue is the product of noncommuting operators and here we fol￾low the Weyl correspondence principle (see Section A.4) in defining two-point correlation functions
as averages of symmetrized products,
Cyz(τ, t; r, x) = 1
2
Tr ˆρ(t)[ˆy(t; x)ˆz(t + τ ; x + r)+ˆz(t + τ ; x + r)ˆy(t; x)]
= 1
2
⟨yˆ(t; x)ˆz(t + τ ; x + r)+ˆz(t + τ ; x + r)ˆy(t; x)⟩. (6.12)
6.2 LINEAR RESPONSE THEORY
Consider, for a system that in the far past (t → −∞) was in thermal equilibrium and gov￾erned by Hamiltonian10 H0, the problem of turning on a time-dependent external interaction F(t)
(limt→−∞ F(t)=0) that couples to system quantity A with energy H′
(t) ≡ −AF(t) so that the
Hamiltonian H = H0 + H′
(t). In classical mechanics, A is a Γ-space function; in quantum me￾chanics A is represented by a Hermitian operator. Table 6.1 lists examples. We seek the response of
Table 6.1 Examples of time-dependent interactions
External interaction F(t) System quantity A
Magnetic field B Magnetization M
Electric field E Electric polarization P
Sound waves Mass density
the system to first order in the interaction, the linear response, the accuracy of which requires the
interaction to be small.11 The response is measured through the change ∆B(t) that occurs in ob￾servable quantity B due to the interaction F(t). We develop a general formalism—linear response
theory—applicable to any interaction in the form H′
(t) ≡ −AF(t), classical or quantum.
8We omit the derivation of the dynamic structure factor. See Stanley[182, Chapter 13], for example. 9See Hohenberg and Halperin[184]. 10The Hamiltonian H0 includes internal interactions between particles, such as in Eq. (4.18). 11Thus we’re developing a time-dependent perturbation theory in nonequilibrium statistical mechanics.Dissipation, fluctuations, and correlations ■ 163
6.2.1 Response functions: General derivations, classical and quantum
6.2.1.1 Classical
The Γ-space distribution function of classical statistical mechanics ρ(p, q) satisfies Liouville’s equa￾tion, (4.5), (∂/∂t)ρ = −iΛρ ≡ Lρ = [H, ρ]P ([∗, ∗]P denotes Poisson bracket). Before turning on
the interaction, the system was in equilibrium with ρ = ρ0 such that [H0, ρ0]P = 0. Write the
perturbed distribution ρ(t) = ρ0 + ∆ρ(t) with ∆ρ(−∞)=0. Then, from Liouville’s equation,
∂
∂t

ρ0 + ∆ρ

= 
H0 + H′
, ρ0 + ∆ρ

P = 
H0, ρ0
P + 
H0, ∆ρ

P + 
H′
, ρ0
P + [H′
, ∆ρ]
P ,
we have, ignoring [H′
, ∆ρ]P, the equation of motion at lowest order,12
∂
∂t∆ρ(t) = 
H′
(t), ρ0
P + 
H0, ∆ρ(t)

P ≡ 
H′
(t), ρ0
P + L0∆ρ(t), (6.13)
where L0 is the Liouville operator associated with H0. The solution of Eq. (6.13) (an inhomoge￾neous Liouville equation, see Exercise 6.3) is, with ∆ρ(−∞)=0,
∆ρ(t) =  t
−∞
dτ eL0(t−τ) 
H′
(τ ), ρ0
P . (6.14)
The modification ∆ρ(t) at time t can be interpreted as a linear superposition of “sources”

H′
(τ ), ρ0
P (disturbances to the quiescent system) propagated in time by eL0(t−τ)
, −∞ <τ<t.
The nonequilibrium ensemble average of a dynamical variable is found from
⟨B⟩t =

dΓB(Γ)ρ(t) = 
dΓB 
Γ)(ρ0 + ∆ρ(t)

≡ ⟨B⟩
0 +

dΓB(Γ)∆ρ(t)
   ∆B(t)
. (6.15)
The change ∆B(t) ≡ ⟨B⟩t − ⟨B⟩0 at lowest order, the linear response, is, using Eq. (6.14),
∆B(t) = 
dΓB(Γ, −∞)
 t
−∞
dτ eL0(t−τ) 
H′
(τ ), ρ0
P
=
 t
−∞
dτ

dΓ

e−L0(t−τ)
B(Γ, −∞)

H′
(τ ), ρ0
P
=
 t
−∞
dτ

dΓB(Γ, t − τ )

H′
(τ ), ρ0
P , (6.16)
where e−Lt is the adjoint of eLt. Now substitute H′
(t) = −A(p, q)F(t) in Eq. (6.16),
∆B(t) = −
 t
−∞
dτF(τ )

dΓB(Γ, t − τ )

A, ρ0
P . (6.17)
Define the response function,
ΦBA(t) ≡

dΓ
ρ0, A
P B(Γ, t) = 
dΓρ0 [A, B(t)]P , (6.18)
so that
∆B(t) =  t
−∞
dτΦBA(t − τ )F(τ ) =  ∞
0
ΦBA(θ)F(t − θ)dθ, (6.19)
12Kubo[185] shows, formally, how to include higher-order approximations, ignored in the linear theory.164 ■ Non-Equilibrium Statistical Mechanics
where the second equality in Eq. (6.18) is established in Exercise 6.4. The response function ΦBA
is also known as the after-effect function. By the principle of causality, the response (the effect)
cannot precede the interaction that elicits it (the cause), implying ΦBA(t − τ )=0 for τ>t or
ΦBA(θ < 0) = 0. Any function f(t) having the property f(t < 0) = 0 is known as a causal
function (see Appendix F).
6.2.1.2 Quantum
In the quantum version of this problem, the distribution function ρ(p, q) is replaced with the density
operator ρˆ, and H0 and H′
(t) are replaced with operators Hˆ 0 and Hˆ ′
(t) = −AFˆ (t). The state
at t = −∞ (thermal equilibrium) is characterized by density operator ρˆ0 such that [Hˆ 0, ρˆ0]=0
(unadorned square brackets [∗, ∗] denote commutator). With ρˆ(t)=ˆρ0 + ∆ˆρ(t) in Eq. (A.41), we
have at lowest order13
iℏ ∂
∂t∆ˆρ(t) = 
Hˆ ′
(t), ρˆ0
+ 
Hˆ 0, ∆ˆρ(t)

. (6.20)
The solution to Eq. (6.20) is (as can be verified),
∆ˆρ(t) = 1
iℏ
 t
−∞
dτU0(t, τ )

Hˆ ′
(τ ), ρˆ0
U†
0 (t, τ ) (6.21)
where [see Eq. (E.6)]
iℏ ∂
∂tU0(t, t′
) ≡ Hˆ 0U0(t, t′
). (6.22)
For time-independent Hˆ 0 (as is the case here14), U0(t, t′
) = exp[−iHˆ 0(t − t
′
)/ℏ].
The response ∆B(t) is, using Eq. (6.21),
∆B(t) = Tr ∆ˆρ(t)Bˆ = 1
iℏ
 t
−∞
dτ Tr
U0(t, τ )

Hˆ ′
(τ ), ρˆ0
U†
0 (t, τ )Bˆ

= 1
iℏ
 t
−∞
dτF(τ ) Tr 
U0(t, τ )

ρˆ0, Aˆ

U†
0 (t, τ )Bˆ

= 1
iℏ
 t
−∞
dτF(τ ) Tr 
ρˆ0
A, U ˆ †
0 (t, τ )BUˆ 0(t, τ )

= 1
iℏ
 t
−∞
dτF(τ )⟨

A, ˆ BˆI (t − τ )

⟩
0
= 1
iℏ
 t
−∞
dτF(τ )⟨

AˆI (τ ), BˆI (t)

⟩
0 ≡
 t
−∞
dτΦBA(t − τ )F(τ )
=
 ∞
0
ΦBA(θ)F(t − θ)dθ, (6.23)
where we’ve used Hˆ ′
(τ ) = −AFˆ (τ ), the cyclic invariance of the trace, and the interaction repre￾sentation15 (see Exercise 6.6). The response function is given by the expressions:
iℏΦBA(t − τ ) = 
⟨

A, ˆ BˆI (t − τ )

⟩0 = ⟨

AˆI (τ ), BˆI (t)

⟩0 −∞ <τ<t
0. τ ≥ t (6.24)
The response function is causal, ΦBA(θ ≤ 0) = 0. Equation (6.24) is known as Kubo’s formula
(for the response function16). Despite the appearance of i in Eq. (6.24) (unit imaginary number),
13Note the similarity between Eqs. (6.20) and (6.13). These equations exemplify the general correspondence between
commutators and Poisson brackets, that [A, ˆ Bˆ]/(iℏ) ℏ→0 = [A, B]P; see Dirac[165, Section 21]. 14For time-dependent H0, Eq. (6.22) still holds but the solution is more complicated; see Fetter and Walecka[186, p57]. 15The interaction picture is for systems, as in linear response theory, with Hamiltonians of the form H = H0 +H′ where
H′ can be time dependent; see Appendix E. The Heisenberg representation applies to systems where H in the Schrodinger ¨
picture is time independent. The time-dependent operators in Eq. (6.23) can be considered in Heisenberg form. These are
just names, however. We’ll generally drop the subscript I or H and simply signify that we have time-dependent operators. 16The are many Kubo formulae.Dissipation, fluctuations, and correlations ■ 165
Φ is real—a real response ∆B to a real interaction H′ implies Φ is real; see Exercise 6.7. More￾over, Φ is independent of F, a requirement of linearity, and it depends on equilibrium quantities;
the nonequilibrium response (for small interactions) is expressed in terms of time correlation func￾tions.17 Whereas in statistical mechanics the partition function Z is the fundamental quantity, in
nonequilibrium statistical mechanics time correlation functions are fundamental. And, unlike the
partition function, time correlation functions can be measured.
We can express the response function in a more explicit form for systems in the canonical
ensemble with density operator ρˆ0 = Z−1(β) exp(−βHˆ 0), where β ≡ (kT)−1 and Z(β) =
Tr exp(−βHˆ 0). To do so, we develop an identity due to Kubo[185] (see Exercise 6.8),

A, ˆ e−βHˆ 
= e−βHˆ
 β
0
eαHˆ 
H, ˆ Aˆ

e−αHˆ
dα = e−βHˆ
 β
0

H, ˆ eαHˆ
Aˆe−αHˆ 
dα, (6.25)
where α is a dummy integration variable. In the second equality, we have a construct resem￾bling the Heisenberg representation, AˆH(t) ≡ eiHt/ ˆ ℏAˆe−iHt/ ˆ ℏ; see Eq. (E.9). It would in fact
be the Heisenberg operator if we were to adopt an imaginary time, t ≡ −iℏα, a step we take:18
AˆH(−iℏα)=eαHˆ Aˆe−αHˆ . In the Heisenberg picture, the equation of motion is generated by the
commutator: 
H, ˆ AˆH(−iℏα)

= −iℏA
ˆ˙
H(−iℏα); see Eq. (E.12). We can then complete the iden￾tity:19
[e−βHˆ
, Aˆ]=iℏe−βHˆ
 β
0
dαA
ˆ˙
H(−iℏα). (6.26)
Combine Eq. (6.26) with Eq. (6.24) and drop subscripts on time-dependent operators,
ΦBA(t) = 1
iℏ Tr ˆρ0[A, ˆ Bˆ(t)] = 1
iℏ Tr[ˆρ0, Aˆ]Bˆ(t)
=
 β
0
dα Tr ˆρ0A
ˆ˙(−iℏα)Bˆ(t) = −
 β
0
dα Tr ˆρ0Aˆ(−iℏα)B
ˆ˙(t), (6.27)
where we’ve used the cyclic invariance of the trace and the Heisenberg equation of motion; see
Exercise 6.10.
6.2.2 The generalized susceptibility χ(ω) and its analytic properties
Consider a monochromatic interaction,20
F(t) = E0e−iωt = limϵ→0+ E0e−iωt+ϵt, (6.28)
where adding a small imaginary part iϵ (ϵ > 0) to the frequency ω ensures convergence of integrals
(we require limt→−∞ F(t)=0, “adiabatic switching”). By combining Eq. (6.28) with Eq. (6.23)
we have the response
∆B(t) = E0 lim
ϵ→0+
 t
−∞
dτΦBA(t − τ )e−i(ω+iϵ)τ = E0 lim
ϵ→0+
 ∞
0
dθΦBA(θ)e−i(ω+iϵ)(t−θ)
.
Thus,
∆B(t) = χBA(ω)E0e−iωt, (6.29)
17Analogous to quantum mechanics, where first-order shifts in eigenvalues depend on zeroth-order wave functions.
18There is a well-developed formalism in nonequilibrium statistical mechanics involving imaginary time. See Kadanoff
and Baym[187], Abrikosov, Gorkov, and Dzyaloshinski[188], and Langreth[189]. Rammer[190] is a modern reference. 19See Eq. (3.6) of Kubo[185]. 20The response to arbitrary (non-monochromatic) interactions can be synthesized through Fourier transformation. At
some point, we have to take the real part of Eq. (6.28).166 ■ Non-Equilibrium Statistical Mechanics
with
χBA(ω) ≡ lim
ϵ→0+
 ∞
0
dθei(ω+iϵ)θΦBA(θ) (6.30)
the generalized susceptibility, the Fourier transform of the response function, and the proportionality
factor between response and interaction.21 Note that the response occurs with the same frequency as
the interaction, a characteristic of linear response; frequency mixing occurs in the nonlinear regime
(as in nonlinear optics; see Boyd[191]).
We note some general properties of χBA(ω). Besides causality, Φ(θ ≤ 0) = 0 (we drop BA),
we require that a constant finite interaction (F(t − θ) = F0 for 0 ≤ θ < ∞) give rise to a constant
finite response,22 which from Eq. (6.23) implies
 ∞
0
Φ(θ)dθ < ∞. (6.31)
As a consequence, χ(ω = 0) exists [see Eq. (6.30)], implying χ(ω) does not have a pole at ω = 0.
It’s assumed that χ(ω) has no poles on the real-ω line.23 To discuss the analytic properties of χ(ω),
let’s momentarily generalize the definition in Eq. (6.30),
χ(ω) =  ∞
0
dθΦ(θ)e(iω−ν)θ, (6.32)
where ν > 0 is not meant to be infinitesimal. The integral exists for all ω and vanishes for ν → ∞.
The generalized susceptibility has no poles in the closed upper half of the complex ω-plane.
24 Such
a function is said to be holomorphic.
25 Poles of χ lie in the lower half of the complex ω-plane.26
A bounded function in the upper half complex ω-plane has, as a boundary value on the
real-ω line, a complex-valued function χ(ω) : R → C with real and imaginary parts satisfying
the Kramers-Kronig relations (Titchmarsh’s theorem; see Appendix F). In a standard notation,
χ(ω) = χ′
(ω)+iχ′′(ω) ≡ Re [χ(ω)] + i Im [χ(ω)], with
χ′
(ω) = 1
πP
 ∞
−∞
χ′′(ω′
)
ω′ − ω
dω′ ≡ H(χ′′)
χ′′(ω) = − 1
πP
 ∞
−∞
χ′
(ω′
)
ω′ − ω
dω′ ≡ H−1(χ′
), (6.33)
where P denotes the Cauchy principal value integral and H the Hilbert transform (see Appendix F).
The real and imaginary parts of χ(ω) are not independent, they’re Hilbert-transform pairs (H−1 =
−H); χ(ω) can be reconstructed from either part.
6.2.3 Identification of χ′′(ω) with dissipation
That the Fourier transform of the response function has real and imaginary parts χ′ and χ′′ satisfying
the Kramers-Kronig relations is a direct consequence of its causal nature.27 We now show that χ′′
is associated with energy dissipation, a result of considerable importance.
21In statistical mechanics, we have the magnetic susceptibility M = χH and the electric susceptibility P = ϵ0χeE,
quantities having no time dependence. Here we’re allowing the susceptibility to have a frequency dependence, χ(ω). The
susceptibility is also known as the admittance (see Kubo[185]) or the compliance, the “displacement/force.” 22This is equivalent to the requirement of no energy dissipation at frequency ω = 0; see Eq. (6.39). 23If χ(ω) were to have poles on the real-ω line, they would represent non-dissipative, reversible contributions to the
response of the system. We ignore this possibility in a theory of irreversible phenomena. 24The closed upper half plane is the upper half plane together with its closure, the real line.
25See [13, p198]. 26By Liouville’s theorem [from complex analysis, not Eq. (4.2)] a bounded entire function is a constant; χ(ω) must have
singularities for complex ω. Complex analysis is presumed familiar to the reader; see [13, Chapter 8]. 27Analyticity and causality are closely linked; one implies the other, see Appendix F.Dissipation, fluctuations, and correlations ■ 167
First, another general property, that because Φ is real, χ′
(ω) is even and χ′′(ω) is odd (see
Exercise 6.11): χ′
(−ω) = χ′
(ω) and χ′′(−ω) = −χ′′(ω). Moreover, χ∗(ω) = χ(−ω). Thus,
χ′
(ω) = 1
2 [χ(ω) + χ(−ω)]
χ′′(ω) = 1
2i [χ(ω) − χ(−ω)] . (6.34)
The real part χ′ is time-reversal symmetric (ω ↔ −ω), but the imaginary part χ′′ is not. Irreversibil￾ity is associated with χ′′; it knows about the arrow of time. The real part is the reactive part of the
response function with the imaginary part the dissipative or the absorptive part.
How does the system’s energy change in time? With W ≡ ⟨Hˆ ⟩ = Tr ˆρHˆ the average energy,
dW
dt = d
dt
Tr ˆρHˆ =✘✘✘✘✿0 Tr( ˙ρH) + Tr(ρH˙ ) = Tr(ρH˙ ′
) = −F˙ Tr(ρA) = −F˙ 
⟨A⟩
0 + ∆A(t)

,
(6.35)
where we’ve dropped the “hat” on operators and Tr ˙ρH = 0 (cyclic invariance of trace). For a
monochromatic interaction of frequency ω, which we write as
F(t) = E0 cos(ωt) = 1
2

E0e−iωt + E∗
0 eiωt
, (6.36)
we time average over a cycle of period T = 2π/ω. Under W˙ → (1/T)
 T
0 (dW/dt)dt in Eq. (6.35),
W˙ = − 1
T
 T
0
dtF˙ 
⟨A⟩
0 + ∆A(t)

= − 1
T
 T
0
dtF˙ ∆A(t) = − 1
T
 ∞
0
dθΦ(θ)
 T
0
dtF F˙ (t − θ)
= iω
4T
 ∞
0
dθΦ(θ)
 T
0
dt

E0e−iωt − E∗
0 eiωt E0e−iω(t−θ) + E∗
0 eiω(t−θ)

,
where we’ve used Eq. (6.36) to find F˙ . Performing the integrations on t, we find the average rate at
which energy changes (through the action of a driving force of frequency ω),
W˙ (ω) = iω
4 |E0|
2
 ∞
0
dθΦ(θ)

e−iωθ − eiωθ
= iω
4 |E0|
2 (χ(ω) − χ(−ω))
= −ω
2 |E0|
2χ′′(ω), (6.37)
where we’ve used Eq. (6.34). Dissipation is indicated if W˙ is negative.
From thermodynamics (see [2, p25]), the work done by a system operating in a cycle, −Wcycle,
is equal to the net heat absorbed in a cycle, −Wcycle = Qcycle (in our sign convention, positive values
of the symbols Q, W represent energy transfers to the system in the form of heat and work; see [2,
p9]).28,29 Thus, from Eq. (6.37), the average rate of heat absorption is proportional to χ′′,
Q˙(ω) = 1
2
|E0|
2ωχ′′(ω). (6.38)
Unless the system is an amplifier, we require dissipation at each frequency,
ωχ′′(ω) ≥ 0. (6.39)
Equation (6.39) is the positivity condition. By this condition, χ′′ is positive for positive frequency
and negative for negative frequency, consistent with the requirement that it be odd from the reality
of the response function. There is no dissipation at ω = 0, consistent with Eq. (6.31) that a constant
external force gives rise to a constant finite response.
28Internal energy is a state variable and thus in a cycle ∆Ucycle =0= Qcycle + Wcycle. Internal energy is a storehouse
of the adiabatic work done on systems; see [2, p8]. Adiabatically isolated systems interact with their surroundings solely
through mechanical means, implying that the adiabatic work stored in a system is the average value of the Hamiltonian ⟨H⟩.
Thus, W ≡ ⟨H⟩ introduced near Eq. (6.35) can be identified with the work W appearing in thermodynamics. See [5, p92]. 29Nothing limits the conversion of work into heat. The second law places limitations on converting heat into work.168 ■ Non-Equilibrium Statistical Mechanics
6.2.4 Explicit formulae for the quantum response
We’ve been able to infer important properties of the response function and its Fourier transform
without exhibiting an explicit expression. We do that now starting from Eq. (6.24) in the form
iℏΦBA(t−τ ) = Tr ρ0[A, ˆ Bˆ(t−τ )] where we work in the canonical ensemble using a basis in which
the internal Hamiltonian H0 is diagonal. This form of Eq. (6.24) is convenient because the time
dependence is in one place. We work with density operator ρ0 = Z−1(β)e−βH0
[see Eq. (A.45)]
and we assume a complete orthonormal basis {|n⟩} such that 
n |n⟩⟨n| = I, ⟨n|m⟩ = δn,m, with
H0|n⟩ = En|n⟩. Then, for τ<t,
iℏΦBA(t − τ ) = 
n
⟨n|ρ0[A, B(t − τ )]|n⟩ = 
n

m
⟨n|ρ0|m⟩⟨m|[A, B(t − τ )∥n⟩
= 1
Z(β)

n
e−βEn ⟨n|[A, B(t − τ )]|n⟩. (6.40)
Let’s work on ⟨n|[A, B(t − τ )|n⟩:
⟨n|[A, B(t − τ )]|n⟩ = ⟨n|AU†
0 (t − τ )BU0(t − τ ) − U†
0 (t − τ )BU0(t − τ )A|n⟩.
Make judicious use of the completeness relation,
⟨n|[A, B(t − τ ]|n⟩ = 
m
⟨n|AU†
0 (t − τ )|m⟩⟨m|BU0(t − τ )|n⟩
−
m
⟨n|U†
0 (t − τ )BU0(t − τ )|m⟩⟨m|A|n⟩
= 
m

ei(Em−En)(t−τ)/ℏAnmBmn − ei(En−Em)(t−τ)/ℏBnmAmn
, (6.41)
where the matrix elements Anm ≡ ⟨n|A|m⟩, etc. Defining ℏωnm ≡ En − Em and combining Eq.
(6.41) with Eq. (6.40), we have, playing with indices,
ΦBA(t − τ ) = 1
iℏZ(β)

n

m
e−βEn 
1 − eβℏωnm
e−iωnm(t−τ)
AnmBmn. (6.42)
It can be shown (as a check, see Exercise 6.13) that ΦBA in Eq. (6.42) is real. The generalized
susceptibility then follows using Eq. (6.30). We note first that
lim
ϵ→0+
 ∞
0
ei[ω−ωnm+iϵ]θdθ = limϵ→0+
i
ω − ωnm + iϵ = iP
 1
ω − ωnm 
+ πδ(ω − ωnm), (6.43)
where we’ve used the Plemelj relation, Eq. (F.20). By combining Eq. (6.42) with Eq. (6.30) and
making use of Eq. (6.43), we have the real and imaginary parts of χ(ω),
χ′
BA(ω) = 1
ℏZ(β)

n

m
e−βEn 
1 − eβℏωnm
AnmBmnP
 1
ω − ωnm 
χ′′
BA(ω) = π
ℏZ(β)

n

m
e−βEn 
eβℏω − 1

AnmBmnδ(ω − ωnm). (6.44)
The absorptive part χ′′ consists of a sequence of δ-functions at the frequencies ωnm associated with
differences between allowed energies—a basic idea of quantum mechanics. The reactive part χ′ has
a sequence of zeroes at ωnm.Dissipation, fluctuations, and correlations ■ 169
6.2.5 Brownian motion of a harmonically bound classical particle
We illustrate these concepts with an example from classical physics, that of a harmonic oscillator
in thermal contact with its environment—the Brownian motion of a harmonically bound particle.30
Assume a particle of mass m attached to a Hookean spring of force constant k interacting with a
heat bath and subject to an external force F(t). We consider a single degree of freedom (i.e., we
treat this as a one-dimensional problem). From Newton’s law,
mx¨(t) = −mω2
0x(t) + Fint(t) + F(t), (6.45)
where ω2
0 ≡ k/m and Fint(t) denotes the instantaneous internal force of interaction of the oscillator
with its surroundings.31 Our knowledge of the internal force is limited to its statistical properties
(see our treatment of the Langevin equation, Chapter 3). Thus we take the average of Eq. (6.45),
m⟨x¨⟩t + mω2
0⟨x⟩t − ⟨Fint⟩t = F(t) (6.46)
where ⟨⟩t denotes a nonequilibrium ensemble average of systems subject to the force F(t). We know
phenomenologically that friction acts in a direction opposing motion with a strength proportional to
the speed; we take
⟨Fint⟩t = −mγ⟨x˙⟩t, (6.47)
where γ > 0 is the friction coefficient. We therefore have the equation of motion
m
d2
dt2 ⟨x⟩t + mγ
d
dt
⟨x⟩t + mω2
0⟨x⟩t = F(t), (6.48)
where we’ve equated averages of derivatives with derivatives of averages; see Eq. (2.21).
We seek the response ⟨x⟩t to the external force F(t) as specified by the relation
⟨x⟩t =
 ∞
−∞
Φ(t − t
′
)F(t
′
)dt
′
. (6.49)
Combine Eq. (6.49) with Eq. (6.48):
 ∞
−∞ 
m
d2
dt2 + mγ
d
dt + mω2
0

Φ(t − t
′
)F(t
′
)dt
′ = F(t). (6.50)
Equation (6.50) implies

m
d2
dt2 + mγ
d
dt + mω2
0

Φ(t − t
′
) = δ(t − t
′
). (6.51)
The response function is thus the Green function associated with the inhomogeneous differential
equation describing the nonequilibrium behavior.32 We’re not using statistical mechanics to find the
response function in this case; this is not a microscopic approach.
There’s no one way to solve differential equations. Let’s find the Fourier transform of Φ. From
Eq. (6.30),33
χ(ω) =  ∞
−∞
dθeiωθΦ(θ), (6.52)
30The harmonic oscillator is among a handful of exactly solvable problems in physics; in essence this is the classic
classical physics example. 31See [2, p7] for a discussion of the distinction between system and surroundings. 32The method of Green functions is presumed known to the reader; see [13, Chapter 9]. 33We don’t have to put in the “causality cutoff”; it will emerge from the analysis.170 ■ Non-Equilibrium Statistical Mechanics
where θ ≡ t − t
′
, with the inverse relation
Φ(θ) = 1
2π
 ∞
−∞
dωe−iωθχ(ω). (6.53)
Combine Eq. (6.53) with Eq. (6.51) and use the integral representation of the delta function, δ(x) =
 ∞
−∞ e−ikxdk/(2π). We find
χ(ω) = 1
m
1
ω2
0 − ω2 − iγω. (6.54)
Much depends on the location of the poles of χ(ω) (for ω complex). It’s straightforward to show that
χ(ω) has simple poles at ω± ≡ −iγ/2 ± ω2
0 − (γ/2)2. In all cases these occur in the lower half
plane. By Titchmarsh’s theorem, therefore (see Section F.2), Φ is a causal function, Φ(θ < 0) = 0.
Combining Eqs. (6.54) and (6.53),
Φ(θ) = − 1
2πm  ∞
−∞
dω
e−iωθ
(ω − ω+)(ω − ω−)
. (6.55)
It’s an exercise in the residue theorem (see Exercise 6.15) to show from Eq. (6.55) that34
Φ(θ) =



exp(−γθ/2)
sin ω2
0 − (γ/2)2θ

mω2
0 − (γ/2)2
θ ≥ 0
0. θ < 0
(6.56)
For ω0 > γ/2, Φ is a damped sine wave; it decays without oscillation for ω0 < γ/2. For the real
and imaginary parts of χ(ω) (for ω real), we have
χ′
(ω) = 1
m
ω2
0 − ω2
(ω2
0 − ω2)
2 + (γω)2
χ′′(ω) = 1
m
γω
(ω2
0 − ω2)
2 + (γω)2
. (6.57)
Absorption is maximized at ω = ω0,resonance absorption, and the reactive part vanishes at ω = ω0.
We also have the positivity condition, ωχ′′(ω) ≥ 0.
6.2.6 The relaxation function
Consider a system subjected to a constant force F = F0 for −∞ <t< 0 which is then switched
off at t = 0. At that point the response ∆B(t) will begin to relax to zero, a process described by
∆B(t > 0) = F0
 0
−∞
dτΦBA(t − τ ) = F0
 ∞
t
ΦBA(θ)dθ, (6.58)
where θ ≡ t − τ . The function
RBA(t) ≡
 ∞
t
ΦBA(θ)dθ ≡ lim
ϵ→0+
 ∞
t
ΦBA(θ)e−ϵθdθ (6.59)
is the relaxation function, which can be defined with a convergence factor if need be. We stipulated,
however, in Eq. (6.31) that Φ(θ) is integrable, implying that
limt→∞ Φ(t)=0. (6.60)
34Equation (6.56) is the particular solution of Eq. (6.51). What about the complementary solution; what about initial
conditions? The general solution to Eq. (6.51) is given in Uhlenbeck and Ornstein[64, p834], reprinted in Wax[44, p93].
Equation (6.56) is the solution for initial conditions x(0) = ˙x(0) = 0.Dissipation, fluctuations, and correlations ■ 171
6.3 FLUCTUATION-DISSIPATION THEOREM
We now establish a fundamental result—the fluctuation-dissipation theorem—a relation between
dissipation (of the energy of external perturbations) and spontaneously occurring fluctuations in
equilibrium. Fortunately we’ve done most of the work in making this connection. Consider the time
correlation function of observables A and B [see Eq. (6.12)]
C0
AB(t) = 1
2
Tr ˆρ0
AˆBˆ(t) + Bˆ(t)Aˆ

. (6.61)
Repeating the analysis of Eqs. (6.40) and (6.41) step by step, we have
C0
AB(t) = 1
2Z(β)

n
e−βEn ⟨n|AˆBˆ(t) + Bˆ(t)Aˆ|n⟩
= 1
2Z(β)

n

m
e−βEn

⟨n|AUˆ †
0 (t)|m⟩⟨m|BUˆ 0(t)|n⟩ + ⟨n|U†
0 (t)BUˆ 0(t)|m⟩⟨m|Aˆ|n⟩

= 1
2Z(β)

n

m
e−βEn 
eiωmnt
AnmBmn + eiωnmt
BnmAmn
.
This can be condensed by interchanging n ↔ m in the second term,
C0
AB(t) = 1
2Z(β)

n

m
e−βEn 
eβℏωnm + 1
e−iωnmt
AnmBmn. (6.62)
We require the Fourier transform,
C0
AB(ω) ≡
 ∞
−∞
C0
AB(t)eiωtdt. (6.63)
Combining Eq. (6.62) with Eq. (6.63), we have
C0
AB(ω) = π
Z(β)

n

m
e−βEn 
eβℏω + 1
AnmBmnδ(ω − ωnm). (6.64)
Note that the definition of χ(ω) in Eq. (6.30) involving a one-sided Fourier transform leads to the
use of the Plemelj formula in Eq. (6.43) and the subsequent division of χ into real and imaginary
parts in Eq. (6.44). Equation (6.63), however, the definition of C0(ω), involves an integration over
all times and leads to the delta function in Eq. (6.64), a purely real expression (see Exercise 6.16).
Equation (6.64) [for C0(ω)] is almost identical to Eq. (6.44) [for χ′′(ω)]; the two differ by a
multiplicative constant and a plus sign. One can show that
χ′′
AB(ω) = 1
ℏ
eβℏω − 1
eβℏω + 1C0
AB(ω) = 1
ℏ tanh( 1
2 βℏω)C0
AB(ω). (6.65)
Equation (6.65) is one form of the celebrated fluctuation-dissipation theorem, which has many
faces—it’s been derived in different guises by different researchers. Credit is usually attributed
to Callen and Welton[192], but antecedents can be seen in Nyquist’s theorem, the Einstein re￾lation, and Onsager reciprocity. Kubo[193] recounts the history of the theorem; Chester[194]
and Zwanzig[195] give extensive literature reviews. Case[196] is an excellent review article. A
fluctuation-dissipation theorem is a relation between χ′′(ω) and the Fourier components of the
time correlation function C0(ω). It’s often given as the inverse of Eq. (6.65),
C0
AB(ω) = ℏ coth( 1
2 βℏω)χ′′
AB(ω) + Cδ(ω), (6.66)172 ■ Non-Equilibrium Statistical Mechanics
where C is a system-dependent constant. One has to treat the ω = 0 limit carefully in inverting Eq.
(6.65). The classical version is found from the formal limit ℏ → 0 in Eq. (6.65),
χ′′
AB(ω) = 1
2
βωC0
AB(ω). (classical) (6.67)
Note the dimension of βω, (energy-time)−1, a classical proxy for ℏ−1 in Eq. (6.65).
Fluctuation-dissipation theorems relate two distinct physical quantities, each accessible to mea￾surement. Fluctuations occur spontaneously, in the absence of external interactions, from the cease￾less motions of microscopic constituents of macroscopic systems.35 Correlations of fluctuations,
in space and time, are revealed through scattering structure factors. Dissipation is the irreversible
absorption of the work done by external forces into microscopic degrees of freedom, and there are
different ways of measuring absorption. One can regard the theorem in the form of Eq. (6.65) as
specifying the efficacy of fluctuations (of frequency ω) in absorbing energy of frequency ω. The
converse, Eq. (6.66), specifies the strength of fluctuations at frequency ω occurring in response to
the absorption of energy of frequency ω. Both quantities are mechanical in origin—absorption of
work done by external forces and fluctuations from particle motions. Note the characteristic com￾parison of two energies: ℏω and kT. The theorem explains the success of the Onsager regression
hypothesis which equates the rate of change of fluctuations with dissipative fluxes; see Eq. (1.42).36
Basically, a system doesn’t “know” whether it’s been brought into a nonequilibrium state by the
action of an external force or as the result of a random fluctuation.
6.4 GREEN-KUBO THEORY OF TRANSPORT COEFFICIENTS
Transport coefficients, introduced phenomenologically in irreversible thermodynamics, can be ex￾pressed as integrals of time correlation functions, formulae known as Green-Kubo relations, from
Green[197] and Kubo[185]. Transport coefficients are classified as mechanical, characterizing the
response to external interactions, electrical conductivity for example, or thermal, those associ￾ated with inhomogeneities—diffusion, viscosity, or heat conduction, the type treated in Chapman￾Enskog theory, Section 4.9.
6.4.1 Mechanical transport processes
As a representative example, consider a system of charges {ek} in an oscillating spatially uniform
electric field E(t) = Ee−iωt ≡ E limϵ→0+ e−i(ω+iϵ)t
, where strictly speaking a convergence factor
should be included. The energy of interaction37
H′
(t) = −

k
ekrk · E(t) ≡ −e−iωtP · E, (6.68)
where ek is the charge of the kth-particle with rk its position vector, a dynamical variable. The total
current density J = (1/V )

k ekr˙k, where V is the system volume. We write its µth-component
Jµ ≡ 1
V

k
ekr˙k,µ. (6.69)
We use Greek letters µ ≡ 1, 2, 3 to label vector components and Roman letters k to label particles.
In many cases we can let V = 1; a system of unit volume.
35Consider Eq. (4.125) relating the width of the velocity distribution (at every point of physical space) to the temperature.
36The regression hypothesis is integral to the proof of Onsager’s (experimentally confirmed) reciprocity relations.
37Equation (6.68) is the standard expression U = −P · E with P the electrical polarization. Note, however, that P is
extensive; it scales with the size of the system. In electromagnetic theory, P is defined as a polarization density.Dissipation, fluctuations, and correlations ■ 173
The change ∆ρ(t) occurring in response to the perturbation H′
(t) [see Eq. (6.21)] induces a
nonequilibrium current (in equilibrium ⟨Jµ⟩0 = Tr ρ0Jµ = 0),
⟨Jµ⟩t = Tr ∆ρ(t)Jµ = 1
iℏ
 t
−∞
dτ e−iωτ Tr{ρ0 [P , Jµ(t − τ )]} · E
= 1
iℏ
 t
−∞
dτ e−iωτ Tr{[ρ0, P ]Jµ(t − τ )} · E, (6.70)
from Eq. (6.23) and our old friend, cyclic invariance of the trace. No confusion should arise over
the symbols in Eq. (6.70): Jµ(t−τ ) is a microscopic operator, Eq. (6.69), and ⟨Jµ⟩t is the nonequi￾librium average. Equation (6.70) implies a generalization of Ohm’s law to include anisotropy,
⟨Jµ⟩t = 
ν
σµν(ω)Eνe−iωt (6.71)
[compare with Eq. (6.29)], where the conductivity tensor is the generalized susceptibility for this
problem,
σµν(ω) ≡ 1
iℏ
 ∞
0
dθeiωθ Tr{[ρ0, Pν]Jµ(θ)}. (6.72)
This can be simplified with the Kubo identity [see Eq. (6.26)],

ρ0, Pν

= iℏρ0
 β
0
dαP˙
ν(−iℏα). (6.73)
Combining Eqs. (6.73) and (6.72),
σµν(ω) =  β
0
dα
 ∞
0
dθeiωθ Tr{ρ0P˙
ν(−iℏα)Jµ(θ)}. (6.74)
But P˙
ν is related to the current density [see Eq. (6.69)],
P˙
ν = 
k
ekr˙k,ν = V Jν, (6.75)
implying the conductivity
σµν(ω) = V
 β
0
dα
 ∞
0
dθeiωθ Tr{ρ0Jν(−iℏα)Jµ(θ)} ≡ V
 β
0
dα
 ∞
0
dθeiωθ⟨Jν(−iℏα)Jµ(θ)⟩
0.
(6.76)
Equation (6.76) (the Kubo formula for the conductivity) specifies that conductivity is proportional
to the time integral of the current autocorrelation function. The longer the current components stay
correlated, the greater is the conductivity. The classical conductivity is found from the limit ℏ → 0,
σµν(ω) = βV  ∞
0
dθeiωθ⟨Jν(0)Jµ(θ)⟩
0. (classical) (6.77)
If Jν, Jµ are uncorrelated, σµν is diagonal and the conductivity is a scalar, σ = 1
3

µ σµµ.
We can rewrite Eq. (6.76) by invoking the stationarity of equilibrium averages—no unique origin
in time. For fixed α, the products Jν(−iℏα)Jµ(θ) generated for 0 ≤ θ < ∞ are the same as those
generated by Jν(θ − iℏα)Jµ(0). Thus,
σµν(ω) = V
 β
0
dα
 ∞
0
dθeiωθ⟨Jν(θ − iℏα)Jµ(0)⟩
0. (6.78)174 ■ Non-Equilibrium Statistical Mechanics
Now let θ = θ′ + iℏα,
σµν(ω) = V
 β
0
dα
 ∞−iℏα
−iℏα
dθ′
eiωθ′
e−ℏωα⟨Jν(θ′
)Jµ(0)⟩
0
= V
 β
0
dαe−ℏωα  ∞
0
dθeiωθ⟨Jν(θ)Jµ(0)⟩
0, (6.79)
where it’s assumed that functions of the complex variable θ − iℏα are sufficiently analytic that the
integration path can be shifted to the real axis.38 Integrating over α,
σµν(ω) = V
ℏω

1 − e−βℏω  ∞
0
dθeiωθ⟨Jν(θ)Jµ(0)⟩
0. (6.80)
We see from the ω → 0 limit of Eq. (6.80) that σ′′
µν(0) = 0; no dissipation at ω = 0.
6.4.2 Thermal transport processes
Linear response theory is well suited to the analysis of mechanical transport processes; just add
the appropriate energy of interaction term to the Hamiltonian. There are transport processes, how￾ever, not associated with perturbations of the Hamiltonian. Diffusion, viscosity, heat conduction—
processes associated with inhomogeneities and involving transport of matter, momentum, and
energy—do not occur in response to forces represented by terms in the microscopic Hamiltonian.39
Yet, as we show, Green-Kubo relations can be developed for thermal transport coefficients.
We follow the approach of Mori[199], the crux of which is the use of local thermodynamic
equilibrium.40 We introduced that concept in Chapter 1 to extend thermodynamics to systems not
rigorously in thermodynamic equilibrium, but in which variations in state variables are negligible
over small regions of space. Nothing in that theory, however, quantifies the size of such regions.
Kinetic theory provides a characteristic length, the mean free path l. Local equilibrium holds when
spatial variations over a mean free path are small (see Section 4.8 on normal solutions),
l
|∇f|
f ≪ 1, (6.81)
where f is the single-particle distribution function. This criterion is equivalent to the collision fre￾quency far exceeding the rate of flow processes. In collision-dominated regimes, collisions quickly
establish a state that’s closely related to the equilibrium state, but only locally. Solutions of the Boltz￾mann equation in this regime—normal solutions [for example, the local Maxwellian Eq. (4.129)]—
are such that spatiotemporal variations occur through a functional dependence on the hydrodynamic
fields ρ(r, t), u(r, t), and T(r, t).
Mori extended the concept of local equilibrium to the N-body distribution—a bold step—yet
its predictions are in agreement with other approaches; see Green[200][201]. We define the N￾body distribution function associated with local equilibrium to be a generalization of the local
Maxwellian,
FLE(Γ) ≡ Z−1 exp 
−

drEˆ(Γ, r)

[kT(r)]
, (6.82)
where r is an arbitrary position in the system (and not a canonical variable), T(r) is the local tem￾perature, and Eˆ(Γ, r) is a microscopic internal energy density (thermal kinetic energy and potential
38As an example,  ∞+iα
−∞+iα exp(−x2)dx =  ∞
−∞ exp(−x2)dx for real constant α; see Exercise 2.47.
39Molecules respond to external fields the same in homogeneous or inhomogeneous systems. Luttinger[198] showed that
thermal transport coefficients can be calculated in a Hamiltonian formalism when suitable external potentials are posited,
such as time-varying, inhomogeneous gravitational fields (which don’t exist on laboratory length scales). 40Mori used quantum mechanics and the grand canonical ensemble, unimportant differences with our analysis.Dissipation, fluctuations, and correlations ■ 175
energy of intermolecular forces),
Eˆ(Γ, r) ≡ 
i

1
2
m[vi − u(r)]2 +
1
2

j̸=i
Φ(ri, rj )

δ(r − ri)
= 
i
1
2
m 
v2
i − 2u · vi + u2
+
1
2

j̸=i
Φ(ri, rj )

δ(r − ri)
≡ εˆ(Γ, r) − u(r) · gˆ(Γ, r) + 1
2
u2(r)ˆρ(Γ, r). (6.83)
In this equation, ri is the canonical position variable of the i
th-particle with mvi its momentum,
Φ(ri, rj ) is the two-body potential energy function [see Eq. (4.18)], the quantities ρˆ(Γ, r), gˆ(Γ, r),
and εˆ(Γ, r) are local microscopic densities of mass, momentum, and total energy41 [defined in
Chapter 4, see Eqs. (4.30), (4.32), and (4.35)], and u(r) is the mean local velocity [see Eq. (4.33)],
u(r) ≡ ⟨gˆ(Γ, r)⟩
⟨ρˆ(Γ, r)⟩
. (6.84)
Equation (6.83) specifies a microscopic internal energy density, a quantity not defined in Section
4.4.1. The average of Eq. (6.83), combined with Eq. (6.84), reproduces Eq. (4.59) for the internal
energy, ⟨E⟩ˆ = ⟨εˆ⟩ − 1
2 ⟨ρˆ⟩u2. A term could be added to Eq. (6.83) having zero average.
We denote the N-body distribution42 F(t). From Liouville’s equation (4.5), ∂F(t)/∂t = LF(t)
[L ≡ −iΛ; see Eq. (4.4)], implying F(t)=eLtF(0). We can then write
F(t) = F(0) + [F(t) − F(0)] = F(0) +  t
0
dτ ∂
∂τ F(τ )
= F(0) +  t
0
dτLF(τ ) = F(0) +  t
0
dτ eLτLF(0).
Assume the system at t = 0 is in the local equilibrium state, F(0) = FLE. In that case,
F(t) = FLE +
 t
0
dτ eLτLFLE. (6.85)
Clearly we need to find the action of L on FLE to have an expression for the nonequilibrium distri￾bution F(t).
43 It’s straightforward to show that (see Exercise 6.17),
LFLE = −FLE 
dr
1
kT(r)
LEˆ(Γ, r). (6.86)
The problem therefore reduces to finding the action of L on Eˆ(Γ, r). Fortunately we’ve already
done much of the work.
Noting that ρ, ˆ gˆ, εˆ represent observables, we have from Liouville’s equation (see Exercise 4.2),
L



ρˆ
gˆ
εˆ


 = − ∂
∂t



ρˆ
gˆ
εˆ


 =



∇ · gˆ
∇ · Pˆ
∇ · Jˆ
ε


 , (6.87)
41In Section 4.4 we denoted microscopic densities with “hats,” ρˆ(r), gˆ(r), and ϵˆ(r). We include the notation Γ in Eq.
(6.83) to distinguish microscopic Γ-space functions [such as ρˆ(Γ, r)] from the average quantity u(r). 42We haven’t had much occasion to use the N-body distribution, which we denoted ρ(Γ, t) in Section 4.1. We used
fs for s-body distributions in Section 4.2, with Fs the s-tuple distribution. The Boltzmann equation is for the one-particle
distribution function f1, where we’ve generally dropped the subscript. 43If the local equilibrium approximation FLE were truly the equilibrium distribution, we’d have LFLE = 0.176 ■ Non-Equilibrium Statistical Mechanics
where we’ve used Eqs. (4.65), (4.69), and (4.71), the microscopic balance equations for mass, mo￾mentum, and energy (in the absence of external forces), where Pˆ = Pˆ K + Pˆ Φ is the microscopic
momentum flux tensor [see Eqs. (4.66) and (4.68)] and Jˆ
ε is the microscopic total energy flux; see
Eq. (4.72). From Eqs. (6.83) and (6.87),
LEˆ(Γ, r) = ∇ · Jˆ
ε(Γ, r) − u(r) ·∇· Pˆ (Γ, r) + 1
2
u2(r)∇ · gˆ(Γ, r). (6.88)
Note the three spatial derivatives; we’re going to integrate by parts.44 From Eq. (6.86) [dropping the
dependence on (Γ, r) in the terms for Eˆ(Γ, r)],

dr
LEˆ(r)
kT(r) =

dr
1
kT(r)

α
∇αJˆ
ε,α −
α,β
uβ∇αPˆβα +
1
2
u2
α
∇αgˆα

=

dr

α

− Jˆ
ε,α +
β
uβPˆβα − 1
2
u2gˆα

∇α
 1
kT(r)

+

dr
1
kT(r)

α,β

Pˆβα − uβgˆα

∇αuβ, (6.89)
where integrated parts vanish. We see that temperature and velocity gradients emerge naturally in
this approach. The next logical step would be to combine Eq. (6.89) with Eq. (6.86) and substitute
back into Eq. (6.85). Equation (6.89) is quite complicated, however; time for approximations.45
We start by noting there is no local dissipation in the local equilibrium state. In this state, the av￾erage kinetic part of the heat flux vanishes, as can be seen using the local Maxwellian Eq. (4.129),46
JLE
Q (r, t) ≈ m
2

(v − u) (v − u)
2fLM(r, v, t)dv = 0 (6.90)
[integrand is odd in (v −u)], and the average kinetic part of the pressure tensor is diagonal with the
local pressure of an ideal gas on the diagonal,47
PLE
αβ(r, t) ≈ m

dv(v −u)α(v −u)βfLM(r, v, t) = n(r, t)kT(r, t)δα,β ≡ P(r, t)δα,β. (6.91)
Thus, spatial inhomogeneities alone are insufficient to drive dissipative transport processes; the
distribution function must have deviations from local-equilibrium form, as we see in Eq. (6.85) and
what we found in Chapman-Enskog theory, see Eq. (4.180).
Under the substitutions vi → u+ (vi − u) in Pˆ K [see Eq. (4.66)], we have for the full pressure
tensor Pˆ ,
Pˆαβ(Γ, r) =ˆρ(Γ, r)uαuβ + [ˆgα(Γ, r) − ρˆ(Γ, r)uα] uβ + [ˆgβ(Γ, r) − ρˆ(Γ, r)uβ] uα
+ Pˆ(Γ, r)δα,β + Pˆdiss
αβ (Γ, r), (6.92)
where Pˆ(Γ, r) = 1
3

α Pˆαα(Γ, r) is the local microscopic pressure [see Eq. (4.50)] and Pˆdiss
αβ is
the remainder of Pˆ having the important property that it vanishes in local equilibrium,
⟨Pˆdiss
αβ ⟩
LE = 0. (6.93)
44The quantity LEˆ is the time derivative of −Eˆ. Thus, in Eq. (6.88) we have a time derivative expressed in terms of
spatial derivatives, similar to that in the Chapman-Enskog theory; see Eq. (4.160). 45The starting point Eq. (6.82) is itself an approximation. Theoretical physics is the art of approximation: It’s better to be
approximately right than precisely wrong.
46The average heat flux JQ = JK + JΦ [see Eq. (4.60)], where JK, JΦ are average fluxes of kinetic and potential
energy associated with thermal motions; see Eq. (4.58). Equation (6.90) assumes negligible potential energy contributions.
47The average pressure tensor P = PK + PΦ, with PΦ the average momentum flux associated with potential energy
[see Eq. (4.45)] and PK that due to thermal motions [see Eq. (4.48)]. Equation (6.91) ignores potential energy contributions.Dissipation, fluctuations, and correlations ■ 177
Thus, we’ve separated momentum flux into convective and dissipative parts. Applying the same
substitution to the energy flux Jˆ
ε, we have a separation into convective and dissipative parts,
Jˆ
ε,α(Γ, r) = uαεˆ(Γ, r) + 1
2
u2 [ˆgα(Γ, r) − uαρˆ(Γ, r)] + uαPˆ(Γ, r)
+
β
uβPˆdiss
αβ (Γ, r) + Jˆdiss
ε,α(Γ, r), (6.94)
where the remainder has the property
⟨Jˆdiss
ε,α⟩
LE = 0. (6.95)
The flows represented by Pˆdiss
αβ and Jˆdiss
ε,α (being zero in local equilibrium) are measures of the
response of the system to the presence of gradients and play the same role as the response ⟨∆B(t)⟩
in linear response theory; see Section 6.2. Let’s calculate the average dissipative energy flow,
⟨Jˆdiss
ε,α(r)⟩t =

dΓJˆdiss
ε,α(Γ, r)F(Γ, t) = 
dΓJˆdiss
ε,α(Γ, r)
 t
0
dτ eLτLFLE(Γ)
=

dΓJˆdiss
ε,α(Γ, r)
 t
0
dτ eLτFLE(Γ)

dr′ 1
kT(r′
)

−LEˆ(Γ, r′
)

=

dΓJˆdiss
ε,α(Γ, r)
 t
0
dτ eLτFLE(Γ)

dr′
×

β

Jˆ
ε,β(Γ, r′
) −
γ
uγ(r′
)Pˆγβ(Γ, r′
) + 1
2
u2(r′
)ˆgβ(Γ, r′
)

∇β
 1
kT(r′
)

− 1
kT(r′
)

β,γ

Pˆγβ(Γ, r′
) − uγ(r′
)ˆgβ(Γ, r′
)

∇βuγ(r′
)

, (6.96)
where in the second equality we’ve used Eq. (6.85), in the third, Eq. (6.86), and in the fourth, Eq.
(6.89). Equation (6.96) is quite complicated. It simplifies, however, when we recognize that we’re
only interested in the linear response. We can therefore evaluate the quantities multiplying gradients
at lowest order, the state of u = 0, implying the local equilibrium distribution function reduces to
that of the canonical ensemble, FLE(Γ) → f0(Γ) ≡ Z−1 exp(−βH(p, q)). In this approximation,
Eq. (6.96) reduces to
⟨Jˆdiss
ε,α(r)⟩t = 
β
 
dΓJˆdiss
ε,α(Γ, r)
 t
0
dτ eLτ f0(Γ)

dr′
Jˆdiss
ε,β (Γ, r′
)

∇β
 1
kT(r)

. (6.97)
In arriving at Eq. (6.97), we’ve: 1) used that the vector flow Jˆdiss
ε,α cannot couple to the tensor flow
Pˆdiss
γβ (Curie’s theorem,48 Section 1.5); 2) evaluated the gradient locally at position r; and 3) used
that Jˆ
ε,α in Eq. (6.94) reduces to Jˆdiss
ε,α in this approximation.
The terms in square brackets in Eq. (6.97) are basically the thermal conductivity tensor. This
can be brought out with a few steps. Define a spatially averaged flux,
Jα(Γ) ≡ 1
V

drJˆdiss
ε,α(Γ, r) (6.98)
48Our treatment therefore relies on the phenomenological theory of Chapter 1. We note that Luttinger’s nominally mi￾croscopic treatment[198] also requires the validity of the phenomenological theory.178 ■ Non-Equilibrium Statistical Mechanics
where V is the system volume. Then, from Eq. (6.97),
⟨Jα⟩ = V 
β
 
dΓJα(Γ)
 t
0
dτ eLτ f0(Γ)Jβ(Γ)

∇β
 1
kT(r)

= − V
kT2

β
  t
0
dτ

dΓf0(Γ)Jβ(Γ)

e−LτJα(Γ)
 
∇βT. (6.99)
Assume the time integral reaches a plateau value over a time on the order of the relaxation time. The
limit of integration can then be extended to infinity, leaving us with an expression for the thermal
conductivity tensor,
καβ = V
kT2
 ∞
0
dτ ⟨Jα(τ )Jβ(0)⟩
0, (6.100)
a result in the Green-Kubo form, proportional to an integral of the heat flow autocorrelation function.
An analogous calculation for the viscosity of an isotropic system yields
η = V
kT  ∞
0
dτ ⟨Pxy(τ )Pxy(0)⟩
0, (6.101)
where Pαβ ≡ V −1  drPˆdiss
αβ (r). Viscosity is proportional to the time integral of the momentum
flux autocorrelation function.
6.4.3 Einstein relation
In Section 3.1 we found a fundamental connection between transport coefficients, Eq. (3.23) (repro￾duced here),
D = kT
q
µ, (3.23)
where D is the diffusion coefficient, µ is the electrical mobility, and q is the charge of the particle.
Mobility is a mechanical transport coefficient, the proportionality factor between drift velocity and
applied electric field, ⟨v⟩d = µE, and D is a thermal transport coefficient, the proportionality
between particle current and density gradient, Jn = −D∇n. Is the Einstein relation “contained” in
a Green-Kubo formula? The answer is yes.
Start with the zero-frequency conductivity, from Eq. (6.80),
σµν ≡ σµν(ω = 0) = V
kT  ∞
0
dt⟨Jµ(0)Jν(t)⟩
0, (6.102)
where Jµ = nqvµ is the electrical current density with vµ the µth-velocity component. Thus,
σµν = V
kT n2q2
 ∞
0
dt⟨vµ(0)vν(t)⟩
0. (6.103)
Allowing that at long times ⟨vµ(0)vν(t)⟩0 → 0 as t → ∞, we can take
limt→∞⟨vµ(0)vν(t)⟩
0 = ⟨vµ⟩
0⟨vν⟩
0 = 0. (6.104)Dissipation, fluctuations, and correlations ■ 179
Then, we can write the integral in Eq. (6.103), using Eq. (6.104),
 ∞
0
dt⟨vµ(0)vν(t)⟩
0 = lim
T→∞
1
2T
 T
0
 T
0
⟨vµ(t)vν(t
′
)⟩
0dtdt
′
= lim
T→∞
1
2T
 T
0
 T
0
dxµ
dt
dxν
dt′
0
dtdt
′
= lim
T→∞
1
2T ⟨(xµ(T) − xµ(0)) (xν(T) − xν(0))⟩
0
≡ Dµν, (6.105)
where the final limit defines a diffusion tensor49 as a generalization of Eq. (2.62). The factor of 2 is
to prevent over counting; from the stationarity of equilibrium averages, ⟨vµ(t)vν(t
′
)⟩0 = f(|t−t
′
|).
Combining Eq. (6.105) with Eq. (6.103),
σµν = V
kT n2q2Dµν = N nq2
kT Dµν, (6.106)
a generalized Einstein relation. A connection between conductivity and diffusivity is inevitable—
charge is attached to particles and particles undergo diffusive motion.50
6.4.4 Comments
Transport coefficients can be expressed as integrals of autocorrelation functions of microscopic
flows, formulae that are remarkably compact. Green-Kubo relations, however, may not be the best
starting point for practical calculations. Evaluating autocorrelation functions is a difficult under￾taking; one is faced with the full N-body problem, and the use of local equilibrium is a strong
assumption difficult to justify in many-body theory. We saw in Chapter 4 that thermal transport
coefficients can be calculated in Chapman-Enskog theory—also not an easy task. Resibois[202]
showed the equivalence of thermal transport coefficients obtained from kinetic equations and corre￾lation functions.
6.5 GENERALIZED LANGEVIN EQUATION
In Chapter 3, we introduced the Langevin equation featuring a random force as a way to model
Brownian motion, colloidal particles incessantly in motion as a result of collisions with molecules
of the suspension medium. The Langevin equation could be considered an “inspired guess” as a
generalization of Newton’s second law with the random force a new kind of force. In nonequilibrium
statistical physics we seek the slowest processes in the approach to equilibrium. Is there a systematic
way to develop Langevin-like equations for arbitrary slow modes? There is, and such equations are
known as generalized Langevin equations. We follow Mori[203].
6.5.1 Derivation
Suppose we have a set of dynamical variables {A1,...,Af } that we deem “relevant” (most likely
because they represent macroscopic degrees of freedom). These variables comprise a subspace (the
A-space) of the infinite-dimensional space of all dynamical variables, for which a scalar product
49One way to define tensors of a given rank is through products of lower-rank tensors; in this case a second-rank tensor
(Dµν or σµν) is defined by the outer product of vectors (first-rank tensors), xµxν or vµvν. See [6, p81]. 50Conduction occurs in charge-neutral systems, the motion of light charge carriers through a system of more massive
ions. We can treat charge carriers as ostensibly independent particles. Note that µ = σ/(nq)180 ■ Non-Equilibrium Statistical Mechanics
must be specified before it can be called Hilbert space.51 We indicate scalar products with the nota￾tion,
(U, V ) ≡
 dΓfeq(Γ)U∗(Γ)V (Γ) (classical)
Tr ρeqU†V. (quantum) (6.107)
Note that, by definition,52 (U, V )∗ = (V,U) and that we’re using correlation functions to define the
scalar product, CAB(t)=(A, B(t)). The quantities A1,...,Af comprise a basis for A-space.53
We can define an operator P that projects from an arbitrary dynamical variable X its components
along the A-variables,
PX ≡ 
f
k=1

f
l=1
AkG−1
kl (Al, X), (6.108)
where G−1
kl is the matrix inverse of Gij ≡ (Ai, Aj ). P satisfies the defining property of projection
operators, P◦PX = PX for all X; see Exercise 6.18.54
We have the identity,
X = PX + [I−P] X, (6.109)
where I is the identity operator. We can consider PX the macroscopic part of X and [I−P]X its
microscopic part, an interpretation based on the orthogonality of [I−P] X and PX (see Exercise
6.18),
(PX, [I−P] X)=0. (6.110)
Thus, [I−P] X has no component along the macroscopic part PX and what is not macroscopic
is microscopic.55 We encountered that idea in our treatment of the Langevin equation where the
random force is independent of B-particle velocity; see Eq. (3.10). It’s traditional to denote the
projection operator I−P ≡Q. One can show that Q◦Q = Q. The Liouville operator can be split
into orthogonal parts56 by applying the identity I = P + Q,
L = IL = PL + QL. (6.111)
Here PL represents the time rate of change of the “relevant” components57 of X with QL the rate
of change of the components of X orthogonal to PX.
We seek the decomposition of the time evolution operator. We state the result and then discuss
how it’s found,
eLt = eQLt +
 t
0
dτ eL(t−τ)
PLeQLτ . (6.112)
This identity can be verified directly through differentiation (see Exercise 6.19), but let’s derive it.
The Laplace transform of eLt generates the resolvent (see Appendix D) of L,[204, p341]
 ∞
0
e−zteLtdt = 1
z − L, (6.113)
51Hilbert space is a complete inner-product space[13, p45]. It’s understood that Hilbert space is infinite dimensional;
finite-dimensional spaces are automatically complete. The term “finite-dimensional Hilbert space” is a misnomer. 52Scalar products, like many quantities in mathematics, are defined by axioms; see [13, p10]. The property (U, V )∗ =
(V,U) is an axiom for the inner product used in many areas of physics. 53Any N linearly independent vectors form a basis for an N-dimensional space. It won’t necessarily be an orthonormal
basis, but such bases can always be constructed with the Gram-Schmidt method; see [13, p14]. 54A projection operator P on Hilbert space H is a linear mapping P:H→H such that P◦P = P. 55This is the same logic used in Chapter 1 to derive the entropy balance equation, the division of internal energy into heat
and work; see discussion near Eq. (1.34). In [2, p10] we noted that, “The division of internal energy into work and heat lines
up with the distinction between macroscopic and non-macrosopic, i.e., microscopic. . . .Work is associated with changes in
observable macroscopic quantities. What is not work is heat, energy transferred to microscopic degrees of freedom.” 56For any X, (PLX, QLX)=0. Note that PQ ≡ 0. 57From LX(Γ) = −∂X/∂t, PLX = −P∂X/∂t = −(∂/∂t)PX, because P has no explicit time dependence.Dissipation, fluctuations, and correlations ■ 181
for |z| > ||L||, the operator norm of L (a condition we blithely assume). Using the identity in Eq.
(D.21),58
1
z − A = 1
z − B − 1
z − A(B − A) 1
z − B , (6.114)
we have, with B ≡ QL and A ≡ L = (P + Q)L,
1
z − L = 1
z − QL +
1
z − LPL 1
z − QL. (6.115)
The inverse Laplace transform of Eq. (6.115) (with the convolution theorem) is Eq. (6.112).
Now for some algebraic steps. First, using Eq. (6.108),
eLtPLAj = 
k
Ak(t)

l
G−1
kl (Al, LAj ) ≡ 
k
Ak(t)Ωkj , (6.116)
where the Ωkj comprise the elements of a matrix of frequencies
Ωkj = 
l
G−1
kl (Al, LAj ) = 
l
G−1
kl 
Al, d
dt
Aj



t=0
≡ 
l
G−1
kl 
Al, A˙
j (0)
. (6.117)
In Eq. (6.116), eLtPLAj , the time development of the projection of the initial slope A˙
j (0) onto A￾space, is represented by a linear combination of the Ak(t) weighted by Ωkj . If all A-variables have
the same signature under time reversal, then Ωkj = 0 [because in that case 
Al(0), A˙
j (0)
= 0].59
Then, with
d
dt

eLtAj

= eLtLAj = eLtPLAj + eLtQLAj ,
we have, using Eq. (6.116),
d
dt
Aj (t) −
k
Ak(t)Ωkj = eQLtQLAj   
fj (t)
+
 t
0
dτ eL(t−τ)
PLeQLτQLAj   
fj (τ )
, (6.118)
where we’ve right-multiplied Eq. (6.112) by QLAj . We denote by fj (t) the term eQLtQLAj , the
time development (under eQLt) of the projection of the initial slope A˙
j (0) onto the orthogonal
complement of A-space. It plays the role of a driving force in the equation of motion for Aj (t) and
is called the “random force,” yet dimensionally [fj ]=[Aj ]/time. There is a separate force term
for each A-variable, each having no projection onto A-space, Pfj (t) ≡ 0. The fj are therefore
uncorrelated with A-variables, (Ai, fj (t)) = (PAi, fj (t)) = (Ai,Pfj (t)) = 0, where we’ve used
PAj = Aj . In that regard, fj (t) behaves like the random force postulated in the Langevin equation;
see Eq. (3.10). The lack of correlation between fj (t) and initial values Ai(0) lies at the foundation
of the Onsager regression hypothesis.
The integrand of Eq. (6.118) involves PLfj , the projection of the initial slope Lfj ≡ ˙fj (0) onto
A-space,
PLfj (τ ) = 
k

l
AkG−1
kl (Al, Lfj (τ )) = −

k

l
AkG−1
kl (LAl, fj (τ ))
= −

k

l
AkG−1
kl ((P + Q)LAl, fj (τ )) = −

k

l
AkG−1
kl (QLAl, fj (τ ))
= −

k

l
AkG−1
kl (fl(0), fj (τ )) ≡ −
k
AkMkj (τ ), (6.119)
58Equation (6.113) is the negative of the resolvent defined in Appendix D; let R → −R in Eq. (D.21). 59Said differently, the frequencies Ωkj vanish unless Al and Aj have different time-reversal symmetries.182 ■ Non-Equilibrium Statistical Mechanics
where we’ve used that60 (X, LY ) = −(LX, Y ) for any X, Y and we’ve introduced memory func￾tions,
Mkj (t) ≡ 
l
G−1
kl (fl(0), fj (t)). (6.120)
Memory functions are related to time correlation functions of random forces, which Kubo[193]
termed the second fluctuation-dissipation theorem.
61 We note that the force-force correlation func￾tion is stationary,
(fi(t), fj (t
′
)) = 
eQLtQLAi, eQLt′
QLAj

=

A˙
i(0), QeQL(t′
−t)
QA˙
j (0)
. (6.121)
With these definitions, Eq. (6.118) can be written62
d
dt
Aj (t) −
k
Ak(t)Ωkj +
k
 t
0
dτAk(t − τ )Mkj (τ ) = fj (t). (6.122)
Equation (6.122), the generalized Langevin equation, is an exact rewrite of the microscopic equa￾tions of motion. It was derived by Mori[203]; see discussions in Forster[205] and Zwanzig[206]. It
separates the dynamics into three effects associated with (Ωkl, Mkj (t), fj (t)): 1) collective oscilla￾tions63 caused by correlations among A-space variables; 2) time-lagged memory effects, mediated
by force-force time correlations, where past values of A-variables control their present rates of
change; and 3) driving forces fj (t) uncorrelated with A-variables. The generalized Langevin equa￾tion is not easier to solve than the Liouville equation; it’s a platform for introducing approximations.
6.5.2 Recovering the Langevin equation for Brownian motion
Let’s verify that the generalized Langevin equation reduces to the Langevin equation for Brow￾nian motion established in Chapter 3. The relevant variables are the velocity components of the
B-particle of mass M, Aj = vj , j = 1, 2, 3. The frequencies Ωkj = 0 because the vj all have
the same signature under time reversal. The correlation matrix for these variables is diagonal,
Gij = (vi, vj ) = δijv2
th, where vth ≡ kT /M, implying G−1
ij = δij/v2
th. We assume the driv￾ing forces vary significantly faster than the particle velocity and can be treated as delta-correlated,
(fl(0), fj (t)) = Γδlj δ(t), (6.123)
where Γ is to be determined. With Eq. (6.123), we have from Eq. (6.120),
Mkj (t) = Γ
v2
th
δkj δ(t). (6.124)
There are no memory effects for delta-correlated forces. We therefore have from Eq. (6.122),
d
dt
vj +
Γ
2v2
th
vj (t) = fj (t), (6.125)
where the factor of two is from integrating over “half” a delta function. Equation (6.125) is iden￾tical to the Langevin equation (3.9) when we set Γ=2kT α/(M2), with α the drag-force friction
coefficient. (This choice differs from Eq. (3.14) by the factor of M2 because fj in the present case
is an acceleration, whereas in the Brownian motion problem the random force is actually a force.)
60The Liouville operator Λ as defined in Eq. (4.4) is Hermitian, but we’re using the abbreviation L = iΛ. The operator
L is skew-Hermitian. 61The first fluctuation-dissipation theorem, Eq. (6.65), relates the imaginary part of the generalized susceptibility to
the spectral properties of time correlation functions. The second, Eq. (6.120), relates memory functions (and their spectral
properties) to time correlation functions of the random force (and their spectral properties).
62The time-lagged memory integral can equally well be written  t
0 dτAk(τ)Mkj (t − τ).
63When we substitute L = iΛ, these terms represent oscillatory motions.Dissipation, fluctuations, and correlations ■ 183
6.6 MEMORY FUNCTIONS
Let’s turn to the larger theme of this chapter, correlation functions. Consider the set of correlation
functions among the “relevant” variables introduced in the last section,
Cij (t) ≡ (Ai(0), Aj (t)), (6.126)
where it’s convenient to continue with the inner product notation of Eq. (6.107). Take the time
derivative of Cij (t) and use the generalized Langevin equation. We find
d
dt
Cij (t) = 
k
Cik(t)Ωkj −
k
 t
0
dτCik(t − τ )Mkj (τ ). (6.127)
The driving force does not appear because (Ai, fj )=0; it’s implicit in the memory function
Mkj (τ ). Equation (6.127) is the memory function equation. It’s an exact consequence of the mi￾croscopic equations of motion; it’s a powerful tool for calculating time correlation functions when
particular forms of the memory function are assumed; see Boon and Yip[207].
Noting the convolution form, introduce the Laplace transform
Cij (z) ≡
 ∞
0
e−ztCij (t)dt. (6.128)
Equation (6.127) can then be written in a form that’s useful if the Laplace transforms M
kj (z) ≡  ∞
0 e−ztMkj (t)dt are simpler objects than the Cik(z),

k

zδkj − Ωkj + M
kj (z)

Cik(z) = Cij (t = 0). (6.129)
One solves Eq. (6.129) for the Cik(z) and then inverse transforms to infer Cik(t).
SUMMARY
We began this chapter with a review of correlation functions, of which there are different kinds,
equilibrium and nonequilibrium. Of particular interest are time correlation functions, CAB(t) =
⟨A(0)B(t)⟩0, where the superscript indicates an average with respect to an equilibrium probability
distribution. Time correlation functions comprise the third leg of the stool supporting nonequilib￾rium statistical mechanics; the other two are stochastic dynamics and kinetic theory.
• Measurements on macroscopic systems are described in terms of correlation functions—a
fundamental reason to study them. Spectral properties of correlation functions are directly
measured through scattering structure factors, elastic and inelastic. In addition, the response
of macroscopic systems to perturbations is expressed in terms of correlation functions. For
a time-dependent interaction, the energy of which is H′
(t) = −AF(t), where A is a Γ￾space function (classical) or a Hermitian operator (quantum) and F(t) is the time history of
turning on the interaction with limt→−∞ F(t)=0, the nonequilibrium response of system
quantity B to interaction A is specified by the response function ΦBA(t) with ∆B(t) =
 t
−∞ dτΦBA(t − τ )F(τ ). The response function is defined with correlation functions,
ΦBA(t) =  dΓf(Γ) [A, B(t)]P (classical)
1/(iℏ) Tr ρ [A, B(t)] , (quantum)
where f(Γ) is the phase-space distribution function, [∗, ∗]P denotes Poisson bracket, ρ is
the density matrix, and [∗, ∗] denotes commutator. The response function has the property
ΦBA(t < 0) = 0. Any function f(t) such that f(t < 0) = 0 is known as a causal function.184 ■ Non-Equilibrium Statistical Mechanics
• For a monochromatic interaction with F(t) ≡ limϵ→0+ E0e−iωt+ϵt, the response can be writ￾ten ∆B(t) = χBA(ω)E0e−iωt, where χBA(ω), the Fourier transform of ΦBA(t), is known
as the generalized susceptibility, χBA(ω) ≡ limϵ→0+
 ∞
0 dθei(ω+iϵ)θΦBA(θ). The quantity
χ(ω) (we drop BA) has no poles on the real-ω line. Moreover, it has no poles in the up￾per half of the complex-ω plane. Any poles of χ(ω) lie in the lower half of the complex-ω
plane. Mathematically (Titchmarsh’s theorem), such a function has as a boundary value on
the real-ω line a complex-valued function χ(ω):R → C with real and imaginary parts satis￾fying the Kramers-Kronig relations. The real and imaginary parts of χ(ω) are therefore not
independent, they’re Hilbert transform pairs; χ(ω) can be reconstructed from knowledge of
either part. In a standard notation for complex-valued functions, χ(ω) = χ′
(ω)+iχ′′(ω).
The imaginary part χ′′(ω) is associated with energy dissipation and satisfies the positivity
condition ωχ′′(ω) ≥ 0 for −∞ <ω< ∞. The real part χ′
(ω) is termed the reactive part,
with the imaginary part the dissipative or absorptive part.
• The fluctuation-dissipation theorem is a fundamental relation between χ′′(ω) and the Fourier
transform of the time correlation function, C(ω),
χ′′
AB(ω) = 1
ℏ tanh( 1
2 βℏω)CAB(ω),
where β ≡ (kT)−1. Any connection between χ′′(ω) and C(ω) is known as a fluctuation￾dissipation theorem. Fluctuation-dissipation theorems connect two distinct physical quanti￾ties, each accessible to measurement. Fluctuations occur spontaneously, in the absence of
external interactions, from the ceaseless motions of microscopic constituents of macroscopic
systems. Correlations of fluctuations, in space and time, are revealed through scattering struc￾ture factors. Dissipation is the irreversible absorption of the work done by external forces into
microscopic degrees of freedom, and there are different ways of measuring absorption. Both
quantities are mechanical in origin, the absorption of work done by external forces and fluc￾tuations from particle motions. Note the characteristic comparison of two energies: ℏω and
kT. The theorem explains the success of the Onsager regression hypothesis which equates
the rate of change of fluctuations with dissipative fluxes. Basically, a system doesn’t “know”
whether it’s been brought into a nonequilibrium state by the action of an external force or as
the result of a random fluctuation.
• Transport coefficients can be calculated from Green-Kubo relations, integrals of time cor￾relation functions of microscopic flows. Transport coefficients are classified as mechanical,
characterizing the response to external perturbations, electrical conductivity for example, and
thermal, those associated with system inhomogeneities, thermal conductivity for example.
Both types can be expressed by Green-Kubo relations.
• Using projection operators onto a space of physically relevant dynamical variables Aj (t)
(usually those with a clear separation in time scales between slow and fast variables), we
derived the generalized Langevin equation, Eq. (6.122), an exact rewrite of the microscopic
equations of motion having the mathematical form of Langevin equations. This analysis intro￾duces three quantites: a set of frequencies Ωij describing collective oscillations of the relevant
variables controlled by correlations among them; a driving force fj (t) having no projection
onto the space of relevant variables; and a set of memory functions, Mjk(t), controlled by
time correlation functions of the random force. The oscillation frequencies vanish for dy￾namical modes of definite time-reversal symmetry. Perhaps the most important finding of this
analysis is that the random forces fj (t) are uncorrelated with the relevant variables Aj (t).Dissipation, fluctuations, and correlations ■ 185
EXERCISES
6.1 We’ve used, several times and without proof, properties of the time evolution operator U(t) ≡
e−iΛt
, with Λ the Liouville operator. Let’s establish some of these properties.
a. First, U(t) is unitary. This is evident because P(Γ, t) is normalized for all times,
 P(Γ, t)dΓ = 1, with P(Γ, t)=e−iΛt
P(Γ, 0); see Eq. (4.6). A deeper reason is Stone’s
theorem: A self-adjoint operator H on Hilbert space H generates a one-parameter con￾tinuous family {U(α)} (or Lie group) of unitary operators on H with U(α)=eiαH,
where −∞ <α< ∞,
64 having the properties U(0) = I (identity operator) and
U(α)U(β) = U(α + β), and thus is an abelian group. Each U(α) has an inverse,
U −1(α) = U(−α), which, because it’s unitary, has adjoint U† = U −1; see [13, p34].
Group elements are continuous: U(α) → U(β) as α → β. Another way of stating the
theorem is that an operator eiαH, with α a real continuous parameter, is unitary if and
only if H is self-adjoint. The operator H is called the generator of the group. Thus, inas￾much as Λ is self-adjoint (see [5, p335]), U(t)=e−itΛ is unitary.
b. An equivalent form of Stone’s theorem is that, for a given self-adjoint operator H, there
is a continuous one-parameter group of unitary transformations {U(α)} with iH =
limα→0(1/α) [U(α) − I] as its generating infinitesimal transformation. The idea is that a
transformation U(α) for finite α can be realized from a succession of infinitesimal trans￾formations (Euler definition of exponential),
U(α)=eiαH = lim
N→∞ 
I + i
α
N
H
N
.
The entire group {U(α)} is generated by iH. It’s usually easier to prove results on in￾finitesimal transformations than for finite, knowing that finite transformations can be built
up from the compound effect of many infinitesimal transformations.65
For DH ⊆ H the domain of H, and for ϕ ∈ DH, suppose that U(α)ϕ ∈ DH. Then, as ϵ →
0, (1/iϵ) [U(ϵ) − I]U(α)ϕ → HU(α)ϕ, implying that (1/iϵ) [U(α + ϵ) − U(α)] ϕ →
HU(α)ϕ as ϵ → 0. Define
limϵ→0
1
ϵ [U(α + ϵ) − U(α)] ≡ d
dα
U(α),
so that
d
dα
U(α)ϕ = iHU(α)ϕ.
Introduce the notation ϕ(0) = U(0)ϕ = Iϕ = ϕ and ϕ(α) = U(α)ϕ. Then we have
d
dα
ϕ(α)=iHϕ(α),
an equation of motion for vectors in DH as they change under U(α).
c. Let (ϕ, χ, ψ) be vectors in DH and assume that χ is obtained from the product χ = ϕψ.
The transformation U(α)=eiαH associates with χ another vector, χ(α) = U(α)χ. The
64See Stone[208] and Riesz and Sz.-Nagy[209, Section 137]. The theorem can be stated in the converse: For {U(α)} a
continuous one-parameter unitary group on H, there is a unique self-adjoint operator H such that U(α)=eiαH for each
α. Note that H is uniquely associated with a family of operators, {U(α)}. For a single operator eiH, the choice of generator
is not unique—H can be replaced with B = H + 2πnI where n is an integer so that eiB = eiH. 65This idea is used frequently in theoretical physics, e.g., rotations and Lorentz transformations; see [6, Chapter 6].186 ■ Non-Equilibrium Statistical Mechanics
vectors ϕ and ψ are also transformed, ϕ(α) = U(α)ϕ and ψ(α) = U(α)ψ. To preserve
algebraic relations,66 we require χ(α) = ϕ(α)ψ(α) for each α, or that
U(α)[ϕψ] = U(α)ϕU(α)ψ.
This result is highly useful in calculations. Prove this relation by showing it for infinitesi￾mal transformations, assuming that H satisfies the product rule, H(ϕψ) = ϕHψ + ψHϕ.
A:
U(α)ϕU(α)ψ = (ϕ + iαHϕ)(ψ + iαHψ) + O(α2
) = ϕψ + iαϕHψ + iαψHϕ + O(α2
)
= ϕψ + iαH(ϕψ) + O(α2
) = U(α)[ϕψ].
It’s shown in Exercise 4.1 that the Liouville operator satisfies the product rule.
d. Show, following similar steps to those in Eq. (6.1), that the equilibrium correlation func￾tion defined in Eq. (6.4) is independent of time. Hint: Use the relation just derived. A:
C0
yz =

dΓy(t)z(t)Peq(Γ) = 
dΓ
eiΛt
(yz)

Peq(Γ)
=

dΓy(Γ)z(Γ)e−iΛt
Peq(Γ) = 
dΓy(Γ)z(Γ)Peq(Γ).
6.2 Derive the following relations (refer to Section 6.1 for their definitions):

C0
yz(ω; q)dq = 2πC0
yz(ω);
 ∞
−∞
C0
yz(ω)dω = 2πC0
yz;
 ∞
−∞
C0
yz(ω; q)dω = 2π

dreiq·rC0
yz(0; r).
6.3 In this exercise we find the solution to the inhomogeneous Liouville equation:
∂
∂tf(t) = Lf(t) + g(t). (P6.1)
a. First find the complementary solution. Show that the solution of (∂/∂t)f(t) = Lf(t)
has the form f(t) = U(t)f(0), where U satisfies (∂/∂t)U(t) = LU(t) with U(0) = I.
Clearly, U(t)=eLt.
b. Show that the solution of Eq. (P6.1) can be written
f(t) = U(t)f(0) +  t
0
dτU(t − τ )g(τ ).
Use the Leibniz integral rule.
6.4 Poisson brackets have a property analogous to the cyclic invariance of the trace [for operators
A, ˆ B, ˆ Cˆ, Tr AˆBˆCˆ = Tr BˆCˆAˆ = Tr CˆAˆBˆ], that for dynamical variables A, B, C, 
dΓA [B,C]
P =

dΓB [C, A]
P =

dΓC [A, B]
P . (P6.2)
Derive this result. Integrate by parts selectively, where for example to “pull” B out of
A[B,C]P, integrate by parts only those derivatives involving B. Assume integrated parts van￾ish, a boundary condition that would have to be checked before using Eq. (P6.2).
66A transformation preserving algebraic structures is called an automorphism.Dissipation, fluctuations, and correlations ■ 187
6.5 Verify that Eq. (6.22) solves Eq. (6.20).
6.6 Show for time-independent operators A, ˆ Bˆ that (used in deriving Eq. (6.23))
Tr
ρˆ0
A, U ˆ †
0 (t, τ )BUˆ 0(t, τ )

= Tr 
ρˆ0
AˆI (τ ), BˆI (t)

(P6.3)
where ρˆ0 ≡ Z−1(β)e−βHˆ 0
is the canonical density operator, U0(t, τ ) = exp[−iHˆ 0(t − τ )],
H0 is time independent, and we’re working in the interaction representation (see Appendix
E). Use cyclic invariance of the trace and that [H0, ρˆ0]=0.
6.7 We stated on general grounds that the response function Φ in Eq. (6.24) must be real (a real
response to a real interaction implies Φ is real). This can be demonstrated explicitly, however.
Show for Hermitian operators A, ˆ Bˆ that i

A, ˆ Bˆ
is Hermitian.
6.8 Derive Eq. (6.25).
a. For operators Aˆ and Hˆ , show that
d
dα

eαHˆ
Aˆe−αHˆ 
= eαHˆ 
H, ˆ Aˆ

e−αHˆ
. (P6.4)
b. Integrate Eq. (P6.4) between 0 and β and multiply by e−βHˆ from the left to arrive at the
middle expression in Eq. (6.25).
c. Show that eαHˆ 
H, ˆ Aˆ

e−αHˆ = 
H, ˆ eαHˆ Aˆe−αHˆ 
.
6.9 What is the dimension of the parameter α in Eq. (6.25)? What is the dimension of ℏα?
6.10 Derive the equality in Eq. (6.27),
 β
0
dα Tr ˆρ0A
ˆ˙(−iℏα)Bˆ(t) = −
 β
0
dα Tr ˆρ0Aˆ(−iℏα)B
ˆ˙(t).
This result follows from the Heisenberg equation of motion, Eq. (E.12) and the cyclic invari￾ance of the trace.
6.11 Show for a real-valued function f(t) that
a. Its Fourier transform g(ω) is such that g(ω) = g∗(−ω).
b. Its real part is an even function of ω and its imaginary part is odd.
6.12 Show that Tr ˙ρH = 0. Hint: Eq. (A.41).
6.13 We’ve said on general grounds that the response function is real. Show that the result in Eq.
(6.42) is real.
a. Show from Eq. (6.42) that
ΦBA(t) − Φ∗
BA(t) = − 1
ℏZ

n,m
 e−βEn − e−βEmi cos(ωnmt) (AnmBmn + BnmAmn)
+ sin(ωnmt) (AnmBmn − BnmAnm)
.
The matrices [Anm] and [Bnm] are Hermitian.188 ■ Non-Equilibrium Statistical Mechanics
b. Show that the terms in curly braces are odd under n ↔ m and thus Φ is real. A symmetric
sum over an antisymmetric function vanishes, 
n,m Snm = 0 if Snm = −Smn.
6.14 Show that the poles of χ(ω) in Eq. (6.54) (with ω a complex variable) occur in the negative
half plane for all ω0.
6.15 Show that Eq. (6.56) follows from Eq. (6.55), a task best done using the residue theorem.
Show that the contour must be closed in the lower half plane for θ > 0 and in upper half
plane for θ < 0. Beware minus signs! Closing in the lower half plane is not standard.
6.16 Show that the expression for C0
AB(ω) in Eq. (6.64) is real. A, B are Hermitian matrices.
6.17 Derive Eq. (6.86). Hint: The Liouville operator acts on canonical variables only (not on r).
6.18 For the projection operator P defined in Eq. (6.108) and inner product (X, Y ) defined in Eq.
(6.107), show that:
a. P◦PX = PX for any X;
b. PAj = Aj for Aj an element of the space of relevant dynamical variables;
c. Gij ≡ (Ai, Aj ) is a Hermitian matrix;
d. P is self-adjoint, i.e., (PX, Y )=(X,PY ) for any X, Y . Hint: The inverse of a Hermitian
matrix is Hermitian;
e. The eigenvalues of P are 0 or 1;
f. (PX, [I−P]X)=0, where I is the identity operator. Thus for any X, [I−P]X is
orthogonal to PX;
g. The operator Q≡I−P is a projection operator, Q◦QX = QX.
6.19 Verify Eq. (6.112) directly. Define a function
F(t) ≡ eQLt +
 t
0
dτ eL(t−τ)
PLeQLτ − eLt.
Note that F(0) = 0. Show that
dF
dt = LF(t).
Use the Leibniz integral rule. Conclude that F(t)=0 for all t.APPENDIX A
Statistical mechanics
S TATISTICAL mechanics must handle two sources of indeterminacy: 1) fluctuations—variations
in measurements on macroscopically identical systems that force us to interpret measured
values as the mean of a large number of measurements, and 2) quantum, that even on well-prepared
microscopic systems, repeated measurements do not produce the same outcomes. We review the
foundations of statistical mechanics required in this book, classical and quantum.
A.1 CLASSICAL ENSEMBLES: PROBABILITY DENSITY FUNCTIONS
A.1.1 Classical dynamics of many particle systems; Γ-space
The state of N interacting point particles in three spatial dimensions is specified at an instant of
time by a point (the system point) in the 6N-dimensional space (Γ-space) spanned by the canonical
variables1 (p1,...,p3N ; q1,...,q3N ). The time evolution of the system point is governed by 6N
coupled first-order differential equations, Hamilton’s equations of motion,2
p˙k = −∂H
∂qk
q˙k = ∂H
∂pk
, k = 1,..., 3N (A.1)
where H = H(p1,...,p3N ; q1,...,q3N ). The time history of the system is a trajectory in Γ￾space. Consider a phase-space function, A(p, q, t) ≡ A(p1(t),...,pn(t), q1(t),...,qn(t), t). Form
its total time derivative:
dA
dt = ∂A
∂t +
k
 ∂A
∂qk
q˙k +
∂A
∂pk
p˙k

= ∂A
∂t +
k
 ∂A
∂qk
∂H
∂pk
− ∂A
∂pk
∂H
∂qk

≡ ∂A
∂t + [A, H]
P , (A.2)
where the second line specifies the Poisson bracket3 of A with H. The Poisson bracket between
phase-space functions u(p, q) and v(p, q) is defined as
[u, v]
P ≡ 
k
 ∂u
∂qk
∂v
∂pk
− ∂u
∂pk
∂v
∂qk

. (A.3)
1Canonical variables are by definition those that satisfy Hamilton’s equations of motion.
2Canonical variables at time t evolve under the action of Hamilton’s equations to new canonical variables at time t+∆t
(a canonical transformation, see texts on classical mechanics or [5, p334]). Canonical coordinates thus stay canonical in their
evolution, with the Jacobian of canonical transformations equal to unity, an essential part of the proof of Liouville’s theorem;
see [5, p47]. The application of Liouville’s theorem to statistical mechanics is taken up in Chapter 4. 3We denote Poisson brackets with subscript P to avoid confusion with the commutator bracket of quantum mechanics.
DOI: 10.1201/9781003512295-A 189190 ■ Non-Equilibrium Statistical Mechanics
Poisson brackets have the algebraic property [A, B]
P = − [B,A]
P, implying that [A, A]
P = 0 for
any A. An immediate consequence is, if ∂H/∂t = 0 (appropriate to statistical mechanics), then
d
dt
H(p, q)=[H, H]
P = 0.
Thus the Hamiltonian is a constant of the motion (if ∂H/∂t = 0): The value of H(p, q) stays
constant in time when its arguments are replaced by solutions of Hamilton’s equations of motion.
A.1.2 The transition to statistical mechanics
Measurements on macroscopic systems represent time averages of fluctuating quantities. To cal￾culate time averages from first principles would require us to know the detailed, microscopic time
histories of interacting particles, a task beyond our computational reach for the practical reason of
limited computing resources and for the fundamental reason that we’re unable to control initial con￾ditions. The equilibrium state is one of maximum entropy.4 Numerous microstates are consistent
with the same macrostate. With each microstate specified by 6N initial conditions (and hence a
point in Γ-space), consider a collection of distinct microstates in Γ-space at the same time,5 (in￾dicated schematically in Fig. A.1). Because we lack the ability to control initial conditions, our
p
q
t
t + ∆t
Figure A.1 A collection of points in Γ-space at time t flows under Hamiltonian dynamics
to the collection at time t + ∆t such that the density of points remains fixed (Liouville’s
theorem). Axes p, q represent the 3N axes associated with pi, qi, i = 1,..., 3N.
knowledge of the dynamics is consistent with the motion of a swarm of equivalent system points
in Γ-space, equivalent in the sense of distinct microstates representing the same macrostate. By Li￾ouville’s theorem,6 any collection of points in Γ-space moves as an incompressible fluid (whether
representing equilibrium systems or not)[5, p55].
An ensemble is an abstract collection of macroscopically identical systems.7 Even though pre￾pared identically in meeting constraints, their system points in Γ-space are not identical. We can
ask for the probability that the system point of a randomly selected member of the ensemble lies in
the range (p, q) to (p + dp, q + dq) (where p and q denote (p1,...,p3N ) and (q1,...,q3N ), with
dp ≡ dp1 ··· dp3N and dq ≡ dq1 ··· dq3N ). Such a probability is proportional to dpdq, which we
can write
ρ(p, q)
dpdq
h3N ≡ ρ(p, q)dΓ, (A.4)
4That is, the maximum value entropy can have subject to prescribed values of state variables such as V and T[2, p37]. 5There is a fundamental restriction on how distinct two microstates must be so as not to be counted as the same. A point
in Γ-space cannot be specified with greater precision than to lie within a volume h3N ; see [2, p110]. 6Derived in 1838. Its significance for statistical mechanics was first recognized by Gibbs; see Gibbs[131, Chapter 1]. 7The ensemble concept is due to Gibbs[131, p5]: “Let us imagine a great number of independent systems, identical in
nature, but differing in phase, that is, in their condition with respect to configuration and velocity.”Statistical mechanics ■ 191
where h3N ensures that the phase-space probability density function8 ρ(p, q) is dimensionless,
which we require to be normalized to unity,

Γ
ρ(p, q)dΓ = 1. (A.5)
If we knew ρ(p, q), then for a phase-space function A(p, q), we could evaluate its ensemble average
⟨A⟩ ≡ 
Γ
ρ(p, q)A(p, q)dΓ. (A.6)
The working assumption of statistical mechanics is that experimentally measurable quantities cor￾respond to appropriate ensemble averages[5, p46].
The task of finding ρ(p, q) is enabled by Liouville’s theorem, another form of which9 is that
ρ(p, q) is a constant of the motion[5, p47]:
d
dt
ρ(p, q)=0. (A.7)
The function ρ(p, q) is certainly a phase-space function, and thus, combining Eqs. (A.2) and (A.7),
∂ρ
∂t + [ρ, H]P = 0. (A.8)
Equation (A.8) is Liouville’s equation, often written in the form of Eq. (4.5). Statistical mechanics
is the study of the solutions of Liouville’s equation.
Any constant of the motion solves Eq. (A.7); conversely any solution of Eq. (A.7) is a constant of
the motion. The most general solution of Liouville’s equation is therefore a function of the constants
of the motion.10 In thermal equilibrium ∂ρ/∂t = 0, and thus we seek solutions of
[ρ, H]P = 0. (A.9)
Standard statistical mechanics proceeds on the assumption that the Hamiltonian is the only relevant
constant of the motion, a procedure justified a posteriori through comparison of the predictions of
theory with experimental results. Thus, we assume that ρ is a function of the Hamiltonian:
ρ(p, q) = F(H(p, q)). (A.10)
It remains to determine F. All microscopic information about a system’s components and their
interactions enters through the Hamiltonian. The nature of F depends on the types of interactions
the systems comprising the ensemble allow with the environment[5, Chapter 4].
A.1.3 Canonical ensemble, closed systems
For systems of fixed NVT allowing energy exchanges with the environment (closed systems)—the
canonical ensemble—the equilibrium phase-space probability density function is found to be11
ρ(p, q) = 1
Z(β)
e−βH(p,q) = 1
N!Zcan(β)
e−βH(p,q)
, (A.11)
8The difference between probabilities and probability densities is reviewed in Appendix B. 9If the flow of ensemble points in Γ-space is incompressible, then the probability density is constant along streamlines. 10See the example on the ideal gas in Section 4.1. 11Deriving Eq. (A.11) is a major task in the development of statistical mechanics; see [5, Chapter 4]. Feynman[3, p1]
stated: “This fundamental law is the summit of statistical mechanics, and the entire subject is either the slide-down from
this summit, as the principle is applied to various cases, or the climb-up to where the fundamental law is derived . . . .” It’s
straightforward to show that [e−βH(p,q), H]P = 0 for any β.192 ■ Non-Equilibrium Statistical Mechanics
where β ≡ (kT)−1 and the canonical partition function
Zcan(N,V,T) ≡ 1
N!

dΓe−βH(p,q)
. (A.12)
The probability density can be written in an equivalent way as a function of energy,
ρ(E) = 1
N!Zcan
e−βEΩ(E), (A.13)
where Ω(E) (density of states function) is such that Ω(E)dE is the number of energy states in
[E,E + dE] and where
Zcan(N,V,T) = 1
N!

e−βEΩ(E)dE. (A.14)
The Boltzmann factor e−βE is a measure of the extent to which the energy states of the system
are populated at temperature T, with Z =  ∞
0 e−βEΩ(E)dE the total number of possible states
accessible to the system at temperature T. The probability P(E)dE = e−βEΩ(E)dE/Z is the ratio
of two numbers: the number of states available to the system at energy E when it has temperature
T, to the total number of states available to the system at temperature T. The connection between
statistical mechanics and thermodynamics12 is contained in the relation
Zcan(N,V,T)=e−βF (N,V,T)
, (A.15)
where F(N,V,T) = U − T S is the Helmholtz free energy function[5, p96].
Once one has obtained Zcan(N,V,T), the standard quantities of thermodynamics can be ob￾tained through partial differentiation, e.g.,
µ = −kT ∂
∂N ln Zcan




T ,V
P = kT ∂
∂V ln Zcan




T ,N
S = k ∂
∂T (T ln Zcan)




V,N
. (A.16)
Fluctuations in the canonical ensemble follow from second partial derivatives of Zcan, e.g.,
∂2 ln Zcan
∂β2 = ⟨(H − ⟨H⟩)
2
⟩ = kT2CV . (A.17)
A.1.4 Grand canonical ensemble, open systems
The grand canonical ensemble is based on systems of fixed µV T allowing particle exchanges with
the environment as well as energy (open systems), where the chemical potential µ is the energy
required to add another particle to the system at constant S, V or at constant T,P[2, p39]. The
ensemble average of a phase-space function A(p, q, N) is[5, p100]
⟨A⟩ = 1
ZG
∞
N=0
eβNµ
N!

ΓN
A(p, q, N)e−βHN (p,q)
dΓN = 1
ZG
∞
N=0
eβµN Zcan,N ⟨A⟩N , (A.18)
where the grand partition function
ZG(µ, V, T) = ∞
N=0
1
N!
eβµN 
ΓN
e−βHN (p,q)
dΓN = ∞
N=0
eβµN Zcan(N,V,T), (A.19)
12The laws of thermodynamics are reproduced by statistical mechanics; see [5, pp92–96].Statistical mechanics ■ 193
with HN (p, q) the Hamiltonian for an N-particle system. If these infinite summations worry your
inner mathematician, µ is usually a negative quantity.13 The grand partition function is related to a
thermodynamic potential, the grand potential Φ(T,V,µ) ≡ F − Nµ[5, p99],
ZG(T,V,µ)=e−βΦ(T ,V,µ)
. (A.20)
Once one has obtained ZG(µ, V, T), the other variables follow from partial differentiation:
N = kT ∂ ln ZG
∂µ




T ,V
P = kT ∂ ln ZG
∂V




T ,µ
S = k ∂
∂T (T ln ZG)




V,µ
. (A.21)
Fluctuations in the grand canonical ensemble are obtained from second derivatives of ZG[5, p101].
A.2 QUANTUM ENSEMBLES: PROBABILITY DENSITY OPERATORS
A.2.1 Quantum indeterminacy
The quantum nature of matter adds another source of indeterminacy. Even in well-defined quantum
systems, measurements produce a range of values and we must seek the mean of a number of
measurements.14 The state of a quantum system at time t is represented by a state vector, |ψ(t)⟩,
an element of an abstract Hilbert space, with every observable (measurable) quantity A represented
by a Hermitian operator, Aˆ. The expectation value ⟨A⟩ψ on systems in state |ψ⟩ is found from the
expression (when |ψ⟩ is normalized, ⟨ψ|ψ⟩ = 1),
⟨A⟩ψ =

dxψ∗(x)Aψˆ (x) ≡ ⟨ψ|Aψˆ ⟩. (A.22)
The act of measurement introduces an uncontrollable interaction with the system that modifies
what we seek to measure. This can be seen using the eigenfunctions of Aˆ, a set of functions {ϕn}∞
n=0
such that Aϕˆ n = λnϕn, where (because Aˆ is Hermitian) the λn are real numbers and the eigenfunc￾tions are orthonormal, ⟨ϕn|ϕm⟩ = δnm. Eigenfunctions of Hermitian operators are complete:
15 An
arbitrary square-integrable function can be represented by an infinite linear combination,
ψ(x, t) = ∞
n=0
an(t)ϕn(x), (A.23)
where the expansion coefficients an(t) = ⟨ϕn|ψ(t)⟩ carry the time dependence. That |ψ(t)⟩ remains
normalized in time16 implies
∞
n=0
|an(t)|
2 = 1. (A.24)
As one can show,
⟨ψ|Aψˆ ⟩ = ∞
n=0
|an(t)|
2λn. (A.25)
The expectation value of A for systems in state |ψ⟩ is therefore an average of the eigenvalues of
Aˆ over the probability distribution |an(t)|
2. We can only measure the eigenvalues of Aˆ. To what
extent, however, can quantum states be controlled? We must introduce a second kind of average, an
ensemble average over a system’s quantum states.
13A classical argument for µ < 0 is given in [2, p39]. At the quantum level, it’s required that µ ≤ 0 for bosons, whereas
for fermions there is no restriction on µ[5, p141]. Cases where µ ≳ 0 for fermions occur in high-density systems where the
effects of the Pauli exclusion principle restrict occupation numbers to N = 0, 1, obviating concerns about convergence. 14Classically, the trajectories of particles subject to the same forces with the same initial conditions are reproducibly the
same; in quantum mechanics the same measurements on identically prepared systems do not produce the same values. 15See for example [13, Chapter 2]. 16The evolution of |ψ(t)⟩ is unitary with ⟨ψ(t)|ψ(t)⟩ = ⟨ψ(0)|ψ(0)⟩. Probability is locally conserved: The Schrodinger ¨
equation implies a continuity equation, ∂|ψ(r, t)|
2/∂t + ∇ · J = 0, with J = ℏ/(2mi) (ψ∗∇ψ − ψ∇ψ∗).194 ■ Non-Equilibrium Statistical Mechanics
A.2.2 Many-particle wave functions
Statistical mechanics utilizes wave functions of many particle systems, and here we run into a new
piece of physics, the indistinguishability of identical particles. Wave functions of an assembly of
identical particles are either symmetric or antisymmetric under interchange of particles (see Ap￾pendix D of [5]). Under the interchange of particles at positions rj and rk (leaving all other particles
unchanged), the basic principles of quantum mechanics require that (for identical particles)
ψ(..., rj ,..., rk,...,t) = θψ(..., rk,..., rj ,...,t), (A.26)
where
θ =

+1 for bosons
−1. for fermions
In either case, |ψ(..., rk,..., rl,...,t)|
2 = |ψ(..., rl,..., rk,...,t)|
2
, implying the fundamental
result that identical particles can’t be labeled.17 To construct an N-particle wave function, we use
as basis functions, products of N single-particle wave functions,18
ψ(r1,..., rN , t) = 
m1
···
mN
c(m1,...,mN , t)ϕm1 (r1)ϕm2 (r2)··· ϕmN (rN ), (A.27)
where {ϕk}∞
k=0 is some complete set (whatever is convenient for the problem at hand), and the
c(m1,...,mN , t) are expansion coefficients. Equation (A.27) is to many-particle wave functions
what Eq. (A.23) is to single-particle wave functions. The expansion coefficients c(m1,...,mN , t)
“do the work” of providing exchange symmetries (because the basis functions do not), i.e.,
c(...,mk,...,ml,...,t) = θc(...,ml,...,mk,...,t). (A.28)
If ml = mk = m, antisymmetry requires that c(. . . , m, . . . , m, . . . , t)=0, the usual form of the
Pauli principle for fermions.
To avoid cumbersome notation, let’s write Eq. (A.27) (many-particle wave function) in the form
ψ(x, t) = 
r
cr(t)ϕr(x), (A.29)
so that r is a multi-index symbol and x denotes the collection of spatial coordinates (which drops
out of the theoretical description). The expectation value of Aˆ is (using two copies of Eq. (A.29))
⟨ψ(t)|Aψˆ (t)⟩ = 
r

s
c∗
r(t)cs(t)⟨ϕr|Aϕˆ s⟩ ≡ 
r

s
c∗
r(t)cs(t)Ars, (A.30)
where Ars = ⟨ϕr|Aϕˆ s⟩ are the matrix elements of Aˆ in the basis set used in Eq. (A.27).
A.2.3 The density operator
A quantum system known to be in state |ψ⟩ is said to be in a pure state. It’s almost always the case
that we have incomplete knowledge of the microscopic state of the system (a mixed state). And what
do we do in the face of incomplete knowledge? I hope you’re saying: “Resort to probability.” Let pi
denote the probability that a randomly selected member of the ensemble is in state |ψ(i)⟩, such that

i
pi = 1. (A.31)
17In that case, why base a theory on ill-defined quantities (wave functions involving particle positions)? The occupation
number formalism (reviewed in Appendix D of [5]) is devoid of coordinate labels. 18The functions {ϕn(r)} form a complete set on the space of position coordinates for a single particle. Products of such
functions form a complete set in the union of the domains of the spatial coordinates for each particle[72, p56].Statistical mechanics ■ 195
The wave function ψ(i)(x) of each possible state can be expressed as in Eq. (A.29):19
ψ(i)
(x) = 
r
c(i)
r ϕr(x). (A.32)
The quantum expectation value of Aˆ in state |ψ(i)⟩ is, from Eq. (A.30) (introducing new notation),
A(i) ≡ ⟨ψ(i)
|Aψˆ (i)
⟩ = 
r

s
c(i)∗ r c(i)
s Ars.
The ensemble average is therefore
⟨A⟩ ≡ 
i
piA(i) = 
i
pi

r

s
c(i)∗ r c(i)
s Ars = 
r

s

i
pic(i)∗ r c(i)
s

Ars. (A.33)
There are two averages: the quantum expectation value A(i)
of Aˆ in state |ψ(i)⟩ and an average over
the states |ψ(i)⟩ which occur with probabilities pi.
Define the density matrix ρ, with matrix elements (note the order of the indices)
ρsr ≡ 
i
pic(i)∗ r c(i)
s . (A.34)
Combining Eqs. (A.34) and (A.33), we have an expression for the ensemble average,
⟨A⟩ = 
r

s
ρsrArs = 
s
(ρA)ss = Tr ρA, (A.35)
where the trace is the sum of diagonal elements, Tr M ≡ 
s Mss. From Eq. (A.34), the trace of
the density matrix is unity,
Tr ρ ≡ 
s
ρss = 
s

i
pi|c(i)
s |
2 = 
i
pi

s
|c(i)
s |
2 = 
i
pi = 1, (A.36)
where we’ve used Eqs. (A.24) and (A.31).
New features appear in quantum statistical mechanics having no classical counterpart. Write Eq.
(A.35) separating the diagonal from the off-diagonal matrix elements,
⟨A⟩ = 
s
ρssAss +
s

r̸=s
ρsrArs. (A.37)
From Eq. (A.34), ρss = 
i pi|c
(i)
s |
2 ≥ 0, and from Eq. (A.36), 
s ρss = 1. The diagonal element
ρss is the probability of finding the system in state s. The off-diagonal elements have no definite
sign and cannot be interpreted as probabilities; they are without classical counterpart.20 If in a given
basis the density matrix is diagonal, the definition of ⟨A⟩ in Eq. (A.35) is the same as in the classical
theory. The diagonal character of a matrix is not basis independent, however.
Define the density operator (or the statistical operator)
ρˆ ≡ 
i
pi|ψ(i)
⟩⟨ψ(i)
|, (A.38)
so that its matrix elements are those of the density matrix ρsr (in the same basis), i.e., ⟨ϕs|ρˆ|ϕr⟩ = 
i pi⟨ϕs|ψ(i)
⟩⟨ψ(i)|ϕr⟩ = 
i pic
(i)
s c
(i)∗ r = ρsr. Using Eq. (A.35), we have
⟨A⟩ = 
sr
ρsrArs = 
sr
⟨ϕs|ρˆ|ϕr⟩⟨ϕr|Aˆ|ϕs⟩ = 
s
⟨ϕs|ρˆAˆ|ϕs⟩ = Tr ˆρAˆ = Tr Aˆρ, ˆ (A.39)
19The basis functions are a complete set; they can be used to represent arbitrary wave functions.
20The density matrix is positive definite, however; see [5, p123].196 ■ Non-Equilibrium Statistical Mechanics
where 
r |ϕr⟩⟨ϕr| = I (completeness relation in Dirac notation). The trace is basis indepen￾dent[13, p34] and thus we can associate the trace with the operator itself. The last equality reflects
the cyclic invariance of the trace, Tr AB = Tr BA[13, p36]. Note that
Tr ˆρ = 
s
⟨ϕs|ρˆ|ϕs⟩ = 
s

i
pi⟨ϕs|ψ(i)
⟩⟨ψ(i)
|ϕs⟩ = 
s

i
pic(i)
s c(i)∗ s = 1, (A.40)
where we’ve used Eq. (A.36).21 Equations (A.39) and (A.40) are the quantum analogs of Eqs. (A.6)
and (A.5). Thus, we’ve replaced the classical ensemble average of the phase-space function A(p, q),
⟨A⟩ = 
Γ ρ(p, q)A(p, q)dΓ, involving the probability density function ρ(p, q), with the trace of the
density operator ρˆ acting on the operator Aˆ representing the observable, ⟨A⟩ = Tr ˆρAˆ.
Generalizing ρˆ(t) to include time, it satisfies the von Neumann equation,
iℏ ∂
∂tρˆ(t) = 
H, ˆ ρˆ(t)

, (A.41)
where the square brackets denote the commutator [A, ˆ Bˆ] ≡ AˆBˆ − BˆAˆ, with Hˆ the Hamiltonian
operator. The von Neumann equation is the quantum analog of the Liouville equation, Eq. (A.8);
quantum statistical mechanics is the study of the solutions of Eq. (A.41). Deriving Eq. (A.41) is
straightforward. Take the partial time derivative of Eq. (A.38) (with the pi time independent) and use
the time-dependent Schrodinger equation (which generates the time dependence of state vectors): ¨
iℏ(∂/∂t)|ψ(i)⟩ = Hˆ |ψ(i)⟩ and −iℏ(∂/∂t)⟨ψ(i)| = ⟨ψ(i)|Hˆ .
In thermal equilibrium ∂ρ/∂t ˆ = 0, implying that ρˆ commutes with Hˆ in equilibrium. Com￾muting operators have a common set of eigenfunctions. Using the eigenfunctions of Hˆ as a basis,
Hϕˆ n = Enϕn (the energy representation), the density matrix is diagonal,
ρrs = ρrδrs. (A.42)
A.2.4 Canonical ensemble
Consider the TVN ensemble. In the energy representation the density matrix is diagonal, implying
the diagonal elements are the probability the system has energy En,
ρn = 1
Z(β)
gne−βEn , (A.43)
where, for discrete energies,
Z(β) = 
n
gne−βEn , (A.44)
with gn the degeneracy of the nth energy state, the number of linearly independent eigenfunctions
each having eigenvalue En. The density operator may be evaluated using Eq. (A.38),
ρˆ = 
n
ρn|ϕn⟩⟨ϕn| = 1
Z

n
gne−βEn |ϕn⟩⟨ϕn| = 1
Z e−βHˆ 
n
|ϕn⟩⟨ϕn| = 1
Z(β)
e−βHˆ
, (A.45)
where we’ve used e−βHˆ ϕn = e−βEn ϕn and completeness. The ensemble average of an observable
quantity A is therefore
⟨A⟩ = Tr 
ρˆAˆ

= 1
Z Tr e−βHˆ
Aˆ = 1
Z(β)

n
gne−βEn ⟨ϕn|Aϕˆ n⟩. (A.46)
Equation (A.46) is one of the more useful equations in quantum statistical mechanics.22
21The density operator is self-adjoint, ρˆ† = ˆρ, positive semi-definite, ⟨ψ|ρˆ|ψ⟩ ≥ 0, and Tr ˆρ = 1. 22It appears on page 1 of Feynman[3].Statistical mechanics ■ 197
A.2.5 Grand canonical ensemble
For ρˆ to be diagonal in the grand canonical ensemble (systems specified by TVµ), it must have a
representation in which Hˆ and the number operator Nˆ (eigenvalues 0, 1, 2,...; see Appendix D of
[5]) are diagonal, implying that ρˆ must commute with Hˆ and Nˆ. For this system,
ρˆ = 1
ZG
e−β(Hˆ −µNˆ)
, (A.47)
where
ZG = Tr e−β(Hˆ −µNˆ) = ∞
N=0
eβµN ZN (β) (A.48)
is the grand partition function, with ZN the canonical partition function for an N-particle system.
Ensemble averages are then given by
⟨A⟩ = 1
ZG
∞
N=0
eβµN ZN (β)⟨A⟩N , (A.49)
where ⟨A⟩N denotes the canonical ensemble average for an N-particle system.
A.3 ENTROPY
Entropy, a state variable discovered by Clausius was an unexpected development (as with any￾thing new) and providing a microscopic interpretation of this quantity became a task for statistical
physics. For isolated systems (microcanonical ensemble) we have the Boltzmann entropy formula
S = k ln W, with W the number of ways that macrostates are realized from microstates. Gibbs
developed another expression for closed systems (canonical ensemble), the Gibbs entropy SG,
SG ≡ −k

dΓρ(p, q) ln ρ(p, q) ≡ −k⟨ln ρ⟩, (A.50)
with ρ(p, q) the Γ-space probability density. The Gibbs formula reduces to the Boltzmann formula
in the microcanonical ensemble (see Section 4.6.3). Von Neumann generalized the Gibbs entropy
for quantum systems,23
SV ≡ −k Tr ρ ln ρ, (A.51)
where now ρ is the density matrix.24 The trace of a function of a matrix A, Tr f(A) = 
j f(λj ),
where {λj} are the eigenvalues of A.
25 Thus, for λj the eigenvalues of ρ,
SV = −k

j
λj ln λj . (A.52)
The von Neumann entropy vanishes for pure-state systems, e.g., λ1 = 1 and λj̸=1 = 0.
23Von Neumann’s 1927 work on entropy (in German) is referenced in [210]. The topic is developed in von Neumann’s
book on quantum mechanics[211, Chapter 5]. Today there are many books on quantum entropy; see for example [212]. 24The logarithm of a matrix is defined by a Taylor series. The expansion of the function ln(1 + x) around x = 0,
ln(1 + x) = x − 1
2
x2 +
1
3
x3 − 1
4
x4 + ··· =
∞
n=1
(−1)n+1
n xn,
is extended to a matrix A,
ln A = ln(A − I + I) ≡ ln(I + B) =∞
n=1
(−1)n+1
n Bn =
∞
n=1
(−1)n+1
n (A − I)
n,
where I is the identity matrix. Note that ln I = 0, the zero matrix. 25The formula Tr f(A) = 
j f(λj ) can be derived 1) by assuming f is an analytic function (possesses a power series)
and 2) by evaluating the trace in a basis of the normalized eigenfunctions of A, knowing that the trace is basis independent.198 ■ Non-Equilibrium Statistical Mechanics
The von Neumann entropy SV = SV (λ1, λ2,...) is a continuous function of the eigenvalues of
the density matrix. It therefore doesn’t have to be associated with systems in thermal equilibrium; it
applies to any density matrix, not necessarily that for equilibrium systems. A generalization of en￾tropy applicable to any distribution of probabilities {pi}N
i=1 was introduced by Shannon (mentioned
in Section 4.6.3; see [2, Chapter 12]) having the form of Eq. (A.52), −K N
i=1 pi ln pi (K is a posi￾tive constant), known as information or the Shannon entropy. The von Neumann entropy is therefore
the information-theoretic Shannon entropy of the spectrum of the density matrix. There are other
types of entropy associated with von Neumann entropy (relative entropy, conditional entropy, the list
is not small) in the framework of quantum information theory to characterize the entropy of quan￾tum entanglement (see for example Nielsen and Chuang[213]). The subject of quantum information
is rapidly developing.
A.4 THE WEYL CORRESPONDENCE PRINCIPLE
Statistical mechanics applies to systems governed by Hamiltonian dynamics whether in its classi￾cal or quantum formulation, with the roles played by classical and quantum mechanics appearing
similar. What the classical and quantum theories share is a common algebraic structure of Poisson
brackets and commutators; both are Lie algebras. In quantum mechanics, the canonical coordinates
q, p are associated with abstract linear operators q, ˆ pˆ acting on an abstract Hilbert space. These are
Hermitian and noncommuting, [ˆq, pˆ]=iℏI, with I the identity operator. The state of a quantum
system is represented by an element of the Hilbert space rather than as a point in Γ-space (which is
forbidden by the Heisenberg uncertainty principle). Dynamical functions are represented as opera￾tors on the Hilbert space instead of functions on Γ-space. Dirac[165, Section 21] found a connection
between the commutator of operators associated with physical observables [A, ˆ Bˆ] and the Poisson
brackets of their classical representations [A, B]P, [A, ˆ Bˆ] ↔ iℏ [A, B]
P I +O(ℏ2). This leaves open
the question of how we find the operator Aˆ associated with a given classical dynamical function
A(p, q).
Weyl developed a general rule for associating self-adjoint operators with classical dynamical
functions.26 The first step is to represent Γ-space functions as Fourier integrals,
A(p, q) =   a(α, β)ei(αp+βx)/ℏdαdβ, (A.53)
where α, β are real parameters with α having the dimension of length and β the dimension of
momentum.27 The Weyl correspondence defines Aˆ(ˆp, qˆ) through the extension of Eq. (A.53) under
p → pˆ, q → qˆ,
Aˆ(ˆp, qˆ) ≡
  a(α, β)ei(αpˆ+βqˆ)/ℏdαdβ. (A.54)
The reality of A(p, q) implies the self-adjointness of Aˆ(ˆp, qˆ).
28 Keep in mind that the Weyl prescrip￾tion is a postulate, the veracity of which is determined by comparison with experiment.
We therefore have constructed a self-adjoint operator Aˆ(ˆp, qˆ) from a weighted superposition
of unitary operators Eˆ(α, β) ≡ ei(αpˆ+βqˆ)/ℏ, where the weighting factor a(α, β) is the Fourier
transform of the classical function A(p, q). It behooves us therefore to understand the properties of
Eˆ(α, β). It’s readily seen to be unitary, Eˆ†(α, β) = Eˆ(−α, −β) = Eˆ−1(α, β). It can be shown
26See Weyl[214, p275] and also McCoy[215]. Curtright, Fairlie, and Zachos[216] is a modern reference. 27Planck’s constant has the dimension of action. For simplicity we work in one dimension. In actuality, α is an abbrevi￾ation for a set of transform variables (α1,...,αN ), dα ≡ dα1 ... dαN , and αp = 
i αipi; the same for β. 28The reality of A(p, q) implies a∗(α, β) = a(−α, −β) where α, β each extend from −∞ to ∞.Statistical mechanics ■ 199
using the Baker-Campbell-Hausdorff equation29 that Eˆ can be written in two ways:
Eˆ(α, β) = 
eiαβ/(2ℏ) · eiβq/ˆ ℏeiαp/ˆ ℏ (qp-ordered form)
e−iαβ/(2ℏ) · eiαp/ˆ ℏeiβq/ˆ ℏ. (pq-ordered form) (A.55)
Thus, eiβq/ˆ ℏeiαp/ˆ ℏ = e−iαβ/ℏeiαp/ˆ ℏeiβq/ˆ ℏ. Another way of writing Eˆ (in two versions) is
Eˆ(α, β)=eiβq/ˆ (2ℏ)
eiαp/ˆ ℏeiβq/ˆ (2ℏ) = eiαp/ˆ (2ℏ)
eiβq/ˆ ℏeiαp/ˆ (2ℏ)
. (A.56)
The trace of Eˆ has the form
Tr Eˆ(α, β)=2πℏδ(α)δ(β). (A.57)
To show this, rely momentarily on the coordinate representation, knowing that the trace is basis
independent. With pˆ = −iℏd/dx, exp[iαp/ˆ (2ℏ)] = exp[(α/2)d/dx], so that, for any function f(x),
exp[(α/2)d/dx]f(x) = f(x + α/2); exp[(α/2)d/dx] acting on an analytic function generates its
Taylor series. Assume a complete set of functions such that 
n ψ∗
n(x)ψn(y) = δ(x − y). Then,
using the second equality in Eq. (A.56),
Tr Eˆ = 
n
⟨n|Eˆ|n⟩ = 
n

dxψ∗
n(x) exp[(α/2)d/dx]eiβx/ℏ exp[(α/2)d/dx]ψn(x)
= 
n

dxψ∗
n(x)eiβ(x+α/2)/ℏψn(x + α) = δ(α)

dxeiβ(x+α/2)/ℏ = 2πℏδ(α)δ(β).
We also have the identities
Eˆ(α′
, β′
)Eˆ(α′′, β′′)=ei(α′
β′′−β′
α′′)/(2ℏ)
Eˆ(α′ + α′′, β′ + β′′)
= ei(α′
β′′−β′
α′′)/ℏEˆ(α′′, β′′)Eˆ(α′
, β′
). (A.58)
As a consequence of Eqs. (A.57) and (A.58), we have another trace property
Tr Eˆ(α′
, β′
)Eˆ†(α, β)=2πℏδ(α′ − α)δ(β′ − β). (A.59)
Equation (A.59) can be used to show that, starting from Eq. (A.54),
a(α, β) = 1
2πℏ Tr Aˆ(ˆp, qˆ)Eˆ†(α, β). (A.60)
Thus we have a way of inverting Eq. (A.54). Fourier analysis on functions A(p, q) implies Fourier
analysis on operators Aˆ(ˆp, qˆ). By combining Eq. (A.60) with Eq. (A.54), we have the identity,
Aˆ = 1
2πℏ
  dαdβ Tr 
AˆEˆ†(α, β)

Eˆ(α, β). (A.61)
Let’s construct the operator associated with the function A(p, q) = qp. The first task is to
find a(α, β). Starting from the inverse of Eq. (A.53), a(α, β) = 
1/(2πℏ)2   dpdqe−i(αp+βq)/ℏ
A(p, q), we must evaluate
a(α, β) = 1
(2πℏ)2
  pqe−i(αp+βq)/ℏdpdq.
29The Baker-Campbell-Hausdorff equation is often presented in quantum texts as a specialized version of a more general
formula, usually without derivation. The theorem states, for operators A, ˆ Bˆ, that eAˆ
eBˆ = eC(A, ˆ Bˆ) with C(A, ˆ Bˆ) =
Aˆ + Bˆ + 1
2 [A, ˆ Bˆ] + 1
12 [A, ˆ [A, ˆ Bˆ]] + 1
12 [[A, ˆ Bˆ], Bˆ] + ··· . See for example Veltman[217, Appendix A.4]. Only when
A, ˆ Bˆ commute with their commutator do we have the closed-form expression eAˆ
eBˆ = eAˆ+Bˆ+ 1
2 [A, ˆ Bˆ]
.200 ■ Non-Equilibrium Statistical Mechanics
Noting that pe−iαp/ℏ = iℏ(∂/∂α)e−iαp/ℏ and qe−iβq/ℏ = iℏ(∂/∂β)e−iβq/ℏ, we have
a(α, β) = 1
(2πℏ)2 (iℏ)
2
  dpdq
∂
∂α
∂
∂β e−i(αp+βq)/ℏ = − 1
4π2
∂
∂α
∂
∂β   dpdqe−i(αp+βq)/ℏ
= −ℏ2
 ∂
∂αδ(α)
 ∂
∂β δ(β)

≡ −ℏ2δ′
(α)δ′
(β),
where the derivative of the Dirac delta function,30 δ′
(α) ≡ dδ(α)/dα. We see the pattern now.
For A(p, q) = pnqm, (n, m) integers, we have with pn ↔ (iℏ∂/∂α)n and qm ↔ (iℏ∂/∂β)m
that a(α, β) = (iℏ)n+mδ(n)(α)δ(m)
(β) where δ(n)(α) ≡ dnδ(α)/dαn. The quantities a(α, β) are
therefore highly singular. The operator Aˆ associated with A(p, q) = qp is, from Eq. (A.54),
Aˆ = −ℏ2
  dαdβδ′
(β)δ′
(α)Eˆ(α, β) = −ℏ2
  dαdβδ(β)δ(α) ∂2
∂β∂αEˆ(α, β)
= −ℏ2
 ∂2
∂β∂αEˆ(α, β)




α=0,β=0
, (A.62)
where we’ve integrated by parts on α and β. Using the qp-ordered form of Eˆ in Eq. (A.55) we find
Aˆ = ˆqpˆ− i(ℏ/2)I, whereas with the pq-ordered form Aˆ = ˆpqˆ+ i(ℏ/2)I. The Weyl rule therefore
leads us to
A(p, q) = qp =⇒ Aˆ =

qˆpˆ− i
2 ℏI (qp-ordered form)
pˆqˆ+ i
2 ℏI. (pq-ordered form) (A.63)
The two are equivalent by the commutation relation [ˆq, pˆ]=iℏI. Under the Weyl correspondence
principle,
A(p, q) = qp =⇒
Weyl
Aˆ = 1
2 (ˆqpˆ+ ˆpqˆ). (A.64)
The Weyl procedure is used to construct Hermitian operators associated with products of non￾commuting operators. One might guess for example that the operator associated with the function
q2p is 1
2 (ˆq2pˆ+ ˆpqˆ2) or qˆpˆqˆ (both are Hermitian). The Weyl rule leads to 1
3

qˆ2pˆ+ ˆqpˆqˆ+ ˆpqˆ2
.
30Derivatives of the Dirac delta function of all orders are defined inside integrals; see Lighthill[99, p19].APPENDIX B
Probability theory
Areview of basic probability theory is given, one that presumes prior exposure to the subject.
More details can be found in Feller[218] or Parzen[38] or [5, Chapter 3].
B.1 EVENTS, SAMPLE SPACE, AND PROBABILITY
Event—a primitive concept in probability theory—denotes the outcome of an experiment that can be
repeated many times. The set of all possible outcomes of an experiment is known as sample space,
denoted Ω. Elementary events are represented by single points (elements) of Ω. Compound events
are represented by subsets of Ω having two or more elementary events. For example, the sample
space for the number of dots showing on the toss of a six-sided die is Ω = (1, 2, 3, 4, 5, 6). These
are elementary events. The event consisting of an even number of dots, i.e., the subset (2, 4, 6), is a
compound event. The entire sample space Ω is referred to as the certain event; the impossible event
(no elementary event) is denoted ∅. Many results in elementary probability theory follow from set
theory. For A ⊂ Ω and B ⊂ Ω, one can form their union A ∪ B (events belonging to A or B) and
their intersection A ∩ B (events belonging to A and B). Events A and B are mutually exclusive1 if
A ∩ B = ∅.
For sample spaces with denumerably many elementary events Ei, Ω ≡ (E1, E2,...,En,...),
a probability function P is defined as any function2 satisfying the Kolmogorov axioms,
1. P(Ei) ≥ 0 for every event Ei,
2. P(Ω) = 1 for the certain event Ω,
3. P(A ∪ B) = P(A) + P(B) if A ∩ B = ∅.
The edifice of probability theory follows as a logical consequence of these axioms. It can be shown
(from the axioms) that P(∅)=0 and that 0 ≤ P(E) ≤ 1 for any event E. Other familiar properties
are consequences of these axioms, such as the probability of the union of two events, a generalization
of the third axiom,
P(A ∪ B) = P(A) + P(B) − P(A ∩ B). (B.1)
The probability of A and B occurring, P(A ∩ B), is calculated using the conditional probability
P(B|A), the probability that B occurs given the occurrence of A. The quantity P(A∩B) is defined
P(A ∩ B) = P(A)P(B|A) = P(B)P(A|B). (B.2)
Events A and B are said to be statistically independent if P(A ∩ B) = P(A)P(B). Independent
events are such that P(B|A) = P(B) and P(A|B) = P(A).
1Elementary events are mutually exclusive.
2Functions assign numbers to each element of its domain. The domain of a probability function is the sample space.
DOI: 10.1201/9781003512295-B 201202 ■ Non-Equilibrium Statistical Mechanics
Although any function P satisfying the axioms constitutes a probability function, we have yet
to specify one. It’s often the case that elementary events occur with equal likelihood (as in the throw
of a fair die). Elementary events (E1,...,EN ) occurring with equal likelihood all have the same
probability,
P(E1) = P(E2) = ··· = P(EN ) = 1
N . (B.3)
For an event A ⊂ Ω consisting of NA elementary events, P(A) is the ratio of the number of sample
points associated with the occurrence of A to the number of points in the sample space, NΩ:
P(A) = NA
NΩ
. (B.4)
Equation (B.4) is the frequency interpretation of probability, formulated by Laplace in 1812. It
reflects the fact that in experiments with well-controlled conditions, the property under observation
does not always occur with the same value—so no deterministic regularity—but over the course of
many trials there is an empirically observed statistical regularity that A occurs a fraction of the time
given by P(A).
3 Probabilities computed using Eq. (B.4) satisfy the axioms, and thus they satisfy
any theorems that follow logically from the axioms. Equation (B.4) requires us to have the ability
to compute the size of sets. Counting denumerable collections of objects4 is a familiar activity, yet
there are specialized techniques for large sets. Combinatorics is a branch of mathematics devoted
to just that. The reader is presumed to have a proficiency with basic combinatorics: permutations,
combinations, binomial coefficients, and Stirling’s approximation.
B.2 RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS
One of the most useful concepts in probability theory is that of random variable. The sample space
for the throw of two coins could be described {HH, HT, TH, TT} (H for heads, T for tails). Or
it could be represented by points in the x-y plane, {(1, 1),(1, 0),(0, 1),(0, 0)}, where we assign H
the number 1 and T the number 0. How we display the sample space is immaterial, yet some ways
are better than others. We’d like to parameterize the sample space in a way that’s aligned with the
business of calculating averages.
Definition. A random variable X assigns a real number (a random number5) to each point of
sample space.6
As an example, let a random variable X represent the number of heads showing in the toss of
two coins with X = 0 for no heads (T T), 1 for one head (T H or HT), and 2 for two heads, HH.
Under such an assignment, the probabilities associated with the sample points are 1/4 for X = 0,
1/2 for X = 1, and 1/4 for X = 2. As another example, let X denote the sum of the dots showing
in the roll of two dice, with f(X) the probability associated with the value of X. Enumeration of
the possibilities shows that f(2) = f(12) = 1/36, f(3) = f(11) = 2/36, f(4) = f(10) = 3/36,
f(5) = f(9) = 4/36, f(6) = f(8) = 5/36 and f(7) = 6/36.
3A random phenomenon is empirically characterized by the property that its observation under controlled conditions
does not always result in the same value but rather different outcomes occur with statistical regularity. Randomness reflects
our inability to control microscopic initial conditions. 4Naive extensions of probability to nondenumerable sets lead to logical difficulties that are resolved only with more
advanced mathematics. The size of nondenumerable sets is the province of measure theory (beyond the level of this book).
See Lo`eve[39, Chapters 1–8] and Cram´er[31, Chapters 4–9] for introductions to measure theory. 5The word random is used because the elements of the sample space are associated with physical experiments in which
the outcome of any one experiment is unpredictable and associated with randomness. 6To be more mathematically precise, random variables are real-valued functions on sample space, X : Ω → S, where
S is the state space, the space of measured values of X, a subset of R. Probabilities are also functions on Ω, but the mapping
is onto [0, 1]. The values of random variables inherit the ordering of points on the real line.Probability theory ■ 203
B.2.1 Probability distributions on discrete sample spaces; joint distributions
The set of probabilities associated with the range of a random variable is a probability distribution.
7
For each value xj of a random variable X, the aggregate of sample points associated with xj form
the event having probability P(X = xj ).
Definition. A function f(x) such that for each xj , f(xj ) = P(X = xj ), is the probability distri￾bution of X.
For the range of X, f(xj ) ≥ 0 and 
j f(xj )=1.
One can have more than one random variable on the same sample space. Consider random vari￾ables X and Y having the values x1, x2,... and y1, y2,... , and let the corresponding probability
distributions be f(xj ) and g(yk). The aggregate of events for which X = xj and Y = yk form the
event having probability P(X = xj , Y = yk).
Definition. A function p(x, y) such that, for each xj and yk, p(xj , yk) = P(X = xj , Y = yk), is
the joint probability distribution of X and Y.
Clearly, p(xj , yk) ≥ 0 and 
jk p(xj , yk)=1. Moreover, for fixed xj

k
p(xj , yk) = f(xj ), (B.5)
while, for fixed yk, 
j
p(xj , yk) = g(yk). (B.6)
Adding the probabilities for all events yk for fixed xj produces the probability distribution for xj
and adding the probabilities for all events xj produces the probability distribution for yk. This
idea generalizes to n random variables X1, X2,...Xn, such that p(xj1, xj2,...,xjn) = P(X1 =
xj1, X2 = xj2,...,Xn = xjn).
Definition. If the joint probability distribution f(x1, x2,...,xn) can be factored in the form
f(x1, x2,...,xn) = f(x1)f(x2)··· f(xn), where f(xi) is the probability distribution of Xi, then
the random variables X1, X2,...,Xn are statistically independent.
B.2.2 Probability densities on continuous sample spaces; joint densities
A wide class of experiments involve continuous sample spaces (those not characterized by isolated
sample points), on which continuous random variables are defined. The probability distribution of
a continuous random variable is called a probability density f(x), of which f(x)dx represents the
probability that its value lies between x and x + dx. No measurement of a continuous quantity is
ever perfectly precise;8 one can only specify a probability that the value of a continuous random
variable lies within a window [x, x + dx].
Definition. A probability density is a function f(x) such that:
f(x) ≥ 0
 ∞
−∞
f(x)dx = 1  b
a
f(x)dx = P(a<x<b). (B.7)
7The term probability distribution can be ambiguous. We take it to be a function f(x) defined on the range of X such that
f(xj ) = P(X = xj ). It can also mean the cumulative probability up to and including the value x, F(x) ≡ 
xi≤x f(xi).
We use the term exclusively in the first sense. 8How tall are you, exactly? Whatever your answer, it can only lie within experimental uncertainties. You can only specify
a continuous quantity within so many decimal places of accuracy.204 ■ Non-Equilibrium Statistical Mechanics
A probability density for several continuous random variables is a straightforward generalization.
Definition. A joint probability density is a function f(x1,...,xn) having the properties:
f(x1,...,xn) ≥ 0
 ∞
−∞
···  ∞
−∞
f(x1,...,xn)dx1 ··· dxn = 1
 bn
an
···  b1
a1
f(x1,...,xn)dx1 ··· dxn = P(a1 < x1 < b1,...,an < xn < bn). (B.8)
B.2.3 Moments of distributions
Definition. The kth moment about the origin of a probability distribution f(x) is found from
µ′
k ≡ 
j
(xj )
kf(x), (k = 0, 1, 2,...) (B.9)
where the prime indicates that the moment is defined about the origin of the range of the random
variable X.
Moments characterize the shape of probability distributions.9 It can happen that the sum in Eq. (B.9)
fails to exist for some value k = r. When the rth moment exists, all moments for k ≤ r exist. The
moment µ′
0 = 1 is the normalization of the distribution.
Definition. The moment associated with k = 1 is the average, or the mean, or the expectation
value, indicated with a variety of notations,
µ′
1 = x = ⟨x⟩ ≡ 
j
xjf(xj ). (B.10)
Equation (B.10) generalizes to functions of random variables, because a function ϕ(X) of a random
variable X is a new random variable having values ϕ(X = xj ) = ϕ(xj ). Thus,
ϕ = ⟨ϕ⟩ = 
j
ϕ(xj )f(xj ). (B.11)
Definition. The kth moment about the mean is defined
µk ≡ 
j
(xj − x)
k f(xj ). (B.12)
Clearly µ1 = 0. The moments µ′
1 and √µ2 are given special symbols: µ ≡ µ′
1 and σ ≡ √µ2. The
quantity µ2 is known as the variance of the distribution, with σ the standard deviation. Equations
(B.10) and (B.11) generalize to continuous random variables where sums are replaced by integrals,
µ′
k =

xkf(x)dx ϕ = ⟨ϕ⟩ =

ϕ(x)f(x)dx. (B.13)
This concludes our review of elementary probability theory (which largely suffices for statistical
mechanics). Additional tools from probability theory required in nonequilibrium statistical physics
are developed in Chapter 2.
9We have defined moments as moments of the probability distribution; moments are also referred to as moments of
random variables.APPENDIX C
Elastic collisions
T HE theory of two-body elastic collisions (see for example Landau and Lifshitz[219]), essential
in formulating the Boltzmann equation (see Chapter 4), is reviewed in this appendix.
Figure C.1 shows two masses m1, m2, located by position vectors r1, r2 relative to origin O,
m2
r2 m1
r1 O
r = r2 − r1
R
Figure C.1 Particles 1, 2 are located by position vectors r1, r2. Center of mass is located
by R.
the laboratory reference frame. The vector R ≡ (m1r1 + m2r2)/(m1 + m2) locates a point (the
center of mass) on the line joining the masses, the separation vector r ≡ r2 − r1. As one can
show, r1 = R − [m2/(m1 + m2)]r and r2 = R + [m1/(m1 + m2)]r; particles can be located
either with r1, r2 or R, r. Now let them have velocities v1, v2 in the lab frame. The particles have
canonical coordinates (p1, r1),(p2, r2), where p1 = m1v1, p2 = m2v2. By differentiating the
formula for R, we have MR˙ = m1r˙1 + m2r˙2 = p1 + p2, where M ≡ m1 + m2. We can
therefore associate with the center of mass a momentum P ≡ MR˙ equal to the total momentum,
P = p1 + p2. With the reduced mass1 µ ≡ m1m2/M we can associate a momentum with the
relative separation, p ≡ µr˙ = (m1p2 − m2p1)/M. With these definitions, p1 = (m1/M)P − p
and p2 = (m2/M)P +p; momenta can be described either with p1, p2 or P , p. The transformation
from (p1, r1, p2, r2) to (P , R, p, r) is measure preserving (has unit Jacobian).
Assume particles interact through a central potential energy function V (|r2 − r1|) = V (|r|) ≡
V (r) that has a repulsive core.2,3 The Hamiltonian is, in either set of variables,
H(p1, r1, p2, r2) = 1
2m1
p2
1+
1
2m2
p2
2+V (r) = 1
2M
P2+
1
2µ
p2+V (r) = H(P , R, p, r). (C.1)
The kinetic energy is thus the sum of the kinetic energy of the center of mass and that of a particle
of mass µ, and, because R is a cyclic coordinate, P is conserved.4 Thus, P2/(2M) is a constant
1Note that µ = (m2/M)m1 = (m1/M)m2; µ is smaller than either of m1, m2. If m1 = m2 = m, µ = 1
2m. 2The Lennard-Jones potential for example has a repulsive core. See Fig. 6.1 in [5]. 3We consider elastic collisions which don’t involve any change in the internal state of particles. 4These are instances of general theorems in classical mechanics; see Goldstein[220, Chapter 1].
DOI: 10.1201/9781003512295-C 205206 ■ Non-Equilibrium Statistical Mechanics
and can be dropped from the Hamiltonian. The motion of two interacting particles in a central force
field reduces to the dynamics of a single particle of mass µ subject to the same force.
Figure C.2 depicts the motion of two interacting particles in the lab frame. The dashed line
m1
m1
m2
m2
r
r′
interaction
region
path of
center of mass
r
θ
time →
Figure C.2 Motion of interacting particles in the lab frame (not drawn to scale).
indicates the path of the center of mass, which moves with constant velocity. Note that r, the relative
separation vector before the interaction, is rotated into that after the interaction, r′
. Primed variables
denote after-collision values. Figure C.3 shows the same collision in the center-of-mass reference
frame (R = 0). The rotation angle θ in Fig. C.3 is the same as that in Fig. C.2.
m1
m2
r
m1
m2
r′ θ
Figure C.3 Motion of interacting particles in the center-of-mass frame.
There are three regimes associated with scattering: before, during, and after the interaction. Be￾fore and after are provided meaning by potentials having a finite range r0 so that V (r>r0)=0,
indicated in Fig. C.2 with the hatched circle, the region of interaction. Particles outside the inter￾action region are free and move with constant velocities. We will use Fig. C.4 to depict two-body
p′
2
p′
1
p2
p1
Figure C.4 Schematic illustration of a two-body collision in the lab frame.
collisions in the lab frame when the detail shown in Fig. C.2 is not required. The before and after￾collision variables pertain to free particles, implying the energy and momentum conservation laws
p1 + p2 = p′
1 + p′
2
1
2m1
p2
1 +
1
2m2
p2
2 = 1
2m1
p′2
1 +
1
2m2
p′2
2 . (C.2)Elastic collisions ■ 207
Angular momentum is conserved (central forces). For given p1, p2, a range of possible momenta
p′
1, p′
2 exist satisfying the conservation laws (which are kinematic and apply regardless of the inter￾action potential). That is, conservation laws don’t uniquely determine the after-collision momenta;5
knowledge of the force law is required. One can’t for example predict the scattering angle on the
basis of conservation laws alone. Let’s show that.
As we’ve seen, two-body motion in a central force field reduces to the dynamics of a fictitious
particle of mass µ located by the vector r with H = p2/(2µ) + V (r). Central-force motion is
planar,6 implying two generalized coordinates are needed to specify r. Choose polar coordinates
(r, ϕ) where the angle ϕ is relative to any line in the plane. The Hamiltonian is
H = 1
2µ
p2
r +
1
2µr2 p2
ϕ + V (r).
Angular momentum is conserved, set pϕ = L; H is a constant of the motion, set H = E. Thus,
with pr = µr˙,
E = µ
2
dr
dt
2
+
L2
2µr2 + V (r) =⇒
dr
dt = ±
 2
µ[E − L2/(2µr2) − V (r)].
The particle µ moves in the effective potential Veff(r) ≡ V (r)+L2/(2µr2). The differential relation
dϕ/dr = ϕ/˙ r˙ = L/(µr2r˙) can be integrated to provide the orbit equation,
ϕ(r) = L
√2µ
 dr
r2
E − V (r) − L2/(2µr2)
+ constant. (C.3)
Thus, to calculate ϕ one must know V (r), E, and L. Note that our analysis applies to scattering or
unbound orbits. Assuming limr→∞ V (r)=0, unbound trajectories occur for E > 0.
Figure C.5 shows scattering in the frame of the separation vector r, i.e., in the reference frame
rbefore
r′
after
r(t)
θ
b
b
g′
rmin
apsidal line
ψ
ψ
g
µ
time →
Figure C.5 Scattering in the frame of the separation vector r (particle 1 in Fig. C.1).
of particle 1. For t → −∞, r(t) → rbefore and as t → ∞, r(t) → r′
after. The particle of mass
µ approaches the scattering center with velocity g ≡ r˙ = v2 − v1, displaced from rbefore by the
impact parameter b. The angular momentum L = r × p has magnitude L = µbg (b is the lever
arm associated with the cross product). By energy conservation, E = µg2/2 = µg′2/2, and hence
g′ = g. The relative speed is not a constant of the motion; g′ takes the value g after the collision.
Angular momentum conservation implies b′ = b. The impact parameter b and the speed g are
5For given p1, p2, the unknowns p′
1, p′
2 in Eq. (C.2) imply six scalar quantities constrained by four equations. In d
dimensions, 2d scalar quantities are constrained by d + 1 equations. Only for d = 1 (head-on collisions) does d +1=2d. 6The vectors r and p are each orthogonal to L = r × p and define a plane. L is a constant vector under central forces.208 ■ Non-Equilibrium Statistical Mechanics
therefore intrinsic properties of a collision. At the point of closest approach, the apsis of the orbit,
r˙ = 0; rmin is found from the solution to
V (rmin) + L2/(2µr2
min) = E. (C.4)
The trajectory is symmetric about the apsidal line (a consequence of angular momentum conserva￾tion). The apsidal angle ψ can be calculated using Eq. (C.3) (for r >˙ 0),
ψ = L
√2µ
 ∞
rmin
dr
r2
E − V (r) − L2/(2µr2)
. (C.5)
The scattering angle θ (between incoming and outgoing velocities) is θ = π − 2ψ.
The scattering angle is thus determined by g, b, and V (r). In experiments, scatterers are exposed
to a beam of particles of the same speed but of varying impact parameters. Figure C.6 illustrates
θ b
db
dθ
Figure C.6 Variation of impact parameter db results in variation of scattering angle dθ.
how a variation db in impact parameter about b leads to an angular variation dθ about θ. A steady
particle beam of intensity I has I particles per unit time per unit area. The rate at which particles
are scattered into differential solid angle dΩ is proportional to I and dΩ. The proportionality factor
is termed the differential scattering cross section, σ(Ω), which has the dimension of area. Thus,
Iσ(Ω)dΩ = number of particles deflected into dΩ per second.
Because the scattering trajectories are planar, there is rotational symmetry about rbefore and dΩ ≡
sin θdθdϕ (ϕ is the azimuth angle) can be replaced with 2π sin θdθ. The rate at which particles are
scattered into 2π sin θdθ is the same as those arriving at the annulus of area 2πbdb. Thus
I · 2πbdb = I · σ(θ)2π sin θdθ,
implying
σ(θ) = b(θ)
sin θ




db
dθ



 . (C.6)
The functional relation between b and θ is established by θ = π − 2ψ, with ψ determined from Eq.
(C.5) with L = µbg and E = µg2/2. Once b = b(θ) is known for a given potential, the differential
scattering cross section is found from Eq. (C.6). The total scattering cross section
σt ≡ 2π
 π
0
σ(θ) sin θdθ (C.7)
is the effective area of the target particle for producing a scattering event.APPENDIX D
Integral equations and
resolvents
Aknowledge of integral equations is crucial to finding solutions of the Boltzmann equation (see
Section 4.8). Resolvents, a topic pertaining to linear operators and used in nonequilibrium
statistical mechanics (see Section 6.5), have a connection with integral equations. We review both.
Linear integral equations have the general form (see for example [13, Chapter 10]),
βy(x) − λ
 b
a
K(x, z)y(z)dz = f(x), (D.1)
where y(x) and f(x) are continuous functions, λ and β are constants, and K(x, z) is the kernel of
the integral equation, a function continuous in both variables. Equation (D.1) specifies a transfor￾mation of a continuous function y(x) into another continuous function f(x). The problem is to find
y(x) for given f(x) and K(x, z).
1
Integral equations are classified in several ways.
• In Fredholm equations the limits of integration in Eq. (D.1) are constants. In Volterra equa￾tions the limits are functions of x.
• If β = 0, Eq. (D.1) is an integral equation of the first kind; if β = 1, it’s an integral equation
of the second kind.
• If f(x)=0, Eq. (D.1) is homogeneous; otherwise it’s inhomogeneous.
Values of λ for which homogeneous integral equations y = λ
 b
a Ky have nontrivial solutions are
eigenvalues of K with the solutions y(x) eigenfunctions. Every eigenvalue has finite multiplicity r,
the number of linearly independent eigenfunctions[72, p113].
The Fredholm theorems are the basic theorems of integral equations; see [72, Chapter 3] or
Smithies[221]. In the following, set β = 1 in Eq. (D.1).
1. Either the inhomogeneous equation (D.1) has a unique solution y(x) for any function f(x)
(when λ is a regular value of K, not an eigenvalue), or the associated homogeneous equation
ψ(x) = λ
 b
a
K(x, z)ψ(z)dz (D.2)
has r ≥ 1 linearly independent solutions ψ1,...,ψr, i.e., λ is an eigenvalue.
1Equation (D.1) is a linear transformation in that corresponding to the superposition c1y1(x)+c2y2(x) is the analogous
combination c1f1(x) + c2f2(x).
DOI: 10.1201/9781003512295-D 209210 ■ Non-Equilibrium Statistical Mechanics
2. If λ is not an eigenvalue associated with Eq. (D.1) (the first alternative—this is known as the
Fredholm alternative theorem), λ is also not an eigenvalue of the “transposed” equation (swap
x, z in K),
g(x) = ϕ(x) − λ
 b
a
K(z, x)ϕ(z)dz, (D.3)
which has a unique solution ϕ for every g. If λ is an eigenvalue (the second alternative) then
it’s also an eigenvalue of the transposed homogeneous equation
χ(x) = λ
 b
a
K(z, x)χ(z)dz (D.4)
with r ≥ 1 linearly independent solutions χ1,...,χr. The latter is the analog of a matrix and
its transpose having the same eigenvalues.
3. If λ = λi is an eigenvalue of K, the inhomogeneous equation (D.1) has a solution if and only
if f(x) is orthogonal to the eigenfunctions of the transposed kernel,
 b
a
f(x)χi(x)dx = 0. (i = 1,...,r) (D.5)
This follows if we multiply Eq. (D.1) by χi(x) and integrate over x. In this case the solution
of Eq. (D.1) is determined only up to linear combinations of eigenfunctions, r
i=1 ciψi. The
constants are uniquely determined by the additional requirements  b
a y(x)ψi(x)dx = 0, i =
1,...,r. The Fredholm theorems mirror similar theorems in linear algebra[72, p6].
These theorems are used in finding solutions of the Boltzmann equation.
A kernel is symmetric if K(x, z) = K(z, x); Hermitian if K(x, z) = K∗(z, x). The same
theorems hold for Hermitian kernels as for operators: Real eigenvalues and orthogonal eigenfunc￾tions associated with distinct eigenvalues; see [13, p299]. For degenerate eigenvalues, the associated
eigenfunctions can be orthogonalized with the Gram-Schmidt method.
A key result is that functions g(x) generated by the integral transform of y(x) with Hermitian
kernel,
g(x) =  b
a
K(x, z)y(z)dz, (D.6)
can be expanded in the eigenfunctions ψn(x) of K [ψn(x) = λn
 b
 a K(x, z)ψn(z)dz], g(x) = ∞
n=1 anψn(x) with an expansion coefficients[72, p136]. Inhomogeneous integral equations can
always be written in the form of integral transforms,
g(x) ≡ y(x) − f(x) = λ
 b
a
K(x, z)y(z)dz. (D.7)
The solution of Eq. (D.7) can then be expressed
y(x) = f(x) + λ
∞
i=1
⟨ψi|f⟩
λi − λψi(x), (D.8)
where ⟨yi|f⟩ ≡  b
a y∗
i (x)f(x)dx. The steps leading to Eq. (D.8) are shown in [13, p300].
For inhomogeneous integral equations of the second kind, which we write in the form
y(x) = f(x) + λ
 b
a
K(x, z)y(z)dz, (D.9)Integral equations and resolvents ■ 211
an iterative solution strategy presents itself:
y(x) = f(x) + λ
 b
a
K(x, z)

f(z) + λ
 b
a
K(z, z′
)y(z′
)dz′

dz
= f(x) + λ
 b
a
K(x, z)f(z)dz + λ2
 b
a
 b
a
K(x, z)K(z, z′
)y(z′
)dz′
dz. (D.10)
Equation (D.10) is exact, but it suggests an approximation scheme, the Neumann series, with yn(x)
the nth-approximant,
yn(x) = f(x) +n
p=1
λp
 b
a
Kp(x, z)f(z)dz, (D.11)
where Kp is defined recursively (K0 ≡ 1),
Ki(x, z) ≡
 b
a
K(x, z′
)Ki−1(z′
, z)dz′
. (i = 1, 2,...) (D.12)
As n → ∞, Eq. (D.11) becomes an infinite series, which if convergent is the solution of second-kind
integral equations, y(x) = limn→∞ yn(x). The Neumann series converges if[72, p141]
(b − a) max |K(x, y)| <
1
|λ|
. (D.13)
The solution to Eq. (D.9) can be written (for sufficiently small λ, summing the series),
y(x) = f(x) + λ
 b
a
R(x, z; λ)f(z)dz, (D.14)
where
R(x, z; λ) ≡ ∞
m=0
λmKm+1(x, z) (D.15)
is known as the resolvent kernel. Using Eq. (D.8), the resolvent associated with a Hermitian kernel
can be written
R(x, z; λ) = ∞
i=1
ψi(x)ψ∗
i (z)
λi − λ . (D.16)
Equation (D.16) formally resembles the eigenfunction expansion of the Green function associated
with self-adjoint inhomogeneous differential equations.2 Equation (D.16), although derived for suf￾ficiently small |λ|, provides for an analytic continuation into the complex λ-plane with the eigen￾values λi appearing as simple poles. The resolvent is a meromorphic function of λ.
Equation (D.13) can be replaced by a less restrictive condition,[209, p147]
 b
a
 b
a
|K(x, y)|
2dxdy <
1
λ2 . (D.17)
Kernels are therefore square integrable and belong to a Hilbert space associated with the domain of
integration a ≤ x ≤ b, a ≤ y ≤ b. We could come up with a special notation for L2 defined on a
two-dimensional integration domain, but we’d soon dispense with it. The integral  b
a |K(x, y)|
2dy
exists for almost all x (Fubini’s theorem[209, p84]) and thus k(x) ≡ [
 b
a |K(x, y)|
2dy]
1/2 is an
2See [13, p258]. The resemblance shouldn’t be taken literally, however. Eigenvalues in the theory of integral equations
correspond to the inverse of eigenvalues in linear algebra and Sturm-Liouville theory. There is a connection between Green
functions and kernels of integral equations[13, p287].212 ■ Non-Equilibrium Statistical Mechanics
element of L2 defined in the usual way as a space of square-integrable functions of a single variable
x for a ≤ x ≤ b. Let h(x) be a member of L2. The integral  b
a K(x, y)h(y)dy has meaning for all
x where k(x) is well defined and defines a function belonging to L2. Thus, kernels generate linear
transformations on Hilbert space, K : L2 → L2, Kh → g, an abbreviation for Kh(x) = g(x) ≡  b
a K(x, y)h(y)dy. The kernel is an integral operator ([13, p17] or Stone[222, p99]) and can be
treated on the same footing as differential operators or matrices.3
Consider the inhomogeneous equation
T y − ly = g, (D.18)
where T is a linear operator, l is a given complex number, and g is a given element of Hilbert space;
the task is to find y. Equation (D.18) becomes Eq. (D.9) under the substitutions g = −f /λ, l = λ−1,
and T = K. For Eq. (D.18) to have a solution y for every g, l must not equal an eigenvalue of T. In
that case, the inverse (T − lI)−1 exists and y = (T − lI)−1g. The operator-valued function
R(l; T) ≡ (T − lI)
−1 (D.19)
is called the resolvent4 of T. The resolvent provides a rigorous way to characterize the spectrum of T
through the analytic structure of R(l; T). The set of complex numbers different from any eigenvalue
of T is called the resolvent set of T, denoted ρ(T); the resolvent is well-defined for l ∈ ρ(T). The
set of points in the complex l-plane such that T − lI has no inverse is called the point spectrum of
T.
5 Kato[223] is a useful reference for the properties of resolvents.
We note two identities, the resolvent equations.
1. For comparing the resolvents of the same operator T, for l1, l2 ∈ ρ(T),
R(l1; T) − R(l2; T)=(l1 − l2)R(l1; T)R(l2; T). (D.20)
Thus, R(l1; T), R(l2; T) commute. Equation (D.20) follows from a one-line proof:
R(l1) − R(l2) = R(l1)R−1(l2)R(l2) − R(l1)R−1(l1)R(l2).
2. For comparing the resolvents of two distinct operators A and B, for l ∈ ρ(A) ∩ ρ(B),
R(l; A) − R(l; B) = R(l; A)(B − A)R(l; B). (D.21)
Equation (D.21) follows in one line:
R(l; A) − R(l; B) = R(l; A)

R−1(l; B) − R−1(l; A)

R(l; B).
Equation (D.21) is used in Chapter 6.
3Matrix operations in Hilbert space in principle require infinite matrices.
4In the theory of integral equations, λK(I −λK)−1 is also called the resolvent of K; the two definitions have the same
analytic structure. The term resolvent was coined by Hilbert. 5There are two other classifications of spectrum based on the properties of the resolvent that need not concern us here:
the continuous spectrum of T and the residual spectrum of T; there are no other possibilities (besides the resolvent set and
the point spectrum)[222, p129].APPENDIX E
Dynamical representations
in quantum mechanics
I
T’S usually taught in one’s first exposure to the subject that quantum systems evolve in time
through the time dependence of state vectors |ψ⟩, elements of a Hilbert space H, with observ￾ables represented by time-independent Hermitian operators1 A defined on H. Expectation values of
A for systems in state |ψ⟩ are treated as inner products on H, ⟨ψ|Aψ⟩. Inner products, however, are
invariant under unitary transformations[13, p33], ⟨ψ|ϕ⟩ = ⟨Uψ|Uϕ⟩ with U†U = I. One has lee￾way therefore in introducing unitary transformations on vectors and operators so long as expectation
values are preserved. Such transformations are referred to as representations in quantum mechanics,
the subject of this appendix.2
E.1 SCHRODINGER REPRESENTATION ¨
We start with the Schrodinger representation ¨ , where the system state at time t is represented by the
vector |ψS(t)⟩, the time dependence of which is governed by the Schrodinger equation ¨
iℏ ∂
∂t|ψS(t)⟩ = HS|ψS(t)⟩, (E.1)
where HS is the Hamiltonian operator, with the subscript S indicating the Schrodinger form of these ¨
quantities. The expectation value at time t of an observable represented by operator AS for a system
in state |ψS(t)⟩ is given by
⟨A⟩t = ⟨ψS(t)|AS|ψS(t)⟩. (E.2)
The time rate of change of ⟨A⟩t can be found from:
iℏ ∂
∂t⟨A⟩t =

iℏ ∂
∂t⟨ψS(t)|

AS|ψS(t)⟩ + ⟨ψS(t)|AS iℏ ∂
∂t|ψS(t)⟩
= −⟨ψS(t)|HSAS|ψS(t)⟩ + ⟨ψS(t)|ASHS|ψS(t)⟩ = ⟨ψS(t)|[AS, HS]|ψS(t)⟩, (E.3)
where we’ve used the rules of Hermitian conjugation on Eq. (E.1), −iℏ(∂/∂t)⟨ψS(t)| =
⟨ψS(t)|H†
S = ⟨ψ(t)|HS (self-adjoint), and the commutator [A, B] ≡ AB − BA.
1We dispense with the “hat” notation for operators.
2Representation is an unfortunate choice of word, as it also connotes the representation of vectors or operators in a
particular basis or representations of abstract groups. The term dynamical picture is often used instead[224, p312–314].
Messiah[225, p312] refers to dynamical representations in quotation marks: “representation.”
DOI: 10.1201/9781003512295-E 213214 ■ Non-Equilibrium Statistical Mechanics
A natural way to represent the dynamics of state vectors is with a linear operator, the time
evolution operator, U(t, t0) that maps the state at time t0 to that at time t, |ψS(t0)⟩→|ψS(t)⟩:
|ψS(t)⟩ ≡ U(t, t0)|ψS(t0)⟩. (E.4)
To construct U(t, t0), we note that it must preserve normalization,3
⟨ψS(t)|ψS(t)⟩ = ⟨ψS(t0)|U†(t, t0)U(t, t0)|ψS(t0)⟩ = ⟨ψS(t0)|ψS(t0)⟩, (E.5)
i.e., it must be unitary, U†U = I. This requirement implies U(t0, t0) = I and that it has the
composition property U(t, t0) = U(t, t1)U(t1, t0): The evolution from t0 to t may be viewed as
a two-step process,4 the evolution from t0 to t1 followed by that from t1 to t. Most importantly, it
must be consistent with the dynamics of |ψS(t)⟩ as obtained from the Schrodinger equation, ¨
iℏ ∂
∂t|ψS(t)⟩ = iℏ ∂
∂tU(t, t0)|ψS(t0)⟩ = HS|ψS(t)⟩ = HSU(t, t0)|ψS(t0)⟩, (E.6)
implying it satisfies the first-order differential equation with initial condition U(t0, t0) = I:
iℏ ∂
∂tU(t, t0) = HSU(t, t0). (E.7)
If the Hamiltonian HS is time independent, Eq. (E.7) has the formal solution
U(t, t0) = exp(−iHS(t − t0)/ℏ), (E.8)
where the exponential of an operator is defined by its series expansion, eA = I + A + 1
2A2 + ··· .
If HS is time dependent, however, there is not a closed-form expression for U(t, t0) and one must
adopt more advanced methods.5
E.2 HEISENBERG REPRESENTATION
Another way to treat time dependence is to work with time-dependent operators instead of time￾dependent state vectors.6 In the Heisenberg representation, operators in the Schrodinger picture ¨
acquire a time dependence specified by (where t0 is an arbitrary reference time),
AH(t) ≡ U†(t, t0)ASU(t, t0), (E.9)
with the state vector propagated backward in time to t0,
|ψH⟩ ≡ U†(t, t0)|ψS(t)⟩ = U†(t, t0)U(t, t0)|ψS(t0)⟩ = |ψS(t0)⟩. (E.10)
One can show that expectation values are invariant under these transformations:
⟨ψH|AH(t)|ψH⟩ = ⟨ψS(t)|AS|ψS(t)⟩. (E.11)
The time-dependent operator AH(t) satisfies an equation of motion. One can show that
iℏ ∂
∂tAH(t)=[AH(t), HS] , (E.12)
where we’ve used that HS commutes with U(t). Equation (E.12) is the Heisenberg equation of
motion. It’s basically the same as calculating the rate of change of the density operator, Eq. (A.41).
3Probability is locally conserved in the Schrodinger equation; there is a continuity equation associated with probability. ¨ 4Note the similarity with the SCK equation of Markov processes, Eq. (2.29).
5See for example Fetter and Walecka[186, p58]. 6The two ways of viewing quantum dynamics as due either to the time dependence of operators or state vectors is
analogous to the difference between passive and active transformations. Consider the mapping of a vector r → r′
, with
r′ = Rr. In the active transformation the vector is actively changed relative to a fixed coordinate system, and in the passive
the vector is left unchanged but the coordinate system is changed. See [6, p58].Dynamical representations in quantum mechanics ■ 215
E.3 INTERACTION REPRESENTATION
The interaction representation (also called the Dirac picture) is intermediate between the
Schrodinger and Heisenberg pictures. Assume a system Hamiltonian of the general form ¨
H = H0 + H′
, (E.13)
where H0, the unperturbed part, contains the effects of internal interactions among system com￾ponents (as well as kinetic energies) and H′
, the interaction term, contains the effects of external
interactions; H0 and H′ are noncommuting operators.7 Whereas in the Schrodinger and Heisenberg ¨
pictures the state vector or the operators carry time dependence, in the interaction picture both carry
part of the time dependence of expectation values.
Consider the problem of finding the time evolution operator for systems described by Eq. (E.13),
iℏ ∂
∂tU(t, t0)=(H0 + H′
)U(t, t0). (E.14)
Even if H0, H′ were time independent, we would have little use for a propagator in the form
U(t, t0) = exp[−i (H0 + H′
) (t − t0) /ℏ] because H0 and H′ don’t commute; in particular8
U(t, t0) ̸= exp[−iH0(t − t0)/ℏ] exp[−iH′
(t − t0)/ℏ]. Nevertheless, we’re going to force U(t, t0)
to have the factored form
U(t, t0) ≡ U0(t, t0)U′
(t, t0), (E.15)
where, by definition,
iℏ ∂
∂tU0(t, t0) ≡ H0U0(t, t0). (E.16)
For H0 time independent (assumed here), U0(t, t0) = exp(−iH0(t − t0)/ℏ). Combine Eqs. (E.15)
and (E.14) and make use of Eq. (E.16) to show that U′ must satisfy
iℏ ∂
∂tU′
(t, t0) = 
U†
0 (t, t0)H′
U0(t, t0)

U′
(t, t0) ≡ H′
I (t)U′
(t, t0), (E.17)
which defines the interaction Hamiltonian in the interaction picture, H′
I (t) ≡ U†
0 (t, t0)H′
U0(t, t0).
Equation (E.17) is exact; no linearizations are involved.
We define, for any operator AS, its counterpart in the interaction picture,
AI (t) ≡ U†
0 (t, t0)ASU0(t, t0). (E.18)
Equation (E.18) is the same as the Heisenberg form of the operator, except for the Hamiltonian H0.
For expectation values,
⟨ψS(t)|AS|ψS(t)⟩ = ⟨ψS(t0)|U†(t, t0)ASU(t, t0)|ψS(t0)⟩
= ⟨ψS(t0)|U′†(t, t0)U†
0 (t, t0)ASU0(t, t0)U′
(t, t0)|ψS(t0)⟩
= ⟨U′
(t, t0)ψS(t0)|U†
0 (t, t0)ASU0(t, t0)|U′
(t, t0)ψS(t0)⟩
≡ ⟨ψI (t)|AI (t)|ψI (t)⟩, (E.19)
where, to preserve inner products, we define the state vector in the interaction representation
|ψI (t)⟩ ≡ U′
(t, t0)|ψS(t0)⟩ = U†
0 (t, t0)|ψS(t)⟩. (E.20)
7If H0, H′ were commuting operators, we’d be adding a constant of the motion to the Hamiltonian; H′ can’t commute
with H0 and constitute a dynamical interaction effecting observable changes in the system. 8See discussion of the Baker-Campbell-Hausdorff equation in Section A.4.216 ■ Non-Equilibrium Statistical Mechanics
Thus we have the identities ⟨ψI (t)|AI (t)|ψI (t)⟩ = ⟨ψS(t)|AS|ψS(t)⟩ = ⟨ψH|AH(t)|ψH⟩ which
show the role of time in the representations. One can show that |ψI (t)⟩ satisfies a Schrodinger-like ¨
equation in which its evolution is controlled by the interaction Hamiltonian,
iℏ ∂
∂t|ψI (t)⟩ = H′
I (t)|ψI (t)⟩. (E.21)
The interaction form of the operator satisfies the equation of motion,
iℏ ∂
∂tAI (t)=[AI (t), H0]. (E.22)
The interaction and Schrodinger pictures coincide for ¨ H0, with H0,I (t) = H0,S ≡ H0. One should
compare Eqs. (E.3), (E.12), and (E.22).
E.4 DENSITY MATRIX
The density matrix is another representation of the state of a quantum system; see Appendix A. It’s
used in quantum statistical mechanics to calculate ensemble averages; see Eq. (A.39). Here we note
it’s form in the three pictures. In the Schrodinger picture, ¨
ρS(t) = 
n
pn|ψn,S(t)⟩⟨ψn,S(t)| = U(t, t0)ρS(t0)U†(t, t0), (E.23)
where pn denotes the probability that a randomly selected member of the ensemble is in state |ψn⟩.
In the Heisenberg picture,
ρH = 
n
pn|ψn,H⟩⟨ψn,H| = ρS(t0). (E.24)
In the interaction picture,
ρI (t) = 
n
pn|ψn,I (t)⟩⟨ψn,I (t)| = U†(t, t0)ρS(t)U(t, t0). (E.25)
Table E.1 summarizes the results of this appendix.
Table E.1 State vector |ψ⟩, operator A, expectation value ⟨ψ|A|ψ⟩, and density matrix ρ
in the Schrodinger, Heisenberg, and interaction representations. ¨ HS is assumed time inde￾pendent and t0 = 0.
Schrodinger ¨ Heisenberg Interaction
|ψ(t)⟩ e−iHS t/ℏ|ψS(0)⟩ |ψH⟩ = constant |ψI (t)⟩ = eiH0t/ℏ|ψS(t)⟩
A AS = constant AH(t)=eiHS t/ℏASe−iHS t/ℏ AI (t)=eiH0t/ℏASe−iH0t/ℏ
⟨ψ|A|ψ⟩ ⟨ψS(t)|AS|ψS(t)⟩ ⟨ψH|AH(t)|ψH⟩ ⟨ψI (t)|AI (t)|ψI (t)⟩
ρ(t) e−iHS t/ℏρS(0)eiHS t/ℏ ρH = constant eiH0t/ℏρS(t)e−iH0t/ℏAPPENDIX F
Causality and analyticity
C AUSALITY (in physics) means that effects cannot precede causes.1 The principle is used in
Chapter 6 to define the response function ΦBA(θ), the time-dependent response of quantity
B to interaction A, so that ΦBA(θ < 0) = 0. Any function having the property f(t < 0) = 0
is known as a causal function. We review the mathematics of causality: Kramers-Kronig relations,
Titchmarsh’s theorem, and Fourier transforms of causal functions.
F.1 KRAMERS-KRONIG RELATIONS
Let F be a function analytic in the upper half of the complex plane C, with all poles having negative
imaginary parts.2 By the Cauchy integral formula[13, p210], we have, for integration path C along
the real axis and closed in the upper half plane in a counterclockwise semicircle of infinite radius,
1
2πi

C
F(ζ)
ζ − z
dζ =

F(z) if Im (z) > 0
0 if Im (z) < 0. (F.1)
Let z = x + iϵ with x real and ϵ > 0 and assuming F vanishes on the infinite semicircle,
F(x + iϵ) = 1
2πi
 ∞
−∞
F(x′
)
x′ − x − iϵ
dx′
. (F.2)
By letting ϵ → 0+ in Eq. (F.2), we have, by continuity, and in the usual way with a pole on the
integration path,
F(x) = 1
2πi
P
 ∞
−∞
F(x′
)
x′ − x
dx′ +
1
2
F(x)
where P denotes Cauchy principal value integral[13, p227]. Under these assumptions, F is con￾strained to satisfy
F(x) = 1
πi
P
 ∞
−∞
F(x′
)
x′ − x
dx′
. (F.3)
In Eq. (F.3), F is restricted to the real line, F : R → C. Separating F into real and imaginary parts,
Re [F(x)] = 1
πP
 ∞
−∞
Im [F(x′
)]
x′ − x
dx′
Im [F(x)] = − 1
πP
 ∞
−∞
Re [F(x′
)]
x′ − x
dx′
. (F.4)
1See Bunge[226]. The idea is refined in the theory of relativity, where the temporal order of events can be reference￾frame dependent; causal influences lie within or on the past lightcone of effects; see [6, p67]. The term causality has different
meanings in philosophy and in statistics. 2In math speak, F : C → C is meromorphic in the lower half plane and holomorphic in the upper half plane.
DOI: 10.1201/9781003512295-F 217218 ■ Non-Equilibrium Statistical Mechanics
The real and imaginary parts are therefore interrelated.3 Said differently, the real and imaginary
parts of F(x) are not independent; the function can be reconstructed from just one of its parts.
These formulae are known as the Kramers-Kronig relations.
4
F.2 TITCHMARSH’S THEOREM
The integrals in Eq. (F.4) are Hilbert transforms[13, p238], which for a real function f(t) is defined
ˆf(t)=(Hf) (t) ≡ 1
πP
 ∞
−∞
f(τ )
τ − t
dτ (F.5)
and we’ve denoted the transform as the result of an operator, H. Note that it’s linear. For complex￾valued functions F : R → C, with F(x) = g(x)+ih(x), we have from the Kramers-Kronig
relations, g(x) = Hh(x) and h(x) = −Hg(x). By combining these results, we have5 HHf = −f
for any f or that the inverse Hilbert transform H−1 = −H. The functions in the Kramers-Kronig
relations are therefore Hilbert-transform pairs, g(x) = Hh(x), h(x) = H−1g(x). The Hilbert
transform is the convolution (in the Fourier sense) of f with g(t) ≡ −1/(πt):
(Hf) (t) = 1
πP
 ∞
−∞
f(τ )
τ − t
dτ ≡
 ∞
−∞
f(τ )g(t − τ )dτ ≡ f(t) ∗ g(t).
By the convolution theorem (see [13, p113]),
F (Hf) (α) = F [f(t) ∗ g(t)] (α)=(Ff) (α) × (Fg) (α) = i sgn(α) (Ff) (α), (F.6)
where the signum function,
sgn(x) ≡



1 if x > 0
0 if x = 0
−1 if x < 0
(F.7)
and (Ff)(α) ≡  ∞
−∞ f(x)e−iαxdx; F[−1/(πx)](α) = i sgn(α) (use  ∞
−∞ du sin(u)/ u = π). The
Hilbert transform thus introduces a phase shift on, but preserves the magnitude of Ff,
The Kramers-Kronig relations are part of Titchmarsh’s theorem (see theorem 95 of [80, p128]
and [13, p240]), necessary and sufficient conditions for a complex-valued square-integrable function
F(x) on the real line [a member of L2(−∞,∞)] to be the boundary value of a holomorphic function
F(z) in the upper half plane. Titchmarsh’s theorem is that the following statements are equivalent:
• F(x) is the limit z → x of an analytic function F(z) bounded in the upper half plane,
 ∞
−∞ |F(x + iy)|
2dx<K for some number K and y > 0;
• For F(x) = f(x) − ig(x), g(x) = Hf(x);
• The Fourier transform of F(x), (FF)(ω)=0 for ω < 0.
The last statement is readily shown:
F(F) = F(f) − iF(g) = F(f) − iF(Hf) = F(f) − i(i sgn(ω))F(f) = (1 + sgn(ω)) F(f).
For ω < 0, (FF)(ω)=0.
3The real and imaginary parts of holomorphic functions are already interrelated through the Cauchy-Riemann conditions
at every point of the upper half plane; the Kramers-Kronig relations are satisfied at the boundary, on the real line. 4See for example Landau and Lifshitz[227, Section 62] or Born and Wolf[228, Section 10.2]. 5The result H2f = −f is one of the most important properties of the Hilbert transform and is used in signal processing.
It can be derived without invoking the Kramers-Kronig relations, but we arrive at the same conclusion faster this way.Causality and analyticity ■ 219
F.3 CAUSAL TRANSFORMS
We first show how to find causal functions—most functions are not causal. Any real function f(t)
can be expressed as a sum of even and odd functions,
f(t) = fe(t) + fo(t) ≡ 1
2 [f(t) + f(−t)] + 1
2 [f(t) − f(−t)] . (F.8)
Clearly, fe(−t) = fe(t) and fo(−t) = −fo(t). The Fourier transform of f can be expressed in
terms of cosine and sine transforms,
F(f)(ω) ≡
 ∞
−∞
e−iωtf(t)dt =
 ∞
−∞
fe(t) cos(ωt)dt − i
 ∞
−∞
fo(t) sin(ωt)dt. (F.9)
For any real function f, Re [Ff](ω) is even and Im [Ff](ω) is odd: Re [Ff](−ω) = Re [Ff](ω)
and Im [Ff](−ω) = − Im [Ff](ω).
For a given odd function fo(t), fe(t) ≡ sgn(t)fo(t) is even. The even function obtained this
way is, for t < 0, the mirror image of fo(t) for t > 0. An analytic representation of causal functions
results from the sum of sgn(t)fo(t) and fo,
fc(t) = [1 + sgn(t)] fo(t), (F.10)
where the subscript on fc denotes causal. Clearly, fc(t < 0) = 0. Thus we have a recipe for
constructing a causal function fc given an odd function fo. Note that fc(0) = 0.
Consider the Fourier transform of fc:
F [fc] (ω) =  ∞
−∞
dtfc(t)e−iωt =
 ∞
−∞
dt[1 + sgn(t)] fo(t)e−iωt
=
 ∞
−∞
sgn(t)fo(t) cos(ωt)dt − i
 ∞
−∞
fo(t) sin(ωt)dt
=
 ∞
−∞
sgn(t)fo(t)e−iωtdt +
 ∞
−∞
fo(t)e−iωtdt. (F.11)
With Ffc = Re [Ffc] − i Im [Ffc], we have
Re [Ffc](ω) =  ∞
−∞
sgn(t)fo(t)e−iωtdt
Im [Ffc](ω)=i  ∞
−∞
fo(t)e−iωtdt. (F.12)
The parity properties of (Ffc) (ω) are the same as those for (Ff) (ω); causal functions are real.
The relations in Eq. (F.12) are the Kramers-Kronig relations in disguise, as we now show. The
real part of Ffc is the Fourier transform of the product sgn(t)fo(t). It can be written as a convolution
of the Fourier transforms of sgn(t) and fo(t),
Re [Ffc] (ω) =  ∞
−∞
dte−iωt  ∞
−∞
dω′
Σ(ω′
)e−iω′
t
 ∞
−∞
dω′′e−iω′′t
Fo(ω′′) (F.13)
where
sgn(t) =  ∞
−∞
e−iωtΣ(ω)dω ⇐⇒ Σ(ω) = 1
2π
 ∞
−∞
eiωt sgn(t)dt
fo(t) =  ∞
−∞
e−iωtFo(ω)dω ⇐⇒ Fo(ω) = 1
2π
 ∞
−∞
eiωtfo(t)dt. (F.14)220 ■ Non-Equilibrium Statistical Mechanics
Equation (F.13) can then be written
Re [Ffc] (ω) =  ∞
−∞
dω′
 ∞
−∞
dω′′Σ(ω′
)Fo(ω′′)
 ∞
−∞
dte−it(ω+ω′
+ω′′)
= 2π
 ∞
−∞
dω′′Fo(ω′′)
 ∞
−∞
Σ(ω′
)δ(ω + ω′ + ω′′)dω′
= 2π
 ∞
−∞
dω′′Fo(ω′′)Σ(ω − ω′′)
= 2πFo(ω) ∗ Σ(ω), (F.15)
where we’ve used that Re [Ffc](ω) is even. It’s straightforward to show (see [13, p238]) that the
Fourier transform of sgn(x) is
Σ(ω) = i
πω . (F.16)
Note also that
Fo(ω) = i
2π Im [Ffc] (ω) (F.17)
Combining Eqs. (F.16) and (F.17) with Eq. (F.15), we find
Re [Ffc] (ω) = 1
πP
 ∞
−∞
dω′ Im [Ffc] (ω′
)
ω′ − ω = H ( Im [Ffc]) (ω). (F.18)
Equation (F.18) implies (by the inverse Hilbert transform) the other Kramers-Kronig relation,
Im [Ffc] = H−1 ( Re [Ffc]) = −H ( Re [Ffc]). (F.19)
Fourier transforms of causal functions, causal transforms, obey the Kramers-Kronig relations.
F.4 THE PLEMELJ FORMULA
The Plemelj formula (also known as the Sokhotski-Plemelj theorem), is a useful relation among
generalized functions6 that often occurs in applications involving causal functions,
limϵ→0
1
x ± iϵ = P 1
x
∓ iπδ(x). (F.20)
To establish Eq. (F.20) we must show that
limϵ→0
 ∞
−∞
f(x)
x ± iϵ
dx = P
 ∞
−∞
f(x)
x
dx ∓ iπf(0), (F.21)
where f(x) is a smooth function vanishing sufficiently fast as x → ±∞ that integrals are conver￾gent. Equation (F.20) is shorthand for Eq. (F.21).
We start with the identity
1
x ± iϵ = x ∓ iϵ
x2 + ϵ2 . (F.22)
Then, for a smooth function f(x),
 ∞
−∞
f(x)
x ± iϵ
dx =
 ∞
−∞
xf(x)
x2 + ϵ2 dx ∓ iϵ
 ∞
−∞
f(x)
x2 + ϵ2 dx. (F.23)
6Generalized functions have meaning only inside integrals, where they are integrated together with smooth functions; see
Lighthill[99] or [13, p66]. Smooth function is generally a code word in mathematics for “all derivatives exist.” Lighthill[99,
p15] defines a good function as one which is everywhere differentiable any number of times and is such that it and all its
derivatives are O(|x|
−N ) as |x|→∞ for all N; f(x) = exp(−x2) is a good function.Causality and analyticity ■ 221
We’re interested in the limit as ϵ → 0. The first integral on the right of Eq. (F.23) can be written
(where we’ll take the limit δ → 0)
 ∞
−∞
xf(x)
x2 + ϵ2 dx =
 −δ
−∞
xf(x)
x2 + ϵ2 dx +
 ∞
δ
xf(x)
x2 + ϵ2 dx +
 δ
−δ
xf(x)
x2 + ϵ2 dx. (F.24)
The limit ϵ → 0 can safely be taken in the first two integrals on the right of Eq. (F.24). In the third
integral, we can, for sufficiently small δ, replace f(x) ≈ f(0) for |x| < δ. Thus,
limϵ→0
 ∞
−∞
xf(x)
x2 + ϵ2 dx = limϵ→0
lim
δ→0
 −δ
−∞
xf(x)
x2 + ϵ2 dx +
 ∞
δ
xf(x)
x2 + ϵ2 dx

+ f(0)  δ
−δ
x
x2 + ϵ2 dx
≡ P
 ∞
−∞
f(x)
x
dx,
where the last integral vanishes by symmetry and the terms in square bracket define the Cauchy
principal value integral. For the second integral on the right of Eq. (F.23), the only significant con￾tribution to the integral is, for small ϵ, from the integration region x ≈ 0, so that
ϵ
 ∞
−∞
f(x)
x2 + ϵ2 dx ≈ ϵf(0)  ∞
−∞
dx
x2 + ϵ2 = ϵf(0) ·
1
ϵ tan−1(x/ϵ)


∞
−∞ = πf(0). (F.25)
We’ve therefore established Eq. (F.21).
An alternate version of the Plemelj relation applies to one-sided Fourier transforms. Consider
the integrals where we put in convergence factors,
I+(ω) ≡
 ∞
0
eiωtdt = lim
λ→0+
 ∞
0
eiωte−λtdt = lim
λ→0
1
λ − iω
I−(ω) ≡
 0
−∞
eiωtdt = lim
λ→0+
 0
−∞
eiωteλtdt = lim
λ→0
1
λ + iω . (F.26)
The sum I+(ω) + I−(ω)=2πδ(ω). Using Eq. (F.20), we have the result, useful for dealing with
causal functions,
 ∞
0
e±iωtdt = πδ(ω) ± iP
 1
ω

. (F.27)
Gel’Fand and Shilov[229, p172] present a rigorous derivation. Equation (F.27) is the Fourier trans￾form of the unit step function
θ(x) = 
1 if x ≥ 0
0. if x < 0 (F.28)
The step function often appears inside integrals and thus the value of θ(x = 0) isn’t usually impor￾tant; the value of a function at a single point doesn’t affect the value of the integral. Sometimes the
step function is defined so that θ(0) = 1
2 .
F.5 TAKE AWAY
The Fourier transform of a causal function in the time domain is, in the frequency domain, an
analytic function in the closed upper half of the complex frequency plane,7 implying it obeys
the Kramer-Kronig relations on the real line: Causality implies analyticity and analyticity implies
causality. As summarized by J.S. Toll[230]: A function of integrable square is zero for all negative
values of its argument if and only if its Fourier transform is a causal transform.
7The closed upper half plane is the upper half plane together with its closure, the real line.Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com Bibliography
[1] J.W. Gibbs. The Collected Works of J. Willard Gibbs: Volume I Thermodynamics. Yale
University Press, 1948.
[2] J.H. Luscombe. Thermodynamics. CRC Press, 2018.
[3] R.P. Feynman. Statistical Mechanics. W.A. Benjamin, 1972.
[4] D.D. Nolte. The tangled tale of phase space. Physics Today, 63:33–38, 2010.
[5] J.H. Luscombe. Statistical Mechanics: From Thermodynamics to the Renormalization Group.
CRC Press, 2021.
[6] J.H. Luscombe. Core Principles of Special and General Relativity. CRC Press, 2019.
[7] S.R. de Groot and P. Mazur. Non-Equilibrium Thermodynamics. North-Holland Publishing
Company, 1962.
[8] R. Clausius. The Mechanical Theory of Heat. John van Voorst, 1867.
[9] I. Prigogine and R. Defay. Chemical Thermodynamics. Longman Greens, 1954.
[10] H.B. Callen. Thermodynanics. John Wiley, 1960.
[11] T.L. Hill. Thermodynamics of Small Systems, volume 1. W.A. Benjamin, 1963.
[12] G.K. Batchelor. An Introduction to Fluid Dynamics. Cambridge University Press, 1967.
[13] B. Borden and J. Luscombe. Mathematical Methods in Physics, Engineering, and Chemistry.
Wiley, 2020.
[14] P.M. Morse and H. Feshbach. Methods of Theoretical Physics. McGraw-Hill, 1953.
[15] L.D. Landau and E.M. Lifshitz. Fluid Mechanics. Pergamon Press, 1959.
[16] P.G. de Gennes and J. Prost. The Physics of Liquid Crystals. Oxford University Press, 1993.
[17] Th. De Donder and P. Van Rysselberghe. Thermodynamic Theory of Affinity. Stanford
University Press, 1936.
[18] D.D. Fitts. Nonequilibrium Thermodynamics. McGraw-Hill, 1962.
[19] L. Onsager. Reciprocal relations in irreversible processes. I. Phys. Rev., 37:405–426, 1931.
[20] L. Onsager. Reciprocal relations in irreversible processes. II. Phys. Rev., 38:2265–2279,
1931.
[21] H.B.G. Casimir. On Onsager’s principle of microscopic reversibility. Reviews of Modern
Physics, 17:343–350, 1945.
223224 ■ Bibliography
[22] D.G. Miller. Thermodynamics of irreversible processes: The experimental verification of the
Onsager reciprocal relations. Chemical Review, 60:15–37, 1960.
[23] G.S. Yablonsky, A.N. Gorban, D. Constales, V.V. Galvita, and G.B. Marin. Reciprocal rela￾tions between kinetic curves. Europhysics Letters, 93:20004, 2011.
[24] C. Kittel. Elementary Statistical Physics. John Wiley, 1958.
[25] H.J. Kreuzer. Nonequilibrium Thermodynamics and its Statistical Foundations. Oxford
University Press, 1981.
[26] I. Prigogine. Thermodynamics of Irreversible Processes. Wiley, 1961.
[27] E.T. Jaynes. The minimum entropy production principle. Annual Review of Physical
Chemistry, 31:579–601, 1980.
[28] G.D. Birkhoff. Proof of the ergodic theorem. Proc. Natl. Acad. Sci. USA, 17:656–660, 1931.
[29] A. Pais. Subtle is the Lord: The Science and Life of Albert Einstein. Oxford University Press,
1982.
[30] K. Pearson. Notes on the history of correlation. Biometrika, 13:25–45, 1920.
[31] H. Cram´er. Mathematical Methods of Statistics. Princeton University Press, 1946.
[32] J.A. Shohat and J.D. Tamarkin. The Problem of Moments. American Mathematical Society,
1943.
[33] R.F. Greene and H.B. Callen. On the formalism of thermodynamic fluctuation theory. Phys.
Rev., 83:1231–1235, 1951.
[34] P.W. Anderson. More is different. Science, 177:393–396, 1972.
[35] N. Wiener. Extrapolation, Interpolation, and Smoothing of Stationary Time Series. MIT
Press, 1950.
[36] J.L. Doob. Stochastic Processes. John Wiley, 1953.
[37] R.M. Gray. Probability, Random Processes, and Ergodic Properties. Springer, 1987.
[38] E. Parzen. Modern Probability Theory and Its Applications. John Wiley, 1960.
[39] M. Lo`eve. Probability Theory. D. Van Nostrand, 1955.
[40] B. Oksendal. Stochastic Differential Equations. Springer, 6th edition, 2003.
[41] E. Parzen. Stochastic Processes. Holden-Day, 1962.
[42] M. Kac. Probability and Related Topics in Physical Sciences. Interscience Publishers, 1959.
[43] A. Papoulis. Probability, Random Variables, and Stochastic Processes. McGraw-Hill, 1991.
[44] N. Wax, editor. Selected Papers on Noise and Stochastic Processes. Dover Publications,
1954.
[45] H. Nyquist. Thermal agitation of electric charge in conductors. Physical Review, 32:110–113,
1928.
[46] J. Johnson. Thermal agitation of electricity in conductors. Physical Review, 32:97–109, 1928.Bibliography ■ 225
[47] E.B. Moullin. Spontaneous Fluctuations of Voltage. Oxford University Press, 1938.
[48] R. Furth, editor. ¨ Investigations on the Theory of the Brownian Movement by Albert Einstein.
Dover Publications, 1956.
[49] A. Nordsieck, W.E. Lamb, and G.E. Uhlenbeck. On the theory of cosmic-ray showers. Phys￾ica, 7:344–360, 1940.
[50] V. Lakshmikantham and M. Rama Mohana Rao. Theory of Integro-differential Equations.
CRC Press, 1995.
[51] N.G. van Kampen. Stochastic Processes in Physics and Chemistry. North-Holland Publishing
Company, 1992.
[52] E.P. Wigner. Derivations of Onsager’s reciprocal relations. J. Chem. Phys., 22:1912–1915,
1954.
[53] R. Bellman. Introduction to Matrix Analysis. McGraw-Hill, 1960.
[54] Y.G. Sinai. Probability Theory. Springer-Verlag, 1992.
[55] M.G. Kendall and A. Stuart. The Advanced Theory of Statistics, volume 1. Hafner, 1963.
[56] J. Marcinkiewicz. Sur une propri´et´e de la loi de Gauss. Mathematische Zeitschrift, 44:
612–618, 1939.
[57] R. Kubo. Generalized cumulant expansion method. J. Phys. Soc. Jpn., 17:1100–1120, 1962.
[58] J. Aczel. Lectures on Functional Equations and Their Applications. Academic Press, 1966.
[59] J.L. Doob. The Brownian movement and stochastic equations. Annals of Mathematics,
43:351–369, 1942.
[60] F. Reif. Fundamentals of Statistical and Thermal Physics. McGraw-Hill, 1965.
[61] G. Polya. Mathematics and Plausible Reasoning, volume 1. Princeton University Press,
1954.
[62] J. Perrin. Atoms. D. Van Nostrand, 1916.
[63] D.S. Lemon and A. Gythiel. Paul Langevin’s 1908 paper “On the theory of Brownian mo￾tion”. American Journal of Physics, 65:1079–1081, 1997.
[64] G.E. Uhlenbeck and L.S. Ornstein. On the theory of the Brownian motion. Physical Review,
36:823–841, 1930.
[65] R.M. Mazo. Brownian Motion. Oxford University Press, 2002.
[66] W.T. Coffey. Development and application of the theory of Brownian motion. In M.W. Evans,
editor, Advances in Chemical Physics, volume 63, pages 69–252. Wiley, 1985.
[67] H. Risken. The Fokker-Planck Equation. Springer-Verlag, 1989.
[68] S. Kheifets, A. Simha, K. Melin, T. Li, and M.G. Raizen. Observation of Brownian motion
in liquids at short times: Instantaneous velocity and memory loss. Science, 343:1493–1496,
2014.
[69] H. Jeffries and B. Jeffries. Methods of Mathematical Physics. Cambridge University Press,
1956.226 ■ Bibliography
[70] R. Paley and N. Wiener. Fourier Transforms in the Complex Domain. American Mathemati￾cal Society, 1934.
[71] S.A. Adelman. Fokker-Planck equations for simple non-Markovian systems. Journal of
Chemical Physics, 64:124–130, 1976.
[72] R. Courant and D. Hilbert. Methods of Mathematical Physics, volume I. Interscience Pub￾lishers, 1953.
[73] I.N. Sneddon. Elements of Partial Differential Equations. McGraw-Hill, 1957.
[74] R. Courant and D. Hilbert. Methods of Mathematical Physics, volume II. Interscience Pub￾lishers, 1962.
[75] M.C. Wang and G.E. Uhlenbeck. On the theory of the Brownian motion II. Reviews of
Modern Physics, 17:323–342, 1945.
[76] H.A. Kramers. Brownian motion in a field of force and the diffusion model of chemical
reactions. Physica, 7:284–304, 1940.
[77] P. Dennery and A. Krzywicki. Mathematics for Physicists. Harper and Row, 1967.
[78] G. Szego. Orthogonal Polynomials. American Mathematical Society, 1939.
[79] E.D. Rainville. Special Functions. Chelsea Publishing, 1971.
[80] E.C. Titchmarsh. Introduction to the Theory of Fourier Integrals. Oxford University Press,
1948.
[81] J.J. Sakurai and J. Napolitano. Modern Quantum Mechanics. Pearson, 2011.
[82] S. Chandrasekhar. Stochastic problems in physics and astronomy. Reviews of Modern
Physics, 15:1–89, 1943.
[83] J.E. Moyal. Stochastic processes and statistical physics. Journal of the Royal Statistical
Society. Series B (Methodological), 11:150–210, 1949.
[84] N.G. van Kampen. A power series expansion of the master equation. Canadian Journal of
Physics, 39:551–567, 1961.
[85] R.F. Pawula. Generalizations and extensions of the Fokker-Planck-Kolmogorov equation.
IEEE Transactions on Information Theory, 13:33–41, 1967.
[86] R.F. Pawula. Approximation of the linear Boltzmann equation by the Fokker-Planck equa￾tion. Physical Review, 162:186–188, 1967.
[87] M. Abramowitz and I.A. Stegun. Handbook of Mathematical Functions. US Government
Printing Office, 1964.
[88] I.S. Gradshteyn and I.M. Ryzhik. Table of Integrals, Series, and Products. Academic Press,
1980.
[89] S.G. Brush. The Kinetic Theory of Gases. Imperial College Press, 2003.
[90] H. Bohr. Almost Periodic Functions. Chelsea Publishing, 1947.
[91] J. Yvon. La th´eorie statistique des fluides et l’´equation d’´etat. Actualit´es scientifiques et
industrielles. Hermann et Cie., Paris, No. 203, 1935.Bibliography ■ 227
[92] H.S. Green. The Molecular Theory of Liquids. North-Holland Publishing Company, 1952.
[93] J. Yvon. Correlations and Entropy in Classical Statistical Mechanics. Pergamon Press, 1969.
[94] N.N. Bogoliubov. Problems of a dynamical theory in statistical physics. In J. de Boer and
G.E. Uhlenbeck, editors, Studies in Statistical Mechanics, volume I, pages 1–118. North￾Holland Publishing Company, 1962.
[95] M. Born and H.S. Green. A general kinetic theory of liquids I. The molecular distribution
functions. Proc. Roy. Soc. A, 188:10–18, 1946.
[96] M. Born and H.S. Green. A General Kinetic Theory of Liquids. Cambridge University Press,
1949.
[97] J.G. Kirkwood. The statistical mechanical theory of transport processes I. General theory. J.
Chem. Phys., 14:180–201, 1946.
[98] J.G. Kirkwood. The statistical mechanical theory of transport processes II. Transport in gases.
J. Chem. Phys., 15:72–76, 1947.
[99] M.J. Lighthill. Introduction to Fourier Analysis and Generalised Functions. Cambridge
University Press, 1958.
[100] J.H. Irving and J.G. Kirkwood. Statistical mechanical theory of transport processes. IV. The
equations of hydrodynamics. J. Chem. Phys., 18:817–829, 1950.
[101] B.D. Todd, D.J. Evans, and P.J. Daivis. Pressure tensor for inhomogeneous fluids. Physical
Review E, 52:1627–1638, 1995.
[102] D. Massignon. M´ecanique Statistique des Fluides. Dunod, 1957.
[103] P. Resibois and M. De Leener. Classical Kinetic Theory of Fluids. John Wiley, 1977.
[104] J.R.T. Seddon, H.J.W. Zandvliet, and D. Lohse. Knudsen gas provides nanobubble stability.
Phys. Rev. Lett., 107:116101, 2011.
[105] L. Boltzmann. Lectures on Gas Theory. University of California Press, 1964.
[106] S. Chapman and T.G. Cowling. The Mathematical Theory of Non-Uniform Gases. Cambridge
University Press, 1970.
[107] G.E. Uhlenbeck and G.W. Ford. Lectures in Statistical Mechanics. American Mathematical
Society, 1963.
[108] E.C.G. Sudarshan and N. Mukunda. Classical Dynamics: A Modern Perspective. John Wiley,
1974.
[109] C. Cercignani. Theory and Application of the Boltzmann Equation. Scottish Academic Press,
1975.
[110] S. Dodelson and F. Schmidt. Modern Cosmology. Academic Press, 2020.
[111] R.E. Masterson. Introduction to Nuclear Reactor Physics. CRC Press, 2017.
[112] J. Meng and X. Li. Lattice Boltzmann model for traffic flow. Physical Review E, 77:036108,
2008.
[113] D.K. Ferry and S.M. Goodnick. Transport in Nanostructures. Cambridge University Press,
1997.228 ■ Bibliography
[114] J.L. Lebowitz and E.W. Montroll, editors. Nonequilibrium Phenomena I. The Boltzmann
Equation. North-Holland Publishing Company, 1983.
[115] H. Grad. Principles of the kinetic theory of gases. In S. Flugge, editor, Handbuch der Physik,
volume 12, pages 205–294. Springer, 1958.
[116] I. Prigogine. Non-Equilibrium Statistical Mechanics. Wiley, 1962.
[117] R.L. Liboff. Kinetic Theory. Prentice Hall, 1990.
[118] E.G.D. Cohen. The Boltzmann equation and its generalization to higher densities. In
E.G.D. Cohen, editor, Fundamental Problems in Statistical Mechanics, pages 110–156.
North-Holland Publishing Company, 1962.
[119] J.R. Dorfman and H. van Beijeren. The kinetic theory of gases. In B.J. Berne, editor, Statis￾tical Mechanics: Time-Dependent Processes, pages 65–179. Plenum Press, 1977.
[120] G.F. Mazenko and S. Yip. Renormalized kinetic theory of dense fluids. In B.J. Berne, editor,
Statistical Mechanics: Time-Dependent Processes, pages 181–231. Plenum Press, 1977.
[121] S.G. Brush. Kinetic Theory, Volume 3: The Chapman-Enskog solution of the transport equa￾tion for moderately dense gases. Pergamon Press, 1972.
[122] P. Ehrenfest and T. Ehrenfest. The Conceptual Foundations of the Statistical Approach in
Mechanics. Cornell University Press, 1959.
[123] S. Chapman. Boltzmann’s H-theorem. Nature, 139:931, 1937.
[124] R.C. Tolman. The Principles of Statistical Mechanics. Oxford University Press, 1938.
[125] J. Jeans. An Introduction to the Kinetic Theory of Gases. Cambridge University Press, 1952.
[126] A. Sommerfeld. Thermodynamics and Statistical Mechanics. Academic Press, 1964.
[127] S. Harris. An Introduction to the Theory of the Boltzmann Equation. Holt, Rinehart, and
Winston, 1971.
[128] T.H. Gronwall. A functional equation in the kinetic theory of gases. Annals of Mathematics,
17:1–4, 1915.
[129] G.-J. Su. Modified law of corresponding states for real gases. Industrial and Engineering
Chemistry, 38:803–806, 1946.
[130] R.P. Feynman. Simulating physics with computers. International Journal of Theoretical
Physics, 21:467–488, 1982.
[131] J.W. Gibbs. The Collected Works of J. Willard Gibbs: Volume II Elementary Principles in
Statistical Mechanics. Yale University Press, 1948.
[132] C. Shannon. A mathematical theory of communication. Bell System Technical Journal,
27:379–423, 623–656, 1948.
[133] C. Shannon and W. Weaver. Mathematical Theory of Communication. University of Illinois
Press, 1949.
[134] E.T. Jaynes. Gibbs vs Boltzmann entropies. American Journal of Physics, 33:391–398, 1965.
[135] P. Zupanovic and D. Kuic. Relation between Boltzmann and Gibbs entropy and example with
multinomial distribution. J. Phys. Commun., 2:045002, 2018.Bibliography ■ 229
[136] P.W. Bridgman. The Nature of Thermodynamics. Harvard University Press, 1941.
[137] E.P. Culverwell. Dr. Watson’s proof of Boltzmann’s theorem on permanence of distributions.
Nature, 50:617, 1894.
[138] R. Landauer. Irreversibility and heat generation in the computing process. IBM Journal of
Research and Development, 5:183–191, 1961.
[139] R. Landauer. Information is a physical entity. Physica A, 263:63–67, 1999.
[140] A. Berut, A. Arakelyan, A. Petrosyan, et al. Experimental verification of Landauer’s principle
linking information and thermodynamics. Nature, 483:187–189, 2012.
[141] J. Hong, B. Lambson, S. Dhuey, et al. Experimental test of Landauer’s principle in single-bit
operations on nanomagnetic memory bits. Sci. Adv., 2:e1501492, 2016.
[142] Y. Jun, M. Gavrilov, and J. Bechhoefer. High-precision test of Landauer’s principle in a
feedback trap. Physical Review Letters, 113:190601, 2014.
[143] J.C. Slater. Atomic radii in crystals. J. Chem. Phys., 41:3199–3204, 1964.
[144] D. Hilbert. Grundzuge einer allgemeinen Theorie der linearen Integralgleichungen ¨ . Teubner,
1912.
[145] D. Hilbert. Begrundung der kinetischen Gastheorie. ¨ Mathematische Annalen, 72:562–577,
1912.
[146] C. Cercignani, R. Illner, and M. Pulvirenti. The Mathematical Theory of Dilute Gases.
Springer-Verlag, 1994.
[147] L. Saint-Raymond. Hydrodynamic Limits of the Boltzmann Equation. Springer, 2009.
[148] C.M. Bender and S.A. Orszag. Advanced Mathematical Methods for Scientists and Engi￾neers. McGraw-Hill, 1978.
[149] M. van Dyke. Perturbation Methods in Fluid Mechanics. Academic Press, 1964.
[150] H. Grad. Theory of rarefied gases. In F.M. Devienne, editor, Rarefied Gas Dynamics, pages
100–138. Pergamon Press, 1960.
[151] H. Grad. Asymptotic theory of the Boltzmann equation. Physics of Fluids, 6:147–181, 1963.
[152] D. Burnett. The distribution of molecular velocities and the mean motion in a non-uniform
gas. Proceedings of the London Mathematical Society, 40:382–435, 1936.
[153] A. Santos, J.J. Brey, and J.W. Dufty. Divergence of the Chapman-Enskog expansion. Phys.
Rev. Lett., 56:1571–1574, 1986.
[154] L.S. Garcia-Colin, R.M. Velasco, and F.J. Uribe. Beyond the Navier-Stokes equations: Bur￾nett hydrodynamics. Physics Reports, 465:149–189, 2008.
[155] C.S. Wang Chang and G.E. Uhlenbeck. The kinetic theory of gases. In J. de Boer and G.E.
Uhlenbeck, editors, Studies in Statistical Mechanics, Volume V, pages 1–100. North-Holland
Publishing Company, 1970.
[156] Y.P. Pao. Boltmann collision operator with inverse-power intermolecular potentials, I. Com￾munications on Pure and Applied Mathematics, 27:407–428, 1974.230 ■ Bibliography
[157] E.T. Whittaker and G.N. Watson. A Course of Modern Analysis. Cambridge University Press,
1927.
[158] J.O. Hirschfelder, C.F. Curtiss, and R. B. Bird. Molecular Theory of Gases and Liquids. John
Wiley, 1954.
[159] N.W. Ashcroft and N.D. Mermin. Solid State Physics. Saunders College Publishing, 1976.
[160] L.D. Landau. Die kinetische Gleichung fur den Fall Coulombscher Wechselwirkung. ¨ Phys.
Z. Sowjetunion, 10:154–164, 1936.
[161] E.M. Lifshitz and L.P. Pitaevskii. Physical Kinetics. Pergamon Press, 1981.
[162] Y.L. Klimontovich. The Statistical Theory of Non-Equilibrium Processes in a Plasma. MIT
Press, 1967.
[163] R. Balescu. Statistical Mechanics of Charged Particles. John Wiley, 1963.
[164] A. Bobylev, I. Gamba, and I. Potapenko. On some properties of the Landau kinetic equation.
Journal of Statistical Physics, 161:1327–1338, 2015.
[165] P.A.M. Dirac. The Principles of Quantum Mechanics. Oxford University Press, 1958.
[166] M.N. Rosenbluth, W.M. MacDonald, and D.L. Judd. Fokker-Planck equation for an inverse￾square force. Physical Review, 107:1–6, 1957.
[167] R.C. Davidson. Theory of Nonneutral Plasmas. Addison-Wesley, 1990.
[168] H.L. Friedman. Ionic Solution Theory. Interscience Publishers, 1962.
[169] A.A. Vlasov. On vibration properties of electron gas. Journal of Experimental and Theoreti￾cal Physics, 8:291–318, 1938.
[170] A.A. Vlasov. Many Particle Theory and Its Application to Plasma. Gordon and Breach,
1961.
[171] L. Tonks and I. Langmuir. Oscillations in ionized gases. Physical Review, 33:195–210, 1929.
[172] J.H. Jeans. On the theory of star-streaming and the structure of the universe. Monthly Notices
of the Royal Astronomical Society, 76:70–84, 1915.
[173] L. Landau. On the vibrations of the electronic plasma. JETP, 16:574–86, 1946.
[174] J.H. Malmberg and C.B. Wharton. Collisionless damping of electrostatic plasma waves.
Physical Review Letters, 13:184–186, 1964.
[175] D. Lynden-Bell. The stability and vibrations of a gas of stars. Monthly Notices of the Royal
Astronomical Society, 124:279–296, 1962.
[176] J. Binney and S. Tremaine. Galactic Dynamics. Princeton University Press, 1987.
[177] K.M. Case. Plasma oscillations. Annals of Physics, 7:349–364, 1959.
[178] T.Y. Wu. Kinetic Equations of Gases and Plasmas. Addison-Wesley, 1966.
[179] G. Schmidt. Physics of High Temperature Plasmas. Academic Press, 1966.
[180] F.F. Chen. Introduction to Plasma Physics and Controlled Fusion. Plenum Press, 1984.Bibliography ■ 231
[181] D.G. Swanson. Plasma Kinetic Theory. CRC Press, 2008.
[182] H.E. Stanley. Introduction to Phase Transitions and Critical Phenomena. Oxford University
Press, 1971.
[183] L. van Hove. Correlations in space and time and Born approximation scattering in systems
of interacting particles. Physical Review, 95:249–262, 1954.
[184] P.C. Hohenberg and B.I. Halperin. Theory of dynamic critical phenomena. Reviews of Mod￾ern Physics, 49:435–479, 1977.
[185] R. Kubo. Statistical-mechanical theory of irreversible processes. I. Journal of Physical
Society of Japan, 12:570–586, 1957.
[186] A.L. Fetter and J.D. Walecka. Quantum Theory of Many-Particle Systems. McGraw-Hill,
1971.
[187] L.P. Kadanoff and G. Baym. Quantum Statistical Mechanics. Benjamin-Cummings Publish￾ing, 1962.
[188] A.A. Abrikosov, L.P. Gorkov, and I.E. Dzyaloshinski. Methods of Quantum Field Theory in
Statistical Physics. Prentice Hall, 1963.
[189] D.C. Langreth. Linear and nonlinear response theory with applications. In J.T. Devreese
and V.E. van Doren, editors, Linear and Nonlinear Electron Transport in Solids, pages 3–32.
Plenum Press, 1976.
[190] J. Rammer. Quantum Field Theory of Non-Equilibrium States. Cambridge University Press,
2007.
[191] R.W. Boyd. Nonlinear Optics. Academic Press, 2020.
[192] H.B. Callen and T.A. Welton. Irreversibility and generalized noise. Phys. Rev., 83:34–40,
1951.
[193] R. Kubo. The fluctuation-dissipation theorem. Reports on Progress in Physics, 29:255–284,
1966.
[194] G.V. Chester. The theory of irreversible processes. Reports on Progress in Physics, 26:
411–472, 1963.
[195] R. Zwanzig. Time correlation functions and transport coefficients in statistical mechanics.
Annual Review of Physical Chemistry, 16:67–102, 1965.
[196] K.M. Case. On fluctuation-dissipation theorems. Transport Theory and Statistical Physics,
2:129–176, 1972.
[197] M.S. Green. Brownian motion in a gas of noninteracting molecules. The Journal of Chemical
Physics, 19:1036–1046, 1951.
[198] J.M. Luttinger. Theory of thermal transport coefficients. Physical Review, 135:
A1505–A1514, 1964.
[199] H. Mori. Statistical-mechanical theory of transport in fluids. Physical Review, 112:
1829–1842, 1958.232 ■ Bibliography
[200] M.S. Green. Markoff random processes and the statistical mechanics of time-dependent
phenomena. II. Irreversible processes in fluids. The Journal of Chemical Physics, 22:
398–413, 1954.
[201] M.S. Green. Comment on a paper of Mori on time-correlation expressions for transport
properties. Physical Review, 119:829–830, 1960.
[202] P. Resibois. On the connection between the kinetic approach and the correlation-function
method for thermal transport coefficients. The Journal of Chemical Physics, 41:2979–2992,
1964.
[203] H. Mori. Transport, collective motion, and Brownian motion. Progress of Theoretical
Physics, 33:423–454, 1965.
[204] E. Hille and R.S. Phillips. Functional Analysis and Semi-groups. American Mathematical
Society, 1957.
[205] D. Forster. Hydrodynamic Fluctuations, Broken Symmetry, and Correlation Functions. W.A.
Benjamin, 1975.
[206] R. Zwanzig. Nonequilibrium Statistical Mechanics. Oxford University Press, 2001.
[207] J.P. Boon and S. Yip. Molecular Hydrodynamics. McGraw-Hill, 1980.
[208] M.H. Stone. On one-parameter unitary groups in Hilbert space. Annals of Mathematics,
33:643–648, 1932.
[209] F. Riesz and B. Sz.-Nagy. Functional Analysis. Ungar, 1955.
[210] I. Bengtsson and K. Zyczkowski. Geometry of Quantum States: An Introduction to Quantum
Entanglement. Cambridge University Press, 2020.
[211] J. von Neumann. Mathematical Foundations of Quantum Mechanics. Princeton University
Press, 1955.
[212] M. Ohya and D. Petz. Quantum Entropy and Its Use. Springer, 2004.
[213] M.A. Nielsen and I.L. Chuang. Quantum Computation and Quantum Information.
Cambridge University Press, 2000.
[214] H. Weyl. The Theory of Groups and Quantum Mechanics. Dover, 1950.
[215] N.H. McCoy. On the function in quantum mechanics which corrresponds to a given function
in classical mechanics. Proc. Natl. Acad. Sci. USA, 18:674–676, 1932.
[216] T.L. Curtright, D.B Fairlie, and C.K. Zachos. A Concise Treatise on Quantum Mechanics in
Phase Space. World Scientific Publishing, 2014.
[217] M. Veltman. Diagrammatica. Cambridge University Press, 1994.
[218] W. Feller. An Introduction to Probability Theory and Its Applications, volume I. John Wiley,
1957.
[219] L.D. Landau and E.M. Lifshitz. Mechanics. Butterworth-Heinemann, 1976.
[220] H. Goldstein. Classical Mechanics. Addison-Wesley, 1950.
[221] F. Smithies. Integral Equations. Cambridge University Press, 1958.Bibliography ■ 233
[222] M.H. Stone. Linear Transformations in Hilbert Space. American Mathematical Society,
1932.
[223] T. Kato. Perturbation Theory for Linear Operators. Springer-Verlag, 1966.
[224] C. Cohen-Tannoudji, B. Diu, and F. Laloe. Quantum Mechanics, volume I. John Wiley, 1977.
[225] A. Messiah. Quantum Mechanics. North-Holland Publishing Company, 1965.
[226] M. Bunge. Causality. Harvard University Press, 1959.
[227] L.D. Landau and E.M. Lifshitz. Electrodynamics of Continuous Media. Pergamon Press,
1960.
[228] M. Born and E. Wolf. Principles of Optics. Cambridge University Press, sixth edition, 1980.
[229] I.M. Gel’Fand and G.E. Shilov. Generalized Functions. Vol. 1 Properties and Operations.
Academic Press, 1964.
[230] J.S. Toll. Causality and the dispersion relation: Logical foundations. Physical Review,
104:1760–1770, 1956.Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com Index
absolute Maxwellian, 118
absolute temperature, xi, 116
active transformation, 214
adiabatic switching, 165
admittance, 166
after-effect function, 164
almost periodic function, mentioned, 93
angular momentum density, 141
anticorrelation, 22
apsis, 208
associated Laguerre polynomials, 133
autocorrelation function, 27, 161
normalized, 46
automorphism, 186
average value of a random variable, 204
B-particle, see Brownian motion
Baker-Campbell-Hausdorff equation, 199,
215
balance equation, xiii, 2, 4
entropy, 8
kinetic energy, 7
local form, 4
microscopic basis, 97
balanced chemical equation, 4
bandwidth, 33
barometric formula, 117, 149
BBGKY hierarchy, 95, 96, 135
bilinear form
definition, 9
of Boltzmann collision operator, 113
binomial distribution, 34, 41, 54
binomial theorem, 55, 57
Birkhoff’s theorem, 19, 23, 29, 48
black body, 33
Boltzmann
H-theorem, 114
constant, k, xi
entropy, 20, 119
factor, exp(−βE), xii, 192
transport equation, 109, 111
collisionless, 150
local in r, t, 113, 115
normal solutions, 123
time scales in, 123
bracket notation for averages ⟨⟩, 204
Brillouin zone, mentioned, 54
broken symmetry, 10
Bromwich contour, 153
Brownian motion, 20, 61, 148
and random walk, 34
in external force fields, 72
of harmonically bound particles, 169
canonical
ensemble, 15, 191
transformation, 29, 92, 120, 160, 189
variables, 189
Carnot’s theorem, 1
Cauchy
distribution, 21
integral formula, 217
principal value integral, 217
causal
function, 164, 170, 217
transform, 220
causality, 164, 217
center of mass
frame, 5, 206
velocity, 5
central limit theorem, 42
certain event, the, 201
Chapman-Enskog method, 127
Chapman-Kolmogorov equation, see SCK
equation
characteristic function, 21, 42
also called moment generating function,
21
many-time, 44
chemical affinity, 8, 9
chemical potential, 2, 192
chemical work, 3
235236 ■ Index
Clausius inequality, 1, 3, 16
closed system, 1, 15, 120, 191
coarse graining, 120
collision
operator, 109
time, 109, 113, 123
collision-dominated regime, 124, 174
collisional invariant, 115, 123
collisionless Boltzmann equation, 150
colloidal particle, 61
combinatorics, mentioned, 202
commutator, defined, 196
complete set of functions, 193
compliance, 166
conditional probability densities, 25
conductivity tensor, 173
conjugate irreversible processes, 11
conservation of probability, 40
conserved moments, 123
conserved quantity, 4
consistency conditions, 25, 45
constant of the motion, 92, 190
constitutive relations, 10, 105
continuity equation, 5, 36
for probability, 70
for quantum probability, 193
continuous symmetry, 10
continuous-time stochastic process, 24
contraction of tensors, 7, 9, 73
convective
derivative, 5
transport, 4
convergence in mean, 26
Coriolis force, 24
correlation functions, 23
autocorrelation, 27, 161
covariance, 21
ensemble averaged, 159
equal-time, xiii, 161
stationary in equilibrium, 160
time, 161
two-time, 161
correlation time, 27
covariance, 21
principle of, 148
cross section, scattering
differential and total, 208
invariance properties, 112
cumulants, 42
many-time, 45
Curie’s theorem, 9, 177
cutoff parameters, 148
cyclic invariance of the trace, 164, 196
cyclic relation, 51
Debye length, 148
Debye-Huckel equation, mentioned, ¨ 149
decay process, 38
degeneracy, 196
degeneracy pressure, mentioned, 101
delta correlated, 65, 182
DeMoivre-Laplace theorem, 42
density
matrix, 195
operator, 195
density of states, xii, 192
detailed balance
Boltzmann equation, 115
Fokker-Planck equation, 77
master equation, 39
diffusion
coefficient, 10, 36
equation, 36
Einstein, 37
flow, defined, 5
tensor, 149, 179
diffusive transport, 4, 10
diluteness criterion, 109, 122
Dirac picture, 215
direct collision, 110
direct product, 91, 117
discrete symmetry, 10
discrete-time stochastic process, 24
dispersion relations, 108
dissipation, 63, 159
distribution function
N-particle, 92
reduced, 94
divergence of second-rank tensor, 6
Doob’s theorem, 47, 68, 72
drag force, 64
drift speed, 66
Dufour effect, 11
dyadic notation, 6, 100
contraction of rank-two tensors, 7, 73
dynamic structure factor, 162
dynamical pictures in quantum mechanics,
213
effective potential, 207
Einstein
diffusion equation, 37Index ■ 237
equivalence principle, 81
fluctuation theory, 20, 160
probability distribution, 20
relation, 65, 66, 178
elastic collisions, 205
electromigration, 151
electrostatic waves, 152
energy
representation, 2
density matrix, 196
ensemble, xii, 19, 92, 159, 190
average, xii, 19, 191
quantum, 195
canonical, 15, 191
grand canonical, 15, 192
microcanonical, 120
entropy, xi
as missing information, 121
Boltzmann, 20, 119
created in irreversible processes, 1
extensive, 2, 118, 119
fluctuation, 12, 20
Gibbs, 119, 197
maximum in equilibrium, 12, 15
production, 13, 15
representation, 2
Shannon, 119, 198
sources, 2, 9, 105
von Neumann, 197
environment, defined, xi
equal a priori probabilities, 120
equation of motion, 7
equation of state
existence, 2
local, 3, 106, 123, 138
equilibrium, xi
detailed balance, 39, 77, 115
Feynman definition, xi, 10, 27
local, 3, 118
maximum entropy, xii, 12
steady state, 15
thermodynamic, xii, 3, 15, 116
equipartition theorem, 32, 63, 64, 116
ergodic, 29
hypothesis, 19, 24, 48
stationary process, 30
ergodicity, 29
Euler equations of fluid dynamics, 124
events in probability theory, 201
exact differential, mentioned, 1, 2, 85, 117
expectation value, 204
quantum, 193
exponential of a matrix, 40
extensive variables, xii, 2, 20, 118
external parameters, xii
Feynman definition of equilibrium, xi, 10, 27
Fick’s law of diffusion, 10, 36
first law of thermodynamics, 2
fluctuation, xii, 12, 47
-dissipation theorem, xiv, 63, 171
second, 182
as a stationary stochastic process, 27
local, 160
time-reversal properties, 23
flux, 4
nonequilibrium thermodynamics, 9, 13
Fokker-Planck
equation, 69, 73
connection with Landau equation,
148
operator, 73
formal solution of differential equations, 40,
62
Fourier’s law of heat conduction, 10, 132
Fredholm theorems, 209
free Brownian particle, 64, 76
free fall, 64
frequency interpretation of probability, 202
Fubini’s theorem, mentioned, 211
functional defined, 109
functional equation, 47, 85
Γ-space, 39, 91, 189
gain-loss form
Boltzmann equation, 111
master equation, 50
Galilean invariance of temperature, 117
Gaussian
distribution, 20, 36, 42
integrals, 55
process, 41
variable, 60
generalized function, 97, 220
generalized Langevin equation, 179, 182
generalized susceptibility, 166
holomorphic in closed upper half plane,
166
generating function, 42, 56
Gibbs
entropy, 119, 197
equation, 2238 ■ Index
paradox, mentioned, 2
Ginzburg-Landau theory, mentioned, 121
global equilibrium, 3
global Maxwellian, 118
gradient notation, 6
Gram-Schmidt method, mentioned, 76, 180,
210
grand canonical ensemble, 15, 192
Green-Kubo relations, 172
H-theorem, see Boltzmann H-theorem
Hamilton’s equations of motion, 189
hard core potential, 205
hard sphere approximation, 122
harmonic analysis, mentioned, 30
heat
energy transfers to microscopic degrees
of freedom, 159
engine, 1
flux vector, 8, 102
Heisenberg
equation of motion, 214
representation, 161, 214
uncertainty principle, 118, 198
Hermite polynomials, 42, 77, 87, 89
Mehler’s formula, 78
Hermitian operator, 193
hierarchy
of reduced probabilities, 95
of stochastic processes, 24
Hilbert decomposition, 125
Hilbert transform, 166, 218
holomorphic function, 166, 217
homogeneity, 105
hydrodynamic
fields, 97, 123
normal modes, 107
regime, 123
hydrodynamic regime, 174
hydrostatic pressure, 6, 101
ideal gas, fiction of, 117
identical particles, 194
imaginary time, 165
impact parameter, 111, 207
impossible event, 201
incompressible flow, 5, 190
independent
events, 201
stochastic process, 28
indeterminism, 193
indistinguishability
factor, 119
of identical particles, 119, 194
inelastic scattering, 162
information theory, 119, 121, 198
information is physical, 121
integral equations, 209
integral invariants of Poincar´e, 111
integral operator, 212
integro-differential equation, 37, 112
intensive variables, xii, 2
local, 3
interaction representation, 215
interactions required for irreversibility, 117
internal energy, 2, 8, 102
inverse collision, 110, 111
irreversible
process, xii, 1
thermodynamics, 1
Irving-Kirkwood approach, 100
isentropic flow, 8, 17
isolated system, 1, 120
isothermal compressibility, 21, 105, 141, 160
isothermal magnetic susceptibility, 160
isotropy, 105
italic font, use of, xi
Jacobian determinants, 51
jellium, 151
Johnson noise, 32
joint probability density, 20, 94, 204
jump length, 70, 86
jump moment, 86
Kelvin relation, 15
kernel of an integral equation, 209
kinetic coefficients, xiii, 11
kinetic equation
Boltzmann, 111
defined, 109
kinetic theory, 91
Klein-Kramers equation, 73
Knudsen
gas, 109
number, 123
Kolmogorov
axioms, 201
extension theorem, 25
forward equation, 68
Kramers equation, 73
Kramers escape problem, 74Index ■ 239
Kramers-Kronig relations, 166, 217
Kramers-Moyal expansion, 86, 91
Kubo’s
formula
response function, 164
identity, 165
laboratory frame, 205
Landau
damping, 151
kinetic equation, 145, 147
H-theorem, 156
length, 148
Landauer’s principle, 121
Langevin equation, 62
generalized, 182
Laplace transform, 153
law of large numbers, mentioned, 19
left eigenvectors, 40, 58, 89
Legendre transformation, mentioned, 2
Lennard-Jones potential, 205
Lie algebra, mentioned, 198
Lie group, mentioned, 185
linear response theory, 162
linearized
Boltzmann equation, 125
collision operator, 125
hydrodynamics, 107
Vlasov equation, 153
Liouville
equation, 92, 191
inhomogeneous, 163
operator, 92
free particles, 93
interacting particles, 95
self-adjoint, 92, 143, 185
stationarity, 92, 160
theorem, 19, 29, 92, 190, 191
complex analysis, 166
local equation of state, 3, 123
local equations, 2
local fluctuation, 160
local Maxwellian, 118, 124, 176
N-body, 174
local thermodynamic equilibrium, 3, 16,
106, 123, 174
from the Boltzmann equation, 118
local velocity, 98
logarithm of a matrix, 197
Lorentz force, 24
Loschmidt’s paradox, 121
µ-space, 111, 118, 119
macroscopic, defined, xi
macrostate, 20
Markov
chain, 28
process, 28
mass fraction, 3, 5
master equation, 37
matrix
exponential, 40
logarithm, 197
Maxwell molecules, 132
Maxwell-Boltzmann distribution, 68, 71, 116
invariant under collisions, 117
solution of the Landau equation, 147
Maxwellian function, 77
global, 118
local, 118
mean free path, 122
mean of a random variable, 204
measure theory, mentioned, 29, 202
mechanical transport coefficients, 172
Mehler’s formula, 78
memory
as correlations, 28
function, 182
function equation, 183
method of characteristics, mentioned, 71
metric transitivity, mentioned, 29
microstate, 20, 121
mixed quantum state, 194
mobility, 66
molecular chaos assumption, 113, 122, 137
moments
also called correlation functions, 21
generating function, 21, 42
of a probability distribution, 204
momentum flux tensor, 99, 176
mutually exclusive events, 201
natural motion in phase space, 29, 39, 120,
159
Navier-Stokes equation, 6, 131
Neumann series, 211
Newton’s law of viscosity, 10, 132
Newtonian fluid, 10
noise power, 33
non-divergent tensor, 130
non-Markov process, 29, 53
noncompensated transformation, 1
nonequilibrium thermodynamics, 1240 ■ Index
normal distribution, 20
normal modes of linearized hydrodynamics,
107
normal solutions of the Boltzmann equation,
106, 123, 128
null space, 125
number density, 10
Nyquist theorem, 33
occupation number formalism, mentioned,
194
Ohm’s law, 10
anisotropic, 173
one-body operator, 93
Onsager
reciprocal relations, xiii, 12, 24, 48
regression hypothesis, 13, 24, 26, 172,
181
open system, 1, 15, 192
Ornstein-Uhlenbeck process, 72
orthogonal complement, 181
partition function, xi, 119, 192
passive transformation, 214
Pauli exclusion principle, 193, 194
peculiar velocity, 101
Peltier effect, 11, 14
periodic function, 30, 54
phase point, 91
phase space, xi, 91
function, 189
probability density, 92, 191
trajectory, 91
phenomenological force-flux relations, 10
Planck spectrum, 33
plasma, 145
dielectric function, 154
frequency, 154
Plemelj formula, 220
Poisson bracket, defined, 189
Poisson-Boltzmann equation, 149
positive definite, defined, 51
positivity condition, 167
power spectrum, 31
pressure tensor, 6, 99
symmetry of, 141
probability
axioms, 201
conditional, 25, 201
current density, 70
density, 203
joint, 20, 204
phase space, 92
distribution, 203
joint, 203
events, 201
frequency interpretation, 202
transition, 29
process with independent increments, 67
products, chemical reactions, 4
pure quantum state, 194
purely random process, 28
quadratic degree of freedom, 32
quadratic form, 51, 58
random, 19
force, 64
independent of velocity, 64
number, 202
phenomenon, defined, 202
variables, 24, 26, 202
convergence, 26
independent, 203
phase space, 92
walk, 34
biased, 54
master equation, 38
symmetric, 34, 35
rate-of-strain tensor, 130
reactants, chemical reactions, 4
reaction coordinate, 9
reduced distribution function, 94
reduced mass, 205
region of interation, 206
relaxation function, 170
relaxation time, 64, 113, 123, 130, 178
renormalization, mentioned, 43, 121
representations
in quantum mechanics, 213
in thermodynamics, 2
resolvent, 209, 212
equations, 212
kernel, 211
set, 212
resonance absorption, 170
response function
classical, 163
quantum, 164
reality of, 165
restituting collision, 111
restriction of a function, 217Index ■ 241
reversibility paradox, 121
reversible process, 1
Riemann integral, 26, 67
s-tuple distribution functions, 94
Sackur-Tetrode equation, 118
sample space, 201
scaled form of Boltzmann’s equation, 124,
128
scaling theories, mentioned, 162
scattering cross section, see cross section,
scattering
Schrodinger representation, ¨ 161, 213
screening, 148
second law of thermodynamics, 1, 2, 23, 120
sedimentation, 81
Seebeck effect, 11, 15
self-consistent field, 151
Shannon entropy, 119, 198
shot noise, 32
signature under time reversal, 24
signum function, 218
similarity transformation, 41, 77
simple fluid, defined, 97
singular perturbation, 127
Slutsky’s theorem, 30
Smoluchowski equation, 80
Smoluchowski-Chapman-Kolmogorov
equation (SCK), 29, 47, 214
differential form, 38
discrete state space, 34
smooth function, 68
solvability conditions, 125
Sonine polynomials, 133
Soret effect, 11
source term, defined, 4
specific (per mass) quantities, 3
spectral density, 31, 162
spectrum, 212
stability condition of thermodynamics, 12
standard deviation, 204
state of
quantum-mechanical systems, 193
classical-mechanical systems, 92, 189
equilibrium systems, xi, 2
statistical-mechanical systems, 92
state space
probability, 34, 38, 56, 202
thermodynamic, 2
state variables, xi, 2, 19, 20, 117, 119, 120
state vector, quantum, 193
stationarity condition, 92, 160
stationary state, see steady state
stationary stochastic process, 27, 49
ergodicity of, 29
Fourier representation, 30
in the wide sense, 27, 29
statistical independence, 201, 203
statistical operator, see density operator
statistical regularity, 202
steady state, 15, 66
step function, 221
Stieltjes integral, 67
stochastic differential equation, 26, 62
stochastic process, 19, 24, 48
continuous time, 24, 37
derivative of, 26
differential, 66
discrete time, 24
Gaussian, 41
integral of, 26
specified by joint probabilities, 24
stoichiometric coefficient, 4
Stokes formula, 64
Stone’s theorem, 185
Stosszahlansatz, 113
stress tensor, 6, 10
structure factor, 162
dynamic, 162
static, 160
subensemble, 25, 62, 64, 68, 83
summational invariant, 115
suspension medium, 61
symmetry condition on joint probabilities,
25, 53
system point in Γ-space, 39, 91, 189
system, defined, xi
systematic force, 64
temperature
defined in kinetic theory, 116
Galilean invariant, 117
temporal homogeneity, 38
tensor notation
bold Roman font, 6
double dot product, 7
dyadic, 6
thermal
conductivity, 10, 105, 132
tensor, 178
diffusivity, 107
expansivity, 21, 141242 ■ Index
motions, 8, 174
noise, 32
speed, 65
transport coefficients, 172
velocities, 101
voltage, 66
wavelength, 118
thermally activated process, 76
thermodynamic
equilibrium, 3, 15
force, 9, 12
limit, 3
thermodynamics, xi
nonequilibrium, 1, 15
versus thermostatics, 2
thermoelectric effect, 11, 14
thermophoresis, 11
time average, xii, 19, 23, 190
time correlation function, 161
time derivatives expressed by space
derivatives, 128, 176
time evolution operator, 92, 160, 214
time-reversal symmetry, 23
time-reversed collision, 111
Titchmarsh theorem, 166, 170, 218
trace, 195
basis independent, 196
cyclic invariance, 196
transition
matrix, 39
moment, 69
vector, 73
probability, 29, 68
rate, 37
transport coefficients, 10
Chapman-Enskog theory, 131
Green-Kubo theory, 172
tuned circuit, 33
two-time correlation functions, 161
ultraviolet catastrophe, mentioned, 33
uniqueness of power series, 42, 57, 124, 127
unit tensor, I, 6
unitary operator, 214
van der Waals pressure reduction, 101
variance of a random variable, 204
virtual variations, xii
viscosity coefficient, 6, 64, 105, 178
kinematic, 107
viscous stress tensor, 6
Vlasov equation, 150
H-theorem, 157
time-reversal invariant, 151
Vlasov-Landau equation, 150
von Neumann
entropy, 197
equation, 196
weak coupling, 145
Weyl correspondence, 162, 198
what is not work is heat, 3, 180
white noise, 31
Wiener-Khinchin theorem, 31, 49
Wiener-Levy process, 37, 55, 67, 85
work-energy theorem, 7
zeroth law of thermodynamics, 2, 3
